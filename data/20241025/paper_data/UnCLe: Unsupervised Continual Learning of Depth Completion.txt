UnCLe: Unsupervised Continual Learning of Depth Completion
Suchisrit Gangopadhyay1∗, Xien Chen1∗, Michael Chu1∗, Patrick Rim1, Hyoungseob Park1, Alex Wong1
I. ABSTRACT Continual (or lifelong) learning is the training paradigm
that addresses the challenge of catastrophic forgetting by
WeproposeUnCLe,astandardizedbenchmarkforUnsuper-
enabling a single model to learn from a continual non-
vised Continual Learning of a multimodal depth estimation
stationary stream of datasets, adapting to new tasks while
task: Depth completion aims to infer a dense depth map
preserving performance on previous tasks. Retraining models
from a pair of synchronized RGB image and sparse depth
for supervised 3D tasks on new domains is often impractical map. We benchmark depth completion models under the
due to the difficulty and high cost of acquiring accurate
practical scenario of unsupervised learning over continuous
ground-truth. This highlights the need for methods that can
streams of data. Existing methods are typically trained on a
learn new data distributions in an unsupervised manner.
static,orstationary,dataset.However,whenadaptingtonovel
Therefore, our work investigates unsupervised continual
non-stationary distributions, they “catastrophically forget”
learningfordepthcompletion,aimingtoenableamultimodal
previously learned information. UnCLe simulates these non-
depthestimationmodeltolearnnewdomainswithoutground-
stationary distributions by adapting depth completion models
truthwhileretainingperformanceonpreviouslyseendomains.
to sequences of datasets containing diverse scenes captured
This paper presents a comprehensive benchmark for ex-
fromdistinctdomainsusingdifferentvisualandrangesensors.
isting unsupervised continual learning methods for depth
We adopt representative methods from continual learning
completion to streamline further research in this area. We
paradigmsandtranslatethemtoenableunsupervisedcontinual
beginwiththemotivatingobservationofhowtheperformance
learningofdepthcompletion.Webenchmarkthesemodelsfor
of three unsupervised depth completion models [5], [6],
indoor and outdoor and investigate the degree of catastrophic
[7] degrades after being finetuned on two sequences of
forgetting through standard quantitative metrics. Furthermore,
datasets.Weadoptthreecanonicalcontinuallearningmethods
weintroducemodelinversionqualityasanadditionalmeasure
(i.e., regularization-based [8], [9] and rehearsal-based [10]
of forgetting. We find that unsupervised continual learning
methods) for the 3D perception task of unsupervised depth
of depth completion is an open problem, and we invite
completion. We evaluate on sequences of both indoor [11],
researchers to leverage UnCLe as a development platform.
[7], [12] and outdoor [13], [14], [15] datasets and detail our
evaluation protocols for future methods.
II. INTRODUCTION
Furthermore, we present model inversion as a new quali-
Three-dimensional (3D) perception is fundamental for
tative evaluation protocol to measure catastrophic forgetting
spatial tasks such as autonomous navigation, robotic manip-
of 3D vision models. Model inversion is the reconstruction
ulation, and augmented reality. The systems to tackle these
of input data given the corresponding output and the frozen
tasks are typically built upon a sensor suite including range
model weights. We can compare continual learning methods
(e.g. lidar, radar) and optics (e.g. camera). Depth completion
by evaluating the fidelity of reconstructed input data from
is the task of recovering the dense depth map of a 3D scene
previously seen domains using the weights trained on new
from the measurements of these range sensors, which often
tasks. This is based on the assumption that a model that can
yield sparse 3D coordinates, by leveraging the dense 2D
reconstruct the input image has experienced less forgetting.
topology captured in RGB images. Our contributions are as follows:
Despite these advantages, existing depth completion meth-
• Wearethefirsttoadaptexisting,canonicalregularization-
ods are predominately trained and tested on a single dataset
based and rehearsal-based continual learning methods
under the assumption that the distribution of 3D scenes is
for unsupervised depth completion. After implementing
stationary. When deployed in the real world with evolving
these methods, we re-tune the existing best training
conditions and unseen domains where the encountered distri-
settings for the depth completion models that we use
butionisnon-stationary,themodelmustbecontinuallytrained
for each continual learning method.
to adapt to new scenes. However, continual re-training can
• We provide extensive quantitative results on a sequence
leadtotheproblemofwhereamodel’sperformancedegrades
of indoor datasets, and on another sequence of outdoor
significantlyonpreviouslylearnedtaskswhentrainedonnew
datasets. In this way, we demonstrate the effectiveness
tasks [1], [2], [3], [4]. This problem is commonly referred to
of continual learning methods for lifelong learning of
as “catastrophic forgetting.”
tasks spanning from robotics to autonomous driving.
• We introduce model inversion, a novel qualitative evalu-
*Equalcontribution
ation protocol to measure catastrophic forgetting of 3D
1AuthorsarewiththeDepartmentofComputerScience,YaleUniversity,
CT06520,USA,{firstname.lastname}@yale.edu vision models.
4202
tcO
32
]VC.sc[
1v47081.0142:viXraIII. RELATEDWORKS GANs or VAEs to produce synthetic data mimicking the
data distributions of previous tasks. Variational autoencoder
A. Continual Learning
approaches[36],[37],[38],[39],[40],[41]areabletocontrol
Regularization-based One approach to mitigating catas- the generated data labels, but suffer from blurry quality.
trophic forgetting is through regularization, which introduces GAN approaches [42], [41], [43] are able to improve the
constraints or penalties to the loss function to ensure that quality of the generated input data over the VAE methods.
new tasks are learned without significantly altering the Feature replay stores data on the feature-level instead of the
weights from previous training. The first class of methods raw input data in order to be less resource-expensive. Some
in this category focuses on weight regularization, selectively approaches [44], [45], [46] use feature distillation between
constrainingchangestonetworkparametersthatareimportant new and old models. [47] stores initial class statistics, like
for previous tasks. Elastic Weight Consolidation (EWC) mean and covariance, to rectify biases in predictions. For
[8] selectively decreases the plasticity of model weights our benchmark, we opt to use Experience Replay as our
determined by the Fisher information matrix. [16] estimates baseline for rehearsal-based continual learning methods as it
parameter importance based on their contribution to loss isthefoundationforrehearsal-basedmethods.Itsdirectuseof
changesduringtraining.[17]assessimportancebymeasuring actual training data, despite being resource-intensive, makes
the sensitivity of model outputs to parameter variations. [18] it a more effective method, especially crucial for the high
combines the regularization techniques of [8] and [16] to fidelity required in unsupervised depth completion tasks. We
leverage their strengths in mitigating catastrophic forgetting. userandomizedsamplingbecausemanyoftheotherdiscussed
The other class of regularization-based techniques focuses sampling methods are either specific to classification tasks
onfunctionregularization,whichaimstopreservethemodel’s or have similar performance.
output behavior on previous tasks by constraining changes Token-based These methods aim to store information
in the intermediate features or final predictions through learned from previous tasks in a small, fixed set of weights
the knowledge distillation (KD) framework [19]. Learning called “tokens,” which can be thought of as learnable biases.
Without Forgetting (LwF) [9] is a pioneering approach that Some methods additionally utilize a small memory buffer,
mitigates catastrophic forgetting by using the new task data but others are able to obtain similar performance without
to approximate the responses of the old model. To further one. They are often cheap and scalable, and recent methods
enhance retention, LwM [20] integrates attention maps into have achieved state-of-the-art results in continual learning for
the KD process to capture essential feature representations, image classification. However, these methods are currently
whileEBLL[21]preservesfeaturereconstructionsbyutilizing for 2D classification tasks [48], [49], [50] or can only be
task-specific autoencoders. Methods like GD [22] leverage applied to transformer-based architectures [51]. Token-based
external unlabeled data to extend regularization beyond the continual learning methods for 3D regression tasks, such
training set, increasing the model’s ability to generalize to as depth completion, have not yet been explored. Thus, we
unseen tasks. Generative approaches such as MeRGANs [23] are not able to obtain baselines for these methods for depth
and LifelongGAN [24] simulate old task data, thus reducing completion.
theneedforstoringlargedatasets.Additionally,feature-based Continual Learning for Depth Estimation The Mon-
regularization techniques like PODNet [25] and LUCIR [26] oDepthCL[51]framework,employsadual-memoryrehearsal-
work to preserve feature similarity, addressing distribution basedmethodtoaddressthechallengesofcatastrophicforget-
shifts between tasks and improving overall stability. Bayesian ting in unsupervised monocular depth estimation. CoDEPS
approaches also offer robust function regularization. FRCL [52] employs a unique domain-mixing strategy for pseudo-
[27] and FROMP [28] use probabilistic models to regularize label generation with efficient experience replay. However,
the functional space, providing smooth task transitions by previous works in this field typically focus on a single
quantifying uncertainty. Broader frameworks like VCL [29] modality and lack standardized benchmarks, limiting their
extend these ideas, using variational inference to maintain a applicability to real-world scenarios where the use of multi-
balancebetweentaskstabilityandplasticity.Inthispaper,we modal data is standard. Our paper contributes to this field by
choose to concentrate on EWC and LWF as we find them to establishing a benchmark for depth completion using both
be milestone works of the two classes of regularization-based images and sparse depth data, addressing these gaps and
approaches. setting a foundation for future research in continual depth
Rehearsal-based Replay-based approaches in continual estimation.
learning mitigate catastrophic forgetting by reintroducing
past examples during the training of new tasks. These
B. Depth Completion
methods include Experience Replay (“Replay”) [10], which
stores actual samples from previous tasks to train on again Depthestimationisthetaskofreconstructingthe3Dscene
and reinforce performance on previously seen data. Within fromvisualinputs,e.g.,apairofcalibratedstereoimages[53],
Experience Replay, there are different selection strategies for [54], [55], [56] or a monocular image [57], [57], [58], [59],
choosing the replay buffer, such as randomized Reservoir [60], [61], [62], [63], [64]. Inferring depth from a single
Sampling [30], [31], [32], class-based sampling [33], [34], image is an ill-posed task; recovering metric depth typically
and Generative replay [35] utilizes generative models like requires an additional sensor, e.g., lidar [65], [66], radar [67].Depth completion [68], [69], [70], [71] is the task of pose matrix g ∈SE(3). To estimate g , we jointly train a
τt τt
inferring a dense metric depth from an RGB image and pose network p (I,I ):
θ τ
synchronized sparse point cloud. Supervised methods [72], Iˆ (x,dˆ,g )=I (cid:0) πg K−1x¯dˆ(x)(cid:1) . (1)
[73], [74], [75], [76], [77], [78], [79] rely on large datasets τ τt τ τt
with ground-truth dense depth maps, which are used to train In this formula, K is the camera’s intrinsic matrix, π is
models capable of mapping sparse depth points and RGB the canonical perspective projection, and x¯ represents the
imagestodensedepthmaps.Whileeffective,theseapproaches homogeneous coordinates [x⊤,1]⊤ of x ∈ Ω. We train
are limited by the availability and cost of obtaining annotated p (I,I )duringtraining-time,butitisnotusedforinference.
θ τ
data, making it less practical for continual learning scenarios In the continual learning setting, our objective is to adapt
wherethecollectionofnewdatamaynotincludedensedepth a pretrained model f and p , trained on an initial dataset
θ θ
map annotations. Supervised depth completion methods are T = {(I(i),z(i),K(i))}n , to a sequence of new datasets
0 0 0 0 i=1
less practical than unsupervised approaches, as achieving T ,T ,...,T , where each T ={(I(i),z(i),K(i))}nk and
1 2 N k k k k i=1
precisedensedepthmapsforeverytaskinacontinuallearning
n denotes the number of training examples in T . The
k k
scenario is challenging.
challengeistoincrementallytrainbothf andp oneachnew
θ θ
Unsupervised depth completion methods [80], [81], [7],
target dataset T without experiencing significant degradation
k
[6], [82], [83], [68], [70], in contrast, leverage the inherent
in performance on the source dataset S and all previously
structure and patterns within the RGB image and sparse
seen target datasets T ∀j < k. This requires effectively
j
depth data to infer missing depth values, making them more
mitigating catastrophic forgetting while enabling the model
adaptable for continual learning as they do not require dense
togeneralizeacrossdifferentdomainsinthedepthcompletion
ground-truth depth maps. [80] utilized Perspective-n-Point
task.
[84] and RANSAC [85] to align adjacent video frames. [68]
Unsupervised Depth Completion Loss. We train f (I,z)
θ
trains a depth prior constrained on the image. FusionNet
to minimize
[6] leverages synthetic data to learn a prior from a scene,
while [86] converts synthetic data to real domain to use L=w phℓ ph+w szℓ sz+w smℓ sm (2)
rendered depth. VOICED [7] approximated a scene with
where each loss term can be weighted differently by their
scaffolding. [82] introduced an adaptive scheme to reduce
respective w.
penalties incurred on occluded regions. KBNet [5] maps
The photometric consistency loss,
the image onto the 3D scene frustum using calibrated back-
projection.[83]decouplesscaleandstructure.[87]usesvisual ℓ = 1 (cid:88)(cid:88) w |Iˆ (x)−I(x)|+
SLAM features and [69] proposed monitored distillation for ph |Ω| co τ (3)
τ∈Tx∈Ω
positive congruent training. w (cid:0) 1−SSIM(Iˆ (x),I(x))(cid:1) ,
However, all of these methods are subject to catastrophic st τ
forgetting as they rely on updating model weights to fit onto encourages both structural and color similarity between the
a new target dataset without the objective of retaining past reconstructed and actual center images.
information. This aspect makes it challenging for them to The sparse depth consistency loss,
a pd rea vp it outo slya sc eo ennt ti an su ka sl nl ee ea drn ti ong bes he att ni dn lg edw wh ie tr he oub to pth erfn oe rw maa nn cd e ℓ sz = |Ω1 | (cid:88) |dˆ(x)−z(x)|. (4)
z
degradation on either. Therefore, it is critical to develop x∈Ωz
methods for these models that enable continual learning, encourages consistency between the input sparse depth and
ensuring that they can effectively adapt to new data while predicted depth map over the sparse depth map’s domain
preserving previous examples. (Ω ).
z
The local smoothness loss,
IV. METHODFORMULATION
Problem definition. The task of unsupervised depth ℓ sm = |Ω1 | (cid:88) λ X(x)|∂ Xdˆ(x)|+λ Y(x)|∂ Ydˆ(x)|. (5)
completion can be formulated as follows: Let I :Ω⊂R2 → x∈Ω
R3 + denote an RGB image at current timestamp t obtained encourages local smoothness by penalizing depth map
from a calibrated camera, and z :Ω z ⊂Ω→R + represent gradients in the x− (∂ X) and y− (∂ Y) directions. These
the corresponding sparse depth map derived from a projected gradients are weighted by the corresponding image gradients
3D point cloud. Given the image I, the sparse depth map z, λ
X
=e−|∂XIt(x)| andλ
Y
=e−|∂YIt(x)| toaccountforobject
and the intrinsic calibration matrix K ∈R3×3, the goal is to edges.
learn a function f (I,z) that estimates a dense depth map,
θ
A. Elastic Weight Consolidation
which accurately recovers the distances between the camera
and all visible points in the 3D scene, without relying on For the task of continual depth completion, we implement
ground-truth depth annotations. Similarly to [5], instead of EWC as follows. When training on a new dataset T ,
k
usinggroundtruthweuseaphotometricreprojectionerrorby we first load the previously trained model f , which is
θ∗
comparing the reconstructed image I from adjacent frames parameterized by the weight matrices θ∗ optimized for T .
t k−1
I for τ ∈{t−1,t+1} with the estimated relative camera These parameters are frozen and treated as a reference during
τtraining on T . The Fisher information matrix F quantifies Let D =T denote the set of new training data points
k i new k
each parameter’s θ importance to the previous model’s fromthecurrenttargetdataset,whereN isthenumberofnew
i
performanceonT .ThefinalEWClossisthereforedefined data points. Similarly, let D = {(I(j),z(j),K(j))}M
k−1 replay r r r j=1
as: denote the set of replayed data points from the buffer, where
M is the number of data points in the replay buffer. Each
(cid:88)1
L =λ F (θ −θ∗)2 (6) training batch is made up of a 50-50 new-replay data ratio,
ewc ewc 2 i i i
wherethereplayhalfisevenlysplitbetweenpreviousdatasets.
i
The total loss function L for a training iteration is then
EWClosspenalizesimportantweightsfrommovingtoofar total
formulated as:
away from their values optimized for the previous task. We
add this to the total loss for unsupervised depth completion L =L +L (8)
total new replay
L in Eq. 2 with λ = 1, which we found to be optimal
ewc
after extensive tuning. where L new is the unsupervised loss L in Eq. 2 computed on
the new training data, and L is the unsupervised loss L
replay
B. Learning Without Forgetting computed on the replayed data.
To implement Learning Without Forgetting (LwF) for During training, the loss L replay computed from the data
the depth completion task, we aim to preserve output points in the replay buffer is equal to total loss L total from
behavior from previous tasks while training only on new the preceding continual training step, ensuring consistency
data. Specifically, we load the checkpoint of the old model and retention of previously acquired knowledge.
f , which are kept frozen during training. This old model
θ D. Model Inversion
processes the new data to generate predictions, denoted as
Iˆ (x,dˆ,g ), where dˆrepresents the predicted depth and g We propose a method of using model inversion to evaluate
τ τt τt
is the relative pose transformation between frames. a model’s forgetting. In other words, we can try to retrieve
We compute a mean squared error (MSE) loss between the an image that a model f ω(I,z) was trained on using just its
outputsofthefrozenmodelandthenewmodelf′.ThisLwF weights and a supervised output complete depth map d. To
θ
loss encourages the new model’s predictions Iˆ′ (x,dˆ,g ) invert a depth completion model, we freeze the model, start
to remain consistent with the frozen model’s pτ redictioτ nt s. with a uniformly random set of pixels I˜, and update I˜ to
Simultaneously, the joint loss function L
lwf
includes the loss minimize ∥f θ(I˜,z)−d∥ 1, where z is the data point’s actual
between the new model’s output and the ground truth depth sparse depth map. To evaluate the model’s forgetting we use
data from the current task. The final loss function is defined the pretrained model f θ(I,z) to get d = f θ(I,z), where I
as: andz aretheactualinputimageandsparsedepthforthedata
point. Thus, we ultimately minimize ∥f (I˜,z)−f (I,z)∥
ω θ 1
to retrieve an old training image from f (I,z), having been
L =λ ·L +L (7) ω
lwf o old new
trained on new data. Although we try to retrieve a trained
Here, L old computes the loss between the frozen model’s image, a similar method can be used to retrieve the input
output Iˆ τ and the new model’s output Iˆ′ τ, while L new is the sparse depth map.
loss between the new model’s output I and the ground truth
n
depth map z . Both L and L include all the terms in V. EXPERIMENTS
n old new
the unsupervised loss algorithm L in Eq. 2. The parameter For evaluation, we compute four key metrics: Mean
λ balances the contribution of the old and new task losses. Absolute Error (MAE), Root Mean Squared Error (RMSE),
o
After many experiments, we set the optimal λ to be 0.5. Inverse MAE (iMAE), and Inverse RMSE (iRMSE). These
o
This approach allows the model to learn from new depth metrics are described in detail in Table XIV. For qualitative
data without requiring access to previous task data, as the results, we use the proposed model inversion method to
frozen model’s predictions act as a proxy for past knowledge. measure catastrophic forgetting on the previous task dataset.
Our experiments consist of continual training on both indoor
C. Experience Replay
and outdoor datasets. For indoor sequences, we first train on
To perform experience replay with depth completion, we NYUv2 (Table I) and evaluate the continual learning process
maintain a replay buffer, which retains a fixed number of over the sequences NYUv2 → ScanNet → VOID (Tables II,
representative samples from each dataset previously trained III, and IV), and NYUv2 → VOID → ScanNet (Tables V,
on.Theselectionsandtheintegrationofthereplaybufferinto VI, and VII). For outdoor sequences, we train on KITTI and
thenewtaskaredesignedtobalancecomputationalefficiency evaluate on KITTI → Virtual KITTI → Waymo (Tables VIII,
with performance retention. Empirically, a buffer size of IX, and X), and KITTI → Waymo → Virtual KITTI (Tables
64 data points for each previous dataset was determined to XI, XII, and XIII).
offer an optimal balance. During each training iteration on
A. Datasets
a new target dataset, a batch of data from the replay buffer
is reintroduced into the training process. The data points We use three datasets for indoor experiments and three
from both the current and historical datasets are processed datasets for outdoor experiments. Indoor datasets: The NYU
as follows: Depth V2 dataset [11] contains 464 indoor scenes capturedTABLE I: Performance of the pretrained models on the NYUv2 (initial indoor) and KITTI (initial outdoor) datasets.
NYUv2 KITTI
Model MAE↓ RMSE↓ iMAE↓ iRMSE↓ MAE↓ RMSE↓ iMAE↓ iRMSE↓
VOICED 94.641 177.134 18.492 39.309 295.410 1159.270 1.200 3.490
FusionNet 102.872 192.23 20.913 42.821 267.690 1157.070 1.080 3.190
KBNet 85.842 169.833 17.065 35.368 256.961 1122.597 1.011 3.155
TABLE II: Evaluation on NYUv2. Starting from NYUv2 pretrained weights, models are trained on ScanNet (NYUv2 → ScanNet) and
subsequently on VOID (NYUv2 → ScanNet → VOID)
NYUv2→ScanNet NYUv2→ScanNet→VOID
Model Method MAE↓ RMSE↓ iMAE↓ iRMSE↓ MAE↓ RMSE↓ iMAE↓ iRMSE↓
Finetuned 102.556 193.830 19.794 44.300 105.918 193.336 19.977 42.623
EWC 99.426 188.504 19.866 45.909 111.247 205.710 19.425 39.442
VOICED
LWF 106.500 198.029 24.327 64.434 111.632 200.986 19.580 40.796
Replay 101.879 187.149 20.214 43.836 104.386 187.938 21.597 45.943
Finetuned 106.999 202.943 20.984 43.806 117.709 205.254 21.647 40.639
EWC 113.555 217.060 23.441 50.584 107.319 201.506 19.765 39.049
FusionNet
LwF 100.577 188.393 18.645 36.827 103.157 192.620 18.413 35.504
Replay 102.422 193.054 20.908 43.278 100.180 187.368 19.583 39.493
Finetuned 94.357 193.184 17.321 38.087 108.699 215.057 19.139 40.711
EWC 90.145 186.402 16.529 36.180 102.277 200.576 17.945 37.814
KBNet
LwF 97.987 186.385 18.598 38.178 91.784 177.807 17.628 37.366
Replay 91.243 175.683 18.957 39.680 95.622 184.347 19.312 40.620
TABLE III: Evaluation on ScanNet. Starting from NYUv2 pretrained weights, models are trained on ScanNet (NYUv2 → ScanNet) and
subsequently on VOID (NYUv2 → ScanNet → VOID)
NYUv2→ScanNet NYUv2→ScanNet→VOID
Model Method MAE↓ RMSE↓ iMAE↓ iRMSE↓ MAE↓ RMSE↓ iMAE↓ iRMSE↓
Finetuned 19.077 47.153 7.421 18.953 20.261 47.069 7.850 18.953
EWC 18.496 46.451 7.155 18.673 19.553 47.143 7.375 18.933
VOICED
LWF 20.715 47.719 8.105 19.566 19.739 47.770 7.408 19.101
Replay 21.478 52.750 7.681 19.395 21.711 53.581 7.544 19.417
Finetuned 18.021 48.733 6.520 18.896 25.064 52.576 9.898 21.274
EWC 17.756 47.644 6.521 18.874 17.943 47.301 9.689 19.085
FusionNet
LwF 16.231 44.917 5.986 17.924 17.349 45.771 6.517 18.304
Replay 17.352 48.241 5.909 17.841 18.149 49.309 6.128 17.923
Finetuned 14.290 41.550 5.289 16.610 15.961 44.048 5.866 17.393
EWC 14.091 41.344 5.230 16.578 16.992 44.559 6.756 17.856
KBNet
LwF 16.060 43.786 5.675 16.748 17.358 45.771 6.028 17.111
Replay 14.690 42.702 5.100 16.192 15.265 43.755 5.721 16.447
TABLE IV: Evaluation on VOID. Starting from NYUv2 pretrained weights, models are trained on ScanNet (NYUv2 → ScanNet) and
subsequently on VOID (NYUv2 → ScanNet → VOID)
NYUv2→ScanNet→VOID
Model Method MAE↓ RMSE↓ iMAE↓ iRMSE↓
Finetuned 37.656 93.155 19.229 46.179
EWC 39.356 95.292 19.058 43.938
VOICED
LwF 37.585 90.167 19.413 44.805
Replay 42.275 100.427 21.953 49.866
Finetuned 36.010 87.008 18.540 43.159
EWC 35.195 87.492 17.600 42.119
FusionNet
LwF 34.572 85.400 17.499 41.203
Replay 35.363 91.210 18.582 45.632
Finetuned 32.313 83.962 16.163 40.282
EWC 35.139 89.733 18.923 45.515
KBNet
LWF 36.032 90.976 19.718 48.217
Replay 34.583 90.770 18.192 45.739TABLE V: Evaluation on NYUv2. Starting from NYUv2 pretrained weights, models are trained on VOID (NYUv2 → VOID) and
subsequently on ScanNet (NYUv2 → VOID → ScanNet)
NYUv2→VOID NYUv2→VOID→ScanNet
Model Method MAE↓ RMSE↓ iMAE↓ iRMSE↓ MAE↓ RMSE↓ iMAE↓ iRMSE↓
Finetuned 99.042 182.196 18.819 38.632 99.281 190.244 19.317 43.954
EWC 106.489 193.701 19.039 38.042 100.611 192.186 20.232 47.167
VOICED
LwF 104.732 188.505 20.060 44.116 100.974 189.749 20.457 44.925
Replay 103.689 190.762 20.651 43.913 103.848 189.240 22.354 48.550
Finetuned 103.278 193.581 19.207 38.495 108.858 208.497 22.423 48.256
EWC 103.478 193.886 18.983 37.579 107.600 206.906 22.223 48.358
FusionNet
LwF 100.143 186.084 18.301 35.863 95.975 180.125 17.545 34.595
Replay 102.832 191.146 20.288 40.693 99.377 186.812 19.513 39.477
Finetuned 122.475 241.367 21.239 45.710 94.590 194.518 17.170 37.546
EWC 107.250 215.432 19.176 41.068 91.339 188.499 16.640 36.704
KBNet
LwF 105.551 186.658 21.038 44.658 88.545 174.485 17.333 36.729
Replay 88.359 172.586 17.765 37.798 90.553 175.286 18.522 38.949
TABLE VI: Evaluation on VOID. Starting from NYUv2 pretrained weights, models are trained on VOID (NYUv2 → VOID) and
subsequently on ScanNet (NYUv2 → VOID → ScanNet)
NYUv2→VOID NYUv2→VOID→ScanNet
Model Method MAE↓ RMSE↓ iMAE↓ iRMSE↓ MAE↓ RMSE↓ iMAE↓ iRMSE↓
Finetuned 38.963 93.236 20.730 47.662 38.825 90.594 22.433 50.817
EWC 38.771 93.812 20.367 47.073 38.763 91.593 22.709 51.732
VOICED
LwF 42.499 97.798 24.867 56.016 45.398 100.679 28.677 60.894
Replay 48.855 111.550 26.400 57.079 50.553 111.130 33.331 72.056
Finetuned 35.221 87.667 18.149 43.783 34.371 85.641 17.435 41.743
EWC 35.446 87.978 18.158 42.460 36.762 93.557 20.917 54.146
FusionNet
LwF 34.572 85.400 17.499 41.203 32.425 82.813 15.834 38.576
Replay 36.542 91.689 19.489 46.890 37.752 94.848 21.040 51.004
Finetuned 32.746 84.514 16.277 40.223 34.129 86.748 18.148 44.097
EWC 34.572 86.971 17.827 43.114 34.643 89.025 18.807 46.172
KBNet
LwF 36.321 91.587 20.385 49.752 38.944 96.792 22.951 54.977
Replay 34.544 90.054 18.392 45.751 39.409 99.833 21.509 52.007
TABLE VII: Evaluation on ScanNet. Starting from NYUv2 pretrained weights, models are trained on VOID (NYUv2 → VOID) and
subsequently on ScanNet (NYUv2 → VOID → ScanNet)
NYUv2→VOID→ScanNet
Model Method MAE↓ RMSE↓ iMAE↓ iRMSE↓
Finetuned 18.921 47.969 7.260 19.078
EWC 18.584 47.835 7.302 19.397
VOICED
LwF 20.229 47.214 7.468 18.396
Replay 25.125 55.480 9.297 20.723
Finetuned 16.889 47.318 6.170 18.574
EWC 16.681 46.848 6.185 18.627
FusionNet
LwF 16.192 44.808 5.937 17.789
Replay 17.392 47.919 6.017 17.799
Finetuned 14.070 41.372 5.169 16.390
EWC 14.710 42.201 5.422 16.767
KBNet
LwF 15.578 42.373 5.428 16.112
Replay 15.122 42.934 5.356 16.431
using RGB-D sensors, offering 1449 densely labeled pairs significant camera motion, which are crucial for testing
of aligned RGB and depth images. It is a benchmark dataset robustness in indoor depth completion. ScanNet [12] is
widely used in indoor depth estimation tasks. NYUv2 is the a large-scale indoor dataset that includes more than 2.5
primary dataset on which we pretrain our depth completion million frames with corresponding RGB-D data. It provides
models before outdoor continual learning. The VOID dataset dense depth ground truth and 3D reconstructions of indoor
[7] provides sparse depth maps and RGB frames for indoor environments.
environments, with approximately 58,000 frames. VOID
For the indoor datasets, we use the evaluation protocol in
emphasizes handling low-texture regions and scenes with
[7] and cap the depth values from 0.2 to 5 meters. DuringTABLE VIII: Evaluation on KITTI. Starting from KITTI pretrained weights, models are trained on VKITTI (KITTI → VKITTI) and
subsequently on Waymo (KITTI → VKITTI → Waymo)
KITTI→VKITTI KITTI→VKITTI→Waymo
Model Method MAE↓ RMSE↓ iMAE↓ iRMSE↓ MAE↓ RMSE↓ iMAE↓ iRMSE↓
Finetuned 345.655 1278.237 1.431 4.054 2327.250 3859.149 6.375 8.733
EWC 353.852 1295.422 1.487 4.202 2625.052 4150.911 7.281 9.161
VOICED
LwF 414.647 1337.016 1.656 4.666 1801.192 3085.398 4.909 6.807
Replay 351.029 1353.348 1.379 3.759 326.585 1344.943 1.247 3.715
Finetuned 281.757 1270.535 1.136 3.715 317.543 1281.393 1.274 3.697
EWC 285.263 1285.849 1.139 3.715 284.831 1298.388 1.117 3.664
FusionNet
LwF 277.504 1182.062 1.147 3.527 312.728 1221.368 1.403 3.945
Replay 290.409 1310.964 1.106 3.537 293.817 1332.329 1.102 3.576
Finetuned 311.888 1270.047 1.353 3.943 297.905 1252.206 1.222 3.655
EWC 286.406 1266.132 1.127 3.667 315.838 1380.112 1.275 3.760
KBNet
LwF 277.309 1166.711 1.117 3.480 304.141 1184.546 1.531 4.204
Replay 299.883 1368.686 1.082 3.473 307.247 1416.628 1.080 3.512
TABLE IX: Evaluation on VKITTI. Starting from KITTI pretrained weights, models are trained on VKITTI (KITTI → VKITTI) and
subsequently on Waymo (KITTI → VKITTI → Waymo)
KITTI→VKITTI KITTI→VKITTI→Waymo
Model Method MAE↓ RMSE↓ iMAE↓ iRMSE↓ MAE↓ RMSE↓ iMAE↓ iRMSE↓
Finetuned 966.268 4031.375 1.295 3.449 7896.476 14200.599 9.852 13.761
EWC 858.869 3881.263 1.142 3.242 5801.778 10616.265 7.568 10.538
VOICED
LwF 881.591 4055.195 1.215 3.403 5001.189 9463.238 6.963 10.024
Replay 934.656 4153.693 1.066 2.866 1050.157 4211.335 1.734 3.228
Finetuned 631.225 3334.405 0.723 3.118 790.000 3685.247 1.153 3.782
EWC 635.105 3365.572 0.666 2.917 787.348 3540.777 1.309 4.081
FusionNet
LwF 612.924 3182.058 0.699 3.009 517.690 1667.856 1.236 2.685
Replay 633.858 3348.971 0.596 2.692 645.891 3328.864 0.678 2.703
Finetuned 658.075 3380.623 1.194 3.793 804.278 3759.132 1.155 3.856
EWC 673.121 3552.306 0.872 3.373 814.097 3819.788 1.392 4.260
KBNet
LwF 741.125 3564.760 1.146 4.120 875.343 3681.855 2.778 6.438
Replay 677.493 3567.431 0.854 3.452 714.782 3718.890 0.930 3.400
TABLE X: Evaluation on Waymo. Starting from KITTI pretrained weights, models are trained on VKITTI (KITTI → VKITTI) and
subsequently on Waymo (KITTI → VKITTI → Waymo)
KITTI→VKITTI→Waymo
Model Method MAE↓ RMSE↓ iMAE↓ iRMSE↓
Finetuned 532.430 1731.000 1.216 2.656
EWC 570.878 1770.593 1.272 2.724
VOICED
LwF 565.197 1758.846 1.280 2.719
Replay 546.847 1756.500 1.199 2.586
Finetuned 535.436 1763.895 1.189 2.627
EWC 496.805 1652.860 1.179 2.637
FusionNet
LwF 633.107 3182.974 0.976 3.727
Replay 486.746 1665.689 1.172 2.645
Finetuned 485.462 1675.315 1.176 2.675
EWC 480.940 1648.952 1.166 2.638
KBNet
LwF 522.286 1736.891 1.270 2.773
Replay 497.199 1743.683 1.178 2.683
training, we use a crop shape of 416×576 for NYUv2 and outdoor continual learning. The Waymo [14] Open Dataset
a crop shape of 416×512 for VOID and Scannet. includes approximately 230,000 frames of high-resolution
imagesanddenseLiDARpointclouds,coveringawiderange
Outdoor datasets: The KITTI dataset [13] is a widely
of driving environments and conditions. Virtual KITTI [15]
used benchmark for autonomous driving, consisting of over
is a synthetic dataset designed to replicate KITTI scenes,
93,000 stereo image pairs and sparse LiDAR depth maps
providing over 21,000 frames with dense ground truth depth.
synchronizedwiththeimagesthatwerecapturedfromawider
Itallowsforevaluatingdomainadaptation,asweusemultiple
range of urban and rural scenes. KITTI is the primary dataset
data domains to simulate domain discrepancies which cause
on which we pretrain our depth completion models beforeTABLE XI: Evaluation on KITTI. Starting from KITTI pretrained weights, models are trained on Waymo (KITTI → Waymo) and
subsequently on VKITTI (KITTI → Waymo → VKITTI)
KITTI→Waymo KITTI→Waymo→VKITTI
Model Method MAE↓ RMSE↓ iMAE↓ iRMSE↓ MAE↓ RMSE↓ iMAE↓ iRMSE↓
Finetuned 1418.898 2543.668 4.608 7.031 388.247 1355.323 1.525 4.340
EWC 1215.182 2238.520 4.056 6.447 442.459 1468.315 1.735 4.799
VOICED
LwF 2144.099 3518.483 5.767 7.693 415.244 1346.477 1.704 4.616
Replay 337.384 1216.277 1.397 3.670 375.981 1216.501 1.512 3.800
Finetuned 300.405 1252.674 1.220 3.745 300.817 1298.835 1.288 4.011
EWC 315.959 1273.695 1.292 3.823 379.437 1326.112 1.517 4.052
FusionNet
LwF 296.444 1206.908 1.191 3.414 292.372 1208.645 1.214 3.661
Replay 291.101 1310.897 1.094 3.493 302.014 1369.085 1.120 3.542
Finetuned 293.960 1265.387 1.181 3.589 314.624 1273.683 1.480 4.225
EWC 320.521 1282.303 1.499 4.256 305.615 1140.672 1.299 3.635
KBNet
LwF 291.704 1131.483 1.298 3.419 333.231 1228.935 1.874 4.548
Replay 307.389 1413.860 1.082 3.478 353.075 1634.394 1.101 3.485
TABLE XII: Evaluation on Waymo. Starting from KITTI pretrained weights, models are trained on Waymo (KITTI → Waymo) and
subsequently on VKITTI (KITTI → Waymo → VKITTI)
KITTI→Waymo KITTI→Waymo→VKITTI
Model Method MAE↓ RMSE↓ iMAE↓ iRMSE↓ MAE↓ RMSE↓ iMAE↓ iRMSE↓
Finetuned 525.367 1706.168 1.223 2.672 6236.390 7681.729 14.570 16.039
EWC 525.354 1684.696 1.232 2.672 7389.857 9277.607 17.713 19.281
VOICED
LwF 565.813 1777.017 1.280 2.750 7508.272 9683.822 16.030 17.836
Replay 583.101 1731.286 1.308 2.756 642.807 1771.024 1.407 2.822
Finetuned 487.874 1653.406 1.174 2.637 518.302 1772.169 1.210 2.708
EWC 480.053 1637.659 1.166 2.627 495.613 1748.918 1.180 2.699
FusionNet
LwF 469.060 1613.890 1.154 2.622 506.399 1644.618 1.210 2.665
Replay 492.232 1685.874 1.149 2.565 508.490 1715.932 1.187 2.652
Finetuned 496.915 1701.736 1.190 2.654 608.243 2144.506 1.213 2.699
EWC 489.191 1709.312 1.180 2.696 620.688 1878.749 1.339 2.881
KBNet
LwF 493.037 1744.968 1.188 2.714 593.374 1777.834 1.387 2.913
Replay 487.310 1704.594 1.170 2.664 581.065 1983.698 1.207 2.694
TABLE XIII: Evaluation on VKITTI. Starting from KITTI pretrained weights, models are trained on Waymo (KITTI → Waymo) and
subsequently on VKITTI (KITTI → Waymo → VKITTI)
KITTI→Waymo→VKITTI
Model Method MAE↓ RMSE↓ iMAE↓ iRMSE↓
Finetuned 858.263 3986.613 1.111 3.289
EWC 909.536 4247.935 1.005 3.423
VOICED
LwF 914.993 4191.131 1.216 3.501
Replay 910.002 4161.021 1.172 3.613
Finetuned 704.775 3421.726 0.947 5.643
EWC 649.886 3419.303 0.631 2.697
FusionNet
LwF 633.432 3199.120 0.935 3.826
Replay 652.735 3407.310 0.631 2.748
Finetuned 790.162 4124.413 1.004 4.177
EWC 747.993 3706.933 0.999 3.301
KBNet
LwF 792.276 3492.584 1.416 4.583
Replay 743.578 3754.860 1.038 3.741
catastrophic forgetting. B. Results
Quantitative. Observing the quantitative results, we see
For KITTI and VKITTI, we cap the depth output during that the continual learning methods we use perform similarly
evaluation from 0.001 to 100 meters and use a depth crop well across datasets and depth completion models. According
of 240×1216. During training, we use a crop of 320×768. toTablesVandII,forindoorexperiments,EWCoutperforms
For Waymo, we cap the evaluation output from 0.001 to 80 thefinetunedbaselineby1.6%onMAEand0.71%onRMSE.
meters and use a depth crop of 768×1920, and we use a LwF outperforms the finetuned model by 4% on MAE and
training crop of 800×640. 6.3%onRMSE.Replayoutperformsthefinetunedby5.7%onTarget Depth Target Image NYUv2 VOID
Finetuned Replay EWC LwF
Fig. 1: Model Inversion Qualitative Results (Top row) We have the target image I we retrieve from the NYUv2 dataset and the target
depth map predicted by inputting the image and its corresponding depth map into the KBnet NYUv2 pretrained model. We show an
attempt at retrieving this image from the pretrained model and the VOID pretrained model for comparison. (Bottom) We also have the
image retrieval attempts for KBnet models trained from NYUv2 to VOID with all of our continual learning methods.
Metric Definition quantitative results in Table V, we observe that the replay
RM MA SE
E
(cid:0)|Ω1 |Ω1| |(cid:80) (cid:80)x x∈ ∈Ω Ω|dˆ |d( ˆx (x) )− −d dgt g( tx (x)| )|2(cid:1)1/2 m o sufe rtt fhh aeo ced v sac n (a ein t .y gr .e int tr hi te ehv e we b ac o ll le tta io nr mer thro eib gj h te otc pbt o lb euo fnu tdn bid n oa g ur ni be dos ix n( )e g. ag bn. odt xh s )e m .b oo ott to hm er
iMAE |Ω1 |(cid:80) x∈Ω|1/dˆ(x)−1/dgt(x)|
iRMSE (cid:0) |Ω1 |(cid:80) x∈Ω|1/dˆ(x)−1/dgt(x)|2(cid:1)1/2 VI. DISCUSSION
TABLE XIV: Error metrics. d gt denotes the ground-truth depth. To the best of our knowledge, this is the first study to
apply continual learning methods to the problem of depth
MAE and 7.4% on RMSE. According to Tables VIII and XI, completion. By establishing a benchmark for continual depth
for outdoor experiments, different from indoor experiments, completionacrossindoorandoutdoordatasets,thispapersets
regularization based continual learning methods harms the thefoundationforfutureworkinextendingcontinuallearning
performance, where EWC performs worse than finetuned by tomorecomplex,multi-modaldepthestimationproblems.We
3.4% on MAE and 1% on RMSE. LwF performs worse than hope that this contribution will motivate further research into
finetuned by 3.7% on MAE and outperforms finetuned by more effective strategies to combat catastrophic forgetting in
2.3% on RMSE. However, Replay outperforms the finetuned depth completion and related 3D tasks.
by 12.9% on MAE and 3.6% on RMSE. Overall, Replay
REFERENCES
based method performs better in both indoor and outdoor
scenarios, while the regularization methods do not seem to [1] R. M. French, “Catastrophic forgetting in connectionist networks,”
Trendsincognitivesciences,vol.3,no.4,pp.128–135,1999.
consistently outperform simple finetuning to a significant
[2] M.McCloskeyandN.J.Cohen,“Catastrophicinterferenceinconnec-
degree. There is one specific case when training VOICED tionistnetworks:Thesequentiallearningproblem,”inPsychologyof
model on waymo dataset where EWC and LWF performs learningandmotivation. Elsevier,1989,vol.24,pp.109–165.
[3] S.Thrun,“Islearningthen-ththinganyeasierthanlearningthefirst?”
much worse than Replay (i.e. Replay outperforms EWC by
Advancesinneuralinformationprocessingsystems,vol.8,1995.
72% and LWF by 84%). [4] R.Ratcliff,“Connectionistmodelsofrecognitionmemory:constraints
Qualitative. For our model inversion method, we currently imposedbylearningandforgettingfunctions.”Psychologicalreview,
vol.97,no.2,p.285,1990.
have qualitative results that show the difference in retrieval
[5] A.WongandS.Soatto,“Unsuperviseddepthcompletionwithcalibrated
quality between baselines and continual learning methods. backprojectionlayers,”inProceedingsoftheIEEE/CVFInternational
Figure 1 shows inversion results from KBnet models that ConferenceonComputerVision,2021,pp.12747–12756.
[6] A.Wong,S.Cicek,andS.Soatto,“Learningtopologyfromsynthetic
have been pretrained on NYUv2 and finetuned on VOID,
dataforunsuperviseddepthcompletion,”IEEERoboticsandAutoma-
where the replay method outperforms the other continual tionLetters,vol.6,no.2,pp.1495–1502,2021.
learning methods, improving the finetuned model by 11.57%. [7] A.Wong,X.Fei,S.Tsuei,andS.Soatto,“Unsuperviseddepthcom-
pletionfromvisualinertialodometry,”IEEERoboticsandAutomation
As expected, the NYUv2 pretrained model is able to retrieve
Letters,vol.5,no.2,pp.1899–1906,2020.
the 2D structure of the scene with higher fidelity than all of [8] J.Kirkpatrick,R.Pascanu,N.Rabinowitz,J.Veness,G.Desjardins,
the models which have been trained on VOID afterwards; A.A.Rusu,K.Milan,J.Quan,T.Ramalho,A.Grabska-Barwinska,
etal.,“Overcomingcatastrophicforgettinginneuralnetworks,”Pro-
whereas the VOID model is unable to retrieve the image at
ceedings of the national academy of sciences, vol. 114, no. 13, pp.
all because it has never been trained on it. In line with the 3521–3526,2017.[9] Z. Li and D. Hoiem, “Learning without forgetting,” in Proceedings [31] M.Riemer,I.Cases,R.Ajemian,M.Liu,I.Rish,Y.Tu,andG.Tesauro,
oftheIEEEconferenceoncomputervisionandpatternrecognition, “Learning to learn without forgetting by maximizing transfer and
2017,pp.5077–5086. minimizinginterference,”arXivpreprintarXiv:1810.11910,2018.
[10] D.Rolnick,A.Ahuja,J.Schwarz,T.Lillicrap,andG.Wayne,“Expe- [32] J.S.Vitter,“Randomsamplingwithareservoir,”ACMTransactions
riencereplayforcontinuallearning,”Advancesinneuralinformation onMathematicalSoftware(TOMS),vol.11,no.1,pp.37–57,1985.
processingsystems,vol.32,2019. [33] D.Lopez-PazandM.Ranzato,“Gradientepisodicmemoryforcontinual
[11] N.Silberman,D.Hoiem,P.Kohli,andR.Fergus,“Indoorsegmentation learning,”Advancesinneuralinformationprocessingsystems,vol.30,
andsupportinferencefromrgbdimages,”inEuropeanconferenceon 2017.
computervision. Springer,2012,pp.746–760. [34] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, “icarl:
[12] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and Incrementalclassifierandrepresentationlearning,”inProceedingsof
M.Nießner,“Scannet:Richly-annotated3dreconstructionsofindoor the IEEE conference on Computer Vision and Pattern Recognition,
scenes,”inProceedingsoftheIEEEconferenceoncomputervision 2017,pp.2001–2010.
andpatternrecognition,2017,pp.5828–5839. [35] H.Shin,J.K.Lee,J.Kim,andJ.Kim,“Continuallearningwithdeep
[13] A.Geiger,P.Lenz,andR.Urtasun,“Arewereadyforautonomous generativereplay,”Advancesinneuralinformationprocessingsystems,
driving?thekittivisionbenchmarksuite,”in2012IEEEConference vol.30,2017.
onComputerVisionandPatternRecognition. IEEE,2012,pp.3354– [36] M.Riemer,T.Klinger,D.Bouneffouf,andM.Franceschini,“Scalable
3361. recollections for continual lifelong learning,” in Proceedings of the
[14] P.Sun,H.Kretzschmar,X.Dotiwalla,A.Chouard,V.Patnaik,P.Tsui, AAAIconferenceonartificialintelligence,vol.33,no.01,2019,pp.
J.Guo,Y.Zhou,Y.Chai,B.Caine,etal.,“Scalabilityinperception 1352–1359.
forautonomousdriving:Waymoopendataset,”inProceedingsofthe
[37] M. Rostami, S. Kolouri, and P. K. Pilly, “Complementary learning
IEEE/CVFConferenceonComputerVisionandPatternRecognition, forovercomingcatastrophicforgettingusingexperiencereplay,”arXiv
2020,pp.2446–2454. preprintarXiv:1903.04566,2019.
[15] A.Gaidon,Q.Wang,Y.Cabon,andE.Vig,“Virtualkitti:Anannotated
[38] B.Pfülb,A.Gepperth,andB.Bagus,“Continuallearningwithfully
virtualdatasetforsceneunderstanding,”inProceedingsoftheIEEE
probabilisticmodels,”arXivpreprintarXiv:2104.09240,2021.
conference on computer vision and pattern recognition workshops,
[39] R.KemkerandC.Kanan,“Fearnet:Brain-inspiredmodelforincre-
2016,pp.28–37.
mentallearning,”arXivpreprintarXiv:1711.10563,2017.
[16] F. Zenke, B. Poole, and S. Ganguli, “Continual learning through
[40] S.Gopalakrishnan,P.R.Singh,H.Fayek,S.Ramasamy,andA.Am-
synapticintelligence,”inInternationalconferenceonmachinelearning.
bikapathi,“Knowledgecaptureandreplayforcontinuallearning,”in
PMLR,2017,pp.3987–3995.
Proceedings of the IEEE/CVF winter conference on applications of
[17] R.Aljundi,F.Babiloni,M.Elhoseiny,M.Rohrbach,andT.Tuytelaars,
computervision,2022,pp.10–18.
“Memoryawaresynapses:Learningwhat(not)toforget,”inProceedings
[41] A.AyubandA.R.Wagner,“Eec:Learningtoencodeandregenerate
of the European conference on computer vision (ECCV), 2018, pp.
imagesforcontinuallearning,”arXivpreprintarXiv:2101.04904,2021.
139–154.
[42] O. Ostapenko, M. Puscas, T. Klein, P. Jahnichen, and M. Nabi,
[18] A.Chaudhry,P.K.Dokania,T.Ajanthan,andP.H.Torr,“Riemannian
“Learning to remember: A synaptic plasticity driven framework for
walkforincrementallearning:Understandingforgettingandintransi-
continuallearning,”inProceedingsoftheIEEE/CVFconferenceon
gence,”inProceedingsoftheEuropeanconferenceoncomputervision
computervisionandpatternrecognition,2019,pp.11321–11329.
(ECCV),2018,pp.532–547.
[43] Y. X. Wu, L. Herranz, X. Liu, J. van de Weijer, B. Raducanu,
[19] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a
and T. Tuytelaars, “Memory replay gans: Learning to generate new
neuralnetwork,”arXivpreprintarXiv:1503.02531,2015.
categorieswithoutforgetting,”inProceedingsofthe32ndInternational
[20] P.Dhar,R.V.Singh,K.-C.Peng,Z.Wu,andR.Chellappa,“Learning
ConferenceonNeuralInformationProcessingSystems,2018,pp.5967–
withoutmemorizing,”inProceedingsoftheIEEE/CVFconferenceon
5977.
computervisionandpatternrecognition,2019,pp.5138–5146.
[44] K. Zhu, W. Zhai, Y. Cao, J. Luo, and Z.-J. Zha, “Self-sustaining
[21] A.Rannen,R.Aljundi,M.B.Blaschko,andT.Tuytelaars,“Encoder
based lifelong learning,” in Proceedings of the IEEE international representationexpansionfornon-exemplarclass-incrementallearning,”
conferenceoncomputervision,2017,pp.1320–1328.
inProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition,2022,pp.9296–9305.
[22] K. Lee, K. Lee, J. Shin, and H. Lee, “Overcoming catastrophic
forgetting with unlabeled data in the wild,” in Proceedings of the [45] X.Liu,C.Wu,M.Menta,L.Herranz,B.Raducanu,A.D.Bagdanov,
IEEE/CVF International Conference on Computer Vision, 2019, pp. S. Jui, and J. v. de Weijer, “Generative feature replay for class-
312–321. incremental learning,” in Proceedings of the IEEE/CVF Conference
[23] A.RiosandL.Itti,“Closed-loopmemoryganforcontinuallearning,” on Computer Vision and Pattern Recognition Workshops, 2020, pp.
arXivpreprintarXiv:1811.01146,2018. 226–227.
[24] S. Zhai, Y. Cheng, W. Zhang, and F. Lu, “Lifelong gan: Continual [46] A.Iscen,J.Zhang,S.Lazebnik,andC.Schmid,“Memory-efficient
learning for conditional image generation,” in Proceedings of the incrementallearningthroughfeatureadaptation,”inComputerVision–
IEEE/CVF International Conference on Computer Vision, 2019, pp. ECCV2020:16thEuropeanConference,Glasgow,UK,August23–28,
2759–2768. 2020,Proceedings,PartXVI16. Springer,2020,pp.699–715.
[25] A.Douillard,M.Cord,C.Ollion,T.Robert,andE.Valle,“Podnet: [47] E. Belouadah and A. Popescu, “Il2m: Class incremental learning
Pooled outputs distillation for small-tasks incremental learning,” in with dual memory,” in Proceedings of the IEEE/CVF international
Proceedings of the IEEE/CVF Conference on Computer Vision and conferenceoncomputervision,2019,pp.583–592.
PatternRecognition(CVPR),2020,pp.1951–1960. [48] A. Douillard, A. Rame, G. Couairon, and M. Cord, “Dytox: Trans-
[26] S.Hou,X.Pan,C.ChangeLoy,Z.Wang,andD.Lin,“Learninga formers for continual learning with dynamic token expansion,” in
unifiedclassifierincrementallyviarebalancing,”inProceedingsofthe ProceedingsoftheIEEEconferenceoncomputervisionandpattern
IEEE/CVFConferenceonComputerVisionandPatternRecognition recognitionworkshops,2022,pp.9285–9295.
(CVPR),2019,pp.831–839. [49] J. Wang, H. He, M. B. A. McDermott, and P. Szolovits, “Learning
[27] M.K.Titsias,J.Schwarz,A.G.d.G.Matthews,R.Pascanu,andY.W. to prompt for continual learning,” in Proceedings of the IEEE/CVF
Teh,“Functionalregularisationforcontinuallearningwithgaussian ConferenceonComputerVisionandPatternRecognition(CVPR),2022,
processes,”arXivpreprintarXiv:1901.11356,2019. pp.139–149.
[28] Y.Pan,F.Li,andW.K.Lee,“Continualdeeplearningbyfunctional [50] Z. Wang, Z. Zhan, L. Qi, J. Zhang, K. Chen, B. Liu, J. Peng,
regularisationofmemorablepast,”arXivpreprintarXiv:2007.15302, andT.Zhang,“Continuallearningwithlifelongvisiontransformer,”
2020. Proceedings of the IEEE/CVF Conference on Computer Vision and
[29] C.V.Nguyen,Y.Li,T.D.Bui,andR.E.Turner,“Variationalcontinual PatternRecognition(CVPR),pp.10893–10903,2022.
learning,”inInternationalConferenceonLearningRepresentations, [51] H.Chawla,A.Varma,E.Arani,andB.Zonooz,“Continuallearning
2018. ofunsupervisedmonoculardepthfromvideos,”inProceedingsofthe
[30] A.Chaudhry,M.Rohrbach,M.Elhoseiny,T.Ajanthan,P.K.Dokania, IEEE/CVF Winter Conference on Applications of Computer Vision,
P.H.Torr,andM.Ranzato,“Ontinyepisodicmemoriesincontinual 2024,pp.8419–8429.
learning,”arXivpreprintarXiv:1902.10486,2019. [52] N.Vödisch,K.Petek,W.Burgard,andA.Valada,“Codeps:Onlinecontinual learning for depth estimation and panoptic segmentation,” completion,”inProceedingsoftheIEEE/CVFConferenceonComputer
arXivpreprintarXiv:2303.10147,2023. VisionandPatternRecognition,2024,pp.20519–20529.
[53] H.XuandJ.Zhang,“Aanet:Adaptiveaggregationnetworkforefficient [72] M.Hu,S.Wang,B.Li,S.Ning,L.Fan,andX.Gong,“Penet:Towards
stereo matching,” in Proceedings of the IEEE/CVF Conference on preciseandefficientimageguideddepthcompletion,”in2021IEEE
ComputerVisionandPatternRecognition,2020,pp.1959–1968. InternationalConferenceonRoboticsandAutomation(ICRA). IEEE,
[54] Z.Berger,P.Agrawal,T.Y.Liu,S.Soatto,andA.Wong,“Stereoscopic 2021,pp.13656–13662.
universalperturbationsacrossdifferentarchitecturesanddatasets,”in [73] A.Li,Z.Yuan,Y.Ling,W.Chi,C.Zhang,etal.,“Amulti-scaleguided
Proceedings of the IEEE/CVF Conference on Computer Vision and cascadehourglassnetworkfordepthcompletion,”inProceedingsof
PatternRecognition,2022,pp.15180–15190. theIEEE/CVFWinterConferenceonApplicationsofComputerVision,
[55] A.Wong,M.Mundhra,andS.Soatto,“Stereopagnosia:Foolingstereo 2020,pp.32–40.
networkswithadversarialperturbations,”inProceedingsoftheAAAI [74] J.Park,K.Joo,Z.Hu,C.Liu,andI.-S.Kweon,“Non-localspatial
ConferenceonArtificialIntelligence,vol.35,2021,pp.2879–2888. propagationnetworkfordepthcompletion,”inEuropeanConference
onComputerVision(ECCV),2020,pp.120–136.
[56] F.Wang,S.Galliani,C.Vogel,P.Speciale,andM.Pollefeys,“Patch-
matchnet:Learnedmulti-viewpatchmatchstereo,”inProceedingsofthe [75] A.Eldesokey,M.Felsberg,K.Holmquist,andM.Persson,“Uncertainty-
IEEE/CVFConferenceonComputerVisionandPatternRecognition, awarecnnsfordepthcompletion:Uncertaintyfrombeginningtoend,”
inProceedingsoftheIEEE/CVFConferenceonComputerVisionand
2021,pp.14194–14203.
PatternRecognition(CVPR),2020,pp.12014–12023.
[57] C.Godard,O.M.Aodha,andG.J.Brostow,“Unsupervisedmonocular
[76] W. Van Gansbeke, D. Neven, B. De Brabandere, and L. Van Gool,
depth estimation with left-right consistency,” in Proceedings of the
“Sparseandnoisylidarcompletionwithrgbguidanceanduncertainty,”
IEEEConferenceonComputerVisionandPatternRecognition(CVPR),
in201916thInternationalConferenceonMachineVisionApplications
2017,pp.270–279.
(MVA). IEEE,2019,pp.1–6.
[58] X. Fei, A. Wong, and S. Soatto, “Geo-supervised visual depth
[77] Y.ZhangandT.Funkhouser,“Deepdepthcompletionofasinglergb-d
prediction,” IEEE Robotics and Automation Letters, vol. 4, no. 2,
image,”inProceedingsoftheIEEEConferenceonComputerVision
pp.1661–1668,2019.
andPatternRecognition,2018,pp.175–185.
[59] A.WongandS.Soatto,“Bilateralcyclicconstraintandadaptiveregular-
[78] Y. Zhang, X. Guo, M. Poggi, Z. Zhu, G. Huang, and S. Mattoccia,
izationforunsupervisedmonoculardepthprediction,”inProceedingsof
“Completionformer:Depthcompletionwithconvolutionsandvision
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,
transformers,”inProceedingsoftheIEEE/CVFConferenceonCom-
2019,pp.5644–5653.
puterVisionandPatternRecognition,2023,pp.18527–18536.
[60] R.Upadhyay,H.Zhang,Y.Ba,E.Yang,B.Gella,S.Jiang,A.Wong, [79] Z. Yu, Z. Sheng, Z. Zhou, L. Luo, S. Cao, H. Gu, H. Zhang, and
and A. Kadambi, “Enhancing diffusion models with 3d perspective H.Shen,“Aggregatingfeaturepointcloudfordepthcompletion,”in
geometryconstraints,”ACMTransactionsonGraphics(TOG),vol.42, ProceedingsoftheIEEE/CVFInternationalConferenceonComputer
no.6,pp.1–15,2023. Vision,2023,pp.8732–8743.
[61] N. Zhang, F. Nex, G. Vosselman, and N. Kerle, “Lite-mono: A [80] F. Ma, G. Cavalheiro, and S. Karaman, “Self-supervised sparse-to-
lightweightcnnandtransformerarchitectureforself-supervisedmonoc- dense: Self-supervised depth completion from lidar and monocular
ulardepthestimation,”inProceedingsoftheIEEE/CVFConferenceon camera,”in2019InternationalConferenceonRoboticsandAutomation
ComputerVisionandPatternRecognition,2023,pp.18537–18546. (ICRA). IEEE,2019,pp.3288–3295.
[62] Y. Zhang, M. Poggi, F. Tosi, X. Guo, Z. Zhu, G. Huang, and [81] S. S. Shivakumar, T. Nguyen, I. D. Miller, S. W. Chen, V. Kumar,
S.Mattoccia,“Monovit:Self-supervisedmonoculardepthestimation and C. J. Taylor, “Dfusenet: Deep fusion of rgb and sparse depth
withavisiontransformer,”inProceedingsoftheIEEE/CVFConference informationforimageguideddensedepthcompletion,”in2019IEEE
onComputerVisionandPatternRecognition,2023,pp.9708–9719. IntelligentTransportationSystemsConference(ITSC). IEEE,2019,
[63] Z.Zeng,Y.Wu,H.Park,D.Wang,F.Yang,S.Soatto,D.Lao,B.-W. pp.13–20.
Hong,andA.Wong,“Rsa:Resolvingscaleambiguitiesinmonocular [82] A.Wong,X.Fei,B.-W.Hong,andS.Soatto,“Anadaptiveframework
depthestimatorsthroughlanguagedescriptions,”Advancesinneural for learning unsupervised depth completion,” IEEE Robotics and
informationprocessingsystems,2024. AutomationLetters,vol.6,no.2,pp.3120–3127,2021.
[64] Z.Zeng,D.Wang,F.Yang,H.Park,S.Soatto,D.Lao,andA.Wong, [83] Z.Yan,K.Wang,X.Li,Z.Zhang,J.Li,andJ.Yang,“Desnet:Decom-
“Wordepth:Variationallanguagepriorformonoculardepthestimation,” posedscale-consistentnetworkforunsuperviseddepthcompletion,”in
inProceedingsoftheIEEE/CVFConferenceonComputerVisionand ProceedingsoftheAAAIConferenceonArtificialIntelligence,vol.37,
PatternRecognition,2024,pp.9708–9719. no.3,2023,pp.3109–3117.
[65] M.Jaritz,R.DeCharette,E.Wirbel,X.Perrotton,andF.Nashashibi, [84] V.Lepetit,F.Moreno-Noguer,andP.Fua,“Epnp:Anaccurateo(n)
“Sparse and dense data with cnns: Depth completion and semantic solutiontothepnpproblem,”Internationaljournalofcomputervision,
segmentation,”in2018InternationalConferenceon3DVision(3DV). vol.81,no.2,p.155,2009.
IEEE,2018,pp.52–60. [85] M. A. Fischler and R. C. Bolles, “Random sample consensus: a
[66] V.Ezhov,H.Park,Z.Zhang,R.Upadhyay,H.Zhang,C.C.Chandrappa, paradigm for model fitting with applications to image analysis and
A.Kadambi,Y.Ba,J.Dorsey,andA.Wong,“All-daydepthcompletion,” automatedcartography,”CommunicationsoftheACM,vol.24,no.6,
in2023IEEE/RSJInternationalConferenceonIntelligentRobotsand pp.381–395,1981.
Systems(IROS). IEEE,2024. [86] A.Lopez-Rodriguez,B.Busam,andK.Mikolajczyk,“Projecttoadapt:
[67] A. D. Singh, Y. Ba, A. Sarker, H. Zhang, A. Kadambi, S. Soatto, Domainadaptationfordepthcompletionfromnoisyandsparsesensor
M.Srivastava,andA.Wong,“Depthestimationfromcameraimage data,” in Proceedings of the Asian Conference on Computer Vision
and mmwave radar point cloud,” in Proceedings of the IEEE/CVF (ACCV),2020.
Conference on Computer Vision and Pattern Recognition, 2023, pp. [87] J.Jeon,H.Lim,D.U.Seo,andH.Myung,“Struct-mdc:Mesh-refined
9275–9285. unsuperviseddepthcompletionleveragingstructuralregularitiesfrom
visualslam,”inIEEERoboticsandAutomationLetters(RA-L),vol.7,
[68] Y.Yang,A.Wong,andS.Soatto,“Densedepthposterior(ddp)from
no.3,2022,pp.6391–6398.
single image and sparse range,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2019, pp.
3353–3362.
[69] T.Y.Liu,P.Agrawal,A.Chen,B.-W.Hong,andA.Wong,“Monitored
distillation for positive congruent depth completion,” in Computer
Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel,
October 23–27, 2022, Proceedings, Part II. Springer, 2022, pp.
35–53.
[70] Y. Wu, T. Y. Liu, H. Park, S. Soatto, D. Lao, and A. Wong,
“Augundo:Scalingupaugmentationsformonoculardepthcompletion
andestimation,”inEuropeanConferenceonComputerVision. Springer,
2024.
[71] H. Park, A. Gupta, and A. Wong, “Test-time adaptation for depth