Markov Potential Game with Final-Time Reach-Avoid Objectives
Sarah. H.Q. Li, Abraham P. Vinod
Abstract—We formulate a Markov potential game with systems where individual vehicles have sufficient maneu-
final-time reach-avoid objectives by integrating potential game verability and local observability, aerial and spatial vehicles
theory with stochastic reach-avoid control. Our focus is on
have more constrained observability and maneuverability.
multi-player trajectory planning where players maximize the
Therefore,weseekamathematicalmodelinwhichindividual
same multi-player reach-avoid objective: the probability of
all participants reaching their designated target states by safety is directly maximized and guaranteed.
a specified time, while avoiding collisions with one another. Contributions. We propose a reach-avoid Markov poten-
Existingapproachesrequirecentralizedcomputationofactions tial game in which players directly maximize their finite
via a global policy, which may have prohibitively expensive
horizon probability of collectively reaching their respective
communication costs. Instead, we focus on approximations of
destinationwhileavoidingotherplayers.Weshowthat1)the
the global policy via local state feedback policies. First, we
adapt the recursive single player reach-avoid value iteration proposed multi-player reach-avoid objective is multilinear in
to the multi-player framework with local policies, and show each player’s local policy space, 2) the Nash equilibrium
that the same recursion holds on the joint state space. To find condition is a relaxation of the globally optimal condition,
eachplayer’soptimallocalpolicy,themulti-playerreach-avoid
and 3) despite using local policies, the value function for
value function is projected from the joint state to the local
the multi-player reach-avoid objective must be recursively
state using the other players’ occupancy measures. Then, we
propose an iterative best response scheme for the multi-player defined over the joint state space. We extend the exist-
value iteration to converge to a pure Nash equilibrium. We ing multiplicative dynamic programming framework to an
demonstrate the utility of our approach in finding collision- iterative best response scheme that utilizes the occupancy
free policies for multi-player motion planning in simulation.
measure of opponent players to project the joint state value
function to a local state value function, and use it to find
the optimal local policy. Finally, we prove and validate in
I. INTRODUCTION
simulationthatourbestresponseschemecaneffectivelyfind
Asadvancedairmobilitysystemsbecomeareality,theex- the Nash equilibrium policy. We conclude with a simulation
pectation for air mobility’s operation scale and vehicle sizes study that evaluates the dependence of the computation
will change significantly, creating potentially unprecedented complexity on state size and the number of players.
traffic flows within more confined air spaces [1], [2]. Aerial
collisions can have catastrophic impacts [3]. As a result,
II. RELATEDLITERATURE
existing air traffic management prioritizes individual vehicle
safetyduringoperation,andoftenrelyonhumanoperatorsto Our proposed routing model’s objective differs from that
certify and minimize vehicle encounters in dense air spaces of established routing models for traffic management [5],
such as take-off and landing zones [4]. Although trained [6]. In established routing models, a time-instantaneous
human operators are irreplaceable for making time-critical safetymetricisusedtoformulateasum-separableobjective.
decisions for air space encounters, their mental limitations When this objective is minimized, the congestion levels
makes them a bottleneck in increasing the traffic throughput of individual routes are minimized with respect to the
of future mobility systems. origin-destination demands of the vehicle population [7].
To overcome these limitations posed by human operators, Alternatively, [8] solves the multi-player routing problem
we propose a safety-prioritized, multi-player routing model in air traffic management via a collection of single-player
using a Markov Decision Process (MDP) that may eventu- routing problems, which can be subsequently solved via
ally support tactical time routing in advanced air mobility existingtechniquesinreachability[9]–[11].Additionally,[9],
systems. We aim to overcome a key challenge with routing [10] also derived dynamic programming-based solutions for
game-type models for ground-based mobility systems: the maximizing the reach-avoid objective.
lack of accountability for individual vehicle’s safety proba- Subsequent works solve the reach-avoid optimization
bilityoverafinitetimehorizon.Forgroundmobilitysystems, problemwithtime-varyingandjointchanceconstraints[11],
individual vehicles rely on their on-board pilots (humans or [12]. Game-theoretical extensions of reach-avoid objectives
autonomous) to ensure individual safety during operation. have been investigated primarily under zero sum-types of
While this makes sense for ground-based transportation interaction, where two teams of players maximize/minimize
thesameobjective[13]–[16].In[17],theauthorsformulated
SarahH.Q.LiiswiththeAutomaticControlLaboratory,ETHZu¨rich, a hierarchical framework for simultaneously incorporating
Physikstrasse3,Zu¨rich,8092,Switzerland(email:huilih@ethz.ch).
high fidelity interaction and high fidelity vehicle dynamics.
AbrahamP.VinodiswiththeMitsubishiElectricResearchLaboratories,
Cambridge,MA,USA(email:abraham.p.vinod@ieee.org) In contrast, our primary focus is on a collaborative game
1
4202
tcO
32
]YS.ssee[
1v09671.0142:viXrasetting where players jointly minimize a common poten- Under policy π , we use yπi(s ,sˆ,t) to denote player i’s
i i i i
tial function via unilateral policy changes. Potential game probabilityoftransitioningto sˆ attime t+1ifplayer iwas
i
theory has been an integral part of traffic research since at s at time t. Intuitively, y (s ,sˆ,t) is the (sˆ,s ) element
i i i i i i
the formulation of the Wardrop equilibrium in [18]. Game- of the Markov transition matrix under policy π (·,t). We
i
theoreticmodelingofmobilitysystemsisexemplifiedbythe observe that each yπi(·,s (t),t) is linear in π (s (t),t),
i i i i
routing game model from [19] and its stochastic variants yπi(cid:0)
s
,sˆ,t(cid:1)
=
(cid:80)
P [sˆ|s ,a
]P(cid:2)
a |π (s
,t)(cid:3)
. (2)
from [7], [20]. However, reach-avoid objectives have not i i i i i i i i i i
ai∈Ai
been explored within a potential game framework, to the
When the context is clear, we drop the superscript π for
i
best of our knowledge.
y (2) to simplify the notation. We denote player i’s state
i
Notation: A set with N elements is denoted as [N] = trajectoryoverT+1timestepsasτ ∈ST+1.Eachtrajectory
i
{0,...,N − 1}. The set of matrices of i rows and j (cid:0) (cid:1)
τ = s (0),...,s (T) isarealizationofsequentialrandom
i i i
columns with real (non-negative) valued entries is given by
variables determined by a Markov process h (π )∈X ,
Ri×j(Ri×j). The ones column vector is denoted by 1 = i i ST+1
+ N such that the probability of trajectory τ occurring is
[1,...,1]T ∈ RN×1. The simplex in RN, {x ∈ RN|1⊤x = i
1}, is denoted by ∆ N. The set of random variables+ that take P(cid:2) τ i|h i(π i)(cid:3) =p i(cid:0) s i(0)(cid:1)(cid:16)T (cid:81)−1 y iπi(cid:0) s i(t),s i(t+1),t(cid:1)(cid:17) .
on values from sample space Ω is denoted as X Ω. t=0
(3)
III. MULTI-PLAYERREACH-AVOIDMDP
Multi-player reach-avoid objective. All players share sim-
OurproblemconcernsdesigningtrajectoriesforN players ilar objective: a) avoid other players at all time steps and
under interaction constraints within a shared state space and b) reach their respective target set T at time T. If either
i
a common finite time horizon. Each player aims to reach of these conditions are violated for any player, all players
a target set at the end of the horizon while avoiding the receive zero reward. To model this objective, we introduce
other players over the entire time horizon. Our formulation the following indicator functions,
is motivated by existing work on stochastic reach-avoid (cid:0) (cid:1)
X (s)= 1 s∈T , (4)
i i
problems [9]–[11],
(cid:40) (cid:0) (cid:1)
1 s ̸=s j ̸=i
A. Multi-player reach-avoid MDP Y ij(s i,s j)= i j ,∀i,j ∈[N]. (5)
1 j =i
We model each player as a finite horizon, finite
Foreachjointplayertrajectory{τ } withτ ∈ST+1,we
state-action MDP, where ith player’s MDP is given by i i∈[N] i
use the indicator function R(τ ,...,τ ) to indicate whether
(S,A ,T,P ,p ,T ). Each player has the same state space 1 N
i i i i
the joint trajectory achieves the multi-player reach-avoid
S and the same time horizon T ∈ N. The action set A
i
objective, defined as
is player-specific with A ∈ N elements. Each action is
i
admissible from each state. Each player i has initial state (cid:0) (cid:1) (cid:81) (cid:81)T (cid:81)
R τ ,...,τ = X (T) Y (t). (6)
s (0), which is stochastically distributed over S according 1 N i ij
i i∈[N] t=0j∈[N]
to the probability distribution p ∈ ∆ , and aims to reach
i S We define the expected value of R (6) under the joint policy
its target state set at time T, given by T ⊆S . At each time
i i (π ,...,π ) as
step t∈[T], player i’s state s (t) is a time-varying Markov 1 N
i
(cid:2) (cid:3)
process. Each player has independent transition dynamics. F(π ,...,π )=E R(τ ,...,τ )|τ ∼h (π ),∀i∈[N] .
1 N 1 N i i i
(7)
Assumption 1 (INDEPENDENT MARKOV TRANSITIONS).
Each player uses their policy π (1) to maximize F (7) with
For every player i ∈ [N] and time step t ∈ [0,T), its i
respect to the other players’ policies π , such that player
next state s (t + 1) is a Markov random variable that −i
i i’s reach-avoid MDP is given by
depends on its current state s (t) and action a (t), denoted
i i
by P i(cid:0) s i(t),a i(t)(cid:1) ∈X
S
such that πm i∈a Πx
i
F(π i,π −i), ∀i∈[N]. (8)
s (t+1)∼P (cid:0) s (t),a (t)(cid:1) , ∀s (t)∈S,t∈[T],a (t)∈A . Player i’s individual reach-avoid problem (8) can become
i i i i i i i
trivially defined depending on the opponent policy π .
(cid:0) (cid:1) −i
We (cid:2)denote (cid:3)the probability distribution of P i s i(t),a i(t) Under π −i, there must be trajectories τ i ∈ ST+1 such that
as P i ·|s i,a i ∈∆ S, for all s i ∈S and a i ∈A i. R(τ 1,...,τ N)=1andτ i ∼h(π i)isrealizablewithpositive
Player i selects each action a i(t) via a state-dependent probability. Otherwise, F(π i,π −i)=0 for all π
i
∈Π i.
and time-varying policy π i. We consider player policies that The objective F (7) is the same for every player’s reach-
depends strictly on each player’s local state. avoid MDP. The result is that each player equally prioritizes
the reachability of their own targets and the reachability of
Definition1(LOCALFEEDBACK). Foreveryplayeri∈[N],
the other players’ targets. If a joint trajectory achieves target
a policy π (·,t) is a function of player i’s state s (t),
i i
reachability and opponent avoidance for player i, but causes
(cid:0) (cid:1)
π :S×[T](cid:55)→X , a (t)∼π s (t),t , ∀t∈[T],s ∈S. anotherplayerj tofailtargetreachabilityorcreatescollision
i Ai i i i i
(1) for players j,k ̸= i, then player i does not prefer such a
We use Π to denote the set of all policies π satisfying (1). trajectory when solving (8).
i i
2B. Markov potential game and Nash equilibrium policies Given that each player’s objective F are identical, F =
i i
Weseekajointpolicy(π ,...,π )thatisjointlyoptimal F (7) is the obvious choice of the potential function.
1 N
for the N-coupled MDPs posed in (8). AsaMarkovpotentialgame,(8)hasasetofNashequilib-
ria that possess well-behaved computational and theoretical
Definition 2 (NASH EQUILIBRIUM). The joint policy
properties, some of which we list below.
(π 1⋆,...,π N⋆ ) is a Nash equilibrium if and only if for all Solution structure.AMarkovgamehasatleastonepure
i∈[N], Nash equilibrium (π⋆,...,π⋆ ) where each π⋆ is determin-
1 N i
F(π⋆,π⋆ )≥F(π ,π⋆ ), ∀π ∈Π . (9) istic: at every state, a unique action is always chosen. [21]
i −i i −i i i
Connections to single-player dynamic programming.
PlayersreachaNashequilibriumwhennoonecanfurther
The reach-avoid MDP in both single and multi-player set-
improve the multi-player reach-avoid objective in (8) via
tings is a non-convex optimization problem on which first-
unilateral policy changes. The Nash equilibrium has the
order gradient methods do not have good guarantees. In the
followinginterpretation:intheeventthatallplayersexcepti
single player setting, existing work show that multiplicative
fix their policies to π⋆ , player i’s policy π⋆ maximizes the
−i i dynamic programming provably converges to the optimal
likelihood of player i’s trajectory τ reaching target T while
i i policy [9], [10] and one may use convex optimization for
avoiding other players over the entire time horizon T.
grid-free computation when the dynamics are linear [11].
Connections to policies with global feedback. A natural
Via potential game theory, we can extend the single-player
alternative to (8) is to directly optimize the multi-player
multiplicative dynamic programming to multi-player policy
reach-avoid objective over the policies that have global state
update schemes by leveraging existing work on multi-player
feedback. For all players i ∈ [N], consider the policy
learningdynamics.Inthenextsection,weshowthatiterative
πˆ :SN ×[T](cid:55)→X that takes global state feedback as
i Ai best response is one such learning dynamic that will find an
(cid:0) (cid:1)
a (t)∼πˆ s (t),...,s (t),t , ∀t∈[T], (10) equilibrium in the joint policy space under NE assumptions.
i i 1 N
Additionally, other gradient-based methods such as Frank-
and the global optimization problem given by
Wolfe [22] and gradient play [23] can also be used in con-
(cid:2) (cid:3)
max E R(τ 1,...,τ N)|τ j ∼h j(πˆ j), ∀j ∈[N] . (11) junction with Algorithm 2 to compute the Nash equilibrium.
πˆ1,...,πˆN
Problem (11) may be viewed as a multi-player extension of IV. MULTI-PLAYERMULTIPLICATIVEDYNAMIC
the single-player reach-avoid problem from [9]–[11] under PROGRAMMING
Assumption1.Itdiffersfrom(8)intwoaspects:first,global In this section, we evaluate the multi-player reach-avoid
policy solution space vs local policy solution space, and objective (7) using the conditional transition distributions
second, global optimality vs unilateral optimality. Since the {y } (2) and show that the objective can be recursively
i i∈N
global policy space (10) subsumes the local policy space (1) computed via a value function on the joint state space. We
and unilateral optimality is a necessary condition for global project the value function using each player’s occupancy
optimality, the joint policy that maximizes (11) will achieve measures to construct an iterative best response scheme that
a better multi-player reach-avoid objective than the Nash extends the single-player multiplicative dynamic program-
equilibrium policy. ming solution.
However, the joint policy (πˆ ,...,πˆ ) has impractical
1 N
A. Multilinear program formulation
memory and operation requirements: each πˆ ’s memory
i
requirementgrowsexponentiallywithadditionalplayers,and We show that the objective (7) is multilinear in each
ateverytimestep,playersmustsharetheirstatesinanall-to- player’spolicyπ byshowingthattheobjectiveismultilinear
i
allcommunicationnetworktogatherthenecessaryinputsfor in the conditional transition distributions {y } (2).
i i∈N
each policy. To avoid these impractical requirements while
Lemma 1. Any real-valued function G : SNK (cid:55)→ R that
leveragingthesingleplayermultiplicativedynamicprogram-
takes in a joint trajectory {τ } , where τ ∈ SK and
ming solution, we relax the global optimality conditions i i∈N i
is a Markov process as described by h(π ) (3), then the
from (11) using the Nash equilibrium conditions (9) and i
expectation of G with respect to {π } is multilinear in
the policy memory requirements via Assumption 1. As seen i i∈N
{y } (2), i.e.,
in Definition 2, a Nash equilibrium implies that the joint i i∈N
policy is coordinate-wise optimal: the joint policy is optimal (cid:2) (cid:3)
E G(τ ,...,τ )|τ ∼h (π ),∀j ∈[N] =
1 N j j j
if collaborations between players are ignored. From this
T
perspective, the Nash equilibrium conditions are necessary (cid:88) (cid:89) (cid:89) (cid:0) (cid:1) (cid:0) (cid:1)
G(τ ,...,τ ) p s (0) y s (t),s (t+1),t ,
1 N j j j j j
towards the optimality of (11), and Nash equilibrium is an τ1,...,τN t=0j∈[N]
approximation and a lower bound to (11).
∈SNK
(13)
Connections to Markov potential games. The game
defined in (8) is a potential game [21]—i.e., there exists an where τ =(s (0),...,s (K−1)) for all i∈[N].
i i i
ordinal potential function F : Π × ... × Π (cid:55)→ R that
1 N Proof. Let τ ,...,τ represent a joint trajectory where at
satisfies ∀π ,πˆ ∈Π , ∀i∈[N], 1 N
i i i each time step t, player i is at state s (t) for all i ∈ [N],
i
F (π ,π )>F (πˆ ,π )⇔F(π ,π )>F(πˆ ,π ). (12) t ∈ [K −1], and let Γ denote the set of all realizable joint
i i −i i i −i i −i i −i
3trajectories, then E(cid:2) G(τ ,τ )|τ ∼ h (π ),∀j ∈ [N](cid:3) can Wenotethateachs,sˆ∈SN from(15)correspondstoajoint
i −i j j j
be evaluated as state of all players: s=(s ,...,s ) and sˆ=(sˆ ,...,sˆ ).
1 N 1 N
(cid:88)(cid:89) (cid:2) (cid:3)
F i(π i,π −i)= P τ i G i(τ i,τ −i), (14) Proposition 1. Under the joint policy π = (π 1,...,π N),
τ∈Γ i Vπ,...,Vπ as defined in (15) are the expected value of the
0 T
where P(cid:2) τ (cid:3) denotes the joint probability of player i being random variable
i
at state s (t) for all time steps t = 0,...,K −1. We can
i T
directly evaluate P(cid:2) τ i(cid:3) as R tT(τ 1,...,τ N)=(cid:89) X i(cid:0) s i(T)(cid:1)(cid:89)(cid:89) Y ij(cid:0) s i(tˆ),s j(tˆ)(cid:1) .
K−1 i tˆ=t i,j
(cid:2) (cid:3) (cid:2) (cid:3) (cid:89) (cid:2) (cid:0) (cid:1)(cid:3) (16)
P τ =P s (0) P s (t+1)|s (t),a (t)∼π s (t),t .
i i i i i i i with respect to π—i.e., Vπ(s ,...,s ) (15) is equivalent to
t=0 t 1 N
yw (h ser (e t),sP(cid:2) (s ti( +t 1+ ),t)1)| as si(t d) e, fia ni( et d) in∼
(2)
π ani(cid:0) ds i P(t (cid:2)) s,t ((cid:1) 0(cid:3) )(cid:3) =
=
V tπ(s 1,...,s N)=Eπ(cid:104) R tT(τ 1,...,τ N)|
i i i (17)
(cid:105)
y(0,τ i,π 0) is the initial state distribution. τ
i
∼h i(π i),τ i(0)=s i,∀i∈[N] .
IfweapplyLemma1to(7),weobservethat(8)isinfact
The proof is provided in App. A. Proposition 1 specifies
a multilinear optimization problem over a compact policy
the more general results from [9], [10] to the finite state-
domain. Its global optimal solution can therefore be difficult
action MDP under decoupled player dynamics in Assump-
tocomputeandcertify.Interestingly,whenN =1,thesingle
tion 1.
player reach-avoid MDP (11), despite being multi-linear in
π still, has a globally optimal solution that multiplicative
i
C. Computing player i’s best response
dynamic programming is guaranteed to find [9]–[11].
Since the multi-player reach-avoid objective F (7) is an A key difference between the single player reach-avoid
multilinearfunctionofindividualplayerpoliciesπ 1,...,π N, MDP and the multi-player reach-avoid MDP is the state
findingevenalocallyoptimalsolutionto(8)viaoptimization availability for policy feedback. In the single-player value
algorithms can be challenging; most gradient-based algo- function, the global state is available, while in the multi-
rithms tend to converge to KKT solutions that are not suffi- player reach-avoid MDP, only the local state is available
cient for guaranteeing optimality in the nonconvex setting. for each player. We first observe that when all players
Instead, we leverage game-theoretic learning dynamics to except i take on policies π (t),...,π (T −1), the multi-
−i −i
synthesize distributed algorithms for finding Nash equilib- player reach-avoid value Vπi,π−i(s) as function of π is
t i
rium policies. By relaxing the global collaborative reach- given by (15) with y explicitly evaluated via the other
−i
avoidproblem(11)asapotentialgame,weuncoveriterative players policies. In addition to this, the expected multi-
best response as a possible multi-player learning scheme playerreach-avoidvalueatstates dependsontheoccupancy
i
for finding joint policy equilibria. As a potential game, we measuresofallotherplayersattimey,whichdependsonthe
knowthatiterativebestresponseoverthedeterministicpolicy policiesπ (0),...,π (t−1).Collectively,thisimpliesthat
−i −i
domainconvergestotheNashequilibriumpolicies.However, the expected multi-player reach-avoid value can be directly
it remains to be seen how each player can compute the best computed as
response to the opponent policies.
B. Multi-player value function E(cid:104) Vπi,π−i(s ,s )|π (cid:105) =(cid:88) ρ (s ,t)(cid:89) Y (cid:0) s ,s (cid:1)
t i −i −i −i −i ij i j
A single player reach-avoid MDP with deterministic ob- s−i j,ℓ
(cid:88) (cid:2) (cid:3)(cid:88)(cid:89)
stacles can be solved via multiplicative dynamic program- P sˆ|s (t),π y (s ,sˆ ,t)V (sˆ,sˆ ), (18)
i i i j j j t+1 i −i
ming [10]. When adapting the single player reach-avoid
sˆi sˆ−i j̸=i
MDP to the multi-player reach-avoid MDP, two critical
(cid:81)
gaps to address are 1) how does having having multiple where ρ −i(s −i,t)= j̸=iρ j(s j,t) correspond to the occu-
players with individual states and policies change the value pancy measures of players [N]/{i} and can be found via
functions’ structure, and 2) how does having stochastic the forward propagation of policies π −i(0),...,π −i(t−1)
obstacles that correspond to players change multiplicative throughplayeri’sMarkovdynamics(Algorithm1).Together,
dynamic programming? ρ j(s j,t)y j(s j,sˆ j,t) denote the joint probability that player
Similar to the single-player value function definition i was in state s j at time t and state sˆ j at time t+1.
from [9], [10], we formulate the multi-player value function Obstacles with Markov dynamics. In (18), we observe
(cid:80) (cid:81)
belowandshowthatitrecursivelycomputesthemulti-player that
sˆ−i
j̸=iy j(s j,sˆ j,t)V t+1(sˆ i,sˆ −i) is the expected
reach-avoid objective. future reward for player i if it makes the transition from
V Tπ(cid:0) s 1,...,s N(cid:1) =(cid:81) X j(s j)(cid:81) Y ij(s i,s j), es xi pt eo ctsˆ ei da ft utt uim ree rt e. wF au rdrth (cid:80)ermor ye, c (o snsi ,d sˆer ( ,1 t)8 V) wit (h sˆo ,u sˆt th )e
.
j i,j sˆ−i −i −i −i t+1 i −i
Vπ(s ,...,s )=(cid:81) Y (s ,s ) (cid:80) (cid:81) y (s ,sˆ ,t)Vπ (sˆ). It becomes
t 1 N ij i j j j j t+1 (cid:80) (cid:81)
i,j sˆ∈SN j ρ −i(s −i,t) Y jℓ(t). (19)
(15)
s−i j,ℓ
4Algorithm 1 Retrieving density trajectory from a policy Algorithm 2 Individual player best response
Require: Pi, p i, π i. Require: π −i,P −i,p −i,T −i.
Ensure: {ρ:S×[T](cid:55)→[0,1]} Ensure: π⋆
i
ρ(s,t)=0, ∀ t∈[T],s∈S 1: for j ∈[N]/{i} do
ρ(s,0)=p i(s) ∀s∈S 2: ρ j =Alg. 1(P j,p j,π j)
for t=0,...,T −1 do 3: end for
for s∈S do 4: for s∈SN do
ρ(s,t+1)= (cid:80) (cid:80) P t,ss′aρ(t,s′)π i(s′,a,t) 5: V T(s)=(cid:81) iX i(s i)(cid:81) j̸=iY(s i,s j)
a∈As′∈S 6: end for
end for
7: for t=T −1,...,0 do
end for
8: for s −i,sˆ −i ∈SN (cid:89)−1 do (cid:16) (cid:17)
9: ρ(s −i,sˆ −i)= P j sˆ j|s j,π j(s j,t) ρ(s −i,t)
j̸=i
Sinceeachplayers’statetrajectoryisdeterminedbyanMDP,
10: end for
the obstacles’ states are random variables rather than deter-
ministiclocationsateachtimestep.Sinceρ −i(s −i,t)arethe 1 11 2:
:
for πs ii (∈
s
i;S t)d =o argmax(cid:89)
Y jℓ(s j,s ℓ)
probability density distributions of all players [N]/{i}, and ai
j,ℓ
player i’s state s at time t is given as an input to V in (18), (cid:88) (cid:88)
i 13: P i(sˆ i|s i,a i) ρ(s −i,sˆ −i)V t+1(sˆ)
the expression in (19) is equivalent to the likelihood of all
playersavoidingeachotherattimesteptunderpoliciesπ −i 14:
fosˆi
r sˆ i ∈S do
s−i,sˆ−i
and conditioned on player i being at state s i, i.e., 15: ρ i(s i,sˆ i)=P i(cid:0) sˆ i|s i,π i(s i,t)(cid:1)
(cid:2) (cid:3) 16: end for
P s (t)̸=s (t), ∀j,ℓ∈[N]|s (t)=s ,τ ∼h (π ),∀j ̸=i .
j ℓ i i j j j
17: end for
(20)
18: for s∈SN do
I en achpa prt li ac yu el rar j, hif ash dj eta er re md ine it se tr im ci sn tais tt ei sc spr (o tc )e =sse ss, ,s thu ec nh (t 2h 0a )t 19: V t(s)=(cid:89) Y jℓ(s j,s ℓ)(cid:88) ρ i(s i,sˆ i)ρ(s −i,sˆ −i)V t+1(sˆ)
j j
recovers the indicator function (cid:81) 1(cid:0) s (t) ̸= s (t)(cid:1) . We j,ℓ sˆ
j,ℓ j ℓ 20: end for
can conclude that (19) is a probabilistic relaxation of the
21: end for
indicator function when players j’s states are given by
deterministic states:
(cid:89) (cid:0) (cid:1) (cid:2)(cid:89) (cid:3)
1 s j(t)̸=s ℓ(t) →P s j(t)̸=s ℓ(t)|s i(t)=s
i
. denotebyW t,remainsanunderapproximationtothemulti-
j,ℓ j,ℓ player reach-avoid objective, since a global policy may be
able to coordinate the players for better performance at the
MultiplicativeBestResponse.Proposition1and(18)en-
cost of additional communication and memory overhead.
ableustodirectlyadaptmultiplicativedynamicprogramming
Policy memory complexity. The output policy of Algo-
from [9], [10] to perform a best response scheme for reach-
rithm 2 requires ST memory units for storage and does
avoid Markov potential games (8). The resulting algorithm
not scale with increasing number of players. Achieving this
is shown in Algorithm 3.
complexity is a key motivation for using local feedback
We formulate the best response value function W⋆ ∈
policies and the game-theoretical framework for evaluating
RS(T+1) as
the multi-player reach-avoid objective.
W⋆(s )=(cid:80) ρ (s ,T)Vπi,π−i(s ,s ), ∀s ∈SN
T i −i −i T i −i i Computationcomplexity.Algorithmtakes1)SNT steps
W⋆(s )=s− mi ax (cid:80) yπi(cid:0) s ,sˆ,t(cid:1)(cid:81) Y(s ,s ) to perform forward propagation, 2) (T +1)S2N to compute
(cid:80)t i
(cid:81)
yπi (∈ sXA ,i sˆsˆ )i
ρ
(i
s
,ti )Vi
⋆
(sj ˆ,ℓ
,sˆ
)j
,
∀ℓ
s ∈SN,
t Tw So N-tim toe cs ote mp po uc tecu tp ha enc py rem vie oa us sur te imρ e(s s− tei, psˆ − vai) lu, e3) fuA nS ct2 io, n4 s)
.
j j j j j t+1 i −i i Exploring parallel computing extensions as well as other
s−i,sˆ−ij̸=i
(21) computational speed ups of Algorithm 2 will be a topic of
and π⋆(s ,t) as an argmax policy that achieves W⋆(s ) for future work.
i i t i
allt,s ∈[T]×S.Thenπ⋆ isplayeri’sbestresponsepolicy One factor that controls Algorithm 2’s computation com-
againsti opponent policy πi , and E(cid:2) W⋆(s )|s ∼ p (cid:3) is the plexity is the occupancy measure at each state. For MDPs
−i 0 i i i
maximum expected multi-player reach-avoid objective (7) with sparse transitions—i.e., most of the player’s occupancy
that player i can achieve against opponent policy π . measures transition predominantly to a small subset of
−i
Unlike standard dynamic programming approaches to states— may be faster to evaluate than the worst-case com-
compute the optimal global policy [9]–[11], player i’s value putation complexity. Furthermore, we can eliminate states
functionisnotrecursivebyitself—i.e.,W isnotrecursively thathaveminimaloccupancymeasureandthereforeminimal
t
defined by W . Instead, we “average” out the effect of contributiontothereach-avoidobjective,astotradeoffcom-
t+1
other players’ state on the global value function using their putation efficiency for accuracy. The impact of occupancy
occupancy measure. The resulting value function, which we measure is also time-dependent—the later on in the MDP
5Figure State size Horizon Players Stochasticity Trial
time horizon, the more smaller occupancy measures matter.
(MRMC) (T) (N) (p) size(K)
Therefore, we propose using the following heuristic to ap- 2 40 15 3 [0.75,0.95] 50
proximate the two-time step occupancy measure ρ(s ,sˆ ) 3 [30,70] 20 2 0.95 50
−i −i
in Alg. 2 line 9 to reduce the computation complexity. TABLE I: Simulation hyper-parameters.

0 ∃j ̸=i,ρ(s −i)≤ϵ
ρ(s ,sˆ )= (cid:81)
−i −i P (sˆ |s )ρ(s ,t) otherwise
 j j j −i
j̸=i
(22)
From Algorithm 2, we can derive the following multi-
playerupdateschemethatconvergestotheNashequilibrium
inpolynomialtime.WenotethatinadditiontoAlgorithm3,
other gradient-based methods such as Frank-Wolfe [22] and
gradient play [23] can also be used in conjunction with
Algorithm 2 to compute the Nash equilibrium.
Algorithm 3 Iterative Best Response
Require: T ,O , Pi.
i ik
Ensure: π⋆,...,π⋆
1 N
1: while k =1,... do
2: i=k mod N
3: V i,π i =Alg. 2(π −k− i1,P −i,p −i,T −i)
4: π ik =π i;π −k
i
=π −k− i1
5: if Vi =Vj, ∀i,j ∈[N] then
6: π⋆ =πk, ∀i∈[N]
i i
7: Exit
8: end if Fig.1:Reach-avoidmetricsoverdifferentactionstochasticityvalues
9: end while (green to black and corresponds to p=0.95 to p=0.75).
Theorem 1. Algorithm 3 converges to a pure-strategy Nash optimality and computation efficiency in two test scenarios
equilibrium in polynomial time [21]. via K Monte carlo trials, the hyper-parameters of each test
scenario is given in Table I and the results are shown in
WhetherAlgorithm3convergestoanoptimaljointpolicy
Figures 1 and 2.
that maximizes the multi-player reach-avoid objective (11)
over the joint policy space Π ×...Π is unknown. How- Reach-avoid performance. In Figure 1, we visualize
1 N
three metrics over each best response iteration K in sub-
ever, it does converge to a coordinate-wise optimal solution
plots from top to bottom: 1) potential value: the reach-
that under-approximates the best multi-player reach-avoid
avoid probability (7), 2) collision likelihood: the collision
objective achieved by joint policies with global state feed-
back in (11).
probability among any two players at any time t∈[T],
V. MULTI-PLAYERMOTIONPLANNING E(cid:2) 1−(cid:81)T t=0(cid:81) i,j∈[N]Y ij(cid:0) s i(t),s j(t)(cid:1) |τ j ∼h j(π jk),∀j ∈[N](cid:3) ,
(23)
WeevaluateAlgorithm3’sefficacyatfindingcollision-free
and3)reachreduction:theprobabilityofallplayersreaching
trajectories in a multi-agent motion planning problem on a
their destination at time T, divided by the probability that
grid-world MDP. The grid world has dimensions M ×M
R C each player reaches their destination on their shortest path,
andisexecutedforT numberofstepsforN players.Players
receive randomized initial and final assigned target squares E(cid:2)(cid:81) X (cid:0) s (T)(cid:1) |τ ∼h (πk),∀j ∈[N](cid:3)
j∈[N] j j j j j
on the far left and far right columns of the grid world, (cid:81) E(cid:2) X (cid:0) s (T)(cid:1) |τ ∼h (π⋆),∀j ∈[N](cid:3), (24)
respectively, and attempt to reach their randomly assigned j∈[N] j j j j j
target squares while avoiding each other. To ensure that whereπ⋆ denotesplayerj’sshortestpathpolicy.Weobserve
j
the players must deal with collisions, the player assigned thatonaverage,allthreemetricsstabilizetotheirasymptotic
the top-left initial state is also assigned the bottom-right values between 5 and 10 best response steps. Furthermore,
destination. Each player’s action is to go up, down, left, because each player always initiates the iterative best re-
or right subjected to world boundaries. Each action has an sponsewiththeirshortestpathpolicyπ⋆,theinitialreduction
j
associated stochasticity p: instead of reaching the action’s in reaching player targets is always none. However, these
targetdestinationdeterministically,thetargetisreachedwith policies also incur collision likelihoods averaging around
probability 1 − p ∈ [0,1] and a neighbor at random is 50%. As players maneuver around each other to reduce
reached with probability p. We evaluate Algorithm 3 output this collision likelihood, the reach reduction first decreases
6[2] R. Goyal, C. Reiche, C. Fernando, and A. Cohen, “Advanced air
101 mobility:Demandanalysisandmarketpotentialoftheairportshuttle
andairtaximarkets,”Sustainability,vol.13,no.13,p.7421,2021.
[3] R.E.WeibelandR.J.Hansman,“Safetyconsiderationsforoperation
of unmanned aerial vehicles in the national airspace system,” Tech.
Metric Rep.,2006.
BRIteration [4] J. P. McGee, A. S. Mavor, and C. D. Wickens, Flight to the future:
100 ComputationTime Humanfactorsinairtrafficcontrol. NationalAcademiesPress,1997.
[5] R. Shone, K. Glazebrook, and K. G. Zografos, “Applications of
stochastic modeling in air traffic management: Methods, challenges
and opportunities for solving air traffic problems under uncertainty,”
30 40 50 60 70
Euro.J.Oper.Res.,pp.1–26,2021.
Statesize [6] D.Hentzen,M.Kamgarpour,M.Soler,andD.Gonza´lez-Arribas,“On
maximizingsafetyinstochasticaircrafttrajectoryplanningwithuncer-
Fig. 2: Computation time (seconds) and best response iteration (k) tain thunderstorm development,” Aerospace Science and Technology,
vol.79,pp.543–553,2018.
vs state sizes.
[7] D.CalderoneandS.Sastry,“Markovdecisionprocessroutinggames,”
inInt’lConf.Cyber-Phys.Syst.,2017,pp.273–279.
[8] A. P. Vinod, S. Yamazaki, A. Chakrabarty, N. Yoshikawa, and
S.DiCairano,“Aircraftapproachmanagementusingreachabilityand
but then gradually increases, while the collision likelihood
dynamicprogramming,”in2024AmericanControlConference(ACC).
decreasesasymptotically.Wenotethathigheractionstochas- IEEE,2024,pp.318–324.
ticitypleadstomoreunavoidablecollisions.Thisisreflected [9] A. Abate, M. Prandini, J. Lygeros, and S. Sastry, “Probabilistic
by the asymptotic trends observed in Figure 1. reachability and safety for controlled discrete time stochastic hybrid
systems,”Automatica,2008.
Computation Efficiency. Next, we evaluate the compu- [10] S. Summers and J. Lygeros, “Verification of discrete time stochastic
tation efficiency as a function of the number of players and hybridsystems:Astochasticreach-avoiddecisionproblem,”Automat-
ica,vol.46,no.12,pp.1951–1961,2010.
state size. We visualize two metrics: 1) the computation
[11] A. Vinod and M. Oishi, “Stochastic reachability of a target tube:
time for a best response iteration and 2) the number of
Theoryandcomputation,”Automatica,2021.
best response iterations before the change in potential value [12] N. Schmid, M. Fochesato, S. H. Li, T. Sutter, and J. Lygeros,
decreases below 1e−5. The results for different state sizes “Computingoptimaljointchanceconstrainedcontrolpolicies,”arXiv
preprintarXiv:2312.10495,2023.
is shown in Figure 2. We observe that the computation time
[13] M.Chen,Z.Zhou,andC.J.Tomlin,“Multiplayerreach-avoidgames
increases approximately linearly over the increasing state via pairwise outcomes,” IEEE Transactions on Automatic Control,
space, and this observation is true whether or not we use vol.62,no.3,pp.1451–1457,2016.
[14] J.F.FisacandS.S.Sastry,“Thepursuit-evasion-defensedifferential
the approximation (22) as described in Section IV to reduce
game in dynamic constrained environments,” in 2015 54th IEEE
numberofstatedensitiestracked.Weuseϵ=(1e−2)0.75t+3 conferenceondecisionandcontrol(CDC). IEEE,2015,pp.4549–
where t is the MDP time step. Additionally, the number of 4556.
[15] M. Chen, Q. Hu, C. Mackin, J. F. Fisac, and C. J. Tomlin, “Safe
bestresponseiterationsalsoincreasesapproximatelylinearly
platooningofunmannedaerialvehiclesviareachability,”in201554th
over increasing state size. IEEE conference on decision and control (CDC). IEEE, 2015, pp.
The average computation time for [2,3,4] players over 4695–4701.
[16] K.MargellosandJ.Lygeros,“Hamilton–jacobiformulationforreach–
10 monte carlo simulations is [0.72,55,1660] seconds. We
avoid differential games,” IEEE Transactions on automatic control,
observe that the computation does scale exponentially as the vol.56,no.8,pp.1849–1861,2011.
number of players increases. With 4 players taking up to ∼ [17] J. F. Fisac, E. Bronstein,E. Stefansson, D. Sadigh, S. S. Sastry, and
A.D.Dragan,“Hierarchicalgame-theoreticplanningforautonomous
25 minutes for one best response iteration. This exponential
vehicles,”in2019Internationalconferenceonroboticsandautomation
increaseinplayercomplexitycanbemitigatedviadistributed (ICRA). IEEE,2019,pp.9590–9596.
computinganddistributedlearningdynamics,whichwewill [18] M. J. Smith, “The existence, uniqueness and stability of traffic
equilibria,”Transp.Res.PartB:Method.,pp.295–304,1979.
explore in future research. We did not observe significant
[19] T.Roughgarden,Selfishroutingandthepriceofanarchy. MITpress,
changes in the number of best response iterations between 2005.
different number of players. [20] S. H. Li, D. Calderone, and B. Ac¸ıkmes¸e, “Congestion-aware path
coordination game with markov decision process dynamics,” IEEE
ControlSystemsLetters,vol.7,pp.431–436,2022.
VI. CONCLUSION
[21] D. Monderer and L. S. Shapley, “Potential games,” Games and
economicbehavior,1996.
Towards a traffic management framework for heteroge-
[22] S. H. Li, Y. Yu, N. I. Miguel, D. Calderone, L. J. Ratliff, and
neousvehicles,weformulatedagame-theoreticextensionof B. Ac¸ıkmes¸e, “Adaptive constraint satisfaction for markov decision
a single agent reach-avoid MDP, provided a game-theoretic process congestion games: Application to transportation networks,”
Automatica,vol.151,p.110879,2023.
interpretation of the optimal decision for resource-sharing
[23] J. R. Marden, “State based potential games,” Automatica, vol. 48,
players, and simulation verified a multi-player extension no.12,pp.3075–3088,2012.
of multiplicative dynamic programming for finding Nash
equilibrium policies. APPENDIX
A. Proof of Proposition 1
REFERENCES
Proof. For each s ∈ SN, we prove the following recursive
[1] L. A. Garrow, B. German, N. T. Schwab, M. D. Patterson, N. Men- identity for (15): if Vπ (s) satisfies (17), then Vπ(s) satis-
donca,Y.O.Gawdiak,andJ.R.Murphy,“Aproposedtaxonomyfor t+1 t
advancedairmobility,”inAIAAAviation2022Forum,2022,p.3321. fies (17).
7If Vπ (s) satisfies (17), it is equivalent to
t+1
T
Vπ (s)=(cid:88) RT (cid:0) (s,τ)(cid:1)(cid:89) (cid:89) P(cid:2) τ (tˆ+1)|τ (tˆ),π (cid:3) ,
t+1 t+1 j j j
τ j tˆ=t+2
(25)
for all s ∈ SN, where the product (cid:81)T P(cid:2) τ (tˆ +
1)|τ (tˆ),π (cid:3) is the probability of realizing ttˆ h= et trajej ctory
j j
τ (t+1),...,τ (T) when τ (t+1) = s for all j ∈ [N].
j j j j
We use (25) to define Vπ (sˆ) and (15) to evaluate Vπ(s)
t+1 t
as
Vπ(s)=(cid:89)
Y(s ,s )
(cid:88) (cid:89) P(cid:2)
sˆ |s ,π
(cid:3)
t j ℓ j j j
j,ℓ sˆ1,...,sˆN j
T
(cid:88) RT (cid:0) (sˆ,τ )(cid:1) (cid:89) (cid:89) P(cid:2) τ (tˆ+1)|τ (tˆ),π (cid:3) (26)
t+1 t+2 j j j
τt+2 tˆ=t+2 j
(cid:80) (cid:80)
We can combine the summations and to
(cid:80) (cid:80) (cid:80)
sˆ τt+2
by noting that is equivalent to a
τt+1 sˆ τt+2
single summation over (sˆ,τ ) ∈ SN(T−t), which
t+2
we define as τ . Under this definition of τ ,
t+1 t+1
(cid:81) P(cid:2) sˆ |s ,π (cid:3)(cid:81) (cid:81)T (cid:81) P(cid:2) τ (tˆ + 1)|τ (tˆ),π (cid:3) =
j j j j j tˆ=t+2 j j j j
(cid:81)T (cid:81) P(cid:2) τ (tˆ+1)|τ (tˆ),π (cid:3) .
tˆ=t+1 j j j j
For the trajectory (s,τ ), the reach-avoid objective
t+1
RT (cid:0) (s,τ )(cid:1) also satisfies the recursive relationship
t+1 t+1
RT(cid:0) (s,τ )(cid:1) =(cid:89) Y(s ,s )RT (τ ).
t t+1 j ℓ t+1 t+1
j,ℓ
Therefore, we can conclude that for all joint states s∈SN.
T
Vπ(s)= (cid:88) RT(cid:0) (s,τ )(cid:1) (cid:89) (cid:89) P(cid:2) τ (tˆ+1)|τ (tˆ),π (cid:3) .
t t t+1 j j j
τt+1 tˆ=t+1 j
(27)
Finally, since Vπ satisfies the expectation evaluation (17),
T
Vπ ,...,Vπ all satisfies (17).
T−1 0
8