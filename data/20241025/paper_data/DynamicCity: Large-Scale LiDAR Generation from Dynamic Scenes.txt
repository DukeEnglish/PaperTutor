Preprint.
LiDAR SCENE GENERATION
DYNAMICCITY: LARGE-SCALE LIDAR GEN-
ERATION FROM DYNAMIC SCENES
HengweiBian1,2,∗ LingdongKong1,3 HaozheXie4 LiangPan1,†,‡ YuQiao1 ZiweiLiu4
1ShanghaiAILaboratory 2CarnegieMellonUniversity 3NationalUniversityofSingapore
4S-Lab,NanyangTechnologicalUniversity
...
T=1 T=2 T=3 T=4 T=5 T=N
Command-Driven DynamicObject
SceneGeneration Generation
1 2 1 2
Forward TurnLeft
3 4 3 4
TurnLeft TurnLeft
5 6 5 6
TurnLeft Forward
Trajectory-GuidedGeneration DynamicSceneInpainting Layout-ConditionedGeneration
Before After
Figure1: DynamicLiDARscenegenerationfromDynamicCity. WeintroduceanewLiDAR
generationmodelthatgeneratesdiverse4Dscenesoflargespatialscales(80×80×6.4meter3)and
longsequentialmodeling(upto128frames),enablingadiversesetofdownstreamapplications. For
moreexamples,kindlyrefertoourProjectPage: https://dynamic-city.github.io.
ABSTRACT
LiDARscenegenerationhasbeendevelopingrapidlyrecently. However,existing
methodsprimarilyfocusongeneratingstaticandsingle-framescenes,overlooking
theinherentlydynamicnatureofreal-worlddrivingenvironments. Inthiswork,
weintroduceDynamicCity,anovel4DLiDARgenerationframeworkcapableof
generatinglarge-scale,high-qualityLiDARscenesthatcapturethetemporalevolu-
tionofdynamicenvironments. DynamicCitymainlyconsistsoftwokeymodels.
1)AVAEmodelforlearningHexPlaneasthecompact4Drepresentation. Instead
ofusingnaiveaveragingoperations, DynamicCityemploysanovelProjection
Moduletoeffectivelycompress4DLiDARfeaturesintosix2Dfeaturemapsfor
HexPlaneconstruction,whichsignificantlyenhancesHexPlanefittingquality(upto
12.56mIoUgain). Furthermore,weutilizeanExpansion&SqueezeStrategyto
reconstruct3Dfeaturevolumesinparallel,whichimprovesbothnetworktraining
efficiencyandreconstructionaccuracythannaivelyqueryingeach3Dpoint(upto
7.05mIoUgain,2.06xtrainingspeedup,and70.84%memoryreduction). 2)A
DiT-baseddiffusionmodelforHexPlanegeneration. TomakeHexPlanefeasible
forDiTgeneration,aPaddedRolloutOperationisproposedtoreorganizeallsix
featureplanesoftheHexPlaneasasquared2Dfeaturemap. Inparticular,various
conditionscouldbeintroducedinthediffusionorsamplingprocess,supporting
∗WorkdoneduringaninternshipatShanghaiAILaboratory. †Correspondingauthor. ‡Projectlead.
1
4202
tcO
32
]VC.sc[
1v48081.0142:viXraPreprint.
versatile4Dgenerationapplications,suchastrajectory-andcommand-driven
generation,inpainting,andlayout-conditionedgeneration. Extensiveexperiments
ontheCarlaSCandWaymodatasetsdemonstratethatDynamicCitysignificantly
outperformsexistingstate-of-the-art4DLiDARgenerationmethodsacrossmultiple
metrics. Thecodewillbereleasedtofacilitatefutureresearch.
1 INTRODUCTION
LiDARscenegenerationhasgarneredgrowingattentionrecently,whichcouldbenefitvariousrelated
applications, such as robotics and autonomous driving. Compared to its 3D object generation
counterpart, generatingLiDARscenesremainsanunder-exploredfield, withmanynewresearch
challengessuchasthepresenceofnumerousmovingobjects,large-scalescenes,andlongtemporal
sequences (Huang et al., 2021; Xu et al., 2024). For example, in autonomous driving scenarios,
a LiDAR scene typically comprises multiple objects from various categories, such as vehicles,
pedestrians,andvegetation,capturedoveralongsequence(e.g.,200frames)spanningalargearea
(e.g.,80×80×6.4meters3).Althoughinitsearlystage,LiDARscenegenerationholdsgreatpotential
toenhancetheunderstandingofthe3Dworld,withwide-reachingandprofoundimplications.
DuetothecomplexityofLiDARdata,manyefficientlearningframeworkshavebeenintroducedfor
large-scale3Dscenegeneration. X3(Renetal.,2024b)utilizesahierarchicalvoxeldiffusionmodel
togeneratelargeoutdoor3DscenesbasedonVDBdatastructure. PDD(Liuetal.,2023a)introduces
apyramiddiscretediffusionmodeltoprogressivelygeneratehigh-quality3Dscenes. SemCity(Lee
etal.,2024)resolvesoutdoorscenegenerationbyleveragingatriplanediffusionmodel. Despite
achievingimpressiveLiDARscenegeneration,theseapproachesprimarilyfocusongeneratingstatic
andsingle-frame3Dsceneswithsemantics,andhencefailtoeffectivelycapturethedynamicnature
ofoutdoorenvironments. Recently,afewworks(Zhengetal.,2024;Wangetal.,2024)haveexplored
4DLiDARgeneration. However,generatinghigh-qualitylong-sequence4DLiDARscenesisstilla
challengingandopenproblem(Nakashima&Kurazume,2021;Nakashimaetal.,2023).
Inthiswork,weproposeanovel4DLiDARgenerationframework,DynamicCity,enablinggenerat-
inglarge-scale,high-qualitydynamicLiDARscenes. DynamicCitymainlyconsistsoftwostages:
1)aVAEnetworkforlearningcompact4Drepresentations,i.e.,HexPlanes(Cao&Johnson,2023;
Fridovich-Keiletal.,2023);2)aHexPlaneGenerationmodelbasedonDiT(Peebles&Xie,2023).
VAEfor4DLiDAR.Givenasetof4DLiDARscenes,DynamicCityfirstencodesthesceneasa3D
featurevolumesequencewitha3Dbackbone. Afterward,weproposeanovelProjectionModule
basedontransformeroperationstocompressthefeaturevolumesequenceintosix2Dfeaturemaps.
Inparticular,theproposedprojectionmodulesignificantlyenhancesHexPlanefittingperformance,
offeringanimprovementofupto12.56%mIoUcomparedtoconventionalaveragingoperations.
AfterconstructingtheHexPlanebasedontheprojectedsixfeatureplanes,weemployanExpansion
&SqueezeStrategy(ESS)todecodetheHexPlaneintomultiple3Dfeaturevolumesinparallel.
Comparedtoindividuallyqueryingeachpoint,ESSfurtherimprovesHexPlanefittingquality(with
upto7.05%mIoUgain),significantlyacceleratestrainingspeed(byupto2.06x),andsubstantially
reducesmemoryusage(byuptoarelative70.84%memoryreduction).
DiTforHexPlane. UsingtheencodedHexPlane,weuseaDiT-basedframeworkforgenerating
HexPlane,enabling4DLiDARgeneration. TrainingaDiTwithtokensequencesnaivelygenerated
fromHexPlanemaynotachieveoptimalquality,asitcouldoverlookspatialandtemporalrelationships
amongtokens. Therefore,weintroducethePaddedRolloutOperation(PRO),whichreorganizes
thesixfeatureplanesintoasquarefeaturemap,providinganefficientwaytomodelbothspatial
andtemporalrelationshipswithinthetokensequence. LeveragingtheDiTframework,DynamicCity
seamlessly incorporates various conditions to guide the 4D generation process, enabling a wide
range of applications including hexplane-conditional generation, trajectory-guided generation,
command-drivenscenegeneration,layout-conditionedgeneration,anddynamicsceneinpainting.
Ourcontributionscanbesummarizedasfollows:
• WeproposeDynamicCity,ahigh-quality,large-scale4DLiDARscenegenerationframe-
workconsistingofatailoredVAEforHexPlanefittingandaDiT-basednetworkforHexPlane
generation,whichsupportsvariousdownstreamapplications.
2Preprint.
• In the VAE architecture, DynamicCity employs a novel Projection Module to benefit in
encoding4DLiDARscenesintocompactHexPlanes,significantlyimprovingHexPlane
fittingquality. Following,anExpansion&SqueezeStrategyisintroducedtodecodethe
HexPlanesforreconstruction,whichimprovesbothfittingefficiencyandaccuracy.
• BuildingonfittedHexPlanes,wedesignaPaddedRolloutOperationtoreorganizeHexPlane
featuresintoamasked2Dsquarefeaturemap,enablingcompatibilitywithDiTtraining.
• ExtensiveexperimentalresultsdemonstratethatDynamicCityachievessignificantlybetter
4D reconstruction and generation performance than previous SoTA methods across all
evaluationmetrics,includinggenerationquality,trainingspeed,andmemoryusage.
2 RELATED WORK
3DObjectGenerationhasbeenacentralfocusinmachinelearning,withdiffusionmodelsplayinga
significantroleingeneratingrealistic3Dstructures.Manytechniquesutilize2Ddiffusionmechanisms
tosynthesize3Doutputs,coveringtasksliketext-to-3Dobjectgeneration(Maetal.,2024),image-to-
3Dtransformations(Wuetal.,2024a),and3Dediting(Rojasetal.,2024).Meanwhile,recentmethods
bypasstherelianceon2Dintermediariesbygenerating3Doutputsdirectlyinthree-dimensional
space,utilizingexplicit(Alliegroetal.,2023),implicit(Liuetal.,2023b),triplane(Wuetal.,2024b),
and latent representations (Ren et al., 2024b). Although these methods demonstrate impressive
3Dobjectgeneration,theyprimarilyfocusonsmall-scale,isolatedobjectsratherthanlarge-scale,
scene-levelgeneration(Hongetal.,2024;Leeetal.,2024). Thislimitationunderscorestheneedfor
methodscapableofgeneratingcomplete3Dsceneswithcomplexspatialrelationships.
LiDARSceneGenerationextendsthescopetolarger,morecomplexenvironments. Earlierworks
usedVQ-VAE(Zyrianovetal.,2022)andGAN-basedmodels(Cacciaetal.,2019;Nakashimaetal.,
2023) to generate LiDAR scenes. However, recent advancements have shifted towards diffusion
models(Xiongetal.,2023;Ranetal.,2024;Nakashima&Kurazume,2024;Zyrianovetal.,2022;
Huetal.,2024;Nunesetal.,2024),whichbetterhandlethecomplexitiesofexpansiveoutdoorscenes.
For example, (Lee et al., 2024) utilize voxel grids to represent large-scale scenes but often face
challengeswithemptyspaceslikeskiesandfields. Whilesomerecentworksincorporatetemporal
dynamicstoextendsingle-framegenerationtosequences(Zhengetal.,2024;Wangetal.,2024),
they often lack the ability to fully capture the dynamic nature of 4D environments. Thus, these
methodstypicallyremainlimitedtoshorttemporalhorizonsorstrugglewithrealisticdynamicobject
modeling,highlightingthegapingeneratinghigh-fidelity4DLiDARscenes.
4D Generation represents a leap forward, aiming to capture the temporal evolution of scenes.
Priorworksoftenleveragevideodiffusionmodels(Singeretal.,2022;Blattmannetal.,2023)to
generate dynamic sequences (Singer et al., 2023), with some extending to multi-view (Shi et al.,
2023)andsingle-imagesettings(Rombachetal.,2022)toenhance3Dconsistency. Inthecontext
of video-conditional generation, approaches such as (Jiang et al., 2023; Ren et al., 2023; 2024a)
incorporate image priors for guiding generation processes. While these methods capture certain
dynamic aspects, they lack the ability to generate long-term, high-resolution 4D LiDAR scenes
with versatile applications. Our method, DynamicCity, fills this gap by introducing a novel 4D
generationframeworkthatefficientlycaptureslarge-scaledynamicenvironments,supportsdiverse
generationtasks(e.g.,trajectory-guided(Bahmanietal.,2024),command-drivengeneration),and
offerssubstantialimprovementsinscenefidelityandtemporalmodeling.
3 PRELIMINARIES
HexPlane (Cao & Johnson, 2023; Fridovich-Keil et al., 2023) is an explicit and structured rep-
resentation designed for efficient modeling of dynamic 3D scenes, leveraging feature planes to
encode spacetime data. A dynamic 3D scene is represented as six 2D feature planes, each
aligned with one of the major planes in the 4D spacetime grid. These planes are represented
asH=[P ,P ,P ,P ,P ,P ],comprisingaSpatialTriPlane(Chanetal.,2022)withP ,
xy xz yz tx ty tz xy
P ,andP ,andaSpatial-TimeTriPlanewithP ,P ,andP . ToquerytheHexPlaneatapoint
xz yz tx ty tz
p=(t,x,y,z),featuresareextractedfromthecorrespondingcoordinatesoneachofthesixplanes
andfusedintoacomprehensiverepresentation. Thisfusedfeaturevectoristhenpassedthrougha
lightweightnetworktopredictsceneattributesforp.
3Preprint.
4DScene𝐐 HexPlane
c(p)
e
ca PE(p)
p
S
3D
Backbone Projection Head
… … e m Hadamard
iT Product
e-
ca Class
p
S Probabilities
(a) Learning HexPlane asan Efficient 4DSceneRepresentation
Diffusion
…
… Decoder Decoder …
DiT … DiT
Conditions
4DScene𝐐 Denoising 4DScene𝐐
t 0
(b) HexPlaneDiffusionwithDiTfor4DSceneGeneration
Figure2: PipelineofdynamicLiDARscenegeneration. OurDynamicCityframeworkconsists
oftwokeyprocedures: (a)EncodingHexPlanewithanVAEarchitecture(cf.Sec.4.1),and(b)4D
SceneGenerationwithHexPlaneDiT(cf.Sec.4.2).
DiffusionTransformers(DiT)(Peebles&Xie,2023)arediffusion-basedgenerativemodelsusing
transformerstograduallyconvertGaussiannoiseintodatasamplesthroughdenoisingsteps. The
forward diffusion adds Gaussian noise over time, with a noised sample at step t given by x =
√ √ t
α x + 1−α ϵ, ϵ ∼ N(0,I),whereα controlsthenoiseschedule. Thereversediffusion,
t 0 t t
u √sing a neural network ϵ θ, aims to denoise x
t
to recover x 0, expressed as: x
t−1
= √1 αt(x
t
−
1−α ϵ (x ,t)). Newsamplesaregeneratedbyrepeatingthisreverseprocess.
t θ t
4 OUR APPROACH
DynamicCitystrivestogeneratedynamic3DLiDARsceneswithsemanticinformation,whichmainly
consistsofaVAEfor4DLiDARencodingusingHexPlane(Cao&Johnson,2023;Fridovich-Keil
etal.,2023)(Sec.4.1),andaDiTforHexPlanegeneration(Sec.4.2). Givena4DLiDARscene,
i.e., adynamic3DLiDARsequenceQ ∈ RT×X×Y×Z×C, whereT, X, Y, Z, andC denotethe
sequencelength,height,width,depth,andchannelsize,respectively,theVAEfirstaimstoencode
anefficient4Drepresentation,HexPlaneH=[P ,P ,P ,P ,P ,P ],whichisthendecoded
xy xz yz tx ty tz
forreconstructing4Dsceneswithsemantics. AfterobtainingHexPlaneembeddings,DynamicCity
leveragesaDiT-basedframeworkfor4DLiDARgeneration. Diverseconditionscouldbeintroduced
intothegenerationprocess,facilitatingarangeofdownstreamapplications(Sec.4.3). Theoverview
oftheproposedDynamicCitypipelineisillustratedinFig.2.
4.1 VAEFOR4DLIDARSCENES
EncodingHexPlane. AsshowninFig.3,theVAEcouldencodea4DLiDARsceneQasaHexPlane
H. It first utilizes a shared 3D convolutional feature extractor f (·) to extract and downsample
θ
featuresfromeachLiDARframe,resultinginafeaturevolumesequenceX ∈RT×X×Y×Z×C.
txyz
To encode and compress X into compact 2D feature maps of H, we propose a novel Pro-
txyz
jection Module with multiple projection networks h(·). To project a high-dimensional feature
input X
in
∈ RD k1×D k2×···×D kn×D r1×D r2×···×D rm×C as a lower-dimensional feature output X
out
∈
R
X
SD
′
kk1 S× rD ∈k2× R· S·· k× ×D Sk rn ×× CC b, yth ge rop uro pj ie nc gti to hn en de imtw eo nr sk ioh nsSr i( n· t) ofi thrs et tr wes oh na ep wes dX imin enin sit oo na s,3 i- .d e.i ,m Se kn ts hio en da il mf ee na stu ior ne
thatwillbekept,andS thedimensionthatwillbereduced,whereS =D1×D2×···×Dn,and
r k k k k
S =D1×D2×···×Dm. Afterward,h (·)utilizesatransformer-basedoperationtoprojectthe
r r r r Sr
4Preprint.
EncodingHexPlane Space
𝓧 𝒉 𝒫
𝑥𝑦𝑧 𝒛 𝑥𝑦
XYZ
𝓧
𝑡𝑥𝑦𝑧 𝒉 𝒉 𝒫
𝒕 𝒚 𝑥𝒛
XYZ Feature 𝒉 𝒫
𝒙 𝑦𝑧
… 𝒇 𝜽 T Volume Space-Time
𝒉 𝒫
𝒚𝒛 𝑡𝑥
…
𝒉 𝒫
4D Scene𝐐 𝒙𝒛 𝑡𝑦
Reconstruction Feature Volume ProjectionModule 𝒉 𝒙𝒚 𝒫 𝒕𝒛 HexPlane ℋ
Loss Sequence
𝓧′
𝑡𝑥𝑦𝑧
H Expansion
Pa
… 𝒈 𝝓 T
…
tcudor dramad
T
Reconstructed ′ Decoding
XYZ Expand൛𝒫 ,𝒫 ,𝒫 ,𝒫 ,𝒫 ,𝒫 ൟ
4D Scene𝐐′ 𝑥𝑦 𝑥𝑧 𝑦𝑧 𝑡𝑥 𝑡𝑦 𝑡𝑧 𝑡𝑥𝑦𝑧 HexPlane
Figure3: VAEforEncoding4DLiDARScenes. WeuseHexPlaneHasthe4Drepresentation. f
θ
andg areconvolution-basednetworkswithdownsamplingandupsamplingoperations,respectively.
ϕ
h(·)denotestheprojectionnetworkbasedontransformermodules.
reshapedfeatureX′ toX′′ ∈RSk×C,whichisthenreshapedtotheexpectedlower-dimensional
SkSr Sk
featureoutputX . Formally,theprojectionnetworkisformulatedas:
out
{D1×D2×···×Dn}×C {D1×D2×···×Dn}×{D1×D2×···×Dm}×C
X k k k =h (X k k k r r r ), (1)
out Sr in
wheretheirfeaturedimensionsareaddedastheupscriptforXinandXout,respectively.
ToconstructthespatialfeatureplanesP ,P ,andP ,theProjectionModulefirstgeneratethe
xy xz yz
XYZ Feature Volume X = h (X ). Rather than directly access the heavy feature volume
xyz t txyz
sequence X , h (·), h (·), and h (·) are applied to X for reducing the spatial dimensions
txyz z y x xyz
of X along the z-axis, y-axis, and x-axis, respectively. The temporal feature planes P ,P ,
xyz tx ty
andP aredirectlyobtainedfromX bysimultaneouslyremovingtwospatialdimensionswith
tz txyz
h (·),h (·),andh (·),respectively. Consequently,wecouldconstructtheHexPlaneHbasedon
zy xz xy
theencodedsixfeatureplanes,includingP ,P ,P ,P ,P ,andP .
xy xz yz tx ty tz
DecodingHexPlane. BasedontheHexPlaneH = [P ,P ,P ,P ,P ,P ],weemployan
xy xz yz tx ty tz
Expansion&SqueezeStrategy(ESS),whichcouldefficientlyrecoverthefeaturevolumesequenceby
decodingthefeatureplanesinparallelfor4DLiDARscenereconstruction. ESSfirstduplicatesand
expandseachfeatureplaneP tomatchtheshapeofX ,resultinginthelistofsixfeaturevolume
txyz
sequences: {XPxy,XPxz,XPyz,XPtx ,XPty ,XPtz }. Afterward,ESSsqueezesthecorresponding
txyz txyz txyz txyz txyz txyz
sixexpandedfeaturevolumeswithHadamardProduct:
X′ = (cid:89) {XPxy,XPxz,XPyz,XPtx ,XPty ,XPtz }. (2)
txyz txyz txyz txyz txyz txyz txyz
Hadamard
Subsequently,theconvolutionalnetworkg (·)isemployedtoupsamplethevolumesforgenerating
ϕ
densesemanticpredictionsQ′:
Q′ =g (Concat(X′ ,PE(Pos(X′ )))), (3)
ϕ txyz txyz
whereConcat(·)andPE(·)denotetheconcatenationandsinusoidalpositionalencoding,respec-
tively. Pos(·)returnsthe4Dpositionpofeachvoxelwithinthe4DfeaturevolumeX′ .
txyz
Optimization. TheVAEistrainedwithacombinedlossL , includingacross-entropyloss, a
VAE
Lovász-softmaxloss(Bermanetal.,2018),andaKullback-Leibler(KL)divergenceloss:
L =L (Q,Q′)+αL (Q,Q′)+βL (H,N(0,I)), (4)
VAE CE Lov KL
5Preprint.
Space Space-Time
Numeric Conditions Image Conditions
Trajectory Command Timestamp HexPlane Layout
MLP Embedding MLP Patchify Patchify
(a)HexPlane
+
+
XY TY YZ
Conditioning Cross-Attention Conditioning
XZ TZ
TX
(b)Padded Rollout
Figure4: PaddedRollout Figure5: ConditionInjectionforDiT
where L is the cross-entropy loss between the input Q and prediction Q′, L is the Lovász-
CE Lov
softmaxloss,andL representstheKLdivergencebetweenthelatentrepresentationHandthe
KL
priorGaussiandistributionN(0,I). NotethattheKLdivergenceiscomputedforeachfeatureplane
ofHindividually,andthetermL referstothecombineddivergenceoverallsixplanes.
KL
4.2 DIFFUSIONTRANSFORMERFORHEXPLANE
After training the VAE, 4D semantic scenes can be embedded as HexPlane H =
[P ,P ,P ,P ,P ,P ]. BuildinguponH,weaimtoleverageaDiT(Peebles&Xie,2023)
xy xz yz tx ty tz
model D to generate novel HexPlane, which could be further decoded as novel 4D scenes (see
τ
Fig.2(b)). However,trainingaDiTusingtokensequencesnaivelygeneratedfromeachfeatureplane
ofHexPlanecouldnotguaranteehighgenerationquality,mainlyduetotheabsenceofmodeling
spatialandtemporalrelationsamongthetokens.
PaddedRolloutOperation. GiventhatthefeatureplanesofHexPlanemaysharespatialortemporal
dimensions,weemploythePaddedRolloutOperation(PRO)tosystematicallyarrangeallsixplanes
intoaunifiedsquarefeaturemap,incorporatingzeropaddingsintheuncoveredcornerareas. As
showninFig.4,thedimensionofthe2Dsquarefeaturemapis(X + Z + T ),whichminimizes
dX dZ dT
theareaforpadding,whered ,d ,andd representthedownsamplingratesalongtheX,Z,andT
X Z T
axes,respectively. Subsequently,wefollowDiTtofirst“patchify”theconstructed2Dfeaturemap,
convertingitintoasequenceofN =((X + Z + T )/p)2tokens,wherepisthepatchsize,chosen
dX dZ dT
so each token holds information from one feature plane. Following patchification, we apply the
frequency-basedpositionalembeddingstoalltokenssimilartoDiT.Notethattokenscorresponding
topaddingareasareexcludedfromthediffusionprocess. Consequently,theproposedPROoffersan
efficientmethodformodelingspatialandtemporalrelationshipswithinthetokensequence.
Conditional Generation. DiT enables conditional generation through the use of Classifier-Free
Guidance (CFG) (Ho & Salimans, 2022). To incorporate conditions into the generation process,
we designed two branches for condition insertion (see Fig. 5). For any condition c, we use the
adaLN-ZerotechniquefromDiT,generatingscaleandshiftparametersfromcandinjectingthem
beforeandaftertheattentionandfeed-forwardlayers. Tohandlethecomplexityofimage-based
conditions,weaddacross-attentionblocktobetterintegratetheimageconditionintotheDiTblock.
4.3 DOWNSTREAMAPPLICATIONS
Beyondunconditional4Dscenegeneration,weexplorenovelapplicationsofDynamicCitythrough
conditionalgenerationandHexPlanemanipulation.
6
Input
Tokens
LayerNorm Scale,
Shift
Self-Attention Multi-Head
Scale
+
Layer
Norm
Cross-Attention Multi-Head
+
Layer
Norm
Scale,
Shift
Feedforward Pointwise
Scale
+Preprint.
Table1: Comparisonsof4DSceneReconstruction. WereportthemIoUscoresofOccSora(Wang
etal.,2024)andourDynamicCityframeworkontheCarlaSC,Occ3D-Waymo,andOcc3D-nuScenes
datasets, respectively, under different resolutions and sequence lengths. Symbol † denotes score
reportedintheOccSorapaper. Otherscoresarereproducedusingtheofficialcode.
OccSora Ours
Dataset #Classes Resolution #Frames
(Wangetal.,2024) (DynamicCity)
10 128×128×8 4 41.01% 79.61%(+38.6%)
CarlaSC 10 128×128×8 8 39.91% 76.18%(+36.3%)
(Wilsonetal.,2022) 10 128×128×8 16 33.40% 74.22%(+40.8%)
10 128×128×8 32 28.91% 59.31%(+30.4%)
Occ3D-Waymo
9 200×200×16 16 36.38% 68.18%(+31.8%)
(Tianetal.,2023)
11 200×200×16 16 13.70% 56.93%(+43.2%)
Occ3D-nuScenes 11 200×200×16 32 13.51% 42.60%(+29.1%)
(Tianetal.,2023) 17 200×200×16 32 13.41% 40.79%(+27.3%)
17 200×200×16 32 27.40%† 40.79%(+13.4%)
Table 2: Comparisons of 4D Scene Generation. We report the Inception Score (IS), Fréchet
InceptionDistance(FID),KernelInceptionDistance(KID),andthePrecision(P)andRecall(R)
ratesofSemCity(Leeetal.,2024),OccSora(Wangetal.,2024),andourDynamicCityframework
ontheCarlaSCandOcc3D-Waymodatasets,respectively,inboththe2Dand3Dspaces.
Metric2D Metric3D
Dataset Method #Frames
IS2D↑ FID2D↓ KID2D↓ P2D↑ R2D↑ IS3D↑ FID3D↓ KID3D↓ P3D↑ R3D↑
CarlaSC OccSora 2.492 25.08 0.013 0.115 0.008 2.257 1559 52.72 0.380 0.151
16
(Wilsonetal.,2022) Ours 2.498 10.95 0.002 0.238 0.066 2.331 354.2 19.10 0.460 0.170
Occ3D-Waymo OccSora 1.926 82.43 0.094 0.227 0.014 3.129 3140 12.20 0.384 0.001
16
(Tianetal.,2023) Ours 1.945 7.138 0.003 0.617 0.096 3.206 1806 77.71 0.494 0.026
First, we showcase versatile uses of image conditions in the conditional generation pipeline: 1)
HexPlane: ByautoregressivelygeneratingtheHexPlane,weextendscenedurationbeyondtemporal
constraints. 2)Layout: Wecontrolvehicleplacementanddynamicsin4Dscenesusingconditions
learnedfrombird’s-eyeviewsketches.
Tomanageegovehiclemotion,weintroducetwonumericalconditioningmethods: 3)Command:
Controlsgeneralegovehiclemotionviainstructions. 4)Trajectory: Enablesfine-grainedcontrol
throughspecifictrajectoryinputs.
InspiredbySemCity(Leeetal.,2024), wealsomanipulatetheHexPlaneduringsamplingto: 5)
Inpaint: Edit4DscenesbymaskingHexPlaneregionsandguidingsamplingwiththemaskedareas.
Formoredetails,kindlyrefertoSec.A.5intheAppendix.
5 EXPERIMENTS
5.1 EXPERIMENTALDETAILS
Datasets. Wetraintheproposedmodelonthe1Occ3D-Waymo,2Occ3D-nuScenes,and3CarlaSC
datasets. TheformertwofromOcc3D(Tianetal.,2023)arederivedfromWaymo(Sunetal.,2020)
andnuScenes(Caesaretal.,2020),whereLiDARpointcloudshavebeencompletedandvoxelizedto
formoccupancydata. Eachoccupancyscenehasaresolutionof200×200×16,coveringaregion
centered on the ego vehicle, extending 40 meters in all directions and 6.4 meters vertically. The
CarlaSCdataset(Wilsonetal.,2022)isasyntheticoccupancydataset,withasceneresolutionof
128×128×8,coveringaregion25.6metersaroundtheegovehicle,withaheightof3meters.
ImplementationDetails. OurexperimentsareconductedusingeightNVIDIAA100-80GGPUs.
TheglobalbatchsizeusedfortrainingtheVAEis8,whiletheglobalbatchsizefortrainingtheDiT
is128. OurlatentHexPlaneHiscompressedtohalfthesizeoftheinputQineachdimension,with
thelatentchannelsC =16. TheweightfortheLovász-softmaxandKLtermsaresetto1and0.005,
respectively. ThelearningratefortheVAEis10−3,whilethelearningratefortheDiTis10−4.
7Preprint.
Occ3D-Waymo CarlaSC
T=1 T=1 T=1 T=1 T=1
T=8 T=8 T=8 T=8 T=8
T=16 T=16 T=16 T=16 T=16
Build Vehicle Ped Road Sidewalk Barrier Ground Veg Other Pole
Figure6: DynamicSceneGenerationResults. Weprovideunconditionalgenerationscenesfrom
the1st,8th,and16thframesonOcc3D-Waymo(Left)andCarlaSC(Right),respectively. Kindly
refertotheAppendixforcompletesequentialscenesandlongertemporalmodelingexamples.
Evaluation Metrics. The mean intersection over union (mIoU) metric is used to evaluate the
reconstruction results of VAE. For DiT, Inception Score, FID, KID, Precision, and Recall are
calculatedforevaluation. Specifically,wefollowpriorwork(Leeetal.,2024;Wangetal.,2024)by
rendering3Dscenesinto2Dimagesandutilizingconventional2Devaluationpipelinesforassessment.
Additionally,wetrainthe3DEncodertodirectlyextractfeaturesfromthe3Ddataandcalculatethe
metrics. Formoredetails,kindlyrefertoSec.A.2intheAppendix.
5.2 4DSCENERECONSTRUCTION&GENERATION
Reconstruction. To evaluate the effectiveness of the proposed VAE in encoding the 4D LiDAR
sequence,wecompareitwithOccSora(Wangetal.,2024)usingtheCarlaSC,Occ3D-Waymo,and
Occ3D-nuScenesdatasets. AsshowninTab.1,DynamicCityoutperformsOccSoraonthesedatasets,
achievingmIoUimprovementsof38.6%,31.8%,and43.2%respectively,whentheinputnumberof
framesis16. TheseresultshighlightthesuperiorperformanceoftheproposedVAE.
Generation. TodemonstratetheeffectivenessofDynamicCityin4Dscenegeneration,wecompare
thegenerationresultswithOccSora(Wangetal.,2024)ontheOcc3D-WaymoandCarlaSCdatasets.
AsshowninTab.2,theproposedmethodoutperformsOccSoraintermsofperceptualmetricsinboth
2Dand3Dspaces. Theseresultsshowthatourmodelexcelsinbothgenerationqualityanddiversity.
Fig.6andFig.15showthe4Dscenegenerationresults,demonstratingthatourmodeliscapable
ofgeneratinglargedynamicscenesinbothreal-worldandsyntheticdatasets. Ourmodelnotonly
exhibitstheabilitytogeneratemovingsceneswithstaticsemanticsshiftingasawhole,butitisalso
capableofgeneratingdynamicelementssuchasvehiclesandpedestrians.
Applications. Fig. 7 presents the results of our downstream applications. In tasks that involve
insertingconditionsintotheDiT,suchascommand-conditionalgeneration,trajectory-conditional
generation,andlayout-conditionalgeneration,ourmodeldemonstratestheabilitytogeneratereason-
ablescenesanddynamicelementswhilefollowingtheprompttoacertainextent. Additionally,the
inpaintingmethodprovesthatourHexPlanehasexplicitspatialmeaning,enablingdirectmodifica-
tionswithinthescenebyeditingtheHexPlaneduringinference.
8Preprint.
Command-DrivenSceneGeneration DynamicObjectInpainting
TurnLeft Forward TurnRight Static Before After
T=1 T=1 T=1 T=1 T=1 T=1
T=8 T=8 T=8 T=8 T=8 T=8
T=16 T=16 T=16 T=16 T=16 T=16
Layout-ConditionedSceneGeneration Trajectory-GuidedGeneration
T=1 T=1 T=1 T=1 T=1 T=1
T=8 T=8 T=8 T=8 T=8 T=8
T=16 T=16 T=16 T=16 T=16 T=16
Build Vehicle Ped Road Sidewalk Barrier Ground Veg Other Pole
Figure7: DynamicSceneGenerationApplications. Wedemonstratethecapabilityofourmodelon
adiversesetofdownstreamtasks. Weshowthe1st,8th,and16thframesforsimplicity. Kindlyrefer
totheAppendixforcompletesequentialscenesandlongertemporalmodelingexamples.
5.3 ABLATIONSTUDIES
WeconductablationstudiestodemonstratetheeffectivenessofthecomponentsofDynamicCity.
VAE.TheeffectivenessoftheVAEisdrivenbytwokeyinnovations: ProjectionModuleandExpan-
sion&SqueezeStrategy(ESS).AsshowninTab.3,theproposedProjectionModulesubstantially
improves HexPlane fitting performance, delivering up to a 12.56% increase in mIoU compared
to traditional averaging operations. Additionally, compared to querying each point individually,
ESSenhancesHexPlanefittingqualitywithuptoa7.05%mIoUimprovement,significantlyboosts
trainingspeedbyupto2.06x,andreducesmemoryusagebyasubstantial70.84%.
HexPlaneDimensions. ThedimensionsofHexPlanehaveadirectimpactonbothtrainingefficiency
andreconstructionquality. Tab.4providesacomparisonofvariousdownsampleratesappliedto
theoriginalHexPlanedimensions, whichare16×128×128×8forCarlaSCand16×200×
200×16forOcc3D-Waymo. Asthedownsamplingratesincrease,boththecompressionrateand
trainingefficiencyimprovesignificantly,butthereconstructionquality,measuredbymIoU,decreases.
9Preprint.
Table3: AblationStudyonVAENetworkStructures. WereportthemIoUscores,trainingtime
(second-per-iteration),andtraining-timememoryconsumption(VRAM)ofdifferentEncoderand
Decoder configurations on CarlaSC and Occ3D-Waymo, respectively. Note that “ESS” denotes
“Expansion&Squeeze”. Thebestandsecond-bestvaluesareinboldandunderlined.
CarlaSC Occ3D-Waymo
Encoder Decoder
mIoU↑ Time(s)↓ VRAM(G)↓ mIoU↑ Time(s)↓ VRAM(G)↓
AveragePooling Query 60.97% 0.236 12.46 49.37% 1.563 69.66
AveragePooling ESS 68.02% 0.143 4.27 55.72% 0.758 20.31
Projection Query 68.73% 0.292 13.59 61.93% 2.128 73.15
Projection ESS 74.22% 0.205 5.92 62.57% 1.316 25.92
Table4: AblationStudyonHexPlaneDownsampling(D.S.)Rates. Wereportthecompression
ratios(C.R.),mIoUscores,trainingspeed(secondsperiteration),andtraining-timememoryconsump-
tiononCarlaSCandOcc3D-Waymo. Thebestandsecond-bestvaluesareinboldandunderlined.
D.S.Rates CarlaSC Occ3D-Waymo
d d d d C.R.↑ mIoU↑ Time(s)↓ VRAM(G)↓ C.R.↑ mIoU↑ Time(s)↓ VRAM(G)↓
T X Y Z
1 1 1 1 5.78% 84.67% 1.149 21.63 Out-of-Memory >80
1 2 2 1 17.96% 76.05% 0.289 8.49 38.42% 63.30% 1.852 32.82
2 2 2 2 23.14% 74.22% 0.205 5.92 48.25% 62.37% 0.935 24.9
2 4 4 2 71.86% 65.15% 0.199 4.00 153.69% 58.13% 0.877 22.30
Table5: AblationStudyonOrganizingHexPlaneasImageTokens. WereporttheInceptionScore
(IS),FréchetInceptionDistance(FID),KernelInceptionDistance(KID),andthePrecision(P)and
Recall(R)ratesonCarlaSC.Thebestvaluesarehighlightedinbold.
Metric2D Metric3D
Method
IS2D↑ FID2D↓ KID2D↓ P2D↑ R2D↑ IS3D↑ FID3D↓ KID3D↓ P3D↑ R3D↑
DirectUnfold 2.496 205.0 0.248 0.000 0.000 2.269 9110 723.7 0.173 0.043
VerticalConcatenation 2.476 12.79 0.003 0.191 0.042 2.305 623.2 26.67 0.424 0.159
PaddedRollout 2.498 10.96 0.002 0.238 0.066 2.331 354.2 19.10 0.460 0.170
Toachievetheoptimalbalancebetweentrainingefficiencyandreconstructionquality,weselecta
downsamplingrateofd =d =d =d =2.
T X Y Z
PaddedRolloutOperation. WecomparethePaddedRolloutOperationwithdifferentstrategies
for obtaining image tokens: 1) Direct Unfold: directly unfolding the six planes into patches and
concatenating them; 2) Vertical Concat: vertically concatenating the six planes without aligning
dimensionsduringtherolloutprocess.AsshowninTab.5,PaddedRolloutOperation(PRO)efficiently
modelsspatialandtemporalrelationshipsinthetokensequence,achievingoptimalgenerationquality.
6 CONCLUSION
WepresentDynamicCity,aframeworkforhigh-quality4DLiDARscenegenerationthatcaptures
thetemporaldynamicsofreal-worldenvironments. OurmethodintroducesHexPlane,acompact
4D representation generated using a VAE with a Projection Module, alongside an Expansion &
Squeeze Strategy to enhance reconstruction efficiency and accuracy. Additionally, our Masked
RolloutOperationreorganizesHexPlanefeaturesforDiT-baseddiffusion,enablingversatile4Dscene
generation. ExtensiveexperimentsdemonstratethatDynamicCitysurpassesstate-of-the-artmethods
inbothreconstructionandgeneration,offeringsignificantimprovementsinquality,trainingspeed,
andmemoryefficiency. DynamicCitypavesthewayforfutureresearchindynamicscenegeneration.
10Preprint.
REFERENCES
AntonioAlliegro,YawarSiddiqui,TatianaTommasi,andMatthiasNießner. Polydiff: Generating3d
polygonalmesheswithdiffusionmodels. arXivpreprintarXiv:2312.11417,2023. 3
SherwinBahmani,XianLiu,YifanWang,IvanSkorokhodov,VictorRong,ZiweiLiu,XihuiLiu,
JeongJoonPark,SergeyTulyakov,GordonWetzstein,AndreaTagliasacchi,andDavidB.Lindell.
Tc4d: Trajectory-conditionedtext-to-4dgeneration. arXivpreprintarXiv:2403.17920,2024. 3
MaximBerman,AmalRannenTriki,andMatthewBBlaschko. Thelovász-softmaxloss: Atractable
surrogate for the optimization of the intersection-over-union measure in neural networks. In
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.4413–4421,2018. 5
AndreasBlattmann,RobinRombach,HuanLing,TimDockhorn,SeungWookKim,SanjaFidler,and
KarstenKreis. Alignyourlatents: High-resolutionvideosynthesiswithlatentdiffusionmodels. In
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.22563–22575,2023. 3
LucasCaccia,HerkevanHoof,AaronCourville,andJoellePineau.Deepgenerativemodelingoflidar
data. InIEEE/RSJInternationalConferenceonIntelligentRobotsandSystems,pp.5034–5040,
2019. 3
HolgerCaesar,VarunBankiti,AlexHLang,SourabhVora,VeniceErinLiong,QiangXu,Anush
Krishnan,YuPan,GiancarloBaldan,andOscarBeijbom. nuscenes: Amultimodaldatasetfor
autonomousdriving. InIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.
11621–11631,2020. 7,15
AngCaoandJustinJohnson. Hexplane: Afastrepresentationfordynamicscenes. InIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.130–141,2023. 2,3,4
EricRChan,ConnorZLin,MatthewAChan,KokiNagano,BoxiaoPan,ShaliniDeMello,Orazio
Gallo,LeonidasJGuibas,JonathanTremblay,SamehKhamis,etal. Efficientgeometry-aware
3dgenerativeadversarialnetworks. InIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.16123–16133,2022. 3
ChristopherChoy,JunYoungGwak,andSilvioSavarese. 4dspatio-temporalconvnets: Minkowski
convolutionalneuralnetworks. InIEEE/CVFConferenceonComputerVisionandPatternRecog-
nition,pp.3075–3084,2019. 16
TriDao,DanielY.Fu,StefanoErmon,AtriRudra,andChristopherRé. Flashattention: Fastand
memory-efficientexactattentionwithio-awareness. InAdvancesinNeuralInformationProcessing
Systems,volume35,pp.16344–16359,2022. 17
Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo
Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.12479–12488,2023. 2,3,4
JonathanHoandTimSalimans. Classifier-freediffusionguidance. arXivpreprintarXiv:2207.12598,
2022. 6,17
FangzhouHong, LingdongKong, HuiZhou, XingeZhu, HongshengLi, andZiweiLiu. Unified
3dand4dpanopticsegmentationviadynamicshiftingnetworks. IEEETransactionsonPattern
AnalysisandMachineIntelligence,46(5):3480–3495,2024. 3
QianjiangHu,ZhiminZhang,andWeiHu. Rangeldm: Fastrealisticlidarpointcloudgeneration. In
EuropeanConferenceonComputerVision,pp.115–135,2024. 3
Siyuan Huang, Yichen Xie, Song-Chun Zhu, and Yixin Zhu. Spatio-temporal self-supervised
representationlearningfor3dpointclouds. InIEEE/CVFInternationalConferenceonComputer
Vision,pp.6535–6545,2021. 2
YanqinJiang,LiZhang,JinGao,WeiminHu,andYaoYao. Consistent4d: Consistent360°dynamic
objectgenerationfrommonocularvideo. arXivpreprintarXiv:2311.02848,2023. 3
11Preprint.
Jumin Lee, Sebin Lee, Changho Jo, Woobin Im, Juhyeong Seon, and Sung-Eui Yoon. Semcity:
Semanticscenegenerationwithtriplanediffusion. InIEEE/CVFConferenceonComputerVision
andPatternRecognition,pp.28337–28347,2024. 2,3,7,8,18
YuhengLiu,XinkeLi,XuetingLi,LuQi,ChongshouLi,andMing-HsuanYang. Pyramiddiffusion
forfine3dlargescenegeneration. arXivpreprintarXiv:2311.12085,2023a. 2
Zhen Liu, Yao Feng, Michael J. Black, Derek Nowrouzezahrai, Liam Paull, and Weiyang Liu.
Meshdiffusion: Score-based generative 3d mesh modeling. In International Conference on
LearningRepresentations,2023b. 3
ZhiyuanMa,YuxiangWei,YabinZhang,XiangyuZhu,ZhenLei,andLeiZhang. Scaledreamer:
Scalabletext-to-3dsynthesiswithasynchronousscoredistillation. InEuropeanConferenceon
ComputerVision,pp.1–19,2024. 3
KazutoNakashimaandRyoKurazume. Learningtodroppointsforlidarscansynthesis. InIEEE/RSJ
InternationalConferenceonIntelligentRobotsandSystems,pp.222–229,2021. 2
KazutoNakashimaandRyoKurazume. Lidardatasynthesiswithdenoisingdiffusionprobabilistic
models. InIEEEInternationalConferenceonRoboticsandAutomation,pp.14724–14731,2024.
3
KazutoNakashima,YumiIwashita,andRyoKurazume. Generativerangeimagingforlearningscene
priorsof3dlidardata. InIEEE/CVFWinterConferenceonApplicationsofComputerVision,pp.
1256–1266,2023. 2,3
Lucas Nunes, Rodrigo Marcuzzi, Benedikt Mersch, Jens Behley, and Cyrill Stachniss. Scaling
diffusionmodelstoreal-world3dlidarscenecompletion. InIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pp.14770–14780,2024. 3
AdamPaszke,SamGross,FranciscoMassa,AdamLerer,JamesBradbury,GregoryChanan,Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style,
high-performancedeeplearninglibrary. AdvancesinNeuralInformationProcessingSystems,32:
8026–8037,2019. 17
William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE/CVF
InternationalConferenceonComputerVision,pp.4195–4205,2023. 2,4,6
HaoxiRan,VitorGuizilini,andYueWang. Towardsrealisticscenegenerationwithlidardiffusion
models. InIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.14738–14748,
2024. 3
JiaweiRen,LiangPan,JiaxiangTang,ChiZhang,AngCao,GangZeng,andZiweiLiu. Dreamgaus-
sian4d: Generative4dgaussiansplatting. arXivpreprintarXiv:2312.17142,2023. 3
JiaweiRen,KevinXie,AshkanMirzaei,HanxueLiang,XiaohuiZeng,KarstenKreis,ZiweiLiu,
Antonio Torralba, Sanja Fidler, Seung Wook Kim, and Huan Ling. L4gm: Large 4d gaussian
reconstructionmodel. arXivpreprintarXiv:2406.10324,2024a. 3
XuanchiRen,JiahuiHuang,XiaohuiZeng,KenMuseth,SanjaFidler,andFrancisWilliams. Xcube:
Large-scale3dgenerativemodelingusingsparsevoxelhierarchies. InIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pp.4209–4219,2024b. 2,3
SaraRojas,JulienPhilip,KaiZhang,SaiBi,FujunLuan,BernardGhanem,andKalyanSunkavall.
Datenerf: Depth-awaretext-basededitingofnerfs. arXivpreprintarXiv:2404.04526,2024. 3
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pp.10684–10695,2022. 3
YichunShi,PengWang,JianglongYe,MaiLong,KejieLi,andXiaoYang. Mvdream: Multi-view
diffusionfor3dgeneration. arXivpreprintarXiv:2308.16512,2023. 3
12Preprint.
KarenSimonyanandAndrewZisserman. Verydeepconvolutionalnetworksforlarge-scaleimage
recognition. arXivpreprintarXiv:1409.1556,2015. 16
UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn,SongyangZhang,QiyuanHu,HarryYang,
OronAshual,OranGafni,DeviParikh,SonalGupta,andYanivTaigman. Make-a-video: Text-to-
videogenerationwithouttext-videodata.InInternationalConferenceonLearningRepresentations,
2022. 3
UrielSinger,ShellySheynin,AdamPolyak,OronAshual,IuriiMakarov,FilipposKokkinos,Naman
Goyal,AndreaVedaldi,DeviParikh,JustinJohnson,andYanivTaigman. Text-to-4ddynamic
scenegeneration. arXivpreprintarXiv:2301.11280,2023. 3
Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui,
JamesGuo,YinZhou,YuningChai,BenjaminCaine,VijayVasudevan,WeiHan,JiquanNgiam,
HangZhao,AlekseiTimofeev,ScottEttinger,MaximKrivokon,AmyGao,AdityaJoshi,YuZhang,
JonathonShlens,ZhifengChen,andDragomirAnguelov. Scalabilityinperceptionforautonomous
driving: Waymoopendataset. InIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion,pp.2446–2454,2020. 7,15
ChristianSzegedy,VincentVanhoucke,SergeyIoffe,JonathonShlens,andZbigniewWojna. Re-
thinkingtheinceptionarchitectureforcomputervision. InIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pp.2818–2826,2015. 16
HaotianTang,ZhijianLiu,ShengyuZhao,YujunLin,JiLin,HanruiWang,andSongHan. Search-
ingefficient3darchitectureswithsparsepoint-voxelconvolution. InEuropeanConferenceon
ComputerVision,pp.685–702,2020. 16
XiaoyuTian,TaoJiang,LongfeiYun,YuchengMao,HuitongYang,YueWang,YilunWang,and
HangZhao. Occ3d: Alarge-scale3doccupancypredictionbenchmarkforautonomousdriving. In
AdvancesinNeuralInformationProcessingSystems,volume36,pp.64318–64330,2023. 7,15,
20,22,23
LeningWang,WenzhaoZheng,YilongRen,HanJiang,ZhiyongCui,HaiyangYu,andJiwenLu.
Occsora: 4doccupancygenerationmodelsasworldsimulatorsforautonomousdriving. arXiv
preprintarXiv:2405.20337,2024. 2,3,7,8,19,27
JoeyWilson,JingyuSong,YueweiFu,ArthurZhang,AndrewCapodieci,ParamsothyJayakumar,
KiraBarton,andMaaniGhaffari. Motionsc: Datasetandnetworkforreal-timesemanticmapping
indynamicenvironments. IEEERoboticsandAutomationLetters,7(3):8439–8446,2022. 7,15,
19,20,21,24,25,26,27
Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and
Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from a single image.
arXivpreprintarXiv:2405.20343,2024a. 3
ShuangWu,YoutianLin,FeihuZhang,YifeiZeng,JingxiXu,PhilipTorr,XunCao,andYaoYao.
Direct3d: Scalable image-to-3d generation via 3d latent diffusion transformer. arXiv preprint
arXiv:2405.14832,2024b. 3
YuwenXiong,Wei-ChiuMa,JingkangWang,andRaquelUrtasun. Ultralidar: Learningcompact
representationsforlidarcompletionandgeneration. arXivpreprintarXiv:2311.01448,2023. 3
XiangXu,LingdongKong,HuiShuai,WenweiZhang,LiangPan,KaiChen,ZiweiLiu,andQingshan
Liu. 4dcontrastivesuperflowsaredense3drepresentationlearners. InEuropeanConferenceon
ComputerVision,pp.58–80,2024. 2
ZehanZheng, FanLu, WeiyiXue, GuangChen, andChangjunJiang. Lidar4d: Dynamicneural
fieldsfornovelspace-timeviewlidarsynthesis. InIEEE/CVFConferenceonComputerVisionand
PatternRecognition,pp.5145–5154,2024. 2,3
VlasZyrianov,XiyueZhu,andShenlongWang. Learningtogeneraterealisticlidarpointclouds. In
EuropeanConferenceonComputerVision,pp.17–35,2022. 3
13Preprint.
APPENDIX
In thisappendix, wesupplement the following materials tosupport the findings and conclusions
drawninthemainbodyofthispaper.
A AdditionalImplementationDetails 15
A.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.2 DiTEvaluationMetrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.3 ModelDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.4 Classifier-FreeGuidance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
A.5 DownstreamApplications. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B AdditionalQuantitativeResults 19
B.1 Per-ClassGenerationResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
C AdditionalQualitativeResults 20
C.1 UnconditionalDynamicSceneGeneration . . . . . . . . . . . . . . . . . . . . . . 20
C.2 HexPlane-GuidedGeneration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.3 Layout-GuidedGeneration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.4 Command-&Trajectory-GuidedGeneration . . . . . . . . . . . . . . . . . . . . . 24
C.5 DynamicInpainting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
C.6 ComparisonswithOccSora . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
D PotentialSocietalImpact&Limitations 28
D.1 SocietalImpact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
D.2 BroaderImpact . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
D.3 KnownLimitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
E PublicResourcesUsed 29
E.1 PublicDatasetsUsed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
E.2 PublicImplementationsUsed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
14Preprint.
A ADDITIONAL IMPLEMENTATION DETAILS
In this section, we provide additional implementation details to assist in reproducing this work.
Specifically,weelaborateonthedetailsofthedatasets,DiTevaluationmetrics,thespecificsofour
generationmodels,anddiscussionsonthedownstreamapplications.
A.1 DATASETS
Ourexperimentsprimarilyutilizetwodatasets: Occ3D-Waymo(Tianetal.,2023)andCarlaSC(Wil-
sonetal.,2022). Additionally,wealsoevaluateourVAEonOcc3D-nuScenes(Tianetal.,2023).
TheOcc3D-Waymodatasetisderivedfromreal-worldWaymoOpenDataset(Sunetal.,2020)data,
whereoccupancysequencesareobtainedthroughmulti-framefusionandvoxelizationprocesses.
Similarly,Occ3D-nuScenesisgeneratedfromthereal-worldnuScenes(Caesaretal.,2020)dataset
using the same fusion and voxelization operations. On the other hand, the CarlaSC dataset is
generatedfromsimulatedscenesandsensordata,yieldingoccupancysequences.
Using these different datasets demonstrates the effectiveness of our method on both real-world
andsyntheticdata. Toensureconsistencyintheexperimentalsetup,weselect11commonlyused
semanticcategoriesandmaptheoriginalcategoriesfrombothdatasetstothese11categories. The
detailedsemanticlabelmappingsareprovidedinTab.6.
Table 6: Summary of Semantic Label Mappings. We unify the semantic classes between the
CarlaSC(Wilsonetal.,2022),Occ3D-Waymo(Tianetal.,2023),andOcc3D-nuScenes(Tianetal.,
2023)datasetsforsemanticscenegeneration.
Class CarlaSC Occ3D-Waymo Occ3D-nuScenes
■ Building Building Building Manmade
■ Barrier Barrier,Wall,Guardrail - Barrier
■ Other Other,Sky,Bridge,Rail GeneralObject GeneralObject
track,Static,Dynamic,
Water
■ Pedestrian Pedestrian Pedestrian Pedestrian
■ Pole Pole,Trafficsign,Traffic Sign,Trafficlight,Pole, Trafficcone
light ConstructionCone
■ Road Road,Roadlines Road Drivablesurface
■ Ground Ground,Terrain - Otherflat,Terrain
■ Sidewalk Sidewalk Sidewalk Sidewalk
■ Vegetation Vegetation Vegetation,Treetrunk Vegetation
■ Vehicle Vehicle Vehicle Bus,Car,Construction
vehicle,Trailer,Truck
■ Bicycle - Bicyclist,Bicycle, Bicycle,Motorcycle
Motorcycle
• Occ3D-Waymo. Thisdatasetcontains798trainingscenes,witheachscenelastingapproxi-
mately20secondsandsampledatafrequencyof10Hz. Thisdatasetincludes15semantic
categories. Weusevolumeswitharesolutionof200×200×16fromthisdataset.
• CarlaSC. This dataset contains 6 training scenes, each duplicated into Light, Medium,
and Heavy based on traffic density. Each scene lasts approximately 180 seconds and is
sampled at a frequency of 10 Hz. This dataset contains 22 semantic categories, and the
sceneresolutionis128×128×8.
• Occ3D-nuScenes. Thisdatasetcontains600scenes,witheachscenelastingapproximately
20secondsandsampledatafrequencyof2Hz. ComparedtoOcc3D-WaymoandCarlaSC,
Occ3D-nuSceneshasfewertotalframesandmorevariationbetweenscenes. Thisdataset
includes17semanticcategories,witharesolutionof200×200×16.
15Preprint.
A.2 DITEVALUATIONMETRICS
InceptionScore(IS).Thismetricevaluatesthequalityanddiversityofgeneratedsamplesusinga
pre-trainedInceptionmodelasfollows:
IS=exp(cid:0)E
[D
(p(y|Q)∥p(y))](cid:1)
, (5)
Q∼pg KL
wherep representsthedistributionofgeneratedsamples. p(y|Q)istheconditionallabeldistribution
g
(cid:82)
givenbytheInceptionmodelforageneratedsampleQ. p(y)= p(y|Q)p (Q)dQisthemarginal
g
distributionoverallgeneratedsamples. D (p(y|Q) ∥ p(y))istheKullback-Leiblerdivergence,
KL
definedasfollows:
D
(p(y|Q)∥p(y))=(cid:88)
p(y
|Q)logp(y i|Q)
. (6)
KL i p(y )
i
i
FréchetInceptionDistance(FID).Thismetricmeasuresthedistancebetweenthefeaturedistribu-
tionsofrealandgeneratedsamples:
(cid:16) (cid:17)
FID=∥µ −µ ∥2+Tr Σ +Σ −2(Σ Σ )1/2 , (7)
r g r g r g
whereµ andΣ arethemeanandcovariancematrixoffeaturesfromrealsamples. µ andΣ are
r r g g
themeanandcovariancematrixoffeaturesfromgeneratedsamples. Trdenotesthetraceofamatrix.
Kernel Inception Distance (KID). This metric uses the squared Maximum Mean Discrepancy
(MMD)withapolynomialkernelasfollows:
KID=MMD2(ϕ(Q ),ϕ(Q )), (8)
r g
whereϕ(Q )andϕ(Q )representthefeaturesofrealandgeneratedsamplesextractedfromthe
r g
Inceptionmodel.
MMDwithapolynomialkernelk(x,y)=(x⊤y+c)discalculatedasfollows:
1 (cid:88) 1 (cid:88) 2 (cid:88)
MMD2(X,Y)= k(x ,x )+ k(y ,y )− k(x ,y ), (9)
m(m−1) i j n(n−1) i j mn i j
i̸=j i̸=j i,j
where X = {Q ,...,Q } and Y = {y ,...,y } are sets of features from real and generated
1 m 1 n
samples.
Precision. This metric measures the fraction of generated samples that lie within the real data
distributionasfollows:
N
Precision= 1 (cid:88) I(cid:0) (f −µ )⊤Σ−1(f −µ )≤χ2(cid:1) , (10)
N g r r g r
i=1
wheref isageneratedsampleinthefeaturespace. µ andΣ arethemeanandcovarianceofthereal
g r r
datadistribution. I(·)istheindicatorfunction. χ2isathresholdbasedonthechi-squareddistribution.
Recall.Thismetricmeasuresthefractionofrealsamplesthatliewithinthegenerateddatadistribution
asfollows:
M
Recall= 1 (cid:88) I(cid:0) (f −µ )⊤Σ−1(f −µ )≤χ2(cid:1) , (11)
M r g g r g
j=1
where: f is a real sample in the feature space. µ and Σ are the mean and covariance of the
r g g
generateddatadistribution. I(·)istheindicatorfunction. χ2isathresholdbasedonthechi-squared
distribution.
2DEvaluations. Werender3Dscenesas2Dimagesfor2Devaluations. Toensurefaircomparisons,
we use the same semantic colormap and camera settings across all experiments. A pre-trained
InceptionV3 (Szegedy et al., 2015) model is used to compute the Inception Score (IS), Fréchet
InceptionDistance(FID),andKernelInceptionDistance(KID)scores,whilePrecisionandRecall
arecomputedusingapre-trainedVGG-16(Simonyan&Zisserman,2015)model.
3DEvaluations. For3Ddata,wetrainedaMinkowskiUNet(Choyetal.,2019)asanautoencoder.
WeadoptthelatestimplementationfromSPVNAS(Tangetal.,2020),whichsupportsoptimized
sparseconvolutionoperations. Thefeatureswereextractedbyapplyingaveragepoolingtotheoutput
ofthefinaldownsamplingblock.
16Preprint.
A.3 MODELDETAILS
GeneralTrainingDetails. WeimplementboththeVAEandDiTmodelsusingPyTorch(Paszke
et al., 2019). We utilize PyTorch’s mixed precision and replace all attention mechanisms with
FlashAttention(Daoetal.,2022)toacceleratetrainingandreducememoryusage. AdamWisusedas
theoptimizerforallmodels.
WetraintheVAEwithalearningrateof10−3,runningfor20epochsonOcc3D-Waymoand100
epochsonCarlaSC.TheDiTistrainedwithalearningrateof10−4,andtheEMArateforDiTisset
to0.9999.
VAE.Ourencoderprojectsthe4DinputQintoaHexPlane,whereeachdimensionisacompressed
versionoftheoriginal4Dinput. First, a3DCNNisappliedtoeachframeforfeatureextraction
anddownsampling,withdimensionalityreductionappliedonlytothespatialdimensions(X,Y,Z).
Next,theProjectionModuleprojectsthe4DfeaturesintotheHexPlane. Eachsmalltransformer
withintheProjectionModuleconsistsoftwolayers,andtheattentionmechanismhastwoheads.
Eachheadhasadimensionalityof16,withadropoutrateof0.1. Afterward,wefurtherdownsample
theT dimensiontohalfofitsoriginalsize.
Duringdecoding,wefirstusethreesmalltransposeCNNstorestoretheT dimension,thenusean
ESSmoduletorestorethe4Dfeatures. Finally,weapplya3DCNNtorecoverthespatialdimensions
andgeneratepoint-wisepredictions.
Diffusion. Wesetthepatchsizepto2forourDiTmodels. TheWaymoDiTmodelhasahiddensize
of768,18DiTblocks,and12attentionheads. TheCarlaSCDiTmodelhasahiddensizeof384,16
DiTblocks,and8attentionheads.
A.4 CLASSIFIER-FREEGUIDANCE
Classifier-FreeGuidance(CFG)(Ho&Salimans,2022)couldimprovetheperformanceofconditional
generativemodelswithoutrelyingonanexternalclassifier. Specifically,duringtraining,themodel
simultaneouslylearnsbothconditionalgenerationp(x|c)andunconditionalgenerationp(x),and
guidanceduringsamplingisprovidedbythefollowingequation:
xˆ =(1+w)·xˆ (c)−w·xˆ (∅), (12)
t t t
where xˆ (c) is the result conditioned on c, xˆ (∅) is the unconditioned result, and w is a weight
t t
parameter controlling the strength of the conditional guidance. By adjusting w, an appropriate
balancebetweentheaccuracyanddiversityofthegeneratedscenescanbeachieved.
A.5 DOWNSTREAMAPPLICATIONS
Thissectionprovidesacomprehensiveexplanationoffivetaskstodemonstratethecapabilityofour
4Dscenegenerationmodelacrossvariousscenarios.
HexPlane. Since our model is based on Latent Diffusion Models, it is inherently constrained to
generateresultsthatmatchthelatentspacedimensions,limitingthetemporallengthofunconditionally
generatedsequences.Wearguethatarobust4Dgenerationmodelshouldnotberestrictedtoproducing
onlyshortsequences. Insteadofincreasinglatentspacesize,weleverageCFGtogeneratesequences
in an auto-regressive manner. By conditioning each new 4D sequence on the previous one, we
sequentiallyextendthetemporaldimension. Thisiterativeprocesssignificantlyextendssequence
length,enablinglong-termgeneration,andallowsconditioningonanyreal-world4Dscenetopredict
thenextsequenceusingtheDiTmodel.
WeconditionourDiTbyusingtheHexPlanefromT framesearlier. ForanyconditionHexPlane,
weapplypatchembeddingandpositionalencodingoperationstoobtainconditiontokens. These
tokens,combinedwithotherconditions,arefedintotheadaLN-ZeroandCross-Attentionbranches
toinfluencethemainbranch.
Layout. Tocontrolobjectplacementinthescene,wetrainamodelcapableofgeneratingvehicle
dynamicsbasedonabird’s-eyeviewsketch. Weapplysemanticfilteringtothebird’s-eyeviewof
theinputscene,markingregionswithvehiclesas1andregionswithoutvehiclesas0. Poolingthis
binaryimageprovideslayoutinformationasaT ×H ×W tensorfromthebird’s-eyeperspective.
17Preprint.
ThelayoutispaddedtomatchthesizeoftheHexPlane,ensuringthatthepositionalencodingofthe
bird’s-eyelayoutalignswiththeXY plane. DiTlearnsthecorrespondencebetweenthelayoutand
vehiclesemanticsusingthesameconditionalinjectionmethodappliedtotheHexPlane.
Command. WhilewehavedevelopedeffectivemethodstocontroltheHexPlaneinbothtemporal
andspatialdimensions,acriticalaspectof4Dautonomousdrivingscenariosisthemotionoftheego
vehicle. Toaddressthis,wedefinefourcommands: STATIC,FORWARD,TURN LEFT,andTURN
RIGHT,andannotateourtrainingdatabyanalyzingegovehicleposes. Duringtraining,wefollow
thetraditionalDiTapproachofinjectingclasslabels,wherethecommandsareembeddedandfed
intothemodelviaadaLN-Zero.
Trajectory. Formorefine-grainedcontroloftheegovehicle’smotion, weextendthecommand-
basedconditioningintoatrajectoryconditionbranch. Forany4Dscene,theXY coordinatesofthe
trajectorytraj∈RT×2arepassedthroughanMLPandinjectedintotheadaLN-Zerobranch.
Inpaint. Wedemonstratethatourmodelcanhandleversatileapplicationsbytrainingaconditional
DiT for the previous tasks. Extending our exploration of downstream applications, and inspired
by (Lee et al., 2024), we leverage the 2D structure of our latent space and the explicit modeling
of each dimension to highlight our model’s ability to perform inpainting on 4D scenes. During
DiTsampling, wedefinea2Dmaskm ∈ RX×Y ontheXY plane, whichisextendedacrossall
dimensionstomaskspecificregionsoftheHexPlane.
Ateachstepofthediffusionprocess,weapplynoisetotheinputHinandupdatetheHexPlaneusing
thefollowingformula:
H =m⊙H +(1−m)⊙Hin, (13)
t t t
where⊙denotestheelement-wiseproduct.Thisprocessinpaintsthemaskedregionswhilepreserving
theunmaskedareasofthescene,enablingpartialscenemodification,suchasturninganemptystreet
intoonewithheavytraffic.
18Preprint.
B ADDITIONAL QUANTITATIVE RESULTS
Inthissection,wepresentadditionalquantitativeresultstodemonstratetheeffectivenessofourVAE
inaccuratelyreconstructing4Dscenes.
B.1 PER-CLASSGENERATIONRESULTS
Weincludetheclass-wiseIoUscoresofOccSora(Wangetal.,2024)andourproposedDynamicCity
frameworkonCarlaSC(Wilsonetal.,2022). AsshowninTab.7,ourresultsdemonstratehigher
IoUacrossallclasses,indicatingthatourVAEreconstructionachievesminimalinformationloss.
Additionally, our model does not exhibit significantly low IoU for any specific class, proving its
abilitytoeffectivelyhandleclassimbalance.
Table7: ComparisonsofPer-ClassIoUScores. WecomparedtheperformanceofOccSora(Wang
etal.,2024),andourDynamicCityframeworkonCarlaSC(Wilsonetal.,2022)across10semantic
classes.Thesceneresolutionis128×128×8.Thesequencelengthsare4,8,16,and32,respectively.
Method mIoU
Resolution:128×128×8 SequenceLength:4
OccSora 41.009 38.861 10.616 6.637 19.191 21.825 93.910 61.357 86.671 15.685 55.340
Ours 79.604 76.364 31.354 68.898 93.436 87.962 98.617 87.014 95.129 68.700 88.569
Improv. 38.595 37.503 20.738 62.261 74.245 66.137 4.707 25.657 8.458 53.015 33.229
Resolution:128×128×8 SequenceLength:8
OccSora 39.910 33.001 3.260 5.659 19.224 19.357 93.038 57.335 85.551 30.899 51.776
Ours 76.181 70.874 50.025 52.433 87.958 85.866 97.513 83.074 93.944 58.626 81.498
Improv. 36.271 37.873 46.765 46.774 68.734 66.509 4.475 25.739 8.393 27.727 29.722
Resolution:128×128×8 SequenceLength:16
OccSora 33.404 19.264 2.205 3.454 11.781 9.165 92.054 50.077 82.594 18.078 45.363
Ours 74.223 66.852 51.901 49.844 79.410 82.369 96.937 84.484 94.082 58.217 78.134
Improv. 40.819 47.588 49.696 46.390 67.629 73.204 4.883 34.407 11.488 40.139 32.771
Resolution:128×128×8 SequenceLength:32
OccSora 28.911 16.565 1.413 0.944 6.200 4.150 91.466 43.399 78.614 11.007 35.353
Ours 59.308 52.036 25.521 29.382 56.811 57.876 94.792 78.390 89.955 46.080 62.234
Improv. 30.397 35.471 24.108 28.438 50.611 53.726 3.326 34.991 11.341 35.073 26.881
19
gnidliuB reirraB
rehtO
nairtsedeP
eloP
daoR
dnuorG klawediS
noitategeV
elciheVPreprint.
C ADDITIONAL QUALITATIVE RESULTS
Inthissection,weprovideadditionalqualitativeresultsontheOcc3D-Waymo(Tianetal.,2023)and
CarlaSC(Wilsonetal.,2022)datasetstodemonstratetheeffectivenessofourapproach.
C.1 UNCONDITIONALDYNAMICSCENEGENERATION
First,wepresentfullunconditionalgenerationresultsinFig.8and9. Theseresultsdemonstratethat
ourgeneratedscenesareofhighquality,realistic,andcontainsignificantdetail,capturingboththe
overallscenedynamicsandthemovementofobjectswithinthescenes.
T=1 T=2 T=3 T=4
T=8 T=7 T=6 T=5
T=9 T=10 T=11 T=12
T=16 T=15 T=14 T=13
Build Vehicle Ped Road Sidewalk Barrier Ground Veg Other Pole
Figure8: UnconditionalDynamicSceneGenerationResults. Weprovidequalitativeexamplesofa
totalof16consectutiveframesgeneratedbyDynamicCityontheOcc3D-Waymo(Tianetal.,2023)
dWataaysemt.oBuenstcvoinewdietdioinnaclo1lsocrseanned1z8oomed-inforadditionaldetails.
20Preprint.
T=1 T=2 T=3 T=4
T=8 T=7 T=6 T=5
T=9 T=10 T=11 T=12
T=16 T=15 T=14 T=13
Build Vehicle Ped Road Sidewalk Barrier Ground Veg Other Pole
Figure9: UnconditionalDynamicSceneGenerationResults. Weprovidequalitativeexamplesof
atotalof16consectutiveframesgeneratedbyDynamicCityontheCarlaSC(Wilsonetal.,2022)
dataset. Bestviewedincolorsandzoomed-inforadditionaldetails.
21Preprint.
C.2 HEXPLANE-GUIDEDGENERATION
WeshowresultsforourHexPlaneconditionalgenerationinFig.10. Althoughthesequencesare
generatedingroupsof16duetothesettingsofourVAE,wesuccessfullygeneratealongsequence
byconditioningonthepreviousone. Theresultcontains64frames,comprisingfoursequences,and
depictsaT-intersectionwithmanycarsparkedalongtheroadside. Thisresultdemonstratesstrong
temporalconsistencyacrosssequences,provingthatourframeworkcaneffectivelypredictthenext
sequencebasedonthecurrentone.
1 2 3 4 5 6 7 8
16 15 14 13 12 11 10 9
17 18 19 20 21 22 23 24
32 31 30 29 28 27 26 25
33 34 35 36 37 38 39 40
48 47 46 45 44 43 42 41
49 50 51 52 53 54 55 56
64 63 62 61 60 59 58 57
Build Vehicle Ped Road Sidewalk Barrier Ground Veg Other Pole
Figure10: HexPlane-GuidedGenerationResults. Weprovidequalitativeexamplesofatotalof64
consectutiveframesgeneratedbyDynamicCityontheOcc3D-Waymo(Tianetal.,2023)dataset.
Bestviewedincolorsandzoomed-inforadditionaldetails.
22Preprint.
C.3 LAYOUT-GUIDEDGENERATION
ThelayoutconditionalgenerationresultispresentedinFig.11. First,weobservethatthelayout
closelymatchesthesemanticpositionsinthegeneratedresult. Additionally,asthelayoutchanges,
the positions of the vehicles in the scene also change accordingly, demonstrating that our model
effectivelycapturestheconditionandinfluencesboththeoverallscenelayoutandvehicleplacement.
T=1 T=2 T=3
T=6 T=5 T=4
T=7 T=8 T=9
T=12 T=11 T=10
T=13 T=14 T=15
Build Vehicle Ped Road Sidewalk Barrier Ground Veg Other Pole
Figure11: Layout-GuidedGenerationResults. Weprovidequalitativeexamplesofatotalof16
consectutiveframesgeneratedbyDynamicCityontheOcc3D-Waymo(Tianetal.,2023)dataset.
Bestviewedincolorsandzoomed-inforadditionaldetails.
23Preprint.
C.4 COMMAND-&TRAJECTORY-GUIDEDGENERATION
We present command conditional generation in Fig. 12 and trajectory conditional generation in
Fig.13. Theseresultsshowthatwhenweinputacommand,suchas"rightturn,"orasequenceof
XY-planecoordinates,ourmodelcaneffectivelycontrolthemotionoftheegovehicleandtherelative
motionoftheentirescenebasedonthesemovementtrends.
Turn Right Turn Right Turn Right Turn Right
T=1 T=2 T=3 T=4
Turn Right Turn Right Turn Right Turn Right
T=8 T=7 T=6 T=5
Turn Right Turn Right Turn Right Turn Right
T=9 T=10 T=11 T=12
Turn Right Turn Right Turn Right Turn Right
T=16 T=15 T=14 T=13
Build Vehicle Ped Road Sidewalk Barrier Ground Veg Other Pole
Figure 12: Command-Guided Scene Generation Results. We provide qualitative examples of
a total of 16 consectutive frames generated under the command RIGHT by DynamicCity on the
CarlaSC(Wilsonetal.,2022)dataset. Bestviewedincolorsandzoomed-inforadditionaldetails.
24Preprint.
T=1 T=2 T=3 T=4
T=8 T=7 T=6 T=5
T=9 T=10 T=11 T=12
T=16 T=15 T=14 T=13
Build Vehicle Ped Road Sidewalk Barrier Ground Veg Other Pole
Figure13: Trajectory-GuidedSceneGenerationResults. Weprovidequalitativeexamplesofa
totalof16consectutiveframesgeneratedbyDynamicCityontheCarlaSC (Wilsonetal.,2022)
dataset. Bestviewedincolorsandzoomed-inforadditionaldetails.
25Preprint.
C.5 DYNAMICINPAINTING
We present the full inpainting results in Fig. 14. The results show that our model successfully
regeneratestheinpaintedregionswhileensuringthattheareasoutsidetheinpaintedregionsremain
consistentwiththeoriginalscene. Furthermore,theinpaintedareasseamlesslyblendintotheoriginal
scene,exhibitingrealisticplacementanddynamics.
Before After Before After Before After
T=1 T=2 T=3
Before After Before After Before After
T=6 T=5 T=4
Before After Before After Before After
T=7 T=8 T=9
Before After Before After Before After
T=12 T=11 T=10
Before After Before After Before After
T=13 T=14 T=15
Build Vehicle Ped Road Sidewalk Barrier Ground Veg Other Pole
Figure14: DynamicInpaintingResults. Weprovidequalitativeexamplesofatotalof16consectu-
tiveframesgeneratedbyDynamicCityontheCarlaSC(Wilsonetal.,2022)dataset. Bestviewedin
colorsandzoomed-inforadditionaldetails.
26Preprint.
C.6 COMPARISONSWITHOCCSORA
We compare our qualitative results with OccSora (Wang et al., 2024) in Fig. 15, using a similar
scene. Itisevidentthatourresultpresentsarealisticdynamicscene,withstraightroadsandcomplete
objectsandenvironments. Incontrast,OccSora’sresultdisplaysunreasonablesemantics,suchas
a pedestrian in the middle of the road, broken vehicles, and a lack of dynamic elements. This
comparisonhighlightstheeffectivenessofourmethod.
OccSora Ours OccSora Ours OccSora Ours
T=1 T=2 T=3
OccSora Ours OccSora Ours OccSora Ours
T=6 T=5 T=4
OccSora Ours OccSora Ours OccSora Ours
T=7 T=8 T=9
OccSora Ours OccSora Ours OccSora Ours
T=12 T=11 T=10
OccSora Ours OccSora Ours OccSora Ours
T=13 T=14 T=15
Build Vehicle Ped Road Sidewalk Barrier Ground Veg Other Pole
Figure15: ComparisonsofDynamicSceneGeneration. Weprovidequalitativeexamplesofatotal
of16consectutiveframesgeneratedbyOccSora(Wangetal.,2024)andourproposedDynamicCity
frameworkontheCarlaSC(Wilsonetal.,2022)dataset. Bestviewedincolorsandzoomed-infor
additionaldetails.
27Preprint.
D POTENTIAL SOCIETAL IMPACT & LIMITATIONS
Inthissection,weelaborateonthepotentialpositiveandnegativesocietalimpactofthiswork,as
wellasthebroaderimpactandsomepotentiallimitations.
D.1 SOCIETALIMPACT
Ourapproach’sabilitytogeneratehigh-quality4DLiDARscenesholdsthepotentialtosignificantly
impactvariousdomains,particularlyautonomousdriving,robotics,urbanplanning,andsmartcity
development. Bycreatingrealistic,large-scaledynamicscenes,ourmodelcanaidindeveloping
morerobustandsafeautonomoussystems. Thesesystemscanbebettertrainedandevaluatedagainst
diversescenarios,includingrarebutcriticaledgecaseslikeunexpectedpedestrianmovementsor
complex traffic patterns, which are difficult to capture in real-world datasets. This contribution
canleadtosaferautonomousvehicles,reducingtrafficaccidents,andimprovingtrafficefficiency,
ultimatelybenefitingsocietybyenhancingtransportationsystems.
Inadditiontoautonomousdriving,DynamicCitycanbevaluablefordevelopingvirtualreality(VR)
environments and augmented reality (AR) applications, enabling more realistic 3D simulations
that could be used in various industries, including entertainment, training, and education. These
advancementscouldhelpimproveskilldevelopmentindrivingschools,emergencyresponsetraining,
andurbanplanningscenarios,fosteringasaferandmoreinformedsociety.
Despitethesepositiveoutcomes,thetechnologycouldbemisused. Theabilitytogeneraterealistic
dynamicscenesmightbeexploitedtocreatemisleadingorfakedata,potentiallyunderminingtrust
in autonomous systems or spreading misinformation about the capabilities of such technologies.
However,wedonotforeseeanydirectharmfulimpactfromtheintendeduseofthiswork,andethical
guidelinesandresponsiblepracticescanmitigatepotentialrisks.
D.2 BROADERIMPACT
Ourapproach’scontributionto4DLiDARscenegenerationstandstoadvancethefieldsofautonomous
driving,robotics,andevenurbanplanning. Byprovidingascalablesolutionforgeneratingdiverse
and dynamic LiDAR scenes, it enables researchers and engineers to develop more sophisticated
modelscapableofhandlingreal-worldcomplexity. Thishasthepotentialtoaccelerateprogressin
autonomoussystems,makingthemsafer,morereliable,andadaptabletoawiderangeofenvironments.
Forexample,researcherscanuseDynamicCitytogeneratesynthetictrainingdata,supplementing
real-worlddata,whichisoftenexpensiveandtime-consumingtocollect,especiallyindynamicand
high-riskscenarios.
The broader impact also extends to lowering entry barriers for smaller research institutions and
startupsthatmaynothaveaccesstovastamountsofreal-worldLiDARdata. Byofferingameansto
generaterealisticanddynamicscenes,DynamicCitydemocratizesaccesstohigh-qualitydatafor
trainingandvalidatingmachinelearningmodels,therebyfosteringinnovationacrosstheautonomous
drivingandroboticscommunities.
However,itiscrucialtoemphasizethatsyntheticdatashouldbeusedresponsibly. Asourmodel
generateshighlyrealisticscenes,thereisariskthatrelianceonsyntheticdatacouldleadtomodels
that fail to generalize effectively in real-world settings, especially if the generated scenes do not
capture the full diversity or rare conditions found in real environments. Hence, it’s important to
complementsyntheticdatawithreal-worlddataandensuretransparencywhenusingsyntheticdatain
modeltrainingandevaluation.
D.3 KNOWNLIMITATIONS
DespitethestrengthsofDynamicCity,severallimitationsshouldbeacknowledged. First,ourmodel’s
abilitytogenerateextremelylongsequencesisstillconstrainedbycomputationalresources,leading
to potential challenges in accurately modeling scenarios that span extensive periods. While we
employ techniques to extend temporal modeling, there may be degradation in scene quality or
consistencywhenattemptingtogeneratesequencesbeyondacertainlength,particularlyincomplex
trafficscenarios.
28Preprint.
Second,thegeneralizationcapabilityofDynamicCitydependsonthediversityandrepresentativeness
ofthetrainingdatasets. Ifthetrainingdatadoesnotcovercertainenvironmentalconditions,object
categories,ordynamicbehaviors,thegeneratedscenesmightlacktheseaspects,resultinginincom-
pleteorlessrealisticdynamicLiDARdata. Thiscouldlimitthemodel’seffectivenessinhandling
unseenorrarescenarios,whicharecriticalforvalidatingtherobustnessofautonomoussystems.
Third,whileourmodeldemonstratesstrongperformanceingeneratingdynamicscenes,itmayface
challenges in highly congested or intricate traffic environments, where multiple objects interact
closelywithrapid,unpredictablemovements. Insuchcases,DynamicCitymightstruggletocapture
thefine-graineddetailsandinteractionsaccurately,leadingtolessrealisticscenegeneration.
Lastly, the reliance on pre-defined semantic categories means that any variations or new object
types not included in the training set might be inadequately represented in the generated scenes.
Addressingtheselimitationswouldrequireintegratingmorediversetrainingdata, improvingthe
model’sadaptability,andrefiningtechniquesforlongersequencegeneration.
E PUBLIC RESOURCES USED
Inthissection,weacknowledgethepublicresourcesused,duringthecourseofthiswork.
E.1 PUBLICDATASETSUSED
• nuScenes1 ......................................................CCBY-NC-SA4.0
• nuScenes-devkit2 ...............................................ApacheLicense2.0
• WaymoOpenDataset3 ......................................WaymoDatasetLicense
• CarlaSC4 .............................................................MITLicense
• Occ3D5 ..............................................................MITLicense
E.2 PUBLICIMPLEMENTATIONSUSED
• SemCity6 ...............................................................Unknown
• OccSora7 ......................................................ApacheLicense2.0
• MinkowskiEngine8 ....................................................MITLicense
• TorchSparse9 .........................................................MITLicense
• SPVNAS10 ...........................................................MITLicense
• spconv11 .......................................................ApacheLicense2.0
1https://www.nuscenes.org/nuscenes
2https://github.com/nutonomy/nuscenes-devkit
3https://waymo.com/open
4https://umich-curly.github.io/CarlaSC.github.io.
5https://tsinghua-mars-lab.github.io/Occ3D.
6https://github.com/zoomin-lee/SemCity.
7https://github.com/wzzheng/OccSora.
8https://github.com/NVIDIA/MinkowskiEngine.
9https://github.com/mit-han-lab/torchsparse.
10https://github.com/mit-han-lab/spvnas.
11https://github.com/traveller59/spconv.
29