Estimating the Spectral Moments of the Kernel Integral Operator
from Finite Sample Matrices
Chanwoo Chun1,4 SueYeon Chung2,4 Daniel D. Lee 3,4
1Weill Cornell Medical College 2New York University
3Cornell Tech 4Flatiron Institute
Abstract offunctionsandinputpoints? Similarly,inlarge-scale
neural networks, we aim to understand the character-
istics of the neural feature representations as both the
Analyzing the structure of sampled features
number of features and input points grow infinitely
from an input data distribution is challeng-
large (Cho and Saul, 2009; Mei et al., 2018; Chung
ing when constrained by limited measure-
et al., 2018; Cohen et al., 2020; Canatar et al., 2021,
ments in both the number of inputs and fea-
2024). Thecentralquestionofthisworkishowtoesti-
tures. Traditional approaches often rely on
mate the spectral properties of the underlying infinite
theeigenvaluespectrumofthesamplecovari-
process when both features and evaluation points are
ancematrixderivedfromfinitemeasurement
finitely sampled.
matrices;however,thesespectraaresensitive
to the size of the measurement matrix, lead- A matrix can be constructed from sampled measure-
ing to biased insights. In this paper, we in- ments of the process, where each row corresponds to
troduceanovelalgorithmthatprovidesunbi- an individual input sample and each column is a sam-
asedestimatesofthespectralmomentsofthe pled feature. It is common practice to analyze the
kernel integral operator in the limit of infi- eigenvalue spectrum of the sample covariance matrix
niteinputsandfeaturesfromfinitelysampled derived from this finite measurement matrix. How-
measurement matrices. Our method, based ever, this spectrum is biased, leading to inaccurate in-
ondynamicprogramming,isefficientandca- sights into the underlying structure. Therefore, prior
pable of estimating the moments of the op- work aims to correct this bias under the assumption
erator spectrum. We demonstrate the accu- that the rows are sampled (Kong and Valiant, 2017).
racyofourestimatoronradialbasisfunction However, in the setup where both rows and columns
(RBF) kernels, highlighting its consistency are sampled, this method produces biased estimates.
with the theoretical spectra. Furthermore,
Inourmodel,weconsiderthemeasurementmatrixasa
we showcase the practical utility and robust-
sampledsubmatrixofalargerunderlyingmatrixwhere
ness of our method in understanding the ge-
both the number of rows and columns approach infin-
ometry of learned representations in neural
ity. A kernel integral operator can be defined as the
networks.
expectedcovarianceofthislargermatrixandwestudy
how to accurately infer the spectral properties of this
operator, in particular its spectral moments. We pro-
1 Introduction
poseanovel,computationallyefficientalgorithmbased
upon dynamic programming, to estimate the spectral
Aprimaryobjectiveofstatisticalinferenceinmachine moments of the underlying kernel operator from a fi-
learning is to accurately estimate the characteristics nite measurement matrix.
ofahigh-dimensionaldistributionbasedonfinitesam-
In the following, we first describe a mathematical
ples. For example, consider a Gaussian process with
frameworkrelatingafinitemeasurementmatrixtothe
an unknown covariance where only a limited set of
spectrumofakernelintegraloperator,whosemoments
sample functions is drawn from the process. In many
are equivalent to that of the kernel covariance opera-
cases,wecannotobservethefunctionsthemselves,but
tor in the reproducing kernel Hilbert space (RKHS)
rather their noisy evaluations at sampled input points
(Bach, 2017). We show how the naive estimators of
(Williams and Rasmussen, 2006). What can we infer
thespectralmomentsbaseduponthefinitecovariance
about the underlying process from such a sampled set
1
4202
tcO
42
]GL.sc[
2v89971.0142:viXramatrix are biased. Then we introduce our method for self-adjoint linear operator, whose spectrum consists
estimating the spectral moments by averaging appro- of a countable, non-increasing sequence of eigenvalues
priateproductsofnon-repeatingcyclesinthemeasure- {λ ≥0}∞ . Thecorrespondingeigenfunctionsarede-
l l=1
ment matrix. Our method employs a recursive proce- fined implicitly as λ e = T e where e ∈ L2(X,ρ )
l l k l l X
dure that is computationally efficient, polynomial in and{e }∞ formsanorthonormalset. Sincetheprod-
l l=1
the size of the matrix and the order of the moments. uct of a bounded operator and trace class operator is
We demonstrate the accuracy of our method with the also trace class, trTn is well-defined for any positive
k
radial basis function (RBF) kernel operator, where a integer n. We define the spectral n-th moment m(n)
direct comparison to the theoretical spectrum and to as the sum over the n-th powers of the eigenvalues:
otherestimationmethodsispossible. Finally,weshow
∞
X
how our estimates can be used toanalyze the learning m(n):= λn ≡trTn. (4)
l k
dynamics of a rectified linear unit (ReLU) neural net-
l=1
work during feature learning, showing how networks
The moments can also be written as the expectation
ofdifferentwidthscanberelatedbytheirkerneloper-
over a product of kernel functions,
ators.
Z n n
Y Y
m(n)= dρ (x ) k(x ,x ) (5)
X j j j+1
2 Kernel operator
j=1 j=1
with the constraint that x = x . The moments
2.1 Kernel as expectation {m(n)}∞ uniquelydetermn i+ ne1 thesp1 ectrumofT via
n=1 k
theStieltjestransform(seeAppendix). Variousmeth-
We model the entries of a P ×Q measurement matrix
odstoestimatethespectralmomentsm(n)ofT from
[Φ ] as arising from the following stochastic process. k
iα
a finite measurement matrix [Φ ] are investigated in
Each row i∈{1,...,P} is characterized by a latent in- iα
this work.
put variable x drawn independently from a probabil-
i
itymeasureρ (x)overalatentspaceX,andeachcol-
X
umnα∈{1,...,Q}ischaracterizedbyalatentvariable 2.3 Kernel covariance operator
w drawn independently from a probability measure
α There exists another operator with the same spectral
ρ (w) over a latent space W. In random feature net-
W moments as T (Bach, 2022). The kernel function can
works,forinstance,x andw canbeseenasaninput k
i α be described as an inner product with a feature map
pattern and neural weights respectively. The (iα)-th
ψ :X →F whereF isaRKHSofreal-valued, square-
entry of the matrix [Φ ] is produced by a function ϕ
iα integrable functions f : W → R and ψ(x) ≡ ϕ(x,·) ≡
that maps the pair (x ,w ) to a real number:
i α f(·). F ⊂ L2(W,ρ ) is the completion of the linear
W
Φ =ϕ(x ,w ). (1) spanofthefunctions{ψ(x)} anditsinnerproduct
iα i α is defined by ⟨f,f′⟩ = R dρx∈X (w)f(w)f′(w) so that
A kernel function k :X ×X →R over input pairs can k(x,x′) = ⟨ψ(x),ψ(F x′)⟩ . NW ow consider the covari-
F
then be defined as the expected value of the product ance operator T :F →F,
c
of ϕ over the features (Bach, 2017): Z
T := dρ (x) |ψ(x)⟩⟨ψ(x)| (6)
Z c X
k(x,x′)= dρ (w)ϕ(x,w)ϕ(x′,w). (2)
W
defined here with Dirac bra-ket notation. T in the
c
RKHS has the same spectral moments as the integral
We assume the function ϕ is square-integrable with
operator T , so m(n)=trTn ≡trTn:
respect to both ρ X and ρ W, so the kernel function is k c k
positive-definite and bounded. The tuple (ϕ,ρ X,ρ W) Z Yn Yn
uniquely defines a generative process for the measure- trT cn = dρ X(x j) ⟨ψ(x j)|ψ(x j+1)⟩ (7)
ment matrices and corresponding kernel. j=1 j=1
where x = x . Additional details about the con-
n+1 1
2.2 Kernel integral operator nection between the two operators and a tight frame
operator in the RKHS are described in the Appendix.
Now consider the integral operator T : L2(X,ρ ) →
k X
L2(X,ρ ) (Cucker and Smale, 2002):
X 2.4 Naive estimator
Z
T kf := dρ X(x)k(·,x)f(x). (3) Although the rows and columns of the measurement
matrixΦaresampledindependently,thematrixcoeffi-
Since ϕ is square-integrable, T is a trace class op- cientswillbecorrelatedduetosimilaritiesinthesam-
k
erator. Therefore T is a compact, bounded, and pled inputs and features. The conventional approach
k
2to analyze the spectral structure of the measurement 3 Related work
matrix is to form the Gram matrix K ∈ RP×P that
represents the similarity between input rows:
3.1 Random matrix theory
Random matrix theory analyzes the spectral charac-
Q Q
1 X 1 X
K = ϕ(x ,w )ϕ(x ,w )= Φ Φ . teristics of ensembles of Wishart matrices. The ba-
ij Q i α j α Q iα jα
sic theory considers a Wishart matrix formed by tak-
α=1 α=1
(8) ing the covariance of a large P × Q random matrix
ThematrixKispositivesemi-definiteanditsmoments [Φ ], whose entries are independently sampled from
iα
are given by trKn. If the similarity between different the standard normal distribution, i.e. Φ ∼ N(0,1).
iα
inputs x i and x j is O(1), then we expect the trace The spectrum of the Wishart matrix converges to a
trKn to scale as O(Pn). We normalize the traces by well-defined limit as P,Q → ∞ when the ratio P is
Q
this scaling factor to give the naive estimator mˆ 0(n): fixed. This limiting spectral distribution differs from
that of an identity covariance matrix and is known
astheMarchenko-Pasturdistribution(Marchenkoand
(cid:20)(cid:18) K(cid:19)n(cid:21)
mˆ (n)=tr . (9) Pastur, 1967).
0 P
Within our framework, a measurement matrix with
independent and identically distributed (i.i.d.) nor-
In the limit of large P and Q, the naive estimates mal entries is generated by taking x,w ∈ Rd with
will converge to the moments of the kernel integral ρ X = N (0,I d×d), ρ W = N (cid:0) 0, d1I d×d(cid:1) and bilinear
operator, mˆ (n) → m(n). For n = 1, mˆ (1) is the map ϕ(x,w)=x⊤w. When d approaches infinity and
0 0
samplevarianceofϕ(x,w)andisanunbiasedestimate is much larger than P and Q, each element Φ iα be-
of m(1). However, for n > 1 and finite P and Q, comes an independent standard normal random vari-
mˆ (n) is a biased estimate of m(n). To understand able.
0
why mˆ (n) is biased, consider the expected value of
0 In this case, the moments of the kernel integral oper-
the second moment estimate: ator are m(n) = d−(n−1). For large d ≫ P,Q, the
spectrum of the sample Gram matrix and the cor-
responding naive spectral moment estimates will be
P Q
⟨mˆ (2)⟩ = 1 1 XX ⟨Φ Φ Φ Φ ⟩ dominated by bias terms, and the leading order fully-
0 Φ P2Q2 iα jα jβ iβ
connectedbiastermsarethesametermsthatgiverise
i,j α,β
totheMarchenko-Pasturdistribution. Inthenextsec-
P Q
1 XX tion, we will see how to better estimate the spectral
= ⟨Φ Φ Φ Φ ⟩
P2Q2 iα jα jβ iβ moments from measurement matrices by eliminating
i̸=jα̸=β
the bias terms.
P Q P Q
+ 1 XX(cid:10) Φ2 Φ2 (cid:11) + 1 XX(cid:10) Φ2 Φ2 (cid:11)
P2Q2 iα iβ P2Q2 iα jα
i α̸=β i̸=j α 3.2 Estimator for fully observed features
P Q
+ 1 XX(cid:10) Φ4 (cid:11) . (10) Kong and Valiant (2017) consider the problem where
P2Q2 iα
i α the inputs are sampled from an underlying distribu-
tionbutthefeaturesarefullyobservedwithfinitecar-
dinality d, e.g. w ∈ {w ,w ,...,w }. In their sce-
The second term in this expansion contains the con- α 1 2 d
D E nario, the measurements can be modeled as a P ×d
nected product Φ2 Φ2 whose expected value is
iα iβ matrix Φ¯ ∈ RP×d; other work in the spectrum esti-
R dρ X(x)k(x,x)2,anddiffersfromthesecondmoment mation literature also considers similar problem setup
of the kernel integral operator. This term is order (Ledoit and Wolf, 2004; Burda et al., 2004; Karoui,
O(cid:0)1(cid:1)
and introduces a finite sampling bias into the 2008; Khorunzhiy, 2008; Bhattacharjee et al., 2024).
P
estimate mˆ 0(2). Similarly, the third and fourth terms Kong and Valiant (2017) models the observed mea-
in the expansion will give rise to bias terms of order surementmatrixasamatrixsketchingprocess: Φ¯ =
(cid:16) (cid:17) (cid:16) (cid:17) iα
O 1 and O 1 respectively. Pd x F where the coefficients x are indepen-
Q PQ k=1 ik kα ik
dentlysampledfromthestandardnormaldistribution,
This analysis generalizes to all higher moments n > andF ∈Rd×disadeterministicmatrix. Theirmethod
1. The naive estimate mˆ 0(n) derived from the seekstoestimatethespectralmomentsofS := 1FF⊤,
sample Gram matrix contains biased terms of order
i.e. m
(n)=tr(Sn)inordertoobtainthesped
ctrum
(cid:16) (cid:17) KV
O 1 + 1 . of S.
P Q
3Note that the naive estimator based upon the Gram where the indices i ∈ {1,...,P} are disjoint, i ̸=
l l
matrix K¯ := 1Φ¯Φ¯⊤ with spectral moments mˆ (n) = i for all l ̸= k, except for the trace constraint i =
d 0 k 1
tr(Kˆn) is biased. Instead, the following simple unbi- i n+1. Similarly, the feature indices α l ∈ {1,...,Q}
ased estimator is proposed: should also be disjoint, α l ̸= α k for all l ̸= k. Since
there is no overlap in the indices, the expected value
mˆ′
(n)=Yn
K¯ (11)
of mˆ′(n) is the product of the expected values of the
KV ilil+1 kernel functions:
l=1
* n + n
where the product indices i l ∈{1,...,P} are disjoint, Y Φ ilαlΦ il+1αl =Y(cid:10) Φ ilαlΦ il+1αl(cid:11) Φ (14)
i ̸= i for all l ̸= k, except for the trace constraint
l k l=1 Φ l=1
i u1 nb= iasi en d+1 r. eliT eshe onpr to ho ef at sh sa ut mt ph ti is onsim thp al te Φe ¯stim isat zo er rois
-
=Yn
⟨k(x ,x )⟩ . (15)
iα i i+1 xi,xi+1
mean,whichisnotarequirementforourmodel. With i=1
only a single realization of {i }n , mˆ′ (n), the vari-
l l=1 KV
ance of the estimate is high. It would be optimal to This is equivalent to the definition of m(n), showing
average over all possible realizations of {i }n , but that mˆ′(n) is an unbiased estimator of m(n).
l l=1
there is no known computationally efficient algorithm
As shown in Figure 1, one can view the product in
to perform the sum for large n. Thus, the authors mˆ′(n) as forming a cyclic path over the coefficients of
propose averaging over sets of increasing indices, i.e.
matrixΦbyfirstchoosingastartingcoefficientΦ ,
i <i <...<i . Thisleadstotheirestimatorwhich
i1,α1
1 2 n multiplying it with a distinct coefficient Φ on the
considers the trace of the following matrix product:
i2,α1
same column, then multiplying with another coeffi-
cient Φ on the same row as the previous one, and
tr(cid:0) K¯n−1K¯(cid:1) i2,α2
mˆ (n)= up . (12) so on until returning to the starting coefficient, creat-
KV (cid:0)P(cid:1)
ing a product with a total of 2n distinct coefficients.
n
K¯ is the upper triangular matrix formed from K¯, Inordertoreducethevarianceofthisestimate,wecan
up
consider averaging
Qn
Φ Φ over all possible
e.g. the diagonal and lower triangular entries are set cyclicpaths{i }n anl d=1 {αil }α nl i ol+ f1 nα ol n-overlappingin-
to zero. l l=1 l l=1
dices:
The estimator mˆ (n) can be used with the measure-
KV
mentmatrixΦinourproblemsetupintwoways. One (P −n)!(Q−n)!
issettingΦ¯ ←ΦwhichisequivalenttotreatingΦasif
mˆ∗(n)=
P! Q!
all features are observed. The other is to set Φ¯ ←Φ⊤, n
X X Y
which is equivalent to the assumption that all the in- Φ Φ , (16)
ilαl il+1αl
puts are observed but the features are sampled. We i1̸=...̸=inα1̸=...̸=αnl=1
refer to the former estimator as mˆ (n) and the
KV-row
latter as mˆ (n). When both inputs and features with the constraint that the final input index is the
KV-col
are not fully observed, we will see that both of these same as the initial input index i n+1 =i 1.
approaches result in biased estimates.
This sum can be performed for small moments n. For
example, with n=2, we have the straightforward ex-
4 Unbiased estimation of spectral pression:
moments
XP K2 XQ K˜2 XP XQ Φ4
cmˆ∗(2)=mˆ (2)− ii − αα + iα
4.1 Unbiased estimator 0 P2 Q2 P2Q2
i=1 α=1 i=1α=1
(17)
In the naive estimation of the second moment, mˆ (2),
0 where c := (P−1)(Q−1), K = 1 PQ Φ Φ and
we saw that certain terms in the sum were biased. PQ ij Q α=1 iα jα
On the other hand, the first term of (10) consisting K˜ αβ = P1 PP i=1Φ iαΦ iβ.
of products of matrix coefficients with disjoint indices
Unfortunately, summing over all disjoint index sets is
is unbiased. This observation can be generalized to
computationally inefficient for larger n, with appar-
higher spectral moments of the kernel integral oper-
ent complexity O(PnQn). Instead, similar to the ap-
ator. An elementary unbiased estimator for m(n) is
proach by Kong and Valiant (2017), we can average
given by
over cyclic paths where both sets of indices are in-
n
mˆ′(n)=Y
Φ Φ (13)
creasing, i.e. 1 ≤ i
1
< i
2
< ... < i
n
≤ P and
ilαl il+1αl
1≤α <α <...<α ≤Q (see Figure 1b):
l=1 1 2 n
4a. b. c.
Figure1: Visualillustrationofthecalculationoftheunbiasedestimator. a. Forcomputingmˆ∗(3),onecanselect
matrix entries such that the entries create a cyclic path of 6 turns without revisiting rows and columns more
than twice, and average over all possible such paths. b. Our method limits the cyclic paths to only increasing
indices. c. Example paths for mˆ(2),...,mˆ(6).
Algorithm 1Computationofmˆ(n)forn=2ton of the partial sums:
max
Require: Φ∈RP×Q, n
max P−n+1 Q P
1: for h←1 to P do mˆ(n)= 1 X X X S(h)[n]Φ . (19)
2: Initialize S as a P ×Q zero matrix.
(cid:0)P(cid:1)(cid:0)Q(cid:1) ij hj
n n h=1 j=ni=h+n−1
3: Set S ←PΦ ∀i∈[1,Q]
hi hi
4: for n←2 to n do where the sums are defined as:
max
5: Update
n2Pa−1 Pb−1
S Φ Φ
S(h)[n]= X
S ← l=h+n−2 k=n−1 lk ak ab ab
ab (P −n+1)(Q−n+1) (h=i1)<i2<...<in−1<(a=in)
∀a∈[h+n−1,P], ∀b∈[n,Q]. n−1 !
X Y
6: Compute Φ ilαlΦ il+1αl Φ inαn
1
mˆ(h)(n)← PP PQ S Φ . 1≤α1<α2<...<αn−1<(b=αn) l=1
PQ i=h+n−1 j=n ij hj (20)
7: end for
8: end for The indices h, a, b in S(h) correspond to the indices
P−n+1 ab
9: Get mˆ(n)← 1 X mˆ(h)(n) ∀n∈[2,n ]. i 1, i n, and α n in the estimator expression (18). Next,
P max we note that S(h) can be written recursively:
h=1 ab
b−1 a−1
S(h)[n+1]= X X S(h)[n]Φ Φ . (21)
ab lk ak ab
k=nl=h+n−1
In this manner, we can iteratively compute S(h)[n]→
1
mˆ(n)= S(h)[n+1]toobtainallthepartialsumsforthespectral
(cid:0)P(cid:1)(cid:0)Q(cid:1)
n n moment estimation.
n
X X Y
Φ Φ Thiscomputationisalsomemoryefficient. Thepartial
1≤i1<i2<...<in≤P1≤α1<α2<...<αn≤Ql=1
ilαl il+1αl
sums S l( kh)[n] can be stored as the (lk)-th element of a
h i
(18) P ×Q matrix S(h)[n] and the computation can be
lk
performed in place. The algorithm first initializes the
where i n+1 = i 1, and the combinatorial product matrixS(h)[1]bysettingitsh-throwtomatchtheh-th
(cid:0)P(cid:1)(cid:0)Q(cid:1)
is the total number of terms in the sum. rowofΦ, withtherestoftheelementsinitializedto0.
n n
Graphically, the paths over increasing index sets cre- ThenS(h)[2]iscomputedvia(21)toobtainmˆ(2)with
ate stair-like cyclic paths in the matrix Φ as shown in (19). This procedure can be repeated to get mˆ(n) for
Figure 1c. allnrangingfrom2tothedesiredn . Theestimate
max
form(1)isthesameasthenaiveestimatemˆ (1)which
0
isunbiased. Pseudo-codeofourrecursivealgorithmto
4.2 Dynamic programming algorithm
computethespectralmomentestimatesisprovidedin
Algorithm 1.
Here we show how to efficiently compute mˆ(n) for
highermomentsviaarecursivedynamicprogramming The computational complexity of our algorithm is
algorithm. Theestimatemˆ(n)canbewritteninterms O(nP2Q), or O(nPQ2) if the algorithm is performed
5on the matrix transpose Φ⊤. In practice, S(h)[n] is defined by weights w ∈Rd and phase shift b∈R as:
should also be normalized at each of step of the re- √
cursiontopreventanyoverflowinthecalculation;this ϕ(x,(w,b))=
2sin(cid:0) w⊤x+b(cid:1)
(22)
normalization is included in the description of Algo-
rithm1. Additionally, step5inAlgorithm1canread- w ∼N
(cid:0) 0,Σ−1(cid:1)
b∼U(0,2π) (23)
ily be vectorized with a simple modification to the cu-
so that w is a random weight vector sampled from a
mulative summation subroutine. To further improve
multivariate normal distribution N
(cid:0) 0,Σ−1(cid:1)
, and b is
the accuracy of the estimates, the rows and columns
a random phase shift sampled from the uniform dis-
of Φ can first be permuted and the same algorithm
tribution U. The similarity between two inputs x and
can be performed to compute additional cyclic path
y is given by taking the expectation over all possible
sums to reduce the variance of the spectral moment
features and is equivalent to the RBF kernel:
estimates.
k(x,y)=e−1 2(x−y)⊤Σ−1(x−y). (24)
4.3 Noisy measurements
In the following, we also assume that the inputs
Ourestimatormˆ(n)isunbiasedeveninthepresenceof x and y are sampled from an input distribution
independent noise, when the noise is injected into the ρ (x) described by the multivariate normal distribu-
X
generative process as Φ iα =ϕ(x i,w α)+γ(x i,w α,ϵ iα), tion N(0,Σ x).
assuming: ϵ is sampled independently from some
iα
probability measure ρ across all (i,α); ⟨γ(x,w,ϵ)⟩ =
ϵ ϵ 5.2 Spectrum of the RBF kernel
0; and ⟨γ(x,w,ϵ)2⟩ <∞.
ϵ
The spectrum of the RBF kernel operator has been
Wecanalsohandlethecasewhenthenoiseϵ iscorre-
iα
described before for d=1, or when the kernel and in-
latedacrossroworcolumnentriesof[Φ ]withasim-
iα
putcovariancesareisotropic,orcanbesimultaneously
ple modification to our estimator. Multiple measure-
factorized in Zhu et al. (1997); Williams and Seeger
mentscanbetakenoverthesamesetofinputsandfea-
tures, {x }P and {w }Q , and can be used to form (2000);WilliamsandRasmussen(2006);Canataretal.
i i=1 α α=1 (2021). The spectrum of the general kernel integral
separate trial measurements of the matrix Φ(t) with t
operator for arbitrary kernel and input covariances is
indexing different trials with independent noise across
described in terms of the d×d positive-definite ma-
trials. With measurement samples from only two tri-
trix Σ Σ−1. Let {η }d be the eigenvalues of Σ Σ−1
als Φ(1) and Φ(2), unbiased estimates of the moments x i i=1 x
can be obtained by alternating trial measurements in and let u := {u i}d i=1 be a multiset of d natural num-
the product terms: mˆ′ (n)=Qn Φ(1) Φ(2) . This bers. Then the following is an eigenvalue of the kernel
alt l=1 ilαl il+1αl integral operator for all u:
procedure can be easily extended with additional tri-
als to further denoise the spectral moment estimates,
d
and the corresponding modifications to our dynamic
λ
=Y(cid:0) η1+uiφ1+2ui(cid:1)−1
(25)
programming algorithm are straightforward (see Ap-
u i ηi
i=1
pendix for more details).
√
where the scalar function φ = 1+ 1+4z. The largest
z 2z
5 RBF kernel operator kernel operator eigenvalue, e.g. the spectral norm of
the operator, is obtained when u = 0 for all i. The
i
corresponding eigenfunctions are given by products of
In this section, we describe the spectrum of the kernel
generalized Hermite polynomials with degree u and
integral operator for the radial basis function (RBF) i
a common multivariate Gaussian function (see Ap-
kernelfunctionwithmultivariateGaussianinputs. We
pendix).
then confirm that our method estimates the correct
spectral moments and compare its performance to The n-th spectral moments can then be computed as
other methods.
d
Y 1
m(n)= . (26)
5.1 Random Fourier features ηnφn −φ−n
i=1 i ηi ηi
RahimiandRecht(2007)andRudiandRosasco(2017)
Note that m(1) = 1 regardless of the choice of kernel
describe the following process using random Fourier
parameters.
features. Consider a two-layer neural network with an
input layer and a feature layer with a sinusoidal non- A particularly interesting case is when Σ = Σ so
x
linearity. Given an input pattern x ∈Rd in the input that η = 1 for all i. In this case, the kernel opera-
i
layer,thevalueofarandomfeatureinthefeaturelayer tor eigenvalues are powers of the golden ratio φ , i.e.
1
6a. c.
mˆ(n) MSE mˆ(2) mˆ(3) mˆ(4)
10−2
10−3
10−4
10−7
10−11 10−2
10−4 10−6
10−12 10−20 10−3 10−5 10−8
2 3 4 5 6 7 2 3 4 5 6 7 102 103 102 103 102 103 Ours
n n Q Q Q Naive
KVrow
b. mˆ(n)1/n MSE d. mˆ(2) mˆ(3) mˆ(4) KVcol
0.075
10−3
10−3
10−5
GT
10−2
0.050 10−4
0.025 10−5 10−3
10−5
10−7
2 3 4 5 6 7 2 3 4 5 6 7 102 103 102 103 102 103
n n P P P
Figure 2: Estimated RBF moments for d=5, Σ =I , Σ=0.25I . Our estimator mˆ is labeled as “Ours”,
x d×d d×d
the two versions of Kong and Valiant estimators mˆ and mˆ are labeled as “KV-row” and “KV-col”
KV-row KV-col
respectively, the naive estimator mˆ is labeled as “naive”, and the analytic ground truth moments m are labeled
0
as “GT”. a. P = 300 and Q = 600. Left: The mˆ(n) values for various estimators, with n ranging from 2 to 7.
Right: The MSE between mˆ(n) and m(n). b. The same as a., but for mˆ1/n(n). The black arrow indicates the
value of the operator norm. c. P is fixed to 300, and Q is varied. d. Q is fixed to 600, and P is varied. Bars
indicate a 50% confidence interval.
λ = φ−t for t ∈ {1,2,...}, where the t-th eigenvalue is unbiased, the n-th root of the estimate, mˆ(n)1/n,
hat s mu1 ltiplicity (cid:0)t+d−1(cid:1) . The moments are given by will be a biased estimate of m(n)1/n. Nevertheless,
t
(cid:16) (cid:17)d we observe that our estimates of the rooted moment
m(n) = 1 , and decrease exponentially for
ϕn−ϕ−n achieve remarkably small MSE (< 10−5, Figure 2b
1 1
n≥2 to zero for larger d. right). We also observe that our rooted moment es-
timate accurately recovers the operator norm (Figure
2b left, compare mˆ(7) with the black arrow).
5.3 Numerical estimation results
Our estimator can be compared to the other estima-
We consider P ×Q measurement matrices [Φ ] tak- tors as both P and Q are varied. If we fix P and vary
iα
ing P samples of inputs patterns {x i}P
i=1
and Q sam- Q, mˆ 0(n) and mˆ KV-col(n) are biased even in the limit
ples of weights and phase shifts {(w ,b )}Q . The of large Q, due to the small sampling of P (Figure
α α α=1
2c). Ontheotherhand, ourestimatorandmˆ (n)
naive estimator mˆ (n), Kong and Valiant’s estimators KV-col
0
asymptotically converge to the true moments at large
mˆ (n) and mˆ (n), and our method, mˆ(n),
KV-row KV-col
Q. Similarly, varying P for fixed Q (Figure 2) shows
are all applied to estimate the spectral moments of
that only our estimator and mˆ (n) asymptoti-
the operator and compared with the analytical for- KV-row
callyconvergestothetruemoments. Ourestimatoris
mula in (26). As shown in Figure 2a, there is ex-
the only unbiased estimator across all finite P and Q.
cellent agreement between our estimates (red dotted
lines) and the true moments (blue solid lines). Our
estimator achieves the smallest mean-squared error, 6 ReLU kernel moments during
showing that it is both unbiased and has low vari-
feature learning
ance (see Appendix for detailed bias-variance analysis
ofthevariousestimatorsalongwiththeirperformance
Here we show how our estimator can be used to ana-
in the presence of independent and correlated noise).
lyze the neural representation of varying widths in a
The difference between the estimators is more pro- neural network during feature learning. Specifically,
nounced when comparing the n-th root of the esti- a single-hidden layer neural network with ReLU ac-
matedn-thmoment,i.e. m(n)1/n. m(2)1/2isthestan- tivation, trained on the Fashion-MNIST dataset with
dard deviation of the spectrum, and lim m(n)1/n Adam-SGD (Xiao et al., 2017; Kingma, 2014) is con-
n→∞
gives the operator norm, the largest eigenvalue of the sidered. The maximal-update parameterization (µP)
kernel integral operator. Note that even though mˆ(n) proposedbyYangetal.(2022)isutilizedtoenableeffi-
7mˆ(2) mˆ(3) mˆ(4) mˆ(5)
0.10
0.04
0.2 0.1
0.05
0.02
0.0 0.0 0.00 0.00
0 100 0 100 0 100 0 100
Width
0.10
32
0.04 64
0.2 0.1
0.05 128
0.02 256
512
0.0 0.0 0.00 0.00 1024
0 100 0 100 0 100 0 100
Epoch Epoch Epoch Epoch
Figure 3: The estimated spectral moments during training of single hidden layer ReLU neural networks. Top
row: networks of different widths have dramatically different naive estimates mˆ (n) of the operator moments.
0
Bottom row: estimates using the unbiased estimator mˆ(n) is similar across all widths. Results were obtained
from networks trained from 29 random initializations. Shades indicate a 50% confidence interval.
cientfeaturelearning. Asthewidthbecomeslarge,the alternative, we propose an unbiased method for esti-
trainednetworkapproachesthemean-fieldlimitwhere matingthespectralmomentsofthekernelintegralop-
each feature becomes an i.i.d. random variable in the eratorfromfinitemeasurementmatrices. Ourmethod
learnedweightdistribution, andconsequentlyremains iscomputationallyefficientandresultsinaccuratemo-
exchangeable after training (Mei et al., 2018; Yang ment estimates, as demonstrated in numerical experi-
et al., 2022; Vyas et al., 2024; Bordelon and Pehlevan, ments with the RBF kernel where analytic results for
2022; Seroussi et al., 2023; Yang et al., 2023). the true operator spectrum are known.
ThemeasurementmatrixΦisconstructedfromanet- Our estimator can be used to gain geometrical insight
workwithQneuronsinthehiddenlayer,andeachrow into measurement matrices of varying sizes. For ex-
ofΦrepresentstheQ-dimensionalhiddenlayeractiva- ample, we can accurately estimate the effective di-
tions from one of P input images. Φ is normalized so mension of the kernel operator T . By consider-
k
h i
that it can be compared across different matrix sizes ing tr T (T +λI)−1 whose Taylor series include
k k
by dividing by the standard deviation of its entries.
weighted sums of the spectral moments (Caponnetto
The spectral moments of the hidden layer representa-
and De Vito, 2007; Bach, 2013), a soft count of the
tionsfornetworkswithwidthsrangingfrom32to1024
number of eigenvalues above a threshold λ can be ob-
are shown in Figure 3. The naive spectral moment es-
tained. This quantity has been widely used to study
timates diverge across the different network widths;
prediction performance in kernel ridge regression.
in contrast, our estimator produces consistent spec-
tral moments across the entire range of widths. This The spectral moment estimates can also be employed
indicates that predictions of learning dynamics from in conjunction with methods to approximate kernels
featurelearningtheoriescanbeappliedtounderstand with sampled features to reduce computational com-
the behavior of neural networks across a wide range plexity (Rahimi and Recht, 2007; Rudi and Rosasco,
of sizes. We defer exploration of how feature learn- 2017; Rudi et al., 2024). For example, kernel approxi-
ing theories such as mean-field theory can be used to mationhasrecentlybeenusedinstate-of-the-arttrans-
model our spectral moment estimates to future work. formermodelsbyreplacingthesoftmaxattentionwith
a kernel (Peng et al., 2021). The decay of the kernel
integral operator spectrum can be used to gauge the
7 Discussion
accuracy of the kernel approximation.
Weshowedhowourestimatorcanbeusedtoquantify
We have shown that conventional methods for ana-
thelearningdynamicsofneuralnetworksduringtrain-
lyzing the spectrum of a measurement matrix with
ing. ThekerneloperatorcanberelatedtotheHessian
finitely sampled inputs and features are biased. As an
8
eviaN
sruOof a quadratic objective function (Dieuleveut et al., Cho, Y. and Saul, L. (2009). Kernel methods for deep
2017; Pedregosa and Scieur, 2020; Sagun et al., 2017; learning. Advances in neural information processing
Martin and Mahoney, 2021) which is valuable for un- systems, 22.
derstanding gradient-based learning dynamics. Other
Chung, S., Lee, D. D., and Sompolinsky, H. (2018).
recentworkproposesspectralestimatestounderstand
Classification and geometry of general perceptual
convergence rates (Dieuleveut et al., 2017) as well as
manifolds. Physical Review X, 8(3):031003.
accelerating the optimization (Pedregosa and Scieur,
2020). Thus, there are many potential avenues for fu- Cohen, U., Chung, S., Lee, D. D., and Sompolinsky,
tureexplorationwhereunbiasedandefficientestimates H.(2020). Separabilityandgeometryofobjectman-
ofthespectralcharacteristicsofthekernelintegralop- ifolds in deep neural networks. Nature communica-
erator will be valuable. tions, 11(1):746.
Cucker,F.andSmale,S.(2002). Onthemathematical
Acknowledgements foundations of learning. Bulletin of the American
mathematical society, 39(1):1–49.
WethankJonathanD.VictorandAbdulkadirCanatar
for valuable feedback on the project. Dieuleveut, A., Flammarion, N., and Bach, F. (2017).
Harder,better,faster,strongerconvergenceratesfor
least-squares regression. Journal of Machine Learn-
References
ing Research, 18(101):1–51.
Bach, F. (2013). Sharp analysis of low-rank kernel
Janson, S. (2018). Renewal theory for asymmet-
matrix approximations. In Conference on learning
ric U-statistics. Electronic Journal of Probability,
theory, pages 185–209. PMLR.
23(none):1 – 27.
Bach, F. (2017). On the equivalence between kernel
Karoui, N. E. (2008). Spectrum estimation for large
quadrature rules and random feature expansions.
dimensional covariance matrices using random ma-
Journal of machine learning research, 18(21):1–38.
trix theory. The Annals of Statistics, 36(6):2757 –
Bach,F.(2022). Informationtheorywithkernelmeth- 2790.
ods. IEEE Transactions on Information Theory,
Khorunzhiy,O.(2008). Estimatesformomentsofran-
69(2):752–775.
dom matrices with gaussian elements. S´eminaire de
Bhattacharjee, R., Dexter, G., Drineas, P., Musco, probabilit´es XLI, pages 51–92.
C., and Ray, A. (2024). Sublinear time eigenvalue
Kingma,D.P.(2014). Adam: Amethodforstochastic
approximation via random sampling. Algorithmica,
optimization. arXiv preprint arXiv:1412.6980.
pages 1–66.
Kong,W.andValiant,G.(2017).Spectrumestimation
Bordelon, B. and Pehlevan, C. (2022). Self-consistent
from samples. The Annals of Statistics, 45(5).
dynamical field theory of kernel evolution in wide
neural networks. Advances in Neural Information Korolyuk, V. S. and Borovskich, Y. V. (2013). The-
Processing Systems, 35:32240–32256. ory of U-statistics, volume 273. Springer Science &
Business Media.
Burda, Z., G¨orlich, A., Jarosz, A., and Jurkiewicz,
J. (2004). Signal and noise in correlation matrix. Ledoit, O. and Wolf, M. (2004). A well-conditioned
Physica A: Statistical Mechanics and its Applica- estimatorforlarge-dimensionalcovariancematrices.
tions, 343:295–310. Journal of multivariate analysis, 88(2):365–411.
Canatar, A., Bordelon, B., and Pehlevan, C. (2021). Marchenko, V. A. and Pastur, L. A. (1967). Distribu-
Spectral bias and task-model alignment explain tionofeigenvaluesforsomesetsofrandommatrices.
generalization in kernel regression and infinitely Matematicheskii Sbornik, 114(4):507–536.
wide neural networks. Nature communications,
Martin, C. H. and Mahoney, M. W. (2021). Implicit
12(1):2914.
self-regularization in deep neural networks: Evi-
Canatar, A., Feather, J., Wakhloo, A., and Chung, S. dence from random matrix theory and implications
(2024). A spectral theory of neural prediction and forlearning. JournalofMachineLearningResearch,
alignment.AdvancesinNeuralInformationProcess- 22(165):1–73.
ing Systems, 36.
Mei, S., Montanari, A., and Nguyen, P.-M. (2018). A
Caponnetto,A.andDeVito,E.(2007). Optimalrates meanfieldviewofthelandscapeoftwo-layerneural
fortheregularizedleast-squaresalgorithm. Founda- networks. Proceedings of the National Academy of
tions of Computational Mathematics, 7:331–368. Sciences, 115(33):E7665–E7671.
9Pedregosa, F. and Scieur, D. (2020). Acceleration Zhu, H., Williams, C. K. I., Rohwer, R., and
through spectral density estimation. In Inter- Morciniec, M. (1997). Gaussian regression and op-
national Conference on Machine Learning, pages timalfinitedimensionallinearmodels. Technicalre-
7553–7562. PMLR. port, Aston University, Birmingham.
Peng, H., Pappas, N., Yogatama, D., Schwartz, R.,
Smith,N.A.,andKong,L.(2021). Randomfeature
attention. arXiv preprint arXiv:2103.02143.
Rahimi, A. and Recht, B. (2007). Random features
for large-scale kernel machines. Advances in neural
information processing systems, 20.
Rudi, A., Marteau-Ferey, U., and Bach, F. (2024).
Finding global minima via kernel approximations.
Mathematical Programming, pages 1–82.
Rudi,A.andRosasco,L.(2017). Generalizationprop-
erties of learning with random features. Advances
in neural information processing systems, 30.
Sagun, L., Evci, U., Guney, V. U., Dauphin, Y., and
Bottou, L. (2017). Empirical analysis of the hes-
sian of over-parametrized neural networks. arXiv
preprint arXiv:1706.04454.
Seroussi, I., Naveh, G., and Ringel, Z. (2023). Sepa-
rationofscalesandathermodynamicdescriptionof
feature learning in some cnns. Nature Communica-
tions, 14(1):908.
Vyas, N., Atanasov, A., Bordelon, B., Morwani, D.,
Sainathan, S., and Pehlevan, C. (2024). Feature-
learningnetworksareconsistentacrosswidthsatre-
alistic scales. Advances in Neural Information Pro-
cessing Systems, 36.
Williams, C. and Seeger, M. (2000). The effect of
theinputdensitydistributiononkernel-basedclassi-
fiers.InICML’00ProceedingsoftheSeventeenthIn-
ternational Conference on Machine Learning, pages
1159–1166. Morgan Kaufmann Publishers Inc.
Williams, C. K. and Rasmussen, C. E. (2006). Gaus-
sian processes for machine learning, volume2. MIT
press Cambridge, MA.
Xiao, H., Rasul, K., and Vollgraf, R. (2017). Fashion-
mnist: a novel image dataset for benchmarking ma-
chine learning algorithms. arXiv.
Yang, A. X., Robeyns, M., Milsom, E., Anson, B.,
Schoots, N., and Aitchison, L. (2023). A theory of
representation learning gives a deep generalisation
of kernel methods. In International Conference on
Machine Learning, pages 39380–39415. PMLR.
Yang, G., Hu, E. J., Babuschkin, I., Sidor, S., Liu,
X., Farhi, D., Ryder, N., Pachocki, J., Chen, W.,
and Gao, J. (2022). Tensor programs v: Tuning
large neural networks via zero-shot hyperparameter
transfer. arXiv preprint arXiv:2203.03466.
10A Kernel integral operator moments and Stieltjes transform
We can easily see that the sequence of spectral moments
{P∞ λn}∞
uniquely determines the non-zero eigen-
i=1 i n=1
values {λ }∞ of a self-adjoint trace-class operator. Consider the following Stieltjes transform where z is in the
i i=1
complex plane:
∞
g(z)=X λ i (A1)
z−λ
i
i=1
Its Taylor series for z−1 near zero is
∞
g(z)=X
λ
z−1+λ2z−2+λ3z−3+λ4z−4+O(cid:0) z−5(cid:1)
(A2)
i i i i
i=1
which is equivalent to
g(z)=m(1)z−1+m(2)z−2+m(3)z−3+m(4)z−4+... (A3)
with moments m(n) = P∞ λn. The moments {m(n)}∞ uniquely define the complex meromorphic function
i=1 i n=1
g(z), and the eigenvalues {λ }∞ can then be determined by the location of the poles of g(z). Therefore, the
i i=1
operator moments uniquely determine the non-zero operator eigenvalues.
Newton’sidentitiescanalsobeusedtoexpresstherelationshipbetweenoperatormomentsandthecharacteristic
equation for the eigenvalues of finite rank d operators. Consider the characteristic polynomial
d
Y
f(λ)= (λ−λ ) (A4)
i
i=1
with roots at the eigenvalues of the operator, i.e. {λ }d . The function f(λ) can be decomposed in decreasing
i i=1
ordersofλwithcoefficientsconsistingoftheelementarysymmetricpolynomialsoftheroots. Newton’sidentities
recursively relate these coefficients with the power sums of λ , which are identical to the spectral moments:
i
{m(n)}d . Thus, the spectral moments uniquely determine the characteristic polynomial of the eigenvalues.
n=1
B Kernel covariance operator and tight frame operator in RKHS
Letρ (x)andρ (w)beprobabilitymeasuresoverlatentspacesX andW respectively. Themapϕ:X×W →R
X W
is square-integrable with respect to both ρ and ρ and determines the (iα)-th coefficient of the measurement
X W
matrix [Φ ] by Φ = ϕ(x ,w ). Let ψ(x) := ϕ(x,·) : W → R. Then the completion of the linear space of the
iα iα i α
functions {ψ(x)} is F ⊂L2(W,ρ ), with the inner product
x∈X W
Z
⟨f|f′⟩ = dρ (w)f(w)f′(w) (A5)
F W
where f,f′ ∈F. Now consider the covariance operator T :F →F,
c
Z
T := dρ (x) |ψ(x)⟩⟨ψ(x)| (A6)
c X
F is also a reproducing kernel Hilbert space (RKHS), so the Riesz representation theorem implies that for all
w ∈ W, there exists a unique evaluation function φ(w) ∈ F such that f(w) = ⟨f|φ(w)⟩ ,∀f ∈ F. The map ϕ
F
can then be expressed as an inner product in the RKHS, ϕ(x,w) ≡ ⟨ψ(x)|φ(w)⟩ . Define the frame operator
F
S :F →F:
Z
S = dρ (w) |φ(w)⟩⟨φ(w)| (A7)
W
Then S is equivalent to the identity operator, since ⟨f|S|f′⟩ = ⟨f|f′⟩ . The set of |φ(w)⟩ is a tight Parseval
F F
frame in F.
Now we can interpret what it means to compose T :
c
Z n
Y
Tn = dρ (x ) |ψ(x )⟩⟨ψ(x )|ψ(x )⟩···⟨ψ(x )|ψ(x )⟩⟨ψ(x )| (A8)
c X l 1 1 2 n−1 n n
l=1
11the trace of which is
Z n n
Y Y
trTn = dρ (x ) ⟨ψ(x )|ψ(x )⟩ (A9)
c X l l l+1
l=1 l=1
with the trace constraint x = x . Since S is the identity operator on F, ⟨ψ(x)|ψ(x′)⟩ is equivalent to
n+1 1
⟨ψ(x)|S|ψ(x′)⟩, so
Z
⟨ψ(x)|ψ(x′)⟩≡ dρ (w) ⟨ψ(x)|φ(w)⟩ ⟨ψ(x′)|φ(w)⟩ . (A10)
W F F
Therefore, the trace becomes
Z n n
Y Y
trTn = dρ (x )dρ (w ) ϕ(x ,w )ϕ(x ,w ). (A11)
c X l W l l l l+1 l
l=1 l=1
Now we can relate T to T defined in the main text. With the definition of the kernel function and T :
c k k
L2(X,ρ )→L2(X,ρ ), we see that
X X
Z n n
Y Y
trTn = dρ (x ) k(x ,x )≡trTn. (A12)
c X i i i+1 k
i=1 i=1
C Proof of estimator consistency
Consider the sets of increasing index sequences:
{i ,i ,...,i }⊂{1,2,...,P}, where i <i <···<i (A13)
1 2 n 1 2 n
and similarly for {α ,α ,...,α }⊂{1,2,...,Q}, where α <α <···<α . There are
(cid:0)P(cid:1)
such sequences for
1 2 n 1 2 n n
{i } and
(cid:0)Q(cid:1)
for {α }.
l n l
Define
n
1 X X Y
mˆ(n)= Φ Φ (A14)
(cid:0)P(cid:1)(cid:0)Q(cid:1) ilαl il+1αl
n n 1≤i1<···<in≤P1≤α1<···<αn≤Ql=1
where i =i . The estimator mˆ(n) is an unbiased estimator of
n+1 1
Z n n
Y Y
m(n):= dρ (x )dρ (w ) ϕ(x ,w )ϕ(x ,w ). (A15)
X l W l l l l+1 l
l=1 l=1
Theorem 1. mˆ(n) is a strongly consistent estimator of m(n), namely
a.s.
mˆ(n)−−→m(n) (A16)
as P,Q→∞.
Proof. Define the function
n
1 X X Y
h({w }|{x })= ϕ(x ,w )ϕ(x ,w ), (A17)
α i (cid:0)P(cid:1)(cid:0)Q(cid:1) il αl il+1 αl
n n 1≤i1<···<in≤P1≤α1<···<αn≤Ql=1
with i =i .
n+1 1
For each fixed {x }, consider h as a U-statistic with asymmetric kernel in the variables {w }. The U-statistic
i α
kernel function is
n
Y
h ({w })= ϕ(x ,w )ϕ(x ,w ). (A18)
kernel αl il αl il+1 αl
l=1
12We first need to show that h is absolutely integrable with respect to ρ⊗n. Applying H¨older’s inequality and
kernel W
using the square-integrability of ϕ, we have
Z (cid:18)Z (cid:19)1/2(cid:18)Z (cid:19)1/2
dρ W(w) (cid:12) (cid:12)ϕ(x il,w)ϕ(x il+1,w)(cid:12) (cid:12)≤ dρ W(w)ϕ2(x il,w) dρ W(w)ϕ2(x il+1,w)
W W W
<∞.
Since the integrals are finite for all x and x , the function h is in L1.
il il+1 kernel
BythestronglawoflargenumbersforU-statisticswithabsolutelyintegrableasymmetrickernels(Janson,2018),
it follows that, as Q→∞,
a.s.
h({w }|{x })−−→g({x }), (A19)
α i i
where
n
1 X Y
g({x })= k(x ,x ). (A20)
i (cid:0)P(cid:1) il il+1
n 1≤i1<···<in≤Pl=1
Recall that k is defined by
Z
k(x,y)= dρ (w)ϕ(x,w)ϕ(y,w). (A21)
W
W
Next, consider g({x }) as a U-statistic over the variables {x }. The corresponding U-statistic kernel function is
i i
n
Y
g ({x })= k(x ,x ). (A22)
kernel il il il+1
l=1
Note g is absolutely integrable. Applying the strong law of large numbers for U-statistics (Korolyuk and
kernel
Borovskich, 2013), it follows that, as P →∞,
a.s.
g({x })−−→m(n), (A23)
i
where
Z n n
Y Y
m(n)= dρ (x ) k(x ,x ) (A24)
X l l l+1
l=1 l=1
with x = x . Combining the two results for almost sure convergence and using the independence between
n+1 1
{x } and {w }, we conclude that, as P,Q→∞,
i α
a.s.
mˆ(n)−−→m(n). (A25)
D Spectrum of the general RBF kernel with Gaussian input
D.1 Derivation of spectral moments
We study the kernel operator for the radial basis function (RBF) kernel
k(x,x′)=e− 21(x−x′)⊤Σ−1(x−x′) (A26)
and input distribution ρ =N(0,Σ ).
X x
Then the n-th spectral moment is moment is given by
Z n n
Y Y
m(n)= dρ (x ) k(x ,x ) (A27)
X i i i+1
i=1 i=1
=(cid:16)
(2π)d|Σ
|(cid:17)−n/2Z Yn
dx
exp−1 n X−1
(x −x )⊤Σ−1(x −x
)+Xn
x⊤Σ−1x
!
(A28)
x i 2 i i+1 i i+1 i x i
i=1 i=1 i=1
13where x =x . The integrand simplifies to
n+1 1
" n n−1 !#
exp−1 X x (cid:0) 2Σ−1+Σ−1(cid:1) x −2 X x Σ−1x +x Σ−1x , (A29)
2 i x i i i+1 1 n
i=1 i=1
which can be written as
 D −Σ−1 0 ··· 0 0 −Σ−1
−Σ−1 D −Σ−1 ··· 0 0 0 
  0 −Σ−1 D ··· 0 0 0  
exp− 21 x¯⊤
n
 

. .
.
. .
.
. .
.
... . .
.
. .
.
. .
.
  x¯
n
(A30)
 
 0 0 0 ··· D −Σ−1 0 
 
 0 0 0 ··· −Σ−1 D −Σ−1
−Σ−1 0 0 ··· 0 −Σ−1 D
where D := 2Σ−1 +Σ−1, and x¯ := (cid:2) x⊤,...,x⊤(cid:3)⊤ ∈ Rnd. Denoting the above block matrix as M, we have
x n 1 n
simplified the moment equation to
(cid:16) (cid:17)−n/2Z (cid:18) 1 (cid:19)
m(n)= (2π)d|Σ | dx¯ exp − x¯⊤Mx¯ . (A31)
x n 2 n n
Solving the Gaussian integral gives
m(n)=(detΣndetM)− 21 . (A32)
x
In general, the determinant of a block circulant matrix:
 
R R ··· R
0 1 n−1
R n−1 R 0 ··· R n−2
M = 

. .
.
. .
.
... . .
.
  , (A33)
R R ··· R
1 2 0
is given by
n−1 n−1 !
Y X
detM = det e2πiql/nR (A34)
l
q=0 l=0
After some algebra, the determinant of the block matrix becomes
n
Y n h (cid:16) q(cid:17)i o
detM = det 2 1−cos 2π Σ−1+Σ−1 . (A35)
n x
q=1
Therefore,
Yn
n h (cid:16) q(cid:17)i
o!− 21
m(n)= det 2 1−cos 2π Σ−1Σ +I . (A36)
n x
q=1
We can simplify further. Let {η }d be the eigenvalues of Σ−1Σ , which are the same as the eigenvalues of
i i=1 x
Σ Σ−1. Note that η is always real, since Σ−1 and Σ are positive semi-definite, and the product of two positive
x i x
semi-definite matrices has real eigenvalues. Then,
m(n)=
Yn Yd h
1+2η −2η
cos(cid:16) 2πq(cid:17)i−1
2 , (A37)
i i n
q=1i=1
which can be written using the law of cosines:
m(n)=
Yn Yd 1 "(cid:18) 1+√ 1+4η i(cid:19)2 +(cid:18) −1+√ 1+4η i(cid:19)2
−
2 cos(cid:16) 2πq(cid:17)#−1 2
(A38)
η 2η 2η η n
i i i i
q=1i=1
14√ √
Let us define ϕ = 1+ 1+4ηi, so that η−1ϕ−1 = −1+ 1+4ηi. Using these definitions, we can rewrite the above
ηi 2ηi i ηi 2ηi
as
m(n)=Yd ϕn ( Yn h(cid:0) η ϕ2 (cid:1)2 −2cos(cid:16) 2πq(cid:17)(cid:0) η ϕ2 (cid:1) +1i)−1 2 (A39)
ηi i ηi n i ηi
i=1 q=1
Now, the following identity can be used: Qn (cid:0) x2−2cos(cid:0) 2πq(cid:1) x+1(cid:1) =(xn−1)2. Therefore, we arrive at
q=1 n
d
Y 1
m(n)= . (A40)
ηnϕn −ϕ−n
i=1 i ηi ηi
D.2 Derivation of eigenvalues
Consider f u(x) = Qd i=1H ui(x i)e−αix2 i where H c(x) is a c-th order polynomial with leading coefficient of 1.
u:={u }d is a multiset of d natural numbers. We require that T f =λ f , to solve for the eigenvalues and
i i=1 k u u u
eigenfunctions. Let us first find an expression for T f :
k u
[T kf u](y)=
q
(2π)d1
Qd i=1η
ie−
21Pd
i=1y
i2Z iY =d
1dx
i
iY =d
1H ui(x
i)exp" X i=d 1−(cid:18) 21
η i
+
1
2
+α
i(cid:19)
x2
i
+x iy
i#
(A41)
After some algebra, we arrive at
[T kf u](y)=v u u tYd 2α1
′η
* Yd H ui(x i)+ e−Pd i=1(cid:16) 21− 4α1 ′ i(cid:17) y i2 (A42)
i=1 i i i=1 N
(cid:26)
1
(cid:27)d (cid:26)
1
(cid:27)d !!
N y ,diag (A43)
2α′ i 2α′
i i=1 i i=1
(cid:16) (cid:17)
where α′ = 1 + 1 +α , and diag {a }d is a diagonal matrix whose i-th diagonal entry is a .
i 2ηi 2 i i i=1 i
Now the eigenvalue equation [T f ](y)=λ f (y) requires that
k u u u
v u u tYd 2α1
′η
* Yd H ui(x i)+ e−Pd i=1(cid:16) 1 2− 4α1 ′ i(cid:17) y i2 =λ uYd H ui(y i)e−Pd i=1αiy i2 (A44)
i=1 i i i=1 N i=1
Equating the exponents 1 − 1 =α gives
2 4α′ i
i
√
−1+ 1+4η
α = i (A45)
i 4η
i
√ q
Recallη−1ϕ−1 = −1+ 1+4ηi. Therefore 1 =η−1ϕ−1,whichmeanstheaboveeigenvalueequationsimplifies:
i ηi 2ηi 2α′ηi i ηi
d !* d + d
Y Y Y
η−1ϕ−1 H (x ) =λ H (y ) (A46)
i ηi ui i u ui i
i=1 i=1 N i=1
N (cid:0)(cid:8) η−1ϕ−2y (cid:9) ,diag(cid:0)(cid:8) η−1ϕ−2(cid:9)(cid:1)(cid:1) . (A47)
i ηi i i ηi
Since each dimension is independent,
d ! d d
Y Y Y
η−1ϕ−1 ⟨H (x )⟩ =λ H (y ). (A48)
i ηi ui i N u ui i
i=1 i=1 i=1
15Consider an example polynomial:
⟨H (x )⟩ =(cid:10) xui +O(xui−1)(cid:11) =(cid:0) η−1ϕ−2(cid:1)uiyui +O(yui−1). (A49)
ui i N i i N i ηi
Showing only the leading order terms of the LHS and RHS of the eigenvalue equation, we see
d !
Y η−1ϕ−1 (cid:0) η−1ϕ−2(cid:1)uiyui +O(yui−1)=λ yui +O(yui−1). (A50)
i ηi i ηi u
i=1
Equating the coefficients of the leading order terms, we find that
d
λ
=Y(cid:0) η1+uiϕ1+2ui(cid:1)−1
. (A51)
u i ηi
i=1
The same eigenvalues can be found via the Taylor series expansion of m(n). Let us define r = η−1ϕ−2 and
i i ηi
s=Qd (cid:0) η−1ϕ−1(cid:1)
. Then,
i=1 i ηi
d d
m(n)=snY 1 =snY(cid:0) 1+rn+r2n+···(cid:1)
(A52)
1−rn i i
i=1 i i=1
=sn(cid:0) 1+rn+r2n+···(cid:1)(cid:0) 1+rn+r2n+···(cid:1)(cid:0) 1+rn+r2n+···(cid:1)
. (A53)
1 1 2 2 3 3
Notice that expanding the above product yields a sum of terms of n-th degree:
(cid:0) srarbrcrd···(cid:1)n
where
1 2 3 4
{a,b,c,d,...} is a multiset of integers. This implies that
d
Y
λ =s rui. (A54)
u i
i=1
Plugging in the definitions for r and s, we get
i
d
λ
=Y(cid:0) η1+uiϕ1+2ui(cid:1)−1
(A55)
u i ηi
i=1
which agrees with the results of the earlier derivation.
E Bias and variance of kernel integral operator moment estimators
In the numerical experiments with the radial basis function (RBF) kernel and Gaussian input distribution, we
observe that our estimator achieves both the lowest bias and variance error, across all configurations that are
tested.
Figure A1) shows the performance of the different estimators along with a detailed breakdown of the error in
terms of the experimentally observed bias and variance.
F Moment estimation with noise
An unbiased estimate of the m(n) can be obtained even when the measurements are corrupted by correlated
noise by utilizing measurements over two or more trials. In these experiments, we add noise to the measurement
matrix resulting in both row-correlated and column-correlated noise,
D E
Φ(t)Φ(t) ∝δ +δ (A56)
iα jβ ij αβ
t
wheretisthetrialindex,andΦ(t) iscentered. Asnotedinthemaintext,thefollowingproductwithalternation
iα
between two trials gives an unbiased estimate of m(n) in the presence of the correlated noise:
n
mˆ′ (n)=Y Φ(1) Φ(2) (A57)
alt-{1,2} ilαl il+1αl
l=1
16Algorithm A1 Computation of mˆ (n) for n=2 to n when T =2
alt max
Require: Φ(1),Φ(2) ∈RP×Q, n
max
1: for h←1 to P do
2: Initialize S as a P ×Q zero matrix.
3: Set S ←PΦ(1) ∀i∈[1,Q]
hi hi
4: for n←2 to n do
max
n2Pa−1 Pb−1 S Φ(2)Φ(1)
5: Update S ← l=h+n−2 k=n−1 lk ak ab ∀a∈[h+n−1,P], ∀b∈[n,Q].
ab (P −n+1)(Q−n+1)
1
6: Compute mˆ(h)(n)← PP PQ S Φ(2).
alt PQ i=h+n−1 j=n ij hj
7: end for
8: end for
P−n+1
9: Get mˆ (n)← 1 X mˆ(h)(n) ∀n∈[2,n ].
alt P alt max
h=1
Algorithm A2 Computation of mˆ (n) for n=2 to n for arbitrary T ≥2
alt max
Require: {Φ(t)}T ∈RP×Q, n ,
t=1 max
1: for h←1 to P do
2: Initialize S as a P ×Q zero matrix.
3: Set S ←PΦ(t1) ∀i∈[1,Q].
hi hi
4: Choose t ∈{1,...,T}
1
5: for n←2 to n do
max
6: Choose t ,t ∈{1,...,T} such that t ̸=t and t ̸=t .
2n−2 2n−1 2n−3 2n−2 2n−2 2n−1
n2Pa−1 Pb−1 S Φ(t2n−2)Φ(t2n−1)
7: Update S ← l=h+n−2 k=n−1 lk ak ab ∀a∈[h+n−1,P], ∀b∈[n,Q].
ab (P −n+1)(Q−n+1)
8: Choose t such that t ̸=t and t ̸=t .
r r 2n−1 r 1
1
9: Compute mˆ(h)(n)← PP PQ S Φ(tr).
alt PQ i=h+n−1 j=n ij hj
10: end for
11: end for
P−n+1
12: Get mˆ (n)← 1 X mˆ(h)(n) ∀n∈[2,n ].
alt P alt max
h=1
withthetraceconstrainti =i . AlgorithmA1detailshowtocomputetheestimatorwithtwotrialmeasure-
n+1 1
ments. In general, when there are T total trials, we can write
n
mˆ′ (n)=Y Φ(t2l−1)Φ(t2l) . (A58)
alt-T ilαl il+1αl
l=1
T = {t }2n is a ordered multiset with cardinality 2n of the trial index set {1,2,...,T}. The necessary and
l l=1
sufficient condition for (A58) to be an unbiased estimator in the presence of correlated noise is that for all i and
j where |i−j| = 1, and for i = 1 and j = 2n, t and t take distinct values. The algorithm for this case is
i j
presentedinAlgorithmA2. Alternatively,onecanusealgorithmA1for(A57)byrandomlyselectingtwodistinct
trials from {1,...,T} repeatedly and then averaging over the resulting estimates.
F.1 Numerical estimation results for noisy measurements
Thecross-trialalternationmethodcanalsobeappliedtothenaiveaswellasKongandValiant(2017)estimators
to remove the effects of independent and correlated noise. We test the estimators with or without the cross-trial
alternation,ontheRBFkernelmeasurementdatawithnonoise,independentnoise,andcorrelatednoise(Figure
A2). These numerical tests confirm that with only two trials T = 2, our estimator is still unbiased even when
the measurement matrix is corrupted by correlated noise (Figure A2e). It can also be seen that for independent
17noise, we only need one trial (T =1) of the measurement matrix to obtain an unbiased estimate (Figure A2b).
Our estimator achieves the lowest bias and variance error across nearly all configurations.
G ReLU network learning implementation
The feature learning example in the main text is demonstrated with a single-hidden layer neural network. Each
input x∈Rd is a flattened vector of Fashion-MNIST image’s 28×28 pixels. The w ∈Rd represents the weight
i
vectorforthei-thneuroninthehiddenlayerwithatotalN numberofneurons. Leta ∈RN betheweightvector
j
for the j-th neuron in the output layer with 10 neurons, each corresponding to one class in the Fashion-MNIST
dataset. Explicitly, the value of the j-th output neuron can be written as:
N
X
y = ϕ(x,w )a (A59)
j i ji
i=1
where a ∈R is an i-th element of the vector a ∈RN, and
ji j
ϕ(x,w):=max(x·w,0). (A60)
We minimize the sum of the square of the difference between the 10-dimensional network output and the 10-
dimensional one-hot vector that indicates the class membership. We use 10,000 images for training the network.
The weights are initialized according to the maximal update parameterization (µP) to ensure feature learning
even when N is large (Yang et al., 2022). For the single-hidden layer neural network that will be trained with
Adam-SGD, the µP initialization is:
(cid:18) (cid:19)
1
w ∼N 0, I (A61)
i N d×d
(cid:18) (cid:19)
1
a ∼N 0, I (A62)
j N N×N
with independent sampling across all i’s and j’s. For proper gradient scaling during the backward pass, µP
requires the following modification to the model:
N
X 1
y = √ ϕ(x,w )a (A63)
j i ji
N
i=1
√
ϕ(x,w):= Nmax(x·w,0). (A64)
which does not affect the forward pass. The learning rates are fixed to .01. Note that this scaling is specific for
N
Adam-SGD (Yang et al., 2022). We randomly sample 32 images from 10,000 training images for a mini-batch,
and train each network for 120 epochs.
Inthemaintext,wetrainnetworkswiththefollowingwidths(N): 32,64,128,256,512,and1024,andcompute
thespectralmomentseveryevenepoch. EachentryofthemeasurementmatrixΦ∈RP×Qinthiscaseisobtained
as:
ϕ(x ,w )
Φ = i α (A65)
iα q
1 PP PQ ϕ(x ,w )2
PQ j=1 β=1 j β
for P =1000 number of test images {x }P , and Q number of neurons {w }Q . In the main text, we showed
i i=1 α α=1
the results when measuring all neurons Q = N in the hidden layer. For all training, Quadro GV100 GPU is
used.
Next, we explore the case where the measurement matrix consists of only partial observations of the neurons
Q<N.
18G.1 Estimation from partial measurements
Here we explore the following question: given a wide neural network (large N), how closely do the moment
estimates from observing all neurons in the hidden layer Q = N and the estimates from observing random
subsamples of the neurons Q<N match?
Tocheckthis,foreachepochoftrainingawidesingle-hiddenlayerneuralnetworkwithN =1024hiddenneurons
usingtheabovespecifications,weestimatethemomentsfromameasurementmatrixΦ fromallneuronsQ=N
all
and the moments from another measurement matrix Φ that observes a smaller set of neurons Q=128<N.
sub
Thenumericalresultsshowthatineveryepoch,thesetwoestimatesmatchveryclosely(FigureA3). Thismeans
thatwedonotneedtoobserveallneuronsinaneuralnetworktodeterminethespectralpropertiesofthekernel
operator. Thiscanbeparticularlyusefulforlarge-scalenetworks,suchasinstate-of-the-artTransformermodels,
where computing the covariance matrix is highly memory and computationally intensive.
We also compare these moments to those from networks trained with smaller sizes (N =128), and find that the
resulting kernel moments estimates also match (Figure A4), as expected based upon the results from the main
text.
19a.
mˆ(n) MSE Bias Variance
10−4 10−8 10−5 10−11
10−8 10−15 10−9 10−17
10−12 10−22 10−13 10−23
2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7
n n n n
b. mˆ(n) MSE Bias Variance
10−2
10−4 10−9
10−6
10−12
10−8 10−16
10−10
10−18
10−12 10−23
10−14
10−24
2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7 Ours
n n n n Naive
c. KVrow
mˆ(n) MSE Bias Variance KVcol
10−1 10−3
10−2
10−5
GT
10−3 11 00 −− 75
10−4 10−7
10−5
10−9 10−6 10−9
2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7
n n n n
d. mˆ(n) MSE Bias Variance
10−2
10−3 10−2 10−4
10−4
10−5
10−4 10−6
10−6 11 00 −− 97 10−6 10−8
2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7
n n n n
Figure A1: Performance of the estimators with the RBF kernel. Columns from left to right: the estimated
1
moments; the mean-square error between the estimated moments and the ground true moments averaged over
multiple samples of Φ’s (⟨mˆ(n)−m(n)⟩ ); bias error (⟨mˆ(n)⟩−m(n)); variance error ((cid:10) mˆ(n)2(cid:11) −⟨mˆ(n)⟩2). a.
Φ
P = 300, Q = 600, d = 5, Σ = I , Σ = 0.25I . b. P = 300, Q = 600, d = 10, Σ = I , Σ = I . c.
x d×d d×d x d×d d×d
P =30, Q=60, d=10, Σ =I , Σ=4I . d. P =30, Q=60, d=4, Σ =I , Σ=0.25I .
x d×d d×d x d×d d×d
20a. No noisemˆ(n) MSE Bias Variance
10−2
10−5
10−2 10−5
11 00 −− 64 10−9 10−5 11 00 −− 118
10−8
10−13 10−8
10−14
2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7
b. Independe.nt noise; T =1 . . .
10−1
10−4 10−2
10−7
10−4 10−8 10−5
10−10
10−7 10−12 10−8 10−13
2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7
c. Correlated.noise; T =1 . . .
11 00 −− 31 11 00 −− 52
10−2
10−5
Ours
10−5 10−8 10−4
10−8 N Ka Viv re
ow
10−7 10−11 10−6 10−11 K GV Tcol
2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7
d. Independe.nt noise; T =2 . . .
10−1
10−4 10−2
10−7
11 00 −− 74 11 00 −− 128 111 000 −−− 864 11 00 −− 11 30
2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7
e. Correlated.noise; T =2 . . .
10−1
10−2 10−4
10−5 10−8
10−4 11 00 −− 107
10−8 10−12 10−7 10−13
2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7 2 3 4 5 6 7
n n n n
FigureA2: PerformanceoftheestimatorsinthepresenceofindependentorcorrelatednoisefortheRBFkernel.
1
P = 75, Q = 15, d = 3, Σ = I , Σ = 0.25I . Columns from left to right: the estimated moments; the
x d×d d×d
mean-squareerrorbetweentheestimatedmomentsandthegroundtruemomentsaveragedovermultiplesamples
of Φ’s (⟨mˆ(n)−m(n)⟩ ); bias error (⟨mˆ(n)⟩−m(n)); variance error ((cid:10) mˆ(n)2(cid:11) −⟨mˆ(n)⟩2). a. No noise case. b.
Φ
The data is corrupted by an additive independent noise sampled from the standard normal distribution. The
number of trials is T =1. c. The data is corrupted by an additive correlated noise (not independently) sampled
from the standard normal distribution. For a given input i, the noise is correlated between entry Φ and Φ
iα iβ
for |α−β| > 10. d. Estimators alternating between measurements from two trials with independent noise. e.
Estimators alternating between measurements from two trials with correlated noise.
21n=2 n=3 n=4 n=5
0.10
N=1024
0.04
Q=1024(full)mˆ(n)
0.2 0.1 0.05
0.02 Q=128(sub)mˆ(n)
Q=128(sub)mˆ0(n)
0.0 0.0 0.00 0.00
0 100 0 100 0 100 0 100
Epoch Epoch Epoch Epoch
FigureA3: SinglehiddenlayerneuralnetworkwithN =1024neuronsinthehiddenlayer. Eachplotcorresponds
1
to a different moment order n. P = 1000 test images are used. Black line: mean value of our estimator mˆ(n)
applied to Φ with all neurons Q = N. Solid red line: mean value of our estimator mˆ(n) applied to Φ with
subsampled neurons Q=128<N. Dotted magenta line: mean value of the naive estimator mˆ (n) applied to Φ
0
with subsampled neurons. The shaded regions indicate a 50% confidence interval. For the naive estimator, the
mean value falls outside the confidence interval.
n=2 n=3 n=4 n=5
0.10 N =Q=1024mˆ(n)
0.04 N =Q=128mˆ(n)
0.2 0.1 0.05 N =Q=1024mˆ0(n)
0.02
N =Q=128mˆ0(n)
N =1024>Q=128mˆ(n)
0.0 0.0 0.00 0.00
0 100 0 100 0 100 0 100
Epoch Epoch Epoch Epoch
Figure A4: Single hidden layer neural networks trained with N =1024 (black) and N =128 (blue) hidden layer
1
neurons.. Each plot corresponds to a different moment order n. Each measurement matrix observes all neurons
Q = N, except for one case (red) where Q = 128 neurons are randomly subsampled from N = 1024 neurons.
P = 1000 test images are used. The solid lines are the values of our estimator mˆ(n), and the dotted lines are
the values of the naive estimator mˆ (n). The shaded regions indicate a 50% confidence interval.
0
22