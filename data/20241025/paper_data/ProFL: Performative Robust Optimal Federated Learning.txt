PROFL: PERFORMATIVE ROBUST OPTIMAL
FEDERATED LEARNING
XueZheng1,TianXie1,XuweiTan1,AylinYener1,XueruZhang1,AliPayani2,MyungjinLee2
1TheOhioStateUniversity,2CiscoResearch
{zheng.1822,xie.1379, tan.1206, zhang.12807}@osu.edu
yener@ece.osu.edu,
{apayani, myungjle}@cisco.com
ABSTRACT
Performative prediction (PP) is a framework that captures distribution shifts that occur during the
trainingofmachinelearningmodelsduetotheirdeployment. Asthetrainedmodelisused,itsgen-
erateddatacouldcausethemodeltoevolve,leadingtodeviationsfromtheoriginaldatadistribution.
The impact of such model-induced distribution shifts in the federated learning (FL) setup remains
unexplored, despitebeingincreasinglylikelytotranspireinreal-lifeusecases. AlthoughJinetal.
(2024)recentlyextendedPPtoFLinastraightforwardmanner,theresultingmodelonlyconverges
toaperformativestablepoint, whichmaybefarfromoptimal. ThemethodsinIzzoetal.(2021);
Miller et al. (2021) can find a performative optimal point in centralized settings, but they require
theperformativerisktobeconvexandthetrainingdatatobenoiseless—assumptionsoftenviolated
inrealisticFLsystems. ThispaperovercomesalloftheseshortcomingsandproposesPerformative
robust optimal Federated Learning (PROFL), an algorithm that finds performative optimal points
in FL from noisy and contaminated data. We present the convergence analysis under the Polyak-
Lojasiewiczcondition,whichappliestonon-convexobjectives. Extensiveexperimentsonmultiple
datasetsvalidateourproposedalgorithms’efficiency.
Keywords Federatedlearning·Performativeprediction·Robust
1 Introduction
Federatedlearning(FL)isadistributedlearningparadigmthatenablesefficientlearningfromheterogeneousclients.
ConventionalFL(e.g.,collaborativemodeltraining)oftenreliesontheunrealisticassumptionthateachclient’sdata
distribution remains static throughout the training process or between the training and testing phases. Recent work
addressesthisweaknessbytrainingmodelsontime-evolvingdata(Guoetal.,2023;Maetal.,2022)ortrainingmodels
onstaticdatabutdeployingthemondifferentdatadistributions(Nguyenetal.,2022;JiangandLin,2022). However,
these works assume that the changes in data distribution are exogenous (i.e., are independent of the FL system).
Examplesofsuchexogenousdistributionshiftsincludevariationsindataquality,lightingconditions,dynamicclimate
data,etc.
Inmanyreal-worldapplications,dataoftenundergoesperformativeendogenousshiftsinducedbythemodel’sdeploy-
ment. Forinstance,contentrecommendedbymachinelearningalgorithmsondigitalplatformscanreshapeconsumer
preferences,steeringthemtowardmodesofconsumptionthatareeasiertopredictandmonetize(DeanandMorgen-
stern,2022). Similarly, navigationsystemscaninfluencetrafficpatterns, whileindividualsmayadapttheirbehavior
togamedecision-makingsystems,suchasthoseusedforloanapprovalsorjobapplications(Hardtetal.,2016;Zhang
etal.,2022). Thesetypesofshiftshavebeenstudiedincentralizedsettingsundertheframeworkofperformativepre-
diction(PP),whichseeksperformativestable(Perdomoetal.,2020)oroptimal(Izzoetal.,2021;Milleretal.,2021)
solutions.Addressingendogenousshiftsinfederatedlearning(FL)isnotasimpleextensionofthesecentralizedefforts
andremainsasignificantchallenge. Failingtodosocanleadtosuboptimalperformanceorevennon-convergence.
4202
tcO
32
]GL.sc[
1v57081.0142:viXraAnother challenge in practical FL systems, beyond model-induced endogenous shifts, is data contamination. Client
data may be noisy, corrupted, or maliciously manipulated. Unlike centralized systems, in FL, such data cannot be
removedbeforetraining, asitremainswiththeclients. DevelopingFLsystemsthatare robusttoendogenousshifts
anddatacontaminationremainsasignificantchallenge.
TheonlyworktodatethataddressestheendogenousshiftsinFLisJinetal.(2024).Theirapproachassumesnoiseless
clientdataandonlyfindsaperformativestablepoint,whichmaybesuboptimal. WhileIzzoetal.(2021);Milleretal.
(2021)achieveaperformativeoptimalpoint,theirmethodsarecentralized,assumenoiselessdata,andrequireconvex
performativerisk. Aswewillshow,themethodthatdirectlyextendsIzzoetal.(2021)tofederatedsettingsishighly
vulnerabletodatacontaminationandperformspoorlywhendistributionshiftsarecomplex.
In this paper, we solve all these challenges simultaneously and propose Performative robust optimal Federated
Learning(PROFL),anFLalgorithmthatfindsaperformativeoptimalsolutionunderendogenousdistributionshifts
with contaminated data. We provide a theoretical analysis of data contamination and the convergence of PROFL.
Compared to the federated extension of Izzo et al. (2021), PROFL employs simple yet effective strategies and con-
vergestotheperformativeoptimalsolutionforalargerclassof(non-convex)objectivefunctions. InAppendixB,we
providemorerelatedworkandhighlightdifferences.
Summaryofcontributions. Ourkeycontributionscomparedtopriorworksare:
1. Finding the optimal solution. Unlike Jin et al. (2024), which only guarantees convergence to the stable point,
PROFLfindstheoptimalpoint.
2. Convergenceanalysisforalargerclassofobjectives. Unlikepriorworks(Izzoetal.,2021;Jinetal.,2024)that
show the convergence of the proposed algorithms for the convex performative risk, we show the convergence of
PROFLunderPolyak-Lojasiewicz(PL)condition,whichisnon-convex.
3. Robustnesstodatacontamination. WeanalyzetheimpactofcontaminateddataontheFLsystemandshowthat
PROFLeffectivelytacklescomplexdistributionshiftsandismoreresilienttoestimationerroranddatacontamina-
tion.
2 ProblemFormulation
Consider a set of clients V = {1,...,N} collaboratively training an FL model θ ∈ Θ ⊂ Rd without sharing their
localdata. Here,Θisacompactset(closedandbounded),anddrepresentsthemodel’sdimension. LetZ denotethe
samplespace. Throughouttraining1,eachclientinteractswiththelocalmodel,causingitsdatatoevolvebasedonthe
perceivedmodel. Suchmodel-induceddatadistributions,studiedinPP(Perdomoetal.,2020),arecharacterizedbya
performativedistributionmapD : Θ → P(Z). Specifically,deployingmodelθ atclientiresultsindatafollowing
i
distributionD (θ). Thegoalistotrainaglobalmodelthatminimizesthetotallossacrossallclients:
i
(cid:88)
θPO =argminL(θ):= α L (θ) (1)
i i
θ
i∈V
whereL (θ)=E [ℓ(Z ;θ)]isthelocalperformativeriskofclientiforagivenlossfunctionℓ(Z ;θ)onlocal
i Zi∼Di(θ) i i
dataZ ∼ D (θ). Thisiscalledperformativeriskbecause, insteadofmeasuringtheriskoverafixeddistribution, it
i i
measurestheexpectedlossofmodelθonthedatadistributionD (θ)itinduces. L(θ)istheperformativerisk(PR)of
i
(cid:80)
allclients. Here,α ≥0representsthefractionofclienti’sdatarelativetothetotal,with α =1. Thesolution
i i∈V i
θPOthatminimizesperformativeriskiscalledperformativeoptimal(PO)point(Perdomoetal.,2020).
InrealisticFLsystems,eachclient’sdatadistributionD (θ)isunknown,andthetrainingdatamaybenoisy,corrupted,
i
or maliciously manipulated. To account for this, we assume each client i has a fraction ϵ ∈ [0,1] of data samples
i
drawn from another fixed but arbitrary distribution Q . Thus, the data acquired from client i follows a mixture
i
distributionZ ∼P (θ)=(1−ϵ )D (θ)+ϵ Q . WetermQ data“contamination”.
i i i i i i i
Our goal is to (i) design FL algorithms that converge to the performative optimal θPO; (ii) study the impact of data
contaminationQ anddeveloprobustsolutions. WhiledatacontaminationhasbeenstudiedinFLwithstaticdata,its
i
effect in performative settings is unexplored. A moment’s thought reveals that when client data depends on the FL
system,contaminationinoneclientmaycauseacascadingeffectacrossthenetwork,makingitcrucialtoinvestigate
howthisdisruptsFLconvergence. Wewillexplorethisinthepaper. WenextpresentthemetricsinPP.
Performative stable point. Since the data distribution D (θ) depends on θ, which is the variable being optimized,
i
finding the performative optimal point is often challenging. Thus, existing works (Perdomo et al., 2020; Mendler-
Du¨nneretal.,2020;Zhao,2022)havemostlyfocusedonfindingtheso-calledperformativestable(PS)point. Inan
1Onecanalsointerprettrainingasretrainingorfine-tuningofmodelsafterdeployment.
2FLsystem,thePSpointθPSisdefinedasfollows:
(cid:88)
θPS =argminL(θPS;θ):= α L (θPS;θ) (2)
i i
θ
i∈V
whereL (θ′;θ) = E [ℓ(Z ;θ)]iscalledthedecoupledperformativerisk(DPR)ofclienti. Unlikeperfor-
i Zi∼Di(θ′) i
mative risk (PR), where the distribution D (θ) depends on θ, the DPR decouples the two; i.e., the data distribution
i
isinducedbyθPS whilethevariablebeingoptimizedisθ. SinceθPS isthefixedpointofEq.(2),itcanbefoundvia
repeatedriskminimization(RRM)orrepeatedgradientdescent(RGD)(Perdomoetal.,2020).RRMrecursivelyfinds
amodelθt+1thatminimizesDPRonthedistributionD(θt)inducedbythepreviousmodel,whereasRGDrecursively
findsamodelθt+1throughgradientdescent,andθt+1isnotnecessarilytheminimizeroftheDPRonD(θt).
FollowingtheideaofRGD,Jinetal.(2024)proposedtheperformativefederatedlearning(PFL)algorithmtofind
the PS point θPS in FL. In PFL, each client i updates its local model to reduce local DPR based on the previous
model’sinduceddistribution: θt+1 ←θt−η ∇ℓ(Zt+1;θt),whereZt+1 ∼D (θt)isdrawnfromtheuncontaminated
i i t i i i i i
distribution. Theseupdatesarerepeatedforseveralepochsbeforebeingsenttotheserverforglobalaggregation. Jin
etal.(2024)showedthatPFLconvergestoθPS undercleandata. AlthoughthePSpointiseasiertofindthanthePO
point,priorworks(Izzoetal.,2021;Milleretal.,2021)showthatthePSpointmaybefarfromoptimalormaynot
evenexist. Thus,algorithmslikePFL,whichtargetthePSpoint,arenotideal.
Performativegradientforperformativeoptimalpoint.ThekeytofindingthePOpointistoupdatethemodelusing
theactualgradientofPR,∇L(θ),alsoknownastheperformativegradient(PG).InFL,eachclientneedstoupdate
itsmodelwithitslocalPG:∇L (θ). AssumeD (θ)hasdensityp(z;f(θ))withparameterf(θ)beingafunctionofθ,
i i
thenL (θ)=(cid:82) ℓ(z;θ)p(z;f (θ))dzwhenpandf arecontinuouslydifferentiable2. PGcanbederived
i i
∇L (θ)=∇L (θ)+∇L (θ) (3)
i i,1 i,2
where
∇L (θ)=E [∇ℓ(Z ;θ)]
i,1 Zi∼Di(θ) i
and
(cid:34) (cid:18)
df
(θ)(cid:19)T
∂logp(Z ;f
(θ))(cid:35)
∇L (θ)=E ℓ(Z ;θ) i i i .
i,2 Zi∼Di(θ) i dθ ∂f (θ)
i
Note that PFL proposed by Jin et al. (2024) only updates the model with ∇L (θ) while completely neglecting
i,1
∇L (θ). This error can accumulate, and eventually, PFL can converge to a PS point far from the optimal. Thus,
i,2
computing the PG for a more accurate estimate is crucial, but estimating ∇L (θ) is challenging. Although Izzo
i,2
et al. (2021) proposed a method to estimate the PG, its performance is only evaluated in a centralized setting under
a specific performative distribution map D (θ) (i.e., when f (θ) is a linear function of θ) without considering data
i i
contamination. Moreover,itassumesthatPRisconvex,whichisoftenunrealistic.
In summary, an FL algorithm that yields the PO point, even under a clean distribution and with rather limiting as-
sumptions, hasnotbeendevelopedpreviously. ThispaperfillsthatgapbyproposinganFLalgorithmthatprovably
convergestothePOpoint,evenwithnon-convexPR,andhandlesdatacontamination.
Remark 2.1. FL finds the solution by alternating between global model aggregation at the central server and local
model updates at the clients. In our proposed algorithms, we focus on the local updates, which work with different
aggregationrulesimposedbytheserver,suchasdifferingweightedaverages.
3 ANaiveExtensionfromCStoFL
ConsiderfirstdirectlyextendingIzzoetal.(2021)totheFLsetting. WeshallcallthisnaivealgortihmPerformative
optimalFederatedLearning(POFL).ThepseudocodeisprovidedinAppendixA.Inthisalgorithm,eachactiveclient
t
iinsetI ⊆ V, aftersynchronizationwiththecurrentglobalmodelθ , usesitslocaldatatoupdatethemodelfrom
t
θt to θt+R. During these updates, the data distribution changes, and consequently, client i would use samples from
i i
thecontaminateddistributionP (θt)toupdateθt. TofindtheoptimalθPO = argmin (cid:80) α L (θ), eachclienti
i i i θ i∈V i i
shouldestimateitslocalperformativegradient∇L (θt)toupdatethelocalmodelθt,i.e.,
i i i
θt+1 ←Proj (cid:0) θt−η∇L (θt)(cid:1) . (4)
i Θ i i i
2AnexampleofD (θ)isamixtureofGaussians,(cid:80)K ν N(f (θ),σ2),wherethemeandependsonthemodel(Izzoetal.,
i k=1 k k k
2021).Thiscanapproximateanysmoothdensitydistributiontoarbitraryprecision.
3ByEq.(3),∇L i(θ it)=∇L i,1(θ it)+∇L i,2(θ it)consistsoftwoterms. POFLestimates∇L i,1(θ it)bysimplyaveraging
∇ℓ(z ;θt) over samples drawn from P (θt). We denote the empirical risk on data drawn from distribution Q or
j i i i i
D i(θ it) as ∇(cid:98)L i,1(Q i,θ it) and ∇(cid:98)L i,1(D i(θ it),θ it), respectively. Then the empirical risk on P i(θ it) is ∇(cid:98)L i,1(θ it). Let
L(cid:98)i,1(θ it)denotestheaverageofℓ(z j;θ it)oversamplesdrawnfromP i(θ it). Wedenote∇(cid:98)L i,2(θ it),∇(cid:98)L i,2(Q i,θ it),and
∇(cid:98)L i,2(D i(θ it),θ it) similarly. ∇(cid:98)L i,2(θ it) is estimated by a weighted average of ℓ(z j;θ it) over samples drawn from
P (θt)withweightvector
i i (cid:12)
(cid:18) df (θ)(cid:19)T ∂logp(z ;f (θ))(cid:12)
w(z ;θt):= i j i (cid:12) ∈Rd (5)
j i dθ ∂f (θ) (cid:12)
i (cid:12)
θ=θt
i
Because the distribution map D (θ) = p(z;f (θ)) and f (θ) are unknown, we need to estimate f (θ) and dfi(θ).
i i i (cid:12) i dθ
Following Izzo et al. (2021), we adopt finite difference approximation dfi(θ)(cid:12) ≈ ∆f (∆θ )†. Specifically, at
dθ (cid:12) θ=θt i i
iteration t, each client i collects the H most recent models (excluding θt) and formi s a matrix (cid:2) θt−H ··· θt−1(cid:3) ,
i i i
then
∆θ =(cid:2) θt−H ··· θt−1(cid:3) −θt1T; (6)
i i i i H
∆f =(cid:2) f (θt−H) ··· f (θt−1)(cid:3) −f (θt)1T,
i i i i i i i H
Where1 isanH-dimensionalall-onesvector,both∆f and∆θ ared×Hmatrices,and(∆θ )†isthepseudo-inverse
H i i i
of∆θ . Foreachiterationofeachclient,weknowθt,butdonotknowtheexactvalueoff (θt). Denotetheestimator
i i i i
asf(cid:98)i(S i),meaningweusethedatasetS
i
andthefunctionf(cid:98)i(.)toestimatethevalueoff i(θ it). Thisestimatoristhen
usedtocomputeanestimateof d df θi. Theestimatorof d df θi isdenotedas d d(cid:99)f θi andcanbecomputedas d d(cid:99)f θi ≈ f(cid:98)i(∆θ i)†.
Finally, d d(cid:99)f θi will be used to calculate ∇(cid:98)L i,2. By combining this with ∇(cid:98)L i,1, we obtain the performative gradient,
allowingeachclienttoupdatethelocalmodelθtusingEq.(4).
i
ConcernswithPOFL.AlthoughIzzoetal.(2021)showedthattheabovemethodforestimatingf i(θ)and df di( θθ) was
effectiveincentralizedsettings,itwasevaluatedunderspecificscenarioswheref (θ)islinearinθanddatasamples
i
perfectlyrepresentD (θ)withoutcontamination. InpracticalFLsettingswithmodel-induceddistributionshift, this
i
methodwillperformpoorlyduetothefollowingreasons:
1. Non-linearityoff (θ). Whenf (θ)isnon-linear, ignoringthehigher-ordertermsintheTaylorexpansionof d(cid:99)fi
i i dθ
maybebiased. WhileIzzoetal.(2021)demonstratedthatthismethodworkswellforlinearf (θ),theimpactof
i
non-linearityonperformanceremainsunclear.
2. Amplified sampling error. Since dfi(θ) is estimated from limited samples, the sampling error of the estimation
dθ
∆f (∆θ )† is inherited when computing PG, ∇L (θt). As shown in Lemmas 5.7, 5.8 and 5.9 in Section 5, the
i i i i
samplingerrorinestimating dfi(θ) isamplifiedwhencomputing∇L (θt),makingtheperformativegradientand
dθ i,2 i
algorithmmoresensitivetosamplingerrors.
3. Contaminated data. Due to contamination Q , the data used for estimating df(θ) and f (θ) does not follow the
i dθ i
actualD (θ). Evenasmallerrorinasinglemodelupdateforoneclientcancascade,affectingsubsequentdataand
i
theentirenetwork.
4 TheProposedAlgorithm: PROFL
The previous section summarized the drawbacks of a straightforward extension of the centralized solution to the
federated setting. To address all these weaknesses, we propose several simple yet effective strategies that lead to a
robustnewFLalgorithm. WeshallcallitPerformativerobustoptimalFederatedLearning(PROFL).Thepseudocode
isprovidedinAlgorithm1. Belowareitskeyfeatures.
1. Handling complex distribution shifts with non-linear f i(θ). PROFL employs the following strategies: (i) re-
ducingthelearningrateηandtheestimationwindowH;(ii)estimating (cid:99)dfi attheserver. ReducingηandH helps
dθ
reducetheestimationerrorresultingfromnon-linearf (θ)andstabilizesthealgorithm.Whenthelocalsamplesize
i
n
i
is small, f(cid:98)i(S i) may significantly deviate from f i(θ), leading to poor estimation of d df θi. Estimating (cid:99)d df θi at the
servermitigatesthenegativeimpactofsamplingerror.
2. Balancingtheimpactofsamplingerrorandcomputationalcosts. PROFLadaptivelyselectsthesamplesizen
i
basedontheerrorboundsofthePGandlocalloss. Whilealargern generallyleadstoamoreaccurateestimate,
i
it may not always be feasible in FL due to clients’ limited computational capabilities. By using predefined error
4Algorithm1PROFL
1: Input: Learningrateη,estimationwindowH,functionf(cid:98),totaliterationsT,localupdateiterationsR,errorboundΦ iforall
i∈V.
2: fort=0toT −1do
3: Ift mod R=0,serversendsθt toaselectedsetI ⊆Vofclientsandclienti∈I setsthelocalmodelθt =θt ;otherwise
t t i
I =I .
t t−1
4: fori∈I inparalleldo
t
5: Deployθtandlocalenvironmentchangesanddrawn samplesS :=(z )ni i∼idP (θt).
i i i j j=1 i i
6: ift≤Hthen
7: ∇(cid:98)L i,1(θ it);S i′ ←ROBUSTGRADIENT(cid:0) S i;θ it(cid:1) .
8: Computeestimatorf(cid:98)(S′ i)off it.
(cid:16) (cid:17)
9: Updateθ it+1 ←Proj
Θ
θ it−η∇(cid:98)L i,1(θ it)
10: else
11: L(cid:98)i,1(θ it);∇(cid:98)L i,1(θ it);S i′ ←ROBUSTGRADIENT(cid:0) S i;θ it(cid:1)
12: Computeestimatorf(cid:98)(S′ i)off it.
13: Formmatrices∆θ and∆f andcompute∆f (∆θ )†.
i i i i
(cid:16) (cid:16) (cid:17)(cid:17)
14: θ it+1 ←Proj
Θ
θ it−η ∇(cid:98)L i,1(θ it)+∇(cid:98)L i,2(θ it) .
15: Input={L(cid:98)i,1(θ it);H;d;η;φ i;∆f i(∆θ i)†;Φ i}
16: n
i
←ADAPTIVESAMPLESIZE(Input).
17: Ift mod R=0,clienti∈I sendsθttotheserverandserverupdatesthemodelθt+1 =(cid:80) α θt+1.
t i i∈It i i
T
18: Output:θ
boundsandrelevantcomputedvalues—suchasmodelvariance, lossfunction, gradientinformation, andlearning
rates—wecandeterminethelowerboundforeachclient’sn ,balancingsamplingerrorandcomputationalcosts.
i
Asthesevalueschangeduringupdates,n shouldbeselectedadaptively. MoredetailsareprovidedinSection5.3.
i
3. Handling data contamination. PROFL mitigates contamination Q
i
by reducing ϵ
i
through local outlier iden-
tification and removal. Since the samples S ∼ P (θt)are noisy, and the raw data is not visible to the cen-
i i i
tral server, contamination must be identified and cleaned locally before estimating the performative gradient
∇L
i
= ∇L i,1+∇L i,2. GiventhecontaminateddatasetS i,eachclientinPROFLusesalgorithmROBUST GRA-
DIENT to identify “clean” samples S i′ = {z j ∈ S i : z j ∼ D i(θ it)} and compute robust gradient ∇L i,1(θ it). S i′
arethenusedtoestimate∇L (θt). Theoutlieridentificationandremovalmechanismreducesϵ (thefractionof
i,2 i i
contaminateddata)andcanbeselectedbasedonpriorknowledgeofϵ ,Q (ifthereareany). InAppendixA,we
i i
provideanexampleofROBUST GRADIENTforageneralcasewithunknownϵ
i
andarbitraryQ i. Itusessingular
value decomposition (SVD) to detect gradients that meet two key properties: 1) having a significant impact on
the model and potentially disrupting training (i.e., gradients with large magnitudes that systematically point in a
specific direction); and 2) being far from the average in the vector space (i.e., “long tail” data). We iteratively
removegradientswhosedistancefromtheaverageexceedsathreshold. Moredetailsonthe ROBUST GRADIENT
mechanismanditscomparisontopriorworkareprovidedinAppendix A.
Insection5,weanalyzebothalgorithms’convergenceandtheimpactofthefactorsaboveonperformance. Compared
to PFL Jin et al. (2024) and methods in Izzo et al. (2021); Miller et al. (2021), we relax assumptions and show
convergenceunderweakerconditions.
5 PerformanceAnalysis
To facilitate analysis, we consider scenarios where D (θ) =
(cid:80)K
ν (θ)D (θ) represents a mixture of K group
i k=1 i,k i,k
distributions. ν
(θ)istheproportionofsamplesfromgroupkand(cid:80)K
ν (θ)=1. Letf (.)denotethefunction
i,k k=1 i,k i,k
foreachgroup’sdistribution.
We study two special cases: (i) Contribution dynamics, where only ν (θ) changes while the group distribution
i,k
(cid:80)
remains fixed, i.e., D i(θ) = k∈[K]ν i,k(θ)D i,k. In this case, f i,k(θ) = ν i,k(θ), and f(cid:98)i,k estimates the sample
proportion from group k. (ii) Distribution dynamics, where only the distribution D (θ) changes while the con-
i,k
(cid:80)
tribution from each group remains fixed, i.e., D (θ) = ν D (θ). We consider a mixture of Gaussians
i k∈[K] i,k i,k
withD (θ) = N(f (θ),σ2), wheref (θ)isthemeanofgroupk forclienti, andtheoverallmeanforclientiis
i,k k k i,k
(cid:80)
f i(θ)= k∈[K]ν i,kf i,k(θ). f(cid:98)i(S i)representstheempiricalmeanfromdatasamplesS i. Ourtheoreticalresultsapply
5tobothcases. Beforepresentingthem,weintroducethetechnicalassumptions. Forsimplicity,∥.∥ isdenotedas∥.∥.
2
ProofsareinAppendixC.
Assumption 5.1. Let W (D,D′) measure the Wasserstein-1 distance between two distributions D and D′. Then
1
∀i∈V,thereexistsγ >0suchthat∀θ,θ′ ∈Θ:W (D (θ),D (θ′))≤γ ∥θ−θ′∥.
i 1 i i i
Assumption5.2. ℓ(z;θ)iscontinuouslydifferentiableandL-smooth,i.e.,forallθ ∈Θandz,z′ ∈Z :∥∇ℓ(z;θ)−
∇ℓ(z′;θ′)∥≤L(∥θ−θ′∥+∥z−z′∥).
Assumption5.3. f (θ)istwicecontinuouslydifferentiableforallθ ∈ Θ,i.e.,thefirstandsecondderivatives dfi(θ)
i dθ
and
d2fi(θ)
existandarecontinuous.
dθ2
Assumption 5.4 (PL Condition Polyak (1964)). The local performative risk L (θ) of client i satisfies Polyak- Lo-
i
jasiewicz(PL)condition,i.e.,forallθ ∈Θ,thefollowingholdsforsomeρ>0:
1 ∥∇L (θ)∥2 ≥ρ(cid:0) L (θ)−L (θPO)(cid:1) .
2 i i i i
Remark5.5. Unlikemostworksthatrequireconvexperformativerisk,wedemonstratetheconvergenceofouralgo-
rithmunderaweakerPLcondition,whichpermitstheperformativerisktobenon-convex.
5.1 Erroroftheperformativegradient
AsdiscussedinSection3,thekeytoconvergingtoθPOisestimatingtheperformativegradient∇L (θt)=∇L (θt)+
i i i,1 i
∇L (θt)inEq.(3)accurately. Thus,weanalyzetheestimationerrorsof∇L (θt)and∇L (θt)andexplorehow
i,2 i i,1 i i,2 i
theseerrorsareinfluencedbythenon-linearityoff (θ),samplingerror,andcontaminateddataQ . Theresultsofthis
i i
analysiswillthenbeusedtoassesstheconvergenceofPROFL(andPOFL)inSection5.2.
(cid:13) (cid:13) (cid:13) (cid:13)
Lemma5.6. UnderAssumption5.3,thereexistF,M < ∞suchthat(cid:13)dfi(θ)(cid:13) ≤ F and(cid:13)d2fi(θ)(cid:13) ≤ M holdforall
(cid:13) dθ (cid:13) (cid:13) dθ2 (cid:13)
i ∈ V.UnderAssumption5.2, thereexistG,ℓ < ∞suchthat∥∇ℓ(z;θ)∥ ≤ Gandℓ(z;θ) ≤ ℓ holdforall
max max
z ∈Z.
Lemma 5.6 is proved using Weierstrass’s theorem. We will use F,M,G,ℓ to denote the upper bounds of these
max
quantitiesintherestofthepaper.
Lemma 5.7 (Estimation error of d(cid:99)fi). Under Assumption 5.3, let λ denote the minimal singular value of ∆θ
dθ i,min i
definedinEq.(6)duringalliterations. Withprobabilityatleast1−φ ,theestimationerrorof d(cid:99)fi isboundedbyωωω
i dθ FFF
specifiedasfollows:
E(cid:34)(cid:13) (cid:13)
(cid:13)
(cid:13)d d(cid:99)f
θi −
d df θi(cid:13) (cid:13)
(cid:13)
(cid:13)2(cid:35)
≤ωωω FFF :=2ϵ2 iF2+(1−ϵ
i)2(cid:18) M2η4 2G4H6
+
2f φ(φ ni; ηn 2i)Hd(cid:19)
λ− i,m2 in. (7)
(cid:13) (cid:13) i
Notably, f (φ ;n ) is a function that depends on the properties of the distribution. For instance, f (φ ;n ) =
φ i i φ i i
O(−logφ )indistributiondynamics,andf (φ ;n )=O(1/φ )incontributiondynamics.
i φ i i i
Lemma5.8(Estimationerrorof∇L ). UnderAssumption5.3,withprobabilityatleast1−φ ,wehave:
i,1 i
(cid:20)(cid:13) (cid:13)2(cid:21)
E (cid:13) (cid:13)∇L i,1(θ it)−∇(cid:98)L i,1(D i(θ it),θ it)(cid:13)
(cid:13)
≤O(f φ(φ i;n i)).
Lemma5.9(Estimationerrorof∇L ). UnderAssumptions5.1,5.2and5.3,thefollowingholdswithprobabilityat
i,2
least1−φ :
i
(cid:20)(cid:13) (cid:13)2(cid:21) (cid:18) (cid:18)
f (φ ;n
)F2(cid:19)(cid:19)
E (cid:13) (cid:13)∇L i,2(θ it)−∇(cid:98)L i,2(D i(θ it),θ it)(cid:13)
(cid:13)
≤(ωωω DDD)2 :=O ℓ2 maxd ωωω
FFF
+ φ i
n
i .
i
E(cid:20)(cid:13)
(cid:13) (cid:13)∇L i,2(Q i,θ it)−∇(cid:98)L i,2(Q i,θ
it)(cid:13)
(cid:13)
(cid:13)2(cid:21)
≤(ωωω QQQ)2 :=O(cid:0) 2ℓ2 maxd(cid:0) ωωω
FFF
+F2(cid:1)(cid:1) .
Additionally, the notationsωωω andωωω represent upper bounds. From Lemmas 5.8 and 5.9, we find that since the
DDD QQQ
estimationerrorofperformativegradientisprimarilydominatedbytheestimationerrorof∇L ,wehave:
i,2
(cid:20)(cid:13) (cid:13)2(cid:21) (cid:20)(cid:13) (cid:13)2(cid:21)
E (cid:13) (cid:13)∇L i,1(θ it)−∇(cid:98)L i,1(D i(θ it),θ it)(cid:13)
(cid:13)
+E (cid:13) (cid:13)∇L i,2(θ it)−∇(cid:98)L i,2(D i(θ it),θ it)(cid:13)
(cid:13)
≤(ωωω DDD)2. (8)
65.2 Convergenceanalysis
Given Lemmas 5.7, 5.8, and 5.9, we are now ready to analyze the convergence. Denote γ = (cid:80) α γ , σ2 =
i∈V i i γ
(cid:80) α (γ−γ )2,ϵ = (cid:80) α ϵ ,andφ = (cid:80) φ .Throughoutthetrainingprocess,letW (D,Q) represent
i∈V i i i∈V i i i∈V i 1 max
themaximumWasserstein-1distanceW (D(θt),Q )betweenactualdatadistributionandcontamination∀i∈V. Let
1 i i
ωωω ,ωωω bethemaximumofωωω andωωω respectivelyinLemma5.9∀i∈V. Letg2 betheminimalvalueofE[∥gt∥2]
QQQ DDD QQQ DDD min
withgt =(cid:80) i∈Vα i(cid:80)K k=1∇(cid:98)L i,k.
t
Theorem5.10(Convergencerate). Let{θ }
t≥0
beasequenceofglobalmodelsgeneratedby PROFL (and POFL).
UnderAssumptions5.1,5.2,5.3,and5.4,withprobabilityatleast1−Tφ,wehave:
E(cid:104) L(cid:16) θT(cid:17) −L(cid:0) θPO(cid:1)(cid:105) ≤(1−ρη)T E(cid:104) L(cid:16) θ0(cid:17) −L(cid:0) θPO(cid:1)(cid:105) +(cid:18) (1−ϵ)ωωω 2+ϵ(ωωω )2− g m2 in(cid:19) η
DDD DDD,,,QQQ
2
(cid:16) (cid:16) (cid:17)(cid:17)
+ L(1+γ) (1−ϵ)ωωω 2+ϵ(ωωω )2+G2 η2
DDD DDD,,,QQQ
+ 1 L2(R−1)2G2(cid:0) (1+γ)2+σ2(cid:1) η3.
2 γ
whereωωω =ωωω +LW (D,Q) +ωωω .
DDD,,,QQQ DDD 1 max QQQ
Theorem 5.10, together with Lemmas 5.7, 5.8 and 5.9 in Section 5.1, highlight the impact of sampling error, data
contamination,clientheterogeneity,andthenon-linearityoff (θ)onconvergence,asdiscussedbelow.
i
(cid:16) (cid:17)
When there is no data contamination and ϵ = 0,ωωω is reduced to M2η2G4H6 + 2fφ(φi;ni)Hd λ−2 , where the
i FFF 2 niη2 i,min
firsttermisduetothenon-linearityoff (θ)andbecomeszeroiff (θ)islinearwithM =0;thesecondterm,related
i i
tosamplingerror,approacheszeroasthesamplesizen →∞. Withsufficientsamples,theerrorterm
fφ(φi;ni)F2
in
i ni
ωωω alsoapproacheszero.
DDD
Whenthereiscontaminationorf (θ)isnon-linear,theerroralwaysexistsevenwithasufficientlylargesamplesize.
i
The term ϵ(ωωω + LW (D,Q) +ωωω )2 reflects the impact of contaminated data, increasing as the fraction of
DDD 1 max QQQ
contaminationϵ and/orthedistancebetweenactualdatadistributionD (θ)andcontaminationQ increase.
i i i
ThetermL2(R−1)2G2η(cid:0) (1+γ)2+σ2(cid:1)
capturestheeffectofclientheterogeneity. RecallthatRisthenumberof
γ
localupdates. InthespecialcasewhereR = 1,meaningclientsupdateandaggregatemodelsateachiteration,this
termbecomeszero.
Corollary5.11. Ifϵ=0(withoutcontamination),theboundinTheorem5.10isreducedtothefollowing:
E(cid:104) L(θT )−L(θPO)(cid:105) ≤(1−ρη)TE(cid:104) L(θ0 )−L(θPO)(cid:105) +(cid:18) ωωω 2− g m2 in(cid:19) η+L(1+γ)(cid:0) ωωω 2+G2(cid:1) η2
DDD
2
DDD
(cid:18) (cid:19)
+ 1 L2(R−1)2G2(cid:0) (1+γ)2+σ2(cid:1) η3.
2 γ
Becauseg2 ≥0,weconsidertwocases:
min
1. Ifg2 > 0,wecanalwaysadjustlearningrateη,estimationwindowH,numberoflocalupdatesR,andsample
min
sizen suchthatωωω 2 <g2 /2andthefollowingholds
i DDD min
E(cid:104) L(θT )−L(θPO )(cid:105) ≤(1−ρη)TE(cid:104) L(θ0 )−L(θPO)(cid:105)
,
i.e.,θt convergestoθPOatlinearconvergencerate.
2. Ifg2 = 0,thismeansthereisatleastoneiterationtthathas∥gt∥ = 0andE[L(θt+1 )−L(θPO)] = E[L(θt )−
min
L(θPO)]. Exceptfortheiterationswhere∥gt∥ = 0,alltheotheriterationshavepositive∥gt∥ > 0. Wecanchoose
η,H,n suchthatωωω 2 <g2 /2andthefollowingholds
i DDD min
E(cid:104) L(θT )−L(θPO)(cid:105) ≤(1−ρη)T−bE(cid:104) L(θ0 )−L(θPO)(cid:105)
,
wherebisthenumberofiterationswith∥gt∥=0.
AllabovetheoreticalresultsapplytobothPROFLandPOFL.TherobuststrategiesinPROFLreducetheupperbound
inTheorem5.10.
75.3 Discussion
Section3introducedsimpleyeteffectivestrategiesinPROFLtoenhancePOFL.Wenowexplainwhythesestrategies
areeffective.
Handling complex distribution shifts with non-linear f (θ). Based on Lemma 5.7, the non-linear function f (θ)
i i
primarilyimpactstheestimationerrorthroughtheterm
M2η2G4H6
,whichdecreasesasηandHarereduced.However,
2
toensurethematrix∆θ inEq.(6)isnonsingular,H −1mustbelargerthanthedimensionalityofθ. Incaseswhere
i
H cannot be decreased, reducing η is essential to attain a smallerωωω . Moreover, the upper bound in Theorem 5.10
FFF
takestheformAη+Bη2+Cη3,whereB,C,η > 0. Thus,asmallerη resultsinasmallerupperbound. However,
reducingηwillalsoincreasetheerrorterm 2fφ(φi;ni)Hd inEq.(7). Nevertheless,ifthesamplesizen issufficiently
niη2 i
large and M2η2G4H6 ≫ 2fφ(φi;ni)Hd, reducing the learning rate is still effective, as we verify in Fig. 2b. In cases
2 niη2
where local samples are limited (small n ), estimating (cid:99)dfi at the server side could be considered, e.g., by clustering
i dθ
clientswithsimilarperformativedistributionmapD andperformglobalaggregationtomitigateestimationerror.
i
BalancingSamplingErrorandComputationalCosts. Weknowthatf (φ ;n )dependsonthedistribution. Once
φ i i
this information, along with the error bound Φ , is known, we can calculate the lower bound of n . For example,
i i
when d = 1, the estimator f(cid:98)i(θ) in distribution dynamics estimates the mean of D i(θ). In this case, f φ(φ i;n i) =
σ i2log(2/φ i),whereσ i2 representsthedatavarianceofclienti. WiththeerrorboundΦ i, PROFL cancomputen
i
as
follows:
2(cid:0) 2ℓ2 H∥∆θ∥−2+F2(cid:1) σ2log(2/φ )
n ≥ max i i .
i 2Φ −M2η2G4H6∥∆θ∥2ℓ2
i max
Here,H,φ ,∥∆θ∥,andηareknown,whileM,G,andF canbeestimatedfrompreviousupdates. Forℓ ,weuse
i max
thelossfromthelastiteration,asthelosstypicallydecreasesduringtraining. Thevarianceσ2isestimatedfromS in
i i
thelastiteration.
Handlingcontaminateddata. Theorem5.10showsthatcontaminateddataaffectsconvergencemainlythroughthe
termϵ(ωωω +LW (D,Q) +ωωω )2,whichisdifficulttomitigatebytuningparameters. Amoreeffectiveapproach
DDD 1 max QQQ
istoreduceϵbyremovingcontamination.Forinstance,ifallcontaminateddataisremoved(ϵ=0),PROFLconverges
totheglobalPOpoint,asstatedinCorollary5.11.
6 Experiments
ExperimentalSetup.Weevaluateouralgorithminfivecasestudies:1)pricingwithdynamicdemands,2)pricingwith
dynamiccontribution, 3)binarystrategicclassification, 4)housepricingregression, and5)regressionwithdynamic
contribution.
Pricing with Dynamic Demands. Consider a company that aims to find the proper prices for various goods. Let
θ ∈ Rd be the prices associated with the set of d goods, and Z ∈ Rd be the retailer i’s demands for these goods.
i
WeassumeZ ∼ N(µ(θ),σ2)ismodeldependentGaussiandistribution,i.e.,meandemandforeachgooddecreases
i
linearlyasthepriceincreases. µ(θ)=µ −γ θ,hereγ ≥0measuresthepricesensitivityandvariesacrossdifferent
0 i i
retailers. Thegoalistomaximizestotalrevenueoverallretailers(cid:80)N α E [θTZ ].
i=1 i Zi∼Di(θ) i
Pricing with Dynamic Contribution. Consider a similar setting as above where a company aims to find prices
for various goods that maximize the total revenue. Suppose the retailers it faces have consumers from K groups
(e.g., old/young) and these consumers have fixed demands but they have options to purchase the goods from other
companies. Hereweassumethedemandsofretaileri’sconsumersfromgroupk ∈[K]havefixeddistributionD =
i,k
N(µ ,σ2),butthefractionofgroupschangedynamicallybasedonpricesθ. Letr (θ)=B−E [θTZ ]be
i,k i i,k Zi∼Di,k i
theexpectedremainingbudgetofgroupk afterpurchasingthegoods(withinitialbudgetB)andassumethefraction
of group k is ν (θ) = ri,k(θ) (i.e., groups with more remaining budgets have more incentives to stay in the
i,k (cid:80)K k=1ri,k(θ)
system). Thentheretaileri’stotaldemandswouldfollowthemixtureofdistributionsD
(θ)=(cid:80)K
ν (θ)D .
i k=1 i,k i,k
BinaryStrategicClassification. ConsideranFLsystemisusedtomakebinarydecisionsabouthumanagents(e.g.,
loan application, hiring, college admission), where client i with data Z = (X ,Y ) may manipulate their features
i i i
strategicallytoincreasingtheirchancesofreceivingfavorableoutcomesatthelowestcosts. Supposetheclientsare
assigned positive decisions whenever
1/(cid:0) 1+exp(−XTθ)(cid:1)
≥ 1/2 and the cost it takes to manipulate feature from
X toX′is 1 ||X′−X||,thenstrategicclientswouldreacttothedecisionmodelθbymanipulatingtheirfeaturesto
i i 2γi
8(cid:16) (cid:17)
argmax −⟨X′,θ⟩− 1 ∥X′−X ∥2 =X −γ θ. Fortheinitialdata,weacquireitfromasyntheticdatawhere
X i′ i 2γi i i 2 i i
X|Y = y ∼ N(µ ,σ2)fory ∈ {0,1}andtworealisticAdult andGiveMeSomeCredit data. Thedetailsofthese
y y
dataareinAppendixE.1. Thegoalistominimizespredictionlossoverallclients.
HousePricingRegression. Consideraregressiontaskthataimstofindtheproperlistingpricesforhousesbasedon
featuressuchassize,age,numberofbedrooms. SupposethefeaturesofhousesindistrictifollowX ∼ N(µ ,σ2).
i i i
Givenpredictivemodelθ,letthelistingpricebeh(X ) = θTX . Sincethehouseswithhigherlistingpricestendto
i i
lowerthedemand,theactualsellingpriceY dependsonθandweassumeY =(a −γ θ)TX +ϵ,whereϵ∼N(0,σ2)
i i i i n
isGaussiannoise. Thegoalistominimizespredictionerror
Regressionwithdynamiccontribution. Consideranexampleofretailinventorymanagement,wheretheregression
model θ predicts product sales considering features like seasonal trends and advertising. Distributor i has retailers
from two groups k ∈ {1,2}, and each retailer’s data distribution is fixed. The fraction of retailers from each group
is ν (θ) = ℓi,−k(θ)+c where −k = {1,2}\k because the retailers are more likely to stop purchasing
i,k (cid:80) k′∈{1,2}(ℓ i,k′(θ)+c)
if they experience higher prediction error ℓ (θ). The goal is to find θ that minimizes expected prediction error
i,k
(cid:80)N α E (cid:2) (θTX −Y )2(cid:3) ,whereY isaweightedcombinationoffixeddistributions.
i=1 i (Xi,Yi)∼Di(θ) i i i
For binary classification, we use both synthetic and real data (Adult and Give Me Some Credit datasets), while the
other cases use synthetic data. All dynamics are synthetic. The parameter choices is in Table 3 of Appendix E.1.
InformationontherealdatasetsisinAppendixE.4. Eachmethodisrun10timesunderthesamesetup,andaverages
andvariancearereported.
Baselines. Todate,onlyJinetal.(2024)hasaddressedmodel-dependentdistributionshiftsinFL.Thus,wecompare
ouralgorithmwiththisPerformativeFederatedLearning(PFL).Unlikeouralgorithm,theirsconvergestoaperforma-
tivestablesolutionθPS understricterassumptions. WealsocomparethePerformativeGradient(PG)algorithmfrom
Izzoetal.(2021),inthecentralizedsetting.
Table1: PerformativeLossofPGandPROFL
α 0 0.25 0.5
PG 5.56±0.00 5.67±0.02 6.32±0.06
PROFL 5.56±0.00 5.59±0.00 5.67±0.00
60
50
60 60
40
30 50 50
20 25 20 25
20 PoFL(no) PoFL(0.1) 60
ProFL(0.1) PoFL(0.2)
10 ProFL(0.2) PoFL(0.3)
ProFL(0.3) 50
20 25
0 5 10 15 20 25 30
Global iteration
(a)DependonLearningRates (b)EstimationonServer (c)AdaptiveLearningRate
Figure1: EffectivenessofProposedmethods
(a)DifferentSampleSizes (b)DifferentLearningRates (c)DifferentH
Figure2: DynamiccontributionwithDifferentSampleSizes,LearningRates,andH
Comparison to Centralized Setting. Table 1 shows that adapting performative gradients to the FL framework im-
provesperformance. Thereare10clientswithγ ∈ [γ−α,γ+α],wherelargerαindicatesmoreheterogeneity. As
i
9
eunever
evitamrofrePclientheterogeneityincreases,theperformanceofbothalgorithmsdegrades. Underhomogeneousdistributionshifts
(α = 0), both algorithms perform similarly. PROFL (ours) outperforms PG when α = 0.25 or α = 0.5, as the
centralizedapproachstrugglestohandlevarianceshiftsbyestimatingasingle df ontheserver.
dθ
Effectiveness of our proposed methods. Fig. 1a shows that contamination can significantly degrade performance,
but the proposed ROBUST GRADIENT can effectively reduce the impact of contaminated data when computing the
gradientsi.e.,theperformancewiththerobustgradientinourmethodisalmostthesameasthecasewhenoutliersdo
notexistatall.Thiscasestudy,pricingwithdynamicdemands,involves10heterogeneousclientswithdifferentD (θ),
i
eachencountersanϵ fractionofcontaminateddatafromafixedbutunknownexogenousdistributionQ . Although
i i
wepresenttheresultswhenQ ∼N(µ ,σ2),weobservedsimilarresultswhenQ followsotherdistributionsandare
i o o i
differentamongclients.
Fig.1bshowsthatwithalimitedsamplesizeandmanyclients(n =60,|V|=100),estimating (cid:99)dfi ontheserverside
i dθ
resultsinalowerlossvaluecomparedtolocalestimationandapproachestheoptimalvalue. Fig.1cdemonstratesthe
effectivenessofadaptivesamplesizing. Wetestedtwotolerances(0.05forpurple,0.1forred),witheachclientpro-
cessingupto1000samplesperiteration. Theadaptiveapproachsignificantlyreducesthesamplesizewhileachieving
similarresultswithintheerrortolerance,leadingtosubstantialcomputationalsavings. Bothexperimentsarebasedon
thecasestudypricingwithdynamiccontribution,involvinganon-linearf (θ).
i
Impact of hyperparameters. We examine the effects of sample size n , learning rate η, and estimation window
i
H. Fig.2ashowsthatincreasingn improvesstabilityandbringsthealgorithmclosertotheoptimalpoint. Fig.2b
i
indicatesthatasmallerη slowsconvergencebutmovesθ nearertotheoptimum. Fig.2cdemonstratesthatasmaller
H resultsinestimatesestimatesof d df θi basedoncloserbutsmallergroupsoff(cid:98)iandθ i,resultinginconvergencecloser
totheperformativeoptimum,albeitataslightlyslowerspeed.
Table2: AccuracyofBinaryClassificationonSyntheticandRealisticDatasets
DATASET POFL PROFL
SameDistribution 88.00±2.69 99.23±0.29
DifferentDistributions 62.44±0.52 92.50±0.29
CreditDataset 94.73±0.00 94.73±0.00
AdultDataset 54.74±0.02 60.78±0.81
Convergencetotheperformativeoptimalsolution. Table2showsthatPROFLoutperformsPFLinthecasestudy
ofbinarystrategicclassificationwith10heterogeneousclients. Since PFL cannothandlecontaminateddata,weset
ϵ
i
= 0 for these experiments. The first two rows show that PROFL is more stable and significantly more accurate
than PFL on Gaussian synthetic data. PFL converges to a PS point rather than a PO point. On the Give Me Some
Creditdataset(thirdrow),bothalgorithmsconvergetothesamepoint,butPROFLhasafasterconvergencerate(see
Fig.7cintheAppendix). WiththeAdult dataset, PROFL achieves60.78%accuracy, outperforming PFL’s54.74%.
These results highlight PROFL’s practical advantages in both convergence speed and accuracy. More results are in
AppendixD.1.
A AlgorithmsandDetails
A.1 PseudocodeofthePOFL
Algorithm2presentsthepseudocodeofthePOFLalgorithmdiscussedinthemainpaper.
A.2 Outlierremovalmechanismforrobustgradient
Toestimate∇L ,weshouldaverage∇ℓ(z ;θt)oversamplesdrawnfromD (θt). Becausethesamplescollectedby
i,1 j i i i
clientiinroundtaredrawnfromP (θt),theyarenoisy,withanunknownϵ fractionofoutliersfromanunknowndis-
i i i
tributionQ i. PROFLneedstoidentifycontaminateddataandeliminatethecorrespondingcontaminatedgradients.We
proposeamechanismthatcanremovethesecontaminatedgradientsandestimategradientsthatarerobusttooutliers.
Importantly,PROFLisindependentofthespecificoutlieridentificationandremovalmechanismusedandcanfunction
with any such mechanism. In our paper, we assume that contaminated data can follow an arbitrary distribution. In
practice,additionalinformationaboutthecontaminationprocessmaybeavailablebasedontheapplicationorcontext,
allowingustodesignmoreefficient(andlessexpensive)outlieridentificationmechanismsleveragingthisknowledge.
10Algorithm2POFL
1: Input:Learningrateη,estimationwindowH,functionf(cid:98),totaliterationsT,localupdateiterationsR.
2: fort=0toT −1do
3: Ift mod R=0,serversendsθt toaselectedsetI ⊆Vofclientsandclienti∈I setsthelocalmodelθt =θt ;otherwise
t t i
I =I .
t t−1
4: fori∈I inparalleldo
t
5: Deployθtandlocalenvironmentchangesanddrawn samplesS :=(z )ni i∼idP (θt).
i i i j j=1 i i
6: ift≤Hthen
7: Computeestimatorf(cid:98)(S i)off it.
(cid:16) (cid:17)
8: Updateθ it+1 ←Proj
Θ
θ it−η∇(cid:98)L i,1(θ it) .
9: else
10: Computeestimatorf(cid:98)(S i)off it.
11: Formmatrices∆θ and∆f andcompute∆f (∆θ )†.
i i i i
(cid:16) (cid:16) (cid:17)(cid:17)
12: θ it+1 ←Proj
Θ
θ it−η ∇(cid:98)L i,1(θ it)+∇(cid:98)L i,2(θ it) .
13: Ift mod R=0,clienti∈I sendsθttotheserverandserverupdatesthemodelθt+1 =(cid:80) α θt+1.
t i i∈It i i
T
14: Output:θ
Algorithm3ROBUSTGRADIENT
1: Input:SetS ofsamples(z )ni ,thresholdsC,J ∈(0,1),modelθt ∈Θ
i j j=1 i
2: Let (cid:104)∇(cid:98) ← |S1 i|(cid:80)
zj∈S
(cid:105)i∇ℓ(z j;θ it).
3: Let ∇ℓ(z j;θ it)−∇(cid:98) bethe|S i|×dmatrixofcenteredgradients.
zj∈Si
4: ApplySVDtothismatrixandfindtoprightsingularvectorv.
5: Computetheoutlierscoreτ of∇ℓ(z ;θt)byEq.(9).
j j i
6: Dividetheinterval[0,maxτ ]intoBequal-lengthsegments.Letϕ bethenumberofτ inthek-thsegment.
j k j
7: Findthesmallestk∈{1,··· ,B}withϕ <C·|S |.Setϕasthelowerboundofthek-thsegment.
k i
8: FindthesetS′ ←{z ∈S :τ <ϕ}.
i j i j
9: Comparetheaveragegradients∇(cid:98) ofS iand∇(cid:98)′ofS i′.If ∥∇(cid:98) ∥− ∇(cid:98)∇(cid:98) ∥′∥ <J or|S i′|≤ n 2i,returnS i′and∇(cid:98)′;otherwise,setS
i
←S i′
andrepeatfromline2.
AsshowninAlgorithm3,ourmechanismtakesnoisysamplesS andcurrentlocalmodelθtasinputs.Themechanism
i i
firstcomputesthegradients∇ℓ(z ;θt)ofallsamplesanditerativelyidentifiesandremovesthecontaminatedgradients.
j i
SinceboththedistributionQ andfractionϵ ofcontaminateddataareunknown,weconsidergradientscontaminatedif
i i
theyexhibittwocrucialproperties:1)theyhavelargeeffectsonthelearnedmodelandcandisturbthetrainingprocess
(i.e., gradients have large magnitudes and systematically point in a specific direction); 2) they are located farther
awayfromtheaverageinthevectorspace(i.e.,“longtail”data). Todetectgradientsthatsatisfytheseproperties,we
adapttheapproachinShanetal.(2023),whichleveragessingularvaluedecomposition(SVD).Insteadofpreselecting
the threshold forthe sum of alloutlier scores τ , whichis difficult to determinewithout any prior knowledge ofthe
j
data, our threshold is based on the two crucial properties, making it much easier to predict the values of C and J.
Specifically,weconstructamatrix[∇ℓ(z j;θ it)−∇(cid:98)]
zj∈Si
usingthecenteredgradientsandfindthetoprightsingular
vector v, where ∇(cid:98) represents the average of all gradients (lines 2-4). The centered gradients that are closer to v are
morelikelytobeoutliersandweassigneachgradientanoutlierscoreτ asfollows:
j
(cid:18)(cid:16) (cid:17)T (cid:19)2
τ
j
= ∇ℓ(z j;θ it)−∇(cid:98) v (9)
Given the outlier scores {τ }, the gradients with scores above a threshold ϕ are considered contaminated and are
j
discarded. UnlikeShanetal.(2023)thatusesafixedpre-definedthreshold,ourmethodfindsϕautomaticallybased
ontheoutlierscoredistribution. Sinceweassumetheoutliersareinthe“longtail”ofthedistribution,wedividethe
interval[0,maxτ ]intoB equal-lengthsegmentsandcomputetherelativefrequencydistributionofscores. ϕisset
j
as the smallest segment with the relative frequency below a threshold (lines 6-7). For the remaining gradients with
τ
j
<ϕ,wecomputetheiraverage∇(cid:98)′(lines8-9).
11B AdditionalRelatedWork
B.1 PerformativePredictioninCentralizedSettings
Performativeprediction(Perdomoetal.,2020)wasfirstintroducedin2020correspondingtotheoptimizationframe-
work to deal with endogenous data distribution shifts where the model deployed to the environment will affect the
subsequent data, resulting in the collected data changing as the deployed model changes. Common applications of
performativepredictionincludestrategicclassification(Hardtetal.,2016;XieandZhang,2024)andpredictivepolic-
ing(Ensignetal.,2018).Perdomoetal.(2020)firstproposesaniterativeoptimizationprocedurenamedRepeatedRisk
Minimizationtofindaperformativestablepointandalsoboundedthedistancebetweentheperformativestablepoint
andtheperformativeoptimalpoint. Mendler-Du¨nneretal.(2020)designedthefirstalgorithmtofindtheperformative
stablepointundertheonlinelearningsetting. Milleretal.(2021)firstsolvedtheproblemofperformativeprediction
bydirectlyoptimizingperformancerisktofindtheperformativeoptimalpoint,butthescopeofthedistributionmapis
restricted.Mendler-Du¨nneretal.(2022)provedtheconvergenceofgreedydeploymentandlazydeploymentaftereach
randomupdateundertheassumptionofsmoothnessandstrongconvexityandgavetheconvergencerate. Izzoetal.
(2021)alsodesignedanalgorithmperformativegradientdescentforfindingtheperformativeoptimalsolution. Zhao
(2022)relaxestheassumptionofstrongconvexitytotheweakly-stronglyconvexcaseandprovestheconvergenceof
thedeployment. Somerstepetal.(2024);Bracaleetal.(2024)focusedonlearningthedistributionmapinperforma-
tivepredictionandproposedaframeworknamedReverseCausalPerformativePrediction. Inaddition,performative
predictionisalsorelatedtoreinforcementlearning(Zhengetal.,2022)andbanditproblems(Chenetal.,2024).
B.2 PerformativePredictioninDistributedSystem
Tothebestofourknowledge,Jinetal.(2024)istheonlyworkdiscussingperformativityunderthedistributedsetting,
wheretheygeneralizedFedavgtoP-Fedavgandprovedtheuniquenessoftheperformativestablesolutionfoundby
thealgorithm.Theyalsoquantifiedthedistancebetweentheperformativestablesolutionandtheperformativeoptimal
solution. Whiletheworkisabreakthroughasthefirsttotakemodel-dependentdistributionshiftsintoaccountunder
the FL setting, it is only a strict generalization of Perdomo et al. (2020) without the ability to find the performative
optimalpointanddealwiththecontaminateddata.
B.3 FederatedLearningunderDynamicData
Previous works (Tan et al., 2023; Li et al., 2020a) pointed out that the training paradigm of FedAvg may violate the
i.i.d. assumptionsandcausesomefeatureclassestobeover-representedwhileotherstobeunder-represented. Lietal.
(2020b,2021)steerthelocalmodelstowardsaglobalmodelbyaddingaregularizationtermtoguaranteeconvergence
whenthedatadistributionsamongdifferentclientsarenon-IID.Lietal.(2020b)proposesFedProxasasolution. Li
etal.(2021)aimsatobtaininggoodlocalmodelsonclientsratherthanaglobalconsensusmodel. Meanwhile,another
lineofworkfocusedondealingwiththestatisticalheterogeneitybyclustering(Briggsetal.,2020;Ghoshetal.,2020;
Sattleretal.,2020)byaggregatingclientswithsimilardistributionintothesamecluster. Whilemostpreviousworks
assumedthe“dynamic”isamongdifferentclients,anotherlineofliteraturefocusesonFLinadynamicenvironment
wherevariousdistributionshiftsoccur. Someworksaimtodealwithtime-varyingcontributionratesofclientswith
localheterogeneity(Sattleretal.,2021;Parketal.,2021;WangandJi,2022;Zhuetal.,2021).
B.4 Huber’sϵ-contaminationModel
ProposedbyHuber(1992),Huber’sepsiloncontaminationmodelservesasaframeworkforrobuststatisticalanalysis
withoutliersandcontaminationsinthedataset. Thismodelassumesthatbesidesthedatadistributionweaimtolearn,
asmallproportionofsamplescancomefromanarbitrarydistribution. Thiscontaminationframeworkiscrucialfor
creatingmachinelearningalgorithmsrobusttooutliers,therebyensuringmorereliableanalysisinpracticalscenarios
wheredataimperfectionsarecommon.Huber’smodelhasbeenfundamentalinthefieldofrobuststatistics,influencing
awiderangeofapplicationsandsubsequentresearch. Specifically,denoteD asthetruedatadistributionandQasa
randomdistribution. P =(1−ϵ)D+QisthedistributionwithsomecorruptionQandtheportionofcorruptiondata
isϵ. ThelocaldatasetofclientiisP =(1−ϵ )D (θ )+ϵ Q .
i i i i i i
12C AdditionalProofsnotDetailedintheMainPaper
C.1 ProofofTheorem5.10
Proof. WithAssumption5.1ofγ -sensitivityandAssumption5.2ofL-smoothness,weobtain
i
∥∇ℓ(z;θ)−∇ℓ(z′;θ′)∥≤L(∥θ−θ′∥+∥z−z′∥)≤L(1+γ )∥θ−θ′∥
i
Thenwehave
L(θt+1 )≤L(θt )+⟨∇L(θt ),θt+1 −θt
⟩+
L(cid:80)N
i=1α i(1+γ i) ∥θt+1 −θt ∥2
,
2
whereθt+1 −θt =−gtηandgt =(cid:80)N α gt.
i=1 i i
Bytakingexpectationonbothsidesweobtainthat
E[L(θt+1
)]
≤E[L(θt )]+E(cid:104) ⟨∇L(θt ),θt+1 −θt ⟩(cid:105)
+
L(cid:80)N
i=1α i(1+γ i) E(cid:104) ∥θt+1 −θt ∥2(cid:105)
2
=E[L(θt )]−E(cid:104) ⟨∇L(θt ),gt⟩(cid:105) η+
L(cid:80)N
i=1α i(1+γ i) E(cid:2) ∥gt∥2(cid:3) η2. (10)
2
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
T1 T2
Let’sstarttofindtheupperboundofT . WithRstepslocalupdates,byusingJensen’sinequality,weobtainthat
2
E(cid:2) ∥gt∥2(cid:3)
(cid:13) (cid:13)2
(cid:13)(cid:88)N (cid:13)
=E (cid:13) (cid:13) α ig it(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1
(cid:13) (cid:13)2
≤E
(cid:13)
(cid:13)
(cid:13)(cid:88)N
α i(cid:0) g it−∇L i(θ it)+∇L i(θ
it)(cid:1)(cid:13)
(cid:13) (cid:13) 
(cid:13) (cid:13)
i=1
(cid:13) (cid:13)2 (cid:13) (cid:13)2
≤2E
(cid:13)
(cid:13)
(cid:13)(cid:88)N
α i(cid:0) g it−∇L i(θ
it)(cid:1)(cid:13)
(cid:13) (cid:13) +2E
(cid:13)
(cid:13)
(cid:13)(cid:88)N
α i∇L i(θ
it)(cid:13)
(cid:13) (cid:13) 
(cid:13) (cid:13) (cid:13) (cid:13)
i=1 i=1
(cid:13) (cid:13)2
≤2E
(cid:13)
(cid:13)
(cid:13)(cid:88)N
α i(cid:0) g it−∇L i(θ
it)(cid:1)(cid:13)
(cid:13)
(cid:13)
+2G2.
(cid:13) (cid:13)
i=1
ThenwestarttofindtheupperboundofT .
1
(cid:104) (cid:105)
−E ⟨∇L(θt ),gt⟩
=−1 2E(cid:20)(cid:13) (cid:13) (cid:13)∇L(θt )(cid:13) (cid:13) (cid:13)2(cid:21) − 1 2E(cid:104)(cid:13) (cid:13)gt(cid:13) (cid:13)2(cid:105) + 21 E(cid:20)(cid:13) (cid:13) (cid:13)∇L(θt )−gt(cid:13) (cid:13) (cid:13)2(cid:21)
=−1 2E(cid:20)(cid:13) (cid:13) (cid:13)∇L(θt )(cid:13) (cid:13) (cid:13)2(cid:21) − 1 2E(cid:104)(cid:13) (cid:13)gt(cid:13) (cid:13)2(cid:105) + 21
E (cid:13)
(cid:13) (cid:13) (cid:13)∇L(θt )−(cid:88)N α i∇L i(θ it)+(cid:88)N α i∇L i(θ
it)−gt(cid:13)
(cid:13) (cid:13)
(cid:13)2

(cid:13) (cid:13)
i=1 i=1
≤−ρE(cid:104) L(θt )−L(θPO )(cid:105) − 1 2E(cid:104)(cid:13) (cid:13)gt(cid:13) (cid:13)2(cid:105)
+E (cid:13)
(cid:13) (cid:13) (cid:13)(cid:88)N α i(cid:16) ∇L i(θt )−∇L i(θ
it)(cid:17)(cid:13)
(cid:13) (cid:13)
(cid:13)2 +E (cid:13)
(cid:13) (cid:13) (cid:13)(cid:88)N α i∇L i(θ
it)−gt(cid:13)
(cid:13) (cid:13)
(cid:13)2

(cid:13) (cid:13) (cid:13) (cid:13)
i=1 i=1
≤−ρE(cid:104) L(θt )−L(θPO )(cid:105) − 1 2E(cid:104)(cid:13) (cid:13)gt(cid:13) (cid:13)2(cid:105) +L2E(cid:34) (cid:88)N α i(1+γ i)2(cid:13) (cid:13) (cid:13)θt −θ it(cid:13) (cid:13) (cid:13)2(cid:35) +E (cid:13) (cid:13) (cid:13) (cid:13)(cid:88)N α i∇L i(θ it)−gt(cid:13) (cid:13) (cid:13) (cid:13)2 
(cid:13) (cid:13)
i=1 i=1
≤−ρE(cid:104) L(θt )−L(θPO )(cid:105) − 1 2E(cid:104)(cid:13) (cid:13)gt(cid:13) (cid:13)2(cid:105) +L2(R−1)2G2η2(cid:0) (1+γ)2+σ γ2(cid:1)
+E (cid:13)
(cid:13) (cid:13) (cid:13)(cid:88)N α i∇L i(θ
it)−gt(cid:13)
(cid:13) (cid:13)
(cid:13)2
.
(cid:13) (cid:13)
i=1
13ThefirstinequalityarisesfromtheapplicationofJensen’sinequalitytotwoterms. Thesecondinequalityarisesfrom
Assumption5.2. ThelastinequalityarisesformLemmaC.1.
PO
Finally,bringT andT intoEq.(10)andsubtractL(θ )frombothsides,wecangetthat
1 2
E[L(θt+1 )−L(θPO)]
≤(1−ρη)E[L(θt )−L(θPO)]+
L2(R−1)2G2η3
(cid:0) (1+γ)2+σ2(cid:1) +LG2η2(1+γ)
2 γ
(cid:13) (cid:13)2  (cid:13) (cid:13)2 
+Lη2(1+γ)E (cid:13) (cid:13) (cid:13)(cid:88)N α i∇L i(θ it)−gt(cid:13) (cid:13)
(cid:13)
+E |(cid:13) (cid:13) (cid:13)(cid:88)N α i∇L i(θ it)−gt(cid:13) (cid:13)
(cid:13)
− 21(cid:13) (cid:13)gt(cid:13) (cid:13)2 η,
(cid:13) (cid:13) (cid:13) (cid:13)
i=1 i=1
whereγ =(cid:80)N α γ andσ2 =(cid:80)N α (γ−γ )2.
i=1 i i γ i=1 i i
DenoteE[L(θt )−L(θPO)]asa . Thenwecanobtainthat
t
a ≤(1−ρη)a +
L2(R−1)2G2η3
(cid:0) (1+γ)2+σ2(cid:1)
t+1 t 2 γ
+LG2η2(1+γ)+Lη2(1+γ)E (cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:88)N
α i∇L i(θ
it)−gt(cid:13)
(cid:13)
(cid:13)
(cid:13)2

(cid:13) (cid:13)
i=1
+E (cid:13) (cid:13) (cid:13) (cid:13)(cid:88)N α i∇L i(θ it)−gt(cid:13) (cid:13) (cid:13) (cid:13)2 − 21(cid:13) (cid:13)gt(cid:13) (cid:13)2 η
(cid:13) (cid:13)
i=1
=(1−ρη)a t+E (cid:13) (cid:13) (cid:13) (cid:13)(cid:88)N α i∇L i(θ it)−gt(cid:13) (cid:13) (cid:13) (cid:13)2 − 21(cid:13) (cid:13)gt(cid:13) (cid:13)2 η
(cid:13) (cid:13)
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
T4
+
21
L2(R−1)2G2η(cid:0) (1+γ)2+σ γ2(cid:1)
+2LG2(1+γ)+2L(1+γ)E (cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:88)N
α i∇L i(θ
it)−gt(cid:13)
(cid:13)
(cid:13)
(cid:13)2 
η2. (11)
(cid:13) (cid:13)
i=1
(cid:124) (cid:123)(cid:122) (cid:125)
T3
Denote the empirical result of the performative gradient of client i in global iteration t and local iteration r as
∇ˆL (θt)=∇ˆL (θt)+∇ˆL (θt). AccordingtoLemmaC.2,wehave
i i i,1 i i,2 i
(cid:13) (cid:13)2
E (cid:13) (cid:13) (cid:13)(cid:88)N α i∇L i(θ it)−gt(cid:13) (cid:13)
(cid:13)
≤(cid:88)N α iE(cid:104)(cid:13) (cid:13)∇L i(θ it)−g it(cid:13) (cid:13)2(cid:105)
(cid:13) (cid:13)
i=1 i=1
≤(1−ϵ)ωωω 2+ϵ(LW (D,Q) +ωωω +ωωω )2.
DDD 1 max QQQ DDD
where ϵ = (cid:80)N α ϵ ,σ2 = (cid:80)N α (ϵ−ϵ )2, andωωω ,ωωω are the maximum ofωωω andωωω of all clients and all
i=1 i i ϵ i=1 i i QQQ DDD QQQ DDD
iterations. WecanfindtheupperboundofT .
3
T 3 ≤
21
L2(R−1)2G2η(cid:0) (1+γ)2+σ γ2(cid:1)
+2LG2(1+γ)+2L(1+γ)E (cid:13)
(cid:13)
(cid:13)
(cid:13)(cid:88)N
α i∇L i(θ
it)−gt(cid:13)
(cid:13)
(cid:13)
(cid:13)2 

(cid:13) (cid:13)
i=1
≤
1(cid:0) L2(R−1)2G2η(cid:0) (1+γ)2+σ2(cid:1)(cid:1) +L(1+γ)(cid:16)
(1−ϵ)ωωω
2+ϵ(cid:0)
LW (D,Q) +ωωω +ωωω
(cid:1)2+G2(cid:17)
.
2 γ DDD 1 max QQQ DDD
Let
C =(cid:32) L2(R−1)2G2η(cid:0) (1+γ)2+σ γ2(cid:1) +L(1+γ)(cid:16) (1−ϵ)ωωω 2+ϵ(cid:0) LW (D,Q) +ωωω +ωωω (cid:1)2+G2(cid:17)(cid:33) η2
2 DDD 1 max QQQ DDD
+(cid:16)
(1−ϵ)ωωω
2+ϵ(cid:0)
LW (D,Q) +ωωω +ωωω
(cid:1)2(cid:17)
η.
DDD 1 max QQQ DDD
14Wewillobtain
a ≤(1−ρη)a +C
t+1 t
and
1−(1−ρη)t 1−(1−ρη)t
a ≤(1−ρη)ta + C =(1−ρη)ta + C ≤(1−ρη)ta +C,
t 0 1−(1−ρη) 0 ρη 0
whichmeans
E[L(θt )−L(θPO)]≤(1−ρη)tE[L(θ0 )−L(θPO)]+C.
C.2 ProofofLemma5.7
Proof. d(cid:98)f is the estimate of df. To simplify the notation in this proof we denote f (z) as f and
dθ dθ iz∼Di(θ it) t
∇f
iz∼Di(θ
it)(z) as ∇f t. f(cid:98)t is the estimate of f(θ). Because f(cid:98)t is estimated by data sampled from P i(θ it) =
(1−ϵ )D (θt)+ϵ Q andweestimatethemeanofmixtureGaussianorthepofBinomialdistributionX ∼(n,p)we
i i i i i
have
f(cid:98)r =f(cid:98)iz∼Pi(θ it)(z)
=(1−ϵ i)f(cid:98)iz∼Di(θ it)(z)+ϵ if(cid:98)iz∼Qi(z)
(cid:16) (cid:17)
=(1−ϵ i) f
iz∼Di(θ
it)(z)+errt
i
+ϵ if(cid:98)iz∼Qi(z),
whereerrt istheerrorresultinsamplingofclientioniterationtand[errt] = 0forallt ∈ [0,T −1].Similarlywe
i i
havedenoteθtasθ ,θt−1asθ ,errtaserr ,anderrt−1aserr .
i t i t−1 i t i t−1
Foreach1≤j ≤dbyanapproximationofTaylor’sseries(ignoringhigherorderterms),weobtain
1
f −f =∇f⊺ (θ −θ )+ (θ −θ )⊺ ∇2f (ξ )(θ −θ ),
t,j t−1,j t−1,j t t−1 2 t t−1 i t−1,j t t−1
whereξ liesonthelinesegmentjoiningθ andθ .
t−1,j t−1 t
Denote
 
a
t−k,1
a = 1 (θ −θ )⊺ ∇2f (ξ )(θ −θ ) and a = . . 
t−k,j 2 t−k+1 t−k i t−k,j t−k+1 t−k k  . 
a
t−k,d
Thenwehave
df
f −f = (θ −θ )+a .
t t−1 dθ t t−1 1
ByusingtheupdateruleofAlgorithm1weobtainthat
f(cid:98)t−f(cid:98)t−1 =(1−ϵ i)(f t−f t−1)+ϵ i(f z∼Qi(z)−f z∼Qi(z))+(1−ϵ i)(err t−err t−1)
 
f −f
t,1 t−1,1
=(1−ϵ ) . . +(err −err ).
i  .  t t−1
f −f
t,d t−1,d
Thenwecanwrite∆ f intermsof df.
1 dθ
(cid:18) (cid:19)
df
∆ 1f =f(cid:98)t−f(cid:98)t−1 =(1−ϵ i) dθ(θ t−θ t−1)+a 1+(err t−err t−1) .
15Similarly,wecanobtain
k
(cid:88)(cid:16) (cid:17)
∆ kf =f(cid:98)t−f(cid:98)t−k = f(cid:98)t−q+1−f(cid:98)t−q
q=1
k
df df (cid:88)
=(1−ϵ ) (θ −θ )+(1−ϵ ) a +(1−ϵ )(err −err ),
i dθ t t−k i dθ q i t t−k
q=1
where1≤k ≤H.
Nowwecanget
 
∆f = ∆ 1f ... ∆ Hf ;
 
∆θ = θ t−θ t−1 ... θ t−θ t−H 
 
= ∇L z∼Pi(θt−1)(z,θ t−1) ... (cid:80)H k=1∇L z∼Pi(θt−k)(z,θ t−k) η.
Definematrices
   
A= a 1 ... (cid:80)H k=1a k  and E = err t−err t−1 ... err t−err t−H .
Thenwecanfindthedifferencebetween d(cid:98)f and df :
dθ dθ
d(cid:98)f df df df
− =∆f(∆θ)†− =−ϵ +(1−ϵ )A(∆θ)†+(1−ϵ )E(∆θ)†
dθ dθ dθ idθ i i
16andwecanobtainthat
(cid:13) (cid:13)2
(cid:13)d(cid:98)f df(cid:13)
E (cid:13) (cid:13)dθ − dθ(cid:13) (cid:13) 
(cid:13) (cid:13)
=E(cid:34)(cid:13)
(cid:13) (cid:13) (cid:13)−ϵ iddf
θ
+(1−ϵ i)A(∆θ)†+(1−ϵ
i)E(∆θ)†(cid:13)
(cid:13) (cid:13)
(cid:13)2(cid:35)
=E(cid:34)(cid:13)
(cid:13)
(cid:13) (cid:13)−ϵ
iddf
θ
+(1−ϵ
i)A(∆θ)†(cid:13)
(cid:13)
(cid:13)
(cid:13)2(cid:35)
+(1−ϵ
i)2E(cid:104)(cid:13) (cid:13)E(∆θ)†(cid:13) (cid:13)2(cid:105)
(12)
≤2ϵ2
iE(cid:34)(cid:13)
(cid:13) (cid:13) (cid:13)ddf
θ(cid:13)
(cid:13) (cid:13)
(cid:13)2(cid:35)
+2(1−ϵ i)2E(cid:104)(cid:13) (cid:13)A(∆θ)†(cid:13) (cid:13)2(cid:105) +(1−ϵ i)2E(cid:104)(cid:13) (cid:13)E(∆θ)†(cid:13) (cid:13)2(cid:105) (13)
≤2ϵ2
iE(cid:34)(cid:13)
(cid:13) (cid:13) (cid:13)ddf
θ(cid:13)
(cid:13) (cid:13)
(cid:13)2(cid:35)
+(1−ϵ i)2(cid:16) 2E(cid:104) ∥A∥2 F(cid:105) +E(cid:104) ∥E∥2 F(cid:105)(cid:17)(cid:13) (cid:13)∆θ†(cid:13) (cid:13)2 (14)
≤2ϵ2
iE(cid:34)(cid:13)
(cid:13) (cid:13) (cid:13)ddf
θ(cid:13)
(cid:13) (cid:13)
(cid:13)2(cid:35)
+(1−ϵ
i)22E (cid:88)H
∥θ t−θ
t−k∥4 (cid:88)d
(cid:13) (cid:13)∇2f i(ξ t−k,j)(cid:13)
(cid:13)2 
(cid:13) (cid:13)∆θ†(cid:13) (cid:13)2
k=1 j=1
(cid:34) H (cid:35)
+(1−ϵ i)2E (cid:88) ∥err t−err t−k∥2 (cid:13) (cid:13)∆θ†(cid:13) (cid:13)2
k=1
≤2ϵ2
i
(cid:13) (cid:13) (cid:13) (cid:13)ddf θ(cid:13) (cid:13) (cid:13) (cid:13)2 +(cid:32) (1−ϵ i)2M 22η4G4H6 +(1−ϵ i)2E(cid:34) (cid:88)H ∥err t−err t−k∥2(cid:35)(cid:33) (cid:13) (cid:13)∆θ†(cid:13) (cid:13)2
k=1
≤2ϵ2
i
(cid:13) (cid:13) (cid:13) (cid:13)ddf θ(cid:13) (cid:13) (cid:13) (cid:13)2 +(cid:32) (1−ϵ i)2M 22η4G4H6 +(1−ϵ i)2E(cid:34) (cid:88)H (cid:16) ∥err t∥2+∥err t−k∥2(cid:17)(cid:35)(cid:33) (cid:13) (cid:13)∆θ†(cid:13) (cid:13)2 (15)
k=1
(cid:32) (cid:33)
M2η4G4H6 2Hd∥err∥2
≤2ϵ2F2+(1−ϵ )2 + i λ−2 . (16)
i i 2 η2 i,min
Where∥.∥ denotestheFrobeniusnorm. (12)and(15)arisefromthepropertythatE[err ]= 0forallt ∈ [0,T]and
F t
err is independent to a and df. (13) arises from Jensen’s inequality of two terms. (14) arises form the properties
t k dθ (cid:13) (cid:13)
of matrix norm that if A and B are two matrix ∥AB∥ ≤ ∥A∥∥B∥ and ∥A∥ ≤ ∥A∥ . (16) arises form (cid:13)∆θ†(cid:13) ≤
F
∥∆θ∥−1 ≤ λ−1 , whereλ isthesmallestsingularvaluesof∆θ ofclientiforallt ∈ [0,T]and∥err∥2 isthe
i,min i,min i
maximumof∥err ∥2 forallt ∈ [0,T]. Because∥err ∥2 dependsonthedatadistribution,theestimationmethods,the
t t
numberofsamplesn ,aswellastheprobabilityφ ofPr(∥err ∥≤1−φ ),weusef (φ ;n )torepresentthisterm.
i i t i φ i i
Finallyweobtrain
E (cid:13)
(cid:13)
(cid:13)
(cid:13)dd(cid:98)f
θ
−
ddf
θ(cid:13)
(cid:13)
(cid:13)
(cid:13)2
≤2ϵ2 iF2+(1−ϵ
i)2(cid:18) M2η4 2G4H6
+
2Hdf
φ
η( 2φ i;n i)(cid:19)
λ− i,m2 in.
(cid:13) (cid:13)
C.3 ProofofLemma5.9
(cid:20)(cid:13) (cid:13)2(cid:21)
Proof. FirstwestarttofindtheupperboundofE (cid:13) (cid:13)∇L i,2(θ it)−∇(cid:98)L
i,2z∼Di(θ
it)(z,θ it)(cid:13)
(cid:13)
.
17ByEq.3weobtainthat
(cid:20)(cid:13) (cid:13)2(cid:21)
E (cid:13) (cid:13)∇L i,2(θ it)−∇(cid:98)L
i,2z∼Di(θ
it)(z,θ it)(cid:13)
(cid:13)
(cid:13) (cid:16) (cid:17)(cid:13)2
=E (cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)ℓ(Z i;θ it)d df i θ,t⊺ ∂logp ∂f(Z i(i θ; itf )i(θ it)) −ℓ(cid:98)(Z i;θ it)d(cid:100) df i θ,t⊺∂logp ∂f(cid:98)Z i(i θ; itf )(cid:98)i(θ it) (cid:13) (cid:13) (cid:13)
(cid:13)
(cid:13)
 .
Whenthelocalclient’ssamplesizen →∞,wehave
i
(cid:20)(cid:13) (cid:13)2(cid:21)
E (cid:13) (cid:13)ℓ(Z i;θ)−ℓ(cid:98)(Z i;θ)(cid:13)
(cid:13)
→0
(cid:13) (cid:16) (cid:17)(cid:13)2
E(cid:13)
(cid:13) (cid:13)∂logp(Z i;f i(θ)) −
∂logp Z i;f(cid:98)i(θ) (cid:13)
(cid:13) (cid:13) →0
(cid:13)
(cid:13)
∂f i(θ) ∂f(cid:98)i(θ) (cid:13)
(cid:13)

Thereby,E(cid:34)(cid:13) (cid:13) (cid:13) (cid:13)ℓ(Z i;θ it)df di θ,t⊺∂logp ∂( fZ i(i θ; if t)i(θ it)) −ℓ(cid:98)(Z i;θ it)d(cid:100)f di θ,t⊺ ∂logp ∂( f(cid:98)Z i(i θ; if t(cid:98) )i(θ it))(cid:13) (cid:13) (cid:13) (cid:13)2(cid:35) mainlyincludestwoparts.
Thefirstpartisthefollowing:
E (cid:13) (cid:13)ℓ(Z i;θ it)(cid:13) (cid:13)2(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)d df i θ,t − d(cid:100) df i θ,t(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2(cid:13) (cid:13) (cid:13) (cid:13)∂logp ∂f(Z i(i θ; itf )i(θ it))(cid:13) (cid:13) (cid:13) (cid:13)2 
≤ℓ2 max(cid:13) (cid:13) (cid:13) (cid:13)∂logp ∂f(Z i(i θ; itf )i(θ it))(cid:13) (cid:13) (cid:13) (cid:13)2 maxE (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)d df i θ,t − d(cid:100) df i θ,t(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2  (17)
=O(cid:0) ℓ2 d(ωωω )(cid:1) . (18)
max FFF
BecauseΘisaclosedandboundedsetandacontinuousfunctionhasamaximumvalueonΘ,thereisanupperbound
(cid:13) (cid:13)2 (cid:13) (cid:13)2
ℓ2
max
of ∥ℓ(Z i;θ)∥2 and an upper bound (cid:13) (cid:13) (cid:13)∂logp ∂( fZ i(i θ; if t)i(θ it))(cid:13) (cid:13)
(cid:13)
max
of (cid:13) (cid:13) (cid:13)∂logp ∂( fZ i(i θ; if t)i(θ it))(cid:13) (cid:13)
(cid:13)
. (18) arises from the upper
(cid:13) (cid:13)2
boundof(cid:13) (cid:13)∂logp(Zi;fi(θ it))(cid:13)
(cid:13) isO(d)anditisrelatedtothecovarianceΣofthedatafortheGaussiandistribution.
(cid:13) ∂fi(θ it) (cid:13)
The second part will decrease to zero as n → 0. The upper bound of this part is O(cid:0) ℓ2 dF2f (φ ;n )(cid:1) by using
i max φ i i
Lemma12inIzzoetal.(2021). Thenwecanobtain
E(cid:20)(cid:13)
(cid:13) (cid:13)∇L i,2(θ it)−∇(cid:98)L
i,2z∼Di(θ
it)(z,θ
it)(cid:13)
(cid:13)
(cid:13)2(cid:21)
≤O(cid:0) ℓ2 maxd(cid:0) (ωωω FFF)+F2f φ(φ i;n i)(cid:1)(cid:1) .
(cid:20)(cid:13) (cid:13)2(cid:21)
NextwefindtheupperboundofE (cid:13) (cid:13)∇L i,2(Q i,θ it)−∇(cid:98)L i,2(Q i,θ it)(cid:13)
(cid:13)
.
18(cid:20)(cid:13) (cid:13)2(cid:21)
E (cid:13) (cid:13)∇L i,2(Q i,θ it)−∇(cid:98)L i,2(Q i,θ it)(cid:13)
(cid:13)
(cid:20)(cid:13) (cid:13)2(cid:21)
=E (cid:13) (cid:13)∇(cid:98)L i,2(Q i,θ it)(cid:13)
(cid:13)
(cid:13) (cid:16) (cid:17)(cid:13)2
=E
(cid:13)
(cid:13) (cid:13)
(cid:13)
(cid:13)ℓ z∼Qi(Z i;θ it)d(cid:100) df i
θ,t⊺∂logp ∂f(cid:98)Z
i(i
θ; itf )(cid:98)i(θ it) (cid:13)
(cid:13) (cid:13)
(cid:13)
(cid:13)


(19)
 (cid:13) (cid:13)2
≤Oℓ2 maxdE
(cid:13)
(cid:13)
(cid:13)d(cid:100) df
i
θ,t(cid:13)
(cid:13) (cid:13)  (20)
(cid:13) (cid:13)
≤O 2ℓ2 maxd E(cid:34)(cid:13) (cid:13) (cid:13) (cid:13)d df i θ,t(cid:13) (cid:13) (cid:13) (cid:13)2(cid:35) +E (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)d df i θ,t − d(cid:100) df i θ,t(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)2    (21)
=O(cid:0) 2ℓ2 d(cid:0) (ωωω )+F2(cid:1)(cid:1) . (22)
max FFF
(19)arisesfromEq.(3). (20)arisesfromLemma10ofIzzoetal.(2021)andLemma5.6. (21)arisesfromJensen’s
inequalityoftwoterms. (22)arisesfromLemma5.7.
C.4 AdditionalLemmasandtheirProofs
LemmaC.1. Letγ =(cid:80)N α γ andσ2 bethevarianceofγ ofallclients.
ThenE(cid:20)
(cid:80)N α (1+γ
)2(cid:13)
(cid:13)θt
−θt(cid:13) (cid:13)2(cid:21)
i=1 i i γ i i=1 i i (cid:13) i(cid:13)
isupperboundedby(R−1)2G2η2(cid:0) (1+γ)2+σ2(cid:1)
.
γ
ProofofLemmaC.1.
E(cid:34) (cid:88)N
α (1+γ
)2(cid:13)
(cid:13)θt
−θt(cid:13) (cid:13)2(cid:35) =(cid:88)N
α (1+γ
)2E(cid:20)(cid:13)
(cid:13)θt
−θt(cid:13) (cid:13)2(cid:21)
i i (cid:13) i(cid:13) i i (cid:13) i(cid:13)
i=1 i=1
(cid:13) (cid:13)2
N (cid:13) N t t (cid:13)
=η2(cid:88) α i(1+γ i)2E (cid:13) (cid:13) (cid:13)(cid:88) α i (cid:88) ∇L i(θ ij)− (cid:88) ∇L i(θ ij)(cid:13) (cid:13) (cid:13)  
i=1 (cid:13)i=1 j=t−b j=t−b (cid:13)
(cid:13) (cid:13)2
N (cid:13) t (cid:13)
≤η2(cid:88) α i(1+γ i)2E (cid:13) (cid:13)
(cid:13)
(cid:88) ∇L i(θ it)(cid:13) (cid:13)
(cid:13)


i=1 (cid:13)j=t−b (cid:13)
N
(cid:88)
≤c2G2η2 α (1+γ )2
i i
i=1
N
(cid:88)
≤(R−1)2G2η2 α (1+γ )2.
i i
i=1
c = t mod R. The first inequality rises from E(cid:2) ∥E[X]−X∥2(cid:3) ≤ E[X2]. The second inequality arises from
Lemma5.6andJensen’sinequality.
Denote(cid:80)N α γ asγ andvarianceofγ asσ2. Because
i=1 i i i γ
N
(cid:88) α (1+γ )2 =E(cid:2) (1+γ )2(cid:3)
i i i
i=1
=(E[1+γ ])2+V[1+γ ]
i i
=(E[1+γ ])2+V[γ ]=(1+γ)2+σ2
i i γ
19wewillfinallyobtain
E(cid:34) (cid:88)N
α (1+γ
)2(cid:13)
(cid:13)θt
−θt(cid:13) (cid:13)2(cid:35)
≤(R−1)2G2η2(cid:0) (1+γ)2+σ2(cid:1)
.
i i (cid:13) i(cid:13) γ
i=1
LemmaC.2. IfthereiscontaminateddatawithdistributionQ andfractionϵ ,withAssumption5.2,wecanobtain
i i
that
E(cid:104)(cid:13) (cid:13)∇L i(θ it)−g it(cid:13) (cid:13)2(cid:105) ≤(1−ϵ i)(ωωω DDD)2+ϵ i(LW 1(D,Q) max+(ωωω QQQ)+(ωωω DDD))2.
The notation (ωωω ) and (ωωω ) is defined in Lemmas 5.8 and 5.9. W (D,Q) is the largest W distance between
DDD QQQ 1 max 1
D (θ)andQ forallclientsi∈V amongalliterationt∈[0,T −1].
i i
ProofofLemmaC.2. FromEq.(3)wecanobtain
∇L i(θ it)−g it =∇L i,1(θ it)−∇(cid:98)L i,1(θ it)+∇L i,2(θ it)−∇(cid:98)L i,2(θ it)
(cid:16) (cid:17)
=(1−ϵ i) ∇L i,1(θ it)−∇(cid:98)L i,1(D i(θ it),θ it)+∇L i,2(θ it)−∇(cid:98)L i,2(D i(θ it),θ it)
+ϵ (cid:0) ∇L (θt)−∇L (Q ,θt)+∇L (θt)−∇L (Q ,θt)(cid:1)
i i,1 i i,1 i i i,2 i i,2 i i
(cid:16) (cid:17)
+ϵ
i
∇L i,1z∼Qi(z,θ it)−∇(cid:98)L i,1(Q i,θ it)+∇L i,2(Q i,θ it)−∇(cid:98)L i,2(Q i,θ it)
(cid:16) (cid:17)
=(1−ϵ i) ∇L i,1(θ it)−∇(cid:98)L i,1(D i(θ it),θ it)+∇L i,2(θ it)−∇(cid:98)L i,2(D i(θ it),θ it)
+ϵ i(cid:0) ∇L i(θ it)−∇L iz∼Qi(z,θ it)(cid:1) −ϵ i(cid:16) ∇(cid:98)L i,2z∼Qi(z,θ it)(cid:17) .
The last equality arises form ∇L i,1(Q i,θ it) − ∇(cid:98)L i,1z∼(Q i,θ it) = 0and∇L i,2(Q i,θ it) = 0. Because
(cid:104) (cid:105)
E ∇L i,1(θ it)−∇(cid:98)L i,1(D i(θ it),θ it)] =0andAssumption5.2ofL-smoothness.
TheproofoftheLemma12ofIzzoetal.(2021)showsthat
(cid:20)(cid:13) (cid:13)2(cid:21)
E (cid:13) (cid:13)∇L i,1(θ it)−∇(cid:98)L i,1(D i(θ it),θ it)(cid:13)
(cid:13)
=O(f φ(φ i;n i)).
20Finally,wecanobtainthat
E(cid:104)(cid:13) (cid:13)∇L i(θ it)−g it(cid:13) (cid:13)2(cid:105)
(cid:20)(cid:13) (cid:13)2(cid:21) (cid:20)(cid:13) (cid:13)2(cid:21)
=(1−ϵ i)2E (cid:13) (cid:13)∇L i,1(θ it)−∇(cid:98)L i,1(D i(θ it),θ it)(cid:13)
(cid:13)
+(1−ϵ i)2E (cid:13) (cid:13)∇L i,2(θ it)−∇(cid:98)L i,2(D i(θ it),θ it)(cid:13)
(cid:13)
(cid:104)(cid:68) (cid:69)(cid:105)
+(1−ϵ i)2ϵ iE ∇L i,1(θ it)−∇(cid:98)L i,1(D i(θ it),θ it),∇L i,2(θ it)−∇(cid:98)L i,2(D i(θ it),θ it)
+ϵ2 iE(cid:104)(cid:13) (cid:13)∇L i(θ it)−∇L i(Q i,θ it)(cid:13) (cid:13)2(cid:105) +ϵ2 iE(cid:20)(cid:13) (cid:13) (cid:13)∇(cid:98)L i,2(Q i,θ it)(cid:13) (cid:13) (cid:13)2(cid:21)
(cid:104)(cid:68) (cid:69)(cid:105)
+2(1−ϵ i)ϵ iE ∇L i,2(θ it)−∇(cid:98)L i,2(D i(θ it),θ it),∇L i(θ it)−∇L i(Q i,θ it)
(cid:104)(cid:68) (cid:69)(cid:105)
−2(1−ϵ i)ϵ iE ∇L i,2(θ it)−∇(cid:98)L i,2(D i(θ it),θ it),∇(cid:98)L i,2(Q i,θ it)
(cid:104)(cid:68) (cid:69)(cid:105)
−ϵ2 iE ∇L i(θ it)−∇L i(Q i,θ it),∇(cid:98)L i,2(Q i,θ it)
(cid:20)(cid:13) (cid:13)2(cid:21) (cid:20)(cid:13) (cid:13)2(cid:21)
≤(1−ϵ i)2E (cid:13) (cid:13)∇L i,1(θ it)−∇(cid:98)L i,1(D i(θ it),θ it)(cid:13)
(cid:13)
+(1−ϵ i)2E (cid:13) (cid:13)∇L i,2(θ it)−∇(cid:98)L i,2(D i(θ it),θ it)(cid:13)
(cid:13)
(cid:104)(cid:13) (cid:13)(cid:13) (cid:13)(cid:105)
+(1−ϵ i)2E (cid:13) (cid:13)∇L i,1(θ it)−∇(cid:98)L i,1(D i(θ it),θ it)(cid:13) (cid:13)(cid:13) (cid:13)∇L i,2(θ it)−∇(cid:98)L i,2(D i(θ it),θ it)(cid:13)
(cid:13)
+ϵ2 iL2(cid:13) (cid:13)D i(θ it)−Q i(cid:13) (cid:13)2 +ϵ2 iE(cid:20)(cid:13) (cid:13) (cid:13)∇(cid:98)L i,2(Q i,θ it)(cid:13) (cid:13) (cid:13)2(cid:21)
+2(1−ϵ i)ϵ iL(cid:13) (cid:13)D i(θ it)−Q i(cid:13) (cid:13)E(cid:104)(cid:13) (cid:13) (cid:13)∇L i,2(θ it)−∇(cid:98)L i,2(D i(θ it),θ it)(cid:13) (cid:13) (cid:13)(cid:105)
(cid:104)(cid:13) (cid:13)(cid:13) (cid:13)(cid:105)
+2(1−ϵ i)ϵ iE (cid:13) (cid:13)∇L i,2(θ it)−∇(cid:98)L i,2(D i(θ it),θ it)(cid:13) (cid:13)(cid:13) (cid:13)∇(cid:98)L i,2(Q i,θ it)(cid:13)
(cid:13)
+ϵ2 iL(cid:13) (cid:13)D i(θ it)−Q i(cid:13) (cid:13)E(cid:104)(cid:13) (cid:13) (cid:13)∇(cid:98)L i,2(Q i,θ it)(cid:13) (cid:13) (cid:13)(cid:105)
≤(1−ϵ )2(ωωω )2+(1−ϵ )2O(f (φ ;n ))(ωωω )+ϵ2(cid:0) L2W2(D,Q) +(ωωω )2+W (D,Q) (ωωω )(cid:1)
i DDD i φ i i DDD i 1 max QQQ 1 max QQQ
+2(1−ϵ )ϵ (LW (D,Q) (ωωω )+(ωωω )(ωωω ))
i i 1 max DDD DDD QQQ
Thenotation(ωωω )and(ωωω )isdefinedinLemma5.9. W (D,Q) isthelargestW distancebetweenD (θ)and
DDD QQQ 1 max 1 i
Q forallclientsi∈V amongalliterationt∈[0,T −1]. Tosimplifytheupperboundfirstweusethefactthat
i
(1−ϵ )2(ωωω )2+(1−ϵ )2O(f (φ ;n ))(ωωω )≈(1−ϵ )2(ωωω )2 (23)
i DDD i φ i i DDD i DDD
becausetheestimationerrorof∇L ismuchsmallerthan∇L byusingLemmas5.8and5.9. Applythefactthat
i,1 i,2
ϵ ∈[0,1]andwecanobtain
i
(1−ϵ )2(ωωω )2+(1−ϵ )2O(f (φ ;n ))(ωωω )+ϵ2(cid:0) L2W2(D,Q) +(ωωω )2+W (D,Q) (ωωω )(cid:1)
i DDD i φ i i DDD i 1 max QQQ 1 max QQQ
+2(1−ϵ )ϵ (LW (D,Q) (ωωω )+(ωωω )(ωωω ))
i i 1 max DDD DDD QQQ
≤(1−ϵ )(ωωω )2+ϵ (cid:0) L2W2(D,Q) +(ωωω )2+W (D,Q) (ωωω )+LW (D,Q) (ωωω )+(ωωω )(ωωω )(cid:1)
i DDD i 1 max QQQ 1 max QQQ 1 max DDD DDD QQQ
≤(1−ϵ )(ωωω )2+ϵ (LW (D,Q) +(ωωω )+(ωωω ))2.
i DDD i 1 max QQQ DDD
Lemma C.3 (Sensitivity of contribution Dynamic). For any client i ∈ V, if Assumption 5.1and 5.3 hold, then with
D (θ)=ν (θ)D (θ)+(1−ν (θ))D (θ)forallθ,θ′ ∈Θ,wehave:
i i,1 i,1 i,1 i,2
W (D (θ),D (θ′))≤(γ +W (D (θ),D (θ))F)∥θ−θ′∥.
1 i i i 1 i,1 i,2
21ProofofLemmaC.3. Intheproof, weconsidertwogroupswithfractionsν andν = 1−ν . Whenν (θ) ≥
i,1 i,2 i,1 i,1
ν (θ′),weobtain:
i,1
W (D (θ),D (θ′))≤ν (θ′)W (D (θ),D (θ′))+(1−ν (θ))W (D (θ),D (θ′))
1 i i i,1 1 i,1 i,1 i,1 1 i,2 i,2
+(ν (θ)−ν (θ′))(W (D (θ),D (θ))+W (D (θ),D (θ′)))
i,1 i,1 1 i,1 i,2 1 i,2 i,2
≤ν (θ′)γ ∥θ−θ′∥+(1−ν (θ′))γ ∥θ−θ′∥
i,1 i i,1 i
+(ν (θ)−ν (θ′))W (D (θ),D (θ))
i,1 i,1 1 i,1 i,2
=γ ∥θ−θ′∥+(ν (θ)−ν (θ′))W (D (θ),D (θ)).
i i,1 i,1 1 i,1 i,2
The first inequality arises from the triangle inequality for the Wasserstein-1 distance. The second inequality arises
fromAssumption5.1. Similarly,whenν (θ)≤ν (θ′),weobtain:
i,1 i,1
W (D (θ),D (θ′))≤γ ∥θ−θ′∥+(ν (θ′)−ν (θ))W (D (θ),D (θ)).
1 i i i i,1 i,1 1 i,1 i,2
(cid:13) (cid:13)
Theestimatefunctionf (θ)=ν here.BycombiningtheseresultsandLemma5.6that(cid:13)dfi(θ)(cid:13)hastheupperbound
i i,1 (cid:13) dθ (cid:13)
F,wefinallyobtain:
W (D (θ),D (θ′))≤γ ∥θ−θ′∥+∥ν (θ′)−ν (θ)∥W (D (θ),D (θ))
1 i i i i,1 i,1 1 i,1 i,2
≤(γ +W (D (θ),D (θ))F)∥θ−θ′∥.
i 1 i,1 i,2
D AdditionalExperimentalResults
D.1 AdditionalExperimentalResultsofConvergencetoPerformativeOptimalSolution
Fig.3showstheresultsofcasestudyhousepricingregression,whereanFLsystemistrainedwith10heterogeneous
clients. Although PFL can stabilize the training process when data changes based on the model, it converges to
an undesirable stable solution. In contrast, our algorithm converges to a stable solution with a much lower loss. In
Fig.3a,theresultsshowthatbothalgorithmsexhibitfasterconvergencerateswithlargerenrollmentfraction.However,
significantoscillationsoccurinPFLwhentheenrollmentfractionislow,whilesuchoscillationsaremuchweakerin
PROFL.
Fig.3bshowstheimpactofvaryingdegreesofheterogeneity,whereγ ∈ [1.5−α,1.5+α]withα ∈ {0.1,0.3,1}.
i
Theresultsshowthat,despitethevaryinglevelsofheterogeneity,ouralgorithmmaintainsthecapacitytoconvergeto
θPO.
(a)EnrollmentFraction (b)DifferentHeterogeneity
Figure3: LinearRegressionResultswithDifferentEnrollmentFractionsandHeterogeneity
InFig.4a,theresultsshowthatbothalgorithmsexhibitfasterconvergencerateswithlargersamplesizes.Withalarger
sample size, the results of PFL are closer to the performative stable point and exhibit fewer oscillations. However,
PFLcannotconvergetotheperformativeoptimalpoint,nomatterhowmanysamplesareused. Fig.4bdemonstrates
thatalargervalueofRcorrespondstofasterconvergence,reducingthecommunicationtimerequiredintheFLsystem
toreachconvergence.
22(a)SampleSize (b)LocalRoundsR
Figure4: LinearRegressionResultswithDifferentSampleSizeandLocalRounds
Fig. 5a shows the results of the case study pricing with dynamic contribution with 10 heterogeneous clients. The
resultsindicatethat,unlikePFL,whichconvergestoθPS,ouralgorithmconvergestoasolution,whichisclosetothe
performativeoptimalpoint. Fig.5bshowstheresultsofthecasestudyregressionwithdynamiccontributionwith10
heterogeneousclients. PROFLhasfasterconvergencespeedandlessoscillation,althoughthereisagapbetweenthe
results of both algorithms and the performative optimal point. The gap is a result of the non-linear f (.), which has
i
beendiscussedinmoredetailinthemainpaper.
(a)Pricing (b)Regression
Figure5: ContributionDynamics
D.2 PerformancewhenθPO =θPS
Inthissection,weintroduceaspecialcasewhereθPO =θPS. Thisspecialcasecanariseinbothstrategicanddynamic
contributionsettings. WepresenttheresultsinFig.6,demonstratingthatwhenθPO = θPS,bothalgorithmsconverge
tothesamepoint.
(a)StrategicDynamic (b)ContributionDynamic
Figure6: DynamicswithθPO =θPS
23D.2.1 PerformancewithoutModel-dependentShifts.
Wealsoevaluatethealgorithmwhenthedatadistributiondoesnotchangebasedonthemodel. Inthiscasestudy,we
usetheAdultdatasetforabinaryclassificationtask. Specifically,thereare10homogeneousclientswithγ =γ =
i,0 i,1
0. WeconsiderfourfeaturesfromtheAdultdataset. Whenthedataremainsconstantasthemodelisupdated, both
PFLandouralgorithmreducetoFEDAVG,andtheyconvergetothesamesolution. ThisisillustratedinFig.6a.
D.2.2 PerformancewhenθPO =θPSwithDynamicContribution
Consider case study pricing with dynamic contribution with K = 2 groups with different fixed distributions but a
different one-dimensional fraction where f (θ) = 0.5+0.5θ and f (θ) = 1−f (θ). Under this special case,
i,1 i,2 i,1
the performative stable and optimal points are identical, i.e., θPO = θPS. We can calculate θPO = a1+a2 and
2(a1−a2)
θPS = aa 11 −+a a2 2. When a 1 +a 2 = 0, θPO = θPS. In this case, we set a 1 = −a 2 = 0.5. Fig. 6b illustrates the results
for this specific case study, showing that both our algorithm and PFL converge to the same solution. However, our
methodexhibitsfasterconvergencecomparedtoPFL.
D.2.3 FiguresofTable2
ThefiguresillustratingtheexperimentalresultsfromTable2inthemainpaperareshowninFig.7.
(a)SameDistributions (b)DifferentDistributions (c)CreditDataset (d)AdultDataset
Figure7: BinaryClassificationonSyntheticandRealisticDataset
E Trainingdetails
The experimental setup and parameter selections follow the methodology established in Izzo et al. (2021); Jin et al.
(2024),ensuringconsistencyandcomparabilitywithexistingresults.
E.1 RealisticData
• Adultdata(BeckerandKohavi,1996)wherethegoalistopredictwhetheraperson’sannualincomeexceeds
$50Kbasedon14features(e.g.,age,yearsofeducation).
• GiveMeSomeCreditdata(CreditFusion,2011)whichhas11features(e.g.,monthlyincome,debtratio)and
canbeutilizedtopredictwhetherapersonhasexperienceda90-daypast-duedelinquencyorworse.
E.2 LossFunctionsandRidgePenalty
Weuseridge-regularizedcross-entropylossforallbinaryclassificationcases, andtheridgepenaltyis0.01. Weuse
ridge-regularizedsquaredlossforallregressioncases,andtheridgepenaltyis 10.
3
E.3 RandomSeeds
Inourexperiments,whichshowtheresultsof10trials,therandomseedsusedrangefrom0to9.
E.4 ParametersandDistributions
TheexperimentalparametersarelistedinTable3,andthedistributionD (·)foreachexperimentisdetailedinTable
i
4. ThecorrespondingcasestudiesfortheexperimentsareshowninTable5.
24Table3: ParametersintheExperiments
Figure/Table η R H n ϵ
i i
Table1: PG 0.01 − 1 500 0
Table1: PROFL 0.01 5 1 500 0
Table2: SameD 0.03 10 500 500 0
i
Table2: DifferentD 0.03 500 1 500 0
i
Table2: Credit 0.01 5 1 5812 0
Table2: Adult 0.03 5 1 2368 0
Figure1a 0.001 5 25 500 0,0.1,0.2,0.4
Figure1b 0.0001 3 10 60 0
Figure1c 0.001 5 100 1000 0
Figure2a 0.001 5 20 50,500,5000 0
Figure2b 0.01,0.03,0.001 5 20 2500 0
Figure2c 0.001 5 4,20,100 500 0
Figure3a PFL:0.005PROFL:0.001 5 0.2,0.5,1 500 0
Figure3b PFL:0.005PROFL:0.001 5 1 500 0
Figure3a 0.03 5 1 2368 0
Figure3b 0.03 5 1 2368 0
Figure4a PFL:0.005PROFL:0.001 5 1 5,50,500 0
Figure4b PFL:0.005PROFL:0.001 5,20,50 1 500 0
Figure5a 0.03 5 1 500 0
Figure5b 0.001 5 1 500 0
Figure6a 0.03 5 1 500 0
Figure6b 0.03 5 1 500 0
Table4: DataDistributionsintheExperiments
Figure/Table Distribution
Table1: PGandPROFL γ
i
∈[1.5−α,1.5+α],α∈{0,0.25,0.5}
Table2:SameD γ ∈[2.8,3.2]andX ∼N(−1−γ θ,0.25)forY =1;
i i,1 i i,1
γ ∈[0,0.04]andX ∼N(1−γ θ,0.25)forY =0.
i,0 i i,0
Table2:DifferentD γ ∈[2.8,3.2],X ∼N(−1−γ θ,0.25)
i i,1 1 i,1
andX ∼N(−0.8−γ θ,0.25)forY =1;
2 i,1
γ ∈[0,0.04]andX ∼N(1−γ θ,0.25)forY =0
i,0 i i,0
Table2:Credit γ =3andγ =0.GiveMeSomeCreditdatasetCreditFusion(2011)
i,1 i,0
Table2:Adult γ ∈[2.8,3.2]andγ ∈[0,0.004].Adultdataset(BeckerandKohavi,1996)
i,1 i,0
Figure1a µ ∈[6,7],σ=1,γ ∈[1,3]
0 i
Figure1b,1c,2a-c γ ∈[0.09,0.11]
i
Figure3a γ ∈[1.5,1.8]
i
Figure3b γ ∈[1.5−α,1.5+α],α∈{0.1,0.3,1}
i
Figure4a γ ∈[1.5,1.8]
i
Figure4b γ ∈[1.5,1.8]
i
Figure5a D ∼N(0.5,0.25),D ∼N(−0.5,0.25)f(θ)=0.5+0.5θ
i,1 i,2
Figure5b y=f (θ)x+3(1−f (θ))x+ε,ε∼N(0,4)
i i
Figure6a γ =0.10homogeneousclientswithAdultdataset
i
Figure6b D ∼N(0.5,0.25),D ∼N(−0.5,0.25)
i,1 i,2
25Table5: TypeofCaseStudyandCorrespondingFiguresorTables
Figure/Table TypeofCaseStudy
Table1: housepricingregression
Table2: binarystrategicclassification
Figure1a pricingwithdynamicdemands
Figure1b,1c,2a-c pricingwithdynamiccontribution
Figure3a,3b,4a,4b housepricingregression
Figure5a pricingwithdynamiccontribution
Figure5b regressionwithdynamiccontribution
Figure6a binarystrategicclassification
Figure6b pricingwithdynamiccontribution
References
Becker,B.andKohavi,R.(1996).Adult.UCIMachineLearningRepository.DOI:https://doi.org/10.24432/C5XW20.
Bracale,D.,Maity,S.,Banerjee,M.,andSun,Y.(2024). Learningthedistributionmapinreversecausalperformative
prediction.
Briggs,C.,Fan,Z.,andAndras,P.(2020). Federatedlearningwithhierarchicalclusteringoflocalupdatestoimprove
trainingonnon-iiddata. In2020InternationalJointConferenceonNeuralNetworks(IJCNN),pages1–9.IEEE.
Chen, Y., Tang, W., Ho, C.-J., and Liu, Y. (2024). Performative prediction with bandit feedback: Learning through
reparameterization. InSalakhutdinov,R.,Kolter,Z.,Heller,K.,Weller,A.,Oliver,N.,Scarlett,J.,andBerkenkamp,
F.,editors,Proceedingsofthe41stInternationalConferenceonMachineLearning,volume235ofProceedingsof
MachineLearningResearch,pages7298–7324.PMLR.
CreditFusion,W.C.(2011). Givemesomecredit.
Dean, S. and Morgenstern, J. (2022). Preference dynamics under personalized recommendations. In Proceedings
ofthe23rdACMConferenceonEconomicsandComputation,EC’22,page795–816.AssociationforComputing
Machinery.
Ensign,D.,Friedler,S.A.,Neville,S.,Scheidegger,C.,andVenkatasubramanian,S.(2018). Runawayfeedbackloops
inpredictivepolicing. InConferenceonfairness,accountabilityandtransparency,pages160–171.PMLR.
Ghosh,A.,Chung,J.,Yin,D.,andRamchandran,K.(2020). Anefficientframeworkforclusteredfederatedlearning.
AdvancesinNeuralInformationProcessingSystems,33:19586–19597.
Guo,Y.,Lin,T.,andTang,X.(2023). Towardsfederatedlearningontime-evolvingheterogeneousdata.
Hardt,M.,Megiddo,N.,Papadimitriou,C.,andWootters,M.(2016). Strategicclassification. InProceedingsofthe
2016ACMConferenceonInnovationsinTheoreticalComputerScience,page111–122.
Huber, P. J. (1992). Robust estimation of a location parameter. In Breakthroughs in statistics: Methodology and
distribution,pages492–518.Springer.
Izzo,Z.,Ying,L.,andZou,J.(2021). Howtolearnwhendatareactstoyourmodel: Performativegradientdescent. In
Meila,M.andZhang,T.,editors,Proceedingsofthe38thInternationalConferenceonMachineLearning,volume
139ofProceedingsofMachineLearningResearch,pages4641–4650.PMLR.
Jiang, L. and Lin, T. (2022). Test-time robust personalization for federated learning. In The Eleventh International
ConferenceonLearningRepresentations.
Jin,K.,Yin,T.,Chen,Z.,Sun,Z.,Zhang,X.,Liu,Y.,andLiu,M.(2024). Performativefederatedlearning: Asolution
to model-dependent and heterogeneous distribution shifts. In Proceedings of the AAAI Conference on Artificial
Intelligence,volume38,pages12938–12946.
Li,T.,Hu,S.,Beirami,A.,andSmith,V.(2021). Ditto: Fairandrobustfederatedlearningthroughpersonalization. In
Meila,M.andZhang,T.,editors,Proceedingsofthe38thInternationalConferenceonMachineLearning,volume
139ofProceedingsofMachineLearningResearch,pages6357–6368.PMLR.
Li, T., Sahu, A. K., Talwalkar, A., and Smith, V. (2020a). Federated learning: Challenges, methods, and future
directions. IEEESignalProcessingMagazine,37(3):50–60.
26Li,T.,Sahu,A.K.,Zaheer,M.,Sanjabi,M.,Talwalkar,A.,andSmith,V.(2020b). Federatedoptimizationinhetero-
geneousnetworks. InDhillon, I., Papailiopoulos, D., andSze, V., editors, Proceedingsof MachineLearningand
Systems,volume2,pages429–450.
Ma,Y.,Xie,Z.,Wang,J.,Chen,K.,andShou,L.(2022).Continualfederatedlearningbasedonknowledgedistillation.
In Raedt, L. D., editor, Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence,
IJCAI-22,pages2182–2188.InternationalJointConferencesonArtificialIntelligenceOrganization. MainTrack.
Mendler-Du¨nner, C., Ding, F., andWang, Y.(2022). Anticipatingperformativitybypredictingfrompredictions. In
Koyejo,S.,Mohamed,S.,Agarwal,A.,Belgrave,D.,Cho,K.,andOh,A.,editors,AdvancesinNeuralInformation
ProcessingSystems,volume35,pages31171–31185.CurranAssociates,Inc.
Mendler-Du¨nner,C.,Perdomo,J.,Zrnic,T.,andHardt,M.(2020).Stochasticoptimizationforperformativeprediction.
In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H., editors, Advances in Neural Information
ProcessingSystems,volume33,pages4929–4939.CurranAssociates,Inc.
Miller, J.P., Perdomo, J.C., andZrnic, T.(2021). Outsidetheechochamber: Optimizingtheperformativerisk. In
InternationalConferenceonMachineLearning,pages7710–7720.PMLR.
Nguyen, A. T., Torr, P., and Lim, S. N. (2022). Fedsr: A simple and effective domain generalization method for
federatedlearning. InKoyejo,S.,Mohamed,S.,Agarwal,A.,Belgrave,D.,Cho,K.,andOh,A.,editors,Advances
inNeuralInformationProcessingSystems,volume35,pages38831–38843.CurranAssociates,Inc.
Park, T. J., Kumatani, K., and Dimitriadis, D. (2021). Tackling dynamics in federated incremental learning with
variationalembeddingrehearsal. arXivpreprintarXiv:2110.09695.
Perdomo,J.,Zrnic,T.,Mendler-Du¨nner,C.,andHardt,M.(2020). Performativeprediction. InInternationalConfer-
enceonMachineLearning,pages7599–7609.PMLR.
Polyak, B. T. (1964). Some methods of speeding up the convergence of iteration methods. Ussr computational
mathematicsandmathematicalphysics,4(5):1–17.
Sattler,F.,Mu¨ller,K.-R.,andSamek,W.(2020). Clusteredfederatedlearning: Model-agnosticdistributedmultitask
optimizationunderprivacyconstraints. IEEETransactionsonNeuralNetworksandLearningSystems.
Sattler,F.,Mu¨ller,K.-R.,andSamek,W.(2021). Clusteredfederatedlearning: Model-agnosticdistributedmultitask
optimizationunderprivacyconstraints.IEEETransactionsonNeuralNetworksandLearningSystems,32(8):3710–
3722.
Shan, J.-W., Zhao, P., and Zhou, Z.-H. (2023). Beyond performative prediction: Open-environment learning with
presence of corruptions. In International Conference on Artificial Intelligence and Statistics, pages 7981–7998.
PMLR.
Somerstep,S.,Sun,Y.,andRitov,Y.(2024). Learninginreversecausalstrategicenvironmentswithramificationson
twosidedmarkets. ArXiv,abs/2404.13240.
Tan, A. Z., Yu, H., Cui, L., and Yang, Q. (2023). Towards personalized federated learning. IEEE Transactions on
NeuralNetworksandLearningSystems,34(12):9587–9603.
Wang,S.andJi,M.(2022). Aunifiedanalysisoffederatedlearningwitharbitraryclientparticipation. Advancesin
NeuralInformationProcessingSystems,35:19124–19137.
Xie,T.andZhang,X.(2024). Non-linearwelfare-awarestrategiclearning.
Zhang,X.,Khalili,M.M.,Jin,K.,Naghizadeh,P.,andLiu,M.(2022). Fairnessinterventionsas(Dis)Incentivesfor
strategicmanipulation. InProceedingsofthe39thInternationalConferenceonMachineLearning,pages26239–
26264.
Zhao, Y. (2022). Optimizing the performative risk under weak convexity assumptions. OPT2022: 14th Annual
WorkshoponOptimizationforMachineLearning,abs/2209.00771.
Zheng, Q., Zhang, A., and Grover, A. (2022). Online decision transformer. In Chaudhuri, K., Jegelka, S., Song,
L.,Szepesvari,C.,Niu,G.,andSabato,S.,editors,Proceedingsofthe39thInternationalConferenceonMachine
Learning,volume162ofProceedingsofMachineLearningResearch,pages27042–27059.PMLR.
Zhu, C., Xu, Z., Chen, M., Konecˇny`, J., Hard, A., and Goldstein, T. (2021). Diurnal or nocturnal? federated learn-
ing of multi-branch networks from periodically shifting distributions. In International Conference on Learning
Representations.
27