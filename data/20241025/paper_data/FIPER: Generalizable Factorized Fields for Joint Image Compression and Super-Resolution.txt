FIPER: GENERALIZABLE FACTORIZED FIELDS FOR
JOINT IMAGE COMPRESSION AND SUPER-RESOLUTION
Yang-CheSun1,ChengYuYeo1,ErnieChu2,Jun-ChengChen3,Yu-LunLiu1
1NationalYangMingChiaoTungUniversity,2JohnsHopkinsUniversity,3AcademiaSinica
https://jayisaking.github.io/FIPER/
ABSTRACT
Inthiswork,weproposeaunifiedrepresentationforSuper-Resolution(SR)and
ImageCompression,termedFactorizedFields,motivatedbythesharedprinciples
betweenthesetwotasks. BothSISRandImageCompressionrequirerecovering
and preserving fine image details—whether by enhancing resolution or recon-
structingcompresseddata. Unlikepreviousmethodsthatmainlyfocusonnetwork
architecture,ourproposedapproachutilizesabasis-coefficientdecompositionto
explicitlycapturemulti-scalevisualfeaturesandstructuralcomponentsinimages,
addressingthecorechallengesofbothtasks. WefirstderiveourSRmodel,which
includesaCoefficientBackboneandBasisSwinTransformerforgeneralizable
FactorizedFields. Then,tofurtherunifythesetwotasks,weleveragethestrong
information-recoverycapabilitiesofthetrainedSRmodulesaspriorsinthecom-
pressionpipeline,improvingbothcompressionefficiencyanddetailreconstruction.
Additionally,weintroduceamerged-basiscompressionbranchthatconsolidates
sharedstructures,furtheroptimizingthecompressionprocess. Extensiveexperi-
mentsshowthatourunifiedrepresentationdeliversstate-of-the-artperformance,
achievinganaveragerelativeimprovementof204.4%inPSNRoverthebaseline
in Super-Resolution (SR) and 9.35% BD-rate reduction in Image Compression
comparedtothepreviousSOTA.
1 INTRODUCTION
SingleImageSuper-Resolution(SISR)aimstoreconstructhigh-qualityimagesfromlow-resolution
counterparts. Specifically, the key lies in accurately restoring fine details and reconstructing the
correctarrangementofvisualfeatures. Thus,geometriccorrespondencesorrepetitivepatterns,such
as stripes, grids, or textures, are commonly used for evaluation due to their rich details that are
crucialtoimagefidelity. EarlyCNN-basedapproachesLimetal.(2017b);Dongetal.(2014)laid
thefoundationforSISR,whichwaslaterenhancedwithGAN-basedmethodsLedigetal.(2017a)
forimprovedperceptualrealism. Follow-upTransformer-basednetworksChenetal.(2021)address
non-localdependencies,withsubsequentSwinTransformer-basedapproachessparkingtremendous
advancementsCondeetal.(2022);Chenetal.(2023d;2024),whichtheninspiremoreandmore
complex, delicate designs of heavy network architectures in SISR. However, these prior works
have primarily focused on network architecture design rather than addressing the capability of
representation. Thevisualpatternsandtheinherentnatureofimagecontentstructurethatplayan
influentialroleinSISRhavenotbeenexplicitlyconsideredintherepresentationlearningprocess.
Thisraisesacriticalquestion: beyondasimplenetworkoutput,canwederiveaformulationthat
moreeffectivelycapturesthesepatternsandalignswiththegoalsofSISR?
Ontheotherhand,imagecompressionservesasafundamentaltaskinlow-levelvisionapplications,
where the traditional compression standards Joint Video Experts Team (JVET) (2023); Wallace
(1992);Taubman&Marcellin(2013)laythegroundwork. Theemerginglearnedimagecompression
modelsChamainetal.(2021);Balle´etal.(2018a);Guo-Huaetal.(2023);Liuetal.(2023a);Minnen
etal.(2018);Chengetal.(2020),compressionalgorithmsofwhichmostlyfollowthepixel-space
transformcoding Chamainetal.(2021);Goyal(2001)paradigm,thenintroduceneuralnetworks
to further optimize compression efficiency by learning more compact latent representations and
improving reconstruction quality. Specifically, they convert pixels into compact representations
through a transform module, which eliminates the redundancy and reduces the bit cost in the
1
4202
tcO
32
]VI.ssee[
1v38081.0142:viXrasubsequententropycodingprocess.However,thecorechallengeofimagecompressionistoaccurately
reconstructtheinformationlostduringcompressionandquantization. Inotherwords,themodelsare
torecovertheimpairedcomponentsinimages,whichcanessentiallybeviewedasreconstructinga
high-qualityimagefromits‘low-resolution’version,muchlikeSuper-Resolution.
Basedontheaforementionedanalysis,itbecomesclearthatalthoughSuper-ResolutionandImage
Compressionappeartobetwodistincttasks,theysharemutualsimilaritiesintwokeyaspects: (1)
Bothtasksrequiremodelstorestorefinedetailsfromlow-qualityimagecontent,aswellasimplicitly
captureandreconstructrepetitivestructuralelements. (2)Bothaimtoconserveimagequality,either
byenhancingresolutionorefficientlycompressingdatawithoutsignificantlossofperceptualfidelity.
Hence,inspiredbyrecentadvancesindecompositionfieldsandmatricesfactorizationin3Dscene
modelingChenetal.(2022a);Mu¨lleretal.(2022);Chanetal.(2021);Fridovich-Keiletal.(2023);
Cao&Johnson(2023);Chenetal.(2023a);Gaoetal.(2023),weproposeaunifiedrepresentation,
FactorizedFields,withgeneralizableCoefficientBackboneandBasisTransformer. Thisapproach
explicitlycapturesmulti-scalevisualfeaturesandrepetitivestructuralcomponentsinimagesthrougha
basis-coefficientdecomposition.Theresultingrepresentationstrikesabalancebetweenbeingcompact
andinformation-rich,enablingtheresolutionofstructuralambiguitiesandtheprecisemodelingof
imagedetailsthroughamulti-frequencyformulation. Inthemeantime,suchaformulationimposesa
factorizationconstraintduringmodeltraining,whichnotonlyenhancesthequalityofsingle-image
super-resolution(SISR)butalsoreducesdistortionandimprovescompressionefficiencybyexplicitly
modelingstructuralelements.Ontopofthese,toleveragetherobustinformation-recoveringcapability
ofSRmodels,weintegratesuchpriorswithImageCompressionmodels,whoseknowledgeofdetail
compensationfurtherrefinesthelostkeyelements.
Finally,weproposemergingthebasesbyintroducinganadditionalcompressionbranch,consolidating
multiplebasesintoonealongsidemulti-imagetransmission. Thisapproachleveragesthemutual
informationacrossmultipleimages,reducingtheneedforredundanttransmissionsandrefiningthe
basisstructure. Themaincontributionsofthispaperaresummarizedasfollows:
• WeproposeFactorizedFields,aunifiedrepresentationthatexplicitlymodelsmulti-scale
visualfeaturesandstructuralcomponentsforbothsuper-resolutionandimagecompression.
• We integrate super-resolution with image compression by introducing SR prior during
decompressiontocompensateforlostdetailsanddevelopingamerged-basiscompression
branchformulti-imagecompression.
• Wedemonstratestate-of-the-artperformanceonbenchmarksforbothsuper-resolutionand
imagecompressionthroughextensiveexperiments.
2 RELATED WORKS
Super-Resolution(SR). Imagesuper-resolutioniscriticalincomputervision,focusingonrecover-
inghigh-resolution(HR)imagesfromlow-resolution(LR)inputs. Followingthefoundationalstudies,
CNN-basedstrategiesDongetal.(2014);Limetal.(2017b);Zhouetal.(2020);Sunetal.(2022);
Kimetal.(2016)wereinitiallyintroducedwithmodelingtechniquessuchasresiduallearningLedig
etal.(2017b);Liuetal.(2020);Tongetal.(2017);Zhangetal.(2018b;a);Chih-ChungHsu(2023);
Limetal.(2017a),orrecursivelearningChenetal.(2024);Taietal.(2017). Besides,subsequent
researchalsoshedslightonGAN-basemethodsLedigetal.(2017a;b);Wangetal.(2019;2021)
toenhancerealisticityanddetailquality. However,theinductivebiasofCNN-basednetworksby
restricting spatial locality hinders the capture of long-range dependency from images, which is
alleviated by Transformer-based SISR networks Chen et al. (2021); Li et al. (2021). Afterward,
SwinIR Liang et al. (2021) is proposed to combine spatial locality and non-local information by
SwinTransformerLiuetal.(2021)withwindowattentionandachievebreakthroughimprovementin
SISR.FollowingSwinIR’ssuccess,severalworkshavebuiltuponitsframeworkCondeetal.(2022);
Zhuetal.(2023);Zhangetal.(2024a)toreachbetterimagequalityaswellassolveinformation
bottleneckChih-ChungHsu(2023). HybridapproachesCRAFT(Lietal.,2023)mergethebenefits
ofconvolutionalandtransformerstructurestofurtherelevateSRperformance. Forbetterfeature
aggregation,DAT(Chenetal.,2023d)andHAT(Chenetal.,2023b)integratespatialandchannel
information using attention mechanisms to enhance their representation capabilities. Moreover,
RGT(Chenetal.,2024)introducesauniquerecursive-generalizationself-attentionmechanismthat
efficientlycapturescomprehensivespatialdetailswithalinearincreaseincomputationalcomplexity.
2Transformed Coordinate
to sample on 2 Rearrange
0 2 X
2
0
T
nar
(b) Pixel Unshuffle
mas
ot mrofs
Sample
p de 2
el C
o o
n o Rearrange
dr
ni
a
et
Y
(a) Sawtooth Pixel Sampling (c) Sawtooth-Aware Downsample
Figure 1: The correlation between coordinate transformation and downsampling. (a) The
sawtoothtransformationexamplewithk =2. (b)ThePixelUnShuffledownsample. (c)Toexplicitly
modeltheinformationforsamplingwithasawtooth,werearrangethefeaturemapinadilation-like
mannerinthedownsamplelayeroftheBasisSwinTransformer. Thisway,thefeaturesampledwould
capturetheinformationintheoriginallayoutcorrectly.
ImageCompression(IC). Deeplearninghassignificantlyadvancedimagecompression,offering
superiorcompressionratiosandimagequalitycomparedtotraditionalmethodslikeJPEGWallace
(1992)andJPEG-2000Taubman&Marcellin(2013). EarlyCNN-basedapproachesChamainetal.
(2021);Balle´ etal.(2018b);Guoetal.(2022)havebeensurpassedbytransformer-basedmodels
Koyuncu et al. (2022); Zhu et al. (2022); Wu et al. (2021); Duan et al. (2023), which leverage
spatio-channelattentionforbetterperformance. GAN-basedmethodsMaoetal.(2023a;b);Feng
etal.(2021);Rippel&Bourdev(2017)havefurthercontributedtoreal-timeadaptivecompression.
Recently, ELIC He et al. (2022) introduced efficient compression with unevenly grouped space-
channel contextual adaptive coding, while LIC TCM Liu et al. (2023a) integrated transformers
and CNNs to capture both global and local image features. The eContextformer Koyuncu et al.
(2024)introducedpatch-wise,checkered,andchannel-wisegroupingtechniquesforparallelcontext
modelingwithashiftedwindowspatial-channelattentionmechanism. GroupedMixerLietal.(2024)
proposedatransformer-basedentropymodelwithtokenmixersforinnerandcross-groupcontext
modeling. Meanwhile,WaveletConditionalDiffusionSongetal.(2024)introducedawavelet-based
modelwithuncertainty-awareloss,balancinghighperceptualqualitywithlowdistortion.
Insummary,whilerecentsuper-resolutionandimagecompressionadvancesfocusonincreasingly
complexarchitectures,ourworktakesadifferentapproach. WeproposeFactorizedFields,aunified
frameworkthatmodelsvisualandstructuralfeatures, offeringamorecomprehensivesolutionto
enhanceperformanceinbothtasks.
3 METHODS
Inthissection,wefirstbrieflyintroducethebackgroundofthefactorfieldsChenetal.(2023a),their
properties,andalsothepreliminaryofLearnedImageCompressioninSec.3.1. Inspiredbyfactor
fields,wenextexplainthemotivationandhowwederiveourFactorizedFieldsforenhancedimage
reconstructionqualityinSec.3.2. WethendescribehowtoadapttheformulationtoSuper-Resolution
inSec.3.3. Finally,weshowhowtoincorporatesuchrepresentationwithimage(s)compressionand
howweintegrateSuper-ResolutionandImageCompressioninSec.3.4.
3.1 PRELIMINARIES
FactorFields. Theconceptofdecompositionfieldsorfactorizedmatricesinthereconstructionof
2Dimagesor3DsceneshasshownsuperiorrenderingqualityandimprovedefficiencyrecentlyChen
etal.(2022a);Gaoetal.(2023);Mu¨lleretal.(2022);Fridovich-Keiletal.(2023);Cao&Johnson
(2023);Chanetal.(2021),whileChenetal.(2023a)firstproposedaunifiedrepresentationcalled
factorfields. Fora1Dsignals(x),thefactorfieldsformulationis:
sˆ(x)=c(x)Tb(γ(x)), (1)
3wherec(x)=(c (x),...,c (x))T arespatial-varyingcoefficientfields,b(x)=(b (x),...,b (x))T
1 N 1 N
arebasisfunctions,andγ(x)isacoordinatetransformation. Thecoefficientandbasisarequeriedby
samplingwiththecoordinatesx. ThisgeneralizestoQ-dimensionalsignals:
sˆ(x)=P(c(x)⊙b(γ(x))), (2)
whereP isaprojectionfunctionand⊙denotestheelement-wiseproduct. Inpractice, formulti-
dimensionalcoefficients:
(cid:16) (cid:110) (cid:111)(cid:17)
sˆ(x)=P ConcatN c(x)⊙b(γ(x)) . (3)
i=1 i i i
Thisformulationallowsthesamebasistobeappliedatmultiplespatiallocationswhilevaryingthe
coefficients,especiallywhenγ isaperiodicfunction.
LearnedImageCompression. Following(Minnen&Singh,2020;Balle´ etal.,2018a),alearned
imagecompressionmodelwithachannel-wiseentropymodelcanbeformulatedas:
z =h (y;ϕ ), y =g (x;ϕ),
a h a
{X ,X }=h (zˆ;θ ), zˆ=Q(z)
mean scale s h
yˆ={Q(y −µ )+µ ,...,Q(y −µ )+µ }, 0<=t<l, µ =e (yˆ ,X ))
0 0 0 t t t t i <i mean
xˆ=g (y;θ), y =Refine (µ ,...,µ ,yˆ).
s θr 0 t
(4)
Theencoderg transformstherawimagexintoalatentrepresentationy. Ahyper-priorencoderh
a a
furtherprocessesytooutputz,capturingspatialdependencies. zisquantizedtozˆ,whichisdecoded
byh toproducefeaturesX andX ,usedtoestimatethemeanµandvarianceσ ofy. The
s mean scale
latenty isdividedintolslices,andeachquantizedaroundcomputedmeansµ . Thesemeansare
t
derivedfromearlierquantizedslicesandX byaslicenetworke . Thequantizedslicesformyˆ.
mean i
Fordecompression,yˆisrefinedusingRefine basedonµ andyˆtoproducey,approximatingthe
θr t
originaly. Finally,g reconstructsthedecompressedimagexˆfromy. Themodelistrainedusinga
s
Lagrangianmultiplier-basedrate-distortionoptimization:
L=R(yˆ)+R(zˆ)+λ·D(x,xˆ), (5)
whereR(yˆ)andR(zˆ)denotebitrates,D(x,xˆ)isthedistortionterm(calculatedbyMSE),andλ
balancescompressionefficiencyandimagefidelity. Inourexperiments,wefollowLiuetal.(2023a),
modifyingonlyg todemonstrateourrepresentation’seffectiveness.
s
3.2 FORMULATIONOFFACTORIZEDFIELDS
AsdiscussedinSec.1,thekeytosuperiorrenderingqualityinimageregressiontasksofbothImage
CompressionandSuper-Resolutionliesinthecapabilityoftherepresentationtocaptureaccurate
structuraldistributionandfinevisualdetails. Meanwhile,transformationssuchasFourierTransform
orWaveletTransformhavelongbeenusedtomodelmulti-frequencyinformationFuolietal.(2021);
Korkmazetal.(2024)toexpressdifferentimplicitfunctionsinimages;however,suchmethodsoften
sufferfromunder-expressionduetothepre-definedandlimitedfrequencybandsorrestrictionfrom
itsformulation. Thus,weseekarepresentationthatexplicitlyincorporatesandfitsmulti-scaleand
multi-frequencycomponentsandyetishighlyflexibleandlearnableaccordingtoindividualfeatures
inanimage.
InspiredfromrecentsuccessonsuchdecompositionfieldsFridovich-Keiletal.(2023);Chenetal.
(2023a;2022a);Mu¨lleretal.(2022);Cao&Johnson(2023);Chanetal.(2021),weprimarilybuilt
ourFactorizedFieldsframeworkonfactorfieldsChenetal.(2023a)fromEq.3:
(cid:16) (cid:110) (cid:111)(cid:17)
Iˆ(x)=P ConcatN c(x)⊙b(γ(x)) . (6)
i=1 i i i
NotethatIˆdenotestheapproximatedimages,andx∈R2arethepixelcoordinates.
Such formulation has several key properties: First of all, by decomposing the images into basis
frequencies,wecanlearntheimplicitfunctionsofanimageandcapturethemutualdependencies
betweenpixelsandacrossspatialcomposition;meanwhile,sincethebasisandcoefficientarespecific
toeverysingleimageandbothlearnableinallspatialdimensions,therestrictiononalimitednumber
of basis (we use N = 6 in all of our experiments) can be alleviated. Finally, as in Chen et al.
4Figure2: Theoverallpipelineofimagesuper-resolutionwithourFactorizedFields. Givena
low-resolutionimageI ,wefirstextractcoefficientfeaturemapX withthecoefficientbackbone,
LR coeff
which is then decoded into coefficient and passed through the basis Swin Transformer for basis,
separately.Finally,asinEq. 8,thecoefficientandbasisaresampled,multiplied,anddecodedforfinal
high-resolutionoutputI ,wheres,H,W denotethescalefactor,height,andwidthrespectively.
HR
(2023a), ofallthetestedcoordinatetransformationγ inEq.3sawtoothγ(x) = xmodk,k ∈ R
performsmostlythebestinimageregressiontasks. Wecaneasilyobservethatsuchtransformation
implicitlycapturespatch-likefrequencyinformationasshowninFig.1a,andthus,weproposethat
byleveragingtheinter-patchinformationfromthesawtoothcoordinatetransformation,thevisual
correspondencebetweenspatiallocationscanbeeffectivelyrepresented.
However,inpractice,wesamplethebasisviabilinearorbicubicsamplingduetomemoryconstraints,
i.e., thefeaturesizeislessthanimageheightandwidth, andthisposesasevereproblem: These
nearbysampledpixelfeaturesareactuallylinearorbasedoncubicinterpolationwithrespecttothe
basis,andsuchinductivebiashinderstherepresentationofnon-linearityinimages. Toresolvethis,
weproposeamodifiedversion,ourFactorizedFields:
Iˆ(x)=P(cid:16) ConcatN K (cid:110) c (x)⊙ψ(α ·b (γ (x)))(cid:111)(cid:17) ,ψ ∈{sin,cos},α ∈R. (7)
i=1j=1 i j i i j
Here,inspiredbytheFourierSeries,withascalarαwhichcanbeviewedasincreasingtosampling
frequencyandatransformationfunctionψtoaddtoimplicitnon-linearityfunctionsandmodulation
of product, our complete Factorized Fields method stands out by the fact that such formulation
significantlyenhancestheabilitytorepresentcomplexnon-linearstructuresinimagesandeffectively
composeshigh-frequencycomponentsbetweenpixels.
3.3 SUPER-RESOLUTIONWITHFACTORIZEDFIELDS
Werepresentasuper-resolvedimageusingourFactorizedFields,wherecoefficientsandbasisare
generatedbynetworksF andF fromalow-resolutionImage:
coeff basis
Iˆ (x)=P(cid:16) ConcatN K (cid:110) cLR(x)⊙ψ(α ·bLR(γ (x)))(cid:111)(cid:17) , (8)
SR i=1j=1 i j i i
wherecLR(x)=Conv(F (I )) (x)andbLR(x)=F (F (I )) (γ (x)). Notethatwesam-
i coeff LR i i basis coeff LR i i
pletheoutputsConv(F (I ))andF (F (I ))withcoordinatesxandγ(x),respectively.
coeff LR basis coeff LR
Ourmodelcomprisesthreemaincomponents: CoefficientBackbone,BasisSwinTransformer,and
FactorizedFieldsReconstruction. AsshowninFig.2, theprocessbeginswithI ∈ R3×H×W.
LR
TheCoefficientBackboneextractsfeaturesX
coeff
∈RCc×Hc×Wc,whicharethenusedtogenerate
coefficientscthroughconvolutionandpixelshuffleoperations,andfedintotheBasisSwinTrans-
formertoproduceamulti-scalebasisb = {b 1,...,b N}, b
i
∈ RCbi×Hbi×Wbi. Thecoefficientsand
basisarecombinedtoreconstructI ∈R3×sH×sW usingEq.8,wheresisthescalefactor. Inour
SR
5Figure 3: The illustration of our joint image-compression and super-resolution framework
comparedwiththetraditionalcompression-onlymethod. (a)Traditionallearning-basedcompres-
sionmethods. (b)Ourapproachsurpasses(a)byincorporatingourSuper-Resolution(SR)Module
fromSec.3.3asinformation-recoveryprior,detailedinSec. 3.4.1. (c)Expandingon(b),Sec.3.4.2
introducesamulti-imagecompressionstrategythatutilizesbothourSRModuleandaBasisMerging
Transformertocapturesharedstructure.
experiments,γ (x)isaSawtoothfunction: γ (x) = x mod k ,withk ∈ R. Weoptimizemodel
i i i i
parametersusinganL lossfunction. Todemonstrateourmethod’seffectiveness,weuseexistingSR
1
methodsChenetal.(2023c;d);Zhangetal.(2024a)astheCoefficientBackbone. FortheBasisSwin
Transformer,weemploySwinTransformerBlocksLiuetal.(2022)withaseriesofdownsampling
operations. Weuseadilation-likedownsamplingtechnique(Fig.1(c))toaccommodatethesawtooth
samplingpattern. Thefinalbasisisrefinedusingadditionalupsamplingandconvolutionlayers.
3.4 IMAGECOMPRESSIONWITHFACTORIZEDFIELDSANDSUPER-RESOLUTION
ImageCompression,atitscore,istostrikeabalancebetweentheamountofinformationcontained
in the latent bits and the final image quality. However, as discussed in Sec. 1, 2, and 3.2, most
recentworksdrawemphasisonhowtoretrieveimplicitelementsthroughdesigningdifferentarchi-
tectures,suchasanalysistransformsandentropymodelsKoyuncuetal.(2024);Lietal.(2024),or
decompressionmodulesSongetal.(2024);Duanetal.(2023),whilethispaperaimsataddressing
therepresentationitselftobettercapturethestructuralcorrelationsandthusachievebetterimage
qualitythroughexplicitmodelingoffrequencycomponentsandFactorizedFieldsformulationasin
Eq.7. Inaddition,withourtrainedSRmodeldescribedinSec.3.3,itintuitivelyservesasastrong
priorforinformationrecovery,i.e.,itcontainsextensiveknowledgeofhowtoreconstructmissing
detailsandenhanceimagequalitybyleveraginglearnedpatternsfromthetrainingdata. Thus,since
Super-ResolutionandImageCompressionsharethecoreprincipleofreconstructingandenhancing
imagedetailsfromlow-qualitysources,wecaneffectivelyintegratethispriorintothecompression
pipeline. Inthefollowing,werespectivelypresenttheproposedjointframeworkforthesingleand
multi-imagecompressiontasks.
3.4.1 SINGLEIMAGECOMPRESSION
TheoverallpipelineisshowninFig.3(b). Todemonstratetherobustnessofourrepresentationand
theeffectivenessoftheSRprior,thecompressionanddecompressionnetworksgreatlyfollowLiu
et al. (2023a), with only the synthesis transform replaced by our SR pipeline, where the details
canbereferencedinSupplementaryMaterials. Inpractice,thetrainingisperformedintwostages.
AfterweobtainthetrainedSRprior,themodelisfine-tunedwithalowerlearningratealongsidethe
compressionmodule,whichisthentrainedend-to-endwiththelossfunctiondefinedinEq. 5.
3.4.2 MULTI-IMAGECOMPRESSION
For each basis b
i
∈ RCbi×Hbi×Wbi associated with any arbitrary image, we can consider it as
encapsulatingtheinherentpixelstructure. Thesebasescanbecombinedintoaunifiedgenericbasis
thatcapturesthestructuraldistributionofimagesandpotentiallyreducesnoise. GivenM images
6Table1: Quantitativecomparisonson4×super-resolutionwithstate-of-the-artmethods. The
bestresultsarecoloredred. Themodelswith†arethosewhousesame-taskpretrainingChenetal.
(2023c). PleaserefertoquantitativeresultsinSec.4.1fordetails.
Params MACs ForwardPass Set5 Set14 B100 Urban100 Manga109
Method (M) (G) Memory(MB) PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑ PSNR↑ SSIM↑
SwinIR(Liangetal.,2021)(ICCV2021W) 28.01 119.68 3,826 32.93 0.9043 29.15 0.7958 27.95 0.7484 27.56 0.8273 32.22 0.9273
ATD(Zhangetal.,2024b)(CVPR2024) 20.26 77.10 6,572 33.14 0.9061 29.25 0.7976 28.02 0.7524 28.22 0.8414 32.65 0.9308
DAT(Chenetal.,2023d))(ICCV2023) 14.80 61.66 4,192 33.15 0.9062 29.29 0.7983 28.03 0.7518 27.99 0.8365 32.67 0.9301
RGT(Chenetal.,2024)(ICLR2024) 13.37 834.25 3,404 33.16 0.9066 29.28 0.7979 28.03 0.7520 28.09 0.8388 32.68 0.9303
HAT†(Chenetal.,2023b)(CVPR2023) 20.77 86.02 3,692 33.18 0.9073 29.38 0.8001 28.05 0.7534 28.37 0.8447 32.87 0.9319
HAT-L†(Chenetal.,2023b)(CVPR2023) 40.84 167.27 6,804 33.30 0.9083 29.47 0.8015 28.09 0.7551 28.60 0.8498 33.09 0.9335
ATD-L* 49.42 184.83 15,582 33.12 0.9062 29.31 0.7985 28.02 0.7514 28.25 0.8422 32.78 0.9309
DAT-L* 43.01 175.42 11,326 33.33 0.9084 29.40 0.8009 28.04 0.7543 28.49 0.8473 33.02 0.9321
ATD-F†(Ours) 45.46 149.87 8,674 33.29 0.9082 29.48 0.8017 28.03 0.7539 28.53 0.8487 33.11 0.9335
DAT-F†(Ours) 40.00 134.42 6,206 33.45 0.9094 29.60 0.8039 28.13 0.7560 28.75 0.8520 33.23 0.9339
HAT-F†(Ours) 45.97 158.79 5,750 33.53 0.9100 29.65 0.8050 28.18 0.7569 28.79 0.8527 33.33 0.9342
HAT-F-ImageNet†(Ours) 45.97 158.79 5,750 33.55 0.9102 29.63 0.8049 28.18 0.7569 28.80 0.8529 33.33 0.9342
HAT-L-F†(Ours) 66.04 240.03 8,888 33.75 0.9116 29.87 0.8091 28.31 0.7597 29.51 0.8637 33.36 0.9343
HAT-F-Basis-First†(Ours) 46.67 161.66 5,696 33.33 0.9085 29.47 0.8015 28.10 0.7554 28.57 0.8494 33.14 0.9336
HAT-F-Concat†(Ours) 45.52 129.05 4,826 33.46 0.9095 29.57 0.8035 28.16 0.7566 28.73 0.8518 33.28 0.9341
HR Bicubic BSRGAN SwinIR
Full Image ESRGAN RGT DAT FIPER (Ours)
HR Bicubic BSRGAN SwinIR
Full Image ESRGAN RGT DAT FIPER (Ours)
Figure4: Visualcomparisonsonsuper-resolution(4×).
andtheirrespectivebasesbn,n∈{1,...,M},weapplyaBasisMergingTransformerF ateach
i merge
locationtointegratetheM elements:
b (h,w)=F ({bn(h,w)|n∈1,...,M}), 0≤h<H , 0≤w <W . (9)
i merge i bi bi
Here,F isastandardtransformer,similarinarchitecturetothatdescribedin(Oquabetal.,2023).
merge
WetreatthebasesastokensandprependthesequencewithaCLStoken,concludingthesequenceto
formthefinalmergedbasis.
Sincecompressioninduceserror,accordingtoFig.3b,theCoefficientfeaturemapX generated
coeff
with Coefficient Backbone contains misinformation, and Basis Swin Transformer that uses such
featureswouldcapturetheinformationwrongfullyforthebasisandthusamplifyerror. Tosolvethis,
weexploitthemergeablepropertyofbasis,compressandtransmitthemergedbasisindependently
alongotherquantizedvariables{Q(y,µ), zˆ},andfinallyreconstructtheM imageswiththemerged
basisandtheirindividualdecodedcoefficients. asillustratedinFig. 3c. Thisway,wecanenjoyless
informationlosswithbasiswhilemaintaininglowbitrates.
4 EXPERIMENTS
4.1 IMAGESUPER-RESOLUTION
ExperimentalSetup. WeconductextensiveexperimentstovalidatetheeffectivenessofourFac-
torizedFieldsrepresentationforSuper-Resolutiontasks. Followingthestrategyoutlinedin(Chen
et al., 2023b) and (Chih-Chung Hsu, 2023), we adapt the same-task pretraining approach for all
theSuper-Resolutionmodels. Unlikethesepreviousworks,weleveragetheSA-1Bdatasetfrom
7(Kirillovetal.,2023),whichincludesapproximately10millionimages,wherewerandomlysample
4millionfortraining,whichismuchlessthatofImageNet(Dengetal.,2009),whichisusedfor
Chenetal.(2023b)andChih-ChungHsu(2023)pretraining. NotethatweuseSA-1Bjustforits
content-richandhigh-resolutionimagessinceSA-1Bisdesignedfortrainingsegmentationmodels,
whereasImageNetfocusesprimarilyonsingle-classprediction. Wefurtherconductexperimentsto
comparetrainingperformancewithSA-1BandImageNetinTab.1. Whiletheperformancesare
mostlyequivalent,wewitnessfasterconvergencewithSA-1Bduringtraining.
To be more focused on the representation itself, we utilize various pre-trained SR models
(SwinIR(Liangetal.,2021),HAT(Chenetal.,2023c),DAT(Chenetal.,2023d),andATD(Zhang
etal.,2024b))astheCoefficientBackbone,withaconsistentBasisTransformerarchitecture.Training
involvesinitializingtheCoefficientBackbonesfrompre-trainedSRmodelsandrandomlyinitializing
the Basis Transformers. Models are pre-trained for 300k iterations on the SA-1B dataset. After
pretraining,weuseDF2K(DIV2K(Agustsson&Timofte,2017)+Flickr2K(Limetal.,2017b))as
thefinetuningdatasetfollowing(Condeetal.,2022;Chenetal.,2023b)for200kiterations.
For pretraining, we utilize AdamW optimizer with learning rate 1e-4, batch size 16, betas (0.9,
0.99),andotherparameterssettoPyTorchdefault,whileweuselearningrate1e-5duringfinetuning.
Throughouttraining,theinputisrandomlycroppedto256×256andbicubiclyresizedto64×64
forCoefficientBackboneinput. AsforthehyperparameterofFactorizedFields,thenumberN of
coefficientandbasisissetto6,andthescalarsα aresetto{1,4,16,64}sincewewanttocapture
j
bothbasefrequencyα =1andhighfrequencyα =64information.
j j
Quantitative Results. Tab. 1 presents the quantitative comparison between our approach and
state-of-the-art(SoTA)methods. Weevaluatethemethodsusingfivebenchmarkdatasets,includ-
ingSet5(Bevilacquaetal.,2012), Set14(Zeydeetal.,2010), BSD100(Martinetal.,2001), Ur-
ban100(Huangetal.,2015),andManga109(Matsuietal.,2017). Forquantitativemetrics,PSNR
andSSIMarereported. Theaveragerelativeimprovementof204.4%inPSNRoverthebaseline
acrossthefivedatasetsiscalculatedusingtheformula(c−b)/(a−b),wherebrepresentsthePSNR
ofarepresentativebaseline,SwinIR,arepresentsthePSNRofSOTA,HAT-L,andcrepresentsthe
PSNR of our best model, HAT-L-F. This formula measures the relative performance gain of our
modelcomparedtothegapbetweenHAT-LandSwinIR.
To validate the effectiveness of our framework, we employ three different SoTA SR mod-
els—ATD Zhang et al. (2024a), DAT Chen et al. (2023d), and HAT Chen et al. (2023c)—as the
CoefficientBackboneinourpipeline,whichwedenoteasATD-F,DAT-F,andHAT-F,respectively.
ThesemodelsexhibitsignificantimprovementswhencomparedtotheircounterpartsATD-L,DAT-L,
andHAT-L,whichpossesssimilarparametercounts. ItisimportanttonotethatonlyChenetal.
(2023c)providesalarge-scalemodel. Tomaintainfairness,wescaleupATDandDATtomatch
thissizeandtrainthemunderthesameconfigurationasourmodel,includingbothpretrainingand
fine-tuningstages. Toensureanequitablecomparisonwithothermethods,wefurthertrainanother
model,HAT-F-ImageNet,usingImageNetasthepretrainingdataset,followingtheprotocolsoutlined
inChenetal.(2021;2023c). Theresultsdemonstratethatitsperformanceremainsconsistentwith
onlyminorperturbations.
Furthermore,intraditionalFourierSeriesandotherimageprocessingmethodsWallace(1992),the
basisistypicallyderivedfirstandthenusedtocomputethecoefficients. Incontrast,ourmethod
derivesthecoefficientfeaturesfirst,asillustratedinFig.2. Toexplorethisdifference,wedevelop
anothervariantofourmodel,denotedHAT-F-Basis-First,wherewereversetheorderofoperations.
Inthiscase,wefirstpasstheimagethroughtheBasisSwinTransformerandthenusetheresulting
basis features and the image input to derive the coefficients. This approach, however, leads to a
giganticperformancedrop,showingtheimportanceoftheorderofthepipeline. Specifically,we
arguethatinourpipeline,theCoefficientBackbonefunctionsmoreasafeatureextractionmodule,
wheretherefinedfeaturesfacilitatedownstreambasisextraction.
Lastly, to evaluate the effectiveness of our Factorized Fields, we trained a model named HAT-F-
Concat, which does not apply the formulation in Eq. 7. Instead, it concatenates the basis and
coefficientdirectlyanddecodestheresultingfeaturestoproducetheoutput. Althoughthisapproach
resultsinreducedperformance,whichindicatestherepresentationdoesactasanimperativerole
in modeling image information, the Basis Swin Transformer with Sawtooth downsampling still
8Table2: Comprehensiveevaluationforimagecompression. UsingVTMasananchorforcalculat-
ingBD-Rate. LatenciesaremeasuredunderanNVIDIAGTX3090GPU.
Latency(s)
Method BD-Rate(%)↓ Params(M)
TotEnc↓ TotDec↓
VTM 0.00 129.21 0.14 -
Xie(MM21’) -0.78 2.93 6.00 50.0
Cheng(CVPR22’) 5.44 1.98 4.69 29.6
STF(CVPR22’) -4.31 0.14 0.13 99.9
ELIC(CVPR22’) -7.24 0.07 0.09 36.9
TCM(CVPR23’) -11.74 0.16 0.15 76.7
TCM-HAT-L-F(Ours) -21.09 0.109 0.264 110.34
TCM-HAT-F-multiM=1(Ours) 27.96 0.232 0.174 131.35
TCM-HAT-F-multiM=2(Ours) 2.70 0.232 0.174 131.35
TCM-HAT-F-multiM=4(Ours) -10.11 0.232 0.174 131.35
TCM-HAT-F-multiM=8(Ours) -16.61 0.232 0.174 131.35
TCM-HAT-F-multiM=16(Ours) -19.88 0.232 0.174 131.35
TCM-HAT-F-multiM=24(Ours) -20.97 0.232 0.174 131.35
(a)Kodak (b)CLIC (c)Tecnick
Figure5: Performance(RD-Curve)evaluationonimagecompressionusingdifferentdatasets.
contributestoimprovedreconstruction,evenwithoutFactorized-Fieldsdecoding,highlightingits
effectiveness.
Visual Comparison. We provide the visual comparison in Fig. 4. The images are randomly
sampledfromtheDIV2Kdataset. Ourmethodfaithfullyreconstructstheimagedetails,whereasthe
otherapproachessufferfromover-smoothingorhallucinatingdetailsabsentinthegroundtruth.
4.2 SINGLE-ANDMULTI-IMAGECOMPRESSIONS
ExperimentalSetup. WeevaluateourFactorizedFieldsrepresentationforimagecompression
tasks, comparing it against state-of-the-art methods. Following our Super-Resolution setting in
Experiment Setup from Sec. 4.1, we use the same set of SA-1B for training. To emphasize our
representation,weinitializecompressionanddecompressionmodulesinFig.3frompre-trainedLiu
etal.(2023a)andtheSRModulefromthoseinSec. 4.1,wethentrainthepipelineendtoendon
256x256patchesfor200kiterations,withAdamWoptimizer(Loshchilov,2017)withlearningrate
1e-5,batchsize16,betas(0.9,0.99)andotherparameterssettoPyTorchdefault.
WeintegrateourSRModulewiththepre-trainedcompressionmoduleTCM,creatingTCM-HAT-F
andTCM-HAT-L-Fmodels. TCM-HAT-F-multirepresentsthemulti-imagecompressionpipeline.
Formulti-imagecompressioninFig. 3(c),wesettheBasisSwinTransformerandtheBasisMerging
Transformertobetrainablewhiletheotherpartsremainfrozen.
Rate-DistortionPerformanceComparison. WecompareourmodelwithState-of-the-Artlearned
end-to-endimagecompressionalgorithms,including(Liuetal.,2023b),(Chenetal.,2022b),Zou
et al. (2022), Xie et al. (2021), Cheng et al. (2020), Balle´ et al. (2018a), Li et al. (2024), Jiang
etal.(2023),Minnen&Singh(2020),Bellard,Qietal.(2023),andHeetal.(2022). Theclassical
imagecompressioncodec,VVC(Team,2021),isalsotestedbyusingVTM12.1.Therate-distortion
performanceonvariousdatasets,includingKodak,Tecnick’soldtestsetwithresolution1200×1200,
andCLICProfessionalValidation,isshowninFig. 5.
9Table3: ComparisonofimprovementsofFactorizedFields. ψandαarethesameinEq. 8
Metric PSNR↑ SSIM↑ LPIPS↓
Baseline(Chenetal.,2023a) 22.04 0.505 0.5296
Ours 38.44 0.999 0.0385
Noψtocontrolmagnitude 13.46 0.147 0.766
Noαforpixel-wisefrequencyinformation 21.25 0.537 0.527
Table4: ValidationoftheeffectivenessofSRprior. ThebestPSNRsaremarkedinred.
Kodak CLIC Tecnick
Method λ=0.0025 λ=0.0067 λ=0.025 λ=0.0025 λ=0.0067 λ=0.025 λ=0.0025 λ=0.0067 λ=0.025
bpp PSNR↑ bpp PSNR↑ bpp PSNR↑ bpp PSNR↑ bpp PSNR↑ bpp PSNR↑ bpp PSNR↑ bpp PSNR↑ bpp PSNR↑
TCMLiuetal.(2023a) 0.1533 30.0834 0.2983 32.5841 0.6253 36.1345 0.1214 31.8207 0.2235 34.2098 0.4503 37.1201 0.1268 32.0588 0.2193 34.3669 0.3981 36.9066
TCM-HAT-F-Scratch 0.1570 30.0857 0.2976 32.5893 0.6211 36.1389 0.1214 31.9421 0.2235 34.2894 0.4503 37.1434 0.1258 32.0632 0.2189 34.3781 0.4001 36.9223
TCM-HAT 0.1567 30.1843 0.2992 32.6454 0.6268 36.2267 0.1220 31.9737 0.2266 34.3319 0.4512 37.2486 0.1262 32.1423 0.2174 34.5124 0.3971 36.9934
TCM-HAT-F 0.1574 30.4012 0.2998 32.8910 0.6276 36.4461 0.1229 32.1917 0.2249 34.4109 0.4512 37.3135 0.1255 32.4591 0.2186 34.7656 0.3975 37.3244
In Tab. 2, our TCM-HAT-L-F model achieved a significant BD-Rate improvement of -21.09%
comparedtoVTM,outperformingpreviousstate-of-the-artmethods. Themulti-imagecompression
approach (TCM-HAT-F-multi) shows increasing performance gains with the number of images
compressedsimultaneously,reaching-20.97%BD-RateimprovementforM =24. Theresultshows
thatdirecttransmissionofbaseswouldreducetheerrorfromCoefficientBackbonetoBasisSwin
TransformerandthatthedistortionincreaseswithM,astheinformationcontainedinthemerged
basisislimited,andmergingmultiplebasesintoonewouldcauseincreasinginformationloss.
OuranalysisrevealsseveralsignificantadvantagesoftheFIPERframeworkinimagecompression
tasks. Theapproachdemonstratessubstantialimprovementsincompressionefficiencyacrossvarious
benchmarkdatasets,includingKodak,CLIC,andTecnick,indicatingitsbroadapplicability. No-
tably,themulti-imagecompressionstrategyshowsparticularlypromisingresultsforlargerimage
sets,suggestingscalabilitybenefits. Furthermore,ourmethodmaintainscompetitivelatencywhile
significantlyimprovingcompressionperformanceandbalancingefficiencyandquality.
4.3 ABLATIONSTUDIES
EffectivenessofFactorizedFieldsDesign. Weconductexperimentstoverifyourmodification
of Factorized Fields, modified from Eq. 3 to Eq. 7. The quantitative performance reported on
single-image regression is shown in Tab. 3, where each result is measured after 256 iterations.
Comparedtobaselineresults,ourrefinementsinmodelingpixel-levelfrequencyhavesignificantly
improvedallperformancemetrics. Additionally,ourresultsdemonstratethatthemodulationfunction
ψandthescalarαareinterdependent,eachessentialtotheother’sfunction.
InfluenceofSRPriorsinImageCompression. Weconductexperimentswithvariousconfigu-
rationstoverifytheproposedimagecompressionpipeline’seffectiveness. AsshowninTab.4,we
presentthequantitativeperformanceofmodelstrainedwithdifferentvaluesofλinEq.5.Specifically,
TCM-HATreferstosubstitutingourSRModulewiththeoriginalHATChenetal.(2023c)inthe
pipelineillustratedinFig.3.b. TCM-HAT-Frepresentsourcompletepipeline, whileTCM-HAT-
F-Scratch denotes the same pipeline but with the SR Module initialized randomly. Our results
demonstrate that integrating SR priors with image compression improves performance, and our
proposedrepresentationfurtherenhancesresults. ThishighlightstherobustnessofourFactorized
Fieldsincapturingfinedetailsinimageregressiontasks.
5 CONCLUSION
We proposed Factorized Fields, a representation that models implicit structures and patterns by
decomposingimagesintomulti-frequencycomponents. ThisapproachaddresseschallengesinSuper-
ResolutionandImageCompressionbyrestoringdetailsandpreservingvisualfidelity. Weintegrate
SRpriorswithImageCompressionforimprovedinformationrecoveryandintroduceabasismerging
techniqueforenhancedrenderingqualityacrossmultipleimages. Experimentsdemonstratestate-of-
the-artperformanceinbothSISRandImageCompressionbenchmarks,addressinglimitationsof
previousmethods.
10REFERENCES
Eirikur Agustsson and Radu Timofte. Ntire 2017 challenge on single image super-resolution:
Datasetandstudy. InTheIEEEConferenceonComputerVisionandPatternRecognition(CVPR)
Workshops,July2017. 8
JohannesBalle´,DavidMinnen,SaurabhSingh,SungJinHwang,andNickJohnston. Variationalim-
agecompressionwithascalehyperprior. InInternationalConferenceonLearningRepresentations,
2018a. URLhttps://openreview.net/forum?id=rkcQFMZRb. 1,4,9
JohannesBalle´,DavidMinnen,SaurabhSingh,SungJinHwang,andNickJohnston. Variationalim-
agecompressionwithascalehyperprior. InInternationalConferenceonLearningRepresentations,
2018b. 3
Fabrice Bellard. The bpg image format. http://bellard.org/bpg/. Last accessed on
09/20/2015. 9
MarcoBevilacqua,AlineRoumy,ChristineGuillemot,etal. Low-complexitysingle-imagesuper-
resolution based on nonnegative neighbor embedding. In Proceedings of the British Machine
VisionConference(BMVC),pp.1–10,2012. 8
AngCaoandJustinJohnson. Hexplane: Afastrepresentationfordynamicscenes. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.130–141,2023. 2,3,
4
LahiruD.Chamain,FabienRacape´,JeanBe´gaint,AkshayPushparaja,andSimonFeltman. End-to-
endoptimizedimagecompressionformachines,astudy. In2021DataCompressionConference
(DCC),2021. 1,3
EricR.Chan,ConnorZ.Lin,MatthewA.Chan,KokiNagano,BoxiaoPan,ShaliniDeMello,Orazio
Gallo,LeonidasGuibas,JonathanTremblay,SamehKhamis,TeroKarras,andGordonWetzstein.
Efficientgeometry-aware3Dgenerativeadversarialnetworks. InarXiv,2021. 2,3,4
AnpeiChen,ZexiangXu,AndreasGeiger,JingyiYu,andHaoSu. Tensorf: Tensorialradiancefields.
InEuropeanConferenceonComputerVision(ECCV),2022a. 2,3,4
AnpeiChen,ZexiangXu,XinyueWei,SiyuTang,HaoSu,andAndreasGeiger. Factorfields: A
unifiedframeworkforneuralfieldsandbeyond. arXivpreprintarXiv:2302.01226,2023a. 2,3,4,
10
FangdongChen,YumengXu,andLiWang. Two-stageoctaveresidualnetworkforend-to-endimage
compression. InProceedingsoftheAAAIConferenceonArtificialIntelligence,2022b. 9
HantingChen,YunheWang,TianyuGuo,ChangXu,YipingDeng,ZhenhuaLiu,SiweiMa,Chunjing
Xu,ChaoXu,andWenGao. Pre-trainedimageprocessingtransformer. InProceedingsofthe
IEEE/CVFconferenceoncomputervisionandpatternrecognition,pp.12299–12310,2021. 1,2,8
XiangyuChen,XintaoWang,WenlongZhang,XiangtaoKong,YuQiao,JiantaoZhou,andChao
Dong. Hat: Hybridattentiontransformerforimagerestoration. arXivpreprintarXiv:2309.05239,
2023b. 2,7,8
XiangyuChen,XintaoWang,JiantaoZhou,YuQiao,andChaoDong. Activatingmorepixelsin
imagesuper-resolutiontransformer. InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition(CVPR),pp.22367–22377,June2023c. 6,7,8,10
Z.Chen,Y.Zhang,J.Gu,L.Kong,X.Yang,andF.Yu. Dualaggregationtransformerforimage
super-resolution. In2023IEEE/CVFInternationalConferenceonComputerVision(ICCV),pp.
12278–12287,2023d. 1,2,6,7,8
ZhengChen,YulunZhang,JinjinGu,LingheKong,andXiaokangYang. Recursivegeneralization
transformer for image super-resolution. In The Twelfth International Conference on Learning
Representations,2024. 1,2,7
11ZhengxueCheng,HemingSun,MasaruTakeuchi,andJiroKatto. Learnedimagecompressionwith
discretizedgaussianmixturelikelihoodsandattentionmodules. InProceedingsoftheIEEE/CVF
conferenceoncomputervisionandpatternrecognition,pp.7939–7948,2020. 1,9
Yi-ShiuanChouChih-ChungHsu,Chia-MingLee. Drct: Savingimagesuper-resolutionawayfrom
informationbottleneck. arXivpreprintarXiv:2404.00722,2023. 2,7,8
MarcosVConde,Ui-JinChoi,MaximeBurchi,andRaduTimofte. Swin2SR:Swinv2transformerfor
compressedimagesuper-resolutionandrestoration. InProceedingsoftheEuropeanConference
onComputerVision(ECCV)Workshops,2022. 1,2,8
JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei. Imagenet: Alarge-scalehier-
archicalimagedatabase. In2009IEEEConferenceonComputerVisionandPatternRecognition,
pp.248–255,2009. doi: 10.1109/CVPR.2009.5206848. 8
ChaoDong,ChenChangeLoy,KaimingHe,andXiaoouTang. Imagesuper-resolutionusingdeep
convolutionalnetworks. IEEETransactionsonPatternAnalysisandMachineIntelligence,pp.
295–307,2014. 1,2
WenhongDuan,ZhengChang,ChuanminJia,ShansheWang,SiweiMa,LiSong,andWenGao.
Learnedimagecompressionusingcross-componentattentionmechanism. IEEETransactionson
ImageProcessing,2023. 3,6
DahuFeng,YanHuang,YiweiZhang,JunLing,AnniTang,andLiSong. Agenerativecompression
framework for low bandwidth video conference. In 2021 IEEE International Conference on
Multimedia&ExpoWorkshops(ICMEW),2021. 3
Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahbæk Warburg, Benjamin Recht, and Angjoo
Kanazawa. K-planes: Explicitradiancefieldsinspace,time,andappearance. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.12479–12488,2023.
2,3,4
DarioFuoli,LucVanGool,andRaduTimofte. Fourierspacelossesforefficientperceptualimage
super-resolution. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pp.2360–2369,2021. 4
QuankaiGao,QiangengXu,HaoSu,UlrichNeumann,andZexiangXu. Strivec: Sparsetri-vector
radiancefields. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pp.17569–17579,2023. 2,3
VivekK.Goyal. Theoreticalfoundationsoftransformcoding. IEEESignalProcessingMagazine,18
(5):9–21,2001. doi: 10.1109/79.952803. 1
ZongyuGuo, ZhizhengZhang, RunsenFeng, andZhiboChen. Causalcontextualpredictionfor
learnedimagecompression. IEEETransactionsonCircuitsandSystemsforVideoTechnology,
2022. 3
WangGuo-Hua,JiahaoLi,BinLi,andYanLu. EVC:Towardsreal-timeneuralimagecompression
withmaskdecay. InTheEleventhInternationalConferenceonLearningRepresentations,2023.
URLhttps://openreview.net/forum?id=XUxad2Gj40n. 1
D.He,Z.Yang,W.Peng,R.Ma,H.Qin,andY.Wang. Elic: Efficientlearnedimagecompression
withunevenlygroupedspace-channelcontextualadaptivecoding. In2022IEEE/CVFConference
onComputerVisionandPatternRecognition(CVPR),2022. 3,9
Jia-BinHuang,AbhishekSingh,andNarendraAhuja. Singleimagesuper-resolutionfromtrans-
formedself-exemplars. InProceedingsoftheIEEEConferenceonComputerVisionandPattern
Recognition(CVPR),pp.5197–5206,2015. 8
WeiJiang,JiayuYang,YongqiZhai,PeirongNing,FengGao,andRonggangWang. Mlic: Multi-
reference entropy model for learned image compression. In Proceedings of the 31st ACM In-
ternationalConferenceonMultimedia,pp.7618–7627,2023. doi: 10.1145/3581783.3611694.
9
12JointVideoExpertsTeam(JVET). VVCSoftware VTM-VTM-21.2. https://vcgit.hhi.
fraunhofer.de/jvet/VVCSoftware_VTM/-/tree/VTM-21.2, 2023. Accessed:
2023-10-23. 1
J.Kim,J.Lee,andK.Lee. Accurateimagesuper-resolutionusingverydeepconvolutionalnetworks.
In2016IEEEConferenceonComputerVisionandPatternRecognition(CVPR),pp.1646–1654,
2016. 2
AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,ChloeRolland,LauraGustafson,Tete
Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Dolla´r, and Ross Girshick.
Segmentanything. arXiv:2304.02643,2023. 8
CansuKorkmaz, AMuratTekalp, andZaferDogan. Traininggenerativeimagesuper-resolution
models by wavelet-domain losses enables better control of artifacts. In Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.5926–5936,2024. 4
A. Burakhan Koyuncu, Han Gao, Atanas Boev, Georgii Gaikov, Elena Alshina, and Eckehard
Steinbach. Contextformer: Atransformerwithspatio-channelattentionforcontextmodelingin
learned image compression. In Proceedings of the European Conference on Computer Vision
(ECCV),2022. 3
ABurakhanKoyuncu,PanqiJia,AtanasBoev,ElenaAlshina,andEckehardSteinbach. Efficient
contextformer: Spatio-channel window attention for fast context modeling in learned image
compression. IEEETransactionsonCircuitsandSystemsforVideoTechnology,2024. 3,6
C.Ledig,L.Theis,F.Huszar,J.Caballero,A.Cunningham,A.Acosta,A.Aitken,A.Tejani,J.Totz,
Z.Wang,andW.Shi. Photo-realisticsingleimagesuper-resolutionusingagenerativeadversarial
network. In2017IEEEConferenceonComputerVisionandPatternRecognition(CVPR),2017a.
1,2
ChristianLedig,LucasTheis,FerencHusza´r,JoseCaballero,AndrewCunningham,AlejandroAcosta,
AndrewAitken,AlykhanTejani,JohannesTotz,ZehanWang,etal. Photo-realisticsingleimage
super-resolutionusingagenerativeadversarialnetwork. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pp.4681–4690,2017b. 2
A.Li,L.Zhang,Y.Liu,andC.Zhu. Featuremodulationtransformer: Cross-refinementofglobalrep-
resentationviahigh-frequencypriorforimagesuper-resolution. In2023IEEE/CVFInternational
ConferenceonComputerVision(ICCV),2023. 2
DaxinLi,YuanchaoBai,KaiWang,JunjunJiang,XianmingLiu,andWenGao. Groupedmixer: An
entropymodelwithgroup-wisetoken-mixersforlearnedimagecompression. IEEETransactions
onCircuitsandSystemsforVideoTechnology,2024. 3,6,9
WenboLi,XinLu,ShengjuQian,JiangboLu,XiangyuZhang,andJiayaJia.Onefficienttransformer-
basedimagepre-trainingforlow-levelvision. arXivpreprintarXiv:2112.10175,2021. 2
JingyunLiang,JiezhangCao,GuoleiSun,KaiZhang,LucVanGool,andRaduTimofte. Swinir:
Imagerestorationusingswintransformer. arXivpreprintarXiv:2108.10257,2021. 2,7,8
BeeLim,SanghyunSon,HeewonKim,SeungjunNah,andKyoungMuLee. Enhanceddeepresidual
networks for single image super-resolution. 2017 IEEE Conference on Computer Vision and
PatternRecognitionWorkshops(CVPRW),pp.1132–1140,2017a. 2
BeeLim,SanghyunSon,HeewonKim,SeungjunNah,andKyoungMuLee. Enhanceddeepresidual
networksforsingleimagesuper-resolution. InTheIEEEConferenceonComputerVisionand
PatternRecognition(CVPR)Workshops,July2017b. 1,2,8
J.Liu,H.Sun,andJ.Katto.Learnedimagecompressionwithmixedtransformer-cnnarchitectures.In
2023IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).IEEEComputer
Society,2023a. 1,3,4,6,9,10
Jie Liu, Wenjie Zhang, Yuting Tang, Jie Tang, and Gangshan Wu. Residual feature aggregation
networkforimagesuper-resolution. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pp.2359–2368,2020. 2
13JinmingLiu,HemingSun,andJiroKatto. Learnedimagecompressionwithmixedtransformer-cnn
architectures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.14388–14397,2023b. 9
ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,ZhengZhang,StephenLin,andBainingGuo.
Swintransformer: Hierarchicalvisiontransformerusingshiftedwindows. InProceedingsofthe
IEEE/CVFinternationalconferenceoncomputervision,pp.10012–10022,2021. 2
ZeLiu,HanHu,YutongLin,ZhuliangYao,ZhendaXie,YixuanWei,JiaNing,YueCao,Zheng
Zhang,LiDong,etal. Swintransformerv2: Scalingupcapacityandresolution. InProceedingsof
theIEEE/CVFconferenceoncomputervisionandpatternrecognition,pp.12009–12019,2022. 6
ILoshchilov. Decoupledweightdecayregularization. arXivpreprintarXiv:1711.05101,2017. 9
QiMao,TinghanYang,YinuoZhang,ShuyinPan,MengWang,ShiqiWang,andSiweiMa. Extreme
imagecompressionusingfine-tunedvqganmodels. 2023a. 3
QiMao,TinghanYang,YinuoZhang,ShuyinPan,MengWang,ShiqiWang,andSiweiMa. Extreme
imagecompressionusingfine-tunedvqganmodels. ArXiv,2023b. 3
DMartin,CFowlkes,DTal,etal. Adatabaseofhumansegmentednaturalimagesanditsapplication
toevaluatingsegmentationalgorithmsandmeasuringecologicalstatistics. InProceedingsofthe
IEEEInternationalConferenceonComputerVision(ICCV),pp.416–423,2001. 8
Yusuke Matsui, Koichi Ito, Yuki Aramaki, et al. Sketch-based manga retrieval using manga109
dataset. MultimediaToolsandApplications,76(20):21811–21838,2017. 8
DavidMinnenandSaurabhSingh. Channel-wiseautoregressiveentropymodelsforlearnedimage
compression. In2020IEEEInternationalConferenceonImageProcessing(ICIP),pp.3339–3343,
2020. doi: 10.1109/ICIP40778.2020.9190935. 4,9
DavidMinnen,JohannesBalle´,andGeorgeDToderici. Jointautoregressiveandhierarchicalpriors
forlearnedimagecompression. Advancesinneuralinformationprocessingsystems,31,2018. 1
ThomasMu¨ller,AlexEvans,ChristophSchied,andAlexanderKeller. Instantneuralgraphicsprimi-
tiveswithamultiresolutionhashencoding. ACMTrans.Graph.,41(4):102:1–102:15,July2022.
doi: 10.1145/3528223.3530127. URLhttps://doi.org/10.1145/3528223.3530127.
2,3,4
MaximeOquab,Timothe´eDarcet,TheoMoutakanni,HuyV.Vo,MarcSzafraniec,VasilKhalidov,
PierreFernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,RussellHowes,Po-Yao
Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran,
Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut,
ArmandJoulin,andPiotrBojanowski.Dinov2:Learningrobustvisualfeatureswithoutsupervision,
2023. 7
Chenyang Qi, Xin Yang, Ka Leong Cheng, Ying-Cong Chen, and Qifeng Chen. Real-time 6k
imagerescalingwithrate-distortionoptimization. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition(CVPR),2023. 9
OrenRippelandLubomirBourdev. Real-timeadaptiveimagecompression. InProceedingsofthe
34thInternationalConferenceonMachineLearning(PMLR),2017. 3
Juan Song, Jiaxiang He, Mingtao Feng, Keyan Wang, Yunsong Li, and Ajmal Mian. High fre-
quencymatters: Uncertaintyguidedimagecompressionwithwaveletdiffusion. arXivpreprint
arXiv:2407.12538,2024. 3,6
Long Sun, Jinshan Pan, and Jinhui Tang. Shufflemixer: An efficient convnet for image super-
resolution. In Advances in Neural Information Processing Systems, pp. 17314–17326, 2022.
2
YingTai,JianYang,andXiaomingLiu. Imagesuper-resolutionviadeeprecursiveresidualnetwork.
InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pp.3147–3155,
2017. 2
14DavidTaubmanandMichaelMarcellin. JPEG2000ImageCompressionFundamentals,Standards
andPractice. SpringerPublishingCompany,Incorporated,2013. 1,3
JointVideoExpertsTeam. Vvcofficialtestmodelvtm. 2021. 9
TongTong,GenLi,XiejieLiu,andQinquanGao. Imagesuper-resolutionusingdenseskipconnec-
tions. InProceedingsoftheIEEEinternationalconferenceoncomputervision,pp.4799–4807,
2017. 2
G.K. Wallace. The jpeg still picture compression standard. IEEE Transactions on Consumer
Electronics,38(1):xviii–xxxiv,1992. doi: 10.1109/30.125072. 1,3,8
XintaoWang,KeYu,ShixiangWu,JinjinGu,YihaoLiu,ChaoDong,YuQiao,andChenChange
Loy. Esrgan: Enhancedsuper-resolutiongenerativeadversarialnetworks. InComputerVision–
ECCV2018Workshops,2019. 2
XintaoWang,LiangbinXie,ChaoDong,andYingShan.Real-esrgan:Trainingreal-worldblindsuper-
resolutionwithpuresyntheticdata. In2021IEEE/CVFInternationalConferenceonComputer
VisionWorkshops(ICCVW),2021. 2
H.Wu,B.Xiao,N.Codella,M.Liu,X.Dai,L.Yuan,andL.Zhang. Cvt: Introducingconvolutions
tovisiontransformers. In2021IEEE/CVFInternationalConferenceonComputerVision(ICCV),
2021. 3
YueqiXie,KaLeongCheng,andQifengChen. Enhancedinvertibleencodingforlearnedimage
compression. InProceedingsoftheACMInternationalConferenceonMultimedia,pp.162–170,
2021. 9
R Zeyde, M Elad, and M Protter. On single image scale-up using sparse-representations. In
ProceedingsoftheInternationalConferenceonCurvesandSurfaces(ICCS),pp.711–730,2010. 8
LehengZhang,YaweiLi,XingyuZhou,XiaoruiZhao,andShuhangGu. Transcendingthelimit
of local window: Advanced super-resolution transformer with adaptive token dictionary. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
2856–2865,2024a. 2,6,8
LehengZhang,YaweiLi,XingyuZhou,XiaoruiZhao,andShuhangGu. Transcendingthelimit
of local window: Advanced super-resolution transformer with adaptive token dictionary. In
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
2856–2865,2024b. 7,8
YulunZhang,KunpengLi,KaiLi,LichenWang,BinengZhong,andYunFu. Imagesuper-resolution
usingverydeepresidualchannelattentionnetworks. InProceedingsoftheEuropeanConference
onComputerVision(ECCV),2018a. 2
Yulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for
imagesuper-resolution. InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pp.2472–2481,2018b. 2
ShangchenZhou,JiaweiZhang,WangmengZuo,andChenChangeLoy. Cross-scaleinternalgraph
neuralnetworkforimagesuper-resolution. Advancesinneuralinformationprocessingsystems,
33:3499–3509,2020. 2
QiangZhu,PengfeiLi,andQianhuiLi. Attentionretractablefrequencyfusiontransformerforimage
superresolution. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pp.1756–1763,2023. 2
YinhaoZhu,YangYang,andTacoCohen. Transformer-basedtransformcoding. InInternational
ConferenceonLearningRepresentations,2022. 3
Renjie Zou, Chunfeng Song, and Zhaoxiang Zhang. The devil is in the details: Window-based
attentionforimagecompression. InCVPR,2022. 9
15A ARCHITECTURE DETAILS
Figure6: DetailedarchitectureofSuper-ResolutionModules.
Figure7: DetailedarchitectureofCompressionPipelineinFig.3(b).
Super-Resolution ThearchitectureofourSuper-ResolutionmodulesisshowninFig.6. Weextract
the feature map X of Coeff Backbone after the last layer, before upsampling. The hidden
coeff
16
elpmaspu
htiw
kcolB
MCT
elpmaspu
htiw
kcolB
MCT
ced
krowteN
ecilS
cne
cne ced
elpmasnwod
htiw
kcolB
MCT
elpmaspu
htiw
kcolB
MCT
vnoC
elpmasnwod
htiw
kcolB
MCT
eludoM
RS
ruOdimensionofBasisSwinTransformerissetto384with8headsinattention,andeachblockscontains
2Swinv2Layerswithwindowattention. Thesawtooth-awaredownsamplereducestheheightand
widthbyhalf,whereupsamplescalarofPixelshuffleforeachbasisoutputissetto4.
ImageCompression ThearchitectureofourImageCompressionpipelineofFig.3(b)isshownin
Fig.7. Weextractintermediatefeatureswithheightandwidth128andconvolvewithstride2forSR
Moduleinput.
B VISUALIZATION
HR Bicubic SwinIR RGT DAT FIPER (ours)
Figure8: Visualcomparisonsonsuper-resolution(4×).
17