Beyond position: how rotary embeddings shape representations and
memory in autoregressive transfomers
Valeria Ruscio Fabrizio Silvestri
Sapienza University of Rome
ruscio@diag.uniroma1.it, fabrizio.silvestri@diag.uniroma1.it
Abstract are inherently permutation-invariant, incorporating
positional information is crucial for the model to
understand the sequential nature of language. Early
Rotary Positional Embeddings (RoPE) en-
methods used fixed sinusoidal encodings, while
hance positional encoding in Transformer
newer strategies, like Rotary Positional Embeddings
models, yet their full impact on model dy-
(RoPE)[Su et al., 2024], apply rotation matrices di-
namics remains underexplored. This pa-
rectly to the token embeddings, integrating positional
per studies how RoPE introduces position-
information through position-dependent rotations.
dependent rotations, causing phase shifts
These rotations introduce phase shifts in the embed-
in token embeddings that influence higher-
dings’ vector components, effectively encoding each
frequency components within the model’s
token’s position within the sequence.
internal representations. Through spectral
Despite RoPE’s effectiveness, its impact on the in-
analysis, we demonstrate that RoPE’s rota-
ternal dynamics of Transformer models—particularly
tion matrices induce oscillatory behaviors in
its interaction with non-linear components like
embeddings, affecting information retention
feed-forward neural networks (FFNs) and self-
across layers and shaping temporal model-
attention—remains underexplored. Understanding
ing capabilities. We show that activation
this interaction is essential for comprehending how
functions in feed-forward networks interact
Transformers process and retain sequential informa-
with RoPE-modulated embeddings to gener-
tion, essential for tasks requiring nuanced temporal
ate harmonics, leading to constructive or de-
modeling. Our work studies three main aspects: the
structive interference based on phase align-
spectral properties of RoPE, its interactions with
ment. Our findings reveal that phase align-
non-linear activation functions, and the emergence of
ment amplifies activations and sharpens at-
constructive and destructive interference patterns due
tention, while misalignment weakens activa-
to phase alignment.
tions and disrupts focus on positional pat-
Our main contributions are: 1) Providing a detailed
terns. This study underscores the impor-
analysis of RoPE’s impact on internal model behavior
tance of frequency components as intrinsic
and positional dependency modeling. 2) Studying
elements of model behavior, offering new in-
how non-linearities in Transformers interact with
sights beyond traditional analyses.
RoPE to produce complex frequency patterns.
1 Introduction 2 Related Works
Central to the Transformer architecture The Transformer architecture introduced by
[Vaswani, 2017] is the self-attention mechanism, [Vaswani, 2017] revolutionized sequence modeling
which allows the model to weight the influence of using self-attention mechanisms, making recurrent or
different parts of the input sequence when generating structures not necessary anymore. To provide the
representations. However, because transformers model with information about the token position in
thesequence, theoriginalTransformerusedsinusoidal
positional encodings added to the input embeddings.
[Shaw et al., 2018] proposed relative positional en-
codings, integrating relative position information into
Preprint version, work in progress.
self-attention to better generalize across varying se-
4202
tcO
32
]GL.sc[
1v76081.0142:viXraBeyond position: how rotary embeddings shape representations and memory in autoregressive
transfomers
quence lengths. Dai et al. [Dai et al., 2018] extended passed the input through the model to obtain the to-
this with Transformer-XL, enhancing the capture of ken embeddings before the application of positional
long-term dependencies. [Huang et al., 2018] intro- encodings. After we manually applied rotation ma-
ducedtheMusicTransformer,whichleveragesrelative trices to these embeddings to simulate phase shifts,
positional encoding to model long-term structure in the embeddings were then fed back into the model to
music generation tasks. studyhowthesimulatedphaseshiftsinfluencedtheat-
[Su et al., 2024] introduced Rotary Position Em- tention scores. This experiment aimed to understand
bedding (RoPE), applying rotation matrices to the sensitivity of the model’s attention mechanism to
embeddings to introduce relative positional depen- phase differences introduced by RoPE.
dence through phase shifts, preserving inner product b)SyntheticSequenceonFFNActivations: Thesecond
structures and enabling generalization to longer set of experiments focused on studying the effects of
sequences. phase alignment and misalignment on the activations
Nonlinear systems, such as feedforward neural within the FFN of the models. We generated syn-
networks with activation functions like ReLU thetic sequences to isolate the impact of input struc-
or GeLU, can generate higher-order harmon- tureonphaseinteractionswithoutalteringtheembed-
ics and exhibit frequency mixing as studied by ding rotations. These sequences consisted of repeated
[Selesnick and Burrus, 1998]. Constructive and instancesofthesametoken, sincethetokensareiden-
destructive interference are fundamental concepts tical, their embeddings (prior to positional encoding)
in signal processing [Oppenheim, 1999], describing are the same, promoting phase alignment after RoPE
how overlapping waves can amplify or attenuate is applied, and sequences constructed by alternating
signals based on their phase alignment. By applying between different tokens, resulting in varying embed-
principles from signal processing, researchers have dings and promoting phase misalignment. Each type
begun to analyze how frequency components and comprised250. Wecomputedmetricssuchasvariance,
phase relationships affect neural network behavior. kurtosis, entropy, and the number of activation peaks
[Chi et al., 2020] investigated how convolutional to quantify differences in activation patterns between
neural networks process frequency information, high- aligned and misaligned inputs.
lighting the importance of understanding the spectral Experiments were performed on Google Colab Pro
characteristics of activations. [Takahashi et al., 2018] with NVIDIA A100 GPU, using the Hugging Face
explored adaptive modulation in neural networks, Transformers library.
drawing parallels between neural activations and
signal modulation techniques.
4 Spectral Properties
3 Methodology
RoPE applies a rotation to the embedding vectors,
parametrized by a frequency θ for each dimension.
k
Our study combines theoretical analysis with empir-
For a position p, the embedding e(p) is rotated as:
ical experiments to explore how Rotary Positional
Embeddings (RoPE) influence Transformer models.
e(p)=e(p)cos(θ p)+e⊥(p)sin(θ p) (1)
WeconductedexperimentsusingautoregressiveTrans- k k
former models that implement Rotary Positional Em-
beddings (RoPE) for consistent positional encoding, where e⊥(p) is the orthogonal component of e(p).
specifically LLaMA 2, LLaMA 3 and LLaMA 3.1 The spectral properties of the rotation matrices intro-
[Touvron et al., 2023]. The primary results presented duced by RoPE can be analyzed by considering the
inthemainpaperareperformedusingLLaMA3,with eigenvalues of these matrices. The eigenvalues of the
full experimental details and additional results pro- rotation matrices represent the frequencies at which
videdintheappendix. Ourstudycomprisestwotypes the embeddings are modulated due to the positional
of experiments aimed at understanding how phase encoding. Bycomputingtheseeigenvalues,wecanun-
shifts introduced by RoPE affect the models’ internal derstand how RoPE introduces phase shifts that vary
dynamics: with position, effectively encoding positional informa-
a) Phase Shift Simulations on Real Embeddings: In tion into the embeddings.
thefirstsetofexperiments,weinvestigatedtheimpact Since these are rotation matrices, the eigenvalues lie
of manual phase shifts on the attention mechanism by on the unit circle in the complex plane, meaning that
applyingrotationaltransformationstotheembeddings therotationpreservesthenormoftheembeddingsbut
extracted from the models. We used 1,000 text sam- alterstheirphase. Specifically,therotationinducesos-
ples, each containing 200 tokens, sourced from The cillatory behavior in the embedding space, which can
BookCorpus [Zhu, 2015]. For each text sample, we be decomposed into frequency components.Valeria Ruscio, Fabrizio Silvestri
4.1 Eigenvalue and Spectral Analysis of smaller θ values allows the model to retain and in-
k
RoPE tegrate information from earlier parts of the text, en-
hancing its ability to understand overarching themes
TodeepenourunderstandingofhowRoPEaffectsthe and plot developments. On the other hand, in real-
embeddings’ behavior in Transformers, we examine time translation systems where recent words are more
the eigenvalues of the rotation matrices R k(p) used relevant, larger θ
k
values enable the model to quickly
in the positional encoding. While rotation matrices adapt to new inputs, improving translation accuracy
are known to have eigenvalues on the unit circle, ana- for rapidly changing contexts.
lyzing them in the context of RoPE reveals important
implications for the model’s temporal dynamics and
5 Memory Dynamics
memory retention capabilities.
Consider the 2D rotation matrix for dimension k:
In Transformer models, RoPE modifies the incorpora-
(cid:18) (cid:19)
R (p)=
cos(ϕ k) −sin(ϕ k)
(2)
tion of positional information by applying a rotation
k sin(ϕ k) cos(ϕ k) matrix R(p) to the queries and keys at each position
p:
where ϕ k =θ k(p). Q′(p)=R(p)Q(p),K′(p)=R(p)K(p) (4)
The eigenvalues λ of R (p) satisfy:
k
The attention score between positions p and q be-
λ=cosϕ k±isinϕ
k
=e±iϕk (3)
comes:
These eigenvalues lie on the unit circle in the complex
Q′(p)·K′(q) Q(p)⊤R(p)⊤R(q)K(q)
plane, with magnitude |λ| = 1 and phase ϕ = θ (p). S(p,q)= √ = √
k k
d d
This result illustrates that the rotation induced by k k
(5)
RoPE causes the embeddings to oscillate with a fre-
Using the property that the transpose of a rotation
quency determined by θ , and the phase changes lin-
k matrixequalsitsinverse,R (p)⊥ =R (p)−1,andthat
early with position p. k k
R (p)−1R(q) = R(q − p), we simplify the attention
This analysis shows how the choice of θ affects the k
k score to:
model’s sensitivity to positional changes. By under-
standingtheeigenvalues,wecaninferhowRoPEmod- Q(p)⊤R(q−p)K(q)
S(p,q)= √ (6)
ulates the embeddings to prioritize either long-term
d
k
dependencies(slowphasechanges)orshort-terminter-
actions (rapid phase changes), thereby impacting the Thisexpressionshowsthattheattentionscoredepends
model’s temporal dynamics and memory retention. solelyontherelativeposition∆p=q−p. Therotation
Implications for temporal dynamics as the se- R∆p introduces a phase shift affecting the alignment
quenceprogresses,theembeddingsundergocontinuous between Q(p) and K(q). As |∆p| increases, the phase
rotations in their respective 2D subspaces. The lin- difference grows, potentially reducing the dot product
ear change in phase with respect to position p means due to misalignment. This leads to a natural decay of
that embeddings at different positions have different attentionscoreswithincreasingpositionaldistance,ef-
phases, leading to oscillatory behavior. This directly fectively incorporating relative positional information
impacts how the model processes sequential informa- into the model.
tion,asthephasedifferencesinfluencethedot-product
attention calculations, leading to constructive or de- 5.1 Relationship between θ and similarity
k
structive interference patterns.
Implications for Memory Retention and Tem- Therelationshipbetweenthepositionaldifference(p−
poral Processing the frequency θ determines the q) and the frequency θ plays a key role in the simi-
k k
rate of phase change. Smaller values of θ result in larity between embeddings. When (p−q)θ ≈ π, the
k k
slower phase changes, meaning that embeddings re- cosineofthisvalueiscloseto-1, whichcanleadtoin-
tain more similarity over longer positional distances. teresting behaviors in how tokens at positions p and q
This corresponds to longer memory retention in the are compared: i) Close Tokens, ii) Decay of Similarity
model. Conversely,largerθ valuesleadtofasterphase and iii) Sign Flips and Oscillations.
k
changes, focusing the model’s attention on short-term When tokens are close in position, (p−q)θ is small,
k
dependencies. By selecting appropriate values for θ , and cos(p−q)θ is close to 1. This results in a high
k k
we can control the timescale over which the model re- similarity between their embeddings, as expected. As
tainsinformation,effectivelytuningtheTransformer’s (p−q)θ increases, the cosine value decreases, reach-
k
temporal processing capabilities. For example, in lan- ing zero at (p−q)θ = π. This implies a decay in
k 2
guage modeling tasks involving long texts, setting similarity between tokens as their positional distanceBeyond position: how rotary embeddings shape representations and memory in autoregressive
transfomers
modulations into the attention mechanism, the mem-
ory function can be approximated as:
K
(cid:88)
M(n)= α cos(θ n) (7)
k k
k=1
Here, θ are the frequency components introduced
k
by RoPE, and α are weighting factors representing
k
the contribution of each frequency component. The
attention score between positions t and t − n is
modulated by cos(θ n), indicating that the influence
k
of position t − n on position t oscillates and decays
based on θ .
k
This formulation resembles a Fourier series, where the
Figure 1: This plot shows how the cosine similarity memoryfunctionM(n)capturesthecumulativeeffect
between token embeddings decays with increasing po- ofpastinputsasasuperpositionofoscillatorycompo-
sitional distance, depending on the frequency param- nents. The frequencies θ k and weights α k determine
eter θ . which temporal frequencies are emphasized or attenu-
k
ated as information propagates through the network.
If α decreases for higher frequencies, the memory of
k
grows. Once(p−q)θ surpasses π,thecosinebecomes distant past inputs decays more rapidly. The weights
k 2
negative. This leads to a situation where tokens that α canbeinfluencedbyboththeattentionmechanism
k
should be similar exhibit negative similarity, meaning andthetransformer’sinherentfrequencyresponse,po-
their embeddings no longer align as intended. For ex- tentially evolving across layers as the model dynami-
ample, an identical query might match a random key cally adjusts its emphasis on different frequency com-
better than an identical key due to the cosine’s sign ponents during processing.
flip. This happens because if xk ·xk is positive (like Viewing the attention mechanism as performing tem-
p q
whentheembeddingsaresimilar),butcos((p−q)θ )is poralfiltering,weformallyderivethememoryfunction
k
negative, the similarity becomes negative. For a ran- using Fourier analysis. Suppose the input sequence x
t
domkey,theinnerproductxk·xk islikelytobearound is decomposed into its frequency components:
p q
0, so the similarity would not be negative, making it
appear as a better match than an identical key. x
t
=(cid:88) A keiθkt (8)
When(p−q)θ ≥π,thecosinestartstoincreaseagain,
k k
which contradicts the idea of continuous decay 1. The
solution is bounding (p−q)θ k to ensure that it stays Here, A k are the Fourier coefficients, and θ k are the
within a range where the cosine function behaves in a corresponding frequencies. The attention mechanism
decaying way. modulates these components by cos(θ n):
k
Figure 1 shows how the average cosine similarity
changes when the positional distance for Various θ
k Attention(x ,x )∼A cos(θ n) (9)
t t−n k k
values changes. The plot shows that embeddings with
smallerθ valuesmaintainhighersimilarityoverlonger
k The memory function then becomes:
distances compared to those with larger θ valuesθ
k k
values, where small θ = 0.01, medium θ = 1 and
k k (cid:88)
M(n)= |A |2cos(θ n) (10)
large θ =100. k k
k
k
5.2 Constructing the Memory Function
This represents the cumulative influence of past in-
puts, modulated by the frequency components θ in-
ToanalyzehowRoPEinfluencesthemodel’sabilityto k
troduced by RoPE. The presence of cosine terms im-
retain information over time, we consider the memory
plies that the memory function oscillates over time,
function M(n), representing the cumulative effect of
with periods and amplitudes determined by θ and
past inputs at lag n. Since RoPE introduces cosine k
α . Depending on the distribution of θ , the model
k k
1This behaviour is due to cosine being an oscillatory may retain or attenuate certain temporal frequencies
function. as information flows through the layers.Valeria Ruscio, Fabrizio Silvestri
6 interactions with attention
mechanism
In Transformer models, the attention mechanism se-
lectively amplifies or suppresses different parts of the
sequence based on learned attention weights, allow-
ing the model to focus on relevant information. When
combined with Rotary Position Embedding (RoPE),
the interplay between phase shifts and nonlinearity
leads to complex interactions where certain frequency
components are enhanced or suppressed.
RoPE introduces phase shifts to the embeddings by
applying a rotation that depends on the token’s posi-
tion. Specifically, for each pair of embedding dimen-
Figure 2: This scatter plot correlates attention scores
sions(2k,2k+1),theembeddingisrotatedbyanangle
withthecosinemodulationtermcos(∆θ),where∆θ =
proportional to the position index p:
θ (p−q) represents the phase difference.
k
q(2k) =q(2k)cos(θ p)−q(2k+1)sin(θ p) (11)
p k k
key vectors is modulated by a cosine function of the
q(2k+1) =q(2k)sin(θ p)+q(2k+1)cos(θ p) (12)
p k k relative position:
and similarly for the key vectors k :
q q⊤k ∝cos(θ (p−q)) (18)
p q k
k(2k) =k(2k)cos(θ q)−k(2k+1)sin(θ q) (13)
q k k The rotational modulation of the embeddings by
RoPE creates a form of frequency-based filtering,
k(2k+1) =k(2k)sin(θ q)+k(2k+1)cos(θ q) (14)
q k k wherecertainphasedifferencesleadtoconstructivein-
whereq2k,q2k+1,k2kandk2k+1arecomponentsofthe terference(higherattentionweights),whileotherslead
to destructive interference (lower attention weights).
originalqueryandkeyvectors,andθ isthefrequency
k
Thisresultsinanoscillatorybehaviorintheattention
parameter associated with the k-th dimension.
scores, effectively modulating how different positions
This means that the attention mechanism becomes
interact based on their relative distances and the fre-
sensitive not just to the content of the tokens but also
quency parameters θ .
to their relative positions. By encoding positional dif- k
To investigate how phase differences introduced by
ferences into the phase of embeddings, RoPE allows
RoPE affect the attention mechanism, we plotted the
themodeltocapturepatternsthatdependonthedis-
attention scores against the cosine of the phase differ-
tance between tokens, such as syntax structures or
ence (cos∆θ) between token embeddings in Figure 2.
rhythmic patterns in text.
The plot reveals a pronounced peak at (cos∆θ) = 1,
The attention score between positions p and q is com-
corresponding to a phase difference of zero. This
puted as:
q⊤k indicates that the model assigns the highest atten-
s pq = √p q (15) tion when there is no phase shift between tokens.
d
k As (cos∆θ) decreases, the attention scores decline
Substituting the rotated query and key vectors, and sharply, showingthemodel’ssensitivitytoevenminor
applying the rotation identities, we find that the dot phase differences. This behavior suggests that phase
product depends on the relative position ∆p=q−p: alignment plays a critical role in the attention mech-
(cid:88) (cid:104) (cid:105) anism, with misaligned phases leading to diminished
q⊤ pkq = k q(2k)k(2k)+q(2k+1)k(2k+1) cos(θ k∆p) attention weights. Figure 3 illustrates the relation-
(16) shipbetweenattentionscoresandpositionaldifference
This shows that the attention score is modulated by a (p−q). The attention scores are highest when the po-
cosinefunctionoftherelativeposition,introducingan sitional difference is zero and decrease rapidly as the
oscillatory behavior: absolute value of (p−q) increases. This pattern indi-
cates that the model predominantly focuses on tokens
s =
√1 (cid:88)(cid:16) q[k]·k[k](cid:17)
cos(θ (q−p)) (17)
thatareclosetoeachotherinthesequence. Therapid
pq k
d k declineinattentionwithincreasingpositionaldistance
k
suggests that RoPE effectively encodes positional in-
whereq[k]·k[k] =q2kk2k+q2k+1k2k+1. Thisexpression formation, causing the model to prioritize local con-
shows that the dot product between the query and text.Beyond position: how rotary embeddings shape representations and memory in autoregressive
transfomers
relationship between the input and the output, pre-
serving the original frequencies introduced by RoPE.
2) The second-order term
(spq−s¯p)2
, introduces
2N
nonlinearitybysquaringtheinput. Thistermcangen-
eratenewfrequencycomponentsthatareharmonicsof
the original frequencies, effectively doubling frequen-
cies or creating combinations.
3) Subsequent higher-order terms in the expan-
sioncanintroduceevenmorecomplexfrequencyinter-
actions, including frequency mixing and the creation
of new frequency components not present in the orig-
inal signal.
These higher-order effects can be significant when the
input (for example, the modulated dot product) has
Figure 3: This scatter plot captures the attention
strong frequency components due to RoPE. The non-
scores between tokens at different positional distances
linearityofthesoftmaxfunctioncanamplifyoratten-
(p−q).
uate certain frequencies, depending on the phase and
amplitude of the input signal.
Inpractice,thiswouldbeveryusefulintime-seriespre- By expanding the softmax function, we observe
diction tasks, since this property enables the model to that the RoPE-induced frequencies don’t simply pass
focusonperiodicpatternsinherentinthedata,suchas through the attention mechanism unaltered. Instead,
daily or seasonal cycles. By adjusting θ , we can tune they interact with the softmax’s nonlinearity, which
k
the model to be more sensitive to specific frequencies, can enhance or suppress specific frequency compo-
improvingitsabilitytoforecastfuturevaluesbasedon nents. The softmax function can be thought of as ap-
historical periodic trends. plyinganonlineartransformationthatemphasizesthe
mostsignificantattentionscores(high-frequencycom-
ponentsresultingfromconstructiveinterference)while
7 interactions with the Sfotmax
diminishinglessimportantones(frequenciesleadingto
destructive interference). This selective enhancement
Theattentionweightsarecomputedusingthesoftmax
resemblestheeffectofaresonancefilterthatamplifies
function:
certain frequencies, shaping the model’s sensitivity to
exp(s )
α = pq (19) specific positional relationships.
pq (cid:80) exp(s )
j pj
The softmax function is highly sensitive to variations 8 interactions with the FFN
in the attention scores s . The oscillatory terms
pq
cos(θ k(p−q)) introduced by RoPE can lead to con- Aftertheattentionmechanism,Transformerstypically
structiveordestructiveinterference,dependingonthe apply a feedforward neural network (FFN), introduc-
relative positions and frequency parameters. Small ing additional nonlinearity. The FFN can be repre-
changes in s pq can cause significant differences in the sented as:
attention weights α , amplifying or attenuating the
pq
contributions from different positions. z =W 2·σ(W 1x+b 1)+b 2 (21)
To analyze how the nonlinearity of the softmax func-
wherexistheinput(theoutputoftheattentionmech-
tion affects the frequency components, we can con-
anism, already RoPE-modulated), W and W are
sideraTaylorseriesexpansionofthesoftmaxfunction 1 2
weight matrices, b and b are bias terms, and σ is
around the mean s¯ : 1 2
p a non-linear activation function (ReLU or GeLU).
1 s −s¯ (s −s¯ )2 The non-linear activation function σ introduces fur-
softmax(s pq)≈
N
+ pq
N
p+ pq
2N
p +... (20) ther nonlinearity by transforming the input in a non-
linear fashion. This nonlinearity interacts with the
where N is the number of positions in the sequence, frequency components of the input in several ways: i)
and s¯ is the mean of s over q. HarmonicGeneration,wherenon-linearitiescanin-
p pq
Bysubstitutingthephase-dependentdotproductinto troducehigher-orderharmonics,addingnewfrequency
the expansion, we can see how the nonlinear softmax components at integer multiples of the original fre-
function introduces higher-order terms that influence quencies. ii) Waveform Distortion, where the ac-
the frequency components: tivation function can distort the waveform of the in-
1) The first-order term spq−s¯p, captures the linear put signal, altering the amplitude and phase of the
NValeria Ruscio, Fabrizio Silvestri
frequency components. iii) Phase Shifts, where the Non-linearactivationsintroducehigher-orderharmon-
linear transformations before and after the activation ics when applied to oscillatory inputs like the RoPE-
function can introduce phase shifts, affecting how dif- modulated activations h (p). This phenomenon is
k
ferent frequency components combine in the network. well-known in signal processing, where non-linearities
applied to sinusoidal signals generate new frequency
components (harmonics) at integer multiples of the
8.1 Phase Modulation in the FFN with
original frequencies.
RoPE
Considering the ReLU activation:
Letx(p)representtheRoPE-modulatedoutputofthe
z (p)=ReLU(h (p)) (24)
attention head for the token at position p: k k
For an oscillatory input h (p) containing components
k
x(p)=R(p)·a(p) (22)
like cos(θ p) and sin(θ p), ReLU clips the negative
k k
parts of the oscillation, resulting in higher-order har-
where a(p) is the unmodulated attention output, and
monics. Specifically: i) The fundamental frequency
R(p) is the RoPE rotation matrix applied at position
θ remains present in the output. ii) Higher harmon-
k
p. Foragivendimensionpairk,k+1,RoPEintroduces
ics such as 2θ , 3θ , etc., are generated due to the
k k
phase shifts:
non-linearity introduced by ReLU. Mathematically,
this harmonic generation can be described using the
x k(p)=a k(p)cos(θ kp)+a k+1(p)sin(θ kp)x k+1(p) Fourier series expansion of a half-wave rectified sinu-
=−a (p)sin(θ p)+a (p)cos(θ p) soid. For example:
k k k+1 k
∞
where a k(p) and a k+1(p) are components of a(p), and
ReLU(cos(θ p))=
1 +1
cos(θ p)−
1 (cid:88) cos(2nθ kp)
θ is the frequency parameter associated with the k- k π 2 k π 4n2−1
k n=1
th dimension. For the k-th neuron, substituting the (25)
phase-shifted expressions for x (p) and x (p), we Thisexpansionshowsthathigher-orderharmonicsare
k k+1
get: introduced into the signal.
These harmonics enable the FFN to capture multi-
h (p)=[w cos(θ p)−w sin(θ p)]a (p) scaledependenciesinthesequence. SinceRoPEcauses
k k1 k k2 k k
embeddings at different positions to oscillate with dif-
+[w sin(θ p)+w cos(θ p)]a (p)+b
k1 k k2 k k+1 1k
ferent phases, the non-linearity can lead to construc-
tive or destructive interference, depending on phase
The first linear layer in the FFN mixes the phase-
alignment.
shiftedcomponentsoftheattentionoutput. Thecoef-
Constructive Interference occurs when the phase
ficients w and w are modulated by trigonometric
k1 k2
shifts of different components h (p) are aligned;
functions of the token’s position p, which means that k
the non-linearity amplifies the output, resulting in
the contribution of the attention outputs a (p) and
k
stronger FFN activations. Thus, certain positional
a (p) to the FFN activations depends on the posi-
k+1
configurations reinforce each other, leading to higher
tional phase shifts introduced by RoPE.
neuron activations.
This has two major effects:
Destructive Interferencehappenswhenthephases
1) Frequency Mixing: The interaction between the
aremisaligned;componentscancanceleachotherout,
weight matrix W and the RoPE-induced phase shifts
1
leading to weaker activations or even zero activations.
leads to frequency mixing. Each dimension of the em-
ThisallowstheFFNtoselectivelysuppresscertainpo-
bedding oscillates with a frequency determined by θ .
k
sitional patterns that are not relevant for the current
The linear transformation effectively mixes these os-
token.
cillating components, resulting in new frequency com-
ponents in the activations of the FFN. This means that the model can selectively enhance or
2) Positional Sensitivity: Since the weights W 1 are suppress connections between tokens based on their
combined with the trigonometric functions cos(θ kp) positional relationships. For example, in a sentence
and sin(θ kp), the linear transformation encodes posi- where a subject and verb are separated by several
tional sensitivity into the FFN’s activations. Tokens words, constructive interference can reinforce their
at different positions contribute differently to the ac- grammatical connection despite the distance. Con-
tivations depending on their phase alignment. versely,destructiveinterferencecanhelpthemodelig-
After the linear transformation, the output passes nore irrelevant or less important tokens, such as filler
throughanon-linearactivationfunction,suchasReLU words,byreducingtheirimpactontheattentionmech-
or GeLU: anism.
z (p)=σ(h (p)) (23) Mathematically, this interference can be analyzed
k kBeyond position: how rotary embeddings shape representations and memory in autoregressive
transfomers
using trigonometric identities. For example, when task. ii)Higher-LevelFeatureRepresentation,because
twophase-shiftedcomponentscos(θ p)andcos(θ p) the second linear layer combines frequency-modulated
k k+1
combine: features into a higher-level representation that cap-
tures both content-based and position-based informa-
cos(θ p)+cos(θ p)=
k k+1 tion,crucialforhandlingthesequentialnatureofdata
(cid:18) (cid:19) (cid:18) (cid:19)
θ +θ θ −θ in autoregressive Transformers.
2cos k k+1p cos k k+1p
2 2
(cid:16) (cid:17) 9 Analysis
The term cos θk+θk+1p represents a new cosine os-
2
cillation with a frequency equal to the average of the
(cid:16) (cid:17) Our study shows that the rotation matrices used in
two original frequencies, while cos θk−θk+1p modu- RoPE introduce specific phase components into the
2
lates the amplitude based on the frequency difference. token embeddings, leading to oscillatory behaviors
(cid:16) (cid:17)
Whenthedifferenceθ −θ issmall,cos θk−θk+1p characterized by distinct frequencies. When these
k k+1 2
frequency-modulated embeddings interact with the
iscloseto1,andthesignalsreinforceeachother,creat-
non-linear activation functions in the feed-forward
ingastrongercombinedwave. Ifthedifferenceislarge,
(cid:16) (cid:17) neural networks (FFNs), higher-order harmonics are
cos θk−θk+1p oscillates rapidly between −1 and 1 as
2 generated due to the nonlinearities introducing new
p changes, leading to alternating constructive and de- frequency components. This interaction results in
structive interference over different positions. patterns of constructive and destructive interference
In the FFN, this phenomenon allows certain posi- basedonthephasealignmentoftheembeddings. Con-
tional configurations to reinforce one another, leading structiveinterferenceamplifiesneuronactivations, en-
to stronger neuron activations where frequencies (or hancing the model’s attention to key positional pat-
phases) align. Conversely, misaligned frequencies lead terns, while destructive interference diminishes acti-
toweakeractivationsorsuppressionofcertainsignals. vations, potentially weakening the model’s ability to
Thisbehavior,causedbytheinterferenceofthephase- capture certain dependencies.
modulated signals, makes the FFN sensitive to posi- Drawing an analogy to signal processing, RoPE func-
tional information and helps it selectively amplify or tions similarly to frequency modulation (FM) by ad-
dampen features depending on token position. This justing the phase of embeddings according to posi-
capabilityisparticularlyimportantfortasksinvolving tional information, enriching the representation space
multi-scale dependencies, such as language modeling, andenablingthemodeltocapturepositionalnuances.
where capturing harmonics allows for better represen- The attention mechanism and FFNs act as nonlinear
tation of positional information at different granulari- filtersthatshapethefrequencycontentofthesemodu-
ties. lated signals in a context-sensitive manner, emphasiz-
ing or attenuating specific components based on the
8.1.1 Second Linear Layer: Projecting model’s learned parameters. The generation of har-
Higher-Level Features monics through these nonlinearities expands the sig-
nal’sfrequencyspectrum,allowingtheTransformerto
After the non-linear activation, the output is passed
represent more complex patterns and efficiently cap-
through the second linear transformation:
ture long-range dependencies. These dynamics en-
FFN(x)=W ·z(p)+b (26) hance the model’s ability to adjust its focus dynam-
2 2
ically, responding to context and varying sequential
The second linear layer W maps the non-linear ac-
2 dependencies inherent in the input data.
tivations back to the original embedding space or to
a space compatible with the subsequent layers, effec-
tivelyprojectingthehigher-levelfeaturesextractedby 10 Conclusion
the FFN. Since the input z(p) contains both the orig-
inal RoPE-induced frequencies and the higher-order In summary, our analysis shows that frequencies are
harmonics generated by the non-linearity, W acts as intertwined with the operation of Transformers uti-
2
a frequency-selective filter that determines which fea- lizing RoPE. The introduction of phase components
tures are propagated forward. and their interaction with non-linear activations gen-
Two major effects occur in the second linear layer: erate oscillatory behaviors and interference patterns
i) Frequency Selection, since W can emphasize cer- thatsignificantlyimpacthowinformationisprocessed
2
tain frequency components (for example, fundamen- across the model’s layers. Understanding these fre-
tal frequencies or specific harmonics) while attenuat- quency dynamics provides valuable perspective into
ing others. This allows the FFN to focus on tem- the internal workings of language models, unfolding
poral or positional dependencies most relevant to the on how they encode and manipulate sequential data.Valeria Ruscio, Fabrizio Silvestri
References
[Chi et al., 2020] Chi, L., Jiang, B., and Mu, Y.
(2020).Fastfourierconvolution.AdvancesinNeural
Information Processing Systems, 33:4479–4488.
[Dai et al., 2018] Dai, Z., Yang, Z., Yang, Y., Cohen,
W.W.,Carbonell,J.,Le,Q.V.,andSalakhutdinov,
R.(2018). Transformer-xl: Languagemodelingwith
longer-term dependency.
[Huang et al., 2018] Huang, C.-Z. A., Vaswani, A.,
Uszkoreit, J., Shazeer, N., Simon, I., Hawthorne,
C.,Dai,A.M.,Hoffman,M.D.,Dinculescu,M.,and
Eck, D. (2018). Music transformer. arXiv preprint
arXiv:1809.04281.
[Oppenheim, 1999] Oppenheim, A. V. (1999).
Discrete-time signal processing. Pearson Education
India.
[Selesnick and Burrus, 1998] Selesnick,I.W.andBur-
rus, C. S. (1998). Generalized digital butterworth
filter design. IEEE Transactions on signal process-
ing, 46(6):1688–1694.
[Shaw et al., 2018] Shaw, P., Uszkoreit, J., and
Vaswani, A. (2018). Self-attention with rel-
ative position representations. arXiv preprint
arXiv:1803.02155.
[Su et al., 2024] Su, J., Ahmed, M., Lu, Y., Pan, S.,
Bo, W., and Liu, Y. (2024). Roformer: Enhanced
transformer with rotary position embedding. Neu-
rocomputing, 568:127063.
[Takahashi et al., 2018] Takahashi, N., Agrawal, P.,
Goswami, N., and Mitsufuji, Y. (2018). Phasenet:
Discretized phase modeling with deep neural net-
works for audio source separation. In Interspeech,
pages 2713–2717.
[Touvron et al., 2023] Touvron, H., Lavril, T., Izac-
ard, G., Martinet, X., Lachaux, M.-A., Lacroix, T.,
Rozi`ere,B.,Goyal,N.,Hambro,E.,Azhar,F.,etal.
(2023). Llama: Open and efficient foundation lan-
guage models. arXiv preprint arXiv:2302.13971.
[Vaswani, 2017] Vaswani, A. (2017). Attention is all
you need. Advances in Neural Information Process-
ing Systems.
[Zhu, 2015] Zhu, Y. (2015). Aligning books and
movies: Towards story-like visual explanations by
watching movies and reading books. arXiv preprint
arXiv:1506.06724.Beyond position: how rotary embeddings shape representations and memory in autoregressive
transfomers
Appendix
11 ADDITIONAL EXPERIMENTS
This set of experiments focuses on studying the effects of phase alignment and misalignment on the activations
within the FFN of the models. We generated 250 synthetic sequences to isolate the impact of input structure on
phase interactions without altering the embedding rotations. These sequences consisted of repeated instances of
the same token to promote phase alignment, and sequences constructed by alternating between different tokens,
resulting in varying embeddings and promoting phase misalignment.
Consider a repeated token t, before applying RoPE, the embedding E(t) is identical at each position. After
applying RoPE, the embedding at position p becomes: E′(p) = R(p)E(t). The phase shift between positions p
and q is:
∆φ=θ(p−q) (27)
where θ is the rotational frequency parameter. The consistent phase increments lead to predictable interference
patterns when computing dot products in attention mechanisms or processing through FFNs. In sequences with
alternating tokens t and t , the embeddings before RoPE are E(t ) and E(t ). After applying RoPE:
1 2 1 2
E′(p)=R(p)E(t ) (28)
1 1
E′(p+1)=R(p+1)E(t ) (29)
2 2
Thephasedifferencebetweenembeddingsisinfluencednotonlybypositionaldifferencesbutalsobytheinherent
differencesinE(t )andE(t ). Thisresultsinmorecomplexinteractionsandpotentialcancellationeffectswithin
1 2
the model’s computations.
11.1 Llama 2
In Llama 2 there’s a noticeable difference between the mean activations of aligned and misaligned sequences.
The aligned sequences often have mean activations close to zero, while the misaligned sequences show varying
mean values, sometimes positive and sometimes negative.
As we can see in Table 1, the standard deviation (Std) of activations is consistently higher for misaligned
sequences compared to aligned sequences. This suggests that misaligned sequences produce more variable acti-
vations, possibly due to the complexity introduced by alternating tokens.
The KS statistics are significant across all layers (p-values effectively zero), indicating that the distributions
of activations for aligned and misaligned sequences are statistically different. in Table 2 we show that the t-
statistics show both positive and negative values, reflecting the direction of mean differences.
The t-statistics switch signs across layers, implying that in some layers, aligned sequences have higher mean
activations, whileinothers, misalignedsequencesdo, thisalternatingpatternsuggeststhatthemodel’sresponse
to alignment varies throughout the network.
For the Aligned Sequences the consistent phase shifts lead to constructive interference, resulting in more
stable and consistent activations (lower standard deviations).Valeria Ruscio, Fabrizio Silvestri
Figure 4: Activation distribution of Llama 2 at layer 22
Figure 5: Scatter plot of the PCA of the FFN activations in Llama 2. In the plot we can only see red dots
because they cover the blue ones representing the aligned sequences.
Misaligned sequences consistently exhibit higher standard deviations, indicating more dispersed activation
values. This suggests that the model finds misaligned sequences more challenging to process, leading to less
predictable activations
Despite significant differences in statistical measures, the high variability causes the distributions of activations
to overlap, making it difficult to distinguish between aligned and misaligned sequences using linear methods
like PCA as we show in Figure 2. Higher variance in aligned activations suggests that the model is actively
processing and transforming aligned sequences with a wider range of activation values, as shown in Table 3.
Misaligned sequences may result in less dynamic processing initially, possibly due to the model’s uncertainty
or inability to effectively integrate misaligned positional information. The increase in variance for misaligned
activations in deeper layers indicates that the model starts to generate more diverse activations when processing
misaligned sequences. This could be due to the accumulation of errors or interference patterns introduced by
misalignment, leading to unstable or erratic activations.
The higher Kurtosis in misaligned activations indicates that misaligned sequences produce activations with
more extreme values, possibly due to destructive interference or the model’s difficulty in processing misaligned
positional information. Certain layers exhibit dramatic differences in kurtosis between aligned and misaligned
activations, highlighting layers that are particularly sensitive to positional misalignment.
The higher entropy in misaligned activations suggests that the model’s activations are more random when
processing misaligned sequences, reflecting uncertainty or instability in the internal representations. And the
increase in entropy for misaligned activations in deeper layers (as we can see in Table 4) may indicate that the
model’s confusion accumulates as it processes misaligned positional information.
We also find an increased number of peaks in misaligned activations, and it may reflect the presence of inter-
ferencepatternscausingfluctuationsinactivationsacrosssequencepositionsandthatmisalignedsequencesleadBeyond position: how rotary embeddings shape representations and memory in autoregressive
transfomers
to less stable activation profiles.
Layer-SpecificBehaviors certainlayers(e.g.,Layer1,Layer4,Layer21)showparticularlylargedifferencesin
meansandstandarddeviations. Theselayersmaybemoresensitivetopositionalinformationandtheinterference
patterns caused by RoPE phase shifts.
The variation in t-statistics and standard deviations across layers indicates that the model integrates positional
informationdifferentlyateachlayer. Earlylayersmightcaptureimmediatepositionaleffects,whiledeeperlayers
might aggregate information over longer contexts.
Thedramaticincreaseinkurtosisandentropyinmisalignedactivationsindeeperlayerssuggeststhatthemodel’s
capacity to handle misalignment diminishes as it progresses through the network.
In conclusion thedatafromLlama2revealssignificantstatisticaldifferencesbetweenalignedandmisaligned
sequencesintermsofmeanactivations, standarddeviations, anddistributionshapesacrosslayers. However, the
highvariability,particularlyinmisalignedsequences, leadstooverlappingactivationdistributionsthatchallenge
linear separation methods like PCA. As the model processes misaligned sequences through deeper layers, vari-
ance, kurtosis, and entropy often increase, indicating accumulating interference effects and the model’s internal
representations become more erratic and less stable, reflecting the disruptive impact of positional misalignment.
These findings suggest that: i) interference patterns matter, in fact, constructive and destructive interference
due to RoPE phase shifts have a substantial impact on the model’s activations; ii) high within-class variability
can mask the differences between classes, necessitating alternative analytical approaches.
11.2 Llama 3
As we can see from Table 5, the tests performed on Llama 3 show that the mean activations for both aligned
and misaligned sequences are very close to zero across all layers. The differences between the means of aligned
and misaligned sequences are minimal, often in the order of 10−4 or 10−3. The standard deviations for both
aligned and misaligned sequences are very similar within each layer. There is a gradual increase in standard
deviation as we move to deeper layers, which is typical in deep neural networks due to the accumulation of
variance.
Figure 6: Activation distribution of Llama 3 at layer 22
The KS statistics are low across all layers, indicating that the distributions of activations for aligned and mis-
aligned sequences are very similar. P-values are effectively zero due to the large sample size, but the low KS
statistics suggest minimal differences in distributions, as we show in Table 6. The t-statistics show both pos-
itive and negative values, reflecting small differences in mean activations. However, the absolute values of the
t-statistics are generally lower compared to Llama 2, except for some layers (e.g., Layer 10 with a t-statistic
of -210.5745), indicating less pronounced mean differences. Despite smaller differences in statistical measures,
PCA plots showed clear separation between aligned and misaligned sequences, and Figure 4 is an example
of this phenomenon. This suggests that the variance relevant to sequence alignment is captured effectively in
the principal components. In fact, the activations seem to have tighter distributions with less overlap between
classes,andasmallmeandifferencescombinedwithlowvariabilityresultindistinctclustersinPCAspace. This
suggests that Llama 3 seems to have specialized neurons that respond differently to aligned and misaligned
sequences, capturing class differences effectively.Valeria Ruscio, Fabrizio Silvestri
Figure 7: Scatter plot showing the PCA of the FFN activations for aligned and misaligned sequences.
In table 7 we can see the variance for both aligned and misaligned activations generally increases, which
is expected as the model processes and transforms the input data. This pattern suggests that beyond the
initial layer, the model might be compensating for the misalignment, or the impact of misalignment on variance
diminishes as the data is processed deeper into the network.
Also the kurtosis values fluctuate, with some layers showing higher kurtosis for aligned activations and others
for misaligned. For instance, in Layer 2, misaligned activations have a higher kurtosis (45.979829) than aligned
activations (7.127898), suggesting that misalignment leads to more extreme activation values in certain layers.
Thesefluctuationsindicatethattheimpactofmisalignmentonthedistributionofactivationsvariesacrosslayers,
possibly due to the differing functions and sensitivities of each layer.
The higher variance and kurtosis in misaligned activations, especially in the initial layers, suggest that
misalignment introduces more variability and extreme activation values. This could be due to interference
patterns caused by misalignment, disrupting the model’s ability to process sequences coherently.
Entropy values for both aligned and misaligned activations are relatively similar, with minor differences. For
example, in Layer 6, the entropy for aligned activations is 2.189469, and for misaligned activations, it is slightly
higher at 2.260520, as shown in Table 8. These small differences suggest that misalignment may introduce
slightly more randomness in certain layers, but the overall effect on entropy is not substantial. The entropy
differences between aligned and misaligned activations are relatively minor in most layers, implying that the
overall randomness in activation distributions is not drastically affected by misalignment. However, the slightly
higher entropy in misaligned activations may reflect additional uncertainty introduced by misalignment.
The misaligned activations consistently exhibit a higher number of peaks than the aligned activations. For
instance, in Layer 2, misaligned activations have 112 peaks, whereas aligned activations have 41. This pattern
persists in many layers, indicating that misaligned sequences produce more complex activation patterns across
sequence positions. The consistently higher number of activation peaks in misaligned activations indicates more
erratic activation patterns across sequence positions. This suggests that misalignment leads to fluctuations in
the activations as the model struggles to integrate positional information correctly.
In some layers the differences between aligned and misaligned activations are less pronounced. For example,
in Layer 1, the variance and entropy are nearly identical for both, and the kurtosis values are extremely high for
bothalignedandmisalignedactivations. Thiscouldindicatethatthemodelissomewhatrobusttomisalignment
at this layer, or that both aligned and misaligned sequences result in similar activation patterns due to the
specific computations performed.
In conclusion these results show that Llama 3 effectively distinguishes between aligned and misaligned se-
quences despite smaller differences in mean activations and standard deviations. The clear separation observed
inPCAplotsindicatesthatLlama3capturesthevariancerelatedtosequencealignmentalongspecificprincipalBeyond position: how rotary embeddings shape representations and memory in autoregressive
transfomers
components. This suggests that the model’s internal representations of positional information are both discrim-
inative and efficiently organized, leading to tighter activation distributions with less overlap between the two
types of sequences.
The findings also suggest that positional misalignment introduces interference patterns that affect the model’s
processing,particularlyintermsofincreasedvariability,presenceofextremevalues,andmorecomplexactivation
patterns. These effects are more pronounced in some layers than others, indicating that certain layers are
more sensitive to positional misalignment. The increased variance and kurtosis in misaligned activations point
to potential interference patterns disrupting the model’s processing, which could impact performance on tasks
requiringprecisepositionalinformation. Thehighernumberofactivationpeaksinmisalignedsequencessuggests
that the model’s activations are more erratic when positional information is misaligned, potentially leading to
less stable representations. This could have implications for tasks where consistent internal representations are
crucial.
11.3 Llama 3.1
AsshowninFigure5andTable9,forLlama3.1themeanactivationsforbothalignedandmisalignedsequences
are close to zero across all layers. Differences between the means of aligned and misaligned sequences are small,
often in the order of 10−4 to 10−3.
The standard deviations are similar between aligned and misaligned sequences within each layer, but there is
a slight increase in standard deviations in deeper layers, which is expected due to the accumulation of variance
in deep networks. KS statistics are low across all layers, indicating minimal differences between the activation
Figure 8: Activations distribution of Llama 3.1 at layer 22
distributions of aligned and misaligned sequences and P-values are effectively zero due to large sample sizes,
but the low KS statistics suggest that the distributions are quite similar. The t-statistics vary in magnitude
and sign across layers: in fact, some layers show high absolute t-statistics (for example in layer 3), indicating
statistically significant mean differences, albeit small in absolute terms. The p-values are generally very low,
indicating statistical significance, but this is influenced by the sample sizes, as shown in Table 10.
As for Llama 3, even if the differences in statistical measures are on the smaller side, PCA plots showed
clear separation between aligned and misaligned sequences. This suggests that the variance relevant to sequence
alignmentiscapturedeffectivelyintheprincipalcomponents. InLlama3.1wefindaslightlylessclearseparation
though,likeinFigure6,wherewecanseethatpartofthedotsrepresentingtheactivationsforalignedsequences
are covered but the red dots. This still suggests that also Llama 3.1 seems to have specialized neurons
that respond differently to aligned and misaligned sequences, capturing class differences effectively. The small
differences in mean activations suggest that Llama 3.1 processes aligned and misaligned sequences similarly
in terms of average activation levels. However, the increased standard deviations indicate that misaligned
sequences may introduce more variability due to destructive interference from RoPE phase shifts. Changes in
model architecture, training data, or regularization techniques between Llama 3 and Llama 3.1 could contribute
to the observed differences.
In Table 11 we can see that as we progress through the layers, the variance for both aligned and misaligned
activations generally increases, which is expected as the model processes and transforms the input data, leading
to more complex representations. The variance is relatively similar between aligned and misaligned activations,Valeria Ruscio, Fabrizio Silvestri
Figure 9: In the PCA plot we can only see red dots because they cover the blue ones representing the eligned
sequences.
with minor fluctuations. In deeper layers (e.g., Layers 19-31), misaligned activations sometimes have slightly
higher variance, which may indicate that the effects of misalignment accumulate over the layers, leading to in-
creased variability in the activations.
In Layer 0, misaligned activations have a significantly higher kurtosis (375.936553) compared to aligned activa-
tions (38.748624). This substantial difference suggests that misaligned sequences produce activations with more
extreme values, possibly due to interference patterns caused by positional misalignment. Throughout the layers,
kurtosis values fluctuate, indicating that the impact of misalignment on the distribution of activations varies
across layers, possibly due to the differing functions and sensitivities of each layer. Layers that show higher
kurtosis in misaligned activations may be more sensitive to positional discrepancies.
AsshowninTable12,Entropyvaluesincreasethroughthelayersanditshowsminordifferencesbetweenaligned
and misaligned activations, this suggests that misalignment may introduce slight increases in randomness or un-
certainty in the activations at certain layers, but the overall effect on entropy is not substantial.
Throughout the layers, misaligned activations consistently exhibit a higher number of peaks than aligned
activations. This consistent pattern indicates that misaligned sequences produce more complex or erratic ac-
tivation patterns across sequence positions, potentially due to the model’s difficulty in integrating positional
information when misaligned.
Misalignedactivations oftenshowhighervarianceandkurtosis,especiallyintheinitiallayers. Thisindicates
that positional misalignment introduces more variability and results in activations with more extreme values.
Theseextremevaluescouldbeduetodestructiveinterferencepatternscausedbymisalignment,wherethemodel’s
expectations of positional information are disrupted, leading to irregular activation magnitudes.
The higher number of activation peaks in misaligned activations suggests that misalignment leads to more
fluctuations in activation patterns across the sequence positions. This could be due to the model receiving
conflicting positional cues, causing it to activate different neurons inconsistently across the sequence.
Layer-Specific Behaviors the model appears to rely heavily on positional information in the early layers
to build coherent internal representations. Misalignment disrupts this process, leading to increased variability
and erratic activation patterns. As the data progresses through the network, the model may compensate for
misalignment to some extent, but the initial disruptions can have lasting effects.
Some layers show higher kurtosis or variance in aligned activations, indicating that the model’s processing of
aligned sequences can also lead to extreme activation values, possibly due to amplification of certain features.
Inthedeeperlayers,thedifferencesinmetricsbetweenalignedandmisalignedactivationsbecomemorenuanced:
while variance remains similar, kurtosis and the number of peaks often remain higher in misaligned activations,
suggesting that the effects of misalignment persist but may be partially mitigated by the network’s depth.Beyond position: how rotary embeddings shape representations and memory in autoregressive
transfomers
Misalignment leads to increased variance and kurtosis, indicating that activations become more scattered
andextreme. Thiscandegradethemodel’sabilitytoextractmeaningfulfeaturesfromtheinputsequences. The
higher number of activation peaks in misaligned activations suggests that the model’s internal representations
are less stable, potentially affecting downstream tasks that rely on consistent feature extraction. So, identifying
layersthataremoresensitivetomisalignmentcanhelptargetinterventions. Forexample,adjustingthepositional
encoding mechanism or adding regularization techniques in these layers might mitigate the negative effects.
In conclusion the analysis of Llama 3.1’s activations reveals that while the model still distinguishes between
aligned and misaligned sequences, the separation is less pronounced than in Llama 3. Small differences in mean
activations and slightly higher standard deviations suggest that Llama 3.1 exhibits increased variability in its
activations, particularly for misaligned sequences. This increased within-class variance leads to more overlap in
activation distributions and reduces the effectiveness of PCA in separating the classes. These findings indicate
thatLlama3.1maybelessefficientatleveragingpositionalencodingtodifferentiatebetweensequencealignments
compared to Llama 3. The reduced separation in PCA plots suggests that the variance associated with class
differences is not as strongly captured in the principal components. It looks like Misaligned sequences often lead
to increased variance and kurtosis in activations, indicating more variability and extreme values. Also, there is
a consistent increase in the number of activation peaks for misaligned sequences across layers, suggesting more
erratic activation patterns.
11.4 TablesValeria Ruscio, Fabrizio Silvestri
Table 1: Means and Standard Deviations of Activations Llama 2
Layer Aligned Mean Aligned Std Misaligned Mean Misaligned Std
Layer 0 -0.0000 0.0310 0.0002 0.0302
Layer 1 0.0565 13.7587 0.0113 4.3314
Layer 2 -0.0005 0.0999 -0.0004 0.0785
Layer 3 0.0000 0.0106 -0.0001 0.0250
Layer 4 -0.0028 0.2404 -0.0010 0.1410
Layer 5 0.0003 0.0181 0.0004 0.0323
Layer 6 0.0002 0.0146 0.0002 0.0377
Layer 7 -0.0001 0.0152 -0.0001 0.0380
Layer 8 0.0002 0.0164 0.0003 0.0412
Layer 9 -0.0001 0.0178 -0.0001 0.0454
Layer 10 0.0000 0.0217 -0.0003 0.0520
Layer 11 0.0003 0.0247 0.0001 0.0542
Layer 12 0.0002 0.0181 0.0000 0.0583
Layer 13 -0.0002 0.0216 -0.0007 0.0668
Layer 14 0.0002 0.0199 -0.0003 0.0734
Layer 15 0.0000 0.0317 -0.0004 0.0898
Layer 16 0.0017 0.3057 0.0006 0.2275
Layer 17 -0.0005 0.0247 0.0001 0.0988
Layer 18 0.0002 0.0200 0.0006 0.0962
Layer 19 0.0029 0.2310 0.0021 0.1632
Layer 20 -0.0007 0.0258 -0.0006 0.0914
Layer 21 -0.0003 0.0251 -0.0014 0.0905
Layer 22 -0.0004 0.0215 -0.0002 0.0961
Layer 23 -0.0004 0.0288 0.0002 0.0851
Layer 24 -0.0003 0.0225 -0.0003 0.1058
Layer 25 -0.0010 0.0357 -0.0010 0.1160
Layer 26 -0.0007 0.0259 -0.0018 0.1260
Layer 27 -0.0012 0.0587 -0.0023 0.1634
Layer 28 -0.0006 0.0432 -0.0016 0.1678
Layer 29 0.0020 0.1829 -0.0015 0.2456
Layer 30 -0.0349 9.5605 -0.0107 2.2626
Layer 31 0.0058 0.7895 0.0208 2.7727Beyond position: how rotary embeddings shape representations and memory in autoregressive
transfomers
Table 2: Statistical Tests Results on Llama 2 FFN activations
Layer KS Statistic KS p-value t-Statistic t-test p-value
Layer 0 0.0378 0.0 -44.8944 0.0
Layer 1 0.2601 0.0 32.0735 1.0334×10−225
Layer 2 0.0603 0.0 -8.5045 1.8242×10−17
Layer 3 0.0793 0.0 49.8652 0.0
Layer 4 0.1608 0.0 -67.6719 0.0
Layer 5 0.0978 0.0 -19.9680 1.0451×10−88
Layer 6 0.1015 0.0 2.2374 2.5260×10−2
Layer 7 0.0991 0.0 4.1280 3.6595×10−5
Layer 8 0.0990 0.0 -13.0585 5.6839×10−39
Layer 9 0.0910 0.0 4.7302 2.2434×10−6
Layer 10 0.0952 0.0 67.2831 0.0
Layer 11 0.0843 0.0 21.1296 4.2522×10−99
Layer 12 0.1050 0.0 19.1474 1.0177×10−81
Layer 13 0.1129 0.0 81.6957 0.0
Layer 14 0.1188 0.0 59.6911 0.0
Layer 15 0.1069 0.0 49.1412 0.0
Layer 16 0.0633 0.0 28.7902 2.8467×10−182
Layer 17 0.1228 0.0 -62.8497 0.0
Layer 18 0.1410 0.0 -41.4314 0.0
Layer 19 0.0787 0.0 31.0820 4.2218×10−212
Layer 20 0.1169 0.0 -13.4339 3.8257×10−41
Layer 21 0.1255 0.0 113.2958 0.0
Layer 22 0.1472 0.0 -15.7370 8.4295×10−56
Layer 23 0.1346 0.0 -67.1515 0.0
Layer 24 0.1671 0.0 -5.2423 1.5862×10−7
Layer 25 0.1376 0.0 -2.9775 2.9063×10−3
Layer 26 0.1837 0.0 90.9769 0.0
Layer 27 0.1319 0.0 67.0804 0.0
Layer 28 0.1768 0.0 57.1779 0.0
Layer 29 0.0594 0.0 117.4058 0.0
Layer 30 0.0464 0.0 -25.2003 3.9785×10−140
Layer 31 0.1882 0.0 -53.1113 0.0Valeria Ruscio, Fabrizio Silvestri
Table 3: FFN Activations Variance and Kurtosis Values per Layer for Lama 2
Layer Aligned Variance Misaligned Variance Aligned Kurtosis Misaligned Kurtosis
Layer 0 0.000958 0.000909 147.376870 450.293038
Layer 1 189.301910 18.760889 2517.226552 9584.869177
Layer 2 0.009974 0.006163 942.513140 1207.720646
Layer 3 0.000112 0.000626 12.324369 301.364062
Layer 4 0.057784 0.019875 251.488492 537.800603
Layer 5 0.000329 0.001046 469.139445 32.133494
Layer 6 0.000214 0.001422 149.209058 15.636808
Layer 7 0.000230 0.001446 55.386710 14.914418
Layer 8 0.000268 0.001696 168.355079 41.643733
Layer 9 0.000317 0.002059 131.712825 11.949710
Layer 10 0.000472 0.002705 70.363190 36.411350
Layer 11 0.000608 0.002938 125.090690 37.491922
Layer 12 0.000328 0.003399 33.755511 11.014107
Layer 13 0.000465 0.004461 101.965093 36.300238
Layer 14 0.000394 0.005394 31.122054 9.695558
Layer 15 0.001007 0.008062 40.867651 62.675431
Layer 16 0.093446 0.051771 69.587139 83.290888
Layer 17 0.000612 0.009752 21.746094 97.290267
Layer 18 0.000401 0.009262 21.032534 63.357352
Layer 19 0.053355 0.026626 65.022751 69.923168
Layer 20 0.000663 0.008347 6.739614 50.176895
Layer 21 0.000629 0.008196 5.125684 11.852076
Layer 22 0.000462 0.009230 9.428607 9.702882
Layer 23 0.000831 0.007249 10.111835 9.616285
Layer 24 0.000505 0.011194 7.011010 8.283153
Layer 25 0.001272 0.013451 8.439607 351.566518
Layer 26 0.000673 0.015878 4.999129 13.778022
Layer 27 0.003445 0.026703 7.704944 8.603530
Layer 28 0.001863 0.028152 9.563569 10.354717
Layer 29 0.033438 0.060331 153.936535 8.230512
Layer 30 91.402138 5.119313 2536.507203 17761.817783
Layer 31 0.623273 7.688102 84.248899 1111.958487Beyond position: how rotary embeddings shape representations and memory in autoregressive
transfomers
Table 4: Activations Entropy and Number of Peaks per Layer for Llama 2
Layer Aligned Entropy Misaligned Entropy Aligned Peaks Misaligned Peaks
Layer 0 2.286771 2.148640 89 127
Layer 1 0.036389 0.010554 78 40
Layer 2 1.239760 1.093074 76 65
Layer 3 2.839637 3.397158 81 84
Layer 4 1.765312 1.179644 57 62
Layer 5 1.978643 2.603812 72 90
Layer 6 2.397626 3.013041 73 80
Layer 7 2.528681 3.194545 87 80
Layer 8 2.296124 2.945954 71 96
Layer 9 2.285897 2.958959 81 93
Layer 10 2.430262 3.093451 82 91
Layer 11 2.285858 2.907948 73 65
Layer 12 2.538281 3.317049 80 86
Layer 13 2.302424 3.115713 79 82
Layer 14 2.578832 3.449447 77 92
Layer 15 2.596895 3.356084 87 89
Layer 16 2.291857 2.060505 77 82
Layer 17 2.786680 3.609248 71 94
Layer 18 2.823713 3.707424 80 78
Layer 19 2.307882 2.066757 63 75
Layer 20 3.022282 3.754197 72 82
Layer 21 3.135118 3.825276 78 74
Layer 22 2.925587 3.800145 71 96
Layer 23 2.883445 3.724280 77 94
Layer 24 3.023099 3.945461 72 97
Layer 25 2.989809 3.815957 62 95
Layer 26 3.144766 4.070633 75 93
Layer 27 3.031415 3.812775 53 76
Layer 28 3.051819 4.006574 83 89
Layer 29 2.023154 2.333842 77 87
Layer 30 3.022282 3.754197 72 82
Layer 31 2.240728 3.235996 88 69Valeria Ruscio, Fabrizio Silvestri
Table 5: FFN Activations Variance and Kurtosis Values per Layer for Lama 3
Layer Aligned Mean Aligned Std Misaligned Mean Misaligned Std
Layer 0 0.0001 0.0145 0.0001 0.0226
Layer 1 -0.0002 0.4865 -0.0004 0.4864
Layer 2 -0.0005 0.0999 -0.0004 0.0785
Layer 3 0.0000 0.0106 -0.0001 0.0250
Layer 4 -0.0028 0.2404 -0.0010 0.1410
Layer 5 0.0003 0.0181 0.0004 0.0323
Layer 6 0.0002 0.0146 0.0002 0.0377
Layer 7 -0.0001 0.0152 -0.0001 0.0380
Layer 8 0.0002 0.0164 0.0003 0.0412
Layer 9 -0.0001 0.0178 -0.0001 0.0454
Layer 10 0.0000 0.0217 -0.0003 0.0520
Layer 11 0.0003 0.0247 0.0001 0.0542
Layer 12 0.0002 0.0181 0.0000 0.0583
Layer 13 -0.0002 0.0216 -0.0007 0.0668
Layer 14 0.0002 0.0199 -0.0003 0.0734
Layer 15 0.0000 0.0317 -0.0004 0.0898
Layer 16 0.0017 0.3057 0.0006 0.2275
Layer 17 -0.0005 0.0247 0.0001 0.0988
Layer 18 0.0002 0.0200 0.0006 0.0962
Layer 19 0.0029 0.2310 0.0021 0.1632
Layer 20 -0.0007 0.0258 -0.0006 0.0914
Layer 21 -0.0003 0.0251 -0.0014 0.0905
Layer 22 -0.0004 0.0215 -0.0002 0.0961
Layer 23 -0.0004 0.0288 0.0002 0.0851
Layer 24 -0.0003 0.0225 -0.0003 0.1058
Layer 25 -0.0010 0.0357 -0.0010 0.1160
Layer 26 -0.0007 0.0259 -0.0018 0.1260
Layer 27 -0.0012 0.0587 -0.0023 0.1634
Layer 28 -0.0006 0.0432 -0.0016 0.1678
Layer 29 0.0020 0.1829 -0.0015 0.2456
Layer 30 -0.0349 9.5605 -0.0107 2.2626
Layer 31 0.0058 0.7895 0.0208 2.7727Beyond position: how rotary embeddings shape representations and memory in autoregressive
transfomers
Table 6: Statistical Tests Results on Llama 3 FFN activations
Layer KS Statistic KS p-value t-Statistic t-test p-value
Layer 0 0.0378 0.0 -44.8944 0.0
Layer 1 0.2601 0.0 32.0735 1.0334×10−225
Layer 2 0.0603 0.0 -8.5045 1.8242×10−17
Layer 3 0.0793 0.0 49.8652 0.0
Layer 4 0.1608 0.0 -67.6719 0.0
Layer 5 0.0978 0.0 -19.9680 1.0451×10−88
Layer 6 0.1015 0.0 2.2374 2.5260×10−2
Layer 7 0.0991 0.0 4.1280 3.6595×10−5
Layer 8 0.0990 0.0 -13.0585 5.6839×10−39
Layer 9 0.0910 0.0 4.7302 2.2434×10−6
Layer 10 0.0952 0.0 67.2831 0.0
Layer 11 0.0843 0.0 21.1296 4.2522×10−99
Layer 12 0.1050 0.0 19.1474 1.0177×10−81
Layer 13 0.1129 0.0 81.6957 0.0
Layer 14 0.1188 0.0 59.6911 0.0
Layer 15 0.1069 0.0 49.1412 0.0
Layer 16 0.0633 0.0 28.7902 2.8467×10−182
Layer 17 0.1228 0.0 -62.8497 0.0
Layer 18 0.1410 0.0 -41.4314 0.0
Layer 19 0.0787 0.0 31.0820 4.2218×10−212
Layer 20 0.1169 0.0 -13.4339 3.8257×10−41
Layer 21 0.1255 0.0 113.2958 0.0
Layer 22 0.1472 0.0 -15.7370 8.4295×10−56
Layer 23 0.1346 0.0 -67.1515 0.0
Layer 24 0.1671 0.0 -5.2423 1.5862×10−7
Layer 25 0.1376 0.0 -2.9775 2.9063×10−3
Layer 26 0.1837 0.0 90.9769 0.0
Layer 27 0.1319 0.0 67.0804 0.0
Layer 28 0.1768 0.0 57.1779 0.0
Layer 29 0.0594 0.0 117.4058 0.0
Layer 30 0.0464 0.0 -25.2003 3.9785×10−140
Layer 31 0.1882 0.0 -53.1113 0.0Valeria Ruscio, Fabrizio Silvestri
Table 7: FFN Activations Variance and Kurtosis Values per Layer for Lama 3
Layer Aligned Variance Misaligned Variance Aligned Kurtosis Misaligned Kurtosis
Layer 0 0.000212 0.000510 32.757828 364.158944
Layer 1 0.236660 0.236592 344029.010913 344226.518989
Layer 2 0.000295 0.000353 7.127898 45.979829
Layer 3 0.000518 0.000487 59.649299 4.726789
Layer 4 0.000692 0.000715 0.795759 0.362406
Layer 5 0.000950 0.001229 0.477034 0.852841
Layer 6 0.001250 0.001461 24.752695 29.889093
Layer 7 0.001621 0.001776 6.151176 18.224953
Layer 8 0.001597 0.001505 2.469859 3.624184
Layer 9 0.002246 0.001545 93.514968 38.368989
Layer 10 0.001963 0.001711 4.624561 4.741636
Layer 11 0.002067 0.001839 0.619448 8.063537
Layer 12 0.002094 0.001896 9.832133 7.831309
Layer 13 0.002460 0.001997 0.611578 0.918006
Layer 14 0.002863 0.002267 15.886125 1.421881
Layer 15 0.002851 0.002462 2.923889 4.253402
Layer 16 0.002688 0.002407 16.426754 10.997486
Layer 17 0.002636 0.002389 12.949386 40.475359
Layer 18 0.001896 0.001674 6.559782 12.719294
Layer 19 0.001825 0.001916 64.318771 64.902025
Layer 20 0.002072 0.001920 60.462527 39.985912
Layer 21 0.002854 0.002619 37.429808 90.184434
Layer 22 0.002032 0.001965 61.272983 98.871141
Layer 23 0.001838 0.001695 3.972599 9.586147
Layer 24 0.002926 0.002369 4.804575 3.953097
Layer 25 0.002487 0.002513 25.634998 28.264629
Layer 26 0.005470 0.005264 58.637025 36.473062
Layer 27 0.008895 0.008619 0.528337 2.938631
Layer 28 0.010616 0.011235 2.364869 3.982332
Layer 29 0.022143 0.022959 92.879826 93.424681
Layer 30 0.079033 0.084350 81.294238 83.782203
Layer 31 1.135274 1.216739 14399.446705 12585.702823Beyond position: how rotary embeddings shape representations and memory in autoregressive
transfomers
Table 8: Activations Entropy and Number of Peaks per Layer for Llama 3
Layer Aligned Entropy Misaligned Entropy Aligned Peaks Misaligned Peaks
Layer 0 1.390817 1.430737 41 127
Layer 1 0.000465 0.000495 49 108
Layer 2 2.313199 2.350038 41 112
Layer 3 2.148607 2.174024 40 125
Layer 4 2.772901 2.794252 43 80
Layer 5 2.632099 2.757681 42 122
Layer 6 2.189469 2.260520 42 112
Layer 7 2.401442 2.433842 42 123
Layer 8 2.352235 2.321993 43 92
Layer 9 1.741799 1.592606 43 107
Layer 10 2.172766 2.104282 43 90
Layer 11 2.958910 2.886623 42 65
Layer 12 2.359208 2.314337 40 77
Layer 13 2.683580 2.580417 41 127
Layer 14 2.200285 2.099395 39 90
Layer 15 2.474309 2.402266 42 85
Layer 16 2.297885 2.244245 37 122
Layer 17 2.198479 2.126024 40 85
Layer 18 1.678259 1.613673 38 116
Layer 19 2.149443 2.159631 40 126
Layer 20 2.183639 2.158820 33 126
Layer 21 2.071865 1.989183 39 118
Layer 22 2.120278 2.078059 43 126
Layer 23 2.316166 2.266680 33 127
Layer 24 2.709468 2.606097 41 127
Layer 25 2.244663 2.255491 40 113
Layer 26 2.247325 2.247983 40 122
Layer 27 3.000723 2.976391 31 104
Layer 28 2.694904 2.718854 32 126
Layer 29 2.196106 2.211723 41 106
Layer 30 2.064699 2.085282 28 103
Layer 31 0.059853 0.064633 40 125Valeria Ruscio, Fabrizio Silvestri
Table 9: Means and Standard Deviations of Activations on Llama 3.1
Layer Aligned Mean Aligned Std Misaligned Mean Misaligned Std
Layer 0 0.0002 0.0201 0.0002 0.0285
Layer 1 -0.0003 0.6456 -0.0007 0.6529
Layer 2 0.0001 0.0213 -0.0000 0.0212
Layer 3 -0.0007 0.0280 -0.0000 0.0283
Layer 4 -0.0001 0.0320 -0.0002 0.0342
Layer 5 0.0003 0.0411 0.0001 0.0420
Layer 6 -0.0006 0.0474 0.0002 0.0447
Layer 7 -0.0006 0.0542 -0.0003 0.0517
Layer 8 -0.0009 0.0509 -0.0011 0.0491
Layer 9 -0.0003 0.0510 0.0001 0.0495
Layer 10 -0.0010 0.0567 0.0002 0.0531
Layer 11 -0.0006 0.0570 -0.0004 0.0556
Layer 12 0.0009 0.0601 0.0002 0.0565
Layer 13 -0.0015 0.0618 -0.0010 0.0574
Layer 14 -0.0000 0.0641 -0.0004 0.0626
Layer 15 -0.0002 0.0692 -0.0001 0.0644
Layer 16 -0.0009 0.0635 -0.0009 0.0623
Layer 17 -0.0001 0.0586 0.0004 0.0624
Layer 18 0.0005 0.0516 0.0002 0.0542
Layer 19 0.0001 0.0534 0.0005 0.0552
Layer 20 -0.0002 0.0545 -0.0002 0.0595
Layer 21 0.0004 0.0649 0.0002 0.0693
Layer 22 -0.0003 0.0534 -0.0003 0.0588
Layer 23 -0.0005 0.0519 -0.0004 0.0568
Layer 24 -0.0004 0.0643 0.0000 0.0662
Layer 25 -0.0022 0.0614 -0.0023 0.0668
Layer 26 0.0013 0.0946 0.0002 0.0993
Layer 27 -0.0020 0.1230 -0.0019 0.1287
Layer 28 -0.0010 0.1285 -0.0005 0.1408
Layer 29 0.0016 0.1895 0.0021 0.2162
Layer 30 0.0101 0.3671 0.0126 0.4100
Layer 31 0.0278 1.2928 0.0332 1.4824Beyond position: how rotary embeddings shape representations and memory in autoregressive
transfomers
Table 10: Statistical Tests Results on Llama 3.1 FFN activations
Layer KS Statistic KS p-value t-Statistic t p-value
Layer 0 0.0077 0 7.0726 1.5203×10−12
Layer 1 0.0338 0 5.0025 5.6581×10−07
Layer 2 0.0075 0 22.8735 8.5424×10−116
Layer 3 0.0138 0 -167.8477 0
Layer 4 0.0164 0 24.3150 1.3607×10−130
Layer 5 0.0039 0 35.4805 9.8455×10−276
Layer 6 0.0202 0 -119.8975 0
Layer 7 0.0166 0 -44.7754 0
Layer 8 0.0111 0 25.2201 2.4134×10−140
Layer 9 0.0092 0 -62.0720 0
Layer 10 0.0204 0 -152.6716 0
Layer 11 0.0130 0 -24.3901 2.1775×10−131
Layer 12 0.0211 0 94.5910 0
Layer 13 0.0227 0 -63.4716 0
Layer 14 0.0119 0 39.8030 0
Layer 15 0.0176 0 -15.0153 5.8282×10−51
Layer 16 0.0075 0 5.3294 9.8537×10−08
Layer 17 0.0083 0 -50.0778 0
Layer 18 0.0102 0 47.7368 0
Layer 19 0.0125 0 -48.6066 0
Layer 20 0.0179 0 -0.4807 6.3075×10−01
Layer 21 0.0146 0 21.3121 8.7696×10−101
Layer 22 0.0235 0 -2.5624 1.0396×10−02
Layer 23 0.0186 0 -6.4478 1.1345×10−10
Layer 24 0.0088 0 -44.2329 0
Layer 25 0.0199 0 16.1789 7.1097×10−59
Layer 26 0.0101 0 78.6355 0
Layer 27 0.0053 0 -3.0907 1.9969×10−03
Layer 28 0.0156 0 -29.7396 2.3682×10−194
Layer 29 0.0052 0 -20.5584 6.4724×10−94
Layer 30 0.0162 0 -47.5514 0
Layer 31 0.0173 0 -27.9946 1.8894×10−172Valeria Ruscio, Fabrizio Silvestri
Table 11: FFN Activations Variance and Kurtosis Values per Layer for Lama 3.1
Layer Aligned Variance Misaligned Variance Aligned Kurtosis Misaligned Kurtosis
Layer 0 0.000404 0.000814 38.748624 375.936553
Layer 1 0.416794 0.426272 341318.653228 326343.909150
Layer 2 0.000453 0.000449 2.488627 12.863954
Layer 3 0.000787 0.000799 25.004033 1.678751
Layer 4 0.001022 0.001166 1.416729 0.632352
Layer 5 0.001692 0.001765 0.312195 1.244076
Layer 6 0.002248 0.001996 13.021483 11.426331
Layer 7 0.002943 0.002671 7.567117 11.564255
Layer 8 0.002586 0.002412 2.057977 1.248498
Layer 9 0.002596 0.002450 26.146935 15.778057
Layer 10 0.003216 0.002822 29.333605 12.334599
Layer 11 0.003248 0.003093 2.299698 6.174563
Layer 12 0.003606 0.003190 2.867246 9.256613
Layer 13 0.003824 0.003291 0.942112 1.653039
Layer 14 0.004103 0.003913 0.351336 2.828304
Layer 15 0.004794 0.004151 5.085419 5.363530
Layer 16 0.004036 0.003882 5.812460 8.859311
Layer 17 0.003435 0.003888 10.594112 32.042719
Layer 18 0.002664 0.002934 13.339556 16.267094
Layer 19 0.002851 0.003044 24.409184 31.016534
Layer 20 0.002967 0.003543 23.392221 26.467337
Layer 21 0.004215 0.004808 47.326836 69.573910
Layer 22 0.002855 0.003463 102.317790 102.110836
Layer 23 0.002698 0.003228 5.542072 37.362970
Layer 24 0.004130 0.004378 5.052662 4.294181
Layer 25 0.003768 0.004465 16.485577 51.648008
Layer 26 0.008952 0.009870 63.531770 66.936732
Layer 27 0.015138 0.016553 0.500493 10.788369
Layer 28 0.016514 0.019820 2.323452 14.297032
Layer 29 0.035926 0.046728 88.718032 234.520944
Layer 30 0.134786 0.168077 78.316231 157.494293
Layer 31 1.671414 2.197644 19658.055751 11466.254428Beyond position: how rotary embeddings shape representations and memory in autoregressive
transfomers
Table 12: Activations Entropy and Number of Peaks per Layer for Llama 3.1
Layer Aligned Entropy Misaligned Entropy Aligned Peaks Misaligned Peaks
Layer 0 1.422649 1.411817 36 127
Layer 1 0.000538 0.000923 41 96
Layer 2 2.478942 2.457450 41 71
Layer 3 2.324757 2.367522 40 111
Layer 4 2.771986 2.841603 41 106
Layer 5 2.763314 2.778450 45 102
Layer 6 2.380713 2.326703 33 97
Layer 7 2.513412 2.460572 40 115
Layer 8 2.313925 2.281893 36 77
Layer 9 1.822700 1.806551 42 99
Layer 10 1.956781 1.906546 41 99
Layer 11 2.812666 2.782584 39 96
Layer 12 2.452796 2.384898 41 80
Layer 13 2.487775 2.412252 34 120
Layer 14 2.716854 2.689150 34 122
Layer 15 2.424438 2.359523 43 86
Layer 16 2.352053 2.331200 39 122
Layer 17 2.015743 2.054148 38 111
Layer 18 1.514254 1.553786 40 89
Layer 19 2.108952 2.147636 39 117
Layer 20 2.479717 2.564007 34 126
Layer 21 1.880695 1.945309 37 107
Layer 22 1.953244 2.045247 34 115
Layer 23 2.146672 2.222197 35 122
Layer 24 2.639829 2.672478 40 114
Layer 25 2.309509 2.395691 39 111
Layer 26 2.166277 2.218198 31 118
Layer 27 3.047661 3.071268 25 100
Layer 28 2.695442 2.768950 33 122
Layer 29 2.344540 2.395680 29 85
Layer 30 2.105706 2.178419 31 117
Layer 31 0.064270 0.086658 28 119