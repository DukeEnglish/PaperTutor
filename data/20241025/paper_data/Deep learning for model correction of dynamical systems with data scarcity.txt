DEEP LEARNING FOR MODEL CORRECTION OF DYNAMICAL
SYSTEMS WITH DATA SCARCITY
CAROLINE TATSUOKA AND DONGBIN XIU∗∗
Abstract. We present a deep learning framework for correcting existing dynamical system
models utilizing only a scarce high-fidelity data set. In many practical situations, one has a low-
fidelity model that can capture the dynamics reasonably well but lacks high resolution, due to the
inherentlimitationofthemodelandthecomplexityoftheunderlyingphysics. Whenhighresolution
databecomeavailable,itisnaturaltoseekmodelcorrectiontoimprovetheresolutionofthemodel
predictions. Wefocusonthecasewhentheamountofhigh-fidelitydataissosmallthatmostofthe
existingdatadrivenmodelingmethodscannotbeapplied. Inthispaper,weaddressthesechallenges
with a model-correction method which only requires a scarce high-fidelity data set. Our method
firstseeks adeep neural network (DNN) model to approximate the existing low-fidelity model. By
using the scarce high-fidelity data, the method then corrects the DNN model via transfer learning
(TL).AfterTL,animprovedDNNmodelwithhighpredictionaccuracytotheunderlyingdynamics
isobtained. Onedistinctfeatureoftheproposemethodisthatitdoesnotassumeaspecificformof
themodelcorrectionterms. Instead,itoffersaninherentcorrectiontothelow-fidelitymodelviaTL.
Asetofnumericalexamplesarepresentedtodemonstratetheeffectivenessoftheproposedmethod.
Key words. Datadrivenmodeling,deepneuralnetworks,modelcorrection,multifidelitymod-
eling,transferlearning.
1. Introduction. Within the field of Uncertainty Quantification (UQ), prac-
titioners wish to account for potential sources of uncertainty in their mathematical
model and, when possible, limit this uncertainty for improved model-based predic-
tions. Model-form uncertainty has especially poseda challenge for modelers to quan-
tify, as to do so directly would require full knowledge of the true underlying process,
whichinmanycasesisunknown[12]. Incompleteknowledgeoftheunderlyingphysics
leadstoaninherentdiscrepancybetweenthemathematicalmodelandthetrueunder-
lying process, thus introducing error to the model’s outputs. Driven by the need for
high-fidelity and low-cost models, developing model correcting techniques has been
a prominent area of research within the field of UQ over the past several decades.
One approach, proposed in [21], attempts to correct this discrepancy by introducing
an external additive correction term, which is modeled by Gaussian Process (GP).
The hyper-parameters of the GP can be recovered via a maximum likelihood esti-
mate [6] or via Bayesian inference methods [21, 23, 13, 30, 15, 20]. This approach
has undergone several extensions and has been successful in some applications, see
[3, 2, 16, 17, 24, 29] and resources therein. In [14], both multiplicative and additive
correction terms are introduced internally (meaning embedded within the physical
processes of the model), and/or externally; these terms are then parameterized via a
constrained optimization problem. The work of [9] proposes a method in which the
correctionterms for a low-fidelity model are parameterizedby a stochastic expansion
(polynomial chaos or stochastic collocation), and an adaptive, sparse grid algorithm
is usedfor obtaining a multi-fidelity model. In [19]and laterextended in[18], model-
error is internally embedded into the model via the model parameters, which are
treatedasrandomvariablesrepresentedbypolynomialchaosexpansions. ABayesian
inference procedure is then suggested for parameterizing the expansion. Recently, in
the context of complex dynamical systems, the work of [31] reviews ways to address
∗Department of Mathematics, The Ohio State University, Columbus, OH 43210, USA. Emails:
{tatsuoka.3, xiu.16}@osu.edu, Funding: This work was partially supported by AFOSR FA9550-22-
1-0011.
1
4202
tcO
32
]GL.sc[
1v31971.0142:viXrastructuralerrorthroughbothinternalandexternalmethods. Onecommonfeatureof
theseapproachesisthattheformofthemodelcorrection,additiveand/ormultiplica-
tive, needs to be chosen first. This limits the applicability of the methods because
for many problems the model errors manifest themselves in much more complicated
manner.
Deep learning has become a powerful tool in the approximation of unknown dy-
namical systems, and more recently has been used in the contextof model correction
inordertoobtainimprovedsystemapproximations. In[4],theflow-mapofanimper-
fectpriormodeliscorrectedbyanadditivecorrectiontermparameterizedbyaneural
network. This deep neural network (DNN), termed the generalized residual neural
network, is trained to learn the discrepancy between the flow-maps of the imperfect
model and the true model. Recently, for physics-informed neural networks (PINNs),
an additional deep neural network operator was proposed to learn the discrepancy
between the true governing dynamics and the misspecified physics encoded in the
loss function [35]. See [8, 34, 7] for other recent works in which DNNs are used for
approximating model discrepancies.
Whilethegeneralizedresidualnetworkpresentedin[4]demonstratesanabilityto
correctimperfectdynamicalmodel,themethodrequireslargeamountsofhigh-fidelity
data for training the DNNs. DNNs are data hungry tools, and the performance and
accuracyoftheirpredictionstypicallyimprovesasthesizeofthehigh-fidelitytraining
datasetincreases. However,for manycomplexsystems,suchlarge-scalequantitiesof
data may not be available, either due to the limited ability for mass data collection
(commoninmanypracticalphysicalorbiologicalsettings),orthehighcomputational
cost for generating such data [1]. This introduces the challenge of data scarcity in
deeplearning,astheDNNcannotbesufficientlytrainedgiventhelimitedhigh-fidelity
dataset. The resulting DNN model thus cannot provide accurate predictions for the
underlying dynamics.
For the purposes of model correction, we propose addressing this challenge of
data scarcity with a transfer learning (TL) approach. Transfer learning is a popular
technique usedindeeplearningfor handlingdata scarcity;transferlearningutilizes a
deepneuralnetworkmodeltrainedonalarge,low-fidelitytrainingdatasettoleverage
the training of the model on a scarce, high-fidelity dataset, effectively ‘transferring’
relevantmodelfeatures. Thistoolhaspreviouslybeenusedinthedeeplearningcom-
munity to handle data scarcity in multi-class classification problems such as image
or text classification [11, 27]. There are currently several available works which uti-
lizes transfer learning to learn PDEs, see for example [33, 28]. In this work, we are
concernedwith approximating unknowndynamicalsystems givenscarcehigh-fidelity
data from the true underlying dynamics. We utilize TL to correct a DNN approxi-
mationto the imperfect, “prior”model’sflow-maptoobtainanimproved“posterior”
model of the true underlying flow-map operator1.
Further, in many realistic settings, high-fidelity data observations may be col-
lected on a coarse time-grid, and also may be collected over non-uniform time inter-
vals. This poses an obstacle when the dynamics of interest lie on a finer time-scale.
In[5], amethod termedinner recurrence is introducedto addressthis issue, inwhich
the network approximationof the system’s flow map is enforced on a finer time scale
than the coarsedata observations. Inthis setting, largeamounts ofhigh-fidelity data
1While “prior” and “posterior” are terminology borrowed from Bayesian statistics, here we
strictly use prior to refer to the existing imperfect model, and posterior to refer to the corrected
model.
2are assumed to be available to train the network, and the coarse data observations
are assumed to lie on a fixed, coarse time-grid. We are able to extend this method,
requiring only scarce high-fidelity data which may be collected over varying coarse
time intervals in order to correct a prior model.
The remainder of the paper is organized as follows: in section 2 we present the
problem statement, setup and the proposed method for model correction. In section
3, we review preliminaries, specifically recalling the flow-map learning methodology
proposed in [25] as well as the generalized residual neural network and the inner
recurrence set ups proposed in [4] and [5] respectively. Section 4 presents numerous
numericalexamples to demonstrate the effectiveness in of the method, andin section
5 we provide concluding remarks.
2. Preliminaries. We first present the problem setup and the related work.
2.1. Problem Setup. Let us consider modeling an unknown autonomous dy-
namical system,
dx
=f(x(t)), (2.1)
dt
where x∈Rn is the state variables and f :Rn →Rn defines the governing equations
of the system. In this setting, we assume f is unknown. Thus, the system cannot be
solved directly for its analysis and prediction.
We assume that we have in our possession a low-fidelity model for the system,
dx
=f(x(t)), (2.2)
dt
e
wheref :Rn →Rn isanapproximationtoe theetrueandunknowngoverningequations,
i.e., f ≈f. This low-fidelity model may be imperfect due to simplified physics, linear
approxeimationsto nonlinear dynamics,incorrectparameterizations,etc. Throughout
thisepaper, this low-fidelity model shall be referred to as the “prior model”.
We also assume a small amount of high-fidelity data for the true system are
available. Thesehigh-fidelitydataarescarcesuchthattheydonotallowustodirectly
construct a high-fidelity model of the unknown system. Our goal is to leverage the
scarce high fidelity data to correct the low-fidelity prior model. The corrected model
shall offer much higher predictive accuracy than the prior model. Hereafter, we shall
refer to the corrected model as the “posterior model”.
2.2. RelatedWork. Ourworkisrelatedtodatadrivenlearningofunknowndy-
namicalsystems. Specifically,weutilizetheframeworkofflowmaplearning(FML),as
itisaflexibleandrigorousmodelingapproach. AlthoughFMLisageneralmethodol-
ogy,weshallfocusonitsformulationinconjunctionwithdeepneuralnetwork(DNN).
2.2.1. Flow Map Learning. For the system (2.1), its flow-map is an operator
Φ:Rn×R7→Rn suchthatΦ(x(t),s)=x(t+s). Theflow-mapgovernstheevolution
of the system over time. Suppose one has observation data on x over discrete time
instances separated over a constant time lag ∆ > 0. Then the state variables over
two consecutive time instances satisfies x = Φ (x ), ∀n. In FML, the goal is
n+1 ∆ n
to conduct a numerical approximation to Φ. That is, find N : Rn 7→ Rn such that
N≈Φ . This is accomplished by minimizing the mean squared loss
∆
minkx −N(x )k2.
n+1 n
3WhenDNNisusedtorepresentN,theproblembecomesminimizationoveritshyper-
parameters. From the computational point of view, it is often advantagous to utilize
the ResNet structure. That is,
N:=I+N(·;Θ), (2.3)
where I denotes the identity operator and N is a deep neural network operator with
itshyperparametersΘ(weightsandbiasesofeachlayer). OncetheDNN operatoris
successfully trained, the hyper parameters are fixed, and we obtain a learned model
that can be used for the system prediction,
x =N(x )=x +N(x ). (2.4)
n+1 n n n
2.2.2. Generalized Residual Network. The FML method using the ResNet
structure (2.3) was generalized in [4] as a model correction method. Assuming there
exists a low fidelity prior model (2.2), its flow map operator Φ : Rn 7→ Rn is then
∆
available. Theworkof[4]thenproposedtofindtheapproximateflowmaptothetrue
system (2.1) as e
N:=Φ +N(·;Θ). (2.5)
∆
Compared to the ResNet structure (2e.3), it is obvious that the identify operator is
replaced by the flow map operator of the prior model (2.2). The DNN operator N
then serves as an additive correctionterm to the prior model.
2.3. Our Contribution. The contribution of this paper is in the introduction
of a flexible DNN-based model correction method. The new method contains two
novel features: (1) it does not assume a fixed form, either additive or multiplicative,
ofthemodelcorrection. Instead,thepriormodelismodified“internally”bythehigh-
fidelity data to capture the unknown dynamics. This is different from most existing
model correction methods; and (2) it requires only a small set of high-fidelity data
to learn the corrected posterior model. This resolves the compuational challenges of
most DNN based learning methods that require a large number of high-fidelity data.
3. Transfer Learning for Model Correction. Inthis section,wepresentthe
technical detail of our main method. The method is based on constructing a DNN
modelforthelow-fidelitypriormodel(2.2),followedbyatransferlearningre-training
of the DNN model using the scarce high-fidelity data. We also discuss how to cope
withthesituationwhenthehigh-fidelitydataarenotonlyscarceinquantitybutalso
coarse in temporal resolution.
Throughout this paper, our discussion shall be over discrete time instances 0 ≤
t
0
<t
1
<···<t
k
<··· over a constant time-lag ∆≡t k−t k−1, ∀k.
3.1. Scarce High-fidelity Data. Fortheunknowndynamicalsystem(2.1),we
assume there are a set of trajectory data of the state variables in the following form,
x(t(i)) , k =0,1,...,K(i), i=1,...,N, (3.1)
k
n o
where N is the total number of the trajectory sequences, and K(i) the time length of
the i-th time sequence. We assume that these data are of high fidelity, i.e., they are
highly accurate measurements of the true values of the state variables x at the time
instances.
4Due to our assumption that system (2.1) is autonomous, we may omit the time
indexasonlytherelativetimedifferencebetweenthedataobservationsneedbenoted.
We also reorganize the data observations into the data pairs over consecutive time
instances,
SHF , x(j) ,x(j) , j =1,...,J , (3.2)
1 2 HF
n o
where J = K(1) +···+K(N) is the total number of such data pairs that can be
HF
extracted from the data set (3.1). This is our high-fidelity training data set, which
in principle shall enable one to learn the unknown dynamics of (2.1) via the FML
framework in Section 2.2.1. However, we assume in this paper that this high-fidelity
datasetisscarce,inthesensethatJ isnotsufficientlylargetoallowustoconstruct
HF
an accurate model for (2.1).
3.2. ConstructionofDNNPriorModel. Inordertoconstructahigh-fidelity
dynamical model for the unknown system (2.1), we need to utilize the low-fidelity
model (2.2) to supplement the scarce high-fidelity training data (3.2). The first step
is to construct a FML model (2.4) for the low-fidelity prior model (2.2). Moreover,
the FML model needs to be in the form of DNN.
ToconstructtheDNNpriormodel,werepeatedlyexecutethepriormodel(2.2)to
generatealargenumber ofdatapairsseparatedbyone time lag∆. Morespecifically,
let Ω ⊂ Rn be the domain of interest in the phase space where we are interested in
modeling the unknown system (2.1). Let J ≫ 1 be the number of data pairs we
LF
seek to generate. Then, for each j =1,...,J ,
LF
• Sample
x(j)
∈Ω. In most cases, random sampling with uniform distribution
1
in Ω is sufficient;
• Solve thee prior model (2.2) with an initial condition
x(j)
over one time step
1
∆ to obtain x(j) =Φ x(j) .
2 ∆ 1
Upon conducting these simulati(cid:16)ons, w(cid:17)e obtain a low-fidelity treaining data set,
e e e
SLF , x(j),x(j) , j =1,...,J . (3.3)
1 2 LF
n o
This procedure should not present a significant computational challenge, assuming
e e
the low-fidelity model (2.2) can be solved fast and inexpensively.
We then construct a FML model based on this low-fidelity data set using DNN,
intheformof (2.3). LetN(·;Θ):Rn →Rn be thecorrespondingDNNoperatorwith
its hyper parameters Θ. The learning of N is accomplished by minimizing the mean
squared loss over the low-efidelity data set (3.3), i.e.,
e
JLF
2
Θ∗
=argmin
x(j)−N x(j);Θ
. (3.4)
2 1
Θ
Xj=1(cid:13) (cid:16) (cid:17)(cid:13)
Once the optimization is finished, the(cid:13) (cid:13)heyper pe arameeters Θ(cid:13) (cid:13)∗ are fixed and shall be
suppressed in our exposition, unless confusion arises otherwise.
To facilitate the discussion, it is necessary to examine the detail of N. Both the
input layer and output layer have n nodes, corresponding to the dimension of the
state variable x. Let us assume we have M ≥1 hidden layers,eachof wheich contains
d≥1nodes. TheDNN operatorcanbe writtenasthe followingcompositionofaffine
and nonlinear transformations,
N=W
M
◦(σ
M
◦W M−1)◦···◦(σ 1◦W 0), (3.5)
5
ewhere W is the weight matrix containing the weights connecting the ith and (i+
i
1)th layer and the biases in the (i+1)th layer, σ is the (nonlinear) component-wise
i
activationfunctionoftheith layer,andthe 0th layeristhe inputlayer. Atthe output
layer, the (M +1)th layer, the linear activation function (σ(x) = x) is used. This
results in an identity operator, which is suppressed.
Let us define a shorthanded notation
W =[W ,...,W ], 0≤m≤M. (3.6)
[0:m] 0 m
The hyper parameters Θ then refer to the collection of all the weight matrices, i.e.,
Θ= W . The parameter optimization problem (3.4) can be written equivalently
[0:M]
as
JLF
2
W∗
=argmin
x(j)−N(x(j);W
) . (3.7)
[0:M] 2 1 [0:M]
W
[0:M] Xj=1(cid:13) (cid:13)
f (cid:13) (cid:13)e e e (cid:13) (cid:13)
Once the training is completed, we obtain the DNN prior model
x =N(x ;W∗ ). (3.8)
n+1 n [0:M]
NotethatthisDNNpriormodelis,atbest,asaccurateasthepriorlow-fidelitymodel
e e e f
(2.2). We remarkthatifthe low-fidelitymodel (2.2)is alreadyin the formofa DNN,
this step can be avoided as the prior DNN model already exists.
3.3. Transfer Learning for Model Correction. The basic premise is that
the trained DNN prior model (3.8), which is an accurate representation of the prior
model (2.2), is able to capture the “bulk” behavior of the dynamics of the unknown
system (2.1). To further improve/correct the DNN prior model, we employ transfer
learning (TL) technique, with the help of the scarce high-fidelity data set (3.2).
The principle of transfer learning (TL) is based on the widely accepted notation
that the early layersof a DNN extract more generalfeatures of a dataset, while later
layers contain higher-level features ([22, 32, 26]). Following this, we “freeze” the
majority of the layers in the DNN prior model (3.8). Specifically, we fix the weights
and biases in most of the layers of the trained DNN prior model by making them
un-modifiable. We then use the high-fidelity data set (3.2) to retrain the parameters
in the last few layers.
Let 0≤ℓ≤M be a number separating the layers in the DNN operator N of the
trained prior model (3.8) into two groups: the first ℓ layers from the input layer (0th
layer) to the (ℓ−1)th layer, and the second group from the ℓth layer to theeoutput
layer (Mth layer). Using the notation (3.6), the hyper parameters can be separated
into the following two groups correspondingly,
W
[0:M]
= W [0:ℓ−1],W
[ℓ:M]
. (3.9)
We fix the first groupofparamete(cid:2)rsto be atthe val(cid:3)ues trainedin the DNN prior
modelvia(3.7),i.e. W [0:ℓ−1] =W [∗ 0:ℓ−1],andre-trainthesecondgroupofparameters
byminimizing themeansquareerrorofthemodel(3.8)overthehigh-fidelitydataset
(3.2). The optimization problemfthen becomes
JHF
2
W∗ =argmin x(j)−N x(j);W∗ ,W . (3.10)
[ℓ:M] 2 1 [0:ℓ−1] [ℓ:M]
W
[ℓ:M] Xj=1(cid:13) (cid:16) (cid:17)(cid:13)
(cid:13) (cid:13) 6e f (cid:13) (cid:13)Once training is completed, we obtain a DNN whose hyper-parameters are
∗ ∗ ∗
W = W ,W , (3.11)
[0:M] [0:ℓ−1] [ℓ:M]
h i
where the first group is from (3.7) andfthe second group from (3.10) separately.
Finally, we define our posterior DNN model operator N using these parameters
(3.11), i.e.,
N,N ·;W∗ . (3.12)
[0:M]
(cid:16) (cid:17)
Subsequently, we obtain a predictive emodel using the posterior DNN: for any given
initial condition x ,
0
x =N(x ), n=0,1,..., (3.13)
n+1 n
Inmostapplications,TLonlyneedstomodify the lastfew layersofapre-trained
DNN. In our setting, this means ℓ ≈ M. Therefore, the total number of parameters
to be re-trained in (3.10) is relatively small in comparison to the total number of
network parameters. This is highly relevant to the scarce high-fidelity data case we
consider here. In fact, for many problems, one may take ℓ = M, which implies that
TL only needs to re-train the parameters in the output layer.
3.4. Connection with Least Squares. Let us consider the special case of
ℓ = M, when TL only re-trains the output layer. The parameter grouping (3.9)
becomes
W
[0:M]
= W [0:M−1],W
M
,
and the minimization problem (3.10) b(cid:2)ecome (cid:3)
JHF
2
W∗ =argmin x(j)−N x(j);W∗ ,W , (3.14)
M 2 1 [0:M−1] M
W
M Xj=1(cid:13) (cid:16) (cid:17)(cid:13)
(cid:13) e f (cid:13)
where W∗ are the fixed par(cid:13) ameters pre-trained via (3.7). Si(cid:13) nce the linear ac-
[0:M−1]
tivation function σ(x) = x is used in the output layer, this minimization problem
becomefs a least squares problem,
AW =B, (3.15)
M
where the matrix A denotes the “feature matrix” of DNN prior model and B is the
high-fidelity data matrix.
To see this relation clearly, let us consider the connection from the (M −1)th
layer, which is the last hidden layer with d≥1 nodes, to the output layer which has
n ≥ 1 nodes. For each i = 1,...,n, consider the ith node of the output layer. Let
wi ∈Rd betheweightsfromthednodesofthe(M−1)th layer,andbi beitsscalar
M M
biasterm. Followingthe standardnotation,weconcatenatethe weightsandbiasinto
a single vector β =[bi ;wi ]∈Rd+1. The matrix W thus takes the form
i M M M
W =[β ,··· ,β ]∈R(d+1)×n. (3.16)
M 1 n
For the feature matrix A, consider the operator of the DNN prior model (3.5)
and let a:Rn →Rd be the output function of the (M −1)th layer,
∗ ∗
a(x)= σ ◦W ◦···◦ σ ◦W (x).
M M−1 1 0
(cid:16) (cid:17)7 (cid:16) (cid:17)
f fNote this is a fixed function as the parameters
W∗
are pre-trained via (3.7)
[0:M−1]
during the DNN prior model construction. Let
f
aˆ(x)=[1;a(x)]∈Rd+1,
where the inclusion of 1 is to accommodate the bias term in the definition of W
M
(3.16). The feature matrixis thenformedby evaluatingthe output vectorsateachof
the first entry of the high-fidelity data pairs (3.2), i.e.,
T T
A= aˆ x(1) ;··· ;aˆ x(JHF) ∈RJHF×(d+1).
1 1
(cid:20) (cid:16) (cid:17) (cid:16) (cid:17) (cid:21)
Finally, the matrix B contains the second entries of the high-fidelity data (3.2),
T T
B= x(1) ;··· , x(JHF) ∈RJHF×n.
2 2
(cid:20)(cid:16) (cid:17) (cid:16) (cid:17) (cid:21)
Upon solving the least squares problem (3.15) and obtaining its solution W∗ ,
M
we obtain the hyper parameters for the posterior model
∗ ∗ ∗
W = W ,W . (3.17)
[0:M] [0:M−1] M
h i
The corresponding DNN operator (3.13)fis
N=W∗
◦ σ
◦W∗
◦···◦ σ
◦W∗
. (3.18)
M M M−1 1 0
(cid:16) (cid:17) (cid:16) (cid:17)
It is obvious that the presented methfod can be viewed asfan “internal” correction
method to the DNN prior model. This is different from most of the existing model
correctionmethods, wherecertainforms ofthe correction(additive or multiplicative)
are assumed.
3.5. High-fidelity Data Over Coarse Time Scale. We now discuss a situa-
tionwhenthehigh-fidelitydataarenotonlyscarceinquantity,theyarealsoacquired
over a time scale larger than desired. This is a rather common situation, as high-
fidelity data are often difficult to acquire freely.
Specifically, let δ > 0 be a time step over which the true system (2.1) can be
sufficiently analyzed and studied. Suppose the high-fidelity trajectory data (3.1) are
alreadyformedintothepairwisedataset(3.2),whereeachpairhasatimestep(much)
bigger than δ. Denote the high-fidelity dataset as
SHF = x(j),x(j);∆(j) , j =1,...,J . (3.19)
1 2 HF
n o
where ∆(j) > δ is the time step separating the pair. For simplicity, we assume
∆(j) = k(j) ·δ with k(j) ≥ 1 is an integer for all j. Our objective is to create an
accuratehigh-fidelityFML modelforthe unknownsystem(2.1)overthe desiredtime
step δ.
To accomplish this, we first train the DNN prior model over the required time
step δ using the procedure described in Section 3.2. Specifically, we
• Generate the low-fidelity data set (3.3) using the time step δ, i.e.,
x(j)
=
2
Φ x(j) , j =1,...,J ;
δ 1 LF
e
• Tra(cid:16)in the(cid:17)DNN via (3.7);
e e
8• Obtain DNN prior model
x =N (x ;W∗ ), (3.20)
n+1 δ n [0:M]
where the subscriptδ is added to emphasize the model is overthe smalltime
e e e f
step.
Transferlearningmodelcorrectionisthenconductedbyutilizingthecoarsehigh-
fidelity dataset (3.19). Since each ∆(j) =k(j)·δ is a multiple of the small step δ, we
modify the TL training (3.10)using k(j) compositionofthe DNN prior operator. Let
N[k] =N ◦···◦N , k times,
δ δ δ
be the k times compositioen of N δe. We condeuct the transfer learning via
W∗
=argminJHF
xe(j)−N[k(j)] x(j);W∗ ,W 2 . (3.21)
[ℓ:M] 2 δ 1 [0:ℓ−1] [ℓ:M]
W
[ℓ:M] Xj=1(cid:13) (cid:16) (cid:17)(cid:13)
(cid:13) e f (cid:13)
(cid:13) (cid:13)
Oncesufficientlytrained,allthehyperparametersarefixedinthe formof (3.11),
and we obtain the posterior DNN model
N ,N ·;W∗ . (3.22)
δ δ [0:M]
(cid:16) (cid:17)
Notethattheposteriormodelisnowdeefinedovertherequiredsmalltimestepδ. The
use of the recurrent loss (3.21) follows from the work of [5], where the mathematical
justification of the approach was discussed.
4. Computational Studies. Wenowpresentnumericalstudiestodemonstrate
the effectiveness of the proposed method. The numerical examples are organized in
two groups. In the first group,the scarce high-fidelity data are observedon the same
time step required by the system resolution; whereas in the second group they are
observedoveracoarsertimestep,thusrequiringthetechniquediscussedinSection3.5.
In all the examples, the true models are known. They are used only to generate the
high-fidelity training data, as well as the validation data to examine the performance
of the trained DNN posterior models.
4.1. Model Correction with Scarce Data. Here we assume the scarce high-
fidelity data (3.2) are observed over a time step ∆ that is sufficient to resolve the
unknown dynamics (2.1). In each of the examples here, a low-fidelity prior model is
knownandused to constructthe DNN priormodel. The DNNs have 3 hidden layers,
while the number of neurons per layer varies from 20 to 80. The number of training
dataJ (3.3)forthe DNNpriormodelistypically∼5timesthe numberofnetwork
LF
parameters. The DNN prior model is first trained for 10,000−20,000 epochs using
a batch size of 100. Transfer learning is then conducted on the last layer W for
M
model correctionwith the high-fidelity data set (3.2), where the number of data J
HF
is60∼200timessmallerthanthatofthelow-fidelitydata(3.3). TheAdamoptimizer
with a learning rate of 1e−3 is used for training.
4.1.1. Damped Pendulum. Let us consider a damped pendulum as the un-
known true system,
x˙ =x
1 2
(4.1)
(x˙
2
=−αx 2−βsin(x 1),
92 5
1.5 4
3
1
2
0.5 1
0 0
-0.5 -1
-2
-1
-3
-1.5 -4
-2 -5
0 5 10 15 20 25 30 35 40 45 50 0 5 10 15 20 25 30 35 40 45 50
Fig. 4.1: Damped pendulum: system prediction up toT =50. Left: x1;Right: x2.
where α=0.1 and β =9. Our prior model is the following harmonic oscillator,
x˙ =x
1 2
(4.2)
(x˙
2
=−βx 1.
Although a very simple system, this represents the typical case of using linearization
to model a nonlinear system.
We consider the modeling domain Ω = [−π,π]×[−2π,2π] and use a constant
time step ∆ = 0.1. Data pairs are collected by randomly sampling initial conditions
fromΩ and evolvingthe state variables overone time step. The number ofnodes per
layerinourDNNistakentobe50. TheDNNmodelistrainedfor10,000epochswith
30,000low-fidelitydatapairsgeneratedfrom(4.2). Wethenuse250high-fidelitydata
pairs from the true system (4.1) to correct the DNN prior model. Figure 4.1 shows
an example trajectory prediction with a randomly selected initial condition x =
0
(−1.615,−0.258). We can clearly observe that the prior model fundamentally mis-
characterizesthesystembehavior. Withthesmallhigh-fidelitydataset,theposterior
modelcancorrectthepriormodelandaccuratelypredictthesystembehavior. Figure
4.2showstheerrorsinthepredictionforuptoT =100,averagedover100trajectories.
We observegoodaccuracyinthe posteriormodelpredictionfor sucharelativelylong
term.
4.1.2. Duffing Equation. We consider the Duffing equations as our true un-
known model,
x˙ =x
1 2
(4.3)
(x˙
2
=−x 1−ǫx3 1,
where ǫ = 0.05. The prior low-fidelity model is again the linear harmonic oscillator
(4.2).
The modeling domain is taken to be Ω=[0,3]2. Using prior model (4.2), a total
of30,000datapairsoveratimelag∆=0.1arerandomlydistributedwithinthetime
frame T = 12 to train the DNN. The DNN model is taken to have 50 neurons per
layer and is trained for 10,000 epochs.
Atotalof500high-fidelitydatapairsaregeneratedfromthetruesystem(4.3)for
model correction. For validation results, we present a case using a randomly selected
initial condition x = (0.233,−2.547) in Figure 4.3. For this relatively long-term
0
1010-3
3.5
3
2.5
2
1.5
1
0.5
0
0 10 20 30 40 50 60 70 80 90 100
Fig. 4.2: Damped pendulum: prediction errors up to T =100.
3 3
2 2
1 1
0 0
-1 -1
-2 -2
-3 -3
0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100
Fig. 4.3: Duffingequation: system prediction up to T =100. Left: x1; Right: x2.
prediction up to T = 100, we observe that the posterior model is able to correct the
deficiency in the low-fidelity prior model and accuratelypredict the systembehavior.
The prediction error, averaged over 100 randomly selected trajectories, is shown in
Figure 4.4. We observe good accuracy from the posterior model.
4.1.3. SEIR Model. We now consider the true unknown system to be a SEIR
model,
S˙ =µ(1−S)−βSI,
E˙ =βSI −(µ+σ)E,
(4.4)
I˙=σI
−(µ+γ)I,
R˙ =γI−µR,
where the susceptible (S),
exposed
(E), infected (I), and recovered(R) are the popu-
lations in a closed epidemic. They are normalized against the total population such
1110-3
10-4
10-5
10-6
10-7
10-8
10-9
0 10 20 30 40 50 60 70 80 90 100
Fig. 4.4: Duffingequation: average l2 error over 100 test trajectories.
that S,E,I,R ∈ [0,1]4 and S +E +I +R = 1 at all times t. We assume that the
unknown aspect of the system is in the parameters: the true values of the parame-
ters lie within the following ranges: µ ∈ [0.1,0.5], β ∈ [0.7,0.11], σ ∈ [0.3,0.7], and
γ ∈[0,0.4].
For the prior model, we consider the same SEIR system (4.4) with incorrect
parametervalues. Specifically,weassumeinthepriormodelallparametersarefixedat
the mean values of their corresponding ranges, i.e., µ=0.3,β =0.9,σ =0.5,γ =0.2.
We generate 30,000 low-fidelity data pairs over a time lag ∆=0.2 to train our DNN
model,anddatapairsarerandomlygeneratedfromtrajectoriesoflengthT =5. The
number of nodes per layer is 50.
For model correction, we assume the true model is the system (4.4) with the
parameters µ = 0.1792, β = 0.8669, σ = 0.3562, and γ = 0.2235 (note these values
remain unknown to the DNN models). We generate 250 high-fidelity data pairs from
the true model for the model correction procedure. In Figure 4.5 we plot solution
trajectories with for the randomly selected initial condition Initial condition x =
0
(0.4208,0.4224,0.0592,0.0976). While the priormodel, with its incorrectparameters,
shows significant errors in the prediction, the posterior model is able to correct the
errorsandaccuratelycapturethesystembehavior. Thehighaccuracyoftheposterior
model can be seen in the error plot Figure 4.6.
4.1.4. Metabolic Pathway. We now examine the three-step metabolic path-
way example [10], a nonlinear system of eight equations describing the conversion of
a substrate to product via intermediate metabolites M and M , enzymes E ,E ,E
1 2 1 2 3
120.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0 2 4 6 8 10 12 14 16 18 20
Fig. 4.5: SEIR Model: system prediction up to T =20.
and mRNAs G ,G ,G :
1 2 3
dG V
1 1
= −k ·G
dt 1+( P )ni1 +(Ka1)na1 1 1
 Ki1 S
 dd
d
dd
d
dGG
E
Et
t
t132
2
=
=
=
=
1
1
KV
V+
+
44
5+·
·(
(
G
GK
K
GP
Pi
i
1
22
3
1) )n
n
+
+i
iV
V2
3
k
k2
3+
+
4
5·
·(
(
E
EK
KM
M
1
2a
a1
22 3) )n na a2
3
− −k
k2
3· ·G
G2
3
(4.5)
dt K +G
5 2
Specd dd
iM
Md
d
fidE
t
t
ct3
1
2
p=
==
arak
kK
mV
c
c6
a
a6
et
t+
1
2
t· eG
·
·1
rG
1E
E3
+
v3
+1
2
a+
K
l·
·
uKS
MK
Km
ek
m
s11 1m
m
16
31
3
c+·
+
aE
·
·
nK(
(3
M
KS
M
Mm
b1
m
e2−
1
2
4
f−M ouM1 n) d2−
)
in−k [c 1ka 0ct2
a
].t·
31
WE
·
1+2
E
e+·
3
tKMK
a·
Km
M
k1 m
1
K
m3
e23
1
m
5+
t·
5
+
h( e·KM
M
(
K
mm
M
P1
2
m4
o−
62
d−
eM
liP
n2)
g)
domain to
beΩ=[0,1]8.OurpriormodelisobtainedbylinearizingthemRNA(E ,E ,E )and
1 2 3
1310-6
10-7
10-8
10-9
10-10
10-11
0 2 4 6 8 10 12 14 16 18 20
Fig. 4.6: SEIRmodel, average l2 error over100 test trajectories.
metabolite (M ,M ) dynamics. We also suppose the carrying capacity parameters
1 2
are incorrectly specified as n = n = n = n = n = n = 1 while for the
i1 i2 i3 a1 a2 a3
true model they are n = n = n = n = n = n =2. Our prior model is the
i1 i2 i3 a1 a2 a3
following,
dG V
1 1
= −k ·G
dt 1+( P )ni1 +(Ka1)na1 1 1
 Ki1 S
 dd
ddd
d
dGG
EEt
t
t
2132
==
=
V1
1
4+
+
·G(
(K
K
1P
Pi
i
+2
3) )n
n
ki
i
4V
V2
32
3
·+
+
E(
(
1K
KM
Ma
a1
22 3) )n na a2
3
− −k
k2
3· ·G
G2
3
(4.6)
=V ·G +k ·E
5 2 5 2
dt
d
dd
M
Md
ddE
t
tt3
1
2
=
=
=V
k
kc
c6
a
a·
t t1
2G
·
·3
E
E+
1
2·
·k
6
K
K·
1
1m
mE
1
33
· ·( (S M− 1−M M1)
12
4− )−k
c ka ct2
at· 3E
·2
E·
3K
·
K1
m 13
m5·( ·M (M1− 2−M P2)
).The prior model (4.6) is used to generate 75,000 data pairs over a time lag of
∆t = 0.05 and data pairs are randomly distributed within the time frame T = 12.5.
The DNN consists of 80 neurons per layer. After training the DNN prior model
for 20,000 epochs, we conduct model correction using 750 high-fidelity data pairs
generated by the true model.
Figure 4.7 depicts predictions made up to T =20 for a randomly selected initial
condition. In Figure 4.8 we plot the averageprediction error for the 8 state variables
over 100 test trajectories. For this complex biological system example, the posterior
DNNprovidesgoodaccuracyandvisibleimprovementoverthepriormodelprediction.
4.2. Coarse data observations. Wenowpresentfourexamplesdemonstrating
the method outlinedinsection2.3.3for handling scarceaswellascoarsehigh-fidelity
data. The DNNs have 5 hidden layers while the number of neurons in the first three
layerscontain50neuronsandthefinaltwolayerscontain20or50neurons. TheDNN
prior model is trained using data generated by the low-fidelity model on the desired
time scale δ for 10,000 epochs, using a batch size of 100 and early-stopping with a
patience of1,000epochs to avoidover-fitting. Transferlearningis thenconductedon
the final two layers of the DNN model, W [M−1:M], using (3.21).
4.2.1. Damped Pendulum. We revisitexample 4.1.2,wherethe true modelis
a damped pendulum system,
x˙ =x
1 2
(4.7)
(x˙
2
=−αx 2−βsin(x 1).
and the prior model is the linear harmonic oscillator,
x˙ =x
1 2
(4.8)
(x˙
2
=−βx 1.
Data pairs are sampled over the modeling domain Ω = [−2π,2π]×[−π,π] and are
generated by randomly selecting an initial condition from Ω and evolving the state
variablesoverthe desiredtime step δ =0.2. The DNN is takento be 5 hidden layers;
the first 3 layers contain 50 neurons and then final two layers contain 20 neurons.
The DNN is first trained with 50,000 data pairs generated by the prior model (4.8)
for 10,000 epochs.
We then use 500 high-fidelity data pairs generated by the true system (4.7) for
model correction; each data pair is separated by a coarse time-lag ∆(j), randomly
sampled from the set ∆(j) ∈{1,1.2,...,9.8,10}.
Figure 4.9 presents an example trajectory prediction up to T = 100 with the
randomlyselectedinitialconditionx =(−0.242,−4.241). Predictionsareconducted
0
on the desired time scale δ = 0.2. Figure 4.10 shows the average error over 100 test
trajectories and demonstrates good accuracy, despite using only coarse and scarce
high-fidelity data for model correction.
4.2.2. Van der Pol oscillator. OurnextexampleistheVanderPoloscillator.
In this system, the strength of the non-linearity is controlled by the scalar factor µ,
x˙ =x
1 2
(4.9)
(x˙
2
=µ(1−x2 1)x 2−x 1.
150.9 0.9
0.8 0.8
0.7 0.7
0.6 0.6
0.5 0.5
0.4 0.4
0.3 0.3
0.2 0.2
0.1 0.1
00 5 10 15 20 25 00 5 10 15 20 25
0.4 0.7
0.35 0.6
0.3
0.5
0.25
0.4
0.2
0.3
0.15
0.2
0.1
0.05 0.1
00 5 10 15 20 25 00 5 10 15 20 25
0.5 0.35
0.45
0.3
0.4
0.35 0.25
0.3
0.2
0.25
0.2 0.15
0.15
0.1
0.1
0.050 5 10 15 20 25 0.050 5 10 15 20 25
1 1
0.9 0.9
0.8 0.8
0.7 0.7
0.6 0.6
0.5 0.5
0.4 0.4
0.3 0.3
0.2 0.2
0.1 0.1
00 5 10 15 20 25 00 5 10 15 20 25
Fig. 4.7: Metabolic pathway: system prediction up to T = 25. Randomly selected initial
condition: G1 = 0.893,G2 = 0.851,G3 = 0.184,E1 = 0.651,E2 = 0.424,E3 = 0.342,M1 =
0.973,M2 =0.932.
For the true model we set µ=1, while the prior model assumes µ=0.5.
We consider the modeling domain Ω = [−2,2]× [−1.5,1.5]. In this example,
50,000 data pairs are generated by the prior model over the time scale δ = 0.2, and
are randomly distributed within the time frame T = 20. The final 2 layers of the
DNN prior model contains 20 neurons.
1610-4
2.5
2
1.5
1
0.5
0
0 5 10 15 20 25
Fig. 4.8: Metabolic pathway: average l2 error over 100 test trajectories.
2 6
1.5
4
1
0.5 2
0
0
-0.5
-1 -2
-1.5
-4
-2
-2.5 -6
0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100
Fig. 4.9: Damped pendulum: system prediction up to T =100. Left: x1; Right: x2.
Our high-fidelity data set consists of 500 data pairs, where each high-fidelity
data pair is separated by a coarse time step randomly selected from the set ∆(j) ∈
{1,1.2,...,9.8,10}. Aftertransferlearning,DNNpredictionsaremadeuptoT =100,
andresultsforarandomlyselectedinitialconditionx =(0.203,−0.924)arepictured
0
inFigure4.11. Figure4.12depictstheaverageerrorover100trajectoriesandwenote
again the ability of the network to correct the prior model for relatively long term
predictions.
170.2
0.18
0.16
0.14
0.12
0.1
0.08
0.06
0.04
0.02
0
0 10 20 30 40 50 60 70 80 90 100
Fig. 4.10: Damped pendulum: average l2 error over 100 test trajectories.
2.5 3
2
2
1.5
1
1
0.5
0 0
-0.5
-1
-1
-1.5
-2
-2
-2.5 -3
0 10 20 30 40 50 60 70 80 90 100 0 10 20 30 40 50 60 70 80 90 100
Fig. 4.11: Van der Pol oscillator: system prediction up toT =100. Left: x1; Right: x2.
4.2.3. Differential-algebraicsystem. Ournextexampleisthefour-dimensional
system of nonlinear differential-algebraic equations modeling an electric network,
u˙ =v /c
1 2
u˙ =u /L
 2 1 (4.10)
 0=v 1−(G 0−G inf)U 0tanh(u 1)−G infu
1
0=v +u +v .
2 2 1
The prior
model
is obtained by truncating the Taylor series of the hyperbolic
tangent function for a cubic approximation in the first algebraic condition,
u˙ =v /c
1 2
u˙ =u /L
 2 1 (4.11)
 0=v 1−(G 0−G inf)U 0(u 1−u3 1)−G infu
1
0=v +u +v .
2 2 1

1810-1
10-2
10-3
10-4
10-5
10-6
10-7
10-8
0 10 20 30 40 50 60 70 80 90 100
Fig. 4.12: Van derPol oscillator: average l2 error over 100 test trajectories.
The node voltage is represented by u while u , v and v are branch currents.
1 2 1 2
The parameter values are defined as C = 10−9,L = 10−6,U = 1,G = −0.1 and
0 0
G∞ =0.25.
The modeling domain is Ω=[−2,2]×[−0.2,0.2]. We generate 60,000data pairs
using the prior model (4.11) over the time step δ = 5×10−9, which are sampled
randomly from trajectories of length T = 5×10−7. The DNN model is 5 hidden
layers with 50 nodes per layer. The high-fidelity data is 500 data pairs generated by
thetrueunderlyingsystemandseparatedbyacoarsetimelagrandomlyselectedfrom
the set∆ ∈{2.5×10−8,3.0×10−8,...,1.5×10−7}. Figure4.13demonstratesthe
k(j)
posterior model’s ability to correct the complex dynamics on the desired time scale
for the randomly selectedinitial condition u =(−0.111,0.148)up to T =2.5×10−6
0
(500 total δ time step predictions). To validate the accuracy of our posterior model
further, Figure 4.14 depicts the averageerror for 100 test trajectories.
4.2.4. Metabolic Pathway. Wenowreexaminethethree-stepmetabolicpath-
way example presented in example 4.1.5. The modeling domain is again taken to be
Ω = [0,1]8. The desired time scale is δ = 0.2 and 60,000 data pairs from the prior
model are randomly generated from trajectories of length T = 20. The DNN model
consists of 5 hidden layers and 50 neurons per layer. We sample 500 high fidelity
data pairs separated by a coarse time lag, which is randomly selected from the set
∆ ∈{1,1.2,...,9.8,10}.
k(j)
In Figure 4.15, predictions are presented up to T = 25 for a randomly selected
initial condition. The average error for each of the 8 state variables over 100 test
trajectories is depicted in Figure 4.16. Given the challenging setting, the posterior
DNN still provides clear improvement over the imperfect prior model and ability to
provide accurate predictions.
5. Conclusion. We presented a model correcting framework for settings in
whichonlyscarcehigh-fidelitydatamaybeavailable. Givenanimperfectpriormodel
and a small high-fidelity data set, we are able to obtain a posterior model capable
190.14
1 0.12
0.1
0.5
0.08
0 0.06
0.04
-0.5 0.02
0
-1
-0.02
-1.5 -0.04
-0.06
0 0.5 1 1.5 2 2.5 0 0.5 1 1.5 2 2.5
10-6 10-6
0.02
0.05
0
-0.02
0
-0.04
-0.06 -0.05
-0.08
-0.1
-0.1
-0.12
-0.15
0 0.5 1 1.5 2 2.5 0 0.5 1 1.5 2 2.5
10-6 10-6
Fig. 4.13: Differential algebraic system: system prediction up to T =2.5×10−6. Top left:
u1. Top right: u2. Bottom left: v1. Bottom right: v2.
of accurate predictions of the true system’s state variables. This is obtained by cor-
recting the prior DNN model via the deep learning technique of transfer learning.
Our method contains two main novel features: (1) the form of the model correction
is not assumed as additive or multiplicative but rather is conducted internally to
the prior DNN model via transfer learning; (2) only a small (potentially coarse in
time-resolution) high-fidelity data set is required for obtaining the posterior model,
circumventingthecomputationalchallengeofDNNbasedlearningmethodsinsettings
where only scarce high-fidelity data are available. We presented numerous numerical
examples demonstratingthe merit ofthe method. Forfuture works,we planto natu-
rally extend this model correction method to partial differential equations as well as
partially observed dynamical systems.
2010-2
10-3
10-4
10-5
10-6
10-7
0 0.5 1 1.5 2 2.5
10-6
Fig. 4.14: DAEsystem: average l2 error over 100 test trajectories.
210.3 0.7
0.25 0.6
0.5
0.2
0.4
0.15
0.3
0.1
0.2
0.05 0.1
00 5 10 15 20 25 00 5 10 15 20 25
0.3 0.14
0.25 0.12
0.1
0.2
0.08
0.15
0.06
0.1
0.04
0.05 0.02
00 5 10 15 20 25 00 5 10 15 20 25
0.7 0.9
0.8
0.6
0.7
0.5
0.6
0.4 0.5
0.3 0.4
0.3
0.2
0.2
0.1
0.1
00 5 10 15 20 25 00 5 10 15 20 25
0.45 0.7
0.4 0.6
0.35
0.5
0.3
0.4
0.25
0.3
0.2
0.2
0.15
0.1 0.1
0.050 5 10 15 20 25 00 5 10 15 20 25
Fig. 4.15: Metabolic pathway: system prediction up to T = 25. Randomly selected initial
condition: G1 = 0.253,G2 = 0.679,G3 = 0.235,E1 = 0.119,E2 = 0.652,E3 = 0.893,M1 =
0.406,M2 =0.622.
2210-4
6
5
4
3
2
1
0
0 5 10 15 20 25
Fig. 4.16: Metabolic pathway: average l2 error over100 test trajectories.
23REFERENCES
[1] L.Alzubaidi,J.Bai,A.Al-Sabaawi,J.Santamaria,A.Albahri,B.Al-dabbagh,M.Fad-
hel, M. Manoufali, J. Zhang, A. Al-Timemy, Y. Duan, A. Abdullah, L. Farhan,
Y.Lu,A.Gupta,F.Albu,A.Abbosh,andY.Gu,Asurveyondeeplearningtoolsdeal-
ingwithdatascarcity: definitions,challenges,solutions, tips,andapplications,Journalof
BigData,10(2023).
[2] M. Bayarri, J. Berger, R. Paulo, J. Sacks, J. Cafeo, C. Cavendish, and J. Tu, A
framework for validation of computer models,Technometrics, 49(2007), pp.138–154.
[3] J.Brynjarsdo´ttir andA.O’Hagan,Learning about physical parameters, the importance of
model discrepancy,InverseProblems,30(2014).
[4] Z.ChenandD.Xiu,Ongeneralizedresidualnetworkfordeeplearningofunknowndynamical
systems,JournalofComputational Physics,438(2021).
[5] V. Churchill and D. Xiu, Learning fine scale dynamics from coarse observations via inner
recurrence,JournalofMachineLearningforModelingandComputing,(2022), pp.61–77.
[6] C.Currin,T.Mitchell,M.Morris,andD.Ylvisaker,Bayesianpredictionofdeterministic
functions, with applications to the design and analysis of computer experiments, Journal
oftheAmericanStatistical Association,86(1991), pp.953–963.
[7] B.Eastman,L.Podina,andM.Kohandel,Apinnapproachtosymbolicdifferentialoperator
discoverywithsparsedata,In: TheSymbiosisofDeepLearningandDifferentialEquations
II,(2022).
[8] M. Ebers, K. Steele, and J. Kutz, Discrepancy modeling framework: learning missing
physics, modeling systemic residuals, and disambiguating between deterministic and ran-
dom effects,SIAMJournalonAppliedDynamicalSystems,23(2024).
[9] M. Eldred, L. Ng, M. Barone, and S. Domino, Multifidelity uncertainty quantification
using spectralstochastic discrepancy models,In: HandbookofUncertaintyQuantification,
R.Ghanem D.HigdonandH.Owhadi,(2017), pp.1–45.
[10] e.a.Engl,HeinzW,Inverse problems insystemsbiology,InverseProblems,25(2009).
[11] Y.GaoandK.Mosalam,Deep transfer learning for image-based structural damage recogni-
tion,Computer-AidedCivilandInfrastructureEngineering,33(2018).
[12] R. Ghanem, D. Higdon, H. Owhadi, and et al, Handbook of Uncertainty Quantification,
SpringerCham,2017.
[13] R. Haylock and A. O’Hagan, On inference for outputs of computationally expensive algo-
rithms with uncertainty on the inputs,BayesianStatistics,5(1996), pp.629–637.
[14] Y.HeandD.Xiu,Numericalstrategyformodelcorrectionusingphysicalconstraints,Journal
ofComputational Physics,313(2016), pp.617–634.
[15] D. Higdon, J. Gattiker, B. Williams, and M. Rightley, Computer model calibration us-
ing high-dimensional output, Journal ofthe AmericanStatistical Association, 103(2008),
pp.570–583.
[16] D. Higdon, M. Kennedy, J. Cavendish, J. Cafeo, and R. Ryne, Combining field data and
computersimulationsforcalibrationandprediction,SIAMJournalofScientificComputa-
tion,26(2004), pp.448–466.
[17] V.JosephandS.Melkote,Statisticaladjustmentstoengineeringmodels,JournalofQuality
Tech.,41(2009), pp.362–375.
[18] S. K., X. Huan, and N. Habib, Embedded model error representation for bayesian model
calibration, International JournalofUncertaintyQuantification, 9(2019), pp.365–394.
[19] S.K.,H.Najm,andR.Ghanem,Onthestatisticalcalibrationofphysicalmodels,International
JournalofChemicalKinetics,47(2015).
[20] C.G.KaufmanandS.R.Sain,BayesianfunctionANOVAmodeling using gaussian process
prior distributions,BayesianAnalysis,5(2010), pp.123–149.
[21] M. C. Kennedy and A. O’Hagan, Bayesian calibration of computer models, Journal of the
RoyalStatistical Society: SeriesB(Statistical Methodology), 63(2001), pp.425–464.
[22] B. Neyshabur, H. Sedghi, and C. Zhang, What is being transferred in transfer learning?,
Advances inNeuralInformationProcessingSystems,33(2020), pp.512–523.
[23] J. Oakley and A. O’Hagan, Probablistic sensitivity analysis of complex models; a bayesian
approach, Journal of theRoyal Statistical Society: SeriesB (Statistical Methodology), 66
(2004).
[24] Z. Qian and C. Wu, Bayesian hierarchical modeling for integration low-accuracy and high-
accuracy experiments,Technometrics, 50(2008), pp.192–204.
[25] T.Qin,K.Wu,andD.Xiu,Datadrivengoverningequationsapproximationusingdeepneural
networks,JournalofComputational Physics,395(2019).
[26] M. Raghu, C. Zhang, J. Kleinberg, and S. Bengio, Transfusion: Understanding transfer
24learningformedicalimaging,AdvancesinNeuralInformationProcessingSystems,(2019),
pp.3342–3352.
[27] S.Ruder,M.Peters,andS.Swayamdipta,Transferlearninginnaturallanguageprocessing,
Proceedingsofthe2019ConferenceoftheNorthAmericanChpateroftheAssociationfor
Computational Linguistics,(2019).
[28] G. Somdatta, K. Kontolati, M. D. Shields, and G. E. Karniadakis, Deep transfer op-
erator learning for partial differential equations under conditional shift, Nature Machine
Intelligence, 4(2022).
[29] S.Wang,W.Chen,andK.Tsui,Bayesianvalidationofcomputermodels,Technometrics,51
(2009), pp.439–451.
[30] D. Williamson and A. T. Blaker, Evolving bayesian emulators for structured chatoic time
series,withapplicationtolargeclimatemodels,SIAM/ASAJournalofUncertaintyQuan-
tification, 2(2014), pp.1–28.
[31] J.-L.Wu, M.E. Levine, T.Schneider, andA. Stuart,Learning about structural errors in
models of complex dynamical systems,JournalofComputational Physics,513(2024).
[32] J.Yosinski,J.Clune,Y.Bengio,andH.Lipson,Howtransferablearefeaturesindeepneural
networks?,Advances inNeuralInformationProcessingSystems,(2014), pp.3320–3328.
[33] Z. Zhang,F. Bao, L. Ju,and G.Zhang,Transnet: Transferable neural networks for partial
differential equations,JournalofScientificComputing, 99(2024).
[34] Z.Zhang,Z.Zou,E.Kuhl,andG.Karniadakis,Discoveringareaction-diffusionmodelfor
alzheimer’s disease by combining pinns with symbolic regression, Computer Methods in
AppliedMechanicsEngineering,419(2024).
[35] Z. Zou, X. Meng, and G. E. Karniadakis, Correcting model misspecfification in physics-
informed neural networks (pinns),JournalofComputational Physics,505(2024).
25This figure "preview-micro.jpg" is available in "jpg"(cid:10) format from:
http://arxiv.org/ps/2410.17913v1This figure "preview-web.jpg" is available in "jpg"(cid:10) format from:
http://arxiv.org/ps/2410.17913v1This figure "preview.jpg" is available in "jpg"(cid:10) format from:
http://arxiv.org/ps/2410.17913v1