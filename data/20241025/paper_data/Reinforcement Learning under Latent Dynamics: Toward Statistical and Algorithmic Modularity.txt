Reinforcement Learning under Latent Dynamics:
Toward Statistical and Algorithmic Modularity
Philip Amortila Dylan J. Foster Nan Jiang
philipa4@illinois.edu dylanfoster@microsoft.com nanjiang@illinois.edu
Akshay Krishnamurthy Zakaria Mhammedi
akshaykr@microsoft.com mhammedi@google.com
Abstract
Real-world applications of reinforcement learning often involve environments where agents operate on
complex, high-dimensional observations, but the underlying (“latent”) dynamics are comparatively simple.
However,outsideofrestrictivesettingssuchassmalllatentspaces,thefundamentalstatisticalrequirements
and algorithmic principles for reinforcement learning under latent dynamics are poorly understood.
This paper addresses the question of reinforcement learning under general latent dynamics from a
statistical and algorithmic perspective. On the statistical side, our main negative result shows that
most well-studied settings for reinforcement learning with function approximation become intractable
when composed with rich observations; we complement this with a positive result, identifying latent
pushforward coverability as a general condition that enables statistical tractability. Algorithmically, we
develop provably efficient observable-to-latent reductions—that is, reductions that transform an arbitrary
algorithm for the latent MDP into an algorithm that can operate on rich observations—in two settings:
onewheretheagenthasaccesstohindsightobservationsofthelatentdynamics[LADZ23],andonewhere
the agent can estimate self-predictive latent models [SAGHCB20]. Together, our results serve as a first
step toward a unified statistical and algorithmic theory for reinforcement learning under latent dynamics.
1 Introduction
Many application domains for reinforcement learning (RL) require the agent to operate on rich, high-
dimensional observations of the environment, such as images or text [WSD15; LFDA16; KFPM21; NRKFG22;
Bak+22; Bro+22]. However, the environment itself can often be summarized by latent dynamics for a
low-dimensional or otherwise simple latent state space. The decoupling of latent dynamics from the complex
observation process naturally suggests a modular framework for algorithm design: first learn a representation
that decodes the latent state from observations, then apply a reinforcement learning algorithm for the latent
dynamics on top of the learned representation. This paper investigates the algorithmic and statistical
foundationsofthisframework. Weask: Can we take existing algorithms and sample complexity guarantees for
reinforcement learning in the latent state space and lift them to the observation space in a modular fashion?
There is a growing body of theoretical and empirical work developing algorithms that combine representation
learning and reinforcement learning to develop scalable algorithms. On the empirical side, a plethora of
representation learning objectives have been deployed to varying degrees of success [PAED17; Tan+17;
ZMCGL21; LSA20; YFK21; Lam+24; Guo+22; HPBL23], but we lack a mathematical framework to
systematically compare these objectives and understand when one might be preferred to another. On the
theoreticalside,allexistingapproachessufferfromthreeprimarydrawbacks: (a)theyaretailoredtorestricted
classes of latent dynamics models (tabular MDPs [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23],
LQR[DR21; Mha+20],orfactoredMDPs[MLJL21]),limitinggenerality; (b)theanalyses,despitefocusingon
restrictivesettings, areunwieldy, limitingprogressinalgorithm development; and(c) theyarenotmodular, in
the sense that the representation learning procedures are specialized to specific choices of latent reinforcement
learning algorithm, limiting ease of use.
1
4202
tcO
32
]GL.sc[
1v40971.0142:viXra1.1 Contributions
We address the aforementioned limitations by introducing a new framework, reinforcement learning under
general latent dynamics.
Reinforcement learning under general latent dynamics (Section 2). In our framework, the agent
performs control based on high-dimensional observations, but the dynamics of the environment are governed
by an unobserved latent state space. Following prior work (particularly the so-called Block MDP formulation
[DKJADL19]), we assume that the latent states can be uniquely decoded from observations, but that the true
decoder is unknown and must be learned. To aid in the decoding process, we supply the learner with a class
of representations that is realizable in the sense that it is powerful enough to represent the true decoder. Our
point of departure from prior theoretical works is that we do not assume specific structure (e.g., tabular
or linear dynamics) on the Markov decision process (MDP) that governs the latent dynamics. Instead, we
make the minimal assumption that the latent dynamics belong to a base MDP class which is statistically
tractable, in the sense that when the latent states are directly observed there exists some reinforcement
learning algorithm with low sample complexity that is capable of learning a near-optimal policy for every
MDP in the class. We take the first steps toward building a unified and modular theory for reinforcement
learning in this setting.
Contributions: Statistical modularity (Section 3). A central consideration for reinforcement learning
under latent dynamics is that representation learning and exploration must be intertwined: an accurate
decoder is required to explore the latent state space, but exploration is required to learn an accurate decoder.
To develop provable sample complexity guarantees, one must prevent errors from compounding during this
interleaving process, a challenging statistical problem which prior work addresses through strong structural
assumptions on the base MDP [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23; DR21; Mha+20;
MLJL21]. For the general latent-dynamics setting we consider, it is unclear whether similar techniques can
be applied, or whether the setting is even statistically tractable, ignoring computational considerations. Thus,
our first contribution considers the question of statistical modularity:1
If a base MDP class is tractable when observed directly, is the corresponding latent-dynamics problem
tractable?
StatisticalmodularityadoptsaminimaxperspectivebyassumingthatthebaseMDPliesinagivenclass, and
demands that the sample complexity of the latent-dynamics setting is controlled by a natural bound on the
samplecomplexityofthebaseMDPclass. Weshow,perhapssurprisingly,thatmost well-studiedreinforcement
learning settings involving function approximation [Li09; RVR13; JKALS17; SJKAL19; MJTS20; DVRZ19;
AJSWY20; WSY20; ZGS21; WAS21; WAJAYJS21; Du+21; JLM21; FKQR21] do not admit statistical
modularity(Theorem3.1). Inotherwords,statisticaltractabilityofanMDPclassdoesnotextendtothelatent-
dynamics setting. Wecomplementthesenegativefindingswithapositiveresult,identifyingpushforward cover-
ability as a general structural condition on the latent dynamics that enables sample efficiency (Theorem 3.2).
Contributions: Algorithmic modularity (Section 4). Beyond developing a modular understanding
of the statistical landscape, we investigate modular algorithm design principles for RL under general latent
dynamics. Specifically, we consider the question of observable-to-latent reductions, whereby RL under latent
dynamics can be reduced to the simpler problem of RL with latent states directly observed:
Can we generically lift algorithms for a base MDP class to solve the corresponding latent-dynamics problem?
This property, which we refer to as algorithmic modularity, enables modular, greatly simplified algorithm
design, allowing one to use an arbitrary base algorithm for the base MDP class to solve the corresponding
latent-dynamics problem. Algorithmic modularity is a stronger property than mere statistical modularity,
and thus is subject to our statistical lower bound. Accordingly, we consider two settings that sidestep the
lower bound through additional feedback and modeling assumptions. Our first algorithmic result considers
hindsight observability [LADZ23], where latent states are revealed during training, but not at deployment
1ThisquestionandassociateddefinitionsarerestatedformallyinSection3.1.
2(Theorem 4.1). Our second considers stronger function approximation conditions that enable the estimation
of self-predictive latent models [SAGHCB20] through representation learning (Theorem 4.2). Both results are
fully modular: they transform any sample-efficient algorithm for the base MDP class into a sample-efficient
algorithm for the latent-dynamics setting. Thus, they constitute the first general-purpose algorithms for RL
under latent dynamics.
Together,webelieveourresultscanserveasafoundationforfurtherdevelopmentofpractical,general-purpose
algorithms for RL under latent dynamics. To this end, we highlight a number of fascinating and challenging
open problems for future research (Section 5).
2 Reinforcement Learning under General Latent Dynamics
In this section we formally introduce our framework, reinforcement learning under general latent dynamics.
2.1 MDP preliminaries
We consider an episodic finite-horizon online reinforcement learning setting. With H denoting the horizon, a
Markov decision process (MDP) M⋆ =(cid:8) X,A,{P⋆}H ,{R⋆}H ,H(cid:9) consists of a state space X, an action
h h=0 h h=1
space A, a reward distribution R⋆ :X ×A→∆([0,1]) (with expectation r⋆(x,a)), and a transition kernel
h h
P⋆ :X ×A→∆(X) (with the convention that P⋆(·|∅) is the initial state distribution).
h 0
At the beginning of the episode, the learner selects a randomized, non-stationary policy π =(π ,...,π ),
1 H
where π : X → ∆(A); we let Π denote the set of all such policies. The episode evolves through the
h rns
followingprocess;beginningfromx ∼P⋆(·|∅),theMDPgeneratesatrajectory(x ,a ,r ),...,(x ,a ,r )
1 0 1 1 1 H H H
via a ∼π (x ), r ∼R⋆(x ,a ), and x ∼P⋆(·|x ,a ). We let PM⋆,π denote the law under this process,
h h h h h h h h+1 h h h
and let EM⋆,π denote the corresponding expectation, and likewise let PM,π and EM,π denote the analogous
laws and expectations in another MDP M.
For a policy π and MDP M, the expected reward for π is given by JM(π):=EM,π(cid:2)(cid:80)H r (cid:3), and the value
h=1 h
functions are given by
(cid:34) H (cid:35) (cid:34) H (cid:35)
(cid:88) (cid:88)
VM,π(x):=EM,π r |x =x , and QM,π(x,a):=EM,π r |x =x,a =a .
h h′ h h h′ h h
h′=h h′=h
We let π ={π }H denote an optimal deterministic policy of M, which maximizes VM,π (over π) at all
M M,h h=1
states (and in particular, satisfies π ∈argmax JM(π)), and write QM,⋆ :=QM,πM. For f :X ×A→R,
we write π (x) := argmax f(x,a)M as well as π V∈Π (xrns ) = max f(x,a). For MDP M, horizon h ∈ [H], and
f a f a
g :X →R, we let TM denote the Bellman (optimality) operator defined via
h
[TMg](x,a)=EM[r +g(x )|x =x,a =a],
h h h+1 h h
and we overload notation by letting [TMf](x,a) = EM[r +V (x )|x =x,a =a]. We also let TM,π
h h f h+1 h h h
denote the Bellman evaluation operator defined via
[TM,πf](x,a)=EM(cid:2) r +E [f(x ,a′)]|x =x,a =a(cid:3) ,
h h a′∼πh+1(·|xh+1) h+1 h h
for any π ∈Π . We define the induced occupancy measures for layer h via
rns
dM,π(x)=PM,π[x =x] & dM,π(x,a)=PM,π[x =x,a =a].
h h h h h
Online reinforcement learning. In online reinforcement learning, the learning algorithm Alg repeatedly
interacts with an unknown MDP M⋆ by executing a policy and observing the resulting trajectory. After T
rounds of interaction, the algorithm outputs a final policy π, with the goal of minimizing their risk, defined
(cid:98)
via
Risk(T,Alg,M⋆):=JM⋆(π )−JM⋆(π). (1)
M⋆ (cid:98)
3Additional assumptions. To simplify presentation, we assume that X and A are countable; we expect
that our results extend to continuous variables with an appropriate measure-theoretic treatment. We assume
that (cid:80)H r ∈[0,1] almost surely for any trajectory in M⋆.
h=1 h
2.2 Framework: Reinforcement learning under general latent dynamics
In reinforcement learning under general latent dynamics, we consider MDPs M⋆ where the dynamics are
governed by the evolution of an unobserved latent state s , while the agent observes and acts on observations
h
x generated from these latent states. Formally, a latent-dynamics MDP consists of two ingredients: a
h
base MDP M = {S,A,{P }H ,{R }H ,H} defined over a latent state space S, and a decodable
lat lat,h h=0 lat,h h=1
emission process ψ :={ψ :S →∆(X)}H , which maps each latent state to a distribution over observations.
h h=1
The former is an arbitrary MDP defined over S, while the latter is defined as follows.
Definition 2.1 (Emission process). An emission process is any function ψ :={ψ :S →∆(X)}H , and is
h h=1
said to be decodable if
∀h,∀s′ ̸=s∈S : suppψ (s)∩suppψ (s′)=∅. . (2)
h h
When ψ ={ψ }H is decodable, we let ψ−1 :={ψ−1 :X →S}H denote the associated decoder.
h h=1 h h=1
With this, we can formally introduce the notion of a latent-dynamics MDP.
Definition 2.2 (Latent-dynamics MDP). For a base MDP M ={S,A,{P }H ,{R }H ,H}, and
a decodable emission process ψ, the latent-dynamics MDP ⟪Mlat ,ψ⟫:=(cid:8) X,Ala ,t {,h Ph=0 }H la ,t {,h Rh=1 }H ,H(cid:9)
lat obs,h h=0 obs,h h=1
is defined as the MDP where the latent dynamics evolve based on the agent’s action a ∈A via the process
h
s ∼P (s ,a ) and r ∼R (s ,a ). The latent state is not observed directly, and instead the agent
h+1 lat,h h h h lat,h h h
observes x ∈X generated by the emission process x ∼ψ (s ).2
h h h+1 h
Note that under these dynamics, the decoder ψ−1 associated with ψ ensures that ψ−1(x )=s almost surely
h h h
for all h∈[H]. That is, the latent states can be uniquely decoded from the observations. To emphasize the
distinction between the latent-dynamics MDP ⟪M ,ψ⟫ (which operates on the observable state space X)
lat
and the MDP M (which operates on the latent state space S), we refer to the latter as a base MDP rather
lat
than, for example, a “latent MDP”, and apply a similar convention to other latent objects whenever possible.3
Departing from prior work, we do not place any inherent restrictions on the base MDP, and in particular
do not assume that the latent space is small (i.e., tabular). Rather, we aim to understand—in a unified
fashion—what structural assumptions on the base MDP M are required to enable learnability under latent
lat
dynamics. To this end, it will be useful to considers specific classes (i.e., subsets) of base MDPs M and
lat
the classes of latent-dynamics MDPs they induce.
Definition 2.3 (Latent-dynamics MDP class). Given a set of base MDPs M and a set of decoders
lat
Φ⊂{X →S}, we let
⟪M ,Φ⟫:={⟪M ,ψ⟫:M ∈M ,ψ is decodable, ψ−1 ∈Φ} (3)
lat lat lat lat
denote the class of induced latent-dynamics MDPs.
Stated another way, ⟪M ,Φ⟫ is the set of all latent-dynamics MDPs ⟪M ,ψ⟫ where (i) the base MDP
lat lat
M lies in M , and (ii), the emission process ψ is decodable, with the corresponding decoder belonging
lat lat
to Φ. The class M represents our prior knowledge about the underlying MDP M ; concrete classes
lat lat
considered in prior work include tabular MDPs [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23], linear
dynamical systems [DMRY20; DR21; Mha+20], and factored MDPs [MLJL21]. In particular, the class
M may itself warrant using function approximation. At the same time, the class Φ represents our prior
lat
knowledge or inductive bias about the emission process, enabling representation learning. In what follows, we
investigatewhatconditionsonM maketheinducedclass⟪M ,Φ⟫tractable,bothstatistically(statistical
lat lat
modularity; Section 3) and via reduction (algorithmic modularity; Section 4).
2Equivalently the dynamics can be described via Robs,h(x h,a h) = Rlat(ψ h−1(x h),a h) and Pobs,h(x
h+1
| x h,a h) =
Plat,h(ψ h− +1 1(x h+1)|ψ h−1(x h),a h)·ψ h+1(x h+1|ψ h− +1 1(x h+1)).
3Forexample,inSection4wewillbeconcernedwithreductionsfromobservation-spacealgorithmsto“basealgorithms” that
operateonthelatentstatespace.
43 Statistical Modularity: Positive and Negative Results
This section presents our main statistical results. We begin by formally defining the notion of statistical
modularity introduced in Section 1, present our main impossibility result (lower bound) and its implications
(Section 3.2), then give positive results for the general class of pushforward-coverable MDPs (Section 3.3).
3.1 Statistical modularity: A formal definition
We first define the statistical complexity for a MDP class (or, model class) M.
Definition 3.1 (Statistical complexity). We say that an MDP class M can be learned up to ε-optimality
using comp(M,ε,δ) samples if there exists an algorithm Alg which, for every M ∈M, attains
Risk(T,Alg,M)≤ε
with probability at least 1−δ after T =comp(M,ε,δ) rounds of online interaction in M.
We say that a base MDP class M admits statistically modularity if, for any decoder class Φ, the induced
lat
latent-dynamics MDP class ⟪M ,Φ⟫ can be learned with statistical complexity that is polynomial in: (i)
lat
the statistical complexity for the base class, and (ii) the capacity of the decoder class.
Definition 3.2 (Statistical modularity). For a decoder class Φ, we say the MDP class M is statistically
lat
modular under complexity comp(M ,ε,δ) if
lat
comp(⟪M ,Φ⟫,ε,δ)=poly(comp(M ,ε,δ),log|Φ|). (4)
lat lat
We say that M admits strong statistical modularity if Eq. (4) holds when comp(M ,ε,δ) is the minimax
lat lat
sample complexity for M .
lat
In the sequel, we examine well-studied MDP classes M (e.g., those which admit low Bellman rank
lat
[JKALS17]) and choose comp(M ,ε,δ) based on natural upper bounds on their optimal sample complexity;
lat
in this case we will simply say they are (or are not) statistical modular, leaving the complexity upper bound
comp implicit. Following prior work [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23; DR21; Mha+20;
MLJL21], we use log|Φ| as a proxy for the statistical complexity of supervised learning with the decoder class
Φ.4
The two most notable examples of statistical modularity covered by prior work are: (i) taking M as the set
lat
oftabularMDPsadmitsstrongstatisticalmodularity[DKJADL19;MHKL20;MFR23],and(ii)takingM as
lat
the set of linear MDPs admits statistical modularity with complexity poly(d,H,|A|,ε−1,log(cid:0) δ−1(cid:1) ) [AKKS20;
UZS22; MCKJA24; MBFR23].5 Interestingly, the latter does not admit strong statistical modularity, because
theoptimalrateforM doesnotscalewith|A|,buttheratefor⟪M ,Φ⟫necessarilydoes[LS20;HLSW21].
lat lat
The results of Mhammedi et al.; Misra et al.; Song et al. [Mha+20; MLJL21; SWFK24] can be viewed as
instances of statistical modularity for other base MDP classes.
3.2 Lower bounds: Impossibility of statistical modularity
Our main result in this section is to show that for most MDP classes M considered in the literature on
lat
sample-efficient reinforcement learning with function approximation [RVR13; JKALS17; SJKAL19; MJTS20;
AJSWY20; Li09; DVRZ19; WSY20; ZGS21; Du+21; JLM21; FKQR21], statistical modularity (under the
natural complexity upper bound for the class of interest) is impossible. Our central technical result is the
followinglowerbound, whichshowsthatstatisticalmodularitycanbeimpossibleevenwhen|M |=1. That
lat
is, even when the base MDP is known to the learner a-priori. The lower bound is a significant generalization
of the result from Song et al. [SWFK24]; we first state the lower bound, then discuss implications.
4Ourmainresultseasilyextendtoinfiniteclassesthroughstandardarguments.
5Inthelattercase,thelatent-dynamicsclass⟪Mlat,Φ⟫maybeseentobeasetoflow-rankMDPs(thatis,linearMDPs
withunknownfeatures),sothatlow-rankMDPalgorithmsmaybeapplieddirectlyontheobservations(AppendixD.2).
5Statistical
Base MDP class M
lat Modularity?
Tabular ✓
Contextual Bandits ✓
Low-Rank MDP ✓
Figure 1: Summary of statistical
Known Deterministic MDP (|M |=1) ✓
lat modularity (SM) results.
Low State Occupancy (∀π:S →∆(A)) ✓ ✓: SM is possible for a nat-
Model Class + Pushforward Coverability ✓ ural choice of comp(·) (e.g.,
Linear CB/MDP ✗⋆ poly(|S|,|A|,H,ε−1,log(cid:0) δ−1(cid:1) ) for
Model Class + Coverability (∀π M :M ∈M) ✗ tabular MDPs).
Known Stochastic MDP (|M lat|=1) ✗ ✗: SM is not possible with natural
Bellman Rank (Q-type or V-type) ✗ choices of comp(·).
Eluder Dimension + Bellman Completeness ✗ ?: open.
Q⋆-Irrelevant State Abstraction ✗ ⋆: SM is possible if willing to pay
Linear Mixture MDP ✗ for (suboptimal) |A| complexity.
Linear Q⋆/V⋆ ✗ See Appendix D.2 for precise
Low State/State-Action Occupancy (∀π :M ∈M) ✗ descriptionsofeachsettingand our
M
Bisimulation ? choices for their complexities.
Low State-Action Occupancy (∀π:S →∆(A)) ?⋆
Model Class + Coverability (∀π:S →∆(A)) ?
Theorem 3.1 (Impossibility of statistical modularity). For every N ≥4, there exists a decoder class Φ with
|Φ|=N andafamilyofbaseMDPsM satisfying(i)|M |=1, (ii)H ≤O(log(N)), (iii)|S|=|X|≤N2,
lat lat
(iv) |A|=2, and such that
1. For all ε,δ >0, we have comp(M ,ε,δ)=0.
lat
2. For an absolute constant c>0, comp(⟪M ,Φ⟫,c,c)≥Ω(N/log(N)).
lat
In other words, even when the base dynamics are fully known, strong statistical modularity (in this case,
poly(log|Φ|) complexity) is impossible; any algorithm for the latent-dynamics setting will require at least
√
min{ S,2Ω(H)/H,|Φ|/log|Φ|} episodes to learn a near-optimal policy for a worst-case latent-dynamics MDP
⟪M ,ψ⟫∈⟪M ,Φ⟫.
lat lat
Intuition for lower bound. The intuition behind the lower bound in Theorem 3.1 is as follows: the
unobserved latent state space consists of N = |Φ| binary trees (indexed from 1 to N), each with N leaf
nodes. The starting distribution is uniform over the roots of the N trees, and the agent receives a reward
of 1 if and only if they navigate to the leaf node that corresponds to the index of their current tree. The
observed state space is identical to the latent state space, but the emission process shifts the index of the tree
by an amount which is unknown to the agent. Despite the base MDP being known and the decoder class
satisfying realizability, the agent requires near-exhaustive search to identify the value of the shift and recover
a near-optimal policy.
A taxonomy of statistical modularity. As a corollary, we prove that many (but not all) well-studied
function approximation settings do not admit statistical modularity by embedding them into the lower bound
construction of Theorem 3.1 (as well as a variant of the result, Theorem D.1). Our results are summarized
in Figure 1. Our impossibility results highlight the following phenomenon: many MDP classes M that
lat
place structural assumptions via the value functions (e.g., MDPs with linear-Q⋆/V⋆ [Du+21] or MDPs with
a Bellman complete value function class of bounded eluder dimension [JLM21; WSY20]) become intractable
under latent dynamics. Intuitively, this is because it is not possible to take advantage of structure in value
functions without learning a good representation, and, simultaneously, these assumptions are too weak by
themselves to enable learning such a representation. Meanwhile, MDP classes M that place structural
lat
assumptions on the transition distribution (e.g., MDPs with low state occupancy complexity [Du+21] or
6low-rank MDPs [AKKS20]) are sometimes (but not always) tractable under latent dynamics.6
We point to Appendix D.2 for background on all the settings in Figure 1 and proofs that they are (or are
not) statistically modular. We remark that it is fairly straightforward to embed most of the MDP classes of
Figure 1 into the construction of Theorem 3.1 since it only uses only a single base MDP M , and we expect
lat
that many other base MDP classes can similarly be shown to be intractable. However, proving the positive
results in Figure 1 requires establishing several new results showing that certain base classes are tractable
under latent dynamics; most notably, we next discuss the case of pushforward coverability.
3.3 Upper bounds: Pushforward-coverable MDPs are statistically modular
Our main postive result concerning statistical modularity is to highlight pushforward coverability [XJ21;
AFK24;MFR24]—astrengthenedversionofthecoverability parameterintroducedinXieetal.[XFBJK23]—as
a general structural parameter that enables sample-efficient reinforcement learning under latent dynamics.
Definition 3.3 (Pushforward coverability). The pushforward coverability coefficient C for an MDP M
push lat
with transition kernel P is defined by
lat
P (s′ |s,a)
C (M )= max inf sup lat,h−1 . (5)
push lat h∈[H]µ∈∆(S)(s,a,s′)∈S×A×S µ(s′)
Concrete examples [AFK24; MFR24] include: (i) tabular MDPs M admit C (M ) ≤ |S|; and (ii)
lat push lat
Low-Rank MDPs M (with or without known features) in dimension d admit C (M ) ≤ d. Further
lat push lat
examples include analytically sparse Low-Rank MDPs [GMR24] and Exogenous Block MDPs with weakly
correlated noise [MFR24]. Our main result is as follows.
Theorem 3.2 (Pushforward-coverable MDPs are statistically modular). Let M be a base MDP class such
lat
that each M ∈M has pushforward coverability bounded by C (M )≤C . Then, for any decoder
lat lat push lat push
class Φ, we have:
1. comp(M ,ε,δ)≤poly(C ,|A|,H,log|M
|,ε−1,log(cid:0) δ−1(cid:1)
), and
lat push lat
2. comp(⟪M ,Φ⟫,ε,δ)≤poly(C ,|A|,H,log|M
|,log|Φ|,ε−1,log(cid:0) δ−1(cid:1)
,loglog|S|).
lat push lat
Theorem 3.2 shows that, modulo a term that is doubly-logarithmic in |S|, latent pushforward coverability
enables statistical modularity. That is, when the base (latent) dynamics satisfy pushforward coverability,
there exists an algorithm for the latent-dynamics setting which scales with the statistical complexity of the
base MDP class and log|Φ|. We suspect that the additional loglog|S| factor is not essential and can be
removed with a more sophisticated analysis. We note that the complexity comp chosen above is not the
minimax complexity for M , since every set of pushforward coverable MDPs is also a set of coverable MDPs
lat
with a potentially smaller coverability parameter [AFK24].
Let us provide some intuition for this result. We firstly note that when M⋆ has pushforward coverability
lat
parameter C , it holds that for any emission process ψ⋆, the observation-level MDP M⋆ :=⟪M⋆ ,ψ⋆⟫
push obs lat
also satisfies pushforward coverability with the same parameter C (Lemma C.5). Yet, despite access to
push
realizable base MDP class M and decoder class Φ, it is unclear whether the latent-dynamics MDP M⋆
lat obs
satisfies any of the observation-level function approximation conditions required by existing approaches that
provide sample complexity guarantees under pushforward coverability. In particular, known algorithms for
thissettingeitherrequireaBellman-completevaluefunctionclass[XFBJK23],aclassrealizingcertaindensity
ratios [AFJSX24; AFK24], or a realizable model class [AFK24], and it is highly nontrivial to construct these
for the latent-dynamics MDP M⋆ =⟪M⋆ ,ψ⋆⟫ given only the base MDP class M and the decoder class
obs lat lat
Φ. Intuitively, this is because the former observation-level function approximation classes capture properties
of the observation-level dynamics which cannot be obtained without some knowledge of the emission process.
6Ifoneiswillingtopayforsuboptimal|A|factors,thenmore(butnotall)classesbecomestatisticallytractable(e.g.,linear
MDPs[JYWJ20]andMDPswithlowstate-actionoccupancy[Du+21]).
7Technical overview: Low-dimensional embeddings for pushforward-coverable MDPs. The idea
behind our positive result is to show that under the conditions of Theorem 3.2, it is possible to construct an
(approximately) Bellman-complete value function class for the latent-dynamics MDP M⋆ , at which point we
obs
can apply the Golf algorithm of Jin et al. [JLM21]. We achieve this via two technical contributions. The
first is the introduction of the mismatch functions Γ , formally defined as follows.
ϕ
Definition 3.4 (Mismatch functions). For a decodable emission process ψ⋆ and decoder ϕ∈Φ, the mismatch
function for ϕ, Γ ={Γ :S →∆(S)}H , is defined, for every h∈[H], as the probability kernel
ϕ ϕ,h h=1
Γ (s′ |s ):=P (ϕ (x )=s′).
ϕ,h h h xh∼ψ h⋆(sh) h h h
The mismatch functions allow us to express functions of the decoders as latent objects, and we will revisit
them in the context of self-predictive estimation (Section 4.3). For the present result, we show (Lemma C.7)
that the mismatch functions can capture the observation-level Bellman backups for any function of the
decoders. That is, for any x ,a , letting s =(ψ⋆)−1(x ) denote the true latent state, we have that for any
h h h h
f :S×A→R and ϕ∈Φ:
lat
[TM o⋆ bs(f ◦ϕ )](x ,a )=[TM l⋆ at(Γ ◦V )](s ,a ). (6)
h lat h+1 h h h ϕ,h+1 flat h h
That is, the Bellman update of f ◦ϕ in the latent-dynamics MDP M⋆ can be expressed as a Bellman
lat h+1 obs
update in the base MDP M⋆ for a different (latent) function Γ ◦V (s ) := (cid:80) Γ (s′ |
lat ϕ,h+1 flat h+1 s′ ϕ,h+1 h+1
h+1
s )max f (s′ ,a′).
h+1 a′ lat h+1
However, the mismatch functions Γ embed some knowledge of the emission process, and (with only decoder
ϕ
and base model realizability) are unknown to the learner. Our second technical contribution bypasses
this by establishing a new structural property for pushforward-coverable MDPs (Lemma E.1): there exist
low-dimensional linear embeddings of their transition kernels which can approximate Bellman backups for an
arbitrary and potentially unknown set of functions, as long as the set is not too large.
Lemma 3.1 (MDPS with pushforward coverability admit low-dimensional embeddings). Let M be a
known MDP with reward function r, transition kernel P, and pushforward coverability parameter C . Let
push
µ = {µ } denote its pushforward coverability distribution (i.e. the minimizer of Definition 3.3) and
h h∈[H]
F ⊆(S×[H]→[0,1]) be an arbitrary class of functions. Suppose that we sample W ∈{±1}d×S as a matrix
of independent Rademacher random variables, and define
1 (cid:16) (cid:17) 1 (cid:16) (cid:17)
ψ (s,a)=r (s,a)⊕ √ W P (·|s,a)/µ1/2(·) and w =1⊕ √ W µ1/2(·)f (·) ∈Rd+1.
h h d h h ·∈S f,h d h h+1 ·∈S
Then for any ε ∈(0,1), as long as we set
apx
C log(cid:0) 16|F|Hδ−1/ε (cid:1)
d≥29 push apx ,
ε
apx
we have that for all f ∈F and h∈[H], with probability at least 1−δ:
E
(cid:104)(cid:0)
clip [⟨w ,ψ (s,a)⟩]−T f
(s,a)(cid:1)2(cid:105)
≤ε ,
µh⊗Unif(A) [0,2] f,h h h h+1 apx
as well as max ∥ψ (s,a)∥2 ≤ C (16log(|S||A|H)+11) and max ∥w ∥2 ≤ 16log(|F|H)+11. We
s,a,h h 2 push f,h f,h 2
emphasize that the feature map ψ ={ψ }H is oblivious to F, in the sense that it can be computed directly
h h=1
from M without any knowledge of F.
We use this property, in conjunction with latent model realizability, to construct linear features that can
approximatetheright-hand-sideofEq. (6),thusyieldingan(approximately)Bellman-completevaluefunction
class for the latent-dynamics MDP M⋆ . A fascinating open question is whether a similar approach can be
obs
used to establish that standard (as opposed to pushforward) coverable MDPs are statistically modular, which
would encompass all other known positive cases of statistical modularity (cf. Figure 1).
84 Algorithmic Modularity
We now turn our attention to algorithmic modularity. Specifically, we aim for observable-to-latent reductions,
whereby—via representation learning—RL under latent dynamics can be efficiently reduced to the simpler
problem of RL with latent states directly observed. Since algorithmic modularity is a stronger property than
statistical modularity, we sidestep the previous lower bounds in Section 3 through additional feedback and
modeling assumptions. Our main result for this section is a new meta-algorithm, O2L, which, under these
assumptions (and when equipped with an appropriately designed representation learning oracle), acts as a
universal reduction in the sense that, whenever the representation learning oracle has low risk, the reduction
transforms any sample-efficient algorithm for any base MDP class into a sample-efficient algorithm for the
induced latent-dynamics MDP class.
4.1 Setup and O2L meta-algorithm
For the results in this section, we denote the (unknown) latent-dynamics MDP of interest by M⋆ :=
obs
⟪M⋆ ,ψ⋆⟫, and use ϕ⋆ :=(ψ⋆)−1 to denote the true decoder. The O2L meta-algorithm (Algorithm 1) learns
lat
a near-optimal policy for M⋆ by alternating between performing representation learning and executing a
obs
black-box “base” RL algorithm (designed for the base MDP) on the learned representation; this approach is
inspiredbyempiricalmethodsthatblendrepresentationlearningandRLinthelatentspace(e.g.,[GKBNB19;
SAGHCB20; Ni+24]).
Concretely, the algorithm takes as input a representation learning oracle RepLearn and a base RL algorithm
Alg that operates in the latent space. In each epoch t∈[T], RepLearn produces a new representation
lat
ϕ(cid:98)(t) : X → S based on data observed so far (potentially using additional side information, which we will
elaborateoninthesequel). Then,thereductioninvokesAlg lat,usingϕ(cid:98)(t) tosimulateaccesstothetruelatent
states. Inparticular,Alg runsforK episodes,whereateachepisodek: (i)Alg producesalatentpolicy
lat lat
π (t,k) :S×[H]→∆(A),(ii)thelatentpolicyistransformedintoanobservation-levelpolicyviacomposition
lat
withϕ(cid:98)(t),i.e. π lat(t,k)◦ϕ(cid:98)(t),whichisthendeployedtoproduceatrajectory{x h(t,k),a h(t,k),r h(t,k)}H h=1,and(iii)the
trajectory is compressed through ϕ(cid:98)(t) and used to update Alg
lat
via {ϕ(cid:98)( ht)(x h(t,k)),a h(t,k),r h(t,k)}H
h=1
(cf. Line 8
of Algorithm 1).7 After the K rounds conclude, Alg produces a final latent policy π(t) :S×[H]→∆(A).
lat (cid:98)lat
The final policy π
(cid:98)
chosen by the O2L algorithm is a uniform mixture of π (cid:98)l(t a) t◦ϕ(cid:98)(t) over all the epochs.
The central assumption behind O2L is that the base algorithm Alg can achieve low-risk in the underlying
lat
base MDP M⋆ if given access to the true latent states s =ϕ⋆(x ). Beyond this assumption, we require
lat h h
that the representation learning oracle RepLearn can learn a sufficiently high-quality representation. In our
applications, this will be made possible by assuming access to a realizable decoder class Φ and two distinct as-
sumptions: hindsight observability (Section 4.2) and conditions enabling self-predictive representation learning
(Section 4.3). We will show that under these conditions, we can instantiate a representation learning oracle
suchthatO2LinheritsthesamplecomplexityguaranteeforAlg ,therebyachievingalgorithmicmodularity.
lat
4.2 Algorithmic modularity via hindsight observability
Our first algorithmic result bypasses the hardness in Section 3 by considering the setting of hindsight
observability, which has garnered recent interest in the context of POMDPs [LADZ23; GCWXWB24; SLS23;
LXJZV24]. Here, we assume that at training time (but not during deployment), the algorithm has access to
additional feedback in the form of the true latent states, which are revealed at the end of each episode.
Assumption 4.1 (Hindsight Observability [LADZ23]). The latent states (ϕ⋆(x ),...,ϕ⋆ (x )) are revealed
1 1 H H
to the learner after each episode (x ,a ,r ,...,x ,a ,r ) concludes.
1 1 1 H H H
We emphasize that in the hindsight observability framework, the learner must still execute observation-space
policies π :X →∆(A), as the latent states are only revealed at the end of each episode. Under hindsight
obs
observability, we can instantiate the representation learning oracle in O2L so that the reduction achieves
7Notethat,ifϕ(cid:98)isinaccurate,thecompressedtrajectorycannotnecessarilybeviewedasbeinggeneratedbyalatentMDP,
andmustinsteadbeviewedascomingfromaPartially Observed MDP(AppendixH.1.1).
9Algorithm 1 O2L: Observable-to-Latent Reduction
1: input: Epochs T ∈N, episodes K ∈N, decoder set Φ, rep. learning oracle RepLearn, base alg. Alg lat.
2: for t=1,2,··· ,T do
3: RepLearn chooses a representation ϕ(cid:98)(t) :X →S ∈Φ based on data collected so far.
Initialize new instance of Alg .
4: lat
5: for k =1,2,··· ,K do //Alg lat plays K rounds in the “ϕ(cid:98)(t)-compressed dynamics.”
6: Alg lat chooses policy π l(t a, tk) :S×[H]→∆(A).
7: Deploy π lat◦ϕ(cid:98)(t) to collect trajectory {x h(t,k),a h(t,k),r h(t,k)}H h=1.
8: Update Alg lat with compressed trajectory {ϕ(cid:98) h(t)(x h(t,k)),a h(t,k),r h(t,k)}H h=1.
9: end for
10: Alg lat returns final policy π (cid:98)(t) :S×[H]→∆(A), deploy π (cid:98)(t)◦ϕ(cid:98)(t) to collect one more trajectory.
11: end for
12: return π (cid:98) =Unif(π (cid:98)(1)◦ϕ(cid:98)(1),...,π (cid:98)(T)◦ϕ(cid:98)(T)).
low risk for any choice of black-box base algorithm Alg . In particular, we make use of online classification
lat
oracles, which use the revealed latent states to achieve low classification loss with respect to ϕ⋆ under
adaptively generated data. We first state a guarantee based on generic classification oracles, then instantiate
it to give a concrete end-to-end sample complexity bound.
Formally,ateachstept,theonlineclassificationoracle,denotedviaRep ,isgiventhestatesandhindsight
class
observations collected so far and produces a deterministic estimate ϕ(cid:98)(t) =Rep class({x( hi),ϕ⋆ h(x( hi))} i<t,h≤H)
for the true decoder ϕ⋆. We measure the regret of the oracle via the 0/1 loss for classification:
T H
Reg class(T):=(cid:88)(cid:88) E π(t)∼p(t)Eπ(t)(cid:104) I(cid:8) ϕ(cid:98) h(t)(x h)̸=ϕ⋆ h(x h)(cid:9)(cid:105) ,
t=1h=1
where p(t) represents a randomization distribution over the policy π(t). Our reduction succeeds under the
assumption that the oracle has low expected regret.
Assumption 4.2. For any (possibly adaptive) sequence π(t), with π(t) ∼p(t), the online classification oracle
Rep has expected regret bounded by
class
E[Reg (T)]≤Est (T),
class class
where Est (T) is a known upper bound.
class
WeapplysuchanoraclewithinO2Lasfollows: attheendofeachiterationt∈[T]inO2L,wesamplek ∼[K]
uniformly, and update the classification oracle with the trajectory (x(t,k),a(t,k),r(t,k)),...,(x(t,k)a(t,k),r(t,k));
1 1 1 H H H
see the proof of Theorem 4.1 for details. We let Risk (TK) denote the risk of the O2L reduction when run
obs
for T epochs of K episodes, and we let Risk (K) := E[Risk(K,Alg ,M⋆ )] denote the expected risk of
⋆ lat lat
Alg when executed on M⋆ with access to the true latent states s =ϕ⋆(x ) for K episodes.
lat lat h h
Theorem 4.1 (Risk bound for O2L under hindsight observability). Let Alg be a base algorithm with base
lat
risk Risk (K), and Rep a representation learning oracle satisfying Assumption 4.2. Then Algorithm 1,
⋆ class
with inputs T,K,Φ, Rep , and Alg ,has expected risk
class lat
2K
E[Risk (TK)]≤Risk (K)+ Est (T).
obs ⋆ T class
This result shows that we can achieve sublinear risk under latent dynamics as long as (i) the base algorithm
achieves sublinear risk Risk (K) given access to the true latent states, and (ii) the classification oracle
⋆
achieves sublinear regret Est (T). Notably, the result is fully modular, meaning we require no explicit
class
conditions on the latent dynamics or the base algorithm, and is computationally efficient whenever the base
algorithm and classification oracle are efficient.
To make Theorem 4.1 concrete, we next provide a representation learning oracle (ExpWeights.Dr; Algo-
rithm 3 in Appendix F.1) based on a derandomization of the classical exponential weights mechanism, which
10satisfies Assumption 4.2 with Est ≲Hlog|Φ| whenever it has access to a class Φ that satisfies decoder
class
realizability.
Lemma 4.1 (Online classification via ExpWeights.Dr). Under decoder realizability (ϕ⋆ ∈ Φ), Exp-
Weights.Dr (Algorithm 3) satisfies Assumption 4.2 with8
Est class(T)=O(cid:101)(Hlog|Φ|).
Instantiating Theorem 4.1 with the above representation learning oracle, we obtain the following algorithmic
modularity result.
Corollary 4.1 (Algorithmic modularity under hindsight observability). For any base algorithm Alg ,
lat
under decoder realizability (ϕ⋆ ∈Φ), O2L with inputs T,K,Φ,ExpWeights.Dr, and Alg achieves
lat
HKlog|Φ|
E[Risk (TK)]≲Risk (K)+ .
obs ⋆ T
Consequently, for any Alg , setting T ≈KHlog|Φ|/Risk (K) achieves E[Risk (TK)]≲Risk (K) with
lat ⋆ obs ⋆
a number of trajectories TK =O(cid:101)(K2Hlog|Φ|/Risk⋆(K)).
Beyond achieving algorithmic modularity, this result shows that under hindsight observability, we can achieve
strongstatisticalmodularity(modulopossibleH factors)forevery baseMDPclassM , animportantresult
lat
in its own right.9 As an example, suppose that Risk (K)=O(K−1/2), which is satisfied by many standard
⋆
algorithms of interest [JKALS17; JYWJ20; JLM21; FKQR21]. Then, setting T according to Corollary 4.1
obtains an expected risk bound of ε using O(Hlog|Φ|/ε5) trajectories.
Remark 4.1 (Online versus offline oracles). Theorem 4.1 critically uses that assumption that Rep
class
satisfies an online classification error bound to handle the fact that data is generated adaptively based on
the estimators ϕ(cid:98)(1),...,ϕ(cid:98)(T) it produces, which is by now a relatively standard technique in the design of
interactive decision making algorithms [FR20; FKQR21; FR23]. We note that under coverability and other
exploration conditions, online oracles for classification can be directly obtained from offline (i.e. supervised)
classification oracles [XFBJK23; BRS24; FHQR24].
4.3 Algorithmic modularity via self-predictive estimation
In this section, we remove the assumption of hindsight observability used in Section 4.2 and instantiate O2L
in the general online RL setting. Rather than assume access to additional side-information, we adopt a
model-based representation learning approach, and augment our ability to perform representation learning by
equipping the representation learning algorithm with a set of base MDPs M in addition to the decoder
lat
class Φ. We will learn a representation by jointly fitting a decoder and the base (latent) dynamics, which is a
common approach in practice [GKBNB19; HLBN19; Haf+19; HLNB21; Sch+20; SAGHCB20; Guo+22].We
firstly present in Section 4.3.1 a new notion of optimistic self-predictive regret which combines self-predictive
representation learning with a form of optimism over a learned latent model. We then show in Section 4.3.2
that any representation learning oracle that attains low regret, when used within O2L (Algorithm 1), leads
to observable-to-latent reductions that ensure low risk for any base algorithm Alg , thereby achieving
lat
algorithmic modularity. Lastly, in Section 4.3.3, we instantiate this oracle under natural structural and
function approximation conditions, yielding end-to-end modularity and sample complexity guarantees.
4.3.1 Self-predictive estimation
Our self-predictive representation learning oracles learn to fit a representation ϕ such that the induced latent
transitions (ϕ (x ) to ϕ (x )) can be accurately modeled by some base (latent) MDP M ∈M . To
h h h+1 h+1 lat lat
describe the objective, let us first introduce some notation. For a given MDP M over either S (resp. X), we
8Inthissection,thenotationsO(cid:101),≈,and≲ignoreonlyconstantsandlogarithmicfactorsofH.
9Formally,whilewehavedefinedthestatisticalmodularityconditionintermsofhigh-probability riskbounds,itisstraightfor-
wardtoextendittoinsteadconsiderexpected riskbounds.
11write M (r ,s |s ,a ) (resp. M (r ,x |x ,a )) for the joint conditional distribution over rewards
h h h+1 h h h h h+1 h h
and next states. Next, for any ϕ∈Φ, we define the pushforward model for M⋆ induced by ϕ via:
obs,h
(cid:2) ϕ ♯M⋆ (cid:3) (r,s′ |x,a):= (cid:88) M⋆ (r,x′ |x,a). (7)
h+1 obs,h obs,h
x′:ϕh+1(x′)=s′
The pushforward model for ϕ captures the forward probability of the estimated latent state ϕ(x′) given a
current observation x. To measure distance between models, we will use squared Hellinger distance (e.g,
(cid:113) (cid:113)
Foster et al. [FKQR21]), defined via D2(P,Q) = (cid:82)(cid:0) dP − dQ(cid:1)2 dν for a common dominating measure
H dν dν
ν. Then, for a base model M and a decoder ϕ, the self-predictive error of (M ,ϕ), at state-action pair
lat lat
x ,a , is given by
h h
[∆ (M ,ϕ)](x ,a ):=D2(cid:0) M (ϕ (x ),a ),(cid:2) ϕ ♯M⋆ (cid:3) (x ,a )(cid:1) .
h lat h h H lat,h h h h h+1 obs,h h h
This term captures the ability of M (ϕ (x ),a ) to predict the next latent state ϕ (x ) which is
obtained by the pushforward modella(cid:2)t, ϕh h ♯Mh ⋆ h (cid:3) (x ,a ). Formally, in our model-bah s+ e1 d rh e+ p1 resentation
h+1 obs,h h h
learning setup, we consider oracles which, for each iteration t within O2L, take as input the trajectories
collected so far and produce an estimate (M(cid:99)(t),ϕ(cid:98)(t)) for the decoder and base model. The representation
lat
learning oracle’s self-predictive regret, for the sequence (M(cid:99)(t),ϕ(cid:98)(t)), is then defined as
lat
T H
Reg
self(T)=(cid:88)(cid:88)
E
π(t)∼p(t)Eπ(t)(cid:104)
[∆ h(M(cid:99) l( at) t,ϕ(cid:98)(t))](x h,a
h)(cid:105)
,
t=1h=0
where p(t) represents a randomization distribution over the policy π(t).
On its own, minimizing this regret may lead to degenerate solutions, a widely observed phenomenon in
practice [Tan+23]. For example, in a standard combination lock MDP (e.g., Agarwal et al.; Misra et al.
[AJKS22; MHKL20]), a degenerate decoder-model pair that maps all observations to a single latent state
will have zero self-predictive loss until we reach the goal, which can take exponentially long.10 We address
this via the notion of optimistic estimation used in Zhang; Foster et al. [Zha22; FGQRS23], which biases the
objective towards latent models with high return. This leads to the following optimistic self-predictive regret,
defined for a parameter γ >0, via
T H
Reg self;opt(T,γ)=(cid:88)(cid:88) E π(t)∼p(t)Eπ(t)(cid:104) [∆ h(M(cid:99) l( at) t,ϕ(cid:98)(t))](x h,a h)(cid:105) +γ−1(JM l⋆ at(π
M l⋆
at)−JM(cid:99)l( at t) (π
M(cid:99)l( at
t))). (8)
t=1h=0
We assume going forward that Rep obtains low optimistic self-predictive regret; in Section 4.3.3 we
self;opt
provide a maximum-likelihood-type estimator and conditions under which this holds.
Assumption 4.3. For a parameter γ > 0 and any (possibly adaptive) sequence π(t), with π(t) ∼ p(t), the
online representation learning oracle Rep
self;opt
is proper (i.e. outputs M(cid:99) l( at)
t
∈ M
lat
for all t ∈ [T]) and
satisfies
E(cid:2)
Reg
(T,γ)(cid:3)
≤Est (T,γ),
self;opt self;opt
where Est (T,γ) is a known upper bound.
self;opt
We note that only the decoder ϕ(cid:98)(t) is used within O2L; the model M(cid:99)(t) is only used for analysis (and possibly
lat
within the representation learner Rep ).
self;opt
10Thisissimilartotheobservationthatnaivevaluefunctionapproximationmethods,suchasFittedQ-Iteration,canfailto
exploreinonlineRLwithoutoptimism. Weexpectthatgivenaccesstoadditionalexploratorydata(e.g.,intheHybrid RL
settingofSongetal.[SZSBKS23]),thelatentoptimismtermcanberemoved.
124.3.2 Main result
WenowstatethemainguaranteeforO2Lwithself-predictiverepresentationlearning. RecallthatRisk (TK)
obs
denotes the risk of the O2L reduction. Compared to the hindsight-observable setting, we require a slightly
strongerperformanceguaranteefromthebasealgorithmAlg : ourresultscaleswiththeworst-case expected
lat
risk for Alg over all M ∈M , defined via Risk (K):= sup E[Risk(K,Alg ,M ))].
lat lat lat base Mlat∈Mlat lat lat
Theorem4.2(RiskboundforO2Lunderself-predictiveestimation). Suppose Rep satisfiesAssumption
self;opt
4.3 with parameter γ >0. Then Algorithm 1, with inputs T,K,Φ, Rep , and Alg has expected risk
self;opt lat
K
E[Risk (TK)]≤c ·Risk (K)+c γ· Est (T,γ)+c γ−1·KH,
obs 1 base 2 T self;opt 3
for absolute constants c ,c ,c >0.
1 2 3
Theorem 4.2 achieves sublinear risk as long as (i) the latent algorithm achieves sublinear risk Risk (K)
base
given access to the true states, and (ii) the self-predictive representation learning oracle achieves sublinear
regret Est (T,γ) for an appropriate choice of γ.11 Intuitively, our result scales with Risk (K) instead
self;opt base
of Risk (K) due to potential symmetries in the self-predictive objective. For example, there might be a
⋆
representation-model pair (M(cid:99)lat,ϕ(cid:98)) that is identical to (M l⋆ at,ϕ⋆) up to permutations of the latent state
space; these cannot be distinguished by a representation learning oracle that does not observe the latent
states directly, and thus the base algorithm may be tasked with solving either of these base MDPs. As with
Theorem 4.1, this result achieves algorithmic modularity (since O2L inherits the risk of the base algorithm),
and is computationally efficient whenever the base algorithm and self-predictive representation learning oracle
are efficient.
Let us provide some intuition behind the proof of Theorem 4.2. Recall that, within the inner loop of O2L,
the latent algorithm Alg
lat
interacts with the ϕ(cid:98)(t)-compressed dynamics generated by compressing the
observations x h,a
h
through the current decoder ϕ(cid:98)( ht) (Line 8). The crux of the analysis is the following
observation: by the self-predictive representation learning guarantee, these dynamics, despite being possibly
non-Markovian and generated from a POMDP (Definition H.1), are well approximated in squared Hellinger
distance by the base model M(cid:99) l( at)
t
estimated by Rep
self;opt
(cf. Lemma H.2). We can then show that Alg lat,
when given data from the ϕ(cid:98)(t)-compressed dynamics, has risk (for solving M(cid:99)(t)) that is proportional to: i) its
lat
base risk if it were to observe states from M(cid:99)(t), and ii) the Hellinger distance between M(cid:99)(t) and the process
lat lat
inducedbyitsϕ(cid:98)(t)-compresseddynamics. ThelastingredientistheuseoflatentoptimisminEq. (8), through
which the risk on M⋆ is upper bounded by the risk on M(cid:99)(t).
lat lat
In the above, showing that Alg
lat
obtains low risk for M(cid:99) l( at)
t
(despite given data from a different process)
is done by establishing a certain form of corruption robustness (Definition H.2). Indeed, Theorem 4.2 is a
special case of a more general theorem (Theorem G.1), which provides a bound that adapts to Alg ’s level
lat
of robustness. We obtain Theorem 4.2 by showing that any algorithm satisfies the property we require (for
a suitably slow rate), but we further show that tighter rates can be achieved by analyzing the specifics of
various algorithms of interest (Appendix H.1.4).
4.3.3 Instantiating the self-predictive estimation oracle
Wenowpresentanalgorithm,SelfPredict.Opt(Algorithm4inAppendixG.1),whichsatisfiesAssumption
4.3underadditionaltechnicalconditions,allowingustoinstantiateTheorem4.2togiveend-to-endguarantees.
Before stating the main guarantee, we highlight a few technical difficulties regarding obtaining finite-sample
guarantees for (online) self-predictive estimation, and use them to motivate our statistical assumptions and
algorithm design.
The statistics of (online) self-predictive estimation. The first challenge is a realizability issue: when
ϕ̸=ϕ⋆, we may not even be able to represent the objective ϕ♯M⋆ as a latent model using only decoder and
obs
11For example, in our estimator of Section 4.3.3, we can first set γ ≈ KH/Riskbase(K) so that the third term matches
Riskbase(K),andthensetT sothatthesecondtermdoes.
13latent model realizability. Since we can never guarantee that ϕ=ϕ⋆ exactly in the presence of statistical
errors, we must introduce a modelling assumption which lets us capture the pushforward models ϕ♯M⋆ . To
obs
this end, we recall the mismatch functions of Definition 3.4, which are defined via
Γ (s′ |s ):=P (ϕ (x )=s′).
ϕ,h h h xh∼ψ h⋆(sh) h h h
In the context of self-prediction, we show that the following mismatch completeness assumption suffices to
capture the pushforward models ϕ♯M⋆ .
obs
Assumption 4.4 (Mismatch completeness). We have a model class L such that, for each ϕ ∈ Φ, and
M ∈M , we have Γ ◦M ∈L, where
lat lat ϕ lat
(cid:88)
[Γ ◦M ] (r ,s |s ,a ):= M (r ,s′ |s ,a )Γ (s |s′ ).
ϕ lat h h h+1 h h lat,h h h+1 h h ϕ,h+1 h+1 h+1
s′ ∈S
h+1
In particular, Lemma C.8 establishes that
[ϕ ♯M⋆ ](·|x,a)=[Γ ◦M⋆ ] (·|ϕ⋆(x),a).
h+1 obs,h ϕ lat h h
Accordingly, we view this assumption as a minimal way to realize the pushforward models ϕ♯M⋆ .
obs
The second challenge is a double-sampling issue, which appears because the decoders in Eq. (8) are coupled
at different horizons. We address this with a novel “debiased” maximum likelihood procedure that subtracts
a form of excess risk (cf. Eq. (59)) to recover an unbiased estimator [Jia24]. Our debiased estimator and
the mismatch completeness assumption can be viewed as analogous to the techniques and assumptions that
are required for squared Bellman error minimization in the context of value function approximation [CJ19;
JLM21].
The last issue stems from seeking an online estimation guarantee: the policies chosen by the latent algorithm
are a function of the estimated decoders, which precludes the use of randomized estimators (e.g. exponential
weights). We bypass this issue by appealing to the structural condition of coverability [XFBJK23], which
allows us to restrict our attention to estimators that achieve low offline estimation error (via Lemma B.7).12
Definition 4.1 (State Coverability ). The state coverability coefficient for an MDP M and a policy class Π
defined over a state space Z, C (M,Π), is given by
cov,st
(cid:26) dM,π(z)(cid:27)
C (M,Π):= max min maxmax h . (9)
cov,st
h∈[H]µ∈∆(Z)π∈Π z∈Z µ(z)
We require coverability in M⋆ over the set of (observation-space) policies played by the O2L reduction (cf.
obs
Line 7). Again appealing to the mismatch functions, we can express this as an assumption about the base
dynamics M⋆ ; we show (Lemma C.1) that the latter is equivalent to assuming coverability in M⋆ over the
lat lat
set of stochastic policies
(cid:40) (cid:41)
(cid:88)
Γ ◦Π := [Γ ◦π ] (a|s)= Γ (s′ |s)π (a|s′)|ϕ∈Φ,π ∈Π , (10)
Φ lat ϕ lat h ϕ,h lat,h lat lat
s′∈S
where Π denotes the set of policies that Alg may execute. While this set may appear complicated, it is
lat lat
sufficient to assume coverability over the set of all deterministic non-stationary policies on M⋆ .13
lat
12Moregenerally,weexpectthatourresultscanbeextendedtoany“decouplingcoefficient” [Zha22;AZ22].
13ThisfollowsfromLemmaC.3bynotingthateachmaximumontherighthandsideofEq. (14)isattainedbyadeterministic
non-stationarypolicy.
14Guarantee for our self-predictive estimation oracle. With these prerequisites, the main guarantee for
our estimator, SelfPredict.Opt (Algorithm 4), is as follows.
Lemma 4.2 (Optimisticself-predictiveestimationviaSelfPredict.Opt). Let Π denote the set of policies
lat
played by Alg , and C =C (M⋆ ,Γ ◦Π ) be the state coverability parameter on M⋆ over the
lat cov,st cov,st lat Φ lat lat
set of stochastic policies Γ ◦Π (Eq. (10)). Then, for any γ >0, under decoder realizability (ϕ⋆ ∈Φ), base
Φ lat
model realizability (M⋆ ∈M ), and mismatch function completeness with class L (Assumption 4.4), the
lat lat lat
estimator in Algorithm 4 with inputs Φ,M ,L , and γ satisfies Assumption 4.3 with14
lat lat
(cid:18)(cid:113) (cid:19)
Est self;opt(T,γ)=O(cid:101) HC cov,st|A|T log(|M lat||L lat||Φ|) .
Instantiating Theorem 4.2 with the above representation learning oracle, we obtain the following algorithmic
modularity result.
Corollary4.2(AlgorithmicmodularityviaSelfPredict.Opt). UnderthesameconditionsasinLemma4.2,
and for any base algorithm Alg , O2L with inputs T,K,Φ,SelfPredict.Opt, and Alg achieves
lat lat
K (cid:113)
E[Risk (TK)]≲c ·Risk (K)+c γ· √ HC |A|log(|M ||L ||Φ|)+c γ−1·KH,
obs 1 base 2 cov,st lat lat 3
T
for absolute constants c ,c ,c . Consequently, for any Alg with base risk Risk (K), setting γ and T
1 2 3 lat base
appropriately gives
E[Risk (TK)]≲Risk (K),
obs base
with a number of trajectories TK =O(cid:101)(K5H3Ccov,st|A|log2(|Mlat||Llat||Φ|)/(Riskbase(K))4).
Forexample,if Alg isabasealgorithmwithRisk (K)=O(K−1/2),settingγ andT appropriatelygives
lat base
an expected risk of ε with a number of trajectories TK = O(cid:101)(cid:0) H3Ccov,st|A|(log(|Mlat||Llat||Φ|))2/ε14(cid:1). This result
shows that statistical modularity can be achieved up to log(|L |) factors for every base MDP class M
lat lat
which is subsumed by coverability, including tabular MDPs and low-rank MDPs.15 Compared to our positive
result for the case of pushforward coverability (Section 3.3), this imposes less dynamics assumptions (since
coverability is implied by pushforward coverability) but requires more representational assumptions (namely,
access to the mismatch-complete class L ). We further remark that the mismatch completeness assumption
lat
alwaysholdsfori)theBlockMDPsetting,sincewecanalwaysconstructL suchthatlog(|L |)=O(HS2),
lat lat
and ii) every MDP class M whenever we also have a realizable set of emission processes (ψ⋆ ∈ Ψ),
lat
since we can construct L such that log(|L |)=log(|Φ||M ||Ψ|). However, the mismatch completeness
lat lat lat
assumption may be more general than either of these settings.
Our results can be viewed as providing a theoretical justification for self-predictive representation learning,
which has been widely used in empirical works [GKBNB19; SAGHCB20]. We consider self-prediction’s ability
to obtain universal observable-to-latent reductions as a strong indicator that it merits further theoretical
study. In particular, many empirical works propose heuristics to alleviate the degeneracy/non-uniqueness
issues inherent with self-prediction [GKBNB19; SAGHCB20; HPBL23; Tan+23]. Our methods provide a
principled way to address these, and it would be interesting to investigate whether this is also empirically
effective. Ingeneral,however,itisunclearwhetherourlossadmitsacomputationallyefficientimplementation,
due to the presence of optimism. Towards this, a fascinating direction for future work is understanding how
self-predictive estimation can be used to obtain algorithmic modularity without the addition of optimism over
the base (latent) models.
5 Discussion
Our work initiates the study of statistical and algorithmic modularity for reinforcement learning under
general latent dynamics. Our positive and negative results serve as a first step toward a unified theory for
reinforcement learning in the presence of high-dimensional observations. To this end, we close with some
important future directions and open problems.
14Inthissection,thenotationsO(cid:101)and≲ignoresconstantsandlogarithmicfactorsof: H,Ccov,st,|A|,T,andlog(|Mlat||Llat||Φ|).
15Thisprovidesapartialanswertothe“ModelClass+Coverability” openquestionofFigure1.
15Statistical modularity. CanweobtainaunifiedcharacterizationforthestatisticalcomplexityofRLunder
latent dynamics with a given class of base MDPs M ? Our results in Section 3 suggest that this will require
lat
new tools that go beyond existing notions of statistical complexity. Toward resolving this problem, concrete
questions that are not yet understood include: (i) Is coverability [XFBJK23] (as opposed to pushforward
coverability) sufficient for learnability under latent dynamics? (ii) Is the Exogenous Block MDP problem
[EMKAL22; MFR24]—a special case of our general framework—statistically tractable? Lastly, are there
additional types of feedback that are weaker than hindsight observability, yet suffice to bypass the hardness
results in Section 3?
Algorithmic modularity. Canwederiveaunifiedrepresentationlearningobjectivethatenablesalgorithmic
modularity whenever statistical modularity is possible? Ideally, such an objective would be computationally
tractable. Alternatively, can we show that algorithmic modularity fundamentally requires stronger modeling
assumptions than statistical modularity? Toward addressing the problems above, a first step might be to
understand: (i)Whataretheminimalstatisticalassumptionsunderwhichwecanminimizetheself-predictive
objective in Section 4.3? (ii) How can we encourage finding good representations via self-prediction beyond
the use of optimism over the base (latent) models; and (iii) When can we minimize self-prediction in a
computationally efficient fashion?
Acknowledgements
Nan Jiang acknowledges funding support from NSF IIS-2112471, NSF CAREER IIS-2141781, Google Scholar
Award, and Sloan Fellowship.
16References
[ACK24] Philip Amortila, Tongyi Cao, and Akshay Krishnamurthy. “Mitigating Covariate Shift in
Misspecified Regression With Applications to Reinforcement Learning”. In: Conference on
Learning Theory. 2024.
[AFJSX24] PhilipAmortila,DylanJ.Foster,NanJiang,AyushSekhari,andTengyangXie.“Harnessing
DensityRatiosforOnlineReinforcementLearning”.In:InternationalConferenceonLearning
Representations. 2024.
[AFK24] Philip Amortila, Dylan J Foster, and Akshay Krishnamurthy. “Scalable Online Exploration
via Coverability”. In: International Conference on Machine Learning. 2024.
[AHKLLS14] Alekh Agarwal, Daniel Hsu, Satyen Kale, John Langford, Lihong Li, and Robert Schapire.
“Taming the monster: A fast and simple algorithm for contextual bandits”. In: International
Conference on Machine Learning. 2014.
[AJKS22] Alekh Agarwal, Nan Jiang, Sham M Kakade, and Wen Sun. Reinforcement learning: Theory
and algorithms. https://rltheorybook.github.io/. Version: January 31, 2022. 2022.
[AJSWY20] Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, and Lin Yang. “Model-Based
Reinforcement Learning with Value-Targeted Regression”. In: International Conference on
Machine Learning. 2020.
[AKKS20] AlekhAgarwal,ShamKakade,AkshayKrishnamurthy,andWenSun.“FLAMBE:Structural
Complexity and Representation Learning of Low Rank MDPs”. In: Neural Information
Processing Systems. 2020.
[AOM17] Mohammad Gheshlaghi Azar, Ian Osband, and Rémi Munos. “Minimax Regret Bounds for
Reinforcement Learning”. In: International Conference on Machine Learning. 2017.
[AZ22] Alekh Agarwal and Tong Zhang. “Model-based RL with Optimistic Posterior Sampling:
Structural Conditions and Sample Complexity”. In: Neural Information Processing Systems.
2022.
[Bak+22] BowenBaker,IlgeAkkaya,PeterZhokov,JoostHuizinga,JieTang,AdrienEcoffet,Brandon
Houghton, Raul Sampedro, and Jeff Clune. “Video PreTraining (VPT): Learning to Act by
Watching Unlabeled Online Videos”. In: Neural Information Processing Systems. 2022.
[BLM13] Stéphane Boucheron, Gábor Lugosi, and Pascal Massart. Concentration inequalities: A
nonasymptotic theory of independence. Oxford university press, 2013.
[Bro+22] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea
Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz,
Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil J Joshi, Ryan Julian,
Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu,
Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn
Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia
Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone,
Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao,
Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. “RT-1: Robotics Transformer for
Real-World Control at Scale”. In: arXiv:2212.06817. 2022.
[BRS24] Adam Block, Alexander Rakhlin, and Abhishek Shetty. “On the Performance of Empirical
Risk Minimization with Smoothed Data”. In: Conference on Learning Theory. 2024.
[CBL06] Nicolo Cesa-Bianchi and Gábor Lugosi. Prediction, Learning, and Games. Cambridge
university press, 2006.
[CJ19] JinglinChenandNanJiang.“Information-TheoreticConsiderationsinBatchReinforcement
Learning”. In: International Conference on Machine Learning. 2019.
[DKJADL19] Simon Du, Akshay Krishnamurthy, Nan Jiang, Alekh Agarwal, Miroslav Dudik, and John
Langford. “Provably Efficient RL With Rich Observations via Latent State Decoding”. In:
International Conference on Machine Learning. 2019.
17[DMKV21] Omar Darwiche Domingues, Pierre Ménard, Emilie Kaufmann, and Michal Valko. “Episodic
ReinforcementLearninginFiniteMDPs:MinimaxLowerBoundsRevisited”.In:Algorithmic
Learning Theory. 2021.
[DMRY20] Sarah Dean, Nikolai Matni, Benjamin Recht, and Vickie Ye. “Robust Guarantees for
Perception-Based Control”. In: Learning for Dynamics and Control. 2020.
[DR21] Sarah Dean and Benjamin Recht. “Certainty Equivalent Perception-Based Control”. In:
Learning for Dynamics and Control. 2021.
[Du+21] Simon S Du, Sham M Kakade, Jason D Lee, Shachar Lovett, Gaurav Mahajan, Wen Sun,
and Ruosong Wang. “Bilinear Classes: A Structural Framework for Provable Generalization
in RL”. In: International Conference on Machine Learning. 2021.
[DVRZ19] Shi Dong, Benjamin Van Roy, and Zhengyuan Zhou. “Provably Efficient Reinforcement
Learning With Aggregated States”. In: arXiv:1912.06366. 2019.
[EFMKL22] Yonathan Efroni, Dylan J Foster, Dipendra Misra, Akshay Krishnamurthy, and John Lang-
ford. “Sample-Efficient Reinforcement Learning in the Presence of Exogenous Information”.
In: Conference on Learning Theory. 2022.
[EMKAL22] Yonathan Efroni, Dipendra Misra, Akshay Krishnamurthy, Alekh Agarwal, and John
Langford. “Provable RL With Exogenous Distractors via Multistep Inverse Dynamics”. In:
International Conference on Learning Representations. 2022.
[FGH23] DylanJFoster,NoahGolowich,andYanjunHan.“TightGuaranteesforInteractiveDecision
Making with the Decision-Estimation Coefficient”. In: Conference on Learning Theory. 2023.
[FGQRS23] Dylan J Foster, Noah Golowich, Jian Qian, Alexander Rakhlin, and Ayush Sekhari. “Model-
Free Reinforcement Learning with the Decision-Estimation Coefficient”. In: Neural Informa-
tion Processing Systems. 2023.
[FHQR24] Dylan J Foster, Yanjun Han, Jian Qian, and Alexander Rakhlin. “Online Estimation via
Offline Estimation: An Information-Theoretic Framework”. In: arXiv:2404.10122. 2024.
[FKQR21] Dylan J Foster, Sham M Kakade, Jian Qian, and Alexander Rakhlin. “The Statistical
Complexity of Interactive Decision Making”. In: arXiv:2112.13487. 2021.
[FR20] Dylan J Foster and Alexander Rakhlin. “Beyond UCB: Optimal and Efficient Contextual
Bandits With Regression Oracles”. In: International Conference on Machine Learning. 2020.
[FR23] Dylan J Foster and Alexander Rakhlin. “Foundations of Reinforcement Learning and
Interactive Decision Making”. In: arXiv:2312.16730. 2023.
[FWYDY20] Fei Feng, Ruosong Wang, Wotao Yin, Simon S Du, and Lin Yang. “Provably Efficient Explo-
ration for Reinforcement Learning Using Unsupervised Learning”. In: Neural Information
Processing Systems. 2020.
[GCWXWB24] Jiacheng Guo, Minshuo Chen, Huan Wang, Caiming Xiong, Mengdi Wang, and Yu Bai.
“Sample-Efficient Learning of POMDPs with Multiple Observations In Hindsight”. In:
International Conference on Learning Representations. 2024.
[Gee00] S. A. van de Geer. Empirical Processes in M-Estimation. Cambridge University Press, 2000.
[GKBNB19] Carles Gelada, Saurabh Kumar, Jacob Buckman, Ofir Nachum, and Marc G Bellemare.
“DeepMDP: Learning Continuous Latent Space Models for Representation Learning”. In:
International Conference on Machine Learning. 2019.
[GMR24] Noah Golowich, Ankur Moitra, and Dhruv Rohatgi. “Exploring and Learning in Sparse
Linear MDPs Without Computationally Intractable Oracles”. In: Symposium on Theory of
Computing. 2024.
[Guo+22] Zhaohan Guo, Shantanu Thakoor, Miruna Pîslar, Bernardo Avila Pires, Florent Altché,
Corentin Tallec, Alaa Saade, Daniele Calandriello, Jean-Bastien Grill, Yunhao Tang, Michal
Valko, Rémi Munos, Mohammad Gheshlaghi Azar, and Bilal Piot. “BYOL-Explore: Explo-
ration by Bootstrapped Prediction”. In: Neural Information Processing Systems. 2022.
18[Haf+19] DanijarHafner,TimothyLillicrap,IanFischer,RubenVillegas,DavidHa,HonglakLee,and
James Davidson. “Learning Latent Dynamics for Planning From Pixels”. In: International
Conference on Machine Learning. 2019.
[HLBN19] DanijarHafner, Timothy Lillicrap,JimmyBa,and MohammadNorouzi. “DreamtoControl:
Learning Behaviors by Latent Imagination”. In: International Conference on Learning
Representations. 2019.
[HLNB21] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. “Mastering Atari
With Discrete World Models”. In: International Conference on Learning Representations.
2021.
[HLSW21] Botao Hao, Tor Lattimore, Csaba Szepesvári, and Mengdi Wang. “Online Sparse Rein-
forcement Learning”. In: International Conference on Artificial Intelligence and Statistics.
2021.
[HPBL23] Danijar Hafner, Jurgis Pasukonis, Jimmy Ba, and Timothy Lillicrap. “Mastering Diverse
Domains Through World Models”. In: arXiv:2301.04104. 2023.
[Jia24] Nan Jiang. “A Note on Loss Functions and Error Compounding in Model-based Reinforce-
ment Learning”. In: arXiv:2404.09946. 2024.
[JKALS17] Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, John Langford, and Robert E Schapire.
“Contextual Decision Processes With Low Bellman Rank Are PAC-Learnable”. In: Interna-
tional Conference on Machine Learning. 2017.
[JLM21] ChiJin,QinghuaLiu,andSobhanMiryoosefi.“BellmanEluderDimension:NewRichClasses
of RL Problems, and Sample-Efficient Algorithms”. In: Neural Information Processing
Systems. 2021.
[JYWJ20] Chi Jin, Zhuoran Yang, Zhaoran Wang, and Michael I Jordan. “Provably Efficient Reinforce-
ment Learning With Linear Function Approximation”. In: Conference on Learning Theory.
2020.
[KAL16] Akshay Krishnamurthy, Alekh Agarwal, and John Langford. “PAC Reinforcement Learning
With Rich Observations”. In: Neural Information Processing Systems. 2016.
[KFPM21] Ashish Kumar, Zipeng Fu, Deepak Pathak, and Jitendra Malik. “RMA: Rapid Motor
Adaptation for Legged Robots”. In: Robotics: Science and Systems. 2021.
[LADZ23] Jonathan Lee, Alekh Agarwal, Christoph Dann, and Tong Zhang. “Learning in POMPDs Is
Sample-Efficient With Hindsight Observability”. In: International Conference on Machine
Learning. 2023.
[Lam+24] Alex Lamb, Riashat Islam, Yonathan Efroni, Aniket Rajiv Didolkar, Dipendra Misra,
Dylan J Foster, Lekan P Molu, Rajan Chari, Akshay Krishnamurthy, and John Langford.
“GuaranteedDiscoveryofControl-EndogenousLatentStateswithMulti-StepInverseModels”.
In: Transactions on Machine Learning Research. 2024.
[LFDA16] Sergey Levine, Chelsea Finn, Trevor Darrell, and Pieter Abbeel. “End-To-End Training of
Deep Visuomotor Policies”. In: The Journal of Machine Learning Research. 2016.
[Li09] Lihong Li. A Unifying Framework for Computational Reinforcement Learning Theory.
Rutgers, The State University of New Jersey, 2009.
[LS20] Tor Lattimore and Csaba Szepesvári. Bandit Algorithms. Cambridge University Press, 2020.
[LSA20] Michael Laskin, Aravind Srinivas, and Pieter Abbeel. “CURL: Contrastive Unsupervised
Representations for Reinforcement Learning”. In: International Conference on Machine
Learning. 2020.
[LXJZV24] Michael Lanier, Ying Xu, Nathan Jacobs, Chongjie Zhang, and Yevgeniy Vorobeychik.
“Learning Interpretable Policies in Hindsight-Observable POMDPs through Partially Super-
vised Reinforcement Learning”. In: arXiv:2402.09290. 2024.
[MBFR23] ZakMhammedi,AdamBlock,DylanJFoster,andAlexanderRakhlin.“EfficientModel-Free
Exploration in Low-Rank MDPs”. In: Neural Information Processing Systems. 2023.
19[MCKJA24] AdityaModi,JinglinChen,AkshayKrishnamurthy,NanJiang,andAlekhAgarwal.“Model-
Free Representation Learning and Exploration in Low-Rank Mdps”. In: Journal of Machine
Learning Research. 2024.
[MFR23] ZakariaMhammedi,DylanJFoster,andAlexanderRakhlin.“RepresentationLearningWith
Multi-Step Inverse Kinematics: An Efficient and Optimal Approach to Rich-Observation
RL”. In: International Conference on Machine Learning. 2023.
[MFR24] ZakariaMhammedi,DylanJFoster,andAlexanderRakhlin.“ThePowerofResetsinOnline
Reinforcement Learning”. In: arXiv:2404.15417. 2024.
[Mha+20] Zakaria Mhammedi, Dylan J Foster, Max Simchowitz, Dipendra Misra, Wen Sun, Akshay
Krishnamurthy, Alexander Rakhlin, and John Langford. “Learning the Linear Quadratic
Regulator From Nonlinear Observations”. In: Neural Information Processing Systems. 2020.
[MHKL20] Dipendra Misra, Mikael Henaff, Akshay Krishnamurthy, and John Langford. “Kinematic
State Abstraction and Provably Efficient Rich-Observation Reinforcement Learning”. In:
International Conference on Machine Learning. 2020.
[MJTS20] Aditya Modi, Nan Jiang, Ambuj Tewari, and Satinder Singh. “Sample Complexity of
Reinforcement Learning Using Linearly Combined Model Ensembles”. In: International
Conference on Artificial Intelligence and Statistics. 2020.
[MLJL21] Dipendra Misra, Qinghua Liu, Chi Jin, and John Langford. “Provable Rich Observation
Reinforcement Learning With Combinatorial Latent States”. In: International Conference
on Learning Representations. 2021.
[Ni+24] Tianwei Ni, Benjamin Eysenbach, Erfan Seyedsalehi, Michel Ma, Clement Gehring, Aditya
Mahajan, and Pierre-Luc Bacon. “Bridging State and History Representations: Understand-
ing Self-Predictive RL”. In: arXiv:2401.08898. 2024.
[NRKFG22] Suraj Nair, Aravind Rajeswaran, Vikash Kumar, Chelsea Finn, and Abhinav Gupta. “R3M:
A Universal Visual Representation for Robot Manipulation”. In: arXiv:2203.12601. 2022.
[OVR16] Ian Osband and Benjamin Van Roy. “On Lower Bounds for Regret in Reinforcement
Learning”. In: arXiv:1608.02732. 2016.
[PAED17] Deepak Pathak, Pulkit Agrawal, Alexei A Efros, and Trevor Darrell. “Curiosity-Driven Ex-
ploration by Self-Supervised Prediction”. In: International Conference on Machine Learning.
2017, pp. 2778–2787.
[RH23] PhilippeRigolletandJan-ChristianHütter.“High-dimensionalstatistics”.In:arXiv:2310.19244.
2023.
[RVR13] Daniel Russo and Benjamin Van Roy. “Eluder Dimension and the Sample Complexity of
Optimistic Exploration”. In: Neural Information Processing Systems. 2013.
[SAGHCB20] Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon Hjelm, Aaron Courville, and Philip
Bachman. “Data-Efficient Reinforcement Learning with Self-Predictive Representations”. In:
International Conference on Learning Representations. 2020.
[Sch+20] Julian Schrittwieser, Ioannis Antonoglou, Thomas Hubert, Karen Simonyan, Laurent Sifre,
Simon Schmitt, Arthur Guez, Edward Lockhart, Demis Hassabis, Thore Graepel, Timothy
Lillicrap, and David Silver. “Mastering Atari, Go, Chess and Shogi by Planning With a
Learned Model”. In: Nature. 2020.
[SJKAL19] Wen Sun, Nan Jiang, Akshay Krishnamurthy, Alekh Agarwal, and John Langford. “Model-
Based RL in Contextual Decision Processes: PAC Bounds and Exponential Improvements
Over Model-Free Approaches”. In: Conference on Learning Theory. 2019.
[SLS23] Ming Shi, Yingbin Liang, and Ness Shroff. “Theoretical Hardness and Tractability of
POMDPs in RL with Partial Hindsight State Information”. In: arXiv:2306.08762. 2023.
[SWFK24] Yuda Song, Lili Wu, Dylan J Foster, and Akshay Krishnamurthy. “Rich-Observation Rein-
forcement Learning with Continuous Latent Dynamics”. In: International Conference on
Machine Learning. 2024.
20[SZSBKS23] Yuda Song, Yifei Zhou, Ayush Sekhari, J Andrew Bagnell, Akshay Krishnamurthy, and
Wen Sun. “Hybrid RL: Using Both Offline and Online Data Can Make RL Efficient”. In:
International Conference on Learning Representations. 2023.
[Tan+17] Haoran Tang, Rein Houthooft, Davis Foote, Adam Stooke, OpenAI Xi Chen, Yan Duan,
JohnSchulman,FilipDeTurck,andPieterAbbeel.“#Exploration:AStudyofCount-Based
Exploration for Deep Reinforcement Learning”. In: Neural Information Processing Systems.
2017.
[Tan+23] Yunhao Tang, Zhaohan Daniel Guo, Pierre Harvey Richemond, Bernardo Avila Pires, Yash
Chandak, Remi Munos, Mark Rowland, Mohammad Gheshlaghi Azar, Charline Le Lan,
ClareLyle,AndrásGyörgy,ShantanuThakoor,WillDabney,BilalPiot,DanieleCalandriello,
and Michal Valko. “Understanding Self-Predictive Learning for Reinforcement Learning”. In:
International Conference on Machine Learning. 2023.
[UZS22] Masatoshi Uehara, Xuezhou Zhang, and Wen Sun. “Representation Learning for Online
and Offline RL in Low-rank MDPs”. In: The Tenth International Conference on Learning
Representations. 2022.
[WAJAYJS21] Gellert Weisz, Philip Amortila, Barnabás Janzer, Yasin Abbasi-Yadkori, Nan Jiang, and
Csaba Szepesvári. “On Query-Efficient Planning in MDPs Under Linear Realizability of the
Optimal State-Value Function”. In: Conference on Learning Theory. 2021.
[WAS21] Gellért Weisz, Philip Amortila, and Csaba Szepesvári. “Exponential Lower Bounds for Plan-
ning in MDPs With Linearly-Realizable Optimal Action-Value Functions”. In: Algorithmic
Learning Theory. 2021.
[WSD15] Niklas Wahlström, Thomas B Schön, and Marc Peter Deisenroth. “From Pixels to Torques:
Policy Learning With Deep Dynamical Models”. In: International Conference on Machine
Learning. 2015.
[WSY20] RuosongWang,RussRSalakhutdinov,andLinYang.“ReinforcementLearningwithGeneral
ValueFunctionApproximation:ProvablyEfficientApproachviaBoundedEluderDimension”.
In: Neural Information Processing Systems. 2020.
[WYDW21] TianhaoWu,YunchangYang,SimonDu,andLiweiWang.“OnReinforcementLearningWith
Adversarial Corruption and Its Application to Block MDP”. In: International Conference
on Machine Learning. 2021.
[XFBJK23] Tengyang Xie, Dylan J Foster, Yu Bai, Nan Jiang, and Sham M Kakade. “The Role of
Coverage in Online Reinforcement Learning”. In: International Conference on Learning
Representations. 2023.
[XJ21] Tengyang Xie and Nan Jiang. “Batch Value-Function Approximation With Only Realizabil-
ity”. In: International Conference on Machine Learning. 2021.
[YFK21] Denis Yarats, Rob Fergus, and Ilya Kostrikov. “Image Augmentation Is All You Need:
Regularizing Deep Reinforcement Learning From Pixels”. In: International Conference on
Learning Representations. 2021.
[ZGS21] Dongruo Zhou, Quanquan Gu, and Csaba Szepesvari. “Nearly Minimax Optimal Reinforce-
ment Learning for Linear Mixture Markov Decision Processes”. In: Conference on Learning
Theory. 2021.
[Zha06] Tong Zhang. “From ϵ-entropy to KL-entropy: Analysis of minimum information complexity
density estimation”. In: The Annals of Statistics. Vol. 34. 5. Institute of Mathematical
Statistics, 2006, pp. 2180–2210.
[Zha22] Tong Zhang. “Feel-Good Thompson Sampling for Contextual Bandits and Reinforcement
Learning”. In: SIAM Journal on Mathematics of Data Science. 2022.
[ZMCGL21] Amy Zhang, Rowan McAllister, Roberto Calandra, Yarin Gal, and Sergey Levine. “Learn-
ing Invariant Representations for Reinforcement Learning Without Reconstruction”. In:
International Conference on Learning Representations. 2021.
21[ZSUWAS22] XuezhouZhang,YudaSong,MasatoshiUehara,MengdiWang,AlekhAgarwal,andWenSun.
“Efficient Reinforcement Learning in Block MDPs: A Model-Free Representation Learning
Approach”. In: International Conference on Machine Learning. 2022.
22Contents
A Additional Discussion of Related Work 24
B Technical Tools 25
C Structural Properties of Coverability and Mismatch Functions 26
D Proofs and Additional Results for Section 3.2: Impossibility Results 31
D.1 Additional Lower Bound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
D.2 Details for Figure 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
D.3 Proofs for Lower Bounds (Theorems 3.1 and D.1) . . . . . . . . . . . . . . . . . . . . . . . . . 37
E Proofs for Section 3.3: Positive Results 44
E.1 Proofs for Latent Model Class + Pushforward Coverability (Theorem 3.2) . . . . . . . . . . . 44
F Proofs and Additional Information for Section 4.2: Hindsight RL 55
F.1 Pseudocode and Proofs for ExpWeights.Dr (Lemma 4.1) . . . . . . . . . . . . . . . . . . . 55
F.2 Proofs for O2L Under Hindsight Observability (Theorem 4.1) . . . . . . . . . . . . . . . . . . 57
G Proofs for Section 4.3: Self-Predictive Estimation 59
G.1 Pseudocode and Proofs for SelfPredict.Opt (Lemma 4.2) . . . . . . . . . . . . . . . . . . 59
G.2 Proofs for Main Risk Bound (Theorem 4.2) . . . . . . . . . . . . . . . . . . . . . . . . . . . . 63
H Additional Results for Section 4.3: Self-Predictive Estimation 65
H.1 O2L with Self-predictive Estimation and CorruptionRobust Base Algorithms . . . . . . . . . 65
H.2 Proofs for Appendix H.1.2: Properties of ϕ-compressed POMDPs . . . . . . . . . . . . . . . . 68
H.3 Proofs for Appendix H.1.3: Risk Bound Under CorruptionRobustness (Theorem G.1) . . . . 73
H.4 Proofs for Appendix H.1.4: Examples of CorruptionRobust Algorithms . . . . . . . . . . . . 75
23A Additional Discussion of Related Work
In this section, we discuss aspects of related work not already covered in greater detail.
Reinforcement learning under latent dynamics (or, with rich observations). Reinforcement
learning under latent dynamics (or, with rich observations) has received extensive investigation in recent
years, however most works have been focused on the Block MDP model in which the latent state space
is tabular/finite [KAL16; DKJADL19; MHKL20; ZSUWAS22; MFR23] (see also the the closely related
framework of Low-Rank MDPs [AKKS20; MCKJA24; ZSUWAS22; UZS22; MBFR23]). Beyond tabular
spaces, Dean et al.; Dean et al.; Mhammedi et al. [DMRY20; DR21; Mha+20] consider continuous linear
dynamics, Misra et al. [MLJL21] considers factored (but discrete) latent dynamics, Efroni et al.; Efroni
et al.; Mhammedi et al. [EMKAL22; EFMKL22; MFR24] consider the Exogenous Block MDP problem in
which a tabular latent state space is augmented with a non-controllable (“exogenous”) factor, and Song et al.
[SWFK24] consider Lipshitz continuous dynamics. To our knowledge, our work is the first to: i) explore
reinforcement learning under general latent dynamics, in particular in settings where the latent space itself
admits function approximation, and ii) take a more modular approach (cf. the taxonomy of Section 3).
On the algorithmic side, the works of Uehara et al. [UZS22] and Zhang et al. [ZSUWAS22], which consider
Low-Rank MDPs and Block MDPs respectively, can be viewed as interleaving representation learning with
“latent” reinforcementlearningalgorithmsthatassumeaccesstoagoodrepresentation,andwereaninspiration
for this work. However, the algorithmic details and analyses are highly specialized to Block/Low-Rank MDPs,
and unlikely to be directly applicable to reinforcement learning under general latent dynamics. Other works
with a modular flavor include:
• Feng et al. [FWYDY20] solve tabular Block MDPs by combining a black-box latent algorithm with an
“unsupervised learning oracle” for representation learning. This approach only leads to guarantees for
tabular Block MDPs, and it is unclear whether the unsupervised learning oracle their approach requires
can be constructed in natural settings.
• Wu et al. [WYDW21] solve tabular block MDPs by combining a corruption-robust latent algorithm
with a representation learning procedure based on clustering. Again, this work is restricted to the
tabular setting, and requires a separation condition which may not be satisfied in general.
General complexity measures for reinforcement learning. Another line of research provides general
complexity measures that enable sample-efficient reinforcement learning, including Bellman rank [JKALS17;
SJKAL19; Du+21; JLM21], eluder dimension [RVR13], coverability [XFBJK23], and the Decision-Estimation
Coefficient (DEC) [FKQR21; FGH23; FGQRS23]. Bellman rank and other complexity measures based on
averageBellmanerror[JKALS17;SJKAL19;Du+21;JLM21]areinsufficienttocharacterizelearnabilityunder
general latent dynamics, as there are classes M that are known to be learnable, yet do not have bounded
lat
Bellman rank or Bellman-Eluder dimension [EMKAL22; XFBJK23]. Meanwhile, variants of Bellman rank
basedonsquaredBellmanerrororrelatednotionsoferrorcan[XFBJK23; AFJSX24]addressthisproblemfor
some settings, but satisfying the modeling/realizability assumptions (e.g., Bellman completeness) required by
these methods in the latent-dynamics setting is non-trivial. For example, the crux of our sample complexity
bounds under latent pushforward coverability in Section 3 (Theorem 3.2) is to prove a rather involved
structural result which shows that Bellman completeness can indeed be satisfied under this assumption, but
it is unclear whether these techniques can be applied to more general latent dynamics classes. We expect that
it is possible to bound the Decision-Estimation Coefficient [FKQR21; FGH23; FGQRS23] for the framework,
but deriving efficient algorithms using this framework is non-trivial.
24B Technical Tools
Lemma B.1. For any sequence of real-valued random variables (X ) adapted to a filtration (F ) , it
t t≤T t t≤T
holds that with probability at least 1−δ,
T T
(cid:88)
X
≤(cid:88) log(cid:0)E (cid:2) eXt(cid:3)(cid:1) +log(cid:0) δ−1(cid:1)
.
t t−1
t=1 t=1
Lemma B.2 (Freedman’s inequality (e.g., Agarwal et al. [AHKLLS14])). Let (X ) be a real-valued
t t≤T
martingale difference sequence adapted to a filtration (F ) . If |X | ≤ R almost surely, then for any
t t≤T t
η ∈(0,1/R), with probability at least 1−δ,
(cid:88)T
X
≤η(cid:88)T
E (cid:2) X2(cid:3) +
log(cid:0) δ−1(cid:1)
.
t t−1 t η
t=1 t=1
Lemma B.3 (CorollaryofLemmaB.2). Let (X ) be a sequence of random variables adapted to a filtration
t t≤T
(F ) . If 0≤X ≤R almost surely, then with probability at least 1−δ,
t t≤T t
T T
(cid:88) X ≤ 3(cid:88) E [X ]+4Rlog(cid:0) 2δ−1(cid:1) ,
t 2 t−1 t
t=1 t=1
and
T T
(cid:88) E [X ]≤2(cid:88) X +8Rlog(cid:0) 2δ−1(cid:1) .
t−1 t t
t=1 t=1
Lemma B.4 (Lemma D.2 of Foster et al. [FHQR24]). Let (X ,F ),...,(X ,F ) be a sequence of measurable
1 1 n n
spaces, and let X(i) = (cid:81)i X and F(i) = ⊗i F . For each i, let P(i) and Q(i) be probability kernels
t=1 t t=1 t
from (X(i−1),F(i−1)) to (X ,F ). Let P and Q be the laws of X ,...,X under X ∼ P(i)(· | X ) and
i i 1 n i 1:i−1
X ∼Q(i)(·|X ), respectively. Then it holds that
i 1:i−1
(cid:34) n (cid:35)
(cid:88)
D2(P,Q)≤7E D2(P(i)(·|X ),Q(i)(·|X ))
H P H 1:i−1 1:i−1
i=1
Lemma B.5 (Lemma A.11 of Foster et al. [FKQR21]). Let P and Q be probability measures on (X,F). For
all h:X →R with 0≤h(X)≤R almost surely under P and Q, we have
E P[h(X)]≤3E Q[h(X)]+4RD H2(P,Q).
Lemma B.6 (Lemma 1 of Jiang et al. [JKALS17]). For any f :X ×A→[0,1], π :S ×[H]→∆(A), we
have
H
(cid:88)
E [f(x ,π(x ))]−J(π)= Eπ[f(x ,a )−Tπf(x ,a )].
x1 1 1 h h h h
h=1
Lemma B.7 (Offline-to-online conversion under coverability [XFBJK23; FHQR24]). Let M be an MDP
over state space Z, Π be a policy set, and C =C (M,Π) be the (state-action) coverability coefficient for
cov cov
M and Π (Definition C.3). Let p(t) ∈∆(Π) be a sequence of distributions over Π, and g(t) :Z×A→[0,1] be
h
a sequence of functions. Then we have that
(cid:118) 
T H (cid:117) T H t−1
(cid:88)(cid:88) E π(t)∼p(t)Eπ(t)(cid:2) g h(t)(x h,a h)(cid:3) ≤O(cid:117) (cid:116)HC covlog(T)(cid:88)(cid:88)(cid:88) E π(i)∼p(i)Eπ(i)(cid:2) g h(t)(x h,a h)(cid:3) +HC cov.
t=1h=1 t=1h=1i=1
25C Structural Properties of Coverability and Mismatch Functions
This appendix contains structural results regarding coverability and the mismatch functions. We firstly recall
the definition of the mismatch functions.
Definition C.1 (Mismatch functions). For decodable emission process ψ⋆, decoder ϕ∈Φ and h∈[H], we
define the mismatch function for ϕ, Γ :S →∆(S), as the probability kernel
ϕ,h
Γ (s′ |s ):=P (ϕ (x )=s′).
ϕ,h h h xh∼ψ h⋆(sh) h h h
We also recall the definition of state coverability.
Definition C.2 (StateCoverability). The coverability coefficient for an MDP M and a policy class Π defined
over a state space Z, C (M,Π), is given by
cov,st
(cid:26) dM,π(z)(cid:27)
C (M,Π):= max min maxmax h . (11)
cov,st
h∈[H]µ∈∆(Z)π∈Π z∈Z µ(z)
We also define the related notion of state-action coverability.
Definition C.3 (State-Action Coverability). The coverability coefficient for an MDP M and a policy class
Π defined over a state space Z and action space A, C (M,Π), is given by
cov
(cid:26) dM,π(z,a)(cid:27)
C (M,Π):= max min max max h . (12)
cov
h∈[H]µ∈∆(Z×A)π∈Πz,a∈Z×A µ(z,a)
In the remainder of the section, we let Π ⊆{S×[H]→∆(A)} denote an arbitrary set of latent policies,
lat
and
(cid:40) (cid:41)
(cid:88)
Γ ◦Π = [Γ ◦π ] (a|s):= Γ (s′ |s)π (a|s′)|ϕ∈Φ,π ∈Π . (13)
Φ lat ϕ lat h ϕ,h lat,h lat lat
s′∈S
Lemma C.1 (State coverability is invariant to rich observations). Let M⋆ =⟪M⋆ ,ψ⋆⟫. Then, we have
obs lat
C (M⋆ ,Π ◦Φ)=C (M⋆ ,Γ ◦Π ).
cov,st obs lat cov,st lat Φ lat
Furthermore, letting {µ ∈∆(S)} denote the distribution which witnesses the right-hand-side, the
lat,h h∈[H]
left-hand-side is witnessed by the distribution
µ (x)=ψ⋆(x|ϕ⋆(x))µ (ϕ⋆(x)).
obs,h h h lat,h h
The lemma follows from the following two observations.
Lemma C.2. Let {Γ } denote the mismatch functions for emission ψ⋆, and let M = ⟪M ,ψ⋆⟫.
ϕ ϕ∈Φ obs lat
Then, for any π ∈Π , ϕ∈Φ, h∈[H], x∈X, we have
lat lat
dMobs,πlat◦ϕ(x)=ψ⋆(x|ϕ⋆(x))dMlat,Γϕ◦πlat(ϕ⋆(x)).
h h h h h
Proof of Lemma C.2. Below, we write s =ϕ⋆(x ). We proceed by induction, simply writing d :=
h h obs,h
dMobs,πlat◦ϕ and d :=dMlat,Γϕ◦πlat. Thebase case (h=1) is obtainedbynoting that d (s)=P (s|∅)
h lat,h h lat,1 lat,1
while d (x)=P (x|∅)=ψ⋆(x|s)P (s|∅). For the general case, via the Bellman flow equations,
obs,1 obs,1 1 lat,1
26we have
(cid:88)
d (x )= P (x |x ,a )d (x )π (a |ϕ(x ))
obs,h h obs,h h h−1 h−1 obs,h−1 h−1 lat h−1 h−1
xh−1,ah−1∈X×A
(cid:88)
=ψ(x |s ) P (s |s ,a )d (s )ψ(x |s )π (a |ϕ(x ))
h h lat,h h h−1 h−1 lat,h−1 h−1 h−1 h−1 lat h−1 h−1
xh−1,ah−1∈X×A
(cid:88)
=ψ(x |s ) P (s |s ,a )d (s )×
h h lat,h h h−1 h−1 lat,h−1 h−1
sh−1,ah−1∈S×A
 
(cid:88)
 ψ(x h−1 |s h−1)π lat(a h−1 |ϕ(x h−1)).
xh−1:ϕ⋆(xh−1)=sh−1
The result is obtained by noting that
(cid:88)
Γ ◦π (a |s )= Γ (s′ |s )π (a |s′)
ϕ lat h−1 h−1 ϕ h−1 lat h−1
s′∈S
(cid:88) (cid:88)
= ψ(x |s )I{ϕ(x )=s′}π (a |s′)
h−1 h−1 h−1 lat h
s′∈Sxh−1:ϕ⋆(xh−1)=sh−1
(cid:88)
= ψ(x |s )π (a |ϕ(x )),
h−1 h−1 lat h−1 h−1
xh−1:ϕ⋆(xh−1)=sh−1
where the second line follows from the definition of the mismatch functions.
Lemma C.3 (Equivalenceofstatecoverabilityandcumulativestatereachability). Let M be an MDP defined
over a state space Z. The following definition is equivalent to Definition C.2:
(cid:88)
C (M,Π):= max maxdM,π(z). (14)
cov,st h
h∈[H] π∈Π
z∈Z
Proof of Lemma C.3. StraightforwardadaptationoftheproofofLemma3fromXieetal.[XFBJK23].
Proof of Lemma C.1. Using Lemma C.2 and Lemma C.3, we have
(cid:88)
C (M ,Π ◦Φ)= max maxdπlat◦ϕ(x)
cov,st obs lat obs
h∈[H] x∈Xπlat,ϕ
= max
(cid:88) maxψ⋆(x|ϕ⋆(x))dΓϕ◦πlat(ϕ⋆(x))
lat
h∈[H] x∈Xπlat,ϕ
= max
(cid:88) (cid:88) maxψ⋆(x|s)dΓϕ◦πlat(s)
lat
h∈[H] s∈Sx:ϕ⋆(x)=sπlat,ϕ
= max (cid:88) maxdΓϕ◦πlat(s) (cid:88) ψ⋆(x|s)
lat
h∈[H] s∈Sπlat,ϕ
x:ϕ⋆(x)=s
=C (M ,Γ ◦Π ).
cov,st lat Φ lat
Lastly, we show that state-action coverability is bounded by state coverability times the size of the action set.
Lemma C.4 (State-action coverability bound). For any MDP M and policy set Π, we have
C (M,Π)≤C (M,Π)|A|.
cov cov,st
27Proof of Lemma C.4. Let µ ∈ ∆(Z) witness C (M,Π). Fix h ∈ [H], which we omit below for
s cov,st
cleanliness. Then, we have
(cid:26) dM,π(z,a)(cid:27) (cid:26) dM,π(z)π(a|z)(cid:27)
min max max ≤max max
µs,a∈∆(Z×A)π∈Πz,a∈Z×A µ s,a(z,a) π∈Πz,a∈Z×A µ s(z)1/|A|
(cid:26) dM,π(z)(cid:27)
≤|A|maxmax
π∈Π z∈Z µ s(z)
=C (M,Π)|A|.
cov,st
LemmaC.5(Pushforwardcoverabilityisinvarianttorichobservations). LetC (M)denotethepushforward
push
coverability parameter for an MDP M (Definition 3.3), and M⋆ :=⟪M⋆ ,ψ⋆⟫. Then, we have
obs lat
C (M⋆ )=C (M⋆ ).
push obs push lat
Furthermore, letting {µ ∈∆(S)} denote the distribution which witnesses the right-hand-side, the
lat,h h∈[H]
left-hand-side is witnessed by the distribution
µ (x)=ψ⋆(x|ϕ⋆(x))µ (ϕ⋆(x)).
obs,h h h lat,h h
Thisfollowsfromananalogousequivalenceofpushforwardcoverabilityandcumulativeconditional reachability.
Lemma C.6 (Equivalence of pushforward coverability and cumulative conditional reachability). Let M
be an MDP defined over a state space Z with transition kernel P. The following definition is equivalent to
pushforward coverability (Definition 3.3):
(cid:88)
C (M):= max max P (z′ |z,a).
push h
h∈[H] z,a∈Z×A
z′∈Z
Proof of Lemma C.6. Fix h∈[H], whose dependence we omit below. For the first direction, letting µ
denote the pushforward coverability distribution, we have:
(cid:88) (cid:88) P(z′ |z,a) (cid:88)
max P(z′ |z,a)= max µ(z′)≤C µ(z′)=C .
z,a∈Z×A z,a∈Z×A µ(z′) push push
z′∈Z z′∈Z z′∈Z
For the second direction, taking µ(z′)∝max P(z′ |z,a), we have
z,a
P(z′ |z,a) P(z′ |z,a) (cid:88) (cid:88)
min max ≤ max maxP(z˜′ |z˜,a˜)≤ maxP(z′ |z,a).
µ∈∆(Z)z,a,z′∈Z×A×Z µ(z′) z,a,z′∈Z×A×Z max z˜,a˜P(z′ |z˜,a˜) z˜,a˜ z,a
z˜′ z′
Proof of Lemma C.5. This result follows by Lemma C.6 since,
(cid:88)
C (M )= maxP (x′ |x,a)
push obs obs
x,a
x′∈X
(cid:88) (cid:88)
= maxψ⋆(x′ |s′)P (s′ |ϕ⋆(x),a)
lat
x,a
s′∈Sx′:ϕ⋆(x′)=s′
(cid:88) (cid:88)
= maxP (s′ |ϕ⋆(x),a) ψ⋆(x′ |s′)
lat
x,a
s′∈S x′:ϕ⋆(x′)=s′
(cid:88)
= maxP (s′ |s,a)=C (M ).
lat push lat
s,a
s′∈S
28We next show that the mismatch functions can be used to express the observation-level backups for any
function of the decoders. For any g :S →R, h∈[H], we define the function [Γ ◦g]:S →R
ϕ,h
(cid:88)
[Γ ◦g](s):= Γ (s′ |s)g(s′).
ϕ,h ϕ,h
s′∈S
We further overload the Bellman operator notation and define, for any g :S →R and M =(r ,P ),
lat lat lat
[TMlatg](s,a)=r (s,a)+E [g(s′)].
h lat s′∼Plat(s,a)
Lemma C.7. Let M =⟪M ,ψ⋆⟫, ϕ⋆ :=(ψ⋆)−1, ϕ∈Φ, and Γ be the mismatch function for emission
obs lat ϕ
ψ⋆ (Definition C.1). Then, for any f :S×A→R, h∈[H], and (x,a)∈X ×A, we have
lat
(cid:104) (cid:105) (cid:104) (cid:105)
TMobs(f ◦ϕ ) (x,a)= TMlat(Γ ◦V ) (ϕ⋆(x),a).
h lat h+1 h ϕ,h+1 flat h
Proof of Lemma C.7. Let f :=f , h∈[H], and (x,a)∈X ×A be given. Then, we have:
lat
(cid:104) (cid:105)
T hMobs(f ◦ϕ h+1) (x,a)=r lat,h(ϕ⋆ h(x),a)+E
sh+1∼Plat,h(ϕ⋆
h(x),a)E
xh+1∼ψ h⋆
+1(sh+1)[V f(ϕ(x h+1))]
 
(cid:88)
=r lat,h(ϕ⋆ h(x),a)+E sh+1∼Plat,h(ϕ⋆ h(x),a) ψ⋆(x h+1 |s h+1)V f(ϕ(x h+1))
xh+1∈X
(cid:34) (cid:35)
(cid:88)
=r (ϕ⋆(x),a)+E Γ (s′ |s )V (s′)
lat,h h sh+1∼Plat,h(ϕ⋆ h(x),a) ϕ h+1 f
s′∈S
=r (ϕ⋆(x),a)+E [Γ ◦V (s )]
lat,h h sh+1∼Plat,h(ϕ⋆ h(x),a) ϕ f h+1
(cid:104) (cid:105)
= TMlat(Γ ◦V ) (ϕ⋆(x),a),
h ϕ f h
where the third line follows from the definition of the mismatch function Γ .
ϕ
We next show that the mismatch functions can be used to realize the pushforward dynamics ϕ♯M⋆ , which
obs
we recall are defined as:
(cid:2) ϕ♯M⋆ (cid:3) (r,s′ |x,a)= (cid:88) M⋆ (r,x′ |x,a). (15)
obs,h obs,h
x′:ϕ(x′)=s′
We also recall the notation [Γ ◦M ] , defined via:
ϕ,h+1 lat h
(cid:88)
[Γ ◦M ] (r ,s |s ,a ):= M (r ,s′ |s ,a )Γ (s |s′ ).
ϕ lat h h h+1 h h lat,h h h+1 h h ϕ,h+1 h+1 h+1
s′ ∈S
h+1
Lemma C.8 (Pushforward model realizability via mismatch functions). For all ϕ∈Φ, h∈[H], we have:
[ϕ ♯M⋆ ](·|x,a)=(cid:2) [Γ ◦M⋆ ] ◦ϕ⋆(cid:3) (·|x,a) (16)
h+1 obs,h ϕ lat h h
Proof of Lemma C.8. Note that Γ can alternatively be written as:
ϕ
(cid:88)
Γ (s′ |s )= ψ⋆(x |s ).
ϕ,h h h h h h
xh:ϕ(xh)=s′
h
29We have
(cid:88)
ϕ ♯M⋆ (r ,s |x ,a )= M⋆ (r ,x |x ,a )
h+1 obs,h h+1 h+1 h h obs,h h+1 h+1 h h
xh+1:ϕh+1(xh+1)=sh+1
 
(cid:88) (cid:88)
=  M l⋆ at,h(r,s′ |ϕ⋆ h(x h),a h)ψ h⋆ +1(x h+1 |s′)
xh+1:ϕh+1(xh+1)=sh+1 r,s′∈R×S
(cid:88) (cid:88)
= M⋆ (r,s′ |ϕ⋆(x ),a ) ψ⋆ (x |s′)
lat,h h h h h+1 h+1
r,s′∈R×S xh+1:ϕh+1(xh+1)=sh+1
(cid:88)
= M⋆ (r,s′ |ϕ⋆(x ),a )Γ (s′ |s )
lat,h h h h ϕ,h+1 h+1
r,s′∈R×S
=[Γ ◦M⋆ ] (r,s |ϕ⋆(x ),a ),
ϕ lat h h+1 h h h
as desired.
30D Proofs and Additional Results for Section 3.2: Impossibility
Results
Thissectioncontainsadditionalinformationandproofsrelatedtoourimpossibilityresultsregardingstatistical
modularity (Section 3.2), and is organized as follows:
• Appendix D.1 contains the statement for an additional lower bound that is useful for establishing the
impossibility results of Figure 1.
• Appendix D.2 contains details for each entry of Figure 1.
• Appendix D.3 contains for proofs for our main lower bound (Theorem 3.1) and the additional lower
bound (Theorem D.1).
D.1 Additional Lower Bound
Theorem D.1 (Alternative lower bound). For every N ≥4, there exists an emission class Ψ and a decoder
class Φ with |Ψ| = |Φ| = N and a family of latent MDPs M satisfying (i) |M | = 1, (ii) H = 1, (iii)
lat lat
|S|=|X|=N, (iv) |A|=N, and such that
1. For all ε,δ >0, we have comp(M ,ε,δ)=0.
lat
2. For an absolute constant c>0, comp(⟪M ,Φ⟫,c,c)≥Ω(N/log(N)).
lat
Proof of Theorem D.1. See Appendix D.3.2.
D.2 Details for Figure 1
Below, we provide details on each entry in Figure 1. More precisely, for each latent class M , we will give a
lat
(brief) description of the MDP class M , give our choice of latent complexity comp for M , and prove
lat lat
that the class is or is not statistically modular for that choice of latent complexity. We view our choices of
latent complexities as natural complexities for the respective classes.
Tabular MDPs (✓).
• Latent class M : Tabular MDPs M =(S,A,P ,R ,H). [AOM17]
lat lat lat lat
• Latent complexity comp: We take comp(M ,ε,δ)=poly(|S|,|A|,H,ε−1,logδ−1), which is attainable,
lat
for example, via the Ucb-Vi algorithm of Azar et al. [AOM17]
• Statistical modularity (✓): Known Block MDP algorithms (e.g. Musik [MFR23], Briee [ZSUWAS22])
have sample complexities of poly(|S|,|A|,H,ε−1,logδ−1,log|Φ|).
Contextual Bandits (✓).
• Latent class M : Contextual bandits with context space S, action space A, reward function r⋆ :
lat lat
S×A→[0,1] and a finite realizable function class satisfying r⋆ ∈F .
lat
• Latent complexity comp: We take comp(M ,ε,δ) = poly(|A|,log|F |,ε−1,logδ−1), attainable via,
lat lat
e.g., the Square-Cb algorithm [FR20].
• Statistical modularity (✓): We note that F ◦ Φ = {[f ◦ϕ]|f ∈F,ϕ∈Φ} is a realizable func-
lat
tion class for the observation-level reward function r⋆ , since r⋆ = [r⋆ ◦ϕ⋆] ∈ F ◦Φ. Thus,
obs obs lat lat
applying the Square-Cb algorithm directly on the observations x(t),a(t),r(t) will give complexity
poly(|A|log(|F ||Φ|),ε−1,logδ−1)=poly(|A|,log|F |,log|Φ|,ε−1,logδ−1).
lat lat
31Low-rank MDP (✓).
• Latent class M : MDPs M = (S,A,H,P ,r ) such that there exists µ⋆ ∈ Rd, θ⋆ ∈ Rd,
lat lat lat lat lat,h lat,h
and a known set of features Ξ = (cid:110) ξ =(cid:8) ξ :S×A→Rd(cid:9)H (cid:111) such that for all h ∈ [H] we
lat lat lat,h h=1
have r (s ,a )=⟨ξ⋆ (s ,a ),θ⋆ ⟩ as well as
lat h h lat,h h h lat,h
P (s |s ,a )=⟨ξ⋆ (s ,a ),µ⋆ (s )⟩ (17)
lat,h h+1 h h lat,h h h lat,h+1 h+1
for some ξ⋆ ∈Ξ .
lat lat
• Latent complexity comp: We take comp(M ,ε,δ) = poly(d,|A|,H,log|Ξ |,ε−1,logδ−1), which is
lat lat
attainable via the VoX algorithm of Mhammedi et al. [MBFR23].
• Statistical modularity (✓): This is obtained by noting that the observation-level dynamics also satisfy
the low-rank property with the same dimension. Formally, letting P be the transition kernel for
obs
⟪M ,ψ⋆⟫ and ϕ⋆ =(ψ⋆)−1, we have
lat
(cid:88)
P (x |x ,a )= P (s |ϕ⋆(x ),a )ψ⋆ (x |s )
obs,h h+1 h h lat,h h+1 h h h h+1 h+1 h+1
sh+1∈S
= (cid:88) (cid:10) ξ⋆ (ϕ⋆(x),a),µ⋆ (s )(cid:11) ψ⋆ (x |s )
lat,h h lat,h+1 h+1 h+1 h+1 h+1
sh+1∈S
(cid:42) (cid:43)
(cid:88)
= ξ⋆ (ϕ⋆(x),a), µ⋆ (s )ψ⋆ (x |s ) .
lat,h h lat,h+1 h+1 h+1 h+1 h+1
sh+1∈S
Thus,thetransitionkernelP isalow-rankMDPwithµ (x ):=(cid:80) µ⋆ (s )ψ⋆ (x |
obs obs,h+1 h+1 sh+1 lat,h+1 h+1 h+1 h+1
s ) and feature class
h+1
(cid:110) (cid:111)
Ξ ◦Φ= ξ ◦ϕ={ξ ◦ϕ :x,a(cid:55)→ξ (ϕ (x),a)}H |ξ ∈Ξ ,ϕ∈Φ .
lat lat h h h h h=1 lat lat
Lastly, since r =[r ◦ϕ⋆], the reward function is also linear with the same unknown feature class.
obs lat
Thus we can apply Vox directly on top of the observations, with the feature class Ξ ◦Φ, which will
lat
achieve a complexity poly(d,|A|,H,log|Ξ |,log|Φ|,ε−1,log(cid:0) δ−1(cid:1) ).
lat
Known Deterministic MDP (|M |=1) (✓).
lat
• Latent class M : M = {M = (S,A,P ,R ,H)} is a set of MDPs of size 1 with both
lat lat lat lat lat
deterministic rewards and deterministic transitions.
• Latent complexity comp: We take comp(M ,ε,δ)=0, which is attainable as M is known and we
lat lat
can simply deploy its optimal policy.
• Statistical modularity (✓): We note that, due to determinism, the latent optimal policy can be
chosen to be open-loop without loss of generality, and thus will always experience the same trajectory
(s⋆,a⋆,...,s⋆ ,a⋆ ). We can define the observation-level policy which commits to this same sequence of
1 1 H H
actions, i.e. π (x )=a⋆ for all x . This will be an optimal policy for any M =⟪M ,ψ⟫, and
obs,h h h h obs lat
can also be learned in 0 samples.
Low State Occupancy (∀π :S →∆(A)) (✓).
• Latent class M : M = {M = (S,A,P ,R ,H)} is a set of MDPs for which we have a
lat lat lat lat lat
realizable value function class, and such that there exists a feature map ζ =(cid:8) ζ :S →Rd(cid:9)H
lat lat,h h=1
such that for all π :S →∆(A) and for all M ∈M , we have
lat lat
(cid:68) (cid:69)
∀h∈[H] ∃θMlat,π : dMlat,π(s)= ζ (s),θMlat,π .
h h lat,h h
Note that the feature map does not need to be known.
32• Latent complexity comp: We take comp(M ,ε,δ)=poly(d,|A|,H,log|F |,ε−1,log(cid:0) δ−1(cid:1) ), which is
lat lat
attainable by the Bilin-Ucb algorithm of Du et al., since i) MDPs with this property have Bilinear
rank bounded by d|A| (see Definition 4.3 and Lemma 4.6 of Du et al. [Du+21]), and ii) one can
construct the value function class F
lat
= {QMlat,⋆ | M
lat
∈ M lat}, which is realizable and has size
log|F |=log|M |.
lat lat
• Statistical modularity (✓): We firstly note that one can construct a realizable value function class for
the set ⟪M lat,Φ⟫, via the set F
obs
=(cid:8) QMlat,⋆◦ϕ|M
lat
∈M lat,ϕ∈Φ(cid:9). This is realizable since, for
any M
obs
:=⟪M lat,ψ⟫, letting ϕ⋆ =ψ−1, we have QMobs,⋆ =QMlat,⋆◦ϕ⋆, and that this class has size
log|M lat||Φ|. WecanthenshowthattheoccupanciesdMobs,πfobs,forf
obs
∈F obs,canalsobeexpressedas
d-dimensionallinearfunctionforanappropriatechoiceoffeatures, whichwillimplythattheBilin-Ucb
algorithm run directly on M will attain a complexity of poly(d,|A|,H,logM ,logΦ,ε−1,log(cid:0) δ−1(cid:1) ).
obs lat
To obtain this, we recall the following lemma:
Lemma C.2. Let {Γ } denote the mismatch functions for emission ψ⋆, and let M =⟪M ,ψ⋆⟫.
ϕ ϕ∈Φ obs lat
Then, for any π ∈Π , ϕ∈Φ, h∈[H], x∈X, we have
lat lat
dMobs,πlat◦ϕ(x)=ψ⋆(x|ϕ⋆(x))dMlat,Γϕ◦πlat(ϕ⋆(x)).
h h h h h
Thanks to the above lemma, we have
dπf◦ϕ(x )=ψ(x |ϕ⋆(x ))dΓϕ◦πf(ϕ⋆(x ))
obs h h h lat h
(cid:68) (cid:69)
=ψ(x |ϕ⋆(x )) [ζ ◦ϕ⋆](x ),θMlat,Γϕ◦πf
h h lat,h h h h
(cid:68) (cid:69)
= ψ(x |ϕ⋆(x ))[ζ ◦ϕ⋆](x ),θMlat,Γϕ◦πf
h h lat,h h h h
andsodπ obf s◦ϕ islinearwithfeaturemappingψ(x
h
|ϕ⋆(x h))[ζ lat,h◦ϕ⋆ h]andparameterθMlat,Γϕ◦πf. Recall
that the feature map need not be known, so that Bilin-Ucb can still be applied despite not knowing ψ
and ϕ⋆.
Model class + Pushforward Coverability (✓).
• Latent class M : M ={M =(S,A,P ,R ,H)} is a set of MDPs that all satisfy pushforward
lat lat lat lat lat
coverability C (M )≤C (cf. Eq. (28) for the definition).
push lat push
• Latentcomplexitycomp: Wetakecomp(M ,ε,δ)=poly(C ,|A|,H,log|M |,ε−1,log(cid:0) δ−1(cid:1) ),which
lat push lat
is attainable by the Golf algorithm via the results of Xie et al. [XFBJK23] (see also Lemma E.3). We
obtainthisbynotingthati)C ≤C |A|,whereC isdefinedinDefinition2ofXieetal.[XFBJK23],
cov push cov
and ii) a realizable model class can be used to construct a realizable value function class F and a
Bellman-complete value function helper class G with sizes log|F|=log|M| and log|G|=O(log|M|).
• Statistical modularity (✓): This is obtained via Theorem 3.2.
Linear CB/MDP (✗⋆).
• Latent class M : MDPs M =(S,A,P ,R ,H) that are linear with respect to a known feature
lat lat lat lat
map ξ⋆ :S×A→Rd (i.e. such that Eq. (17) holds for ξ⋆ ).
lat lat
• Latent complexity comp: We take comp(M ,ε,δ)=poly(d,H,ε−1,log(cid:0) δ−1(cid:1) ), which is attainable via
lat
the Lsvi-Ucb algorithm of Jin et al. [JYWJ20]. Note that this guarantee does not depend on the
number of actions.
• Statistical intractability (✗): The latent model used in the construction of Theorem D.1 is a set (of size
1) of linear MDPs with d=1. In particular, that construction was a contextual bandit so we only have
to realize a reward function, and since there is only one latent model so we can trivially embed this with
d=1 via ξ⋆ (s,a)=r (s,a), where r is the reward function of the MDP used in Theorem D.1.
lat lat lat
33• Statistical modularity with additional |A|-dependence: As in the Low-rank MDP case above, ⟪M ,ψ⟫
lat
is low-rank with unknown feature set Φ′ = {ξ⋆ ◦ϕ|ϕ∈Φ}. Thus, by the same conclusion, a the
lat
Vox algorithm will have complexity poly(d,|A|,H,log|Φ|), which is of the desired form if we allow
suboptimal dependence on |A|.
Model class + Coverability (∀π :M ∈M) (✗).
M
• Latent assumption: M ={M =(S,A,P ,R ,H)} is a set of MDPs that all satisfy coverability
lat lat lat lat
with respect to the policy class Π ={π |M ∈M}, i.e. we have
M M
∀M lat ∈M lat : C cov(M lat)= inf sup sup
(cid:13)
(cid:13)
(cid:13)dM hlat,π(cid:13)
(cid:13)
(cid:13) <∞
µh∈∆(S×A)h∈[H]π∈ΠM(cid:13) µ
h
(cid:13)
∞
• Latent complexity comp: We take comp(M ,ε,δ)=poly(C ,H,log|M |,ε−1,log(cid:0) δ−1(cid:1) ), which is
lat cov lat
attainable by the Golf algorithm via the results of Xie et al. [XFBJK23] (see also Lemma E.3). We
obtain this by noting that a realizable model class can be used to construct a realizable value function
class F and a complete value function class G of sizes log|F|=log|M| and log|G|=O(log|M|).
• Statistical intractability (✗): The latent models used in the construction of Theorem 3.1 are a set of
coverable MDPs – in particular, these are trivially coverable with C =1 since there is a single latent
cov
model and we can take µ=dM l⋆ at,π Ml⋆ at. We remark that it is an interesting open question whether this
impossibility result continues to hold if we require coverability with respect to the class Π of all possible
latent policies.
Known Stochastic MDP (|M |=1) (✗).
lat
• Latent class M : M ={M =(S,A,P ,R ,H)} is a set of MDPs of size 1.
lat lat lat lat lat
• Latent complexity comp: We take comp(M ,ε,δ)=0, which is attainable as M is known and we
lat lat
can simply deploy its optimal policy.
• Statistical intractability (✗): This is precisely the setting of Theorem 3.1, which shows that at least
Ω(N/log(N)) samples will be needed, where N =|Φ|.
Bellman rank (Q-type or V-type) (✗)
• Latent assumption: M = {M =(S,A,P ,R ,H)} is a set of latent models such that each
lat lat lat lat
M ∈M has Q-type Bellman rank d or V-type Bellman rank d [JLM21]. Letting F be a realizable
lat lat
value function class for M , in the Q-type case, this means that the |Π |×|F| matrix
lat F
(cid:104) (cid:105)
EQ(π,f)=Eπ f (s ,a )−r −maxf (s ,a′) ,
h h h h h h+1 h+1
a′
admits a rank d factorization. In the V-type case, the matrix
(cid:104) (cid:105)
EV(π,f)=E f (s ,a )−r −maxf (s ,a′)
h sh∼dπ h,ah∼πf h h h h
a′
h+1 h+1
admits a rank-d matrix factorization.
• Latent complexity comp: We take comp(M ,ε,δ) = poly(d,H,|A|log|F|,ε−1,log(cid:0) δ−1(cid:1) ) for the V-
lat
type Bellman rank case, which is achievable by the Olive algorithm of Jiang et al. [JKALS17], and
comp(M ,ε,δ) = poly(d,H,log|F|,ε−1,log(cid:0) δ−1(cid:1) ) for Q-type Bellman rank, which is achievable by
lat
the Bilin-Ucb algorithm of Du et al. [Du+21].
• Statistical intractability (✗): We note that the construction in Theorem 3.1 has |M | = 1, which
lat
trivially has Bellman rank equal to 1, so Theorem 3.1 precludes statistical modularity with complexity
comp.
34Eluder dimension + Bellman Completeness (✗)
• LatentclassM : M ={M =(S,A,P ,R ,H)}isasetofMDPssuchthatthereisafunction
lat lat lat lat lat
class F satisfying
lat
∀f
lat
∈F lat,M
lat
∈M
lat
: TMlatf
lat
∈F lat.
Furthermore, each M ∈ M has Bellman-Eluder dimension bounded by d (see Definition 8 of
lat lat
[JLM21]).
• Latent complexity comp: We take comp(M ,ε,δ)=poly(d,H,log|F|,ε−1,log(cid:0) δ−1(cid:1) ), which is attain-
lat
able by the Golf algorithm of Jin et al. [JLM21].
• Statistical intractability (✗): As in the Bellman rank case, the construction in Theorem 3.1 has
|M lat|=1, so we can take F
lat
={QMlat,⋆ |M
lat
∈M lat} which is evidently complete for TMlat, and
has Eluder dimension 1, so Theorem 3.1 precludes statistical modularity with complexity comp.
Q⋆-irrelevant State Abstraction (✗)
• Latent class M : M =(S,A,P ,R ,H) such that there is a known state abstraction function
lat lat lat lat
ζ
lat
:S →Z such that ζ lat(s)=ζ lat(s′) implies that QMlat,⋆(s,a)=QMlat,⋆(s′,a) for all a∈A.
• Latentcomplexitycomp: Wetakecomp(M ,ε,δ)=poly(|Z|,|A|,H,ε−1,log(cid:0) δ−1(cid:1) )whichisattainable
lat
by the Olive algorithm of Jiang et al. [JKALS17].
• Statistical intractability (✗): We take M = {M } as the MDP class from the construction of
lat lat
Theorem 3.1. Let Q⋆
lat
:= QMlat,⋆. Note that we have Q⋆ lat(s,a) ∈ {0,1} for all s,a, so we can
take a latent abstract state space Z = {(0,0),(0,1),(1,0),(1,1)} and a state abstraction function
ζ such that ζ (s) = (i,j) if Q⋆ (s,0) = i and Q⋆ (s,1) = j. This satisfies the property of a
lat lat lat lat
Q⋆-irrelevant abstraction, since ζ (s) = ζ (s′) = (i,j) implies that Q⋆ (s,0) = Q⋆ (s′,0) = i
lat lat lat lat
and Q⋆ (s,1) = Q⋆ (s′,1) = j. This has a constant-sized abstract space (|Z| = 4) and |A| = 2, so
lat lat
Theorem 3.1 precludes statistical modularity with complexity comp.
Linear Mixture MDP (✗).
• Latent class M : MDPs M =(S,A,P ,R ,H) such that there is a known feature map ζ =
lat lat lat lat lat
{ζ :s′,s,a(cid:55)→Rd}H such that
lat,h h=1
∀h∈[H],∃θ ∈Rd : P (s′ |s,a)=⟨ζ (s′ |s,a),θ ⟩
h lat,h lat,h h
• Latent complexity comp: We take comp(M ,ε,δ)=poly(d,H,ε−1,log(cid:0) δ−1(cid:1) ), which is attainable by
lat
the Ucrl-Vtr+ algorithm of Zhou et al. [ZGS21]
• Statistical intractability (✗): We take M = {M } to be the construction of Theorem 3.1. Here,
lat lat
there is a single latent model, so this is trivially embeddable with ζ (s′ |s,a)=P⋆ (s′ |s,a)∈R1.
lat,h lat,h
This has dimension d=1, so Theorem 3.1 precludes statistical modularity with complexity comp.
Linear Q⋆/V⋆ (✗).
• Latent class M : MDPs M = (S,A,P ,R ,H) such that there are known features maps
lat lat lat lat
α :S×A→Rd and β :S →Rd such that for all M ∈M , there exists unknown parameters
lat lat lat lat
θ Q,θ
V
∈Rd such that QMlat,⋆(s,a)=⟨α lat(s,a),θ Q⟩ and VMlat,⋆(s)=⟨β lat(s),θ V⟩.
• Latent complexity comp: We take comp(M ,ε,δ)=poly(d,H,ε−1,log(cid:0) δ−1(cid:1) ), which is attainable by
lat
the Bilin-Ucb algorithm of Du et al. [Du+21].
• Statistical intractability (✗): We can take M to be the latent MDP class from the construction of
lat
Theorem 3.1. Since there is a single latent model, this is trivially embeddable with dimension 1, i.e.
we can take ζ (s,a) = Q⋆ (s,a) and β (s) = V⋆ (s). This has dimension d = 1, so Theorem 3.1
lat lat lat lat
precludes statistical modularity with complexity comp.
35Low State or State-Action Occupancy (∀π :M ∈M) (✗).
M
• Latent class M : In the Low State Occupancy model, M = {M = (S,A,P ,R ,H)} is
lat lat lat lat lat
a set of MDPs such that there exists a feature map ζV = (cid:8) ζ :S →Rd(cid:9)H such that for all
lat lat,h h=1
π ∈{π |M ∈M } and for all M ∈M , we have
Mlat lat lat lat lat
(cid:68) (cid:69)
∀h∈[H] ∃θMlat,π : dMlat,π(s)= ζV (s),θMlat,π .
h h lat,h h
FortheState-ActionOccupancymodel,wehavethatthereexistsafeaturemapζQ =(cid:8) ζ :S×A→Rd(cid:9)H
lat lat,h h=1
such that for all π ∈{π |M ∈M } and for all M ∈M , we have
Mlat lat lat lat lat
(cid:68) (cid:69)
∀h∈[H] ∃θMlat,π : dMlat,π(s,a)= ζQ (s,a),θMlat,π .
h h lat,h h
Note that the feature map does not need to be known in either case.
• Latentcomplexitycomp: Wetakecomp(M ,ε,δ)=poly(d,|A|,H,log|F |,ε−1,log(cid:0) δ−1(cid:1) )forthestate
lat lat
occupancy case and comp(M ,ε,δ)=poly(d,H,log|M |,ε−1,log(cid:0) δ−1(cid:1) ). Both are attainable by the
lat lat
Bilin-UcbalgorithmofDuetal.,sincei)MDPswiththispropertyhaveBilinearrankboundedbyd|A|
and d respectively (see Definition 4.3 and Lemma 4.6 of [Du+21]), and ii) one can construct the value
function class F
lat
={QMlat,⋆ |M
lat
∈M lat} which is realizable and has size log|F lat|=log|M lat|.
• Intractability: We can take the construction of Theorem 3.1, which has |M |=1 and thus is trivially
lat
embeddable with dimension 1, i.e. we can take ζ lV at(s)=dMlat,πMlat(s) and ζ lQ at(s,a)=dMlat,πMlat(s,a).
Bisimulation (?)
• Latent class M : MDPs M = (S,A,P ,R ,H) such that there is a known state abstraction
lat lat lat lat
function ζ : S → Z such that ζ (s) = ζ (s) implies that R (s,a) = R (s,a) for all a ∈ A as
lat lat lat (cid:101) lat lat (cid:101)
well as (cid:80) P (s′ |s,a)=(cid:80) P (s′ |s,a) for all z′.
s′:ζlat(s′)=z′ lat s′:ζlat(s′)=z′ lat (cid:101)
• Latentcomplexitycomp: Wetakecomp(M ,ε,δ)=poly(|Z|,|A|,H,ε−1,log(cid:0) δ−1(cid:1) )whichisattainable
lat
by the Olive algorithm of [JKALS17].
• Openness (?): A negative result does not follow from existing constructions, since the dynamics from
the tree-based construction of Theorem 3.1 are not bisimilar unless |Z| = |S|, which allows for the
application of tabular methods. At the same time, a positive result does not follow from existing
methods, since it is non-trivial to extend existing Block MDP methods to use the bisimulation state
abstraction in a way that only pays for |Z|.
Low State-Action Occupancy (∀π :S →∆(A)) (?⋆)
• Latent class M : M = {M = (S,A,P ,R ,H)} is a set of MDPs such that there exists a
lat lat lat lat lat
feature map ζQ =(cid:8) ζ :S×A→Rd(cid:9)H such that for all π :S →∆(A) and for all M ∈M ,
lat lat,h h=1 lat lat
we have
(cid:68) (cid:69)
∀h∈[H] ∃θMlat,π : dMlat,π(s,a)= ζQ (s,a),θMlat,π .
h h lat,h h
Note that the feature map does not need to be known.
• We take comp(M ,ε,δ)=poly(d,H,log|M |,ε−1,log(cid:0) δ−1(cid:1) ), which is attainable by the Bilin-Ucb
lat lat
algorithmofDuetal.,sincei)MDPswiththispropertyhaveBilinearrankboundedbyd(seeDefinition
4.3 and Lemma 4.6 of [Du+21]), and ii) one can construct a realizable value function class of size
log|F|=log|M|.
• Openness (?): A negative result does not follow from existing constructions, since the dynamics from
the tree-based construction of Theorem 3.1 do not have linear occupancies for all π :S →∆(A) unless
d=|S|, which allows for the application of tabular methods, and the dynamics from the bandit-based
construction Theorem D.1 do not have linear occupancies for all π :S →∆(A) unless d=|A|. At the
same time, unlike the low state occupancy case, a positive result does not follow as it is unclear if we
can express the observation-space occupancies linearly.
36• Statistical tractability with additional (suboptimal) |A|-dependence (✓): Note that we can reduce to
the Low State Occupancy case (✓), since
(cid:42) (cid:43)
dπ(s)= (cid:88) dπ(s,a)= θπ,(cid:88) ζQ (s,a) :=(cid:10) θπ,ζV (s)(cid:11) .
lat lat
a∈A a∈A
However, this blows up the feature norm bound of the feature map ζV (s) by a factor of |A|, which will
lat
appear logarithmically in the bound obtained by Bilin-Ucb.
Model class + Coverability (∀π :S →∆(A)) (?).
• Latent class M : M ={M =(S,A,P ,R ,H)} is a set of MDPs that all satisfy coverability
lat lat lat lat lat
with respect to all policies π :S →∆(A), i.e. we have
lat
∀M lat ∈M lat : C cov(M lat)= inf sup sup
(cid:13)
(cid:13)
(cid:13)dM hlat,π(cid:13)
(cid:13)
(cid:13) <∞
µh∈∆(S×A)h∈[H]π:S→∆(A)(cid:13) µ
h
(cid:13)
∞
• Latent complexity comp: We take comp(M ,ε,δ)=poly(C ,H,log|M |,ε−1,log(cid:0) δ−1(cid:1) ), which is
lat cov lat
attainable by the Golf algorithm via the results of Xie, Foster, Bai, Jiang, and Kakade (see also
Lemma E.3). We obtain this by noting that a realizable model class can be used to construct a
realizable value function class F and a complete value function class G of sizes log|F|=log|M| and
log|G|=O(log|M|).
• Openness (?): A negative result does not follow from the existing constructions. The tree-based con-
structionofTheorem3.1satisfiescoverabilitywithC =exp(Ω(H))andthebandit-basedconstruction
cov
of Theorem D.1 satisfies coverability with C =|A|. In both cases, the lower bounds cannot be used
cov
to rule out statistical modularity with the above latent complexity. Similarly, it unclear how to obtain
a positive result for the latent-dynamics class ⟪M ,Φ⟫.
lat
D.3 Proofs for Lower Bounds (Theorems 3.1 and D.1)
D.3.1 Main lower bound (Theorem 3.1)
We will prove the following result.
Theorem 3.1 (Impossibility of statistical modularity). For every N ≥4, there exists a decoder class Φ with
|Φ|=N andafamilyofbaseMDPsM satisfying(i)|M |=1, (ii)H ≤O(log(N)), (iii)|S|=|X|≤N2,
lat lat
(iv) |A|=2, and such that
1. For all ε,δ >0, we have comp(M ,ε,δ)=0.
lat
2. For an absolute constant c>0, comp(⟪M ,Φ⟫,c,c)≥Ω(N/log(N)).
lat
Proof. Let N be given and assume without loss of generality that it is a power of 2. We first construct the
class of latent-dynamics MDPs, following Song et al. [SWFK24].
Latent MDP. The construction has a single “known” latent MDP M , so that the only uncertainty in
lat
the family of latent-dynamics MDPs we construct arises from the emission processes. We set M ={M }.
lat lat
Set H =log (N)+1 and A={0,1}. We define the state space and latent transition dynamics as follows.
2
• The state space can be partitioned as S =S1,...,SN.
• Each block Si corresponds to a standard depth-H binary tree MDP with deterministic dynamics (e.g.,
Osband et al.; Domingues et al. [OVR16; DMKV21]). There is a single “root” node at layer h = 1,
which we denote by si , and N “leaf” nodes at layer H, which we denote by (cid:8) si,j (cid:9) . For each
root leaf j∈[N]
h=1,...,H−1, choosing action 0 leads to the left successor of the current state deterministically, and
choosingaction1leadstotherightsucessor; thisprocesscontinuesuntilwereachaleafnodeatlayerH.
37• The initial state distribution is P (∅)=Unif(s1 ,...,sN ).
lat,1 root root
• There are no rewards for layers 1,...,H −1. For layer H, the reward is
R (si,j ,·)=I{j =i}. (18)
H leaf
Thisconstructioncansummarizedasfollows. Atlayer1,wedrawtheindexofoneofN binarytreesuniformly
at random, and initialize into the root of the tree. From here, we receive a reward of 1 if we successfully
navigate to the leaf node whose index agrees with the index of the tree itself, and receive a reward of 0
otherwise.
Note that the total number of latent states in this construction is |S|=N ·|S |=N(2N −1)
1
Observation space and decoder class. Let us introduce some additional notation. For each block
Si, let Si := {si,j} denote the states in block i that are reachable at layer h, so that Si = (cid:8) si (cid:9)
h h j∈[2h−1] 1 root
and Si = {si,j } . We define X = S so that |X| ≤ 4N2, and consider a class of emission processes
H leaf j∈[N]
corresponding to deterministic maps. Let Σ denote the set of cyclic permutations on N elements, excluding
the identity permutation. That is, each σ ∈Σ takes the form
i
σ :k (cid:55)→k+i mod N for i∈{1,...,N}.
i
For each σ ∈Σ, we consider the emission process
ψσ(·|s(i,j))=I .
h h s(σ(i),j)
h
That is, ψσ shifts the index of the binary tree containing s(i,j) according to σ. Let Ψ = {ψσ | σ ∈ Σ}.
h
Consider the decoder class
Φ=Ψ−1 :=(cid:8) si (cid:55)→sψ−1(i) |ψ ∈Ψ(cid:9) ,
which has |Φ|=N. We consider the class of rich-observation MDPs given by
⟪M lat,Φ⟫:=(cid:8) Mi :=⟪M lat,ψσi⟫|σ
i
∈Σ(cid:9) . (19)
It is clear that this class of rich-observation MDPs satisfies the decodability assumption for emissions Ψ.
Sample complexity lower bound. To lower bound the sample complexity, we prove a lower bound on
the constrained PAC Decision-Estimation Coefficient (DEC) of [FGH23]. For an arbitrary MDP M (defined
over the space X) and ε∈[0,21/2], define16
dec (M,M)= inf sup (cid:8)E [JM(π )−JM(π)]| E (cid:2) D2(cid:0) M(π),M(π)(cid:1)(cid:3) ≤ε2(cid:9) ,
ε π∼p M π∼q H
p,q∈∆(Π)M∈M
where M(π) denotes the law over trajectories (x ,a ,r ),...,(x ,a ,r ) induced by executing the policy
1 1 1 H H H
π in the MDP M, JM(π) denotes the expected reward for policy π under M, and π denotes the optimal
M
policy for M. We further define
dec (M)=supdec (M,M),
ε ε
M
where the supremum ranges over all MDPs defined over X and A. We now appeal to the following technical
lemma.
Lemma D.1. For all ε2 ≥4/N, we have that dec (⟪M ,Φ⟫)≥ 1.
ε lat 2
16FormeasuresPandQ,wedefinesquaredHellingerdistancebyD2(P,Q)=(cid:82) (√ dP−√ dQ)2.
H
38InlightofLemmaD.1,itfollowsfromTheorem2.1inFosteretal.[FGH23]17 thatanyPACRLalgorithmthat
usesT episodesofinteractionforT log(T)≤c·N musthaveE[JM(π )−JM(π)]≥c′ foraworst-caseMDPin
M (cid:98)
M,wherec,c′ >0areabsoluteconstants. ThisimpliesthatanyPACRLwhichhasE[JM(π )−JM(π)]≤c′
M (cid:98)
must have T log(T)≥c·N and thus T ≥c·N/log(N).
Proof of Lemma D.1. Define M as the latent-space MDP that has identical dynamics to M but, has
lat lat
zero reward in every state, and define M :=⟪M ,id⟫ as the rich-observation MDP obtained by composing
lat
M with the “identity” emission process id that sets x =s . Observe that M and Mi, induce identical
lat h h
dynamics in observation space if rewards are ignored: For all policies π,
PM,π[(x ,a ),...,(x ,a )=·]=PMi,π[(x ,a ),...,(x ,a )=·]. (20)
1 1 H H 1 1 H H
It follows that for each i, for all policies π, we have
D2(cid:0) Mi(π),M(π)(cid:1)
H
=D2(cid:0)
(⟪M ,ψ ⟫)(π),(⟪M
,id⟫)(π)(cid:1)
H lat i lat
N
=(cid:88) PM,π(cid:2) x =s(ψi(j),j)(cid:3) ·D2(I ,I )
H leaf H 1 0
j=1
N
=2(cid:88) PM,π(cid:2)
x
=s(ψi(j),j)(cid:3)
H leaf
j=1
N
= 2 (cid:88) PM,π(cid:2) x =s(ψi(j),j) |x =s(ψi(j))(cid:3) , (21)
N H leaf 1 root
j=1
N
= 2 (cid:88) PM,π(cid:104) x =s(j,ψi−1(j)) |x =s(j) (cid:105) , (22)
N H leaf 1 root
j=1
since the learner receives identical feedback in the MDPs Mi and M unless they reach the observation
x = s(ψi(j),j) for some j (corresponding to latent state s(j,j) in Mi), in which case they receiver reward 1
H leaf leaf
in Mi but reward 0 in M. We now claim that for any q ∈∆(Π), there exists a set of at least N/2 indices
I ⊂[N] such that
q
E (cid:2) D2(cid:0) Mi(π),M(π)(cid:1)(cid:3) ≤ 4 (23)
π∼q H N
for all i∈I . To see this, note that by Eq. (22), we have
q
 
N N
E i∼Unif([N])E π∼q(cid:2) D H2(cid:0) Mi(π),M(π)(cid:1)(cid:3) ≤ E π∼q N2 (cid:88) N1 (cid:88) PM,π(cid:104) x H =s( lej a,ψ fi−1(j)) |x 1 =s r( oj) ot(cid:105) 
j=1 i=1
 
N
2 (cid:88) 1 2
≤ E π∼q
N
N= N,
j=1
where the second inequality uses that (cid:80)N PM,π(cid:104) x =s(j,ψi−1(j)) |x =s(j) (cid:105) ≤1, as the events in the sum
i=1 H leaf 1 root
are mutually exclusive (and the event we condition on does not depend on i). We conclude by Markov’s
inequality that P (cid:2)E (cid:2) D2(cid:0) Mi(π),M(π)(cid:1)(cid:3) ≥4/N(cid:3) ≤1/2, giving I ≥N/2.
i∼Unif([N]) π∼q H q
17Theorem2.1inFosteretal.[FGH23]isstatedwithrespecttosup M∈conv(M)decε(M,M),buttheactualproof(Section2.2)
givesastrongerresultthatscaleswithsup Mdecε(M,M).
39From Eq. (27), we conclude that for all ε2 ≥4/N,
(cid:110) (cid:104) (cid:105)(cid:111)
dec (M,M)≥ inf inf sup E JMi(π )−JMi(π) .
ε π∼p Mi
q∈∆(Π)p∈∆(Π)i∈Iq
To lower bound this quantity, observe that for any index i and any policy π, we have
N
JMi(π )−JMi(π)= 1 (cid:88) PM(i),π(cid:2) x ̸=s(ψi(j),j) |x =s(ψi(j))(cid:3)
Mi N H leaf 1 root
j=1
N
=1−
1 (cid:88) PM(i),π(cid:2)
x =s(ψi(j),j) |x
=s(ψi(j))(cid:3)
N H leaf 1 root
j=1
N
=1−
1 (cid:88) PM,π(cid:2)
x =s(ψi(j),j) |x
=s(ψi(j))(cid:3)
N H leaf 1 root
j=1
N
=1−
1 (cid:88) PM,π(cid:104)
x
=s(j,ψi−1(j))
|x =s(j)
(cid:105)
,
N H leaf 1 root
j=1
where the third inequality uses Eq. (20). We conclude that for any distribution p,q ∈∆(Π),
(cid:110) (cid:104) (cid:105)(cid:111)
sup E JMi(π )−JMi(π)
π∼p Mi
i∈Iq
(cid:110) (cid:104) (cid:105)(cid:111)
≥E E JMi(π )−JMi(π)
i∼Unif(Iq) π∼p Mi
N
≥1− 1 (cid:88) E PM,π(cid:104) x =s(j,ψi−1(j)) |x =s(j) (cid:105)
N i∼Unif(Iq) H leaf 1 root
j=1
N
=1−
1 (cid:88) 1 (cid:88) PM,π(cid:104)
x
=s(j,ψi−1(j))
|x =s(j)
(cid:105)
≥1−
1
≥
1
N |I | H leaf 1 root |I | 2
q q
j=1 i∈Iq
as long as N ≥4, where the second-to-last inequality uses that for all j, the events (cid:8) x =s(j,ψi−1(j)) |x =
H leaf 1
s(j) (cid:9) are disjoint for all i. Since this lower bound holds uniformly for all q,p∈∆(Π), we conclude that
root
1
dec (⟪M ,Φ⟫,M)≥ .
ε lat 2
D.3.2 Proof of alternative lower bound (Theorem D.1)
We will prove the following result.
Theorem D.1 (Alternative lower bound). For every N ≥4, there exists an emission class Ψ and a decoder
class Φ with |Ψ| = |Φ| = N and a family of latent MDPs M satisfying (i) |M | = 1, (ii) H = 1, (iii)
lat lat
|S|=|X|=N, (iv) |A|=N, and such that
1. For all ε,δ >0, we have comp(M ,ε,δ)=0.
lat
2. For an absolute constant c>0, comp(⟪M ,Φ⟫,c,c)≥Ω(N/log(N)).
lat
Proof of Theorem D.1. We repeat more or less repeat the same proof as Theorem 3.1, but with the
appropriate modifications to translate from the contextual tree-based construction in Theorem 3.1 to the
contextual bandit-based construction in the theorem statement. Let N be given and assume without loss of
generality that it is a power of 2.
40Latent MDP. Our construction has a single “known” latent MDP M ; that is, the only uncertainty in
lat
the family of rich-observation MDPs we construct arises from the emission processes. Set M ={M }.
lat lat
Set H =1 and A=[N]. We define the state space and latent transition dynamics as follows.
• The state space can be partitioned as S =S1,...,SN.
• Each block Si corresponds to a single state si with N actions denoted by ai, i∈[N].
• The initial state distribution is P (∅)=Unif(s1,...,sN).
lat,1
• The reward function is
R (si,aj)=I{j =i}. (24)
1
Informally, this construction can summarized as a contextual bandit (with uniform context distribution),
with a reward of 1 if and only if we play the action corresponding to the index of the context drawn.
Note that the total number of latent states in this construction is |S| = N and the number of actions is
|A|=N.
Observation space and decoder class. We define X = S so that |X| = |S|, and consider a class of
emission processes corresponding to deterministic maps. Let Σ denote the set of cyclic permutations on N
elements, excluding the identity permutation. That is, each σ ∈Σ takes the form
i
σ :k (cid:55)→k+i mod N, for i∈{1,...,N}.
i
For each σ ∈Σ, we consider the emission process
ψσ(·|si)=I (·)
sσ(i)
That is, ψσ shifts the context si according to σ. Let Ψ={ψσ |σ ∈Σ}. Consider the decoder class
Φ=Ψ−1 :=(cid:8) si (cid:55)→sψ−1(i) |ψ ∈Ψ(cid:9) ,
which has |Φ|=N. We consider the class of rich-observation MDPs given by
⟪M lat,Φ⟫:=(cid:8) Mi :=⟪M lat,ψσi⟫|σ
i
∈Σ(cid:9) . (25)
It is clear that this class of rich-observation MDPs satisfies the decodability assumption for emissions Ψ.
Sample complexity lower bound. To lower bound the sample complexity, we prove a lower bound on
the constrained PAC Decision-Estimation Coefficient (DEC) of [FGH23]. For an arbitrary MDP M (defined
over the space X) and ε∈[0,21/2], define18
dec (M,M)= inf sup (cid:8)E [JM(π )−JM(π)]| E (cid:2) D2(cid:0) M(π),M(π)(cid:1)(cid:3) ≤ε2(cid:9) ,
ε π∼p M π∼q H
p,q∈∆(Π)M∈M
where M(π) denotes the law over observations (x ,a ,r ) induced by executing the policy π in the MDP M,
1 1 1
JM(π) denotes the expected reward for policy π under M, and π denotes the optimal policy for M. We
M
further define
dec (M)=supdec (M,M),
ε ε
M
where the supremum ranges over all MDPs defined over X and A. We now appeal to the following technical
lemma.
Lemma D.2. For all ε2 ≥4/N, we have that sup dec (M,M)≥ 1.
M ε 2
18FormeasuresPandQ,wedefinesquaredHellingerdistancebyD2(P,Q)=(cid:82) (√ dP−√ dQ)2.
H
41InlightofLemmaD.2,itfollowsfromTheorem2.1inFosteretal.[FGH23]19 thatanyPACRLalgorithmthat
usesT episodesofinteractionforT log(T)≤c·N musthaveE[JM(π )−JM(π)]≥c′ foraworst-caseMDPin
M (cid:98)
M,wherec,c′ >0areabsoluteconstants. ThisimpliesthatanyPACRLwhichhasE[JM(π )−JM(π)]≤c′
M (cid:98)
must have T log(T)≥c·N and thus T ≥c·N/log(N).
Proof of Lemma D.2. Define M as the latent-space MDP that has identical dynamics to M but, has
lat lat
zero reward for every state-action pair, and define M :=⟪M ,id⟫ as the rich-observation MDP obtained
lat
by composing M with the identity emission process that sets x =s . In the rest of the proof, we use the
lat h h
shorthand ψ
i
:=ψσi. Observe that M and Mi, induce identical dynamics in observation space if rewards are
ignored, i.e. for all policies π :X →∆(A),
PM,π[(x ,a )=·]=PMi,π[(x ,a )=·]. (26)
1 1 1 1
It follows that for each i, for all policies π, we have
D2(cid:0) Mi(π),M(π)(cid:1)
H
=D2(cid:0)
(⟪M ,ψ ⟫)(π),(⟪M
,id⟫)(π)(cid:1)
H lat i lat
N
(cid:88) (cid:104) (cid:105)
= PM,π x =sψi(j),a =aj ·D2(I ,I )
1 1 H 1 0
j=1
N
(cid:88) (cid:104) (cid:105)
=2 PM,π x =sψi(j),a =aj
1 1
j=1
N
2 (cid:88) (cid:104) (cid:105)
= PM,π a =aj |x =sψi(j)
N 1 1
j=1
N
= N2 (cid:88) PM,π(cid:104) a
1
=aψ i−1(j) |x
1
=sj(cid:105)
j=1
since the learner receives identical feedback in the MDPs Mi and M unless they play the action a = aj
1
given observation x
1
=sψi(j) (corresponding to latent state si in Mi), in which case they receiver reward 1
in Mi but reward 0 in M. We now claim that for any q ∈∆(Π), there exists a set of at least N/2 indices
I ⊂[N] such that
q
E (cid:2) D2(cid:0) Mi(π),M(π)(cid:1)(cid:3) ≤ 4 (27)
π∼q H N
for all i∈I . To see this, note that by Eq. (22), we have
q
 
N N
E i∼Unif([N])E π∼q(cid:2) D H2(cid:0) Mi(π),M(π)(cid:1)(cid:3) ≤ E π∼q N2 (cid:88) N1 (cid:88) PM,π(cid:104) a 1 =aψ i−1(j) |x 1 =j(cid:105) 
j=1 i=1
 
N
2 (cid:88) 1 2
≤ E π∼q
N
N= N.
j=1
We conclude by Markov’s inequality that P (cid:2)E (cid:2) D2(cid:0) Mi(π),M(π)(cid:1)(cid:3) ≥4/N(cid:3) ≤1/2, giving I ≥
i∼Unif([N]) π∼q H q
N/2.
From Eq. (27), we conclude that for all ε2 ≥4/N,
(cid:110) (cid:104) (cid:105)(cid:111)
dec (⟪M ,Φ⟫,M)≥ inf inf sup E JMi(π )−JMi(π) .
ε lat π∼p Mi
q∈∆(Π)p∈∆(Π)i∈Iq
19Theorem2.1inFosteretal.[FGH23]isstatedwithrespecttosup M∈conv(M)decε(M,M),buttheactualproof(Section2.2)
givesastrongerresultthatscaleswithsup Mdecε(M,M).
42To lower bound this quantity, observe that for any index i and any policy π, we have
N
JMi(π Mi)−JMi(π)=1− N1 (cid:88) PM(i),π[a
1
=a(j) |x
1
=s(ψi(j))]
j=1
N
1 (cid:88)
=1−
N
PM,π[a
1
=a(j) |x
1
=s(ψi(j))]
j=1
N
=1− N1 (cid:88) PM,π(cid:104) a
1
=a(ψi−1(j)) |x
1
=s(j)(cid:105) ,
j=1
where the third inequality uses Eq. (26). We conclude that for any distribution p,q ∈∆(Π),
(cid:110) (cid:104) (cid:105)(cid:111)
sup E JMi(π )−JMi(π)
π∼p Mi
i∈Iq
(cid:110) (cid:104) (cid:105)(cid:111)
≥E E JMi(π )−JMi(π)
i∼Unif(Iq) π∼p Mi
N
≥1− N1 (cid:88) E i∼Unif(Iq)PM,π(cid:104) a
1
=a(ψi−1(j)) |x
1
=s(j)(cid:105)
j=1
N
=1− N1 (cid:88) |I1
|
(cid:88) PM,π(cid:104) a
1
=a(ψi−1(j)) |x
1
=s(j)(cid:105) ≥1− |I1
|
≥ 1
2
q q
j=1 i∈Iq
as long as N ≥4. Since this lower bound holds uniformly for all q,p∈∆(Π), we conclude that
1
dec (⟪M ,Φ⟫,M)≥ .
ε lat 2
43E Proofs for Section 3.3: Positive Results
This section is dedicated to the proof of our upper bound establishing that pushforward-coverable MDPs are
statistically modular (Theorem 3.2).
E.1 Proofs for Latent Model Class + Pushforward Coverability (Theorem 3.2)
In this section, we establish positive results under latent MDP classes which satisfy pushforward coverability.
We assume that every model in M satisfies pushforward coverability, defined as follows:
lat
Definition E.1 (Pushforward coverability). The pushforward coverability coefficient C for an MDP M
push
with transition kernel P is defined by
P (s′ |s,a)
C (M)= max inf sup h−1 . (28)
push h∈[H]µ∈∆(S)(s,a,s′)∈S×A×S µ(s′)
The pushforward coverability coefficient for an MDP class M is defined by
C (M)= max C (M).
push push
M∈M
Note that for any MDP M we always have
C (M,Π )≤C (M)|A|, (29)
cov rns push
where C is the state-action coverability coefficient (Definition C.3). Thus, an MDP with low pushforward
cov
coverability is also an MDP with low state-action coverability for all policies (upto a dependence on |A|).
We will show the show the following result.
Theorem 3.2 (Pushforward-coverable MDPs are statistically modular). Let M be a base MDP class such
lat
that each M ∈M has pushforward coverability bounded by C (M )≤C . Then, for any decoder
lat lat push lat push
class Φ, we have:
1. comp(M ,ε,δ)≤poly(C ,|A|,H,log|M
|,ε−1,log(cid:0) δ−1(cid:1)
), and
lat push lat
2. comp(⟪M ,Φ⟫,ε,δ)≤poly(C ,|A|,H,log|M
|,log|Φ|,ε−1,log(cid:0) δ−1(cid:1)
,loglog|S|).
lat push lat
The proof comes in three parts. We will firstly show that MDP that satisfies pushforward coverability admit
low-dimensional feature maps that can approximate Bellman backups (Appendix E.1.1), then establish that a
regret bound for the Golf algorithm [XFBJK23] under misspecification (Appendix E.1.2), and then combine
these ingredients (Appendix E.1.3).
E.1.1 A structural result: Pushforward-coverable MDPs are approximately low-rank
Our central technical result for this section is Lemma E.1, which is based on a variant of the Johnson-
Lindenstrauss lemma and establishes that under pushforward coverability, we can define a linear feature class
which satisfies an approximate form of Bellman completeness. We define the clipping operator via
clip (x):=max{min{x,2},0}.
[0,2]
Lemma E.1 (Existence of a low-dimensional embedding). Let M be a known MDP with reward function r,
transition kernel P, and pushforward coverability parameter C . Let µ={µ } denote its pushforward
push h h∈[H]
coverability distribution (i.e. the minimizer of Definition 3.3) and F ⊆ (S ×[H] → [0,1]) be an arbitrary
class of functions. Suppose that we sample W ∈{±1}d×S as a matrix of independent Rademacher random
variables, and define
1 (cid:16) (cid:17)
ψ (s,a)=r (s,a)⊕ √ W P (·|s,a)/µ1/2(·) ∈Rd+1
h h d h h ·∈S
44and
1 (cid:16) (cid:17)
w =1⊕ √ W µ1/2(·)f (·) ∈Rd+1.
f,h d h h+1 ·∈S
Then for any ε ∈(0,1), as long as we set
apx
C log(cid:0) 16|F|Hδ−1/ε (cid:1)
d≥29 push apx ,
ε
apx
we have that with probability at least 1−δ, for all f ∈F and h∈[H],
E
(cid:104)(cid:0)
clip [⟨w ,ψ (s,a)⟩−T f
(s,a)](cid:1)2(cid:105)
≤ε ,
µh⊗Unif(A) [0,2] f,h h h h+1 apx
as well as max ∥ψ (s,a)∥2 ≤ C (16log(|S||A|H)+11) and max ∥w ∥2 ≤ 16log(|F|H)+11. We
s,a,h h 2 push f,h f,h 2
emphasize that the feature map ψ ={ψ }H is oblivious to F, in the sense that it can be computed directly
h h=1
from M without any knowledge of F.
Proof of Lemma E.1. Fix h∈[H], whose dependence we omit for cleanliness. We begin by verifying that,
in expectation, ⟨w ,ψ(s,a)⟩ is equal to Tf(s,a). For this, note that
f
1(cid:88)d (cid:32) (cid:88) P(s′ |s,a)(cid:33)(cid:32) (cid:88) (cid:33)
⟨w ,ψ(s,a)⟩=r(s,a)+ W W µ1/2(s′′)f(s′′)
f d i,s′ µ1/2(s′) i,s′′
i=1 s′∈S s′′∈S
(cid:88) 1(cid:88)d (cid:88) (cid:88) P(s′ |s,a)
=r(s,a)+ P(s′ |s,a)f(s′)+ W W µ1/2(s′′)f(s′′).
d
i,s′
µ1/2(s′)
i,s′′
s′∈S i=1s′∈S s′′∈S
s′′̸=s′
Consequently, we have
(cid:12) (cid:12)
(cid:12) (cid:12)
|Tf(s,a)−⟨w f,ψ(s,a)⟩|=(cid:12) (cid:12) (cid:12) (cid:12)d1(cid:88)d (cid:88) (cid:88) W i,s′P µ(s 1/′ 2| (s s, ′)a) W i,s′′µ1/2(s′′)f(s′′)(cid:12) (cid:12) (cid:12) (cid:12). (30)
(cid:12) i=1s′∈S s′′∈S (cid:12)
(cid:12) s′′̸=s′ (cid:12)
Note that this remaining noise term is zero-mean – we will show in the sequel that it can be made small by
picking d appropriately. We next examine the norms of the vectors ψ(s,a) and w . Note that we have
f
∥ψ(s,a)∥2 = 1(cid:88)d (cid:32) (cid:88) W P(s′ |s,a)(cid:33)2
2 d i,s′ µ1/2(s′)
i=1 s′∈S
(cid:88) P2(s′ |s,a) 1(cid:88)d (cid:88) (cid:88) P(s′ |s,a)P(s′′ |s,a)
= + W W
µ(s′) d i,s′ i,s′′ µ1/2(s′) µ1/2(s′′)
s′∈S i=1s′∈S s′′∈S
s′′̸=s′
1(cid:88)d (cid:88) (cid:88) P(s′ |s,a)P(s′′ |s,a)
≤C + W W , (31)
push d i,s′ i,s′′ µ1/2(s′) µ1/2(s′′)
i=1s′∈S s′′∈S
s′′̸=s′
where we have used that
(cid:88) P2(s′ |s,a) (cid:88)
≤C P(s′ |s,a)=C
µ(s′) push push
s′∈S s′∈S
45by definition of pushforward coverability. Further note that we have
d (cid:32) (cid:33)2
∥w ∥2 = 1(cid:88) (cid:88) W µ1/2(s′)f(s′)
f 2 d i,s′
i=1 s′∈S
d
1(cid:88) (cid:88) (cid:88)
=E [f(s′)]+ W W µ1/2(s′)f(s′)·µ1/2(s′′)f(s′′)
s′∼µ
d
i,s′ i,s′′
i=1s′∈S s′′∈S
s′′̸=s′
d
1(cid:88) (cid:88) (cid:88)
≤1+ W W µ1/2(s′)f(s′)·µ1/2(s′′)f(s′′). (32)
d
i,s′ i,s′′
i=1s′∈S s′′∈S
s′′̸=s′
We will now appeal to the following technical lemma to upper bound Eq. (30), Eq. (31), and Eq. (32) by
establishing that the Rademacher noise terms concentrate to their expectations. The proof of the lemma will
be given in the sequel.
Lemma E.2. Let u,v ∈ Rn, and let W ∈ {±1}d×n have independent Rademacher entries. Then with
probability at least 1−δ,
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12)
(cid:12) (cid:12) (cid:12)d1 (cid:88) (cid:88) (cid:88) W i,jW i,ku jv
k(cid:12)
(cid:12) (cid:12) (cid:12)≤∥u∥ 2∥v∥
2·(cid:114)
32log d(2δ−1) +∥u∥2 2∥v∥2 2· 64log
d(cid:0) 2δ−1(cid:1)
. (33)
(cid:12) i∈[d]j∈[n]k∈[n] (cid:12)
(cid:12) k̸=j (cid:12)
Furthermore, for any set of vectors V ⊂Rn, we also have
1 max(cid:88) (cid:88) (cid:88)
W W v v ≤
max∥v∥2(16log|V|+9)+max∥v∥2·(cid:114)
32log(2δ−1)
+max∥v∥4·64log(cid:0) 2δ−1(cid:1)
.
d v∈V i,j i,k j k v∈V 2 v∈V 2 d v∈V 2 d
i∈[d]j∈[n]k∈[n]
k̸=j
Let (s,a)∈S×A and f ∈F. To bound |⟨ψ(s,a),w ⟩−Tf(s,a)| (cf. Eq. (30)), we apply the first bound of
f
Lemma E.2 with u=(cid:0) P(s′ |s,a)/µ1/2(s′)(cid:1) and v =(cid:0) µ1/2(s′)f(s′)(cid:1) , which gives
s′∈S s′∈S
(cid:114)
32C log(2δ−1)
log(cid:0) 2δ−1(cid:1)
|⟨ψ(s,a),w ⟩−Tf(s,a)|≤ push +64C :=ε(δ−1), (34)
f d push d
where we have again used that ∥u∥2 =(cid:80) P2(s′|s,a) ≤C and also that ∥v∥2 =1 since ∥f∥ ≤1 for all
2 s′∈S µ(s′) push 2 ∞
(cid:26)(cid:18) (cid:19) (cid:27)
f ∈F. ToboundEq. (31),weapplythesecondboundofLemmaE.2withV = Ph−1(s′|s,a) ,
µ h1/2(s′) s′∈S s,a∈S×A
h×[H]
which gives
(cid:114)
32log(2δ−1)
64log(cid:0) 2δ−1(cid:1)
max ∥ψ (s,a)∥2 ≤C (16log|S||A|H +9)+C +C2 :=B .
s,a∈S×A,h∈[H] h 2 push push d push d 1
(cid:26)(cid:16) (cid:17) (cid:27)
Lastly, to bound Eq. (32), we take V = µ1/2(s′)f (s′) in Lemma E.2, which establishes that
h h s′∈S f∈F
h∈[H]
(cid:114)
32log(2δ−1)
64log(cid:0) 2δ−1(cid:1)
max ∥w ∥2 ≤9+16log|F|H + + :=B .
f∈F,h∈[H] f,h 2 d d 2
Note that Eq. (34) establishes that the Bellman backup Tf(s,a) is well-approximated by ⟨ψ(s,a),w ⟩ only
f
at a single state-action pair (s,a). We can obtain an L -approximation guarantee by taking a union bound
∞
over S and A, which would incur a dependence on log|S| in the final sample complexity. Here, we bypass this
46by instead requiring only an approximation guarantee under the L (µ⊗Unif(A)) norm. Via (pushforward)
2
(cid:104) (cid:105)
coverability, this will ensure that Eπ (⟨w ,ψ(s,a)⟩−Tf(s,a))2 is well-controlled for all policies π, which
f
will be sufficient for our downstream sample-complexity analysis of Golf. However, directly establishing
an L (µ⊗Unif(A)) approximation guarantee is technically challenging since it would require establishing
2
a fourth-order (rather than second-order) equivalent of Eq. (33). The remainder of the proof will obtain
an L (µ⊗Unif(A)) approximation guarantee by instead sampling a dataset of size n from µ⊗Unif(A)
2
and taking a union bound over that dataset to ensure a uniform bound on all state-action pairs in that
dataset. Via an additional concentration bound, this will ensure that the error is well-behaved under the
L (µ⊗Unif(A)) norm.
2
For each h∈[H], sample a dataset D ={(s(i),a(i))}n i.i.d. from µ ⊗Unif(A). By a union bound over n,
h h i=1 h
F, and H, we have that
∀i∈[n],f ∈F,h∈[H]: (cid:12) (cid:12)(cid:10) ψ h(s( hi),a( hi)),w f,h(cid:11) −T hf h+1(s( hi),a( hi))(cid:12) (cid:12)≤ε(n|F|Hδ−1), (35)
where we recall the definition of ε(·) from Eq. (34). Now, let
(cid:0) (cid:1)2
X (s,a):= clip [⟨ψ (s,a),w ⟩]−T f (s,a) .
f,h [0,2] h f,h h h+1
Note that |X (s,a)|≤4 and
f,h
X (s,a)≤(⟨ψ (s,a),w ⟩−T f (s,a))2,
f,h h f,h h h+1
since T f (s,a)∈[0,2] and the clipping operator is 1-Lipshitz. Note that
h h+1
E [X (s,a)]:=E
(cid:104)(cid:0)
clip [⟨ψ (s,a),w ⟩]−T f
(s,a)(cid:1)2(cid:105)
,
(s,a)∼µh⊗Unif(A) f,h µh⊗Unif(A) [0,2] h f h h+1
where this expectation is only over the sampling of the data point (s,a) (and not the Rademacher matrix
W). Let
X :=X (s(i),a(i)).
i,f,h f,h h h
By boundedness of X (s,a) and Hoeffding’s inequality, we have that with probability at least 1−δ:
f,h
(cid:12) (cid:12)1 (cid:88)n (cid:12) (cid:12) (cid:114) log(2δ−1)
(cid:12) X −E [X (s,a)](cid:12)≤4 .
(cid:12)n i,f,h µ⊗Unif(A) f,h (cid:12) n
(cid:12) (cid:12)
i=1
Taking another union bound over F and H as well as the event in Eq. (35) gives that
(cid:12) (cid:12)1 (cid:88)n (cid:12) (cid:12) (cid:114) log(2|F|Hδ−1)
∀f ∈F,h∈[H]: (cid:12) X −E [X (s,a)](cid:12)≤4 , (36)
(cid:12)n i,f,h µ⊗Unif(A) f,h (cid:12) n
(cid:12) (cid:12)
i=1
and ∀i∈[n],f ∈F,h∈[H]: X ≤ε2(n|F|Hδ−1), (37)
i,f,h
recalling the definition of ε(·) from Eq. (34). Then, re-arranging Eq. (36) gives us that
E
(cid:104)(cid:0)
clip [⟨ψ (s ,a ),w ⟩]−T f (s ,a
)(cid:1)2(cid:105)
≤
1 (cid:88)n
X
+4(cid:114) log(2|F|Hδ−1)
µ⊗Unif(A) [0,2] h h h f h h+1 h h n i,f,h n
i=1
(cid:114)
log(2|F|Hδ−1)
≤ε2(n|F|Hδ−1)+4 , (38)
n
We now conclude the proof by picking n and d appropriately to ensure that the right-hand-side is bounded
by ε , which will ensure the desired claim that
apx
E
(cid:104)(cid:0)
clip [⟨ψ (s ,a ),w ⟩]−T f (s ,a
)(cid:1)2(cid:105)
≤ε .
µ⊗Unif(A) [0,2] h h h f h h+1 h h apx
47For convenience, we introduce absolute constants c and c′ whose precise values may change from line to line.
We pick n=64log(cid:0) 2|F|Hδ−1(cid:1) /ε2 . Plugging this into (38) gives
apx
E
(cid:104)(cid:0)
clip [⟨ψ (s ,a ),w ⟩]−T f (s ,a
)(cid:1)2(cid:105)
≤ε2(n|F|Hδ−1)+c·ε (39)
µ⊗Unif(A) [0,2] h h h f h h+1 h h
Noting that n≤128|F|Hδ−1 and plugging this into ε (Eq. (34)) gives
ε2
apx
(cid:114) 64log(16|F|Hδ−1/ε ) 128log(cid:0) 16|F|Hδ−1/ε (cid:1)
ε(n|F|Hδ−1)≤C1/2 apx +C apx . (40)
push d push d
Setting
C log(cid:0) 16|F|Hδ−1/ε (cid:1)
d≥29 push apx
ε
apx
ensures that
ε
ε2(n|F|Hδ−1)≤ε(n|F|Hδ−1)≤ apx (41)
2
by Eq. (40). Combining Eq. (38) and Eq. (41), we get
E
(cid:104)(cid:0)
clip [⟨ψ (s ,a ),w ⟩]−T f (s ,a
)(cid:1)2(cid:105)
≤ε , (42)
µ⊗Unif(A) [0,2] h h h f h h+1 h h apx
as desired. It only remains to establish the concentration results of Lemma E.2.
Proof of Lemma E.2. We establish the first claim. Let i∈[d] be fixed, and consider the random variable
(cid:88) (cid:88)
Z := W W v u .
i i,j i,k j k
j∈[n]k∈[n]
k̸=j
Note that E[Z ]=0 by independence of W and W for every j ̸=k. By Exercise 6.9 of Boucheron et al.
i i,j i,k
[BLM13], we have that
16λ2
logE[exp(λZ )]≤ ∥u∥2∥v∥2.
i 2(1−64∥u∥2∥v∥2λ) 2 2
2 2
Since Z are independent, it follows that
i
(cid:34) (cid:32) (cid:88)d (cid:33)(cid:35) 16λ2
logE exp λ Z ≤ ∥u∥2∥v∥2d.
i 2(1−64∥u∥2∥v∥2λ) 2 2
i=1 2 2
Hence, (cid:80)d Z is a sub-Gamma random variable with parameters ν = 16∥u∥2∥v∥2d and c = 64∥u∥2∥v∥2,
i=1 i 2 2 2 2
and it follows from Equation (2.5) on page 29 of Boucheron et al. [BLM13] that for all ε>0,
(cid:32) d √ (cid:33)
P (cid:88) Z ≥∥u∥ ∥v∥ 32dε+64∥u∥2∥v∥2ε ≤e−ε.
i 2 2 2 2
i=1
Taking a union bound, and using that the random variable is symmetric, we obtain the desired claim.
We now establish the second claim. Let V ⊂Rn be a subset of vectors. Let i∈[d] be fixed, and re-consider
the random variable
(cid:88) (cid:88)
Z := max W W v v .
i i,j i,k j k
v∈V
j∈[n]k∈[n]
k̸=j
48Again appealing to Exercise 6.9 of Boucheron et al. [BLM13], we have that
 
logE[exp(λ(Z i−E[Z i]))]≤ 2(1−16 6λ 42 Bλ)E  m v∈a Vx (cid:88) (cid:88) W i,jW i,kv j2v k2 

j∈[n]k∈[n]
k̸=j
 
16λ2 (cid:88)n
≤ 2(1−64Bλ)E m v∈a Vx v j2v k2 
j,k=1
16λ2
= max∥v∥4
2(1−64Bλ) v∈V 2
where B :=max ∥v∥4. Since Z are independent, it follows that
v∈V 2 i
logE(cid:34) exp(cid:32) λ(cid:88)d
(Z −E[Z
])(cid:33)(cid:35)
≤
16λ2
max∥v∥4d.
i i 2(1−64Bλ) v∈V 2
i=1
Hence,(cid:80)d Z isasub-Gammarandomvariablewithparametersν =16max ∥v∥4dandc=64max ∥v∥4,
i=1 i v∈V 2 v∈V 2
and it follows from Equation (2.5) on page 29 of Boucheron et al. [BLM13] that for all ε>0,
(cid:32) d (cid:114) (cid:33)
P
1(cid:88)
Z ≥ E[Z ]+max∥v∥2
32ε +64max∥v∥4ε
≤e−ε.
d i i v∈V 4 d v∈V 2d
i=1
To conclude, it remains only to show the bound E[Z ]≤max ∥v∥2(16log|V|+9). This follows by a standard
i v 2
log-sum-exp approach. Below, we abbreviate ρ :=W . We can observe that for any λ>0:
j i,j
 
E[Z i]=E max (cid:88) (cid:88) ρ jρ kv jv k 
v∈V 
j∈[n]k∈[n]
k̸=j
   
≤
λ1 log

(cid:88) E

exp
 λ
(cid:88) (cid:88)
ρ jρ kv jv
k






v∈V j∈[n]k∈[n]
k̸=j
    2
n
1 (cid:88) (cid:88)
≤ λlog  E exp λ ρ jv j     (43)
v∈V j=1
Note that X :=(cid:80) ρ v is subGaussian with parameter ∥v∥2, since:
j j j 2
E exp λ(cid:88)n
ρ jv
j 
=
(cid:89)n
E[exp(λρ jv j)]≤
(cid:89)n exp(cid:32) λ2 2v j2(cid:33) =exp(cid:18) λ 22
∥v∥2
2(cid:19)
.
j=1 j=1 j=1
Then, it follows (e.g. Lemma 1.12 of Rigollet et al. [RH23]) that X2−E[X2] satisfies a sub-exponential MGF
bound with parameter 16∥v∥2, i.e.
2
(cid:18) (cid:19)
E[exp(cid:0) λ(X2−E[X2])(cid:1) ]≤exp 256 λ2∥v∥4 ∀|λ|≤ 1 .
2 2 16∥v∥2
2
We also note that
n
(cid:88)
E[X2]= v v E[ε ε ]=∥v∥2.
i j i j 2
i,j=1
49Algorithm 2 GOLF [JLM21]
input: Function classes F and G, confidence width β >0.
initialize: F(0) ←F, D(0) ←∅ ∀h∈[H].
h
1: for episode t=1,2,...,T do
2: Select policy π(t) ←π f(t), where f(t) := argmax f∈F(t−1)f(x 1,π f,1(x 1)).
3: Execute π(t) for one episode and obtain trajectory (x(t),a(t),r(t)),...,(x(t),a(t),r(t)).
4: Update dataset: D(t) ←D(t−1)∪(cid:8)(cid:0) x(t),a(t),x(t) (cid:1)(cid:9) ∀1 h∈1 [H].1 H H H
h h h h h+1
Compute confidence set:
5:
(cid:26) (cid:27)
F(t) ← f ∈F :L(t)(f ,f )− min L(t)(g ,f )≤β ∀h∈[H] ,
h h h+1 h h h+1
gh∈Gh
(cid:88) (cid:16) (cid:17)2
where L(t)(f,f′):= f(x,a)−r−maxf′(x′,a′) , ∀f,f′ ∈F.
h
a′∈A
(x,a,r,x′)∈D(t)
h
6: end for
7: Output π (cid:98) =Unif(π(1:T)).
Adding and subtracting E[X2] in Eq. (43) gives
(cid:32) (cid:33)
≤
1
log
(cid:88) E(cid:2) exp(cid:0) λ(cid:0) X2−∥v∥2(cid:1) +λ∥v∥2(cid:1)(cid:3)
λ 2 2
v∈V
(cid:32) (cid:33)
=
1
log
(cid:88) E(cid:2) exp(cid:0) λ(cid:0) X2−∥v∥2(cid:1)(cid:1)(cid:3) exp(cid:0) λ∥v∥2(cid:1)
λ 2 2
v∈V
(cid:32) (cid:33)
≤
1
log
(cid:88) exp(cid:0) 128λ2∥v∥4+λ∥v∥2(cid:1)
∀|λ|≤
1
λ 2 2 16max ∥v∥2
v∈V v 2
1 1
≤ log|V|+max128λ∥v∥4+max∥v∥2 ∀|λ|≤
λ v 2 v 2 16max v∥v∥2
2
Picking λ= 1 concludes the proof.
16maxv∥v∥2
2
E.1.2 Golf with on-policy misspecification
Consider the version of Golf [JLM21] in Algorithm 2. We have the following guarantee for the regret of
Golf, which extends Jin et al. [JLM21] to allow for on-policy misspecification.
Lemma E.3. Suppose that QM o⋆ bs,⋆ ∈F and G satisfies ε apx-completeness in the sense that for all h∈[H]
and f ∈ F , there exists g ∈ G such that Eπ(cid:16) g−TM o⋆ bsf(cid:17)2 ≤ ε2 for all π ∈ Π := {π :f ∈F}. Let
h+1 h h apx F f
C :=C (M⋆ ,Π ) (Definition C.3). Then for an appropriate choice of β, Algorithm 2 ensures that
cov cov obs F
(cid:112) (cid:112)
Reg≤H C T log(|F||G|HT/δ)+HT C log(T)ε .
cov cov apx
(cid:104) (cid:105)
Proof of Lemma E.3. For each f ∈F , let apx[f ]=argmin sup Eπ (g −T f )2 . Let
h+1 h+1 h gh∈Gh π∈Π h h h+1
(cid:2) (cid:3)
δ h(t)(·,·):=f h(t)(·,·)−T ff h(t +) 1(·,·) & δ(cid:101) h(t)(·,·):=f h(t)(·,·)−apx f h(t +)
1
(·,·),
and note that by Jensen’s inequality we have that for all π, Eπ(cid:2) δ h(t)(·,·)(cid:3) ≤Eπ(cid:104) δ(cid:101) h(t)(·,·)(cid:105) +ε apx. We further
adopt the shorthand d(t)(x,a):=dπ(t)(x,a) and d˜(t)(x,a):=(cid:80) d(t)(x,a). As a consequence of realizability
h h h i<t h
(Q⋆ ∈ F ) and approximate Bellman completeness, standard concentration arguments (proved in the
obs,h h
sequel) lead to the following result.
50Lemma E.4 (Optimism and small in-sample squared Bellman errors). With probability at least 1−δ, by
taking β =clog(TH|F||G|/δ)+Tε , we have that for all t∈[T],
apx
(i) Q⋆ ∈F(t), and (ii)
(cid:88) d˜(t)(x,a)(cid:16) δ(cid:101)(t)(x,a)(cid:17)2
≤O(β).
obs,h h h
x,a
The rest of the proof proceeds similarly to the analysis of Section 3.2 in Xie et al. [XFBJK23]. Namely, by
optimism (Lemma E.4) and a standard Bellman error decomposition (Lemma B.6) we have
T H T H
Reg≤(cid:88)(cid:88) E d(t)(cid:2) δ h(t)(x,a)(cid:3) ≤TH ·ε apx+(cid:88)(cid:88) E d(t)(cid:104) δ(cid:101) h(t)(x,a)(cid:105) .
h h
t=1h=1 t=1h=1
Let us defining the burn-in time
τ (x,a)=min{t|d˜(t)(x,a)≥C µ⋆(x,a)},
h h cov h
where µ⋆ is the coverability distribution for the set of policies Π (i.e., the distribution µ⋆ that achieves
h F h
the minimum in the coverability definition). Using the same decomposition into “burn-in phase” and “stable
phase” in Xie et al. [XFBJK23], we have:
T H T H
(cid:88)(cid:88) (cid:104) (cid:105) (cid:88)(cid:88) (cid:104) (cid:105)
E
d(t)
δ(cid:101) h(t)(x,a) ≤2HC cov+ E
d(t)
δ(cid:101) h(t)(x,a)I{t≥τ h(x,a)} .
h h
t=1h=1 t=1h=1
Applying a change of measure argument on the second term then gives:
(cid:118) (cid:118)
(cid:88)T (cid:88)H E
d
h(t)(cid:104) δ(cid:101) h(t)(x,a)I{t≥τ h(x,a)}(cid:105) ≤H(cid:117) (cid:117) (cid:116)(cid:88)T (cid:88)(cid:0)I{t≥τ h d( ˜(x t), (a x) ,} ad )h(t)(x,a)(cid:1)2 (cid:117) (cid:117) (cid:116)(cid:88)T (cid:88) d˜ h(t)(x,a)(cid:16) δ(cid:101) h(t)(x,a)(cid:17)2
t=1h=1 t=1 x,a h t=1 x,a
(cid:124) (cid:123)(cid:122) (cid:125)(cid:124) (cid:123)(cid:122) (cid:125)
(A) (B)
By the same reasoning as in Xie et al. [XFBJK23], we have (A)≤O((cid:112) C log(T)), and by Lemma E.4 we
√ cov
have (B)≤O( βT). Using that β =log(TH|F|/δ)+Tε2 gives the desired result. It remains to establish
apx
the concentration results of Lemma E.4.
Proof of Lemma E.4. For any function f, define a random variable
X
(h,f)=(cid:0)
f (s(t),a(t))−r(t)−f (s(t)
)(cid:1)2 −(cid:0)
T f (s(t),a(t))−r(t)−f (s(t)
)(cid:1)2
.
t h h h h h+1 h+1 h h+1 h h h h+1 h+1
Let F ={s(i),a(i),r(i),...,s(i),a(i),r(i)} . Note that
t,h 1 1 1 H H H i<t
E(cid:2) r(t)+f (s(t) )|F (cid:3) =Eπ(t) [T f(s ,a )]. (44)
h h+1 h+1 t,h h h h
and thus that
(cid:104) (cid:105)
E[X (h,f)|F ]=Eπ(t) (f (s ,a )−T f (s ,a ))2 .
t t,h h h h h h h h
Next, note that
(cid:104) (cid:105)
Var[X (h,f)|F ]≤E (X (h,f))2 |F
t t,h t t,h
≤E(cid:104)(cid:0)
f (s(t),a(t))−T f
(s(t),a(t))(cid:1)2(cid:0)
f (s(t),a(t))+T f
(s(t),a(t))+2(cid:0)
r(t)−f (s(t)
)(cid:1)(cid:1)2
|F
(cid:105)
h h h h h h h h h h h h h h h h+1 h+1 t,h
≤16E(cid:104)(cid:0) f (s(t),a(t))−T f (s(t),a(t))(cid:1)2 |F (cid:105) =16E[X (h,f)|F ].
h h h h h h h t,h t t,h
By Freedman’s inequality (Lemma B.2, Lemma B.3), we have that with probability at least 1−δ:
(cid:12) (cid:12)  
(cid:12)(cid:88) (cid:88) (cid:12) (cid:115) (cid:88)
(cid:12)
(cid:12)
X i(h,f)− E[X i(h,f)|F i,h](cid:12) (cid:12)≤O log(1/δ) E[X i(h,f)|F i,h]+log(1/δ)
(cid:12) (cid:12)
i<t i<t i<t
51Taking a union bound over [T]×[H]×F, we have that for all t,h,f, with probability at least 1−δ:
(cid:12) (cid:12)  (cid:115) 
(cid:12)
(cid:12)
(cid:12)(cid:88)
X
i(h,f)−(cid:88) Eπ(i)(cid:104)
(f h(s h,a h)−T hf h(s h,a
h))2(cid:105)(cid:12)
(cid:12) (cid:12)≤O
ι(cid:88) Eπ(i)(cid:104)
(f h(s h,a h)−T hf h(s h,a
h))2(cid:105)
+ι,
(cid:12) (cid:12)
i<t i<t i<t
(45)
where ι=log(|F|HT/δ). We now show that
(cid:88) X (h,f(t))≤β+O(cid:0) Tε2 +ι(cid:1) =O(β), (46)
i apx
i<t
which will imply, from Eq. (45), that
(cid:88) Eπ(t)(cid:104)
(f (s ,a )−T f (s ,a
))2(cid:105)
≤O(ι+β)=O(β),
h h h h h h h
i<t
as desired. To see Eq. (46), let
∆
=(cid:88)(cid:0) apx(cid:2)
T f(t)
(cid:3)
(s(i),a(i))−r(i)−f(t) (s(i)
)(cid:1)2 −(cid:0)
T f(t)(s(i),a(i))−r(i)−f(t) (s(i)
)(cid:1)2
t h h+1 h h h h+1 h+1 h h h h h h+1 h+1
i<t
and then note that:
(cid:88)
X
(h,f(t))=(cid:88)(cid:0)
f(t)(s(i),a(i))−r(i)−f(t) (s(i)
)(cid:1)2 −(cid:0)
T f(t)(s(i),a(i))−r(i)−f(t) (s(i)
)(cid:1)2
i h h h h h+1 h+1 h h h h h h+1 h+1
i<t i<t
=(cid:88)(cid:0)
f(t)(s(i),a(i))−r(i)−f(i) (s(i)
)(cid:1)2 −(cid:0) apx(cid:2)
T f(t)
(cid:3)
(s(i),a(i))−r(i)−f(t) (s(i)
)(cid:1)2
+∆
h h h h h+1 h+1 h h+1 h h h h+1 h+1 t
i<t
≤(cid:88)(cid:0)
f(t)(s(i),a(i))−r(i)−f(t) (s(i)
)(cid:1)2
− inf
(cid:88)(cid:0)
g(s(i),a(i))−r(i)−f(t) (s(i)
)(cid:1)2
+∆
h h h h h+1 h+1 h h h h+1 h+1 t
i<t
gh∈Gh
i<t
≤β+∆ .
t
where the second-to-last line follows from apx(cid:2) T f(t) (cid:3) ∈G and the last line follows from the definition of the
h h+1
confidence set. It remains to show that ∆ ≤O(Tε2 +ι), which we do via a similar concentration argument.
t apx
Namely, let
Y
(h,f)=(cid:0)
apx[T f ](s(t),a(t))−r(t)−f(k) (s(t)
)(cid:1)2 −(cid:0)
T f (s(t),a(t))−r(t)−f(k) (s(t)
)(cid:1)2
,
t h h+1 h h h h+1 h+1 h h h h h h+1 h+1
and note that, as before,
(cid:104) (cid:105)
E[Y (h,f)|F ]=Eπ(t) (apx[T f ](s ,a )−T f (s ,a ))2 ,
t t,h h h+1 h h h h h h
and
Var[Y (h,f)|F ]≤16E[Y (h,f)|F ],
t t,h t t,h
by the same calculation as earlier. Thus, by Freedman’s inequality and a union bound, we have that, with
probability at least 1−δ,
(cid:12) (cid:12)
(cid:12) (cid:12)(cid:88)
Y
(h,f)−(cid:88) Eπ(t)(cid:104)
(apx[T f ](s ,a )−T f (s ,a
))2(cid:105)(cid:12)
(cid:12) (47)
(cid:12) t h h+1 h h h h h h (cid:12)
(cid:12) (cid:12)
i<t i<t
 
(cid:115)
≤O
ι(cid:88) Eπ(t)(cid:104)
(apx[T hf h+1](s h,a h)−T hf h(s h,a
h))2(cid:105)
+ι, (48)
i<t
where ι=log(|F|HT/δ). Recalling the misspecification assumption, this implies that
(cid:88) Y (h,f)≤O(cid:0) tε2 +ι(cid:1) ,
t apx
i<t
for all h,f,t, with high probability. This concludes the result for (ii). For (i), this follows identically to the
proof of Lemma 40 in Jin et al. [JLM21], since this only uses the property that Q⋆ ∈F.
52E.1.3 Sample-efficient latent-dynamics RL under pushforward coverability
We conclude by combining the previous two results to obtain the main result for this section.
Theorem 3.2 (Pushforward-coverable MDPs are statistically modular). Let M be a base MDP class such
lat
that each M ∈M has pushforward coverability bounded by C (M )≤C . Then, for any decoder
lat lat push lat push
class Φ, we have:
1. comp(M ,ε,δ)≤poly(C ,|A|,H,log|M
|,ε−1,log(cid:0) δ−1(cid:1)
), and
lat push lat
2. comp(⟪M ,Φ⟫,ε,δ)≤poly(C ,|A|,H,log|M
|,log|Φ|,ε−1,log(cid:0) δ−1(cid:1)
,loglog|S|).
lat push lat
Proof of Theorem 3.2. LetM⋆ :=⟪M⋆ ,ψ⋆⟫∈⟪M ,Φ⟫betheunknownlatent-dynamicsMDP.Define
obs lat lat
observation-level value functions
F ={QMlat,⋆◦ϕ|M
lat
∈M lat,ϕ∈Φ},
so that QM o⋆ bs,⋆ =QM l⋆ at,⋆◦ϕ⋆ ∈F via decoder and model realizability, and log|F h|≤ log|M lat||Φ|. Consider
any function class L ⊆ {S → [0,1]} and MDP M = (r ,P ). For a given value ε > 0, setting d
lat lat lat apx
according to Lemma E.1 implies that there exists a d-dimensional feature map φ (s,a)∈Rd+1 such that
Mlat,h
for all ℓ∈L and h∈[H], there exists w ∈Rd+1 such that
ℓ,h
E
(cid:20)(cid:16)
clip (cid:2)(cid:10) φ (s,a),w (cid:11)(cid:3) −TMlatℓ
(s,a)(cid:17)2(cid:21)
≤ε , (49)
µMlat⊗Unif(A) [0,2] Mlat,h ℓ,h h h+1 apx
whereµ isthepushforwardcoverabilitydistributionforM . Moreover,themapφ isexplicitlycomputed
Mlat lat h
as a function of M by a randomized algorithm with success probability 1−δ, with no knowledge of the
lat
class L required. We consider the class
(cid:40) (cid:41)
(cid:88)
L= Γ ϕ◦QMlat,⋆(s,a):= Γ ϕ(s′ |s)QMlat,⋆(s′,a)|ϕ∈Φ,M
lat
∈M
lat
, (50)
s′∈S
where Γ :S →∆(S) is the mismatch function for decoder ϕ and emission ψ⋆, defined in Definition 3.4. Note
ϕ
that L has size log|L|≤log|M ||Φ|, and that we have
lat
TM o⋆ bs(QMlat,⋆◦ϕ )(x,a)=TM l⋆ at(Γ ◦VMlat,⋆)(ϕ⋆(x),a)
h h h h ϕ,h+1 h h
by Lemma C.7. By Lemma C.1 we have that µ (x) = ψ⋆(x | ϕ⋆(x))µ (ϕ⋆(x)) is the coverability
distribution for MDP M⋆ , and
M o⋆ bs,h h h M l⋆ at,h h
obs
E [f(s,a)]=E [f(ϕ⋆(x),a)].
µ Ml⋆ at⊗Unif(A) µ Mo⋆ bs⊗Unif(A)
Now, define
(cid:110) (cid:111)
G = (x,a)(cid:55)→clip [⟨φ (ϕ(x),a),w⟩]|ϕ∈Φ,∥w∥2 ≤11+16log(|M ||Φ|H) .
Mlat,h [0,2] Mlat,h 2 lat
Recall the definition of w (for f : S ×A → [0,1]) from Lemma E.1, and note that by the norm bound
f
max ∥w ∥2 ≤11+16log(|M ||Φ|H) given by Lemma E.1, we have (x,a)(cid:55)→⟨φ (ϕ(x),a),w ⟩∈G
ℓ∈L ℓ 2 lat Mlat,h ℓ h
for every ℓ∈L. Next, note that by the norm bound max ∥ψ(s,a)∥2 ≤C (11+16log(|S||A|H)), given by
s,a 2 push
LemmaE.1,wehaveeveryg ∈G satisfies∥g ∥ ≤cC1/2 log(|M ||Φ||S||A|H):=B forsomeabsolute
h Mlat,h h ∞ push lat
constantc. Therefore,G Mlat,hhassizelog|G Mlat,h|≤O(cid:101)(d·log(B)+log|Φ|)=O(cid:101)(dloglog(|S|)+log|Φ|),where
theO(cid:101)notationignoreslogarithmicfactorsofC push,|A|,log|M lat|,andlog|Φ|.20 DefineG
h
=∪ Mlat∈MlatG Mlat,h,
which has size log|G h| ≤ log|M lat|+(O(cid:101)(dloglog(|S|)+log|Φ|)). Together, these results with Lemma E.1
imply that for all f ∈F , there exists g ∈G such that
h+1 h+1 h h
E (cid:20)(cid:16) g (x ,a )−(cid:104) TM o⋆ bsf (cid:105) (x ,a )(cid:17)2(cid:21) ≤ε .
µ Mo⋆ bs,h⊗Unif(A) h h h h h+1 h h apx
20Formally,thisrequiresastandardcoveringnumberargument;weomitthedetails.
53This, in turn, implies that for all π ∈Π we have
obs rns
Eπobs(cid:20)(cid:16)
g (x ,a
)−(cid:104)
TMo⋆
bsf
(cid:105)
(x ,a
)(cid:17)2(cid:21)
≤C |A|ε ,
h h h h h+1 h h push apx
since µ ⊗Unif(A) satisfies coverability (Definition C.3) with parameter C (M⋆ ,Π )≤C |A| (Eq.
(29)).
M o⋆ bs,h cov obs rns push
Then, it follows by Lemma E.3 that if we run Algorithm 2 with the classes F and G we will get
(cid:113) (cid:113)
Reg≤H C |A|T log(|M ||Φ|HT/δ)(dloglog(|S|)+log|Φ|)+HT C2 |A|2log(T)ε
push lat push apx
(cid:115) (cid:0) (cid:1)
log C2 |M ||Φ|2Hδ−1/ε loglog(|S|) (cid:113)
≤H C5 |A|T log(|M ||Φ|HT/δ) push lat apx +HT C2 |A|2log(T)ε
push lat ε push apx
apx
Choosing ε
apx
= √1 to balance leads to
T
(cid:113) (cid:113)
Reg≲HT3/4 C5 |A|log(|M ||Φ|HT/δ)log(cid:0) C2 |M ||Φ|2Hδ−1T(cid:1) loglog(|S|)+HT3/4 C2 |A|2log(T)
push lat push lat push
(cid:113)
≲HT3/4 C5 |A|2log(|M ||Φ|HT/δ)log(cid:0) TC2 |M ||Φ|2H/δ(cid:1) loglog(|S|),
push lat push lat
which gives a risk bound of
Risk≲ 1 H(cid:113) C5 |A|2log(|M ||Φ|HT/δ)log(cid:0) TC2 |M ||Φ|2H/δ(cid:1) loglog(|S|).
T1/4 push lat push lat
Equating this to ε gives a sample complexity of
T =poly(C ,A,H,log|M
|,log|Φ|,ε−1,log(cid:0) δ−1(cid:1)
,loglog(|S|)),
push lat
as desired. We have not made much effort to optimize the rate; in particular, a faster rate is possible by
using the Golf.Dbr algorithm of Amortila et al. [ACK24], which improves over the Golf algorithm when
there is misspecification.
54F Proofs and Additional Information for Section 4.2: Hindsight RL
This appendix contains additional information and proofs related to algorithmic modularity under hindsight
observations (Section 4.2), and is organized as follows:
• Appendix F.1 contains the pseudocode and proofs related to the online representation learning oracle
ExpWeights.Dr (Lemma 4.1).
• Appendix F.2 contains the proof for our risk bound of the O2L algorithm under hindsight observability
(Theorem 4.1).
F.1 Pseudocode and Proofs for ExpWeights.Dr (Lemma 4.1)
Algorithm 3 Derandomized Exponential Weights (ExpWeights.Dr)
input: Decoder set Φ
for t=1,2,··· ,T do
Get dataset (cid:8) x(i),ϕ⋆(x(i))(cid:9)
h h i∈[t−1],h∈[H]
for h=1,...,H do
For ϕ∈Φ, compute
(cid:32) t−1 (cid:33)
q(t)(ϕ )∝exp
−(cid:88) I(cid:2)
ϕ
(x(i))̸=ϕ⋆(x(i))(cid:3)
,
h h h h h h
i=1
and set
ϕ¯(t)(x)=argmaxP (ϕ (x)=s). (51)
h
s∈S
ϕh∼q h(t) h
end for
Return ϕ¯ (t) ={ϕ¯(t)}H .
h h=1
end for
The main result for this estimator is the following.
Lemma 4.1 (Online classification via ExpWeights.Dr). Under decoder realizability (ϕ⋆ ∈ Φ), Exp-
Weights.Dr (Algorithm 3) satisfies Assumption 4.2 with21
Est class(T)=O(cid:101)(Hlog|Φ|).
Proof of Lemma 4.1. For each h ∈ [H], consider the realizable online classification problem where
x(t) ∼dπ(t), for π(t) chosen adversarially, and y(t) =ϕ⋆(x(t)). Consider the exponential weights estimator
h h h h h
(cid:32) t−1 (cid:33)
q(t)(ϕ)∝exp
−(cid:88) I(cid:2) ϕ(x(i))̸=ϕ⋆(x(i))(cid:3)
.
h h h h
i=1
For every sequence (x(t))T , these distributions satisfy the deterministic regret bound
h t=1
T
(cid:88) (cid:104) (cid:104) (cid:105)(cid:105)
E
ϕ(cid:98)(t)∼q(t)
I ϕ(cid:98)( ht)(x h(t))̸=ϕ⋆ h(x( ht)) ≤2log|Φ|,
h h
t=1
by Corollary 2.3 of Cesa-Bianchi et al. [CBL06]. Taking conditional expectations over x(t) ∼dπ(t) and using
h h
Lemma B.3 gives that with probability at least 1−δ:
T
(cid:88) E ϕ(cid:98)(t)∼q(t)Eπ(t)(cid:104) I(cid:104) ϕ(cid:98)( ht)(x h)̸=ϕ⋆ h(x h)(cid:105)(cid:105) ≤4log|Φ|+8log(cid:0) 2δ−1(cid:1) .
h h
t=1
21Inthissection,thenotationsO(cid:101),≈,and≲ignoreonlyconstantsandlogarithmicfactorsofH.
55Taking a union bound over h∈[H] and summing over h∈[H] we obtain that with probability at least 1−δ:
T H
(cid:88)(cid:88) E ϕ(cid:98)(t)∼q(t)Eπ(t)(cid:104) I(cid:104) ϕ(cid:98) h(t)(x h)̸=ϕ⋆ h(x h)(cid:105)(cid:105) ≤4Hlog|Φ|+8Hlog(cid:0) 2Hδ−1(cid:1) .
h h
t=1h=1
Now, recall that at each time t, we define the improper decoder ϕ¯(t) via:
h
ϕ¯(t)(x)=argmaxP (ϕ(t)(x)=s) (52)
h ϕ(t)∼q(t) h
s∈S h h
Let ℓ (x ,q(t))=P (ϕ(t)(x )̸=ϕ⋆(x )). Note that ℓ satisfies
h h h ϕ(t)∼q(t) h h h h
h h
T H T H
(cid:88)(cid:88) E Eπ(t)(cid:2)I(cid:2) ϕ(t)(x )̸=ϕ⋆(x )(cid:3)(cid:3) =(cid:88)(cid:88) Eπ(t)E (cid:2)I(cid:2) ϕ(t)(x )̸=ϕ⋆(x )(cid:3)(cid:3) (53)
ϕ(t)∼q(t) h h h h ϕ(t)∼q(t) h h h h
h h h h
t=1h=1 t=1h=1
T H
=(cid:88)(cid:88) Eπ(t) [ℓ (x ,q(t))]. (54)
h h h
t=1h=1
By abuse of notation we also denote ℓ (x ,ϕ¯ )=I(cid:2) ϕ¯ (x)̸=ϕ⋆(x)(cid:3). We will show that
h h h h
∀x,t,h:ℓ (x ,ϕ¯(t))≤2ℓ (x ,q(t)), (55)
h h h h h h
from which we will obtain that with probability at least 1−δ:
T H
Reg (T)=(cid:88)(cid:88) Eπ(t)(cid:2)I(cid:2) ϕ¯(t)(x )̸=ϕ⋆(x )(cid:3)(cid:3) ≤8Hlog|Φ|+16Hlog(cid:0) 2Hδ−1(cid:1) .
class h h h h
t=1h=1
Integrating the high-probability regret bound gives
E[Reg (T)]=O(Hlog(H|Φ|)),
class
as desired. Towards establishing Eq. (55), let us fix x and let s denote the argmax in Eq. (52). There are
max
two cases:
• P (ϕ(t)(x)=s )≥ 1:
ϕ(t)∼q(t) h max 2
h h
→ If s =ϕ⋆(x), ℓ(x,ϕ¯(t))=0 so we are done.
max h
→ Otherwise, s ̸= ϕ⋆(x) and we have ℓ(x,ϕ¯(t)) = 1. However, since ϕ⋆(x) ̸= s we have
max h max
ϕ(t)(x)=s =⇒ ϕ(t)(x)̸=ϕ⋆(x) and so
h max h h
1 1
P (ϕ(t)(x)̸=ϕ⋆(x))≥P (ϕ(t)(x)=s )≥ = ℓ(x,ϕ¯(t)).
ϕ(t)∼q(t) h h ϕ(t)∼q(t) h max 2 2 h
h h h h
• P (ϕ(t)(x)=s )< 1:
ϕ(t)∼q(t) h max 2
h h
→ If s =ϕ⋆(x), ℓ(x,ϕ¯(t))=0 so we are done.
max h h
→ Otherwise, s ̸=ϕ⋆(x) and we have ℓ(x,ϕ¯(t))=1. However, by definition of s as the mode
max h max
we also have
1
P (ϕ(t)(x)=ϕ⋆(x))≤P (ϕ(t)(x)=s )< ,
ϕ(t)∼q(t) h h ϕ(t)∼q(t) h max 2
h h h h
so in particular we have
1 1
ℓ(x,q(t))=P (ϕ(t)(x)̸=ϕ⋆(x))> = ℓ(x,ϕ¯(t)).
h ϕ(t)∼q(t) h h 2 2 h
h h
56F.2 Proofs for O2L Under Hindsight Observability (Theorem 4.1)
Theorem 4.1 (Risk bound for O2L under hindsight observability). Let Alg be a base algorithm with base
lat
risk Risk (K), and Rep a representation learning oracle satisfying Assumption 4.2. Then Algorithm 1,
⋆ class
with inputs T,K,Φ, Rep , and Alg ,has expected risk
class lat
2K
E[Risk (TK)]≤Risk (K)+ Est (T).
obs ⋆ T class
Proof of Theorem 4.1. Let (ϕ(cid:98)(t))
t∈[T]
denote the decoders chosen by Rep class, and let ρ(t) denote the
distribution over decoders induced at time t from the interaction of Rep ,Alg , and M⋆ . Let π(t,k) :=
class lat obs obs
π(t,k)◦ϕ(cid:98)(t) and p(t,k) denote the distribution over (observation-space) policies played at epoch t and episode k,
lat obs
induced by the interaction of Rep ,Alg , and M⋆ . We adopt the notation π(t,K+1) :=π(t) ∼p(t,K+1)
class lat obs lat (cid:98)lat lat
for the final policy output by Alg in epoch t and (x(t,K+1),a(t,K+1),r(t,K+1)) for the trajectory collected
lat h h h
from that (observation-level) policy π (cid:98)l(t a) t◦ϕ(cid:98)(t). We firstly note that by assumption, we have the guarantee
(cid:34) T K+1 H (cid:35)
E (cid:88) (cid:88) (cid:88) E π(t,k)∼p(t,k)Eπ o( bt s,k)(cid:104) I(cid:104) ϕ(cid:98)( ht)(x h)̸=ϕ⋆ h(x h)(cid:105)(cid:105) ≤(K+1)Est class(T)≤2KEst class(T). (56)
obs obs
t=1 k=1 h=1
which follows by applying Assumption 4.2 to the distributions p¯(t) = 1 (cid:80)K+1p(t,k) and noting that
obs (K+1) k=1 obs
T H K+1
(cid:88)(cid:88) K1
+1
(cid:88) E π(t,k)∼p(t,k)Eπ o( bt s,k)(cid:104) I(cid:104) ϕ(cid:98) h(t)(x h)̸=ϕ⋆ h(x h)(cid:105)(cid:105)
obs obs
t=1h=1 k=1
T H
=(cid:88)(cid:88) E π¯(t)∼p¯(t)Eπ¯ o( bt s)(cid:104) I(cid:104) ϕ(cid:98)( ht)(x h)̸=ϕ⋆ h(x h)(cid:105)(cid:105) ≤Est class(T).
obs obs
t=1h=1
Let Risk(K,Alg lat,ϕ,M o⋆ bs) = JM o⋆ bs(π M⋆
⋆
)−JM o⋆ bs(π (cid:98)lat◦ϕ) be the random variable denoting the risk of
the final policy output by Alg after o Kbs rounds of interaction with M⋆ when given feature ϕ in any
lat obs
epoch t. For any ϕ:X →S, let E denote the law over trajectories (x(k),a(k),r(k)) and policies
ϕ h h h k∈[K+1],h∈[H]
(π(k)◦ϕ) generatedafterK roundsofinteractionwhenAlg isgivenfeatureϕinanyepoch. (Recall
lat k∈[K+1] lat
that, for all of the above definitions, a new instance of Alg is initialized at every epoch, so we do not have
lat
to specify which epoch it is, only the current feature ϕ). Finally, let G be the “good” event
t
(cid:110) (cid:111)
G
t
= ∀k ∈[K+1],∀h∈[H]: ϕ(cid:98) h(t)(x h(t,k))=ϕ⋆ h(x h(t,k)) .
Recallthat,inanyroundt,Alg
lat
onlyobservesthelatent(“compressed”)trajectories(ϕ(cid:98) h(t)(x h(t,k)),a h(t,k),r h(t,k))
as history for choosing policies. We can therefore conclude that, when ϕ(cid:98)(t)(x(t,k)) = ϕ⋆(x(t,k)) for all
h h
k ∈[K+1],h∈[H], the distribution over final policies π(t) chosen by Alg will be identical as if we had
(cid:98)lat lat
chosen ϕ⋆ as our decoder. In particular, this implies
(cid:104) (cid:105)
E
ϕ(cid:98)(t)
I{G t}Risk(K,Alg lat,ϕ(cid:98)(t),M o⋆ bs) =E ϕ⋆[I{G t}Risk(K,Alg lat,ϕ⋆,M o⋆ bs)]
≤Risk (K), (57)
⋆
where the second line simply follows by removing the indicator function, recalling that Risk (K) =
⋆
E[Risk(K,Alg ,M⋆ )], and using that Risk(K,Alg ,ϕ⋆,M⋆ )=Risk(K,Alg ,M⋆ ).
lat lat lat obs lat lat
57Then, we have:
T
1 (cid:88) (cid:104) (cid:104) (cid:105)(cid:105)
E[Risk obs(TK)]=
T
E
ϕ(cid:98)(t)∼ρ(t)
E
ϕ(cid:98)(t)
Risk(K,Alg lat,ϕ(cid:98)(t),M o⋆ bs)
t=1
T
1 (cid:88) (cid:104) (cid:104) (cid:105)(cid:105)
≤
T
E
ϕ(cid:98)(t)∼ρ(t)
E
ϕ(cid:98)(t)
I{G t}Risk(K,Alg lat,ϕ(cid:98)(t),M o⋆ bs)
t=1
T
1 (cid:88) (cid:104) (cid:105)
+ E E [I{¬G }]
T ϕ(cid:98)(t)∼ρ(t) ϕ(cid:98)(t) t
t=1
T T
1 (cid:88) 1 (cid:88)
≤ Risk (K)+ P(¬G )
T ⋆ T t
t=1 t=1
T
1 (cid:88)
=Risk (K)+ P(¬G ),
⋆ T t
t=1
where the first equality applies the tower rule for conditional expectation, the second equality applies linearity
of conditional expectations and the upper bound Risk(K,Alg lat,ϕ(cid:98)(t),M o⋆ bs)≤1, and the third lines applies
the upper bound Eq. (57).It remains to bound the last term. Here, note that by a union bound,
(cid:34)K+1 H (cid:35)
P(¬G t)≤E
(cid:88) (cid:88)
E
π(t,k)∼p(t,k)Eπ(t,k)I(cid:110)
ϕ(cid:98)(t)(x h(t,k))̸=ϕ⋆(x(
ht,k))(cid:111)
,
k=1 h=1
where we have used that trajectory k in round t is sampled from policy π(t,k), which is in turn sampled from
p(t,k). Summing over t and using the bound in Eq. (56) concludes the proof.
58G Proofs for Section 4.3: Self-Predictive Estimation
This appendix contains additional information and proofs related to algorithmic modularity under self-
predictive estimation (Section 4.3), and is organized as follows:
• Appendix G.1 contains the pseudocode and proofs related to the online representation learning oracle
SelfPredict.Opt (Lemma 4.2).
• AppendixG.2containstheproofforourriskboundoftheO2Lalgorithmunderself-predictiveestimation
(Theorem 4.2).
G.1 Pseudocode and Proofs for SelfPredict.Opt (Lemma 4.2)
The pseudocode for our self-predictive estimation procedure is given in Algorithm 4.
Algorithm 4 Optimistic Self-Predictive Latent Model Estimation (SelfPredict.Opt)
1: input: Decoder set Φ, Latent model class M lat, Mismatch-complete class L lat, Optimism parameter γ
2: Set β := 1 2(cid:112) CcovHlog(T)/T
3: for t=1,2,··· ,T do
4: Get dataset D(t) ={x( hi),a( hi),r h(i),x( hi +) 1} i∈[t−1],h∈[H]
Compute
5:
(cid:40) H n
M(cid:99)(t)◦ϕ(cid:98)(t) = argmax (γβ)−1J M(π M)+(cid:88)(cid:88) log(cid:0) [M h◦ϕ h](r h(i),ϕ h+1(x( hi +) 1)|x( hi),a( hi))(cid:1) (58)
[M◦ϕ]∈Mlat◦Φ h=1i=1
n (cid:41)
− max (cid:88) log(cid:0) [M′ ◦ϕ′](r(i),ϕ (x(i) )|x(i),a(i))(cid:1) . (59)
h h h h+1 h+1 h h
[M′◦ϕ′]∈Llat◦Φ
i=1
(cid:110) (cid:111)
6: Return ϕ(cid:98)(t) = ϕ(cid:98)(t) .
h
h∈[H]
7: end for
Our main result concerning the SelfPredict.Opt estimator for online optimistic self-predictive estimation
is the following. We recall our notation for the instantaneous self-prediction error
[∆ (M ,ϕ)](x ,a ):=D2(cid:0) M (ϕ (x ),a ),(cid:2) ϕ ♯M⋆ (cid:3) (x ,a )(cid:1) .
h lat h h H lat,h h h h h+1 obs,h h h
Lemma 4.2 (Optimisticself-predictiveestimationviaSelfPredict.Opt). Let Π denote the set of policies
lat
played by Alg , and C =C (M⋆ ,Γ ◦Π ) be the state coverability parameter on M⋆ over the
lat cov,st cov,st lat Φ lat lat
set of stochastic policies Γ ◦Π (Eq. (10)). Then, for any γ >0, under decoder realizability (ϕ⋆ ∈Φ), base
Φ lat
model realizability (M⋆ ∈M ), and mismatch function completeness with class L (Assumption 4.4), the
lat lat lat
estimator in Algorithm 4 with inputs Φ,M ,L , and γ satisfies Assumption 4.3 with22
lat lat
(cid:18)(cid:113) (cid:19)
Est self;opt(T,γ)=O(cid:101) HC cov,st|A|T log(|M lat||L lat||Φ|) .
Proof of Lemma 4.2. We will firstly establish that the algorithm obtains low offline estimation error.
Lemma G.1 (SelfPredict.Opt attains low offline estimation error). For any γ > 0, under decoder
realizability (ϕ⋆ ∈Φ), model realizability (M⋆ ∈M ), and mismatch function completeness with class L
lat lat lat
(Assumption 4.4), the estimator in Algorithm 4 with inputs Φ, M , L , and γ satisfies that for all t∈[T],
lat lat
22Inthissection,thenotationsO(cid:101)and≲ignoresconstantsandlogarithmicfactorsof: H,Ccov,st,|A|,T,andlog(|Mlat||Llat||Φ|).
59with probability at least 1−δ,
H t−1
(cid:88)(cid:88) E π(i)∼p(i)Eπ(i)(cid:104) [∆ h(M(cid:99)(t),ϕ(cid:98)(t))](x h,a h)(cid:105) +γ−1(cid:16) JM l⋆ at(π
M l⋆
at)−JM(cid:99)(t) (π M(cid:99)(t))(cid:17)
h=0i=1
≤O(cid:0) log(cid:0) |M ||L ||Φ|HTδ−1(cid:1)(cid:1) . (60)
lat lat
Given this result, we can appeal to offline-to-online conversions to establish the final result. Let C :=
cov
C (M⋆ ,Π ◦Φ) denote the (state-action) coverability coefficient in M⋆ over the set of policies Π ◦Φ.
cov obs lat obs lat
Note that by Lemma C.1 we have C (M⋆ ,Π ◦ Φ) = C and therefore by Lemma C.4 we
cov,st obs lat cov,st
have C (M⋆ ,Π ◦ Φ) ≤ C |A|. Let η > 0 be a parameter to be chosen later, and β =
cov obs lat cov,st off
O(cid:0) log(cid:0) |M ||L ||Φ|HTδ−1(cid:1)(cid:1) be the offline estimation error guaranteed by Lemma G.1. We abbrevi-
lat lat
ate α:=(cid:112) C covHlog(T), Ep(t) [·]:=E π(t)∼p(t)Eπ(t) [·], and Ep(cid:101)(t) :=(cid:80) it =− 11E π(i)∼p(i)Eπ(i) [·]. Then, we have:
T H
(cid:88)(cid:88) Ep(t)(cid:104) [∆ h(M(cid:99)(t),ϕ(cid:98)(t))](x h,a h)(cid:105) +γ−1(cid:16) JM l⋆ at(π
M l⋆
at)−JM(cid:99)(t) (π M(cid:99)(t))(cid:17)
t=1h=1
(cid:118)
(cid:117) T H T
≤α(cid:117) (cid:116)(cid:88)(cid:88) Ep(cid:101)(t)(cid:104) [∆ h(M(cid:99)(t),ϕ(cid:98)(t))](x h,a h)(cid:105) +O(HC cov)+γ−1(cid:88)(cid:16) JM l⋆ at(π
M l⋆
at)−JM(cid:99)(t) (π M(cid:99)(t))(cid:17)
t=1h=1 t=1
(cid:32) T H (cid:33) T
≤α η
2
(cid:88)(cid:88) Ep(cid:101)(t)(cid:104) [∆ h(M(cid:99)(t),ϕ(cid:98)(t))](x h,a h)(cid:105) + 21
η
+γ−1(cid:88)(cid:16) JM l⋆ at(π
M l⋆
at)−JM(cid:99)(t) (π M(cid:99)(t))(cid:17) +O(HC cov)
t=1h=1 t=1
where in the first inequality we have used Lemma B.7 with g h(t) =∆ h(M(cid:99)(t),ϕ(cid:98)(t)) and in the second inequality
we have used the AM-GM inequality with parameter η. Collecting terms, we proceed via:
T (cid:32) H (cid:33)
= α 2η (cid:88) (cid:88) Ep(cid:101)(t)(cid:104) ∆ h(M(cid:99)(t),ϕ(cid:98)(t))(x h,a h)(cid:105) +(γη 2α )−1(cid:16) JM l⋆ at(π
M l⋆
at)−JM(cid:99)(t) (π M(cid:99)(t))(cid:17) + 2α
η
+O(HC cov)
t=1 h=1
αη α
≤ Tβ + +O(HC )
off cov
2 2η
(cid:18)(cid:113) (cid:19)
≤O C |A|Hlog(T)Tβ +HC |A|
cov,st off cov,st
(cid:18)(cid:113) (cid:19)
≤O HC |A|T
log(T)log(cid:0)
|M ||L
||Φ|HTδ−1(cid:1)
,
cov,st lat lat
whereinthefirstinequalitywehaveusedLemmaG.1andthedefinitionofγ inAlgorithm4(cf. Eq. (58))and
in the second inequality we have chosen η =1/√ T to balance the terms and used the bound C cov ≤C cov,st|A|.
We convert to an expected regret bound by picking δ appropriately, which gives the final result. It remains to
show Lemma G.1.
Proof of Lemma G.1. Fix an iteration t∈[T], and abbreviate M(cid:99):=M(cid:99)(t) and ϕ(cid:98):=ϕ(cid:98)(t). We follow the
analysis of maximum likelihood estimation from Geer; Zhang; Agarwal et al. [Gee00; Zha06; AKKS20]. In
particular, we quote Lemma 24 of [AKKS20], which in an abstract conditional estimation framework with
density class F states the following.
Lemma G.2 (Lemma 24 of Agarwal et al. [AKKS20]). Let D = {(x ,y )} be a dataset collected with
i i
x ∼p(i)(x ,y ) and y ∼f⋆(·|x ), L(f,D)=(cid:80)n ℓ(f,(x ,y )) be any loss function that decomposes
i 1:i−1 1:i−1 i i i=1 i i
additively, f(cid:98):D →F be an estimator, D′ be a tangent sequence D′ ={(x (cid:101)i,y (cid:101)i)} sampled independently via
x ∼p(i)(x ,y ) and y ∼f⋆(·|x ). Then, with probability at least 1−δ, we have
(cid:101)i 1:i−1 1:i−1 (cid:101)i (cid:101)i
−logE D′exp(cid:16) L(f(cid:98)(D),D′)(cid:17) ≤−L(f(cid:98)(D),D)+log(cid:0) |F|δ−1(cid:1) , (61)
60For our purposes, we have that F =M ◦Φ, the data distribution is collected adaptively (for each h∈[H])
lat
via π(i) ∼p(i), x(i),a(i) ∼dM o⋆ bs,π(i), and r(i),x(i) ∼M⋆ (·|x(i),a(i)). For the loss function L, we take
h h h h h+1 obs h h
L((M,ϕ),D)=−(cid:88)H (cid:88)t log(cid:32) [MM o⋆
◦bs
ϕ(r h( ]i
+
()
r1
(, i)ϕ
,h
ϕ+1(x ((
h
xi +)
(i1
))| )x |(
h
xi) (, i)a ,(
h
ai) () i))(cid:33)
−
γ− 21
(JM l⋆ at(π
M l⋆
at)−JM(π M)).
h=0i=1 h h h h+1 h+1 h h
We begin by upper bounding the quantity −L((M(cid:99),ϕ(cid:98))(D),D) appearing on the right-hand side of Eq. (61),
Tor oweq au rdiv sa tl hen ist ,ly nol to ew te hr ab tounding L((M(cid:99),ϕ(cid:98))(D),D). Let us abbreviate V(cid:98) = JM(cid:99)(π M(cid:99)) and V⋆ = JM l⋆ at(π
M l⋆
at).
H t
(cid:88)(cid:88) (cid:16)(cid:104) (cid:105) (cid:17)
L((M(cid:99),ϕ(cid:98))(D),D)= log M(cid:99)h◦ϕ(cid:98)h (r h(i),ϕ(cid:98)h+1(x( hi +) 1)|x( hi),a( hi))
h=0i=1
(cid:88)H (cid:88)t (cid:16) (cid:17) γ−1
− log M o⋆ bs(r h(i),ϕ(cid:98)h+1(x( hi +) 1)|x( hi),a( hi)) +
2
(V(cid:98) −V⋆)
h=0i=1
H t
(cid:88)(cid:88) (cid:16)(cid:104) (cid:105) (cid:17)
≥ log M(cid:99)h◦ϕ(cid:98)h (r h(i),ϕ(cid:98)h+1(x( hi +) 1)|x( hi),a( hi))
h=0i=1
(cid:88)H (cid:88)t (cid:16) (cid:17) γ−1
− h=0[M′◦ϕm ′]a ∈x
Llat◦Φ
i=1log [M h′ ◦ϕ′ h](r h(i),ϕ(cid:98)h+1(x( hi +) 1)|x( hi),a( hi)) +
2
(V(cid:98) −V⋆)
H t
≥(cid:88)(cid:88) log(cid:0)(cid:2) M⋆ ◦ϕ⋆(cid:3) (r(i),ϕ⋆ (x(i) )|x(i),a(i))(cid:1)
lat,h h h h+1 h+1 h h
h=0i=1
−(cid:88)H
max
(cid:88)t
log(cid:0) [M′ ◦ϕ′](r(i),ϕ⋆ (x(i) )|x(i),a(i))(cid:1) +
γ−1
(V⋆−V⋆)
h=0[M′◦ϕ′]∈Llat◦Φ
i=1
h h h h+1 h+1 h h 2
H t
=(cid:88)(cid:88) log(cid:0)(cid:2) M⋆ ◦ϕ⋆(cid:3) (r(i),ϕ⋆ (x(i) )|x(i),a(i))(cid:1)
lat,h h h h+1 h+1 h h
h=0i=1
H t
−(cid:88) max (cid:88) log(cid:0) [M′ ◦ϕ′](r(i),ϕ⋆ (x(i) )|x(i),a(i))(cid:1) ,
h h h h+1 h+1 h h
h=0[M′◦ϕ′]∈Llat◦Φ
i=1
where in the second line we have used Lemma C.8 with Assumption 4.4 and in the third line we have used
the ERM property of M(cid:99)◦ϕ(cid:98)together with decoder and model realizability. We claim that this implies
L((M(cid:99),ϕ(cid:98))(D),D)≥−log(cid:0) |L lat◦Φ|Hδ−1(cid:1) (62)
by concentration. Indeed, for each h∈[H],i∈[t], and [M′◦ϕ′]∈L ◦Φ, let
lat
(cid:32) (cid:33)
Z[M′◦ϕ′] =−1
log
M o⋆ bs(r h(i),ϕ⋆ h+1(x( hi +) 1)|x( hi),a( hi))
i,h 2 [M′◦ϕ′](r(i),ϕ⋆ (x(i) )|x(i),a(i))
h h+1 h+1 h h
Applying Lemma B.1, we have that
(cid:88)t log(cid:32) M o⋆ bs(r h(i),ϕ⋆ h+1(x( hi +) 1)|x( hi),a( hi)) (cid:33)
[M′◦ϕ′](r(i),ϕ⋆ (x(i) )|x(i),a(i))
i=1 h h+1 h+1 h h
≥(cid:88)t −2log(cid:32) E Eπ(i)(cid:34) exp(cid:32) −1 log(cid:32) M o⋆ bs(r h(i),ϕ⋆ h+1(x( hi +) 1)|x( hi),a( hi)) (cid:33)(cid:33)(cid:35)(cid:33) −log(cid:0) δ−1(cid:1) , (63)
π(i)∼p(i) 2 [M′◦ϕ′](r(i),ϕ⋆ (x(i) )|x(i),a(i))
i=1 h h+1 h+1 h h
with probability at least 1 − δ, where we have recalled that data is gathered adaptively according to
π(i) ∼p(i).We now quote the following lemma from Zhang; Agarwal et al. [Zha06; AKKS20].
61Lemma G.3 (Lemma 25 of Agarwal et al. [AKKS20]). For any D ∈∆(X) and p,q ∈[X →∆(Y)], we have
(cid:18) (cid:19)
−2logE x∼D,y∼q(·|x)exp −1 2log(q(y|x)/p(y|x)) ≥E x∼D(cid:2) D H2(q(·|x),p(·|x))(cid:3)
Proof of Lemma G.3. We include the proof for completeness. The result follows via the following steps.
(cid:18) (cid:19)
1 (cid:112)
−2logE x∼D,y∼q(·|x)exp − 2log(q(y|x)/p(y|x)) =−2logE
x∼D,y∼q(·|x)
p(y|x)/q(y|x)
(cid:16) (cid:112) (cid:17)
≥2 1−E
x∼D,y∼q(·|x)
p(y|x)/q(y|x) (∀x:log(x)≤x−1)
(cid:104) (cid:16) (cid:112) (cid:17)(cid:105)
=E
x∼D
2 1−E
y∼q(·|x)
p(y|x)/q(y|x)
=E (cid:2) D2(p(·|x),q(·|x))(cid:3)
x∼D H
By Lemma G.3, we have that the right-hand-side of Eq. (63) is further lower bounded by
(cid:88)t log(cid:32) M o⋆ bs(r h(i),ϕ⋆ h+1(x( hi +) 1)|x( hi),a( hi)) (cid:33)
[M′◦ϕ′](r(i),ϕ⋆ (x(i) )|x(i),a(i))
i=1 h h+1 h+1 h h
t
≥(cid:88) E Eπ(i)(cid:2) D2(cid:0) ϕ⋆ ♯M⋆ (·|x(i),a(i)),[M′◦ϕ′](·|x(i),a(i))(cid:1)(cid:3) −log(cid:0) δ−1(cid:1)
π(i)∼p(i) H h+1 obs h h h h
i=1
≥−log(cid:0) δ−1(cid:1)
,
where the last line follows from the non-negativity of squared Hellinger. Taking a union bound over
M′◦ϕ′ ∈L ◦Φ and h∈[H] gives the desired lower bound in Eq. (62).
lat
To conclude the proof, it remains to lower bound the left-hand side in Eq. (61). Here, note that:
(cid:16) (cid:17) γ−1
−logE D′exp L((M(cid:99),ϕˆ)(D),D′) +
2
(V⋆−V(cid:98))
   
=−logE D′exp−1
2
h(cid:88)H =1(cid:88) i=t 1log(cid:104) M(cid:99)M ho ◦⋆ bs ϕ(cid:98)( hr (cid:101) (cid:105)h(i () r, h(ϕ(cid:98) i)h ,+ ϕ(cid:98)1 h( +x (cid:101) 1( h (i +) x1
(
hi) +)| 1)x( h |i) x,
(
ha i)( h ,i) a)
(
hi))
   
=− h(cid:88)H =1(cid:88) i=t 1logE π(i)∼p(i)Eπ(i) exp−1 2log(cid:104) M(cid:99)M ho ◦⋆ bs ϕ(cid:98)( hr (cid:105)h(i () r, h(ϕ(cid:98) i)h ,+ ϕ(cid:98)1 h( +x 1( h (i +) x1
(
hi) +)| 1)x( h |i) x,
(
ha i)( h ,i) a)
(
hi)), (64)
where we have used that in the “tangent sequence” D′ the current sample (r(i),x(i) ) is independent of
(cid:101)h (cid:101)h+1
(r(i),x(i) ). To bound this term, we again appeal to Lemma G.3, concluding that
h h+1
   
− h(cid:88)H =1(cid:88) i=t 1logE π(i)∼p(i)Eπ(i) exp−1 2log(cid:104) M(cid:99)M ho ◦⋆ bs ϕ(cid:98)( hr (cid:105)h(i () r, h(ϕ(cid:98) i)h ,+ ϕ(cid:98)1 h( +x 1( h (i +) x1
(
hi) +)| 1)x( h |i) x,
(
ha i)( h ,i) a)
(
hi))
H t
≥
21 (cid:88)(cid:88)
E
π(i)∼p(i)Eπ(i)(cid:104)
D
H2(cid:16)
[M(cid:99)h◦ϕ(cid:98)h](x h,a h),ϕ(cid:98)h+1♯M o⋆ bs(x h,a
h)(cid:17)(cid:105)
h=1i=1
Combining everything, we have:
(cid:32) H t (cid:33)
1
2
(cid:88)(cid:88)
E
π(i)∼p(i)Eπ(i)(cid:104)
D
H2(cid:16)(cid:104) M(cid:99)h◦ϕ(cid:98)h(cid:105)
(x h,a h),ϕ(cid:98)h+1♯M o⋆ bs(x h,a
h)(cid:17)(cid:105)
+γ−1(V⋆−V(cid:98))
h=1i=1
≤log(cid:0)
|L
||Φ|Hδ−1(cid:1) +log(cid:0)
|M
||Φ|δ−1(cid:1)
lat lat
62Taking an additional union bound over t∈[T], we have that with probability at least 1−δ:
H t
(cid:88)(cid:88) E
π(i)∼p(
hi)Eπ(i)(cid:104) D H2(cid:16)(cid:104) M(cid:99)h◦ϕ(cid:98)h(cid:105) (x h,a h),ϕ(cid:98)h+1♯M o⋆ bs(x h,a h)(cid:17)(cid:105) +γ−1(cid:16) JM l⋆ at(π
M l⋆
at)−JM(t) (π M(t))(cid:17)
h=1i=1
≤O(cid:0) log(cid:0)
|M ||L
||Φ|HTδ−1(cid:1)(cid:1)
,
lat lat
for all t∈[T], as desired.
Corollary4.2(AlgorithmicmodularityviaSelfPredict.Opt). UnderthesameconditionsasinLemma4.2,
and for any base algorithm Alg , O2L with inputs T,K,Φ,SelfPredict.Opt, and Alg achieves
lat lat
K (cid:113)
E[Risk (TK)]≲c ·Risk (K)+c γ· √ HC |A|log(|M ||L ||Φ|)+c γ−1·KH,
obs 1 base 2 cov,st lat lat 3
T
for absolute constants c ,c ,c . Consequently, for any Alg with base risk Risk (K), setting γ and T
1 2 3 lat base
appropriately gives
E[Risk (TK)]≲Risk (K),
obs base
with a number of trajectories TK =O(cid:101)(K5H3Ccov,st|A|log2(|Mlat||Llat||Φ|)/(Riskbase(K))4).
Proof of Corollary 4.2. The first inequality simply follows by plugging the bound of Est from
self;opt
Lemma 4.2 into Theorem 4.2. For the second inequality, let ∆ = c (cid:112) HC |A|log(|M ||L ||Φ|).
2 cov,st lat lat
The result follows by setting γ s.t. c γ−1HK = Risk (K) i.e. γ = c KH , and T such that
3 base 3Riskbase(K)
γ √K T∆ =Risk base(K) i.e. T = RiK sk4 ba∆ se2 (γ K2
)2
= (RiK sk4 b∆ ase2 (H K2 ))4. Then the result follows by direct substitution and by
noting that K ≤1 since Risk (K)≤1.
T base
G.2 Proofs for Main Risk Bound (Theorem 4.2)
Our main risk bound (Theorem 4.2) follows as a special case of a more general theorem (Theorem G.1),
which holds for algorithm that satisfies a property we refer to as CorruptionRobust-ness (Definition H.2). We
now state the more general theorem, postponing its proof (and a formal definition of corruption robustness)
until Appendix H.
Theorem G.1 (Risk bound for O2L under self-predictive estimation and CorruptionRobustness). Assume
Rep satisfies Assumption 4.3 with parameter γ > 0 and that M is realizable (i.e. M⋆ ∈ M ).
self;opt lat lat lat
Furthermore, let Alg be CorruptionRobust (Definition H.2) with parameter α. Then, O2L (Algorithm 1)
lat
with inputs T,K,Φ,Alg , and Rep has expected risk
lat self;opt
E[Risk (TK)]≤c ·Risk (K)+c γ· K Est (T,γ)+c γ−1·(cid:0) α2+H(cid:1) (65)
obs 1 base 2 T self;opt 3
for absolute constants c ,c ,c >0.
1 2 3
Our main risk bound (Theorem 4.2) follows from the following lemma, which establishes that any Alg
lat
is CorruptionRobust in the sense of Definition H.2 for a sufficiently large corruption robustness parameter.
Below,foranyPOMDPM(cid:102)overstate-actionspaceS×A,wewriteM(cid:102)(s 1:h,a 1:h)fortheconditionalprobability
over reward r
h
and s
h+1
given s 1:h,a 1:h, i.e. M(cid:102)h(s 1:h,a 1:h)=M(cid:102)h(r h,s
h+1
=·|s 1:h,a 1:h).
63Lemma G.4. Let M⋆ be any reference MDP and M(cid:102) be any POMDP with the same state and action space.
Then for any algorithm Alg , we have
lat
EM(cid:102),Alg
lat[Risk (K)]≤c
EM⋆,Alg
lat[Risk (K)]
M⋆ 1 M⋆
(cid:34) K H (cid:35)
+c 2EM(cid:102),Alg lat (cid:88)(cid:88) EM(cid:102),π(k)(cid:104) D H2(cid:16) M h⋆(s h,a h),M(cid:102)h(s 1:h,a 1:h)(cid:17)(cid:105) ,
k=1h=1
where c ,c > 0 are absolute constants. In particular, Alg is CorruptionRobust (Definition H.2) with
√1 2 lat
α=c KH.
2
Proof of Lemma G.4. Let us abbreviate Alg := Alg . For i ∈ [K], let τ(i) denote the trajectory
lat
(s(i),a(i),r(i),...,s(i),a(i),r(i)). Let P:=PM⋆,Alg denote the law of {(π(i),τ(i))} under Alg in the true
1 1 1 H H H i∈[K]
MDP M⋆, and Q:=PM(cid:102),Alg denote the law of {(π(i),τ(i))} under Alg under the POMDP M(cid:102). Let us
i∈[K]
write M⋆(π) and M(cid:102)(π) for the laws of trajectory τ sampled from policy π in M⋆ or M(cid:102)respectively. Let π
(cid:98)
denotethepolicyoutputbythealgorithmafterK roundsofinteractionwiththeenvironment. ByLemmaB.5
we have
(cid:104) (cid:105) (cid:104) (cid:105) (cid:16) (cid:17)
EM(cid:102),Alg JM⋆ (π )−JM⋆ (π) ≤3EM⋆,Alg JM⋆ (π )−JM⋆ (π) +4D2 PM⋆,Alg ,PM(cid:102),Alg .
M⋆ (cid:98) M⋆ (cid:98) H
BythesubadditivitypropertyforsquaredHellingerdistance(LemmaB.4)appliedtothesequenceπ(1),τ(1),...,π(K),τ(K),
we have
(cid:34) K
D2(cid:16) PM⋆,Alg ,PM(cid:102),Alg(cid:17) ≤7EM(cid:102),Alg (cid:88) D2(P(π(k) |π(1:k−1),τ(1:k−1)),Q(π(k) |π(1:k−1),τ(1:k−1)))+
H H
k=1
(cid:35)
D2(P(τ(k) |π(1:k),τ(1:k−1)),Q(τ(k) |π(1:k),τ(1:k−1)))
H
(cid:34) K (cid:35)
=7EM(cid:102),Alg (cid:88) D2(P(τ(k) |π(1:k),τ(1:k−1)),Q(τ(k) |π(1:k),τ(1:k−1)))
H
k=1
(cid:34) K (cid:35)
=7EM(cid:102),Alg (cid:88) D2(cid:16) M⋆(π(k)),M(cid:102)(π(k))(cid:17)
H
k=1
(cid:34) K H (cid:35)
≤49EM(cid:102),Alg (cid:88)(cid:88) EM(cid:102),π(k)(cid:104) D H2(cid:16) M h⋆(s h,a h),M(cid:102)h(s 1:h,a 1:h)(cid:17)(cid:105)
k=1h=1
where in the second step we have used that P(π(k) |π(1:k),τ(1:k−1))=Q(π(k) |π(1:k),τ(1:k−1)) since the histories
are equivalent, in the third step we have used that the trajectories are generated by the MDP/PODMP
M⋆ and M(cid:102), respectively, in the fourth step we have again applied the subadditivity property of the squared
Hellinger distance (Lemma B.4) to the sequence (s ,a ,r ,...,s ,a ,r ).
1 1 1 H H H
Theorem4.2(RiskboundforO2Lunderself-predictiveestimation). Suppose Rep satisfiesAssumption
self;opt
4.3 with parameter γ >0. Then Algorithm 1, with inputs T,K,Φ, Rep , and Alg has expected risk
self;opt lat
K
E[Risk (TK)]≤c ·Risk (K)+c γ· Est (T,γ)+c γ−1·KH,
obs 1 base 2 T self;opt 3
for absolute constants c ,c ,c >0.
1 2 3
√
Proof of Theorem 4.2. This follows from Theorem G.1 as well as Lemma G.4, by taking α=c KH
2
and simplifying.
64H Additional Results for Section 4.3: Self-Predictive Estimation
This section contains a more general result for algorithmic modularity under self-predictive estimation
(Theorem G.1), from which our main result is derived as a special case, along with associated background,
applications, and proofs. This section is organized as follows.
• Appendix H.1 presents: definitions for the ϕ-compressed POMDP and CorruptionRobust algorithms
(Appendix H.1.1), statements for properties of the ϕ-compressed dynamics (Appendix H.1.2). The risk
bound for O2L under self-predictive estimation and CorruptionRobustness (Theorem G.1) is given in
Appendix H.1.3, and a statement that the Golf algorithm is CorruptionRobust (Appendix H.1.4).
• Appendix H.2 presents for the proofs for the properties of the ϕ-compressed POMDPs.
• Appendix H.3 presents a proof for the risk bound of O2L under self-predictive estimation and
CorruptionRobustness.
• Appendix H.4 presents a proof that the Golf algorithm is CorruptionRobust.
H.1 O2L with Self-predictive Estimation and CorruptionRobust Base Algorithms
H.1.1 Definitions: ϕ-compressed POMDP and CorruptionRobustness
Consider iteration k ∈ [K] of epoch t ∈ [T] within O2L. Suppose that RepLearn has chosen decoder
ϕ=ϕ(t) :X →S. Then, the latent algorithm has observed the data D(t,k) ={ϕ(x(t,k)),a(t,k),r(t,k),ϕ(x(t,k))}
h h h h+1
collected from the preceding policies in the epoch: π(t,1)◦ϕ(t),...,π(t,k−1)◦ϕ(t) (Line 8). Due to possible
lat lat
inaccuracies in the decoder ϕ, the dataset D(t,k) may not be generated from a Markovian process and must
instead be viewed as being generated from a PODMP, formally defined as follows.
DefinitionH.1(ϕ-compressedPOMDP). Theϕ-compressedPOMDPM(cid:102)⋆ inducedbyM⋆ andϕisdefinedby:
ϕ obs
1. Latent state space X
2. Action space A
3. Observation state space S
4. Latent reward functions R⋆ :X ×A→[0,1]
obs,h
5. Latent dynamics P⋆ :X ×A→∆(X)
obs,h
6. (Deterministic) observation function O :X →S defined by O (x)=ϕ (x),
h h h
7. Horizon H
8. Initial latent distribution P⋆ (x |∅)
obs 0
Note that the latent space for the POMDP is the observation space of the latent-dynamics MDP M⋆ , and
obs
vice-versa; we adopt this terminology because—from the perspective of the base algorithm, the observations
x can be viewed as a Markovian (yet partially observed process) that generates the learned states ϕ(x )
h h
on which the algorithm acts. We write P(cid:101)πlat := PM(cid:102)ϕ⋆,πlat for the probability distribution over trajectories
ϕ
(x ,s ,a ,r ) in the ϕ-compressed POMDP when playing policy π :S×[H]→∆(A), where x ∈X
h h h h h∈[H] lat h
are the POMDP’s latent states, s ∈ S are the observed states, and a ∈ A are the actions. We let
h h
E (cid:101)π ϕlat := EM(cid:102)ϕ⋆,πlat denote the corresponding expectation. We write P(cid:101)ϕ,h(s
h+1
| s 1:h,a 1:h) = P(cid:101) ϕπlat(s
h+1
|
s 1:h,a 1:h) and r˜ ϕ,h(r
h
|s 1:h,a 1:h)=P(cid:101) ϕπlat(r
h
|s 1:h,a 1:h) for the conditional distributions of next states and
rewards given the first h state-action pairs, which are policy-independent. We also write M(cid:102) ϕ⋆(r h,s
h+1
|
s 1:h,a 1:h) = r˜ ϕ,h(r
h
| s 1:h,a 1:h)P(cid:101)ϕ,h(s
h+1
| s 1:h,a 1:h) for the joint one-step probability. We will abbreviate
M(cid:102) ϕ⋆(s 1:h,a 1:h):=M(cid:102) ϕ⋆(r h,s
h+1
=·|s 1:h,a 1:h).
65Note that for any π lat, P(cid:101) ϕπ ,l hat(s
h+1
|s h,a h) is a well-defined (Markovian, policy-dependent) probability kernel,
which is equivalent to
(cid:88)
P(cid:101) ϕπ ,l hat(s
h+1
|s h,a h)= P(cid:101) ϕπ ,l hat(s 1:h−1,a
1:h−1
|s h,a h)P(cid:101)ϕ,h(s
h+1
|s 1:h,a 1:h) (66)
s1:h−1,a1:h−1
=E (cid:101)π ϕlat(cid:104) P(cid:101)ϕ,h(s
h+1
|s 1:h,a 1:h)|s h,a h(cid:105) (67)
Similarly, r˜πlat(r |s ,a ) is a Markovian and policy-dependent reward distribution which is equivalent to
ϕ,h h h h
(cid:88)
r˜ ϕπ ,l hat(r
h
|s h,a h)= P(cid:101) ϕπ ,l hat(s 1:h−1,a
1:h−1
|s h,a h)r˜ ϕ,h(r
h
|s 1:h,a 1:h) (68)
s1:h−1,a1:h−1
=E (cid:101)π ϕlat[r˜ ϕ,h(r
h
|s 1:h,a 1:h)|s h,a h]. (69)
Finally, we let
M(cid:102) ϕπ ,l hat,⋆(r h,s
h+1
|s h,a h)=E (cid:101)π ϕlat(cid:104) M(cid:102) ϕ⋆(r h,s
h+1
|s 1:h,a 1:h)|s h,a h(cid:105) (70)
denote the associated one-step model over joint rewards and transitions.
Our CorruptionRobustness condition asserts that the agent—when observing data from the ϕ(t)-compressed
dynamics M(cid:102) ϕ⋆—attains a risk bound for M
lat
which is proportional to its risk when observing data from M
lat
itself, plus a term that captures the degree of misspecification between M(cid:102) ϕ⋆ and M lat.
Definition H.2 (CorruptionRobust algorithm). We say that Alg is CorruptionRobust with parameters
lat
α and Risk if there exists a constant c such that, for any (ϕ,M )∈Φ×M , we have
base 1 lat lat
EM(cid:102)ϕ⋆,Alg lat[Risk(K,Alg lat,M lat)]≤c 1·Risk base(K)
(cid:118) 
(cid:117) K H
+αEM(cid:102)ϕ⋆,Alg lat(cid:117) (cid:116)(cid:88)(cid:88) E π(k)∼p(k)E (cid:101)π ϕl( ak t)(cid:104) D H2(cid:16) M lat,h(s h,a h),M(cid:102) ϕ⋆ ,h(s 1:h,a 1:h)(cid:17)(cid:105) ,
lat
k=1h=1
where we recall the definition of the random variable Risk(K,Alg ,M ) from Eq. (1), the expectation
lat lat
EM(cid:102)ϕ⋆,Alg lat denotes the interaction protocol of Alg
lat
in the ϕ-compressed dynamics M(cid:102) ϕ⋆, and p(k) denotes
the randomization distribution over latent policies that Alg plays.
lat
H.1.2 Basic properties of the ϕ-compressed dynamics (Definition H.1)
We establish a number of basic properties for the ϕ-compressed POMDP and their relation to the self-
prediction guarantee obtained by Rep . These properties are proved in Appendix H.2. Firstly, we have
self;opt
the following change-of-measure lemma:
Lemma H.1 (Change of measure lemma). For any ϕ ∈ Φ, f ∈ [S ×A → [0,1]], h ∈ [H], and π ∈
lat
[S×[H]→∆(A)], we have:
E (cid:101)π ϕlat[f(s h,a h)]=Eπlat◦ϕ[[f ◦ϕ](x h,a h)]. (71)
Thenextlemmastatesthatthekernelsoftheϕ-compressedPOMDParewell-approximatedbythe(Markovian)
latent model fit by Rep . We recall the instantaneous self-prediction error
self;opt
[∆ (M ,ϕ)](x ,a ):=D2(cid:0) M (ϕ (x ),a ),(cid:2) ϕ ♯M⋆ (cid:3) (x ,a )(cid:1) .
h lat h h H lat,h h h h h+1 obs,h h h
Lemma H.2 (Near-markovianity of the ϕ-compressed dynamics). For any decoder ϕ, base model M , and
lat
policy π :S×[H]→∆(A), we have:
lat
H H
(cid:88) E (cid:101)π ϕlat(cid:104) D H2(cid:16) M lat,h(s h,a h),M(cid:102) ϕ⋆ ,h(s 1:h,a 1:h)(cid:17)(cid:105) ≤(cid:88) Eπlat◦ϕ[[∆ h(M lat,ϕ)](x h,a h)]. (72)
h=0 h=0
66Furthermore, we also have
H H
(cid:88) E (cid:101)π ϕlat(cid:104) D H2(cid:16) M lat,h(s h,a h),M(cid:102) ϕ⋆ ,, hπlat(s h,a h)(cid:17)(cid:105) ≤(cid:88) Eπlat◦ϕ[[∆ h(M lat,ϕ)](x h,a h)]. (73)
h=0 h=0
A corollary is the following lemma establishing errors between expectations under M , the model estimated
lat
by Rep self;opt, and those under the ϕ-compressed POMDP M(cid:102) ϕ⋆.
Lemma H.3 (Simulationlemma). For any latent model M with Markovian transition kernel {P } ,
lat lat,h h∈[H]
latent policy π :S×[H]→∆(A), and decoder ϕ∈Φ, we have that for all f :S×A→[0,1]:
lat
|EMlat,πlat[f(s h,a h)]−E (cid:101)π ϕlat[f(s h,a h)]|≤ (cid:88) Eπlat◦ϕ(cid:104)(cid:13) (cid:13)[P lat◦ϕ] h(x h′,a h′)−ϕ h+1♯P o⋆ bs,h(x h′,a h′)(cid:13) (cid:13) tv(cid:105) , (74)
h′<h
and thus for any sequence of policies π(t), latent models M(t), and decoders ϕ(t), we have:
lat lat
(cid:118)
(cid:88)T (cid:88)H |EM l( at t),π l( at t) [f(s h,a h)]−E (cid:101)π ϕl (( a tt t )) [f(s h,a h)]|≤H√ TH(cid:117) (cid:117) (cid:116)(cid:88)T (cid:88)H Eπ l( at t)◦ϕ(t)(cid:2) [∆ h(M l( at) t,ϕ(t))](x h,a h)(cid:3) .
t=1h=0 t=1h=0
H.1.3 Risk bound for O2L under CorruptionRobustness
We state the main risk bound for O2L under self-predictive estimation and the above definition of corruption
robustness.
Theorem G.1 (Risk bound for O2L under self-predictive estimation and CorruptionRobustness). Assume
Rep satisfies Assumption 4.3 with parameter γ > 0 and that M is realizable (i.e. M⋆ ∈ M ).
self;opt lat lat lat
Furthermore, let Alg be CorruptionRobust (Definition H.2) with parameter α. Then, O2L (Algorithm 1)
lat
with inputs T,K,Φ,Alg , and Rep has expected risk
lat self;opt
E[Risk (TK)]≤c ·Risk (K)+c γ· K Est (T,γ)+c γ−1·(cid:0) α2+H(cid:1) (65)
obs 1 base 2 T self;opt 3
for absolute constants c ,c ,c >0.
1 2 3
H.1.4 Examples of CorruptionRobust algorithms
Inthissection,weestablishthattheGolfalgorithmsatisfiestheCorruptionRobustdefinition(DefinitionH.2)
with a parameter α≈K−1/2. This improves upon the rate that would be obtained by invoking the generic
guarantee in Lemma G.4. We expect that several other algorithms can be analyzed in a similar way, thereby
leading to tight rates in the same fashion. We restate the pseudocode in Algorithm 5 for convenience.
LetM
lat
=(r lat,P lat)begiven,andweletQ⋆
lat
:=QMlat,⋆,andT lat,hf(s,a):=r lat,h(s,a)+E s′∼Plat,h(s,a)[V f(s′)].
We assume that the algorithm has a latent function class F which realizes Q⋆ , as well as a helper class
alg lat
G which is T -complete for F .
alg lat alg
Assumption H.1 (T -completeness). We have:
lat
Q⋆ ∈F , and T F ⊆G .
lat alg lat alg alg
For our analysis of Golf, it is most natural to quantify the corruption levels in the following way.
Assumption H.2 (Corruption levels of M
lat
and M(cid:102) ϕ⋆). Let ε2
rep
be such that, for any sequence of policies
π(k) played by the algorithm when interacting with the ϕ-compressed POMDP, we have
lat
(cid:88)K (cid:88)H E
π l( ak t)∼p( lak
t)E (cid:101)π ϕl( ak t)(cid:20) (r lat,h(s h,a h)−r˜ ϕπ ,( hk) (s h,a h))2+(cid:13) (cid:13) (cid:13)P lat,h(s h,a h)−P(cid:101) ϕπ ,( hk) (s h,a h)(cid:13) (cid:13) (cid:13)2 tv(cid:21) ≤ε2 rep. (75)
k=1h=1
67Algorithm 5 GOLF [JLM21]
input: Function classes F and G, confidence width β >0.
initialize: F(0) ←F, D(0) ←∅ ∀h∈[H].
h
1: for episode t=1,2,...,T do
2: Select policy π(t) ←π f(t), where f(t) := argmax f∈F(t−1)f(x 1,π f,1(x 1)).
3: Execute π(t) for one episode and obtain trajectory (x(t),a(t),r(t)),...,(x(t),a(t),r(t)).
4: Update dataset: D(t) ←D(t−1)∪(cid:8)(cid:0) x(t),a(t),x(t) (cid:1)(cid:9) ∀1 h∈1 [H].1 H H H
h h h h h+1
Compute confidence set:
5:
(cid:26) (cid:27)
F(t) ← f ∈F :L(t)(f ,f )− min L(t)(g ,f )≤β ∀h∈[H] ,
h h h+1 h h h+1
gh∈Gh
(cid:88) (cid:16) (cid:17)2
where L(t)(f,f′):= f(x,a)−r−maxf′(x′,a′) , ∀f,f′ ∈F.
h
a′∈A
(x,a,r,x′)∈D(t)
h
6: end for
7: Output π (cid:98) =Unif(π(1:T)).
We note that
K H (cid:20) (cid:18) (cid:19)(cid:21)
ε2
rep
≲(cid:88)(cid:88) E (cid:101)π ϕl( ak t) D H2 M lat,h(s h,a h),M(cid:102) ϕ⋆ ,, hπ l( ak t) (s h,a h)
k=1h=1
K H
≤(cid:88)(cid:88) E (cid:101)π ϕl( ak t)(cid:104) D H2(cid:16) M lat,h(s h,a h),M(cid:102) ϕ⋆ ,h(s 1:h,a 1:h)(cid:17)(cid:105)
k=1h=1
by the data-processing inequality (cf. Eq. (87) and Eq. (77)) and the inequality ∥p−q∥2 ≤D2(p,q), and
tv H
thus a CorruptionRobustness bound in terms of ε implies a CorruptionRobustness bound in the sense of
rep
Definition H.2.
Theorem H.1 (Latent GOLF is CorruptionRobust). Under Assumption H.1 and Assumption H.2, Algo-
rithm 5 with β
=c(cid:0) log(cid:0) |F||G|KHδ−1(cid:1)
+ε
(cid:1)
, has regret
rep
(cid:88)K (cid:16) (cid:112) (cid:17) (cid:16) (cid:113) (cid:17)
JMlat(π Mlat)−JMlat(π(k))≤O H C covKlog(K)log(|F||G|HK/δ) +O H3/2 KC covlog(K)ε2
rep
,
k=1
and consequently is CorruptionRobust (Definition H.2) with parameters
H3/2(cid:112) (cid:18) H (cid:112) (cid:19)
α= √ C log(K) and Risk (K)=O √ C log(K)log(|F||G|HK) .
cov base cov
K K
Corollary H.1 (GolfappliedinO2L). Let us suppose that the appropriate assumptions for the estimator in
√
(cid:0) (cid:1)
Algorithm 4 to have regret bounded by Est (T,γ)=O HC T log(C |M ||L ||Φ|HT) (Lemma 4.2)
self cov cov lat lat
hold. Then, we can take γ ≈ K−1/2 and T ≈ K4, and the bound Theorem G.1 gives an expected risk of ε
with a number of trajectories TK =poly(C cov,H,log|M lat|,log|Φ|,log|L lat|)·1/ε10, improving over the 1/ε14
rate of the universal result (Corollary 4.2).
H.2 Proofs for Appendix H.1.2: Properties of ϕ-compressed POMDPs
Lemma H.1 (Change of measure lemma). For any ϕ ∈ Φ, f ∈ [S ×A → [0,1]], h ∈ [H], and π ∈
lat
[S×[H]→∆(A)], we have:
E (cid:101)π ϕlat[f(s h,a h)]=Eπlat◦ϕ[[f ◦ϕ](x h,a h)]. (71)
68Proof of Lemma H.1. Recall that P(cid:101) ϕπlat denotes the law of (x h,s h,a h)
h∈[H]
in the ϕ-compressed POMDP
when playing policy π . For clarity, and to differentiate a random variable from its realization, in the proofs
lat
below we will use upper-case notation such as {S =s ,A =a ,X =x } to indicate realizations
h h h h h h
of random variables in the POMDP.
Let d˜π hlat(s,a) = P(cid:101) ϕπlat(S
h
= s,A
h
= a) be the marginalized occupancy measure for in the ϕ-compressed
POMDP M(cid:102)⋆. We write dπlat◦ϕ :=dM o⋆ bs,πlat◦ϕ. The left-hand side in Eq. (71) is equal to:
ϕ h h
E (cid:101)π ϕlat[f(s h,a h)]= (cid:88) d˜π hlat(s,a)f(s,a),
s∈S,a∈A
Meanwhile, the right-hand side is equal to:
(cid:88) (cid:88)
Eπlat◦ϕ[[f ◦ϕ](x ,a )]= f(s,a) dπlat◦ϕ(x,a).
h h h h
s∈S,a∈A x:ϕ(x)=s
So it only remains to show that, for each s ∈ S and a ∈ A, we have d˜πlat(s,a) = (cid:80) dπlat◦ϕ(x,a).
h x:ϕ(x)=s h
Firstly, note that it is enough to show that (cid:80) dπlat◦ϕ(x ) = d˜πlat(s ), since d˜πlat(s ,a ) =
xh:ϕ(xh)=sh h h h h h h h
d˜πlat(s )π (a |s ) and (cid:80) dπlat◦ϕ(x ,a )=(cid:80) dπlat◦ϕ(x )π (a |ϕ(x ))=π (a |
h h lat h h xh:ϕ(xh)=sh h h h xh:ϕ(xh)=sh h h lat h h lat h
s )(cid:80) dπlat◦ϕ(x ). Toward this, we have:
h xh:ϕ(xh)=sh h h
 
(cid:88) (cid:88) (cid:88)
dπ hlat◦ϕ(x h)=  dπ hl −at 1◦ϕ(x h−1,a h−1)P o⋆ bs,h(x h |x h−1,a h−1)
xh:ϕ(xh)=sh xh:ϕ(xh)=sh xh−1,ah−1∈X×A
(cid:88) (cid:88)
= dπlat◦ϕ(x ,a ) P⋆ (x |x ,a )
h−1 h−1 h−1 obs,h h h−1 h−1
xh−1,ah−1∈X×A xh:ϕ(xh)=sh
(cid:88)
= dπlat◦ϕ(x ,a )P⋆ (ϕ(x )=s |x ,a )
h−1 h−1 h−1 obs,h h h h−1 h−1
xh−1,ah−1∈X×A
At the same time,
d˜π hlat(s h)=P(cid:101) ϕπlat(S
h
=s h)=(cid:88) P(cid:101) ϕπlat(X
h−1
=x (cid:101),A
h−1
= (cid:101)a)P(cid:101) ϕπlat(S
h
=s
h
|X
h−1
=x (cid:101),A
h−1
= (cid:101)a)
x(cid:101),(cid:101)a
(cid:88)
= P(cid:101) ϕπlat(X
h−1
=x (cid:101),A
h−1
= (cid:101)a)P o⋆ bs(ϕ(x h)=s
h
|x h−1,a h−1),
x(cid:101),(cid:101)a
where in the second equality we have used the definition of the observation function s =O(x )=ϕ(x ).
h h h
To conclude, it remains to show that for all h, we have:
dπ hlat◦ϕ(x h,a h)=P(cid:101) ϕπlat(X
h
=x h,A
h
=a h).
We do this by induction. Again, note that it is sufficient to establish dπ hlat◦ϕ(x h)=P(cid:101) ϕπlat(X
h
=x h). The case
h=1 is clear. For the general case, we have:
(cid:88)
dπlat◦ϕ(x )= dπlat◦ϕ(x ,a )P⋆ (x |x ,a )
h h h−1 h−1 h−1 obs h h−1 h−1
xh−1,ah−1∈X×A
(cid:88)
= P(cid:101) ϕπlat(X
h
=x h−1,A
h−1
=a h−1)P o⋆ bs(x
h
|x h−1,a h−1)
xh−1,ah−1∈X×A
(cid:88)
= P(cid:101) ϕπlat(X
h
=x h−1,A
h−1
=a h−1)P(cid:101) ϕπlat(X
h
=x
h
|X
h−1
=x h−1,A
h−1
=a h−1)
xh−1,ah−1∈X×A
=P(cid:101) ϕπlat(X
h
=x h).
69Lemma H.2 (Near-markovianity of the ϕ-compressed dynamics). For any decoder ϕ, base model M , and
lat
policy π :S×[H]→∆(A), we have:
lat
H H
(cid:88) E (cid:101)π ϕlat(cid:104) D H2(cid:16) M lat,h(s h,a h),M(cid:102) ϕ⋆ ,h(s 1:h,a 1:h)(cid:17)(cid:105) ≤(cid:88) Eπlat◦ϕ[[∆ h(M lat,ϕ)](x h,a h)]. (72)
h=0 h=0
Furthermore, we also have
H H
(cid:88) E (cid:101)π ϕlat(cid:104) D H2(cid:16) M lat,h(s h,a h),M(cid:102) ϕ⋆ ,, hπlat(s h,a h)(cid:17)(cid:105) ≤(cid:88) Eπlat◦ϕ[[∆ h(M lat,ϕ)](x h,a h)]. (73)
h=0 h=0
Proof of Lemma H.2. We begin with the first event. Note that, for any π , the PODMP kernel
lat
M(cid:102) ϕ⋆ ,h(r h,s
h+1
=·|s 1:h,a 1:h) can be written as:
(cid:88)
M(cid:102) ϕ⋆ ,h(r h,s
h+1
=·|s 1:h,a 1:h)= P(cid:101) ϕπlat(r h,s
h+1
=·|x h,a h,s 1:h,a 1:h)P(cid:101) ϕπlat(x h,a
h
|s 1:h,a 1:h)
xh,ah∈X×A
(cid:88)
= P(cid:101) ϕπlat(r h,s
h+1
=·|x h,a h)P(cid:101) ϕπlat(x h,a
h
|s 1:h,a 1:h),
xh,ah∈X×A
where we have used M(cid:102)(r h,s
h+1
=·|s 1:h,a 1:h)=P(cid:101) ϕπlat(r h,s
h+1
=·|s 1:h,a 1:h), the law of total probability,
and that x ,a is a sufficient statistic for r and s . We further note that
h h h h+1
P(cid:101) ϕπlat(r h,s
h+1
=·|x h,a h)=M o⋆ bs,h(r h,ϕ h+1(x h+1)=·|x h,a h), (76)
since s = O (x ) = ϕ (x ) is a deterministic function of x and r ,x ∼ M⋆ (x ,a ).
h+1 h+1 h+1 h+1 h+1 h+1 h h+1 obs,h h h
Thus, for a fixed h and t, and omitting the h indices on the decoder ϕ for cleanliness, the expectation in
equation Eq. (72) becomes:
E (cid:101)π ϕlat(cid:104) D H2(cid:16) M lat,h(s h,a h),M(cid:102) ϕ⋆ ,h(r h,s
h+1
=·|s 1:h,a 1:h)(cid:17)(cid:105)
(cid:88) (cid:88) (cid:16) (cid:17)
≤ P(cid:101) ϕπlat(s 1:h,a 1:h) P(cid:101) ϕπlat(x h,a
h
|s 1:h,a 1:h)D H2 M lat,h(s h,a h),P(cid:101) ϕπlat(r h,s
h+1
=·|x h,a h)
s1:h,a1:h∈(S×A)h xh,ah
(Jensen)
(cid:88) (cid:16) (cid:17)
= P(cid:101) ϕπlat(s 1:h,a 1:h)P(cid:101) ϕπlat(x h,a
h
|s 1:h,a 1:h)D H2 M lat,h(ϕ(x h),a h),P(cid:101) ϕπlat(r h,ϕ(x h+1)=·|x h,a h)
s1:h,a1:h∈(S×A)h
xh,ah∈X×A
= (cid:88) P(cid:101) ϕπlat(x h,a h)D H2(cid:0) M lat(ϕ(x h),a h),M o⋆ bs,h(r h,ϕ(x h+1)=·|x h,a h)(cid:1) (Simplifying & Eq. (76))
xh,ah∈X×A
=Eπlat◦ϕ(cid:2) D2(cid:0) M (ϕ(x ),a ),M⋆ (r ,ϕ(x )=·|x ,a )(cid:1)(cid:3) (Change of measure (Lemma H.1))
H lat h h obs,h h h+1 h h
=Eπlat◦ϕ(cid:2) D2(cid:0) M (ϕ(x ),a ),ϕ♯M⋆ (x ,a )(cid:1)(cid:3) , (By definition of ϕ♯M⋆ )
H lat h h obs,h h h obs
as desired. Summing over h∈[H] we obtain the desired bound. The bound Eq. (73) is a consequence of Eq.
(72) and the data-processing inequality. Namely, using the definition of M(cid:102)⋆,πlat from Eq. (70) and the joint
ϕ,h
convexity of the squared Hellinger distance we have:
D H2(cid:16) M lat,h(·|s h,a h),M(cid:102) ϕ⋆ ,, hπlat(·|s h,a h)(cid:17) ≤E (cid:101)π ϕlat(cid:104) D H2(cid:16) M lat,h(·|s h,a h),M(cid:102) ϕ⋆ ,h(·|s 1:h,a 1:h)(cid:17) |s h,a h(cid:105) . (77)
Thus, we have
(cid:104) (cid:16) (cid:17)(cid:105)
Eπ ϕlat D H2 M lat,h(·|s h,a h),M(cid:102) ϕ⋆ ,, hπlat(·|s h,a h)
(cid:104) (cid:104) (cid:16) (cid:17) (cid:105)(cid:105)
≤Eπ ϕlat Eπ ϕlat D H2 M lat,h(·|s h,a h),M(cid:102) ϕ⋆ ,h(·|s 1:h,a 1:h) |s h,a
h
(cid:104) (cid:16) (cid:17)(cid:105)
=Eπ ϕlat D H2 M lat,h(·|s h,a h),M(cid:102) ϕ⋆ ,h(·|s 1:h,a 1:h) ,
70as desired.
Lemma H.3 (Simulationlemma). For any latent model M with Markovian transition kernel {P } ,
lat lat,h h∈[H]
latent policy π :S×[H]→∆(A), and decoder ϕ∈Φ, we have that for all f :S×A→[0,1]:
lat
|EMlat,πlat[f(s h,a h)]−E (cid:101)π ϕlat[f(s h,a h)]|≤ (cid:88) Eπlat◦ϕ(cid:104)(cid:13) (cid:13)[P lat◦ϕ] h(x h′,a h′)−ϕ h+1♯P o⋆ bs,h(x h′,a h′)(cid:13) (cid:13) tv(cid:105) , (74)
h′<h
and thus for any sequence of policies π(t), latent models M(t), and decoders ϕ(t), we have:
lat lat
(cid:118)
(cid:88)T (cid:88)H |EM l( at t),π l( at t) [f(s h,a h)]−E (cid:101)π ϕl (( a tt t )) [f(s h,a h)]|≤H√ TH(cid:117) (cid:117) (cid:116)(cid:88)T (cid:88)H Eπ l( at t)◦ϕ(t)(cid:2) [∆ h(M l( at) t,ϕ(t))](x h,a h)(cid:3) .
t=1h=0 t=1h=0
Proof of Lemma H.3. Firstly note that, from Lemma H.1, the left-hand-side of Eq. (74) is equivalent to
|EMlat,πlat[f(s h,a h)]−E (cid:101)π ϕlat[f(s h,a h)]|=|EMlat,πlat[f(s h,a h)]−EM o⋆ bs,πlat◦ϕ[[f ◦ϕ](x h,a h)]| (78)
For any π : S ×[H] → ∆(A), let dπlat = dMlat,πlat denote the occupancy in M , and similarly for any
lat lat,h h lat
π : X ×[H] → ∆(A) let dπobs (x ,a ) = dM o⋆ bs,πobs(x ,a ) denote the occupancy in M⋆ . We overload
obs obs,h h h h h h obs
notation by letting dπlat◦ϕ(s,a):=(cid:80) dπ◦ϕ (x,a). We will establish the stronger result that
obs,h x:ϕ(x)=s obs,h
(cid:13) (cid:13)dπlat (·)−dπlat◦ϕ(·)(cid:13) (cid:13) ≤ (cid:88) Eπlat◦ϕ(cid:2) ∥[P ◦ϕ](x ,a )−ϕ♯P⋆ (x ,a )∥ (cid:3) , (79)
(cid:13) lat,h obs,h (cid:13) lat h′ h′ obs h′ h′ tv
tv
h′<h
where the tv norm on the left-hand-side is over S×A. Note that this implies the desired bound on Eq. (78)
by Holder’s inequality. We prove this by induction over h. For the base case (h=0), we have:
(cid:12) (cid:12)
(cid:88) (cid:12) (cid:12)
(cid:12)dπlat (s ,a )−dπlat◦ϕ(s ,a )(cid:12)
(cid:12) lat,1 1 1 obs 1 1 (cid:12)
(cid:12) (cid:12)
s1,a1
(cid:12) (cid:12)
(cid:88) (cid:12) (cid:88) (cid:12)
= (cid:12)P (s |∅)π (a |s )− dπlat◦ϕ(x ,a )(cid:12)
(cid:12) lat,0 1 lat 1 1 obs 1 1 (cid:12)
(cid:12) (cid:12)
s1,a1 x1=ϕ(x1)=s1
(cid:12) (cid:12)
(cid:88) (cid:12) (cid:88) (cid:12)
= (cid:12)P (s |∅)π (a |s )− P⋆ (x |∅)π (a |ϕ(x ))(cid:12)
(cid:12) lat,0 1 lat 1 1 obs,0 1 lat 1 1 (cid:12)
(cid:12) (cid:12)
s1,a1 x1=ϕ(x1)=s1
(cid:12) (cid:12)
(cid:88)(cid:12) (cid:12)(cid:88)
= (cid:12)P (s |∅)−ϕ ♯P⋆ (s |∅)(cid:12) π (a |s )
(cid:12) lat,0 1 1 obs,0 1 (cid:12) lat 1 1
(cid:12) (cid:12)
s1 a1
=(cid:13) (cid:13)P lat,0(∅)−ϕ 1♯P o⋆ bs,0(∅)(cid:13) (cid:13) tv.
For the general case, let us further overload notation by letting dπ◦ϕ (s )=(cid:80) dπ◦ϕ (s ,a ) and P⋆ (s |
x ,a )=ϕ♯P⋆ (s |x ,a )=(cid:80) P⋆ (x
|xobs, ,h
a
h
).
Letahuso abs l, sh
o
ah bbrh
eviate
πo :=bs πh
.
h−1 h−1 obs h h−1 h−1 xh:ϕ(xh)=sh obs h h (cid:12)−1 h−1
(cid:12)
lat
Firstly note that it is sufficient to establish the result for (cid:80) (cid:12)dπ (s )−dπ◦ϕ (s )(cid:12), since
sh∈S(cid:12) lat,h h obs,h h (cid:12)
(cid:88) (cid:12) (cid:12) (cid:88) (cid:12) (cid:12)
(cid:12)dπ (s ,a )−dπ◦ϕ (s ,a )(cid:12)= (cid:12)dπ (s )−dπ◦ϕ (s )(cid:12)π(a |s )
(cid:12) lat,h h h obs,h h h (cid:12) (cid:12) lat,h h obs,h h (cid:12) h h
sh,ah∈S×A sh,ah∈S×A
(cid:88)(cid:12) (cid:12)
= (cid:12)dπ (s )−dπ◦ϕ (s )(cid:12).
(cid:12) lat,h h obs,h h (cid:12)
sh∈S
71Below, all summations over s (resp. x ) with domain unspecified are over S (resp. X), and likewise for
h h
summations over s ,a or x ,a . We have:
h h h h
(cid:12) (cid:12)
(cid:88)(cid:12) (cid:12)
(cid:12)dπ (s )−dπ◦ϕ (s )(cid:12)
(cid:12) lat,h h obs,h h (cid:12)
(cid:12) (cid:12)
sh
(cid:12) (cid:12)
(cid:88)(cid:12) (cid:88) (cid:12)
= (cid:12)dπ (s )− dπ◦ϕ (x )(cid:12)
(cid:12) lat,h h obs,h h (cid:12)
(cid:12) (cid:12)
sh xh:ϕ(xh)=sh
(cid:12)
(cid:88)(cid:12) (cid:88)
= (cid:12) dπ (s ,a )P (s |s ,a )
(cid:12) lat,h h−1 h−1 lat,h h h−1 h−1
(cid:12)
sh sh−1,ah−1
(cid:12)
(cid:88) (cid:88) (cid:12)
− dπ◦ϕ (x ,a )P⋆ (x |x ,a )(cid:12)
obs,h h−1 h−1 obs,h h h−1 h−1 (cid:12)
(cid:12)
xh:ϕ(xh)=shxh−1,ah−1
(cid:12)
(cid:88)(cid:12) (cid:88)
= (cid:12) dπ (s ,a )P (s |s ,a )
(cid:12) lat,h h−1 h−1 lat,h h h−1 h−1
(cid:12)
sh sh−1,ah−1
(cid:12)
(cid:88) (cid:12)
− dπ◦ϕ (x ,a )P⋆ (s |x ,a )(cid:12)
obs,h h−1 h−1 obs,h h h−1 h−1 (cid:12)
(cid:12)
xh−1,ah−1
(cid:12)
(cid:88)(cid:12) (cid:88) (cid:88)
= (cid:12) dπ (s ,a )P (s |s ,a )− dπ◦ϕ (x ,a )P (s |ϕ(x ),a )
(cid:12) lat,h h−1 h−1 lat,h h h−1 h−1 obs,h h−1 h−1 lat,h h h−1 h−1
(cid:12)
sh sh−1,ah−1 xh−1,ah−1
(cid:12)
(cid:88) (cid:88) (cid:12)
+ dπ◦ϕ (x ,a )P (s |ϕ(x ),a )− dπ◦ϕ (x ,a )P⋆ (s |x ,a )(cid:12)
obs,h h−1 h−1 lat,h h h−1 h−1 obs,h h−1 h−1 obs,h h h−1 h−1 (cid:12)
(cid:12)
xh−1,ah−1 xh−1,ah−1
(cid:12) (cid:12)
(cid:12) (cid:12)
≤ (cid:88) (cid:12) (cid:12) (cid:12)dπ lat,h(s h−1,a h−1)− (cid:88) dπ ob◦ sϕ ,h(x h−1,a h−1)(cid:12) (cid:12) (cid:12)(cid:88) P lat,h(s h |s h−1,a h−1)
sh−1,ah−1(cid:12) xh−1:ϕ(xh−1)=sh−1 (cid:12) sh
(cid:12) (cid:12)
(cid:12) (cid:12)
+(cid:88)(cid:12) (cid:12)
(cid:12)
(cid:88) d oπ b◦ sϕ ,h(x h−1,a h−1)(cid:0) (P lat,h◦ϕ)(s h |x h−1,a h−1)−P o⋆ bs,h(s h |x h−1,a h−1)(cid:1)(cid:12) (cid:12)
(cid:12)
sh (cid:12)xh−1,ah−1 (cid:12)
(cid:13) (cid:13)
≤(cid:13)dπ (·)−dπ◦ϕ (ϕ−1(·))(cid:13)
(cid:13) lat,h−1 obs,h−1 (cid:13)
tv
(cid:12) (cid:12)
(cid:88) (cid:88)(cid:12) (cid:12)
+ dπ◦ϕ (x ,a ) (cid:12)(P ◦ϕ)(s |x ,a )−P⋆ (s |x ,a )(cid:12)
obs,h h−1 h−1 (cid:12) lat,h h h−1 h−1 obs,h h h−1 h−1 (cid:12)
(cid:12) (cid:12)
xh−1,ah−1 sh
≤(cid:13) (cid:13) (cid:13)dπ lat,h−1(·)−dπ ob◦ sϕ ,h−1(ϕ−1(·))(cid:13) (cid:13) (cid:13) tv+Eπ◦ϕ(cid:104)(cid:13) (cid:13)[P lat,h◦ϕ](x h−1,a h−1)−ϕ♯P o⋆ bs,h(x h−1,a h−1)(cid:13) (cid:13) tv(cid:105) .
From which it follows that, for each h, we have:
(cid:13) (cid:13) (cid:13)dπ lat,h(·)−dπ ob◦ sϕ ,h(ϕ−1(·))(cid:13) (cid:13) (cid:13)
tv
≤ (cid:88) Eπ◦ϕ(cid:104)(cid:13) (cid:13)[P lat◦ϕ] h′(x h′,a h′)−ϕ h′+1♯P o⋆ bs,h′(x h′,a h′)(cid:13) (cid:13) tv(cid:105)
h′<h
≤ (cid:88) Eπ◦ϕ(cid:104)(cid:13) (cid:13)[P lat◦ϕ] h′(x h′,a h′)−ϕ h′+1♯P o⋆ bs,h′(x h′,a h′)(cid:13) (cid:13) tv(cid:105) .
h′∈[H]
72H.3 Proofs for Appendix H.1.3: Risk Bound Under CorruptionRobustness (The-
orem G.1)
Theorem G.1 (Risk bound for O2L under self-predictive estimation and CorruptionRobustness). Assume
Rep satisfies Assumption 4.3 with parameter γ > 0 and that M is realizable (i.e. M⋆ ∈ M ).
self;opt lat lat lat
Furthermore, let Alg be CorruptionRobust (Definition H.2) with parameter α. Then, O2L (Algorithm 1)
lat
with inputs T,K,Φ,Alg , and Rep has expected risk
lat self;opt
E[Risk (TK)]≤c ·Risk (K)+c γ· K Est (T,γ)+c γ−1·(cid:0) α2+H(cid:1) (65)
obs 1 base 2 T self;opt 3
for absolute constants c ,c ,c >0.
1 2 3
Proof of Theorem G.1. Let us write π(t,K+1) =π(t) and, for any t,k ∈[T]×[K+1], π(t,k) :=π(t,k)◦ϕ(t).
lat (cid:98)lat obs lat
Letp(t,k) denotethedistributionsofplayedpoliciesπ(t,k) inducedbytheinteractionof Alg andRep
obs obs lat self;opt
inside the O2L algorithm. Let us write the online sum of self-prediction errors as
T K+1 H
ε2
rep
:=(cid:88) (cid:88) (cid:88) E π(t,k)∼p(t,k)Eπ o( bt s,k)(cid:2) D H2(cid:0) [M l( at t)◦ϕ(t)] h(x h,a h),ϕ( ht +) 1♯M o⋆ bs,h(x h,a h)(cid:1)(cid:3) (80)
obs
t=1 k=1 h=0
Since the final output policy of O2L satisfies π =Unif(π(1),...,π(T)) (Line 12), we have
(cid:98)lat (cid:98)lat (cid:98)lat
T
E[Risk obs(TK)]= T1 (cid:88) E(cid:104) JM o⋆ bs(π o⋆ bs)−JM o⋆ bs(π (cid:98)o(t b) s)(cid:105) .
t=1
We take the following decomposition on the risk
JM o⋆ bs(π o⋆ bs)−JM o⋆ bs(π (cid:98)o(t b) s)=JM l⋆ at(π
M l⋆
at)−JM l( at t) (π
M l( at
t))+JM l( at t) (π
M l( at
t))−JM l( at t) (π (cid:98)l(t a) t)+ (cid:124)JM l( at t) (π (cid:98)l(t a) t) (cid:123)− (cid:122)JM o⋆ bs(π (cid:98)o(t b)
s
(cid:125)).
(cid:124) (cid:123)(cid:122) (cid:125)
At
Bt
(81)
(cid:104) (cid:105) √ (cid:104) (cid:105) √
We will show that E (cid:80)T A ≲ TReg (K)+α T E[ε ] and that E (cid:80)T B ≲ THE[ε ], then
t=1 t base rep t=1 t rep
return to the first term JM l⋆ at(π
M l⋆
at)−JM l( at t) (π
M l( at
t)) at the end of the proof.
(cid:104) (cid:105)
To bound E (cid:80)T A , we note that
t=1 t
(cid:118) 
T T (cid:117) K H
(cid:88) E[A t]≤c 1TRisk base(K)+α(cid:88) E (cid:117) (cid:116)(cid:88)(cid:88) E π(t,k)∼p(t,k)E (cid:101)π ϕl (( a tt t ),k)(cid:104) D H2(cid:16) M l( at) t,h(s h,a h),M(cid:102) ϕ⋆ (t),h(s 1:h,a 1:h)(cid:17)(cid:105) 
lat lat
t=1 t=1 k=1h=1
(cid:118) 
T (cid:117) K H
≤c 1TRisk base(K)+α(cid:88) E (cid:117) (cid:116)(cid:88)(cid:88) E π(t,k)∼p(t,k)Eπ l( at t,k)◦ϕ(t)(cid:2) [∆ h(M l( at) t,ϕ(t))](x h,a h)(cid:3) 
lat lat
t=1 k=1h=1
(cid:118) 
√ (cid:117) T K H
≤c 1TRisk base(K)+α T E (cid:117) (cid:116)(cid:88)(cid:88)(cid:88) E π(t,k)∼p(t,k)Eπ o( bt s,k)(cid:2) [∆ h(M l( at) t,ϕ(t))](x h,a h)(cid:3) 
obs obs
t=1k=1h=1
√
≤c TRisk (K)+α T E[ε ].
1 base rep
where the first line follows from the CorruptionRobust definition (Definition H.2), the second line follows
from Lemma H.2, the third line follows by Cauchy-Schwartz, and the last line recalls the definition of ε
rep
from Eq. (80).
73Fortheterm(cid:80)T t=1B t, foranyπ
lat
:S×[H]→∆(A)weletQπ lal tat
(t),h
=T hM l( at t) Qπ lal tat
(t),h+1
betheQπlat function
of the latent MDP M(t). Note that
lat
T
(cid:88)(cid:110) JM l( at t) (π (cid:98)l(t a) t)−Eπ(cid:98)l( at t)◦ϕ(t)(cid:104) [Qπ l(cid:98) a( t(t t)
)
◦ϕ(t)] 1(x 1,a 1)(cid:105)(cid:111) (82)
t=1
T
=(cid:88) EM l( at t),π(cid:98)l( at t)(cid:104) Qπ l(cid:98) a( t(t t) ),1(s 1,a 1)(cid:105) −Eπ(cid:98)l( at t)◦ϕ(t)(cid:104) [Q lπ(cid:98) a( t(t t)
)
◦ϕ(t)] 1(x 1,a 1)(cid:105)
t=1
T
≤(cid:88) Eπ(cid:98)l( at t)◦ϕ(t)(cid:2)(cid:13) (cid:13)[P l( at t)◦ϕ(t)] 0(∅)−ϕ( 1t)♯P o⋆ bs,0(∅)(cid:13) (cid:13) tv(cid:3) (by Lemma H.3)
t=1
T H
≤(cid:88)(cid:88) Eπ(cid:98)l( at t)◦ϕ(t)(cid:2)(cid:13) (cid:13)[P l( at t)◦ϕ(t)] h(x h,a h)−ϕ( ht +) 1♯P o⋆ bs,h(x h,a h)(cid:13) (cid:13) tv(cid:3)
t=1h=0
√
≤ THε , (by Cauchy-Schwartz)
rep
so it is enough to bound
T (cid:26) (cid:20) (cid:21) (cid:27)
(cid:88) Eπ(cid:98)l( at t)◦ϕ(t) [Q lπ(cid:98) al( ta (t t t)
)
◦ϕ(t)] 1(x 1,a 1) −JM o⋆ bs(π (cid:98)o(t b) s) .
t=1
Fix t and h, whose indexing we omit below for cleanliness. Note that, for any π : S ×[H] → ∆(A), we
lat
have:
Eπlat◦ϕ(cid:20)(cid:16)
[Qπlat ◦ϕ] (x ,a )−TM o⋆ bs,πlat◦ϕ [Qπlat ◦ϕ] (x ,a
)(cid:17)2(cid:21)
(83)
lat h h h h lat h+1 h h
≤2Eπlat◦ϕ(cid:104)(cid:0) [r ◦ϕ] −r⋆ (cid:1)2 (x ,a )(cid:105) (84)
lat h obs,h h h
+2Eπlat◦ϕ(cid:20)(cid:16) E Plat,h(ϕ(xh),ah)(cid:104) Qπ lal tat ,h+1(·,π lat)(cid:105) −E P o⋆ bs,h(xh,ah)(cid:2) [Qπ lal tat ◦ϕ] h+1(·,π lat)(cid:3)(cid:17)2(cid:21) (85)
≤2Eπlat◦ϕ(cid:104)(cid:0) [r lat◦ϕ] h−r o⋆ bs,h(cid:1)2 (x h,a h)+(cid:13) (cid:13)P lat,h(ϕ(x h),a h)−ϕ h+1♯P o⋆ bs,h(x h,a h)(cid:13) (cid:13)2 tv(cid:105) (86)
≤4Eπlat◦ϕ(cid:2) D2(cid:0) M (ϕ (x ),a ),ϕ ♯M⋆ (x ,a )(cid:1)(cid:3) , (87)
H lat,h h h h h+1 obs,h h h
where the final line follows from two applications of the data-processing inequality (since M (r ,s |
lat,h h h+1
ϕ (x ),a )=R (r |ϕ (x ),a )P (s |ϕ (x ),a )andϕ ♯M⋆ (r ,s |x ,a )=R⋆ (r |
h h h lat,h h h h h lat,h h+1 h h h h+1 obs,h h h+1 h h obs,h h
x ,a )ϕ ♯P⋆ (s | x ,a )) as well as the bound ∥p−q∥2 ≤ D2(p,q). Summing this over t,h and
h h h+1 obs,h h+1 h h tv H
using a standard decomposition for regret (Lemma B.6) gives:
T (cid:26) (cid:20) (cid:21) (cid:27)
(cid:88) Eπ(cid:98)l( at t)◦ϕ(t) [Qπ l(cid:98) al( ta (t t t)
)
◦ϕ(t)] 1(x 1,a 1) −JM o⋆ bs(π (cid:98)o(t b) s)
t=1
T H (cid:20) (cid:21)
=(cid:88)(cid:88) Eπ(cid:98)l( at t)◦ϕ(t) [Qπ l(cid:98) al( ta (t t t)
)
◦ϕ(t)] h(x h,a h)−T hM o⋆ bs,π(cid:98)l( at t)◦ϕ(t) [Q lπ(cid:98) al( ta (t t t)
)
◦ϕ(t)] h+1(x h,a h) (Lemma B.6)
t=1h=1
(cid:118)
√ (cid:117) T H (cid:34)(cid:18) (cid:19)2(cid:35)
≤ TH(cid:117) (cid:116)(cid:88)(cid:88) Eπ(cid:98)l( at t)◦ϕ(t) [Qπ l(cid:98) al( ta (t t t)
)
◦ϕ(t)] h(x h,a h)−T hM o⋆ bs,π(cid:98)l( at t)◦ϕ(t) [Q lπ(cid:98) al( ta (t t t)
)
◦ϕ(t)] h+1(x h,a h)
t=1h=1
(cid:118)
≤√ 4TH(cid:117) (cid:117) (cid:116)(cid:88)T (cid:88)H Eπ(cid:98)l( at t)◦ϕ(t)(cid:104) D H2(cid:16)(cid:104) M l( at) t,h◦ϕ( ht)(cid:105) (x h,a h),ϕ h(t +) 1♯M o⋆ bs,h(x h,a h)(cid:17)(cid:105) (By Eq. (87))
t=1h=1
√
≤ 4THε .
rep
74Returning to the decomposition of Eq. (81) and combining everything gives:
E[Risk obs]≤
T1(cid:40) (cid:88)T E(cid:104)
JM l⋆ at(π
M l⋆
at)−JM l( at t) (π
M l( at
t))(cid:105)(cid:41)
+
T1(cid:16) α√
T
+4√ TH(cid:17)
E[ε rep]+c 1·Risk base(K)
t=1
≤ T1(cid:40) (cid:88)T E(cid:104) J(π⋆)−JM l( at t) (π M(t))+γε2 rep(cid:105)(cid:41) + γ T−1(cid:16) α√ T +4√ TH(cid:17)2 +c 1·Risk base(K)
lat
t=1
≤γ2K
Est
(T,γ)+2γ−1(cid:0) α2+16H(cid:1)
+c ·Risk (K),
T self;opt 1 base
where the second inequality follows by AM-GM applied to the middle term and the third inequality follows
from: i) Jensen’s inequality, ii) Assumption 4.3 applied to the distributions p¯(t) = 1 (cid:80)K p(t,k), iii) the
obs K k=1 obs
bound K+1≤2K, and iv) the inequality (x+y)2 ≤2(x2+y2).
H.4 Proofs for Appendix H.1.4: Examples of CorruptionRobust Algorithms
Theorem H.1 (Latent GOLF is CorruptionRobust). Under Assumption H.1 and Assumption H.2, Algo-
rithm 5 with β
=c(cid:0) log(cid:0) |F||G|KHδ−1(cid:1)
+ε
(cid:1)
, has regret
rep
(cid:88)K (cid:16) (cid:112) (cid:17) (cid:16) (cid:113) (cid:17)
JMlat(π Mlat)−JMlat(π(k))≤O H C covKlog(K)log(|F||G|HK/δ) +O H3/2 KC covlog(K)ε2
rep
,
k=1
and consequently is CorruptionRobust (Definition H.2) with parameters
H3/2(cid:112) (cid:18) H (cid:112) (cid:19)
α= √ C log(K) and Risk (K)=O √ C log(K)log(|F||G|HK) .
cov base cov
K K
Proof of Theorem H.1. Recall that the agent is observing data from the ϕ-compressed POMDP M(cid:102)⋆, and
ϕ
thusthe datasets areof the formD(k) =D(k) ={ϕ(x(i)),a(i),r(i),ϕ(x(i) )}k−1. Forany π ∈Π , wedefine
h ϕ,h h h h h+1 i=1 lat lat
T(cid:101) ϕπ ,hlatf(s h,a h)=r˜ ϕπ ,l hat(s h,a h)+E s′∼P(cid:101)ϕπ ,l hat(sh,ah)[f(s′)],
where r˜πlat and P(cid:101)πlat are the policy-dependent Markov operators defined in Eq. (66) and Eq. (68).
ϕ,h ϕ,h
As a consequence, we observe the following misspecification guarantee for T .
lat
Lemma H.4 (Misspecification guarantee for T ).
lat
∀f :S×A→[0,1]:
(cid:88)K (cid:88)H
E (cid:101)π
ϕ(k)(cid:20)(cid:16)
T lat,hf(s h,a h)−T(cid:101) ϕπ ,h(k) f(s h,a
h)(cid:17)2(cid:21)
≤O(ε2 rep).
k=1h=1
Proof of Lemma H.4. Follows from Assumption H.2 and the definitions of T(cid:101) ϕπ ,h(k) and T lat,h.
We recall that Assumption H.2 implies the following simulation lemma from M
lat
to M(cid:102) ϕ⋆.
Lemma H.5 (Simulation lemma).
∀f :S×A→[0,1] (cid:12) (cid:12) (cid:12)EMlat,πlat[f(s h,a h)]−E (cid:101)π ϕlat[f(s h,a h)](cid:12) (cid:12) (cid:12)≤ (cid:88) E (cid:101)π ϕlat(cid:104)(cid:13) (cid:13) (cid:13)P lat,h′(s h′,a h′)−P(cid:101) ϕπ ,l hat ′(s h′,a h′)(cid:13) (cid:13)
(cid:13)
(cid:105)
tv
h′<h
and thus
(cid:88)K (cid:88)K EMlat,π(k) [f(s h,a h)]≤(cid:88)K (cid:88)K E (cid:101)π ϕ(k) [f(s h,a h)]+H3/2(cid:113) Kε2 rep.
k=1h=1 k=1h=1
75We begin with the following lemmas, which will be proved in the sequel.
Lemma H.6 (Optimism). For the choice of β in Theorem H.1, with probability at least 1−δ, we have that
for all k ∈[K]:
Q⋆ ∈F(k).
lat
Lemma H.7 (Small in-sample squared Bellman errors). With probability at least 1−δ, we have that for all
k ∈[K], h∈[H], and f ∈F(k):
k (cid:88)−1
E (cid:101)π
ϕ(i)(cid:20)(cid:16)
f(s h,a h)−T(cid:101) ϕπ ,h(i) f(s h,a
h)(cid:17)2(cid:21)
≤O(β).
i=1
Letuswriteπ(k) :=π(k)◦ϕ. Letusintroducetheshorthandd˜(k) :=(cid:80)k−1dπ o( bk s) ,wheredπ istheoccupancy
obs obs,h i=1 obs,h obs
for M⋆ , and also the burn-in time
obs
(cid:40) k−1 (cid:41)
κ (x,a):=min k :(cid:88) dπ(k) (x,a)≥C µ⋆(x,a) .
h obs,h cov h
i=1
Let us recall, from the analysis of [XFBJK23], that for any h∈[H] and f :S×A→[0,1] we have
K
(cid:88) Eπ(k) [f(s ,a )I{k <κ (s ,a )}]≤2C , (88)
h h h h h cov
k=1
as well as
(cid:88)H (cid:88)K (cid:88)(dπ ho( bk s) (x,a)I{k ≥κ h(x,a)})2
≤O(HC log(K)). (89)
d˜(k)(x,a) cov
h=1k=1 s,a h
K H
(cid:88) JMlat(π Mlat)−JMlat(π(k))≤(cid:88)(cid:88) EMlat,π(k) [f(k)(s h,a h)−T latf(k)(s h,a h)] (Optimism (Lemma H.6))
k k=1h=1
≤(cid:88)K (cid:88)H E (cid:101)π ϕ(k) [f(k)(s h,a h)−T latf(k)(s h,a h)]+H3/2(cid:113) Kε2
rep
(Lemma H.5)
k=1h=1
=(cid:88)K (cid:88)H
Eπ(k)◦ϕ[[(f(k)−T f(k))◦ϕ](x ,a
)]+H3/2(cid:113)
Kε2 (Lemma H.1)
lat h h rep
k=1h=1
K H
≤(cid:88)(cid:88) Eπ(k)◦ϕ[[(f(k)−T f(k))◦ϕ](x ,a )I{k ≥κ (x ,a )}]
lat h h h h h
k=1h=1
(cid:113)
+2HC +H3/2 Kε2 (burn-in time Eq. (88))
cov rep
K H
≤(cid:88)(cid:88) Eπ(k)◦ϕ(cid:104)(cid:104) (f(k)−T(cid:101) ϕπ ,h(k) f(k))◦ϕ(cid:105) (x h,a h)I{k ≥κ h(x h,a h)}(cid:105)
k=1h=1
(cid:124) (cid:123)(cid:122) (cid:125)
(I)
+(cid:88)K (cid:88)H Eπ(k)◦ϕ(cid:104)(cid:104)
(T(cid:101) ϕπ ,h(k) f(k)−T
lat,hf(k))◦ϕ(cid:105)
(x h,a
h)(cid:105)
+2HC
cov+H3/2(cid:113)
Kε2
rep
k=1h=1
(cid:124) (cid:123)(cid:122) (cid:125)
(II)
76Note that, by change of measure (Lemma H.1) and the misspecification guarantee (Lemma H.4), the second
term is bounded by:
(II)=(cid:88)K (cid:88)H E (cid:101)π ϕ(k)(cid:104) (T(cid:101) ϕπ ,h(k) f(k)−T lat,hf(k))(s h,a h)(cid:105) ≤(cid:113) KHε2 rep.
k=1h=1
Turning to the first term, we have:
H K
(cid:88)(cid:88) Eπ o( bk s)(cid:104)(cid:104) (f(k)−T(cid:101) ϕπ ,h(k) f(k))◦ϕ(cid:105) (x h,a h)I{k ≥κ h(x h,a h)}(cid:105) (90)
h=1k=1
(cid:118) (cid:118)
≤(cid:117) (cid:117) (cid:116)(cid:88)H (cid:88)K (cid:88)(dπ ho( bk s) (x,a d) ˜(I k{ )k (x≥ ,aκ )h(x,a)})2(cid:117) (cid:117) (cid:116)(cid:88)H (cid:88)K
E
d˜( obk
s)(cid:20)(cid:16)
(f(k)−T(cid:101) ϕπ
,h(k)f(k))◦ϕ(cid:17)2
(x h,a
h)(cid:21)
(91)
h=1k=1 x,a h h=1k=1
(cid:118)
(cid:112) (cid:117) (cid:117)(cid:88)H (cid:88)K (cid:20)(cid:16) (cid:17)2 (cid:21)
≤ HC covlog(K)(cid:116) E
d˜(k)
(f(k)−T(cid:101) ϕπ ,h(k)f(k))◦ϕ (x h,a h) (coverability potential Eq. (89))
obs
h=1k=1
(cid:118)
=(cid:112) HC
covlog(K)(cid:117)
(cid:117)
(cid:116)(cid:88)H (cid:88)K k (cid:88)−1
E (cid:101)π
ϕ(i)(cid:20)(cid:16)
f(k)(s h,a h)−T(cid:101) ϕπ ,h(k)f(k)(s h,a
h)(cid:17)2(cid:21)
h=1k=1i=1
(change of measure, Lemma H.1)
(cid:16) (cid:112) (cid:17)
≤O H C Klog(K)β , (92)
cov
where we have used that, from Lemma H.7, we have:
(cid:88)H (cid:88)K k (cid:88)−1
E (cid:101)π
ϕ(i)(cid:20)(cid:16)
f(k)(s h,a h)−T(cid:101) ϕπ(i) f(k)(s h,a
h)(cid:17)2(cid:21)
≤O(βHK).
h=1k=1i=1
This gives an upper bound on the regret of
(cid:88)K (cid:16) (cid:112) (cid:113) (cid:17)
JMlat(π Mlat)−JMlat(π(k))≤O H C covKlog(K)β+H3/2 Kε2
rep
.
k=1
(cid:16) (cid:16) (cid:17) (cid:17)
Using that β =O log |F||G|HK +ε and simplifying gives
δ rep
(cid:88)K (cid:16) (cid:112) (cid:17) (cid:16) (cid:113) (cid:17)
JMlat(π Mlat)−JMlat(π(k))≤O H C covKlog(K)log(|F||G|HK/δ) +O H3/2 KC covlog(K)ε2
rep
,
k=1
as desired. It only remains to establish the concentrations results.
Concentration analysis. We establish the concentration results of Lemma H.6 and Lemma H.7.
Proof of Lemma H.7. Let
X k(h,f)=(cid:0) f h(s( hk),a( hk))−r h(k)−f h+1(s( hk +) 1)(cid:1)2 −(cid:16) T(cid:101) ϕπ(k) f h(s( hk),a( hk))−r h(k)−f h+1(s( hk +) 1)(cid:17)2 .
Let F ={s(i),a(i),r(i),...,s(i),a(i),r(i)}k . Note that
k,h 1 1 1 H H H i=1
E(cid:2)
r(k)+f (s(k) )|F
(cid:3) =E(cid:2)
r(k)+f (s(k)
)|π(k)(cid:3)
h h+1 h+1 k,h h h+1 h+1
=E(cid:2)E(cid:2)
r(k)+f (s(k)
)|s(k),a(k),π(k)(cid:3) |π(k)(cid:3)
h h+1 h+1 h h
(cid:104) (cid:105)
=E T(cid:101)π(k) f(s(k),a(k))|π(k)
ϕ h h
=E (cid:101)π ϕ(k)(cid:104) T(cid:101) ϕπ(k) f(s h,a h)(cid:105) ,
77and thus that
E[X k(h,f)|F k,h]=E (cid:101)π
ϕ(k)(cid:20)(cid:16)
f h(s h,a h)−T(cid:101) ϕπ(k) f h(s h,a
h)(cid:17)2(cid:21)
.
Next, note that
(cid:104) (cid:105)
Var[X (h,f)|F ]≤E (X (h,f))2 |F
k k,h k k,h
(cid:20)(cid:16) (cid:17)2 (cid:21)
≤16E f h(s( hk),a( hk))−T(cid:101) ϕπ(k) f h(s( hk),a( hk)) |F
k,h
=16E[X (h,f)|F ].
k k,h
By Freedman’s inequality (Lemma B.2, Lemma B.3), we have that with probability at least 1−δ:
(cid:12) (cid:12)  
(cid:12)(cid:88) (cid:88) (cid:12) (cid:115) (cid:88)
(cid:12)
(cid:12)
X t(h,f)− E[X t(h,f)|F t,h](cid:12) (cid:12)≤O log(1/δ) E[X t(h,f)|F t,h]+log(1/δ)
(cid:12) (cid:12)
t<k t<k t<k
Taking a union bound over [K]×[H]×F, we have that for all k,h,f, with probability at least 1−δ:
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)(cid:88) X t(h,f)−(cid:88) E (cid:101)π ϕ(k)(cid:20)(cid:16) f h(s h,a h)−T(cid:101) ϕπ(k) f h(s h,a h)(cid:17)2(cid:21)(cid:12) (cid:12)
(cid:12)
(93)
(cid:12) (cid:12)
t<k t<k
≤O(cid:32)(cid:115)
ι(cid:88) E (cid:101)π
ϕ(k)(cid:20)(cid:16)
f h(s h,a h)−T(cid:101) ϕπ(k)f h(s h,a
h)(cid:17)2(cid:21) +ι(cid:33)
, (94)
t<k
where ι=log(|F|HK/δ). We now show that
(cid:88)
X (h,f(k))≤β+O(ε +ι)=O(β), (95)
t rep
t<k
which will imply, from Eq. (93), that
(cid:88) E (cid:101)π
ϕ(k)(cid:20)(cid:16)
f h(s h,a h)−T(cid:101) ϕπ(k) f h(s h,a
h)(cid:17)2(cid:21)
≤O(ι+β)=O(β),
t<k
as desired. To see Eq. (95), let
∆
k
=(cid:88)(cid:0) T latf h(k)(s( ht),a( ht))−r h(t)−f h(k +) 1(s h(t +) 1)(cid:1)2 −(cid:16) T(cid:101) ϕπ(t) f h(k)(s h(t),a h(t))−r h(t)−f h(k +) 1(s( ht +) 1)(cid:17)2
t<k
and then note that:
(cid:88) X t(h,f(k))=(cid:88)(cid:0) f h(k)(s( ht),a( ht))−r h(t)−f h(k +) 1(s h(t +) 1)(cid:1)2 −(cid:16) T(cid:101) ϕπ(t) f h(k)(s( ht),a h(t))−r h(t)−f h(k +) 1(s h(t +) 1)(cid:17)2
t<k t<k
=(cid:88)(cid:0)
f(k)(s(t),a(t))−r(t)−f(k) (s(t)
)(cid:1)2 −(cid:0)
T f(k)(s(t),a(t))−r(t)−f(k) (s(t)
)(cid:1)2
+∆
h h h h h+1 h+1 lat h h h h h+1 h+1 k
t<k
≤(cid:88)(cid:0)
f(k)(s(t),a(t))−r(t)−f(k) (s(t)
)(cid:1)2
− inf
(cid:88)(cid:0)
g(s(t),a(t))−r(t)−f(k) (s(t)
)(cid:1)2
+∆
h h h h h+1 h+1 h h h h+1 h+1 k
t<k
gh∈Ght<k
≤β+∆ .
k
where the second-to-last line follows from T F ⊆ G and the last line follows from the definition of the
lat
confidence set. It remains to show that ∆ ≤O(ε +ι), which we do via a similar concentration argument.
k rep
Namely, let
Y t(h,f)=(cid:0) T latf h(s( ht),a( ht))−r h(t)−f h(k +) 1(s( ht +) 1)(cid:1)2 −(cid:16) T(cid:101) ϕπ(t) f h(s h(t),a h(t))−r h(t)−f h(k +) 1(s( ht +) 1)(cid:17)2 ,
78and note that, as before,
E[Y t(h,f)|F t,h]=E (cid:101)π
ϕ(t)(cid:20)(cid:16)
T latf h(s h,a h)−T(cid:101) ϕπ(t) f h(s h,a
h)(cid:17)2(cid:21)
,
and
Var[Y (h,f)|F ]≤16E[Y (h,f)|F ],
t t,h t t,h
by the same calculation as earlier. Thus, by Freedman’s inequality and a union bound, we have that, with
probability at least 1−δ,
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)(cid:88) Y t(h,f)−(cid:88) E (cid:101)π ϕ(k)(cid:20)(cid:16) T latf h(s h,a h)−T(cid:101) ϕπ(k) f h(s h,a h)(cid:17)2(cid:21)(cid:12) (cid:12)
(cid:12)
(96)
(cid:12) (cid:12)
t<k t<k
≤O(cid:32)(cid:115)
ι(cid:88) E (cid:101)π
ϕ(k)(cid:20)(cid:16)
T latf h(s h,a h)−T(cid:101) ϕπ(k)f h(s h,a
h)(cid:17)2(cid:21) +ι(cid:33)
, (97)
t<k
where ι=log(|F|HK/δ). Recalling the misspecification assumption Lemma H.4, this implies that
(cid:88)
Y (h,f)≤O(ε +ι),
t rep
t<k
for all h,f,k, with high probability. Applying this to f =f(k) concludes the result.
Proof of Lemma H.6. We use similar arguments to the preceding lemma. Let Q⋆ :=Q⋆ . The aim
is to show that, for all h∈[H],k ∈[K],g ∈G, we have:
lat,h Mlat,h
(cid:88)(cid:0) g(s(t),a(t))−r(t)−Q⋆ (s(t) )(cid:1)2 −(cid:0) Q⋆ (s(t),a(t))−r(t)−Q⋆ (s(t) )(cid:1)2 ≥−β,
h h h lat,h+1 h+1 lat,h h h h lat,h h+1
t<k
from which the conclusion will follow. We show that
(cid:88)(cid:0) g(s(t),a(t))−r(t)−Q⋆ (s(t) )(cid:1)2 −(cid:16) T(cid:101)π(t) Q⋆ (s(t),a(t))−r(t)−Q⋆ (s(t) )(cid:17)2 ≥−β/2, (98)
h h h lat,h+1 h+1 ϕ lat,h h h h lat,h h+1
t<k(cid:124) (cid:123)(cid:122) (cid:125)
:=Wt(h,g)
and also that
(cid:88)(cid:16) T(cid:101)π(t) Q⋆ (s(t),a(t))−r(t)−Q⋆ (s(t) )(cid:17)2 −(cid:0) Q⋆ (s(t),a(t))−r(t)−Q⋆ (s(t) )(cid:1)2 ≥−β/2. (99)
ϕ lat,h h h h lat,h h+1 lat,h h h h lat,h h+1
t<k(cid:124) (cid:123)(cid:122) (cid:125)
:=Vt(h)
For Eq. (98), note that
E[W t(h,g)|F t,h]=E (cid:101)
ϕπ(t)(cid:20)(cid:16)
g h(s h,a h)−T(cid:101) ϕπ ,h(t) Q⋆ lat,h(s h,a
h)(cid:17)2(cid:21)
, (100)
and that Var[W (h,g)|F ]≤16E[W (h,g)|F ]. By Freedman, this gives
t t,h t t,h
(cid:12) (cid:12)  
(cid:12)(cid:88) (cid:88) (cid:12) (cid:115) (cid:88) 1
(cid:12)
(cid:12)
W t(h,g)− E[W t(h,g)|F t,h](cid:12) (cid:12)≤O ι E[W t(h,g)|F t,h]+ι≤ 2E[W t(h,g)|F t,h]+O(ι),
(cid:12) (cid:12)
t<k t<k t<k
or in other words
(cid:88) 1(cid:88)
W (h,g)≥ E[W (h,g)|F ]−O(ι)≥−O(ι),
t 2 t t,h
t<k t<k
79using the non-negativity of Eq. (100). For Eq. (99), note that
E[V t(h)|F t,h]=−E (cid:101)π
ϕ(t)(cid:20)(cid:16)
T lat,hQ⋆ lat,h−T(cid:101) ϕπ ,h(t) Q⋆
lat,h(cid:17)2(cid:21)
≥−ε rep, (101)
and that Var[V t(h)|F t,h]≤16E (cid:101)π
ϕ(t)(cid:20)(cid:16)
T lat,hQ⋆ lat,h−T(cid:101) ϕπ ,h(t)Q⋆
lat,h(cid:17)2(cid:21)
. By Freedman, this gives
(cid:12) (cid:12) (cid:12) (cid:12)(cid:88) V t(h)−(cid:88) E[V t(h)|F t,h](cid:12) (cid:12) (cid:12) (cid:12)≤O(cid:32)(cid:115) ι(cid:88) E (cid:101)π ϕ(t)(cid:20)(cid:16) T latQ⋆ lat,h+1(s h,a h)−T(cid:101) ϕπ ,h(t)Q⋆ lat,h+1(s h,a h)(cid:17)2(cid:21) +ι(cid:33)
(cid:12) (cid:12)
t<k t<k t<k
(102)
=O(ε +ι), (103)
rep
or in other words
(cid:88) (cid:88)
V (h)≥ E[V (h)|F ]−O(ε +ι)≥−O(ε +ι),
t t t,h rep rep
t<k t<k
where we have used Eq. (101).
80