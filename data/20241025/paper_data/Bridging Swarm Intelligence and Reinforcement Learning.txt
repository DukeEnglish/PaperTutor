Bridging Swarm Intelligence and Reinforcement Learning
KarthikSoma YannBouteiller
PolytechniqueMontreal PolytechniqueMontreal
Montreal,Canada Montreal,Canada
HeikoHamann GiovanniBeltrame
UniversityofKonstanz PolytechniqueMontreal
Konstanz,Germany Montreal,Canada
ABSTRACT thehouse-huntingbehaviorofhoneybeestocreatetheweighted
Swarmintelligence(SI)exploreshowlargegroupsofsimpleindi- voterrule[14,23].Inthisrule,afterscoutingoneof𝑛potentialnest-
viduals(e.g.,insects,fish,birds)collaboratetoproducecomplex ingareas,beescomebacktoperforma“dance"[24]thatdescribes
behaviors,exemplifyingthatthewholeisgreaterthanthesumof thecoordinatesoftheoptionthattheyhaveexplored.Accordingto
itsparts.AfundamentaltaskinSIisCollectiveDecision-Making theweightedvotermodel,thisdanceisperformedatafrequency
(CDM),whereagroupselectsthebestoptionamongseveralalter- thatisproportionaltotheestimatedqualityoftheexploredarea.
natives,suchaschoosinganoptimalforagingsite.Inthiswork, Otherbeesgoscouttheareacorrespondingtothefirstdancethey
wedemonstrateatheoreticalandempiricalequivalencebetween witness,andthisprocessrepeatsuntiltheentirecolonyconverges
CDMandsingle-agentreinforcementlearning(RL)inmulti-armed tothesameoption.
banditproblems,utilizingconceptsfromopiniondynamics,evo- Next,weturntowardreinforcementlearning(RL),wherean
lutionarygametheory,andRL.Thisequivalencebridgesthegap
agent1learnstosolveataskbyinteractingwiththeenvironment
betweenSIandRLandleadsustointroduceanovelabstractRL tomaximizearewardsignal[20].RLhasbeensuccessfullyapplied
updaterulecalledMaynard-CrossLearning.Additionally,itprovides tosolvecomplexproblemsinvariousfieldssuchasrobotics[9],
anewpopulation-basedperspectiveoncommonRLpracticeslike nuclearfusion[17],andgames[10].Inthispaper,wearespecifically
learningrateadjustmentandbatching.Ourfindingsenablecross- interested in multi-armed bandits [20], in which a single agent
disciplinaryfertilizationbetweenRLandSI,allowingtechniques makeschoicesamongdifferentoptions(or“arms”)tomaximizeits
fromonefieldtoenhancetheunderstandingandmethodologiesof reward.Amongthemanylearningalgorithmsdesignedtosolvethis
theother. task(Upper-confidence-Bound[1](UCB),𝜖-greedy[20],Gradient
Bandit [25]...), we consider the Cross Learning [5] update rule,
closelyrelatedtotheGradientBanditalgorithm.
KEYWORDS
AlthoughSIandRLareseeminglydisjoint,weshowthatthese
SwarmIntelligence,ReinforcementLearning,EvolutionaryGame
fieldscaninfactbebridgedviatheReplicatorDynamic[16](RD),
Theory,OpinionDynamics
afamousequationusedinEvolutionaryGameTheory(EGT)to
ACMReferenceFormat: model the outcome of evolutionary processes through the idea
KarthikSoma,YannBouteiller,HeikoHamann,andGiovanniBeltrame.2025. of survival of fittest. In the rest of the paper, we demonstrate a
BridgingSwarmIntelligenceandReinforcementLearning.arXivpreprint, mathematicalequivalencebetweendifferentconceptsfromSIand
11pages.
RL:
• Wefirstshowthatalargepopulationwhosemembersfol-
1 INTRODUCTION
lowthevoterrulecanbeseenasasingleabstractRLagent
SwarmIntelligence(SI)takesinspirationfromhowacollectiveof followingtheCrossLearningupdaterule.
naturalentities,followingsimple,local,anddecentralizedrules,can • Next,viaasimilarargument,weshowthattheweighted
produceemergentandcomplexbehaviors[3].Researchershave voterrule,yieldsanotherabstractRLupdatethatwecoin
extractedcoreprinciplessuchascoordination,cooperation,and Maynard-CrossLearning.
localcommunicationfromthesenaturalsystems,andappliedthem • WevalidatetheseequivalenceswithRLandpopulationex-
toartificialsystems,(e.g.,swarmrobotics[7,8]andoptimization perimentsandofferanewperspectiveabouttwocommon
algorithms[6]). practicesinRL,learningrateadjustmentandbatching.
Inthispaper,wefocusthespecificSIproblemofCollectiveDeci-
sionMaking(CDM).InCDM,individualsworktogethertoreachan
2 PRELIMINARIES
agreementonthebestoptionfromasetofalternatives,aproblem
2.1 Multi-armedbanditsandCrossLearning
commonlycalledthebest-of-ndecisionproblem.Tosolvethisprob-
lem,researchershaveturnedtoopiniondynamics[26],afieldthat Multi-armedbanditsareoneofthesimplesttypesofenvironments
studieshowopinionsspreadinapopulation.Inparticular,inthe encounteredinRLliterature.Theyconsistofadiscretesetofavail-
voterrule[4,12],anindividualcopiestheopinionofarandomly ableactions,called“arms",amongstwhichanagenthastofindthe
chosenneighbor.Similarly,researchershavetakeninspirationfrom mostrewarding.Inan𝑛-armedbandit,pullingarm𝑎∈{1,...,𝑛}
ThisworkislicencedundertheCreativeCommonsAttribution4.0International 1Toavoidconfusion,weuse“agent”inthecontextofRLand“individual”inthecontext
(CC-BY4.0)licence. ofapopulation,whereverpossible
4202
tcO
32
]AM.sc[
1v71571.0142:viXrareturnsareal-valuedreward𝑟 𝑎 ∈ [0,1] sampledfromahidden variantoftheTRDforlaterconvenienceinthepaper,theMaynard
distribution𝑟(𝑎).TheobjectiveforanRLagentplayingamulti- SmithReplicatorDynamic[18](MRD):
armedbanditistolearnapolicy,denotedbytheprobabilityvector 𝜋
𝜋 =(𝜋1,....𝜋 𝑛),thatmaximizestherewardsobtaineduponpulling 𝜋(cid:164)𝑎 = 𝑣𝜋𝑎 (𝑞 𝑎𝜋 −𝑣𝜋 ) (4)
thearms.Differentexplorationstrategiesexisttofindsuchpolicies,
oneofthembeingCrossLearning[5]: 2.3 Collective-decisionmakinginswarms
CrossLearning(CL).Let𝑘beanactionand𝑟 𝑘 acorresponding ConsiderapopulationPof𝑁 individualstryingtoreachaconsen-
rewardsample(𝑟 𝑘 ∼𝑟(𝑘)).CLupdatesthepolicy𝜋 as: susonwhichamongst𝑛availableoptionsistheoptimal.Similar
(cid:40) topopulationgames,eachindividual𝑖hasanopinion,denotedby
∀𝑎,𝜋 𝑎 ←𝜋 𝑎+𝑟 𝑘
1−𝜋
𝑎
if𝑎=𝑘
(1) 𝑂 𝑝 ∈{1,...,𝑛},aboutwhichoptiontheyprefer.Weagaincallthe
−𝜋 𝑎 otherwise populationvector𝜋 = (𝜋1,...,𝜋 𝑛),whichinthiscontextrepre-
Forconvenience,wedenotetheexpectedpolicyupdateonaction sentsthefractionofindividualssharingeachopinion.Theweighted
𝑎’sprobability𝜋 whensamplingreward𝑟 fromaction𝑘as: voterrulemodelsthedanceofhoneybeesduringnest-hunting[14]:
𝑎 𝑘
(cid:40) Weightedvoterrule:Anyindividual𝑖 ∈ P ofopinion𝑂 𝑖 = 𝑎
𝑑𝜋 𝑎(𝑘)=E 𝑟𝑘∼𝑟(𝑘)[𝑟 𝑘] −1 𝜋− 𝑎𝜋 𝑎 i of t𝑎 he= rw𝑘
ise
(2) f (1o )llo 𝑖w ess tt imhe atw ee si tg hh ete qd uv alo it te yr or fu il te s𝑅 cw urv ro ete nr t:
opinion𝑟 𝑎 ∼𝑟(𝑎),where
InCL,everyreward𝑟 sampledwhenapplyingtheassociated
0≤𝑟
𝑎
≤1.
𝑘 (2) Afterobtaining𝑟 ,𝑖locallybroadcastsitsopinionatafrequency
action𝑘 directlyaffectstheprobabilitiesaccordedbypolicy𝜋 to 𝑎
proportionalto𝑟 .
allavailableactions.Asnotedearlier,CLiscloselyrelatedtothe 𝑎
(3) 𝑖 switchesitsopiniontothefirstopinion𝑏 thatitperceives
GradientBanditalgorithm,whichperformsasimilarupdateatthe
fromitsneighborhood.Assumingallindividualsarewellmixed
parameterlevel(called“preferences”)ofaparametricpolicyrather
inthepopulation[11],thecorrespondingexpectedprobability
thandirectlyupdatingtheprobabilityvector.
of𝑖switchingtoopinion𝑏istheproportionofvotescastfor𝑏:
2.2 Evolutionarygametheory
𝑃(𝑏 ←𝑎)= (cid:205)𝑁 𝑙𝑏 𝑁E 𝑙E[𝑟 [𝑏 𝑟]
𝑙]
(where𝑁
𝑘
isthenumberofindividuals
Evolutionarygametheory(EGT)studiespopulationgames[15].In ofopinion𝑘).Thisprobabilitycanfurtherbewritten (cid:205)𝜋 𝑙𝑏 𝜋E 𝑙E[𝑟 [𝑏 𝑟]
𝑙]
asingle-populationgame,apopulationPismadeofalargenumber bydividingboththenumeratorandthedenominatorby𝑁.
ofindividuals,whereanyindividual𝑖 isassociatedwithatype, Notethatinthismodel,beesdonotdirectlyobservethequality
denotedby𝑇 𝑖 ∈{1,...,𝑛}.Thepopulationvector𝜋 =(𝜋1,...,𝜋 𝑛) estimateofotherindividuals,butonlytheiropinion.Thismakes
representsthefractionofindividualsineachtype((cid:205) 𝑖𝜋
𝑖
=1).In- theweightedvoterrulewell-adaptedtoswarmsofcommunication-
dividualsarerepeatedlypairedatrandomtoplayagame,each limitedorganisms.
receivingaseparatepayoffdefinedbythegamebi-matrix2𝐴.In-
dividualsadapttheirtypebasedonthesepayoffsaccordingtoan 3 THEORY
updaterule.3Onenotablesuchruleisimitationofsuccess[15]:
Remark1. Population-policyequivalence.Asnotedby[2],apop-
ImitationDynamics:Anyindividual𝑖 ∈Poftype𝑇 𝑖 =𝑎follows ulationvector𝜋 =(𝜋1,...,𝜋 𝑛)canbeabstractedasamulti-armed
thevoterrule4𝑅voter:
banditRLpolicy(andvice-versa).Inthisview,uniformlysampling
(1) 𝑖samplesarandomindividual𝑗 ∼U(P)toimitate.Let𝑇 𝑗 be𝑏. anindividualoftype𝑎fromthepopulationisequivalenttosampling
(2) Bothindividuals𝑖and𝑗 playthegamedefinedby𝐴toreceive action𝑎fromthepolicy.
payoffs𝑟
𝑎
and𝑟
𝑏
respectively(0≤𝑟
𝑎,𝑏
≤1).Ingeneral,each
payoffmaydependonthetypesofbothindividuals. 3.1 VotersandCrossLearning
(3) 𝑖switchestotype𝑏withprobability𝑟 5.
𝑏 Proposition1. Aninfinitepopulationofindividualsfollowing
Onecaneasilyseewhythisruleiscalled“imitationofsuccess”:
𝑖 imitates 𝑗 basedon 𝑗’spayoff.Whenaggregatedtotheentire
𝑅 votercanbeseenasanRLagentfollowingExactCrossLearning7 ,
i.e.,
population,imitationofsuccessyieldsafamousequationinEGT,
calledtheTaylorReplicatorDynamic[16,21](TRD)(seeLemma2):
𝑑voter𝜋
𝑎
=E 𝑘∼𝜋,𝑟𝑘∼𝑟(𝑘)[𝑑CL𝜋 𝑎(𝑘,𝑟 𝑘)], (5)
𝜋(cid:164)𝑎 =𝜋 𝑎(𝑞 𝑎𝜋 −𝑣𝜋 ), (3) w ach tie or ne -𝜋 prc oa bn abb ie lits ie ee sn ua ns deb rot th heth pe opp uo lp au til oa nti -o pn olv ice yct eo qr ua in vad leth ne cev ,e 𝑑c vt oo tr ero 𝜋f
where𝜋(cid:164)𝑎isthederivativeofthe𝑎-thcomponentofthepopulation isthesingle-stepchangeofpopulation𝜋 underthevoterrule(i.e.,
vector𝜋 𝑎,𝑞 𝑎𝜋 :=E[𝑟 𝑎]istheexpectedpayoffofthetype𝑎against thechangeintypeproportionsafterallindividualssimultaneously
thecurrentpopulation,and𝑣𝜋 :=(cid:205) 𝑏𝜋 𝑏E[𝑟 𝑏]isthecurrentaverage perform𝑅 voteronce),and𝑑CL𝜋(𝑘,𝑟 𝑘)istheupdateperformedbyCL
payoffoftheentirepopulation.6Further,wedescribeanotheruseful onthepolicy𝜋 foragivenaction-rewardsample(𝑘,𝑟 𝑘).
2Abimatrixgameisdefinedbyapairofidentical-shapematrices,oneperagent. ToproveProposition1,weusetwointermediateresults(Lem-
3Inpopulationgames,anupdateruleissometimescalledarevisionprotocol. mas1and2).Theseresultsarealreadyknownfromtheliterature
4VoterruleisnotaterminologyusedinEGT.Instead,itcomesfromopiniondynamics. (althoughtothebestofourknowledgewearethefirsttointegrate
5Thisdefinitionofvoterrulediffersfromopiniondynamicsasindividualsdonot
themandapplytheminthiscontext).Weprovideproofsusingour
switchdeterministically,butrathermakeaprobabilisticswitch.
6InEvolutionaryGameTheory,expectedpayoffsareoftenreferredtoas“fitness",as
theymodelthereproductivefitnessofthedifferenttypes. 7Wecall"exact"thealgorithmthatappliestheexpectedupdate.formalismforbothLemmas,aswewilllaterfollowasimilarrea-
soningtoprovetheCDM/RLequivalence.Thefirstresultdescribes
∑︁
apolicy-populationequivalencebetweenCLandtheTRD: 𝑑𝜋 𝑎 = 𝑃(𝑎←𝑏)−𝑃(𝑏 ←𝑎) (10)
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
𝑏≠𝑎(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
inflow outflow
Lemma1. Inexpectation,anRLagentlearningviatheCLupdate ∑︁
rulefollows[2]:
= 𝜋 𝑏𝜋 𝑎E[𝑟 𝑎]−𝜋 𝑎𝜋 𝑏E[𝑟 𝑏]
𝑏≠𝑎
(cid:104)∑︁ ∑︁ (cid:105) ∑︁
E 𝑘∼𝜋[𝑑𝜋 𝑎(𝑘)] =𝜋 𝑎(𝑞 𝑎𝜋 −𝑣𝜋 ), (6) =𝜋 𝑎 𝜋 𝑏E[𝑟 𝑎]− 𝜋 𝑏E[𝑟 𝑏] 𝜋 𝑏+𝜋 𝑎 =1
𝑏≠𝑎 𝑏≠𝑎 𝑏≠𝑎
(cid:104) ∑︁ (cid:105)
where𝑞 𝑎𝜋 isthevalueofaction𝑎,and𝑣𝜋 isthevalueofpolicy𝜋. =𝜋 𝑎 (1−𝜋 𝑎)E[𝑟 𝑎]− 𝜋 𝑏E[𝑟 𝑏]
𝑏≠𝑎
(cid:104) ∑︁ (cid:105)
=𝜋 𝑎 E[𝑟 𝑎]− 𝜋 𝑏E[𝑟 𝑏]
Proof. Letuscomputetheexpectationoveractionssampled 𝑏
from𝜋 inEq.2.Forconvenience,wewrite =𝜋 𝑎(𝑞 𝑎𝜋 −𝑣𝜋 ) (11)
E[𝑑𝜋 𝑎] :=E 𝑘∼𝜋[𝑑𝜋 𝑎(𝑘)],andE[𝑟 𝑘] :=E 𝑟𝑘∼𝑟(𝑘)[𝑟 𝑘]:
□
𝑛
E[𝑑𝜋 𝑎] =∑︁ 𝜋 𝑘.𝑑𝜋 𝑎(𝑘) (7) CombiningtheseresultsyieldsProposition1,asLemmas1and2
yield:
𝑘=1
=𝜋 𝑎.𝑑𝜋 𝑎(𝑎)+∑︁ 𝜋 𝑘.𝑑𝜋 𝑎(𝑘) 𝑑voter𝜋 𝑎 =E 𝑘∼𝜋,𝑟𝑘∼𝑟(𝑘)[𝑑CL𝜋 𝑎(𝑘,𝑟 𝑘)]
𝑘≠𝑎
Proposition1showshowTRDconnectsmulti-agentimitationdy-
∑︁
=𝜋 𝑎E[𝑟 𝑎](1−𝜋 𝑎)+ 𝜋 𝑘E[𝑟 𝑘](−𝜋 𝑎) namicsandsingle-agentExactCrossLearning.Inpractice,RLup-
𝑘≠𝑎 datesdonotfollowtheirexactexpectationduetofinitesampling.
=𝜋 𝑎(cid:104) E[𝑟 𝑎]−𝜋 𝑎E[𝑟 𝑎]−∑︁ 𝜋 𝑘E[𝑟 𝑘](cid:105) Theyrelyonactionsamplesfromthepolicyandrewardsamples
fromtheenvironment.Tocircumventhighvarianceandimprove
𝑘≠𝑎
(cid:104) ∑︁ (cid:105) convergenceproperties,RLpractitionerstypicallyperformthese
=𝜋 𝑎 E[𝑟 𝑎]− 𝜋 𝑘E[𝑟 𝑘] policyupdatesinabatchedfashion,which,accordingtoPropo-
𝑘 sition1,isequivalenttomakingtheseupdatesclosertoinfinite-
=𝜋 𝑎(𝑞 𝑎𝜋 −𝑣𝜋 ) (8) populationdynamics.Infact,thereisanaptpopulation-basedin-
terpretationofthispractice,showninthenextsection.
□
3.2 Learningrateandbatch-size
Insteadofstudyingthemean-fieldeffectofaggregatedindividual
Theterm𝑞 𝑎𝜋 −𝑣𝜋 iscommonlyknownasthe“advantage”of voters,wecanlookattheindividualeffectofeachvoteronthe
action𝑎inRL.Fromthatperspective,itdescribeshowgoodaction𝑎 entirepopulation.Thiseffectyieldsinterestinginsightsregarding
isincomparisontothecurrentpolicy𝜋.ButRemark1enables twocommonpracticesinRL:adjustingthelearningrate,i.e.,scaling
lookingatLemma1underadifferentlight:theright-handsidesof downRLupdatesbyasmallfactor,andbatching,i.e.,averagingRL
Eqs.3and6areequivalent.Inotherwords,underthepopulation- updatesoverseveralsamples.
policy equivalence, the CL update rule tangentially follows the Insteadofaninfinitepopulation,letusconsideranear-infinite8
TRD(inexpectation).Furthermore,itisknownthatapopulation population P of 𝑁 ≫ 1 individuals. Again, we describe P by
adopting𝑅voteralsoyieldstheTRD: thepopulationvector𝜋.Asingleindividual𝑖oftype𝑘sampling
payoff𝑟
𝑘
andfollowing𝑅voterhasthefollowinginfluence(outflow
Lemma2. Aninfinitepopulationofindividualsadopting𝑅
voter
from𝑎to𝑘)onthepopulationvectorfortypes𝑎≠𝑘:
followstheTRD[15,16]i.e.,
1
∀𝑎≠𝑘,𝑃(𝑖)(𝑘 ←𝑎)= 𝜋
𝑎 𝑁
𝑟
𝑘
(12)
𝑑𝜋
𝑎
=𝜋 𝑎(𝑞 𝑎𝜋 −𝑣𝜋 ), (9) (cid:124) ty(cid:123) p(cid:122) e(cid:125)
a (cid:124)(cid:123)(cid:122)(cid:125)
andswit(cid:124) ch(cid:123) in(cid:122) g(cid:125)
totypek
pickingi
whileitsinfluenceontype𝑘isthesumofinflows(to𝑘):
Proof. Let𝑃(𝑎←𝑏)denotetheinflowofindividualsoftype𝑏
i an dt oo pt ty inp ge t𝑎 y, pi. ee, 𝑎t .h Te hp ero pp oo pr ut li ao tn ioo nf hth ase apo pp rou pla ot rio tin onle oa fvi 𝜋ng it ny dp ie vi𝑏 dua an ld s ∑︁ 𝑃(𝑖)(𝑘 ←𝑎)=(1−𝜋 𝑘) 𝑁1 𝑟 𝑘 . (13)
𝑏
oftype𝑏,eachhavingaprobability𝜋 ofmeetinganindividualof
𝑎≠𝑘
𝑎
type𝑎,andaconditionalprobabilityE[𝑟 𝑎]ofswitchingtoitstype.
Thus,weget𝑃(𝑎←𝑏)=𝜋 𝑏𝜋 𝑎E[𝑟 𝑎]: 8Theassumption𝑁 ≫1enablesapproximatingflowsbytheirexpectation.Denoting𝛼 := 1 yieldsthefollowinglearningruleattributableto entire population P, whereas there is no such explicit effect in
𝑁
asingleindividualontheentirepopulationvector: 𝑅voter.However,theweightedvoterrule𝑅wvoterdoescontainan
(cid:40) expliciteffect,makingtheanalysismuchmoreintuitive.
∀𝑎,𝜋
𝑎
←𝜋 𝑎+𝛼𝑟
𝑘
1−𝜋
𝑎
if𝑎=𝑘
. (14) InthisSection,wewillshowthat,similartohowimitationof
−𝜋 𝑎 otherwise successyieldstheCLupdaterule,whentheentirepopulationis
NotehowtheRLupdatedescribedinEq.14differsfromtheone consideredasanabstractRLagent,swarmsofbeesperforming
describedinEq.1onlybyascalingfactor𝛼 = 1. CDMforhouse-huntingfollowanabstractRLalgorithmthatwe
𝑁
Thepopulation-policyequivalencegivesaninterestinginter- coinMaynard-CrossLearning.
pretationtothelearningrate𝛼 commonlyusedinRL.Underthe Letusnowconsideranear-infinitepopulationof𝑁 honeybees,
populationperspective,𝛼 describesthenumberofindividualsin reachingaconsensusonwhichnestingsitetoselectvia𝑅wvoter.
thepopulation.InSec.5,weempiricallyshowthattheCLupdate Under𝑅wvoter,individualshaveatangibleinfluenceontherest
ruledescribedinEq.1doesnottypicallyconvergetotheoptimal of the population: remember that in this model, individuals de-
action.However,usingasmallenoughlearningrate(i.e.,alarge terministicallyswitchtothefirstactiontheywitness.Hence,the
enoughpopulationsize)alleviatesthisissue9. influenceofeachindividualisequaltotheratioofitsbroadcast-
Todescribetheaggregatedeffectof𝑅voterontheentirepopu- ingfrequency𝑟 𝑘,bythetotalbroadcastingfrequencyoftheentire
lation,wecannowsumtheeffectdescribedinEq.14acrossall population(cid:205) 𝑗𝑟(𝑗).Inotherwords,anindividual𝑖oftype𝑇 𝑖 =𝑘
individuals.Letusdenote𝑟(𝑖) thepayoffsampledbyindividual𝑖, andpayoffsample𝑟 𝑘 ∼𝑟(𝑘)hasthesameinfluenceonallother
𝑁 thenumberofindividualsoftype𝑘,and𝑞𝜋 theaveragepayoff membersofthepopulation:
𝑘 𝑘
acrossindividualsoftype𝑘.Theaggregatedupdateis:
𝑑𝜋
𝑎
=∑︁ 𝑖𝑁
=0
𝑟 𝑁(𝑖) (cid:40) −1 𝜋− 𝑎𝜋 𝑎 i of t𝑎 he= rw𝑘
ise
(15) 𝑃(𝑖)(𝑘 ←·)=
(cid:205)
𝑗𝑟 𝑟𝑘
(𝑗)
. (17)
(cid:40)
= 𝑁1 ∑︁
𝑘
𝑁 𝑘𝑞 𝑘𝜋 −1 𝜋− 𝑎𝜋 𝑎 i of t𝑎 he= rw𝑘
ise Thus,theinflowfromtype𝑏totype𝑘attributableto𝑖is
(cid:32) (cid:33)
= 𝑁1 𝑁 𝑎𝑞 𝑎𝜋 (1−𝜋 𝑎)−𝜋 𝑎∑︁ 𝑁 𝑘𝑞 𝑘𝜋
𝑟
= 𝑁1
(cid:32)
𝑁 𝑎𝑞 𝑎𝜋 −𝜋 𝑎∑︁
𝑘
𝑁 𝑘𝑞
𝑘𝜋𝑘 (cid:33)≠𝑎 𝑃(𝑖)(𝑘 ←𝑏)= =𝜋
𝑁1𝑏 𝜋(cid:205) 𝑏𝑗𝑟
1𝑘
( (cid:205)𝑗)
𝑟 𝑘
𝑟(𝑗)
(18)
𝑁 𝑗
=𝜋 𝑎𝑞 𝑎𝜋 −𝜋 𝑎∑︁ 𝜋 𝑘𝑞 𝑘𝜋 (𝜋 𝑘 = 𝑁 𝑁𝑘 ) =𝛼 𝑣𝑟 𝑘 𝜋𝜋
𝑏
. (19)
𝑘
=𝜋 𝑎(𝑞 𝑎𝜋 −∑︁ 𝜋 𝑘𝑞 𝑘𝜋 )
𝑘 Andthetotalinflowintotype𝑘attributableto𝑖is
=𝜋 𝑎(𝑞 𝑎𝜋 −𝑣𝜋 ), (16)
whichistheTRD.
Notehow,asacorollaryofProposition1,summingtheupdate ∑︁ 𝑃(𝑖)(𝑘 ←𝑏)=∑︁ 𝛼 𝑣𝑟 𝑘 𝜋𝜋
𝑏
(20)
fromEq.14overthepopulationexactlyyieldstheexpectationof
𝑏≠𝑘 𝑏≠𝑘
theCLupdateruledescribedinEq.1.BysummingEq.14,wehave 𝑟
retrievedthesameupdateaswhataveragingEq.1overalarge
=𝛼 𝑣𝑘 𝜋(1−𝜋 𝑘) . (21)
batchwouldhaveestimated:itsexpectation,whichistheTRD.10
FromtheRLperspectivethisresultmeansthatbatchingupdates
removestheneedforusingasmalllearningrate(seeSec.5),atleast Eqs.19and21yieldanRLupdateruledescribingtheeffectofa
ingradient-freemulti-armedbanditswhereouranalysisprovides singleindividualandcorrespondingsampledpayoff(i.e.,reward
mathematicalgroundingtothiscommonlyacceptedruleofthumb. sample)overtheentirepopulation(i.e.,policy),called:
Maynard-CrossLearning(MCL).Let𝑘beanactionand𝑟
𝑘
∼𝑟(𝑘)
3.3 SwarmsandMaynard-CrossLearning acorrespondingrewardsample.MCLupdatesthepolicy𝜋 as:
Arguably,themeaningofEq.14isnon-intuitivefromthepopulation
perspective:itdescribestheeffectofasingleindividual𝑖 onthe
(cid:40)
9 saA ms pim ledpl ii ne dd ivb iy duE aq ls.1 in4 s, ta es as du om fi pn ag rat lh lea lt l, yw inhe on nep se tr ef por am cre od ssse thq eue en nt ti ia relly poo pn ulr aa tn iod no ,m thly
e
∀𝑎,𝜋 𝑎 ←𝜋 𝑎+𝛼 𝑣𝑟 𝑘 𝜋 1 −𝜋− 𝑎𝜋 𝑎 oif t𝑎 he= rw𝑘 ise (22)
voterrulestillyieldstheReplicatorDynamic,whichweconjecture.Weleaveaproof
ofthisconjectureforfuturework.
10Asexpectedfromthepopulationperspective,sincethisderivationisessentially
anotherproofofLemma2. where𝑣𝜋 isthecurrentvalueofpolicy𝜋.Findingtheaggregatedpopulationeffectamountstosumming 4.1 Environment
Eq.22acrossallindividuals: Weconsiderthestandardmulti-armedstatelessbanditsettingde-
scribedinpreliminaries(seeSec.2.1).AsitisclearfromRemark1,
𝑑𝜋
𝑎
=∑︁ 𝑖𝑁
=0
𝑁𝑟( 𝑣𝑖 𝜋) (cid:40) 1 −𝜋− 𝑎𝜋 𝑎 i of t𝑎 he= rw𝑘
ise
(23) w me enc ta sn .Tu hs ee et nh ve irs oa nm me ee nn tv ii nro cn onm se idn et rf ao tir oR nL rea tn ud rnp so rp eu wl aa rt dio sn sae mxp pe ler di-
=∑︁
𝑘
𝑁 𝑁𝑘 𝑣𝑞 𝜋𝑘𝜋 (cid:40) 1 −𝜋− 𝑎𝜋 𝑎 i of t𝑎 he= rw𝑘
ise
𝑄f br u 𝑎o 𝜋tm io ∈nth (Ne −h ∞(𝑄id ,𝑎𝜋 +de ∞,𝜎n )2d ) ii ss istr tu hib s eu edt mio t eon ag𝑟 ne(𝑎 n o) ferw Nath e ,e an th n𝑎 e dsi es 𝜎rp 2eu w tl hl ae erd d. vA s aa rn m iao p nr lm ce esa .,l w Td hhis eet srr eei-
= 𝑁1 𝑣𝜋(𝑁 𝑎𝑞 𝑎𝜋 (1−𝜋 𝑎)− 𝑘∑︁ ≠𝑎𝑁 𝑘𝑞 𝑘𝜋𝜋 𝑎) r
f
hue iw
dn
dca
t
er
i
nd os
n
din
𝑠
s(e
t𝑟
re i)d
b=
ut to
i1
o+b n𝑒1e
−
𝑟𝑟b (o 𝑎isu ).n
u
Md see odd reob one vt e𝑟w r,∼e te hNn is([ t𝑄0 r, a𝑎𝜋1 n,]
𝜎
s, f2f oo
)
r,r mmw
aa
th
k
ioi ic
n
nh
g
ss qti
h
ug
i
em
s
eto
zh
eid
e
s
= 𝑁1 𝑣𝜋(𝑁 𝑎𝑞 𝑎𝜋 −∑︁
𝑘
𝑁 𝑘𝑞 𝑘𝜋𝜋 𝑎) 𝑞N 𝑎𝜋a ∈nd [0s ,h 1if ]t .s Tth he ism
𝑞
𝑎e 𝜋an caa nw ba ey efr so tim m𝑠 a( t𝑄 ed𝑎𝜋 a) sto Ea 𝑟∼n 𝑟e (w 𝑎)m [𝑟e ],an byde sn amot ped linb gy
= 𝑣1 𝜋(𝜋 𝑎𝑞 𝑎𝜋 −𝜋 𝑎∑︁ 𝜋 𝑘𝑞 𝑘𝜋 ) a inl gar thge emn .u Fm ub rte hr eo r,f ts ha rm eep dle iffs e( r1 e0 n7 ts ka im ndp sle os f) ef nro vm iro𝑟 n( m𝑎) ena tn sd ara ev uer sa eg d-
,
=
𝜋
𝑎 (𝑞𝜋𝑎 −𝑣𝜋 )
𝑘
(24)
where∀𝑎:𝑞 𝑎𝜋’sarenear0,spreadbetween0and1,ornear1.
𝑣𝜋
4.2 RLexperiments
whichistheMRD.
Non-batched:Intheseexperiments,anRLagentstartswithan
We have shown that a population whose individuals follow
initialrandompolicy𝜋.Theagentthensamplesonlyoneaction𝑘
𝑅wvoter aggregatestotheMRD.AnargumentsimilartoSec.3.2
from 𝜋 in an iterative fashion. Further, pulling action𝑘 in the
yieldsthe“batched”versionofEq.22,thatwecallExactMaynard-
CrossLearning(EMCL):11
environment,theagentreceivesanoisyrewardsignal𝑟
𝑘
∼𝑟(𝑘).
ForCL,theagentutilizesEq.14tomakeanupdate.Whereasfor
(cid:40) MCL,Eq.22cannotbeuseddirectly,sincewedonothaveaccess
∀𝑎,𝜋 𝑎 ←𝜋 𝑎+E 𝑘,𝑟𝑘𝑣𝑟 𝑘 𝜋 1 −𝜋− 𝑎𝜋 𝑎 i of t𝑎 he= rw𝑘 ise (25) t oo v𝑣 e𝜋 r. rW ewe at rh de sr ,e wfo hre e, ra ep 𝛾p ir sox ai wm ea it ge h𝑣 t𝜋 inb gy fe am ctp ol roy foin rg rea cm eno tv rin ewga av rder sa :ge
EMCListheRLalgorithmfollowedbyswarmsofbeesthatmakea 𝑟¯←𝛾𝑟+(1−𝛾)𝑟¯. (27)
collectivedecisionvia𝑅wvoter:
Moreover,sincethisupdaterulecanmake𝜋 invalid,i.e.,compo-
Proposition2. Aninfinitepopulationofindividualsfollowing nentscouldbecomenegativeoraboveone(seefootnote11),we
𝑅 wvotercanequivalentlybeseenasanRLagentfollowingEMCL clamp𝜋 between0and1:
(cid:32) (cid:40) (cid:33)
𝑑wvoter𝜋 𝑎 =𝑑EMCL𝜋 𝑎, (26) ∀𝑎:𝜋
𝑎
←clamp 𝜋 𝑎+𝑟 𝑟¯𝑘 −1 𝜋− 𝑎𝜋 𝑎 i of t𝑎 he= rw𝑘
ise
(28)
where𝜋isboththepopulationvectorandthepolicyunderthepopulation-
policyequivalence,𝑑wvoter𝜋 isthesingle-stepchangeofpopulation𝜋 Wecallthistrainingsteparun,andperform𝑅runsperseed.
undertheweightedvoterrule(i.e.,thechangeintypeproportionsafter Batched:Intheseexperiments,weimplementthebatchedvari-
allindividualssimultaneouslyperform𝑅 wvoteronce),and𝑑EMCL𝜋 is antsofupdaterulesCL(Eq.2)andMCL(Eq.25),henceforthnamed
theupdateperformedbyEMCLonthepolicy𝜋. B-CLandB-MCL.B-CLisastraightforwardbatchingoftheCL
updaterule,averagingoverabatchof𝐵samplesinsteadofonesam-
Proof. TheproofofProposition2istrivialatthispoint.Wehave pletoupdate𝜋.Whereas,withB-MCL,𝑣𝜋 isnolongeramoving
alreadyshownthat𝑑wvoter𝜋 istheMRD,anddividingeverything averageoftherewardsbutratherthemeanofbatchrewards.We
by𝑣𝜋 intheproofofLemma1(startingfromEq.8)yieldsthat alsoneedtoexplicitlyclampthepolicybetween0and1toensure
𝑑EMCL𝜋 isalsotheMRD. □ itremainsvalid.B-MCLalsouses𝐵samplessimultaneouslysimilar
toB-CL.Similartonon-batchedexperiments,weperform𝑅runs
perseed.
4 METHODS
Wepresenttwotypesofexperimentstovalidatethefindingsfrom
4.3 PopulationExperiments
theprevioussection.First,weimplementthetwoRLupdaterules,
CLandMCLintwovariants:batchedandnon-batched.Second,we Inthissection,wefocusonthepopulationupdaterules,VR,and
conductpopulation-basedexperimentsusing𝑅voterand𝑅wvoter(VR, WVR(seepreliminariesSecs.2.3and2.2).Sincewecannotsimu-
WVR)fordifferentpopulationsizes.Moreover,wealsonumerically latePforinfinitesizes,wechoosetwofinitepopulationsizesof
simulatetheTRDandMRDtoshowhowtheaboveexperiments 10and1000.ForbothVRandWVR,westartwithanequalpro-
comparewiththeanalyticalsolutions. portionofindividualsassociatedwithanytype/opinion.Further,
eachindividualreceivesastochasticpayoff/qualityestimateforits
type/opinion.Thereafter,withVR,everyoneispairedwithanother
11MCLhasavalidimplementationonlywhenthelearningrate𝛼issmallenough,
randomindividual.Allindividualsthengeneratearandomnumber
whileEMCLhasavalidimplementationwhenthebatch-size𝑁usedtoestimatethe
expectationislargeenough.Inbothcases,𝑣𝜋alsoneedstobeestimated. between0and1,andiftherandomnumberisgreaterthanthepayoffofthepairedindividual,theyswitchtotheirpartner’stype ConvergencerateofMRDis≥TRD.Asnotedby[15],TRDand
(rule3of𝑅voter).WhereaswithWVR,eachindividualswitchesto MRDcanberearrangedintheform:
anopinionsampledfromthedistributiondefinedby𝑣,where𝑣 𝜋 𝑞𝜋
istheratioofvotesfortype𝑖bythetotalnumberofvotesinthe 𝜋(cid:164)𝑎 =𝑣𝜋 ( 𝑣𝑎 𝜋𝑎 −𝜋 𝑎) (TRD) (32)
population:12
𝜋 𝑞𝜋
𝑣 𝑖 =
∀𝑝∈P(cid:205) (cid:205):𝑂𝑝 𝑟= 𝑞𝑖𝑟 𝑝
. (29)
𝜋(cid:164)beingtheu𝜋 p(cid:164)𝑎 da= te1 "( sp𝑣𝑎 e𝜋 e𝑎 d"− an𝜋 d𝑎 𝑣)
𝜋
beingbound( eM dR bD et) ween0an( d33 1)
.
∀𝑞∈P TheMRDspeedisthusgreaterthantheTRDspeed.Empirically,we
SimilartoRLexperiments,weperform𝑅runsperseed. observethatMRDconvergesfasterthanTRD,especiallywhenthe
𝑞𝜋’sarecloseto0,asseeninthefirstcolumnofFigure1.Whereas,
𝑎
4.4 TRDandMRD whenthe𝑞 𝑎𝜋’sarenear1thereisnovisibledifference(as𝑣𝜋 ≈1).
ToempiricallyvalidatePropositions1and2,wenumericallysimu- Byextension,thisalsoimpliesthatMCL(forsmall𝛼),B-MCL(for
lateboththedifferentialequations3(TRD)and4(MRD).Asthese largebatchsize),andWVR(forlargepopulation)haveconvergence
equationsarecontinuous,wediscretizethembyastep𝛿(discretiz- rates≥CL,B-CL,andVRrespectively.
ingstep).Further,westartfromaninitialrandompopulation/policy PopulationexperimentswithVRandWVRfollowTRDand
(𝜋)andsimulateitsevolutionaccordingtoTRDandMRDbetween MRDrespectivelyforlargepopulations.ItcanbeseeninFigure
timeintervals[0,𝑡 𝑓],usingtheprivilegedinformation𝑞 𝑎𝜋 notavail- 3thatbothVRandWVRfollowTRDandMRDrespectivelywhen
abletoRLandpopulationexperiments. thepopulationsizeislarge.SeesupplementarySec.A.2forother
𝜋 𝑎 ←𝜋 𝑎+𝛿𝜋 𝑎[𝑞 𝑎𝜋 −∑︁ 𝜋 𝑙𝑞 𝑙𝜋 ] (30) e stn av ri tr do en vm iae tn ints g.A fros mso to hn ea as nt ah lye tp ico ap lu sl oa lt ui to in ons .h Sr ii mnk ils a, rV tR ota hn ed RW LV exR -
𝑙 periments,WVRperformspoorlyincomparisontoVRforsmall
𝜋 𝑎 ←𝜋 𝑎+𝛿 𝑣𝜋 𝜋𝑎 [𝑞 𝑎𝜋 −∑︁ 𝜋 𝑙𝑞 𝑙𝜋 ] (31) populationsizes.
𝑙 Finally,fromthediscussionsrelatedtobatchedRLupdatesand
populationexperiments,weempiricallyvalidateProposition1and
5 RESULTS Proposition2.
Forallexperiments,weusethehyperparametersdescribedinsup-
plementarySec.A.3. 6 CONCLUSIONS
WithPropositions1and2,wehavedemonstratedhowRDisthe
Non-batchedRLupdaterulesCLandMCLfollowTRDand underlyingconnectionbetweenReinforcementLearningandCol-
MRDrespectivelywhenthelearningrate(𝛼)issmall.These lectiveDecision-Making.Further,wehaveempiricallyvalidated
resultsarepresentedinFigures1and2.Forallenvironments,CL thiscorrespondence.Thiscorrespondenceopensabridgebetween
andMCLfollowTRDandMRDrespectively,whichcanbeexplicitly thesetwofields,enablingtheflowofideas,newperspectives,and
seenwiththedottedlineoftheanalyticalsolutions(TRD,MRD)ex- analogiesforboth.Forexample,itcanbeseenthatCrossLearn-
actlyatthecenteroftheaveragerewardcurvesoftheCLandMCL ing,Maynard-CrossLearning,and,moregenerally,Reinforcement
updaterules.Thisempiricallyvalidatesthat,withasmall𝛼,Eqs.14 Learningtakeasingle-agentperspective,whereinformationfrom
and22followtheTRDandMRDrespectively,evenwheniteratively consecutiveactionsamples/batchesisaccumulatedintoonecentral-
appliedwithsinglesamples.However,assoonas𝛼isincreased,CL izedagent’spolicy.Ontheotherhand,𝑅votersand𝑅wvoterstakea
andMCLstartdeviatingfromtheirrespectiveanalyticalsolutions multi-agentperspective,whereeveryindividualimplementssimple
(seeFigure2).Thisisawell-knowneffectinoptimizationliterature localanddecentralizedrules,makingindependentdecisions,which
butfromapopulationperspective(seeSec.3.2)weseethatalarger leadstoanemergentcollectivepolicy.
𝛼 correspondstoasmallerpopulation,henceleadingtoapoor SignificanceforRL.SimilartohowwediscoveredanewRL
approximationoftheexpectedupdate.Further,wealsoobserve updaterule(i.e.,Maynard-CrossLearning)fromSwarmIntelligence,
thatMCLperformspoorlycomparedtoTRDwithlarger𝛼. otherideassuchasmajorityrule[22],andcross-inhibition[13],
BatchedRLupdaterulesB-CLandB-MCLfollowTRDand canbeusedtocreatenewupdaterulesforReinforcementLearn-
MRDrespectivelywhenthebatchsize(𝐵)islargeenough.As ing.Moreover,SwarmIntelligencealgorithmsareofteninspiredby
seeninFigure4,itisclearthatB-CLandB-MCLfollowTRDand nature,andthusrequireindividualstofollowphysics.Thisman-
MRDrespectivelyforlargebatchsizes(thiscanbeseenfromhow datespracticalconstraintssuchascongestion[19],communication,
theanalyticalsolutionisexactlyatthecenteroftheaveragere- andfinitesizeeffects,whicharegenerallyignoredinReinforce-
wardcurvesofB-CLandB-MCL).However,assoonwemakethe mentLearningandpopulationgames.Comparingtheperformance
batchsizessmaller,thebatchedupdatesdeviatefromtheiranalyti- ofReinforcementLearningagentswiththeirequivalentswarm
calsolutions(seesub-section3.2).SeesupplementarySec.A.1for counterpartsonsuchconstraintsisadirectionforfuturework.
otherenvironments.WealsoobservethatB-MCLperformspoorly SignificanceforSI.Thepopulation-policyequivalencehigh-
comparedtoB-CLwithsmallerbatchsizes,similartoobservations lightshowcertainSwarmIntelligencemethodsareequivalentto
madewithnon-batchedRLexperiments. single-agentReinforcementLearning,demonstratingagencyofthe
entireswarmasasinglelearningentity.Therefore,onecouldimag-
12Weimplementthethirdruleof𝑅wvoterinacentralizedfashionforthesesimple
numericalsimulations,butinreality,itisacompletelydecentralizedrule. inethatMulti-AgentReinforcementLearningwouldsimilarlyyieldRLnon-batchedexperiments-qπ’s: nearzero,α: 0.001
a
0.32
100
0.26
80
0.20
60
0.13
40
0.07
20
0.01
-0.05 0
RLnon-batchedexperiments-qπ’s: evenlyspaced,α: 0.001
a
1.03
100
0.87
80
0.70
60
0.53
40
0.36
20
0.20
0.03 0
RLnon-batchedexperiments-qπ’s: nearone,α: 0.001
a
1.08 100
1.01 80
0.94
60
0.87
40
0.80
20
0.73
0.66 0
0 20 40 60 80 100 0k 20k 40k 60k 80k 100k 0k 20k 40k 60k 80k 100k 0k 20k 40k 60k 80k 100k
Time Runs Runs Runs
TRD MRD MCL CL
Figure1:Resultsfornon-batchedRLexperimentswithsmall𝛼.Thedottedlinesrepresentthemeanrewardaccordingtothe
analyticalsolutions.Thedarkershadesrepresentthemeanreward(orpercentageofoptimalactions)andthelightershade
representstheirvariancefortheCLandMCLupdaterules.Astherewardscalesaredifferentacrossenvironments(rows),itis
importanttolookattheratioofoptimalactions(lastcolumn)
RLnon-batchedexperiments-qπ’s: betweenzeroandone,α: 0.1
a
1.01 100
0.86
80
0.71
60
0.56
40
0.40
20
0.25
0.10 0
0 20 40 60 80 100 0 200 400 600 800 1000 0 200 400 600 800 1000 0 200 400 600 800 1000
Time Runs Runs Runs
TRD MRD MCL CL
Figure2:Resultsfornon-batchedRLexperimentsforlarge𝛼.Itisclearthatwithalargeralpha,theRLsolutionsdivergefrom
theanalyticalsolutions.
draweR
draweR
draweR
draweR
%Optimalactions
%Optimalactions
%Optimalactions
%OptimalactionsPopulationExperimentsqπ’s: betweenzeroandone,size=10
a
0.98 100
0.87
80
0.77
60
0.67
0.56 40
0.46 20
0.36
PopulationExperimentsqπ’s: betweenzeroandone,size=1000
a
0.94
100
0.85
80
0.76
60
0.67
0.58 40
0.49 20
0.40
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Time Runs Runs Runs
TRD MRD VR WVR
Figure3:Resultsforpopulationexperiments.Thedottedlinesrepresenttheaveragepayoff/qualityaccordingtotheanalytical
solutions.Thedarkershadesrepresentthemeanpayoff/qualityand%ofpopulationswiththeoptimaltype/opinion,whilethe
lightershaderepresentsthevarianceinpayoffwithVRandopinionwithWVR.
RLbatchedexperimentsqπ’s: betweenzeroandone,batchsize=10
a
0.98 100
0.87
80
0.75
60
0.64
0.52 40
0.41 20
0.29
RLbatchedexperimentsqπ’s: betweenzeroandone,batchsize=1000
a
0.94
100
0.85
80
0.76
60
0.67
0.58 40
0.49 20
0.40
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Time Runs Runs Runs
TRD MRD B-CL B-MCL
Figure4:ResultsforbatchedRLexperiments.Thedottedlinesrepresenttheaveragerewardaccordingtotheanalyticalsolutions.
Thedarkershadesrepresentthemeanrewardand%optimalaction,whilethelightershaderepresentsthevarianceinrewards
withB-CLandB-MCL.
ytilauq/ffoyaP
ytilauq/ffoyaP
draweR
draweR
%Optimaltype/opinion
%Optimaltype/opinion
%Optimalaction
%Optimalactionequivalentmulti-swarmsystems,wheretwoormorecoexisting 05.004
swarmswouldcompete/collaborateforresources(i.e.,prisoners [13] AndreagiovanniReina,JamesA.R.Marshall,VitoTrianni,andThomasBose.
dilemma,hawkdove,etc).Further,ideasthatempowerReinforce-
2017.Modelofthebest-of-Nnest-siteselectionprocessinhoneybees.Physical
ReviewE95,5(May2017). https://doi.org/10.1103/physreve.95.052411
mentLearning,couldbeportedtoswarmintelligenceandswarm [14] AndreagiovanniReina,ThierryNjougouo,ElioTuci,andTimoteoCarletti.2024.
robotics. Speed-accuracytrade-offsinbest-of-𝑛collectivedecisionmakingthroughhet-
erogeneousmean-fieldmodeling.Phys.Rev.E109(May2024),054307.Issue5.
https://doi.org/10.1103/PhysRevE.109.054307
REFERENCES [15] WilliamHSandholm.2010.Populationgamesandevolutionarydynamics.MIT
press.
[1] PeterAuer,NicolòCesa-Bianchi,andPaulFischer.2002. Finite-timeAnalysis
[16] WilliamH.Sandholm,EminDokumacı,andRatulLahkar.2008.Theprojection
o hf ttt ph se ://M dou il .oti ra gr /m 10e .d 10B 2a 3n /Adi :1t 0P 1r 3o 6b 8l 9e 7m 0. 43M 52achineLearning47(052002),235–256. dynamicandthereplicatordynamic.GamesandEconomicBehavior64,2(2008),
666–683. https://doi.org/10.1016/j.geb.2008.02.003 SpecialIssueinHonorof
[2] DaanBloembergen,KarlTuyls,DanielHennes,andMichaelKaisers.2015.Evo-
MichaelB.Maschler.
lutionaryDynamicsofMulti-AgentLearning:ASurvey. JournalofArtificial [17] JaeminSeo,SangKyeunKim,AzarakhshJalalvand,RoryConlin,AndrewRoth-
IntelligenceResearch53(082015),659–697. https://doi.org/10.1613/jair.4818
stein,JosephAbbate,KeithErickson,JosiahWai,RicardoShousha,andEgemen
[3] EricBonabeau,MarcoDorigo,andGuyTheraulaz.1999.SwarmIntelligence:From
Kolemen.2024.Avoidingfusionplasmatearinginstabilitywithdeepreinforce-
N osa ot /u 9r 7a 8l 0t 1o 9A 51rt 3i 1fi 5c 8ia 1l .0S 0y 1s .t 0e 0m 0s 1.OxfordUniversityPress. https://doi.org/10.1093/ mentlearning.Nature626(022024),746–751. https://doi.org/10.1038/s41586-
024-07024-9
[4] ClaudioCastellano,SantoFortunato,andVittorioLoreto.2009.Statisticalphysics
ofsocialdynamics.ReviewsofModernPhysics81,2(May2009),591–646. https: [18] J Uo nh in veM rsa ity yn Pa rr ed ssS ,m Ci at mh. b1 r9 id8 g2 e. ,UEv Ko .lutionandtheTheoryofGames. Cambridge
//doi.org/10.1103/revmodphys.81.591
[19] KarthikSoma,VivekShankarVardharajan,HeikoHamann,andGiovanniBel-
[5] JohnG.Cross.1973.AStochasticLearningModelofEconomicBehavior.The
trame.2023.CongestionandScalabilityinRobotSwarms:AStudyonCollective
oQ ru ga /r Rt ee Prl Ey cJ :oou ur pn :qa jl eo cf oE nc :vo :n 8o 7m :yi :c 1s 98 77 3, :i2 :2( :p19 :27 33 9) -, 22 63 69 .–266. https://EconPapers.repec. DecisionMaking.In2023InternationalSymposiumonMulti-RobotandMulti-
[6] MarcoDorigo,MauroBirattari,andThomasStutzle.2006. Antcolonyopti-
AgentSystems(MRS).199–206. https://doi.org/10.1109/MRS60187.2023.10416793
/m /diz oa i.t oio rgn /. 10IE .1E 1E 09C /o Mm Cp Iu .2t 0a 0ti 6o .3n 2a 9l 6In 91telligenceMagazine1,4(2006),28–39. https:
[20] R duic ch tia or nd .S MS ITut pto rn esa s.ndAndrewGBarto.2018.Reinforcementlearning:Anintro-
[21] PeterD.TaylorandLeoB.Jonker.1978.Evolutionarystablestrategiesandgame
[7] MarcoDorigo,GuyTheraulaz,andVitoTrianni.2021. SwarmRobotics:Past,
Present,andFuture[PointofView].Proc.IEEE109,7(2021),1152–1165. https: 1d 0y 1n 6a /m 00i 2c 5s. -5M 56a 4th (7e 8m )9a 0t 0ic 7a 7l -B 9iosciences40,1(1978),145–156. https://doi.org/10.
//doi.org/10.1109/JPROC.2021.3072740
[22] GabrieleValentini,EliseoFerrante,HeikoHamann,andMarcoDorigo.2016.Col-
[8] HeikoHamann.2018.SwarmRobotics:AFormalApproach. https://doi.org/10.
lectivedecisionwith100Kilobots:speedversusaccuracyinbinarydiscrimination
1007/978-3-319-74528-2 problems. AutonomousAgentsandMulti-AgentSystems30,3(2016),553–580.
[9] Elia Kaufmann, Leonard Bauersfeld, Antonio Loquercio, Matthias Mueller, https://doi.org/10.1007/s10458-015-9323-3
VladlenKoltun,andDavideScaramuzza.2023. Champion-leveldroneracing [23] GabrieleValentini,HeikoHamann,andMarcoDorigo.2014. Self-organized
usingdeepreinforcementlearning. Nature620(082023),982–987. https: collectivedecisionmaking:theweightedvotermodel.InProceedingsofthe2014
//doi.org/10.1038/s41586-023-06419-4 InternationalConferenceonAutonomousAgentsandMulti-AgentSystems(Paris,
[10] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis France)(AAMAS’14).InternationalFoundationforAutonomousAgentsand
Antonoglou,DaanWierstra,andMartinA.Riedmiller.2013. PlayingAtari MultiagentSystems,Richland,SC,45–52.
withDeepReinforcementLearning.CoRRabs/1312.5602(2013).arXiv:1312.5602 [24] PKirkVisscherandScottCamazine.1999.Collectivedecisionsandcognitionin
http://arxiv.org/abs/1312.5602 bees.Nature397,6718(February1999),400. https://doi.org/10.1038/17047
[11] MartinA.Nowak.2006. FiveRulesfortheEvolutionofCooperation. Sci- [25] RonaldJWilliams.1992.SimpleStatisticalGradient-FollowingAlgorithmsfor
ence 314, 5805 (2006), 1560–1563. https://doi.org/10.1126/science.1133755 ConnectionistReinforcementLearning.,229-256pages.
arXiv:https://www.science.org/doi/pdf/10.1126/science.1133755 [26] HaoxiangXia,HuiliWang,andZhaoguoXuan.2011. Opiniondynamics:A
[12] SidneyRedner.2019. Reality-inspiredvotermodels:Amini-review. Comptes multidisciplinaryreviewandperspectiveonfutureresearch.InternationalJournal
Rendus.Physique20,4(May2019),275–292. https://doi.org/10.1016/j.crhy.2019. ofKnowledgeandSystemsScience(IJKSS)2,4(2011),72–91.A SUPPLEMENTARYMATERIAL
A.1 BatchedRLexperiments
RLbatchedexperimentsqaπ’s:nearzero,batchsize=1000
0.18
100
0.15
80
0.12
60
0.09
0.06 40
0.03 20
-0.00
RLbatchedexperimentsqaπ’s:nearone,batchsize=1000
1.03 100
1.00
80
0.96
60
0.93
0.90 40
0.86 20
0.83
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Time Runs Runs Runs
TRD MRD B-CL B-MCL
𝑝𝑖
Figure5:BatchedRLfiguresforlargebatchsizeandthetwootherenvironmentswhere𝑞 𝑎 ’sarenearzeroandnearone.
A.2 Populationexperiments
PopulationExperimentsqaπ’s:nearzero,size=1000
0.18
100
0.15
80
0.12
60
0.09
0.06 40
0.03 20
-0.00
PopulationExperimentsqaπ’s:nearone,size=1000
1.03 100
1.00
80
0.97
60
0.93
0.90 40
0.87 20
0.83
0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100 0 20 40 60 80 100
Time Runs Runs Runs
TRD MRD VR WVR
𝑝𝑖
Figure6:Populationexperimentsforlargepopulationsizeandthetwootherenvironmentswhere𝑞 𝑎 ’sarenearzeroandnear
one.
draweR
draweR
ytilauq/ffoyaP
ytilauq/ffoyaP
%Optimalaction
%Optimalaction
%Optimaltype/opinion
%Optimaltype/opinionA.3 Hyperparametrs
Inthissection,welistoutvarioushyperparametersusedbyRLandpopulationexperiments.
Hyperparameter value
Arms(𝑛) 10
Learningrate(𝛼) {0.001,0.1}
Epochs(𝐸) 100
Variance(𝜎2) 1
Runs(𝑅) {1000000,1000}
Weightfactor(𝛾) 0.01
Discretizingfactor(𝛿) 𝛼
finaltime(𝑡 𝑓) 𝛼×𝑅=100
Table1:HyperparametersusedforRLnon-batchedexperiments
Hyperparameter value
Arms(𝑛) 10
Epochs(𝐸) 100
Variance(𝜎2) 1
Runs(𝑅) 100
Batchsize(𝐵) {10,1000}
Discretizingfactor(𝛿) 1
finaltime(𝑡 ) 𝑅=100
𝑓
Table2:HyperparametersusedforRLbatchedexperiments
Hyperparameter value
Types/opinions(𝑛) 10
Epochs(𝐸) 100
Variance(𝜎2) 1
Runs(𝑅) 100
populationsize(𝐵) {10,1000}
Discretizingfactor(𝛿) 𝛼
finaltime(𝑡 ) 𝑅=100
𝑓
Table3:Hyperparametersusedforpopulationexperiments