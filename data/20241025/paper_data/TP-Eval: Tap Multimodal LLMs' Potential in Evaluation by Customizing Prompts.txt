TP-EVAL: TAP MULTIMODAL LLMS’ POTENTIAL
IN EVALUATION BY CUSTOMIZING PROMPTS
YuxuanXie1,2,∗,TianhuaLi1,2,∗,WenqiShao1,KaipengZhang1,†
1OpenGVLab,ShanghaiArtificialIntelligenceLaboratory
2SchoolofElectronicInformationandElectricalEngineering,ShanghaiJiaoTongUniversity
{xieyuxuan666,herobrine}@sjtu.edu.cn {shaowenqi,zhangkaipeng}@pjlab.org.cn
∗Equalcontribution †Correspondingauthor
ABSTRACT
Recently,multimodallargelanguagemodels(MLLMs)havereceivedmuchatten-
tionfortheirimpressivecapabilities. TheevaluationofMLLMsisbecomingcrit-
icaltoanalyzingattributesofMLLMsandprovidingvaluableinsights. However,
current benchmarks overlook the problem of prompt sensitivity - minor prompt
variations may lead to significant performance fluctuations. Thus, inappropri-
ate prompts may obscure the models’ capabilities, underestimating the models’
performance. Moreover,differentmodelshavedifferentpreferencesfordifferent
prompts, and thus, using the same prompt for all models will cause evaluation
bias. This paper analyzes this deficiency in existing benchmarks and further in-
troducesanewevaluationframeworknamedTP-Eval,whichintroducesaprompt
customizationmethodtoreduceevaluationbiasesandtapmodels’potential. TP-
Evalwillrewritetheoriginalpromptstodifferentcustomizedpromptsfordifferent
models. In particular, we propose some well-designed modules for prompt cus-
tomization tailored to the scenario of MLLM evaluation. Extensive experiments
demonstratetheeffectivenessofourapproachtouncoveringmodels’capabilities,
and TP-Eval should benefit the community in developing more comprehensive
andconvincingMLLMevaluationbenchmarks.
1 INTRODUCTION
Original prompt in MMT Are there any similarities …
Are there any similarities
between the two pictures? Focus on visual cues and MMT-Bench
Q: [prompt] [image]
LLaVA ：Yes Yes Yes … compare the two image Spot Similarity
carefully, determine whether A: [text]
they have similarities. A few
Compare the two image Test set examples
carefully, determine whether
they have similarities. Focus on visual cues, are there 3
any similarities between the evaluate 1
two pictures?
1
prompt
customizer
Task accuracy [customized
2
models prompt ]
0 0.233 0.318 0.456 0.5
Fucus on visual cues and compare…
(a)Exampleofpromptsensitivityinmulti-modalbenchmark. (b)FrameworkofTP-Eval.
Figure1: (a)showsunderestimationcausedbyunsuitablepromptsinMMT-Bench, (b)showsour
proposedevaluationframeworkresolvingthisbycustomizingprompts.
1
4202
tcO
32
]VC.sc[
1v17081.0142:viXraLargelanguagemodels(LLMs),suchasChatGPT,andClaude,arebecomingamilestoneinachiev-
ingartificialgeneralintelligence(AGI).Recently, beyondtextconversation, multimodallargelan-
guagemodels(MLLMs),likeGPT-4o(Achiametal.(2023)),Deepseek(Luetal.(2024)),InternVL
(Chen et al. (2024)) and LLaVA (Liu et al. (2024a)), have received much attention for their im-
pressivecapabilitiestounderstandmultimodalinputs(thispaperfocusesonimageandtext). Subse-
quently,researcherspresentvariousbenchmarkstoevaluatetheirperformanceindifferentscenarios.
Mostapplyprompt-basedbenchmarkingapproachestoaskmodelsmultimodalquestionsandassess
their responses. For instance, MMT-Bench by Ying et al. (2024) comprehensively evaluates per-
formance in 162 general tasks spanning 32 categories. Meanwhile, MMMU by Yue et al. (2024)
encompassessixcoredisciplinesdrawnfromuniversitycurriculaandassessesperformanceonmul-
tidisciplinary tasks requiring domain-specific knowledge and meticulous reasoning. Convincing
benchmarkingiscrucialtoanalyzetheattributesofmodels,providevaluableinsights,andguidethe
developmentofMLLMs.
Nevertheless,recentresearch(Zhanetal.(2022;2023;2024))foundthatLLMsandMLLMsexhibit
pronouncedsensitivitytopromptvariations. Thus,minormodificationstoquestionsinbenchmarks
mayleadtosignificantoutputdifferences. Thismakesprompt-basedbenchmarkingunreliablesince
models’lowaccuracymaybeowedtounsuitableprompts,nottheirinnercapability. Furthermore,
many MLLMs’ benchmarks use simple and uniform prompts for all samples in a specific task,
which aggravates the problem and causes general underestimation. Additionally, different models
show various sensitivity to the same prompt changes, and existing evaluation frameworks fail to
considersuchprompt-inducedbiasandmaynotbeabletoconductaconvincingcomparison.
Toaddresstheaforementioneddeficiencies,thispaperintroducesTP-Eval,anovelevaluationframe-
workforMLLMsthatcustomizesoptimalpromptsfordifferentmodelstofullytaptheirpotential
during evaluation while mitigating the effects leading to performance underestimation by prompt
sensitivity. Wepositthatthisframeworkenablesresearcherstoassessthestrengthsandweaknesses
of various models more accurately. To ensure fairness across models while also managing labor
costs, it is essential for the prompt customization process to be automated. A relevant technique
isautomaticpromptoptimization,asexemplifiedbyrecentmethodssuchasProTeGiPryzantetal.
(2023)andOPROYangetal.(2023),whichemployanoptimizer-scorerarchitecture.Thesemethods
generatemultiplecandidatepromptsandscorethemonatrainingsettoidentifythemosteffective
option.
Inspiredbythis,TP-Evalimplementspromptcustomizationthroughautomaticpromptoptimization
tailored to MLLMs’ evaluation. In particular, related prompt optimization methods consider text
only,whileourpromptcustomizationincorporatestextwithimages. Moreover,thedatascaleofthe
MLLM benchmark is usually limited (e.g., 20 validation samples per task in MMT-Bench) due to
thehighconstructioncost,whilerelatedpromptoptimizationmethodsdidnotconsiderthisfew-shot
scenarioandeasilycausedoverfitting. Thus,ourmethodintroducesanovelerrorintrospectionfrom
wrongresponsesandemployssomedesignstolimitthepromptsemanticchange.Theysignificantly
improvetheperformanceofourmethod.
We conduct extensive experiments to reveal the presence of prompt-induced underestimation and
biasinMLLMevaluationanddemonstratethattheTP-Evalframeworkeffectivelymitigatesthese
issues. Moreover, our experimental results demonstrate that TP-Eval also works well in zero-shot
settings. Theprimarycontributionsofthispapercanbeoutlinedasfollows:
• WeidentifyandanalyzepromptdesigndeficienciesinexistingMLLMs’benchmarksthat
leadtounderestimationandevaluationbiasduetopromptsensitivityinMLLMs.
• We propose TP-Eval, a novel evaluation framework for MLLMs that customizes optimal
promptsfordistinctmodelsandmakesitpracticalthroughautomaticpromptoptimization
tailoredtoMLLMs’benchmarks.
• We conducted extensive experiments on advanced MLLM benchmarks and various
MLLMstodemonstratetheeffectivenessofourmethodinalleviatingtheunderestimation
biasinevaluation.
2Prompt LLaVA DeepSeek
Isthepersoninthepicturewearingahelmet? 0.65 0.79
Evaluateiftheindividualinthepicturewearingadequateheadgear
thatprovidessafetyandvisibilitytominimizeinterpretationambiguity. 0.88 0.61
Istheindividualinthepicturewearinganadequateheadgear
thatprovidessafetyandisvisibletominimizeinterpretationambiguity? 0.69 0.83
Table1: Similarpromptchangeshavedifferenteffectsontwomodelsforhelmetanomalydetection
taskinMMT-Bench.
2 MULTIMODAL LARGE LANGUAGE MODEL EVALUATION
2.1 ANALYSISFOREXISTINGBENCHMARKS
In order to comprehensively evaluate the overall reasoning capabilities of MLLMs, many bench-
markshavebeenproposed,encompassingawiderangeoftasksthatassessvariousaspectsofmodel
performance. SomenotablebenchmarksareMMBenchbyLiuetal.(2024b),MMMUbyYueetal.
(2024),MM-VetbyYuetal.(2023),SEED-BenchbyLietal.(2023)andMMT-benchbyYingetal.
(2024). Unlikethepromptsusedintext-onlybenchmarksforLLMs,MLLMs’benchmarksprimar-
ilyconveythemajorityofthequestioninformationthroughimages. Additionally, consideringthe
substantial human effort required to design a specific textual prompt for each image, the prevail-
ing approach is to provide a simple prompt template or even an identical prompt for a given task,
like How many {<object>} are there in the image? for counting task and What
emotion is expressed in the artwork in the picture? for artwork emotion
recognitiontask.
However, extensive research demonstrates that LLMs are sensitive to minor modifications of tex-
tual prompts, so whether MLLMs are also sensitive to prompt design in existing benchmarks?
AsshowninFig. 1a,theoriginalpromptAre there any similarities between the
two pictures? of the spot similarity task in MMT-bench will lead to an anomalous response
fromthellava-1.5-7b,whoansweredYestoall180questions,resultinginanextremelylowaccu-
racyrate. However,byslightlyrephrasingthequestion,themodelachievesnearlydoubleaccuracy.
Thissuggeststhatthemodel’scapabilityisunderestimatedduetoinadequatepromptdesign. Fur-
ther investigation into the accuracy change brought from the phase Focus on visual cues
indicates that the model’s responsiveness to prompts is challenging to predict by humans, raising
questionsaboutwhetherseeminglyreasonablepromptsinexistingbenchmarkscantrulyandaccu-
ratelyassessthemodel’scapabilities.
Nevertheless,designingmoresuitablepromptsforallmodelsinbenchmarkswon’tsolvethisprob-
lemfundamentallysincedifferentMLLMs’modelarchitectureandtrainingdataaredifferent,lead-
ing to different behaviors, preferences, and sensitivity to prompts. Previous research on prompt
engineeringforLLMshasindicatedthatpromptdesignstrategieseffectiveforonemodelmayprove
ineffectiveforanother(Sclaretal.(2023)).SimilarphenomenahavealsobeenobservedinMLLMs.
An intuitive example can be found in Table 1 whereby customizing a more detailed prompt for
LLaVA will enhance the accuracy of the helmet anomaly detection task in MMT-Bench. How-
ever, thisspecificpromptdeclinedDeepSeek’saccuracysignificantly. Whenutilizingthisprompt,
LLaVA’sperformancewillsurpassthatofDeepSeek,andsubtleadjustmentsmayreversethisout-
come,whichimpliesthatcomparingtheoutputsoftwomodelsunderanidenticalpromptmaynot
necessarilyprovideavalidperformanceranking.
Theabovediscussionsregardingpromptsindicatethattheexistingbenchmarksandevaluationmeth-
odsmaynotaccuratelyassessthetruecapabilitiesofmodelsorfacilitateareliablecomparisonof
their performance, and simplistic prompt templates in MLLM benchmarks exacerbate this issue.
Actionshouldbetakentomitigatetheinfluenceofpromptsonmodelevaluations.
32.2 IDEALEVALUATION
Theidealevaluationshouldbeabletoevaluatethetruecapabilitiesofthemodel.However,duetothe
significantperformanceinfluencecausedbypromptdesign,howdowedefinethetruecapabilities
duringevaluation? Wearguethatmodels’truecapabilitiesareperformanceunderoptimalprompts,
consideringthatuserswillalsorefinethepromptstogetdesirableresponseswhenusingMLLMs.
The optimal prompts should be derived from slight modifications from the benchmarks’ original
prompts while maintaining the semantic integrity of the task instructions. The optimal prompts
for different models may be identical or different. Therefore, we propose TP-Eval, an evaluation
framework that customizes the best prompts for each model in each task, thereby tapping their
potentialanduncoveringtheirtruecapabilities.
Manual exploration of optimal prompts during evaluation is time-consuming and impractical. In-
spired by existing works on automatic prompt optimization for LLMs, we propose to use an au-
tomated prompt customizer to leverage original prompts from benchmarks and a few examples to
customizespecificpromptsforeachMLLMunderevaluation,therebytappingtheirpotential.
However,existingtext-onlypromptoptimizationmethodsarenotapplicable. Ontheonehand,the
datascaleformulti-modaltasksisrelativelysmall,especiallyforevaluationdata,whichnecessitates
thatthepromptcustomizerpossessesastrongfew-shotcapability,whichisoverlookedbyexisting
methods.Ontheotherhand,thedesirablepromptcustomizationrequiresanewframeworktoutilize
visual information beyond text, and the cost associated with calling MLLM APIs is prohibitively
high,makingextensivecallsimpractical. Therefore,anovelpromptcustomizationmethodtailored
specificallyformulti-modalbenchmarksisneeded.
3 RELATED WORKS
3.1 RESEARCHONPROMPTSENSITIVITY
Some studies have revealed that even minor prompt modifications, which have negligible impact
onhumansemanticunderstanding,canleadtosignificantshiftsintheoutputofLLMs(Zhanetal.
(2022; 2023)). This property has been widely exploited in the creation of adversarial examples,
where small perturbations to the embeddings or input text can induce the model to generate in-
correct or misleading answers (Zhan et al. (2024)). This sensitivity allows minor adjustments to
questionsinLLMbenchmarkstosignificantlyimpactthefinalevaluationperformance. Recentre-
search has begun exploring variations in prompt formatting to achieve better results (Sclar et al.
(2023)). SimilarphenomenaalsooccurforMLLM.However,addressingthisdeficiencyinMLLM
benchmarkdesignremainsrelativelyunderexplored. Inthiswork,weprovideadetailedanalysisof
promptdesignissuesandintroduceaneffectiveevaluationframeworkwithpromptcustomizationto
avoidtheaboveproblemsandbiasfromprompts.
3.2 PROMPTENGINEERING&OPTIMIZATION
PromptengineeringseekstoidentifyeffectivepromptsforLLMstooptimizetheirtaskperformance.
Tominimizemanualeffort,researchershaveexploredautomaticpromptoptimization,broadlycate-
gorizedintocontinuousanddiscretemethods. Discretemethodsdirectlyoptimizenaturallanguage
prompts using techniques such as reinforcement learning (Zhang et al. (2022)) or prompt editing
(Prasadetal.(2022)). Incontrast,continuousmethods(Lesteretal.(2021);Li&Liang(2021))per-
formoptimizationwithintheLLMs’embeddingspace,enablinggradient-basedapproaches. Given
theunprecedentedcapabilitiesofLLMs,recentresearchhasstartedleveragingthemaspromptop-
timizers. Forexample,Yang&Li(2023)integratesLLMswithevolutionaryalgorithmstoenhance
prompt optimization, while Yang et al. (2023); Pryzant et al. (2023) focuses on adapting concepts
and techniques from gradient-based model optimizers, including gradient descent (Pryzant et al.
(2023))andmomentummethods(Yangetal.(2023)),forLLM-basedpromptoptimization.
Our work follows discrete methods and employs MLLM as prompt optimizers. In particular, we
combine error introspection, semantic change, and accuracy as “pseudo-gradients” proposed by
Tang et al. (2024) to guide the MLLM optimizer in the multimodal scenario. We also introduce a
finalre-rankingschemeforbetterperformance.
44 METHOD
4.1 OVERVIEW
MLLM-based Your task is to generate the instruction <PROMPT>.
Optimizer Prompt Optimizer Meta-Prompt Below are some previous instruction with their
𝓜𝑶 scores and introspections.
New Prompts
<PROMPT> <SCORE> <INTROSPECTION>
select …
MLLM to be evaluated
𝓜𝑻 Iteration Here are some examples of the task:
Prompt Pool <QUESTION> <ANSWER> <IMAGE>
Prompt 0;Score 0; Introspection 0 …
New Prompts Prompt 1; Score 1; Introspection 1
Scorer AR nsa ww er Scores Prompt 2; Score … 2; Introspection 2 G the en ie nr sa tt re u ca tn io i nn s <tr Pu Rct Oio Mn Pth Ta >t i as b d oi vf efe ar nen dt h f aro s m all
& Prompt 𝑛; Score 𝑛; Introspection 𝑛 higher score with respect to introspection.
MLLM-based Introspection You can only edit at most {counter} words.
Answer Analyzer
𝓜𝑨 re-ranking Answer Analyzer
The current instruction is <PROMPT>.
But it gets the following examples wrong:
Q: Original Prompt 𝒑𝟎 [image]
Optimal Prompt 𝒑∗
< <Q RAU WES ATI NO SN W>
E
< RIM
>
A <TG RE U>
E ANSWER> …
A: [text] Give a brief reason why it got them wrong.
Figure2: Theoverviewofourautomaticpromptcustomizationstructure.
Fig. 1billustratestheoverallpipelineofTP-Eval,giventheinitialtextpromptswithafewexamples
D from the evaluation dataset for a task, and a MLLM M to be evaluated. We introduce a
few T
promptcustomizationmethodtoobtaintheoptimalpromptp∗ forM ,thendoanidealevaluation
T
tomaximizeitspotentialontheoriginaltestsetD .
test
WeshowtheoverallframeworkofourcustomizationmethodinFig. 2. Startingfromtheinitialtext
promptp forataskfromthemultimodalevaluationdataset,weutilizeGPT-4ominiasanoptimizer
0
M and a few examples D (questions and answers) from the evaluation dataset to obtain an
O few
optimal prompt p∗ for the MLLM M . Specifically, we first feed p to a scorer, which consists
T 0
of the M and an answer analyzer M (GPT-4o mini), to output the scores and introspection.
T A
Thenweusetheseresultstoconstructawell-designedmeta-promptfortheoptimizerM toobtain
O
optimized prompts P = {p ,p ,··· ,p }. We feed them to the scorer and iteratively run this
1 1 2 n
frameworktocollectN setsofoptimizedprompts{P ,P ,··· ,P }withtheirscores. Finally,we
1 2 N
selecttheoptimalpromptp∗accordingtothescores.Pleasenotethatwewillfeedthecorresponding
imagestoM andcorrespondingimagesandanswerstoM . Wewillintroducethedetailsofthe
T A
promptcustomizationmethodinthefollowing.
4.2 SCORER
Inthei-thiteration,wefeedthepromptsetP (usingp inthefirstiteration)tothescorertoobtain
i 0
thecorrespondingscoresandintrospection(i.e.,pseudogradient)ofeachprompt.
4.2.1 SCOREGENERATION
WefirstfeedthesepromptswithcorrespondingimagestoM toobtainmodels’responses. Then
T
considering the variations of answers and most benchmarks apply choice questions, we use M
A
(GPT4o-mini)extractchoicesandthencomputetheaccuracya onD forp .
pi few i
Using accuracy as a reward only may lead to drastic changes in the new prompt and destroy the
optimization. Thus we utilize a semantic similarity metric as proposed by Tang et al. (2024) to
limit the changes in each iteration. Specifically, we use BERT by Kenton & Toutanova (2019) to
extracttheembeddingofthecurrentpromptp andtheoriginalpromptp ,thencalculatetheircosine
i 0
similarityass .
pi
5Wecombinea ands asthefinalscorec =αa +(1−α)s ,whereαisaweightingcoefficient
pi pi pi pi pi
tomakeatrade-offbetweenoptimizationandoriginalsemanticmeaningmaintain.
4.2.2 INTROSPECTIONGENERATION
Wearguethatscoresarequantitativeandnotinformativeenough,especiallyinthefew-shotexam-
ples,andthus,weintroducetoemployadditionalintrospectionduringoptimization.Specifically,we
aimtohelptheoptimizerbetterunderstandthedeficienciesinthecurrentprompt. Toachievethis,
werepresentintrospectionI ontheincorrectresponsesinD ofM underp ,allowingM to
i few T i O
explicitlyreferencethereasonsfortheseerrorswhengeneratingnewprompts. Weshowtheprompt
structuretogenerateintrospectioninFig. 2andthefullpromptinthesupplementarymaterials.
4.3 OPTIMIZER
WeusetheoptimizerM (GPT4o-mini)togenerateanewpromptsetP fromallhistoryprompts
O i+1
{P ,··· ,P }. Specifically, wedesignameta-promptasshowninFig. 2andcompleteitusingK
0 i
promptswithTop-K scoresfrom{P ,··· ,P }. Wealsofeedtheirscoresandintrospectiontothe
0 i
optimizer. Themetapromptiscomposedoffourparts: description,pseudogradient(i.e.,prompts
with their scores and introspection), examples (questions with ground-truth answers from D ),
few
and instruction. The description is used to describe the prompt optimization tasks. The pseudo
gradient and examples are used to provide information for the optimization. The instruction is
used to generate new prompts. In particular, to ensure smooth optimization and not overlook op-
timalprompts,weuseadecayingeditdistanceYou can only edit at most {counter}
words to limit the changes. Please note that for identical question benchmarks (e.g., MMMU),
wewilladdaninitializedmeaninglessphraseandoptimizeitratherthanthewholeprompt,seethe
experimentssectionformoredetails.
4.4 ITERATION
We use the above scorer-optimizer framework iterative to obtain N prompt set {P ,P ,··· ,P }
1 2 N
withscoresforeachprompt. Thenweselecttheoptimalpromptfromallhistoryprompts.
Incontrasttorelatedpromptoptimizationmethodsusinglarge-scaletrainingdatatoobtaincandidate
promptsandselectingtheoptimalpromptwiththehighestaccuracy,MLLMevaluationcanprovide
only limited examples in the optimization. Thus, we have to consider the problem of overfitting
andbiasinthefewexamples. Theintroducedsemanticcosinesimilarityanddecayingeditdistance
canalleviatethisproblem. Moreover,intheselectionoftheoptimalprompts,weemployahigher
weighting coefficient α∗ > α to re-compute each prompt’s score and select the prompt with the
highestscore.
5 EXPERIMENT
5.1 EXPERIMENTALSETUP
Models. The MLLMs to be evaluated (i.e., M ) are LLaVA-1.5-7B, DeepSeek-VL-7B,
T
Mini-InternVL-Chat-4B-V1-5. We use GPT-4o-mini for optimizer (M ) and answer
O
analyzer(M ).
A
Benchmarks. We use MMT-Bench and MMMU as the evaluation benchmarks. MMT-Bench is
designed for the evaluation of general capabilities, while MMMU is designed for multi-discipline
evaluation. Consideringourlimitedresources,weselectasubsetofMMT-BenchasMMT-S,which
contains83tasks(19categories). WeusethedevelopmentsetandvalidationsetofMMMU.
Settings of prompt optimization We evaluate our method in two settings: optimizing the
whole prompt or optimizing the newly added phrase. MMT-Bench follows the most prevalent
MLLM benchmark format, which uses the same prompt template within a task (e.g., How many
<object> are there in the image? for the task of object counting). Thus, we opti-
mizethewholepromptforeachtaskinMMT-S.InMMMU,eachquestionisidentical,andthuswe
addaninitializedmeaninglessphraseAnswer the questions about {task name}asthe
prompttobeoptimizedandmovetheoriginalpromptto<QUESTION>inthemetaprompt.
6Implementation details. For MMT-S, we utilize the officially designated validation set as D ,
few
which comprises approximately 10% of the total data, with roughly 20 samples per task. For
MMMU, we combine the development and validation sets and allocate half of the data as D .
few
WefollowVLMEvalKitbyDuanetal.(2024)toimplementtheanswerextractionmoduleinM .
A
The total optimization iteration N = 16, with each round generating three new prompts. In each
iteration,weselectthetopeight(i.e.,K =8)promptsforthemetaprompt. Wesetthetemperature
to1.0whengeneratingnewprompts. Duringtheoptimizationphase,wesetαto0.8toencourage
theexplorationofpromptsthatyieldhigheraccuracy. Inthefinalstep,wesetα∗to0.6toselectthe
optimalprompt.
5.2 MAINRESULTS
5.2.1 PERFORMANCEANALYSIS
Model OriginalScore TP-EvalScore #ImprovedTask Ratio
LLaVA-1.5-7B 50.4 54.4 32 25.1%
DeepSeek-VL-7B 55.2 57.3 21 23.3%
Mini-InternVL-Chat-4B-V1-5 54.6 56.9 16 40.4%
Table 2: Overall result for MMT-S. All three models exhibited significant performance improve-
mentsacrossasubstantialnumberoftasksfollowingpromptcustomization.
   
 / / D 9 $
 ' H H S V H H N
     , Q W H U Q 9 /
   
   
  
 
 P
 X O W L S O H B L P D J  L H  P B  D D  J Q  D O  H D  Q O  R  B R \  F  H V  P D  Y L  O V  D L  D  O O ]  X \ D W  D B L  W L G R  H  R Q  L W  Q
 G
 Q H  W B  R F  M  H W
 F
 O L  X  O B R  L G  X Q  J J  Q  H H  G  Q P  H  F U H  H V Q  W  B W  D  T  D Q  X  F W G  R  L L  W L  R Q  H  Q J  Q  B W  U B  H W  F  L H  R V  P W  J  K  D Q  D L  J O W  O  H L   X R  W F Q  L  R  Q  L Y D  L  P W  V L  D X R  D  J Q  O  H B  B F  W U R  D G  Q H  V O D W L R Q  Y L V R  X F  D U  D  X W  O  W K  B  R U  F  Q H  D  R H  S W G  P L R  R Q  X L  V Q  B J  G U  Y L  L Y  V L  X Q  H  D O J  P  B U R W  H L  L F R  P P R Q  H J  D  W Q  P J L  H H W  H L  P B  B R  U  S X H Q  W  R Q U  U L  G  D H  H  O U Y  B V D  W  X O  D  Q Q  G G  H L  U Q  V W J  D Q G L Q J
Figure 3: Results of different models on MMT-S (L2-category). Accuracy improvement is calcu-
latedbyaccuracyusingtheoptimizedpromptdividedbyaccuracyusingtheoriginalprompt. Three
models showed varying improvement across different task types, while performance gains differ
betweenmodels,highlightingtheunderestimationandbiasintroducedbyoriginalpromptsandthe
effectivenessofourmethod.
MMT-Bench Table 2 shows the overall results of three open-source models’ re-evaluated perfor-
manceafterpromptcustomizationonMMT-S,wheretheyexhibitvaryingdegreesofimprovement
andshowhiddenpotentialacrossdifferenttasks. Itwasobservedthat32taskscouldyieldaperfor-
mance enhancement of 25.1% through prompt customization on LLaVA, ultimately leading to an
7
     W Q H P H Y R U S P ,  \ F D U X F F $overallscoreimprovementof4%.WithrespecttoDeepSeekandInternVL,theformerdemonstrated
a pronounced instruction-following capability during the experiments, while the latter exhibited a
tendencytowardsdetailedproblemanalysis. Thesecharacteristicsrenderbothmodelsmorerobust
to prompt changes, resulting in less accuracy improvement. The varying improvements suggest
thatmodelshavingsimilarscoresmayexperienceshiftsintheirrankingswhenthepromptchanges.
It also proves that prompt design flaws generally exist in MMT-Bench, resulting in a substantial
underestimationofmodels’performance,whileourevaluationmethodcantaptheirpotential.
Fig. 3shows more detailed L2-category levelresults ofMMT-S. Accuracy improvementis calcu-
lated by accuracy using the optimized prompt divided by accuracy using the original prompt. All
three models did not demonstrate significant improvements in relatively simple and easily com-
prehensible tasks like visual recognition, as well as in more challenging and complex tasks such
as image retrieval. This outcome is comprehensive since, for the former, the model completely
understands what it should do, and designing a more detailed prompt doesn’t help; for the latter,
model performance is mainly constrained by its inner true ability rather than inadequate prompts.
Furthermore, certain tasks, such as anomaly detection, have been proven improvements across all
threemodels,suggestingitsgeneralpromptdesignflaws. Inothertaskslikemultipleimageanalysis
andlocalizationtasks,modelsshowobviousotherness,whereLLaVAandInternVL’sperformances
demonstratesignificantenhancements,butDeepSeek’sbarelymaintains. Thisalsoemphasizesthe
validity of our proposed TP-Eval framework in mitigating bias and comparing model capabilities
whileensuringfairness.
MMMUWeconductedacomprehensivecomparisonoftheresultsforall30tasksinMMMU.We
evaluatedtheperformancewiththeoriginalquestions,withtheadditionoftheinitialprefixprompt,
andwiththeoptimizedprefixpromptonLLaVA.Theresultsandimprovementsaresummarizedin
Fig. 4.
It is evident that even adding the domain-specific initial prefix prompt (i.e., task name) can ef-
fectively guide the model to focus on and respond within that specific domain, thereby mitigat-
ing underestimation, but they are still too simple and not optimal. Compared to the initial prefix
prompt, our optimized prefix prompts showed general performance improvements. Additionally,
due to the semantic similarity metric in scores and extensive question information incorporated in
themeta-prompt,theoptimizersuccessfullygeneratesprefixpromptswithhigherhumanreadability
andstronggeneralizationcapabilitieswithinthedomain.
 
   
     2  , Q U  L L  W J  L D L Q  O    3 4  U H X  I H  L V  [ W   L  3 R  U Q  R P S W                     
 2 S W L P D O  3 U H I L [  3 U R P S W
     
   
                    
   
     
   
                   
   
   $   U   W  	  ' H V L J Q  % X V L Q H V V  6 F L H  + Q  H F  D H  O W  + K  X  	  P   D 0  Q H  L W G  L H L F  V L   Q  	 H   6 R F L D O  6 F  7 L H  H Q  F F  K H   	  ( Q J L Q H H U L Q J  $ / /  / / D 9 $  ' H H S V H H N  , Q W H U Q 9 /  
Figure 4: Overall performance with different Figure 5: Result of applying optimized
promptmethodsonMMMUwithLLaVA.Inmost prompts to other models. Applying cus-
cases,theresultsafteroptimizationsurpassthose tomizedpromptsfromonemodeltoanother
achievedwiththeinitialprompts,andtheygener- yields performance changes that differ from
allyoutperformtheoriginalquestionsaswell. eachmodel’sinherentcharacteristics.
5.2.2 OPTIMALITYANALYSIS
Fig. 5presentstheoverallresultsobtainedfromahybridizationofcustomizationoutcomesacross
differentmodelswithinMMT-S.Itisevidentthatpromptsoptimizedusingamodelitselfasascorer
8
 \ F D U X F F $
 $ 9 D / /
 N H H V S H H '
 / 9 Q U H W Q ,yieldsuperiorperformance. Notably, whenpromptsoptimizedonInternVLareappliedtoLLaVA
andDeepSeek-VL,theirperformancewilldecline. Thisoutcomenotonlysupportsthattheoptimal
prompts proposed for one specific model may not be universally applicable, thereby underscoring
the necessity of customizing prompts to tap the models’ full potential but also indicates that our
methodhasindeedapproachedtheobjectiveofcustomizationandcaneffectivelysupportTP-Eval
framework.
5.2.3 ERRORANALYSIS
Similar to many text-only prompt optimizations, our method, while ensuring an overall enhance-
mentinperformanceandacloseralignmentwiththetruecapabilityforevaluatedmodels,maystill
encounter optimization failures for a limited number of tasks. This can, in turn, result in a slight
performance deterioration when using optimized prompts. For instance, although LLaVA has an
overall improvement of 25% across 32 tasks, it also experiences an approximate 6% performance
declineon6tasks. Wearguethatacriticalfactorcontributingtothisistherelativelysmallsizeof
the validation set currently designated by the official multi-modal benchmarks, which may cause
overfittingonthetrainingset. Despiteoureffortstoincorporateintrospectionmechanismsformore
effectiveutilizationoffew-shotdata,andtheimplementationofre-rankingandmeta-promptdesign
strategiestomitigateoverfitting,thischallengepersists,butitsimpactremainsrelativelyminor.
5.3 ABLATIONSTUDY
    
 2 U L J L Q
     1 R  , Q W U R V S H F W L R Q     
 2 X U V
 / / D 9 $
      ' H H S V H H N
     , Q W H U Q 9 /
    
   
    
   
                                            
 5 H  5 D Q N L Q J  3 D U D P H W H U  
    Figure 7: Influence of re-ranking. Both exces-
 D U W Z R U N B H P R W L R Q B U H F R J Q L W L R Q  K H O P H W B D Q D P D O \ B G H W H F W L R Q  E H K D Y L R U B D Q D P D O \ B G H W H F W L R Q
sivelyhighandlowαcanleadtoareductionin
Figure 6: Performance on whether to use intro- performance, and each model achieves optimal
spectionornot. performancewithα∈[0.5,0.6].
IntrospectionFig. 6illustratestheresultsofLLaVAinthreetasksofMMT-Swhenintrospection
isnotincorporated. Itisevidentthattheoptimizationresultsonbothartworkemotionrecognition
and helmet anomaly detection tasks are significantly inferior to those achieved with our method.
Notably, the latter even experiences a failure in optimization, resulting in a performance decline.
Thisunderscorestheeffectivenessofintegratingintrospectiontoenhancethefew-shotoptimization
capability on multi-modal benchmarks. Furthermore, the figure indicates that the accuracy of be-
havioranomalydetectionisbetterwithoutintrospection. Thisphenomenonarisesfromtheprompt
explicitlydesignatingchoiceAasnormalandchoiceBasabnormal, disregardingtherandomized
initial order of the choices presented in this task. This is an instance of semantic overfitting that
leads to misleadingly high performance. Thus, the introduction of introspection can also enhance
resultinterpretability.
Re-ranking parameter. Fig. 7 illustrates the impact of varying the proportion of accuracy dur-
ing the re-ranking phase on optimization results. As depicted, when setting the parameter to 0.8,
whichinfactomitsthere-rankingstage,leadstosignificantoverfittingandultimatelydegradesthe
optimizationoutcomes. Conversely,adisproportionatelylowcorrectnessratiomayresultintheex-
clusion of potentially optimal prompts, thereby underfitting and hindering the optimized prompts
fromfullyleveragingthemodel’scapabilities. Basedonourexperiments,weconcludethatavalue
between0.5and0.6isappropriatetoensurebotheffectivenessandcoherenceacrossthemodels.
9
 \ F D U X F F $
 \ F D U X F F $5.4 ZERO-SHOTEXPLORATION
Consideringthatthetaskmaysufferfromextremelylimiteddataavailabilityorinvolveprivacycon-
cernsthatpreventthedisclosureofanswers, itbecomesimpracticaltoconstructevenonetraining
sample.Inresponse,weproposeanapproachthatleveragestherobustIn-ContextLearning(ICL)ca-
pabilitiesofLLMstoextendourmethodtothezero-shotscenario. Specifically,weaimtooptimize
promptsforanewlyintroducedtaskthroughtheuseofaselectionofpreviouslysuccessfullyopti-
mized examples, thereby facilitating zero-shot customizing. We anticipate that LLMs can discern
certain vocabulary, phrases, and reasoning patterns that the model under examination may prefer
from these ICL examples. A straightforward experiment result illustrating this observation can be
foundinTable3, whereweselect3tasksfromall32MMT-SunderestimatedtasksforLLaVAas
targetsandusetherestasICLexamples. Weusethiszero-shotICL-basedoptimizationfashionto
refinetheoriginalprompts,whichalsoenhancestheoriginalaccuracyandisclosetothatofoptimal
promptslearnedby20examples.
Taskname Originalprompt Zero-shot Few-shot
helmetanomalydetection 0.65 0.86 0.92
artworkemotionrecognition 0.3 0.33 0.41
spotsimilarity 0.23 0.42 0.52
Table3: Zero-shotpromptoptimizationutilizingIn-contextLearning.
6 CONCLUSION
WeinvestigatedMLLMbenchmarksandfoundthatoverlysimplisticorunsuitabletextualprompts
mayleadtoanunderestimationofmodels’capabilities. Toaddressthisissue,weproposeanideal
evaluationframework,TP-Eval,whichcustomizesthemostsuitabletaskpromptforeachmodelto
mitigateprompt-inducedbiasesandtapthemodels’potential. Toachievethisgoal,wedrewonthe
successfulexperiencesofautomaticpromptoptimizationontext-onlyLLMsanddesignedaprompt
optimizationmethodtailoredtothefew-shotscenarioofMLLMbenchmarks. Ourexperimentre-
sultsforthreemodelsontheMMTandMMMUindicatetheeffectivenessofourmethod.
REFERENCES
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technical
report. arXivpreprintarXiv:2303.08774,2023.
ZheChen,WeiyunWang,HaoTian,ShenglongYe,ZhangweiGao,ErfeiCui,WenwenTong,and
KongzhiHu. Howfararewetogpt-4v? closingthegaptocommercialmultimodalmodelswith
open-sourcesuites,2024.
Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Xiaoyi Dong,
Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source
toolkitforevaluatinglargemulti-modalitymodels,2024.
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep
bidirectionaltransformersforlanguageunderstanding. InProceedingsofnaacL-HLT,volume1,
pp. 2,2019.
BrianLester,RamiAl-Rfou,andNoahConstant. Thepowerofscaleforparameter-efficientprompt
tuning. arXivpreprintarXiv:2104.08691,2021.
BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan. Seed-bench: Bench-
markingmultimodalllmswithgenerativecomprehension,2023.
XiangLisaLiandPercyLiang.Prefix-tuning:Optimizingcontinuouspromptsforgeneration.arXiv
preprintarXiv:2101.00190,2021.
10HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. Advances
inneuralinformationprocessingsystems,36,2024a.
Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan,
JiaqiWang,ConghuiHe,ZiweiLiu,KaiChen,andDahuaLin. Mmbench: Isyourmulti-modal
modelanall-aroundplayer?,2024b.
Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng
Ren,ZhuoshuLi,HaoYang,YaofengSun,ChengqiDeng,HanweiXu,ZhendaXie,andChong
Ruan. Deepseek-vl: Towardsreal-worldvision-languageunderstanding,2024.
Archiki Prasad, Peter Hase, Xiang Zhou, and Mohit Bansal. Grips: Gradient-free, edit-based in-
structionsearchforpromptinglargelanguagemodels. arXivpreprintarXiv:2203.07281,2022.
ReidPryzant,DanIter,JerryLi,YinTatLee,ChenguangZhu,andMichaelZeng.Automaticprompt
optimizationwith”gradientdescent”andbeamsearch. arXivpreprintarXiv:2305.03495,2023.
Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models’ sen-
sitivity to spurious features in prompt design or: How i learned to start worrying about prompt
formatting. arXivpreprintarXiv:2310.11324,2023.
XinyuTang, XiaoleiWang, WayneXinZhao, SiyuanLu, YaliangLi, andJi-RongWen. Unleash-
ing the potential of large language models as prompt optimizers: An analogical analysis with
gradient-basedmodeloptimizers. arXivpreprintarXiv:2402.17564,2024.
Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V. Le, Denny Zhou, and Xinyun
Chen. LargeLanguageModelsasOptimizers. arXive-prints,art.arXiv:2309.03409,September
2023. doi: 10.48550/arXiv.2309.03409.
HengYangandKeLi. Instoptima: Evolutionarymulti-objectiveinstructionoptimizationvialarge
languagemodel-basedinstructionoperators. arXivpreprintarXiv:2310.17630,2023.
KainingYing,FanqingMeng,JinWang,ZhiqianLi,HanLin,YueYang,HaoZhang,WenboZhang,
YuqiLin,ShuoLiu, etal. Mmt-bench: Acomprehensivemultimodalbenchmarkforevaluating
largevision-languagemodelstowardsmultitaskagi. arXivpreprintarXiv:2404.16006,2024.
Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang,
andLijuanWang. Mm-vet:Evaluatinglargemultimodalmodelsforintegratedcapabilities,2023.
Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,
Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: A massive multi-discipline multi-
modalunderstandingandreasoningbenchmarkforexpertagi. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.9556–9567,2024.
PengweiZhan, YangWu, ShaoleiZhou, YunjianZhang, andLimingWang. Mitigatingtheincon-
sistencybetweenwordsaliencyandmodelconfidencewithpathologicalcontrastivetraining. In
FindingsoftheAssociationforComputationalLinguistics: ACL2022,pp.2226–2244,2022.
PengweiZhan,JingYang,XiaoHuang,ChunleiJing,JingyingLi,andLimingWang. Contrastive
learningwithadversarialexamplesforalleviatingpathologyoflanguagemodel. InProceedings
of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long
Papers),pp.6493–6508,2023.
PengweiZhan,JingYang,HeWang,ChaoZheng,andLimingWang. Rethinkingword-leveladver-
sarialattack:Thetrade-offbetweenefficiency,effectiveness,andimperceptibility.InProceedings
of the 2024 Joint International Conference on Computational Linguistics, Language Resources
andEvaluation(LREC-COLING2024),pp.14037–14052,2024.
TianjunZhang, XuezhiWang, DennyZhou, DaleSchuurmans, andJosephEGonzalez. Tempera:
Test-timepromptingviareinforcementlearning. arXivpreprintarXiv:2211.11890,2022.
11