Preprint
TRAINING FREE GUIDED FLOW MATCHING WITH OP-
TIMAL CONTROL
LuranWang1, ChaoranCheng2, YizhenLiao3, YanruQu2, Geliu2
1UniversityofCambridge
2UniversityofIllinoisUrbana-Champaign
3TsinghuaUniversity
lw703@cam.ac.uk, chaoran7@illinois.edu
liaoyz0711@gmail.com, yanruqu2@illinois.edu
geliu@illinois.edu
ABSTRACT
Controlledgenerationwithpre-trainedDiffusionandFlowMatchingmodelshas
vastapplications.OnestrategyforguidingODE-basedgenerativemodelsisthrough
optimizingatargetlossR(x )whilestayingclosetothepriordistribution. Along
1
this line, some recent work showed the effectiveness of guiding flow model by
differentiating through its ODE sampling process. Despite the superior perfor-
mance,thetheoreticalunderstandingofthislineofmethodsisstillpreliminary,
leavingspaceforalgorithmimprovement. Moreover,existingmethodspredomi-
natelyfocusonEuclideandatamanifold,andthereisacompellingneedforguided
flowmethodsoncomplexgeometriessuchasSO(3),whichprevailsinhigh-stake
scientific applications like protein design. We present OC-Flow, a general and
theoreticallygroundedtraining-freeframeworkforguidedflowmatchingusing
optimalcontrol. Buildinguponadvancesinoptimalcontroltheory,wedevelop
effectiveandpracticalalgorithmsforsolvingoptimalcontrolinguidedODE-based
generationandprovideasystematictheoreticalanalysisoftheconvergenceguaran-
teeinbothEuclideanandSO(3). Weshowthatexistingbackprop-through-ODE
methods can be interpreted as special cases of Euclidean OC-Flow. OC-Flow
achieved superior performance in extensive experiments on text-guided image
manipulation,conditionalmoleculegeneration,andall-atompeptidedesign.
1 INTRODUCTION AND RELATED WORK
SDEandODE-basedgenerativemodelssuchasdiffusionandcontinuousnormalizingflow(CNF)
haveexhibitedexcellentperformanceonvariousdomainssuchasimages(Hoetal.,2020;Esser
etal.,2024),audio(Zhangetal.,2023;Défossezetal.,2022),anddiscretedata(Louetal.,2023;
Chengetal.,2024). Particularly,thesimplicityofRiemannianFlowMatchingonSO(3)manifold
(Chen&Lipman,2023)empowersdenovogenerationofsmallmolecules(Songetal.,2024;Xu
etal.,2023)andproteins(Yimetal.,2023;Boseetal.,2023;Lietal.,2024),leadingtoenormous
advancementinbiomedicine. Controlledgenerationfrompre-traineddiffusionandflowmatching
priorshasgainedgrowinginterestinnumerousfields,asitencompassesawiderangeofpractical
tasksincludingconstrainedgeneration(Giannoneetal.,2023),solvinginverseproblems(Liuetal.,
2023;Ben-Hamuetal.,2024),andinstructionalignment(Blacketal.,2023;Esseretal.,2024).
Thereareseverallinesofworkforguidingdiffusionandflowmodels. Classifier-freeguidance(CFG)
(Ho&Salimans,2022)trainsconditionalgenerativemodelsthattakeconditionsasinput. Reward
fine-tuningapproachesupdatethegenerativemodelparameterstoalignwithcertaintargetobjective
functions(Blacketal.,2023). Bothmethodsrequirespecializedtrainingroutineswhicharecostly
andnotextendabletonewtasks. Training-freeguidanceondiffusion(Kawaretal.,2022;Chung
etal.,2024;Songetal.,2023)altersthescoresintheSDEgenerationprocesswiththegradientsfrom
thetargetfunctiontoachieveposteriorsampling. Theseguidancesoftenrelyonstrongassumptions
ofthedenoisingprocessandrequireestimatingtargetfunctiongradientsw.r.tnoisedsampleswhich
areoftenintractable. Accurateposteriorsamplingisonlyguaranteedforalimitedfamilyofobjective
1
4202
tcO
32
]GL.sc[
1v07081.0142:viXraPreprint
functionssuchaslinear. Therefore,effortsthatdeploysuchguidancetoflowmodelsbybridgingthe
ODEpathandSDEpathsharesimilarconstraints(Pokleetal.,2023;Yimetal.,2024).
Notably,tworecentworksshowedtheeffectivenessofguidingpre-trainedflowmodelsbydiffer-
entiatingthroughtheODEsamplingprocess,outperformingpopularguidance-basedapproaches.
Particularly,Ben-Hamuetal.(2024)differentiatesalossR(x)throughtheforward-ODEw.r.ttheini-
tialnoisex ,whichinducesimplicitregularizationbyprojectingthegradientontothe“datamanifold”
0
underGaussianpathassumption. Thisstrongconfinementtothepriormighthinderoptimization
whenthetargetrewardfunctiondivergesfromthepriordistribution. Liuetal.(2023)formulates
anoptimalcontrolproblemwhereacontroltermu ateachtimestepissolvedtoguidetheODE
t
trajectory. However,thegradientdecompositiontrickusedinLiuetal.(2023)ignorestherunning
costofcontroltermsandthuscouldleadtosuboptimalbehavior. Despitethegoodperformance,there
isalackofsystematictheoreticalanalysisontheconvergencebehaviorandexplicitregularizationof
thedifferentiate-through-ODEapproachestobetterguidealgorithmdesigninthisspace.Furthermore,
existingworkspredominantlyfocusontheEuclideanmanifoldduetoitssimplicity,andthereisa
compellingneedforatheoreticallygroundedguidedflowmatchingframeworkonmorecomplex
geometriessuchasSO(3)whichisheavilyusedinscientificapplications.
Tofillthegapbetweenthepracticalapplicationsofguidedgenerationandtheoreticalgrounds,we
proposeOC-Flow,ageneral,practical,andtheoreticallygroundedframeworkfortraining-freeguided
flowmatchingunderoptimalcontrolformulation. Ourkeycontributionsareasfollows:
1. Weformulate“controlledgenerationwithpre-trainedODE-basedpriors”asanoptimalcontrol
problemwithacontroltermu andarunningcostthatregulatesthetrajectoryproximitytotheprior
t
modelwhileoptimizingfortargetloss. Buildinguponadvancesinoptimalcontroltheory,wedevelop
effectiveoptimizationalgorithmsforbothEuclideanandSO(3)spacethroughiterativeupdatesofa
co-stateflowandcontrolterm,withtheoreticalguaranteesundercontinuous-timeformulation.
2. InEuclideanspace,weshowthatrunningcostboundstheKLdivergencebetweenpriorand
OC-Flow-inducedjointdistribution. WedevelopasimplealgorithmforOC-Flowthroughiterative
gradientupdateandprovideconvergenceanalysis. WefurtherdemonstratethatDflowandFlow-grad
canbeinterpretedasspecialcasesofEuclideanOC-Flow,providingaunifiedviewoftheproblem.
3. Wepresentoneofthefirstguidedflow-matchingalgorithmontheSO(3)manifoldwiththeoretical
grounds. OurapproachextendstheExtendedMethodofSuccessiveApproximations(E-MSA)to
SO(3)witharigorousconvergenceanalysis. Additionally,weproposeapproximationtechniquesto
enablecomputationallyefficientOC-FlowonSO(3).
4. We provide an efficient and practical implementation of OC-Flow, by introducing the vector-
Jacobianproductandasynchronousupdatescheme. Weshowtheeffectivenessofourmethodwith
extensiveempiricalexperiments,includingtext-guidedimagemanipulation,controllablegeneration
ofsmallmoleculesonQM9,andenergyoptimizationofflow-basedall-atompeptidedesign.
2 PRELIMINARIES AND PERSPECTIVES ABOUT FLOW MATCHING
EuclideanFlowMatching. Flowmatching(Lipmanetal.,2022;Liuetal.,2022)providesan
efficientframeworkfortrainingagenerativemodelbyapproximatingthetime-dependentvectorfield
associatedwiththeflowrepresentedasψ :[0,1]×Rd →Rd.Thisvectorfieldu :[0,1]×Rd →Rd
t t
definesaprobabilitypathoftheevolutionofaninitialnoisydistribution,denotedbyp ,towards
0
atargetdistribution,p ,withthepushforwardprobabilityasp := (ψ ) p . Thedynamicsofthe
1 t t ∗ 0
vectorfieldthatgovernsthisflowcanbedescribedbytheflowordinarydifferentialequation(ODE)
oftheformx˙ = u (x ), wherewefollowtheconventiontouseNewton’snotationwithrespect
t t t
to time t and the state at time t is given by x := ψ (x ). Lipman et al. (2022) demonstrates
t t 0
that a tractable flow matching objective can be obtained by conditioning on the target data x .
1
The primary goal ofconditional flow matching isto train a model, fp : [0,1]×Rd → Rd, such
t
thatitminimizesthedifferencebetweenitsoutputandthegroundtruthconditionalvectorfieldas
L = E ∥fp(x )−u (x | x ,x )∥2. The trained model fp can be employed as the
CFM t,p(x0,x1) t t t t 1 0
marginalvectorfieldduringtheinferencephase. Inthiscontext,onceanoisesamplex isdrawn,the
0
system’sevolutioncanbedescribedbythefollowingdifferentialequation:
x˙ =fp(x ), x ∼p (x). (1)
t t t 0 0
2Preprint
Figure 1: Visualization of different backprop-through-ODE guidance algorithms. D-Flow (left):
updatex directly;FlowGrad(middle):updateeachcontroltermbygradientinformation;OC-Flow
0
(ours,right): incorporatestherunningcostandrewardratioαinterminalreward. weleveragethe
co-stateflowµ tointegratethegradientinformationtoupdatethecontrolterms{θ }.
t t
RotationGroupSO(3). TheformulationofflowmatchingcannaturallyextendtoRiemannian
manifolds(Chen&Lipman,2023). InaRiemannianmanifold,aflowisdefinedasatime-dependent
diffeomorphismsφ :G→G,whichdescribethecontinuousevolutionofpointsonthemanifold
t
overtime,generatedbyavectorfieldV,withV(p)∈T Gforeachp∈GwhereT isthetangent
p p
spaceatpointp∈G. Theflowevolvesaccordingto:dx =V(x )=L V(e). TheCFMobjective
dt t t xt
inEquation1canalsobeadaptedforRiemannianflowmatching,inwhichthegroundtruthvector
fieldcanbecalculatedasthetimederivativeusingtheexponentialandlogarithmmaps. Detailsabout
rotationgroupSO(3)canbefoundinAppendix A.Inthiswork,wefocusspecificallyontheSO(3),
theRiemannianmanifoldofall3DrotationsequippedwiththecanonicalFrobeniusinnerproduct.
In previous flow-based protein design models, each amino acid is associated with a rotation that
definesitsorientation(Yimetal.,2023;Boseetal.,2023;Lietal.,2024). Guidingsuchpre-trained
generativemodelstowardthedesirableproteinpropertiescanpotentiallyhaveaprofoundimpacton
thepharmaceuticalindustry.
3 OPTIMAL CONTROL FRAMEWORK FOR GUIDED FLOW MATCHING
Givenapre-trainedflowmodel,fp(x),parameterizedbyaneuralnetwork,ourgoalistodetermine
t
theoptimalcontroltermsθ thatmaximizetherewardR(x)whilemaintainingtheproximityofthe
t
resultingvectorfieldtotheoriginalvectorfieldinducedbyfp(x). Therewardcanbecustomizedfor
t
diversetaskssuchasinverseproblemR=∥H(x)−y∥2,conditionalgenerationR=(f(x)−c)2,
and constrained generation R = ∥x − y∥2. To ensure proximity, we incorporate a penalty on
(cid:82)T
the state trajectory or control terms L(θ ), also known as the running cost. Optionally, one
0 t
mayalsointroduceametricd(·,·)topenalizethedeviationbetweenthenewterminalstatexθ and
1
the prior terminal state xp. The modified terminal reward function is then defined as: Φ(xθ) =
1 1
R(xθ)−d(xθ,xp). andscalingtheterminalrewardbyaconstantα,wecanformulatetheproblem
1 1 1
asastandardoptimalcontroltask:
(cid:90) T
J(θ):=αΦ(xθ)+ L(θ )dt subjectto x˙θ =h (xθ,θ ). (2)
T t t t t t
0
A fundamental result in optimal control theory is Pontryagin’s Maximum Principle (PMP) (Pon-
tryagin(2018)),whichprovidesthenecessaryconditionsforoptimalsolutionsincontrolproblems.
Specifically,atthecoreofPMPistheintroductionoftheHamiltonianfunction,H. ThisHamiltonian
isdefinedintermsofthestateofthesystem,thecontrol,andanewentitycalledtheco-stateµ,which
residesinthecotangentspaceofthestatemanifold.
H(t,x,µ,θ)=⟨µ,h (xθ,θ )⟩−L(θ) (3)
t t t
Theco-statevariablescanbethoughtofasshadowpricesthatrepresentthesensitivityoftheoptimal
valuefunctiontochangesinthestatevariables. Meanwhile,theterminalrewardisalsoencodedto
µ . PMPstatesthatforacontroltobeoptimal,theHamiltonianmustbemaximizedbytheevolution
T
oftheco-stateµ . ThedetailsofPMPconditionscanbefoundinAppendix B.1.
t
3.1 OC-FLOWONEULIDEANMANIFOLD
WefirstdevelopthealgorithmandtheoreticalanalysisforOC-flowinEuclideanspace. Oneofthe
simplestchoicesforthecontroltermisanadditivecontrol(Kobilarov&Marsden(2011)),which
3Preprint
directlyperturbsthepriortrajectory. Infact,withthelinearexpansion,theadditivecontrolcouldbe
seenasageneralcaseandiswidelyusedinoptimalcontrol. Specifically,withθ representingthe
t
controlinput,thenewstatedynamicsandthecorrespondingrunningcostcanbedefinedas:
1
x˙ =h (xθ,θ )=fp(x )+θ L(θ )=− ||θ ||2 (4)
t t t t t t t t 2 t
Therunningcostcanbeinterpretedasaconstraintonthetrajectoryand,equivalently,asaconstraint
ontheterminalprobabilitydistributionalongaGaussianpath.Thisleadstothefollowingobservation:
Proposition1. UndertheAffineGaussianProbabilityPathsframework,thepersampleθ canbe
t
interpreted as a function of the data target x . The expectation of the running cost (cid:82)1 1∥θ ∥2dt
1 0 2 t
providesanupperboundforaconstantC timestheKullback-Leibler(KL)divergencebetweenthe
joint distributions of thedata point x and theprior terminal point xp, denoted as p (xp,x ) =
1 1 1
p (xp|x )p (x ),andthatofthedatapointx andthecontrolledterminalpointxθ,denotedas
1 1 data 1 1
p (xθ,x ):
1 1
(cid:20) 1(cid:90) 1 (cid:21)
E ∥θ (x )∥2dt ≥C·KL(p (xθ,x )∥p (xp,x )).
x1∼pdata(x1) 2 t 1 1 1 1 1
0
One effective approach for directly apply- Algorithm1OC-FlowonEuclideanSpace
ing PMP to optimal control tasks is the Ex-
tendedMethodofSuccessiveApproximations 1: Given: Pre-trainedmodel: fp,initialstate: x 0
2: Initialize: Controltermsθ0, learningrateη,
(E-MSA) Li et al. (2018). E-MSA builds
weightdecayβ
uponthebasicMSAalgorithm(Chernousko&
3: fork =0toMaxIterationsdo
Lyubushin (1982)), which iteratively updates
4: Solveforthestatetrajectory:
the terms in the PMP conditions (Appendix (cid:16) (cid:17)
B.2). TheprimaryenhancementofE-MSAover X tθ +k ∆t = fp(t,X tθk)+θk ∆t
the basic MSA is its ability to extend conver- 5: Updatecontrol:
genceguaranteesbeyondalimitedclassoflinear θk+1 =βθk+η∇ Φ(Xk)
quadraticregulators(Aleksandrov(1968)). 6:
endfot
r
t xt 1
AkeyassumptionistheglobalLipschitzconditionforthefunctionsinvolved. However,notethatthis
assumptioncanberelaxedtoalocalLipschitzconditionifwecandemonstratethatxθ isbounded,
t
whichcanbesafelyassumedprovidedthatappropriateregularizationtechniquesareapplied.
WhentheE-MSAalgorithmisappliedtotheguidedcontrolledgenerationtaskasdefinedearlier,
thetrajectoryoftheco-statesµ canbecalculatedinclosedform. Further,theycanbeexpressedas
t
∇ Φ(Xk). Overall,thefollowingupdateruleisderived:
xt 1
Theorem2: Assumethattherewardfunction,thepriormodel,andtheirderivativessatisfyLipschitz
continuity,boundedbyaLipschitzconstantL. UtilizingtheE-MSA,foreachiterationk,foreach
constantγ > 2C withC isafunctionofL,suchthatundertheaddictivecontrolandtherunning
costdefinedinEquation4,theoptimalupdateisfollowing:
γ α
θk+1 = θk+ ∇ Φ(Xk). (5)
t 1+γ t 1+γ xt 1
Thisupdateruleforthecontroltermθ guaranteesanincreaseintheobjectivefunctiondefinedin
t
equation2.
(cid:18) (cid:19)
2C
J(θk+1)−J(θk)≥ 1− ϵk, ϵk ≥0 (6)
γ γ γ
Inpractice,continuousODEsaresolvedthroughdiscretization. Thediscretizedformulationofour
algorithmisoutlinedinAlgorithm1. Inthisversion, theweightdecaytermisparameterizedas:
β = γ andthelearningrateisdefinedas: η = α .
1+γ 1+γ
3.2 PRACTICALIMPLEMENTATIONANDACCELERATION
3.2.1 ADJOINTMETHODANDCHOICEOFOPTIMIZER
Asignificant portionofthecomputational timeinthe OC-Flowalgorithm isspenton evaluating
thegradient∇ Φ(x ). Thecomputationalcostofdirectlybackpropagatingthroughthemodelto
xt 1
computetheJacobianisprohibitiveduetothecomplexityoffp,whichisparameterizedasadeep
4Preprint
learningmodel. InspiredbyLiuetal.(2023),thegradient∇ Φ(x )canbeefficientlycomputed
xt 1
usingavector-Jacobianproductbythedouble-backwardstrick,whichisexpressedas:
(cid:0) (cid:1) (cid:0) (cid:1)
∇ Φ= ∇ Φ ·Φ x .
xk/N x(k+1)/N x(k+1)/N k/N
Alternatively, since our algorithm does not require a fixed step size and follows a deterministic
optimizationprocess,itiscompatiblewithsophisticatedoptimizerssuchasLBFGSwithlinesearch,
whichautomaticallydeterminesaquasi-optimalstepsize. NotethatLBFGSmayresultinfaster
convergencebutwiththecostofincreasingmemoryconsumption(Table1)similartoBen-Hamu
etal.(2024). ItisalsonotcompatiblewithOC-FlowinSO(3). Inourexperiments,weobserved
good convergence with the adjoint approach and only used LBFGS in QM9 where the memory
consumptionissmaller,forafaircomparisonwithBen-Hamuetal.(2024).
3.2.2 ASYNCHRONOUSSETTINGFORFLEXIBLEUPDATESCHEDULING
Inpractice,discretizationtechniquesareemployedtosimulatetheODEsgoverningboththestate
trajectoryx andtheco-statesµ andoperateinasynchronoussetting,wherethenumberoftime
t t
stepsforthestatetrajectoryx coincideswiththenumberofcontroltermsθ .
t t
Here we show that OC-Flow can be extended to an asynchronous framework, providing greater
flexibilityinscheduling. Wesubdividethetimeinterval∆tintoN equallyspacedsubintervals. Let
{x }denotethestatetrajectoryoverthetimeinterval[t,t+∆t],and{xθ}representthetrajectory
t t
whenthecontroltermθ isappliedinthefirstsubinterval. Thetrajectorycanbeapproximatedas
t
follows:
N−1 (cid:32) N−1 (cid:33)
∆t (cid:88) ∆t 1 (cid:88) 1
x =x + fp(xθ )+ θ ≈x +∆t fp(x )+ θ (7)
t+∆t t N t+l N∆t N t t N t+l N∆t N t
l=1 l=1
Consequently,theasynchronoussettingallowsthealgorithmtobeappliedwithoutmodificationwhile
enablingfinerupdatestoboththecontroltermsandstatetrajectoriesbyadjustingthefrequencyN of
controltermupdatesrelativetothestatetrajectorysimulation(seeAppendixC.3fortheproofand
justificationoftheapproximationsinEquation7). Inourpeptidedesignexperiment,asynchronous
settingisappliedforefficientcomputing.
Table1: Comparisonofcloselyrelatedmethodsforbackprop-throughguided-ODEinEuclidean
manifold. Fortimecomplexity,N isthenumberofsteps,Listhenumberofthelayersoftheprior
flowmodel. Additionally,ouralgorithmistheonlyoneallowingavariablenumberofcontrolterms.
Effective Running EffectiveNum.of Memory Convergence Generalization
Update Cost ControlTerms Complexity Analysis toSO(3)
OC-Flow(Ours) {θ t}N
0
∥u t∥2 [1,N] O(L) ✓ ✓
OC-Flow-LBFGS(Ours) {θ t}N
0
∥u t∥2 [1,N] O(NL) ✓ ✗
FlowGrad {θ t}N
0
0 N O(L) ✗ ✗
D-Flow θ 0 implicit 1 O(NL) ✗ ✗
3.3 CONNECTIONTOOTHERBACKPROP-THROUGHGUIDED-ODEAPPROACHES
Severalpreviousworksdiscussedbackprop-through-ODEguidanceinflow-matchingmodels.Notable
examplesincludeD-Flow(Ben-Hamuetal.,2024)andFlowGrad(Liuetal.,2023). Anillustration
oftheiralgorithmsandoursisshowninFigure1. Inthissection,wedemonstratethatourframework
ismoregeneral,andbothofthesemethodscanbeviewedasspecialcasesofouralgorithm.
FlowGrad formulates the optimization task in a manner similar to our optimal control target in
Equation2. Specifically,itdirectlyappliesgradientdescenttothecontrolvariables:
α
θk+1 =θk+α∇ Φ(Xk)=θk+ (∇ X )−1∇ Φ(Xk),
t t θt 1 t N xt t+1 xt 1
whichcanbeinterpretedasalimitingcaseofouralgorithminEquation5,whereγ →∞andgiven
withdt→0,∇ X →I. However,asshowninEquation6,theconvergencerateisacomplex
xt t+1
functionofγ,soinpractice,γ istreatedasatunableparameter. FlowGrad’ssettingγ → ∞may
underminetheconvergencespeed.
D-Flowoptimizestherewardbyapplyinggradientdescenttotheinitialnoisex :
0
Xk+1 =Xk+LBFGS(∇ Φ(Xk)).
0 0 x0 1
5Preprint
Infact,withtheupdaterulex =x +f(x )δt+θ δt,theupdateofθ canbeseenasanincrementto
δt 0 0 t 0
x . TheLBFGSalgorithmprovidesadynamiclearningrate,aligningwithourframework,whereγis
0
allowedtovaryacrossiterations. Hence,D-Flowcanbeviewedasaspecialcaseofourasynchronous
algorithmwhenthenumberofcontroltermsis1. Amoredetailedcompareofthealgorithmscanbe
foundinTable1.
4 OPTIMAL CONTROL FRAMEWORK FOR GUIDED FLOW MATCHING IN SO(3)
MostoptimizationalgorithmsaredesignedforEuclideanspacesandencounterdifficultieswhen
extendedtonon-Euclideansettings,suchasthetopologicallyandgeometricallycomplexstructure
ofSO(3). Inthissection,wereviewthecurrentresultsforoptimalcontrolonSO(3)andextendthe
applicabilityoftheE-MSAtothismanifold,providingrigorousproofofitsconvergence.
4.1 OC-FLOWFORSO3
Tobegin,wedefinethevectorfieldgoverningthesystemdynamics. Thestatetrajectory,influenced
bycontroltermsθ ∈so(3),evolvesaccordingtothefollowingdifferentialequation:
t
x˙ θ =xθ(cid:0) f (xθ)+θ (cid:1) (8)
t t t t t
Hereweemploytheleft-invariantvectorfield,whichalignsnaturallywiththestructuralcharacteristics
oftheLiegroupso(3). Thisalignmentisreflectedinthesystemdynamicsdescribedbyequation
8, where the tangent space is expressed as T SO(3) = SO(3)×so(3). Under the left-invariant
x
vectorfield,itcanbeshownthattheHamiltonianbecomesalinearfunctionalinso(3)∗(Jurdjevic
(1996),Colombo&Dimarogonas(2020)). Withtheco-stateµ ∈so(3)∗,theHamiltonianfunction
t
H :[0,T]×SO(3)×so(3)∗×so(3)→Rcanberedefinedas:
H(t,x,µ,θ)=⟨µ ,f (x ,θ)⟩−L(θ) (9)
t t t
Inspired by the Method of Successive Approximations (MSA), a direct approach to apply PMP
conditionsontheSO(3)manifoldinvolvesiterativelyupdatingthecotangentvectorandthestate
trajectory. Ateachiteration,weutilizetrajectoriesofµ andx asshowninStep4andStep5in
t t
Algorithm 2, and then apply an update rule to determine the control term θ for the subsequent
t
iteration. Drawing inspiration from our algorithm in Euclidean space, with weight decay β and
learningrateηtheupdateforθ canbewrittenas:
t
θk+1 =βθk+ηµ˜θk ,
t t t
whereµ˜θk isdefinedby⟨µ˜θk,v⟩ = µθk(v)forallv ∈ so(3). Theexistenceofµ˜θk canbederived
t t t t
fromtheRieszRepresentationTheorem(Goodrich(1970)).Thisformulationleadstotheintroduction
of the OC-FLow algorithm on SO(3), which provides an optimization procedure for the task of
guidedflowmatching,asdetailedinAlgorithm2.
Algorithm2OC-FlowonSO(3)
1: Given: Effectivepre-trainedmodel: f ep,initialstate: x 0
2: Initialize: Controltermθ0 ∈so(3),learningrateη,weightdecayβ,controlfrequencyN
3: fork =0toMaxIterationsdo
(cid:16) (cid:17)
4: Solveforthestatetrajectory: X˙ tθk = f ep(t,X tθk)+ N1θk X tθk, X 0θk =x 0
5: Solvefortheadjointvariables:µ˙θk =−ad∗ µθk−(dL )∗∂H, µθk =(dL )∗∇Φ(xθ)
t ∂ ∂H µ xθ T ∂x T xθ T T
6: Updatecontrol: θk+1 =βθk+ηµ˜θk
t
7: endfor
4.2 CONVERGENCEOFOC-FLOWONSO3
ToderivetheproofoftheconvergenceofourAlgorithm2,wefirstestablishthatunderthePMP
conditionsonSO(3),theobjectivefunctionJ(θ),asdefinedinequation2,canbebounded. Thisis
formalizedinthefollowingproposition:
Proposition 3: Assume that the reward function, the prior model, and their derivatives satisfy
Lipschitzcontinuity,boundedbyaLipschitzconstantL. Then,thereexistsaconstantC >0such
6Preprint
thatforanyθ,ϕ∈so(3),thefollowinginequalityholds:
(cid:90) 1
J(θ)+ ∆ H(t)dt−C||ϕ −θ ||2dt≤J(ϕ), (10)
ϕ,θ t t
0
whereXθ andPθ satisfythePMPconditionsonSO(3)manifold,and∆H denotesthechangein
ϕ,θ
theHamiltonian,definedas:
∆H (t):=H(t,xθ,µθ,ϕ )−H(t,xθ,µθ,θ ).
ϕ,θ t t t t t t
Proposition2providesalowerboundforthedifferenceintheobjectivefunctionvaluesundertwo
distinctcontroltermsthatsatisfythePMPconditionsdescribedbytheHamiltonianequationsin
Appendix B.1.
However,applyingthisresultdirectlyasanoptimizationalgorithmpresentsseveralchallenges. First,
thedifferenceintheHamiltonian∆H (t)isnotinherentlybounded. Second,theterm||ϕ−θ||2is
ϕ,θ
non-negative,whichcomplicatestheminimizationprocess. Toaddresstheseissues,inspiredbythe
methodofE-MSA,weintroduceapositiveconstantγ anddefininganExtended-Hamiltonian:
γ 1 γ
H˜(t,x,µ,θ,ϕ)=H(t,x,µ,θ)− ||θ−ϕ||2 =⟨µ,f (x)+θ⟩− ||θ||2− ||θ−ϕ||2. (11)
2 t 2 2
TheintroductionoftheExtended-HamiltonianenablesthecombinationoftheoriginalHamiltonian
withthepenaltyterm||ϕ−θ||2 intoaunifiedexpressionthatcanbeoptimizedjointly. Anatural
approach to achieve this is by updating θ to maximize the Extended-Hamiltonian. The resulting
updateruleisgivenby:
γ 1
θk+1 =argmaxH˜(t,xθk ,µθk ,θ,θk)= θk+ µθ =βθk+ηµθk (12)
t θ 1+γ 1+γ t t
Byperformingthismaximizationstepateachiteration,weensurethatthechangeintheExtended-
Hamiltonian, ∆H˜, is non-negative, indicating that the algorithm progresses towards an optimal
solution. Furthermore,wecanshowthatwhentheupdateprocessconverges,i.e.,when∆H˜ =0or
equivalently∆H = 0,thealgorithmhasreachedtheoptimalcontrolpoint. Theseinsightscanbe
formalizedinthefollowingproposition:
Proposition4: LetXθ andPθ satisfythePMPconditions. IftheupdaterulefollowsAlgorithm2,
(cid:82)1
wedefineϵ := ∆ H(t)dt,andϵ isboundedas:
k 0 θk+1,θk k
(cid:90) 1
ϵ := ∆ H(t)dt lim ϵ =0. (13)
k θk+1,θk k
0 k→∞
Furthermore,whenϵ =0,wehaveθ =θ∗ :=argmax J(θ)
k θ
Withtheseresults,wecannowextendtheresultinE-MSAtotheSO(3)manifoldandestablisha
boundfortheoptimizationalgorithmbasedonthederivedtheoreticalproperties:
Theorem5: Assumethattherewardfunction,thepriormodel,andtheirderivativessatisfyLipschitz
continuity,boundedbyaLipschitzconstantL. Letθ0 ∈so(3)beanyinitialmeasurablecontrolwith
J(θ0)<+∞. Supposealsothatinf J(θ)>−∞. Iftheupdateofθsatisfiesequation12,for
θ∈so(3)
sufficientlylargeγ,thefollowinginequalityholdsforsomeconstantD >0:
Dϵ ≤J(θk+1)−J(θk) (14)
k
Therefore,byinvokingProposition3,wecanconcludethataftereachupdate,thetargetfunctionis
non-decreasingandwhentheupdateprocessterminates,theoptimalsolutionhasbeenattained. This
establishestheconvergenceoftheOC-FlowalgorithmontheSO(3)manifold.
4.3 PRACTICALIMPLEMENTATION
Inpractice,directlyoptimizingAlgorithm2usingexistingODEmethodsischallengingduetothe
natureoftheadjointvariableµ˙ ,whichisalinearfunctionalinthedualspaceso(3)∗. Insteadwe
t
canoptimizeµ˜ asdefinedinSection4.1. Wecandecomposeµ˜˙ intoitsprojectionsontoasetof
t t
orthogonalbaseswithintheso(3)group.
7Preprint
Figure2: Visualizationoftext-guidedgeneratedfaceswithdifferentexpressions.
Afrequentlyusedchoiceforthebasisinso(3)isthecanonicalbasis{E ,E ,E }(McCannetal.
1 2 3
(2023)), which satisfies the condition ⟨v,E ⟩ = 2 for all v ∈ so(3). This property allows us to
i
expressthetimederivativeoftheadjointvariableµ˜˙ as: µ˜˙ =⟨µ˜˙,E ⟩andµ˜˙ = 1(cid:80)3 µ˜˙ E . Thus,
t i i 2 i=1 i i
withtheclosedformsforthepartialderivativesrelatestoHamiltonian,thepracticalupdateforµ in
t
Algorithm2canbewrittenasfollowing. Meanwhilethemethodofasynchronoussettingandvjp
accelerationcanalsobeapplied. SeeAppendix C.3.
∂fp
µ˜˙k =−⟨µ˜k,[fp+θ ,E ]⟩−⟨µ˜k, t xkE ⟩,
t,i t t t i t ∂x t j
3
∆t(cid:88)
µ˜k =µ˜k− µ˜˙k E , µ˜k =⟨∇Φ(xk),xkE ⟩. (15)
t−∆t t 2 t,i i T,i T T i
i=1
5 EXPERIMENTS
5.1 TEXT-GUIDEDIMAGEMANIPULATION
We first validate our OC-Flow on the traditional text-to-image generation task. Previous work
has demonstrated the importance of alignment with the given text prompt using either auto-
matic metrics or human preference as the reward (Black et al., 2023; Esser et al., 2024). In
our text-guided image manipulation task, we want to guide the generative model pre-trained on
the celebrity face dataset CelebA-HQ (Karras, 2017) to three text guidance {sad, angry,
curly hair} showing different facial expressions or traits. Following the same setup in
Liu et al. (2023), given an input image x , the reward for alignment with the text prompt
g
can be effectively evaluated by the CLIP model (Radford et al., 2021) pre-trained in a con-
trastive way to score the similarity between arbitrary image-text pairs. Following (Liu et al.,
2023), we adopt the pre-trained Rectified Flow (RF) (Liu et al., 2022) as the generative prior.
We choose two state-of-the-art text-guided image
Table2: ComparisonofmethodsonLPIPS,
manipulationbaselines,StyleCLIP(Patashniketal.,
ID,andCLIPmetrics. LowerLPIPSandID
2021)andFlowGrad(Liuetal.,2023). WerunStyle-
indicatebetterperformance,whilehigherID
CLIPandFlowGradwiththeirofficialimplementa-
andCLIPvaluesarepreferred.
tionanddefaultparameterconfigurations. Forours,
Method LPIPS↓ ID↑ CLIP↑
wesettimestepN =100,stepsizeη =2.5,weight
decay=0.995,andthenumberofoptimizationsteps CG+RF 0.346 0.643 0.292
M =15. CG+LDM 0.383 0.513 0.298
DiffusionCLIP 0.398 0.659 0.285
Forqualitativecomparison,weshowgeneratedexam- StyleCLIP+e4e 0.359 0.704 0.267
plesofdifferenttext-guidedexpressionsinFigure2. FlowGrad+RF 0.302 0.737 0.299
Notethatthefacesinthereferenceinputaremostly OC-Flow(Ours) 0.207 0.732 0.302
smiling. Therefore,duetothelargegapbetweenref-
erenceandguideddistributions, StyleCLIPfailsto
manipulatewithsad. Lackinginregularization,FlowGradmaychangethecontenttoomuchwith
curly hair. OurOC-Flowgenerallyproducesthebestresultswithagoodalignmentwiththetext
promptwhilepreservingthegenerativepriorsoastoproducereasonablefacesthatarenotdistorted
much.
8Preprint
Figure 3: Visualization of OC-Flow generated molecules with various dipole moments (µ / D)
condition.
5.2 MOLECULEGENERATIONFORQM9
WefurtherinstantiateourOC-FlowforcontrollablemoleculegenerationontheQM9dataset(Rud-
digkeitetal.,2012;Ramakrishnanetal.,2014),acommonlyusedmoleculardatasetcontainingsmall
moleculeswithupto9heavyatomsfromC,O,N,F.FollowingHoogeboometal.(2022);Ben-Hamu
etal.(2024),wetargetforconditionalgenerationofmoleculeswithspecifiedquantumchemical
propertyvaluesincludingpolarizabilityα,orbitalenergiesε ,ε andtheirgap∆ε,dipole
HOMO LUMO
momentµ,andheatcapacityc . Suchaconditionalgenerationsettingofmoleculeswithdesired
v
propertieshasprofoundpracticalapplicationsindrugdesignandvirtualscreening.
Todefinethelossfunction,separateclassifiersforeachpropertyarefirsttrainedtopredicttheproperty
valueforthegeneratedmolecule(Hoogeboometal.,2022),andthelosscanbethencalculatedasthe
meanabsoluteerror(MAE)betweenthepredictedandthereferencepropertyvalues. Thepre-trained
unconstrainedgenerativemodelistakenfromSongetal.(2024)(EquiFM),aflow-basedgenerative
modelthatusesanequivariantvectorfieldparameterizationforgeneratingtheatomcoordinatesand
typesviathelearnedflowdynamics. Todemonstratethezero-shotguidanceperformanceonsucha
conditionalgenerationtask,wecompareourapproachwithothergradient-basedmethodsofD-Flow
(Ben-Hamuetal.,2024)andFlowGrad(Liuetal.,2023)onthesamepre-trainedEquiFM.Tobe
comparabletoD-Flow,wefollowitssettingtousetheL-BFGSoptimizerwith5optimizationsteps
withlinearsearch. Wegenerate1000moleculesforeachpropertyandreporttheMAEinTable3.
The unconditional EquiFM is also included as an upper bound for the guided models. It can be
seenthatourapproach,asageneralizationofbothD-FlowandFlowGrad,consistentlyoutperforms
bothofthemwithlowerMAEs. WeprovideguidedgenerationsamplesinFigure3withrespectto
differenttargetdipolemoments. Acleartrendfromhydrocarbonswithmoresymmetricstructuresto
moleculeswithmorehigh-electronegativityatomsofoxygenandnitrogencanbeobserved,indicating
anincreaseinthedipolemoment.
Aswehavetheoreticallydemon-
Table3: MAEforguidedgenerationsonQM9(lowerisbetter).
stratedtheimpactoftheregular-
Property α ∆ε ε ε µ c
izationstrengthfromtheoptimal HOMO LUMO v
Unit Bohr³ meV meV meV D cal
control perspective, we further K·mol
experimentwithadifferentγand OC-Flow(Ours) 1.383 367 183 342 0.314 0.819
examine the quality of the con- D-Flow 1.566 355 205 346 0.330 0.893
ditionally generated molecules FlowGrad 2.484 517 273 429 0.542 1.270
byevaluatingadditionalmetrics EquiFM 8.969 1439 622 1438 1.593 6.873
following Song et al. (2024).
Specifically,wecalculatetheatomstabilitypercentage(ASP),moleculestabilitypercentage(MSP),
andvalid&uniquepercentage(VUP).Ideally,thesemetricsshouldnotbegreatlylowerthanthe
pre-trainedmodel,andahigherstrengthofregularizationshouldleadtohigherscores. Indeed,as
demonstratedinTable4,inwhichweprovidethesescoresfortwodifferentsettingsofγ =0.01and
10,allscoresarehigherwithahigherstrengthofregularizationatthecostofalsoahigherMAE.In
thisway,ourOC-Floweffectivelypreventsexploitationfromdirectgradientdescentthatmayhack
thelossfunctionandprovidesmoreflexibleandfine-grainedcontrolovertheguidedgeneration.
Table4: MAEandotherevaluationmetricsforourapproachwithγ =0.01/γ =10.
Property α ∆ε ε ε µ c
HOMO LUMO v
MAE↓ 1.383/1.557 367/365 183/188 342/339 0.314/0.320 0.819/0.852
ASP↑ 94.8/96.0 95.2/96.1 95.2/96.1 95.3/96.1 95.8/96.1 95.2/96.1
MSP↑ 64.4/69.9 67.9/70.5 65.8/69.8 68.5/70.8 68.0/70.1 67.1/68.9
VUP↑ 86.2/88.6 88.2/89.8 86.2/87.7 87.6/88.7 88.2/89.0 89.4/88.5
9Preprint
Figure4: VisualizationofOC-Flowgeneratedpeptideandunconditionalgeneratedpeptide(5djd_C).
5.3 PEPTIDEDESIGN
We evaluate our OC-Flow approach for peptide Backbone design using a test set derived from
(Lietal.,2024), whichincludes158complexesclusteredbasedon40%sequenceidentityusing
mmseqs2(Steinegger&Söding,2017).OurexperimentsfocusonPepFloww/Bb,amodeldesignedto
exclusivelysamplepeptidebackboneswhileoptimizingtranslationsinEuclideanspaceandrotations
in SO(3) space. The model employs the MadraX force field (Orlando et al., 2024) for energy
optimization,andperformanceisevaluatedusingseveralkeymetrics. ThesemetricsincludeMadraX
energy,whichassessesthetotalenergyofthegeneratedpeptidestructures,alongwithRosetta-based
measuresofstabilityandaffinity. Currently,ourstabilityandaffinitymetricsarerepresentedbytheir
respectivemeans: stabilityquantifiestheenergystatesofpeptide-proteincomplexes,whileaffinity
measuresthebindingenergies.
In addition, we employ an existing metric, denoted as IMP, which measures the percentage of
generatedpeptidesthatexhibitlowerenergythantheoriginalgroundtruth. Additionally,weusethe
root-mean-squaredeviation(RMSD)toevaluatestructuralaccuracybyaligningthegeneratedpeptides
totheirnativestructuresandcalculatingtheC RMSD.Tofurtheranalyzestructuralcharacteristics,
α
wecomputethesecondary-structuresimilarityratio(SSR),whichreflectstheproportionofshared
secondary structures, and the binding site ratio (BSR), which quantifies the overlap between the
binding sites of the generated and native peptides on the target protein. Structural diversity is
assessedusingtheaverageofoneminusthepairwiseTM-Score(Zhang&Skolnick,2005)among
thegeneratedpeptides,representingtheirdissimilarities.
Wecompareourmethodtothepre-trainedunconditionalPepFlowmodel(Lietal.,2024),serving
asabaseline. Wealsoincludeablationswhereourmodelguidesonlytranslations(Euclidean)or
rotations(SO(3)). AsshowninTable5,ourOC-Flowmethod,appliedtobothEuclideanandSO(3)
spaces,consistentlyoutperformsthebaseline,eventhoughweonlyoptimizefortheMadraxtarget
function. Thisindicatesthatouralgorithmnotonlyachieveshighertargetfunctionscoresbutalso
capturesmorenaturalstructuralconfigurations. Itgeneratespeptidebackbonesthataremorestable,
energetically favorable, and diverse, while improving key metrics such as stability, affinity, IMP,
diversity,SSR,andBSR.Incomparison,optimizinginEuclideanspacealoneyieldsonlymarginal
improvementsinIMP,whileoptimizingrotationsaloneachievescomparableperformance. More
experimentaldetailscanbefoundinAppendix B.1.
Table5: EvaluationofOC-Flowpeptidedesign.
MadraX ↓ RMSD ↓ SSR%↑ BSR%↑ Stability↓ Affinity↓ Diversity↑ imp(%)↑
Ground-truth -0.588 - - - -84.893 -36.063 - -
PepFlow -0.195 1.645 0.794 0.874 -45.660 -26.538 0.310 14.3
OC-Flow(trans) -0.229 1.774 0.797 0.876 -48.380 -27.328 0.323 14.4
OC-Flow(rot) -0.221 1.643 0.794 0.872 -48.636 -27.211 0.310 14.5
OC-Flow(trans+rot) -0.263 2.127 0.797 0.869 -48.853 -27.468 0.338 15.0
6 CONCLUSIONS AND DISCUSSION
Inthispaper,weproposeOC-Flow,ageneralandtheoreticallygroundedframeworkfortraining-
freeguidedflowmatchingunderoptimalcontrolformulation. Ourframeworkprovidesaunified
perspectiveonexistingbackprop-through-ODEapproachesandlaysthefoundationforsystematic
analysisoftheoptimizationdynamicsofthissetting. Extensiveempiricalexperimentsdemonstrate
10Preprint
theeffectivenessofOC-Flow. FutureextensionsofOC-Flowincludegeneralizingbeyondadditive
controltermsandbridgingconnectionwithfine-tuningregimeswherecontroltermscanbesolvedas
learningupdatestothemodelparameters. AnotherextensioncouldbescalinguptheSO(3)OC-Flow
toguidegenerativetasksforlargermolecularsystemssuchasproteinmotifscaffolding. Wealsonote
thatourpracticalalgorithmsutilizediscretizationwhichcouldleadtogapsinconvergenceanalysis
derivedundercontinuousformulation. Weleavethediscussionsofdiscretizationanalysisasafuture
directionforstudy. Wehopethatourfindingscanguidealgorithmdesignandmotivatefurthermodel
improvementinguidedflowmatching.
REFERENCES
Vladimir V Aleksandrov. On the accumulation of perturbations in the linear systems with two
coordinates. VestnikMGU,3:67–76,1968.
Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, and Yaron Lipman. D-flow:
Differentiatingthroughflowsforcontrolledgeneration. arXivpreprintarXiv:2402.14017,2024.
KevinBlack,MichaelJanner,YilunDu,IlyaKostrikov,andSergeyLevine.Trainingdiffusionmodels
withreinforcementlearning. arXivpreprintarXiv:2305.13301,2023.
AvishekJoeyBose,TaraAkhound-Sadegh,KilianFatras,GuillaumeHuguet,JarridRector-Brooks,
Cheng-HaoLiu,AndreiCristianNica,MaksymKorablyov,MichaelBronstein,andAlexanderTong.
Se(3)-stochasticflowmatchingforproteinbackbonegeneration. arXivpreprintarXiv:2310.02391,
2023.
RickyTQChenandYaronLipman. Riemannianflowmatchingongeneralgeometries. arXivpreprint
arXiv:2302.03660,2023.
ChaoranCheng,JiahanLi,JianPeng,andGeLiu. Categoricalflowmatchingonstatisticalmanifolds.
arXivpreprintarXiv:2405.16441,2024.
FelixLChernouskoandAALyubushin. Methodofsuccessiveapproximationsforsolutionofoptimal
controlproblems. OptimalControlApplicationsandMethods,3(2):101–114,1982.
HyungjinChung,JeongsolKim,GeonYeongPark,HyelinNam,andJongChulYe.Cfg++:Manifold-
constrainedclassifierfreeguidancefordiffusionmodels. arXivpreprintarXiv:2406.08070,2024.
LeonardoJesusColomboandDimosV.Dimarogonas. Symmetryreductioninoptimalcontrolof
multiagentsystemsonliegroups. IEEETransactionsonAutomaticControl,65(11):4973–4980,
2020. doi: 10.1109/TAC.2020.3004795.
Alexandre Défossez, Jade Copet, Gabriel Synnaeve, and Yossi Adi. High fidelity neural audio
compression. arXivpreprintarXiv:2210.13438,2022.
FloorEijkelboom,GrigoryBartosh,ChristianAnderssonNaesseth,MaxWelling,andJan-Willem
vandeMeent. Variationalflowmatchingforgraphgeneration. arXivpreprintarXiv:2406.04843,
2024.
PatrickEsser,SumithKulal,AndreasBlattmann,RahimEntezari,JonasMüller,HarrySaini,Yam
Levi,DominikLorenz,AxelSauer,FredericBoesel,etal. Scalingrectifiedflowtransformersfor
high-resolutionimagesynthesis. InForty-firstInternationalConferenceonMachineLearning,
2024.
LawrenceCEvans. Anintroductiontomathematicaloptimalcontroltheoryversion0.2. Lecture
notesavailableathttp://math.berkeley.edu/evans/control.course.pdf,1983.
GiorgioGiannone,AkashSrivastava,OleWinther,andFaezAhmed. Aligningoptimizationtrajec-
torieswithdiffusionmodelsforconstraineddesigngeneration. AdvancesinNeuralInformation
ProcessingSystems,36:51830–51861,2023.
RobertKentGoodrich. Arieszrepresentationtheorem. ProceedingsoftheAmericanMathematical
Society,24(3):629–636,1970.
11Preprint
JonathanHoandTimSalimans. Classifier-freediffusionguidance. arXivpreprintarXiv:2207.12598,
2022.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
neuralinformationprocessingsystems,33:6840–6851,2020.
EmielHoogeboom,VıctorGarciaSatorras,ClémentVignac,andMaxWelling. Equivariantdiffusion
formoleculegenerationin3d. InInternationalconferenceonmachinelearning,pp.8867–8887.
PMLR,2022.
VelimirJurdjevic. OptimalproblemsonLiegroups,pp.368–406. CambridgeStudiesinAdvanced
Mathematics.CambridgeUniversityPress,1996.
TeroKarras. Progressivegrowingofgansforimprovedquality,stability,andvariation. arXivpreprint
arXiv:1710.10196,2017.
BahjatKawar,RoyGanz,andMichaelElad. Enhancingdiffusion-basedimagesynthesiswithrobust
classifierguidance. arXivpreprintarXiv:2208.08664,2022.
GwanghyunKimandJongChulYe. Diffusionclip: Text-guidedimagemanipulationusingdiffusion
models. 2021.
MarinB.KobilarovandJerroldE.Marsden. Discretegeometricoptimalcontrolonliegroups. IEEE
TransactionsonRobotics,27(4):641–655,2011. doi: 10.1109/TRO.2011.2139130.
JiahanLi, ChaoranCheng, ZuofanWu, RuihanGuo, ShitongLuo, ZhizhouRen, JianPeng, and
JianzhuMa. Full-atompeptidedesignbasedonmulti-modalflowmatching. InProceedingsofthe
InternationalConferenceonMachineLearning(ICML),2024.
QianxiaoLi,LongChen,ChengTai,andEWeinan. Maximumprinciplebasedalgorithmsfordeep
learning. JournalofMachineLearningResearch,18(165):1–29,2018.
YaronLipman,RickyTQChen,HeliBen-Hamu,MaximilianNickel,andMattLe. Flowmatching
forgenerativemodeling. arXivpreprintarXiv:2210.02747,2022.
XingchaoLiu,ChengyueGong,andQiangLiu. Flowstraightandfast: Learningtogenerateand
transferdatawithrectifiedflow. arXivpreprintarXiv:2209.03003,2022.
XingchaoLiu,LemengWu,ShujianZhang,ChengyueGong,WeiPing,andQiangLiu. Flowgrad:
Controlling the output of generative odes with gradients. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.24335–24344,2023.
AaronLou,ChenlinMeng,andStefanoErmon. Discretediffusionlanguagemodelingbyestimating
theratiosofthedatadistribution. arXivpreprintarXiv:2310.16834,2023.
BrennanSMcCann,AnnikaAnderson,MoradNazari,DavidCanales,andRyneBeeson.On-manifold
poseoptimizationonse(3)forspacecraftcoveragemaximization. InAAS/AIAAAstrodynamics
SpecialistsConference,2023.
GabrieleOrlando,LuisSerrano,JoostSchymkowitz,andFredericRousseau. Integratingphysicsin
deeplearningalgorithms: aforcefieldasapytorchmodule. Bioinformatics,40(4):btae160,April
2024. doi: 10.1093/bioinformatics/btae160.
OrPatashnik,ZongzeWu,EliShechtman,DanielCohen-Or,andDaniLischinski. Styleclip: Text-
drivenmanipulationofstyleganimagery.InProceedingsoftheIEEE/CVFinternationalconference
oncomputervision,pp.2085–2094,2021.
AshwiniPokle,MatthewJMuckley,RickyTQChen,andBrianKarrer. Training-freelinearimage
inversionviaflows. arXivpreprintarXiv:2310.04432,2023.
LevSemenovichPontryagin. Mathematicaltheoryofoptimalprocesses. Routledge,2018.
AlecRadford, JongWookKim, ChrisHallacy, AdityaRamesh, GabrielGoh, SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,pp.
8748–8763.PMLR,2021.
12Preprint
RaghunathanRamakrishnan,PavloODral,MatthiasRupp,andOAnatolevonLilienfeld. Quantum
chemistrystructuresandpropertiesof134kilomolecules. ScientificData,1,2014.
LarsRuddigkeit,RuudVanDeursen,LorenzCBlum,andJean-LouisReymond. Enumerationof166
billionorganicsmallmoleculesinthechemicaluniversedatabasegdb-17. Journalofchemical
informationandmodeling,52(11):2864–2875,2012.
AlessandroSaccon,JohnHauser,andA.PedroAguiar. Explorationofkinematicoptimalcontrolon
theliegroupso(3)*. IFACProceedingsVolumes,43(14):1302–1307,2010. ISSN1474-6670. doi:
https://doi.org/10.3182/20100901-3-IT-2016.00237. URLhttps://www.sciencedirect.
com/science/article/pii/S1474667015371457. 8thIFACSymposiumonNonlin-
earControlSystems.
JunJohnSakuraiandJimNapolitano. Modernquantummechanics. CambridgeUniversityPress,
2020.
JiamingSong,QinshengZhang,HongxuYin,MortezaMardani,Ming-YuLiu,JanKautz,Yongxin
Chen,andArashVahdat. Loss-guideddiffusionmodelsforplug-and-playcontrollablegeneration.
InInternationalConferenceonMachineLearning,pp.32483–32498.PMLR,2023.
Yuxuan Song, Jingjing Gong, Minkai Xu, Ziyao Cao, Yanyan Lan, Stefano Ermon, Hao Zhou,
andWei-YingMa. Equivariantflowmatchingwithhybridprobabilitytransportfor3dmolecule
generation. AdvancesinNeuralInformationProcessingSystems,36,2024.
MartinSteineggerandJohannesSöding. Mmseqs2enablessensitiveproteinsequencesearchingfor
theanalysisofmassivedatasets. Naturebiotechnology,35(11):1026–1028,2017.
MinkaiXu,AlexanderSPowers,RonODror,StefanoErmon,andJureLeskovec. Geometriclatent
diffusionmodelsfor3dmoleculegeneration. InInternationalConferenceonMachineLearning,
pp.38592–38610.PMLR,2023.
JasonYim,AndrewCampbell,AndrewYKFoong,MichaelGastegger,JoséJiménez-Luna,Sarah
Lewis,VictorGarciaSatorras,BastiaanSVeeling,ReginaBarzilay,TommiJaakkola,etal. Fast
proteinbackbonegenerationwithse(3)flowmatching. arXivpreprintarXiv:2310.05297,2023.
JasonYim,AndrewCampbell,EmileMathieu,AndrewYKFoong,MichaelGastegger,JoséJiménez-
Luna, Sarah Lewis, Victor Garcia Satorras, Bastiaan S Veeling, Frank Noé, et al. Improved
motif-scaffoldingwithse(3)flowmatching. arXivpreprintarXiv:2401.04082,2024.
ChenshuangZhang,ChaoningZhang,ShengZheng,MengchunZhang,MaryamQamar,Sung-Ho
Bae, and In So Kweon. A survey on audio diffusion models: Text to speech synthesis and
enhancementingenerativeai. arXivpreprintarXiv:2303.13336,2023.
YangZhangandJeffreySkolnick. Tm-align: aproteinstructurealignmentalgorithmbasedonthe
tm-score. Nucleicacidsresearch,33(7):2302–2309,2005.
13Preprint
A BACKGROUND OF RIEMANNIAN MANIFOLD AND SO(3) GROUP
ALiegroupGisasmoothmanifoldequippedwithgroupoperations,suchasmultiplicationand
inversion,whicharesmoothmaps. Specifically,GisconsideredsmoothwhenitpossessesaC∞
differentialstructure. WhenGisendowedwithaleft-invariantRiemannianmetric, itbecomesa
Riemannianmanifold,wheretheinnerproductofanytwotangentvectorsv,w ∈ T Gatapoint
h
h∈Gispreservedunderleftmultiplication. Thispropertyisexpressedas:
⟨L (v),L (w)⟩=⟨v,w⟩,
h h
whereL :G→Gistheleftmultiplicationmap,and⟨·,·⟩:TG×TG→Rrepresentstheinner
h
product. Moreover,thetangentspaceatanypointx∈GisgivenbyT G=L T G,whereT Gis
x x e e
thetangentspaceattheidentityelemente,whichisidentifiedwiththeLiealgebrag. Consequently,
thefulltangentbundleTGcanbewrittenasG×g.
Ateachpointx∈G,atangentspaceT Gisattached,representingthespaceoftangentvectorsat
x
thatpoint. ThecollectionofthesetangentspacesformsthetangentbundleTG, whichitselfisa
smoothmanifold. Additionally,foranypointh∈G,thecotangentspaceT∗Gisdefinedasthedual
h
spaceofT G,consistingoflinearfunctionals(co-states)thatactonthetangentvectors.
h
RotationGroupSO(3): ThespecialorthogonalgroupSO(3),describing3Drotations,isacompact
3-dimensionalLiegroup. ItsLiealgebraso(3)consistsofskew-symmetricmatrices. Thegroup
SO(3)isdefinedas:
SO(3)={r ∈R3×3 :r⊤r =rr⊤ =I,det(r)=1}
ItisamatrixLiegroup,anditsLiealgebraisgivenby:
so(3)={r ∈R3×3 :r⊤ =−r}
ParametrizationsofSO(3): Theskew-symmetricmatricesr ∈so(3)canbeuniquelyrepresented
byavectorω ∈R3,suchthatforanyv ∈R3,rv =ω×v,where×denotesthecrossproduct. This
vectorisknownastherotationvector,whereitsmagnitude∥ω∥representstheangleofrotation,and
itsdirectione = ω definestheaxisofrotation. ThemappingfromR3 totheskew-symmetric
ω ∥ω∥
ˆ
matrixisreferredtoasthehatoperation,(·).
AnothercommonparametrizationofSO(3)isthroughEulerangles,describedusingthreeangles
(ϕ,θ,ψ). Inthex-convention,therotationisexpressedasasequenceofthreerotations: arotation
aboutthez-axisbyϕ,followedbyarotationabouttheupdatedx-axisbyθ,andfinally,arotation
abouttheupdatedz-axisbyψ.
Onecommoninnerproductonso(3)istheinducedFrobeniusinnerproduct,givenby:
⟨A,B⟩=tr(A⊤B), ∀A,B ∈so(3),
which equips SO(3) with a Riemannian structure. The manifold SO(3) has constant Gaussian
curvatureandisdiffeomorphictoasolidballwithantipodalpointsidentified. Theexponentialmap
exp:so(3)→SO(3),originatingfromtheidentityelement,isdefinedasmatrixexponentiation:
(cid:88)∞ Ak
exp(A)= , ∀A∈so(3)
k!
k=0
andcanberepresentedmorecompactlyviaRodrigues’rotationformula:
sinθ 1−cosθ
exp(A)=I+ A+ A2, ∀A∈so(3)
θ θ2
whereθ = ∥A∥ = 1∥A∥ istherotationangle. Similarly,thelogarithmmaplog : SO(3) →
so(3) 2 F
so(3),alsooriginatingfromtheidentity,isthematrixlogarithm:
(cid:88)∞ (R−I)k
log(R)= (−1)k+1
k
k=1
ormorecompactlyas:
θ
log(R)= A, ∀R∈SO(3)
sinθ
where A =
(R−R⊤)
∈ so(3) and θ = ∥A∥ is the rotation angle. In spherical geometry, the
2 so(3)
geodesicdistancebetweentworotationsisgivenbyd(R ,R )=∥log(R⊤R )∥ ,andinterpolation
1 2 1 2 F
betweenrotationscanbeperformedusingexp(tA).
14Preprint
B BACKGROUND OF PMP AND E-MSA
In this section, we shall introduce the details of the Pontryagin’s Maximum Principle on both
EuclideanSpaceandSO(3)manifoldaswellasthealgorithmsofMSAandE-MSA.
B.1 PMP
Asdescribedinsection3,PMPoffersasetofnecessaryconditionsforanoptimalcontrolstrategy.
PMPstatesthatforanoptimaltrajectory,thereexistsaco-statetrajectoryµ suchthattheHamiltonian
t
(Equation3)ismaximized(orminimized,dependingontheproblem)withrespecttothecontrol
at every time step. Additionally, the state and co-state evolve according to a system of coupled
differentialequations,wheretheco-statevariablesevolveinthecotangentspaceofthestatevariables.
Thegoverningdifferentialequationsareshownbelow:
Pontryagin’sMaximumPrincipleLetθ∗ ∈ U beanessentiallyboundedoptimalcontrol,i.e. a
solutionto(2)withesssup ∥θ∗∥ ⟨∞(esssupdenotestheessentialsupremum). Denoteby
t∈[0,T] t ∞
X∗thecorrespondingoptimallycontrolledstateprocess. Then,thereexistsanabsolutelycontinuous
co-stateprocessP∗ :[0,T]→RdsuchthattheHamilton’sequations
X˙∗ =∇ H(t,X∗,P∗,θ∗), X∗ =x, (16)
t p t t t 0
P˙∗ =−∇ H(t,X∗,P∗,θ∗), P∗ =∇Φ(X∗), (17)
t x t t t T T
aresatisfied. Moreover,foreacht∈[0,T],wehavetheHamiltonianmaximizationcondition
H(t,X∗,P∗,θ∗)≥H(t,X∗,P∗,θ) forallθ ∈Θ. (18)
t t t t t
ThePMPconditionscanbenaturallygeneralisedtotheLieGroup. Pontryagin’sMaximumPrinciple
(PMP) for Lie groups (Saccon et al. (2010)) provides the conditions that govern the flow of the
statex anditsassociatedcotangentflowλ,describingtheHamiltonianequationsthattheoptimal
t
state-adjointtrajectorymustsatisfy. Specifically,theHamiltonianequationsaregivenby:
∂
X−1X˙∗ = H(t,X∗,P∗,θ∗), X∗ =x,
t t ∂p t t t 0
∂
µ˙θ =−ad∗ µθ−(dL )∗ H∗, µθ =(dL )∗∇ Φ(xθ). (19)
t ∂ ∂H µ t xθ t ∂x T xθ T x T
Thedualmap(dL )⋆ : T⋆ SO(3) → T⋆SO(3)pullsbackacotangentvectoratghtoacotangent
g gh h
vectorath. Thecoadjointrepresentationad∗ actsonso(3)∗andisdefinedas:
X
⟨ad∗ µ,Y⟩=−⟨µ,ad Y⟩,
X X
wheread Y = [X,Y] = XY −YX forµ ∈ so(3)∗ andX,Y ∈ so(3). Additionally, foreach
X
t∈[0,T],theHamiltonianmaximizationconditionissatisfied:
H(t,X∗,µ∗,θ∗)≥H(t,X∗,µ∗,θ) forallθ ∈Θ. (20)
t t t t t
B.2 EXTENDED-METHODOFSUCCESSIVEAPPROXIMATIONS(E-MSA)
B.2.1 METHODOFSUCCESSIVEAPPROXIMATIONS(MSA)
One numerical method for solving the Pontryagin Maximum Principle (PMP) is the Method of
Successive Approximations (MSA) Chernousko & Lyubushin (1982), an iterative approach that
alternatesbetweenpropagationandoptimizationstepsbasedonthePMPconditions. Wefirstpresent
thesimplestformofMSA.
Considerthegeneralstatedynamics:
X˙∗ =f(t,X∗,θ∗),
t t
Givenaninitialguessθ0 ∈U fortheoptimalcontrol,foreachiterationk =0,1,2,...,wefirstsolve
thestatedynamics:
X˙θk =f(t,Xθk ,θk), Xθk =x,
t t t 0
15Preprint
toobtainXθk,followedbysolvingtheco-stateequation:
µ˙θk =−∇ H(t,Xθk ,µθk ,θk), µθk =−∇Φ(Xθk ),
t x t t t T T
todetermineµθk. Finally,thecontrolisupdatedusingthemaximizationcondition:
θk+1 =argmaxH(t,Xθk ,µθk ,θ),
t t t
θ∈Θ
fort∈[0,T]. ThisprocessissummarizedinAlgorithm3.
Algorithm3BasicMSA
1: Initialize: θ0 ∈U;
2: fork =0to#Iterationsdo
3: SolveX˙θk =f(t,Xθk,θk), Xθk =x;
t t t 0
4: Solveµ˙θ tk =−∇ xH(t,X tθk,µθ tk,θ tk), µθ Tk =−∇Φ(X Tθk);
5: Setθ tk+1 =argmax θ∈ΘH(t,X tθk,µθ tk,θ)foreacht∈[0,T];
6: endfor
B.2.2 E-MSA
E-MSAintroducestheaugmentedHamiltonian
1 1
H˜(t,x,µ,θ,v,q):=H(t,x,µ,θ)− ρ∥v−f(t,x,θ)∥2− ρ∥q+∇ H(t,x,µ,θ)∥2.
2 2 x
Then,theydefinethefollowingsetofalternativenecessaryconditionsforoptimality:
Proposition3(ExtendedPMP)Supposethatθ∗isanessentiallyboundedsolutiontotheoptimal
controlproblem(2). Then,thereexistsanabsolutelycontinuousco-stateprocessµ∗ suchthatthe
tuple(X∗,µ∗,θ∗)satisfiesthenecessaryconditions
X˙∗ =∇ H˜(t,X∗,µ∗,θ∗,X˙∗,µ∗), X∗ =x
t µ t t t t t 0
P˙∗ =−∇ H˜(t,X∗,µ∗,θ∗,X˙∗,µ∗), µ∗ =−∇ Φ(X∗)
t x t t t t t T x T
H˜(t,X∗,µ∗,θ∗,X˙∗,µ∗)≥H˜(t,X∗,µ∗,0,X˙∗,µ∗), θ ∈Θ,t∈[0,T]
t t t t t t t t t
ThekeycontributionisthatthecontroltermsθcanbeupdatedbyiterationusingtheExtended-PMP
asshownbelow. Meanwhile,itisproventhatunderthisupdaterule,foreachiteration,thetarget
functionJ(θ)isnon-deceasing.
Algorithm4ExtendedMSA
1: Initialize: θ0 ∈U. Hyper-parameter: ρ;
2: fork =0to#Iterationsdo
3: SolveX˙θk =f(t,Xθk,θk), Xθk =x;
t t t 0
4: SolveP˙ tθk =−∇ xH(t,X tθk,P tθk,θ tk), P Tθk =−∇Φ(X Tθk);
5: Setθ tk+1 =argmax θ∈ΘH˜(t,X tθk,P tθk,θ,X˙ tθk,P˙ tθk)foreacht∈[0,T];
6: endfor
C PROOFS AND THEOREMS
C.1 PROOFFORPROPOSITION1
WerestateProposition1here:
Proposition1. UndertheAffineGaussianProbabilityPathsframework,thepersampleθ canbe
t
interpreted as a function of the data target x . The expectation of the running cost (cid:82)1 1∥θ ∥2dt
1 0 2 t
providesanupperboundforaconstantC timestheKullback-Leibler(KL)divergencebetweenthe
16Preprint
joint distributions of thedata point x and theprior terminal point xp, denoted as p (xp,x ) =
1 1 1
p (xp|x )p (x ),andthatofthedatapointx andthecontrolledterminalpointxθ,denotedas
1 1 data 1 1
p (xθ,x ):
1 1
1(cid:90) 1
E [ ∥θ (x )∥2dt]≥C·KL(p (xθ,x )∥p (xp,x )).
x1∼pdata(x1) 2 t 1 1 1 1 1
0
ProofForAffineGaussianProbabilityPaths,theconditionalflowdistributionofthepriorvectorfield
canbewrittenas: Pp(x|x )=N(µ (x ),σ (x )2I),theconditionalvectorfieldiswrittenas:
t 1 t 1 t 1
ϕ (x)=µ (x )+σ (x )x
t t 1 t 1
withitsdynamics:
ϕ˙ (x)=µ˙ (x )+σ˙ (x )x
t t 1 t 1
Or
σ˙
u (x|x )=µ˙ (x )+ t(x−µ (x ))
t 1 t 1 σ t 1
t
Now we add the addictive control term and assume it only alters the mean of the distribution by
denotingitasθ (x )bynoticingitisnotproportionaltox:
t 1
ϕ˙ (x)=θ (x )+µ˙ (x )+σ˙ (x )x
t t 1 t 1 t 1
Theresultingpushing-forwardprob:
(cid:90)
Pθ(x)= Pθ(x|x )q(x )dx
t t 1 1 1
Pu(x|x )=N(µθ(x ),σ (x )2I)
t 1 t 1 t 1
Usually,besidesmaximisingthereward,wehopethenewdistributionnottoomuchawayfromthe
originaldistribution. OnecommonconstraintontheterminalstatedistributionisthetheKullback-
Leibler(KL)divergence.
InspiredbyVariationalFlowMatching(Eijkelboometal.(2024)),wecan(KL)divergencebetween
thejointdistributionsofthedatapointx andthepriorterminalpointxp,denotedasp (xp,x )=
1 1 1
p (xp|x )p (x ),andthatofthedatapointx andthecontrolledterminalpointxθ,denotedas
1 1 data 1 1
p (xθ,x ):KL(Pp(x,x )||Pu(x,x ))
1 1 1 1 1 1
TheKLtermcanbesimplifiedas:
(cid:90) (cid:90) Pp(x|x )p (x )
KL(Pp(x,x )||Pθ(x,x ))= Pp(x|x )q(x )log( 1 1 data 1 )dxdx
1 1 1 1 1 1 1 Pθ(x|x )p (x ) 1
1 1 data 1
=E [KL(Pp(x|x )||Pθ(x|x ))]
x1∼pdata(x1) 1 1 1 1
GivenPp(x|x )andPθ(x|x )aretwoGaussianswithsamevariancebutdifferentmean,andwhen
1 1 1 1
weconsdiertheconstraintpersample,theconstraintcanbewrittenas:
1||µθ(x )−µp(x )||2
1 1 1 1
2 σ (x )2
1 1
Given:
(cid:90) 1 (cid:90) 1
||µθ(x )−µp(x )||2 =|| θ dt||2 ≤ ∥θ ∥2dt
1 1 1 1 t t
0 0
Therefore,wehavethefollowinginequality:
1(cid:90) 1 1
E [ ∥θ (x )∥2dt]≥ ·KL(p (xθ,x )∥p (xp,x ))
x1∼pdata(x1) 2 t 1 4σ (x )2 1 1 1 1
0 1 1
17Preprint
C.2 PROOFFORTHEOREM2
SincetheExtendedMethodofSuccessiveApproximations(E-MSA)isappliedinthiscontext,its
convergencepropertiesaredirectlyinherited. Inthissection,wefocusonhowourupdateruleis
derivedfromtheE-MSAupdaterule,specificallyundertheframeworkofadditivecontroltermsand
runningcost.
DefineHamiltonianH andExtendedHamiltonianH˜:
1
H(t,x,µ,θ)=µ ·f(x,t)+µ ·θ − ||θ ||2
t t t 2 t
1 γ
H˜(t,x,µ,θ,x˙,µ˙)=µ ·f(x,t)+µ ·θk+1− ||θk+1||2− ||θk+1−θk||2
t t t 2 t 2 t t
ApplyExtendedMSA:
x˙k =θk+f(xk,t)
t t t
µ˙k =−∇ H(t,xk,µk,θk)=−∇ f(xk,t)µk
t x x t t
µk =α∇ Φ(xk)
1 x1 1
θk+1 =argmax H˜(t,x,µ,θ,x˙,µ˙)
t θt
µk canbecalculatedinclosedformasbelow,withT expisthetime-orderexponential:
t
(cid:18)(cid:90) 1 (cid:19)
µk =T exp ∇ f(xk,s)ds ·α∇ Φ(Xk)
t x s x1 1
t
Thus,theupdateruleofθ is:
t
γ α (cid:18)(cid:90) 1 (cid:19)
θk+1 = θk+ T exp ∇ f(xk,s)ds ·∇ Φ(Xk)
t 1+γ t 1+γ x s x1 1
t
Further,thetime-orderexponentialtermcanbesimplifiedasfollowing:
Evans(1983)providesmethodtocalculatep(t)=∇ x(1)efficientlybydefiningtheadjointand
xt
usingthefollowingODEs:
p˙(t)=−∇ f(xk,t)p(t)
x t
p(1)=∇ x(1)=I
x(1)
Sakurai&Napolitano(2020)providesanalternativeviewpointoftheajointODE,theyshowthere
wouldbeaclosedformsolution:
p˙(t)=A(t)p(t)
(cid:20) (cid:90) 1 (cid:21)
p(t)=T exp − A(s)ds p(1)
t
Thus,combinetheclosedformsolutionandtheadjointrepresentation,wefindthat:
(cid:20)(cid:90) 1 (cid:21)
∇ x(1)=T exp ∇ f(xk,s)ds
xt x s
t
GiventhelinearityoftheODEs,multiplyaconstanttotheterminalbychangep(t)=∇ x(1)into
xt
p(t)=∇ x(1)∇ Φ(x ),theconclusionswouldnotchange,wehave:
xt x1 1
(cid:20)(cid:90) 1 (cid:21)
∇ x(1)∇ Φ(x )=T exp ∇ f(xk,s)ds ∇ Φ(x )
xt x1 1 x s x1 1
t
Therefore,ourupdateruleisinfact:
γ
θk+1 = θk+α∇ Φ(Xk)
t 1+γ t xt 1
ThesolutionscanbenaturallygeneralisedtothespaceofRN ifweuseTr(ATB)toreplaceA·B.
18Preprint
C.3 ASYNCHRONOUSSETTING
In practice, as described by equation 15, discretization techniques are employed to simulate the
ordinarydifferentialequations(ODEs)governingboththestatetrajectoryx andthecorresponding
t
cotangentvectorµ . Mostexistingmethods,aswellasthealgorithmpresentedearlier,operateunder
t
asynchronoussetting,wherethenumberoftimestepsforthestatetrajectoryx matchesthenumber
t
ofcontroltermsθ .
t
However,OC-Flowcanbeextendedtoanasynchronousframeworkbyapproximationtoallowgreater
flexibilityinupdatescheduling.
Ratherthanemployingthestandardupdaterulex =x +f(t,x ,θ )∆t,wesubdividethetime
t+∆t t t t
interval∆tintoN equallyspacedsubintervals,applyingthecontroltermθ onlyduringthefirst
t
subinterval. Theupdateforx isgivenby:
t+∆t
∆t ∆t ∆t
x =x + f(x ,θ )+ fp(xθ )+···+ fp(xθ ) (21)
t+∆t t N t t N t+∆t N (N−1)∆t
N N
Moreover,forintermediatesteps,wedefine:
i−1
∆t (cid:88)∆t
x =x + f(x ,θ )+ fp(xθ ),
t+i N∆t t N t t N t+l N∆t
l=1
wherex denotesthestateatthei-thsubinterval.
t+i∆t
N
Recallthatf(x ,θ )=fp(x )+θ ,whenwehave ∆t issmallenough,theupdateruleinequation7
t t t t N
canbeapproximatedasacaseinEquation4byconsidering∇ x :
θ t+∆t
∆t ∆t ∆t
∇ x = + ∇ fp(xθ )+···+ ∇ fp(xθ )
θ t+∆t N N θ t+∆ Nt N θ t+(N− N1)∆t
∆t ∆t ∆t
= +( )2∇ fp(xθ )+···+( )N∇ fp(xθ )
N N x t+∆ Nt N x t+(N− N1)∆t
(cid:18) (cid:19)
∆t ∆t
= +O ( )2
N N
Therefore,ifwedenotex asthetrajectoryofthestatevariablexoverthetimeinterval[t,t+∆t],
t
andxθasthetrajectorywhenthecontroltermθ isappliedinthefirstsubinterval,itcanbereasonably
t t
approximatedas:
i−1
∆t(cid:88) ∆t
x ≈x + fp(x )+ θ .
t+∆t t N t+l N∆t N t
l=1
Asaresult,forasynchronoussetting,thestep4inalgorithm1shouldbemodifiedas:
(cid:32) i−1 (cid:33)
Xθk = 1 (cid:88) fp(x )+ 1 θk ∆t
t+∆t N t+l N∆t N
l=1
For the case on the SO(3) manifold, the asynchronous setting can be deployed using the Taylor
expansionofthematrixexponentialexp(A),andnotingthatwhen ∆t and∆taresufficientlysmall,
N
thetermsbecomecommutative. Wecanderivetheapproximationasfollows:
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
∆t ∆t ∆t
x =x exp f(x ,θ ) exp fp(xθ ) ···exp fp(xθ )
t+∆t t N t t N t+∆t N (N−1)∆t
N N
(cid:32) i−1 (cid:33)
∆t(cid:88) ∆t
≈x exp fp(x )+ θ .
t N t+l N∆t N t
l=1
(cid:18) (cid:19)
1
≈x exp ∆t(fp(x )+ θ ) .
t e t N t
Thevector-Jacobianmethodcanalsobeappliedtocomputetheterm ∂ ∂f xtp (xk tE j)inAlgorithm2:
∂fp (cid:18) ∂fp (cid:19)
⟨µ˜ , t xkE ⟩=Tr µ˜T t (xkE ) .
t ∂x t j t ∂x t j
19Preprint
C.4 PROOFFORPROPOSITION3
Thekeyinequalityweusedfortheproofofproposition3isfollowing:
⟨h,v⟩≤∥h∥∥v∥ ∥x∥=1
forh,v ∈so(3)andx∈SO(3)Beforeprovetheproposition3,wefirstlyshowthatthecotangent
vector{µ }isalsobounded.
t
Lemma2AssumeallfunctionssatisfyLipschitzcondition. Then,thereexistsaconstantK′⟩0such
thatforanyθ,
∥µθ∥≤K′,
t
forallt∈[0,T].
ProofUsingnecessaryconditionandsettingτ :=T −t,µ˜θ :=µθ weget
τ T−τ
µ˜˙θ =ad∗ µ˜θ +(dL )∗(∇ ⟨µ˜θ,f⟩) µ˜θ =(dL )∗∇Φ(xθ)
τ ∂H τ x x τ 0 xθ T
∂µ T
WithLipschitzcondition,wehave∥µ˜θ∥ ≤ ∥∇Φ(xθ)∥∥xθ∥ ≤ K and∥∇ f(t,xθ,θ )∥ ≤ K and
0 T T x t t
∥ad∗ µ∥≤∥∂H∥∥µ∥≤K∥µ∥. Hence,
∂H ∂µ
∂µ
∥µ˜˙θ∥≤K∥µ˜θ∥,
τ τ
and
(cid:90) t (cid:90) t
∥µ˜θ∥−∥µ˜θ∥≤∥µ˜θ −µ˜θ∥≤ ∥µ˜˙θ∥ds≤ (K∥µ˜θ∥)ds
τ 0 τ 0 s τ
0 0
andbyGronwall’sinequality,
∥µ˜θ∥≤∥µ˜θ∥eKT =:K′.
τ 0
Thisprovestheclaimsinceitholdsforanyτ.
Nowgivenallrelatedtermsarebounded,wecanprovetheproposition2.
Proposition3Assumethattherewardfunction,thepriormodel,andtheirderivativessatisfyLipschitz
continuity,boundedbyaLipschitzconstantL. Then,thereexistsaconstantC⟩0suchthatforany
θ,ϕ∈so(3),thefollowinginequalityholds:
(cid:90) 1
J(θ)+ ∆ H(t)dt−C||ϕ −θ ||2dt≤J(ϕ), (22)
ϕ,θ t t
0
whereXθ andPθ satisfythePMPconditionsinequation19,and∆H denotesthechangeinthe
ϕ,θ
Hamiltonian,definedas:
∆H (t):=H(t,xθ,µθ,ϕ )−H(t,xθ,µθ,θ ).
ϕ,θ t t t t t t
Proof: Firstly,bythedefinitionofHamiltoninaandPMPconditions,wealwayshave:
(cid:90) T
I(xθ,µθ,θ):= ⟨µθ,fθ⟩−H(t,xθ,µθ,θ)−L(θ)dt≡0
t t t t t
0
Defineδµ =µϕ−µθ andδf =fϕ−fθ,thedifferenceinI canbedecomposedas:
t t t t t t
0≡I(xϕ,µϕ,ϕ)−I(xθ,µθ,θ)
t t
(cid:90) T
= ⟨µθ,δf ⟩+⟨δµ ,fθ⟩+⟨δµ ,δf ⟩dt
t t t t t t
0
(cid:90) T
− H(t,xϕ,µϕ,ϕ)−H(t,xθ,µθ,θ)dt
t t t t
0
(cid:90) T
− L(ϕ )−L(θ )dt
t t
0
20Preprint
NowdefineU(t)=(cid:82)t f ds=log(x x−1)andbyintegratingbyparts:
0 s t 0
(cid:90) T (cid:90) T
⟨µθ,δf ⟩dt=⟨µθ,δU ⟩|T − ⟨µ˙θ,δU ⟩dt
t t t t 0 t t
0 0
(cid:90) T (cid:90) T
⟨δµ ,δf ⟩dt=⟨δµ ,δU ⟩|T − ⟨δµ˙ ,δU ⟩dt
t t t t 0 t t
0 0
Nowwehave:
(cid:90) T
⟨µθ,δf ⟩+⟨δµ ,fθ⟩dt
t t t t
0
(cid:90) T ∂Hθ ∂Hθ
=⟨µθ,δU ⟩|T + ⟨δµ , ⟩+⟨µθ,ad δf ⟩+⟨ ,δf xθ⟩dt
t t 0 t ∂µ t ∂Hθ t ∂x t t
0 ∂µ
Similarly,weget:
(cid:90) T 1(cid:90) T 1(cid:90) T
⟨δµ ,δf ⟩dt= ⟨δµ ,δf ⟩dt+ ⟨δµ ,δf ⟩dt
t t 2 t t 2 t t
0 0 0
1 1(cid:90) T 1(cid:90) T
= ⟨δµ ,δU ⟩|T − ⟨δµ˙ ,δU ⟩dt+ ⟨δµ ,δf ⟩dt
2 t t 0 2 t t 2 t t
0 0
1 1(cid:90) T
= ⟨δµ ,δU ⟩|T + ⟨µϕ,ad δU ⟩−⟨µθ,ad δU ⟩dt
2 t t 0 2 t ∂Hϕ t t ∂Hθ t
0 ∂µ ∂µ
1(cid:90) T ∂Hϕ ∂Hθ
+ ⟨(dL )∗ −(dL )∗ ,δU ⟩dt
2 0 xϕ t ∂x xθ t ∂x t
1(cid:90) T ∂Hϕ ∂Hθ
+ ⟨δµ , − ⟩dt
2 t ∂µ ∂µ
0
Withmeanvaluetheoremandx,µareboundedbyconstantL,wecanalwaysfindxγ betweenxϕ
t t
andxθ,µγ betweenµϕandµθ,γ betweenϕandθ,sothat:
t t t t
(cid:90) T ∂ ∂
⟨(dL )∗ H(t,xϕ,µϕ,ϕ)−(dL )∗ H(t,xθ,µθ,θ),δU ⟩dt
0 xϕ t ∂x t t xθ t ∂x t t t
(cid:90) T ∂ ∂
= ⟨(dL )∗ H(t,xθ,µθ,ϕ)−(dL )∗ H(t,xθ,µθ,θ),δU ⟩dt
xθ t ∂x t t xθ t ∂x t t t
0
(cid:90) T
+ ⟨∇ x((dL xγ)∗)xγ tδU t∇ xH(t,xϕ t,µϕ t,ϕ),δU t⟩dt
t
0
(cid:90) T
+ ⟨(dL )∗∇2H(t,xγ,µϕ,ϕ)xγδU ,δU ⟩dt
xθ x t t t t t
t
0
(cid:90) T
+ ⟨(dL )∗∇ ∇ H(t,xθ,µγ,ϕ)δµ ,δU ⟩dt
xθ µ x t t t t
t
0
(cid:90) T ∂ ∂
≤ ⟨(dL )∗ H(t,xθ,µθ,ϕ)−(dL )∗ H(t,xθ,µθ,θ),δU ⟩dt
xθ t ∂x t t xθ t ∂x t t t
0
(cid:90) T
+C ||δU ||2+||δµ ||||δU ||dt
t t t
0
Usingthesamemethodweget:
(cid:90) T 1 1(cid:90) T
⟨δµ ,δf ⟩dt≤ ⟨δµ ,δU ⟩|T + ⟨µϕ,ad δU ⟩−⟨µθ,ad δU ⟩dt
t t 2 t t 0 2 t ∂Hϕ t t ∂Hθ t
0 0 ∂µ ∂µ
1(cid:90) T ∂ ∂
+ ⟨ H(t,xθ,µθ,ϕ)− H(t,xθ,µθ,θ),δU xθ⟩dt
2 ∂x t t ∂x t t t t
0
1(cid:90) T ∂ ∂
+ ⟨ H(t,xθ,µθ,ϕ)− H(t,xθ,µθ,θ),δµ ⟩dt
2 ∂µ t t ∂µ t t t
0
(cid:90) T
+C ||δU ||2+||δµ ||||δU ||dt
t t t
0
21Preprint
Withboundaryconditions:
1 1
⟨µθ+ δµ ,δU ⟩|T =⟨µθ + δµ ,δU ⟩
t 2 t t 0 T 2 T T
1
=⟨(dL )∗∇Φ(xθ),δU ⟩+ ⟨(dL )∗∇Φ(xϕ)−(dL )∗∇Φ(xθ),δU ⟩
xθ t T t 2 xϕ t T xθ t T t
≤Φ(xϕ)−Φ(xθ)+K||δU ||2
T T T
UsingsamemethodtoH(t,xϕ,µϕ,ϕ)−H(t,xθ,µθ,θ),Thusweobtain:
t t t t
(cid:90) T (cid:90) T
[Φ(xθ)+ L(θ )]−[Φ(xϕ)+ L(ϕ )]
T t T t
0 0
(cid:90) T 1(cid:90) T
≤K||δU ||2− ∆H (t)dt+ ⟨µϕ,ad δU ⟩−⟨µθ,ad δU ⟩dt
T ϕ,θ 2 t ∂Hϕ t t ∂Hθ t
0 0 ∂µ ∂µ
1(cid:90) T ∂ ∂
+ ⟨ H(t,xθ,µθ,ϕ)− H(t,xθ,µθ,θ),xθδU ⟩dt
2 ∂x t t ∂x t t t t
0
1(cid:90) T ∂ ∂
+ ⟨ H(t,xθ,µθ,ϕ)− H(t,xθ,µθ,θ),δµ ⟩dt
2 ∂µ t t ∂µ t t t
0
(cid:90) T
+C ||δU ||2+||δµ ||||δU ||dt
t t t
0
Bydefinition:
(cid:90) T
δU = f(t,xϕ,ϕ)−f(t,xθ,θ)dt
t T T
0
andso
(cid:90) t
||δU ||≤ ||f(s,xϕ,ϕ)−f(s,xθ,θ)||ds
t s s
0
(cid:90) t
≤ ||f(s,xϕ,ϕ)−f(s,xθ,ϕ)||ds
s s
0
(cid:90) t
+ ||f(s,xθ,ϕ)−f(s,xθ,θ)||ds
s s
0
(cid:90) T
≤ ||f(s,xθ,ϕ)−f(s,xθ,θ)||ds
s s
0
(cid:90) t
+K ||δU ||ds
s
0
ByGronwall’sinequality:
(cid:90) T
||δU ||≤eKT ||f(s,xθ,ϕ)−f(s,xθ,θ)||ds
t s s
0
Toestimateδµ,weusethesamesubstitutionasinLemma6withτ =T −t,weget:
(cid:90) τ
δµ˜ =δµ˜ + (dL )∗∇ H(s,x˜ϕ,µ˜ϕ,ϕ)−(dL )∗∇ H(s,x˜θ,µ˜θ,θ)ds
τ 0
0
x˜ϕ
s
x s s x˜θ
s
x s s
(cid:90) τ
+ ad∗ µ˜ϕ−ad∗ µ˜θds
∂H τ ∂H τ
0 ∂µ ∂µ
22Preprint
UsingLemma1andLiptichitzconditions:
(cid:90) τ
||δµ˜||≤||δµ˜||+ ||(dL )∗∇ H(s,x˜ϕ,µ˜ϕ,ϕ)−(dL )∗∇ H(s,x˜θ,µ˜θ,θ)||ds
τ 0
0
x˜ϕ
s
x s s x˜θ
s
x s s
(cid:90) τ
+ ||ad∗ µ˜ϕ−ad∗ µ˜θ||ds
∂H τ ∂H τ
0 ∂µ ∂µ
(cid:90) T (cid:90) τ
≤K||δU ||+KK′ ||δU ||dt+K ||δµ˜ ||ds
T t s
0 0
(cid:90) T
+ ||(dL )∗∇ H(s,x˜θ,µ˜θ,ϕ)−(dL )∗∇ H(s,x˜θ,µ˜θ,θ)||ds
x˜θ x s s x˜θ x s s
s s
0
(cid:90) T
≤eKTK(||δU ||+K′ ||δU ||dt)
T t
0
(cid:90) T
+eKTK ||∇ H(s,x˜θ,µ˜θ,ϕ)−∇ H(s,x˜θ,µ˜θ,θ)||ds
x s s x s s
0
UsingtheboundofU ,weobtain:
t
(cid:90) T
||δµ˜||≤K′′( ||f(s,xθ,ϕ)−f(s,xθ,θ)||ds)
τ s s
0
(cid:90) T
+eKTK ||∇ H(s,x˜θ,µ˜θ,ϕ)−∇ H(s,x˜θ,µ˜θ,θ)||ds
x s s x s s
0
Alsoweobtain:
1(cid:90) T
⟨µϕ,ad δU ⟩−⟨µθ,ad δU ⟩dt
2 t ∂Hϕ t t ∂Hθ t
0 ∂µ ∂µ
1(cid:90) T
= ⟨ad∗ µϕ−ad∗ µθ,δU ⟩dt
2 ∂Hθ t ∂Hθ t t
0 ∂µ ∂µ
(cid:90) T
≤C ||δµ ||||δU ||dt
t t
0
Finallyweget:
23Preprint
(cid:90) T
J(θ)−J(ϕ)≤− ∆H (t)dt
ϕ,θ
0
1
+ K′′∥δU ∥2
2 T
(cid:90) T
+K′′ (cid:0) ∥δU ∥2+∥δµ ∥2(cid:1) dt
t t
0
1(cid:90) T
+ ∥δµ ∥∥f(t,xθ,ϕ )−f(t,xθ,θ )∥dt
2 t t t t t
0
1(cid:90) T
+ ∥δU ∥∥∇ H(t,xθ,µθ,ϕ )−∇ H(t,xθ,µθ,θ )∥dt
2 t x t t t x t t t
0
(cid:90) T
≤− ∆H (t)dt
ϕ,θ
0
(cid:32) (cid:33)2
(cid:90) T
+C ∥f(t,xθ,ϕ )−f(t,xθ,θ )∥dt
t t t t
0
(cid:32) (cid:33)2
(cid:90) T
+C ∥∇ H(t,xθ,µθ,ϕ )−∇ H(t,xθ,µθ,θ )∥2dt
x t t t x t t t
0
(cid:90) T
≤− ∆H (t)dt
ϕ,θ
0
(cid:90) T
+C ∥f(t,xθ,ϕ )−f(t,xθ,θ )∥2dt
t t t t
0
(cid:90) T
+C ∥∇ H(t,xθ,µθ,ϕ )−∇ H(t,xθ,µθ,θ )∥2dt
x t t t x t t t
0
Therefore,giventheformoftheaddictivecontroltermsandtherunningcost,wecanderivethefinal
termofourclaim:
(cid:90) 1
J(θ)+ ∆ H(t)dt−C||ϕ −θ ||2dt≤J(ϕ), (23)
ϕ,θ t t
0
C.5 PROOFFORPROPOSITION4
DuetothesimilaritybetweentheboundderivedinProposition3andtheboundobtainedfromthe
E-MSAmethod,theproofsofProposition4andTheorem5followthesamereasoningasoutlinedin
Section3.3ofLietal.(2018). Forthesakeofcompleteness,weprovidethefullderivationshere.
Proposition4LetXθ andPθ satisfythePMPconditionsinequation19. Iftheupdaterulefollows
(cid:82)1
equation12,wedefineϵ := ∆ H(t)dt,andϵ isboundedas:
k 0 θk+1,θk k
(cid:90) 1
ϵ := ∆ H(t)dt lim ϵ =0. (24)
k θk+1,θk k
0 k→∞
Furthermore,whenϵ =0,wehaveθ =θ∗ :=argmax J(θ)Toestablishconvergence,define
k θ
(cid:90) T
ϵ := ∆H (t)dt≥0.
k θk+1,θk
0
ProofBydefinition,ifϵ =0,thenfromtheupdaterulewhichmaximizestheHamiltonian,wemust
k
have
γ (cid:90) 1
0=−ϵ ≤− ∥θk+1−θk∥2dt≤0.
k 2
0
andso
maxH˜(xθk ,µθk ,θ,x˙θk ,µ˙θk )=H˜(xθk ,µθk ,θk,x˙θk ,µ˙θk
),
t t t t t t t t t
θ
24Preprint
Therefore,wealwayshavethequantityϵ ≥0anditmeasuresthedistancefromtheoptimalsolution,
k
andifitequals0,thenwereachtheoptimum.
C.6 PROOFFORTHEOREM5
Theorem5: Assumethattherewardfunction,thepriormodel,andtheirderivativessatisfyLipschitz
continuity,boundedbyaLipschitzconstantL. Letθ0 ∈so(3)beanyinitialmeasurablecontrolwith
J(θ0)<+∞. Supposealsothatinf J(θ)>−∞. Iftheupdateofθsatisfiesequation12,for
θ∈so(3)
sufficientlylargeγ,thefollowinginequalityholds:
Dϵ ≤J(θk+1)−J(θk) (25)
k
forsomeconstantD >0
ProofUsingProposition3wehave
(cid:90) T
J(θk)−J(θk+1)≤−ϵ +C ∥θk+1−θk∥2dt
k
0
FromtheAlgorithm2maximazaingstep,weknowthat
γ
H(t,Xθk ,Pθk ,θk)≤H(t,Xθk ,Pθk ,θk+1)− ∥θk+1−θk∥2
t t t t t t 2
Hence,wehave
2C
J(θk)−J(θk+1)≤−(1− )ϵ .
γ k
Pickγ >2C,thenweshallhaveJ(θk)−J(θk+1)≤−Dϵ withD =(1− 2C)>0.
k γ
Moreover,wecanrearrangeandsumtheaboveexpressiontoget
M (cid:18) (cid:19)
(cid:88) ϵ ≤D−1(cid:0) J(θM+1)−J(θ0)(cid:1) ≤D−1 inf J(θ)−J(θ0) ,
k
θ∈U
k=0
andhence(cid:80)∞
ϵ <+∞,whichimpliesϵ →0andthealgorithmconvergestotheoptimum.
k=0 k k
D EXPERIMENTAL DETAILS
D.1 TEXT-GUIDEDIMAGEMANIPULATION
Inourtext-to-imagegenerationexperiment,weadoptedthepipelinepresentedinLiuetal.(2023),
utilizingthegenerativepriorfromLiuetal.(2022). Weemployedstandardevaluationmetrics:LPIPS
andID(faceidentitysimilarity)asintroducedinKim&Ye(2021)toassessthedifferencesbetween
theoriginalimageandthemanipulatedimage. Additionally,theCLIPscorewasusedtoevaluatethe
alignmentbetweenthegeneratedimageandtheprovidedtextprompt.
Toenforceconsistencywiththeoriginalimage,weintroducedaconstrainttermtotheterminalreward
functiontopenalizesignificantdeviationsfromtheoriginalimage:
Φ(x )=λCLIP(x ,T)−(1−η)|x −xp|
1 1 1 1
Here,thehyperparameterλwassetto0.7acrossallexperiments,andtheEulerdiscretizationstep
wassettoN =100andthenumberofoptimizationiterationsM =15. AsdiscussedinTheorem2,
increasingthelearningrateηresultsingreateremphasisontheterminalreward,leadingtoahigher
CLIPscorebutlowerLPIPSandIDscores. Theweightdecayisafunctionofγ,whichistunedto
(cid:16) (cid:17)
maximize 1− 2C ϵk foriterationk. Inthisexperiment,duetothelimitationofstorage,wesetγ
γ γ
thesameforallk. Inourimplementation,thelearningrateηwassetto2.5,andtheweightdecayβ
wassetto0.995.
BaselineconfigurationswerealignedwiththosereportedinLiuetal.(2023),andtheresultspresented
inTable2reflectthesameexperimentalconditions. Forquantitativecomparison,weusedtheCelebA
dataset,randomlysampling1,000images,whichweremanipulatedbasedontextguidance: {old,sad,
smiling,angry,curlyhair}.
25Preprint
D.2 MOLECULEGENERATION
In our QM9 generation experiment, we mostly followed the conditional generation pipeline in
Hoogeboometal.(2022). AnequivariantgeometricGNNwastrainedforeachpropertyonhalfof
theQM9dataastheclassifier,whichwasthenfrozenduringourtraining-freecontrolledgeneration.
TheEquiFM(Songetal.,2024)checkpoint,trainedonthewholeQM9trainingdata,wasloadedas
thegenerativeprior. Thetesttimepropertiesweresampledfromthewholetrainingdataset,making
itslightlydifferentfromthesettingsinBen-Hamuetal.(2024). Therefore,wereimplementedthe
D-Flowalgorithmwith5optimizerstepsand5innerstepseachwithalinearsearchusingtheL-BFGS
optimizer. TheresultsroughlymatchedthosereportedintheD-FlowpaperwithslightlyworseMAEs
asweincludedthewholetrainingdatasetforpropertysampling. OurproposedOC-Flowalsoused
thesameoptimizationhyperparametersandalsoalmostthesamerunningtimeastheD-Flow. For
FlowGrad, we followed the suggestion in the original paper to use 20 SGD steps to update the
learnableparts,whichranslightlyfasterthanOC-FlowandD-Flow.
Forallproperties,MAEwasusedastheoptimizationtargetandγ istheregularizationcoefficient
suchthatγ(cid:82)1 ∥θ ∥2dtistheadditionalOCloss. Foralloptimizationmethods,wealwaysuseda
0 t
fixednumberof50Eulerstepssoθcanbeindexedbydiscreteindices. Astheintegralisdonewitha
stepsizeof1/50,anyγ iseffectivelyγ˜ =2γ/50=0.04γ whentakingthederivativewithrespect
toθorx. Therefore,γ =10effectivelycorrespondstoγ˜ =0.4,whichisstillavalidoptimization
scheme.
D.3 PEPTIDEDESIGN
Inourpeptideexperiments,weadoptedPepFlow(Lietal.,2024)asthebaselinemodel,utilizingthe
pre-trainedcheckpointprovidedintheoriginalPepFlowpaper. Thetestdatasetsplitwasalsobased
ontheonedefinedinthePepFlowframework. Forhyperparametertuning,werandomlyselected
10 samples from the dataset. After tuning, the full set of 162 samples was used for training and
evaluationtoensurecomprehensiveperformanceassessment.
To achieve flexible update scheduling, we employed an asynchronous setting in OC-Flow. This
approachmaintainsthesametimestepasPepFlowwhileprovidingfinerupdatestobothcontrolterms
andstatetrajectories. Thetotalsimulationwasconductedwith10intervals,eachfurtherdividedinto
20subintervals. Thisdesignenablesimprovedflexibilityandfinergranularityinupdateswithout
increasingthenumberofoveralltimesteps.
Weappliedthepre-trainedmodelastheinitializationforourexperiments,allowingustobuildupon
thepre-trainedweightsandachieveconsistentperformanceimprovementsthroughhyperparameter
adjustments. Thefinalhyperparameterswereselectedbasedonoptimalperformanceduringtuning
and were applied across the full dataset. In OC-Flow(rot), we used α = 0.95 and β = 0.8; in
OC-Flow(trans),α=0.9andβ =1.2;andinOC-Flow(both),α=0.95andβ =2.0.
26