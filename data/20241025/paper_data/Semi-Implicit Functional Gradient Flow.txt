Semi-Implicit Functional Gradient Flow
∗† ∗‡ §
Shiyue Zhang Ziheng Cheng Cheng Zhang
Abstract
Particle-based variational inference methods (ParVIs) use non-parametric variational families
represented by particles to approximate the target distribution according to the kernelized
Wasserstein gradient flow for the Kullback-Leibler (KL) divergence. Recent works introduce
functional gradient flows to substitute the kernel for better flexibility. However, the deterministic
updating mechanism may suffer from limited exploration and require expensive repetitive runs
for new samples. In this paper, we propose Semi-Implicit Functional Gradient flow (SIFG), a
functional gradient ParVI method that uses perturbed particles as the approximation family.
Thecorrespondingfunctionalgradientflow, whichcanbeestimatedviadenoisingscorematching,
exhibits strong theoretical convergence guarantee. We also present an adaptive version of our
methodtoautomaticallychoosethesuitablenoisemagnitude. Extensiveexperimentsdemonstrate
the effectiveness and efficiency of the proposed framework on both simulated and real data
problems.
1 Introduction
Bayesian inference aims to efficiently estimate and sample from unnormalized posterior distribu-
tion to model complex data and reason under uncertainty, which is a fundamental and challenging
task in modern machine learning. Typical methods including Markov Chain Monte Carlo (MCMC)
and Variational Inference (VI), have been developed to address the intractability of the target
distribution. By reformulating the inference problem into an optimization problem, VI seeks to
find an approximation within a specific distribution family that minimizes the Kullback-Leibler
(KL) divergence to the posterior [Jordan et al., 1999; Wainwright and Jordan, 2008; Blei et al.,
∗
Equal contribution.
†
Peking University. Email: zhangshiyue@stu.pku.edu.cn
‡
University of California, Berkeley. Email: ziheng cheng@berkeley.edu
§
Peking University. Email: chengzhang@math.pku.edu.cn
1
4202
tcO
32
]LM.tats[
1v53971.0142:viXra2016]. Though VI allows fast training and feasible scaling to large datasets when equipped with
efficient optimization algorithms, the main restriction is that the choice of variational distribution
family may limit its approximation power. On the other hand, MCMC simulates a Markov chain to
directly draw samples from the posterior [Duane et al., 1987; Robert and Stramer, 2002; Neal, 2011;
Welling and Teh, 2011; Chen et al., 2014]. While being asymptotically unbiased, MCMC often takes
a long time to converge, and it is also difficult to access the convergence.
Recently, there has been a growing interest in the gradient flow formulation of both MCMC and
VI, leading to the development of particle-based variational inference methods (ParVIs) that tend to
combine the best of both worlds [Liu and Wang, 2016; Chen et al., 2018; Liu et al., 2019; di Langosco
et al., 2021; Fan et al., 2022; Alvarez-Melis et al., 2022]. In ParVIs, the approximating distribution
is represented by a set of particles, which are iteratively updated by minimizing the KL divergence
to the posterior according to the gradient flow. This non-parametric nature significantly improves
the flexibility of ParVIs upon classical VIs, and the interaction between particles also makes ParVIs
more particle-efficient than MCMCs. One of the most prominent particle based VI method is Stein
Variational Gradient Descent (SVGD) [Liu and Wang, 2016]. It updates the particles by simulating
the gradient flows of the KL divergence within a reproducing kernel Hilbert space (RKHS), where
the gradient flows have a tractable form [Liu, 2017; Chewi et al., 2020]. However, the performance of
SVGD heavily relies on the choice of the kernel function and the quadratic computational complexity
of the kernel matrix also hinders practical usage of a large number of particles.
To address these issues, functional gradient flow methods have been proposed to expand the
function class for gradient flow approximation [Hu et al., 2018; Grathwohl et al., 2020; di Langosco
et al., 2021; Dong et al., 2023; Cheng et al., 2023]. By leveraging the more general neural network
induced functional gradients, these approaches have presented improved performance over vanilla
SVGDwhilenotrequiringexpensivekernelcomputation. However,duetothedeterministicupdating
mechanism, current approaches often struggle with limited exploration and can encounter mode
collapse when dealing with non-convex multi-modal distributions. Additionally, multiple runs are
required to generate different sets of samples, which adds to the computational burden.
In this work, we propose Semi-Implicit Functional Gradient flow (SIFG), a novel ParVI method
that leverages a semi-implicit variational family, represented by perturbed particles, to approximate
the target distribution. The injected Gaussian noise enhances the algorithm’s exploration, and
the corresponding Wasserstein gradient flow is efficiently estimated via denoising score matching,
which scales well in high-dimensional settings. Additionally, this added stochasticity enables the
2generation of diverse samples on the fly, addressing a common limitation of ParVI methods due
to their deterministic nature. We provide a theoretical convergence guarantee of SIFG, which is a
non-trivial generalization of the convergence theory of variational inference with mixture gaussians
withfixedvariance[Huixetal.,2024]. Wealsoproposeanadaptiveprocedurethatcanautomatically
adjust the noise magnitude, balancing between sample accuracy and diversity. Extensive numerical
experiments on both simulated and real data sets are conducted to demonstrate the advantage of
our method over existing ParVI methods.
2 Related Works
Existing functional gradient methods for ParVIs mainly focus on designing more expressive
function space for the velocity field in gradient flow dynamics than the Reproducing Kernel Hilbert
Space (RKHS). di Langosco et al. [2021] proposed to use neural networks rather than kernels to
approximate the optimal velocity with L regularization. Dong et al. [2023] and Cheng et al. [2023]
2
generalize the regularizer to quadratic and L functions. While functional gradient flow methods
p
enjoy more flexibility because of the expansion from RKHS to neural networks, the updating
mechanisms therein remain deterministic. This may lead to lack of exploration ability and sample
diversity: the particles may get stuck at local modes for complex distributions, and only a single set
of samples could be obtained from one run.
Previous works explore the possibilities of utilizing broader variational family for variational
inference, including gaussian-based variational inference and incorporating semi-implicit variational
family with particle gradient flows. Lambert et al. [2022] and Diao et al. [2023] proposed convergence
analysis for non-degenerate gaussian distribution family. Furthermore, Huix et al. [2024] presented
the theoretical analysis for gaussian mixture family. In this work, we generalize and improve the
convergence theory from gaussian mixture family to the general semi-implicit family. Lim and
Johansen [2024] proposed to assist Semi-Implicit Variational Inference (SIVI) with particle guidance
by alternatively updating the variational parameters and the particles representing the semi-implicit
distribution through minimizing the regularized KL objective. The simulation of particles requires
a McKean–Vlasov SDE which is similar to the Langevin diffusion. As a functional gradient method,
our method do not use Langevin type of particle updating, and selecting the noise to be Gaussian
enables us to directly use the KL divergence as the objective.
33 Background
Notations Throughout this paper, we use z ∈ Rd to denote the particle samples before pertur-
bation, and we use x ∈ Rd to denote the perturbed particle samples. Let P(Rd) denote all the
probability distributions on Rd that are absolute continuous with respect to the Lebesgue measure.
We do not distinguish a probabilistic measure with its density function. Let ∥·∥ be the standard
Euclideannormofavectorandtheoperatornormofamatrixorhigh-dimensionaltensor. Wedenote
the inner product in Rd (or L2(Rd)) and a certain Hilbert space H by ⟨·,·⟩ and ⟨·,·⟩ , respectively.
H
δF δF
We use to denote the first variation of the functional F, and we use ∇ F := ∇ to denote
δµ
W2
δµ
the Wasserstein-2 gradient of F.
3.1 Functional Gradient Flow
Let π be the target distribution we wish to sample from. Particle-based variational inference
methods aim to minimize the KL divergence between the distribution µ represented by a set of
particles and the target distribution π,
µ∗ := argminD (µ∥π), (3.1)
KL
µ∈Q
where Q is the space of probability measures. Starting from the initial distribution µ and the
0
initial particle z ∼ µ , we update the particle z following dz = v (z )dt, where v is the velocity
0 0 t t t t t
field at time t. The distribution µ of z follows the continuity equation ∂ µ +∇·(µ v ) = 0, and
t t t t t t
the KL divergence decreases with the following rate:
d π
D (µ ∥π) = −E ⟨∇log ,v ⟩. (3.2)
dt
KL t µt
µ
t
t
The optimal v in a Hilbert space H is the minimizer of the following objective:
t
π 1
min−E ⟨∇log ,v ⟩+ ∥v ∥2 . (3.3)
vt∈H µt µ t t 2 t H
SVGD [Liu and Wang, 2016] chooses H to be the Reproducing Kernel Hilbert Space with kernel
k(·,·), and the optimal velocity field takes the following form
v∗(·) = E [k(·,x)∇logπ(x)+∇ k(·,x)], (3.4)
t µ x
Functional gradient flow methods define H to be L2(µ ) and parameterize v as a neural network.
t t
By Stein’s identity, objective (3.3) has a tractable form
(cid:20) (cid:21)
1
v∗ = argminE −⟨∇logπ,v⟩−∇·v+ ∥v∥2 , (3.5)
t µt 2
v∈F
where F is the neural network family.
43.2 Semi-implicit Variational Inference
In SIVI, the variational family takes a hierarchical architecture:
(cid:90)
x|z ∼ q (x|z),z ∼ µ (z),µ (x) = q (x|z)µ (z)dz (3.6)
ϕ ξ ϕ ϕ ξ
where ϕ,ξ are model parameters. Variants of semi-implicit variational inference fit the model
parameters in different ways, including maximizing the surrogate evidence lower bound (ELBO)
[Yin and Zhou, 2018], using unbiased gradient estimator of the exact ELBO [Titsias and Ruiz, 2019],
minimizing Fisher divergence [Yu and Zhang, 2023] and minimizing KSD [Cheng et al., 2024].
4 Proposed Method
4.1 Semi-implicit Particle-based Variational Family
Supposeµ isthecurrentparticledistributionattimet. Inspiredfromthesemi-implicitstructure,
t
we utilize the semi-implicit version of the current distribution µ by injecting noise to the current
t
particles. In this paper we focus on isotropic gaussian noise with variance σ2. This leads to the
following semi-implicit variational distribution µˆ
t
(cid:90)
µˆ (x) = q (x|z)dµ (z) (4.1)
t σ t
where q σ(x|z) ∝
e−∥x 2− σz 2∥2
is a gaussian transition kernel. To sample from the target distribution
using the semi-implicit family, we turn to minimize the KL divergence between µˆ and π.
t
Following common practice in variational inference, we consider the KL divergence between the
perturbed density µˆ and the target distribution π.
t
(cid:90)
µˆ (x)
t
F(µˆ ) := D (µˆ ∥π) = log dµˆ (x) (4.2)
t KL t t
π(x)
4.2 Functional Gradient Flow
We construct a Wasserstein gradient flow that minimizes the above KL divergence. Consider
the dynamics of the original particles
d
z = v (z ), z ∼ µ , (4.3)
t t t t t
dt
The continuity equation is
dµ
t
+∇·(µ v ) = 0, (4.4)
t t
dt
5L 2-GF Iter:2000 Ada-GWG Iter:2000 Ada-SIFG Iter:2000 Groundtruth
4 4 4 4
2 2 2 2
0 0 0 0
2 2 2 2
4 4 4 4
4 2 0 2 4 4 2 0 2 4 4 2 0 2 4 4 2 0 2 4
Figure 1: Comparison of sampled particles of different methods at 2000 iterations (sufficient for all
methods to converge) against the ground truth samples on the 2D gaussian mixture distribution.
By the chain rule, the time derivative of the KL divergence is
dF δ dµˆ
t
= ⟨ F(µˆ), ⟩ (4.5)
t
dt δµˆ dt
t
(cid:90)
µˆ (x) dµ (z)
t t
= ⟨log , q (x|z) ⟩ (4.6)
σ
π(x) dt
Plugging in the continuity equation gives
(cid:90) (cid:90)
dF µˆ (x) dµ (z)
t t
= log q (x|z) dx (4.7)
σ
dt π(x) dt
(cid:90) (cid:90)
µˆ (x)
t
=− log q (x|z)∇ ·(µ (z)v (z))dzdx (4.8)
σ z t t
π(x)
(cid:90) (cid:90)
µˆ (x)
t
= log ∇ q (x|z)µ (z)v (z)dzdx (4.9)
z σ t t
π(x)
(cid:90) (cid:20)(cid:90) (cid:21)
µˆ (x)
t
= log ∇ q (x|z)dx v (z)µ (z)dz (4.10)
z σ t t
π(x)
d
The dissipation property of the Wasserstein gradient flow states that F = −E ∥∇ F∥2, and
dt
µt W2
the optimal velocity v∗ is the negative of the Wasserstein gradient [Chewi et al., 2020]. From (4.10)
t
we have
v∗(z) =−∇ F(µ )(z) (4.11)
t W2 t
(cid:90)
µˆ (x)
t
=− log ∇ q (x|z)dx (4.12)
z σ
π(x)
(cid:90)
µˆ (x)
t
= log ∇ q (x|z)dx (4.13)
x σ
π(x)
(cid:90)
π(x)
= ∇log q (x|z)dx (4.14)
σ
µˆ (x)
t
π(x)
=E ∇log (4.15)
qσ(x|z) µˆ (x)
t
6where the equation of (4.13) is guaranteed by the symmetry of the gaussian transition kernel.
Following the framework of functional gradient flow, we use a neural network f to approximate
γ
π(x)
∇log given perturbed samples x. Previous works utilize Hutchinson estimation of ∇·f to
γ
µˆ (x)
t
approximate this score difference by minimizing the loss function below.
π
L(γ) := E [∥f ∥2−⟨∇log ,f ⟩] (4.16)
µˆt γ 2 µˆ γ
t
= E [∥f ∥2−(∇logπ)Tf −∇·f ] (4.17)
µˆt γ 2 γ γ
However, Hutchinson estimation can have large variance, especially in high dimensions. Note that
we utilized the semi-implicit variational family µˆ , it is feasible to directly use neural network g to
t γ
estimate ∇logµˆ using denoising score matching (DSM) [Vincent, 2011]. by minimizing the loss
t
function below:
L(γ) : = E E [∥g (x)−∇ logq (x|z)∥2] (4.18)
µt(z) qσ(x|z) γ x σ
x−z
= E E [∥g (x)+ ∥2] (4.19)
µt(z) qσ(x|z) γ σ2
Using DSM improves the accuracy and efficiency of computing the training objective, further
demonstrating the advantages of semi-implicit particle-based variational families for functional
gradient computation. We call our method Semi-Implicit Functional Gradient Flow (SIFG), and
summarize the overall sampling procedures in Algorithm 1.
4.3 Adaptively Choosing the Noise Magnitude
SIFG introduces an additional tuning parameter σ for balancing between sample accuracy and
diversity. Increasing σ may improve the diversity of sample outputs, but at the same time it may
also reduce the sample approximation accuracy. If we focus on achieving sample accuracy such that
the KL divergence continues decaying after updating current particles, we can add a simple gradient
descent procedure on σ according to the following gradient estimate
d µˆ (z+σw)
F (µ ) = E ∇log σ,t ·w (4.21)
dσ σ t z∼µt(z),w∼N(0,I) π(z+σw)
≈ E [f (z+σw)−∇logπ(z+σw)]·w (4.22)
z∼µt(z),w∼N(0,I) γ
Please refer to the appendix A for detailed derivation. This way, we can adaptively adjust the
noise level σ on the fly. We call this adaptive version of SIFG, Ada-SIFG. The full procedure of
Ada-SIFG can be found in Algorithm 2 of the appendix A.
7Algorithm 1 SIFG: semi-implicit functional gradient flow
Require: Unnormalizedtargetdistributionπ,initialparticles{zi}n ,initialparameterγ ,iteration
0 i=1 0
number N,N′, particle step size h, parameter step size η, noise magnitude σ.
for k = 0,··· ,N −1 do
Assign γ0 = γ
k k
Obtain perturbed samples xi = zi +ϵi, where ϵi ∼ N(0,σ2)
k k k k
for t = 0,··· ,N′−1 do
Compute
1 (cid:88)n xi −zi
L(cid:98)(γ) =
n
∥f γ(xi k)− k
σ2
k∥2 (4.20)
i=1
Update γ kt+1 = γ kt +η∇ γL(cid:98)(γ kt)
end for
Update γ =
γN′
k+1 k
Update particles zi = zi +h(∇logπ(xi)−f (xi)) for i = 1,··· ,n
k+1 k k γ k+1 k
end for
Obtain perturbed samples xi = zi +ϵi , where ϵi ∼ N(0,σ2)
N N N N
return Particles {xi }n
N i=1
5 Theoretical Analysis
In this section, we state the main theoretical results of Algorithm 1, including convergence
guarantee and sample complexity analysis.
5.1 Optimization Guarantees
We propose the optimization analysis of the gradient flow. Consider the discrete dynamics of
the original particles
z = z +hv (z ), (5.1)
(k+1)h kh k kh
and the interpolation
z = z +thv (z ),for t ∈ [0,1], (5.2)
(k+t)h kh k kh
The optimal velocity is v∗(z) = E [∇logπ(x) − ∇logµˆ (x)], assume neural network
k qσ(x|z) kh
parameterized v (z) = E [∇logπ(x)−f (x)]. Under the neural network approximation [Cheng
k qσ(x|z) γ
8et al., 2023] and bounded assumptions with the moment condition of the particle distribution along
the trajectory, we obtain the descent lemma for the gradient flow.
Assumption 5.1. For any k, E ∥f −∇logµˆ ∥2 ≤ ε .
µˆ kh γ kh 2 k
By triangular inequality, this assumption implies that
E ∥v −v∗∥2 = E ∥E (f −∇logµˆ )∥2 ≤ ε . (5.3)
µ kh k k µ kh qσ(x|z) γ kh k
Assumption 5.2. The score of the target distribution ∇logπ is L-Lipschitz, i.e. for any x,y ∈ Rd,
∥∇logπ(x)−∇logπ(y)∥ ≤ L∥x−y∥.
Assumption 5.3. The norms of the velocity field v is A-Lipschitz, then for any z ∈ Rd, ∥v (z)∥ ≤
k k
A∥z∥+∥v (0)∥ := A∥z∥+B.
k
This assumption of v corresponds to Cheng et al. [2023].
k
(cid:90)
Assumption 5.4. The absolute moments of µ is bounded by m for α ≤ 5. i.e. ∥z∥αdµ (z) ≤
t α t
m < ∞.
α
Now we present the descent lemma of Algorithm 1.
Proposition 5.5. Suppose Assumption 5.1, 5.2, 5.3, 5.4 hold. Then the following inequality holds
1 1 1
for h < : F(µˆ )−F(µˆ ) ≤ − h∥∇ F(µ )∥2 + hϵ +h2[Cm +M]+h3[Cm +M],
A (k+1)h kh 2 W2 kh L2(µ kh) 2 k 3 5
where C,M are constants that do not relate to k or h.
Please refer to appendix B.1 for detailed proof. We generalize the results of Huix et al. [2024],
from n-dirac distribution to general gaussian-based semi-implicit distributions. The dependencies in
Proposition 5.5 does not include the number of modes n which can go to infinity.
Using the above descent lemma, we obtain the convergence of the average of squared gradient
norms.
Theorem 5.6. Assume Proposition 5.5 holds, then the average of squared gradient norms satisfies
1 (cid:88)K
∥∇ F(µ )∥2 ≤
R
+
S
+
1 (cid:88)K
ϵ for K > min{
A2F(µˆ 0)
,
A3F(µˆ 0)
}, where
K
k=1
W2 kh L2(µ kh) K1
2
K2
3
K
k=1
k Cm 3+M Cm 5+M
(cid:112) 2 1
R := 4 F(µˆ 0)(Cm 3+M) and S := 4(F(µˆ 0))3(Cm 5+M)3.
Please refer to appendix B.1 for detailed proof. Taking K → ∞ implies that the average of
squared gradient norms is at the order of the average training error.
95.2 Statistical Guarantees
In this section, we further focus on the sample complexity required to satisfy Assumption 5.1.
Specifically, we establish a statistical guarantee of the score matching for certain neural network
family via empirical risk minimization (ERM).
We formally define the deep ReLU neural network family as S(L,W,M,S,B) := {s(x) =
L
(cid:88)
(A σ(·) + b ) ◦ ··· ◦ (A x + b ) : A ∈ Rdi×di+1,b ∈ Rdi+1,maxd ≤ W, (∥A ∥ + ∥b ∥ ) ≤
L L 1 1 i i i i 0 i 0
i=1
S,max∥A ∥ ∨∥b ∥ ≤ B,sup∥s(x)∥ ≤ M}, where σ(x) = max{x,0}.
i ∞ i ∞ ∞
x
n
1 (cid:88)
Consider ERM fˆ := argmin ℓ(z ;f ), where ℓ(z;f ) := E ∥f (x)−∇logq (x|z)∥2
γ n i γ γ qσ(x|z) γ σ
fγ∈S
i=1
and {z } are i.i.d. samples of µ(z). The population loss is given by ℓ(f ) := E ∥f (x)−
i γ µ(z)qσ(x|z) γ
∇logq (x|z)∥2 = E ∥f (x)−∇logµˆ(x)∥2+c =: ℓ (f )+c ,wherec = E ∥∇logµˆ(x)−
σ µˆ(x) γ ∗ sm γ ∗ ∗ µ(z)q(x|z)
∇logq(x|z)∥2 is a constant independent of f .
γ
Assumption5.7. µ(z)issub-Gaussian,i.e.,thereexistsC ,C > 0suchthatµ(x) ≤ C exp(−C ∥z∥2).
1 2 1 2
Theorem 5.8. Under Assumption 5.7, for any δ > 0, it holds with probability no less than 1−2δ
that,
(cid:115) 
c (M2+ d ) N
ℓ sm(fˆ γ) ≤ 2 inf ℓ sm(f γ)+O ∗ σ2 log . (5.4)
fγ∈S n δ
(cid:0) dn (cid:1)
where logN = SLlog WLBσlog( ) .
δ
PleaserefertoappendixB.2fordetailedproof. Thistheoremsuggestthatwhentheapproximation
1
error of neural network family is negligible, we can get a O(cid:101)(√ )-accurate score estimation with
n
high probability, where n is the particle number. Together with Theorem 5.6, we conclude that the
total sample complexity to achieve ε-accuracy is K ·n = O(cid:101)(1/ε4), exhibiting the sample efficiency
of our methods.
6 Numerical Experiments
In this section, we compare SIFG and Ada-SIFG with other ParVI methods including SVGD
[Liu and Wang, 2016], L -GF [di Langosco et al., 2021] and Ada-GWG [Cheng et al., 2023] on
2
both synthetic and real data problems. In BNN experiments, we also test SGLD [Welling and Teh,
2011]. Through out this section, the initial particle distribution is the standard gaussian distribution
except for the gaussian mixture experiments. For Ada-SIFG, the noise magnitude σ is clipped to be
10Figure 2: The moving trajectory of particles from initial to 2000 iterations of different methods.
The red dot is the initial location, the orange dot is the particle location at 1600 iteration and the
black dot is the location at 2000 iteration. We randomly choose 50 particles for illustration.
larger than 0.001 unless otherwise specified. Please refer to appendix C for more detailed setting
information.
6.1 Gaussian Mixture
Utilizing the semi-implicit type of variational family incorporate the advantages of better
exploration ability and the sample diversity. To visualize the effectiveness of our method, our first
example is sampling from a 2D toy Gaussian mixture distribution. We compare with ParVI methods
L -GF [di Langosco et al., 2021] and Ada-GWG [Cheng et al., 2023] on a 5-mode gaussian mixture
2
distribution which the marginal probability of each cluster is 1/5, and the standard deviations are
0.1,0.2,0.3,0.4,0.5. The initial distribution is the gaussian distribution N((3,0),0.25I). From figure
1 we can see that the samples of L -GF and Ada-GWG both stuck at certain modes at convergence,
2
while SIFG uniformly covers all the modes because of incorporating better exploration ability. To
illustrate the exploration ability and the sample diversity of SIFG compared to other methods, we
randomly choose 50 particles of each methods and trace the moving trajectories of these particles
from iteration 0 to 2000. We also denote the particles in orange at iteration 1600 where all methods
converge in distribution. From figure 2 we can see that the particles of L -GF and Ada-GWG
2
barely move near convergence (from iteration 1600 to 2000), while the particles of SIFG vary due to
perturbation at each training epoch but still preserve the sampling quality.
To further quantitatively compare with other methods, we consider the 5-cluster 10D Gaussian
112.00
L2-GF
10 1.75
Ada-GWG
1.50
8 SIFG( =0.1)
L2-GF 1.25
Ada-SIFG( =0.1)
Ada-GWG
6 1.00
SIFG( =0.1)
Ada-SIFG( =0.1) 0.75
4
0.50
2
0.25
0.00
0 500 1000 1500 2000 0 5000 10000 15000 20000
Iteration Iteration
Figure 3: Left: KL divergence of different methods versus number of iterations on gaussian mixture
distribution. Right:KLdivergenceofdifferentmethodsversusnumberofiterationsonthemonomial
gamma distribution.
mixture where the variances of the mixture components are 0.1, 0.2, 0.3, 0.4 and 0.5. The number
of particles is 1000. The initial noise magnitude for both SIFG and Ada-SIFG is 0.1. The left of
figure 3 shows the KL divergence against different numbers of iterations. On this toy example, SIFG
with fixed noise level 0.1 significantly accelerates the convergence compared to L -GF, while being
2
competitive against Ada-GWG. Ada-SIFG continues to improve the convergence of SIFG. This
result demonstrates the effectiveness of injecting noise into the sampling process.
6.2 Monomial Gamma
To further investigate the exploration advantage of SIFG, following Cheng et al. [2023],
we consider the heavy tailed 2D Monomial Gamma distribution where the target π(x ,x ) ∝
1 2
exp(−0.3(|x |0.9+|x |0.9)). The number of particles is 1000. The initial noise magnitude for both
1 2
SIFG and Ada-SIFG is 0.1. Similar to the gaussian mixture experiment, the right of figure 3 shows
SIFG converges faster than L -GF and Ada-GWG, and Ada-SIFG converges even faster than SIFG.
2
Both SIFG and Ada-SIFG converge more steadily than L -GF and Ada-GWG. Additionally, the
2
variances of SIFG and Ada-SIFG are also significantly smaller than which of L -GF and Ada-GWG.
2
Faster convergence indicates higher exploration ability to efficiently traverse the heavy tail. This is
achieved by perturbing the particles during training. Being able to use denoising score matching to
estimate the score function contributes to the smaller variance.
12
ecnegreviD
LK
ecnegreviD
LK4.0
SIFG( =0.1)
3.8 Ada-SIFG( =0.1)
100 100 3.6 SIFG( =0.01)
Ada-SIFG( =0.01)
3.4
3.2 L2-GF L2-GF
Ada-GWG Ada-GWG 3.0
SIFG( =0.03) SIFG( =0.03)
Ada-SIFG( =0.03) Ada-SIFG( =0.03) 2.8
SVGD SVGD 2.6
0 100 200 300 400 500 0 100 200 300 400 500 0 500 1000 1500 2000
Sample Sample Iteration
Figure 4: Left and Middle: Sorted Amari distances of different methods on MEG dataset. On the
left is the experiment for 10 particles and 50 random repetitions. In the middle is the experiment
for 100 particles and 5 random repetitions. Right: Test RMSE for BNN on Boston dataset. The
number in parentheses specifies the initial σ. The results are averaged from 10 independent runs.
6.3 Independent Component Analysis
The task of Bayesian independent component analysis (ICA) is to infer the unknown unmixing
matrix W ∈ Rd×d given the observations x from x = W−1s, where s ∈ Rd are the latent independent
sources. Suppose X = {x ∈ Rd} are the observed samples, assuming each component s has the
n i
same distribution s ∼ p , then the likelihood is:
i s
d
(cid:89)
p(x|W) = |detW| p ([Wx] )
s i
i=1
Following Korba et al. [2021], we choose a gaussian prior over the unmixing matrix p(W ) ∼
ij
N(0,θ2) and choose p (·) = (cosh(·))−1. We compare our methods SIFG and Ada-SIFG with KSDD,
s
SVGD, L2GF and Ada-GWG on the MEG dataset [Vigario et al., 1997], which has 122 channels
and 17730 observations. We extract the first 5 channels such that d = 5 and the dimension of
matrix W is 25. We compute the Amari distance [Amari et al., 1995] between the ground truth
W estimated by using a long run (105 samples) of standard HMC algorithm and the estimates
0
generated by different methods to assess convergence. The measure equals to zero if and only if the
two matrices are the same up to scale and permutation.
To assess the sample efficiency of different methods, we choose 10 and 100 particles for sampling,
and we independently repeat 50 and 5 times so that we collect 500 samples for each method. The
left and middle of figure 4 shows the Amari distance results after sorting. SIFG outperforms L -GF
2
and Ada-GWG in both particle number settings, and is not significantly affected by the relatively
small number of particles. On the contrary, SVGD is apparently affected by small particle size.
13
ecnatsiD
iramA
ecnatsiD
iramA
ecnegreviD
LKThe improvement for both adaptive methods Ada-GWG and Ada-SIFG is not significant for small
number of particles, while evident for larger number of particles. This is because the number
of particles affects the training of neural networks. This suggests that the effectiveness of the
adaptation procedure relies on the approximation accuracy of neural networks.
6.4 Bayesian Neural Networks
We compare our algorithm with SGLD, SVGD, L2-GF and Ada-GWG on Bayesian neural
networks (BNN). Following [Cheng et al., 2023], we conduct the two-layer network with 50 hidden
units and ReLU activation function, and we use a Gamma(1, 0.1) prior for the inverse covariances.
The datasets are all randomly partitioned into 90% for training and 10% for testing. The mini-batch
size is 100 for all datasets. All results are averaged over 10 independent random trials. Table 1
shows the average test RMSE and NLL with their standard deviation. We see that Ada-SIFG can
achieve comparable or better results than the other methods. Ada-SIFG consistently improves over
L -GF by a noticeable margin. To examine the effect of adaptively tuning the noise magnitude σ,
2
we additionally compare the test RMSE against iterations for two sets of SIFG and Ada-SIFG with
different σ initialization. From the right of figure 4 we can see that on this specific task, setting
σ = 0.01 produces better results than σ = 0.1. Although the latter is sub-optimal, our adaptive
method makes significant improvements, approaching toward the better performance of initializing
σ = 0.01. The adaptive scheme serves as a correction on selecting proper σ automatically during
training. This suggests that our adaptive method is robust even when the initial noise magnitude
choice is not ideal. More experiment setting details can be found in the appendix C.
Table 1: Test RMSE and test NLL of Bayesian neural networks on several UCI datasets. The results
are averaged from 10 independent runs with the standard deviation in the subscripts.
TestRMSE(↓) TestNLL(↓)
Dataset
SGLD SVGD L2-GF Ada-GWG Ada-SIFG SGLD SVGD L2-GF Ada-GWG Ada-SIFG
Boston 2.917±0.04 2.944±0.03 3.016±0.12 2.662±0.06 2.641±0.02 2.563±0.01 2.567±0.01 2.706±0.04 2.680±0.04 2.496±0.02
Concrete 8.218±0.27 7.640±0.03 7.556±0.08 6.590±0.09 6.619±0.03 3.539±0.02 3.455±0.01 3.449±0.01 3.341±0.01 3.323±0.01
Power 4.176±0.01 4.940±0.00 4.258±0.06 4.074±0.05 4.017±0.00 2.863±0.00 3.016±0.00 3.070±0.02 2.967±0.01 2.829±0.00
Wine 0.431±0.00 0.422±0.00 0.419±0.00 0.416±0.00 0.413±0.00 0.576±0.00 0.559±0.00 0.552±0.01 0.552±0.00 0.535±0.00
Protein 4.609±0.00 4.940±0.00 5.024±0.02 4.967±0.01 4.863±0.00 2.946±0.00 3.016±0.00 3.075±0.02 3.040±0.01 2.999±0.00
Diabetes 0.382±0.00 0.386±0.00 0.381±0.00 0.384±0.00 0.379±0.00 0.466±0.00 0.484±0.00 0.471±0.00 0.479±0.00 0.449±0.00
147 Conclusion
Wepresentedanewfunctionalgradientflowmethod, calledSIFG,whichutilizesthesemi-implicit
variationalfamilyrepresentedbyparticlesbyinjectinggaussiannoisetotheparticlesbeforeupdating
according to the gradient flow. We generalize the convergence guarantees of variational inference
with gaussian mixtures in discrete time setting. We considered the sample complexity in the ERM
regime. We also introduced an adaptive version, called Ada-SIFG, that can automatically tune
the noise magnitude to improve sampling quality. Extensive numerical experiments showed that
Ada-SIFG outperforms existing ParVI methods.
Acknowledgements
This work was supported by National Natural Science Foundation of China (grant no. 12201014
and grant no. 12292983). The research of Cheng Zhang was supported in part by National
EngineeringLaboratoryforBigDataAnalysisandApplications, theKeyLaboratoryofMathematics
andItsApplications(LMAM)andtheKeyLaboratoryofMathematicalEconomicsandQuantitative
Finance (LMEQF) of Peking University. The authors are grateful for the computational resources
provided by the High-performance Computing Platform of Peking University.
References
David Alvarez-Melis, Yair Schiff, and Youssef Mroueh. Optimizing functionals on the space of
probabilities with input convex neural networks. Transactions on Machine Learning Research,
2022. ISSN 2835-8856. URL https://openreview.net/forum?id=dpOYN7o8Jm.
S. I. Amari, A. Cichocki, and H. Yang. A new learning algorithm for blind signal separation.
Advances in neural information processing systems, 8, 1995.
DavidM.Blei,AlpKucukelbir,andJonD.McAuliffe. Variationalinference: Areviewforstatisticians.
Journal of the American Statistical Association, 112:859 – 877, 2016.
Olivier Bousquet. Concentration inequalities and empirical processes theory applied to the analysis
of learning algorithms. PhD thesis, E´cole Polytechnique: Department of Applied Mathematics
Paris, France, 2002.
15ChangyouChen,RuiyiZhang,WenlinWang,BaiLi,andLiqunChen. Aunifiedparticle-optimization
framework for scalable bayesian sampling. ArXiv, abs/1805.11659, 2018.
Tianqi Chen, Emily Fox, and Carlos Guestrin. Stochastic gradient hamiltonian monte carlo. In
International Conference on Machine Learning, pages 1683–1691, 2014.
Z. Cheng, S. Zhang, L. Yu, and C. Zhang. Particle-based variational inference with generalized
wasserstein gradient flow. Advances in Neural Information Processing Systems, 2023.
Ziheng Cheng, Longlin Yu, Tianyu Xie, Shiyue Zhang, and Cheng Zhang. Kernel semi-implicit
variational inference. arXiv preprint arXiv:2405.18997, 2024.
Sinho Chewi, Thibaut Le Gouic, Chen Lu, Tyler Maunu, and Philippe Rigollet. Svgd as a kernelized
wassersteingradientflowofthechi-squareddivergence. Advances in Neural Information Processing
Systems, 33, 2020.
Lauro Langosco di Langosco, Vincent Fortuin, and Heiko Strathmann. Neural variational gradient
descent. arXiv preprint arXiv:2107.10731, 2021.
Michael Diao, Krishnakumar Balasubramanian, Sinho Chewi, and Adil Salim. Forward-backward
gaussian variational inference via jko in the bures–wasserstein space. In International Conference
on Machine Learning, pages 7960–7991, 2023.
Hanze Dong, Xi Wang, LIN Yong, and Tong Zhang. Particle-based variational inference with
preconditioned functional gradient flow. In The Eleventh International Conference on Learning
Representations, 2023. URL https://openreview.net/forum?id=6OphWWAE3cS.
S. Duane, A. D. Kennedy, B J. Pendleton, and D. Roweth. Hybrid Monte Carlo. Physics Letters B,
195(2):216 – 222, 1987.
Jiaojiao Fan, Qinsheng Zhang, Amirhossein Taghvaei, and Yongxin Chen. Variational wasserstein
gradient flow. In International Conference on Machine Learning, pages 6185–6215. PMLR, 2022.
Will Grathwohl, Kuan-Chieh Wang, J¨orn-Henrik Jacobsen, David Duvenaud, and Richard Zemel.
Learning the stein discrepancy for training and evaluating energy-based models without sampling.
In International Conference on Machine Learning, pages 3732–3747. PMLR, 2020.
Tianyang Hu, Zixiang Chen, Hanxi Sun, Jincheng Bai, Mao Ye, and Guang Cheng. Stein neural
sampler. arXiv preprint arXiv:1810.03545, 2018.
16Tom Huix, Anna Korba, Alain Durmus, and Eric Moulines. Theoretical guarantees for variational
inference with fixed-variance mixture of gaussians. arXiv preprint arXiv:2406.04012, 2024.
Michael I. Jordan, Zoubin Ghahramani, T. Jaakkola, and Lawrence K. Saul. An introduction to
variational methods for graphical models. Machine Learning, 37:183–233, 1999.
Anna Korba, Pierre-Cyril Aubin-Frankowski, Szymon Majewski, and Pierre Ablin. Kernel stein
discrepancy descent. In International Conference on Machine Learning, pages 5719–5730. PMLR,
2021.
Marc Lambert, Sinho Chewi, Francis Bach, Silvere Bonnabel, and Philippe Rigollet. Variational
inference via wasserstein gradient flows. Advances in Neural Information Processing Systems, 35,
2022.
Jen Ning Lim and Adam Johansen. Particle semi-implicit variational inference. arXiv preprint
arXiv:2407.00649, 2024.
ChangLiu,JingweiZhuo,PengyuCheng,RuiyiZhang,JunZhu,andLawrenceCarin. Understanding
and accelerating particle-based variational inference. International Conference on Machine
Learning. PMLR, pp. 4082-4092, 2019.
Qiang Liu. Stein variational gradient descent as gradient flow. Advances in neural information
processing systems, 30, 2017.
Qiang Liu and Dilin Wang. Stein variational gradient descent: A general purpose bayesian inference
algorithm. Advances in neural information processing systems, 29, 2016.
Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-
instruct: A universal approach for transferring knowledge from pre-trained diffusion models.
Advances in Neural Information Processing Systems, 36, 2024.
Radford Neal. MCMC using hamiltonian dynamics. In S Brooks, A Gelman, G Jones, and
XL Meng, editors, Handbook of Markov Chain Monte Carlo, Chapman & Hall/CRC Handbooks
of Modern Statistical Methods. Taylor & Francis, 2011. ISBN 9781420079425. URL http:
//books.google.com/books?id=qfRsAIKZ4rIC.
G. O. Robert and O. Stramer. Langevin diffusions and metropolis-hastings algorithms. Methodology
and Computing in Applied Probability, 4:337–357, 2002.
17Nathan Srebro, Karthik Sridharan, and Ambuj Tewari. Smoothness, low noise and fast rates.
Advances in neural information processing systems, 23, 2010.
Michalis K Titsias and Francisco Ruiz. Unbiased implicit variational inference. In The 22nd
International Conference on Artificial Intelligence and Statistics, pages 167–176. PMLR, 2019.
R. Vigario, J. Sarela, and E. Oja. Meg data for studies using independent component analysis. 1997.
URL http://www.cis.hut.fi/projects/ica/eegmeg/MEGdata.html.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computa-
tion, 23(7):1661–1674, 2011.
M.J.WainwrightandM.I.Jordan. Graphicalmodels, exponentialfamilies, andvariationalinference.
Foundations and Trends in Maching Learning, 1(1-2):1–305, 2008.
Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48. Cam-
bridge university press, 2019.
Max Welling and Yee W Teh. Bayesian learning via stochastic gradient langevin dynamics. In
International Conference on Machine Learning, pages 681–688, 2011.
MingzhangYinandMingyuanZhou. Semi-implicitvariationalinference. InInternational Conference
on Machine Learning, pages 5660–5669. PMLR, 2018.
Longlin Yu and Cheng Zhang. Semi-implicit variational inference via score matching. arXiv preprint
arXiv:2308.10014, 2023.
18A Details of Ada-SIFG
We follow the derivation idea of Luo et al. [2024] to prove the gradient estimate result (4.22).
d ∂ µˆ (z+σw)
F (µ ) = E log σ,t (A.1)
dσ σ t z∼µt(z),w∼N(0,I)∂σ π(z+σw)
µˆ (z+σw) ∂
= E ∇log σ,t ·w+E logµˆ (x) (A.2)
z∼µt(z),w∼N(0,I) π(z+σw) x∼µˆσ,t∂σ σ,t
µˆ (z+σw)
= E ∇log σ,t ·w (A.3)
z∼µt(z),w∼N(0,I) π(z+σw)
≈ E [f (z+σw)−∇logπ(z+σw)]·w (A.4)
z∼µt(z),w∼N(0,I) γ
where the equality of (A.4) is because
(cid:90) (cid:90)
∂ ∂ ∂ ∂
E logµˆ (x) = µˆ (x)dx = µˆ (x)dx = 1 = 0. (A.5)
x∼µˆσ,t∂σ σ,t
∂σ
σ,t
∂σ
σ,t
∂σ
By this gradient descent scheme on the noise magnitude σ, we present the full procedure of
Ada-SIFG in Algorithm 2.
B Proofs
B.1 Proof of Proposition 5.5 and Theorem 5.6
B.1.1 Main Lemmas
Lemma B.1. Assuming the second moment of distribution ρ is finite, k σ(x) = Z
0e−∥ 2x σ∥ 22
, where Z
0
(cid:90)
satisfies k (x)dx = 1, then for all θ ∈ Rd, k ∈ N , there exists constant C,M > 0, such that
σ +
(cid:82) ∥θ−y∥kk (θ−y)dρ(y)
σ ≤ C∥θ∥k +M (B.1)
(cid:82)
k (θ−y)dρ(y)
σ
(cid:90)
Proof. Since y2dρ(y) < ∞, there exists a sufficient large constant R and a positive constant
0
(cid:90) (cid:90)
C > 0, both do not relate to θ, such that y2dρ(y) > C for all R ≥ R , then dρ(y) >
0 0 0
∥y∥<R ∥y∥<R
C
0
for all R ≥ R . Note that {y : ∥θ−y∥ < R +∥θ∥} ⊇ {y : ∥y∥ < R } by triangular inequality,
R2 0 0 0
0
(cid:90)
we have y2dρ(y) > C .
0
∥θ−y∥<R0+∥θ∥
19Algorithm 2 Ada-SIFG: adaptive semi-implicit functional gradient flow
Require: Unnormalizedtargetdistributionπ,initialparticles{zi}n ,initialparameterγ ,iteration
0 i=1 0
number N,N′, particle step size h, parameter step size η, initial noise magnitude σ , noise
0
magnitude step size η˜, lower and upper bounds on σ: lb,ub.
for k = 0,··· ,N −1 do
Assign γ0 = γ
k k
Obtain perturbed samples xi = zi +ϵi, where ϵi ∼ N(0,σ2)
k k k k k
for t = 0,··· ,N′−1 do
Compute
1 (cid:88)n xi −zi
L(cid:98)(γ) =
n
∥f γ(xi k)− k
σ2
k∥2 (A.6)
i=1 k
Update γ kt+1 = γ kt +η∇ γL(cid:98)(γ kt)
end for
Update γ =
γN′
k+1 k
n
Compute g(cid:91) rad(σ ) = 1 (cid:88) [∇logπ(xi)−f (xi)]·ϵi
k n k γ k+1 k k
i=1
(cid:91)
Update σ = clip(σ +η˜·grad(σ ),lb,ub)
k+1 k k
Update particles zi = zi +h(∇logπ(xi)−f (xi)) for i = 1,··· ,n
k+1 k k γ k+1 k
end for
Obtain perturbed samples xi = zi +ϵi , where ϵi ∼ N(0,σ2 )
N N N N N
return Particles {xi }n
N i=1
From the above analysis, the denominator
(cid:90) (cid:90)
k (θ−y)dρ(y) > k (θ−y)dρ (y)
σ σ t
∥θ−y∥<R0+∥θ∥
>
(cid:90) e−(R0 2+ σ∥ 2θ∥)2
dρ(y)
∥θ−y∥<R0+∥θ∥
(B.2)
> Z
0(cid:90) e−(R0 2+ σ∥ 2θ∥)2
dρ(y)
∥y∥<R0
> Z
0e−(R0 2+ σ∥ 2θ∥)2
·
RC 0
2
0
20Then
(cid:82) ∥θ−y∥kk σ(θ−y)dρ(y) (cid:82) ∥θ−y∥>R∥θ−y∥kk σ(θ−y)dρ(y)+(cid:82) ∥θ−y∥<R∥θ−y∥kk σ(θ−y)dρ(y)
=
(cid:82) (cid:82)
k (θ−y)dρ(y) k (θ−y)dρ(y)
σ σ
(B.3)
(cid:82) ∥θ−y∥kk (θ−y)dρ(y)
≤ ∥θ−y∥>R σ +Rk (B.4)
(cid:82)
k (θ−y)dρ(y)
σ
≤
Z 0Rke− 2R σ2 2
+Rk (B.5)
(cid:82)
k (θ−y)dρ(y)
σ
≤
Rke− 2R σ2 2
·
R 02
+Rk (B.6)
e−(R0 2+ σ∥ 2θ∥)2 C
0
The inequality (B.5) uses monotonically decreasing property for Rke− 2R σ2 2 when R > R 1 = √ kσ,
and plugging in (B.2) gives the inequality (B.6).
(cid:82) ∥θ−y∥kk (θ−y)dρ(y) R2
ChooseR = max{R +∥θ∥,R },wehave σ ≤ ( 0+1)Rk ≤ C(∥θ∥k+M),
0 1 (cid:82)
k (θ−y)dρ(y) C
σ 0
R2 R2
where C = ( 0 +1)2k−1,M = ( 0 +1)·max{2k−1Rk,Rk}, the proof is complete.
C C 0 1
0 0
Lemma B.2. Denote ψ = Id+tϕ, then for any ρ ∈ P(Rd) and ρ = (ψ ) ρ , we have
t 0 t t # 0
(cid:12)
d d (cid:12)
G(ρˆ)− G(ρˆ)(cid:12) ≤ Lt∥ϕ∥2 (B.7)
dt t dt t (cid:12) L2(ρ0)
(cid:12)
t=0
(cid:90)
where G(ρˆ) := −logπ(x)dµˆ (x), L is the Lipschitz constant of ∇logπ.
t t
Proof. Analogous to (4.14),
(cid:90)
d
G(ρˆ) = ⟨ −∇logπ(x)q (x|z)dx,ϕ(ψ−1(z))⟩ (B.8)
dt t σ t L2(ρt)
Then
(cid:12)
d d (cid:12) (cid:90) (cid:90)
G(ρˆ)− G(ρˆ)(cid:12) = ⟨ −∇logπ(x)q (x|z)dx,ϕ(ψ−1(z))⟩ −⟨ −∇logπ(x)q (x|z)dx,ϕ(z)⟩
dt t dt t (cid:12) σ t L2(ρt) σ L2(ρ0)
(cid:12)
t=0
(cid:90) (cid:90)
= ⟨[ −∇logπ(x)q (x|ψ (z))dx− −∇logπ(x)q (x|z)dx],ϕ(z)⟩
σ t σ L2(ρ0)
(cid:90)
= ⟨ [∇logπ(x)−∇logπ(x+tϕ(z))]q (x|z)dx,ϕ(z)⟩
σ L2(ρ0)
≤ ⟨L∥tϕ(z)∥,∥ϕ(z)∥⟩ = Lt∥ϕ∥2
L2(ρ0) L2(ρ0)
(B.9)
21Lemma B.3. Denote ψ = Id+tϕ, where ∥ϕ(z)∥ ≤ A∥z∥+B, then for any fifth absolute moment
t
1
finite ρ ∈ P(Rd) and ρ = (ψ ) ρ , t < , there exists constants C,M such that
0 t t # 0
A
(cid:12)
d d (cid:12)
E(ρˆ)− E(ρˆ)(cid:12) ≤ t[Cm (ρ )+M]+t2[Cm (ρ )+M] (B.10)
dt t dt t (cid:12) 3 0 5 0
(cid:12)
t=0
(cid:90) (cid:90) (cid:90)
where E(µˆ ) := logµˆ (x)dµˆ (x), m (ρ ) := ∥z∥3dρ and m (ρ ) := ∥z∥5dρ are the third
t t t 3 0 0 5 0 0
and fifth absolute moments of ρ .
0
Proof. Analogous to (4.14),
(cid:90)
d
E(ρˆ) = ⟨ ∇logρˆ(x)q (x|z)dx,ϕ(ψ−1(z))⟩
dt t t σ t L2(ρt)
(cid:90) (cid:82) ∇ q (y|x)dρ (y) (B.11)
= ⟨ q (x|z) x σ t dx,ϕ(ψ−1(z))⟩
σ (cid:82) q (y|x)dρ (y) t L2(ρt)
σ t
Then
(cid:12)
d d (cid:12)
E(ρˆ)− E(ρˆ)(cid:12)
dt t dt t (cid:12)
(cid:12)
t=0
(cid:90) (cid:82) ∇ q (y|x)dρ (y) (cid:90) (cid:82) ∇ q (y|x)dρ (y)
=⟨ q (x|z) x σ t dx,ϕ(ψ−1(z))⟩ −⟨ q (x|z) x σ 0 dx,ϕ(z)⟩
σ (cid:82) q (y|x)dρ (y) t L2(ρt) σ (cid:82) q (y|x)dρ (y) L2(ρ0)
σ t σ 0
(cid:90) (cid:82) ∇ q (y|x)dρ (y) (cid:90) (cid:82) ∇ q (y|x)dρ (y)
x σ t x σ 0
=⟨[ q (x|ψ (z)) dx− q (x|z) dx],ϕ(z)⟩
σ t (cid:82)
q (y|x)dρ (y)
σ (cid:82)
q (y|x)dρ (y)
L2(ρ0)
σ t σ 0
1 (cid:90) (cid:82) yq (y|x)dρ (y) (cid:82) yq (y|x)dρ (y) t
= ⟨ [q (x|ψ (z)) σ t −q (x|z) σ 0 ]dx,ϕ(z)⟩ + ∥ϕ∥2
σ2 σ t (cid:82) q (y|x)dρ (y) σ (cid:82) q (y|x)dρ (y) L2(ρ0) σ2 L2(ρ0)
σ t σ 0
(B.12)
(cid:82) (cid:82)
yq (y|x)dρ (y) yq (y|x)dρ (y)
σ t σ 0
Denote A := q (x|ψ (z)) , then A = q (x|z) .
t σ t (cid:82) 0 σ (cid:82)
q (y|x)dρ (y) q (y|x)dρ (y)
σ t σ 0
For simplicity, we denote k σ(w) = Z
0e−∥ 2w σ∥ 22
, where Z
0
satisfies
(cid:90)
k σ(w)dw = 1, then q σ(x|z) =
k (x−z).
σ
By Lagrangian mean value theorem, there exists t′,t′′ ∈ [0,t], such that:
q σ(x|ψ t(z)) = Z
0e−∥x−(z+ 2σt 2ϕ(z))∥2
= Z
0[e−∥x 2− σz 2∥2 +t·e−∥x−(z+ 2σt′ 2ϕ(z))∥2 (x−(z σ+ 2t′ϕ(z)))
ϕ(z)] (B.13)
and
(cid:82) (cid:82) (cid:82)
yk (x−y)dρ (y) yk (x−y)dρ(y) ∂ (cid:12) yk (x−y)dρ (y)
ϵ t = σ +t· (cid:12) ( σ t ) (B.14)
(cid:82) k σ(x−y)dρ t(y) (cid:82) k σ(x−y)dρ(y) ∂t(cid:12) t=t′′ (cid:82) k σ(x−y)dρ t(y)
22dρ (y)
Using continuity equation t = −∇·(ρ (y)ϕ(ψ−1(y))) for y ∼ ρ , we have:
dt t t t
(cid:82)
∂ yk (x−y)dρ (y)
σ t
(B.15)
(cid:82)
∂t k (x−y)dρ (y)
σ t
(cid:82)
yk
(x−y)dρt(y) (cid:82)
k (x−y)dρ
(y)−(cid:82)
yk (x−y)dρ
(y)(cid:82)
k
(x−y)dρt(y)
= σ dt σ t σ t σ dt (B.16)
(cid:82)
( k (x−y)dρ (y))2
σ t
(cid:82) ϕ(ψ−1(y))∇(yk (x−y))dρ (y) (cid:82) yk (x−y)dρ (y)(cid:82) ϕ(ψ−1(y))∇k (x−y)dρ (y)
= t σ t − σ t t σ t (B.17)
(cid:82) (cid:82)
k (x−y)dρ (y) ( k (x−y)dρ (y))2
σ t σ t
(cid:82) ϕ(ψ−1(y))(1+yx−y)k (x−y)dρ (y) (cid:82) yk (x−y)dρ (y)(cid:82) ϕ(ψ−1(y))x−yk (x−y)dρ (y)
= t σ2 σ t − σ t t σ2 σ t
(cid:82) (cid:82) (cid:82)
k (x−y)dρ (y) k (x−y)dρ (y) k (x−y)dρ (y)
σ t σ t σ t
(B.18)
Sinceψ (y) = y+tϕ(y)and∥ϕ(y)∥ ≤ A∥y∥+B,then(1−tA)∥y∥−tB ≤ ∥y∥−t∥ϕ(y)∥ ≤ ∥ψ (y)∥.
t t
1 1 tB 1
For t < , we have ∥y∥ ≤ ∥ψ (y)∥+ , which means ∥ψ−1(y)∥ ≤ ∥y∥+
A 1−tA t 1−tA t 1−tA
tB
.
1−tA
A B
This means that ∥ϕ(ψ−1(y))∥ ≤ ∥y∥+ .
t 1−tA 1−tA
By triangular inequality, we have
x−y
∥ϕ(ψ−1(y))(1+y )∥
t σ2
A A∥x∥ B ∥x∥ A B A
≤ ∥x−y∥3+(2 + )∥x−y∥2+( ( ∥x∥+ )+ )∥x−y∥
(1−tA)σ2 (1−tA)σ2 1−tA σ2 1−tA 1−tA 1−tA
A B
+( ∥x∥+ )
1−tA 1−tA
(B.19)
The right hand side is a polynomial of (∥x∥,∥x−y∥), the order of which is at most 3. Since
the fifth moment of ρ is finite, then the second moment is also finite. Using basic inequalities
0
4 8
∥x∥3+ ≥ ∥x∥2,∥x∥3+ ≥ ∥x∥ and Lemma B.1, we obtain:
27 27
(cid:82) ϕ(ψ−1(y))(1+yx−y)k (x−y)dρ (y)
∥ t σ2 σ t ∥ ≤ C ∥x∥3+M (B.20)
(cid:82) 1 1
k (x−y)dρ (y)
σ t
where C ,M > 0 are constants that does not relate to x.
1 1
Analogously,
(cid:82) yk (x−y)dρ (y)(cid:82) ϕ(ψ−1(y))x−yk (x−y)dρ (y)
∥ σ t t σ2 σ t ∥ ≤ C ∥x∥3+M (B.21)
(cid:82) (cid:82) 2 2
k (x−y)dρ (y) k (x−y)dρ (y)
σ t σ t
To conclude, we obtain
23(cid:82)
∂ (cid:12) yk (x−y)dρ (y)
∥ (cid:12) ( σ t )∥ ≤ (C +C )∥x∥3+(M +M ) := C ∥x∥3+M (B.22)
∂t(cid:12) t=t′′ (cid:82) k σ(x−y)dρ t(y) 1 2 1 2 3 3
1
for t < .
A
From (B.13) and (B.14) we have
1
∥A −A ∥
t 0
Z
0
=∥t[(cid:82) (cid:82)y kk σσ (( xx −− yy )) dd ρρ (( yy )) ·e−∥x−(z+ 2σt′ 2ϕ(z))∥2 (x−(z σ+ 2t′ϕ(z))) ϕ(z)+e−∥x 2− σz 2∥2 · ∂∂ t(cid:12) (cid:12)
(cid:12)
t=t′′((cid:82) (cid:82)y kk σσ (( xx −− yy )) dd ρρ tt (( yy )) )]
+t2e−∥x−(z+ 2σt′ 2ϕ(z))∥2 (x−(z σ+ 2t′ϕ(z))) ϕ(z)· ∂∂ t(cid:12) (cid:12)
(cid:12)
t=t′′((cid:82) (cid:82)y kk σσ (( xx −− yy )) dd ρρ tt (( yy )) )∥
≤t[∥ϕ(z)∥(C 4∥x∥+M
4)∥(x−(z σ+ 2t′ϕ(z))) ∥e−∥x−(z+ 2σt′ 2ϕ(z))∥2 +e−∥x 2− σz 2∥2
·(C 3∥x∥3+M 3)]
+t2∥ϕ(z)∥∥(x−(z σ+ 2t′ϕ(z))) ∥e−∥x−(z+ 2σt′ 2ϕ(z))∥2
·(C 3∥x∥3+M 3)
(B.23)
Then
(cid:90) (cid:82) yq (y|x)dρ (y) (cid:82) yq (y|x)dρ (y)
σ t σ 0
∥q (x|ψ (z)) −q (x|z) ∥dx
σ t (cid:82) σ (cid:82)
q (y|x)dρ (y) q (y|x)dρ (y)
σ t σ 0
(cid:90)
= ∥A −A ∥dx
t 0
≤tZ
0(cid:90)
[∥ϕ(z)∥(C 4∥x∥+M
4)∥(x−(z σ+ 2t′ϕ(z))) ∥e−∥x−(z+ 2σt′ 2ϕ(z))∥2 +e−∥x 2− σz 2∥2
·(C 3∥x∥3+M 3)]dx
+t2Z
0(cid:90) ∥ϕ(z)∥∥(x−(z σ+ 2t′ϕ(z))) ∥e−∥x−(z+ 2σt′ 2ϕ(z))∥2
·(C 3∥x∥3+M 3)dx
≤t[(C ∥z+t′ϕ(z)∥+M )∥ϕ(z)∥+C ∥z∥+M ]+t2(C ∥z+t′ϕ(z)∥3+M )∥ϕ(z)∥
5 5 6 6 7 7
(B.24)
1
Plugging in t′ ≤ t < and ∥ϕ(z)∥ ≤ A∥z∥+B, we obtain
A
(cid:90) (cid:82) yq (y|x)dρ (y) (cid:82) yq (y|x)dρ (y)
∥q (x|ψ (z)) σ t −q (x|z) σ 0 ∥dx ≤ t(C ∥z∥2+M )+t2(C ∥z∥4+M )
σ t (cid:82) σ (cid:82) 8 8 9 9
q (y|x)dρ (y) q (y|x)dρ (y)
σ t σ 0
(B.25)
Using this result and ∥ϕ(z)∥ ≤ A∥z∥+B in (B.12), we finally obtain:
24(cid:12)
d d (cid:12)
E(ρˆ)− E(ρˆ)(cid:12)
dt t dt t (cid:12)
(cid:12)
t=0
(cid:90)
1 t (B.26)
≤ [t(C ∥z∥2+M )+t2(C ∥z∥4+M )]∥ϕ(z)∥dρ (z)+ ∥ϕ∥2
σ2 8 8 9 9 0 σ2 L2(ρ0)
≤t[Cm (ρ )+M]+t2[Cm (ρ )+M]
3 0 5 0
The proof is complete.
B.1.2 Proof of main results
Proposition B.4. Suppose Assumption 5.1, 5.2, 5.3, 5.4 hold. Then the following inequality holds
1 1 1
for h < : F(µˆ )−F(µˆ ) ≤ − h∥∇ F(µ )∥2 + hϵ +h2[Cm +M]+h3[Cm +M],
A (k+1)h kh 2 W2 kh L2(µ kh) 2 k 3 5
where C,M are constants that do not relate to k or h.
Proof. The KL divergence can be split into two terms:
(cid:90) (cid:90) (cid:90)
µˆ (x)
t
F(µˆ ) := log dµˆ (x) = −logπ(x)dµˆ (x)+ logµˆ (x)dµˆ (x) := G(µˆ )+E(µˆ ) (B.27)
t t t t t t t
π(x)
Since
(cid:12) (cid:34) (cid:12) (cid:35)
d (cid:12) (cid:90) h d d (cid:12)
F(µˆ )−F(µˆ ) = h (cid:12) F(µˆ )+ F(µˆ )− (cid:12) F(µˆ ) dt (B.28)
(k+1)h kh dt(cid:12) kh+t dt kh+t dt(cid:12) kh+t
(cid:12) 0 (cid:12)
t=0 t=0
and
(cid:12)
d (cid:12) 1 1
(cid:12) F(µˆ ) = ⟨∇ F(µ ),v ⟩ ≤ − ∥∇ F(µ )∥2 + ϵ (B.29)
dt(cid:12)
(cid:12)
kh+t W2 kh kh µ kh 2 W2 kh L2(µ kh) 2 k
t=0
Set ϕ = v ,ρ = µ and ρ = µ in Lemma B.2, we have
kh 0 kh t kh+t
(cid:12)
d d (cid:12)
G(µˆ )− (cid:12) G(µˆ ) ≤ Lt∥ϕ∥2 ≤ t(C m +M ) (B.30)
dt kh+t dt(cid:12)
(cid:12)
kh+t L2(µˆ kh) 1 2 1
t=0
where m is the upper bound of the second moment of µ .
2 kh
By Lemma B.3, we have
(cid:12)
d d (cid:12)
E(µˆ )− (cid:12) E(µˆ ) ≤ t[C m +M ]+t2[C m +M ] (B.31)
dt kh+t dt(cid:12) kh+t 2 3 2 2 5 2
(cid:12)
t=0
25where m and m are the upper bounds of the third and fifth absolute moment of µ .
3 5 kh
4 4
Note that ∥z∥3+ ≥ ∥z∥2, we have m + ≥ m .
3 2
27 27
Then
(cid:34) (cid:12) (cid:35)
(cid:90) h d d (cid:12)
F(µˆ )− (cid:12) F(µˆ ) dt
dt kh+t dt(cid:12) kh+t
0 (cid:12)
t=0
(cid:34) (cid:12) (cid:12) (cid:35)
(cid:90) h d d (cid:12) d d (cid:12) (B.32)
= G(µˆ )− (cid:12) G(µˆ )+ E(µˆ )− (cid:12) E(µˆ ) dt
dt kh+t dt(cid:12) kh+t dt kh+t dt(cid:12) kh+t
0 (cid:12) (cid:12)
t=0 t=0
≤h2[Cm +M]+h3[Cm +M]
3 5
Plug this back to (B.28) gives the desired result, the proof is complete.
Theorem B.5. Assume Proposition 5.5 holds, then the average of squared gradient norms satisfies
1 (cid:88)K
∥∇ F(µ )∥2 ≤
R
+
S
+
1 (cid:88)K
ϵ for K > min{
A2F(µˆ 0)
,
A3F(µˆ 0)
}, where
K
k=1
W2 kh L2(µ kh) K1
2
K2
3
K
k=1
k Cm 3+M Cm 5+M
(cid:112) 2 1
R := 4 F(µˆ 0)(Cm 3+M) and S := 4(F(µˆ 0))3(Cm 5+M)3.
Proof. From Proposition 5.5 we have:
1 1
F(µˆ )−F(µˆ ) ≤ − h∥∇ F(µ )∥2 + hϵ +h2[Cm +M]+h3[Cm +M] (B.33)
(k+1)h kh 2 W2 kh L2(µ kh) 2 k 3 5
Take the sum from k = 0 to k = K −1, we obtain
K K
1 (cid:88)
∥∇ F(µ )∥2 ≤
2F(µˆ 0)
+
1 (cid:88)
ϵ +h(2(Cm +M))+h2(2(Cm +M)) (B.34)
K W2 kh L2(µ kh) hK K k 3 5
k=1 k=1
2F(µˆ ) a a
0 1 1
Denote a = ,b = 2(Cm
3
+M),c = 2(Cm
5
+M), choose h = min{( )2,( )3}, then
K b c
A2F(µˆ ) A3F(µˆ ) 1 a
choosing K > min{ 0 , 0 } leads to h < . In this case, we have +bh+ch2 ≤
Cm +M Cm +M A h
3 5
1 2 1
2((ab)2 +a3c3). Plugging back a,b,c gives the desired result.
B.2 Proof of Theorem 5.8
n
1 (cid:88)
Consider ERM sˆ:= argmin ℓ(z ;s), where ℓ(z;s) := E ∥s(x)−∇logq(x|z)∥2 and {z }
n i q(x|z) i
s∈S
i=1
are i.i.d. samples of µ(z). The population loss is given by ℓ(s) := E ∥s(x)−∇q(x|z)∥2 =
µ(z)q(x|z)
26E ∥s(x) − ∇logµˆ(x)∥2 + c , where c = E ∥∇logµˆ(x) − ∇logq(x|z)∥2 is a constant
µˆ(x) ∗ ∗ µ(z)q(x|z)
independent of s.
Proof. Recall that ℓ(z;s) = E ∥s(x)−∇logq(x|z)∥2. Let R > 0 be some constant defined later
q(x|z)
and the corresponding truncated loss ℓtr(z;s) := ℓ(z;s)1 . For any s ∈ S,z ∈ Rd, we have
{∥z∥∞≤R}
ℓ(z;s) ≤ 2E (cid:2) ∥s(x)∥2+∥∇logq(x|z)∥2(cid:3) ≤ 2(M2+d/σ2) =: b. (B.35)
q(x|z)
For the minimizer of empirical score matching sˆ,
ℓ(sˆ) = E ℓtr(sˆ;z)+E ℓ(sˆ;z)1(∥z∥ > R)
µ(z) µ(z) ∞
(B.36)
≤ E ℓtr(sˆ;z)+P(∥z∥ > R)b
µ(z) ∞
By Assumption 5.7, P(∥z∥ > R) ≤ Cexp(−R2/C) for some constant C. Hence with probability
∞
no less than 1 − Cnexp(−R2/C), for all 1 ≤ i ≤ n, we have ∥z ∥ ≤ R and consequently
i ∞
n n
1 (cid:88) 1 (cid:88)
sˆ= argmin ℓ(z ;s) = argmin ℓtr(z ;s) =: argminℓˆtr(s).
i i
n n
s∈S s∈S s∈S
i=1 i=1
Define the empirical Rademacher complexity of a function class F as
n
(cid:12)1 (cid:88) (cid:12)
R (F) := E sup(cid:12) σ f(z )(cid:12), σ ∼ Unif({−1,1}n). (B.37)
n σ (cid:12)n i i (cid:12)
f∈F
i=1
For r > 0, let S := {s ∈ S : ℓˆtr(s) ≤ r} and F = (cid:8) ℓtr(·;s) : s ∈ S (cid:9) . Note that for any f,g ∈ F ,
r r r r
n n (cid:115)
(cid:13) 1 (cid:88) 1 (cid:88) (cid:13) 1 (cid:88)
(cid:13)√ σ f(z )− √ σ g(z )(cid:13) ≲ ∥f(z )−g(z )∥2 ≲ ∥f −g∥ . (B.38)
(cid:13) n
i=1
i i n
i=1
i i (cid:13) ψ2 n
i
i i L2(P n)
√
and diam(F ,∥·∥ ) ≤ 2 br. Then by Dudley’s bound [Srebro et al., 2010; Wainwright, 2019],
r L2(P n)
we have
 √ (cid:115) 
R (F ) ≲ inf
 α+(cid:90) 2 br logN(F r,∥·∥ L2(P n),ε) dε
. (B.39)
n r
α≥0 α n 
√
By Lemma B.6, for any K ≥ 2R and α := CM drexp(−C′K2/σ2),
K
 √ (cid:115) 
R (F ) ≲ inf
 α+(cid:90) 2 br logN(F r,∥·∥ L2(P n),ε) dε
n r
α≥αK α n 
 √ (cid:115) √ 
 (cid:90) 2 br logN(S ,∥·∥ ,ε/ 4r) 
≲ inf α+ r L∞([−K,K]d) dε
α≥αK α n  (B.40)
 √ (cid:115) 
 (cid:90) 2 br SLlog(LWKBr) 
≲ inf α+ ϵ dε
α≥αK α n 
(cid:114)
brSLlog(LWKBr)
≲ +α .
K
n
27(cid:110)
1
(cid:0)Mdn(cid:1)(cid:111)
Let K = max 2R,Cσlog2 and we finally obtain
bSL
(cid:115)
(cid:0) (cid:1)
brSLlog LWRBrσlog(dn)
R (F ) ≲ . (B.41)
n r
n
By Bousquet [2002, Theorem 6.1], with probability at least 1−δ, for all s ∈ S,
(cid:32)(cid:114) (cid:33)
γ +blog(1/δ) γ +blog(1/δ)
ℓtr(s) ≤ ℓˆtr(s)+O ℓˆtr(s)· R + R , (B.42)
n n
whereγ = bSLlog(cid:0) LWRBσlog(dn)(cid:1) . Hencewithprobabilitynolessthan1−Cnexp(−R2/C)−δ,
R
we have
(cid:115) 
γ +blog(1/δ) γ +blog(1/δ)
ℓtr(sˆ) ≤ inf ℓtr(s)+O inf ℓtr(s)· R + R 
s∈S s∈S n n
(B.43)
(cid:115) 
γ +log(1/δ) γ +blog(1/δ)
R R
≤ inf ℓ(s)+O inf ℓ(s)· + .
s∈S s∈S n n
1
Combining it with (B.36) and defining R = Clog2(n/δ), we conclude that with probability at least
1−2δ,
(cid:115) 
(cid:0) (cid:1)
bSLlog LWBσlog(dn/δ) +blog(1/δ)
ℓ(s) ≤ inf ℓ(s)+O inf ℓ(s)· . (B.44)
s∈S s∈S n
Or equivalently,
(cid:115) 
(cid:0) (cid:1)
bSLlog LWBσlog(dn/δ) +blog(1/δ)
ℓ sm(s) ≤ inf ℓ sm(s)+O (inf ℓ sm(s)+c ∗)· 
s∈S s∈S n
(B.45)
(cid:115) 
(cid:0) (cid:1)
bSLlog LWBσlog(dn/δ) +blog(1/δ)
≤ 2inf ℓ sm(s)+O c ∗· .
s∈S n
√
LemmaB.6. ThereexistsconstantsC,C′ > 0suchthat, foranyK ≥ 2R,ε ≥ C drM exp(−C′K2/σ2),
√
N(F ,∥·∥ ,ε) ≤ N(S ,∥·∥ ,ε/ 4r) (B.46)
r L2(P n) r L∞([−K,K]d)
√
Proof. Given an (ε/ 4r)-net {{s }N in (S ,∥·∥ ), we aim to show that {ℓtr(·;s )}N
j j=1 r L∞([−K,K]d) j j=1
is an ε-net in (F ,∥·∥ ). In fact, for any s ∈ S , there exists s such that ∥s −s∥ ≤
r L2(P n) r j j L∞([−K,K]d)
28√
ε/ 4r. Therefore, for any z ∈ [−R,R]d,
E q(x|z)(cid:13) (cid:13)s(x)−s j(x)(cid:13) (cid:13)2 ≤ E q(x|z)(cid:13) (cid:13)s(x)−s j(x)(cid:13) (cid:13)21 {∥x−z∥∞≤K/2}+M2P(∥x−z∥
∞
> K/2)
≤ ∥s−s ∥2 +CdM2exp(−K2/Cσ2)
j L∞([−K,K]d)
(B.47)
≤ ε2/4r+CdM2exp(−C′K2/σ2)
≤ ε2/2r.
Note that in the second inequality we use K ≥ 2R and probability of Gaussian-tail. Hence,
(cid:118)
(cid:117) n
∥ℓtr(·;s)−ℓtr(·;s j)∥ L2(P n) ≤ (cid:117) (cid:116) n1 (cid:88)(cid:104) E q(x|zi)(cid:0) s(x)−s j(x)(cid:1)T(cid:0) s(x)+s j(x)−2∇logq(x|z i)(cid:1)(cid:105)2
i=1
(cid:118)
(cid:117) n
≤ (cid:117) (cid:116) n1 (cid:88) E q(x|zi)(cid:13) (cid:13)s(x)−s j(x)(cid:13) (cid:13)2E q(x|zi)(cid:13) (cid:13)s(x)+s j(x)−2∇logq(x|z i)(cid:13) (cid:13)2
i=1
(cid:112)
≤ ε2/2r·2r = ε.
(B.48)
This concludes the proof.
C Additional Details of Experiments
C.1 Gaussian Mixture
For the 2D experiment, the target marginal probability of each cluster is 1/5, the mean of each
cluster is randomly sampled from the standard gaussian distribution, the standard deviations of the
clusters are 0.1, 0.2, 0.3, 0.4, 0.5. The initial distribution is the gaussian distribution N((3,0),0.25I).
The number of particles is 1000.
For the neural network structure, we follow the setting of Cheng et al. [2023]. For L -GF,
2
Ada-GWG and Ada-SIFG, we parameterize f as 3-layer neural networks with tanh activation
γ
function. Each hidden layer has 32 neurons. The inner loop iteration is 5 and we use SGD optimizer
with Nesterov momentum (momentum 0.9) to train f with learning rate η=1e-3. For L -GF and
γ 2
Ada-GWG, the particle step size is 0.1. For Ada-SIFG, the particle step size is 0.01.
For Ada-GWG, we set the initial exponent p = 2 and learning rate η˜= 5e-7. For Ada-SIFG,
0
we set the initial noise magnitude σ = 0.12 and learning rate ηˆ= 1e-9.
0
For the 10D experiment, the target marginal probability of each cluster is 1/5, the mean of each
cluster is randomly sampled from the standard gaussian distribution, the standard deviations of the
clusters are 0.1, 0.2, 0.3, 0.4, 0.5. The initial distribution is the standard gaussian distribution. The
29number of particles is 1000. The neural network structure is the same as which in the 2D Gaussian
mixture experiment for L -GF, Ada-GWG, SIFG and Ada-SIFG. For L -GF and Ada-GWG, the
2 2
particle step size is 0.1. For SIFG and Ada-SIFG, the particle step size is 0.01.
For Ada-GWG, we set the initial exponent p = 2 and learning rate η˜= 5e-7. For SIFG, we
0
set the initial noise magnitude σ = 0.1. For Ada-SIFG, we align with the initial noise magnitude
0
σ = 0.1 of SIFG, and choose the learning rate ηˆ= 1e-9.
0
We run the experiment on 5 random seeds. The average results and the variances are represented
in the figure using lines and shades.
C.2 Monomial Gamma
We follow the same setting as Cheng et al. [2023]. The number of particles is 1000. For L -GF
2
and Ada-GWG, the neural network structure and its training optimizer are the same as which in
the Gaussian mixture experiment. For all methods, the inner loop iteration is 5. For L -GF and
2
Ada-GWG, we use Adam optimizer with learning rate η=1e-3 to update the particles for better
stability. For SIFG and Ada-SIFG, we use SGD optimizer with learning rate η=1e-2.
For Ada-GWG, we set the initial exponent p = 2 and learning rate η˜= 1. For SIFG, we set the
0
initial noise magnitude σ = 0.1. For Ada-SIFG, we align with the initial noise magnitude σ = 0.1
0 0
of SIFG, and choose the learning rate ηˆ= 1e-10.
We run the experiment on 5 random seeds. The average results and the variances are represented
in the figure using lines and shades.
C.3 Independent Component Analysis
We follow the setting of Korba et al. [2021]. For L -GF, Ada-GWG, SIFG and Ada-SIFG, we
2
parameterize f as 3-layer neural networks with tanh nonlinearities. Each hidden layer has 120
γ
neurons. The inner loop N′ is 20. We use SGD optimizer with learning rate η=1e-3 to train f .
γ
The particle step size is 1e-3.
For Ada-GWG, we set the initial exponent p = 2 and learning rate η˜ = 0.008 (chosen from
0
{0.004,0.008,0.012} for the best performance). We clip the exponent p within [1.1,6]. The gradient
of A(p) is also clipped within [−0.1,0.1].
For SIFG and Ada-SIFG, we set the initial noise magnitude σ = 0.03. For Ada-SIFG, the
0
learning rate for σ is 2e-6 (chosen from {1e-6, 2e-6, 4e-6} to obtain the best performance).
30∥x−y∥2
For SVGD, we use RBF kernel exp(− ) where h is the heuristic bandwidth [Liu and
h
Wang, 2016]. For 10 particles, the particle step size is 1e-3 (chosen from {1e-2, 1e-3, 1e-4} to obtain
the best performance). For 100 particles, the initial step size is 1e-1 (chosen from {5e-2, 1e-1, 2e-1}
to obtain the best performance).
Additionally, we run standard HMC with step size 5e-4 for 10000 iterations as the ground truth
for the posterior distribution.
C.4 Bayesian Neural Networks
We follow the settings of Cheng et al. [2023]. For the UCI datasets, the datasets are randomly
partitioned into 90% for training and 10% for testing. Then, we further split the training dataset
by 10% to create a validation set for hyper-parameter selection as done in Cheng et al. [2023]. We
select the step size of particle updates from {1e-4, 2e-4, 5e-4, 1e-3} for the best performance. For
L -GF, Ada-GWG and Ada-SIFG, we parameterize f as 3-layer neural networks. Each hidden
2 γ
layer has 300 neurons, and we use LeakyReLU as the activation function with a negative slope of
0.1. The inner loop N′ is 10. We use the Adam optimizer and choose the learning rate from {1e-3,
1e-4} to train f .
γ
For Ada-GWG, we choose the initial exponent p to be 2 and set the learning rate η˜ to be
0
1e-4. The gradient of A(p) is clipped within [-0.2, 0.2]. For Ada-SIFG, we choose the initial noise
magnitude σ from {0.1,0.01}. The learning rate for σ is chosen from {2e-5, 4e-5, 1e-4} for the best
performance. For SVGD, we use the RBF kernel as done in [Liu and Wang, 2016]. For methods
except SGLD, the iteration number is chosen to be 2000 to converge. For SGLD, the iteration
number is set to 10000 to converge.
31