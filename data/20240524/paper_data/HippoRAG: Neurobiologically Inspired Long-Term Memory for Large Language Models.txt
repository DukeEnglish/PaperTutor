HippoRAG: Neurobiologically Inspired
Long-Term Memory for Large Language Models
BernalJiménezGutiérrez YihengShu
TheOhioStateUniversity TheOhioStateUniversity
jimenezgutierrez.1@osu.edu shu.251@osu.edu
YuGu MichihiroYasunaga YuSu
TheOhioStateUniversity StanfordUniversity TheOhioStateUniversity
gu.826@osu.edu myasu@cs.stanford.edu su.809@osu.edu
Abstract
Inordertothriveinhostileandever-changingnaturalenvironments,mammalian
brainsevolvedtostorelargeamountsofknowledgeabouttheworldandcontinually
integrate new information while avoiding catastrophic forgetting. Despite the
impressiveaccomplishments,largelanguagemodels(LLMs),evenwithretrieval-
augmentedgeneration(RAG),stillstruggletoefficientlyandeffectivelyintegrate
alargeamountofnewexperiencesafterpre-training. Inthiswork,weintroduce
HippoRAG, a novel retrieval framework inspired by the hippocampal indexing
theoryofhumanlong-termmemorytoenabledeeperandmoreefficientknowledge
integrationovernewexperiences. HippoRAGsynergisticallyorchestratesLLMs,
knowledgegraphs,andthePersonalizedPageRankalgorithmtomimicthedifferent
rolesofneocortexandhippocampusinhumanmemory. WecompareHippoRAG
withexistingRAGmethodsonmulti-hopquestionansweringandshowthatour
methodoutperformsthestate-of-the-artmethodsremarkably,byupto20%. Single-
step retrieval with HippoRAG achieves comparable or better performance than
iterativeretrievallikeIRCoTwhilebeing10-30timescheaperand6-13timesfaster,
andintegratingHippoRAGintoIRCoTbringsfurthersubstantialgains. Finally,
weshowthatourmethodcantacklenewtypesofscenariosthatareoutofreachof
existingmethods.1
1 Introduction
Millionsofyearsofevolutionhaveledmammalianbrainstodevelopthecrucialabilitytostorelarge
amountsofworldknowledgeandcontinuouslyintegratenewexperienceswithoutlosingprevious
ones. Thisexceptionallong-termmemorysystemeventuallyallowsushumanstokeepvaststoresof
continuouslyupdatingknowledgethatformsthebasisofourreasoninganddecisionmaking[15].
Despitetheprogressoflargelanguagemodels(LLMs)inrecentyears,suchacontinuouslyupdating
long-termmemoryisstillconspicuouslyabsentfromcurrentAIsystems. Dueinparttoitseaseof
useandthelimitationsofothertechniquessuchasmodelediting[35],retrieval-augmentedgeneration
(RAG)hasbecomethedefactosolutionforlong-termmemoryinLLMs,allowinguserstopresent
newknowledgetoastaticmodel[28,33,50].
However,currentRAGmethodsarestillunabletohelpLLMsperformtasksthatrequireintegrating
newknowledgeacrosspassageboundariessinceeachnewpassageisencodedinisolation. Many
1Codeanddataareavailableathttps://github.com/OSU-NLP-Group/HippoRAG.
Preprint.Underreview.
4202
yaM
32
]LC.sc[
1v13841.5042:viXraOffline Indexing Online Retrieval
& % " & Which Stanford professor works on the Answer: !
neuroscience of Alzheimer’s?
$ ! ' # Prof. Thomas
Current & % !
RAG ' # "
!
Human
!
Memory
HippoRAG "
!
#
$
! !
!
%
Figure1: KnowledgeIntegration&RAG.Tasksthatrequireknowledgeintegrationareparticularly
challengingforcurrentRAGsystems. Intheaboveexample,wewanttofindaStanfordprofessor
thatdoesAlzheimer’sresearchfromapoolofpassagesdescribingpotentiallythousandsStanford
professorsandAlzheimer’sresearchers. Sincecurrentmethodsencodepassagesinisolation,they
wouldstruggletoidentifyProf. Thomasunlessapassagementionsbothcharacteristicsatonce. In
contrast,mostpeoplefamiliarwiththisprofessorwouldrememberhimquicklyduetoourbrain’s
associativememorycapabilities,thoughttobedrivenbytheindexstructuredepictedintheC-shaped
hippocampusabove(inblue). Inspiredbythismechanism,HippoRAGallowsLLMstobuildand
leverageasimilargraphofassociationstotackleknowledgeintegrationtasks.
important real-world tasks, such as scientific literature review, legal case briefing, and medical
diagnosis, require knowledge integration across passages or documents. Although less complex,
standardmulti-hopquestionanswering(QA)alsorequiresintegratinginformationbetweenpassages
in a retrieval corpus. In order to solve such tasks, current RAG systems resort to using multiple
retrievalandLLMgenerationstepsiterativelytojoindisparatepassages[49,61]. Nevertheless,even
perfectlyexecutedmulti-stepRAGisstilloftentimesinsufficienttoaccomplishmanyscenariosof
knowledgeintegration,asweillustrateinwhatwecallpath-findingmulti-hopquestionsinFigure1.
Incontrast,ourbrainsarecapableofsolvingchallengingknowledgeintegrationtaskslikethesewith
relativeease. Thehippocampalmemoryindexingtheory[58],awell-establishedtheoryofhuman
long-termmemory,offersoneplausibleexplanationforthisremarkableability. TeylerandDiscenna
[58]proposethatourpowerfulcontext-based,continuallyupdatingmemoryreliesoninteractions
betweentheneocortex,whichprocessesandstoresactualmemoryrepresentations,andtheC-shaped
hippocampus,whichholdsthehippocampalindex,asetofinterconnectedindiceswhichpointto
memoryunitsontheneocortexandstoresassociationsbetweenthem[15,59].
Inthiswork,weproposeHippoRAG,aRAGframeworkthatservesasalong-termmemoryforLLMs
bymimickingthismodelofhumanmemory. Ournoveldesignfirstmodelstheneocortex’sabilityto
processperceptualinputbyusinganLLMtotransformacorpusintoaschemalessknowledgegraph
(KG)asourartificialhippocampalindex. Givenanewquery,HippoRAGidentifiesthekeyconcepts
inthequeryandrunsthePersonalizedPageRank(PPR)algorithm[23]ontheKG,usingthequery
conceptsastheseeds,tointegrateinformationacrosspassagesforretrieval. PPRenablesHippoRAG
toexploreKGpathsandidentifyrelevantsubgraphs,essentiallyperformingmulti-hopreasoningina
singleretrievalstep.
Thiscapacityforsingle-stepmulti-hopretrievalyieldsstrongperformanceimprovementsofaround3
and20pointsovercurrentRAGmethods[8,27,41,53,54]ontwopopularmulti-hopQAbenchmarks,
MuSiQue[60]and2WikiMultiHopQA[25]. Additionally,HippoRAG’sonlineretrievalprocessis
10to30timescheaperand6to13timesfasterthancurrentiterativeretrievalmethodslikeIRCoT
[61],whilestillachievingcomparableperformance. Furthermore,ourapproachcanbecombined
2withIRCoTtoprovidecomplementarygainsofupto4%and20%onthesamedatasetsandeven
obtainimprovementsonHotpotQA,alesschallengingmulti-hopQAdataset. Finally,weprovide
acasestudyillustratingthelimitationsofcurrentmethodsaswellasourmethod’spotentialonthe
previouslydiscussedpath-findingmulti-hopQAsetting.
2 HippoRAG
Inthissection,wefirstgiveabriefoverviewofthehippocampalmemoryindexingtheory,followed
byhowHippoRAG’sindexingandretrievaldesignwasinspiredbythistheory,andfinallyoffera
moredetailedaccountofourmethodology.
2.1 TheHippocampalMemoryIndexingTheory
Thehippocampalmemoryindexingtheory[58]isawell-establishedtheorythatprovidesafunctional
descriptionofthecomponentsandcircuitryinvolvedinhumanlong-termmemory. Inthistheory,
TeylerandDiscenna[58]proposethathumanlong-termmemoryiscomposedofthreecomponents
thatworktogethertoaccomplishtwomainobjectives: patternseparation,whichensuresthatthe
representationsofdistinctperceptualexperiencesareunique,andpatterncompletion,whichenables
theretrievalofcompletememoriesfrompartialstimuli[15,59].
The theory suggests that pattern separation is primarily accomplished in the memory encoding
process,whichstartswiththeneocortexreceivingandprocessingperceptualstimuliintomoreeasily
manipulatable,likelyhigher-level,features,whicharethenroutedthroughtheparahippocampal
regions(PHR)tobeindexedbythehippocampus. Whentheyreachthehippocampus,salientsignals
areincludedinthehippocampalindexandassociatedwitheachother.
Afterthememoryencodingprocessiscompleted,patterncompletiondrivesthememoryretrieval
processwheneverthehippocampusreceivespartialperceptualsignalsfromthePHRpipeline. The
hippocampus then leverages its context-dependent memory system, thought to be implemented
throughadenselyconnectednetworkofneuronsintheCA3sub-region[59],toidentifycompleteand
relevantmemorieswithinthehippocampalindexandroutethembackthroughthePHRforsimulation
intheneocortex. Thus,thiscomplexprocessallowsfornewinformationtobeintegratedbychanging
onlythehippocampalindexinsteadofupdatingneocorticalrepresentations.
2.2 Overview
Ourproposedapproach,HippoRAG,iscloselyinspiredbytheprocessdescribedabove. Asshown
inFigure2,eachcomponentofourmethodcorrespondstooneofthethreecomponentsofhuman
long-termmemory. AdetailedexampleoftheHippoRAGprocesscanbefoundinAppendixA.
OfflineIndexing. Ourofflineindexingphase,analogoustomemoryencoding,startsbyleveraginga
stronginstruction-tunedLLM,ourartificialneocortex,toextractknowledgegraph(KG)triples. The
KGisschemalessandthisprocessisknownasopeninformationextraction(OpenIE)[3,4,45,79].
This process extracts salient signals from passages in a retrieval corpus as discrete noun phrases
rather than dense vector representations, allowing for more fine-grained pattern separation. It is
thereforenaturaltodefineourartificialhippocampalindexasthisopenKG,whichisbuiltonthe
wholeretrievalcorpuspassage-by-passage. Finally,toconnectbothcomponentsasisdonebythe
parahippocampal regions, we use off-the-shelf dense encoders fine-tuned for retrieval (retrieval
encoders). Theseretrievalencodersprovideadditionaledgesbetweensimilarbutnotidenticalnoun
phraseswithinthisKGtoaidindownstreampatterncompletion.
Online Retrieval. These same three components are then leveraged to perform online retrieval
bymirroringthehumanbrain’smemoryretrievalprocess. Justasthehippocampusreceivesinput
processedthroughtheneocortexandPHR,ourLLM-basedneocortexextractsasetofsalientnamed
entitiesfromaquerywhichwecallquerynamedentities. Thesenamedentitiesarethenlinkedto
nodesinourKGbasedonthesimilaritydeterminedbyretrievalencoders;werefertotheseselected
nodesasquerynodes. Oncethequerynodesarechosen,theybecomethepartialcuesfromwhichour
synthetichippocampusperformspatterncompletion. Inthehippocampus,neuralpathwaysbetween
elementsofthehippocampalindexenablerelevantneighborhoodstobecomeactivatedandrecalled
upstream. Toimitatethisefficientgraphsearchprocess, weleveragethePersonalizedPageRank
3Neocortex ParahippocampalRegions Hippocampus
LLM Retrieval Encoders KG + Personalized PageRank
Passages (Thomas, (!,
Offline & Are lzs he ea ir mch ee r’s s, ) ) " "
# #
Indexing & ( eS mta pn lofo yr sd ,, ( , $ ! $
Thomas) !) ! !
Open IE % %
Online Stanford Node "
Retrieval Query Specificity $ #
Alzheimer’s ! !
NER !
%
Figure2: DetailedHippoRAGMethodology. Wemodelthethreecomponentsofhumanlong-term
memorytomimicitspatternseparationandcompletionfunctions. Forofflineindexing(Middle),
we use an LLM to process passages into open KG triples, which are then added to our artificial
hippocampalindex,whileoursyntheticparahippocampalregions(PHR)detectsynonymy. Inthe
exampleabove,triplesinvolvingProfessorThomasareextractedandintegratedintotheKG.For
online retrieval (Bottom), our LLM neocortex extracts named entities from a query while our
parahippocampal retrieval encoders link them to our hippocampal index. We then leverage the
PersonalizedPageRankalgorithmtoenablecontext-basedretrievalandextractProfessorThomas.4
(PPR)algorithm[23],aversionofPageRankthatdistributesprobabilityacrossagraphonlythrough
asetofuser-definedsourcenodes. ThisconstraintallowsustobiasthePPRoutputonlytowardsthe
setofquerynodes,justasthehippocampusextractsassociatedsignalsfromspecificpartialcues.2
Finally,asisdonewhenthehippocampalsignalissentupstream,weaggregatetheoutputPPRnode
probabilityoverthepreviouslyindexedpassagesandusethattorankthemforretrieval.
2.3 DetailedMethodology
OfflineIndexing. OurindexingprocessinvolvesprocessingasetofpassagesP usinganinstruction-
tunedLLMLandaretrievalencoderM. AsseeninFigure2wefirstuseLtoextractasetofnoun
phrasenodesN andrelationedgesE fromeachpassageinP viaOpenIE.Thisprocessisdonevia
1-shotpromptingoftheLLMwiththepromptsshowninAppendixI.Specifically,wefirstextracta
setofnamedentitiesfromeachpassage. WethenaddthenamedentitiestotheOpenIEpromptto
extractthefinaltriples,whichalsocontainconcepts(nounphrases)beyondnamedentities. Wefind
thatthistwo-steppromptconfigurationleadstoanappropriatebalancebetweengeneralityandbias
towardsnamedentities. Finally,weuseM toaddtheextrasetofsynonymyrelationsE′discussed
abovewhenthecosinesimilaritybetweentwoentityrepresentationsinN isaboveathresholdτ. As
statedabove,thisintroducesmoreedgestoourhippocampalindexandallowsformoreeffective
patterncompletion. Thisindexingprocessdefinesa|N|×|P|matrixP,whichcontainsthenumber
oftimeseachnounphraseintheKGappearsineachoriginalpassage.
OnlineRetrieval. Duringtheretrievalprocess,wepromptLusinga1-shotprompttoextractaset
ofnamedentitiesfromaqueryq, ourpreviouslydefinedquerynamedentitiesC = {c ,...,c }
q 1 n
(StanfordandAlzheimer’sinourFigure2example). ThesenamedentitiesC fromthequeryare
q
then encoded by the same retrieval encoder M. Then, the previously defined query nodes are
chosen as the set of nodes in N with the highest cosine similarity to the query named entities
C . More formally, query nodes are defined as R = {r ,...,r } such that r = e where k =
q q 1 n i k
argmax cosine_similarity(M(c ),M(e )),representedastheStanfordlogoandtheAlzheimer’s
j i j
purpleribbonsymbolinFigure2
2Intriguingly,someworkincognitivesciencehasalsofoundacorrelationbetweenhumanwordrecalland
theoutputofthePageRankalgorithm[19].
4AfterthequerynodesR arefound,werunthePPRalgorithmoverthehippocampalindex,i.e.,a
q
KGwith|N|nodesand|E|+|E′|edges(triple-basedandsynonymy-based),usingapersonalized
#»
probabilitydistribution n definedoverN,inwhicheachquerynodehasequalprobabilityandall
othernodeshaveaprobabilityofzero. Thisallowsprobabilitymasstobedistributedtonodesthatare
primarilyinthe(joint)neighborhoodofthequerynodes,suchasProfessorThomas,andcontributeto
eventualretrieval. AfterrunningthePPRalgorithm,weobtainanupdatedprobabilitydistribution
#» #»
n′overN. Finally,inordertoobtainpassagescores,wemultiplyn′withthepreviouslydefinedP
#»
matrixtoobtain p,arankingscoreforeachpassage,whichweuseforretrieval.
Node Specificity. We introduce node specificity as a neurobiologically plausible way to further
improveretrieval. Itiswellknownthatglobalsignalsforwordimportance,likeinversedocument
frequency(IDF),canimproveinformationretrieval. However,inorderforourbraintoleverageIDF
for retrieval, the number of total “passages” encoded would need to be aggregated with all node
activationsbeforememoryretrievaliscomplete. Whilesimplefornormalcomputers,thisprocess
wouldrequireactivatingconnectionsbetweenanaggregatorneuronandallnodesinthehippocampal
index every time retrieval occurs, likely introducing prohibitive computational overhead. Given
theseconstraints,weproposenodespecificityasanalternativeIDFsignalwhichrequiresonlylocal
signalsandisthusmoreneurobiologicallyplausible. Wedefinethenodespecificityofnodeias
s =|P |−1,whereP isthesetofpassagesinP fromwhichnodeiwasextracted,informationthat
i i i
isalreadyavailableateachnode. Nodespecificityisusedinretrievalbymultiplyingeachquerynode
#»
probability n withs beforePPR;thisallowsustomodulateeachoftheirneighborhood’sprobability
i
aswellastheirown. WeillustratenodespecificityinFigure2throughrelativesymbolsize: the
StanfordlogogrowslargerthantheAlzheimer’ssymbolsinceitappearsinfewerdocuments.
3 ExperimentalSetup
3.1 Datasets
Weevaluateourmethod’sretrievalcapabilitiesprimarilyontwochallengingmulti-hopQAbench-
marks, MuSiQue (answerable) [60] and 2WikiMultiHopQA [25]. For completeness, we also
include the HotpotQA [70] dataset even though it has been found to be a much weaker test for
multi-hopreasoningduetomanyspurioussignals[60],aswealsoshowinAppendixB.Tolimit
theexperimentalcost,weextract1,000questionsfromeachvalidationsetasdoneinpreviouswork
[48,61]. Inordertocreateamorerealisticretrievalsetting,wefollowIRCoT[61]andcollectall
candidatepassages(includingsupportinganddistractorpassages)fromourselectedquestionsand
formaretrievalcorpusforeachdataset. ThedetailsofthesedatasetsareshowninTable1.
Table1: RetrievalcorporaandextractedKGstatisticsforeachofour1,000questiondevsets.
MuSiQue 2Wiki HotpotQA
#ofPassages(P) 11,656 6,119 9,221
#ofUniqueNodes(N) 91,729 42,694 82,157
#ofUniqueEdges(E) 21,714 7,867 17,523
#ofUniqueTriples 107,448 50,671 98,709
#ofContrieverSynonymEdges(E′) 145,990 146,020 159,112
#ofColBERTv2SynonymEdges(E′) 191,636 82,526 171,856
3.2 Baselines
Wecompareagainstseveralstrongandwidelyusedretrievalmethods: BM25[52],Contriever[27],
GTR[41]andColBERTv2[53]. Additionally,wecompareagainsttworecentLLM-augmented
baselines: Propositionizer[8],whichrewritespassagesintopropositions,andRAPTOR[54],which
constructs summary nodes to ease retrieval from long documents. In addition to the single-step
retrievalmethodsabove,wealsoincludethemulti-stepretrievalmethodIRCoT[61]asabaseline.
4Manydetailsaroundthehippocampalmemoryindexingtheoryareomittedfromthisstudyforsimplicity.
Weencourageinterestedreadertofollowthereferencesin§2.1formore.
5Table2: Single-stepretrievalperformance. HippoRAGoutperformsallbaselinesonMuSiQueand
2WikiMultiHopQAandachievescomparableperformanceonthelesschallengingHotpotQAdataset.
MuSiQue 2Wiki HotpotQA Average
R@2 R@5 R@2 R@5 R@2 R@5 R@2 R@5
BM25[52] 32.3 41.2 51.8 61.9 55.4 72.2 46.5 58.4
Contriever[27] 34.8 46.6 46.6 57.5 57.2 75.5 46.2 59.9
GTR[41] 37.4 49.1 60.2 67.9 59.4 73.3 52.3 63.4
ColBERTv2[53] 37.9 49.2 59.2 68.2 64.7 79.3 53.9 65.6
RAPTOR[54] 35.7 45.3 46.3 53.8 58.1 71.2 46.7 56.8
Proposition[8] 37.6 49.3 56.4 63.1 58.7 71.1 50.9 61.2
HippoRAG(Contriever) 41.0 52.1 71.5 89.5 59.0 76.2 57.2 72.6
HippoRAG(ColBERTv2) 40.9 51.9 70.7 89.1 60.5 77.7 57.4 72.9
Table3:Multi-stepretrievalperformance.CombiningHippoRAGwithstandardmulti-stepretrieval
methodslikeIRCoTresultsinsignificantimprovementsonallthreedatasets.
MuSiQue 2Wiki HotpotQA Average
R@2 R@5 R@2 R@5 R@2 R@5 R@2 R@5
IRCoT+BM25(Default) 34.2 44.7 61.2 75.6 65.6 79.0 53.7 66.4
IRCoT+Contriever 39.1 52.2 51.6 63.8 65.9 81.6 52.2 65.9
IRCoT+ColBERTv2 41.7 53.7 64.1 74.4 67.9 82.0 57.9 70.0
IRCoT+HippoRAG(Contriever) 43.9 56.6 75.3 93.4 65.8 82.3 61.7 77.4
IRCoT+HippoRAG(ColBERTv2) 45.3 57.6 75.8 93.9 67.0 83.0 62.7 78.2
3.3 Metrics
WereportretrievalandQAperformanceonthedatasetsaboveusingrecall@2andrecall@5(R@2
andR@5below)forretrievalandexactmatch(EM)andF1scoresforQAperformance.
3.4 ImplementationDetails
Bydefault,weuseGPT-3.5-turbo-1106[42]withtemperatureof0asourLLMLandContriever
[27]orColBERTv2[53]asourretrieverM. Weuse100examplesfromMuSiQue’strainingdata
totuneHippoRAG’stwohyperparameters: thesynonymythresholdτ at0.8andthePPRdamping
factorat0.5,whichdeterminestheprobabilitythatPPRwillrestartarandomwalkfromthequery
nodesinsteadofcontinuingtoexplorethegraph. Generally,wefindthatHippoRAG’sperformance
isratherrobusttoitshyperparameters. MoreimplementationdetailscanbefoundinAppendixH.
4 Results
WepresentourretrievalandQAexperimentalresultsbelow. Giventhatourmethodindirectlyaffects
QAperformance,wereportQAresultsonourbest-performingretrievalbackboneColBERTv2[53].
However,wereportretrievalresultsforseveralstrongsingle-stepandmulti-stepretrievaltechniques.
Single-Step Retrieval Results. As seen in Table 2, HippoRAG outperforms all other methods,
including recent LLM-augmented baselines such as Propositionizer and RAPTOR, on our main
datasets,MuSiQueand2WikiMultiHopQA,whileachievingcompetitiveperformanceonHotpotQA.
Wenoticeanimpressiveimprovementof11and20%forR@2andR@5on2WikiMultiHopQA
andaround3%onMuSiQue. Thisdifferencecanbepartiallyexplainedby2WikiMultiHopQA’s
entity-centricdesign,whichisparticularlywell-suitedforHippoRAG.Ourlowerperformanceon
HotpotQAismainlyduetoitslowerknowledgeintegrationrequirements,asexplainedinAppendix
B,aswellasaduetoaconcept-contexttradeoffwhichwealleviatewithanensemblingtechnique
describedinAppendixF.2.
Multi-Step Retrieval Results. For multi-step or iterative retrieval, our experiments in Table 3
demonstrate that IRCoT [61] and HippoRAG are complementary. Using HippoRAG as the re-
6Table4: QAperformance. HippoRAG’sQAimprovementscorrelatewithitsretrievalimprovements
onsingle-step(rows1-3)andmulti-stepretrieval(rows4-5).
MuSiQue 2Wiki HotpotQA Average
Retriever EM F1 EM F1 EM F1 EM F1
None 12.5 24.1 31.0 39.6 30.4 42.8 24.6 35.5
ColBERTv2 15.5 26.4 33.4 43.3 43.4 57.7 30.8 42.5
HippoRAG(ColBERTv2) 19.2 29.8 46.6 59.5 41.8 55.0 35.9 48.1
IRCoT(ColBERTv2) 19.1 30.5 35.4 45.1 45.5 58.4 33.3 44.7
IRCoT+HippoRAG(ColBERTv2) 21.9 33.3 47.7 62.7 45.7 59.2 38.4 51.7
triever for IRCoT continues to bring R@5 improvements of around 4% for MuSiQue, 18% for
2WikiMultiHopQAandanadditional1%onHotpotQA.
QuestionAnsweringResults. WereportQAresultsforHippoRAG,thestrongestretrievalbaselines,
ColBERTv2andIRCoT,aswellasIRCoTusingHippoRAGasaretrieverinTable4. Asexpected,
improved retrieval performance in both single and multi-step settings leads to strong overall im-
provementsofupto3%,17%and1%F1scoresonMuSiQue,2WikiMultiHopQAandHotpotQA
respectivelyusingthesameQAreader. Notably,single-stepHippoRAGisonparoroutperforms
IRCoTwhilebeing10-30timescheaperand6-13timesfasterduringonlineretrieval(AppendixG).
5 Discussions
5.1 WhatMakesHippoRAGWork?
OpenIEAlternatives. TodetermineifusingGPT-3.5isessentialforbuildingourKG,wereplace
it with an end-to-end OpenIE model REBEL [26] and an instruction-tuned LLM Llama-3 [1].
As shown in Table 5 row 2, building our KG using REBEL results in large performance drops,
underscoringtheimportanceofLLMflexibility. Specifically,GPT-3.5producestwiceasmanytriples
as REBEL, indicating its bias against producing triples with general concepts and leaving many
usefulassociationsbehind.
Intermsofopen-sourceLLMs,Table5(rows3-4)showsthattheperformanceofLlama-38Bis
comparable to GPT-3.5, although its 70B counterpart performs worse. This surprising behavior
isduetothismodel’sproductionofill-formattedoutputsthatresultinthelossofaround20%of
thepassages,comparedtoabout4%forthe8Bmodelandlessthan1%forGPT-3.5. Thestrong
performanceofLlama-38Bisencouragingbecausethatoffersacheaperalternativeforindexingover
largecorpora. ThestatisticsfortheseOpenIEalternativescanbefoundinAppendixC.
PPRAlternatives. AsshowninTable5(rows5-6),toexaminehowmuchofourresultsaredueto
#»
thestrengthofPPR,wereplacethePPRoutputwiththequerynodeprobability n multipliedbynode
specificityvalues(row5)andaversionofthisthatalsodistributesasmallamountofprobabilitytothe
Table5: DissectingHippoRAG.Tounderstandwhatmakesitworkwell,wereplaceitsOpenIE
moduleandPPRwithplausiblealternativesandablatenodespecificityandsynonymy-basededges.
MuSiQue 2Wiki HotpotQA Average
R@2 R@5 R@2 R@5 R@2 R@5 R@2 R@5
HippoRAG 40.9 51.9 70.7 89.1 60.5 77.7 57.4 72.9
REBEL[26] 31.7 39.6 63.1 76.5 43.9 59.2 46.2 58.4
OpenIE
Llama-3-8B-Instruct[1] 39.4 50.9 71.1 88.0 59.4 76.4 56.6 71.8
Alternatives
Llama-3-70B-Instruct[1] 36.3 45.9 56.9 68.8 51.6 66.1 48.3 60.3
PPR R NodesOnly 37.1 41.0 59.1 61.4 55.9 66.2 50.7 56.2
q
Alternatives R Nodes&Neighbors 25.4 38.5 53.4 74.7 47.8 64.5 42.2 59.2
q
w/oNodeSpecificity 37.6 50.2 70.1 88.8 56.3 73.7 54.7 70.9
Ablations
w/oSynonymyEdges 40.2 50.2 69.2 85.6 59.1 75.7 56.2 70.5
7directneighborsofeachquerynode(row6). First,wefindthatPPRisamuchmoreeffectivemethod
forincludingassociationsforretrievalonallthreedatasetscomparedtobothsimplebaselines. Itis
interestingtonotethataddingtheneighborhoodofR nodeswithoutPPRleadstoworseperformance
q
thanonlyusingthequerynodesthemselves.
Ablations. AsseeninTable5(rows7-8),nodespecificityobtainsconsiderableimprovementson
MuSiQueandHotpotQAandyieldsalmostnochangein2WikiMultiHopQA.Thisislikelybecause
2WikiMultiHopQAreliesonnamedentitieswithlittledifferencesintermsoftermweighting. In
contrast,synonymyedgeshavethelargesteffecton2WikiMultiHopQA,suggestingthatnoisyentity
standardization is useful when most relevant concepts are named entities, and improvements to
synonymydetectioncouldleadtostrongerperformanceinotherdatasets.
5.2 HippoRAG’sAdvantage: Single-StepMulti-HopRetrieval
AmajoradvantageofHippoRAGoverconventionalRAGmethodsinmulti-hopQAisitsability
toperformmulti-hopretrievalinasinglestep. Wedemonstratethisbymeasuringthepercentage
of queries where all the supporting passages are retrieved successfully, a feat that can only be
accomplishedthroughsuccessfulmulti-hopreasoning. Table8inAppendixDshowsthatthegap
betweenourmethodandColBERTv2,usingthetop-5passages,increasesevenmorefrom3%to6%
onMuSiQueandfrom20%to38%on2WikiMultiHopQA,suggestingthatlargeimprovementscome
fromobtainingallsupportingdocumentsratherthanachievingpartiallyretrievalonmorequestions.
We further illustrate HippoRAG’s unique single-step multi-hop retrieval ability through the first
exampleinTable6. Inthisexample, eventhoughAlhandrawasnotmentionedinViladeXira’s
passage, HippoRAG can directly leverage Vila de Xira’s connection to Alhandra as his place of
birth to determine its importance, something that standard RAG methods would be unable to do
directly. Additionally,eventhoughIRCoTcanalsosolvethismulti-hopretrievalproblem,asshown
inAppendixG,itis10-30timesmoreexpensiveand6-13timesslowerthanoursintermsofonline
retrieval,arguablythemostimportantfactorwhenitcomestoservingendusers.
Table6:Multi-hopquestiontypes.Weshowexampleresultsfordifferentapproachesonpath-finding
vs.path-followingmulti-hopquestions.
Question HippoRAG ColBERTv2 IRCoT
Inwhich 1.Alhandra 1.Alhandra
1.Alhandra
Path- districtwas 2.Dimuthu 2.ViladeXira
2.ViladeXira
Following Alhandra Abayakoon 3.Póvoade
3.Portugal
born? 3.Ja‘ar SantaIria
WhichStanford
1.ThomasSüdhof 1.BrianKnutson 1.BrianKnutson
Path- professorworkson
2.KarlDeisseroth 2.EricKnudsen 2.EricKnudsen
Finding theneuroscience
3.RobertSapolsky 3.LisaGiocomo 3.LisaGiocomo
ofAlzheimer’s?
5.3 HippoRAG’sPotential: Path-FindingMulti-HopRetrieval
ThesecondexampleinTable6,alsopresentinFigure1,showsatypeofquestionsthatistrivial
forinformedhumansbutoutofreachforcurrentretrieverswithoutfurthertraining. Thistypeof
questions,whichwecallpath-findingmulti-hopquestions,requiresidentifyingonepathbetweena
setofentitieswhenmanypathsexisttoexploreinsteadoffollowingaspecificpath,asinstandard
multi-hopquestions.5
More specifically, a simple iterative process can retrieve the appropriate passages for the first
questionbyfollowingtheonepathsetbyAlhandra’soneplaceofbirth,asseenbyIRCoT’sperfect
performance. However, an iterative process would struggle to answer the second question given
themanypossiblepathstoexplore—eitherthroughprofessorsatStanfordUniversityorprofessors
workingontheneuroscienceofAlzheimer’s. Itisonlybyassociatingdisparateinformationabout
ThomasSüdhofthatsomeonewhoknowsaboutthisprofessorwouldbeabletoanswerthisquestion
5Path-findingquestionsrequireknowledgeintegrationwhensearchentitieslikeStanfordandAlzheimer’sdo
nothappentoappeartogetherinapassage,aconditionwhichisoftensatisfiedfornewinformation.
8easily. AsseeninTable6,bothColBERTv2andIRCoTfailtoextractthenecessarypassagessince
theycannotaccesstheseassociations.Ontheotherhand,HippoRAGleveragesitswebofassociations
initshippocampalindexandgraphsearchalgorithmtodeterminethatProfessorThomasisrelevant
tothisqueryandretrieveshispassagesappropriately. Moreexamplesofthesepath-findingmulti-hop
questionscanbefoundinourcasestudyinAppendixE.
6 RelatedWork
ParametricLong-TermMemory. Itiswell-accepted,evenamongskepticalresearchers,thatthe
parametersofmodernLLMsencodearemarkableamountofworldknowledge[2,10,17,21,24,31,
47,62],whichcanbeleveragedbyanLLMinflexibleandrobustways[64,65,74].Nevertheless,our
abilitytoupdatethisvastknowledgestore,anessentialpartofanylong-termmemorysystem,isstill
surprisinglylimited. AlthoughmanytechniquestoupdateLLMsexist,suchasstandardfine-tuning,
model unlearning and model editing [12, 37, 38, 39, 40, 76], it is clear that no methodology has
emergedasarobustsolutionforcontinuallearninginLLMs[20,35,78].
RAG as Long-Term Memory. On the other hand, using RAG methods as a long-term memory
system offers a simple way to update knowledge over time [28, 33, 50, 56]. More sophisticated
RAG methods, which perform multiple steps of retrieval and generation from an LLM, are even
abletointegrateinformationacrossneworupdatedknowledgeelements[30,49,55,61,69,71,73],
another crucial aspect of long-term memory systems. As discussed above, however, this type of
onlineinformationintegrationisunabletosolvethemorecomplexknowledgeintegrationtasksthat
weillustratewithourpath-findingmulti-hopQAexamples.
Someothermethods,suchasRAPTOR[54],MemWalker[7]andGraphRAG[14],integrateinfor-
mationduringtheofflineindexingphasesimilarlytoHippoRAGandmightbeabletohandlethese
more complex tasks. However, these methods integrate information by summarizing knowledge
elements,whichmeansthatthesummarizationprocessmustberepeatedanytimenewdataisadded.
Incontrast,HippoRAGcancontinuouslyintegratenewknowledgebysimplyaddingedgestoitsKG.
LongContextasLong-TermMemory. ContextlengthsforbothopenandclosedsourceLLMshave
increaseddramaticallyinthepastyear[9,13,16,46,51]. Thisscalingtrendseemstoindicatethat
futureLLMscouldperformlong-termmemorystoragewithinmassivecontextwindows. However,
theviabilityofthisfutureremainslargelyuncertaingiventhemanyengineeringhurdlesinvolvedand
theapparentlimitationsoflong-contextLLMs,evenwithincurrentcontextlengths[32,34,77].
LLMs&KGs. Combiningthestrengthsoflanguagemodelsandknowledgegraphshasbeenan
activeresearchdirectionformanyyears,bothforaugmentingLLMswithaKGindifferentways
[36,63,66]oraugmentingKGsbyeitherdistillingknowledgefromanLLM’sparametricknowledge
[5,67]orusingthemtoparsetextdirectly[6,22,75]. Inanexceptionallycomprehensivesurvey,
Panetal.[43]presentaroadmapforthisresearchdirectionandhighlighttheimportanceofwork
whichsynergizesthesetwoimportanttechnologies[29,57,72,80]. Liketheseworks,HippoRAG
isastrongandprincipledexampleofthesynergywemuststrikebetweenthesetwotechnologies,
combiningthepowerofLLMsforknowledgegraphconstructionwiththestrengthsofstructured
knowledgeandgraphsearchforimprovedaugmentationofanLLM’scapacities.
7 Conclusions&Limitations
Ourproposedneurobiologicallyprincipledmethodology,althoughsimple,alreadyshowspromisefor
overcomingtheinherentlimitationsofstandardRAGsystemswhileretainingtheiradvantagesover
parametricmemory. HippoRAG’sknowledgeintegrationcapabilities,demonstratedbyitsstrong
resultsonpath-followingmulti-hopQAandpromiseonpath-findingmulti-hopQA,aswellasits
dramaticefficiencyimprovementsandcontinuouslyupdatingnature,makesitapowerfulmiddle-
groundframeworkbetweenstandardRAGmethodsandparametricmemoryandoffersacompelling
solutionforlong-termmemoryinLLMs.
Nevertheless,severallimitationscanbeaddressedinfutureworktoenableHippoRAGtoachievethis
goalbetter. First,wenotethatallcomponentsofHippoRAGarecurrentlyusedoff-the-shelfwithout
any extra training. There is therefore much room to improve our method’s practical viability by
performingspecificcomponentfine-tuning.ThisisevidentintheerroranalysisdiscussedinAppendix
9FthatshowsmosterrorsmadebyoursystemareduetoNERandOpenIE,whichcouldbenefitfrom
directfine-tuning. Giventhattherestoftheerrorsaregraphsearcherrors,alsoinAppendixF,we
notethatseveralavenuesforimprovementsoversimplePPRexist,suchasallowingrelationstoguide
graphtraversaldirectly. Finally,andperhapsmostimportantly,HippoRAG’sscalabilitystillcallsfor
furthervalidation. AlthoughweshowthatLlama-3couldobtainsimilarperformancetoclosed-source
modelsandthusreducecostsconsiderably,weareyettoempiricallyprovetheefficiencyandefficacy
ofoursynthetichippocampalindexasitssizegrowswaybeyondcurrentbenchmarks.
Acknowledgments
The authors would like to thank colleagues from the OSU NLP group and Percy Liang for
their thoughtful comments. This research was supported in part by NSF OAC 2112606, NIH
R01LM014199,ARLW911NF2220144,andCisco. Theviewsandconclusionscontainedherein
are those of the authors and should not be interpreted as representing the official policies, either
expressedorimplied,oftheU.S.government. TheU.S.Governmentisauthorizedtoreproduceand
distributereprintsforGovernmentpurposesnotwithstandinganycopyrightnoticeherein.
References
[1] AI@Meta. Llama3modelcard. 2024. URLhttps://github.com/meta-llama/llama3/
blob/main/MODEL_CARD.md.
[2] B. AlKhamissi, M. Li, A. Celikyilmaz, M. T. Diab, and M. Ghazvininejad. A review on
languagemodelsasknowledgebases. ArXiv,abs/2204.06031,2022. URLhttps://arxiv.
org/abs/2204.06031.
[3] G.Angeli,M.J.JohnsonPremkumar,andC.D.Manning. Leveraginglinguisticstructurefor
opendomaininformationextraction. InC.ZongandM.Strube,editors,Proceedingsofthe53rd
AnnualMeetingoftheAssociationforComputationalLinguisticsandthe7thInternationalJoint
ConferenceonNaturalLanguageProcessing(Volume1: LongPapers),pages344–354,Beijing,
China,July2015.AssociationforComputationalLinguistics. doi: 10.3115/v1/P15-1034. URL
https://aclanthology.org/P15-1034.
[4] M.Banko,M.J.Cafarella,S.Soderland,M.Broadhead,andO.Etzioni. Openinformation
extractionfromtheweb. InProceedingsofthe20thInternationalJointConferenceonArtifical
Intelligence,IJCAI’07,page2670–2676,SanFrancisco,CA,USA,2007.MorganKaufmann
PublishersInc.
[5] A. Bosselut, H. Rashkin, M. Sap, C. Malaviya, A. Celikyilmaz, and Y. Choi. COMET:
Commonsense transformers for automatic knowledge graph construction. In A. Korhonen,
D.Traum,andL.Màrquez,editors,Proceedingsofthe57thAnnualMeetingoftheAssociation
forComputationalLinguistics,pages4762–4779,Florence,Italy,July2019.Associationfor
ComputationalLinguistics. doi:10.18653/v1/P19-1470. URLhttps://aclanthology.org/
P19-1470.
[6] B. Chen and A. L. Bertozzi. AutoKG: Efficient Automated Knowledge Graph Genera-
tion for Language Models. In 2023 IEEE International Conference on Big Data (Big-
Data),pages3117–3126,LosAlamitos,CA,USA,dec2023.IEEEComputerSociety. doi:
10.1109/BigData59044.2023.10386454. URLhttps://doi.ieeecomputersociety.org/
10.1109/BigData59044.2023.10386454.
[7] H. Chen, R. Pasunuru, J. Weston, and A. Celikyilmaz. Walking Down the Memory Maze:
BeyondContextLimitthroughInteractiveReading. CoRR,abs/2310.05029,2023. doi: 10.
48550/ARXIV.2310.05029. URLhttps://doi.org/10.48550/arXiv.2310.05029.
[8] T.Chen,H.Wang,S.Chen,W.Yu,K.Ma,X.Zhao,H.Zhang,andD.Yu. Densexretrieval:
What retrieval granularity should we use? arXiv preprint arXiv:2312.06648, 2023. URL
https://arxiv.org/abs/2312.06648.
[9] Y.Chen,S.Qian,H.Tang,X.Lai,Z.Liu,S.Han,andJ.Jia. Longlora: Efficientfine-tuningof
long-contextlargelanguagemodels. arXiv:2309.12307,2023.
10[10] Y.Chen,P.Cao,Y.Chen,K.Liu,andJ.Zhao. Journeytothecenteroftheknowledgeneurons:
Discoveriesoflanguage-independentknowledgeneuronsanddegenerateknowledgeneurons.
Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):17817–17825, Mar.
2024. doi: 10.1609/aaai.v38i16.29735. URLhttps://ojs.aaai.org/index.php/AAAI/
article/view/29735.
[11] G.CsárdiandT.Nepusz. Theigraphsoftwarepackageforcomplexnetworkresearch. 2006.
URLhttps://igraph.org/.
[12] N.DeCao, W.Aziz, andI.Titov. Editingfactualknowledgeinlanguagemodels. InM.-F.
Moens,X.Huang,L.Specia,andS.W.-t.Yih,editors,Proceedingsofthe2021Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing,pages6491–6506,OnlineandPuntaCana,
DominicanRepublic,Nov.2021.AssociationforComputationalLinguistics. doi: 10.18653/v1/
2021.emnlp-main.522. URLhttps://aclanthology.org/2021.emnlp-main.522.
[13] Y.Ding,L.L.Zhang,C.Zhang,Y.Xu,N.Shang,J.Xu,F.Yang,andM.Yang. Longrope:
Extendingllmcontextwindowbeyond2milliontokens. ArXiv,abs/2402.13753,2024. URL
https://api.semanticscholar.org/CorpusID:267770308.
[14] D.Edge,H.Trinh,N.Cheng,J.Bradley,A.Chao,A.Mody,S.Truitt,andJ.Larson. From
localtoglobal: Agraphragapproachtoquery-focusedsummarization. 2024. URLhttps:
//arxiv.org/abs/2404.16130.
[15] H. Eichenbaum. A cortical–hippocampal system for declarative memory. Nature Reviews
Neuroscience,1:41–50,2000. URLhttps://www.nature.com/articles/35036213.
[16] Y.Fu, R.Panda, X.Niu, X.Yue, H.Hajishirzi, Y.Kim, andH.Peng. Dataengineeringfor
scalinglanguagemodelsto128kcontext,2024.
[17] M.Geva,J.Bastings,K.Filippova,andA.Globerson. Dissectingrecalloffactualassociations
inauto-regressivelanguagemodels. InH.Bouamor,J.Pino,andK.Bali,editors,Proceedings
of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP
2023,Singapore,December6-10,2023,pages12216–12235.AssociationforComputational
Linguistics,2023. doi: 10.18653/V1/2023.EMNLP-MAIN.751. URLhttps://doi.org/10.
18653/v1/2023.emnlp-main.751.
[18] C.GormleyandZ.J.Tong. Elasticsearch: Thedefinitiveguide. 2015. URLhttps://www.
elastic.co/guide/en/elasticsearch/guide/master/index.html.
[19] T.L.Griffiths,M.Steyvers,andA.J.Firl. Googleandthemind. PsychologicalScience,18:
1069–1076,2007. URLhttps://cocosci.princeton.edu/tom/papers/google.pdf.
[20] J.-C.Gu,H.-X.Xu,J.-Y.Ma,P.Lu,Z.-H.Ling,K.-W.Chang,andN.Peng. ModelEditingCan
HurtGeneralAbilitiesofLargeLanguageModels,2024.
[21] W. Gurnee and M. Tegmark. Language models represent space and time. In The Twelfth
InternationalConferenceonLearningRepresentations,2024. URLhttps://openreview.
net/forum?id=jE8xbmvFin.
[22] J.Han,N.Collier,W.Buntine,andE.Shareghi. PiVe: PromptingwithIterativeVerification
ImprovingGraph-basedGenerativeCapabilityofLLMs,2023.
[23] T.H.Haveliwala. Topic-sensitivepagerank. InD.Lassner,D.D.Roure,andA.Iyengar,editors,
ProceedingsoftheEleventhInternationalWorldWideWebConference,WWW2002,May7-11,
2002, Honolulu, Hawaii, USA, pages 517–526. ACM, 2002. doi: 10.1145/511446.511513.
URLhttps://dl.acm.org/doi/10.1145/511446.511513.
[24] Q.He,Y.Wang,andW.Wang. Canlanguagemodelsactasknowledgebasesatscale?,2024.
[25] X. Ho, A.-K. Duong Nguyen, S. Sugawara, and A. Aizawa. Constructing a multi-hop QA
dataset for comprehensive evaluation of reasoning steps. In D. Scott, N. Bel, and C. Zong,
editors,Proceedingsofthe28thInternationalConferenceonComputationalLinguistics,pages
6609–6625,Barcelona,Spain(Online),Dec.2020.InternationalCommitteeonComputational
Linguistics. doi: 10.18653/v1/2020.coling-main.580. URLhttps://aclanthology.org/
2020.coling-main.580.
11[26] P.-L. Huguet Cabot and R. Navigli. REBEL: Relation extraction by end-to-end language
generation. InM.-F.Moens,X.Huang,L.Specia,andS.W.-t.Yih,editors,Findingsofthe
Association for Computational Linguistics: EMNLP 2021, pages 2370–2381, Punta Cana,
DominicanRepublic,Nov.2021.AssociationforComputationalLinguistics. doi: 10.18653/
v1/2021.findings-emnlp.204. URLhttps://aclanthology.org/2021.findings-emnlp.
204.
[27] G. Izacard, M. Caron, L. Hosseini, S. Riedel, P. Bojanowski, A. Joulin, and E. Grave.
Unsupervised dense information retrieval with contrastive learning, 2021. URL https:
//arxiv.org/abs/2112.09118.
[28] G. Izacard, P. Lewis, M. Lomeli, L. Hosseini, F. Petroni, T. Schick, J. A. Yu, A. Joulin,
S.Riedel,andE.Grave. Few-shotlearningwithretrievalaugmentedlanguagemodels. ArXiv,
abs/2208.03299,2022. URLhttps://arxiv.org/abs/2208.03299.
[29] J.Jiang,K.Zhou,Z.Dong,K.Ye,X.Zhao,andJ.-R.Wen. StructGPT:Ageneralframeworkfor
largelanguagemodeltoreasonoverstructureddata.InH.Bouamor,J.Pino,andK.Bali,editors,
Proceedingsofthe2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,
pages9237–9251,Singapore,Dec.2023.AssociationforComputationalLinguistics. doi: 10.
18653/v1/2023.emnlp-main.574. URL https://aclanthology.org/2023.emnlp-main.
574.
[30] Z.Jiang,F.Xu,L.Gao,Z.Sun,Q.Liu,J.Dwivedi-Yu,Y.Yang,J.Callan,andG.Neubig.Active
retrievalaugmentedgeneration. InH.Bouamor,J.Pino,andK.Bali,editors,Proceedingsof
the2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages7969–7992,
Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.
emnlp-main.495. URLhttps://aclanthology.org/2023.emnlp-main.495.
[31] S. Kambhampati. Can large language models reason and plan? Annals of the New York
AcademyofSciences,2024. URLhttps://nyaspubs.onlinelibrary.wiley.com/doi/
abs/10.1111/nyas.15125.
[32] M.Levy,A.Jacoby,andY.Goldberg. Sametask,moretokens: theimpactofinputlengthon
thereasoningperformanceoflargelanguagemodels,2024.
[33] P.Lewis,E.Perez,A.Piktus,F.Petroni,V.Karpukhin,N.Goyal,H.Küttler,M.Lewis,W.-t.
Yih,T.Rocktäschel,S.Riedel,andD.Kiela. Retrieval-augmentedgenerationforknowledge-
intensiveNLPtasks.InProceedingsofthe34thInternationalConferenceonNeuralInformation
Processing Systems, NIPS ’20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN
9781713829546. URLhttps://dl.acm.org/doi/abs/10.5555/3495724.3496517.
[34] T.Li, G.Zhang, Q.D.Do, X.Yue, andW.Chen. Long-contextLLMsStrugglewithLong
In-contextLearning,2024.
[35] Z.Li,N.Zhang,Y.Yao,M.Wang,X.Chen,andH.Chen. Unveilingthepitfallsofknowledge
editing for large language models. In The Twelfth International Conference on Learning
Representations,2024. URLhttps://openreview.net/forum?id=fNktD3ib16.
[36] L.LUO,Y.-F.Li,R.Haf,andS.Pan. Reasoningongraphs: Faithfulandinterpretablelargelan-
guagemodelreasoning. InTheTwelfthInternationalConferenceonLearningRepresentations,
2024. URLhttps://openreview.net/forum?id=ZGNWW7xZ6Q.
[37] K.Meng,D.Bau,A.Andonian,andY.Belinkov. Locatingandeditingfactualassociationsin
gpt. InNeuralInformationProcessingSystems,2022.
[38] E. Mitchell, C. Lin, A. Bosselut, C. Finn, and C. D. Manning. Fast model editing at scale.
ArXiv,abs/2110.11309,2021.
[39] E.Mitchell,C.Lin,A.Bosselut,C.D.Manning,andC.Finn. Memory-basedmodeleditingat
scale. ArXiv,abs/2206.06520,2022.
[40] T.T.Nguyen, T.T.Huynh, P.L.Nguyen, A.W.-C.Liew, H.Yin, andQ.V.H.Nguyen. A
surveyofmachineunlearning. arXivpreprintarXiv:2209.02299,2022.
12[41] J. Ni, C. Qu, J. Lu, Z. Dai, G. Hernandez Abrego, J. Ma, V. Zhao, Y. Luan, K. Hall, M.-
W. Chang, and Y. Yang. Large dual encoders are generalizable retrievers. In Y. Goldberg,
Z.Kozareva,andY.Zhang,editors,Proceedingsofthe2022ConferenceonEmpiricalMethods
inNaturalLanguageProcessing,pages9844–9855,AbuDhabi,UnitedArabEmirates,Dec.
2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.669.
URLhttps://aclanthology.org/2022.emnlp-main.669.
[42] OpenAI. GPT-3.5 Turbo, 2024. URL https://platform.openai.com/docs/models/
gpt-3-5-turbo.
[43] S.Pan,L.Luo,Y.Wang,C.Chen,J.Wang,andX.Wu. Unifyinglargelanguagemodelsand
knowledgegraphs: Aroadmap. IEEETransactionsonKnowledgeandDataEngineering,pages
1–20,2024. doi: 10.1109/TKDE.2024.3352100.
[44] A. Paszke, S. Gross, F. Massa, A. Lerer, J. Bradbury, G. Chanan, T. Killeen, Z. Lin,
N.Gimelshein,L.Antiga,A.Desmaison,A.Köpf,E.Z.Yang,Z.DeVito,M.Raison,A.Te-
jani, S.Chilamkurthy, B.Steiner, L.Fang, J.Bai, andS.Chintala. Pytorch: Animperative
style,high-performancedeeplearninglibrary. InH.M.Wallach,H.Larochelle,A.Beygelz-
imer, F. d’Alché-Buc, E. B. Fox, and R. Garnett, editors, Advances in Neural Information
ProcessingSystems32: AnnualConferenceonNeuralInformationProcessingSystems2019,
NeurIPS2019,December8-14,2019,Vancouver,BC,Canada,pages8024–8035,2019. URL
https://dl.acm.org/doi/10.5555/3454287.3455008.
[45] K.Pei,I.Jindal,K.C.-C.Chang,C.Zhai,andY.Li.Whentousewhat:Anin-depthcomparative
empiricalanalysisofOpenIEsystemsfordownstreamapplications. InA.Rogers, J.Boyd-
Graber,andN.Okazaki,editors,Proceedingsofthe61stAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume1: LongPapers),pages929–949,Toronto,Canada,July
2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.acl-long.53. URL
https://aclanthology.org/2023.acl-long.53.
[46] B.Peng,J.Quesnelle,H.Fan,andE.Shippole. Yarn: Efficientcontextwindowextensionof
largelanguagemodels,2023.
[47] F.Petroni,T.Rocktäschel,S.Riedel,P.Lewis,A.Bakhtin,Y.Wu,andA.Miller. Language
modelsasknowledgebases? InK.Inui, J.Jiang, V.Ng, andX.Wan, editors, Proceedings
ofthe2019ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandthe9th
International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages
2463–2473,HongKong,China,Nov.2019.AssociationforComputationalLinguistics. doi:
10.18653/v1/D19-1250. URLhttps://aclanthology.org/D19-1250.
[48] O.Press,M.Zhang,S.Min,L.Schmidt,N.Smith,andM.Lewis. Measuringandnarrowingthe
compositionalitygapinlanguagemodels. InH.Bouamor,J.Pino,andK.Bali,editors,Findings
oftheAssociationforComputationalLinguistics: EMNLP2023,pages5687–5711,Singapore,
Dec.2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.findings-emnlp.
378. URLhttps://aclanthology.org/2023.findings-emnlp.378.
[49] O.Press,M.Zhang,S.Min,L.Schmidt,N.A.Smith,andM.Lewis. Measuringandnarrowing
thecompositionalitygapinlanguagemodels,2023.URLhttps://openreview.net/forum?
id=PUwbwZJz9dO.
[50] O. Ram, Y. Levine, I. Dalmedigos, D. Muhlgay, A. Shashua, K. Leyton-Brown, and
Y. Shoham. In-context retrieval-augmented language models. Transactions of the Associ-
ationforComputationalLinguistics,11:1316–1331,2023. doi: 10.1162/tacl_a_00605. URL
https://aclanthology.org/2023.tacl-1.75.
[51] M. Reid, N. Savinov, D. Teplyashin, D. Lepikhin, T. Lillicrap, J.-b. Alayrac, R. Soricut,
A.Lazaridou,O.Firat,J.Schrittwieser,etal. Gemini1.5: Unlockingmultimodalunderstanding
acrossmillionsoftokensofcontext. arXivpreprintarXiv:2403.05530,2024. URLhttps:
//arxiv.org/abs/2403.05530.
[52] S.E.RobertsonandS.Walker.Somesimpleeffectiveapproximationstothe2-poissonmodelfor
probabilisticweightedretrieval. InW.B.CroftandC.J.vanRijsbergen,editors,Proceedings
13ofthe17thAnnualInternationalACM-SIGIRConferenceonResearchandDevelopmentin
Information Retrieval. Dublin, Ireland, 3-6 July 1994 (Special Issue of the SIGIR Forum),
pages232–241.ACM/Springer,1994. doi: 10.1007/978-1-4471-2099-5\_24. URLhttps:
//link.springer.com/chapter/10.1007/978-1-4471-2099-5_24.
[53] K.Santhanam,O.Khattab,J.Saad-Falcon,C.Potts,andM.Zaharia.ColBERTv2:Effectiveand
efficientretrievalvialightweightlateinteraction. InM.Carpuat,M.-C.deMarneffe,andI.V.
MezaRuiz,editors,Proceedingsofthe2022ConferenceoftheNorthAmericanChapterofthe
AssociationforComputationalLinguistics: HumanLanguageTechnologies,pages3715–3734,
Seattle,UnitedStates,July2022.AssociationforComputationalLinguistics. doi: 10.18653/v1/
2022.naacl-main.272. URLhttps://aclanthology.org/2022.naacl-main.272.
[54] P.Sarthi,S.Abdullah,A.Tuli,S.Khanna,A.Goldie,andC.D.Manning. RAPTOR:recursive
abstractiveprocessingfortree-organizedretrieval. CoRR,abs/2401.18059,2024. doi:10.48550/
ARXIV.2401.18059. URLhttps://arxiv.org/abs/2401.18059.
[55] Z.Shao,Y.Gong,Y.Shen,M.Huang,N.Duan,andW.Chen. Enhancingretrieval-augmented
largelanguagemodelswithiterativeretrieval-generationsynergy. InH.Bouamor,J.Pino,and
K.Bali,editors,FindingsoftheAssociationforComputationalLinguistics:EMNLP2023,pages
9248–9274,Singapore,Dec.2023.AssociationforComputationalLinguistics. doi: 10.18653/
v1/2023.findings-emnlp.620. URLhttps://aclanthology.org/2023.findings-emnlp.
620.
[56] W.Shi,S.Min,M.Yasunaga,M.Seo,R.James,M.Lewis,L.Zettlemoyer,andW.tauYih.
Replug: Retrieval-augmentedblack-boxlanguagemodels. ArXiv,abs/2301.12652,2023. URL
https://api.semanticscholar.org/CorpusID:256389797.
[57] J. Sun, C. Xu, L. Tang, S. Wang, C. Lin, Y. Gong, L. Ni, H.-Y. Shum, and J. Guo. Think-
on-graph: Deep and responsible reasoning of large language model on knowledge graph.
In The Twelfth International Conference on Learning Representations, 2024. URL https:
//openreview.net/forum?id=nnVO1PvbTv.
[58] T.J.TeylerandP.Discenna. Thehippocampalmemoryindexingtheory. Behavioralneuro-
science,1002:147–54,1986. URLhttps://pubmed.ncbi.nlm.nih.gov/3008780/.
[59] T. J. Teyler and J. W. Rudy. The hippocampal indexing theory and episodic memory: Up-
dating the index. Hippocampus, 17, 2007. URL https://pubmed.ncbi.nlm.nih.gov/
17696170/.
[60] H.Trivedi,N.Balasubramanian,T.Khot,andA.Sabharwal. MuSiQue: Multihopquestionsvia
single-hopquestioncomposition. Trans.Assoc.Comput.Linguistics,10:539–554,2022. doi:
10.1162/TACL\_A\_00475. URLhttps://aclanthology.org/2022.tacl-1.31/.
[61] H. Trivedi, N. Balasubramanian, T. Khot, and A. Sabharwal. Interleaving retrieval with
chain-of-thoughtreasoningforknowledge-intensivemulti-stepquestions. InA.Rogers,J.Boyd-
Graber,andN.Okazaki,editors,Proceedingsofthe61stAnnualMeetingoftheAssociationfor
ComputationalLinguistics(Volume1: LongPapers),pages10014–10037,Toronto,Canada,
July2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.acl-long.557.
URLhttps://aclanthology.org/2023.acl-long.557.
[62] C.Wang,X.Liu,Y.Yue,X.Tang,T.Zhang,C.Jiayang,Y.Yao,W.Gao,X.Hu,Z.Qi,Y.Wang,
L. Yang, J. Wang, X. Xie, Z. Zhang, and Y. Zhang. Survey on factuality in large language
models: Knowledge,retrievalanddomain-specificity,2023.
[63] J. Wang, Q. Sun, N. Chen, X. Li, and M. Gao. Boosting language models reasoning with
chain-of-knowledgeprompting,2023.
[64] X.Wang,J.Wei,D.Schuurmans,Q.V.Le,E.H.Chi,S.Narang,A.Chowdhery,andD.Zhou.
Self-consistencyimproveschainofthoughtreasoninginlanguagemodels. InTheEleventh
InternationalConferenceonLearningRepresentations,2023. URLhttps://openreview.
net/forum?id=1PL1NIMMrw.
14[65] J. Wei, X. Wang, D. Schuurmans, M. Bosma, brian ichter, F. Xia, E. H. Chi, Q. V. Le, and
D.Zhou. Chainofthoughtpromptingelicitsreasoninginlargelanguagemodels. InA.H.Oh,
A.Agarwal, D.Belgrave, andK.Cho, editors, AdvancesinNeuralInformationProcessing
Systems,2022. URLhttps://openreview.net/forum?id=_VjQlMeSB_J.
[66] Y.Wen,Z.Wang,andJ.Sun. Mindmap: Knowledgegraphpromptingsparksgraphofthoughts
inlargelanguagemodels. arXivpreprintarXiv:2308.09729,2023.
[67] P.West, C.Bhagavatula, J.Hessel, J.Hwang, L.Jiang, R.LeBras, X.Lu, S.Welleck, and
Y.Choi. Symbolicknowledgedistillation: fromgenerallanguagemodelsto commonsense
models. In M. Carpuat, M.-C. de Marneffe, and I. V. Meza Ruiz, editors, Proceedings of
the 2022 Conference of the North American Chapter of the Association for Computational
Linguistics: HumanLanguageTechnologies, pages4602–4625, Seattle, UnitedStates, July
2022.AssociationforComputationalLinguistics. doi: 10.18653/v1/2022.naacl-main.341. URL
https://aclanthology.org/2022.naacl-main.341.
[68] T.Wolf,L.Debut,V.Sanh,J.Chaumond,C.Delangue,A.Moi,P.Cistac,T.Rault,R.Louf,
M.Funtowicz,J.Davison,S.Shleifer,P.vonPlaten,C.Ma,Y.Jernite,J.Plu,C.Xu,T.L.Scao,
S.Gugger,M.Drame,Q.Lhoest,andA.M.Rush. Huggingface’stransformers: State-of-the-art
naturallanguageprocessing. ArXiv,abs/1910.03771,2019. URLhttps://arxiv.org/abs/
1910.03771.
[69] W.Xiong,X.Li,S.Iyer,J.Du,P.Lewis,W.Y.Wang,Y.Mehdad,S.Yih,S.Riedel,D.Kiela,
andB.Oguz. Answeringcomplexopen-domainquestionswithmulti-hopdenseretrieval. In
InternationalConferenceonLearningRepresentations,2021. URLhttps://openreview.
net/forum?id=EMHoBG0avc1.
[70] Z. Yang, P. Qi, S. Zhang, Y. Bengio, W. W. Cohen, R. Salakhutdinov, and C. D. Manning.
HotpotQA: A dataset for diverse, explainable multi-hop question answering. In E. Riloff,
D. Chiang, J. Hockenmaier, and J. Tsujii, editors, Proceedings of the 2018 Conference on
EmpiricalMethodsinNaturalLanguageProcessing,Brussels,Belgium,October31-November
4,2018,pages2369–2380.AssociationforComputationalLinguistics,2018. doi: 10.18653/
V1/D18-1259. URLhttps://aclanthology.org/D18-1259/.
[71] S.Yao,J.Zhao,D.Yu,N.Du,I.Shafran,K.Narasimhan,andY.Cao. ReAct: Synergizingrea-
soningandactinginlanguagemodels.InInternationalConferenceonLearningRepresentations
(ICLR),2023.
[72] M.Yasunaga,A.Bosselut,H.Ren,X.Zhang,C.D.Manning,P.Liang,andJ.Leskovec. Deep
bidirectionallanguage-knowledgegraphpretraining. InNeuralInformationProcessingSystems
(NeurIPS),2022. URLhttps://arxiv.org/abs/2210.09338.
[73] O. Yoran, T. Wolfson, B. Bogin, U. Katz, D. Deutch, and J. Berant. Answering questions
by meta-reasoning over multiple chains of thought. In The 2023 Conference on Empirical
MethodsinNaturalLanguageProcessing,2023. URLhttps://openreview.net/forum?
id=ebSOK1nV2r.
[74] W.Yu,D.Iter,S.Wang,Y.Xu,M.Ju,S.Sanyal,C.Zhu,M.Zeng,andM.Jiang. Generate
ratherthanretrieve: Largelanguagemodelsarestrongcontextgenerators. InTheEleventh
InternationalConferenceonLearningRepresentations,2023. URLhttps://openreview.
net/forum?id=fB0hRu9GZUS.
[75] K.Zhang,B.JimenezGutierrez,andY.Su. Aligninginstructiontasksunlockslargelanguage
modelsaszero-shotrelationextractors. InA.Rogers,J.Boyd-Graber,andN.Okazaki,edi-
tors,FindingsoftheAssociationforComputationalLinguistics: ACL2023,pages794–812,
Toronto,Canada,July2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.
findings-acl.50. URLhttps://aclanthology.org/2023.findings-acl.50.
[76] N. Zhang, Y. Yao, B. Tian, P. Wang, S. Deng, M. Wang, Z. Xi, S. Mao, J. Zhang, Y. Ni,
etal. Acomprehensivestudyofknowledgeeditingforlargelanguagemodels. arXivpreprint
arXiv:2401.01286,2024.
15[77] X.Zhang,Y.Chen,S.Hu,Z.Xu,J.Chen,M.K.Hao,X.Han,Z.L.Thai,S.Wang,Z.Liu,and
M.Sun. ∞bench: Extendinglongcontextevaluationbeyond100ktokens,2024.
[78] Z.Zhong,Z.Wu,C.D.Manning,C.Potts,andD.Chen. Mquake: Assessingknowledgeediting
inlanguagemodelsviamulti-hopquestions. InConferenceonEmpiricalMethodsinNatural
LanguageProcessing,2023. URLhttps://aclanthology.org/2023.emnlp-main.971.
pdf.
[79] S. Zhou, B. Yu, A. Sun, C. Long, J. Li, and J. Sun. A survey on neural open information
extraction: Current status and future directions. In L. D. Raedt, editor, Proceedings of the
Thirty-FirstInternationalJointConferenceonArtificialIntelligence,IJCAI-22,pages5694–
5701. International Joint Conferences on Artificial Intelligence Organization, 7 2022. doi:
10.24963/ijcai.2022/793. URL https://doi.org/10.24963/ijcai.2022/793. Survey
Track.
[80] H. Zhu, H. Peng, Z. Lyu, L. Hou, J. Li, and J. Xiao. Pre-training language model incorpo-
rating domain-specific heterogeneous knowledge into a unified representation. Expert Sys-
tems with Applications, 215:119369, 2023. ISSN 0957-4174. doi: https://doi.org/10.1016/
j.eswa.2022.119369. URL https://www.sciencedirect.com/science/article/pii/
S0957417422023879.
16Appendices
Withinthissupplementarymaterial,weelaborateonthefollowingaspects:
• AppendixA:HippoRAGPipelineExample
• AppendixB:DatasetComparison
• AppendixC:AblationStatistics
• AppendixD:Single-StepMulti-Hop: All-RecallMetric
• AppendixE:Path-FindingMulti-HopCaseStudy
• AppendixF:ErrorAnalysisonMuSiQue
• AppendixG:CostandEfficiencyComparison
• AppendixH:ImplementationDetails&ComputeRequirements
• AppendixI:LLMPrompts
17Question & Answer
Question In which district was Alhandraborn?
Answer Lisbon
Supporting Passages
1. Alhandra(footballer)
Luís Miguel AssunçãoJoaquim (born 5 March 1979 in Vila Franca de Xira, Lisbon), known as Alhandra,
is a Portuguese retired footballer who played mainly as a left back –he could also appear as a midfielder.
2. Vila Franca de Xira
Vila Franca de Xirais a municipality in the Lisbon District in Portugal. The population in 2011 was
136,886, in an area of 318.19 km². Situated on both banks of the Tagus River, 32 km north-east of the
Portuguese capital Lisbon, settlement in the area dates back toneolithic times, as evidenced by findings
in the Cave of PedraFurada. Vila Franca de Xirais said to have been founded by French followers of
Portugal's first king, Afonso Henriques, around 1200.
Distractor Passages (Excerpts)
1. Chirakkalkulam
Chirakkalkulamis a small residential area near Kannur town of Kannur District, Kerala state, South
India. Chirakkalkulamis located between Thayatheruand Kannur City. Chirakkalkulam'ssignificance
arises from the birth of the historic ArakkalKingdom.
2. Frank T. and Polly Lewis House
The Frank T. and Polly Lewis House is located inLodi, Wisconsin, United States. It was added to the
National Register of Historic Places in 2009. The house is located within the Portage Street Historic
District.
3. Birth certificate
In the U.S., the issuance of birth certificates is a function of the Vital Records Office of the states, capital
district, territories and former territories …
Figure 3: HippoRAG Pipeline Example (Question and Annotations). (Top) We provide an
examplequestionanditsanswer. (Middle&Bottom)Thesupportinganddistractorpassagesforthis
question. Twosupportingpassagesareneededtosolvethisquestion. Theexcerptsofthedistractor
passagesarerelatedtothe“district”mentionedinthequestion.
A HippoRAGPipelineExample
TobetterdemonstratehowourHippoRAGpipelineworks,weusethepath-followingexamplefrom
theMuSiQuedatasetshowninTable6. WeuseHippoRAG’sindexingandretrievalprocessesto
followthisquestionandasubsetoftheassociatedcorpus. Thequestion,itsanswer,anditssupporting
anddistractorpassagesareasshowninFigure3. TheindexingstageisshowninFigure4,showing
boththeOpenIEprocedureaswellastherelevantsubgraphofourKG.Finally, weillustratethe
retrieval stage in Figure 5, including query NER, query node retrieval, how the PPR algorithm
changesnodeprobabilities,andhowthetopretrievalresultsarecalculated.
18Indexing: Passage NER and OpenIEfor Supporting Passages
1. Alhandra(footballer)
NER:
["5 March 1979", "Alhandra", "Lisbon", "Luís Miguel AssunçãoJoaquim", "Portuguese", "Vila Franca de
Xira"]
OpenIE:
[("Alhandra", "is a", "footballer"),
("Alhandra", "born in", "Vila Franca de Xira"),
("Alhandra", "born in", "Lisbon"),
("Alhandra", "born on", "5 March 1979"),
("Alhandra", "is", "Portuguese"),
("Luís Miguel AssunçãoJoaquim", "is also known as", "Alhandra")]
2. Vila Franca de Xira
NER:
["2011", "Afonso Henriques", "Cave of PedraFurada", "French", "Lisbon", "Lisbon District", "Portugal",
"Tagus River", "Vila Franca de Xira"]
OpenIE:
[("Vila Franca de Xira", "is a municipality in", "Lisbon District"),
("Vila Franca de Xira", "located in", "Portugal"),
("Vila Franca de Xira", "situated on", "Tagus River"),
("Vila Franca de Xira", "is", "founded by French followers of Afonso Henriques"),
("Tagus River", "located near", "Lisbon"),
("Cave of PedraFurada", "evidenced settlement in", "neolithic times"),
("Afonso Henriques", "was Portugal's first king in", "1200"),
("Vila Franca de Xira", "had population of", "136,886 in 2011"),
("Vila Franca de Xira", "has area of", "318.19 km²")]
Indexing: Subgraph Related to the Question
vila Portugal
located in
equivalent
founded by French
Lisbon 5 March 1979 followers of Lisbon District
born in Afonso Henriques
born on is isa municipality in
footballer is a Alhandra Vila Franca Tagus River
born in de Xira situated on
is also known as equivalent
is
Luís Miguel municipality of had population of
Portuguese Assunção Vila Franca de 136,886 in 2011
Joaquim Xira
had area of
318.19 km2
Figure4: HippoRAGPipelineExample(Indexing). NERandOpenIEaresequentiallyconducted
oneachpassageofthecorpus. Thus,anopenknowledgegraphisformedfortheentirecorpus. We
onlyshowtherelevantsubgraphfromtheKG.
19Retrieval: Query NER & Node Retrieval
Question In which district was Alhandraborn?
NER ["Alhandra"]
Node Retrieval {"Alhandra": "Alhandra"}
Retrieval: PPR
Node Probabilities Changes by PPR
Alhandra 1.000 ⇒0.533 5 March 1979 0.000 ⇒0.045
Vila Franca de Xira 0.000 ⇒0.054 Luís Miguel AssunçãoJoaquim 0.000 ⇒0.044
Lisbon 0.000 ⇒0.049 Portugal 0.000 ⇒0.009
footballer 0.000 ⇒0.047 Tagus River 0.000 ⇒0.007
Portuguese 0.000 ⇒0.046 José Pinto Coelho 0.000 ⇒0.004
…
Retrieval: Top Results
*Top-ranked nodes from PPR are highlighted.
1. Alhandra(footballer)
Luís Miguel Assunção Joaquim (born 5 March 1979in Vila Franca de Xira, Lisbon), known as Alhandra,
is a Portugueseretired footballerwho played mainly as a left back –he could also appear as a midfielder.
2. Vila Franca de Xira
Vila Franca de Xira is a municipality in the LisbonDistrict in Portugal. The population in 2011 was
136,886, in an area of 318.19 km². Situated on both banks of the Tagus River, 32 km north-east of the
Portuguesecapital Lisbon, settlement in the area dates back toneolithic times, as evidenced by findings
in the Cave of PedraFurada. Vila Franca de Xirais said to have been founded by French followers of
Portugal's first king, Afonso Henriques, around 1200.
3. Portugal
Portugueseis the official language of Portugal. Portugueseis a Romance language that originated in
what is now Galicia and Northern Portugal, originating from Galician-Portuguese, which was the
common language of the Galician and Portuguesepeople until the independence of Portugal.
Particularly in the North of Portugal, there are still many similarities between the Galician culture and
the Portugueseculture. Galicia is a consultative observer of the Community of PortugueseLanguage
Countries. According to the Ethnologueof Languages, Portugueseand Spanish have a lexical similarity
of 89% -educated speakers of each language can communicate easily with one another.
4. Huguenots
The first Huguenots to leave France sought freedom from persecution in Switzerland and the
Netherlands … A fort, named Fort Coligny, was built to protect them from attack from the Portuguese
troops and Brazilian Native Americans. It was an attempt to establish a French colony in South America.
The fort was destroyed in 1560 by the Portuguese, who captured part of the Huguenots. The Portuguese
threatened the prisoners with death if they did not convert to Catholicism …
5. East Timor
Democratic Republic of Timor -LesteRepúblikaDemokrátikaTimórLorosa'e(Tetum) República
Democráticade Timor -Leste(Portuguese) Flag Coat of arms Motto: Unidade, Acção, Progresso
(Portuguese) Unidade, Asaun, Progresu(Tetum) (English: ``Unity, Action, Progress '') Anthem: Pátria
(Portuguese) (English:`` Fatherland'') Capital and largest city Dili 8 °20 ′ S 125 °20 ′ E / 8.34 °S 125.34 °
E / -8.34; 125.34 Coordinates: 8 °20 ′ S 125 °20 ′ E / 8.34 °S 125.34 °E / -8.34; 125.34 …
Figure5: HippoRAGPipelineExample(Retrieval). Forretrieval,thenamedentitiesinthequery
are extracted from the question (Top), after which the query nodes are chosen using a retrieval
encoder. Inthiscase,thenameofthequerynamedentity,“Alhandra”,isequivalenttoitsKGnode.
(Middle) We then set the personalized probabilities for PPR based on the retrieved query nodes.
AfterPPR,thequerynodeprobabilityisdistributedaccordingtothesubgraphinFigure4,leadingto
someprobabilitymassonthenode“VilaFrancedeXira”. (Bottom)Thesenodeprobabilitiesare
thensummedoverthepassagestheyappearintoobtainthepassage-levelranking. Thetop-ranked
nodesafterPPRarehighlightedinthetop-rankedpassages.
20B DatasetComparison
Toanalyzethedifferencesbetweenthethreedatasetsweuse,wepayspecialattentiontothequalityof
thedistractorpassages,i.e.,whethertheycanbeeffectivelyconfoundedwiththesupportingpassages.
WeuseContriever[27]tocalculatethematchscorebetweenquestionsandcandidatepassagesand
show their densities in Figure 6. In an ideal case, the distribution of distractor scores should be
close to the mean of the support passage scores. However, it can be seen that the distribution of
thedistractorscoresinHotpotQAismuchclosertothelowerboundofthesupportpassagescores
comparedtotheothertwodatasets.
MuSiQue 2WikiMultihopQA HotpotQA
5
5 5
4
4 4
3
3 3
2 2 2
1 1 1
0 0 0
0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9
Scores Scores Scores
Distractors Max supporting document Min supporting document
Figure6: Densityofsimilarityscoresofcandidatepassages(distractorsandsupportingpassages)
obtainedbyContriever. ThesimilarityscoreofHotpotQAdistractorsisnotsignificantlylargerthan
thatoftheleastsimilarsupportingpassages,meaningthatthesedistractorsarenotveryeffective.
C AblationStatistics
WeuseGPT-3.5Turbo,REBEL[26]andLlama-3(8Band70B)[1]forOpenIEablationexperiments.
AsshowninTable7,comparedtobothGPT-3.5TurboandbothLlamamodels,REBELgenerates
around half the number of nodes and edges. This illustrates REBEL’s lack of flexibility in open
informationextractionwhencomparedtousingbothopenandclosed-sourceLLMs. Meanwhile,
bothLlama-3versionsproduceasimilaramountofOpenIEtriplesthanGPT-3.5Turbo.
Table7: KnowledgegraphstatisticsusingdifferentOpenIEmethods.
Model Count MuSiQue 2Wiki HotpotQA
#ofUniqueNodes(N) 91,729 42,694 82,157
GPT-3.5Turbo[42] #ofUniqueEdges(E) 21,714 7,867 17,523
(Default) #ofUniqueTriples 107,448 50,671 98,709
#ofColBERTv2SynonymEdges(E′) 191,636 82,526 171,856
#ofUniqueNodes(N) 36,653 22,146 30,426
#ofUniqueEdges(E) 269 211 262
REBEL[26]
#ofUniqueTriples 52,102 30,428 42,038
#ofColBERTv2SynonymEdges(E′) 48,213 33,072 39,053
#ofUniqueNodes(N) 89,759 44,251 78,959
#ofUniqueEdges(E) 25,868 8,497 20,325
Llama-3-8B-Instruct[1]
#ofUniqueTriples 117,127 54,347 101,822
#ofColBERTv2SynonymEdges(E′) 186,568 92,913 157,446
#ofUniqueNodes(N) 83,647 38,800 73,958
#ofUniqueEdges(E) 20,694 6,917 15,663
Llama-3-70B-Instruct[1]
#ofUniqueTriples 111,996 51,379 100,372
#ofColBERTv2SynonymEdges(E′) 177,979 75,463 150,291
D Single-StepMulti-Hop: All-RecallMetric
HippoRAGperformsbetteronmulti-hopQAbenchmarksthanstandardRAGmainlybecauseof
itsabilitytoperformmulti-hopretrievalinasinglestep. Table8demonstratesthisbymeasuring
21
ytisneD ytisneD ytisneDthepercentageofquerieswhereallsupportingpassagesareretrievedsuccessfully,somethingthat
canonlybeachievedthroughsuccessfulmulti-hopreasoning. Thefactthatthegapsshowninthis
tablearemuchlargerthanthegapsinTable2indicatesthatmuchofHippoRAG’simprovementsare
comingfromtheabilitytoretrieveallsupportingdocumentsnotbyperformingbetterpartialretrieval.
Table8: All-Recallmetric. Wemeasurethepercentageofqueriesforwhichallsupportingpassages
aresuccessfullyretrieved(all-recall,denotedasAR@2orAR@5)andfindevenlargerperformance
improvementsforHippoRAG.
MuSiQue 2Wiki HotpotQA Average
AR@2 AR@5 AR@2 AR@5 AR@2 AR@5 AR@2 AR@5
ColBERTv2[53] 6.8 16.1 25.1 37.1 33.3 59.0 21.7 37.4
HippoRAG 10.2 22.4 45.4 75.7 33.8 57.9 29.8 52.0
E CaseStudyonPath-FindingMulti-HopQA
Asdiscussedabove,path-findingmulti-hopquestionsacrosspassagesareexceedinglychallenging
forsingle-stepandmulti-stepRAGmethodssuchasColBERTv2andIRCoT.Thesequestionsrequire
integrating information across multiple passages to find relevant entities among many possible
candidates,suchasfindingallStanfordprofessorswhoworkontheneuroscienceofAlzheimer’s.
E.1 Path-FindingMulti-HopQuestionConstructionProcess
Thesequestionsandthecuratedcorporaaroundthemwerebuiltthroughthefollowingprocedure.
Thefirsttwoquestionsfollowaslightlyseparateprocessasthethirdoneaswellasthemotivating
exampleinthemainpaper. Forthefirsttwo,wefirstidentifyabookormovieandthenfoundthe
book’sauthororthemovie’sdirector. Wewouldthenfind1)atraitforeitherthebook/movieand2)
anothertraitfortheauthor/director. Thesetwotraitswouldthenbeusedtoextractdistractorsfrom
Wikipediaforeachquestion.
Forthethirdquestionandourmotivatingexample,wefirstchooseaprofessororadrugatrandomas
theanswerforeachquestion. Wethenobtaintheuniversitytheprofessorworksatorthediseasethe
drugtreatsaswellasoneothertraitfortheprofessorordrug(inthesequestionsresearchtopicand
mechanismofactionwerechosen). Inthesequestions,distractorswereextractedfromWikipedia
usingtheUniversityordiseaseontheonehandandtheresearchtopicormechanismofactionon
theother. Thisprocess,althoughquitetedious,allowedustocuratethesechallengingbutrealistic
path-findingmulti-hopquestions.
E.2 QualitativeAnalysis
InTable9,weshowthreemoreexamplesfromthreedifferentdomainsthatillustrateHippoRAG’s
potentialforsolvingretrievaltasksthatrequiresuchcross-passageknowledgeintegration.
InthefirstquestionofTable9,wewanttofindabookpublishedin2012byanEnglishauthorwho
won a specific award. In contrast to HippoRAG, ColBERTv2 and IRCoT are unable to identify
MarkHaddonassuchanauthor. ColBERTv2focusesonpassagesrelatedtoawardswhileIRCoT
mistakenlydecidesthatKateAtkinsonistheanswertosuchquestionsinceshewonthesameaward
fora bookpublished in1995. For thesecond question, wewanted tofind awar filmbased ona
non-fictionbookdirectedbysomeonefamousforsci-fiandcrimemovies. HippoRAGisabletofind
ouranswerBlackHawkDownbyRidleyScottwithinthefirstfourpassages,whileColBERTv2
missestheanswercompletelyandretrievesotherfilmsandfilmcollections. Inthisinstance,even
thoughIRCoTisabletoretrieveRidleyScott,itdoessomainlythroughparametricknowledge. The
chain-of-thoughtoutputdiscusseshisandDenisVilleneuvefameaswellastheirsci-fiandcrime
experience. Giventhethree-stepiterationrestrictionusedhereandtheneedtoexploretwodirectors,
thespecificwarfilmBlackHawkDownwasnotidentified. Althoughabitconvoluted,peopleoften
askthesefirsttwoquestionstorememberaspecificmovieorbooktheywatchedorheardaboutfrom
onlyahandfulofdisjointeddetails.
22Finally,thethirdquestionismoresimilartothemotivatingexampleinthemainpaperandshows
the importance of this type of question in real-world domains. In this question, we ask for a
drugusedtotreatlymphocyticleukemiathroughaspecificmechanism(cytosolicp53interaction).
While HippoRAG is able to leverage the associations within the supporting passages to identify
theChlorambucilpassageasthemostimportant,ColBERTv2andIRCoTareonlyabletoextract
passagesassociatedwithlymphocyticleukemia. Interestinglyenough,IRCoTusesitsparametric
knowledgetoguessthatVenetoclax,whichalsotreatsleukemia,woulddosothroughtherelevant
mechanismeventhoughnopassageinthecurateddatasetexplicitlystatedthis.
Table9:Rankingresultexamplesfordifferentapproachesonseveralpath-findingmulti-hopquestions.
Question HippoRAG ColBERTv2 IRCoT
1.WorldBookClub
Whichbookwas 1.OrangesAre 1.KateAtkinson
Prizewinners
publishedin2012byan NottheOnlyFruit 2.LeonGarfield
2.LeonGarfield
Englishauthorwhoisa 2.WilliamTrevor Awards
Awards
WhitbreadAward Legacies 3.TwelveBar
3.TwelveBar
winner? 3.MarkHaddon Blues(novel)
Blues(novel)
Whichwarfilmbased 1.PaulGreengrass
1.WarFilm 1.RidleyScott
onanonfictionbook 2.Listofbook-based
2.TimedeZarn 2.PeterHyams
wasdirectedbysomeone warfilms
3.OutlineofSci-Fi 3.PaulGreengrass
famousinthescience 3.KoreanWarFilms
4.BlackHawk 4.Listofbook-based
fictionandcrime 4.AlltheKing’s
Down warfilms
genres? MenBook
Whatdrugisusedto 1.Chlorambucil
1.Lymphocytic 1.Venetoclax
treatchronic 2.Lymphocytic
leukemia 2.Lymphocytic
lymphocyticleukemia leukemia
2.Obinutuzumab leukemia
byinteractingwith 3.Mosquitobite
3.Venetoclax 3.Idelalisib
cytosolicp53? allergy
F ErrorAnalysis
F.1 Overview
WemainlyanalyzetheerrorsbyHippoRAGontheMuSiQuedataset. AsshowninTable10,these
errorscanbeattributedtothreetypes. TheNERlimitationisthemainerrortype(nearlyhalf),as
NERmaynotextractenoughinformationfromthequeryforretrieval,e.g.,forthequestion“When
wasoneinternetbrowser’sversionofWindows8madeaccessible?”,onlythephrase“Windows8”is
extracted. Nothingabout“browser”or“accessibility”isextractedforsubsequentgraphsearch.
For OpenIE errors, we discuss some examples in §F.3. Despite the correct functioning of NER
andOpenIE,thePPRalgorithmsometimesfailstoidentifyrelevantpassagesinthegraphcorrectly.
Forinstance,considerthequestion“HowmanyrefugeesemigratedtotheEuropeancountrywhere
Huguenotsfeltakinshipforemigration?”. Despitethesupportingpassagesbeingtitled“Huguenots”
asshowninthequestion,thePPRalgorithmstrugglesduetomultiplepassageswithsimilartopics
withinthecorpus.Evenwhentheterm“Huguenots”isaccuratelyextractedfromboththequestionand
thesupportingpassages,andthePPRalgorithminitiatessearcheswiththenodeslabeled“European”
and“Huguenots”,itincorrectlyprioritizesotherpassagescontaining“European”and“Huguenots”
thatdonotactuallysupportthequestion. Thismisdirectionoccursbecausethealgorithmdoesnot
effectivelydiscriminatebetweenpassageswithsimilartopicsbutdifferentrelevancetothequestion’s
context.
Table10: ErroranalysisonMuSiQue.
ErrorType ErrorPercentage(%)
NERLimitation 48
Incorrect/MissingOpenIE 28
PPR 24
23F.2 Conceptsvs. ContextTradeoff
Given our method’s entity-centric nature in extraction and indexing, it has a strong bias towards
concepts that leaves many contextual signals unused. This design enables single-step multi-hop
retrievalwhilealsoenablingcontextualcuestoavoiddistractingfrommoresaliententities. Asseen
inthefirstexampleinTable11,ColBERTv2usesthecontexttoretrievepassagesthatarerelatedto
famousSpanishnavigatorsbutnot“SergioVillanueva”,whoisaboxer. Incontrast,HippoRAGis
abletohoneinon“Sergio”andretrieveonerelevantpassage.
Unfortunately,thisdesignisalsooneofourmethod’sgreatestlimitationssinceignoringcontextual
cuesaccountsforaround48%oferrorsinasmall-scaleerroranalysis;moredetailscanbefoundin
AppendixF.Thisproblemismoreapparentinthesecondexamplesincetheconceptsaregeneral,
makingthecontextmoreimportant. SincetheonlyconcepttaggedbyHippoRAGis“protons”,it
extractspassagesrelatedto“Uranium”and“nuclearweapons”whileColBERTv2usesthecontextto
extractmorerelevantpassagesassociatedwiththediscoveryofatomicnumbers.
Table11: Examplesshowingtheconcept-contexttradeoffonMuSiQue.
Question HippoRAG ColBERTv2
Whosefatherwasanavigator
whoexploredtheeastcoast SergioVillanueva FranciscodeEliza(navigator)
ofthecontinentalregionwhere CésarGaytan ExplorationofN.America
SergioVillanuevawould FaustinoReyes VicentePinzón(navigator)
laterbeborn?
Whatundertakingincludedthe
Uranium Atomicnumber
personwhodiscoveredthatthe
Chemicalelement Atomictheory
numberofprotonsineach
Historyofnuclearweapons Atomicnucleus
element’satomsisunique?
Table12: Single-stepretrievalperformance. HippoRAGperformssignificantlybetteronMuSiQue
and2WikiMultiHopQAthanallbaselinesandachievescomparableperformanceonthelesschalleng-
ingHotpotQAdataset.
Model Retriever MuSiQue 2Wiki HotpotQA Average
R@2 R@5 R@2 R@5 R@2 R@5 R@2 R@5
Contriever 34.8 46.6 46.6 57.5 57.2 75.5 46.2 59.9
Baseline
ColBERTv2 37.9 49.2 59.2 68.2 64.7 79.3 53.9 65.6
Contriever 41.0 52.1 71.5 89.5 59.0 76.2 57.2 72.6
HippoRAG
ColBERTv2 40.9 51.9 70.7 89.1 60.5 77.7 57.4 72.9
HippoRAGw/ Contriever 42.3 54.5 71.3 87.2 60.6 79.1 58.1 73.6
Uncertainty-basedEnsemble ColBERTv2 42.5 54.8 71.9 89.0 62.5 80.0 59.0 74.6
Togetabettertrade-offbetweenconceptsandcontext,weintroduceanensemblingsettingwhere
HippoRAGscoresareensembledwithdenseretrieverswhenourparahippocampalregionshows
uncertainty regarding the link between query and KG entities. This process represents instances
whennohippocampalindexwasfullyactivatedbytheupstreamparahippocampalsignalandthus
theneocortexmustbereliedonmorestrongly. Weonlyuseuncertainty-basedensemblingifone
ofthequery-KGentityscorescosine_similarity(M(c ),M(e ))islowerthanathresholdθ,for
i j
example,iftherewasnoStanfordnodeintheKGandtheclosestnodeintheKGissomethingthat
hasacosinesimilaritylowerthanθsuchasStanfordMedicalCenter. Thefinalpassagescorefor
uncertainty-basedensemblingistheaverageoftheHippoRAGscoresandstandardpassageretrieval
usingmodelM,bothofwhicharefirstnormalizedintothe0to1overallpassages.
WhenHippoRAGisensembledwithM under“Uncertainty-basedEnsembling”,itfurtherimproves
onMuSiQueandoutperformsourbaselinesinR@5forHotpotQA,asshowninTable12. When
usedincombinationwithIRCoT,asshowninTable13,theColBERTv2ensembleoutperformsall
previousbaselinesinbothR@2andR@5onHotpotQA.Althoughthesimplicityofthisapproachis
24promising,moreworkneedstobedonetosolvethiscontext-contexttradeoffsincesimpleensembling
doeslowerperformanceinsomecases,especiallyforthe2WikiMultiHopQAdataset.
Table 13: Multi-step retrieval performance. Combining HippoRAG with standard multi-step
retrievalmethodslikeIRCoTresultsinsignificantimprovementsonallthreedatasets.
Model Retriever MuSiQue 2Wiki HotpotQA Average
R@2 R@5 R@2 R@5 R@2 R@5 R@2 R@5
IRCoT Contriever 39.1 52.2 51.6 63.8 65.9 81.6 52.2 65.9
ColBERTv2 41.7 53.7 64.1 74.4 67.9 82.0 57.9 70.0
IRCoT+HippoRAG Contriever 43.9 56.6 75.3 93.4 65.8 82.3 61.7 77.4
ColBERTv2 45.3 57.6 75.8 93.9 67.0 83.0 62.7 78.2
IRCoT+HippoRAGw/ Contriever 44.4 58.5 75.3 91.5 66.9 85.0 62.2 78.3
Uncertainty-basedEnsemble ColBERTv2 40.2 53.4 74.5 91.2 68.2 85.3 61.0 76.6
F.3 OpenIELimitations
OpenIEisacriticalstepinextractingstructuredknowledgefromunstructuredtext. Nonetheless,
itsshortcomingscanresultingapsinknowledgethatmayimpairretrievalandQAcapabilities. As
showninTable14,GPT-3.5Turbooverlooksthecrucialsongtitle“Don’tLetMeWaitTooLong”
duringtheOpenIEprocess. Thistitlerepresentsthemostsignificantelementwithinthepassage. A
probablereasonisthatthemodelisinsensitivetosuchalongentity. Besides,themodeldoesnot
accuratelycapturethebeginningandendingyearsofthewar,whichareessentialforthequery. This
isanexampleofhowmodelsroutinelyignoretemporalproperties. Overall,thesefailureshighlight
theneedtoimprovetheextractionofcriticalinformation.
Table14: OpeninformationextractionerrorexamplesonMuSiQue.
Question Passage MissedTriples
“Don’tLetMeWaitTooLong”wassequenced
What company is the label re- (Don’t Let Me Wait
onsideoneoftheLP,betweentheballads“The
sponsible for “Don’t Let Me Too Long, sequenced
LightThatHasLightedtheWorld”and“Who
WaitTooLong”apartof? on,sideoneoftheLP)
CanSeeIt”...
When did the president of the (Mexican-American
Jefferson Davis fought in the Mexi-
ConfederateStatesofAmerica War, starts, 1846),
can–American War (1846–1848), as the
end his fight in the Mexican- (Mexican-American
colonelofavolunteerregiment...
Americanwar? War,ends,1848)
G CostandEfficiencyComparison
One of HippoRAG’s main advantages against iterative retrieval methods is the dramatic online
retrievalefficiencygainsbroughtonbyitssingle-stepmulti-hopretrievalabilityintermsofboth
costandtime. Specifically,asseeninTable15,retrievalcostsforIRCoTare10to30timeshigher
thanHippoRAGsinceitonlyrequiresextractingrelevantnamedentitiesfromthequeryinsteadof
processingalloftheretrieveddocuments. Insystemswithextremelyhighusage,acostdifferenceof
anorderofmagnitudesuchasthisonecouldbeextremelyimportant. ThedifferencewithIRCoTin
termsoflatencyisalsosubstantial,althoughmorechallengingtomeasureexactly. Alsoasseenin
Table15,HippoRAGcanbe6to13timesfasterthanIRCoT,dependingonthenumberofretrieval
roundsthatneedtobeexecuted(2-4inourexperiments).6
6WeuseasinglethreadtoquerytheOpenAIAPIforonlineretrievalinbothIRCoTandHippoRAG.Since
IRCoTisaniterativeprocessandeachoftheiterationsmustbedonesequentially,thesespeedcomparisonsare
appropriate.
25Table15: AveragecostandefficiencymeasurementsforonlineretrievalusingGPT-3.5Turboon
1,000queries.
ColBERTv2 IRCoT HippoRAG
APICost($) 0 1-3 0.1
Time(minutes) 1 20-40 3
AlthoughofflineindexingtimeandcostsarehigherforHippoRAGthanIRCoT—around10times
slowerand$15moreexpensiveforevery10,000passages7,thesecostscanbedramaticallyreduced
byleveragingopensourceLLMs. AsshowninourablationstudyinTable5Llama-3-8B-Instruct[1]
performssimilarlytoGPT-3.5Turbowithoutanyspecificprompttuning. AsseeninTable16,this8B
Llama-3modelisabouthalfthecostasGPT-3.5TurbowhenusinganLLMdeployingservicelike
TogetherAI8.Additionally,sincethesecostscouldbeevenfurtherreducedbylocallydeployingthis
model,thebarriersforusingHippoRAGatscalecouldbewellwithinthecomputationalbudgetof
manyorganizations. Finally,wenotethatevenifLLMgenerationcostdropssignificantly,theonline
retrievalefficiencygainsdiscussedaboveremainintactgiventhatthenumberoftokensrequired
forIRCoTvs. HippoRAGstayconstantandLLMuseislikelytoalsoremainthesystem’smain
computationalbottleneck.
Table16: Averagecostandlatencymeasurementsforoffline indexingusingGPT-3.5Turboand
Llama-3(8Band70B)throughtheTogetherAIAPIon10,000passages.
Model Metric ColBERTv2 IRCoT HippoRAG
GPT-3.5Turbo-1106(MainResults) APICost($) 0 0 15
Time(minutes) 7 7 60
GPT-3.5Turbo APICost($) 0 0 8
Time(minutes) 7 7 60
Llama-3-8B-Instruct APICost($) 0 0 3
Time(minutes) 7 7 45
Llama-3-70B-Instruct APICost($) 0 0 7
Time(minutes) 7 7 100
H ImplementationDetails&ComputeRequirements
Apartfromthedetailsincludedin§3.4,weuseimplementationsbasedonPyTorch[44]andHug-
gingFace [68] for both Contriever [27] and ColBERTv2 [53]. We use the python-igraph [11]
implementationofthePPRalgorithm. ForBM25,weemployElasticSearch[18]. Formulti-step
retrieval,weusethesamepromptimplementationasIRCoT[61]andretrievethetop-10passagesat
eachstep.Wesetthemaximumnumberofreasoningstepsto2forHotpotQAand2WikiMultiHopQA
and4forMuSiQueduetotheirmaximumreasoningchainlength. WecombineIRCoTwithdifferent
retrieversbyreplacingitsbaseretrieverBM25witheachretrievalmethod,includingHippoRAG,
notedas“IRCoT+HippoRAG”below.9 FortheQAreader,weusetop-5retrievedpassagesasthe
contextand1-shotQAdemonstrationwithCoTpromptingstrategy[61].
Intermsofcomputerequirements,mostofourcomputerequirementsareunfortunatelynotdisclosed
bytheOpenAIandTogetherAIAPIs. WerunColBERTv2andContrieverforindexingandretrieval
weuse4NVIDIARTXA6000GPUswith48GBofmemory. Finally,weused2AMDEPYC7513
32-CoreProcessorstorunthePersonalizedPageRankalgorithm.
7Tospeedupindexing,weuse10threadsqueryinggpt-3.5-turbo-1106throughtheOpenAIAPIinparallel.
Atthetimeofwriting,thecostoftheAPIis$1foramillioninputtokensand$2foramillionoutputtokens.
8https://www.together.ai/products#inference
9SincetheoriginalIRCoTdoesnotprovideascoreforeachretrievedpassage,weemploybeamsearchfor
theiterativeretrievalprocess.Eachcandidatepassagemaintainsthehighesthistoricalscoreduringbeamsearch.
26Passage NER (Indexing)
Instruction:
Your task is to extract named entities from the given paragraph.
Respond with a JSON list of entities.
One-Shot Demonstration:
Paragraph:
```
Radio City
Radio City is India's first private FM radio station and was started on 3 July 2001. It plays Hindi, English
and regional songs. Radio City recently forayed into New Media in May 2008 with the launch of a music
portal -PlanetRadiocity.comthat offers music related news, videos, songs, and other music-related
features.
```
{"named_entities": ["Radio City", "India", "3 July 2001", "Hindi","English", "May 2008",
"PlanetRadiocity.com"]}
Input:
Paragraph:
```
PASSAGE TO INDEX
```
Figure7: PromptforpassageNERduringindexing.
Query NER (Retrieval)
Instruction:
You’re a very effective entity extraction system. Please extract all named entities that are important for
solving the questions below. Place the named entities in JSON format.
One-Shot Demonstration:
Question: Which magazine was started first Arthur’s Magazine or First for Women?
{"named_entities": ["First for Women", "Arthur’s Magazine"]}
Input:
Question: QUERY TO INDEX
Figure8: PromptforqueryNERduringretrieval.
I LLMPrompts
ThepromptsweusedforindexingandqueryNERareshowninFigure7andFigure8,whilethe
OpenIEpromptisshowninFigure9.
27Open Information Extraction
Instruction:
Your task is to construct an RDF (Resource Description Framework) graph from the given passages and
named entity lists.
Respond with a JSON list of triples, with each triple representing a relationship in the RDF graph.
Pay attention to the following requirements:
- Each triple should contain at least one, but preferably two, of the named entities in the list for each
passage.
- Clearly resolve pronouns to their specific names to maintain clarity.
Convert the paragraph into a JSON dict, it has a named entity list and a triple list.
One-Shot Demonstration:
Paragraph:
```
Radio City
Radio City is India’s first private FM radio station and was started on 3 July 2001.It plays Hindi, English
and regional songs. Radio City recently forayed into NewMedia in May 2008 with the launch of a music
portal - PlanetRadiocity.com thatoffers music related news, videos, songs, and other music-related
features.
```
{"named_entities": ["Radio City", "India", "3 July 2001", "Hindi","English", "May 2008",
"PlanetRadiocity.com"]}
{"triples":
[
["Radio City", "located in", "India"],
["Radio City", "is", "private FM radio station"],
["Radio City", "started on", "3 July 2001"],
["Radio City", "plays songs in", "Hindi"],
["Radio City", "plays songs in", "English"],
["Radio City", "forayed into", "New Media"],
["Radio City", "launched", "PlanetRadiocity.com"],
["PlanetRadiocity.com", "launched in", "May 2008"],
["PlanetRadiocity.com", "is", "music portal"],
["PlanetRadiocity.com", "offers", "news"],
["PlanetRadiocity.com", "offers", "videos"],
["PlanetRadiocity.com", "offers", "songs"]
]
}
Input:
Convert the paragraph into a JSON dict, it has a named entity list and a triple list.
Paragraph:
```
PASSAGE TO INDEX
```
{"named_entities": [NER LIST]}
Figure9: PromptforOpenIEduringindexing.
28