An Empirical Study of Training State-of-the-Art LiDAR Segmentation Models
JiahaoSun♡,1,2,∗ XiangXu♡,3,∗ LingdongKong♡,1,4,∗ YouquanLiu♡,5 LiLi♡,6
ChenmingZhu♡,1,7 JingweiZhang♡,1 ZeqiXiao♡,8 RunnanChen♡,7
TaiWang♡,1 WenweiZhang♡,1 KaiChen♡,1 ChunmeiQing♡,2
♡MMDetection3DContributors
1ShanghaiAILaboratory 2SouthChinaUniversityofTechnology
3NanjingUniversityofAeronauticsandAstronautics 4NationalUniversityofSingapore
5FudanUniversity 6DurhamUniversity 7UniversityofHongKong
8NanyangTechnologicalUniversity,Singapore
https://github.com/open-mmlab/mmdetection3d
SemanticKITTI nuScenes
Abstract
74 mIoU(%) 84 mIoU(%)
71.8
Intherapidlyevolvingfieldofautonomousdriving,pre-
80.9
cisesegmentationofLiDARdataiscrucialforunderstand- 71 81 80.1
69.4
ingcomplex3Denvironments.Traditionalapproachesoften
relyondisparate, standalone codebases, hindering unified 68 78
76.2
advancements and fair benchmarking across models. To
address these challenges, we introduce MMDetection3D- 65 75
lidarseg, a comprehensive toolbox designed for the ef- 63.8 63.6 73.3
ficient training and evaluation of state-of-the-art LiDAR
62 72
segmentation models. We support a wide range of seg- MinkUNet Cylinder3D MinkUNet Cylinder3D
mentation models and integrate advanced data augmen-
MMDet3D-lidarseg Pointcept Cylinder3DRepo
tation techniques to enhance robustness and generaliza-
tion. Additionally,thetoolboxprovidessupportformultiple
Figure1.Performancecomparisonsofstate-of-the-artLiDARseg-
leadingsparseconvolutionbackends, optimizingcomputa-
mentationmodels[20,102]fromdifferentcodebasesonthevali-
tional efficiency and performance. By fostering a unified
dationsetsoftheSemanticKITTI[4]andnuScenes[28]datasets.
framework,MMDetection3D-lidarsegstreamlinesdevelop-
mentandbenchmarking,settingnewstandardsforresearch
andapplication. Ourextensivebenchmarkexperimentson
sure distances to objects, enabling the capture of high-
widely-used datasets demonstrate the effectiveness of the
resolution, three-dimensional information about the envi-
toolbox. Thecodebaseandtrainedmodelshavebeenpub-
ronment when mounted on vehicles, drones, or other plat-
licly available, promoting further research and innovation
forms[25,32,86]. Thiscapabilityiscrucialforapplications
inthefieldofLiDARsegmentationforautonomousdriving.
suchasautonomousdriving,whereunderstandingcomplex
3Denvironmentsisessentialforsafenavigation[3,35,61].
LiDARprovidesrichspatialdetailsthatcomplementother
sensors like cameras and radars, offering a robust founda-
1.Introduction tionforperceivingandinterpretingthesurroundingworld.
LiDAR segmentation is the process of classifying indi-
LiDAR (Light Detection and Ranging) is an advanced
vidual points in a LiDAR-generated point cloud into dis-
remote sensing technology that uses laser pulses to mea-
tinctsemanticcategories,suchasvehicles,pedestrians,and
roadways [4,8,28,90]. This segmentation is fundamental
*Jiahao,Xiang,andLingdongcontributedequallytothiswork.
*WorkdonewhenJiahaowasaninternatShanghaiAILaboratory. forinterpretingthestructureandtypeofobjectswithinthe
1
4202
yaM
32
]VC.sc[
1v07841.5042:viXraTable1. Anoverviewofthesupportedmodels,sparseconvolutionbackends,and3DdataaugmentationtechniquesfromexistingLiDAR
segmentationcodebases.MMDet3D-lidarsegisanabbreviationofourcodebase,“✓”denotesfeaturesthatareofficiallysupported.
MMDet3D- LaserMix Pointcept PCSeg SPVNAS Cylin3D PVKD
lidarseg [44] [22] [58] [78] [102] [36]
Model
MinkUNet[20] ✓ ✓ ✓ ✓ ✗ ✗ ✗
SPVCNN[78] ✓ ✓ ✓ ✓ ✓ ✗ ✗
Cylinder3D[102] ✓ ✓ ✗ ✓ ✗ ✓ ✓
PolarNet[100] ✓ ✓ ✗ ✗ ✗ ✗ ✗
CENet[17] ✓ ✗ ✗ ✗ ✗ ✗ ✓
FRNet[95] ✓ ✗ ✗ ✗ ✗ ✗ ✗
Backend
MinkowskiEngine[20] ✓ ✓ ✓ ✗ ✗ ✗ ✗
SpConvv1[97] ✓ ✓ ✗ ✗ ✗ ✓ ✓
SpConvv2[23] ✓ ✗ ✓ ✗ ✗ ✗ ✗
TorchSparse[77] ✓ ✓ ✓ ✓ ✓ ✗ ✗
TorchSparse++[79] ✓ ✗ ✗ ✗ ✓ ✗ ✗
DataAugmentation
LaserMix[44] ✓ ✓ ✗ ✓ ✗ ✗ ✗
PolarMix[89] ✓ ✗ ✗ ✓ ✗ ✗ ✗
FrustumMix[95] ✓ ✗ ✗ ✗ ✗ ✗ ✗
TestTimeAugmentation ✓ ✗ ✓ ✓ ✗ ✗ ✓
environment,whichiscriticalforsituationalawarenessand In response to the aforementioned challenges, in this
decision-makinginautonomoussystems[2,7,18,47,54,72]. work, we present MMDetection3D-lidarseg, a state-of-
Effective LiDAR segmentation enhances an autonomous the-art toolbox designed to unify, optimize, and stream-
vehicle’s ability to navigate complex scenarios, avoid ob- line the training and benchmarking of LiDAR segmenta-
stacles,andmakeinformeddecisions[9,10,57,64,74,99]. tion models. MMDetection3D-lidarseg integrates multiple
advanced features to facilitate cohesive and efficient de-
Despite its importance, the recent development of Li-
velopment, offeringaunifiedframeworkthatenhancesthe
DARsegmentationmodelshasfacedsignificantchallenges.
comparabilityandreproducibilityofresearchfindings. Our
The landscape is fragmented, with numerous standalone
toolbox supports the exploration and comparison of vari-
and often incompatible codebases [36,78,102]. As shown
ous sparse convolutional backends, providing a standard-
in Tab. 1, this fragmentation creates inefficiencies in re-
ized benchmark that improves the performance of widely
searchanddevelopment,makingitdifficulttoperformfair
usedsegmentationmodels.
and comprehensive comparisons between different mod-
Ourdesignemphasizesseveralkeyprinciplesasfollows:
els [44,58,87]. Additionally, many existing solutions of-
ten struggle to meet the demands of dynamic and var- • gUnifiedFramework: ByconsolidatingvariousLi-
ied autonomous driving environments, which require seg- DAR segmentation models and techniques into a sin-
mentation frameworks that are flexible, scalable, and ro- gle, comprehensive toolbox, we eliminate the frag-
bust[15,21,47,61,62,65,68,69]. mentationofcodebases,enablingmoreefficientdevel-
One major challenge in LiDAR segmentation is the in- opmentandeasierbenchmarkingofmodels.
tegration of different sparse convolution backends [20,23,
• (cid:247)Optimization&Efficiency: Ourtoolboxincludes
77,79,97],whichareessentialforefficientlyprocessingthe
optimized implementations of state-of-the-art algo-
sparseandirregularnatureofLiDARpointclouds[5,49,50,
rithms, ensuring that models can be trained and eval-
53]. Sparseconvolutionalnetworkshavedemonstratedsig-
uated quickly and effectively. This is crucial for both
nificant improvements in performance and computational
academicresearchandreal-worldapplications, where
efficiency for 3D point cloud processing [78]. However,
timeandresourceefficiencyareparamount.
exploring and comparing these backends within a unified
framework has been challenging due to the lack of stan- • A Flexibility & Scalability: The MMDetection3D-
dardizedtoolsandbenchmarks[21,58]. lidarsegtoolkitisdesignedtobeflexibleandscalable,
2capable of adapting to different driving scenarios and may sacrifice some 3D spatial information [1,40,41,95].
handling large-scale point cloud data. This flexibility The evolution of LiDAR semantic segmentation has been
ensures that the toolbox can meet the evolving needs markedbycontinuousexplorationofthesemethodologies,
ofresearchersandpractitionersinthefield. each offering unique trade-offs in terms of accuracy, com-
putationalefficiency, andscalability[3,70,76,85]. Recent
•  ComprehensiveBenchmarking: Tofacilitatefair advancementshavefocusedonintegratingmulti-modaldata
andcomprehensivecomparisons,weprovideasuiteof sources[13,14,19,39,55,59,60,71,96],enhancingreal-time
standardized benchmarks that cover a wide range of processingcapabilities[12,17,43,83,95,101],andimprov-
tasks, including fully-, semi-, and weakly-supervised ing model robustness under diverse environmental condi-
LiDARsegmentation. Thesebenchmarkshelpensure tions[33,34,42,45,92,98]. Theseadvancementshighlight
that different models can be evaluated on a common the dynamic nature of research in LiDAR segmentation,
ground,promotingtransparencyandrigorinresearch. strivingtobalancethecompetingdemandsofprecision,ef-
ficiency,androbustness[11,31,98].
• ⋔ Public Availability & Collaborations: By mak-
ing the codebase and trained models publicly avail- 2.2.3DOperators
able,weencouragecollaborationandopeninnovation
SparseconvolutionisafundamentaloperatorinLiDAR
in the field of LiDAR segmentation. This openness
segmentation. Tounderstanditssignificance, wefirstcon-
helpsacceleratethepaceofadvancementsandfosters
sidertheN-dimensionaldenseconvolution.Letxinandxout
acommunity-drivenapproachtoproblem-solving. i j
representtheinputandoutputfeatures,respectively,where
MMDetection3D-lidarseg represents a significant step i and j are N-dimensional coordinates (e.g., i = (2,1,1)
forward in the development of LiDAR segmentation mod- in3Dspace). ThesizeoftheconvolutionkernelisK,with
els, addressing key challenges and setting new standards kerneloffsets∆N(K). Theweightsoftheconvolutionker-
for research and application. As shown in Fig. 1, our im-
nel,W,areofshapeKN ×Cin×Cout,whereCinandCout
plemented models achieve consistent and significant per- represent the dimension of input and out features, respec-
formance improvements over the existing codebases. By tively. Foreachoffsetδ(e.g.,δ =(0,1,−1)∈∆3(3)),the
providingacomprehensiveandflexibleplatform,ourtool- weightsW canbebrokendownintoKN matricesofshape
box aims to accelerate innovation in autonomous driving Cin×Cout,denotedasW δ. Denseconvolutionwithstrides
technologies,ultimatelycontributingtothedevelopmentof canthenbeformulatedasfollows:
saferandmorereliableautonomoussystems. (cid:88)
xout = xin ·W . (1)
i s·i+δ δ
2.RelatedWork δ∈∆N(K)
2.1.LiDARSemanticSegmentation Here,thebiasinconvolutionisignoredforsimplicity.How-
ever, indatasetscommonlyusedforLiDARsegmentation,
Segmenting LiDAR-acquired point clouds into mean- voxelized point cloud samples often have less than 1% of
ingful semantic categories is indispensable for achieving nonzerovoxels. Thissparsitymakesdenseconvolutionin-
comprehensive 3D scene understanding, especially in the efficient due to the large number of meaningless computa-
context of autonomous driving [30, 82, 91]. The evo- tions,leadingtolimitedmodelperformance. Sparseconvo-
lution of LiDAR semantic segmentation methodologies lution addresses this inefficiency by computing only when
has been characterized by a transition from early hand- theinputvoxelisnonzero. Itcanbeformulatedasfollows:
crafted features [48, 88] and traditional machine learn-
(cid:88) (cid:88)
ing techniques [26,27] to the dominance of deep learn- xout = 1[s·j+δ]·xin ·W , (2)
i s·j+δ δ
ing approaches [37, 38, 81]. Convolutional neural net-
δ∈∆N(K) j
works(CNNs),initiallydevelopedforimageanalysis,have
been adapted for processing unstructured point cloud data where1[·]isabinaryindicator. Thecomputationofsparse
throughinnovativestrategies[84].Onesuchstrategyisvox- convolutionisdeterminedbymapsM=(i,j,δ).Forgiven
elization [20,35,78,102], which transforms point clouds input coordinates and convolution kernel size, sparse con-
into a structured 3D grid format suitable for standard 3D volution first calculates the output coordinates and gener-
CNN operations. However, voxelization often incurs high atesmapsM,thenperformstheconvolutionoperation. In
computational costs and memory usage due to the spar- essence,sparseconvolutionmodifiesthedefinitionofdense
sity of point cloud data. Alternatively, projection-based convolutionbyperformingcomputationsonlyatasparseset
methods[24,56,67,93,100]rasterizepointcloudsinto2D ofoutputlocationsratherthanacrosstheentirefeaturemap.
range images or bird’s eye view maps, leveraging the effi- It is arguably the most critical building block for state-of-
ciencyof2DCNNs. Despitetheirefficiency,thesemethods the-art voxel-based LiDAR segmentation models. Various
3backends have been developed to efficiently handle sparse 3.1.Datasets
convolution tasks, and MMDetection3D-lidarseg supports
• SemanticKITTI[4]isapioneeringdatasetforLiDAR
five prevailing sparse convolution backends: SpConv [97],
segmentation, offering an extensive collection of an-
MinkowskiEngine [20], TorchSparse [77], SpConv2 [23],
notated3Dpointcloudsderivedfromreal-worlddriv-
andTorchSparse++[79]. Wealsocomparetheadvantages
ingscenarios. ItextendstheKITTIVisionBenchmark
anddisadvantagesofthesesparseconvolutionsintheexper-
Suite [32] by providing dense, per-point annotations
imentalsection,aimingtoassistresearchersinselectingthe
across a wide range of categories, including various
mostsuitablelibraryfortheirspecificneeds.
typesofvehicles,pedestrians,andbothnaturalandur-
banstructures. Notably,thisdatasetfeaturestemporal
2.3.3DDataAugmentations consistency,capturingthedynamicnatureofurbanen-
vironmentsthroughsequencesofscansovertime. The
Dataaugmentationplaysacrucialroleinenhancingthe extensive coverage and challenging scenarios make
performance and generalizability of deep learning models, SemanticKITTIafundamentalresourceforadvancing
especiallyindomainscharacterizedbylimitedtrainingdata andbenchmarkingLiDARsegmentationmodelsinthe
orsubstantialvarianceinoperationalconditions[44,89,95]. contextofautonomousdriving.
In the context of LiDAR segmentation, 3D data augmen-
• nuScenes-lidarseg [28], a variant of the well-known
tations are essential for simulating a wide range of sce-
nuScenes [8] dataset, serves as a diverse and large-
narios and environmental conditions that autonomous ve-
scale benchmark for semantic segmentation in au-
hiclesmayencounter[42]. Techniquessuchasrandomro-
tonomous vehicle sensor data. This dataset includes
tations, scaling, and jittering of point clouds help models
detailed annotations for a variety of object classes
becomerobusttovariationsinobjectorientationsandsizes.
acrossnumerousscenescapturedindifferenturbanen-
Moresophisticatedmethods,includingsyntheticocclusion,
vironments and weather conditions, utilizing LiDAR,
simulatingvaryingsensorranges,andgeneratingadversar-
radar, and camera data. The complexity and diver-
ial examples, further enrich the model’s exposure to chal-
sity of nuScenes, featuring night scenes and adverse
lenging conditions, thereby improving its real-world per-
weather conditions, present significant challenges for
formance [46,54]. Additionally, domain adaptation tech-
segmentationmodels,makingitaninvaluableresource
niques aim to bridge the gap between synthetic and real-
fortestingrobustnessandversatility. Additionally,its
worlddata,enablingmodelstrainedprimarilyonsimulated
multi-modal nature supports research into sensor fu-
environmentstotransfertheircapabilitiestoreal-worldset-
sion techniques, enhancing its contribution to the de-
tings [6,39,43,52,66,73,94]. Continuous innovation in
velopment of comprehensive perception systems for
3D data augmentation techniques is pivotal for advancing
autonomousdriving.
thestate-of-the-artinLiDARsegmentation,ensuringmod-
els are not only accurate but also resilient to the diverse
• ScribbleKITTI [85] introduces an innovative ap-
challenges they face in real-world applications [42,92].
proachtodatasetannotationforLiDARsegmentation.
These augmentations, coupled with robust training frame-
It shares the same scenes with SemanticKITTI [4]
works like MMDetection3D-lidarseg, provide a compre-
but is annotated with line scribbles rather than dense,
hensivetoolkitfordevelopingnext-generationLiDARseg-
per-point labeling. This method significantly reduces
mentationmodelscapableofmeetingthedemandsofmod-
theannotationeffortwhilestillprovidinghigh-quality
ernautonomoussystems[47].
training data for weakly-supervised learning algo-
rithms. This efficiency of scribble-based annotations
enables the creation of extensive datasets with less
3.MMDetection3D-lidarseg
manualeffort,facilitatingthedevelopmentandtesting
ofadvancedsegmentationmodelsinautonomousdriv-
MMDetection3D-lidarseg includes high-quality imple-
ingapplications.
mentationsofpopularLiDARsemanticsegmentationmod-
els and sparse convolution backends. A summary of sup-
3.2.Models
portedmodelsandbackends,comparedtoothercodebases,
is provided in Tab. 1. MMDetection3D-lidarseg supports • MinkUNet [20] is a highly efficient model for Li-
a wider range of models and backends than other code- DARsegmentation,builtontheMinkowskiEngine. It
bases, offeringunparalleledflexibilityandcomprehensive- employssparseconvolutionaloperationstoefficiently
ness. The following is a brief introduction to the datasets, processpointclouddata,extendingtheU-Netarchitec-
models,dataaugmentationtechniques,andsparseconvolu- tureinto3Dspaces. Thisdesignenablesthesegmen-
tionbackendsthatwesupport. tationoflarge-scalepointcloudsbyleveragingthein-
4herentsparsityofLiDARdata,significantlyenhancing Voxel-based Segmentor
computationalefficiency. Decoder Head
Voxelization Voxel Feature * 3D Backbone
• SPVCNN [78] combines the strengths of point-based Encoder *
Auxiliary Head
and voxel-based processing methods to achieve high
Point Feature*
efficiency and accuracy. This hybrid approach pro- Encoder
videsascalablesolutionthatadaptstothedensityvari-
ations of point cloud data, ensuring detailed feature
Projection-based Segmentor
extractionwhilemaintainingcomputationalefficiency
Decoder Head
acrossdiversescenes. Pixel Feature*
Projection 2D Backbone
Encoder *
Auxiliary Head
• Cylinder3D [102] introduces a cylindrical partition-
ing strategy that aligns more naturally with the dis- Point Feature*
Encoder
tribution of point clouds captured by circular LiDAR
scans. This approach enhances segmentation perfor-
Figure2. Overviewofvoxel-basedandprojection-basedLiDAR
mance, particularly for cylindrical objects like poles
segmentors illustrated with abstractions in the MMDetection3D-
andtrees,bypreservingtheirgeometricintegrity.
lidarsegcodebase.Modulesmarkedwith∗areoptional.
• CENet[17]focusesoncapturingandutilizingcontex-
tualinformationfromtherangeviewofLiDARpoint
image to extract features, which are subsequently decoded
cloudsforefficientsegmentation,withmulti-levelfea-
intosemanticlabels[87,93].
turefusionand.
Both types of segmentors may also incorporate a point
• PolarNet [100] transforms LiDAR point clouds into featureencodertodirectlyprocessrawpointclouddataand
a polar bird’s eye view representation, facilitating ef- anauxiliaryheadtoprovideadditionaloutputsorintermedi-
ficient processing through partitioning the space into atesupervision,enhancingthemodel’soverallperformance
polarbins. Thisrepresentationisparticularlyadvanta- and robustness [42,95,102]. With these abstractions, the
geous for LiDAR data, aligning with the sensor’s na- frameworkofvoxel-basedandprojection-basedsegmentors
tivedatastructureandsimplifyingthedetectionofob- isillustratedinFig.2.Themodulardesignofthesesegmen-
jectsin360-degreesurroundings. torsallowsresearcherstodeveloptheirownmodelsbycre-
ating new components and assembling them with existing
• FRNet[95]introducesanovelFrustum-Rangerepre- ones. Thisflexibilitynotonlyacceleratesthedevelopment
sentation for scalable LiDAR segmentation and pro- processbutalsofacilitatesexperimentationandinnovation
poses two novel 3D data augmentation techniques, inthefieldofLiDARsegmentation.
FrustumMixandRangeInterpolation,toassistintrain-
ingrobustandgeneralizablemodels. Itachievescom- 3.3.DataAugmentationTechniques
petitiveperformancewhilestillmaintainingpromising
• LaserMix[44]introducesanovelaugmentationtech-
scalabilityforreal-timeLiDARsegmentation.
nique specifically designed for LiDAR point clouds,
inspiredbytheconceptofmixupforimagedata. This
AlthoughthearchitecturesofvariousLiDARsegmenta-
techniqueinvolvesmixingmultiplepointcloudsalong
tion models differ, they can generally be categorized into
the azimuth and inclination directions, significantly
voxel-based and projection-based segmentors, each shar-
enhancingthemodel’sabilitytolearngeneralizedfea-
ingcommoncomponents[1,41,96]. Voxel-basedsegmen-
turesandimproverobustnessagainstoverfitting.
tors[20,78,102]typicallyconsistofavoxelizationmodule,
avoxelfeatureencoder,a3Dbackbone,andadecodehead. • PolarMix [89] generates synthetic samples by mix-
Thesecomponentsworktogethertoconvertrawpointcloud ingpolarrepresentationsofLiDARpointclouds. This
data into a structured voxel grid, extract meaningful fea- methodisparticularlyeffectiveforaugmentingdatain
tures,processthesefeaturesthroughadeep3Dneuralnet- scenarios where the spatial relationships and orienta-
work,andfinallydecodetheprocessedfeaturesintoseman- tionsofobjectsarecriticalforunderstandingthescene.
ticlabels. Conversely,projection-basedsegmentors[17,95]
include a projection module, a pixel feature encoder, a 2D • FrustumMix [95] enhances context awareness by
backbone,andadecodehead. Theprojectionmodulecon- swapping frustum regions from two scenes. This
verts the 3D point cloud into a 2D image representation, method splits two point clouds into several non-
suchasarangeimageorbird’seyeviewmap[67,100].The overlapping frustum regions along the inclination or
pixel feature encoder and 2D backbone then process this azimuth direction. FrustumMix maintains the invari-
5ant structure within the frustum while enhancing the always have the same size as the maps correspond-
contextualrelationshipsbetweenfrustums. ing to the symmetric kernel offset (−a,−b,−c),
TorchSparse applies adaptive matrix multiplication
• Test Time Augmentation (TTA), typically applied
grouping to trade computation for better regularity,
duringmodelinference, enhancessegmentationaccu- achieving a 1.4× to 1.5× speedup for matrix multi-
racy by applying various transformations (e.g., rota-
plication. TorchSparsealsooptimizesdatamovement
tions, scaling) to the input data at test time and ag-
byadoptingvectorized,quantized,andfusedlocality-
gregatingthepredictions. TTAmitigatestheimpactof
aware memory access, reducing the memory move-
noiseandvariationsinthepointclouddata,resultingin mentcostby2.7×.
more consistent and reliable segmentation outcomes,
particularlyinchallengingorambiguousscenes.How- • SpConv v2 [23] extends the well-known implicit
ever, it requires significantly more time during infer- GEMM formulation for 2D convolution to 3D sparse
ence,makingitlesssuitableforreal-timeapplications convolution. The sparse convolution workload is
suchasautonomousdrivingperception. equivalent to a dense GEMM. Similar to fetch-
on-demand, implicit GEMM overlaps computation
3.4.SparseConvolutionBackends with memory access, hiding memory latency through
pipelining. Like “im2col” in 2D convolution, an
• SpConv [97] can be regarded as a vanilla implemen-
implicit GEMM implementation is output-stationary,
tation of the sparse convolution. SpConv employs
achievingthetheoreticalminimumDRAMwrite-back
thegather-GEMM-scatterdataflowonGPUs,whichis
traffic. However, despite lower DRAM traffic com-
weight-stationaryandfeaturesanouterhostloopover
pared to fetch-on-demand, implicit GEMM has non-
kernel offsets. For each offset, it computes a kernel
negligible redundant computation. To address this is-
map, gathers all input features into a K2 × Cin ma-
sue,SpConvv2excludesunsortedimplicitGEMMin
trixinDRAM,andmultipliestheweightsusingdense
their design space and uses bitmask sorting to mini-
matrix multiplication, handled by existing vendor li-
mize computation overhead. Each output point is as-
braries like cuBLAS and cuDNN. SpConv then scat-
signedaK2 dimensionalbitmaskindicatingthepres-
ters the results back to output positions. This imple-
ence of its neighbors. These bitmasks are treated as
mentationonlyneedstooptimizescatterandgatherin
numbers and sorted, adjusting the computation order
CUDA.However,thisdataflowisfundamentallyinef-
fordifferentoutputsaccordingly. Inpracticalapplica-
ficientduetothelackofoverlapbetweencomputation
tions,sortingcanreduceredundantcomputationbyup
andmemoryaccess.
to3×.
• MinkowskiEngine [20] takes a fetch-on-demand ap-
• TorchSparse++ [79] simultaneously supports gather-
proach and merges the gather, matrix multiplication,
GEMM-scatter, fetch-on-demand, and sorted im-
and scatter kernel calls into a single CUDA kernel.
plicit GEMM dataflows. Overlapped computation
Instead of materializing the K2 × Cin matrix gather
and memory access can be achieved with relatively
buffer in DRAM, it fetches input features on demand
low engineering complexity compared to SpConv2.
into the L1 shared memory, performs matrix multi-
TorchSparse++ creates a highly efficient Sparse Ker-
plication in the on-chip storage, and directly scatters
nelGeneratorthatgeneratesperformantsparseconvo-
the partial sums (residing in the register file) to cor-
lutionkernelsatlessthanone-tenthoftheengineering
responding outputs position without first instantiating
cost of current state-of-the-art systems. Additionally,
theminaDRAMscatterbuffer. Thismethodoverlaps
TorchSparse++designedtheSparseAutotuner,which
computation with memory access and saves DRAM
extendsthedesignspaceofexistingsparseconvolution
writes to gather and scatter buffers. However, it can-
librariesandsearchesforthebestdataflowconfigura-
notavoidDRAMwritestothefinaloutputtensor, re-
tionsfortrainingandinferenceworkloads.
sulting in 4× to 10× larger write-back traffic than
thetheoreticaloptimalvalue. Furthermore,theblock- 4.Experiments
fused fetch-on-demand dataflow suffers from write-
In this section, we embark on comprehensive bench-
backcontentionsbetweendifferentthreads.
marksofdiverseLiDARsegmentationmodelsfacilitatedby
• TorchSparse [77] directly optimizes the two bottle- MMDetection3D-lidarseg. Firstly,weprovideanoverview
necks of the vanilla gather-GEMM-scatter dataflow: of the datasets, metrics, and detailed implementations em-
irregular computation and data movement. Due to ployed within the codebase. Subsequently, we delve into
sparse convolutions with odd kernel size and a stride experimental results from distinct setups, encompassing
of1,themapscorrespondingtokerneloffsets(a,b,c) fully-, semi-, and weakly-supervised learning paradigms.
6Table 2. Results of state-of-the-art LiDAR segmentation mod- 71 mIoU (%)
Voxel
els [17, 20, 78, 95, 100, 102] on the validation sets of Se-
Mink-50-w32
Bird’s Eye View
manticKITTI[4]andnuScenes[28]. Mixdenoteacombination 31.8M
Mink-34-w32
of LaserMix [44] and PolarMix [89] or FrustumMix [95]. TTA 68 37.9M Range View
denotestesttimeaugmentation.Allscoresaregiveninpercentage Mink-18-w32 Fusion
(%).Thebestscoresundereachmetricarehighlightedinbold. 21.7M
65 SPVCNN
SemKITTI nuScenes 21.8M
Model Mix TTA
mIoU mAcc mIoU mAcc
Mink-18-w20
Mink-18-w16
Representation:Voxel 62 8.8M 5.4M FRNet
Cylinder3D 10.0M
✗ ✗ 66.9 92.4 76.4 94.1 55.9M
MinkUNet ✓ ✗ 70.4 92.8 77.6 94.2 PolarNet CENet
59 6.8M
✓ ✓ 71.8 93.2 80.1 94.7 13.6M
FPS
✗ ✗ 63.7 91.0 75.8 93.7
(Hz)
Cylinder3D ✓ ✗ 67.0 91.7 79.3 94.3
56
✓ ✓ 69.4 92.5 80.9 94.5 4 9 14 19 24 29 34
Representation:Bird’sEyeView
Figure3.Performancecomparisonsofstate-of-the-artLiDARseg-
✗ ✗ 57.2 91.0 71.7 93.1 mentationmodels[17,20,78,95,102]fromdifferentLiDARrepre-
PolarNet ✓ ✗ 60.1 91.3 72.3 93.2 sentationgroups(voxel,bird’seyeview,rangeview,fusion)onthe
✓ ✓ 60.7 91.7 72.3 93.4 validationsetofSemanticKITTI[4]. Wereportthesegmentation
accuracy(mIoU),inferencelatency(FPS),andmodelparameters.
Representation:RangeView
Thelargertheareacoverage,thelargerthemodelcapacity.
✗ ✗ 61.9 90.3 - -
CENet
✓ ✗ 62.2 90.5 - -
✗ ✗ 64.1 92.2 76.8 93.4 dation, and 100 scenes for testing. It is annotated with 32
FRNet ✓ ✗ 67.6 92.3 77.7 94.0 semantic categories, and a merged set of 16 labels is used
✓ ✓ 68.7 92.5 79.0 94.3 forevaluation. ScribbleKITTI[85]sharesthesamescenes
as SemanticKITTI [4] but is annotated with line scribbles.
Representation:Fusion
It contains 189 million labeled points, with approximately
✗ ✗ 66.4 92.5 76.0 93.9 8.06%labeledpointsfortraining.
SPVCNN
✓ ✗ 68.4 92.3 77.0 94.1
Implementation Details. In our experiments, we employ
AdamW [63] as the default optimizer. We set the ini-
tial learning rate to 0.01 and utilize the OneCycle sched-
Finally,wedelveintoaseriesofablationstudies,aimingto uler [75] to dynamically adjust the learning rate during
scrutinize and compare the various components facilitated training.TrainingisperformedacrosseightA100GPUsfor
within the codebase. Through these rigorous evaluations, 50 epochs on SemanticKITTI [4] and ScribbleKITTI [85]
we aim to provide valuable insights into the performance datasets.ForthenuScenes[28]dataset,weextendthetrain-
and efficacy of LiDAR segmentation modes, contributing ingto80epochstoensureoptimalperformance. EachGPU
toadvancementsinthiscriticalfield. operateswithabatchsizeof2,makingefficientuseofpar-
allelprocessingcapabilities. Ourdefaultdataaugmentation
4.1.ExperimentalSettings pipeline includes point cloud rotation, translation, scaling,
andflippingontheglobalview. Additionally,weintroduce
Datasets. Weconductcomprehensiveexperimentsonthree
a randomized choice of LaserMix [44] and PolarMix [89]
popularLiDAR-basedsemanticsegmentationbenchmarks.
forthevoxel-based,fusion-based,andbird’seyeviewmod-
SemanticKITTI [4] is a large-scale outdoor dataset con-
els,whileadoptingFrustumMix[95]forrange-viewmodels
sistingof22sequencescollectedfromvariousscenariosin
to achieve better performance. For detailed configurations
Karlsruhe,Germany.Typically,sequences00to10areused
andreproducibility,pleaserefertoourGitHubrepository1.
for training, while sequence 08 is used for validation. All
Evaluation Metrics. For performance evaluation, we ad-
points are annotated with 28 classes, and a merged set of
heretostandardprotocolsbyemployingmeanIntersection-
19classesisusedforevaluation. nuScenes[28]isamulti-
over-Union(mIoU)astheprimarymetricandmeanAccu-
modaldatasetwidelyusedinautonomousdriving. Itiscol-
racy(mAcc)asasupplementarymetric.Inadditiontothese
lectedaroundstreetsinBostonandSingaporeandfeatures
metrics, Weevaluate thereal-timeprocessing applicability
sparserLiDARpoints. Thedatasetincludes1,000driving
scenes, with 750 scenes for training, 150 scenes for vali- 1https://github.com/open-mmlab/mmdetection3d
7Table3.Resultsofdata-efficientlearningalgorithms[16,29,44,51,80,85,95,103]withstate-of-the-artLiDARsegmentationbackbones[20,
78,95,100–102]onthesemi-supervisedlearningbenchmarksoftheSemanticKITTI[4],nuScenes[28],andScribbleKITTI[85]datasets,
respectively. Allmethodsaretestedontheofficialvalidationsets. Thelabeleddataareuniformlysampledwiththe1%,10%,20%,and
50% quota from the training set of the original dataset. Sup.-Only denotes training with the labeled data only, while the data-efficient
learningmethodsusebothlabeledandunlabeleddata.†Lim3D[51]adoptsadifferentdatasamplingstrategyinsteadofuniformsampling.
Allscoresaregiveninpercentage(%).Thebestscoresundereachmetricarehighlightedinbold.
SemanticKITTI nuScenes ScribbleKITTI
Model Venue Backbone
1% 10% 20% 50% 1% 10% 20% 50% 1% 10% 20% 50%
Representation:RangeView
Sup.-Only - 44.9 60.4 61.8 63.1 51.9 68.1 70.9 74.6 42.4 53.5 55.1 57.0
LaserMix[44] CVPR’23 FRNet 52.9 62.9 63.2 65.0 58.7 71.5 72.3 75.0 45.8 56.8 57.7 59.0
FrustumMix[95] arXiv’23 55.8 64.8 65.2 65.4 61.2 72.2 74.6 75.4 46.6 57.0 59.5 61.2
Representation:Bird’sEyeView
Sup.-Only - 45.1 54.6 55.6 56.5 50.9 67.5 69.5 71.0 42.6 52.8 53.4 54.4
MeanTeacher[80] NeurIPS’17 PolarNet 47.4 55.6 56.6 57.1 51.9 68.1 69.7 71.1 43.7 53.4 54.4 54.9
LaserMix[44] CVPR’23 51.0 57.7 58.6 60.0 54.0 69.5 70.8 71.9 45.7 55.5 56.0 56.6
Representation:Voxel
Sup.-Only - 45.4 56.1 57.8 58.7 50.9 65.9 66.6 71.2 39.2 48.0 52.1 53.8
MeanTeacher[80] NeurIPS’17 45.4 57.1 59.2 60.0 51.6 66.0 67.1 71.7 41.0 50.1 52.8 53.9
LaserMix[44] CVPR’23 Cylinder3D 50.6 60.0 61.9 62.3 55.3 69.9 71.8 73.2 44.2 53.7 55.1 56.8
LiM3D†[51] CVPR’23 - 61.6 62.6 62.8 - - - - - 60.3 60.5 60.9
FrustumMix[95] arXiv’23 55.7 62.5 63.0 64.9 60.0 70.0 72.6 74.1 45.6 55.7 58.2 60.8
Sup.-Only - 53.9 64.0 64.6 65.4 58.3 71.0 73.0 75.1 48.6 57.7 58.5 60.0
MeanTeacher[80] NeurIPS’17 MinkUNet 56.1 64.7 65.4 66.0 60.1 71.7 73.4 75.2 49.7 59.4 60.0 61.7
LaserMix[44] CVPR’23 60.9 66.6 67.2 68.0 62.8 73.6 74.8 76.1 57.2 61.1 61.4 62.4
Representation:Fusion
Sup.-Only - 52.7 64.1 64.5 65.1 57.9 71.7 73.0 74.6 47.2 57.3 58.2 58.8
MeanTeacher[80] NeurIPS’17 SPVCNN 54.4 64.8 65.2 65.7 59.4 72.5 73.1 74.7 49.9 58.3 58.6 59.1
LaserMix[44] CVPR’23 60.3 66.6 67.0 67.6 63.2 74.1 74.6 75.8 57.1 60.8 60.7 61.0
of models by measuring their inference speed. We quan- 4.2.BenchmarkExperiments
tify this using two common metrics: Frames Per Second
Fully-Supervised LiDAR Semantic Segmentation. We
(FPS) and Iteration Per Second (Iter/s). By incorporating
benchmark benchmarking on voxel-based, fusion-based,
these metrics into our evaluation framework, we ensure a
bird’s eye view, and range view segmentors using the
comprehensive assessment of model performance, encom-
validation sets of SemanticKITTI [4] and nuScenes [28].
passingbothaccuracyandefficiencyconsiderations.
The evaluated LiDAR segmentation models include
Evaluation Protocol. To uphold fair evaluation practices MinkUNet [20], SPVCNN [78], Cylinder3D [102], Po-
in our comparisons, we employ consistent evaluation pro- larNet [100], CENet [17], and FRNet [95], as detailed
tocols across different LiDAR segmentation models and in Tab. 2. We report performance under default data
data augmentation techniques. One crucial aspect of our augmentation, mixing data augmentation, and, where ap-
methodology is reporting evaluation metrics at the point plicable, test-time augmentation (TTA) for each model.
level rather than voxels or range images. This approach MinkUNet [20] achieves the highest mIoU of 71.8% on
ensures fair comparisons even when models use different SemanticKITTI[4],whileCylinder3D[102]attains80.9%
rasterizations,asitdirectlyassessessegmentationaccuracy mIoUonnuScenes[28]. OurresultsinTab.2illustratethat
atthefinestgranularityoftheinputdata. Furthermore, we our default training settings and advanced data augmenta-
conduct ablation studies to isolate the effects of individual tion techniques significantly enhance model performance.
componentsandaugmentations. Bysystematicallyvarying For instance, MinkUNet’s mIoU on SemanticKITTI im-
andanalyzingeachfactorindependently, wegainadeeper provedby5.0%withourdefaultsettings,andanadditional
understandingofitsimpactonmodelperformance. 3.5%withmixingdataaugmentation. TTAfurtherboosted
8performanceby1.4%,resultinginnearlya10%overallim- Table 4. Results of state-of-the-art LiDAR segmentation mod-
provement. These findings underscore the importance of els[17,20,78,95,100,102]underweakly-supervisedlearningse-
tupsonScribbleKITTI[85]. MixdenoteacombinationofLaser-
comprehensivetrainingstrategiesandrobustdataaugmen-
Mix [44] and PolarMix [89] or FrustumMix [95]. TTA denotes
tationinmaximizingmodeleffectiveness.
test time augmentation. All scores are given in percentage (%).
Inference Speed. Fig. 3 illustrates the inference speed Thebestscoresundereachmetricarehighlightedinbold.
of each model on the SemanticKITTI [4] dataset. All
FPS measurements are conducted on a single NVIDIA
ScribbleKITTI
GeForce RTX 3060 GPU with the data type float32. Model Mix TTA mIoU mAcc
For fusion- and voxel-based segmentors, SPVCNN [78]
Representation:Voxel
employs Torchsparse as its sparse convolution back-
end, Cylinder3D [102] uses SpConv, and all variants of ✗ ✗ 61.2 88.5
MinkUNet [20] utilize SpConv v2 as their sparse con- MinkUNet ✓ ✗ 62.6 89.9
volution backend, respectively. The results indicate that ✓ ✓ 65.7 90.5
range view methods exhibit faster inference speeds, with ✗ ✗ 58.8 87.4
Cylinder3D
CENet [17] achieving 33 FPS. While voxel-based meth- ✓ ✗ 59.8 88.8
odsgenerallyofferhigheraccuracy,theirlargermodelsizes
Representation:Bird’sEyeView
result in slower inference speeds. Among the MinkUNet
✗ ✗ 55.7 87.6
variants, MinkUNet-50-w32 achieves the highest accuracy PolarNet
✓ ✗ 57.2 88.4
but the slowest inference, whereas MinkUNet-18-w16 of-
fers faster inference with lower accuracy. MinkUNet-34- Representation:RangeView
w32 provides a balanced trade-off between performance ✗ ✗ 57.6 88.3
andspeed,makingitaversatilechoiceforvariousapplica- FRNet ✓ ✗ 60.2 88.5
tions.Webelievetheseinsightscouldbecomecriticalforre- ✓ ✓ 63.1 89.9
searchersandpractitionerswhoneedtooptimizeforeither
Representation:Fusion
speed or accuracy depending on the specific requirements
✗ ✗ 60.1 88.5
oftheirdeploymentscenarios,suchasreal-time,in-vehicle
SPVCNN ✓ ✗ 62.9 89.9
drivingsceneunderstandingandsegmentation.
✓ ✓ 65.7 90.4
Semi-SupervisedLiDARSemanticSegmentation.Under
the data-efficient learning setting, we leverage partially la-
beleddatasetstoevaluatehowmodelscaneffectivelylearn
notation effort by using line scribbles instead of dense la-
with limited supervision. We conduct this study on Se-
bels (which corresponds to around 8.06% of the original
manticKITTI [4], nuScenes [28], and ScribbleKITTI [85].
labels),providingapracticalsolutionforlarge-scaledataset
By using a combination of labeled and unlabeled data, we
creation [85]. Weakly-supervised learning leverages these
benchmark methods that employ techniques such as con-
sparse annotations to guide the model in learning robust
sistencyregularizationandpseudo-labelingtoimproveseg-
features. Theperformancemetrics,presentedinTab.4,re-
mentation accuracy, including MeanTeacher [80], Laser-
veal that state-of-the-art segmentors [20,78,95,100,102]
Mix[44],Lim3D[51],andFrustumMix[95]. Consistency
can attain competitive accuracy with a fraction of the an-
regularization ensures that predictions remain stable under
notation cost, highlighting their efficiency for large-scale
smallperturbationsoftheinput,whilepseudo-labelinggen-
applications. These results are particularly promising for
erates labels for the unlabeled data to iteratively refine the
applications where detailed annotations are impractical or
model. Preliminary results, shown in Tab. 3, indicate that
toocostly,suggestingthatweakannotationscanbeaviable
semi-supervisedlearningmethodscanachieveperformance
alternativefortraininghigh-performancemodels.
comparabletofullysupervisedcounterparts,demonstrating
their potential in reducing the need for extensive manual
4.3.AblationStudy
annotations. These methods not only lower the cost and
effortassociatedwithdatalabelingbutalsoenabletheuti- Sparse Convolution Backends. One of the key features
lizationoflargerdatasetsthatincludebothlabeledandunla- ofourMMDetection3D-lidarsegcodebaseisitssupportfor
beledsamples,enhancingtherobustnessandgeneralization five common sparse convolution backends: SpConv [97],
ofdifferentLiDARsegmentationapproaches. MinkowskiEngine[20],Torchsparse[77],SpConvv2[23],
Weakly-SupervisedLiDARSegmentation. Forthelabel- andTorchsparse++[79]. Wecomparetheefficacyofthese
efficient learning task, we utilize the ScribbleKITTI [85] sparse convolution backends using the MinkUNet-34-w32
dataset to evaluate LiDAR segmentors trained with weak backbone[20]onthevalidationsetsofSemanticKITTI[4]
annotations. This approach significantly reduces the an- and nuScenes [28]. Fig. 4 reports the training speed
9SemanticKITTI nuScenes
10 15 Minkowski Engine (FP32)
8.3 12.5
8 7.7 12 11.1
6.3 SpConv(FP16)
6 9
7.1
6.3
4
1.9
2.9
2.1 2.4
3.1
2.2
6 4.7 5.2 4.5 4.24.1 SpConv(FP32)
2 3
SpConvv2(FP16)
0 0
Training Speed (Iter/s) Training Speed (Iter/s)
SpConvv2(FP32)
40 35.8 50 46.7
42.3 TorchSparse(FP16)
32 28.7 40 37.1
24 22.3 30 24.1 25.3 27.8 23.1 TorchSparse(FP32)
16 11.2 11.510.3 14.7 15.7 12.2 20 19.8 21.3
TorchSparse++(FP16)
8 10
0 0 TorchSparse++(FP32)
Inference Speed (FPS) Inference Speed (FPS)
Figure4.Performancecomparisonsofdifferentsparseconvolutionbackends[20,23,77,79,97]onthevalidationsetsofSemanticKITTI[4]
andnuScenes[28]datasets. AllexperimentsareconductedusingtheMinkUNet-34-w32backbone[20]. Boththetrainingspeed(Iter/s,
iterationspersecond)andinferencespeed(FPS,framespersecond)aremeasuredusingasingleNVIDIAA100GPU.
(Iter/s, iterations per second) and inference speed (FPS, fersthefastestinferencespeedat32.4FPS.Thecommonly
frames per second) of different backends, with measure- used MinkUNet-34-w32 provides an optimal balance be-
mentstakenonasingleNVIDIAA100GPU.Experiments tween the LiDAR segmentation accuracy and speed, mak-
show that, for all sparse convolution backends, enabling ing it a preferred choice for various applications. This de-
Automatic Mixed Precision (AMP) reduces memory foot- tailedcomparisonallowsresearcherstomakeinformedde-
print and increases both training iteration speed and infer- cisionsaboutwhichmodelvarianttousebasedontheirspe-
ence speed, except for the Minkowski Engine [20], which cificneeds. MinkUNet-18-w16andMinkUNet-34-w32are
is not compatible with the float16 data type. In terms composed of BasicBlocks, while MinkUNet-50-w32 and
of training speed, SpConv v2 [23] with AMP turned on MinkUNet-101-w32arebuiltwithBottleneckBlocks,sim-
is the fastest, reaching 8.3 iterations per second on Se- ilar to ResNet architectures. This variety in model com-
manticKITTI and 12.5 iterations per second on nuScenes. plexityandperformancemetricsprovidesflexibilityforde-
For inference speed, Torchsparse++ [79] using float16 ployment in diverse scenarios, where either high accuracy
datatypeleads,achieving35.8FPSonSemanticKITTIand orfasterinferencespeedsmaybeprioritized.
46.7FPSonnuScenes. Theresultsalsoshowthatallsparse
Mixing-Based Data Augmentations. We evaluate the ef-
convolutional backends perform faster on nuScenes than
fectiveness of mixing-based data augmentation techniques
on SemanticKITTI due to the denser point clouds in Se-
onSemanticKITTI[4],withresultsdetailedinTab.6. Us-
manticKITTI, which require higher resolution during vox-
ingPolarMix[89]andLaserMix[44]individuallyenhances
elization. Thesefindingsprovidevaluableinsightsintothe theperformanceofMinkUNet[20]by2%mIoUand1.1%
trade-offsandbenefitsofeachbackend,guidingresearchers
mIoU, respectively. Combining PolarMix and LaserMix
inselectingtheappropriatebackendbasedontheirspecific with a random choice strategy yields a 3.5% improvement
requirementsfortrainingefficiency,memoryusage,andin-
over using them separately. Similar improvements are ob-
ferencespeed.
served with Cylinder3D [102]. For range view segmen-
LiDARSegmentationModelCapacities. Wecomparesix tors like FRNet [95], FrustumMix offers superior perfor-
variants of the MinkUNet [20] segmentor, evaluating their mancegainscomparedtoPolarMixorLaserMix,establish-
trainingmemoryfootprint,trainingspeed,accuracy,andin- ing it as the default setting for range view segmentation
ferencespeedonSemanticKITTI,asshowninTab.5. Us- benchmarks. These results underscore the importance of
inganNVIDIAA100GPUformeasurements,wefindthat advanceddataaugmentationtechniquesinenhancingmodel
MinkUNet-50-w32 with mixed precision training achieves robustnessandperformance,providingaclearpathforfur-
the highest mIoU of 67.6%, while MinkUNet-18-w16 of- ther improvements in LiDAR segmentation models. The
10Table5. Ablationstudyonmodelcapacities. Webenchmarksix Table6. Ablationstudyontheuseof3Ddataaugmentationtech-
variantsoftheMinkUNet[20]onSemanticKITTI[4].Param:the niques. WebenchmarkPolarMix[89],LaserMix[44],andFrus-
numberoftrainableparameters. AMP:whethertouseautomatic tumMix[95]withtheMinkUNet[20],Cylinder3D[102],andFR-
mixed precision during training. Mem: memory consumption. Net[95]backbones,respectively.Thebestscoreundereachback-
Train: thenumberoftraininginteractionspersecond. Infer: the boneconfigurationishighlightedinbold.
frame-per-secondrateduringinference. Thebestscoresinterms
oftrainingspeed(Iter/s),inferencespeed(FPS),andsegmentation PolarMix LaserMix FrustumMix mIoU
accuracy(mIoU)arehighlightedinbold. [89] [44] [95] (%)
Model:MinkUNet
Param Mem Train Infer mIoU
Amp
(M) (GB) (Iter/s) (FPS) (%) ✗ ✗ ✗ 66.9
✓ ✗ ✗ 68.9
Variant:MinkUNet-18-W16 ✗ ✓ ✗ 67.7
5.4 ✗ 1.9 7.1 30.7 64.1 ✓ ✓ ✗ 70.4
5.4 ✓ 1.1 10.0 32.4 64.8
Model:Cylinder3D
Variant:MinkUNet-18-W20 ✗ ✗ ✗ 63.7
8.8 ✗ 2.4 5.3 25.0 65.2 ✓ ✗ ✗ 64.5
8.8 ✓ 1.4 8.3 30.3 65.4 ✗ ✓ ✗ 65.6
✓ ✓ ✗ 67.0
Variant:MinkUNet-18-W32
21.7 ✗ 3.9 8.3 18.5 66.0 Model:FRNet
21.7 ✓ 2.3 10.0 31.8 65.8 ✗ ✗ ✗ 64.1
✓ ✗ ✗ 66.5
Variant:MinkUNet-34-W32
✗ ✓ ✗ 65.1
37.9 ✗ 4.7 2.4 14.7 66.9 ✗ ✗ ✓ 67.6
37.9 ✓ 2.8 8.3 28.7 66.6
Variant:MinkUNet-50-W32
ered based on the specific requirements of the application,
31.8 ✗ 11.5 2.8 11.0 65.5
balancing the need for improved accuracy with the con-
31.8 ✓ 6.0 7.7 13.7 67.6
straintsofcomputationalresourcesandinferencespeed.
Variant:MinkUNet-101-W32
70.8 ✗ 14.2 1.9 8.0 66.9 5.Discussions&FutureDirections
70.8 ✓ 8.8 5.6 9.9 67.2
The comprehensive evaluation of LiDAR segmentation
models using the MMDetection3D-lidarseg toolbox has
providedseveralkeyinsightsintotheperformanceandutil-
combinationofmultipledataaugmentationstrategiesmax-
ity of various models and techniques in real-world appli-
imizes the exposure of the model to diverse scenarios, im-
cations. Our experiments have highlighted the strengths
proving its ability to generalize and perform well in real-
and weaknesses of different segmentation strategies, data
worlddensesceneunderstandingapplications.
augmentationtechniques,andsparseconvolutionbackends,
Test Time Augmentation (TTA). Tab. 7 presents the im-
contributingvaluableknowledgetothefieldofLiDARseg-
pactofvariousTTAstrategiesonmodelperformance.Start-
mentation. Thekeytakeawaysaresummarizedasfollows:
ing with flipping along different axes, TTA improves the
performanceofMinkUNet[20]by1.6%mIoU.Addingro- • Ourcodebaseunifiedframeworkstreamlinesdevelop-
tation, scaling, and translation further boosts MinkUNet’s ment and benchmarking by supporting diverse mod-
performanceto71.8%mIoUonSemanticKITTI[4]. While elsandbackends. Thisreducesfragmentationandsets
TTA also benefits Cylinder3D [102] and FRNet [95], ex- newstandardsformodelevaluation.
tensive use of TTA significantly increases inference time.
• Advanced 3D data augmentation techniques, such as
For example, comprehensive TTA techniques result in a
LaserMix, PolarMix, and FrustumMix significantly
108-fold increase in inference time compared to no TTA,
enhance model robustness and generalization. Com-
underscoring the trade-off between performance gains and
biningthesestrategiesyieldsnotableperformanceim-
computational cost. These findings highlight the potential
provementsandhelpsmodelsadapttodiverseenviron-
ofTTAinimprovingmodelaccuracyincontrolledenviron-
mentalconditions.
mentswhilealsoillustratingthepracticallimitationsinreal-
timeapplications,suchasin-vehicleLiDARsegmentation. • Thecomparisonofsparseconvolutionbackendsshows
The choice of TTA techniques should be carefully consid- that SpConv v2 and Torchsparse++ offer superior
11Table7. Ablationstudyontheuseofthetesttimeaugmentation learningwillreducetheneedforextensivemanualannota-
(TTA)duringthemodelinferencestage. Webenchmarkdifferent tions,facilitatinglarge-scaledatasetcreation.
combinationswiththeMinkUNet[20],Cylinder3D[102],andFR-
Beyond autonomous driving, the principles and tech-
Net[95]backbones,respectively,onSemanticKITTI[4].Thebest
niques from MMDetection3D-lidarseg can be applied to
scoreundereachbackboneconfigurationishighlightedinbold.
robotics, augmented reality, and urban planning. Future
research will explore these applications, adapting and op-
mIoU
Flip Rotate Scale Translate Time timizing the toolbox for various use cases. By making the
(%)
codebaseandtrainedmodelspubliclyavailable, weaimto
Model:MinkUNet foster a collaborative research environment. Contributions
✗ ✗ ✗ ✗ 70.4 1× from the broader 3D computer vision research community
✓ ✗ ✗ ✗ 71.2 4× will drive innovation and accelerate the development of
✓ ✓ ✗ ✗ 71.4 12× morereliableLiDARsegmentationmodels.
✓ ✓ ✓ ✗ 71.7 36×
✓ ✓ ✓ ✓ 71.8 108× 6.BroaderImpact
Model:Cylinder3D
Positive Societal Impacts. MMDetection3D-lidarseg can
✗ ✗ ✗ ✗ 67.0 1×
significantlyadvanceautonomousdrivingandrelatedfields
✓ ✗ ✗ ✗ 65.3 4×
byimprovingLiDARsegmentation. Thisenhancementcan
✓ ✓ ✗ ✗ 65.4 12×
leadtosaferandmoreefficientautonomoussystems,reduc-
✓ ✓ ✓ ✗ 69.0 36×
ing traffic accidents and fatalities. Beyond transportation,
✓ ✓ ✓ ✓ 69.4 108×
applicationsinrobotics,augmentedreality,urbanplanning,
Model:FRNet
and environmental monitoring can benefit from improved
✗ ✗ ✗ ✗ 67.6 1× 3Denvironmentunderstanding,leadingtoadvancementsin
✓ ✗ ✗ ✗ 68.1 4× automation,immersiveexperiences,infrastructuredevelop-
✓ ✓ ✗ ✗ 68.1 12× ment,andaccurateenvironmentalmapping.
✓ ✓ ✓ ✗ 68.4 36× Negative Societal Impacts. The potential negative im-
✓ ✓ ✓ ✓ 69.0 108×
pacts include the misuse of enhanced LiDAR technology
for surveillance, which might raise privacy concerns. Ad-
ditionally, increased automation could reduce the demand
training and inference speeds with AMP. This is cru- forhumanlaborinindustriesliketransportationandmanu-
cial for deploying LiDAR segmentation models in facturing,potentiallyleadingtojobdisplacement. Tomiti-
resource-constrainedandreal-timeenvironments. gatetheserisks,itiscrucialtoadheretoethicalguidelines,
ensuring privacy-preserving techniques are integrated into
• Evaluations of MinkUNet variants reveal that larger technologydevelopment.
models like MinkUNet-50-w32 achieve higher ac-
curacy but at increased computational costs, while 7.Conclusion
smallermodelsofferfasterinferenceattheexpenseof
accuracy. MinkUNet-34-w32 strikes a good balance In this work, we introduced MMDetection3D-lidarseg,
betweenaccuracyandefficiency. a comprehensive toolbox designed to facilitate the train-
ing and evaluation of state-of-the-art LiDAR segmenta-
• Test Time Augmentation (TTA) strategies improve tion models. By supporting a wide range of segmen-
modelperformancebutsignificantlyincreaseinference tation models and integrating advanced data augmenta-
time, highlightingatrade-offcrucialforreal-timeap- tiontechniques, ourcodebaseenhancestherobustnessand
plicationslikeautonomousdriving. generalization capabilities of LiDAR segmentation mod-
els. Furthermore, the toolbox provides support for multi-
FutureworkwillexpandMMDetection3D-lidarsegtoin- ple leading sparse convolution backends, optimizing com-
clude more state-of-the-art LiDAR segmentation models, putationalefficiencyandperformanceacrossvarioushard-
providingabroaderrangeoftoolsforresearchers. Further ware configurations. Our extensive benchmark experi-
research into novel data augmentation techniques will aim ments on widely-used datasets, including SemanticKITTI,
to enhance model robustness and generalization by simu- nuScenes, and ScribbleKITTI, demonstrate the effective-
lating a wider variety of real-world scenarios. Optimiza- ness of MMDetection3D-lidarseg in diverse autonomous
tion of current backends will continue, aiming for greater drivingscenarios.Theresultshighlightthetoolbox’sability
efficiencyandperformance. Enhancingthetoolbox’scapa- to streamline development, improve benchmarking consis-
bilities to support semi-supervised and weakly-supervised tency,andsetnewstandardsforresearchandapplicationin
12thefieldofLiDARsegmentation. Bymakingthecodebase • PolarSeg15 ..................BSD3-ClauseLicense
and trained models publicly available, MMDetection3D-
• MinkowskiEngine16 ..................MITLicense
lidarsegpromotesfurtherresearch,collaboration,andinno-
vation within the community. We believe that this unified
• TorchSparse17 ....................... MITLicense
framework will accelerate advancements in autonomous
driving technologies, ultimately contributing to the devel- • SPVNAS18 ..........................MITLicense
opment of safer and more reliable autonomous systems.
Future work will focus on expanding the capabilities of • Cylinder3D19 ..................ApacheLicense2.0
MMDetection3D-lidarseg, including the incorporation of
• SpConv20 .....................ApacheLicense2.0
additional segmentation models, further optimization of
sparse convolution backends, and the integration of more • LaserMix21 ....................CCBY-NC-SA4.0
advanced data augmentation techniques. We also plan to
explore the application of this toolbox in other domains • PolarMix22 ..........................MITLicense
that can benefit from precise 3D environment understand-
ing,suchasroboticsandaugmentedreality. References
[1] AngelikaAndo,SpyrosGidaris,AndreiBursuc,GillesPuy,
Acknowledgements
AlexandreBoulch,andRenaudMarlet. Rangevit:Towards
We acknowledge the use of the following public re- vision transformers for 3d semantic segmentation in au-
tonomousdriving. InIEEE/CVFConferenceonComputer
sources,duringthecourseofthiswork:
VisionandPatternRecognition,pages5240–5250,2023.
• MMCV2 ......................ApacheLicense2.0 [2] MehmetAygun,AljosaOsep,MarkWeber,MaximMaxi-
mov,CyrillStachniss,JensBehley,andLauraLeal-Taixe´.
• MMDetection3 .................ApacheLicense2.0 4dpanopticlidarsegmentation. InIEEE/CVFConference
onComputerVisionandPatternRecognition,pages5527–
• MMDetection3D4 ..............ApacheLicense2.0 5537,2021.
[3] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-
• MMEngine5 ...................ApacheLicense2.0 zel, Sven Behnke, Ju¨rgen Gall, and Cyrill Stachniss. To-
wards 3d lidar-based semantic scene understanding of 3d
• OpenPCSeg6 ..................ApacheLicense2.0 pointcloudsequences: Thesemantickittidataset. Interna-
tionalJournalofRoboticsResearch,40:959–96,2021.
• nuScenes7 .....................CCBY-NC-SA4.0
[4] Jens Behley, Martin Garbade, Andres Milioto, Jan Quen-
zel,SvenBehnke,CyrillStachniss,andJuergenGall. Se-
• nuScenes-devkit8 ..............ApacheLicense2.0
mantickitti: Adatasetforsemanticsceneunderstandingof
• SemanticKITTI9 ............... CCBY-NC-SA4.0 lidarsequences.InIEEE/CVFInternationalConferenceon
ComputerVision,pages9297–9307,2019.
• SemanticKITTI-API10 ................MITLicense [5] Jens Behley, Andres Milioto, and Cyrill Stachniss. A
benchmarkforlidar-basedpanopticsegmentationbasedon
• ScribbleKITTI11 ........................Unknown kitti. In IEEE International Conference on Robotics and
Automation,pages13596–13603,2021.
• lidar-bonnetal12 ......................MITLicense [6] AlexandreBoulch,CorentinSautier,Bjo¨rnMichele,Gilles
Puy, and Renaud Marlet. Also: Automotive lidar self-
• CENet13 .............................MITLicense
supervisionbyoccupancyestimation. InIEEE/CVFCon-
ferenceonComputerVisionandPatternRecognition,pages
• FRNet14 .......................ApacheLicense2.0
13455–13465,2023.
2https://github.com/open-mmlab/mmcv. [7] Christopher Lang Alexander Braun, Lars Schillingmann,
3https://github.com/open-mmlab/mmdetection. and Abhinav Valada. A point-based approach to ef-
4https://github.com/open-mmlab/mmdetection3d. ficient lidar multi-task perception. arXiv preprint
5https://github.com/open-mmlab/mmengine. arXiv:2404.12798,2024.
6https://github.com/PJLab-ADG/OpenPCSeg.
7https://www.nuscenes.org/nuscenes. 15https://github.com/edwardzhou130/PolarSeg.
8https://github.com/nutonomy/nuscenes-devkit. 16https://github.com/NVIDIA/MinkowskiEngine.
9http://semantic-kitti.org. 17https://github.com/mit-han-lab/torchsparse.
10https://github.com/PRBonn/semantic-kitti-api. 18https://github.com/mit-han-lab/spvnas.
11https://github.com/ouenal/scribblekitti. 19https://github.com/xinge008/Cylinder3D.
12https://github.com/PRBonn/lidar-bonnetal. 20https://github.com/traveller59/spconv.
13https://github.com/huixiancheng/CENet. 21https://github.com/ldkong1205/LaserMix.
14https://github.com/Xiangxu-0103/FRNet. 22https://github.com/xiaoaoran/polarmix.
13[8] HolgerCaesar,VarunBankiti,AlexHLang,SourabhVora, neural networks. In IEEE/CVF Conference on Computer
Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, VisionandPatternRecognition,pages3075–3084,2019.
GiancarloBaldan,andOscarBeijbom. nuscenes: Amulti- [21] MMDetection3D Contributors. MMDetection3D: Open-
modaldatasetforautonomousdriving. InIEEE/CVFCon- MMLab next-generation platform for general 3D object
ferenceonComputerVisionandPatternRecognition,pages detection. https://github.com/open-mmlab/
11621–11631,2020. mmdetection3d,2020.
[9] Anh-Quan Cao and Raoul De Charette. Monoscene: [22] Pointcept Contributors. Pointcept: A codebase for point
Monocular 3d semantic scene completion. In IEEE/CVF cloud perception research. https://github.com/
Conference on Computer Vision and Pattern Recognition, Pointcept/Pointcept,2023.
pages3991–4001,2022. [23] Spconv Contributors. Spconv: Spatially sparse convolu-
[10] Anh-QuanCao,AngelaDai,andRaouldeCharette.Pasco: tionlibrary. https://github.com/traveller59/
Urban 3d panoptic scene completion with uncertainty spconv,2022.
awareness. InIEEE/CVFConferenceonComputerVision [24] Tiago Cortinhal, George Tzelepis, and Eren Erdal Aksoy.
andPatternRecognition,2024. Salsanext: Fast, uncertainty-aware semantic segmentation
[11] Jun Cen, Peng Yun, Shiwei Zhang, Junhao Cai, Di Luan, oflidarpointclouds.InInternationalSymposiumonVisual
MingqianTang,MingLiu,andMichaelYuWang. Open- Computing,pages207–222,2020.
worldsemanticsegmentationforlidarpointclouds. InEu- [25] Bertrand Douillard, James Underwood, Noah Kuntz,
ropean Conference on Computer Vision, pages 318–334, Vsevolod Vlaskine, Alastair Quadros, Peter Morton, and
2022. AlonFrenkel.Onthesegmentationof3dlidarpointclouds.
InIEEEInternationalConferenceonRoboticsandAutoma-
[12] QiChen,SourabhVora,andOscarBeijbom. Polarstream:
tion,pages2798–2805,2011.
Streaminglidarobjectdetectionandsegmentationwithpo-
larpillars. InAdvancesinNeuralInformationProcessing [26] MartinEster,Hans-PeterKriegel,Jo¨rgSander,andXiaowei
Systems,volume34,2021. Xu. Adensity-basedalgorithmfordiscoveringclustersin
largespatialdatabaseswithnoise. InACMSIGKDDCon-
[13] Runnan Chen, Youquan Liu, Lingdong Kong, Nenglun
ferenceonKnowledgeDiscoveryandDataMining, pages
Chen, Xinge Zhu, Yuexin Ma, Tongliang Liu, and Wen-
226–231,1996.
pingWang. Towardslabel-freesceneunderstandingbyvi-
[27] MartinA.FischlerandRobertC.Bolles. Randomsample
sionfoundationmodels.InAdvancesinNeuralInformation
consensus: Aparadigmformodelfittingwithapplications
ProcessingSystems,volume36,2023.
toimageanalysisandautomatedcartography.Communica-
[14] RunnanChen, YouquanLiu, LingdongKong, XingeZhu,
tionsoftheACM,24(6):381–395,1981.
YuexinMa,YikangLi,YuenanHou,YuQiao,andWenping
[28] WhyeKitFong,RohitMohan,JuanaValeriaHurtado,Lub-
Wang. Clip2scene:Towardslabel-efficient3dsceneunder-
ingZhou,HolgerCaesar,OscarBeijbom,andAbhinavVal-
standing by clip. In IEEE/CVF Conference on Computer
ada. Panopticnuscenes: Alarge-scalebenchmarkforlidar
VisionandPatternRecognition,pages7020–7030,2023.
panoptic segmentation and tracking. IEEE Robotics and
[15] XieyuanliChen, ShijieLi, BenediktMersch, LouisWies-
AutomationLetters,7(2):3795–3802,2022.
mann,Ju¨rgenGall,JensBehley,andCyrillStachniss.Mov-
[29] Geoff French, Timo Aila, Samuli Laine, Michal Mack-
ingobjectsegmentationin3dlidardata: Alearning-based
iewicz, and Graham Finlayson. Semi-supervised seman-
approach exploiting sequential data. IEEE Robotics and
ticsegmentationneedsstrong, high-dimensionalperturba-
AutomationLetters,6(4):6529–6536,2021.
tions. InBritishMachineVisionConference,2020.
[16] Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong
[30] Biao Gao, Yancheng Pan, Chengkun Li, Sibo Geng, and
Wang. Semi-supervisedsemanticsegmentationwithcross
Huijing Zhao. Are we hungry for 3d lidar data for se-
pseudosupervision.InIEEE/CVFConferenceonComputer
mantic segmentation? a survey of datasets and methods.
VisionandPatternRecognition,pages2613–2622,2021.
IEEE Transactions on Intelligent Transportation Systems,
[17] HuixianCheng,XianfengHan,andGuoqiangXiao.Cenet: 23(7):6063–6081,2021.
Toward concise and efficient lidar semantic segmentation
[31] Stefano Gasperini, Mohammad-Ali Nikouei Mahani, Al-
forautonomousdriving. InIEEEInternationalConference
varoMarcos-Ramiro,NassirNavab,andFedericoTombari.
onMultimediaandExpo,pages1–6,2022.
Panoster: End-to-endpanopticsegmentationoflidarpoint
[18] HuixianCheng,XianfengHan,andGuoqiangXiao. Tran- clouds.IEEERoboticsandAutomationLetters,6(2):3216–
srvnet: Lidar semantic segmentation with transformer. 3223,2021.
IEEE Transactions on Intelligent Transportation Systems, [32] AndreasGeiger,PhilipLenz,andRaquelUrtasun. Arewe
24(6):5895–5907,2023. readyforautonomousdriving? thekittivisionbenchmark
[19] Ran Cheng, Ryan Razani, Ehsan Taghavi, Enxu Li, and suite. In IEEE/CVF Conference on Computer Vision and
Bingbing Liu. Af2-s3net: Attentive feature fusion with PatternRecognition,pages3354–3361,2012.
adaptivefeatureselectionforsparsesemanticsegmentation [33] Martin Hahner, Christos Sakaridis, Mario Bijelic, Felix
network.InIEEE/CVFConferenceonComputerVisionand Heide, Fisher Yu, Dengxin Dai, and Luc Van Gool. Li-
PatternRecognition,pages12547–12556,2021. darsnowfallsimulationforrobust3dobjectdetection. In
[20] Christopher Choy, JunYoung Gwak, and Silvio Savarese. IEEE/CVF Conference on Computer Vision and Pattern
4d spatio-temporal convnets: Minkowski convolutional Recognition,pages16364–16374,2022.
14[34] Martin Hahner, Christos Sakaridis, Dengxin Dai, and tions. InAdvancesinNeuralInformationProcessingSys-
LucVanGool.Fogsimulationonreallidarpointcloudsfor tems,volume36,2023.
3d object detection in adverse weather. In IEEE/CVF In- [46] LingdongKong,XiangXu,JunCen,WenweiZhang,Liang
ternationalConferenceonComputerVision,pages15283– Pan,KaiChen,andZiweiLiu. Calib3d:Calibratingmodel
15292,2021. preferences for reliable 3d scene understanding. arXiv
[35] Fangzhou Hong, Lingdong Kong, Hui Zhou, Xinge Zhu, preprintarXiv:2403.17010,2024.
HongshengLi, andZiweiLiu. Unified3dand4dpanop- [47] Lingdong Kong, Xiang Xu, Jiawei Ren, Wenwei Zhang,
tic segmentation via dynamic shifting networks. IEEE Liang Pan, Kai Chen, Wei Tsang Ooi, and Ziwei Liu.
TransactionsonPatternAnalysisandMachineIntelligence, Multi-modaldata-efficient3dsceneunderstandingforau-
46(5):3480–3495,2024. tonomousdriving. arXivpreprintarXiv:2405.05258,2024.
[36] Yuenan Hou, Xinge Zhu, Yuexin Ma, Chen Change Loy, [48] LoicLandrieu,HugoRaguet,BrunoVallet,Cle´mentMal-
and Yikang Li. Point-to-voxel knowledge distillation for let, and Martin Weinmann. A structured regularization
lidarsemanticsegmentation. InIEEEConferenceonCom- framework for spatially smoothing semantic labelings of
puter Vision and Pattern Recognition, pages 8479–8488, 3d point clouds. ISPRS Journal of Photogrammetry and
2022. RemoteSensing,132:102–118,2017.
[37] Qingyong Hu, Bo Yang, Sheikh Khalid, Wen Xiao, Niki [49] Jiale Li, Hang Dai, and Yong Ding. Self-distillation for
Trigoni, and Andrew Markham. Towards semantic seg- robustlidarsemanticsegmentationinautonomousdriving.
mentationofurban-scale3dpointclouds:Adataset,bench- InEuropeanConferenceonComputerVision, pages659–
marksandchallenges. InIEEE/CVFConferenceonCom- 676,2022.
puter Vision and Pattern Recognition, pages 4977–4987, [50] Jiale Li, Hang Dai, Hao Han, and Yong Ding. Mseg3d:
2021. Multi-modal 3d semantic segmentation for autonomous
[38] QingyongHu, BoYang, LinhaiXie, StefanoRosa, Yulan driving. InIEEE/CVFConferenceonComputerVisionand
Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham. PatternRecognition,pages21694–21704,2023.
Randla-net: Efficientsemanticsegmentationoflarge-scale [51] Li Li, Hubert P. H. Shum, and Toby P. Breckon. Less is
pointclouds.InIEEE/CVFConferenceonComputerVision more: Reducing task and model complexity for 3d point
andPatternRecognition,pages11108–11117,2020. cloud semantic segmentation. In IEEE/CVF Conference
[39] MaximilianJaritz,Tuan-HungVu,RaouldeCharette,Em- onComputerVisionandPatternRecognition,pages9361–
ilieWirbel,andPatrickPe´rez. xmuda: Cross-modalunsu- 9371,2023.
perviseddomainadaptationfor3dsemanticsegmentation. [52] Rong Li, Raoul de Charette, and Anh-Quan Cao.
InIEEE/CVFConferenceonComputerVisionandPattern Coarse3d: Class-prototypes for contrastive learning in
Recognition,pages12605–12614,2020. weakly-supervised3dpointcloudsegmentation. InBritish
[40] Alok Jhaldiyal and Navendu Chaudhary. Semantic seg- MachineVisionConference,2022.
mentation of 3d lidar data using deep learning: a re- [53] Rong Li, Shijie Li, Xieyuanli Chen, Teli Ma, Wang Hao,
view of projection-based methods. Applied Intelligence, JuergenGall,andJunweiLiang. Tfnet: Exploitingtempo-
53(6):6844–6855,2023. ralcuesforfastandaccuratelidarsemanticsegmentation.
[41] LingdongKong,YouquanLiu,RunnanChen,YuexinMa, arXivpreprintarXiv:2309.07849,2023.
Xinge Zhu, Yikang Li, Yuenan Hou, Yu Qiao, and Zi- [54] Ye Li, Lingdong Kong, Hanjiang Hu, Xiaohao Xu, and
wei Liu. Rethinking range view representation for lidar Xiaonan Huang. Optimizing lidar placements for robust
segmentation. In IEEE/CVF International Conference on driving perception in adverse conditions. arXiv preprint
ComputerVision,pages228–240,2023. arXiv:2403.17009,2024.
[42] LingdongKong,YouquanLiu,XinLi,RunnanChen,Wen- [55] Guibiao Liao, Jiankun Li, and Xiaoqing Ye. Vlm2scene:
wei Zhang, Jiawei Ren, Liang Pan, Kai Chen, and Ziwei Self-supervised image-text-lidar learning with foundation
Liu. Robo3d: Towards robust and reliable 3d perception models for autonomous driving scene understanding. In
against corruptions. In IEEE/CVF International Confer- AAAI Conference on Artificial Intelligence, pages 3351–
enceonComputerVision,pages19994–20006,2023. 3359,2024.
[43] Lingdong Kong, Niamul Quader, and Venice Erin Liong. [56] Venice Erin Liong, Thi Ngoc Tho Nguyen, Sergi Wid-
Conda: Unsuperviseddomainadaptationforlidarsegmen- jaja,DhananjaiSharma,andZhuangJieChong. Amvnet:
tation via regularized domain concatenation. In IEEE In- Assertion-basedmulti-viewfusionnetworkforlidarseman-
ternationalConferenceonRoboticsandAutomation,pages ticsegmentation. arXivpreprintarXiv:2012.04934,2020.
9338–9345,2023. [57] MinghuaLiu,YinZhou,CharlesR.Qi,BoqingGong,Hao
[44] Lingdong Kong, Jiawei Ren, Liang Pan, and Ziwei Liu. Su,andDragomirAnguelov. Less: Label-efficientseman-
Lasermixforsemi-supervisedlidarsemanticsegmentation. ticsegmentationforlidarpointclouds. InEuropeanCon-
InIEEE/CVFConferenceonComputerVisionandPattern ferenceonComputerVision,pages70–89,2022.
Recognition,pages21705–21715,2023. [58] Youquan Liu, Yeqi Bai, Lingdong Kong, Runnan Chen,
[45] Lingdong Kong, Shaoyuan Xie, Hanjiang Hu, Lai Xing YuenanHou, BotianShi, andLiYikang. Openpcseg: An
Ng,BenoitR.Cottereau,andWeiTsangOoi. Robodepth: opensourcepointcloudsegmentationcodebase. https:
Robust out-of-distribution depth estimation under corrup- //github.com/PJLab-ADG/PCSeg,2023.
15[59] Youquan Liu, Runnan Chen, Xin Li, Lingdong Kong, [72] Giulia Rizzoli, Francesco Barbato, and Pietro Zanuttigh.
YuchenYang,ZhaoyangXia,YeqiBai,XingeZhu,Yuexin Multimodalsemanticsegmentationinautonomousdriving:
Ma,YikangLi,YuQiao,andYuenanHou. Uniseg: Auni- A review of current approaches and future perspectives.
fiedmulti-modallidarsegmentationnetworkandtheopen- Technologies,10(4):90,2022.
pcsegcodebase.InIEEE/CVFInternationalConferenceon [73] Corentin Sautier, Gilles Puy, Alexandre Boulch, Renaud
ComputerVision,pages21662–21673,2023. Marlet,andVincentLepetit. Bevcontrast:Self-supervision
[60] Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, inbevspaceforautomotivelidarpointclouds. InInterna-
WenweiZhang,LiangPan,KaiChen,andZiweiLiu. Seg- tionalConferenceon3DVision,2024.
mentanypointcloudsequencesbydistillingvisionfounda- [74] KshitijSirohi,RohitMohan,DanielBu¨scher,WolframBur-
tionmodels.InAdvancesinNeuralInformationProcessing gard, and Abhinav Valada. Efficientlps: Efficient lidar
Systems,volume36,2023. panoptic segmentation. IEEE Transactions on Robotics,
[61] Youquan Liu, Lingdong Kong, Xiaoyang Wu, Runnan 38(3):1894–1914,2021.
Chen,XinLi,LiangPan,ZiweiLiu,andYuexinMa.Multi- [75] Leslie N Smith and Nicholay Topin. Super-convergence:
spacealignmentstowardsuniversallidarsegmentation. In Very fast training of neural networks using large learn-
IEEE/CVF Conference on Computer Vision and Pattern ingrates. InArtificialIntelligenceandMachineLearning
Recognition,2024. forMulti-DomainOperationsApplications,volume11006,
[62] RomainLoiseau,MathieuAubry,andLo¨ıcLandrieu. On- pages369–386,2019.
line segmentation of lidar sequences: Dataset and algo- [76] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aure-
rithm. InEuropeanConferenceonComputerVision,pages lienChouard,VijaysaiPatnaik,PaulTsui,JamesGuo,Yin
301–317,2022. Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan,
Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev,
[63] Ilya Loshchilov and Frank Hutter. Decoupled weight de-
ScottEttinger, MaximKrivokon, AmyGao, AdityaJoshi,
cayregularization. InInternationalConferenceonLearn-
YuZhang, JonathonShlens, ZhifengChen, andDragomir
ingRepresentations,2018.
Anguelov. Scalabilityinperceptionforautonomousdriv-
[64] Rodrigo Marcuzzi, Lucas Nunes, Louis Wiesmann, Jens
ing: Waymo open dataset. In IEEE/CVF Conference on
Behley, and Cyrill Stachniss. Mask-based panoptic lidar
Computer Vision and Pattern Recognition, pages 2446–
segmentationforautonomousdriving. IEEERoboticsand
2454,2020.
AutomationLetters,8(2):1141–1148,2023.
[77] HaotianTang,ZhijianLiu,XiuyuLi,YujunLin,andSong
[65] QiangMeng,XiaoWang,JiaBaoWang,LiujiangYan,and
Han. Torchsparse: Efficientpointcloudinferenceengine.
Ke Wang. Small, versatile and mighty: A range-view
ProceedingsofMachineLearningandSystems,4:302–315,
perception framework. arXiv preprint arXiv:2403.00325,
2022.
2024.
[78] Haotian Tang, Zhijian Liu, Shengyu Zhao, Yujun Lin, Ji
[66] BjoernMichele,AlexandreBoulch,GillesPuy,Tuan-Hung
Lin, HanruiWang, andSongHan. Searchingefficient3d
Vu,RenaudMarlet,andNicolasCourty. Saluda: Surface-
architectures with sparse point-voxel convolution. In Eu-
basedautomotivelidarunsuperviseddomainadaptation. In
ropean Conference on Computer Vision, pages 685–702,
InternationalConferenceon3DVision,2024.
2020.
[67] Andres Milioto, Ignacio Vizzo, Jens Behley, and Cyrill
[79] HaotianTang,ShangYang,ZhijianLiu,KeHong,Zhong-
Stachniss. Rangenet++: Fast and accurate lidar semantic
mingYu,XiuyuLi,GuohaoDai,YuWang,andSongHan.
segmentation. In IEEE/RSJ International Conference on
Torchsparse++:Efficientpointcloudengine. InIEEE/CVF
IntelligentRobotsandSystems,pages4213–4220,2019.
Conference on Computer Vision and Pattern Recognition
[68] Aljosˇa Osˇep, Tim Meinhardt, Francesco Ferroni, Neehar Workshops,pages202–209,2023.
Peri, Deva Ramanan, and Laura Leal-Taixe´. Better call [80] AnttiTarvainenandHarriValpola.Meanteachersarebetter
sal: Towardslearningtosegmentanythinginlidar. arXiv rolemodels: Weight-averagedconsistencytargetsimprove
preprintarXiv:2403.13129,2024. semi-superviseddeeplearningresults.InAdvancesinNeu-
[69] AldiPiroli,VinzenzDallabetta,JohannesKopp,MarcWa- ralInformationProcessingSystems,volume30,2017.
lessa, Daniel Meissner, and Klaus Dietmayer. Label- [81] HuguesThomas,CharlesRQi,Jean-EmmanuelDeschaud,
efficientsemanticsegmentationoflidarpointcloudsinad- Beatriz Marcotegui, Franc¸ois Goulette, and Leonidas J
verseweatherconditions. IEEERoboticsandAutomation Guibas. Kpconv: Flexibleanddeformableconvolutionfor
Letters,9(6):5575–5582,2024. point clouds. In IEEE/CVF International Conference on
[70] GillesPuy,AlexandreBoulch,andRenaudMarlet.Usinga ComputerVision,pages6411–6420,2019.
waffleironforautomotivepointcloudsemanticsegmenta- [82] LarissaT.Triess,MariellaDreissig,ChristophB.Rist,and
tion. InIEEE/CVFInternationalConferenceonComputer J.MariusZo¨llner. Asurveyondeepdomainadaptationfor
Vision,pages3379–3389,2023. lidar perception. In IEEE Intelligent Vehicles Symposium
[71] Gilles Puy, Spyros Gidaris, Alexandre Boulch, Oriane Workshops,pages350–357,2021.
Sime´oni, Corentin Sautier, Patrick Pe´rez, Andrei Bursuc, [83] Larissa T. Triess, David Peter, Christoph B. Rist, and
andRenaudMarlet.Threepillarsimprovingvisionfounda- J. Marius Zo¨llner. Scan-based semantic segmentation of
tionmodeldistillationforlidar. InIEEE/CVFConference lidarpointclouds: Anexperimentalstudy. InIEEEIntelli-
onComputerVisionandPatternRecognition,2024. gentVehiclesSymposium,pages1116–1121,2020.
16[84] MarcUecker,TobiasFleck,MarcelPflugfelder,andJ.Mar- [97] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely
ius Zo¨llner. Analyzing deep learning representations of embeddedconvolutionaldetection. Sensors, 18(10):3337,
pointcloudsforreal-timein-vehiclelidarperception.arXiv 2018.
preprintarXiv:2210.14612,2022. [98] KaichengYu,TangTao,HongweiXie,ZhiweiLin,Zhong-
[85] Ozan Unal, Dengxin Dai, and Luc Van Gool. Scribble- wei Wu, Zhongyu Xia, Tingting Liang, Haiyang Sun,
supervisedlidarsemanticsegmentation.InIEEE/CVFCon- JiongDeng,DayangHao,YongtaoWang,XiaodanLiang,
ferenceonComputerVisionandPatternRecognition,pages and Bing Wang. Benchmarking the robustness of lidar-
2697–2707,2022. camera fusion for 3d object detection. arXiv preprint
arXiv:2205.14951,2022.
[86] Nina Varney, Vijayan K. Asari, and Quinn Graehling.
Dales: Alarge-scaleaeriallidardatasetforsemanticseg- [99] DimitrisZermas,IzzatIzzat,andNikolaosPapanikolopou-
mentation. InIEEE/CVFConferenceonComputerVision los. Fastsegmentationof3dpointclouds: Aparadigmon
andPatternRecognitionWorkshops,pages186–187,2020. lidardataforautonomousvehicleapplications.InIEEEIn-
ternationalConferenceonRoboticsandAutomation,pages
[87] YuanWang,TianyueShi,PengYun,LeiTai,andMingLiu.
5067–5073,2017.
Pointseg: Real-timesemanticsegmentationbasedon3dli-
[100] YangZhang,ZixiangZhou,PhilipDavid,XiangyuYue,Ze-
darpointcloud. arXivpreprintarXiv:1807.06288,2018.
rongXi,BoqingGong,andHassanForoosh. Polarnet: An
[88] Martin Weinmann, Boris Jutzi, Stefan Hinz, and Cle´ment
improved grid representation for online lidar point clouds
Mallet. Semantic point cloud interpretation based on op-
semanticsegmentation. InIEEE/CVFConferenceonCom-
timalneighborhoods,relevantfeaturesandefficientclassi-
puter Vision and Pattern Recognition, pages 9601–9610,
fiers. ISPRSJournalofPhotogrammetryandRemoteSens-
2020.
ing,105:286–304,2015.
[101] YimingZhao,LinBai,andXinmingHuang. Fidnet: Lidar
[89] AoranXiao,JiaxingHuang,DayanGuan,KaiwenCui,Shi-
pointcloudsemanticsegmentationwithfullyinterpolation
jian Lu, and Ling Shao. Polarmix: A general data aug-
decoding. InIEEE/RSJInternationalConferenceonIntel-
mentationtechniqueforlidarpointclouds. InAdvancesin
ligentRobotsandSystems,pages4453–4458,2021.
NeuralInformationProcessingSystems,volume35,pages
[102] XingeZhu,HuiZhou,TaiWang,FangzhouHong,Yuexin
11035–11048,2022.
Ma,WeiLi,HongshengLi,andDahuaLin.Cylindricaland
[90] AoranXiao,JiaxingHuang,DayanGuan,FangnengZhan, asymmetrical3dconvolutionnetworksforlidarsegmenta-
and Shijian Lu. Transfer learning from synthetic to real tion. In IEEE/CVF Conference on Computer Vision and
lidarpointcloudforsemanticsegmentation. InAAAICon- PatternRecognition,pages9939–9948,2021.
ferenceonArtificialIntelligence,pages12795–2803,2022.
[103] YangZou,ZhidingYu,BVKVijayaKumar,andJinsong
[91] AoranXiao,JiaxingHuang,DayanGuan,XiaoqinZhang, Wang. Unsuperviseddomainadaptationforsemanticseg-
ShijianLu,andLingShao. Unsupervisedpointcloudrep- mentation via class-balanced self-training. In European
resentationlearningwithdeepneuralnetworks: Asurvey. ConferenceonComputerVision,pages289–305,2018.
IEEETransactionsonPatternAnalysisandMachineIntel-
ligence,45(9):11321–11339,2023.
[92] Shaoyuan Xie, Lingdong Kong, Wenwei Zhang, Jiawei
Ren,LiangPan,KaiChen,andZiweiLiu. Robobev: To-
wardsrobustbird’seyeviewperceptionundercorruptions.
arXivpreprintarXiv:2304.06719,2023.
[93] ChenfengXu, BichenWu, ZiningWang, WeiZhan, Peter
Vajda, KurtKeutzer, andMasayoshiTomizuka. Squeeze-
segv3: Spatially-adaptive convolution for efficient point-
cloudsegmentation. InEuropeanConferenceonComputer
Vision,pages1–19,2020.
[94] JingyiXu, WeidongYang, LingdongKong, YouquanLiu,
Rui Zhang, Qingyuan Zhou, and Ben Fei. Visual foun-
dation models boost cross-modal unsupervised domain
adaptation for 3d semantic segmentation. arXiv preprint
arXiv:2403.10001,2024.
[95] XiangXu,LingdongKong,HuiShuai,andQingshanLiu.
Frnet: Frustum-rangenetworksforscalablelidarsegmen-
tation. arXivpreprintarXiv:2312.04484,2023.
[96] XuYan,JiantaoGao,ChaoZhengChaodaZheng,Ruimao
Zhang, Shuguang Cui, and Zhen Li. 2dpass: 2d priors
assisted semantic segmentation on lidar point clouds. In
EuropeanConferenceonComputerVision,pages677–695,
2022.
17