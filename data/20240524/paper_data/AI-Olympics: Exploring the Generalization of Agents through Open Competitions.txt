AI-Olympics: Exploring the Generalization of Agents through Open Competitions
ChenWang1, YanSong1, ShuaiWu2, SaWu1, RuizhiZhang1,
ShuLin1 and HaifengZhang1,2,3,∗
1InstituteofAutomation,ChineseAcademyofSciences
2NanjingArtificialIntelligenceResearchofIA
3SchoolofArtificialIntelligence,UniversityofChineseAcademyofSciences
∗CorrespondingAuthor
{chen.wang,yan.song,sa.wu,ruizhi.zhang,chen.wang,shu.lin,haifeng.zhang}@ia.ac.cn,
wushuai@airia.cn
Abstract generalization skills. A demonstration video is available at
https://youtu.be/SFXRe1JI6C8.
Between2021and2023,AI-Olympics—aseriesof
onlineAIcompetitions—washostedbytheonline 2 AI-OlympicsEnvironment
evaluation platform Jidi in collaboration with the
AI-Olympicsenvironmentisatwo-dimensionalphysicalsim-
IJCAI committee. In these competitions, an agent
ulator built from scratch with minimal dependencies, utiliz-
is required to accomplish diverse sports tasks in
ingPygame[Shinde,2021]forvisualization. Thedesignen-
a two-dimensional continuous world, while com-
sures flexible deployment and extensibility. The underlying peting againstan opponent. This paper providesa
enginereplicatesthephysicaldynamicsofagentsandelastic
brief overview of the competition series and high-
collisionsbetweengeometricshapes,includingstraightlines,
lights notable findings. We aim to contribute in-
curves,andcircles.Leveragingthisgameengine,varioussce-
sights to the field of multi-agent decision-making
nariosandtaskscanbegenerated,spanningdifferentconfig-
and explore the generalization of agents through
urations. Three essential characteristics define the environ-
engineeringefforts.
ment: (1)continuouscontrolwithpartialobservation,(2)
zero-sum,and(3)multi-tasking.
1 Introduction
2.1 MobilityandObservability
In recent years, AI-based decision-making has gained much Each agent is depicted as an elastic circle on the map,
attentioninbothacademiaandindustries. Particularly,Rein- equipped with the ability to exert impetus and torque, albeit
forcementLearning(RL)hasmaturedrapidlyyieldinghigh- at the cost of internal energy, enabling movement and inter-
level performance in various tasks in games [Schrittwieser action with the environment. Running out of energy results
et al., 2019], robotics [Kober et al., 2013], advertising [Cai in a loss of control over the agent. Within the game envi-
et al., 2017], etc. However, many of these carefully trained ronment, agents can collide and experience friction, thereby
agentshavebeencriticizedfortheirpoorgeneralizationabil- alteringtheirstatesaccordingly.AsillustratedinFigure1,the
ities when applied to a slightly difference task [Leibo et al., agentcanperceiveitssurroundingswithinalimitedrange,al-
2021; Gorsane et al., 2022; Agapiou et al., 2022]. Imple- lowing it to observe any other object within the map. Other
mentinganadaptiveagentthatcanperformequallywellona objectsonthemaparerepresentedasgeometricshapes,each
series of tasks is still an open and challenging problem. On distinguished by its unique color to denote specific charac-
theotherhand,assessinganagent’sgeneralizationskillsnor- teristics. As a result, we can construct various maps with
mallyreliesonbenchmarksuitsencompassingvariousenvi- recognizable items placed, thereby enhancing intelligent AI
ronments[Packeretal.,2018;Kirketal.,2023],tasks[Leibo recognitionandgeneralizationacrossscenarios.
et al., 2021; Oh et al., 2017] and opponents [Leibo et al.,
2021; Agapiou et al., 2022], which are often static and may 2.2 Zero-sumGames
proveinflexibleamidtherapidevolutionofagents. Typically, each game in AI-Olympics contains two control-
To advance the research in this domain, we have devel- lable agents with conflicting interests, rendering it a two-
opedaPython-basedtwo-dimensionalphysicalgameengine playerzero-sumgameextensivelyexploredinpriorresearch
AI-Olympics environment, accompanied by a variety of sce- [Silver et al., 2016; Silver et al., 2017; Song et al., 2024].
narios. Meanwhile, we innovatively conducted a series of Bothagentssharethesamemobilityandobservabilityforfair
AIcompetitionsutilizingdiversescenarioscreatedwithinthe competition,andeachcanseektogainanedgebyobstructing
framework for a more generalized evaluation. In this pa- pathways.Suchuncertaintyregardingtheopponent’sstrategy
per,weprovideabriefintroductiontoAI-Olympicsenviron- during the evaluation phase can pose a significant challenge
ment and the accompanying scenarios, as well as the corre- totheparticipants,encouragingthemtoconsidergeneraliza-
sponding competition series and the evaluation of different tionskills.
4202
yaM
32
]AM.sc[
1v85341.5042:viXraEnergy
Impetus &
Torque
(a)Wrestling (b)Billiard (c)Table-Hockey
Energy RGB
Figure1: Anillustrationofagent’smobilityandvision. Anagent
consumesstoredenergyandmoves,whileonlyobservingpartially
ofitssurroundings.
(d)Football (e)Curling
Figure 3: Five additional scenarios built from AI-Olympics game
engine.
Contest Scenarios Participants
RLCN-2021-Winter Running 145
Figure2: ThreeexamplemapsinRunninggame. Agentsarerepre- RLCN-2022-Spring Curling 103
sentedascirclesandthearrowattachedtothemshowsthedriving RLCN-2022-Fall Wrestling 103
forceapplied. RLCN-2022-Winter Table-Hockey 89
RLCN-2023-Spring Billiard 59
2.3 MultipleDesignedTasks RLCN-2022-Summer Integrated(4) 166
Thegameengineiscraftedtogeneratediversescenariosfea- IJCAI-ECAI2022 Integrated(4) 75
turingcommonitemsonthemap,allowingforanexploration IJCAI2023 Integrated(6) 58
of how agents can generalize their skills across tasks. We
havedevelopedsixuniquescenarios,eachcomprisingacom- Table1: Between2021and2023,AI-OlympicsCompetitionSeries
petitive sports task with different objectives and involving hasheldeightonlineAIcontests,amongwhichsixofthemarepre-
twoagentswithconflictinginterests. Thesescenariosare: liminary and two are coordinated with the IJCAI committee. The
competitionseriescoversadifferentrangeofscenariosandhasat-
Runninggame. Runninggameinvolvestwoagentsexplor-
tractednumbersofparticipants.
ingamazeandseekingtoreachthegoalline(finishline)as
fastaspossible.Figure2givesthreeexamplemapswithcom-
monmapelements. Forexample,theblacklinerepresentsa ing agent movement control and map items. Consequently,
stickywallwheretheagentlosesspeedwhencollisions,and a well-trained agent can adeptly identify critical elements
the red dashed line represents the goal line. The first agent within its environment, enabling efficient exploration and
thatreachesthegoallinewinsthegame. navigationtoovercomeobstacles.
Other single games. Five additional sports-related games
3 AI-OlympicsCompetitionSeries
are depicted in Figure 3. Wrestling game has the opposite
objectivecomparedtotheRunninggame—eachagentshould AI-Olympics competition series adopts the tasks outlined in
keep itself away from the goal line (border line). In Curl- Section 2.3 as themes and is evaluated by our online plat-
ing game, players engage ina turn-based format wherethey form, Jidi (http://www.jidiai.cn). The platform offers live
strategically “throw” their rocks towards the central goal, rankings and hosts AI competitions akin to Kaggle (https://
aiming to position them as close to the center as possible. www.kaggle.com/),specificallyfocusingondecision-making
Both Table-Hockey and Football share similar map layouts, challenges. Theobjectiveofthecompetitionseriesistopro-
requiring players to score goals while also playing defen- vide testbeds and promote practical engineering exploration
sively. However, in Table-Hockey, agents are restricted to of agents’ generalization abilities. Notably, as shown in Ta-
movement within their half of the field, while in Football, ble 1, we have conducted several preliminary contests, two
agentsenjoygreaterfreedomofmovement. InBilliardgame, of which were partnered with the IJCAI committee to gain
theobjectiveofeachagentistopocketballsoftheassigned furtherattention.
color.
3.1 EvaluationProtocol
Integratedgames. Theintegratedgamecombines4–6dis-
tinct scenarios into one unified task. Two players engage in Platform Jidi offers online evaluation services for submit-
consecutive games, with the victor of the majority of games tedagentsinvarioussimulatedenvironments. Theregistered
as the series winner. While each game features unique ob- userssubmittheircodefilesonthewebsite,thentheplatform
jectives, they are underpinned by shared elements includ- performs back-end evaluation and updates the results on thefront-end webpage. Jidi is able to provide real-time ranking incorporating the new maps into their training, thereby ex-
whichishelpfulforparticipantstotesttheiragents. ForAI- pandingthebreadthoftheirtrainingdatathroughtechniques
Olympics competition series, since the games are all zero- suchasdataaugmentation[ShortenandKhoshgoftaar,2019].
sum, Jidi performs a Swiss-system Tournament for player Consequently,thisbroaderscopeoftrainingdatacontributes
pairing during the evaluation process, which has been used totheenhancementofagents’generalizationskillsovertime.
invarioustypesofcompetitionsincludingsportstournaments
Improving performance via scenario-specific strategies.
andacademicevents[Csato´,2013].
Despitetherandomorderofscenarios,manyagentsstillman-
age to apply a targeted policy to each scenario in the inte-
3.2 GeneralizationAbilitiesAssessment
grated game. In some scenarios such as Curling, which de-
During the preliminary competition, we utilize individual mands more intricate skills, participants demonstrated a no-
scenariosastestbedsandinviteparticipationineachspecific table trend for adapting their strategies to the current game
task. Additionally,weexploreanintegratedscenariowherein situation. Forinstance, someagentswillstopinfrontofthe
theagentengagesinallgamessequentially. Throughoutthe goal line (hog line) and observe the situation first and then
competition,agentsareassessedfortheirgeneralizationabil- move backward to leave space for “throwing”. This aligns
itiesfromvariousperspectives. with our motivation to run the competition which is to ask
Map generalization. In the Running game competition, theagenttoactadaptivelyinthegameinsteadofreactingto
agents are required to efficiently navigate the track map and experience.
reach the goal line (finish line) before their opponent. The Gainingadvantagesincompetitivegamesthroughaggres-
competitionspansmultipleroundsofevaluation,withnewly siveactions. Thepresenceofopponentsintroducesvarying
designedmapsaddedtothepoolineachround. Thisensures degreesofchallenge. Inscenariosemphasizingcompetition,
thatthesubmittedagentsaretestednotonlyonfamiliarmaps we observe certain agents exhibiting interference behaviors.
but also on unseen layouts. These new maps feature iden- Forexample,insomemapsofRunninggame,agentsactively
tical elements, as depicted in Figure 2. A proficient agent engageintacticssuchas(1)intentionallycollidingwithop-
must adeptly interpret the elements on the map and adjust ponentstoslowtheirprogressbypushingthemagainstwalls,
itsmovementstrategyaccordingly,therebydemonstratingits or(2)strategicallymaneuveringtoobstructthepathsofoth-
generalizationabilitiesacrossvariousmaps. ersduringturns.
Scenario generalization. In the integrated games compe- EmployingdiverseAI-relatedmethods. Afterdiscussing
titions, agents are required to compete in multiple sequen- with several participants, we have gained valuable insights
tial sports scenarios, each characterized by a distinct layout intothemethodologiestheyusedtotacklethetasksathand,
andobjectivedescribedinSection2.3. Topreventtheagent including:
frommemorizingthescenariosequence,weemployrandom • reinforcement learning coupled with computer vision
shuffling of orders, forcing the agent to identify the current techniques,
scenario during the evaluation phase. Furthermore, a future
• learningfromdemonstrationtoacceleratetraining,
extensionmayinvolvetheadditionofnewtasksineacheval-
• self-playframeworkforrobustness,
uationround,placingmoreemphasisontheagent’sabilityto
generalizeacrossscenarios. • complexrewardshapingtoguidebehavior,
Opponentgeneralization. Throughoutthecompetitionse- • historicaldatasummarizationforlong-termplanning,
ries,theevaluationprotocolrequirestheplayernotonlysolve • curriculumlearningtoeasethetrainingprocess,and
multipletasksbutalsocompetewithvariousopponents.Play-
• heuristicmethodstofilluptheopponentpool.
ing the same game with different opponents can yield dis-
Inpractice,participantsoftenutilizedacombinationofthese
parate outcomes. Consequently, participants must take into
methodswhenimplementingtheiragent. Incontrasttostate-
accountthestrategiesemployedbyotherplayers.Thiscanbe
of-the-art algorithms in academia, the competition under-
achieved by reviewing replays of other players on the web-
scored the practicality of machine learning methods and of-
site and designing counter-strategies that can be generalized
feredvaluableengineeringinsightstothefieldofgameAI.
acrossvariousopponents.
5 Conclusion
4 AnalysisoftheSubmittedAgents
AI-Olympics competition series has provided a unique plat-
Duringourcompetitionseries,someinterestingfindingsecho
form for participants to showcase their expertise in the field
ourmotivationaswellasprovideinsightsintothefield.
of game AI. Through the evaluation processes and diverse
Enhancing map generalization through increased data. challenges, participants have demonstrated their proficiency
IntheRunninggamecompetition,atthebeginningthereare inaddressingcomplexscenariosandadaptingtodynamicen-
two maps in the candidate pool, then in subsequent rounds vironments. The competition’s emphasis on practicality and
two additional maps are introduced into the pool, each with agent generalization abilities underscores the complexity of
anincreasedprobabilityofselection.Asthecompetitionpro- game AI research nowadays and brings insights for further
gresses,weobserveaconsistentimprovementinthemobility researchendeavors. Inthefuture,wewillkeepexploringthe
and completion rates of participants’ agents on newly intro- generalizationskillsofagentsandcontinuetogathercommu-
duced maps. This phenomenon is attributed to participants nitysupporttotackletheproblem.Acknowledgments [Shinde,2021] Piyush N Shinde. Pygame: Develop games
using python. International Journal for Research in Ap-
Haifeng Zhang thanks the support of the National Natural
pliedScienceandEngineeringTechnology,2021.
ScienceFoundationofChina,GrantNo. 62206289.
[ShortenandKhoshgoftaar,2019] Connor Shorten and
TaghiMKhoshgoftaar. Asurveyonimagedataaugmen-
References
tation for deep learning. Journal of big data, 6(1):1–48,
[Agapiouetal.,2022] John P Agapiou, Alexander Sasha 2019.
Vezhnevets, Edgar A Due´n˜ez-Guzma´n, Jayd Matyas, Yi- [Silveretal.,2016] DavidSilver,AjaHuang,ChrisJMaddi-
ran Mao, Peter Sunehag, Raphael Ko¨ster, Udari Mad- son,ArthurGuez,LaurentSifre,GeorgeVanDenDriess-
hushani, Kavya Kopparapu, Ramona Comanescu, et al. che, Julian Schrittwieser, Ioannis Antonoglou, Veda Pan-
Meltingpot2.0. arXivpreprintarXiv:2211.13746,2022. neershelvam, Marc Lanctot, et al. Mastering the game
of go with deep neural networks and tree search. nature,
[Caietal.,2017] HanCai,KanRen,WeinanZhang,Klean-
529(7587):484–489,2016.
thisMalialis,JunWang,YongYu,andDefengGuo. Real-
time bidding by reinforcement learning in display adver- [Silveretal.,2017] David Silver, Julian Schrittwieser,
tising. ProceedingsoftheTenthACMInternationalCon- KarenSimonyan,IoannisAntonoglou,AjaHuang,Arthur
ferenceonWebSearchandDataMining,2017. Guez,ThomasHubert,LucasBaker,MatthewLai,Adrian
Bolton, et al. Mastering the game of go without human
[Csato´,2013] La´szlo´ Csato´. Ranking by pairwise compar-
knowledge. nature,550(7676):354–359,2017.
isons for swiss-system tournaments. Central European
JournalofOperationsResearch,21:783–803,2013. [Songetal.,2024] YanSong,HeJiang,ZhengTian,Haifeng
Zhang,YingpingZhang,JiangchengZhu,ZonghongDai,
[Gorsaneetal.,2022] Rihab Gorsane, Omayma Mahjoub,
Weinan Zhang, and Jun Wang. An empirical study on
Ruan John de Kock, Roland Dubb, Siddarth Singh, and google research football multi-agent scenarios. Machine
ArnuPretorius. Towardsastandardisedperformanceeval- IntelligenceResearch,pages1–22,2024.
uationprotocolforcooperativemarl. AdvancesinNeural
InformationProcessingSystems,35:5510–5521,2022.
[Kirketal.,2023] Robert Kirk, Amy Zhang, Edward
Grefenstette,andTimRockta¨schel. Asurveyofzero-shot
generalisationindeepreinforcementlearning. Journalof
ArtificialIntelligenceResearch,76:201–264,2023.
[Koberetal.,2013] JensKober,J.AndrewBagnell,andJan
Peters. Reinforcement learning in robotics: A survey.
The International Journal of Robotics Research, 32:1238
–1274,2013.
[Leiboetal.,2021] JoelZLeibo,EdgarADuen˜ez-Guzman,
Alexander Vezhnevets, John P Agapiou, Peter Sunehag,
Raphael Koster, Jayd Matyas, Charlie Beattie, Igor Mor-
datch, and Thore Graepel. Scalable evaluation of multi-
agentreinforcementlearningwithmeltingpot. InInterna-
tionalconferenceonmachinelearning,pages6187–6199.
PMLR,2021.
[Ohetal.,2017] JunhyukOh,SatinderSingh,HonglakLee,
and Pushmeet Kohli. Zero-shot task generalization with
multi-task deep reinforcement learning. In International
Conference on Machine Learning, pages 2661–2670.
PMLR,2017.
[Packeretal.,2018] Charles Packer, Katelyn Gao, Jernej
Kos, Philipp Kra¨henbu¨hl, Vladlen Koltun, and Dawn
Song. Assessing generalization in deep reinforcement
learning. arXivpreprintarXiv:1810.12282,2018.
[Schrittwieseretal.,2019] Julian Schrittwieser, Ioannis
Antonoglou, Thomas Hubert, Karen Simonyan, L. Sifre,
Simon Schmitt, Arthur Guez, Edward Lockhart, Demis
Hassabis,ThoreGraepel,TimothyP.Lillicrap,andDavid
Silver. Mastering atari, go, chess and shogi by planning
withalearnedmodel. Nature,5887839:604–609,2019.