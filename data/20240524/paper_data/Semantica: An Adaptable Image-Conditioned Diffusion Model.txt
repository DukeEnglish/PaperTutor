Semantica: An Adaptable Image-Conditioned
Diffusion Model
ManojKumar NeilHoulsby EmielHoogeboom
GoogleDeepmind GoogleDeepmind GoogleDeepmind
mechcoder@google.com neilhoulsby@google.com emielh@google.com
Abstract
Weinvestigatethetaskofadaptingimagegenerativemodelstodifferentdatasets
withoutfinetuneing. Tothisend,weintroduceSemantica,animage-conditioned
diffusionmodelcapableofgeneratingimagesbasedonthesemanticsofacondi-
tioningimage. Semanticaistrainedexclusivelyonweb-scaleimagepairs,thatisit
receivesarandomimagefromawebpageasconditionalinputandmodelsanother
randomimagefromthesamewebpage. Ourexperimentshighlighttheexpressivity
of pretrained image encoders and necessity of semantic-based data filtering in
achievinghigh-qualityimagegeneration. Oncetrained,itcanadaptivelygenerate
newimagesfromadatasetbysimplyusingimagesfromthatdatasetasinput. We
studythetransferpropertiesofSemanticaonImageNet,LSUNChurches,LSUN
BedroomandSUN-397.
1 Introduction
Machinelearninginitiallyfocusedonoptimizingandimprovingmodelsonsmalldatasets. Thefield
hastransitionedtotraininggeneralpurposemodelsonweb-scaledataandthenfinetuningthemfor
specifictasksonsmallerdatasets. Thisparadigmshifthasleadtostate-of-the-artresultsonanumber
ofdifferentdomains. Inthispaper, wefocusontherelativelyunderexploredtaskofadaptingan
imagegenerativemodeltodifferentdatasets. Oneapproachistosimplytrainagenerativemodelon
alargedatasetofunlabelledimagesandfinetunethemonsmallerdatasets. Whilethisapproachis
straight-forwardintheory,itrequirescleverarchitectureorregularizerdesigntopreventoverfittingin
practice(SeeSec.2.1).Asmodelsscaleup,finetuningforeverydatasetalsojustbecomesincreasingly
impractical.
Instead,weapplytheprincipleofin-contextlearningtoadaptaimagegenerativemodel. Wetrain
ourmodelcalledSemanticausingimageepisodes,whichareimagepairsthatbelongtothesameweb
page. Therefore,trainingreliesexclusivelyonthehypothesisthatimagesfromthesamewebpage
shouldhavesomecommonsemanticattributes. Forexample,itishighlylikelythatimagesscraped
fromaWikipediapageondolphins,containpicturesofdolphins. Atadaptation,themodelreceives
animageandthengeneratesanotherimagethatpreservessemanticinformation.
Semantica consists of a pre-trained image encoder and a diffusion model. The encoder extracts
representations from the conditioning image and guides the diffusion model. We thoroughly in-
vestigatetheimpactofdifferentchoicesfortheimageencoder(contrastiveorself-supervised)and
theconditioningmechanisms(globalorlocal). Ourexperimentsalsohighlighttheimportanceof
semanticdatafilteringtogeneratehigh-qualityimages. EachrowinFig1showcasesaconditioning
imagetotheleftandfivegeneratedsamplestotheright. Semanticaiscapableofgeneratinghigh
qualityanddiverseimages,reflectiveofsemanticinformationfromtheconditioningimageacrossa
numberofdifferentdatasets.
Preprint.Underreview.
4202
yaM
32
]VC.sc[
1v75841.5042:viXraImageNet
SUN397
LSUNChurches
LSUNBedroom
Figure 1: Left: Conditioning image Right: Five samples at guidance 0.5. Semantica is trained
exclusivelyonweb-imagepairs. Duringadaptation,itreceivesaconditioningimageandgenerates
samplesreflectiveofsemanticinformation. Semanticarequiresnolabelsupervisionorfinetuning.
22 Relatedwork
2.1 GenerativeTransfer
Therehasbeenextensiveresearchthatstudiestheadaptationofsource-pretrainedgenerativemodels
toatargetdatasetwithadaptationofweights. Initialworksstudythetransferofdiscriminatorsand
generatorsinGANsfromasourcedatasettoatargetdataset[47]. Further,[17]showthatImageNet
pretrainingonalargeGANmodelisbeneficialfortransfertosmalldatasets. Anumberofpapers
focusonimprovinggenerationqualitybyadaptingonlyasubsetofparameters. Theseincludescale
andshiftparameters[32],updatingonlythehigherdiscriminatorlayers[28],linearcombinationsof
scaleandshiftparameters[41],modulatingkernelsorconvolutions[59,58,10,2]andsingularvalues
[38],mappingnetworksfromnoisetolatents[46,29,53]andlatentoffsets[12]. Variousworksapply
regularizationlossesbyenforcingconstraintstosamples/weightsbythesourcegeneratorincluding
elasticweightregularization[27], domaincorrespondence[33,16,22], contrastivelearning[60],
spatialalignment[51],inversion[49,23,44],randommasksondiscriminators[61]andalignment
freespatialcorrelation[30]. GiventheincreasingpopularityofVQ-VAEanddiffusionbasedmodels,
recentwork[43]and[61]explorefew-shotfinetuningonVQ-VAEtokensanddiffusionmodels. We
defertothesurvey[1]foradetailedexpositionofallthesemethods. Incontrasttotheseworks,we
exploretrainingageneratoronweb-scaleimagesandstudytheirtransfertostandardsmall-scale
imagedatasets. Retrievalaugmentedmodels[7,5]computenearestneighboursforaqueryimage
across a bank of memory images. These retrieved neighbors facilitate the training or generation
process. Unlikethesemethods,wedonotrequireaccesstoamemorybankofimagesduringtrainor
testtime.
2.2 Diffusion
Diffusion and score-based generative models have become increasingly successful in modelling
images[20,40,31,4],videos[18,42]andaudio[25]. Asgenerationqualityhassteadilyimproved,
theyhavebeenusedincontextswithmoreandmoreconditioningvariables. Well-knownexamples
aretext-to-imageandtext-to-videomodelling,wheretheconditioningvariableistext. Inthiscase,
theconditioningvariablecanbeseenasasequencefromwhichcross-attentionlayerscommunicate
to the feature maps of the image or video, i.e. what the diffusion model is learning to generate
[40,31]. Asthedesireforcontrollablegenerationincreases,solutionssuchasControlNet[56]have
beendeveloped. ControlNettakesinconditioningimagesofthesamesizeasthegenerations,and
uses a copy of the UNet to learn an encoder for the conditioning signals. Although this encoder
trains fast due to parameter initialization from a pretrained diffusion UNet, it is difficult to deal
with different sized inputs. In those cases, only conditioning via cross-attention as done in [52]
overcomesthein-placeadditionsbetweentheControlNetencoderandthebaseUNet. Conditioning
onimagesascontexthasproducedimpressiveresults,turningscribblesoredgedetectionsintohigh
qualityimagegenerations[48]anddiscriminativetasks[3,26]. Incontrastwiththeabovementioned
techniques,ourframeworkreliesongeneralweb-basedpretrainingforsemantic-basedadaptiveimage
generation. Finally,while[13]studythetransferoffew-shotdiffusionmodelsbetweensmalldatasets
(CIFAR-100->miniImageNet,weseeinSec. 5.6thatthiscanleadtosub-optimaltransfer.
3 Background
3.1 Diffusion
Diffusion models learn to generate examples by gradually denoising a diffusion process. For a
singledatapoint,theirlosscanbeexpressedasasquarederrorbetweentheoriginaldatapointandits
prediction:
E (cid:2) w(t)||x−f(z )||2(cid:3) wherez =α x+σ ϵ (1)
t∼U(0,1),ϵ∼N(0,I) t t t t t
ItishelpfultodefineSNR(t)=α2/σ2. Inthecaseofw(t)=SNR(t)thelossaboveisequivalentto
t t
alossinϵ-space,thesimplelossfrom[19]. Aftertraining,thedenoisingmodelgeneratessamplesby
takingsmallsteps. Startingatt=1withinitialGaussiannoiseandoneslowlydenoisesfortimesteps
t=1,1−1/N,...wherethenumberofsamplingstepsisN. Althoughmanysamplersarepossible,
inthispaperweusethestandardDDPMsampler[19].
3Architecture In early days, diffusion literature typically used UNets that consisted of ResNet
blocks,withoptionalself-attentionlayers. MorerecentarchitectureeitherusefullTransformers(DiT
[35],StableDiffusion)ortransformerbackbonesUViTs[21]. Thistransformerbackbonemakesit
especiallyeasytouseconditioningsignalsinthesearchitectureviacross-attentionlayers. Tobe
precise,onecansaythatthedenoisingneuralnetworktakesinz
t
∈RH×W×3,t∈R,y ∈RTy×Dy.
In principle it does not matter which diffusion model we use to generate images. In fact, every
generativemodelwouldsuffice. Inpracticeweusethesimplediffusionframework[21]becauseit
canlearntogeneratehighresolutionimagesend-to-endwithouttheneedofaseparateautoencoder.
4 Method
4.1 EpisodicDataset
ThetrainingdatasetforourmodelisasubsetofWebLI[9]thatconsistsofroughlyonebillionimages
from the web. In particular, we use Episodic WebLI [8], where each episode contains randomly
sampledlooselyrelatedimages(i.e.,theyareclusteredaccordingtotheirURL).Werandomlysample
animageastheconditioninginputandanotherimagefromthesameepisodeastheground-truth
"target"image. Wedonotuseanytextinputsandonlyrelyonimageconditioning.
4.2 ImageEncoder
Training an image-conditioned diffusion model requires two components: 1) An image encoder
thatextractssemanticinformationfromaconditioningimage. 2)Adiffusionmodelthatdenoises
anoisytargetimageconditionedontheimagerepresentations. WecouldtrainaseparateVision
Transformer or ResNet model end-to-end as an image encoder with the diffusion model to learn
usefulconditioningrepresentations.
Instead,weleveragepre-trainedimageencodersandconditionourdiffusionmodelontheir“frozen"
representations. Thisofferstwoadvantages. First,wecanprecomputerepresentationsforallimages
inthedatasetthateliminatesexpensiveforwardpassesthroughtheencoderduringtraining. Second,
byrelyingonpretrainedencoders,wecanjustfocusonscalingthediffusionmodel.
We investigate ViT image encoders trained with two pretraining strategies: training on web-text
imagepairs[37](weusetheSigLIP[55]),andself-supervisionwithteacherpseudo-labels(DINO)
[6,34]. Furthermore,weexploretwoconditioningstrategiestoincorporateconditioninginformation:
1. Tokenlevelconditioning: Thediffusionmodelcross-attendstoasequenceoftokensfrom
thefinaltransformerblockoftheimageencoder.
2. Globalfeatureconditioning: Theencoderprovidesasingleglobalfeaturerepresentation
fromtheencoder,beingtheCLStokenforDINOandtheoutputoftheMAPblockforSigLIP.
Wenormalizeandembedthiswithadenseprojectionandaddittothenoiseembedding.
ThissubsequentlyconditionsthediffusionmodelviaFiLMlayers[36]asdoneinstandard
label-conditioneddiffusion.
4.3 DataFiltering
Eachepisodeconsistsofimagesthatarelooselyrelated,whereasourmodelassumesconditioning
andtargetimagessharesemanticinformation. Thismismatchmayleadthemodeltowastecapacity
onmodelingirrelevantnoisyconditioning-targetpairs. Toaddressthis,wefilteroutpairswithlow
semanticsimilarity. Thepretrainedencoder(DINO/SigLIP)computestheglobalCLSrepresentation
from the conditioning and target image. We compute the cosine similarity between the global
conditioning/target representations and filter out pairs below a lower threshold. Additionally, to
ensuregenerationofinterestingimages,wealsofilterconditioning-targetpairswithsimilarityabove
ahighthreshold. Thismayensurethatgeneratedimagesretainthesemanticsoftheconditioning
image,whilebeingsufficientlydifferent.
45 Experiments
5.1 EvaluationDatasets
Weuse10%oftheEpisodicWebLIdatasetforthearchitecturedesignablationsandweusetheentire
datasetforthefinalmodel. WeevaluatethemodelbyconditioningitonImageNetvalidationset
imagesandthencomputingtheFIDbetweenthegeneratedsamplesandtheImageNetvalidationset.
WeinvestigatethetransferofSemanticaonfourdatasets: ImageNet[39],LSUN-OutdoorChurch,
LSUN-Bedroom[54]andSUN-397[50]. WereporttheFIDforeachdatasetusing50Kconditioning
imagesrandomlysampledfromwithinthedataset. WeusethevalidationdatasetforImageNetwhile
thetrainsetforallotherdatasets.Toassessdiversity,wecomputethemeanLPIPS[57]scorebetween
50Kgeneratedsamplesandtheircorrespondingconditioningimages.
5.2 ImageEncoder
Here, we evaluate the different encoder and conditioning design choices (Sec. 4.2). We train
baselinediffusionmodelswithtwofrozenimageencoders(DINO-v2B/14andSigLIPB/16)and
twoconditioningstrategies(cross-attentionandFILM).
100 DINO FILM Unfiltered 80 Baseline
90 S Si ig gL LI IP P F CI ALM 80 Semantic Filter 70 N Ho ig hp ea rtc Lh Ring
80 DINO CA 70 60
60
70
50 50
60
50 40 40
40 30 30
5 10 15 20 25 30 5 10 15 20 25 30 5 10 15 20 25 30
Steps in 10K Steps in 10K Steps in 10K
Figure 2: We plot ImageNet FID as a function of number of training steps on Episodic WebLI.
Left: FIDoftwoimageencodersDINO-v2B/14andSigLIPB/16usingtwodifferentconditioning
methods: 1)Allencoderoutputtokensviacross-attention(CA)2)Globalfeaturerepresentationvia
FILM.Center: FIDofDINO-v2B/14+CrossAttentionwithandwithoutdatafiltering. Right: FID
ofDINO-v2B/14+CrossAttentionwith1x1patchingandhigherlearningrate.
Fig. 2 left shows ImageNet FID as a function of number of training steps on Episodic WebLI.
Token-levelconditioningsignificantlyoutperformsglobalfeatureconditioningforbothDINO-v2and
SigLIP,highlightingtheimportanceoflow-levelfeaturesforthisparticulartask. WhileSigLIPFiLM
performsbetterthanDINO-v2FiLM,thistrendreverseswithcross-attention. Thissuggestsagreater
compatibilityofDINO-v2low-levelfeaturesoverCLIPlow-levelfeaturesforsemantics-basedimage
generation.
Forthisreason,wewillnowuseDINO-v2frozenrepresentationswithacross-attentionconditioning
mechanismasourmainarchitecture.
5.3 DataFiltering
Weinvestigatetheimpactofsemanticdatafiltering,asdescribedinSec. 4.3. Weheuristicallyset
the lower and higher thresholds to 0.3 and 0.9 respectively. Fig. 2 middle shows the FID of the
DINO-v2B/14+CrossAttentionwithandwithoutdatafiltering. Similarity-baseddatafilteringin
DINOfeaturespacepositivelyimpactsthegenerationqualityandimprovesFIDbygreaterthan10.
Infuturework,wecanexploretuningthesethresholdsforadesiredquality-diversitytradeofforeven
directlyconditioningthediffusionmodelonthedesiredsimilaritywiththeconditioningimage.
5.4 Hyperparameters
Forourbaselinediffusionmodel,weinheritallhyper-parametersfromtheImageNetlabelconditioned
diffusion model. The denoising model follows a U-ViT architecture that operates on 256×256
images. Thearchitectureconsistsofainitial2×2patchificationlayer[11,45]thatdownsamples
the noised image by a factor of 2. The model has four downsampling stages, where each stage
downsamplesthefeaturemapsbyafactorof2atitsoutput. Theresolutionofthelowestfeaturemap
5
DIF DIF DIFis16×16. Transformerblocksoperateatthestageswiththetwolowestresolutions16×16and
32×32andconvolutionalblocksoperateintheremainingstages. Thefourdownsamplingstages
have128,256,512and1024channels. Thefirstthreestageshavethreeblockseachandthelaststage
hassixteenblocks. TheoptimizerisAdam[24]withparametersβ =0.9,β =0.99,ϵ=1e−12,
1 2
batchsizeof2048andalearningrateof1e−4. WealsousePolyakaveragingwithadecayfactorof
0.9999. Thediffusionlossparametersincludev-predictionwithlossinepsilonphaseandacosine
adjustedschedulewithanoiseresolutionof32. WeusetheDDPMsamplerwithaninterpolationof
0.2(standarddeviationisσ0.2 σ0.8)andnoguidanceforourablations. Eachablationrunutilizes256
t→s st
TPUv3[15]chipsaround2days. However,theconsistentrankingofdifferentablationsthroughout
trainingcanallowforamuchshortertrainingscheduletoidentifythebestmodel.
Wefinallyperformalightweightsweepofthediffusionmodelhyper-parameters. Notably,twominor
modifications improve generation quality: 1) Replacing the 2x2 patching with no patching. The
modelconsistsofa1×1convolutionattheinputandoutput. Thedownsamplingproceedswithan
additionalstageconsistingoftwoconvolutionalblocksand128channelseach. 2)Increasingthe
learningratefrom1e−4to2e−4. Fig. 2rightshowstheimprovementinFIDoftheDINOB/14
encoderwiththesemodifications. Otherchangessuchastuningnoiseschedules,dropoutratesor
Polyakdecayfactorscausenegligiblechangesordecreasesamplequality.
5.5 FinalTraining
Inourfinaltrainingphase,wemakethefollowingfouradjustments:
1. ReplaceDINO-v2ViT-B/14conditioningfeatureswiththelargerDINOViT-L/14condi-
tioningfeatures.
2. Scale the model by doubling the number of channels across all UViT stages. The only
exceptionisthecompute-heavyfirstdownsamplingstagewhichwekeepat128.
3. ExpandtrainingdatatoincludetheentiretyofepisodicWebLIwhichwaspreviously10%
intheablations.
4. Extendthetrainingdurationfrom300Kstepstotwomillionsteps
We train our model on 256 Cloud TPUv5 chips [14] for approximately 12 days with a constant
learningrateof2e−4.
5.6 LabelGroupedBaseline
HerewecompareSemanticatoabaselinethathasdirectlabelsupervision(forexample,ImageNet).
Recallthattheconditioningandtargetimagebelongtothesamewebpage. However,inthepresence
of label supervision (as in ImageNet), the target and conditioning image can just belong to the
sameclasslabel. Soasasupervisedbaseline,wegroupimagesonImageNetaspertheirlabeland
trainSemanticaonthisdataset. Table1comparestheFIDoftheLabelGroupedbaseline(LG)to
Semantica. SinceLGistrainedonImageNet,itsignificantlyoutperformsSemantica(FID4.8vsFID
18.4). However,thistrendreversesonallotherdatasets,whereSemanticaoutperformsLG.Both
thesupervisedbaselineandSemanticarelyontheDINOv2encoderwhichwastrainedonawide
varietyofdatasources. Thereforetheencoderitselfmayprovideusefulrepresentationsonanumber
ofdatasets. ButtrainingLGjustonImageNet,mightlimitthediffusionmodel’sexposurefromnon
ImageNetimages,potentiallyexplainingitssignificantperformancedroponallotherdatasets.
5.7 OneShotResults
WeevaluateSemantica’sone-shotimagegenerationcapabilities. EachrowinFig. 1showcasesa
conditioningimagetotheleftandfivegeneratedsamplestotheright. WeobservethatSemanticais
capableofgeneratinghighqualityanddiverseimages.
Table 1 reports quantitative results. Semantica receives 1000 and 397 conditioning images for
ImageNetandSUN397respectively,oneperclass. Themodelgenerates50and120samplesper
classforImageNetandSUN397totalling50000and47840generatedsamples. Wethenreportthe
FIDwith50000groundtruthsamples. SemanticaachievesaFIDof19.1and9.5onImageNetand
SUN397respectively. Onincreasingtheguidanceto0.5,theFIDimprovesto9.3and7.1respectively.
6ImageNet Bedroom Church SUN397
Labelgrouped 4.8 46.2 27.1 29.7 ImageNet SUN397
Semantica 18.4 6.2 17.3 6.7 Semantica 19.1 9.5
Guidance@0.5 +Guidance@0.5 9.3 7.1
Labelgrouped 5.1 34.2 20.4 22.4 Copy 38.4 70.6
Semantica 6.2 2.4 4.0 2.5
Table 1: Left: Comparison between Semantica and a Label Grouped baseline (LG) trained on
ImageNet. TheconditioningandtargetpairshavethesamelabelforLG.LGoutperformsSemantica
in-distributionandperformsworseonout-of-distributiondatasets. Right(OneShotGeneration
Results): EvaluatesSemanticaone-shotimagegenerationcapabilities. Semanticareceives1000and
397conditioningimagesforImageNetandSUN397respectively,one-per-class. Themodelgenerates
50 and 120 samples per-class for ImageNet and SUN397 totalling 50000 and 47840 generated
samples. WethenreporttheFIDwith50000groundtruthsamples. Copyisabaselinewherethe
sameper-classconditioningimageisjustreplicated50and120timesrespectively.
SimplyreplicatingtheconditioningimageCopy50and120times,achievesamuchworseFIDof
38.4and70.6.
5.8 EffectofGuidance
We investigate the quality/diversity tradeoff offered by Semantica’s guidance parameter. Fig. 3
LeftillustratestheFIDandLPIPSonImageNetfordifferentguidancefactors. Notethatwefilter
out nearduplicates with asemantic similarity greaterthan 0.9 fromthe training set. This makes
it unlikely that the model copies the conditioning image exactly to achieve a perfect FID. With
noguidance,SemanticaachievesaFIDof18.4onImageNet. ThehighLPIPSscore0.72further
reflectsthemodel’sabilitytogeneratediversesamples. Onincreasingtheguidanceuptoacertain
extent,wetradeoffqualitywithdiversity. Notably,atguidancefactor0.5,whichiscommonlyusedin
labelconditionedgeneration,SemanticaachievesaFIDof6.2withaLPIPSscoreof0.68. Fig. 4,
displaysfiveconditioningimagesfromImageNetandthegeneratedsamplesatdifferentguidance
factors. Atguidancefactor0.0,thesamplesreflectabroadsemanticcategoryfromtheconditioning
image. Increasingtheguidancefactorleadstosamplesthatincorporatemorespecificdetailsfromthe
conditioningimage. Forexample,withtheconditioningimageofthedogandthekid,Semantica
starswithasampleofadog. Thespecificbreedofthedogandthechildintheimageappearaswe
amplifytheguidance.
0.72
0.69
0.70 0.68
0.67
0.68 0.66
0.65
0.66 0.64 Bedroom
Church
ImageNet 0.63 Sun
0.64
2 4 6 8 10 12 14 16 18 20 22 2 4 6 8 10 12 14 16 18 20
FID FID
Figure3: LPIPS(Higherismorediverse)vsFID(Lowerisbetter)forfivedifferentdatasets(Left:
ImageNet,Right: LSUNBedroom,LSUNChurches,SUN397). Eachpointrepresentsadifferent
guidancefactor. Asexpected,wetradeoffqualitywithdiversityonincreasingguidance,buteven
withlowerdiversitythemodeldoesnotcollapsecompletelytoitsconditioningimage.
Fig. 3 Right demonstrates quantitative results on three smaller datasets LSUN Bedroom, LSUN
ChurchandSUN-397. Weobserveasimilartradeoffbetweenqualityanddiversityaswevarythe
guidance factor. With zero guidance, Semantica achieves a FID of 6.2, 17.3 and 6.7 on LSUN
7
SPIPL SPIPLFigure 4: Left: Conditioning Image from ImageNet. Right: Generated samples with guidance
factors0.0,0.1,0.2,0.5and1.0. Atguidancefactor0.0,thesamplesreflectabroadsemanticcategory
fromtheconditioningimage. Increasingtheguidancefactorleadstosamplesthatincorporatemore
specificdetailsfromtheconditioningimage.
Bedroom, LSUN Church and SUN397 respectively. On increasing the guidance to 0.5, the FID
improvesto2.4,4.0and2.5respectively.
Fig. 5showcasessamplesforeachsmalldatasetacrossvariousguidancefactors. Inrowfour,the
bed(mainobject)persistsacrossallguidancelevels,whilethechairandthefenceappearathigh
guidancelevels. Rowfiveexhibitsasimilareffect: thenumberofminaretsinthegeneratedchurch
increasesfromonetotwoandtheshapeofthemaindomebeginstoresembletheconditioningimage.
Inrowone,thesampleresemblestoyswithzeroguidance,thesampleresemblestoysbuttransforms
intoacrowdedconventionasguidanceincreases.
6 Conclusion
WepresentSemantica,animage-conditioneddiffusionmodelcapableofgeneratinghigh-qualityand
diverseimagesbasedonthesemanticsofaconditioningimage.Ourpretrainingstrategyissimple,the
diffusionmodelmodelsatargetimagewiththeconditioningimagecomingfromthesamewebpage.
Ourexperimentsalsofurtheremphasizetheexpressivityofpretrainedimageencodersandnecessity
ofsemantic-baseddatafiltering.
Limitations WeidentifythefollowinglimitationsofSemantica. 1)WhileadaptingSemanticato
downstreamdatasets,requiresnofinetuneing,alimitationisthattrainingSemanticarequireshigh
computationalresourcesasisthecasewithmostgenerativemodels. However,anyimprovements
thatmakediffusionmodelsmoreefficientislikelytotransfertoSemantica. 2)Ourmodeldepends
8SUN397
LSUNBedrooms
LSUNChurches
Figure5: Left: ConditioningImagesfromSUN397(Toptworows),LSUNBedrooms(Middletwo
rows)andLSUNchurches(Lasttworows). Right: Generatedsampleswithguidancefactors0.0,0.1,
0.2,0.5,1.0and1.5.
onafrozenencoderandwedonotmodifytheweights. Withmoreresources, wecanalsostudy
the tradeoffs with this design decision. 3) With a very high guidance factor (>1.0) , Semantica
mightconvergetoanoversaturatedconditioningimage. Anareaofimprovementwouldbetofind
waystosteerittoanotherimagewiththesamesemanticinformation. 4)Ourmodelasisdoesnot
incorporateadditionalconditioningsignals. Generalizingthemodeltoutilizeadditionalconditioning
informationcanbeanexcitingareaoffutureresearch. 5)Whileourmodelgenerallyproducesdiverse
andhigh-qualityimages,sometimesartifactscanappearespeciallyatguidance0.0andinareaslike
lower-levelstructureandhumanfaces. Scalingourmodelfurthermayremovetheseartifacts.
9References
[1] MiladAbdollahzadeh,ToubaMalekzadeh,ChristopherTHTeo,KeshigeyanChandrasegaran,Guimeng
Liu,andNgai-ManCheung. Asurveyongenerativemodelingwithlimiteddata,fewshots,andzeroshot.
arXivpreprintarXiv:2307.14397,2023.
[2] AibekAlanov,VadimTitov,andDmitryPVetrov. Hyperdomainnet: Universaldomainadaptationfor
generativeadversarialnetworks. AdvancesinNeuralInformationProcessingSystems,35:29414–29426,
2022.
[3] YutongBai,XinyangGeng,KarttikeyaMangalam,AmirBar,AlanYuille,TrevorDarrell,JitendraMalik,
andAlexeiAEfros. Sequentialmodelingenablesscalablelearningforlargevisionmodels. arXivpreprint
arXiv:2312.00785,2023.
[4] YogeshBalaji,SeungjunNah,XunHuang,ArashVahdat,JiamingSong,KarstenKreis,MiikaAittala,
TimoAila,SamuliLaine,BryanCatanzaro,TeroKarras,andMing-YuLiu.ediff-i:Text-to-imagediffusion
modelswithanensembleofexpertdenoisers. CoRR,abs/2211.01324,2022.
[5] AndreasBlattmann,RobinRombach,KaanOktay,JonasMüller,andBjörnOmmer. Retrieval-augmented
diffusionmodels. AdvancesinNeuralInformationProcessingSystems,35:15309–15324,2022.
[6] MathildeCaron,HugoTouvron,IshanMisra,HervéJégou,JulienMairal,PiotrBojanowski,andArmand
Joulin. Emergingpropertiesinself-supervisedvisiontransformers. InProceedingsoftheIEEE/CVF
internationalconferenceoncomputervision,pages9650–9660,2021.
[7] Arantxa Casanova, Marlene Careil, Jakob Verbeek, Michal Drozdzal, and Adriana Romero Soriano.
Instance-conditionedgan. AdvancesinNeuralInformationProcessingSystems,34:27517–27529,2021.
[8] XiChen,JosipDjolonga,PiotrPadlewski,BasilMustafa,SoravitChangpinyo,JialinWu,CarlosRiquelme
Ruiz,SebastianGoodman,XiaoWang,YiTay,etal. Pali-x: Onscalingupamultilingualvisionand
languagemodel. arXivpreprintarXiv:2305.18565,2023.
[9] Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian
Goodman,AdamGrycner,BasilMustafa,LucasBeyer,etal. Pali:Ajointly-scaledmultilinguallanguage-
imagemodel. arXivpreprintarXiv:2209.06794,2022.
[10] YulaiCong,MiaoyunZhao,JianqiaoLi,SijiaWang,andLawrenceCarin. Ganmemorywithnoforgetting.
AdvancesinNeuralInformationProcessingSystems,33:16481–16494,2020.
[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal.Animageisworth
16x16words:Transformersforimagerecognitionatscale. arXivpreprintarXiv:2010.11929,2020.
[12] YuxuanDuan,LiNiu,YanHong,andLiqingZhang. Weditgan: Few-shotimagegenerationvialatent
spacerelocation. InProceedingsoftheAAAIConferenceonArtificialIntelligence,volume38,pages
1653–1661,2024.
[13] Giorgio Giannone, Didrik Nielsen, and Ole Winther. Few-shot diffusion models. arXiv preprint
arXiv:2205.15463,2022.
[14] Google. Cloud tpu v5e inference introduction. https://cloud.google.com/tpu/docs/
v5e-inference,2023. Accessed21May2024.
[15] Google. System architecture | cloud tpu | google cloud. https://cloud.google.com/tpu/docs/
system-architecture-tpu-vm,2023. Accessed21May2024.
[16] YaoGou,MinLi,YilongLv,YusenZhang,YuhangXing,andYujieHe.Rethinkingcross-domainsemantic
relationforfew-shotimagegeneration. AppliedIntelligence,53:22391–22404,2023.
[17] TimofeyGrigoryev,AndreyVoynov,andArtemBabenko. When,why,andwhichpretrainedGANsare
useful? InInternationalConferenceonLearningRepresentations,2022.
[18] JonathanHo,WilliamChan,ChitwanSaharia,JayWhang,RuiqiGao,AlexeyA.Gritsenko,DiederikP.
Kingma,BenPoole,MohammadNorouzi,DavidJ.Fleet,andTimSalimans.Imagenvideo:Highdefinition
videogenerationwithdiffusionmodels. CoRR,abs/2210.02303,2022.
[19] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. InHugoLarochelle,
Marc’AurelioRanzato,RaiaHadsell,Maria-FlorinaBalcan,andHsuan-TienLin,editors,Advancesin
NeuralInformationProcessingSystems33:AnnualConferenceonNeuralInformationProcessingSystems
2020,NeurIPS,2020.
10[20] JonathanHo,ChitwanSaharia,WilliamChan,DavidJ.Fleet,MohammadNorouzi,andTimSalimans.
Cascadeddiffusionmodelsforhighfidelityimagegeneration. J.Mach.Learn.Res.,23:47:1–47:33,2022.
[21] EmielHoogeboom,JonathanHeek,andTimSalimans. simplediffusion:End-to-enddiffusionforhigh
resolutionimages. InAndreasKrause,EmmaBrunskill,KyunghyunCho,BarbaraEngelhardt,Sivan
Sabato,andJonathanScarlett,editors,InternationalConferenceonMachineLearning,ICML,volume202
ofProceedingsofMachineLearningResearch,pages13213–13232.PMLR,2023.
[22] XingzhongHou,BoxiaoLiu,ShuaiZhang,LulinShi,ZiteJiang,andHaihangYou. Dynamicweighted
semanticcorrespondenceforfew-shotimagegenerativeadaptation. InProceedingsofthe30thACMInter-
nationalConferenceonMultimedia,MM’22,page1214–1222,NewYork,NY,USA,2022.Association
forComputingMachinery.
[23] YuichiKato,MasahikoMikawa,andMakotoFujisawa.Fasterfew-shotfaceimagegenerationwithfeatures
ofspecificgroupusingpivotaltuninginversionandpca. In2023InternationalConferenceonArtificial
IntelligenceinInformationandCommunication(ICAIIC),pages419–424,2023.
[24] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980,2014.
[25] ZhifengKong,WeiPing,JiajiHuang,KexinZhao,andBryanCatanzaro. Diffwave:Aversatilediffusion
modelforaudiosynthesis. CoRR,abs/2009.09761,2020.
[26] FengLi, QingJiang, HaoZhang, TianheRen, ShilongLiu, XueyanZou, HuaizheXu, HongyangLi,
ChunyuanLi,JianweiYang,etal. Visualin-contextprompting. arXivpreprintarXiv:2311.13601,2023.
[27] YijunLi,RichardZhang,JingwanLu,andEliShechtman. Few-shotimagegenerationwithelasticweight
consolidation. arXivpreprintarXiv:2012.02780,2020.
[28] SangwooMo,MinsuCho,andJinwooShin. Freezethediscriminator:asimplebaselineforfine-tuning
gans. arXivpreprintarXiv:2002.10964,2020.
[29] ArnabKumarMondal,PiyushTiwary,ParagSingla,andPrathoshAP. Few-shotcross-domainimage
generationviainference-timelatent-codelearning. InTheEleventhInternationalConferenceonLearning
Representations,2023.
[30] JongboMoon,HyunjunKim,andJae-PilHeo. Progressivefew-shotadaptationofgenerativemodelwith
align-freespatialcorrelation. ProceedingsoftheAAAIConferenceonArtificialIntelligence,37(2):1923–
1930,Jun.2023.
[31] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob
McGrew,IlyaSutskever,andMarkChen. GLIDE:towardsphotorealisticimagegenerationandediting
withtext-guideddiffusionmodels. InKamalikaChaudhuri,StefanieJegelka,LeSong,CsabaSzepesvári,
GangNiu,andSivanSabato,editors,InternationalConferenceonMachineLearning,ICML,volume162
ofProceedingsofMachineLearningResearch,pages16784–16804.PMLR,2022.
[32] AtsuhiroNoguchiandTatsuyaHarada.Imagegenerationfromsmalldatasetsviabatchstatisticsadaptation.
InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,pages2750–2758,2019.
[33] UtkarshOjha,YijunLi,JingwanLu,AlexeiAEfros,YongJaeLee,EliShechtman,andRichardZhang.
Few-shotimagegenerationviacross-domaincorrespondence.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages10743–10752,2021.
[34] MaximeOquab,TimothéeDarcet,ThéoMoutakanni,HuyVo,MarcSzafraniec,VasilKhalidov,Pierre
Fernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2:Learningrobustvisual
featureswithoutsupervision. arXivpreprintarXiv:2304.07193,2023.
[35] WilliamPeeblesandSainingXie.Scalablediffusionmodelswithtransformers.InIEEE/CVFInternational
ConferenceonComputerVision,ICCV2023,Paris,France,October1-6,2023,pages4172–4182.IEEE,
2023.
[36] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual
reasoningwithageneralconditioninglayer.InProceedingsoftheAAAIconferenceonartificialintelligence,
volume32,2018.
[37] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,GretchenKrueger,andIlyaSutskever. Learning
transferablevisualmodelsfromnaturallanguagesupervision. InICML,2021.
11[38] EstherRobb,Wen-ShengChu,AbhishekKumar,andJia-BinHuang. Few-shotadaptationofgenerative
adversarialnetworks. arXivpreprintarXiv:2010.11943,2020.
[39] OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,SeanMa,ZhihengHuang,
AndrejKarpathy,AdityaKhosla,MichaelBernstein,etal.Imagenetlargescalevisualrecognitionchallenge.
Internationaljournalofcomputervision,115:211–252,2015.
[40] ChitwanSaharia,WilliamChan,SaurabhSaxena,LalaLi,JayWhang,EmilyDenton,SeyedKamyarSeyed
Ghasemipour,BurcuKaragolAyan,S.SaraMahdavi,RaphaGontijoLopes,TimSalimans,JonathanHo,
DavidJ.Fleet,andMohammadNorouzi. Photorealistictext-to-imagediffusionmodelswithdeeplanguage
understanding. CoRR,abs/2205.11487,2022.
[41] MohamadShahbazi,ZhiwuHuang,DandaPaniPaudel,AjadChhatkuli,andLucVanGool. Efficient
conditionalgantransferwithknowledgepropagationacrossclasses. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages12167–12176,2021.
[42] UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn,SongyangZhang,QiyuanHu,HarryYang,
OronAshual,OranGafni,DeviParikh,SonalGupta,andYanivTaigman. Make-a-video:Text-to-video
generationwithouttext-videodata. CoRR,abs/2209.14792,2022.
[43] KihyukSohn,HuiwenChang,JoséLezama,LuisaPolania,HanZhang,YuanHao,IrfanEssa,andLuJiang.
Visualprompttuningforgenerativetransferlearning. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages19840–19851,2023.
[44] KowshikThopalli,RakshithSubramanyam,PavanTuraga,andJayaramanJThiagarajan. Target-aware
generativeaugmentationsforsingle-shotadaptation. arXivpreprintarXiv:2305.13284,2023.
[45] HugoTouvron,MatthieuCord,MatthijsDouze,FranciscoMassa,AlexandreSablayrolles,andHervéJégou.
Trainingdata-efficientimagetransformers&distillationthroughattention. InInternationalconferenceon
machinelearning,pages10347–10357.PMLR,2021.
[46] YaxingWang,AbelGonzalez-Garcia,DavidBerga,LuisHerranz,FahadShahbazKhan,andJoostvande
Weijer.Minegan:effectiveknowledgetransferfromganstotargetdomainswithfewimages.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages9332–9341,2020.
[47] YaxingWang,ChenshenWu,LuisHerranz,JoostVandeWeijer,AbelGonzalez-Garcia,andBogdan
Raducanu. Transferringgans: generatingimagesfromlimiteddata. InProceedingsoftheEuropean
ConferenceonComputerVision(ECCV),pages218–234,2018.
[48] ZhendongWang,YifanJiang,YadongLu,PengchengHe,WeizhuChen,ZhangyangWang,Mingyuan
Zhou,etal.In-contextlearningunlockedfordiffusionmodels.AdvancesinNeuralInformationProcessing
Systems,36:8542–8562,2023.
[49] XintianWu,HuanyuWang,YimingWu,andXiLi. D3t-gan:Data-dependentdomaintransfergansfor
few-shotimagegeneration. arXivpreprintarXiv:2205.06032,2022.
[50] JianxiongXiao,JamesHays,KristaA.Ehinger,AudeOliva,andAntonioTorralba. Sundatabase:Large-
scalescenerecognitionfromabbeytozoo. In2010IEEEComputerSocietyConferenceonComputer
VisionandPatternRecognition,pages3485–3492,2010.
[51] JiayuXiao,LiangLi,ChaofeiWang,Zheng-JunZha,andQingmingHuang. Fewshotgenerativemodel
adaptionviarelaxedspatialstructuralalignment.InProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition,pages11204–11213,2022.
[52] XingqianXu,JiayiGuo,ZhangyangWang,GaoHuang,IrfanEssa,andHumphreyShi. Prompt-free
diffusion:Taking"text"outoftext-to-imagediffusionmodels. arXivpreprintarXiv:2305.16223,2023.
[53] CeyuanYang,YujunShen,ZhiyiZhang,YinghaoXu,JiapengZhu,ZhirongWu,andBoleiZhou.One-shot
generativedomainadaptation. InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pages7733–7742,2023.
[54] FisherYu,AriSeff,YindaZhang,ShuranSong,ThomasFunkhouser,andJianxiongXiao. Lsun: Con-
structionofalarge-scaleimagedatasetusingdeeplearningwithhumansintheloop. arXivpreprint
arXiv:1506.03365,2015.
[55] XiaohuaZhai,BasilMustafa,AlexanderKolesnikov,andLucasBeyer. Sigmoidlossforlanguageimage
pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
11975–11986,2023.
12[56] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-imagediffusion
models. InIEEE/CVFInternationalConferenceonComputerVision,ICCV2023,Paris,France,October
1-6,2023,pages3813–3824.IEEE,2023.
[57] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InProceedingsoftheIEEEconferenceoncomputer
visionandpatternrecognition,pages586–595,2018.
[58] MiaoyunZhao, YulaiCong, andLawrenceCarin. Onleveragingpretrainedgansforgenerationwith
limiteddata. InInternationalConferenceonMachineLearning,pages11340–11351.PMLR,2020.
[59] YunqingZhao,KeshigeyanChandrasegaran,MiladAbdollahzadeh,andNgai-ManManCheung. Few-shot
imagegenerationviaadaptation-awarekernelmodulation. AdvancesinNeuralInformationProcessing
Systems,35:19427–19440,2022.
[60] YunqingZhao,HenghuiDing,HoujingHuang,andNgai-ManCheung. Acloserlookatfew-shotimage
generation.InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages
9140–9150,2022.
[61] Jingyuan Zhu, Huimin Ma, Jiansheng Chen, and Jian Yuan. Few-shot image generation via masked
discrimination. arXivpreprintarXiv:2210.15194,2022.
13A BroaderImpactStatement
Thispaperintroducesamodelcapableofgeneratingimagesthatreflectthesemanticinformationof
agiveninputimage. Potentialpositiveapplicationsincludebeingacreativeaid,augmentationof
animageretrievalsystemorenhancingthesizeofanimagedataset. Whilethemodellacksdirect
controllabilitythusmitigatingsomenegativeapplicationslikecreationofpersonalizeddeepfakes,our
modelcanstillgeneratedeepfakes. Therefore,weadvocateforresponsibleusageforourmodel.
B StandardErrors
Ingeneral,thevarianceinFIDissmall. Wecomputethevarianceacrossdifferentrandomseeds
anddifferentrandomlysampledoneshotdatasetsonImageNet. Withafixedone-shotdatasetand
fivedifferentrandomseeds,wereportaFIDof9.4±0.02. Wealsosamplefivedifferentone-shot
datasetsandreportaFIDof9.5±0.06.
C ImageNetSamples
Figure6: Set1: ConditioningImagesfromImageNet
14Figure7: Set1: RandomSamples1
D LSUNBedroomSamples
E SUN397Samples
F LSUNChurchSamples
15Figure8: Set1: RandomSamples2
16Figure9: Set1: RandomSamples3
17Figure10: Set2: ConditioningImagesfromImageNet
18Figure11: Set2: RandomSamples1
19Figure12: Set2: RandomSamples2
20Figure13: Set2: RandomSamples3
21Figure14: Set3: ConditioningImagesfromImageNet
22Figure15: Set3: RandomSamples1
23Figure16: Set3: RandomSamples2
24Figure17: Set3: RandomSamples3
25Figure18: ConditioningImagesfromLSUNBedroom
26Figure19: RandomSamples1
27Figure20: RandomSamples2
28Figure21: RandomSamples3
29Figure22: ConditioningImagesfromSUN397
30Figure23: RandomSamples1
31Figure24: RandomSamples2
32Figure25: RandomSamples3
33Figure26: Set1: ConditioningImagesfromLSUNChurches
34Figure27: RandomSamples1
35Figure28: RandomSamples2
36Figure29: RandomSamples3
37