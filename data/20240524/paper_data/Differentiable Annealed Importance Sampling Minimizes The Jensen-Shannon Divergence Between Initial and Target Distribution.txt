Differentiable Annealed Importance Sampling Minimizes The
Jensen-Shannon Divergence Between Initial and Target Distribution
JohannesZenn123 RobertBamler12
Abstract dynamicstomovethesamplestowardsatargetdistribution.
Differentiable annealed importance sampling Weshowtheoreticallythat,inthelimitofmanytransitions,
(DAIS), proposed by Geffner & Domke (2021) DAISminimizesthesymmetrizedKullback-Leibler(KL)di-
andZhangetal.(2021),allowsoptimizing,among vergence(Kullback&Leibler,1951),alsoknownasJensen-
others,overtheinitialdistributionofAIS.Inthis Shannon divergence, between its initial and target distri-
paper,weshowthat,inthelimitofmanytransi- bution. TheJensen-Shannondivergenceisthesumofthe
tions, DAISminimizesthesymmetrizedKLdi- reverseandtheforwardKLdivergence. Whilethereverse
vergence(Jensen-Shannondivergence)between KLdivergence,usedinVI,isknowntobemode-seeking,
theinitialandtargetdistribution. Thus,DAIScan theforwardKLdivergence,usedinMarkovianScoreClimb-
be seen as a form of variational inference (VI) ing(MSC)(Naessethetal.,2020),istypicallyassociated
inthatitsinitialdistributionisaparametricfitto withamass-coveringbehavior(Murphy,2023). TheJensen-
anintractabletargetdistribution. Weempirically Shannondivergenceaveragesbetweenbothdivergences.
evaluatetheusefulnessoftheinitialdistribution
Weempiricallyanalyzeimplicationsofthistheoreticalresult
asavariationaldistributiononsyntheticandreal-
byasking:“Howusefulistheinitialdistributionq ofDAIS
0
worlddata,observingthatitoftenprovidesmore
asaparametricapproximationofthetargetdistribution?”
accurateuncertaintyestimatesthanstandardVI
Thisquestioncorrespondstoaninferencescheme(whichwe
(optimizing the reverse KL divergence), impor-
denotebyDAIS )thatisidenticaltoDAISattrainingtime
0 tanceweightedVI,andMarkovianscoreclimbing
(i.e.,itmovesparticlessampledfromq alonganMCMC
0
(optimizingtheforwardKLdivergence).
chain). At inference time, however, DAIS only uses q ,
0 0
omitting expensive AIS steps. Here, we refer to fitting
themodelparametersas“training”andrunningthemodel
1.Introduction
on test data as “inference”. Generally, we do not expect
DAIS tomatchtheperformanceofDAIS.However,DAIS
Annealedimportancesampling(AIS)(Neal,2001)allows 0 0
(cid:82) has the advantage of providing an explicit and compact
estimating the normalization constant Z := f(z)dz of
representationoftheapproximatetargetdistribution.
an unnormalized distribution f(z). In probabilistic ma-
chinelearning,AIScanbeusedforBayesianmodelselec- Wecalladistributionexplicitifithasananalyticexpression
tion (Knuth et al., 2015), where the goal is to maximize (as opposed to, e.g., an algorithmic prescription for sam-
(cid:82)
themarginallikelihoodp(x)= p(z,x)dzoverafamily plingfromit). Itisoftenstatedthatanexplicitexpression
of candidate probabilistic models p. Here, p(z,x) is the fortheexactBayesianposteriorp(z|x)=p(z,x)/p(x)is
model’sjointdistributionoverlatentvariableszand(fixed) intractable in large models, since calculating p(x) is pro-
observed data x. In this paper, we investigate the initial hibitivelyexpensive. However,thisadagemissesanimpor-
distributionofAISor,morespecifically,differentiableAIS tantpoint. Evenifwehadanoraclethattoldusthevalueof
(DAIS)(Geffner&Domke,2021;Zhangetal.,2021).DAIS p(x),theexplicitexpressionfortheexactposteriorwould
combinesaspectsofMarkovChainMonteCarlo(MCMC) typicallybefartoocomplicatedtobeofanypracticaluse
and Variational Inference (VI): it draws samples from a indownstreamtasksbecauseitwould,ingeneral,haveone
(variational)initialdistributionq 0andthenfollowsMCMC termperdatapoint. Wearguethatweareactuallyinterested
inacompactapproximationoftheposteriordistribution,i.e.,
1Tu¨bingen AI Center 2University of Tu¨bingen 3IMPRS-
IS. Correspondence to: Johannes Zenn <johannes.zenn@uni- atractableparameterizeddistributionthatonecanefficiently
tuebingen.de>. evaluateandsamplefrom. Havingacompactapproximate
posteriorenablesvariousdownstreamapplications,suchas
Proceedings of the 41st International Conference on Machine
continuallearning(Nguyenetal.,2018),pruninginBNNs
Learning,Vienna,Austria.PMLR235,2024.Copyright2024by
(Xiao et al., 2023), and compression of neural networks
theauthor(s).
1
4202
yaM
32
]LM.tats[
1v04841.5042:viXraDifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
(Tan&Bamler,2022)orotherdata(Yangetal.,2020).
Starting from our theoretical contribution—showing that
DAISminimizestheJensen-Shannondivergencebetween
q andf/Z—weprovideanempiricalanalysisofDAIS
0 0
forapproximateBayesianinference. WecompareDAIS ,
0
DAIS,VI,importanceweightedVI(IWVI),andMSC.We
findthatDAIS oftenprovidesmoreaccurateuncertainty
0
estimates than VI, IWVI, and MSC on Gaussian process
regressionandreal-worlddatasetsforlogisticregression.
2.RelatedWork
In the following, we discuss related work on variational
distributionsthatareaugmentedbyMCMCtransitions,the
forwardandreverseKLdivergence,andDAIS.Wecover Figure1. Thelandscapespannedbyvariouslowerboundstothe
(cid:82)
relatedworkonmethodstoestimatenormalizationconstants normalizationconstantZ = f(z)dzwhereN denotesnumber
ofparticlesandKthenumberofimportancesamplingtransitions.
inSection3. Relatedworkonboundswithrespecttothe
AdiscussionandfurtherdetailscanbefoundinSection3.
Jensen-ShannondivergenceisgiveninSection4.
MCMC-AugmentedVariationalDistributions. Sequen- transitions. Theyminimizeacontrastivedivergencewhich,
tialMonteCarlosamplers(SMCS)(DelMoraletal.,2006) inthelimit,convergestotheJensen-Shannondivergence.
are methods derived from prarticle filters (Doucet et al., Markovianscoreclimbing(MSC)(Naessethetal.,2020),
2001) to estimate normalization constants. While SMCS thatwealsocomparetoinSection5,optimizestheforward
andannealedimportancesampling(AIS)bothdescribethe KLdivergenceusingunbiasedstochasticgradients. MSC
samemathematicalframework,SMCStypicallyintegrate samplesaMarkovchainandusesthesamplestofollowthe
aresamplingstepthatletsthemodelfocuson“important” scoreofthevariationaldistribution. TheMarkovkernelfor
particles. AIScanbeseenasafinite-differenceapproxima- theMCMCdynamicsisbasedonimportancesampling.
tiontothermodynamicintegration(TI)(Gelman&Meng,
1998)whichcomputesratiosofnormalizationconstantsby
DifferentiableAnnealedImportanceSampling(DAIS).
solving a one-dimensional integration problem. Masrani
Geffner & Domke (2021) and Zhang et al. (2021) pro-
etal.(2019)connectTIandvariationalinference(VI)re-
pose DAIS concurrently focusing on different aspects of
sulting in tighter variational bounds by using importance
themodel(empiricalresultsandaconvergenceanalysisfor
samplingtocomputetheintegral. Thinetal.(2021)propose
linearregression,respectively). Doucetetal.(2022)iden-
avariantofthealgorithmbasedonsequentialimportance
tifythatthebackwardtransitionsofAISareconveniently
sampling. Hamiltonianvariationalinferencecombinesvari-
chosenbutsuboptimal. TheyproposeMonteCarlodiffu-
ational inference and MCMC iterations differentiably by
sion(MCD)whichparameterizesthetimereversalofthe
augmentingthevariationaldistributionwithMCMCsteps
forwarddynamicsandwhichcanbelearnedbymaximizing
(Salimans et al., 2015; Wolf et al., 2016). The Hamilto-
alowerboundonthemarginallikelihood,orequivalently,
nian VAE (Caterini et al., 2018) builds on Hamiltonian
adenoisingscorematchingloss. Geffner&Domke(2023)
ImportanceSampling(Neal,2005)andimprovesonHVI
provideamoregeneralframeworkforMCDandinvestigate
byusingoptimallychosenreverseMCMCkernels. DAIS ,
0 variousdynamicsandnumericalsimulationschemes. Zenn
investigated here, uses an MCMC-augmented variational
&Bamler(2023)extendDAIStoasequentialMonteCarlo
distributionduringtrainingbutnotduringinference.
samplerandprovideatheoreticalargumentforleavingout
thegradientscorrespondingtotheresamplingoperation.
VIWithForwardandReverseKLDivergence. Black
boxVI(Ranganathetal.,2014)istypicallyassociatedwith 3.EstimatingNormalizationConstants
thereverseKLdivergence(betweenthevariationaldistri-
butionandtherealposteriordistribution). However, also Inthissection,wediscusshowvariationalinference(VI),
variousotherdivergenceshavebeeninvestigatedtoobtain importanceweightedVI(IWVI),and(differentiable)AIS
anapproximationoftheposteriordistribution(Hernandez- ((D)AIS)canbeunderstoodasapproachestoreducingthe
Lobatoetal.,2016;Li&Turner,2016;Diengetal.,2017; varianceofimportancesampling(IS).Figure1summarizes
Wanetal.,2020). MostrelatedtoDAIS ,Ruiz&Titsias thespacespannedbythesemethods,highlightsalimiting
0
(2019)refinethevariationaldistributionbyrunningMCMC behavior (Domke & Sheldon, 2018), and shows how our
2DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
theoreticalresult(Theorem4.1)fitsintothisunifyingframe- 3.2.ImportanceWeightedVariationalInference
work. Section3.1andSection3.2giveanoverviewover
Importanceweightedvariationalinference(IWVI)(Burda
(IW)VI.Section3.3thenintroducesournotationfor(D)AIS.
etal.,2016;Domke&Sheldon,2018)reducesthevariance
oftheimportanceweightw(z)byaveragingN independent
Importance Sampling is a principal technique for es-
samplesfromq,whereN ≥2,i.e.,itusestheweights
timating the integral over a (nonnegative) function f by
samplingfroman (cid:90)ormalizedproposal (cid:20)d fis (t zri )b (cid:21)utionq, w IN WVI(cid:0) z(1:N)(cid:1) := N1 (cid:88)N f q(( zz (( ii )) )) (3)
Z := f(z)dz =E z∼q q(z) , (1) i=1
wherez(i) ∼qforalli. Theresultingbound,
whereweassumethatthesupportofqcontainsthesupport
off. Weneedtobeabletoefficientlydrawsamples(orpar- logZ ≥E (cid:104) log(cid:16) wN (cid:0) z(1:N)(cid:1)(cid:17)(cid:105)
z(1:N)∼q IWVI
ticles)zfromq,andtoevaluatef(z)andq(z)forthesesam- (4)
=:ELBON (f,q),
ples.Howmanysamplesarenecessarytoobtainanaccurate IWVI
estimateofZ dependsonhowwellq approximatesf/Z. recovers ELBO (f,q) for N = 1 (see Equation (2) and
VI
Inhighdimensions,themismatchbetweenf/Z andany redhighlightsonthexaxisofFigure1).Samplingfromqat
simpleproposaldistributionq grows,whichleadstohigh inferencetimerequiresasampling-importance-resampling
variance of the importance weight w(z) := f(z)/q(z), (SIR)procedure(Cremeretal.,2017)whichwedenoteby
andthenumberofsampleshastogrowexponentiallyinthe IWVI . The corresponding Markov kernel is known as
SIR
dimension(Agapiouetal.,2017). WenowdiscusshowVI, i-SIRandstudiedindetailbyAndrieuetal.(2018). AsN
IWVI,andAISallaimtoreducethisvariance. grows,theboundbecomestighter. InthelimitofN →∞,
Domke & Sheldon (2018) find (based on results due to
3.1.VariationalInference Maddisonetal.(2017,Proposition1))thefollowing.
Variationalinference(VI)appliesthelogarithmtotheimpor- Theorem 3.1 (Theorem 3 in Domke & Sheldon (2018)).
tanceweightsw(z) := f(z)/q(z)inEquation(1). This For large N, the gap ∆N IWVI := logZ − ELBON IWVI
of importance weighted VI is proportional to the vari-
reducesexponentiallygrowingvariancestolinearlygrowing
ones,butitintroducesbias,resultinginalowerboundon
ance of w IN WVI, defined in Equation (3). Formally, if
logZ byJensen’sinequality(Bleietal.,2017), l αim >su 0p sN uc→ h∞ thaE tq E[1/
(cid:2)
|w wIN NWVI] −< log∞ Z|a 2n +d α(cid:3)th <er ∞e e ,x ti hs et ns some
(cid:20) (cid:18) f(z)(cid:19)(cid:21) q IWVI
logZ ≥E z∼q log q(z) =:ELBO VI(f,q). (2)
lim N∆N =
Var q(cid:2) w IN WVI(cid:0) z(1:N)(cid:1)(cid:3)
. (5)
N→∞ IWVI 2Z2
Equation (2) is called the evidence lower bound (ELBO)
sinceVIisoftenusedtoestimatetheevidencelogp(x)= Proof. SeeDomke&Sheldon(2018,Theorem3).
(cid:82)
log p(z,x)dz of a probabilistic model p(z,x) (latent
variableszandobserveddatax). VImaximizestheELBO Thus,forlargeN,ahighervarianceofwN corresponds
IWVI
overparametersofq,leadingtothebestapproximationof to a larger gap (see Section 3.1). Annealed importance
logp(x),whichisusefulforapproximateBayesianmodel sampling,discussednext,providesawaytofurtherreduce
selection(Beal&Ghahramani,2003;Kingma&Welling, thesamplingvarianceforafixednumberofparticlesN.
2014). In addition, VI is a popular method for approxi-
mate Bayesian inference as the distribution q that maxi- 3.3.(Differentiable)AnnealedImportanceSampling
mizes the ELBO also approximates the (intractable) pos-
AnnealedimportanceSampling(AIS)reducesthevariance
teriorp(z|x)=p(z,x)/Z. Thisisbecausemaximizing
of wN (z(1:N)) further by recalling that the variance of
theELBOoverparametersofqisequivalenttominimizing IWVI
theimportanceweightsf(z(i))/q(z(i))inEquation(3)isa
thegap∆ := logZ −ELBO (f,q) ≥ 0, whichturns
VI VI
consequenceofadistributionmismatchbetweenqandf/Z.
outtobetheKLdivergencefromthetrueposteriortoq,i.e.,
Toreducethedistributionmismatch, AISinterpolatesbe-
∆ =D (q(z)∥p(z|x))(Bleietal.,2017).
VI KL tween q and f/Z with distributions {π (z )}K over
βk k k=0
As we discuss next, IWVI and AIS can both be seen as auxiliaryvariablesz ,...,z forK interpolationsteps.
0 K
methodsthatreducethegapofVIbyreducingthesampling
In detail, AIS estimates the normalization constant of an
variance. AISistypicallyconsideredasamplingmethod,
unnormalizeddistributionf overallz ,...,z ,
prioritizingthequalityofsamplesoranaccurateestimate AIS 0 K
ofZoveragoodapproximateposteriordistributionq,which K
(cid:89)
wouldbetheperspectiveofVI.InSection4, weanalyze f AIS(z 0:K):=f(z K) B k(z k−1|z k). (6)
(differentiable)AISfromtheperspectiveofVI. k=1
3DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
Here,theso-calledbackwardtransitionkernelsB haveto (DAIS) drops this MH step. The resulting “uncorrected”
k
benormalizedintheirfirstargumentsothatf andf have transitionkernelsF donotleaveπ invariant. Instead,the
(cid:82)
AIS k βk
the same normalization constant, f (z )dz = backwardkernelsB aredefinedbystartingfromanexact
AIS 0:K 0:K k
(cid:82)
f(z )dz = Z. Originally, Neal (2001) estimates Z sampleandreversingtheforwardchain. Withthesemodifi-
K K
withIS,usingaproposaldistributionoftheform cations,theresultinglowerboundELBON,K(f ,q )
DAIS DAIS DAIS
has the same functional form as ELBON,K(f ,q ),
K AIS AIS AIS
(cid:89) where only the kernels F and B differ. Furthermore,
q (z ):=q (z ) F (z |z ), (7) k k
AIS 0:K 0 0 k k k−1 itcanbeoptimizedwithreparameterizationgradients.
k=1
Theorem3.1(Domke&Sheldon,2018, Theorem3)also
wherewecallq theinitialdistributionandF aforward
0 k appliestothisbound.Therefore,forlargeN,thegapofAIS
transitionkernel.Insteadofusingq AISwithIStoestimateZ,
is proportional to the variance of
wN,K
, which, for good
wecanuseq alsowithIWVItoobtainaboundonZ, AIS
AIS choicesoftheforwardandbackwardkernelsF andB ,is
k k
logZ ≥E (cid:104) log(cid:16) w (cid:0) z(1:N)(cid:1)(cid:17)(cid:105) smallerthanthevarianceoftheestimatorofIWVI.
z 0(1 :K:N)∼q AIS AIS 0:K (8)
=:ELBON,K(f ,q ) 4.AnalyzingtheInitialDistributionofDAIS
AIS AIS AIS
withtheannealedimportanceweights In Theorem 4.1 of this section we present our main the-
oretical result that DAIS minimizes the Jensen-Shannon
wN,K(cid:0) z(1:N)(cid:1)
=
1 (cid:88)N f(z K(i)) (cid:89)K B k(z k(i −) 1|z k(i))
. (9)
divergencebetweenitsinitialdistributionanditstargetdis-
AIS 0:K N q (z(i)) F (z(i)|z(i) ) tribution.Then,wediscusscompactrepresentationsinterms
i=1 0 0 k=1 k k k−1 oftheirsamplingcomplexity. Finally,weframefindingthe
initialdistributionofDAISasaformofVI.
Equation (8) and Equation (9) hold for arbitrary normal-
izedforwardandbackwardkernelsF k andB k (aslongas 4.1.DAISMinimizestheJensen-ShannonDivergence
support(q )⊇support(f )). Inpractice,however,we
AIS AIS
wanttodrawsamplesz(i)
fromdistributionsthatinterpolate
Theorem 4.1 below presents our main theoretical contri-
k
butionshowingthatDAISminimizestheJensen-Shannon
smoothly between q (z ) and f(z ) so that none of the
0 0 K
divergence between its initialand target distribution. We
factorsontheright-handsideofEquation(9)hasalarge
seeTheorem4.1asstartingpointtomotivateDAIS and
variance. One typically realizes each F (z |z ) as a 0
k k k−1
theanalysisoftheshapeofthefittedinitialdistributionq .
MarkovChainMonteCarloprocess(typicallyHamiltonian 0
The result also holds true for AIS, but it is most relevant
MonteCarlo(HMC)),whichleavesadistributionπ (z )
invariant,wherethedistributions{π }K interpolβ ak tebk e- forDAIS,whichlearnsparametersofitsinitialdistribution.
tween π := q and π := f/Zβ .k Tk h= e0 most common One can also show Theorem 4.1 by combining results of
β0 0 βK
Grosse et al. (2013) and Crooks (2007) from the physics
choiceofπ usesthegeometricmean(akaannealing),i.e.,
βk
literature. Brekelmans et al. (2022) state a related bound
γ (z)
(cid:18) f(z)(cid:19)βk onthedifferencebetweenanupperandalowerboundon
π βk(z):= β Zk with γ βk(z):=q 0(z)
q (z)
(10) theevidence(whereasTheorem4.1makesanasymptotic
βk 0
statementaboutthegapbetweenlowerboundandevidence).
where Z
βk
= (cid:82) γ βk(z)dz and 0 = β
0
< β
1
< Theorem 4.1. Let support(q 0) ⊃ support(f). We as-
··· < β = 1. To ensure that Equation (9) can sume that the transitions between consecutive annealing
K
be evaluated for samples z 1:K ∼ q AIS, one typically distributions are perfect and that β k are equally spaced,
sets B k to the reversal of F k, i.e., B k(z k−1|z k) := i.e., β k = k/K. Then, for large K and N = 1, the gap
F(z k|z k−1)γ βk(z k−1)/γ βk(z k), whichisproperlynor- ∆1 A, IK
S
:= logZ − ELBO1 A, IK S(f AIS,q AIS) is a divergence
malized. Withthischoice,Equation(9)simplifiesto betweentheinitialdistributionq 0andthetargetdistribution
f/Z. Up to corrections of O(1/K3), the divergence is
wN,K(cid:0) z(1:N)(cid:1)
=
1 (cid:88)N (cid:89)K γ βk(cid:0) z k(i −) 1(cid:1)
. (11)
proportionaltotheJensen-Shannondivergence,
AIS 0:K N γ (cid:0) z(i) (cid:1) 1 (cid:18) 1
i=1k=1 βk−1 k−1 ∆1 A, IK
S
=
K
2D KL(f(z)/Z∥q 0(z))
(cid:19)
DifferentiableAnnealedImportanceSampling. Toen- 1 (12)
+ D (q (z)∥f(z)/Z)
surethatF (z |z )leavesπ invariant,theHMCim- 2 KL 0
k k k−1 βk
plementationofF
k
involvesaMetropolis-Hastings(MH) +O(cid:0) 1/K3(cid:1)
.
acceptanceorrejectionstep,whichmakesthemethodnon-
differentiable. Differentiableannealedimportancesampling Proof. WeprovethetheoreminAppendixA.
4DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
StatementforN >1. ForageneralN >1wefindthat Limitations. We show Theorem 4.1 for large K which
∆N,K ≤∆1,K
andtherefore mightbeprohibitiveinrealworldexperiments.Additionally,
AIS AIS
wegenerallyusemorethanN =1particlesinDAIS.There-
lim ∆N,K ≤ 1 D (q ,f/Z)+O(cid:0) 1/K3(cid:1) , (13) fore,weexpectthatTheorem4.1holdsonlyapproximately
K→∞ AIS K JS 0 inpractice. ForlargeN,Theorem3.1(Domke&Sheldon,
where D denotes the Jensen-Shannon divergence. This 2018, Theorem 3) holds and the gap closes. As a result,
JS
showsthattheboundforN >1canbeupper-boundedby we see a combination of effects in our experiments with
theJensen-Shannondivergencebutdoesnotgiveadditional practical(moderate)valuesforK andN (seeSection5).
insightson,e.g.,symmetry. ForN =1theinequalityisan
equality. SeeAppendixAforadiscussioningreaterdetail. 4.2.CompactRepresentationoftheInitialDistribution
TheinitialdistributionofDAIS,q ,providesbothananalyt-
0
ReverseKLDivergence. VIisknowntounderestimate
icalexpression(i.e.,itisexplicit)andallowsfortractable
uncertainties(Bleietal.,2017)whichisduetoanasymme-
samplingandevaluation(i.e.,itiscompact). Inthefollow-
tryintheELBO.Morespecifically,ELBO (Equation(2))
VI ing,wediscusspositiveimplicationsofthisrepresentation.
minimizes the reverse KL divergence D (q∥f/Z),
KL
which takes the expectation over q. Therefore, penalties Importancesamplingislikelytosufferfrominefficiencies
forregionsinz-spacewhereq(z)>f(z)/Z areweighted duetothesamplingcomplexityinhighdimensions.Namely,
higher thanpenalties forregionswhere q(z) < f(z)/Z. iff becomesincreasinglycomplicated, andthusthemis-
Asaresult,ELBO ismode-seekingandtendstounderes- matchbetweenf/Z andtheproposaldistributionqgrows,
VI
timatethetrueposteriorvariance. ForasmallN,IWVIalso thevarianceoftheimportanceweightgrowsexponentially
showsamode-seekingbehavior,whichcanbeovercomeby initsdimension(Bamleretal.,2017). Agapiouetal.(2017)
increasingN. Theorem4.1statesthat,atleastforK →∞ showthatalsothenumberofsamplesgrowsexponentially
andN =1,ELBON,K doesnotsufferfromsuchanasym- inthedimensionoftheproblem. Furthermore,Chatterjee
DAIS
metry. WeinvestigatethesituationforK <∞andN ≥1 &Diaconis (2018)showthat thesample sizeis exponen-
empiricallyinvariousexperimentsinSection5. tialintheKLdivergencebetweenthetwomeasures(here:
D (f/Z∥q))iftheyarenearlysingularwithrespectto
KL
Forward KL Divergence. In contrast, the forward KL eachother(whichisoftenthecaseinpractice).
divergenceD (f/Z∥q)ismass-coveringsincetheex-
KL As we discuss in Section 3 above, DAIS can be seen as
pectationistakenoverf/Z andregionsinz-spacewhere
importance sampling on an augmented space. Therefore,
q(z) < f(z)/Z are weighted higher than the regions
DAIScanalsosufferfromhighsamplingcomplexityinhigh
whereq(z)>f(z)/Z. WhiletheforwardKLdivergence
dimensions. WhileweshouldexpectsamplesfromDAISto
islesslikelytounderestimateposteriorvariances,itoften
technicallyfollowthetargetdistributionmorefaithfullythan
performspoorlyinmoderatetohighdimensionswhenpos-
samplesfromtheinitialdistributionq ,obtainingaccurate
0
teriorcorrelationsincrease(Dhakaetal.,2021). Section5
estimatesofsummarystatistics(e.g., meanandvariance)
provides empirical evidence that DAIS (implicitly mini-
0 of the target distribution from DAIS samples can be pro-
mizingtheJensen-Shannondivergence)indeedoutperforms
hibitivelyexpensiveinhighdimensions. Bycontrast,q is
0
MSC(minimizingtheforwardKLdivergence).
typicallyparameterizedbyinterpretablesummarystatistics
thatcanbereadoffwithoutrequiringempiricalestimates
overexponentiallymanyexpensiveMCMCsamples.
10−2 In Section 5 below, we observe how difficult it can be in
practicetoestimatesummarystatisticsfromDAISsamples.
Figure2showsthemeanabsoluteerror(MAE)oftheesti-
matedmeanandstandarddeviationfortheGaussianprocess
MAEofmeanforDAIS experiment(RBF ,d=25inSection5.2)asafunctionof
1
MAEofstd.forDAIS
MAEofmeanforDAIS0 the number of DAIS samples used for the estimate (red
10−3 MAEofstd.forDAIS0 curves). Notethesuddenjumpsoftheestimator. Wecom-
paretothemeanandstandarddeviationsreadoffdirectly
105 106 107 fromtheinitialdistributionq 0(horizontalbluelines).
numberofsamples
4.3.TheInitialDistributionofDAISforInference
Figure2. Mean absolute error of estimated mean and standard
deviationasafunctionofthenumberofDAISsamplesusedfor In the following, we investigate the initial distribution q 0
theestimate.Theestimatorconvergespoorly(seeSection4.2). of DAIS as a candidate for an approximate posterior dis-
5
)EAM(rorreetulosbanaemDifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
N=1 N=2 N=4 N=8 N=16
M MS SC C 18 1 6c c c-- s-- c-- c-- s-- s-- s-- s-- s-- s-- s-- s-- s-- s-- s-- ccc ccc cc sccc cc s sss sss sssu ssu ssc ss sssu ss sss sss ccc ccc ccc ccc cc scc s css sssu ssu ssu ssc ssu ss sssu ss ccc ccc ccc ccc ccc ccc cssu cs cc su csu ssu ssc ssu ss sss ccc ccc ccc ccc ccc ccc cc s css css css cc s css cssu ss sss −5
8 ccsssssssssssss cccccssssssssss cccccccssssssss ccccccccsssssss cccccccccccssss
4 cccssssssssssss ccccsssssssssss cccccssssssssss cccccccssssssss cccccccccssssss
2 cssssssssssssss ccsssssssssssss cccssssssssssss ccccsssssssssss cccccssssssssss
(IW)VI cssssssssssssss cssssssssssssss ccsssssssssssss cccssssssssssss ccccsssssssssss
1 3 5 7 9 11 13 15 1 3 5 7 9 11 13 15 1 3 5 7 9 11 13 15 1 3 5 7 9 11 13 15 1 3 5 7 9 11 13 15 60
−
d d d d d
Figure3. DensityofvariationaldistributionsofVI,IWVI,MSC,andDAIS0(K)evaluatedonsamplesfromad-dimensionalbimodal
Gaussiantargetdistribution. “-”: unabletofindanoptimum,“c”: mass-coveringdistribution,“s”: mode-seekingdistribution,“u”:
undecidablewhether“c”or“s”.DAIS0achieveshigherdensitiesinhigherdimensiondforincreasingKacrossallconsideredN.MSC
doesnotconvergeforN =1.MSC1clearnsvariationaldistributionsthatarelessmass-coveringforlargerdthanDAIS0(16).MSC8c
achievessometimeshigherdensitiesinhigherdimensiondbutperformsinconsistentacrossN.ResultsarediscussedinSection5.1.
tribution. We denote this scheme as DAIS : At training parallelchainsasMSC andresultswith1chainasMSC .
0 8c 1c
N,K
time,DAIS mirrorsDAISandmaximizesELBO . At
0 DAIS
inferencetime,DAIS 0dropstheAISstepsandsolelyuses 5.1.BimodalTargetDistribution
q asavariationalapproximationtothetargetdistribution.
0
Thisexperimentisdesignedtoinvestigatethebehaviorof
FollowingtheapproachbyZhangetal.(2021)weuseDAIS DAIS for N ≥ 1 and K ≥ 1. More specifically, we
0
withHMCdynamics. Thus,Equation(11)simplifiesto investigatetherelationshipbetweentheparametersN and
K alongsidethedimensiondofthevariationaldistribution
1 (cid:88)N f(z K) (cid:89)K N(v k(i);0,M)
, (14)
ofDAIS 0,andcompareittothecompactmethods(IW(VI),
N q 0(z 0) N(v(i) ;0,M) MSC).Thus,theexperimentcoverstheentirespacespanned
i=1 k=1 k−1
inFigure1. Forfurtherdetails,seeAppendixB.
whereN(v k(i);0,M)denotesthedensityofthenormaldis- Weconsiderabimodaltargetdistributionf(z)/Zthatisan
tributionatpointv(i) withcovariancematrixM andmean equally-weightedmixtureoftwod-dimensionalGaussians
k
0. v(i) and M are the momenta and the (diagonal) mass withmeans(0,...,0)and(1,...,1),respectively,andco-
matrk ixofHMC,respectively. Theinitialdistributionq is variancematrices0.252I. Welearnthevariationaldistribu-
0
afullyfactorizednormaldistribution. Weusegradientup- tionsofVI,IWVI,DAIS 0,andMSC(both,with1chainand
datestolearnthemeansandvariancesofq ,theannealing 8chains). Toevaluatethelearnedvariationaldistributions,
0
schedule(β ,...,β ),andthestepwidthofthesampler. we draw 1,000 samples from the target distribution and
1 K−1
computethelogdensityunderq, i.e., E [log(q(z))].
z∼f/Z
AshighlightedinSection4.2,wedonotexpectthattheini- In Figure 3 we plot results for N ∈ {1,2,4,8,16} parti-
tialdistributionofDAIS(DAIS 0)generallyoutperformsthe cles and K ∈ {2,4,8,16} transitions in d ∈ {1,...,15}
DAISmethod. However,wehighlightthatDAIS provides
0 dimensions. Figure4showsthedensityalongthelinefrom
variousdesirablepropertiesduetoitscompactrepresenta- (0,...,0)to(1,...,1)forN =1andd=3.
tion. Additionally, DAIS ismuchmorecomputationally
0
efficientatinferencetimecomparedtoDAIS.Incomparison
toVI,DAIS isexpectedbelessmode-seeking(Section4.1),
0
target
andweposethattheJensen-Shannondivergenceislesssus-
0.8 VI
ceptibletoproblemsoptimizingtheforwardKLdivergence. DAIS0(2)
Section5providesexperimentalevidence. DAIS0(4)
0.6 DAIS0(16)
Jensen-Shannon
5.Experiments 0.4
InSection5.1,weinvestigateDAIS qualitativelyforvar-
0 0.2
ious dimensions, N, and K on toy data by analyzing its
mode-seeking or mass-covering behavior. In Section 5.2 0.0
andSection5.3,weshowquantitativeresultsonbothgener-
2 1 0 1 2 3 4
atedandrealworlddata. Throughoutthissection,wecom- − linec− onnectingmodesofbimodaltarget(rescaled)
pareDAIS ,IWVI,MSC,IWVI ,andDAIS,wherethe
0 SIR
firstthreefindacompactrepresentationoftheapproximate Figure4. Densitiesonthediagonalbetweenthetwomodesofthe
posteriorwhilethelattertworequiresamplingatinference bimodalGaussiandistribution(sameexperimentasFigure3with
time. AsMSCisknowntohaveconvergenceissuesdueto N =1,d=3).VIcoversasinglemodewhileDAIS0coversboth
highvariance(Kimetal.,2022)wereportresultsusing8 modeswithincreasingK(detailsinSection5.1).
6
K
ytisned
])z(qgol[Z/f
zE
∼DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
2
2
1 1
0
0
1
−
−1 −2
3
2 −
−
0 2 4 6 8 10 0 2 4 6 8 10
data analyticalposteriormean analytical(diagonal)posteriorcovariance IWVI DAIS0 MSC8c
Figure5. Gaussianprocessregressionongenerateddata,usingapriorwithRBFkernelwithtwodifferentsetsofparameters(see⋆in
Table1).Weshow97.5%quantilesoftheposteriorcovarianceforanalytic(shadedgray;covariancematrixisdiagonalizedondatapoints,
seeAppendixC),IWVI(red),DAIS0(blue),andMSC8c(yellow).Learnedmeansareindistinguishablefromtheanalyticmean(black)at
thislinewidth.DAIS0oftenprovidesmoreaccurateuncertaintyestimatescomparedtotheothermethods(detailsinSection5.2).
Mode-SeekingVersusMass-Covering. InFigure4we NumberofTransitionsK. ForafixedN,wefindthat
canclearlyseethatVIandDAIS withK =2transitions withincreasingK,DAIS (q )achieveshigherlogdensi-
0 0 0
leadtoamode-seekingdistribution. ForanincreasingK tiescomparedtoVI,IWVI,andMSC,especiallyinhigher
(i.e.,K ∈{4,16}),wefindthatthevariationaldistribution dimensions. Forexample,forN =4,wefindthatq covers
0
of DAIS becomes more mass-covering and increasingly bothmodeswithmassforadimensionuptod = 7while
0
similar to the solution that (numerically) minimizes the VI and IWVI collapse to a single mode after d = 1 and
Jensen-Shannondivergence(regularlydashedline). Thisis d=2,respectively. Theseresultsalignwithourtheoretical
consistent with Theorem 4.1. We find that we can unam- findings. MSC collapsesafterd = 5whichweattribute
1c
biguouslyclassifyalmostallofthelearnedvariationaldis- totheoptimizationoftheforwardKLdivergence. MSC
8c
tributionsintobeingmode-seeking(“s”)ormass-covering performsbetterbutinconsistentlyforhigherdimensions.
(“c”)bycalculatingthedistanceofthemeanofthevaria-
tionaldistributiontobothmodesofthetargetdistribution, NumberofParticlesN. Withanincreasingnumberof
andtotheirmidpoint(AppendixBprovidesahistogram particlesN, weseethatIWVIandDAIS improve. This
0
of these distances and more details on the classification). follows the theoretical result on the number of particles
Figure3showscorrespondingclassificationsaslabels“s” (presentedinTheorem3.1(Domke&Sheldon,2018,The-
and“c”foreachcombination(N,K,d). Welabelthefew orem3))whichsuggeststochooseN aslargeaspossible.
caseswheretheclassificationisnotperfectlyunambiguous Also, MSCshowshigherdensitieswithanincreasingN;
with“u”whichisshortforfor“undecidable”. forMSC theresultsareofteninconsistentorundecidable.
8c
Comparing (IW)VI and DAIS (16) in Figure 3, we find
0
thatDAIS (16)typicallycoversbothmodesofthetarget 5.2.GaussianProcessesRegression
0
distribution for larger dimensions (across all N). We at-
We investigate a Gaussian process (GP) regression task
tributethisfindingtotheJensen-Shannondivergencethat
on synthetic data. We generate the data by (i) sampling
isimplicitlyminimizedbyDAIS . MSCdoesnotconverge
0 a ground truth process from a GP prior f ∼GP(0,k),
toreasonablevalues(“-”)forN = 1. Withasinglechain
(ii)randomlysamplingpositionsI onthedomain[0,10],
MSC outperforms(IW)VIoverallN butseemstobeless
1c and(iii)generatingdatafromaGaussianlikelihoodmodel
mass-coveringthanDAIS . With8parallelchainsMSC
0 8c y ∼N(f ,0.1·I). WesetktobeaRadialBasisFunction
I
sometimesoutperformstheothermethodsinfindingamass-
(RBF) kernel (Williams & Rasmussen, 2006) and inves-
coveringdistribution. However,wefindthatthevariational
tigate two sets of hyperparameters (RBF and RBF , see
1 2
distributionsofMSC areoftennotcenteredonthemid-
8c AppendixC).WelearnavariationaldistributionforIWVI,
pointbetweenthetwomodeswhichweattributetothehigh
MSC,andDAIS onthepositionsI.Whilethissetupmight
0
varianceofthemethod(seeAppendixB).Thisisinaccor-
soundartificialastheanalyticposteriorcanbecomputedin
dancewithKimetal.(2022)whoreporthighvariancefor
closedform(duetotheGaussianlikelihood),onecaneasily
MSCandDhakaetal.(2021)whonotethattheforwardKL
thinkofproblemswithnon-Gaussianlikelihoodsorlarge
divergenceisdifficulttooptimizeforlargedimensions.
latentspacesthatcanbephrasedinthesameframework.
7DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
N =1 N =2 N =4 N =16
1.0 HMC IWVI DAIS0 MSC8c
0.5
0 20 0 20 0 20 0 20
sortedindexof(w,b) sortedindexof(w,b) sortedindexof(w,b) sortedindexof(w,b)
Figure6. Bayesianlogisticregression:standarddeviationsoflearnedposteriorweightvector(indicesareplottedonthex-axis,wereport
±σ)forIWVI(red),MSC8c(yellow)andDAIS0(blue),comparedtoHMCsamples(dashed)onionosphere.DAIS0providesuncertainty
estimatesthatimprovewithincreasingK ∈{2,4,8,16}(shownbyincreasingopacity).DetailsinSection5.3.
Figure 5 shows two GPs that are marked with “⋆” in Ta- representation indeed helps in some cases (e.g., mean in
ble 1. We train each GP with N = 16 particles. DAIS RBF ,d = 10,25). MSC , minimizing the forward KL
0 1 8c
usesK =16transitions. Theblacklineandgrayareashow divergence, performs competitively but slightly worse in
thetheoreticallybestposteriormeanandthecorresponding comparison to the other methods. We attribute this find-
posteriorcovariancethatanyGaussianmeanfieldapproach ingtotheinstabilityoftheforwardKLdivergenceduring
canpossiblyachieve. Wecalculatetheposteriorapproxima- optimization. WeprovidealargerstudyinAppendixC.
tionbyfindingtheanalyticposterioronI,diagonalizingits
covariancematrix,andthencalculatingthepredictiveposte- 5.3.BayesianLogisticRegression
rioronthefullrange[0,10]. Byvisualinspectionwefind
WeinvestigateaBayesianlogisticregressiontaskwithfive
thattheuncertaintiesofDAIS aremoreaccuratecompared
0
real-worlddatasets(Gorman&Sejnowski,1988;Sigillito
toIWVIandMSC . MSC visuallyoutperformsIWVI.
8c 8c
etal.,1989;Kellyetal.). WemodeltheBayesianlogisticre-
Table 1 shows mean absolute errors (MAEs) for a larger gressionwithafactorizingstandardnormalprioronweights
setofGPmodels. Wecomparemodelswithcompactdis- wandbiasb. ThelikelihoodischosentobeBernoullidis-
tributions(IWVI,DAIS 0,MSC 8c)andsamplingbasedap- tributedBern(r)withparameterr = sigmoid(w⊤x+b).
proaches (DAIS using 105 and IWVI SIR using 103 ×103 Here,xdenotesadatapoint. WelearnafactorizingNormal
samples). Bestresultsarewrittenbold. Thebestmethod variationaldistributionforz =(w,b).
withcompactrepresentationiswritteninitalicfont. Within
Table2showsMAEsbetweenlearnedmeansandlearned
the methods that have compact representations, DAIS
0
standarddeviationsandmeansandstandarddeviationsof
achieves the best MAE with respect to the standard de-
HMCsamplesforvariousmethods. DAIS reachesthebest
viationinmostcases(exceptforRBF ,10whererelativeer- 0
2
errorsforthreeoutoffivedatasetswithcomparablemean
rorsareamagnitudesmalleringeneral). Ingeneral,DAIS
0
MAEandlowerMAEforthestandarddeviation. DAIS
performs competitively across all methods. Our theoreti- 0
outperformsDAISinseveralcasesbyasignificantmargin
cal result Theorem 4.1 might explain this, as the Jensen-
(e.g.,sonarandionosphere)intermsofstandarddeviation
Shannondivergence(minimizedbyDAIS )doesnotsuffer
0
whichwemainlyattributetothelargersamplingcomplexity
fromamode-seekingbehaviorbutlearnsaq thatcovers
0
ofDAIS(seealsoSection4.2). Similarly,whencomparing
moremassofthetargetdistribution. Forthemeanestima-
IWVI and thesample-based IWVI , we find that IWVI
tion, IWVI outperforms DAIS while IWVI performs SIR
0 SIR
(compact)outperformsthesamplingbasedmethodforlarger
best. ComparingDAIS andDAIS,wefindthatacompact
0
dimensionalproblems. AlthoughMSC showserrorscom-
8c
Table1. Meanabsoluteerror(comparedtoanalyticsolution)ofmeanandstandarddeviationofGPregression(N =16).DAIS0(K)
givesaccurateuncertaintyestimatesformostcases(withinallcompactmethods)whiledegradingthepredictedmeanonlynegligibly
(detailsinSection5.2).RBF denotedifferentpriorparameters.Figure5shows“⋆”visually.Resultsareaveragedover3runs.
1,2
{ }
d MAE IWVI IWVI
SIR
DAIS0(16) DAIS(16) MSC8c
RBF1 ⋆ 1 20 5 sm m st td de e. .a an n 35 34 ... . 836 5 340 7±± ± .. 01 10 0. .0 01 12 30 00 0· ·· ·1 11 10 00 0− −− −4 42 2 2 43 3. .. .8 15 63 70 2± ±±1 4. .0 0. .7 52 40 07 2· ·· ·1 11 10 00 0− −− −2 24 5 14 9 .1 0. .5. 5 37 4 87 ±±± .1. 0. 0 .3 0471 101 0· ·· ·1 11 10 00− 0−−3 −43 2 3 27 . 1 .0. 1.7 32 62 98± .± 0 .. 032 . 22 63 75 ·· ·· 11 11 00 00 −− −− 23 23 31 3 1 .. . . 77 6 1 74 6 4± ± ± .. . . 02 1 1 14 4 6 5· · · ·1 1 1 10 0 0 0− − − −2 2 2 2
± ± ± ± ±
RBF2 ⋆ 1 20 5 sm s m tt dde e ..a an n 1 2 5 4.. . . 17 5 2 47 4 1± ± ± .. . . 01 1 8 04 0 4 2· · · ·1 1 1 10 0 0 0− − − −3 3 4 2 4 19 3. .. .0 7 810 1 11± ± ±1 1 .. 0. . 30 44 90 90 ·· · ·11 1 100 00 −− −− 3 245 112 .6 3.. . 14 17 54 2 ±± ± ..1 0. 0. 9 041 1 360 0· ·· ·1 11 10 00 − 0−− 3 −33 2 14 19 .. .. 396 5 494 0 ±± ± ..1 . 00. 3 162 0 360 · ··· 1 111 0 00 0− −− −24 4 2 7 421 . ... 8 015 5 767 ± ±± 2 ... 0. 32 54 47 70 ·· · ·11 1 100 0 0−− − −23 4 2
± ± ± ± ±
8
)σ
(.dts
±DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
Table2. MeanabsoluteerrorcalculatedbetweenlearnedmeanandstandarddeviationtoHMCsamplesforBayesianlogisticregression
(N =16).DAIS0(16)outperformsothermethodson3/5datasets(detailsinSection5.3).Resultsareaveragedover3runs.
d MAE IWVI IWVI
SIR
DAIS0(16) DAIS(16) MSC8c
s ioo nn oa sr
phere
6 31
5
s sm mt td de e. .a an
n
8 47 4. .. .6 39 76 95 3±± ±. .. .0 00 08 47 99 85 8· ·· ·1 11 10 00 0− −− −2 22
2
8 88 7. .. .2 38 16 46 8±± ±. .. .0 01 17 77 41 0· ·· ·1 11 10 00 0− −− −2 22
2
48 4 3.. . .35 2 248 7 5±±
±
.. . .01 1 329 3 63· ·· ·1 1 110 0 00− − −−2 2
22
1 16 7. .. .1 02 60 63 4± ±±. .. .0 11 48 03 99 ·· ·· 11 11 00 00 −− −− 12 21 1
2
87.
.
..3 5123 546± ±±.
.
..0
0
018
5
261
0
4··
·
·11
1
100
0
0−−
−
−21
1
2
± ± ± ± ±
heartdisease 16 sm tde .an 2 2. .1 596 ± .. 02 73 2· ·1 10 0 −− 22 2 2. .63 70 ± .. 01 65 0· ·1 10 0− −22 2 1. .2 06 8±. .2 26 8· ·1 10 0− −2 2 2 1..2 12 8±. .2 23 9· ·1 10 0− −2 2 3 2. .3 87 1±. .0 13 02 ·· 11 00 −− 21
± ± ± ± ±
heartattack 14 sm tde .an 25 .. 80 05 ±. .7 15 5· ·1 10 0− −2 2 4 3. .8 111 ± .. 15 47 ·· 11 00 −− 22 4 3. .6 38 5± .. 11 16 ·· 11 00 −− 22 4 3. .6 27 8±. .0 06 97 4· ·1 100 −− 22 1 7. .7 45 1±. .0 31 51 ·· 11 00 −− 21
± ± ± ± ±
loan 12 sm tde .an 1 9. .6 27 3±. .7 66 8· ·1 10 0− −2 3 2 1. .13 48 ± .. 04 77 7· ·1 10 0− −22 2 1.0 .22 8±0 .. 00 67 09 ·· 11 00 −− 22 2 1. .0 36 2±. .2 16 3· ·1 100 −− 22 1 5.7 .06 8±.0 .0 18 69 ·· 11 00 −− 21
± ± ± ± ±
parabletotheothermethods,itfallsbehindonmostdatasets. consequences of our work, none which we feel must be
Theresultsareconsistentwiththepreviousexperimentsand specificallyhighlightedhere.
Theorem4.1. MoreresultscanbefoundinAppendixD.
SoftwareandData
Figure 6 depicts standard deviations of HMC samples
(black)andthelearnedstandarddeviationsofthecompact
Thedatasetsthatwerunexperimentsonarealleitherpub-
methodsontheionospheredataset(IWVI:red,MSC : yel-
8c
liclyavailableorcanbegeneratedbycode. Wereleasethe
low, DAIS : blue, increasing opacity corresponds to in-
0 codetorunallexperimentsathttps://github.com/
creasingK ∈ {2,4,8,16}fortraining). WeuseN = 16
jzenn/DAIS0.
particlesfortraining.WefindthatDAIS outperformsIWVI
0
andMSC forallN (seealsoTable2). ForalargerN,the
8c
Acknowledgements
differencesbetweenIWVIandDAIS arevisuallyindistinct.
0
MSC seemstooverestimatesomevariances.
8c
Johannes Zenn would like to thank Tim Z. Xiao, Marvin
Pfo¨rtner,andTristanCinquinforhelpfuldiscussions. The
6.Conclusion
authorswouldliketothanktheanonymousreviewersfor
helpfulcommentsandpointingoutrelatedworkconcern-
Inthiswork,weinvestigatetheinitialdistributionofDAIS.
ing the theoretical statement in the paper. Funded by the
We show theoretically that, for many transition steps, it
DeutscheForschungsgemeinschaft(DFG,GermanResearch
minimizes the Jensen-Shannon divergence (symmetrized
Foundation)underGermany’sExcellenceStrategy–EXC
KL divergence) to the target distribution. Therefore, by
number 2064/1 – Project number 390727645. This work
using q as an approximate posterior (a method that we
0
was supported by the German Federal Ministry of Edu-
callDAIS ),weexpectittobelesspronetounderestimat-
0
cationandResearch(BMBF):Tu¨bingenAICenter,FKZ:
ing variances (compared to VI) and easier to optimize in
01IS18039A.RobertBamleracknowledgesfundingbythe
higher-dimensionalsettings(comparedtoMSC).Addition-
GermanResearchFoundation(DFG)forproject448588364
ally,wearguethatq isanexplicitandcompactrepresen-
0
oftheEmmyNoetherProgramme. Theauthorswouldlike
tationofthetargetdistribution,whichprovidesadvantages
toacknowledgesupportofthe‘TrainingCenterMachine
oversamplingbasedmethodsforvariousdownstreamtasks.
Learning,Tu¨bingen’withgrantnumber01—S17054. The
Inexperimentsonbothsyntheticandreal-worlddata,we
authorsthanktheInternationalMaxPlanckResearchSchool
verifythatDAIS indeedoftenfulfillsourexpectationsby
0
forIntelligentSystems(IMPRS-IS)forsupportingJohannes
findingdistributionswithmoreaccuratevariancesinhigher
Zenn.
dimensions(comparedtoothercompactandsamplingbased
methods). WhileDAIS ismoreexpensivethanVIattrain-
0
References
ingtime,inferencewithDAIS isjustasexpensiveasVI
0
butitissignificantlycheaperthanDAIS.
Agapiou, S., Papaspiliopoulos, O., Sanz-Alonso, D., and
Stuart,A.M. Importancesampling: Intrinsicdimension
ImpactStatement andcomputationalcost. StatisticalScience,2017.
Thispaperpresentsworkwhosegoalistoadvancethefield
Andrieu,C.,Lee,A.,andVihola,M. Uniformergodicityof
of Machine Learning. There are many potential societal
theiteratedconditionalsmcandgeometricergodicityof
9DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
particlegibbssamplers. Bernoulli,pp.842–872,2018. Doucet,A.,DeFreitas,N.,andGordon,N. Anintroduction
to sequential monte carlo methods. Sequential Monte
Bamler, R., Zhang, C., Opper, M., andMandt, S. Pertur- Carlomethodsinpractice,2001.
bative black box variational inference. In Advances in
NeuralInformationProcessingSystems,2017. Doucet,A.,Grathwohl,W.S.,Matthews,A.G.D.G.,and
Strathmann, H. Score-based diffusion meets annealed
Beal,M.J.andGhahramani,Z. Thevariationalbayesian importancesampling. InAdvancesinNeuralInformation
em algorithm for incomplete data: with application to ProcessingSystems,2022.
scoringgraphicalmodelstructures. BayesianStatistics,
7,2003. Geffner, T. and Domke, J. Mcmc variational inference
viauncorrectedhamiltonianannealing. InAdvancesin
Blei, D. M., Kucukelbir, A., and McAuliffe, J. D. Varia- NeuralInformationProcessingSystems,2021.
tionalinference: Areviewforstatisticians. Journalofthe
AmericanStatisticalAssociation,112,2017. Geffner,T.andDomke,J. Langevindiffusionvariational
inference. InInternationalConferenceonArtificialIntel-
Brekelmans, R., Huang, S., Ghassemi, M., Steeg, G. V., ligenceandStatistics,2023.
Grosse,R.B.,andMakhzani,A. Improvingmutualin-
Gelman,A.andMeng,X.-L. Simulatingnormalizingcon-
formation estimation with annealed and energy-based
bounds. InInternationalConferenceonLearningRepre- stants: Fromimportancesamplingtobridgesamplingto
sentations,2022. pathsampling. StatisticalScience,1998.
Gorman, R. P. and Sejnowski, T. J. Analysis of hidden
Burda,Y.,Grosse,R.,andSalakhutdinov,R. Importance
unitsinalayerednetworktrainedtoclassifysonartargets.
weightedautoencoders. InInternationalConferenceon
Neuralnetworks,1,1988.
LearningRepresentations,2016.
Grosse, R. B., Maddison, C. J., and Salakhutdinov, R. R.
Caterini, A. L., Doucet, A., and Sejdinovic, D. Hamilto-
Annealingbetweendistributionsbyaveragingmoments.
nian variational auto-encoder. In Advances in Neural
InAdvancesinNeuralInformationProcessingSystems,
InformationProcessingSystems,2018.
2013.
Chatterjee,S.andDiaconis,P. Thesamplesizerequiredin
Hernandez-Lobato, J., Li, Y., Rowland, M., Bui, T.,
importancesampling. TheAnnalsofAppliedProbability,
Herna´ndez-Lobato,D.,andTurner,R. Black-boxalpha
2018.
divergenceminimization. InInternationalConferenceon
MachineLearning,2016.
Cremer, C., Morris, Q., and Duvenaud, D. Reinterpret-
ingimportance-weightedautoencoders. arXivpreprint
Kelly,M.,Longjohn,R.,andNottingham,K. Theucima-
arXiv:1704.02916,2017.
chinelearningrepository. URLhttps://archive.
ics.uci.edu.
Crooks,G.E. Measuringthermodynamiclength. Physical
ReviewLetters,99(10):100602,2007.
Kim, K., Oh, J., Gardner, J., Dieng, A. B., and Kim, H.
Markov chain score ascent: A unifying framework of
DelMoral,P.,Doucet,A.,andJasra,A. Sequentialmonte
variational inference with markovian gradients. In Ad-
carlosamplers. JournaloftheRoyalStatisticalSociety:
vancesinNeuralInformationProcessingSystems,2022.
SeriesB(StatisticalMethodology),68,2006.
Kingma,D.P.andBa,J. Adam: Amethodforstochastic
Dhaka, A. K., Catalina, A., Welandawe, M., Andersen,
optimization. InInternationalConferenceforLearning
M.R.,Huggins,J.,andVehtari,A. Challengesandoppor-
Representations,2015.
tunitiesinhighdimensionalvariationalinference. InAd-
vancesinNeuralInformationProcessingSystems,2021. Kingma,D.P.andWelling,M. Auto-encodingvariational
bayes. InInternationalConferenceonLearningRepre-
Dieng,A.B.,Tran,D.,Ranganath,R.,Paisley,J.,andBlei,
sentations,2014.
D. Variationalinferenceviaχupperboundminimization.
InAdvancesinNeuralInformationProcessingSystems, Knuth,K.H.,Habeck,M.,Malakar,N.K.,Mubeen,A.M.,
2017. andPlacek,B. Bayesianevidenceandmodelselection.
DigitalSignalProcessing,47,2015.
Domke,J.andSheldon,D.R. Importanceweightingand
variationalinference. InAdvancesinNeuralInformation Kullback,S.andLeibler,R.A. Oninformationandsuffi-
ProcessingSystems,2018. ciency. TheAnnalsofMathematicalStatistics,22,1951.
10DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
Li, Y. and Turner, R. E. Re´nyi divergence variational in- Wan, N., Li, D., and Hovakimyan, N. F-divergence vari-
ference. InAdvancesinNeuralInformationProcessing ational inference. In Advances in Neural Information
Systems,2016. ProcessingSystems,2020.
Maddison,C.J.,Lawson,J.,Tucker,G.,Heess,N.,Norouzi, Williams,C.K.andRasmussen,C.E. Gaussianprocesses
M., Mnih, A., Doucet, A., and Teh, Y. Filtering vari- formachinelearning,volume2. MITpressCambridge,
ational objectives. In Advances in Neural Information MA,2006.
ProcessingSystems,2017.
Wolf, C., Karl, M., andvan derSmagt, P. Variationalin-
Masrani,V.,Le,T.A.,andWood,F. Thethermodynamic ference with hamiltonian monte carlo. arXiv preprint
variationalobjective. InAdvancesinNeuralInformation arXiv:1609.08203,2016.
ProcessingSystems,2019.
Xiao,T.Z.,Liu,W.,andBamler,R. Acompactrepresen-
Murphy,K.P. ProbabilisticMachineLearning: Advanced tationforbayesianneuralnetworksbyremovingpermu-
Topics. MITPress,2023. tation symmetry. In UniReps: the First Workshop on
UnifyingRepresentationsinNeuralModels,2023.
Naesseth,C.,Lindsten,F.,andBlei,D. Markovianscore
climbing: Variationalinferencewithkl(p——q). InAd- Yang,Y.,Bamler,R.,andMandt,S. Variationalbayesian
vancesinNeuralInformationProcessingSystems,2020. quantization. InInternationalConferenceonMachine
Learning,2020.
Neal,R.M. Annealedimportancesampling. Statisticsand
Computing,11,2001. Zenn, J. and Bamler, R. Resampling gradients vanish in
differentiable sequentialmonte carlosamplers. In The
Neal, R.M. Hamiltonianimportancesampling. InBanff FirstTinyPapersTrackatInternationalConferenceon
InternationalResearchStation(BIRS)WorkshoponMath- LearningRepresentations,2023.
ematicalIssuesinMolecularDynamics,2005.
Zhang,G.,Hsu,K.,Li,J.,Finn,C.,andGrosse,R.B. Dif-
Nguyen,C.V.,Li,Y.,Bui,T.D.,andTurner,R.E. Varia- ferentiableannealedimportancesamplingandtheperils
tionalcontinuallearning. InInternationalConferenceon of gradient noise. In Advances in Neural Information
LearningRepresentations,2018. ProcessingSystems,2021.
Ranganath,R.,Gerrish,S.,andBlei,D. Blackboxvaria-
tionalinference. InInternationalConferenceonArtificial
IntelligenceandStatistics,2014.
Ruiz,F.andTitsias,M. Acontrastivedivergenceforcom-
biningvariationalinferenceandmcmc. InInternational
ConferenceonMachineLearning,2019.
Salimans,T.,Kingma,D.,andWelling,M. Markovchain
montecarloandvariationalinference: Bridgingthegap.
InInternationalConferenceonMachineLearning,2015.
Sigillito,V.G.,Wing,S.P.,Hutton,L.V.,andBaker,K.B.
Classificationofradarreturnsfromtheionosphereusing
neuralnetworks. JohnsHopkinsAPLTechnicalDigest,
10,1989.
Tan, Z. and Bamler, R. Post-Training Neural Network
Compression With Variational Bayesian Quantization.
InAdvancesinNeuralInformationProcessingSystems,
WorkshoponChallengesinDeployingandMonitoring
MachineLearningSystems,2022.
Thin, A., Kotelevskii, N., Doucet, A., Durmus, A.,
Moulines, E., and Panov, M. Monte carlo variational
auto-encoders. InInternationalConferenceonMachine
Learning,2021.
11DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
A.ProofoftheMainTheoreticalResult
WeproveTheorem4.1ofthemainpaper. Forthereader’sconvenience,werestatethetheorembelow.
Theorem4.1. Weassumethatsupport(q )⊃support(f),perfecttransitionsbetweentwoannealingdistributionsand
0
equallyspacedβ ,i.e.,β = k/K. Then,forlargeK andN = 1,thegap∆1,K := logZ −ELBO1,K(f ,q )isa
k k AIS AIS AIS AIS
divergencebetweentheinitialdistributionq andthetargetdistributionf/Z. UptocorrectionsoforderO(1/K3),this
0
divergenceisproportionaltotheJensen-Shannondivergence(symmetrizedKL-divergence),
(cid:18) (cid:19)
∆1 A, IK
S
= K1 1 2D KL(cid:0) f(z)/Z(cid:13) (cid:13)q 0(z)(cid:1) + 21 D KL(cid:0) q 0(z)(cid:13) (cid:13)f(z)/Z(cid:1) +O(cid:0) 1/K3(cid:1) . (15)
Proof. WestartfromEquation(8)andEquation(11)ofthemaintext,whichwereproducehereforconvenience,
logZ ≥E (cid:104) log(cid:16) w (cid:0) z(1:N)(cid:1)(cid:17)(cid:105) =:ELBON,K(f ,q ) (16)
z 0(1 :K:N)∼q AIS AIS 0:K AIS AIS AIS
and
wN,K(cid:0) z(1:N)(cid:1)
=
1 (cid:88)N (cid:89)K γ βk(cid:0) z k(i −) 1(cid:1)
(17)
AIS 0:K N γ (cid:0) z(i) (cid:1)
i=1k=1 βk−1 k−1
Equation (17) holds for DAIS if we assume perfect transitions (Grosse et al., 2013). Inserting γ (z) = Z π (z),
βk βk βk
Z =Z =Z,andZ =Z =1,weobtainforN =1(Grosseetal.,2013),
βK 1 β0 0
(cid:34) K (cid:35) K
∆1,K =logZ−E logZ −logZ
+(cid:88)
log
π βk(z k−1) =(cid:88)
D (π ∥π ). (18)
AIS z0:K∼q AIS βK β0 π (z ) KL βk−1 βk
k=1
βk−1 k−1
k=1
Asstatedinthemaintext,∆1,K isadivergencesinceitisasumofdivergencesandsince∆1,K =0forq =f/Z as,in
AIS AIS 0
thiscase,π =q ∀k ∈{0,...,K}since,bydefinition,
βk 0
γ (z) (cid:18) f(z)(cid:19)βk (cid:90)
π (z):= βk with γ (z):=q (z) and Z = γ (z)dz. (19)
βk Z βk 0 q (z) βk βk
βk 0
WenowshowEquation(15). Foreachtermontheright-handsideofEquation(18),wehave
D (π ∥π )=h (β )−h (β ), where h (η):=−E [logπ (z)]. (20)
KL βk−1 βk k−1 k k−1 k−1 k−1 πβk−1(z) η
UsingTaylor’stheorem,wecanexpress
h (β )−h (β )=(β −β )h′ (β )+R (β ), (21)
k−1 k k−1 k−1 k k−1 k−1 k 1 k
whereprimesdenotederivatives. WeusetheLagrangefromfortheremainder
1
R (β )= (β −β )2h′′ (η ), (22)
1 k 2 k k−1 k−1 k−1
forsomeη ∈[β ,β ].
k−1 k−1 k
Wecanshowthatthefirsttermoftheright-handsideofEquation(21)iszeroby
(cid:16) (cid:17)
h′ (β )=−∇ E [logπ (z)]
k−1 k−1 η πβk−1(z) η
η=βk−1
(cid:90) ∇ (π (z)) (cid:18)(cid:90) (cid:19) (23)
=− π (z)
η η η=βk−1
dz =−∇ π (z)dz =−∇ (1)=0.
βk−1 π (z) η η η
βk−1 η=βk−1
Therefore,onlythesecondtermontheright-handsideofEquation(21)contributes.
12DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
FromEquation(19),wecombinetheleftmostandthemiddleequationandarriveat
(cid:0) (cid:1)
logπ (z)=logq (z)−η logf(z)−logq (z) −logZ . (24)
η 0 0 η
InsertingEquation(24)intoEquation(20)andtakingthesecondderivativeweget
h′′ (η )=∂2logZ /∂η2 . (25)
k−1 k−1 ηk−1 k−1
InsertingEquation(23)andEquation(25)intoEquation(21)andtheresultintoEquation(18),wefind
∆1,K
=(cid:88)K
D (π ∥π )=
1 (cid:32) 1 (cid:88)K ∂2logZ ηk−1(cid:33)
. (26)
AIS KL βk−1 βk 2K K ∂η2
k=1 k=1 k−1
Here,thetermintheparenthesesapproximatesanintegral(sinceη ∈[β ,β
]=(cid:2)k−1, k(cid:3)
∀k). Usingthattheerror
k−1 k−1 k K K
forsuchnumericalintegrationscalesasO(1/K2),wethusfind
∆1,K = 1 (cid:18)(cid:90) 1 ∂2logZ η dη+O(cid:0) 1/K2(cid:1)(cid:19) (27)
AIS 2K ∂2η
0
(cid:32)(cid:18) (cid:19) (cid:18) (cid:19) (cid:33)
= 1 ∂logZ η − ∂logZ η +O(cid:0) 1/K3(cid:1) . (28)
2K ∂η ∂η
η=1 η=0
Thus,forlargeK,onlytheboundarytermsremain. WecalculatethembyinsertingthedefinitionsofZ fromEquation(10)
η
andusingtherelation∂xη/∂η =xηlogx. Wefind
∂logZ 1 (cid:90) (cid:18) f(z)(cid:19)η f(z)
η = q (z) log dz. (29)
∂η Z 0 q (z) q (z)
η 0 0
Thus,recallingthatZ =1andZ =Z arethenormalizationconstantsofq andf,respectively,
0 1 0
(cid:18) (cid:19) (cid:20) (cid:21)
∂lo ∂g ηZ η =E
z∼f(z)/Z
log qf( (z z)
)
=D KL(cid:0) f(z)/Z(cid:13) (cid:13)q 0(z)(cid:1) ; (30)
η=1 0
(cid:18) (cid:19) (cid:20) (cid:21)
− ∂lo ∂g ηZ η =E
z∼q0(z)
logq f0 (( zz )) =D KL(cid:0) q 0(z)(cid:13) (cid:13)f(z)/Z(cid:1) . (31)
η=0
InsertingEquations(30)and(31)intoEquation(28)leadstotheclaiminEquation(15).
StatementforN >1. StartingfromEquation(17)weobtainforthegapwithgeneralN,
∆N,K =−E
(cid:34) log(cid:32) 1 (cid:88)N (cid:89)K γ βk(cid:0) z k(i −) 1(cid:1) (cid:33)(cid:35)
. (32)
AIS z 0(1 :K:N)∼q AIS N i=1k=1γ βk−1(cid:0) z k(i −) 1(cid:1)
DuetothesumoverN,theright-handsidecannownolongerbeseparatedintoasumofK terms. However,wecanstill
boundtheright-handsidebypullingthesumoutofthelogarithmusingJensen’sinequality,resultingin
∆N,K ≤∆1,K,
(33)
AIS AIS
whichimplies
lim ∆N,K ≤ 1 D (q ,f/Z)+O(cid:0) 1/K3(cid:1) , (34)
K→∞ AIS K JS 0
whereD denotestheJensen-Shannondivergence. ThisshowsthattheboundforN > 1canbeupper-boundedbythe
JS
Jensen-Shannondivergencebutdoesnotgiveadditionalinsightson,e.g.,symmetry. ForN =1theinequalityisanequality.
13DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
B.MultidimensionalBimodalTargetDistributions
WeprovidefurtherdetailsfortheexperimentonmultidimensionalbimodalGaussiantargetdistributions. Theexperimentis
discussedinSection5.1ofthemaintext. WeuseanIntelXEONCPUE5-2650v4forrunningthesmall-scaleexperiments
andasingleNVIDIAGeForceGTX1080Tiforthelarge-scaleexperiments.
Themodelpisdefinedasfollows
1 1
p(z)= N(z;0,0.252I)+ N(z;1,0.252I). (35)
2 2
WeinitializethevariationaldistributionsofVI,IWVIandDAIS with
0
(cid:18) (cid:19)
1
q (z)=N z; 1,I , (36)
⋆ 2
wherethemeanandthediagonalofthecovariancematrixarelearnableparameters. Wetrainthemodelsfor7,500iterations
withtheAdamoptimizer(Kingma&Ba,2015)andalearningrateof10−2.
DAISutilizestheparameterizationofZhangetal.(2021). Inadditiontotheparametersofq ,welearnthescalecofthe
0
massmatrixM =cI,theannealingschedule(β ,...,β ),andthestepwidthsofthesampler.
1 K−1
MSCistrainedforacomparablenumberofiterationswithalearningrateof10−4.
ClassificationIntoMode-Seeking“s”andMass-Covering“c”andUndecidable“u”. Togetafirstclassificationon
“c”or“s”,wemeasurethedistanceofthemeanofthevariationaldistributiontobothmodesofthetargetdistribution,and
totheirmidpoint(0.5,...,0.5). Itturnsoutthatthiscriterionclearlyidentifiesinalmostallcaseswhetheradistribution
is mode-seeking (see Figure 7). For cases where we do not clearly identify whether a distribution is mass-covering or
mode-seeking,weusetheletter“u”,shortfor“undecidable”. Afterwards,wemanuallyverifiedtheclassificationsbymaking
aplotsimilartoFigure4foreach“square”ofFigure3.
N =1 N =2 N =4 N =8 N =16
15 (IW)VI 15 15 15 20
DAIS0
10 MSC1c 10 10 10
MSC8c 10
5 5 5 5
0 0 0 0 0
0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0 0.0 0.5 1.0
projectionofmeanontothethe1-direction,scaledsuchthatthetwomodesofthetargetdistributionareat0and1
Figure7. Distanceofthemeanofthevariationaldistributiontobothmodesofpanditsmidpoint.Distancesaremostly0.0,1.0,or0.5
for(IW)VI,DAIS0,andMSC1cindicatingthatthemeanofthevariationaldistributionfallsononeofthetwomodesorthemidpoint.For
MSC8ctherearemoreoutliers.
14
stnuoc stnuoc stnuoc stnuoc stnuocDifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
C.GaussianProcessRegression
We provide further details on model, the joint distribution we plot in the main text, and quantitative results of the GP
regressionexperimentonsyntheticdatadiscussedinSection5.2ofthemaintext. WeuseanIntelXEONCPUE5-2650v4
forrunningthesmall-scaleexperimentsandasingleNVIDIAGeForceGTX1080Tiforthelarge-scaleexperiments.
C.1.Model
Thekernels,calledRBF andRBF inthemaintext,areinstancesofthefollowingRBFkernelwithdifferentlengthscales
1 2
ρ =0.8andρ =3.0and
1 2
(cid:18) (t−s)2(cid:19)
k(t,s)=exp − . (37)
2ρ2
We discretize the domain [0,10] on 75 points. We initialize the variational distributions of VI, IWVI and DAIS with
0
standardnormaldistributions.
All models are trained for 50,000 iterations with the Adam optimizer and a learning rate of 10−3. DAIS utilizes the
parameterizationofZhangetal.(2021). Inadditiontotheparametersofq ,welearnthediagonaldofthemassmatrix
0
M =diag(d),theannealingschedule(β ,...,β ),andthestepwidthsofthesampler.
1 K−1
C.2.JointDistribution
ThissectionprovidesmorebackgroundonhowweproducetheplotsoftheGPregressionexperimentinthemaintext.
Letf denotethelatentprocessofinterest. Letf denotethepartoff thatwehavedataonandletf denotetheremaining
o u
part. Theprioronf canthenbewrittenasfollows.
(cid:18)(cid:18) (cid:19)(cid:19) (cid:18)(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)(cid:19)
f f m Σ Σ
p(f)=p o =N o ; o , o,o o,u (38)
f f m Σ Σ
u u u u,o u,u
p(f )p(f |f )=N(f ;m ,Σ )N(f ;m +Σ Σ−1(f −m ),Σ −Σ Σ−1Σ ) (39)
o u o o o o,o u u u,o o,o o o u,u u,o o,o o,u
WemodelthedataywithaGaussianlikelihoodmodelwithfixedvarianceσ2 =0.1.
p(f |y)=N(y;f ,σ2I) (40)
o o
Ifweconditionondataywecomputethejointasfollows.
p(f|y)=p(f |y)p(f |f ,y)=p(f |y)p(f |f ) (41)
o u o o u o
(cid:18)(cid:18) f (cid:19) (cid:18) m+ (cid:19)
=N o ; ,
f Σ Σ−1m++Σ Σ−1m +m
u u,o o,o u u,o o,o o u
(42)
(cid:18) Σ+ Σ+(Σ Σ−1)⊤ (cid:19)(cid:19)
u,o o,o
Σ Σ−1Σ+ (Σ Σ−1Σ+(Σ Σ−1)⊤+Σ −Σ Σ−1Σ
u,o o,o u,o o,o u,o o,o u,u u,o o,o o,u
wherem+andΣ+areeither(a)theanalyticallyinferredmeanandvarianceoftheposteriorGaussiangiventhedataor(b)
thelearnedvariationalapproximationgivendata.
Forthefirstcase,weget
m+ =m +Σ (Σ +σ2I)−1(x−m ) and (43)
o o,o o,o o
Σ+ =Σ −Σ (Σ +σ2I)−1Σ . (44)
o,o o,o o,o o,o
Inthesecondcase,wemodeltheGaussiandistributionbyameanvectorandavariancevector
m+ =mˇ and Σ+ =diag(sˇ). (45)
WeplottheshadedregionbycalculatingEquation(42)with
Σ+ =diag(diag(Σ −Σ (Σ +σ2I)−1Σ )). (46)
o,o o,o o,o o,o
15DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
C.3.ComplementaryQuantitativeResults
Table3,Table4,Table5,Table6,Table7,Table8,Table9,Table10,Table11,Table12provideadditionalresultsonour
theexperiment.
Table3. MAE(comparedtoanalyticsolution)ofmeanandstandarddeviationofGPregression.MoredetailsareprovidedinSection5.2.
Resultsareaveragedover3runs.RBF :differentpriorparameters.
1,2
{ }
N=1
d MAE IWVI DAIS0(2) DAIS0(4) DAIS0(8) DAIS0(16)
10 mean 2.29 ·10−4 ±3.9·10−5 2.30 ·10−4 ±7.9·10−5 6.65 ·10−4 ±9.5·10−5 1.30 ·10−3 ±6.9·10−5 6.11 ·10−4 ±2.2·10−4
RBF1
25
s mtd e.
an
5 1. .1 76 3· ·1 10 0− −2 4± ±1 1. .7 3· ·1 10 0− −5
5
4 1. .8 54 8· ·1 10 0− −2 4± ±1 2. .0 2· ·1 10 0− −5
5
3 2. .9 47 6· ·1 10 0− −2 4± ±2 4. .1 8· ·1 10 0− −5
5
3 4. .1 98 5· ·1 10 0− −2 4± ±3 9. .0 0· ·1 10 0− −5
5
2 5. .5 51 3· ·1 10 0− −2 4± ±1 6. .3 5· ·1 10 0− −5
5
std. 4.03 ·10−2 ±2.8·10−6 3.97 ·10−2 ±5.1·10−6 3.79 ·10−2 ±4.0·10−6 3.51 ·10−2 ±1.4·10−4 2.61 ·10−2 ±1.2·10−5
10 mean 1.08 ·10−3 ±2.6·10−4 9.84 ·10−4 ±1.5·10−4 1.39 ·10−3 ±9.5·10−5 1.26 ·10−3 ±2.9·10−4 1.57 ·10−3 ±5.8·10−4
RBF2
25
s mtd e.
an
1 3. .4 00 0· ·1 10 0− −2 4± ±8 8. .7 3· ·1 10 0− −5
5
1 4. .0 70 6· ·1 10 0− −2 4± ±7 7. .2 0· ·1 10 0− −5
5
5 6. .7 61 5· ·1 10 0− −3 4± ±4 1. .1 7· ·1 10 0− −5
4
5 8. .5 93 2· ·1 10 0− −3 4± ±9 6. .5 5· ·1 10 0− −5
5
5 9. .4 35 6· ·1 10 0− −3 4± ±9 1. .5 7· ·1 10 0− −5
4
std. 4.87 ·10−2 ±2.6·10−5 4.33 ·10−2 ±1.4·10−5 3.55 ·10−2 ±9.5·10−6 2.88 ·10−2 ±4.4·10−5 2.27 ·10−2 ±2.9·10−5
Table4. MAE(comparedtoanalyticsolution)ofmeanandstandarddeviationofGPregression.MoredetailsareprovidedinSection5.2.
Resultsareaveragedover3runs.RBF :differentpriorparameters.
1,2
{ }
N=1
d MAE IWVISIR DAIS(2) DAIS(4) DAIS(8) DAIS(16)
10 mean 1.13 ·10−3 ±5.0·10−5 4.64 ·10−3 ±1.9·10−4 2.19 ·10−2 ±3.1·10−4 7.55 ·10−3 ±4.5·10−4 1.16 ·10−2 ±1.8·10−4
RBF1
25
s mtd e.
an
1 2. .4 48 4· ·1 10 0− −2 4± ±1 1. .1 4· ·1 10 0− −4
5
4 3. .7 22 0· ·1 10 0− −2 3± ±9 7. .2 3· ·1 10 0− −5
5
2 2. .4 20 9· ·1 10 0− −2 3± ±4 2. .1 9· ·1 10 0− −5
4
6 5. .9 75 8· ·1 10 0− −3 3± ±2 2. .3 7· ·1 10 0− −4
3
2 4. .2 45 1· ·1 10 0− −2 3± ±1 3. .4 8· ·1 10 0− −4
4
std. 2.75 ·10−2 ±2.4·10−4 3.97 ·10−2 ±5.1·10−5 3.16 ·10−2 ±9.0·10−5 2.17 ·10−2 ±1.5·10−3 2.71 ·10−2 ±3.6·10−5
10 mean 1.54 ·10−5 ±5.2·10−6 1.15 ·10−2 ±4.7·10−4 1.90 ·10−3 ±3.0·10−5 1.87 ·10−3 ±4.0·10−5 2.78 ·10−3 ±9.3·10−5
RBF2
25
s mtd e.
an
1 3. .6 39 3· ·1 10 0− −3 4± ±9 2. .4 2· ·1 10 0− −5
5
6 2. .4 00 7· ·1 10 0− −3 2± ±1 1. .1 5· ·1 10 0− −4
3
2 4. .2 85 3· ·1 10 0− −3 3± ±1 2. .6 4· ·1 10 0− −5
4
1 2. .6 14 2· ·1 10 0− −3 2± ±1 1. .4 8· ·1 10 0− −5
3
1 3. .9 19 4· ·1 10 0− −3 2± ±2 3. .0 9· ·1 10 0− −4
4
std. 1.20 ·10−2 ±1.6·10−4 5.19 ·10−2 ±2.0·10−3 2.46 ·10−2 ±1.3·10−4 1.82 ·10−2 ±6.2·10−4 3.21 ·10−2 ±5.5·10−4
Table5. MAE(comparedtoanalyticsolution)ofmeanandstandarddeviationofGPregression.MoredetailsareprovidedinSection5.2.
Resultsareaveragedover3runs.RBF :differentpriorparameters.
1,2
{ }
N=2
d MAE IWVI DAIS0(2) DAIS0(4) DAIS0(8) DAIS0(16)
10 mean 4.21 ·10−4 ±9.6·10−5 3.19 ·10−4 ±6.8·10−5 7.78 ·10−4 ±3.4·10−4 6.10 ·10−4 ±3.6·10−4 8.30 ·10−4 ±2.2·10−4
RBF1
25
s mtd e.
an
4 1. .9 75 7· ·1 10 0− −2 4± ±2 1. .1 7· ·1 10 0− −5
5
4 2. .5 53 3· ·1 10 0− −2 4± ±5 2. .8 9· ·1 10 0− −6
5
3 4. .5 19 0· ·1 10 0− −2 4± ±1 5. .0 8· ·1 10 0− −5
5
2 3. .7 66 0· ·1 10 0− −2 4± ±2 9. .5 6· ·1 10 0− −5
5
1 6. .8 94 3· ·1 10 0− −2 4± ±6 5. .8 0· ·1 10 0− −5
5
std. 3.98 ·10−2 ±4.9·10−6 3.89 ·10−2 ±4.1·10−6 3.75 ·10−2 ±3.4·10−5 3.39 ·10−2 ±7.5·10−5 2.12 ·10−2 ±4.7·10−5
10 mean 1.36 ·10−3 ±2.5·10−4 1.02 ·10−3 ±1.5·10−4 2.01 ·10−3 ±6.8·10−4 1.45 ·10−3 ±1.2·10−4 1.47 ·10−3 ±5.3·10−5
RBF2
25
s mtd e.
an
8 3. .8 20 0· ·1 10 0− −3 4± ±6 4. .4 7· ·1 10 0− −5
5
4 4. .0 67 8· ·1 10 0− −3 4± ±7 6. .1 0· ·1 10 0− −5
5
5 6. .1 10 8· ·1 10 0− −3 4± ±4 8. .4 9· ·1 10 0− −5
5
5 9. .2 13 3· ·1 10 0− −3 4± ±5 7. .3 7· ·1 10 0− −5
5
5 1. .4 07 5· ·1 10 0− −3 3± ±9 1. .0 6· ·1 10 0− −5
4
std. 4.66 ·10−2 ±2.2·10−5 4.07 ·10−2 ±2.1·10−5 3.45 ·10−2 ±3.0·10−5 2.66 ·10−2 ±2.3·10−5 1.95 ·10−2 ±2.2·10−5
16DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
Table6. MAE(comparedtoanalyticsolution)ofmeanandstandarddeviationofGPregression.MoredetailsareprovidedinSection5.2.
Resultsareaveragedover3runs.RBF :differentpriorparameters.
1,2
{ }
N=2
d MAE IWVISIR DAIS(2) DAIS(4) DAIS(8) DAIS(16)
10 mean 8.62 ·10−4 ±4.5·10−5 5.69 ·10−3 ±3.3·10−4 1.67 ·10−2 ±1.3·10−3 1.15 ·10−2 ±3.2·10−4 1.64 ·10−2 ±8.9·10−4
RBF1
25
s mtd e.
an
1 7. .1 92 0· ·1 10 0− −2 4± ±4 4. .6 5· ·1 10 0− −4
6
4 4. .4 04 4· ·1 10 0− −2 3± ±5 7. .3 8· ·1 10 0− −5
4
1 5. .9 76 4· ·1 10 0− −2 3± ±3 4. .5 9· ·1 10 0− −4
4
2 8. .5 48 6· ·1 10 0− −2 3± ±5 1. .5 0· ·1 10 0− −5
4
2 3. .1 42 4· ·1 10 0− −2 3± ±2 4. .0 7· ·1 10 0− −4
5
std. 3.54 ·10−2 ±4.6·10−4 3.94 ·10−2 ±6.6·10−4 3.27 ·10−2 ±2.4·10−4 3.64 ·10−2 ±1.2·10−5 2.69 ·10−2 ±1.7·10−4
10 mean 1.00 ·10−5 ±2.0·10−6 7.30 ·10−3 ±1.9·10−4 1.97 ·10−3 ±7.6·10−6 1.85 ·10−3 ±5.7·10−5 2.52 ·10−3 ±3.7·10−5
RBF2
25
s mtd e.
an
1 1. .6 64 5· ·1 10 0− −3 4± ±1 1. .3 1· ·1 10 0− −4
5
3 1. .4 31 4· ·1 10 0− −3 2± ±1 2. .5 9· ·1 10 0− −4
3
2 1. .2 13 2· ·1 10 0− −3 2± ±3 1. .0 7· ·1 10 0− −5
3
1 1. .7 21 5· ·1 10 0− −3 2± ±1 5. .2 2· ·1 10 0− −5
4
1 1. .9 95 8· ·1 10 0− −3 2± ±7 3. .7 4· ·1 10 0− −5
4
std. 6.84 ·10−3 ±2.3·10−4 4.29 ·10−2 ±1.9·10−3 2.33 ·10−2 ±1.4·10−3 1.35 ·10−2 ±1.8·10−4 1.87 ·10−2 ±1.6·10−4
Table7. MAE(comparedtoanalyticsolution)ofmeanandstandarddeviationofGPregression.MoredetailsareprovidedinSection5.2.
Resultsareaveragedover3runs.RBF :differentpriorparameters.
1,2
{ }
N=4
d MAE IWVI DAIS0(2) DAIS0(4) DAIS0(8) DAIS0(16)
10 mean 3.09 ·10−4 ±9.1·10−5 4.36 ·10−4 ±6.9·10−5 7.07 ·10−4 ±1.6·10−4 7.60 ·10−4 ±2.2·10−4 8.57 ·10−4 ±1.8·10−4
RBF1
25
s mtd e.
an
4 2. .7 65 8· ·1 10 0− −2 4± ±2 3. .5 0· ·1 10 0− −5
5
4 2. .2 64 7· ·1 10 0− −2 4± ±2 4. .8 5· ·1 10 0− −5
5
3 4. .2 41 8· ·1 10 0− −2 4± ±4 4. .6 0· ·1 10 0− −5
5
2 4. .2 71 7· ·1 10 0− −2 4± ±1 7. .7 9· ·1 10 0− −5
5
1 7. .2 10 6· ·1 10 0− −2 4± ±2 1. .4 5· ·1 10 0− −5
4
std. 3.93 ·10−2 ±6.0·10−6 3.81 ·10−2 ±7.0·10−6 3.68 ·10−2 ±9.7·10−6 2.94 ·10−2 ±6.9·10−6 1.71 ·10−2 ±1.1·10−5
10 mean 9.76 ·10−4 ±1.6·10−4 1.23 ·10−3 ±2.1·10−4 2.04 ·10−3 ±4.5·10−4 1.88 ·10−3 ±6.4·10−4 2.25 ·10−3 ±3.4·10−4
RBF2
25
s mtd e.
an
3 4. .8 44 1· ·1 10 0− −3 4± ±8 7. .0 8· ·1 10 0− −5
5
7 4. .3 97 0· ·1 10 0− −4 4± ±8 7. .6 6· ·1 10 0− −5
5
4 6. .7 73 9· ·1 10 0− −3 4± ±5 6. .7 3· ·1 10 0− −5
5
5 1. .1 38 1· ·1 10 0− −3 3± ±9 1. .3 1· ·1 10 0− −5
4
5 9. .4 96 0· ·1 10 0− −3 4± ±6 1. .3 3· ·1 10 0− −5
4
std. 4.47 ·10−2 ±1.4·10−5 3.85 ·10−2 ±1.8·10−5 3.34 ·10−2 ±1.6·10−5 2.49 ·10−2 ±2.9·10−5 1.71 ·10−2 ±3.6·10−5
Table8. MAE(comparedtoanalyticsolution)ofmeanandstandarddeviationofGPregression.MoredetailsareprovidedinSection5.2.
Resultsareaveragedover3runs.RBF :differentpriorparameters.
1,2
{ }
N=4
d MAE IWVISIR DAIS(2) DAIS(4) DAIS(8) DAIS(16)
10 mean 2.44 ·10−4 ±2.1·10−5 6.90 ·10−3 ±3.4·10−4 6.45 ·10−3 ±5.0·10−5 1.50 ·10−2 ±2.2·10−4 1.65 ·10−2 ±5.2·10−4
RBF1
25
s mtd e.
an
9 8. .9 53 4· ·1 10 0− −3 4± ±1 3. .2 6· ·1 10 0− −4
5
4 5. .2 01 2· ·1 10 0− −2 3± ±2 7. .4 3· ·1 10 0− −4
4
1 6. .3 83 8· ·1 10 0− −2 3± ±1 3. .3 7· ·1 10 0− −4
4
2 8. .2 48 2· ·1 10 0− −2 3± ±1 4. .9 0· ·1 10 0− −4
4
1 6. .4 74 7· ·1 10 0− −2 3± ±1 1. .6 2· ·1 10 0− −4
4
std. 3.24 ·10−2 ±8.6·10−4 3.91 ·10−2 ±6.1·10−4 3.12 ·10−2 ±2.4·10−4 3.43 ·10−2 ±3.5·10−4 2.79 ·10−2 ±1.0·10−4
10 mean 1.34 ·10−5 ±4.3·10−6 4.98 ·10−3 ±2.1·10−5 2.00 ·10−3 ±5.1·10−5 1.84 ·10−3 ±1.1·10−4 2.66 ·10−3 ±7.9·10−5
RBF2
25
s mtd e.
an
1 1. .5 30 0· ·1 10 0− −3 4± ±1 8. .5 1· ·1 10 0− −4
6
3 7. .1 54 7· ·1 10 0− −3 3± ±1 1. .2 6· ·1 10 0− −4
3
1 3. .9 17 6· ·1 10 0− −3 2± ±1 1. .4 6· ·1 10 0− −4
3
1 1. .7 01 2· ·1 10 0− −3 2± ±4 1. .1 8· ·1 10 0− −5
3
1 1. .8 80 8· ·1 10 0− −3 2± ±5 2. .7 9· ·1 10 0− −5
4
std. 5.58 ·10−3 ±3.3·10−4 3.92 ·10−2 ±1.8·10−4 3.75 ·10−2 ±1.9·10−3 1.08 ·10−2 ±4.5·10−4 1.56 ·10−2 ±2.9·10−4
Table9. MAE(comparedtoanalyticsolution)ofmeanandstandarddeviationofGPregression.MoredetailsareprovidedinSection5.2.
Resultsareaveragedover3runs.RBF :differentpriorparameters.
1,2
{ }
N=16
d MAE DAIS0(2) DAIS0(4) DAIS0(8)
10 mean 7.50 ·10−4 ±2.1·10−4 1.35 ·10−3 ±3.4·10−4 1.02 ·10−3 ±1.5·10−4
RBF1
25
s mtd e.
an
3 3. .6 84 6· ·1 10 0− −2 4± ±7 5. .9 8· ·1 10 0− −5
5
2 6. .5 56 8· ·1 10 0− −2 4± ±6 5. .9 4· ·1 10 0− −6
5
1 6. .1 87 1· ·1 10 0− −2 4± ±5 9. .3 9· ·1 10 0− −5
5
std. 3.66 ·10−2 ±4.5·10−6 3.55 ·10−2 ±5.7·10−5 2.64 ·10−2 ±9.0·10−6
10 mean 1.94 ·10−3 ±2.5·10−4 1.80 ·10−3 ±2.0·10−4 2.88 ·10−3 ±7.4·10−4
RBF2
25
s mtd e.
an
4 6. .1 15 4· ·1 10 0− −3 4± ±1 6. .3 0· ·1 10 0− −4
5
6 8. .0 54 5· ·1 10 0− −3 4± ±5 9. .3 8· ·1 10 0− −5
5
4 1. .9 35 1· ·1 10 0− −3 3± ±6 2. .3 5· ·1 10 0− −5
4
std. 3.48 ·10−2 ±4.1·10−5 2.90 ·10−2 ±4.1·10−5 2.20 ·10−2 ±5.3·10−6
17DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
Table10. MAE(comparedtoanalyticsolution)ofmeanandstandarddeviationofGPregression.MoredetailsareprovidedinSection5.2.
Resultsareaveragedover3runs.RBF :differentpriorparameters.
1,2
{ }
N=16
d MAE DAIS(2) DAIS(4) DAIS(8)
10 mean 1.15 ·10−2 ±1.2·10−3 7.61 ·10−3 ±7.1·10−4 1.60 ·10−2 ±4.0·10−4
RBF1
25
s mtd e.
an
4 4. .0 96 2· ·1 10 0− −2 3± ±3 7. .0 2· ·1 10 0− −4
4
9 6. .6 26 6· ·1 10 0− −3 3± ±2 9. .3 3· ·1 10 0− −4
4
1 7. .8 92 3· ·1 10 0− −2 3± ±4 4. .0 8· ·1 10 0− −4
4
std. 3.77 ·10−2 ±3.7·10−4 3.15 ·10−2 ±1.0·10−3 3.08 ·10−2 ±3.4·10−4
10 mean 4.55 ·10−3 ±2.6·10−4 2.46 ·10−3 ±1.3·10−4 2.38 ·10−3 ±4.0·10−4
RBF2
25
s mtd e.
an
3 6. .2 09 1· ·1 10 0− −3 3± ±1 4. .4 5· ·1 10 0− −4
4
2 1. .3 93 0· ·1 10 0− −3 2± ±2 1. .6 7· ·1 10 0− −5
3
2 7. .0 29 2· ·1 10 0− −3 3± ±7 7. .3 7· ·1 10 0− −4
4
std. 3.88 ·10−2 ±8.5·10−4 2.73 ·10−2 ±1.8·10−3 5.17 ·10−3 ±1.7·10−4
Table11. MAE(comparedtoanalyticsolution)ofmeanandstandarddeviationofGPregression.MoredetailsareprovidedinSection5.2.
Resultsareaveragedover3runs.RBF :differentpriorparameters.
1,2
{ }
d MAE MSC1c(N=1) MSC1c(N=2) MSC1c(N=4) MSC1c(N=16)
10 mean 9.22 ·10−1 ±0.0 4.10 ·10−2 ±7.3·10−3 3.11 ·10−2 ±7.6·10−3 1.87 ·10−2 ±4.5·10−3
RBF1
25
s mtd e.
an
6 9. .6 26 0· ·1 10 0− −2 1± ±0 0. .0
0
5 2. .0 72 2· ·1 10 0− −2 1± ±5 4. .2 6· ·1 10 0− −4
2
4 5. .6 08 2· ·1 10 0− −2 2± ±3 6. .0 4· ·1 10 0− −4
3
24 .. 72 25 ·· 11 00 −− 22 ±± 01 .. 75 7· ·1 10 0− −4
3
std. 4.54 ·10−2 ±0.0 4.20 ·10−2 ±1.6·10−4 4.02 ·10−2 ±2.7·10−5 3.89 ·10−2 ±1.5·10−4
10 mean 6.23 ·10−1 ±0.0 1.72 ·10−2 ±2.6·10−3 7.30 ·10−3 ±1.6·10−3 4.16 ·10−3 ±2.9·10−4
RBF2
25
s mtd e.
an
8 7. .8 10 7· ·1 10 0− −2 1± ±0 0. .0
0
7 1. .7 10 1· ·1 10 0− −3 1± ±5 7. .2 1· ·1 10 0− −4
3
3 4. .5 47 3· ·1 10 0− −3 2± ±3 2. .3 5· ·1 10 0− −4
3
1 3. .6 67 1· ·1 10 0− −3 2± ±5 .7.7 3· ·1 10 0− −5
3
std. 7.11 ·10−2 ±0.0 4.81 ·10−2 ±5.7·10−4 4.54 ·10−2 ±3.8·10−4 4.18 ·10−2 ±3.1·10−4
Table12. MAE(comparedtoanalyticsolution)ofmeanandstandarddeviationofGPregressionforMSCwith8parallelchains(MSC8c).
Resultsareaveragedover3runs.RBF :differentpriorparameters.
1,2
{ }
d MAE MSC8c(N=1) MSC8c(N=2) MSC8c(N=4) MSC8c(N=16)
10 mean 9.22 ·10−1 ±0.0 1.66 ·10−2 ±3.6·10−3 1.66 ·10−2 ±5.2·10−3 1.74 ·10−2 ±2.4·10−3
RBF1
25
s mtd e.
an
6 9. .6 26 0· ·1 10 0− −2 1± ±0 0. .0
0
4 4. .5 17 7· ·1 10 0− −2 2± ±1 6. .3 3· ·1 10 0− −3
3
4 2. .2 09 3· ·1 10 0− −2 2± ±9 3. .6 7· ·1 10 0− −4
3
3 1. .6 16 4· ·1 10 0− −2 2± ±1 1. .4 6· ·1 10 0− −3
3
std. 4.54 ·10−2 ±0.0 3.95 ·10−2 ±1.0·10−4 3.89 ·10−2 ±1.4·10−4 3.77 ·10−2 ±1.5·10−4
10 mean 6.23 ·10−1 ±0.0 7.54 ·10−3 ±2.5·10−3 4.07 ·10−3 ±9.2·10−4 1.57 ·10−3 ±2.7·10−4
RBF2
25
s mtd e.
an
8 7. .8 10 8· ·1 10 0− −2 1± ±0 0. .0
0
4 2. .0 74 7· ·1 10 0− −3 2± ±7 2. .7 0· ·1 10 0− −4
3
2 1. .3 94 5· ·1 10 0− −3 2± ±6 2. .6 3· ·1 10 0− −4
3
7 2. .8 15 6· ·1 10 0− −4 2± ±2 3. .4 4· ·1 10 0− −4
3
std. 7.11 ·10−2 ±0.0 4.61 ·10−2 ±3.6·10−4 4.28 ·10−2 ±5.8·10−4 4.07 ·10−2 ±5.7·10−4
18DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
D.BayesianLogisticRegression
Weprovidefurtherdetailsonmodel,thesamplingprocedure,andquantitativeresultsofthelogisticregressionexperiments
on the five datasets considered. The experiment is discussed in Section 5.3 of the main text. We use an Intel XEON
CPUE5-2650v4forrunningthesmall-scaleexperimentsandasingleNVIDIAGeForceGTX1080Tiforthelarge-scale
experiments.
D.1.Model
Wetrainourmodelsfor100,000iterationsonfull-batchgradients. WeusetheAdamoptimizerwithalearningrateof10−3.
DAISutilizestheparameterizationofZhangetal.(2021). Inadditiontotheparametersofq ,welearnthediagonaldofthe
0
massmatrixM =diag(d),theannealingschedule(β ,...,β ),andthestepwidthsofthesampler.
1 K−1
D.2.Sampling
WesamplethemodeldescribedinthemaintextwithHMCandcomparethelearnedvariationaldistributionsofVI,IWVI,
andDAIS tothesamples(thatwetreatas“groundtruth”). Weusetheleapfrogintegratorwithanidentitymassmatrixfor
0
n stepsandastepsizeofϵ ,n burn-insteps,takeeveryn -thsampleandsamplen intotal. WeapplyMHcorrection
l HMC b e t
steps. Table13showsthecorrespondinghyperparameters.
Table13. HyperparametersofHMCsamplingforthesonardatasetandtheionospheredataset.
ϵ n n n n
HMC l b e t
sonar 0.001 50 10,000 10 10,000
ionosphere 0.001 50 10,000 5 10,000
D.3.ComplementaryQuantitativeResults
Table14,Table15,Table16,Table17,Table18,Table19,Table20,Table21,andTable22provideadditionalresultsonthe
experiment.
Table14. MAEcalculatedbetweenlearnedmeanandstandarddeviationtoHMCsamplesforBayesianlogisticregression.Detailscanbe
foundinSection5.3.Resultsareaveragedover3runs.
N=1
d MAE IWVI DAIS0(2) DAIS0(4) DAIS0(8) DAIS0(16)
sonar 61 mean 9.00 ·10−2 ±1.0·10−3 8.67 ·10−2 ±2.2·10−3 8.76 ·10−2 ±2.9·10−3 8.69 ·10−2 ±1.8·10−3 8.68 ·10−2 ±9.7·10−4
std. 2.86 ·10−1 ±1.9·10−3 1.77 ·10−1 ±9.5·10−4 1.23 ·10−1 ±1.5·10−3 1.22 ·10−1 ±1.4·10−3 1.20 ·10−1 ±4.3·10−4
ionosphere 35 mean 9.23 ·10−2 ±5.8·10−4 8.66 ·10−2 ±5.0·10−4 7.87 ·10−2 ±3.5·10−3 8.38 ·10−2 ±2.2·10−3 8.57 ·10−2 ±2.9·10−3
std. 2.32 ·10−1 ±8.5·10−4 2.01 ·10−1 ±1.1·10−3 1.57 ·10−1 ±6.5·10−3 1.20 ·10−1 ±1.3·10−3 8.40 ·10−2 ±1.3·10−3
heart-disease 16 mean 1.80 ·10−2 ±7.9·10−5 1.65 ·10−2 ±1.3·10−3 1.93 ·10−2 ±2.1·10−3 1.82 ·10−2 ±5.6·10−4 1.69 ·10−2 ±8.8·10−4
std. 1.72 ·10−1 ±7.2·10−4 1.15 ·10−1 ±1.4·10−3 4.82 ·10−2 ±3.8·10−4 4.66 ·10−2 ±8.6·10−4 3.61 ·10−2 ±1.3·10−4
heart-attack 14 mean 5.05 ·10−2 ±1.1·10−3 4.97 ·10−2 ±2.5·10−3 5.01 ·10−2 ±1.5·10−3 4.68 ·10−2 ±3.2·10−3 5.25 ·10−2 ±1.5·10−3
std. 2.19 ·10−1 ±1.1·10−3 1.25 ·10−1 ±8.5·10−4 3.23 ·10−2 ±2.9·10−4 3.14 ·10−2 ±5.9·10−4 2.69 ·10−2 ±7.5·10−4
loan 12 mean 1.03 ·10−2 ±6.5·10−4 1.26 ·10−2 ±8.5·10−4 1.27 ·10−2 ±1.0·10−3 1.34 ·10−2 ±7.4·10−4 1.17 ·10−2 ±2.4·10−3
std. 1.46 ·10−1 ±1.3·10−3 9.30 ·10−2 ±1.1·10−3 2.47 ·10−2 ±1.4·10−3 2.75 ·10−2 ±2.4·10−3 2.25 ·10−2 ±1.3·10−3
19DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
Table15. MAEcalculatedbetweenlearnedmeanandstandarddeviationtoHMCsamplesforBayesianlogisticregression.Detailscanbe
foundinSection5.3.Resultsareaveragedover3runs.
N=1
d MAE IWVISIR DAIS(2) DAIS(4) DAIS(8) DAIS(16)
sonar 61 mean 9.28 ·10−2 ±7.6·10−4 3.77 ·10−1 ±2.2·10−2 2.06 ·10−1 ±1.4·10−2 1.77 ·10−1 ±1.5·10−2 1.68 ·10−1 ±8.2·10−3
std. 2.90 ·10−1 ±2.1·10−3 3.38 ·10−1 ±2.1·10−2 1.40 ·10−1 ±7.1·10−3 1.25 ·10−1 ±7.9·10−3 1.02 ·10−1 ±8.1·10−3
ionosphere 35 mean 8.80 ·10−2 ±5.0·10−4 9.75 ·10−2 ±3.2·10−3 1.08 ·10−1 ±1.9·10−2 1.11 ·10−1 ±2.4·10−2 8.90 ·10−2 ±2.3·10−3
std. 2.32 ·10−1 ±8.7·10−4 1.55 ·10−1 ±8.3·10−4 1.13 ·10−1 ±1.1·10−2 8.57 ·10−2 ±9.8·10−3 4.32 ·10−2 ±3.8·10−3
heart-disease 16 mean 1.82 ·10−2 ±1.8·10−4 1.68 ·10−2 ±1.2·10−3 1.84 ·10−2 ±1.4·10−3 1.85 ·10−2 ±7.7·10−4 1.62 ·10−2 ±7.8·10−4
std. 1.71 ·10−1 ±6.2·10−4 1.17 ·10−1 ±1.4·10−3 4.31 ·10−2 ±4.2·10−4 4.64 ·10−2 ±9.4·10−4 3.63 ·10−2 ±2.3·10−4
heart-attack 14 mean 4.80 ·10−2 ±1.3·10−3 4.96 ·10−2 ±2.9·10−3 5.14 ·10−2 ±2.3·10−3 4.72 ·10−2 ±3.6·10−3 5.33 ·10−2 ±1.0·10−3
std. 2.20 ·10−1 ±1.2·10−3 1.25 ·10−1 ±9.0·10−4 2.76 ·10−2 ±2.5·10−4 2.95 ·10−2 ±5.7·10−4 2.55 ·10−2 ±7.4·10−4
loan 12 mean 1.78 ·10−2 ±6.2·10−4 1.42 ·10−2 ±1.2·10−3 1.18 ·10−2 ±8.0·10−4 1.49 ·10−2 ±8.6·10−4 1.18 ·10−2 ±2.9·10−3
std. 1.43 ·10−1 ±1.3·10−3 9.46 ·10−2 ±1.3·10−3 2.02 ·10−2 ±1.2·10−3 2.59 ·10−2 ±2.5·10−3 2.15 ·10−2 ±1.4·10−3
Table16. MAEcalculatedbetweenlearnedmeanandstandarddeviationtoHMCsamplesforBayesianlogisticregression.Detailscanbe
foundinSection5.3.Resultsareaveragedover3runs.
N=2
d MAE IWVI DAIS0(2) DAIS0(4) DAIS0(8) DAIS0(16)
sonar 61 mean 8.81 ·10−2 ±8.7·10−4 8.53 ·10−2 ±1.3·10−3 8.65 ·10−2 ±3.5·10−4 8.51 ·10−2 ±2.2·10−3 8.49 ·10−2 ±1.5·10−3
std. 2.09 ·10−1 ±1.2·10−3 1.31 ·10−1 ±1.2·10−3 9.08 ·10−2 ±1.3·10−3 9.08 ·10−2 ±1.3·10−3 8.86 ·10−2 ±8.2·10−4
ionosphere 35 mean 4.84 ·10−2 ±3.3·10−4 4.59 ·10−2 ±3.2·10−4 4.46 ·10−2 ±5.5·10−4 4.69 ·10−2 ±4.7·10−4 4.87 ·10−2 ±5.9·10−4
std. 1.04 ·10−1 ±3.6·10−4 9.00 ·10−2 ±2.1·10−4 7.53 ·10−2 ±1.6·10−4 5.94 ·10−2 ±8.2·10−4 4.14 ·10−2 ±1.7·10−3
heart-disease 16 mean 1.56 ·10−2 ±1.7·10−3 1.83 ·10−2 ±1.6·10−3 1.92 ·10−2 ±1.3·10−3 1.88 ·10−2 ±1.5·10−3 2.08 ·10−2 ±3.3·10−3
std. 1.28 ·10−1 ±5.0·10−4 7.57 ·10−2 ±1.0·10−3 3.28 ·10−2 ±8.8·10−4 3.15 ·10−2 ±1.3·10−3 1.98 ·10−2 ±1.3·10−3
heart-attack 14 mean 4.95 ·10−2 ±3.9·10−3 4.86 ·10−2 ±2.5·10−3 4.74 ·10−2 ±1.8·10−3 4.60 ·10−2 ±2.9·10−3 5.05 ·10−2 ±1.5·10−3
std. 1.45 ·10−1 ±1.8·10−3 6.95 ·10−2 ±1.5·10−3 3.02 ·10−2 ±1.3·10−3 2.62 ·10−2 ±1.8·10−3 2.81 ·10−2 ±9.8·10−4
loan 12 mean 9.57 ·10−3 ±1.0·10−3 1.35 ·10−2 ±2.4·10−3 1.81 ·10−2 ±3.3·10−3 9.02 ·10−3 ±7.1·10−4 1.53 ·10−2 ±1.1·10−3
std. 9.55 ·10−2 ±7.8·10−4 5.30 ·10−2 ±1.7·10−3 1.23 ·10−2 ±1.4·10−3 1.44 ·10−2 ±1.3·10−3 9.15 ·10−3 ±8.4·10−4
Table17. MAEcalculatedbetweenlearnedmeanandstandarddeviationtoHMCsamplesforBayesianlogisticregression.Detailscanbe
foundinSection5.3.Resultsareaveragedover3runs.
N=2
d MAE IWVISIR DAIS(2) DAIS(4) DAIS(8) DAIS(16)
sonar 61 mean 9.03 ·10−2 ±1.4·10−3 3.53 ·10−1 ±1.8·10−2 1.85 ·10−1 ±2.0·10−2 1.35 ·10−1 ±1.1·10−2 1.58 ·10−1 ±7.7·10−3
std. 2.13 ·10−1 ±1.0·10−3 2.59 ·10−1 ±1.6·10−2 1.17 ·10−1 ±1.3·10−2 8.64 ·10−2 ±3.2·10−3 9.04 ·10−2 ±7.9·10−3
ionosphere 35 mean 7.94 ·10−2 ±9.8·10−4 9.14 ·10−2 ±6.5·10−3 1.39 ·10−1 ±3.1·10−2 1.19 ·10−1 ±3.2·10−3 9.99 ·10−2 ±1.7·10−2
std. 1.81 ·10−1 ±6.5·10−4 1.20 ·10−1 ±5.8·10−3 1.26 ·10−1 ±2.5·10−2 7.04 ·10−2 ±3.7·10−3 6.06 ·10−2 ±3.9·10−3
heart-disease 16 mean 1.63 ·10−2 ±1.4·10−3 1.84 ·10−2 ±1.7·10−3 1.84 ·10−2 ±5.8·10−4 1.86 ·10−2 ±1.5·10−3 1.98 ·10−2 ±4.7·10−3
std. 1.28 ·10−1 ±6.2·10−4 7.61 ·10−2 ±8.5·10−4 3.00 ·10−2 ±2.1·10−3 3.18 ·10−2 ±1.4·10−3 2.08 ·10−2 ±1.3·10−3
heart-attack 14 mean 4.71 ·10−2 ±4.3·10−3 4.85 ·10−2 ±2.8·10−3 4.79 ·10−2 ±1.8·10−3 4.67 ·10−2 ±2.9·10−3 5.08 ·10−2 ±1.8·10−3
std. 1.45 ·10−1 ±1.8·10−3 6.93 ·10−2 ±1.4·10−3 2.66 ·10−2 ±1.3·10−3 2.42 ·10−2 ±1.4·10−3 2.77 ·10−2 ±9.6·10−4
loan 12 mean 1.64 ·10−2 ±7.5·10−4 1.44 ·10−2 ±2.4·10−3 1.62 ·10−2 ±2.7·10−3 8.57 ·10−3 ±5.4·10−4 1.52 ·10−2 ±1.3·10−3
std. 9.39 ·10−2 ±1.8·10−3 5.31 ·10−2 ±1.7·10−3 9.09 ·10−3 ±1.2·10−3 1.26 ·10−2 ±1.5·10−3 8.07 ·10−3 ±4.3·10−4
20DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
Table18. MAEcalculatedbetweenlearnedmeanandstandarddeviationtoHMCsamplesforBayesianlogisticregression.Detailscanbe
foundinSection5.3.Resultsareaveragedover3runs.
N=4
d MAE IWVI DAIS0(2) DAIS0(4) DAIS0(8) DAIS0(16)
sonar 61 mean 8.68 ·10−2 ±1.1·10−3 8.45 ·10−2 ±8.5·10−4 8.64 ·10−2 ±9.0·10−4 8.89 ·10−2 ±2.3·10−3 8.42 ·10−2 ±1.1·10−3
std. 1.48 ·10−1 ±1.1·10−3 9.98 ·10−2 ±1.5·10−3 6.92 ·10−2 ±1.9·10−3 6.95 ·10−2 ±1.0·10−3 6.63 ·10−2 ±1.8·10−3
ionosphere 35 mean 4.61 ·10−2 ±8.7·10−4 4.48 ·10−2 ±4.0·10−4 4.41 ·10−2 ±1.1·10−3 4.42 ·10−2 ±1.8·10−3 4.59 ·10−2 ±9.7·10−4
std. 7.97 ·10−2 ±5.7·10−4 6.88 ·10−2 ±3.2·10−4 5.63 ·10−2 ±6.3·10−4 5.65 ·10−2 ±6.7·10−4 4.26 ·10−2 ±3.0·10−3
heart-disease 16 mean 1.59 ·10−2 ±6.4·10−4 1.69 ·10−2 ±1.5·10−3 2.21 ·10−2 ±1.4·10−3 1.84 ·10−2 ±3.3·10−4 2.05 ·10−2 ±2.4·10−3
std. 8.56 ·10−2 ±9.9·10−4 5.08 ·10−2 ±1.3·10−3 2.00 ·10−2 ±1.2·10−3 1.68 ·10−2 ±6.5·10−4 1.34 ·10−2 ±1.2·10−3
heart-attack 14 mean 4.95 ·10−2 ±3.4·10−3 4.74 ·10−2 ±1.5·10−3 4.72 ·10−2 ±4.0·10−3 4.85 ·10−2 ±3.5·10−3 4.99 ·10−2 ±4.4·10−3
std. 7.97 ·10−2 ±6.6·10−4 3.46 ·10−2 ±2.2·10−3 3.49 ·10−2 ±5.2·10−3 2.65 ·10−2 ±4.2·10−3 2.74 ·10−2 ±1.4·10−3
loan 12 mean 1.31 ·10−2 ±5.4·10−3 1.09 ·10−2 ±3.5·10−4 1.42 ·10−2 ±1.4·10−3 1.31 ·10−2 ±1.2·10−3 1.85 ·10−2 ±2.3·10−3
std. 5.52 ·10−2 ±4.5·10−4 2.79 ·10−2 ±1.2·10−3 1.16 ·10−2 ±5.1·10−4 7.90 ·10−3 ±1.7·10−3 5.57 ·10−3 ±6.3·10−4
Table19. MAEcalculatedbetweenlearnedmeanandstandarddeviationtoHMCsamplesforBayesianlogisticregression.Detailscanbe
foundinSection5.3.Resultsareaveragedover3runs.
N=4
d MAE IWVISIR DAIS(2) DAIS(4) DAIS(8) DAIS(16)
sonar 61 mean 8.86 ·10−2 ±1.2·10−3 2.45 ·10−1 ±4.3·10−2 1.54 ·10−1 ±1.6·10−2 1.21 ·10−1 ±6.7·10−3 1.40 ·10−1 ±9.4·10−3
std. 1.52 ·10−1 ±1.1·10−3 1.76 ·10−1 ±2.8·10−2 9.18 ·10−2 ±1.0·10−2 7.43 ·10−2 ±6.3·10−3 7.80 ·10−2 ±3.1·10−3
ionosphere 35 mean 7.67 ·10−2 ±7.0·10−4 1.29 ·10−1 ±3.8·10−2 2.27 ·10−1 ±2.3·10−2 1.01 ·10−1 ±5.4·10−3 8.49 ·10−2 ±2.8·10−3
std. 1.39 ·10−1 ±9.9·10−4 1.25 ·10−1 ±3.0·10−2 1.68 ·10−1 ±1.9·10−2 7.11 ·10−2 ±1.7·10−3 5.82 ·10−2 ±4.7·10−3
heart-disease 16 mean 1.70 ·10−2 ±1.4·10−4 1.63 ·10−2 ±1.3·10−3 2.20 ·10−2 ±2.2·10−3 1.82 ·10−2 ±3.7·10−4 1.98 ·10−2 ±2.2·10−3
std. 8.54 ·10−2 ±1.0·10−3 5.05 ·10−2 ±1.3·10−3 1.92 ·10−2 ±1.2·10−3 1.71 ·10−2 ±7.7·10−4 1.43 ·10−2 ±1.2·10−3
heart-attack 14 mean 4.70 ·10−2 ±2.9·10−3 4.79 ·10−2 ±1.8·10−3 4.62 ·10−2 ±5.6·10−3 4.87 ·10−2 ±2.4·10−3 5.10 ·10−2 ±4.7·10−3
std. 8.13 ·10−2 ±3.5·10−4 3.47 ·10−2 ±1.8·10−3 3.14 ·10−2 ±5.1·10−3 2.48 ·10−2 ±3.8·10−3 2.69 ·10−2 ±1.2·10−3
loan 12 mean 2.23 ·10−2 ±5.5·10−3 1.16 ·10−2 ±1.1·10−3 1.23 ·10−2 ±2.1·10−3 1.37 ·10−2 ±2.8·10−4 1.85 ·10−2 ±3.6·10−3
std. 5.22 ·10−2 ±3.9·10−4 2.79 ·10−2 ±1.3·10−3 9.15 ·10−3 ±9.6·10−4 6.22 ·10−3 ±1.7·10−3 5.36 ·10−3 ±2.8·10−5
Table20. MAEcalculatedbetweenlearnedmeanandstandarddeviationtoHMCsamplesforBayesianlogisticregression.Detailscanbe
foundinSection5.3.Resultsareaveragedover3runs.
N=16
d MAE DAIS0(2) DAIS0(4) DAIS0(8)
sonar 61 mean 8.79 ·10−2 ±1.9·10−3 8.82 ·10−2 ±2.0·10−3 8.37 ·10−2 ±1.3·10−3
std. 6.11 ·10−2 ±1.0·10−3 4.43 ·10−2 ±5.0·10−4 4.57 ·10−2 ±4.8·10−4
ionosphere 35 mean 4.30 ·10−2 ±1.0·10−3 4.53 ·10−2 ±8.9·10−4 4.49 ·10−2 ±1.1·10−3
std. 4.27 ·10−2 ±2.9·10−4 3.41 ·10−2 ±6.1·10−4 3.46 ·10−2 ±4.5·10−4
heart-disease 16 mean 2.03 ·10−2 ±2.1·10−3 1.94 ·10−2 ±3.0·10−3 2.21 ·10−2 ±4.6·10−3
std. 1.41 ·10−2 ±1.8·10−3 9.70 ·10−3 ±1.4·10−3 7.85 ·10−3 ±4.2·10−4
heart-attack 14 mean 4.89 ·10−2 ±3.4·10−3 5.48 ·10−2 ±6.4·10−3 4.81 ·10−2 ±2.3·10−3
std. 3.16 ·10−2 ±3.3·10−3 3.99 ·10−2 ±5.8·10−4 3.50 ·10−2 ±2.8·10−3
loan 12 mean 1.73 ·10−2 ±3.7·10−3 2.03 ·10−2 ±3.3·10−3 1.50 ·10−2 ±2.7·10−3
std. 8.10 ·10−3 ±1.3·10−3 1.65 ·10−2 ±2.5·10−3 1.22 ·10−2 ±2.2·10−3
21DifferentiableAISMinimizesTheJensen-ShannonDivergenceBetweenInitialandTargetDistribution
Table21. MAEcalculatedbetweenlearnedmeanandstandarddeviationtoHMCsamplesforBayesianlogisticregression.Detailscanbe
foundinSection5.3.Resultsareaveragedover3runs.
N=16
d MAE DAIS(2) DAIS(4) DAIS(8)
sonar 61 mean 1.87 ·10−1 ±6.0·10−2 1.20 ·10−1 ±7.9·10−3 1.07 ·10−1 ±5.7·10−3
std. 1.25 ·10−1 ±2.5·10−2 7.26 ·10−2 ±6.0·10−3 6.06 ·10−2 ±5.0·10−3
ionosphere 35 mean 1.36 ·10−1 ±4.3·10−2 2.05 ·10−1 ±5.2·10−2 1.15 ·10−1 ±9.7·10−3
std. 1.08 ·10−1 ±2.1·10−2 1.27 ·10−1 ±2.7·10−2 7.53 ·10−2 ±2.0·10−3
heart-disease 16 mean 1.99 ·10−2 ±2.0·10−3 1.94 ·10−2 ±3.0·10−3 2.21 ·10−2 ±4.2·10−3
std. 1.44 ·10−2 ±1.6·10−3 1.07 ·10−2 ±1.3·10−3 8.96 ·10−3 ±1.5·10−4
heart-attack 14 mean 4.85 ·10−2 ±5.1·10−3 5.49 ·10−2 ±6.4·10−3 4.87 ·10−2 ±2.2·10−3
std. 3.15 ·10−2 ±3.4·10−3 3.65 ·10−2 ±8.4·10−4 3.35 ·10−2 ±2.6·10−3
loan 12 mean 1.66 ·10−2 ±2.9·10−3 1.95 ·10−2 ±2.9·10−3 1.33 ·10−2 ±1.8·10−3
std. 9.12 ·10−3 ±1.6·10−3 1.52 ·10−2 ±2.6·10−3 1.21 ·10−2 ±2.1·10−3
Table22. MAEcalculatedbetweenlearnedmeanandstandarddeviationtoHMCsamplesforBayesianlogisticregression.Detailscanbe
foundinSection5.3.Resultsareaveragedover3runs.
d MAE MSC1c(N=1) MSC1c(N=4) MSC1c(N=8) MSC1c(N=16)
sonar 61 mean 4.71 ·10−1 ±7.1·10−3 2.30 ·10−1 ±1.9·10−2 1.57 ·10−1 ±9.5·10−3 1.39 ·10−1 ±5.9·10−3
std. 8.85 ·10−1 ±8.7·10−7 2.10 ·10−1 ±2.1·10−2 1.43 ·10−1 ±1.4·10−3 9.43 ·10−2 ±1.1·10−2
ionosphere 35 mean 7.62 ·10−1 ±1.2·10−2 2.68 ·10−1 ±2.4·10−2 2.35 ·10−1 ±1.1·10−2 2.10 ·10−1 ±9.4·10−3
std. 5.87 ·10−1 ±4.4·10−7 1.68 ·10−1 ±1.5·10−3 1.32 ·10−1 ±1.2·10−2 1.02 ·10−1 ±1.8·10−3
heart-disease 16 mean 6.72 ·10−1 ±1.3·10−2 3.63 ·10−1 ±1.4·10−2 3.50 ·10−1 ±1.1·10−2 3.40 ·10−1 ±5.7·10−3
std. 3.47 ·10−1 ±3.6·10−11 7.21 ·10−2 ±2.3·10−3 4.44 ·10−2 ±1.6·10−3 3.45 ·10−2 ±1.5·10−3
heart-attack 14 mean 9.63 ·10−1 ±2.3·10−2 1.87 ·10−1 ±1.9·10−2 1.83 ·10−1 ±6.2·10−3 1.69 ·10−1 ±1.5·10−3
std. 5.47 ·10−1 ±1.0·10−6 8.66 ·10−2 ±3.2·10−3 7.91 ·10−2 ±2.0·10−3 7.15 ·10−2 ±3.2·10−3
loan 12 mean 4.83 ·10−1 ±1.6·10−2 1.97 ·10−1 ±1.2·10−2 1.80 ·10−1 ±9.3·10−3 1.76 ·10−1 ±5.3·10−3
std. 4.55 ·10−1 ±2.1·10−15 7.94 ·10−2 ±7.0·10−3 6.32 ·10−2 ±1.8·10−3 5.28·10−2 ±1.1·10−3
Table23. MAEcalculatedbetweenlearnedmeanandstandarddeviationtoHMCsamplesforBayesianlogisticregressionwithMSC
with8parallelchains(MSC8c).Resultsareaveragedover3runs.
d MAE MSC8c(N=1) MSC8c(N=4) MSC8c(N=2) MSC8c(N=16)
sonar 61 mean 4.54 ·10−1 ±2.3·10−3 1.70 ·10−1 ±8.2·10−3 1.37 ·10−1 ±6.9·10−3 1.33 ·10−1 ±8.1·10−3
std. 7.96 ·10−1 ±7.3·10−4 1.40 ·10−1 ±2.9·10−3 1.06 ·10−1 ±4.7·10−3 7.26 ·10−2 ±1.6·10−3
ionosphere 35 mean 7.61 ·10−1 ±9.7·10−3 2.11 ·10−1 ±1.2·10−2 2.15 ·10−1 ±4.6·10−3 2.14 ·10−1 ±5.0·10−3
std. 5.01 ·10−1 ±2.7·10−3 1.30 ·10−1 ±1.5·10−3 1.09 ·10−1 ±4.6·10−3 8.55 ·10−2 ±2.4·10−4
heart-disease 16 mean 6.73 ·10−1 ±5.9·10−3 3.43 ·10−1 ±1.2·10−2 3.46 ·10−1 ±5.2·10−4 3.37 ·10−1 ±3.2·10−3
std. 2.67 ·10−1 ±6.8·10−3 5.33 ·10−2 ±6.4·10−3 3.77 ·10−2 ±4.0·10−3 2.81 ·10−2 ±1.0·10−3
heart-attack 14 mean 9.87 ·10−1 ±5.1·10−3 1.79 ·10−1 ±6.2·10−3 1.75 ·10−1 ±4.7·10−3 1.75 ·10−1 ±1.1·10−3
std. 4.66 ·10−1 ±3.4·10−3 8.18 ·10−2 ±6.5·10−3 7.91 ·10−2 ±1.4·10−3 7.41 ·10−2 ±3.5·10−3
loan 12 mean 4.88 ·10−1 ±8.3·10−3 1.83 ·10−1 ±7.9·10−3 1.72 ·10−1 ±4.8·10−3 1.76 ·10−1 ±8.9·10−4
std. 3.71 ·10−1 ±7.5·10−3 6.09 ·10−2 ±7.1·10−3 5.45 ·10−2 ±5.5·10−4 5.08 ·10−2 ±1.6·10−3
22