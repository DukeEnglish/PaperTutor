Characterizing Dynamical Stability of Stochastic Gradient
Descent in Overparameterized Learning
Dennis Chemnitz, Maximilian Engel
July 30, 2024
Abstract
For overparameterized optimization tasks, such as the ones found in modern machine
learning, global minima are generally not unique. In order to understand generalization
in these settings, it is vital to study to which minimum an optimization algorithm con-
verges. The possibility of having minima that are unstable under the dynamics imposed
by the optimization algorithm limits the potential minima that the algorithm can find. In
this paper, we characterize the global minima that are dynamically stable/unstable for both
deterministic and stochastic gradient descent (SGD). In particular, we introduce a charac-
teristic Lyapunov exponent which depends on the local dynamics around a global minimum
and rigorously prove that the sign of this Lyapunov exponent determines whether SGD can
accumulate at the respective global minimum.
1 Introduction
Sincethesuccessof“AlexNet”[KSH12]in2012, theoverparameterizationparadigmhasbecome
ubiquitous in modern machine learning. While it is well known that artificial neural networks
with sufficiently many parameters can approximate arbitrary goal functions (see e.g. [HSW89]),
thetrainingprocess,especiallyforoverparameterizednetworks,isnotwellunderstoodandleaves
many open questions. For one, the fact that the loss landscapes are usually non-convex, makes
it difficult to rigorously guarantee that the optimization algorithms converge to global minima.
Another key open problem in understanding the training of overparameterized networks is
that of generalization (see e.g. [ZBH+21]). Classical wisdom suggests that overparameterized
networks should suffer from overfitting problems due to their high expressivity. In practice,
however, this is not the case. Put shortly, sufficiently large (i.e. deep, wide or both) artificial
neural networks are capable of fitting arbitrary data, due to their large number of parameters
(typically weights and biases). Thus, finding a set of parameters for which the network interpo-
lates the training data does not in itself indicate that the network makes reasonable predictions
for inputs outside the training data set. However, the parameters found by common optimiza-
tion algorithms tend to perform well on unseen data. This is essential for the success of modern
machine learning.
Onepossibleexplanationforthisphenomenonisthatofdynamicalstability(seee.g[WME18]).
Since the learning rate is in practice not infinitesimally small, some global minima can become
dynamically unstable, in the sense that the optimization algorithm will not converge to these
solutions, even if it is initialized arbitrarily close to them. This excludes unstable optimal solu-
tions from being found by the optimization algorithm, thereby reducing the effective hypothesis
class to stable global minima.
1
4202
luJ
92
]GL.sc[
1v90202.7042:viXraFor gradient descent, a second order Taylor expansions shows that a global minimum is only
stable if the largest eigenvalue of the Hessian of the empirical loss is less than 2/η, where η is
the learning rate. Thus gradient descent can only converge to minima which are sufficiently flat,
which has been been associated to good generalization (see e.g. [HS97]). Moreover, numerical
experiments [CKL+20] have demonstrated that (deterministic) gradient descent operates at the
edge of stability, meaning that the largest eigenvalue of the Hessian oscillates around the critical
value 2/η for the later stages of training.
Transferring these ideas to the stochastic case requires an appropriate notion of dynamical
stability for stochastic gradient descent [WME18, MY21]. The goal of this work is to estab-
lish a coherent mathematical framework in which the dynamical stability of deterministic and
stochastic optimization algorithms can be investigated and the convergence properties of global
minima can be characterized.
1.1 Contributions
Weinvestigatewhichglobalminimaoftheempiricallossfunctioncanbeobtainedaslimitsofthe
two most fundamental optimization algorithms, namely gradient descent (GD) and stochastic
gradientdescent(SGD).Specifically,weconsiderthelimitsofGDandSGDasrandomvariables,
denoted by XGD and XSGD, respectively. These random variables depend on the randomness
lim lim
during initialization in the case of GD and on the randomness during initialization and training
in the case of SGD. For a given global minimum x∗ of the empirical loss L, we introduce the
quantity
µ(x∗) := log∥1−ηHessL(x∗)∥.
Asisreadilyseeninsection2.3,theabovecondition∥HessL(x∗)∥ < 2 isequivalenttoµ(x∗) < 0.
η
We choose this reformulation because it has a direct analogue for SGD.
Our main contributions are the following.
• We introduce a set of mild conditions (cf. (H1), (H2), (H3) below) under which we can
rigorously prove that the sign of µ characterizes the support of XGD (cf. Theorem A).
lim
• For global minima x∗, we introduce a new quantity, denoted by λ(x∗) (cf. (2.12) below),
which can be seen as a characteristic Lyapunov exponent ([Ose68]) and corresponds to a
new notion of dynamic stability/instability for SGD. It should be noted that this notion
differs1 from the ones introduced in [WME18] and [MY21] (cf. Appendix A).
• Under an additional mild assumption on the global minimum in question, we rigorously
prove that the sign of λ characterizes the support of XSGD (cf. Theorem B).
lim
This clarifies the concept of dynamical stability for stochastic gradient descent and estab-
lishes a connection to a substantial body of mathematical and physical literature on Lyapunov
exponents (see e.g. [Ose68, BL85, Arn98]). Moreover, it provides a foundation for new theoretic
and numerical investigations of the learning process under stochastic gradient descent. Possible
questions of interest include how the value of λ evolves during training with stochastic gradient
descent and which implications a small, respectively large, value of λ has on generalization.
From a mathematical perspective our analysis of SGD amounts to a study of the asymptotic
behavior of discrete-time random dynamical systems (see e.g. [Arn98] and also section 3.5)
1Inparticularournotionoflinarstabilityisstrictlyweakerthantheoneintroducedin[WME18, Defintion1]
(cf. Appendix A).
2possessing a manifold of equilibria. In the unstable case, we use a method based on moment
Lyapunov exponents [AKO86]. A more detailed discussion of this approach is given in section
3.1 an throughout the proofs.
For simplicity, we restrict our study to scalar regression problems and, in the case of SGD,
to mini-batch size 1, but we point out that, with minor modifications, our main result applies
to a broader class of tasks and algorithms (cf. section 2.6 below).
1.2 Literature
Closest to our work is a series of two papers [WME18, MY21], which studies the dynamical
stabilityofstochasticgradientdescent. WeexplaintherelationtothisworkindetailinAppendix
A. Convergence rates for SGD have been studied in
The most critical part of our proof based on techniques originally developed to study syn-
chronization in stochastic differential equations [BS88, Bax91], which has seen recent attention
motivated by fluid dynamics [CZH21, BBPS22, BCZG23].
The edge of stability phenomenon for gradient descent mentioned in the introduction has
been numerically observed in [WME18, CKL+20] and theoretically studied in [ALP22]. More-
over, the difference between global minima found by GD and SGD [KMN+17, WME18] and
SGD and Adam [KS17] has been investigated numerically.
The dynamics of stochastic gradient descent has also been studied using approximations by
stochastic differential equations, called stochastic modified equations (see e.g. [LTE17, LTE19]).
We note that such an approach is not compatible with our analysis, as it assumes an infinites-
imally small learning rate. Moreover, stochastic modified equations have been extended to a
stochastic flow in [GKK24], building a framework analogous to the one we introduce in section
3.5.
1.3 Organization of the Paper
We start by introducing the fundamental setting of the optimization task (section 2.1), as well
as the learning algorithms we consider (2.2). In section 2.3, we heuristically derive the notions
of linear stability for GD and SGD. These derivations are made rigorous in our main results,
which we present in section 2.4. In section 2.5, the main quantities of interest, µ and λ, are
related to the neural tangent kernel [JGH18]. Finally, we discuss possible generalizations of our
main results in section 2.6. Section 3 provides the proofs of the main theorems. An overview
over the structure of the proofs is given in section 3.1.
2 Setting and Main Result
2.1 Network Model
In the following, we consider a scalar regression problem. Let fˆ: Rd → R be a ground-truth
function, which is supposed to be reconstructed from N given data pairs (y ,z = fˆ(y )) ,
i i i i∈[N]
where [N] = {1,...,N}. To do so, we consider a parameterized network model given by a
smooth function2 F : RD ×Rd → R and try, using the given data, to find an x ∈ RD such that
2Forthepurposeofbeinggeneral,wewillnotspecifyhowthenetworkfunctionFlookslike. Typicalnetwork
architectures used in practice include fully connected networks, convolutional neural networks (image classifica-
tion) and transformers (large language models).
3F(x,·) ≈ fˆ. Absent of other information, it is reasonable to prescribe
1 (cid:88)
x ∈ argmin ℓ(F(x,y ),z ), (2.1)
i i
N
i∈[N]
for some loss-function ℓ : R × R → R , with ℓ(z¯,z) = 0 if and only if z¯ = z, ∂ ℓ(z,z) = 0
≥0 1
and ∂2ℓ(z,z) = 1. Even though other choices for ℓ are possible, it is advised to think of the
1
square loss function ℓ(z¯,z) := 1(z¯−z)2. For each i ∈ [N], we define the individual loss function
2
L : RD → [0,∞) by
i
L (x) := ℓ(F(x,y ),z ) (2.2)
i i i
and the empirical loss function L : RD → R as the average of the individual loss functions, i.e.
N
1 (cid:88)
L(x) := L (x). (2.3)
i
N
i=1
Equation (2.1) can thus be rewritten as
x ∈ argminL(x). (2.4)
If the number of given training examples N exceeds the number of learnable parameters D, the
problem is overdetermined and the global minimum of L is usually unique, such that x is fully
determined by (2.4). However, in general it will not be possible to find an x ∈ RD such that
F(x,·) interpolates the given data, i.e. the set
M := (cid:8) x ∈ RD : L(x) = 0(cid:9) = (cid:8) x ∈ RD : F(x,y ) = z , ∀i ∈ [N](cid:9)
i i
will be empty.
On the other hand, if the number of learnable parameters exceeds the number of training
examples, i.e. D > N, the optimization problem is overparameterized. As described in the
introduction, this will be the case of interest here. Given a sufficiently expressive network model
F(meaningthatF(x,·)canexpressasufficientlyrichfamilyoffunctions),thesetofinterpolation
solutions M will usually be infinite. In fact, if the set of gradients
{∇ F(x,y ) : i ∈ [N]} ⊂ RD
x i
islinearlyindependentforeveryx ∈ M, thesetMisanembeddedsmooth(D−N)-dimensional
submanifold of RD with normal space
N(x) = span{∇ F(x,y ) : i ∈ [N]} (2.5)
x i
and tangent space
T(x) = N(x)⊥ = {v ∈ RD : wtv = 0, ∀w ∈ N(x)}. (2.6)
As a consequence of Sard’s theorem (see e.g. [Mil97]), this condition will be satisfied for generic
training data3 and in the following we will assume that M is a smooth manifold as described
(cf. Hypothesis (H1) below).
While x ∈ M ensures F(x,y ) = fˆ(y ) for all i ∈ [N], it does not guarantee F(x,y) ≈ fˆ(y) for
i i
anyy outsidethetrainingdataset. Thisisthegeneralizationgapmentionedintheintroduction.
If M is infinite, (2.4) does not determine x uniquely and the question of generalization is highly
dependent on which x ∈ M is chosen.
3Put more precisely, for every smooth network model F and every choice of (y ,...,y ), there exists a set
1 N
Z ⊆ RN of full Lebesgue measure, such that, if (z ,...,z ) ∈ Z, then M is an embedded smooth (D−N)-
1 N
dimensional manifold (cf. [Coo21]).
42.2 Learning Algorithms
In practice, a wide range of optimization algorithms (cf. e.g. [SSH21]) are used to find a global
minimum x∗ ∈ M. In this work, we will restrict our study to the two most basic optimization
algorithms: gradient descent (GD) and stochastic gradient descent (SGD). Both algorithms are
initializedatarandompointXGD/SGD ∈ RD, whichisdistributedaccordingtosomeprobability
0
measure ν : B(RD) → [0,1], called initial distribution.4 We will assume that ν is equivalent to
Lebesgue measure5 (cf. Hypothesis (H2) below).
Gradient descent tries to successively reduce the empirical loss L by taking small steps in
the opposite direction of its gradient ∇L. Formally the update rule is given by
XGD := XGD−η∇L(XGD), (2.7)
n+1 n n
where η > 0 is a small real number called learning rate. Ideally, the hope is that gradient
descent converges to some point XGD ∈ M as n tends to infinity. While gradient descent is
lim
remarkably reliable at finding global minima in practice, convergence to a global minimum (or
even convergence in the first place) can not be guaranteed in our general setting, since L is
generally non-convex. Therefore we define XGD as an M∪{∅}-valued random variable by
lim
(cid:40)
lim XGD , if (XGD) converges to some point in M,
XGD := n→∞ n n
lim
∅ , otherwise,
accounting for a possible failure of convergence. Note that the randomness in XGD stems purely
lim
from the random initialization XGD. Once XGD is drawn all subsequent steps and thus XGD
0 0 lim
are deterministic.
In contrast, stochastic gradient descent is a stochastic optimization algorithm. After initial-
izing XSGD randomly according to the measure ν, SGD updates are performed according to the
0
gradients of the individual loss functions L (2.2), corresponding to randomly chosen training
i
data. Concretely, we set
XSGD := XSGD−η∇L (XSGD), (2.8)
n+1 n ξn+1 n
where (ξ n) n∈N is a sequence of independent random variables, which are uniformly distributed
over [N]. Note that ∇L can be interpreted as an unbiased estimator for the gradient ∇L of
ξn
the empirical loss function L (2.3) since
E[∇L (x)] = ∇L(x),
ξn
for each x ∈ RD and each n ∈ N. Unfortunately, the problems with guaranteeing convergence
for SGD persist such that, again, we define XSGD as a M∪{∅}-valued random variable by
lim
(cid:40)
lim XSGD , if (XSGD) converges to some point in M,
XSGD := n→∞ n n
lim
∅ , otherwise.
Inthefollowing, weareinterestedinstudyingwhichpointsGDandSGDcanfind. Putmore
GD/SGD
precisely, we aim to characterize the support of the random variable X , which is defined
lim
as
(cid:16) (cid:17) (cid:110) (cid:16) (cid:17) (cid:111)
supp XGD/SGD := x ∈ M : U ⊆ M open nbhd. of x ⇒ P XGD/SGD ∈ U > 0 .
lim lim
4Here and in the following, B(RD) denotes the family of Borel sets.
5This is for example consistent with ν being a normal distribution, which is the most common choice in
practice.
52.3 Linear Stability
If (stochastic) gradient descent is randomly initialized at some x ∈ M or reaches M after some
finite number of iterations, it remains stationary from there on, i.e.
XGD/SGD ∈ M ⇒ XGD/SGD = XGD/SGD = XGD/SGD , ∀m > n.
n n m lim
This can be seen easily by computing the individual and empirical loss functions as
∇L (x) = ∂ ℓ(F(x,y ),z )∇ F(x,y ),
i 1 i i x i
N
1 (cid:88)
∇L(x) = ∂ ℓ(F(x,y ),z )∇ F(x,y )
1 i i x i
N
i=1
and noting that the right-hand sides vanish if x ∈ M. Since the initial distribution ν is assumed
GD/SGD
to have full support, it is a priori possible that X takes any value in M. However, under
lim
mild assumptions (cf. (H2) and (H3) below), the optimization algorithm can only reach a global
minimum in a finite number of steps with probability zero. In other words, (S)GD will usually
converge to some global minimum, say x∗ ∈ M, without reaching it in a finite number of steps.
This is only possible, if x∗ is dynamically stable, meaning that, once the optimization algorithms
gets sufficiently close to x∗, it will stay close.
In order to study dynamical stability at some x∗ ∈ M, we linearize the optimization step
(2.7) around x∗. We introduce the function G : RD → RD by
η
G (x) := x−η∇L(x), (2.9)
η
such that XGD = G(XGD). If x is close to x∗, by differentiability, G (x) is well approximated
n+1 n η
by
G (x) = x∗+G′(x∗)(x−x∗)+o(∥x−x∗∥).
η η
Here, G′(x∗) ∈ RD×D denotes the Jacobian of G at x∗. Consequently, for several iterations, we
η η
have
Gn(x) = x∗+G′(x∗)n(x−x∗)+o(∥x−x∗∥).
η η
The Jacobian G′(x∗) can be computed as
η
N
η (cid:88)
G′(x∗) = 1 −ηHessL(x∗) = 1 − ∇ F(x∗,y )∇ F(x∗,y )t. (2.10)
η D D N x i x i
i=1
Notethat,bydefinitionofthenormalspaceN (2.5)andtangentspaceT (2.6),thematrixG′(x∗)
η
satisfies G′(x∗)v = v ∈ T(x∗) for all v ∈ T(x∗) and G′(x∗)w ∈ N(x∗) for all w ∈ N(x∗). Thus,
η η
G′(x∗) respects the splitting RD = T(x∗)⊕N(x∗) and the restriction G′(x∗)| : T(x∗) →
η η T(x∗)
T(x∗) equals the identity. We say that x∗ is linearly stable under GD with learning rate η if
1
µ(x∗) := lim log(∥G′(x∗)n| ∥) = log(ρ (G′(x∗)| )) < 0.
n→∞ n η N(x∗) Spec η N(x∗)
Here ∥·∥ denotes the operator norm and ρ the spectral radius of a matrix which is given by
Spec
ρ (A) := lim ∥An∥1/n = max{|λ| : λ is an eigenvalue of A}.
Spec
n→∞
6Conversely, if µ(x∗) > 0, we say that x∗ is linearly unstable under GD. By (2.10), we have
Spec(G′(x∗)| ) = 1−ηSpec(HessL(x∗)| ).
η N(x∗) N(x∗)
While the Hessian HessL(x∗) is only positive semi-definite, its restriction to N(x∗) is even
positive definite and thus
Spec(HessL(x∗)| ) ⊂ R .
N(x∗) >0
In particular, the Hessian HessL(x∗)| is a symmetric positive definite matrix. Therefore,
N(x∗)
the condition for linear stability µ(x∗) < 0 can be equivalently expressed as
µ(x∗) < 0 ⇔ Spec(G′(x∗)| ) ⊂ (−1,1) ⇔ Spec(HessL(x∗)| ) ⊂ (0,2/η)
η N(x∗) N(x∗)
2 2
⇔ ∥HessL(x∗)| )∥ < ⇔ ∥HessL(x∗)∥ < ,
N(x∗)
η η
where the last equivalence holds if η < 2. While this formulation is more common in the
literature (cf. e.g. [WME18, CKL+20, ALP22]), we will stick to the expression µ(x∗) < 0 here,
as it can be more easily extended to stochastic gradient descent.
Ourstabilityanalysisforstochasticgradientdescentisanalogoustothatforgradientdescent.
For notational convenience we introduce the functions G : RD → RD for each i ∈ [N] by
η,i
G (x) := x−η∇L (x). (2.11)
η,i i
Their Jacobians at some global minimum x∗ ∈ M can be computed as
G′ (x∗) = 1 −ηHessL (x∗) = 1 −η∇ F(x∗,y )∇ F(x∗,y )t.
η,i D i D x i x i
For points x ∈ RD close to x∗, we have
(cid:2) G ◦···◦G (cid:3) (x) = x∗+(cid:2) G′ (x∗)...G′ (x∗)(cid:3) (x−x∗)+o(∥x−x∗∥).
η,ξn η,ξ1 η,ξn η,ξ1
NotethatthematricesG′ (x∗)alsoallrespectthesplittingRD = T(x∗)⊕N(x∗)withG′ (x∗)| =
η,i η,i T(x∗)
Id . Analogously to gradient descent, we would like to call x∗ linearly stable, if
T(x∗)
λ(x∗) = nl →im
∞
n1 log(cid:0)(cid:13) (cid:13)G η′ ,ξn(x∗)...G η′ ,ξ1(x∗)| N(x∗)(cid:13) (cid:13)(cid:1) < 0.
Atfirstglance,thisdefinitionraisestwoproblems. Firstly,itisnotclearwhetherthelimitexists.
Secondly, the value of λ(x∗) seems to depend on the random sequence (ξ ,ξ ,...). However,
1 2
Kingman’s sub-additive ergodic theorem ([Kin68], see also [Ste89])6 states that for almost every
realization (ξ ,ξ ,...) we have
1 2
nl →im
∞
n1 log(cid:0)(cid:13) (cid:13)G η′ ,ξn(x∗)...G η′ ,ξ1(x∗)| N(x∗)(cid:13) (cid:13)(cid:1) = nin ∈f
N
n1 E(cid:2) log(cid:0)(cid:13) (cid:13)G η′ ,ξn(x∗)...G η′ ,ξ1(x∗)| N(x∗)(cid:13) (cid:13)(cid:1)(cid:3) .
Hence, using the right-hand side as a definition for λ(x∗), i.e.
λ(x∗) := nin ∈f
N
n1 E(cid:2) log(cid:0)(cid:13) (cid:13)G η′ ,ξn(x∗)...G η′ ,ξ1(x∗)| N(x∗)(cid:13) (cid:13)(cid:1)(cid:3) ∈ [−∞,∞), (2.12)
6The integrability condition for Kingman’s ergodic theorem holds trivially here, as we only consider finitely
man different matrices.
7solves both issues. It should be noted that the sequence
n (cid:55)→ n1 E(cid:2) log(cid:13) (cid:13)G η′ ,ξn(x∗)...G η′ ,ξ1(x∗)| N(x∗)(cid:13) (cid:13)(cid:3)
is monotonically decreasing and one may equivalently define λ(x∗) by
λ(x∗) := nl →im
∞
n1 E(cid:2) log(cid:13) (cid:13)G η′ ,ξn(x∗)...G η′ ,ξ1(x∗)| N(x∗)(cid:13) (cid:13)(cid:3) ∈ [−∞,∞). (2.13)
We will call x∗ linearly stable (or linearly unstable) under stochastic gradient descent if
λ(x∗) < 0 (or λ(x∗) > 0, respectively).7
2.4 Main Result
The main contribution of our work is to rigorously show that the definitions for linear stabil-
GD/SGD
ity/instability, heuristically derived in section 2.3, characterize the supports of X and
lim
thus characterize the qualitative implicit bias of gradient descent/stochastic gradient descent.
In our derivation we made some assumptions, which can be formalized as follows.
Hypothesis (H1). For each x ∈ M the set of vectors
{∇ F(x,y ) : i ∈ [N]} ⊂ RD
x i
is linearly independent. In particular, M is an embedded sub-manifold of RD.
Hypothesis (H2). The initial distribution ν is equivalent to Lebesgue measure, i.e.
ν(B) = 0 ⇔ Leb(B) = 0, ∀B ∈ B(RD).
Recallthat, byanargumentin[Coo21], theHypothesis(H1)isfulfilledforgenerictrainingdata,
while (H2) holds for the most common choice of initial distribution.
Furthermore, the heuristic argument presented in the previous section relied on the assump-
tion that with probability one the optimization algorithms do not reach M in a finite number
of steps. Hypothesis (H2) ensures that, with probability one, the algorithms are not initialized
on M. However, the setting presented so far is too general to exclude that the optimization
algorithmsreachMafteranypositivefinitenumberofiterationswithpositiveprobability. Thus
we require an additional assumption. From now on, consider a fixed learning rate η > 0.
Hypothesis (H3). The map G and the maps G ,...,G , defined in (2.9) and (2.11), are
η η,1 η,N
non-singular, i.e. the pre-image of every Lebesgue-null set is a Lebesgue-null set.
In particular, a continuously differentiable map G : RD → RD is non-singular, if its Jacobian
is invertible Lebesgue almost everywhere. Whether this is true for the maps G ,G ,...,G
η η,1 η,N
depends on the network function F. Yet it is reasonable to assume that this should be satisfied
for the common neural network architechtures, at least for almost every learning rate η.
GD/SGD
Recall that the support of X is defined as
lim
(cid:16) (cid:17) (cid:110) (cid:16) (cid:17) (cid:111)
supp XGD/SGD := x ∈ M : U ⊆ M open nbhd. of x ⇒ P XGD/SGD ∈ U > 0 .
lim lim
For gradient descent, we will show the following main result.
7As mentioned in the introduction, this definition differs from the ones introduced in [WME18] and [MY21].
See Appendix A for a detailed comparison.
8Theorem A. Suppose (H1), (H2) and (H3) are satisfied. Let x∗ ∈ M.
(i) If µ(x∗) < 0, then x∗ ∈ supp(cid:0) XGD(cid:1) .
lim
(ii) If µ(x∗) > 0, then x∗ ∈/ supp(cid:0) XGD(cid:1) .
lim
The analogous result for stochastic gradient descent requires some extra assumptions on the
global minimum x∗ in question.
Definition 2.1. A global minimum x∗ ∈ M is said to be regular, if
(i) for every i ∈ N, we have
(cid:26) (cid:27)
1 2
∥∇ F(x∗,y )∥2 ∈/ , , and (2.14)
x i
η η
(ii) there exists no proper sub-set ∅ ⊊ A ⊊ [N], such that
∇ F(x∗,y )·∇ F(x∗,y ) = 0, ∀i ∈ A, j ∈ [N]\A.
x i x j
Note that almost every family of vectors (∇ F(x∗,y )) will satisfy these conditions. Thus
x i i∈[N]
it is reasonable to assume that, for most network functions F, almost every global minimum
x∗ ∈ M is regular. With this definition in hand, we get the following result for SGD.
Theorem B. Suppose (H1), (H2) and (H3) are satisfied. Let x∗ ∈ M be regular.
(i) If λ(x∗) < 0, then x∗ ∈ supp(cid:0) XSGD(cid:1) .
lim
(ii) If λ(x∗) > 0, then x∗ ∈/ supp(cid:0) XSGD(cid:1) .
lim
Remark 2.2. Both Theorem A and Theorem B fail to address the case µ(x∗) = 0, respectively
λ(x∗) = 0. Since the support of XGD/SGD is closed by definition and µ : M → R is continuous,
lim
most points with µ(x∗) = 0 should have global minima x′ ∈ M with µ(x′) < 0 nearby and
thus be in the support of XGD. Making this argument rigorous would require more precise
lim
knowledge of the network function F and is outside the scope of this paper. Continuity of λ is
a more subtle issue. In general, λ : M → R is only upper semi-continuous8. However, a recent
result by Avila, Esken and Viana [AEV23] shows that λ is continuous at all points x∗, where the
matrices G′ (x∗),...,G′ (x∗) are all invertible. This is, in particular, the case for all regular
η,1 η,N
x∗ ∈ M.
2.5 Relation to the Neural Tangent Kernel
We point out that, for any x∗ ∈ M, the values of µ(x∗) and λ(x∗) can be deduced entirely from
the neural tangent kernel K : Rd×Rd → R, first introduced in [JGH18], which is defined by
x∗
K (y,y′) = ∇ F(x∗,y)t∇ F(x∗,y′),
x∗ x x
or more specifically, its Gram matrix G ∈ RN×N given by
x∗
[G ] := K (y ,y ).
x∗ i,j x∗ i j
8This can readily be seen from the fact that it is defined as an infimum of continuous function.
9In more detail, let S ∈ RD×N denote the matrix whose i-th column is given by ∇ F(x∗,y ),
x∗ x i
i.e.
 
| |
S
x∗
:= ∇ xF(x∗,y 1) ... ∇ xF(x∗,y N). (2.15)
| |
Clearly, S maps RN isomorphically onto N(x∗) and straightforward calculations, using linear
x∗
independence of the gradients, show that
1
[HessL(x∗)]S = S G and [HessL (x∗)]S = S G ,
x∗
N
x∗ x∗ i x∗ x∗ x∗,[i]
where G denotes the matrix G with every but the i-th row set to zero. It should be noted
x∗,[i] x∗
that S is not a square matrix and thus not invertible as a matrix. However as an isomorphism
x∗
from RN to N(x∗) it can be inverted and we let S−1 ∈ RN×D denote the matrix associated to
x∗
this inverse isomorphism. It satisfies S−1S = 1 and (S S−1)| = 1 | . Thus the
x∗ x∗ N x∗ x∗ N(x∗) D N(x∗)
restrictions of the Jacobians of G and G to N(x∗) can be expressed as
η η,i
(cid:16) η (cid:17)
G′(x∗)| = S 1 − G S −1 (2.16)
η N(x∗) x∗ N N x∗ x∗
and
G′ (x∗)| = S (cid:0)1 −ηG (cid:1) S −1. (2.17)
η,i N(x∗) x∗ N x∗,[i] x∗
From this we get ρ (G′(x∗)| ) = ρ (1 − ηG ) and, using the symmetry of (1 −
Spec η N(x∗) Spec N N x∗ N
ηG ), also
N x∗
(cid:16) η (cid:17) (cid:13) η (cid:13)
µ(x∗) = ρ 1 − G = (cid:13)1 − G (cid:13). (2.18)
Spec N N x∗ (cid:13) N N x∗(cid:13)
For λ(x∗) an analogous statement can be obtained. Indeed, we have the estimates
log(cid:13) (cid:13)(cid:0) G η′ ,ξn(x∗)...G η′ ,ξ1(x∗)(cid:1) | N(x∗)(cid:13) (cid:13)
=
log(cid:13)
(cid:13)S
x∗(cid:0)1
N −ηG
x∗,[ξn](cid:1) ...(cid:0)1
N −ηG
x∗,[ξ1](cid:1)
S
x∗−1(cid:13)
(cid:13)
≤ log∥S
x∗∥+log(cid:13) (cid:13)(cid:0)1
N −ηG
x∗,[ξn](cid:1) ...(cid:0)1
N −ηG
x∗,[ξ1](cid:1)(cid:13) (cid:13)+log(cid:13)
(cid:13)S
x∗−1(cid:13)
(cid:13)
and
log(cid:13) (cid:13)(cid:0)1
N −ηG
x∗,[ξn](cid:1) ...(cid:0)1
N −ηG
x∗,[ξ1](cid:1)(cid:13)
(cid:13)
= log(cid:13) (cid:13)S x∗−1(cid:0) G η′ ,ξn(x∗)...G η′ ,ξ1(x∗)(cid:1) S x∗(cid:13) (cid:13)
≤ log(cid:13) (cid:13)S x∗−1(cid:13) (cid:13)+log(cid:13) (cid:13)(cid:0) G η′ ,ξn(x∗)...G η′ ,ξ1(x∗)(cid:1) | N(x∗)(cid:13) (cid:13)+log∥S x∗∥.
Together they imply
λ(x∗) = nl →im
∞
n1 E(cid:2) log(cid:13) (cid:13)G η′ ,ξn(x∗)...G η′ ,ξ1(x∗)| N(x∗)(cid:13) (cid:13)(cid:3)
= nl →im
∞
n1 E(cid:2) log(cid:13) (cid:13)(cid:0)1
N
−ηG
x∗,[ξn](cid:1) ...(cid:0)1
N
−ηG
x∗,[ξ1](cid:1)(cid:13) (cid:13)(cid:3)
= nin ∈f
N
n1 E(cid:2) log(cid:13) (cid:13)(cid:0)1
N
−ηG
x∗,[ξn](cid:1) ...(cid:0)1
N
−ηG
x∗,[ξ1](cid:1)(cid:13) (cid:13)(cid:3)
. (2.19)
10Thisobservationisusefulfortworeasons. Ontheonehanditconnectsthecentralquantities
in the present work to an ongoing line of research of neural networks in the infinite width limit
[JGH18]. On the other hand, it turns out that the expressions (2.18) and (2.19) are nicer to
work with than the corresponding expressions in the previous section and will be exclusively
used in the proofs of the main theorems.
2.6 Possible Extensions and Outlook
While our main results are general in the sense that they require only very weak assumptions on
the network function F, we want to point out that we only consider scalar regression problems
and training with gradient descent or stochastic gradient descent with step size 1. As mentioned
above, we have made these restrictions to simplify the already extensive proofs as much as
possible. Themainresultscanbeextended, withminormodifications, toamoregeneralsetting.
In practice SGD is usually implemented with so-called mini-batches to allow for parallel
computations. For SGD with mini-batches of size 1 ≤ B ≤ N, the iterative step (2.8) is
replaced by
XSGD := XSGD−η∇L (XSGD),
n+1 n Ξn+1 n
(cid:18) (cid:19)
N
where (Ξ n) n∈N is an i.i.d. sequence of size B subsets Ξ n ⊆ [N] chosen uniformly from the B
possible subsets and
1 (cid:88)
L (x) := L (x).
Ξ i
B
i∈Ξ
For B = 1 this coincides with the SGD algorithm presented in section 2.2 and for B = N the
iterative step is deterministic and coincides with GD. Stochastic gradient descent with mini-
batch size 1 < B < N can thus be seen as an interpolation between the two algorithms. The
derivations from sections 2.3 and 2.5 can be carried out analogously for mini-batch SGD and
one can express the Lyapunov exponent of mini-batch SGD with learning rate η and mini-batch
size B as
1 (cid:104) (cid:13)(cid:16) η (cid:17) (cid:16) η (cid:17)(cid:13)(cid:105)
λ(x∗) = inf E log(cid:13) 1 − G ... 1 − G (cid:13) ,
n∈N n (cid:13) N B x∗,[Ξn] N B x∗,[Ξ1] (cid:13)
where G denotes the Gram matrix G of the neural tangent kernel with all rows whose
x∗,[Ξ] x∗
index is not in Ξ set to 0. In order to derive a version of Theorem B it is necessary to adapt
the notion of regular global minima (cf. Definition 2.1) to ensure that the analog of Lemma 3.9
in the proof below still remains valid. We leave this as a problem for future work. The rest of
the proof of Theorem B can be extended to mini-batch SGD with only minor changes. Such an
extension of Theorem B could help to better understand the impact of the learning rate and the
mini-batchsizeongeneralizationproperties(seee.g.[HHS17,GDG+17,KMN+17]fornumerical
studies).
Furthermore it would be interesting to extend our analysis to other versions of stochastic
gradient descent such as SGD with momentum [RHW86] or Adam [KB14]. The derivations in
section 2.3 can be extended to these algorithms by linearizing the iteration steps around fixed
points where all moment terms are zero. In the case of Adam, the parameter ϵ appearing in the
numeratorofthefinalupdatestepisusuallychosenextremelysmall([KB14]suggestsϵ = 10−8).
Thus the linearization only approximates the dynamics of the actual algorithm in a vanishingly
small neighborhood and it is questionable whether it is still meaningful for the dynamics of the
algorithm.
11Our analysis can be extended to multi-dimensional regression problems with ground truth
function fˆ: Rd → Rd˜ and network function F : RD×Rd → Rd˜ . In fact one can interpret such a
multi-dimensional regression problem as a scalar regression problem with ground truth function
fˆ: Rd×[d˜] → R and network function F : RD ×(Rd×[d˜]) → R. This has the effect of splitting
one training example (y,z) ∈ Rd × Rd˜ into d˜ training examples ((y,1),z ),...,((y,d˜),z ) ∈
1 d˜
(Rd ×[d˜])×R. A SGD step with the training example (y,z) now corresponds to an SGD step
withthemini-batch((y,1),z ),...,((y,d˜),z ). Similarlytomini-batchSGD,themainchallenge
1 d˜
in extending Theorem B to multi-dimensional regression problems is to adapt the notion of a
regular global minimum (cf. Definition 2.1) in such a way that Lemma 3.9 still remains valid
for multi-dimensional regression.
Unfortunately our results can not be easily extended to classification problems. For a classi-
fication task with k classes, it is common to encode the training data as pairs (y,z) ∈ Rd×Rk,
where z = e is the i-th unit vector, where i ∈ [k] is the class y belongs to. Then one considers
i
a network function F : RD ×Rd → Rk, which is then post-composed with the softmax function
softmax : Rk → [0,1]k given by
β
eβzi
softmax (z ,...z ) := ,
β 1 k i (cid:80)k eβzj
j=1
where β > 0 is some positive real number commonly referred to as inverse temperature. For
training, the most common loss function is cross-entropy loss given by
n
(cid:88)
ℓ(z¯,z) := − z log(z¯).
i i
i=1
Inordertohaveℓ(z¯,z) = 0,onenecessarilyneedsz¯= z. However,thesoftmaxfunctionsoutputs
are in (0,1)k for any finite input values. Thus it is not possible to reach training error zero and
M = ∅.
Finally, it should be mentioned that our analysis assumes that the model is trained to
convergence. In practice, it has been observed that stopping the algorithm early can improve
generalization. While the global minima with λ(x∗) > 0 are asymptotically unstable, the the
finite time Lyapunov exponents
λ w,n(x∗) := n1 log(cid:13) (cid:13)(cid:0)1 N −ηG x∗,[ξn](cid:1) ...(cid:0)1 N −ηG x∗,[ξ1](cid:1) w(cid:13) (cid:13)
can,evenforlargen,stillbenegativewithsmall,butpositiveprobability. Thiseffectiscaptured
by a large deviation principle ([AK87]), the rate function of which depends on the moment
Lyapunov exponents (see Appendix A for a definition).
3 Proofs of the Main Results
3.1 Overview
TheremainderofthispaperwillconsistoftheproofsofTheoremAandTheoremB.Forthemain
arguments, it will be convenient to work in a local coordinate system in which M corresponds
to a linear subspace. Such a coordinate system will be introduced in section 3.2. Theorem A (i)
then follows from an elementary argument presented in section 3.3. In order to prove Theorem
A (ii), in section 3.4, we employ a center-stable manifold theorem. In section 3.5, we introduce
12a random dynamical systems framework to treat stochastic gradient descent. Theorem B (i)
is then proved in section 3.6. The proof is similar to the proof of Theorem A (i) presented
in section 3.3, but the possibility for non-uniform hyperbolicity adds an additional challenge.
Finally, Theorem B (ii) is proved in sections 3.7-3.9. The proof is inspired by previous work on
theinstabilityofinvariantsubspacesforstochasticdifferentialequations(seee.g.[BS88,Bax91]).
While these works rely on H¨ormander conditions [H¨or67], we use a criterion by LePage [LP82]
to establish a spectral gap for the projective semi-group. The assumptions for the criterion of
LePage are checked in section 3.7. In section 3.8, we construct a local Lyapunov function similar
to the recent works [BBPS22, BCZG23]. The proof of Theorem B is then completed in section
3.9.
Throughout the entire section 3 we assume (H1), (H2) and (H3) as standing assumptions.
3.2 Local Coordinates
In the following, for some fixed x∗ ∈ M, we introduce a local coordinate system for a neigh-
borhood of x∗, in which the generally non-linear manifold M becomes a linear subspace aligned
with the coordinate axes. Using these coordinates will be helpful in all further proofs.
Lemma 3.1. There exist an open neighborhood x∗ ∈ Uˆ ⊂ RD and an open neighborhood (0,0) ∈
Vˆ ⊆ RD−N ×RN, as well as a smooth diffeomorphism χ : Vˆ → Uˆ, such that
(i) χ(0,0) = x∗,
(ii) χ(Vˆ ∪(RD−N ×{0})) = Uˆ ∩M,
(iii) the Jacobian at the origin is given by
χ′(0,0) = (cid:0) A S (cid:1) , (3.1)
x∗
where A ∈ RD×(D−N) is some matrix which induces an isomorphism from RD−N onto
T(x∗) and S ∈ RD×N is the matrix defined in (2.15),
x∗
(iv) and χ is bi-Lipschitz with Lipschitz constant L , i.e. both χ and χ−1 are Lipschitz contin-
χ
uous with said Lipschitz constant.
Proof. Let A ∈ RD×(D−N) be some matrix which induces an isomorphism from RD−N onto
T(x∗). Since M is a embedded smooth manifold, we can find neighborhoods 0 ∈ U ⊆ T(x∗)
T
and x∗ ∈ U˜ ⊆ RD and smooth map ζ : U → N(x∗) with ζ(0) = 0, ζ′(0) = 0 and
T
{x∗+x+ζ(x) : x ∈ U } = U˜ ∩M.
T
This allows us to define a smooth map χ : A−1U ×RN → RD by
T
χ(v,w) = x∗+Av+S w+ζ(Av).
x∗
It can be easily verified that χ is injective and
χ(A−1U ×{0}) = U˜ ∩M.
T
Also, the Jacobian is given by
χ′(v,w) = (cid:0) A+ζ′(Av)A S (cid:1) ,
(x0)
13Vˆ
Uˆ
χ x∗ M
(0,0)
RD−N ×{0} Uˆ ∩M
Uˆ ∩(x∗+N(x∗))
{0}×RN
Figure 1: Schematic representation of the construction of χ. Objects with the same color are
mapped onto each other.
so in particular χ is a local diffeomorphism and (3.1) holds. Set Vˆ := χ−1(U˜) and Uˆ := χ(Vˆ).
If we now let χ be its restriction χ : Vˆ → Uˆ, it is a smooth diffeomorphism, satisfying i) to iii).
If χ is not bi-Lipschitz we can reassign Vˆ to a smaller neighborhood, which is pre-compact in
the original Vˆ. After reassigning Uˆ and restricting χ accordingly, χ will be bi-Lipschitz.
The learning dynamics of (stochastic) gradient decent can, at least locally, be lifted in the
new coordinates via the function χ. We define functions Gˆ ,Gˆ ,...Gˆ : V∗ → Vˆ by
η η,1 η,N
Gˆ (v,w) = χ−1(G (χ(v,w))) and Gˆ (v,w) = χ−1(G (χ(v,w))),
η η η,i η,i
where V∗ ⊆ Vˆ is an open set given by
 
(cid:92)
V∗ := χ−1 G η−1(Uˆ)∩ G η− ,i1(Uˆ). (3.2)
i∈[N]
Let τ : V∗ → N∪{∞} be the maximal number n for which Gˆn+1(v,w) is well defined, i.e.
η
τ(v,w) := inf{n ∈ N : Gˆn(v,w) ∈/ V∗}.9 (3.3)
η
One can easily check that for (v,w) ∈ V∗ and 1 ≤ n ≤ τ(v,w), we have
Gˆn(v,w) = χ−1(Gn(χ(v,w))) (3.4)
η η
The corresponding statement for SGD will be given in section 3.5 once the appropriate notation
has been introduced (cf. (3.20)).
Since all points in M are fixed points for Gˆ ,Gˆ ,...Gˆ and by Lemma 3.1 (ii), we have
η η,1 η,N
Vˆ ∪(RD−N ×{0}) ⊆ V∗ and
Gˆ (v,0) = (v,0) and Gˆ (v,0) = (v,0).
η η,i
9Of course, inf∅:=∞.
14Furthermore, using the chain rule, equations (2.16) and (2.17) as well as Lemma 3.1 (iii), we
can compute
(cid:18) (cid:19) (cid:18) (cid:19)
η 0 0 0 0
Gˆ′(0,0) = 1 − and Gˆ′ (0,0) = 1 −η , (3.5)
η D N 0 G x∗ η,i D 0 G x∗,[i]
wheretheright-handsidesarebothblockmatriceswithdimensions((D−N)+N)×((D−N)+N).
Lemma 3.2. For every δ > 0, there exists an open neighborhood V ⊆ V∗, such that for each
δ
(v,w) ∈ V and each i ∈ [N], we have
δ
(cid:13) (cid:18) (cid:19)(cid:13)
(cid:13) (cid:13) (cid:13)Gˆ η(v,w)−(cid:104) Gˆ η′(0,0)(cid:105) wv (cid:13) (cid:13)
(cid:13)
≤ δ∥w∥, (3.6)
as well as
(cid:13) (cid:18) (cid:19)(cid:13)
(cid:13) (cid:13) (cid:13)Gˆ η,i(v,w)−(cid:104) Gˆ η′ ,i(0,0)(cid:105) wv (cid:13) (cid:13)
(cid:13)
≤ δ∥w∥. (3.7)
Note that this lemma would hold trivially if the right hand-sides of (3.6) and (3.7) were
replaced by the term δ∥(v,w)∥.
Proof. It is sufficient to consider each of the functions Gˆ ,Gˆ ,...,Gˆ separately, find a neigh-
η η,1 η,N
borhood V on which the corresponding inequality from (3.6) or (3.7) holds and conclude the
δ
proof by choosing the intersection over all these V . In the following, we will only consider Gˆ
δ η
and will show how to find an V such that (3.6) holds, as the proofs for Gˆ ,...,Gˆ work
δ η,1 η,N
analogously.
Let 0 ∈ V ⊆ V∗ be an open, convex neighborhood, such that
δ
(cid:13) (cid:13)
(cid:13)∂ Gˆ (v,w)−∂ Gˆ (0,0)(cid:13) ≤ δ, ∀(v,w) ∈ V .
(cid:13) w η w η (cid:13) δ
Here ∂ Gˆ (v,w) ∈ RD×N denotes the partial Jacobian with respect to the latter N components.
w η
We have
(cid:104) (cid:105)(cid:18) v(cid:19) (cid:90) 1(cid:104) (cid:105) (cid:104) (cid:105)
Gˆ (v,w)− Gˆ′(0,0) = Gˆ (v,0)+ ∂ Gˆ (v,tw) wdt−(v,0)− ∂ Gˆ (0,0) w
η η w η w η w η
0
(cid:90) 1(cid:104) (cid:105)
= ∂ Gˆ (v,tw)−∂ Gˆ (0,0) wdt
w η w η
0
and for (v,w) ∈ V in particular
δ
(cid:13) (cid:13) (cid:13) (cid:13)Gˆ η(v,w)−(cid:104) Gˆ η′(0,0)(cid:105)(cid:18) wv(cid:19)(cid:13) (cid:13) (cid:13)
(cid:13)
≤ (cid:90) 1(cid:13) (cid:13) (cid:13)∂ wGˆ η(v,tw)−∂ wGˆ η(0,0)(cid:13) (cid:13) (cid:13)∥w∥dt ≤ δ∥w∥.
0
This finishes the proof.
3.3 Gradient Descent - the Stable Case
In this section we will prove Theorem A (i).
Theorem A (i). Let x∗ ∈ M with µ(x∗) < 0. Then x∗ ∈ supp(cid:0) XGD(cid:1) .
lim
15In the following we will let Π ∈ R(D−N)×D and Π ∈ R(D−N)×N be the matrices which
v w
project a vector (v,w)t ∈ RD onto its v-, respectively w-component, i.e. in block-matrix form
Π := (cid:0)1 0(cid:1) and Π := (cid:0) 0 1 (cid:1) .
v D−N w N
Proof of Theorem A (i). Suppose µ(x∗) < 0 and let U ⊆ M be some neighborhood of x∗. Our
goal is to prove P(XGD ∈ U) > 0. Choose δ > 0 such that eµ(x∗) +δ =: γ < 1, let V be the
lim δ
corresponding neighborhood given by Lemma 3.2. Let R > 0 be some radius, such that
δ
B (R )×B (R ) ⊆ V .
RD−N δ RN δ δ
We assume without loss of generality that U has the form
U = χ(B (R)×{0}) (3.8)
RD−N
for some 0 < R < R . Similarly to the definition of τ, we define a map τ : V → N∪{∞} by
δ δ δ
(cid:110) (cid:111)
τ (v,w) := inf n ∈ N : Gˆn(v,w) ∈/ V . (3.9)
δ η δ
Note that τ (v,w) ≤ τ(v,w).
δ
As a consequence of Lemma 3.2 and the expression (2.18) for µ(x∗), for all (v,w) ∈ V , we
δ
have
(cid:13) (cid:13) (cid:13)Π wGˆ η(v,w)(cid:13) (cid:13)
(cid:13)
≤ (cid:13) (cid:13) (cid:13) (cid:13)(cid:104) Π wGˆ η′(0,0)(cid:105)(cid:18) wv(cid:19)(cid:13) (cid:13) (cid:13) (cid:13)+(cid:13) (cid:13) (cid:13) (cid:13)Π w(cid:18) Gˆ η(v,w)−(cid:104) Gˆ η′(0,0)(cid:105)(cid:18) wv(cid:19)(cid:19)(cid:13) (cid:13) (cid:13)
(cid:13)
≤ (cid:13) (cid:13) (cid:13)(cid:16) 1 N − Nη G x∗(cid:17) w(cid:13) (cid:13) (cid:13)+(cid:13) (cid:13) (cid:13) (cid:13)Gˆ η(v,w)−(cid:104) Gˆ η′(0,0)(cid:105)(cid:18) wv(cid:19)(cid:13) (cid:13) (cid:13)
(cid:13)
≤
eµ(x∗)∥w∥+δ∥w∥
= γ∥w∥. (3.10)
Using this bound inductively, for all (v,w) ∈ V and 1 ≤ n ≤ τ (v,w), we get
δ δ
(cid:13) (cid:13)
(cid:13)Π Gˆn(v,w)(cid:13) ≤ γn∥w∥. (3.11)
(cid:13) w η (cid:13)
Recalling equation (3.5) and using Lemma 3.2 implies
(cid:13) (cid:13) (cid:13)Π vGˆ η(v,w)−v(cid:13) (cid:13)
(cid:13)
= (cid:13) (cid:13) (cid:13) (cid:13)Π v(cid:18) Gˆ η(v,w)−(cid:104) Gˆ η′(0,0)(cid:105)(cid:18) wv(cid:19)(cid:19)(cid:13) (cid:13) (cid:13)
(cid:13)
≤ δ∥w∥,
for all (v,w) ∈ V and thus also for all 1 ≤ n < τ (v,w),
δ δ
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Π Gˆn+1(v,w)−Π Gˆn(v,w)(cid:13) ≤ δ(cid:13)Π Gˆn(v,w)(cid:13) ≤ δγn∥w∥. (3.12)
(cid:13) v η v η (cid:13) (cid:13) w η (cid:13)
With this, we can bound
n
(cid:13) (cid:13) (cid:88) (cid:13) (cid:13)
(cid:13)Π Gˆn(v,w)(cid:13) ≤ ∥v∥+ (cid:13)Π Gˆm(v,w)−Π Gˆm−1(v,w)(cid:13)
(cid:13) v η (cid:13) (cid:13) v η v η (cid:13)
m=1
n
(cid:88) δ
≤ ∥v∥+ δγm−1∥w∥ ≤ ∥v∥+ ∥w∥, (3.13)
1−γ
m=1
16B (Rˆ)×B (Rˆ)
RD−N RN
B (R )×B (R )
RD−N v RN w
(0,0) RD−N ×{0}
B (R)×{0}
RD−N
{0}×RN
V
δ
Figure 2: Schematic representation of the proof of Theorem A (i). The red arrows show posible
sample trajectories.
for all (v,w) ∈ V and 1 ≤ n+1 < τ (v,w). Now set
δ δ
(cid:18) (cid:19)
R (1−γ)R
R := and R = min ,R .
v w δ
2 2δ
Suppose for some (v,w) ∈ B (R )×B (R ), we have τ (v,w) < ∞. Then by definition
RD−N v RN w δ
of τ , we have
Gˆτ δ(v,w)
(v,w) ∈/ V , so by (3.8) in particular
δ η δ
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Π Gˆτ δ(v,w)(v,w)(cid:13) ≥ R or (cid:13)Π Gˆτ δ(v,w)(v,w)(cid:13) ≥ R . (3.14)
(cid:13) v η (cid:13) δ (cid:13) w η (cid:13) δ
However, (3.13) implies
(cid:13) (cid:13) δ δ
(cid:13)Π Gˆτ δ(v,w)(v,w)(cid:13) ≤ ∥v∥+ ∥w∥ < R + R ≤ R ≤ R
(cid:13) v η (cid:13) 1−γ v 1−γ w δ
and (3.11) implies
(cid:13) (cid:13)
(cid:13)Π Gˆτ δ(v,w)(v,w)(cid:13) ≤ γτ δ(v,w)∥w∥ ≤ ∥w∥ < R ≤ R ,
(cid:13) w η (cid:13) w δ
contradicting (3.14). Thus τ (v,w) = ∞, for all (v,w) ∈ B (R )×B (R ). Now (3.12)
δ RD−N v RN w
(cid:16) (cid:17)
implies that Π Gˆn(v,w) is a Cauchy sequence and (3.13) shows that
v η
(cid:13) (cid:13) δ δ
(cid:13) lim Π Gˆn(v,w)(cid:13) ≤ ∥v∥+ ∥w∥ < R + R ≤ R.
(cid:13) n→∞ v η (cid:13) 1−γ v 1−γ w
Furthermore, (3.11) shows Π Gˆn(v,w) → 0. Thus, for each (v,w) ∈ B (R )×B (R ) the
w η RD−N v RN w
(cid:16) (cid:17)
sequence Gˆn(v,w) converges with
η
lim Gˆn(v,w) ∈ B (R)×{0}.
η RD−N
n→∞
17Let U˜ = χ(B (R )×B (R )). Suppose XGD ∈ U˜. By (3.4) and continuity of χ we have
RD−N v RN w 0
(cid:16) (cid:17)
XGD = lim Gn(XGD) = lim χ Gˆn(cid:0) χ−1(XGD)(cid:1)
lim η 0 η 0
n→∞ n→∞
(cid:16) (cid:17)
= χ lim
Gˆn(cid:0) χ−1(XGD)(cid:1)
∈ χ(B (R)×{0}) = U,
η 0 RD−N
n→∞
so XGD ∈ U˜ implies XGD ∈ U. Since U˜ is an open set, we have
0 lim
(cid:16) (cid:17)
P(XGD ∈ U) ≥ P XGD ∈ U˜ > 0,
lim 0
completing the proof.
3.4 Gradient Descent - the Unstable Case
In this section we will prove Theorem A (ii).
Theorem A (ii). Let x∗ ∈ M with µ(x∗) > 0. Then x∗ ∈/ supp(cid:0) XGD(cid:1) .
lim
In the case µ(x∗) < 0 considered in the previous section, all eigenvalues of 1 −ηG lie
N x∗
strictly inside the unit circle. As a consequence we were able to construct an open neighborhood
ofx∗,suchthatforeveryinitialconditioninthatneighborhood,GDconvergestosomeXGD ∈ M
lim
near x∗. In the case µ(x∗) > 0 we consider in this section, we only know that some eigenvalue
of 1 − ηG lies strictly outside the unit circle while there could still be eigenvalues on or
N x∗
strictly inside the unit circle. As a consequence on should not expect to be able to construct a
neighborhood U ⊆ RD of x∗, such that GD does not converge to some XGD ∈ M near x∗ for
lim
any initial condition in U. Instead we will construct a neighborhood U ⊆ RD of x∗, such that
GD does not converge to some XGD ∈ M near x∗ for Lebesgue-almost any initial condition in
lim
U. In fact, we will show that the set of initial conditions in U for which GD does converge to
some XGD ∈ M near x∗ is contained in a lower dimensional manifold called the center-stable
lim
manifold10. While for the preceding and all subsequent sections it is convenient to consider
vectors in RD as consisting of a D −N-dimensional tangential part v and an N-dimensional
transversal part w, here it will be more convenient to consider vectors in RD as consisting of a
center-stable part v corresponding to the eigenvalues strictly inside or on the unit circle and
−
an unstable part v corresponding to the eigenvalues strictly outside the unit circle.
+
For the remainder of this section, consider a fixed x∗ ∈ M with µ(x∗) > 0. Let µ¯ ,...,µ¯
1 D
be the eigenvalues of be the eigenvalues of Gˆ′(0,0) appearing according to their multiplicity11
and ordered by their absolute values, i.e.
|µ¯ | ≥ ··· ≥ |µ¯ |.
1 D
Note that the eigenvalue 1 must appear at least with multiplicity D−N, corresponding to the
tangential part and log|µ¯ | = µ(x∗) > 0, which implies |µ¯ | > 1. Let D > 1 be the number of
1 1 +
eigenvalues with absolute value greater 1 and let D = D −D be the number of eigenvalue
− +
with absolute value less or equal to 1, counted with multiplicity in both cases. Furthermore, let
S¯ ∈ RD×D be an invertible matrix, such that
 
µ¯
1 (cid:18) (cid:19)
Gˆ′(0,0) = S¯ ... S¯−1 = S¯ A + 0 S¯−1,
η   0 A
−
µ¯
D
10While the set, we call center-stable manifold is indeed a C1-manifold (cf. [KH95, Theorem 6.2.8]), we will
only show that it is the graph of a Lipschitz continuous function.
11Since Gˆ′(0,0) is symmetric, the algebraic and geometric multiplicities coincide.
η
18where A
+
∈ RD+×D+ and A
−
∈ RD−×D− are given by
   
µ¯ µ¯
1 D++1
A
+
:= 

... 

and A
−
:= 

...  .
µ¯ µ¯
D+ D
Note that ∥A ∥ = 1 and that A is invertible with ∥A−1∥−1 = |µ¯ | > 1, where µ¯ is
− + + D+ D+
the eigenvalue with the smallest absolute value that is still larger than 1. Finally, let G¯ :
η
RD+ ×RD− → RD+ ×RD− given by
(cid:18) (cid:18) (cid:19)(cid:19)
v
G¯ (v ,v ) = S¯Gˆ S¯−1 + .
η + − η v
−
The following is a version of the center-stable manifold theorem.
Theorem 3.3 (center-stable manifold). There exist open neighborhoods 0 ∈ V
−
⊆ RD−, 0 ∈
V
+
⊆ RD+ and a map β∗ : V
−
→ V
+
such that for any (v +,v −) ∈ (V
+
×V −)\graph(β∗), there
exists some n ∈ N, such that G¯n(v ,v ) ∈/ V ×V .
η + − + −
If G¯ is a local diffeomorphism at 0, this result is standard (see e.g. [KH95, chapter 6.2]).
η
In our setting, however, it is possible that µ¯ = 0 for some i ∈ [D]. Still, Theorem 3.3 follows
i
from the arguments in [KH95] under mild modifications. Alternatively, stable/unstable/center
manifold theorems in non-locally diffeomorphic setting have also been obtain by reducing the
problem to the locally diffeomorphic case by an abstract method called inverse limits ([RS80,
QXZ09]).
In order to prove Theorem A (ii), we require the following lemma. Since the equivalent
statement will also be useful to prove instability of SGD, we formulate it to cover both GD and
SGD.
Lemma 3.4. Let A ⊆ RD be a Lebesgue-null set. Then
(cid:16) (cid:17)
P ∃n ∈ N , s.t. XGD/SGD ∈ A = 0.
0 n
Proof. By Hypothesis (H2), we have
P(XGD/SGD
∈ A) = ν(A) = 0. By Hypothesis (H3), we
0
also have
P(XGD/SGD
∈ A) = 0, for every n ≥ 1 and thus
n
(cid:16) (cid:17)
P ∃n ∈ N , s.t. XGD/SGD ∈ A = 0.
0 n
Proof of Theorem A (ii). Suppose µ(x∗) > 0. Let x∗ ∈ U ⊆ RD be the open neighborhood
given by
U := χ(S¯(V ×V )),
+ −
where V and V are the neighborhoods given by Theorem 3.3. Suppose ω ∈ Ω is such that
+ −
XGD(ω) ∈ U ∩M. Then there must exists an m ∈ N, such that Gn(XGD(ω)) = XGD(ω) ∈ U
lim η m m+n
for all n ∈ N . Equivalently, we must have G¯n(S¯−1χ−1(XGD(ω))) ∈ V ×V , for all n ∈ N .
0 η m + − 0
By Theorem 3.3 this means that we can only have XGD(ω) ∈ U ∩M, if there exists an m ∈ N
lim
such that XGD(ω) ∈ χ(S¯graph(β∗)). However, since χ is a diffeomorphism and S¯ an invertible
m
matrix, the set χ(S¯graph(β∗)) has Lebesgue-measure zero. By Lemma 3.4, this implies
P(XGD(ω) ∈ U ∩M) = 0,
lim
completing the proof.
193.5 Random Dynamical System Framework for SGD
While in sections 3.3 and 3.4 we studied the dynamics of gradient descent, the rest of the paper
will be concerned with establishing the analogous results for stochastic gradient descent. For
this,wewillformulateSGDasarandomdynamicalsystem (see[Arn98]forageneralintroduction
into the theory). Formally, for some appropriate probability space (Ω,F,P), we will introduce
a map
φ : N ×Ω×RD → RD,
0
(n,ω,x) (cid:55)→ φ(n)(x),
ω
(n)
where evaluation of the map φ should correspond to applying n iterations of SGD with ω
ω
serving as the seed for the random choices of training examples. We expect the map φ to satisfy
(cid:16) (cid:17)
φ(n+m)(x) = φ(m) φ(n)(x) , (3.15)
ω θnω ω
whereθnω istheseedforthesametrainingexamples, butshiftedbynsteps. Putmoreclearly, if
ω generates the sequence of training examples (ξ ,ξ ,...) ∈ [N]N , then θnω should generate the
1 2
N
sequence (ξ ,ξ ,...) ∈ [N] . Since the training examples are chosen independently with
n+1 n+2
identical distributions the sequences (ξ ,ξ ,...) and (ξ ,ξ ,...) are equal in distribution.
1 2 n+1 n+2
In other words the map θ : Ω → Ω leaves the probability measure P invariant, i.e.
P(E) = P(cid:0) θ−1(E)(cid:1) , ∀E ∈ F. (3.16)
A map θ : Ω → Ω satisfying (3.16) is called metric dynamical system. Equation (3.15) is
known as the cocycle property and a map φ satisfying it is called a cocycle over θ. A pair (θ,φ),
consistingofametricdynamicalsystemandacocycleoverit,iscalledrandomdynamicalsystem
(cf. [Arn98, Definition 1.1.1]).
Since Ω should encode the randomness both at initialization and during training, we let
Ω := RD ×[N]N and F := B(RD)⊗P([N])⊗N ,
where P([N]) denotes the power set of [N]. If β denotes the uniform measure on [N] we can
define the probability measure P : F → [0,1] by
P := ν ⊗β⊗N .
Thus, the elements ω ∈ Ω have the form ω = (ω ,ω ,ω ,...) and the random variables
init 1 2
XSGD,ξ ,ξ ,... used in SGD (cf. section 2.2) can be formally defined by
0 1 2
XSGD(ω) := ω and ξ (ω) := ω .
0 init n n
The shift operator θ : Ω → Ω can now be defined by
θ : ω = (ω ,ω ,ω ,...) (cid:55)→ θω := (ω ,ω ,ω ,...).
init 1 2 init 2 3
Clearly θ satisfies (3.16) and thus defines a metric dynamical system on Ω. Furthermore θ is
ergodic with respect to the sub-sigma algebra Fˆ := σ(ξ ,ξ ,...), i.e.
1 2
θ−1(E) = E ⇒ P(E) ∈ {0,1}, ∀E ∈ Fˆ.
20Recall (cf. (2.8) and (2.11)) that the random variables XSGD,XSGD,... can be defined recur-
1 2
sively by
XSGD(ω) := G (XSGD(ω)), ∀n ≥ 0.
n+1 η,ξn+1(ω) n
The map φ : N ×Ω×RD → RD will be defined by
0
φ(0)(x) := x and φ(n)(x) := (cid:2) G ◦···◦G (cid:3) (x). (3.17)
ω ω η,ξn(ω) η,ξ1(ω)
It is easy to check that φ satisfies (3.15). Thus φ is a cocylce over θ and (θ,φ) a random
dynamical system. The random variables XSGD can now be expressed as
n
XSGD(ω) = φ(n)(cid:0) XSGD(ω)(cid:1) .
n ω 0
Note that, again, points x ∈ M are fixed points of φ, meaning
φ(n)(x) = x, ∀n ∈ N ,ω ∈ Ω,x ∈ M. (3.18)
ω 0
In order to introduce an RDS framework for the linearization of SGD (cf. section 2.3),
for SGD in local coordinates (cf. section 3.2) and for linearized SGD in local coordinates, we
will again fix a global minimum x∗ ∈ M for the remainder of this section. We define a map
Φ : N ×Ω → RD×D by
0
Φ : (n,ω) (cid:55)→ Φ(n) := D φ(n)(x∗), (3.19)
ω x ω
where D φ(n) (x∗) denotes the Jacobian of the map φ(n) . By the chain-rule to (3.17), using
x ω ω
(3.18), we can compute
Φ(0) = 1 and Φ(n) = G′ (x∗)...G′ (x∗).
ω D ω η,ξn(ω) η,ξ1(ω)
From this, it is easy to see that Φ is a matrix cocycle, i.e. that it satisfies
Φ(n+m) = Φ(m) Φ(n).
ω θnω ω
Since the matrices G′ ,...,G′ all respect the splitting RD = T(x∗)⊕N(x∗) with G′ | =
η,1 η,N η,i T(x∗)
Id (cf. section 2.3), the same holds true for Φ(n) for all ω ∈ Ω and all n ∈ N .
T(x∗) ω 0
Recall that χ : Vˆ → Uˆ, introduced in Lemma 3.1, defines a diffeomorphism from a neighbor-
hood Vˆ of the origin onto a neighborhood Uˆ of x∗. Furthermore, we introduced a neighborhood
V∗ ⊆ Vˆ (cf. (3.2)), which allowed us to locally lift the maps G ,...,G via χ to maps
η,1 η,n
Gˆ ,...,Gˆ : V∗ → Vˆ defined by
η,1 η,n
Gˆ (v,w) := χ−1(G (χ(v,w))).
η,i η,i
Analogously to the definition of τ : V∗ → N∪{∞} in section 3.2 (cf. (3.3)), we introduce a map
τ : Ω×V∗ → N∪{∞} by
(cid:110) (cid:104) (cid:105) (cid:111)
τ (v,w) := inf n ∈ N : Gˆ ◦···◦Gˆ (v,w) ∈/ V∗ .
ω η,ξn(ω) η,ξ1(ω)
This allows us to define a map ψ : N ×Ω×V∗ ⊇ D → Vˆ, where
0 ψ
D := {(n,ω,v,w) ∈ N ×Ω×V∗ : n ≤ τ (v,w)},
ψ 0 ω
by
(cid:104) (cid:105)
ψ(n)(v,w) := Gˆ ◦···◦Gˆ (v,w).
ω η,ξn(ω) η,ξ1(ω)
21Clearly, ψ satisfies the local cocycle property, meaning
(cid:16) (cid:17)
ψ(n+m)(x) = ψ(m) ψ(n)(x)
ω θnω ω
whenever the left-hand side is well defined, i.e. whenever (n + m,ω,v,w) ∈ D . This turns
ψ
the pair (θ,ψ) into a local random dynamical system (cf. [Arn98, Definition 1.2.1]). The local
cocycle ψ can be seen as the local lift of φ via χ, as one can easily check that
(cid:16) (cid:17)
ψ(n)(v,w) = χ−1 φ(n)(χ(v,w)) , (3.20)
ω ω
whenever n ≤ τ (v,w).
ω
Similarly to the definition of Φ, we introduce a matrix cocycle Ψ : N ×Ω → RN×N by
0
Ψ(0) := 1 and Ψ(n) = (cid:0)1 −ηG (cid:1) ...(cid:0)1 −ηG (cid:1) . (3.21)
ω N ω N x∗,[ξn] N x∗,[ξ1]
Note that, as opposed to ψ, the matrix cocycle Ψ is defined globally. This is possible since
(n)
τ (0,0) = ∞, for all ω ∈ Ω. Also by (3.5), we can express the Jacobian of ψ at the origin by
ω ω
(cid:32) (cid:33)
1 0
D ψ(n)(0,0) = D−N .
(v,w) ω 0 Ψ(n)
ω
Alternatively, this can also be seen by differentiating (3.21) and using Lemma 3.1, as well as
(2.17). Also, differentiating (3.20) at (0,0) yields the identity
(cid:32) (cid:33)
1 0
Φ(n) = (cid:0) A S (cid:1) D−N (cid:0) A S (cid:1)−1 , (3.22)
ω x∗ 0 Ψ(n) x∗
ω
where (cid:0) A S (cid:1) ∈ RD×D is the matrix from Lemma 3.1 (iii). The following corollary is a simple
x∗
reformulation of Lemma 3.2.
Corollary 3.5. For every δ > 0, there exists a neighborhood (0,0) ∈ V ⊆ V∗, such that for
δ
each (v,w) ∈ V , we have
δ
(cid:13) (cid:16) (cid:17)(cid:13)
(cid:13)ψ(1)(v,w)− v,Ψ(1)w (cid:13) ≤ δ∥w∥.
(cid:13) ω ω (cid:13)
Finally, we can reformulate (2.19) in terms of Ψ to get the expression
1 (cid:104) (cid:13) (cid:13)(cid:105)
λ(x∗) = inf E log(cid:13)Ψ(n)(cid:13) (3.23)
n∈N n (cid:13) ω (cid:13)
for the Lyapunov exponent. By Kingman’s sub-additive ergodic theorem12, we have
1 (cid:13) (cid:13)
λ(x∗) = lim log(cid:13)Ψ(n)(cid:13), (3.24)
n→∞ n (cid:13) ω (cid:13)
for almost every ω ∈ Ω.
12Note that the fact that λ(x∗) does not depend on ω requires θ to be ergodic, which it is not: However, the
cocycle Ψ is measurable with respect to the sub-sigma algebra Fˆ = σ(ξ ,ξ ,...). Thus we may consider the
1 2
probability space (Ω,Fˆ,P| ), on which θ is ergodic, when applying the sub-additive ergodic theorem.
Fˆ
223.6 Stochastic Gradient Descent - the Stable Case
In this section we will prove Theorem B (i).
Theorem B (i). Let x∗ ∈ M with λ(x∗) < 0 and suppose
1
∥∇ F(x∗,y )∥2 ̸= , (3.25)
x i
η
for every i ∈ [N]. Then x∗ ∈ supp(cid:0) XSGD(cid:1) .
lim
Since this is the SGD equivalent of Theorem A (i), the proof will be similar. However,
there is a major obstacle. A crucial ingredient in the proofs of Theorem A (i) is the expression
∥1 −ηG ∥ = eµ(x∗) (cf. (3.10)), which allowed us to derive the bound (cf. (3.11))
N x∗
(cid:13) (cid:13)
(cid:13)Π Gˆn(v,w)(cid:13) ≤ γn∥w∥. (3.26)
(cid:13) w η (cid:13)
For stochastic gradient descent, we only have a bound of the form
(cid:13) (cid:13)
(cid:13)Ψ(n)(cid:13) ≤ C˜ (ω)en(λ(x∗)+δ),
(cid:13) ω (cid:13) δ
where for any δ > 0, C˜ is a random variable which is finite almost surely. This can be seen as a
δ
consequenceof(3.24). RandomvariablessimilartoC˜ aresometimescalledOseledet’s regularity
δ
functions in the literature. If C˜ (ω) was almost surely bounded by some deterministic constant,
δ
it would be possible to derive the equivalent statement to (3.26) with an additional factor
on the right-hand side. Unfortunately, the Oseledet’s regularity function C˜ will generally be
δ
unbounded. This is one of the defining features of so-called non-uniform hyperbolicity. Instead,
we will derive the equivalent statement to (3.26) by essentially using the upper semi-continuity
oftheLyapunovexponent(cf. Lemma3.6below). ThisallowsforaproofofTheoremB(i)along
the lines of the proof of Theorem A (i). It should be noted that this does not save the proof of
Theorem B (ii). Thus an entirely different approach will be presented in sections 3.7-3.9.
Lemma 3.6. Let x∗ ∈ M be a point for which (3.25) is satisfied. For each γ > eλ(x∗), there
exists a δ > 0 and a random variable C : Ω → (0,∞] such that P(C (ω) < ∞) = 1 and
γ γ
(cid:13) (cid:13)
(cid:13)Π ψ(n)(v,w)(cid:13) ≤ C (ω)γn∥w∥, ∀ω ∈ Ω, (v,w) ∈ V , n ≤ τ (v,w),
(cid:13) w ω (cid:13) γ δ δ,ω
where V is the neighborhood from Lemma 3.2/Corollary 3.5 and τ : V → N∪{∞} the map
δ δ,ω δ
given by
(cid:110) (cid:111)
τ (v,w) := inf n ∈ N : ψ(n)(v,w) ∈/ V .
δ,ω ω δ
Proof. Let x∗ ∈ M be such that (3.25) holds and γ > eλ(x∗). Choose an ε > 0 such that
eλ(x∗)+2ε ≤ γ. By (3.23), there exists an n∗ ∈ N, such that
(cid:104) (cid:13) (cid:13)(cid:105)
E log(cid:13)Ψ(n∗)(cid:13) < n∗(λ(x∗)+ε). (3.27)
(cid:13) ω (cid:13)
(1)
Since (3.25) holds, the matrix Ψ is invertible for every ω ∈ Ω and we may define constants
ω
0 < K < K < ∞ by
1 2
K
1
:= inf (cid:13) (cid:13) (cid:13)(cid:16) Ψ( ω1)(cid:17)−1(cid:13) (cid:13) (cid:13)−1 = min (cid:13) (cid:13) (cid:13)(cid:0)1
N
−ηG x∗,[i](cid:1)−1(cid:13) (cid:13) (cid:13)−1 , (3.28)
ω∈Ω(cid:13) (cid:13) i∈[N]
(cid:13) (cid:13)
K
2
:= sup(cid:13) (cid:13)Ψ( ω1)(cid:13)
(cid:13)
= max(cid:13) (cid:13)1
N
−ηG x∗,[i](cid:13) (cid:13). (3.29)
ω∈Ω i∈[N]
23By the cocycle property we have
(cid:13) (cid:13)
Kn∥w∥ ≤ (cid:13)Ψ(n)w(cid:13) ≤ Kn∥w∥. (3.30)
1 (cid:13) ω (cid:13) 2
For some δ > 0 to be determined later, let V and τ be as described in the formulation of the
δ δ,ω
Lemma. By Corollary 3.5, we have the na¨ıve bound
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Π ψ(1)(v,w)(cid:13) ≤ (cid:13)Π ψ(1)(v,w)−Ψ(1)w(cid:13)+(cid:13)Ψ(1)w(cid:13) ≤ (K +δ)∥w∥
(cid:13) w ω (cid:13) (cid:13) w ω ω (cid:13) (cid:13) ω (cid:13) 1
for all ω ∈ Ω and (v,w) ∈ V . Thus we also have
δ
(cid:13) (cid:13)
(cid:13)Π ψ(n)(v,w)(cid:13) ≤ (K +δ)n∥w∥, (3.31)
(cid:13) w ω (cid:13) 2
for all ω ∈ Ω, (v,w) ∈ V and n ≤ τ (v,w).
δ δ,ω
Suppose that ω ∈ Ω and (v,w) ∈ V satisfy τ (v,w) ≥ n∗. By the cocycle properties for ψ
δ δ,ω
and Ψ, the inequality (3.31), Corollary 3.5 and the inequality (3.30), we have
(cid:13) (cid:13)
(cid:13)Π ψ(n∗)(v,w)−Ψ(n∗)w(cid:13)
(cid:13) w ω ω (cid:13)
n∗
≤
(cid:88)(cid:13)
(cid:13)Π
ψ(n∗−n−1)(cid:16)
ψ(1)
(cid:16) v,Ψ(n)w(cid:17)(cid:17)
−Π
ψ(n∗−n−1)(cid:16)
v,Ψ(1)
Ψ(n)w(cid:17)(cid:13)
(cid:13)
(cid:13) w θn+1ω θnω ω w θn+1ω θnω ω (cid:13)
n=1
n∗
≤ (cid:88) (K +δ)n∗−n−1(cid:13) (cid:13)Π ψ(1) (cid:16) v,Ψ(n)w(cid:17) −Ψ(1) Ψ(n)w(cid:13) (cid:13)
2 (cid:13) w θnω ω θnω ω (cid:13)
n=1
n∗
≤
(cid:88)
δ(K
+δ)n∗−n−1(cid:13) (cid:13)Ψ(n)w(cid:13)
(cid:13)
2 (cid:13) ω (cid:13)
n=1
n∗
≤
(cid:88)
δ(K
+δ)n∗−n−1Kn∗−n(cid:13) (cid:13)Ψ(n∗)w(cid:13)
(cid:13).
2 1 (cid:13) ω (cid:13)
n=1
We now fix δ > 0 to have a small enough value, s.t.
n∗
(cid:88) δ(K +δ)n∗−n−1Kn∗−n ≤ εn∗.
2 1
n=1
Thus we have
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Π ψ(n∗)(v,w)−Ψ(n∗)w(cid:13) ≤ εn∗(cid:13)Ψ(n∗)w(cid:13),
(cid:13) w ω ω (cid:13) (cid:13) ω (cid:13)
for all ω ∈ Ω and (v,w) ∈ V with τ (v,w) ≥ n∗. In that case we also have
δ δ,ω
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Π ψ(n∗)(v,w)(cid:13) ≤ (cid:13)Ψ(n∗)w(cid:13)+(cid:13)Π ψ(n∗)(v,w)−Ψ(n∗)w(cid:13)
(cid:13) w ω (cid:13) (cid:13) ω (cid:13) (cid:13) w ω ω (cid:13)
(cid:13) (cid:13)
≤ (1+εn∗)(cid:13)Ψ(n∗)w(cid:13) ≤ Cˆ(ω)∥w∥, (3.32)
(cid:13) ω (cid:13)
(cid:13) (cid:13)
where we define Cˆ(ω) := (1+εn∗)(cid:13)Ψ(n∗)(cid:13). By (3.27), we have
(cid:13) ω (cid:13)
(cid:104) (cid:16) (cid:17)(cid:105) (cid:104) (cid:16) (cid:17)(cid:105)
E log Cˆ(ω) = log(1+εn∗)+E log Ψ(n∗)
ω
≤ εn∗+n∗(λ(x∗)+ε) = n∗(λ(x∗)+2ε) < n∗log(γ). (3.33)
24Next we introduce another random variable C˜ : Ω → R∪{∞} by
ℓ−1
C˜(ω) :=
sup(cid:88)(cid:104) log(cid:16)
Cˆ(θkn∗
ω)(cid:17) −n∗log(γ)(cid:105)
.
ℓ∈N
k=0
Note that the random variable Cˆ is measurable with respect to the sigma algebra F :=
n∗
(cid:16) (cid:17)
σ(ξ ,...,ξ ) and that the random variables Cˆ(θkn∗ω) are thus independent and by θ-
1 n∗
k∈N
0
invariance (cf. (3.16)) identically distributed. By the strong law of large numbers, we have, for
almost every ω ∈ Ω,
ℓ−1
lim
1 (cid:88)(cid:104) log(cid:16)
Cˆ(θkn∗
ω)(cid:17) −n∗log(γ)(cid:105)
=
E(cid:104) log(cid:16) Cˆ(ω)(cid:17)(cid:105)
−n∗log(γ) < 0
ℓ→∞ ℓ
k=0
and thus in particular P(C˜(ω) < ∞) = 1. By definition, the random variable C˜ can be used to
obtain the bound
ℓ−1
(cid:88) log(cid:16) Cˆ(θkn∗ ω)(cid:17) ≤ C˜(ω)+ℓn∗log(γ). (3.34)
k=0
Combining (3.32) and (3.34)), we get
(cid:34)ℓ−1 (cid:35)
(cid:13) (cid:13)Π ψ(ℓn∗)(v,w)(cid:13) (cid:13) ≤ (cid:89) Cˆ(cid:16) θkn∗ ω(cid:17) ∥w∥
(cid:13) w ω (cid:13)
k=0
(cid:32)ℓ−1 (cid:33)
= exp
(cid:88) log(cid:16) Cˆ(cid:16) θkn∗ ω(cid:17)(cid:17)
∥w∥
k=0
≤
eC˜(ω)+ℓn∗log(γ)∥w∥
=
eC˜(ω)γℓn∗
∥w∥, (3.35)
for all ω ∈ Ω, (v,w) ∈ V and ℓ ∈ N with τ (v,w) ≥ ℓn∗.
δ δ,ω
Now let ω ∈ Ω, (v,w) ∈ V and n ∈ N with τ (v,w) ≥ n. Let ℓ ∈ N and 0 ≤ k ≤ n∗−1 be
δ δ,ω 0
such that n = ℓn∗+k. Using the cocycle property and the bounds (3.31) and (3.35), we obtain
(cid:13) (cid:13) (cid:13) (cid:16) (cid:17)(cid:13)
(cid:13)Π ψ(n)(v,w)(cid:13) = (cid:13)Π ψ(k) ψ(ℓn∗)(v,w) (cid:13)
(cid:13) w ω (cid:13) (cid:13) w θℓn∗ω ω (cid:13)
(cid:13) (cid:13)
≤ (K +δ)k(cid:13)Π ψ(ℓn∗)(v,w)(cid:13)
2 (cid:13) w ω (cid:13)
≤ (K
+δ)keC˜(ω)γℓn∗
∥w∥
2
(cid:18)
K
+δ(cid:19)k
=
2 eC˜(ω)γℓn∗+k∥w∥
γ
(cid:18)
K
+δ(cid:19)n∗−1
≤
2 eC˜(ω)γn∥w∥.
γ
Thus we can define the random variable C : Ω → R∪∞ by
γ
(cid:18)
K
+δ(cid:19)n∗−1
C (ω) :=
2 eC˜(ω),
γ
γ
satisfying P(C (ω) < ∞) = 1.
γ
25With Lemma 3.6 in place, we can prove Theorem B (i) analogously to the proof of Theorem
A (i) presented in section 3.3.
Proof of Theorem B (i). Suppose λ(x∗) < 0 and let U ⊆ M be some neighborhood of x∗.
Our goal is to prove P(XSGD ∈ U) > 0. Choose γ ∈ R, such that eλ(x∗) < γ < 1 and let
lim
δ > 0 and C : Ω → R∪∞ be such that the conclusion of Lemma 3.6 holds, i.e. such that
γ
P(C (ω) < ∞) = 1 and that we have
γ
(cid:13) (cid:13)
(cid:13)Π ψ(n)(v,w)(cid:13) ≤ C (ω)γn∥w∥, (3.36)
(cid:13) w ω (cid:13) γ
for all ω ∈ Ω, (v,w) ∈ V and n ≤ τ (v,w). This bound will serve as the equivalent to (3.11)
δ δ,ω
in the proof of Theorem A (i). Let R > 0 be some radius, such that
δ
B (R )×B (R ) ⊆ V .
RD−N δ RN δ δ
We may assume without loss of generality that U has the form
U = χ(B (R)×{0})
RD−N
for some R ≤ R . As a consequence of Corollary 3.5 we get
δ
(cid:13) (cid:13)
(cid:13)Π ψ(1)(v,w)−v(cid:13) ≤ δ∥w∥,
(cid:13) v ω (cid:13)
for all (v,w) ∈ V and thus also for all 1 ≤ n < τ (v,w),
δ δ,ω
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Π ψ(n+1)(v,w)−Π ψ(n)(v,w)(cid:13) ≤ δ(cid:13)Π ψ(n)(v,w)(cid:13) ≤ δC (ω)γn∥w∥. (3.37)
(cid:13) v ω v ω (cid:13) (cid:13) w ω (cid:13) γ
With this, we can bound
n−1
(cid:13) (cid:13) (cid:88) (cid:13) (cid:13)
(cid:13)Π ψ(n)(v,w)(cid:13) ≤ ∥v∥+ (cid:13)Π ψ(m+1)(v,w)−Π ψ(m)(v,w)(cid:13)
(cid:13) v ω (cid:13) (cid:13) v ω v ω (cid:13)
m=0
n−1
(cid:88)
≤ ∥v∥+ δC (ω)γm∥w∥
γ
m=0
δC (ω)
γ
≤ ∥v∥+ ∥w∥, (3.38)
1−γ
for all (v,w) ∈ V and 1 ≤ n+1 < τ (v,w). Now set
δ δ,ω
(cid:18) (cid:19)
R (1−γ)R R
δ
R := and R (ω) = min , .
v w
2 2δC (ω) C (ω)
γ γ
Suppose for some ω ∈ Ω with C (ω) < ∞ and some (v,w) ∈ B (R )×B (R (ω)), we
γ RD−N v RN w
(τ (v,w))
have τ (v,w) < ∞. Then by definition ψ δ,ω (v,w) ∈/ V , so in particular
δ,ω ω δ
(cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Π ψ(τ δ,ω(v,w)) (v,w)(cid:13) ≥ R or (cid:13)Π ψ(τ δ,ω(v,w)) (v,w)(cid:13) ≥ R . (3.39)
(cid:13) v ω (cid:13) δ (cid:13) w ω (cid:13) δ
However, (3.38) implies
(cid:13) (cid:13) δC (ω) δC (ω)
(cid:13)Π ψ(τ δ,ω(v,w)) (v,w)(cid:13) ≤ ∥v∥+ γ ∥w∥ < R + γ R (ω) ≤ R ≤ R
(cid:13) v ω (cid:13) 1−γ v 1−γ w δ
26and (3.36) implies
(cid:13) (cid:13)
(cid:13) (cid:13)Π wψ ω(τ δ,ω(v,w)) (v,w)(cid:13)
(cid:13)
≤ C γ(ω)γτ δ,ω(v,w)∥w∥ ≤ C γ(ω)∥w∥ < C γR
w
≤ R δ,
contradicting (3.39). Thus τ (v,w) = ∞, for all (v,w) ∈ B (R (ω))×B (R (ω)). Now
δ,ω RD−N v RN w
(cid:16) (cid:17)
(n)
(3.37) implies that ψ (v,w) is a Cauchy sequence and (3.38) shows that
ω
(cid:13) (cid:13) δC (ω) δC (ω)
(cid:13) lim Π ψ(n)(v,w)(cid:13) ≤ ∥v∥+ γ ∥w∥ < R + γ R (ω) ≤ R.
(cid:13) n→∞ v ω (cid:13) 1−γ v 1−γ w
(n)
Furthermore, (3.36)showsΠ ψ (v,w) → 0. Thus, foreach(v,w) ∈ B (R )×B (R (ω))
w ω RD−N v RN w
(cid:16) (cid:17)
(n)
the sequence ψ (v,w) converges with
ω
lim ψ(n)(v,w) ∈ B (R)×{0}.
ω RD−N
n→∞
Let U˜(ω) = χ(B (R )×B (R (ω))) whenever C (ω) < ∞. Suppose XSGD(ω) ∈ U˜(ω).
RD−N v RN w γ 0
By (3.20) and continuity of χ we have
(cid:16) (cid:17)
XSGD(ω) = lim φ(n)(XSGD(ω)) = lim χ ψ(n)(cid:0) χ−1(XSGD(ω))(cid:1)
lim ω 0 ω 0
n→∞ n→∞
(cid:16) (cid:17)
= χ lim
ψ(n)(cid:0) χ−1(XSGD(ω)(cid:1)
∈ χ(B (R)×{0}) = U,
ω 0 RD−N
n→∞
so XSGD(ω) ∈ U˜(ω) implies XSGD(ω) ∈ U. By construction U˜ is measurable with respect to
0 lim
σ(ξ ,ξ ,...) and thus independent of XSGD. Therefore we have
1 2 0
(cid:16) (cid:17) (cid:104) (cid:16) (cid:17)(cid:105)
P(XSGD(ω) ∈ U) ≥ P XSGD(ω) ∈ U˜(ω) = E ν U˜(ω) .
lim 0
Since U˜ is a non-empty open set almost surely, by Hypothesis (H2) we get
(cid:104) (cid:16) (cid:17)(cid:105)
P(XSGD(ω) ∈ U) ≥ E ν U˜(ω) > 0,
lim
completing the proof.
3.7 Generated Matrix Semi-groups
It remains to prove Theorem B (ii). The proof of Theorem B (i) in the previous section was of
a quenched nature. One might expect that the best approach to proving Theorem B (ii) is to
construct ω-wise center-stable manifolds similar to the ones constructed in section 3.4. While
an invariant manifold theory (Pesin theory) has been developed for random dynamical systems
[LQ95], these results only provide center-stable manifolds for single points x ∈ M. For the
argument in section 3.4 it was crucial to have a center-stable manifold for an open subset of M.
The authors are not aware of any method to construct such a random center-stable manifold.
In the following, we will present an annealed argument. Instead of showing that, given
λ(x∗) > 0, for almost every ω the points which converge to any X ∈ M near x∗ form a
lim
ν -null set, we will show that ν-almost every initial condition x ∈ RD does not converge
0
to any X ∈ M near x∗ almost surely. Both statements are equivalent to Theorem B (ii) by
lim
27Fubini’stheorem, buttheyaredifferentinflavor. Theformerisastatementonthedeterministic
dynamicsforafixedω,whilethelatterconcernsthestochasticbehaviorofaMarkovprocess. The
advantage of the stochastic approach is that under the conditions we impose, namely regularity
of x∗ in the sense of Definition 2.1, the center-stable manifolds get “washed away” by the
randomness: while for every ω there might exist a manifold of initial conditions which still
converge to M near x∗, for every initial condition X ∈/ M, the probability of converging to
0
some X ∈ M near x∗ is zero.
lim
We will show this by constructing a Lyapunov function (see e.g. [BH22]) defined on a
neighborhood of x∗ which goes to infinity near M. This is inspired by previous work on the
instability of invariant subspaces for stochastic differential equations [Bax91, BS88, BBPS22,
BCZG23, CZH21]. While these works rely on conditions of H¨ormander-type to establish the
existence of a spectral gap in the so-called projective process, such a spectral gap is excluded by
the discrete nature of our problem. Instead we use an argument due to LePage [LP82] (cf. also
[BL85] for a survey in English) to find a spectral gap in a different topology. LePage’s argument
needs the matrix semi-group on which the linear co-cycle Ψ is supported to satisfy two algebraic
properties, namely being contracting and strongly irreducible (cf. Definition 3.8 below). In this
section, we will show that they follow from the regularity of x∗.
For some x∗ ∈ M, we denote the support of the matrix-valued random variable Ψ(n) by
•
S (x∗), i.e.
n
(cid:16) (cid:17)
S (x∗) := supp Ψ(n) ⊂ RN×N.
n •
From the definition of Ψ (3.21), one can readily see that
S (x∗) = (cid:8)(cid:0)1 −ηG (cid:1) ...(cid:0)1 −ηG (cid:1) : (ξ ,...,ξ ) ∈ [N]n(cid:9) .
n N x∗,[ξn] N x∗,[ξ1] 1 n
Furthermore, we denote the total support of Ψ by S(x∗), i.e.
∞
(cid:91)
S(x∗) := S (x∗) ⊂ RN×N.
n
n=0
Clearly,S(x∗)isamatrixsemi-groupwithunity,generatedbyS (x∗),i.e. S(x∗)containsexactly
1
those matrices, which can be expressed as the product of an arbitrary number of elements in
S (x∗), including the empty product, which is defined to be the identity matrix.
1
Recall from Definition 2.1, that a point x∗ ∈ M is called regular, if for every i ∈ N, we have
(cid:26) (cid:27)
1 2
[G ] = ∥∇ F(x∗,y )∥2 ∈/ , ,
x∗ i,i x i
η η
and if there exists no proper subset ∅ ⊊ A ⊊ [N], such that
[G ] = ∇ F(x∗,y )·∇ F(x∗,y ) = 0, ∀i ∈ A, j ∈ [N]\A.
x∗ i,j x i x j
Proposition 3.7. Let x∗ ∈ M be regular. Then S(x∗) ⊆ GL(N,R).
Proof. Since x∗ ∈ M is regular, in particular
1
∥∇ F(x∗,y )∥2 ̸= , ∀i ∈ [N].
x i
η
Thus S (x∗) ⊆ GL(N,R) and since S(x∗) is generated by S (x∗) also S(x∗) ⊆ GL(N,R).
1 1
28Definition 3.8. A matrix semi-group of invertible matrices S ⊆ GL(N,R) is called...
(i) contracting, if there exists a sequence (M n) n∈N ⊂ S, such that
M
n
lim = M,
n→∞ ∥M n∥
for some rank-1 matrix M ∈ RN×N.
(ii) strongly irreducible, ifforeveryproperlinearsubspace{0} ⊊ W ⊊ RN thesetofsubspaces
{MW : M ∈ S} contains infinitely many elements.
Lemma 3.9. Let x∗ ∈ M be a regular point with λ(x∗) > 0. Then S(x∗) ⊆ GL(N,R) is both
contracting and strongly irreducible.
Proof. Let x∗ ∈ M be a regular point with λ(x∗) > 0. For ease of notation, we will write
G := G and G := G . We start by proving that S(x∗) is contracting. From the original
x∗ [i] x∗,[i]
definition (2.12) of λ(x∗), we get the inequality
0 < λ(x∗) ≤ E(cid:2) log(cid:13) (cid:13)G′ (x∗)(cid:13) (cid:13)(cid:3) ≤ maxlog(cid:13) (cid:13)G′ (x∗)(cid:13) (cid:13)
η,ξ1
i∈[N]
η,i
= maxlog(cid:13) (cid:13)(cid:0)1
D
−η∇ xF(x∗,y i)∇ xF(x∗,y i)t(cid:1) | N(x∗)(cid:13) (cid:13).
i∈[N]
In particular, there must exist an i∗ ∈ [N] with
(cid:12) (cid:12)
(cid:12)1−η∥∇ F(x∗,y )∥2(cid:12) = |1−ηG | > 1.
(cid:12) x i∗ (cid:12) i∗,i∗
Without loss of generality, we assume i∗ = 1. Consider the sequence (M ) ∈ S(x∗)N given by
n
M :=
(cid:0)1
−ηG
(cid:1)n
.
n N [1]
Recall that G is the matrix G with all but the first row replaced by zeros. Thus [G ] is a rank
[1] [1]
1 matrix with non-trivial eigenvalue G and the eigenvalues of 1 −ηG are µ = 1−ηG
1,1 N [1] 1 1,1
with multiplicity 1 and µ = 1 with multiplicity N −1. Using basic finite-dimensional spectral
2
theory, 1 −ηG can be decomposed as 1 −ηG = A+B, where A is a rank-1 matrix with
N [1] N [1]
An = µn−1A, AB = BA = 0 and
1
∥Bn∥
lim = 0. (3.40)
n→∞ ∥An∥
Now we can compute
M = (cid:0)1 −ηG (cid:1)n = (A+B)n = An+Bn
n N [1]
and thus
M An+Bn ∥An∥ An Bn
n
lim = lim = lim + .
n→∞ ∥M n∥ n→∞ ∥An+Bn∥ n→∞ ∥An+Bn∥∥An∥ ∥An+Bn∥
As a consequence of (3.40), we have
∥A ∥ Bn
n
lim = 1 and lim = 0
n→∞ ∥An+Bn∥ n→∞ ∥An+Bn∥
29and hence
M An µn−1A A
lim n = lim = lim 1 = ,
n→∞ ∥M n∥ n→∞ ∥An∥ n→∞ µn 1−1∥A∥ ∥A∥
which is indeed a rank-1 matrix.
In order prove that S(x∗) is strongly irreducible, consider a proper linear subspace {0} ⊊
W ⊊ RN for wich we intend to show that {MW : M ∈ S} contains infinitely many elements.
Consider the sets A,B ⊆ [N] given by
A := {i ∈ [N] : e ∈ W} and B := {i ∈ [N] : Ge ∈ W⊥},
i i
where e denotes the i-th unit vector. Since G is positive definite13, we have etGe > 0 for all
i i i
i and thus A ∩ B = ∅. Also, we have G = etGe = 0, for each i ∈ A and j ∈ B. By the
i,j i j
assumption that x∗ is regular, this implies B ̸= [N]\A and thus A∪B ̸= [N]. In other words,
there must exists an i∗ ∈ [N], such that e ∈/ W and Ge ∈/ W⊥. The latter implies that there
i∗ i∗
must exists some w∗ ∈ W such that etGw∗ = w∗tGe ̸= 0. Again, we assume without loss of
i i
generality that i∗ = 1. Consider the sequence of subspaces (W n) ∈ {MW : M ∈ S}N 0 given by
W := (1 −ηG )nW and let
n N [1]
∥G w∥
[1]
κ := sup .
n (cid:13) (cid:13)
w∈Wn\{0} (cid:13) (cid:13)w− G1 1,1G [1]w(cid:13)
(cid:13)
Note that since the term in the supremum only depends on the direction of w and not on ∥w∥,
it is sufficient to take the supremum over the unit ball and by compactness the supremum must
be attained. Since G w is always a multiple of e and e ∈/ W = W, the denominator is always
[1] 1 1 0
non-zero for n = 0. Furthermore, etGw∗ ̸= 0 implies G w∗ ̸= 0 and we have 0 < κ < ∞.
1 [1] 0
Also, using G2 = G G , we get
[1] i,i [1]
G (1 −ηG )w = (1−ηG )G w and
[1] N [1] 1,1 [1]
1 1
(1 −ηG )w− G (1 −ηG )w = w− G w.
N [1] G [1] N [1] G [1]
1,1 1,1
Applying this iteratively, allows us to compute
∥G (1 −ηG )nw∥
[1] N [1]
κ = sup
n (cid:13) (cid:13)
w∈W\{0} (cid:13)(1 −ηG )nw− 1 G (1 −ηG )nw(cid:13)
(cid:13) N [1] G1,1 [1] N [1] (cid:13)
∥G w∥
= sup |1−ηG |n [1]
1,1 (cid:13) (cid:13)
w∈W\{0} (cid:13)w− 1 G w(cid:13)
(cid:13) G1,1 [1] (cid:13)
= |1−ηG |nκ .
1,1 0
Since G ̸= 2, we have |1−ηG | ̸= 1 and the sequence (κ ) consists of pairwise distinct ele-
1,1 η 1,1 n
ments. Thus,inparticular,thesequence(W )consistsofpairwisedistinctsubspaces,completing
n
the proof.
13Recall that G is the Gram matrix of the neural tangent kernel.
303.8 Construction of Lyapunov Functions
The goal of this section is, given λ(x∗) > 0, to construct a Lyapunov function F∗ : RN \{0} →
[0,∞) which goes to infinity near 0 and such that
(cid:104) (cid:16) (cid:17)(cid:105)
E F∗ ψ(1)(v,w) ≤ γF∗(v,w),
ω 2
for some γ ∈ (0,1) and every (v,w) in some neighborhood of (0,0). This means that the value
of F∗(w) must decrease on average along trajectories. Since F∗ is large near the origin, points
get “pushed away” from the set {w = 0}, which will allow us to prove a lack of convergence in
the subsequent section.
Following recent works in fluid dynamics [BBPS22, BCZG23], we will study the family of
linear operators (P q : C0(SN−1) → C0(SN−1)) q∈R given by
  
(cid:13) (cid:13)q Ψ(1) s
[P qf](s) = E (cid:13) (cid:13)Ψ( ω1)s(cid:13)
(cid:13)
f(cid:13) (cid:13)Ψ(ω
1) s(cid:13)
(cid:13).
(cid:13) ω (cid:13)
HereC0(SN−1)denotestheBanachspaceofreal-valuedcontinuousfunctionsonSN−1, equipped
with the supremum-norm. It can be readily seen that the operators P are bounded in this
q
norm14. The arguments in [BBPS22, BCZG23] rely on the fact that P admits a spectral gap
0
in their settings. Unfortunately the discrete nature of our setting prohibits such a spectral gap,
at least in C0(SN−1). In order to circumvent this obstacle we must consider a different Banach
space.
For α ∈ (0,1), we let Cα(SN−1) denote the Banach space of α-H¨older continuous functions
on the unit sphere SN−1 ⊂ RN, i.e.
Cα(SN−1) = (cid:8) f : SN−1 → R : ∃h > 0 s.t. |f(s )−f(s )| ≤ h∥s −s ∥α, ∀s ,s ∈ SN−1(cid:9)
1 2 1 2 1 2
and
|f(s )−f(s )|
1 2
∥f∥ = ∥f∥ + sup .
Cα ∞ ∥s −s ∥α
s1,s2∈SN−1,s1̸=s2 1 2
As we will only consider the space of H¨older-continuous functions on SN−1 here, we will abbre-
viate Cα = Cα(SN−1). Also, L(Cα) denotes the space of bounded linear operators from Cα to
itself.
Lemma 3.10 ([BL85, Part A-Proposition V.4.1]). If S(x∗) is contracting and strongly irre-
ducible, then there exists an α ∈ (0,1) such that
(i) there exists an qˆ > 0, such that for q ∈ (−qˆ,qˆ) the operator P ∈ L(C0) restricts to a
p
well-defined, bounded operator P ∈ L(Cα) and the map P : (−qˆ,qˆ) → L(Cα), q (cid:55)→ P is
q • q
analytic
(ii) and the operator P satisfies
0
1
limsup∥Pnf −κ(f)1∥n < 1,∀f ∈ Cα,
0 Cα
n→∞
for some probability measure κ on SN−1. Here, we let 1 ∈ Cα denote the constant function
with value 1.
14In fact ∥P ∥≤Kq for q≥0 and ∥P ∥≤K−q for q≤0, with K and K defined by (3.28) and (3.29).
q 2 q 1 1 2
31Fortheproof, wereferto[BL85]. Theintegrabilityassumptionin[BL85, PartA-Proposition
(1)
V.4.1] is satisfied trivially in our setting, as Ψ can only take finitely many values.
ω
Lemma 3.10 (ii) says that 1 an dominant eigenvalue of P , i.e. it is isolated and the spectral
0
value with the largest absolute value. By classical perturbation theory ([Kat95]) this implies
that for q sufficiently close to 0 the largest spectral value of P is also an isolated eigenvalue and
q
both the dominant eigenvalue and the corresponding Riesz projections are analytic in q. Put
more precisely, we get the following corollary (cf. also [BL85, Part A-Theorem V.4.3]). Here C˙α
denotes the dual space of Cα and Q∗ ∈ L(C˙α) the dual operator of an operator Q ∈ L(Cα).
Corollary 3.11. In the setting of the previous lemma, there exists a 0 < q˜ < qˆ and analytic
maps r : (−q˜,q˜) → R, f : (−q˜,q˜) → Cα, κ : (−q˜,q˜) → C˙α and Q : (−q˜,q˜) → L(Cα), such that
• • •
P f = Q f +r(q)⟨κ ,f⟩f , (3.41)
q q q q
where Q f = 0, Q∗κ = 0, ⟨κ ,f ⟩ = 1 and
q q q q q q
limsup(cid:13) (cid:13)Qn(cid:13) (cid:13)n1
< r(p).
q
n→∞
Furthermore, r(0) = 1, f = 1 and κ ∈ C˙α is given by ⟨κ ,f⟩ = κ(f), where κ is the probability
0 0 0
measure from Lemma 3.10 (ii).
The arguments for the rest of the section are similar to the ones made in [BBPS22, chapter
4]. Henceforth, let α ∈ (0,1) be as in Lemma 3.10.
Theorem 3.12. Let x∗ ∈ M be regular with λ(x∗) > 0. There exist constants p > 0, γ ∈ (0,1)
and α ∈ (0,1) and a positive function f∗ ∈ Cα such that for all s ∈ SN−1 we have
  
(cid:13) (cid:13)−p Ψ(1) s
E (cid:13) (cid:13)Ψ( ω1)s(cid:13) (cid:13) f∗ (cid:13) (cid:13)Ψ(ω 1) s(cid:13) (cid:13) = γf∗(s). (3.42)
(cid:13) ω (cid:13)
Proof. Notethat(3.42)statesthatP f∗ = γf∗. Inthefollowing,wewillshowthat d r(q)| =
−p dq q=0
λ(x∗) > 0. This will imply that for sufficiently small p > 0, we can set γ := r(−p) < 1 and
f∗ := f . Since, forsmallp, thefunctionf isclose15 tof = 1, itisindeedapositivefunction
−p −p 0
and by (3.41), we have P f∗ = γf∗.
−p
In order to show d r(q)| = λ(x∗), note that for q sufficiently close to 0, we have ⟨κ ,1⟩ =
dq q=0 q
⟨κ ,f ⟩ ≠ 0. By Corollary 3.11, this allows us to express r(q) by
q 0
1
logr(q) = lim log∥Pn1∥ .
n→∞ n q Cα
Using Jensen’s inequality, we can estimate
1 1
logr(q) = lim log∥Pn1∥ ≥ lim log∥P 1∥
n→∞ n q Cα n→∞ n q C0
(cid:32) (cid:33)
1 (cid:104)(cid:13) (cid:13)q(cid:105)
= lim log sup E (cid:13)Ψ(1)s(cid:13)
n→∞ n (cid:13) ω (cid:13)
s∈SN−1
(cid:32) (cid:33)
= lim
1
log sup
E(cid:20) eqlog(cid:13) (cid:13) (cid:13)Ψ( ω1)s(cid:13) (cid:13) (cid:13)(cid:21)
n→∞ n
s∈SN−1
1 (cid:104) (cid:13) (cid:13)(cid:105)
≥ q lim sup E log(cid:13)Ψ(1)s(cid:13) .
n→∞ n (cid:13) ω (cid:13)
s∈SN−1
15in the Cα-sense, but thus in particular in the C0-sense
32By the Oseledet’s theorem ([Ose68], cf. also [Arn98, Theorem 3.4.11]) the limit in the last line is
equal to λ(x∗). Thus we get r(q) ≥ eqλ(x∗), for sufficiently small q. Since we already know that
r is differentiable in 0 from Corollary 3.11 and since r(0) = 1, this implies d r(q)| = λ(x∗),
dq q=0
completing the proof.
We define a function F∗ : RN \{0} → R by
>0
(cid:18) (cid:19)
w
F∗(w) := ∥w∥−pf∗ . (3.43)
∥w∥
Corollary 3.13. We have
(cid:104) (cid:105)
E F∗(Ψ(1)w) = γF∗(w). (3.44)
ω
Proof. This is a direct consequence of (3.42). Let w ∈ RN \{0} and set s := w ∈ SN−1. Then
∥w∥
(cid:34) (cid:32) (cid:33)(cid:35)
(cid:104) (cid:105) (cid:13) (cid:13)−p Ψ(1) w
E F∗(Ψ(1)w) = E (cid:13)Ψ(1)w(cid:13) f∗ ω
ω (cid:13) ω (cid:13) (1)
∥Ψ w∥
ω
(cid:34) (cid:32) (cid:33)(cid:35)
(cid:13) (cid:13)−p Ψ(1) s
= ∥w∥−pE (cid:13)Ψ(1)s(cid:13) f∗ ω
(cid:13) ω (cid:13) (1)
∥Ψ s∥
ω
= ∥w∥−pγf∗(s) = γF∗(w),
showing the claim.
This establishes that F∗ is a Lyapunov function for the linearized process induced by Ψ. For
the remainder of this section, we will show that, on a neighborhood of the origin, F∗ is also
a Lyapunov function for the Markov-process induced by the non-linear cocycle ψ. Since f∗ is
continuous, positiveandhasacompactdomainitisbothboundedandboundedawayfromzero,
i.e. we can find constants 0 < C ≤ C < ∞ such that
− +
C ≤ f∗(s) ≤ C , ∀s ∈ SN−1. (3.45)
− +
As a direct consequence, we also get the bound
C ∥w∥−p ≤ F∗(w) ≤ C ∥w∥−p, ∀w ∈ RN \{0}. (3.46)
− +
Lemma 3.14. For every ε > 0, there exists a δ > 0 such that for w,w˜ ∈ RN \{0} we have
∥w−w˜∥ |F∗(w)−F∗(w˜)|
< δ ⇒ < ε. (3.47)
∥w∥ F∗(w)
Proof. Using α-H¨older continuity of f∗ and the bounds in (3.45), we can estimate
(cid:12) (cid:18) (cid:19) (cid:18) (cid:19)(cid:12)
|F∗(w)−F∗(w˜)| = (cid:12) (cid:12)∥w∥−pf∗ w −∥w˜∥−pf∗ w˜ (cid:12) (cid:12)
(cid:12) ∥w∥ ∥w˜∥ (cid:12)
(cid:12) (cid:18) (cid:19) (cid:18) (cid:19)(cid:12) (cid:18) (cid:19)
≤ ∥w∥−p(cid:12) (cid:12)f∗ w −f∗ w˜ (cid:12) (cid:12)+(cid:12) (cid:12)∥w∥−p−∥w˜∥−p(cid:12) (cid:12)f∗ w˜
(cid:12) ∥w∥ ∥w˜∥ (cid:12) ∥w˜∥
(cid:12) (cid:12)
≤ F∗(w)C −−1∥f∗∥ Cα(cid:13) (cid:13) (cid:13) (cid:13)∥w
w∥
− ∥w w˜ ˜∥(cid:13) (cid:13) (cid:13) (cid:13)α +F∗(w)C +C −−1(cid:12) (cid:12) (cid:12)1−(cid:18) ∥ ∥w w˜∥ ∥(cid:19)−p(cid:12) (cid:12) (cid:12).
(cid:12) (cid:12)
33In order to complete the proof, we will show that for each ε > 0 there exists a δ > 0, such that
for w,w˜ ∈ RN \{0} with ∥w∥−1∥w−w˜∥ < δ we have
(cid:13) (cid:13) (cid:13) (cid:13)∥w w∥ − ∥w w˜ ˜∥(cid:13) (cid:13) (cid:13) (cid:13) ≤ (cid:18) ∥fC ∗− ∥ε Cα(cid:19) α1 =: ε 1 and (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)1−(cid:18) ∥∥ ww˜ ∥∥(cid:19)−p(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
≤ C C+ −ε =: ε 2. (3.48)
We can bound
(cid:13) (cid:13) (cid:13) (cid:18) (cid:19) (cid:13)
(cid:13)
(cid:13)
w
−
w˜ (cid:13)
(cid:13) =
∥w∥−1(cid:13)
(cid:13)w−w˜+ 1−
∥w∥ w˜(cid:13)
(cid:13)
(cid:13)∥w∥ ∥w˜∥(cid:13) (cid:13) ∥w˜∥ (cid:13)
(cid:12) (cid:12)
∥w−w˜∥ (cid:12)∥w∥−∥w˜∥(cid:12) ∥w−w˜∥
≤ + ≤ 2
∥w∥ ∥w∥ ∥w∥
to see that the first inequality in (3.48) is satisfied for w,w˜ with ∥w∥−1∥w−w˜∥ < δ := 1ε .
1 2 1
Since the map t (cid:55)→ t−p is continuous at t = 1 (cid:55)→ 1, there also exists a δ > 0 such that
2
(cid:12) (cid:12)
(cid:12) (cid:12) ∥w˜∥(cid:12) (cid:12) ∥w−w˜∥ (cid:12) (cid:18) ∥w˜∥(cid:19)−p(cid:12)
(cid:12) (cid:12)1− ∥w∥(cid:12) (cid:12) ≤ ∥w∥ < δ 2 ⇒ (cid:12) (cid:12)1− ∥w∥ (cid:12) (cid:12) < ε 2.
(cid:12) (cid:12)
Thus (3.47) will be satisfied for δ = min(δ ,δ ).
1 2
Lemma 3.15. For every ε > 0, there exists a δ > 0 s.t.
(cid:16) (cid:17) (cid:16) (cid:17)
F∗ ψ(1)(v,w) ≤ (1+ε)F∗ Ψ(1)w , (3.49)
ω 2 ω
for all ω ∈ Ω and (v,w) ∈ V , where V is the neighborhood given in Lemma 3.2/Corollary 3.5.
δ δ
Proof. Let ε > 0 and choose δ˜> 0 such that the conclusion (3.47) of the previous lemma holds.
Recall that (cf. (3.28))
K
1
:= inf (cid:13) (cid:13) (cid:13)(cid:16) Ψ( ω1)(cid:17)−1(cid:13) (cid:13) (cid:13)−1 > 0
ω∈Ω(cid:13) (cid:13)
and let δ = K δ˜. Then, by Lemma 3.2, we have
1
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)Ψ(1)w−ψ(1)(v,w)(cid:13) ≤ δ∥w∥ ≤ K−1δ(cid:13)Ψ(1)w(cid:13) = δ˜(cid:13)Ψ(1)w(cid:13),
(cid:13) ω ω (cid:13) 1 (cid:13) ω (cid:13) (cid:13) ω (cid:13)
for all ω ∈ Ω and (v,w) ∈ V . By (3.47), this implies
δ
(cid:12) (cid:16) (cid:17) (cid:16) (cid:17)(cid:12)
(cid:12)F∗ Ψ(1) w −F∗ ψ(1) (v,w) (cid:12)
(cid:12) ω ω (cid:12)
< ε,
(cid:16) (cid:17)
F∗ Ψ(1) w
ω
which implies (3.49).
Corollary3.16. Foreveryε > 0, thereexistsaδ > 0s.t. forallω ∈ Ωandall(v,w) ∈ V \(RD−N ×{0})
δ
(1)
we have ψ (v,w) ̸= 0 and
ω 2
(cid:104) (cid:16) (cid:17)(cid:105)
E F∗ ψ(1)(v,w) ≤ (γ +ε)F∗(w).
ω 2
In particular, if we choose 0 < ε < 1−γ this shows that F∗ is indeed a Lyapunov function
on a neighborhood of the origin.
34Proof. First note that if we choose δ < K , using Corollary 3.5, we get
1
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13)ψ(1)(v,w) (cid:13) ≥ (cid:13)Ψ(1)w(cid:13)−(cid:13)Ψ(1)w−ψ(1)(v,w) (cid:13) ≥ K ∥w∥−δ∥w∥ > 0
(cid:13) ω 2(cid:13) (cid:13) ω (cid:13) (cid:13) ω ω 2(cid:13) 1
for all ω ∈ Ω and all (v,w) ∈ V \(RD−N ×{0}). Let ε > 0 and choose δ > 0 small enough,
δ
such that both δ < K and that the conclusion of Lemma 3.15 holds. Together with Corollary
1
3.13 this yields
(cid:104) (cid:105) (cid:104) (cid:105)
E F∗(ψ(1)(v,w) ) ≤ (1+ε)E F∗(Ψ(1)w) = (1+ε)γF∗(w) ≤ (γ +ε)F∗(w),
ω 2 ω
for all ω ∈ Ω and all (v,w) ∈ V \(RD−N ×{0}).
δ
3.9 Stochastic Gradient Descent - the Unstable Case
Theorem B (ii). Let x∗ ∈ M be regular with λ(x∗) > 0. Then x∗ ∈/ supp(XSGD).
lim
Proof of Theorem B (ii). Suppose x∗ ∈ M is regular with λ(x∗) > 0. In the following, we
will show that there exists a δ > 0 such that the neighborhood 0 ∈ V ⊆ RD from Lemma
δ
3.2/Corollary 3.5 satisfies
(cid:16) (cid:17)
P ∃n ∈ N, s.t. ψ(n)(v,w) ∈/ V = 1, ∀(v,w) ∈ V \(cid:0)RD−N ×{0}(cid:1) . (3.50)
ω δ δ
We first argue, why this is sufficient to show x∗ ∈/ supp(XSGD). Let x∗ ∈ U ⊆ M be the
lim
neighborhood given by
U := χ(V )∩M =
χ(cid:0)
V
∩(cid:0)RD−N ×{0}(cid:1)(cid:1)
.
δ δ
Suppose for now that ω is such that XSGD ∈ U. By the openness of χ(V ), there then either
lim δ
exists an n ∈ N s.t. XSGD ∈ U or there exists an m ∈ N , such that XSGD ∈ χ(V )\M for
0 n 0 m+n δ
all n ∈ N . By Lemma 3.4, the former happens with probability zero. The probability for the
0
latter to happen can be estimated by
P(cid:0) ∃m ∈ N , ∀n ∈ N , XSGD(ω) ∈ χ(V )\M(cid:1)
0 0 m+n δ
∞
≤ (cid:88) P(cid:16) ∀n ∈ N , φ(n) (XSGD(ω)) ∈ χ(V )\M(cid:17)
0 θmω m δ
m=1
∞
= (cid:88) P(cid:16) ∀n ∈ N , ψ(n) (χ−1(XSGD(ω))) ∈ V \(RD−N ×{0})(cid:17) .
0 θmω m δ
m=1
Since XSGD only depends on ω and ω ,...,ω and the random map ψn only depends on
m init 1 m θmω
ω ,...ω , they are independent. Thus, if (3.50) holds, this probability will also be zero.
m+1 m+n
Itremainstofindaδ > 0suchthat(3.50)holds. Letp > 0,γ ∈ (0,1)andf∗ : SN−1 → (0,∞)
besuchthattheconclusionofTheorem3.12holds. Also,letF∗ bethefunctiondefinedin(3.43),
choosesome0 < ε < 1−γ andletδ besuchthattheconclusionofCorollary3.16holds. Without
loss of generality, we assume V ⊆ RD−N ×B (1)16. Recall that τ : V → N∪{∞} is given
δ RN δ,ω δ
by
(cid:110) (cid:111)
τ (v,w) := inf n ∈ N : ψ(n)(v,w) ∈/ V .
δ,ω ω δ
16If not, consider the intersection of V and RD−N ×B (1) henceforth.
δ RN
35We can reformulate (3.50) as
P(τ (v,w) < ∞) = 1, ∀(v,w) ∈ V \(cid:0)RD−N ×{0}(cid:1) .
δ,ω δ
Using Corollary 3.16 inductively, one can show that
(cid:104) (cid:16) (cid:17)(cid:105)
E 1 F∗ ψ(n)(v,w) ≤ (γ +ε)nF∗(w),
τ δ,ω(v,w)≥n ω 2
for all n ∈ N and (v,w) ∈ V . Since V ⊆ RD−N ×B (1), we have
δ δ RN
F∗(w) ≥ C ∥w∥−p ≥ C , ∀(v,w) ∈ V .
− − δ
This allows us to compute
P(τ (v,w) = ∞) = lim P(τ (v,w) > n)
δ,ω δ,ω
n→∞
(cid:104) (cid:16) (cid:17)(cid:105)
≤ C−1 lim E 1 F∗ ψ(n)(v,w)
−
n→∞
τ δ,ω(v,w)>n ω 2
≤ C−1 lim (γ +ε)nF∗(w) = 0,
−
n→∞
for all (v,w) ∈ V \(RD−N ×{0}).
δ
Acknowledgements
The authors would like to thank Tim van Erven for his insightful feedback. Furthermore, the
authors thank the DFG SPP 2298 for supporting their research. Both authors have been addi-
tionallysupportedbyGermany’sExcellenceStrategy–TheBerlinMathematicsResearchCenter
MATH+ (EXC-2046/1, project ID: 390685689), in the case of D.C. via the Berlin Mathematical
School and in the case of M.E. via projects AA1-8 and AA1-18. Furthermore, M.E. thanks the
DFG CRC 1114 and the Einstein Foundation for support.
References
[AEV23] Artur Avila, Alex Eskin, and Marcelo Viana. Continuity of the Lyapunov exponents
of random matrix products. arXiv:2305.06009, 2023.
[AK87] Ludwig Arnold and Wolfgang Kliemann. Large deviations of linear stochastic dif-
ferential equations. In Stochastic Differential Systems, pages 115–151, Berlin, Hei-
delberg, 1987. Springer Berlin Heidelberg.
[AKO86] L.Arnold,W.Kliemann,andE.Oeljeklaus. Lyapunovexponentsoflinearstochastic
systems. In Lyapunov exponents (Bremen, 1984), volume 1186 of Lecture Notes in
Math., pages 85–125. Springer, Berlin, 1986.
[ALP22] SanjeevArora,ZhiyuanLi,andAbhishekPanigrahi.Understandinggradientdescent
on the edge of stability in deep learning. In Proceedings of the 39th International
Conference on Machine Learning, volume 162 of Proceedings of Machine Learning
Research, pages 948–1024. PMLR, 17–23 Jul 2022.
[Arn84] LudwigArnold. Aformulaconnectingsampleandmomentstabilityoflinearstochas-
tic systems. SIAM Journal on Applied Mathematics, 44(4):793–802, 1984.
36[Arn98] Ludwig Arnold. Random dynamical systems. Springer Monographs in Mathematics.
Springer-Verlag, Berlin, 1998.
[Bax91] P. H. Baxendale. Statistical equilibrium and two-point motion for a stochastic flow
of diffeomorphisms. In Spatial stochastic processes, volume 19 of Progr. Probab.,
pages 189–218. Birkh¨auser Boston, Boston, MA, 1991.
[BBPS22] J. Bedrossian, A. Blumenthal, and S. Punshon-Smith. Almost-sure exponential
mixing of passive scalars by the stochastic Navier-Stokes equations. Ann. Probab.,
50(1):241–303, 2022.
[BCZG23] A. Blumenthal, M. Coti Zelati, and R. S. Gvalani. Exponential mixing for random
dynamical systems and an example of Pierrehumbert. Ann. Probab., 51(4):1559–
1601, 2023.
[BH22] Michel Bena¨ım and Tobias Hurth. Markov chains on metric spaces—a short course.
Universitext. Springer, Cham, [2022] ©2022.
[BL85] Philippe Bougerol and Jean Lacroix. Products of random matrices with applica-
tions to Schr¨odinger operators, volume 8 of Progress in Probability and Statistics.
Birkh¨auser Boston, Inc., Boston, MA, 1985.
[BS88] P. H. Baxendale and D. W. Stroock. Large deviations and stochastic flows of diffeo-
morphisms. Probab. Theory Related Fields, 80(2):169–215, 1988.
[CKL+20] Jeremy Cohen, Simran Kaur, Yuanzhi Li, J Zico Kolter, and Ameet Talwalkar.
Gradient descent on neural networks typically occurs at the edge of stability. In
International Conference on Learning Representations, 2020.
[Coo21] YaimCooper. Globalminimaofoverparameterizedneuralnetworks. SIAM J. Math.
Data Sci., 3(2):676–691, 2021.
[CZH21] Michele Coti Zelati and Martin Hairer. A noise-induced transition in the Lorenz
system. Comm. Math. Phys., 383(3):2243–2274, 2021.
[GDG+17] Priya Goyal, Piotr Doll´ar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski,
Aapo Kyrola, Andrew Tulloch, Yangqing Jia, and Kaiming He. Accurate, large
minibatch sgd: Training imagenet in 1 hour. arXiv:1706.02677, 2017.
[GKK24] Benjamin Gess, Sebastian Kassing, and Vitalii Konarovskyi. Stochastic modified
flows, mean-field limits and dynamics of stochastic gradient descent. Journal of
Machine Learning Research, 25(30):1–27, 2024.
[HHS17] EladHoffer,ItayHubara,andDanielSoudry. Trainlonger,generalizebetter: closing
the generalization gap in large batch training of neural networks. In Advances in
Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.
[H¨or67] Lars H¨ormander. Hypoelliptic second order differential equations. Acta Math.,
119:147–171, 1967.
[HS97] SeppHochreiterandJurgenSchmidhuber. Flatminima. Neuralcomputation,9:1–42,
02 1997.
37[HSW89] Kurt Hornik, Maxwell B. Stinchcombe, and Halbert L. White. Multilayer feedfor-
ward networks are universal approximators. Neural Networks, 2:359–366, 1989.
[JGH18] Arthur Jacot, Franck Gabriel, and Clement Hongler. Neural tangent kernel: Con-
vergence and generalization in neural networks. In Advances in Neural Information
Processing Systems, volume 31. Curran Associates, Inc., 2018.
[Kat95] Tosio Kato. Perturbation theory for linear operators. Classics in Mathematics.
Springer-Verlag, Berlin, 1995. Reprint of the 1980 edition.
[KB14] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization.
arXiv:1412.6980, 2014.
[KH95] AnatoleKatokandBorisHasselblatt.Introductiontothemoderntheoryofdynamical
systems, volume54ofEncyclopedia of Mathematics and its Applications. Cambridge
University Press, Cambridge, 1995. With a supplementary chapter by Katok and
Leonardo Mendoza.
[Kin68] J. F. C. Kingman. The ergodic theory of subadditive stochastic processes. Journal
of the Royal Statistical Society. Series B (Methodological), 30(3):499–510, 1968.
[KMN+17] Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy,
and Ping Tak Peter Tang. On large-batch training for deep learning: Generalization
gap and sharp minima. In International Conference on Learning Representations,
2017.
[KS17] Nitish Shirish Keskar and Richard Socher. Improving generalization performance by
switching from adam to sgd. arXiv:1712.07628, 2017.
[KSH12] AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton. Imagenetclassificationwith
deep convolutional neural networks. In Advances in Neural Information Processing
Systems, volume 25. Curran Associates, Inc., 2012.
[LP82] E´mile Le Page. Th´eor`emes limites pour les produits de matrices al´eatoires. In
Probability measures on groups (Oberwolfach, 1981), volume 928 of Lecture Notes in
Math., pages 258–303. Springer, Berlin-New York, 1982.
[LQ95] P.-D.LiuandM.Qian. Smooth ergodic theory of random dynamical systems, volume
1606 of Lecture Notes in Mathematics. Springer-Verlag, Berlin, 1995.
[LTE17] Qianxiao Li, Cheng Tai, and Weinan E. Stochastic modified equations and adaptive
stochastic gradient algorithms. In Proceedings of the 34th International Conference
onMachineLearning,volume70ofProceedingsofMachineLearningResearch,pages
2101–2110. PMLR, 06–11 Aug 2017.
[LTE19] QianxiaoLi, ChengTai, andWeinanE. Stochasticmodifiedequationsanddynamics
of stochastic gradient algorithms i: Mathematical foundations. Journal of Machine
Learning Research, 20(40):1–47, 2019.
[Mil97] John W. Milnor. Topology from the differentiable viewpoint. Princeton Landmarks
in Mathematics. Princeton University Press, Princeton, NJ, 1997. Based on notes
by David W. Weaver, Revised reprint of the 1965 original.
38[MY21] Chao Ma and Lexing Ying. On linear stability of SGD and input-smoothness of
neural networks. In Advances in Neural Information Processing Systems, 2021.
[Ose68] V.I.Oseledec. Amultiplicativeergodictheorem.CharacteristicLjapunov,exponents
of dynamical systems. Trudy Moskov. Mat. Obˇsˇc., 19:179–210, 1968.
[QXZ09] Min Qian, Jian-Sheng Xie, and Shu Zhu. Smooth ergodic theory for endomorphisms,
volume 1978 of Lecture Notes in Mathematics. Springer-Verlag, Berlin, 2009.
[RHW86] David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. Learning repre-
sentations by back-propagating errors. Nature, 323:533–536, 1986.
[RS80] David Ruelle and Michael Shub. Stable manifolds for maps. In Global theory of dy-
namical systems (Proc. Internat. Conf., Northwestern Univ., Evanston, Ill., 1979),
volume 819 of Lecture Notes in Math., pages 389–392. Springer, Berlin, 1980.
[SSH21] Robin M Schmidt, Frank Schneider, and Philipp Hennig. Descending through a
crowded valley - benchmarking deep learning optimizers. In Proceedings of the 38th
International Conference on Machine Learning, volume 139 of Proceedings of Ma-
chine Learning Research, pages 9367–9376. PMLR, 18–24 Jul 2021.
[Ste89] J. Michael Steele. Kingman’s subadditive ergodic theorem. Ann. Inst. H. Poincar´e
Probab. Statist., 25(1):93–98, 1989.
[WME18] Lei Wu, Chao Ma, and Weinan E. How sgd selects the global minima in over-
parameterized learning: A dynamical stability perspective. In Advances in Neural
Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
[ZBH+21] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals.
Understanding deep learning (still) requires rethinking generalization. Commun.
ACM, 64(3):107–115, 2021.
A Comparison to second moment linear stability
A different notion of linear stability for SGD was introduced in [WME18]. Adapted to the
notation introduced in section 3.5, their condition can be expressed as follows.
Definition A.1 ([WME18, Definition 2]). A global minimum x∗ ∈ M is called second moment
linearly stable, if there exists a constant C such that
(cid:20)(cid:13) (cid:13)2(cid:21)
E (cid:13)Φ(n)x(cid:13) ≤ C∥x∥2,
(cid:13) ω (cid:13)
for all x ∈ RD and all n ∈ N. Here Φ(n) denotes the linearization around x∗ of n steps of SGD
ω
with seed ω as defined in (3.19).
In contrast, we call a global minimum x∗ with λ(x∗) < 0 almost surely stable. As will be
argued below, second moment linear stability is almost a strictly stronger condition then almost
sure stability. The fact that second moment stability and almost-sure stability are in general
39not equivalent, can already be seen for 1-dimensional linear stochastic processes. Let (X n) n∈N
0
be the stochastic process given by X = 1 and
0
X = Y X ,
n+1 n+1 n
where (Y n) n∈N is an i.i.d. sequence of real-valued random variables with E[log +|Y 1|] < ∞.17 By
the strong law of large numbers, we have
n
1 1 (cid:88)
lim log|X | = lim log|Y | = E[log|Y |] ∈ [−∞,∞), almost surely.
n n 1
n→∞ n n→∞ n
k=1
Thus, if E[log|Y |] < 0, then the linear process (X ) is almost-surely stable, i.e. X → 0 with
1 n n
probability 1. At the same time, since X and Y are independent we have
n n+1
E(cid:2)
|X
|2(cid:3)
=
E(cid:2)
|Y
|2(cid:3)E(cid:2)
|X
|2(cid:3)
=
E(cid:2)
|Y
|2(cid:3)E(cid:2)
|X
|2(cid:3)
n+1 n+1 n 1 n
and therefore
E(cid:2)
|X
|2(cid:3)
=
E(cid:2)
|Y
|2(cid:3)n
. Thus (X ) is second moment stable, i.e. there exists a
n 1 n
C > 0 such that
E(cid:2)
|X
|2(cid:3)
< C, if and only if
E(cid:2)
|Y
|2(cid:3)
≤ 1. However, while
E(cid:2)
|Y
|2(cid:3)
≤ 1 implies
n 1 1
E[log|Y |] < 0 by Jensen’s inequality, the converse is clearly not true.18
1
Second moment stability is closely related to so-called moment Lyapunov exponents (see
e.g. [Arn84]), which have been linked to central limit theorems as well as large deviation theory
for the convergence of finite-time Lyapunov exponents [BL85, AKO86, AK87].
Definition A.2. For any p ∈ R, the p-th moment Lyapunov exponent Λ (x∗) of a global
p
minimum x∗ ∈ M is given by
1 (cid:104)(cid:13) (cid:13)p(cid:105)
Λ (x∗) := sup lim logE (cid:13)Ψ(n)w(cid:13) .
p w∈RNn→∞ n (cid:13) ω (cid:13)
Convergence for each w ∈ RN follows from a subadditivity argument. The precise relation
betweensecondmomentLyapunovexponentsandsecondorderstabilitycanbestatedasfollows.
Proposition A.3. In order for a global minimum x∗ ∈ M to be second moment linearly stable,
it is
(i) necessary to have Λ ≤ 0 and
2
(ii) sufficient to have Λ < 0.
2
Proof. Note that by (3.22), a global minimum x∗ is second moment linearly stable if and only
if there exists a C > 0 such that
(cid:20)(cid:13) (cid:13)2(cid:21)
E (cid:13)Ψ(n)w(cid:13) ≤ C∥w∥2,
(cid:13) ω (cid:13)
for all w ∈ RN and all n ∈ N. Now (i) follows directly from the definition of the second moment
Lyapunov exponent. In order to prove (ii), suppose Λ < 0 for all w ∈ RN and thus in
2,w
particular for the unit vectors e ,...,e . Thus
1 N
(cid:20)(cid:13) (cid:13)2(cid:21)
lim E (cid:13)Ψ(n)e (cid:13) = 0, ∀i ∈ [N].
(cid:13) ω i(cid:13)
n→∞
17Here log |Y |=max(0,log|Y |).
+ 1 1
18Suppose for example that Y takes the values 1 and 2 with probability 1 each. A simple calculation shows
E[log|Y |]= 1(log(2)−log(3))<1 0, while E(cid:2) |Y |2(cid:3)3 = 19 >1. 2
1 2 1 9
40Let C > 0 be given by
(cid:20)(cid:13) (cid:13)2(cid:21)
C = sup E (cid:13)Ψ(n)e (cid:13) < ∞.
(cid:13) ω i(cid:13)
i∈[N],n∈N
Now for w = w e +···+w e , we have
1 1 N N
(cid:20)(cid:13) (cid:13)2(cid:21) (cid:13) (cid:13) (cid:32) (cid:88)N (cid:33)(cid:13) (cid:13)2 (cid:13) (cid:13)(cid:88)N (cid:13) (cid:13)2
E (cid:13) (cid:13)Ψ( ωn)w(cid:13) (cid:13) = E (cid:13) (cid:13)Ψ( ωn) w ie i (cid:13) (cid:13)  = E (cid:13) (cid:13) w iΨ( ωn)e i(cid:13) (cid:13) 
(cid:13) (cid:13) (cid:13) (cid:13)
i=1 i=1

(cid:32) N
(cid:33)2
(cid:34) N (cid:35)
(cid:88) (cid:13) (cid:13) (cid:88) (cid:13) (cid:13)2
≤ E  w i(cid:13) (cid:13)Ψ ω(n)e i(cid:13) (cid:13)  ≤ E N w i2(cid:13) (cid:13)Ψ( ωn)e i(cid:13) (cid:13)
i=1 i=1
N
(cid:88)
≤ NC w2 = NC∥w∥2.
i
i=1
Proposition A.4. For each p ∈ R, we have Λ (x∗) ≥ pλ(x∗). In particular Λ (x∗) ≤ 0 implies
p 2
λ(x∗) ≤ 0.
Proof. As a consequence of Oseledet’s theorem [Ose68], we have
1 (cid:104) (cid:13) (cid:13)(cid:105)
λ(x∗) = sup lim E log(cid:13)Ψ(n)w(cid:13) .
w∈RNn→∞ n (cid:13) ω (cid:13)
The proposition follows from Jensen’s inequality.
To summarize, if x∗ ∈ M is second moment stable in the sense of [WME18], it satisfies
Λ (x∗) ≤ 0 and thus λ(x∗) ≤ 0. Most of these points should be regular in the sense of Definition
2
2.1 and even satisfy λ(x∗) < 0. By Theorem B, these global minima lie in the support of XSGD.
lim
On the other hand the one dimensional example demonstrated above suggests that it is possible
to have global minima x∗ ∈ M with λ(x∗) < 0 and Λ (x∗) > 0. Given that they are regular,
2
these global minima will be in the support of XSGD, but will not be second moment linearly
lim
stable in the sense of [WME18]. Notably [WME18] contains a sufficient condition for second
order linear stability (cf. [WME18, Theorem 1]). By the argument above, their condition is also
a sufficient condition for λ(x∗) ≤ 0.
In [MY21], a more general notion of “k-th order linear stability” is introduced. If p = k is an
even natural number this is equivalent to the following generalization of second moment linear
stability (cf. [MY21, Remark 1]).
Definition A.5. For p > 0, a global minimum x∗ ∈ M is called p-th moment linearly stable, if
there exists a constant C such that
(cid:104)(cid:13) (cid:13)p(cid:105)
E (cid:13)Φ(n)x(cid:13) ≤ C∥x∥p,
(cid:13) ω (cid:13)
for all x ∈ RD and all n ∈ N.
Proposition A.3 can be extended to p-th moment linear stability mutatis mutandis.
41