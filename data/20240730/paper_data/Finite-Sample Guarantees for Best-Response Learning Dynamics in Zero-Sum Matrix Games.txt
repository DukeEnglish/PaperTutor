Finite-Sample Guarantees for Best-Response Learning
Dynamics in Zero-Sum Matrix Games
Fathima Zarin Faizal* Asuman Ozdaglar* Martin J. Wainwright†
Department of Electrical Engineering and Computer Sciences*,†
Department of Mathematics†
Massachusetts Institute of Technology*,†
{fathima,asuman,mjwain}@mit.edu
July 30, 2024
Abstract
We study best-response type learning dynamics for two player zero-sum matrix games. We
consider two settings that are distinguished by the type of information that each player has
about the game and their opponent’s strategy. The first setting is the full information case,
in which each player knows their own and the opponent’s payoff matrices and observes the
opponent’s mixed strategy. The second setting is the minimal information case, where players
do not observe the opponent’s strategy and are not aware of either of the payoff matrices
(instead they only observe their realized payoffs). For this setting, also known as the radically
uncoupled case in the learning in games literature, we study a two-timescale learning dynamics
that combine smoothed best-response type updates for strategy estimates with a TD-learning
update to estimate a local payoff function. For these dynamics, without additional exploration,
we provide polynomial-time finite-sample guarantees for convergence to an ǫ-Nash equilibrium.
1 Introduction
Game theory is used to model interactions between two or more players, with the assumption that
each player behaves rationally so as to maximize their individual payoffs subject to their available
information. A standard solution concept is that of a Nash equilibrium, which is often justified
as the steady-state outcome of the learning process of players acting in their self-interest. This
motivates the study of various learning dynamics by which players choose which action to play
while learning the strategy of their opponents and adjust their play accordingly. Key questions
associated with a given set of learning dynamics are stability, convergence to equilibria, and in a
more refined analysis, the rate of convergence to an equilibrium.
Among the most natural and well-studied forms of learning dynamics is fictitious play (FP),
first introduced by Brown [Bro49; Bro51]. It is a very simple and interpretable form of dynamics:
players use information from previous rounds of play to estimate the opponent’s strategy, and
then play a best-response action based on this estimated strategy. For two-player zero-sum games,
Robinson [Rob51] established the asymptotic convergence of FP. This was followed by a number of
papersthatstudieditsconvergence propertiesfordifferentclassesofgames; seeSection1.2belowfor
further details. Classical FP assumes that each player observes their opponent’s actions and knows
the payoff function, which can be viewed as a full information setting. More broadly, in studying
convergence and related questions for learning dynamics, a key consideration is the underlying
1
4202
luJ
92
]CO.htam[
1v82102.7042:viXrainformation structure, meaning the knowledge available to each player while choosing an action.
While the simplest setting is that of full information, many games in practice involve settings of
partial or noisy information about the opponent’s play or the payoff structure, which makes it far
more challenging to develop and analyze learning dynamics. In this paper, in addition to studying
the full information setting, we also study the opposite extreme, in which players observe only their
individual payoffs resulting from their own action at each round, which we refer to as the minimal
information setting.
Despite much work on (asymptotic) convergence guarantees for normal-form games, there are
relatively few results on the iteration complexity of best-response dynamics. Here, for a given
tolerance ǫ > 0, the iteration complexity refers to the number of rounds K(ǫ) required to obtain
strategies that form an ǫ-optimal Nash equilibrium. Of particular relevance is the recent work of
Chen et al. [Che+23a], which considered a smoothed best-response type dynamics for two-player
zero-sum normal-form and Markov games. They established explicit bounds on the iteration com-
plexity, but in the absence of an additional exploration device—such as mixing with the uniform
distribution—their bounds on K(ǫ) scale exponentially in (1/ǫ).
1.1 Our contributions
In this paper, we study smoothed best-response type dynamics for two-player zero-sum matrix
games. We analyze their behavior in both a full information setting, as well as in the minimal
information setting, alsoknown asthe radically uncoupled setting. Inthelattersetting, each player
observes only theirown realized payoff ateach round, andhas nootherinformation abouteitherthe
game or their opponent’s play. Focusing on smoothed best-response without any modifications to
encourage additional exploration, we prove that the number of iterations K(ǫ) required to converge
to ǫ-optimal Nash equilibrium scales polynomially in 1/ǫ for both information structures. To the
bestof ourknowledge, this isthe firstknown polynomial-time guarantee forbest-responsedynamics
in the minimal information setting that does not require additional exploration.
More specifically, our first main result applies to the simpler case of full information, where
players can perform smoothed best-response updates based on knowledge of the opponent’s mixed
strategy. We introduce a modified version of Lyapunov functions used in previous work [Har98;
Che+23a], anduseittoprovethatthedynamics convergetoanǫ-NashequilibriuminK(ǫ) (1/ǫ2)
≍
iterations. In contrast to previous analysis [Che+23a], our modification has desirable smoothness
properties that allow us to establish this polynomial scaling in 1/ǫ for the smoothed best-response.
We then turn to the minimal information case, where players do not observe their opponent’s
play, but instead only observe the resulting payoffs at each round. For this more challenging setting,
we analyze a form of two-time-scale stochastic approximation, with updates that occur on different
time scales. First, each player maintains and updates an estimate of their local payoff function
or q-value—i.e., the average payoff as a function of their actions. The local payoff function carries
informationabouttheopponent’sstrategy. Atthesametime,theplayersalsomodifytheirstrategies
by forming a smoothed best-response based on the estimated q-values. The updates of the q-values
are in the spirit of TD learning [Sut88], and involve an adaptive learning rate [LC05] that ensures
unbiased estimates of the local payoff function. When recast as a form of stochastic approximation
[Bor22], this unbiased property means that the estimated q-values are updated using zero-mean
noisy estimates of the underlying payoff function. However, the noise variance explodes as the
player’s strategies approach the boundary of the probability simplex. We overcome this variance
explosion—without any modifications to theupdates—by carefulinitialization coupled with explicit
tracking of how quickly the strategies approach the boundary. Our analysis also shows the need for
an appropriate separation of the time scales on which the two updates evolve. These choices, along
2with careful design of Lyapunov functions with favorable smoothness properties, allow us to prove
that the iteration complexity scales at least as quickly as (1/ǫ)8+ν. Here the offset ν > 0 can be
chosen arbitrarily close to zero at the price of growth in constant pre-factors.
1.2 Related work
So as to put our contributions in context, we now discuss related work on fictitious play, smoothed
best-response dynamics, as well as value-based methods for minimal information settings. This
overview is far from comprehensive: we limit our discussion to the literature most closely related to
our work.
There is a long line of work on the behavior of fictitious play (FP) for various classes of finite
normal-form games; all of the classical work assumes knowledge of the payoff matrices. Following
the introduction of fictitious play [Bro49; Bro51], Robinson [Rob51] established the convergence of
both the payoffs and strategies of discrete-time FP applied to two-player zero-sum games. Later
work [Sha58] proved the payoffs converge at least as quickly as k−1/(|A1|+|A2|−2), where 1, 2 are
A A
theactionsetsofeachplayer. Karlin[Kar59]conjecturedthattheactualconvergenceratewask−1/2,
but this conjecture does not hold for general tie-breaking rules [DP14; ALW21]. Miyasawa [Miy61]
established FP convergence for two-person non-zero-sum games in which players have at most two
actions and satisfy a notion of non-degeneracy; other classes known to behave well under FP dy-
namics include common interest games [MS96a] and weighted-potential games [MS96b]. On the
negative side, Shapley [Sha63] provided a two-person non-zero-sum game with three actions for
which FP fails to converge; moreover, other counterexamples have been given for certain kinds of
coordination games [FY98].
Harris [Har98] studied the continuous-time version of fictitious play, and used a Lyapunov func-
tion argument to show that it converges at the rate (1/t) for two-player zero-sum games. Our
analysis makes use of a modification of the Lyapunov function from this paper. Smoothed versions
of fictitious play—in which actual best-responses are replaced by smoothed versions so as to encour-
age exploration—have been studied in various papers [FK93; HS02]; we also analyze a smoothed
version in this paper. While we focus on smoothed best-response dynamics based on fictitious play,
it should be noted that various other dynamics for games have been studied, including gradient
descent, mirror descent and its variants (e.g., [ZDR22; Sok+22]); regret matching [HM00]; extragra-
dientversionsofmultiplicative updates[CWC21]; aswellasvariousregret-basedandonlinelearning
methods (e.g., [CL06; DDK; RS13; Syr+15; CP20]).
There is also a more recent and evolving line of work on the minimal information setting. Leslie
and Collins [LC05] introduced the use of adaptive stepsizes to estimate the average payoffs of each
player,andestablisheditsasymptoticconvergence. Ouralgorithmincludesupdatesofthistype,and
theadaptivestepsizesensureakeyunbiasednessproperty. Inaddition,inspiredbythepapers[LC03;
Che+23a], we also make use of a doubly smoothed best-response to update the strategies where we
usealearning ratetomix thesmoothedbest-responsewiththecurrent strategyestimate. Thus, our
set-up has connections with the entropy-regularized algorithms studied by Bakhtin et al. [Bak+22].
Their analysis is predicated upon availability of the exact payoff functions, as well as the opponent’s
actions, whereas by contrast, we assume only availability of the random payoff resulting from the
unobservedactions. Chenetal.[Che+23a]alsoanalyzethisformofminimalinformation; whiletheir
mainfocusisonstochasticgames, theyalsoestablishaboundoniterationcomplexity fortwoplayer
zero-sum matrix games. However, they do not make use of adaptive stepsizes [LC05], and provide
guarantees relative to ǫ-Nashequilibrium up to a smoothing bias. Incorporating the smoothing bias
means that the end-to-end guarantees grow exponentially in the inverse tolerance (1/ǫ), whereas
one of our main contributions is to provide schemes (and analysis) with polynomial scaling. Some
3recent work [Cai+23; Ren+24] has also provided guarantees with polynomial dependence on (1/ǫ),
but for schemes that depart from the usual best-response learning dynamics. Cai et al. [Cai+23]
enforce exploration by limiting how quickly strategies are allowed to approach the boundary of
the probability simplex; Chen et al. [Che+23a] also note that mixing with a uniform distribution
can alleviate these issues. On the other hand, Renard et al. [Ren+24] regularize using the Tsallis
entropy, resulting in a non-standard strategy update. In contrast, our analysis applies to the usual
smoothed best-response dynamics without such modifications.
2 Background and problem set-up
We now provide background along with a more precise set-up of the problem. We begin in Sec-
tion 2.1 with notation and the basic formalism of two-player zero-sum games. In Section 2.2, we
discuss various types of learning dynamics, including the two sets of updates (for full and minimal
information respectively) that we study in this paper.
2.1 Two player zero-sum games
We consider a pair of rational players, labeled by the index i 1,2 , that are playing a zero-sum
∈ { }
matrix game, each seeking to maximize their own payoff. Throughout this work, for each player
i, we refer to their opponent with the notation i. For each i 1,2 , we use i to denote the
− ∈ { } A
finite action set of player i, and define the maximum cardinality A = max 1 , 2 . A mixed
max
|A | |A |
strategy for player i corresponds to some distribution in the probability simplex on
R|Ai|,
which we
(cid:8) (cid:9)
denote by ∆i. We use the Cartesian product set ∆= ∆1 ∆2 to represent the joint strategy space
×
for both players.
LetRi R|Ai|×|A−i| beamatrix thatrepresents therewards orpayoffs obtained by player i.Our
∈
assumptionofhavingazero-sumgameisexpressedbytheequality Ri+(R−i)⊺ = 0. Withoutlossof
generality (sincethepayoff matrices canberescaledasneeded), weassumethat max Ri(ai,a−i)
ai,a−i| | ≤
1. A pair of strategies π¯ = (π¯1,π¯2) ∆ is said to be a Nash equilibrium if
∈
(π¯i)⊺Riπ¯−i (πi)⊺Riπ¯−i, for all πi ∆i, and i 1,2 . (1)
≥ ∈ ∈ { }
Von Neumann’s minimax theorem [Neu28] ensures the existence of a Nash equilibrium in any zero-
sum game and also guarantees that the average payoffs at equilibrium for each player—that is, the
quantities π¯i⊺ Riπ¯−i for i 1,2 —are unique.
∈{ }
Nash gap. Let us now introduce a measure of the closeness to a Nash equilibrium or (NE for
short). For any strategy pair π = (π1,π2) ∆, the Nash gap is defined as
∈
NG(π)= max π πi ⊺ Riπ−i . (2)
πb∈∆i −
i=1,2
X (cid:8) (cid:0) (cid:1) (cid:9)
b
Observe that any mixed strategy π ∆ with Nash gap bounded as NG(π) ǫ is guaranteed to
∈ ≤
be an ǫ-Nash equilibrium, in the sense that it will satisfy the NE inequalities (1) up to an additive
offset of ǫ.
Information structures. In our work, we consider two levels of information available to the
players: full information and minimal information. In the full information setting, players have
access to their payoff matrix and the other player’s payoff matrix, and observes the other player’s
4mixed strategy at all times. In the minimal information setting, players only have access to the
realized payoffs of their own actions. Notably, they do not observe the opponent’s strategies or
actions and are not privy to either of the payoff matrices. Our goal is to show that natural best-
responsetypedynamics canbeusedtorecover anǫ-Nashequilibrium inapolynomialin 1/ǫ number
of iterations for both of these information models.
2.2 Learning dynamics
Letusdescribeandprovideintuitionforthetypesoflearningdynamicsthatweanalyzeinthispaper.
We begin by describing fictitious play and smoothed best-response, and then specify the dynamics
that we analyze: in particular, see Algorithm 1 for the full information setting, and Algorithm 2 for
the minimal information setting.
2.2.1 Fictitious play and (smoothed) best-response
Consider a vector qi R|Ai| with entries qi(ai) for ai i. We interpret qi(ai) as the expected
∈ ∈ A
local payoff of player i when playing action i, i.e., the entry of Riπ−i corresponding to the action
ai. The best-response function for player i maps a vector qi R|Ai| to a strategy via
∈
bri(qi) argmax πTqi. (3a)
∈ πb∈∆i
In terms of this notation, two-player best-response dynbamics in discrete time takes the following
form. Each player i 1,2 begins at time k = 1 with some initial mixed strategy πi. At time
∈ { } 1
steps k = 1,2,..., each player i first plays a random action Ai πi, and then updates their mixed
k ∼ k
strategy πi via
k
1
πi = πi + bri(Riπ−i) πi . (3b)
k+1 k k+1 k − k
(cid:0) (cid:1)
In terms of information structure, note that each player i requires access to the local payoff vector
Riπ−i at each iteration. This can be computed when the payoffs Ri are known, and the player has
k
access to the opponent’s mixed strategy π−i. If the mixed strategy is not known, then it can be
k
estimated using the opponent’s previous actions.
For various reasons (among them ensuring exploration), a general idea used in the literature
is to consider a perturbed zero-sum matrix game, typically using entropy regularization. Doing so
leads toafamily of smoothed versions of best-responseindexed by aregularization parameter τ > 0.
For a probability vector πi ∆i, we define the Shannon entropy function
∈
H(πi) := πi(ai)logπi(ai). (4a)
−
aXi∈Ai
Now suppose that rather than the pure best-response, player i instead plays the τ-regularized best-
response
σ (qi) := argmax π⊺qi+τH(π) . (4b)
τ
πb∈∆i
(cid:8) (cid:9)
Since the entropy function has a gradient that divberges for stbrategies approaching the boundary
of the probability simplex, the strategy being played always places non-zero mass on each action,
5thus ensuring exploration. Some calculation shows that the optimization problem (4b) admits the
closed-form expression
eq(ai)/τ
σ (q)(ai) = for each ai i (5)
τ
eq(a)/τ ∈ A
a∈Ai
i.e., the strategy played is a softmaxPfunction. As τ 0, the smoothed best-response σ (q )
τ i
→
converges to the best-response.
Using a smoothed best-response leads to a natural relaxation of a Nash equilibrium. We say
that a mixed strategy π = (π1,π2) is a τ-regularized Nash equilibrium if
πi =σ (Riπ−i), i= 1,2. (6)
τ
The existence of a strategy pair π satisfying this condition is guaranteed by an application of the
Brouwer fixed point theorem.1 Moreover, it can be shown that a τ-regularized equilibrium is an ap-
proximate Nash equilibrium in a precise sense: given any tolerance ǫ > 0, setting τ ǫ/(2logA )
max
≤
ensures that any τ-regularized Nash equilibrium has Nash gap (2) at most ǫ. This notion of a
τ-regularized equilibrium plays a central role in our dynamics and analysis.
We are now equipped to specify the two classes of smoothed best response dynamics that we
study in this paper.
2.2.2 Dynamics for full information
We begin by analyzing doubly-smoothed best-response updates in the simpler setting of full infor-
mation. Since each player has access to their opponent’s mixed strategy, each player i can update
their strategy according to
πi = πi +β σ (Riπ−i) πi for i 1,2 , (7)
k+1 k k τ k − k ∈ { }
(cid:0) (cid:1)
whereβ (0,1)isasequenceofstepsizes. Notethattheseupdatescorrespondtoadampedversion
k
∈
of the usual operator power method2 for attempting to find a τ-regularized Nash equilibrium (6).
The double smoothing arises from the smoothing from the stepsize used and the smoothed best-
response to the local payoffs.
Algorithm 1 Learning dynamics for full informaton
Input: K, πi Unif( i), temperature τ and stepsizes β ∞
1 ∼ A i i=1
for k = 1,...,K do
(cid:8) (cid:9)
πi = πi +β σ (Riπ−i) πi for i 1,2
k+1 k k τ k − k ∈ { }
end for
(cid:0) (cid:1)
Output: π
K+1
We give a pseudocode specification of the full information dynamics in Algorithm 1. We note
that in the full information setting, the utility of the smoothed best-response is in ensuring that the
best-response is unique.
1In fact, thereis a uniquestrategy pair satisfying condition (6); see Proposition 1 in thesequel for details.
2Theτ-smoothedbest-responseupdateisacontractionforsufficientlylargevaluesofτ,inwhichcaseconvergence
follows easily. However, given that we use τ-regularized NE as an approximation to NE, our analysis focuses on the
small τ-regime, where theoperator is not a contraction.
62.2.3 Dynamics for minimal information
In the minimal information setting that we consider, player i has very limited information about
their opponent: ateach round, they observe only the random payoff resulting from their own action,
and the unobserved action of their opponent. Consequently, it is not possible to directly compute
the exact q-values given by Riπ−i at each round k.
k
In view of this restriction, a natural idea (e.g., see the papers [LC05; Say+21; Che+23a]) is
to run a separate iteration so as to estimate these q-values. In particular, for each i 1,2 , let
ei(ai) RAi denote the standard basis vector with one in position indexed by ai i.∈ In{ term} s of
∈ ∈ A
this notation, consider the update
ei(Ai)
qi = qi α k Ri(Ai,A−i) qi(Ai) for k = 1,2,..., (8a)
k+1 k − k πi(Ai) k k − k k
k k
(cid:0) (cid:1)
where α > 0 denotes a positive stepsize, and πi is the current mixed strategy of player i. Only
k k
the component corresponding to the action Ai is updated for the q-values. Note that in order to
k
compute this update, the only additional information required by player i is the payoff Ri(Ai,A−i)
k k
received by playing action Ai. We re-iterate that player i does not require knowledge of the payoff
k
matrix Ri nor the action A−i of their opponent.
k
Since the update (8a) is in the spirit of TD Learning [Sut88], we refer to it as a TD update. The
rescalingofthestepsizebyπi(Ai)inthisupdatewasoriginallyproposedbyLeslieandCollins[LC05].
k
It can be understood as a form of importance reweighting designed to ensure that
E qi qi π ,qi = Riπ−i qi where π =(π1,π2),
k+1− k | k k k − k k k k
so that in expectation(cid:2) , the TD updates(cid:3) have Riπ−i = qi as a fixed point.
Algorithm 2 Learning dynamics for the minimal information setting
Input: K, qi = 0 R|Ai|, πi Unif( i),τ,c , β ∞
1 ∈ 1 ∼ A α,β i i=1
for k = 1,...,K do
(cid:8) (cid:9)
Play Ai πi independently for i 1,2
k ∼ k ∈{ }
πi = πi +β σ (qi) πi for i 1,2
k+1 k k τ k − k ∈{ }
q ki +1 = q ki − α πk ie (cid:0) (i A(A ii k )) R(Ai k,A(cid:1) − ki) −q ki(Ai k) for i ∈ {1,2 }
k k
end for
(cid:0) (cid:1)
Output: π = (π1 ,π2 )
K+1 K+1 K+1
In parallel with the TD updates for the q-values, the players update their strategies according
to the smoothed best-response dynamics
πi = πi +β σ (qi) πi for i 1,2 . (8b)
k+1 k k τ k − k ∈ { }
(cid:0) (cid:1)
We note that the smoothing in these strategy updates plays a very important role. In order for the
TD updates (8a) to converge, each action has to be played infinitely often [Sut88]. As noted earlier,
smoothed best-response updates ensure that this property holds.
See Algorithm 2 for a pseudocode description of our dynamics for the minimal information
setting. As shown by our analysis, it is necessary to update the strategies more slowly than the
q-values,andweachievethistimescaleseparationbysettingβ = c α forsomescalarc (0,1)
k α,β k α,β
∈
to be specified in our analysis.
73 Main results
In this section, we begin in Section 3.1 with guarantees for the dynamics in Algorithm 1 that apply
to the full information setting. In Section 3.2, we turn to the two time-scale updates in Algorithm 2
that apply to the minimal information setting. In the rest of this paper, all variables of the form
c ,i N denote numerical constants.
i
∈
3.1 Full information
In this section, we provide some finite-sample guarantees on the the full information procedure
summarized in Algorithm 1. Our proof makes use of a novel Lyapunov function for the strategies,
given by
2
(π) := max π,Riπ−i +τH(π) , (9)
V πb∈∆i
i=1
X(cid:8) (cid:8)(cid:10) (cid:11) (cid:9)(cid:9)
b b
where H denotes the Shannon entropy function (4a). Note that the Lyapunov function has a
V
natural game-theoretic interpretation: it corresponds to the sum of the average payoffs when each
player chooses the best-response to the entropy-regularized payoffs. Our Lyapunov function can be
viewed as a smoothed version of the function used by Harris [Har98] to prove finite-time guarantees
for continuous time zero-sum games; see Section 3.3 for further discussion of these connections.
Theorem 1 provides an upper bound on the Nash gap for three different stepsizes. In stating
these claims, we make use of the shorthand V := (π ) for the initial value of the Lyapunov
1 1
V
function from equation (9), where π is the initial set of mixed strategies.
1
V
Theorem 1 (Nash gap finite-sample guarantees). Consider the full information procedure (Algo-
rithm 1) run for for K rounds with initialization π .
1
(a) For a constant stepsize sequence β β (0,1), we have
k
≡ ∈
2A2 β
NG(π ) (1 β)KV + max +2τ logA . (10a)
K+1 1 max
≤ − τ
(b) For inverse linear decay β = β/k for some β (1,2], we have
k
∈
V 8A2 β2
NG(π ) 1 + max +4τ2βlogA . (10b)
K+1 ≤ (K +1)β τ(β 1)K max
−
(c) For inverse polynomial decay β = β/(k+k )η for some scalar β (0,1), exponent η (0,1)
k 0
1 ∈ ∈
and offset k 2η 1−η, we have
0 ≥ β
(cid:0) (cid:1)
NG(π )
exp( −1−β η(K +k 0+1)1−η)
V +
4βA2
max +2τ logA . (10c)
K+1 ≤ exp( −1−β η(1+k 0)1−η) 1 τ(K +k 0)η max
See Section 4.1 for the proof of Theorem 1.
For all three stepsizes, the first term in the upper bound involves the initial Lyapunov value V ,
1
and soreflects therate at which thealgorithm “forgets” its initialization as itconverges. Thesecond
term in each upper bound scales with (1/τ). The third term in each bound scales linearly in τ, and
8corresponds to a form of bias introduced by the players using a τ-regularized best-response instead
of an actual best-response.
For a given target accuracy ǫ (0,1), recall that the iteration complexity K(ǫ) is the smallest
∈
number of iterations required to ensure that ENG(π ) ǫ for all k K(ǫ). In order to obtain
k+1
≤ ≥
explicit bounds on K(ǫ), we can choose the temperature parameter τ and stepsizes so as to ensure
that after K(ǫ) rounds, each of the three terms in the upper bound is at most ǫ. We summarize
3
our conclusions in the following:
Corollary 1. Consider the full information procedure (Algorithm 1) run with τ =c ǫ/A for a
β max
pre-factor c that can depend on the stepsize parameter β. Then the iteration complexity K(ǫ) is
β
bounded as follows:
(a) For the constant stepsizes β β := ǫ2/(c A3 ), we have
k ≡ 1 max
c A3 V
K(ǫ) 1+ 2 max log 1 .
≤ ǫ2 ǫ
(cid:0) (cid:1)
(b) For the inverse linear decay β = β/k for some β (1,2], we have
k
∈
c A3
K(ǫ) 1+ 3 max.
≤ ǫ2
1
(c) If β = β/(k+k )η where β (0,1), η (0,1) and k 2η 1−η, then
k 0 ∈ ∈ 0 ≥ β
(cid:0) (cid:1)
c A3 β
K(ǫ) 1+ 4 max 1/η .
≤ ǫ2
(cid:0) (cid:1)
3.2 Minimal information
We now turn to the analysis of Algorithm 2 that applies to the minimal information setting. In this
case, we give explicit bounds on the iteration complexity K(ǫ), meaning the minimum number of
rounds required toensure that ENG(π ) ǫ. We show the existence of a polynomial iteration
K(ǫ)+1
≤
complexity with a scaling of the order (1/ǫ)8+ν, where the offset parameter ν > 0 can be chosen
arbitrarily close to zero. The price of taking ν 0+ manifests in the growth of certain pre-factors;
→
we use the notation g(ν) and variants thereof to indicate terms of this type.
Our result applies to Algorithm 2 where the temperature and the timescale separation constant
are set as
g (ν)ǫ g (ν)τ3
τ α,β
τ = and c = , respectively. (11)
12logA α,β 6A2
max max
Theorem 2 (Nash gap finite-sample guarantees). Suppose that Algorithm 2 is run with the param-
eters (11), and with the initial mixed strategies π being uniform. Then the iteration complexity
1
K(ǫ) is bounded as follows:
(a) For the constant stepsize β β :=
g1(ν)ǫ8+ν
, we have
k ≡ c5A1 m0 ax
c A10 1
K(ǫ) 6 max log . (12a)
≤ g (ν) ǫ8+ν ǫ
2
(cid:0) (cid:1)
9(b) Fortheinversepolynomialstepsizeβ = β forsomeexponent η (0,1), offsetk 2η 1/(1−η)
k (k+k0)η ∈ 0 ≥ β
and β :=
g1(ν)ǫ8+ν
, we have (cid:0) (cid:1)
c7A1 m0
ax
A10 1 1/(1−η)
K(ǫ) c (η) max log ,
≤ 8 g (ν)ǫ8+ν ǫ
2
(cid:16) (cid:17)
where c (η) depends on η.
8
One of the main challenges in proving Theorem 2 is in controlling the variance of the q-updates.
In particular, playing a softmax response leads to probabilities of playing certain actions that are
are exponentially small in τ. Consequently, a naive lower bound on the probability of playing any
action would give an exponential dependence on 1/τ. Since our choice of τ is proportional to the
target accuracy ǫ, such an approach leads to a guarantee that grows exponentially in (1/ǫ). This
type of growth occurs in some analysis from past work [Che+23a]. Our analysis resolves this by
studying how quickly the iterates approach the boundary of theprobability simplex when the initial
policies are taken to be uniform. See Section 4.2 for the details of this argument.
3.3 Connections among Lyapunov functions
It is worthwhile drawing some connections between the Lyapunov functions used in our analysis,
and those from related work. The proofs of both Theorems 1 and 2 make use of the following
Lyapunov function for the strategy updates
2
(π) := max π,Riπ−i +τH(π) , (13a)
V πb∈∆i
i=1
X(cid:8) (cid:8)(cid:10) (cid:11) (cid:9)(cid:9)
where H is the Shannon entropy (4a). (The proobf of Theorem 2 ablso involves additional Lyapunov
functions needed to track the q-updates.)
As noted previously, our Lyapunov function is an entropically-regularized version of the Lya-
V
punov function used by Harris [Har98], given by
2
(π) := max π,Riπ−i . (13b)
VH πb∈∆i
i=1
X (cid:10) (cid:11)
The work of Chen et al. [Che+23a] makes use of a thbird Lyapunov function, also closely related,
given by
2
(π) = max π,Riπ−i +τH(π) τH(πi) . (13c)
alt
V πb∈∆i −
i=1
X(cid:8) (cid:8)(cid:10) (cid:11) (cid:9) (cid:9)
It differs from our Lyapunov function (13a) bby the subtractiobn of additional entropy terms. As we
discuss below, for zero-sum games, this Lyapunov function has a natural interpretation in terms of
the Kullback–Leibler divergence.
Connections to . Let us first compare and contrast with the Harris Lyapunov function .
VH V VH
The Lyapunov function is directly related to Nash equilibria, since we have (π) = 0 whenever
VH VH
π is a Nash equilibrium. In contrast, since the Shannon entropy is non-negative, our Lyapunov
function (13a) need not be zero at a Nash equilibrium nor at a τ-regularized Nash equilibrium.
Although we cannot hope to drive down (π) to zero as π approaches a Nash equilibrium (as
V
in a standard Lyapunov analysis), our function does have three key properties that enable our
V
analysis:
10(a) First, we have the bound
NG(π) (π), (14a)
≤ V
so that any strategy pair π with (π) ǫ is guaranteed to have Nash gap at most ǫ.
V ≤
(b) Second, we have the approximation error bound
(π) (π) 2τ logA , (14b)
V −VH ≤ max
(cid:12) (cid:12)
so that is a good approximat(cid:12)ion to for sm(cid:12) all τ.
V VH
(c) Third, the Lyapunov function is differentiable and L :=
2A2
max-smooth with respect to the
V τ
Euclidean norm on ∆, meaning that
(π) (π˜) L π π˜ for all π,π˜ ∆. (14c)
2 2
k∇V −∇V k ≤ k − k ∈
Connections to alt. This (1/τ)-scaling of the smoothness constant in item (c) is crucial to our
V
analysis, and allows us to prove a finite-sample guarantee with polynomial scaling in (1/ǫ). In
contrast, for the Lyapunov function (13c) used by Chen et al. [Che+23a], the smoothness constant
increases exponentially in the inverse temperature (1/τ). Since obtaining an ǫ-Nash-gap requires re-
ducingthetemperature,thisscalingmeansthatthefinite-sampleguaranteesinthepaper[Che+23a]
also scale exponentially in (1/ǫ).
Despite its less desirable smoothness properties, the Lyapunov function (13c) turns out to have
a very natural interpretation in terms of the Kullback-Leibler divergence. In particular, let KL(p,q)
denote the KL divergence between two discrete distributions p and q, and define the function
2
(π) := KL(πi,σ (Riπ−i)). (15)
KL τ
V
i=1
X
By construction, this function is zero if and only if πi = σ (Riπ−i) for i 1,2 , so its minima
τ
∈ { }
correspond to the set of τ-regularized Nash equilibria (6). In fact, for any zero-sum game, this KL-
based function is proportional to , and we can use it to show that τ-regularized Nash equilibria
alt
V
are unique. We summarize our conclusions in the following:
Proposition 1. For any zero-sum game and any τ > 0, we have the equivalence
alt(π) τ KL(π). (16)
V ≡ V
Moreover, there is a unique τ-regularized Nash equilibrium (6), corresponding to the unique global
minimum of the function .
KL
V
See Appendix A for the proof of this claim.
The proof of the equivalence (16) hinges crucially on the zero-sum nature of the game. Since
is a strictly convex function by inspection, it implies that the KL-based function is strictly
alt KL
V V
convex foranyzero-sumgame. (Again, thisstrictconvexity neednotbetrueingeneral.) Theglobal
minima of are equivalent to τ-regularized Nash equilibria, and since is strictly convex, the
KL KL
V V
claimed uniqueness condition follows.
114 Proofs of main results
This section is devoted to the proofs of our main results, with Sections 4.1 and 4.2 devoted to the
proofs of Theorems 1 and 2 respectively.
4.1 Proof of Theorem 1
We begin by an auxiliary result that plays a key role in this proof (as well as that of Theorem 2).
Lemma 1. The Lyapunov function from equation (9) has the following properties:
V
(a) It istwice continuously differentiable, andtheℓ -operator norm ofitsHessian isuniformly
2 2
|||·|||
bounded as
A2
2 (π) L := max for all π ∆. (17a)
2
|||∇ V ||| ≤ τ ∈
(b) We have
2
(π),σ (Riπ−i) πi (π)+2τ logA for all π ∆. (17b)
πi τ max
∇ V − ≤ −V ∈
i=1
X(cid:10) (cid:11)
See Section 4.1.2 for the proof of this claim.
Using this auxiliary result, we now prove the theorem itself.
4.1.1 Main argument
It is convenient to introduce the shorthand V := (π ) for the value of the Lyapunov function at
k k
V
iteration k. All three sub-claims in Theorem 1 are derived as consequences of a drift inequality for
: in particular, we claim that
V
2A2 β2
V 1 β V + max k +2β τ logA for all iterations k = 1,2,.... (18)
k+1 k k k max
≤ − τ
(cid:0) (cid:1)
We begin by establishing this claim, and then use it to analyze the three types of stepsizes covered
in the theorem itself.
Using the Hessian bound (17a) on from Lemma 1, we have
V
L
(π ) (π )+ (π ), π π + π π 2.
V k+1 ≤ V k h∇V k k+1 − k i 2k k+1 − k k2
Vk+1 Vk
Recalling that π | {πz =} β|σ{z(R}1π2) π1,σ (R2π1) π2 , observe that we have
k+1 − k k τ k − k τ k − k
(cid:0) 2 (cid:1)
(π ), π π = β (π ), σ (Riπ−i) πi .
h∇V k k+1 − k i k h∇π ki V k τ k − ki
nXi=1 o
Using Hölder’s inequality, we can bound the second-order term as
2 2
π π 2 = β2 σ (Riπ−i) πi 2 β2 σ (Riπ−i) πi σ (Riπ−i) πi .
k k+1 − k k2 k k τ k − kk2 ≤ k k τ k − kk1 k τ k − kk∞
i=1 i=1
X X
12Since both σ (Riπ−i) and πi belong to the probability simplex, we have σ (Riπ−i) πi 1
τ k k k τ k − kk∞ ≤
and σ (Riπ−i) πi 2, whence
k τ k − kk1 ≤
π π 2 4β2.
k k+1 − k k2 ≤ k
Putting together the pieces yields the bound
2 2A2
V V +β (π ), σ (Riπ−i) πi + maxβ2. (19)
k+1 ≤ k k h∇π ki V k τ k − ki τ k
nXi=1 o
It remains to bound the first-order term on the right-hand side of inequality (19), and we do so
usinginequality (17b)fromLemma 1. Substituting this boundinto inequality (19)andre-arranging
yields the claimed drift inequality (18).
The remainder of the proof involves exploiting inequality (18), and for each of the three stepsize
choices covered in the theorem statement, using it to establish the bounds on V . From inequal-
K+1
ity (14a) that relates to the Nash gap, such upper bounds translate directly to upper bounds on
V
the Nash gap.
Herewecovertheconstantstepsizecase. Theremainingstepsizecases(inverselinearandinverse
polynomial) involve more technical calculations, and so are deferred to Appendix B.
Constant stepsizes. In this case, we have β β (0,1) for all iterations k = 1,2,.... For this
k
≡ ∈
choice, the general bound (18) implies that
2A2 β
V (1 β)KV + max +2τ logA .
K+1 1 max
≤ − τ
For a target error ǫ (0,1), it remains to make suitable choices of the temperature parameter τ
∈
and the stepsize β. First, setting τ = √A β yields
max
V (1 β)KV +4A3/2 β.
K+1 ≤ − k max
Thus, if we set β = ǫ2/(16A3 ), we can conclude that it suffip ces to take
max
16A3 V
K(ǫ)= max log 1
ǫ2 ǫ
(cid:0) (cid:1)
iterationsinordertoensurethatV ǫ,whichinturnresultsintheNashgapbeing less than ǫ.
K(ǫ)+1
≤
4.1.2 Proof of Lemma 1
We split our proof into parts, corresponding to the two statements in the lemma. In both cases, we
make use of Danskin’s theorem [Dan67] to compute the gradient
(π)= R−i⊺ σ (R−iπi). (20)
πi τ
∇ V
Proof of part (a). In order to compute the Hessian, we take the derivative of the gradient from
equation (20). Doing so via chain rule yields
2 (π)= R−i⊺ J σ (R−iπi) = 1 R−i⊺ ΣiR−i.
∇πi V πi τ τ τ
(cid:0) (cid:1)
13where Σi := diag σ (R−iπi) σ R−iπi σ R−iπi ⊺ .
τ τ − τ τ
By the sub-multiplicativity of the operator norm, we have
(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
1
2 (π) R−i Σi R−i ,
|||∇πi V |||2 ≤ τ||| |||2 ||| τ|||2 ||| |||2
Recall that for any matrix M Rm×n, we have [HJ12]
∈
n m
M max max M , max M .
2 ij ij
||| ||| ≤ i=1,...,m | | j=1,...,n | |
j=1 i=1
(cid:8) X X (cid:9)
Since the matrix R−i has entries in the interval [ 1,1], this bound implies that R−i A .
2 max
− ||| ||| ≤
Turning to the matrix Σi, we recognize it as the covariance matrix of a multinomial random
τ
vector, so that it is PSD. Consequently, the operator norm Σi is equivalent to the maximum
|||
τ|||2
eigenvalue λ (Σi). Now observe that
max τ
Σi = diag σ (R−iπi) σ R−iπi σ R−iπi ⊺ diag σ (R−iπi) ,
τ τ − τ τ (cid:22) τ
where denotes the PSD(cid:0)order. Thu(cid:1)s, we (cid:0)have (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
(cid:22)
Σi = λ (Σi) λ diag σ (R−iπi) 1.
||| τ|||2 max τ ≤ max τ ≤
h i
(cid:0) (cid:1)
Putting together the pieces yields the bound 2 (π)
A2
max, as claimed.
|||∇ V |||2 ≤ τ
Proof of part (b). Again making use of the gradient representation (20) yields
(π),σ (Riπ−i) πi = R−i⊺ σ (R−iπi),σ (Riπ−i) πi
i τ τ τ
∇ V − −
(cid:10) (cid:11) = σ(cid:10)τ(R−iπi)⊺R−iσ τ(Riπ−i) σ τ(R(cid:11)−iπi)⊺R−iπi, i = 1,2.
−
Using the zero-sum property of the game, we have 2 σ (R−iπi)⊺R−iσ (Riπ−i)= 0. Combined
i=1 τ τ
with the definition (9) of the Lyapunov function, we find that
P
2 2
(π),σ (Riπ−i) πi = σ (R−iπi)⊺R−iπi
i τ τ
∇ V − −
i=1 i=1
X(cid:10) (cid:11) X
2
= σ (R−iπi)⊺R−iπi+τν(σ (R−iπi)) τν(σ (R−iπi))
τ τ τ
− −
Xi=1n o
(π)+2τ logA .
max
≤ −V
4.2 Proof of Theorem 2
Wenow turn tothe proofofTheorem 2intheminimal information setting. Itisconvenient toprove
the following (slightly more general) claim that provides an explicit expression for the number of
rounds required to find an ǫ-Nash equilibrium.
Theorem 3. Let Algorithm 2 be initialized according to Theorem 2, and consider any ν > 0.
(a) For the constant stepsize β β :=
g1(ν)ǫ8+ν
, define
k ≡ c5A1 m0 ax
log ǫ
K(ǫ,ν):=
3T1
.
log 1 g (ν)β
(cid:0) 3 (cid:1)
l − m
(cid:0) (cid:1)
14(b) Fortheinversepolynomialstepsizeβ = β forsomeexponent η (0,1), offsetk = (2η)1/(1−η) ,
k (k+k0)η ∈ 0 β
and β =
g1(ν)ǫ8+ν
, define (cid:6) (cid:7)
c6A1 m0
ax
1 (1 η) 3T 1
K(ǫ,ν) := − log 1 +(1+k )1−η 1−η k 1 .
0 0
g (ν) β ǫ − −
3
ln o m
(cid:0) (cid:1)
Then ENG(π ) ǫ. Here, g ,g are non-decreasing functions of ν. Particularly, g (ν) 0
K(ǫ,ν)+1 1 3 1
≤ →
as ν 0.
→
Lyapunov functions. Recall the definition (9) of the Lyapunov function . In order to track
V
the quality of qi as an estimate of Riπ−i, we introduce the q-Lyapunov function
2
(π,q) := qi Riπ−i 2 . (21a)
W − 2
i=1
X(cid:13) (cid:13)
(cid:13) (cid:13)
The total Lyapunov function for our analysis is the sum
T
(π,q):= (π)+ (π,q), (21b)
T V W
Since our goal is to bound the Nash gap, it is useful to observe that
(i) (ii)
NG(π) (π) (π,q), (22)
≤ V ≤ T
where step (i) follows from the bound (14a); and step (ii) follows from the definition of the total
Lyapunov function, and the non-negativity of (π,q). Thus, in order to bound the Nash gap, it
W
suffices to upper bound (π,q).
T
Recall that our algorithm generates a sequence π ,q of strategy and q-value pairs. Intro-
{
q k}k≥1
duce the shorthand notation
V := (π ), W := (π ,q ), and T := (π ,q )
k V k k W k k k T k k
for the values of the three Lyapunov functions as a function of the iteration number.
4.2.1 Drift inequality for total Lyapunov function
T
The first step in proving Theorem 2 is to establish a drift inequality (or recursive bound) on the
expected values ET of the total Lyapunov function over iterations k. The main challenge in this
k
T
step is that the variance of the q-updates explodes exponentially in (1/τ) as the strategy vectors π
k
approach the boundary of the probability simplex. So as to avoid this explosion, we need to track
carefully the rate at which the iterates approach the boundary. For a tolerance parameter δ (0,1),
∈
we say that the iterates π are δ-good up to time K if, for i 1,2 , we have
k k≥1
{ } ∈ { }
min πi(ai) δ for k = 1,...,K. (23)
ai∈Ai k ≥
Lemma 2. Suppose that the iterates π are δ-good up to time K. Then for all k = 1,...,K,
k k≥1
{ }
we have
EV 1 β (1 r) EV + 2A2 maxβ k2 + 2AmaxEW +4β τ logA , and (24a)
k+1 ≤ − k − k τ rτ3 k k max
EW (cid:0)(1 α )2+ 3(cid:1)A2 max(1−αk)βk EW + 4Amaxα2 k +4A2 β2+r(1 α )β EV , (24b)
k+1 ≤ − k rτ3 k δ max k − k k k
for any choice o(cid:0)f scalar r (0, 1). (cid:1)
∈ 2
15See Section 4.3.1 for the proof.
A few remarks follow: note that inequality (24b) contains a term that grows as (1/δ) in terms
of the distance to the boundary; this term arises because the variance of the TD updates explodes
at the boundary. For this reason, we need to track the distance to the boundary as a function of
the iteration number. We use the freedom in choosing r (0,1/2) in a later part of the argument.
∈
Our next step is to use Lemma 2 to derive a recursive bound on ET . In particular, summing
k
together inequalities (24a) and (24b) yields
ET 1 β 1 2r EV + 1 α + 5A2 maxβk EW +4τβ logA + 6A3 maxβ k2 + 4Amaxα2 k.
k+1 ≤ − k − k − k rτ3 k k max τ δ
(cid:16) (cid:17) (25)
(cid:0) (cid:0) (cid:1)(cid:1)
The right-hand side contains contraction factors in front of EV and EW ; we would like to relate
k k
these contraction factors so as to form a single term involving the sum ET = EV +EW . We do so
k k k
via a careful choice of the timescale separation constant that relates the two stepsizes via βk = c .
αk α,β
Choosing the separation constant
rτ3
c (r,τ) := (26)
α,β 6A2
max
ensures that c (r,τ) = 1 1 , and hence
α,β 6A2 max ≤ 1−2r+5A2 max
rτ3 rτ3
5A2 β 1 5A2
α + max k = β + max β (1 2r),
− k rτ3 k − c (r,τ) rτ3 ≤ − k −
α,β
(cid:0) (cid:1)
enabling us to match the contraction terms for V and W . Moreover, the choice (26) ensures that
k k
6A3 β2 A α2
max k max k.
τ ≤ δ
Combining these relations with the bound (25) yields
5A2 α2
ET 1 β (1 2r) ET +4τβ logA + max k. (27)
k+1 k k k max
≤ − − δ
(cid:0) (cid:1)
Note that this drift inequality is valid as long as the iterates are δ-good (cf. equation (23)).
Given the drift inequality (27), our next step is to characterize the number of iterations of
Algorithm 2 required to find an ǫ-Nash equilibrium when the iterates π are δ-good till then.
k k≥1
{ }
Our analysis involves the temperature and stepsize parameter specifications
(1 2r)ǫ (1 2r)c (r,τ(r))2δǫ
α,β
τ(r):= − , β(ǫ,r,δ) := − . (28)
12logA 30A2
max max
The choices of τ, β and c ensure that α 1 for all k = 1,2,.... We summarize in the
k k≥1 α,β k
{ } ≤
following:
Lemma 3. For any δ (0,1) and r (0, 1), initialize Algorithm 2 with the timescale separation
∈ ∈ 2
constant from equation (26) and the temperature and stepsize parameters from equation (28). Define
K(ǫ,δ,r) as follows:
16(a) For the constant stepsize β β(ǫ,r,δ),
k
≡
log ǫ
K(ǫ,δ,r) :=
3T1
. (29a)
log 1 β(ǫ,r,δ) 1 2r
(cid:0) (cid:1)
− −
(cid:6) (cid:7)
(cid:0) (cid:0) (cid:1)(cid:1)
β(ǫ,r,δ)
(b) For the inverse polynomial stepsize β = for some exponent η (0,1), and the offset
k (k+k0)η ∈
k = ( 2η )1/(1−η) ,
0 β(ǫ,r,δ)
(cid:6) (cid:7) 1 η T
K(ǫ,δ,r) := − log 1 +(1+k )(1−η) 1/(1−η) k 1 . (29b)
0 0
(1 2r)β(ǫ,r,δ) ǫ − −
−
(cid:6)(cid:0) (cid:1) (cid:7)
If the iterates π of Algorithm 2 are δ-good till time K(ǫ,δ,r), then E[NG(π ] ǫ.
k k≥1 K(ǫ,δ,r)+1
{ } ≤
See Section 4.3.2 for a proof of this claim.
4.2.2 Controlling the distance to the boundary
Now the delicacy with Lemma 3 is that it can be applied to the actual iterates of our process only
when3 they areδ-good atleastup toiteration K(ǫ,δ,r). Moreover, note thatthe stepsizeparameter
β from equation (28) depends on the target accuracy ǫ, as well as the distance δ to the boundary.
As a degree of freedom, our results so far allow us to choose δ (0,1) and r (0,1/2), and we do
∈ ∈
so carefully in the following analysis.
Define K (β,δ) to be the maximum number of iterations of Algorithm 2 with the stepsize
good
parameter β (0,1) that can be taken while ensuring that the iterates remain δ-good (cf. equa-
∈
tion (23)), when the initial mixed strategies π are uniform.
1
For a given target accuracy ǫ (0,1) and ν > 0 for a desired rate of convergence, our proof strategy
∈
is as follows:
• We establish a lower bound on K (β,δ) for any β,δ (0,1).
good ∈
• We then make a careful choice of δ(ǫ,ν) and r(ǫ,ν) as a function of ǫ and ν such that the
lower bound on K β(ǫ,r(ǫ,ν),δ(ǫ,ν)),δ(ǫ,ν) is higher than K ǫ,δ(ǫ,ν),r(ǫ,ν) (defined
good
inLemma 3), implyingthattheiterates π areδ(ǫ,ν)-good tilltimeK ǫ,δ(ǫ,ν),r(ǫ,ν) .
k k≥1
(cid:0) { } (cid:1) (cid:0) (cid:1)
Finally, we recall that the parameter ν > 0 appears in the final rate that w(cid:0)e prove. In th(cid:1)e
following result, we choose the free parameters r and δ as a function of this parameter; seethe proof
for details. The following result summarizes our conclusions:
Lemma 4. Let ν > 0. Define:
(a) For the constant stepsize case,
ǫ 1
1+ν
δ(ǫ,ν) := ,
3T A
1 max
(cid:0) (cid:1)
(b) For the inverse polynomial stepsize case,
ǫ e
1+ν
δ(ǫ,ν) := ,
3T A
1 max
(cid:0) (cid:1)
where e is Euler’s number.
3To beexplicit, theproof of Lemma 3exploits thedrift inequality (27), which is only valid for our process when
theiterates are δ-good.
17There exists a r(ǫ,ν) (0, 1) such that
∈ 2
K β(ǫ,r(ǫ,ν),δ(ǫ,ν)),δ(ǫ,ν) K ǫ,δ(ǫ,ν),r(ǫ,ν) .
good ≥
See Section 4.3.3 for the pr(cid:0)oof. (cid:1) (cid:0) (cid:1)
In order to complete the proof, let us summarize what has been established thus far:
• For any pair of scalars δ (0,1) and r (0, 1), Lemma 3 specifies the number of iterations
∈ ∈ 2
K(ǫ,δ,r) sufficient to ensure that E[NG(π )] ǫ. (This guarantee is predicated upon
K(ǫ,δ,r)+1
≤
the iterates π of Algorithm 2 being δ-good until time K(ǫ,δ,r), and that Algorithm 2
k k≥1
{ }
is initialized according to equations (26) and (28).
• For a given target Nash gap ǫ (0,1) and ν > 0, Lemma 4 specifies the pair (δ,r) as a
∈
function of the pair (ǫ,ν), so that we may write δ(ǫ,ν) and r(ǫ,ν) to indicate this dependence.
Note that Lemma 3 applies to these choices of δ(ǫ,ν) and r(ǫ,ν) as well.
• For these choices of (δ,r), Lemma 4 shows that the iterates π of Algorithm 2 are
k k≥1
{ }
δ(ǫ,ν)-good for at least K⋆ := K ǫ,δ(ǫ,ν),r(ǫ,ν) iterations. (This guarantees assumes that
we initialize with the timescale constant from equation (28).)
(cid:0) (cid:1)
It follows from the results of Lemmas 3 and 4 that the Nash gap evaluated at time K⋆ is
at most ǫ. By construction, the quantity K(ǫ,ν) defined in Theorem 3 is precisely K⋆.
4.3 Proofs of key technical lemmas
We now collect together the proofs of the key technical lemmas that were used in the proof of The-
orem 2 from Section 4.2. More specifically, we prove Lemmas 2, 3 and 4 in Sections 4.3.1, 4.3.2,
and 4.3.3, respectively.
4.3.1 Proof of Lemma 2
This section is devoted to the proof of the two drift inequalities stated in Lemma 2.
Proof of -inequality (24a). Our proof makes use of the previously stated Lemma 1 on the
V
properties of the Lyapunov function . We also require some additional properties, which we
V
summarize in the following auxiliary result:
Lemma 5. The Lyapunov function has the following properties:
V
(a) For any mixed strategy π ∆,
∈
2
τ
(π) σ (Riπ−i) πi 2 .
V ≥ 2 τ − 2
i=1
X(cid:13) (cid:13)
(cid:13) (cid:13)
(b) For any mixed strategy π ∆ and vector pair u = (u1,u2) R|A1| R|A2|, we have
∈ ∈ ×
2
2A
(π),σ (ui) σ (Riπ−i) r (π)+2τ logA + max (π,u),
∇πi V τ − τ ≤ V max rτ3 W
i=1
X(cid:10) (cid:11)
valid for any constant r (0,1).
∈
18See Appendix C.1 for the proof of this auxiliary claim.
From the proof of Theorem 1, recall the inequality
2 2A2
V V +β (π ), σ (Riπ−i) πi + maxβ2. (31)
k+1 ≤ k k h∇π ki V k τ k − ki τ k
nXi=1 o
Next we modify the first-order terms by adding and subtracting σ (Riπ−i) for each i inside the
τ K
inner product,
2 2
(π ),σ (qi) πi = (π ),σ (Riπ−i) πi
∇π ki V k τ k − k ∇π ki V k τ K − k
i=1 i=1
X(cid:10) (cid:11) X(cid:10) (cid:11)
2
+ (π ),σ (qi) σ (Riπ−i) . (32)
∇π ki V k τ k − τ K
i=1
X(cid:10) (cid:11)
Applying Lemma 1(b) and Lemma 5(b) gives upper bounds on each of the terms in the RHS of
equation (32):
2
(π ),σ (Riπ−i) πi πi,π−i +2τ logA , and (33a)
∇π ki V k τ k − k ≤ −V k k max
i=1
X(cid:10) (cid:11) (cid:0) (cid:1)
2
2A
(π),σ (qi) σ (Riπ−i) r (π )+2τ logA + max (π ,q ). (33b)
∇π ki V τ k − τ ≤ V k max rτ3 W k k
i=1
X(cid:10) (cid:11)
Combining equations (31) and (33) yields the claimed inequality (24a).
Proof of the -inequality (24b). Define the i -length vector:
W |A |
Ri(Ai,A−i) qi(Ai)
Fi(Ai,A−i,qi,πi)(ai) =I − ,ai i.
ai=Ai πi(Ai) ∈ A
(cid:0) (cid:1)
Define the following conditional expectation of Fi(Ai,A−i,qi,πi):
Fi (qi) =E Fi(Ai,A−i,qi,πi) qi,π .
k Ai∼π ki,A−i∼π k−i k | k
(cid:2) (cid:3)
Note that Fi (qi) = wi, where wi = qi Riπ−i. Also, by assuming that π δe, as qi(ai)
k k − k k k − k k ≥ k ∈
[ 1,1] for ai i and k 1 (which can be shown by induction),
− ∈ A ≥
Ri(ai,A−i) qi(ai) 2
E F(Ai,A−i,qi,πi) 2 qi,πi,A−i = k − k
k k k k 2 | k k k πi(ai)
(cid:2)(cid:13) (cid:13) (cid:3)
aXi∈Ai (cid:0) k (cid:1)
(cid:13) (cid:13) 4A
max
.
≤ δ
Then from our dynamics,
qi = qi +α Fi(Ai,A−i,qi,πi),
k+1 k k k k k k
Riπ−i = Riπ−i+β Ri σ q−i π−i ,
k+1 k k τ k − k
(cid:0) (cid:0) (cid:1) (cid:1)
19where the latter equation follows by pre-multiplying the smoothed best-response dynamics by Ri.
Subtracting the second equation from the first yields
wi = wi +α F(Ai,A−i,qi,πi) β Ri σ q−i π−i
k+1 k k k k k k − k τ k − k
= (1 −α k)w ki +α
k
F(Ai k,A− ki,q ki,π ki (cid:0)) −(cid:0)Fi k((cid:1)q ki) −(cid:1)β kRi σ
τ
q k−i −π k−i .
(cid:0) (cid:1) (cid:0) (cid:0) (cid:1) (cid:1)
Taking the square of the second norm on both sides,
wi 2 = (1 α )2 wi 2 +α2 F(Ai,A−i,qi) Fi (qi) 2 +β2 Ri σ q−i π−i 2
k+1 2 − k k 2 k k k k − k k 2 k τ k − k 2
(cid:13) +α(cid:13) wi,F(Ai,A(cid:13)−i,q(cid:13)i) Fi (cid:13)(qi) α β F(Ai,A−i,(cid:13)qi) F(cid:13)i (qi(cid:0)),R(cid:0)i σ(cid:1) q−i (cid:1)(cid:13)π−i
(cid:13) (cid:13)k k k (cid:13)k (cid:13)k − k(cid:13) k − k k k k (cid:13)k − (cid:13)k k τ k −(cid:13) k
(cid:10) (cid:11) (cid:10) −β k(1 −α k) w ki,R(cid:0)i σ(cid:0)τ q(cid:1) k−i −π(cid:1) k− (cid:11)i .
(cid:10) (cid:0) (cid:0) (cid:1) (cid:1)(cid:11)
Taking a conditional expectation on both sides of the above equation gives
8A α2
E wi 2 qi,π (1 α )2E wi 2 qi,π + max k +4A2 β2
k+1 2 | k k ≤ − k k 2 | k k δ max k
(cid:2)(cid:13) (cid:13) (cid:3) (cid:2)(cid:13) (cid:13) (cid:3) +β (1 α ) wi,Ri π−i σ q−i . (34)
(cid:13) (cid:13) (cid:13) (cid:13) k − k k k − τ k
(cid:10) (cid:0) (cid:0) (cid:1)(cid:1)(cid:11)
Using the Cauchy-Schwarz inequality, we have for any c > 0,
wi,Ri σ q−i π−i wi Ri π−i σ q−i
k τ k − k ≤ k 2||| |||2 k − τ k 2
A 1
(cid:10) (cid:0) (cid:0) (cid:1) (cid:1)(cid:11) (cid:13) ma(cid:13)x c wi (cid:13)2(cid:0)+ π−i(cid:0) σ(cid:1)(cid:1)(cid:13)q−i 2
≤ (cid:13) 2(cid:13) k(cid:13)2 c k − τ (cid:13)k 2
A max(cid:0) c(cid:13) (cid:13)wi(cid:13) (cid:13)2 + 2(cid:13) (cid:13)π−i σ (cid:0) R−i(cid:1) π(cid:13) (cid:13)i (cid:1) 2 + 2 q−i R−iπi 2 .
≤ 2 k 2 c k − τ k 2 cτ2 k − k 2
(cid:0) (cid:13) (cid:13) (cid:13) (cid:0) (cid:1)(cid:13) (cid:13) (cid:13) (cid:1)
Therefore, (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
2
A 4 2 (π ,q )
wi,Ri σ q−i π−i max c (π ,q )+ (π )+ W k k .
k τ k − k ≤ 2 W k k cτV k cτ2
i=1
X(cid:10) (cid:0) (cid:0) (cid:1) (cid:1)(cid:11) (cid:0) (cid:1)
Choosing c= 2Amax for any r (0,1) gives us
rτ ∈
2 3A2
wi,Ri σ q−i π−i max (π ,q )+r (π ). (35)
k τ k − k ≤ rτ3 W k k V k
i=1
X(cid:10) (cid:0) (cid:0) (cid:1) (cid:1)(cid:11)
Combining the inequalities in equations (34) and (35) gives the overall drift inequality
3A2 (1 α )β
E (π ,q ) (1 α )2+ max − k k E (π ,q )
W k+1 k+1 ≤ − k rτ3 W k k
(cid:2) (cid:3) (cid:0) 4A α2 (cid:1) (cid:2) (cid:3)
+ max k +4A2 β2+r(1 α )β E (π ) . (36)
δ max k − k k V k
(cid:2) (cid:3)
4.3.2 Proof of Lemma 3
We begin with the drift inequality (27) on the total Lyapunov values T . Solving this drift
k k≥1
{ }
inequality gives the following bounds on the expected Nashgap attime K+1 if π are δ-good
k k≥1
{ }
till time K.
20Lemma 6. If the iterates π are δ-good for times k = 1,...,K, then:
k k≥1
{ }
(a) For the constant stepsize β β,
k
≡
4 5A2 β
ENG(π ) (1 β(1 2r))KT + τ logA + max .
K+1 ≤ − − 1 1 2r max (1 2r)c (r,τ)2δ
α,β
− −
(b) For the inverse polynomial stepsize β = β for some exponent η (0,1) and offset
k (k+k0)η ∈
k = (2η)1/(1−η) ,
0 β
(cid:6) (cid:7)
ENG(π )
exp −(1 (− 1−2r η) )β (K +k 0+1)1−η
T +
4
τ logA
K+1 ≤ exp (1−2r)β (1+k )η 1 1 2r max
− (1−η) 0 −
10A2 β
+ max .
(1 2r)c (r,τ)2δ(K +k )η
α,β 0
−
The results in Lemma 3 then follow from Lemma 6 by setting each term in the upper bounds
on the Nash gap to be equal to ǫ.
3
Proof of Lemma 6. We split our analysis into different cases, depending on the choice of stepsizes.
Constant stepsizes. For constant stepsizes β β (0,1), by iterating the drift inequality (27),
k
≡ ∈
K
ET 1 β 1 2r KET +4τ logA β 1 (1 2r)β K−j (37)
K+1 1 max
≤ − − − −
j=1
(cid:0) (cid:0) (cid:1)(cid:1) X (cid:0) (cid:1)
5A2 K
+ max β2 1 (1 2r)β K−j .
c (r,τ)2δ − −
α,β
j=1
X (cid:0) (cid:1)
(38)
Bounding the second term: Consider the sequence u defined with the initialization u = 0
k k≥1 1
{ }
and the recursion u = 1 (1 2r)β u +(1 2r)β. Then for K = 1,2,..., we have
k+1 k
− − −
(cid:0) (cid:1)
K
K−j
u = (1 2r)β 1 (1 2r)β .
K+1
− − −
j=1
X (cid:0) (cid:1)
Since β 1 and r < 0.5, it follows from an inductive argument that u 1 for all k = 1,2,....
k
≤ ≤
Using this fact, we have
K
4
K−j
4τ logA β 1 (1 2r)β τ logA .
max max
− − ≤ 1 2r
j=1 −
X (cid:0) (cid:1)
Bounding the third term: Note that
K K
1
K−j j
1 (1 2r)β = 1 (1 2r)β .
− − − − ≤ (1 2r)β
j=1 j=0 −
X(cid:0) (cid:1) X(cid:0) (cid:1)
21Using this fact, it follows that
5A2 K 5A2 β
max β2 1 (1 2r)β K−j max .
c (r,τ)2δ − − ≤ (1 2r)c (r,τ)2δ
α,β α,β
j=1 −
X (cid:0) (cid:1)
Thus, equation (38) can be simplified to
4 5A2 β
ET 1 β 1 2r KET + τ logA + max .
K+1 ≤ − − 1 1 2r max (1 2r)c (r,τ)2δ
α,β
− −
(cid:0) (cid:0) (cid:1)(cid:1)
Inverse linear stepsizes. For the case of sublinearly decaying stepsizes, by assuming that for all
k K, π ∆ , we have
k δ
≤ ∈
K K K
T 1 (1 2r)β T +4τ logA β 1 (1 2r)β
K+1 i 1 max i j
≤ − − − −
i=1 i=1 j=i+1
Y(cid:0) (cid:1) X Y (cid:0) (cid:1)
5A2 K K
+ max α2 1 (1 2r)β .
c (r,τ)2δ i − − j
α,β
i=1 j=i+1
X Y (cid:0) (cid:1)
Bounding the first term:
K K
1 (1 2r)β = exp log 1 (1 2r)β
i i
− − − −
i=1 i=1
Y(cid:0) (cid:1) (cid:0)X (cid:0) (cid:1)(cid:1)
K
(i)
exp (1 2r)β
i
≤ − −
i=1
(cid:0) X (cid:1)
(ii) K+1 1
exp (1 2r)β dx
≤ − − (x+k )η
Z1 0
(cid:0) (1 2r)β (cid:1)
= exp − (K +k +1)1−η (1+k )η .
0 0
− (1 η) −
−
(cid:0) (cid:0) (cid:1)(cid:1)
where (i) follows from the inequality log(1 x) x for x [0,1) and (ii) follows from bounding
− ≤ − ∈
the Riemann sum.
Bounding the second term: Consider the sequence u defined by the initialization u = 0
k k≥1 1
{ }
followed by the recursion u = 1 (1 2r)β u +(1 2r)β . Then for K = 1,2,..., we have
k+1 k k k
− − −
(cid:0) (cid:1)
K K
u = (1 2r)β 1 (1 2r)β .
K+1 i j
− − −
i=1 j=i+1
X Y (cid:0) (cid:1)
Note that β 1 for k = 1,2... and r < 1 by assumption; therefore, it follows from an inductive
k ≤ 2
argument that u 1 for all k = 1,2,.... Using this fact, we have the bound
k
≤
K K
4
4τ logA β 1 (1 2r)β τ logA .
max i j max
− − ≤ 1 2r
i=1 j=i+1 −
X Y (cid:0) (cid:1)
22Bounding the third term: Consider the sequence u defined by the initialization u = 0 and
k k≥1 1
{ }
the recursion u = 1 (1 2r)β u +β2. Then for K = 1,2,..., we have
k+1 − − k k k
(cid:0) (cid:1)
K K
u = β2 1 (1 2r)β .
K+1 i − − j
i=1 j=i+1
X Y (cid:0) (cid:1)
It can be shown by induction [see Che+23b, p.36] that u 2 β for k = 1,2,..., from which
k ≤ (1−2r) k
it follows that
5A2 K K 10A3 β
max β2 1 (1 2r)β max .
c (r,τ)2δ i − − j ≤ (1 2r)c (r,τ)2δ(K +k )η
α,β α,β 0
i=1 j=i+1 −
X Y (cid:0) (cid:1)
4.3.3 Proof of Lemma 4
We prove the following more general claim:
Lemma 7. Suppose that Algorithm 2 is initialized with uniform initial strategies π .
1
(a) For any ξ(ν) (1,1+ν), there exists r(ν) (0, 1) such that Algorithm 2 with the constant
∈ ∈ 2
stepsize β β(ǫ,r(ν),δ(ǫ,ν)) and δ(ǫ,ν) := ǫ 1+ν 1 satisfies
k ≡ 3T1 Amax
(cid:0) (cid:1) ξ(ν)log ǫ
K β(ǫ,r(ǫ,ν),δ(ǫ,ν)),δ(ǫ,ν)
3T1
. (39a)
good
≥ log 1 (1 2r(ν))β(ǫ,r(ν),δ(ǫ,ν))
(cid:0) (cid:1)
− −
(cid:0) (cid:1)
(cid:0) (cid:1)
β(ǫ,r,δ(ǫ,ν))
(b) Consider the inverse polynomial stepsize β = for some exponent η (0,1), the off-
k (k+k0)η ∈
set k = (2η)1/(1−η) , andδ(ǫ,ν) := ǫ 1+ν e . Then forany choice of ξ(ν) (1,√1+ν)
0 β 3T1 Amax ∈
such that log(1 β ) ξ(ν)β , there exists some r(ν) (0, 1) such that
(cid:6) − 1(cid:7)≥ − 1 (cid:0) (cid:1) ∈ 2
K β(ǫ,r(ǫ,ν),δ(ǫ,ν)),δ(ǫ,ν) ξ(ν) (1 −η) log 3T 1 +(1+k )1−η 1−1 η k ,
good ≥ 1 2rβ(ǫ,r(ν),δ(ǫ,ν)) ǫ 0 − 0
−
(cid:0) (cid:1) (cid:2) (cid:0) (cid:1) (cid:3) (39b)
In the above lemma, ξ(ν) > 1 is the factor by which K (β,δ) is greater than K(ǫ,δ,r) (cf.
good
Lemma3)forthespecificchoicesofδ andr mentioned inthelemma. OurproofofLemma7exploits
the following auxiliary result:
Lemma 8. Let β,δ (0,1). Either of the following two conditions are sufficient to ensure that the
∈
iterates π of Algorithm 2 with timescale constant β are δ-good up to time K.
k k≥1
{ }
(a) For the constant stepsize β β, it suffices to have K
log(Amaxδ)
.
k ≡ ≤ log(1−β)
(b) For β (0, 1), suppose that β = β where η (0,1) and k = 2η 1/η , and ξ > 1 is
∈ 2 k (k+k0)η ∈ 0 β
chosen to ensure that log(1 β ) ξβ . Then it suffices to have
− 1 ≥ − 1 (cid:0) (cid:1)
(1 η) e
(K +k )1−η − log +(1+k )1−η,
0 0
≤ ξβ A δ
max
(cid:0) (cid:1)
23SeeAppendix C.2fortheproof. Equivalently, theupperbounds onK inLemma 8arelower bounds
on K (β,δ).
good
Now, we choose δ and r as a function of ǫ and ν to ensure that these lower bounds on K (β,δ)
good
for the choice of β in Lemma 3 are bigger than K(ǫ,δ,r). The statement in Lemma 4 follows
for the specific choice of δ(ǫ,ν) = ǫ 1+ν 1 for the case of constant stepsizes and δ(ǫ,ν) =
3T1 Amax
ǫ 1+ν e for the case of sublinearly decaying stepsizes. The exact value of r will be discussed
3T1 Amax (cid:0) (cid:1)
below for each stepsize.
(cid:0) (cid:1)
Constant stepsizes. In this case, we have β β for any β (0,1). Let ξ (1,1+ν). There
k
≡ ∈ ∈
exists a small enough r(ν) (0, 1) such that
∈ 2
log(1 β) 1+ν
− . (40)
log 1 β 1 2r(ν) ≤ ξ
− −
Applying this bound yields (cid:0) (cid:0) (cid:1)(cid:1)
log(A δ(ǫ,ν)) log ǫ ξlog ǫ
max
= (1+ν)
3T1 3T1
.
log(1 β) log(1 β) ≥ log 1 β 1 2r(ν)
(cid:0) (cid:1) (cid:0) (cid:1)
− − − −
Therefore, the iterates π remain δ(ǫ,ν)-good up until t(cid:0) ime (cid:0) (cid:1)(cid:1)
k k≥1
{ }
ξlog ǫ
3T1
.
log 1 β 1 2r(ν)
(cid:0) (cid:1)
− −
(cid:0) (cid:0) (cid:1)(cid:1)
The statement of Lemma 7 follows for β = β(ǫ,δ(ǫ,ν),r(ν)).
Inverse polynomial stepsizes. Let ξ(ν) (1,√1+ν). If β = β(ǫ,δ,r) where η (0,1), k =
∈ k (k+k0)η ∈ 0
2η 1/η , as β(ǫ,δ,r) 0 as r 0 (equation (28)), there will be a small enough r(ν) that satisfies
β → →
log(1 β(ǫ,δ,r(ν))) ξ(ν)β(ǫ,δ,r(ν)). Find a small enough r(ν) (0, 1) such that we also have
(cid:0) (cid:1) − ≥ − ∈ 2
ξ2(ν)
1+ν . (41)
≥ 1 2r(ν)
−
Since ξ2(ν) < 1+ν by assumption, such a choice for r(ν) is possible. With such a choice, we have
1 η e (1 η) 3T
1
− log = 1+ν − log
ξ(ν)β(ǫ,δ(ǫ,ν),r(ν)) A δ(ǫ,ν) ξ(ν)β(ǫ,δ(ǫ,ν),r(ν)) ǫ
max
(cid:0) (cid:1) (cid:0) (cid:1) ξ(ν)(1 η) (cid:0)3T (cid:1)
1
− log .
≥ β(ǫ,δ(ǫ,ν),r(ν))(1 2r(ν)) ǫ
−
(cid:0) (cid:1)
Therefore, the iterates π are δ(ǫ,ν)-good for all K such that
k k≥1
{ }
ξ(ν) (1 η) 3T
(K +k )1−η − log 1 +(1+k )1−η.
0 0
≤ 1 2r(ν)β(ǫ,δ(ǫ,ν),r(ν)) ǫ
−
(cid:0) (cid:1)
245 Discussion
In this paper, we studied best-response type learning dynamics that arise from the discretization
of continuous-time fictitious play applied to two-player zero-sum matrix games. We analyzed these
dynamics in both the full information setting as well as the more challenging setting of minimal
information, in which each player observes only their random payoff. In the latter context, we
providedthefirstpolynomial-scaling finite-sampleguarantees forthesebest-responsetypedynamics
in the minimal information setting without additional exploration. The analysis involved some new
ideas,includingcarefultrackingofvarianceofthestochasticupdatesastheyapproachtheboundary
of the probability simplex.
We note that there is a straightforward extension of our results to two-player constant-sum
games, even if the players are unaware of the associated constant sum of the payoffs. In terms
of open questions, we suspect that our current results are not sharp in terms of dependence on ǫ
and A for last-iterate convergence to a Nash equilibrium. For instance, in the simpler setting of
max
full information, the optimal rate is known to scale as (1); we are not yet aware if our fictitious
O ǫ
play dynamics can achieve this rate, or if the (1) guarantees that we have provided are, in
O ǫ2
fact, unimprovable. Additionally, our analysis in this paper was based upon a fixed temperature
parameter τ. Studying dynamics withatime-varying temperature τ might helpeliminate the
k k≥1
{ }
constant smoothingbias inthecurrent bounds ontheNashgap. Finally, amoregeneral direction of
interest is finding other classes of games forwhich it is possibleto establish finite-sample guarantees
with polynomial scaling.
Acknowledgments
FZF was supported by theIrwinMarkJacobs andJoan KleinJacobs Presidential Fellowship. MJW
was partially supported by ONR grant N00014-21-1-2842 and NSF DMS-2311072.
References
[ALW21] J. Abernethy, K. A. Lai, and A. Wibisono. “Fast convergence of fictitious play for diag-
onal payoff matrices”. In: Proceedings of the 2021 ACM-SIAM Symposium on Discrete
Algorithms (SODA). SIAM. 2021, pp. 1387–1404 (cit. on p. 3).
[Bak+22] A. Bakhtin et al. “Mastering the game of no-press Diplomacy via human-regularized
reinforcement learning and planning”. In: arXiv preprint arXiv:2210.05492 (2022) (cit.
on p. 3).
[Bor22] V. S. Borkar. Stochastic Approximation: A Dynamical Systems Viewpoint (2nd ed.)
Hindustan Book Agency and Springer Nature, 2022 (cit. on p. 2).
[Bro49] G. W. Brown. Some notes on computation of games solutions. Tech. rep. P-78. The
Rand Corporation, 1949 (cit. on pp. 1, 3).
[Bro51] G. W. Brown. “Iterative Solution of Games by Fictitious Play”. In: Activity Analysis
of Production and Allocation. Ed. by T. C. Koopmans. New York: Wiley, 1951 (cit. on
pp. 1, 3).
[Cai+23] Y. Cai et al. “Uncoupled and Convergent Learning in Two-Player Zero-Sum Markov
GameswithBanditFeedback”. In:Advances in Neural Information Processing Systems.
Ed. by A. Oh et al. Vol. 36. Curran Associates, Inc., 2023, pp. 36364–36406 (cit. on
p. 4).
25[Che+23a] Z.Chenetal.“AFinite-SampleAnalysisofPayoff-BasedIndependentLearninginZero-
Sum Stochastic Games”. In: arXiv preprint arXiv:2303.03100 (2023) (cit. on pp. 2–4,
7, 10, 11).
[Che+23b] Z. Chen et al. A Lyapunov Theory for Finite-Sample Guarantees of Asynchronous Q-
Learning and TD-Learning Variants. 2023. arXiv: 2102.01567 [cs.LG] (cit. on pp. 23,
32).
[CL06] N. Cesa-Bianchi and G. Lugosi. Prediction, Learning, and Games. Cambridge Univer-
sity Press, 2006 (cit. on p. 3).
[CP20] X. Chen and B. Peng. “Hedging in games: Faster convergence of external and swap
regrets”. In: Advances in Neural Information Processing Systems 33 (2020), pp. 18990–
18999 (cit. on p. 3).
[CWC21] S. Cen, Y. Wei, and Y. Chi. “Fast policy extragradient methods for competitive games
with entropy regularization”. In: Advances in Neural Information Processing Systems
34 (2021), pp. 27952–27964 (cit. on p. 3).
[Dan67] J. M. Danskin. The theory of max-min and its application to weapons allocation prob-
lems. Springer, 1967 (cit. on p. 13).
[DDK] C. Daskalakis, A. Deckelbaum, and A. Kim. “Near-Optimal No-Regret Algorithms
for Zero-Sum Games”. In: Proceedings of the 2011 Annual ACM-SIAM Symposium on
Discrete Algorithms (SODA), pp. 235–254. eprint: https://epubs.siam.org/doi/pd
f/10.1137/1.9781611973082.21 (cit. on p. 3).
[DP14] C. Daskalakis and Q. Pan. “A counter-example to Karlin’s strong conjecture for fic-
titious play”. In: 2014 IEEE 55th Annual Symposium on Foundations of Computer
Science. IEEE. 2014, pp. 11–20 (cit. on p. 3).
[FK93] D.Fudenberg andD.M.Kreps.“Learning MixedEquilibria”. In:Games and Economic
Behavior 5.3 (1993), pp. 320–367 (cit. on p. 3).
[FY98] D.P.FosterandH.P.Young.“Onthenonconvergence of fictitious playincoordination
games”. In: Games and Economic Behavior 25.1 (1998), pp. 79–96 (cit. on p. 3).
[Har98] C.Harris.“OntheRateofConvergenceofContinuous-TimeFictitiousPlay”.In:Games
and Economic Behavior 22.2 (1998), pp. 238–259 (cit. on pp. 2, 3, 8, 10).
[HJ12] R. A. Horn and C. R. Johnson. Matrix analysis. Cambridge university press, 2012 (cit.
on p. 14).
[HM00] S. Hart and A. Mas-Colell. “A simple adaptive procedure leading to correlated equilib-
rium”. In: Econometrica 68.5 (2000), pp. 1127–1150 (cit. on p. 3).
[HS02] J.HofbauerandW.H.Sandholm. “OntheGlobal Convergence ofStochastic Fictitious
Play”. In: Econometrica 70.6 (2002), pp. 2265–2294 (cit. on p. 3).
[Kar59] S. Karlin. Mathematical Methods and Theory in Games, Programming and Economics.
Vol. 2: The Theory of Infinite Games. Addison-Wesley Publishing Company, 1959 (cit.
on p. 3).
[LC03] D. S. Leslie and E. J. Collins. “Convergent multiple-timescales reinforcement learning
algorithms in normal form games”. In: The Annals of Applied Probability 13.4 (2003),
pp. 1231–1251 (cit. on p. 3).
26[LC05] D. S. Leslie and E. J. Collins. “Individual Q-Learning in Normal Form Games”. In:
SIAM Journal on Control and Optimization 44.2 (2005), pp. 495–514. eprint: https
://doi.org/10.1137/S0363012903437976 (cit. on pp. 2, 3, 7).
[Miy61] K. Miyasawa. On the convergence of the learning process in a 2 x 2 non-zero-sum
two-person game. Princeton University Princeton, 1961 (cit. on p. 3).
[MS96a] D. Monderer and L. S. Shapley. “Fictitious play property for games with identical
interests”. In: Journal of economic theory 68.1 (1996), pp. 258–265 (cit. on p. 3).
[MS96b] D. Monderer and L. S. Shapley. “Potential games”. In: Games and economic behavior
14.1 (1996), pp. 124–143 (cit. on p. 3).
[Neu28] J. von Neumann. “Zur Theorie der Gesellschaftsspiele”. In: Mathematische Annalen.
Vol. 100. 1928, pp. 295–320 (cit. on p. 4).
[Ren+24] T. Renard et al. Convergence of a model-free entropy-regularized inverse reinforcement
learning algorithm. 2024. arXiv: 2403.16829 [cs.LG] (cit. on p. 4).
[Rob51] J.Robinson.“AnIterativeMethodofSolvingaGame”.In:Annals of Mathematics 54.2
(1951), pp. 296–301 (cit. on pp. 1, 3).
[RS13] S. Rakhlin and K. Sridharan. “Optimization, learning, and games with predictable
sequences”. In: Advances in Neural Information Processing Systems 26 (2013) (cit. on
p. 3).
[Say+21] M. Sayin et al. “Decentralized Q-learning in zero-sum Markov games”. In: Advances in
Neural Information Processing Systems 34 (2021), pp. 18320–18334 (cit. on p. 7).
[Sha58] H. N. Shapiro. “Note on a computation method in the theory of games”. In: Commu-
nications on Pure and Applied Mathematics 11.4 (1958), pp. 587–593 (cit. on p. 3).
[Sha63] L. S. Shapley. “Some topics in two-person games”. In: 1963 (cit. on p. 3).
[Sok+22] S. Sokota et al. “A unified approach to reinforcement learning, quantal response equi-
libria, and two-player zero-sum games”. In: arXiv preprint arXiv:2206.05825 (2022)
(cit. on p. 3).
[Sut88] R.S.Sutton.“Learningtopredictbythemethodsoftemporaldifferences”.In:Machine
learning 3 (1988), pp. 9–44 (cit. on pp. 2, 7).
[Syr+15] V. Syrgkanis et al. “Fast convergence of regularized learning in games”. In: Advances
in Neural Information Processing Systems 28 (2015) (cit. on p. 3).
[WJ08] M. J. Wainwright and M. I. Jordan. “Graphical models, exponential families and varia-
tional inference”. In: Foundations and Trends in Machine Learning 1 (2008), pp. 1–305
(cit. on p. 28).
[ZDR22] S.Zeng, T. Doan, and J.Romberg. “Regularized gradient descent ascent fortwo-player
zero-sum Markov games”. In: Advances in Neural Information Processing Systems 35
(2022), pp. 34546–34558 (cit. on p. 3).
27A Proof of Proposition 1
We prove the claimed equivalence (16) as a consequence of the following auxiliary result: the KL-
based function can be written as
KL
V
2
(π) = H(πi)+ (R−iπi) , (42)
KL τ
V − G
i=1
X(cid:8) (cid:9)
where (θ):= log eθ(a)/τ for any vector θ R|Ai|.
Gτ a∈Ai
∈
Taking this auxiliary result (42) as given, let us complete the proof of the equivalence (16). By
(cid:0)P (cid:1)
standard results on exponential families (e.g., [WJ08]), the function is convex, and its Legendre
τ
G
dual (up to a rescaling by τ) is the Shannon entropy. Consequently, we can write
1 1
(Riπ−i)= max π, Riπ−i +H(π) = max π,Riπ−i +τH(π) .
τ
G πb∈∆i τ τπb∈∆i
(cid:8)(cid:10) (cid:11) (cid:9) (cid:8)(cid:10) (cid:11) (cid:9)
Substituting this equivalenceintobtheauxiliary claibm(42)andre-abrranging yieldsthbeclaimed equiv-
alence (π) τ (π).
alt KL
V ≡ V
It remains to prove the auxiliary claim (42). By definition of the function , we have
KL
V
2 2
(π) = KL πi,σ Riπ−i ( =i) τ H πi πi,logσ Riπ−i
KL τ τ
V − −
i=1 i=1
X (cid:0) (cid:0) (cid:1)(cid:1) X(cid:8) (cid:0) (cid:1) (cid:10) (cid:0) (cid:1)(cid:11)(cid:9)
2
( =ii) H πi πi, 1 Riπ−i G Riπ−i 1
τ
− − τ −
i=1
X(cid:8) (cid:0) (cid:1) (cid:10) (cid:0) (cid:1) (cid:11)(cid:9)
2 2
1
= H πi + Riπ−i πi,Riπ−i
τ
− G − τ
i=1 i=1
X(cid:8) (cid:0) (cid:1) (cid:0) (cid:1)(cid:9) X(cid:10) (cid:11)
2
(i =ii) H πi +G R−iπi ,
τ
−
i=1
X(cid:8) (cid:0) (cid:1) (cid:0) (cid:1)(cid:9)
where step (i) follows from the definition of the KL divergence; step (ii) follows from the definition
of ; and step (iii) follows from the zero-sum property.
τ
G
Finally, we establish uniqueness of the τ-regularized Nash equilibrium (6). Note that any τ-
regularized NE is a global minimizer of over the set ∆. Since is continuous and the
KL KL
V V
set ∆ is compact, the global minimum is achieved. Since is a strictly convex function, the
alt
V
equivalence (16) ensures that is also strictly convex. Consequently, the function has a
KL KL
V V
unique global minimum, meaning that there is a unique τ-regularized NE.
B Auxiliary calculations for Theorem 1
In this appendix, we work through the cases of inverse linear and inverse polynomial stepsizes,
corresponding to the claims in parts (b) and (c) of Theorem 1.
28B.1 Inverse linear stepsizes
Consider the inverse linear stepsizes β = β for some β > 0. We have that V := π satisfies
k k k V k
the equation
(cid:0) (cid:1)
K 2A2 K K K K
π 1 β (π )+ max β2 (1 β )+2τ logA β (1 β ).
V K+1 ≤ − k V 1 τ k − j max k − j
k=1 k=1 j=k+1 k=1 j=k+1
(cid:0) (cid:1) Y(cid:0) (cid:1) X Y X Y
(43)
Bounding the first term:
K K K
(a)
(1 β ) = exp log(1 β ) exp β
k k k
− − ≤ −
k=1 k=1 k=1
Y (cid:0)X (cid:1) (cid:0) X (cid:1)
(b) K+1 1
exp β dx
≤ − x
Z1
1(cid:0) (cid:1)
β
= ,
K +1
(cid:0) (cid:1)
where in (a), we used the bound log(1 x) x for x (0,1) and in (b), we bounded the Riemann
− ≤ − ∈
sum.
Bounding the second term: Usingaseries ofarguments similartotheupperboundforthe firstterm
yields
K K K K+1 1
(1 β )= exp log(1 β ) exp β exp β dx (44)
j j j
− − ≤ − ≤ − x
j=k+1 j=k+1 j=k+1
Zk+1
Y (cid:0) X (cid:1) (cid:0) X (cid:1) (cid:0) (cid:1)
k+1
β
= . (45)
K +1
(cid:0) (cid:1)
Substituting this bound into the expression for the second term yields
2A2 K K 2A2 K k+1 8A2 β2 K 1
max β2 (1 β ) max β2 β max .
τ k − j ≤ τ k K +1 ≤ τ(K +1)β (k+1)2−β
k=1 j=k+1 k=1 k=1
X Y X (cid:0) (cid:1) X
We now consider five cases.
Case 1: If β = 1, then we have
K 1 K+1 1
dx = log(K +1).
(k+1)2−β ≤ x
k=1
Z1
X
Therefore,
8A2 β2 K 1 8A2 log(K +1)
max max .
τ(K +1)β (k+1)2−β ≤ τ(K +1)
k=1
X
29Case 2: If β > 2, then we can write
K 1 K+1 K+2 (K +2)β−1
= kβ−2 xβ−2dx .
(k+1)2−β ≤ ≤ β 1
k=1 k=2
Z2
−
X X
Therefore,
8A2 β2 K 1 8A2 β2(K +2)β−1
max max
τ(K +1)β (k+1)2−β ≤ τ(β 1)(K +1)β
k=1 −
X
8A2 β22β−1
max .
≤ τ(β 1)(K +1)
−
Case 3: If β = 2, then
K
1
= K.
(k+1)2−β
k=1
X
Therefore,
8A2 β2 K 1 8A2 β2K
max max
τ(K +1)β (k+1)2−β ≤ τ(K +1)β
k=1
X
32A2
max.
≤ τK
Case 4: If β (1,2), then
∈
K 1 K+1 1 K+1 1 Kβ−1
= dx .
(k+1)2−β k2−β ≤ x2−β ≤ β 1
k=1 k=2
Z1
−
X X
Therefore,
8A2 β2 K 1 8A2 β2Kβ−1
max max
τ(K +1)β (k+1)2−β ≤ τ(β 1)(K +1)β
k=1 −
X
8A2 β2
max .
≤ τ(β 1)K
−
Case 5: Finally, if β (0,1), then we have
∈
K 1 K+1 1 K+1 1 1
= dx .
(k+1)2−β k2−β ≤ x2−β ≤ 1 β
k=1 k=2
Z1
−
X X
Therefore,
8A2 β2 K 1 8A2 β2
max max .
τ(K +1)β (k+1)2−β ≤ τ(1 β)(K +1)β
k=1 −
X
30Bounding the third term: Using equation (44) and a series of arguments similar to the upper bound
for the second term:
K K K
k+1
β
2τ logA β (1 β ) 2τ logA β
max k j max k
− ≤ K +1
k=1 j=k+1 k=1
X Y X (cid:0) (cid:1)
K
4τ logA β 1
max
.
≤ (K +1)β (k+1)1−β
k=1
X
Case 1: If β = 1, then we have
K
1
= K.
(k+1)1−β
k=1
X
Therefore,
K
4τ logA β 1
max
4τ logA .
(K +1)β (k+1)1−β ≤ max
k=1
X
Case 2: If β > 1, then we can write
K 1 K+1 K+2 (K +2)β
= kβ−1 xβ−1dx .
(k+1)1−β ≤ ≤ β
k=1 k=2
Z2
X X
Therefore,
4τ logA β K 1 4τ logA (K +2)β
max = max 4τ2βlogA .
(K +1)β (k+1)1−β (K +1)β ≤ max
k=1
X
Case 3: If β < 1, then
K 1 K+1 1 K+1 1 (K +1)β
= dx .
(k+1)1−β k1−β ≤ x1−β ≤ β
k=1 k=2
Z1
X X
Therefore,
K
4τ logA β 1
max
= 4τ logA .
(K +1)β (k+1)1−β max
k=1
X
The statement of Theorem 1 for inverse linear stepsizes is for the specific case when β (1,2].
∈
B.2 Inverse polynomial stepsize
Nowsupposethatβ = β ,η (0,1)andβ (0,1). Recallthat π satisfiestheinequality
K (K+k0)η ∈ ∈ V K
(cid:0) (cid:1)
K 2A2 K K K K
π 1 β V + max β2 (1 β )+2logA τ β (1 β ), (46)
V K ≤ − k 1 τ k − j max k − j
k=1 k=1 j=k+1 k=1 j=k+1
(cid:0) (cid:1) Y(cid:0) (cid:1) X Y X Y
31Bounding the first term:
K K+1 1
(1 β ) exp β dx
− k ≤ − (x+k )η
k=1
Z1 0
Y (cid:0) (cid:1)
β
= exp (K +k +1)1−η (1+k )1−η .
0 0
− 1 η −
−
(cid:0) (cid:0) (cid:1)(cid:1)
Bounding the second term: We define the sequence u via the recursion
k k≥1
{ }
u = 1 β u +β2 with initialization u = 0.
k+1 − k k k 1
(cid:0) (cid:1)
Unwrapping this recursion, we find that u = K β2 K (1 β ). It can be shown by
K k=1 k j=k+1 − j
induction [see Che+23b, p.36] that u 2β . Therefore,
K K
≤ P Q
2A2 K K 2A2 u 4A2 β 4βA2
max β2 (1 β ) = max K max K = max .
τ k − j τ ≤ τ τ(K +k )η
0
k=1 j=k+1
X Y
Bounding the third term: We define the sequence u via the recursion
k k≥1
{ }
u = 1 β u +β with initial value u = 0.
k+1 k k k 1
−
(cid:0) (cid:1)
Note that since β (0,1), it follows that u 1 for all k = 1,2,.... Expanding out the recursion
k k
yields u = K β∈ K (1 β ). Combin≤ ing with the inequality u 1, we find that
K k=1 k j=k+1 − j k ≤
P Q β 4βA2
π exp (K +k +1)1−c (1+k )1−c (π )+ max +2τ logA .
V K+1 ≤ − 1 η 0 − 0 V 1 τ(K +k )η max
0
−
(cid:0) (cid:1) (cid:0) (cid:0) (cid:1)(cid:1)
C Auxiliary results for Theorem 2
In this appendix, we collect together the proofs of various auxiliary results used in proving Theo-
rem 2.
C.1 Proof of Lemma 5
We prove each of the two parts in turn.
Proof of part (a). For any pair π = (π1,π2) of mixed strategies, define the function
Fi(πi,π−i) = max πi πi ⊺ Riπ−i+τH(πi) τH(πi) , i = 1,2,
πbi∈∆i − −
(cid:8)(cid:0) (cid:1) (cid:9)
and observe that Fi(πi,π−i) 0. Abdditionally, Fi(σ (Ribπ−i),π−i) = 0. Therefore, by the first-
τ
≥
order optimality condition, we must have
Fi(σ (Riπ−i),π−i), π π = 0 for all mixed strategies π ,π ∆ .
πi τ 1 2 1 2 i
h∇ − i ∈
32Note that as the negative of the Shannon entropy is 1-strongly convex on the probability simplex;
consequently, the function Fi is τ-strongly convex in its first argument. Therefore, we have
Fi(πi,π−i)= Fi(πi,π−i) Fi(σ (Riπ−i),π−i)
τ
−
τ
Fi(σ (Riπ−i),π−i), πi σ (Riπ−i) + σ (Riπ−i) πi 2
≥ h∇πi τ − τ i 2 τ − 2
τ
= σ (Riπ−i) πi 2 . (cid:13) (cid:13)
2 τ − 2 (cid:13) (cid:13)
(cid:13) (cid:13)
Using this lower bound, we(cid:13)can write (cid:13)
τ (i)
σ (Riπ−i) πi 2 Fi(πi,π−i) (π),
2 τ − 2 ≤ ≤ V
i=1,2 i=1,2
X (cid:13) (cid:13) X
(cid:13) (cid:13)
where step (i) follows from the non-negativity of the Shannon entropy. This completes the proof.
Proof of part (b). For each i 1,2 , consider the constrained optimization problem
∈ { }
max πi, ui +τH(πi) such that πi(a) = 1.
πi∈R|Ai| h i a
+
(cid:8) (cid:9) P
Thegradient H(πi) diverges whenever any element ofπi approaches zero, sothat we canargue
2
k∇ k
that the optimum will be achieved at a vector πi with strictly positive co-ordinates. We introduce a
⋆
Lagrange multiplier λ for the sum-constraint. The associated KKT conditions imply the optimum
πi satisfies the condition
⋆
ui+τ H(πi)+λe = 0,
∇ ⋆
where e is a vector of all ones, and λ is the Lagrange multiplier. Solving for the optimum and using
the fact that λ is chosen to ensure that πi satisfies the normalization constraint yields πi =σ (ui).
⋆ ⋆ τ
Equivalently, we have shown that ui+τ H(σ (ui))+λe= 0, from which it follows that
τ
∇
ui +τ H(σ (ui)), π π = 0 for any pair π ,π ∆ .
τ 1 2 1 2 i
h ∇ − i ∈
Consequently, we can write
(π),σ (ui) σ (Riπ−i) = R−i⊺ σ (R−iπi),σ (ui) σ (Riπ−i) = Si +Si,
∇1 V τ − τ τ τ − τ 1 2
(cid:10) (cid:11) (cid:10) (cid:11)
where we define
Si := τ H(σ (ui)),σ (ui) σ (Riπ−i), and
1 ∇ τ τ − τ
S 2i := R(cid:10)−i⊺ σ τ(R−iπi)+ui,σ τ(ui) −σ τ(Riπ−i) .
(cid:10) (cid:11)
In order to complete the proof, it suffices to show that
2 2 2
(a) (b) 2A
Si 2τ logA , and Si r (π)+ max ui Riπ−i 2 . (47)
1 ≤ max 2 ≤ V rτ3 − 2
i=1 i=1 i=1
X X X (cid:13) (cid:13)
(cid:13) (cid:13)
We prove each of these two claims in turn.
33Proof of the bound (47)(a). SinceH isaconcavefunction,thefirst-ordertangentboundimplies
that
(i) (ii)
Si τH(σ (ui)) τH(σ (Riπ−i)) τH(σ (ui)) τ logA .
1 ≤ τ − τ ≤ τ ≤ max
In the above argument, step (i) follows from the non-negativity of H, whereas step (ii) follows from
the fact that the discrete entropy is at most log cardinality of the space.
Proof of the bound (47)(b). We first begin by re-arranging as follows
Si = Riσ (R−iπi)+Riπ−i Riπ−i +ui,σ (ui) σ (Riπ−i)
2 − τ − τ − τ
= (cid:10)Ri π−i σ τ(R−iπi) ,σ τ(ui) σ τ(Riπ−i) + ui Riπ−i (cid:11),σ τ(ui) σ τ(Riπ−i) .
− − − −
Now applyin(cid:10)g th(cid:0)e Cauchy–Schwa(cid:1)rz inequality to each (cid:11)of th(cid:10)e two terms yields (cid:11)
Si Ri π−i σ (R−iπi) σ (ui) σ (Riπ−i) + ui Riπ−i σ (ui) σ (Riπ−i)
2 ≤ − τ 2 τ − τ 2 − 2 τ − τ 2
= (cid:13) R(cid:0)i π−i σ (R−iπi(cid:1))(cid:13) (cid:13) + ui Riπ−i (cid:13)σ (u(cid:13)i) σ (Riπ(cid:13)−i(cid:13)) (cid:13)
(cid:13) − τ (cid:13) (cid:13)2 − 2 (cid:13) τ (cid:13) − τ (cid:13) (cid:13) 2 (cid:13)
≤
n τ1(cid:13) (cid:13) R(cid:0)i π−i −σ τ(R−iπ(cid:1)i)(cid:13) (cid:13) 2+(cid:13) (cid:13) ui −Riπ−(cid:13) (cid:13)io 2(cid:13) (cid:13) ui −Riπ−i 2, (cid:13) (cid:13)
n(cid:13) (cid:0) (cid:1)(cid:13) (cid:13) (cid:13) o (cid:13) (cid:13)
where in th(cid:13)e last step we used th(cid:13)e fac(cid:13)t that σ ()(cid:13)is 1(cid:13)-Lipschitz. (cid:13)Next we apply the inequality
τ · τ
ab
a2c+b c2
with the choices a = Ri π−i σ (R−iπi) + ui Riπ−i , b = ui Riπ−i and
≤ 2 − τ 2 − 2 − 2
an arbitrary c> 0. Doing so yields
(cid:13) (cid:0) (cid:1)(cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
1 2 1
Si c Ri π−i σ (R−iπi) + ui Riπ−i + ui Riπ−i 2
2 ≤ 2τ − τ 2 − 2 c − 2
( )
h(cid:13) (cid:0) (cid:1)(cid:13) (cid:13) (cid:13) i (cid:13) (cid:13)
1 (cid:13) (cid:13) (cid:13) (cid:13) 2 1(cid:13) (cid:13)
2c Ri π−i σ (R−iπi) 2 + ui Riπ−i 2 + ui Riπ−i 2
≤ 2τ − τ 2 − 2 c − 2
( )
h(cid:13) (cid:0) (cid:1)(cid:13) (cid:13) (cid:13) i (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13) (cid:13) (cid:13)
wherethelaststepusestheinequality(x+y)2 2(x2+y2). Introducingtheshorthandwi = ui Riπ−i,
≤ −
we have
c c 1
Si Ri 2 π−i σ (R−iπi) 2 + wi 2+ wi 2
2 ≤ τ||| |||2 − τ 2 τ k2 2τc 2
cA max π(cid:13) (cid:13)−i σ (R−iπi) 2(cid:13) (cid:13)+ c w(cid:13) (cid:13)i 2+ 1 w(cid:13) (cid:13)i 2(cid:13) (cid:13).
≤ τ − τ 2 τk k2 2cτ 2
(cid:13) (cid:13) (cid:13) (cid:13)
Lemma 5(a) yields (cid:13) (cid:13) (cid:13) (cid:13)
2 2
2cA c 1
Si max (π)+ + wi 2 .
2 ≤ τ2 V τ 2cτ 2
i=1 i=1
X X(cid:0) (cid:1)(cid:13) (cid:13)
(cid:13) (cid:13)
Choosing our free parameter as c=
rτ2
for some r (0,1), we find that
2Amax ∈
2 2 2
rτ A 2A
Si r (π)+ + max wi 2 r (π)+ max wi 2 ,
2 ≤ V 2A rτ3 2 ≤ V rτ3 2
max
i=1 i=1 i=1
X X(cid:0) (cid:1)(cid:13) (cid:13) X (cid:13) (cid:13)
(cid:13) (cid:13) (cid:13) (cid:13)
as claimed.
34C.2 Proof of Lemma 8
From the dynamics (8b), we have the elementwise inequality πi πi(1 β ). Iterating this
k+1 (cid:23) k − k
inequality for k = 1,...,K +1 yields
K
1
πi (1 β ).
K+1 (cid:23) A − i
max
i=1
Y
wherewehavemadeuseofthelowerboundπ 1 e,asguaranteed byouruniforminitialization.
1 (cid:23) Amax
Therefore, in order to ensure that πi δ for all k = 1,...,K +1, it suffices to have
k (cid:23)
K
1
(1 β ) δ. (48)
k
A − ≥
max
k=1
Y
Here we make use of the fact that β (0,1) for all k. We now analyze the condition (48) for our
k
∈
two different choices of stepsizes.
Constant stepsizes. For the constant stepsizes β β (0,1), condition (48) is equivalent to
k
≡ ∈
log(A δ)
(1 β)K A δ, or equivalently K max .
max
− ≥ ≤ log(1 β)
−
Inverse polynomial stepsizes. Now consider the inverse polynomial stepsizes β = β for
k (k+k0)η
some exponent η (0,1) and offset k = 2η 1/η . Choose ξ > 1 to be the smallest possible number
∈ 0 β
such that log(1 β ) ξβ . For β < 1, we also have ξβ < 1. Now, to find an elementwise lower
− 1 ≥ − 1 2(cid:0) (cid:1)
bound for π , we write
K+1
K K
(1 β )= exp log(1 β )
i i
− −
i=1 i=1
Y (cid:0)X (cid:1)
K
(i) ξβ
exp
≥ − (i+k )η
0
i=1
(cid:0) X (cid:1)
K
ξβ ξβ
= exp exp
− (1+k )η − (i+k )η
0 0
i=2
(cid:0) (cid:1) (cid:0) X (cid:1)
(ii) ξβ K ξβ
exp exp dx
≥ − (1+k )η − (x+k )η
0 Z1 0
(cid:0) ξβ (cid:1) (cid:0) ξβ (cid:1)
= exp exp (K +k )1−η (1+k )1−η
− (1+k )η − 1 η 0 − 0
0
−
(iii) (cid:0) (cid:1) ξβ(cid:0) (cid:0) (cid:1)(cid:1)
exp 1 exp (K +k )1−η (1+k )1−η ,
0 0
≥ − − 1 η −
−
(cid:0) (cid:1) (cid:0) (cid:0) (cid:1)(cid:1)
where step (i) follows from the inequality log(1 β ) ξβ for k N; step (ii) follows from
k k
− ≥ − ∈
bounding the Riemann sum; and step (iii) follows from that fact that
ξβ
1,
(1+k )η ≤
0
35since ξβ < 1 for a small enough β. Now,
ξβ
exp 1 exp (K +k )1−η (1+k )1−η A δ
0 0 max
− − 1 η − ≥
−
(cid:0) (cid:1) (cid:0) (cid:0) 1 η e (cid:1)(cid:1)
= (K +k )1−η (1+k )1−η − log .
0 0
⇒ − ≤ ξβ A δ
max
(cid:0) (cid:1)
36