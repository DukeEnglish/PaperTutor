The generator gradient estimator is an adjoint state method for
stochastic differential equations
Quentin Badolle1, Ankit Gupta1, Mustafa Khammash1∗
1Department of Biosystems Science and Engineering, ETH Zurich, Basel, Switzerland
∗ mustafa.khammash@bsse.ethz.ch
Abstract
Motivated by the increasing popularity of overparameterized Stochastic Differential Equations (SDEs) like
NeuralSDEs,Wang,BlanchetandGlynnrecentlyintroducedthegenerator gradient estimator,anovelunbiased
stochasticgradientestimatorforSDEswhosecomputationtimeremainsstableinthenumberofparameters[1].
In this note, we demonstrate that this estimator is in fact an adjoint state method, an approach which is
knowntoscalewiththenumberofstatesandnot thenumberofparametersinthecaseofOrdinaryDifferential
Equations (ODEs). In addition, we show that the generator gradient estimator is a close analogue to the exact
IntegralPathAlgorithm(eIPA)estimatorwhichwasintroducedbyGupta,RathinamandKhammashforaclass
of Continuous-Time Markov Chains (CTMCs) known as stochastic chemical reactions networks (CRNs) [2].
GivenaterminaltimeT,weconsiderafamilyofdiffusions{Xx(t,s)∈Rd :s∈[t,T]}indexedbyaninitialcondition
θ
x ∈ Rd at time t and a parameter θ ∈ Θ ⊂ Rn. The dynamics of the process (X θx(t,s)) s∈[t,T] are generated by an
Itoˆ SDE given by:
s s
Xx(t,s)=x+ µ (r,Xx(t,r))dr+ σ (r,Xx(t,r))dB(r), (1)
θ θ θ θ θ
Zt Zt
where µ is the drift term and σ the volatility term. We are interested in estimating the gradient of:
θ θ
T
v (t,x):=E ρ (s,Xx(t,s))ds+g (Xx(t,T)) , (2)
θ θ θ θ θ
" Zt #
where ρ represents the reward rate and and g the terminal reward.
θ θ
1 The generator gradient estimator as an adjoint state method
Asin[1],withoutlossofgenerality,wefocusonthegradientofv attimet=0. Foreaseofexposition,weintroduce
θ
Xx(t):=Xx(0,t). We denote a the diffusion matrix given by:
θ θ θ
1
a (t,x):= σ (t,x)σ (t,x)⊺, (3)
θ θ θ
2
which we use to define the generator L of (Xx(t)) as:
θ θ
L f(t,x):=(∇f(t,x))⊺µ (t,x)+Tr(∇2f(t,x)a (t,x)), (4)
θ θ θ
where f is a twice differentiable function. ∇ corresponds to a space gradient, ∇2 to a space hessian and Tr to the
trace operator. It was rigorously shown in [1] that the derivative of v can be expressed as:
θ
T
∂ v (0,x)=E ∂ L v (t,Xx(t))+∂ ρ (t,Xx(t)) dt+∂ g (Xx(T)) , (5)
θi θ θi θ θ θ θi θ θ θi θ θ
" Z0
(cid:16) (cid:17)
#
where ∂ L has been defined as:
θi θ
1
4202
luJ
92
]CO.htam[
1v69102.7042:viXra∂ L f(t,x):=(∇f(t,x))⊺∂ µ (t,x)+Tr(∇2f(t,x)∂ a (t,x)), (6)
θi θ θi θ θi θ
and ∂ denotes the element-wise partial derivative with respect to the i-th coordinate of θ.
θi
We firstprovideaninformalproofofeq.(5)toestablishthe connectionwiththe adjointstatemethod(see[3, 4]for
a presentationofthis approach). Similarly to [1], we startfromthe Feynman-Kacformulawhichstates that, under
sufficient regularityconditions, v is the solutionof the PartialDifferential Equation(PDE) givenby (see section7
θ
in chapter 5 of [5]):
∂ v +L v +ρ =0, v (T,·)=g , (7)
t θ θ θ θ θ θ
where ∂ denotes a time derivative. Assuming enough smoothness, we formally differentiate the PDE in eq. (7)
t
with respect to θ to obtain:
i
∂ ∂ v +L ∂ v +∂ L v +∂ ρ =0, ∂ v (T,·)=∂ g . (8)
t θi θ θ θi θ θi θ θ θi θ θi θ θi θ
Given two functions f and g defined on Rd and taking values in R, we define the inner product h·,·i as:
hf,gi:= f(x)g(x)dx.
Rd
Z
Introduce a family of probability mass functions {p(t,·) : t ∈ [0,T]} indexed by time, which we leave unspecified
for the moment. Taking the inner product of eq. (8) with p(t,·) and integrating over [0,T] leads to:
T T
∂ ∂ v (t,·),p(t,·) dt+ L ∂ v (t,·),p(t,·) dt
t θi θ θ θi θ
Z0 D E Z0 D E (9)
T T
+ ∂ L v (t,·),p(t,·) dt+ ∂ ρ (t,·),p(t,·) dt=0.
θi θ θ θi θ
Z0
D E
Z0
D E
By integration by parts, the first term on the left-hand side of eq. (9) can be rewritten as:
T T
∂ ∂ v (t,·),p(t,·) dt= ∂ ∂ v (t,x′ )p(t,x′ )dtdx′
t θi θ t θi θ
Z0
D E
ZRd Z0
T T
= ∂ v (t,x′ )p(t,x′ ) dx′ − ∂ v (t,x′ )∂ p(t,x′ )dtdx′
θi θ θi θ t
ZRd (cid:20) (cid:21)0 ZRd Z0
T
= ∂ v (T,x′ )p(T,x′ )−∂ v (0,x′ )p(0,x′ ) dx′ − ∂ v (t,·),∂ p(t,·) dt.
θi θ θi θ θi θ t
ZRd
(cid:16) (cid:17)
Z0
D E (10)
Nowchoosep(t,·)tobep (t,·),theprobabilitymassfunctionof(Xx(t))attimetandwriteδ theKroneckerdelta.
θ θ x
Observe in eq. (10) that:
∂ v (T,·)=∂ g , ∂ v (0,·)p (0,·)=∂ v (0,·)δ .
θi θ θi θ θi θ θ θi θ x
Using the Fokker-Planckequation, we then have:
T T
∂ ∂ v (t,·),p (t,·) dt= ∂ g ,p (T,·) −∂ v (0,x)− ∂ v (t,·),L∗p (t,·) dt
t θi θ θ θi θ θ θi θ θi θ θ θ
Z0
D E D E
Z0
D E
T
= ∂ g ,p (T,·) −∂ v (0,x)− L ∂ v (t,·),p (t,·) dt, (11)
θi θ θ θi θ θ θi θ θ
D E
Z0
D E
where L∗ is the adjoint operator of L . Notice that the third term in eq. (11) is the opposite of the second term in
θ θ
eq. (9). Therefore, plugging eq. (11) in eq. (9), we get:
2T T
∂ v (0,x)= ∂ L v (t,·),p (t,·) dt+ ∂ ρ (t,·),p (t,·) dt+ ∂ g ,p (T,·) , (12)
θi θ θi θ θ θ θi θ θ θi θ θ
Z0
D E
Z0
D E D E
where p (t,·) is the solution of an adjoint state equation which is here the Fokker-Planck equation with initial
θ
condition δ . Eq. (12) is just another way to write eq. (5). This informal derivation draws an explicit connection
x
between eq. (5) used for the generator gradient estimator and the well-known adjoint state method.
The generator gradient estimator based on eq. (5) relies on estimates of ∇v and ∇2v obtained from auxiliary
θ θ
pathwise differentiation estimators. Remarkably, the number of such auxiliary estimators scales with the number
of states d and not the number of parameters n, exactly as with the adjoint state method in the case of ODEs.
2 The generator gradient estimator as an analogue to the exact Inte-
gral Path Algorithm (eIPA) estimator
Observethatthederivationofeq.(5)doesnotrelyontheexplicitexpressionforthegeneratorL . Infact,asimilar
θ
expression was obtained in the context of a class of CTMCs known as CRNs [2] (see [6, 7, 8] for an introduction
to these models). To make the parallel explicit, we need to introduce some notations. Let us consider a network
with d molecular species. The state of the system at any time can be described by a vector in Nd whose i-th
component corresponds to the number of molecules of the i-th species. The chemical species interact through m
chemical reactions and every time the k-th reaction fires, the state of the system is displaced by the d-dimensional
stoichiometric vector ζ k ∈ Zd. The time-homogeneous propensity function λ θ = (λ θ,k) k∈[[1,m]] parameterised by a
parameter θ ∈Θ ⊂Rn depends on the state of the system x∈Nd. Given a terminal time T, we consider a family
of Markov jump processes {Xx(t):t∈[0,T]} indexed by an initial condition x at time t=0 and the parameter θ.
θ
The generator of (Xx(t)) is given by:
θ
m
L f(x):= λ (x)∆ f(x), (13)
θ θ,k ζk
k=1
X
where, given a vector z ∈ Rd, ∆ is the spatial finite difference operator: ∆ f(x) := f(x+z)−f(x). Given a
z z
collection of independent, unit-rate Poisson processes {(Y k(t))} k∈[[1,m]], we associate to each reaction k a counting
process (R (t)) defined as:
θ,k
t
R (t):=Y λ (Xx(s))ds . (14)
θ,k k θ,k θ
(cid:18)Z0 (cid:19)
The random time change representation of (Xx(t)) is given by [7, 9]:
θ
m
Xx(t)=x+ ζ R (t). (15)
θ k θ,k
k=1
X
We redefine v as:
θ
v (t,x):=E g(Xx(T −t)) , (16)
θ θ
h i
where g corresponds to a terminal reward. Theorem 3.3 in [2] gives an expression for ∂ v which is rigorously
θi θ
derived. Using the time-homogeneity of (Xx(t)) to rewrite the formula given there, we get:
θ
m T
∂ v (0,x)= E ∆ v (t,Xx(t))∂ λ (Xx(t))dt . (17)
θi θ ζk θ θ θi θ,k θ
k=1 " Z0 #
X
This expression can straightforwardly be extended to include some dependence of the terminal reward g on θ,
a reward rate ρ and time-dependent propensities. To make the comparison between eq. (17) and eq. (5)-(6)
θ
3transparent,letus rewriteeq.(5) fora time-homogeneousSDE,inthe absenceofrewardrateandin the casewhen
the terminal reward is independent of θ:
T
∂ v (0,x)=E (∇v (t,Xx(t)))⊺∂ µ (Xx(t))+Tr(∇2v (t,Xx(t))∂ a (Xx(t)) dt . (18)
θi θ θ θ θi θ θ θ θ θi θ θ
" Z0
(cid:16) (cid:17)
#
In particular,the spatial finite difference terms ∆ v in eq. (17) are directanalogues to ∇v and∇2v in eq. (18),
ζk θ θ θ
and the parametric partial derivatives ∂ λ are the equivalent of ∂ µ and ∂ a .
θi θ,k θi θ θi θ
Estimationof∂ v basedoneq.(17)aspartoftheexactIntegralPathAlgorithm(eIPA)introducedin[2]shareskey
θi θ
ideas with the generator gradient estimator. First, eIPA replaces the unknown quantities v by so called auxiliary
θ
processeswhichshouldbecomparedtotheestimatesof∂ v obtainedfrompathwisedifferentiationestimators(see
θi θ
thediscussionaroundeq.(2.6)in[1]andsubsection3.4in[2]). Secondly,tocontrolthecomputationtimepersample,
the auxiliaryprocessesofeIPAaregeneratedatagivenjump time forreactionk onlyifacertainBernoullirandom
variable equals 1 (again, see subsection 3.4 in [2]). This is similar in spirit to the integral randomisation strategy
usedbythe generatorgradientestimator(seetheparagraphrelatedtoeq.(2.9)in[1]). Strikingly,itwasillustrated
in [2] that the eIPA estimator exhibits low variancewhen comparedto other unbiasedestimators,as is the case for
the generator gradient estimator (see the numerical examples in section 4 of [1] and [2]).
References
1. WangS,BlanchetJ,andGlynnP.AnEfficientHigh-dimensionalGradientEstimatorforStochasticDifferential
Equations. arXiv preprint arXiv:2407.10065v1 2024
2. GuptaA,RathinamM,andKhammashM.Estimationofparametersensitivitiesforstochasticreactionnetworks
using tau-leap simulations. SIAM Journal on Numerical Analysis 2018;56:1134–67
3. C´eaJ. Conceptionoptimale ou identificationde formes,calculrapide de la d´eriv´eedirectionnelle de la fonction
couˆt. ESAIM: Mod´elisation math´ematique et analyse num´erique 1986; 20:371–402
4. Plessix RE. A review of the adjoint-state method for computing the gradient of a functional with geophysical
applications. Geophysical Journal International 2006; 167:495–503
5. Karatzas I and Shreve S. Brownian motion and stochastic calculus. Vol. 113. springer, 2014
6. E´rdi P and T´oth J. Mathematical models of chemical reactions: theory and applications of deterministic and
stochastic models. Manchester University Press, 1989
7. Anderson DF and Kurtz TG. Stochastic analysis of biochemical systems. Vol. 674. Springer International
Publishing, 2015
8. Wilkinson DJ. Stochastic modelling for systems biology. CRC press, 2018
9. Kurtz TG. Representations of Markov processes as multiparameter time changes. The Annals of Probability
1980 :682–715
4