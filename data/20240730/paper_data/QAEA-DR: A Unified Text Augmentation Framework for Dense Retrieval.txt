1
QAEA-DR: A Unified Text Augmentation
Framework for Dense Retrieval
Hongming Tan1,2, Shaoxiong Zhan1, Hai Lin1,2, Hai-Tao Zheng1,2,∗ and Wai Kin (Victor) Chan1,2,∗
1Shenzhen International Graduate School, Tsinghua University, Shenzhen, China
2Pengcheng Laboratory, Shenzhen, China
Abstract—In dense retrieval, embedding long texts into dense the original texts. As a result, current data augmentation
vectors can result in information loss, leading to inaccurate methods have not resolved inherent deficiencies in dense
query-text matching. Additionally, low-quality texts with exces-
retrieval, specifically the loss of key information exacerbated
sive noise or sparse key information are unlikely to align well
by the presence of low-quality text. To address this issue, it
with relevant queries. Recent studies mainly focus on improving
the sentence embedding model or retrieval process. In this is essential to consider data augmentation specifically applied
work, we introduce a novel text augmentation framework for to the retrieval text itself. Intuitively, we can enhance the
dense retrieval. This framework transforms raw documents into original text by implementing text augmentation methods to
information-dense text formats, which supplement the original
generate high-quality alternative texts, which concentrate key
texts to effectively address the aforementioned issues without information to improve semantic similarity with the query.
modifying embedding or retrieval methodologies. Two text rep-
resentations are generated via large language models (LLMs) Taking inspiration from existing challenges and unexplored
zero-shot prompting: question-answer pairs and element-driven optimization strategies, we consider transforming raw texts
events. We term this approach QAEA-DR: unifying question- intomoreinformation-denseformats[14]thatpresentessential
answer generation and event extraction in a text augmentation
factualdetailsconciselyanddirectlyforbetterdenseretrieval.
framework for dense retrieval. To further enhance the quality
Specifically, we propose that dense retrieval can be improved
of generated texts, a scoring-based evaluation and regeneration
mechanism is introduced in LLM prompting. Our QAEA-DR through information extraction to generate new text embed-
model has a positive impact on dense retrieval, supported by dings, a text augmentation strategy that outperforms reliance
both theoretical analysis and empirical experiments. onoriginaltextalone.Thesegeneratedtextembeddingvectors
Index Terms—Dense retrieval, text augmentation, information achievehighfidelitybycondensinginformationandremoving
extraction, large language model, vector database. noise, and they show higher similarity with the query vector
than the original text vector. To implement this idea, we need
to address three issues. (i) The first issue is What information
I. INTRODUCTION
extraction tools can effectively resolve the inherent challenges
DEnse retrieval [1], [2] is a information retrieval method
ofkeyinformationlossandlow-qualitytextindenseretrieval?
that uses text embeddings to find the relevant texts for a
To address this issue, we focus on two high-level information
givenquery.Indenseretrieval,sentenceembeddingstransform
extraction methods: question-answer generation (QAG) and
sentencesintosemanticvectorrepresentations,improvingpas-
event extraction (EE).
sage retrieval performance over word embeddings.
Inspired by the longstanding tradition of Question An-
A major challenge in dense retrieval is the risk of losing
swering Systems (QAS) [15], QA pairs should be the ideal
essential information when converting long texts into fixed-
text format for dense retrieval due to their high accuracy in
length dense vectors, as maintaining the fidelity of sparse
providing precise responses to users’ similar questions. QA
representations for long texts often requires very high di-
pairs are information-dense as they focus on specific points
mensions [3]. Additionally, this limitation is emphasized in
from raw texts, presenting significant factual details directly
cases where the source texts are inundated with low-quality,
and succinctly. This QA format aligns well with the query
noisy text, resulting in inconsistent retrieval quality. On one
typically centered on a single topic, minimizing redundant
hand, recent works propose advanced retrievers or sentence
information and offering a streamlined retrieval process for
embedding models to improve dense retrieval [4]–[9]. On the
targeted inquiry [16]. Additionally, studies indicate that QA
other hand, input enhancement for retrieval represents a dis-
pairsanddocumentscancomplementeachotherasknowledge
tinct optimization strategy for retrieval tasks, including query
bases [17], suggesting the incorporation of QA pairs into
transformation and data augmentation [10]. Unlike query
vector databases for enhanced dense retrieval.
transformation [11], [12], data augmentation improves data
Additionally,weshouldconsidereventextractionasanother
qualitybeforeretrieval,enhancingperformancewithoutadding
crucial information extraction method based on knowledge
user wait time. However, in text retrieval, data augmentation
graphs. Event extraction is a particularly information-dense
methods typically focus on generating new query-text pairs
form that extracts structured information from unstructured
for the retriever training [13], rather than directly enhancing
text to answer the “5W1H” questions (who, when, where,
what, why, how) [18]. It captures both entities and relation-
*Correspondingauthor:Hai-TaoZhengandWaiKin(Victor)Chan.(Email:
zheng.haitao@sz.tsinghua.edu.cn;chanw@sz.tsinghua.edu.cn) ships, aiming to extract high-level information from text and
4202
luJ
92
]LC.sc[
1v70202.7042:viXra2
present it in a structured, information-dense format. Conse- should convert the structured text, previously output in JSON
quently, events correspond to potential user “5W1H” queries format by a large language model, back into unstructured
and involve reorganizing and rewriting the original text to naturallanguagesuitableforsentenceembedding.Weemploy
ensurepreciseinformationdelivery,thusaligningsemantically a straightforward conversion strategy: for QA pairs, we con-
with these queries. catenate the question and answer to create one single text;
Furthermore, we observe that QA pairs and events possess for events, we sequentially combine all elements of the same
both subtle connections and clear distinctions. Event-based event into one text. By converting back to unstructured text
knowledge representations share similarities with QA pairs: in this straightforward manner, we also explore different text
(1) They both capture high-level semantic information at the organization strategies in our experiments. As a result, both
sentence and paragraph levels rather than focusing solely on the original and newly generated text chunks are embedded
keywordsandentities,providingdeeperinsightsthankeyword and incorporated into the final vector database. Importantly,
extraction or named entity recognition. (2) Each QA pair we anticipate that the generated vectors will exhibit a higher
typically corresponds to an element in event representations, similarity to the input query vectors than the original text
aseventscananswer“5W1H”questions.Forexample,“when” vectors, thereby improving retrieval performance.
aligns with event time, “where” with location, and “who” In this paper, based on the above discussion, we introduce
or “what” with subjects or objects. Meanwhile, QA pairs QAEA-DR, a framework that integrates Question-Answer
and events differ fundamentally in structure. (1) QA pairs Generation (QAG) and Event Extraction (EE) into a Text
match individual information points and align with query Augmentation Framework for Dense Retrieval. QAEA-DR
semantics but each represents only a small portion of the employs two types of generated text representations through
sourcetext,potentiallylimitingtheirabilitytohandlecomplex LLM prompting: QA pairs and element-driven events. To
queries. (2) Events synthesize entities and relationships and further enhance the quality and robustness of text generation,
incorporate various elements to potentially offer deeper and we conduct scoring and text regeneration as the verifica-
richersemanticsthanQApairs.However,thelackoffocuson tion component in QAEA-DR. After generation, both QA
a single information point in events reduces their alignment pairs and events are converted back into unstructured texts.
with queries. Therefore, we incorporate both QAG and EE Subsequently, these generated texts are organized using two
intothetextaugmentationframeworkfortheircomplementary distincttextorganizationstrategiesandtransformedintodense
benefits. vectors.Atlast,thesegeneratedvectorsareaddedtothevector
(ii) The second issue is What text generation model should database as high-quality retrieval vectors. Our experiments
be used? Our desired text generation model aims to: (1) demonstrate that incorporating both event vectors and QA
effectively produce multiple QA pairs and events from any pair vectors into the vector database maximizes retrieval
given raw text, with the quantity of generated outputs corre- performance. In summary, the contributions of this paper are
sponding to the text’s information content; (2) ideally manage as follows:
all generation tasks within a unified model framework. In • Tothebestofourknowledge,QAEA-DRisthefirstcom-
light of these requirements, we opt for large-scale pre-trained prehensive and universal text augmentation framework
languagemodels(LLMs,e.g.,ChatGPT1)astextaugmentation designed for dense retrieval.
generators. Previous works in QAG and EE not only lack • QAEA-DRinnovativelyintegratestheinformationextrac-
multilingual capabilities but also exhibit limited open-domain tion methods of QAG and EE into a unified framework
generalization, which distances them further from the ideal of of text generation and organization.
aunifiedmodelframework.Unlikepreviousmodels,LLMsex- • QAEA-DR employs an end-to-end LLM-based training-
cel in text comprehension and generalization, enabling strong freetextgenerator,integratingdiversepromptsofgenera-
semantic understanding and information extraction capabili- tion and scoring-based output evaluation for high-quality
ties. Despite the distinct nature of QAG and EE, LLMs could and controllable text outputs.
integrate these tasks into a unified framework that employs • QAEA-DR is evaluated through theoretical analysis and
zero-shotpromptingandsupportsmultilingualdata.Wedesign empirical validations on various embedding models and
prompt instructions for QAG and EE to generate JSON- retrieval datasets to demonstrate its effectiveness and
formatted QA pairs and events. robustness.
Moreover, to ensure the output quality in unsupervised,
II. RELATEDWORK
training-free LLM generator, we introduce a penalty point
systemthatdeductspointsbasedonspecifiedcriteriaafterthe In this section, we first review dense retrieval along with
firstgeneration.Ifscoresfallbelowapredeterminedthreshold, sentenceembedding.Next,wediscusspreviousinputenhance-
we regenerate the text based on the deducted points to ensure mentmethodsforretrieval.Finally,weintroducesomerelated
enhanced text quality. works on information extraction.
(iii) The last issue to address is How can the generated
A. Dense Retrieval
structured text be utilized for dense retrieval? As a text aug-
mentationmethod,ourgoalistoseamlesslyaddthegenerated Dense retrieval has become an important research area
structured texts into the datastore for retrieval. Initially, we following the development of pre-trained Transformer lan-
guage models (PLMs) [2], [19]–[22]. To enhance text re-
1https://chat.openai.com trieval performance, dense retrieval leverages PLM-based text3
embeddings to encode queries and documents into a shared • EE has evolved from rule-based approaches [35] to
semanticvectorspace,focusingonmatchingsemanticcontents machine learning methods like Dynamic Multi-Pooling
beyond mere keywords. This text embedding application in ConvolutionalNeuralNetworks(DMCNN)[36]andJoint
retrieval is fundamental to Retrieval-Augmented Generation EventExtractionviaRecurrentNeuralNetworks(JRNN)
(RAG) [23], [24], which reduces the hallucinations in LLMs. [37],andmorerecentlytoChatGPTforEventExtraction
Recent advancements in dense retrieval include architectural (ChatEE) [38], and the Generative Template-based Event
innovations, optimized training methodologies, and efficient Extraction(GREE)[39],reflectingsignificantprogressin
indexing techniques, all of which contribute to improved the field.
retrievalaccuracyandefficiency[4]–[8],[25].Sincetheintro- • There are methods that achieve multi-event extraction,
duction of Sentence-BERT [26] and Dense Passage Retrieval such as Jointly Multiple Event Extraction (JMEE) [40]
(DPR) [2], numerous sentence embedding models have been and Proxy Nodes Clustering Network (ProCNet) [41].
proposed to enhance dense passage retrieval. Advanced sen- Nevertheless, current multi-event extraction methods are
tence embedding models, which have been highlighted in the closed-domain and are limited by their reliance on pre-
retrieval task of massive text embedding benchmark (MTEB) defined event schemas.
[27], include Contriever [28], M3E2, BGE [9], etc. Our text On the other hand, Question-Answer Generation (QAG), an
augmentation method serves as a preprocessing module for extension of Question Generation (QG) [42], [43], generates
dense retrieval and is compatible with various embedding several QA pairs given a text. Notably, QAG can also be
models mentioned above. classified as IE since QA pairs are structured texts.
• QAG have progressed from rule-based models [44]–[46]
B. Input Enhancement in Retrieval to generative-based PLMs like Information-Maximizing
Hierarchical Conditional VAEs (Info-HCVAE) [47] and
In addition to the optimization methods for the retriever,
Language Models for Question and Answer Generation
input enhancement strategy represents a distinct optimization
(LMQG) [48].
approach for retrieval tasks [10]. In particular, input data
to a retriever includes user query and datastore. Therefore, However, current multi-event extraction and QAG face
input enhancement for retrieval can be categorized into two issues such as a lack of multilingual support, uncontrollable
types: query transformation and data augmentation. Query generation quantity and quality, and incompatibility within
transformation modifies the input query during retrieval, for a unified model framework. Although these models show
example, Hypothetical Document Embeddings (HyDE) [12] relatively good results on some datasets, they are far from our
that generate pseudo documents from queries, and KNN- goalofageneralizable,quality-controllable,andunsupervised
basedQueryExpansion(KNN-QE)[11]thatenhancesqueries unified framework in open-domain applications. QAEA-DR
using local conceptual word embeddings. Data augmentation combines EE and QAG, leveraging LLM to build an end-to-
improves the data to be retrieved before the retrieval process, end framework that involves prompt-based generation, evalu-
including synthesizing data, clarifying ambiguities, updating ation, and regeneration.
outdated data, etc. Compared to query transformation, data
augmentation models have the advantage of not consuming III. APPROACH
user waiting time in retrieval, which is particularly important A. Notations and Problem Definition
in practical applications. Mainstream studies focus on data
In this paper, we focus on text augmentation approach for
augmentation for cross-modal retrieval, such as Make-An-
densepassageretrieval.Aretrievaldatasettypicallycomprises
Audio[29],AMSH[30],andReACC[31].Intermsoftext-to-
three types of data: the corpus, queries, and labeled query-
text retrieval, methods like InPars [13] generates new query-
text relationships. Initially, let C = {t ,t ,...,t } represent
1 2 n
text pairs as training data. As current data augmentation
a corpus, where each t is a text chunk (simplified as text
i
methods do not consider enhancing the original text in text
in the following discussion) and n is the total number of
retrieval, we propose a text augmentation framework in this
texts in corpus. The initial step in dense passage retrieval
paper. is to construct a mapping function Φ : C → Rd, where d
is the vector dimension, such that semantically similar texts
C. Information Extraction are close in the vector space. Specifically, the function Φ
uses sentence embedding model to transform all texts into
Information extraction (IE) automatically isolates text frag-
dense vectors (i.e. embedding) stored in a vector database.
ments and extracts structured data from unstructured sources
We denote the resulting vector database as VDB , where
through NLP [32]. On one hand, IE is integral to constructing ori
VDB = {v ,v ,...,v } with each vector v ∈ VDB
knowledge graphs (KGs), which have attracted considerable ori 1 2 n i ori
corresponding to a text t in C. Given a query text q, which
attentionasastructuredformofknowledge[33].Tasksrelated i
is also mapped to a vector v ∈ Rd by Φ, the retriever
to KG-based IE include named entity recognition, relation q
calculates the top-k vectors v ∈ VDB with the highest
extraction, and event extraction [34]. In particular, event ex- i ori
similaritytoqueryvectorv ,resultinginasubsetS ⊆VDB ,
traction(EE)capturesbothentitiesandrelationshipstoextract q ori
where |S| = k. The vector similarity, denoted as sim(v ,v ),
high-level structured information from raw text. q i
measures the distance between v and each vector v in
q i
2https://huggingface.co/moka-ai/m3e-base VDB
ori
(e.g., calculating the cosine similarity based on the4
Step-1: High-level Structured Information Extraction Step-2: Reversion to Unstructured Form
LLM-based Text Generation Texts Remain Independent (TRI)
QAG QA Pairs Q: In which state is Ackley located? What is the population of Ackley, Population Change. 2010. Ackley, Iowa.
Regeneration LLM A: Iowa. Iowa? 1,589 during the 2010 census. A pock pl ue ly a. t P ioo np u ol fa Ati co kn le o yf w1, a5 s8 r9 e. c T oh rde ed as
Prompting Q A:: 1W ,5h 8a 9t dis u t rh ine g p to hp eu 2la 0t 1io 0n c o enf sA uc sk .ley, Iowa? Q A:: FW rah na kt l ic no au nn dti e Hs a i rs d A inc .kley located in? In which state is Ackley located? Iowa. 1,589 during the 2010 census.
City Establishment. Ackley, Iowa.
What counties is Ackley located in? Ackley. Located in Franklin and Hardin
E Pv ra S ol cu moa r pt ii n to ig nn g& LLI Mnd ae sp Ee vn ad lue an tt o r D Ine fm oro mgr aa tp ioh nic InL foo rc mat aio tin o n Franklin and Hardin. C U Ho a.Su r. dn s it t ni ae Cts e. o A uo nc f k tIo il ee w sy . ais , wa ic ti hty in l o Fc raa nte kd li nin a t nh de
Events
Texts Merge intoOne (TMO)
Event
Extraction Event Type: Population Count Event Type: City Establishment What is the population of Ackley, Iowa? Population Change. 2010. Ackley, Iowa.
G Pe ron mer pa tt ii no gn LLM T Suim bje e:
c
2 t0
:
1 A0
c k l e
L yo c a Oti bo jn e:
c
A t:c Pk ole py u, lI ao tw ioa
n of 1,589
T Suim bje e:
c
N t:o At cs kp le ec yi f i e d Location: Ackley, Iowa 1 s ct o, a5 ut8 ne9
t
i is ed su A r ic si kn Alg e
c
yt
k
h ll eoe yc 2 a l0 ote1 cd a0 ?
t
c
e
Ie don w is nu a ?s . . FW rIn ah nw a kth
l
ii nc h A p 1,oc 5k p 8l ue 9ly a d. t P uioo rn ip
n
u gol fa
t
A hti eco k n 2le 0o y 1f
0
w1 , ca5 es8
n
r9 se. uc T so .h r Cde
i
e td
y
as
Frozen E wv ae sn rt e D coe rs dc eri dp t ai so n 1: , 5T 8h 9e dp uo rp inu gla tt hio en 2 o 0f 1 A 0c ck ele ny s us. O Evb ej ne tc t D: eLo scc ra ipte td io i nn : F Ar ca kn lk el yin is a an d ci tH ya lr od ci an t eC do u inn t ti he es U.S. and Hardin. E L Cos ot c ua a nb t tel ii edsh s i .m n A e F crn kat len. yA k lc ii snk l ae a y n c, id tI y o H w la oa r cd. a iA tn ec dk l ie ny . t he
state of Iowa, within Franklin and Hardin Counties. U Ha.S r. d s it na Cte o uo nf tIo iew s.a, within Franklin and
Step-4: Enhanced Dense Retrieval Step-3: Integration into Vector Database TRI or TMO
Ackley, Iowa. Ackley is a city in Augmented VDB Generated Texts
Franklin and Hardin Counties in
A C oTe rpx ut s( 𝒕 () 𝑪 i )n t ph oe p uU la. tS i. o ns ta wt ae s o 1f , 58Io 9w a a. t T th he e Dense Vector Query Original Text Vectors Dense Vector
2010 census. Model Vector Corpus (𝑪) Model
Generated Text Vectors
Relevance Original Text Generated Text
Vectors Vectors
Query (𝒒) What county is Vectorized
ackley iowa in? Top-k vectors Index
gene tr ea xt te d ve o cr t oo rr siginal Similar Original Text Vector All Original Text Vectors
Input Data Search
Original Text Vector +
Original 𝐓𝐞𝐱𝐭 𝑺𝒊𝒎(𝒗𝒕,𝒗𝒒)=𝟎.𝟕𝟑 Best Match Text 𝑺𝒊𝒎𝒗𝒒𝒂,𝒗𝒒 = 𝟎.𝟖𝟑 Origin…al T …ext Vector Generated Text Vectors
Ackley, Iowa. Ackley is a city in What counties is Ackley located in?
Franklin and Hardin Counties … Franklin and Hardin. Original VDB Augmented VDB
Fig. 1. QAEA-DR example. The dashed arrows represent the text augmentation path of QAEA. In Step-1: High-level Structured Information Extraction
through LLM-based text generators with frozen parameters, the generated QA pairs and events preserve similar key information in different formats. This
is followed by Step-2: Reversion to Unstructured Form and Step-3: Integration into the Vector Database. In Step-4: Enhanced Dense Retrieval, the results
demonstratethatthereexistsageneratedvectorwithhigherqueryrelevancecomparedtotheoriginaltextvector.Thisisbecausethekeyinformationdensity
ofthegeneratedtextvectorisenhancedthroughinformationextraction,makingitsemanticallyclosertothequery.
inner product ⟨v ,v ⟩). Evaluation metrics (e.g., NDCG) are {event(1),event(2),...,event(m)}, where l and m rep-
q i i i i
used to calculate retrieval scores based on labeled query-text resent the total number of QA pair texts and event texts
relationships. Here, we define our QAEA-DR as follows: generated from t , respectively. Subsequently, we mainly
i
consider two text organization strategies:
Definition III.1 (QAEA-DR). QAEA-DR is a text augmen-
tation framework that augments the original corpus C by – Texts Remain Independent (TRI): Maintain the gen-
generating QA pairs and element-driven events using LLM- erated set as QA = {qa(1),qa(2),...,qa(l)} and
i i i i
based generators. This process enriches the vector database EVENT = {event(1),event(2),...,event(m)}, for
i i i i
VDB ori byaddingnewvectorrepresentationsderivedfromthe i=1,...,n.
augmented texts. The similarity of the query to generated text – Texts Merge into One (TMO): In this mode, indi-
vectors should exceed that of original text vectors, potentially vidual texts generated from the same original text
enhancing retrieval quality. t are concatenated, forming singleton set QA =
i i
{qa : qa(1) +qa(2) +...+qa(l)} and EVENT =
i i i i i
B. Overview of QAEA-DR {event i :event( i1) +event( i2) +...+event( im)},for
i=1,...,n. The “+” denotes text concatenation.
Fig. 1 shows the complete workflow of QAEA-DR, illus-
trating an example of the framework in action. Specifically, Overall, TRI decomposes texts, retaining all segments
QAEA-DR operates as follows: extracted from the original, but may include noisy texts
• Step-1: Structured Information Extraction. Each text t i unrelated to all queries. Conversely, TMO consolidates
from the corpus C, where i = 1,...,n, is augmented generated texts to reduce the density of noise.
usingLLMpromptingtogenerateJSONformatQApairs • Step-3: Integration into Vector Database. The trans-
QA and events EVENT . We discuss the design of formedtextsaremappedtovectorsbythefunctionΦ.For
json json
LLMpromptsforstructuredtextaugmentationinSection QA texts, V QA i =Φ(QA i)={v q(j a) i |j =1,...,l} (TRI)
I teII x- tC s. eA ffs eci tl il vu es lt yrate ed xtri an ctFi kg e. y1, inb fo ot rh mt ay tip oe ns .o Sf ps et cru ific ctu ar lle yd , o tor r{ dv aq ta ai b} as( eTM VDO B), Qw Ah =ere (cid:83)i n i== 11 V, Q. A. i. ., Sn i, mr ie ls au rll yts , i Vn EVth Ee NTv ie =c-
each QA pair presents an individual information point, Φ(EVENT ) = {v(j) | k = 1,...,m} (TRI) or
while each event summarizes multiple points. {v
}(i TMO),poe pv uen lati
tesVDB
=(cid:83)n
V .
eventi EVENT i=1 EVENTi
• Step-2: Reversion to Unstructured Form. The gener- These generated vectors are then integrated into the
ated structured texts are converted back into unstruc- final vector database VDB = VDB + VDB +
final ori QA
tured natural language texts, resulting in a set of QA VDB to augment the original vector space.
EVENT
texts {qa( i1),qa( i2),...,qa( il)} and a set of event texts • Step-4: Enhanced Dense Retrieval. The query vector v q5
searches for the top-k similarity vectors in VDB final. For Prompt Template for QA pair / Event Generation
anygivenqueryqassociatedwithapositivelyrelatedtext I thn es t gr eu nc et ri ao tn io: nY oo fu mar ue l ta ipn l ee x Qpe Ar t P i An IN Ra St /u Er Val E L Na Tng Su ia ng te h P e r fo oc re ms s oin f g J S(N OL NP ;). Given a piece of text, your task is to complete
- Based on the given text, construct questions and answers from these 5 types of questions: [Factual
t , there exists a vector in either VDB or VDB Inquiry],[Explanation and Definition],[Cause and Effect],[Comparison and Contrast],[Evaluation and Opinion]
i QA EVENT - Each event include the following 7 elements : [{{“event type”, “time”, “location”, “event subject”, “event object”,
that exhibits higher similarity with the query vector v q “ - e {v Le in stt ” m, o“ ri em cp oa nc st” tr} a} in] ts here …}
Input Data:
than the original text vector v . - Original Text: {text}
i Output Indicator:
NLP Expert: The generated QA PAIRS/EVENTS based on the original text is as follows (in JSON format):
In conclusion, our main contribution is the implementation
Prompt Template for QA pair / Event Scoring-based Quality Evaluation
ofQAEA-DR,which,inStep-1,Step-2,andStep-3,generates Instruction: You are an expert in Natural Language Processing (NLP). Given a piece of original text and a series of QA
PAIRS/EVENTS generated from the text, your task is to evaluate these QA PAIRS/ EVENTS quality and provide
improvement suggestions in the form of JSON:
two new types of text vectors—QA pair vectors and event - {“total score”, “detail”: [{“deduction reason”, “deduction score”, “related content”} ]}
- {List scoring rules here (e.g., Relevance Rule: deduct 1 point for generated text irrelevance to the original content)…}
vectors—and integrates them into the vector database. These Input Data:
- Original Text: {text}
- Generated QA PAIRS/EVENTS
generated vectors enhance the retrieval performance in the Output Indicator:
NLP Expert: The scoring results is as follows (in JSON format):
finalStep-4ofdenseretrieval.Fig.1demonstratesthatinStep-
Prompt Template for QA pair / Event Regeneration
4,thebestmatchwiththequeryisderivedfromthegenerated Instruction: You are an expert in Natural Language Processing (NLP). Based on the scoring results, your task is to
regenerate QA PAIRS/EVENTS in the form of JSON according to the predetermined tolerance threshold and
improvement suggestions.
vectors with high similarity. - {List detailed improvement rules here …}
Input Data:
In the following sections, we will first discuss text aug- - Original Text: {text}
- Generated QA PAIRS/EVENTS
- Tolerance Threshold 𝜏
mentation details in Step 1 to 3. Then, we substantiate the - Scoring Results
Output Indicator:
effectiveness of QAEA-DR theoretically in the subsequent NLP Expert: The corrected QA PAIRS/EVENTS based on the original text are as follows (in JSON format):
section, addressing why generated vectors in Step-4 could
Fig.2. PrompttemplatesforQApair/eventgeneration,scoring-basedquality
exhibithighersimilaritywiththequeryvectorthantheoriginal
evaluationandregeneration,respectively.
text vector.
As illustrated in Step-2 of Fig. 1, the output of QAG is pro-
C. LLM-based Text Augmentation in QAEA
cessed by simply concatenating the “question” and “answer”
In this section, we describe our implementation of LLM- strings into natural language texts.
basedtextaugmentationandtheunifyingpropertiesofQAEA. Event Extraction. Since our text augmentation method for
QAEAisdefinedasatextaugmentationmoduleexcludingthe dense retrieval is initially designed for application on a small-
retrieval component. It combines original texts, QA pairs, and scale Chinese news passage retrieval dataset (sCNPR) we
events into a new vector database to enhance natural texts created from a scientific project, we naturally consider event
throughinformationextraction.QAEAcorrespondstoSteps1 extraction. Unlike previous zero-shot prompt-based ChatEE
to 3 in Fig. 1. [38], which requires predefined event types and supports
Following standard LLM-based prompt engineering prac- single-eventextraction,ourapproachallowstheLLMtodetect
tices[49],ourdefinedsingle-stepzero-shotpromptconsistsof andgeneratemultipleeventtypesfromtheoriginaltext.Inthe
threecomponents:instruction,inputdata,andoutputindicator. EE prompt instructions, we first direct the LLM to identify
For both QAG and EE prompting tasks, we make targeted multiple event types and use these generated types as triggers
adjustments to the prompt instructions. Fig. 2 illustrates the to populate event elements. Drawing from the event element
three-step prompts defined for both QAG and EE, including categorization in the ACE 2005 dataset and common real-
generation,scoring-basedqualityevaluation,andregeneration. world event attributes, we define that each event includes
We achieve different functionalities by modifying the instruc- the elements “event type,” “time,” “location,” “event subject,”
tions for each type of prompt. “event object,” “event,” and “impact.” In terms of the output
Question-Answer Generation. In QAG, our goal is to gen- format, we guide the LLM to generate event outputs in JSON
erate as many informative structured QA pairs as possible formatEVENT json:[{“eventtype”,“time”,“location”,“event
through instruction. Due to the lack of a universally rec- subject”, “event object”, “event”, “impact”}], where outputs
ognized question generation directive, we employ question defaulttonullifcorrespondingeventelementsareabsentinthe
categorization to guide the LLM in producing diverse QA originaltext.Fig.1showsthattheoutputofEEistransformed
pairs. In designing question types, we observe that rhetorical backintounstructuredtextbyconcatenatingalleventelements
patterns in writing (e.g., cause and effect) serve as methods within an event, separated by periods.
fororganizinginformationandcanbegeneralizedforquestion Text Evaluation and Regeneration. In the open domain,
categorization. Consequently, the content directive specifies evaluating the quality of QAG and EE is difficult due to the
five question types for varied outputs: factual inquiry, ex- lack of labels. To address this, we introduce a robust prompt-
planation and definition, cause and effect, comparison and based mechanism for evaluating the quality of generated
contrast,andevaluationandopinion.Additionally,theprompt texts and regenerating them if necessary, as outlined in Fig.
instructs LLM to highlight frequently occurring entities and 2 and Algorithm 1. Using separate roles for generation and
relationships in the original text, which should be reflected evaluation by different LLMs, LLM and LLM ,
generator evaluator
in the generated QA pairs. Regarding the output format, the the double-check system scores and potentially rewrites
instructions guide the LLM to produce QA pairs in JSON outputsbasedonpredefinedcriteria.Asshowninthescoring-
format QA : {“question type”: [[“question”, “answer”]]}, based quality evaluation prompt template in Fig. 2, each
json
where“questiontype”includesfivecategories,eachcapableof generated text starts with a perfect score of 10, and points are
containing multiple QA pairs depending on LLM generation. deducted for failures in Relevance, Clarity, Consistency, and6
Algorithm 1 QAEA can be viewed as a special case of TRI, and we will further
Input: A corpus C = {t ,t ,...,t }, large language model discuss their differences in the experimental analysis. Given a
1 2 n
LLMs, embedding model Φ, score threshold τ text t ∈ C, we generate a text set {t(j)} including QA pair
i i
Output: Augmented vector database VDB final textsandeventtexts,wherej recordsthetotalnumberoffinal
1: Initialization: VDB final,VDB QA,VDB EVENT ←∅ generatedtexts.Intermsofvectorrepresentation,similarly,we
2: VDB ori ←Φ(C) combine the QA pair vectors V QA
i
and event vectors V EVENTi
3 4: : for foe rac eh act hi ∈ typC ed ino {“QA”,“EVENT”} do int Wo eV G fiE rN sti i= nvo{ kv ei(j t) h} e. conceptoffidelityoftheretrievalprocess
5: type json ←LLM generator(t i,type) andnormalizedmarginfrompreviouswork[3].Subsequently,
6: Score json ←LLM evaluator(type json) withoutlossofgenerality,wedemonstratethatthesegenerated
7: if Score in Score json ≤τ then vectors either maintain or enhance the fidelity of the retrieval
8: type json ←LLM generator(t i,type json,Score json) process. Theorem III.3 introduces the effectiveness of text
9: end if augmentation for dense retrieval. Theorem III.4 demonstrates
10: Unstructured form: {element( ij)}←type json the effectiveness of both QA Pair texts and event texts.
11: if Texts Remain Independent then Fidelity refers to the ability of dense vector models to
12: type i ←{element( ij)} maintain the distinctions made by traditional sparse bag-of-
13: else wordsretrievalmodels.Unlikesparserepresentationsforexact
14: type i ←Concatenate[element( ij) ] matching, dense vector models map texts of arbitrary length
15: end if into a fixed-length vector space, which may result in a loss
16: V type
i
←Φ(type i) offidelityandconsequentlyinformationloss.Importantly,our
17: VDB type ←VDB type∪V type QAEA generates information-dense new texts that removes
i
18: end for noisy texts and refines key information to improve fidelity. To
19: end for measure fidelity, we introduce normalized margin to indicate
20: VDB final ←VDB ori+VDB QA+VDB EVENT the distinction between the truly relevant text and other texts.
21: return VDB final
Definition III.2 (Normalized Margin). Let v , v , and v be
q 1 2
sentenceembeddingsinRd.Thenormalizedmarginisdefined
as:
Completeness.Forexample,theRelevancerulechecksifeach
⟨v ,v −v ⟩
generatedtextfaithfullyreflectstheoriginalcontent,deducting µ(v ,v ,v )= q 1 2 (1)
q 1 2 ∥v ∥·∥v −v ∥
one point for irrelevance. The scoring details are outlined q 1 2
in the JSON format Score json: {“total score”,“detail” : Thenormalizedmargininretrievalmodelsservesasaquan-
[{“deduction reason”,“deduction score”,“related content”}]}.
titativemeasuretoevaluatefidelityandprovidesacomparative
Outputs that score at or below a set threshold τ enter a
perspective on vector similarity. It indicates how distinctly a
regeneration prompting, where texts are adjusted or rewritten
target text is separated from irrelevant ones in vector space,
to ensure that outputs meet rigorous standards. This approach enhancing retrieval accuracy and relevance. Assuming v is
1
not only provides a reliable method for assessing generated
the target text vector, we expect a larger normalized margin
textsbut alsocontrolsthe qualityofthe LLMoutputsthrough between v and v (µ(v ,v ,v ) > 0), which indicates a
1 2 q 1 2
a structured regeneration process based on scoring outcomes.
greater difference in relevance between the target text and
Unified Framework. Algorithm 1 details the QAEA module
other texts for a given query.
and shows that QAEA manages QA pairs and events in a
Theorem III.3 uses normalized margin to demonstrate the
similar manner. We conclude that QAEA is identified as a
effectiveness of text augmentation. The theorem holds under
“unified” framework from two perspectives:
certain constraints, including relevance enhancement, irrele-
• Text generation. QAEA employs standardized prompt vance consistency, and orthogonality. Under ideal text aug-
templates for generation, scoring-based quality evalua- mentation, the generated vectors of the target text should
tion,andregeneration,ensuringauniformframeworkfor exhibit improved query relevance, while those of non-target
both QAG and EE. texts should not be more relevant to the query than the
• Text organization. Additionally, all structured texts from original vectors. Additionally, in sparse retrieval, orthogonal
both QA pairs and events are eventually converted into vectors can be achieved by dividing the vocabulary into non-
unstructured text, then embedded and incorporated into overlapping segments. Similarly, in dense models, we assume
the vector database in a unified manner for dense re- that vector representations of different texts are orthogonal
trieval. when the content is irrelevant.
Theorem III.3 (Text Augmentation). Given a text t , let
i
D. Theory in QAEA-DR {v(j)} represent a set of generated text vectors, where j
i
Now, we theoretically explain the effectiveness of QAEA- records the total number of generated texts, and let v repre-
i
DR. In the following theoretical analysis, we consider only sent the original text vector. Consider a text t most relevant
1
the case of Texts Remain Independent (TRI) mentioned in to a query q and a competing text t , we have generated text
2
Section III-B. It is evident that Texts Merge into One (TMO) vectorsets{v(j)}and{v(j)},respectively.Thereexistsatleast
1 27
one generated vector v(0) ∈ {v(j)} such that the normalized a portion of the original text’s information. Given the text t
1 1 1
margins andtherelatedquery,theidealgeneratedvectorv(0) enhances
1
µ(v ,v(0),v )≥µ(v ,v ,v ) (2) retrieval fidelity by reducing noise and condensing query-
q 1 2 q 1 2
relevant information from the text.
and
Building on the demonstrated effectiveness of text aug-
µ(v ,v(0),v(j) )≥µ(v ,v ,v ),∀j (3)
q 1 2 q 1 2 mentation, we show that incorporating both QA pair vectors
are both satisfied under the following conditions: and event vectors into the text augmentation framework en-
(i)RelevanceEnhancement:∃v(0) ∈{v(j)},s.t.⟨v ,v(j)⟩≥ hances retrieval fidelity more effectively than using a single
1 1 q 1
⟨v ,v ⟩; type of generated text. Within the constraints of relevance
q 1
(ii) Irrelevance Consistency: ∀j, ⟨v ,v(j)⟩≤⟨v ,v ⟩; enhancement and irrelevance consistency, the introduction of
q 2 q 2
new high-quality generated vectors will only improve fidelity,
(iii) Orthogonality: Any two generated text vectors across
{v(j)} and {v(j)} are orthogonal to each other. Additionally, making this conclusion clear.
1 2
different vector segments derived from a text are orthogonal Theorem III.4 (QAEA). Given a text t most relevant to a
1
to each other. query q and a competing text t , we generate sets of QA pair
2
vectors {v(j)} and event vectors {v(j) } for t , and {v(j)}
Proof. First, we construct hypothetical orthogonal noise vec- qa1 event1 1 qa2
tors v
noise1
for v
1
and v
noise2
for v 2. Representing v
1
and v
2
and{v e(j v) ent2}fort 2,respectively,wherej recordsthenumber
as: of generated texts. Let v 1 and v 2 represent the original text
vectorsoft andt .Given{v(j)}={v(j)}∪{v(j) },there
v
1
=v 1(0) +v noise1, (4)
exists at
lea1
st
one2 generated1
vector
v(q 0a )1
∈
{v(e jv )}ent s1
uch that
1 1
v =v(j) +v ,∀j (5) the normalized margins
2 2 noise2
Then, the squared norm expansion for the difference between µ(v ,v(0),v )≥µ(v ,v(j),v ),∀j (14)
q 1 2 q qa1 2
v and v gives us:
1 2 and
∥v 1−v 2∥2 =∥v 1(0)−v 2(j) +v
noise1
−v noise2∥2 µ(v q,v 1(0),v 2)≥µ(v q,v e(j v) ent1,v 2),∀j (15)
=∥v(0)−v(j)∥2+∥v −v ∥2 are satisfied under the following conditions:
1 2 noise1 noise2
+2⟨v 1(0)−v 2(j),v
noise1
−v noise2⟩. (6) ⟨v( ,i) vR (je )leva ⟩n ≥ce ⟨E vn ,h van ⟩c ;ement:∃j,s.t.⟨v q,v q(j a) 1⟩≥⟨v q,v 1⟩or
q event1 q 1
Giventheorthogonalitycondition(iii),thecrosstermvanishes: (ii) Irrelevance Consistency: ∀j, ⟨v ,v(j)⟩ ≤ ⟨v ,v ⟩ and
q qa2 q 2
⟨v 1(0)−v 2(j),v noise1 −v noise2⟩=0, (7) ⟨v q (, iiv i)e(j v O) en rt t2 h⟩ og≤ on⟨v aq li, tyv :2⟩ A;
ny two generated text vectors across
thus, {v(j)} and {v(j) } are orthogonal to each other.
qa1 event1
∥v 1−v 2∥2 =∥v 1(0)−v 2(j)∥2+∥v noise1 −v noise2∥2 Proof. We start by noting that the set {v 1(j)} = {v q(j a) 1} ∪
≥∥v(0)−v(j)∥2. (8) {v(j) }. From the relevance enhancement condition, there
1 2 event1
exists v(0) ∈ {v(j)} ∪ {v(j) } that maximizes the inner
Similarly, we have: 1 qa1 event1
product with v . The remaining proof follows similarly to the
q
∥v −v ∥2 ≥∥v(0)−v ∥2. (9) proof of Theorem III.3 and can be easily derived.
1 2 1 2
For the numerator of normalized margin, we have: Theorem III.4 proposes that integrating more generated
text vector representations is more likely to increase retrieval
⟨v q,v 1(0)−v 2⟩≥⟨v q,v 1⟩−⟨v q,v 2⟩ (10) fidelity compared to using only one type of generated text
vector representation.
⟨v ,v(0)−v(j)⟩≥⟨v ,v ⟩−⟨v ,v ⟩ (11)
q 1 2 q 1 q 2
IV. EXPERIMENT
based on the conditions (i) and (ii).
A. Datasets and Baselines
By combining inequalities 9 and 10, we conclude that:
Datasets. We utilize four passage retrieval datasets to eval-
µ(v q,v 1(0),v 2)≥µ(v q,v 1,v 2) (12) uate our QAEA-DR. Due to the high computational cost of
multiple LLM tasks, we used a subset of the complete open
Similarly, combining inequalities 8 and 11, we conclude that:
datasets for our experiments.
µ(v q,v 1(0),v 2(j) )≥µ(v q,v 1,v 2),∀j (13) • sCNPR: a proprietary small-scale Chinese News Passage
Retrieval dataset (sCNPR) that we created from real-
Therefore, the formulas (2) and (3) hold, proving that the
world news articles and user queries. sCNPR covers
text augmentation approach maintains or improves retrieval
diverse topics including economic, social, political, sci-
fidelity.
entific, and entertainment news. sCNPR contains 1083
The constraints in Theorem III.3 are based on the assump- news texts and 2382 user queries, with an average text
tion that each generated text is of high quality and contains length of 655 words.8
TABLEI
THENDCG(×100)COMPARISONSOFBASELINESANDOURQAEA-DRONFOURDATASETS
sCNPR T2Retrieval
m3e bge-zh m3e bge-zh
NDCG@1 NDCG@10 NDCG@1 NDCG@10 NDCG@1 NDCG@10 NDCG@1 NDCG@10
Baseline 70.23 79.31 73.17 81.73 78.33 86.59 82.19 89.21
GPT GLM GPT GLM GPT GLM GPT GLM GPT GLM GPT GLM GPT GLM GPT GLM
QAEA(TRI) 77.92 77.92 85.03 84.99 83.15 81.83 88.96 88.22 74.60 74.77 83.65 83.58 80.78 80.52 88.51 88.34
QAEA(TMO) 72.84 72.71 81.25 80.92 75.74 75.86 83.38 83.27 79.13 79.23 87.24 87.19 82.74 83.09 89.61 89.74
HotpotQA MSMARCO
dpr bge-en dpr bge-en
NDCG@1 NDCG@10 NDCG@1 NDCG@10 NDCG@1 NDCG@10 NDCG@1 NDCG@10
Baseline 79.38 74.24 95.79 92.70 70.23 80.64 95.93 98.02
GPT GLM GPT GLM GPT GLM GPT GLM GPT GLM GPT GLM GPT GLM GPT GLM
QAEA(TRI) 85.26 85.17 78.92 78.47 96.79 96.72 93.61 93.49 78.20 78.09 86.95 86.73 94.93 94.90 97.64 97.60
QAEA(TMO) 81.97 81.66 76.01 75.96 96.06 95.99 92.91 92.83 74.33 74.96 83.80 83.72 96.21 96.17 98.19 98.15
ThevaluesinboldrepresentthelargestNDCG.
• T2Retrieval [50]: an open Chinese passage retrieval JSON output, we manually create 20 JSON samples for fine-
dataset with 4-level relevance labels, useful for assessing tuning to ensure the stable JSON output. Additionally, we
models’ ability to distinguish fine-grained relevance. We select DeepSeek-v23 as the evaluator for scoring the quality
sampled5000instancesfromtrainingset,withanaverage of generated texts, using API calls for text generation. Given
text length of 438 words. thatDeepSeek-v2significantlyoutperformsGPT-3.5-turboand
• MS MARCO [51]: an open English passage retrieval ChatGLM3-6B on LLM evaluation benchmarks, it serves as
dataset from Bing, featuring single relevance labels. We aneffectiveindependentevaluatortoenhancethereliabilityof
sampled5000instancesfromtrainingset,withanaverage the output assessments. All experiments run on the NVIDIA
text length of 58 words. RTX 3090 GPU (24 GB).
• HotpotQA[52]:anopenEnglishpassageretrievaldataset Experimental Variables. In addition to using different
from Wikipedia, featuring single relevance labels. We LLMs, we analyze the impact of various embedding models
sampled5000instancesfromtrainingset,withanaverage Φ, thresholds τ (default set at 9 out of 10), dataset size, and
text length of 67 words. text organization strategies (Texts Remain Independent (TRI)
and Texts Merge into One (TMO)) on QAEA-DR. We also
Baselines. We select four sentence embedding models that
evaluate the separate effects of QA pairs and events.
donotusetextaugmentationandembedonlytheoriginaltexts
into a vector database (VDB ) as our baselines for dense
ori
retrieval. We evaluate our QAEA-DR method, which applies C. Evaluation Metrics
text augmentation for augmented vector database (VDB final), We employ Normalized Discounted Cumulative Gain
against these baselines to demonstrate its performance im- (NDCG) as the critical metric within the MTEB [27] evalu-
provement under the same standard retrieval process. We ation framework to assess the retrieval performance. Besides,
utilize the Chinese sentence embedding models m3e-base and MTEBalsorecordsmetricslikemeanreciprocalrank(MRR),
bge-large-zh-v1.5 [9]. For English, we employ the embedding recall,precision,etc.NDCGmeasuresretrievalrankingquality
models dpr-ctx encoder-multiset-base [2] and bge-large-en- by accounting for the positions of relevant documents, pro-
v1.5. These sentence embedding models represent effective viding a more comprehensive evaluation compared to other
and illustrative examples of sentence embedding techniques. metrics.
Besides, our LLM-based QAG method is compared with
the currently popular LMQG model [48] to demonstrate the
D. Main Results
effectiveness of our approach. In our experiment, we employ
In this section, we present the main experimental results
the End2end T5-large-squad-QAG model of LMQG for com-
comparing the performance of QAEA-DR and the baselines
parison.Ontheotherhand,therearenoeffectiveopen-domain
acrossfourdatasetsbasedonNDCG@1andNDCG@10met-
multi-event extraction models available for comparison.
rics. Table I displays the QAEA-DR results from a complete
three-step prompting process.
B. Implementation Details Results Across Datasets. Overall, our QAEA-DR outper-
forms the baselines across four datasets, achieving optimal
LLM Implementation. We employ two LLMs as text aug-
results as shown in Table I. Our method exhibits effectiveness
mentation generators and another LLM as a evaluator for
and robustness across datasets with varying languages, text
evaluating the quality of the generated texts. In our exper-
sources, and text lengths. Notably, on the sCNPR dataset,
iments, we utilize API calls to GPT-3.5-turbo and locally
which consists of relatively long news texts, QAEA-DR
deployed ChatGLM3-6B [53] as the two generators for QAG
shows the most significant improvement, with the average
and EE. We set the temperature parameter to 0 to ensure the
NDCG@1 score rising from 71.70% to 80.54%. The results
generated text is precise and faithful to the original input.
In our experiments, since ChatGLM3-6B is not friendly with 3https://platform.deepseek.com9
QAEA Recall@1 Correctness (sCNPR) QAEA Recall@1 Correctness (T2Retrieval) TABLEII
Recall@1 Better Recall@1 Better COMPARISONSOFLLMSINGLE-GENERATIONANDWITHREGENERATION
15 14.69% 14.19% Recall@1 Worse 6 4.25% Recall@1 Worse PERFORMANCEINQAEA(TRI):NDCG@1(×100)
4
2.71% 2.33%
10 2 1.59% sCNPR T2Retrieval
5.38% 6.05% 0 m3e bge-zh m3e bge-zh
5 −2 1.79% 0.80% LLM GPT GLM GPT GLM GPT GLM GPT GLM
Single-generation 77.34 67.88 83.04 74.90 74.01 74.03 80.28 80.33
0 −4 4.12% +Regeneration 77.92 77.92 83.15 81.83 74.60 74.77 80.78 80.52
−5 4.72% 2.81%
6.51%
3.44% −− 86
7.97%
dprHotpotQA
bge-en
dpM
r
SMARC bO
ge-en
bge-zh bge-zh m3e m3e
−10
bge-zh bge-zh m3e m3e
LLM GPT GLM GPT GLM GPT GLM GPT GLM
(TRI) (TMO) (TRI) (TMO) (TRI) (TMO) (TRI) (TMO) Single-generation 84.83 83.82 96.61 96.45 77.28 77.88 94.80 94.45
QAEA Recall@1 Correctness (HotpotQA) QAEA Recall@1 Correctness (MS MARCO) +Regeneration 85.26 85.17 96.79 96.72 78.20 78.09 94.93 94.90
10 Recall@1 Better 15 Recall@1 Better ThevaluesinboldrepresentthelargestNDCG@1.
8 7.79% Recall@1 Worse 11.80% Recall@1 Worse
10
6 TABLEIII
5 5.99% 5.24% COMPARISONSOFLLMSINGLE-GENERATIONANDWITHREGENERATION
4 3.32% PERFORMANCEINQAEA(TMO):NDCG@1(×100)
1.26%
2 1.43% 0
0.53% 0.98% 1.16% sCNPR T2Retrieval
0 0.46% 0.25% 0.73% −5 3.83% LLM GPTm3 Ge LM GPb Tge- Gzh LM GPTm3 Ge LM GPb Tge- Gzh LM
−2 1.91% 6.99% Single-generation 72.38 71.45 75.25 74.01 79.07 79.07 82.33 82.53
bge-en bge-en dpr dpr
−10
bge-en bge-en dpr dpr +Regeneration 72.84 72.71 75.74 75.86 79.13 79.23 82.74 83.09
(TRI) (TMO) (TRI) (TMO) (TRI) (TMO) (TRI) (TMO)
HotpotQA MSMARCO
dpr bge-en dpr bge-en
Fig. 3. QAEA-DR vs Baseline on Recall@1. The blue bar represents the
LLM GPT GLM GPT GLM GPT GLM GPT GLM
percentage of the entire dataset where QAEA-DR correctly recalls at rank
Single-generation 81.82 80.66 95.91 95.87 73.51 74.15 96.10 96.04
1, while the baseline does not; the orange bar indicates the opposite. The
+Regeneration 81.97 81.66 96.06 95.99 74.33 74.96 96.21 96.17
differencebetweentheblueandorangebarsquantifiestheactualimprovement
thatQAEA-DRprovidesoverthebaseline. ThevaluesinboldrepresentthelargestNDCG@1.
from the sCNPR dataset are significant since sCNPR consists NPR, HotpotQA, and MS MARCO datasets. It records
of long text data and has not been used for embedding an average NDCG@1 score of 86.04% across these
model training. On the HotpotQA and MS MARCO datasets, datasets, exceeding the score of 82.98% recorded by
which primarily consist of short texts, there is an average TMO. As outlined in Section III-D, QAEA satisfies Rel-
increaseofapproximately4pointsinNDCG@1.Althoughthe evance Enhancement and Irrelevance Consistency under
T2Retrieval dataset presents a challenging task with its fine- ideal conditions. However, in practical settings such as
grained scoring metrics, QAEA-DR still manages to achieve T2Retrieval,wheretextsarelengthyandfilledwithirrel-
an average increase of nearly one point in NDCG@1. evant content, TRI may increase the risk of irrelevance
Results Across Embedding Models. Table I demonstrates mismatchingduetoretainingunclear,irrelevantgenerated
that QAEA-DR improves retrieval performance across four texts.
embedding models. It is worth noting that the initial retrieval • QAEA (TMO) demonstrates consistent stability, outper-
performance of the embedding models affects the extent of forming baselines across all datasets. For example, in
improvement achieved by QAEA-DR. For instance, in En- the T2Retrieval dataset, TMO enhances the density of
glish embedding models, QAEA-DR increases the average key information and reduces the density of noise by
NDCG@1scoreby6.93%onthetraditionaldpr-ctx encoder- reassembling generated texts, resulting in better results
multiset-base(dpr)modelandby0.64%onthemoreadvanced than TRI.
bge-large-en-v1.5 (bge-en) model. In Chinese embedding sce- • From Fig. 3, QAEA with either TMO or TRI overall
narios, both the m3e-base (m3e) and bge-large-zh-v1.5 (bge- improves recall@1 performance compared to the base-
zh)modelsshowcomparableretrievalcapabilities,andQAEA- lines, as indicated by the longer blue bars than orange.
DRdeliverssimilarenhancementstoeach.Therefore,QAEA- TMO consistently exhibits fewer instances of reduced
DR not only significantly enhances embedding models with recall@1 performance compared to TRI (shorter orange
weaker retrieval capabilities but also consistently improves bars) across different datasets and embedding models,
those already performing well. indicating more stable performance. In datasets like
ImpactofTextOrganization.Wealsoobserveobviouslydif- T2Retrieval with some typos, redundancies, or irrelevant
ferent retrieval performances between the two final generated texts, TRI may extract noisy texts, leading to an increase
text forms: Texts Remain Independent (TRI) and Texts Merge inpoorperformancecases;TMOmergesgeneratedtexts,
into One (TMO), as described in Section III-B. Overall, TRI minimizingtheimpactofnoisytextandresultinginfewer
canbeconsideredaformoftextdecomposition,whereasTMO recall@1 worse cases.
involvesreassemblingthedecomposedtexts.FromTableI,we Impact of LLM Generation. To demonstrate the robust-
observe the following: ness of text enhancement methods across different LLMs,
• QAEA (TRI) achieves higher peak results on the sC- we conducted the experimental analysis using two models:
)%(
tesatad
eritne
eht
fo egatnecreP
)%(
tesatad
eritne
eht
fo
egatnecreP
)%(
tesatad
eritne
eht
fo egatnecreP
)%(
tesatad
eritne
eht
fo
egatnecreP10
TABLEIV
THENDCG(×100)COMPARISONSOFABLATIONSTUDYONQAEA(TMO)
sCNPR T2Retrieval
m3e bge-zh m3e bge-zh
NDCG@1 NDCG@10 NDCG@1 NDCG@10 NDCG@1 NDCG@10 NDCG@1 NDCG@10
Baseline(Original) 70.23 79.31 73.17 81.73 78.33 86.59 82.19 89.21
GPT GLM GPT GLM GPT GLM GPT GLM GPT GLM GPT GLM GPT GLM GPT GLM
QA 68.85 67.38 77.67 76.44 72.12 71.41 80.26 79.74 74.79 73.89 83.48 82.99 78.61 78.32 85.95 85.81
Event 68.30 68.43 77.29 77.43 71.41 71.50 79.74 79.60 70.55 70.49 80.47 80.62 76.67 76.38 84.69 84.61
Original+QA 72.54 72.04 81.07 80.47 76.07 75.90 83.51 83.25 78.73 78.35 87.04 86.71 82.85 82.69 89.62 89.64
Original+Event 72.51 72.38 81.01 80.75 74.48 74.85 82.59 82.67 77.24 77.39 86.01 86.08 82.26 82.19 89.26 89.21
QA+Event 69.02 69.02 77.80 77.43 72.08 72.25 80.31 80.05 75.72 75.79 84.52 84.37 80.00 80.45 87.18 87.63
Original+QA+Event(QAEA) 72.84 72.71 81.25 80.92 75.74 75.86 83.38 83.27 79.13 79.23 87.24 87.19 82.74 83.09 89.61 89.74
HotpotQA MSMARCO
dpr bge-en dpr bge-en
NDCG@1 NDCG@10 NDCG@1 NDCG@10 NDCG@1 NDCG@10 NDCG@1 NDCG@10
Baseline(Original) 79.38 74.24 95.79 92.7 70.23 80.64 95.93 98.02
GPT GLM GPT GLM GPT GLM GPT GLM GPT GLM GPT GLM GPT GLM GPT GLM
QA 79.07 79.27 72.12 72.10 95.33 95.33 90.76 90.44 71.67 71.78 81.85 81.38 93.98 93.28 96.98 96.53
Event 73.01 72.24 67.80 67.18 94.44 94.40 89.63 88.83 65.29 64.80 76.56 76.10 91.56 90.86 95.36 94.93
Original+QA 82.47 82.55 76.28 76.38 95.91 95.91 92.83 92.85 74.50 74.48 84.03 84.01 96.08 96.12 98.13 98.14
Original+Event 80.81 80.50 75.32 75.27 96.10 96.06 92.95 92.89 72.44 73.19 82.81 83.05 95.85 95.89 98.02 98.03
QA+Event 78.69 79.15 72.28 72.02 95.56 95.60 91.44 91.37 72.33 72.10 82.31 82.02 94.56 94.67 97.29 97.35
Original+QA+Event(QAEA) 81.97 81.66 76.01 75.96 96.06 95.99 92.91 92.83 74.33 74.96 83.80 83.72 96.21 96.17 98.19 98.15
ThevaluesinboldrepresentthelargestNDCG.
ChatGLM3-6B (GLM) as a smaller-scale model and GPT- Ablation Performance of QAEA with Ablation Performance of QAEA with
GPT-turbo-3.5: NDCG@1 (×100) ChatGLM3-6B: NDCG@1 (×100)
3.5-turbo (GPT) as a medium-scale model. From Table I we
MS MA 78R .2C 0O/dpr MS M 7A 8R .0C 9O/dpr
observe that GPT exhibits better performance than GLM, as
SCNPR/bge-zh 74.50 MS MARCO/bge-en SCNPR/bge-zh MS MARCO/bge-en
expected.However,theaverageNDCG@1differencebetween 83.15 96.21 81.83 96.17
GPT and GLM is only 0.07%. Table II and Table III illustrate
70.23
that the performance gaps between the two LLMs increase SCNPR/m3e75.53 70.23 74.60 T 72 9R .e 1t 3rieval/ Sm C3 Ne
PR/m3e
70.23 74.77 T2 7R 9.e 2tr 3ieval/m3e
from 0.07% to 1.1% if the regeneration mechanism is absent. 77.92 72.71
T prh oi ms pd te im ngo mns etr that oe ds ut nh de er ro db iu ffs et rn ee ns ts Lo Lf Mth se asco gm enp el re at te ort sh .r See p- es cte ifp - HotpotQA/b 9g 6e .7-e 9n 82.47 T 82 2R .7e 4trieval /bge-zh HotpotQA/b 9g 6e .- 7e 2n 83.01 82.19 T2 8R 3e .0tr 9ieval /bge-zh
Baseline (Original)
ically, Table II compares the retrieval performance of QAEA Hotp8o5t.Q26A/dpr Q EvA e n+ t +O r Oig ri in gia nl a( lT (R TI R) I) Hotp8 o5 tQ.1 A7 /dpr
QA + Original (TMO)
(TRI) in two scenarios: (1) using single generation (Single- E Qv Ae n +t E+ v O enr ti g +i n Oa rl i( gT inM aO l () TRI)
QA + Event + Original (TMO)
generation) and (2) using generation with additional scoring-
based quality evaluation and regeneration (+Regeneration). Fig.4. AblationPerformanceofQAEA:NDCG@1(×100)
Similarly, Table III compares the QAEA (TMO) performance
between Single-generation and enhanced with Regeneration.
NDCG@1averageperformancedropsby0.90%.Additionally,
Regeneration with scoring-based quality evaluation consis-
bothOriginal+EventandOriginal+QAexceedtheperformance
tently improves retrieval performance across all cases and
of the baselines, which further demonstrates the effectiveness
confirms the method’s effectiveness.
and importance of both QA and Event components. We
observe that Original+QA outperforms Original+Event; we
E. Ablation Study on QAEA
believe this is because QA pairs are more likely to match the
WedemonstratetheeffectivenessoftheQAandEventcom-
query both in form and semantics. Moreover, even without
ponents of QAEA framework. From the equation VDB final = the original texts, single QA or Event components show
VDB
ori
+VDB
QA
+VDB EVENT, we understand that QAEA
performance close to the baselines. In the case of the MS
incorporates three textual representations: original texts, QA
MARCO dataset with the DPR model, they even outperform
pairs, and events. Hence, we consider seven scenarios derived
the baselines, which suggests the potential for these com-
fromdifferentcombinationsofthesecomponents:(1)Baseline
ponents to replace rather than merely augment the vector
(or Original, which includes only the original texts); (2)
database.
QA (only QA pair texts); (3) Event (only event texts); (4)
Additionally, Fig. 4 clearly shows the ablation perfor-
Original+QA; (5) Original+Event; (6) QA+Event; (7) Origi-
mance of both QAEA (TMO) and QAEA (TRI). We observe
nal+QA+Event (complete QAEA implementation).
thatQA+Event+Original(TRI)orQA+Event+Original(TMO)
Table IV shows comparison results of the ablation study
achieveoptimalperformance,withmanysubsetsalsoperform-
on QAEA. For discussion convenience, the table only show
ing well.
the QAEA (TMO) case and the conclusion also applies to
QAEA (TRI). First of all, the complete QAEA implementa-
V. ANALYSIS
tion, Original+QA+Event, shows superior performance across
most metrics compared to the subsets. When removing the In this section, we conduct the analysis of QAEA-DR
QAcomponent,theNDCG@1averageperformanceofQAEA framework and evaluate its performance under different con-
drops by 0.15%. When the Event component is removed, the ditions and configurations.11
100 HotpotQA 100 MSMARCO 100 T2Retrieval TABLEV
95 95 95 b b bg g ge e e- - -z z zh h h( ( (B T Ta R Ms Ie O)li )ne) AVERAGENUMBEROFQAPAIRSANDEVENTSGENERATEDFROM
90 90 90 m m3 3e e( (B Ta Rs Ie )line) ORIGINALTEXTTHROUGHLLMS
m3e(TMO)
85 85 85 Dataset Model Avg.NumberofQA Avg.NumberofEvent
80 80 80 perText perText
75 b bg ge e- -e en n( (B Ta Rs Ie )line) 75 b bg ge e- -e en n( (B Ta Rs Ie )line) 75 GPT 12.38 4.37
bge-en(TMO) bge-en(TMO) sCNPR
70 d dp pr r( (B Ta Rs Ie )line) 70 d dp pr r( (B Ta Rs Ie )line) 70 GLM 11.06 4.18
651000dpr(TMO) 2000 5000 651000dpr(TMO) 2000 5000 651000 2000 5000 T2Retrieval GPT 6.13 3.71
GLM 5.91 3.61
Fig.5. Analysisofdifferentdatasetsizes:NDCG@1(×100) GPT 5.60 2.51
HotpotQA
GLM 5.07 2.35
GPT 4.31 2.27
sCNPR T2Retrieval MSMARCO GLM 3.94 2.21
85
bbmmgg33eeee--zz((hhTT((MRTT IOMR ))IO)) 85
80
80
75 75 (TMO) demonstrates more stable performance than QAEA
70 5 7 9 70 5 7
bbmmgg33eeee--zz((hhTT((MRTT IOMR
)
9)IO))
(TMO) across datasets, particularly at higher thresholds.
Threshold Threshold
HotpotQA MSMARCO
95 C. Impact of Generated Text Quantity
90
89 50 bbddggppeerr--ee((TTnnMR((TT IO)MR)IO))
80
bbddggppeerr--ee((TTnnMR((TT IO)MR)IO)) perW oe rii gn iv ne as ltig tea xte
t
oth ne Qim Ap Ea Act -Dof Rth pe en rfu om rmbe ar nco ef .g Te an be lr eat Ved st uex mts
-
80 marizestheaveragenumberofQApairsandeventsgenerated
5 7 9 5 7 9
Threshold Threshold fromoriginaltextsusingGPTorGLMacrossfourdatasets.In
all datasets, GPT generates more structured texts than GLM.
Fig.6. AnalysisofdifferentregenerationthresholdsinQAEA-DR:NDCG@1
(×100) Referring to Table I, where GPT generally outperforms GLM
in QAEA (TRI) retrieval performance, we infer a positive
correlationbetweenthenumberofgeneratedtextsandretrieval
A. Impact of Dataset Size
performance.TableValsoindicatesthattheaveragenumberof
We investigate the impact of dataset size across three open QApairsgeneratedis2.15timestheaveragenumberofevents.
datasets as shown in Fig. 5. Across dataset sizes of 1000, This aligns with intuition since each QA pair only conveys an
2000,and5000,QAEA-DRconsistentlydemonstratessuperior individual information point, while event summarizes richer
performance over the baseline. Besides, as dataset size varies, information.
QAEA-DR and the baseline show similar trends across the
threedatasets,whichindicatestheconsistencyoftextaugmen- TABLEVI
COMPARISONOFLMQGANDOURLLM-BASEDGPT-QAGON
tation. The experimental results also corroborate our earlier
RETRIEVALPERFORMANCE:NDCG@1(×100)
discussions on the two text organization strategies. QAEA
(TMO) shows good stability throughout the experiment, con- MSMARCO HotpotQA
dpr bge-en dpr bge-en
sistently outperforming the baseline. In contrast, although
LMQG(TRI) 69.88 89.64 78.66 93.55
QAEA (TRI) shows significant performance improvements in LMQG(TMO) 64.55 90.15 77.49 93.90
most cases, its performance fluctuates greatly due to its high GPT-QAG(TRI) 74.07 92.89 80.23 95.29
GPT-QAG(TMO) 71.67 93.98 79.07 95.33
sensitivitytothequalityoftextgeneration,leadingtounstable
ThevaluesinboldrepresentthelargestNDCG.
results across different dataset sizes.
B. Impact of Regeneration Threshold D. QAG Comparisons
We study the impact of the score threshold in text regener- Table VI presents the QAG retrieval performance of our
ation process. Higher threshold τ means texts are more likely LLM-based text augmentation method compared to the pop-
to require regeneration to meet these standards as shown in ular QAG model LMQG. We use the QA component of the
Algorithm 1. QAEA framework as described in Section IV-E and employ
Fig. 6 shows the retrieval performance at three different GPT-3.5-turbo as the text generation LLM, denoted as GPT-
regeneration score thresholds: 5, 7, and 9, out of a total QAG. Both LMQG and GPT-QAG generate vector databases
score of 10. Figure presents the best results from GLM and of QA pair texts VDB for dense retrieval. Since LMQG
QA
GPT. For the sCNPR and T2Retrieval datasets, NDCG@1 only supports English, we display comparison results on two
increases with higher thresholds, indicating that stricter fil- English datasets. We observe that under both TRI and TMO
tering improves retrieval by regenerating lower-quality out- QA text organization strategies, our GPT-QAG significantly
puts. For HotpotQA and MS MARCO, performance remains outperformsLMQG,demonstratingtheeffectivenessofLLM-
relatively stable across threshold adjustments, which suggests based QAG using a three-step prompting process. Notably,
that threshold changes have lesser impact due to the already evaluating the text retrieval capability of generated QA pairs
high quality of the initial text generation. Meanwhile, QAEA offers a new insight for assessing QAG performance.
)001×(1@GCDN
)001×(1@GCDN
)001×(1@GCDN
)001×(1@GCDN
)001×(1@GCDN
)001×(1@GCDN
)001×(1@GCDN12
TABLEVII [2] V.Karpukhin,B.Oguz,S.Min,P.Lewis,L.Wu,S.Edunov,D.Chen,
TIMECOMPLEXITYCOMPARISONSFORQAEA(TMO)ANDQAEA(TRI) and W.-t. Yih, “Dense passage retrieval for open-domain question
ONSCNPR answering,” in EMNLP - Conf. Empir. Methods Nat. Lang. Process.,
Proc.Conf.,2020,pp.6769–6781.
TMO TRI [3] Y. Luan, J. Eisenstein, K. Toutanova, and M. Collins, “Sparse, dense,
Baseline(Original) nT sim andattentionalrepresentationsfortextretrieval,”Trans.Assoc.Comput.
GPT GLM GPT GLM Linguist.,vol.9,pp.329–345,2021.
QA nT sim nT sim 12.4nT sim 11.1nT sim [4] J. Zhan, J. Mao, Y. Liu, J. Guo, M. Zhang, and S. Ma, “Jointly
Event nT sim nT sim 4.4nT sim 4.2nT sim optimizingqueryencoderandproductquantizationtoimproveretrieval
QA+Event 2nT sim 2nT sim 16.8nT sim 15.3nT sim performance,”inIntConfInfKnowledgeManage,2021,pp.2487–2496.
QA+Event+Original 3nT sim 3nT sim 17.8nT sim 16.3nT sim [5] H. Li, A. Mourad, S. Zhuang, B. Koopman, and G. Zuccon, “Pseudo
relevance feedback with deep language models and dense retrievers:
Successesandpitfalls,”ACMTrans.Inf.Syst.,vol.41,no.3,pp.1–40,
E. Time Complexity 2023.
[6] J.Ni,G.H.Abrego,N.Constant,J.Ma,K.Hall,D.Cer,andY.Yang,
The computational time of dense retrieval is independent “Sentence-t5: Scalable sentence encoders from pre-trained text-to-text
of text length since all texts are embedded into vectors of models,”inProc.Annu.Meet.Assoc.ComputLinguist.,2022,pp.1864–
1874.
the same dimensionality. Hence, the retrieval time is directly
[7] H. Yu, C. Xiong, and J. Callan, “Improving query representations
proportional to the number of text vectors in the vector for dense retrieval with pseudo relevance feedback,” in Int Conf Inf
database. For n texts and the associated vector database KnowledgeManage,2021,pp.3592–3596.
[8] J. Johnson, M. Douze, and H. Je´gou, “Billion-scale similarity search
VDB = {v ,v ,...,v } under baseline, the time taken for
ori 1 2 n withgpus,”IEEETrans.BigData,vol.7,no.3,pp.535–547,2019.
each query to traverse the texts is n × T , where T is [9] S. Xiao, Z. Liu, P. Zhang, and N. Muennighof, “C-pack: Packaged
sim sim
the computational cost to calculate sim(v ,v ), v ∈ VDB . resources to advance general chinese embedding,” arXiv:2309.07597,
q i i ori 2023.
Table VII presents the time complexity for QAEA (TMO)
[10] P.Zhao,H.Zhang,Q.Yu,Z.Wang,Y.Geng,F.Fu,L.Yang,W.Zhang,
and QAEA (TRI) on the sCNPR dataset. Similar conclusions and B. Cui, “Retrieval-augmented generation for ai-generated content:
regarding time complexity are observed across other datasets. Asurvey,”arXiv:2402.19473,2024.
[11] Y. Wang, H. Huang, and C. Feng, “Query expansion with local con-
• ForQAEA(TMO),eachoriginaltextgeneratesamerged ceptual word embeddings in microblog retrieval,” IEEE Trans. Knowl.
QA pair text and a merged event text. In the TMO DataEng.,vol.33,no.4,pp.1737–1749,2019.
[12] L.Gao,X.Ma,J.Lin,andJ.Callan,“Precisezero-shotdenseretrieval
case, the presence of any component—Original, QA,
withoutrelevancelabels,”inProc.Annu.Meet.Assoc.ComputLinguist.,
or Event—increases the retrieval time by nT sim. It is 2023,pp.1762–1777.
observed that having only a QA or Event component [13] L. Bonifacio, H. Abonizio, M. Fadaee, and R. Nogueira, “Inpars:
Unsupervised dataset generation for information retrieval,” in SIGIR -
without the original text results in computational times
Proc.Int.ACMSIGIRConf.Res.Dev.Inf.Retr.,2022,pp.2387–2392.
comparable to the baseline. [14] Y.YangandA.Nenkova,“Detectinginformation-densetextsinmultiple
• For QAEA (TRI), each original text generates and re- newsdomains,”inProcNatlConfArtifIntell,vol.28,no.1,2014.
[15] B.F.GreenJr,A.K.Wolf,C.Chomsky,andK.Laughery,“Baseball:
mainsmultipleQApairsandevents.IntheTRIscenario,
anautomaticquestion-answerer,”inProc.WesternJt.ComputerConfer-
adding a QA or Event component may increase compu- ence:ExtendingMan’sIntellect,IRE-AIEE-ACM,1961,pp.219–224.
tational time several-fold compared to the baseline. [16] W. W. Cohen, W. Chen, M. De Jong, N. Gupta, A. Presta, P. Verga,
andJ.Wieting,“Qaisthenewkr:question-answerpairsasknowledge
Tosummarize,weconsiderthetimeoverheadofQAEA-DRto
bases,” in Proc. AAAI Conf. Artif. Intell., AAAI, vol. 37, no. 13, 2023,
be relatively flexible, and the individual QA or Event compo- pp.15385–15392.
nentintheTMOcasematchesthebaselinetimeconsumption, [17] K.Lee,S.E.Han,S.W.Hwang,andM.Lee,“Whentoreaddocuments
or qa history: On unified and selective open-domain qa,” in Proc.
demonstrating a direction for optimizing text augmentation
Annu. Meet. Assoc. Comput Linguist. Association for Computational
that balances high performance with time efficiency. Linguistics(ACL),2023,pp.6420–6432.
[18] W.XiangandB.Wang,“Asurveyofeventextractionfromtext,”IEEE
VI. CONCLUSION Access,vol.7,pp.173111–173137,2019.
[19] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,
In this paper, we introduce QAEA-DR, a novel unified text Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” Adv. neural
augmentation framework for dense retrieval. This approach inf.proces.syst.,vol.30,2017.
[20] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova,“Bert:Pre-trainingof
optimizestheoriginaltextbygeneratingmultipleQApairsand
deepbidirectionaltransformersforlanguageunderstanding,”inNAACL
events via LLM-based information extraction, which concen- HLT - Conf. N. Am. Chapter Assoc. Comput. Linguistics: Hum. Lang.
trates on key information and removes noisy text. As a result, Technol.-Proc.Conf.,2019,pp.4171–4186.
[21] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,
the augmented vector database increases retrieval fidelity and
Y.Zhou,W.Li,andP.J.Liu,“Exploringthelimitsoftransferlearning
effectively mitigates the issue of losing key information in with a unified text-to-text transformer,” J. Mach. Learn. Res., vol. 21,
dense retrieval. We conduct comprehensive experiments to no.1,pp.5485–5551,2020.
[22] A. Yates, R. Nogueira, and J. Lin, “Pretrained transformers for text
demonstrate the effectiveness and robustness of QAEA-DR,
ranking:Bertandbeyond,”inSIGIR-Proc.Int.ACMSIGIRConf.Res.
even for datasets mainly comprising short texts. QAEA-DR Dev.Inf.Retr.,2021,pp.1154–1156.
indicates broader applicability by offering insights into open- [23] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal,
H. Ku¨ttler, M. Lewis, W.-t. Yih, T. Rockta¨schel et al., “Retrieval-
domain LLM-based QAG and EE, and serving as a universal
augmented generation for knowledge-intensive nlp tasks,” Adv. neural
text optimizer in Retrieval-Augmented Generation (RAG). inf.proces.syst.,vol.33,pp.9459–9474,2020.
[24] O.Ram,Y.Levine,I.Dalmedigos,D.Muhlgay,A.Shashua,K.Leyton-
REFERENCES Brown,andY.Shoham,“In-contextretrieval-augmentedlanguagemod-
els,”Trans.Assoc.Comput.Linguist.,vol.11,pp.1316–1331,2023.
[1] K. Lee, M.-W. Chang, and K. Toutanova, “Latent retrieval for weakly [25] C.Zheng,L.Zhu,X.Lu,J.Li,Z.Cheng,andH.Zhang,“Fastdiscrete
supervised open domain question answering,” in ACL - Annu. Meet. collaborativemulti-modalhashingforlarge-scalemultimediaretrieval,”
Assoc.Comput.Linguist.,Proc.Conf.,2019,pp.6086–6096. IEEETrans.Knowl.DataEng.,vol.32,no.11,pp.2171–2184,2020.13
[26] N.ReimersandI.Gurevych,“Sentence-bert:Sentenceembeddingsusing [48] A.Ushio,F.Alva-Manchego,andJ.Camacho-Collados,“Anempirical
siamesebert-networks,”inEMNLP-IJCNLP-Conf.Empir.MethodsNat. comparison of lm-based question and answer generation methods,” in
Lang.Process.Int.Jt.Conf.Nat.Lang.Process.,Proc.Conf.,2019,pp. Proc.Annu.Meet.Assoc.ComputLinguist.,2023,pp.14262–14272.
3982–3992. [49] L. Giray, “Prompt engineering with chatgpt: a guide for academic
[27] N. Muennighoff, N. Tazi, L. Magne, and N. Reimers, “Mteb: Massive writers,”AnnBiomedEng,vol.51,no.12,pp.2629–2633,2023.
text embedding benchmark,” in EACL - Conf. Eur. Chapter Assoc. [50] X.Xie,Q.Dong,B.Wang,F.Lv,T.Yao,W.Gan,Z.Wu,X.Li,H.Li,
Comput.Linguist.,Proc.Conf.,2023,pp.2014–2037. Y.Liuetal.,“T2ranking:Alarge-scalechinesebenchmarkforpassage
[28] G.Izacard,M.Caron,L.Hosseini,S.Riedel,P.Bojanowski,A.Joulin, ranking,” in SIGIR - Proc. Int. ACM SIGIR Conf. Res. Dev. Inf. Retr.,
andE.Grave,“Unsuperviseddenseinformationretrievalwithcontrastive 2023,pp.2681–2690.
learning,”Proc.Mach.Learn.Res.,2021. [51] T. Nguyen, M. Rosenberg, X. Song, J. Gao, S. Tiwary, R. Majumder,
[29] R. Huang, J. Huang, D. Yang, Y. Ren, L. Liu, M. Li, Z. Ye, J. Liu, andL.Deng,“Msmarco:Ahumangeneratedmachinereadingcompre-
X. Yin, and Z. Zhao, “Make-an-audio: Text-to-audio generation with hensiondataset,”CEURWorkshopProc.,vol.2640,p.660,2016.
prompt-enhanceddiffusionmodels,”inProc.Mach.Learn.Res. PMLR, [52] Z.Yang,P.Qi,S.Zhang,Y.Bengio,W.Cohen,R.Salakhutdinov,and
2023,pp.13916–13932. C.D.Manning,“Hotpotqa:Adatasetfordiverse,explainablemulti-hop
questionanswering,”inProc.Conf.Empir.MethodsNat.Lang.Process.,
[30] H. Wang, G. Lin, S. Hoi, and C. Miao, “Paired cross-modal data
EMNLP,2018,pp.2369–2380.
augmentation for fine-grained image-to-text retrieval,” in MM - Proc.
[53] Z.Du,Y.Qian,X.Liu,M.Ding,J.Qiu,Z.Yang,andJ.Tang,“Glm:
ACMInt.Conf.Multimed.,2022,pp.5517–5526.
Generallanguagemodelpretrainingwithautoregressiveblankinfilling,”
[31] S. Lu, N. Duan, H. Han, D. Guo, S.-w. Hwang, and A. Svyatkovskiy,
inProc.Annu.Meet.Assoc.ComputLinguist.,2022,pp.320–335.
“Reacc: A retrieval-augmented code completion framework,” in Proc.
Annu.Meet.Assoc.ComputLinguist.,2022,pp.6227–6240.
[32] J. Cowie and W. Lehnert, “Information extraction,” Commun ACM,
vol.39,no.1,pp.80–91,1996.
[33] S.Ji,S.Pan,E.Cambria,P.Marttinen,andS.Y.Philip,“Asurveyon
knowledgegraphs:Representation,acquisition,andapplications,”IEEE
Trans.NeuralNetw.Learn.Syst.,vol.33,no.2,pp.494–514,2021.
[34] D. Wadden, U. Wennberg, Y. Luan, and H. Hajishirzi, “Entity, rela-
tion,andeventextractionwithcontextualizedspanrepresentations,”in
EMNLP-IJCNLP - Conf. Empir. Methods Nat. Lang. Process. Int. Jt.
Conf.Nat.Lang.Process.,Proc.Conf.,2019,pp.5784–5789.
[35] M. A. Valenzuela-Esca´rcega, G. Hahn-Powell, M. Surdeanu, and
T. Hicks, “A domain-independent rule-based framework for event ex-
traction,” in ACL-IJCNLP - Annu. Meet. Assoc. Comput. Linguist. Int.
Jt.Conf.Nat.Lang.Process.,Proc.Syst.Demonstr.,2015,pp.127–132.
[36] Y. Chen, L. Xu, K. Liu, D. Zeng, and J. Zhao, “Event extraction via
dynamicmulti-poolingconvolutionalneuralnetworks,”inACL-IJCNLP
-Annu.Meet.Assoc.Comput.Linguist.Int.Jt.Conf.Nat.Lang.Process.
AsianFed.Nat.Lang.Process.,Proc.Conf.,2015,pp.167–176.
[37] T. H. Nguyen, K. Cho, and R. Grishman, “Joint event extraction via
recurrentneuralnetworks,”inConf.NorthAm.ChapterAssoc.Comput.
Linguist.: Hum. Lang. Technol., NAACL HLT - Proc. Conf., 2016, pp.
300–309.
[38] X.Wei,X.Cui,N.Cheng,X.Wang,X.Zhang,S.Huang,P.Xie,J.Xu,
Y.Chen,M.Zhangetal.,“Zero-shotinformationextractionviachatting
withchatgpt,”arXiv:2302.10205,2023.
[39] H.Huang,X.Liu,G.Shi,andQ.Liu,“Eventextractionwithdynamic
prefix tuning and relevance retrieval,” IEEE Trans. Knowl. Data Eng.,
vol.35,no.10,pp.9946–9958,2023.
[40] X.Liu,Z.Luo,andH.-Y.Huang,“Jointlymultipleeventsextractionvia
attention-based graph information aggregation,” in Proc. Conf. Empir.
MethodsNat.Lang.Process.,EMNLP,2018,pp.1247–1256.
[41] X. Wang, L. Gui, and Y. He, “Document-level multi-event extraction
witheventproxynodesandhausdorffdistanceminimization,”inProc.
Annu.Meet.Assoc.ComputLinguist.,2023,pp.10118–10133.
[42] P.Lewis,L.Denoyer,andS.Riedel,“Unsupervisedquestionanswering
by cloze translation,” in ACL - Annu. Meet. Assoc. Comput. Linguist.,
Proc.Conf.,2019,pp.4896–4910.
[43] S. Zhang and M. Bansal, “Addressing semantic drift in question gen-
eration for semi-supervised question answering,” in EMNLP-IJCNLP
- Conf. Empir. Methods Nat. Lang. Process. Int. Jt. Conf. Nat. Lang.
Process.,Proc.Conf.,2019,pp.2495–2509.
[44] M. Heilman and N. A. Smith, “Good question! statistical ranking for
question generation,” in NAACL HLT - Hum. Lang. Technol.: Annu.
Conf.NorthAm.ChapterAssoc.Comput.Linguist.,Proc.Conf.,2010,
pp.609–617.
[45] D.Lindberg,F.Popowich,J.Nesbit,andP.Winne,“Generatingnatural
language questions to support learning on-line,” in ENLG - Europ.
WorkshopNat.Lang.Gener.,Proc.,2013,pp.105–114.
[46] I.Labutov,S.Basu,andL.Vanderwende,“Deepquestionswithoutdeep
understanding,”inACL-IJCNLP-Annu.Meet.Assoc.Comput.Linguist.
Int.Jt.Conf.Nat.Lang.Process.AsianFed.Nat.Lang.Process.,Proc.
Conf.,2015,pp.889–898.
[47] D. B. Lee, S. Lee, W. T. Jeong, D. Kim, and S. J. Hwang, “Gener-
ating diverse and consistent qa pairs from contexts with information-
maximizing hierarchical conditional vaes,” Proc. Annu. Meet. Assoc.
ComputLinguist.,pp.208–224,2020.