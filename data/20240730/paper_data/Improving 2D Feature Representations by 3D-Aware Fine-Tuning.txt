Improving 2D Feature Representations by
3D-Aware Fine-Tuning
Yuanwen Yue1 Anurag Das2 Francis Engelmann1,3
Siyu Tang1 Jan Eric Lenssen2
1 ETH Zurich 3 Google
2 Max Planck Institute for Informatics, Saarland Informatics Campus
(a) (b)
Lifting 3D-aware Downstream
ùëΩùë∫ features to 3D fine-tuning tasks
(c) ((dd))
Fig.1: We propose 3D-aware fine-tuning to improve 2D foundation features. Our
method starts with lifting 2D image features (e.g. DINOv2 [44]) (b) to a 3D rep-
resentation. Then we finetune the 2D foundation model using the 3D-aware features
(c).Wedemonstratethatincorporatingthefine-tunedfeatures (d)resultsinimproved
performanceondownstreamtaskssuchassemanticsegmentationanddepthestimation
onavarietyofdatasetswithsimplelinearprobing(right).Featuremapsarevisualized
using principal component analysis (PCA).
578511c8a9_DSC09683
Abstract. Current visual foundation models are trained purely on un-
structured 2D data, limiting their understanding of 3D structure of ob-
jectsandscenes.Inthiswork,weshowthatfine-tuningon3D-awaredata
improvesthequalityofemergingsemanticfeatures.Wedesignamethod
to lift semantic 2D features into an efficient 3D Gaussian representa-
tion, which allows us to re-render them for arbitrary views. Using the
rendered3D-awarefeatures,wedesignafine-tuningstrategytotransfer
such 3D awareness into a 2D foundation model. We demonstrate that
models fine-tuned in that way produce features that readily improve
downstream task performance in semantic segmentation and depth es-
timation through simple linear probing. Notably, though fined-tuned on
a single indoor dataset, the improvement is transferable to a variety of
indoordatasetsandout-of-domaindatasets.Wehopeourstudyencour-
ages the community to consider injecting 3D awareness when training
2D foundation models. Project page: https://ywyue.github.io/FiT3D.
Keywords: Representation learning ¬∑ Foundation models ¬∑ Gaussian
splatting ¬∑ Scene understanding
4202
luJ
92
]VC.sc[
1v92202.7042:viXra2 Y. Yue et al.
1 Introduction
Ever since the emergence of deep neural networks, vision systems are largely
trained on 2D datasets. With the scalability of recent architectures, like vision
transformers (ViT) [13], several large vision models [7,26,35,44,50] have been
trainedfromarichsetof2Dimagesbyeithersupervisedorself-supervisedlearn-
ing. Visual foundation models have shown impressive utility as general feature
extractors that can be applied to improve results on downstream tasks, such as
segmentation [38,57], depth estimation [32,52,64], or correspondence estima-
tion [1,66]. They are trained on a large amount of readily available 2D images
and, thus, learn statistics about object and scene structure in 2D-pixel space.
Images, as a simple projection of our 3D world, are easy to obtain and pro-
vide an efficient way to depict the visual world while at the same time discard-
ing explicit 3D geometry information. It is expected that vision systems purely
trainedon2Dimagescannotfullyunderstandtheunderlying3Dstructureofour
world [15]. There are several promising properties of our 3D world, for example,
multi-viewconsistency,andmulti-viewfusionforsolvingsingle-viewambiguities.
Acruciallimitationofthetrainingsetupsofthesemodelsisthattheydon‚Äôtfully
reason about the 3D structure of seen objects. Training images are presented to
thenetworkinanunstructuredway,withoutanymulti-vieworvideocorrespon-
dencesthatwouldallowmatchingobservationsofthesameobjectfrommultiple
views.Asaconsequence,thesemodelshavelimited3Dunderstandingofobjects
observed from, e.g., different views are not producing view-consistent features.
Incontrast,whenwehumansobserveimages,weeffortlesslyachieveaholistic
understanding by not only perceiving the 2D visual content but also exploiting
the inferred underlying 3D structure, which we have learned through lifelong
observation of stereo and temporal information. In this work, we investigate
if large scale 2D vision models can also profit from equipping them with such
3D-aware understanding abilities induced by showing the right type of data.
To this end, we design a novel two-stage approach to improve the 3D-aware
understanding ability of 2D foundation models. In the first stage, we aim to
obtain 3D-aware features as training data. Motivated by recent advancements
in neural scene representation, we design an approach to lift multi-view 2D
foundationfeaturesintoanefficient3DGaussianrepresentation[33].Thelifting
process exploits multi-view consistency and allows 2D features from different
viewstocomplementeachother.Moreover,thefusedfeatures(Fig.1(c))exhibit
high resolution with fine details thanks to the learned 3D structure, emerging
from multi-view RGB guidance. Once trained, the 3D Gaussians can render
features for arbitrary views. In the following, we refer to features obtained in
this way as 3D-aware. In the second stage, we utilize the rendered 3D-aware
features to finetune the 2D foundation models (Fig. 2). To this end, we design
anefficientfine-tuningstrategytotransfersuch3Dawarenessinto2Dfoundation
models. After fine-tuning, we evaluate the feature quality on downstream tasks
thatmightprofitfromabetter3Dunderstanding,namelysemanticsegmentation
anddepthestimation.ExtensiveexperimentsdemonstratethatincorporatingtheImproving 2D Feature Representations by 3D-Aware Fine-Tuning 3
DINOv2 DINOv2-reg CLIP MAE DeiT-III
Input
DINOv2 (fine-tuned) DINOv2-reg (fine-tuned) CLIP (fine-tuned) MAE (fine-tuned) DeiT-III (fine-tuned)
Fig.2: Our 3D-aware fine-tuning is universal and applicable to a variety of 2D vision
models, e.g. DINOv2 [44], DINOv2-reg [10], CLIP [48], MAE [26], and DeiT-III [58]
(c.f. Sec. 4.5).
3D-aware features improves downstream tasks with simple linear probing and
exhibits generalization ability on out-of-domain datasets.
2 Related Work
We give an overview about recent self-supervised 2D representation learning
techniques in Sec. 2.1, and how emerging features have been distilled into 3D
representations in Sec. 2.2. Then, we discuss previous work that utilizes 3D
information to improve 2D representation methods in Sec. 2.3.
2.1 2D Representation Learning
Representation learning [4] has achieved remarkable progress in the image do-
main. It aims to learn generalizable visual features from a rich set of data. Self-
supervisedrepresentationlearninghasgainedparticularinterestsinceitdoesnot
require labeled data. Early works employ pretext tasks for pre-training, which
aim to exploit inherent data attributes to automatically generate surrogate la-
bels[12,14,21,43,45,60].Later,contrastivelearning[25]hasbeenpopularlyused
for representation learning by leveraging discriminative signals between images
or groups of images [6,7,23,27,44]. More recently, motivated by BERT [11], a
newparadigmofmaskedimagemodeling[3,8,26]hasbeenproposedforscalable
visual learning. Nevertheless, all those methods are only trained on 2D image
data, without accessing the underlying 3D structure. Our work aims to supple-
ment the features purely learned from 2D observations with 3D awareness.
2.2 Distilled Feature Fusion Fields
Neuralradiancefields(NeRF)[42]emergeasapromisingscenerepresentationfor
high-quality 3D reconstruction and novel view synthesis. Recently, some works
[16,22,34,36,59] explore distilling pre-trained image features (e.g. DINO [7],
CLIP [48], LSeg [37], or OpenSeg [20]) into NeRF via neural rendering. With-
out requiring any labels, such distilled feature fusion fields enable several zero-
shot 3D scene understanding tasks, e.g. segmentation, scene editing, and open-
vocabulary queries. We share similar inspiration from these works by distilling4 Y. Yue et al.
2Dfeaturesintoa3Drepresentation.However,insteadoffocusingonperception
tasks with feature fields, we are interested in leveraging the rendered 3D-aware
features to in turn improve the 2D feature extractor. We demonstrate that the
transferred 3D awareness can readily improve the 2D features on both semantic
and geometric tasks. Moreover, we extend the recent Gaussian-based represen-
tation [33] by designing a method to distill 2D features into 3D Gaussians while
keeping high efficiency and memory under bound. There are several concurrent
works introducing 3D Gaussians with semantic features [47,54,68]. However,
none of these works distill features back into 2D models. Our work shows, for
thefirsttime,thatsemanticfeaturesfusedinto3Drepresentationscaneffectively
improve 2D foundation models via fine-tuning.
2.3 Injecting 3D Priors to 2D
Existing works mainly focus on fusing multi-view 2D features into the 3D rep-
resentation [24,30,31,41,46,53,56,61]. Little attention has been paid to the
other direction of incorporating 3D awareness into 2D representation learning.
Pri3D [29] uses geometric constraints (multi-view consistency and 2D-3D cor-
respondence) from RGB-D reconstructions to learn 3D priors for image-based
representations with contrastive learning. Recently, inspired by the masked au-
toencoder(MAE)[26],severalworksadoptthemaskedimagemodelingstrategy
tolearn3Dpriors[2,28,62].However,allthesemethodsrequirepre-trainingthe
2D feature extractor, typically a Vision Transformer (ViT) backbone [13], using
their hand-crafted pretext tasks. The pre-trained models are then employed to
downstreamtasksviafine-tuning.Bycontrast,weaimtotransferthe3Daware-
ness embedded in multi-view fused features to the 2D feature extractor through
fine-tuning with little computational resources. Our 3D-aware features readily
improve downstream task performance with simple linear probing. In addition,
we find our 3D-aware features exhibit cleaner and more detailed feature maps
compared with the original 2D features (see Sec. D in appendix), while several
concurrent works specifically denoise or sharpen 2D feature maps [18,63].
3 Method
In this section, we introduce our method for fine-tuning 2D foundation models
with 3D-aware features. We present a two-stage pipeline (c.f. Fig 3). In the first
stage, we lift per-view 2D features into a multi-view consistent and 3D-aware
representation. The representation and setup are described in Sec. 3.1. In the
second stage, we use the obtained 3D-aware feature representations as training
dataset to finetune the 2D feature extractor, which is detailed in Sec. 3.2. Last,
we describe the linear probing methodology for feature evaluation in Sec. 3.3.
3.1 Lifting Features to 3D
Lifting semantic 2D features into 3D has been a trend recently and several dif-
ferent options exist (c.f. Sec. 2). For our purposes of using larger amounts ofImproving 2D Feature Representations by 3D-Aware Fine-Tuning 5
Stage I: Lifting Features to 3D (Per-scene) Stage II: 3D-Aware Fine-Tuning (Multi-scene)
Feature Gaussian
Splatting
‚Ä¶ ‚Ä¶
‚Ä¶ Featu Sr pe l aG tta inu gs s ian ‚Ä¶ Featu Sr pe l aG tta inu gssian
‚Ä¶ ‚Ä¶ ‚Ä¶
Feature Gaussian
Splatting
Multi-view images ‚Ä¶ ‚Ä¶
2D 2D

Fig.3:Overallpipeline.Wepresentatwo-stagepipeline.Inthefirststage,welift2D
foundationfeatures(e.g.DINOv2[44])into3D-awarefeaturesbytraining3DGaussian
representationG .Inthesecondstage,weusetherenderedfeaturestofinetunethe2D
i
foundation model Œµ2D. With ‚Üí we denote gradient flow.
scenes as training data for 2D models, the most important aspect is efficiency.
Therepresentationneedsto(1)beabletoefficientlyfitalargenumberofscenes
into3Drepresentationsand(2)haveafastrenderingmechanismforefficientin-
tegration into a fine-tuning loop of a 2D foundation model. Thus, we utilize the
recent advances in 3D Gaussian splatting [33], which enable fast optimization
and real-time rendering. Fig. 4 illustrates how we extend Gaussian splatting to
lift 2D foundation features and we detail the method below.
3DfeatureGaussians.Adaptingtheformulationof3DGaussiansplatting[33],
we define a set of 3D Gaussians as
G ={(¬µ,s,R,Œ±,SH,f) )} , (1)
j 1‚â§j‚â§M
where ¬µ ‚àà R3 is the 3D mean of the Gaussian, S = diag(s) ‚àà R3√ó3 is the
Gaussian scale, R ‚àà R3√ó3 its orientation, Œ± ‚àà R is a per-Gaussian opacity, and
SH a vector of spherical harmonic coefficients, encoding view-dependent color.
The Gaussian covariance matrix is obtained by combining scale and orientation
as Œ£ = RSS‚ä§R‚ä§. In addition to the original parameters, we introduce a per-
Gaussianfeaturevectorf ‚ààRD tostoredistilled2Dfeaturesin3Dspace.Those
feature vectors are rasterized into a 2D feature image with our designed feature
rasterizer. Inspired by the differentiable color rasterizer of Gaussian splatting,
we rasterize the features using point-based Œ±-blending as follows:
i‚àí1
(cid:88) (cid:89)
Flow = f Œ± (1‚àíŒ± ) (2)
i i i
i‚ààN j=1
where N is a set of ordered Gaussians overlapping the pixel, f is the feature of
i
each Gaussian and Œ± is given by evaluating a 2D Gaussian with covariance Œ£
i
multiplied with a learned per-point opacity.
Up-projectingfeatures.Astronglimitationof3DGaussiansasrepresentation
istheirmemoryconsumption.SincetherecanbemillionsofGaussiansperscene,
it is impossible to store, e.g., the 384-dimensional DINO features directly on6 Y. Yue et al.
Color
Rasterizer
ùúÄ2D
Rendered Images GT Images
Feature NC
N
3D Feature Gaussians
Rasterizer
Fig.4: Lifting 2D features into 3D Gaussian representation. We equip each
Gaussianwithalow-dimensionalfeaturevectorf.Werendercolorsusingthesamecolor
rasterizer as Gaussian splatting [33]. We design a feature rasterizer to render a low-
dimensionalfeatureimageFlow,whichissubsequentlyprojectedtoahigh-dimensional
featureimageFhigh usingasimpleCNN.Weuse2DfoundationfeaturesFfrommodel
Œµ2D to supervise the feature learning.
each of the 3D Gaussians. Therefore, to stay memory efficient and keep the fast
rendering process, we opt for storing lower dimensional features f ‚ààRD with D
<< 384 and train a scene-specific pixel-space CNN decoder d : Flow (cid:55)‚Üí Fhigh to
up-project feature images into high-dimensional feature space after rendering.
We analyze the trade-off introduced by this approach in Sec. 4.6.
Optimization. For a given scene, the full 3D Gaussian representation, includ-
ing our distilled features, is obtained using optimization. Let {I } be a
i 1‚â§i‚â§N
set of multi-view images of a scene with corresponding camera parameters,
{F } a corresponding set of feature maps from a 2D feature extractor
i 1‚â§i‚â§N
(e.g. DINOv2 [44]), and rrgb, rfeat rasterization functions that render a set of
GaussiansintoanRGBorfeatureimage,respectively,usingthecameraposeP
i
of image i. Then, we optimize the Gaussian parameters, to optimally represent
images I and feature images F :
i i
N
GÀÜ= argmin (cid:88) Lc(rrgb(G,P ),I )+Lf(d(rfeat(G,P ),F ), (3)
i i i i
{(¬µ,s,R,Œ±,SH,f)i}i=1
where Lc is a pixel-wise l loss combined with a D-SSIM term on RGB images,
1
and Lf is a pixel-wise l loss on feature images. Notably, we only optimize f
1
with gradients coming from Lf (feature images) and the rest of the parameters
only on Lc (RGB loss). This has proven to be essential to obtain a consistent
3D feature representation, as a loss from feature space does not lead to correct
Gaussian mean, covariance and opacity. We speculate that the reason for this
is the missing 3D consistency of the 2D feature extractor. Only through forcing
them into a 3D consistent representation, we make them consistent in return.Improving 2D Feature Representations by 3D-Aware Fine-Tuning 7
Algorithm 1 3D-aware fine-tuning algorithm
Input: Pre-trainedFeatureGaussianrepresentations{G ,...,G },pre-trained2Dfea-
1 K
ture extractor Œµ2D, a set of images {I }N and associated camera poses {P }N .
Œ∏ i i=1 i i=1
Output: Fine-tuned 2D feature extractor Œµ2D.
Œ∏ÀÜ
1: Load G ‚àº{G ,...,G }
1 K
2: while fine-tuning do
3: Sample an image I and camera pose P , i‚àºU{1,N}
i i
4: Retrieve associated feature Gaussian G and CNN decoder d
5: Render Fhigh ‚Üêd(rfeat(G,P ))
i
6: Step Œ∏ by minimizing L(Œµ2D(I ),Fhigh)
Œ∏ i
7: end while
return Œµ2D
Œ∏ÀÜ
3.2 3D-Aware Fine-Tuning
The procedure described in the last section is used to fit 3D feature Gaussian
representationsofK scenes.Thealgorithmof3D-awarefine-tuningisoutlinedin
Algorithm1.Thefine-tuningprocessrequirestrainingpairsoforiginal2Dfeature
mapsand3D-awarefeaturemaps.Sinceitismemory-intensivetosavethefeature
maps,wegeneratethetrainingpairsonthefly.Consideringitistime-consuming
to load each pre-trained Gaussian when rendering features, we pre-load all the
Gaussians into CPU memory. In each step of the training loop, we randomly
sample a view from all the training images, then retrieve its associated feature
Gaussian and scene-specific CNN decoder and finally render features Fhigh as
thegroundtruthfeaturesforfine-tuning.Thefine-tuninglossisal lossbetween
1
Fhigh (resized) and the output features of the fine-tuned 2D feature extractor.
The above design makes the fine-tuning process efficient and keeps memory
consumption under control. Notably, we only need to fine-tune the 2D feature
extractor with a small number of epochs (e.g. 1 epoch for DINOv2 [44]) with
a small learning rate without additionally introducing any network component.
The fine-tuning process is fast and computation-friendly. An analysis of fine-
tuning time is in Sec. 4.6.
3.3 Linear Probing for Downstream Tasks
After fine-tuning on 3D-aware features, we evaluate the emerging features on
a set of standard benchmark downstream tasks. To this end, we train a linear
head on top of the features to solve tasks of semantic segmentation and depth
estimation on several datasets.
Semantic segmentation. A linear layer is trained to predict class logits from
patchtokens.Thelinearlayerproducesalow-resolutionlogitmap,whichisthen
upsampled to full resolution to obtain a segmentation map.
Depth estimation. We concatenate the [CLS] token of the ViT to each patch
token. We divide the depth prediction range into 256 uniformly distributed8 Y. Yue et al.
bins [5] and use a linear normalization. Then a simple linear layer is trained
using a classification loss.
Feature assembly. We concatenate original 2D features with our fine-tuned
features. We observe this is key to preserving the generalization ability of the
original 2D feature extractor while incorporating the 3D awareness in our fine-
tunedfeatures.DifferentstrategiesforfeatureassemblyareevaluatedinSec.4.6.
4 Experiments
4.1 Datasets
Training. We train the feature Gaussians on ScanNet++ [65], which is a large-
scaledatasetof3Dindoorscenescontainingsub-millimeterresolutionlaserscans,
registeredDSLRimages,andcommodityRGB-DstreamsfromiPhone.Wetrain
on the official training split of 230 scenes, which contain 140451 views.
Evaluation. To examine the effectiveness of the fine-tuned features, we con-
duct extensive experiments on downstream 2D tasks including semantic seg-
mentationanddepthestimation.Thereisnodirectcompetitorinourstudyand
we instead focus on whether our 3D-aware fine-tuning can bring performance
gains compared with the standard 2D feature extractor. We conduct most of
the experiments with DINOv2 [44] while also demonstrating the universality of
our approach with other vision models in Sec. 4.5. We first evaluate on Scan-
Net++ [65] validation set, which contains 50 scenes with 30638 images. Then
we move on to other indoor datasets ScanNet [9] and NYUd [55]. which have a
similardatadistributionwithScanNet++butwerecapturedwithdifferentsen-
sors. To investigate the generalization ability of the fine-tuned features, we also
perform out-of-domain evaluation on generally distributed datasets including
ADE20k [67], Pascal VOC [17] and the outdoor dataset KITTI [19].
4.2 Implementation Details
Feature Gaussians.WewrotecustomCUDAkernelsforfeaturerasterization.
Each Gaussian is initialized with a random feature vector with a dimension of
64.Weimplementtheup-projectingCNNwithasingleconvolutionallayerwith
akernelsizeof3√ó3.WetrainthefeatureGaussiansofeachscenefornovelview
synthesis and feature rendering jointly for 30000 iterations.
Fine-tuning. We finetune DINOv2 small with a feature dimension of 384 with
a batch size of 2 with a learning rate of 1e-5 for 1 epoch. We use horizontal flip
as data augmentation. We use the AdamW [40] optimizer with a weight decay
factor 1e-4. The fine-tuning on a single Nvidia Tesla A100 takes 8.5 hours.
Linear probing. We follow the linear probing protocol with DINOv2 [44] to
ensureafaircomparison.Forsemanticsegmentation,wetrainthelinearlayerfor
40K iterations with 8 GPUs. For depth estimation, we train the linear layer for
38400 iterations with 8 GPUs. In addition, we use the same data augmentation
and learning rate schedule with DINOv2.Improving 2D Feature Representations by 3D-Aware Fine-Tuning 9
4.3 Within-domain Evaluation
Quantitative comparison. We demonstrate the effectiveness of incorporating
our 3D-aware features on downstream semantic segmentation (see Tab. 1) and
depth estimation task (see Tab. 2) for indoor scenes. For semantic segmentation
task, our 3D aware features consistently improve DINOv2 features, achieving a
significantperformancegainof2.6%,2.0%mIoU,and1.2%onScanNet++[65],
NYUv2 [55] and ScanNet [9] datasets, respectively. Our 3D-aware DINOv2 fea-
tures also improve performance on the depth estimation task. In particular,
our enhanced features consistently reduce the RMSE across datasets by achiev-
ing 0.34 vs. 0.37 (DINOv2) for ScanNet++ [65], 0.42 vs. 0.44 (DINOv2) for
NYUv2 [55] and 0.29 vs. 0.31 (DINOv2) for ScanNet [9] datasets.
Qualitative comparison. We qualitatively show the benefits of 3D-aware fea-
tures in Fig. 5 and Fig. 6. We observe the improvements are mainly reflected
in two aspects: (1) cleaner segmentation/depth estimation in homogeneous or
texturelessregions,e.g.onwallsandboards,and(2)betterpredictionwithfine-
grained details, e.g. on legs of chairs or tables. For (1), during the lifting of
2D features to 3D, features from multiple views are aggregated into a holistic
representation, thus information from one view implicitly complements other
views.Wehypothesizethatsuchmulti-viewawarenessistransferredtoDINOv2
through fine-tuning. By contrast, standard DINOv2 struggles to infer accurate
segmentation or depth from a single image when with ambiguity, thus leading
tonoisyprediction.For(2),inourfeatureliftingprocess,wetrainthegeometry
properties (e.g. position and opacity) of Gaussians with RGB color as supervi-
sion.TheRGBguidancehelpsfeatureGaussianslearndetailed3Dstructureand
renderhigh-resolutionfeaturemaps(c.f.Fig.1(c)).During thefine-tuningpro-
cess,themodellearnstoestimatefine-grainedfeaturesofobjects(c.f.Fig.1(d)
vs. (b)), which is helpful for capturing detailed structure in downstream tasks.
Table 1: Semantic segmentation scores on indoor datasets. 3D-aware fine-
tuning consistently leads to improved performance on semantic segmentation in com-
parison to standard DINOv2 across different indoor datasets.
ScanNet++[65] NYUv2[55] ScanNet[9]
Method mAcc(‚Üë) mIoU(‚Üë) aAcc(‚Üë) mAcc(‚Üë) mIoU(‚Üë) aAcc(‚Üë) mAcc(‚Üë) mIoU(‚Üë) aAcc(‚Üë)
DINOv2[44] 40.84 30.19 80.25 76.88 65.55 82.43 55.86 43.6 73.54
+Ours 43.4 32.76 83.54 80.52 67.5 83.37 58.32 44.84 74.37
Table2:Depthestimationscoresonindoordatasets.3D-awarefine-tuningcon-
sistentlyleadstoimprovedperformanceondepthestimationincomparisontostandard
DINOv2 across different indoor datasets.
ScanNet++[65] NYUv2[55] ScanNet[9]
Method RMSE(‚Üì) Rel(‚Üì) RMSE(‚Üì) Rel(‚Üì) RMSE(‚Üì) Rel(‚Üì)
DINOv2[44] 0.3742 0.2836 0.4423 0.1392 0.3089 0.1557
+Ours 0.3361 0.2401 0.4198 0.1300 0.2921 0.145910 Y. Yue et al.
+
+
te
N
n
a
c
S
2
v
U
Y
N
te
N
n
a
c
S
Input DINOv2 Ours Ground Truth
Fig.5: Semantic segmentation on indoor datasets with linear probing. In-
corporating our 3D-aware fine-tuned features helps obtain cleaner and more compact
segmentation results, especially for detailed structures and in homogeneous regions.
4.4 Out-of-domain Evaluation
WetrainfeatureGaussiansandfine-tuneDINOv2onScanNet++,adatasetthat
contains only indoor scenes with the usual content, e.g. tables, chairs and other
indoor furniture. We want to analyze how the gains obtained in this setting
generalize to other domains, e.g. outdoor scenes. For semantic segmentation,
we conduct linear probing on ADE20k [67] and Pascal VOC [17]. For depth
estimation, we conduct linear probing on KITTI [19].Improving 2D Feature Representations by 3D-Aware Fine-Tuning 11
+
+
te
N
n
a
c
S
2
v
U
Y
N
te
N
n
a
c
S
Input DINOv2 Ours Ground Truth
Fig.6:Depthestimationonindoordatasetswithlinearprobing.Incorporating
our3D-awarefine-tunedfeatureshelpsobtaincleanerdepthintexturelessregionsand
more detailed depth on fine-grained structures, e.g. legs of tables or chairs.
Quantitative comparison. We observe that the improvement brought by 3D-
aware features is generalizable to out-of-domain challenging datasets and also
outdoor driving scenes, although to a smaller degree. As shown in Tab. 3, for
semantic segmentation task, incorporating our 3D-aware features brings a gain
of1.6%mIoUontheADE20k[67]andagainof1.2%mIoUonPascalVOC[17]
overstandardDINOv2features.Furthermore,wealsocompareourperformance
on urban scene dataset KITTI [19] for depth estimation and observe our 3D-
aware features help to reduce RMSE from 3.03 (DINOv2) to 2.91.12 Y. Yue et al.
Table 3: Quantitative performance on out-of-domain datasets.3D-awarefine-
tuning noticeably improves semantic segmentation on ADE20k and Pascal VOC and
depth estimation on KITTI, demonstrating the transferability of the fine-tuned fea-
tures, even under a significant domain gap.
ADE20k[67] PascalVOC[17] KITTI[19]
Method mAcc(‚Üë) mIoU(‚Üë) aAcc(‚Üë) mAcc(‚Üë) mIoU(‚Üë) aAcc(‚Üë) RMSE(‚Üì) Rel(‚Üì)
DINOv2[44] 56.74 44.28 79.73 90.61 81.14 95.72 3.03 0.10
+Ours 58.71 45.93 81.05 91.04 82.35 96.14 2.91 0.09
Table 4: Generalization on other 2D vision models. Our 3D-aware fine-tuning
applies to other 2D vision models and readily improves their performance.
DINOv2-reg CLIP MAE DeiT-III
mIoU(‚Üë) RMSE(‚Üì) mIoU(‚Üë) RMSE(‚Üì) mIoU(‚Üë) RMSE(‚Üì) mIoU(‚Üë) RMSE(‚Üì)
Original 30.92 0.4190 25.61 0.4324 17.19 0.4855 18.62 0.4350
+Ours 33.39 0.3824 28.82 0.3960 20.27 0.4795 22.98 0.3820
Qualitative comparison. We show qualitative comparison on out-of-domain
datasets in Fig. 7. We observe similar improvements as in the within-domain
datasets. Even though the 3D-aware fine-tuning is only conducted on indoor
dataset ScanNet++, the fine-tuned features exhibit transferability to improve
segmentation results for the detailed structures of common objects like bicycle
and animal, and help achieve more compact segmentation of objects like tree,
building and pillar. On depth estimation, the incorporated 3D-aware features
are helpful in capturing the detailed structure of trees.
4.5 Generalization to Other Vision Models
We conduct experiments on more vision models (DINOv2-reg [10], CLIP [48],
MAE [26], DeiT-III [58]) to prove the universality of our method. We show
the linear probing results of semantic segmentation and depth estimation on
ScanNet++ validation set in Tab. 4. Our method consistently improves all the
models. We also visualize the features in Fig. 2. Note that there is little visual
difference between the original MAE features and the fined-tuned features but
our method still improves them.
4.6 Ablation Studies and Analysis
We conduct ablation studies on semantic segmentation on NYUv2 dataset with
DINOv2.
Feature dimension of each Gaussian. We attach a low-dimensional feature
vector with each Gaussian and then up-project it to the same space with DI-
NOv2. Tab. 5 indicates that increasing the feature dimension from 32 to 64 will
improvetheperformanceoffine-tunedDINOv2withanacceptablehighermem-
ory and longer training time. Increasing the feature dimension further to 128 isImproving 2D Feature Representations by 3D-Aware Fine-Tuning 13
DINOv2 Ours
C
O DINOv2 Ours
V
la
c
s
a
P DINOv2 Ours
DINOv2 Ours
k
0
2 E DINOv2 Ours
D
A
DINOv2 Ours DINOv2 Ours DINOv2 Ours
IT
T
IK
DINOv2 Ours
Fig.7: Results on out-of-domain datasets with linear probing. Our 3D-aware
fine-tuned features help achieve better segmentation and capture detailed structure.
not feasible in our hardware due to the large memory consumption. We chose
a feature dimension of 64 as a good compromise between model performance,
memory consumption, as well as training time.
Featureassemblystrategy.Westudydifferentstrategiestoassemblethefine-
tuned features with the original DINOv2 features in Tab. 7. We explore simple
channel-wise adding and concatenation. Alternatively, we first concatenate the
fine-tuned features with the original DINOv2 features then use a liner layer to
fusethemtothesamefeaturespaceofDINOv2.Weobservesimpleconcatenation
works well in incorporating learned 3D-aware features while preserving original
generalization ability.
Fine-tuning epochs. We finetune DINOv2 using the features rendered by the
pre-trainedfeatureGaussians.Tab.8suggeststhatasingleepochissufficientto
transfer the 3D awareness to DINOv2 and more epochs may harm the model‚Äôs
generalization ability.
Fine-tuning vs. adapter.Besidesdirectlyfine-tuningDINOv2,weexplorean
alternative strategy where we keep DINOv2 frozen and introduce an adapter on
topofthat.TheadapterisasingleSwinTransformerblock[39].Weobservethe14 Y. Yue et al.
Table5:Ablationstudyonfeaturedimension Table 6: Ablation study on
of3DGaussian.Increasingfeaturedimensionsim- fine-tuning vs. adapter. An
proves performance at the cost of larger memory adapterisatinynetworkplugged
consumption and longer training time. intothefrozenDINOv2features.
FeaturedimensionAveragememory(MB)AverageTrainingtime(h)mAcc(‚Üë)mIoU(‚Üë)aAcc(‚Üë) Strategy mAcc(‚Üë) mIoU(‚Üë) aAcc(‚Üë)
3 62 4 3 47 90 5 1 1. .3 6 87 08 ..7 57 2 6 67 7. .1 55 8 83 3. .34 74 Fine-tuning 91.04 82.35 96.14
128 750 2.5 - - - Adapter 90.97 82.02 95.96
Table 7: Ablation study on fea- Table 8: Ablation study on fine-tuning
ture assembly. We study different epochs. We find fine-tuning with a single
strategies to assemble fine-tuned fea- epoch with 8.5 hours is sufficient to achieve
tures with the original DINOv2. good performance.
Strategy mAcc(‚Üë) mIoU(‚Üë) aAcc(‚Üë) Epochs Fine-tun.time(h) mAcc(‚Üë) mIoU(‚Üë) aAcc(‚Üë)
Adding 77.97 66.0 82.85 1 8.5 80.52 67.5 83.37
Linearfusion 78.22 66.39 82.89 2 17 78.72 67.25 83.54
Concatenation 80.52 67.5 83.37 3 25.5 79.5 67.18 83.24
adaptercanachievecomparableperformancewithfine-tuning(Tab.6),however,
with longer training time. We stick with the fine-tuning strategy for simplicity
without introducing any additional network component.
Limitations and discussion.Ourworkmakesaninitialsteptotransfermulti-
viewconsistentand3D-awarefeaturesencodedbya3DGaussianrepresentation
to 2D foundation model via fine-tuning. We demonstrate the 3D-aware features
are helpful for downstream tasks. However, we still need the original features to
keep the generalization ability. We attribute this to the limited diversity of our
3Dtrainingdata(onlyasingleindoordataset)andhypothesizethatthiscanbe
remedied by fine-tuning on larger-scale data.
5 Conclusion
In this work, we present a method to inject 3D awareness into 2D foundation
models. We first lift 2D foundation features into a 3D Gaussian representation
andthenusetherenderedmulti-viewconsistentand3D-awarefeaturestointurn
fine-tune the 2D foundation model. Our experiments show that incorporating
the fine-tuned features readily leads to improved performance on both semantic
and geometric tasks through simple linear probing. Although we only conduct
the 3D-aware fine-tuning on a single dataset ScanNet++, we demonstrate the
learned 3D awareness is transferable across a variety of datasets in different
domains. We hope our work inspires future research to consider equipping 2D
foundation models with 3D-aware understanding.
Acknowledgements Francis Engelmann is partially supported by an ETH AI
CenterpostdoctoralresearchfellowshipandanETHZurichCareerSeedAward.
This project was also partially supported by Saarbr√ºcken Research Center for
Visual Computing, Interaction and AI.Improving 2D Feature Representations by 3D-Aware Fine-Tuning 15
References
1. Amir,S.,Gandelsman,Y.,Bagon,S.,Dekel,T.:DeepViTFeaturesasDenseVisual
Descriptors. In: European Conference on Computer Vision (ECCV) Workshops
(2022) 2
2. Bachmann,R.,Mizrahi,D.,Atanov,A.,Zamir,A.:MultiMAE:Multi-modalMulti-
taskMaskedAutoencoders.In:EuropeanConferenceonComputerVision(ECCV)
(2022) 4
3. Bao, H., Dong, L., Piao, S., Wei, F.: BEiT: BERT Pre-training of Image Trans-
formers. In: International Conference on Learning Representations (ICLR) (2022)
3
4. Bengio,Y.,Courville,A.,Vincent,P.:RepresentationLearning:AReviewandNew
Perspectives. Transactions on Pattern Analysis and Machine Intelligence (PAMI)
(2013) 3
5. Bhat, S.F., Alhashim, I., Wonka, P.: Adabins: Depth Estimation Using Adaptive
Bins. In: International Conference on Computer Vision and Pattern Recognition
(CVPR) (2021) 8
6. Caron,M.,Misra,I.,Mairal,J.,Goyal,P.,Bojanowski,P.,Joulin,A.:Unsupervised
LearningofVisualFeaturesbyContrastingClusterAssignments.In:International
Conference on Neural Information Processing Systems (NeurIPS) (2020) 3
7. Caron, M., Touvron, H., Misra, I., J√©gou, H., Mairal, J., Bojanowski, P., Joulin,
A.: Emerging Properties in Self-Supervised Vision Transformers. In: International
Conference on Computer Vision (ICCV) (2021) 2, 3
8. Chen,M.,Radford,A.,Child,R.,Wu,J.,Jun,H.,Luan,D.,Sutskever,I.:Gener-
ative Pretraining From Pixels. In: International Conference on Machine Learning
(ICML) (2020) 3
9. Dai, A., Chang, A.X., Savva, M., Halber, M., Funkhouser, T., Nie√üner, M.: Scan-
Net:Richly-Annotated3DReconstructionsofIndoorScenes.In:InternationalCon-
ference on Computer Vision and Pattern Recognition (CVPR) (2017) 8, 9, 23
10. Darcet,T.,Oquab,M.,Mairal,J.,Bojanowski,P.:VisionTransformersNeedReg-
isters.In:InternationalConferenceonLearningRepresentations(ICLR)(2024) 3,
12
11. Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of Deep
Bidirectional Transformers for Language Understanding. NAACL (2018) 3
12. Doersch,C.,Gupta,A.,Efros,A.A.:UnsupervisedVisualRepresentationLearning
by Context Prediction. In: International Conference on Computer Vision (ICCV)
(2015) 3
13. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An Image Is Worth
16x16Words:TransformersforImageRecognitionatScale.In:InternationalCon-
ference on Learning Representations (ICLR) (2020) 2, 4
14. Dosovitskiy,A.,Springenberg,J.T.,Riedmiller,M.,Brox,T.:DiscriminativeUnsu-
pervisedFeatureLearningWithConvolutionalNeuralNetworks.In:International
Conference on Neural Information Processing Systems (NeurIPS) (2014) 3
15. El Banani, M., Raj, A., Maninis, K.K., Kar, A., Li, Y., Rubinstein, M., Sun,
D., Guibas, L., Johnson, J., Jampani, V.: Probing the 3D Awareness of Visual
FoundationModels.In:InternationalConferenceonComputerVisionandPattern
Recognition (CVPR) (2024) 2
16. Engelmann,F.,Manhardt,F.,Niemeyer,M.,Tateno,K.,Tombari,F.:OpenNeRF:
Open Set 3D Neural Scene Segmentation with Pixel-Wise Features and Rendered16 Y. Yue et al.
Novel Views. In: International Conference on Learning Representations (ICLR)
(2024) 3
17. Everingham,M.,Eslami,S.A.,VanGool,L.,Williams,C.K.,Winn,J.,Zisserman,
A.: The Pascal Visual Object Classes Challenge: A Retrospective. International
Journal of Computer Vision (2015) 8, 10, 11, 12
18. Fu,S.,Hamilton,M.,Brandt,L.,Feldman,A.,Zhang,Z.,Freeman,W.T.:FeatUp:
A Model-Agnostic Framework for Features at Any Resolution. In: International
Conference on Learning Representations (ICLR) (2024) 4
19. Geiger, A., Lenz, P., Stiller, C., Urtasun, R.: Vision Meets Robotics: The Kitti
Dataset. The International Journal of Robotics Research (2013) 8, 10, 11, 12, 20
20. Ghiasi, G., Gu, X., Cui, Y., Lin, T.Y.: Scaling Open-Vocabulary Image Segmen-
tation With Image-Level Labels. In: European Conference on Computer Vision
(ECCV) (2022) 3
21. Gidaris, S., Singh, P., Komodakis, N.: Unsupervised Representation Learning by
Predicting Image Rotations. In: International Conference on Learning Represen-
tations (ICLR) (2018) 3
22. Goel,R.,Sirikonda,D.,Saini,S.,Narayanan,P.:InteractiveSegmentationofRadi-
anceFields.In:InternationalConferenceonComputerVisionandPatternRecog-
nition (CVPR) (2023) 3
23. Grill,J.B.,Strub,F.,Altch√©,F.,Tallec,C.,Richemond,P.,Buchatskaya,E.,Doer-
sch,C.,AvilaPires,B.,Guo,Z.,GheshlaghiAzar,M.,etal.:BootstrapYourOwn
Latent-aNewApproachtoSelf-SupervisedLearning.In:InternationalConference
on Neural Information Processing Systems (NeurIPS) (2020) 3
24. Ha,H.,Song,S.:SemanticAbstraction:Open-world3DSceneUnderstandingFrom
2D Vision-Language Models. In: Conference on Robot Learning (CoRL) (2022) 4
25. Hadsell, R., Chopra, S., LeCun, Y.: Dimensionality Reduction by Learning an
InvariantMapping.In:InternationalConferenceonComputerVisionandPattern
Recognition (CVPR) (2006) 3
26. He, K., Chen, X., Xie, S., Li, Y., Doll√°r, P., Girshick, R.: Masked Autoencoders
Are Scalable Vision Learners. In: International Conference on Computer Vision
and Pattern Recognition (CVPR) (2022) 2, 3, 4, 12
27. He, K., Fan, H., Wu, Y., Xie, S., Girshick, R.: Momentum Contrast for Unsuper-
vised Visual Representation Learning. In: International Conference on Computer
Vision and Pattern Recognition (CVPR) (2020) 3
28. Hou, J., Dai, X., He, Z., Dai, A., Nie√üner, M.: Mask3D: Pre-training 2D Vision
TransformersbyLearningMasked3DPriors.In:InternationalConferenceonCom-
puter Vision and Pattern Recognition (CVPR) (2023) 4
29. Hou, J., Xie, S., Graham, B., Dai, A., Nie√üner, M.: Pri3D: Can 3D Priors Help
2D Representation Learning? In: International Conference on Computer Vision
(ICCV) (2021) 4
30. Huang,R.,Peng,S.,Takmaz,A.,Tombari,F.,Pollefeys,M.,Song,S.,Huang,G.,
Engelmann,F.:Segment3D:LearningFine-GrainedClass-Agnostic3DSegmenta-
tion without Manual Labels. European Conference on Computer Vision (ECCV)
(2024) 4
31. Jatavallabhula, K.M., Kuwajerwala, A., Gu, Q., Omama, M., Chen, T., Li, S.,
Iyer, G., Saryazdi, S., Keetha, N., Tewari, A., et al.: ConceptFusion: Open-set
Multimodal 3D Mapping. Robotics: Science and Systems (RSS) (2023) 4
32. Ke, B., Obukhov, A., Huang, S., Metzger, N., Daudt, R.C., Schindler, K.: Re-
purposingDiffusion-BasedImageGeneratorsforMonocularDepthEstimation.In:
International Conference on Computer Vision and Pattern Recognition (CVPR)
(2024) 2Improving 2D Feature Representations by 3D-Aware Fine-Tuning 17
33. Kerbl, B., Kopanas, G., Leimk√ºhler, T., Drettakis, G.: 3D Gaussian Splatting for
Real-Time Radiance Field Rendering. ACM Transactions on Graphics (2023) 2,
4, 5, 6
34. Kerr, J., Kim, C.M., Goldberg, K., Kanazawa, A., Tancik, M.: LERF: Language
Embedded Radiance Fields. In: International Conference on Computer Vision
(ICCV) (2023) 3
35. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead, S., Berg, A.C., Lo, W.Y., et al.: Segment Anything. In: International
Conference on Computer Vision (ICCV) (2023) 2
36. Kobayashi, S., Matsumoto, E., Sitzmann, V.: Decomposing Nerf for Editing via
FeatureFieldDistillation.In:InternationalConferenceonNeuralInformationPro-
cessing Systems (NeurIPS) (2022) 3
37. Li, B., Weinberger, K.Q., Belongie, S., Koltun, V., Ranftl, R.: Language-driven
SemanticSegmentation.In:InternationalConferenceonLearningRepresentations
(ICLR) (2022) 3
38. Li, F., Zhang, H., Xu, H., Liu, S., Zhang, L., Ni, L.M., Shum, H.Y.: Mask DINO:
Towards a Unified Transformer-Based Framework for Object Detection and Seg-
mentation. In: International Conference on Computer Vision and Pattern Recog-
nition (CVPR) (2023) 2
39. Liu,Z.,Lin,Y.,Cao,Y.,Hu,H.,Wei,Y.,Zhang,Z.,Lin,S.,Guo,B.:SwinTrans-
former:HierarchicalVisionTransformerUsingShiftedWindows.In:International
Conference on Computer Vision (ICCV) (2021) 13
40. Loshchilov, I., Hutter, F.: Decoupled Weight Decay Regularization. In: Interna-
tional Conference on Learning Representations (ICLR) (2019) 8
41. Mazur,K.,Sucar,E.,Davison,A.J.:Feature-realisticNeuralFusionforReal-time,
OpenSetSceneUnderstanding.In:InternationalConferenceonRoboticsandAu-
tomation (ICRA) (2023) 4
42. Mildenhall, B., Srinivasan, P.P., Tancik, M., Barron, J.T., Ramamoorthi, R., Ng,
R.: NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In:
European Conference on Computer Vision (ECCV) (2020) 3
43. Noroozi,M.,Favaro,P.:UnsupervisedLearningofVisualRepresentationsbySolv-
ingJigsawPuzzles.In:EuropeanConferenceonComputerVision(ECCV)(2016)
3
44. Oquab, M., Darcet, T., Moutakanni, T., Vo, H., Szafraniec, M., Khalidov, V.,
Fernandez, P., Haziza, D., Massa, F., El-Nouby, A., et al.: DINOv2: Learning
Robust Visual Features Without Supervision. Transactions on Machine Learning
Research (2023) 1, 2, 3, 5, 6, 7, 8, 9, 12, 19, 20
45. Pathak,D.,Girshick,R.,Doll√°r,P.,Darrell,T.,Hariharan,B.:LearningFeatures
byWatchingObjectsMove.In:InternationalConferenceonComputerVisionand
Pattern Recognition (CVPR) (2017) 3
46. Peng, S., Genova, K., Jiang, C., Tagliasacchi, A., Pollefeys, M., Funkhouser, T.:
OpenScene: 3D Scene Understanding with Open Vocabularies. In: International
Conference on Computer Vision and Pattern Recognition (CVPR) (2023) 4
47. Qin,M.,Li,W.,Zhou,J.,Wang,H.,Pfister,H.:LangSplat:3DLanguageGaussian
Splatting.In:InternationalConferenceonComputerVisionandPatternRecogni-
tion (CVPR) (2024) 4
48. Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,G.,Agarwal,S.,Sastry,G.,
Askell,A.,Mishkin,P.,Clark,J.,etal.:LearningTransferableVisualModelsFrom
NaturalLanguageSupervision.In:InternationalConferenceonMachineLearning
(ICML) (2021) 3, 1218 Y. Yue et al.
49. Ranftl,R.,Bochkovskiy,A.,Koltun,V.:VisionTransformersforDensePrediction.
In: International Conference on Computer Vision (ICCV) (2021) 20
50. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-Resolution
Image Synthesis With Latent Diffusion Models. In: International Conference on
Computer Vision and Pattern Recognition (CVPR) (2022) 2
51. Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z.,
Karpathy, A., Khosla, A., Bernstein, M., et al.: Imagenet large scale visual recog-
nition challenge. International journal of computer vision (2015) 20
52. Saxena, S., Herrmann, C., Hur, J., Kar, A., Norouzi, M., Sun, D., Fleet, D.J.:
The Surprising Effectiveness of Diffusion Models for Optical Flow and Monocular
DepthEstimation.In:InternationalConferenceonNeuralInformationProcessing
Systems (NeurIPS) (2024) 2
53. Shen, W., Yang, G., Yu, A., Wong, J., Kaelbling, L.P., Isola, P.: Distilled Feature
FieldsEnableFew-ShotLanguage-GuidedManipulation.In:ConferenceonRobot
Learning (CoRL) (2023) 4
54. Shi,J.C.,Wang,M.,Duan,H.B.,Guan,S.H.:LanguageEmbedded3DGaussians
for Open-Vocabulary Scene Understanding. In: International Conference on Com-
puter Vision and Pattern Recognition (CVPR) (2024) 4
55. Silberman, N., Hoiem, D., Kohli, P., Fergus, R.: Indoor Segmentation and Sup-
portInferenceFromRGBDImages.In:EuropeanConferenceonComputerVision
(ECCV) (2012) 8, 9, 19, 20, 23
56. Takmaz, A., Fedele, E., Sumner, R.W., Pollefeys, M., Tombari, F., Engelmann,
F.: OpenMask3D: Open-Vocabulary 3D Instance Segmentation. In: International
Conference on Neural Information Processing Systems (NeurIPS) (2023) 4
57. Tan, H., Wu, S., Pi, J.: Semantic Diffusion Network for Semantic Segmentation.
In:InternationalConferenceonNeuralInformationProcessingSystems(NeurIPS)
(2022) 2
58. Touvron, H., Cord, M., J√©gou, H.: Deit III: Revenge of the ViT. In: European
Conference on Computer Vision (ECCV) (2022) 3, 12
59. Tschernezki,V.,Laina,I.,Larlus,D.,Vedaldi,A.:NeuralFeatureFusionFields:3D
Distillation of Self-Supervised 2D Image Representations. In: International Con-
ference on 3D Vision (3DV) (2022) 3
60. Wang, X., Gupta, A.: Unsupervised Learning of Visual Representations Using
Videos. In: International Conference on Computer Vision (ICCV) (2015) 3
61. Weder, S., Blum, H., Engelmann, F., Pollefeys, M.: LabelMaker: Automatic Se-
mantic Label Generation from RGB-D Trajectories. In: International Conference
on 3D Vision (3DV) (2024) 4
62. Weinzaepfel,P.,Leroy,V.,Lucas,T.,Br√©gier,R.,Cabon,Y.,Arora,V.,Antsfeld,
L., Chidlovskii, B., Csurka, G., Revaud, J.: CroCo: Self-Supervised Pre-training
for 3D Vision Tasks by Cross-View Completion. In: International Conference on
Neural Information Processing Systems (NeurIPS) (2022) 4
63. Yang,J.,Luo,K.Z.,Li,J.,Weinberger,K.Q.,Tian,Y.,Wang,Y.:DenoisingVision
Transformers. In: European Conference on Computer Vision (ECCV) (2024) 4
64. Yang, L., Kang, B., Huang, Z., Xu, X., Feng, J., Zhao, H.: Depth Anything: Un-
leashingthePowerofLarge-ScaleUnlabeledData.In:InternationalConferenceon
Computer Vision and Pattern Recognition (CVPR) (2024) 2
65. Yeshwanth, C., Liu, Y.C., Nie√üner, M., Dai, A.: ScanNet++: A High-Fidelity
Dataset of 3D Indoor Scenes. In: International Conference on Computer Vision
(ICCV) (2023) 8, 9, 19, 20, 22Improving 2D Feature Representations by 3D-Aware Fine-Tuning 19
66. Zhang, J., Herrmann, C., Hur, J., Polania Cabrera, L., Jampani, V., Sun, D.,
Yang, M.H.: A Tale of Two Features: Stable Diffusion Complements DINO for
Zero-Shot Semantic Correspondence. In: International Conference on Neural In-
formation Processing Systems (NeurIPS) (2023) 2
67. Zhou, B., Zhao, H., Puig, X., Fidler, S., Barriuso, A., Torralba, A.: Scene Parsing
Through ade20K Dataset. In: International Conference on Computer Vision and
Pattern Recognition (CVPR) (2017) 8, 10, 11, 12, 19
68. Zhou,S.,Chang,H.,Jiang,S.,Fan,Z.,Zhu,Z.,Xu,D.,Chari,P.,You,S.,Wang,
Z., Kadambi, A.: Feature 3DGS: Supercharging 3D Gaussian Splatting to Enable
Distilled Feature Fields. In: International Conference on Computer Vision and
Pattern Recognition (CVPR) (2024) 4
In the appendix, we provide (1) experiments with other DINOv2 ViT vari-
ants (Appendix A) (2) experiments on more tasks and heads (Appendix B) (3)
experiments on impact of feature dimensions for linear probing (Appendix C)
(4) more visualization and K-Means clustering of features (Appendix D).
A Experiments With More DINOv2 ViT Variants
To demonstrate that the effectiveness of our 3D-aware fine-tuning is agnostic
to DINOv2 architecture variants, we conduct additional experiments using the
ViT-Base architecture with a feature dimension of 768. We show the results
of semantic segmentation and depth estimation across multiple in-domain and
out-of-domaindatasetsinTab.9andTab.10,respectively.Weobserveasimilar
trend of improvement with the ViT-B architecture. For example, on the fine-
tuning dataset ScanNet++, incorporating our fine-tuned features brings an im-
provementof3.47%mIoUonsemanticsegmentationandreduces0.03RMSEon
depth estimation. On other indoor datasets NYUv2 and out-of-domain dataset
ADE20k, our 3D-aware fine-tuning consistently improves the original DINOv2.
Thisexperimentindicatesthatour3D-awarefine-tuningisapplicabletodifferent
ViT architectures and readily benefits downstream tasks.
Table 9: Results of ViT variants on semantic segmentation. Our 3D-aware
fine-tuning yields consistent improvements on semantic segmentation for both ViT-S
and ViT-B architectures.
ScanNet++[65] NYUv2[55] ADE20k[67]
Method Arch. mAcc(‚Üë) mIoU(‚Üë) aAcc(‚Üë) mAcc(‚Üë) mIoU(‚Üë) aAcc(‚Üë) mAcc(‚Üë) mIoU(‚Üë) aAcc(‚Üë)
DINOv2[44] ViT-S 40.84 30.19 80.25 76.88 65.55 82.43 56.74 44.28 79.73
+Ours ViT-S 43.4 32.76 83.54 80.52 67.5 83.37 58.71 45.93 81.05
DINOv2[44] ViT-B 42.99 32.72 82.05 80.56 68.45 84.03 59.11 47.16 80.79
+Ours ViT-B 46.35 36.19 85.5 80.58 70.56 85.72 62.18 49.5 82.5220 Y. Yue et al.
Table 10: Results of ViT variants on depth estimation. Our 3D-aware fine-
tuningyieldsconsistentimprovementsondepthsegmentationforbothViT-SandViT-
B architectures.
ScanNet++[65] NYUv2[55] KITTI[19]
Method Arch. RMSE(‚Üì) Rel(‚Üì) RMSE(‚Üì) Rel(‚Üì) RMSE(‚Üì) Rel(‚Üì)
DINOv2[44] ViT-S 0.3742 0.2836 0.4423 0.1392 3.0322 0.0965
+Ours ViT-S 0.3361 0.2401 0.4198 0.1300 2.9125 0.0891
DINOv2[44] ViT-B 0.3439 0.2576 0.3986 0.1218 2.9071 0.095
+Ours ViT-B 0.3174 0.2324 0.3802 0.1171 2.7923 0.0897
Table 11: Results on image Table 12: Results with DPT head on depth
classification. Our features do estimation. Beyond linear probing, we evaluate
not improve image classification with DPT head for depth estimation and observe
results. consistent improvement.
Method Acc.(‚Üë) Method RMSE(‚Üì) Rel(‚Üì)
DINOv2[44] 80.02 DINOv2[44] 0.3027 0.2149
+Ours 80.00 +Ours 0.2830 0.1936
B Experiments on More Tasks and Heads
Image classification. We additionally evaluate our approach with DINOv2
smallonimageclassification.WetrainalinearprobingonImageNet-1K[51]for
12500 iterations on a single GPU. As shown in tab. 11, our features do not im-
prove image classification results. This is expected as classification mainly relies
on CLS token of ViT while our method aims to improve image patch features.
DPT head. Beyond linear probing, we evaluate DINOv2 small with the DPT
head [49] for depth estimation on ScanNet++. In comparison with the linear
probingresults(Tab.2inthemainpaper),theDPTheadimprovesbothresults
and our features are still helpful in this setup (see Tab. 12). This demonstrates
thatimprovementbroughtthe3D-awarefeaturesisnotlimitedtolinearprobing
but also applicable to more complex heads.
C Experiments on Feature Dimensions
We concatenate original 2D features with our fine-tuned features, which will
introduceincreasedfeaturedimension.Inthisexperiment,wecomparewithDI-
NOv2 small with duplicate features for linear probing of semantic segmentation
and depth estimation on ScanNet++. As shown in Tab. 13, simply duplication
‚óã only leads to little improvement compared with incorporating our fine-tuned
2
features ‚óã. This verifies that it is not the number of feature dimensions that
3
leads to improvement.Improving 2D Feature Representations by 3D-Aware Fine-Tuning 21
Table 13: Results of duplicating DINOv2 features for linear probing. We
verify that it is not the number of feature dimensions that leads to improvement by
showing that simple duplication of original features does not help.
F mIoU (‚Üë) RMSE (‚Üì)
dim
‚óã DINOv2 384 30.19 0.3742
1
‚óã DINOv2 √ó 2 768 30.31 0.3676
2
‚óã 3 DINOv2 + Ours 768 32.76 0.3361
D Visual Analysis of Features
We train feature Gaussians and conduct 3D-aware fine-tuning on ScanNet++.
In Fig 8, we visualize the features rendered by pre-trained feature Gaussians
(4th column), features of DINOv2 (2nd column) and our fine-tuned features (3rd
column).Thecolorsoffeaturesinallvisualizationsareproducedusingprinciple
componentanalysis(PCA).ThestandardDINOv2featuressufferfromnoiseand
rough object boundaries. After lifting those features to 3D by training feature
Gaussians, we observe the rendered features enjoy cleaner and sharper object
boundaries. We then fine-tune DINOv2 using those rendered features, which
results in compact and clean feature representations.
Although the fine-tuning is only conducted on ScanNet++, we observe the
resultingfine-tunedDINOv2cangeneralizetootherindoordatasets(e.g.NYUv2
andScanNet)andproducescleanerfeaturemapsandmorepronouncedstructure
details (Fig. 9). Similar patterns can also be found in out-of-domain datasets
(e.g. Pascal VOC, ADE20k and KITTI), as shown in Fig. 10. Visualizations of
these feature representations indicate that 3D-aware fine-tuning is helpful and
transferable. We observe the improvements are mainly reflected in two aspects:
(1) cleaner and more compact feature maps. (2) clearer object boundaries and
structured details emerge.
Feature clustering. We also use a simple K-Means clustering to directly ex-
amine the semantic concepts encoded in the feature representations. We show
the K-means clustering results in Fig. 11. The improvements in our features are
directly reflected in those simple clustering results. As shown in Fig. 11, the
K-Means results of DINOv2 (3rd column) are strongly affected by artifacts and
noise.Bycontrast,ourclusteringresults(5thcolumn)aremuchcleanerandmore
compact. In addition, we observe the PCA features and K-Means clustering of
our 3D-aware fine-tuned features exhibit higher temporal consistency than the
standard DINOv2 features. Please check our demos on our project page to see
the full visualizations of video sequences.22 Y. Yue et al.
Input DINOv2 DINOv2 (fine-tuned) Feature Gaussians
Fig.8: Feature visualization on ScanNet++ [65].Wevisualizethefeaturesren-
deredbypre-trainedfeatureGaussians(4th column),featuresofDINOv2(2nd column)
and our fine-tuned features (3rd column).Improving 2D Feature Representations by 3D-Aware Fine-Tuning 23
Input DINOv2 DINOv2 (fine-tuned) Input DINOv2 DINOv2 (fine-tuned)
Fig.9:FeaturevisualizationonindoordatasetsNYUv2[55]andScanNet[9].
Our 3D-aware fine-tuning helps obtain more compact features and capture detailed
structures.24 Y. Yue et al.
Input DINOv2 DINOv2 (fine-tuned) Input DINOv2 DINOv2 (fine-tuned)
Input DINOv2 DINOv2 (fine-tuned)
Fig.10: Feature visualization on out-of-domain datasets. Our 3D-aware fine-
tuning is generalizable to out-of-domain datasets and helps obtain more compact fea-
tures and capture detailed structures.Improving 2D Feature Representations by 3D-Aware Fine-Tuning 25
Input DINOv2 DINOv2 (fine-tuned)
Fig.11:K-Meansclusteringoffeatures.WeshowthePCAfeaturesandK-Means
clustering results of DINOv2 (2, 3th columns) and our 3D-aware fine-tuning features
(4,5th columns).OurK-Meansclusteringresultsaremorecompactanddetailedthan
DINOv2.