[
    {
        "title": "To accept or not to accept? An IRT-TOE Framework to Understand Educators' Resistance to Generative AI in Higher Education",
        "authors": "Jan-Erik KalmusAnastasija Nikiforova",
        "links": "http://arxiv.org/abs/2407.20130v1",
        "entry_id": "http://arxiv.org/abs/2407.20130v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20130v1",
        "summary": "Since the public release of Chat Generative Pre-Trained Transformer\n(ChatGPT), extensive discourse has emerged concerning the potential advantages\nand challenges of integrating Generative Artificial Intelligence (GenAI) into\neducation. In the realm of information systems, research on technology adoption\nis crucial for understanding the diverse factors influencing the uptake of\nspecific technologies. Theoretical frameworks, refined and validated over\ndecades, serve as guiding tools to elucidate the individual and organizational\ndynamics, obstacles, and perceptions surrounding technology adoption. However,\nwhile several models have been proposed, they often prioritize elucidating the\nfactors that facilitate acceptance over those that impede it, typically\nfocusing on the student perspective and leaving a gap in empirical evidence\nregarding educators viewpoints. Given the pivotal role educators play in higher\neducation, this study aims to develop a theoretical model to empirically\npredict the barriers preventing educators from adopting GenAI in their\nclassrooms. Acknowledging the lack of theoretical models tailored to\nidentifying such barriers, our approach is grounded in the Innovation\nResistance Theory (IRT) framework and augmented with constructs from the\nTechnology-Organization-Environment (TOE) framework. This model is transformed\ninto a measurement instrument employing a quantitative approach, complemented\nby a qualitative approach to enrich the analysis and uncover concerns related\nto GenAI adoption in the higher education domain.",
        "updated": "2024-07-29 15:59:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20130v1"
    },
    {
        "title": "What Can Interactive Visualization do for Participatory Budgeting in Chicago?",
        "authors": "Alex KaleDanni LiuMaria Gabriela AyalaHarper SchwabAndrew McNutt",
        "links": "http://arxiv.org/abs/2407.20103v1",
        "entry_id": "http://arxiv.org/abs/2407.20103v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20103v1",
        "summary": "Participatory budgeting (PB) is a democratic approach to allocating municipal\nspending that has been adopted in many places in recent years, including in\nChicago. Current PB voting resembles a ballot where residents are asked which\nmunicipal projects, such as school improvements and road repairs, to fund with\na limited budget. In this work, we ask how interactive visualization can\nbenefit PB by conducting a design probe-based interview study (N=13) with\npolicy workers and academics with expertise in PB, urban planning, and civic\nHCI. Our probe explores how graphical elicitation of voter preferences and a\ndashboard of voting statistics can be incorporated into a realistic PB tool.\nThrough qualitative analysis, we find that visualization creates opportunities\nfor city government to set expectations about budget constraints while also\ngranting their constituents greater freedom to articulate a wider range of\npreferences. However, using visualization to provide transparency about PB\nrequires efforts to mitigate potential access barriers and mistrust. We call\nfor more visualization professionals to help build civic capacity by working in\nand studying political systems.",
        "updated": "2024-07-29 15:31:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20103v1"
    },
    {
        "title": "Visual Support for the Loop Grafting Workflow on Proteins",
        "authors": "Filip OpálenýPavol UlbrichJoan Planas-IglesiasJan ByškaJan ŠtouračDavid BednářKatarína FurmanováBarbora Kozlíková",
        "links": "http://arxiv.org/abs/2407.20054v1",
        "entry_id": "http://arxiv.org/abs/2407.20054v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20054v1",
        "summary": "In understanding and redesigning the function of proteins in modern\nbiochemistry, protein engineers are increasingly focusing on exploring regions\nin proteins called loops. Analyzing various characteristics of these regions\nhelps the experts design the transfer of the desired function from one protein\nto another. This process is denoted as loop grafting. We designed a set of\ninteractive visualizations that provide experts with visual support through all\nthe loop grafting pipeline steps. The workflow is divided into several phases,\nreflecting the steps of the pipeline. Each phase is supported by a specific set\nof abstracted 2D visual representations of proteins and their loops that are\ninteractively linked with the 3D View of proteins. By sequentially passing\nthrough the individual phases, the user shapes the list of loops that are\npotential candidates for loop grafting. Finally, the actual in-silico insertion\nof the loop candidates from one protein to the other is performed, and the\nresults are visually presented to the user. In this way, the fully\ncomputational rational design of proteins and their loops results in newly\ndesigned protein structures that can be further assembled and tested through\nin-vitro experiments. We showcase the contribution of our visual support design\non a real case scenario changing the enantiomer selectivity of the engineered\nenzyme. Moreover, we provide the readers with the experts' feedback.",
        "updated": "2024-07-29 14:40:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20054v1"
    },
    {
        "title": "Exploring Large Language Models to generate Easy to Read content",
        "authors": "Paloma MartínezLourdes MorenoAlberto Ramos",
        "links": "http://arxiv.org/abs/2407.20046v1",
        "entry_id": "http://arxiv.org/abs/2407.20046v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20046v1",
        "summary": "Ensuring text accessibility and understandability are essential goals,\nparticularly for individuals with cognitive impairments and intellectual\ndisabilities, who encounter challenges in accessing information across various\nmediums such as web pages, newspapers, administrative tasks, or health\ndocuments. Initiatives like Easy to Read and Plain Language guidelines aim to\nsimplify complex texts; however, standardizing these guidelines remains\nchallenging and often involves manual processes. This work presents an\nexploratory investigation into leveraging Artificial Intelligence (AI) and\nNatural Language Processing (NLP) approaches to systematically simplify Spanish\ntexts into Easy to Read formats, with a focus on utilizing Large Language\nModels (LLMs) for simplifying texts, especially in generating Easy to Read\ncontent. The study contributes a parallel corpus of Spanish adapted for Easy To\nRead format, which serves as a valuable resource for training and testing text\nsimplification systems. Additionally, several text simplification experiments\nusing LLMs and the collected corpus are conducted, involving fine-tuning and\ntesting a Llama2 model to generate Easy to Read content. A qualitative\nevaluation, guided by an expert in text adaptation for Easy to Read content, is\ncarried out to assess the automatically simplified texts. This research\ncontributes to advancing text accessibility for individuals with cognitive\nimpairments, highlighting promising strategies for leveraging LLMs while\nresponsibly managing energy usage.",
        "updated": "2024-07-29 14:30:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20046v1"
    },
    {
        "title": "MambaGesture: Enhancing Co-Speech Gesture Generation with Mamba and Disentangled Multi-Modality Fusion",
        "authors": "Chencan FuYabiao WangJiangning ZhangZhengkai JiangXiaofeng MaoJiafu WuWeijian CaoChengjie WangYanhao GeYong Liu",
        "links": "http://arxiv.org/abs/2407.19976v1",
        "entry_id": "http://arxiv.org/abs/2407.19976v1",
        "pdf_url": "http://arxiv.org/pdf/2407.19976v1",
        "summary": "Co-speech gesture generation is crucial for producing synchronized and\nrealistic human gestures that accompany speech, enhancing the animation of\nlifelike avatars in virtual environments. While diffusion models have shown\nimpressive capabilities, current approaches often overlook a wide range of\nmodalities and their interactions, resulting in less dynamic and contextually\nvaried gestures. To address these challenges, we present MambaGesture, a novel\nframework integrating a Mamba-based attention block, MambaAttn, with a\nmulti-modality feature fusion module, SEAD. The MambaAttn block combines the\nsequential data processing strengths of the Mamba model with the contextual\nrichness of attention mechanisms, enhancing the temporal coherence of generated\ngestures. SEAD adeptly fuses audio, text, style, and emotion modalities,\nemploying disentanglement to deepen the fusion process and yield gestures with\ngreater realism and diversity. Our approach, rigorously evaluated on the\nmulti-modal BEAT dataset, demonstrates significant improvements in Fr\\'echet\nGesture Distance (FGD), diversity scores, and beat alignment, achieving\nstate-of-the-art performance in co-speech gesture generation.",
        "updated": "2024-07-29 13:09:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.19976v1"
    }
]