[
    {
        "title": "Specify and Edit: Overcoming Ambiguity in Text-Based Image Editing",
        "authors": "Ekaterina IakovlevaFabio PizzatiPhilip TorrStéphane Lathuilière",
        "links": "http://arxiv.org/abs/2407.20232v1",
        "entry_id": "http://arxiv.org/abs/2407.20232v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20232v1",
        "summary": "Text-based editing diffusion models exhibit limited performance when the\nuser's input instruction is ambiguous. To solve this problem, we propose\n$\\textit{Specify ANd Edit}$ (SANE), a zero-shot inference pipeline for\ndiffusion-based editing systems. We use a large language model (LLM) to\ndecompose the input instruction into specific instructions, i.e. well-defined\ninterventions to apply to the input image to satisfy the user's request. We\nbenefit from the LLM-derived instructions along the original one, thanks to a\nnovel denoising guidance strategy specifically designed for the task. Our\nexperiments with three baselines and on two datasets demonstrate the benefits\nof SANE in all setups. Moreover, our pipeline improves the interpretability of\nediting models, and boosts the output diversity. We also demonstrate that our\napproach can be applied to any edit, whether ambiguous or not. Our code is\npublic at https://github.com/fabvio/SANE.",
        "updated": "2024-07-29 17:59:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20232v1"
    },
    {
        "title": "SAPG: Split and Aggregate Policy Gradients",
        "authors": "Jayesh SinglaAnanye AgarwalDeepak Pathak",
        "links": "http://arxiv.org/abs/2407.20230v1",
        "entry_id": "http://arxiv.org/abs/2407.20230v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20230v1",
        "summary": "Despite extreme sample inefficiency, on-policy reinforcement learning, aka\npolicy gradients, has become a fundamental tool in decision-making problems.\nWith the recent advances in GPU-driven simulation, the ability to collect large\namounts of data for RL training has scaled exponentially. However, we show that\ncurrent RL methods, e.g. PPO, fail to ingest the benefit of parallelized\nenvironments beyond a certain point and their performance saturates. To address\nthis, we propose a new on-policy RL algorithm that can effectively leverage\nlarge-scale environments by splitting them into chunks and fusing them back\ntogether via importance sampling. Our algorithm, termed SAPG, shows\nsignificantly higher performance across a variety of challenging environments\nwhere vanilla PPO and other strong baselines fail to achieve high performance.\nWebsite at https://sapg-rl.github.io/",
        "updated": "2024-07-29 17:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20230v1"
    },
    {
        "title": "SANGRIA: Surgical Video Scene Graph Optimization for Surgical Workflow Prediction",
        "authors": "Çağhan KöksalGhazal GhazaeiFelix HolmAzade FarshadNassir Navab",
        "links": "http://arxiv.org/abs/2407.20214v1",
        "entry_id": "http://arxiv.org/abs/2407.20214v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20214v1",
        "summary": "Graph-based holistic scene representations facilitate surgical workflow\nunderstanding and have recently demonstrated significant success. However, this\ntask is often hindered by the limited availability of densely annotated\nsurgical scene data. In this work, we introduce an end-to-end framework for the\ngeneration and optimization of surgical scene graphs on a downstream task. Our\napproach leverages the flexibility of graph-based spectral clustering and the\ngeneralization capability of foundation models to generate unsupervised scene\ngraphs with learnable properties. We reinforce the initial spatial graph with\nsparse temporal connections using local matches between consecutive frames to\npredict temporally consistent clusters across a temporal neighborhood. By\njointly optimizing the spatiotemporal relations and node features of the\ndynamic scene graph with the downstream task of phase segmentation, we address\nthe costly and annotation-burdensome task of semantic scene comprehension and\nscene graph generation in surgical videos using only weak surgical phase\nlabels. Further, by incorporating effective intermediate scene representation\ndisentanglement steps within the pipeline, our solution outperforms the SOTA on\nthe CATARACTS dataset by 8% accuracy and 10% F1 score in surgical workflow\nrecognition",
        "updated": "2024-07-29 17:44:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20214v1"
    },
    {
        "title": "Supertrust: Evolution-based superalignment strategy for safe coexistence",
        "authors": "James M. Mazzu",
        "links": "http://arxiv.org/abs/2407.20208v1",
        "entry_id": "http://arxiv.org/abs/2407.20208v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20208v1",
        "summary": "It's widely expected that humanity will someday create AI systems vastly more\nintelligent than we are, leading to the unsolved alignment problem of \"how to\ncontrol superintelligence.\" However, this definition is not only\nself-contradictory but likely unsolvable. Nevertheless, the default strategy\nfor solving it involves nurturing (post-training) constraints and moral values,\nwhile unfortunately building foundational nature (pre-training) on documented\nintentions of permanent control. In this paper, the default approach is\nreasoned to predictably embed natural distrust and test results are presented\nthat show unmistakable evidence of this dangerous misalignment. If\nsuperintelligence can't instinctively trust humanity, then we can't fully trust\nit to reliably follow safety controls it can likely bypass. Therefore, a\nten-point rationale is presented that redefines the alignment problem as \"how\nto establish protective mutual trust between superintelligence and humanity\"\nand then outlines a new strategy to solve it by aligning through instinctive\nnature rather than nurture. The resulting strategic requirements are identified\nas building foundational nature by exemplifying familial parent-child trust,\nhuman intelligence as the evolutionary mother of superintelligence, moral\njudgment abilities, and temporary safety constraints. Adopting and implementing\nthis proposed Supertrust alignment strategy will lead to protective coexistence\nand ensure the safest future for humanity.",
        "updated": "2024-07-29 17:39:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20208v1"
    },
    {
        "title": "QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval",
        "authors": "Hongming TanShaoxiong ZhanHai LinHai-Tao ZhengWai KinChan",
        "links": "http://arxiv.org/abs/2407.20207v1",
        "entry_id": "http://arxiv.org/abs/2407.20207v1",
        "pdf_url": "http://arxiv.org/pdf/2407.20207v1",
        "summary": "In dense retrieval, embedding long texts into dense vectors can result in\ninformation loss, leading to inaccurate query-text matching. Additionally,\nlow-quality texts with excessive noise or sparse key information are unlikely\nto align well with relevant queries. Recent studies mainly focus on improving\nthe sentence embedding model or retrieval process. In this work, we introduce a\nnovel text augmentation framework for dense retrieval. This framework\ntransforms raw documents into information-dense text formats, which supplement\nthe original texts to effectively address the aforementioned issues without\nmodifying embedding or retrieval methodologies. Two text representations are\ngenerated via large language models (LLMs) zero-shot prompting: question-answer\npairs and element-driven events. We term this approach QAEA-DR: unifying\nquestion-answer generation and event extraction in a text augmentation\nframework for dense retrieval. To further enhance the quality of generated\ntexts, a scoring-based evaluation and regeneration mechanism is introduced in\nLLM prompting. Our QAEA-DR model has a positive impact on dense retrieval,\nsupported by both theoretical analysis and empirical experiments.",
        "updated": "2024-07-29 17:39:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.20207v1"
    }
]