[
    {
        "title": "LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models",
        "authors": "Feng LiRenrui ZhangHao ZhangYuanhan ZhangBo LiWei LiZejun MaChunyuan Li",
        "links": "http://arxiv.org/abs/2407.07895v1",
        "entry_id": "http://arxiv.org/abs/2407.07895v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07895v1",
        "summary": "Visual instruction tuning has made considerable strides in enhancing the\ncapabilities of Large Multimodal Models (LMMs). However, existing open LMMs\nlargely focus on single-image tasks, their applications to multi-image\nscenarios remains less explored. Additionally, prior LMM research separately\ntackles different scenarios, leaving it impossible to generalize cross\nscenarios with new emerging capabilities. To this end, we introduce\nLLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame\n(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To\nenable these capabilities, we regard the interleaved data format as a general\ntemplate and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4\nprimary domains with 14 tasks and 41 datasets. We also curate the\nLLaVA-Interleave Bench to comprehensively evaluate the multi-image performance\nof LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading\nresults in multi-image, video, and 3D benchmarks, while maintaining the\nperformance of single-image tasks. Besides, our model also exhibits several\nemerging capabilities, e.g., transferring tasks across different settings and\nmodalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT",
        "updated": "2024-07-10 17:59:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07895v1"
    },
    {
        "title": "AdaptiGraph: Material-Adaptive Graph-Based Neural Dynamics for Robotic Manipulation",
        "authors": "Kaifeng ZhangBaoyu LiKris HauserYunzhu Li",
        "links": "http://arxiv.org/abs/2407.07889v1",
        "entry_id": "http://arxiv.org/abs/2407.07889v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07889v1",
        "summary": "Predictive models are a crucial component of many robotic systems. Yet,\nconstructing accurate predictive models for a variety of deformable objects,\nespecially those with unknown physical properties, remains a significant\nchallenge. This paper introduces AdaptiGraph, a learning-based dynamics\nmodeling approach that enables robots to predict, adapt to, and control a wide\narray of challenging deformable materials with unknown physical properties.\nAdaptiGraph leverages the highly flexible graph-based neural dynamics (GBND)\nframework, which represents material bits as particles and employs a graph\nneural network (GNN) to predict particle motion. Its key innovation is a\nunified physical property-conditioned GBND model capable of predicting the\nmotions of diverse materials with varying physical properties without\nretraining. Upon encountering new materials during online deployment,\nAdaptiGraph utilizes a physical property optimization process for a few-shot\nadaptation of the model, enhancing its fit to the observed interaction data.\nThe adapted models can precisely simulate the dynamics and predict the motion\nof various deformable materials, such as ropes, granular media, rigid boxes,\nand cloth, while adapting to different physical properties, including\nstiffness, granular size, and center of pressure. On prediction and\nmanipulation tasks involving a diverse set of real-world deformable objects,\nour method exhibits superior prediction accuracy and task proficiency over\nnon-material-conditioned and non-adaptive models. The project page is available\nat https://robopil.github.io/adaptigraph/ .",
        "updated": "2024-07-10 17:57:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07889v1"
    },
    {
        "title": "Generative Image as Action Models",
        "authors": "Mohit ShridharYat Long LoStephen James",
        "links": "http://arxiv.org/abs/2407.07875v1",
        "entry_id": "http://arxiv.org/abs/2407.07875v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07875v1",
        "summary": "Image-generation diffusion models have been fine-tuned to unlock new\ncapabilities such as image-editing and novel view synthesis. Can we similarly\nunlock image-generation models for visuomotor control? We present GENIMA, a\nbehavior-cloning agent that fine-tunes Stable Diffusion to 'draw joint-actions'\nas targets on RGB images. These images are fed into a controller that maps the\nvisual targets into a sequence of joint-positions. We study GENIMA on 25\nRLBench and 9 real-world manipulation tasks. We find that, by lifting actions\ninto image-space, internet pre-trained diffusion models can generate policies\nthat outperform state-of-the-art visuomotor approaches, especially in\nrobustness to scene perturbations and generalizing to novel objects. Our method\nis also competitive with 3D agents, despite lacking priors such as depth,\nkeypoints, or motion-planners.",
        "updated": "2024-07-10 17:41:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07875v1"
    },
    {
        "title": "Green Screen Augmentation Enables Scene Generalisation in Robotic Manipulation",
        "authors": "Eugene TeohSumit PatidarXiao MaStephen James",
        "links": "http://arxiv.org/abs/2407.07868v1",
        "entry_id": "http://arxiv.org/abs/2407.07868v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07868v1",
        "summary": "Generalising vision-based manipulation policies to novel environments remains\na challenging area with limited exploration. Current practices involve\ncollecting data in one location, training imitation learning or reinforcement\nlearning policies with this data, and deploying the policy in the same\nlocation. However, this approach lacks scalability as it necessitates data\ncollection in multiple locations for each task. This paper proposes a novel\napproach where data is collected in a location predominantly featuring green\nscreens. We introduce Green-screen Augmentation (GreenAug), employing a chroma\nkey algorithm to overlay background textures onto a green screen. Through\nextensive real-world empirical studies with over 850 training demonstrations\nand 8.2k evaluation episodes, we demonstrate that GreenAug surpasses no\naugmentation, standard computer vision augmentation, and prior generative\naugmentation methods in performance. While no algorithmic novelties are\nclaimed, our paper advocates for a fundamental shift in data collection\npractices. We propose that real-world demonstrations in future research should\nutilise green screens, followed by the application of GreenAug. We believe\nGreenAug unlocks policy generalisation to visually distinct novel locations,\naddressing the current scene generalisation limitations in robot learning.",
        "updated": "2024-07-10 17:32:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07868v1"
    },
    {
        "title": "Controlling Space and Time with Diffusion Models",
        "authors": "Daniel WatsonSaurabh SaxenaLala LiAndrea TagliasacchiDavid J. Fleet",
        "links": "http://arxiv.org/abs/2407.07860v1",
        "entry_id": "http://arxiv.org/abs/2407.07860v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07860v1",
        "summary": "We present 4DiM, a cascaded diffusion model for 4D novel view synthesis\n(NVS), conditioned on one or more images of a general scene, and a set of\ncamera poses and timestamps. To overcome challenges due to limited availability\nof 4D training data, we advocate joint training on 3D (with camera pose), 4D\n(pose+time) and video (time but no pose) data and propose a new architecture\nthat enables the same. We further advocate the calibration of SfM posed data\nusing monocular metric depth estimators for metric scale camera control. For\nmodel evaluation, we introduce new metrics to enrich and overcome shortcomings\nof current evaluation schemes, demonstrating state-of-the-art results in both\nfidelity and pose control compared to existing diffusion models for 3D NVS,\nwhile at the same time adding the ability to handle temporal dynamics. 4DiM is\nalso used for improved panorama stitching, pose-conditioned video to video\ntranslation, and several other tasks. For an overview see\nhttps://4d-diffusion.github.io",
        "updated": "2024-07-10 17:23:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07860v1"
    }
]