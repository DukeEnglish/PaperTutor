[
    {
        "title": "The Human Factor in AI Red Teaming: Perspectives from Social and Collaborative Computing",
        "authors": "Alice Qian ZhangRyland ShawJacy Reese AnthisAshlee MiltonEmily TsengJina SuhLama AhmadRam Shankar Siva KumarJulian PosadaBenjamin ShestakofskySarah T. RobertsMary L. Gray",
        "links": "http://arxiv.org/abs/2407.07786v1",
        "entry_id": "http://arxiv.org/abs/2407.07786v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07786v1",
        "summary": "Rapid progress in general-purpose AI has sparked significant interest in \"red\nteaming,\" a practice of adversarial testing originating in military and\ncybersecurity applications. AI red teaming raises many questions about the\nhuman factor, such as how red teamers are selected, biases and blindspots in\nhow tests are conducted, and harmful content's psychological effects on red\nteamers. A growing body of HCI and CSCW literature examines related\npractices-including data labeling, content moderation, and algorithmic\nauditing. However, few, if any, have investigated red teaming itself. This\nworkshop seeks to consider the conceptual and empirical challenges associated\nwith this practice, often rendered opaque by non-disclosure agreements. Future\nstudies may explore topics ranging from fairness to mental health and other\nareas of potential harm. We aim to facilitate a community of researchers and\npractitioners who can begin to meet these challenges with creativity,\ninnovation, and thoughtful reflection.",
        "updated": "2024-07-10 16:02:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07786v1"
    },
    {
        "title": "The V-Lab VR Educational Application Framework",
        "authors": "Vasilis ZafeiropoulosGeorge AnastassakisTheophanis OrphanoudakisDimitris KallesAnastasios FanariotisVassilis Fotopoulos",
        "links": "http://dx.doi.org/10.1145/3565066.3608246",
        "entry_id": "http://arxiv.org/abs/2407.07698v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07698v1",
        "summary": "This paper presents the V-Lab, a VR application development framework for\neducational scenarios mainly involving scientific processes executed in\nlaboratory environments such as chemistry and biology laboratories. This work\nis an extension of the Onlabs simulator which has been developed by the\nHellenic Open University as a distance teaching enabler for similar subjects,\nhelping to alleviate the need for access to the physical laboratory\ninfrastructure; thus, shortening training periods of students in the laboratory\nand making their training during the periods of physical presence more\nproductive and secure. The extensions of the Onlabs to deliver an enhanced and\nmodular framework that can be extended to multiple educational scenarios is the\nwork performed within the context of the European project XR2Learn (Leveraging\nthe European XR industry technologies to empower immersive learning and\ntraining).",
        "updated": "2024-07-10 14:31:36 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07698v1"
    },
    {
        "title": "The Language of Weather: Social Media Reactions to Weather Accounting for Climatic and Linguistic Baselines",
        "authors": "James C. YoungRudy ArthurHywel T. P. Williams",
        "links": "http://arxiv.org/abs/2407.07683v1",
        "entry_id": "http://arxiv.org/abs/2407.07683v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07683v1",
        "summary": "This study explores how different weather conditions influence public\nsentiment on social media, focusing on Twitter data from the UK. By considering\nclimate and linguistic baselines, we improve the accuracy of weather-related\nsentiment analysis. Our findings show that emotional responses to weather are\ncomplex, influenced by combinations of weather variables and regional language\ndifferences. The results highlight the importance of context-sensitive methods\nfor better understanding public mood in response to weather, which can enhance\nimpact-based forecasting and risk communication in the context of climate\nchange.",
        "updated": "2024-07-10 14:08:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07683v1"
    },
    {
        "title": "StoryDiffusion: How to Support UX Storyboarding With Generative-AI",
        "authors": "Zhaohui LiangXiaoyu ZhangKevin MaZhao LiuXipei RenKosa Goucher-LambertCan Liu",
        "links": "http://arxiv.org/abs/2407.07672v1",
        "entry_id": "http://arxiv.org/abs/2407.07672v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07672v1",
        "summary": "Storyboarding is an established method for designing user experiences.\nGenerative AI can support this process by helping designers quickly create\nvisual narratives. However, existing tools only focus on accurate text-to-image\ngeneration. Currently, it is not clear how to effectively support the entire\ncreative process of storyboarding and how to develop AI-powered tools to\nsupport designers' individual workflows. In this work, we iteratively developed\nand implemented StoryDiffusion, a system that integrates text-to-text and\ntext-to-image models, to support the generation of narratives and images in a\nsingle pipeline. With a user study, we observed 12 UX designers using the\nsystem for both concept ideation and illustration tasks. Our findings\nidentified AI-directed vs. user-directed creative strategies in both tasks and\nrevealed the importance of supporting the interchange between narrative\niteration and image generation. We also found effects of the design tasks on\ntheir strategies and preferences, providing insights for future development.",
        "updated": "2024-07-10 13:59:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07672v1"
    },
    {
        "title": "AffectGPT: Dataset and Framework for Explainable Multimodal Emotion Recognition",
        "authors": "Zheng LianHaiyang SunLicai SunJiangyan YiBin LiuJianhua Tao",
        "links": "http://arxiv.org/abs/2407.07653v1",
        "entry_id": "http://arxiv.org/abs/2407.07653v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07653v1",
        "summary": "Explainable Multimodal Emotion Recognition (EMER) is an emerging task that\naims to achieve reliable and accurate emotion recognition. However, due to the\nhigh annotation cost, the existing dataset (denoted as EMER-Fine) is small,\nmaking it difficult to perform supervised training. To reduce the annotation\ncost and expand the dataset size, this paper reviews the previous dataset\nconstruction process. Then, we simplify the annotation pipeline, avoid manual\nchecks, and replace the closed-source models with open-source models. Finally,\nwe build \\textbf{EMER-Coarse}, a coarsely-labeled dataset containing\nlarge-scale samples. Besides the dataset, we propose a two-stage training\nframework \\textbf{AffectGPT}. The first stage exploits EMER-Coarse to learn a\ncoarse mapping between multimodal inputs and emotion-related descriptions; the\nsecond stage uses EMER-Fine to better align with manually-checked results.\nExperimental results demonstrate the effectiveness of our proposed method on\nthe challenging EMER task. To facilitate further research, we will make the\ncode and dataset available at: https://github.com/zeroQiaoba/AffectGPT.",
        "updated": "2024-07-10 13:34:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07653v1"
    }
]