[
    {
        "title": "Training on the Test Task Confounds Evaluation and Emergence",
        "authors": "Ricardo Dominguez-OlmedoFlorian E. DornerMoritz Hardt",
        "links": "http://arxiv.org/abs/2407.07890v1",
        "entry_id": "http://arxiv.org/abs/2407.07890v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07890v1",
        "summary": "We study a fundamental problem in the evaluation of large language models\nthat we call training on the test task. Unlike wrongful practices like training\non the test data, leakage, or data contamination, training on the test task is\nnot a malpractice. Rather, the term describes a growing set of techniques to\ninclude task-relevant data in the pretraining stage of a language model. We\ndemonstrate that training on the test task confounds both relative model\nevaluations and claims about emergent capabilities. We argue that the seeming\nsuperiority of one model family over another may be explained by a different\ndegree of training on the test task. To this end, we propose an effective\nmethod to adjust for training on the test task by fine-tuning each model under\ncomparison on the same task-relevant data before evaluation. We then show that\ninstances of emergent behavior largely vanish once we adjust for training on\nthe test task. This also applies to reported instances of emergent behavior\nthat cannot be explained by the choice of evaluation metric. Our work promotes\na new perspective on the evaluation of large language models with broad\nimplications for benchmarking and the study of emergent capabilities.",
        "updated": "2024-07-10 17:57:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07890v1"
    },
    {
        "title": "Vegetable Peeling: A Case Study in Constrained Dexterous Manipulation",
        "authors": "Tao ChenEric CousineauNaveen KuppuswamyPulkit Agrawal",
        "links": "http://arxiv.org/abs/2407.07884v1",
        "entry_id": "http://arxiv.org/abs/2407.07884v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07884v1",
        "summary": "Recent studies have made significant progress in addressing dexterous\nmanipulation problems, particularly in in-hand object reorientation. However,\nthere are few existing works that explore the potential utilization of\ndeveloped dexterous manipulation controllers for downstream tasks. In this\nstudy, we focus on constrained dexterous manipulation for food peeling. Food\npeeling presents various constraints on the reorientation controller, such as\nthe requirement for the hand to securely hold the object after reorientation\nfor peeling. We propose a simple system for learning a reorientation controller\nthat facilitates the subsequent peeling task. Videos are available at:\nhttps://taochenshh.github.io/projects/veg-peeling.",
        "updated": "2024-07-10 17:51:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07884v1"
    },
    {
        "title": "Towards Robust Alignment of Language Models: Distributionally Robustifying Direct Preference Optimization",
        "authors": "Junkang WuYuexiang XieZhengyi YangJiancan WuJiawei ChenJinyang GaoBolin DingXiang WangXiangnan He",
        "links": "http://arxiv.org/abs/2407.07880v1",
        "entry_id": "http://arxiv.org/abs/2407.07880v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07880v1",
        "summary": "This study addresses the challenge of noise in training datasets for Direct\nPreference Optimization (DPO), a method for aligning Large Language Models\n(LLMs) with human preferences. We categorize noise into pointwise noise, which\nincludes low-quality data points, and pairwise noise, which encompasses\nerroneous data pair associations that affect preference rankings. Utilizing\nDistributionally Robust Optimization (DRO), we enhance DPO's resilience to\nthese types of noise. Our theoretical insights reveal that DPO inherently\nembeds DRO principles, conferring robustness to pointwise noise, with the\nregularization coefficient $\\beta$ playing a critical role in its noise\nresistance. Extending this framework, we introduce Distributionally\nRobustifying DPO (Dr. DPO), which integrates pairwise robustness by optimizing\nagainst worst-case pairwise scenarios. The novel hyperparameter $\\beta'$ in Dr.\nDPO allows for fine-tuned control over data pair reliability, providing a\nstrategic balance between exploration and exploitation in noisy training\nenvironments. Empirical evaluations demonstrate that Dr. DPO substantially\nimproves the quality of generated text and response accuracy in preference\ndatasets, showcasing enhanced performance in both noisy and noise-free\nsettings. The code is available at https://github.com/junkangwu/Dr_DPO.",
        "updated": "2024-07-10 17:48:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07880v1"
    },
    {
        "title": "Generative Image as Action Models",
        "authors": "Mohit ShridharYat Long LoStephen James",
        "links": "http://arxiv.org/abs/2407.07875v1",
        "entry_id": "http://arxiv.org/abs/2407.07875v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07875v1",
        "summary": "Image-generation diffusion models have been fine-tuned to unlock new\ncapabilities such as image-editing and novel view synthesis. Can we similarly\nunlock image-generation models for visuomotor control? We present GENIMA, a\nbehavior-cloning agent that fine-tunes Stable Diffusion to 'draw joint-actions'\nas targets on RGB images. These images are fed into a controller that maps the\nvisual targets into a sequence of joint-positions. We study GENIMA on 25\nRLBench and 9 real-world manipulation tasks. We find that, by lifting actions\ninto image-space, internet pre-trained diffusion models can generate policies\nthat outperform state-of-the-art visuomotor approaches, especially in\nrobustness to scene perturbations and generalizing to novel objects. Our method\nis also competitive with 3D agents, despite lacking priors such as depth,\nkeypoints, or motion-planners.",
        "updated": "2024-07-10 17:41:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07875v1"
    },
    {
        "title": "Toto: Time Series Optimized Transformer for Observability",
        "authors": "Ben CohenEmaad KhwajaKan WangCharles MassonElise RaméYoussef DoubliOthmane Abou-Amal",
        "links": "http://arxiv.org/abs/2407.07874v2",
        "entry_id": "http://arxiv.org/abs/2407.07874v2",
        "pdf_url": "http://arxiv.org/pdf/2407.07874v2",
        "summary": "This technical report describes the Time Series Optimized Transformer for\nObservability (Toto), a new state of the art foundation model for time series\nforecasting developed by Datadog. In addition to advancing the state of the art\non generalized time series benchmarks in domains such as electricity and\nweather, this model is the first general-purpose time series forecasting\nfoundation model to be specifically tuned for observability metrics.\n  Toto was trained on a dataset of one trillion time series data points, the\nlargest among all currently published time series foundation models. Alongside\npublicly available time series datasets, 75% of the data used to train Toto\nconsists of fully anonymous numerical metric data points from the Datadog\nplatform.\n  In our experiments, Toto outperforms existing time series foundation models\non observability data. It does this while also excelling at general-purpose\nforecasting tasks, achieving state-of-the-art zero-shot performance on multiple\nopen benchmark datasets.",
        "updated": "2024-07-11 16:18:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07874v2"
    }
]