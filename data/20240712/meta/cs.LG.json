[
    {
        "title": "Pentagonal Photonic Crystal Mirrors: Scalable Lightsails with Enhanced Acceleration via Neural Topology Optimization",
        "authors": "L. NorderS. YinM. J. de JongF. StalloneH. AydogmusP. M. SbernaM. A. BessaR. A. Norte",
        "links": "http://arxiv.org/abs/2407.07896v1",
        "entry_id": "http://arxiv.org/abs/2407.07896v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07896v1",
        "summary": "The Starshot Breakthrough Initiative aims to send one-gram microchip probes\nto Alpha Centauri within 20 years, using gram-scale lightsails propelled by\nlaser-based radiation pressure, reaching velocities nearing a fifth of light\nspeed. This mission requires lightsail materials that challenge the\nfundamentals of nanotechnology, requiring innovations in optics, material\nscience and structural engineering. Unlike the microchip payload, which must be\nminimized in every dimension, such lightsails need meter-scale dimensions with\nnanoscale thickness and billions of nanoscale holes to enhance reflectivity and\nreduce mass. Our study employs neural topology optimization, revealing a novel\npentagonal lattice-based photonic crystal (PhC) reflector. The optimized\ndesigns shorten acceleration times, therefore lowering launch costs\nsignificantly. Crucially, these designs also enable lightsail material\nfabrication with orders-of-magnitude reduction in costs. We have fabricated a\n60 x 60 mm$^2$, 200nm thick, single-layer reflector perforated with over a\nbillion nanoscale features; the highest aspect-ratio nanophotonic element to\ndate. We achieve this with nearly 9,000 times cost reduction per m$^2$.\nStarshot lightsails will have several stringent requirements but will\nultimately be driven by costs to build at scale. Here we highlight challenges\nand possible solutions in developing lightsail materials - showcasing the\npotential of scaling nanophotonics for cost-effective next-generation space\nexploration.",
        "updated": "2024-07-10 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07896v1"
    },
    {
        "title": "LLaVA-NeXT-Interleave: Tackling Multi-image, Video, and 3D in Large Multimodal Models",
        "authors": "Feng LiRenrui ZhangHao ZhangYuanhan ZhangBo LiWei LiZejun MaChunyuan Li",
        "links": "http://arxiv.org/abs/2407.07895v1",
        "entry_id": "http://arxiv.org/abs/2407.07895v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07895v1",
        "summary": "Visual instruction tuning has made considerable strides in enhancing the\ncapabilities of Large Multimodal Models (LMMs). However, existing open LMMs\nlargely focus on single-image tasks, their applications to multi-image\nscenarios remains less explored. Additionally, prior LMM research separately\ntackles different scenarios, leaving it impossible to generalize cross\nscenarios with new emerging capabilities. To this end, we introduce\nLLaVA-NeXT-Interleave, which simultaneously tackles Multi-image, Multi-frame\n(video), Multi-view (3D), and Multi-patch (single-image) scenarios in LMMs. To\nenable these capabilities, we regard the interleaved data format as a general\ntemplate and compile the M4-Instruct dataset with 1,177.6k samples, spanning 4\nprimary domains with 14 tasks and 41 datasets. We also curate the\nLLaVA-Interleave Bench to comprehensively evaluate the multi-image performance\nof LMMs. Through extensive experiments, LLaVA-NeXT-Interleave achieves leading\nresults in multi-image, video, and 3D benchmarks, while maintaining the\nperformance of single-image tasks. Besides, our model also exhibits several\nemerging capabilities, e.g., transferring tasks across different settings and\nmodalities. Code is available at https://github.com/LLaVA-VL/LLaVA-NeXT",
        "updated": "2024-07-10 17:59:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07895v1"
    },
    {
        "title": "Training on the Test Task Confounds Evaluation and Emergence",
        "authors": "Ricardo Dominguez-OlmedoFlorian E. DornerMoritz Hardt",
        "links": "http://arxiv.org/abs/2407.07890v1",
        "entry_id": "http://arxiv.org/abs/2407.07890v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07890v1",
        "summary": "We study a fundamental problem in the evaluation of large language models\nthat we call training on the test task. Unlike wrongful practices like training\non the test data, leakage, or data contamination, training on the test task is\nnot a malpractice. Rather, the term describes a growing set of techniques to\ninclude task-relevant data in the pretraining stage of a language model. We\ndemonstrate that training on the test task confounds both relative model\nevaluations and claims about emergent capabilities. We argue that the seeming\nsuperiority of one model family over another may be explained by a different\ndegree of training on the test task. To this end, we propose an effective\nmethod to adjust for training on the test task by fine-tuning each model under\ncomparison on the same task-relevant data before evaluation. We then show that\ninstances of emergent behavior largely vanish once we adjust for training on\nthe test task. This also applies to reported instances of emergent behavior\nthat cannot be explained by the choice of evaluation metric. Our work promotes\na new perspective on the evaluation of large language models with broad\nimplications for benchmarking and the study of emergent capabilities.",
        "updated": "2024-07-10 17:57:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07890v1"
    },
    {
        "title": "AdaptiGraph: Material-Adaptive Graph-Based Neural Dynamics for Robotic Manipulation",
        "authors": "Kaifeng ZhangBaoyu LiKris HauserYunzhu Li",
        "links": "http://arxiv.org/abs/2407.07889v1",
        "entry_id": "http://arxiv.org/abs/2407.07889v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07889v1",
        "summary": "Predictive models are a crucial component of many robotic systems. Yet,\nconstructing accurate predictive models for a variety of deformable objects,\nespecially those with unknown physical properties, remains a significant\nchallenge. This paper introduces AdaptiGraph, a learning-based dynamics\nmodeling approach that enables robots to predict, adapt to, and control a wide\narray of challenging deformable materials with unknown physical properties.\nAdaptiGraph leverages the highly flexible graph-based neural dynamics (GBND)\nframework, which represents material bits as particles and employs a graph\nneural network (GNN) to predict particle motion. Its key innovation is a\nunified physical property-conditioned GBND model capable of predicting the\nmotions of diverse materials with varying physical properties without\nretraining. Upon encountering new materials during online deployment,\nAdaptiGraph utilizes a physical property optimization process for a few-shot\nadaptation of the model, enhancing its fit to the observed interaction data.\nThe adapted models can precisely simulate the dynamics and predict the motion\nof various deformable materials, such as ropes, granular media, rigid boxes,\nand cloth, while adapting to different physical properties, including\nstiffness, granular size, and center of pressure. On prediction and\nmanipulation tasks involving a diverse set of real-world deformable objects,\nour method exhibits superior prediction accuracy and task proficiency over\nnon-material-conditioned and non-adaptive models. The project page is available\nat https://robopil.github.io/adaptigraph/ .",
        "updated": "2024-07-10 17:57:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07889v1"
    },
    {
        "title": "Learning In-Hand Translation Using Tactile Skin With Shear and Normal Force Sensing",
        "authors": "Jessica YinHaozhi QiJitendra MalikJames PikulMark YimTess Hellebrekers",
        "links": "http://arxiv.org/abs/2407.07885v1",
        "entry_id": "http://arxiv.org/abs/2407.07885v1",
        "pdf_url": "http://arxiv.org/pdf/2407.07885v1",
        "summary": "Recent progress in reinforcement learning (RL) and tactile sensing has\nsignificantly advanced dexterous manipulation. However, these methods often\nutilize simplified tactile signals due to the gap between tactile simulation\nand the real world. We introduce a sensor model for tactile skin that enables\nzero-shot sim-to-real transfer of ternary shear and binary normal forces. Using\nthis model, we develop an RL policy that leverages sliding contact for\ndexterous in-hand translation. We conduct extensive real-world experiments to\nassess how tactile sensing facilitates policy adaptation to various unseen\nobject properties and robot hand orientations. We demonstrate that our 3-axis\ntactile policies consistently outperform baselines that use only shear forces,\nonly normal forces, or only proprioception. Website:\nhttps://jessicayin.github.io/tactile-skin-rl/",
        "updated": "2024-07-10 17:52:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2407.07885v1"
    }
]