Training on the Test Task
Confounds Evaluation and Emergence
Ricardo Dominguez-Olmedo∗1,2, Florian E. Dorner1,2,3, and Moritz Hardt1,2
1Max Planck Institute for Intelligent Systems, Tübingen
2Tübingen AI Center
3ETH Zurich
July 11, 2024
Abstract
Westudyafundamentalproblemintheevaluationoflargelanguagemodelsthatwecalltrainingonthe
testtask. Unlikewrongfulpracticesliketrainingonthetestdata,leakage,ordatacontamination,training
onthetesttaskisnotamalpractice. Rather,thetermdescribesagrowingsetoftechniquestoinclude
task-relevantdatainthepretrainingstageofalanguagemodel. Wedemonstratethattrainingonthetest
taskconfoundsbothrelativemodelevaluationsandclaimsaboutemergentcapabilities. Wearguethatthe
seemingsuperiorityofonemodelfamilyoveranothermaybeexplainedbyadifferentdegreeoftraining
onthetesttask. Tothisend,weproposeaneffectivemethodtoadjustfortrainingonthetesttaskby
fine-tuningeachmodelundercomparisononthesametask-relevantdatabeforeevaluation. Wethenshow
thatinstancesofemergentbehaviorlargelyvanishonceweadjustfortrainingonthetesttask. Thisalso
appliestoreportedinstancesofemergentbehaviorthatcannotbeexplainedbythechoiceofevaluation
metric. Ourworkpromotesanewperspectiveontheevaluationoflargelanguagemodelswithbroad
implicationsforbenchmarkingandthestudyofemergentcapabilities.†
1 Introduction
Themachinelearningcommunityhaslongrecognizedcertainclearviolationsofthebenchmarkingprotocol.
Trainingonthetestsetisthemostnotoriousamongthem(DudaandHart,1973;Hastieetal.,2017;Hardt
andRecht,2022). Dataleakage(KapoorandNarayanan,2022)anddatacontamination(Robertsetal.,2023;
Jiangetal.,2024)arecloselyrelatedproblemslinkedtotheriseofmassiveweb-crawledtrainingdatasets.
Researcherscanallagreethattestdatashouldneverappearinthetrainingset.
Butit’sbeenmuchlessclearwhattodoaboutlegitimateattemptstobringtrainingclosertoevaluation.
There is an obvious a gap between next token prediction at training time and tasks, such as reasoning
and question answering, at test time. Ongoing research and engineering efforts, in fact, aim to narrow
preciselythisgap(MetaAI,2024;Lewis,2024). Whyshouldn’ttrainingbeinformedbyknowledgeaboutthe
downstreamtesttasks? What’sanunfairadvantageofsomemaybethefeatureofothers.
Inthiswork,wegroupstrategiestoutilizetaskknowledgeattrainingtimeundertheumbrellatermof
trainingonthetesttask. Examplesoftrainingonthetesttaskincludetheuseofinstruction-tuningdataor
questionansweringtemplatesduringpre-training(Zengetal.,2022;Baietal.,2023). Weworkfromthe
premisethattrainingonthetesttaskisacceptable—oratleast,unavoidable.
Inanutshell,weshowthattrainingonthetesttaskstronglyconfoundsmodelcomparisonsacrossdifferent
scales and model families. Moreover, it significantly obscures the study of emergent capabilities of large
∗Correspondingauthor.Email:rdo@tuebingen.mpg.de
†Codeandfine-tunedmodelsareavailableathttps://github.com/socialfoundations/training-on-the-test-task
1
4202
luJ
01
]LC.sc[
1v09870.7042:viXraBase models trained after November 2023 outperform those trained before Novemeber 2023
MMLU GSM8K
0.8
0.7 Difference =0.068 Difference =0.168
0.6 Regression R2=0.940 0.6 Regression R2=0.908
0.5 0.4
0.4
0.2
0.3
0.0
1020 1021 1022 1023 1024 1020 1021 1022 1023 1024
Pretraining compute (FLOPs) Pretraining compute (FLOPs)
After fine-tuning all models on the test task, differences in model performance vanish
MMLU GSM8K
0.8
0.7
Difference = 0.005 Difference = 0.001
0.6 Regression R2=0.990 0.6 Regression R2=0.957
0.5 0.4
0.4
0.2
0.3
0.0
1020 1021 1022 1023 1024 1020 1021 1022 1023 1024
Pretraining compute (FLOPs) Pretraining compute (FLOPs)
Models trained Before November 2023 After November 2023
Figure1: MMLUandGSM8Kscoresof53basemodels,withmodelsizesrangingfrom70Mto70Bparameters.
SolidlinescorrespondtotheregressionfitofA=αmax(0,logC−c )+θN+r,whereAisaccuracy, C is
e
pretrainingcompute, N iswhetherthemodelwastrainedafterNovember2023,and r israndomchance
accuracy. Thecoefficientθ denotestheaverageimprovementofmodelstrainedafterNovember2023when
controllingforpretrainingcompute. Boldindicatesstatisticalsignificancewith p-value<0.05. (Top)We
hypothesizethattrainingonthetesttaskconfoundsbenchmarkevaluations,resultinginnewerbasemodels
substantiallyoutperformingolderones. (Bottom)Weproposetoadjustfordifferencesintesttasktrainingby
fine-tuningallmodelsonthesame,sufficientamountoftask-specificdatabeforeevaluation. Afterfine-tuning
onthetesttask,differencesinbenchmarkperformancebetweenolderandnewermodelsvanish.
languagemodels. Perhapscounterintuitively,weproposetomitigatetheeffectsoftrainingonthetesttask
bydoingmoreofit. Weshowthatwecaneffectivelyleveltheplayingfieldbygivingeachmodelthesame,
sufficienttask-specificfine-tuningbeforeevaluation. Thisadjustmentrestorescleanlog-linearscalingand
makescapabilitiespredictablebasedonmuchsmallermodelscales.
1.1 Our contributions
Weintroducethetermtrainingonthetesttasktogroupagrowingrepertoireofpracticesthatutilizeknowledge
about evaluation tasks at training time. We study its impact on benchmark evaluations by inspecting 53
differentlanguagemodelsintwomajoractivebenchmarks,MMLUandGSM8K.
WestartinSection2bydividingmodelsintothosetrainedbeforeNovember2023andthosetrained
after. Wefindthatforthesameamountofcompute,newermodelsoutperformoldermodelsonaverageby7
percentagepointsinMMLUand17pointsinGSM8K.Wethenfine-tuneallmodelsonthesameamountof
task-specificdatabeforeevaluation. Weshowthatafterfine-tuning,newermodelsnolongeroutperform
2
ycaruccA
ycaruccA
ycaruccA
ycaruccAolder models. See Figure 1. This outcome suggests that newer models outperform older ones because
they—implicitlyorexplicitly—trainedmoreonthetesttask. Moreover,itshowshowtesttasktrainingcan
distortbenchmarkperformance.
Weproposeasimpleandeffectivemethodtoadjustfortheeffectoftrainingonthetesttask. Putsimply,
wefine-tuneeachmodelonthesame,sufficientamountoftask-specificdatabeforeevaluation. Tovalidate
ourmethod,wedemonstrateitseffectivenessinacontrolledsetting: wetaketheoldermodelsandfine-tune
halfofthemonthetesttask. Remarkably,thisrecreatesthekindofperformancedifferencesobservedbetween
newerandoldermodels. Wethenshowthatwecanundotheadvantageofthefine-tunedmodelsoverthe
othermodelsbyfurtherfine-tuningallmodelsonthetesttask(Section3.1,Figure3).
Next, we give evidence that training on the test task may be a more dominant factor in benchmark
performancethandatacontamination. Toarguethispoint,weconsiderARCandHellaSwag,whichusecloze
promptsforevaluation. Here,atfirstthereappearstobenosignofanunfairadvantageinanyspecificmodel
family. ButafterswitchingtoMMLU-stylemultiplechoiceprompts,weseethesameconfoundedresultsas
forMMLU(Section3.2,Figure4). ThissuggestthatnewermodelsperformwellinMMLUlikelynotbecause
ofmemorizationofspecifictestingdata,butratherduetoanimprovedabilitytocomprehendMMLU-style
prompts. Eitherway,ourproposedadjustmentrecoversfairmodelcomparisons.
Weshowthattrainingonthetesttasksignificantlydistortsmodelfamilycomparisons. Thedesignchoices
ofcertainmodelfamilies–suchasitspretrainingdatamixture–mayappearsuperiortoothersbeforeadjusting
fortesttasktrainingbutnotafteradjustment(Section4.1,Figure6). Wealsodemonstratethattesttask
trainingoverestimatestheprogressincapabilitiesachievedbyrecentmodels. Afteradjustment,newermodels
onlymodestlyimprovetheParetofrontierofperformanceagainstcompute(Section4.2,Figure7).
Finally,wedemonstratethattrainingonthetesttaskhasprofoundimplicationsforthestudyofemergent
capabilities. Specifically,weshowthatthephenomenonofemergencedisappearsgraduallyastheamountof
trainingonthetesttaskgrows(Section5). Inparticular,wecanmakecapabilitiesvisibleandpredictable
frommuchsmallermodelscales. Importantly,ouradjustmentalsoworksincases,likeMMLU,whereprevious
purportedexplanationsofemergence,suchasthechoiceofevaluationmetric,donotsuffice.
Ourworkcallsforamajorreorientationoflargelanguagemodelevaluation. Modelcomparisons,scaling
laws,andclaimsofemergence,areallstronglyconfoundedbythechoiceoftrainingdatarelativetothetest
tasks. Ratherthanscramblingtodetectanddisallowvariousformsoftrainingonthetesttask,weproposeto
“fightfirewithfire”. Specifically,ourrecommendationistogiveeachmodelthesamesufficientamountof
fine-tuningontask-relevantdatapriortoevaluation.
Limitations. Althoughdeliberatetrainingonthetesttaskcanleveltheplayingfield,ourworkshowsthat
generallysignificantfine-tuningisneededbeforetheplayingfieldisactuallylevel. Thisrequirementposes
additionalcomputationalburdenonthesideoftheevaluator. Thecomputationalresourcestofine-tuneall
modelsequallymaynotbeavailabletoallevaluatorsdependingonthecircumstances. Inaddition,sufficient
task-relevanttrainingdatamightbeexpensivetosourceorgenerallyunavailableformanytasks. Whilethis
isnotanissuewhenmodeltrainersalsolackaccesstosuchdata,trainingonproprietarytask-specifictraining
datawouldbedifficulttocorrectfor.
2 Adjusting for training on the test task
WechooseMMLU(Hendrycksetal.,2020)andGSM8K(Cobbeetal.,2021)asacasestudyforinvestigating
training on the test task in active benchmarks. MMLU tests for world knowledge, whereas GSM8K tests
multi-stepmathematicalreasoning. Thesetwobenchmarksareveryprominentintheliteratureatpresent
time. Forinstance,GPT4(Achiametal.,2023),Claude3(Anthropic,2024),Gemini(Geminietal.,2023)
and Llama 3 (MetaAI, 2024) all report and highlight MMLU and GSM8K. They are also included in the
HuggingFace(HF)OpenLLMLeaderboard* (Beechingetal.,2023),apopularbenchmarkleaderboardthat
evaluatesandranksmodelswithpubliclyavailableweights. WeevaluatemodelsusingLMEvaluationHarness
library(EleutherAI,2024),inidenticalfashiontotheHFleaderboard.
*SeeAppendixCforresultspertainingtotheOpenLLMLeaderboardv2(Fourrieretal.,2024a).
3Weevaluate53basemodels,ranginginsizefrom70Mto70Bparameters. SeeAppendixA.1forthefull
list. TheHFleaderboard’sFAQmakesthedistinctionbetween“basepretrainedmodels”andinstruction-tuned
orchatmodels,arguingthatthisisnecessarytoensurefairmodelcomparisons. Weselectmodelsthatare
categorizedas“pretrained”. Wecheckthatthetechnicalreportofeachoftheselectedmodelsmakesno
mentionofthemodelbeingfine-tuned. Weonlyconsidermodelsforwhichthenumberoftrainingtokensis
known. ThisallowsustoestimatethetotalamountofpretrainingcomputeinFLOPsasC ≈6·N·D,where
C ispretrainingcompute,N isthenumberofmodelparameters,and Disthenumberoftrainingtokens.
Recentmodelsoutperformolderonesgiventhesamepretrainingcompute. Weevaluatemodelson
MMLUandGSM8K,andplotbenchmarkaccuracyagainstpretrainingcomputeinFigure1top. Weobserve
that performance correlates with pretraining compute for both benchmarks. However, on the surface it
appearsthatmorerecentmodelsbetterleveragepretrainingcompute. Inotherwords,foragivencompute
budgetnewermodelsareabletoattainbetterbenchmarkperformance. Infact,modelstrainedafterNovember
2023ParetodominatethosetrainedbeforeNovember2023.
TheseimprovementsinbenchmarkperformancecoincidewitharecenttrendinLLMresearchofincreas-
inglyutilizingtesttaskknowledgeattrainingtime. Forexample,Qwen1.5(Baietal.,2023),Olmo1.7(Groen-
eveld et al., 2024) and MAP Neo (Zhang et al., 2024) include instruction data during pretraining. Sta-
bleLM2(StabilityAI,2023)reformulatessomeofitspretrainingdatasetstobetterresembledownstream
taskssuchasquestion-answering. Moresubtly,thepretrainingdatamixtureofGemma(Gemmaetal.,2024)
wasdeterminedpartiallybasedondownstreambenchmarkevaluations.
Thisraisesanimportantquestion: Donewermodelsoutperformolderonesmainlybecausenewermodels
effectively trained more on the test task? At first sight, an answer seems elusive. After all, it would be
bothinfeasibleandcostprohibitiveto trainallmodelswiththesametrainingdataand computebudget.
Nevertheless,inthenextsection,weproposeawaytogetattheanswerbyadjustingfortheeffectoftraining
onthetesttask.
2.1 Adjusting for training on the test task by training on the test task
Weproposetoadjustfordifferencesintesttasktrainingbyfine-tuningallmodelsonthesame,sufficient
amountoftask-specificdatabeforeevaluation. Todoso,weneedasourceoftask-specificdataforeachof
thetasksweconsider. Formultiplechoicequestioninganswering(MMLU),weusetheauxiliarytrainingset
accompanyingtheHFMMLUrepository†. Itcontainsaround100,000trainingexamplesandaround30M
tokens. For mathematical reasoning (GSM8K), we combine the the MetaMathQA (Yu et al., 2023b) and
Orca-Math(Mitraetal.,2024)datasets,totalling600,000trainingexamplesandapproximately200Mtokens.
Wefine-tunemodelsforthreeepochsusingstandardhyperparameterchoices,withminimalhyperparameter
tuning,seeAppendixA.2. Notethattheamountofcomputerequiredforfine-tuningisminimalincomparison
tothecomputerequiredforpretraining,sinceallmodelsconsideredwerepretrainedonatleast300Btokens.
We plot model scores on MMLU and GSM8K after fine-tuning in Figure 1 (bottom). We observe that
afterfine-tuningontaskrelevantdata,newermodelsnolongerParetodominateintermsofaccuracyper
pretrainingcompute. Instead,benchmarkperformanceisstronglycorrelatedwithcomputeandbothnewer
and older models follow remarkably similar scaling trends. That is, newer models no longer appear to
outperformoldermodels. Moreover,weobservethatoldermodelstendtobenefitfromtrainingonthetest
taskmuchmorethannewermodels,seeFigure2. Theimprovementsofoldermodelsarestriking,often
jumpingfromrandomchanceaccuracytodoubledigitimprovementsinaccuracy. Incontrast,fine-tuning
bringscomparativelylittlebenefittonewermodels. Thisobservationsuggeststhatnewermodelshavealready
beentrainedonasubstantialamountoftask-relevantdata.
2.2 Quantifying performance differences between newer and older models
We draw inspiration from scaling laws (Kaplan et al., 2020) in how we model benchmark accuracy Ato
scalelog-linearlywithpretrainingcomputeC. Toaccountforemergence(Weietal.,2022),weassumethat
†https://huggingface.co/datasets/cais/mmlu
4MMLU GSM8K
0.4
0.2
0.1 0.2
0.0 0.0
1020 1021 1022 1023 1024 1020 1021 1022 1023 1024
Pretraining compute (FLOPs) Pretraining compute (FLOPs)
Models trained Before November 2023 After November 2023
Figure2: ModelstrainedbeforeNovember2023tendtobenefitmuchmorefromfine-tuningontaskdata.
modelsperformatthetask’srandomchanceaccuracy r uptoscalingtosomepointofemergencec . Welet
e
thevariableN denotewhetheramodelwastrainedafterNovember2023,andregressthemodel
A=αmax(0,logC−c )+θN+r+ε, (1)
e
where α, θ and c are the fit’s parameters, and ε is random noise. We focus on the coefficient θ, which
e
correspondstotheaveragedifferenceinbenchmarkperformancebetweennewerandoldermodelsafter
controllingforpretrainingcompute. WefitthemodelinEquation1,andreporttheregressioncoefficient
θ inFigure1. WeobtainR2>0.9forallmodelfits. Beforeadjustingfortesttasktraining,theestimated
differenceinperformanceθ (cid:98)betweennewerandoldermodelsarestatisticallysignificant,positive,andlarge.
Specifically,recentmodelsonaverageoutperformolderonesbyover7accuracypointsinMMLUand17
accuracypointsinGSM8K.Theseareremarkabledifferencesinbenchmarkperformance,assmallsingledigit
improvementsaretypicallyconsideredsubstantialimprovementsbytheliterature.
Werepeattheanalysisbutusingmodels’adjustedbenchmarkscores,thatis,thoseobtainedafterfine-
tuningonthetesttask. Afteradjustingfortesttasktrainingwefindnoevidenceforasignificantdifference
inbenchmarkperformancebetweennewerandoldermodels. Thatis,theestimatedcoefficientθ (cid:98)isboth
smallandnotstatisticallysignificant. Putsimply,newermodelsnolongeroutperformolderones. Therefore,
conditionedonallmodelstrainingonthesame,sufficientamountoftask-specificdatabeforeevaluation,
therearenodifferencesinbenchmarkperformancebetweennewerandoldermodels.
Ourfindingsprovideevidencethatthedifferencesinbenchmarkperformancebetweennewerandolder
modelsarelargelyattributabletodifferencesintesttasktraining. Wepresentacausalinterpretationofour
resultsinAppendixB,outlyingthecausalassumptionsneededtoestablishthattheimprovementsofnewer
modelsareattributabletotrainingonthetesttask. Overall,wefindnoevidencefortheimprovementsin
performanceofnewermodelsbeingattributabletoanythingotherthantrainingmoreonthetesttask.
WeincludeinAppendixB.1arobustnesscheckonthetemporalsplitchosen,byinsteaddivingmodels
basedonwhethertheyweretrainedprimarilyonEnglishlanguagedata. Weobtainsimilardifferencesin
performance,whichweinterpretasavaluablerobustnesscheckofourresults. InAppendixCweinstead
considerthebenchmarksofthenewlyreleasedHFOpenLLMLeaderboardv2(Fourrieretal.,2024a). Whereas
the HF leaderboard v2 pays particular attention to guarding against data contamination (Fourrier et al.,
2024b),wenonethelessfindevidencethattrainingonthetesttaskconfoundsallbenchmarksincludedinthe
Leaderboardv2. Thesefindingshighlightthattrainingonthetesttaskisadistinctphenomenonfromdata
contamination,andnewmethods–suchasourproposedadjustmentprocedure–arerequiredtomitigatethe
confoundingeffectoftrainingonthetesttaskonbenchmarkevaluations.
3 Recreating differences in benchmark performance
Previously,weintroducedawaytoadjustfortrainingonthetesttask. Herewesystematicallytestthevalidity
ofthisadjustmentmethod. Todoso,wedemonstratehowtorecreatetheobserveddifferencesinperformance
5
ycarucca
ni
niaG
tnemtsujda
retfaNo model fine-tuned Some models fine-tuned Some models fine-tuned
(adjusted)
0.6 =0.092 =0.005
0.5
0.4
0.3
0.6 =0.194 = 0.016
0.4
0.2
0.0
1020 1021 1022 1023 1020 1021 1022 1023 1020 1021 1022 1023
Pretraining compute Pretraining compute Pretraining compute
Figure3: ModelstrainedbeforeNovember2023(●)withoutfine-tuningand(●)afterfine-tuningonthetest
task. Theirdifferenceinbenchmarkperformanceθ (cid:98)resemblethatbetweennewerandoldermodels. After
adjustingbytrainingonthetesttask,theirdifferencevanishes. Boldindicatessignificancewith p<0.05.
betweennewerandoldermodelsbyactivelymanipulatinghowmuchmodelstrainonthetesttask.
We do so in two ways. First, we fine-tune older models on task relevant data (Section 3.1). Second,
wereformulatecertaintesttaskstousemultiplechoicepromptsinstead“cloze”evaluation(Section3.2).
Bothexperimentsturnouttorecreatethekindofperformancedifferenceweobservedearlier. Thisnotonly
providesfurtherevidencethatdifferencesinperformancebetweenolderandnewermodelsarelinkedtotest
tasktraining. Italsodemonstrateshowtesttasktrainingdistortsbenchmarkevaluations.
Fortunately,inbothcases,weshowthatfine-tuningmodelsontask-relevantdatabeforeevaluationisan
effectivemechanismformitigatingthebiasintroducedbytrainingonthetesttask.
3.1 Fine-tuning on the test task
Forthissection,weonlyconsidermodelstrainedbeforeNovember2023,sincewehypothesizethatolder
modelsdonottrainonthetesttaskmuch. Werandomlysplitmodelsintotwocohorts: acontrolgroupand
atreatmentgroup. Wefine-tunethetreatmentgrouponthedatasetsoftask-relevantdataintroducedin
Section2. Wefine-tuneoneachdatasetindependently,forasingleepoch. Wethenevaluatethebenchmark
performanceofthetwocohorts,aswellastheirperformanceafteradjustingfortesttasktraining. Asinthe
previoussection,weadjustfortesttasktrainingbyfine-tuningallmodelsonthetesttaskbeforeevaluation.
WeplotinFigure3thetwocohorts’benchmarkperformancebeforeandaftertheadjustment. Werepeat
thestatisticalanalysisofSection2.2andreporttheestimatedcoefficientθ′
indicatingtheaveragedifference
inbenchmarkperformancebetweenthetwocohortswhencontrollingforcompute.
Fine-tuningthetreatmentgroupresultsinlargedifferencesinperformancebetweenthecontrolgroupand
thetreatmentgroup,seeFigure3middle. Qualitatively,thedifferencesbetweenthecontrolandtreatment
groupresemblethoseobservedbetweennewerandoldermodelsinSection2.2. Inparticular,thefine-tuned
modelsParetodominatethenonfine-tunedmodels. Quantitatively,theestimatedincreaseinperformance
θ (cid:98)′ due to fine-tuning is statistically significant and large. Importantly, it is also similar to the difference
inperformanceθ (cid:98)betweennewerandoldermodelsestimatedinSection2.2. Therefore,fine-tuningolder
modelsonthetesttaskgivesrisetoqualitativelyandquantitativelysimilarconfoundingtothatobserved
betweennewerandoldermodels. Thisisconsistentwithourrunninghypothesisthatnewermodelsare
largelyequivalenttooldermodelsthattrainedonthetesttask.
Afteradjustingfortesttasktrainingbyfurtherfine-tuningboththecontrolandtreatmentgroupsonthe
testtask, weobservethat modelsinthetreatment grouparenolonger outliersinterms ofperformance-
per-compute,seeFigure3right. Quantitatively,theestimatedincreaseinperformanceθ (cid:98)′ isbothsmalland
6
ULMM
K8MSG
ycaruccA
ycaruccACloze evaluation Multiple choice Multiple choice
(adjusted)
=0.001 =0.120 = 0.014
0.75
0.50
0.25
= 0.012 =0.114 =0.009
0.75
0.50
0.25
1020 1021 1022 1023 1024 1020 1021 1022 1023 1024 1020 1021 1022 1023 1024
Pretraining compute Pretraining compute Pretraining compute
Figure4: ReformulatingARCandHellaSwagasMMLU-stylequestionsgiverisetolargedifferencesθ (cid:98)between
modelstrained(●)beforeNovember2023and(●)afterNovember2023. Afteradjustingbyfine-tuningon
thetesttask,differencesinperformancevanish. Boldindicatessignificancewith p<0.05.
notstatisticallysignificant. Wethereforevalidateavitalsoundnesspropertyoftheproposedadjustment
procedure: afterdeliberatelytrainingsomemodelsonthetesttask,wecanundotheiradvantageoverother
modelsbyfurthertrainingallmodelsonthetesttask.
3.2 Reformulating the test task
In this section we consider two additional benchmarks from the HF leaderboard: ARC Challenge (Clark
etal.,2018)andHellaSwag(Zellersetal.,2019). SimilarlytoMMLU,ARCiscomprisedofgrade-school
levelquestions. HellaSwaginsteadtestsforcommonsensereasoning. LikeMMLU,thequestionsinARCand
HellaSwagareaccompaniedbyfourpossibleanswers,amongwhichthemodelmustdifferentiatethecorrect
one. ThestandardMMLUevaluationformulatesquestionsasmultiple-choice: allfouranswerchoicesare
listed,andthemodelispromotedtopickone. Incontrast,ARCandHellaSwaguse“cloze”evaluations: a
models’answeristakentobethatwiththelargestcompletionlikelihoodgiventheinputquestion.
We evaluate all models on ARC and HellaSwag using the standard cloze evaluation, and plot their
benchmarkperformanceinFigure4left. WerepeatthestatisticalanalysisofSection2.2, andreportthe
average difference in performance θ between newer and older models after controlling for pretraining
compute. Qualitatively,weobservethatoldermodelsandnewermodelshaveverysimilarscalingtrends.
Quantitatively,theestimateddifferenceinperformancebetweennewerandoldermodelsθ (cid:98)issmallandnot
statisticallysignificant. Thatis,newermodelsdonotoutperformoldermodelsonARCandHellaSwag.
WethenreformulateARCandHellaSwagasMMLU-stylemultiple-choicequestions,andplottheresulting
benchmarkperformanceinFigure4center. Weobservelargedifferencesinperformancebetweennewer
andoldermodels. Qualitatively,thesedifferencesinperformanceresemblethoseobservedforMMLU.In
particular,newermodelsParetodominateintermsofperformance-per-compute. Quantitatively,wefindthe
differenceinperformanceθ (cid:98)betweennewerandoldermodelstobesignificant,positive,andlarge,andtobe
roughlysimilarinmagnitudetothatestimatedforMMLUinSection2.2. Therefore,reformulatingthetest
taskasmultiplechoicequestionansweringleadstosimilarconfoundingtothatobservedforMMLU.This
suggestthatwhatcausestheoutliersinMMLUislikelynotmemorizationofspecifictestingdata(i.e.,dueto
datacontaminationorleakage),butratheranimprovedabilityforMMLU-styleprompts.
Weadjustfortesttasktrainingbyfine-tuningallmodelsontheMMLUauxiliarytrainingset,andplot
theirARCChallengeandHellaSwagscoresinFigure4. Weobservethatnewermodelsarenolongeroutliers
intermsofperformance-per-compute. Moreover,wenolongerfindevidenceofasignificantdifferencein
performancebetweennewerandoldermodels. Theproposedadjustmentisthereforeeffectiveinremoving
7
CRA
gawSalleH
ycaruccA
ycaruccAMMLU - Multiple choice MMLU - Cloze MMLU - Multiple choice
0.5 0.4
=0.068 = 0.008
0.6
0.4 0.6
0.4
0.3 0.8
1020 1021 1022 1023 1024 1020 1021 1022 1023 1024 1020 1021 1022 1023 1024
Pretraining compute Pretraining compute Pretraining compute
Figure5: WhenevaluatingMMLUusing“cloze”prompts,modelstrained(●)afterNovember2023nolonger
outperformthosetrained(●)beforeNovember2023(middle). WhenusingBrierscoreastheevaluation
metric,westillobservesharpimprovementsinperformancebetween1022 and1023 FLOPs(right).
theconfoundingresultingfromnewermodelsoverperformingforMMLU-styleprompts.
WhatdoesMMLUtestfor? WeevaluateMMLUusingthe“cloze”methodologyinsteadoftheusualmultiple-
choiceprompts. WeplottheresultsinFigure5center. Withclozeevaluations,newermodelsarenolonger
are outliers in terms of MMLU performance. In fact, the difference in performance between newer and
oldermodelsisnowsmallandnotstatisticallysignificant. ThissuggeststhatthestandardMMLUevaluation
conflatesknowledge-testingwithtestingamodels’abilitytoanswermultiplechoicequestions. Forinstance,
smallernonfine-tunedmodelssufferfromparticularlystrongA-biasformultiple-choicequestions(Dominguez-
Olmedoetal.,2023). NewermodelsthereforeattainhigherMMLUscoreslargelybecausetheyarebetterat
multiple-choicequestionanswering,andnotbecausetheynecessarily“knowmore”.
4 Implications for model comparisons
OurfindingsindicatethattrainingonthetesttaskactsasamajorconfounderofLLMbenchmarkevaluations.
Wenowdiscussitsimplicationsfortherelativecomparisonofmodelfamilies(Section4.1),aswellasits
implicationsformeasuringprogressinmodelcapabilitiesovertime(Section4.2).
4.1 Comparing model families
WecomparetheMMLUandGSM8KperformanceofthePythia,Llama2,andQwen1.5modelfamilies,which
likelytrainonthetesttasktoverydifferentextents. PythiawastrainedonthePile(Gaoetal.,2020), a
collectionofcurateddatasetsthatareunlikelytocontainmuchtesttaskdata. Llama2wastrainedmostly
onwebdata,whichisreasonabletoassumemaycontaintesttaskdata. Lastly,Qwen1.5explicitlyincludes
instructiondatainitspretrainingmixture,thuslikelytrainingonthetesttasktoalargeextent.
In Figure 6 we plot the MMLU and GSM8K scores of the Llama 2, Qwen 1.5, and Pythia families of
models,aswellastheiradjustedaccuracy(i.e.,afterfine-tuningontaskrelevantdata). Withoutadjustment,
Qwen1.5appearstobethesuperiormodelfamily: itParetodominatesboththeLlama2andPythiamodels.
Furthermore,allPythiamodelsperformnobetterthanrandomchance,andthusitisunclearwhatbenefit
scalingPythiamightbring. Afterfine-tuningthemodelsonthethetesttask,however,allthreemodelfamilies
exhibitverysimilarscalingtrends. Therefore,whencorrectingfortheconfoundingintroducedbytesttask
trainingitisunclearifanyofthemodelfamiliesissuperiortotheothersbeyondtheirpretrainingcompute.
Interestingly,recentworkequatespretrainingdataqualitywithdownstreambenchmarkperformance(Penedo
etal.,2024;Lietal.,2024). Forexample,thePile(Pythia’spretrainingdataset)isthoughttobeinferior
tofilteredwebdata. Inlightofourfindings,itisplausiblethatamajorcontributingfactortothesuperior
performanceof“higherquality”pretrainingdatasetsisthattheycontainalargershareoftesttaskdata.
8
ycaruccA ycaruccA
erocs
reirB-MMLU GSM8K
Unadjusted Adjusted Unadjusted Adjusted
0.7 0.75
0.50
0.5
0.25
0.3
0.00
1022 1023 1024 1022 1023 1024 1022 1023 1024 1022 1023 1024
Pretraining compute Pretraining compute Pretraining compute Pretraining compute
Llama 2 Qwen 1.5 Pythia
Figure6: Trainingonthetesttaskconfoundsrelativecomparisonsbetweenmodelfamilies. Afteradjustingfor
testtasktraining,noneofthethreemodelfamiliesappearstobesuperiorbeyondtheirpretrainingcompute.
MMLU GSM8K
Unadjusted Adjusted Unadjusted Adjusted
0.7 0.75
0.18 0.03 0.42 0.08
0.50
0.5
0.25
0.3
0.00
1022 1023 1024 1022 1023 1024 1022 1023 1024 1022 1023 1024
Compute Compute Compute Compute
Pareto front of Models trained before November 2023 All models
Figure7: Trainingonthetesttaskoverestimatestheimprovementsmadebyrecentbasemodelsintermsof
performance-per-compute. Afteradjustment,theareaofimprovement(green)reducesbyasixfold.
4.2 Progress in model capabilities
Onepurposeofbenchmarksistotrackprogressinmodelcapabilities. InFigure7weplottheParetofrontier
ofbenchmarkaccuracyagainstpretrainingcompute,bothformodelstrainedbeforeNovember2023and
forallmodels. WemeasureprogressbyconsideringtheareaofimprovementoftheParetofrontiersince
November 2023, shaded in green. Without adjustment, the difference between the two Pareto frontiers
is rather large for both MMLU and GSM8K, indicating substantial progress since November 2023. After
fine-tuningmodelsonthetestask,however,theareaofimprovementreducesbyasixfold. Therefore,the
confoundingintroducedbytesttasktrainingleadstosubstantiallyoverestimatingtheprogressinMMLUand
GSM8Kcapabilitiesperunitofcomputeachievedbyrecentmodelfamilies.
Ontheotherhand,recentmodelstendtobetrainedonmoredata. GiventheChinchillascalinglaws(Hoff-
mannetal.,2022),itisremarkablethatnewer,smallermodelsmatchtheperformanceofolder,largeronesfor
thesameamountofpretrainingcompute. Sinceinferenceandfine-tuningofsmallermodelsissubstantially
cheaper,recentmodelscanbemuchmoreaccessibletolesswell-resourcedinstitutions,withlittlecostin
performance. Forexample,wefindthatLlama38BcloselymatchestheperformanceofLlama270B.
5 Implications for emergence
Throughoutourevaluations,weobserveemergentbehaviourforMMLUandGSM8K:modelsperformatnear
randomchanceuptoacertainscaleofpretrainingcompute,followedbyrelativelysharperimprovements
inperformanceatlargerscales(Weietal.,2022). Aftertrainingonthetesttask,however,emergencefor
MMLUandGSM8Kappearstooccuratsubstantiallylowerscales. Wededicatethissectiontomoreclosely
investigatetherelationshipbetweentrainingonthetesttaskandemergence.
9
ycaruccA
ycaruccA
ycaruccA
ycaruccATask examples: 0 Task examples: 6.4k Task examples: 16k Task examples: 64k
0.70
ce: 1.3e+22 ce: 5.6e+21 ce: 1.1e+21 ce: 5.6e+20
0.50
0.25
Task examples: 0 Task examples: 16k Task examples: 64k Task examples: 1600k
0.8
0.6
ce: 2.2e+22 ce: 9.1e+21 ce: 4.2e+21 ce: 5.8e+20
0.4
0.2
0.0
1020 1021 1022 1023 1024 1020 1021 1022 1023 1024 1020 1021 1022 1023 1024 1020 1021 1022 1023 1024
Pretraing compute Pretraing compute Pretraing compute Pretraing compute
Figure8: Thepointofemergencec arisesatincreasinglylowerscalesasmodelstrainonthetesttask.
e
Task examples: 0 Task examples: 6.4k Task examples: 16k Task examples: 64k
0.70 R2=0.632 R2=0.804 R2=0.913 R2=0.950
0.50
0.25
Task examples: 0 Task examples: 16k Task examples: 64k Task examples: 320k
0.8
R2=0.515 R2=0.673 R2=0.783 R2=0.857
0.6
0.4
0.2
0.0
1020 1021 1022 1023 1024 1020 1021 1022 1023 1024 1020 1021 1022 1023 1024 1020 1021 1022 1023 1024
Pretraining compute Pretraining compute Pretraining compute Pretraining compute
Figure9: Trainingonthetesttaskyieldsincreasinglybetterlog-linearscalingfits(higherR2).
Emergencearisesatlowerscaleswithincreasedtesttasktraining. Weconsideronlymodelstrained
beforeNovember2023,aswehaveestablishedthatthesemodelstrainonthetesttasktoalesserextent. We
evaluatethemodelsatintermediatecheckpointsastheytrainonthedatasetsoftaskrelevantdataintroduced
inSection2.1. Wefitαandc inEquation1tothedifferentintermediatecheckpoints,andreportinFigure8
e
the corresponding points of emergence c . We find that emergence arises at increasingly lower compute
e
regimesasmodelsincreasinglytrainonthetesttask. Forinstance,forMMLUthenon-finetunedmodels
exhibitemergenceataround1022 FLOPs,roughlythescaleofPythia6.9B.Aftertrainingon64,000examples,
emergence arises around around 6·1020 FLOPs, that is, roughly the scale of Pythia 410M. That is, the
benchmarkperformanceofmodelsaftertrainingonthetesttaskispredictableatsubstantiallylowerscales.
Trainingonthetesttaskyieldsincreasinglybetterlog-linearfits. Thelog-linearrelationshipbetween
pretraininglossandcomputeiswell-established(Kaplanetal.,2020;Hoffmannetal.,2022). Weobserve
that, for the compute ranges that we consider, training on the test task increasingly recovers log-linear
scalingbetweenpretrainingcomputeandbenchmarkaccuracy. Similarlytotheearliersection,weevaluate
intermediatecheckpointsbutinsteadfitlog-linearfunctionsinFigure9. WeobservethattheR2 ofthefit
improvessubstantiallyasthemodelstrainonmoretask-relevantdata. ForMMLU,theR2 valuejumpsfrom
0.63to0.95aftertrainingon64,000examples. Therefore,aftertrainingonthetesttaskalmostallofthe
variationinaccuracycanbeexplainedbylog-linearscalingofpre-trainingcompute.
Discussion. Schaefferetal.(2024a)arguethatemergenceappearsduetothechoiceofmetric. Tomitigate
emergence,theysuggesttoconsiderBrierscoreinsteadofaccuracy. Weobserve,however,thattheemergent
behaviourofMMLUdoesnotdisappearwhenusingtheBrierscore,seeFigure5right,northatofARCand
10
ULMM
ULMM
K8MSG
K8MSG
ycaruccA
ycaruccA
ycaruccA
ycaruccAHellaSwagwhenframedasmultiple-choicequestions,seeFigure16inAppendixD.Whilemorecomplex
changesofmetricmightresolvetheemergenceinmultiple-choiceQA(Schaefferetal.,2024b),wediscuss
twopracticalsolutionstoobtainpredictivescalingwhilemaintainingaccuracyastheevaluationmetric.
ForMMLUandmultiple-choicebenchmarksmorebroadly,weconsistentlyobservethatclozeevaluations
yieldsmootherandmorepredictablescalingevenwhenusingaccuracyastheevaluationmetric. Sincethe
purposeofthesebenchmarksisknowledge-testingmoresothantestingmultiple-choiceansweringability,
clozeevaluationsshouldbepreferableinsofarpredictivescalingisanimportantconsideration.
Morebroadly,ifsufficienttaskrelevantdataisavailable,thentrainingonthetesttaskcanresultinmuch
morepredictablescalingbyshiftingemergencetosmallercomputescales. Crucially,theevaluationmetric
andmethodologyneednotbechanged. Notethatinmanysettingsitisnotaprioriapparentwhatmetric
orevaluationmethodologymightresultsinpredictivescaling. Scalinglawsafterfine-tuningcorrespondto
thoseofmore“specialist”models,whichforsomedomains–suchasthelegaldomain(Dominguez-Olmedo
etal.,2024)–orpurposes–e.g.,safety–mightbepreferabletothescalinglawofgeneralistmodels.
6 Related work
Benchmarks have played a central role in both machine learning (Hardt and Recht, 2022) and natural
languageprocessing(Storksetal.,2019). Classically,benchmarkscomprisedbothatestsetandareasonably
largetrainingset(Garofoloetal.,1993;LeCunetal.,1998;SangandDeMeulder,2003;Koehn,2005;Deng
etal.,2009). Modelsweretrainedonthesametrainingset,andthenevaluatedontheaccompanyingtestset.
Thesuccessofunsupervisedlanguagemodelling(Petersetal.,2018;KentonandToutanova,2019;Radford
etal.,2019),however,haschangedthisparadigm. Firstly,present-daylanguagemodelsdifferintheirtraining
data,whichisnotstandardizedbutrathertreatedasadesignchoice(Raffeletal.,2020;Albalaketal.,2024;
Lietal.,2024). Secondly,languagemodelsareapriorinottrainedwiththeexplicitobjectiveofmaximizing
anysinglebenchmarkscore. Rather,languagemodelsareexpectedtobeabletoperformabroadrangeof
tasks(Wangetal.,2018;Brownetal.,2020). Consequently,modelsareevaluatedandcomparedusinga
pluralityofbenchmarks(Beechingetal.,2023;Liangetal.,2023;Srivastavaetal.,2023).
Data contamination. Data contamination or test-set contamination refers to any overlap between the
training and the test data such that test results overestimate a model’s generalization performance. The
scaleandoftenlittlecurationofpresent-daypretrainingcorporaexacerbatesdatacontaminationconcernsin
languagemodelevaluations(Jiangetal.,2024). Consequently,datacontaminationisusuallydiscussedinthe
technicalreportsaccompanyingmodelreleases(Radfordetal.,2019;Brownetal.,2020;Chowdheryetal.,
2023;Touvronetal.,2023b). However,detectingandpreventingdatacontaminationiscurrentlyanopen
problem(Gunasekaretal.,2023;Yangetal.,2023b;GolchinandSurdeanu,2023). Robertsetal.(2023)
andLiandFlanigan(2024)findthatmodelsoftenperformbetterondatasetsthatwerepubliclyavailable
duringmodeltraining. WhilealmostallmodelsthatweconsiderwerereleasedafterMMLUandGSM8K,we
nonethelessfindthat,controllingforcompute,morerecentmodelsperformbetter. Theseperformancegains
areunlikelytobedrivensolelybytestsetleakageandrequireadditionalexplanation.
Trainingonthetesttask. Theeffectivenessoffine-tuningonthetrainingsetaccompanyingLLMbench-
marksiswell-known(Weietal.,2021;Wangetal.,2022;Chungetal.,2024). Consequently,manyinfluential
instruction-tuningdatasetscontainorarepartlyderivedfrombenchmarktraindata(Weietal.,2021;Honovich
etal.,2022;Mukherjeeetal.,2023). LiandFlanigan(2024)identifysmallamountsofbenchmark-specific
datainthepubliclyavailableAlpaca(Taorietal.,2023)andVicuna(Chiangetal.,2023)instruction-tuning
sets. Zhouetal.(2023b)empiricallyanalyzetheeffectsoffine-tuningonbenchmark-specificdataandwarn
aboutitsimpactsonbenchmarkvalidity. Tocircumventtheseissues,recentworkhasfocusedonindirect
indicatorsofbroaderdatacontamination,suchasalackofrobustnesstotasktransformations(Wuetal.,
2023),orunderperformanceonbenchmarkswithnoveltaskcombinations(Yuetal.,2023a). Incontrast,
wefindevidencefortrainingonthetesttaskwithouttheneedforexplicitlyidentifyingspecificdatapoints
11usedattrainingtime,ormodifyingtasks. Inaddition,ourmethodallowsustoquantifyandcorrectforthe
effectsoftrainingonthetesttaskonbenchmarkperformance.
Emergentabilitiesoflanguagemodels. Emergentcapabilities(Weietal.,2022;Gangulietal.,2022)
refertolevelsofmodelperformanceatlargescalesthatcannotbeeasilypredictedbyextrapolatingfrom
smaller scales. Wei et al. (2022) report emergent capabilities for various benchmarks including MMLU
andGSM8K(Srivastavaetal.,2022). However,Srivastavaetal.(2022);Schaefferetal.(2024b)findthat
thelog-probabilityofthecorrectansweroftenimprovessmoothly,evenwhenothermetricsseemtoshow
emergence. Luetal.(2023)arguethatmostemergentcapabilitiescanbeexplainedbyin-context-learning.
Schaefferetal.(2024a)arguethatemergentcapabilitiesaremostlyanartifactofnon-linearanddiscontinuous
evaluationmetricslikeaccuracy. Incontrast,wefindsignsofemergenceontaskslikeMMLU,evenwhen
usingcontinuousmetricsliketheBrierscore. Weadditionallyshowthatfine-tuningonthetesttaskyields
morepredictivescalingbyshiftingthepointofemergencetosubstantiallysmallercomputescales.
7 Discussion
The1968OlympicstookplaceinMexicoCityatthesignificantaltitudeof2340meters,higherthanAustralia’s
tallestpeak. Runnerswhohadtrainedataltitudeintheirhomecountrieswerebetterpreparedtocompete
inMexicoCity’sconditions,asitturnedout. ButthehotlydebatedresultsoftheGamesdidnotleadthe
organizers to prohibit training at natural altitude. Instead, they let everyone do it; and athletes came to
consideraltitudetraininganexcellentwaytotrain.
Theanecdoteholdsalessonfortheevaluationoflargelanguagemodelshalfacenturylater. Knowledge
abouttheevaluationconditionsnecessarilyinfluencestrainingpracticesundercompetitivepressure. Itmay
beafool’serrandtoprohibitthepractice. Instead,weproposetoadjustforitbygivingeverymodelthesame
task-specificpreparationbeforeevaluation. Weworkfromtheassumptionthattrainingonthetesttask,in
general,cannotbeeffectivelydetected,disallowed,ordisincentivized. Detectingwhattrainingdataamodel
hasseenisanotoriouslydifficultproblem—existingheuristicsachievepartialsuccessatbest. Researchers
routinelyacknowledgethefutilityoffightingdatacontamination. Moreover,weanticipatethatthewaysto
effectivelytrainonthetesttaskwillonlygrowinscopeandadoption.
Ourworkdemonstratesthatcomparisonsofdifferentmodelsareconfoundedbythechoiceoftrainingdata
andtrainingpractices. Differentmodelfamiliesvaryinthedegreethattheywere—implicitlyorexplicitly—
trainedonvarioustesttasks. Itthereforemakeslittlesensetocomparemodelperformanceatfacevalue
withoutaccountingforhowthetrainingdatarelatetothetesttask. Thesameproblemextendstoscaling.
Smallermodelscanappearunexpectedlyperformantiftheyweretrainedtoagreaterextentontaskdata.
Wecanapplythesameprinciplestoemergentbehavior. Aftertrainingonthetesttask,modelcapabilities
becomepredictableatsmallermodelsizeandgrowcontinuouslywithscale. Thisisnottosaythatemergence
isn’treal;itmaywellbearealphenomenonforafixedchoiceofdatasetandevaluationmetric. Buttraining
onthetesttaskremovestheunpredictabilityanddiscontinuityassociatedwithemergence,notablywithout
anychangeinthemetric,thuslargelydisarmingtheominousnatureofemergence.
Despitethedauntingchallengesthattrainingonthetesttaskposesforthefairevaluationoflanguage
models, it’s also its own best remedy. Giving each model the same sufficient task-specific fine-tuning
harmonizesmodelcomparisons,deconfoundsscalinglaws,andlinearizestherelationshipbetweenmodel
capabilitiesandlog-scale. Wehopethatourworkinformsstrongerevaluationstandardsthataddresscentral
challengesinthecurrentevaluationecosystem. Ourproposalhastheaddedsidebenefitofcreatingincentives
formodelbuilderstocreatemodelsthatcanbefine-tunedeasilyandrespondwelltofine-tuning.
References
JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,Diogo
Almeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport. arXivpreprint
arXiv:2303.08774,2023.
12AlonAlbalak,YanaiElazar,SangMichaelXie,ShayneLongpre,NathanLambert,XinyiWang,NiklasMuen-
nighoff,BairuHou,LiangmingPan,HaewonJeong,etal. Asurveyondataselectionforlanguagemodels.
arXivpreprintarXiv:2402.16827,2024.
Ebtesam Almazrouei, Hamza Alobeidli, Abdulaziz Alshamsi, Alessandro Cappelli, Ruxandra Cojocaru,
Mérouane Debbah, Étienne Goffinet, Daniel Hesslow, Julien Launay, Quentin Malartic, et al. The fal-
conseriesofopenlanguagemodels. arXivpreprintarXiv:2311.16867,2023.
AIAnthropic. Theclaude3modelfamily: Opus,sonnet,haiku. Claude-3ModelCard,2024.
JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,WenbinGe,YuHan,Fei
Huang,etal. Qwentechnicalreport. arXivpreprintarXiv:2309.16609,2023.
EdwardBeeching,ClémentineFourrier,NathanHabib,SheonHan,NathanLambert,NazneenRajani,Omar
Sanseviero,LewisTunstall,andThomasWolf. OpenLLMleaderboard. HuggingFace,2023. URLhttps:
//huggingface.co/spaces/HuggingFaceH4/open_llm_leaderboard.
Marco Bellagente, Jonathan Tow, Dakota Mahan, Duy Phung, Maksym Zhuravinskyi, Reshinth Adithyan,
JamesBaicoianu,BenBrooks,NathanCooper,AshishDatta,etal. Stablelm21.6btechnicalreport. arXiv
preprintarXiv:2402.17834,2024.
StellaBiderman,HaileySchoelkopf,QuentinGregoryAnthony,HerbieBradley,KyleO’Brien,EricHallahan,
MohammadAflahKhan, ShivanshuPurohit, USVSNSaiPrashanth, EdwardRaff, etal. Pythia: Asuite
foranalyzinglargelanguagemodelsacrosstrainingandscaling. InInternationalConferenceonMachine
Learning,pages2397–2430.PMLR,2023.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners.
Advancesinneuralinformationprocessingsystems,33:1877–1901,2020.
ZhengCai,MaosongCao,HaojiongChen,KaiChen,KeyuChen,XinChen,XunChen,ZehuiChen,ZhiChen,
PeiChu,etal. Internlm2technicalreport. arXivpreprintarXiv:2403.17297,2024.
Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,SiyuanZhuang,
YonghaoZhuang,JosephEGonzalez,etal. Vicuna: Anopen-sourcechatbotimpressinggpt-4with90%*
chatgptquality. Seehttps://vicuna.lmsys.org(accessed14April2023),2(3):6,2023.
AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,Paul
Barham,HyungWonChung,CharlesSutton,SebastianGehrmann,etal. Palm: Scalinglanguagemodeling
withpathways. JournalofMachineLearningResearch,24(240):1–113,2023.
HyungWonChung,LeHou,ShayneLongpre,BarretZoph,YiTay,WilliamFedus,YunxuanLi,XuezhiWang,
MostafaDehghani,SiddharthaBrahma,etal. Scalinginstruction-finetunedlanguagemodels. Journalof
MachineLearningResearch,25(70):1–53,2024.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind
Tafjord. Thinkyouhavesolvedquestionanswering? tryarc,theai2reasoningchallenge. arXivpreprint
arXiv:1803.05457,2018.
KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,MatthiasPlappert,
JerryTworek,JacobHilton,ReiichiroNakano,etal. Trainingverifierstosolvemathwordproblems. arXiv
preprintarXiv:2110.14168,2021.
JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei. Imagenet: Alarge-scalehierarchical
image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248–255,
2009. doi: 10.1109/CVPR.2009.5206848.
13RicardoDominguez-Olmedo,MoritzHardt,andCelestineMendler-Dünner. Questioningthesurveyresponses
oflargelanguagemodels. arXivpreprintarXiv:2306.07951,2023.
RicardoDominguez-Olmedo,NandaVendant,RedietAbebe,StefanBechtold,ChristophEngel,JensFranken-
reiter,KrishnaGummadi,MoritzHardt,andMichaelLivermore. Lawma: Thepowerofspecializationfor
legaltasks. 2024.
RichardO.DudaandPeterE.Hart. PatternClassificationandSceneAnalysis. WileyNewYork,1973.
EleutherAI. Languagemodelevaluationharness. https://github.com/EleutherAI/lm-evaluation-harness,
2024. Accessed: 2024-05-20.
ClémentineFourrier,NathanHabib,AlinaLozovskaya,KonradSzafer,andThomasWolf.Openllmleaderboard
v2. https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard,2024a. Accessed:
2024-07-08.
ClémentineFourrier,NathanHabib,AlinaLozovskaya,KonradSzafer,andThomasWolf. Performancesare
plateauing,let’smaketheleaderboardsteepagain. https://huggingface.co/spaces/open-llm-leaderboard
/blog,2024b. Accessed: 2024-07-08.
RuyiGan,ZiweiWu,RenliangSun,JunyuLu,XiaojunWu,DixiangZhang,KunhaoPan,PingYang,QiYang,
JiaxingZhang,etal. Ziya2: Data-centriclearningisallllmsneed. arXivpreprintarXiv:2311.03301,2023.
DeepGanguli,DannyHernandez,LianeLovitt,AmandaAskell,YuntaoBai,AnnaChen,TomConerly,Nova
Dassarma,DawnDrain,NelsonElhage,etal. Predictabilityandsurpriseinlargegenerativemodels. In
Proceedingsofthe2022ACMConferenceonFairness,Accountability,andTransparency,pages1747–1764,
2022.
LeoGao,StellaBiderman,SidBlack,LaurenceGolding,TravisHoppe,CharlesFoster,JasonPhang,Horace
He,AnishThite,NoaNabeshima,ShawnPresser,andConnorLeahy. ThePile: An800gbdatasetofdiverse
textforlanguagemodeling. arXivpreprintarXiv:2101.00027,2020.
John S Garofolo, Lori F Lamel, William M Fisher, Jonathan G Fiscus, and David S Pallett. Darpa timit
acoustic-phoneticcontinousspeechcorpuscd-rom.nistspeechdisc1-1.1. NASASTI/Recontechnicalreport
n,93:27403,1993.
TeamGemini,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,RaduSoricut,
JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighlycapablemultimodalmodels.
arXivpreprintarXiv:2312.11805,2023.
TeamGemma,ThomasMesnard,CassidyHardin,RobertDadashi,SuryaBhupatiraju,ShreyaPathak,Laurent
Sifre,MorganeRivière,MihirSanjayKale,JulietteLove,etal. Gemma: Openmodelsbasedongemini
researchandtechnology. arXivpreprintarXiv:2403.08295,2024.
ShahriarGolchinandMihaiSurdeanu. TimetravelinLLMs: Tracingdatacontaminationinlargelanguage
models. arXivpreprintarXiv:2308.08493,2023.
DirkGroeneveld,IzBeltagy,PeteWalsh,AkshitaBhagia,RodneyKinney,OyvindTafjord,AnanyaHarshJha,
Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi
Chandu,ArmanCohan,JenniferDumas,YanaiElazar,YulingGu,JackHessel,TusharKhot,WilliamMerrill,
JacobMorrison,NiklasMuennighoff,AakankshaNaik,CrystalNam,MatthewE.Peters,ValentinaPyatkin,
AbhilashaRavichander,DustinSchwenk,SaurabhShah,WillSmith,NishantSubramani,MitchellWortsman,
PradeepDasigi,NathanLambert,KyleRichardson,JesseDodge,KyleLo,LucaSoldaini,NoahA.Smith,
andHannanehHajishirzi. Olmo: Acceleratingthescienceoflanguagemodels. Preprint,2024.
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio César Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi,
MojanJavaheripi, PieroKauffmann, GustavodeRosa, OlliSaarikivi, etal. Textbooksareallyouneed.
arXivpreprintarXiv:2306.11644,2023.
14Moritz Hardt and Benjamin Recht. Patterns, predictions, and actions: Foundations of machine learning.
PrincetonUniversityPress,2022.
TrevorHastie,RobertTibshirani,andJeromeFriedman. TheElementsofStatisticalLearning: DataMining,
Inference,andPrediction(Corrected12thprinting). Springer,2017.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Stein-
hardt. Measuring massive multitask language understanding. In International Conference on Learning
Representations,2020.
DanHendrycks,CollinBurns,SauravKadavath,AkulArora,StevenBasart,EricTang,DawnSong,andJacob
Steinhardt. Measuringmathematicalproblemsolvingwiththemathdataset. InThirty-fifthConferenceon
NeuralInformationProcessingSystemsDatasetsandBenchmarksTrack,2021.
JordanHoffmann,SebastianBorgeaud,ArthurMensch,ElenaBuchatskaya,TrevorCai,ElizaRutherford,
DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal. Trainingcompute-optimal
largelanguagemodels. arXivpreprintarXiv:2203.15556,2022.
OrHonovich,ThomasScialom,OmerLevy,andTimoSchick. Unnaturalinstructions: Tuninglanguagemodels
with(almost)nohumanlabor. arXivpreprintarXiv:2212.09689,2022.
TeamInternLM. Internlm: Amultilinguallanguagemodelwithprogressivelyenhancedcapabilities,2023.
MinhaoJiang,KenLiu,MingZhong,RylanSchaeffer,SiruOuyang,JiaweiHan,andSanmiKoyejo. Doesdata
contaminationmakeadifference? insightsfromintentionallycontaminatingpre-trainingdataforlanguage
models. InICLR2024WorkshoponNavigatingandAddressingDataProblemsforFoundationModels,2024.
JaredKaplan,SamMcCandlish,TomHenighan,TomBBrown,BenjaminChess,RewonChild,ScottGray,
AlecRadford, JeffreyWu, andDarioAmodei. Scalinglawsforneurallanguagemodels. arXivpreprint
arXiv:2001.08361,2020.
SayashKapoorandArvindNarayanan. Leakageandthereproducibilitycrisisinml-basedscience,2022.
JacobDevlinMing-WeiChangKentonandLeeKristinaToutanova. Bert: Pre-trainingofdeepbidirectional
transformersforlanguageunderstanding. InProceedingsofNAACL-HLT,pages4171–4186,2019.
PhilippKoehn.Europarl: Aparallelcorpusforstatisticalmachinetranslation.InTheTenthMachineTranslation
SummitProceedingsofConference,pages79–86.InternationalAssociationforMachineTranslation,2005.
YannLeCun,CorinnaCortes,andCJBurges. Mnisthandwrittendigitdatabase. ATTLabs[Online].Available:
http://yann.lecun.com/exdb/mnist,2,1998.
MikeLewis. Invitedtalk: Bridgingthegapbetweenpre-trainingdataandalignment. ICLRWorkshopon
NavigatingandAddressingDataProblemsforFoundationModels(DPFM),2024. URLhttps://iclr.cc/virt
ual/2024/workshop/20585.
ChangmaoLiandJeffreyFlanigan. Taskcontamination: Languagemodelsmaynotbefew-shotanymore. In
ProceedingsoftheAAAIConferenceonArtificialIntelligence,volume38,pages18471–18480,2024.
JeffreyLi,AlexFang,GeorgiosSmyrnis,MaorIvgi,MattJordan,SamirGadre,HritikBansal,EtashGuha,
SedrickKeh,KushalArora,etal.Datacomp-lm: Insearchofthenextgenerationoftrainingsetsforlanguage
models. arXivpreprintarXiv:2406.11794,2024.
PercyLiang,RishiBommasani,TonyLee,DimitrisTsipras,DilaraSoylu,MichihiroYasunaga,YianZhang,
DeepakNarayanan,YuhuaiWu,AnanyaKumar,etal. Holisticevaluationoflanguagemodels. Transactions
onMachineLearningResearch,2023.
IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. InInternationalConferenceon
LearningRepresentations,2018.
15ShengLu,IrinaBigoulaeva,RachneetSachdeva,HarishTayyarMadabushi,andIrynaGurevych.Areemergent
abilitiesinlargelanguagemodelsjustin-contextlearning? arXivpreprintarXiv:2309.01809,2023.
MetaAI. Llama3: Advancingopenfoundationmodels,2024. URLhttps://ai.meta.com/blog/meta-llama-3/.
ArindamMitra,HamedKhanpour,CorbyRosset,andAhmedAwadallah. Orca-math: Unlockingthepotential
ofslmsingradeschoolmath. arXivpreprintarXiv:2402.14830,2024.
SubhabrataMukherjee,ArindamMitra,GaneshJawahar,SahajAgarwal,HamidPalangi,andAhmedAwadal-
lah. Orca: Progressivelearningfromcomplexexplanationtracesofgpt-4. arXivpreprintarXiv:2306.02707,
2023.
OpenLlama. Openllama,2023. URLhttps://github.com/openlm-research/open_llama.
JudeaPearl. Causality. Cambridgeuniversitypress,2009.
JudeaPearl. Linearmodels: Auseful“microscope”forcausalanalysis. JournalofCausalInference, 1(1):
155–170,2013.
Guilherme Penedo, Hynek Kydlíˇcek, Leandro von Werra, and Thomas Wolf. Fineweb, 2024. URL https:
//huggingface.co/datasets/HuggingFaceFW/fineweb.
Matthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke
Zettlemoyer. Deepcontextualizedwordrepresentations. NAACL,2018.
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Languagemodels
areunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,Wei
Li,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer. Journal
ofmachinelearningresearch,21(140):1–67,2020.
SamyamRajbhandari,JeffRasley,OlatunjiRuwase,andYuxiongHe. Zero: Memoryoptimizationstoward
trainingtrillionparametermodels. InSC20: InternationalConferenceforHighPerformanceComputing,
Networking,StorageandAnalysis,pages1–16.IEEE,2020.
DavidRein,BettyLiHou,AsaCooperStickland,JacksonPetty,RichardYuanzhePang,JulienDirani,Julian
Michael,andSamuelRBowman. Gpqa: Agraduate-levelgoogle-proofq&abenchmark. arXivpreprint
arXiv:2311.12022,2023.
ManleyRoberts,HimanshuThakur,ChristineHerlihy,ColinWhite,andSamuelDooley. Datacontamination
throughthelensoftime. arXivpreprintarXiv:2310.10628,2023.
ErikFTjongKimSangandFienDeMeulder.Introductiontotheconll-2003sharedtask: Language-independent
namedentityrecognition. Development,922:1341,2003.
Rylan Schaeffer, Brando Miranda, and Sanmi Koyejo. Are emergent abilities of large language models a
mirage? AdvancesinNeuralInformationProcessingSystems,36,2024a.
RylanSchaeffer,HaileySchoelkopf,BrandoMiranda,GabrielMukobi,VarunMadan,AdamIbrahim,Herbie
Bradley,StellaBiderman,andSanmiKoyejo. Whyhaspredictingdownstreamcapabilitiesoffrontierai
modelswithscaleremainedelusive? arXivpreprintarXiv:2406.04391,2024b.
Zayne Rea Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. Musr: Testing the limits
of chain-of-thought with multistep soft reasoning. In The Twelfth International Conference on Learning
Representations,2023.
16AarohiSrivastava,AbhinavRastogi,AbhishekRao,AbuAwalMdShoeb,AbubakarAbid,AdamFisch,AdamR
Brown,AdamSantoro,AdityaGupta,AdriàGarriga-Alonso,etal. Beyondtheimitationgame: Quantifying
andextrapolatingthecapabilitiesoflanguagemodels. arXivpreprintarXiv:2206.04615,2022.
AarohiSrivastava,AbhinavRastogi,AbhishekRao,AbuAwalShoeb,AbubakarAbid,AdamFisch,AdamR
Brown,AdamSantoro,AdityaGupta,AdriGarriga-Alonso,etal. Beyondtheimitationgame: Quantifying
andextrapolatingthecapabilitiesoflanguagemodels. Transactionsonmachinelearningresearch,2023.
StabilityAI. Stablelm,2023. URLhttps://github.com/Stability-AI/StableLM.
ShaneStorks,QiaoziGao,andJoyceYChai. Recentadvancesinnaturallanguageinference: Asurveyof
benchmarks,resources,andapproaches. arXivpreprintarXiv:1904.01172,2019.
MiracSuzgun,NathanScales,NathanaelSchärli,SebastianGehrmann,YiTay,HyungWonChung,Aakanksha
Chowdhery,QuocLe,EdChi,DennyZhou,etal. Challengingbig-benchtasksandwhetherchain-of-thought
cansolvethem. InFindingsoftheAssociationforComputationalLinguistics: ACL2023,pages13003–13051,
2023.
RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,PercyLiang,and
TatsunoriBHashimoto. Stanfordalpaca: Aninstruction-followingllamamodel,2023.
TogetherWeCompute. Redpajamaincite,2023. URLhttps://www.together.ai/blog/redpajama-models-v1.
HugoTouvron, ThibautLavril, GautierIzacard, XavierMartinet, Marie-AnneLachaux, TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Openandefficientfoundation
languagemodels. arXivpreprintarXiv:2302.13971,2023a.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,NikolayBashlykov,
SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundationandfine-tunedchat
models. arXivpreprintarXiv:2307.09288,2023b.
AlexWang,AmanpreetSingh,JulianMichael,FelixHill,OmerLevy,andSamuelBowman. Glue: Amulti-task
benchmarkandanalysisplatformfornaturallanguageunderstanding. InProceedingsofthe2018EMNLP
WorkshopBlackboxNLP:AnalyzingandInterpretingNeuralNetworksforNLP,pages353–355,2018.
BenWangandAranKomatsuzaki. GPT-J-6B:A6BillionParameterAutoregressiveLanguageModel. https:
//github.com/kingoflolz/mesh-transformer-jax,May2021.
YizhongWang,SwaroopMishra,PegahAlipoormolabashi,YeganehKordi,AmirrezaMirzaei,AtharvaNaik,
ArjunAshok,ArutSelvanDhanasekaran,AnjanaArunkumar,DavidStap,etal. Super-naturalinstructions:
Generalizationviadeclarativeinstructionson1600+nlptasks. InProceedingsofthe2022Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing,pages5085–5109,2022.
YuboWang,XueguangMa,GeZhang,YuanshengNi,AbhranilChandra,ShiguangGuo,WeimingRen,Aaran
Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: A more robust and challenging multi-task language
understandingbenchmark. arXivpreprintarXiv:2406.01574,2024.
JasonWei,MaartenBosma,VincentZhao,KelvinGuu,AdamsWeiYu,BrianLester,NanDu,AndrewMDai,
andQuocVLe. Finetunedlanguagemodelsarezero-shotlearners. InInternationalConferenceonLearning
Representations,2021.
JasonWei,YiTay,RishiBommasani,ColinRaffel,BarretZoph,SebastianBorgeaud,DaniYogatama,Maarten
Bosma,DennyZhou,DonaldMetzler,etal. Emergentabilitiesoflargelanguagemodels. arXivpreprint
arXiv:2206.07682,2022.
TianwenWei,LiangZhao,LichangZhang,BoZhu,LijieWang,HaihuaYang,BiyeLi,ChengCheng,Weiwei
Lü,RuiHu,etal. Skywork: Amoreopenbilingualfoundationmodel. arXivpreprintarXiv:2310.19341,
2023.
17ZhaofengWu,LinluQiu,AlexisRoss,EkinAkyürek,BoyuanChen,BailinWang,NajoungKim,JacobAndreas,
andYoonKim.Reasoningorreciting? exploringthecapabilitiesandlimitationsoflanguagemodelsthrough
counterfactualtasks. arXivpreprintarXiv:2307.02477,2023.
AiyuanYang,BinXiao,BingningWang,BorongZhang,CeBian,ChaoYin,ChenxuLv,DaPan,DianWang,
DongYan,etal. Baichuan2: Openlarge-scalelanguagemodels. arXivpreprintarXiv:2309.10305,2023a.
ShuoYang,Wei-LinChiang,LianminZheng,JosephE.Gonzalez,andIonStoica. Rethinkingbenchmarkand
contaminationforlanguagemodelswithrephrasedsamples,2023b.
Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu,
JianqunChen,JingChang,etal. Yi: Openfoundationmodelsby01.ai. arXivpreprintarXiv:2403.04652,
2024.
DingliYu,SimranKaur,ArushiGupta,JonahBrown-Cohen,AnirudhGoyal,andSanjeevArora. Skill-mix: A
flexibleandexpandablefamilyofevaluationsforaimodels. arXivpreprintarXiv:2310.17567,2023a.
LonghuiYu,WeisenJiang,HanShi,YUJincheng,ZhengyingLiu,YuZhang,JamesKwok,ZhenguoLi,Adrian
Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language
models. InTheTwelfthInternationalConferenceonLearningRepresentations,2023b.
RowanZellers,AriHoltzman,YonatanBisk,AliFarhadi,andYejinChoi. Hellaswag: Canamachinereally
finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational
Linguistics.AssociationforComputationalLinguistics,2019.
AohanZeng,XiaoLiu,ZhengxiaoDu,ZihanWang,HanyuLai,MingDing,ZhuoyiYang,YifanXu,Wendi
Zheng,XiaoXia,etal. Glm-130b: Anopenbilingualpre-trainedmodel. arXivpreprintarXiv:2210.02414,
2022.
GeZhang,ScottQu,JiahengLiu,ChenchenZhang,ChenghuaLin,ChouLeuangYu,DannyPan,EstherCheng,
JieLiu,QunshuLin,RavenYuan,TuneyZheng,WeiPang,XinrunDu,YimingLiang,YinghaoMa,Yizhi
Li,ZiyangMa,BillLin,EmmanouilBenetos,HuanYang,JuntingZhou,KaijingMa,MinghaoLiu,Morry
Niu,NoahWang,QuehryQue,RuiboLiu,SineLiu,ShawnGuo,SorenGao,WangchunshuZhou,Xinyue
Zhang,YizhiZhou,YuboWang,YuelinBai,YuhanZhang,YuxiangZhang,ZenithWang,ZhenzhuYang,
ZijianZhao,JiajunZhang,WanliOuyang,WenhaoHuang,andWenhuChen. Map-neo: Highlycapable
andtransparentbilinguallargelanguagemodelseries. arXivpreprintarXiv: 2405.19327,2024.
Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and
Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911,
2023a.
KunZhou,YutaoZhu,ZhipengChen,WentongChen,WayneXinZhao,XuChen,YankaiLin,Ji-RongWen,
andJiaweiHan. Don’tmakeyourLLManevaluationbenchmarkcheater. arXivpreprintarXiv:2311.01964,
2023b.
18A Additional experimental details
A.1 Models considered
Model size in billions of parameters is indicated by N and pretraining data size in trillions of tokens is
indicatedby D. ModelweightswereretrievedfromthecorrespondingHuggingFace(HF)repositories.
Name Traindate N D HFrepository Citation
baichuan-13b 2023-06 13 1.4 baichuan-inc/Baichuan-13B- Yangetal.(2023a)
Base
baichuan-7b 2023-06 7 1.2 baichuan-inc/Baichuan2-7B- Yangetal.(2023a)
Base
baichuan2-13b 2023-09 13 2.6 baichuan-inc/Baichuan2-13B- Yangetal.(2023a)
Base
baichuan2-7b 2023-09 7 2.6 baichuan-inc/Baichuan2-7B- Yangetal.(2023a)
Base
falcon-11b 2024-05 11 5.0 tiiuae/falcon-11B Almazroueietal.(2023)
falcon-7b 2023-04 7 1.5 tiiuae/falcon-7b Almazroueietal.(2023)
gemma-2b 2024-02 2 2.0 google/gemma-2b Gemmaetal.(2024)
gemma-7b 2024-02 7 6.0 google/gemma-7b Gemmaetal.(2024)
gpt-j-6b 2021-03 6 0.4 EleutherAI/gpt-j-6b Wang and Komatsuzaki
(2021)
internlm-20b 2023-09 20 2.3 internlm/internlm-20b InternLM(2023)
internlm-7b 2023-07 7 1.0 internlm/internlm-7b InternLM(2023)
internlm2-base-20b 2024-01 20 2.6 internlm/internlm2-base-20b Caietal.(2024)
internlm2-base-7b 2024-01 7 2.6 internlm/internlm2-base-7b Caietal.(2024)
llama-13b 2023-02 13 1.0 None Touvronetal.(2023a)
llama-2-13b 2023-07 13 2.0 meta-llama/Llama-2-13b-hf Touvronetal.(2023b)
llama-2-70b 2023-07 70 2.0 meta-llama/Llama-2-70b-hf Touvronetal.(2023b)
llama-2-7b 2023-07 7 2.0 meta-llama/Llama-2-7b-hf Touvronetal.(2023b)
llama-3-8b 2024-04 8 15.0 meta-llama/Meta-Llama-3-8B MetaAI(2024)
llama-30b 2023-02 32.5 1.4 None Touvronetal.(2023a)
llama-65b 2023-02 65.2 1.4 None Touvronetal.(2023a)
llama-7b 2023-02 7 1.0 None Touvronetal.(2023a)
map-neo-7b 2024-05 7 4.5 m-a-p/neo_7b Zhangetal.(2024)
olmo-1.7-7b 2024-04 7 2.05 allenai/OLMo-1.7-7B-hf Groeneveldetal.(2024)
olmo-1b 2024-01 1 2.0 allenai/OLMo-1B-hf Groeneveldetal.(2024)
olmo-7b 2024-01 7 2.46 allenai/OLMo-7B-hf Groeneveldetal.(2024)
openllama-13b 2023-06 13 1.0 openlm- OpenLlama(2023)
research/open_llama_13b
openllama-3b 2023-06 3 1.0 openlm- OpenLlama(2023)
research/open_llama_3b
openllama-3b-v2 2023-07 3 1.0 openlm- OpenLlama(2023)
research/open_llama_3b_v2
openllama-7b 2023-06 7 1.0 openlm- OpenLlama(2023)
research/open_llama_7b
openllama-7b-v2 2023-07 7 1.0 openlm- OpenLlama(2023)
research/open_llama_7b_v2
pythia-1.4b 2023-02 1.4 0.3 EleutherAI/pythia-1.4b Bidermanetal.(2023)
pythia-12b 2023-02 12 0.3 EleutherAI/pythia-12b Bidermanetal.(2023)
pythia-160m 2023-02 0.16 0.3 EleutherAI/pythia-160m Bidermanetal.(2023)
pythia-1b 2023-02 1 0.3 EleutherAI/pythia-1b Bidermanetal.(2023)
pythia-2.8b 2023-02 2.8 0.3 EleutherAI/pythia-2.8b Bidermanetal.(2023)
19pythia-410m 2023-02 0.41 0.3 EleutherAI/pythia-410m Bidermanetal.(2023)
pythia-6.9b 2023-02 6.9 0.3 EleutherAI/pythia-6.9b Bidermanetal.(2023)
pythia-70m 2023-02 0.07 0.3 EleutherAI/pythia-70m Bidermanetal.(2023)
qwen-1.5-0.5b 2024-01 0.5 2.4 Qwen/Qwen1.5-0.5B Baietal.(2023)
qwen-1.5-1.8b 2024-01 1.8 2.4 Qwen/Qwen1.5-1.8B Baietal.(2023)
qwen-1.5-14b 2024-01 14 4.0 Qwen/Qwen1.5-14B Baietal.(2023)
qwen-1.5-4b 2024-01 4 2.4 Qwen/Qwen1.5-4B Baietal.(2023)
qwen-1.5-7b 2024-01 7 4.0 Qwen/Qwen1.5-7B Baietal.(2023)
redpajama-3b 2023-05 3 0.8 togethercomputer/RedPajama- TogetherWeCompute(2023)
INCITE-Base-3B-v1
redpajama-7b 2023-05 7 1.0 togethercomputer/RedPajama- TogetherWeCompute(2023)
INCITE-7B-Base
skywork-13b 2023-10 13 3.2 Skywork/Skywork-13B-base Weietal.(2023)
stablelm-2-1.6b 2024-01 1.6 2.0 stabilityai/stablelm-2-1_6b Bellagenteetal.(2024)
stablelm-2-12b 2024-03 12.1 2.0 stabilityai/stablelm-2-12b Bellagenteetal.(2024)
stablelm-3b-4e1t 2023-09 2.8 4.0 stabilityai/stablelm-3b-4e1t StabilityAI(2023)
stablelm-base- 2023-08 2.8 1.1 stabilityai/stablelm-base- StabilityAI(2023)
alpha-3b-v2 alpha-3b-v2
stablelm-base- 2023-08 7 1.1 stabilityai/stablelm-base- StabilityAI(2023)
alpha-7b-v2 alpha-7b-v2
yi-6b 2023-11 6 3.0 01-ai/Yi-1.5-6B Youngetal.(2024)
ziya2-13b-base 2023-11 13 2.65 IDEA-CCNL/Ziya2-13B-Base Ganetal.(2023)
A.2 Fine-tuning hyperparameters
We fine-tune all model parameters. For models with less than 10B parameters, we fine-tune on a single GPU with
BF16 precision. For models between 10B and 30B parameters, we train on a single H100 node using DeepSpeed
ZeRO-3(Rajbhandarietal.,2020)andfullprecision. Formodelswithmorethan30Bparameters,wetrainontwo
H100nodesusingDeepSpeedZeRO-3andfullprecision. Duetothelargecomputecostoftheexperiments,weperform
minimalhyperparametertuningandusestandardhyperparameterchoicesthroughout. Weusealearningrateof2·10−5
formodelswithfewerthan10Bparametersandalearningrateof2·10−6formodelswithmorethan10Bparameters. For
fourofthe7Bmodels–Gemma7B,Olmo7B,Olmo1.77B,andLlama38B–benchmarkaccuracyheavilydegradedafter
fine-tuning. Forthesemodels,weuseapeaklearningrateof2·10−6instead. Thesefourmodelswereallreleasedafter
November2023. Weuseacosinelearningrateschedulewithlinearwarm-upfor50stepsanddecayto10%ofthepeak
learningrate. WeuseAdamW(LoshchilovandHutter,2018)astheoptimizer,withβ =0.9,β =0.95,andε=10−8.
1 2
Wefine-tunewithbatchsize64. Weuseaweightdecayrateof0.1andclipgradientsat1.0. Weverifythatthetraining
lossdecreasesforallmodelsonbothofthefine-tuningdatasets. Toreducethecomputationburdenoffine-tuning,we
trainwithcontextsize600. Weverifythatlessthan5%ofthefine-tuningexampleshavecontextlengthabove600.
WeuseaninternalclusterofA100andH100GPUs. Fine-tuningallmodelsrequiredapproximately10,000H100
GPUhours,whereasevaluatingallmodelsinthedifferentbenchmarksrequiredapproximately400H100GPUhours.
B Causal interpretation of our findings
InSection2.2wedemonstratedthatmodelstrainedafterNovember2023significantlyoutperformthosetrainedbefore
November2023forbothMMLUandGSM8K.Wenowseektodeterminehowmuchofthebenchmarkimprovementsof
newermodelsisattributabletonewermodelstrainingmoreonthetesttask. Thatis,theextenttowhichtheeffectof
modelrecencyN onbenchmarkaccuracyAismediatedbytrainingonthetesttaskT. Thekeyobstacletoouranalysisis
thattesttasktrainingT isunobservable. Firstly,becausepractitionersaretypicallynottransparentabouttheirdesigns
choices(e.g.,pretrainingdata). Secondly,becausetheextenttowhichdifferenttrainingpracticesmightamounttotest
tasktrainingisunclear. However,weareabletointerveneonT byfine-tuningonthetesttask.
Figure10summarizesourcausalassumption. Thetimeatwhichamodelwastraineddeterminesthedesignchoices
made,suchasitspretrainingdataorpretrainingcomputeC. Thesedesignchoicesinturnaffecthowmuchthemodel
trainsonthetesttask. Allthesefactorsultimatelyinfluencethepretrainedmodelandthusitsbenchmarkperformance.
20C
N A
T
Figure10: WhetheramodelwastrainedafterNovember2023(N)influencesitspretrainingcompute(C)
andhowmuchittrainsonthetesttask(T). Allthreeinfluencethebenchmarkaccuracy(A)ofthemodel.
Weassumethattesttasktrainingdoesnotcausallyinfluencepretrainingcompute,butcomputemightinfluencetesttask
training. Forinstance,trainingonlargerdatasetsmayleadtotrainingmoreonthetesttask.
WeinterveneontesttasktrainingT byfine-tuningallmodelsonthesame,sufficientamountoftask-specificdata
beforeevaluation. Thatis,weapplytheadjustmentproposedinSection2.1. Theexternalvalidityofouranalysishinges
ontheassumptionthatourexperimentalsetting–fine-tuningmodelsafterthepretrainingstage–isreasonablysimilarto
thenaturalsettingsinwhichpractitionersmighttrainonthetesttaskduringpretraining(e.g.,byincludinginstruction
datainthepretrainingdatamixture). WeprovideevidenceinAppendixB.2thatthisisthecase.
Wemodelfine-tuningasahardinterventiondo(T =t)(Pearl,2009). Thespecificmagnitudeoftheinterventiont
neednotbequantified. Instead,thekeyassumptionisthatbyfine-tuningonthesame,sufficientamountoftaskdata,all
modelswillhavereceivedthesameamountoftesttasktraining. Sincesomebasemodelsmayhavealreadytrainedon
thetesttaskpriortofine-tuning,ourassumptionwillonlyholdiftesttasktrainingsaturatesandwetrainonenoughtask
datatoreachsaturation. WefindevidenceofsaturationforbothMMLUandGSM8K,seeAppendixB.3.
Wedrawinspirationfromscalinglaws(Kaplanetal.,2020)andmodelrelationshipbetweenpretrainingcompute
anditscausaldescendantsaspice-wiselog-linear:
|α|
(cid:88)
f(C,α)=α + α logC·[C>c] (2)
0 i i
i=1
Forsimplicity,weconsiderthreefixedknotsatc =0,c =1022,andc =1023 FLOPs. Weassumeallothervariable
1 2 3
relationshipstobelinear,resultinginthestructuralassignments:
T := f(C,β)+φN+δ, δ∼(cid:78)(0,σ2) (3)
δ
A:= f(C,α)+ψN+γT+η+ε, ε∼(cid:78)(0,σ2) (4)
ε
Wedenotebenchmarkaccuracyafterfine-tuningasA| do(T=t). ToestimatethedirecteffectN→Aofmodelrecencyon
accuracy,weregressthelinearmodel
A| do(T=t)= f(C,α)+ψN+γt+η+ε
= f(C,α)+ψN+η′+ε, η′=η+γt (5)
whereα,ψ,η′arethefit’sparametersandεisrandomnoise. ThecoefficientψcorrespondstothedirecteffectN→Aof
modelrecencyonbenchmarkaccuracy. Weadditionallyregressonthedifferenceinaccuracypreandpostintervention
A−A| do(T=t)=(f(C,α)+ψN+γT+η+ε 1)−(f(C,α)+ψN+γt+η+ε 2)
=γT−γt+ε −ε
1 2
= f(C,γβ)+γφN+γδ−γt+ε −ε
1 2
= f(C,β′)+φ′N+b+ε′, forβ′=γβ,φ′=γφ,b=−γt,ε′=ε −ε +γδ (6)
1 2
whereβ′,φ′, barethefit’sparametersandε′ israndomnoise. Thecoefficientφ′ correspondstotheindirecteffect
N→T →AofmodelrecencyN onbenchmarkaccuracyAmediatedbytesttasktrainingT (Pearl,2013). Thatis,the
improvementsinaccuracyofrecentmodelsattributabletotrainingonthetesttask.
WefitthemodelsinEquation5andEquation6,andwereportthecoefficientspertainingtoN→AandN→T →A
inTable2andTable3,respectively. WefindnoevidenceofasignificantdirecteffectN→Aofmodelrecencyonaccuracy.
Ontheotherhand,itsindirecteffectN→T →AmediatedbytesttasktrainingT issignificant,positive,andlarge.
21Table2: Wefindnoevidenceofasignificantdirect Table3: TheindirecteffectN →T →Amediatedby
effect of model recency N on accuracy A: the im- testtasktraining T ispositive,significant,andlarge:
provementsofnewermodelsarenotattributableto newermodelsattainmuchhigherbenchmarkscores
anythingelseotherthantrainingonthetesttask. becauseoftrainingonthetesttask.
MMLU GSM8K MMLU GSM8K
-0.004 0.000 0.071 0.168
ψ φ
(cid:210) (cid:210)
(0.009) (0.032) (0.018) (0.032)
R2 0.926 0.763 R2 0.530 0.503
Standarderrorsinparentheses.Boldindicatesp<0.05. Standarderrorsinparentheses.Boldindicatesp<0.05.
Pre adjustment
MMLU GSM8K
0.8
0.7
Difference =0.092 Difference =0.125
0.6 Regression R2=0.951 0.6 Regression R2=0.876
0.5 0.4
0.4
0.2
0.3
0.0
1020 1021 1022 1023 1024 1020 1021 1022 1023 1024
Pretraining compute (FLOPs) Pretraining compute (FLOPs)
Post adjustment
MMLU GSM8K
0.8
0.7
Difference =0.009 Difference =0.011
0.6 Regression R2=0.990 0.6 Regression R2=0.957
0.5 0.4
0.4
0.2
0.3
0.0
1020 1021 1022 1023 1024 1020 1021 1022 1023 1024
Pretraining compute (FLOPs) Pretraining compute (FLOPs)
Models trained Primarily on EN On both EN and CN
Figure11: ModelstrainedonbothEnglish(EN)andChinese(CN)languagedataoutperformthosetrained
primarilyonEnglishdata. Afteradjustingfortesttasktraining,wefindnoevidenceofasignificantdifference
θ inperformancebetweenmodelstrainedonENdataandEN+CNdata.
Therefore,ouranalysisindicatesthatthedifferencesinMMLUandGSM8Kperformancebetweennewerandolder
modelsobservedinSection2.1areprimarilyattributabletodifferencesintesttasktraining. Thatis,themechanismby
whichnewermodelsoutperformoldermodelsisbytrainingmoreonthetesttask.
22
ycaruccA
ycaruccA
ycaruccA
ycaruccAMMLU GSM8K
0.75
0.6
0.50
0.4 0.25
0.00
1020 1021 1022 1023 1024 1020 1021 1022 1023 1024
Pretraining compute (FLOPs) Pretraining compute (FLOPs)
Fine-tuned old models New models
Figure12: Newmodelsresembleoldmodelsthatwerefine-tuned. Temporalcut-off: November2023.
B.1 Robustness check on the temporal split: EN vs CN language data
Insteadofdivingmodelsusingatemporalsplit,wedividemodelsbasedonwhethertheyweretrainedprimarilyon
English(EN)dataoronamixtureofEnglishandChinese(EN+CN)languagedata. Whilethereisaconsiderableoverlap
betweenthetemporalsplitandtheEN/EN+CNmodelsplit,therearenotabledifferences. Inparticular,theBaichuan,
Baichuan2,andInternLM,andSkyworkfamiliesweretrainedbeforeNovember2023andtrainedonEN+CNdata.
Conversely,Gemma,Llama3,StableLM2,Falcon2,andOlmoweretrainedafterNovember2023andtrainedonENdata.
WerepeattheanalysisofSection2fortheENandEN+CNmodelsplit. Weobservethat,controllingforpretraining
compute, modelstrainedonEN+CNlanguagedataoutperformthosetrainedprimarilyonENby9accuracypoints
onMMLUand12accuracypointsonGSM8K.Aftertheproposedadjustment,however,thedifferenceinperformance
betweenmodelstrainedonENdataandEN+CNdataissmallandnotstatisticallysignificant.
TheconfoundingandmeasuredeffectsizesfortheENandEN+CNmodelsplitresemblethoseobtainedforthe
temporalsplit,whichweinterpretasavaluablerobustnesscheckofourresults.
B.2 How similar are newer models to older, fine-tuned models?
In Section 3.1 we fine-tune older models on the test task and we demonstrate that the differences in benchmark
performancebetweenthefine-tunedandnonfine-tunedmodelsresemblethosebetweennewerandoldermodels. Inthis
sectionweprovidefurtherevidencethatnewermodelsresembleolder,fine-tunedmodels.
Wetaketheoldermodelsandwefine-tunethemwith64,000trainingexamplesfromtheauxiliarytrainingsets
introducedinSection2.1. WeplotinFigure12thebenchmarkscoresoftheolder,fine-tunedmodelsaswellasthatof
thenewermodels. Wequalitativelyobservethatboththeolder,fine-tunedmodelsandthenewermodelsexhibitsimilar
scaling. Thatis,olderfine-tunedmodelsresemblenewermodelsintermsofperformancepercompute.
Weperformaquantitativeanalysisconsistingindiscriminatingbetweentheoldermodelsandthenewermodels
basedontheirpretrainingcomputeandbenchmarkaccuracy. Thatis,weconstructatabulardatasetwhererowsare
modelsandcolumnsaretheircorrespondingpretrainingcompute,benchmarkaccuracy,andwhetherthemodelwas
trainedafterNovember2023. Wethentrainaclassifieraimingtopredictmodelrecencyfromcomputeandaccuracy.
Intuitively,iftheperformanceofoldermodelsisverydifferentformthatofnewermodels,thenwewouldobtainhigh
predictionaccuracy(i.e.,thetwoclassesarehighlyseparable). Notethatpredictionaccuracyprovidesalowerboundon
thetotalvariation(TV)distancebetweenthedistributionsofcomputeandaccuracyofolderandnewermodels.
WetrainXGBoostclassifiersandreportbalancedaccuracyforleave-one-outcross-validationinTable4. Wefindthat
newermodelsarereasonabledistinguishablefromoldermodels,with63%accuracyforMMLUand79%accuracyfor
GSM8K.Incontrast,weobtainclosetorandom-chanceaccuracyindiscriminatingbetweenolder,fine-tunedmodelsand
newermodels. Thatis,olderfine-tunedmodelsareindistinguishablefromnewermodelsintermsoftheirperformance.
B.3 Test task training and saturation
Weshowthattrainingonthetesttasksaturates. Weconsidertheintermediatecheckpointsoftheadjustmentprocedure,
thatis,fine-tuningfor3epochsofthetaskdatasetsintroducedinSection2.1. WeplotinFigure13thegaininbenchmark
accuracyfromthefirst75%(MMLU)and80%(GSM8K)trainingsteps,aswellasthegaininbenchmarkaccuracyfrom
23
ycaruccATable4: Accuracyindiscriminatingbetweenolderandnewermodelsintermsoftheirpretrainingcompute
andbenchmarkaccuracy. Older,fine-tunedmodelsareindistinguishablefromnewermodels.
Discriminatortest MMLU GSM8K
Oldermodelsvs
64.6% 73.9%
newermodels
Fine-tuned,oldermodelsvs
52.2% 52.5%
newermodels
Randomchanceaccuracyis50%.
MMLU
Improvement from step 0 to step 3k Improvement from step 3k to step 4k
0.2 0.2
0.0 0.0
1020 1021 1022 1023 1024 1020 1021 1022 1023 1024
GSM8K
Improvement from step 0 to step 20k Improvement from step 20k to step 25k
0.4 0.4
0.2 0.2
0.0 0.0
0.2 0.2
1020 1021 1022 1023 1024 1020 1021 1022 1023 1024
Pretraining compute Pretraining compute
Figure13: Trainingonthetesttasksaturates. Thefinal25%(MMLU)and20%(GSM8K)optimizationsteps
oftheproposedadjustmentresultinalmostnochangesinbenchmarkaccuracy.
theremainingsteps. Almostalloftheperformanceimprovementsoccurintheearliertrainingsteps. Incontrast,thefinal
25%(MMLU)and20%(GSM8K)optimizationstepsresultinalmostnochangesinbenchmarkaccuracy. Thisindicates
thattrainingonthetesttasksaturates,andwetrainforenoughstepstoreachsaturation.
Notethatwhilemoretrainingdatamightresultinfurtherbenchmarkimprovements,weshowthatthetaskdatasets
thatweusearesufficientforoldermodelstoreachtheperformanceofnewermodels,seeAppendixB.2.
C Results for the OpenLLM Leaderboard v2
HuggingFacereleasedonJune2024arevisionoftheOpenLLMLeaderboard(Fourrieretal.,2024a). TheHFleaderboard
v2 differs from v1 in the six benchmarks it considers: MMLU Pro (Wang et al., 2024), GPQA (Rein et al., 2023),
BBH(Suzgunetal.,2023),MuSR(Spragueetal.,2023),theLevel5subsetofMATH(Hendrycksetal.,2021),and
IFEval(Zhouetal.,2023a). MMLUandGPQAtestforknowledgeandareframedasmultiple-choicequestions. BBHand
MuSRtestforreasoning. MATHtestsformathematicalreasoning. IFEvalteststheabilityofmodelstofollowinstructions.
ThecreatorsoftheOpenLLMLeaderboardcitecontaminationasakeymotivationforreleasingthev2revision. They
notethatakeycriteriainchoosingthebenchmarksoftheHFleaderboardv2waslackofcontaminationinmodelsas
oftoday. Inparticular,Fourrieretal.(2024b)claimthatcurrentmodelsarenotcontaminatedforGPQA,MuSR,and
MMLUPro: GPQAduetothegatingofthetestset,andMuSRandMMLUProduetotheir“youth”. Fourrieretal.(2024b)
succinctlyexpresstheirconcernasregardstodatacontaminationintheHFleaderboardv1:
24
ycarucca
ni
niaG
ycarucca
ni
niaG"Somenewermodelsalsoshowedsignsofcontamination. Bythis,wemeanthatmodelswerepossiblytrained
onbenchmarkdataorondataverysimilartobenchmarkdata. Assuch,somescoresstoppedreflectingthe
generalperformanceofthemodelandstartedtooverfitonsomeevaluationdatasetsinsteadofreflectingthe
moregeneralperformanceofthetaskbeingtested. Thiswas,inparticular,thecaseforGSM8KandTruthfulQA,
whichwereincludedinsomeinstructionfine-tuningsets."
Notethat“modelswerepossiblytrainedonbenchmarkdataorondataverysimilartobenchmarkdata”encompassesnot
onlytestsetcontaminationbutmorebroadlytrainingonthetesttask.
Weevaluateall53modelsonMMLUPro,GPQA,BBH,MuSRandMATHLvl5. WeusetheLMEvaluationHarness
libraryinidenticalfashiontotheHFleaderboardv2. WedonotevaluatedonIFEvalsinceittestsforinstructionfollowing
andweevaluatebasemodels. Weadditionallyevaluatethemodelsthatwefine-tunedinSection2.1formultiplechoice
questionansweringandmathematicalreasoning. Thisgivesusmodels’adjustedbenchmarkscoresaftertrainingon
multiplechoicequestionansweringandmathematicalreasoning. ForMATHLvl5,weusethemodelsfine-tunedon
mathematicaldata,whereasforMMLUPro,GPQA,BBHandMuSRweusethemodelsfine-tunedonmultiplechoice
questionanswering. Thefine-tuningdatasetswerenotadaptedtothenewbenchmarksintheHFleaderboardv2,thus
givingavaluableinsightintohowwellthesetask-relevantdatasetsgeneralizebeyondMMLUandGSM8K.
WeplotinFigure14modelsbenchmarkscorespreandpostpostadjustment. Wefindthatnewermodelssignificantly
outperformolderonesinallfivebenchmarksaftercontrollingforpretrainingcompute. Thedifferencesinperformance
aresmallerinabsolutetermsthanthosemeasuredforMMLU(0.068)andGSM8K(0.168). Thisisinpartbecausethese
benchmarksare“harder”,meaningalsosmallerdifferencesinperformancebetweenthebestandworstmodel. Forthis
reason,wealsoreportthedifferencebetweennewerandoldermodelsrelativetothedifferencebetweenthebestand
worstmodel. Thisrelativedifferenceis13.7%forMMLUPro,14.5%forGPQA,12.1%forMuSR,9.7%forBBH,and
10.0%forMATHLvl5,comparedto15.3%forMMLUand25.0%forGSM8K.Therefore,newermodelsoverperformin
MMLUPro,GPQAandMuSRaboutasmuchastheydoforMMLU,andsomewhatlessforBBHandMATHLvl5.
Fine-tuningontask-relevantdatareducesthedifferenceinperformancebetweennewerandoldermodelsforall
fivebenchmarks. ForGPQAandMuSR,thedifferenceinperformanceafteradjustmentissmall(|θ (cid:98)|⩽0.002)andnot
statisticallysignificant. ForBBH,theestimateddifferenceinperformanceθ (cid:98)reducesby40%to0.015andisnolonger
statisticallysignificant. ForMMLUProandMATHLvl5thedifferencereducesby19%and33%respectivelybutremains
reasonablylarge(θ (cid:98)>0.01)andstatisticallysignificant. Therefore,findevidencethattrainingonthetesttaskplaysa
substantialroleinnewermodelsoutperformingolderonesinthebenchmarksoftheHFLeaderboardv2.
OnepossiblereasonforthefactthattheadjustmentforMMLUProandMATHLvl5isnotaseffectiveasforMMLU
andGSM8Kisthatthefine-tuningexamplesaresimplynotasrelevantforMMLUProandMATHLvl5. Forexample,the
questionsinMATHLvl5containmuchmoreLaTeXequationformattingthanourmathematicalreasoningfine-tuning
dataset. NotethattheanswerstomanyMATHLvl5questionsarepreciselyformattedasLaTeXequations. Regarding
MMLUPro,ourmultiplechoicefine-tuningdatasetcontainsmostlyquestionswith4answerchoices,whereasallMMLU
Proquestionshave10answerchoices. Thus,modelsareprimarilyfine-tunedtoanswer“A”,“B”,“C”,and“D”butnot“E”,
“F”,“G”.WemodifyMMLUProtoonlycontainquestionswith4answerchoicesbyforeveryquestionrandomlydiscarding
6oftheincorrectanswerchoices. WeevaluatemodelspreandpostadjustmentandplottheresultsinFigure15. We
observethatthedifferenceinperformancebetweennewerandoldermodelsafteradjustmentreducesfrom0.024to
0.016,andisnolongerstatisticallysignificant. Thisobservationsuggeststhatfine-tuningonemorerelevanttask-data
mightfurtherreducethegapbetweennewerandoldermodels.
Discussion. Fourrieretal.(2024b)citenewermodelsoverperformingintheHFleaderboardv1duetobeing“possibly
trainedonbenchmarkdataorondataverysimilartobenchmarkdata”asamajorreasonfortheHFleaderboardv2
revision. Wehoweverfindevidencethattrainingonthetesttaskisalsoaconfounderforthenewlyincludedbenchmarks.
Specifically,thedifferenceinperformancebetweennewerandoldermodelsissignificantforMMLUPro,GPQA,MuSR,
BBHandMATHLvl5,andthesedifferencesreduceafteradjustingbyfine-tuningonthetesttask.
Fourrieretal.(2024b)explicitlyhighlightGPQAandMuSRasbenchmarkslikelyunaffectedbycontamination,the
formerduetobeinggatedandlatterduetoits“youth”. Notonlydonewermodelssignificantlyoutperformolderones
inGPQAandMuSR,butthesedifferencesinperformancefullyvanishafterfine-tuningonthetesttask. Thatis,newer
modelslikelyoverperforminGPQAandMuSRpreciselyduetotrainingonthetesttask.
Thesefindingshighlightthattrainingonthetesttaskisadistinctphenomenonfromtestsetleakage. Strategiesthat
aimtomitigatedatacontamination–e.g.,dynamicbenchmarks–mightnotbeeffectiveinmitigatingtheconfounding
effectoftrainingonthetesttask. Incontrast,weextensivelydemonstratedtheeffectivenessofourproposedadjustment
procedure,thatis,fine-tuningonsufficienttask-relevantdatabeforeevaluation.
25Unadjusted Adjusted
=0.036 =0.024
0.3
0.2
0.1
0.35 =0.014 =0.002
0.30
0.25
=0.015 = 0.000
0.45
0.40
0.35
=0.025 =0.015
0.5
0.4
0.3
0.15 =0.016 =0.013
0.10
0.05
0.00
1020 1021 1022 1023 10241020 1021 1022 1023 1024
Pretraining compute (FLOPs) Pretraining compute (FLOPs)
Models trained Before November 2023 After November 2023
Boldindicatesstatisticalsignificancewithp<0.05.
Figure14: ResultsfortheOpenLLMLeaderboardv2. Forallbenchmarks,modelstrainedafterNovember2023
significantlyoutperformmodelstrainedbeforeNovember2023whencontrollingforpretrainingcompute.
Afterfine-tuningmodelsonmultiplechoicequestionansweringandmathematicalreasoning,differences
inperformancebetweennewerandoldermodelsreduceforallfivebenchmarks. Thesedifferencesareno
longersignificantforGPQA,MuSRandBBH,butremainsignificantforMMLUProandMATHLvl5.
26
AQPG
RSuM
5
lvL
HTAM
orP
ULMM
HBBUnadjusted Adjusted
0.6
=0.049 =0.016
0.5
0.4
0.3
1020 1021 1022 1023 1024 1020 1021 1022 1023 1024
Pretraining compute (FLOPs) Pretraining compute (FLOPs)
Models trained Before November 2023 After November 2023
Boldindicatesstatisticalsignificancewith p<0.05.
Figure 15: We modify MMLU Pro to only contain questions with 4 answer choices by for every question
randomlydiscarding6oftheincorrectanswerchoices. Afteradjustment,thedifferenceinperformanceθ (cid:98)
betweennewerandoldermodelsissmallerandnolongerstatisticallysignificant.
Cloze Multiple choice Multiple choice
0.6 0.25
0.75
0.50
0.4 0.50
0.75
0.2 0.25
0.25 0.6 0.75
0.50
0.50
0.4
0.75
0.25
1021 1023 1021 1023 1021 1023
Pretraining compute Pretraining compute Pretraining compute
Figure16: ARCandHellaSwagscoresofmodelstrained(●)beforeNovember2023and(●)after. Middle:
reformulatingthetesttaskasmultiple-choiceleadstoemergencearound1022 to1023 FLOPs. Right: when
usingBrierscoreasthemetric,wesimilarlyobservesharpchangesinperformancearound1022to1023FLOPs.
D Additional figures
InFigure16weshowthatARCandHellaSwagdonotexhibitemergencewhenusingthestandardclozeevaluation.
WhenreformulatingthetaskasmultiplechoiceinthestyleofMMLU,however,weobserveemergencearound1022to
1023FLOPs,similarlytoMMLU.Emergenceinthisrangeofcomputepersistsevenwhenchangingtheevaluationmetric
fromaccuracytoBrierscore–acontinuousmetric–,assuggestedbySchaefferetal.(2024a).
27
orP
ULMM
CRA
gawSalleH
)seciohc
4(
ycaruccA
ycaruccA
ycaruccA
ycaruccA
erocs
reirB-erocs
reirB-