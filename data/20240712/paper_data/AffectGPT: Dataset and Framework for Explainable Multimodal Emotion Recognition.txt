AffectGPT: Dataset and Framework for
Explainable Multimodal Emotion Recognition
ZhengLian1,HaiyangSun1,LicaiSun1,JiangyanYi1,BinLiu1,JianhuaTao2,3
1InstituteofAutomation,ChineseAcademyofSciences
2DepartmentofAutomation,TsinghuaUniversity
3BeijingNationalResearchCenterforInformationScienceandTechnology,TsinghuaUniversity
lianzheng2016@ia.ac.cn
Abstract
ExplainableMultimodalEmotionRecognition(EMER)isanemergingtaskthat
aimstoachievereliableandaccurateemotionrecognition. However,duetothe
highannotationcost,theexistingdataset(denotedasEMER-Fine)issmall,making
itdifficulttoperformsupervisedtraining. Toreducetheannotationcostandexpand
thedatasetsize,thispaperreviewsthepreviousdatasetconstructionprocess. Then,
wesimplifytheannotationpipeline,avoidmanualchecks,andreplacetheclosed-
source models with open-source models. Finally, we build EMER-Coarse, a
coarsely-labeleddatasetcontaininglarge-scalesamples. Besidesthedataset,we
proposeatwo-stagetrainingframeworkAffectGPT.ThefirststageexploitsEMER-
Coarsetolearnacoarsemappingbetweenmultimodalinputsandemotion-related
descriptions; the second stage uses EMER-Fine to better align with manually-
checkedresults.Experimentalresultsdemonstratetheeffectivenessofourproposed
methodonthechallengingEMERtask. Tofacilitatefurtherresearch,wewillmake
thecodeanddatasetavailableat: https://github.com/zeroQiaoba/AffectGPT.
1 Introduction
Emotionrecognitionisanimportantresearchtopicinhuman-computerinteraction. Itsmaingoalis
topredictthemostlikelylabelfromafixedspace[1](suchasthesevenbasicemotionsinEkman’s
theory [2]). However, emotions are complex. Limiting the label space and fixing the number
of predictions may lead to inaccurate descriptions of emotions. Meanwhile, traditional emotion
recognitionlackstheexplanationprocess,whichiscrucialtoenhancetheannotationreliability.
Tothisend,researchersproposeanewtaskcalledExplainableMultimodalEmotionRecognition
(EMER)[3]. Unliketraditionalemotionrecognition,EMERexploitsmulti-modalandmulti-faceted
cluestopredictemotionsinanopen-vocabulary(OV)manner. Thesecluescanalsoserveassupport
andevidenceforthesepredictions. Therefore,EMERprovidesapromisingwayforaccurateand
reliableemotionrecognition. However,duetothehighannotationcost,previousworksonlycontain
asmallnumberoflabeledsamples(denotedasEMER-Fine)[3]. Thesesamplescanonlyevaluate
theperformanceofpre-trainedsystemsandarenotenoughforsupervisedtraining.
Toreducetheannotationcost,wereviewthepreviousdatasetconstructionprocess. Itcontainsfour
steps: pre-labelingaudioandvideoclues,manuallycheckingtheseclues,disambiguatingsubtitles,
andtranslatingtoobtainbilingualdescriptions[3]. Thisprocessreliesonmanualchecksandclosed-
sourcemodels. Toreducetheannotationcost,wetrytoavoidmanualchecksanduseopen-source
modelsinstead. Then,webuildEMER-Coarse,acoarsely-labeleddatasetcontaininglarge-scale
data. Sinceemotionrecognitionfocusesonidentifyinghumanemotionalstates,weconstructthis
datasetbasedonMER2024-SEMI[4],whichcontains115,595human-centricvideos.
Preprint.Underreview.
4202
luJ
01
]CH.sc[
1v35670.7042:viXraBesidesEMER-Coarse,weproposeAffectGPT,atwo-stagetrainingframeworkforEMER.Inthe
firststage,weuselarge-scaleEMER-Coarsetolearnacoarsealignmentbetweenmultimodalinputs
andemotion-relateddescriptions. Inthesecondstage,weusesmall-scaleEMER-Finetobetteralign
withmanually-checkedresults. Themaincontributionsofthispapercanbesummarizedasfollows:
• (Dataset)WebuildEMER-Coarse,alarge-scaledatasetforEMER.Thisdatasetcontains
115,595samples,muchmorethanpreviousdatasetsandsufficientforsupervisedtraining.
• (Method)WeproposeAffectGPT,atwo-stageframeworkforEMER.Thefirststagelearns
acoarsemappingandthesecondstagebetteralignswithmanually-checkedresults.
• (Performance)Experimentalresultsdemonstratetheeffectivenessofthisframework. Our
systematicanalysiscanalsoprovidesomeinspirationforsubsequentresearchers.
2 TaskandEvaluation
ThissectionreviewsthetaskdefinitionandevaluationmetricsofEMER.Unliketraditionalemotion
recognition,EMERaimstopredictemotionsinanexplainableandopen-vocabularymanner. Follow-
ingpreviousworks[3],wefocusonemotionrecognitionandusetheoverlapbetweenpredictedand
annotatedresultsastheevaluationmetric. Sincewedonotfixthelabelspace,differentmodelsmay
generatesynonyms. Toremovetheirimpacts,wefirstgroupalllabelsusingGPT-3.5[5](“gpt-3.5-
turbo-16k-0613”): Pleaseassumetheroleofanexpertinthefieldofemotions. Weprovideasetof
emotions. Pleasegrouptheemotions,witheachgroupcontainingemotionswiththesamemeaning.
Directlyoutputtheresults. Theoutputformatshouldbealistcontainingmultiplelists.
Specifically,assumethatG(·)istheGPT-generatedmappingfunctionbetweenlabelsandgroupIDs.
{y }M and{yˆ}N aretheannotatedandpredictedlabels,respectively. Here,M andN arethe
i i=1 i i=1
numberoflabels. Beforemetriccalculation,wefirstmapeachlabelintoitsgroupID:
Y ={G(x)|x∈{y }M }, Yˆ ={G(x)|x∈{yˆ}N }. (1)
i i=1 i i=1
Then,wecalculatetheaverageofprecisionandrecallasthefinalmetric:
|Y ∩Yˆ| |Y ∩Yˆ|
Accuracys =
|Yˆ|
, Recalls =
|Y|
, (2)
Accuracys+Recalls
Avg= . (3)
2
3 EMER-Coarse
This section reviews the previous dataset construction pipeline [3] and attempts to reduce the
annotationcost. Specifically,thepreviouspipelineconsistsoffoursteps: pre-labelingtogenerate
multimodalclues,manualcheckingtheseclues,disambiguationofsubtitles,andtranslationtoobtain
bilingualdescriptions. Themaincostliesinmanualchecksandtheuseofclosed-sourcemodelsfor
pre-labeling,disambiguation,andtranslation. Toreducethecost,wetrytoavoidmanualchecksand
replacetheseclosed-sourcemodelswithopen-sourcemodels. Inthissection,wetestthemainstream
open-sourceLLMsandMLLMs. Sincetheresultsvaryslightlybetweendistinctruns,werunall
experimentstwiceandreporttheaveragescoreandstandarddeviation.
3.1 Pre-labeling
Previously, the pre-labeling process relied on the closed-source GPT-4 (“gpt-4-vision-preview”).
Tofinditsreplacement,weevaluatetheperformanceofsomerepresentativeopen-sourceMLLMs.
According to previous findings [3], adding subtitles using a two-step strategy can achieve better
performance,i.e.,firstextractingemotion-relateddescriptionsfromMLLMsandthenusingthemto
disambiguatethesubtitle. Inthissection,wefollowthisstrategyandreportresultsinTable1. Inthis
table,someresultsaretakenfrompreviousworks[3]astheyfollowthesameexperimentalsetup.
BesidesthesingleMLLM,canweachievebetterperformanceifwecombinedifferentMLLMs? To
answerthisquestion,wefurtherselectthetop-performingaudioandvideoMLLMsandreportthe
2performanceoftheircombinations. InTable1,weobservethatthesecombinationsusuallybring
performanceimprovement. Amongthem,thecombinationofSALMONNandChat-UniViperforms
best,evensurpassingGPT-4. Therefore,weuseitforpre-labeling.
Table1: PerformanceofdifferentMLLMsandtheircombinations. Followingpreviousworks[3],we
considerlanguageinfluenceandreporttheresultsunderdifferentlanguages.
English Chinese
Model L V A
Avg Accuracys Recalls Avg Accuracys Recalls
Audio+Subtitle
√ √
Qwen-Audio[6] √ × √ 40.23±0.09 49.42±0.18 31.04±0.00 43.53±0.04 53.71±0.00 33.34±0.09
OneLLM[7] √ × √ 43.04±0.06 45.92±0.05 40.15±0.06 46.77±0.01 52.07±0.06 41.47±0.08
SECap[8] √ × √ 46.94±0.10 54.52±0.15 39.37±0.05 47.09±0.15 55.55±0.23 38.64±0.08
SALMONN[9] × 48.06±0.04 50.20±0.04 45.92±0.04 48.53±0.03 52.24±0.00 44.82±0.05
Video+Subtitle
√ √
Otter[10] √ √ × 44.40±0.09 50.71±0.10 38.09±0.09 46.92±0.04 52.65±0.16 41.18±0.08
VideoChat[11] √ √ × 45.70±0.09 42.90±0.27 48.49±0.10 45.63±0.04 47.20±0.12 44.05±0.05
Video-LLaMA[12] √ √ × 44.74±0.14 44.14±0.13 45.34±0.15 47.27±0.03 47.98±0.07 46.56±0.01
Video-LLaVA[13] √ √ × 47.12±0.15 48.58±0.02 45.66±0.29 49.59±0.05 53.95±0.03 45.23±0.13
VideoChat2[14] √ √ × 49.60±0.28 54.72±0.41 44.47±0.15 49.90±0.06 57.12±0.08 42.68±0.04
OneLLM[7] √ √ × 50.99±0.08 55.93±0.09 46.06±0.06 51.84±0.08 56.43±0.04 47.26±0.11
LLaMA-VID[15] √ √ × 51.29±0.09 52.71±0.18 49.87±0.00 52.45±0.02 57.30±0.00 47.61±0.03
mPLUG-Owl[16] √ √ × 52.79±0.13 54.54±0.13 51.04±0.13 51.43±0.03 56.40±0.11 46.47±0.18
Video-ChatGPT[17] √ √ × 50.73±0.06 54.03±0.04 47.44±0.07 55.34±0.02 61.15±0.10 49.52±0.06
Chat-UniVi[18] √ √ × 53.09±0.01 53.68±0.00 52.50±0.02 54.20±0.02 58.54±0.01 49.86±0.03
GPT-4V[19] × 56.69±0.04 48.52±0.07 64.86±0.00 57.34±0.01 54.61±0.02 60.07±0.01
Audio+Video+Subtitle
√ √ √
SECap+mPLUG-Owl √ √ √ 57.71±0.05 50.05±0.23 65.38±0.33 55.22±0.22 51.65±0.27 58.79±0.16
SALMONN+Video-ChatGPT √ √ √ 58.71±0.24 53.16±0.17 64.26±0.31 55.10±0.16 53.44±0.14 56.76±0.19
SECap+Video-ChatGPT √ √ √ 57.41±0.09 52.03±0.04 62.79±0.14 56.49±0.02 56.50±0.01 56.48±0.05
SECap+Chat-UniVi √ √ √ 59.13±0.08 48.85±0.29 69.41±0.13 56.49±0.14 52.38±0.07 60.59±0.22
SALMONN+mPLUG-Owl √ √ √ 59.77±0.05 51.77±0.01 67.76±0.11 55.94±0.21 51.74±0.19 60.14±0.23
SALMONN+Chat-UniVi 59.47±0.08 51.62±0.00 67.31±0.15 57.54±0.06 51.65±0.06 63.42±0.06
√ √ √
EMER(Multi) 80.05±0.24 80.03±0.37 80.07±0.10 85.20±0.03 87.09±0.00 83.31±0.05
3.2 DisambiguationandTranslation
Disambiguation and translation deal with plain text data and these modules previously relied on
GPT-3.5. Tofinditsalternative,wetestsometypicalopen-sourceLLMs. Experimentalresultsare
showninTable2. Weobservethatifonlythetranslationmoduleisreplacedwithopen-sourceLLMs,
theperformancedropissmall.Butifwereplacebothtranslationanddisambiguation,theperformance
dropisobvious. Theseresultsshowthatfornon-complextasks(e.g.,translation),theperformance
ofopen-sourceLLMsisclosetoGPT-3.5. Butforcomplextasks(e.g.,disambiguation),thereis
stillagapbetweenopen-sourceLLMsandGPT-3.5. Thereasonmaybethatwedonottestlarger
LLMsduetolimitedGPUmemory. Generally,largerLLMshelpsolvemorecomplextasks,whichis
leftforourfuturework. Meanwhile,weobservethatQwen2-7BperformsbetterthanLLaMA3-8B
intranslation. Therefore,weuseQwen2-7BfortranslationandGPT-3.5fordisambiguation. This
replacementreducestheOpenAIAPIcallcostandmaintainstheoverallperformance.
Finally,weusetheabovestrategytoautomaticallyannotateMER2024-SEMI[4]. Theseannotation
resultstakeintoaccountallacoustic, visual, andlexicalclues. Sincetheseresultshavenotbeen
manuallychecked,theremaybesomeinaccuracies. WecallthisdatasetEMER-Coarse.
Table2: Choiceofopen-sourceLLMsfortranslationanddisambiguation. Sincethecombinationof
SALMONNandChat-UniViperformsbest(seeTable1),weconductanalysisonthiscombination.
English Chinese
Translate Disambiguate
Avg Accuracys Recalls Avg Accuracys Recalls
GPT-3.5 GPT-3.5 59.47±0.08 51.62±0.00 67.31±0.15 57.54±0.06 51.65±0.06 63.42±0.06
LLaMA3-8B GPT-3.5 57.13±0.27 49.63±0.32 64.64±0.22 55.50±0.02 50.85±0.19 60.15±0.16
LLaMA3-8B LLaMA3-8B 55.50±0.09 49.91±0.04 61.08±0.22 52.59±0.74 47.03±0.42 58.15±1.05
Qwen2-7B GPT-3.5 58.22±0.11 49.68±0.21 66.76±0.00 56.65±0.27 52.95±0.23 60.36±0.32
Qwen2-7B Qwen2-7B 53.38±0.60 44.74±0.67 62.01±0.54 55.15±0.03 47.92±0.06 62.37±0.12
34 AffectGPT
BesidesEMER-Coarse,weproposeatwo-stageframeworkAffectGPT.Thissectionintroducesthis
frameworkfromthreeaspects: trainingprocess,modelarchitecture,andexperimentalsetup.
TrainingProcess ThefirststageusesEMER-Coarsetolearnacoarsealignmentbetweenmulti-
modalinputsandemotion-relatedoutputs. ThesecondstageusesEMER-Finetobetteralignwith
manually-checkedresults. ConsideringthatEMER-Finehasmorereliablelabels,weevaluatethe
performanceofdifferentsystemsonit. However,thesecondstageisalsotrainedonEMER-Fine,so
wefurthersplititintotrainingandtestsets. ThestatisticsareshowninTable3.
Model Architecture AffectGPT is borrowed from Video-
LLaMAwithsomemodifications. Consideringthattheorigin Table3: Datasetstatistics.
frameworktrainsaudioandvideobranchesseparatelybutemo-
tionrecognitionrequirestheintegrationofmultimodalclues, Dataset Split #ofsamples
EMER-Coarse – 115,595
wemodifyVideo-LLaMAtosupportaudio-video-textalign-
train 266
ment training. Specifically, we input the audio, video, and
EMER-Fine test 66
subtitlesimultaneously,andtrytolearnamappingbetween whole 332
multimodalinputsandemotion-relateddescriptions. Therea-
sonwhywedonotdesignmoreeffectiveframeworksbutuseVideo-LLaMAisthatthemainpurpose
ofthispaperistostudytheeffectivenessofEMER-Coarseandthetwo-stagetrainingprocess. The
impactofdifferentmodelarchitecturesislefttoourfuturework.
ExperimentalSetup AffectGPTisimplementedwithPyTorch.Alltrainingandinferenceprocesses
arecarriedoutwithan80GNVIDIATeslaA100GPU.Duringtraining,wesetthemaximumnumber
ofepochsto100. Duetothedifferentnumberoftrainingsamplesineachstage,thefirststageiterates
1000timesperepochandthesecondstageiterates88timesperepoch. Meanwhile,wesetthebatch
sizeofeachiterationto3. LimitedbyourGPUmemorycapacity,wedonottestalargerbatchsize.
Duringtraining,wefreezetheweightsoftheacousticencoder,visualencoder,andLLM,andonly
trainQ-FormertolearnthemappingbetweenunimodalencodersandLLM.
5 ResultsandDiscussion
AffectGPTisatwo-stagetrainingframework. Toverifyitseffectiveness,weperformablationstudies
oneachstage. ConsideringthatVideo-LLaMAprovidespretrainedQ-Formers,wefirstrevealtheir
necessityandstudywhetherAffectGPTcanbetraineddirectlyonrandomlyinitializedweights. Then,
westudytheimpactofdifferentLLMsanddiscussthenecessityofeachstage. Finally,weshowthe
performanceofAffectGPTontheEMERtask. Forconvenience,inthissection,weabbreviatethe
firststageasstage1andthesecondstageasstage2.
Duringtraining,AffectGPTlearnsamappingbetweenaudio-video-textinputsandemotion-related
outputs. TheseoutputsareinEnglishandhavealreadyconsideredthedisambiguationprocess(see
Section 3). In the previous evaluation pipeline (see Table 1), we need additional translation and
disambiguationoperations,whichincreasestheevaluationcost. Toreducethecost,inthissection,
weextractemotionlabelsdirectlyfromtheoutputofAffectGPTforperformanceevaluation.
5.1 AblationStudyonStage1
ChoiceofEvaluationSet Video-LLaMAprovidespretrainedQ-Formers. Inthissection,wetryto
analyzewhethertheseweightscanhelpthemodelconvergeandachievebetterperformance. Before
comparing different initialization strategies, we need to determine which dataset should be used
for evaluation. In this paper, we have three choices: the training set, the test set, and the entire
EMER-Fine. InFigure1,wepresenttheresultsondifferentdatasets. Weobservethatincreasingthe
numberofsamplescanreducethefluctuationofaccuracyandhelpusdrawmorereliableconclusions.
Therefore, in stage1, we evaluate the performance on the entire EMER-Fine. It should be noted
thatfurtherincreasingthedatasetsizemayobtainmorestableresults,thereforeweplantoexpand
EMER-Fineinthefuture.
4(a) Pretrain+EMER-Fine(Whole) (b) Pretrain+EMER-Fine(Train) (c) Pretrain+EMER-Fine(Test)
(d) Random+EMER-Fine(Whole) (e) Random+EMER-Fine(Train) (f) Random+EMER-Fine(Test)
Figure1: Ablationstudyonstage1. Inthesefigures,wetrainmodelswithdifferentinitialization
strategiesandreporttheirresultsondifferentsets. Besidestheoriginalaccuracycurve,wealsoadda
smoothedcurve. Meanwhile,weintroducetwobaselineswithoutstage1training.
ImpartofInitializationStrategies Figure2revealstheimpactofdifferentinitializationstrategies.
Figure 2(a) shows the curve of training loss. We observe that the model converges around 100
epochs,whichprovestherationalityofourchoiceofthemaximumnumberofepochs. Meanwhile,
differentinitializationstrategiesonlyhaveimpactsintheinitialepochs,andthemodelwilleventually
converge to a similar loss. Figure 2(b) shows the emotion recognition results. We observe that
differentinitializationstrategieshavelimitedimpacts,provingthatourlarge-scaleEMER-Coarseis
sufficienttotrainthemodelfromrandomlyinitializedweights. Therefore,wecanconcludethatthe
initializationstrategyhaslimitedimpactinstage1training.
(a) Trainingloss (b) Accuracy
Figure2: Impactofdifferentinitializationstrategies. Weplotthecurveoftraininglossandaccuracy.
Asforaccuracy,weevaluatetheperformanceontheentireEMER-Fine.
ChoiceofLLMs ThissectionanalyzestheimpactofdifferentLLMs. TheoriginalVideo-LLaMA
usesVicuna(amodelbasedonLLaMA).WetrytoreplaceVicunawithLLaMA-2-Chat(amodel
basedonLLaMA-2)andstudyitsimpact. ThepretrainedQ-FormerprovidedbyVideo-LLaMAis
onlyusedtoconnectencodersandVicuna. IfwereplacetheLLM,wecannotusethesepretrained
weights. Forafaircomparison,allexperimentsadopttherandominitializationstrategy,andexperi-
5mentalresultsareshowninFigure3. Figure3(a)showsthetraininglossandFigure3(b)showsthe
emotionrecognitionresults. Interestingly,weobservethatthetraininglossofLLaMA-2islower
thanthatofVicuna,butVicunaperformsbetterthanLLaMA-2inemotionrecognition. Thereason
maybethatwefixtheweightsofLLMsanddonotuseLoRAforsupervisedfine-tuning,whichmay
limittheperformanceofLLaMA-2ondownstreamtasks. Meanwhile,theseresultsalsoprovethat
thereisnostrongcorrelationbetweentraininglossandtestaccuracy. Fromanotherperspective,these
resultsalsoshowthatLLMsaffecttheperformanceofAffectGPT.Therefore,weplantoexplorethe
impactofotherLLMsinthefuture.
(a) Trainingloss (b) Accuracy
Figure 3: Impact of different LLMs. We use the random initialization strategy and evaluate the
performanceontheentireEMER-Fine.
Effectiveness of Stage1 In Figures 1∼3, we add two baselines, both of which rely on Video-
LLaMA.Specifically,oneusesamulti-stepstrategy,i.e.,firstextractsemotion-relateddescriptions
fromVideo-LLaMAandthenusesthesedescriptionstodisambiguatesubtitles. Theotherdoesnot
useamulti-stepstrategy,i.e.,directlyinputsaudio-video-textcluesintoVideo-LLaMA.FromFigures
1∼3,wecanseethatnomatterwhichinitializationstrategyandwhichLLMareused,ourAffectGPT
alwaysoutperformstwobaselines. Theseresultsdemonstratetheeffectivenessofstage1. Thatis,
trainingonEMER-Coarseusuallyleadstoperformanceimprovements.
5.2 AblationStudyonStage2
ChoiceofEvaluationSet Instage1,wechoosetheentireEMER-Fineforperformanceevaluation.
Butforstage2,whichpartofthedatasetshouldweuse? Figure4showstheresultsondifferentsets.
InFigure4(b),weobservethatthetrainingaccuracysteadilyimproveswithincreasingepochs. These
resultsprovethatourmodelcanwellfittrainingdata. Itisnotappropriatetousethetrainingaccuracy
forperformanceevaluation. InFigure4(c),weobservethatthetestaccuracyfluctuatesgreatly. The
reasonmaybethatthetestdataislimited. Therefore,insubsequentanalysis,weusethesmoothed
testaccuracyforperformanceevaluation.
(a) EMER-Fine(Whole) (b) EMER-Fine(Train) (c) EMER-Fine(Test)
Figure4: Ablationstudyonstage2. Inthesefigures,weshowtheresultsondifferentsubsets.
6NecessityofTwo-stageTraining AffectGPTisatwo-stagetrainingframework. Butcanweonly
trainonstage2andignorestage1? Thissectionattemptstostudythenecessityofeachstageunder
differentinitializationstrategies. ExperimentalresultsareshowninFigure5. Fromthetrainingloss
(see Figures 5(a) and 5(c)), we observe that with the help of stage1, the model can obtain better
initializationweights,sothatitconvergesfasterduringstage2. Fromthetestaccuracy(seeFigures
5(b)and5(d)),weobservethatthemodelwithstage1usuallyperformsbetterthanthemodelswithout
stage1. Thisphenomenonismoreobviousundertherandominitializationstrategy. Fromanother
perspective, wecannotignorestage1andusetherandominitializationstrategyatthesametime.
LimitedEMER-Fineisnotenoughtotrainawell-performingmodelfromscratch.
(a) Trainingloss(pretrain) (b) Accuracy(pretrain)
(c) Trainingloss(random) (d) Accuracy(random)
Figure5: Necessityoftwo-stagetrainingframework.
5.3 MainResults
InTable4,weshowtheperformanceofAffectGPTonthetestsetofEMER-Fineunderdifferent
initializationstrategies. ComparedwiththeoriginalVideo-LLaMA(w/ostage1andw/ostage2),
trainingonEMER-CoarseandEMER-Fineremarkablyimprovestheperformance. Theseresults
revealthequalityofourEMER-Coarsedataset. Meanwhile,two-stageresultsaregenerallybetter
thanone-stageresults,whichfurtherdemonstratestheeffectivenessofourtwo-stageframework.
Table4: PerformanceofAffectGPT.WereportresultsonthetestsetofEMER-Fine.
PretrainedWeights RandomWeights
Stage1 Stage2
Avg Accuracys Recalls Avg Accuracys Recalls
– – 28.64 32.22 25.05 05.87 07.58 04.17
– best 61.75 62.03 61.46 58.22 59.60 56.84
50-epoch – 53.82 48.04 59.60 50.06 42.36 57.76
50-epoch best 62.78 63.11 62.45 65.08 64.29 65.86
100-epoch – 56.65 47.53 65.78 48.04 40.51 55.56
100-epoch best 64.56 64.49 64.62 62.88 65.91 59.85
76 Conclusion
EMERisanewlyproposedtaskthataimstoachievereliableandaccurateemotionrecognition. To
promoteitsdevelopment, weproposeEMER-Coarse(alarge-scalecoarsely-labeleddataset)and
AffectGPT(atwo-stagetrainingframework). Meanwhile,werevealtheimpactofeachmoduleand
studytheinfluenceofdifferentinitializationstrategiesandLLMs. Overall,thispapercanserveasa
complementtoexistingworksonEMER.
References
[1] ZhengLian,LicaiSun,YongRen,HaoGu,HaiyangSun,LanChen,BinLiu,andJianhuaTao.
Merbench: Aunifiedevaluationbenchmarkformultimodalemotionrecognition. arXivpreprint
arXiv:2401.03429,2024.
[2] PaulEkmanandDacherKeltner. Universalfacialexpressionsofemotion. Californiamental
healthresearchdigest,8(4):151–158,1970.
[3] ZhengLian,LicaiSun,MingyuXu,HaiyangSun,KeXu,ZhuofanWen,ShunChen,BinLiu,
andJianhuaTao. Explainablemultimodalemotionreasoning. arXivpreprintarXiv:2306.15401,
2023.
[4] ZhengLian,HaiyangSun,LicaiSun,ZhuofanWen,SiyuanZhang,ShunChen,HaoGu,Jinming
Zhao,ZiyangMa,XieChen,etal. Mer2024: Semi-supervisedlearning,noiserobustness,and
open-vocabularymultimodalemotionrecognition. arXivpreprintarXiv:2404.17113,2024.
[5] OpenAI. Chatgpt,2022.
[6] YunfeiChu,JinXu,XiaohuanZhou,QianYang,ShiliangZhang,ZhijieYan,ChangZhou,and
JingrenZhou. Qwen-audio: Advancinguniversalaudiounderstandingviaunifiedlarge-scale
audio-languagemodels. arXivpreprintarXiv:2311.07919,2023.
[7] JiamingHan,KaixiongGong,YiyuanZhang,JiaqiWang,KaipengZhang,DahuaLin,YuQiao,
PengGao,andXiangyuYue. Onellm: Oneframeworktoalignallmodalitieswithlanguage.
arXivpreprintarXiv:2312.03700,2023.
[8] Yaoxun Xu, Hangting Chen, Jianwei Yu, Qiaochu Huang, Zhiyong Wu, Shi-Xiong Zhang,
GuangzhiLi,YiLuo,andRongzhiGu. Secap: Speechemotioncaptioningwithlargelanguage
model. InProceedingsoftheAAAIConferenceonArtificialIntelligence,pages19323–19331,
2024.
[9] ChangliTang,WenyiYu,GuangzhiSun,XianzhaoChen,TianTan,WeiLi,LuLu,ZejunMA,
andChaoZhang. Salmonn: Towardsgenerichearingabilitiesforlargelanguagemodels. In
TheTwelfthInternationalConferenceonLearningRepresentations,2023.
[10] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,JingkangYang,andZiweiLiu. Otter: A
multi-modalmodelwithin-contextinstructiontuning. arXivpreprintarXiv:2305.03726,2023.
[11] KunChangLi,YinanHe,YiWang,YizhuoLi,WenhaiWang,PingLuo,YaliWang,LiminWang,
andYuQiao. Videochat: Chat-centricvideounderstanding. arXivpreprintarXiv:2305.06355,
2023.
[12] HangZhang,XinLi,andLidongBing.Video-llama:Aninstruction-tunedaudio-visuallanguage
modelforvideounderstanding. arXivpreprintarXiv:2306.02858,2023.
[13] BinLin,BinZhu,YangYe,MunanNing,PengJin,andLiYuan. Video-llava: Learningunited
visualrepresentationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023.
[14] KunchangLi,YaliWang,YinanHe,YizhuoLi,YiWang,YiLiu,ZunWang,JilanXu,Guo
Chen,PingLuo,LiminWang,andYuQiao. Mvbench: Acomprehensivemulti-modalvideo
understandingbenchmark. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,2024.
8[15] YanweiLi,ChengyaoWang,andJiayaJia. Llama-vid: Animageisworth2tokensinlarge
languagemodels. arXivpreprintarXiv:2311.17043,2023.
[16] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,
Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large
languagemodelswithmultimodality. arXivpreprintarXiv:2304.14178,2023.
[17] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFahadShahbazKhan. Video-chatgpt:
Towardsdetailedvideounderstandingvialargevisionandlanguagemodels. arXivpreprint
arXiv:2306.05424,2023.
[18] PengJin,RyuichiTakanobu,CaiwanZhang,XiaochunCao,andLiYuan. Chat-univi: Unified
visualrepresentationempowerslargelanguagemodelswithimageandvideounderstanding.
arXivpreprintarXiv:2311.08046,2023.
[19] OpenAI. Gpt-4v(ision)systemcard,2023.
[20] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. arXiv
preprintarXiv:2304.08485,2023.
[21] DeyaoZhu, JunChen, XiaoqianShen, XiangLi, andMohamedElhoseiny. Minigpt-4: En-
hancingvision-languageunderstandingwithadvancedlargelanguagemodels. arXivpreprint
arXiv:2304.10592,2023.
9A DetailsaboutMLLMs
InTable5,weprovidemodelcardsfordifferentMLLMs.
Table5: ModelcardsforMLLMs.
Models SupportModality Link
Otter Video,Text https://github.com/Luodian/Otter
VideoChat Video,Text https://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat
VideoChat2 Video,Text https://github.com/OpenGVLab/Ask-Anything/tree/main/video_chat2
Video-LLaVA Video,Text https://github.com/PKU-YuanGroup/Video-LLaVA
Video-LLaMA Video,Text https://github.com/DAMO-NLP-SG/Video-LLaMA
Video-ChatGPT Video,Text https://github.com/mbzuai-oryx/Video-ChatGPT
LLaMA-VID Video,Text https://github.com/dvlab-research/LLaMA-VID
mPLUG-Owl Video,Text https://github.com/X-PLUG/mPLUG-Owl
Chat-UniVi Video,Text https://github.com/PKU-YuanGroup/Chat-UniVi
SALMONN Audio,Text https://github.com/bytedance/SALMONN
Qwen-Audio Audio,Text https://github.com/QwenLM/Qwen-Audio
SECap Audio,Text https://github.com/thuhcsi/SECap
OneLLM Audio,Video,Text https://github.com/csuhan/OneLLM
PandaGPT Audio,Video,Text https://github.com/yxuansu/PandaGPT
B BaselineResultsonDifferentSubsets
InTables6∼8,wereporttheresultsofdifferentMLLMsonthreesubsetsofEMER-Fine.
Table6: MainresultsontheentireEMER-Fine(332samples).
English Chinese
Model L V A
Avg Accuracys Recalls Avg Accuracys Recalls
Audio+Subtitle
√ √
Qwen-Audio[6] √ × √ 40.23±0.09 49.42±0.18 31.04±0.00 43.53±0.04 53.71±0.00 33.34±0.09
OneLLM[7] √ × √ 43.04±0.06 45.92±0.05 40.15±0.06 46.77±0.01 52.07±0.06 41.47±0.08
SECap[8] √ × √ 46.94±0.10 54.52±0.15 39.37±0.05 47.09±0.15 55.55±0.23 38.64±0.08
SALMONN[9] × 48.06±0.04 50.20±0.04 45.92±0.04 48.53±0.03 52.24±0.00 44.82±0.05
Video+Subtitle
√ √
Otter[10] √ √ × 44.40±0.09 50.71±0.10 38.09±0.09 46.92±0.04 52.65±0.16 41.18±0.08
VideoChat[11] √ √ × 45.70±0.09 42.90±0.27 48.49±0.10 45.63±0.04 47.20±0.12 44.05±0.05
Video-LLaMA[12] √ √ × 44.74±0.14 44.14±0.13 45.34±0.15 47.27±0.03 47.98±0.07 46.56±0.01
Video-LLaVA[13] √ √ × 47.12±0.15 48.58±0.02 45.66±0.29 49.59±0.05 53.95±0.03 45.23±0.13
VideoChat2[14] √ √ × 49.60±0.28 54.72±0.41 44.47±0.15 49.90±0.06 57.12±0.08 42.68±0.04
OneLLM[7] √ √ × 50.99±0.08 55.93±0.09 46.06±0.06 51.84±0.08 56.43±0.04 47.26±0.11
LLaMA-VID[15] √ √ × 51.29±0.09 52.71±0.18 49.87±0.00 52.45±0.02 57.30±0.00 47.61±0.03
mPLUG-Owl[16] √ √ × 52.79±0.13 54.54±0.13 51.04±0.13 51.43±0.03 56.40±0.11 46.47±0.18
Video-ChatGPT[17] √ √ × 50.73±0.06 54.03±0.04 47.44±0.07 55.34±0.02 61.15±0.10 49.52±0.06
Chat-UniVi[18] × 53.09±0.01 53.68±0.00 52.50±0.02 54.20±0.02 58.54±0.01 49.86±0.03
Audio+Video+Subtitle
√ √ √
SECap+mPLUG-Owl √ √ √ 57.71±0.05 50.05±0.23 65.38±0.33 55.22±0.22 51.65±0.27 58.79±0.16
SALMONN+Video-ChatGPT √ √ √ 58.71±0.24 53.16±0.17 64.26±0.31 55.10±0.16 53.44±0.14 56.76±0.19
SECap+Video-ChatGPT √ √ √ 57.41±0.09 52.03±0.04 62.79±0.14 56.49±0.02 56.50±0.01 56.48±0.05
SECap+Chat-UniVi √ √ √ 59.13±0.08 48.85±0.29 69.41±0.13 56.49±0.14 52.38±0.07 60.59±0.22
SALMONN+mPLUG-Owl √ √ √ 59.77±0.05 51.77±0.01 67.76±0.11 55.94±0.21 51.74±0.19 60.14±0.23
SALMONN+Chat-UniVi 59.47±0.08 51.62±0.00 67.31±0.15 57.54±0.06 51.65±0.06 63.42±0.06
√ √ √
EMER(Multi) 80.05±0.24 80.03±0.37 80.07±0.10 85.20±0.03 87.09±0.00 83.31±0.05
10Table7: MainresultsonthetrainingsetofEMER-Fine(266samples).
English Chinese
Model L V A
Avg Accuracys Recalls Avg Accuracys Recalls
Audio+Subtitle
√ √
Qwen-Audio(a)[6] √ × √ 40.62±0.14 50.03±0.22 31.22±0.06 44.90±0.06 55.40±0.00 34.40±0.11
OneLLM[7] √ × √ 43.65±0.01 46.75±0.03 40.55±0.02 47.68±0.08 53.21±0.20 42.15±0.05
SECap[8] √ × √ 45.49±0.03 52.82±0.00 38.17±0.06 45.10±0.19 53.14±0.29 37.05±0.10
SALMONN[9] × 47.26±0.05 49.21±0.05 45.31±0.05 47.93±0.22 51.22±0.19 44.63±0.25
Video+Subtitle
√ √
Otter[10] √ √ × 46.06±0.12 52.82±0.13 39.30±0.11 48.40±0.11 54.47±0.20 42.34±0.02
VideoChat[11] √ √ × 45.54±0.00 43.25±0.15 47.82±0.15 45.79±0.11 47.78±0.00 43.80±0.22
Video-LLaMA[12] √ √ × 45.68±0.13 45.31±0.11 46.05±0.15 47.45±0.07 48.42±0.09 46.48±0.05
Video-LLaVA[13] √ √ × 48.20±0.10 49.37±0.03 47.04±0.24 50.63±0.03 55.13±0.03 46.13±0.03
VideoChat2[14] √ √ × 51.03±0.39 56.08±0.51 45.97±0.27 50.31±0.05 57.45±0.09 43.16±0.00
OneLLM[7] √ √ × 50.39±0.14 55.25±0.16 45.54±0.13 49.86±0.10 54.39±0.05 45.33±0.14
LLaMA-VID[15] √ √ × 51.39±0.03 52.96±0.07 49.81±0.14 52.12±0.00 56.76±0.00 47.49±0.01
mPLUG-Owl[16] √ √ × 53.78±0.13 56.08±0.19 51.47±0.07 51.72±0.12 57.42±0.20 46.03±0.03
Video-ChatGPT[17] √ √ × 51.88±0.07 55.03±0.06 48.73±0.09 54.67±0.02 60.97±0.13 48.37±0.08
Chat-UniVi[18] × 53.06±0.14 53.53±0.09 52.60±0.19 53.41±0.01 58.22±0.02 48.61±0.00
Audio+Video+Subtitle
√ √ √
SECap+mPLUG-Owl √ √ √ 56.07±0.02 48.11±0.38 64.02±0.35 54.27±0.21 50.73±0.18 57.81±0.24
SALMONN+Video-ChatGPT √ √ √ 58.46±0.18 53.09±0.04 63.84±0.32 55.17±0.05 52.60±0.04 57.74±0.14
SECap+Video-ChatGPT √ √ √ 57.16±0.02 52.13±0.00 62.18±0.05 56.84±0.11 57.76±0.06 55.91±0.16
SECap+Chat-UniVi √ √ √ 58.82±0.08 48.22±0.20 69.42±0.03 54.74±0.03 51.03±0.10 58.44±0.05
SALMONN+mPLUG-Owl √ √ √ 58.44±0.00 50.91±0.08 65.98±0.08 55.27±0.18 51.22±0.16 59.33±0.19
SALMONN+Chat-UniVi 58.69±0.04 50.59±0.01 66.79±0.09 57.85±0.05 52.51±0.05 63.20±0.04
√ √ √
EMER(Multi) 80.23±0.25 79.81±0.44 80.65±0.06 84.68±0.02 87.02±0.09 82.34±0.06
Table8: MainresultsonthetestsetofEMER-Fine(66samples).
English Chinese
Model L V A
Avg Accuracys Recalls Avg Accuracys Recalls
Audio+Subtitle
√ √
Qwen-Audio[6] √ × √ 38.66±0.13 46.97±0.00 30.35±0.25 38.03±0.00 46.97±0.00 29.09±0.00
OneLLM[7] √ × √ 40.56±0.32 42.55±0.38 38.56±0.25 43.09±0.35 47.47±0.51 38.70±0.19
SECap[8] √ × √ 52.78±0.63 61.36±0.76 44.19±0.51 55.05±0.00 65.15±0.00 44.95±0.00
SALMONN[9] × 51.28±0.00 54.17±0.00 48.38±0.00 50.93±0.76 56.31±0.76 45.56±0.76
Video+Subtitle
√ √
Otter[10] √ √ × 37.72±0.00 42.22±0.00 33.22±0.00 41.05±0.24 45.45±0.00 36.64±0.48
VideoChat[11] √ √ × 46.34±0.45 41.49±0.77 51.19±0.13 44.98±0.62 44.90±0.61 45.06±0.64
Video-LLaMA[12] √ √ × 40.97±0.15 39.44±0.18 42.50±0.13 46.55±0.13 46.25±0.00 46.84±0.25
Video-LLaVA[13] √ √ × 42.75±0.35 45.38±0.20 40.13±0.51 45.44±0.38 49.24±0.00 41.64±0.76
VideoChat2[14] √ √ × 43.83±0.16 49.24±0.00 38.42±0.32 48.29±0.47 55.81±0.76 40.77±0.19
OneLLM[7] √ √ × 53.40±0.19 58.65±0.19 48.14±0.19 59.84±0.00 64.65±0.00 55.03±0.00
LLaMA-VID[15] √ √ × 50.90±0.60 51.69±0.63 50.11±0.57 53.78±0.06 59.47±0.00 48.08±0.13
mPLUG-Owl[16] √ √ × 48.84±0.13 48.33±0.13 49.34±0.38 50.25±0.63 52.27±0.25 48.23±1.01
Video-ChatGPT[17] √ √ × 46.12±0.00 50.00±0.00 42.25±0.00 58.02±0.00 61.87±0.00 54.17±0.00
Chat-UniVi[18] × 53.20±0.54 54.29±0.38 52.11±0.69 57.37±0.06 59.85±0.00 54.90±0.13
Audio+Video+Subtitle
√ √ √
SECap+mPLUG-Owl √ √ √ 64.42±0.32 57.95±0.38 70.90±0.26 59.00±0.25 55.30±0.63 62.70±0.13
SALMONN+Video-ChatGPT √ √ √ 59.71±0.47 53.48±0.69 65.93±0.25 54.82±0.63 56.82±0.88 52.83±0.38
SECap+Video-ChatGPT √ √ √ 58.43±0.35 51.60±0.19 65.26±0.51 55.11±0.35 51.45±0.32 58.76±0.38
SECap+Chat-UniVi √ √ √ 60.38±0.07 51.39±0.66 69.37±0.53 63.71±0.83 57.98±0.74 69.43±0.91
SALMONN+mPLUG-Owl √ √ √ 65.15±0.26 55.28±0.26 75.03±0.26 58.57±0.35 53.78±0.32 63.36±0.38
SALMONN+Chat-UniVi 62.64±0.22 55.84±0.06 69.44±0.38 56.25±0.11 48.17±0.09 64.33±0.13
√ √ √
EMER(Multi) 79.31±0.19 80.91±0.13 77.70±0.25 87.29±0.19 87.37±0.38 87.20±0.00
11