Disentangled Representation Learning through Geometry
Preservation with the Gromov-Monge Gap
Th´eo Uscidda1,2,∗ Luca Eyring2,3,4,∗ Karsten Roth2,4,5 Fabian Theis2,3,4
Zeynep Akata2,3,4,† Marco Cuturi1,6,†
1CREST-ENSAE 2Helmholtz Munich 3Technical University of Munich
4MCML 5Tu¨bingen AI Center, University of Tu¨bingen 6Apple
theo.uscidda@ensae.fr luca.eyring@tum.de
Abstract
Learning disentangled representations in an unsupervised manner is a fundamental challenge in
machine learning. Solving it may unlock other problems, such as generalization, interpretability, or
fairness. While remarkably difficult to solve in general, recent works have shown that disentanglement
is provably achievable under additional assumptions that can leverage geometrical constraints, such as
local isometry. To use these insights, we propose a novel perspective on disentangled representation
learning built on quadratic optimal transport. Specifically, we formulate the problem in the Gromov-
Monge setting, which seeks isometric mappings between distributions supported on different spaces. We
propose the Gromov-Monge-Gap (GMG), a regularizer that quantifies the geometry-preservation of an
arbitrary push-forward map between two distributions supported on different spaces. We demonstrate
the effectiveness of GMG regularization for disentanglement on four standard benchmarks. Moreover, we
show that geometry preservation can even encourage unsupervised disentanglement without the standard
reconstruction objective - making the underlying model decoder-free, and promising a more practically
viable and scalable perspective on unsupervised disentanglement.
1 Introduction
Learninglow-dimensionalrepresentationsofhigh-dimensionaldataisafundamentalchallengeinunsupervised
deep learning [5, 43], where the emphasis is put on learning representations that allow for efficient adaptation
across a wide range of tasks [5, 28, 43]. Disentanglement [5, 27, 28, 43, 60] has shown significant promise in
facilitating such generalization [4, 5, 27, 28, 29, 30, 43, 44, 60], alongside interpretability and fairness [42, 67].
Mostworks[5,13,27,33,43,60]regarddisentanglementasaone-to-onemapbetweenlearnedrepresentations
and ground-truth latent factors; seeking to recover these factors from data alone in an unsupervised fashion.
While unsupervised disentanglement is theoretically impossible [43], the inductive biases of autoencoder
architectures ensure effective disentanglement in practice [29, 59, 74]. Most approaches operate on variational
autoencoder(VAE)frameworks[35],usingobjectivesthatmatchlatentVAEposteriorstofactorizedpriors[10,
13, 27, 33, 38]. Recent works [29, 31, 51] provide a new perspective, showing how geometric constraints
on representation spaces may enable disentanglement. In particular, Horan et al. [29] demonstrate that
unsupervised disentanglement is always possible under the assumption of local isometry to the data and
sufficient non-Gaussianity of the generative factors, thus supporting the isometry desideratum.
In this work, we show how these geometric desiderata can be effectively quantified through the lens of
optimal transport (OT) theory [55, 61], by treating mapping to or from the latent space as transport maps T
from or to the data manifold, respectively. While the more classical tools described in the OT toolbox [55]
∗equalcontributionand†equaladvising
1
4202
luJ
01
]GL.sc[
1v92870.7042:viXraprovide tools to align distributions supported on the same space X, using an inter-domain cost c(x,y) for
any two points x,y∈X, such approaches are not well defined when mapping between latent and data space
with very different dimensionalities, for which no “natural” cost function exists.
Instead, we rely in this work on the Gromov-Wasserstein (GW) formalism [48, 63, 64, 69] formulation of
OT, which considers instead intra-domain costs c ,c in each space. This formulation seeks instead the
X Y
most geometry-preserving mapping between two distributions by favoring maps that minimize the distortion
between intra-domain costs by comparing c (x,x′),c (T(x),T(x′)). While this distortion itself can be used
X Y
as a regularization for a given map T as done in Nakagawa et al. [51], we borrow a page from the Monge
gap formalism proposed by Uscidda and Cuturi [68] to propose a modified and better-posed regularizer that
substracts to the distortion of T, the best distortion that can be achieved by any map (or a proxy thereof).
In a nod to [68], we propose the novel Gromov-Monge gap (GMG), which measures if T maps points
while preserving geometric properties as much as possible - such as (scaled) isometry (distance preserving) or
conformity (angle preserving). In contrast to the distortion, the GMG takes the most geometry-preserving
mapping into account, resulting in a regularizer that is properly defined, as shown in our various results,
showing notably that the GMG and its finite sample version are weakly convex. We show how the GMG
can serve as an effective regularizer to different geometry-preserving desiderata [29, 41]. Motivated by this
formalism, we build upon the Monge gap - a regularizer introduced in Uscidda and Cuturi [68] that measures
whether a map T transports a reference distribution at minimal displacement cost - to propose the novel
Gromov-Monge gap (GMG), which measures if T maps points while preserving geometric properties as
much as possible - such as (scaled) isometry (distance preserving) or conformity (angle preserving). In
contrast to the distortion, the GMG takes the most geometry-preserving mapping into account, resulting in a
regularizer that is properly defined, as shown in our various results highlighting that the GMG and its finite
sample version are weakly convex. We lay out how the GMG can serve as an effective regularizer to different
geometry-preserving desiderata [29, 41].
Our experiments on four standard disentangled representation learning benchmarks show that the
integration of these geometry-preserving desiderata through the Gromov-Monge Gap (GMG) significantly
improves disentanglement performance across various methods, from the standard β-VAE to the combination
of β-TCVAE with support factorization [60], outperforming a distortion-based regularization. Moreover,
we demonstrate that these geometric regularizations can replace the standard reconstruction loss, enabling
measurableunsuperviseddisentanglementevenwithout a decoder,whichisnotfeasibleinstandardframeworks
that rely on the decoder-based reconstruction term to prevent collapse. This finding suggests the potential for
more scalable unsupervised disentangled representation learning approaches and bridges to popular, weakly-
or self-supervised encoder-only representation learning methods [3, 14, 24, 72].
2 Background and Related Works
2.1 Disentangled Representation Learning
The Disentanglement Formalism. Disentanglement has varying operational definitions [28, 43, 60]. In
this work, we follow the common understanding [43, 44, 60, 67] where data x is generated by a process p(x|z)
operating on ground-truth latent factors z∼p(z), modeling underlying source of variations (s.a. object shape,
color, background...). Given a dataset D ={x }N , unsupervised disentangled representation learning aims
i i=1
to find a mapping e s.t. e (x )≈E[z|x ], up to element-wise transformations. Notably, this is to be achieved
ϕ ϕ i i
without prior information on p(z) and p(x|z).
Unsupervised Disentanglement through Prior Matching. Most unsupervised disentanglement
methods operate on variational autoencoders (VAEs)[35], which define a generative model of the form
p (x,z) = p(z)p (x|z). Here, p (x|z) is a product of exponential family distributions with parameters
θ θ θ
computed by a decoder d (z). The latent prior p(z) is usually chosen to be a normal Gaussian N(0,I), and
θ
the probabilistic encoder q (z|x) is realized through a neural network e (x) that predicts the parameters of
ϕ ϕ
2the latent such that q (z|x)=N(z|e (x)). The β-VAE [27]
ϕ ϕ
L (θ,ϕ):=E [logp (x|z)−βD (q (z|x)||p(z))] (1)
β x∼pdata,z∼qϕ(z|x) θ KL ϕ
achieves disentanglement by enforcing stronger, β-weighted prior matching on top of the reconstruction
objective, assuming statistical factor independence [60]. Several follow-ups refine latent prior matching
through different objectives or prior choices [10, 13, 38, 59].
Disentanglement through a Geometric Lens. Recent studies [12, 26, 31, 41, 51] indicate that disen-
tanglement can arise by encouraging learned representations to preserve meaningful geometric features of the
data, suchasscaleddistancesbetweensamples. Notably, Horanetal.[29]demonstratedthatdisentanglement
is provably feasible when the generative factors are sufficiently non-Gaussian and locally isometric to the
data. In this work, we explore how to promote geometry preservation using quadratic OT between the latent
and data spaces, which we introduce in the next section.
2.2 Quadratic Optimal Transport
Gromov-{Monge, Wasserstein} Formulations. OT [55] involves transferring one probability distribu-
tion to another while incorporating inductive biases. When these distributions lie on incomparable domains,
the task is addressed using the Gromov-Monge and GW problems, also known as OT quadratic formulations.
Formally, consider two compact X ⊂ RdX, Y ⊂ RdY, each of them equipped with an intra-domain cost
c : X ×X → R and c : Y ×Y → R. For p ∈ P(X) and q ∈ P(Y)—two distributions supported on
X Y
each domain—the Gromov-Monge problem [50] seeks a map T :X →Y that push-forwards p onto q, while
minimizing the distortion of the costs:
(cid:90)
inf 1|c (x,x′)−c (T(x),T(x′))|2dp(x)dp(x′). (GMP)
2 X Y
T:T♯p=q X×X
When it exists, we call a solution T⋆ to (GMP) a Gromov-Monge map for costs c ,c . However, solving this
X Y
problem is difficult, and existence is not guaranteed in general [19]. Moreover, this formulation is ill-suited for
discrete distributions p,q, as the constraint set might be empty. Replacing maps by coupling π ∈Π(p,q), i.e.
distributions on X ×Y with marginals p and q, we obtain the Gromov-Wasserstein (GW) problem [48, 64]
(cid:90)
GWcX,cY(p,q):= min 1|c (x,x′)−c (y,y′)|2dπ(x,y)dπ(x′,y′). (GWP)
2 X Y
π∈Π(p,q) (X×Y)2
A solution π⋆ of (GWP) always exists, making GWcX,cY(p,q) a well-defined quantity. It quantifies the
minimal distortion of the geometries induced by c and c achievable when coupling p and q.
X Y
Discrete Solvers. When both p and q are instantiated as samples, the GW Prob. ((GWP)) translates to a
quadratic assignment problem, whose objective can be regularized using entropy [15]. For empirical measures
p = 1 (cid:80)n δ , q = 1 (cid:80)n δ and ε≥0, we set:
n n i=1 xi n n j=1 yj
n
(cid:88)
GWc εX,cY(p n,q n):= min (c X(x i,x j)−c Y(y i,y j))2P ijP
i′j′
−εH(P), (EGWP)
P∈Uni,j,i′,j′=1
where U = {P ∈ Rn×n,P1 = PT1 = 11 } and H(P) = −(cid:80)n P log(P ). As ε → 0, we recover
n + n n n n i,j=1 ij ij
GWcX,cY =GWcX,cY. In addition to yielding better statistical [73] and regularity [58] properties, entropic
0
regularization also enhances computational performance. In practice, we can solve (EGWP) using a mirror
descent scheme that iterates the Sinkhorn algorithm [54, 62].
3Neural Solvers. While for classical OT, numerous neural methods have been proposed [22, 37, 45, 66, 68],
the GW setting has received less attention. To our knowledge, the only neural Gromov-Monge formulation
proposed thus far is [52], which involves a min-max-min optimization procedure. On the other hand, Klein
et al. [36] recently proposed an approach to compute neural GW couplings.
3 Disentanglement with the Gromov-Monge gap
This section details our novel optimal transport perspective to achieve disentanglement from geometric
considerations(see§2.1),usingtheVAEframework. Toachievethis,wefirstinvestigatehowonecanpromote
an arbitrary map T :X →Y between two domains X and Y to preserve predefined geometric features. In a
VAE, T can represent either the encoder e , which produces latent codes from the data, or the decoder d ,
ϕ θ
which reconstructs the data from the latent codes. As a result, in the former, the source domain X is the
data, and the target domain Y is the latent space, with roles swapped in the latter. If we assume that d
θ
perfectly reconstructs the data from the latents produced by e , it is equivalent whether e preserves the
ϕ ϕ
geometric features from data to latents or d preserves them from latents to data. Consequently, in what
θ
follows, T can refer to either the encoder or the decoder without distinction.
Outline. Leveraging this perspective, this section begins by defining cost functions to encode geometric
featuresandthenotionofdistortionsin§3.1. Weleveragethisconceptin§3.2tointroducetheGromov-Monge
Gap (GMG), a regularizer that measures whether a map moves points while preserving geometric features as
much as possible, i.e., minimizing distortion. §3.3 then shows how the GMG can be estimated and computed
fromsamplestobepracticallyapplicableintheVAEframework,whichtransitionsinto§3.4studyingconvexity
properties of the GMG. Put together, §3.1-§3.4 define the practical GMG which allows us to learn a latent
space that matches, as much as possible, geometrical constraints in the data space. Finally in §3.5, we use
the GMG with different choices of costs to propose effective disentangled representation learning objectives.
3.1 From the distortion...
We encode the geometric features of interest through two cost functions defined on each domain: c :
X
X ×X →R and c :Y ×Y →R. We then want T to preserve these costs, i.e., c (x,x′)≈c (T(x),T(x′))
Y X Y
for x,x′ ∈X. In practice, two types of cost functions are particularly meaningful.
[i] (Scaled) squared Euclidean distance: c (x,x′) = ∥x−x′∥2 and c = α2∥y−y∥2, with α > 0. A
X 2 Y 2
map T preserving c ,c preserves the scaled distances between the points, i.e. it is a scaled isometry.
X Y
When α=1, we recover the standard definition of an isometry.
[ii] Cosine similarity: c (x,x′) = cos-sim(x,x′) := ⟨ x , x′ ⟩ and c (y,y′) = cos-sim(y,y′) similarly.
X ∥x∥2 ∥x′∥2 Y
On has cos-sim(x,x′)=cos(θ ) where θ is the angle between x and x′. A map T preserving c ,c
x,x′ x,x′ X Y
then preserves the angles between the points, i.e. it is a conformal map. Note that if T is (scaled) isometry
(see above), it is automatically a conformal map.
In the following, we say that c ,c are [i] or [ii] if they belong to these families of costs. Introducing a
X Y
reference distribution r ∈ P(X), weighting the areas of X where we penalize deviations of c (x,x′) from
X
c (T(x),T(x′)), we can quantify this property using the following criterion.
Y
Definition 3.1 (Distortion). The distortion (DST) of a map T is defined as:
(cid:90)
DcX,cY(T):= 1(c (x,x′)−c (T(x),T(x′)))2dr(x)dr(x′) (DST)
r 2 X Y
X×X
DcX,cY(T) quantifies how much T distorts the geometric features induced by c and c on the support
r X Y
of r, i.e., when DcX,cY(T) = 0, one has c (x,x′) = c (T(x),T(x′)) for x,x′ ∈ Spt(r). In disentangled
r X Y
representation learning, it can be desirable to regularize the decoder to be isometric [29, 31, 51]. However,
a fully geometry-preserving mapping might not necessarily exist between the latent space and the data
4<<llaatteexxiitt sshhaa11__bbaassee6644==""WWOOHHMM88BBKKqq77PPiivv00bb11lljjGGwwHHddooddIIPP55ww=="">>AAAAAACCxxHHiiccjjVVHHLLSSssNNAAFFDD22NNrr11ppffVVZZdduuggkkVVwwVVRRKKpp66LLIIooiiMMssWW77AAOO00SSDDKKdd11qqFF55OOZZkkIIppeeggPPuuNNVVvvEE//99AA//88II77YYwwppqqEEZZ22QQ55MMyy559955yyZZee66++ffBBCCJJVVjjvvNNaassOObbmmFFxxaaXXiissuullllddWW1199YY33yy55llYY77jjTTPPJJeeIIvvFFQQSSyy77vvppffyyQQEESS88ppYYQQKKeeDDeeRR33AAvv99ggHHff8800aammOOdd++6644TTEEUUccXXaahhxxwwnnuuhhNN44zzEEQQDDBBPPEEddWW88vvSS55XXnnKKppjjlljj00LL33BBxxUUkkKK99GGXXHH77BBFFffqqIIwwZZAAhhBBEEccEERRTTiiAAhh55SSeeSS77hhwwkkBBDDXXww44QQ44SSUUiiYYOOMMcc99SSqqTTNNKKIIttTThhkkffssiiLL55DD22ll33mmbbEERR7777ZZkkaaNNaaNNTTAAnnoollKKWW33sskkSSaammPPEEllYYnn22aabbeeGGaaccNNffuubb9988RR4466rruuNN66ee//nnXXiiGGxxCCjjffEE//qqWWbbZZvv55XXpp22ttRRGGOODDYY11CCCCooppssQQwwuujjqqWWuu22SSmmKK//rrmm99ppeeqqFFDDkkkkxxGGnnccpp77ggkkzzIIxxyy22mmffbbaaFFJJTTuu++66ttZZ++JJvvJJllOOzzeess//yy33AAzzvv++ppYY00YYPPffnnOOGGddBB++66DDqq11qqqqHHzzVVqqllffppKKPPuuooggdd77GGKKff55nnmmEEOOss77RRQQMMtt44PP++IIJJzz99aaZZFFVViippllXX22mmWWooVVccss4411vvyy33rr44AAFF00UUjj3344==<<//llaatteexxiitt>>
<<llaatteexxiitt sshhaa11__bbaassee6644==""oowwYYRRSSEEppggggxxFFTTffPP99EEGGxxppFFMM33GGggeeRREE=="">>AAAAAACCxxHHiiccjjVVHHLLSSssNNAAFFDD22NNrr11ppffVVZZdduuggkkVVwwVVRRKKpp66LLIIooiiMMssWW77AANNqqkkSSSSdd11ttCC88mmJJkkIIppeeggPPuuNNVVvvEE//99AA//88II7744xxTTUUIIjjoohhyyZZllzz77zzkkzz991144//ii00IIhhHHeeee11YYCC00ssLLii22vvFFFFddLLaa++ssbbmm11vvll77ZZ2222SSHHMMeessFFaaQQRRiinnvv++pp55ggUUZZiiwwllggxxllxxLLooZZZZ1177ssRR66zzjjjj8899VVvvHHPPHHuuAAjjTT55EEppOOMMttaaPPvvVVEESSDDssPPAAkk00QQ11ss55ttyyxxaakk66eettnnzzwwDDWWggAArrMMaaaaffkkFF11xxggggRRYYAAccMMRRggSSSSMMIIRRPPAAhh66eennDDhhIICCOOuujjyyllxxnnFFCCoo44wwzz33KKJJEE22ppyyxxGGGGRR66xxYY//qqOOaaNNcczzbbEEJJ7755SSmm00OOqqBBTTIInnoo55KKWW00cckkCCaallPPEE55YYnnWWbbrreeKK66ddFFffuubb9911RR77qqrrttNN66OO88bbrr55hhYYiiVVttii//99LLNNMMvv++rrUU77VVIIDDHHGGqqaawwiippppkkwwzzqqrrrrAAuuOOSS66KK++rrmm99ppeeqqJJDDllkkxxCCkk88ooDDggnnHHGGjjllrrMM++2211gghhdduu++qqttpp++NNvvOOllOOxxaahh++YY33BBzzvv66ppYY00YYPPffnnOOOOddBB++66jjqq11qqrrHHzzVVqqllffmmZZGGXXccQQee99nnFFII88zzxxBBHHZZddooooKKWW99HH//GGEEZZ++vvCCiiiixxhh55ZZ++ppVVssFFooddvvFFttWWQQ88ffWWrrSSPPffQQ====<<//llaatteexxiitt>>
<latexit sha1_base64="AW9izWFVa/+JUF/eQvYTjFzhE4I=">AAAC2HicjVHLSsNAFD3GV31Hu3QTLELdlEQUXYoudCNUsFXUWibjtA7mRTIRShHciVt/wK1+kfgH+hfeGSP4QHRCkjPn3nNm7r1+EshMue7zgDU4NDwyWhobn5icmp6xZ+eaWZynXDR4HMTpoc8yEchINJRUgThMUsFCPxAH/sWWjh9cijSTcbSveolohawbyY7kTBHVtssnIVPnnAX97d2r0+V2Ut1fatsVt+aa5fwEXgEqKFY9tp9wgjPE4MgRQiCCIhyAIaPnGB5cJMS10CcuJSRNXOAK46TNKUtQBiP2gr5d2h0XbER77ZkZNadTAnpTUjpYJE1MeSlhfZpj4rlx1uxv3n3jqe/Wo79feIXEKpwT+5fuI/O/Ol2LQgfrpgZJNSWG0dXxwiU3XdE3dz5VpcghIU7jM4qnhLlRfvTZMZrM1K57y0z8xWRqVu95kZvjVd+SBux9H+dP0FyueSu11b2VysZmMeoS5rGAKs1zDRvYQR0N8u7hHg94tI6sa+vGun1PtQYKTRlflnX3BuW/lrg=</latexit>
<latexit sha1_base64="4GIDvDiyTKla9jjqIcm/u5cqCO0=">AAAC5XicjVHLSsNAFD2Nr1pfVZdugkXQTUmlokvRhW6ECtYKVstkHGswLyYTUUK37tyJW3/Arf6K+Af6F94ZU/CB6IQkZ86958zce93Y9xLlOC8Fa2BwaHikOFoaG5+YnCpPz+wnUSq5aPLIj+SByxLhe6FoKk/54iCWggWuL1ru+aaOty6ETLwo3FNXsTgKWDf0Tj3OFFGdst0OmDrjzM+2dnrHWVuJSyWDjEdJr9eJF/eWOuWKU3XMsn+CWg4qyFcjKj+jjRNE4EgRQCCEIuyDIaHnEDU4iIk7QkacJOSZuEAPJdKmlCUogxF7Tt8u7Q5zNqS99kyMmtMpPr2SlDYWSBNRniSsT7NNPDXOmv3NOzOe+m5X9Hdzr4BYhTNi/9L1M/+r07UonGLN1OBRTbFhdHU8d0lNV/TN7U9VKXKIidP4hOKSMDfKfp9to0lM7bq3zMRfTaZm9Z7nuSne9C1pwLXv4/wJ9pertXp1ZbdeWd/IR13EHOaxSPNcxTq20UCTvK/xgEc8WV3rxrq17j5SrUKumcWXZd2/A6T0nT0=</latexit>
<latexit sha1_base64="prOyY/hvL/0ffnJ1/tS7WQX8cmo=">AAAC5XicjVHLSsNAFD2Nr1pfVZdugkVQhJJKRZeiLlxWsFawWibjWIN5MZmIErp1507c+gNu9VfEP9C/8M6Ygg9EJyQ5c+49Z+be68a+lyjHeSlYA4NDwyPF0dLY+MTkVHl6Zj+JUslFk0d+JA9clgjfC0VTecoXB7EULHB90XLPt3S8dSFk4kXhnrqKxVHAuqF36nGmiOqU7eV2wNQZZ3623evEx1lbiUslg4xHSa+3uLfUKVecqmOW/RPUclBBvhpR+RltnCACR4oAAiEUYR8MCT2HqMFBTNwRMuIkIc/EBXookTalLEEZjNhz+nZpd5izIe21Z2LUnE7x6ZWktLFAmojyJGF9mm3iqXHW7G/emfHUd7uiv5t7BcQqnBH7l66f+V+drkXhFOumBo9qig2jq+O5S2q6om9uf6pKkUNMnMYnFJeEuVH2+2wbTWJq171lJv5qMjWr9zzPTfGmb0kDrn0f50+wv1Kt1auru/XKxmY+6iLmMI9FmucaNrCDBprkfY0HPOLJ6lo31q1195FqFXLNLL4s6/4dRrydGA==</latexit> <latexit sha1_base64="rA+zmkB1cl8CAMuT1NVBQ4iXlr8=">AAAC2HicjVHLSsNAFD2N73e0SzfBIlSEkoiiS1EXLitYK7ZaJuNUg3kxmQilCO7ErT/gVr9I/AP9C++MEXwgOiHJmXPvOTP3Xj8Ng0y57nPJGhgcGh4ZHRufmJyanrFn5w6yJJdcNHgSJvLQZ5kIg1g0VKBCcZhKwSI/FE3/YlvHm5dCZkES76teKo4jdhYH3YAzRVTHLi+3I6bOOQv7O1ed9GSlur/UsStuzTXL+Qm8AlRQrHpiP6GNUyTgyBFBIIYiHIIho6cFDy5S4o7RJ04SCkxc4ArjpM0pS1AGI/aCvme0axVsTHvtmRk1p1NCeiUpHSySJqE8SVif5ph4bpw1+5t333jqu/Xo7xdeEbEK58T+pfvI/K9O16LQxYapIaCaUsPo6njhkpuu6Js7n6pS5JASp/EpxSVhbpQffXaMJjO1694yE38xmZrVe17k5njVt6QBe9/H+RMcrNS81dra3mplc6sY9SjmsYAqzXMdm9hFHQ3y7uEeD3i0jqxr68a6fU+1SoWmjC/LunsDiuuWkw==</latexit>
Source p Target q No reg. + 2(T) + 2(T)
Dp GMp
Source p Target q No reg. + cos(T) + cos(T)
Dp GMp
Figure 1: Learning of geometry-preserving maps with the (DST) and the (GMG). Provided a source
distribution p, and a target q defining a fitting constraint, we minimize L(θ):=S (T ♯p,q)+λR(T ), where
ε θ θ
S is the Sinkhorn divergence [23], an OT-based fitting loss. We compare the effect of each regularizer
ε
R = GMc pX,cY and R = D pcX,cY, and additionally train a map without regularizer as a baseline. For all
experimentswithregularizer, weuseλ=1. Onthetopline, weuse[i]c =c =∥·−·∥ , aimingtopreserve
X Y 2
the distances between the points. On the bottom line, we use [ii] c =c =cos-sim(·,·), aiming to preserve
X Y
angles. Without tuning λ, the (GMG) provides the best compromise between preserving geometric features
and fitting the marginal constraint.
distribution. This means there usually exists an inherent trade-off between the accurate reconstruction of
the data distribution and this reconstruction being, e.g., a ”fully isometric” map. If such a map does not
exist, the reconstruction loss and the distortion term cannot be 0 simultaneously. In practice, this means
that the distortion loss will move away from accurately reconstructing the data, which negatively impacts
the quality of the learned latent representations. This naturally raises the question of how to formulate a
geometric regularization that takes the most geometry-preserving mapping into account.
3.2 ...to The Gromov-Monge Gap
Recently, Uscidda and Cuturi [68] introduced the Monge gap, a regularizer that measures whether a map T
transports a reference distribution at the minimal displacement cost. In practice, this regularizer is combined
with fitting losses to compute Monge maps, which are defined by two main features: (i) they fit a marginal
constraint with (ii) minimal displacement cost. Building on this concept, we replace ”displacement” with
”distortion” to introduce the Gromov-Monge gap, a regularizer that assesses whether a map T transports a
reference distribution at the minimal distortion cost. In § 3.5, we use it, alongside fitting losses, to compute
Gromov-Mongemaps,asdefinedinEq.(GMP),whicharesimilarlydefinedby(i)fittingamarginalconstraint
with (ii) minimal distortion cost.
Definition 3.2 (Gromov-Monge gap). The Gromov-Monge gap (GMG) of a map T is defined as:
GMcX,cY(T):=DcX,cY(T)−GWcX,cY(r,T♯r) (GMG)
r r
From Eq. (GWP), we recall that GWcX,cY(r,T♯r) is the minimal distortion achievable when transporting
r to T♯r. Thus, GMcX,cY(T) quantifies the difference between the distortion incurred when transporting
r
r to T♯r via T, to this minimal distortion. More formally, GMcX,cY(T) is the optimality gap of T in the
r
Gromov-Monge Prob. (GMP) between r and T♯r, which is always feasible, even when r is discrete, as T
belongs to the constraint set. In light of this, it is a well-defined and
5• The GMG measures how close T is to be a Gromov-Monge map for costs c ,c . Indeed,
X Y
GMcX,cY(T)≥0 with equality i.f.f. T is a Gromov-Monge map solution of Prob. (GMP) between r and
r
T♯r, i.e., T moves r with minimal (but eventually non zero) distortion.
• When transport without distortion is possible, the GMG coincides with the distortion. When
there exists another map U : X → Y transporting r to T♯r with zero distortion, i.e., U♯r = T♯r and
D rcX,cY(U)=0, then GMc rX,cY(T)=D rcX,cY(T). Indeed, GWcX,cY(r,T♯r)=0 in that case, as the coupling
π =(Id,U)♯r sets the GW objective to zero, thereby minimizing it.
The last point (ii) is fundamental and illustrates how the GMG functions as a debiased distortion. Indeed,
it compares the distortion induced by T to a baseline distortion, defined as the minimal achievable distortion
when transforming the reference distribution into its image under T. Thus, when transformation without
any distortion is achievable, the reference distortion becomes zero, and the GMG precisely aligns with the
distortion itself, i.e., GMc rX,cY(T)=D rcX,cY(T). In this context, the GMG offers the optimal compromise:
it avoids the over-penalization induced by the distortion when fully preserving c ,c is not feasible, yet it
X Y
coincides with it when such preservation is feasible.
The Influence of the Reference Distribution. A crucial property of DcX,cY is that if T transforms r
r
without distortion, it will also apply distortion-free to any distribution s whose support is contained within
that of r. Formally, if DcX,cY(T) = 0 and s ∈ P(X) with supp(s) ⊆ supp(r), then DcX,cY(T) = 0. This
r s
raises a question for the GMG: If T maps r with minimal distortion, does it similarly map s with minimal
distortion? We answer this question positively with Prop. (3.3) when the costs are the (scaled) Euclidean
distances or the cosine similarity. Intuitively, this means that if T moves r while preserving (scaled) distances
or angles as much as possible, it will also preserve these properties as much as possible when moving any
”smaller” distribution within r.
Proposition 3.3. When c ,c are [i] or [ii] (see § 3.1), if GMcX,cY(T)=0, then for any s∈P(X) s.t.
X Y r
Spt(s)⊆Spt(r), one has GMcX,cY(T)=0.
s
3.3 Estimation and Computation from Samples
Plug-In Estimation. In practice, we estimate Eq. (DST) and Eq. (GMG) using i.i.d. samples x ,...,x
1 n
from the reference distribution r. We then consider the empirical version r := 1 (cid:80)n δ of r and use a
n n i=1 xi
plug-in estimator for both cases, i.e., we estimate the distortion via
n
(cid:88)
D rc nX,cY(T)= n1
2
(c X(x i,x j)−c Y(T(x i),T(x j)))2, (2)
i,j=1
and the GMG via GMc rX n,cY(T)=D rc nX,cY(T)−GW cX,cY(r n,T♯r n), where T♯r
n
= n1 (cid:80)n i=1δ T(xi). To better
understand what the discrete GMG quantifies, we can reformulate it using the minimal distortion achieved
by a permutation σ ∈S between the x and the T(x ).
n i i
Proposition 3.4. When c ,c are [i] or [ii] , the empirical GMG reads:
X Y
n
GMc rX n,cY(T)=D rc nX,cY(T)− σm ∈i Sn
n
n1
2
i(cid:88) ,j=1(cid:0) c X(x i,x j)−c Y(T(x σ(i)),T(x σ(j)))(cid:1)2 (3)
As a Monte Carlo estimator, DcX,cY(T) is naturally consistent. We can ask the same question for
rn
GMcX,cY(T), which requires studying the convergence of the empirical GW distance GW (r ,T♯r ). For
rn cX,cY n n
the costs c and c of interest, we show that consistency holds.
X Y
Proposition 3.5. When c ,c are [i] or [ii], GMcX,cY(T)→GMcX,cY(T) almost surely.
X Y rn r
6Efficient Computation. Computing GMcX,cY(T) requires solving a discrete GW problem between r
rn n
and T♯r to obtain GW (r ,T♯r ). To alleviate computational challenges, we estimate this term using
n cX,cY n n
an entropic regularization ε≥0, as introduced in Eq. (EGWP):
GMcX,cY(T):=DcX,cY(T)−GWcX,cY(r ,T♯r ). (4)
rn,ε rn rn,ε n n
Choosing ε=0, we recover the unregularized one GMcX,cY =GMcX,cY. Moreover, the entropic estimator
preserves the positivity, as for ε ≥ 0, we have
GMcX,cr Yn,0
≥ 0 (see
Arn
.1). As described in § 2, we compute
GWcX,cY(r ,T♯r ) using Peyr´e et al. [54]’s solver.
r Wn,ε
hile it always has O(n2) memory complexity, when
rn,ε n n
c =c =⟨·,·⟩orc =c =∥·−·∥2, thissolverrunsinO(n2d)time[62, Alg.2]. Sincethecosinesimilarity
X Y X Y 2
is equivalent to the inner product, up to pre-normalization of x and T(x ), the computation of the GMG for
i i
the costs of interest [i] or [ii] scales as O(n2d) in time. We use ott-jax’s [16] implementation of this scheme.
3.4 (Weak) Convexity of the Gromov-Monge gap
As laid out, the GMG can be used as a regularization loss to push any model T to be more geometry-
preserving. A natural question that arises when defining such a regularizer is: what are its regularity
properties, and in particular, is it convex? In the following, we study the convexity of T (cid:55)→ GMcX,cY(T),
r
and its finite-sample counterpart T (cid:55)→ GMcX,cY(T). We focus on the costs of interest [i] or [ii]. For
rn
simplicity, we replace cosine similarity with inner product—i.e., c =c =⟨·,·⟩—as they are equivalent, up
X Y
to normalization of r and T. We then study the convexity of the GMG for (i) the (scaled) squared Euclidean
distances and (ii) the inner product, denoted respectively by (i) GM2 and (ii) GM⟨·,·⟩. To that end, we
r r
introduce a weaker notion of convexity, previously defined for functions on Rd [17], which we extend here to
L (r)={T |∥T∥2 :=(cid:82) ∥T(x)∥2dr(x)<+∞}.
2 L2(r) X 2
Definition 3.6 (Weak convexity.). With γ > 0, a functional F : L (r) → R is γ-weakly convex if
2
F :T (cid:55)→F(T)+ γ∥T∥2 is convex.
γ 2 L2(r)
A weakly convex functional is convex up to an additive quadratic perturbation. The weak convexity
constant γ quantifies the magnitude of this perturbation and indicates a degree of non-convexity of F. A
lower γ suggests that F is closer to being convex, while a higher γ indicates greater non-convexity.
Theorem 3.7. Both GM2 and GM⟨·,·⟩, as well as their finite sample versions, are weakly convex.
r r
• Finite sample. We note X∈Rn×d the matrix that stores the x , i.e. the support of r , as rows. Then,
i n
(i) GM2 and (ii) GM⟨·,·⟩ are respectively (i) γ and (ii) γ -weakly convex, where: γ =
λ
(1Xrn
X⊤)−λ
(1Xrn
X⊤) and γ =γ
2 +,n
max
∥in xne ∥r 2,n
.
inner,n
max n min n 2,n inner,n i=1...n i 2
• Asymptotic. (i) GM2 and (ii) GM⟨·,·⟩ are respectively (i) γ and (ii) γ -weakly convex, where:
r r 2 inner
γ =λ (E [xx⊤]) and γ =γ +max ∥x∥2.
inner max x∼r 2,n inner x∈Spt(r) 2
From a practitioner’s perspective, we analyze the insights provided by Thm. (3.7) in three parts.
• First, we have γ ≥ γ . Therefore, GM2 is less convex than GM⟨·,·⟩, making it harder to optimize,
2 inner r r
and the same argument holds for their estimator. In other words, we provably recover that, in practice,
preserving the (scaled) distances is harder than simply preserving the angles.
• Second, as γ = λ (E [xx⊤]) ≥ λ (Cov [x]), this exhibits a tradeoff w.r.t. Prop. (3.3): by
inner max x∼r max x∼r
choosing a bigger reference distribution r, we trade the convexity of the GMG. For γ , the dependency in
2
r is even worse. In practice, we then choose r with support as small as possible, precisely where we want
T to move points with minimal distortion.
• Third, and probably the most surprising, the finite sample GMG is more convex in high dimension. Indeed,
γ is the spectral width of 1XX⊤, containing the (rescaled) inner-products between the x ∼ r.
inner,n n i
When n>d, λ (XX⊤)=0 as rank(XX⊤)=d. Then, γ increases, which in turn decreases the
min inner,n
GMG’s convexity. On the other hand, when d>n, λ (XX⊤)>0 if X is full rank. Intuitively, GM⟨·,·⟩
min rn
is nearly convex when XX⊤ is well conditioned. Assuming that the x are normalized, this might happen
i
7in high dimension, as those points will be orthogonal with high probability. This property suggests that, in
practice, and contrary to the insights provided by the statistical OT literature [25, 56, 71, 73], the GMG
might not benefit a large sample size.
3.5 Learning with the Gromov-Monge gap
General Learning Procedure. Provided a source distribution p and a target q defining a marginal
constraint, learning with the GMG remains to optimize a loss of the form
L(θ):=∆(T ,p,q)+λ GMcX,cY(T ) (5)
θ GMG r θ
where∆isafittingloss, whichcanaccesspaired, orunpaired, samplesofpandq. Intheory, fromProp.(3.3),
we can choose any reference r s.t. Spt(p)⊂Spt(r). In practice, given the insights of Thm. (3.6), we usually
consider r =p. Note that replacing GMc rX,cY by D rcX,cY in Eq. (5), we similarly define the learning procedure
with the distortion. We compare their effect in Figure 1.
VAE Learning Procedure. In the VAE setting, (i) when we apply the GMG (or the distortion) to the
encoder e , the fitting loss is defined through the prior matching constraint, as described in § 2.1. Conversely,
ϕ
(ii) when we apply the GMG to the decoder d , the fitting loss is defined through the reconstruction loss.
ϕ
Additionally, in both cases, our goal is to promote the latent space to preserve certain geometric features of
the data. Therefore, in (i) we use r =p the data distribution as reference r, while in (ii) we use the latent
data
distribution r =q . Introducing weightings λ ,λ ≥0, determining which mapping we regularize, this
ϕ enc dec
remains to optimize the loss
L (θ,ϕ)=L (θ,ϕ)+λ GMcX,cY(e )+λ GMcX,cY(d ), (6)
β-GMG β enc pdata ϕ dec qϕ θ
where L is introduce in § 2.1. Note that this loss can easily be extended to β-TCVAE and the combination
β
of other regularization terms. While previous work [41, 51] chooses to apply the geometric regularizations to
the decoder, we investigate regularizing both, separately or simultaneously. For completeness, we also derive
the VAE-loss when learning with the distortion:
L (θ,ϕ)=L (θ,ϕ)+λ DcX,cY(e )+λ DcX,cY(d ), (7)
β-DST β enc pdata ϕ dec qϕ θ
The choice of c ,c . Recently, Lee et al. [41] elucidated that fully isometric regularization—preserving
X Y
c =c =∥·−·∥2—can be overly restrictive. They introduced a Jacobian-based regularizer to learn scaled
X Y 2
isometry–which preserves the costs c =∥·−·∥2 and c =α2∥·−·∥2 with α2 >0. Similarly, Nakagawa
X 2 Y 2
et al. [51] proposed using distortion (DST) with these costs and a learnable scaling α2. In this work, we
follow their direction and consider both the distortion and the GMG for all the costs of interest [i] and [ii]
introduced in 3.1, defining a hierarchy of geometric regularization. For c =∥·−·∥2 and c =α2∥·−·∥2,
X 2 Y 2
we refer to this as scaled isometric regularization (SIR) for learnable α > 0 and isometric regularization
(IR) with fixed α=1. We refer to it as conformal regularization (CR) when c =c =cos-sim(·,·). We
X Y
emphasize that in each setting, using the GMG does not aim to find a map that fully preserves the (scaled)
distances (SIR and IR) or the angles (CR), but rather one that preserves them as much as possible while
matching the prior when regularizing the encoder or reconstructing the data when regularizing the decoder.
4 Experiments
Experimental setup. We evaluate the effectiveness of the (GMG) as regularizer in disentangled repre-
sentation learning. We use the standard β-VAE and β-TCVAE as our base models and incorporate the
GMG on top of them. Moreover, we consider the recently proposed HFS [60] regularization on top of both
β-VAE and β-TCVAE, totaling four base models. Our primary goal is to investigate the differences between
using the GMG and the (DST) as regularizers, specifically examining whether the (GMG) leads to more
8Table1: Effectofdifferentgeometricregularizationondisentanglement(DCI-D,Shapes3D[33]). Wehighlight
the best method per regularization, and the best/second best per column.
β-VAE β-TCVAE β-VAE + HFS β-TCVAE + HFS
Base 65.8 ±15.6 75.0 ±3.4 88.1 ±7.4 90.2 ±7.5
Isometric (IR)
+ (DST) 71.5 ±3.6 75.8 ±6.6 92.1 ±9.7 90.9 ±7.6
+ (GMG) 72.0 ±12.5 78.9 ±5.0 92.5 ±4.4 91.7 ±6.0
Scaled Isometric (SIR)
+ Jacobian 61.4 ±12.8 76.7 ±4.5 90.5 ±3.8 91.5 ±5.6
+ (DST) 67.4 ±7.1 77.9 ±4.5 93.2 ±9.7 94.5 ±6.9
+ (GMG) 70.0 ±5.9 81.0 ±3.2 93.3 ±8.6 96.1 ±3.8
Conformal (CR)
+ (DST) 76.8 ±4.1 81.3 ±4.7 87.5 ±3.3 91.9 ±9.4
+ (GMG) 82.1 ±4.5 83.7 ±8.8 95.7 ±5.8 96.9 ±4.9
disentangled representations compared to the raw distortion. Additionally, we aim to determine which
geometric regularization (IR, SIR, CR) is most beneficial for disentanglement and what part of the pipeline
should be regularized. Lastly, we investigate whether a geometric regularization can help prevent the collapse
of learned representation in the Decoder-free setting. We evaluate the learned latents with DCI-D [20]
as it was found to be the metric most suitable to measure disentanglement [18, 44]. We benchmark over
multiple datasets commonly used to assess the performances of disentangled representation learning methods:
Shapes3D [33], DSprites [27], SmallNORB [40], and Cars3D [57].
4.1 Evaluating Different Geometric Regularizations
Regularizing the Decoder. First, we focus on the difference between optimizing for different geometry-
preserving regularizations. We compare between IR, SIR, and CR [41] realized through either the (DST),
or (GMG). Additionally, we include the Jacobian-based SIR as introduced in Lee et al. [41]. We report full
results on Shapes3D [9] over 5 seeds in Table 1. We observe that the GMG outperforms the sole distortion
loss for all levels of regularization and baselines. Moreover, we find that a CR performs best with respect to
disentanglement compared to both IR and SIR. Note, that employing a CR has not been benchmarked for
disentangled representation learning before. These results elucidate the clear benefit of using the GMG in
its CR implementation in terms of learning more disentangled representations significantly improving upon
previously proposed regularization.
Thus, next we benchmark the GMG in its CR form against its distortion counterpart across three more
datasets again over four different baselines. We report full results in Table 2. Again we observe that the
GMG outperforms or performs equally well to its distortion equivalent, confirming the benefits of accounting
for the optimal possible mapping in the regularization. Note that for SmallNORB and Cars3D, we found no
benefits with respect to DCI-D in adding an HFS regularization and obtained the best results without it.
We emphasize that using the GMG as CR significantly improves results for all datasets versus not using
any isometric regularization. This establishes the GMG as an effective regularization method beneficial for
disentangled representation learning.
Regularizing the Encoder. Lastly, we also analyze a CR on e , as well as regularizing both d and e
ϕ θ ϕ
together. We report full results over two datasets in Table 4. Again, the GMG on d achieves best DCI-D
θ
over all baselines. This result is expected in the light of Theorem 3.7. Interestingly, regularizing solely d
θ
outperforms regularizing both e and d . We hypothesize this is due to the regularization of the decoder
ϕ θ
9Table 2: Effect of (GMG) and (DST) leveraged as a conformal regularization (CR) on the disentanglement of
learned representations as measured by DCI-D over four datasets. We highlight the best, and second best
result for each dataset and method.
CR β-VAE β-TCVAE β-VAE + HFS β-TCVAE + HFS
Shapes3D [33]
Base 65.8 ±15.6 75.0 ±3.4 88.1 ±7.4 90.2 ±7.5
+ (DST) 76.8 ±4.1 81.3 ±4.7 87.5 ±3.3 91.9 ±9.4
+ (GMG) 82.1 ±4.5 83.7 ±8.8 95.7 ±5.8 96.9 ±4.9
DSprites [27]
Base 26.2 ±18.5 32.3 ±19.3 33.6 ±17.9 48.7 ±10.2
+ (DST) 28.6 ±19.3 32.4 ±8.5 39.3 ±18.1 49.0 ±11.2
+ (GMG) 39.5 ±15.2 42.2 ±3.6 46.7 ±2.0 50.1 ±8.5
SmallNORB [40]
Base 26.8 ±0.2 29.8 ±0.4 26.8 ±0.2 29.8 ±0.4
+ (DST) 28.2 ±0.3 29.9 ±0.4 28.2 ±0.3 29.9 ±0.4
+ (GMG) 28.3 ±0.6 29.9 ±0.5 28.3 ±0.6 29.9 ±0.5
Cars3D [57]
Base 29.6 ±5.7 32.3 ±4.6 29.6 ±5.7 32.3 ±4.6
+ (DST) 26.8 ±3.6 33.7 ±4.2 26.8 ±3.6 33.7 ±4.2
+ (GMG) 30.1 ±5.6 36.4 ±5.7 30.1 ±5.6 36.4 ±5.7
also offering a stronger signal as its gradients impact both the decoder and the encoder, as in this case, the
reference r is the distribution of encoded images.
4.2 Towards Decoder-free Disentanglement
Recently,workssuchas[1,11,21,47,70]haveshownthepossibilityofdisentanglementthroughself-supervised,
contrastive learning objectives in an effort to align with the scalability of encoder-only representation
learning[3,14,24,72]. However, theseencoder-onlyapproaches
Table3: Disentanglement(DCI-D)without
still require weak supervision or access to multiple views of an
a decoder trained with various regulariza-
image to learn meaningful representations of the data samples.
tions on Shapes3D [9].
As the goal of geometry preservation connects the data mani-
foldandthelatentdomainthroughaminimaldistortionobjective
and is applicable to both the encoder and decoder of a VAE Decoder-free β-VAE β-TCVAE
(§3, Table 4), we posit that its application may provide sufficient Base 0.0 ±0.0 0.0 ±0.0
training signal to learn meaningful representations and encour-
Isometric (IR)
age disentanglement, eliminating the need for a reconstruction
loss and decoder. Table 3 shows preliminary results on unsu- + (DST) 38.2 ±0.8 42.7 ±1.6
pervised decoder-free disentangled representation learning on + (GMG) 13.9 ±0.4 20.5 ±0.5
the Shapes3D benchmark, where the decoder and associated
Scaled Isometric (SIR)
reconstruction objective have been removed.
+ (DST) 45.6 ±1.2 53.5 ±1.0
Standard approaches such as β-VAE or β-TCVAE collapse
+ (GMG) 15.2 ±0.3 25.2 ±0.6
and do not achieve measurable disentanglement (DCI-D of 0.0).
However,theinclusionofeitherDSTorGMGsignificantlyraises Conformal (CR)
achievable disentanglement and, combined with the β-TCVAE
+ (DST) 37.0 ±0.4 46.1 ±1.5
matching objective, can achieve DCI-D scores of up to 53.5 with-
+ (GMG) 37.0 ±0.9 38.8 ±1.1
out needing any decoder or reconstruction loss. While these are
10preliminary insights, we believe they offer promise for more scalable approaches to unsupervised disentangled
representation learning and potential bridges to popular and scalable self-supervised representation learning
approaches. Note, that here the distortion loss significantly outperforms the GMG. This is expected due to
the nature of the GMG, as the distortion loss offers a more restrictive and, thus, stronger signal for learning
representations, which is necessary in the absence of a reconstruction objective. This highlights that while
in most scenarios (§ 4.1, Figure 1), the GMG is preferable over the distortion loss, there also exist settings
where a more restrictive optimization signal is desirable.
5 Conclusion
Inthiswork,weintroduceanoptimaltransport(OT)perspectiveonunsuperviseddisentangledrepresentation
learning to incorporate general latent geometrical constraints. We derive the Gromov-Monge gap (GMG), a
provablyweaklyconvexOTregularizerthatmeasuresthepreservationofgeometricalpropertiesbyatransport
map T. By formulating disentangled representation learning as a transport problem, we integrate the GMG
intostandardtrainingobjectives,allowingforincorporatingandstudyingvariousgeometricconstraintsonthe
disentanglement of learned representation spaces. Including these geometry preserving regularization offers
significant performance benefits across four standard disentanglement benchmarks when applied to existing
disentanglementmethods. Moreover,weshowpromisingresultsondecoder-freeunsuperviseddisentanglement.
We demonstrate that optimizing for geometric constraints through the OT lens can provide sufficient training
signal and regularization on the model encoder to achieve measurable disentanglement without explicit
reconstruction objectives. This opens a possible door towards more scalable unsupervised disentanglement
and bridges to weakly- & self-supervised encoder-only representation learning efforts.
Acknowledgements
Co-funded by the European Union (ERC, DeepCell - 101054957). Views and opinions expressed are, however,
those of the author(s) only and do not necessarily reflect those of the European Union or the European
Research Council. Neither the European Union nor the granting authority can be held responsible for them.
Luca Eyring and Karsten Roth thank the European Laboratory for Learning and Intelligent Systems (ELLIS)
PhD program for support. Karsten Roth also thanks the International Max Planck Research School for
Intelligent Systems (IMPRS-IS) for support. Zeynep Akata was supported by BMBF FKZ: 01IS18039A,
by the ERC (853489 - DEXIM), by EXC number 2064/1 – project number 390727645. Fabian J. Theis
consults for Immunai Inc., Singularity Bio B.V., CytoReason Ltd, Cellarity, and has ownership interest in
Dermagnostix GmbH and Cellarity.
References
[1] Laurence Aitchison and Stoil Krasimirov Ganev. InfoNCE is variational inference in a recognition
parameterised model. Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL
https://openreview.net/forum?id=chbRsWwjax. 10
[2] Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky, David
Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Claudio Fantacci, Jonathan Godwin, Chris Jones, Ross
Hemsley, Tom Hennigan, Matteo Hessel, Shaobo Hou, Steven Kapturowski, Thomas Keck, Iurii Kemaev,
Michael King, Markus Kunesch, Lena Martens, Hamza Merzic, Vladimir Mikulik, Tamara Norman, John
Quan, George Papamakarios, Roman Ring, Francisco Ruiz, Alvaro Sanchez, Rosalia Schneider, Eren
Sezener, Stephen Spencer, Srivatsan Srinivasan, Luyu Wang, Wojciech Stokowiec, and Fabio Viola. The
DeepMind JAX Ecosystem, 2020. URL http://github.com/deepmind. 25
11[3] Adrien Bardes, Jean Ponce, and Yann LeCun. VICReg: Variance-invariance-covariance regularization
for self-supervised learning. In International Conference on Learning Representations, 2022. URL
https://openreview.net/forum?id=xm6YD62D1Ub. 2, 10
[4] Vito´ria Barin-Pacela, Kartik Ahuja, Simon Lacoste-Julien, and Pascal Vincent. On the identifiability of
quantized factors, 2024. 1
[5] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new
perspectives, 2014. 1
[6] D. Bertsimas and J.N. Tsitsiklis. Introduction to linear optimization. Athena Scientific, 1997. 19
[7] Garrett Birkhoff. Tres observaciones sobre el algebra lineal. Universidad Nacional de Tucuma´n Revista
Series A, 5:147–151, 1946. 19
[8] Stephen Boyd and Lieven Vandenberghe. Convex Optimization. Cambridge University Press, March
2004. ISBN 0521833787. URL http://www.amazon.com/exec/obidos/redirect?tag=citeulike-20&
path=ASIN/0521833787. 21
[9] Chris Burgess and Hyunjik Kim. 3d shapes dataset. https://github.com/deepmind/3dshapes-dataset/,
2018. 9, 10
[10] Christopher P. Burgess, Irina Higgins, Arka Pal, Lo¨ıc Matthey, Nick Watters, Guillaume Desjardins,
and Alexander Lerchner. Understanding disentangling in beta-vae. CoRR, abs/1804.03599, 2018. URL
http://arxiv.org/abs/1804.03599. 1, 3
[11] Andrea Burns, Aaron Sarna, Dilip Krishnan, and Aaron Maschinot. Unsupervised disentanglement
without autoencoding: Pitfalls and future directions. arXiv preprint arXiv:2108.06613, 2021. 10
[12] Nutan Chen, Alexej Klushyn, Francesco Ferroni, Justin Bayer, and Patrick van der Smagt. Learning flat
latent manifolds with vaes, 2020. 3
[13] Ricky T. Q. Chen, Xuechen Li, Roger B Grosse, and David K Duvenaud. Isolating sources of dis-
entanglement in variational autoencoders. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman,
N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, vol-
ume 31. Curran Associates, Inc., 2018. URL https://proceedings.neurips.cc/paper/2018/file/
1ee3dfcd8a0645a25a35977997223d22-Paper.pdf. 1, 3
[14] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In Hal Daum´e III and Aarti Singh, editors, Proceedings of
the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning
Research, pages 1597–1607. PMLR, 13–18 Jul 2020. URL https://proceedings.mlr.press/v119/
chen20j.html. 2, 10
[15] Marco Cuturi. Sinkhorn Distances: Lightspeed Computation of Optimal Transport. In Advances in
Neural Information Processing Systems (NeurIPS), volume 26, 2013. 3
[16] Marco Cuturi, Laetitia Meng-Papaxanthos, Yingtao Tian, Charlotte Bunne, Geoff Davis, and Olivier
Teboul. Optimal Transport Tools (OTT): A JAX Toolbox for all things Wasserstein. arXiv Preprint
arXiv:2201.12324, 2022. 7
[17] Damek Davis, Dmitriy Drusvyatskiy, Kellie J. MacPhee, and Courtney Paquette. Subgradient methods
for sharp weakly convex functions, 2018. 7
[18] AndreaDittadi,FrederikTra¨uble,FrancescoLocatello,ManuelWuthrich,VaibhavAgrawal,OleWinther,
Stefan Bauer, and Bernhard Sch¨olkopf. On the transfer of disentangled representations in realistic
settings. In International Conference on Learning Representations, 2021. URL https://openreview.
net/forum?id=8VXvj1QNRl1. 9
12[19] Th´eo Dumont, Th´eo Lacombe, and Fran¸cois-Xavier Vialard. On the existence of monge maps for the
gromov-wasserstein problem. 2022. 3
[20] Cian Eastwood and Christopher K. I. Williams. A framework for the quantitative evaluation of
disentangled representations. In International Conference on Learning Representations, 2018. URL
https://openreview.net/forum?id=By-7dz-AZ. 9
[21] Cian Eastwood, Julius von Ku¨gelgen, Linus Ericsson, Diane Bouchacourt, Pascal Vincent, Mark Ibrahim,
and Bernhard Scho¨lkopf. Self-supervised disentanglement by leveraging structure in data augmentations.
In Causal Representation Learning Workshop at NeurIPS 2023, 2023. URL https://openreview.net/
forum?id=JoISqbH8vl. 10
[22] Luca Vincent Eyring, Dominik Klein, Giovanni Palla, Soeren Becker, Philipp Weiler, Niki Kilbertus, and
Fabian J. Theis. Modeling single-cell dynamics using unbalanced parameterized monge maps. bioRxiv,
2022. doi: 10.1101/2022.10.04.510766. URL https://www.biorxiv.org/content/early/2022/10/05/
2022.10.04.510766. 4
[23] Jean Feydy, Thibault S´ejourn´e, Fran¸cois-Xavier Vialard, Shun-Ichi Amari, Alain Trouv´e, and Gabriel
Peyr´e. InterpolatingbetweenOptimalTransportandMMDusingSinkhornDivergences. InInternational
Conference on Artificial Intelligence and Statistics (AISTATS), volume 22, 2019. 5
[24] QuentinGarrido,YubeiChen,AdrienBardes,LaurentNajman,andYannLeCun. Onthedualitybetween
contrastive and non-contrastive self-supervised learning. In The Eleventh International Conference on
Learning Representations, 2023. URL https://openreview.net/forum?id=kDEL91Dufpa. 2, 10
[25] Aude Genevay, L´enaic Chizat, Francis Bach, Marco Cuturi, and Gabriel Peyr´e. Sample Complexity of
Sinkhorn Divergences. In International Conference on Artificial Intelligence and Statistics (AISTATS),
volume 22, 2019. 8
[26] Amos Gropp, Matan Atzmon, and Yaron Lipman. Isometric autoencoders, 2020. 3
[27] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir
Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained
variational framework. In International Conference on Learning Representations, 2017. URL https:
//openreview.net/forum?id=Sy2fzU9gl. 1, 3, 9, 10, 25
[28] Irina Higgins, David Amos, David Pfau, Sebastien Racaniere, Loic Matthey, Danilo Rezende, and
Alexander Lerchner. Towards a definition of disentangled representations, 2018. 1, 2
[29] Daniella Horan, Eitan Richardson, and Yair Weiss. When is unsupervised disentanglement possible?
In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman Vaughan, editors, Advances in Neural
Information Processing Systems, 2021. URL https://openreview.net/forum?id=XqEF9riB93S. 1, 2,
3, 4
[30] Kyle Hsu, Will Dorrell, James C. R. Whittington, Jiajun Wu, and Chelsea Finn. Disentanglement via
latentquantization. InThirty-seventh Conference on Neural Information Processing Systems, 2023. URL
https://openreview.net/forum?id=LLETO26Ga2. 1
[31] In Huh, changwook jeong, Jae Myung Choe, Young-Gu Kim, and Dae Sin Kim. Isometric quotient varia-
tional auto-encoders for structure-preserving representation learning. In Thirty-seventh Conference on
Neural Information Processing Systems, 2023. URL https://openreview.net/forum?id=EdgPb3ngR4.
1, 3, 4
[32] L Kantorovich. On the transfer of masses (in russian). In Doklady Akademii Nauk, volume 37, page 227,
1942. 17
13[33] Hyunjik Kim and Andriy Mnih. Disentangling by factorising. In Jennifer Dy and Andreas Krause,
editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pages 2649–2658. PMLR, 10–15 Jul 2018. URL https://proceedings.
mlr.press/v80/kim18b.html. 1, 9, 10, 25
[34] Diederik P Kingma and Jimmy Ba. Adam: A Method for Stochastic Optimization. In International
Conference on Learning Representations (ICLR), 2014. 25
[35] DiederikP.KingmaandMaxWelling. Auto-EncodingVariationalBayes. In2ndInternationalConference
on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track
Proceedings, 2014. 1, 2
[36] Dominik Klein, Th´eo Uscidda, Fabian Theis, and Marco Cuturi. Entropic (gromov) wasserstein flow
matching with genot, 2024. 4
[37] Alexander Korotin, Daniil Selikhanovych, and Evgeny Burnaev. Neural optimal transport. 2022. doi:
10.48550/ARXIV.2201.12220. URL https://arxiv.org/abs/2201.12220. 4
[38] Abhishek Kumar, Prasanna Sattigeri, and Avinash Balakrishnan. VARIATIONAL INFERENCE OF
DISENTANGLED LATENT CONCEPTS FROM UNLABELED OBSERVATIONS. In International
Conference on Learning Representations, 2018. URL https://openreview.net/forum?id=H1kG7GZAW.
1, 3
[39] Jean-Franc¸ois Le Gall. Int´egration, Probabilit´es et Processus Al´eatoires. 20
[40] Yann LeCun, Fu Jie Huang, and L´eon Bottou. Learning methods for generic object recognition with
invariance to pose and lighting. Proceedings of the IEEE Computer Society Conference on Computer
Vision and Pattern Recognition, 2:II97–II104, 2004. ISSN 1063-6919. Proceedings of the 2004 IEEE
Computer Society Conference on Computer Vision and Pattern Recognition, CVPR 2004 ; Conference
date: 27-06-2004 Through 02-07-2004. 9, 10
[41] Yonghyeon Lee, Sangwoong Yoon, MinJun Son, and Frank C. Park. Regularized autoencoders for
isometric representation learning. In International Conference on Learning Representations, 2022. URL
https://openreview.net/forum?id=mQxt8l7JL04. 2, 3, 8, 9
[42] F. Locatello, G. Abbati, T. Rainforth, S. Bauer, B. Sch¨olkopf, and O. Bachem. On the fairness of
disentangled representations. In Advances in Neural Information Processing Systems 32 (NeurIPS 2019),
pages 14584–14597. Curran Associates, Inc., December 2019. URL https://papers.nips.cc/paper/
9603-on-the-fairness-of-disentangled-representations. 1
[43] Francesco Locatello, Stefan Bauer, Mario Lucic, Gunnar Raetsch, Sylvain Gelly, Bernhard Sch¨olkopf,
and Olivier Bachem. Challenging common assumptions in the unsupervised learning of disentangled
representations. In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th
International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research,
pages4114–4124.PMLR,09–15Jun2019. URLhttps://proceedings.mlr.press/v97/locatello19a.
html. 1, 2, 25
[44] Francesco Locatello, Ben Poole, Gunnar Raetsch, Bernhard Sch¨olkopf, Olivier Bachem, and Michael
Tschannen. Weakly-supervised disentanglement without compromises. In Hal Daum´e III and Aarti
Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of
Proceedings of Machine Learning Research, pages 6348–6359. PMLR, 13–18 Jul 2020. URL https:
//proceedings.mlr.press/v119/locatello20a.html. 1, 2, 9, 25
[45] Ashok Makkuva, Amirhossein Taghvaei, Sewoong Oh, and Jason Lee. Optimal transport mapping via
input convex neural networks. In International Conference on Machine Learning (ICML), volume 37,
2020. 4
14[46] Tudor Manole and Jonathan Niles-Weed. Sharp convergence rates for empirical optimal transport
with smooth costs. The Annals of Applied Probability, 34(1B), February 2024. ISSN 1050-5164. doi:
10.1214/23-aap1986. URL http://dx.doi.org/10.1214/23-AAP1986. 20
[47] Stefan Matthes, Zhiwei Han, and Hao Shen. Towards a unified framework of contrastive learning for
disentangled representations. In Thirty-seventh Conference on Neural Information Processing Systems,
2023. URL https://openreview.net/forum?id=QrB38MAAEP. 10
[48] FacundoM´emoli.Gromov–wassersteindistancesandthemetricapproachtoobjectmatching.Foundations
of computational mathematics, 11:417–487, 2011. 2, 3, 20
[49] Gaspard Monge. M´emoire sur la th´eorie des d´eblais et des remblais. Histoire de l’Acad´emie Royale des
Sciences, pages 666–704, 1781. 5, 17
[50] Facundo M´emoli and Tom Needham. Comparison results for gromov-wasserstein and gromov-monge
distances, 2022. 3
[51] Nao Nakagawa, Ren Togo, Takahiro Ogawa, and Miki Haseyama. Gromov-wasserstein autoencoders,
2023. 1, 2, 3, 4, 8
[52] Maksim Nekrashevich, Alexander Korotin, and Evgeny Burnaev. Neural gromov-wasserstein optimal
transport. arXiv preprint arXiv:2303.05978, 2023. 4
[53] K. B. Petersen and M. S. Pedersen. The matrix cookbook, October 2008. URL http://www2.imm.dtu.
dk/pubdb/p.php?3274. Version 20081110. 22, 24
[54] Gabriel Peyr´e, Marco Cuturi, and Justin Solomon. Gromov-wasserstein averaging of kernel and distance
matrices. In International Conference on Machine Learning, pages 2664–2672, 2016. 3, 7
[55] Gabriel Peyr´e and Marco Cuturi. Computational Optimal Transport. Foundations and Trends in
Machine Learning, 11(5-6), 2019. ISSN 1935-8245. 1, 3
[56] Aram-Alexandre Pooladian and Jonathan Niles-Weed. Entropic estimation of optimal transport maps.
arXiv preprint arXiv:2109.12004, 2021. 8
[57] Scott E Reed, Yi Zhang, Yuting Zhang, and Honglak Lee. Deep visual analogy-making. In C. Cortes,
N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing
Systems, volume 28. Curran Associates, Inc., 2015. URL https://proceedings.neurips.cc/paper/
2015/file/e07413354875be01a996dc560274708e-Paper.pdf. 9, 10
[58] Gabriel Rioux, Ziv Goldfeld, and Kengo Kato. Entropic gromov-wasserstein distances: Stability,
algorithms, and distributional limits, 2023. 3, 20
[59] Michal Rolinek, Dominik Zietlow, and Georg Martius. Variational autoencoders pursue pca directions
(byaccident). InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
(CVPR), June 2019. 1, 3
[60] Karsten Roth, Mark Ibrahim, Zeynep Akata, Pascal Vincent, and Diane Bouchacourt. Disentanglement
of correlated factors via hausdorff factorized support. In The Eleventh International Conference on
Learning Representations, 2023. URL https://openreview.net/forum?id=OKcJhpQiGiX. 1, 2, 3, 8, 25
[61] Filippo Santambrogio. Optimal Transport for Applied Mathematicians. Birkh¨auser, NY, 55(58-63):94,
2015. 1, 18
[62] Meyer Scetbon, Gabriel Peyr´e, and Marco Cuturi. Linear-time gromov wasserstein distances using low
rank couplings and costs. In International Conference on Machine Learning, pages 19347–19365. PMLR,
2022. 3, 7
15[63] Othmane Sebbouh, Marco Cuturi, and Gabriel Peyr´e. Structured transforms across spaces with cost-
regularized optimal transport, 2023. 2
[64] Karl-Theodor Sturm. The space of spaces: curvature bounds and gradient flows on the space of metric
measure spaces, 2020. 2, 3
[65] Thibault S´ejourn´e, Fran¸cois-Xavier Vialard, and Gabriel Peyr´e. The unbalanced gromov wasserstein
distance: Conic formulation and relaxation, 2023. 18, 19
[66] Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras,
Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch
optimal transport, 2023. 4
[67] Frederik Tr¨auble, Elliot Creager, Niki Kilbertus, Francesco Locatello, Andrea Dittadi, Anirudh Goyal,
Bernhard Sch¨olkopf, and Stefan Bauer. On disentangled representations learned from correlated data.
In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine
Learning, volume 139 of Proceedings of Machine Learning Research, pages 10401–10412. PMLR, 18–24
Jul 2021. URL https://proceedings.mlr.press/v139/trauble21a.html. 1, 2
[68] Th´eoUsciddaandMarcoCuturi. Themongegap: Aregularizertolearnalltransportmaps, 2023. 2, 4, 5
[69] Titouan Vayer. A contribution to optimal transport on incomparable spaces, 2020. 2
[70] JuliusvonKu¨gelgen,YashSharma,LuigiGresele,WielandBrendel,BernhardScho¨lkopf,MichelBesserve,
and Francesco Locatello. Self-supervised learning with data augmentations provably isolates content
from style. In Advances in Neural Information Processing Systems, 2021. 10
[71] Jonathan Weed and Francis Bach. Sharp asymptotic and finite-sample rates of convergence of empirical
measures in wasserstein distance, 2017. URL https://arxiv.org/abs/1707.00087. 8
[72] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stephane Deny. Barlow twins: Self-supervised
learning via redundancy reduction. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th
International Conference on Machine Learning, volume139ofProceedings of Machine Learning Research,
pages12310–12320.PMLR,18–24Jul2021. URLhttps://proceedings.mlr.press/v139/zbontar21a.
html. 2, 10
[73] Zhengxin Zhang, Ziv Goldfeld, Youssef Mroueh, and Bharath K. Sriperumbudur. Gromov-wasserstein
distances: Entropic regularization, duality, and sample complexity, 2023. 3, 8
[74] DominikZietlow, MichalRolinek, andGeorgMartius. Demystifyinginductivebiasesfor(beta-)vaebased
architectures. InMarinaMeilaandTongZhang,editors,Proceedings of the 38th International Conference
on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 12945–12954.
PMLR, 18–24 Jul 2021. URL https://proceedings.mlr.press/v139/zietlow21a.html. 1
16Appendix
A Proofs
A.1 Positivity of the Entropic GMG
Recall that
GMcX,cY(T):= 1DcX,cY(T)−GWcX,cY(r ,T♯r )
rn,ε n rn ε n n
n
(cid:88)
=D rc nX,cY(T)− Pm ∈i Un ni,j,i′,j′=1(c X(x i,x j)−c Y(y i,y j))2P ijP
i′j′
−εH(P),
For any coupling P∈U , since
−εH(P)=−ε(cid:80)n
P log(P )<0, one has:
n i,j=1 ij ij
n n
(cid:88) (cid:88)
(c (x ,x )−c (y ,y ))2P P −εH(P)< (c (x ,x )−c (y ,y ))2P P
X i j Y i j ij i′j′ X i j Y i j ij i′j′
i,j,i′,j′=1 i,j,i′,j′=1
As a result, applying minimization on both sides yields that GWcX,cY(r ,T♯r )<GWcX,cY(r ,T♯r ), and
ε n n 0 n n
therefore:
GWcX,cY(T)>GWcX,cY(T)=GWcX,cY(T)≥0.
ε 0
A.2 Reminders on Monge and Kantorovich OT
In this section, we recall the Monge and Kantorovich formulations of OT, which we will use to prove various
results. These are the classical formulations of OT. Although we introduce them here after discussing the
Gromov-Monge and Gromov-Wasserstein formulations, it should be noted that they are generally introduced
beforehand. Indeed, the Gromov-Monge and Gromov-Wasserstein formulations were historically developed to
derive OT formulations for comparing measures supported on incomparable spaces.
Monge Formulation. Instead of intra-domain cost functions, we consider here an inter-domain continuous
cost function c:X ×Y →R. This assumes that we have a meaningful way to compare elements x,y from
the source and target domains. The Monge [49] problem (MP) between p ∈ P(X) and p ∈ P(Y) consists
of finding a map T :X →Y that push-forwards p onto p, while minimizing the average displacement cost
quantified by c
(cid:90)
inf c(x,T(x))dp(x). (MP)
T:T♯p=p X
We call any solution T⋆ to this problem a Monge map between p and q for cost c. Similarly to the Gromov-
Monge Problem (GMP), solving the Monge Problem (MP) is difficult, as the constraint set is not convex and
might be empty, especially when p,q are discrete.
Kantorovich Formulation. Instead of transport maps, the Kantorovich problem (KP) seeks a couplings
π ∈Π(p,q):
(cid:90)
W (p,q):= min c(x,y)dπ(x,y). (KP)
c
π∈Π(p,q) X×Y
An optimal coupling π⋆ solution of (KP), always exists. Studying the equivalence between (MP) and (KP) is
easier than in the Gromov-Monge and Gromov-Wasserstein cases. Indeed, when (MP) is feasible, the Monge
and Kantorovich formulations coincide and π⋆ =(Id,T⋆)♯p.
17A.3 Conditionally Positive Kernels
In this section, we recall the definition of a conditionally positive kernel, which is involved in multiple proofs
relying on the linearization of the Gromov-Wasserstein problem as a Kantorovich problem.
Definition A.1. A kernel k : Rd × Rd → R is conditionally positive if it is symmetric and for any
x ,...,x ∈Rd and a∈Rn s.t. a⊤1 =0, one has
1 n n
n
(cid:88)
a a k(x ,x )≥0
i j i j
i,j=1
Conditionally positive kernels include all positive kernels, such as the inner-product k(x,y)=⟨x,y⟩, the
cosine similarity k(x,y) = cos-sim(x,y) = ⟨ x , y ⟩, but also the negative squared Euclidean distance
∥x∥2 ∥y∥2
k(x,y)=−∥x−y∥2. Therefore, each of the costs of interest is either a conditionally positive kernel - for the
2
inner product and the cosine distance - or its opposite is - the squared Euclidean distance.
B Proofs of § 3.2
Proposition 3.3. When c ,c are [i] or [ii] (see § 3.1), if GMcX,cY(T)=0, then for any s∈P(X) s.t.
X Y r
Spt(s)⊆Spt(r), one has GMcX,cY(T)=0.
s
Proof. Let T,r,s as described and suppose that GMc(T)=0. Then, πr :=(Id,T)♯r is an optimal Gromov-
r
Wasserstein coupling, solution of Problem (GWP) between r and T♯r for costs c and c . Therefore, from
X Y
[65, Theorem. 3], πr is an optimal Kantorvich coupling, solution of Problem (KP) between r and T♯r for the
linearized cost:
(cid:90)
c˜:(x,y)∈X ×Y (cid:55)→ 1|c (x,x′)−c (y,y′)|2dπr(x′,y′) (8)
2 X Y
X×Y
Additionally,X×Y isacompactsetasaproductofcompactsets,sosince(x,y)(cid:55)→|c (x,x′)−c (y,y′)|2
X Y
is continuous as c and c are continuous, it is bounded on X ×Y. Afterward, since πr has finite mass, by
X Y
Lebesgue’s dominated convergence Theorem, it follows that c˜is continuous, and hence uniformly continuous,
again since X ×Y is compact.
Afterwards, by virtue of [61, Theorem 1.38], Spt(πr) is a c˜-cyclically monotone (CM) set (see [61,
Definition. 1.36]). From the definition of cyclical monotonicity, this property translates to subsets. Then,
by defining πs =(Id,T)♯s, as Spt(p)⊂Spt(r), one has Spt(πs)=Spt((Id,T)♯s)⊂Spt((Id,T)♯r)=Spt(πr),
so Spt(πs) is c˜-CM. Finally, since X and Y are compact, and c˜ is uniformly continuous, the c˜-cyclical
monotonicity of its support implies that the coupling πp is a Kantorovich optimal coupling between its
marginals for cost c˜, thanks to [61, Theorem 1.49]. By re-applying [65, Theorem. 3], we get that πs solves
the Gromov-Wasserstein problem between its marginals for costs c and c . In other words, πs =(Id,T)♯s
X Y
is Gromov-Wasserstein optimal coupling between s and T♯s so T is a Gromov-Monge map between s and
T♯s and GMcX,cY(T)=0.
s
C Proofs of § 3.3
Proposition 3.4. When c ,c are [i] or [ii] , the empirical GMG reads:
X Y
n
GMc rX n,cY(T)=D rc nX,cY(T)− σm ∈i Sn
n
n1
2
i(cid:88) ,j=1(cid:0) c X(x i,x j)−c Y(T(x σ(i)),T(x σ(j)))(cid:1)2 (3)
Proof. We start by showing a more general results, stating that when c ,c are conditionally positive kernels
X Y
(see A.1), the discrete GW couplings between uniform, empirical distributions supported on the same number
of points, ae permutation matrices.
18Proposition C.1 (Equivalence between Gromov-Monge and Gromov-Wasserstein problems in the discrete
case.). Let p = 1 (cid:80)n δ and q = 1 (cid:80)n δ two uniform, empirical measures, supported on the same
n n i=1 xi n n i=1 yi
number of points. We denote by P ={P∈Rn×n,∃σ ∈S ,P :=δ } the set set of permutation matrices.
n n ij j,σ(i)
Assume that c and c (or −c and −c ) are conditionally positive kernels (see A.1). Then, the GM and
X Y X Y
GW formulations coincide, in the sense that we can restrict the GW problem to permutations, namely
n
(cid:88)
GW (p ,p )= min (c (x ,x )−c (y ,y ))2P P
cX,cY n n
P∈Uni,j,i′,j′=1
X i i′ Y j j′ ij i′j′
n
(cid:88)
= n1 2 min (c X(x i,x i′)−c Y(y j,y j′))2P ijP i′j′ (9)
P∈Pni,j,i′,j′=1
n
(cid:88)
= 1 min (c (x ,x )−c (y ,y ))2
n2
σ∈Sni,j=1
X i j Y σ(i) σ(j)
Proof. Let P⋆ ∈U solution of the Gromov-Wasserstein between p and p , i.e.
n n n
n
(cid:88)
P⋆ ∈argmin (c (x ,x )−c (y ,y ))2P P
X i i′ Y j j′ ij i′j′
P∈Un
i,j,i′,j′=1
that always exists by continuity of the GW objective function on the compact U . We show that P⋆ can be
n
chosen as a (rescaled) permutation matrix without loss of generality.
As we assume that c and c (or −c and −c ) are conditionally positive kernels, from [65, Theorem. 3],
X Y X Y
P⋆ also solves:
n
(cid:88)
P⋆ ∈argmin (c (x ,x )−c (y ,y ))2P⋆Q (10)
X i i′ Y j j′ ij i′j′
Q∈Un
i,j,i′,j′=1
We then define the linearized cost matrix C˜ ∈Rn×n, s.t.
n
C˜ = (cid:88) (c (x ,x )−c (y ,y ))2P⋆
ij X i i′ Y j j′ ij
i′,j′=1
which allows us to reformulate Eq. (10) as
P⋆ ∈argmin⟨C˜,Q⟩ (11)
Q∈Un
Birkhoff’s theorem states that the extremal points of U are equal to the permutation matrices P .
n n
Moreover, a seminal theorem of linear programming [6, Theorem 2.7] states that the minimum of a linear
objective on a bounded polytope, if finite, is reached at an extremal point of the polyhedron. Therefore,
as P⋆ solves Eq. (11), it is an extremal point of U , so it can always be chosen as a permutation matrix.
n
Therefore, the equivalence between GW and GM follows.
To conclude the proof of Prop. 3.4, we simply remark that:
• r = 1 (cid:80)n δ andT♯r = 1 (cid:80)n δ areuniform, empiricaldistribution, andsupportedonthesame
n n i=1 xi n n i=1 T(xi)
number of points;
• The costs of interests [i] or [ii] are either conditionally positive, or their opposite is, as detailed be-
low Def (A.1).
Proposition C.2. When c ,c are [i] or [ii], GMcX,cY(T)→GMcX,cY(T) almost surely.
X Y rn r
19Proof. We first note that the empirical estimator of the distortion is consistent, as both costs [i] or [ii] are
continuous, and X is compact. We then need to study, in both cases, the convergence of GWcX,cY(r ,T♯r )
n n
to GWcX,cY(r ,T♯r).
n
Tothatend,wefirstremarkthatas,almostsurely,r →r indistribution,onealsohasthat,almostsurely,
n
T♯r →T♯r in distribution. Indeed, since Y is compact, T is bounded so for any bounded and continuous
n
f :Y →R and X ∼r, f ◦T(X) is well defined and bounded so integrable. Afterwards, one can simply adapt
the proof of the almost sure weak convergence of empirical measure based on the strong law of large numbers
to show that, almost surely, T♯r →T♯r in distribution. See for instance [39, Theorem 10.4.1].
n
[i] We start by the (scaled) squared euclidean distances. Up to replacing r by α2♯r and T by T ◦( 1 ),
α2
and similarly for r , we can assume without loss of generality that α=1. As, almost surely, both r →r
n n
and T♯r →T♯r in distribution, the results follows from [48, Thm 5.1, (e)].
n
[ii] We continue with the cosine similarity. To that end, we first consider the inner product, i.e.,
c =c =⟨·,·⟩, and show that if p →p and q →q in distribution, then GW⟨·,·⟩(p ,q )→GW⟨·,·⟩(p,q).
X Y n n n n
As noticed by Rioux et al. [58, Lemma 2]–in the first version of the paper– the GW for inner product costs
can be reformulated as:
(cid:90) (cid:90)
GW⟨·,·⟩(p,q)= ⟨x,x′⟩dp(x)dp(x′)+ ⟨y,y′⟩dq(y)dq(y′)
X×X Y×Y (12)
(cid:90)
+ min min −4⟨Mx,y⟩dπ(x,y)+4∥M∥2,
2
M∈Mπ∈Π(p,q) X×Y
wherewedefineM=[−M/2,M/2]dX×dY withM =(cid:113)(cid:82) ∥x∥2dp(x)(cid:82) ∥y∥2dq(y). Inparticular, theyshow
X 2 Y 2
this result for the entropic GW problem with ε>0, but their proof is also valid for ε=0. The above terms
only involving the marginal, i.e., not involved in the minimization, are naturally stable under convergence
in distribution, as X and Y are compact, so as X ×X and Y ×Y. As a result, we only need to study the
stability of this quantity under the convergence in distribution of the following functional:
(cid:90)
F(p,q)= min min −4⟨Mx,y⟩dπ(x,y)+4∥M∥2, (13)
2
M∈Mπ∈Π(p,q) X×Y
We first remark that:
|F(p,q)−F(p ,q )|
n n
(cid:90) (cid:90)
≤ sup | min −4⟨Mx,y⟩dπ(x,y)− min −4⟨Mx,y⟩dπ(x,y)|
M∈M π∈Π(p,q) X×Y π∈Π(p,q) X×Y
(cid:90) (cid:90)
≤ sup | min 2∥Mx−y∥2dπ(x,y)− min 2∥Mx−y∥2dπ(x,y)2|
M∈M π∈Π(p,q) X×Y 2 π∈Π(pn,qn) X×Y 2 (14)
(cid:90) (cid:90)
+2· sup | ∥Mx∥2dp(x)− ∥Mx∥2dp (x)|
2 2 n
M∈M X X
(cid:90) (cid:90)
+2· | ∥y∥2dq(y)− ∥y∥2dq (y)|
2 2 n
Y Y
Then, we show the convergence of each term separately.
• For the first term, we remark that (up to a constant factor) it can be reformulated:
sup |W2(M♯p,q)−W2(M♯p ,q )|
2 2 n n
M∈M
where we remind that that W2 is the (squared) Wasserstein distance, solution of Eq. (KP) induced by
2
c(x,y)=∥x−y∥2. By virtue of [46, Theorem 2], there exists a constant C >0, s.t. we can uniformly
2
bound
sup |W2(M♯p,q)−W2(M♯p ,q )|≤Cn−1/d
2 2 n n
M∈M
and the convergence follows.
20• For the second one, this follows from from the convergence in distribution of p to p along with the
n
Ascoli-Arzela theorem, since both M and X are compact sets, so the {f |f : x (cid:55)→ ∥Mx∥2} are
M M 2
uniformly bounded and equi-continuous.
• For the third one, this follows from the convergence in distribution of q to q.
n
As a result, we finally get GW⟨·,·⟩(p ,q ) → GW⟨·,·⟩(p,q). Finally, we remark that for any p,q,
n n
GWcos-sim(p,q) = GW⟨·,·⟩(proj ♯p,proj ♯q), where proj (x) = x/∥x∥ . Using similar arguments
Sd−1 Sd−1 Sd−1 2
invoked previously, as p → p in distribution p, proj ♯p → proj ♯p in distribution, and similarly
n Sd−1 n Sd−1
proj ♯q →proj ♯q in distribution. As a result:
Sd−1 n Sd−1
GWcos-sim(p ,q )=GW⟨·,·⟩(proj ♯p ,proj ♯q )
n n Sd−1 n Sd−1 n
→GW⟨·,·⟩(proj ♯p,proj ♯q) (15)
Sd−1 Sd−1
=GWcos-sim(p,q)
which yields the desired convergence by using p =r and q =T♯r .
n n n n
D Proofs of § 3.4
Theorem 3.7. Both GM2 and GM⟨·,·⟩, as well as their finite sample versions, are weakly convex.
r r
• Finite sample. We note X∈Rn×d the matrix that stores the x , i.e. the support of r , as rows. Then,
i n
(i) GM2 and (ii) GM⟨·,·⟩ are respectively (i) γ and (ii) γ -weakly convex, where: γ =
λ
(1Xrn
X⊤)−λ
(1Xrn
X⊤) and γ =γ
2 +,n
max
∥in xne ∥r 2,n
.
inner,n
max n min n 2,n inner,n i=1...n i 2
• Asymptotic. (i) GM2 and (ii) GM⟨·,·⟩ are respectively (i) γ and (ii) γ -weakly convex, where:
r r 2 inner
γ =λ (E [xx⊤]) and γ =γ +max ∥x∥2.
inner max x∼r 2,n inner x∈Spt(r) 2
WestartbyrecallingthestandarddefinitionofweaklyconvexfunctiononRd,alongwithtechnicallemmas
that we will in the proof of Thm. (3.7).
Definition D.1. A function f :Rd →R is γ-weakly convex if f +γ∥·∥2 is convex.
2
Lemma D.2. Let A ∈ S (R) a symmetric matrix and define the quadratic form f : x ∈ Rd (cid:55)→ x⊤Ax.
d A
Then, f is max(0,−λ (A))-weakly convex.
A min
Proof. We use the fact that a twice continuously differentiable function is convex i.f.f. its hessian is positive
semi-definite [8, §(3.1.4)]. Therefore, f is convex i.f.f. ∇2f =A≥0. If λ (A)≥0, then A≥0 so f is
A A min A
convex, i.e. 0-weakly convex. Otherwise, f − 1λ (A)∥·∥2 has hessian A−λ (A)≥0, so it is convex,
A 2 min 2 min
which yields that f is −λ (A)-weakly convex.
A min
Lemma D.3. Let (f ) a family of γ-weakly convex functions, with potentially infinite I. Then, f :x∈
i i∈I
Rd (cid:55)→sup f (x) is γ-weakly convex.
i∈I i
Proof. Asthef areγ-weaklyconvex,f +1γisconvex,sox(cid:55)→sup f (x)+1γ∥x∥2 =(sup f (x))+1γ∥x∥2
i i 2 i∈I i 2 2 i∈I i 2 2
is convex [8, Eq. (3.7)]. Therefore, the γ-weak convexity of f follows
Proof of Thm. (3.7). Finite sample. We first study the weak convexity of GM⟨·,·⟩, i.e. the Gromov-Monge
rn
gap for the inner product. For a map T ∈L (r), it reads
2
n
(cid:88)
GM⟨·,·⟩(T)= 1 1|⟨x ,x ⟩−⟨T(x ),T(x )⟩|2
rn n2 2 i j i j
i,j=1
n
(cid:88)
− min 1|⟨x ,x ⟩−⟨T(x ),T(x )⟩|2P P
2 i i′ j j′ ij i′j′
P∈Uni,j,i′,j′=1
21As r and T♯r are uniform empirical supported on the same number of points, using Prop. C.1, we can
n n
reformulate the RHS with permutation matrices, which yields
n
(cid:88)
GM⟨·,·⟩(T)= 1 1|⟨x ,x ⟩−⟨T(x ),T(x )⟩|2
rn n2 2 i j i j
i,j=1
n
(cid:88)
− 1 min 1|⟨x ,x ⟩−⟨T(x ),T(x )⟩|2P P
n2 2 i i′ j j′ ij i′j′
P∈Pni,j,i′,j′=1
From this expression, GM⟨·,·⟩ can be reformulated as a matrix input function. Indeed, it only depends
rn
on the map T via its values on the support of r , namely x ,...,x . Therefore, we write t :=T(x ), and
n 1 n i i
defineX,T∈Rn×d whichcontainobservationsx andt respectively, storedasrows. Then, studyingGM⟨·,·⟩
i i rn
remains to study
n n
(cid:88) (cid:88)
f(T):= 1 1|⟨x ,x ⟩−⟨t ,t ⟩|2− 1 min 1|⟨x ,x ⟩−⟨t ,t ⟩|2P P
n2 2 i j i j n2 2 i i′ j j′ ij i′j′
i,j=1
P∈Pni,j,i′,j′=1
By developing each term and exploiting that for any P∈P , P1 =P⊤1 = 11 , we derive
n n n n n
n n
(cid:88) (cid:88)
f(T)= 1 −⟨x ,x ⟩·⟨t ,t ⟩− min 1 −⟨x ,x ⟩·⟨t ,t ⟩P P
n2 i j i j n2 i i′ j j′ ij i′j′
i,j=1
P∈Pn
i,j,i′,j′=1
n n
(cid:88) (cid:88)
= max 1 ⟨x ,x ⟩·⟨t ,t ⟩P P − 1 ⟨x ,x ⟩·⟨t ,t ⟩
n2 i i′ j j′ ij i′j′ n2 i j i j
P∈Pn
i,j,i′,j′=1 i,j=1
= max⟨ 1 P⊤XX⊤P,TT⊤⟩−⟨ 1 XX⊤,TT⊤⟩
n2 n2
P∈Pn
= max⟨ 1 (P⊤XX⊤P−XX⊤),TT⊤⟩
n2
P∈Pn
= max⟨ 1 (P⊤XX⊤P−XX⊤)T,T⟩
n2
P∈Pn
= max⟨A T,T⟩
X,P
P∈Pn
where we define A := 1 (P⊤XX⊤P−XX⊤) ∈ Rn×n. To study the convexity of this matrix input
X,P n2
function, we vectorize it. From [53, Eq. (520)], we note that, for any M∈Rn×n
⟨MT,T⟩=vec(T)⊤vec(MT)=vec(T)⊤(M⊗I )vec(T)
n
where vec is the vectorization operator, raveling a matrix along its rows, and ⊗ is the Kronecker product.
Applying this identity, we reformulate:
f(T)= max vec(T)⊤(A ⊗I )vec(T) (16)
X,P n
P∈Un
To study the convexity of r, we study the convexity of each r (T):=vec(T)⊤(A ⊗I )vec(T),
AX,P X,P n
which are quadratic forms induced by the A ⊗I . This remains to study the (semi-) positive definiteness
X,P n
of the matrices A ⊗I . As each A ∈Rn×n is symmetric and square, A ⊗I is also symmetric
X,P n X,P X,P n
and from [53, Eq. (519)] its eigenvalues are the outer products of the eigenvalues of A and I , namely
X,P n
eig(A ⊗I )={λ (A )·λ (I )}
X,P n i X,P j n 1≤i,j≤n
={λ (A ),...,λ (A ),...,λ (A ),...,λ (A )} (17)
1 X,P 1 X,P n X,P n X,P
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
ntimes ntimes
22It follows that the minimal eigenvalue of A ⊗I is λ (A ⊗I ) = λ (A ). Utilizing the
X,P n min X,P n min X,P
expression of A
X,P
λ (A )= 1 λ (P⊤XX⊤P−XX⊤)
min X,P n2 min
≥ 1 (λ (P⊤XX⊤P)+λ (−XX⊤)) (18)
n2 min min
= 1 (λ (P⊤XX⊤P)−λ (XX⊤))
n2 min max
Reminding that P∈U , one has P⊤ =P−1, so P⊤XX⊤ and XX⊤ are similar, and they have the same
n
eigenvalues. In particular λ (P⊤XX⊤P)=λ (XX⊤). Combining these results, it follows that
min min
λ (A ⊗I )=λ (A )≥ 1 (λ (XX⊤)−λ (XX⊤)) (19)
min X,P n min X,P n2 min max
We then remind that each r is the quadratic form defined by A ⊗I , so by applying Prop. D.2,
AX,P X,P n
it is A ⊗I -weakly convex, and hence 1 (λ (XX⊤)−λ (XX⊤))-weakly convex. Therefore, ap-
X,P n n2 max min
plying Prop. (D.3), r is 1 (λ (XX⊤)−λ (XX⊤))-weakly convex, in Rd. Reminding that γ =
n2 max min inner
1(λ (XX⊤)−λ (XX⊤)), r is 1γ weakly convex. This implies that T (cid:55)→ f(T)+ 1γ ∥T∥2 is
n max min n inner n inner 2
convex. By reminding that T stores the T(x ) as rows, 1∥T∥2 =∥T∥ . Consequently, GM⟨·,·⟩ is γ
i n 2 L2(rn) rn inner
in L (r ).
2 n
We then study the convexity of GM2 . We follow exactly the same approach. One has:
rn
n
(cid:88)
GM2 (T)= 1 1|∥x −x ∥2−∥T(x )−T(x )∥2|2
rn n2 2 i j 2 i j 2
i,j=1
n
(cid:88)
− 1 min 1|∥x −x ∥2−∥T(x )−T(x )∥2|2|2P P
n2 2 i j 2 i j 2 ij i′j′
P∈Pni,j,i′,j′=1
Similarly, studyingtheconvexityofGM2 (T)remainstostudytheconvexityofthematrixinputfunction:
rn
n
(cid:88)
g(T):= 1 1|∥x −x ∥2−∥t −t ∥2|2
n2 2 i j 2 i j 2
i,j=1
n
(cid:88)
− 1 min 1|∥x −x ∥2−∥t −t ∥2|2P P
n2 2 i j 2 i j 2 ij i′j′
P∈Pni,j,i′,j′=1
As before, by developing each term, one has:
n n
(cid:88) (cid:88)
g(T)= max 1 ⟨x ,x ⟩·⟨t ,t ⟩P P + 1 P ∥x ∥2∥t ∥2
n2 i i′ j j′ ij i′j′ 2n ij i 2 i 2
P∈Pn
i,j,i′,j′=1 i,j=1
 
n n
(cid:88) (cid:88)
− n1
2
⟨x i,x j⟩·⟨t i,t j⟩+ 21
n
∥x i∥2 2∥t i∥2 2
i,j=1 i,j=1
The quadratic terms in P can be factorized as before using A . For the new terms w.r.t. the inner
X,P
product case, we introduce D :=diag(∥x ∥2,...,∥x ∥2), and remark that we can rewrite:
X 1 2 n 2
n n
1 (cid:88) P ∥x ∥2∥t ∥2− 1 (cid:88) ∥x ∥2∥t ∥2 =vec(T)⊤(cid:0) 1 (P⊤−I )⊗D (cid:1) vec(T)
2n ij i 2 i 2 2n i 2 i 2 2n n X
i,j=1 i,j=1
As we can always symetrize the matrix when considering its associated quadratic form, we have:
n n
1 (cid:88) P ∥x ∥2∥t ∥2− 1 (cid:88) ∥x ∥2∥t ∥2 =vec(T)⊤(cid:0)1( 1 (P⊤+P)−I )⊗D (cid:1) vec(T)
2n ij i 2 i 2 2n i 2 i 2 2 2n n X
i,j=1 i,j=1
23As a result, we denote B = 1(1(P⊤+P)−I )⊗D and finally get:
X,P n 2 n X
g(T)= max vec(T)⊤(A ⊗I +B )vec(T)
X,P n X,P
P∈Pn
As we did for f, studying the weak convexity of f remains to lower bound the minimal eigenvalue of
A ⊗I +B . First, one remark that:
X,P n X,P
λ (A ⊗I +B )≥λ (A ⊗I )+λ (B )
min X,P n X,P min X,P n min X,P
As we we have already lower bounded λ (A ⊗I )≥ 1 (λ (XX⊤)−λ (XX⊤)), we focus on
min X,P n n2 min max
the RHS. Similarly, one has:
λ (B )=λ (cid:0) 1 (1(P⊤+P)−I )⊗D (cid:1)
min X,P min 2n 2 n X
≥λ (cid:0) 1 (P⊤+P)⊗D (cid:1) +λ (cid:0) − 1 I ⊗D (cid:1) (20)
min 4n X min 2n n X
≥λ (cid:0) 1 (P⊤+P)⊗D (cid:1) −λ (cid:0) 1 I ⊗D (cid:1)
min 4n X max 2n n X
For both terms, we apply again [53, Eq. (519)]. For the LHS, one has:
eig(cid:0) 1 (P⊤+P)⊗D (cid:1) ={λ ( 1 (P⊤+P))λ (D )} (21)
4n X i 4n j X 1≤i,j≤n
We remark that 1(P⊤+P) is a symetric bi-stochastic matrix, so λ (1(P⊤+P)) ≥ −1. Therefore,
2 min 2
λ ( 1 (P⊤+P))≥− 1 . As a result, since the eigenvalues of D are the ∥x ∥2, this yields:
min 4n 2n X i 2
λ (cid:0) 1 (P⊤+P)⊗D (cid:1) ≥− 1 max ∥x ∥2
min 4n X 2ni=1,...,n i 2
Similarly, we have:
−λ (cid:0) 1 I ⊗D (cid:1) ≥− 1 max ∥x ∥2
max 2n n X 2ni=1,...,n i 2
from which we deduce that:
λ (B )≥−1 max ∥x ∥2
min X,P ni=1,...,n i 2
We can then lower bound:
λ (A ⊗I +B )≥ 1 (λ (XX⊤)−λ (XX⊤))− 1 max ∥x ∥2
min X,P n X,P n2 min max ni=1,...,n i 2
(22)
=−1γ
n 2,n
which yields the 1γ -weak convexity of g, and finally the γ -weak convexity of GM2 .
n 2,n 2,n rn
Asymptotic. For any T, we note that, almost surely, ∥T∥2 →∥T∥2 . As a result, since convexity
L2(rn) L2(r)
ispreservedunderpointwiseconvergenceandbyvirtueofProp.(C.2),westudythe(almostsure)convergence
of γ and γ .
inner,n 2,n
We start by γ . We first remark that λ (1XX⊤)=λ (1X⊤X). Moreover, as A∈S+(R)(cid:55)→
inner,n max n max n d
λ (A) is continuous and 1X⊤X→E [xx⊤] almost surely, one has λ (1XX⊤)→λ (E [xx⊤])
max n x∼r max n max x∼r
almost surely. Moreover, for any n>d, λ (1XX⊤)=0. As a result, γ →λ (E [xx⊤]) almost
min n inner,n max x∼r
surely, which provides the desired asymptotic result.
Wecontinuewithγ . Wefirstremarkthatmax ∥x ∥2 ≤sup ∥x∥2. Asaresult,bydefining
2,n i=1,...,n i 2 x∈Spt(r) 2
γ˜ =γ +max ∥x∥2, GM2 is also γ˜ -weakly convex. Moreover, max ∥x∥2 does not
2,n inner,n x∈Spt(r) 2 rn 2,n x∈Spt(r) 2
depends on n, γ˜ → λ (E [xx⊤])+max ∥x∥2 almost surely, which also provides the desired
2,n max x∼r x∈Spt(r) 2
asymptotic result.
24Table 4: Disentanglement of regularizing the Encoder and the Encoder and Decoder as measured by DCI-D
on two different datasets. We highlight best, second best, and third best results for each method and dataset.
DCI-D β-VAE β-TCVAE β-VAE + HFS β-TCVAE + HFS
Shapes3D [33]
Base 67.7 ±7.8 75.6 ±8.7 88.1 ±7.4 89.5 ±7.9
+ Enc-(DST) 69.2 ±9.1 77.2 ±7.5 87.7 ±7.7 90.5 ±5.9
+ Enc-(GMG) 70.9 ±9.5 79.6 ±6.6 92.5 ±5.9 93.5 ±6.9
+ Dec-(DST) 76.8 ±4.1 81.3 ±4.7 87.5 ±3.3 91.9 ±9.4
+ Dec-(GMG) 82.1 ±4.5 83.7 ±8.8 95.7 ±5.8 96.9 ±4.9
+ Enc-Dec-(GMG) 72.8 ±7.7 79.3 ±13.9 93.3 ±5.0 91.8 ±7.3
DSprites [27]
Base 27.6 ±13.4 36.0 ±5.3 38.7 ±15.7 48.1 ±10.8
+ Enc-(DST) 32.8 ±15.0 36.5 ±5.9 33.9 ±15.9 48.9 ±11.1
+ Enc-(GMG) 27.5 ±14.3 37.4 ±5.8 31.0 ±14.3 45.9 ±10.9
+ Dec-(DST) 28.6 ±19.3 32.4 ±8.5 39.3 ±18.1 49.0 ±11.2
+ Dec-(GMG) 39.5 ±15.2 42.2 ±3.6 46.7 ±2.0 50.1 ±8.5
+ Enc-Dec-(GMG) 33.1 ±14.9 40.2 ±7.0 28.7 ±14.6 46.0 ±11.3
E Additional Empirical Results
F Experimental Details
All our experiments build on python 3 and the jax-framework [2], alongside ott-jax for optimal transport
utilities.
To effectively conduct comprehensive and representative research on disentangled representation learning,
we convert the public PyTorch framework proposed in [60] to an equivalent jax variant. We verify our
implementation through replications of baseline and HFS results in Roth et al. [60], mainting relative perfor-
mance orderings and close absolute disentanglement scores (as measured using DCI-D, whose implementation
directly follows from [43] and leverages gradient boosted tree implementations from scikit-learn).
Forexactandfaircomparison,weutilizestandardhyperparamaterchoicesfromRothetal.[60](whichlever-
ageshyerparametersdirectlyfrom[43],[44]andhttps://github.com/google-research/disentanglement_
lib). Consequently, the base VAE architecture utilized across all experiment is the same as the one utilized
in [60] and [44]: With image input sizes of 64×64×N (with N the number of input image channels, usually
c c
3). The latent dimensionality, if not otherwise specified, is set to 10. The exact VAE model architecture is as
follows:
• Encoder: [conv(32,4×4, stride 2) + ReLU] × 2, [conv(64,4×4, stride 2) + ReLU] × 2, MLP(256),
MLP(2 × 10)
• Decoder: MLP(256), [upconv(64,4×4, stride 2) + ReLU] × 2, [upconv(32,4×4, stride 2) + ReLU],
[upconv(n ,4×4, stride 2) + ReLU]
c
Similar, we retain all training hyperparameters from [60] and [44]: Using an Adam optimizer ([34],
β =0.9,β =0.999,ϵ=10−8) and a learning rate of 10−4. Similarly, we utilize a batch-size of 64, for which
1 2
we also ablate all baseline methods. The total number of training steps is set to 300000.
As commonly done for this setting [43, 44, 60], we also perform a small grid search over all the hyperpa-
rameters. We report the full details in Tab. 5.
25Table 5: Hyperparameter grid searches for different baseline and proposed methods.
Method Parameter Values
β-VAE β [2, 4, 6, 8, 10, 16]
β-TCVAE β [2, 4, 6, 8, 10, 16]
+ HFS γ [1, 10]
+ DST λ [0.1, 1, 5, 10, 20]
+ GMG λ [0.1, 1, 5, 10, 20]
For λ and λ , we set λ =0 and λ =1 for the Decoder setting and λ =1 and λ =0 for the
enc dec enc dec enc dec
Encoder setting. For the joint one, we set λ =0.5 and λ =0.5. All runs run on an RTX 2080TI GPU.
enc dec
26