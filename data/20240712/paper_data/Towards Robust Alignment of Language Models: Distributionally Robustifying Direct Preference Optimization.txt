Towards Robust Alignment of Language Models:
Distributionally Robustifying Direct Preference
Optimization
JunkangWu1‚àóYuexiangXie2 ZhengyiYang1 JiancanWu1
JiaweiChen3 JinyangGao2 BolinDing2 XiangWang1‚Ä† XiangnanHe1‚Ä†
1UniversityofScienceandTechnologyofChina
2AlibabaGroup 3ZhejiangUniversity
{jkwu0909, yangzhy1998, wujcan, xiangwang1223, xiangnanhe}@gmail.com,
{yuexiang.xyx, jinyang.gjy, bolin.ding}@alibaba-inc.com,sleepyhunt@zju.edu.cn
Abstract
ThisstudyaddressesthechallengeofnoiseintrainingdatasetsforDirectPrefer-
enceOptimization(DPO),amethodforaligningLargeLanguageModels(LLMs)
withhumanpreferences. Wecategorizenoiseintopointwisenoise,whichincludes
low-qualitydatapoints,andpairwisenoise,whichencompasseserroneousdata
pairassociationsthataffectpreferencerankings. UtilizingDistributionallyRobust
Optimization(DRO),weenhanceDPO‚Äôsresiliencetothesetypesofnoise. Our
theoreticalinsightsrevealthatDPOinherentlyembedsDROprinciples, confer-
ringrobustnesstopointwisenoise,withtheregularizationcoefficientŒ≤ playinga
criticalroleinitsnoiseresistance. Extendingthisframework,weintroduceDis-
tributionallyRobustifyingDPO(Dr. DPO),whichintegratespairwiserobustness
byoptimizingagainstworst-casepairwisescenarios. Thenovelhyperparameter
Œ≤‚Ä≤inDr. DPOallowsforfine-tunedcontroloverdatapairreliability,providinga
strategicbalancebetweenexplorationandexploitationinnoisytrainingenviron-
ments. EmpiricalevaluationsdemonstratethatDr. DPOsubstantiallyimprovesthe
qualityofgeneratedtextandresponseaccuracyinpreferencedatasets,showcasing
enhancedperformanceinbothnoisyandnoise-freesettings. Thecodeisavailable
athttps://github.com/junkangwu/Dr_DPO.
1 Introduction
AligningLargeLanguageModels(LLMs)[32,41,1,8]withhumanpreferencesiscriticalfortheir
implementationinreal-worldscenarios. Centraltothealignmentisthefine-tuningofLLMsusing
human feedback [33], ensuring they adhere to human values and mitigate safety risks. Among
thealignmentmethods,ReinforcementLearningfromHumanFeedback(RLHF)[33]isbecoming
awidelyadoptedtechnology. Itinitiallylearnsarewardmodelonpairwisepreferencedata, and
optimizesLLMsusingtheProximalPolicyOptimization(PPO)[37]method. However,itsinherent
reinforcementlearningnatureposessignificantchallengestocomputationalefficiencyandtraining
stability[35,46]. Addressingthese,DirectPreferenceOptimization(DPO)[35]eschewstheexplicit
rewardmodellearning,usinghumanpreferencestotraintheLLMsdirectly. Itachievesthesame
objectives[2]asRLHFbylearninganoptimalproxyforeachpointwiseinstanceandsimultaneously
rankingpreferencesinapairwisemanner,offeringgreatersimplicityandtrainingstability[21].
‚àóWorkdoneatAlibabaGroup.
‚Ä†XiangWangandXiangnanHearethecorrespondingauthors.
Preprint.Underreview.
4202
luJ
01
]GL.sc[
1v08870.7042:viXra: high quality review : low quality review
: mistaken preferred review : correct preferred review
Review AÔºö‚ÄùFantastic experience Review BÔºö‚ÄùFantastic, uh,
with delicious food and excellent something food, and the service
service!‚Äù was... yeah."
pointwise noise
Prompt: Please rank the restaurant reviews dataset based on your experience:
Review C: "The meal was Review D: "An amazingculinary
quite good, and the staff ‚â∫ adventure; the flavors were
was friendly." extraordinary!"
Low ( Noisy ) High ( Clean )
ùíöùíç i rn ac no kr ir ne gct ùíöùíò pairwise noise Quality of pairwise preference
Figure1:Left:Anexampleillustratingpointwiseandpairwisenoise.Right:Comparisonofgradients
betweenDPOandDr. DPOundervaryinglevelsofpairwisenoise.
Whileofferinganeffectivesolutionbydirectlylearningapolicyfromcollecteddata,DPOinevitably
heightens the dependency on the data quality [25]. However, training data is frequently marred
bynoise,potentiallyposingasignificantchallengetoDPO.Herewedelineatetwoprimarynoise
categoriesbasedontheirorigins:
‚Ä¢ Pointwisenoise[16]referstolow-qualitydatapointscontainingirrelevantorincoherentinformation.
TakingthemoviereviewsinFigure1(Left)asanexample,itmightmanifestasreviewsfilledwith
meaninglesschatter,thusrenderingthemuninformative.
‚Ä¢ Pairwisenoise[39,12],ontheotherhand,arisesfromerroneousassociationsbetweendatapairs,
leading to misjudged preference rankings. Revisiting the movie reviews in Figure 1 (Left), it
isevidentinmisrankedreviewswhereaninferiorreview(y )isincorrectlyratedhigherthana
l
superiorone(y ).
w
Thepresenceofnoisypreferencesnaturallyraisesacriticalquestion: HowrobustisDPOagainst
pointwiseandpairwisenoise? Toanswerthis,weexamineDPOthroughthelensofDistributionally
RobustOptimization(DRO)[30,14]. AtthecoreofDROistrainingamodelacrossadistributional
family,whichisdeterminedbyanempiricaldistributionwithinarobustradiusŒ∑. Asaresult,DRO
endowsthemodelwithenhancedrobustnessw.r.t. distributionaluncertainty,usuallycausedbythe
datanoise. ByincorporatingDROprinciples,wecanassesstheresilienceofDPOtothepointwise
andpairwisenoise. Specifically,ourDROlensonDPOoffersinsightfulfindingsasfollows:
‚Ä¢ DPOisequivalenttoapplyingDROontherewardfunction. TheprincipalcontributionofDPO
isderivingtheoptimalpolicyforPPOinaclosed-formexpression. Thisachievementfacilitatesthe
implicitdeterminationofaworst-casedistributionforoptimization,guidedbytheKullback-Leibler
(KL)divergencecriterion. SuchanapproachendowsDPOwithintrinsicpointwiserobustness,
enablingittoexploreabetterpolicymodelratherthanrelyingsolelyonthereferencemodel.
‚Ä¢ The DPO‚Äôs Œ≤ and DRO‚Äôs Œ∑ share an inverse relationship, highlighting noise levels in the
referencemodel. UtilizingDROtheory,ourfindingsrevealthathighernoiselevelswithinthe
referencemodelnecessitateabroadersearcharea,indicatinganeedforalargerŒ∑(orasmallerŒ≤).
These findings elucidate the strengths of DPO in ensuring pointwise robustness. Recent effort
[10] has started addressing pairwise noise in DPO frameworks; however, this method relies on
explicitnoiseestimation,aprocessthatiscomputationallyintensiveandmaynotfullycapturenoise
complexities. Buildingontheseinsights,weintroducetheDistributionallyRobustifyingDPO(Dr.
DPO)3framework,aimingtoincorporatepairwiserobustnesswithintheDPOparadigm. Thecore
ideaisoptimizingagainsttheworst-casepairwisescenarios,enablingthemodelstoimplicitlyadjust
theimportanceofdatapairsinthegradientspaceandeliminatetheexplicitnoiseestimation. Towards
theadjustment,Dr. DPOintroducesasimplehyperparameterŒ≤‚Ä≤ ‚àà (0,+‚àû)tomodulatetheloss
function, balancingbetweenexplorationandexploitationofpairwisepreferences. Œ≤‚Ä≤ servesasa
pivotal‚Äúknob‚Äù,allowingthenavigationfromaconservativestrategythatdiminishestheinfluenceof
3Theabbreviation‚ÄúDr.DPO‚Äùnotonlyencapsulates‚ÄúDistributionallyRobustifyingDPO‚Äùbutisplayfully
intendedtoechotheabbreviationfor"Doctor,"addingaquirkyelementtothenaming.
2potentiallynoisypairs(e.g.,Œ≤‚Ä≤ =0.5)toarisk-tolerantstancethatleveragessuchpairs(e.g.,Œ≤‚Ä≤ =2).
Consequently,Dr. DPOfostersamoreresilientoptimizationprocessthateffectivelymitigatesthe
influenceofbothpointwiseandpairwisenoise.
Inanutshell,ourcontributionisthedevelopmentofDr. DPO,whichrobustifiesDPOwithjusta
single additional line of code. Empirical evaluations reveal that Dr. DPO significantly enhances
performanceacrossdiversesettings,suchascontrollingthesentimentingeneratedtextandimproving
theresponsequalityinsingle-turndialogues,underbothnoisyandnoise-freeconditions.
2 Preliminaries
Bradley-Terry Model. Given a context x within a finite space of contexts X, we employ the
policyœÄ(y|x)toindependentlygenerateapairofactions(y ,y ). Theseactionsarepresentedto
1 2
humanraters,whothenindicatetheirpreference,withthepreferredactionlabeledasy andtheless
w
preferredasy ,satisfyingy ‚™∞ y . Althoughwecannotdirectlyobservethelatentrewardmodel
l w l
r‚àó(x,y)thatunderliesthesepreferences,theBradley-Terry(BT)model[7]offersawell-established
approachformodelingpairwisecomparisons,whichisgivenas:
exp(r‚àó(x,y ))
p‚àó(y ‚™∞y |x)= 1 . (1)
1 2 exp(r‚àó(x,y )+exp(r‚àó(x,y )))
1 2
GiventhedatasetO = (x(i),y(i),y(i))N sampledfromp‚àó, wecanparametrizearewardmodel
w l i=1
r (x,y)andestimatetheparametersbyoptimizingthefollowinglogisticregressionloss:
œï
L (r ,O)=‚àíE [logœÉ(r (x,y )‚àír (x,y ))], (2)
R œï (x,yw,yl)‚àºO œï w œï l
whereœÉ(¬∑)isthesigmoidfunction. AsthesizeofdatasetOgrows,theempiricaldistributionofthe
datasetOconvergestotheunderlyingdistributionp‚àó,andtherewardmodelr convergestothetrue
œï
rewardmodelr‚àó.
ReinforcementLearningfromHumanFeedback(RLHF)[33]. ThestandardRLHFparadigmis
composedofthreephases: i)supervisedfine-tuning,ii)rewardmodeling,andiii)RLfine-tuning.
Usingtherewardmodelr learnedfromtherewardmodeling,wecanthenfine-tunethepolicyœÄ by
œï Œ∏
optimizingthefollowingobjective:
maxE [r (x,y)]‚àíŒ≤D [œÄ (y|x)||œÄ (y|x)]. (3)
œÄŒ∏
x‚àºO,y‚àºœÄŒ∏(y|x) œï KL Œ∏ ref
Inpractice,boththelanguagemodelpolicyœÄ andthereferencepolicyœÄ aretypicallyinitializedto
Œ∏ ref
thesamesupervisedfine-tuning(SFT)modelœÄ . Here,Œ≤ isaparameterthatcontrolsthestrength
SFT
oftheregularizationterm,andD representstheKLdivergencepenaltyusedtoregularizethepolicy
KL
œÄ tobeclosetoœÄ .
Œ∏ ref
Directed Preference Optimization (DPO) [35]. DPO offers an alternative approach to the RL
paradigmdescribedabove. Itestablishesafunctionalmappingbetweentherewardmodelandthe
optimalpolicyunderaKLdivergenceconstraintwiththefollowingformulation:
œÄ (y|x)
r(x,y)=Œ≤log Œ∏ +Œ≤logZ(x), (4)
œÄ
ref(y|x)
(cid:80)
whereZ(x)= œÄ (y|x)exp(r(x,y)/Œ≤)isthepartitionfunction. Byincorporatingthisreward
y ref
into the BT model, the DPO objective enables the comparison of response pairs, facilitating the
discriminationbetweenpreferredanddispreferredactions,givenby:
œÄ (y |x) œÄ (y |x)
L (œÄ ;œÄ )=‚àíE [logœÉ(Œ≤log Œ∏ w ‚àíŒ≤log Œ∏ l )]. (5)
DPO Œ∏ ref (x,yw,yl)‚àºO œÄ (y |x) œÄ (y |x)
ref w ref l
DistributionallyRobustOptimization(DRO)[22,24,43]. DROprovidesastrategicframeworkto
effectivelymitigatetheuncertaintyinherentintrainingdata. Itachievesthisbyoptimizingforthe
worst-caseexpectedlossacrossasetofpotentialdistributionsQ. Thesedistributionsareconfined
withinarobustnessradiusŒ∑anchoredaroundtheempiricaltrainingdistributionQ ,andarebounded
0
byaprescribeddivergencemetricD . TheformalformulationofDROcanbesuccinctlyexpressed
œï
asfollows:
L =maxE [L(x;Œ∏)], s.t. D (Q,Q )‚â§Œ∑, (6)
DRO Q œï 0
Q
31.0 1.0
0.9
0.9
0.8
20% low-quality data
0.8
0.7 40% low-quality data
High data quality (DPO) 60% low-quality data
0.6 Low data quality (DPO) 0.7 80% low-quality data
100% low-quality data
0.5
0.6
0 2 4 6 8 10 12 14 10 4 10 3 10 2 10 1
KL( || ref)
Figure2: Left: ImpactofpointwisenoiseontheexpectedrewardfrontierandKLdivergencein
DPO(Œ≤ =0.1). Right: Comparativeanalysisoftheeffectofpointwisenoiseontheexpectedreward
frontierfordifferentŒ≤ values.
whereL(x;Œ∏)representsthetraininglossforaninputx. Intuitively,modelsemployingDROexhibit
increasedrobustnessduetothepresenceofQthatactsasan‚Äúadversary‚Äù,optimizingthemodelunder
adistributionsetwithadversarialperturbationsinsteadofasingletrainingdistribution.
3 AnalyzingDPO‚ÄôsPointwiseRobustness
Inthissection,weexaminetheresilienceofDPOtopointwisenoise. WeanalyzehowDPOresponds
tonoise,highlightingitsstrengthsandvulnerabilities.
3.1 PointwiseNoiseImpairsDPOPerformance
WestartbyinvestigatingtheimpactofpointwisenoiseonDPObyconductingexperimentsonthe
IMDBsentimentdataset[27]. Followingthesetupin[18],wefine-tunetheGPT-2-large[34]model
and employ SiEBERT [17], a specialized variant of RoBERTa-large [26], for reward calculation.
Weintroducepointwisenoisebyintegratinganadditionalproportionoflow-qualitypairs(y ,y )
w l
generatedbytheunrefinedGPT-2-largemodeltothetrainingset. ToquantitativelymeasureDPO‚Äôs
robustnessagainstpointwisenoise,weevaluatetheperformanceofeachalgorithmbyexaminingthe
trade-offbetweentheachievedrewardandtheKLdivergencefromthereferencepolicy.
Figure2(Left)revealsthatbeyondaKL(œÄ ||œÄ )thresholdof10.0,bothmodelsconvergeintermsof
Œ∏ ref
reward.Notably,theDPOmodeltrainedwithhigh-qualitydata(bluepoints)significantlyoutperforms
itslow-qualitydatacounterpart(orangepoints),highlightingthecriticalimpactofdataqualityon
optimizingmodelperformance.
3.2 PointwiseRobustnessinRewardModeling
InSection3.1,weinvestigatethenegativeimpactofpointwisenoiseontherewardofindividual
instances. ToaddressthisissueandenhancetherobustnessofLLMs,weproposeintegratingDRO
duringtherewardmodelingstage.WedefinetheRewardModelingDRO(RM-DRO)objective,which
optimizestheexpectedrewardundertheworst-casenoisedistributionwithinaspecifiedambiguity
setasfollows:
m œÄa Œ∏xE x‚àºO,y‚àºœÄŒ∏(y|x)[r œï(x,y)] s.t. D œï(œÄ Œ∏(y|x),œÄ ref(y|x))‚â§Œ∑. (7)
Thedirectconsequenceofpointwisenoiseistheresultantunreliabilityofthereferencemodel(SFT).
ByadoptingRM-DRO,weaimtomaximizeasurrogateobjectivethataccountsforvariouspotential
distributionswithinarobustnessradiusŒ∑aroundthereferencedistributionœÄ (y|x),measuredby
ref
thedistancemetricD . Withthisformulation,weprovideafreshperspectiveonDPO.
œï
A.DPOisImplicitlyaPointwiseDRO.
Thm3.1(OptimalRewardFunctionunderKLDivergence). LettheKullback-Leibler(KL)divergence
(cid:16) (cid:17)
betweenpolicyœÄ andreferencepolicyœÄ bedefinedas: D (œÄŒ∏|œÄ )=(cid:82) œÄ (x)log œÄŒ∏(x) dx.
Œ∏ ref KL ref Œ∏ œÄref(x)
4
draweR draweROptimizing the RM-DRO objective as defined in Equation (7) yields an optimal reward r (x,y)
KL
givenby:
œÄ (y|x)
r (x,y)=Œ≤‚àó(Œ∑)log Œ∏ ‚àíŒ±. (8)
KL œÄ
ref(y|x)
Here,Œ±,Œ≤ areLagrangemultipliers,Œ≤‚àó(Œ∑)denotestheoptimalvalueofŒ≤ thatminimizesEquation
(7),actingastheregularizationcoefficientinDPO.ByderivingtheoptimalvalueofŒ±,givenby:
r (y|x)
Œ±‚àó =‚àíŒ≤logE [exp( Œ∏ )], (9)
x‚àºO,y‚àºœÄref Œ≤
Equation(8)canbere-expressedtomatchtheultimateformoftherewardfunctioninEquation(4).
PleasecheckAppendixD.1fordetailedproofs. Forabroaderdiscussiononoptimalrewardfunctions
undergeneralœï-divergences,seeAppendixE.1. Consistentwiththerewardfunctionformulation
presentedinRafailovetal.[35],Theorem3.1notonlyreaffirmspreviouslyestablishedresultsbut
alsointroducesseveralnovelinsights,asdelineatedbelow:
WhyDPOisRobusttoPointwiseNoise. Weproposethatthereferencedistributioncloselymirrors
theempiricaltrainingdistribution,giventhepre-trainingstep(SFT)commontobothRLHFandDPO
methods. ThisensuresthereferencedistributionintheDPOphaseaccuratelyreflectsthetrainingdata
noise. IntermsofDRO,whilethereferencemodel(œÄ )maynotbeentirelyreliable,theimplicit
ref
robustframeworkofDPOcountersdataperturbationseffectively. The‚Äúworst-casedistribution‚Äùis
definedasthedistributionthatmaximizesriskwithinestablisheddivergenceconstraints,analogous
toanadversarialnoisemodelinDRO.VaryingŒ≤ enablesDPOtoexhibitvaryingsearchspacefora
betterœÄ ,leadingtoimprovedperformance. FormorecomparisonbetweenDPOandDRO,please
Œ∏
refertoAppendixE.2.
Moreover, the incorporation of DRO provides a new interpretation of the coefficient Œ≤ in DPO,
transforming it from a mere heuristic design into a ‚Äúnoise reflector‚Äù. We provide Lemma 3.2 to
disclosetherelationshipbetweenŒ≤ andŒ∑.
B.TheOptimalValueofŒ≤ ReflectstheNoisewithintheSFTModel.
Lemma3.2. [15,Lemma5]TheoptimalŒ≤‚àó(Œ∑)inDPOismonotonicallydecreasingwithrespectto
Œ∑andobeysthefollowingrelationship:
(cid:113)
Œ≤‚àó(Œ∑)= V [r(x,y)]/2Œ∑, (10)
œÄref
whereV [r(x,y)]denotesthevarianceoftherewardmodelr(x,y)underthereferencedistribution
œÄref
œÄ .
ref
Lemma3.2elucidatestheinversecorrelationbetweentheparameterŒ≤ andtherobustnessradiusŒ∑.
ThevalueofŒ∑delineatestheboundariesofthesearchspace;towit,anincreaseinnoisenecessitates
anexpansionoftherequiredsearchspace,therebymandatingalargerŒ∑. Weempiricallyvalidatethis
relationshipbyconductingexperimentsontheIMDBdataset,trainingmodelswithvaryingŒ≤ values
underdifferentlevelsofpointwisenoise. Theresults,illustratedinFigure2(right),revealthatthe
modelwithalowerŒ≤ value(0.01)outperformsitscounterpartwithahigherŒ≤ value(0.1)whenboth
aretrainedusingunrefineddatageneratedbyGPT-2,composedentirelyoflow-qualityinformation
(100%). Thisisattributabletothefactthatunderconditionsof100%low-qualitydata,areduced
Œ≤ valueaffordsalargersearchspacetocounteractthehighlevelofpointwisenoisewithintheSFT
model. Furthermore,weobservethatasthequantityofpointwisenoiseincreases,modelswithlower
Œ≤ valuesdemonstratemoresubstantialimprovementsinperformance,underscoringareducedvalue
ofoptimalŒ≤ isareflectorofahighlevelofpointwisenoisewithintheSFTmodel.
4 Dr. DPO:TowardPairwiseRobustness
Inthissection,weinvestigatetheimpactofpairwisenoiseandintroduceDr. DPOasamitigation
strategy. Weconcludewithatheoreticalexaminationofitsrobustnessagainstsuchnoise.
4.1 PairwiseNoiseImpairsDPOConvergenceandPerformance
WepreviouslyexploredDPO‚Äôspointwiserobustness,whilerecentwork[10]hasexamineditsre-
siliencetopairwisenoise. However,methodsthatrelyonexplicitnoiseestimationmayoverlook
5complexnoisebehaviors. WethusempiricallyexaminehowpairwisenoiseaffectsDPO‚Äôsperfor-
mance. Similar to the experimental settings in Section 3.1, we corrupt the dataset with pairwise
noisebyrandomlyswappingpairs(y ,y )inthepreferencedatasetO = {x(i),y(i),y(i)}N . As
w l w l i=1
illustratedinFigure3(Left),whenexposedtoelevatedlevelsofpairwisenoise(specifically,10%and
30%labelflipping),DPOexhibitsadiminishedrateofincreaseinKL(œÄ ||œÄ )overanequivalent
Œ∏ ref
numberoftrainingepochs(epoch=1). Thispatternisindicativeofadeceleratedconvergencerate
intermsofrewardvalue,especiallywhencomparedtothetrendofthemodeltrainedonnoise-free
data(0%flippedpairs). ThustheoverallperformanceofDPOdropssignificantly.
DespiteitseffectivenessinmitigatingpointwisenoisethroughtheadjustmentofŒ≤,DPOstillsuffers
fromtheissueofpairwisenoise. AsshowninFigure3(Right),themodeltrainedwith40%flipped
dataexhibitsasignificantperformancedegradationcomparedtothemodeltrainedwithnoise-free
data(0%flipped). EvenwhenthemodelistrainedwithalowerŒ≤ value,itstillfailstoachievethe
samelevelofperformanceasthemodeltrainedwithnoise-freedata. ThisobservationrevealsDPO‚Äôs
vulnerabilitytopairwisenoise,motivatingenhancementstoitsrobustnessagainstsuchinterference.
4.2 DistributionallyRobustifyingDPO
BuildingupontheprinciplesofDRO,weintroducetheDistributionallyRobustifyingDPO(Dr. DPO)
framework,designedtoenhanceDPO‚Äôsresiliencetopairwisenoisewhilepreservingitsinherent
robustnesstopointwisenoise. TheDr. DPOobjectiveisformulatedasfollows:
m Oa ‚Ä≤xE (x,yw,yl)‚àºO‚Ä≤[h(x,y w,y l)] s.t. D œï(O‚Ä≤,O)‚â§Œ∑‚Ä≤. (11)
Here, h(x,y ,y ) = logœÉ(r (x,y )‚àír (x,y ))denotesthelog-likelihoodobjectiveofdataset
w l œï w œï l
point. Theœï-divergence,denotedasD (O‚Ä≤,O),quantifiesthediscrepancybetweenthehypothetical
œï
distributionO‚Ä≤andthedatasetdistributionO. Additionally,Œ∑‚Ä≤signifiestherobustnessradius,which
quantifiesthedegreetowhichthemodelcanwithstandperturbations.
Thm4.1. ConsiderthescenariowheretheKLdivergenceisemployedtomeasurethediscrepancy
betweenthehypotheticaldistributionO‚Ä≤ anddatasetdistributionO , wederivetheultimateloss
functionforDr. DPOasfollows:
h (x,y ,y )
L (œÄ ;œÄ )=‚àíŒ≤‚Ä≤logE [exp( DPO w l )]. (12)
Dr.DPO Œ∏ ref O Œ≤‚Ä≤
whereh representsthelog-likelihoodintheDPOframework,definedas:
DPO
œÄ (y |x) œÄ (y |x)
h (x,y ,y)=logœÉ(Œ≤log Œ∏ w ‚àíŒ≤log Œ∏ l ), (13)
DPO w l œÄ (y |x) œÄ (y |x)
ref w ref l
withŒ≤ andŒ≤‚Ä≤beingregularizationcoefficientrespectively.
PleasecheckAppendixD.2fordetailedproofs. Incontrasttotheobjectivefunctiondescribedin
Equation(7),ourDr. DPOmethodspecificallytargetsthepairwisenoisepresentwithinthedataset.
Ratherthanassigningauniformweightof 1 toeachinstance(x(i),y(i),y(i))[35],Dr. DPOseeks
N w l
toreweighttheseinstancesbyanoptimaldistributionO‚Ä≤. Thisapproachisdrivenbytheintentionto
capturetheincorrectpairsinthedata,therebyenhancingtherobustnessoftheresultingpolicy.
Thm4.2(UpperBoundforDr. DPO). Leth ‚àà[a,b]andLN representstheDr. DPOlosson
DPO Dr.DPO
N samples. GivenahypotheticaldistributionO‚Ä≤satisfyingD (O‚Ä≤,O)‚â§Œ∑‚Ä≤todatasetdistribution
KL
O,wehavethatwithprobabilityatleast1‚àíŒ¥:
L ‚â§LN +B(Œ¥,N,Œ≤‚Ä≤), (14)
O‚Ä≤ Dr.DPO
where:
(cid:114)
2bexp((b‚àía)/Œ≤‚Ä≤) N 1
B(Œ¥,N,Œ≤‚Ä≤)= ln . (15)
N ‚àí1+exp((b‚àía)/Œ≤‚Ä≤) 2 Œ¥
PleasecheckAppendixD.3fordetailedproofs. Inscenariosinvolvingpairwisenoise,considerO‚Ä≤
asthe‚Äúideal‚Äùdistributionthatdiscernsthecorrectrankingbetweenpairwiseinstancesaccurately.
Theorem4.2suggeststheideallossrelativetoO‚Ä≤isupperboundedbytheproposedDr. DPOloss.
ThisboundisachievedwhenB(Œ¥,N,Œ≤‚Ä≤)approacheszero,orinotherwords,thenumberofsamples
61.0 1.0
0.9 0.9
0.8
0.8
0.7 0% Flipped pairs
0.7 10% Filpped pairs
10% Flipped pairs
0.6 20% Filpped pairs
30% Flipped pairs
0.6 30% Filpped pairs
0.5 40% Filpped pairs
0 2 4 6 8 10 12 14 10 4 10 3 10 2 10 1
KL( || ref)
Figure 3: Left: Impact of pairwise noise on the expected reward frontier and KL divergence in
DPO(Œ≤ =0.1). Right: Comparativeanalysisoftheeffectofpairwisenoiseontheexpectedreward
frontierfordifferentŒ≤ values.
N approachesinfinity. Furthermore,thisupperboundcanofferguidanceinreal-worldapplications.
Forinstance,inconjunctionwithLemma3.2,weinferthatincreasingtherobustnessradiusresultsin
adecreaseinŒ≤‚Ä≤,consequentlyincreasingB(Œ¥,N,Œ≤‚Ä≤). Insuchacase,toensureatightupperbound,
itbecomesnecessarytoenlargethesamplesizeN. Thisrelationshipbetweenrobustnessradius,Œ≤‚Ä≤,
andN providesinsightsforguidingthetrainingofLLMmodelstoachievedesiredperformance.
4.3 WhyisDr. DPORobusttoPairwiseNoise?
OurapproachextendstheanalysispresentedinRafailovetal.[35]. Tounderstandtheresilienceof
Dr. DPOtopairwisenoise,weexaminethegradientofitslossfunction,denoted‚àá L :
Œ∏ Dr.DPO
‚àá L (œÄ ;œÄ )=‚àíŒ≤E [w(x,y ,y ) œÉ(rÀÜ (x,y )‚àírÀÜ (x,y )) (‚àá ‚àí‚àá )],
Œ∏ Dr.DPO Œ∏ ref (x,yw,yl)‚àºO w l Œ∏ l Œ∏ w Œ∏,yw Œ∏,yl
Boostmismatchedpairgradients.
Reduceincorrectpair‚Äôsimpact.
whererÀÜ (x,y)=Œ≤log œÄŒ∏(y|x) representstherewardfunctionimplicitlylearnedbythepolicyœÄ ,
Œ∏ œÄref(y|x) Œ∏
relativetoareferencepolicyœÄ . Inthisframework,‚àá and‚àá aregradientsincreasingthe
ref Œ∏,yw Œ∏,yl
probabilityofthe‚Äúchosen‚Äùactiony anddecreasingitforthe‚Äúrejected‚Äùactiony ,respectively. The
w l
factorœÉ(rÀÜ (x,y )‚àírÀÜ (x,y ))servestoamplifythegradientcontributionsfrommismatchedaction
Œ∏ l Œ∏ w
pairs,whichisaprincipalaspectoftheDr. DPO‚Äôsdesignaimedatenhancinglearningfromcompara-
tivefeedback. Conversely,thefunctionw(x,y ,y ),definedasw(x,y ,y )=
exp(h(x,yw,yl)/Œ≤‚Ä≤)
w l w l E O[exp(h(x,yw,yl)/Œ≤‚Ä≤)]
(cf. Appendix D.4), acts to mitigate the influence of these incorrect pairings. It achieves this by
preferentially weighting correct action pairs over incorrect ones, thus refining the policy update
mechanism. Moreover,theparameterŒ≤‚Ä≤doesnotrequireintensivetuning;settingittoadefaultvalue
of1typicallyyieldsstableenhancements. Remarkably,Dr. DPOisstraightforwardtoimplement,
requiringonlyanadditionallineofcodewithnegligiblecomputationaloverhead.
5 Experiments
Inthissection,weconductanempiricalassessmentofDr.DPOtoevaluateitsabilitytomitigatenoise
impactsinpreferencedatasetsandtoimproveperformanceinnoise-freeenvironments.Weoutlineour
experimentaldesign,includingthedatasetsused,evaluationmetrics,andcomparativebenchmarks.
OurresultsunderscoreDr. DPO‚Äôseffectiveness,supportingitsutilityinrelevantscenarios.
5.1 HowWellcanDr. DPOResistthePairwiseNoise?
DatasetsandSetup. Weconductexperimentsontwodatasets: IMDB[27]andAnthropicHH[3].
TheIMDBdatasetiswidelyutilizedforsentimentanalysistasks. TheAnthropicHHdatasetconsists
ofapproximately170,000dialoguesbetweenhumansandautomatedassistants. Theobjectivesof
theseexperimentsweretwofold: firstly,toevaluatetherobustnessoftheproposedDr. DPOagainst
pairwise noise; and secondly, to investigate whether Dr. DPO exhibits superior performance on
noise-freedatasets. Toachievethefirstobjective,weintroducerandominversionsbetweenselected
7
draweR draweRTable1: Preferenceaccuracyandwin-ratecomparisonontheAnthropicHHdatasetwithvarious
levelsofnoise. Thecolumnsindicatetheperformanceonboththenoise-freedatasetandthedatasets
withinvertedresponselabels,denotedbytheirrespectiveflipratios. Theperformanceimprovements
ofcDPO,IPO,andDr. DPOoverDPOarealsopresented.
PreferenceAccuracy WinRate
Models
0%Flipped 10%Flipped 20%Flipped 30%Flipped 40%Flipped 0%Flipped 40%Flipped
DPO 63.63 62.27 61.28 58.54 55.23 54.36 49.00
cDPO 63.48‚àí0.23% 62.67+0.64% 61.48+0.32% 58.69+0.27% 55.31+0.15% 51.46‚àí5.33% 45.68‚àí6.78%
IPO 65.95+3.64% 64.82+4.10% 63.52+3.65% 61.45+4.98% 56.64+2.55% 50.81‚àí6.53% 54.82+11.88%
rDPO 64.37+1.16% 62.72+0.72% 62.53+2.04% 60.56+3.45% 57.11+3.40% 52.15‚àí4.06% 53.54+9.26%
Dr.DPO 66.22+4.07% 65.38+5.00% 64.19+4.74% 62.65+7.02% 58.83+6.52% 56.67+4.25% 61.65+25.81%
andrejectedresponsesinthetrainingdataatvaryingnoiselevels‚Äîspecifically,withprobabilities
of 10%, 20%, 30%, and 40%. For the second objective, we benchmark Dr. DPO against other
DPO-relatedbaselinestodiscernitsrelativeadvantages. PleaserefertoAppendixFformoredetails.
Baselines. We compare Dr. DPO with four baseline methods: (i) The standard DPO, which
is the state-of-the-art (SOTA) method for directly optimizing the policy and reward model; (ii)
ConservativeDPO(cDPO[36]),avariantintroducedbytheauthorsofDPOtoaddressscenarioswith
probabilisticallyflippedlabelsbyincorporatingabinarycross-entropy(BCE)loss;and(iii)IPO [2],
aninnovativeapproachthatenableslearningdirectlyfrompreferences,bypassingboththereward
modelingphaseandrelianceontheBTmodel. (iv)rDPO[10]isavariantofDPOthatde-biasesthe
effectofpreferencenoiseandmakesthepolicyrobust.
Weadopttwometrics,PreferenceAccuracy,andWin-Rate,intheexperiments. Theexperimental
resultsandthecorrespondingdiscussionsareshownbelow.
Preference Accuracy. The preference accuracy measures the proportion of test instances from
theAnthropicHHdatasetwherethemodel‚Äôspredictedrewardforthepreferredresponse,r(x,y ),
w
exceedsthatofthelesspreferredalternative,r(x,y ).
l
Table1illustratestheeffectofnoiseonpreferenceaccuracy,showingadecreasefrom63.63%to
55.23%asthelabelflipratiorisesfrom0%to40%. Thisindicatesadeterioratedabilitytodistinguish
betweenresponseswithincreasingnoise. Inanoise-freeenvironment,cDPOachievesapreference
accuracyof63.48%,whichfallsto55.31%ata40%noiselevel,highlightingthelimitedimpactof
BCElossadjustments. Incomparison,theIPOmethodconsistentlyoutperformsDPObyanaverage
of3.78%inaccuracy,provingitsefficacy. rDPOreportsa3%accuracygainoverDPO.Notably,Dr.
DPOexhibitsoutstandingnoiseresistance,achievingthehighestpreferenceaccuraciesof66.22%in
theabsenceofnoiseand58.83%with40%noise,superiortoDPO,cDPO,IPO,andrDPO.
Win-Rate. We further draw on the evaluation framework introduced by Rafailov et al. [35] to
calculatetheWin-Rate. ThismetricassesseshowoftentheGPT-4modelselectsaresponsegenerated
by our model rather than the chosen response within the dataset. The Win-Rate computation is
specificallydesignedforthesingle-turndialogueportionofHHdataset‚Äôstestsubset.
Weassessedresponsequalityacrossmodels‚ÄîDPO,cDPO,IPO,andDr. DPO‚Äîacrossnoiselevels.
Table1indicatesDPO‚Äôsdominanceinanoiselesssettingata56.67%winrate. Withnoise,DPO
and cDPO faltered, whereas Dr. DPO excelled, surpassing both its noise-free performance and
competitorswitha61.65%winrate. ThisunderscoresDr. DPO‚Äôsefficacyinnoisemitigationand
responsequalityenhancement,showcasingstrongnoisyenvironmentadaptability.
5.2 ComparingDr. DPOwithBaselinesonMT-Bench
ToevaluatethegenerationqualityofDPOanditsvariants,weconductpairwisecomparisonsusing
theMT-Benchframework[47]. Thisframework,groundedinGPT-4,reliablyalignswithhuman
evaluativepreferences,exhibitinganagreementrateexceeding80%onthequalityofoutputsfrom
LLMs. AdheringtotheestablishedMT-Benchguidelines[47]4,ourapproachinvolvesgenerating
modelresponsesatacontrolledtemperatureof0.7andrestrictingthetokencounttoamaximumof
4https://github.com/lm-sys/FastChat/blob/main/fastchat/llm_judge/gen_model_answer.
py
80% Flipped 40% Flipped
vc sD DP PO O 8.8% 83.6% 7.5% vc sD DP PO O 5.63% 86.3% 8.13% 0.65 D DP PO O ( (= *)0.1) 0.64
0.60 Dr. DPO (=0.1)
0.62
vs DIP PO O 16.8% 68.8% 14.4% vs DIP PO O 10.3% 76.9% 12.5% 0.55 0.60 0% Flipped
0.50 0.58 1 20 0% % F Fl li ip pp pe ed d
D vr. s D DP PO O 18.2% 70.4% 11.3%D vr. s D DP PO O 20.1% 71.1% 8.8% 0.45 0.56 3 40 0% % F Fl li ip pp pe ed d
0 20 40 60 80 100 0 20 40 60 80 100 0.4 JSD =0.3 =0.5 =0.7 1.0 2.0 3.0 5.0
Win Tie Lose Win Tie Lose Divergence 0
Figure4: MT-Bench,assessingDPOanditsvariants‚ÄôperformancewithGPT-4,presentstheirWin,
Tie, andLossratesat0%and40%pairwisenoiselevelsinthefirsttwofigures. Thethirdfigure
displaysthewinrateofDr.DPOoverdifferentœï-divergences.ThefourthfiguredetailsthePreference
accuracyofDr. DPOforvaryingŒ≤‚Ä≤values.
10% Flipped 20% Flipped 30% Flipped 40% Flipped 0.62 0.62
0.62 0.62
0.58 0.58
0.58 0.58
DPO
0.54 Dr. DPO 0.54 0.54 0.54
0.5 0.5 0.5 0.5
20 40 60 80 100 20 40 60 80 100 20 40 60 80 100 20 40 60 80 100
Training Progress (%) Training Progress (%) Training Progress (%) Training Progress (%)
Figure5: Preferenceaccuracyacrosstrainingstepsfordifferentlevelsofpairwisenoise.
1024. WesystematicallycomparetheoutputsfromthebaseDPOmodelanditsvariants,whichhave
beenfine-tunedwith0%and40%flippedpairsontheHHdataset.
Figure 4 (1,2) shows that Dr. DPO consistently outperforms DPO in both noise-free and noisy
datasets, becoming the only method to exceed DPO‚Äôs performance in the MT-Bench evaluation.
WhileIPOslightlyimprovesoverDPOinthenoise-freedataset,itunderperformsinthenoisydataset
whereDPOprevails. Incontrast,Dr. DPOdemonstratesrobust,significantenhancementsinboth
conditions,highlightingitssuperiorabilitytogeneratehigh-qualityresponses.
5.3 AblationStudiesonDr. DPO
Weconductablationstudiestoinvestigatetheimpactoftheœï-divergenceandŒ≤‚Ä≤ontheperformance
ofDr. DPO,andprovidetheconvergenceanalysis. MoreablationstudiesarelistedinAppendixF.
EvaluatingtheImpactofœï-divergenceonDr. DPO.Figure4(3)exploresDr. DPO‚Äôsperformance
withvariousœï-divergences,includingJensen-Shannon(JS)andŒ±-divergence. Demonstratedresults
indicatethatDr. DPOconsistentlyoutperformsthebaselineDPOwhenŒ≤‚Ä≤ issetto1.0,servingas
aviabledefaultwithoutrequiringfurtheradjustments. ThisisincontrasttothebaselineŒ≤ = 0.1
setting,wherealthoughimprovementscanberealizedbymanuallytuningŒ≤‚àó,theprocessbecomes
time-consumingandimpracticalforregularuse.
EvaluatingtheImpactofŒ≤‚Ä≤. Figure4(4)illustrateshowvaryingŒ≤‚Ä≤ acrossdifferentnoiselevels
affectspreferenceaccuracy. Theexperimentalresultsrevealatrendwhereinincreasednoiselevels
correspond to a reduced optimal value for Œ≤‚Ä≤, which is consistent with our theoretical analysis
providedinSection3. Consequently,weproposeadefaultsettingofŒ≤‚Ä≤ =1.0forbalancingaccuracy
androbustnessinthepresenceofnoise.
ConvergenceAnalysis. Figure5showsthatDr. DPOnotonlyconvergesfasterbutalsosurpasses
DPOintheearlytrainingstage,attributedtoitssuperiormanagementofflippednoisypairs. This
enhancementmeetsourgoalofboostingDPO‚Äôsrobustnessinnoisyenvironments.
6 Conclusion
Inthisstudy,weanalyzeDPO‚ÄôsrobustnessfromaDROperspective,highlightingitsresilienceto
pointwisenoise. WeestablishalinkbetweenDPO‚ÄôsregularizationandDRO‚Äôsrobustness,showing
thatasmallerregularizationparameterŒ≤ enhancesstabilityagainstuncertaindata. Ourexperiments
confirmthecrucialroleofŒ≤ innoiseresistancebutuncoverDPO‚Äôsweaknessagainstpairwisenoise.
Toaddressthis,weintroduceanovelDistributionallyRobustifyingDPO(Dr. DPO)frameworkwith
9
ycaruccA
ecnereferP
etaR
niW
ycaruccA
dcnereferPanadditionalparameterŒ≤‚Ä≤thatbalancesdatapairimportanceintrainingtoenhancemodelrobustness.
TheDr. DPO‚Äôsfine-tuningofexplorationandexploitationcouldmarkedlyimprovethealignmentof
languagemodels,assuringreliableperformanceinthepresenceofreal-worldnoise.
References
[1] R.Anil,S.Borgeaud,Y.Wu,J.Alayrac,J.Yu,R.Soricut,J.Schalkwyk,A.M.Dai,A.Hauth,
K.Millican,D.Silver,S.Petrov,M.Johnson,I.Antonoglou,J.Schrittwieser,A.Glaese,J.Chen,
E.Pitler,T.P.Lillicrap,A.Lazaridou,O.Firat,J.Molloy,M.Isard,P.R.Barham,T.Hennigan,
B. Lee, F. Viola, M. Reynolds, Y. Xu, R. Doherty, E. Collins, C. Meyer, E. Rutherford,
E. Moreira, K. Ayoub, M. Goel, G. Tucker, E. Piqueras, M. Krikun, I. Barr, N. Savinov,
I. Danihelka, B. Roelofs, A. White, A. Andreassen, T. von Glehn, L. Yagati, M. Kazemi,
L. Gonzalez, M. Khalman, J. Sygnowski, and et al. Gemini: A family of highly capable
multimodalmodels. CoRR,abs/2312.11805,2023.
[2] M.G.Azar,M.Rowland,B.Piot,D.Guo,D.Calandriello,M.Valko,andR.Munos. Ageneral
theoreticalparadigmtounderstandlearningfromhumanpreferences. CoRR,abs/2310.12036,
2023.
[3] Y.Bai,A.Jones,K.Ndousse,A.Askell,A.Chen,N.DasSarma,D.Drain,S.Fort,D.Ganguli,
T.Henighan,N.Joseph,S.Kadavath,J.Kernion,T.Conerly,S.E.Showk,N.Elhage,Z.Hatfield-
Dodds, D. Hernandez, T. Hume, S. Johnston, S. Kravec, L. Lovitt, N. Nanda, C. Olsson,
D.Amodei,T.B.Brown,J.Clark,S.McCandlish,C.Olah,B.Mann,andJ.Kaplan. Training
a helpful and harmless assistant with reinforcement learning from humanfeedback. CoRR,
abs/2204.05862,2022.
[4] A. Ben-Tal and M. Teboulle. An old-new concept of convex risk measures: the optimized
certaintyequivalent. MathematicalFinance,2007.
[5] S.Biderman,H.Schoelkopf,Q.G.Anthony,H.Bradley,K.O‚ÄôBrien,E.Hallahan,M.A.Khan,
S.Purohit,U.S.Prashanth,E.Raff,A.Skowron,L.Sutawika,andO.vanderWal. Pythia: A
suiteforanalyzinglargelanguagemodelsacrosstrainingandscaling. InICML,2023.
[6] S.P.BoydandL.Vandenberghe. Convexoptimization. Cambridgeuniversitypress,2004.
[7] R.A.BradleyandM.E.Terry. Rankanalysisofincompleteblockdesigns: I.themethodof
pairedcomparisons. Biometrika,39(3/4):324‚Äì345,1952.
[8] S.Bubeck,V.Chandrasekaran,R.Eldan,J.Gehrke,E.Horvitz,E.Kamar,P.Lee,Y.T.Lee,
Y.Li,S.M.Lundberg,H.Nori,H.Palangi,M.T.Ribeiro,andY.Zhang. Sparksofartificial
generalintelligence: EarlyexperimentswithGPT-4. CoRR,abs/2303.12712,2023.
[9] X. Chen, S. He, B. Jiang, C. T. Ryan, and T. Zhang. The discrete moment problem with
nonconvexshapeconstraints. Oper.Res.,69(1):279‚Äì296,2021.
[10] S.R.Chowdhury,A.Kini,andN.Natarajan. ProvablyrobustDPO:aligninglanguagemodels
withnoisyfeedback. InICML,2024.
[11] P.F.Christiano,J.Leike,T.B.Brown,M.Martic,S.Legg,andD.Amodei. Deepreinforcement
learningfromhumanpreferences. InNeurIPS,2017.
[12] G.Cui,L.Yuan,N.Ding,G.Yao,W.Zhu,Y.Ni,G.Xie,Z.Liu,andM.Sun. Ultrafeedback:
Boostinglanguagemodelswithhigh-qualityfeedback. CoRR,abs/2310.01377,2023.
[13] H. Dong, W. Xiong, D. Goyal, R. Pan, S. Diao, J. Zhang, K. Shum, and T. Zhang. RAFT:
rewardrankedfinetuningforgenerativefoundationmodelalignment. CoRR,abs/2304.06767,
2023.
[14] J.C.DuchiandH.Namkoong. Learningmodelswithuniformperformanceviadistributionally
robustoptimization. CoRR,abs/1810.08750,2018.
[15] L. Faury, U. Tanielian, E. Dohmatob, E. Smirnova, and F. Vasile. Distributionally robust
counterfactualriskminimization. InAAAI,2020.
[16] S. Gunasekar, Y. Zhang, J. Aneja, C. C. T. Mendes, A. D. Giorno, S. Gopi, M. Javaheripi,
P.Kauffmann,G.deRosa,O.Saarikivi,A.Salim,S.Shah,H.S.Behl,X.Wang,S.Bubeck,
R.Eldan,A.T.Kalai,Y.T.Lee,andY.Li. Textbooksareallyouneed. CoRR,abs/2306.11644,
2023.
10[17] J.Hartmann,M.Heitmann,C.Siebert,andC.Schamp. Morethanafeeling: Accuracyand
applicationofsentimentanalysis. InternationalJournalofResearchinMarketing,2023.
[18] A. Havrilla, M. Zhuravinskyi, D. Phung, A. Tiwari, J. Tow, S. Biderman, Q. Anthony, and
L.Castricato. trlx: Aframeworkforlargescalereinforcementlearningfromhumanfeedback.
InEMNLP,2023.
[19] J.-B.Hiriart-UrrutyandC.Lemar√©chal. Fundamentalsofconvexanalysis. SpringerScience&
BusinessMedia,2004.
[20] R. Huang, J. Huang, W. Liu, and H. Ding. Coresets for wasserstein distributionally robust
optimizationproblems. InNeurIPS,2022.
[21] H.Ivison,Y.Wang,V.Pyatkin,N.Lambert,M.Peters,P.Dasigi,J.Jang,D.Wadden,N.A.
Smith,I.Beltagy,andH.Hajishirzi. Camelsinachangingclimate: EnhancingLMadaptation
withtulu2. CoRR,abs/2311.10702,2023.
[22] B.Jin,D.Lian,Z.Liu,Q.Liu,J.Ma,X.Xie,andE.Chen. Sampling-decomposablegenerative
adversarialrecommender. NeurIPS,2020.
[23] H.Lam,Z.Liu,andX.Zhang. Orthounimodaldistributionallyrobustoptimization: Representa-
tion,computationandmultivariateextremeeventapplications.arXivpreprintarXiv:2111.07894,
2021.
[24] D.Lian,Q.Liu,andE.Chen. Personalizedrankingwithimportancesampling. InWWW,2020.
[25] T.Liu, Y.Zhao, R.Joshi, M.Khalman, M.Saleh, P.J.Liu, andJ.Liu. Statisticalrejection
samplingimprovespreferenceoptimization. CoRR,abs/2309.06657,2023.
[26] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer,
and V. Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint
arXiv:1907.11692,2019.
[27] A.L.Maas,R.E.Daly,P.T.Pham,D.Huang,A.Y.Ng,andC.Potts. Learningwordvectors
forsentimentanalysis. InACL,2011.
[28] P.Michel,T.Hashimoto,andG.Neubig. Modelingthesecondplayerindistributionallyrobust
optimization. InICLR,2021.
[29] P. Michel, T. Hashimoto, and G. Neubig. Distributionally robust models with parametric
likelihoodratios. InICLR,2022.
[30] H. Namkoong and J. C. Duchi. Variance-based regularization with convex objectives. In
NeurIPS,2017.
[31] X.Nguyen,M.J.Wainwright,andM.I.Jordan. Estimatingdivergencefunctionalsandthe
likelihoodratiobyconvexriskminimization. IEEETrans.Inf.Theory,2010.
[32] OpenAI. GPT-4technicalreport. CoRR,abs/2303.08774,2023.
[33] L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.L.Wainwright,P.Mishkin,C.Zhang,S.Agarwal,
K.Slama,A.Ray,J.Schulman,J.Hilton,F.Kelton,L.Miller,M.Simens,A.Askell,P.Welinder,
P.F.Christiano,J.Leike,andR.Lowe. Traininglanguagemodelstofollowinstructionswith
humanfeedback. InNeurIPS,2022.
[34] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever. Language models are
unsupervisedmultitasklearners. 2019.
[35] R.Rafailov,A.Sharma,E.Mitchell,C.D.Manning,S.Ermon,andC.Finn. Directpreference
optimization: Yourlanguagemodelissecretlyarewardmodel. InNeurIPS,2023.
[36] R.Rafailov,A.Sharma,E.Mitchell,C.D.Manning,S.Ermon,andC.Finn. Anoteondpo
withnoisypreferencesandrelationshiptoipo. 2023. URLericmitchell.ai/cdpo.pdf.
[37] J.Schulman,F.Wolski,P.Dhariwal,A.Radford,andO.Klimov. Proximalpolicyoptimization
algorithms. CoRR,abs/1707.06347,2017.
[38] S.Shafieezadeh-Abadeh,P.M.Esfahani,andD.Kuhn.Distributionallyrobustlogisticregression.
InNeurIPS,2015.
[39] M.Sharma,M.Tong,T.Korbak,D.Duvenaud,A.Askell,S.R.Bowman,N.Cheng,E.Dur-
mus,Z.Hatfield-Dodds,S.R.Johnston,S.Kravec,T.Maxwell,S.McCandlish,K.Ndousse,
O.Rausch,N.Schiefer,D.Yan,M.Zhang,andE.Perez. Towardsunderstandingsycophancyin
languagemodels. CoRR,abs/2310.13548,2023.
11[40] A. Sinha, H. Namkoong, and J. C. Duchi. Certifying some distributional robustness with
principledadversarialtraining. InICLR,2018.
[41] H. Touvron, L. Martin, K. Stone, P. Albert, A. Almahairi, Y. Babaei, N. Bashlykov, S. Ba-
tra, P.Bhargava, S.Bhosale, D.Bikel, L.Blecher, C.Canton-Ferrer, M.Chen, G.Cucurull,
D.Esiobu,J.Fernandes,J.Fu,W.Fu,B.Fuller,C.Gao,V.Goswami,N.Goyal,A.Hartshorn,
S.Hosseini,R.Hou,H.Inan,M.Kardas,V.Kerkez,M.Khabsa,I.Kloumann,A.Korenev,P.S.
Koura,M.Lachaux,T.Lavril,J.Lee,D.Liskovich,Y.Lu,Y.Mao,X.Martinet,T.Mihaylov,
P.Mishra,I.Molybog,Y.Nie,A.Poulton,J.Reizenstein,R.Rungta,K.Saladi,A.Schelten,
R.Silva,E.M.Smith,R.Subramanian,X.E.Tan,B.Tang,R.Taylor,A.Williams,J.X.Kuan,
P.Xu,Z.Yan,I.Zarov,Y.Zhang,A.Fan,M.Kambadur,S.Narang,A.Rodriguez,R.Stojnic,
S.Edunov, andT.Scialom. Llama2: Openfoundationandfine-tunedchatmodels. CoRR,
abs/2307.09288,2023.
[42] C. Wang, Y. Jiang, C. Yang, H. Liu, and Y. Chen. Beyond reverse KL: generalizing direct
preferenceoptimizationwithdiversedivergenceconstraints. ICLR,2024.
[43] C. Wu, D. Lian, Y. Ge, Z. Zhu, and E. Chen. Influence-driven data poisoning for robust
recommendersystems. TPAMI,2023.
[44] Z.Yuan,H.Yuan,C.Tan,W.Wang,S.Huang,andF.Huang. RRHF:rankresponsestoalign
languagemodelswithhumanfeedbackwithouttears. InNeurIPS,2023.
[45] R. Zhai, C. Dan, J. Z. Kolter, and P. Ravikumar. DORO: distributional and outlier robust
optimization. InICML,2021.
[46] Y.Zhao,R.Joshi,T.Liu,M.Khalman,M.Saleh,andP.J.Liu. Slic-hf: Sequencelikelihood
calibrationwithhumanfeedback. CoRR,abs/2305.10425,2023.
[47] L.Zheng,W.Chiang,Y.Sheng,S.Zhuang,Z.Wu,Y.Zhuang,Z.Lin,Z.Li,D.Li,E.P.Xing,
H.Zhang,J.E.Gonzalez,andI.Stoica. Judgingllm-as-a-judgewithmt-benchandchatbot
arena. CoRR,abs/2306.05685,2023.
[48] D.Zhu,Y.Ying,andT.Yang. Labeldistributionallyrobustlossesformulti-classclassification:
Consistency,robustnessandadaptivity. InICML,2023.
12A RelatedWork
ReinforcementLearningfromHumanFeedback. RLHF[11,3,41,33]hasemergedasakey
methodforaligninglanguagemodelswithhumanvaluesandpreferences,mitigatingthegeneration
ofbiasedorfactuallyincorrectoutputs. Comparedtosupervisedlearning,RLHFisrathercomplex,
lessstable,andrequiresmorememoryresources. Thesechallengeshavemotivatedthedevelopment
ofalternativestotheRLHFpipeline. Forexample, RAFT[13]usesanexistingrewardmodelto
selectthebestsetoftrainingsamplesbasedonthemodeloutputs,whileRRHF[44]leveragesamuch
simplerrankinglosstoalignhumanpreferencesandretaintheperformanceofPPO.DPO[35]is
anotheralternativetoRLHFthatusesapreferencelossfunctiontodirectlyoptimizetheLLMs,and
hasbeenshowntobemorestableandlesscomputationallyintensivethanRLHF.Despitetheseefforts,
allthemethodsignorethenoiseinthetrainingdata,whichcanleadtosuboptimalperformance.
DistributionallyRobustOptimization. DROdiffersfromtraditionalrobustoptimizationmethods
[22, 24, 43] by minimizing the worst-case error within an uncertainty set defined by constraints
likeœï-divergence[30,14],Wassersteindistance[38,40,20],andshape[23,9]. [28,29]introduced
parametrizationtotheuncertaintysetforgreaterarchitecturalflexibility. Separately,[45]addressed
sensitivitytooutliersinDRO,divergingfromtheotherstudies.
B Limitations
ThecurrentworkintroducesDr. DPO,anenhancementtoDirectPreferenceOptimization(DPO)that
addresseslabelflippingnoiseintrainingdatasetsthroughanadditionalhyperparameterŒ≤‚Ä≤. Despite
therobustperformanceindicatedbyempiricalresultswithadefaultŒ≤‚Ä≤ valueof1.0,theneedfor
parametertuningindifferentapplicationsremains. ThesensitivityofŒ≤‚Ä≤ todataandtaskspecifics
maynecessitateasearchprocesstofullyleverageDr. DPO‚Äôspotential.
Moreover, the experimental foundation of this study is built upon a 2.8B model. The scalability
and generalization of Dr. DPO on larger models, such as 7B or greater, have not been explored.
Understanding Dr. DPO‚Äôs effectiveness across various model sizes is a critical aspect for future
investigation.
C BroaderImpacts
ThispaperpresentsworkwhosegoalistoadvancethefieldofMachineLearning. Therearemany
potentialsocietalconsequencesofourwork,noneofwhichwefeelmustbespecificallyhighlighted
here.
13D AppendixofProofs
D.1 ProofofTheorem3.1
Thm3.1(OptimalRewardFunctionunderKLDivergence). LettheKullback-Leibler(KL)divergence
(cid:16) (cid:17)
betweenpolicyœÄ andreferencepolicyœÄ bedefinedas: D (œÄŒ∏|œÄ )=(cid:82) œÄ (x)log œÄŒ∏(x) dx.
Œ∏ ref KL ref Œ∏ œÄref(x)
Optimizing the RM-DRO objective as defined in Equation (7) yields an optimal reward r (x,y)
KL
givenby:
œÄ (y|x)
r (x,y)=Œ≤‚àó(Œ∑)log Œ∏ ‚àíŒ±. (8)
KL œÄ
ref(y|x)
Here,Œ±,Œ≤ areLagrangemultipliers,Œ≤‚àó(Œ∑)denotestheoptimalvalueofŒ≤ thatminimizesEquation
(7),actingastheregularizationcoefficientinDPO.ByderivingtheoptimalvalueofŒ±,givenby:
r (y|x)
Œ±‚àó =‚àíŒ≤logE [exp( Œ∏ )], (9)
x‚àºO,y‚àºœÄref Œ≤
Equation(8)canbere-expressedtomatchtheultimateformoftherewardfunctioninEquation(4).
Proof.
DefinitionD.1(œï-divergence[31]). Foranyconvexfunctionœïwithœï(1) = 0,theœï-divergence
betweenQandQ is:
0
œÄ
D (œÄ ,œÄ ):=E [œï( Œ∏ )] (16)
œï Œ∏ ref œÄref œÄ
ref
where D (Q,Q ) = ‚àû if Q is not absolutely continuous with respect to Q . Specially, when
œï 0 0
œï(x)=xlogx‚àíx+1,œï-divergencedegeneratestothewell-knownKLdivergence.
DefinitionD.2(Convexconjugate[19]). Weconsiderapair(A,B)oftopologicalvectorspacesand
abilinearform‚ü®¬∑,¬∑‚ü©‚ÜíRsuchthat(A,B,‚ü®¬∑,¬∑‚ü©)formadualpair. Foraconvexfunctionf :R‚ÜíR,
domf :={x‚ààR:f(x)<‚àû}istheeffectivedomainoff. Theconvexconjugate,alsoknownas
theLegendre-Fencheltransform,off :A‚ÜíRisthefunctionf‚àó :B ‚ÜíRdefinedas
f‚àó(b)=sup{ab‚àíf(a)}, b‚ààB (17)
a
Thm D.3 (Interchange of minimization and integration [4]). Let (‚Ñ¶,F) be a measurable space
equippedwithœÉ-algebraF,Lp(‚Ñ¶,F,P)bethelinearspaceofmeasurablerealvaluedfunctions
f : ‚Ñ¶ ‚Üí Rwith||f|| < ‚àû,andletX := Lp(‚Ñ¶,F,P),p ‚àà [1,+‚àû]. Letg : R√ó‚Ñ¶ ‚Üí Rbea
p
normalintegrand,anddefineonX. Then,
(cid:90) (cid:90)
min g(x(œâ),œâ)dP(œâ)= ming(s,œâ)dP(œâ) (18)
x‚ààX ‚Ñ¶ ‚Ñ¶ s‚ààR
Toeasethederivation, wedenotethelikelihoodratioL(y|x) = œÄ (y|x)/œÄ (y|x). Notethatthe
Œ∏ ref
œï-divergencebetweenœÄ andœÄ isconstrained,andthusL(.)iswell-defined.Forbrevity,weusually
Œ∏ ref
shortL(y|x)asL. AndintermsofDefinitionD.1ofœï-divergence,theexpressionofRM-DRO(cf.
Equation(7))becomes:
Lœï =maxE [r (y|x)L] s.t. E [œï(L(y|x))]‚â§Œ∑ (19)
RM-DRO
L
x‚àºD,y‚àºœÄref Œ∏ œÄref
NotethatE [r (y|x)L]andE [œï(L(y|x))]arebothconvexinL. WeusetheLagrangianfunction
œÄref Œ∏ œÄref
solver:
Lœï = min max{E [r (y|x)L(y|x)]‚àíŒ≤[E [œï(L(y|x))]‚àíŒ∑]+Œ±(E [L(y|x)]‚àí1)}
RM-DRO
Œ≤‚â•0,Œ±L(y|x)
x‚àºD,y‚àºœÄref Œ∏ œÄref œÄref
= min (cid:8) Œ≤Œ∑‚àíŒ±+Œ≤ max{E [r Œ∏(y|x)+Œ± L(y|x)‚àíœï(L(y|x))]}(cid:9)
Œ±‚â•0,Œ≤ L(y|x) x‚àºD,y‚àºœÄref Œ≤
= min (cid:8) Œ≤Œ∑‚àíŒ±+Œ≤E [max{r Œ∏(y|x)+Œ± L(y|x)‚àíœï(L(y|x))}](cid:9)
Œ≤‚â•0,Œ± x‚àºD,y‚àºœÄref L(y|x) Œ≤
= min (cid:8) Œ≤Œ∑‚àíŒ±+Œ≤E [œï‚àó(r Œ∏(y|x)+Œ± )](cid:9)
Œ≤‚â•0,Œ± x‚àºD,y‚àºœÄref Œ≤
(20)
14The first equality holds due to the strong duality [6]. The second equality is a re-arrangement
for optimizing L(y|x). The third equation follows by the Theorem D.3. The last equality is
establishedbasedonthedefinitionofconvexconjugateD.2. WhenwechooseKL-divergence,we
haveœï (x)=xlogx‚àíx+1. Itcanbededucedthatœï‚àó (x)=ex‚àí1. Then,wehave:
KL KL
LKL = min (cid:8) Œ≤Œ∑‚àíŒ±+Œ≤E [œï‚àó(r Œ∏(y|x)+Œ± )](cid:9)
RM-DRO Œ≤‚â•0,Œ± x‚àºD,y‚àºœÄref Œ≤
(21)
= min (cid:8) Œ≤Œ∑‚àíŒ±+Œ≤E [exp(r Œ∏(y|x)+Œ± )‚àí1](cid:9)
Œ≤‚â•0,Œ± x‚àºD,y‚àºœÄref Œ≤
andthemaximumoftermL(y|x)inEquation(20)isachievedwhen
r (y|x)+Œ±
L(y|x)=exp( Œ∏ ). (22)
Œ≤
WedifferentiatetheLagrangianfunctionw.r.t. Œ±andsetittozero:
‚àÇ (cid:8) Œ≤Œ∑‚àíŒ±+Œ≤E [exp(r Œ∏(y|x)+Œ± )‚àí1](cid:9) =0 (23)
‚àÇŒ± x‚àºD,y‚àºœÄref Œ≤
Then,wehave:
r (y|x)
Œ±‚àó =‚àíŒ≤logE [exp( Œ∏ )] (24)
x‚àºD,y‚àºœÄref Œ≤
SubstitutingtheoptimalŒ±‚àóintotheLagrangianfunction(21),wehave:
LKL = min (cid:8) Œ≤Œ∑‚àíŒ±+Œ≤E [exp(r Œ∏(y|x)+Œ± )‚àí1](cid:9)
RM-DRO Œ≤‚â•0,Œ± x‚àºD,y‚àºœÄref Œ≤
=min(cid:8) Œ≤Œ∑+Œ≤logE [exp(r Œ∏(y|x) )](cid:9) (25)
Œ≤‚â•0 x‚àºD,y‚àºœÄref Œ≤
r (x,y)
=Œ≤‚àó(Œ∑)logE [exp( DPO )]+C,
x‚àºD,y‚àºœÄref Œ≤‚àó(Œ∑)
whereŒ≤‚àó(Œ∑)signifiestheoptimalvalueofŒ≤ thatminimizestheLagrangianfunctionandC =Œ≤Œ∑.
Besides,ifweplugtheoptimalŒ±‚àóintotheoptimalL(y|x),wehave:
r (y|x)+Œ±‚àó
L‚àó(y|x)=exp( Œ∏ )
Œ≤‚àó
(26)
r (y|x) 1
=exp( Œ∏ )
Œ≤‚àó Z(x)
whereZ(x)=E [exp(rŒ∏(y|x))]. Here,werearrangeEquation(26)andobtaintheexpres-
x‚àºD,y‚àºœÄref Œ≤
sionofr (x,y):
KL
œÄ
r (x,y)=Œ≤‚àólogL‚àó(y|x)+Œ≤logZ(x)=Œ≤‚àólog Œ∏ +Œ≤logZ(x) (27)
KL œÄ
ref
Thetheoremisproven.IncomparisontotheproofspresentedinDPO[35],ourproofiscomprehensive
and direct, applicable to any œï-divergence constraints in the general PPO objective. DPO [35]
represents a specific case that employs strategies to construct an objective in the form of KL-
divergence.
15D.2 ProofofTheorem4.1
Thm4.1. ConsiderthescenariowheretheKLdivergenceisemployedtomeasurethediscrepancy
betweenthehypotheticaldistributionO‚Ä≤ anddatasetdistributionO , wederivetheultimateloss
functionforDr. DPOasfollows:
h (x,y ,y )
L (œÄ ;œÄ )=‚àíŒ≤‚Ä≤logE [exp( DPO w l )]. (12)
Dr.DPO Œ∏ ref O Œ≤‚Ä≤
whereh representsthelog-likelihoodintheDPOframework,definedas:
DPO
œÄ (y |x) œÄ (y |x)
h (x,y ,y)=logœÉ(Œ≤log Œ∏ w ‚àíŒ≤log Œ∏ l ), (13)
DPO w l œÄ (y |x) œÄ (y |x)
ref w ref l
withŒ≤ andŒ≤‚Ä≤beingregularizationcoefficientrespectively.
Proof. TosolvetheoptimizationprobleminEquation(11),wefirstintroducetheLagrangianfunction:
O(O‚Ä≤,Œ≤‚Ä≤,Œ±‚Ä≤)=E [h(x,y ,y )]
(x,yw,yl)‚àºO‚Ä≤ w l
O‚Ä≤ (28)
+Œ≤‚Ä≤(D (O‚Ä≤,O)‚àíŒ∑‚Ä≤)+Œ±‚Ä≤(E [ ]‚àí1).
œï O O
Then,wecanobtaintheoptimaldistributionO‚Ä≤,‚àóbysolvingthefollowingsaddle-pointproblem:
O‚Ä≤,‚àó =argmaxminO(Q,Œ≤‚Ä≤,Œ±‚Ä≤).
(29)
O‚Ä≤ Œ≤‚Ä≤,Œ±‚Ä≤
Specifically,whentheKLdivergenceisselectedasthemeasureofœï-divergence,thatis,KL(O‚Ä≤,O)=
(cid:80)N O‚Ä≤log(O‚Ä≤/O ),theoptimaldistributionO‚Ä≤,‚àó canbederivedasfollows:
i=1 i i i KL
(cid:18) (cid:19)
1 h(x,y ,y )
O‚Ä≤,‚àó = exp w l , (30)
KL Z‚àó Œ≤‚Ä≤
whereZ‚àó =E [exp(h(x,y ,y )/Œ≤‚Ä≤)]denotesthepartitionfunction. Inthiscase,wecan
(x,yw,yl)‚àºO w l
deriveaclosed-formexpressionoftheultimateobjectiveO(O‚Ä≤,‚àó,Œª‚Ä≤)asfollows:
KL
h(x,y ,y )
O(O‚Ä≤,‚àó,Œ≤‚Ä≤)=Œ≤‚Ä≤logE [exp( w l )] (31)
KL O Œ≤‚Ä≤
InordertoattainaDistributionallyRobustifyingDROobjectivethatencompassesbothpointwise
andpairwiserobustness,weconsiderthepreviouslyestablishedfactthattheDPOapproachconfers
pointwise robustness. Consequently, by substituting the term h(x,y ,y ) in Equation (31) with
w l
theDPOobjectivefromEquation(5),wecanderiveacomprehensiveobjectivethatintegratesthe
strengthsofbothmethods:
L (œÄ ;œÄ )=‚àíŒ≤‚Ä≤logE [exp(h (x,y ,y )/Œ≤‚Ä≤)]. (32)
Dr.DPO Œ∏ ref O DPO w l
Hereh =logœÉ(Œ≤log œÄŒ∏(yw|x) ‚àíŒ≤log œÄŒ∏(yl|x))denotestheoptimalpolicyusinginDPO.
DPO œÄref(yw|x) œÄref(yl|x)
16D.3 ProofofTheorem4.2
Thm4.2(UpperBoundforDr. DPO). Leth ‚àà[a,b]andLN representstheDr. DPOlosson
DPO Dr.DPO
N samples. GivenahypotheticaldistributionO‚Ä≤satisfyingD (O‚Ä≤,O)‚â§Œ∑‚Ä≤todatasetdistribution
KL
O,wehavethatwithprobabilityatleast1‚àíŒ¥:
L ‚â§LN +B(Œ¥,N,Œ≤‚Ä≤), (14)
O‚Ä≤ Dr.DPO
where:
(cid:114)
2bexp((b‚àía)/Œ≤‚Ä≤) N 1
B(Œ¥,N,Œ≤‚Ä≤)= ln . (15)
N ‚àí1+exp((b‚àía)/Œ≤‚Ä≤) 2 Œ¥
Proof. Firstly,weassumethattheoptimalpolicyO‚Ä≤satisfiesthefollowingconstraint:
O‚Ä≤ ‚àà{Q|D (O‚Ä≤,O)‚â§Œ∑‚Ä≤} (33)
KL
Underthisassumption,thelossfunctionL canbeboundedas:
O‚Ä≤
L =E [h(x,y ,y )]
O‚Ä≤ O‚Ä≤ w l
‚â§ max E [h(x,y ,y )]
O‚Ä≤ w l
D KL(O‚Ä≤,O)‚â§Œ∑‚Ä≤
(cid:20) (cid:18) (cid:19)(cid:21) (34)
h(x,y ,y )
=Œ≤‚Ä≤logE exp w l
O Œ≤‚Ä≤
=L .
Dr.DPO
WenowintroduceMcDiarmid‚Äôsinequalityasafoundationalresult:
ThmD.4(McDiarmid‚ÄôsInequality). LetX ,...,X ‚ààXN beasetofN ‚â•1independentrandom
1 N
variablesandassumethatthereexistsc ,...,c >0suchthatf :XN ‚ÜíRsatisfies:
1 N
|f(x ,...,x ,...,x )‚àíf(x ,...,x‚Ä≤,...,x )|‚â§c . (35)
1 i N 1 i N i
Foralli‚àà1,2,...N andanypointsx ,...x ,x‚Ä≤ ‚ààX. Letf(S)denotef(X ,...,X ),thenforall
1 N i 1 N
œµ>0,thefollowinginequalitieshold:
(cid:32) (cid:33)
‚àí2œµ2
P[f(S)‚àíE{f(S)}‚â•œµ]‚â§exp . (36)
(cid:80)N c2
i=1 i
GivenadatasetwithN samples,foranypairofsamples: (x,y ,y ),(x‚Ä≤,y‚Ä≤ ,y‚Ä≤),wehave:
w l w l
|w(x,y ,y )h((x,y ,y ))‚àíw(x‚Ä≤,y‚Ä≤ ,y‚Ä≤)h((x‚Ä≤,y‚Ä≤ ,y‚Ä≤))|
w l w l w l w l
‚â§2sup|w(x,y ,y )h(x,y ,y )|
w l w l
O (37)
2bexp((b‚àía)/Œ≤‚Ä≤)
‚â§ ,
N ‚àí1+exp((b‚àía)/Œ≤‚Ä≤)
wherethesecondinequalityholdsasw(x,y ,y )=
exp(h(x,yw,yl)/Œ≤‚Ä≤)
andh ‚àà[a,b].
w l E O[exp(h(x,yw,yl)/Œ≤‚Ä≤)] DPO
ByapplyingMcDiarmid‚Äôsinequality,weobtain:
P(cid:0) L ‚àíLN ‚â•Œµ(cid:1) ‚â§exp(cid:32) ‚àí2Œµ2 (cid:18) N ‚àí1+exp((b‚àía)/Œ≤‚Ä≤)(cid:19)2(cid:33) . (38)
Dr.DPO Dr.DPO N 2bexp((b‚àía)/Œ≤‚Ä≤)
Setting:
(cid:32) 2Œµ2 (cid:18) N ‚àí1+exp((b‚àía)/Œ≤‚Ä≤)(cid:19)2(cid:33)
Œ¥ =exp ‚àí , (39)
N 2bexp((b‚àía)/Œ≤‚Ä≤)
wecansolveforŒµas:
(cid:114)
2bexp((b‚àía)/Œ≤‚Ä≤) N 1
Œµ= ln . (40)
N ‚àí1+exp((b‚àía)/Œ≤‚Ä≤) 2 Œ¥
Thus,foranyŒ¥ ‚àà(0,1),weconcludethatwithprobabilityatleast1‚àíŒ¥:
(cid:114)
2bexp((b‚àía)/Œ≤‚Ä≤) N 1
L ‚â§L ‚â§LN + ln . (41)
O‚Ä≤ Dr.DPO Dr.DPO N ‚àí1+exp((b‚àía)/Œ≤‚Ä≤) 2 Œ¥
17D.4 Proofofw(x,y ,y )
w l
Inthissection,wepresentthederivationofthegradientfortheDr. DPOobjectivefunctionasfollows:
(cid:20) (cid:18) (cid:19)(cid:21)
h (x,y ,y )
‚àá L (œÄ ;œÄ )=‚àí‚àá Œ≤‚Ä≤logE exp DPO w l . (42)
Œ∏ Dr.DPO Œ∏ ref Œ∏ O Œ≤‚Ä≤
Theright-handsideofEquation(42)canberewrittenas:
(cid:20) (cid:18) (cid:19)(cid:21)
h (x,y ,y )
‚àá Œ≤‚Ä≤logE exp DPO w l
Œ∏ O Œ≤‚Ä≤
(43)
(cid:20) (cid:20) (cid:18) (cid:19)(cid:21)(cid:21)
h (x,y ,y )
=‚àá Œ≤‚Ä≤logE exp DPO w l ‚àá h .
hDPO(x,yw,yl) O Œ≤‚Ä≤ Œ∏ DPO
Consideringthegradientwithrespecttothefunctionh (x,y ,y )yields:
DPO w l
(cid:16) (cid:17)
(cid:20) (cid:20) (cid:18) (cid:19)(cid:21)(cid:21) exp hDPO(x,yw,yl)
‚àá Œ≤‚Ä≤logE exp h DPO(x,y w,y l) = Œ≤‚Ä≤ . (44)
hDPO(x,yw,yl) O Œ≤‚Ä≤ E (cid:104) exp(cid:16) hDPO(x,yw,yl)(cid:17)(cid:105)
O Œ≤‚Ä≤
FocusingonthegradientwithrespecttoŒ∏,wehave:
(cid:18) œÄ (y |x) œÄ (y |x) (cid:19) œÉ‚Ä≤(u)
‚àá h =‚àá logœÉ Œ≤log Œ∏ w ‚àíŒ≤log Œ∏ l = ‚àá u, (45)
Œ∏ DPO Œ∏ œÄ (y |x) œÄ (y |x) œÉ(u) Œ∏
ref w ref l
whereu=Œ≤log œÄŒ∏(yw|x) ‚àíŒ≤log œÄŒ∏(yl|x). Leveragingthepropertiesofthesigmoidfunction,where
œÄref(yw|x) œÄref(yl|x)
œÉ‚Ä≤(x)=œÉ(x)(1‚àíœÉ(x))andœÉ‚Ä≤(‚àíx)=1‚àíœÉ(x),wederivethefinalgradientexpression:
(cid:20) (cid:18) (cid:19)(cid:21)
h (x,y ,y )
‚àí‚àá Œ≤‚Ä≤logE exp DPO w l
Œ∏ O Œ≤‚Ä≤
(cid:16) (cid:17)
exp hDPO(x,yw,yl) (cid:20) (cid:18) (cid:19) (cid:21)
=‚àí
Œ≤‚Ä≤
Œ≤œÉ
Œ≤logœÄ Œ∏(y l|x)
(y |x) ‚àíŒ≤log
œÄ Œ∏(y w|x) (46)
E (cid:104) exp(cid:16) hDPO(x,yw,yl)(cid:17)(cid:105) œÄ ref l œÄ ref(y w|x)
O Œ≤‚Ä≤
¬∑[‚àá logœÄ (y |x)‚àí‚àá logœÄ (y |x)].
Œ∏ Œ∏ w Œ∏ Œ∏ l
Here,acrucialindicatorthatdistinguishesDr. DPOfromtraditionalDPOisencapsulatedbythe
weightterm:
(cid:16) (cid:17)
exp hDPO(x,yw,yl)
Œ≤‚Ä≤
w(x,y ,y )= , (47)
w l (cid:104) (cid:16) (cid:17)(cid:105)
E exp hDPO(x,yw,yl)
O Œ≤‚Ä≤
whichgravitatestowardsauniformdistributionastheparameterŒ≤‚Ä≤ approachesinfinity. Insucha
scenario,thegradientofDr. DPOalignswiththatofthestandardDPO.Thisrelationshipfurnishesa
deeperinsightintohowDr.DPOcanbelinkedanddifferentiatedfromDPOthroughtheincorporation
of a dynamic tuning parameter Œ≤‚Ä≤, enhancing the adaptability of policy optimization in varied
environments.
18E Analysis
E.1 Analysisaboutgeneralœï-divergence.
LemmaE.1(OptimalRewardFunctionunderGeneralœï-Divergence). [42,Theorem1]Givena
œï-divergenceD withcorrespondingderivativeœï‚Ä≤,theoptimalrewardfunctionr (x,y)underthe
œï œï
RM-DROframeworkisdefinedby:
œÄ (y|x)
r (x,y)=Œ≤‚àó(Œ∑)œï‚Ä≤( Œ∏ )‚àíŒ±, (48)
œï œÄ
ref(y|x)
whereŒ±isLagrangemultiplier.
Proof. Todeterminetheoptimalexpressionforr (x,y),onemustidentifytheoptimalL(y|x). As
œï
establishedinTheorem3.1,theoptimalL(y|x)isgivenby:
(cid:20) (cid:21)
r (y|x)+Œ±
Œ≤argmaxE Œ∏ L(y|x)‚àíœï(L(y|x))
x‚àºD,y‚àºœÄref Œ≤
L(y|x)
(cid:34) (cid:26) (cid:27)(cid:35)
r (y|x)+Œ±
=Œ≤E argmax Œ∏ L(y|x)‚àíœï(L(y|x)) (49)
x‚àºD,y‚àºœÄref Œ≤
L(y|x)
(cid:20) (cid:18) (cid:19)(cid:21)
r (y|x)+Œ±
=Œ≤E œï‚àó Œ∏ .
x‚àºD,y‚àºœÄref Œ≤
Tofindthismaximum,wedifferentiatetheconvexfunctionw.r.t. L(y|x)andequatethederivativeto
zero:
(cid:26) (cid:27)
‚àÇ r (y|x)+Œ±
Œ∏ L(y|x)‚àíœï(L(y|x)) =0. (50)
‚àÇL Œ≤
Solvingforr (y|x)yieldstheoptimalexpression:
Œ∏
r (y|x)=Œ≤œï‚Ä≤(L(y|x))‚àíŒ±. (51)
Œ∏
GiventhatŒ±isaconstant,thecriticalcomponentoftheexpressionisŒ≤œï‚Ä≤(L(y|x)). Whilethisresult
alignswithTheorem1from[42],ourapproachisgroundedinacomprehensiveDROframework,
providingamoredirectandcompletetheoreticaljustification. Thus,thelemmaissubstantiated.
Comparison with Wang et al. [42]. Since Œ± is a constant that does not affect the optimization
process,LemmaE.1revealsthattherewardfunctionr(x,y)isinfluencednotonlybythechoice
ofœï-divergencebutalsobytheparameterŒ≤. Thisobservationsuggestsanintuitiveunderstanding
that various œï-divergences enforce constraints with unique geometric characteristics, which, in
turn, dictate the optimal value of Œ≤‚àó(Œ∑) within DRO. Interestingly, our findings contradict the
conclusionspresentedinWangetal.[42],whichsuggestthatvariousœï-divergencesleadtodifferent
alignmentaccuracy. Instead,ourresultsdemonstratethatbyfine-tuningtheparameterŒ≤,comparable
performancecanbeachievedacrossdifferentdivergences(cf. Table2). Thisunderscoresthecritical
roleoftherobustradiusindeterminingtheefficacyofDROframeworks. Fordetailedexperimental
settings, please refer to Section 5.1. Moreover, a thorough analysis of the behavior of diverse
œï-divergencescanbefoundinAppendixF.
We compute the gradients w.r.t. the chosen likelihood ratios, œÄŒ∏(yw|x), and the rejected ratios,
œÄref(yw|x)
œÄŒ∏(yl|x). AsdepictedinFigure6,thechoiceofœï-divergenceinfluencesthegradientupdaterules
œÄref(yl|x)
in a consistent manner, though the performance varies with the scaling parameter Œ≤. With an
optimalselectionofŒ≤,thedisparitiesbetweentheperformancesofdifferentœï-divergencesdiminish
significantly(refertoTable2foracomparativeanalysis).
Table2: Comparisonofwinratesacrossvariousœï-divergenceswithadjustmentstoŒ≤.
œï-divergence winrate Œ≤ winrate Œ≤
KL 54.36 0.1 55.40 0.15
JSD 54.36 0.1 54.75 5e-2
Œ±=0.3 45.02 0.1 54.59 1e-4
Œ±=0.5 44.06 0.1 56.60 1e-6
Œ±=0.7 41.17 0.1 58.45 1e-6
19Gradient of chosen logratios Gradient of rejected logratios
0.10
0 KL( =0.1)
FKL( =1e 3)
0.08
1
JS( =0.05)
alpha=0.7( =0.01)
0.06
2
0.04
3
0.02
4 0.00
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
Figure6: Gradientanalysiswithrespectivetodifferentœï-divergence.
E.2 ComparisonbetweenDPOandDRO
ThefundamentalprincipleoftheDROframeworkistoguardagainsttheworstpossibledistribution
withinadefinedambiguityset. Notably,the‚Äúmin‚Äùoperationmanifestsimplicitlyoutsideoftheloss
functionL . Toprovideamoreinsightfulexaminationofthemin-maxcomponent,itisinstructive
DPO
totraceDPO‚ÄôsgenesisbacktoitsrootsinRLHF.Atwo-phaseprocesswithinRLHFelucidatesthis
mechanism:
1. Reward Modeling Phase: In this phase, we integrate human preferences via a negative log-
likelihoodloss:
minL(r,O)=min‚àíE [logœÉ(r (x,y )‚àír (x,y ))]
rœï rœï
(x,yw,yl)‚àºO œï w œï l
Itoptimizesforarewardfunctionr byminimizingthepredicteddisparityinpreferencesbetween
œï
winning and losing outcomes y and y , as determined through interaction instances (x,y ,y )
w l w l
gatheredfromthetrainingsetO.
2. ReinforcementLearning(RL)Fine-TuningPhase: Thisstageleveragesthelearnedreward
function to generate feedback for the language model, adopting a policy improvement step that
embodiesour‚Äômax‚Äôoperationperceivedduringthefine-tuningphase:
maxE [r (x,y)]‚àíŒ≤D [œÄ (y|x)||œÄ (y|x)].
œÄŒ∏
x‚àºO,y‚àºœÄŒ∏(y|x) œï KL Œ∏ ref
AselucidatedintheintroductionofDPOinpreliminaries,
This allows for the direct optimization of the policy by reparameterizing the
rewardfunctionusingthepolicy(i.e.,thelanguagemodel)inasupervisedmanner.
Subsequently,theclosed-formsolutionfromtheRLphaseissubstitutedintotheRewardModeling
Phase,andthereparameterizationofr intoœÄ yields:
œï Œ∏
œÄ (y |x) œÄ (y |x)
minL (œÄ ;œÄ )=min‚àíE [logœÉ(Œ≤log Œ∏ w ‚àíŒ≤log Œ∏ l )]. (52)
œÄŒ∏ DPO Œ∏ ref œÄŒ∏ (x,yw,yl)‚àºO œÄ ref(y w |x) œÄ ref(y l |x)
ComparingDPOandDROenhancesunderstanding:
DPO
‚Ä¢ Motivation: SuboptimalinitialdistributionofSFTmodel(referencemodel).
‚Ä¢ Max Part: Explore maximization criterion around the reference model, here aiming for
maximalreward.
‚Ä¢ MinPart: OptimizetheBTmodelonthenovelrewardfunction.
DRO
‚Ä¢ Motivation: Suboptimalinitialtrainingsetdistribution.
20‚Ä¢ MaxPart: Exploremaximizationcriterionaroundtheinitialtrainingsetdistribution,tradi-
tionallyaloss,butvariesindifferentapplications.
‚Ä¢ MinPart: Optimizethemodelonthisnoveldistribution.
Inconclusion,ourmethodologyfaithfullyembodiestheessenceofDRObyinstitutingaprotective
mechanismagainstthedistributiondeterminedbyaselectambiguitysetandaspecificcriterion. The
‚Äúmin‚Äùoperation, thoughindirectlyrepresentedinEquation(5), isanintegralpartofourmodel‚Äôs
optimizationprocess,fittingwellwithintheDROframework‚Äôsintent.
21F AppendixofExperiments
1 # pi_logps : policy logprobs, shape (B,)
2 # ref_logps : reference model logprobs, shape (B,)
3 # yw_idxs : preferred completion indices in [0, B-1], shape (T,)
4 # yl_idxs : dispreferred completion indices in [0, B-1], shape (T,)
5 # beta : regularization coefficient
6 # beta_1 : regularization coefficient for pairwise robustness
7
8 pi_yw_logps, pi_yl_logps = pi_logps[yw_idxs], pi_logps[yl_idxs]
9 ref_yw_logps, ref_yl_logps = ref_logps[yw_idxs], ref_logps[yl_idxs]
10 pi_logratios = pi_yw_logps - pi_yl_logps
11 ref_logratios = ref_yw_logps - ref_yl_logps
12 losses = -F.logsigmoid( beta * ( pi_logratios - ref_logratios))
13
14 #DPO
15 DPO_loss = losses.mean()
16
17 #Dr. DPO
18 DrDPO_loss = - beta_1 * torch.log(torch.mean(torch.exp( - losses / beta_1)))
Figure7: PseudocodeforourproposedDr. DPO,aswellastheoriginalDPOobjective.
Figure7presentsaPyTorch-stylepseudocodecomparisonbetweenthestandardobjectiveandour
proposedDr. DPOobjective. TheimplementationsimplicityoftheDr. DPOlossishighlighted,as
itnecessitatesnoadditionallinesofcodebeyondwhatisrequiredforthestandardobjective. This
easeofintegrationunderscoresthepracticalityofadoptingDr. DPOinexistingmachinelearning
workflowswithouttheneedforextensivecodemodifications.
F.1 ExperimentsSetuponHH
Forourpreliminaryresearch,weconductedexperimentsontheAnthropicHHdataset[3],which
comprises170,000human-automatedassistantdialogues. Eachdialogueconcludeswithtwolarge
languagemodel-generatedresponsesandanaccompanyingpreferencelabeldenotingthehuman‚Äôs
favoredchoice. OurtrainingregimenwasinlinewiththeDPO-establishedprotocol[35]. Webuilt
upon the Pythia 2.8B model, as described in [5], to develop our Supervised Fine-Tuning (SFT)
model. TheSFTmodelwasfine-tunedontheAnthropicHHdatasetoverthecourseofoneepoch,
employingabatchsizeof64andalearningrateof5√ó10‚àí7. Inaddition,wefurtherrefinedthe
modelusingtheAnthropicHHdatasetandtheDPOlossfunction(orotherbaselineapproaches)
throughanadditionalepochoffine-tuning. Totestthemodel‚Äôsresiliencetonoise,weintroduced
randominversionsbetweenselectedandrejectedresponsesinthetrainingdatawithprobabilitiesof
10%,20%,30%,and40%. Throughouttheseexperiments,weconsistentlysettheŒ≤ parameterto0.1
andadoptedtheKullback-Leibler(KL)divergenceasthemetricforœï-divergence. Wecarriedoutall
computationaltasksonasuiteoffour80GBA100GPUs.
F.2 ExperimentsonRedditTL;DRDataset
ForafaircomparisonwithDPO,wemaintainedtheparametersŒ≤ =0.5andlr =1e‚àí6,andchose
Œ≤‚Ä≤ = 1.0withoutextensivetuning. ThisapproachensuresthatourevaluationoftheproposedDr.
DPOframeworkisconsistentandcomparabletotheexistingbaseline.
Finally, the table below presents the win-rate comparison on the TL;DR dataset under various
samplingtemperatures,furthersupportingourclaims:
Table3: ComparisonofDPOandDr. DPOacrossvarioussamplingtemperatures.
SamplingTemperature 0.0 0.25 0.5 0.75 1.0
DPO 45.06 46.74 46.70 39.50 21.28
Dr. DPO 62.36 67.34 71.29 63.91 27.75
22AsevidencedbyTable3,Dr. DPOconsistentlyoutperformsDPOacrossdifferentsamplingtempera-
tures,particularlyatlowertemperatureswhicharecrucialforcomplextaskssuchassummarization.
F.3 ExperimentsonAmbiguousDatasets
Toincorporatethefeedbackregardingtheevaluationofourapproachondatasetswithambiguity-
induced noise, we conducted additional experiments. These were aimed at understanding how
performs under varying conditions of data perturbation, specifically through token masking and
substitution. ThecomparativeanalysisbetweenthetraditionalDPOandDr. DPOwascarriedout
underconsistentexperimentalconditionstoensurethevalidityandreliabilityoftheresults.
ExperimentalSetup. Tosimulateambiguousdatasets,weintroducedrandomnessintheformof
tokenmaskingandsubstitutionatdifferentratios,therebyincreasingthedifficultyofthedataset. The
intentionwastoassesstheresilienceandadaptabilityofourDr. DPOmethodunderchallenging
conditions that are akin to real-world scenarios. The experiments were conducted using the HH
dataset,knownforitscomplexityandrelevanceinevaluatingdataprocessingalgorithms.
The configurations for both DPO and Dr. DPO were kept consistent with previous experiments
to maintain comparability. Specifically, we set Œ≤ = 0.1 and the learning rate lr = 5e‚àí7 for
both approaches. For Dr. DPO, an additional hyperparameter, Œ≤‚Ä≤, was introduced and set to 1.0.
Notably,wedidnotundertakeextensivehyperparametertuning,optinginsteadforastraightforward
comparison.
ExperimentalResults. Theresultsofourexperimentsaresummarizedinthetablebelow,illustrating
preferenceaccuraciesundervaryingnoiseconditions:
Table4: PreferenceAccuracyontheHHDatasetwithVaryingNoiseRatios
MaskingRatio 0.05 0.10 0.15 ReplacingRatio 0.05 0.10 0.15
DPO 58.77 56.64 54.39 DPO 57.84 54.43 52.40
Dr. DPO 59.21 58.44 56.10 Dr. DPO 58.45 55.60 53.37
Discussion.TheresultsindicatethatDr.DPOconsistentlyoutperformsDPOacrossdifferentlevelsof
inducednoise,whetherthroughmaskingorreplacingtokens. Notably,theimprovementinpreference
accuracybecomesmorepronouncedasthenoiseratioincreases,suggestingthatDr. DPOismore
robusttoambiguity-inducednoisecomparedtoDPO.ThesefindingsvalidateourhypothesisthatDr.
DPOcanbetterhandlethecomplexitiesanduncertaintiesinherentinreal-worlddatasets.
ItisalsoimportanttonotethattheseresultswereobtainedwithoutextensivetuningoftheDr. DPO-
specifichyperparameter,Œ≤‚Ä≤.Futureworkcouldinvolveamoredetailedexplorationofhyperparameter
settingstopotentiallyunlockfurtherimprovementsinperformance.
In conclusion, the additional experiments conducted in response to feedback have not only rein-
forcedtheeffectivenessofourDr. DPOapproachbutalsoopenedavenuesforfutureresearchinto
optimizationstrategiesforprocessingambiguousdatasets.
F.4 Reward
Reward. TherewardmetriciscomputedontheIMDBdataset,whichisselectedfortheavailability
of a ground-truth reward function provided by a sentiment classifier. Figure 8 demonstrates that
theDr. DPOalgorithmachievesenhancedstabilityandsuperiorrewardperformanceundervarying
pairwisenoiseanddifferentŒ≤. Additionally,bysettingŒ≤‚Ä≤toafixedvalueof1,weaddresstheissue
ofDPO‚ÄôssensitivitytotheparameterŒ≤. ThisconsistentsettingofŒ≤‚Ä≤eliminatestheneedforextensive
parametertuning,whichsignificantlybenefitspracticalapplications.
F.5 ImpactofVaryingŒ≤
Concomitantly, we have carried out an evaluation of the performance enhancement of Dr. DPO
relativetoDPOunderdiversebetavalues,asshowninFigure9.TheaforementionedDr.DPOassures
astableperformanceaugmentationregardlessofbetaselection,furtherattestingtotheefficacyofthe
2330% Flipped 40% Flipped
0.9 0.75
0.70
0.8 30%
32%
Improv.
Improv. 0.65
DPO
0.7
Dr. DPO 0.60
0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5
Figure8: EvaluationofIMDBat0%and40%flippedpairratios.
10% Flipped 20% Flipped 30% Flipped 40% Flipped
0.62
0.58 0.63
0.62 0.60
0.62
DPO 0.56
0.61 0.60 0.58
Dr. DPO
0.60 0.58 0.56 0.54
0.59
0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5 0.1 0.2 0.3 0.4 0.5
Figure9: PreferenceaccuracyacrossvaryingŒ≤fordifferentlevelsofpairwisenoiseontheAnthropic
HHdataset.
Dr. DPOmethodology. Notably,theattainedresultsarebasedonthedefaultvalueofŒ≤‚Ä≤,setat1.0,
negatingthenecessityofadditionalparameteradjustmentstoŒ≤‚Ä≤forasteadyperformanceuplift.
F.6 ImpactofDifferentTemperature
Ultimately,weventuredtoexperimentwiththetemperaturecoefficientevaluationforGPT-4(cf.Table
5),where,atavalueof0.7,theDr. DPOmethodconsistentlyoutperformsitsbaselinecounterparts,
henceforth,reaffirmingthemethod‚Äôsvalidityandeffectiveness.
Table5: ComparisonofWinRatePerformanceontheAnthropicHHDatasetat0%and40%Flipped
PairRatios(Temperature=0.7).
Models 0%Flipped 40%Flipped Improv.
DPO 48.30 48.49 +0.39
cDPO 47.96 46.14 ‚àí3.79
IPO 51.20 52.39 +2.32
Dr.DPO 53.62 56.71 +5.76
F.7 Discussion
ComparisonwithLDR[48]: UnlikeLDR,whichappliesDROforrobustmulti-classclassification,
Dr. DPO is tailored for preference learning tasks. While LDR offers pointwise robustness by
adjustingweightsforindividualclasslabelsperinstancex,Dr. DPOprovidespairwiserobustnessby
optimizingweightsforeachpairofresponses(y ,y )withindatasetO. Furthermore,LDRseeks
w l
toreduceoverfittingbydecreasingtheweightsofselectedinstances, whereasDr. DPOcounters
mismatchedpaireffectsbyup-weightingchosenresponsepairs.
24
ycaruccA
ecnereferP
draweR