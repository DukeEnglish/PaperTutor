Dynamical Measure Transport
and Neural PDE Solvers for Sampling
JingtongSun∗, JuliusBerner∗ LorenzRichter∗
CaliforniaInstituteofTechnology ZuseInstituteBerlin
didaDatenschmiedeGmbH
MariusZeinhofer JohannesMüller KamyarAzizzadenesheli
SimulaResearchLaboratory RWTHAachen NVIDIA
UniversityHospitalFreiburg
AnimaAnandkumar
CaliforniaInstituteofTechnology
Abstract
Thetaskofsamplingfromaprobabilitydensitycanbeapproachedastransporting
atractabledensityfunctiontothetarget,knownasdynamicalmeasuretransport.
Inthiswork,wetackleitthroughaprincipledunifiedframeworkusingdetermin-
istic or stochastic evolutions described by partial differential equations (PDEs).
This framework incorporates prior trajectory-based sampling methods, such as
diffusionmodelsorSchrödingerbridges,withoutrelyingontheconceptoftime-
reversals. Moreover,itallowsustoproposenovelnumericalmethodsforsolving
thetransporttaskandthussamplingfromcomplicatedtargetswithouttheneed
for the normalization constant or data samples. We employ physics-informed
neuralnetworks(PINNs)toapproximatetherespectivePDEsolutions,implying
bothconceptionalandcomputationaladvantages. Inparticular,PINNsallowfor
simulation-anddiscretization-freeoptimizationandcanbetrainedveryefficiently,
leadingtosignificantlybettermodecoverageinthesamplingtaskcomparedto
alternativemethods. Moreover,theycanreadilybefine-tunedwithGauss-Newton
methodstoachievehighaccuracyinsampling.
1 Introduction
Weconsidertheproblemofsamplingfromatargetprobabilitydensity
ρ (cid:90)
p = target, Z := ρ (x)dx, (1)
target Z target
forwhichonlyanunnormalizedfunctionρ : Rd →(0,∞)canbeevaluated,butthenormalizing
target
constant Z is typically intractable. This challenging task has wide applications, for instance, in
Bayesianstatistics[66],computationalphysics[63],quantumchemistry[26,47]andotherscientific
disciplines[23,39]. Variousparticle-basedmethods,suchasimportancesampling,Markovchain
Monte Carlo (MCMC), Sequential Monte Carlo, etc., have been designed in the last decades to
approachthistask[17,34,35]. However,theyoftensufferfromslowconvergence,inparticularfor
high-dimensional, multimodaldistributions. Inordertoaddressthisissueandimprovesampling
performance,twoparadigmshavebeenintroduced:
∗Equalcontribution.
Preprint.Underreview.
4202
luJ
01
]GL.sc[
1v37870.7042:viXra1. Enhancingthesamplingproblemwithalearningtask,whereusuallysomefunctionislearnedin
ordertoimprovesamplingquality(e.g.,inavariationalinferencesetting).
2. Formulating the sampling problem as a dynamical measure transport from a tractable initial
densityfunctiontothecomplicatedtarget.
Inthiswork,weaimtoadvancebothparadigms. Inparticular,werelyontheunderlyingprincipled
frameworkofpartialdifferentialequations(PDEs)asaunifiedframeworkforderivingbothexisting
andnewsamplingalgorithms.
Tobemoreprecise,weconsiderthetaskofidentifyingevolutionsfromaninitialdistributiontothe
targetonafinite-timehorizon.Therearetwobroadapproachestotacklethis,viz.,particleanddensity-
basedapproaches. Particle-basedapproachessampleso-calledparticlesfromtheinitialdistribution
andevolvethemusingdifferentialequations,eitherwith(deterministic)ordinarydifferentialequations
(ODEs)orstochasticdifferentialequations(SDEs). Incontrast,fordensity-basedapproaches,the
evolutionsofthedensities(oftheparticles)canbedescribedbyassociatedPDEs,viz.,thecontinuity
ortheFokker-Planckequation,respectively. Inparticular,thePDEscouplethedriftoftheODEor
SDEandthedensity,givingusthechoicetoaddadditionalconstraints(leadingtouniqueoptimal
values)ortolearnbothsimultaneously(leadingtonon-uniquesolutions). Whileuniquesolutionscan
exhibitbeneficialproperties(suchasdriftswithsmallmagnitude),theexistenceofmultipleoptimal
solutionscanbemoresuitedforgradient-basedoptimizationmethods.
For the task of numerically approximating the high-dimensional PDEs at hand, we can leverage
differentdeep-learningmethods. Weshowthatwecanrecovermultiplepreviousmethodswhen
consideringlossesbasedonbackwardstochasticdifferentialequations(BSDEs). Thishighlightsthe
foundationalroleofthePDEframework[24,48]. Employingtheframeworkofphysics-informed
neuralnetworks(PINNs)[55],wederivenovelvariationalformulationswithbothuniqueandnon-
uniquesolutions. Moreimportantly,thePINNlossesonlyrequireevaluatingthePDEresidualon
random points in the spatio-temporal domain. In contrast, previous works based on dynamical
measure transport rely on discretized trajectories of the dynamics for training. We numerically
evaluateourPINN-basedapproachesonchallenginghigh-dimensionalexamplesandshowbetter
performance. In particular, we can improve mode coverage in multimodal settings compared to
simulation-basedapproaches.
Ourcontributionscanbesummarizedasfollows:
• We provide a unifying PDE perspective on generative modeling and sampling via dynamical
measuretransport.
• WederivesuitableobjectivestonumericallysolvethesePDEsusingdeeplearning. Thisrecovers
knownmethodsasspecialcasesandprovidesarangeofnovelobjectiveswithbeneficialnumerical
properties.
• We propose further improvements based on efficient parametrizations, sampling schemes, and
optimizationroutines. Thisleadstostate-of-the-artperformanceonaseriesofbenchmarks.
1.1 Relatedwork
TherearenumerousMonteCarlo-basedmethodsforsamplingfromunnormalizeddensities,including
MarkovchainMonteCarlo(MCMC)[27],AnnealedImportanceSampling(AIS)[42],andSequen-
tial Monte Carlo (SMC) [15, 18]. However, these methods typically only guarantee asymptotic
convergencetothetargetdensity,withpotentiallyslowconvergenceratesinpracticalscenarios[59].
Variationalmethods,suchasmean-fieldapproximations[71]andnormalizingflows[51],offeran
alternativeapproach. Inthesemethods, theproblemofdensityestimationistransformedintoan
optimizationproblembyfittingaparametricfamilyoftractabledistributionstothetargetdensity. In
thecontextofnormalizingflows,wewanttomentionworksonconstructingbetterlossfunctions[19]
orgradientestimators[67].
Inthiswork,weprovideacomprehensivePDEperspectiveonSDE-basedsamplingmethods. Our
approachislooselyinspiredby[36],however,extendedtodiffusionmodels,optimaltransport(OT),
andSchrödingerbridges(SBs). Moreover,weconsiderotherparametrizationsanddonotrelyon
theODEforsamplingthecollocationpoints(ξ,τ). Foracorrespondingmean-fieldgames(MFG)
perspective, we refer to [76]. We also mention path space measure perspectives on SDE-based
methodsin[57,69].
2p prior annealed SDE p target
annealed ODE general ODE
Figure1: WeplotthreeevolutionsoftheprocessX definedin(2)and(3)betweenaGaussianprior
densityp andaGaussianmixturetargetdensityp ,correspondingtoSDEsandODEswhich
prior target
havebeenlearnedwiththreedifferentlossfunctions. Thetoppaneldisplaysastochasticevolution
stemmingfromthelossLanneal,forwhichweadditionallyplothistogramsofthepriorandthetarget,
logFP
respectively. Inthesecondrowweshowdeterministicevolutions,onceobtainedwithLanneal and
logCE
once with L . Note that the stochastic and the left deterministic evolution follow the same
logCE
annealingstrategy,whereasthegenerallossL leadstoadifferentdensitypath. Wereferto
logCE
Section3forthedetailsofthedifferentmethods.
ThePDEfordiffusionmodelshasbeenderivedin[5]basedonpriorworkby[21,53]instochastic
optimalcontrol. Wereferto[10]forthecorrespondingPDEsprominentinOTandSBs. Versionsof
theHamilton-Jacobi-Bellman(HJB)regularizerhavebeenusedfornormalizingflowsingenerative
modelingby[50],forgeneralizedSBsby[29,33],forMFGby[31,60],andforgenerativeadversarial
modelsby[75].
For the usage of PINNs for a generalized SB in the context of colloidal self-assembly, we refer
to[46]. Anorthogonaldirectiontoourapproachisusingdivergence-freeneuralnetworks,which
automaticallysatisfythecontinuityequationandonlyrequiretofittheboundarydistributionsp
target
andp [58]. Wefurthermentionthathigher-dimensionalFokker-Planckequationshavealsobeen
prior
tackledwithtime-varyingGaussianmixtures[7],andthereexistSDE-basedneuralsolversforHJB
equations[49,56]andcombinationswithPINNs[48].
Finally,wewanttohighlightrecentworksonsimulation-freelearningof(stochastic)dynamicsusing
flowmatching[32,65]andactionmatchingtechniques[43]. However,thesemethodsrelyonsamples
fromthetargetdistributionp . Similarly,manyworksonsolvingSBandOTproblemsusing
target
deeplearningrequiresamplesfromthetargetdistribution[8,13,20,70].
2 Samplingviadynamicalmeasuretransport
Ourapproachistoidentifyadynamicalsystemthattransportsachosenpriordensitytothedesired
targetviaadeterministicorstochasticprocess. Tobemoreprecise,weconsidertheSDE
dX =µ(X ,t)dt+σ(t)dW , X ∼p , (2)
t t t 0 prior
whereW isastandardBrownianmotion,or,bysetting2σ =0,theODE
dX =µ(X ,t)dt, X ∼p , (3)
t t 0 prior
andourgoalistolearnthedriftµ∈C(Rd×[0,T],Rd)suchthatX ∼p ,seeFigure1.
T target
2Weconsiderageneraldiffusioncoefficientfunctionσ∈C([0,T],Rd×d),includingthespecialcasewhere
σisconstantzero,i.e.,σ=0.
3Dynamicalsystemscanbeviewedonatrajectorylevel, asspecifiedabove, oronadensitylevel,
wherewedenotewithp (·,t)thedensityoftherandomvariableX attimet ∈ [0,T]. Itiswell
X t
knownthatsuchdensitiescanbedescribedbyPDEs[52]Inparticular,weknow3thatthedensityp
X
ofthestochasticprocessin(2)fulfillstheFokker-Planckequation
∂ p +div(p µ)− 1Tr(σσ⊤∇2p )=0, p (·,0)=p , (4)
t X X 2 X X prior
and,analogously,thatthedensityp ofthedeterministicprocess(3)fulfillsthecontinuityequation
X
∂ p +div(p µ)=0, p (·,0)=p , (5)
t X X X prior
noting that our desired goal adds the additional boundary condition p (·,T) = p . A valid
X target
strategytoidentifyadriftthatfulfillsourgoalisthustolookforpairsµandp thatfulfilleither
X
oftheabovePDEs. Itisimportanttonotethatthereexistinfinitelymanysuchpairs,corresponding
toinfinitelymanybridgesbetweenthepriorandthetargetdensity. Wewilllaterdiscusswaysto
constraintheproblem,leadingtouniquesolutions.
Remark2.1(Connectionstoothermethods). Wenotethattheaboveframeworkincorporatesexisting
samplingmethodsthatcanberelatedtoeitherSDEsorODEs. Intheformersetting,Schrödinger
(half-)bridges,diffusionmodels,orannealedflowscanbeunderstoodaslearningstochasticevolutions
[5,57,68,69,77,78]. Inthelater,continuousnormalizingflows(sometimescombinedwithMCMC)
areininstanceoflearnedODEs[2,37,38,74]. Wenote,however,thatthepreviouslymentioned
methodsrelyonsimulating(partsof)theprocessX fortraining,whichrequirestimediscretization
andtypicallyresultsinunstableandslowconvergence. OurPDE-basedattempt,ontheotherhand,
allowsforsimulation-freetraining,aswillbeexplainedinthenextsection.
3 Learningtheevolution
Ageneralstrategytosolvethesamplingtaskistoidentifysolutionpairsµandp thatsolvethe
X
PDE(4)or(5),respectively,andourtaskthuscorrespondstothenumericalapproximationofPDEs.
Sinceourgeneralsetupallowsforinfinitelymanysolutions,itseemsparticularlysuitabletoconsider
variationalformulationsofthePDEs. Tobemoreprecise,weconsiderlossfunctionals
L:C(Rd×[0,T],Rd)×C(Rd×[0,T],R)→R , (6)
≥0
thatarezeroifandonlyifapair(µ,p )fulfillsthecorrespondingPDE.Inthefollowing,wewill
X
designdifferentlossfunctionsthatfollowtheframeworkofPINNs,i.e. correspondtotherespective
PDEresidualterms.
3.1 Generalevolution
Letusfirststudythegeneralcase. Fornumericalstability,itisreasonabletoconsiderthePDEs(4)or
(5)inlog-space,andwenotethatthefunctionV :=logp fulfillsthelog-transformedFokker-Planck
X
equation
R (µ,V):=∂ V +div(µ)+∇V ·µ− 1∥σ⊤∇V∥2− 1Tr(σσ⊤∇2V)=0, (7)
logFP t 2 2
orthelog-transformedcontinuityequation
R (µ,V):=∂ V +div(µ)+∇V ·µ=0, (8)
logCE t
respectively4. Havingapproximationsµ (cid:101)andV(cid:101) ofthedriftandlog-density,wecannowdefinelosses
ofthetype
(cid:20)(cid:16) (cid:17)2(cid:21) (cid:20)(cid:16) (cid:17)2(cid:21)
L(µ (cid:101),V(cid:101))=α 1E R(µ (cid:101),V(cid:101))(ξ,τ) +α 2E V(cid:101)(ξ,0)−logp prior(ξ)
(9)
(cid:20)(cid:16) (cid:17)2(cid:21)
+α 3E V(cid:101)(ξ,T)−logp target(ξ) ,
3Weassumethatthecoefficientfunctionsanddensitiesaresufficientlyregularsuchthatweobtainunique
strongsolutionstotheconsideredPDEs.
4Wenotethatequation(7)isaHamilton-Jacobi-BellmanequationwhenbeingconsideredasaPDEinthe
functionV,seealso[5].
4withsuitablychosenrandomvariables(ξ,τ)andweightsα ,α ,α >0,notingthattherespective
1 2 3
PDEisfulfilledifandonlyifL(µ (cid:101),V(cid:101)) = 0. Inpractice,oneoftenchooses(ξ,τ) ∼ Unif(Ω)with
Ω⊂Rd×[0,T]beingasufficiently5largecompactset. Moreover,onecanconsiderparametrizations
ofthefunctionV(cid:101) thatfulfilltheboundaryconditionsbydesign,e.g.,
V(cid:101)φ,z(·,t)= Tt logρt za (r tg )et +(cid:0) 1− Tt(cid:1) logp prior+ Tt (cid:0) 1− Tt(cid:1) φ(·,t), (10)
suchthattheloss(9)reducesto
(cid:20)(cid:16) (cid:17)2(cid:21)
L(µ (cid:101),V(cid:101))=E R(µ (cid:101),V(cid:101))(ξ,τ) , (11)
see also [36]. In the above, z ∈ C([0,T],R) and φ ∈ C(Rd × [0,T],Rd) are functions that
parametrizetheapproximationV(cid:101). Ifφisoptimized(andnotfixedas,e.g.,intheannealingcase,see
below),thefunctionz ∈C([0,T],R)canbereducedtoaconstantfunctiont(cid:55)→z¯,wherez¯∈Risa
learnableparameter,seealsoAppendixA.1.
Specifically,wecandefinethetwolossfunctions
(cid:20)(cid:16) (cid:17)2(cid:21)
L logFP(µ (cid:101),V(cid:101)):=E R logFP(µ (cid:101),V(cid:101))(ξ,τ) (12)
and
(cid:20)(cid:16) (cid:17)2(cid:21)
L logCE(µ (cid:101),V(cid:101)):=E R logCE(µ (cid:101),V(cid:101))(ξ,τ) . (13)
3.2 Constrainedevolution
Wenowdiscusswaystoconstraintheevolutioninordertogetuniquesolutions. Tothisend,wecan
fixp andonlylearnµ(annealing),wecanfixµandonlylearnp (time-reversal)orwecanadd
X X
regularizersonµ,whilestilllearningbothµandp (optimaltransportandSchrödingerbridges).
X
Annealing. Wecanprescribeadensitypathfrompriortotargetbyspecifyingp . Thiscan,for
X
instance,bedonebychoosingφ=0in(10)[36],whichyieldsthetypicalgeometricpathoftentaken
inAnnealedImportanceSampling(AIS)[42,69]. Thisthenamountstoconsideringtheresiduals
Ranneal(µ):=R (µ,V), Ranneal(µ):=R (µ,V), (14)
logFP (cid:101) logFP (cid:101) logCE (cid:101) logCE (cid:101)
wherenowV isfixed(uptothelearnablenormalizationz(t)),e.g.,bysettingV = V usingthe
0,z
parametrization(10),thusyieldinguniqueminimizers. Wereferto[1,Theorem8.3.1],whichproves
thatundermildconditionswecanalwaysfindadriftasthegradientofapotential,i.e. µ=∇Φ,such
thatthecorrespondingODEorSDEhastheprescribeddensity,seealso[43].
Score-based generative modeling. For the stochastic dynamics, we may consider the concept
oftime-reversalasrecentlyappliedinscore-basedgenerativemodeling. Tothisend,wemayset
µ=σσ⊤∇V −f forafixedfunctionf,whichyields
R (σσ⊤∇V −f,V):=∂ V −div(f)−∇V ·f+1∥σ⊤∇V∥2+1Tr(σσ⊤∇2V)=0. (15)
logFP t 2 2
Onecannowreadilyseethatthetime-reversalofthefunctionV fulfilling(15),whichwedenotewith
V,fulfills(whenreplacingσwithσ)
R (f,V)=0. (16)
logFP
ThiscorrespondstotheSDE
dY =f(Y ,t)dt+σ(t)dW , Y ∼p , (17)
t t t 0 target
andwecanthusinterpretV = logp ,asalsoderivedin[5]. Inconsequence,aviablestrategyis
Y √
topickf andσsuchthatp (·,T)≈p (e.g.,f(x,t)=−xandσ(t)= 2,seeAppendixD.3),
Y prior
andminimizetheloss
L score(V(cid:101)):=L logFP(σσ⊤∇V(cid:101) −f,V(cid:101)). (18)
5Inordertosolvefortheexactsolution,wetheoreticallyneedthattherangeof(ξ,τ)equalsRd×[0,T].To
mitigatealargeapproximationerror,wethuschooseacompactdomainΩlargeenoughsuchthatthedensityp
X
hassufficientlysmallvaluesonthecomplementofΩ.WepresentfurtherapproachesinAppendixD.1.
5
⃗
⃗
⃗
⃗
⃗
⃗ ⃗Table1: Summaryofourconsideredlosses. Emptycellsdonothaveadirectcorrespondence.
Method Stochastic Deterministic BSDEversion Unique
Generalbridge L logFP(µ (cid:101),V(cid:101)) L logCE(µ (cid:101),V(cid:101)) Bridge[8,57] ✗
Prescribed/annealedbridge Lanneal(µ) Lanneal(µ) CMCD[69] ✓
logFP (cid:101) logCE (cid:101)
Time-reversal/diffusionmodel L score(V(cid:101)) DIS[5] ✓
Regularizeddrift/SB/OT L SB(µ (cid:101),V(cid:101)) L OT(µ (cid:101),V(cid:101)) ✓
For this loss, we do not need learn z and enforce logp
prior
in our parametrization of V(cid:101) in (10),
since the drift µ only depends on the gradient of V(cid:101) and the boundary condition is specified by
V(·,0)=logp (·,T)≈logp ,seealso[5].
Y prior
OptimaltransportandSchrödingerbridges. Anotherwaytogetuniquesolutionsastoadda
regularizationtothedrift. Inparticular,wemayseekthedriftµthatminimizesanenergyoftheform
(cid:34) (cid:35)
(cid:90) T
E 1 ∥µ(X ,s)∥2ds . (19)
2 s
0
Fornonzeroσ, thiscorrespondstothedynamicSchrödingerbridge(SB)problem[12]. Inthese
cases,theoptimalsolutioncanbewrittenasµ:=∇Φ,whereΦsolvestheHamilton-Jacobi-Bellman
(HJB)equation
RSB (Φ):=∂ Φ+ 1∥∇Φ∥2+ 1Tr(σσ⊤∇2Φ)=0, (20)
HJB t 2 2
see Appendix A.2 and, e.g., [3, 6, 54, 69]. For σ = 0, it is connected to optimal transport (OT)
problemsw.r.t.theWassersteinmetric[3]andtheHJBequationturnsinto
ROT (Φ):=∂ Φ+ 1∥∇Φ∥2 =0. (21)
HJB t 2
Wecanaddsuchregularizationusingthelosses
(cid:20)(cid:16) (cid:17)2(cid:21)
L SB(Φ(cid:101),V(cid:101)):=L logFP(∇Φ(cid:101),V(cid:101))+αE RS HB JB(Φ(cid:101))(ξ,τ) , (22)
(cid:20)(cid:16) (cid:17)2(cid:21)
L OT(Φ(cid:101),V(cid:101)):=L logCE(∇Φ(cid:101),V(cid:101))+αE RO HJT B(Φ(cid:101))(ξ,τ) , (23)
whereα>0isasuitablychosenweight.
3.3 Connectionstopreviousattempts
Inthissection,weshowthatwecanre-derivealreadyexistingmethodsindiffusion-basedsampling
viaourPDEperspective. ThiscanbedonebyreplacingourPINN-basedlosseswithlossesbasedon
backwardstochasticdifferentialequations(BSDEs). Thoselossesbuildonastochasticrepresentation
ofthePDEathand,essentiallycomingfromItô’sformula,see[48]andthereferencesthereinfor
details. In the following proposition we relate BSDE-based versions of our losses to alternative
trajectory-basedlosses, indicatedbyLBSDE . WeprovideanoverviewinTable1. Interestingly,
method
thepropositionshowsthatmanyofthediffusion-basedmethodscaninfactbederivedwithoutthe
conceptoftime-reversal. WerefertoAppendixA.3fortheproofandfurtherdetails.
Proposition3.1(Equivalencetotrajectory-basedmethods). TheBSDEversionsofourlossesare
equivalenttopreviouslyexistinglossesinthefollowingsense.
(i) Assumingthereparametrizationµ (cid:101)=f +σuandσ⊤∇V(cid:101) =u+v,itholds
LB loS gD FPE(µ (cid:101),V(cid:101))=LB BS riD dgE e(u,v), (24)
whereLBSDE isderivedin[57].
Bridge
(ii) Itholds
Lanneal,BSDE(µ)=LBSDE (µ), (25)
logFP (cid:101) CMCD (cid:101)
where LBSDE refers to (a version of) the Controlled Monte Carlo Diffusion (CMCD) loss
CMCD
derivedin[69].
6(iii) Assumingthereparametrizationµ (cid:101)=f +σuandσ⊤∇V(cid:101) =u,itholds
LBSDE(V(cid:101))=LBSDE(u), (26)
score DIS
whereLBSDEreferstotheTime-ReversedDiffusionSampler(DIS)lossderivedin[5].
DIS
Remark 3.2 (Numerical implications of PINN- and BSDE-based losses). From a numerical per-
spective, the derived PINN- and BSDE-based losses have advantages and disadvantages. Since
BSDE-basedmethodsbuildonastochasticrepresentationofthePDE,neithersecond-ordernortime
derivatives have to be computed. This also leads to the fact that for our sampling problems, the
gradientsofthesolutions(usuallycorrespondingtothelearneddrift)canbelearneddirectly. Itcomes
attheprice,however,thatonlystochasticdynamicscanbeapproached. PINN-basedlosses,onthe
otherhand,aremoregeneral,e.g.,theycanbereadilyappliedtodeterministicevolutionsaswell.
Moreover,theyaresimulation-freeanddonotrelyontime-discretization,overallresultinginlower
timespergradientstepsformoderatedimensions. Furthermore,off-policytrainingbasicallycomes
bydesign,whichmightbeadvantageousformodediscovery.
Remark3.3(Subtrajectory-basedlosses). Anotherequivalencecanbededucedwhenconsideringthe
diffusionlossintroducedin[48]insteadoftheBSDEloss,whichdoesnotaimtolearnItô’sformula
ontheentiretimeinterval,butratheronsubintervals[t ,t ]⊂[0,T],whichmayberandomlydrawn
0 1
during optimization. Along the lines of Proposition 3.1, one can readily show that applying the
diffusionlosstothelog-transformedFokker-Planckequation(7),onecanrecoversubtrajectory-based
lossessuggestede.g. in[77],seealso[57,AppendixA.7].
4 Gauss-NewtonmethodsforimprovedconvergenceofPINNs
Trainingphysics-informedneuralnetworkscanbechallenging. Itiswell-documentedintheliterature
thatdifferentialoperatorsinthelossfunctioncomplicatethetrainingandcanleadtoill-conditioning
[14,30,73]. Atthesametime,accuratesolutionsarecrucialforachievinghighsamplingquality. To
obtainoptimalresultsinPINNtraining,wethereforecombinetheAdamoptimizerwithaGauss-
Newton method which we derive from an infinite-dimensional function space perspective. This
viewpointhasrecentlybeenexploredin[40,41].
Gauss-Newtonmethodinfunctionspace. Weconsiderlossfunctionsoftheform
(cid:20)(cid:16) (cid:17)2(cid:21)
L(V(cid:101)):=E R(V(cid:101))(ξ,τ) , (27)
whereRisanonlinearPDEoperator. Forexample,derivedfrom(15),wesetR=R tobe6
score
R score(V(cid:101))=∂ tV(cid:101) −div(f)−∇V(cid:101) ·f + 1 2∥σ⊤∇V(cid:101)∥2+ 21Tr(σσ⊤∇2V(cid:101)).
TooptimizeLinfunctionspace,asensiblechoiceisGauss-Newton’smethodfornonlinearleast-
squaresproblems,duetoitslocalquadraticconvergenceproperties[16]anddocumentedsuccess
inPINNtraining[25],whicharetobecontrastedtomuchslowerratesoffirst-ordermethodslike
gradientdescent[45].TherationaleofGauss-NewtonistolinearizeRintheleastsquaresformulation
(27)andtosolvetheresultingquadraticminimizationproblemateverystep.Moreprecisely,choosing
astartvalueV(cid:101)0,weoptimizeLvia
V(cid:101)k+1 =V(cid:101)k−[DR score(V(cid:101)k)∗DR score(V(cid:101)k)]−1(DL(V(cid:101)k)), k =0,1,2,... (28)
whereDLandDRdenotetheFréchetderivativesofLandR,respectively,andDR(V(cid:101)k)∗ isthe
adjointofDR(V(cid:101)k). Inthecaseoftheexampleabove,i.e.,equation(15),itholds
DR score(V(cid:101))[δ V(cid:101)]=∂ tδ
V(cid:101)
−∇δ
V(cid:101)
·f +σ⊤∇V(cid:101) ·∇δ
V(cid:101)
+Tr(σσ⊤∇2δ V(cid:101)) (29)
andthecomputationoftheinverseentailssolvingthefollowingPDEateverystepoftheiteration:
Findδ suchthatforallδ¯ (inasuitabletestspace)itholds
V(cid:101) V(cid:101)
(cid:104) (cid:105)
E DR score(V(cid:101)k)[δ V(cid:101)](ξ,τ)DR score(V(cid:101)k)[δ¯ V(cid:101)](ξ,τ) =DL(V(cid:101)k)(δ¯ V(cid:101)).
6Here,weassumedthatinitialandfinalconditionsareexactlysatisfied,asdescribedinSection3.1.
7logCE: GMM logCE: Many-well
marginal marginal
histogram histogram
7.5 5.0 2.5 0.0 2.5 5.0 7.5 3 2 1 0 1 2 3
x x
Figure2: Thegroundtruthmarginalinthefirstdimensionandhistogramsofsamplesfromourbest
performingmethodusingthelossL ontheGMM(left)andmany-well(right)examples.
logCE
Totransferthisfunctionspaceoptimizationtoacomputablealgorithmforneuralnetworkoptimization,
wediscretizeitinthetangentspaceoftheneuralnetworkansatz. Theadvantageofthisapproach
isthatweareguaranteedtofollowthedynamicsof(28)uptoaprojectionontothetangentspace
[41,Theorem1]. TomakethedependenceofaneuralnetworkapproximationV(cid:101) onthetrainable
parametersexplicit,wewriteV(cid:101) =V θ. Here,thevectorθ ∈Rpcollectstheptrainableparametersof
theneuralnetworkansatz. Discretizingthealgorithm(28),weobtainaniterationoftheform
θ =θ −η G(θ )†∇L(θ ), k =0,1,2,...
k+1 k k k k
whereL(θ)=L(V )and∇L(θ)denotesthegradientofLw.r.t. θ,typicallycomputedviaautomatic
θ
differentiation. Byη > 0wedenoteastep-sizeandG(θ )† istheMoore-Penroseinverseofthe
k k
GramianG(θ ). ThematrixG(θ )isderivedfromtheoperatorDR (V )∗DR (V )via
k k score θk score θk
G(θ )
=E(cid:2)
DR (V )[∂ V ](ξ,τ)DR (V )[∂ V
](ξ,τ)(cid:3)
.
k ij score θk θi θk score θk θj θk
It is detailed in [41, Appendix C] that this approach corresponds to the standard Gauss-Newton
methodforasuitablychosenresidual. AsastandardGauss-Newtonmethod,itcanbeimplemented
inamatrix-freeway[61],relyingonaniterativesolver,suchastheconjugategradientmethodto
obtainG(θ )†∇L(θ). Inpractice,weuseanadditivedamping,i.e.,weuseG(θ )+εIinsteadof
k k
G(θ ),forsomeε>0,whichguaranteesinvertibilityofthematrix.
k
5 Numericalexperiments
Inthissection,weevaluateourPINN-basedlossesondifferentbenchmarkproblems. Specifically,
we consider the losses listed in Table 1 and compare them with state-of-the-art trajectory-based
methods. Forthebenchmarkproblems,wefollow[57]andconsideraGaussianmixturemodelas
wellashigh-dimensional, multimodalmany-welldistributions, whichresembletypicalproblems
inmoleculardynamics. WerefertoAppendixCforadescriptionofthetargetsanddetailsonour
implementation. Inourexperiments,wecompareagainstthePathIntegralSampler(PIS)[78]and
theTime-ReversedDiffusionSampler(DIS)[5],includingthelog-variancelosssuggestedin[57].
OurresultsaresummarizedinTable2–inordertohaveafaircomparisonwithPISandDISwe
didnotemploytheGauss-Newtonmethodhere. Ingeneral,weseethattheODEmethodsusually
outperform the SDE methods, in particular significantly improving upon the baseline methods.
InFigure2weillustratethatwecanindeedaccuratelycoverthemodesofthetargetdistributions. We
refertoFigures3and4intheappendixforadditionalvisualizations. InTable3,wereportresults
attainedbyfine-tuningwiththeGauss-NewtonmethodderivedinSection4,showingthatwecan
indeedfurtherimprovesamplingperformance.
Ingeneral,wealsoobservethatlearningapotentialΦ(withµ=∇Φ)ratherthanthedriftµdirectly,
suchasintheSBandOTlosses,ismorechallengingandcanleadtoworseperformance.Inparticular,
theHJBregularizationonlyprovidesgoodresultsfortheODEcase. Interestingly,thereisnoclear
advantage of the methods with prescribed density (i.e., using the losses Lanneal,Lanneal,L ),
logFP logCE score
indicatingthat,ingeneral,non-uniquenessmightimproveperformance. Fortheannealinglosses,
we presume that the performance significantly depends on the chosen annealing strategies. In
particular, the applied geometric annealing defined in Section 3.2 is known to be suboptimal for
certainprior-targetconfigurations,see,e.g.,[22,42]andAppendixC.1foranillustrativeexample.
8Table 2: Metrics for the benchmark problems in different dimensions d. We report errors for
estimating the log-normalizing constant (∆logZ) and the standard deviations of the marginals
(∆std). Furthermore, we report the normalized effective sample size (ESS) and the Sinkhorn
distance(W2)[11],seeAppendixB.3fordetails. Finally,wepresentthetimeinsecondsforone
γ
gradientstep. Thearrows↑and↓indicatewhetherwewanttomaximizeorminimizeagivenmetric.
Ourmethodsarecoloredinblue(SDE)anddarkblue(ODE).
Problem Method Loss ∆logZ ↓ W2 ↓ ESS↑ ∆std↓ sec./it. ↓
γ
GMM PIS-KL[78] 1.094 0.467 0.0051 1.937 0.503
(d=2) PIS-LV[57] 0.046 0.020 0.9093 0.023 0.500
DIS-KL[5] 1.551 0.064 0.0226 2.522 0.565
DIS-LV[57] 0.056 0.020 0.8660 0.004 0.536
SDE L 0.000 0.020 1.0000 0.004 0.011
logFP
SDE-anneal Lanneal 5.364 0.172 0.1031 0.209 0.062
logFP
SDE-score L 0.009 0.020 0.9818 0.096 0.013
score
SB L 0.002 0.020 0.9959 0.050 0.017
SB
ODE L 0.000 0.020 1.0000 0.003 0.008
logCE
ODE-anneal Lanneal 4.227 0.044 0.1427 0.753 0.020
logCE
OT L 0.005 0.057 0.9932 0.065 0.080
OT
MW PIS-KL[78] 3.567 1.699 0.0004 1.409 0.441
(d=5,m=5,δ=4) PIS-LV[57] 0.214 0.121 0.6744 0.001 0.402
DIS-KL[5] 1.462 1.175 0.0012 0.431 0.490
DIS-LV[57] 0.375 0.120 0.4519 0.001 0.437
SDE L 0.161 0.123 0.8167 0.016 0.017
logFP
SDE-anneal Lanneal 0.842 0.257 0.3464 0.004 0.014
logFP
SDE-score L 3.969 0.427 0.0124 0.004 0.026
score
SB L 7.855 0.328 0.0314 0.045 0.029
SB
ODE L 0.000 0.118 0.9993 0.000 0.008
logCE
ODE-anneal Lanneal 0.025 0.121 0.9506 0.005 0.010
logCE
OT L 0.010 0.120 0.9862 0.002 0.020
OT
MW PIS-KL[78] 0.101 6.821 0.8172 0.001 0.479
(d=50,m=5,δ=2) PIS-LV[57] 0.087 6.823 0.8453 0.000 0.416
DIS-KL[5] 1.785 6.854 0.0225 0.009 0.522
DIS-LV[57] 1.783 6.855 0.0227 0.009 0.450
SDE L 0.038 6.820 0.9511 0.001 0.050
logFP
SDE-anneal Lanneal 0.270 6.899 0.9171 0.021 0.067
logFP
SDE-score L 1.989 6.803 0.1065 0.016 0.053
score
SB L 189.71 7.552 0.0106 0.051 0.053
SB
ODE L 0.003 6.815 0.9937 0.002 0.023
logCE
ODE-anneal Lanneal 1.759 6.821 0.2100 0.017 0.043
logCE
OT L 0.104 6.824 0.9027 0.001 0.043
OT
5.1 Limitations
WenotethatourapproachassumesknowledgeofasuitablesetΩ ⊂ Rd×[0,T]forsamplingthe
randomvariableξ,i.e. thedataonwhichthePINNlossisevaluated. Weincuranapproximation
errorifthesetΩischosentoosmall. Ontheotherhand,ifitistoolarge,lowprobabilityareasof
p canleadtoinstabilitiesandmightrequireclipping. Weprovideinitialresultsforadaptive
target
methodsinAppendixCandleaveanextensiveevaluationforfuturework. Wealsomentionthat
thecomputationofdivergencesandLaplaciansusingautomaticdifferentiationcanbeprohibitivein
veryhighdimensionsandmightrequire(stochastic)estimators,suchasHutchinson’straceestimator.
Finally,itiscommonlyknownthatPINNscanbesensitivetohyperparametersettings.
9Table3: ComparisonoftrainingonlywithAdam(200k iterations)versuspretrainingwithAdam
(100kiterations)andfinetuningwiththeGauss-Newton(GN)method(500iterations)fortheloss
L . FortheGNmethod,weuseamaximumof500stepsfortheconjugategradientmethod,a
logCE
dampingofε=10−5,andaline-searchforthelearningrate. WerefertoAppendixB.3andTable2
fordetailsonthemetrics. Thearrows↑and↓indicatewhetherwewanttomaximizeorminimizea
givenmetric.
Problem Optimizer Loss↓ ∆logZ ↓ W2 ↓ 1−ESS↓ ∆std↓ sec./it.↓
γ
GMM Adam 4.62e-4 3.73e-5 2.03e-2 3.15e-5 3.16e-3 0.007
(d=2) Adam+GN 1.62e-4 2.91e-6 2.03e-2 7.23e-6 1.33e-3 6.071
MW Adam 3.27e-3 8.79e-5 1.18e-1 6.62e-4 3.06e-4 0.008
(d=5,m=5,δ=4) Adam+GN 2.57e-3 2.15e-4 1.18e-1 1.56e-4 1.32e-4 7.486
6 Conclusion
WeprovideaprincipledframeworkfordynamicalmeasuretransportbasedonSDEsthatallowsthe
useofPINNsforsamplingfromunnormalizeddensities. Inparticular,theframeworkallowsusto
learnthedriftsofSDEsorODEsinordertoendupatthetargetdensityinafinitetime. ThePDE
frameworkunifiesvarioussamplingmethodsthatarebasedon,e.g.,normalizingflows,diffusion
models,optimaltransport,andSchrödingerbridges,butalsoaddsnovelapproaches,e.g.,byaccepting
non-uniquesolutions. Moreover,ityieldsflexibleobjectivesthatarefreeoftime-discretizationsand
simulations. Webenchmarkourmethodsonmultimodaltargetdistributionswithupto50dimensions.
WhilesomeSDE-basedmethodsarestillunstable,ODE-basedvariantsyieldcompetitivemethodsthat
canoutperformvariousbaselines. Weanticipatethatourmethodscanbeimprovedevenfurtherusing
combinationswithsimulation-basedlossesaswellascommontricksforPINNs,seeAppendixC.
Acknowledgements
The research of L. Richter was partially funded by Deutsche Forschungsgemeinschaft (DFG)
throughthegrantCRC1114“ScalingCascadesinComplexSystems”(projectA05,projectnumber
235221301). J. Berner acknowledges support from the Wally Baer and Jeri Weiss Postdoctoral
Fellowship. A.AnandkumarissupportedinpartbyBrenendowedchairandbytheAI2050senior
fellowprogramatSchmidtSciences.
10References
[1] L.Ambrosio,N.Gigli,andG.Savare. GradientFlows: InMetricSpacesandintheSpaceof
ProbabilityMeasures. LecturesinMathematics.ETHZürich.BirkhäuserBasel,2005.
[2] MichaelArbel,AlexMatthews,andArnaudDoucet. AnnealedflowtransportMonteCarlo. In
InternationalConferenceonMachineLearning,pages318–330.PMLR,2021.
[3] Jean-David Benamou and Yann Brenier. A computational fluid mechanics solution to the
Monge-Kantorovichmasstransferproblem. NumerischeMathematik,84(3):375–393,2000.
[4] Jean-DavidBenamou,GuillaumeCarlier,andFilippoSantambrogio. Variationalmeanfield
games. Active Particles, Volume 1: Advances in Theory, Models, and Applications, pages
141–171,2017.
[5] JuliusBerner,LorenzRichter,andKarenUllrich. Anoptimalcontrolperspectiveondiffusion-
basedgenerativemodeling. TransactionsonMachineLearningResearch,2024.
[6] KennethFCaluyaandAbhishekHalder. WassersteinproximalalgorithmsfortheSchrödinger
bridgeproblem: Densitycontrolwithnonlineardrift. IEEETransactionsonAutomaticControl,
67(3):1163–1178,2021.
[7] NanChenandAndrewJMajda.EfficientstatisticallyaccuratealgorithmsfortheFokker–Planck
equationinlargedimensions. JournalofComputationalPhysics,354:242–268,2018.
[8] Tianrong Chen, Guan-Horng Liu, and Evangelos A Theodorou. Likelihood training of
SchrödingerBridgeusingForward-BackwardSDEstheory. arXivpreprintarXiv:2110.11291,
2021.
[9] XingyuChen,JianhuanCen,andQingsongZou. Adaptivetrajectoriessamplingforsolving
PDEswithdeeplearningmethods. arXivpreprintarXiv:2303.15704,2023.
[10] YongxinChen, TryphonTGeorgiou, andMichelePavon. Ontherelationbetweenoptimal
transportandSchrödingerbridges: Astochasticcontrolviewpoint. JournalofOptimization
TheoryandApplications,169:671–691,2016.
[11] MarcoCuturi. Sinkhorndistances: Lightspeedcomputationofoptimaltransport. Advancesin
neuralinformationprocessingsystems,pages2292–2300,2013.
[12] Paolo Dai Pra. A stochastic control approach to reciprocal diffusion processes. Applied
mathematicsandOptimization,23(1):313–329,1991.
[13] ValentinDeBortoli,JamesThornton,JeremyHeng,andArnaudDoucet. DiffusionSchrödinger
bridgewithapplicationstoscore-basedgenerativemodeling. AdvancesinNeuralInformation
ProcessingSystems,34:17695–17709,2021.
[14] TimDeRyck,FlorentBonnet,SiddharthaMishra,andEmmanueldeBézenac. Anoperator
preconditioningperspectiveontraininginphysics-informedmachinelearning. International
ConferenceonLearningRepresentations,2024.
[15] PierreDelMoral,ArnaudDoucet,andAjayJasra. SequentialMonteCarlosamplers. Journal
oftheRoyalStatisticalSociety: SeriesB(StatisticalMethodology),68(3):411–436,2006.
[16] Peter Deuflhard and Gerhard Heindl. Affine invariant convergence theorems for Newton’s
methodandextensionstorelatedmethods. SIAMJournalonNumericalAnalysis,16(1):1–10,
1979.
[17] ArnaudDoucet,NandoDeFreitas,NeilJamesGordon,etal. SequentialMonteCarlomethods
inpractice,volume1. Springer,2001.
[18] ArnaudDoucet,AdamMJohansen,etal. Atutorialonparticlefilteringandsmoothing: Fifteen
yearslater. Handbookofnonlinearfiltering,12(656-704):3,2009.
[19] LorisFelardos,JérômeHénin,andGuillaumeCharpiat. Designinglossesfordata-freetraining
ofnormalizingflowsonBoltzmanndistributions. arXivpreprintarXiv:2301.05475,2023.
11[20] DavidLopesFernandes,FranciscoVargas,CarlHenrikEk,andNeillDFCampbell. Shooting
Schrödinger’scat. InFourthSymposiumonAdvancesinApproximateBayesianInference,2021.
[21] WendellHFlemingandRaymondWRishel. Deterministicandstochasticoptimalcontrol,
volume1. SpringerScience&BusinessMedia,2012.
[22] Andrew Gelman and Xiao-Li Meng. Simulating normalizing constants: From importance
samplingtobridgesamplingtopathsampling. Statisticalscience,pages163–185,1998.
[23] PaulGlasserman. MonteCarlomethodsinfinancialengineering,volume53. Springer,2004.
[24] JiequnHan,ArnulfJentzen,etal. Deeplearning-basednumericalmethodsforhigh-dimensional
parabolicpartialdifferentialequationsandbackwardstochasticdifferentialequations. Commu-
nicationsinmathematicsandstatistics,5(4):349–380,2017.
[25] AnasJnini,FlavioVella,andMariusZeinhofer. Gauss-Newtonnaturalgradientdescentfor
physics-informedcomputationalfluiddynamics. arXivpreprintarXiv:2402.10680,2024.
[26] GurtejKanwar,MichaelSAlbergo,DenisBoyda,KyleCranmer,DanielCHackett,Sébastien
Racaniere,DaniloJimenezRezende,andPhialaEShanahan. Equivariantflow-basedsampling
forlatticegaugetheory. PhysicalReviewLetters,125(12):121601,2020.
[27] RobertEKass,BradleyPCarlin,AndrewGelman,andRadfordMNeal. MarkovchainMonte
Carloinpractice: aroundtablediscussion. TheAmericanStatistician,52(2):93–100,1998.
[28] DiederikKingma,TimSalimans,BenPoole,andJonathanHo. Variationaldiffusionmodels.
AdvancesinNeuralInformationProcessingSystems,34:21696–21707,2021.
[29] Takeshi Koshizuka and Issei Sato. Neural Lagrangian Schrödinger bridge. arXiv preprint
arXiv:2204.04853,2022.
[30] AditiKrishnapriyan,AmirGholami,ShandianZhe,RobertKirby,andMichaelWMahoney.
Characterizingpossiblefailuremodesinphysics-informedneuralnetworks. AdvancesinNeural
InformationProcessingSystems,34:26548–26560,2021.
[31] AlexTongLin,SamyWuFung,WuchenLi,LevonNurbekyan,andStanleyJOsher.Alternating
thepopulationandcontrolneuralnetworkstosolvehigh-dimensionalstochasticmean-field
games. ProceedingsoftheNationalAcademyofSciences,118(31):e2024713118,2021.
[32] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow
matchingforgenerativemodeling. arXivpreprintarXiv:2210.02747,2022.
[33] Guan-HorngLiu,TianrongChen,OswinSo,andEvangelosATheodorou. Deepgeneralized
Schrödingerbridge. arXivpreprintarXiv:2209.09893,2022.
[34] JunSLiuandJunSLiu. MonteCarlostrategiesinscientificcomputing,volume10. Springer,
2001.
[35] LucaMartino,DavidLuengo,andJoaquínMíguez. Independentrandomsamplingmethods.
Springer,2018.
[36] Bálint Máté and François Fleuret. Learning interpolations between Boltzmann densities.
TransactionsonMachineLearningResearch,2023.
[37] Alex Matthews, Michael Arbel, Danilo Jimenez Rezende, and Arnaud Doucet. Continual
repeated annealed flow transport Monte Carlo. In International Conference on Machine
Learning,pages15196–15219.PMLR,2022.
[38] Laurence Illing Midgley, Vincent Stimper, Gregor NC Simm, Bernhard Schölkopf, and
JoséMiguelHernández-Lobato. Flowannealedimportancesamplingbootstrap. InNeurIPS
2022AIforScience: ProgressandPromises,2022.
[39] CharlesJMode. ApplicationsofMonteCarlomethodsinbiology,medicineandotherfieldsof
science. IntechOpen,2011.
12[40] JohannesMüllerandMariusZeinhofer. Achievinghighaccuracywithpinnsviaenergynatural
gradient descent. In International Conference on Machine Learning, pages 25471–25485.
PMLR,2023.
[41] JohannesMüllerandMariusZeinhofer. OptimizationinSciML–afunctionspaceperspective.
arXivpreprintarXiv:2402.07318,2024.
[42] RadfordMNeal. Annealedimportancesampling. Statisticsandcomputing,11(2):125–139,
2001.
[43] KirillNeklyudov,RobBrekelmans,DanielSevero,andAlirezaMakhzani. Actionmatching:
Learningstochasticdynamicsfromsamples. InInternationalconferenceonmachinelearning,
pages25858–25889.PMLR,2023.
[44] ENelson. DynamicaltheoriesofBrownianmotion. Press,Princeton,NJ,1967.
[45] JorgeNocedalandStephenJWright. Numericaloptimization. Springer,1999.
[46] ImanNodozi, JaredO’Leary, AliMesbah, andAbhishekHalder. Aphysics-informeddeep
learningapproachforminimumeffortstochasticcontrolofcolloidalself-assembly. In2023
AmericanControlConference(ACC),pages609–615.IEEE,2023.
[47] Frank Noé, Simon Olsson, Jonas Köhler, and Hao Wu. Boltzmann generators: Sampling
equilibriumstatesofmany-bodysystemswithdeeplearning. Science,365(6457):eaaw1147,
2019.
[48] NikolasNüskenandLorenzRichter. InterpolatingbetweenBSDEsandPINNs: deeplearning
forellipticandparabolicboundaryvalueproblems. arXivpreprintarXiv:2112.03749,2021.
[49] Nikolas Nüsken and Lorenz Richter. Solving high-dimensional Hamilton–Jacobi–Bellman
PDEsusingneuralnetworks:perspectivesfromthetheoryofcontrolleddiffusionsandmeasures
onpathspace. PartialDifferentialEquationsandApplications,2(4):1–48,2021.
[50] DerekOnken,SamyWuFung,XingjianLi,andLarsRuthotto. OT-Flow: Fastandaccurate
continuousnormalizingflowsviaoptimaltransport. InProceedingsoftheAAAIConferenceon
ArtificialIntelligence,volume35,pages9223–9232,2021.
[51] GeorgePapamakarios,EricTNalisnick,DaniloJimenezRezende,ShakirMohamed,andBalaji
Lakshminarayanan. Normalizing flows for probabilistic modeling and inference. J. Mach.
Learn.Res.,22(57):1–64,2021.
[52] GrigoriosAPavliotis. Stochasticprocessesandapplications. TextsinAppliedMathematics,60,
2014.
[53] MichelePavon. Stochasticcontrolandnonequilibriumthermodynamicalsystems. Applied
MathematicsandOptimization,19(1):187–202,1989.
[54] MichelePavonandAntonWakolbinger. Onfreeenergy,stochasticcontrol,andSchrödinger
processes. InModeling,EstimationandControlofSystemswithUncertainty,pages334–348.
Springer,1991.
[55] MaziarRaissi, ParisPerdikaris, andGeorgeEmKarniadakis. Physicsinformeddeeplearn-
ing(partI):Data-drivensolutionsofnonlinearpartialdifferentialequations. arXivpreprint
arXiv:1711.10561,2017.
[56] LorenzRichterandJuliusBerner. RobustSDE-basedvariationalformulationsforsolvinglinear
PDEsviadeeplearning.InInternationalConferenceonMachineLearning,pages18649–18666.
PMLR,2022.
[57] LorenzRichterandJuliusBerner. Improvedsamplingvialearneddiffusions. InInternational
ConferenceonLearningRepresentations,2024.
[58] Jack Richter-Powell, Yaron Lipman, and Ricky TQ Chen. Neural conservation laws: A
divergence-freeperspective. AdvancesinNeuralInformationProcessingSystems,35:38075–
38088,2022.
13[59] Christian P Robert, George Casella, and George Casella. Monte Carlo statistical methods,
volume2. Springer,1999.
[60] LarsRuthotto,StanleyJOsher,WuchenLi,LevonNurbekyan,andSamyWuFung. Amachine
learning framework for solving high-dimensional mean field game and mean field control
problems. ProceedingsoftheNationalAcademyofSciences,117(17):9183–9193,2020.
[61] NicolNSchraudolph. Fastcurvaturematrix-vectorproductsforsecond-ordergradientdescent.
Neuralcomputation,14(7):1723–1738,2002.
[62] YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,and
Ben Poole. Score-based generative modeling through stochastic differential equations. In
InternationalConferenceonLearningRepresentations,2020.
[63] GabrielStoltz,MathiasRousset,etal. Freeenergycomputations: Amathematicalperspective.
WorldScientific,2010.
[64] KejunTang,XiaoliangWan,andChaoYang. DAS-PINNs: Adeepadaptivesamplingmethod
forsolvinghigh-dimensionalpartialdifferentialequations. JournalofComputationalPhysics,
476:111868,2023.
[65] Alexander Tong, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks,
KilianFatras,GuyWolf,andYoshuaBengio. Improvingandgeneralizingflow-basedgenerative
modelswithminibatchoptimaltransport. InICMLWorkshoponNewFrontiersinLearning,
Control,andDynamicalSystems,2023.
[66] MAntóniaAmaralTurkman,CarlosDanielPaulino,andPeterMüller. ComputationalBayesian
statistics: anintroduction,volume11. CambridgeUniversityPress,2019.
[67] LorenzVaitl,KimANicoli,ShinichiNakajima,andPanKessel. Gradientsshouldstayonpath:
betterestimatorsofthereverse-andforwardKLdivergencefornormalizingflows. Machine
Learning: ScienceandTechnology,3(4):045006,2022.
[68] Francisco Vargas, Will Grathwohl, and Arnaud Doucet. Denoising diffusion samplers. In
InternationalConferenceonLearningRepresentations,2023.
[69] FranciscoVargasandNikolasNüsken. Transport,variationalinferenceanddiffusions: with
applications to annealed flows and Schrödinger bridges. arXiv preprint arXiv:2307.01050,
2023.
[70] FranciscoVargas,PierreThodoroff,AustenLamacraft,andNeilLawrence.SolvingSchrödinger
bridgesviamaximumlikelihood. Entropy,23(9):1134,2021.
[71] Martin J Wainwright, Michael I Jordan, et al. Graphical models, exponential families, and
variationalinference. FoundationsandTrendsinMachineLearning,1(1–2):1–305,2008.
[72] Sifan Wang, Shyam Sankaran, Hanwen Wang, and Paris Perdikaris. An expert’s guide to
trainingphysics-informedneuralnetworks. arXivpreprintarXiv:2308.08468,2023.
[73] SifanWang, YujunTeng, andParisPerdikaris. Understandingandmitigatinggradientflow
pathologies in physics-informed neural networks. SIAM Journal on Scientific Computing,
43(5):A3055–A3081,2021.
[74] HaoWu, JonasKöhler, andFrankNoé. Stochasticnormalizingflows. AdvancesinNeural
InformationProcessingSystems,33:5933–5944,2020.
[75] Liu Yang and George Em Karniadakis. Potential flow generator with l2 optimal transport
regularityforgenerativemodels. IEEETransactionsonNeuralNetworksandLearningSystems,
33(2):528–538,2020.
[76] BenjaminJZhangandMarkosAKatsoulakis. Amean-fieldgameslaboratoryforgenerative
modeling. arXivpreprintarXiv:2304.13534,2023.
14[77] DinghuaiZhang,RickyTianQiChen,Cheng-HaoLiu,AaronCourville,andYoshuaBengio.
Diffusion generative flow samplers: Improving learning signals through partial trajectory
optimization. arXivpreprintarXiv:2310.02679,2023.
[78] QinshengZhangandYongxinChen. PathIntegralSampler: astochasticcontrolapproachfor
sampling. InInternationalConferenceonLearningRepresentations,2022.
15A Theoreticalaspects
A.1 DetailsonPINN-basedlosses
Inthissection,wewillelaborateondetailsregardingthePINN-basedlossesintroducedinSection3.
WefirstremarkthatundermildconditionstheFokker-Planckandcontinuityequationsin(4)and(5)
(cid:82)
aremassconserving,i.e.,∂ p (x,t)dx=0fort∈[0,T]. Inparticular,sinceourinitialcondition
t X
p isnormalized,thesolutionp (·,t)needstoalsobenormalizedforallt∈[0,T]. Wethusneed
prior X
tomakeourparametrization(10),i.e.,
V(cid:101)φ,z(·,t)= Tt logρt za (r tg )et +(cid:0) 1− Tt(cid:1) logp prior+ Tt (cid:0) 1− Tt(cid:1) φ(·,t), (30)
sufficientlyexpressive. Fortheannealingcase(i.e.,whenconsideringthelossesLannealorLanneal),
logFP logCE
wethereforeneedtouseatime-dependentfunctionz ∈C([0,T],R)(asopposedtoaconstant),since
otherwisep
X
=exp(V(cid:101))could,ingeneral,notbeanormalizeddensityfort∈(0,T). Notethatif
V(cid:101)φ,z = V,i.e. ifitfulfillsthelog-transformedFokker-Planckequation(7),conservationofmass
impliesthatz(T)=Z andthustheterminalconditionV(cid:101)φ,z(·,T)=logp targetissatisfied.
A.2 SchrödingerbridgesandHamilton-Jacobi-Bellmanequation
Letuspresentasketchoftheproofthattheoptimaldriftforaprescribeddensitycanbewrittenasa
gradientfield,which,inthecaseofSchrödingerbridgeoroptimaltransportproblems,solvesanHJB
equation,seealso[3,6,29,43,54]. Letusconsidertheoptimizationproblem
1(cid:90) T (cid:90)
inf ∥µ(x,t)∥2p(x,t)dxdt (31a)
µ 2 0 Rd
1
s.t. ∂ p=−div(pµ)+ Tr(σσ⊤∇2p), p(·,0)=p p(·,T)=p , (31b)
t 2 prior target
forasufficientlysmoothdensityp. IntroducingaLagrangemultiplierΦ: Rd×[0,T]→R,wecan
rewritetheproblemas
(cid:90) T (cid:90) (cid:18) 1 (cid:18) 1 (cid:19)(cid:19)
supinf ∥µ∥2p+Φ ∂ p+div(pµ)− Tr(σσ⊤∇2p) dxdt, (32)
Φ µ 0 Rd 2 t 2
whereweomithereandinthefollowingtheargumentsofthefunctionsfornotationalconvenience.
Usingintegrationbyparts,wecancalculate
(cid:90) T (cid:90) T
(cid:2) (cid:3)t=T
Φ∂ pdt= Φp − p∂ Φdt. (33)
t t=0 t
0 0
and
(cid:90) (cid:90)
ΦTr(σσ⊤∇2p)dx= pTr(σσ⊤∇2Φ)dx, (34)
Rd Rd
whereweassumethatpanditspartialderivativesvanishsufficientlyfastatinfinity. Usingtheproduct
ruleandStokes’theorem,weobtainthat
(cid:90) (cid:90) (cid:90) (cid:90)
Φdiv(pµ)dx= div(Φpµ)dx− pµ·∇Φdx=− pµ·∇Φdx. (35)
Rd Rd Rd Rd
LeveragingFubini’stheoremandcombiningthelastthreecalculationswith(32),weobtainthat
supinf
(cid:90) (cid:90) T (cid:18)(cid:18)(cid:18) 1 ∥µ∥2−µ·∇Φ(cid:19) p−(cid:18)
∂ Φ+
1 Tr(σσ⊤∇2Φ)(cid:19) p(cid:19) dt+(cid:2) Φp(cid:3)t=T(cid:19)
dx.
Φ µ Rd 0 2 t 2 t=0
(36)
Inviewofthebinomialformula,weobservethattheminimizerisgivenby
µ=∇Φ. (37)
Wecanthuswrite(36)as
inf (cid:90) (cid:90) T (cid:18)(cid:18) ∂ Φ+ 1 ∥∇Φ∥2+ 1 Tr(σσ⊤∇2Φ)(cid:19) pdt−(cid:2) Φp(cid:3)t=T(cid:19) dx, (38)
Φ Rd 0 t 2 2 t=0
16which corresponds to the action matching objective in [43]. We also refer to [1, Theorem 8.3.1]
for existence and uniqueness results. If we additionally minimize (31a) over all densities p with
p(·,0)=p andp(·,T)=p ,weobtaintheproblem
prior target
inf (cid:90) (cid:90) T (cid:18)(cid:18) ∂ Φ+ 1 ∥∇Φ∥2+ 1 Tr(σσ⊤∇2Φ)(cid:19) pdt−(cid:2) Φp(cid:3)t=T(cid:19) dx, (39)
Φ,p Rd 0 t 2 2 t=0
Computingthefunctionalderivativew.r.t.p,weobtainthefirst-orderoptimalitycondition
1 1
∂ Φ=− Tr(σσ⊤∇2Φ)− ∥∇Φ∥2, (40)
t 2 2
whichyieldstheHamilton-Jacobi-Bellmanequationin(20).
A.3 BSDE-basedlossesandequivalenceswithdiffusion-basedsamplingmethods
Inthischapter,wegivesomebackgroundonBSDE-basedlossesforPDEsandwillshowthatwithour
PDEframeworkwecanrecoveralreadyexistinglossesthathavemostlybeenderivedinthecontext
ofdiffusion-basedgenerativemodeling. Thisapproachusuallyreliesontheconceptoftime-reversal
ofSDEs. Tobemoreprecise,theideaistoconsiderthetwocontrolledSDEs
dXu =(f +σu)(Xu,s)ds+σ(s)dW , Xu ∼p , (41)
s s s 0 prior
dYv =(−f +σv)(Yv,s)ds+σ(s)dW , Yv ∼p , (42)
s s s 0 target
wheref ∈C(Rd×[0,T],Rd)andσ ∈C(Rd×[0,T],Rd×d)arefixedandthecontrolfunctionsu
andvarelearnedsuchthatattheoptimumu=u∗andv =v∗,Xu∗ isthetime-reversalofYv∗,see
[57]. Clearly,ifthetime-reversalpropertyisfulfilled,wehaveXu∗ ∼p andthussolvedour
T target
samplingproblem. Theabovesettingcorrespondstoageneralbridgebetweenthepriorandtarget
densityand–justlikeinourgeneralsettinginSection3.1–hasinfinitelymanysolutions. Twoways
toattainuniquenessaretoeithersetv =0andchoosef suitablysuchthatp (·,T)≈p ,which
Y prior
correspondstoscore-basedgenerativemodeling,see[57,Section3.2],ortoconstraintoanannealing
strategybetweenp priorandp target,i.e. toprescribep Xu∗,see[69].
Finally,beforeprovingthelossequivalencesfromProposition3.1,letusbrieflyintroduceBSDE-
basedlosses. Formoredetails,wereferto[48]. BSDE-basedlossesbuildonastochasticrepresenta-
tionofthePDE,whichisessentiallycomingfromItô’sformula,whichstates
V(X ,T)−V(X
,0)=(cid:90) T (cid:18)
∂ V +
1 Tr(cid:0) σσ⊤∇2V(cid:1) +µ·V(cid:19)
(X ,s)ds
T 0 s 2 s
0 (43)
(cid:90) T
+ σ⊤∇V(X ,s)·dW ,
s s
0
whereX isdefinedbytheSDE
dX =µ(X ,s)ds+σ(s)dW . (44)
s s s
Now,foraPDE
∂ V + 1 Tr(cid:0) σσ⊤∇2V(cid:1) +µ·V +h(·,·,V,∇V,∇2V)=0, (45)
t 2
whereh∈C(Rd×[0,T]×R×Rd×Rd×d,R)isapossiblynonlinearfunctionthatmaydependon
thesolutionV andtheirderivatives,wemayturn(43)into
(cid:90) T (cid:90) T
V(X ,T)−V(X ,0)=− h(cid:0) ·,·,V,∇V,∇2V)(cid:1) (X ,s)ds+ σ⊤∇V(X ,s)·dW . (46)
T 0 s s s
0 0
ThegeneralideaofBSDE-basedlossesisnowtolearnanapproximationV(cid:101) ≈V s.t. (46)isfulfilled,
e.g. viatheloss
(cid:34)(cid:32)
(cid:90) T (cid:16) (cid:17)
LBSDE(V(cid:101))=E V(cid:101)(X T,T)−V(cid:101)(X 0,0)+ h ·,·,V(cid:101),∇V(cid:101),∇2V(cid:101)) (X s,s)ds
0
(47)
(cid:33)2(cid:35)
(cid:90) T
− σ⊤∇V(cid:101)(X s,s)·dW
s
,
0
wheretypicallyatleastoneofthevaluesV(cid:101)(X 0,0)andV(cid:101)(X T,T)canbereplacedbytherespective
boundaryvaluesofthePDE.WecannowproveProposition3.1.
17
⃗ ⃗ ⃗ ⃗ProofofProposition3.1. (i)LetusstartwithL fromSection3.1andrecallthecorresponding
logFP
PDE(7),namely
∂ V +div(µ)+∇V ·µ− 1∥σ⊤∇V∥2− 1Tr(σσ⊤∇2V)=0. (48)
t 2 2
Pickingµ=f +σu∗,asintheSDE(41),wecanwrite
∂ V+div(f+σu∗)+∇V·(f+σu∗)−1∥σ⊤∇V∥2+1Tr(σσ⊤∇2V)−Tr(σσ⊤∇2V)=0. (49)
t 2 2
ApplyingtheBSDElossbrings
LBSDE(u,V(cid:101))=
logFP
(cid:34)(cid:32)
(cid:90) T (cid:16) (cid:17)
E div(f +σu)+σ⊤∇V(cid:101) ·(u−w)− 1∥σ⊤∇V(cid:101)∥2−Tr(σσ⊤∇2V(cid:101)) (Xw,s)ds
2 s
0
(cid:33)2(cid:35)
(cid:90) T p (Xw)
− σ⊤∇V(cid:101)(X sw,s)·dW s+log ptarget (XwT
)
,
0 prior 0
(50)
whereXw isdefinedby
dXw =(f +σw)(Xw,s)ds+σ(s)dW , Xw ∼p , (51)
s s s 0 prior
notingthatuhasbeenreplacedbyagenericforwardcontrolw,see,e.g. [48,Section5.2.1]. Sincethe
PDE(49)dependsonthetwofunctionsu∗andV,theBSDElossnowalsodependsontwounknowns
insteadofonlyone,asdefinedin(47). Now,consideringthetime-reversedSDEYv givenby
dYv =(−f +σv)(Xw,s)ds+σ(s)dW , Yv ∼p , (52)
s s s 0 target
werecallNelson’srelation
u∗+v∗ =σ⊤∇logp Xu∗ =σ⊤∇V, (53)
whichrelatestheoptimalcontrolstothesolutionV =logp Xu∗ [44]. Inserting(53)into(50),weget
(cid:34)(cid:32) (cid:90) T (cid:18) (cid:18) v−u(cid:19)(cid:19)
LBSDE(u,v)=E div(f −σv)−(u+v)· w+ (Xw,s)ds
Bridge 2 s
0
(54)
(cid:33)2(cid:35)
(cid:90) T p (Xw)
− (u+v)(Xw,s)·dW +log target T ,
s s p (Xw)
0 prior 0
whichisthelossderivedin[57]whenreplacingthevariancewiththesecondmoment,seealsothe
commentsin[57,AppendixA.2]and[49].
(ii)TheequivalenceoftheBSDEversionoftheannealedloss,Lanneal,BSDE,withLBSDE canbe
logFP CMCD
seenbyfirstnotingthatthePDE(48)withfixedV leadstotheBSDEloss
(cid:34)(cid:32)
(cid:90) T
LBSDE(µ)=E (cid:0) div(µ)+∇V ·(µ−γ)− 1∥σ⊤∇V∥2−Tr(σσ⊤∇2V)(cid:1) (Xγ,s)ds
logFP (cid:101) (cid:101) (cid:101) 2 s
0
(55)
(cid:33)2(cid:35)
(cid:90) T p (Xγ)
− σ⊤∇V(Xγ,s)·dW +log target T ,
s s p (Xγ)
0 prior 0
whereXγ isdefinedby
dXγ =γ(Xγ,s)ds+σ(s)dW , Xγ ∼p , (56)
s s s 0 prior
notingthatµhasbeenreplacedbyagenericforwarddriftγ. Adoptingtothechoicesin[69],we
chooseµ= 1σσ⊤∇V +∇ϕ,whereϕ∈C(Rd×[0,T],R),andnotetheidentity
(cid:101) 2
(cid:90) T (cid:90) T (cid:90) T
1 div(σσ⊤∇V)(Xγ,s)ds= σ⊤∇V(Xγ,s)◦dW − σ⊤∇V(Xγ,s)·dW , (57)
2 s s s s s
0 0 0
18
⃗ ⃗ ⃗ ⃗wherethefirststochasticintegralistheStratonovichintegral. Pluggingthosechoicesinto(55),we
readilyrecoverthelossin[69]whenreplacingthevariancewiththesecondmoment(andtaking
γ = µ, inwhich case, however, one mustassurethatno gradientsw.r.t. thedriftin theSDEare
(cid:101)
taken),namely
(cid:34)(cid:32)
(cid:90) T
LBSDE (ϕ)=E (cid:0) ∆ϕ− 1∥σ⊤∇V∥2+∇V ·(µ−γ)(cid:1) (Xγ,s)ds
CMCD 2 (cid:101) s
0
(58)
(cid:33)2(cid:35)
(cid:90) T p (Xγ)
− σ⊤∇V(Xγ,s)◦dW +log target T ,
s s p (Xγ)
0 prior 0
seealsothecommentsabove. Byslightlyabusingnotation,wemayagainset∇ϕ=µsuchthatwe
(cid:101)
canwriteLBSDE (µ)insteadofLBSDE (ϕ).
CMCD (cid:101) CMCD
(iii)RecallingPDE(15),
∂ V −div(f)−∇V ·f + 1∥σ⊤∇V∥2+ 1Tr(σσ⊤∇2V)=0, (59)
t 2 2
wegettheBSDE-basedloss
(cid:34)(cid:32)
(cid:90) T
LBSDE(V(cid:101))=E (cid:0) div(−f)+ 1∥σ⊤∇V∥2−σ⊤∇V ·w(cid:1) (Xw,s)ds
score 2 s
0
(60)
(cid:33)2(cid:35)
(cid:90) T p (Xw)
− σ⊤∇V(Xw,s)·dW +log target T ,
s s p (Xw)
0 prior 0
whereXw isdefinedby
dXw =(−f +σw)(Xw,s)ds+σ(s)dW , Xw ∼p . (61)
s s s 0 prior
Makingthechoiceu=σ⊤∇V,thelossthenturnsinto
(cid:34)(cid:32)
(cid:90) T
LBSDE(u)=E (cid:0) div(−f)+ 1∥u∥2−u·w(cid:1) (Xw,s)ds
DIS 2 s
0
(62)
(cid:33)2(cid:35)
(cid:90) T p (Xw)
− u(Xw,s)·dW +log target T ,
s s p (Xw)
0 prior 0
whichcorrespondstotheDISmethodderivedin[5]whentakingthesecondmomentinsteadofthe
variance,seealso[57,Section3.2].
B Computationalaspects
B.1 Implementation
Neuralnetworks: Weperformedagrid-searchoverdifferentarchitecturechoices.Forthenetworks
µ,φ,andΦ,weexperimentedwithbothFourier-MLPsasin[78]andstandardMLPswithresidual
connections. In settings where we need to compute Laplacians of our network, we additionally
considered the OT-Flow architecture [50, 60]. For the annealing losses, we parametrize z by a
smallFourier-MLP.Fortheothermethods,zdoesnotneedtodependont,andwejustuseasingle
trainable parameter, see also Appendix A.1. For the loss L , we additionally experimented
score
with parametrizations of V(cid:101) that omit the prior density p
prior
and normalizing constant z in (10),
seeSection3.2.
Hyperparameters: Forallmethods,wechoosep =N(0,I)anduseddomainsoftheform
prior
Ω=(cid:8) (x,t)∈Rd×[0,T]: tΩ +(1−t)Ω ≤x≤tΩ +(1−t)Ω (cid:9) , (63)
target prior target prior
wheretheinequalitiesaretobeunderstoodcomponentwise. Wetunedtherectangulardomainsof
thepriorandtargetdistributionsΩ ,Ω ,Ω ,Ω ∈Rd foreachproblem. Moreover,
prior prior target target
wesetσ toaconstantvalue,i.e.,σ(t) = σ¯I. Forthediffusionmodel,wepickasimpleVP-SDE
from[62]withf(x,t):=−σ¯2xandsufficientlylargeσ¯ andT toensurethatp (·,T)≈p . For
2 √ Y prior
theothermethods,wechooseT =1andσ¯ ∈{0, 2}.
19Trainingandinference: EachexperimentisexecutedonasingleGPU.Wetrainwithbatch-size
4096for200kgradientsteps(oruntilconvergence)usingtheAdamoptimizerwithanexponentially
decayinglearningrate. Weperformedagrid-searchoverthepenaltyparameterαoftheHJBlosses
in (22) and (23), the initial learning rate as well as its decay per step. We use 100k samples to
evaluateourmethodsandsimulateourSDEsandODEsusingtheEuler-MaruyamaandFourth-order
Runge-Kutta(with3/8rule)scheme,respectively.
B.2 Log-likelihoodsandimportanceweights
Thissectiondescribeswaystocomputethelog-likelihoodandimportanceweightsforsamplesX
T
obtainedfromthestochasticprocessX.
ODEs: Inthesettingofnormalizingflows,wecancomputetheevolutionofthelog-densityalong
thetrajectories. Using dX =µ(X ,t)aswellas(8),onecanshowthat
dt t t
d
V(X ,t)=(∇V ·µ−div(µ)−∇V ·µ)(X ,t)=−div(µ)(X ,t), (64)
dt t t t
whichisoftenreferredtoasthechange-of-variablesformula. RecallingthatV = logp ,wecan
X
thencomputethe(unnormalized)importanceweights
ρ
w(k) := target(X(k)) (65)
p T
XT
ofsamples(X(k))K byintegrating(64).
T k=1
SDEs: Ifwehave(anapproximationto)thescore∇V =∇logp ofanSDEX,wecantransform
X
itintoanODEwiththesamemarginalsusing
1
µ =µ − σσ⊤∇V. (66)
ODE SDE 2
TheaboverelationcanbeverifiedviatheFokker-Planckequation(4),andtheresultingODEisoften
referredtoasprobabilityflowODE[62]. Notethatthisalsoallowsustousethechange-of-variables
formulain(64)forSDEs.
Thelog-likelihoodscanbesimulatedtogetherwiththeODEin(3)andallowustocomputeimportance
weights in the target space Rd. If the optimal drift of the SDE can be described via a change of
path measures, such as for the annealed flows [69] or diffusion models [5], we can also perform
importancesamplinginpathspaceC([0,T],Rd),see,e.g.,[5,AppendixA.12]forfurtherdetails.
B.3 Metrics
Weevaluatetheperformanceofourmethodsonthefollowingmetrics.
Normalizingconstants: Wecouldobtainanestimatelogz(T)ofthelog-normalizingconstant
logZ byourparametrizationin(10). However,sinceweareinterestedinthesamplequalityofour
models,weusethelog-likelihoodtocomputealowerboundforlogZ,seeAppendixB.2. Notethat
wedonotemployimportancesamplingforestimatingthelog-normalizingconstant.
Standarddeviations: Wealsoanalyzetheerrorwhenapproximatingcoordinate-wisestandard
deviationsofthetargetdistributionp ,i.e.,
target
1 (cid:88)d (cid:113)
V[X ], where X ∼p , (67)
d T,i T target
k=1
usingsamples(X(k))K fromourmodeltoapproximatethevariance.
T k=1
Effectivesamplesize: Onewouldliketohavethevarianceoftheimportanceweightssmall,or,
equivalently,maximizethe(normalized)effectivesamplesize
(cid:16) (cid:17)2
(cid:80)K w(k)
k=1
ESS:= . (68)
n(cid:80)K (w(k))2
k=1
ThecomputationoftheimportanceweightsisoutlinedinAppendixB.2.
205.0 5.0
2.5 2.5
0.0 0.0
2.5 2.5
5.0 5.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5
t t x
(a)SDE(L )
logFP
5.0 5.0
2.5 2.5
0.0 0.0
2.5 2.5
5.0 5.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5
t t x
(b)SDE-anneal(Lanneal)
logFP
5.0 5.0
2.5 2.5
0.0 0.0
2.5 2.5
5.0 5.0
0.0 0.5 1.0 1.5 2.0 2.5 0.0 0.5 1.0 1.5 2.0 2.5 7.5 5.0 2.5 0.0 2.5 5.0 7.5
t t x
(c)SDE-score(L )
score
5.0 5.0
2.5 2.5
0.0 0.0
2.5 2.5
5.0 5.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5
t t x
(d)SB(L )
SB
Figure3: TrajectoriesandmarginalsofourconsideredSDE-basedmethodsfortheGMMexample.
NotethatwealsoshowthecorrespondingODEspecifiedin(66)thatcanbeusedtoevaluatethe
log-likelihoods,seeAppendixB.2. Weprovideanexplanationforthesuboptimalperformanceof
LannealinAppendixC.1.
logFP
C Experiments
Inthefollowing,wedescribeourtargetdistributionsinmoredetail.
Gaussianmixturemodel(GMM): Weconsiderthedensity
m
1 (cid:88)
ρ (x)=p (x)= N(x;µ ,Σ ). (69)
target target m i i
i=1
Following[78],wechoosem=9,Σ =0.3I,and
i
(µ )9 ={−5,0,5}×{−5,0,5}⊂R2 (70)
i i=1
toobtainwell-separatedmodes.Theperformanceofourconsideredmethodsonthistargetdistribution
isvisualizedinFigures3and4.
Many-well(MW):Atypicalprobleminmoleculardynamicsconsiderssamplingfromthestationary
distributionofLangevindynamics. Inourexampleweshallconsiderad-dimensionalmany-well
21
x
x
x
x
x
x
x
x5.0
2.5
0.0
2.5
5.0
0.0 0.2 0.4 0.6 0.8 1.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5
t x
(a)ODE(L )
logCE
5.0
2.5
0.0
2.5
5.0
0.0 0.2 0.4 0.6 0.8 1.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5
t x
(b)ODE-anneal(Lanneal)
logCE
5.0
2.5
0.0
2.5
5.0
0.0 0.2 0.4 0.6 0.8 1.0 7.5 5.0 2.5 0.0 2.5 5.0 7.5
t x
(c)OT(L )
OT
Figure4: TrajectoriesandmarginalsofourconsideredODE-basedmethodsfortheGMMexample.
WeprovideanexplanationforthesuboptimalperformanceofLannealinAppendixC.1.
logCE
potential,correspondingtothe(unnormalized)density
(cid:32) m d (cid:33)
(cid:88) 1 (cid:88)
ρ (x)=exp − (x2−δ)2− x2 (71)
target i 2 i
i=1 i=m+1
withm∈Ncombineddoublewellsandaseparationparameterδ ∈(0,∞),seealso[5,74]. Note
that, due to the many-well structure of the potential, the density contains 2m modes. For these
multimodalexamples,wecancomputereferencesolutionsbynumericalintegrationsinceρ
target
factorizesinthedimensions.
C.1 Challengesinannealingstrategies
AsdescribedinSection3.2,theideaofannealingistoprescribethesolutionp (orV :=logp )as
X X
agradualpathfromp top . Itisnotsurprisingthattheactualchoiceofthispathoftenhasa
prior target
significanteffectonthenumericalperformanceoftheannealing. Inthispaper,weusethepopular
geometricpathbetweenthepriorandthetarget,whichinlog-spacecanbewrittenas
V(·,t)= t logρtarget +(cid:0) 1− t(cid:1) logp , (72)
T z(t) T prior
cf. (10)andnotingthatz(t)takescareofp beingadensityforeacht∈[0,T].
X
WehaveseeninournumericalexperimentsinSection5andinparticularinTable2thatthegeometric
annealingstrategycanleadtomoreorlesssatisfyingperformances,dependingontheproblemat
hand. FortheGMMexperiment,forinstance,theannealinglossperformanceisratherratherbad,
bothfortheSDEandtheODE.Thereasonforthiscanbeseenbylookingatthedensitypaththatis
prescribedwith(72),displayedinFigure5a. Wecanseethatthemodesofthetargetappearonly
verylateinthepath,makingthetaskoffindingthedriftµthatachievesthesedensitiesratherhard.
Lookingatthepaththatthenon-uniquelossL hasidentified,ontheotherhand,werealizethat
logCE
22
x
x
xTable4: Effectofaddingadditionalsamples(ξ,τ)alongthetrajectoriesofX forthelossL ,
logCE
seeAppendixD.1. Wesimulateandcache10ktrajectoriesofX discretizedat200timestepsevery
5kgradientsteps. Ineverygradientstep,wethencomputethelossusingarandomsubsetof4096
samples from the cache and 4096 uniformly distributed samples. Using only half the number of
iterations,i.e.,100k,wecanstillimproveuponthemetricsinTable2. Thearrows↑and↓indicate
whetherwewanttomaximizeorminimizeagivenmetric.
Problem Sampling Loss↓ ∆logZ ↓ W2↓ 1−ESS↓ ∆std↓ sec./it.↓
γ
GMM Uniform 4.62e-4 3.73e-5 2.03e-2 3.15e-5 3.16e-3 0.007
(d=2) Uniform+Traj. 2.05e-4 3.11e-6 2.03e-2 4.53e-6 1.71e-3 0.027
MW Uniform 3.27e-3 8.79e-5 1.18e-1 6.62e-4 3.06e-4 0.008
(d=5,m=5,δ=4) Uniform+Traj. 3.19e-3 4.40e-5 1.18e-1 2.54e-4 3.06e-4 0.029
MW Uniform 4.83e-2 3.43e-3 6.82 6.31e-3 2.10e-3 0.023
(d=50,m=5,δ=2) Uniform+Traj. 3.05e-1 2.17e-3 6.82 3.70e-3 2.99e-4 0.051
thetargetmodesappearmuchearlier,thusallowingfortheidentificationofthecorrespondingµ,see
Figure5b. Weleaveittofurtherresearchtocomeupwithmoreadvancedannealingstrategiesthat
sufferlessfromtheartifactsdescribedabove.
D Extensions
Inthissection,wementionpotentialextensionsofourframework.
D.1 Sampling
Let us investigate two choices of how to choose the random variables (ξ,τ) to penalize the loss
in(11). Wewillshowhowthesechoicesallowustobalanceexplorationandexploitation.
Uniform Wecansimplychose(ξ,τ) ∼ Unif(Ω)forasufficientlylargecompactsetΩ ⊂ Rd×
[0,T]. ThischoiceallowsustouniformlyexplorethedomainΩ,whichisparticularlyinterestingat
thebeginningofthetraining. Moreover,differentfrommostothermethods,wedonotneedtorelyon
(iterative)simulationsoftheSDEin(2). InordertospecifyΩ,however,weneedpriorinformationto
estimatethedomainwhereV isabovesomeminimalthreshold.
AlongtheTrajectories WecanalsosimulatetheSDEusingthepartiallylearneddriftcoefficientµ
toexploitthelearneddynamics. Thiscorrespondstothechoicesτ ∼Unif([0,T])andξ ∼X . Note
τ
thatwejustusetheSDE/ODEforsamplingthecollocationpoints,andwearenotbackpropagating
throughthesolver(toupdatethedriftµ). Inotherwords,wedetachξfromthecomputationalgraph.
InTable4,weshowthatthiscanleadtofasterandbetterconvergence. Insteadofusingthedriftµ,
onecouldalternativelysampleξ accordingtoexp(V(cid:101)(·,τ)),i.e.,thecurrentapproximationofthe
densityofX ,usingothersamplingmethods,suchastheMetropolis-adjustedLangevinalgorithm.
τ
Moreover,wewanttomentionimprovedsamplingstrategiesforPINNs,see,e.g.,[9,64]. Similar
to Quasi-Monte Carlo methods, one could also leverage low-discrepancy samplers for the time
coordinateτ,as,e.g.,usedby[28].
D.2 PINNs
WecanmakeuseofaplethoraoftricksthathavebeenproposedtostabilizethetrainingofPINNs[72].
For instance, for the networks, one could additionally consider random weight factorization and
Fourierfeaturesforthespatialcoordinates. Moreover,wecanchoosethepenaltyparameterλforthe
HJBlossL adaptivelybasedontheresidualsandtheirgradients. Finally,wecouldalsoexplore
HJB
theOT-FlowarchitectureforΦ,whichhasbeensuccessfullyemployedby[29,50,60].
23Geometric annealing for GMM
t = 0.1 t = 0.2 t = 0.3 t = 0.4 t = 0.5
5
0
5
t = 0.6 t = 0.7 t = 0.8 t = 0.9 t = 1.0
5
0
5
5 0 5 5 0 5 5 0 5 5 0 5 5 0 5
(a)Wecanobservethattheprescribedevolutionbythegeometricannealing(72)seemstobesuboptimalin
thesensethatmostmodesofthetargetonlyappearlateintheannealingpath,whichmightmakefindingthe
correspondingdriftµharder.
Learned density evolution for GMM
t = 0.1 t = 0.2 t = 0.3 t = 0.4 t = 0.5
5
0
5
t = 0.6 t = 0.7 t = 0.8 t = 0.9 t = 1.0
5
0
5
5 0 5 5 0 5 5 0 5 5 0 5 5 0 5
(b)ThegenerallossL ,ontheotherhand,optimizesµandV simultaneouslyandthusletsthealgorithm
logCE
findanannealingbyitself.
Figure5: WedisplaydifferentevolutionsoftheGaussianpriortothe2-dimensionalGMMtarget
definedin(69),oncewithaprescribedgeometricannealingdefinedin(72)andoncelearnedviathe
generallossL definedin(13).
logCE
24D.3 NoiseSchedule
Wecanconsidertime-dependentdiffusioncoefficientsσ,whichhavebeensuccessfullyemployedfor
diffusionmodels. Forinstance,wecanadapttheVP-SDEin[62]with
(cid:112) σ(t):= 2β(t)I and f(x,t):=−β(t)x, (73)
where
(cid:18)(cid:18) (cid:19) (cid:19)
1 t t
β(t):= 1− σ + σ . (74)
2 T min T max
Ourframeworkalsoallowsfordiffusioncoefficientsσ,whichdependonthespatialcoordinatex.
Finally,wecouldalsolearnthediffusion,forinstance,usingtheparametrizationσ =diag(exp(s))
foraneuralnetworks.
D.4 Mean-FieldGames
Moregenerally,wecouldextendourframeworkto(stochastic)mean-fieldgames(MFG),mean-field
controlproblems,andgeneralizedSBsusingtheobjective
(cid:34) (cid:35)
(cid:90) T
L(µ,σ)=E L(cid:0) X(t),t,µ(X(t),t)(cid:1) dt +G(p ), (75)
T
0
see[4,29,31,33,60,76]. Intheabove,theLagrangianLdefinestherunningcosts,andthefunction
GspecifiestheterminalcostsattimeT.
25
⃗ ⃗