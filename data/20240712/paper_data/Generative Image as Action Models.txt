Generative Image as Action Models
MohitShridhar1,∗,YatLongLo1,∗,StephenJames1
1DysonRobotLearningLab,∗EqualContribution
genima-robot.github.io
target target
elbow wrist
target target
base gripper
pick up the red cup open the box slide the block flip up the toilet seat move the hanger pull the microwave door
lift the lid off hang the scarf touch the button lift the slide the book put the marker
the saucepan on the stand with your elbow transparent bag and pick it up in the mug
Abstract:Image-generationdiffusionmodelshavebeenfine-tunedtounlocknew
capabilities such as image-editing and novel view synthesis. Can we similarly
unlockimage-generationmodelsforvisuomotorcontrol? WepresentGENIMA,a
behavior-cloningagentthatfine-tunesStableDiffusionto“drawjoint-actions”as
targetsonRGBimages.Theseimagesarefedintoacontrollerthatmapsthevisual
targetsintoasequenceofjoint-positions. WestudyGENIMAon25RLBenchand
9real-worldmanipulationtasks. Wefindthat,byliftingactionsintoimage-space,
internetpre-traineddiffusionmodelscangeneratepoliciesthatoutperformstate-
of-the-artvisuomotorapproaches,especiallyinrobustnesstosceneperturbations
andgeneralizingtonovelobjects. Ourmethodisalsocompetitivewith3Dagents,
despitelackingpriorssuchasdepth,keypoints,ormotion-planners.
Keywords: DiffusionModels,ImageGeneration,BehaviorCloning,Visuomotor
1 Introduction
Image-generation diffusion models [1, 2, 3] are generalists in producing visual-patterns. From
photo-realistic images [4] to abstract art [5], diffusion models can generate high-fidelity images
bydistillingmassivedatasetsofcaptionedimages[6]. Moreover, ifbothinputsandoutputsarein
image-space,thesemodelscanbefine-tunedtounlocknewcapabilitiessuchasimage-editing[7,8],
semanticcorrespondences[9,10],ornovelviewsynthesis[11,12]. Canwesimilarlyunlockimage-
generationmodelsforgeneratingrobotactions?“open the box” (1) (2)
“open the box”
SD-Turbo
Controller ...
with ControlNet
reaches targets
draws targets
Tiled RGB Targets Joint Positions
Figure1.GENIMAOverview.GENIMAisabehavior-cloningagentthatmapsRGBobservationsandlanguagegoalstojoint-positionactions.
GENIMAiscomposedoftwostages:(1)adiffusionagentthatfine-tunesSD-Turbo[27]withControlNet[8]todrawtargetjoint-positions.
Thesejoint-positionsarefromthet+Ktimestepinthedemonstration,andeachjointisrenderedasauniquely-coloredsphere.In(2),these
targetsarefedintoanACT[19,23]controllerthattranslatesthevisualtargetsintoasequenceofKjoint-positions.Thecontrolleristrained
withrandombackgroundstoignorethecontext(seeFigure2).Bothstagesaretrainedindependentlyandusedsequentiallyduringinference.
Prior works in robotics have used image-generation for subgoal generation [13, 14, 15, 16], data-
augmentation[17,18,19],andfeatures-extractionfor3Dagents[20,21]. Subgoalgenerationpre-
dictsgoalimagesastargets,however,producingexactpixel-leveldetailsaboutinteractionswithde-
formableobjects,granularmedia,andotherchaoticsystems,isofteninfeasible. Data-augmentation
methods randomize scenes with image-generation outputs to improve robustness to lighting, tex-
tures,anddistractors,buttheydonotuseimage-generationmodelstodirectlygenerateactions. 3D
agentsusepre-trainedfeaturesfromdiffusionmodelstoimprovegeneralization,however,theyrely
onprivilegedinformationsuchasdepth,keypoints,task-specificscenebounds,andmotion-planners.
In this work, we use image-generation models in their native formulation: drawing images. We
present GENIMA, a multi-task behavior-cloning agent that directly fine-tunes Stable Diffusion [1]
to “draw joint-actions”. To supervise the fine-tuning, we format expert demonstrations into an
image-to-image dataset. The input is an RGB image with a language goal, and the output is the
sameimagewithjoint-positiontargetsfromafuturetimestepinthedemonstration. Thetargetsare
rendered as colored spheres for each joint, as shown on the previous page. These visual targets
are fed into a controller that maps them to a sequence of joint-positions. This formulation frames
action-generationasanimage-generationproblemsuchthataction-patternsbecomevisual-patterns.
WestudyGENIMAon25simulatedand9real-worldtasks. InRLBench[22],GENIMAoutperforms
state-of-the-art visuomotor approaches such as ACT [19, 23] in 16/25 tasks, and DiffusionPoli-
cies [24] in 25/25 tasks. More than task performance, we show that GENIMA is robust to scene
perturbationslikerandomizedobjectcolors,distractors,andlightingchanges,andalsoingeneraliz-
ingtonovelobjects.WefindthatRGB-to-jointmethodscanapproachtheperformanceofprivileged
3Dnext-best-posemethods[25,26]withoutusingpriorslikedepth,keypoints,ormotion-planners.
We validate these results with real-world tasks that involve dynamic motions, full-body control,
transparentanddeformableobjects. Insummary,ourcontributionsare:
• Anovelproblem-formulationthatframesjoint-actiongenerationasimage-generation.
• Aproof-of-conceptsystemfordrawingandexecutingjoint-actions.
• Empiricalresultsandinsightsfromsimulatedandreal-worldexperiments.
Ourcodeandpre-trainedcheckpointsareavailableatgenima-robot.github.io.
2 GENIMA
GENIMA is a behavior-cloning agent that maps RGB observations O
t
and a language goal g into
joint-positionactionsAjoints. Thekeyobjectiveistoliftactionsintoimage-spacesuchthatinternet-
t
pretraineddiffusionmodelscanlearnaction-patternsasvisual-patterns. Weachievethiswithatwo-
stage process: (1) fine-tune Stable Diffusion [1] to draw images with target joint-positions Aimage
t+K
that are K timesteps ahead, and (2) train a controller to translate these targets into to a sequence
ofexecutablejoint-positionsAjoints = {a ,a ...a }. Thissimpletwo-stageprocessoffloadsse-
t t1 t2 tK
manticandtask-levelreasoningtoageneralistimage-generationmodel,whilethecontrollerreaches
nearbyjoint-positionsindicatedbythevisualtargets. Thesectionsbelowdescribethetwostagesin
detail,andFigure1providesanoverview.
22.1 DiffusionAgent
Thediffusionagentcontrolswhattodonext. TheagenttakesRGBobservationsO andlanguage
t
goalgasinput,andoutputsanimagewithtargetjoint-positionsAimage . Thisproblemformulation
t+K
isaclassicimage-to-imagegenerationsetup,soanydiffusionfine-tuningpipeline[7,28,29]canbe
used.WespecificallyuseControlNet[8]topreservespatiallayoutsandfordata-efficientfine-tuning.
Fine-Tuning Data. To supervise the fine-tuning, we take expert demonstrations, and randomly
sampleobservationsandtargetjoint-positionsfromt+K timesteps. Forthattimestep, weobtain
6-DoFposesofeachrobotjoint,whichisavailablethroughrobot-APIs(fromforward-kinematics).
We place spheres at those poses with pyrender1, and render them on four camera observations
O = {ofront,owrist,oleft,oright}withknownintrinsicsandextrinsics. Ona7-DoFFrankaPanda, we
t t t t t
only render four joints: base, elbow, wrist, and gripper, to avoid cluttering the image. Each
jointisrepresentedwithanidentifyingcolor,withseparatecolorsforgripperopenandclose. The
spheresincludehorizontalstripesparalleltothejoint’srotationaxis,actingasgraduationsindicating
thedegreeofrotation. Onlyhorizontalstripesareneededaseachjointhasonlyonerotationaxis.
Fine-TuningwithControlNet.Givenanimage-to-imagedataset,wefinetuneStableDiffusion[27]
withControlNet[8]todrawtargetsonobservations. ControlNetisatwo-streamarchitecture: one
streamwithafrozenStableDiffusionUNetthatgetsnoisyinputandlanguagedescriptions, anda
trainablesecondstreamthatgetsaconditioningimagetomodulatetheoutput. Thisarchitecturere-
tainsthetext-to-imagecapabilitiesofStableDiffusion,whilefine-tuningoutputstospatiallayoutsin
theconditioningimage. GENIMAusesRGBobservationsastheconditioningimagetodrawprecise
targets. WeuseSD-Turbo[27]–adistilledmodelthatcangeneratehigh-qualityimageswithin1to
4diffusionsteps–asthebasemodelforfine-tuning. WeusetheHuggingFaceimplementation[30]2
ofControlNetwithoutmodifications. SeeAppendixDformoredetailsonfine-tuning.
Tiled Diffusion. Fine-tuning Stable Diffusion on robot data poses three key challenges. Firstly,
StableDiffusionmodelsworkbestwithimageresolutionsof512×512orhigherduetotheirtraining
data. Inrobotics,largeimagesincreaseinferencelatency. Secondly,multi-viewgenerationsuffers
frominconsistenciesacrossviewpoints. However,multi-viewsetupsarecrucialtoavoidocclusions
andimprovespatial-robustness. Thirdly,diffusionisquiteslow,especiallyforgeneratingfourtarget
imagesateverytimestep. Inspiredbyview-synthesisworks[31,32],wesolveallthreechallenges
with a simple solution: tiling. We tile four observations of size 256×256 into a single image of
512×512. Tilinggeneratesfourimagesat5HzonanNVIDIAA100or4HzonanRTX3090.
2.2 Controller
The controller translates target images Aimage into executable joint-positions Ajoints. The controller
t+K t
canbeimplementedwithanyvisuomotorpolicythatmapsRGBobservationstojoint-positions. We
specificallyuseACT[23,33]–aTransformer-basedpolicyarchitecture[34]–foritsfastinference-
speed and training stability. However, in theory, any framework like Diffusion Policies [24] or
RL-based methods [35] can be used. Even classical controllers can be used if pose-estimates of
targetspheresareprovided,butinourimplementation,weoptforlearnedcontrollersforsimplicity.
Training. During training, the controller receives current joint-
positions,thelanguagegoal,andRGBimageswithground-truth
targets overlaid on random backgrounds. The random back-
grounds, as shown in Figure 2, force ACT to follow targets and
ignoreanycontextualinformationinthescene. Weusethesame
hyperparameters and settings from the original ACT codebase3
Figure2.ControllerTrainingvs.Inference.
withminormodifications. Toimproverobustnesstofuzzydiffu-
Duringtraining, thecontrollergetsground-
sionoutputs, weaugmentimageswithrandom-crops[36], color truthtargetswithrandombackgrounds(left).
Duringinference,thetargetimagesaregener-
jitters,elastictransforms,andGaussiannoise. WeuseL1lossfor
atedbythediffusionagent(right).
1https://pyrender.readthedocs.io/en/latest/examples/quickstart.html
2https://huggingface.co/docs/diffusers/en/using-diffusers/controlnet
3https://github.com/tonyzhaozh/act
3joint-actions,andcross-entropylossforgripperopenandcloseactions. Forlanguage-conditioning,
weuseFiLM[37]layersfollowingMT-ACT[19]. Thecontrolleristrainedindependentlyfromthe
diffusionagent. SeeAppendixFforhyperparametersandAppendixEforcontrollerdetails.
Inference. During inference, the controller gets target images from the diffusion agent. The con-
troller predicts a sequence of K joint-actions, and executes K actions or less before querying the
diffusionagentinaclosed-loopfashion. Thecontrollerrunsat∼50HzonanNVIDIARTX3090.
3 Experiments
Westudy GENIMA inbothsimulatedandreal-worldenvironments. Specifically, weareinterested
inansweringthefollowingquestions:
§3.1 HowdoesGENIMAcompareagainststate-of-artvisuomotorpoliciesand3Dbaselines?
§3.2 Whatarethebenefitsofdrawingactionswithinternet-pretrainedimage-generationmodels?
§3.3 WhichfactorsaffectGENIMA’sperformance?
§3.4 HowwelldoesGENIMAperformonreal-worldtasks?
Westartwithbenchmarkingourmethodinsimulatedenvironmentsforreproducibleandfaircom-
parisons. Thefollowingsectionsdescribeoursimulationsetupandevaluationmethodology.
Simulation Setup. All simulated experiments are set in CoppeliaSim [38] interfaced through
PyRep[39]. Therobotisa7-DoFFrankaEmikaPandaplacedbehindatabletop. Observationsare
captured from four RGB cameras: front, left shoulder, right shoulder, and wrist, each
witharesolutionof256×256. Therobotiscommandedwithjoint-positionactionsviaPIDcontrol,
orend-effectoractionsviaanIKsolver.
25RLBenchTasks. Wechoose25(outof100)tasksfromRLBench[22]. WhilemostRLBench
tasks are suited for discrete, quasi-static motions that benefit 3D next-best-pose agents [25, 26],
we pick tasks that are difficult to execute with end-effector control. Tasks such as open box and
open microwaverequiresmoothnon-linearmotionsthatsampling-basedmotion-plannersandIK
solversstrugglewith. EachRLBenchtaskincludesseveralvariations,butweonlyusevariation0
toreducetrainingtimewithlimitedresources. However,ourmethodshouldbeapplicabletomulti-
variation settings without any modifications. We generate two datasets: 50 training demos and
50 evaluation episodes per task. For both datasets, objects are placed randomly, and each episode
is sanity-checked for solvability. Language goals are constructed from instruction templates. See
AppendixAfordetailsonindividualtasks.
Evaluation Metric. Multi-task agents are trained on all 25 tasks, and evaluated individually on
eachtask. Scoresareeither0forfailuresor100forsuccesses,withnopartialsuccesses. Wereport
averagesuccessrateson50evaluationepisodesacrossthelastthreeepochcheckpoints:50×3=150
episodespertask, whichaddsupto150×25 = 3750intotal. Weuseasinglesetofcheckpoints
foralltaskswithoutanytask-specificoptimizationsorcherry-picking.
Visuomotor Baselines. We benchmark GENIMA against three state-of-the-art visuomotor ap-
proaches:ACT[19,23],DiffusionPolicies[24],andSuSIE[13]. ACTisatransformer-basedpolicy
that has achieved compelling results in bimanual manipulation. Although GENIMA uses ACT as
the controller, our controller has never seen RGB observations from demonstrations, just sphere
targetswithrandombackgrounds. DiffusionPoliciesisawidelyadoptedvisuomotorapproachthat
generates multi-modal trajectories through diffusion. SuSIE is the closest approach to GENIMA,
butinsteadofdrawingtargetactions,SuSIEgeneratestargetRGBobservationsasgoalimages. We
adapt SuSIE to our setting by training a controller that maps target and current RGB observations
to joint-position actions. All multi-task baselines are conditioned with language goals. ACT and
DiffusionPoliciesuseFiLMconditioning[37],whereasSuSIEusesthegoalasaprompt.
4Diff. 3DDiff.
Task GENIMA ACT SuSIE
Policy Actor
basketball in hoop 50.0 32.7 5.3 0.0 100
insert usb 26.0 18.0 0.0 0.0 29.3 GENIMA 60.0
move hanger 94.0 42.0 21.3 0.0 76.0
open box 79.3 69.3 36.0 3.3 6.0 32.5
open door 85.3 75.3 26.6 6.7 76.7 Action 26.7
Modes
open drawer 77.3 82.7 67.3 0.0 71.3
Delta Joints 19.8
open grill 48.7 40.0 26.0 0.0 93.3
open microwave 46.7 22.6 10.0 0.0 58.0
10 actions 22.9
open washer 46.0 22.6 2.0 2.0 82.7 Prediction
open window 69.3 8.0 24.6 0.0 96.7 Horizon 7.3
phone on base 18.7 13.3 1.0 2.0 94.0 3
pick up cup 36.0 43.3 24.6 0.7 92.7
play jenga 90.0 99.3 40.0 1.3 92.0 10 actions 35.8
press switch 72.7 65.3 74.7 22.7 83.3 Execution 5 actions 26.7
Horizon
push button 76.7 31.3 7.3 2.7 46.7 1 action 5.3
put books on shelf 14.7 44.0 1.0 0.0 36.7
put knife on board 12.7 14.7 4.0 2.0 77.3 SDXL-Turbo 62.9
put rubbish in bin 26.7 15.3 6.7 0.0 96.0 D Seif tf tu ins gio sn Non-Tiled 59.9
scoop with spatula 11.3 22.7 1.0 0.0 66.0
34.4
slide block 12.7 22.0 0.0 0.0 99.3
take lid off 48.0 44.7 72.0 1.3 100 Full-Context ACT 56.4
take plate off 21.3 37.3 4.0 0.0 72.7 Controller
Full-Context DP 32.2
toilet seat up 93.3 45.3 50.0 2.0 94.0 Settings
turn on lamp 12.0 19.3 6.0 4.0 4.0 Random Back. DP 0.0
Baseline ACT w/ SD Feat. 0.0
turn tap 71.3 59.3 32.7 20.7 99.3
Success Rate (%)
average 49.6 39.6 21.8 2.9 73.8
Table1. Visuomotorand3DBaselineson25RLBenchtasks. Successrates Figure3.AblationsandSensitivityAnalyses.We
(%)formulti-taskagentstrainedwith50demosandevaluatedon50episodesper studyfactorsthataffectGENIMA’sperformanceby
task.Wereportaveragescoresacrossthelastthreecheckpoints.Thefourmethods trainingamulti-taskagenton3tasks: take lid
ontheleftareRGB-to-jointagents.Therightmostmethodisa3Dnext-best-pose off,open box,andslide block.Wereportav-
agentwithextrapriors:depth,keypoints,scenebounds,andmotion-planners. eragesuccessratesacrossthe3tasks.
3D Baseline. We also benchmark GENIMA against 3D Diffuser Actor [40] – a state-of-the-art
agentinRLBench. 3DDiffuserActorusesCLIP[41]toextractvisionandlanguagefeatures, and
diffusesend-effectorposeswitha3Dtransformer. Weuse3DDiffuserActorasthebest-performing
representativeof3Dnext-best-poseagents[25,26]suchasPerAct[42],Hiveformer[43],RVT[44],
Act3D [45], DNAct [20], and GNFactor[21]. Theseworks rely on severalpriors: depth cameras,
motion-planners to reach poses, keypoints that segment trajectories into bottlenecks, task-specific
scenebounds,andquasi-staticassumptionformotions.
3.1 Visuomotorand3DBaselines
OurkeyresultisthatweshowGENIMA–animage-generationmodelfine-tunedtodrawactions–
worksatallforvisuomotortasks.Inthesectionsbelow,wegobeyondthisinitialresultandquantify
GENIMA’sperformanceagainststate-of-the-artvisuomotorand3Dbaselines.
GENIMAoutpeformsACT,DiffusionPolicies,andSuSIE.Table1presentsresultsfromRLBench
evaluations. GENIMAoutperformsACT[19,23]in16/25tasks,particularlyintaskswithocclusions
(e.g., open window) and complex motions (e.g., turn tap). Against SuSIE [13], GENIMA per-
formsbetteron23/25tasks,asSuSIEstrugglestogenerateexactpixel-leveldetailsforgoals. Dif-
fusionPolicy[24]performspoorlyinmulti-tasksettingswithjoint-positioncontrol.Weensuredthat
ourimplementationiscorrectbytrainingDiffusionPolicyonjusttake lid off,whichachieveda
reasonablesuccessrateof75%,butwecouldnotscaleitto25tasks.
RGB-to-jointagentsapproachtheperformanceof3Dnext-best-poseagents. Without3Dinput
and motion-planners, most prior works [42, 46] report zero-performance for RGB-only agents in
RLBench. However, our results in Table 1 show that RGB-to-joint agents like GENIMA and ACT
can be competitive with 3D next-best-pose agents. GENIMA outperforms 3D Diffuser Actor in
6/25tasks,particularlyintaskswithnon-lineartrajectories(e.g.,open box,open door),andtiny
objects (e.g., turn on lamp). GENIMA also performs comparably (within 3%) on 3 more tasks:
insert usb,play jenga,toilet seat up,despitelackingpriors. 3DDiffuserActorperforms
betteronmosttasks,buttrainingGENIMAforlongerorwithmoredatacouldbridgethisgap.
5
Absolute EE
Delta EE
5 actions
1 action
No StripesObject Color Distractors Lighting Table Texture Background Texture Camera Pose
GENIMA -35% -14% -18% -77% -10% -96%
ACT -74% -72% -39% -92% 0% -94%
Figure4.PerformancedropsfromColosseum[46]perturbations.WeevaluateGENIMAandACTon6perturbationcategories:randomized
objectandpartcolors,distractorobjects,lightingcolorandbrightnessvariations,randomizedtabletextures,randomizedbackgrounds,and
cameraposechanges.Wereportsuccessratesfrom150evaluationepisodespertask,whereperturbationsarerandomlysampledepisodically.
ACToverfitstoobjectsandlightingconditions,whereasGENIMAismorerobusttosuchperturbations.Seesupplementaryvideoforexamples.
3.2 SemanticandSpatialGeneralization
WhileallevaluationsinSection3.1trainandtestonthesameenvironment,thekeybenefitofusing
image-generationmodelsisinimprovinggeneralizationofvisuomotorpolicies. Inthissection,we
examinesemanticandspatialgeneralizationaspectsofvisuomotorpolicies.
GENIMAisrobusttosemanticperturbationsonColosseumtasks. Weevaluatethesamemulti-
task GENIMA andACTagents(fromSection3.1)on6perturbationcategoriesinColosseum[46]:
randomizedobjectandpartcolors, distractorobjects, lightingcolorandbrightnessvariations, ran-
domizedtabletextures,randomizedscenebackgrounds,andcameraposechanges.Figure4presents
resultsfromtheseperturbationtests. Despitebeinginitializedwithapre-trainedResNet[47],ACT
overfits and significantly drops in performance with changes to object color, distractors, lighting,
andtabletextures. WhereasGENIMAhasminimaldropsinperformancefromanemergentproperty
thatrevertsscenestocanonicaltexturesandcolorsfromthetrainingdata.Seesupplementaryvideos
forexamples. However,bothmethodsfailtogeneralizetounseencameraposes.
GENIMA extrapolates to spatial loca- Train
tions with aligned image-action spaces. Success
Failure
By drawing actions on images, GEN- Success
Failure
IMA keeps the image-space and action-
space aligned. This alignment has been
shown to improve spatial generalization
anddata-efficiencyinpriorworks[42,48,
49]. We observe similar benefits in Fig-
GENIMA ACT
ure 5, where ACT struggles in the upper-
right region with minimal training exam-
Figure5.SpatialGeneralization.TrainandtestsaucepanlocationsforGEN-
ples, but GENIMA succeedsinextrapolat- IMAandACTevaluatedontake lid off. ACTstrugglestoextrapolateto
theupper-rightregion,whereasGENIMAusesalignedimage-actionspacesfor
ingtothoselocations.
betterspatialgeneralization.
3.3 AblationsandSensitivityAnalyses
We investigate factors that affect GENIMA’s performance. We report average success rates from
multi-task GENIMA trained on 3 tasks: take lid off, open box, and slide block. Our key
resultsarepresentedinFigure3,andthesectionsbelowsummarizeourfindings.
Absolutejoint-positionisthebestperformingaction-mode. Deltaaction-modesaccumulateer-
rors, and end-effector control through IK struggles with non-linear trajectories. Joint-position ac-
tionsarealsomoreexpressive,allowingforfull-bodycontrolandotherembodiments.
Longer action sequence predictions are crucial. In line with prior works [23, 24], modeling
trajectorydistributionsrequirespredictinglongeractionsequences. WefindthatpredictingK =20
actionsisoptimal,sinceobservationsarerecordedat20HzinRLBench.
Longer execution horizons avoid error accumulation. Shorter execution horizons lead to jerky
motionsthatputtherobotinunfamiliarstates. Wefindthatexecutingall20actionsworksbest.
SDXLimprovesperformanceoverSD.LargerbasemodelssuchasSDXL-Turbo[27]havemore
capacitytomodelaction-patterns. NewerTransformer-basedmodels[50]mightscaleevenbetter.
6Tileddiffusionimprovesgenerationspeedwhilekeepingperformance. Tiledgenerationoffour
target images takes 0.2 seconds, whereas generating individual images takes 0.56 seconds. Both
methodsachievesimilarperformance,howevertiledgenerationismoremulti-viewconsistentacross
awidersetoftasks. SeeAppendixGforreference.
Withoutstripesonspheres,performancedropsinhalf. Thesestripesactasgraduationindicators
forjoint-angles. IncludingstripeshelpsStableDiffusionlearnjointrotationsasavisual-pattern.
Full-context controllers overfit to observations. Instead of random backgrounds, if controllers
are trained with target spheres overlaid on RGB observations from demos, they tend to ignore the
targets,andjustuseobservationsforactionprediction. Thishurtsperformanceandgeneralization.
ACT works better than DiffusionPolicy as the controller. Similar to results in Section 3.1, we
find that ACT is better at joint-action prediction than DiffusionPolicy (DP). ACT also has a faster
inferencespeedof0.02seconds,whereasDiffusionPolicytakes0.1seconds(for20diffusionsteps).
GENIMAisdata-efficient. Westudy
data-efficiencybyconstrainingposes
ofobjectsintrainingdemos. Follow-
ingR&D[49], wesampleposesina
grid-style that maximizes workspace
coveragebasedonthenumberofde-
mos. GENIMA achieves 80% of the
peakperformancewith25demos.
Figure6.Data-EfficiencyandDiffusionSpeedofGENIMA.
GENIMA works with 5 diffusion
steps or more. With SD-Turbo [27] as the base model, GENIMA can generate target images with
just 5 diffusion steps within 0.2 seconds. Future works can use better schedulers and distillation
methodstofurtherimprovegenerationspeedandquality.
3.4 Real-robotEvaluations
In-Distribution Out-of-Distribution
Task GENIMA ACT GENIMA ACT Category
We validate our results by benchmark-
lid off 80 60 60 20 newsaucepan
ing GENIMA and ACT on a real-robot place teddy 100 80 100 60 newtoy
setup. Our setup consists of a Franka elbow touch 80 40 60 40 newbackground
hang scarf 40 60 60 20 newscarf
EmikaPandawith2externaland2wrist put marker 40 40 40 20 movingobjects
cameras. Wetrainmulti-taskagentsfrom slide book 100 20 100 0 darkerlighting
avoid lamp 40 0 0 0 newobject
scratch on 9 tasks with 50 demos per lift bag 80 60 40 40 distractors
task. These tasks involve dynamic be- flip cup 20 80 20 40 newcup
haviors (e.g., slide book), transparent Table2.Real-robotResults.Successratesofmulti-taskGENIMAandACTon
9real-worldtasks,evaluatedon5episodespertask.
objects(e.g.,lift bag),deformableob-
jects(e.g.,hang scarf),andfull-bodycontrol(e.g.,elbow touch). SeeFigure7andsupplemen-
taryvideosforexamples. AppendixBcoverstaskdetails. WhencomparingGENIMAandACT,we
ensure that the initial state is exactly the same by using an image-overlay tool to position objects.
Table 2 reports average success rates from 5 evaluation episodes per task. We also report out-
of-distributionperformancewithunseenobjectsandsceneperturbations. InlinewithSection3.2,
GENIMAisbetterthanACTatgeneralizingtoout-of-distributiontasks. GENIMAalsoexhibitssome
recoverybehaviorfrommistakes,butDAgger-style[51]trainingmightimproverobustness.
4 RelatedWork
Visuomotoragentsmapimagestoactionsinanend-to-endmanner[52,53,54]. ACT[23]usesa
transformer-basedpolicyarchitecture[34]toencodeResNet[47]featuresandpredictaction-chunks.
MT-ACT[19]extendsACTtothemulti-tasksettingswithlanguage-conditioning. MVP[55]uses
self-supervisedvisualpre-trainingonin-the-wildvideosandfine-tunesforreal-worldrobotictasks.
Diffusion Policy [24] uses the diffusion process to learn multi-modal trajectories. RT-2 [56] fine-
tunevision-languagemodelstopredicttokenizedactions. Octo[57]canbeadaptedtonewsensory
7(a) (b) (c) (d) (e)
Figure7.Real-worldTasks.5outof9tasks:(a)flip cup,(b)hang scarf,(c)elbow touch,(d)lid off,and(e)slide book.
inputs and action spaces by mapping inputs into a common tokenized format. All these methods
adaptparadigmsfromvisionandlanguageliketokenizationandpre-trainingforpredictingactions.
Incontrast,GENIMAliftsactionsbackintoimage-spacetouseimage-generationmodelsnatively.
3D next-best-pose agents encode 3D input and output 6-DoF poses [25, 26]. These poses are
keypoints (or bottlenecks in the trajectory) that are executed with a motion-planner or IK-solver.
C2F-ARM[26]andPerAct[42]usecalibratedmulti-camerasetupstovoxelizescenesinto3Dgrids.
Voxelgridsarecomputationallyexpensive,soAct3D[45]and3DDiffuserActor[40]replacevoxels
withsampled3Dpoints. RVT[44]rendersRGB-Dinputintoorthographicprojections,anddetects
actions in image-space. GNFactor [21] and DNAct [20] lift Stable Diffusion features into 3D to
improve generalization. Chained Diffuser [58] and HDP [59] use diffusion-based policies as low-
levelcontrollerstoreachkeypointspredictedby3Dnext-best-poseagents. Allthese3Dnext-best-
pose agents rely on several priors: depth cameras, keypoints, task-specific scene bounds, and/or
motion-planners. WhereasGENIMAisasimpleRGB-to-jointagentwithoutanyofthesepriors.
Diffusion models for robot learning. In addition to modeling policies [60, 61, 62, 63, 64] with
diffusion[65,66],diffusionmodelshavebeenusedinrobotlearninginvariousways. Thisincludes
offlinereinforcementlearning[61,63,67],imitation-learning[68,69,70],subgoalgeneration[16,
71, 72], and planning [58, 59, 73, 74, 75, 76]. Others include affordance prediction [77], skill
acquisition[78,79]andchaining[80],rewardfunctions[81],grasping[82],andsim-to-real[83].
Image-generation models in robot learning have been used for out-of-distribution data genera-
tion [69], and video-conditioned policies [15, 84, 85]. ROSIE [18] and GenAug [17] use image-
generationoutputstoaugmentdatasets. SuSIE[13],RT-Sketch[86],andRT-Trajectory[87],condi-
tionpoliciesongoalsgeneratedbyimagegenerationmodelsintheformofobservations,observation
sketches, andtrajectorysketches, respectively. GENIMA doesnotuseimage-generationmodelsto
generateobservations,videos,ortrajectories,butinsteaddrawsjoint-actionsastargets.
Representingactionsinimageshasbeenshowntoimprovespatial-robustnessandgeneralization.
PIVOT[88]annotatesobservationswithmarkerstoqueryvision-languagemodelsforend-effector
actions. RoboTap[89], ATM[90], andTrack2Act[91]trackdensepointsonimagestolearnend-
effectorpolicies. R&D[49]rendersgrippersonimagestoaidthediffusionprocess. Yangetal.[92]
planbyin-paintingnavigationactions. C3DM[93]iterativelyzooms-intoimagestopredict6-DoF
poses. Incomparison,GENIMAdoesnottrackpoints,andlearnsjoint-actionsinimage-space.
5 ConclusionandLimitations
We presented GENIMA, a multi-task agent that fine-tunes Stable Diffusion to draw joint-actions.
Ourexperimentsbothinsimulationandreal-worldtasksindicatethatfine-tunedimage-generation
modelsareeffectiveinvisuomotorcontrol. Whilethispaperisaproof-of-concept, GENIMAcould
beadaptedtootherembodiments,andalsotodrawphysicalattributeslikeforcesandaccelerations.
GENIMAisquitecapable,butnotwithoutlimitations. LikeallBC-agents,GENIMAonlydistillsex-
pertbehaviorsanddoesnotdiscovernewbehaviors. GENIMAalsousescameracalibrationtorender
targets,assumingtherobotisalwaysvisiblefromsomeviewpoint. Wediscusstheselimitationsand
offerpotentialsolutionsinAppendixJ.Butoverall,weareexcitedaboutthepotentialofpre-trained
diffusionmodelsinrevolutionizingrobotics,akintohowtheyrevolutionizedimage-generation.
8Acknowledgments
BigthankstothemembersoftheDysonRobotLearningLabfordiscussionsandinfrastructurehelp:
Nic Backshall, Nikita Chernyadev, Iain Haughton, Yunfan Lu, Xiao Ma, Sumit Patidar, Young-
gyo Seo, Sridhar Sola, Eugene Teoh, Jafar Uruc¸, and Vitalis Vosylius. Special thanks to Tony Z.
ZhaoandChengChiforopen-sourcingACTandDiffusionPolicies,andtheHuggingFaceTeamfor
diffusers.
References
[1] R.Rombach,A.Blattmann,D.Lorenz,P.Esser,andB.Ommer. High-resolutionimagesyn-
thesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages10684–10695,2022.
[2] A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever.
Zero-shottext-to-imagegeneration. InInternationalconferenceonmachinelearning,pages
8821–8831.Pmlr,2021.
[3] B.Poole,A.Jain,J.T.Barron,andB.Mildenhall. Dreamfusion: Text-to-3dusing2ddiffu-
sion. arXivpreprintarXiv:2209.14988,2022.
[4] A. Gupta, L. Yu, K. Sohn, X. Gu, M. Hahn, L. Fei-Fei, I. Essa, L. Jiang, and J. Lezama.
Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662,
2023.
[5] D.Podell,Z.English,K.Lacey,A.Blattmann,T.Dockhorn,J.Mu¨ller,J.Penna,andR.Rom-
bach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv
preprintarXiv:2307.01952,2023.
[6] C.Schuhmann, R.Beaumont, R.Vencu, C.Gordon, R.Wightman, M.Cherti, T.Coombes,
A.Katta,C.Mullis,M.Wortsman,etal. Laion-5b: Anopenlarge-scaledatasetfortraining
nextgenerationimage-textmodels. AdvancesinNeuralInformationProcessingSystems,35:
25278–25294,2022.
[7] T.Brooks,A.Holynski,andA.A.Efros. Instructpix2pix: Learningtofollowimageediting
instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages18392–18402,2023.
[8] L.Zhang, A.Rao, andM.Agrawala. Addingconditionalcontroltotext-to-imagediffusion
models. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
pages3836–3847,2023.
[9] L. Tang, M. Jia, Q. Wang, C. P. Phoo, and B. Hariharan. Emergent correspondence from
imagediffusion. AdvancesinNeuralInformationProcessingSystems,36:1363–1389,2023.
[10] E. Hedlin, G. Sharma, S. Mahajan, H. Isack, A. Kar, A. Tagliasacchi, and K. M. Yi. Un-
supervisedsemanticcorrespondenceusingstablediffusion. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[11] R. Liu, R. Wu, B. Van Hoorick, P. Tokmakov, S. Zakharov, and C. Vondrick. Zero-1-to-3:
Zero-shotoneimageto3dobject. InProceedingsoftheIEEE/CVFInternationalConference
onComputerVision,pages9298–9309,2023.
[12] Y.Shi,P.Wang,J.Ye,M.Long,K.Li,andX.Yang. Mvdream: Multi-viewdiffusionfor3d
generation. arXivpreprintarXiv:2308.16512,2023.
[13] K. Black, M. Nakamoto, P. Atreya, H. Walke, C. Finn, A. Kumar, and S. Levine. Zero-
shot robotic manipulation with pretrained image-editing diffusion models. arXiv preprint
arXiv:2310.10639,2023.
9[14] Y.Du,M.Yang,P.Florence,F.Xia,A.Wahid,B.Ichter,P.Sermanet,T.Yu,P.Abbeel,J.B.
Tenenbaum,etal. Videolanguageplanning. arXivpreprintarXiv:2310.10625,2023.
[15] Y.Du, S.Yang, B.Dai, H.Dai, O.Nachum, J.Tenenbaum, D.Schuurmans, andP.Abbeel.
Learninguniversalpoliciesviatext-guidedvideogeneration.AdvancesinNeuralInformation
ProcessingSystems,36,2024.
[16] I.Kapelyukh,V.Vosylius,andE.Johns. Dall-e-bot: Introducingweb-scalediffusionmodels
torobotics. IEEERoboticsandAutomationLetters,2023.
[17] Z. Chen, S. Kiami, A. Gupta, and V. Kumar. Genaug: Retargeting behaviors to unseen
situationsviagenerativeaugmentation. arXivpreprintarXiv:2302.06671,2023.
[18] T. Yu, T. Xiao, A. Stone, J. Tompson, A. Brohan, S. Wang, J. Singh, C. Tan, J. Peralta,
B.Ichter,etal. Scalingrobotlearningwithsemanticallyimaginedexperience. arXivpreprint
arXiv:2302.11550,2023.
[19] H. Bharadhwaj, J. Vakil, M. Sharma, A. Gupta, S. Tulsiani, and V. Kumar. Roboagent:
Generalization and efficiency in robot manipulation via semantic augmentations and action
chunking. arXivpreprintarXiv:2309.01918,2023.
[20] G. Yan, Y.-H. Wu, and X. Wang. Dnact: Diffusion guided multi-task 3d policy learning.
arXivpreprintarXiv:2403.04115,2024.
[21] Y.Ze,G.Yan,Y.-H.Wu,A.Macaluso,Y.Ge,J.Ye,N.Hansen,L.E.Li,andX.Wang. Gn-
factor: Multi-taskrealrobotlearningwithgeneralizableneuralfeaturefields. InConference
onRobotLearning,pages284–301.PMLR,2023.
[22] S.James,Z.Ma,D.R.Arrojo,andA.J.Davison. Rlbench: Therobotlearningbenchmark&
learningenvironment. IEEERoboticsandAutomationLetters,5(2):3019–3026,2020.
[23] T.Z.Zhao,V.Kumar,S.Levine,andC.Finn. Learningfine-grainedbimanualmanipulation
withlow-costhardware. arXivpreprintarXiv:2304.13705,2023.
[24] C. Chi, S. Feng, Y. Du, Z. Xu, E. Cousineau, B. Burchfiel, and S. Song. Diffusion policy:
Visuomotorpolicylearningviaactiondiffusion. arXivpreprintarXiv:2303.04137,2023.
[25] S.JamesandA.J.Davison. Q-attention: Enablingefficientlearningforvision-basedrobotic
manipulation. IEEERoboticsandAutomationLetters,7(2):1612–1619,2022.
[26] S. James, K. Wada, T. Laidlow, and A. J. Davison. Coarse-to-fine q-attention: Efficient
learningforvisualroboticmanipulationviadiscretisation. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages13739–13748,2022.
[27] A.Sauer,D.Lorenz,A.Blattmann,andR.Rombach.Adversarialdiffusiondistillation.arXiv
preprintarXiv:2311.17042,2023.
[28] C.Mou,X.Wang,L.Xie,Y.Wu,J.Zhang,Z.Qi,andY.Shan.T2i-adapter:Learningadapters
todigoutmorecontrollableabilityfortext-to-imagediffusionmodels. InProceedingsofthe
AAAIConferenceonArtificialIntelligence,volume38,pages4296–4304,2024.
[29] H. Ye, J. Zhang, S. Liu, X. Han, and W. Yang. Ip-adapter: Text compatible image prompt
adapterfortext-to-imagediffusionmodels. arXivpreprintarXiv:2308.06721,2023.
[30] P.vonPlaten,S.Patil,A.Lozhkov,P.Cuenca,N.Lambert,K.Rasul,M.Davaadorj,D.Nair,
S.Paul,W.Berman,Y.Xu,S.Liu,andT.Wolf. Diffusers: State-of-the-artdiffusionmodels.
https://github.com/huggingface/diffusers,2022.
[31] M. Zhao, C. Zhao, X. Liang, L. Li, Z. Zhao, Z. Hu, C. Fan, and X. Yu. Efficientdreamer:
High-fidelity and robust 3d creation via orthogonal-view diffusion prior. arXiv preprint
arXiv:2308.13223,2023.
10[32] J.Li, H.Tan, K.Zhang, Z.Xu, F.Luan, Y.Xu, Y.Hong, K.Sunkavalli, G.Shakhnarovich,
and S. Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction
model. arXivpreprintarXiv:2311.06214,2023.
[33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I.Polosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessingsystems,
30,2017.
[34] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end
objectdetectionwithtransformers. InEuropeanconferenceoncomputervision,pages213–
229.Springer,2020.
[35] P.Arm,M.Mittal,H.Kolvenbach,andM.Hutter. Pedipulate: Enablingmanipulationskills
usingaquadrupedrobot’sleg. arXivpreprintarXiv:2402.10837,2024.
[36] D.Yarats,R.Fergus,A.Lazaric,andL.Pinto.Masteringvisualcontinuouscontrol:Improved
data-augmentedreinforcementlearning. arXivpreprintarXiv:2107.09645,2021.
[37] E.Perez,F.Strub,H.DeVries,V.Dumoulin,andA.Courville. Film: Visualreasoningwith
ageneralconditioninglayer. InProceedingsoftheAAAIconferenceonartificialintelligence,
volume32,2018.
[38] E. Rohmer, S. P. N. Singh, and M. Freese. Coppeliasim (formerly v-rep): a versatile and
scalablerobotsimulationframework. InProc.ofTheInternationalConferenceonIntelligent
RobotsandSystems(IROS),2013. www.coppeliarobotics.com.
[39] S.James,M.Freese,andA.J.Davison. Pyrep: Bringingv-reptodeeprobotlearning. arXiv
preprintarXiv:1906.11176,2019.
[40] T.-W. Ke, N. Gkanatsios, and K. Fragkiadaki. 3d diffuser actor: Policy diffusion with 3d
scenerepresentations. arXivpreprintarXiv:2402.10885,2024.
[41] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell,
P.Mishkin,J.Clark,etal. Learningtransferablevisualmodelsfromnaturallanguagesuper-
vision. InInternationalconferenceonmachinelearning,pages8748–8763.PMLR,2021.
[42] M.Shridhar,L.Manuelli,andD.Fox. Perceiver-actor: Amulti-tasktransformerforrobotic
manipulation. InConferenceonRobotLearning,pages785–799.PMLR,2023.
[43] P.-L.Guhur,S.Chen,R.G.Pinel,M.Tapaswi,I.Laptev,andC.Schmid. Instruction-driven
history-aware policies for robotic manipulations. In Conference on Robot Learning, pages
175–187.PMLR,2023.
[44] A.Goyal,J.Xu,Y.Guo,V.Blukis,Y.-W.Chao,andD.Fox. Rvt: Roboticviewtransformer
for3dobjectmanipulation. InConferenceonRobotLearning,pages694–710.PMLR,2023.
[45] T. Gervet, Z. Xian, N. Gkanatsios, and K. Fragkiadaki. Act3d: Infinite resolution action
detectiontransformerforroboticmanipulation. arXivpreprintarXiv:2306.17817,2023.
[46] W. Pumacay, I. Singh, J. Duan, R. Krishna, J. Thomason, and D. Fox. The colos-
seum: A benchmark for evaluating generalization for robotic manipulation. arXiv preprint
arXiv:2402.08191,2024.
[47] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages770–
778,2016.
[48] A.Zeng,P.Florence,J.Tompson,S.Welker,J.Chien,M.Attarian,T.Armstrong,I.Krasin,
D.Duong,V.Sindhwani,etal.Transporternetworks:Rearrangingthevisualworldforrobotic
manipulation. InConferenceonRobotLearning,pages726–747.PMLR,2021.
11[49] V.Vosylius, Y.Seo, J.Uruc¸, andS.James. Renderanddiffuse: Aligningimageandaction
spacesfordiffusion-basedbehaviourcloning,2024.
[50] P.Esser,S.Kulal,A.Blattmann,R.Entezari,J.Mu¨ller,H.Saini,Y.Levi,D.Lorenz,A.Sauer,
F.Boesel,etal.Scalingrectifiedflowtransformersforhigh-resolutionimagesynthesis.arXiv
preprintarXiv:2403.03206,2024.
[51] S.Ross,G.Gordon,andD.Bagnell. Areductionofimitationlearningandstructuredpredic-
tion to no-regret online learning. In Proceedings of the fourteenth international conference
onartificialintelligenceandstatistics,pages627–635.JMLRWorkshopandConferencePro-
ceedings,2011.
[52] S.Levine,C.Finn,T.Darrell,andP.Abbeel.End-to-endtrainingofdeepvisuomotorpolicies.
JournalofMachineLearningResearch,17(39):1–40,2016.
[53] S.JamesandE.Johns. 3dsimulationforrobotarmcontrolwithdeepq-learning. NIPS2016
Workshop(DeepLearningforActionandInteraction),2016.
[54] S. James, A. J. Davison, and E. Johns. Transferring end-to-end visuomotor control from
simulation to real world for a multi-stage task. In Conference on Robot Learning, pages
334–343.PMLR,2017.
[55] I. Radosavovic, T. Xiao, S. James, P. Abbeel, J. Malik, and T. Darrell. Real-world robot
learningwithmaskedvisualpre-training. InConferenceonRobotLearning,pages416–426.
PMLR,2023.
[56] A.Brohan,N.Brown,J.Carbajal,Y.Chebotar,X.Chen,K.Choromanski,T.Ding,D.Driess,
A. Dubey, C. Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to
roboticcontrol. arXivpreprintarXiv:2307.15818,2023.
[57] O. M. Team, D. Ghosh, H. Walke, K. Pertsch, K. Black, O. Mees, S. Dasari, J. Hejna,
T. Kreiman, C. Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint
arXiv:2405.12213,2024.
[58] Z. Xian, N. Gkanatsios, T. Gervet, T.-W. Ke, and K. Fragkiadaki. Chaineddiffuser: Uni-
fying trajectory diffusion and keypose prediction for robotic manipulation. In 7th Annual
ConferenceonRobotLearning,2023.
[59] X.Ma, S.Patidar, I.Haughton, andS.James. Hierarchicaldiffusionpolicyforkinematics-
awaremulti-taskroboticmanipulation. arXivpreprintarXiv:2403.03890,2024.
[60] H.Ha,P.Florence,andS.Song. Scalingupanddistillingdown:Language-guidedrobotskill
acquisition. InConferenceonRobotLearning,pages3766–3777.PMLR,2023.
[61] W. Li, X. Wang, B. Jin, and H. Zha. Hierarchical diffusion for offline decision making. In
InternationalConferenceonMachineLearning,pages20035–20064.PMLR,2023.
[62] X.Li,V.Belagali,J.Shang,andM.S.Ryoo. Crosswaydiffusion:Improvingdiffusion-based
visuomotorpolicyviaself-supervisedlearning. arXivpreprintarXiv:2307.01849,2023.
[63] Z.Wang,J.J.Hunt,andM.Zhou. Diffusionpoliciesasanexpressivepolicyclassforoffline
reinforcementlearning. arXivpreprintarXiv:2208.06193,2022.
[64] X.Liu,F.Weigend,Y.Zhou,andH.B.Amor.Enablingstatefulbehaviorsfordiffusion-based
policylearning. arXivpreprintarXiv:2404.12539,2024.
[65] J.Sohl-Dickstein,E.Weiss,N.Maheswaranathan,andS.Ganguli. Deepunsupervisedlearn-
ingusingnonequilibriumthermodynamics.InInternationalconferenceonmachinelearning,
pages2256–2265.PMLR,2015.
12[66] J.Ho,A.Jain,andP.Abbeel. Denoisingdiffusionprobabilisticmodels. Advancesinneural
informationprocessingsystems,33:6840–6851,2020.
[67] A.Ajay,Y.Du,A.Gupta,J.Tenenbaum,T.Jaakkola,andP.Agrawal. Isconditionalgenera-
tivemodelingallyouneedfordecision-making? arXivpreprintarXiv:2211.15657,2022.
[68] M.Reuss,M.Li,X.Jia,andR.Lioutikov. Goal-conditionedimitationlearningusingscore-
baseddiffusionpolicies. arXivpreprintarXiv:2304.02532,2023.
[69] X.Zhang,M.Chang,P.Kumar,andS.Gupta. Diffusionmeetsdagger: Superchargingeye-
in-handimitationlearning. arXivpreprintarXiv:2402.17768,2024.
[70] Y.Ze,G.Zhang,K.Zhang,C.Hu,M.Wang,andH.Xu. 3ddiffusionpolicy. arXivpreprint
arXiv:2403.03954,2024.
[71] C. Chen, F. Deng, K. Kawaguchi, C. Gulcehre, and S. Ahn. Simple hierarchical planning
withdiffusion. arXivpreprintarXiv:2401.02644,2024.
[72] W. Liu, Y. Du, T. Hermans, S. Chernova, and C. Paxton. Structdiffusion: Language-
guided creation of physically-valid structures using unseen objects. arXiv preprint
arXiv:2211.04604,2022.
[73] M. Janner, Y. Du, J. B. Tenenbaum, and S. Levine. Planning with diffusion for flexible
behaviorsynthesis. arXivpreprintarXiv:2205.09991,2022.
[74] Z.Liang,Y.Mu,M.Ding,F.Ni,M.Tomizuka,andP.Luo. Adaptdiffuser: Diffusionmodels
asadaptiveself-evolvingplanners. arXivpreprintarXiv:2302.01877,2023.
[75] Z. Liang, Y. Mu, H. Ma, M. Tomizuka, M. Ding, and P. Luo. Skilldiffuser: Interpretable
hierarchicalplanningviaskillabstractionsindiffusion-basedtaskexecution. arXivpreprint
arXiv:2312.11598,2023.
[76] Z. Wang, T. Oba, T. Yoneda, R. Shen, M. Walter, and B. C. Stadie. Cold diffusion on the
replaybuffer: Learningtoplanfromknowngoodstates. InConferenceonRobotLearning,
pages3277–3291.PMLR,2023.
[77] Y. Ye, X. Li, A. Gupta, S. De Mello, S. Birchfield, J. Song, S. Tulsiani, and S. Liu. Af-
fordancediffusion: Synthesizinghand-objectinteractions. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages22479–22489,2023.
[78] L.Chen,S.Bahl,andD.Pathak. Playfusion: Skillacquisitionviadiffusionfromlanguage-
annotatedplay. InConferenceonRobotLearning,pages2012–2029.PMLR,2023.
[79] M.Xu,Z.Xu,C.Chi,M.Veloso,andS.Song. Xskill: Crossembodimentskilldiscovery. In
ConferenceonRobotLearning,pages3536–3555.PMLR,2023.
[80] U. A. Mishra, S. Xue, Y. Chen, and D. Xu. Generative skill chaining: Long-horizon skill
planningwithdiffusionmodels.InConferenceonRobotLearning,pages2905–2925.PMLR,
2023.
[81] F. Nuti, T. Franzmeyer, and J. F. Henriques. Extracting reward functions from diffusion
models. AdvancesinNeuralInformationProcessingSystems,36,2024.
[82] J.Urain,N.Funk,J.Peters,andG.Chalvatzaki. Se(3)-diffusionfields: Learningsmoothcost
functionsforjointgraspandmotionoptimizationthroughdiffusion. In2023IEEEInterna-
tionalConferenceonRoboticsandAutomation(ICRA),pages5923–5930.IEEE,2023.
[83] Y.Li, Z.Wu, H.Zhao, T.Yang, Z.Liu, P.Shu, J.Sun, R.Parasuraman, andT.Liu. Aldm-
grasping: Diffusion-aided zero-shot sim-to-real transfer for robot grasping. arXiv preprint
arXiv:2403.11459,2024.
13[84] H.He,C.Bai,L.Pan,W.Zhang,B.Zhao,andX.Li.Large-scaleactionlessvideopre-training
viadiscretediffusionforefficientpolicylearning. arXivpreprintarXiv:2402.14407,2024.
[85] A.Ajay,S.Han,Y.Du,S.Li,A.Gupta,T.Jaakkola,J.Tenenbaum,L.Kaelbling,A.Srivas-
tava,andP.Agrawal. Compositionalfoundationmodelsforhierarchicalplanning. Advances
inNeuralInformationProcessingSystems,36,2024.
[86] P.Sundaresan,Q.Vuong,J.Gu,P.Xu,T.Xiao,S.Kirmani,T.Yu,M.Stark,A.Jain,K.Haus-
man,D.Sadigh,J.Bohg,andS.Schaal. Rt-sketch:Goal-conditionedimitationlearningfrom
hand-drawnsketches,2024.
[87] J.Gu,S.Kirmani,P.Wohlhart,Y.Lu,M.G.Arenas,K.Rao,W.Yu,C.Fu,K.Gopalakrish-
nan,Z.Xu,etal. Rt-trajectory: Robotictaskgeneralizationviahindsighttrajectorysketches.
arXivpreprintarXiv:2311.01977,2023.
[88] S. Nasiriany, F. Xia, W. Yu, T. Xiao, J. Liang, I. Dasgupta, A. Xie, D. Driess, A. Wahid,
Z.Xu, etal. Pivot: Iterativevisualpromptingelicitsactionableknowledgeforvlms. arXiv
preprintarXiv:2402.07872,2024.
[89] M.Vecerik,C.Doersch,Y.Yang,T.Davchev,Y.Aytar,G.Zhou,R.Hadsell,L.Agapito,and
J. Scholz. Robotap: Tracking arbitrary points for few-shot visual imitation. arXiv preprint
arXiv:2308.15975,2023.
[90] C.Wen,X.Lin,J.So,K.Chen,Q.Dou,Y.Gao,andP.Abbeel.Any-pointtrajectorymodeling
forpolicylearning. arXivpreprintarXiv:2401.00025,2023.
[91] H. Bharadhwaj, R. Mottaghi, A. Gupta, and S. Tulsiani. Track2act: Predicting point
tracks from internet videos enables diverse zero-shot robot manipulation. arXiv preprint
arXiv:2405.01527,2024.
[92] C.-F. Yang, H. Xu, T.-L. Wu, X. Gao, K.-W. Chang, and F. Gao. Planning as in-painting:
A diffusion-based embodied task planning framework for environments under uncertainty.
arXivpreprintarXiv:2312.01097,2023.
[93] V.Saxena,Y.Koga,andD.Xu. Constrained-contextconditionaldiffusionmodelsforimita-
tionlearning. arXivpreprintarXiv:2311.01419,2023.
[94] M.Shridhar,L.Manuelli,andD.Fox. Cliport: Whatandwherepathwaysforroboticmanip-
ulation. InConferenceonrobotlearning,pages894–906.PMLR,2022.
[95] J.Devlin,M.-W.Chang,K.Lee,andK.Toutanova. Bert: Pre-trainingofdeepbidirectional
transformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
[96] K. Black, M. Janner, Y. Du, I. Kostrikov, and S. Levine. Training diffusion models with
reinforcementlearning. arXivpreprintarXiv:2305.13301,2023.
[97] T.Karras,M.Aittala,T.Aila,andS.Laine. Elucidatingthedesignspaceofdiffusion-based
generative models. Advances in Neural Information Processing Systems, 35:26565–26577,
2022.
[98] O.Ronneberger,P.Fischer,andT.Brox.U-net:Convolutionalnetworksforbiomedicalimage
segmentation. In Medical image computing and computer-assisted intervention–MICCAI
2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings,
partIII18,pages234–241.Springer,2015.
[99] T. E. Lee, J. Tremblay, T. To, J. Cheng, T. Mosier, O. Kroemer, D. Fox, and S. Birchfield.
Camera-to-robot pose estimation from a single image. In 2020 IEEE International Confer-
enceonRoboticsandAutomation(ICRA),pages9426–9432.IEEE,2020.
14[100] A. Kodaira, C. Xu, T. Hazama, T. Yoshimoto, K. Ohno, S. Mitsuhori, S. Sugano, H. Cho,
Z. Liu, and K. Keutzer. Streamdiffusion: A pipeline-level solution for real-time interactive
generation. arXivpreprintarXiv:2312.12491,2023.
[101] B.Wallace,M.Dang,R.Rafailov,L.Zhou,A.Lou,S.Purushwalkam,S.Ermon,C.Xiong,
S.Joty,andN.Naik. Diffusionmodelalignmentusingdirectpreferenceoptimization. arXiv
preprintarXiv:2311.12908,2023.
[102] L.Yang,B.Kang,Z.Huang,X.Xu,J.Feng,andH.Zhao. Depthanything: Unleashingthe
poweroflarge-scaleunlabeleddata. arXivpreprintarXiv:2401.10891,2024.
[103] A.S.Luccioni,C.Akiki,M.Mitchell,andY.Jernite. Stablebias: Analyzingsocietalrepre-
sentationsindiffusionmodels. arXivpreprintarXiv:2303.11408,2023.
[104] A.Birhane,V.U.Prabhu,andE.Kahembwe. Multimodaldatasets: misogyny,pornography,
andmalignantstereotypes. arXivpreprintarXiv:2110.01963,2021.
[105] P.Schramowski,M.Brack,B.Deiseroth,andK.Kersting. Safelatentdiffusion: Mitigating
inappropriatedegenerationindiffusionmodels. InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition,pages22522–22531,2023.
15A RLBenchTasks
Weselect25outof100tasksfromRLBench[22]foroursimulationexperiments.Onlyvariation0
isusedtoreducetrainingtimewithlimitedresources. Inthefollowingsections, wedescribeeach
ofthe25tasksindetail,includinganymodificationsfromtheoriginalcodebase.
A.1 BasketballinHoop
Task: Pickupthebasketballanddunkitinthehoop.
filename: basketball in hoop.py
Modified: No.
SuccessMetric: Thebasketballgoesthroughthehoop.
A.2 InsertUSBinComputer
Task: PickuptheUSBandinsertitintothecomputer.
filename: insert usb in computer.py
Modified: No.
SuccessMetric: TheUSBtipisinsertedintotheUSBportonthecomputer.
A.3 MoveHanger
Task: Movethehangerfromoneracktoanother.
filename: move hanger.py
Modified: No.
SuccessMetric: Thehangerishungontheotherrackandthegripperisnotgraspinganything.
A.4 OpenBox
Task: Graspthelidandopenthebox.
filename: open box.py
Modified: ForthedataefficiencyexperimentinFigure6,weperformgridsamplingofboxposes
forbothtrainingandevaluationfollowingR&D[49].Agridsizeof5cm×20cmisused,withayaw
rotationrangeof45°aroundthezaxis. Allotherexperimentsusethedefaultrandomsampling.
SuccessMetric: Thejointbetweenthelidandtheboxisat90°.
A.5 OpenDoor
Task: Gripthehandleandpushthedooropen.
filename: open door.py
Modified: No.
SuccessMetric: Thedoorisopenedwiththedoorjointat25°.
A.6 OpenDrawer
Task: Openthebottomdrawer.
filename: open drawer.py
Modified: No.
SuccessMetric: Theprismaticjointofthebuttondrawerisfullyextended.
A.7 OpenGrill
Task: Graspthehandleandraisethelidtoopenthegrill.
filename: open grill.py
Modified: No.
SuccessMetric: Thelidjointofthegrillcoverreaches50°.
16A.8 OpenMicrowave
Task: Pullopenthemicrowavedoor.
filename: open microwave.py
Modified: No.
SuccessMetric: Themicrowavedoorisopenwithitsjointreaches80°.
A.9 OpenWasher
Task: Pullopenthewashingmachinedoor.
filename: open washing machine.py
Modified: No.
SuccessMetric: Thewashingmachinedoorisopenwithitsjointreaches40°.
A.10 OpenWindow
Task: Rotatethehandletounlocktheleftwindow,thenopenit.
filename: open window.py
Modified: No.
SuccessMetric: Thewindowisopenwithitsjointreaches30°.
A.11 PhoneOnBase
Task: Putthephoneontheholderbase
filename: phone on base.py
Modified: No.
SuccessMetric: Thephoneisplacedonthebaseandthegripperisnotholdingthephone.
A.12 PickUpCup
Task: Pickuptheredcup.
filename: pick up cup.py
Modified: No.
SuccessMetric: Theredcupispickedupbythegripperandheldwithinthesuccessregion.
A.13 PlayJenga
Task: TaketheprotrudingblockoutoftheJengatowerwithoutthetowertoppling.
filename: play jenga.py
Modified: No.
SuccessMetric: TheprotrudingblockisnolongerontheJengatower,therestofthetowerremains
standing.
A.14 PressSwitch
Task: Flicktheswitch.
filename: press switch.py
Modified: No.
SuccessMetric: Theswitchisturnedon.
A.15 PushButton
Task: Pushdownthemaroonbutton.
filename: push button.py
Modified: No.
SuccessMetric: Themaroonbuttonispusheddown.
17A.16 PutBooksonShelf
Task: Pickupbooksandplacethemonthetopshelf.
filename: put books on bookshelf.py
Modified: No.
SuccessMetric: Thebooksareonthetopshelf.
A.17 PutKnifeonBoard
Task: Pickuptheknifeandputitonthechoppingboard.
filename: put knife on chopping board.py
Modified: No.
SuccessMetric: Theknifeisonthechoppingboardandthegripperisnotholdingit.
A.18 PutRubbishinBin
Task: Pickuptherubbishandplaceitinthebin.
filename: put rubbish in bin.py
Modified: No.
SuccessMetric: Therubbishisinsidethebin.
A.19 ScoopwithSpatula
Task: Scoopuptheblockandliftitupwiththespatula.
filename: scoop with spatula.py
Modified: No.
SuccessMetric: Thecubeiswithinthesuccessregion,liftedupbythespatula.
A.20 SlideBlocktoTarget
Task: Slidetheblocktowardsthegreensquaretarget.
filename: slide block to target.py
Modified: ForthedataefficiencyexperimentinFigure6,weperformgridsamplingofblockposes
forbothtrainingandevaluationfollowingR&D[49]. Agridsizeof15cm×40cmisused,witha
yawrotationrangeof90°aroundthezaxis.Allotherexperimentsusethedefaultrandomsampling.
SuccessMetric: Theblockisinsidethegreentargetarea.
A.21 TakeLidoffSaucepan
Task: Takethelidoffthesaucepan
filename: take lid off saucepan.py
Modified: ForthedataefficiencyexperimentinFigure6, weperformgridsamplingofsaucepan
poses for both training and evaluation following R&D [49]. A grid size of 35cm×44cm is used,
with a yaw rotation range of 90° around the z axis. All other experiments use the default random
sampling.
SuccessMetric: Thelidisliftedofffromthesaucepantothesuccessregionaboveit.
A.22 TakePlateoffColoredDishRack
Task: Taketheplateofftheblackdishrackandleaveitonthetabletop.
filename: take plate off colored dish rack.py
Modified: No.
SuccessMetric: Theplateisliftedofftheblackdiskrackandplacedwithinthesuccessregionon
thetabletop.
18A.23 ToiletSeatUp
Task: Liftthelidofthetoiletseattoanuprightposition.
filename: toilet seat up.py
Modified: No.
SuccessMetric: Thelidjointisat90°.
A.24 TurnonLamp
Task: Pressthebuttontoturnonthelamp.
filename: lamp on.py
Modified: No.
SuccessMetric: Thelampisturnedonbypressingthebutton.
A.25 TurnTap
Task: Graspthelefttapandturnit.
filename: turn tap.py
Modified: No.
SuccessMetric: Thelefttapisrotatedby90°fromtheinitialposition.
B Real-WorldTasks
Weevaluateon9real-worldtasks. Inthefollowingsections,wedescribeeachof9tasksindetail,
includingtestsweperformtoassessout-of-distributiongeneralization. Figure8showsobjectsand
sceneperturbations.
B.1 LidOff
Task: Takethelidoffthesaucepan.
In-Distribution: Ablacksaucepanwithanoval-shapedlidhandleseenduringtraining.
Out-of-Distribution: Anunnseensmallersaucepanwitharound-shapedlidhandle.
SuccessMetric: Thelidispickedupfromthesaucepanandplacedontherightside.
B.2 PlaceTeddy
Task: Placetheteddyintothedrawer
In-Distribution: Abeige-colorteddybeartoyseenduringtraining.
Out-of-Distribution: Anunseenblueplushtoy.
SuccessMetric: Thetoyisinsidethedrawer.
B.3 ElbowTouch
Task: Touchtheredbuttonwiththeelbowjoint.
In-Distribution: Thebuttonisplacedoverablueclothseenduringtraining.
Out-of-Distribution: Thebuttonisplacedoveranunseenpinkcloth.
SuccessMetric: Therobottouchesthebuttonwithitselbowjoint.
B.4 HangScarf
Task: Hangthescarfonthehanger.
In-Distribution: Aseengreen-and-blackcheckeredscarfseenduringtraining.
Out-of-Distribution: Anunseenredcheckeredscarfwithadifferentthickness.
SuccessMetric: Thescarfhangsstillonthelowestpegofthehanger.
19lid off place teddy elbow press hang scarf flip cup avoid lamp slide book lift bag put marker
ID
OOD
Figure8.Realtaskobjects..Photosofobjectsfromin-distribution(ID)andout-of-distribution(OOD)evaluations.
B.5 PutMarker
Task: Putthehighlighterintothemug.
In-Distribution: Ahighlighterandmugseenduringtraining.
Out-of-Distribution: Thesamehighlighterandmugismovedaroundduringexecution.
SuccessMetric: Thehighlighterisinsidethemug.
B.6 SlideBook
Task: Slidethebooktothedrawer’sedgeandpickitupfromtheside.
In-Distribution: Abookseenduringtraining.
Out-of-Distribution: Thesamebookandscenebutwithdarkerlightingconditions.
SuccessMetric: Thebookisliftedupfromthedrawer.
B.7 AvoidLamp
Task: Pickupthespongeandplaceitinsidethedrawerwithoutbumpingintotheobstacle.
In-Distribution: Aspongeandlamp(astheobstacle)seenduringtraining.
Out-of-Distribution: Thesamesponge,butwitheithercupstandorwaterbottleastheobstacle.
SuccessMetric: Thespongeisplacedintothedrawerwithoutbumpingintotheobstacle.
B.8 LiftBag
Task: Liftuptheplasticbag.
In-Distribution: Aplasticbagseenduringtraining.
Out-of-Distribution: Thesameplasticbag,butplacedondistractorsofdifferentheights.
SuccessMetric: Thebagisliftedupfromthedrawer.
B.9 FlipCup
Task: Pickupthecup,rotateit,andplaceitinanuprightposition.
In-Distribution: Aplasticwineglassseenduringtraining.
Out-of-Distribution: Aunseenceramiccoffeecup.
SuccessMetric: Thecupisstandinguprightonthedrawer.
20C HardwareSetup
C.1 Simulation
Oursimulated experimentsuse afour-camerasetup: front, left shoulder, right shoulder,
andwrist. AllcamerasaresettodefaultcameraposesfromRLBench[22]withoutanymodifica-
tions,exceptfortheperturbationtestsinSection3.2.
C.2 Real-Robot
Hardware Setup. Real-robot experiments use a 7-DoF Franka Emika Panda equipped with a
Robotiq 2F-140 gripper. We use four RealSense D415 cameras to capture RGB images. Two
cameras on the end-effector (upper wrist, lower wrist) to provide a wide field-of-view, and
two external cameras (front, right shoulder) that are fixed on the base. We use a TARION
camera mount4 for the right shoulder camera. The extrinsics between the cameras and robot
base-framearecalibratedwiththeeasy handeyepackage5inROS.
Data Collection. We collect demonstrations for real-
Leader
world tasks using a joint-mirroring setup similar to
ALOHA[23].Figure9showsthedatacollectionsetup.A
LeaderFrankaismovedbytheoperatorandtheFollower
FrankamirrorstheLeader’smovementinjointspace. Vi-
sualobservationsandjointstatesarerecordedat30FPS.
When training controllers, we set the action prediction Follower
horizon to match the data recording frequency to avoid
bigjumpsorslowtrajectoryexecution.
C.3 TrainingandEvaluationHardware Figure9.Joint-mirroringsetupusedfordata-collection.
The diffusion agents of GENIMA and SuSIE [13], and the 3D Diffuser Actor [40] baseline were
trainedonasingleNVIDIAA100GPUwith80GBVRAM.Thecontrollersweretrainedonasingle
NVIDIA L4 GPU with 24GB VRAM. Evaluation inference for real-world agents was done on an
NVIDIAGeForceRTX3090GPUwith24GBVRAM.
D ControlNetOverview
ControlNet [8] is a fine-tuning architecture that preserves the text-to-image capabilities of Stable
Diffusionwhilefollowingthespatiallayoutofaconditioningimage. ControlNethasachievedcom-
pellingresultsinseveralimage-to-imagedomainssuchassketch-to-image, normal-map-to-image,
depth-to-image, canny-edge-to-image, segmentations-to-image, andhuman-pose-to-image. Partic-
ularly,themethodpreservesspatialstructuresandcanbetrainedonsmalldatasets. Thisisachieved
throughatwo-streamarchitecturesimilartopriorworkslikeCLIPort[94].
Two-streamarchitecture. ControlNet’sarchitectureiscomposedoftwostreams: frozenandtrain-
able.Thefrozen-streamisapre-trainedcopyofStableDiffusion’sUNet,whoseparametersarekept
frozen throughout fine-tuning. The trainable-stream is another copy of the UNet’s downsampling
encoder, whose parameters are fine-tuned. The frozen-stream gets sampled latents, a prompt, and
time embeddings as input. The trainable-stream gets latents of the conditioning image (that is en-
coded with a frozen autoencoder), a prompt, and time embeddings as input. The two streams are
connected through zero-convolution layers where outputs from each layer of the trainable-stream
areaddedtothedecoderlayersofthefrozen-stream.
4https://amzn.eu/d/7xDDfJH
5https://github.com/IFL-CAMP/easy_handeye
21Zero-convolution connections regulate the flow of information from the trainable-stream to the
frozen-stream. Zero-convolutionlayersare1×1convsinitializedwithzeroedweightsandbiases.
Atthestartofthefine-tuningprocess,thetrainable-streammakesnocontributiontothefinaloutput
because of the zero-intialization. But after fine-tuning, the trainable layers modulate the output to
followthespatiallayoutintheconditioningimage.
Training Loss. ControlNet is trained with a standard diffusion loss that predicts noise added to a
noise image. This is implemented as an L2-loss on the latents. For more details on the training
process,refertotheoriginalControlNetpaper[8].
E ACTOverview
Action Chunking with Transformers (ACT) [23] predicts action chunks (or sequences) to reduce
the effective horizon of long-horizon tasks. This helps alleviate compounding errors in behavior-
cloningwhenlearningfromhumandemonstrations. ThechunksizeisfixedatlengthK. Givenan
observation,themodeloutputsthenextK actionstobeexecutedsequentially.
Architecture.Imagesareencodedwithapre-trainedResNet-18[47].Thevisionfeaturesfromeach
cameraandproprioceptivefeaturesarethenfedintoaconditionalvariationalautoencoder(CVAE).
The CVAE consists of a BERT-like [95] transformer encoder and decoder. The encoder takes in
the current joint position and target action sequence to predict the mean and variance of a style
variablez. Thisstylevariablezhelpsindealingwithmulti-modaldemonstrations. Itisonlyusedto
conditiontheactiondecoderduringtrainingandisdiscardedattesttimebyzeroingitout.Theaction
decoderisbasedonDETR[34],andistrainedtomaximizethelog-likelihoodofactionchunksfrom
humandemonstrationsusingtwolosses: anactionreconstructionlossandaKLregularizationterm
toencourageaGaussianpriorforz.
TemporalSmoothing. Toavoidjerkyrobotmotions,ACT[23]usestemporalensemblingateach
timestep. An exponential weighted scheme w = exp(−m ∗ i) is applied to obtain a weighted
i
averageofactionsfromoverlappingpredictionsacrosstimesteps,wherew isthecoefficientforthe
0
oldestactionandmcontrolsthespeedforincorporatingnewobservations.
OurmodificationstoACT.WemadeseveralmodificationstoACT[23]inourimplementationto
improvedata-efficiencyandrobustness:
• Dataaugmentation: weuserandom-crops[36],colorjitters, elastictransformsandGaus-
siannoisefromTorchvision6. TheoriginalACT[23]doesnotuseanydataaugmentation.
• Slidingwindowsampling: weapplyaslidingwindowalongeachdemonstrationtrajectory
toobtainactionchunks. TheoriginalACT[23]samplesactionchunkssparselyfromeach
trajectory. Thesliding-windowensuresfulldata-coverageeveryepoch.
• Discretegripperloss: weusecrossentropylossforgripperopenandcloseactionsinstead
ofanL1-loss. Thismakesthepredictionclosertohowthedatawascollected.
• Temporalensemblesmoothing: wedonotusetemporalsmoothingforourACT[23]con-
trollers. Itoversmoothenstrajectories,whichreducesprecisionandrecoverybehaviors.
• Transformer decoder features: the original implementation conditions action predictions
on only the first decoder layer, leaving some unused layers7. We replace it with the last
decoderfeatureinstead.
6https://pytorch.org/vision/0.15/transforms.html
7https://github.com/tonyzhaozh/act/issues/25
22F Hyperparameters
Inthissection,weprovidetrainingandevaluationhyperparametersforGENIMAandotherbaselines.
NotethatGENIMA’scontroller,SuSIE[96],andtheACT[23]baselineallsharethesamehyperpa-
rametersandaugmentationsettingsforfairone-to-onecomparisons. Real-worldGENIMAandACT
agentsalsousethesamehyperparameters(exceptforthecamerasetup).
BaseModel SD-Turbo[27]
TargetKtimestep 20
Learningrate 1e−5
Weightdecay 1e−2
Epochs 200
Batchsize 24
Imagesize 512×512(tiled)
Imageaugmentation color jitter,random crop
Trainscheduler DDPM[66]
Testscheduler EulerAncestralDiscrete[97]
Learningratescheduler constant
Learningratewarm-upsteps 500
Inferencediffusionsteps 10(forRLBench)
Jointswithrenderedspheres base,elbow,wrist,gripper
Sphereradiusforeachcamera
3cm,8cm,6.5cm,6.5cm
(wrist,front,right,left)
Table3.DiffusionAgenthyperparametersforGENIMAandSuSIE[13].
Backbone ImageNet-trainedResNet18[47]
Actiondimension 8(7joints+1gripperopen)
Cameras wrist,front,rightshoulder,leftshoulder
Learningrate 1e−5
Weightdecay 1e−4
Imagesize 256×256
ActionsequenceK 20
Executionhorizon 20
Observationhorizon 1
#encoderlayers 4
#decoderlayers 6
#heads 8
Feedforwarddimension 2048
Hiddendimension 256
Dropout 0.1
Epochs 1000
Batchsize 96
Temporalensembling False
ActionNormalization zeromean,unitvariance
color jitter,random crop,
Imageaugmentation
elastic,andGaussian noise
Table4.ControllerhyperparametersforGENIMA,SuSIE[13],andACT[23]baseline.
23Backbone ImageNet-trainedResNet18[47]
NoisePredictor UNet[98]
ActionDimension 8(7joints+1gripperopen)
Cameras wrist,front,rightshoulder,leftshoulder
Learningrate 1e−4
Weightdecay 1e−4
Imagesize 256×256
Observationhorizon 1
ActionsequenceK 16
Executionhorizon 16
Train,testdiffusionsteps 50,50
Hiddendimension 512
Epochs 1000
Batchsize 128
Scheduler DDPM[66]
ActionNormalization [-1,1]
color jitter,random crop,
Imageaugmentation
elastic,andGaussian noise
Table5.DiffusionPolicy[24]hyperparameters.
Learningrate 1e−4
Weightdecay 5e−4
Actionhistorylength 3
Train,testdiffusionsteps 100
Embeddingdimension 120
Trainingiterations 550000
Batchsize 8
Positionscheduler scaledlinear
Rotationscheduler squaredcosinenoise
Lossweightw 30
1
Lossweightw 10
2
Table6.3DDiffuserActor[40]hyperparameters.
G Tiledvs. Non-TiledGeneration
Figure 10 shows an example of tiled vs non-tiled generation. Non-tiled generation only gets one
camera-viewinputatatimeduringdiffusion. Withoutthefullscenecontext, non-tiledgeneration
tendstoproduceinconsistentandambiguouspredictionslikeduplicateelbowtargets.
Tiled Non-Tiled
Figure10.Non-tiledgenerationmakesinconsistentpredictionsliketheduplicateel-
bowtargetshighlightedwithredcirclesontheright.
24H BaseDiffusionModelsandFine-TuningPipelines
GENIMA’s formulation is agnostic to the choice of base Stable Diffusion model and also the fine-
tuningpipeline. TheSD-Turbo[27]basemodelusedinGENIMAcanbereplacedwithabiggerbase
model like SDXL-Turbo [27, 5] that is trained on larger images. Likewise, instead of fine-tuning
withControlNet[8],wecanalsouseInstruct-pix2pix[7]. SeeFigure11forexamples.
SD-Turbo SDXL-Turbo Instruct Pix2Pix
Figure11.Drawingjoint-actionswithSD-Turbo[27],SDXL-Turbo[27,5],andInstruct-pix2pix[7].
I SuSIEGoalPredictions
Figure12showsexamplesofgoalimagesgeneratedbySuSIE[13]withfined-tunedControlNet[8].
Ingeneral,SuSIEstrugglestopreciselypredictpixel-leveldetailsofdynamicsceneswithcomplex
objectinteractionssuchasturn tapandtake plate off.
open grill turn on lamp
Input Predicted Goal Input Predicted Goal
take plate off turn tap
Input Predicted Goal Input Predicted Goal
Figure12.ExamplesofgoalspredictedbySuSIE[13].
25J LimitationsandPotentialSolutions
WhileGENIMAisquitecapable,itisnotwithoutlimitations. Inthefollowingsections,wediscuss
someoftheselimitationsandpotentialsolutions.
Cameraextrinsicsduringtraining. Tocreateafine-tuningdataset, GENIMA reliesoncalibrated
cameras with known extrinsics to render target spheres. While the calibration process is quick, it
canbedifficulttoobtainextrinsicsforpre-existingdatasetsorin-the-wilddata. Asimplesolution
couldbetousecamerapose-estimationmethodslikeDREAM[99]. DREAMtakesasingleRGB
imageofaknownrobotandoutputsextrinsicswithcomparableerrorratestotraditionalhand-eye
calibration.
Robot visibility in observations. One strong assumption our method
makes is that the robot is always visible from some viewpoint in order
to draw actions near joints. This assumption might not always hold,
especiallyincamerasetupswithheavyocclusionorwrist-onlyinput. A
potential solution could be to provide a virtual rendering of the robot-
state, which is commonly available from visualization and debugging
tools like RViz8. The virtual rendering can be tiled with observations
suchthatthediffusionagentcanincorporateboththevirtualrobot-state
andobservations. SeeFigure13foranillustration.
Figure 13. Tiled prediction with
Slow speed of diffusion agent. The diffusion agent runs at a consid- virtualrobot-state(bottomleft).
erably lower frequency (5 Hz) than the controller (50 Hz). This makes GENIMA less reactive and
pronetoerroraccumulation. Butdiffusionspeedisunlikelytobeamajorissueinthefuturewith
rapid-advances from the image-generation community. Recent works like StreamDiffusion [100]
runat91.07HzonasingleNVIDIARTX4090.
Jerky motions. The actions generated by GENIMA, especially right after generating a new target
image, can sometimes result in jerky motions. This behavior is noticeable for some tasks in the
supplementaryvideos. Suchbehaviorcouldbearesultoftheagentnotbeingtrainedenough. We
also tried temporal ensembling [23] to smoothen outputs, but this hurt the ability to recover from
mistakes. Futureworkscouldexperimentwithothersmoothingtechniques.
Controllerfailstofollowtargets.Sometimesthecontrollervisiblyfailstoreachthetargetprovided
bythediffusionagent. Thiscouldbebecausethecontrollerdoesnotknowhowtoreachthetarget
fromthecurrentstate,givenitslimitedtrainingdata.Onesolutioncouldbetopre-trainthecontroller
toreacharbitraryrobot-configurationstomaximizetheworkspacecoverage.
Objectrotations. AllRGB-to-jointagentsinSection3.1strugglewithtasksthatrandomizeinitial
object poses with a wide-range of rotations. For instance, in phone on base, GENIMA achieves
19%, whereas 3D Diffuser Actor [40] achieves 94%. A small change to the phone’s rotation, re-
sultsinwidelydifferenttrajectoriesforpickingandplacingitonthebase. Thiseffectcouldmake
behavior-cloning difficult. A potential solution could be to pre-train RGB-to-joint agents on tasks
thatinvolveheavyrotationssuchthattheyacquiresomerotation-equivariantbehavior.
Discovering new behaviors. Like all behavior-cloning agents, GENIMA only distills behaviors
fromhumandemonstrations,anddoesnotdiscovernewbehaviors. Itmightbepossibletofine-tune
GENIMA on new tasks with Reinforcement-Learning (RL). Furthermore, advances in preference
optimization[96,101]forStableDiffusionmodelscouldbeincorporatedtoshapenewbehaviors.
8https://github.com/ros-visualization/rviz
26K Thingsthatdidnotwork
Inthissection,webrieflydescribethingswetriedbutdidnotworkinpractice.
Predictingtargetspheresatfixedintervals. Insteadofpredictingspherescontinuouslyatt+K
timesteps,wetriedfixedintervalsofK e.g.,20,40,60etc.Thesefixedintervalsactaswaypoints,
wherethesphereshoverinplaceuntilthetargetisreached(insteadofalwaysbeingK stepsahead).
In this setting, controllers cannot be trained with random backgrounds, because the trajectory be-
tween fixed intervals is offloaded to the controller without any visual context. So we trained con-
trollerswithfullcontext,butfoundthatthesecontrollerstendtoignoretargetspheres,anddirectly
usethevisualcontextforpredictingactions.
Othersphereshapesandtextures. Weexperimentedwithafewvari-
ations of target spheres. Our goal was to design simple shapes that are
easytodrawwithimage-generationmodels,withouthavingtodrawthe
fullrobotwithproperlinksandjoints. Figure14illustratesanexample
inwhichwemadethevisualappearancemoreasymmetric. Thesphere’s
surfaceisdividedintooctantswithdifferentcolorsandblackdots. The
dotsindicategripperopenandcloseactions. Butinpractice, wefound
thatasimplespherewithhorizontalstripesworksthebest. Futureworks
could improve GENIMA’s performance by simply iterating on the tar-
get’sappearance. Figure14.SphereOctants.
Controller with discrete actions. We tried training controllers that output discrete joint actions
similar to RT-2’s [56] discrete end-effector actions. We discretized joint actions into bins. Each
jointhasitsownpredictionheadandistrainedwithcrossentropyloss. Wefoundthatthediscrete
controllerstendtogeneratesmoothertrajectoriesbutlackslacktheprecisionforfine-grainedtasks
likemanipulatingsmalldoorhandlesorpressingtinybuttons.
Observationswithrenderedcurrentjoint-states. InsteadofjustRGB
observationsasinputtothediffusionagent,weexperimentedwithren-
dering the current joint-states with spheres as a visual reference. This
ledtoagentswithsignificantlyworseperformance,likelyduetothein-
putspheresoccludingimportantvisualinformationinthescene.
Segmenting target spheres for the controller. We experimented with
providing binary segmentation masks of the target spheres to the con-
troller. They had a negligible effect on success. Training with random
Figure 15. Depth by DepthAny-
backgroundsseemssufficienttomakethecontrollerfocusontargets. thing[102]fromRGBinput.
Depth-based ControlNet. We tried conditioning ControlNet on depth input instead of RGB. We
usedDepthAnything[102]togeneratedepthmapsfromRGBobservationsasshowninFigure15.
TheperformancewithdepthwasworseorcomparabletoRGBinput.Futureworkscouldexperiment
withfusingRGBanddepth-basedControlNets[8].
L SafetyConsiderations
Real-robotsystemscontrolledwithStableDiffusion[1]requiresthoroughandextensivesafetyeval-
uations. Internet pre-trained models exhibit harmful biases [103, 104], which may affect models
fine-tuned for action prediction. This issue is not particular to Stable Diffusion, and even com-
monly used ImageNet-pretrained9 ResNets [47] exhibit similar biases. Potential solutions include
safetyguidance[105]anddetectingout-of-distributionorinappropriategenerationswithclassifiers
topauseactionpredictionandaskforhumanassistance. Keepinghumans-in-the-loopwithlivevi-
sualizationsofactionpredictions,andincorporatinglanguage-drivenfeedback,couldfurtherhelpin
mitigatingissues.
9https://excavating.ai/
27