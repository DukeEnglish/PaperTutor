Gradient-free variational learning
with conditional mixture networks
ConorHeins∗† HaoWu†
VERSESAIResearchLab VERSESAIResearchLab
LosAngeles,CA,USA LosAngeles,CA,USA
MaxPlanckInstituteofAnimalBehavior hao.wu@verses.ai
DepartmentofCollectiveBehaviour
Konstanz,Germany
conor.heins@verses.ai
DimitrijeMarkovic† AlexanderTschantz
VERSESAIResearchLab VERSESAIResearchLab
LosAngeles,CA,USA LosAngeles,CA,USA
ChairofCognitiveComputationalNeuroscience SchoolofEngineeringandInformatics
TechnischeUniversita¨tDresden UniversityofSussex
Dresden,Germany Brighton,UK
dimitrije.markovic@tu-dresden.de alec.tschantz@verses.ai
JeffBeck‡ ChristopherBuckley‡
DepartmentofNeurobiology VERSESAIResearchLab
DukeUniversity LosAngeles,CA,USA
Durham,NC,USA SchoolofEngineeringandInformatics
jeff.beck@duke.edu UniversityofSussex
Brighton,UK
christopher.buckley@verses.ai
Abstract
Balancingcomputationalefficiencywithrobustpredictiveperformanceiscrucial
in supervised learning, especially for critical applications. Standard deep learn-
ingmodels,whileaccurateandscalable,oftenlackprobabilisticfeatureslikecali-
bratedpredictionsanduncertaintyquantification.Bayesianmethodsaddressthese
issues but can be computationally expensive as model and data complexity in-
crease.Previousworkshowsthatfastvariationalmethodscanreducethecompute
requirements of Bayesian methods by eliminating the need for gradient compu-
tation or sampling, but are often limited to simple models. We demonstrate that
conditional mixture networks (CMNs), a probabilistic variant of the mixture-of-
experts(MoE)model, aresuitableforfast, gradient-freeinferenceandcansolve
complex classification tasks. CMNs employ linear experts and a softmax gating
network. By exploiting conditional conjugacy and Po´lya-Gamma augmentation,
we furnish Gaussian likelihoods for the weights of both the linear experts and
the gating network. This enables efficient variational updates using coordinate
ascentvariationalinference(CAVI),avoidingtraditionalgradient-basedoptimiza-
tion. Wevalidate thisapproachbytrainingtwo-layerCMNsonstandard bench-
∗Correspondingauthor
†Co-firstauthors
‡Co-seniorauthors
Preprint.Underreview.
4202
guA
92
]GL.sc[
1v92461.8042:viXramarks from the UCI repository. Our method, CAVI-CMN, achieves competitive
andoftensuperiorpredictiveaccuracycomparedtomaximumlikelihoodestima-
tion(MLE)withbackpropagation,whilemaintainingcompetitiveruntimeandfull
posterior distributions over all model parameters. Moreover, as input size or the
number of experts increases, computation time scales competitively with MLE
and other gradient-based solutions like black-box variational inference (BBVI),
making CAVI-CMN a promising tool for deep, fast, and gradient-free Bayesian
networks.
1 Introduction
Modernmachinelearningmethodsattempttolearnfunctionsofcomplexdata(e.g.,images,audio,
text)topredictinformationassociatedwiththatdata,suchasdiscretelabelsinthecaseofclassifi-
cation[Bernardoetal.,2007]. Deepneuralnetworks(DNNs)havedemonstratedsuccessinthisdo-
main,owingtotheiruniversalfunctionapproximationproperties[ParkandSandberg,1991]andthe
softregularizationinheritedfromstochasticgradientdescentlearningviabackpropagation[Amari,
1993]. However,despiteitscomputationalefficiency,accuracy,andscalabilitytoincreasinglylarge
datasets and models, DNNs trained this way do not provide well calibrated predictions and un-
certainty estimates, and practitioners typically utilize post-hoc calibration methods on validation
datasets [Wang et al., 2021, Shao et al., 2020]. This limits the applicability and reliability of us-
ing DNNs in safety-critical applications like autonomous driving, medicine, and disaster response
[Papamarkouetal.,2024],whereuncertainty-sensitivedecision-makingisrequired.
Bayesianmachinelearningaddressestheissuesofpoorcalibrationanduncertaintyquantificationby
offeringaprobabilisticframeworkthatcastslearningmodelparametersθθθasaprocessofinference
- namely, calculating a posterior distribution over model parameters, given observed data (DDD =
(cid:0) (cid:1)
(xxx ,yyy ),...,(xxx ,yyy ) ),usingBayes’rule:
1 1 n n
(cid:0) (cid:1) p(θθθ,DDD)
p θθθ |DDD = (1)
p(θθθ)
Theresultingposteriordistributioncapturesbothexpectationsaboutmodelparametersθθθ andtheir
uncertainty. Theuncertaintyisthenincorporatedinpredictionsthatare,inprinciple,well-calibrated
to novel datapoints coming from the same set. This probabilistic treatment allows methods like
Bayesian neural networks (BNNs) [Herna´ndez-Lobato and Adams, 2015] to maintain the expres-
siveness of deep neural networks while also encoding uncertainty over network weights and thus
the network’s predictions. However, these methods are known to come with a significant increase
in computational cost and thus scale poorly when applied to large datasets and high-dimensional
models[Izmailovetal.,2021].
Inthis paperwe introducea gradient-freevariational learningalgorithmfor aprobabilistic variant
of a two-layer, feedforward neural network — the conditional mixture network or CMN — and
measure its performance on supervised learning benchmarks. This methodrests on coordinate as-
cent variational inference (CAVI) [Wainwright et al., 2008, Hoffman et al., 2013] and hence we
name it CAVI-CMN. We compare CAVI-CMN to maximum likelihood estimation and two other
Bayesian estimation techniques: the No U-Turn Sampler (NUTS) variant of Hamiltonian Monte
Carlo [Hoffman et al., 2014] and black-box variational inference [Ranganath et al., 2014]. We
demonstrate that CAVI-CMN maintains the predictive accuracy and scalability of an architecture-
matchedfeedforwardneuralnetworkfitwithmaximumlikelihoodestimation(i.e.,gradientdescent
viabackpropagation),whilemaintainingfulldistributionsovernetworkparametersandgenerating
calibratedpredictions,asmeasuredinrelationshiptostate-of-the-artBayesianmethodslikeNUTS
andBBVI.
Wesummarizethecontributionsofthisworkbelow:
• Introduce and derive a variational inference scheme for the conditional mixture network,
whichwetermCAVI-CMN.Thisreliesontheuseofconjugatepriorsforthelinearexperts
andPo´lya-Gammaaugmentation[Polsonetal.,2013]forthegatingnetworkandthefinal
softmaxlayer.
2• CAVI-CMN matches, and sometimes exceeds, the performance of maximum likelihood
estimation(MLE)intermsofpredictiveaccuracy,whilemaintainingtheprobabilisticben-
efitsofbeingBayesian,likequantifyinguncertaintyandhavelowcalibrationerror. Thisis
shownacrossasuiteof8differentsupervisedclassificationtasks(2synthetic,6real).
• CAVI-CMNdisplaysallthebenefitsexplainedabovewhilerequiringdrasticallylesstime
toconvergeandoverallruntimethantheotherstate-of-the-artBayesianmethodslikeNUTS
andBBVI.
Therestofthispaperisorganizedasfollows: first,wediscussrelatedworksincludetheMoEarchi-
tecture and existing (Bayesian and non-Bayesian approaches) to fitting these models. We then in-
troducetheprobabilisticconditionalmixturemodelandderiveavariationalinferencealgorithmfor
optimizing posterior distributions over its latent variables and parameters. We present experimen-
tal results comparing the performance of CAVI-based conditional mixture models with sampling
basedmethods,suchasBBVI,NUTS,andtraditionalMLEbasedestimation,wheregradientsofthe
log likelihood are computed using backpropagation and used to update the network’s parameters.
Finally,wediscusstheimplicationsofthesefindingsandpotentialdirectionsforfutureresearch.
2 Relatedwork
The Mixture-of-Experts (MoE) architecture is a close relative of the CMN model we introduce
here. Jacobsetal.[1991]originallyintroducedMoEsasawaytoimprovetheperformanceofneu-
ral networks by combining the strengths of multiple specialized models [Gormley and Fru¨hwirth-
Schnatter, 2019]. MoE models process inputs by averaging the predictions of individual learners
or experts, where each expert’s output is weighted by a different mixing coefficient before the av-
eraging. The fundamental idea behind MoE is that the input space can be partitioned in such a
way that different experts (models) can be trained to excel in different regions of this space, with
a gating network determining the appropriate expert (or combination of experts) for each input.
Thisleadstocomposable(andsometimesinterpretable)latentdescriptionsofarbitraryinput-output
relationships [Eigen et al., 2013], further bolstered by the MoE’s capacity for universal function
approximation [Nguyen et al., 2016, Nguyen and Chamroukhi, 2018]. Indeed, the powerful self-
attention mechanism employed by transformers has has demonstrated the power and flexibility of
MoE models [Movellan and Gabbur, 2020]. Non-Bayesian approaches to MoE typically rely on
maximum likelihood estimation (MLE) [Jacobs et al., 1991, Jordan and Jacobs, 1994], which can
sufferfromoverfittingandpoorgeneralizationduetothelackofregularizationmechanisms[Bishop
andSvenskn,2003],especiallyinlowdatasizeregimes.
Toaddresstheseissues,BayesianapproachestoMoEhavebeendeveloped,whichincorporateprior
information and yield posterior distributions over model parameters [Bishop and Svenskn, 2003,
Mossavat and Amft, 2011]. This Bayesian treatment enables the estimation of model evidence
(log marginal-likelihood) and provides a natural framework for model comparison and selection
[Svense´n,2003,Zens,2019]. BayesianMoEmodelsoffersignificantadvantages,suchasimproved
robustness against overfitting and a better understanding of uncertainty in predictions. However,
theyalsointroducecomputationalchallenges,particularlywhendealingwithhigh-dimensionaldata
andcomplexmodelstructures.
TheintroductionofthePo´lya-Gamma(PG)augmentationtechniqueinPolsonetal.[2013]enabled
a range of novel and more computationally efficient algorithms for Bayesian treatment of MoE
models[Lindermanetal.,2015,Heetal.,2019,Sharmaetal.,2019,ViroliandMcLachlan,2019,
Zensetal.,2023]. Herewecomplementthesepastworks,whichmostlyrestonimprovingsampling
methods with PG augmentation, by introducing a closed-form update rules for MoE’s with linear
expertsintheformofcoordinateascentvariationalinference(CAVI).
3 Methods
Inthissectionwefirstmotivatetheuseofconditionalmixturemodelsforsupervisedlearning,and
then introduce the conditional mixture network (CMN), the probabilistic model whose properties
andcapabilitieswedemonstrateintheremainderofthepaper.
33.1 Conditionalmixturesforfunctionapproximation
Feedforward neural networks are highly expressive, approximating nonlinear functions through
sequences of nonlinear transformations, but the posterior distributions over their weights are in-
tractable,requiringexpensivetechniqueslikeMCMCorvariationalinference[MacKay,1992,Blun-
delletal.,2015,Daxbergeretal.,2021].
WecircumventtheseproblemsbyfocusingontheMixture-of-Experts(MoE)models[Jacobsetal.,
1991],andparticularlyavariantofMoEthatisamenabletogradientfreeCAVIparameterupdates.
MoEs can be made tractable to gradient-free CAVI when the expert likelihoods are constrained
tobemembersoftheexponentialfamily(seeSection2formoredetailsontheMoEarchitecture),
andwhenthegatingnetworkisformulatedinsuchawaytoallowexactBayesianinference(through
lowerboundsonthelog-sigmoidlikelihood[JaakkolaandJordan,1997,BishopandSvenskn,2003]
orPo´lya-Gammaaugmentation[Polsonetal.,2013]).
TheMoEcanbereformulatedprobabilisticallyasamixturemodelbyintroducingalatentassign-
mentvariable,zn,leadingtoajointprobabilitydistributionoftheform
n
(cid:89)
p(Y,Z,Θ)=p(θ )p(π) p(yn|zn,θ )p(zn|π),
1:K 1:K
i=1
where yn is an observation, Θ = {θ ,π}, p (yn|θ ) is the kth-component’s likelihood and zn
1:K k k
isadiscretelatentvariablethatassignsthenth datapointtooneoftheK mixturecomponents, i.e.
p (yn|θ ) = p(yn|zn=k,θ ). Forinstance,ifeach‘expert’likelihoodp (yn|θ )isaGaussian
k k 1:K k k
distribution,thentheMoEbecomesaGaussianMixtureModel,whereθ =(µ ,Σ ).
k k k
Theproblemoflearningthemodel’sparameters,thenbecomesoneofdoinginferenceoverthela-
tentvariablesZandparametersΘofthemixturemodel.However,mixturemodelsaregenerallynot
tractable for exact Bayesian inference, so some form of approximation or sampling-based scheme
isrequiredtoobtainfullposteriorsovertheirparameters. However, ifeachexpert(i.e., likelihood
distribution) in the MoE belongs to the exponential family, the model becomes conditionally con-
jugate. This allows for derivation of exact fixed-point updates to an approximate posterior over
each expert’s parameters. The approach we propose, CAVI-CMN, does exactly this – we take ad-
vantage of the conditional conjugacy of mixture models, along with an augmentation trick for the
the gating network, to make all parameters amenable to an approximate Bayesian treatment. The
conditionally-conjugateformofthemodelallowsustousecoordinateascentvariationalinferenceto
obtainposteriorsovertheweightsofboththeindividuallinearexpertsandthegatingnetwork[Wain-
wrightetal.,2008,Hoffmanetal.,2013,Bleietal.,2017], withoutresortingtocostlygradientor
samplingcomputations.
Goingforwardweusethetermconditionalmixturenetworks(CMN)toemphasize(1)thediscrim-
inative nature of proposed application of this approach, where the model is designed to predict an
outputy givenaninputxand(2)thefactthatindividualMoElayerscanbestackedhierarchically
intoafeedforwardarchitecture. ThismakesCMNsparticularlysuitablefortaskssuchassupervised
classification and regression, where the goal is effectively that of function approximation; predict
someoutputvariableygiveninputregressorsx.
3.2 Conditionalmixturenetworkoverview
The conditional mixture network maps from a continuous input vectorxxx ∈ Rd to its label y ∈
0
{1,...,L}. Thisisachievedwithtwolayers: aconditionalmixtureoflinearexperts,whichoutputs
ajointcontinuous-discretelatent(cid:0) xxx ∈Rh,z ∈{1,...,K}(cid:1) andamultinomiallogisticregression,
1 1
whichmapsfromthecontinuouslatentxxx tothecorrespondinglabely. Theprobabilisticmapping
1
canbedescribedintermsofthefollowingoperations:
z ∼Mult(z ;xxx ,βββ )
1 1 0 0
xxx =AAA ·[xxx ;1]+uuu , uuu ∼N(000,ΣΣΣ )
1 z1 0 z1 z1 z1
y ∼Mult(y;xxx ,βββ )
1 1
where we pad the input variablexxx with a constant value set to 1, to absorb the bias term within
0
the mapping matrixAAA ∈ Rh×d+1, and where Mult(z;xxx,βββ) denotes a multinomial distribution
z1
parameterizedwitharegressorxxxandlogisticregressioncoefficientsβββ. Notethatforeverypairof
4regressorsandlabels(xxxn,yn),weassumeacorrespondingpairoflatentvariables(xxxn,zn). Written
0 1 1
inthisway,itbecomesclearthanCMNisamixtureoflineartransformsthatiscapableofmodeling
non-lineartransferfunctionsviaapiecewiselinearapproximation.
Inordertoobtainanormallydistributedposterioroverthemultinomiallogisticregressionweights,
βββ andβββ ,weusePo´lya-Gammaaugmentation[Polsonetal.,2013,Lindermanetal.,2015]applied
0 1
tothestickbreakingconstructionforthemultinomialdistribution:
k−1
(cid:0) (cid:1) (cid:89)(cid:0) (cid:1)
p z =k|βββ,xxx =π (βββ,xxx) 1−π (βββ,xxx)
k j
j=1
1 (2)
π (βββ,xxx)= ,∀j <K
j (cid:8) (cid:9)
1+exp −βββ ·[xxx;1]
j
π =1
K
where for the gating network (input layer) we will have coefficients of dimensionβββ ∈ RK−1×d,
0
andfortheoutputlikelihoodcoefficientsofdimensionβββ ∈RL−1×h.
1
3.3 Generativemodelfortheconditionalmixturenetwork
βββ
k,0
K−1
xxxn zn ΣΣΣ−1
0 1 k
xxxn AAA
1 k
K
yn βββ
l,1
N L−1
Figure 1: A Bayesian network representation of the two-layer conditional mixture network, with
input-outputpairsxxxn,yn andlatentvariablesxxxn,zn. Observationsareshadednodes,whilelatents
0 1 1
andparametersaretransparent.
Given a set of labels Y =
(cid:8) y1,y2,...,yN(cid:9)
, and regressors XXX =
(cid:8) xxx1,xxx2,...,xxxN(cid:9)
, that define
0 0 0 0
i.i.d input-output pairs xxxn,yn, we write the joint distribution over labels Y, latents XXX ,Z , and
0 1 1
parametersΘΘΘas:
N
p(YYY,XXX ,ZZZ ,ΘΘΘ|XXX )=p(ΘΘΘ)(cid:89) p (cid:0) yn|xxx (cid:1) p (cid:0) xxxn|xxxn,zn(cid:1) p (cid:0) zn|xxxn(cid:1)
1 1 0 βββ111 1 λλλ1 1 0 1 βββ0 1 0
n=1
p(ΘΘΘ)=p(βββ 1)p(βββ 0)p(λλλ 1) (3)
L−1 K−1 K
(cid:89) (cid:89) (cid:89) (cid:16) (cid:17)
= p(βββ ) p(βββ ) p AAA ,ΣΣΣ−1
l,1 k,0 j j
l=1 k=1 j=1
Notethatthismodelstructure, withinputandtargetvariables, isoftenreferredtoasadiscrimina-
tive model, as opposed to a generative model [Bernardo et al., 2007]. However, we use the term
generativemodeltoemphasizethefactthatthemodelcontainspriorsoverlatentvariables(XXX ,Z ),
1 1
(cid:18) (cid:16) (cid:17)(cid:19)
and parameters ΘΘΘ= βββ ,βββ ,AAA ,ΣΣΣ−1 , and that we are estimating posteriors
1:L−1,1 1:K−1,0 1:K 1:K
5over these quantities, by maximizing a lower bound on marginal likelihood of the observed target
variables Y. Note that going forward, we will sometimes useλλλ as notational shorthand for the
1
parametersAAA ,ΣΣΣ−1 ofthefirstlayer’slinearexperts.
1:K 1:K
Wespecifythefollowingconditionallyconjugatepriorsfortheparametersofthetwo-layerCMN:
(cid:16) (cid:17)
p AAA |ΣΣΣ−1 =MN (AAA ;000,ΣΣΣ ,v III )
k k k k 0 d+1
(cid:18) (cid:16) (cid:17)(cid:19) (cid:89)h (cid:16) (cid:17)
p ΣΣΣ−1 ≡diag σσσ−2 = Γ σ−2;a ,b
k k k,i 0 0
(4)
i=1
p(cid:0) βββ (cid:1) =N (cid:16) βββ ;000,σ2III (cid:17)
k,0 k,0 0 d+1
p(cid:0) βββ (cid:1) =N (cid:16) βββ ;000,σ2III (cid:17)
l,1 l,1 1 h+1
Inthefollowingsectionweintroduceamean-fieldvariationalinferenceschemeweuseforperform-
inginferenceandlearninginthetwo-layerCMN.
3.4 Coordinateascentvariationalinferencewithconjugatepriors
In this section we detail a variational approach for inverting the probabilistic model described in
Equation(3)andcomputinganapproximateposterioroverlatentsandparametersspecifiedas
N
p(cid:0)
XXX ,ZZZ
,ΘΘΘ|Y,XXX(cid:1)
=
p(Y,XXX 1,ZZZ 1,ΘΘΘ,XXX) ≈q(ΘΘΘ)(cid:89) q(zn)q(cid:0) xxxn|zn(cid:1)
(5)
1 1 p(cid:0) Y|XXX(cid:1) 1 1 1
n=1
where q(cid:0) xxxn|zn(cid:1) corresponds to a component specific multivariate normal distribution, and q(zn)
1 1 1
toamultinomialdistribution. Importantly,theapproximateposterioroverparametersq(ΘΘΘ)further
factorizes[Svense´n,2003]as
L−1 K−1 K
q(ΘΘΘ)=
(cid:89) q(cid:0)
βββ
(cid:1) (cid:89) q(cid:0)
βββ
(cid:1)(cid:89) q(cid:16)
AAA
,ΣΣΣ−1(cid:17)
l,1 k,0 j j
l=1 k=1 j=1
(cid:124) (cid:123)(cid:122) (cid:125)
=q(λλλ1)
(cid:0) (cid:1) (cid:0) (cid:1)
q βββ =N βββ ;µµµ ,ΣΣΣ
l,1 l,1 l,1 l,1
(cid:0) (cid:1) (cid:0) (cid:1) (6)
q βββ =N βββ ;µµµ ,ΣΣΣ
l,0 l,0 k,0 k,0
q(cid:16)
AAA
|ΣΣΣ−1(cid:17)
=MN
(cid:0)
AAA ;MMM ,ΣΣΣ ,VVV
(cid:1)
j j j j j j
h
(cid:16) (cid:17) (cid:89) (cid:16) (cid:17)
q ΣΣΣ−1 = Γ σ−2;a ,b
j i,j j i,j
i=1
The above form of the approximate posterior allows us to define tractable conditionally conjugate
updatesforeachfactor.Thisbecomesevidentfromthefollowingexpressionfortheevidencelower-
bound(ELBO)onthemarginalloglikelihood
 
L(q)=E
q(XXX1,ZZZ1)q(ΘΘΘ)(cid:88)N lnp
ΘΘ qΘ
(cid:0)(cid:0) zy nn (cid:1),x qxxn
1
(cid:0)
xxx,z n1n |z|xxx nn
0
(cid:1)(cid:1)
+E
q(ΘΘΘ)(cid:20) lnp q( (β ββ ββ β1) )p q(( ββ ββ ββ
0
)) qp (( λλλ λλλ
1
))(cid:21)
(7)
n=1 1 1 1 1 0 1
We maximize the ELBO using an iterative update scheme for the parameters of the approximate
posterior,oftenreferredtoasvariationalBayesianexpectationmaximisation(VBEM)[Beal,2003]
orcoordinateascentvariationalinference(CAVI)[BishopandNasrabadi,2006,Bleietal.,2017].
Theprocedureconsistsoftwoparts:
6First,wefixtheposteriorovertheparameters(torandomlyinitializedvalues). Giventheposterior
overparameters,weupdatetheposterioroverlatentvariables(variationalE-step)as
q (cid:0) xxxn|zn(cid:1) ∝exp(cid:26) E (cid:104) lnp (cid:0) yn|xxxn(cid:1) +lnp (cid:0) xxxn|xxxn,zn(cid:1)(cid:105)(cid:27)
t 1 1 qt−1(βββ1)qt−1(λλλ1) βββ1 1 λλλ1 1 0 1
 (cid:34) (cid:35) (8)
q (zn)∝exp E (cid:68) lnp (cid:0) yn,xxxn|xxxn,zn(cid:1)(cid:69) +lnp (cid:0) zn|xxxn(cid:1) 
t 1  qt−1(ΘΘΘ) βββ1,λλλ1 1 0 1 qt(xxxn 1|z 1n) βββ0 1 0 
Second,theposterioroverlatentsthatwasupdatedintheE-step,isusedtoupdatetheposteriorover
parameters(variationalM-step)as
 
q (βββ )∝exp(cid:88)N E (cid:104) lnp (cid:0) yn|xxxn(cid:1)(cid:105)
t 1

qt(xxxn 1,z 1n) βββ1 1

n=1
 
q (βββ )∝exp(cid:88)N E (cid:104) lnp (cid:0) zn|xxxn(cid:1)(cid:105) (9)
t 0

qt(z 1n) βββ1 1 0

n=1
 
q (λλλ )∝exp(cid:88)N E (cid:104) lnp (cid:0) xxxn|zn,xxxn(cid:1)(cid:105)
t 1

qt(xxxn 1,z 1n) λλλ1 1 1 0

n=1
In the variational inference literature, the distinction between latents and parameters is often de-
scribedintermsof‘local’vs‘global’latentvariables[Hoffmanetal.,2013],wherelocalvariables
aredatapoint-specific,andglobalvariablesaresharedacrossdatapoints. Theexactformoftheup-
datestotheparametersofthelinearexpertsinEquation(9),i.e. q (λλλ )=q (AAA ,ΣΣΣ−1 )arefound
t 1 t 1:K 1:K
inAppendixB.
Importantly,theupdateequationsdescribedinEquation(8)andinthefirsttwolinesofEquation(9)
are not computationally tractable without an additional approximation, known as Po´lya-Gamma
augmentation of the multinomial distribution. The full details of the augmentation procedure are
describedinAppendixA.1. Herewewillbrieflysketchthemainstepsanddescribethehighlevel,
augmented update equations. The Po´lya-Gamma augmentation introduces datapoint-specific aux-
iliaryvariables(ωωωn,ωωωn),thathelpustransformthelog-probabilityofthemultinomialdistribution
1 0
intoaquadraticfunction[Polsonetal.,2013,Lindermanetal.,2015]overcoefficients(βββ ,βββ ),and
latentsxxxn. Thisquadraticformenablestractableupdateofq(cid:0) xxxn|zn(cid:1) intheformofam1 ulti0 variate
1 1 1
normaldistribution,andatractableupdatingofposteriorsovercoefficientsq(βββ )andq(βββ ).
1 0
Withtheintroductionoftheauxiliaryvariablesthevariationalexpectationandmaximisationsteps
areexpressedas
7Updatelatents(‘E-step’)
(cid:40) (cid:20) (cid:21)(cid:41)
q (cid:0) xxxn|zn(cid:1) ∝exp E (cid:10) l(yn,xxxn,ωωωn,βββ )(cid:11) +lnp (cid:0) xxxn|xxxn,zn(cid:1)
t 1 1 qt−1(βββ1)qt−1(λλλ1) 1 1 1 qt−1(ωωωn 1|yn) λλλ1 1 0 1
(cid:26) (cid:27)
q (cid:0) ωωω |yn(cid:1) ∝p(cid:0) ωωωn|y (cid:1) exp E (cid:2) l(yn,xxxn,ωωωn,βββ )(cid:3)
t 1 1 n qt−1(βββ1)qt(xxxn 1|z 1n) 1 1 1
q (cid:0) ωωω |zn(cid:1) ∝p(cid:0) ωωωn|zn(cid:1) exp(cid:110) E (cid:2) l(zn,xxxn,ωωωn,βββ )(cid:3)(cid:111)
t 0 1 0 1 qt−1(βββ0) 1 0 0 0
(cid:26) (cid:104) (cid:105)(cid:27)
q (zn)∝exp E ¯l (yn,βββ )+R (xxxn,λλλ )+¯l (zn,xxxn,βββ )
t 1 qt−1(ΘΘΘ) z 1n,t 1 z 1n,t 0 1 t 1 0 0
Updateparameters(‘M-step’)
 
q (βββ
)∝exp(cid:88)N
E (cid:2) l(yn,xxxn,βββ
,ωωωn)(cid:3)
t 1

qt(xxxn 1,z 1n)qt(ωωωn 1|yn) 1 1 1

n=1
 
q (βββ
)∝exp(cid:88)N
E (cid:2) l(zn,xxxn,βββ
,ωωωn)(cid:3)
t 0

qt(z 1n)qt(ωωωn 0|z 1n) 1 0 0 0

n=1
(10)
where we omitted terms whose form did not change. R
z
1n,t(xxxn 0,λλλ 1) reflects a contribution to
q(zn) that depends on the expected log partition of the linear (Matrix Normal Gamma) likelihood
1
p (xxxn|xxxn,zn). Note that the updates to each subset of posteriors (latents or parameters) have an
λλλ1 1 0 1
analyticformduetotheconditionalconjugacyofthemodel. Importantly,bothpriorsandposterior
oftheauxiliaryvariablesarePo´lya-Gammadistributed[Polsonetal.,2013].
Finally, in the above update equations, we have replaced instances of the multinomial distribution
p(cid:0) z|xxx,βββ(cid:1) with its augmented form p(cid:0) ω|z(cid:1) el(z,xxx,ωωω,βββ) where the function l(·) is quadratic with
respecttothecoefficientsβββ andtheinputvariablesxxx,leadingtotractableupdateequations.
4 Results
To evaluate the effectiveness of the CAVI-based approach, we compared it to other approximate
inferencealgorithms,usingseveralrealandsyntheticdatasets. WecomparedCMNsfitwithCAVI
tothefollowingthreeapproaches:
MLE — We obtained point estimates for the parametersAAA ,ΣΣΣ−1 ,βββ ,βββ of the CMN using
1:K 1:K 0 1
maximum-likelihoodestimation. Forgradient-basedoptimizationofthelossfunction(the
negativeloglikelihood),weusedtheAdaBeliefoptimizerwithparameterssettoitsdefault
values as introduced in Zhuang et al. [2020] (α = 1e−3, β = 0.9, β = 0.999), and
1 2
runtheoptimizationfor20,000steps. Thisimplementsdeterministicgradientdescent,not
stochastic gradient descent, because we fit the model in ‘full-batch’ mode, i.e., without
splitting the data into mini-batches and updating model parameters using noisy gradient
estimates.
NUTS-HMC — We use the No-U-Turn Sampler (NUTS), an extension to the Hamiltonian
MonteCarlo(HMC)samplers,thatincorporatesadaptivestepsizes[Hoffmanetal.,2014].
MarkovChainMonteCarloconvergesindistributiontosamplesfromatargetdistribution,
soforthismethodweobtainsamplesfromajointdistributionp(AAA ,ΣΣΣ−1 ,βββ ,βββ |Y,XXX )
1:K 1:K 0 1 0
that approximate the true posterior. We used 800 warm-up steps, 16 independent chains,
and64samplesforeachchain.
BBVI—Black-BoxVariationalInference(BBVI)method[Ranganathetal.,2014]. Inconstrast,
toCAVI,theBBVImaximizesevidencelowerbound(ELBO)usingstochasticestimation
ofthegradientsofvariationalparameters.WhileBBVIdoesnotrequireconjugaterelation-
ships in the generative model, we use the same CMN model and variational distributions
asweuseforCAVI-CMN,inordertoensurefaircomparison. Forstochasticoptimization,
8Figure2: Performanceandruntimeresultsofthedifferentinferencealgorithmsonthe‘Pinwheel’
dataset. Thestandarddeviation(verticallines)oftheperformancemetricisdepictedtogetherwith
themeanestimate(circles)overdifferentruns. Thetoprowofsubplotsshowperformancemetrics
across training set sizes: test accuracy (top left); log predictive density (top center), and expected
calibration error (top right). The bottom row shows runtime metrics as a function of increasing
trainingsetsize: thenumberofiterationsrequiredtoachieveconvergence(lowerleft);andthetotal
runtime, estimated using the product of the number of iterations to convergence and the average
cost (in seconds) for running one iteration (lower right). The number of iterations required for
convergence was calculated by determining the number of gradient steps (or M steps, for CAVI)
taken before the ELBO (or negative log likelihood, for MLE) reached 95% of its maximum value
(seeAppendixEfordetailsonhowthesemetricswerecomputed).
we used the AdaBelief optimizer with learning rate α = 5e−3 (other hyperparameters
same as for MLE), used 8 samples to estimate the ELBO gradient (the num particles
argumentoftheTrace ELBO()class),andrantheoptimizerfor20,000steps).
FortheBayesianmethods(CAVI,NUTS,andBBVI),weusedthesameformfortheCMNpriors
(seeEquation(4)fortheirparameterization)andfixedthepriorparameterstothefollowingvalues,
usedforalldatasets: v =10,a =2,b =1,σ ,σ =5. Foralldatasets,wefixedthedimension
0 0 0 0 1
ofthecontinuouslatentxxx tobeh = L−1,whereListhenumberofclasses. ForthePinwheels
1
dataset(seeSection4.1below),wesetthenumberoflinearexperts(andhencethedimensionofthe
discretelatentzzz )tobeK =10,whileforallotherdatasetsweusedK =20.
1
4.1 Comparisononsyntheticdatasets
ForsyntheticdatasetsweselectedthePinwheelsandtheWaveformDomains[BreimanandStone,
1988]datasets. Thepinwheelsdatasetisasyntheticdatasetdesignedtotestthemodel’sabilityto
handle nonlinear decision boundaries and data with non-Gaussian densities. The dataset consists
of multiple clusters arranged in a pinwheel pattern, posing a challenging task for mixture models
[Johnson et al., 2016] due to the curved and elongated spatial distributions of the data. See Ap-
pendix C for the parameters we used to simulate the pinwheels dataset. Similarly, the Waveform
Domainsdatasetconsistsofsyntheticdatageneratedtoclassifythreedifferentwaveformpatterns,
whereeachclassisdescribedby21continuousattributes[BreimanandStone,1988].
We fit all inference methods using different training set sizes, where each next training set was
twiceaslargeastheprevious(forpinwheels: wetrainedusingtrainsizes50to1600;forwaveform
domains: train sizes 60 to 3840). This was done in order to study the robustness of performance
in the low data regime. For each training size, we used the same test-set to evaluate performance
9Figure 3: Performance and runtime results of the different models on the ‘Waveform Domains’
dataset. Thewaveformsdatasetconsistsofsyntheticdatageneratedtoclassifythreedifferentwave-
formpatterns.Eachinstanceisdescribedby21continuousattributes.Seehereformoreinformation
aboutthedataset. DescriptionsofeachsubplotaresameasintheFigure2legend.
(forpinwheels,500examples;forwaveformdomains:1160datapoints). Foreachinferencemethod
andexamplessetsize,wefitusingthesamebatchoftrainingdata,butwith16randomly-initialized
models(differentinitialposteriorsamplesorparameters).
Weassesstheperformanceofthedifferentinferencemethodsusingthreemainmetrics: predictive
accuracy(TestAccuracy),log-predictivedensity(LPD),andexpectedcalibrationerror(ECE).Log
predictivedensityisacommonmeasureofpredictiveaccuracyformethodsthatoutputprobabilities
[Gelmanetal.,2014], andexpectedcalibrationerrormeasureshowwellamodel’spredictionsare
calibratedtotheclassprobabilitiesobservedinthedata[Guoetal.,2017]. InFigure2wevisualize
eachofthesemetricsforthePinwheelsdatasetandinFigure3fortheWaveformdatasetasafunction
of training set size. The CAVI-based approach achieves comparable log predictive density and
calibration error to the other two Bayesian methods, which all outperform maximum likelihood
estimationinLPDandECE.Thisholdsacrosstrainingsetsizes,indicatingbettersampleefficiency.
4.2 Comparisononreal-worlddatasets
TofurthervalidatetheperformanceofCAVI-CMN,weconductedexperimentsusing6real-world
classificationdatasetsfromtheUCIMachineLearningRepository[Kellyetal.,2024]. Table1sum-
marizestheperformanceofthedifferentalgorithmsonall7differentUCIdatasets(theWaveform
domainsdatasetandthe6realdatasets),usingthewidely-applicableinformationcriterion(WAIC)
as a measure of performance. WAIC is an approximate estimate of leave-one-out cross-validation
[Vehtarietal.,2017].
The CAVI-CMN approach consistently provided higher WAIC scores in comparison to the MLE
algorithm,andWAICscoresthatwereonparwithBBVIandNUTS.Theresultsconfirmthatusing
fully conjugate priors within the CAVI framework, does not diminishes the inference and the pre-
dictiveperformanceofthealgorithm,whencomparedtothestate-of-the-artBayesianmethodslike
NUTSandBBVI.Importantly,CAVI-CMNofferssubstantialadvantagesintermsofcomputational
efficiencyasexploredinthenextsection.
10Rice BreastCancer Waveform VehicleSilh. Banknote Sonar Iris
CAVI -0.1820 -0.0504 -0.2921 -0.3281 -0.0206 -0.1544 -0.0747
MLE -0.3599 -0.3133 -0.5759 -0.7437 -0.3133 -0.3133 -0.5514
NUTS -0.1278 -0.0324 -0.3753 -0.3767 -0.0110 -0.0306 -0.0413
BBVI -0.1739 -0.0763 -0.3618 -0.4154 -0.0382 -0.0583 -0.1544
Table1: Comparisonofwidely-applicableinformationcriterion(WAIC)fordifferentmethodseval-
uatedon7differentUCIdatasets.
4.3 Runtimecomparison
TheNUTSalgorithm,althoughconsideredstate-of-the-artintermsofinferencerobustnessandac-
curacy(forwellcalibratedmodels[Gelmanetal.,2020]),isnotoriouslydifficulttoapplytolarge-
scaleproblems[CobbandJalaian,2021]. Hence,thepreferredalgorithmofchoiceforprobabilistic
machinelearningapplicationshavebeenmethodsgroundedinvariationalinference,suchasblack-
boxvariationalinference(BBVI)[Ranganathetal.,2014]andstochasticvariationalinference(SVI)
[Hoffmanetal.,2013].
In this subsection, we analyze the runtime efficiency of the MLE and BBVI algorithms for CMN
models,incomparisontoaCAVI-basedapproach. Thefocusisoncomparingthecomputationtime
asthenumberofparametersincreasesalongdifferentcomponentsofthemodel.
To ensure comprehensive comparison, we varied the complexity of the models by adjusting the
numberofcomponents,thedimensionalityoftheinputspace,andthenumberofdatapoints. These
modificationseffectivelyincreasethenumberofparametersallowingustoobservehoweachalgo-
rithmscaleswithmodelcomplexity.
Figure4: RelativescalingoffittingtimeinsecondsforMaximumLikelihood,BBVI,andCAVI,as
afunctionofthenumberofparameters. Thenumberofparametersitselfwasmanipulatedinthree
illustrativeways: changingtheinputdimensiond,changingthenumberoflinearexpertsK inthe
conditionalmixturelayer,andchangingthedimensionalityofthecontinuouslatentvariableh.
TheruntimeperformanceforvaryingdatasizeonPinwheeldatasetissummarizedinthebottomtwo
subplotsofFigure2whichshowstheruntimeinseconds,andstepsuntilconvergencefordifferent
algorithms. We used the steps until convergence to assess the runtime for each algorithm. As
expected, all algorithms exhibit an increase in runtime as the number of training data increases
(which also scales the number of parameters for BBVI and CAVI). However the rate of increase
variessignificantlyacrossdifferentalgorithms,withCAVI-CMNapproachshowingthebestscaling
behavior.
Similarly,inFigure4weplottherelativeruntimesofMaximumLikelihood,CAVI,andBBVI(pro-
portional to the runtime of the least complex variant), as we increase the number of parameters
along different elements of CMN. This shows how fitting CMNs with CAVI scales competitively
withgradient-basedmethodslikeBBVIandMaximumLikelihoodEstimation. However,theright-
mostsubplotindicatesthatasweincreasethedimensionalityofthelatentvariableXXX ,CAVI-CMN
1
11scalesmoredramaticallythantheothertwomethods.Thisinheritsfromthecomputationaloverhead
ofmatrixoperationsrequiredbystoringmultivariateGaussiansposteriorsovereachcontinuousla-
tent,i.e.,q(xxxn|zn)=N(xxxn;µµµn,ΣΣΣn).RunningtheCAVIalgorithminvolvesoperations(likematrix
1 1 1 1 1
inversions and matrix-vector products) whose (naive) complexity is quadratic in matrix size. This
explainsthenonlinearscalingofruntimeasafunctionofh,thedimensionofXXX . Thereareseveral
1
waystoaddressthisissue:
• Low-Rank Approximations: One might use low-rank approximations to the covariance
matrixΣΣΣn(e.g.,Choleskyoreigendecompositions).
1
• Diagonal Covariance Structure: Further constrain the covariance structure of q(xxxn)
1
by forcing the latent dimensions to be independent in the posterior, i.e., q(xxxn|zn) =
1 1
(cid:81)h N(xn ;µn ,(σn )2). Thiswouldthenmeanthatthenumberofparameterstostore
i=1 i,1 i,1 i,1
wouldonlygrowasK(2h)inthesizeofthedataset,ratherthanasK(h+O(h2)).
• Full Mean-Field Approximation: Enforce a full mean-field approximation betweenXXX
1
andZ ,sothatoneonlyneedstostoreq(xxxn)q(zn)ratherthanq(xxxn|zn)q(zn). Thiswould
1 1 1 1 1 1
reduce the number of multivariate normal parameters that would have to be stored and
operateduponbyafactorofK.
• SharedConditionalCovarianceStructure:Assumethattheconditionalcovariancestruc-
tureissharedacrossalltrainingdatapoints,i.e.,ΣΣΣn =ΣΣΣ ,foralln∈{1,2,...,n}.
1 1
All of these adjustments would help mitigate the quadratic runtime scaling of CAVI-CMN as the
latentdimensionhincreases.
In summary, both sets of runtime analyses (both absolute and relative) suggest CAVI-CMN may
be an attractive alternative to BBVI suitable for large-scale and time-sensitive applications, which
similarlyoffersafullyBayesiantreatmentoflatentvariablesandparameters,whilemaintainingfast
absoluteruntimeandtime-to-convergence.
5 Conclusion
We demonstrate that the CAVI-based approach for conditional mixture networks (CMN) signifi-
cantlyoutperformsthetraditionalmaximumlikelihoodestimation(MLE)basedapproach,interms
ofpredictiveperformanceandcalibration. Theimprovementinprobabilisticperformanceoverthe
MLEbasedapproachescanbeattributedtoimplicitregularisationviapriorinformation,andproper
handlingofposterioruncertaintyoverlatentstatesandparameters,leadingtoabetterrepresentation
oftheunderlyingdata,reflectedinimprovedcalibrationerrorandlogpredictivedensity,eveninlow
dataregimes.
OneofthekeyadvantagesoftheCAVI-basedapproachisitscomputationalefficiencycomparedto
the other Bayesian inference methods such as Black-Box variational inference and the No-U-turn
sampler (NUTS). While NUTS can sample from the full joint posterior distribution, which maxi-
mizesperformanceintermsofinferencequality,thiscomesattheexpenseofsubstantialcomputa-
tionalresources,especiallyforhighdimensionalandcomplexmodels[Hoffmanetal.,2013]. The
variationalmethodsofferascalablealternativetothisanswer,intheformofmethodslikeblack-box
variational inference (BBVI). Although BBVI is highly efficient in comparison to NUTS, it takes
longer to converge than CAVI when applied to CMN. Hence, we expect CAVI to be a more prac-
tical choice for large-scale application, especially when further combined with data mini-batching
methods[Hoffmanetal.,2013].
ThebenchmarkresultsshowthatCAVI-CMNalgorithmachievescomparableperformancetoBBVI
and NUTS in terms of predictive accuracy, log-predictive density and expected calibration error,
whilebeingsignificantlyfaster. Thisbalancebetweenpredictivelikelihoodandcalibration(jointly
viewedasindicatorsofsampleefficiency)isparticularlyimportantinreal-worldapplicationswhere
robustprediction,reflectiveofunderlyinguncertainty,arecrucial.
Furthermore, a straightforward mixture of linear components present in CMN, offers additional
interoperability benefits. By using the conditionally conjugate priors, and a corresponding mean-
field approximation over latent variables and model parameters, we facilitate easier interpretation
of the model parameters and their uncertainties. This is particularly valuable in domains where
12understandingtheunderlyingdata-generatingprocessisasimportantasthepredictiveperformance,
suchasinhealthcare,finance,andscientificresearch. Anotherimportantpointisthattheconjugate
form of the CMN means that variational updates end up resembling sums of sufficient statistics
collectedfromthedata;thismeanstheCAVIalgorithmwedescribedisreadilyamenabletoonline
computationandminibatching, wheresufficientstatisticscancomputedandsummedon-the-flyto
updatemodelparametersinastreamingfashion[Hoffmanetal.,2013]. Thisapproachwillbecome
necessarywhenscalingCAVI-CMNtodeeper(morethantwo-layer)models[ViroliandMcLachlan,
2019]andlargerdatasets,wherestoringallthesufficientstatisticsofthedatainmemorybecomes
prohibitive.
Overall,thesefindingsunderscorethepracticaladvantagesofCAVI-CMNandhighlightitspromise
asanewtoolforfastprobabilisticmachinelearning.
Codeavailability
The code for using CAVI and the other 3 methods to fit the CMN model on the pinwheel and
UCIdatasets,isavailablefromthecavi-cmnrepository,whichcanbefoundatthefollowinglink:
https://github.com/VersesTech/cavi-cmn.
AcknowledgmentsandDisclosureofFunding
TheauthorswouldliketothankthemembersoftheVERSESMachineLearningFoundationsgroup
forcriticaldiscussionsandfeedbackthatimprovedthequalityofthiswork,withspecialthanksto
TommasoSalvatori, TimVerbelen, MagnusKoudahl, ToonvanderMaele, HampusLinander, and
KarlFriston.
References
Rice (Cammeo and Osmancik). UCI Machine Learning Repository, 2019. DOI:
https://doi.org/10.24432/C5MW4Z.
Shun-ichi Amari. Backpropagation and stochastic gradient descent method. Neurocomputing, 5
(4-5):185–196,1993.
Matthew James Beal. Variational algorithms for approximate Bayesian inference. University of
London,UniversityCollegeLondon(UnitedKingdom),2003.
JMBernardo,MJBayarri,JOBerger,APDawid,DHeckerman,AFMSmith,andMWest. Gener-
ativeordiscriminative? gettingthebestofbothworlds. Bayesianstatistics,8(3):3–24,2007.
Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine learning, vol-
ume4. Springer,2006.
ChristopherM.BishopandMarkusSvenskn. Bayesianhierarchicalmixturesofexperts. InUAI’03
ProceedingsoftheNineteenthconferenceonUncertaintyinArtificialIntelligence,pages57–64.
MorganKaufmannPublishersInc.,2003. ISBN0-127-05664-5.
DavidMBlei, AlpKucukelbir, andJonDMcAuliffe. Variationalinference: Areviewforstatisti-
cians. JournaloftheAmericanstatisticalAssociation,112(518):859–877,2017.
Charles Blundell, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. Weight uncertainty
inneuralnetwork. InInternationalconferenceonmachinelearning, pages1613–1622.PMLR,
2015.
L. Breiman and C.J. Stone. Waveform Database Generator (Version 1). UCI Machine Learning
Repository,1988. DOI:https://doi.org/10.24432/C5CS3C.
AdamDCobbandBrianJalaian.Scalinghamiltonianmontecarloinferenceforbayesianneuralnet-
workswithsymmetricsplitting. InUncertaintyinArtificialIntelligence,pages675–685.PMLR,
2021.
13Erik Daxberger, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer, and
PhilippHennig.Laplaceredux-effortlessbayesiandeeplearning.AdvancesinNeuralInformation
ProcessingSystems,34:20089–20103,2021.
DanieleDuranteandTommasoRigon. ConditionallyConjugateMean-FieldVariationalBayesfor
Logistic Models. Statistical Science, 34(3):472 – 485, 2019. doi: 10.1214/19-STS712. URL
https://doi.org/10.1214/19-STS712.
DavidEigen,Marc’AurelioRanzato,andIlyaSutskever.Learningfactoredrepresentationsinadeep
mixtureofexperts. arXivpreprintarXiv:1312.4314,2013.
AndrewGelman,JessicaHwang,andAkiVehtari. Understandingpredictiveinformationcriteriafor
bayesianmodels. Statisticsandcomputing,24:997–1016,2014.
AndrewGelman,AkiVehtari,DanielSimpson,CharlesCMargossian,BobCarpenter,YulingYao,
LaurenKennedy,JonahGabry,Paul-ChristianBu¨rkner,andMartinModra´k. Bayesianworkflow.
arXivpreprintarXiv:2011.01808,2020.
IsobelClaireGormleyandSylviaFru¨hwirth-Schnatter. Mixtureofexpertsmodels. InHandbookof
mixtureanalysis,pages271–307.ChapmanandHall/CRC,2019.
Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q Weinberger. On calibration of modern neural
networks. InInternationalconferenceonmachinelearning,pages1321–1330.PMLR,2017.
Jingyu He, Nicholas G Polson, and Jianeng Xu. Data augementation with polya inverse gamma.
arXivpreprintarXiv:1905.12141,2019.
Jose´ MiguelHerna´ndez-LobatoandRyanAdams. Probabilisticbackpropagationforscalablelearn-
ingofbayesianneuralnetworks. InInternationalconferenceonmachinelearning,pages1861–
1869.PMLR,2015.
Matthew D Hoffman, David M Blei, Chong Wang, and John Paisley. Stochastic variational infer-
ence. JournalofMachineLearningResearch,2013.
MatthewDHoffman,AndrewGelman,etal. Theno-u-turnsampler:adaptivelysettingpathlengths
inhamiltonianmontecarlo. J.Mach.Learn.Res.,15(1):1593–1623,2014.
PavelIzmailov, SharadVikram, MatthewDHoffman, andAndrewGordonGordonWilson. What
arebayesianneuralnetworkposteriorsreallylike? InInternationalconferenceonmachinelearn-
ing,pages4629–4640.PMLR,2021.
TSJaakkolaandMIJordan. Bayesianparameterestimationthroughvariationalmethods. Statistics
andComputing,1997.
RobertAJacobs,MichaelIJordan,StevenJNowlan,andGeoffreyEHinton. Adaptivemixturesof
localexperts. Neuralcomputation,3(1):79–87,1991.
Matthew J Johnson, David K Duvenaud, Alex Wiltschko, Ryan P Adams, and Sandeep R Datta.
Composinggraphicalmodelswithneuralnetworksforstructuredrepresentationsandfastinfer-
ence. Advancesinneuralinformationprocessingsystems,29,2016.
Michael I Jordan and Robert A Jacobs. Hierarchical mixtures of experts and the em algorithm.
Neuralcomputation,6(2):181–214,1994.
Markelle Kelly, Rachel Longjohn, and Kolby Nottingham. The uci machine learning repository,
2024. URLhttps://archive.ics.uci.edu. https://archive.ics.uci.edu.
Scott Linderman, Matthew J Johnson, and Ryan P Adams. Dependent multinomial models made
easy: Stick-breakingwiththepo´lya-gammaaugmentation. Advancesinneuralinformationpro-
cessingsystems,28,2015.
Volker Lohweg. Banknote Authentication. UCI Machine Learning Repository, 2013. DOI:
https://doi.org/10.24432/C55P57.
14DavidJCMacKay. Apracticalbayesianframeworkforbackpropagationnetworks. Neuralcompu-
tation,4(3):448–472,1992.
Iman Mossavat and Oliver Amft. Sparse bayesian hierarchical mixture of experts. In 2011 IEEE
StatisticalSignalProcessingWorkshop(SSP),pages653–656.IEEE,2011.
JavierRMovellanandPrasadGabbur.Probabilistictransformers.arXivpreprintarXiv:2010.15583,
2020.
PeteMowforthandBarryShepherd. Statlog(VehicleSilhouettes). UCIMachineLearningReposi-
tory. DOI:https://doi.org/10.24432/C5HG6N.
Hien D Nguyen and Faicel Chamroukhi. Practical and theoretical aspects of mixture-of-experts
modeling: Anoverview. WileyInterdisciplinaryReviews: DataMiningandKnowledgeDiscov-
ery,8(4):e1246,2018.
HienDNguyen,LukeRLloyd-Jones,andGeoffreyJMcLachlan. Auniversalapproximationtheo-
remformixture-of-expertsmodels. Neuralcomputation,28(12):2585–2593,2016.
Theodore Papamarkou, Maria Skoularidou, Konstantina Palla, Laurence Aitchison, Julyan Arbel,
DavidDunson,MaurizioFilippone,VincentFortuin,PhilippHennig,AliaksandrHubin,etal.Po-
sitionpaper:Bayesiandeeplearningintheageoflarge-scaleai.arXivpreprintarXiv:2402.00809,
2024.
Jooyoung Park and Irwin W Sandberg. Universal approximation using radial-basis-function net-
works. Neuralcomputation,3(2):246–257,1991.
NicholasGPolson,JamesGScott,andJesseWindle. Bayesianinferenceforlogisticmodelsusing
po´lya–gammalatentvariables. JournaloftheAmericanstatisticalAssociation,108(504):1339–
1349,2013.
Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial
intelligenceandstatistics,pages814–822.PMLR,2014.
Terry Sejnowski and R. Gorman. Connectionist Bench (Sonar, Mines vs. Rocks). UCI Machine
LearningRepository. DOI:https://doi.org/10.24432/C5T01Q.
ZhihuiShao,JianyiYang,andShaoleiRen. Calibratingdeepneuralnetworkclassifiersonout-of-
distributiondatasets. arXivpreprintarXiv:2006.08914,2020.
Archit Sharma, Siddhartha Saxena, and Piyush Rai. A flexible probabilistic framework for large-
marginmixtureofexperts. MachineLearning,108:1369–1393,2019.
ChristopherMBishopMarkusSvense´n. Bayesianhierarchicalmixturesofexperts. InToappearin:
UncertaintyinArtificialIntelligence: ProceedingsoftheNineteenthConference,page1,2003.
AkiVehtari, AndrewGelman, andJonahGabry. Practicalbayesianmodelevaluationusingleave-
one-outcross-validationandwaic. Statisticsandcomputing,27:1413–1432,2017.
CinziaViroliandGeoffreyJMcLachlan. Deepgaussianmixturemodels. StatisticsandComputing,
29:43–51,2019.
MartinJWainwright,MichaelIJordan,etal.Graphicalmodels,exponentialfamilies,andvariational
inference. FoundationsandTrends®inMachineLearning,1(1–2):1–305,2008.
Deng-BaoWang,LeiFeng,andMin-LingZhang. Rethinkingcalibrationofdeepneuralnetworks:
Do not be afraid of overconfidence. Advances in Neural Information Processing Systems, 34:
11809–11820,2021.
WilliamWolberg, OlviMangasarian, NickStreet, andW.Street. BreastCancerWisconsin(Diag-
nostic). UCIMachineLearningRepository,1995. DOI:https://doi.org/10.24432/C5DW2B.
GregorZens. Bayesianshrinkageinmixture-of-expertsmodels: identifyingrobustdeterminantsof
classmembership. AdvancesinDataAnalysisandClassification,13(4):1019–1051,2019.
15Gregor Zens, Sylvia Fru¨hwirth-Schnatter, and Helga Wagner. Ultimate po´lya gamma samplers–
efficient mcmc for possibly imbalanced binary and categorical data. Journal of the American
StatisticalAssociation,pages1–12,2023.
Juntang Zhuang, Tommy Tang, Yifan Ding, Sekhar C Tatikonda, Nicha Dvornek, Xenophon Pa-
pademetris,andJamesDuncan.Adabeliefoptimizer:Adaptingstepsizesbythebeliefinobserved
gradients. Advancesinneuralinformationprocessingsystems,33:18795–18806,2020.
16A VariationalBayesianMultinomialLogisticRegression
Inthissection,wefocusonasinglemultinomiallogisticregressionmodel(notinthecontextofthe
CMN),buttheensuingvariationalupdateschemederivedinAppendixA.4isappliedinpracticeto
boththegatingnetwork’sparametersβββ aswellasthoseofthefinaloutputlikelihoodfortheclass
0
labelβββ .
1
A.1 Stick-breakingreparameterizationofamultinomialdistribution
Multinomiallogisticregressionconsiderstheprobabilitythatanoutcomevariableybelongstoone
of K mutually-exclusive classes or categories. The probability of y belonging to the kth class is
givenbythecategoricallikelihood:
p(y =k|xxx,βββ)=p (11)
k
The problem of multinomial logistic regression is to identify or estimate the values of regression
coefficients βββ that explain the relationship between some dataset of given continuous input re-
gressorsXXX = (xxx1,xxx2,...,xxxN)andcorrespondingcategoricallabelsY = (y1,y2,...,yN),yn ∈
1,2,...,K.
Wecanuseastick-breakingconstructiontoparameterizethelikelihoodoveryusingasetofK−1
stick-breaking coefficients: πππ = (π ,...,π ). Each coefficient is parameterized with an input
1 K−1
regressorxxx,andacorrespondingsetofregressionweightsβββ . Stick-breakingcoefficientπ isthen
j j
givenbyasigmoidtransformoftheproductoftheregressionweightsandtheinputregressors:
(cid:0) (cid:1)
π =σ βββ [xxx;1] ,
j j
(cid:0) (cid:1) 1
where σ βββ [xxx;1] = ,
j (cid:8) (cid:9)
1+exp −βββ [xxx;1]
j (12)
d
(cid:88)
and βββ [xxx;1]= w x +a .
j j,i i j
i=1
Theoutcomelikelihoodisthenobtainedviastickbreakingtransform4asfollows
K (cid:89)−1 (cid:0) (cid:1)K (cid:89)−1 (cid:16) (cid:0) (cid:1)(cid:17) K (cid:89)−1 exp(cid:8) βββ j[xxx;1](cid:9)
p =π (1−π )=σ βββ [xxx;1] 1−σ βββ [xxx;1] = (13)
k K j K j (cid:8) (cid:9)
1+exp βββ [xxx;1]
j=1 j=1 j=1 j
whereπ =1,andβββ =⃗0.
K K
Finally,wecanexpressthelikelihoodintheformofaCategoricaldistributionas
(cid:16) (cid:8) (cid:9)(cid:17)δk,y
K−1 exp βββ [xxx;1]
(cid:89) k
Cat(y;xxx,βββ)= . (14)
(cid:16) (cid:8) (cid:9)(cid:17)Nk,y
k=1 1+exp βββ k[xxx;1]
whereN = 1fork ≤ y,andN = 0otherwise(orN =
1−(cid:80)k−1δ
),andδ = 1for
k,y k,y k,y j=1 j,y k,y
k =yandiszerootherwise.
A.2 Po´lya-Gammaaugmentation
ThePo´lya-Gammaaugmentationscheme[Polsonetal.,2013,Lindermanetal.,2015,Duranteand
Rigon,2019]isdefinedas
4Thisblogposthashelpfuldiscussiononthestick-breakingformofthemultinomiallogisticlikelihoodand
providesmoreintuitionbehinditsfunctionalform.
17(cid:0) eψ(cid:1)a (cid:90) ∞
=2−beκψ e−ωψ2/2p(ω)dω (15)
(cid:0) (cid:1)b
1+eψ 0
(cid:0) (cid:1)
whereκ = a−b/2andp ω|b,0 isthedensityofthePo´lya-GammadistributionPG(b,0)which
doesnotdependonψ.TheusefulpropertiesofthePo´lya-Gammaaretheexponentialtiltingproperty
expressedas
e−ωψ2/2PG(ω;b,0)
PG(ω;b,ψ)= (16)
E(cid:2) e−ωψ2/2(cid:3)
theexpectedvalueofω,ande−ωψ2/2givenas
(cid:90) ∞ b (cid:18) ψ(cid:19)
E[ω]= ωPG(ω;b,ψ)dω = tanh ,
2ψ 2
o (17)
(cid:104) (cid:105) (cid:18) ψ(cid:19)
E e−ωψ2/2 =cosh−b
2
and the Kulback-Leibler divergence between q(ω) = PG(ω;b,ψ) and p(ω) = PG(ω;b,0) ob-
tainedas
D
(cid:2) q(ω)||p(ω)(cid:3) =−E[ω]ψ2 +blncosh(cid:18) ψ(cid:19) =−bψ tanh(cid:18) ψ(cid:19) +blncosh(cid:18) ψ(cid:19)
. (18)
KL 2 2 4 2 2
WecanexpressthelikelihoodfunctioninEquation(14)usingtheaugmentationas
K−1
p(y,ωωω|ψψψ)=p(cid:0) y|ψψψ(cid:1) p(cid:0) ωωω|y,ψψψ(cid:1) = (cid:89) 2−bk,yeκk,yψk−ωkψ k2/2PG(ω k;b k,y,0)
k=1
K−1 (cid:90) ∞
p(cid:0) y|ψψψ(cid:1) = (cid:89) 2−bk,yeκk,yψk e−ωkψ k2/2PG(ω k;b k,y,0)dω
k
(19)
k=1 0
K−1
(cid:0) (cid:1) (cid:89) (cid:0) (cid:1)
p ωωω|y,ψψψ = PG ω ;b ,ψ
k k,y k
k=1
whereb ≡ N ,κ = δ −N /2,andψ =βββ [xxx;1]. Givenapriordistributionp(ψψψ) =
k,y k,y k,y k,y k,y k kkk
p(βββ)p(xxx),wecanwritethejointp(y,ωωω,ψψψ)as
p(y,ωωω,ψψψ)=p(cid:0) ωωω|y(cid:1) p(ψψψ)el(y,ψψψ,ωωω),
K−1
(cid:88)
l(y,ψψψ,ωωω)= l (y,ψ ,ω ) , (20)
k k k
k=1
l (y,ψ ,ω )=κ ψ −b ln2−ω ψ2/2.
k k k y,k k y,k k k
A.3 Evidencelower-bound
GivenasetofobservationsDDD
=(cid:0) y1,...,yN(cid:1)
theaugmentedjointdistributioncanbeexpressedas
N
p(DDD,ΩΩΩ,XXX,βββ)=p(βββ)(cid:89) p(xxxn)p(cid:0) ωωωn|yn(cid:1) el(yn,ψψψn,ωωωn)
n=1
Wecanexpresstheevidencelower-bound(ELBO)as
18 
(cid:88)N p(yn,ψψψn,ωωωn)
L(q)=E q(ΩΩΩ)q(XXX)q(βββ)−lnq(βββ)+ ln
q(ωωωn)q(xxxn)

n=1
 
p(βββ) (cid:88)N p(cid:0) ωωωn|yn(cid:1) p(xxxn) (21)
=E q(ΩΩΩ)q(XXX)q(βββ)ln
q(βββ)
+ l(yn,ψψψn,ωωωn)+ln
q(ωωωn)
+ln q(xxxn)
n=1
≥lnp(DDD)
whereweusethefollowingformsfortheapproximateposterior
N N K−1
q(cid:0) ΩΩΩ|Y(cid:1)
=
(cid:89) q(cid:0) ωωωn|yn(cid:1)
=
(cid:89) (cid:89) PG(cid:0)
b ,ξ
(cid:1)
,
k,yn k,n
n=1 n=1 k=1
N N
(cid:89) (cid:89)
q(XXX)= q(xxxn)= N (xxxn;µµµn,ΣΣΣn) , (22)
n=1 n=1
K−1
(cid:89)
q(βββ)= N (βββ ;µµµ ,ΣΣΣ ) .
k k k
k=1
A.4 Coordinateascentvariationalinference
Themean-fieldassumptioninEquation(22)allowstheimplementationofasimpleCAVIalgorithm
[Wainwrightetal.,2008,Beal,2003,Hoffmanetal.,2013,Bleietal.,2017]whichsequentiallymax-
(cid:0) (cid:1)
imizestheevidencelowerboundinEquation(21)withrespecttoeachfactorinq ΩΩΩ|Y q(XXX)q(βββ),
viathefollowingupdates:
Updatetolatents(‘E-step’)
q(t,l)(xxxn)∝p(xxxn)exp(cid:110) E (cid:2) l(yn,ψψψn,ωωωn)(cid:3)(cid:111)
q(t−1)(βββ)q(t,l−1)(ωωωn)
q(t,l)(cid:0) ωn|yn(cid:1) ∝p(cid:0) ωn|yn(cid:1) exp(cid:110) E (cid:2) l (yn,ψn,ωn)(cid:3)(cid:111)
k k q(t−1)(βββ)q(t,l)(xxxn) k k k
∀n∈{1,...,N}, andforq(t,0)(cid:0) ωωωn|yn(cid:1) =q(t−1,L)(cid:0) ωωωn|yn(cid:1) (23)
Updatetoparameters(‘M-step’)
 
q(t)(βββ
)∝exp(cid:88)N
E (cid:2)
l(yn,ψψψn,ωωωn)(cid:3)
k q(t)(xxxn)q(t)(ωωωn|yn)
 
n=1
at each iteration t, and multiple local iteration l during the variational expectation step—until the
convergenceoftheELBO.
Specifically,theupdateequationsfortheparametersofthelatents(the‘E-step’)are:
 
q(t,l)(xxxn)∝N (cid:0) xxxn;0,−2λλλ (cid:1) exp(cid:88)K κ Tr(cid:16) µµµ(t−1)[xxxn;1]T(cid:17) − ⟨ω k⟩ Tr(cid:16) MMM(t−1)[xxxn;1][xxxn;1]T(cid:17)
2,0 k,yn k 2 k
 
k=1
λλλ(n,t,l)
=K (cid:88)−1(cid:26)
κ
(cid:104) µµµ(t−1)(cid:105)
−⟨ωn⟩
(cid:104) MMM(t−1)(cid:105) (cid:27)
1 k,yn k k t,l−1 k
1:D D+1,1:D
k=1
K−1
λλλ(n,t,l) =λλλ − 1 (cid:88) ⟨ωn⟩ [MMM ]
2 2,0 2 k t,l−1 k 1:D,1:D
k=1
(cid:104) (cid:105)T
MMM(t−1) =ΣΣΣ(t−1)+µµµ(t−1) µµµ(t−1)
k k k k
(24)
19and
q(t,l)(cid:0) ω kn|yn(cid:1) ∝e−ω kn⟨ψ k2⟩/2PG(cid:0) ω kn;b k,yn,0(cid:1)
(cid:113)
ξn = E (cid:2) ψ2(cid:3)
k q(t−1)(βββ)q(t,l)(xxxn) k
(cid:115)
(cid:18) (cid:19)
ξn = Tr
MMM(t−1)MMMˆ(n,t,l)
(25)
k k
 
whereMMMˆ(n,t,l)
=(cid:104)MMM(n,t,l (cid:105))
T
µµµ(n,t,l)
 , andMMM(n,t,l)
=ΣΣΣ(n,t,l)+µµµ(n,t,l)(cid:104) µµµ(n,t,l)(cid:105)T
.
µµµ(n,t,l) 1
Similarly,fortheparameterupdates(‘M-step’)weget
 
q(t)(βββ )∝N (cid:16) βββ ;0,−2λλλ′ (cid:17) exp(cid:88)N κ Tr(cid:16) µµµˆ(n,t)βββT(cid:17) − ⟨ω k⟩n t Tr(cid:18) MMMˆ(t) βββ βββT(cid:19)
k k 2,0 k,yn k 2 i k k
 
n=1
λλλ( kt ,)
1
=(cid:88) κ k,ynµµµˆ(n,t) (26)
i
λλλ(t) =λλλ′ − 1
(cid:88)N
b k,yn
tanh(cid:32)
ξ
k(n,t)(cid:33)
MMMˆ(n,t)
k,2 2,0 4 ξ(n,t) 2
n=1 k
(cid:104) (cid:105)
whereµµµˆ(n,t) = µµµ(n,t);1 .
B VariationalBayesianMixtureofLinearTransforms
The variational ‘M-step’ in Equation (9) to update the parameters of the linear experts reduces to
astraightforwardformwheneachexpertparameterizesamultivariateGaussianlikelihoodoverthe
latent variablesXXX with Matrix Normal Gamma priors over the parameters of the linear function
1
thatmapsXXX toadistributionoverXXX .
0 1
Recalltheformoftheupdatetotheposteriorparametersofthelinearexperts:
 
(cid:16) (cid:17) (cid:88)N (cid:104) (cid:105)
q AAA ,ΣΣΣ−1 ∝exp E lnp(xxxn|zn,xxxn,AAA ,ΣΣΣ−1 ) (27)
1:K 1:K

q(xxxn 1,z 1n) 1 1 0 1:K 1:K

n=1
(cid:16) (cid:17)
Theapproximateposteriorsq AAA ,ΣΣΣ−1 andq(XXX ,Z )havethefollowingform:
1:K 1:K 1 1
 
K h
(cid:16) (cid:17) (cid:89) (cid:89)
q AAA 1:K,ΣΣΣ− 1:1
K
= MN(AAA k;MMM k,σσσ− k2I,VVV k) Γ(σ i− ,k2;a k,b i,k)
k=1 i=1
N K
q(cid:0) XXX |Z (cid:1) = (cid:89) (cid:89) N(xxxn;µµµn ,ΣΣΣn )
1 1 1 k,1 k,1
n=1k=1
N
(cid:89)
q(Z )= Cat(zn;γγγn)
1 1
n=1
Theparametersofthekthexpertq(AAA ,ΣΣΣ−1)canwrittenintermsofweightedupdatestotheMatrix
k k
NormalGamma’scanonicalparametersMMM ,VVV ,a andb :
k k k k
20N
VVV−1 =VVV−1 +(cid:88) γnxxxn(xxxn)⊤
k k,0 k 0 0
n=1
 
N
MMM
k
=MMM k,0VVV− k,1 0+(cid:88) γ knµµµn k,1(xxxn 0)⊤ VVV
k
n=1
(cid:80)N γn
a =a + n=1 k
k k,0 2
 
N
1 (cid:88) (cid:104) (cid:105) (cid:104) (cid:105) (cid:104) (cid:105)
b i,k =b i,k,0+ 2 γ kn ΣΣΣn k,1+µµµn k,1(µµµn k,1)⊤ ii− MMM kVVV− k1MMMT k ii+ MMM k,0VVV− k,1 0MMMT k,0 ii
n=1
(28)
wherethenotation[·] selectstheithelementofthediagonalofthematrixinthebrackets.
ii
C DatasetDescriptions
Wefitallinferencemethodsusingdifferenttrainingsetsizes,whereeachnexttrainingsetwastwice
aslargeastheprevious. Foreachtrainingsize,weusedthesametest-settoevaluateperformance.
Thetestsetwasensuredtohavethesamerelativeclassfrequenciesasinthetrainingset(s).Foreach
inference method and examples set size, we fit using the same batch of training data, but with 16
randomly-initializedmodels(differentinitialposteriorsamplesorparameters).
C.1 PinwheelsDataset
The pinwheels dataset is a synthetic dataset designed to test a model’s ability to handle nonlinear
decision boundaries and data with non-Gaussian densities [Johnson et al., 2016]. The structure of
thepinwheelsdatasetisdeterminedby4parameters: thenumberofclustersordistinctspirals;the
angulardeviation,whichdefineshowfarthespirallingclustersdeviatefromtheorigin;thetangential
deviation,whichdefinesthenoisevarianceof2-Dpointswithineachcluster;andtheangularrate,
whichdeterminesthecurvatureofeachspiral. Forevaluatingthefourmethods(CAVI-CMN,MLE,
BBVI,andNUTS)onthesyntheticpinwheelsdataset,wegeneratedadatasetwith5clusters,withan
angulardeviationof0.7,tangentialdeviationof0.3andangularrateof0.2.Weselectedthesevalues
by looking at the maximum achieved test accuracy across all the methods for different parameter
combinations and tried to upper-bound it 80%, which provides a low enough signal-to-noise ratio
tobeabletomeaningfullyshowdifferencesinprobabilisticmetricslikecalibrationandWAIC.For
pinwheels,wetrainedusingtrainsizes50to1600,doublingthenumberoftrainingexamplesateach
successive training set size. We tested using 500 held-out test examples generated using the same
parametersasusedforthetrainingset(s).
C.2 WaveformDomainsDataset
TheWaveformDomainsdatasetconsistsofsyntheticdatageneratedtoclassifythreedifferentwave-
formpatterns,whereeachclassisdescribedby21continuousattributes[BreimanandStone,1988].
For waveform domains, we fit each model on train sizes ranging from 60 to 3840 examples, and
testedonaheld-outsizeof1160examples. Seehereformoreinformationaboutthedataset.
C.3 VehicleSilhouettesDataset
This dataset involves classifying vehicle silhouettes into one of four types (bus, van, or two car
models) based on features extracted from 2D images captured at various angles [Mowforth and
Shepherd]. We fit each model on train sizes ranging from 20 to 650 examples, and tested on a
held-outsizeof205examples. Seehereformoreinformationaboutthedataset.
21C.4 RiceDataset
The Rice dataset contains measurements related to the classification of rice varieties, specifically
Cammeo and Osmancik [mis, 2019]. We fit each model on train sizes ranging from 40 to 2560
examples,andtestedonaheld-outsizeof1250. Seehereformoreinformationaboutthedataset.
C.5 BreastCancerDataset
The‘BreastCancerDiagnosis’dataset[Wolbergetal.,1995]containsfeaturesextractedfrombreast
mass images, which are then used to classify tumors as malignant or benign. See here for more
information about the dataset. We fit each model on train sizes ranging from 25 to 400 examples,
andtestedonaheld-outsizeof169.
C.6 Sonar(MinesvsRocks)Dataset
TheSonar(MinesvsRocks)datasetconsistsofsonarsignalsbouncedoffmetalcylindersandrocks
under various conditions. The dataset includes 111 patterns from metal cylinders (mines) and 97
patterns from rocks. Each pattern is represented by 60 continuous attributes corresponding to the
energywithinspecificfrequencybands[SejnowskiandGorman].Thetaskistoclassifyeachpattern
aseitheramine(M)orarock(R).Forthisdataset,wefiteachmodelontrainsizesrangingfrom8
to128examplesandtestedonaheld-outsizeof80examples. Seehereformoreinformationabout
thedataset.
C.7 BanknoteAuthenticationDataset
The‘BanknoteAuthentication’dataset[Lohweg,2013]containsfeaturesextractedfromimagesof
genuine and forged banknotes. It is primarily used for binary classification tasks to distinguish
betweenauthenticandcounterfeitbanknotes. Seehereformoreinformationaboutthedataset.
D UCIPerformanceResults
In Figures 5 to 9 we report the same performance and runtime metrics as in Figure 2 for 7 UCI
datasets,andfindthatwiththeexceptionoftheSonardataset,CAVIperformscompetitivelywithor
betterthanMLEonalldatasets,andalwaysoutperformsMLEintermsofLPDandECE.Runtime
scaling is similar as reported for the Pinwheels dataset in the main text; CAVI-CMN always con-
vergesinfewerstepsandisfasterthanBBVI,andeitheroutperformsoriscompetitivewithMLEin
termsofruntime.
E ModelConvergenceDetermination
Foreachinferencealgorithm,thenumberofiterationstakentoconvergewasdeterminedbyrunning
each algorithm for a sufficiently high number of gradient (respectively, CAVI update) steps such
thattheELBO(orloglikelihood-LL-forMLE)stoppedsignificantlychanging. Thiswasdeter-
mined(throughanecdotalinspectionovermanydifferentinitializationsandrunsacrossthedifferent
UCIdatasets)tobe20,000gradientstepsforBBVI,20,000gradientstepsforMLE,and500com-
binedCAVIupdatestepsforCAVI-CMN.Todeterminethetimetakentosufficientlyconverge,we
recorded the value of the ELBO or LL at each iteration, and fit an exponential decay function to
the negative of each curve. The parameters of the estimated exponential decay were then used to
determinethetimeatwhichthecurvedecayedto95%decayofitsvalue. Thistimewasreportedas
thenumberofstepstakentoconverge.
22Figure 5: Performance and runtime results of the different models on the ‘Vehicle Silhouettes’
dataset. DescriptionsofeachsubplotaresameasintheFigure2legend.
Figure6:Performanceandruntimeresultsofthedifferentmodelsonthe‘Rice’dataset.Descriptions
ofeachsubplotaresameasintheFigure2legend.
23Figure 7: Performance and runtime results of the different models on the ‘Breast Cancer’ dataset.
DescriptionsofeachsubplotaresameasintheFigure2legend.
Figure 8: Performance and runtime results of the different models on the ‘Connectionist Bench
(Sonar,Minesvs. Rocks)’dataset. DescriptionsofeachsubplotaresameasintheFigure2legend..
24Figure9:Performanceandruntimeresultsofthedifferentmodelsonthe‘BanknoteAuthentication’
dataset. DescriptionsofeachsubplotaresameasintheFigure2legend.
25