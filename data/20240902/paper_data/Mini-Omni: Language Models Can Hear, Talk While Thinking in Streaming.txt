Mini-Omni: Language Models Can Hear, Talk While
Thinking in Streaming
ZhifeiXie∗ ChangqiaoWu∗
TsinghuaUniversity
xzf24@mails.tsinghua.edu.cn changqiao.wu@gmail.com
https://github.com/gpt-omni/mini-omni
Abstract
Recentadvancesinlanguagemodelshaveachievedsignificantprogress. GPT-4o,
asanewmilestone,hasenabledreal-timeconversationswithhumans,demonstrat-
ingnear-humannaturalfluency. Suchhuman-computerinteractionnecessitates
modelswiththecapabilitytoperformreasoningdirectlywiththeaudiomodality
and generate output in streaming. However, this remains beyond the reach of
currentacademicmodels,astheytypicallydependonextraTTSsystemsforspeech
synthesis,resultinginundesirablelatency. ThispaperintroducestheMini-Omni,
anaudio-basedend-to-endconversationalmodel,capableofreal-timespeechinter-
action. Toachievethiscapability,weproposeatext-instructedspeechgeneration
method,alongwithbatch-parallelstrategiesduringinferencetofurtherboostthe
performance. Ourmethodalsohelpstoretaintheoriginalmodel’slanguageca-
pabilitieswithminimaldegradation,enablingotherworkstoestablishreal-time
interactioncapabilities. Wecallthistrainingmethod"AnyModelCanTalk". We
alsointroducetheVoiceAssistant-400Kdatasettofine-tunemodelsoptimizedfor
speechoutput. Toourbestknowledge,Mini-Omniisthefirstfullyend-to-end,
open-sourcemodelforreal-timespeechinteraction,offeringvaluablepotentialfor
futureresearch.
Figure1: TheMini-Omnimodelarchitecture.
∗equalcontribution
Technicalreport.
4202
guA
03
]IA.sc[
2v52761.8042:viXra1 Introduction
Recent developments in large language models have progressed rapidly, with models becoming
increasinglypowerful,suchasoff-the-shelfLlama3.1[meta,2024],Mixtral[mixtral,2024],Qwen-2
[Yang et al., 2024a], and the well-known GPT-4. As an extension of their capabilities, language
modelsarebeginningtomasterunderstandingothermodalities,exemplifiedbyLLaVA[Liuetal.,
2024],Qwen2-Audio[Chuetal.,2024]andVideo-llama[Zhangetal.,2023b]. Despitetheirstrength
inspecifictasks,asignificantgapremainsthathindersfurtherintegrationoflargelanguagemodels
intodailyapplication: real-timevoiceinteractioncapability. GPT-4o[openai,2024],introducedby
OpenAI,isthefirstmodeltofeaturereal-timemultimodalspeechinteractioncapabilities. Itcan
understandandengagewithvision,audio,andtextwhileenablingreal-timespeechconversations,
althoughitremainsclosed-source.Othermodelstypicallyadopttwoapproachestoincorporatespeech
capabilities. Thefirstisacascademethod,wherethelanguagemodelgeneratestext,followedbya
text-to-speech(TTS)modelforaudiosynthesis. Thisapproachintroducessignificantlatencydueto
thetimerequiredfortextgeneration,severelyimpactinguserexperience. Thesecond,anend-to-end
methodlikeSpeechGPT[Zhangetal.,2023a],generatestextbeforecontinuingtogenerateaudio.
However,thisstillrequireswaitingfortextgeneration. Largelanguagemodelsneedrealend-to-end
speechoutputcapabilitiestoprovidereal-timefeedback.
Enhancingmodelswithspeechoutputcapabilitiesisachallengingtask,primarilyduetofourfactors:
(1)ComplexityofAudioReasoning:Ourexperimentsindicatethatdirecttrainingforaudiomodality
reasoningishighlychallenging,oftenresultinginincoherentoutputsfromthemodel. (2)Model
Complexity: Incorporatingadditionalmodulesforspeechinputandoutputincreasestheoverall
complexity. (3)DifficultyinModalityAlignment: Thereasoningabilitiesdevelopedfortextare
difficulttotransfertotheaudiodomain. (4)ResourceDemands:Adaptingamodel’stextcapabilities
to the speech modality requires converting all data labels into audio and retraining, significantly
increasingresourceconsumption.
Inthispaper,weproposeMini-Omni,thefirstopen-sourcemulti-modellargelanguagemodelwith
real-timeconversationalcapabilities, featuringfullyend-to-endspeechinputandoutputabilities.
Italsoincludesvariousotheraudio-to-textfunctionalitiessuchasAutomaticSpeechRecognition
(ASR).Weadaptcurrentlyavailableoff-the-shelfmethodsfordiscretizingspeechtokensandemploy
thesimplestmodelarchitecture,makingiteasyforourmodelandapproachtobeadaptedbyother
researchers. Directaudioreasoningposessignificantchallenges;however,ourapproachsuccessfully
addressesthisusingonlya0.5Bmodelandalimitedamountofsynthesizedaudiodata. Importantly,
ourtrainingframeworkachievesthiswithoutheavyrelianceonextensivemodelcapabilitiesorlarge
volumesofdata.
Toleverageandpreservetheoriginalcapabilitiesofthelanguagemodel,weproposeaparallelgenera-
tionparadigminwhichthetransformersimultaneouslyproducesaudioandtexttokens. Subsequently,
we observed a minimal impact of the audio modality on text capabilities and further introduced
batch-basedparallelgeneration,whichsignificantlyenhancesthemodel’sreasoningabilityduring
streamingaudiooutput. Asapoinerr,weoptednottosacrificeaudioqualityforasimplerandlower
bitrateaudioencoder,inordertoreducethecomplexityofaudioinferenceinthemodel. However,to
ensureaudioquality,weselectedSNAC[Siuzdak,2024],amusic-gradeencoderfeatures8layersof
codebooksandprocesseshundredsoftokenspersecond. Innovatively,weappliedtext-instructed
delayedparallelgenerationtoaddresstheissueoflongSNACcodebooksequences. Experiments
showthattheaudiooutputqualityisonparwithcommonTTSsystems.
Wealsoproposeamethodthatrequiresminimaltrainingandmodificationoftheoriginalmodel,
enablingotherworkstorapidlydeveloptheirownspeechcapabilities. Werefertothisapproachas
"AnyModelCanTalk",designedtoachievespeechoutputusingalimitedamountofadditional
data. Theapproachextendspeechcapabilitiesthroughadditionaladaptersandpre-trainedmodels,
fine-tuning with a small amount of synthesized data. This is combined with the aforementioned
parallelmodelingapproachtoenablestreamingoutputinthenewmodalitywhilepreservingthe
originalmodel’sreasoningcapabilities.
To evaluate the capabilities of Mini-Omni, we first assessed its performance on traditional text-
to-speechmulti-modaltasks,includingtext-basedquestionanswering(textQA),automaticspeech
recognition(ASR),text-to-speechresponse,andspeech-basedquestionanswering(speechQA).The
model demonstrated strong proficiency in these fundamental tasks. Additionally, we conduct a
seriesofexperimentstoinvestigatetheimpactontheoriginalmodel’scapabilitiesandassessthe
2effectivenessandvariationsofourinferencemethod. Preliminaryexperimentsdemonstratethatbatch
parallelinferencepreservesthemodel’soriginalcapabilities. Wewillconductfurtherexperiments
andprovideadditionaldetailsinduecourse.
Lastly, we observed that most open-source QA datasets contain mixed code or overly lengthy
text, renderingthemunsuitableforspeechmodel. Toovercomethislimitation, weintroducethe
VoiceAssistant-400Kdataset,comprisingover400,000entriesspecificallygeneratedbyGPT-4ofor
speechassistantsupervisedfine-tuning(SFT).
Insummary,wemakethefollowingcontributions:
• WeintroduceMini-Omni,thefirstopen-sourceend-to-endmultimodallargemodelwithaudio
inputandaudiostreamingoutputcapabilities. Weproposeauniquetext-instructparallelgeneration
method that enables speech inference outputs aligned with textual capabilities, achieved with
minimaldata. Wefurtherenhancethiswithdelayedparallelism,acceleratingaudioinferencespeed.
• Weintroduce"AnyModelCanTalk",aninnovativeapproachthatenhancesperformancewithout
alteringthearchitectureoflargemodelsbyfocusingontrainingandinference.Ourmethodemploys
athree-phasetrainingprocessforspeech-to-textandtext-to-speechadapters,includingannealing
andSFT.Ourmethodinvolvesminimaltrainingandmodificationoftheoriginalmodel,aimingto
provideareferenceforincorporatinginteractioncapabilitiesintoothermodels.
• Weidentifiedshortcomingsinexistingopen-sourceQAdatasetswhentrainingaudioassistantsand
proposedadedicateddatasetforspeechmodeloutputs,calledVoiceAssistant-400K.Thisdataset,
synthesizedusingGPT-4o,canbeusedtofine-tunemodelstodevelopthetoneofavoiceassistant.
2 RelatedWork
MultimodalUnderstandingRecently,researchershavebeenincreasinglyfocusedonadvancing
unifiedmodelsforcross-modalunderstanding. Theseapproachestypicallyemployawell-pretrained
neural network as the encoder for relevant modalities, using a lightweight adapter to align the
encoder’soutputwiththetextinputoflanguagemodel. ClassicalworkssuchasLLaVA[Liuetal.,
2024],Flamingo[Alayracetal.,2022]andBLIP[Lietal.,2022]areusedforvisualunderstanding,
whileintheaudiodomain,modelslikeWhisper[Radfordetal.,2023]andBeats[Chenetal.,2022]
arecommonlyutilizedasencodersforsemanticandacousticfeatures. InLlama3.1, Whisperis
employed,whileSpeechVerse[Dasetal.,2024]leveragesWavLM[Huetal.,2024];SALMONN
[Tangetal.,2023],combineWhisperandBeatstoextractfeatures. Suchworksareoftenconstrained
toproducingoutputinthetextmodality.
AudioLanguageModelingRecently,anincreasingnumberofstudieshaveemployedaudiotokeniza-
tiontobridgethegapbetweenaudioandtext. Audiotokenizationconvertscontinuousaudiosignals
intodiscreteaudiotokens,enablinglargelanguagemodelstoperforminferenceandevencross-modal
interactions. Asaresult, avarietyofspeech-texttasks, suchasASR,TTS,musicunderstanding
and generation, and sound editing, can be accomplished. MegaTTS [Jiang et al., 2023] utilized
audiocodecsforspeechsynthesis,whileeffortslikeInstructTTS[Yangetal.,2024b],SpearTTS
[Kharitonov et al., 2023], and Voicebox [Le et al., 2024] have further explored optimizations in
decodingmethodsandconditioningtechniques,employingDiffusionastheconverterfromtokensto
audio.
Real-TimeHuman-MachineInteractionModelsSincetheintroductionofGPT-4o[openai,2024],
real-timeconversationalmodelshaveachievedunprecedentedresults,providingnear-instantaneous
voicefeedbacktouserinputs,markingasignificantmilestoneforthenextgenerationofmulti-modal
largemodels. However,thetechnicalimplementationsremainproprietary. Modelswithreal-time
interactioncapabilitiesarecurrentlyscarce. SpeechGPT[Zhangetal.,2023a]isanearlyend-to-end
speechinteractionmodel;however,itstillsuffersfromlatencyduetotheAudio-Text-Text-Audio(A-
T-T-A) process, similar to Spectron [Nachmani et al., 2023]. LauraGPT [Chen et al., 2023] also
employs a similar approach but not for voice conversation scenario. VITA [Fu et al., 2024] and
Qwen-audio2[Chuetal.,2024]aretwomodelsthatsupportvoiceinput,buttheyoutputtextandrely
onexternalTTSsystemsforspeechsynthesis. Mini-Omniisafullyend-to-endspeech-to-speech
conversationalmodel. Throughourexploration,wehaveidentifiedthebiggestchallengeinadvancing
thisfield: thelogicalinconsistencyinreasoningwhenonlytheaudiomodalityispresent,whichwe
willaddressinthefollowingchapter.
33 Mini-Omni
OurinnovationstemsfromexistingmethodssuchasSpeechGPT[Zhangetal.,2023a]andSpectron
[Nachmanietal.,2023]utilizetheA-T-T-Aapproach,whichmitigatesthechallengesofdirectaudio
learningbyguidingthespeechgenerationprocessthroughtext. However,generatingtextfirstand
thenaudioissuboptimalforreal-timedialoguescenarios. Toaddressthis,weproposeanovelmethod
forsimultaneoustextandaudiogeneration. Thisapproachhypothesizesthattextoutputshavehigher
informationdensity,allowingforthesameresponsewithfewertokens. Duringthegenerationof
audiotokens,themodeleffectivelyconditionsoncorrespondingtexttokens,akintoanonlineTTS
system. Priortogeneratingaudiotokens, paddingwithN tokensensuresthatthecorresponding
texttokensareproducedfirst,allowingthistoserveasahyperparameteradjustment. Additionally,
the model can also condition on speaker and style embeddings, facilitating control over speaker
characteristicsandstylisticelements. Inthissection,wewilldetailhowweimplementourideastep
bystep.
3.1 AudioLanguageModeling
ConsiderY =(y ∈V |i=1,...,t )asatextutterancefromavocabularyV withlengtht .
i txt txt txt txt
TheprobabilityofY canbeexpressedasp(Y) =
(cid:81)ttxt
p(y | y ,...,y ). Now,whendealing
i=1 i 1 i−1
withacontinuousspeechsignal,wecanconvertitintodiscretespeechtokens(dst),representedas
D =(d ∈V |i=1,··· ,t )usingatokenizer. InthiscontextV isthevocabularyofdiscrete
i dst dst dst
speech tokens. These discrete speech tokens can be treated as spoken language within V and
dst
modeled in a manner similar to text. We combine text and speech in a new vocabulary V by
voxt
V =V ∪V .Therefore,wecanmodeltheprobabilityofbothspeechandtexttokensasZ,where
voxt txt dst
Z =(z ∈V|i=1,··· ,t).
Thisprobabilityisexpressedasp(Z)=(cid:81)t
p(z |z ,··· ,z ),Z
i i=1 i 1 i−1
representdiscretespeechtokensD(V =V )ortexttokensY(V =V )orvariouscombinationsof
dst txt
Y andD. Fortheaudioandtexttokensgeneratedsimultaneously,thenegativelog-likelihoodloss
canbeformulatedasinEquation(1).
m nj
(cid:88)(cid:88)
L(T,A|C)= logP(T ,A |T ,A ;X ) (1)
i,j i,j <i,j <i,j j
j=1i=1
whereT,Aisthetext-audiooutputpairsinthetrainingcorpusC,andmisthenumberoftraining
examples. X istheinputconditionofj-thexample,n ismaxnumberoftokensofsampleT and
j j j
A ,T andA representthei-thtexttokenandaudiotokenofj-thsample.
j i,j i,j
3.2 DecodingStrategies
AudioGenerationwithtextinstruction. Languagemodelshaveundergonesubstantialadvance-
ments, demonstrating exceptional reasoning capabilities within the text modality. In response,
Mini-Omni has been restructured to transfer these reasoning abilities to streaming audio output
throughatext-audioparalleldecodingapproach. Thismethodsimultaneouslyoutputsbothaudioand
texttokens,withtheaudiogeneratedviatext-to-speechsynthesis,ensuringreal-timedeliverywhile
leveragingthetext-basedreasoningstrengths. Toalignwiththeinputsoflargemodels,allsequences
generatedinparallelaresummedbeforeproducingthenexttoken,asillustratedinFigure1. This
approachenablesthemodeltoachievereal-timevoiceoutputinchatscenarioswithminimalfirst
tokendelay.
Text-delayParallelDecoding. ParallelgenerationwasfirstintroducedbyMusicGen[Copetetal.,
2024] to accelerate the music generation process, and we have integrated this approach into the
textmodalitytoenhancereasoningcapabilities. Paralleldecodingisfeasiblebecauseaudiotoken
codebooksusedinlanguagemodeltrainingtypicallyconsistofmultiplelayers;generatingalllayers
simultaneouslycansignificantlyincreasemodelspeed. Forreal-timespeechoutputmodels,parallel
decodingisevenmorecritical,allowingforthegenerationofhundredsofaudiotokenspersecond
onstandarddevices. Inthispaper,weemploySNACastheaudioencoder,whichcomprisesseven
tokenlayerswithcomplementaryrelationships. Therefore,weemployeightsub-LanguageModel
headstogenerateeighttokens,includingtext,inasinglestep,whilemaintainingaone-stepdelay
betweenadjacentlayers. Sinceaudiotokensarederivedfromtextsynthesis,thetexttokenisoutput
first,followedbySNACtokensfromthefirsttotheseventhlayer. Theprocessoftext-firstdelay
paralleldecodingweproposeisillustratedinFigure2(b).
4Figure2: Mini-Omniincorporatestext-instructmechanismsalongsideBatchparallelgeneration
techniques.
BatchParallelDecoding. Althoughthepreviouslyintroducedparallelgenerationmethodeffectively
transfersreasoningcapabilitiesfromthetextmodalitytotheaudiomodality,ourexperimentsreveal
thatthemodel’sreasoningperformancestillvariesbetweentextandaudiotasks,withaudioresponses
tendingtobesimpler. Wehypothesizethatthisisduetolimitationsinmodelcapacityorinsufficient
audio data. To address this issue and further enhance the model’s reasoning capabilities during
dialogue, maximizing the transfer of its text-based abilities, we experimentally employ a Batch
approach. Giventhemodel’sstrongerperformanceinthetextmodality,weexpandtheinferencetask
forasingleinputtoabatchsizeof2: onesamplerequiresbothtextandaudioresponses,asdescribed
earlier,whiletheothersampleonlyrequiresatextresponse,focusingontext-basedaudiosynthesis.
However,thetexttokenoutputfromthefirstsampleisdiscarded,andthetextoutputfromthesecond
sampleisembeddedintothecorrespondingtexttokenpositionsofthefirstsample. Simultaneously,
theaudiofromthefirstsampleisstreamedusingthecontentfromthetext-onlyresponseofthesecond
sample;wetermthisprocessbatchparalleldecoding.Throughthismethod,weeffectivelyandalmost
entirelytransferthe model’stext-based capabilities totheaudio modality withminimalresource
overhead,significantlyenhancingitsreasoningabilitiesinthenewmodality. Theinferenceprocessof
batchparalleldecodingisillustratedinFigure2(c). Webelievebatchparalleldecodingrepresentsa
keyalgorithmicinnovationthatenablessuchasmallmodeltoexhibitstrongconversationalabilities.
3.3 AnyModelCanTalk
In this section, we present our training methodology. Our approach is designed to preserve the
capabilities of the original model as much as possible. This is achieved firstly due to the strong
performanceofourbasemodel,andsecondlybecauseourmethodcanbeappliedtootherworksthat
excelintextoutputbutlackrobustspeechinteractioncapabilities.
Audio Encoding: The audio input primarily focuses on feature extraction from the input audio,
withoptionsincludingHubertoraseparatelypretrainedaudioencoder. Givenourfocusonspeech
input,Whisper[Radfordetal.,2023]andQwen2-audio[Chuetal.,2024]alsodemonstrateeffective
performanceforgeneralaudiotasks. Foraudiooutput,selectingaudiotokenswithamulti-codebook
approachbettercapturesaudiodetails. Weexperimentedwithflatteningforaudiotokenmodeling,
butitresultedinexcessivelylongtokens,whicharedetrimentaltostreamingandleadtounstable
learning. Instead,paralleldecoding,inspiredbyMusicGen[Copetetal.,2024],employsadelay
patterncombinedwithtextconditions,asillustratedinFigure2.
Three-StageTraining. Ourtrainingmethodologyisdividedintothreedistinctstages: (1)Modality
Alignment. Thegoalofthisstageistoenhancethetextmodel’sabilitytounderstandandgenerate
speech. ThecoremodelofMini-Omniisentirelyfrozen,withgradientsallowedonlyintwoadapters.
Duringthisstage,weusedatafromspeechrecognitionandspeechsynthesistotrainthemodel’s
5Figure3: Mini-Omni‘three-stagetrainingphases: modalityexpansion,modalityadaptationtraining,
andholisticfine-tuning.
speechrecognitionandsynthesiscapabilities. (2)AdaptionTraining. Oncethenewmodalityis
alignedwiththetextmodel’sinput,theadaptersarefrozen. Inthisstage,wefocussolelyontraining
themodel’stextcapabilitieswhengivenaudioinputs,asaudiooutputissimplysynthesizedfrom
text. Themodelistrainedusingdatafromspeechrecognition,spokenquestionanswering,andtext
responsetasks. (3)Multi-modalFinetuning. Inthefinalstage,theentiremodelisfine-tunedusing
comprehensivedata. Atthispoint,allmodelweightsareunfrozenandtrained. Sincetheprimary
modalityalignmenttasksarehandledduringadaptertraining,theoriginalmodel’scapabilitiesare
maximallypreserved.
ModelInputIds. Giventheeightparalleloutputsequences,theinputalsorequireseightsequences,
leadingtosignificantcomplexity. Therefore,webrieflyoutlinetheorganizationofmodelinputshere.
Themodelcanaccepteithertextoraudioinputs,whichareplacedinthecorrespondingmodality
sequences. Foraudioinputs,theinputtokensandWhisperfeaturesaretransformedintotensorsofthe
samedimensionviaadaptersandthenconcatenated. Dependingonthetask,weplacethe<answer>
specialtokenindifferentpositionstoguidethemodel’soutput,achievingmulti-modaloutput. The
organizationofsometasksisillustratedinFigure4. Beforebeingfedintothemodel,allsequences
aresummedandaveragedtointegratefeatures.
Figure4: DiagramoftheinputsectionofMini-Omniparallelgeneration. The<answer>special
tokenisplacedattheendofthesequencetobegenerated,asdeterminedbythetask.
4 Experiments
hissectionpresentsthefoundationalcapabilitytestresultsforMini-Omni. Wefirstdescribethe
training datasets, data processing methods, and hyperparameters. We then evaluate the model’s
performanceoncoretaskslikespeechrecognitionandprovideseveralusecaseexamples. Wewill
includeallrelevantexperimentsinthenextversionassoonaspossible.
64.1 Datasets
Toestablishfoundationalspeechcapabilities,wetrainedthemodelusingthreespeechrecognition
datasetstotalingapproximately8,000hours,focusingonspeechunderstandingandsynthesis. For
textmodality,weincorporated2milliondatapointsfromtheOpen-Orca[OpenOrca]datasetand
integratedthemwithothermodalitiestopreservetextualaccuracy. Moss’sSFTdataset[Sunetal.,
2024]wasutilizedwithzero-shotTTStosynthesize1.5millionspeechQApairs. Toavoidunsuitable
codeandsymbolicoutputs,wecreatedtheVoiceAssistant-400KdatasetwithGPT-4o. Datasetsare
detailedinTable1. Stage1involvesASRdatafortrainingspeechadapters. Stage2usesTextQAand
AudioQAforaudio/textinputandtextresponsetraining. Stage3focusesonmultimodalinteraction
usingtheaudiomodalityofAudioQA.Finalstagetrainingincludesannealingandfine-tuningwith
VoiceQA.
Task Stages Dataset Modality items
Libritts[Zenetal.,2019] A1|T1 586h
ASR 1,2,3 VCTK[datashare,2024] A1|T1 44h
MultilingualLibriSpeech[Pratapetal.,2020] A1|T1 8000h
TextQA 2,3 Open-Orca[OpenOrca] T1|T2 2000K
AudioQA 3 Moss-002-sft-data[Sunetal.,2024] A1|T1|A2|T2 1500K
Alpaca-GPT4[vicgalle,2024] A1|T1|A2|T2 55k
Identityfinetune[sayan1101,2024] A1|T1|A2|T2 2k
QAassistant[Mihaiii,2024a] A1|T1|A2|T2 27k
voiceQA final Rlhf[Anthropic,2024] A1|T1|A2|T2 367k
Trivia-singlechoice[Mihaiii,2024c] A1|T1|A2|T2 17k
Trivia-Multichoice[Mihaiii,2024b] A1|T1|A2|T2 20k
OpenAssistant[OpenAssistan,2024] A1|T1|A2|T2 2k
Table1:ThedatasetsandtheirusagefortrainingMini-Omniareasfollows:Inthemodalitynotation,
TandArepresentthetextandaudiomodalities,withsubscripts1and2indicatinginputoroutput.
4.2 TrainingParameters
Ourmodelistrainedon8A100GPUs,utilizingacosineannealinglearningrateschedulerwitha
minimumlearningrateof4e-6andamaximumlearningrateof4e-4. Eachtrainingepochconsists
of40,000steps,withbatchsize192foreachstep. ThebaselanguagemodelemploysQwen2-0.5B
[Yangetal.,2024a],atransformerarchitecturewith24blocksandaninternaldimensionof896. The
speechencoderusestheWhisper-smallencoder,withASRadapterconnectedviatwo-layerMLP,
andtheTTSadapterextendstheoriginalmodelbyadding6additionaltransformerblocks. During
fine-tuning,weuselearnratefrom4e-6to5e-5.
4.3 ExperimentalResults
Wefirstevaluatedthemodel’sperformanceonASRtaskstoassessitsspeechunderstandingcapabili-
ties. Basicexperimentsonspeechrecognitioncapabilitieswereconductedusingthefourtestsets
fromLibriSpeech[Panayotovetal.,2015]: test-clean,test-other,dev-clean,anddev-other. Results
arepresentedinTable2,wherewecomparetheaccuracyofouradoptedspeechrecognitionsystems,
wav2vec2[Baevskietal.,2020]andWhisper-small,aswellastheVITA[Fuetal.,2024].Thefindings
indicatethatwhileMini-Omni’sspeechrecognitionperformanceslightlylagsbehindWhisper-small’s
[Radfordetal.,2023]decoder,itstillachievesanexcellentlevelofaudiocomprehension.
Method test-clean test-other dev-clean dev-other
wav2vec2-base[Baevskietal.,2020] 6.0 13.4 - -
VITA[Fuetal.,2024] 8.14 18.41 7.57 16.57
whisper-small[Radfordetal.,2023] 3.4 7.6 - -
Mini-Omni 4.5 9.7 4.6 9.2
Table2: Comparisonofthemodel’sASRwiththebasemodelused.
74.4 CaseStudy
Here,wepresentseveralcasestodemonstrateMini-Omni’scapabilitiesinspeechunderstanding
andreasoning. Theseexamplesrevealthatspeech-basedreasoningissomewhatweakercomparedto
text-basedreasoning,highlightingthenecessityforbatchgeneration. Formoreimpressiveexamples,
pleaserefertohttps://github.com/gpt-omni/mini-omni.
Figure5: RealstreamingoutputexamplesofMini-Omni
5 Conclusion
Inthiswork,weintroduceMini-Omni,thefirstmulti-modalmodelwithdirectspeech-to-speech
capabilities. Buildingonpreviousapproachesthatusetext-guidedspeechgeneration,wepropose
aparalleltextandaudiogenerationmethodthatleveragesminimaladditionaldataandmodulesto
rapidly transfer a language model’s text capabilities to the audio modality, supporting streaming
outputinteractionswithhighmodelanddataefficiency. Weexplorebothtext-instructedstreaming
parallelgenerationandbatchparallelgeneration,whichfurtherenhancethemodel’sreasoningability
andefficiency. Ourapproachsuccessfullyaddresseschallengingreal-timedialoguetasksusinga
modelwithonly0.5billionparameters. WehavedevelopedtheAnyModelCanTalkmethod,based
onapreandpost-adapterdesign,tofacilitaterapidspeechadaptationofothermodelswithminimal
additionaltraining. Additionally,wehavereleasedtheVoiceAssistant-400Kdatasetforfine-tuning
speechoutput,designedtominimizethegenerationofcodesymbolsandassisthumansinavoice
8assistant-likemanner. Allourdata,inference,andtrainingcodeswillbeprogressivelyopen-sourced
athttps://github.com/gpt-omni/mini-omni. Wehopetoprovideguidanceandsupportfor
otherworkfocusedonlanguagemodelspeechinteraction.
References
Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,Karel
Lenc,ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguage
modelforfew-shotlearning. Advancesinneuralinformationprocessingsystems,35:23716–23736,
2022.
Anthropic. https://huggingface.co/datasets/anthropic/hh-rlhf,2024.
AlexeiBaevski,YuhaoZhou,AbdelrahmanMohamed,andMichaelAuli. wav2vec2.0:Aframework
forself-supervisedlearningofspeechrepresentations. Advancesinneuralinformationprocessing
systems,33:12449–12460,2020.
QianChen,YunfeiChu,ZhifuGao,ZeruiLi,KaiHu,XiaohuanZhou,JinXu,ZiyangMa,Wen
Wang,SiqiZheng,etal. Lauragpt: Listen,attend,understand,andregenerateaudiowithgpt. arXiv
preprintarXiv:2310.04673,2023.
SanyuanChen,YuWu,ChengyiWang,ShujieLiu,DanielTompkins,ZhuoChen,andFuruWei.
Beats: Audiopre-trainingwithacoustictokenizers. arXivpreprintarXiv:2212.09058,2022.
YunfeiChu,JinXu,QianYang,HaojieWei,XipinWei,ZhifangGuo,YichongLeng,YuanjunLv,
JinzhengHe,JunyangLin,etal. Qwen2-audiotechnicalreport. arXivpreprintarXiv:2407.10759,
2024.
Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, and
AlexandreDéfossez. Simpleandcontrollablemusicgeneration. AdvancesinNeuralInformation
ProcessingSystems,36,2024.
NilakshDas,SaketDingliwal,SrikanthRonanki,RohitPaturi,DavidHuang,PrashantMathur,Jie
Yuan, Dhanush Bekal, Xing Niu, Sai Muralidhar Jayanthi, et al. Speechverse: A large-scale
generalizableaudiolanguagemodel. arXivpreprintarXiv:2405.08295,2024.
datashare. https://datashare.ed.ac.uk/handle/10283/2651,2024.
ChaoyouFu,HaojiaLin,ZuweiLong,YunhangShen,MengZhao,YifanZhang,XiongWang,DiYin,
LongMa,XiawuZheng,etal. Vita: Towardsopen-sourceinteractiveomnimultimodalllm. arXiv
preprintarXiv:2408.05211,2024.
ShujieHu, LongZhou, ShujieLiu, SanyuanChen, HongkunHao, JingPan, XunyingLiu, Jinyu
Li,SunitSivasankaran,LinquanLiu,etal. Wavllm: Towardsrobustandadaptivespeechlarge
languagemodel. arXivpreprintarXiv:2404.00656,2024.
Ziyue Jiang, Yi Ren, Zhenhui Ye, Jinglin Liu, Chen Zhang, Qian Yang, Shengpeng Ji, Rongjie
Huang,ChunfengWang,XiangYin,etal. Mega-tts:Zero-shottext-to-speechatscalewithintrinsic
inductivebias. arXivpreprintarXiv:2306.03509,2023.
EugeneKharitonov,DamienVincent,ZalánBorsos,RaphaëlMarinier,SertanGirgin,OlivierPietquin,
Matt Sharifi, Marco Tagliasacchi, and Neil Zeghidour. Speak, read and prompt: High-fidelity
text-to-speech with minimal supervision. Transactions of the Association for Computational
Linguistics,11:1703–1718,2023.
MatthewLe,ApoorvVyas,BowenShi,BrianKarrer,LedaSari,RashelMoritz,MaryWilliamson,
VimalManohar,YossiAdi,JayMahadeokar,etal. Voicebox: Text-guidedmultilingualuniversal
speechgenerationatscale. Advancesinneuralinformationprocessingsystems,36,2024.
JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip: Bootstrappinglanguage-imagepre-
trainingforunifiedvision-languageunderstandingandgeneration. InInternationalconferenceon
machinelearning,pages12888–12900.PMLR,2022.
9HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pages26296–26306,2024.
meta. llama3.1,2024. URLhttps://llama.meta.com/.
Mihaiii. https://huggingface.co/datasets/mihaiii/qa-assistant-2,2024a.
Mihaiii. https://huggingface.co/datasets/mihaiii/triviamultichoice,2024b.
Mihaiii. https://huggingface.co/datasets/mihaiii/triviasinglechoice,2024c.
mixtral. https://mistral.ai/,2024.
EliyaNachmani,AlonLevkovitch,RoyHirsch,JulianSalazar,ChulayuthAsawaroengchai,Soroosh
Mariooryad,EhudRivlin,RJSkerry-Ryan,andMichelleTadmorRamanovich.Spokenquestionan-
sweringandspeechcontinuationusingspectrogram-poweredllm.arXivpreprintarXiv:2305.15255,
2023.
openai. https://openai.com/,2024.
OpenAssistan. https://huggingface.co/datasets/openassistant/oasst1,2024.
OpenOrca. https://huggingface.co/datasets/open-orca/openorca/.
VassilPanayotov,GuoguoChen,DanielPovey,andSanjeevKhudanpur. Librispeech: anasrcorpus
basedonpublicdomainaudiobooks. In2015IEEEinternationalconferenceonacoustics,speech
andsignalprocessing(ICASSP),pages5206–5210.IEEE,2015.
Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. Mls: A
large-scalemultilingualdatasetforspeechresearch. arXivpreprintarXiv:2012.03411,2020.
AlecRadford,JongWookKim,TaoXu,GregBrockman,ChristineMcLeavey,andIlyaSutskever.
Robust speech recognition via large-scale weak supervision. In International conference on
machinelearning,pages28492–28518.PMLR,2023.
sayan1101. https://huggingface.co/datasets/sayan1101/identity-finetune-data,2024.
HubertSiuzdak. https://github.com/hubertsiuzdak/snac/,2024.
TianxiangSun,XiaotianZhang,ZhengfuHe,PengLi,QinyuanCheng,XiangyangLiu,HangYan,
YunfanShao,QiongTang,ShiduoZhang,etal. Moss: Anopenconversationallargelanguage
model. MachineIntelligenceResearch,pages1–18,2024.
ChangliTang,WenyiYu,GuangzhiSun,XianzhaoChen,TianTan,WeiLi,LuLu,ZejunMa,and
ChaoZhang.Salmonn:Towardsgenerichearingabilitiesforlargelanguagemodels.arXivpreprint
arXiv:2310.13289,2023.
vicgalle. https://huggingface.co/datasets/vicgalle/alpaca-gpt4,2024.
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,
Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint
arXiv:2407.10671,2024a.
DongchaoYang,SongxiangLiu,RongjieHuang,ChaoWeng,andHelenMeng.Instructtts:Modelling
expressivettsindiscretelatentspacewithnaturallanguagestyleprompt. IEEE/ACMTransactions
onAudio,Speech,andLanguageProcessing,2024b.
HeigaZen,VietDang,RobClark,YuZhang,RonJWeiss,YeJia,ZhifengChen,andYonghuiWu.
Libritts: Acorpusderivedfromlibrispeechfortext-to-speech. arXivpreprintarXiv:1904.02882,
2019.
Dong Zhang, Shimin Li, Xin Zhang, Jun Zhan, Pengyu Wang, Yaqian Zhou, and Xipeng Qiu.
Speechgpt: Empoweringlargelanguagemodelswithintrinsiccross-modalconversationalabilities.
arXivpreprintarXiv:2305.11000,2023a.
HangZhang,XinLi,andLidongBing. Video-llama: Aninstruction-tunedaudio-visuallanguage
modelforvideounderstanding. arXivpreprintarXiv:2306.02858,2023b.
10