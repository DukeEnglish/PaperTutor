Dissecting Out-of-Distribution Detection and Open-Set
Recognition: A Critical Analysis of Methods and Benchmarks
Hongjun Wang1, Sagar Vaze2, Kai Han1*
1The University of Hong Kong.
2University of Oxford.
*Corresponding author(s). E-mail(s): kaihanx@hku.hk;
Contributing authors: hjwang@connect.hku.hk; sagar@robots.ox.ac.uk;
Abstract
Detecting test-time distribution shift has emerged as a key capability for safely deployed machine
learningmodels,withthequestionbeingtackledundervariousguisesinrecentyears.Inthispaper,
we aim to provide a consolidated view of the two largest sub-fields within the community: out-of-
distribution (OOD) detection and open-set recognition (OSR). In particular, we aim to provide
rigorous empirical analysis of different methods across settings and provide actionable takeaways
for practitioners and researchers. Concretely, we make the following contributions: (i) We perform
rigorouscross-evaluationbetweenstate-of-the-artmethodsintheOODdetectionandOSRsettings
andidentifyastrongcorrelationbetweentheperformancesofmethodsforthem;(ii)Weproposea
new,large-scalebenchmarksettingwhichwesuggestbetterdisentanglestheproblemtackledbyOOD
detectionandOSR,re-evaluatingstate-of-the-artOODdetectionandOSRmethodsinthissetting;
(iii)Wesurprisinglyfindthatthebestperformingmethodonstandardbenchmarks(OutlierExposure)
struggleswhentestedatscale,whilescoringruleswhicharesensitivetothedeepfeaturemagnitude
consistentlyshowpromise;and(iv)Weconductempiricalanalysistoexplainthesephenomenaand
highlightdirectionsforfutureresearch.Code:https://github.com/Visual-AI/Dissect-OOD-OSR
Keywords:Out-of-DistributionDetection,Open-setRecognition
1 Introduction
images which come from different datasets to the
training set, while OSR methods are evaluated on
Any practical machine learning model is likely to the ability to detect test images which come from
encounter test-time samples which differ substan- different semantic categories to the training set.
tially from its training set; i.e., models are likely Researcheffortsinbothofthesefieldslargelyoccur
to encounter test-time distribution shift. As such, independently,withlittlecross-pollinationofideas.
detecting distribution shift has emerged as a key Thoughmanypriorworkshaverecognizedthesim-
research problem in the community [1–3]. Specif- ilarity of the two sub-fields [8–11], there has been
ically, out-of-distribution (OOD) detection [4, 5] little benchmarking to understand the underlying
andopen-setrecognition (OSR)[6,7]haveemerged similarities and differences between them.
as two rich sub-fields to tackle this task. In fact, In this study, we investigate the detection of
both tasks explicitly tackle the setting in which distribution shifts, with a focus on exploring and
multi-way classifiers must detect if test samples analyzing OOD detection and OSR methods and
are‘unseen’withrespecttotheirtrainingset,with benchmarks. Our aim is to gain a comprehen-
a variety of methods and benchmarks proposed sive understanding of the underlying similarities
within each field. OOD detection methods test on
1
4202
guA
03
]VC.sc[
2v75761.8042:viXraand differences between these two tasks. We per-
form rigorous cross-evaluation between methods
developed for OOD detection and OSR on cur-
rent standard benchmarks, finding that methods
which perform well for one are likely to perform
wellfortheother(Section3).Weexperimentboth
with methods which require specialized training
strategies (e.g., Outlier Exposure [4] (OE) and
ARPL [7]) as well as different post-hoc scoring
rules (e.g., MSP [2], MLS [8] and Energy [3]). We
thoroughly evaluate all methods on both standard
OOD detection and OSR benchmarks, after which
we find that OE achieves almost saturating perfor-
Cat Dog Plane Bird Apple
manceontheOODdetectiontaskandalsoobtains
state-of-the-art results on the OSR task. We fur- Semantic Shift
Fig. 1: Semantic shift vs. covariate shift. We
ther find that the scoring rules which are sensitive
systematically perform cross-evaluation between
to the magnitude of the deep image embeddings
SOTA methods for OSR and OOD detection and
(like MLS [8] and Energy Scoring [3]) show the
propose a large-scale benchmark setting in which
best performance across tasks and datasets.
we disentangle the tasks tackled in the two fields,
Next, we propose a reconciling perspective on
proposing that they tackle semantic shift (x-axis)
the tasks tackled by the two fields, and propose a
and covariate shift (y-axis) respectively.
new benchmark to assess this (Section 4). Specif-
ically, we propose a new, large-scale benchmark
setting, in which we disentangle different distribu- scale to larger benchmarks, while the magnitude-
tion shifts, namely semantic shift and covariate aware scoring rules, especially MLS [8], still show
shift, that occur in OOD detection and OSR promise. We further provide empirical insights by
(see Figure 1). Though these concepts have been analysing the representations extracted by differ-
discussed before [12, 13], the standard large-scale ent models under different distribution shifts. Our
benchmarksinOODdetectionhavenotadequately analysis suggest that the strong performance of
separated them. For example, semantic shift and OE on existing benchmarks is largely attributed
covariateshiftsimultaneouslyoccurwhendetecting to the fact that the auxiliary OOD data used for
OOD samples from Places using pre-trained Ima- training has a distribution overlap with the OOD
geNetmodels.Weproposeaconceptualframework testing data (as measured by distances in feature
tounderstandthemandfurtherproposelarge-scale space).Meanwhile,wefindthatitisnotstraightfor-
evaluation settings, including for pre-trained Ima- wardtofindauxiliaryOODdatawhichreflectsthe
geNet models. For example, to isolate semantic range of possible distribution shifts with respect
shift on ImageNet, we leverage the recently intro- to large-scale datasets. We believe there are still
duced Semantic Shift Benchmark (SSB) [8], in manyopenquestionstobeansweredintheshared
whichtheoriginalImageNet-1K[14]isregardedas space of OOD detection and OSR, and hope the
‘seen’closed-setdatawhile‘unseen’dataiscarefully findings in our work can serve as a platform for
drawn from the disjoint set of ImageNet-21K- future investigation.
P[15].Forcovariateshift,weleverageImageNet-C
[16] and ImageNet-R [17] to demonstrate distribu- 2 Related work
tion shift with respect to the standard ImageNet
dataset. Furthermore, to account for the tension
Open-set recognition. Previous work [20] coins
betweenbeingrobust tocovariateshift(alsoknown
‘open-set recognition’, the objective of which is
as OOD-generalisation [18, 19]) and being able to
to identify unknown classes while classifying the
detect the presence of it, we further introduce a
known ones. OpenMax resorts to Activation Vec-
new metric ‘Outlier-Aware Accuracy’ (OAA).
tor(AV)andmodelsthedistributionofAVsbased
Finally, we examine SoTA OOD detection and
on the Extreme Value Theorem (EVT). Recent
OSR methods on our large-scale benchmark to
works [21–23] show that the generated data from
validate whether the findings on the standard
synthetic distribution would be helpful to improve
(small-scale)datasetsstillholdonourconsolidated
OSR. OSRCI [22] generates images belonging to
large-scale evaluation. Through large-scale anal-
the unknown classes but similar to the training
ysis, we surprisingly find that OE struggles to
data to train an open-set classifier. [23] adver-
sarially trains discriminator to distinguish closed
2
tfihS
etairavoC
hpargofnI
hctekS
gnitniaP
laeRfrom open-set images and introduces real open-set suited for defining semantic shift. As such, we
samplesformodelselection.Prototype-basedmeth- aim to build a large-scale benchmark with a clear
ods [6, 7] (i.e., ARPL / ARPL+CS) adjust the underlying taxonomy. [12] introduces ImageNet-A
boundariesofdifferentclassesandidentifyopen-set (i.e., collections of natural adversarial examples)
images based on distances to the learned proto- and ImageNet-O (i.e., samples of held-out classes
types of known classes. MLS [8] uses maximum fromImageNet-21K)forrobustnessevaluationand
logitscoresratherthansoftmaxscorestomaintain unseen classes recognition, while [29] curates a
the magnitude information. set of artificial datasets to disentangle the eval-
Out of Distribution Detection.Thegoalof uation of non-semantic distributional shift and
OODdetectionisgenerallyspecifiedasidentifying semantic-shift.However,theyfocusmoreonachiev-
test-time samples coming from a ‘different distri- ing robustness to non-semantic distributional shift
bution’ from the training data. [2] formalizes the anddonotdevelopcross-evaluationbetweenstate-
task of out-of-distribution detection and provides of-the-art methods in the OOD detection and
a paradigm to evaluate deep learning out-of- OSR settings. [30] treats both semantic and non-
distributiondetectorsusingthemaximumsoftmax semantic tasks in an anomaly detection (AD)
probability(MSP).AtestsamplewithalargeMSP paradigmandappliespopularADmethodstothem
score is detected as an in-distribution (ID) exam- onCIFAR10.Ourworkexplicitlyexplorestherela-
pleratherthanout-of-distribution(OOD)example. tion between OSR and OOD detection tasks, and
ODIN [24] and its learnable variant G-ODIN [25] verify the effectiveness of respective popular meth-
addadversarialperturbationstobothIDandOOD ods in each field. Two surveys [10, 11] summarize
samples and employ temperature scaling strat- anumberofapproacheswithintheOODdetection
egy on the softmax output to separate them. [3] and OSR settings, along with anomaly detection
proposes the energy score derived from the logit and Novelty Detection. [32] constructs a uni-
outputs for OOD uncertainty estimation. [5] recti- fied benchmark to verify existing OOD detection
fies the distribution of per-unit activations in the methods, delineating ‘far-OOD’ and ‘near-OOD’.
penultimate layer for ID and OOD data. Grad- Meanwhile,[33]rethinkstheimportanceofIDmis-
Norm[26]calculatesgradientsbybackpropagating classifications in the OOD context and examines
theKLdivergencebetweenthesoftmaxoutputand different approaches on selective classification in
a uniform distribution, assuming that the magni- the presence of OOD datasets. In our work, we
tude of gradients is higher for ID data than that not only discuss the link between robustness and
forOODdata.ASH[27]removesalargeportionof OOD detection, but also propose a new metric
the activations based on the pth-percentile of the to reconcile the tasks. Concurrent work [34] pro-
entirerepresentationatalatelayer.Theremaining videsacodebaseforrepresentativemethodswithin
activationsareutilizedtocalculateanenergyscore OSR and OOD detection. In this paper, we cat-
forOODdetection.SHE[28]quantifiesthedissim- egorize shift detection methods into two types:
ilarity between the ID training samples from each scoringrules(e.g.,MSP,MLS,etc),whichoperate
category and the testing samples based on the fea- post-hoc on pre-trained networks, and specialized
tures extracted from the penultimate layer of the training, which modifies the networks’ optimiza-
model. This dissimilarity is then used as the score tionprocedures(e.g.,ARPL/ARPL+CS,OE,etc).
tojudgewhetheratestingsampleisOODornot.
OutlierExposure(OE)[4]andGradNorm[26]both AuxiliarydatainOODdetection.Inspired
design a loss based on the KL divergence between by OE [4], recent work [37–40] leverage auxiliary
the softmax output and a uniform probability dis- data in some form to enhance the model’s abil-
tributiontoencouragemodelstooutputauniform ity to detect OOD data. This could be through
softmax distribution on outliers. The former lever- posterior sampling [37], adversarial training [38],
ages real OOD data for training while the latter augmenting distributions [39] or model perturba-
directly employs the vector norm of gradients to tion[40].POEM[37]focusesonposteriorsampling
perform uncertainty estimation. tolearnadecisionboundarybetweenIDandOOD
Relations between OOD detection and data. ATOM [38] introduces an adversarial train-
OSR. Prior works discuss the separation between ingmethodwithinformativeoutliermining,which
covariate and semantic distributional shift [13, 29– is specifically designed to improve the robustness
31]. [13] discusses separately detecting covariate against adversarial attacks, in which the adversar-
and concept distributional shift on small-scale ial data is considered as a special type of OOD
datasets (i.e., CIFAR-10/100). However, similarly data. DAL [39] addresses the distribution discrep-
to [8], we suggest that small-scale datasets with ancybetweenauxiliaryandunseenrealOODdata,
no explicit taxonomies (like CIFAR) are not well by training predictors over the worst OOD data
3Table 1: Summary of representative OOD detection and OSR techniques. The works are categorized
based on the task that they are developed for (i.e., OOD detection and OSR) and the methodology
employed (i.e., scoring rules and training strategies).
Developed for
Scoring rule Training strategy
OOD or OSR
MSP [2] OOD MSP CE
MLS [8] OSR MLS CE
ODIN [35] OOD MSP CE
GODIN [25] OOD MSP CE
GradNorm [26] OOD MSP CE
SEM [36] OOD MSP CE
Energy [3] OOD Energy CE
ReAct [5] OOD MSP/Energy/ODIN CE
ASH [27] OOD MSP/Energy/ODIN/ReAct CE
SHE [28] OOD MSP/Energy/ODIN CE
ARPL+CS [7] OSR MSP ARPL+CS
OE [4] OOD MSP OE
in a Wasserstein ball. DOE [40] leverages implicit benchmarks.Giventhestronginherentconnections
data transformation through the embedding fea- between OOD detection and OSR, a comprehen-
tures’ perturbation to minimize a distribution sive cross-benchmarking comparison is crucial to
discrepancymeasurementcalledworstOODregret, shedlightonthefuturedevelopmentofthebroader
aiming to enhance the model’s robustness to dis- distribution shift detection problem.
tribution shifts. Our work offers unique insights As a starting point to reconcile OOD detec-
into the selection of auxiliary data to optimize tion and OSR, in this section we perform cross-
OOD detection performance. By uncovering the evaluation of methods from both sub-fields.
relationship between the auxiliary data and the
model’sOODdetectionperformance,ourworkhas 3.1 Experimental setup
thepotentialtoinformstrategiesforauxiliarydata
selection and manipulation toward more reliable Problem setting. Let X D denote an
∈ R
OOD detection solutions. input sample and C denote the label of
∈ R
Key similarities and distinctions with interest. Test-time distribution shift occurs when
prior work. While several papers [30, 34] have the testing joint distribution is not equal to the
jointly considered methods in the OOD detection training joint distribution, i.e., P test(X,C) =
̸
and OSR tasks, few works have clearly distin- P train(X,C). This shift can be further divided
guished the academic and practical differences (or into two types: covariate shift and semantic
similarities) between them. In this work, we not shift. Covariate shift occurs when P test(C X) =
|
only provide empirical analysis but also propose a P train(C X) but P test(X) = P train(X). Seman-
| ̸
conceptual framework and large-scale benchmark tic shift occurs when P test(C X) = P train(C X)
| ̸ |
to better reconcile these problems. but P test(X)=P train(X). In OOD detection and
OSR for multi-class classification, the label space
containsmultiplesemanticcategories c , ,c ,
3 Cross-benchmarking of OOD 1 L
{ ··· }
where L is the total number of categories in the
detection and OSR methods
testing data. The model needs to identify the dis-
tribution from which test-time samples originate
Despite the growing popularity of OOD detec- and conduct classification based on the posterior
tion and OSR studies, these two tasks have probability, represented as p(C =c X).
i
largely evolved independently and in isolation Methods. We distinguish two | categories of
from each other, as shown in Table 1. Indeed, shiftdetectionmethods:scoring rules (whichoper-
methods designed for OSR can be seamlessly ate post-hoc on top of pre-trained networks); and
adopted to address the OOD detection problem, specializedtraining (whichchangetheoptimization
and vice versa. Recent generalized OOD detection procedure of the networks).
frameworks [30, 34] unify tasks relevant to OOD For scoring rules, we compare the maximum
detection and OSR. However, there is still a lack softmaxprobability(MSP)[2],theMaximumLogit
of cross-evaluation between methods developed Score (MLS) [8], ODIN [24], GODIN [25], Energy
for OOD detection and OSR on current standard scoring [3], GradNorm [26] and SEM [36]. We
4further experiment with ReAct [5], an activation epochsis200andthebatchsizeis128.ForCIFAR-
pruning technique which can be employed in con- 100, we also set the initial learning rate to 0.1,
junction with any scoring rule. While MLS was which is then divided by 5 at 60th, 120th, 160th
developedforOSR[8],theotherscoringruleswere epochs. The model is trained for 200 epochs with
developed for OOD detection. For now, we note a batch size of 128, a weight decay of 5e 4, and
−
that MLS, Energy and GradNorm are all sensi- Nesterov momentum of 0.9, following [49]. Addi-
tive to the magnitude of the feature norm of the tional results using other network architectures
network, while the others are not. We refer to the and training setups can be found in Section D in
former scoring rules as ‘magnitude aware’. Appendix.
For specialized training, we first experiment Metrics. Following standard practise in both
withthestandardcross-entropy(CE)loss.Wealso OOD and OSR tasks, we use the Area Under the
use ARPL + CS [7] from the OSR literature. This ReceiverOperatingcharacteristicCurve(AUROC)
methodlearnsasetof‘reciprocalpoints’whichare as an evaluation metric throughout this paper as
trained to be far away from all training category wefindthatothermetricswerecorrelatedstrongly
embeddings.Wenotethatthereciprocalpointscan with the AUROC. For results on other metrics,
be treated as a linear classification layer, allowing please refer to the supplementary (Section S5).
ustouseanyofthescoringrulesmentionedabove
on top of this representation. Finally, we train 3.2 Quantitative results
models with Outlier Exposure (OE) [4] from the
OODdetectionliterature,whererealoutlierexam- InTable2,webenchmarkOODdetectionandOSR
ples are used during training as examples of OOD tasks across nine common datasets, with different
detection. In this case, the model is encouraged to training strategies and scoring rules. The results
predict a uniform softmax output. areaveragedfromfiveindependentruns.Although
Datasets. For the OOD detection setting, there is not always one clear winner regarding
we treat CIFAR10 [41] as in-distribution data methodology, we have three main observations.
and train models on it. As OOD data, we use Firstly, MLS [8] and Energy [3] tend to
six common datasets: SVHN [42], Textures [43], performbestacrossOODandOSRdatasets.
LSUN-Crop[44],LSUN-Resize[44],iSUN[45]and Wehypothesizethatthisisbecausebotharesensi-
Places365[46],allofwhichhavemutuallyexclusive tive to the magnitude of the feature vector before
classesonCIFAR10.Inthesupplementary,wealso the networks’ classification layer. To verify our
provide experiments using CIFAR-100 as the ID conjecture, we investigate the magnitude of fea-
training data (see Tables S1-S3 in Section S2) and tures by projecting the features of both ID and
using different training configurations (see Tables OOD/open-set samples into a two-dimensional
S4-S7 in Section S3). The supplementary experi- space in Figure 2. We experiment on generic and
mentsyieldconsistentresults,reinforcingthemain fine-graineddatasets,namely,CIFAR-10andCUB.
findings to be discussed in this section. Thisprojectionisachievedbytrainingalinearlayer
For the OSR benchmark, following the stan- with an output dimension of two, after the penul-
dard protocols in [47], we set up four sub-tasks timate layer of the model. The feature magnitude
containing CIFAR10, CIFAR+10, CIFAR+50 and of ID data is larger than that of open-set/OOD
TinyImageNet[48].Inallcases,modelsaretrained data.Thisisconsistentwiththefindingin[8]that
on a subset of categories with remaining used ‘unfamiliar’ examples tend to have lower feature
as ‘unseen’ at test time. The CIFAR+N settings magnitude than ID samples, providing a strong
involve training on four classes from CIFAR10 signal for distribution shift detection.
and evaluating on N classes from CIFAR-100. Secondly, we observe that Outlier Expo-
Note that, for a given method, benchmarking on sure [4] provides excellent performance
OOD detection involves training a single model on the OOD detection benchmarks, often
and evaluating on multiple downstream datasets. nearly saturating performance. More results
In contrast, OSR benchmarks involve training a can be found in Section A in Appendix. It also
different model for each evaluation. often boosts OSR performance, though to a lesser
Training configurations. We train the degree, a phenomenon which we explore next in
ResNet18 from scratch on all benchmarks. For Section 4.
CIFAR10,wealwayssettheinitiallearningrateto Thirdly, for small-scale datasets, OOD
0.1 and apply the cosine annealing schedule, using detection accuracy is positively related to
SGDwiththemomentumof0.9.Theweightdecay ID accuracy, while an inverse correlation is
factor is set to 5e 4. The number of total training observed for large-scale datasets. In Figure 3,
−
we further include the results using the recent
5Table 2: Evaluation on small-scale OOD detection and OSR benchmarks with various methods, using
CIFAR10 as ID. The results are averaged from five independent runs. We report the in-distribution
accuracy as ‘ID’ and denote intractable results as ‘-’, resulting from unaffordable computational cost.
Bold values represent the best results, while underlined values represent the second best results. Different
methods have their optimal scope but MLS and Energy demonstrate their stability and models trained
with OE dominate on almost all OOD datasets.
(a)EvaluationbasedonResNet-18trainedwiththeCEloss.
T Mr ea ti hn oin dg ScoringRule OODdetectionbenchmarks
AVG CIFAR10
CIFAR+10OSR CIb Fe An Rch +m 5a 0rks
TinyImageNet
Overall
SVHN Textures LSUN LSUN-R iSUN Places365 ID=95.45 ID=97.13 ID=96.6 ID=96.8 ID=83.4 AVG
MSP 93.65 91.35 95.49 94.88 94.33 90.77 93.41 91.78 93.81 90.20 79.82 88.90 91.61
MLS 94.49 91.54 96.94 96.13 95.52 91.64 94.38 92.54 95.62 91.81 81.31 90.32 92.53
ODIN 92.23 83.76 94.96 96.16 95.31 90.88 90.88 89.77 81.37 80.22 80.96 83.08 88.56
GODIN 97.60 96.21 99.59 97.81 97.74 94.33 97.21 90.22 91.17 87.38 76.05 86.21 92.21
SEM 75.65 72.02 75.18 70.93 72.52 76.14 73.74 40.21 43.87 42.70 - 42.26 61.15
Energy 94.64 91.64 97.14 96.29 95.68 91.78 94.53 92.52 95.68 91.86 81.28 90.34 92.62
MLS+ReAct 92.56 89.97 95.39 95.78 95.17 90.69 93.26 92.57 94.92 90.88 81.65 90.01 91.78
ODIN+ReAct 91.29 83.50 94.70 96.05 95.19 82.55 90.55 86.65 87.76 88.40 81.30 86.03 88.74
Energy+ReAct 92.68 90.05 95.67 96.03 95.42 90.89 93.46 92.58 95.02 90.99 81.67 90.07 91.92
MLS+ASH 95.50 88.87 90.06 92.84 85.84 82.44 89.26 89.19 90.15 82.11 78.76 85.05 87.58
MLS+SHE 86.30 76.40 84.72 81.12 80.56 81.39 81.75 79.19 74.35 78.02 78.78 77.59 80.09
(b)EvaluationbasedonResNet-18trainedwiththeARPL+CSloss.
T Mr ea ti hn oin dg ScoringRule OODdetectionbenchmarks
AVG CIFAR10
CIFAR+10OSR CIb Fe An Rch +m 5a 0rks
TinyImageNet
Overall
SVHN Textures LSUN LSUN-R iSUN Places365 ID=91.02 ID=96.96 ID=96.77 ID=96.69 ID=86.91 AVG
MSP 93.41 91.64 94.29 94.02 94.28 90.77 93.07 92.53 95.71 94.03 82.80 91.27 92.41
MLS 96.36 90.20 96.59 96.95 96.88 93.29 95.05 93.16 96.58 94.67 84.79 92.30 93.95
ODIN 75.92 71.64 86.25 95.14 95.19 75.97 83.35 58.04 74.80 71.52 63.13 66.87 76.76
GODIN 95.78 89.61 95.41 96.88 96.17 92.59 94.41 91.99 95.73 93.76 81.25 90.68 92.92
SEM 76.42 74.26 84.45 76.08 77.73 71.23 76.70 35.01 38.27 44.15 - 39.14 64.18
Energy 96.52 90.11 96.76 97.16 97.07 93.45 95.18 93.22 96.74 94.82 82.10 91.72 93.80
MLS+ReAct 95.87 92.37 96.37 96.34 96.30 92.97 95.04 92.70 96.42 94.53 82.05 91.43 93.59
ODIN+ReAct 71.87 73.36 83.19 92.34 92.36 69.10 80.37 55.71 62.88 61.85 54.29 58.68 71.70
Energy+ReAct 96.06 92.35 96.59 96.58 96.53 93.17 95.21 92.80 96.61 94.70 82.14 91.56 93.75
MLS+ASH 94.85 91.57 91.14 96.43 88.35 89.11 91.91 91.89 93.26 91.81 79.15 89.03 90.76
MLS+SHE 83.20 81.54 84.36 88.18 83.50 82.13 83.82 77.10 74.25 74.85 75.44 75.41 80.46
(c)EvaluationbasedonResNet-18trainedwiththeOEloss.
T Mr ea ti hn oin dg ScoringRule OODdetectionbenchmarks
AVG CIFAR10
CIFAR+10OSR CIb Fe An Rch +m 5a 0rks
TinyImageNet
Overall
SVHN Textures LSUN LSUN-R iSUN Places365 ID=94.16 ID=97.8 ID=98.3 ID=97.92 ID=83.4 AVG
MSP 99.21 98.81 99.02 98.52 98.55 97.29 98.57 96.29 99.29 98.70 78.67 93.24 96.44
MLS 99.21 98.82 99.02 98.53 98.57 97.32 98.58 96.28 99.32 98.72 80.19 93.63 96.60
ODIN 99.43 98.73 99.14 98.78 98.75 96.41 98.54 96.29 95.27 94.30 79.97 91.46 95.71
GODIN 97.25 95.17 89.05 83.42 84.63 89.51 89.84 93.64 92.01 91.63 78.21 88.87 89.45
SEM 98.13 97.04 98.77 97.01 97.16 94.86 97.16 30.19 33.73 33.91 - 32.61 73.69
Energy 99.20 98.78 99.02 98.55 98.58 97.31 98.57 93.12 99.33 98.74 80.16 92.84 96.28
GradNorm 99.95 99.71 99.83 99.46 99.42 97.93 99.38 96.57 99.26 98.51 60.56 88.73 95.12
MLS+ReAct 95.18 92.22 79.46 83.34 83.68 87.46 86.89 95.43 98.73 97.93 79.92 93.00 89.34
ODIN+ReAct 84.16 82.92 64.00 73.90 75.45 71.65 75.35 87.52 87.78 85.62 79.47 85.10 79.25
Energy+ReAct 94.41 91.36 73.88 80.03 81.16 86.19 84.51 95.43 98.74 78.67 79.84 88.17 85.97
MLS+ASH 99.10 98.55 98.88 98.52 98.53 97.31 98.49 95.40 89.21 91.54 75.30 87.86 94.24
MLS+SHE 97.79 93.68 94.16 89.34 88.77 90.26 92.33 82.01 76.45 87.05 70.20 78.93 86.97
vision-language model, CLIP [50], for reference, but it is still inferior to magnitude-aware meth-
which are not included for fitting the lines. We ods’performanceinTable2(a).Itappearsthatthe
experiment with three variants, namely, zero-shot Mixupmechanism,whichdistributesconfidenceto
(zs), finetuning (ft), and linear probing (lp). We both involved categories, decreases the maximum
find that only the linear probing CLIP falls into magnitude value. Therefore, we highlight that the
the correlation fitted for other methods, while the training setup (e.g., Mixup) that neglects mag-
zero-shot and finetuning counterparts are not well nitude information occurs poorer performance in
aligned with the trend. both OOD detection and OSR tasks. (2) Sensitiv-
Additional analysis. (1) Mixup hurts ity of methods to hyper-parameters. In Figure 4,
magnitude-awaremethods’performance.Toinvesti- we present the results of different scoring rules
gatetheimpactofdifferenttrainingsetups,wealso averaged across different training methods. For
adopt Mixup [51] to our training procedure. As methodsthataresensitivetotheselectionofhyper-
showninTable3,magnitude-awaretechniques(i.e., parameters (e.g., the perturbation magnitude of
MLS and Energy) demonstrate stability. Interest- ODIN,thresholdsofReAct),weaveragetheresults
ingly, we also find that the MSP method achieves among five different selections (instead of using
the best performance compared to other methods, five different random seeds like others). Therefore,
in Figure 4, the result for each scoring rule is the
6
EC
SC+LPRA
EO30 I OD O d Da t da a: tC a:IF TA eR xt1 u0 r es 30 I OD p d ea nt -a s: e C t dIF aA taR :1 C0 IFAR10
20 20
10 10
0
0
-10
-10
Acc: 97.13 Acc: 95.45
-20 AUROC: 92.54 AUROC: 94.38
-20
-20 -10 0 10 20 30 -20 -10 0 10 20
30 ID data: CUB 25 ID data: CUB
OOD data: Waterbird Open-set data: CUB
20
20
15
10
10 Acc: 90.38
0 AUROC: 88.29
5
-10
0
Acc: 82.54
-20 AUROC: 81.87 -5
-30 -10
-20 -10 0 10 20 30 -15 -10 -5 0 5 10 15 20 25
ID data OOD / Open-set data
Fig. 2: Visualization of feature projections for images from ID and open-set / OOD datasets. We project
the features into a two-dimensional space using an additional linear layer with an output dimension of
two after the penultimate layer. We conduct OOD detection and OSR experiments using ResNet-18 on
CIFAR-10 (first row) and ResNet-50 on CUB (second row) datasets. For CIFAR-10, the OOD experiment
uses the full CIFAR-10 dataset as ID data and Textures as OOD data, while the OSR experiment utilizes
the first six classes in CIFAR-10 as ID data and the remaining four as open-set data. For CUB, the OOD
experiment employs the full CUB dataset as ID and Waterbird as OOD data, while the OSR experiment
uses six classes in CUB as ID data and four CUB classes as open-set data. These classes are randomly
selected from the ID and open-set splits introduced in SSB. Notably, these visualizations reveal that the
feature magnitudes of ID data exceed those of OOD or OSR data.
93 CLIP (lp) 75 scoring rules (i.e., MLS and Energy) offer obvi-
92 ous advantages for evaluating model performance.
70 CLIP (lp) Considering the error bars of MLS and Energy,
91
65 MLSistheoptimalchoiceforallthescenarios.Fur-
90 CE CE
89 OA OR E EP _ _3 YL 0 F0 Ck C 15M CLIP (ft) 60 OA OR E EP _ _P YL l Fa Cce Cs 1 5M CLIP (ft) thermore, we observe that ODIN and ReAct, two
techniques that are not magnitude-aware, exhibit
88 CLIP (zs) 55 CLIP (zs)
91 92 93 94 95 96 66 68 70 72 74 76 78 80 instability.Thisinstabilitycanbeattributedtothe
Closed Set Performance (Accuracy) Closed Set Performance (Accuracy)
Fig. 3: OSR performance vs. OOD detection per- relianceonacarefullytunednoisevalueforstochas-
formance of different training methods averaged tic predictions for ODIN and threshold value to
across various scoring rules. CLIP variants are truncate activation for ReAct. We can also find
included here for reference and are not used to fit from Table 2 that ReAct, which has been shown
the correlation. to be effective in the literature, does not seem
to bring performance gain in well-trained models
with a high in-distribution accuracy. Here, we fol-
average of 15 independent runs (i.e., 3 training
low the techniques from [8] to obtain the highest
methods 5runs).Wefindthatmagnitude-aware
× ID accuracy possible. It appears that when the
classifier is strong enough, it is difficult for ReAct
7
)CORUA(
ecnamrofreP
DOO
)CORUA(
ecnamrofreP
DOOTable 3: Evaluation on small-scale OOD detection and OSR benchmarks with various scoring rules on
ResNet-18, using CIFAR10 as ID training data. The results are averaged from five independent runs. We
adopt Mixup [51] to the training procedure and report the in-distribution accuracy as ‘ID’. Bold values
represent the best results, while underlined values represent the second best results. Magnitude-aware
techniques (i.e., MLS and Energy) demonstrate stability.
T Mr ea ti hn oin dg ScoringRule OODdetectionbenchmarks
AVG CIFAR10
CIFAR+10OSR CIb Fe An Rch +m 5a 0rks
TinyImageNet
Overall
SVHN Textures LSUN LSUN-R iSUN Places365 ID=95.81 ID=97.40 ID=96.82 ID=96.91 ID=83.75 AVG
MSP 87.63 81.48 95.01 88.22 88.58 86.02 87.82 92.01 93.96 90.44 80.02 89.11 88.34
MLS 86.83 79.62 94.95 88.05 87.99 81.99 86.57 92.88 95.99 92.16 81.55 90.65 88.17
ODIN 89.43 75.56 87.31 55.94 61.36 83.63 75.54 85.27 79.24 83.51 78.81 81.71 78.01
GODIN 94.28 89.71 91.52 88.50 87.45 81.28 88.79 90.13 91.01 86.88 72.48 85.13 87.33
Energy 78.87 76.46 93.21 88.09 85.69 78.53 83.48 92.79 95.92 92.10 81.53 90.59 86.32
MLS+ReAct 91.47 77.25 93.89 88.73 88.84 84.32 87.42 90.33 95.02 92.18 79.42 89.24 88.15
ODIN+ReAct 72.19 57.63 69.15 54.50 54.62 72.75 63.47 83.30 81.02 85.82 77.21 81.84 70.82
Energy+ReAct 89.80 75.02 93.62 87.11 87.40 81.76 85.79 93.00 91.69 86.45 81.28 88.11 86.66
to bring extra improvement. Besides, ReAct is shift by failing to light in-distribution activation
sensitivetothechoiceoftheactivationpruningper- pathways. We investigate how activations at vari-
centile. The optimal percentile values are different ous stages of a deep network vary under different
for different open-set/OOD datasets (see Section ‘unseen’datasets.Figure5showshistogramsofthe
S4 in the supplementary). For identifying out-of- maximumactivationsattheoutputsfromlayer 1
distribution inputs, we recommend more stable to layer 4 of ResNet-18 [53] trained on CIFAR10
and deterministic magnitude-aware scoring rules. when evaluated on data with different shifts (here
we use ‘layer’ to refer to ResNet block).
100 85 Foropen-setdata,wefindthatearlylayeracti-
95 80 vationsarelargelythesameasfortheIDtestdata.
90 75 It is only later in the network that the activation
70 patternsbegintodiffer.Thisisintuitiveasthelow-
85
65 level textures and statistics of the open-set data
80
60 do not vary too much from the training images.
75 55 Furthermore,ithaslongbeenknownthatearlyfil-
70 50 tersinCNNstendtofocusontexturaldetailssuch
MSP MLS OOD OIN D_En mer egy thR oe d+Ac sMt LSRe +A Oct DIN R +e EA nct e rgy MSP MLS OOODI DN _mEn eer tg hy oR de s+Ac Mt LS Re +A Oct DIN R +e EA nct e rgy a ds ate ad sg ee tss ,[ s5 u4 c]. hI an sc So Vnt Hra Ns ,t, inw de ucfi end vet rh ya dt iffso em ree ntO aO ctD
i-
(a) small-scale (b) large-scale
Fig. 4: OOD detection performance of various vationsintheearlylayers.Ourexplanationforthis
scoring rules averaged across different models. phenomenonisanalogous:SVHNcontainsverydif-
Magnitude-aware scoring rules, particularly MLS, ferentimagestatisticsandlow-levelfeaturestothe
are the most efficient and stable techniques. training dataset of CIFAR10, and hence induces
different activations in early layers. Most interest-
ingly,however,somedatasetswhichshowmarkedly
different early layer activations actually display
3.3 Qualitative analysis
moresimilar activationsatlaterlayers(likeSVHN,
In this section, we qualitatively interrogate the see Figure A1).
learned representations of Cross-Entropy and Out- Meanwhile, OE displays show substantially dif-
lierExposurenetworksinordertoexplainthestark ferent intermediate activations. Interestingly, the
performance boost of OE on existing OOD detec- maximum activation in early layers look very simi-
tion benchmarks. Specifically, we use the value of lar to the ID testing data, but tend to be less so
themaximallyactivatedneuronatvariouslayersto later on in the network. It is clear that activations
analyze how the networks respond to distribution in later layers are more discriminative after using
shifts. We pass every sample through the network, OE loss when compared with using CE loss.
and plot the histogram of maximum activations
at every layer in Figure 5 (see Figure A2 for the 4 Disentangling distribution
analogous results by training with the ARPL+CS
shifts
method).
This is inspired by [8], who show the ‘maxi-
Having analysed methodologies for detecting dis-
mum logit score’ (MLS, the maximum activation
tributionshiftacrosstheOODdetectionandOSR
at a network’s output layer) can achieve SOTA
settings, we turn our attention to the benchmarks.
for OSR. Furthermore, [52] propose that networks
While it is clear that OSR specifically aims to
respondtoa‘lackoffamiliarity’underdistribution
8
CORUA
puxiM+EC
CORUACE OE
Maximum activation Maximum activation
Layer 1 Layer 2 Layer 3 Layer 4 Layer 1 Layer 2 Layer 3 Layer 4
Training data Testing data (ID) Testing data (w/ semantic shift) Testing data (w/ covariate shift)
Fig. 5: Histogram of activations for ResNet-18 pretrained on a subset of CIFAR10 with four training
classes and evaluated on: training and ID testing data; open-set data (disjoint six classes in CIFAR10)
and OOD data (from Textures, LSUN and Places365). Specifically, each subplot shows the maximum
activation(alongchannel,widthandheightdimension)attheoutputsfromlayer 1tolayer 4ofResNet-
18, displayed from left to right in the figures. The behavior of OE is different from CE, whose activation
maps become more separable in the deeper rather than the shallower layers. See Section B in Appendix
for results on more datasets.
detectunseencategories,thereisnospecificationof varied without the category label changing. In
thetypeofdistributionshiftwhichOODdetection this framing, given marginal training distributions
benchmarks aim to capture, or how they would p (Y )andp (Y ),detectingsemanticshift
train S train C
relate to a real-world scenario. In this section, isthetaskofflaggingwhenp (Y )=p (Y ).
test S train S
̸
we propose a lens through which to consolidate Analogously, we wish to flag covariate shift if
types of distribution shift. Specifically, we propose p (Y )=p (Y ).
test C train C
̸
that‘distributionshift’canbeparameterisedalong To motivate this setting, consider the percep-
two broad, orthogonal, axes: semantic shift and tual system in an autonomous vehicle, which has
covariate shift. Pure semantic shift is when new been trained to recognize cars during the day. A
categoriesareencountered,andistheexplicitfocus semantic shift detector is necessary for when the
of OSR, while covariate shift refers to the setting system encounters a new category, e.g., to flag
whenthesemanticsoftestimagesremainconstant, that bicycle is an unknown concept. Meanwhile, a
but other features change. covariate shift detector is necessary for when the
Formally,similarlyto[55],weconsideralatent system is deployed at night-time, where the cate-
variablemodelofthedatagenerationprocess,with goriesmaybefamiliar,buttheperformanceofthe
latent z: system could be expected to degrade.
z p(z) yi p(yi z) i 1...L x p(xz) 4.1 Datasets
∼ ∼ | ∈{ } ∼ |
(1)
Here,xisanimageandyi representsanimage Asastartingpoint,wenotethat[8]introducedthe
attribute. The set of attributes could include tra- Semantic Shift Benchmark (SSB), a distribution
ditional features such as ‘color’ or ‘texture’, or shift benchmark with isolates semantic shift. We
refertomoreabstractfeaturessuchas‘beakshape’ mainlyfocusonImageNet-SSB[56]andCUB-SSB
of a bird. We define a set of semantic attributes, [57] datasets. ‘Seen’ classes in ImageNet-SSB are
Y , such that the category label of an image is the original ImageNet-1K classes, while ‘unseen’
S
a function of these attributes. Furthermore, we classes selected from the disjoint set of ImageNet-
definecovariateattributes,Y ,whichcanbefreely 21K-P [15]. Meanwhile, CUB-SSB splits the 200
C
9
erutxeT
NUSL
563secalP
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneDTable 4: Results of OOD detection and OSR benchmarks on large-scale datasets, using ResNet-50 model
trained with the OE loss compared with CE and ARPL baselines. The results are averaged from five
independent runs. We separately introduce outlier data from different data sources including Places and
YFCC15M to feed OE.
Training CovariateShift SemanticShift
ScoringRule Overall
Method ImageNet-SSB CUB Scars FGVC
ImageNet-C ImageNet-R AVG (Easy/Hard) (Easy/Hard) (Easy/Hard) (Easy/Hard) AVG
CE 67.92 86.71 77.32 80.28 75.05 88.29 79.33 94.03 82.24 90.65 82.55 84.05 82.71
ARPL+CS 63.94 82.77 73.36 79.92 74.60 83.50 75.49 94.78 83.63 87.04 77.71 82.08 80.34
MLS
OE(w/Places) 61.77 80.53 71.15 82.42 75.58 79.16 73.83 91.02 78.69 88.38 79.19 80.81 78.88
OE(w/YFCC15M) 64.12 82.01 73.07 79.37 72.55 75.19 70.28 84.03 71.34 74.20 66.63 71.12 71.51
Table 5: Evaluation on large-scale OOD detection and OSR benchmarks using ResNet-50 model trained
with different losses and scoring rules. The results are averaged from five independent runs. Bold values
represent the best results, while underlined values represent the second best results. Models trained with
the CE loss outperforms the ones with ARPL on both covariate shift and semantic shift.
(a)EvaluationbasedonResNet-50trainedwiththeCEloss.
Training CovariateShift SemanticShift
ScoringRule Overall
Method
Waterbird ImageNet-SSB CUB
ImageNet-C ImageNet-R AVG AVG
(Easy/Hard) (Easy/Hard) (Easy/Hard)
MSP 64.63 80.53 81.65 75.33 75.54 80.16 75.01 88.11 79.43 80.68 78.11
MLS 67.92 86.71 81.87 75.18 77.92 80.28 75.05 88.29 79.33 80.74 79.33
ODIN 63.69 85.62 79.51 71.54 75.09 74.56 75.27 86.24 73.88 77.49 76.29
Energy 68.05 87.04 82.49 74.60 78.05 79.76 74.96 88.81 79.06 80.65 79.35
MLS+ReAct 66.64 84.82 81.69 75.12 77.07 80.28 75.07 88.29 79.33 80.74 78.91
ODIN+ReAct 61.69 83.25 79.48 71.50 73.98 74.56 75.29 86.24 73.88 77.49 75.74
Energy+ReAct 66.88 83.92 82.48 74.55 76.96 79.76 74.99 88.81 79.06 80.66 78.81
(b)EvaluationbasedonResNet-50trainedwiththeARPLloss.
Training CovariateShift SemanticShift
ScoringRule Overall
Method
Waterbird ImageNet-SSB CUB
ImageNet-C ImageNet-R AVG AVG
(Easy/Hard) (Easy/Hard) (Easy/Hard)
MSP 61.85 78.68 79.42 72.30 73.06 79.90 74.67 83.53 75.64 78.44 75.75
MLS 63.94 82.77 79.48 72.09 74.57 79.92 74.60 83.50 75.49 78.38 76.47
ODIN 61.88 77.03 73.76 69.26 70.48 68.72 71.23 73.87 69.77 70.90 70.69
Energy 64.13 83.25 79.64 71.86 74.72 79.87 74.49 83.70 75.46 78.38 76.55
MLS+ReAct 62.69 80.69 79.44 72.07 73.72 79.92 74.60 83.44 75.43 78.35 76.04
ODIN+ReAct 62.23 76.08 73.75 69.23 70.32 68.72 71.23 67.42 63.91 67.82 69.07
Energy+ReAct 62.89 81.17 79.60 71.83 73.87 79.87 74.49 83.70 75.41 78.37 76.12
bird classes in CUB into ‘seen’ and ‘unseen’ cate- choose Waterbirds [18] to test the model trained
gories.Furthermore,theunseencategoriesaresplit ontheCUB-SSB‘Seen’classes.Waterbirdsinserts
intoEasyandHardclassesbytheirattributes,and bird photographs from the CUB dataset into
the splitting rule depends on semantic similarity backgrounds picked from the Places dataset [46],
of every pair of visual attributes in the unknown meaning it has the same semantic categories to
classes and the training classes. For all the above CUB but in different contexts.
datasets, categories appearing in the training set Discussion.Wenotethatthereisnouniquely
would not be included in the evaluation set. optimal framing for discussing distribution shift,
For covariate shift, we propose ImageNet-C and here briefly discuss alternate proposals. For
[16] and ImageNet-R [17] to demonstrate distribu- instance, [58] propose a fine-grained analysis of
tion shift with respect to the standard ImageNet the shifts, where the test time distribution is con-
dataset. Both datasets contain images from a trolled for specific attributes such as shape and
subset of the ImageNet-1K categories, but with pose. Also related, [9] discuss that indications of
different low-level image statistics. ImageNet-C ‘unfamiliarity’ in a neural network could refer to
applies four main corruptions (e.g., noise, blur, many things, including confusing classes and sub-
weather,anddigital)withvaryingintensitiestothe populationshift.Weproposeoursimpleframingas
validationimagesofImageNet-1K,whileImageNet- awaytofillthe‘negativespace’leftbytheseman-
R collects various artistic renditions of foreground tic shift detection task of OSR. Furthermore, we
classes from the ImageNet-1K dataset. We also suggest it is important to study distribution shift
in this way, as classifiers are specifically optimized
10
EC
SC+LPRASVHN Texture LSUN LSUN-R iSUN Places365
2.0 2.0 2.0 2.0 2.0 2.0
1.5 1.5 1.5 1.5 1.5 1.5
1.0 1.0 1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5 0.5 0.5
0.0 0 1 2 3 0.0 0 1 2 3 0.0 0 1 2 3 0.0 0 1 2 3 0.0 0 1 2 3 0.0 0 1 2 3
Maximum activation Maximum activation Maximum activation Maximum activation Maximum activation Maximum activation
11 1 1 707 5 2 ... . . 505 0 5 5 4 3
2
5 4 37 6 3 3 2 2 1. . . . .5 0 5 0
5
3 3 2 2 1. . . . .5 0 5 0
5
2 2 1 1. . . .5 0 5
0
5 2. .0 5 1 2 1 1 0. .0 5 1 0. .0 5 0.5
0.0 0 5 10 15 20 0 0 5 10 15 20 0 0 5 10 15 20 0.0 0 5 10 15 20 0.0 0 5 10 15 20 0.0 0 5 10 15 20
Maximum activation Maximum activation Maximum activation Maximum activation Maximum activation Maximum activation
2.0 2.0 2.0 2.0 2.0 2.0
1.5 1.5 1.5 1.5 1.5 1.5
1.0 1.0 1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5 0.5 0.5
0.0 2 4 6 0.00 2 4 6 8 10 0.00 2 4 6 8 10 12 0.00 2 4 6 8 0.0 2 4 6 8 0.0 2 4 6
Maximum activation Maximum activation Maximum activation Maximum activation Maximum activation Maximum activation
ID OOD Aux
Fig.6:Thedistributionofthemaximumactivationoftheoutputfeaturefromthelastblockisinvestigated
forIDtrainingdata,OODtestingdata,andauxiliarytrainingdataonsmall-scaledatasets.Whenequipped
with OE loss, the highly correlated auxiliary data (300K images) can greatly enhance the OOD detection
performance. Therefore, careful selection of auxiliary data is crucial.
0.30 0.6 0.10 0.10
0.25 0.5 0.08 0.08
0 0. .2 10 5 0 0. .4 3 0.06 0.06
0.10 0.2 0.04 0.04
0.05 0.1 0.02 0.02
0.00 0.0 0.00 0.00
5 10 15 20 25 2 4 6 8 10 0 10 20 30 40 50 60 70 80 0 20 40 60 80 100
Maximum activation Maximum activation Maximum activation Maximum activation
CE ARPL OE (YFCC15M) OE (Places)
ID OOD Aux
Fig. 7: Analysis on the distribution of the maximum activation of the output feature from the last layer,
for ID training data, OOD testing data, and auxiliary training data on large-scale datasets. Compared
to the 300K images for small-scale datasets, there is less similarity between the auxiliary data (i.e.,
YFCC15M and Places) and the OOD data. This finding aligns with the results in Table 4.
todifferentiatebetweenonesetfeatures(Y )while 4.2 Quantitative analysis
S
in fact being invariant to others (Y ). As such, we
C In Tables 4 and 5, we evaluate a selection of
wouldexpectmodelstoreactdifferentlytochanges
previously discussed methods on our large-scale
their distributions.
benchmark for both OOD detection and OSR.
Finally,wenotethatforthecovariatelyshifted
Through this large-scale evaluation, we find that
samples,weideallywishtodevelopclassifierswhich
in terms of training methods, among CE, ARPL
are robust and can perform well despite the pres-
(+CS), and OE, there are no clear winners across
ence of distribution shift. However, given that
theboard.Itissurprisingthatthebestperformeron
machine learning models performance degrades
the previous small scale benchmarks (see Table 2),
underdistributionshift,wewishtobeabletodetect
OE, appears to struggle when scaled up (last two
whentheshiftispresent.Tomeasurewhetherthere
rows in Table 5). We analyse this contradiction in
is a trade-off between robust models and those
thenextsection.Intermsofscoringrules,weagain
which can detect covariate shift, we introduce a
find that the magnitude-aware scoring rules (MLS
new metric in Section 4.5.
and Energy), consistently produce the best perfor-
mance regardless of the methods and benchmarks
(bothstandardsmall-scaleonesandourlarge-scale
ones).
11
deniarterP
EO
EO
TiV
)K003
/w(
)M51CCFY
/w(
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneDQuery Gallery
ID test ID: CIFAR-10 train, Aux: 300k random images
OOD
(a) Retrieval on small-scale datasets
ID test ID: ImageNet train, Aux: YFCC-15M
OOD
(b) Retrieval on large-scale datasets
Semantic shift ID: ImageNet, Aux: YFCC-15M
Query
ID
Auxiliary
(c) Retrieval on semantic shift dataset
High Similarities Low
Fig. 8: Visualization of nearest neighbors of test samples retrieved from the union of ID and auxiliary
training data. We search for the nearest neighbors of samples in both small-scale (e.g., Textures and
Places365) and large-scale (e.g., ImageNet-C and ImageNet-R) OOD datasets. We also investigate the
nearestneighboursforthesamplewithsemanticshiftusingImageNet-SSB.Itisclearthattheimprovement
in OOD detection performance with OE is closely tied to the similarity between OOD and auxiliary data.
4.3 OE on large-scale datasets critical difference between OE and other methods
is that OE uses auxiliary OOD data for training.
Here, we investigate why OE performs worse than
Intuitively,ifthedistributionoftheauxiliaryOOD
other methods on a large-scale benchmark. One
12
01-RAFIC
teNegamI
-teNegamI
-teNegamI
-teNegamI
serutxeT
563secalP
tset
tset
C
R
BSS80
95
75
94
70
93
65
92
60
91
4.8754.8804.8854.8904.8954.9004.9054.910 4.0 4.5 5.0 5.5 6.0 6.5
Distance Distance
80
95
94 75
93 70
92 65
91 60
0.05 0.10 0.15 0.20 0.25 0.50 0.55 0.60 0.65 0.70 0.75 0.80 0.85
Distance Distance
(a) small-scale (b) large-scale
Fig. 9: OOD detection performance vs. OOD-AUX data distance of different auxiliary training data for
both (a) small-scale and (b) large-scale OOD data. The OOD detection performance decreases as the
distance between OOD data and auxiliary data increases, for both measurements based on Top-K nearest
neighbors and deep kernel distance.
training data accurately reflects the distribution retrieved nearest neighbors are found in the aux-
of the actual OOD testing data, we would expect iliary data. However, such phenomenon does not
better detection performance. Otherwise, incom- occur consistently in the large-scale datasets. This
plete or biased outlier data can negatively impact observation aligns with the correlation between
learning.Toanalyzethisfurther,weplotthedistri- thedistancefromOODdatatoauxiliarydataand
butionofmaximumactivationoftheoutputfeature the OOD detection performance of models trained
(fromthelastlayer)forsamplesfromdifferentdata using OE in Table 4.
sources: ID data, OOD data, and auxiliary data.
The results are shown in Figure 6. It is worth not- 4.4 Dataset proximity vs. OOD
ing that the OOD detection performance strongly
detection performance
(negatively) correlates with the overlapped region
oftheIDandOODcurve.Additionally,whenusing To verify that the dataset proximity between aux-
300K random images as auxiliary OOD data (as iliary data and OOD data correlates to the OOD
shown in the 2nd row), there is a high correlation detectionperformance,wemeasurethecorrelation
with actual OOD data, resulting in excellent per- between OOD detection performance and dataset
formance (see Table 2). We also provide results proximity. We quantify this proximity by calculat-
on a large-scale dataset in Figure 7 and further ing the distance between OOD data and auxiliary
qualitative investigation in Section C in Appendix. data. For a specific OOD dataset, we compute the
We further retrieve nearest neighbors for the distance via Top-K nearest neighbors and a deep
given samples on both small-scale (e.g., Textures kernel method [59], respectively. For Top-K near-
and Places365) and large-scale (e.g., ImageNet-C est neighbors, we compute the average of all the
andImageNet-R)benchmarksusingmodelstrained distances between the normalized feature of each
by OE (see Figure 8). By retrieving the nearest OOD sample and its Top-K nearest neighbors in
neighbors from the union of ID data and auxiliary
data,weobservethatforsmall-scalescenarios,the
13
K-poT
srobhgien
tseraen
lenrek
peeD
)CORUA(
ecnamrofrep
DOO
)CORUA(
ecnamrofrep
DOO
)CORUA(
ecnamrofrep
DOO
)CORUA(
ecnamrofrep
DOOthe auxiliary dataset. It can be formulated by: This is because we expect a good model to cor-
rectly classify all samples with any covariate shift
Dist ( ood, aux)= |iD=o 1od | K k=1d(Z iood,Z kaux) , and Tid he en nti ufy mban ery ore fm tha ein cin oug nO teO dD ins sa tm anp cle es s. is then
nn D D P PK |Dood | divided by the total number of testing instances
(2) to produce the OAA, which is robust to non-
where oodand auxrepresenttheOODandauxil- semantic shift. This measure is computed under
iarydaD tasets.ZoDod andZaux arethel 2-normalized different thresholds based on the scoring rules and
extracted feature from the pretrained model given aggregated:
OOD and auxiliary samples. d(, ) is the distance mOAA metric. The OAA values across all
· ·
measurefornearestneighborsretrieval.Wealsocal- thresholds can also be aggregated into a single
culate the deep kernel distance between the OOD valuewithin[0,1]asanoverallmeasure,meanOOA
and auxiliary datasets following [59]: (mOAA). To compute the mOAA, we consider
testing images including both ID data from id
Dist ( ood, aux)=M\MD2 ood, aux;ϕ , and OOD data from ood. The mOAA is definD ed
dk D D u D D as follows: D
(3)
(cid:0) (cid:1)
2
w thh eer me aM\ xiM mD umu( ·, m·) eais
n
t dh ie scU rep-s at na cti yc (e Msti Mm Dat )or [6o 0f
] mOAA=
1 N
OAA =
1 N pc ii+pc io
,
N i N id + ood
based on i.i.d. samples from ood and aux, i=1 i=1 |D | |D |
which is considered as an unbD iased estimD ator X X (4)
having nearly minimal variance [61]. ϕ(x,y) = where pci denotes the correct predictions among
i
(1 ϵ)κ Zood,Zaux +ϵ q(x,y), where κ(, ) the testing samples predicted as ID, pc io denotes
− · · the correct predictions among the testing samples
and q(, ) are Gaussian kernels. ϵ is sampled from
(cid:2) · ·(cid:0) (cid:1) (cid:3) predicted as OOD (see Figure 10), represents
(0,1).Theintuitionbehindthedeepkernelmethod |·|
the size of the testing set, and N is the number of
is to effectively distinguish data distributions via
differentthresholds.ThemOAAscorerangesfrom
representative deep features of samples from each
0to1.Ahighervalueindicatesbetterperformance,
distribution.Wecomputethedistance(denotedas
withascoreof1representingperfectdetectionand
OOD-AUXdatadistance)forbothsmall-scaleand
recognition,whileascoreof0representstheworst
large-scale OOD data in Figure 9, which further
performanceintermsofseparationandrecognition.
validates that closer proximity between auxiliary
data and OOD data leads to better performance
Based on the numerical results in Figure 11
by OE models.
and Table 6, we have observed a turning point for
the threshold that achieves the optimal balance
4.5 Outlier-aware accuracy
between model robustness and OOD detection.
Finally, we introduce a new metric to reconcile Itisworthnotingthatthismetrichasaconnec-
theproblemsofdetecting covariateshiftandbeing tion to AURC [62]. While both AURC and OAA
robust to it. Although AUROC is commonly used considerclassifierperformance,ourproposedOAA
to compare different techniques for distinguishing specifically measures the ‘correct prediction rate’,
out-of-distributionsamples,itdoesnotcapturethe providing an interpretable value between 0 and 1.
model’s ability to reliably classify testing samples Therefore, we believe that this metric can be effec-
in the presence of distribution shifts. To analyze tively used to study the tradeoff between OOD
the relationship of performance between covariate detectionandgeneralizationwithgreaterprecision.
shiftandrobustness,weintroduceanovelmeasure,
whichwetermOutlier-AwareAccuracy (OAA).At 5 Summary of empirical
a given threshold, and a given set of predictions phenomena
(both ID vs. OOD predictions, and predictions
within the closed-set categories), we compute the
In the previous sections, we thoroughly evaluated
aggregate frequency of ‘correct’ predictions. The
methods for OOD detection and OSR in terms of
definition of ‘correct’ varies depending on the pre-
scoringrules,trainingmethods,andauxiliarydata.
diction. Specifically, as shown in Figure 10, all
To summarize these phenomena, we briefly high-
instancespredictedasIDsamplesshouldhaveaccu-
lightthekeyobservationsasfollows:(i)Magnitude-
rate class predictions, and all OOD samples that
aware scoring rules (i.e., MLS and energy) offer
are not already categorized should be detected.
obvious advantages for both OOD detection and
14Predicted Categories Predicted Categories
Predicted ID Predicted OOD Predicted ID Predicted OOD
Square Square True ID
True OOD
/ Correct / Incorrect
Triangle Triangle
Threshold Scores Threshold Scores
Outlier-Aware Accuracy = 𝑝 !% ! + 𝑝 ! % & = ! = 4+4 Outlier-Aware Accuracy = 𝑝 !% ! + 𝑝 ! % & = ! = 4+0
(OOA) Ω" +Ω# + 16 (OOA) Ω" +Ω# + 16
Fig. 10: Demonstration for Outlier-Aware Accuracy (OAA). OAA measures the frequency of ‘correct’
predictions made by the model at a given threshold. The ‘correct’ predictions are defined as: (1) Among
the testing samples predicted as ID, those whose semantic class labels are correctly predicted, denoted as
pci; (2) True OOD samples that have incorrect semantic class predictions, denoted as pco.
i i
Table 6:WecomputethemeanOAA(mOAA)rateacrossallthresholdstocomparedifferentapproaches.
Methods MSP MLS Energy MLS+ReAct Energy+ReAct
CE 0.661 0.658 0.655 0.655 0.641
ARPL 0.664 0.660 0.657 0.659 0.646
OE(w/YFCC15M) 0.623 0.611 0.611 0.583 0.597
0.80 0.80 the distribution shift problem into covariate shift
0.73 0.73 and semantic shift, proposing large-scale evalua-
0.66 0.66 tionprotocolsforbothsettings.Ourstudyrevealed
0.59 0.59 thatthebestperformingmethodcurrentOSRand
OOD datasets (Outlier Exposure) does not gener-
0.52 0.52
alizewelltoourchallenginglarge-scalebenchmark.
0.45 0.45
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 We also discovered that magnitude-aware scoring
CE ARPL rulesaregenerallymorereliablethanothers.Over-
Fig.M 1SP 1:OMLS utliO eDIN r-AwEne argy reAML cS c + u Re rAc at cyO (DI ON + AReA Act )oE fnerg ay + cRe lA act s- all, our new benchmark can serve as an improved
sifier trained with CE/ARPL loss computed by testbed for measuring progress in OSR and OOD
different thresholds. detection while providing insights into these two
problems. We hope that our thorough empirical
investigationontheOODdetectionandOSRmeth-
OSR. Compared with non-magnitude-aware tech-
odsandbenchmarkscanshedlightforfuturestudy
niques (i.e., ODIN and ReAct), we recommend
and applications on the broader data distribution
more stable and deterministic magnitude-aware
shift detection problem.
scoring rules. (ii) Outlier Exposure (OE) is by far
the most effective training method for improving
Acknowledgments
performance on OOD detection and OSR bench-
markswhentheauxiliarydataishighlycorrelated
This work is supported by Hong Kong Research
to the actual OOD data. However, while finding
Grant Council - Early Career Scheme (Grant No.
suchauxiliarydataispossibleforsmall-scalebench-
27208022); National Natural Science Foundation
marks, it is highly non-trivial to find such data in
of China (Grant No. 62306251); a Facebook AI
(more realistic) large-scale settings.
Research Scholarship; and HKU Seed Fund for
Basic Research.
6 Conclusion
In this study, we explore Out-of-Distribution
(OOD)detectionandOpen-setRecognition(OSR).
Weconductedathoroughcross-evaluationofmeth-
ods for OOD detection and OSR. Additionally, we
introducedanewbenchmarksettingthatseparates
15References
Z.: Exploring covariate and concept shift
for out-of-distribution detection. In: NeurIPS
[1] Scheirer,W.J.,Rocha,A.,Sapkota,A.,Boult, Workshop (2021)
T.E.: Towards open set recognition. IEEE
TPAMI (2013) [14] Russakovsky, O., Deng, J., Su, H., Krause, J.,
Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,A.,
[2] Hendrycks, D., Gimpel, K.: A baseline for Khosla, A., Bernstein, M., et al.: Imagenet
detectingmisclassifiedandout-of-distribution large scale visual recognition challenge. IJCV
examples in neural networks. ICLR (2017) (2015)
[3] Liu,W.,Wang,X.,Owens,J.,Li,Y.:Energy- [15] Ridnik, T., Ben-Baruch, E., Noy, A., Zelnik-
based out-of-distribution detection. NeurIPS Manor, L.: Imagenet-21k pretraining for
(2020) the masses. arXiv preprint arXiv:2104.10972
(2021)
[4] Hendrycks, D., Mazeika, M., Dietterich, T.:
Deepanomalydetectionwithoutlierexposure. [16] Hendrycks, D., Dietterich, T.: Benchmarking
In: ICLR (2019) neural network robustness to common corrup-
tions and perturbations. In: ICLR (2019)
[5] Sun, Y., Guo, C., Li, Y.: React: Out-of-
distribution detection with rectified activa- [17] Hendrycks, D., Basart, S., Mu, N., Kadavath,
tions. In: NeurIPS (2021) S.,Wang,F.,Dorundo,E.,Desai,R.,Zhu,T.,
Parajuli, S., Guo, M., et al.: The many faces
[6] Chen, G., Qiao, L., Shi, Y., Peng, P., Li, J., of robustness: A critical analysis of out-of-
Huang,T.,Pu,S.,Tian,Y.:Learningopenset distribution generalization. In: CVPR (2021)
network with discriminative reciprocal points.
In: ECCV (2020) [18] Sagawa, S., Koh, P.W., Hashimoto, T.B.,
Liang, P.: Distributionally robust neural net-
[7] Chen,G.,Peng,P.,Wang,X.,Tian,Y.:Adver- works for group shifts: On the importance of
sarial reciprocal points learning for open set regularization for worst-case generalization.
recognition. IEEE TPAMI (2021) In: ICLR (2019)
[8] Vaze, S., Han, K., Vedaldi, A., Zisserman, [19] Ye,N.,Li,K.,Bai,H.,Yu,R.,Hong,L.,Zhou,
A.: Open-set recognition: a good closed-set F., Li, Z., Zhu, J.: Ood-bench: Quantifying
classifier is all you need? In: ICLR (2022) and understanding two dimensions of out-of-
distribution generalization. In: CVPR (2022)
[9] Tran,D.,Liu,J.,Dusenberry,M.W.,Phan,D.,
Collier,M.,Ren,J.,Han,K.,Wang,Z.,Mariet, [20] Scheirer, W.J., Rezende Rocha, A., Sapkota,
Z., Hu, H., et al.: Plex: Towards reliability A., Boult, T.E.: Toward open set recognition.
usingpretrainedlargemodelextensions.arXiv IEEE TPAMI (2012)
preprint arXiv:2207.07411 (2022)
[21] Ge,Z.,Demyanov,S.,Garnavi,R.:Generative
[10] Yang,J.,Zhou,K.,Li,Y.,Liu,Z.:Generalized openmaxformulti-classopensetclassification.
out-of-distribution detection: A survey. Inter- In: BMVC (2017)
national Journal of Computer Vision, 1–28
(2024) [22] Neal, L., Olson, M., Fern, X., Wong, W.-K.,
Li, F.: Open set learning with counterfactual
[11] Salehi, M., Mirzaei, H., Hendrycks, D., Li, images. In: ECCV (2018)
Y., Rohban, M.H., Sabokrou, M.: A unified
surveyonanomaly,novelty,open-set,andout- [23] Kong, S., Ramanan, D.: Opengan: Open-
of-distributiondetection:Solutionsandfuture set recognition via open data generation. In:
challenges. TMLR (2021) CVPR (2021)
[12] Hendrycks, D., Zhao, K., Basart, S., Stein- [24] Liang, S., Li, Y., Srikant, R.: Enhancing
hardt,J.,Song,D.:Naturaladversarialexam- the reliability of out-of-distribution image
ples. In: CVPR (2021) detectioninneuralnetworks.In:ICLR(2018)
[13] Tian, J., Hsu, Y.-C., Shen, Y., Jin, H., Kira, [25] Hsu, Y.-C., Shen, Y., Jin, H., Kira,
16Z.: Generalized odin: Detecting out-of- [38] Chen, J., Li, Y., Wu, X., Liang, Y., Jha, S.:
distribution image without learning from Atom: Robustifying out-of-distribution detec-
out-of-distribution data. In: CVPR (2020) tion using outlier mining. In: ECML PKDD
(2021)
[26] Huang, R., Geng, A., Li, Y.: On the impor-
tance of gradients for detecting distributional [39] Wang, Q., Ye, J., Liu, F., Dai, Q., Kalan-
shifts in the wild. In: NeurIPS (2021) der, M., Liu, T., Hao, J., Han, B.: Out-of-
distribution detection with implicit outlier
[27] Djurisic, A., Bozanic, N., Ashok, A., Liu, R.: transformation. In: ICLR (2023)
Extremely simple activation shaping for out-
of-distribution detection. In: ICLR (2023) [40] Wang,Q.,Fang,Z.,Zhang,Y.,Liu,F.,Li,Y.,
Han, B.: Learning to augment distributions
[28] Zhang, J., Fu, Q., Chen, X., Du, L., Li, Z., for out-of-distribution detection. In: NeurIPS
Wang, G., Han, S., Zhang, D., et al.: Out-of- (2024)
distributiondetectionbasedonin-distribution
data patterns memorization with modern [41] Krizhevsky, A., Hinton, G., et al.: Learning
hopfield energy. In: ICLR (2023) multiple layers of features from tiny images
(2009)
[29] Ahmed, F., Bengio, Y., Seijen, H., Courville,
A.: Systematic generalisation with group [42] Cimpoi,M.,Maji,S.,Kokkinos,I.,Mohamed,
invariant predictions. In: ICLR (2020) S.,Vedaldi,A.:Describingtexturesinthewild.
In: CVPR (2014)
[30] Deecke, L., Ruff, L., Vandermeulen, R.A.,
Bilen, H.: Transfer-based semantic anomaly [43] Ovadia, Y., Fertig, E., Ren, J., Nado, Z.,
detection. In: ICML (2021) Sculley, D., Nowozin, S., Dillon, J., Lakshmi-
narayanan, B., Snoek, J.: Can you trust your
[31] Yang, J., Wang, H., Feng, L., Yan, X., Zheng, model’s uncertainty? evaluating predictive
H., Zhang, W., Liu, Z.: Semantically coher- uncertainty under dataset shift. In: NeurIPS
ent out-of-distribution detection. In: ICCV (2019)
(2021)
[44] Yu, F., Seff, A., Zhang, Y., Song, S.,
[32] Kim, J., Koo, J., Hwang, S.: A unified bench- Funkhouser, T., Xiao, J.: Lsun: Construction
mark for the unknown detection capability of ofalarge-scaleimagedatasetusingdeeplearn-
deep neural networks. Expert Systems with ing with humans in the loop. arXiv preprint
Applications (2021) arXiv:1506.03365 (2015)
[33] Xia,G.,Bouganis,C.-S.:Augmentingsoftmax [45] Xu, P., Ehinger, K.A., Zhang, Y., Finkel-
information for selective classification with stein,A.,Kulkarni,S.R.,Xiao,J.:Turkergaze:
out-of-distribution data. In: ACCV (2022) Crowdsourcing saliency with webcam based
eyetracking.arXivpreprintarXiv:1504.06755
[34] Yang, J., Wang, P., Zou, D., Zhou, Z., Ding, (2015)
K.,Peng,W.,Wang,H.,Chen,G.,Li,B.,Sun,
Y., et al.: Openood: Benchmarking general- [46] Zhou, B., Lapedriza, A., Khosla, A., Oliva,
izedout-of-distributiondetection.In:NeurIPS A., Torralba, A.: Places: A 10 million image
(2022) database for scene recognition. IEEE TPAMI
(2017)
[35] Liang, S., Li, Y., Srikant, R.: Enhancing
the reliability of out-of-distribution image [47] Neal, L., Olson, M., Fern, X., Wong, W.-K.,
detectioninneuralnetworks.In:ICLR(2017) Li, F.: Open set learning with counterfactual
images. In: ECCV (2018)
[36] Yang, J., Zhou, K., Liu, Z.: Full-spectrum
out-of-distribution detection. IJCV (2023) [48] Le, Y., Yang, X.: Tiny imagenet visual recog-
nition challenge. CS 231N (2015)
[37] Ming, Y., Fan, Y., Li, Y.: Poem: Out-of-
distributiondetectionwithposteriorsampling. [49] DeVries, T., Taylor, G.W.: Improved regu-
In: ICML (2022) larization of convolutional neural networks
with cutout. arXiv preprint arXiv:1708.04552
17(2017) [61] Gretton, A., Fukumizu, K., Harchaoui, Z.,
Sriperumbudur,B.K.:Afast,consistentkernel
[50] Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh, two-sample test. In: NeurIPS (2009)
A., Goh, G., Agarwal, S., Sastry, G., Askell,
A., Mishkin, P., Clark, J., Krueger, G., [62] Geifman, Y., Uziel, G., El-Yaniv, R.: Bias-
Sutskever, I.: Learning transferable visual reduced uncertainty estimation for deep neu-
models from natural language supervision. In: ral classifiers. In: ICLR (2019)
ICML (2021)
[51] Zhang,H.,Ciss´e,M.,Dauphin,Y.,Lopez-Paz,
D.: mixup: Beyond empirical risk minimiza-
tion. In: ICLR (2017)
[52] Dietterich, T.G., Guyer, A.: The familiarity
hypothesis: Explaining the behavior of deep
opensetmethods.PatternRecognition(2022)
[53] He, K., Zhang, X., Ren, S., Sun, J.: Deep
residual learning for image recognition. In:
CVPR (2016)
[54] Krizhevsky, A., Sutskever, I., Hinton, G.E.:
Imagenet classification with deep convolu-
tional neural networks. In: NeurIPS (2012)
[55] Wiles,O.,Gowal,S.,Stimberg,F.,Rebuffi,S.-
A., Ktena, I., Dvijotham, K.D., Cemgil, A.T.:
A fine-grained analysis on distribution shift.
In: ICLR (2022)
[56] Russakovsky, O., Deng, J., Su, H., Krause, J.,
Satheesh,S.,Ma,S.,Huang,Z.,Karpathy,A.,
Khosla,A.,Bernstein,M.,Berg,A.C.,Fei-Fei,
L.: ImageNet Large Scale Visual Recognition
Challenge. IJCV (2015)
[57] Wah, C., Branson, S., Welinder, P., Perona,
P., Belongie, S.: The Caltech-UCSD Birds-
200-2011 Dataset. Technical Report CNS-TR-
2011-001, California Institute of Technology
(2011)
[58] Zhao, B., Yu, S., Ma, W., Yu, M., Mei, S.,
Wang, A., He, J., Yuille, A., Kortylewski, A.:
Robin:Abenchmarkforrobustnesstoindivid-
ualnuisancesinreal-worldout-of-distribution
shifts. In: ECCV (2022)
[59] Liu, F., Xu, W., Lu, J., Zhang, G., Gretton,
A.,Sutherland,D.J.:Learningdeepkernelsfor
non-parametric two-sample tests. In: ICML
(2020)
[60] Gretton, A., Borgwardt, K.M., Rasch, M.J.,
Scho¨lkopf,B.,Smola,A.:Akerneltwo-sample
test. JMLR (2012)
18Appendix A Appendix C
More experimental results on bench- Correlation of OOD and auxiliary
marks training data
We additionally evaluate different methods on In Figure A3, we also visualize t-SNE projec-
Scars-SSB and FGCV-Aircraft-SSB datasets from tions of representations for various datasets: ID
the Semantic-Shift Benchmark [8] to further inves- data (CIFAR10), auxiliary training OOD data
tigate the performance of scoring rules against (300K [4] vs. YFCC15M), and different test-time
semantic shifts. From Table A1 to Table A4, we OODdatasets.AsseeninFigureA3andFigureA4,
can observe that magnitude-aware scoring rules using 300K images generally leads to better over-
perform well among different training methods. lap with test-time OOD data. Consequently, OE
We also investigate the effect of OE using dif- trained with 300K as auxiliary ODD achieves
ferentauxiliarydata(e.g.,PlacesandYFCC-15M). superior performance compared to its counterpart
As shown in Table A3 and Table A4, we can trained with YFCC15M (Table 2 vs. Table A5
see that OE performance heavily depends on the in Section A) because it shows a better overlap
auxiliary training data. with the test-time OOD data.
The degeneration of OE when applied to large-
scale datasets drives us to think about the core Appendix D
contribution behind the OE method. To find out
thereason,weapplyOEtothesmall-scaledatasets Different architectures and training
with different auxiliary data (i.e., YFCC-15M) in setups
Table A5. Compared with results using 300K ran-
Apart from ResNet, we also conduct experi-
dom images, the one using YFCC-15M cannot
ments using DenseNet121 on small-scale datasets
exceed the performance of the OE baseline. This
andusingDinoViT-S/8onlarge-scaledatasets.As
indicates that the selection of auxiliary data is
showninTableA6andTableA7,magnitude-aware
essential to the OE method and the success of OE
approachesstillperformothersonseveraldatasets.
may come from the similarity of auxiliary data
distribution and test-time outliers.
Appendix B
ActivationsofOODandopen-setdata
at different layers
We provide the maximum activations for inter-
mediate layers of ResNet-18 trained on CIFAR10
when evaluated on in-distribution data and data
with different shifts. Activations in later layers are
more discriminative between ID and OOD/open-
setsamples.AfterusingtheOEloss,wecaneasily
notice that the OOD samples are more separable
than the model trained with CE loss in Figure 5.
We also show the maximum activation of the
remaining datasets in Figure A1.
We also visualize the histogram of maximum
activations of the model trained using ARPL+CS
at every layer in Figure A2. Our findings align
with the observations in Section 3.3 in the main
paper, indicating that the early layer activations
closelyresemblethoseoftheIDtestdatawhilethe
activationpatternsbegintodifferinthedeeperlay-
ers.Notably,theARPL+CSmethoddemonstrates
superior separation compared to CE, but it lags
behindOE,asillustratedinFigure5andFigureA1.
These findings align with results in Table 2.
19Table A1: Results of OOD detection and OSR benchmarks on large-scale datasets, using ResNet-50
model trained with the CE loss. The results are averaged from five independent runs.
Training CovariateShift SemanticShift
ScoringRule Overall
Method ImageNet-C ImageNet-R ImageNet-SSB CUB Waterbirds Scars FGVC
ID=63.05 ID=76.13 AVG (Easy/Hard) (Easy/Hard) (Easy/Hard) (Easy/Hard) (Easy/Hard) AVG
MSP 64.63 80.53 72.58 80.16 75.01 88.11 79.43 81.65 75.33 94.15 82.34 90.63 82.55 82.94 81.21
MLS 67.92 86.71 77.32 80.28 75.05 88.29 79.33 81.87 75.18 94.03 82.24 90.65 82.55 82.95 82.01
ODIN 63.69 85.62 74.66 74.56 75.27 86.24 73.88 79.51 71.54 92.87 80.88 90.97 80.97 80.67 79.67
Energy 68.05 87.04 77.55 79.76 74.96 88.81 79.06 82.49 74.60 93.92 82.03 90.86 82.82 82.93 82.03
MLS+ReAct 66.64 84.82 75.73 80.28 75.07 88.29 79.33 81.69 75.12 94.01 82.23 90.61 82.57 82.92 81.72
ODIN+ReAct 61.69 83.25 72.47 74.56 75.29 86.24 73.88 79.48 71.50 92.80 80.85 90.88 80.92 80.64 79.28
Energy+ReAct 66.88 83.92 75.40 79.76 74.99 88.81 79.06 82.48 74.55 93.89 82.00 90.80 82.79 82.91 81.66
Table A2: Results of OOD detection and OSR benchmarks on large-scale datasets, using ResNet-50
model trained with the ARPL loss. The results are averaged from five independent runs.
Training CovariateShift SemanticShift
ScoringRule Overall
Method ImageNet-C ImageNet-R ImageNet-SSB CUB Waterbirds Scars FGVC
ID=63.05 ID=76.13 AVG (Easy/Hard) (Easy/Hard) (Easy/Hard) (Easy/Hard) (Easy/Hard) AVG
MSP 61.85 78.68 70.27 79.90 74.67 83.53 75.64 79.42 72.30 94.83 83.96 86.81 78.01 80.91 79.13
MLS 63.94 82.77 73.36 79.92 74.60 83.50 75.49 79.48 72.09 94.78 83.63 87.04 77.71 80.82 79.58
ODIN 61.88 77.03 69.46 68.72 71.23 73.87 69.77 73.76 69.26 82.08 69.10 70.24 73.47 72.15 71.70
Energy 64.13 83.25 73.69 79.87 74.49 83.70 75.46 79.64 71.86 94.70 83.56 87.28 77.74 80.83 79.64
MLS+ReAct 62.69 80.69 71.69 79.92 74.60 83.44 75.43 79.44 72.07 94.77 83.66 87.01 77.69 80.80 79.28
ODIN+ReAct 62.23 76.08 69.16 68.72 71.23 67.42 63.91 73.75 69.23 82.07 69.09 70.20 73.49 70.91 70.62
Energy+ReAct 62.89 81.17 72.03 79.87 74.49 83.70 75.41 79.60 71.83 94.69 83.56 87.27 77.71 80.81 79.35
Table A3: Results of OOD detection and OSR benchmarks on large-scale datasets, using ResNet-50
model trained with the OE loss combined with auxiliary data from Places. The results are averaged from
five independent runs.
Training CovariateShift SemanticShift
ScoringRule Overall
Method
Waterbird ImageNet-SSB CUB
ImageNet-C ImageNet-R AVG AVG
(Easy/Hard) (Easy/Hard) (Easy/Hard)
MSP 61.02 75.30 79.11 73.88 72.33 82.20 73.45 75.91 69.18 75.19 73.76
MLS 61.77 80.53 79.31 73.88 73.87 82.42 75.58 79.16 73.83 77.75 75.81
ODIN 57.74 82.31 71.28 69.30 70.16 81.75 70.87 73.71 66.05 73.10 71.63
Energy 64.10 81.11 76.39 70.86 73.12 83.47 75.61 78.56 73.01 77.66 75.39
MLS+ReAct 62.39 79.76 77.00 71.93 72.77 81.23 73.07 72.09 70.16 74.14 73.45
ODIN+ReAct 58.28 77.94 69.74 70.06 69.01 80.27 70.54 74.40 68.00 73.30 71.15
Energy+ReAct 62.26 80.91 75.32 69.71 72.05 82.10 73.79 77.30 70.74 75.98 74.02
Table A4: Results of OOD detection and OSR benchmarks on large-scale datasets, using ResNet-50
model trained with the OE loss combined with auxiliary data from YFCC-15M. The results are averaged
from five independent runs.
Training CovariateShift SemanticShift
ScoringRule Overall
Method
Waterbird ImageNet-SSB CUB
ImageNet-C ImageNet-R AVG AVG
(Easy/Hard) (Easy/Hard) (Easy/Hard)
MSP 59.02 70.01 73.67 68.71 67.85 68.44 71.60 71.11 65.27 69.11 68.48
MLS 64.12 82.01 79.72 74.08 74.98 79.37 72.55 75.19 70.28 74.35 74.67
ODIN 60.80 74.36 66.80 68.94 67.73 72.01 66.87 71.48 67.91 69.57 68.65
Energy 64.31 81.50 77.95 72.76 74.13 81.50 74.33 77.78 70.09 75.93 75.03
MLS+ReAct 60.15 79.42 76.99 73.58 72.54 72.30 72.74 73.46 69.67 72.04 72.29
ODIN+ReAct 60.98 74.05 66.10 68.89 67.51 64.67 61.79 72.26 67.76 66.62 67.06
Energy+ReAct 61.87 79.79 78.83 71.98 73.12 71.24 73.26 75.78 69.85 72.53 72.83
Table A5: Results of OOD detection and OSR benchmarks on small-scale datasets, using ResNet-18
model trained with the OE loss combined with auxiliary data from YFCC-15M. The results are averaged
from five independent runs.
T Mr ea ti hn oin dg ScoringRule OODdetectionbenchmarks
AVG CIFAR10
CIFAR+10OSR CIb Fe An Rch +m 5a 0rks
TinyImageNet
Overall
SVHN Textures LSUN LSUN-R iSUN Places365 ID=95.47 ID=97.52 ID=97.76 ID=97.29 ID=87.30 AVG
MSP 98.96 99.50 98.17 94.44 94.50 99.61 97.53 90.17 91.21 88.17 80.54 87.52 93.53
MLS 98.97 99.50 98.19 94.46 94.55 99.61 97.55 90.36 93.47 89.25 81.44 88.63 93.98
ODIN 99.02 97.40 97.12 87.21 87.84 97.18 94.30 87.93 79.37 79.47 81.58 82.09 89.41
Energy 98.93 99.48 98.12 94.19 94.33 99.61 97.44 89.90 94.91 90.20 81.43 89.11 94.11
MLS+ReAct 98.86 99.49 97.76 94.69 94.78 99.60 97.53 89.77 91.99 89.92 81.32 88.25 93.82
ODIN+ReAct 98.89 96.66 95.48 82.50 83.60 96.37 92.25 83.31 84.32 82.37 81.43 82.86 88.49
Energy+ReAct 98.83 99.47 97.70 94.51 94.66 99.61 97.46 89.45 93.00 90.21 81.40 88.52 93.88
20
EC
LPRA
EO
EO
EOCE OE
Maximum activation Maximum activation
Layer 1 Layer 2 Layer 3 Layer 4 Layer 1 Layer 2 Layer 3 Layer 4
Training data Testing data (ID) Testing data (w/ semantic shift) Testing data (w/ covariate shift)
Fig. A1:Distributionofpre-unitactivationsforResNet-18pretrainedonCIFAR10,evaluatedontraining
and ID testing data; open-set data (from CIFAR-100) and OOD data (from SVHN, LSUN, iSUN).
Specifically, each subplot shows the maximum activation (along the channel, width and height dimension)
at the outputs from layer 1 to layer 4 of the ResNet-18 trained on CIFAR10, displayed from left to
right in the figures.
Maximum activation Maximum activation
Layer 1 Layer 2 Layer 3 Layer 4 Layer 1 Layer 2 Layer 3 Layer 4
Training data Testing data (ID) Testing data (w/ semantic shift) Testing data (w/ covariate shift)
Fig.A2:HistogramofactivationsforResNet-18pretrainedusingARPL+CSonasubsetofCIFAR10(with
four training classes) and evaluated on: training and testing ID data; open-set data (disjoint six classes in
CIFAR10)andOODdata.Specifically,eachsubplotshowsthemaximumactivation(alongchannel,width
and height dimension) at the outputs from layer 1 to layer 4 of the ResNet-18, displayed from left to right
in the figures. Activation maps become notably separable in the last layer between ID and open-set data.
21
NHVS
R-NUSL
NUSi
NHVS
R-NUSL
NUSi
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
ytisneD
erutxeT
NUSL
563secalP
ytisneD
ytisneD
ytisneDOE OE OE OE
CE ARPL+CS CE ARPL+CS
(300k) (YFCC-15M) (300k) (YFCC-15M)
(a) SVHN (b) Textures
(c) LSUN (d) LSUN-R
(e) iSUN (f) Places365
ID OOD Auxiliary
Fig. A3: t-SNE visualization of representations extracted by models with CE/ARPL+CS/OE loss. Each
point denotes a sample and its color denotes which distribution it comes from. The pink/green/brown
dots stand for ID/OOD/auxiliary data respectively. Together with quantitative results shown in Table 2,
we can observe that the performance boost can be achieved only when the auxiliary data distribution has
sufficient overlap with the test-time OOD data distribution (e.g., Texture and Places365).
OE (YFCC-15M) OE (Places)
ID OOD Auxiliary
Fig. A4:t-SNEvisualizationofrepresentationsextractedbymodelswithOElossusingdifferentauxiliary
data (e.g., Places and YFCC-15M), tested on ImageNet-R. Each point denotes a sample and its color
denotes which distribution it comes from. The pink/green/brown dots stand for ID/OOD/auxiliary data
respectively. Together with quantitative results shown in Table 5, the dispersion of OOD and auxiliary
data may lead to the unsatisfying performance boost on large-scale datasets.
22Table A6: Evaluation on small-scale OOD detection and OSR benchmarks with various methods, using
CIFAR10 as ID. The results are averaged from five independent runs. We report the in-distribution
accuracy as ‘ID’ and denote intractable results as ‘-’, resulting from unaffordable computational cost.
Bold values represent the best results, while underlined values represent the second best results. Different
methods have their optimal scope but MLS and Energy demonstrate stability and models trained with
OE dominate on almost all OOD datasets.
(a)EvaluationbasedonDenseNet-121trainedwiththeCEloss.
T Mr ea ti hn oin dg ScoringRule OODdetectionbenchmarks
AVG CIFAR10
CIFAR+10OSR CIb Fe An Rch +m 5a 0rks
TinyImageNet
Overall
SVHN Textures LSUN LSUN-R iSUN Places365 ID=95.43 ID=97.04 ID=96.73 ID=96.91 ID=83.52 AVG
MSP 95.17 91.56 93.39 94.61 94.40 89.39 93.09 91.63 93.77 90.47 79.76 88.91 91.42
MLS 97.05 91.85 94.53 96.11 95.84 89.46 94.14 92.63 95.70 91.88 81.52 93.59 93.92
ODIN 95.01 84.09 86.57 96.31 95.70 77.41 89.18 87.62 79.42 83.57 78.92 82.38 86.46
GODIN 96.31 90.85 94.04 96.23 95.37 88.99 93.63 90.30 91.34 87.76 76.23 86.41 90.74
SEM 75.65 72.02 75.18 70.93 72.52 76.14 73.74 51.26 47.60 45.36 - 48.07 58.67
Energy 97.25 91.97 94.68 96.28 96.02 89.57 94.30 92.73 95.89 92.01 81.04 90.42 92.75
MLS+ReAct 78.39 78.31 89.42 91.55 91.20 85.23 85.68 92.40 94.36 90.28 81.04 89.52 87.22
ODIN+ReAct 83.47 78.74 89.23 94.05 93.79 83.64 87.15 83.27 84.25 80.62 78.27 82.27 85.20
Energy+ReAct 85.55 82.68 87.81 90.92 89.61 77.07 85.61 92.63 95.28 91.03 81.53 90.12 87.41
MLS+ASH 97.01 91.40 93.72 95.56 90.39 86.30 92.40 92.21 94.35 90.56 80.88 89.50 91.24
MLS+SHE 89.80 75.02 91.62 87.11 87.40 81.76 85.45 78.80 81.92 77.27 79.92 79.48 83.06
(b)EvaluationbasedonDenseNet-121trainedwiththeARPL+CSloss.
T Mr ea ti hn oin dg ScoringRule OODdetectionbenchmarks
AVG CIFAR10
CIFAR+10OSR CIb Fe An Rch +m 5a 0rks
TinyImageNet
Overall
SVHN Textures LSUN LSUN-R iSUN Places365 ID=92.11 ID=97.00 ID=96.78 ID=96.43 ID=86.88 AVG
MSP 94.94 91.81 95.12 94.25 94.39 89.57 93.25 92.60 95.77 94.15 82.71 91.31 92.47
MLS 98.80 90.24 94.48 96.88 97.01 95.41 95.05 93.28 96.65 94.80 84.91 92.41 93.99
ODIN 78.02 73.46 82.30 93.26 91.40 73.77 82.04 67.81 80.82 73.40 72.52 73.64 78.68
GODIN 94.83 90.86 96.01 96.28 95.88 92.12 94.33 91.11 95.32 93.12 80.89 90.11 92.64
SEM 78.30 76.35 85.52 76.88 78.03 71.93 77.84 41.00 40.35 42.61 - 41.32 65.67
Energy 97.82 92.05 97.24 97.85 97.72 94.12 96.15 93.63 96.87 94.99 82.12 91.90 94.45
MLS+ReAct 85.27 94.50 82.44 87.50 82.98 89.43 87.02 92.12 95.33 92.91 80.50 90.22 88.97
ODIN+ReAct 83.29 79.08 85.12 88.62 83.47 82.05 83.61 68.20 73.12 70.24 55.10 66.67 76.83
Energy+ReAct 92.37 89.45 91.88 87.07 94.13 91.52 91.07 92.96 96.78 94.91 82.04 91.67 91.27
MLS+ASH 96.75 92.01 92.87 97.58 90.79 91.80 93.63 93.19 94.15 93.62 81.28 90.56 92.40
MLS+SHE 85.75 79.92 82.80 83.80 81.27 86.27 83.30 77.52 74.02 75.25 76.53 75.83 80.31
(c)EvaluationbasedonDenseNet-121trainedwiththeOEloss.
T Mr ea ti hn oin dg ScoringRule OODdetectionbenchmarks
AVG CIFAR10
CIFAR+10OSR CIb Fe An Rch +m 5a 0rks
TinyImageNet
Overall
SVHN Textures LSUN LSUN-R iSUN Places365 ID=94.82 ID=97.83 ID=98.40 ID=97.95 ID=83.48 AVG
MSP 99.27 98.86 99.38 98.54 98.58 98.01 98.77 96.42 99.32 98.88 78.90 93.38 96.61
MLS 99.27 98.87 99.38 98.54 98.58 98.02 98.78 96.40 99.36 98.83 80.37 93.74 96.76
ODIN 98.84 97.95 99.13 99.00 98.99 96.45 98.39 94.47 93.61 94.55 76.30 89.73 94.93
GODIN 97.01 95.11 88.98 83.36 84.57 89.39 89.74 93.77 92.63 92.31 78.20 89.23 89.54
SEM 97.43 96.74 98.62 96.99 97.11 94.80 96.95 41.25 47.11 42.75 - 43.70 79.20
Energy 99.26 98.87 99.41 98.52 98.56 98.04 98.78 93.32 99.36 98.81 80.21 92.93 96.4
Gradnorm 99.96 99.72 99.83 99.47 99.42 97.94 99.39 96.63 99.27 98.73 61.53 89.04 95.25
MLS+ReAct 85.25 87.12 89.13 88.12 88.74 83.28 86.94 92.43 95.56 97.06 76.63 90.42 88.33
ODIN+ReAct 84.27 85.71 86.13 88.12 85.35 88.07 86.78 85.83 84.21 90.46 75.25 83.94 85.64
Energy+ReAct 81.40 87.30 83.98 82.49 84.35 83.72 83.87 95.89 98.80 82.73 80.31 89.43 86.09
MLS+ASH 99.08 98.57 98.82 98.50 98.51 97.25 98.46 95.62 91.63 93.89 79.21 90.09 95.11
MLS+SHE 98.22 95.08 97.96 91.08 91.36 90.85 94.09 83.19 84.53 82.75 78.80 82.32 89.38
Table A7: Results of various methods on OOD detection benchmarks using CIFAR10 as ID training
data. The results are averaged from five independent runs. We train a DenseNet121 with the CE loss and
report the in distribution accuracy as ‘ID’.
Training CovariateShift SemanticShift
ScoringRule
Method
ImageNet-C ImageNet-R ImageNet-SSB
AVG AVG Overall
ID=63.05 ID=76.13 (Easy/Hard)
MSP 64.63 80.53 72.58 80.16 75.01 77.59 75.08
DenseNet121+CE
MLS 67.92 86.71 77.32 80.28 75.05 77.67 77.49
MSP 61.85 78.68 70.27 74.56 75.27 74.92 72.59
DenseNet121+ARPL
MLS 63.94 82.77 73.36 79.76 74.96 77.36 75.36
MSP 59.23 82.47 70.85 80.28 75.07 77.68 74.26
DinoViT-S/8
MLS 64.00 89.22 76.61 81.24 75.99 78.62 77.61
23
EC
SC+LPRA
EODissecting Out-of-Distribution Detection and Open-Set
Recognition: A Critical Analysis of Methods and
Benchmarks
–Supplementary Material–
Hongjun Wang1, Sagar Vaze2, Kai Han1∗
1The University of Hong Kong 2University of Oxford
Contents
S1Bar charts for cross-benchmarking results 2
S2OOD detection using CIFAR-100 as the ID training data 3
S3Influence of training configurations for OOD detection 5
S4Hyperparameters investigation for ReAct 6
S5Correlation of different metrics 7
S6Investigation on applying OE to the strong CLIP model 8
∗Correspondingauthor.
1
4202
guA
03
]VC.sc[
2v75761.8042:viXraS1 Bar charts for cross-benchmarking results
In this section, we provide more experimental results for different benchmarks in different ways.
Apart from the numerical results in Table 2 in the main paper, we also present the bar charts
for the comparison among different (1) scoring rules and (2) training methods in Figure S1.
In Figure S1 (a), we can observe that magnitude-aware methods (i.e., MLS and Energy) are
stableamongdifferenttrainingmethodsonbothbenchmarkswhileODINisencumberedbybeing
combined with ARPL+CS method.
InFigureS1(b),wecanseethattheOEmethodwiththehelpof300Krandomauxiliaryimages
outperforms others by a large margin. We also notice that the choice of auxiliary data is very
importantinOEperformancesinceOEwithYFCC-15Mcannotachieveasexcellentperformance
as the one with 300K random images.
(a) Performance of different scoring rules across OOD and OSR tasks
(b) Performance of different training methods across OOD and OSR tasks
Figure S1: Analysis of different scoring rules and training methods on standard benchmarks.
(a) Among various scoring rules, MLS and Energy show their reliability across OOD and OSR
datasets. (b) For different training methods, Outlier Exposure using different auxiliary data
obtains an obvious performance boost compared with others on OOD detection and have slight
gains on OSR.
2S2 OOD detection using CIFAR-100 as the ID training data
To further verify our findings in Section 3.2 in the paper, we also perform experiments using
CIFAR-100astheIDtrainingdata. ResultsareshowninTableS1,TableS2andTableS3. While
ODIN outperforms all the methods with the CE loss (due to superior performance on SVHN and
LSUN), the magnitude-aware methods consistently perform well (especially when combined with
post-processing techniques). This aligns with the findings observed on CIFAR-10.
Table S1: Results of various methods on OOD detection benchmarks, using CIFAR-100 as ID.
The results are averaged from five independent runs. We train ResNet-18 with the CE loss and
report the in distribution accuracy as ‘ID’.
Training OODdetectionbenchmarks
ScoringRule
Method
AVG
SVHN Textures LSUN LSUN-R iSUN Places365
ID=78.69
MSP 83.56 78.38 78.21 79.60 79.07 73.56 78.73
MLS 85.21 79.33 77.75 81.63 81.12 73.48 79.75
ODIN 97.47 77.05 90.49 77.59 79.38 71.96 82.32
GODIN 52.70 58.40 59.64 76.34 76.59 62.09 64.29
SEM 65.54 31.85 83.58 35.81 37.38 51.01 50.86
Energy 85.71 79.50 77.12 82.19 81.72 73.26 79.92
GradNorm 65.73 62.82 61.13 64.41 64.60 61.08 63.30
MLS+ReAct 84.51 83.94 84.96 80.30 79.98 78.32 82.00
ODIN+ReAct 96.39 78.74 89.62 75.88 77.28 71.78 81.62
Energy+ReAct 84.36 83.48 77.93 77.09 77.26 75.55 79.28
Table S2: Results of various methods on OOD benchmarks, using CIFAR-100 as ID. The results
areaveragedfromfiveindependentruns. WetrainResNet-18withtheARPL+CSlossandreport
the in distribution accuracy as ‘ID’.
Training OODdetectionbenchmarks
ScoringRule
Method
AVG
SVHN Textures LSUN LSUN-R iSUN Places365
ID=78.86
MSP 79.95 79.00 80.28 88.16 87.71 74.69 81.63
MLS 81.27 79.73 79.93 89.63 89.23 74.77 82.43
ODIN 87.72 64.90 78.21 82.03 82.60 65.12 76.76
GODIN 73.81 62.55 59.30 66.42 75.00 51.41 64.75
SEM 55.94 33.81 86.74 32.23 34.34 57.19 50.04
Energy 81.69 79.84 78.91 90.57 90.21 74.58 82.63
GradNorm 50.79 52.91 49.03 54.68 56.20 50.14 52.29
MLS+ReAct 77.71 80.32 85.93 88.44 87.78 76.41 82.77
ODIN+ReAct 20.02 33.07 20.25 36.67 36.74 37.82 30.76
Energy+ReAct 62.27 63.78 56.36 80.73 81.33 60.05 67.42
3
EC
SC+LPRATable S3: Results of various methods on OOD detection benchmarks, using CIFAR-100 as ID.
The results are averaged from five independent runs. We train ResNet-18 with the OE loss and
report the in distribution accuracy as ‘ID’.
Training OODdetectionbenchmarks
ScoringRule
Method
AVG
SVHN Textures LSUN LSUN-R iSUN Places365
ID=77.16
MSP 93.88 87.76 73.66 73.10 74.72 74.49 79.60
MLS 94.32 88.44 72.53 73.22 75.21 74.10 79.64
ODIN 97.20 83.94 85.96 71.61 74.60 71.90 80.87
GODIN 74.18 83.35 67.85 74.71 77.22 66.61 73.99
SEM 68.48 47.58 80.55 48.33 46.99 49.15 56.85
Energy 94.26 88.47 71.19 72.56 74.67 73.70 79.14
GradNorm 86.54 79.75 53.73 55.55 57.59 63.90 66.18
MLS+ReAct 94.50 89.04 77.72 80.31 80.88 76.73 83.20
ODIN+ReAct 96.76 82.66 85.65 76.91 78.48 70.50 81.83
Energy+ReAct 94.07 89.32 68.41 75.63 77.25 73.94 79.77
4
EOS3 Influence of training configurations for OOD detection
As demonstrated by ?, there exists a positive correlation between the closed-set accuracy and
open-set performance. Likewise, we would like to verify the correlation between closed-set per-
formance (Accuracy) and OOD detection performance (AUROC). Therefore, we train ResNet-18
with various configurations to investigate the relationship between closed-set accuracy and OOD
detection performance ?. For the OOD detection task, we use CIFAR10 as ID data and six com-
monlyuseddatasetsasOODdata: SVHN,Textures,LSUN,LSUN,iSUN,andPlaces365. Forthe
OSR task, we experiment with CIFAR10, CIFAR+10, CIFAR+50, and TinyImageNet following
the class split convention in the OSR literature. (1) For the ReAct config, the models are trained
with a batch size of 128 for 100 epochs. The initial learning rate is 0.1 and decays by a factor
of 10 at epochs 50, 75, and 90. (2) For the MLS config, we train the models with a batch size
of 128 for 600 epochs with the cosine annealed learning rate, restarting the learning rate to the
initial value at epochs 200 and 400. Besides, we linearly increase the learning rate from 0 to
the initial value at the beginning. The initial learning rate is 0.1 for CIFAR10/100 but 0.01 for
TinyImageNet. Broadly speaking, we train a ResNet-18 on the ID data, with an SGD optimizer
and cosine annealing schedule. We train ARPL + CS and OE largely based on the official public
implementation. For the auxiliary outlier dataset in the OE loss, we follow ? and use a subset of
80MillionTinyImages? with300Kimages,removingallexamplesthatappearinCIFAR10/100,
PlacesorLSUNclasses. TheresultsunderdifferentconfigurationsarereportedinTablesS4toS7.
ComparingTableS5andTableS6,wecanseethattheIDperformanceinTableS5isbetterthan
thatinTableS6(92.99vs91.02). However,theOODdetectionperformanceinTableS5isinferior
to that in Table S6, indicating that the linear correlation for OSR revealed in ? may not hold for
the OOD detection task.
TableS4: ResultsonOODdetectionandOSRbenchmarks,usingResNet-18trainedwiththeCE
loss, adopting the ReAct config. The results are averaged from five independent runs.
T Mr ea ti hn oin dg ScoringRule OODdetectionbenchmarks
AVG CIFAR10
CIFAR+10OSR CIb Fe An Rch +m 5a 0rks
TinyImageNet
Overall
SVHN Textures LSUN-C LSUN-R iSUN Places365 ID=92.27 ID=97.13 ID=96.6 ID=96.8 ID=83.4 AVG
MLS 85.42 82.20 86.56 86.40 85.40 84.62 85.1 92.54 95.62 91.81 81.31 90.32 87.19
ODIN 70.62 65.32 72.49 71.68 70.43 69.10 69.94 59.77 51.37 50.22 80.96 60.58 66.20
Energy 85.45 82.20 86.62 86.45 85.44 84.65 85.14 92.52 95.68 91.86 81.28 90.34 87.22
MLS+ReAct 83.94 79.67 98.18 93.87 92.31 88.42 89.40 92.57 94.92 90.88 81.65 90.01 89.64
ODIN+ReAct 83.84 79.62 98.31 94.00 92.44 88.48 89.45 56.65 47.76 48.40 81.30 58.53 77.08
Energy+ReAct 83.08 76.63 95.06 94.17 92.18 85.46 87.76 92.58 95.02 90.99 81.67 90.07 88.14
Table S5: Results on OOD detection and OSR benchmarks, using ResNet-18 trained with the
ARPL+CSloss,adoptingtheReActconfig. Theresultsareaveragedfromfiveindependentruns.
T Mr ea ti hn oin dg ScoringRule OODdetectionbenchmarks
AVG CIFAR10
CIFAR+10OSR CIb Fe An Rch +m 5a 0rks
TinyImageNet
Overall
SVHN Textures LSUN-C LSUN-R iSUN Places365 ID=92.99 ID=97.13 ID=97.75 ID=97.83 ID=85.1 AVG
MLS 79.18 86.85 95.59 95.00 94.47 89.98 90.18 93.23 97.93 96.27 82.50 92.48 91.10
ODIN 50.14 38.35 74.25 42.96 43.46 53.99 50.53 92.57 97.37 95.23 53.20 84.59 64.15
Energy 79.08 86.82 95.68 95.07 94.54 90.03 90.20 93.19 97.96 96.30 80.45 91.98 90.91
MLS+ReAct 84.19 89.11 94.98 95.52 94.98 90.23 91.50 92.63 97.96 96.30 79.78 91.67 91.57
ODIN+ReAct 49.22 37.19 65.89 37.80 38.77 47.55 46.07 91.00 94.74 91.84 47.47 81.26 60.15
Energy+ReAct 84.13 89.13 95.10 95.63 95.09 90.32 91.57 92.59 98.01 96.33 79.90 91.71 91.62
Table S6: Results on OOD detection and OSR benchmarks, using ResNet-18 trained with the
ARPL+CS loss, adopting the MLS config. The results are averaged from five independent runs.
T Mr ea ti hn oin dg ScoringRule OODdetectionbenchmarks
AVG CIFAR10
CIFAR+10OSR CIb Fe An Rch +m 5a 0rks
TinyImageNet
Overall
SVHN Textures LSUN-C LSUN-R iSUN Places365 ID=91.02 ID=96.96 ID=96.77 ID=96.69 ID=86.91 AVG
MLS 96.36 90.20 96.59 96.95 96.88 93.29 95.05 93.16 96.58 94.67 84.79 92.30 93.95
ODIN 75.92 71.64 86.25 95.14 95.19 75.97 83.35 58.04 74.80 71.52 63.13 66.87 76.76
Energy 96.52 90.11 96.76 97.16 97.07 93.45 95.18 93.22 96.74 94.82 82.10 91.72 93.80
MLS+ReAct 95.87 92.37 96.37 96.34 96.30 92.97 95.04 92.70 96.42 94.53 82.05 91.43 93.59
ODIN+ReAct 71.87 73.36 83.19 92.34 92.36 69.10 80.37 55.71 62.88 61.85 54.29 58.68 71.70
Energy+ReAct 96.06 92.35 96.59 96.58 96.53 93.17 95.21 92.80 96.61 94.70 82.14 91.56 93.75
5
EC
SC+LPRA
SC+LPRATable S7: Results on OOD detection and OSR benchmarks, using ResNet-18 trained with the
OEloss,adoptingtheofficialconfigurationofOE.Theresultsareaveragedfromfiveindependent
runs.
T Mr ea ti hn oin dg ScoringRule OODdetectionbenchmarks
AVG CIFAR10
CIFAR+10OSR CIb Fe An Rch +m 5a 0rks
TinyImageNet
Overall
SVHN Textures LSUN-C LSUN-R iSUN Places365 ID=94.77 ID=97.59 ID=97.38 ID=97.34 ID=77.96 AVG
MLS 95.94 94.57 76.58 85.20 87.16 88.46 87.99 96.26 98.95 98.20 77.88 92.82 89.92
ODIN 93.32 92.84 65.85 84.50 87.24 82.29 84.34 93.44 96.18 93.69 77.49 90.20 86.68
Energy 95.84 94.45 75.50 84.55 86.64 88.19 87.54 96.33 98.95 98.04 77.73 92.76 89.62
MLS+ReAct 95.46 94.32 93.57 90.66 90.75 88.97 92.29 96.20 98.93 98.18 77.60 92.73 92.46
ODIN+ReAct 87.64 88.52 89.56 86.19 86.57 76.02 85.75 93.03 95.45 92.75 76.90 89.53 87.26
Energy+ReAct 87.84 89.29 88.66 90.14 93.69 94.67 90.72 91.04 98.93 98.02 77.48 91.37 90.98
S4 Hyperparameters investigation for ReAct
To ensure fairness and assess stability in our cross-evaluation, we report the averaged results
from five independent trials, carefully selected around the optimal values. We identify the best
hyper-parametersofReActwhichtruncatesactivationsabovethresholdtolimittheeffectofnoise.
Therefore,weinvestigatethechoiceofthepercentileofactivationsfortruncation. Ourexperiments
involvevaryingthepercentileofpruningactivations. AsshowninFigureS2,ReAct’sperformance
exhibits high sensitivity to the choice of the activation pruning percentile, and the optimal value
of the percentile differs for each dataset.
100
90
80
SVHN
Textures
LSUN
LSUN-R
iSUN
70 Places365
CIFAR10
CIFAR+10
CIFAR+50
TinyImageNet
60
99 95 90 85 80 70
Rectification percentile
Figure S2: Investigation on the optimal hyper-parameters of ReAct for different datasets across
OOD and OSR. ReAct is sensitive to the choice of the activation pruning percentile, and the
optimal value of the percentile differs for each dataset.
6
EO
CORUAS5 Correlation of different metrics
To demonstrate that our takeaways (in Section 3.2 in the main paper) are metric-agnostic, we
furtherevaluateusingthefollowingmetrics: (1)theareaundertheprecision-recallcurve(AUPR);
(2) OSCR ? which summarises the trade-off between closed-set accuracy and out-distribution /
open-set performance as the threshold on the open-set score is varied. Results are shown in
Table S8. The same takeaways key findings remain consistent when utilizing AUROC (see also
Table 2 in the main paper).
Table S8: Results of various methods on OOD detection benchmarks under different metrics
(AUPR/OSCR), using CIFAR10 as ID. We train ResNet-18 with the OE loss. The results are
averaged from five independent runs.
OODdetectionbenchmarks
Method SVHN Textures LSUN LSUN-R iSUN Places365
AUROC AUPR OSRC AUROC AUPR OSRC AUROC AUPR OSRC AUROC AUPR OSRC AUROC AUPR OSRC AUROC AUPR OSRC
OE+MLS 99.21 99.54 94.69 98.82 98.81 95.16 99.02 98.13 94.01 98.53 91.91 90.88 98.57 91.94 90.94 97.32 99.42 95.25
OE+ODIN 99.43 99.42 94.64 98.73 94.05 93.21 99.14 97.12 92.89 98.78 78.48 83.85 98.75 79.07 84.42 96.41 95.98 93.01
OE+Energy 99.20 99.52 94.65 98.78 98.76 95.14 99.02 98.01 93.94 98.55 91.38 90.58 98.58 91.52 90.70 97.31 99.44 95.24
OE+MLS+ReAct 95.18 99.50 94.45 92.22 98.80 95.01 79.46 97.90 93.49 83.34 92.22 91.07 83.68 92.26 91.13 87.46 99.41 95.10
OE+ODIN+ReAct 84.16 99.37 94.37 82.92 93.11 92.34 64.00 96.13 91.17 73.90 73.62 79.22 75.45 74.52 80.24 71.65 95.31 92.09
OE+Energy+ReAct 94.41 99.49 94.41 91.36 98.76 94.98 73.88 97.79 93.41 80.03 91.77 90.80 81.16 91.94 90.92 86.19 99.43 95.09
7S6 Investigation on applying OE to the strong CLIP model
Pre-trained vision language models like CLIP ? have recently shown remarkable robustness to
distributionshifts. Thesuccessofthesemodelssuggeststhatpre-trainingondiverseandextensive
datasetsisapromisingapproachforenhancingrobustness. Byutilizingpre-trainedvisual-language
models such as CLIP, it is possible to extend OOD detection to various challenging ID datasets.
(a) Img-img (b) Txt-img (c) Avg
FigureS3: Theaverageperformancevs. distancetoYFCCofalldatasetswhenusedasleave-one-
out ID datasets in turn for the CLIP model. We compute cosine similarity with both image/text
embeddingsfromIDdataandimageembeddingsfromauxiliarydata. Noclearcorrelationbetween
the distance to YFCC and OOD detection performance is observed.
We finetune CLIP using OE with auxiliary data and apply MLS to the similarity scores to
separate ID and OOD samples. For auxiliary data, we use the subset of YFCC-100M ?, known
as YFCC-15M. It consists of the 15 million images which were used in CLIP pretraining. Specif-
ically, we consider the following ID datasets: iNaturalist, SUN397, Places, hard splits of SSB
according to ? (ImageNet-21K, CUB, SCars and FGVC), variants of ImageNet (ImageNet-R
and ImageNet-Sketch), variants of MNIST (MNIST, EMNIST, QMNIST and FashionMNIST),
Food101,STL10,OxfordPets,Flowers102,CIFAR-10,CIFAR-100,SVHN,Texture,LSUN,LSUN-
R,iSUN,Places365. AsfortheOODtestdatasets,wetreatonedatasetasthehold-outIDdataset
in turn to evaluate the performance of others OOD datasets that do not overlap with the ID
dataset. FigureS3examinesdifferentquerytypesofinputs(e.g., images, prompts). Weuseaset
ofpre-definedpromptsforeachclass,whicharecollectedfrompriorworks??. Wecomputecosine
similaritywiththeL -normalizedembeddingsofimagesortheembeddingofpromptsofeachclass
2
by averaging over the prompt pool. We present the OOD detection performance vs. OOD-AUX
distance in Figure S4 and Figure S5 by comparing different features. Interestingly, there is no
obvious trend shared among all the cases. This is likely due to the fact that CLIP is pretrained
on YFCC-15M, while in OOD detection, YFCC-15M is treated as auxiliary data to mimic OOD
datatobepushedawayfromtheIDdata,hurtingtheoriginalpretrainedstrongembeddingspace
of CLIP.
8ImageNet ImageNet-R iNaturalist SUN397 CIFAR-10 CIFAR-100
MNIST EMNIST QMNIST FashionMNIST Food101 STL10
OxfordIIITPet Flowers102 SVHN Texture LSUN LSUN-R
iSUN Places365 CUB (Hard) SCars (Hard) FGVC (Hard) ImageNet-21K (Hard)
Figure S4: OOD detection performance vs. OOD-AUX distance to YFCC, when taking each
dataset as ID dataset and others as OOD datasets for CLIP model. We calculate the distance
between image features of all OOD samples and those of YFCC-15M.
9ImageNet ImageNet-R iNaturalist SUN397 CIFAR-10 CIFAR-100
MNIST EMNIST QMNIST FashionMNIST Food101 STL10
OxfordIIITPet Flowers102 SVHN Texture LSUN LSUN-R
iSUN Places365 CUB (Hard) SCars (Hard) FGVC (Hard) ImageNet-21K (Hard)
Figure S5: OOD detection performance vs. OOD-AUX distance to YFCC, when taking each
dataset as ID dataset and others as OOD datasets for CLIP model. We calculate the distance
between image features of all OOD samples and prompt features of YFCC-15M.
10