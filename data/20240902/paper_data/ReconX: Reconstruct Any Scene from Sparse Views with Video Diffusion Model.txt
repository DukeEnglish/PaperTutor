RECONX: RECONSTRUCT ANY SCENE FROM SPARSE
VIEWS WITH VIDEO DIFFUSION MODEL
FangfuLiu∗1,WenqiangSun∗2,HanyangWang∗1,YikaiWang†1,HaowenSun1,
JunliangYe1,JunZhang2,YueqiDuan†1
1TsinghuaUniversity, 2HKUST
{liuff23,hanyang-21,sunhw24,yejl23}@mails.tsinghua.edu.cn,
wsunap@connect.ust.hk, yikaiw@outlook.com, eejzhang@ust.hk,
duanyueqi@tsinghua.edu.cn
ABSTRACT
Advancementsin3Dscenereconstructionhavetransformed2Dimagesfromthe
real world into 3D models, producing realistic 3D results from hundreds of in-
put photos. Despite great success in dense-view reconstruction scenarios, ren-
dering a detailed scene from insufficient captured views is still an ill-posed op-
timization problem, often resulting in artifacts and distortions in unseen areas.
In this paper, we propose ReconX, a novel 3D scene reconstruction paradigm
that reframes the ambiguous reconstruction challenge as a temporal generation
task. Thekeyinsightistounleashthestronggenerativeprioroflargepre-trained
video diffusion models for sparse-view reconstruction. However, 3D view con-
sistency struggles to be accurately preserved in directly generated video frames
frompre-trainedmodels. Toaddressthis,givenlimitedinputviews,theproposed
ReconXfirstconstructsaglobalpointcloudandencodesitintoacontextualspace
asthe3Dstructurecondition. Guidedbythecondition,thevideodiffusionmodel
then synthesizes video frames that are both detail-preserved and exhibit a high
degreeof3Dconsistency, ensuringthecoherenceofthescenefromvariousper-
spectives. Finally, we recover the 3D scene from the generated video through a
confidence-aware 3D Gaussian Splatting optimization scheme. Extensive exper-
iments on various real-world datasets show the superiority of our ReconX over
state-of-the-art methods in terms of quality and generalizability. Project Page:
https://liuff19.github.io/ReconX.
1 INTRODUCTION
WiththerapiddevelopmentofphotogrammetrytechniquessuchasNeRF(Mildenhalletal.,2020)
and 3D Gaussian Splatting (3DGS) (Kerbl et al., 2023), 3D reconstruction has become a popular
research topic in recent years, finding various applications from virtual reality (Dalal et al., 2024)
to autonomous navigation (Adamkiewicz et al., 2022) and beyond (Martin-Brualla et al., 2021b;
Liuetal.,2024a;Wuetal.,2024a;Charatanetal.,2024). However, sparse-viewreconstructionis
anill-posedproblem(Gaoetal.,2024;Yuetal.,2021)sinceitinvolvesrecoveringacomplex3D
structurefromlimitedviewpointinformation(i.e.,evenasfewastwoimages)thatmaycorrespond
tomultiplesolutions.Thisuncertainprocessrequiresadditionalassumptionsandconstraintstoyield
aviablesolution.
Recently, powered by the efficient and expressive 3DGS (Kerbl et al., 2023) with fast rendering
speed and high quality, several feed-forward Gaussian Splatting methods (Charatan et al., 2024;
Szymanowiczetal.,2024b;Chenetal.,2024a)havebeenproposedtoexplore3Dscenereconstruc-
tionfromsparseviewimages.Althoughtheycanachievepromisinginterpolationresultsbylearning
scene-priorknowledgefromfeatureextractionmodules(e.g.,epipolartransformer(Charatanetal.,
2024)),insufficientcapturesofthescenestillleadtoanill-posedoptimizationproblem(Wuetal.,
2024b). As a result, they often suffer from severe artifact and implausible imagery issues when
renderingthe3Dscenefromnovelviewpoints,especiallyinunseenareas.
∗EqualContribution †CorrespondingAuthors.
1
4202
guA
92
]VC.sc[
1v76761.8042:viXraReconX pixelSplat
Sparse Views PSNR:24.57 PSNR:17.62
MultiView
Optim.
MVSplat
PSNR:18.83
Video Gen.
Recon
Figure1: AnoverviewofourReconXframeworkforsparse-viewreconstruction. Unleashing
the strong generative prior of video diffusion models, we can create more observations for 3D re-
constructionandachieveimpressiveperformance.
To address the limitations, we propose ReconX, a novel 3D scene reconstruction paradigm that
reformulatestheinherentlyambiguousreconstructionproblemasagenerationproblem. Ourkeyin-
sightistounleashthestronggenerativepriorofpre-trainedlargevideodiffusionmodels(Blattmann
et al., 2023a,b; Xing et al., 2023) to create more observations for the downstream reconstruction
task. Despite the capability to synthesize video clips featuring plausible 3D structure (Gao et al.,
2024), recoveringahigh-quality3Dscenefromcurrentvideodiffusionmodelsisstillchallenging
duetothepoor3Dviewconsistencyacrossgenerated2Dframes. Groundedbytheoreticalanalysis,
we explore the potential of incorporating 3D structure guidance into the video generative process,
whichbridgesthegapbetweentheunder-determined3Dcreationproblemandthefully-observed3D
reconstructionsetting. Specifically,givensparseimages,wefirstbuildaglobalpointcloudthrough
apose-freestereoreconstructionmethod. Thenweencodeitintoarichcontextrepresentationspace
asthe3Dconditionincross-attentionlayers,whichguidesthevideodiffusionmodeltosynthesize
detail-preservedframeswith3Dconsistentnovelobservationsofthescene. Finally,wereconstruct
the3DscenefromthegeneratedvideothroughGaussianSplattingwitha3Dconfidence-awareand
robustsceneoptimizationscheme,whichfurtherdeblurstheuncertaintyinvideoframeseffectively.
ExtensiveexperimentsverifytheefficacyofourframeworkandshowthatReconXoutperformsex-
isting methods for high quality and generalizability, revealing the great potential to craft intricate
3Dworldsfromvideodiffusionmodels. Theoverviewandexamplesofreconstructionsareshown
inFigure1.
Insummary,ourmaincontributionsare:
• WeintroduceReconX,anovelsparse-view3Dscenereconstructionframeworkthatreframesthe
ambiguousreconstructionchallengeasatemporalgenerationtask.
• Weincorporatethe3Dstructureguidanceintotheconditionalspaceofthevideodiffusionmodel
to generate 3D consistent frames and propose a 3D confidence-aware optimization scheme in
3DGStoreconstructthescenegiventhegeneratedvideo.
• Extensive experiments demonstrate that our ReconX outperforms existing methods for high-
fidelityandgeneralizabilityonavarietyofreal-worlddatasets.
2 RELATED WORK
Sparse-view reconstruction. NeRF and 3DGS typically demand hundreds of input images and
rely on the multi-view stereo reconstruction (MVS) approaches (e.g., COLMAP (Scho¨nberger &
Frahm, 2016)) to estimate the camera parameters. To address the issue of low-quality 3D recon-
structioncausedbysparseviews,PixelNeRF(Yuetal.,2021)proposesusingconvolutionalneural
networkstoextractfeaturesfromtheinputcontext. Moreover,FreeNeRF(Yangetal.,2023)adopts
thefrequencyanddensityregularizedstrategiestoalleviatetheartifactscausedbyinsufficientinputs
withoutanyadditionalcost. Tomitigatetheoverfittingtoinputsparseviewsin3DGS,FSGS(Zhu
etal.,2023)andSparseGS(Xiongetal.,2023)employadepthestimatortoregularizetheoptimiza-
tionprocess. However, thesemethodsallrequireknowncameraintrinsicsandextrinsics, whichis
2notpracticalinreal-worldscenario. Benefitingfromtheexistingpowerful3Dreconstructionmodel
(i.e.,DUSt3R(Wangetal.,2024a)),InstantSplat(Fanetal.,2024)isabletoacquireaccuratecamera
parametersandinitial3Drepresentationsfromunposedsparse-viewinputs,leadingtotheefficient
andhigh-quality3Dreconstruction.
Regression model for generalizable view synthesis. While NeRF and 3DGS are optimized per-
scene,alineofresearchaimstotrainfeed-forwardmodelsthatoutputa3Drepresentationdirectly
fromafewinputimages,bypassingtheneedfortime-consumingoptimization.Splatterimage(Szy-
manowicz et al., 2024b) performs an efficient feed-forward manner for monocular 3D object re-
construction by predicting a 3D Gaussian for each image pixel. Meanwhile, pixelSplat (Charatan
et al., 2024) proposes predicting the scene-level 3DGS from the image pairs, using the epipolar
transformertobetterextractscenefeatures. Followingthat,MVsplat(Chenetal.,2024a)introduces
thecostvolumeanddepthrefinementstoproduceacleanandhigh-quality3DGaussiansinafaster
way. To reconstruct a complete scene from a single image, Flash3D (Szymanowicz et al., 2024a)
adoptsahierarchical3DGSlearningpolicyanddepthconstrainttoachievehigh-qualityinterpola-
tionandextrapolationviewsynthesis. Althoughthesemethodsleveragethe3Ddatapriors,theyare
limited by the scarcity and diversity of 3D data. Consequently, these methods struggle to acquire
high-qualityrenderingsinunseenareas,especiallywhenOODdataisusedasinput.
Generative models for 3D reconstruction. Constructing comprehensive 3D scenes from limited
observationsdemandsgenerating3Dcontent,particularlyforunseenareas.Earlierstudiesdistillthe
knowledgeinthepre-trainedtext-to-imagediffusionmodels(Rombachetal.,2022;Sahariaetal.,
2022; Ramesh et al., 2022) into a coherent 3D model. Specifically, the Score Distillation Sam-
pling (SDS) technique (Wu et al., 2024b; Lin et al., 2023; Liu et al., 2024c; Wang et al., 2024b)
isadoptedtosynthesizea3Dobjectfromthetextprompt. Toenhancethe3Dconsistency,several
approaches(Wuetal.,2024a;Shietal.,2023;Liuetal.,2023)injectthecamerainformationinto
diffusionmodels,providingstrongmulti-viewpriors. Furthermore,ZeroNVS(Sargentetal.,2023)
andCAT3D(Gaoetal.,2024)extendthemulti-viewdiffusiontothescenelevelgeneration. More
recently,videodiffusionmodels(Blattmannetal.,2023a;Xingetal.,2023)haveshownanimpres-
siveabilitytoproducerealisticvideosandarebelievedtoimplicitlyunderstand3Dstructures(Liu
et al., 2024b). SV3D (Voleti et al., 2024) and V3D (Chen et al., 2024b) explore fine-tuning the
pre-trained video diffusion model for 3D object generation. Meanwhile, MotionCtrl (Wang et al.,
2024c) and CameraCtrl (He et al., 2024) achieve scene-level controllable video generation from
asingleimagebyexplicitlyinjectingthecameraposeintovideodiffusionmodels. However, they
don’tworkfortheunconstrainedsparse-viewreconstruction,whichrequiresstrong3Dconsistency.
3 PRELIMINARIES
Video Diffusion Models. Diffusion models (Ho et al., 2020; Song et al., 2020) have emerged as
thecutting-edgeparadigmtogeneratehigh-qualityvideos. Thesemodelslearntheunderlyingdata
distributionbyaddingandremovingnoiseonthecleandata. Theforwardprocessaimstotransform
acleandatasamplex ∼p(x)toapureGaussiannoisex ∼N(0,I),followingtheprocess:
0 T
√ √
x = α¯ x + 1−α¯ ϵ, ϵ∼N(0,1), (1)
t t 0 t
wherex andα¯ denotesthenoisydataandnoisestrengthatthetimestept. Thedenoisingneural
t t
network ϵ is trained to predict the noises added in the forward process, which is achieved by the
θ
MSEloss:
(cid:104) (cid:105)
L=E ∥ϵ−ϵ (x ,t,c)∥2 , (2)
x∼p,ϵ∼N(0,I),c,t θ t 2
wherecrepresentstheembeddingsofconditionsliketextorimageprompt. Forthevideodiffusion
models,LatentDiffusionModels(LDMs)(Rombachetal.,2022),whichcompressimagesintothe
latent space, are commonly employed to mitigate the computation complexity while maintaining
competitiveperformance.
3DGaussianSplatting. 3DGS(Kerbletal.,2023)representsasceneexplicitlybyutilizingasetof
3DGaussianspheres,achievingafastandhigh-qualityrendering. A3DGaussianismodeledbya
positionvectorµ∈R3,acovariancematrixΣ∈R3×3,anopacityα∈R,andsphericalharmonics
(SH)coefficientc ∈ Rk (Ramamoorthi&Hanrahan,2001). Moreover,theGaussiandistributionis
formulatedasthefollowing:
G(x)=e− 21(x−µ)TΣ−1(x−µ), (3)
3whereΣ=RSSTRT,S denotesthescalingmatrixandRistherotationmatrix.
In the rendering stage, the 3D Gaussian spheres are transformed into 2D camera planes through
rasterization (Zwicker et al., 2001). Specifically, given the perspective transformation matrix W
andJacobinoftheprojectionmatrixJ,the2Dcovariancematrixinthecameraspaceiscomputed
as
Σ′ =JWΣWTJT. (4)
For every pixel, the Gaussians are traversed in depth order from the image plane, and their view-
dependentcolorsc arecombinedthroughalphacompositing,leadingtothepixelcolorC:
i
i−1
(cid:88) (cid:89)
C = c α (1−α ). (5)
i i i
i∈N j=1
End-to-endDenseUnconstrainedStereo. DUSt3R(Wangetal.,2024a)isanewmodeltopredict
adenseandaccurate3Dscenerepresentationsolelyfromimagepairswithoutanypriorinformation
aboutthescene. Giventwounposedimages{I ,I }, thisend-to-endmodelistrainedtoestimate
1 2
thepointmaps{P ,P }andconfidencemaps{C ,C },whichcanbeutilizedtorecoverthe
1,1 2,1 1,1 2,1
cameraparametersanddensepointcloud. Thetrainingprocedureforviewv ∈{1,2}isformulated
asaregressionloss:
(cid:13) (cid:13)
L=(cid:13) (cid:13) (cid:13)z1 ·P v,1− zˆ1 ·Pˆ v,1(cid:13) (cid:13) (cid:13), (6)
i i
whereP andPˆdenotetheground-truthandpredictionpointmaps,respectively. Thescalingfactors
z = norm(P ,P ) and zˆ=norm(Pˆ ,Pˆ ) are adopted to normalize the point maps, which
i 1,1 2,1 i 1,1 2,1
merelyindicatethemeandistanceDofallvalidpointsfromtheorigin:
norm(P 1,1,P 2,1)=
|D
|+1
|D |
(cid:88) (cid:88) (cid:13)
(cid:13)P
vi(cid:13)
(cid:13). (7)
1 2
v∈{1,2}i∈Dv
4 MOTIVATION FOR RECONX
In this paper, we focus on the fundamental problem of 3D scene reconstruction and novel view
synthesis (NVS) from very sparse view (e.g., as few as two) images. Most existing works (Chen
etal.,2024a;Yuetal.,2021;Charatanetal.,2024;Szymanowiczetal.,2024a)utilize3Dpriorand
geometricconstraints(e.g.,depth,normal,costvolume)tofillthegapbetweenobservedandnovel
regions in sparse-view 3D reconstruction. Although capable of producing highly realistic images
from the given viewpoints, these methods often struggle to generate high-quality images in areas
not visible from the input perspectives due to the inherent problem of insufficient viewpoints and
theresultinginstabilityinthereconstructionprocess.Toaddressthis,anaturalideaistocreatemore
observationstocollapsetheunder-determined3Dcreationproblemintoafullyconstrained3Dre-
constructionsetting. Recently,videogenerativemodelshaveshownpromiseforsynthesizingvideo
clipsfeaturing3Dstructure(Voletietal.,2024;Blattmannetal.,2023a;Xingetal.,2023). Thisin-
spiresustounleashthestronggenerativeprioroflargepre-trainedvideodiffusionmodelstocreate
temporal consistent video frames for sparse-view reconstruction. Nevertheless, it is non-trivial as
themainchallengeliesinpoor3Dviewconsistencyamongvideoframes,whichsignificantlylimits
thedownstream3DGStrainingprocess.Toachieve3Dconsistencywithinvideogeneration,wefirst
analyzethevideodiffusionmodelingfroma3Ddistributionalview. Letxbethesetofrendering
2D images from any 3D scene in the world, q(x) be the distribution of the rendering data x, our
goalistominimizethedivergenceD:
min D(q(x)∥p (x)), (8)
θ,ψ
θ∈Θ,ψ∈Ψ
where p be a diffusion model parameterized by θ ∈ Θ (the parameters in the backbone) and
θ,ψ
ψ ∈ Ψ(anyembeddingfunctionsharedbyalldata). Invanillavideodiffusionmodel(Xingetal.,
2023), they choose CLIP (Radford et al., 2021) model g to add an image-based condition (i.e.,
ψ =g).However,insparse-view3Dreconstruction,onlylimited2Dimagesastheconditioncannot
providesufficientguidanceforapproximatingq(x)(Charatanetal.,2024;Chenetal.,2024a;Wu
et al., 2024b). Motivated by this, we explore the potential of incorporating the native 3D prior
43D Structure Guidance Confidence-aware
3DGS Optimization
Image Cross-Attn ⊕ ⊕ ⊕
3D Structure frame confidence map
Cross-Attn
Point Cloud downsample
extraction
Consistent Frames
CLIP
...?...
... ...
Input Sparse Views
Video Diffusion U-Net
Figure2:PipelineofReconX.Givensparse-viewimagesasinput,wefirstbuildaglobalpointcloud
andprojectitinto3Dcontextrepresentationspaceas3Dstructureguidance. Thenweinjectthe3D
structure guidance into the video diffusion process and guide it to generate 3D consistent video
frames. Finally,wereconstructthe3DscenefromthegeneratedvideothroughGaussianSplatting
with a 3D confidence-aware and robust scene optimization scheme. In this way, we unleash the
strongpowerofthevideodiffusionmodeltoreconstructintricate3Dscenesfromverysparseviews.
(denotedbyF)tofindamoreoptimalsolutioninEquation8andderiveatheoreticalformulation
forouranalysisinProposition1.
Proposition1. Letθ∗,ψ∗ =g∗betheoptimalsolutionofthesolelyimage-basedconditionaldiffu-
sionschemeandθ˜∗,ψ˜∗ ={g∗,F∗}betheoptimalsolutionofdiffusionschemewithnative3Dprior.
SupposethedivergenceD isconvexandtheembeddingfunctionspaceΨincludesallmeasurable
functions,wehaveD(q(x)∥p (x))<D(q(x)∥p (x)). (ProofinourAppendix)
θ˜∗,ψ˜∗ θ∗,ψ∗
Towardsthisend,wereformulatetheinherentlyambiguousreconstructionproblemasageneration
problembyincorporating3Dnativestructureguidanceintothediffusionprocess.
5 METHOD
5.1 OVERVIEWOFRECONX
Given K sparse-view (i.e., as few as two) images I = (cid:8) Ii(cid:9)K ,(cid:0) Ii ∈RH×W×3(cid:1) , our goal is to
i=1
reconstruct the underlying 3D scene, where we can synthesize novel views of unseen viewpoints.
In our framework ReconX, we first build a global point cloud P = {p ,1≤i≤N} ∈ RN×3
i
from I and project P into the 3D context representation space F as the structure guidance F(P)
(Sec. 5.2). Then we inject F(P) into the video diffusion process to generate 3D consistent video
frames I′ = (cid:8) Ii(cid:9)K′ ,(K′ > K), thus creating more observations (Sec. 5.3). To alleviate the
i=1
negative artifacts caused by the inconsistency among generated videos, we utilize the confidence
mapsC ={C
}K′
fromtheDUSt3RmodelandLPIPSloss(Zhangetal.,2018a)toachievearobust
i i=1
3Dreconstruction(Sec.5.4).Inthisway,wecanunleashthefullpowerofthevideodiffusionmodel
toreconstructintricate3Dscenesfromverysparseviews. OurpipelineisdepictedinFigure2.
5.2 BUILDINGTHE3DSTRUCTUREGUIDANCE
GroundedbythetheoreticalanalysisinSec.4,weleverageanunconstrainedstereo3Dreconstruc-
tionmethodDUSt3R(Wangetal.,2024a)withpoint-basedrepresentationstobuildthe3Dstructure
guidance F. Given a set of sparse images I =
(cid:8) Ii(cid:9)K
, we first construct a connectivity graph
i=1
G(V,E)ofK inputviewssimilartoDUSt3R,whereverticesV andeachedgee=(n,m)∈E indi-
catesthattheimagesInandImsharesvisualcontents. ThenweuseGtorecoveragloballyaligned
pointcloudP.Foreachimagepaire=(n,m),wepredictpairwisepointmapsPn,n,Pm,nandtheir
correspondingconfidencemapsCn,n,Cm,n ∈RH×W×3. Forclarity,let’sdenotePn,e :=Pn,nand
Pm,e := Pm,n. Sinceweaimtorotateallpairwisepredictionsintoasharedcoordinateframe,we
5
cnE
noisuffiD
noitazilamroN
duolCtnioP reddebmE ssorC noitnettA NFF nttA-fleS
ceD
NFFintroducetransformationmatrixT andscalingfactorσ associatedwitheachpaire∈E tooptimize
e e
globalpointcloudP as:
HW
(cid:88)(cid:88)(cid:88)
P∗ =argmin Cv,e∥Pv−σ T Pv,e∥. (9)
i i e e i
P,T,σ
e∈Ev∈e i=1
MoredetailsofthepointcloudextractioncanbefoundinWangetal.(2024a). Havingalignedthe
pointcloudsP,wenowprojectitintoa3DcontextrepresentationspaceF throughatransformer-
basedencoderforbetterinteractionwithlatentfeaturesofthevideodiffusionmodel. Specifically,
we embed the input point cloud P into a latent code using a learnable embedding function and a
cross-attentionencodingmodule:
(cid:16) (cid:17)
F(P)=FFN CrossAttn(PosEmb(P˜),PosEmb(P)) , (10)
whereP˜ isadown-sampledversionofP at1/8scaletoefficientlydistillinputpointstoacompact
3Dcontextspace. Finally,wegetthe3DstructureguidanceF(P)whichcontainssparsestructural
informationofthe3DscenethatcanbeinterpretedbythedenoisingU-Net.
5.3 3DCONSISTENTVIDEOFRAMESGENERATION
Inthissubsection,weincorporatethe3DstructureguidanceF(P)intothevideodiffusionprocess
toobtain3Dconsistentframes. Toachieveconsistencybetweengeneratedframesandhigh-fidelity
rendering views of the scene, we utilize the video interpolation capability to recover more unseen
observations, where the first frame and the last frame of input to the video diffusion model are
two reference views. Specifically, given sparse-view images I = (cid:8) Ii (cid:9)K as input, we aim to
ref i=1
render consistent frames f(Ii−1,Ii ) = {Ii−1,I ,...,I ,Ii } ∈ R(T+2)×3×H×W where T is
ref ref ref 2 T ref
the number of generated novel frames. To unify the notation, we denote the embedding of image
conditioninthepretrainedvideodiffusionmodelasF =g(I )andtheembeddingof3Dstructure
g ref
guidanceasF =F(P). Subsequently,weinjectthe3Dguidanceintothevideodiffusionprocess
F
byinteractingwiththeU-NetintermediatefeatureF throughthecross-attentionofspatiallayers:
in
QKT QKT
F =Softmax( √ g )V +λ ·Softmax( √ F)V , (11)
out g F F
d d
whereQ = F W ,K = F W ,V = F W ,K = F W′ ,V = F W′ arethequery,key,
in Q g g K g g V F F K F F V
andvalueof2Dand3Dembeddingsrespectively. W ,W ,W′ ,W ,W′ aretheprojectionma-
Q K K V V
tricesandλ denotesthecoefficientthatbalancesimage-conditionedand3Dstructure-conditioned
F
features. Giventhefirstandlasttwoviewsconditionc fromF and3Dstructureconditionc
view g struc
from F , we apply the classifier-free guidance (Ho & Salimans, 2022) strategy to incorporate the
F
conditionandourtrainingobjectiveis:
(cid:104) (cid:105)
L =E ∥ϵ−ϵ (x ,t,c ,c )∥2 , (12)
diffusion x∼p,ϵ∼N(0,I),cview,cstruc,t θ t view struc 2
wherex isthenoiselatentfromtheground-truthviewsofthetrainingdata.
t
5.4 CONFIDENCE-AWARE3DGSOPTIMIZATION.
Built upon the well-designed 3D structure guidance, our video diffusion model generates highly
consistent video frames, which can be used to reconstruct the 3D scene. As conventional 3D re-
construction methods are originally designed to handle real-captured photographs with calibrated
camerametrics,directlyapplyingtheseapproachestothesegeneratedvideosarenotperfecttore-
cover the coherent scene due to the uncertainty of unconstrained images (Wang et al., 2024a; Fan
et al., 2024). To alleviate the uncertainty issue, we adopt a confidence-aware 3DGS mechanism
to reconstruct the intricate scene. Different from recent approaches (Martin-Brualla et al., 2021a;
Renetal.,2024)whichmodeltheuncertaintyinper-image,weinsteadfocusonaglobalalignment
amongaseriesofframes.Forthegeneratedframes{I }K′ ,wedenoteCˆ andC theper-pixelcolor
i i=1 i i
valueforgeneratedandground-truthviewi. Then,wemodelthepixelvaluesasaGaussiandistri-
bution in our 3DGS, where the mean and variance of I are C and σ . The variance σ measures
i i i i
6RealEstate10K ACID
Method
PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓
pixelNeRF(Yuetal.,2021) 20.43 0.589 0.550 20.97 0.547 0.533
GPNR(Suhailetal.,2022) 24.11 0.793 0.255 25.28 0.764 0.332
AttnRend(Duetal.,2023) 24.78 0.820 0.213 26.88 0.799 0.218
MuRF(Xuetal.,2024) 26.10 0.858 0.143 28.09 0.841 0.155
pixelSplat(Charatanetal.,2024) 25.89 0.858 0.142 28.14 0.839 0.150
MVSplat(Chenetal.,2024a) 26.39 0.839 0.128 28.25 0.843 0.144
ReconX(Ours) 28.31 0.912 0.088 28.84 0.891 0.101
Table1: Quantitativecomparisonsforsmallanglevarianceininputviews. Foreachscene,the
model takes two views as input and renders three novel views for evaluation. We outperform all
baseline methods in terms of PSNR, LPIPS, and SSIM for novel view synthesis on the real-world
RealEstate10K(Zhouetal.,2018)andACID(Liuetal.,2021)datasets.
thediscrepancybetweenthegeneratedviewandtherealview. Ourtargetistominizethefollowing
negativelog-likelihoodamongallframes:
(cid:32) (cid:32) (cid:33)(cid:33)
1 ∥Cˆ′ −C ∥2
L =−log exp − i i 2 . (13)
Ii (cid:112) 2πσ2 2σ2
i
whereCˆ′ =A(Cˆ ,{Cˆ }K′ \Cˆ )andAistheglobalalignmentfunction. Throughempiricalstudy,
i i i i=1 i
wefindawell-alignedmappingfunctionAfromthetransformerdecoderofDUSt3R,whichbuilds
theconfidencemaps{C
}K′
foreachgeneratedframes{I
}K′
. Specifically,theconfidencescore
i i=1 i i=1
tendstobelowerinareasthataredifficulttoestimate(e.g.,regionswithsolidcolors)whilethescore
willbehigherinareaswithlessuncertainty. Moreover, weintroduceLPIPS(Zhangetal.,2018b)
losstoremovetheartifactsandfurtherenhancethevisualquality. Towardsthisend,weformulate
theconfidence-aware3DGSlossas:
K′
L =(cid:88) C (cid:16) λ L (Iˆ,I )+λ L (Iˆ,I )+λ L (Iˆ,I )(cid:17) . (14)
conf i rgb 1 i i ssim ssim i i lpips lpips i i
i=1
whereL ,L ,andL denotetheL ,SSIM,andLPIPSloss,respectively,withλ ,λ ,and
1 ssim lpips 1 rgb ssim
λ beingtheircorrespondingcoefficientparameters. Incomparisontothephotometricloss(e.g.,
lpips
L andL ),LPIPSlossismainlyfocusedonthehigh-levelsemanticinformation.
1 ssim
6 EXPERIMENTS
Inthissection,weconductextensiveexperimentstoevaluateoursparse-viewreconstructionframe-
workReconX.Wefirstpresentthesetupoftheexperiment(Sec6.1). Thenwereportourqualitative
and quantitative results compared to representative baseline methods in various settings such as
different angle variances from input views and cross-dataset generalization (Sec 6.2). Finally, we
conduct ablation studies to further verify the efficacy of our framework design (Sec 6.3). Please
refertoourprojectpageformorecomparisonsandvisualizations.
6.1 EXPERIMENTSETUP
Implementation Details. In our framework, we choose DUSt3R (Wang et al., 2024a) as our un-
constrained stereo 3D reconstruction backbone and I2V model DynamiCrafter (Xing et al., 2023)
(@ 512 × 512 resolution) as the video diffusion backbone. We first finetune the image cross-
attention layers with 2000 steps on the learning rate 1×10−4 for warm-up. Then we incorporate
the3Dstructureconditionc intothevideodiffusionmodelandfurtherfinetunethespatiallayers
struc
with 30K steps on the learning rate of 1 × 10−5. Our video diffusion was trained on 3D scene
datasets by sampling 32 frames with dynamic FPS at the resolution of 512×512 in a batch. The
AdamW(Loshchilov&Hutter,2017)optimizerisemployedforoptimization. Attheinferenceof
ourvideodiffusion,weadoptDDIMsampler(Songetal.,2022)usingmulti-conditionclassifierfree
guidance (Ho & Salimans, 2022). Similar to Xing et al. (2023), we adopt tanh gating to learn λ
F
7Input Views Ground Truth ReconX (Ours) MVSplat pixelSplat
Figure3:Qualitativecomparisonforsmallanglevarianceininputviews. Thefirstfourrowsare
fromRealEstate10K(Zhouetal.,2018),whilethelastoneisfromACID(Liuetal.,2021). Given
two nearby input sparse views, our approach produces more accurate and perceptually appealing
imagesintestscenesthanthebaselines.
ACID RealEstate10K
Method
PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓
pixelSplat(Charatanetal.,2024) 16.83 0.476 0.494 19.62 0.730 0.270
MVSplat(Chenetal.,2024a) 16.49 0.466 0.486 19.97 0.732 0.245
ReconX(Ours) 24.53 0.847 0.083 23.70 0.867 0.143
Table 2: Quantitative comparison for large angle variance in input views. Given input views
withlargeanglevariance,ourresultshavelargerimprovementsthanSOTAmethods.
adaptively. Thetrainingisconductedon8NVIDIAA800(80G)GPUsintwodays. In3DGSopti-
mizationstage,wechoosethepointmapsoffirstandendframesastheinitialglobalpointcloudand
all32generatedframesareusedtoreconstructthescene. Ourimplementationfollowsthepipeline
oftheoriginal3DGS(Kerbletal.,2023),butunlikethismethod,weomittheadaptivecontrolpro-
cessandattainhigh-qualityrenderingsinjust1000steps. Thecoefficientsλ ,λ ,andλ are
rgb ssim lpips
setto0.8,0.2,and0.5,respectively.
Datasets. ThevideodiffusionmodelofReconXistrainedonthreedatasets: RealEstate-10K(Zhou
etal.,2018),ACID(Liuetal.,2021),andDL3DV-10K(Lingetal.,2023)basedonthepretrained
model. RealEstate-10KisadatasetdownloadedfromYouTube,whichissplitinto67,477training
scenesand7,289testscenes. TheACIDdatasetconsistsofnaturallandscapescenes, with11,075
training scenes and 1,972 testing scenes. DL3DV-10K is a large-scale outdoor dataset containing
8Input Views Ground Truth ReconX (Ours) MVSplat pixelSplat
ACID: scene 80ef892d52be7fe9, frame 0 to 600
RealEstate10K: scene 00e8df74b6805da7, frame 0 to 115
RealEstate10K: scene 0c1012a308ee2788, frame 0 to 123
Figure 4: Qualitative comparison for large angle variance in input views. Given more chal-
lengingsettings(i.e.,inputviewsarefarawayfromeachotherastheframeIDshows),ourmethod
ReconXdemonstratesmoreimprovementsandsuperioritythanbaselinemethods.
10,510 videos with consistent capture standards. For each scene video, we randomly sample 32
contiguousframeswithrandomskipsandservethefirstandlastframesastheinputforourvideo
diffusion model. To further validate our cross-data generalizability, we also directly evaluate our
method on DTU (Jensen et al., 2014) dataset and NeRF-LLFF (Mildenhall et al., 2019) dataset
whicharetwopopularmulti-viewdatasetsinNVStask.
Baselines and Metrics. We compare our ReconX with original 3DGS (Kerbl et al., 2023) and
several representative baselines that focus on sparse-view 3D reconstruction, including: NeRF-
based pixelNeRF (Yu et al., 2021) and MuRF (Xu et al., 2024); Light Field based GPNR (Suhail
et al., 2022) and AttnRend (Du et al., 2023); and the recent state-of-the-art 3DGS-based pixel-
Splat(Charatanetal.,2024)andMVSplat(Chenetal.,2024a). Forquantitativeresults,wereport
the standard metrics in NVS, including PSNR, SSIM (Wang et al., 2004), LPIPS (Zhang et al.,
2018b). Forafaircomparison,weconductexperimentson256×256resolutionsfollowingexisting
methods(Charatanetal.,2024;Chenetal.,2024a).
6.2 COMPARISONWITHBASELINES
Comparisonforsmallanglevarianceininputviews. Forfaircomparisonwithbaselinemethods
likeMuNeRF(Xuetal.,2024),pixelSplat(Charatanetal.,2024),andMVSplat(Chenetal.,2024a),
wefirstcompareourreconXwithbaselinemethodfromsparseviewswithsmallanglevariance(see
Table1andFigure3). WeobservethatourReconXsurpassesallpreviousstate-of-the-artmodelsin
termsofallmetricsonvisualqualityandqualitativeperception.
Comparisonforlargeanglevarianceininputviews. AsMVSplatandpixelSplataremuchbetter
thanpreviousbaselines,weconductthoroughcomparisonswiththeminmoredifficultsettings. In
morechallengingsettings(i.e.,givensparseviewswithlargeanglevariance),ourproposedReconX
demonstrate more significant improvement than baselines, especially in unseen and generalized
viewpoints (see Table 2 and Figure 4). This clearly shows the effectiveness of ReconX in creat-
ingmoreconsistentobservationsfromvideodiffusiontomitigatetheinherentill-posedsparse-view
reconstructionproblem.
Cross-dataset generalization. Unleashing the strong generative power of the video diffusion
modelthrough3Dstructureguidance,ourReconXisinherentlysuperioringeneralizingtoout-of-
distributionnovelscenes. Todemonstrateourstronggeneralizability,weconducttwocross-dataset
9Input Views Ground Truth ReconX (Ours) MVSplat pixelSplat
DTU: Scan6, frame 0 to 48
DTU: Scan14, frame 0 to 48
NeRF-LLFF: Flower, frame 0 to 33
Figure5:Qualitativeresultsincross-datasetgeneralization.Modelstrainedonthesourcedataset
RealEstate10KaretestedonNeRF-LLFFandDTUdatasets.
evaluations. For a fair comparison, we train the models solely on the RealEstate10K and directly
testthemontwopopularNVSdatasets(i.e.,NeRF-LLFF(Mildenhalletal.,2019)andDTU(Jensen
etal.,2014)). AsshowninTable3andFigure5,thecompetitivebaselinemethodsMVSplat(Chen
etal.,2024a)andpixelSplat(Charatanetal.,2024)failtorendersuchOODdatasetswhichcontain
differentcameradistributionsandimageappearance,leadingtodramaticperformancedegradation.
Incontrast, ourReconXshowsimpressivegeneralizabilityandthegainislargerwhenthedomain
gapfromtrainingandtestdatabecomeslarger.
Assessing more-view quality. Our framework ReconX is agnostic to the number of input views.
Specifically,givenN viewsasinput,wesampleaplausiblecameratrajectorytorenderimagepairs
usingourvideodiffusionmodelsandfinallyoptimizethe3Dscenefromallgeneratedframes. Fora
faircomparisonwithChenetal.(2024a),weverifythisbytestingonDTUwiththreecontextviews.
Our results are PSNR: 22.83, SSIM: 0.512, LPIPS: 0.317, MVSplat’s are PSNR: 14.30, SSIM:
0.508,LPIPS:0.371,andpixelSplat’sarePSNR:12.52,SSIM:0.367,LPIPS:0.585. Comparedto
the two-view results (Table 3), our ReconX and MVSplat both achieve better performance given
more input views while we are much better than MVSplat. However, pixelSplat performs worse
when using more views also shown in Chen et al. (2024a). More visualization of our results with
moreinputviewscanbefoundinourprojectpage.
6.3 ABLATIONSTUDYANDANALYSIS
We carry out ablation studies on RealEstate10K to analyze the design of our ReconX framework
in Table 4 and Figure 6. A naive combination of pretrained video diffusion model and Gaussian
Splattingisregardedasthe“base”.Specifically,weablationonthefollowingaspectsofourmethod:
3Dstructureguidance,confidence-awareoptimization,andLPIPSloss. Theresultsindicatethatthe
omissionofanyoftheseelementsleadstoadegradationintermsofqualityandconsistency.Notably,
thebasiccombinationoforiginalvideodiffusionmodeland3DGSleadstosignificantdistortionof
thescene. Theabsenceof3Dstructureguidancecausesinconsistentgeneratedframesespeciallyin
distantinputviews,resultinginblurandartifactissues. Thelackofconfidence-awareoptimization
leadstosuboptimalresultsinsomelocaldetailareas.AddingLPIPSlossinconfidence-aware3DGS
optimizationwouldprovideclearerrenderingviews. Thisillustratestheeffectivenessofouroverall
framework (Figure 2), which drives generalizable and high-fidelity 3D reconstruction given only
sparseviewsasinput.
10Input Views Ground Truth ReconX (Ours) base w/o 3D structure guidance w/o confidence-aware opt. w/o LPIPS loss
Figure 6: Visualization results of ablation study. We ablate the design choices of 3D structure
guidance,confidence-awareoptimization,andtheLPIPSloss.
NeRF-LLFF DTU
Trainingdata Method
PSNR↑ SSIM↑ LPIPS↓ PSNR↑ SSIM↑ LPIPS↓
pixelSplat(Charatanetal.,2024) 11.42 0.312 0.611 12.89 0.382 0.560
RealEstate10K(Zhouetal.,2018) MVSplat(Chenetal.,2024a) 11.60 0.353 0.425 13.94 0.473 0.385
ReconX(Ours) 21.05 0.768 0.178 19.78 0.476 0.378
Table3:Quantitativeresultsincross-datasetgeneralization.Modelstrainedonthesourcedataset
RealEstate10K are tested on unseen scenes from target datasets NeRF-LLFF (Mildenhall et al.,
2019)andDTU(Jensenetal.,2014),withoutanyfine-tuning.
7 CONCLUSION
In this paper, we introduce ReconX, Setup PSNR↑ SSIM↑ LPIPS↓
a novel sparse-view 3D reconstruc-
ReconX(Ours) 28.31 0.912 0.088
tion framework that reformulates the
inherently ambiguous reconstruction base 19.70 0.789 0.229
problem as a generation problem. w/o3Dstructureguidance 25.13 0.901 0.131
Thekeytooursuccessisthatweun- w/oconfidence-awareopt. 26.83 0.897 0.097
leash the strong prior of video diffu- w/oLPIPSloss 27.47 0.906 0.111
sion models to create more plausible
observations frames for sparse-view
reconstruction. Grounded by empiri- Table 4: Quantitative results of ablation study. We re-
calstudyandtheoreticalanalysis,we portthequantitativemetricsinablationsofourframework
propose to incorporate 3D structure inreal-worlddataRealEstate10K(Zhouetal.,2018).
guidanceintothevideodiffusionpro-
cessforbetter3Dconsistentvideoframesgeneration. What’smore,weproposea3Dconfidence-
aware scheme to optimize the final 3DGS from generated frames, which effectively address the
uncertaintyissue. ExtensiveexperimentsdemonstratethesuperiorityofourReconXthelateststate-
of-the-artmethodsintermsofhighqualityandstronggeneralizabilityinunseendata.
LimitationsandFutureWork. AlthoughourReconXachievesremarkablereconstructionresults
in novel viewpoints, the quality still seems to be limited to the backbone itself as we choose the
U-NetbaseddiffusionmodelDynamiCrafter(Xingetal.,2023)inthiswork. Weexpectthemtobe
solvedwithopen-sourcedlargervideodiffusionmodels(e.g.,DiT-basedframework). Inthefuture,
itisinterestingtointegrate3DGSoptimizationdirectlywiththevideogenerationmodel, enabling
more efficient end-to-end 3D scene reconstruction. We are also interested in exploring consistent
4Dscenereconstruction. WebelievethatReconXprovidesapromisingresearchdirectiontocraft
intricate3Dworldsfromvideodiffusionmodelsandwillinspiremoreworksinthefuture.
REFERENCES
MichalAdamkiewicz,TimothyChen,AdamCaccavale,RachelGardner,PrestonCulbertson,Jean-
netteBohg,andMacSchwager. Vision-onlyrobotnavigationinaneuralradianceworld. IEEE
RoboticsandAutomationLetters,7(2):4606–4613,2022.
11Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik
Lorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal.Stablevideodiffusion:Scaling
latentvideodiffusionmodelstolargedatasets. arXivpreprintarXiv:2311.15127,2023a.
AndreasBlattmann, RobinRombach, HuanLing, TimDockhorn, SeungWookKim, SanjaFidler,
andKarstenKreis. Alignyourlatents:High-resolutionvideosynthesiswithlatentdiffusionmod-
els. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pp.22563–22575,2023b.
DavidCharatan,SizheLesterLi,AndreaTagliasacchi,andVincentSitzmann. pixelsplat: 3dgaus-
siansplatsfromimagepairsforscalablegeneralizable3dreconstruction. InProceedingsofthe
IEEE/CVFConferenceonComputerVisionandPatternRecognition,pp.19457–19467,2024.
YuedongChen,HaofeiXu,ChuanxiaZheng,BohanZhuang,MarcPollefeys,AndreasGeiger,Tat-
Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view
images. arXivpreprintarXiv:2403.14627,2024a.
Zilong Chen, Yikai Wang, Feng Wang, Zhengyi Wang, and Huaping Liu. V3d: Video diffusion
modelsareeffective3dgenerators. arXivpreprintarXiv:2403.06738,2024b.
AnuragDalal, DanielHagen, KjellGRobbersmyr, andKristianMuriKnausga˚rd. Gaussiansplat-
ting: 3dreconstructionandnovelviewsynthesis,areview. IEEEAccess,2024.
Yilun Du, Cameron Smith, Ayush Tewari, and Vincent Sitzmann. Learning to render novel views
fromwide-baselinestereopairs. InCVPR,2023.
Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu,
Boris Ivanovic, Marco Pavone, Georgios Pavlakos, et al. Instantsplat: Unbounded sparse-view
pose-freegaussiansplattingin40seconds. arXivpreprintarXiv:2403.20309,2024.
RuiqiGao, AleksanderHolynski, PhilippHenzler, ArthurBrussee, RicardoMartin-Brualla, Pratul
Srinivasan, Jonathan T Barron, and Ben Poole. Cat3d: Create anything in 3d with multi-view
diffusionmodels. arXivpreprintarXiv:2405.10314,2024.
Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan
Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint
arXiv:2404.02101,2024.
Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint
arXiv:2207.12598,2022.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
neuralinformationprocessingsystems,33:6840–6851,2020.
RasmusJensen,AndersDahl,GeorgeVogiatzis,EnginTola,andHenrikAanæs. Largescalemulti-
viewstereopsisevaluation.InProceedingsoftheIEEEconferenceoncomputervisionandpattern
recognition,pp.406–413,2014.
BernhardKerbl,GeorgiosKopanas,ThomasLeimku¨hler,andGeorgeDrettakis. 3dgaussiansplat-
ting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023.
URLhttps://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/.
Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten
Kreis,SanjaFidler,Ming-YuLiu,andTsung-YiLin. Magic3d: High-resolutiontext-to-3dcon-
tent creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.300–309,2023.
Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo,
Zixun Yu, Yawen Lu, Xuanmao Li, Xingpeng Sun, Rohan Ashok, Aniruddha Mukherjee, Hao
Kang, Xiangrui Kong, Gang Hua, Tianyi Zhang, Bedrich Benes, and Aniket Bera. Dl3dv-10k:
Alarge-scalescenedatasetfordeeplearning-based3dvision, 2023. URLhttps://arxiv.
org/abs/2312.16256.
12Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo
Kanazawa. Infinite nature: Perpetual view generation of natural scenes from a single image.
InICCV,2021.
Fangfu Liu, Hanyang Wang, Weiliang Chen, Haowen Sun, and Yueqi Duan. Make-your-3d: Fast
andconsistentsubject-driven3dcontentgeneration. arXivpreprintarXiv:2403.09625,2024a.
Fangfu Liu, Hanyang Wang, Shunyu Yao, Shengjun Zhang, Jie Zhou, and Yueqi Duan.
Physics3d: Learning physical properties of 3d gaussians via video diffusion. arXiv preprint
arXiv:2406.04338,2024b.
Fangfu Liu, Diankun Wu, Yi Wei, Yongming Rao, and Yueqi Duan. Sherpa3d: Boosting high-
fidelitytext-to-3dgenerationviacoarse3dprior. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pp.20763–20774,2024c.
YuanLiu,ChengLin,ZijiaoZeng,XiaoxiaoLong,LingjieLiu,TakuKomura,andWenpingWang.
Syncdreamer: Generatingmultiview-consistentimagesfromasingle-viewimage. arXivpreprint
arXiv:2309.03453,2023.
Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017.
RicardoMartin-Brualla,NohaRadwan,MehdiSMSajjadi,JonathanTBarron,AlexeyDosovitskiy,
andDanielDuckworth. Nerfinthewild: Neuralradiancefieldsforunconstrainedphotocollec-
tions. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
pp.7210–7219,2021a.
RicardoMartin-Brualla,NohaRadwan,MehdiSMSajjadi,JonathanTBarron,AlexeyDosovitskiy,
andDanielDuckworth. Nerfinthewild: Neuralradiancefieldsforunconstrainedphotocollec-
tions. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
pp.7210–7219,2021b.
Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ra-
mamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with
prescriptivesamplingguidelines,2019. URLhttps://arxiv.org/abs/1905.00889.
BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRamamoorthi,and
Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In European
ConferenceonComputerVision,pp.405–421.Springer,2020.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual
modelsfromnaturallanguagesupervision. InInternationalconferenceonmachinelearning,pp.
8748–8763.PMLR,2021.
RaviRamamoorthiandPatHanrahan. Anefficientrepresentationforirradianceenvironmentmaps.
InProceedingsofthe28thannualconferenceonComputergraphicsandinteractivetechniques,
pp.497–500,2001.
Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-
conditionalimagegenerationwithcliplatents. arXivpreprintarXiv:2204.06125,1(2):3,2022.
WeiningRen,ZihanZhu,BoyangSun,JiaqiChen,MarcPollefeys,andSongyouPeng.Nerfon-the-
go: Exploitinguncertaintyfordistractor-freenerfsinthewild. InProceedingsoftheIEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pp.8931–8940,2024.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconfer-
enceoncomputervisionandpatternrecognition,pp.10684–10695,2022.
Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar
Ghasemipour,RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,etal. Photorealistic
text-to-imagediffusionmodelswithdeeplanguageunderstanding. Advancesinneuralinforma-
tionprocessingsystems,35:36479–36494,2022.
13Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang,
Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, et al. Zeronvs: Zero-shot 360-degree
viewsynthesisfromasinglerealimage. arXivpreprintarXiv:2310.17994,2023.
JohannesLutzScho¨nbergerandJan-MichaelFrahm. Structure-from-MotionRevisited. InConfer-
enceonComputerVisionandPatternRecognition(CVPR),2016.
YichunShi,PengWang,JianglongYe,MaiLong,KejieLi,andXiaoYang. Mvdream: Multi-view
diffusionfor3dgeneration. arXivpreprintarXiv:2308.16512,2023.
JiamingSong,ChenlinMeng,andStefanoErmon. Denoisingdiffusionimplicitmodels,2022. URL
https://arxiv.org/abs/2010.02502.
YangSong,JaschaSohl-Dickstein,DiederikPKingma,AbhishekKumar,StefanoErmon,andBen
Poole. Score-basedgenerativemodelingthroughstochasticdifferentialequations. arXivpreprint
arXiv:2011.13456,2020.
MohammedSuhail,CarlosEsteves,LeonidSigal,andAmeeshMakadia.Generalizablepatch-based
neuralrendering. InECCV,2022.
StanislawSzymanowicz,EldarInsafutdinov,ChuanxiaZheng,DylanCampbell,Joa˜oFHenriques,
ChristianRupprecht,andAndreaVedaldi. Flash3d: Feed-forwardgeneralisable3dscenerecon-
structionfromasingleimage. arXivpreprintarXiv:2406.04343,2024a.
Stanislaw Szymanowicz, Chrisitian Rupprecht, and Andrea Vedaldi. Splatter image: Ultra-fast
single-view 3d reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vi-
sionandPatternRecognition,pp.10208–10217,2024b.
VikramVoleti,Chun-HanYao,MarkBoss,AdamLetts,DavidPankratz,DmitryTochilkin,Chris-
tian Laforte, Robin Rombach, and Varun Jampani. Sv3d: Novel multi-view synthesis and 3d
generation from a single image using latent video diffusion. arXiv preprint arXiv:2403.12008,
2024.
ShuzheWang,VincentLeroy,YohannCabon,BorisChidlovskii,andJeromeRevaud. Dust3r: Ge-
ometric3dvisionmadeeasy. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pp.20697–20709,2024a.
Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Pro-
lificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation.
AdvancesinNeuralInformationProcessingSystems,36,2024b.
Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment:
fromerrorvisibilitytostructuralsimilarity. IEEEtransactionsonimageprocessing,13(4):600–
612,2004.
Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo,
and Ying Shan. Motionctrl: A unified and flexible motion controller for video generation. In
ACMSIGGRAPH2024ConferencePapers,pp.1–11,2024c.
Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and
Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from a single image.
arXivpreprintarXiv:2405.20343,2024a.
RundiWu, BenMildenhall, PhilippHenzler, KeunhongPark, RuiqiGao, DanielWatson, PratulP
Srinivasan,DorVerbin,JonathanTBarron,BenPoole,etal.Reconfusion:3dreconstructionwith
diffusion priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pp.21551–21561,2024b.
Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying
Shan.Dynamicrafter:Animatingopen-domainimageswithvideodiffusionpriors.arXivpreprint
arXiv:2310.12190,2023.
14Haolin Xiong, Sairisheek Muttukuru, Rishi Upadhyay, Pradyumna Chari, and Achuta Kadambi.
Sparsegs: Real-time 360 {\deg} sparse view synthesis using gaussian splatting. arXiv preprint
arXiv:2312.00206,2023.
HaofeiXu,AnpeiChen,YuedongChen,ChristosSakaridis,YulunZhang,MarcPollefeys,Andreas
Geiger,andFisherYu. Murf: Multi-baselineradiancefields. InCVPR,2024.
Jiawei Yang, Marco Pavone, and Yue Wang. Freenerf: Improving few-shot neural rendering with
free frequency regularization. In Proceedings of the IEEE/CVF conference on computer vision
andpatternrecognition,pp.8254–8263,2023.
AlexYu,VickieYe,MatthewTancik,andAngjooKanazawa. pixelnerf:Neuralradiancefieldsfrom
oneorfewimages. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpattern
recognition,pp.4578–4587,2021.
RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pp.586–595,2018a.
RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang. Theunreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InProceedingsoftheIEEEconferenceon
computervisionandpatternrecognition,pp.586–595,2018b.
TinghuiZhou,RichardTucker,JohnFlynn,GrahamFyffe,andNoahSnavely.Stereomagnification:
Learningviewsynthesisusingmultiplaneimages. ACMTrans.Graph.(Proc.SIGGRAPH),37,
2018. URLhttps://arxiv.org/abs/1805.09817.
ZehaoZhu,ZhiwenFan,YifanJiang,andZhangyangWang. Fsgs:Real-timefew-shotviewsynthe-
sisusinggaussiansplatting. arXivpreprintarXiv:2312.00451,2023.
Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Surface splatting. In
Proceedingsofthe28thannualconferenceonComputergraphicsandinteractivetechniques,pp.
371–378,2001.
15A THEORETICAL PROOF
Proposition 1. Let θ∗,ψ∗ = g∗ be the optimal solution of the solely image-based conditional
diffusion scheme and θ˜∗,ψ˜∗ = {g∗,F∗} be the optimal solution of diffusion scheme with native
3D prior. Suppose the divergence D is convex and the embedding function space Ψ includes all
measurablefunctions,wehaveD(q(x)∥p (x))<D(q(x)∥p (x)).
θ˜∗,ψ˜∗ θ∗,ψ∗
Proof. AccordingtotheconvexityofD andJensen’sinequalityD(E[X]) ≤ E[D(X)],whereX is
arandomvariable,wehave:
(cid:16) (cid:17) (cid:16) (cid:17)
D q(x)∥p (x) =D E q(x|s)∥E p (x|s)
θ˜∗,ψ˜∗ q(s) q(s) θ˜∗,ψ˜∗
(cid:16) (cid:17)
≤E D q(x|s)∥p (x|s) (15)
q(s) θ˜∗,ψ˜∗
(cid:16) (cid:17)
=E D q(x|s)∥p (x|s) ,
q(s) θ˜∗,g∗,F∗
whereweincorporateanintermediatevariables,whichrepresentsaspecificscene. q(x|s)indicates
theconditionaldistributionofrenderingdataxgiventhespecificscenes.Accordingtothedefinition
ofθ˜∗,g∗,F∗,wehave:
(cid:16) (cid:17)
E D q(x|s)∥p (x|s) = min E D(q(x|s)∥p (x|s))
q(s) θ˜∗,g∗,F∗ q(s) θ,g,F
θ,g,F
=minE q(s) min D(cid:0) q(x|s)∥p θ,g(s),F(s)(x)(cid:1) (16)
θ g(s),F(s)
=minE minD(q(x|s)∥p (x)),
q(s) θ,g,E
θ g,E
where E is the general 3D encoder in 3D structure conditional scheme while it is a redundant
embeddinginsolelyimage-basedconditionalscheme,i.e.,ψ ={g,E(∅)}.CombiningEquation15
and16,wehave:
(cid:16) (cid:17)
D q(x)∥p (x) ≤minE minD(q(x|s)∥p (x))
θ˜∗,ψ˜∗ q(s) θ,g,E
θ g,E
(cid:0) (cid:1)
< min D(q(x)∥p θ,g,E(x))= min D q(x)∥p θ,g,E(∅)(x) (17)
θ,g,E θ,g,E(∅)
=minD(q(x)∥p (x))=D(q(x)∥p (x)).
θ,ψ θ∗,ψ∗
θ,ψ
The second inequality holds because given any specific scene s in any parameter θ ∈ Θ,
approximating q(x|s) is simpler than q(x) by only tuning the encoder E of p 1, i.e.,
θ,g,E
min D(q(x|s)∥p (x))<min D(q(x)∥p (x)).
E θ,g,E E θ,g,E
Consequently,theproofofProposition1hasbeendone.
1Asimpleverifiablecaseistooptimizetheparametersof3DGSbyonly2Dimages(solelyimage-based
conditional learning) or using a SFM initialization from collected images (native 3D conditional learning)
beforeoptimization.Thelatterprovidesamoreconstrainedandoptimalsolutionspace.
16