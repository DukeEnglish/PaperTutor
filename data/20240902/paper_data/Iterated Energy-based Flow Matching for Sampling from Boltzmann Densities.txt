Iterated Energy-based Flow Matching for Sampling from Boltzmann Densities
DongyeopWoo1 SungsooAhn1
Abstract arenotapplicabletothisproblem. Unliketheconventional
settings with access to samples from the target distribu-
In this work, we consider the problem of train-
tion, creatingthedatasetisexpensivefortheseproblems.
ingageneratorfromevaluationsofenergyfunc-
One could resort to simulating the actual molecular dy-
tions or unnormalized densities. This is a
namicsorleverageMonteCarlo(MC)techniquessuchas
fundamentalprobleminprobabilisticinference,
annealedimportancesampling(Neal,2001)orsequential
which is crucial for scientific applications such
MonteCarlo(DelMoraletal.,2006). However,suchsam-
as learning the 3D coordinate distribution of a
plingalgorithmsarecomputationallyexpensiveanddonot
molecule. Tosolvethisproblem,weproposeiter-
easilyscaletohighdimensions.
atedenergy-basedflowmatching(iEFM),thefirst
off-policyapproachtotraincontinuousnormaliz- Tosolvethisproblem,researchershavedevelopedframe-
ingflow(CNF)modelsfromunnormalizedden- workstodirectlytraingenerativemodelsusingenergyfunc-
sities. Weintroducethesimulation-freeenergy- tionevaluations.Asimpleapproachistotrainthegenerative
basedflowmatchingobjective,whichtrainsthe modelstominimizethereverseKullback-Leibler(KL)diver-
modeltopredicttheMonteCarloestimationofthe genceestimatedbysamplesfromthegeneratorweightedby
marginalvectorfieldconstructedfromknownen- theunnormalizeddensities(Wirnsbergeretal.,2022). How-
ergyfunctions. Ourframeworkisgeneralandcan ever,thisapproachsuffersfromthemode-seekingbehaviour
beextendedtovariance-exploding(VE)andopti- thatleadstoageneratorthatonlycapturesaparticularmode
maltransport(OT)conditionalprobabilitypaths. ofthesystem. Alternatively,researchershaveconsidered
WeevaluateiEFMonatwo-dimensionalGaussian minimizingthecombinationoftheforwardandthereverse
mixturemodel(GMM)andaneight-dimensional KLdivergence(Noe´etal.,2019)ortheα-divergence(Midg-
four-particledouble-wellpotential(DW-4)energy leyetal.,2023). Theseworksdependedonthenormalizing
function. OurresultsdemonstratethatiEFMout- flow architecture which allows explicit evaluation of the
performsexistingmethods,showcasingitspoten- datalikelihood.
tialforefficientandscalableprobabilisticmodel-
Without using the normalizing flow models, researchers
ingincomplexhigh-dimensionalsystems.
havealsoconsideredtrainingdiffusion-basedsamplers,e.g.,
path integral sampler (Zhang & Chen, 2021, PIS), time-
1.Introduction reverseddiffusionsampler(Berneretal.,2024, DIS),de-
noisingdiffusionsampler(Vargasetal.,2022,DDS).How-
Inthiswork,weareinterestedintraininganeuralnetwork ever,theymostlyrequiresimulationofthediffusion-based
tosamplefromthedistributionq(x)∝exp(−E(x))where modelstogeneratetrajectoriestotrainon,whichmaybe
E(x)isaknownenergyfunction. Asignificantexampleof expensivetocollect. Inaddition,theirobjectiveison-policy,
thisproblemincludessamplingfromtheBoltzmanndistri- andtheycannotreusesamplescollectedfrompreviousver-
butionofamolecule,wherethe3Datomiccoordinatesxare sionsofthegenerativemodelwithoutimportancesampling.
distributedaccordingtothephysicalenergyofthemolecular
Recently, iterated denoising energy matching (Akhound-
system. Anaccuratesamplerforthisproblemiscrucialfor
Sadegh et al., 2024, iDEM) has been proposed as a
manyscientificapplications,e.g.,inferringtheprobability
simulation-free and off-policy approach to train the gen-
thataproteinwillbefoldedatagiventemperaturefordrug
erators. Theyproposetotrainthedenoisingdiffusionmodel
discovery(Noe´ etal.,2019).
togeneratethesamples. ThegeneratoristrainedonMonte
However, therecentlysuccessfuldeepgenerativemodels Carloestimationofthescorefunction. Itconsistsofitera-
tivelycollectingsamplesandestimatingthescorefunction.
1PohangUniversityofScienceandTechnology. Correspon-
Continuousgenerativeflownetwork(Bengioetal.,2021;
dence to: Dongyeop Woo <dongyeop.woo@postech.ac.kr>,
SungsooAhn<sungsoo.ahn@postech.ac.kr>. Lahlouetal.,2023;Senderaetal.,2024,GFlowNet)also
servesasaframeworktotrainthediffusionmodelsusing
Preprint.
1
4202
guA
92
]GL.sc[
1v94261.8042:viXraIteratedEnergy-basedFlowMatchingforSamplingfromBoltzmannDensities
energyfunctionsinasimulation-freeandoff-policymanner. Algorithm1IteratedEnergy-basedFlowMatching
Input: Networku ,noisescheduleσ2,priorp ,replay
Inthiswork,weconsiderthefirstsimulation-freeandoff- θ t 0
bufferB,numberofdatapointaddedtoreplaybufferB ,
policyframeworktolearntheBoltzmanndistributionusing 1
batchsizeB ,andnumberofMCsamplesK.
acontinuousnormalizingflow(CNF),analternativetothe 2
while Outer-Loop do
currentdiffusion-basedmodels. Unlikethediffusion-based
models, CNFs deterministically generate the data using Set{x( 0b)}B b=1
1
∼p 0(x 0).
vectorfields. Theyarepromisingintheirflexibilitytobe Set{x(b)}B1 ←ODEsolve({x(b)}B1 ,u ,0,1).
1 b=1 0 b=1 θ
generalizedacrossavarietyofprobabilitypaths(Lipman UpdatethebufferB =(B∪{x(b)}B1 ).
1 b=1
etal.,2022),withvariationsthatarefasterandeasiertotrain while Inner-Loop do
comparedtothediffusion-basedmodels(Tongetal.,2024). Sample{x(b)}B2 fromthebufferB.
1 b=1
Inparticular,ourkeyideaistoestimatetheflowmatching Samplet(b)uniformlyfrom[0,1].
objectiveusinganewlyderivedMonteCarloestimationof Samplex(b) ∼p (x(b)|x(b))forb=1,...,B .
t(b) 1 2
thedata-generatingvectorfield. Ouralgorithmisstructured ComputethelossfunctionL definedas:
EFM
similarlytotheiDEMalgorithm. Weemployasimulation-
freeandoff-policyobjectivewhichinvolvesestimationof (cid:88)B2
L = ∥u (x(b),θ)−U (x(b),t(b))∥2.
thedata-generatingvectorfieldtotraintheCNF.Wealso EFM t(b) K
useareplaybuffertoreusethesamplesandimprovesample- b=1
efficiencyfortraining. Weconsiderthevarianceexploding UpdateθtominimizeL .
EFM
andconditionaloptimaltransportprobabilitypathstoimple- endwhile
mentouridea. endwhile
Weevaluateouralgorithmforthetwo-dimensionalGaussian
mixturemodel(GMM)andeight-dimensionalfour-particle
double-wellpotential(DW-4)energyfunctions. OuriEFM fromthepriordistributionp andsolvingtheordinarydiffer-
0
isshowntooutperformtheexistingworks,includingiDEM, entialequation(ODE)expressedbydx = u (x;θ)dt. We
t
demonstratingthepromiseofourwork. provideamoredetaileddescriptionofCNFinAppendixA.
2.IteratedEnergy-basedFlowMatching 2.2.IteratedEnergy-basedFlowMatching(iEFM)
Here, we propose our iterated energy-based flow match-
2.1.FlowMatchingObjective
ing (iEFM) for simulation-free and off-policy training of
In this work, we are interested in learning a Boltzmann CNFsusingenergyfunctionsoftheBoltzmanndistribution.
distribution µ(x) associated with an energy function E : Tothisend,weproposeanewestimatorforthemarginal
Rd →Rdefinedasfollows: vectorfieldv (x)inEquation(2)anddescribeaniterative
t
exp(−E(x)) (cid:90) schemetocollectsamplesandupdatetheCNFmodels. We
µ(x)= , Z = exp(−E(x))dx, (1) provideacompletedescriptionofiEFMinAlgorithm1.
Z
Rd
whereZ istheintractablenormalizingconstant. Unlikethe Energy-basedflowmatching. Wefirstproposeournew
conventional settings of training a generative model, our objective,whichinvolvesestimationofthetargetvectorfield
schemedoesnotrequiresamplesfromµ(x),butassumes v t(x)thatconstructstheBoltzmanndistribution. Among
theabilitytoevaluatetheenergyfunctionE(x). thepossiblevectorfields,weconsiderthefollowingform
proposedinLipmanetal.(2022):
Our scheme aims to train a continuous normalizing
flow (CNF) to match a random process x t with marginal (cid:90) p (x|x )p (x )
density p
t
for t ∈ [0,1] which starts at simple Gaussian v t(x)= v t(x|x 1) t p1 (x)1 1 dx 1, (3)
prior p (x ) = N(x ;0,σ2) and ends at the target distri- t
0 0 0 0
bution p (x ) = µ(x ). In particular, we let u (x;θ) pa-
1 1 1 t wherev (x|x )istheconditionalvectorfieldthatgenerates
rameterizedbyaneuralnetworkregressthetime-dependent t 1
theconditionalprobabilitypathp (x|x ).Wenotehowvari-
vectorfieldv :Rd →Rdwitht∈[0,1]thatgeneratesthe t 1
t ousconditionalvectorfieldscanbeusedtogeneratethedata
marginaldensityp .Inparticular,weconsiderthefollowing
t distribution,e.g.,varianceexploding,variancepreserving,
flowmatching(FM)objective(Lipmanetal.,2022):
andoptimaltransportconditionalvectorfields.
L (θ)=E [∥u (x;θ)−v (x)∥2], (2)
FM t∼[0,1],x∼pt(x) t t 2 Ourkeyideaistoexpressthetargetvectorfieldv (x)us-
t
wheretissampleduniformlyfrom[0,1].Consequently,one ing ratio of expectations over a distribution q(x ;x,t) ∝
1
cansamplefromtheCNFbystartingfromaninitialvaluex p (x|x ) and apply Monte Carlo estimation. A detailed
0 t 1
2IteratedEnergy-basedFlowMatchingforSamplingfromBoltzmannDensities
derivationisasfollows:
(cid:82)
v (x|x )p (x|x )p (x )dx
t 1 t 1 1 1 1
v t(x)= (cid:82)
p (x|x )p(x )dx
t 1 1 1
(cid:82)
v (x|x )q(x ;x,t)p (x )dx
t 1 1 1 1 1
= (cid:82) (4)
q(x ;x,t)p(x )dx
1 1 1
E [v (x|x )p (x )]
=
x1∼q(x1;x,t) t 1 1 1
.
E [p(x )]
x1∼q(x1;x,t) 1
FromEquation(4),onecanestimatev (x)≈U (x,t):
t K
1 (cid:80)K v (x|x(i))p (x(i))
U (x,t)= K i=1 t 1 1 1
K 1 (cid:80)K p (x(i))
K i=1 1 1 (5)
(cid:80)K v (x|x(i))p (x(i))
= i=1 t 1 1 1 ,
(cid:80)K p (x(i))
i=1 1 1
wherex(1),...,x(K)areK samplesfromtheauxiliarydis-
1 1
tributionq(x ;x,t). Intuitively,thismarginalvectorfield
1
estimator U (x,t) can also be understood as a weighted
K
estimateofconditionalvectorfields:
Figure1.ContourplotfortheenergyfunctionofaGMMwith40
U
(x,t)=(cid:88)K
w v (x|x(i)),
modes.Coloredpointsrepresentsamplesfromeachmethod.
K i t 1
q(x ;x,t) ∝ p (x|x ) which depends on choice of the
i=1 1 t 1
(6)
p (x(i)) conditionalprobabilitypath. Ingeneral,weconsideracon-
w := 1 1 . ditionalprobabilitypathofthefollowingform:
i (cid:80) p (x(j))
j 1 1
p (x|x )=N(x;µ (x ),σ2I), (8)
Here,eachconditionalvectorfieldtermdescribesthedirec- t 1 t 1 t
tioninwhichtheprobabilitymassmovestoreachsampled
whereµ :Rd →Rd isatime-dependentinvertiblediffer-
endpointcandidatesx( 1i). entiablet
function,σ
t
∈ Risatime-dependentscalar,and
Finally,wederiveourEFMobjectiveasanapproximation I∈Rd×disanidentitymatrix. Foroptimaltransport(OT)
oftheFMobjectiveinEquation(2)asfollows: conditionalprobabilitypaths,weset:
L EFM(θ)=E t∼[0,1],x∼rt(x)∥u t(x;θ)−U K(x,t)∥2, (7) p t(x|x 1)=N(x;tx 1,(1−(1−σ 1)t)2I), (9)
wherer (x)isareferencedistributionwithsupportRd. It
t whereσ isafixedhyper-parameter. Thenthedistribution
1
isnoteworthyhowtheoriginalFMobjectivewasproposed
q(x ;x,t) ∝ p (x|x )correspondstothefollowingdistri-
1 t 1
withr (x) = p (x), buttheoptimalvectorfielddoesnot
t t bution:
change with a different choice of reference distribution.
Hence, similar to the iDEM approach (Akhound-Sadegh
(cid:18)
x
(cid:18) σ2(cid:19) (cid:19)
q(x ;x,t)=N x ; , t I , (10)
etal.,2024),ouralgorithmisoff-policy. Furthermore,eval- 1 1 t t2
uatingtheEFMobjectivedoesnotrequireanysimulationof
theCNF,henceourEFMobjectiveisalsoasimulation-free whereIistheidentitymatrix. Forvarianceexploding(VE)
approach. conditional probability paths, p t(x|x 1) is defined by the
meanµ (x ) = x andstandarddeviationσ followinga
t 1 1 t
Iterativetrainingwithreplaybuffer. Sinceourenergy-
geometricnoiseschedule, whichleadstothedistribution
basedflowmatchingobjectiveisoff-policy,weemployan
q(x ;x,t)∝p (x|x )expressedasfollows:
1 t 1
additionalreplaybuffertostorepreviouslyusedsamples. In
particular,ouriEFMalgorithmiteratesbetweentwosteps(a) q(x ;x,t)=N(x ;x,σ2I). (11)
1 1 t
storingthesamplesfromtheCNFintothereplaybufferand
(b)trainingtheCNFusingtheenergy-basedflowmatching
3.Experiments
objectivedefinedonsamplesfromthereplaybuffer.
Conditional probability paths. Computation of the WeevaluateiEFMontwosystems: atwo-dimensional40
EFM objective requires sampling from the distribution Gaussianmixturemodel(GMM)andaneight-dimensional
3IteratedEnergy-basedFlowMatchingforSamplingfromBoltzmannDensities
Table1.PerformancecomparisonforGaussianmixturemodel(GMM)and4-particledouble-well(DW-4)energyfunction.Theperfor-
manceismeasuredusingnegativelog-likelihood(NLL)and2-Wassersteinmetrics(W ).Wereporttheresultswithmeanandstandard
2
deviationmeasuredoverthreeindependentruns.Wemarkresultswithinthestandarddeviationofthebestnumberinbold.†Wecompare
withthenumbersreportedbyAkhound-Sadeghetal.(2024).
Energy→ GMM(d=2) DW-4(d=8)
Method↓ NLL W NLL W
2 2
FAB†(Midgleyetal.,2023) 7.14±0.01 12.0±5.73 7.16±0.01 2.15±0.02
PIS†(Zhang&Chen,2021) 7.72±0.03 7.64±0.92 7.19±0.01 2.13±0.02
DDS†(Vargasetal.,2022) 7.43±0.46 9.31±0.82 11.3±1.24 2.15±0.04
iDEM†(Akhound-Sadeghetal.,2024) 6.96±0.07 7.42±3.44 7.17±0.00 2.13±0.04
iEFM-VE(ours) 7.08±0.04 4.91±0.60 7.53±0.00 2.21±0.00
iEFM-OT(ours) 6.92±0.05 5.10±0.89 7.37±0.01 2.07±0.00
using a trained model. Except for iEFM, the probability
is calculated using the optimal transport conditional flow
matching (OT-CFM) model (Tong et al., 2024), which is
trainedinasample-basedtrainingmannerusingasample
generated by each baseline method. For iEFM, NLL is
directly evaluated by solving ODE. To evaluate W , we
2
generateasamplefromthetrainedmodelandmeasureW
2
betweengeneratedsampleandthedataset.
Mainresults. WereportNLLandW forGMMandDW-4
2
in Table 1. For GMM, iEFM matches or outperforms all
Figure2.Exampleof10trajectoriesoncontourplotfortheenergy consideredbaselinesonNLLandW 2.ForDW-4,iEFM-OT
functionofGMM.Coloredlinerepresentthetrajectoryofsampled outperformsallconsideredbaselinesonW 2.
ODEsolution.Arrowsrepresenttheprogressionoftime.
In Figure 1, we visualize the sample from iEFM of both
probabilitypathswiththesamplefromiDEMandground
four-particle double-well potential (DW-4) energy func-
truth GMMs. Notably, iEFM can capture the sharpness
tion. For each energy function, we report negative log-
ofGMMmodewhiletheiDEMoftensamplesslightlyfar
likelihood(NLL)and2-Wassersteindistance(W ).
2 points from the mode. This explains why iDEM signif-
Baselines.WecompareiEFMtofourrecentworks:thepath icantly reduces W in GMM. Additionally, we visualize
2
integral sampler (PIS) (Zhang & Chen, 2021), denoising the trajectory of the ODE solution from both probability
diffusionsampler(DDS)(Vargasetal.,2022),flowannealed pathsinFigure2. Asexpected,iEFM-OThasastraighter
bootstrapping (FAB) (Midgley et al., 2023), and iterated flow than iEFM-VE, even though there is no theoretical
denoisingenergymatching(iDEM)(Akhound-Sadeghetal., guaranteethattheflowofiEFM-OTismarginallyOT.
2024). Forbaselines,wereporttheresultsfromAkhound-
Sadeghetal.(2024).
4.Conclusion
Architecture. We parameterize the vector field u using
θ In this work, we address the problem of sampling from
anMLPwithsinusoidalpositionalembeddingsforGMM
Boltzmanndistributionswhenonlyenergyfunctionsorun-
andanEGNNflowmodel(Satorrasetal.,2021)forDW-4
normalizeddensitiesareavailable. Totacklethisproblem,
followingthepriorwork(Akhound-Sadeghetal.,2024).
weintroduceiEFM,thefirstoff-policyapproachtotraining
Metrics. WeuseNLLandW asourmetrics. Formetric aCNFmodel. TheEFMobjective,anovelsimulation-free
2
evaluation, we adopt pre-generated ground truth samples off-policyobjectivethatinvolvesMCestimationofMVFs,
asdatasets. Sincewecangenerategroundtruthsamplesin enablesiEFMtoeffectivelyutilizepreviouslyusedsamples
GMM,weadoptitasadataset. InDW-4,weuseasample with the replay buffer. Our experimental results demon-
generated by MCMC in Klein et al. (2024) as a dataset. stratethatiEFMeitheroutperformsormatchesiDEMon
Eventhoughthisisnotaperfectgroundtruth,webelieveit GMMandDW-4benchmarks. Futureresearchdirections
isareasonableapproximationofthegroundtruthsamples. include validating and extending iEFM to complex high-
dimensionalsystems.
TotestNLL,weevaluatethelogprobabilityofthesample
4IteratedEnergy-basedFlowMatchingforSamplingfromBoltzmannDensities
References Sendera,M.,Kim,M.,Mittal,S.,Lemos,P.,Scimeca,L.,
Rector-Brooks, J., Adam, A., Bengio, Y., and Malkin,
Akhound-Sadegh,T.,Rector-Brooks,J.,Bose,A.J.,Mittal,
N. Ondiffusionmodelsforamortizedinference: Bench-
S.,Lemos,P.,Liu,C.-H.,Sendera,M.,Ravanbakhsh,S.,
markingandimprovingstochasticcontrolandsampling.
Gidel, G., Bengio, Y., et al. Iterated denoising energy
arXivpreprintarXiv:2402.05098,2024.
matchingforsamplingfromboltzmanndensities. arXiv
preprintarXiv:2402.06121,2024. Tong,A.,FATRAS,K.,Malkin,N.,Huguet,G.,Zhang,Y.,
Rector-Brooks,J.,Wolf,G.,andBengio,Y. Improving
Bengio,E.,Jain,M.,Korablyov,M.,Precup,D.,andBen-
andgeneralizingflow-basedgenerativemodelswithmini-
gio,Y. Flownetworkbasedgenerativemodelsfornon-
batchoptimaltransport. TransactionsonMachineLearn-
iterativediversecandidategeneration. AdvancesinNeu-
ingResearch,2024. ISSN2835-8856. URLhttps://
ral Information Processing Systems, 34:27381–27394,
openreview.net/forum?id=CD9Snc73AW. Ex-
2021.
pertCertification.
Berner, J., Richter, L., and Ullrich, K. An optimal con-
trol perspective on diffusion-based generative model- Vargas, F., Grathwohl, W. S., and Doucet, A. Denoising
ing. TransactionsonMachineLearningResearch,2024. diffusionsamplers. InTheEleventhInternationalCon-
ISSN 2835-8856. URL https://openreview. ferenceonLearningRepresentations,2022.
net/forum?id=oYIjw37pTP.
Wirnsberger,P.,Papamakarios,G.,Ibarz,B.,Racaniere,S.,
Chen,R.T.,Rubanova,Y.,Bettencourt,J.,andDuvenaud, Ballard,A.J.,Pritzel,A.,andBlundell,C. Normalizing
D.K. Neuralordinarydifferentialequations. Advances flowsforatomicsolids. MachineLearning: Scienceand
inneuralinformationprocessingsystems,31,2018. Technology,3(2):025009,2022.
DelMoral,P.,Doucet,A.,andJasra,A. Sequentialmonte Zhang,Q.andChen,Y. Pathintegralsampler: Astochastic
carlosamplers. JournaloftheRoyalStatisticalSociety controlapproachforsampling. InInternationalConfer-
SeriesB:StatisticalMethodology,68(3):411–436,2006. enceonLearningRepresentations,2021.
Klein,L.,Kra¨mer,A.,andNoe´,F. Equivariantflowmatch-
ing. AdvancesinNeuralInformationProcessingSystems,
36,2024.
Lahlou, S., Deleu, T., Lemos, P., Zhang, D., Volokhova,
A.,Herna´ndez-Garcıa,A.,Ezzine,L.N.,Bengio,Y.,and
Malkin,N. Atheoryofcontinuousgenerativeflownet-
works.InInternationalConferenceonMachineLearning,
pp.18269–18300.PMLR,2023.
Lipman, Y., Chen, R.T., Ben-Hamu, H., Nickel, M., and
Le,M. Flowmatchingforgenerativemodeling. InThe
Eleventh International Conference on Learning Repre-
sentations,2022.
Midgley,L.I.,Stimper,V.,Simm,G.N.C.,Scho¨lkopf,B.,
andHerna´ndez-Lobato,J.M. Flowannealedimportance
samplingbootstrap.InTheEleventhInternationalConfer-
enceonLearningRepresentations,2023. URLhttps:
//openreview.net/forum?id=XCTVFJwS9LJ.
Neal,R.M. Annealedimportancesampling. Statisticsand
Computing,11(2):125–139,2001.
Noe´,F.,Olsson,S.,Ko¨hler,J.,andWu,H.Boltzmanngener-
ators:Samplingequilibriumstatesofmany-bodysystems
withdeeplearning. Science,365(6457):eaaw1147,2019.
Satorras, V. G., Hoogeboom, E., and Welling, M. E (n)
equivariantgraphneuralnetworks. InInternationalcon-
ference on machine learning, pp. 9323–9332. PMLR,
2021.
5IteratedEnergy-basedFlowMatchingforSamplingfromBoltzmannDensities
A.PreliminaryonContinuousNormalizingFlow
LetRddenotethedataspaceandµ:Rd →R denotethedensityfunctionofdata-generatingdistribution. Continuous
>0
NormalizingFlow(CNF)(Chenetal.,2018)isadeepgenerativemodelutilizingneuralordinarydifferentialequation(ODE)
todescribeµ. Inthissection,wereviewtheformaldefinitionofCNF.
TheCNFisdefinedbyatime-dependentvectorfieldv :Rd →Rdfort∈[0,1]whichinducesaflowmapϕ :Rd →Rd
t t
viathefollowingODE:
d
ϕ (x)=v (ϕ (x)), ϕ (x)=x (12)
dt t t t 0
Theflowmapϕ (x)describeshowpointxattimet=0movesatgiventimet. Withsomewell-behavedconditionsona
t
vectorfieldv ,ϕ (x)iswell-definedbytheexistenceanduniquenessoftheODEsolutionoftheinitialvalueproblem. Then,
t t
wecandefinetheprobabilitydensitypathp :Rd →R+fort∈[0,1]withsimplegivenpriordensityp asfollows:
t 0
p =[ϕ ] p (13)
t t ∗ 0
Ifonecanfindaproperflowmapϕ whichmakesthedistributionp matchwiththetargetdensityµ,samplingfromthe
t 1
targetdensityµcanbedonebysolvinganODE,basedonthefactthatx ∼ p impliesϕ (x ) ∼ x . Insuchcase,the
0 0 1 0 1
vectorfieldv issaidtogenerateaprobabilitydensityp andtargetdensityµ.
t t
Now,weparameterizev (x)withaneuralnetworku (x;θ).Theflowinducedbyu (x,θ)iscalledaContinuousNormalizing
t t t
Flow(CNF).Ourgoalistofindaproperu sothatitsinducedflowmakesp matchwithµ. Onewaytoachievethisisto
t 1
makeu regresstov thatgeneratesµ(Lipmanetal.,2022). ThisidealeadsflowmatchingobjectivesinEquation(2).
t t
6