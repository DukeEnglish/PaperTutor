3D Whole-body Grasp Synthesis with Directional Controllability
GeorgiosPaschalidis1 RomanaWilschut1 DimitrijeAntic´1 OmidTaheri2 DimitriosTzionas1
1UniversityofAmsterdam,theNetherlands 2MaxPlanckInstituteforIntelligentSystems,Tu¨bingen,Germany
{g.paschalidis, d.antic, d.tzionas}@uva.nl romana.wilschut@gmail.com otaheri@tue.mpg.de
Figure1. WedevelopCWGrasp, anovelframeworkforsynthesizing3Dwhole-bodygraspsforanobjectplacedonareceptacle. Our
frameworkcapitalizesonanovelcombinationofgeometric-basedreasoningandcontrollabledata-drivensynthesismethods. Byaddinga
novelcontrollabilityinthesynthesisprocess,weachieverealisticresultsatafractionofthecomputationalcostw.r.t.thestateoftheart.
Abstract Ourkeyideaisthatperforminggeometry-basedreasoning
“early on,” instead of “too late,” provides rich “control”
Synthesizing 3D whole-bodies that realistically grasp signals for inference. To this end, CWGrasp first samples
objectsisusefulforanimation,mixedreality,androbotics. a plausible reaching-direction vector (used later for both
This is challenging, because the hands and body need to thearmandhand)fromaprobabilisticmodelbuiltviaray-
look natural w.r.t. each other, the grasped object, as well casting from the object and collision checking. Then, it
as the local scene (i.e., a receptacle supporting the ob- generates a reaching body with a desired arm direction,
ject). Only recent work tackles this, with a divide-and- aswellasa“guiding”graspinghandwithadesiredpalm
conquer approach; it first generates a “guiding” right- direction that complies with the arm’s one. Eventually,
hand grasp, and then searches for bodies that match this. CWGrasp refines the body to match the “guiding” hand,
However, the guiding-hand synthesis lacks controllability while plausibly contacting the scene. Notably, generating
and receptacle awareness, so it likely has an implausible already-compatible“parts”greatlysimplifiesthe“whole”.
direction (i.e., a body can’t match this without penetrat- Moreover, CWGrasp uniquely tackles both right- and left-
ing the receptacle) and needs corrections through major handgrasps. WeevaluateontheGRABandReplicaGrasp
post-processing. Moreover, the body search needs exhaus- datasets. CWGrasp outperforms baselines, at lower run-
tive sampling and is expensive. These are strong limita- time and budget, while all components help performance.
tions.WetacklethesewithanovelmethodcalledCWGrasp. Codeandmodelswillbereleased.
1
4202
guA
92
]VC.sc[
1v07761.8042:viXra1.Introduction
Synthesizingvirtual3Dhumansthatgraspobjectsrealisti-
callyisimportantforapplicationssuchasvirtualassistants,
animation, robotics, games, or synthetic image datasets.
Importantly,thisinvolvesthewholebody,sothatthebody
approachesanobject,armsreachit,andhandsgraspit. But
thisischallenging;thebodyandhandsshouldlooknatural
Figure2. Controllablereaching-bodysynthesiswithourCReach.
and fully coordinated, the body should approach an object
Weshowtwoexampleswheremultiplebodies(shownwithseveral
withoutpenetratingthescene,thehandsshoulddexterously
colors)aregeneratedtoreachatargetwrist/objectlocation,while
contacttheobject,whilealsotrainingdataisscarce. Dueto
thearmpreservesadesireddirection,shownwithagrayarrow.
these,existingworktacklesonlypartsoftheproblem,i.e.,
disembodiedhands,orbodieswithnon-dexteroushands.
Only the recent FLEX [48] method tackles whole bod-
ies, following a divide-and-conquer way for tractability.
To this end, it first generates a hand-only grasp through
GrabNet[44]. Then,thisgraspinghandguidesasearchfor
aplausiblebody. Thatis,manybodiesaresampledinran- Figure3. Controllabilityforhand-graspsynthesis. Thegoalisto
domposesandscenelocations,andareoptimizedtomatch grasptheredwineglass. Left–GrabNet[44]: DuetoGrabNet’s
the guiding hand. However, there is a key problem; the lackofcontrollability, samplingitslatentspaceproducesplausi-
guiding hand has a random direction that likely disagrees blegrasps(shownwithseveralcolors),butwithrandomdirection.
withthedirectionbodiescanapproachfromwithoutpene- Right–OurCGrasp:Weaddcontrollability,sodrawingsamples
tratingreceptacles.Thus,theguidinghandneedsmajorcor- producesplausibleandvariedgrasps(shownwithseveralcolors),
thathaveadesired3Dpalmdirection(shownwithagrayarrow).
rectionsthroughpost-processing. Thisproducespromising
resultsbutneedsexhaustivesampling(500bodysamples),
andisexpensive(separaterefinementpersample). CReach model: We train a conditional variational au-
We identify two main reasons for the above prob- toencoder (cVAE) for producing a reaching SMPL-X [39]
lems:(1)Performingbody-andreceptacle-awarereasoning body. This goes beyond GNet in three ways: (1) It is
“too late”, and (2) GrabNet’s total lack of controllability1. conditioned not only on target object/wrist location, but,
These are key limitations. We tackle these by developing uniquely, also on a desired 3D arm direction; see Fig. 2.
CWGrasp (“Controllable Whole-body Grasp synthesis”), (2) It is trained not only on GRAB [44] data, which has a
anewmethodcomprisedofthefollowingnovelmodules. limited range of target object/wrist locations, but also on
ReachingFieldmodel: First,weneedtoright-awayde- CIRCLE [3] data that is richer for reaching body poses.
tect the directions from which a body’s arm and hand can (3) It can generate both right- and left-arm reaching. We
reachanobject,withoutpenetratingthereceptaclesupport- calltheresultingmodelCReachfor“ControllableReach.”
ing the object. Think of a mug lying on a shelf and emit- CGrasp model: We train a cVAE to generate a grasp-
ting“light”;someraystravelunblockedinfreespace,while ing MANO [41] hand. This goes beyond GrabNet in two
other ones get blocked by shelf panels. Our key insight is ways: (1) It is conditioned not only on object shape1, but,
that the “well-lit” space near the object reveals its reacha- uniquely, also on a desired 3D palm direction; see Fig. 3.
bility. So, we cast rays from the object, detect collisions (2)Itcangeneratebothright-andleft-handgrasps. Wecall
withreceptacleswithinadistanceof2meters,consideronly theresultingmodelCGraspfor“ControllableGrasp.”
thenon-collidingrays,andtraversethesewhilecastingnew CWGraspframework: WeconditionbothCReachand
raystowardsthefloorforfurthercollisionchecks. Weuse CGrasponthesamedirection,producedbyReachingField.
the rays that pass all tests for building a new probabilistic Importantly, this produces a reaching SMPL-X body
3Dvectorfield,calledReachingField. (CReach), and a guiding MANO grasping hand (CGrasp)
Sampling the ReachingField provides a 3D direction thatareright-awaycompatiblewitheachother,sotheyonly
vectorthatcanbeusedasacontrolsignalforconditioning need a small refinement to be “put together.” To this end,
thesynthesisofareachingbodyandgraspinghand. How- we conduct optimization [24, 39, 48] that searches for the
ever,existingsynthesizersforthis,suchasGNet[45]forthe SMPL-XposethatletsSMPL-X’shandmatchtheguiding
bodyandGrabNet[44]forthehand,lacksuchcontrollabil- MANO hand, while contacting the floor and not penetrat-
ity1,2. Weresolvethiswithtwonovelmodules,asfollows. ingthereceptacle. Thankstoourcontrollableinference,we
sampleonly1bodyandhandfromCReachandCGrasp,re-
1GrabNet[44]useswristtranslationandrotationonlyfortraining.For
spectively,instrongcontrasttoFLEX’s500differentsam-
inferencetheonlyinputisobjectshape,sograspshavearandomdirection.
2GNet[45]takesasinputonlyobjectshapeandheight. ples. Thismakesourframeworkroughly16×faster.
2We evaluate on the GRAB [44] and ReplicaGrasp [48] siders a per-vertex contact likelihood. GrabNet lacks con-
datasets. Both CGrasp and CReach accurately preserve a trollability, so it produces grasps with random directions.
specifieddirection. Importantly,addingcontrollabilitydoes ContactGen[31]learnsanobject-conditionedjointdistribu-
notharm;CGraspperformsroughlyonparwiththreebase- tion of a contact-, part- and direction-map. Specifically, it
lines (GrabNet [44], ContactGen [31], DexGraspNet [52]) considersthedirectionofcontactatalowlevelandexploits
whileuniquelycontrollingpalmdirection. Last,forwhole- thisforsynthesis. Wearenotawareofworkthatexplicitly
bodygrasps,ourCWGraspmethodoutperformsFLEX[48] considersthepalm’sdirection. OurCGrasptacklesthis.
in almost all metrics, while CWGrasp’s generated bodies,
Graspsfromimages: ObMan[26]infershandandob-
areperceivedasmorerealistic.
ject meshes from an image, while H+O [47] infers key-
Insummary,herewemakefourmaincontributions:
points.GanHand[11]infersobjectposeandgrasptypewith
1. The ReachingField model that generates 3D directions
a rough hand pose [17], refining it via contact constraints.
foranobjecttobereached,helpingasacontrolsignal.
TOCH[60]doessucharefinementusinga3DSDF.
2. The CReach model that generates a SMPL-X body
Motion generation: D-Grasp [10] learns hand-object
reachingobjectswithadesired(right/left)armdirection.
interactions via RL; given a hand and object pose, the
3. The CGrasp model that generates a (right/left) MANO
task is to grasp and move an object to a goal pose.
handgraspinganobjectwithadesiredpalmdirection.
ManipNet [55] generates hand-object interaction (HOI)
4. ThenovelCWGraspmethodthatcombinestheabovefor
motions for single or both hands, using spatial features.
generatingdexterousSMPL-Xgraspsforanobjectlying
GeneOHDiffusion[32]denoisesHOImotionviadiffusion,
onareceptacle. Thisis16xfasterthanaSotAbaseline,
and a hand-keypoint trajectory representation. GRIP [46]
anduniquelytacklesbothright-andleft-handgrasps.
andGEARS[61]synthesizeinteractingfingermotionfrom
Toconclude,thisworkcontributesanovelmethodologyfor
givenhandandobjecttrajectories.GraspXL[57]introduces
controllablesynthesis.
a RL pipeline for generating grasping motions without us-
ingpre-capturedHOIdata.
2.RelatedWork
2.1.Hand-onlyGrasps
2.2.Whole-bodyGrasps
Early research focused on modeling [13, 36] and classify-
ing [14, 17] grasps. Then, research focused on generating Theshaperepresentationusedforbodymodelsrangesfrom
graspsforrobot[5,56]andhumanhands[6,7,44]. cylinders[35]andsuper-quadrics[19]tomesh-basedstatis-
Handmodels:Someworkmodelshandshapeexplicitly ticalmodels[1,2,34,38,39,54]. WeuseSMPL-X[39].
withmeshes[4,37],withMANO[41]beingpopular.Other
Interactingwithscenes: Wangetal.[51]firstinferin-
workusesimplicitshape,suchas3Ddistancefields[12,28]
termediatekeyposesandthengeneratein-betweenmotions.
orasumof3DGaussians[42]. HereweuseMANO.
SAMP[25]stochasticallyinfersseveralgoallocationsand
Data: Many datasets have been captured with single-
orientations on target objects, and then infers in-between
[8, 9, 18, 22, 33, 62] or two-hand [23, 29] images. Re-
motionviaanaction-conditionedVAE.NSM[43]issimilar,
centworkcaptureswhole-bodymeshes[39]graspingrigid
butproducesasingleinteractiondeterministically. Givena
objects [44], or articulated objects while also containing
bodyposeandchairmesh,COUCH[59]infersdiversecon-
RGB images [16]. HOIDiffusion [58] uses a diffusion
tactsonthechair,andinfersbodyposesthatmatchthese.
model for generating synthetic hand-object images con-
Staticgrasps:FLEX[48]generatesSMPL-Xgrasps,by
ditioned on 3D hand-object grasps provided by GrabNet.
optimizingthebodytomatchaguidinghand-graspinferred
DexGraspNet[52]buildsalargedatasetbyapplyinganop-
viaGrabNet[44]. OurCWGraspmethodisinspiredbythis
timizationframeworkon3Dobjects,leveragingadifferen-
butismoreefficientthankstocontrollableinference.
tiableforceclosureestimatorandenergyfunctions.
Contact: ContactGrasp[7]usesrealcontactmapsfrom Dynamicgrasps: CIRCLE[3]andWANDR[15]infer
the ContactDB [6] dataset to infer a grasping hand pose, (short- and long-term, respectively) motion for reaching a
given a posed object mesh. ContactOpt [21] infers likely targetwristlocation. GOAL[45]infersastatictargetbody
hand-object contacts and optimizes hand pose to match grasp via interaction-aware features, and infers motion to
these. GraspTTA [27] infers an initial grasp for an ob- thegoal. SAGA[53]generatessuchmotionsstochastically.
ject point cloud, and optimizes it to match a target contact IMoS[20]infersguidingarm-onlymotionsthat“drive”the
map. Grasp’D[50]takesahand,anobjectasapointcloud wholebody. Givenobjecttrajectories, OMOMO[30]uses
and SDF, and generates grasps via optimization on con- a conditional denoising diffusion model to generate hand
tactforces. GrabNet[44]infersaninitialgraspforaBPS- jointpositionsforeachobjectstate,andthenconditionson
encoded[40]objectandrefinesitwithaneuralnetthatcon- thesetogenerateafullbodywithnon-articulatedhands.
3OPT.
Reaching
Arm Shape
Field Direction Param.
Wrist
Sampling Joint
Pose
Param.
Transl
Param.
Interaction
Hand
Figure4. CWGraspoverview. WefirstsampleaplausiblereachingdirectionfromReachingField. Next,weconditionbothCGraspand
CReachonthisdirectionandobtainahandandabody,respectively,thatsatisfythesampleddirection.Finally,weincludeanoptimization
stagetoresolvepossiblepenetrationswiththesurroundingobject,whilealsomakingthehandandthebodyarmfullycompatible. With
ourframework,wegeneratebothleft-handandright-handgrasps.
3.Method per hand. The pose θ = (θ ,θ ) consists of θ ∈ R22×3
b h b
forthebodyandθ ∈ R2×15×3 forbothhandsasaxisan-
h
We generate realistic whole bodies grasping an object
gles. Theshapeparametersβ arecoefficientsinalearned
wb
that lies on a receptacle, by building a novel control-
low-dimensionallinearspace. Fordetails,see[39].
lable synthesis method, called CWGrasp (Fig. 4). To
CoarseNetfromGrabNet[44]: GrabNetgenerates3D
this end, we develop ReachingField (Sec. 3.2), a novel
MANO grasps for an object, consisting of: CoarseNet,
generative model for reaching-arm/hand directions. We
for generating an initial grasp, and RefineNet, for refin-
exploit this to condition two novel controllable models
ing this. Here we focus only on grasp generation; refine-
for reaching-body (CReach, Sec. 3.3), and hand-grasp
ment is typically studied separately [21, 60]. Thus, we
(CGrasp, Sec. 3.4) synthesis. We combine these in the
buildonCoarseNet. CoarseNetgeneratesa3Dgraspfora
CWGraspframework(Sec.3.5).
givenobject(butwitharandomdirection),andismodeled
withaVAE.GivenanobjectshaperepresentedwithBasis
3.1.Preliminaries
PointSets[40], BPS , awristrotation, θwrist, andtransla-
o h
CGrasp produces MANO hand grasps, while CReach and tion, γ , the encoder Q generates a latent code Z ∈ R16,
h
CWGraspproducereachingandgraspingSMPL-Xbodies, namely: Q(Z|θwrist,γ ,BPS ). The decoder maps this,
h h o
accordingly. Thesearedescribedbelow. concatenated with the object shape, BPS , to a MANO
o
Hand model: To model the hand we use MANO [41], translation, γ¯ ∈ R3, and joint angles, θ¯ ∈ R16×6, i.e.:
h h
a differentiable function M (β ,θ ,γ ) parameterized by P(θ¯ ,γ¯ |Z,BPS ). Training CoarseNet needs minimiz-
h h h h h h o
shape, β ∈ R10, pose, θ , and translation, γ ∈ R3. ing 5 losses: L , L , L , L , L ; the last
h h h KL edge vertex do2h dh2o
Theoutputisa3Dmesh, M , riggedwithaskeletonwith twousehand-to-objectdistances, d , andobject-to-hand
h h2o
16 joints; 1 for the wrist, and 15 for fingers. The pose ones,d . Fordetails,see[44].
o2h
θ ∈R16×6isencodedusingthecontinuous6Drepresenta- GNetfromGOAL[45]: GNetgeneratesa3DSMPL-X
h
tion;theglobalrotation(first6parameters)isdenotedwith grasping body for an object shape and location (but is un-
θwrist ∈ R6. The shape parameters β are coefficients in a awareofreceptacles),andismodeledwithaVAE(similarly
h h
learnedlow-dimensionallinearspace. Fordetails,see[41]. to CoarseNet). It has an encoder, Q(Z|θ ,γ ,L ), a
wb wb target
Whole-body model: We use SMPL-X [39] that mod- decoder,P(θ¯ ,γ¯ |Z,L ),whereZ isthelatentcode,
wb wb target
els the whole body (wb) as a differentiable function θ isbodypose,γ isbodytranslation. L istheob-
wb wb target
M (β ,θ ,γ ) parameterized by shape, β ∈ R10, ject’s“targetcondition”comprisingitsBPSrepresentation
wb wb wb wb wb
pose, θ , and translation, γ ∈ R3; here we ignore fa- and the height of its centroid. Training GNet needs min-
wb wb
cial parameters. The output is a 3D mesh, M , rigged imizing losses on body and hand vertices and poses, gaze
wb
with a skeleton with 22 joints for the body and 15 joints direction,andhand-objectdistance. Fordetailssee[45].
43.2.ReachingField–Reaching-DirectionDetection
Wedevelopalocal-scene-awaremethodfordetectingplau-
sibledirectionsanobjectcanbereachedbyabody’sarms.
We need this to be probabilistic so that we can sam-
ple it, get a plausible reaching direction, and condition
on this a novel controllable model for reaching body syn-
Figure5. Reachabilityofanobject. Left: Wecastraysfromthe
thesis (CReach, Sec. 3.3) and grasping hand synthesis
objecttothesurroundingarea. Right: Wefindtheraysthatdon’t
(CGrasp,Sec.3.4). Thus,wedevelopanovelmodel,called
intersectwiththereceptacle.Theseshowthedirectionstheobject
ReachingField,encodingaprobabilistic3Dvector-fielddis-
canbereached,andareusedtobuildourReachingField.
tributionofunoccludedreachingdirections,asfollows.
Ray casting: Let objects O and M lie in a scene, rep-
resented as 3D meshes with vertices V
O
∈ RNO×3 and exp(cid:16)
1
(cid:17)
V
M
∈ RNM×3, respectively. The goal is to reach the ob-
p =
sign(ai)ai
, (1)
i (cid:16) (cid:17)
ject O (and later grasp it), while not intersecting with the (cid:80) exp 1
“occluder”M. Tofindscene“areas”anddirectionstheob-
i sign(ai)ai
(cid:40)
ject can be reached from, we cast rays from the object to −1 when h ≥T,
wheresign(a )= O
surrounding space, and check for collisions. To this end, i +1 when h <T.
O
for object O, we define its center c ∈ R3 and construct
a spherical grid S(c,2d ) centered around c with radius The term a is the smallest among the two angles that
z i
proportional to its bounding box dimensions (d ,d ,d ) a ray r can form with the vertical axis z. So a =
x y z i i
see Fig. 5. Rays r i = c+t·d i are cast from c towards min(r (cid:99)iz,r(cid:92) i(−z)) and h O is the object height. For details,
eachgridpoints i,whered i aredirectionvectorsfromcto seeSec.6.1.
s . Wethencheckforcollisions.
i
3.3.CReach–ControllableReachingBodies
Rayfiltering: Weevaluateandprunerayswith4steps.
Our goal is controllable synthesis of a SMPL-X body
- Primary check: Discard any ray r intersecting with M. “reaching” an object. We do this by extending GNet [45]
i
The corresponding directions cannot “afford” a plausible withanovelconditiononarmdirection;seeFig.4.
reachingbodyduetoocclusions. Formulation:Thedirectionfromwhichabody-armap-
proaches objects is key for grasping. We provide this to
- Secondary projection: Project the remaining rays onto a CReachasanormalizedvector,d ∈R3. Generatedbod-
arm
horizontalplanetodetectfurtherocclusionsfromM. This iesshouldhaveanarmdirectionthatalignswiththis,sowe
helpswhenOliesonornexttoM. computeSMPL-X’snormalizedelbow-to-wristvector.
Training: During training, we use the di-
- Vertical sampling: Along the remaining rays, we sample
rection vector, d , as condition for both en-
points p = c + k · d in the range k ∈ [k ,k ]. We arm
t dh ee tn ecc ta ps
otij
tv ee nr tt ii ac la ol cra cy
lusj
dr
ei rj
si
f hro inm deth rie ns ge apo bi on
dtj
s yto froth
me1
g str
ao2
nu dn id n, gt .o
Pco (d θ¯e wr b,γ¯Q wb( |Z Z| ,θ
Lw tb
ar, gγ
etw
,βb, wβ
bw
,db, arL
mt ,ar hge
int, t)d
,a wrm
h) erea Znd isthd ee lc ao td ener
t
code, θ is body pose, γ is body translation, β
wb wb wb
-Wiggle-roomdetection: Theabovefocusesonaplausible is the body shape parameters, d arm is the desired arm
arm direction and rough body position but ignores that a direction (new over GNet), L target is the GT wrist joint
body has a certain volume, so its boundary can penetrate as “target” (CIRCLE doesn’t have objects, so we use the
wrist positions as target locations), and h refers to the
nearby receptacles. To resolve this, we “swipe” all pro- int
interactionhand(leftorright). Particularly,weseth =0
jectedfilteredrayswithinasmallrangearoundthevertical int
axis,anddiscardthosethatintersectwithM.
forright-handreachingbodies,whileforleft-handh
int
=1.
We also add a loss between the GT direction, dGT, and
arm
ReachingField formulation: We build a probabilistic thepredictedone,dpred,asfollows,wherew =5:
arm darm
modelforsamplingplausiblereachingdirections/rays.Note
(cid:104) (cid:105)
thatnotallraysareequallylikely; thisdependsontheob- L =w ·E |dGT −dpred| , (2)
darm darm arm arm
ject “height” (distance from the floor). Think of an object
placedveryhigh; humansneedtostretchuptoreachit,so We use CIRCLE [3] data for training, mixed with
reachingdirectionsfrombelowarelikely,whileonesfrom GRAB [44]. Crucially, CIRCLE captures reaching bodies
aboveunlikely. Thelikelihoodofeachrayr is: forawiderrangeoftargetwrists.
i
5Inference: The decoder takes the arm direction, d L . ThesetermsaresimilartoFLEX[48],exceptforthe
arm reg
(from ReachingField), object centroid, L , and param- L andtheL . Wedescribeindetailallourlosstermsin
target fl reg
etersβ andh ,andoutputsaSMPL-Xbody;seeFig.2. Sec.6.4.
wb int
3.4.CGrasp–ControllableGraspingHands L =λ L +λ L +λ L +
opt p p fl fl θ θ
(5)
Ourgoaliscontrollablesynthesis;webuildCGraspbyex- λ gL g+λ rhL rh+λ regL reg
tendingGrabNet[44]toconditiononpalmdirection.
Formulation: The direction from which a hand grasps Search space: We operate in the original search space
objects is important. We provide this to CGrasp as a nor- [24,39],asthisgivesusflexibilitytotuneourmethod. This
malized vector, d ∈ R3. All generated hands need to isincontrastto[48]thatusesacompact“black-box”latent
grasp
have a palm direction that agrees with this vector. To this space at the expense of “giving up” control. Even if our
end,weannotatetwoverticesontheouterpalmofMANO, CReachgeneratesabodyfromadesiredapproachingdirec-
asitisrelativelyrigidsoverticesstayconsistentduringmo- tion, there is a chance the body penetrates the receptacle;
tion; we do this annotation only once. Then, the direction seeFig.7. Startingtheoptimizationfromthatpointmight
vector,d ,isdefinedbytheseverticesandisnormalized. traptheoptimizerinalocalminimum. Topreventthis,we
grasp
Apart from directional controllability we also enhance firsttranslatethebody1malongtheraydirectionto“free”
the spatial awareness of CGrasp by adding an InterField, itfrombigpenetrations,beforestartingoptimization.
as this cannot be easily learned from the few GRAB sub- Optimizer: WeusetheAdamoptimizerandperformit-
jects. Inspired by GNet [45] and others [16, 55], we com- erativeoptimizationfor1500iterations. Tooptimizeasin-
pute3Dhand-to-objectInterFieldvectors,f inter ∈ R99×3. glebodyweneedaround20sec.
In detail, we sample (offline and once) 99 “interaction” Sampleefficiency:Sinceoursynthesizedreachingbody
hand vertices, v hin ,iter,i ∈ {1,...,99}, evenly distributed (CReach) and grasping hand (CGrasp) can be right-away
acrossMANO’sinnersurface. Then,wecompute3Dvec- compatiblew.r.t.eachother,ourmethodcanbeverysample
tors, f inter, encoding the distance and direction from the efficient. We sample in total 1 plausible “reaching” direc-
sampled hand vertices, v hinter, to their closest object ones, tion(ReachingField). Thenweexploitthistoconditionall
v o′. oursynthesis,whilesamplingonly1body(CReach)and1
Training: During training, we add the InterField, guidinghand(CGrasp)perconditioningdirection. Incom-
f inter, to the encoder Q(Z|BPS o,f inter). The decoder parison, baselines [48] need an exhaustive set of 500 hy-
P(θ¯ h,γ¯ h,f¯ inter|Z,BPS o,d grasp) takes the d grasp as input pothesis,duetotheirnon-controllableinference(GrabNet).
andpredictstheparameters(θ¯ ,γ¯ )ofaMANOgrasp,to-
h h Whole body left-hand generation: CWGrasp can
gether with its respective InterField. Z is the latent code
uniquely generate both right- and left-hand whole-body
andBPS istheobjectshape. Wealsoaddalossbetween
o grasps. For generating left-hand ones, we first condition
the GT direction, dGT , and predicted one, dpred, and one
grasp grasp CReach accordingly to get a reaching body that interacts
betweentheground-truthfeatures,f iG nT ter,andthepredicted with the left hand. Then, for the guiding hand grasp, we
ones,fpred:
mirror the sampled ray from ReachingField, generate with
inter (cid:104) (cid:105)
L =(1−c )·E |dGT −dpred| , (3) CGrasp a right hand that grasps the mirrored object, and
dgrasp KL grasp grasp
eventuallymirroragain,toobtainalefthandthatgraspsthe
(cid:104) (cid:105)
L =(1−c )·E |fGT −fpred| , (4) actualobject,whilealsosatisfyingthegivendirection.
finter KL inter inter
wherec KL =0.005isaKL-divergenceconstant. 4.Experiments
Inference: The decoder takes the desired grasp direc-
tion, d (sampled from ReachingField), concatenated WeevaluatethefullCWGraspmethodaswellasitscompo-
grasp
withBPS ,andoutputsaMANOgrasp. nents. InSec.4.1wemeasuretheaccuracyofCReachand
o
CGrasp w.r.t. their input conditions. In Sec. 4.2 we evalu-
3.5.CWGrasp–Whole-BodySynthesis
ate CGrasp and compare it against three SotA hand-grasp
For a 3D object lying on a receptacle mesh in a scene, the pipelines. Finally,weevaluateourfullCWGraspmethodin
goalistogenerateaSMPL-Xbodydexterouslygraspingit, Sec.4.3andSec.4.4andcompareagainstFLEX[48].
whilerealisticallycontactingthescene. Datasets: We evaluate CGrasp on the GRAB [44]
Objective function: We build an objective function dataset,andCWGrasponReplicaGrasp[48].
(Eq. (5)) consisting of a hand-matching term L , a body Baselines: For hand grasping we compare against
rh
pose term L , a head-direction term L (often referred GrabNet [44], ContactGen [31] and DexGraspNet [52].
θ g
to as “gaze” term), a floor-body penetration term L , a For whole-body grasping generation we compare against
fl
receptacle-bodypenetrationtermL andaregularizerterm FLEX[48],whichistheonlyexistingrelevantbaseline.
p
6Figure6. Comparisonforwhole-bodygrasping. Toprow: ResultsofourCWGrasp. Bottomrow: ResultsofFLEX[48]. Fromthebest
10samplesthatFLEXgeneratesweshowthesmallest-lossone.OurCWGraspgenerates1output,yetitproducesmorerealisticgrasps.
MSE↓(cm) Angle(degrees)↓ Samplingtime(sec)↓
CReachRH 4 7.67 0.46
CReachLH 3.6 7.23 0.46
CGrasp - 4.57 0.47
Table 1. Accuracy for condition preserving. We observe
thatCReachandCGraspgeneratebodiesandhands,thataccu-
ratelypreservethedirectionvectorprovidedascondition. We
Figure7.Insomecases(seeleft)theoutputofCReachpenetrates reportCReachperformanceforbothright-andleft-armreach-
withthereceptacleobjectevenifitsarmsatisfiesthesampleddi- ingbodygeneration.Wealsoreportthesamplingtimeforeach
rectionofReachingFieldandthewristisuponthetargetlocation. model.
Toalleviatetheseeffectsbeforeoptimizationwetranslatethebody
1m(seeright)inthedirectionoftheray.
4.2.CGrasp-evaluation
FortheevaluationofCGraspweuse6objectsfromGRAB
4.1.CReachCGrasp-PerformanceEvaluation
dataset and generate 200 grasps per object. Using these
We evaluate the accuracy of CReach and CGrasp w.r.t. to 1200 grasps, we evaluate CGrasp and baselines according
their input conditions. For CGrasp we first extract all the tothefollowingmetrics:
handdirectionsfromthetestsetofGRABdatasetandclus- Contactratio:: Foreachgraspwecalculatethesigneddis-
tertheminto200centersusingK-Means. Wethenusethe6 tance between the hand and object mesh. We search for
testobjectsoftheGRABdatasetandgenerate2000grasps contacts only in the inner surface of the grasp and address
(10 for each direction cluster center) for each object. We acontactwhentheabsolutevalueofthesigneddistanceis
calculatethepalmdirectionofthegeneratedgraspandmea- smallerthan1mm. InFig.8weshowthecontactmapsfor
suretheanglebetweenthenormalizeddirection. InTab.1 allfourmethodsandreportthecontactratioinTab.2.
wereportthemeanangularerror. Penetration percentage: We measure whether the signed
CReach is conditioned not only on hand direction but distance between the hand and object mesh of the ”nega-
alsoon(rightorleft)wristlocation. Wefirstextractallarm tive”pointsislargerthanaspecifiedthreshold,eg. 1mm.
directionsandwristlocations,andusingK-Meansweclus- Penetrationvolume: Toquantifythepenetration, wealso
terbothoftheminto200clusters. Then, usingthecenters calculatethepenetrationvolume. Thisisachievedbyvox-
of these 400 clusters we obtain 40000 different combina- elizing both the hand and the object meshes using voxels
tions of hand directions and wrist locations. We input all ofsize1mm3. Wethencountthenumberofvoxelsinthe
these combinations to CReach and generate 40000 reach- intersectionareabetweenthetwomeshes. Weestimatethe
ingbodiesforeacharm(rightandleft). Forallbodies,we penetrationvolumebymultiplyingthenumberofintersect-
measurethemeanangularerrorofthehanddirection. asin ingvoxelsbythevolumeofasinglevoxel.
CGrasp. Tomeasurethewristlocationaccuracywecalcu- Penetration depth: We first compute the signed distance
latethemeandistancebetweenthegivenwristlocationsand function to check for a possible penetration. If the signed
thegeneratedones;seeTab.1. Last,wereportthesampling distanceisnegative,wetranslatethehanduntilitbecomes
timeforeachmodelinTab.1. positiveandthusmeasurethepenetrationdepth.
7Cont. Penetr. Penetr. Penetr. Hand
ratio perc. vol.↓ depth pose↑
↑ %↓ mm3 mm↓ divers.
GrabNet R ✗ 0.13 0.024 1.27 2.6 0.0672
ContactGen R ✗ 0.09 0.011 1.04 2 0.0675
DexGraspNet O ✗ 0.11 0.001 1.25 1.2 0.0708
CGrasp(ours) R ✓ 0.12 0.029 1.16 2.8 0.0672
Table 2. Quantitative evaluation: CGrasp & SotA (Sec. 4.2).
The “type” column denotes regression (R) or optimization (O)
DexGraspNet ContactGen methods. The “control” column indicates whether a method is
controllableviadirectionalconditioning. Boldindicatesthebest
performance.OurCGraspperformsonparwithestablishedmeth-
ods, while uniquely being controllable via a direction condition.
Thatis,thebenefitofcontrollabilitydoesn’thinderperformance.
4.4.PerceptualStudy(CWGraspvsFLEX)
We sample object-and-receptacle configurations, and for
each one, we generate two whole-body grasps with
CWGrasp and FLEX (referred to as “sample”). For each
GrabNet CGrasp(ours)
sample,weconducttwocomparisonsbyrenderingawhole-
Figure 8. Contact-map comparison: CGrasp & SotA, i.e., bodyviewandazoomed-inviewontothehandandobject.
DexGraspNet[52],ContactGen[31],andContactGen[31]. Con- We randomize the order of visualization. Each sample is
tact likelihood is color-coded via heatmaps; red color denotes a
shownto35participants,whochoosewhichmethodgener-
highlikelihoodandbluecoloralowone. Thirdmethodsinvolve
atesthemostrealisticgrasp. Intotal,weshow28samples,
mostlyfingertips,whileCGraspalsoinvolvespartsofthepalm.
of which 4 are catch trials (2 participants are filtered out).
The70.8%ofparticipantspreferourCWGraspwhencon-
sideringthefull-bodyview,while71.6%prefersCWGrasp
Hand pose diversity: Finally, we are interested to see if
whenconsideringthezoomed-inview,and71.23%prefers
our CGrasp generalizes in terms of hand pose diversity.
CWGraspwhenconsideringbothviews.Thatis,ourresults
For each method, we translate and rotate all the generated
areperceivedassignificantlymorerealisticthantheSOTA.
grasps,sotheyarecenteredtotheoriginandhavethesame
orientation, and then calculate the Euclidean distance be-
5.Conclusion
tween each grasp with all the others. Taking the mean of
all these distances we have a good estimation of the pose WedevelopCWGrasp,amethodthatgenerateswhole-body
diversityofthegrasp. graspsforobjectsthroughnovelcontrollablesynthesis. To
CGrasp performs on par with baselines, but, uniquely this end, we first learn ReachingField, a novel model for
featurescontrollabilityuponthepalmdirection. InFig.13 estimatingdirectionsabodycanapproachtheobjectfrom.
weshowgeneratedgraspsfromallmethods. But current body/grasp generators lack controllability. So,
welearnthenovelCReachmodelthatgeneratesareaching
bodyconditionedonadesiredarmdirection,andthenovel
4.3.CWGrasp-evaluation
CGrasp model that generates a grasping hand conditioned
To test the performance of our CWGrasp we evaluate on onadesiredpalmdirection. WeconditionbothCReachand
the ReplicaGrasp dataset [48]. We describe in detail our CGrasponReachingField’ssampleddirection,toproducea
quantitative evaluation in Sec. 8.1. Notably, FLEX starts handgraspandreachingbodythatarecompatiblewitheach
its optimization using 500 samples and in the end, keeps other. Finally, our CWGrasp method combines these with
the 10 samples with the smallest overall loss. From these only a small refinement, efficiently producing grasps that
10samples,wealwaysusetheonewiththesmallestvalue, areperceivedassignificantlymorerealisticthantheSOTA.
so we evaluate CWGrasp when FLEX reaches its highest Future Work: We tackle right- and left-hand grasps; fu-
performance. In Fig. 6 we present results both from our tureworkwilllookintobi-manualgrasping[16,20,44,55].
CWGraspandFLEX. CWGraspproducesbodiesthatlook Sometimesbodieslook“unstable”whenkneelingdownor
more natural while performing better in terms of grasping stretching up; intuitive-physics reasoning [49] might help.
based on the contact ratio in Tab. 3 and the contact maps Last,willusegeneratedgraspsastargetsformotionsynthe-
inFig.16. WeshowmorequalitativeresultsinSec.8.2. sis[25,45,51,53]tonavigatescenesandgraspobjects.
8
epyT
.lortnoCReferences national Conference on Robotics and Automation (ICRA),
pages1533–1539,1986. 3
[1] V.Allen,B.Curless,Z.Popovic´,andA.Hertzmann. Learn-
[14] M.R.Cutkosky. OnGraspChoice,GraspModels,andthe
ingacorrelatedmodelofidentityandpose-dependentbody
DesignofHandsforManufacturingTasks. Transactionson
shapevariationforreal-timesynthesis.InInternationalCon-
RoboticsandAutomation(TRA),5(3):269–279,1989. 3
ference on Computer Graphics and Interactive Techniques
[15] Markos Diomataris, Nikos Athanasiou, Omid Taheri, Xi
(SIGGRAPH),pages147–156,2006. 3
Wang, Otmar Hilliges, and Michael J. Black. WANDR:
[2] D.Anguelov,P.Srinivasan,D.Koller,S.Thrun,J.Rodgers, Intention-guidedhumanmotiongeneration.InComputerVi-
andJ.Davis. SCAPE:ShapeCompletionandAnimationof sionandPatternRecognition(CVPR),pages927–936,2024.
People. In International Conference on Computer Graph-
3
icsandInteractiveTechniques(SIGGRAPH),page408–416,
[16] Z.Fan, O.Taheri, D.Tzionas, M.Kocabas, M.Kaufmann,
2005. 3
M.J.Black,andO.Hilliges.ARCTIC:ADatasetforDexter-
[3] J. P. Arau´jo, J. Li, K. Vetrivel, R. Agarwal, J. Wu, D. ousBimanualHand-ObjectManipulation. InComputerVi-
Gopinath, A. W. Clegg, and K. Liu. CIRCLE: Capture in sionandPatternRecognition(CVPR),pages12943–12954,
RichContextualEnvironments.InComputerVisionandPat- 2023. 3,6,8
ternRecognition(CVPR),pages21211–21221,2023. 2,3, [17] T. Feix, J. Romero, H. Schmiedmayer, A. M. Dollar, and
5,12 D.Kragic. TheGRASPTaxonomyofHumanGraspTypes.
[4] L. Ballan, A. Taneja, J. Gall, L. Van Gool, and M. Polle- Transactions on Human-Machine Systems (THMS), 46(1):
feys. Motion Capture of Hands in Action Using Discrimi- 66–77,2016. 3
nativeSalientPoints. InEuropeanConferenceonComputer [18] G.Garcia-Hernando, S.Yuan, S.Baek, andT.Kim. First-
Vision(ECCV),pages640–653,2012. 3 Person Hand Action Benchmark With RGB-D Videos and
[5] A.BicchiandV.Kumar. RoboticGraspingandContact: A 3DHandPoseAnnotations.InComputerVisionandPattern
Review. In International Conference on Robotics and Au- Recognition(CVPR),pages409–419,2018. 3
tomation(ICRA),pages348–353,2000. 3 [19] D.M.GavrilaandL.S.Davis. 3-Dmodel-basedtrackingof
[6] S. Brahmbhatt, C. Ham, C. C. Kemp, and J. Hays. Con- humansinaction: amulti-viewapproach. InComputerVi-
tactDB: Analyzing and Predicting Grasp Contact via Ther- sion and Pattern Recognition (CVPR), pages 73–80, 1996.
malImaging. InComputerVisionandPatternRecognition 3
(CVPR),pages8709–8719,2019. 3 [20] A. Ghosh, R. Dabral, V. Golyanik, C. Theobalt, and P.
Slusallek. IMoS: Intent-Driven Full-Body Motion Synthe-
[7] S.Brahmbhatt,A.Handa,J.Hays,andDFox.ContactGrasp:
sisforHuman-ObjectInteractions. InEurographics, 2023.
Functional Multi-finger Grasp Synthesis from Contact. In
3,8
InternationalConferenceonIntelligentRobotsandSystems
(IROS),pages2386–2393,2019. 3 [21] P. Grady, C. Tang, C. D. Twigg, M. Vo, S. Brahmbhatt,
and C. C. Kemp. ContactOpt: Optimizing Contact To Im-
[8] S. Brahmbhatt, C. Tang, C. D. Twigg, C. C. Kemp, and J.
proveGrasps. InComputerVisionandPatternRecognition
Hays.ContactPose:ADatasetofGraspswithObjectContact
(CVPR),pages1471–1481,2021. 3,4
andHandPose.InEuropeanConferenceonComputerVision
[22] S.Hampali, M.Rad, M.Oberweger, andV.Lepetit. HOn-
(ECCV),pages361–378,2020. 3
notate: A Method for 3D Annotation of Hand and Object
[9] Y. Chao, W. Yang, Y. Xiang, P. Molchanov, A. Handa, J.
Poses.InComputerVisionandPatternRecognition(CVPR),
Tremblay,Y.S.Narang,K.VanWyk,U.Iqbal,S.Birchfield,
pages3196–3206,2020. 3
J.Kautz,andD.Fox.DexYCB:ABenchmarkforCapturing
[23] S. Hampali, S. D. Sarkar, M. Rad, and V. Lepetit. Key-
HandGraspingofObjects. InComputerVisionandPattern
pointTransformer: SolvingJointIdentificationinChalleng-
Recognition(CVPR),pages9044–9053,2021. 3
ing Hands and Object Interactions for Accurate 3D Pose
[10] S.Christen,M.Kocabas,E.Aksan,J.Hwangbo,J.Song,and
Estimation. In Computer Vision and Pattern Recognition
O.Hilliges. D-Grasp: PhysicallyPlausibleDynamicGrasp
(CVPR),pages11090–11100,2022. 3
SynthesisforHand-ObjectInteractions. InComputerVision
[24] Mohamed Hassan, Vasileios Choutas, Dimitrios Tzionas,
andPatternRecognition(CVPR),pages20577–20586,2022.
andMichaelJ.Black. Resolving3DHumanPoseAmbigui-
3
tieswith3DSceneConstraints. InInternationalConference
[11] E.Corona,A.Pumarola,G.Alenya,F.Moreno-Noguer,and onComputerVision(ICCV),pages2282–2292,2019. 2,6
G.Rogez. GanHand: PredictingHumanGraspAffordances [25] M. Hassan, D. Ceylan, R. Villegas, J. Saito, J. Yang, Y.
in Multi-Object Scenes. In Computer Vision and Pattern Zhou, and M. J. Black. Stochastic Scene-Aware Motion
Recognition(CVPR),pages5031–5041,2020. 3 Prediction. InInternationalConferenceonComputerVision
[12] Enric Corona, Tomas Hodan, Minh Vo, Francesc Moreno- (ICCV),pages11374–11384,2021. 3,8
Noguer,ChrisSweeney,RichardA.Newcombe,andLingni [26] Y.Hasson,G.Varol,D.Tzionas,I.Kalevatykh,M.J.Black,
Ma. LISA:learningimplicitshapeandappearanceofhands. I.Laptev,andC.Schmid. LearningJointReconstructionof
InComputerVisionandPatternRecognition(CVPR),pages Hands and Manipulated Objects. In Computer Vision and
20501–20511,2022. 3 PatternRecognition(CVPR),pages11807–11816,2019. 3
[13] M.CutkoskyandP.Wright. Modelingmanufacturinggrips [27] H.Jiang,S.Liu,J.Wang,andX.Wang. Hand-ObjectCon-
andcorrelationswiththedesignofrobotichands. InInter- tactConsistency Reasoningfor HumanGrasps Generation.
9In International Conference on Computer Vision (ICCV), [42] S.Sridhar,A.Oulasvirta,andC.Theobalt.InteractiveMark-
pages11107–11116,2021. 3 erless Articulated Hand Motion Tracking Using RGB and
[28] K.Karunratanakul,J.Yang,Y.Zhang,M.J.Black,K.Muan- Depth Data. In International Conference on Computer Vi-
det,andS.Tang. GraspingField: LearningImplicitRepre- sion(ICCV),pages2456–2463,2013. 3
sentations for Human Grasps. In International Conference [43] S.Starke,H.Zhang,T.Komura,andJ.Saito. NeuralState
on3DVision(3DV),pages333–344,2020. 3 MachineforCharacter-SceneInteractions. Transactionson
[29] T.Kwon, B.Tekin, J.Stu¨hmer, F.Bogo, andM.Pollefeys. Graphics(TOG),38(6):1–14,2019. 3
H2O:TwoHandsManipulatingObjectsforFirstPersonIn- [44] O.Taheri,N.Ghorbani,M.J.Black,andD.Tzionas.GRAB:
teractionRecognition. InInternationalConferenceonCom- ADatasetofWhole-BodyHumanGraspingofObjects. In
puterVision(ICCV),pages10138–10148,2021. 3 European Conference on Computer Vision (ECCV), pages
[30] Jiaman Li, Jiajun Wu, and C. Karen Liu. Object motion 581–600,2020. 2,3,4,5,6,8,12,16,17
guided human motion synthesis. In ACM Transactions on [45] O.Taheri,V.Choutas,M.J.Black,andD.Tzionas. GOAL:
Graphics(TOG),2023. 3 Generating4DWhole-BodyMotionforHand-ObjectGrasp-
[31] ShaoweiLiu, YangZhou, JimeiYang, SaurabhGupta, and ing. InComputerVisionandPatternRecognition(CVPR),
ShenlongWang. Contactgen: Generativecontactmodeling pages13263–13273,2022. 2,3,4,5,6,8,14
forgraspgeneration.InProceedingsoftheIEEE/CVFInter-
[46] Omid Taheri, Yi Zhou, Dimitrios Tzionas, Yang Zhou,
nationalConferenceonComputerVision,2023. 3,6,8,16,
Duygu Ceylan, Soren Pirk, and Michael J. Black. GRIP:
17
Generating Interaction Poses Using Spatial Cues and La-
[32] XueyiLiuandLiYi. GeneOHdiffusion: Towardsgeneral-
tentConsistency. InInternationalConferenceon3DVision
izablehand-objectinteractiondenoisingviadenoisingdiffu-
(3DV),2024. 3
sion. InInternationalConferenceonLearningRepresenta-
[47] B.Tekin,F.Bogo,andM.Pollefeys. H+O:UnifiedEgocen-
tions(ICLR),2024. 3
tricRecognitionof3DHand-ObjectPosesandInteractions.
[33] Y. Liu, Y. Liu, C. Jiang, K. Lyu, W. Wan, H. Shen, B.
InComputerVisionandPatternRecognition(CVPR),pages
Liang,Z.Fu,H.Wang,andL.Yi. HOI4D:A4DEgocen-
4511–4520,2019. 3
tric Dataset for Category-Level Human-Object Interaction.
[48] P.Tendulkar,D.Sur´ıs,andC.Vondrick. FLEX:Full-Body
InComputerVisionandPatternRecognition(CVPR),pages
Grasping Without Full-Body Grasps. In Computer Vision
21013–21022,2022. 3
andPatternRecognition(CVPR),pages21179–21189,2023.
[34] M. Loper, N. Mahmood, J. Romero, G. Pons-Moll, and
2,3,6,7,8,14,16,18,19,20,21
M.J.Black. SMPL:ASkinnedMulti-PersonLinearModel.
[49] ShashankTripathi,LeaMu¨ller,Chun-HaoP.Huang,Taheri
TransactionsonGraphics(TOG),34(6):1–16,2015. 3
Omid,MichaelJ.Black,andDimitriosTzionas. 3DHuman
[35] D.MarrandH.K.Nishihara.Representationandrecognition
Pose Estimation via Intuitive Physics. In Computer Vision
ofthespatialorganizationofthree-dimensionalshapes.Pro-
and Pattern Recognition (CVPR), pages 4713–4725, 2023.
ceedingsoftheRoyalSocietyofLondon.SeriesB.Biological
8,14
Sciences,200(1140):269–294,1978. 3
[50] D. Turpin, L. Wang, E. Heiden, Y. Chen, M. Macklin, S.
[36] A.T.Miller,S.Knoop,H.I.Christensen,andP.K.Allen.Au-
Tsogkas,S.Dickinson,andS.Garg.Grasp’D:Differentiable
tomatic grasp planning using shape primitives. In Inter-
Contact-RichGraspSynthesisforMulti-FingeredHands. In
national Conference on Robotics and Automation (ICRA),
European Conference on Computer Vision (ECCV), pages
pages1824–1829,2003. 3
201–221,2022. 3
[37] I. Oikonomidis, N. Kyriazis, and A. A Argyros. Efficient
model-based3dtrackingofhandarticulationsusingKinect. [51] Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, and Xi-
InBritishMachineVisionConference(BMVC),pages1–11, aolongWang. SynthesizingLong-Term3DHumanMotion
2011. 3 andInteractionin3DScenes. InComputerVisionandPat-
[38] AhmedA.A.Osman,TimoBolkart,DimitriosTzionas,and ternRecognition(CVPR),pages9401–9411,2021. 3,8
MichaelJ.Black. SUPR:ASparseUnifiedPart-BasedHu- [52] Ruicheng Wang, Jialiang Zhang, Jiayi Chen, Yinzhen Xu,
man Body Model. In European Conference on Computer Puhao Li, Tengyu Liu, and He Wang. Dexgraspnet: A
Vision(ECCV),2022. 3 large-scale robotic dexterous grasp dataset for general ob-
[39] G.Pavlakos,V.Choutas,N.Ghorbani,T.Bolkart,A.A.A. jectsbasedonsimulation. In2023IEEEInternationalCon-
Osman,D.Tzionas,andM.J.Black. ExpressiveBodyCap- ferenceonRoboticsandAutomation(ICRA),pages11359–
ture: 3DHands, Face, andBodyFromaSingleImage. In 11366,2022. 3,6,8,16,17
ComputerVisionandPatternRecognition(CVPR),2019. 2, [53] Y.Wu,J.Wang,Y.Zhang,S.Zhang,O.Hilliges,F.Yu,and
3,4,6 S.Tang.SAGA:StochasticWhole-BodyGraspingwithCon-
[40] S. Prokudin, C. Lassner, and J. Romero. Efficient Learn- tact. InEuropeanConferenceonComputerVision(ECCV),
ingonPointCloudsWithBasisPointSets. InInternational 2022. 3,8
ConferenceonComputerVision(ICCV),pages4332–4341, [54] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir,
2019. 3,4 WilliamT.Freeman,RahulSukthankar,andCristianSmin-
[41] J.Romero,D.Tzionas,andM.J.Black. Embodiedhands: chisescu.GHUM&GHUML:Generative3DHumanShape
Modelingandcapturinghandsandbodiestogether.Transac- andArticulatedPoseModels. InComputerVisionandPat-
tionsonGraphics(TOG),36(6):1–17,2017. 2,3,4 ternRecognition(CVPR),pages6183–6192,2020. 3
10[55] H. Zhang, Y. Ye, T. Shiratori, and T. Komura. ManipNet:
Neural Manipulation Synthesis with a Hand-Object Spatial
Representation. Transactions on Graphics (TOG), 40(4),
2021. 3,6,8
[56] H. Zhang, J. Tang, S. Sun, and Lan. X. Robotic Grasping
from Classical to Modern: A Survey. arXiv:2202.03631,
2022. 3
[57] Hui Zhang, Sammy Christen, Zicong Fan, Otmar Hilliges,
andJieSong.GraspXL:Generatinggraspingmotionsfordi-
verseobjectsatscale.InEuropeanConferenceonComputer
Vision(ECCV),2024. 3
[58] MengqiZhang, YangFu, ZhengDing, SifeiLiu, Zhuowen
Tu, and Xiaolong Wang. Hoidiffusion: Generating realis-
tic3dhand-objectinteractiondata. InComputerVisionand
PatternRecognition(CVPR),pages8521–8531,2024. 3
[59] X.Zhang,B.L.Bhatnagar,S.Starke,V.Guzov,andG.Pons-
Moll.COUCH:TowardsControllableHuman-ChairInterac-
tions.InEuropeanConferenceonComputerVision(ECCV),
pages518–535,2022. 3
[60] K. Zhou, B. L. Bhatnagar, J. E. Lenssen, and G. Pons-
Moll. TOCH:Spatio-TemporalObject-to-HandCorrespon-
denceforMotionRefinement. InEuropeanConferenceon
ComputerVision(ECCV),pages1–19,2022. 3,4
[61] KeyangZhou,BharatLalBhatnagar,JanEricLenssen,and
Gerard Pons-Moll. GEARS: Local geometry-aware hand-
objectinteractionsynthesis.InComputerVisionandPattern
Recognition(CVPR),pages20634–20643,2024. 3
[62] C.Zimmermann,D.Ceylan,J.Yang,B.Russell,M.Argus,
andT.Brox. FreiHAND:ADatasetforMarkerlessCapture
ofHandPoseandShapeFromSingleRGBImages. InIn-
ternational Conference on Computer Vision (ICCV), pages
813–822,2019. 3
113D Whole-body Grasp Synthesis with Directional Controllability
Supplementary Material
6.Implementationdetails simulate all these we rotate clockwise the remaining rays
aroundtheverticalaxisforanangleof30o,whenwework
Inthissection, wedescribeindepthallthecomponentsof
with right-hand interactions. For left-hand interactions we
ourpipeline. WestartfromourReachingFieldinSec.6.1,
apply a counterclockwise rotation to the rays. We discard
wherewepresentstep-by-stephowwebuildthisprobabilis-
the rays that penetrate the receptacle M during this rota-
tic model. In Sec. 6.2 and Sec. 6.3 we refer to the main
tion. We choose this angle because after experiments we
novelties of CReach and CGrasp respectively. Finally in
found that the body volume in most cases could fit in this
Sec.6.4weexplainhowweimplementthefinaloptimiza- 30oanglerange.
tion stage of our pipeline, by describing all the losses and
WemodelourReachingFieldwithaprobabilitydistribu-
thehyperparametersweuse.
tion,asnotallraysarethesamelikelytodenoteapotential
plausible reaching direction. As a ray is described by its
6.1.Reaching-Field
direction andits origin, we wanted ourdistribution to take
Weexplainanalyticallythestepswefollowedforobtaining into account both of these factors. Particularly, we are in-
the Reaching-Field and also provide a visual depiction of terested in the upward or downward orientation of the ray
each step in Fig. 10. To cover all possible scenarios, we and the height of its origin. Using both of these we form
provideexamplesfordifferentobjectheights,rangingfrom a multinomial distribution conditioned to the ray’s origin
lowtohigh.Givenanobject(seeFig.10A.),wefirstdefine height. Takingintoaccountthisheightisveryimportantas
asphericalgridaroundtheobject(seeFig.10B.),andcast itdeterminestheheightoftheinteraction. Usingtheheight
raystowardsallthedirectionsthatareformedbetweenthe ofinteractionwedecidewhichraysweshouldkeepfroma
centeroftheobjectandthesampledpointsonthespherical groupofrays. Forexample,whentheheightofinteraction
grid (see Fig. 10 C.). We then follow a filtering process ishighthehumanbodyshouldbeatalowerlevel. Asare-
whichincludesthefollowingsteps: sultrayswithadownwardorientationshouldhaveahigher
probability, while the probability for upward rays should
Primary Check: First, we only keep rays that do not bealmostzero.Oppositelywhentheheightofinteractionis
intersectwiththereceptacle,asshowninFig.10D. lowwewanttoassignhigherprobabilitytoupwardraysand
low probability to downward or parallel rays. This means
Secondary projection: For cases, where the primary that if an objectlies on the floor it ismore likely for a hu-
check is not enough(see example in the middle column mantotilttowardtheobject,orbendtheirkneesandgrasp
of Fig. 10), we project the already filtered rays to the itthanlyingonthefloor. Forthein-betweenheightsofin-
horizontalplaneandcheckagainforintersectionswiththe teraction,wewantaboveaspecificthresholdT ourraysto
receptacle,asillustratedinFig.10E. followtheformerpatternandbelowthatthresholdthelatter
one. Weachievethisbyassigningtoeachraytheprobabil-
Verticalsampling: Bysamplingpointsacrosstheremain- ityofEq.(1).
ing rays and casting vertical rays towards the ground (see
6.2.CReach
Fig. 10 F.), we detect potential occlusion regions that lie
beneatharayandpreventahumanfromstandingthereand Dataset: TotrainourCReachweusetogetherGRAB[44]
approachingtheobjectinthatraydirection. and CIRCLE [3] data. GRAB dataset is limited mainly to
interactionsfromstraightpositions,whilewewantCReach
Wiggle-room detection: Even if a ray satisfies all the tocaptureinteractionsfromanyposition. Thefactthatthe
aboveconditions, itisstillpossibleahumancouldnotuse CIRCLEdatasetcapturesawiderangeofinteractionsfrom
ittoapproachtheobject. Thisishappeningbecausecasting variouspositionswasagoodmotivationforustouseitdur-
rays does not take into account the humans’s volume. To ing CReach training. Both datasets capture both right and
interact with an object O without penetrating the recepta- left-handinteractions.
cle M, humans have to position their bodies accordingly. To train a single network for both interactions our first
Furthermore,insomecaseswhentheywanttousetheright step was to identify these interactions in both datasets, as
or the left hand for an interaction it is more convenient to there isn’t any pertinent annotation. We achieve this by
approachtheobjectfromaparticulardirection. Usually,we using the fact that when we interact with a hand we look
approachanobjectfromitsrighttointeractwiththeright- towards that hand. So for each human body, we calculate
hand and from its left to interact with the left-hand. To threevectors. First,wecalculatethegazevectorg byus-
dir
12Figure9. CReachoverview. WetrainaCVAEthatisabletogeneratebothrightandleft-handreachingbodieswiththesamenetwork.
Theencodertakesasinputthebodyposeparametersθ ,thebodytranslationγ ,thewristjointlocationL (oftheright-handorthe
wb wb target
left-hand),thearmdirectiond (oftheright-handortheleft-hand)andthebodyshapeparametersβ . Thedecodertakesasinputthe
arm wb
latentcodeZ,thewristjointlocationL ,thearmdirectiond ,thebodyshapeparameters,andtheinteractionhandflagh . The
target arm int
decodergeneratesthebodyposeparametersθ¯ andthetranslationγ¯ ofaSMPL-Xbody.
wb wb
ingverticesinthebackandthefrontofthehead, andalso 6.4.Optimization
we calculate the vectors from the center of the eyes to the
During the optimization our goal is to optimize over the
rightandleftwristjoints. Wedenotethesevectorsasd
erw SMPL-X pose parameters, the translation, and the global
andd respectively. Aftercalculatingtheanglesbetween
elw orientation of the body, so that the right hand of the body
thosevectorsandg wefollowtheconditionofEq.(6)to
dir aligns with the generated right hand of the CGrasp, while
identifywhetheraninteractionofourdatasetusestheright
some additional constraints (e.g. no penetrations with the
ortheleft-hand.
receptaclemesh,contactwiththeground)arealsosatisfied.
InEq.(5)wegiveouroptimizationfunction:
(cid:92) (cid:92)
g d ≤g d :right-handinteraction
dir erw dir elw
(6)
L =λ L +λ L +λ L +λ L +λ L +λ L
(cid:92) (cid:92) opt p p fl fl θ θ g g rh rh reg reg
g d >g d :left-handinteraction
dir erw dir elw
After we identify the hand of interaction, we annotate ThefirsttermL pisthepenetrationloss,betweenthehuman
theright-handinteractionswith0andtheleft-handinterac- bodyandthereceptacle.Itconsistsoftwoterms,thatdefine
tionswith1.Becausetheright-handinteractionsweremuch anintersectionandconnectionlossasfollows
morefrequentthantheleft-hand,wemirrorthewholecom-
bination of GRAB and CIRCLE data so we have an equal
numberofinteractionsbetweenthetwohands. InFig.9we 1 N (cid:88)Vb
L = |min(0,d(V ,M))|,
depict the architecture of our CReach, while in Fig. 2 you pinter N bi
canseebodiesgeneratedfromourCReachusingeitherthe
Vb
i=1
rightorthelefthand. (7)
Ndisc
6.3.CGrasp 1 (cid:88)Vb
L = d(Vdisc,M))
OurCGrasp is also a CVAE and has two key components. pcon N Vb
i=1
bi
First,weuseanInterFieldasinputtotheencoderandcon-
ditionthedecodertoadirectionvector. InFig.11wegive d(·) denotes the signed distance function between the ver-
avisualrepresentationoftheInterField. InFig.12wede- ticesofthebodyV andthereceptaclemeshM. Usingthe
b
pictgeneratedgraspsofourCGraspforthe6testobjectsof firsttermwepenalizetheverticesthatintersectwiththere-
the GRAB dataset, together with the corresponding direc- ceptacle M and are inside its volume. On the other hand,
tionvectorsusedasconditions. Inallcases,thedirectionof usingthesecondtermwepenalizetheparts-verticesofthe
thegraspfollowsthegivendirectionvector. humanbodythataredisconnectedfromtherestofthebody.
13This happens when a part of the receptacle M intervenes
betweenthemandtherestofthebody.
ForthefloorlossL weusetheunder-groundtermof[49].
fl
First, wefindtheheighth(V )oftheverticesofthebody
bi
withrespecttothegroundplane. Thenthefloorlossisde-
finedas:
(cid:18)
h(V
)(cid:19)2
L =β tanh bi (8)
underground 1 β
2
Wesetβ =1andβ =0.15similarwith[49]. According
1 2
to [49] this loss is only for vertices that penetrate with the
floor(h(V ) < 0). Whenh(V ) > 0weuseasfloorloss
bi bi
theEq.(9).
L =|min(V )|,i=1···N . (9)
fl biz Vb
Both Eq. (8) and Eq. (9) are responsible for keeping our
bodiesontheground. However,theproblemwithEq.(9)is
that it can not obtain actual ground contact with the floor,
which is desirable. FLEX [48] during optimization sets
Eq. (9) for each iteration to zero to ensure that the gener-
ated body would touch the ground. The problem with this
termisthatitdoesn’tensurethatmoreverticesofthebodies
would have contact with the floor. In some cases, such as
whenwestandonthetipofourtoethisisdesirable,butin
othercases,whenwestandorkneelweneedsomethingdif-
ferent.UsingEq.(8)asourfloorlossweareabletoachieve
this.
To ensure that the generated body has eye contact with
theobjectofinteractionweusethegazelossthatwasintro-
ducedin[45]. Asthe3Dpositionoftheinteractionobject
Oisknown,wespecifytwoverticesAandBintheheadof
thebody. ThevertexAisonthebackofthehead,whilethe
vertexB isinfrontandspecificallyintheareaoftheeyes.
−−→
Using the three points O,A,B we define the vectors AO
−→
andBOandspecifythegazelossastheirin-betweenangle,
seeEq.(10).
(cid:32) −−→−→ (cid:33)
AOBO
L g =cos−1 −−→ −→ . (10)
|AO ||BO |
AsweoptimizeovertheSMPL-Xposeparametersθwe
shouldsomehowensurethatthepredictedposeswouldre-
main in the full pose space of our CReach. This is very
important, because if we deviate from this manifold then
our predictions might not result to humans. FLEX [48]
doesn’t need a such loss because it leverages the Vposer
fromSMPL-X,andsamplesbodyposeparametersdirectly
fromitslatentmanifold. Inourcase,weusethebodypose
loss in Eq. (11) that is responsible for keeping our body
poseparameterspredictionsθˆinsidethefullposespaceof
CReach.
Figure 10. The ReachingField for three different object heights.
L =∥θ−θˆ∥2 (11)
θ Inthelastrow,weprovideavisualrepresentationoftheReaching
Field,whileintheaboverows,weshowallthestepswefollowto
obtainthisprobabilisticdirectionfield.(cid:252)Zoomtoseedetails.
14Figure 11. Visual representation of the InterField. We depict with green color the 99 sampled “interaction” hand vertices, vinter,i ∈
h,i
{1,...,99},evenly-distributedacrossMANO’sinnersurface. Withorangecolor,wedepictthe3Dvectors,f ,thatencodeboththe
inter
distanceanddirectionfromthesampledhandvertices,vinter,totheirclosestverticesontheobject,v′.(cid:252)Zoomtoseedetails.
h o
Figure 12. Qualitative results of our CGrasp, together with the given direction vectors. For the six test objects in the GRAB dataset,
wegeneratethreegraspsperobjectusingdifferentdirectionvectorsasconditions. Theconditioningdirectionvectorsareshownbygold
arrows.(cid:252)Zoomtoseedetails.
Usingthislosstheoptimizeradaptsthebodyposetothe istic effect we noticed during our experiments, and we fix
givenenvironmentandasaresult,wecouldgeneratenovel itbypenalizingdeviationsfromtheinitialbodypostureof
plausible body poses. However, in some cases using only CReach. Particularly, we calculate the normalized vector
this loss is not enough. That’s why we add a regularizer betweenthecenterofthefeetF andthepelvisjointP. We
loss term L . In Fig. 15 we depict on the left a failure take as the center of the feet the center between the right
reg
case when we do not use our L . The problem is that andleftanklejoints. Aswedonotwanttohavedeviations
reg
using only our body pose loss we can not ensure that the fromthisvectorduringoptimization,wecalculatethisvec-
orientation of the final body will be realistic. Even if the torateachoptimizationstepandminimizeitsanglewiththe
predicted pose parameters θˆbelong to our CReach latent initialvector− F−→ P. InEq.(12)wegiveourL loss.
reg
manifold,duetoEq.(11),wedonothaveanycontrolover
theglobalorientationofthebody. InFLEXtheyalsorefer
−−→−−ˆ→
to a similar problem and they alleviate it by using a pose L =cos−1(FPFP) (12)
reg
ground prior that ensures the bodies will have a realistic
orientation. Because they start with a random body, with-
We use our L loss when the height of the object is
out this prior their optimization can generate flying bodies reg
over a specific threshold. When the object is below this
that hover horizontally over the ground or have extremely
threshold,the’tilt’effectcannotoccur,asthebodycanonly
unrealistic orientations. However, in CWGrasp that is not
kneel or bend down very low to grasp the object. In these
needed,asourCReachgeneratesaninitialbodythatneeds
cases, we remove this term so our pipeline has more free-
onlysmallrefinement. This”tilt”resultistheonlyunreal-
domduringthegenerationprocess.
15To ensure that the body’s right hand is going to be contains192differentconfigurations. Forourexperiments,
alignedwiththeCGrasp’sgeneratedrighthandweusethe we use the 6 test set objects from GRAB and other 6 ran-
lossL ,wherewecomparetheverticesofCGrasp’sright- domlysampledfromthetrainset. Foreachoneofthese12
rh
handwiththeverticesofthepredictedrighthand,afterex- objectswerandomlychoose20configurations. Weusethe
tracting from the predicted body. We also make a similar sameconfigurationsbothforCWGraspandFLEX. Forthe
comparisonbetweentheverticesofCGrasp’swristandthe quantitative evaluation of CWGrasp in Sec. 8.1 we calcu-
predictedwrist. InEq.(13)wespecifybothoftheseterms. late five metrics for both pipelines and also we conduct a
perceptual study. In Sec. 8.2 we have included qualitative
comparisonsbetweenCWGraspandFLEX.
L =∥V −Vrˆh ∥2+λ ∥V −Vwˆrist∥2 (13)
rh rh body wrist wrist body
WeuseAdamoptimizerwithalearningrateof0.01.Our 8.1.CWGrasp–Quantitativeevaluation
optimization runs in two stages. During the first stage we
optimizeoveralltheabovelossesfor800iterations. After For our quantitative evaluation, we calculate for each
these 800 iterations, we exclude the penetration loss from method the penetration percentage between the body B
the objective function and optimize using the rest losses andthereceptacleMandalsobetweentheright-handRH
over the pose parameters of the right shoulder, elbow, and and the object O. We also calculate the contact ratio be-
wrist. To keep these pose parameters inside the full pose tween the right-hand RH and the object O, and the body
spaceweuseforeachjointasimilarlosswithEq.(11). The diversity across the generated bodies. Finally, we mea-
maingoalofthesecondoptimizationstepistoimproveand sure the average optimization time of each method. In
refinethehandgrasp. Tab.3wereportourresults. Wecalculateallthesemetrics
7.CGraspEvaluation
W itae tive ev lyalua at ge aio nu str C thG eras Sp otb Aoth baq su ea lin nt eit sativ Ge rl ay bNan ed
t
q [4u 4al ]-
,
Samples Iters Pe Bne −t. M Pe Rne Ht. −O Con Rt Hac −t O BodyDiv. Time(s)
# # ↓ ↓ ↑ ↑ ↓
ContactGen[31]andDexGraspNet[52]. Wehaveusedthe
FLEX 500 - 0.3 1.16 14.67 63.86 357
6objectsoftheGRABdatasetandforeachobject,wehave
CWGrasp 1 1500 0.7 0.7 28.92 61.77 23
generated200graspswitheachmethod.ForourCGraspwe
Table 3. Quantitative evaluation between FLEX and CWGrasp.
needalsodirectionvectorsasinput.Thatiswhywefirstex-
Bold indicates the best performance. We report the penetration
tractedforeachtestobjectthecorrespondingdirectionvec-
percentage for both the whole body and the right-hand, as well
tors from the GRAB test set, and then sampled randomly
as the contact ratio between the right-hand and the object. We
from them to feed our CGrasp. Our CGrasp performs on
providealsothebodydiversityandtheaverageoptimizationtime.
parwithallthesebaselinesaccordingtoTab.2,whilepro-
vidingtheuserwithcontrollabilityoverthegraspdirection.
Fromthe1200graspsthatwegenerateforeachmethod,we as we do in Sec. 4.2 for the quantitative evaluation of our
calculate the corresponding contact map. Taking a look at CGrasp. Afterextractingtheright-handfromthegenerated
thecontactmapsinFig.8weobservethatourbaselinesin- bodywecalculatealsothecontactmapsforbothCWGrasp
volve mostly the fingertips, whereas CGrasp activates also andFLEX,seeFig.16.OurCWGraspgeneratesgraspsthat
partsofthepalm. InFig.13weincludequalitativeresults activatealmostthewholepalm,whileFLEXgraspstendto
forallfourmethods. use only the tips of the fingers. CWGrasp is superior to
FLEX in almost all metrics and performs on par in body
8.CWGraspevaluation
posediversityandbody-receptaclepenetration.Thebiggest
advantage of CWGrasp is its better performance at a very
WeevaluateourCWGraspbothquantitativelyandqualita-
lowcomputationtimeandbudget.
tively against FLEX [48]. For the evaluation, we use the
ReplicaGrasp dataset that was introduced by [48]. This Perceptual study: To further evaluate our method we
datasetconsistsofdifferentreceptaclesthatsupportthecon- conduct a perceptual study. In Fig. 17 we include the de-
tact meshes from the GRAB dataset under different con- scriptionofourperceptualtogetherwithwhatweaskfrom
figurations. Each configuration consists of one receptacle our participants. In Fig. 18 we include some images that
and one contact mesh. The location and orientation of the correspond to the samples we used in our study. Most of
receptaclemeshesarefixedacrossallconfigurations. How- theparticipantspreferCWGraspconsideringeitherthefull-
ever,thelocationandorientationofthecontactmeshesvary, bodyview,orthezoomed-inview,orevenwhenconsider-
makingeachconfigurationuniqueacrossthewholedataset. ing both views. The result of our study is that CWGrasp
Foreachoneofthe50GRABdatasetobjectsReplicaGrasp generatesmorerealisticresultsthantheSOTA.
16Figure13.Comparisonofhand-onlygraspsynthesismodels:DexGraspNet[52],ContactGen[31],GrabNet[44],andCGrasp(ours).Each
rowshowsgraspsgeneratedbyadifferentmodelforthesameobjectset(binoculars,camera,fryingpan,mug,toothpaste,wineglass).
8.2.CWGrasp–Qualitativeevaluation
In this section, we include many qualitative comparisons
between our CWGrasp and FLEX, and also present addi-
tional results of our pipeline for left-hand grasping bod-
ies. In Fig. 19, Fig. 20, Fig. 21 we present examples us-
ing CWGrasp and FLEX. For each example, we depict
boththefullbodyandtheright-handgrasp. CWGraspgen-
erates more realistic and natural bodies and also performs
betterinthegraspingpart. InmanycasesFLEXbodiesdo
not touch the object, which also explains the difference in
contact maps between the two methods, see Fig. 16. Fi-
nally, for these examples, we can observe that FLEX bod-
Figure14.FailurecasesofourCWGraspmethod.
ies approach the object from random directions, or have
weirdorientations(thebodydoesnot“lookat”theobject),
Failure cases: In Fig. 14 we provide failure cases of our
see Fig. 19 second line, Fig. 20 fourth line, Fig. 21 fourth
CWGraspmethod. Thereachingarmandhandlookplausi-
line. Oppositely, this is not happening with CWGrasp as
ble,exceptforthebottom-leftcasewherethearmpenetrates
weleveragethedirectioninformationfromReachingField.
thereceptacle. Thelattershowsthatoccasionallysampling
Inmorecomplexenvironmentswithwallsandmorefurni-
our ReachingField might fail, so subsequent optimization
ture, we believe that CWGrasp would perform even better
can be trapped in a local minimum, however, empirically,
thanFLEX,andourdirectioncontrollabilitywouldgiveus
this doesn’t happen often. In all other cases, the body or
agreatadvantage.InFig.22wedepictwholegraspingbod-
legsgettrappedinalocalminimum. Thiscouldbetackled
iesthatinteractwiththeright-hand. AsFLEXislimitedto
withaninvolvedmodelingofthefullscene, butthisisout
only right-hand whole grasping bodies generation for the
ofourscopehere,soweleaveitforfuturework.
left-handcasewepresentresultsonlyfromourpipeline.
17Figure15.Left:FailurecasewhenwedonotuseourregularizertermL inourlossfunction.Insomecases,whenL isnotused,our
reg reg
modelpreferstogeneratebodiesthattiltunrealisticallytowardtheobjecttopreventpenetrationswiththereceptacleM,whilereaching
theobjectwiththehand.Right:ThecorrespondingoutputofCWGraspwhenweuseourL lossterm.
reg
FLEX[48] CWGrasp(ours)
Figure 16. Contact likelihood visualization for FLEX and Figure 17. Perceptual study protocol: We ask 35 participants
our CWGrasp method, using a heatmap color-coding. Red to observe grasps generated by two methods for 28 object-and-
colordenoteshighlikelihood,andbluedenoteslowlikelihood. receptacleconfigurations, andtospecifywhichgraspismorere-
CWGrasp“activates”abiggerpartofthepalmforinteraction, alistic in terms of natural pose, realistic hand grasp, and overall
whileFLEXusesmainlythetipsofthefingers. interactionrealism.(cid:252)Zoomintoseedetails.
Figure18. Examplesfromourperceptualstudy. ThefirstrowshowsgraspsgeneratedbyourCWGraspmethod,whilethesecondrow
showsgraspsgeneratedbyFLEX[48].Foreachsample,weshowthefullbodyand“scene,”aswellasacloseupontothehandandobject
(fromadifferentview).(cid:252)Zoomintoseedetails.
18Figure 19. Qualitative comparison of our CWGrasp method against FLEX [48]. The first and third row show results generated by
CWGrasp,whilethesecondandfourthoneshowresultsgeneratedbyFLEX,forthesameobjectandreceptacleconfigurations.Foreach
graspwedepictboththefull-body(withgraycolor)and“scene,”aswellasaclose-upontothehandandobject(withblueandredcolor,
respectively,andfromadifferentview). ForFLEX,outofallitsgeneratedsamples,wevisualizetheonewiththesmallesttotalloss. In
contrast,ourCWGraspgeneratesonlyonesample.
19Figure 20. Qualitative comparison of our CWGrasp method against FLEX [48]. The first and third row show results generated by
CWGrasp,whilethesecondandfourthoneshowresultsgeneratedbyFLEX,forthesameobjectandreceptacleconfigurations.Foreach
graspwedepictboththefull-body(withgraycolor)and“scene,”aswellasaclose-upontothehandandobject(withblueandredcolor,
respectively,andfromadifferentview). ForFLEX,outofallitsgeneratedsamples,wevisualizetheonewiththesmallesttotalloss. In
contrast,ourCWGraspgeneratesonlyonesample.
20Figure 21. Qualitative comparison of our CWGrasp method against FLEX [48]. The first and third row show results generated by
CWGrasp,whilethesecondandfourthoneshowresultsgeneratedbyFLEX,forthesameobjectandreceptacleconfigurations.Foreach
graspwedepictboththefull-body(withgraycolor)and“scene,”aswellasaclose-upontothehandandobject(withblueandredcolor,
respectively,andfromadifferentview). ForFLEX,outofallitsgeneratedsamples,wevisualizetheonewiththesmallesttotalloss. In
contrast,ourCWGraspgeneratesonlyonesample.
21Figure 22. Qualitative results of CWGrasp when using the left hand. Note that CWGrasp is unique in generating both right-hand and
left-handwhole-bodygrasps,whileperformanceissimilarforbothcases.
22