A Gradient Analysis Framework for Rewarding Good
and Penalizing Bad Examples in Language Models
Yi-LinTuan WilliamYangWang
UniversityofCalifornia,SantaBarbara UniversityofCalifornia,SantaBarbara
ytuan@cs.ucsb.edu william@cs.ucsb.edu
Abstract
Beyondmaximumlikelihoodestimation(MLE),thestandardobjectiveofalan-
guagemodel(LM)thatoptimizesgoodexamplesprobabilities,manystudieshave
exploredwaysthatalsopenalizebadexamplesforenhancingthequalityofout-
putdistribution,includingunlikelihoodtraining,exponentialmaximizingaverage
treatment effect (ExMATE), and direct preference optimization (DPO). To sys-
tematicallycomparethesemethodsandfurtherprovideaunifiedrecipeforLM
optimization,inthispaper,wepresentauniqueangleofgradientanalysisofloss
functions that simultaneously reward good examples and penalize bad ones in
LMs. Through both mathematical results and experiments on CausalDialogue
andAnthropicHH-RLHFdatasets,weidentifydistinctfunctionalcharacteristics
amongthesemethods. WefindthatExMATEservesasasuperiorsurrogatefor
MLE,andthatcombiningDPOwithExMATEinsteadofMLEfurtherenhances
boththestatistical(5-7%)andgenerative(+18%winrate)performance.
1 Introduction
The optimization of language models (LM) has long relied on maximum likelihood estimation
(MLE)[6,11,29]. WhileMLEaimstoconcentrateprobabilitydistributionsoncorrecttokensateach
timestep,thisapproachhasinherentlimitations. Solelyoptimizingforcorrectexamplescanleadto
over-optimismonthereferredtoken[14]andunintendeddistribution(suchasuniform)overunused
tokens,regardlessofthedatascale. Consequently,aparadigmshifthasoccurred,recognizingthe
needtoconsiderbothpositiveandnegativeexamplesinLMoptimization.
Toaddresstheshortcomingsofexclusivelyrewardingcorrectdata,novelstrategieshaveemerged,
originatingfrombinaryclassifiers[26]andextendingtosequentialmulti-classclassifierslikeLMs.
Techniquessuchasunlikelihoodtraining[35]andexponentialmaximizingaveragetreatmenteffect
(ExMATE) [32] introduce distinct loss functions and negative sample constructions to mitigate
issueslikerepetitionintextgenerationandenhancemodelresponseagility. Meanwhile,generative
adversarialnetworks(GANs)forLMs[30,36]andreinforcementlearningfromhumanfeedback
(RLHF)[21,37]eitherdirectlytakesmachinegenerationasnegativedataorfurtherannotatesprefer-
encebyhumanstooptimizethemodelviaGANorRLframeworks[9,25,27,31]. Recently,direct
preferenceoptimization(DPO)[23]streamlinestheRLHFapproachintoasupervisionlossobjective,
significantlyreducingcomputationalcostswhilemaintainingefficacy. Theseapproachescollectively
signify a broader shift towards optimizing LMs by simultaneously increasing the probability of
preferreddataanddecreasingtheprobabilityofdislikeddata.
Inthispaper,weaimtosystematicallycompareLMoptimizationmethodsthatsharetheprinciple:
rewardinggoodandpenalizingbadexamples. Specifically,weaddressthefollowingquestions: (1)
WhataretheessentialdifferencesamongthesemethodsinLMoptimization? (2)Whichmethod
is more suitable for each scenario? (3) Can we identify a superior optimization recipe based on
mathematicalanalysis? Toanswerthesequestions,weproposeagradientanalysisapproachtailored
Preprint.
4202
guA
92
]LC.sc[
1v15761.8042:viXranegative
s pa om sip til ve es sa mp o ps leit sive ne sg aati mv ple es sa mp o ps leit sive
n se ag ma pti lv ee
s
samples
(a) DPO (b) Unlikelihood (c) ExMATE
Figure 1: (a) DPO, (b) Unlikelihood, and (c) ExMATE loss functions when taking only either
P (y+|x+)(positiveexamples)orP (y−|x−)(negativeexamples)asthecontrolvariables. Weplot
θ θ
DPOinthecaseofP (·)=1,β =1,andP (y−|x−)orP (y+|x+)is0.1foreasyvisualization.
ref θ θ
Theirfunctioncharacteristicsaredifferent,thusmakingthemsuitablefordifferenceusecases.
forfrequentlyencounteredLMscenarios,enablingustomathematicallyestimatehoweachrewarding-
good-penalizing-bad(RGPB)methodupdatestheLMoutputdistributionandelucidatetheirdistinct
properties. Additionally, we conduct experiments on datasets such as CausalDialogue [32] and
AnthropicHH-RLHF[3],employingevaluationsusingstatisticalmetricsandGPT4assessmentsto
verifyourmathematicalfindingsandvalidatethepracticalimplicationsofourresearch.
2 RelatedWork
Asrewardinggoodexamplesandpenalizingbadoneshasbeenonewidely-usedframeworkinLM
research, we discuss two major differences in these works: (1) the method to construct negative
dataand(2)themethodtooptimizetheLMusingthenegativedata. Moreover,wediscuss(3)the
differenceofthemfromotherlinesofresearch,e.g.,contrastivelearning,thatisoftendeemedsimilar.
Negative Data Construction. Word2Vec [8, 19], which aims to strengthen word embeddings,
performsnegativesamplingbyintentionallyselectingincorrectpositionsforaword. Unlikelihood
training[17,35],whichaimstopreventrepetitionsintextgeneration,usesalreadypredictedwordsin
thecontextasnegativesamples. ExMATE[32,33],whichaimstoenhanceLMresponsesensitivity
topriorutterancesorcontrols,constructsnegativesamplesbyreplacingthecontextwithaslightly
incorrectpredecessor. GANsforLMs[16,30,36]usethegenerator’soutputsasnegativesamples.
RLHF[3,14,21,37]anditsderivatives,suchasDPO[23],IPO[1],andKTO[7],collecthuman
feedbacktolabelpairwisepreferredandrejectedresponsesgeneratedbyafine-tunedmodel,withthe
rejectedresponsesservingasnegativedata. Inthiswork,wediscussoften-seencasesingenerative
LMs,e.g.,whenthenegativedatathatarealsofluentlanguage. Thisrequiresthemethodtoidentify
thenuanceddifferencebetweenthepositiveandnegativedata.
OptimizationMethod. Word2Vec,GANsforLMs,andUnlikelihoodtraininguseasimilarloss
function long employed for optimizing binary classifiers [9, 19, 35]. ExMATE [32], inspired by
the average treatment effect and the directed acyclic graph structure of conversations [12, 22],
proposesanexponentialtricktolinearizegradientstomaintainlanguagefluency. DPO[23]andits
derivatives[1,7,13],alsosupervisionlossfunctions,arefirstlyderivedfromRLHF[23]toreduce
theresource-intensiveinteractionsinRLframeworks[2,15,25,31],relyingonanassumedreward
model [3, 21] and Kullback–Leibler divergence regularization. In this paper, we mainly discuss
Unlikelihood,ExMATE,andDPOastheyrepresentthreedistinctlinesofresearchtowardsthesame
goal. Wediscusstheirfunctioncharacteristicsmathematicallyandempiricallyunderthesamesetups.
DifferentfromotherContrastsinML.Contrastivelearning[5,10,20,28,34]aimstolearnsimilar
representations for similar data points and vice versa. The methods we discuss here, instead of
learning representation space based on similarity among data points, aim to directly reshape the
modeloutputdistributionbasedoneachdatapoint’sintrinsiccorrectness,i.e.,whethertheinputand
outputlabelsmatch.
23 Preliminary
WhentrainingagenerativeLMgwithparameterssetθ,ateachtimestept,wefeedthemodelan
inputsequencexandapartoftheexpectedoutputy. Theinitialpartofy,denotedasy ,indicating
<t
thefirstt−1tokensiny. ThemodelpredictsaprobabilitydistributionoveravocabularysetV per
timestepby:
(cid:0) (cid:1)
P (·|x,y )=softmax g (x,y ) . (1)
θ <t θ <t
WeuseP intheresttodenotetheLM,whichisacombinationofg andthesoftmaxfunction.
θ θ
AssumingwithtrainingdataDthatinvolvescorrecttextpairs{(x+,y+) }|D|,wherethesuperscript
i i=1
+indicatesthedatasampleisdeemedcorrect,themodelisoftenoptimizedbyMLEas:
θ =argminL , (2)
MLE
θ
(cid:34) T (cid:35)
1 (cid:88)
L = E −logP (y+|x+,y+ ) . (3)
MLE (x+,y+)∼D T θ t <t
t=1
Minimizing L implies increasing the probability P (y+|x+), as P (y+|x+) =
MLE θ θ
(cid:81)T P (y+|x+,y+ ). If assuming the model capacity and data scale are sufficiently large, the
t=1 θ t <t
optimalcanbeachieved.
Nonetheless,withoutthosestrongassumptionsandcomputationsupports,studieshaveshownthat
consideringnegativeexamples(x−,y−)canimprovemodelperformance[1,7,8,17,19,23,32,35].
WereferthesemethodsastypesofRewarding-Good-and-Penalizing-BadtraininglossandRGPB
forshortinthelatersections. Inthispaper,wediscussthreetypesofRGPBmethods: DPO[23],
Unlikelihoodtraining[35],andExMATE[32].
Withourdefinitionsofpositiveexamples(x+,y+)andnegativeexamples(x−,y−)fromtraining
dataD,theobjectivesofDPO,Unlikelihood(ULforbrevity),andExMATEaretoupdatethemodel
parametersθtorespectivelyminimizethelossfunctionsL ,L ,L . Ourunification
DPO UL ExMATE
ofnegativesamplingandGANswithUnlikelihoodisinAppendixA.Theirformulationsare:
(cid:20) (cid:18) P (y+|x+) P (y−|x−) (cid:19)(cid:21)
L =−E logσ βlog θ −βlog θ , (4)
DPO D P (y+|x+) P (y−|x−)
ref ref
(cid:34) T (cid:35)
1 (cid:88) (cid:16) (cid:17)
L =−E logP (y+|x+,y+ )+βlog 1−P (y−|x−,y− ) , (5)
UL D T θ t <t θ t <t
t=1
(cid:34) T (cid:18) T (cid:19)(cid:35)
1 (cid:88) 1 (cid:88)
L =−E logP (y+|x+,y+ )−βexp logP (y−|x−,y− ) . (6)
ExMATE D T θ t <t T θ t <t
t=1 t=1
Forgradientanalysisinthenextsections,wefirstderivetheirgradientwithrespecttothemodel
parametersθ(AlltheproofsanddetailedderivationsofthispaperareinAppendixB-E).Thegradient
ofalossfunctionListhenusedtoupdatethemodelθ ←θ−∇ L. Wepresentthegradientshere
θ
withnotationsf+ :=P (y+|x+),f− :=P (y−|x−),andf :=P forbrevity:
θ θ θ θ ref ref
(cid:104) (cid:16) f− f+ (cid:17)(cid:16)∇ f+ ∇ f−(cid:17)(cid:105)
∇ L =−βE σ βlog θ −βlog θ θ θ − θ θ , (7)
θ DPO D f− f+ f+ f−
ref ref θ θ
(cid:16)∇ f+ −∇ f−(cid:17)
∇ L =−E θ θ +β θ θ , (8)
θ UL D f+ 1−f−
θ θ
(cid:16)∇ f+ (cid:17)
∇ L =−E θ θ −β∇ f− . (9)
θ ExMATE D f+ θ θ
θ
4 FactorsImpactRGPBGradientsinGenerativeLanguageModels
4.1 LanguageModelProperties
Beforedivingintothegradientanalysis,weaskwhatarethepropertiesofgenerativeLMsandwhat
makestheirgradientsdifferentfromtheusualclassificationproblem.
3Multiple Time Steps. We are fundamentally tackling every time steps instead of the whole
P(y+|x+)andP(y−|x−). WehighlightthegoalofanRGPBmethodforlanguagemodels: Wefeed
themodelwithdifferentinputsx+andx−,andaskthemodeltorespectivelyoptimizetheprobability
ofthetokeny+anddeoptimizetheprobabilityofthetokeny−foreverytimestepst.
t t
Multiple Classes. Generating responses from a language model is a sequence of multi-class
classificationproblems,i.e.,themodelpredictsaprobabilitydistributionP(·|x,y )∈[0,1]|V| at
<t
eachtimesteptoverthewholevocabularysetV. Thegenerationresultisoftenbasedonthewhole
probabilitydistribution(e.g.,Softmaxsampling,nucleussampling),notjustasingletokenprobability.
Therefore,beyondy+andy−,othertokensinV canhaveimpact.
t t
Literal Similarity. Being natural language, y+ and y− may use the same tokens at some time
steps. Forexample,wheny+andy−arerespectively“I’mdoinggreattoday”and“I’mdoinggreat
yesterday”,theyaremostlythesamewithminorwordchanges;whentheyarerespectively“Weenjoy
inhiking”and“Theylovehiking”,theyusesinglesameword. Whethery+andy−sharesomesame
tokensplaysavitalroleinthegradient.
4.2 InformationandGradientDifferencesbetweenpositiveandnegativesamples
BesidesthecharacteristicsoflanguagemodelsinSection4.1,asshowninEquations7-9inSection3,
f+,f−,∇ f+,∇ f−arethekeystodeterminethegradientformodelupdate.
θ θ θ θ θ θ
Amongthem,theinformationdifferenceandgradientdifferencecanhavehighimpact. Wedefine
themasfollowing:
Definition4.1. (InformationDifference)|ϵ| := |f+−f−|. Thedifferencebetweendatasamples
θ θ
(x+,y+)and(x−,y−)intermsoftheirprobabilitymassesforanyθ.
Definition4.2. (GradientDifference)∥∇ f+−∇ f−∥ ,wherepindicatesp-norm.
θ θ θ θ p
Lemma4.1. InLMswithsoftmaxfunctionforfinalprediction,theGradientDifferenceisdetermined
by(1)thesoftmaxdistributiondifference∥P (·|x+,y+ )−P (·|x−,y− )∥ (weuseitasthegradient
θ <t θ <t p
differenceintherestofthepaper)and(2)thesamenessoftargetoutputtokens. ProvedinAppendixB.
Informationdifferenceandgradientdifferencehaveasimilarform,butgradientdifferenceconsiders
theprobabilitydistributionoverthewholevocabularysetinsteadofsingletokenprobabilitymass.
This is also the reason that gradient difference for each time step t is considered separately and
informationdifferenceistheaggregationofalltimestepsprobabilitymasses.
Thesetwovariablesandtheabovelanguagemodelpropertiesarethekeysforgradientanalysisinthe
nextsection.
5 RGPBGradientAnalysisinGenerativeLanguageModels
With the Multiple Time Steps and Literal Similarity properties of LMs, we split the gradient
analysisintotwoparts: (1)gradientattimesteptthaty+ ̸=y−,and(2)gradientattimesteptthat
t t
y+ =y−.
t t
Furthermore, wedropthenegationsignofEquations7-9toconsiderthecaseofgradientascent,
and denote that (1) P+(·) := P (·|x+,y+ ), P−(·) := P (·|x−,y− ), and (2) f+ = u ∈ [0,1],
θ <t θ <t θ
f− =u+ϵ∈[0,1]forbrevity,where|ϵ|isthedefinedinformationdifferenceanditisanimportant
θ
factorforgradients.
5.1 Fortimestepstthaty+ ̸=y−.
t t
Withchainrule,thegradientscanbesplitintothetwoparts: (1)Fromthelossfunctiontothelogits
(i.e., ∂L),and(2)fromthelogitstothemodelparametersθ (i.e., ∂gθ). Weassumeherethatthe
∂gθ ∂θ
gradientdifferenceissmall,i.e.,P+ ≈P− ∈[0,1]|V|,sotheirlogits’derivativesareapproximately
t t
thesameanddenotedas∇ θζ ∈R|V|×|θ| := ∂ ∂g θθ+ ≈ ∂ ∂g θθ− .
4WerewriteEquations7-9asfollowingsandfirstlookintothegradientsthatflowthroughatoken
z ∈V whenz =y+orz =y−.
t t
β(u+ϵ)β (cid:26) 1−P+(y+)+P−(y+),ifz =y+
∇ L =∇ ζ· t t t (10)
θ DPO θ (u+ϵ)β +uβ −P+(y−)−(1−P−(y−)),ifz =y−
t t t
(cid:40) 1−P+(y+)+βP−(y t−)P−(y t+),ifz =y+
∇ θL UL =∇ θζ· −P+(y−)t −βP−(y1− −P ),− i( fy zt−)
=y−
t (11)
t t t
(cid:26) 1−P+(y+)+βP−(y−)P−(y+),ifz =y+
∇ L =∇ ζ· t t t t (12)
θ ExMATE θ −P+(y−)−βP−(y−)(1−P−(y−)),ifz =y−
t t t t
FromEquations10-12,allmethodsresultinnon-negativegradientsforz = y+ andnon-positive
t
gradients for z = y−, indicating that whenever y+ ̸= y−, the model outputs are updated as
t t t
expectation to raise the probability of y+ and lower the probability of y−. However, their
t t
updatingrates|∇ |andstopcriterionaredifferent: (1)DPO’s|∇ |increaseswithϵbutisalwaysnot
θ θ
infinityandbecomeszerowhenϵ→−u(f− →0). (2)Unlikelihood’s|∇ |increaseswithP−(y−)
θ θ t
(oftencorrelatedwithϵ),but|∇ |fory+explodes. Moreover,|∇ |fory+onlybecomeszerowhen
θ t θ t
P+(y+)=1. (3)ExMATE’s|∇ |forbothy+andy−increasewithP−(y−)andarebounded. The
t θ t t t
|∇ |fory+alsoonlybecomeszerowhenP+(y+)=1.
θ t t
Thegradientsfortokensz ∈V exceptfory+andy−:
t t
β(u+ϵ)β (cid:16) (cid:17)
∇ L =∇ ζ· −P+(z)+P−(z) ≈0, (13)
θ DPO θ (u+ϵ)β +uβ
(cid:16) P−(y−) (cid:17)
∇ L =∇ ζ· −P+(z)+β t P−(z) , (14)
θ UL θ 1−P−(y−)
t
(cid:16) (cid:17)
∇ L =∇ ζ· −P+(z)+βP−(y−)P−(z) . (15)
θ ExMATE θ t
FromEquations13-15andwiththesmallgradientdifferenceassumptionthatP+ ≈P−,(1)DPO
t t
doesnotupdateprobabilityofnon-referredtokens(neithery+nory−),alwaysonlycompensating
t t
P−(y−)forP+(y+). (2)UnlikelihoodstopsthegradientwhenP−(y−)= 1 . WhenP−(y−)>
t t t 1+β t
1 ,UnlikelihoodreducesP−(y−)toincreasetheprobabilitiesofnon-referredtokensandP+(y+);
1+β t t
whenP−(y−)< 1 ,Unlikelihoodalsocompensatesprobabilitiesofnon-referredtokenstoraise
t 1+β
P+(y+). (3)ExMATEonlydecaysP−(y−)tocompensateforP+(y+)whenP−(y−)→ 1. When
t t t t β
P−(y−)→0,ExMATEcompromisesP(z)forP(y+).
t t
Aboveall,DPOaimstoonlyexchangeprobabilitiesofy+ andy− andstopstoincreasey+ when
t t t
y−reacheszeroprobability. Ontheotherhand,ExMATEprioritizestoincreasetheprobabilityof
t
y+andonlystopswheny+reachesthehighestprobabilitybycompensatingbothy−andallother
t t t
tokensz. Unlikelihoodalsoaimstobothincreasetheprobabilityofy+untilitreachesthehighest
t
probabilityandalwaysdecaytheprobabilityofy−,butitalsoalwayscompensatetheprobabilityof
t
allothertokenszforeithery+ory−.
t t
5.2 Fortimestepstthaty+ =y−.
t t
AnothercasesinLMsiswheny+ =y−. Sincenowy+ =y− :=y andweassumethatP+ ≈P−,
t t t t t t t
wecanapproximatethat∇ f+ ≈∇ f− =:∇ f.
θ θ θ θ θ
Thegradientsbecomethefollowingandwecaninterpretthatwhenthegradientispositive,both
P+(y+)andP−(y−)willraise,andviceversa.
t t
β(u+ϵ)β−1ϵ
∇ L =∇ f · , (16)
θ DPO θ ((u+ϵ)β +uβ)u
1−(1+β)u−ϵ
∇ L =∇ f · , (17)
θ UL θ u(1−u−ϵ)
(cid:16)1 (cid:17)
∇ L =∇ f · −β . (18)
θ ExMATE θ u
5(c) ExMATE
Figure2:TheestimatedgradientsofDPO,Unlikelihood,andExMATEfortimestepstthaty+ =y−.
t t
(1)DPO’sgradients(Figure2(a)andEquation16)highlydependonϵ. DPOincreasesP+(y )and
t
P−(y )whenϵ>0(f− >f+)anddecreasesthemwhenϵ<0. Thisleadstothemodeldecaying
t θ θ
bothf+andf−whenreachingϵ<0,whichmaynotbedesiredineverycases. Moreover,when
θ θ
f+ ≈f−(orϵ→0),themodeldoesnotlearnthings.(2)Unlikelihood(Figure2(b)andEquation17)
θ θ
decays P+(y ) and P−(y ) when ϵ > 1−(1+β)u and the decay rate explodes as ϵ → 1−u.
t t
Meanwhile,whenuislower,UnlikelihoodmostlyincreasesP+(y )andP−(y );whenuishigher,it
t t
mostlyreducesP+(y )andP−(y ). Thishighrateofnegativegradientsisareasonforeasilybroken
t t
languageaftertraining. (3)Differently,ExMATE’sgradients(Figure2(c)andEquation18)only
dependonuandarealwayspositivewhenu<1/β. Thepositivegradientsalsohavehighervalues
thanthenegativeones. Therefore,ExMATEmostlyprioritizestoincreaseP+(y )andP−(y ).
t t
5.3 Summary
Overall,DPOmathematically(1)doesnotoptimizeP(y+|x+)ifP(y−|x−)isalreadyminimized
and (2) tends to decrease all probabilities, so it is suitable for model optimization when some
probability decays are acceptable, P(y+|x+) is not required to be optimized, and ϵ is not nearly
zero. UnlikelihoodmathematicallyaimstooptimizebothP(y+|x+)andP(y−|x−)byupdating
theprobabilityofothertokensandthegradientsareoftenlargeorexplodedtofacilitatetheupdate,
so it is more suitable for cases when minimizing P(y−|x−) is nearly important as maximizing
P(y+|x+)andtheliteralsimilarityofy+andy−islower. ExMATEaimstooptimizeP(y+|x+)by
firstreducingP(y−|x−)andthenreducingtheprobabilityofothertokensifP(y−|x−)isalready
minimized. Moreover,itsgradientsaremostlyboundedandlessdependonϵ. Itispreferredwhenthe
ϵ→0orwhenmaximizingP(y+|x+)shouldbeprioritized.
6 Experiments
Beyondmathematicalresults,weareinterestedinRGPBmethods’empiricalresultsonrealdata
andoff-the-shelfLMs. Wefirstverifywhetherourassumptionsingradientanalysisofinformation
andgradientdifferencesholdinrealscenarios,e.g.,diverseperfectionlevelsofmodels(pre-trained
orrandomlyinitialized)anddistinctrelationshipsbetweenthepositiveandnegativesamples. We
thenask: CananyoftheRGPBmethodsgeneralizetodifferentcases? Whataretheirempirical
properties? Dotheymatchthemathematicalresults?
Wewillfirstdescribeoursettingsandthenpresenttheresults.
Tasks. Weexperimentedontwotextgenerationdatasetswithdifferentrelationshipsbetweenthe
positive and negative examples: (1) CausalDialogue [32], a conversation dataset with multiple
(x+,y+)and(x−,y−)pairsextractedfromtheutterancedirectedacyclicgraphs(DAG).They+and
y−arethesamewhilethex+andx−haveonlysubtledifference.ThegoalistomaximizeP (y+|x+)
θ
whileminimizingP (y−|x−). Thistaskisexpectedtohavesmallinformationdifference(ϵ→0).
θ
6
OPD
)a(
doohilekilnU
)b((a) (b) (c)
Figure3: (a)Allmodel’sinformationdifferencesonCausalDialoguearenearlyzero(<1e-26). (b)
informationdifferencesonAnthropicHH-RLHFarehigherthanonCausalDialogue. (c)Allmodel’s
gradientdifferencesonCausalDialogueandthefirstthreetimesteps. Allaresmall,especiallyforthe
firsttimestep,randomlyinitializedmodels,andlargernumberofparameters.
(2) Anthropic HH-RLHF [3], a dataset of human-machine dialogues ended with paired human
preferredresponseandhumanrejectedresponse. Thistaskisexpectedtohavehigherinformation
differencebetweenthepositiveandnegativeexamples. Also,sincebothy+ andy− aremachine
generationinsteadofhumanwrittenresponses,weexpectthatalowerP (y+|x+)isacceptable.
θ
Methods. WecompareDPO,Unlikelihood,andExMATEwiththeircoefficientβ tunedamong
{0.05,0.1,0.5,1,5}. We also train models using MLE as a reference of LM performance with-
out considering negative examples. The MLE fine-tuned model is also referred to as SFT in
the following to match the naming conventions of RLHF literatures [21]. For the initial mod-
els and training recipe, we follow prior works [23, 32]. We fine-tune T5 models [24] on
CausalDialogue for five epochs, fix learning rate as 1e-5, allow a maximum of 128 input to-
kens and put no restriction on the output length. We use Pythia-2.8B and Pythia-6.9B [4] on
Anthropic HH-RLHF for one epoch with fix learning rate 5e-7. Our implementations follow
theiropen-sourcecodebases: https://github.com/Pascalson/CausalDialogueandhttps:
//github.com/eric-mitchell/direct-preference-optimization.
Evaluation. Weprimarilyevaluateamodelbyperplexityandagility[13,32]. Perplexity,defined
asexp[−1 (cid:80)T logP (y+|x+,y+ )],istoquantifythecertaintyofamodelfor(x+,y+)andis
T t=1 θ t <t
usedtoautomaticallyestimateamodel’sfluency. Agility,definedasf+−f−(whichisalso−ϵ),is
θ θ
toquantifywhetherthemodelsuccessfullyrewarding-goodwhilepenalizingbadexamplesintheir
probabilitymasses. Inadditiontostatisticalevaluation,weevaluatebyGPT4thequalityofsampled
responsesfromthetrainedmodels.
6.1 Thevaluesofinformationandgradientdifferencesinrealscenarios
WefirstempiricallyverifywhetherourassumptionsinSection5oftheinformationdifferenceand
gradient difference (Definitions 4.1 and 4.2) between (x+,y+) and (x−,y−) hold: The gradient
differenceismostlylowandnegligibleandtheinformationdifferencecanbenearlyzeroorhigher.
Wetest8situationsintotal,includingCausalDialoguewithpretrainedandrandomizedT5-small
(60.5M),T5-base(223M),T5-large(738M)models,AnthropicHelpfulandHarmlessDialoguewith
pretrainedPythia-2.8BandPythia-6.9B.
ResultsareshowninFigure3(a)(b),whereeachpointisthevalueforapairof(x+,y+)and(x−,y−).
OnCausalDialogue,theinformationdifferenceisnearlyzeroforallmodelsizes,eventhoughslightly
higherwhenusingnon-pretrainedmodels. Differently,AnthropicHH-RLHFwithlargeLMshas
higherinformationdifference. Thekeyofinformationdifferenceistheliteralsimilaritybetween
(x+,y+)and(x−,y−).
Thegradientdifference,asinFigure3(c),islowforeverygenerationsteps,especiallythefirststep
onCausalDialogue,anditisalwayszeroforAnthropicHH-RLHF,sincethex+andx−arealways
thesame. Thereasonoftheincreasinggradientdifferencealongtimestepsisthatdialogueresponses
oftenhavesimilaropeningsandtheliteraldifferencewillaccumulatealonggenerationsteps.
7(a) CausalDialogue (b) Anthropic HH-RLHF
Figure 4: (a) Perplexity (log scale) and agility of MLE, DPO, Unlikelihood, and ExMATE on
CausalDialogue. Unlikelihoodimprovesagility,DPOdegradesboth,andExMATEispreferredfor
improving both. (b) Perplexity and agility of SFT(MLE), DPO, Unlikelihood, and ExMATE on
AnthropicHH-RLHF.DPOachieveshighagilitybycompromisingperplexity;ExMATEimproves
bothmetricsbysmallvalues.
6.2 ComparingRGPBmethodsinthecaseoflowinformationdifference.
SinceCausalDialoguehaslowinformationandgradientdifferences,Figure4(a)showsempirical
resultsinarealscenariosdiscussedinSection5.2. DPOintroducesalmostzerogradientsandresults
inhighperplexityandzeroagility,givingnoconvergenceandeffectivelearning. Ontheotherhand,
sinceUnlikelihoodoftenintroducesgradientstodecayprobabilities,theperplexityishigherthan
simplyusingMLE.However,thegoodnewsis,astheprobabilitiesareoverallsmall,Unlikelihood
does not introduce unwanted exploded gradient is this case. ExMATE simultaneously improve
perplexityandintroducesthesecondhighestagilityscore,reflectingthefactingradientanalysisthat
itprioritizetoincreaseprobabilityof(x+,y+).
6.3 ComparingRGPBmethodsinthecaseofhigherinformationdifference.
Another case we mainly discuss is Anthropic
HH-RLHF that shows higher information dif-
ferencesandmatchesthecaseofgradientanal-
ysis in Section 5.1. The results in Figure 4(b)
showsthatDPOachievesthehighestagilityand
compromisesmuchperplexity. Thisisexpected
thatDPOcanperformbetterinthiscasecom-
paredtosituationswithlowerinformationdif-
ferences due to no zero gradient issue. How-
ever,DPOstilltendstodecaytheprobabilities
as our gradient analaysis. On the other hand,
ExMATEacheivesthesecondbestagility(but Figure5: Fine-tunedPythia6.9BbySFT,Unlike-
only slightly higher agility compared to other lihood(UL),ExMATE,orSFT+DPO.
methods)andthelowestperplexity.
WealsotestwithlargermodelPythia6.9BandfollowedpriorworktoimproveDPObyfirstfine-
tuningthemodelwithSFT(calledSFT+DPO)andplottheresultsinFigure5. Theresultsthatboth
perplexityandagilityofallmethodsareimproved,showingthattheseRGPBmethodsareallscalable
tomodelsizeandSFT+DPOstillretainsthepropertyofDPOthatcompromisingperplexity.
6.4 DiscussionofNewMethods: ExMATEwithSFTorExMATEwithDPO?
TofindabetterrecipeofRGPBbeyondMLE,wefirstobservethat(1)MLE/SFT,Unlikelihood,
andExMATEhavemanysimilartrendswhileExMATEconsistentlyachieveslowestperplexityand
higheragility,and(2)DPO’strendisanoutlierand,aspriorworkdiscussed,mayrequirethemodel
tobefirstfine-tunedbySFTtoreachcertainperformance.
8Therefore, we compare: (1) ExMATE vs
SFT+ExMATE by replacing the DPO in
SFT+DPOframeworkwithExMATEsinceEx-
MATE is an overall best performed RGPB
method,and(2)SFT+DPOvsExMATE+DPO
byreplacingtheSFTstagewithExMATEsince
ExMATEsharessimilarperplexityandagility
trends with SFT but is better. The results are
shown in Figure 6 and the red line is the SFT
result for reference. The plots clearly show
that: (1) All methods improve agility. (2) Ex-
MATE+SFTimprovesbothagilityandperplex-
itywhileDPO+SFTsacrificemuchperplexity
foragility, showingthedifferentpropertiesof Figure 6: ExMATE vs. SFT+ExMATE and
ExMATE and DPO. (3) ExMATE+DPO im- SFT+DPOvs. ExMATE+DPO.
provesbothagilityandperplexitycomparedto
SFT+DPO,indicatingExMATEcanalsobeabettersurrogateforMLE.Theresultsalsodemonstrate
thatdevelopingabettersupervisedlossfunctioncansimultaneouslyhavepreferredstatisticalprop-
erties(highagilityandlowperplexity)andcanbeempiricallyaggregatedwithothermethodsfor
performanceboost.
6.5 ValidatingevaluationresultsbyGPT4andhumanjudgements
Thispaperfocusesonanalyzingthestatisticalef-
Table1: GPT4evaluationresults.
fectofRGPBmethods, which can beshownby
perplexityandagilitymetrics. Theyarethedirect win lose
objectives of RGPB methods and provide us an
ExMATEvsDPO 0.47 0.40
overviewofthelearnedoutputdistributionprop-
erties. Inaddition,wealsoevaluatethegenerated ExMATEvsSFT+DPO 0.32 0.36
responsesbyGPT4andhumanjudgementstogain
ExMATE+DPOvsSFT+DPO 0.56 0.38
other types of understanding. Such judgements
canalsogiveusanunderstandingofthelinkbe-
tweenGPT4orhumanevaluationwiththestatisticalpropertiesreflectedinperplexityandagility.
Specifically,wegiveGPT4aconversationandtwogeneratedresponsesandaskGPT4tochoosethe
betterresponse. TheresponsesaregeneratedfromPythia-2.8BmodelstrainedonHH-RLHFdata
using(1)ExMATEvsDPO,(2)ExMATEvsSFT+DPO,and(3)ExMATE+DPOvsSFT+DPO,of
whichstatisticalresultsareshowninFigure4(b). Table1presentstheGPT4evaluationresults. To
verifythetrustfulnessoftheGPT4evaluationrestuls,weaskhumanannotatorsthesametaskand
gain0.842Cohen’skappa,indicatingstrongagreementbetweenGPT4andhumanratings[18]. From
theresults,weinterpretthatagilityandperplexityarebothimportantmetrics: (1)Whenonlyone
ofthemisbetter,theirsamplingresultsmaybeindifference. Worseperplexityoftenleadstoless
fluentresponseandbetteragilityoftenleadstolesshelpfulorsaferesponses(ExMATEvsDPOand
ExMATEvsSFT+DPO).(2)Whenbotharebetter,thesamplingresultsalsoshowimprovements
(ExMATE+DPOvsSFT+DPO).
7 Discussion
In this paper, we gather and analyze loss functions that essentially share the same goal: simulta-
neouslyrewardinggooddatasamplesandpenalizingbaddatasamplesinLMoutputdistribution.
The representative methods we discuss are: unlikelihood training, exponential maximizing ATE
(ExMATE),anddirectpreferenceoptimization(DPO)astheirproposedtimeorder. Weprovidea
novelperspectivetoconsiderthecharacteristicsofgenerativeLMsintogradientanalysis:themultiple
timesteps, multipleclasses, andliteralsimilarity. Ourapproachsplitsgradientanalysisintotwo
primarycases: (1)Wheny+ ̸= y− and(2)y+ = y−. GeneralizationPerformance. Fromboth
t t t t
mathematicalresultsandexperiments,weconcludethatDPOalthoughcansignificantlyincrease
agility,whichisdefinedasthegapbetweenprobabilitymassesofpositiveandnegativesamples,it
largelycompromisesperplexityandfailstointroduceeffectivegradientswhentheinitialinformation
differenceissmall. Incontrast,ExMATEconsistentlyenlargesagilityandpreventstheprobability
9dropsinthesametimeacrosssituations,butwitharelativelysmallimprovements. Thesedemonstrate
thatagilityandperplexityarenotnecessarilytrade-offsbutthequantityofimprovementsneedsto
beenhanced. ABetterRecipe. WithfurtherexperimentstakingExMATEasasurrogateofSFTor
DPO,wesuggestamoreunifiedoptimizationmethodistofirsttrainLMsbyExMATEinsteadof
MLE.Ifthetestcasedoesnotrequiresufficientlyhighprobabilityofgivenpositiveexamples,then
weuseDPOtodofurtherfine-tuning.
Limitations. We approach the difference of RGPB methods from the view of gradient analysis
incommonLMscenarios,sotheresultsmaynotsuitableforallpossiblecases,suchas(x−,y−)
isbrokenlanguagewithoutanysimilaritywith(x+,y+)orsinglestepbinaryclassifier. Also,we
focusonwhethertheLMoutputdistributionisasdesired: havinghigherprobabilityofthepositive
examplesandlowerprobabilityofthenegativeexamples,whicharereflectedinperplexityandagility
metrics. However,othermetricsofgenerationquality,suchaslanguagediversity,informativeness,
factuality,whichrelatelesstotheoutputdistributionarenotspecificallydiscussed. Theyaredeemed
orthogonaltothefunctioncharacteristicsandcanbeincludedasthedifferencebetweenpositiveand
negativeexamples.
Broader Impacts and Future Work. As this work discusses, RGPB methods aim to optimize
generativeLMsinsteadofMLE.Therefore,whileapplyingthesuggestedmethodcantrainhigher
qualityofgenerativemodels,itcanbepurposelyusedtogenerateharmfulresponses. Weexpectthe
proposedgradientanalysisapproachcanfacilitatefutureimprovementofRGPBmethodsonboth
perplexityandagility, e.g., firstdesigningthedesiredgradientsandthentransferringbacktothe
lossfunction. Weexpectthatfutureworkcanbeinspiredtorethinktheimportanceofperplexity
andagilityindifferenttasksandchoosethemoresuitableRGPBmethod. Furthermore,asrecent
advancementsinotheraspectsofLMs,suchasmodelarchitecturesandsystemframeworks,wehope
moreadvancementsinlossfunctionsforLMcanaddresshowtolearnamoredesiredoutputdistrition
beyondMLE.
References
[1] MohammadGheshlaghiAzar,MarkRowland,BilalPiot,DanielGuo,DanieleCalandriello,MichalValko,
andRémiMunos. Ageneraltheoreticalparadigmtounderstandlearningfromhumanpreferences. arXiv
preprintarXiv:2310.12036,2023.
[2] Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron
Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction. In International
ConferenceonLearningRepresentations(ICLR),2016.
[3] YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,AnnaChen,NovaDasSarma,DawnDrain,
Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training a helpful and harmless assistant with
reinforcementlearningfromhumanfeedback. arXivpreprintarXiv:2204.05862,2022.
[4] Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric
Hallahan,MohammadAflahKhan,ShivanshuPurohit,USVSNSaiPrashanth,EdwardRaff,etal. Pythia:
Asuiteforanalyzinglargelanguagemodelsacrosstrainingandscaling. InInternationalConferenceon
MachineLearning,pages2397–2430.PMLR,2023.
[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastivelearningofvisualrepresentations. InInternationalconferenceonmachinelearning,pages
1597–1607.PMLR,2020.
[6] KyunghyunCho,BartvanMerriënboer,CaglarGulcehre,DzmitryBahdanau,FethiBougares,Holger
Schwenk,andYoshuaBengio. LearningphraserepresentationsusingRNNencoder–decoderforstatistical
machinetranslation. InAlessandroMoschitti,BoPang,andWalterDaelemans,editors,Proceedingsofthe
2014ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),pages1724–1734,
Doha,Qatar,October2014.AssociationforComputationalLinguistics. doi:10.3115/v1/D14-1179. URL
https://aclanthology.org/D14-1179.
[7] Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model
alignmentasprospecttheoreticoptimization. arXivpreprintarXiv:2402.01306,2024.
[8] YoavGoldbergandOmerLevy. word2vecexplained:derivingmikolovetal.’snegative-samplingword-
embeddingmethod. arXivpreprintarXiv:1402.3722,2014.
10[9] IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,Aaron
Courville,andYoshuaBengio. Generativeadversarialnets. Advancesinneuralinformationprocessing
systems(NIPS),2014.
[10] MichaelGutmannandAapoHyvärinen. Noise-contrastiveestimation: Anewestimationprinciplefor
unnormalizedstatisticalmodels. InProceedingsofthethirteenthinternationalconferenceonartificial
intelligenceandstatistics,pages297–304.JMLRWorkshopandConferenceProceedings,2010.
[11] SeppHochreiterandJürgenSchmidhuber.Longshort-termmemory.Neuralcomputation,9(8):1735–1780,
1997.
[12] PaulWHolland. Statisticsandcausalinference. JournaloftheAmericanstatisticalAssociation,81(396):
945–960,1986.
[13] JiwooHong,NoahLee,andJamesThorne. Orpo:Monolithicpreferenceoptimizationwithoutreference
model,2024.
[14] NatashaJaques,JudyHanwenShen,AsmaGhandeharioun,CraigFerguson,AgataLapedriza,NoahJones,
ShixiangGu,andRosalindPicard. Human-centricdialogtrainingviaofflinereinforcementlearning. In
Proceedingsofthe2020ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP),
2020.
[15] KirthevasanKandasamy,YoramBachrach,RyotaTomioka,DanielTarlow,andDavidCarter. Batchpolicy
gradientmethodsforimprovingneuralconversationmodels. InInternationalConferenceonLearning
Representations(ICLR),2017.
[16] JiweiLi,WillMonroe,TianlinShi,SébastienJean,AlanRitter,andDanJurafsky. Adversariallearning
forneuraldialoguegeneration. InProceedingsofthe2017ConferenceonEmpiricalMethodsinNatural
LanguageProcessing(EMNLP),2017.
[17] MargaretLi,StephenRoller,IliaKulikov,SeanWelleck,Y-LanBoureau,KyunghyunCho,andJason
Weston. Don’tsaythat!makinginconsistentdialogueunlikelywithunlikelihoodtraining. InProceedings
ofthe58thAnnualMeetingoftheAssociationforComputationalLinguistics(ACL),2020.
[18] MaryLMcHugh. Interraterreliability:thekappastatistic. Biochemiamedica,22(3):276–282,2012.
[19] TomasMikolov,KaiChen,GregCorrado,andJeffreyDean. Efficientestimationofwordrepresentations
invectorspace. arXivpreprintarXiv:1301.3781,2013.
[20] AaronvandenOord,YazheLi,andOriolVinyals. Representationlearningwithcontrastivepredictive
coding. arXivpreprintarXiv:1807.03748,2018.
[21] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,ChongZhang,
SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollowinstructionswith
humanfeedback. AdvancesinNeuralInformationProcessingSystems(NeurIPS),2022.
[22] JudeaPearl. Causality. Cambridgeuniversitypress,2009.
[23] RafaelRafailov,ArchitSharma,EricMitchell,ChristopherDManning,StefanoErmon,andChelseaFinn.
Directpreferenceoptimization: Yourlanguagemodelissecretlyarewardmodel. AdvancesinNeural
InformationProcessingSystems(NIPS),2024.
[24] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,
WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer.
TheJournalofMachineLearningResearch,21(1):5485–5551,2020.
[25] Marc’AurelioRanzato,SumitChopra,MichaelAuli,andWojciechZaremba. Sequenceleveltrainingwith
recurrentneuralnetworks. arXivpreprintarXiv:1511.06732,2015.
[26] RenandMalik. Learningaclassificationmodelforsegmentation. InProceedingsninthIEEEinternational
conferenceoncomputervision.IEEE,2003.
[27] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy
optimizationalgorithms. arXivpreprintarXiv:1707.06347,2017.
[28] KihyukSohn. Improveddeepmetriclearningwithmulti-classn-pairlossobjective. Advancesinneural
informationprocessingsystems,29,2016.
[29] IlyaSutskever, OriolVinyals, andQuocVLe. Sequencetosequencelearningwithneuralnetworks.
Advancesinneuralinformationprocessingsystems(NIPS),2014.
11[30] Yi-Lin Tuan and Hung-Yi Lee. Improving conditional sequence generative adversarial networks by
stepwiseevaluation.IEEE/ACMTransactionsonAudio,Speech,andLanguageProcessing,27(4):788–798,
2019.
[31] Yi-LinTuan,JinzhiZhang,YujiaLi,andHung-yiLee. Proximalpolicyoptimizationanditsdynamic
versionforsequencegeneration. arXivpreprintarXiv:1808.07982,2018.
[32] Yi-LinTuan,AlonAlbalak,WendaXu,MichaelSaxon,ConnorPryor,LiseGetoor,andWilliamYang
Wang.CausalDialogue:Modelingutterance-levelcausalityinconversations.InFindingsoftheAssociation
forComputationalLinguistics(ACL),2023.
[33] Yi-Lin Tuan, Xilun Chen, Eric Michael Smith, Louis Martin, Soumya Batra, Asli Celikyilmaz,
William Yang Wang, and Daniel M Bikel. Towards safety and helpfulness balanced responses via
controllablelargelanguagemodels. arXivpreprintarXiv:2404.01295,2024.
[34] KilianQWeinbergerandLawrenceKSaul. Distancemetriclearningforlargemarginnearestneighbor
classification. Journalofmachinelearningresearch,10(2),2009.
[35] SeanWelleck,IliaKulikov,StephenRoller,EmilyDinan,KyunghyunCho,andJasonWeston. Neuraltext
generationwithunlikelihoodtraining. InInternationalConferenceonLearningRepresentations(ICLR),
2020.
[36] LantaoYu,WeinanZhang,JunWang,andYongYu. Seqgan:Sequencegenerativeadversarialnetswith
policygradient. InProceedingsoftheAAAIconferenceonartificialintelligence(AAAI),2017.
[37] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei, Paul
Christiano,andGeoffreyIrving. Fine-tuninglanguagemodelsfromhumanpreferences. arXivpreprint
arXiv:1909.08593,2019.
12A MethodsUnification
NegativeSampling. Asanearlyapproachforlanguagemodeling,specificallyforword2vecmodel[19],
negativesamplingisderivedfromthegoal[8]:
(cid:89) (cid:89)
argmax P(D=1|x,y;θ) P(D=0|x,y;θ) (19)
θ
(x,y)∈D (x,y)∈D′
Usingourunifiednotations,theequationbecomes:
argmaxlogP (y+|x+)+log(1−P (y−|x−)) (20)
θ θ
θ
GenerativeAdversarialNetworks. ThediscriminatorofaGAN[9,36]isoptimizedtoidentifyitsinput
dataisrealorfakeby:
argmaxE logD(y+|x+)+E log[1−D(y−|x−)] (21)
D
Dreal Dfake
Thediscriminatorisnormallyappendedwithasigmoidfunction, soD(·)outputsascalarwithin[0,1]. It
isthereforepossibletoviewD(·)asaprobabilityfunctionwithparameterssetθ. Theresultingformulais
replacingtheD inEquation21withP ,thusalmostthesameasunlikelihoodtraining[17,35]. Thethree
θ
differencesare:(1)thenegativesamplesaregeneratedbythegeneratorinGANandrule-basedconstructedin
Unlikelihoodtraining,(2)thesecondterminUnlikelihoodtrainingismultipliedwithacoefficienttoadjustthe
negativesamplesweightduringtraining,(3)Unlikelihoodtrainingconsidermultipletimestepsandmultiple
classes.
B ProofofLemma4.1
InLMswithsoftmaxfunctionbeforethefinalprediction,theoutputprobabilitydistributioncanbewrittenas
ezj
,wherethez ,z ,...z arethepredictedlogitforeachtokeninthevocabularysetVwithsizeK.The
(cid:80)K ezk 1 2 K
k=1
gradientwithrespecttoθfromalogitz is:
i
∇ f
=(cid:88)K
(
∂ ezj )∂z
i , (22)
θ θ ∂z i(cid:80)K ezk ∂θ
i=1 k=1
∂ ezj (cid:26) softmaxj(1−softmaxj),ifi=j
= θ θ (23)
∂z i(cid:80)K k=1ezk −softmaxj θsoftmaxi θ,ifi̸=j
Sincethegradientofagivensample(j-thtokenhere)isafunctionofsoftmax andj, thedifferenceof
θ
gradientsbetweentwogivensamplesisdeterminedby(1)theirsoftmaxdistributiondifferenceand(2)whether
thetargettokens(j)arethesame.
C GradientsDerivationsofSection3
C.1 Derivationof∇ L
θ DPO
(cid:20) (cid:18) P (y+|x+) P (y−|x−) (cid:19)(cid:21)
∇ L =∇ −E logσ βlog θ −βlog θ
θ DPO θ D P (y+|x+) P (y−|x−)
ref ref
(cid:20) (cid:18) P (y+|x+) P (y−|x−) (cid:19)(cid:21) (cid:16) (cid:17)
=−E σ −βlog θ +βlog θ ∇ βlogP (y+|x+)−βlogP (y−|x−)
D P (y+|x+) P (y−|x−) θ θ θ
ref ref
(cid:20) (cid:18) P (y+|x+) P (y−|x−) (cid:19)(cid:21) (cid:16) (cid:17)
=−E σ −βlog θ +βlog θ β ∇ logP (y+|x+)−∇ logP (y−|x−)
D P (y+|x+) P (y−|x−) θ θ θ θ
ref ref
(cid:20) (cid:18) P (y+|x+) P (y−|x−) (cid:19)(cid:21) (cid:16)∇ P (y+|x+) ∇ P (y−|x−)(cid:17)
=−E σ −βlog θ +βlog θ β θ θ − θ θ
D P (y+|x+) P (y−|x−) P (y+|x+) P (y−|x−)
ref ref θ θ
(24)
13C.2 Derivationof∇ L
θ UL
∇ L =∇ −E
(cid:34) 1 (cid:88)T
logP (y+|x+,y+
)+βlog(cid:16)
1−P (y−|x−,y−
)(cid:17)(cid:35)
θ UL θ D T θ t <t θ t <t
t=1
(cid:34) (cid:35)
1 (cid:16) (cid:17)
≈∇ −E logP (y+|x+)+βlog 1−P (y−|x−) (25)
θ DT θ θ
(cid:34) (cid:35)
1 ∇ P (y+|x+) −∇ P (y−|x−)
=−E θ θ +β θ θ
DT P (y+|x+) 1−P (y−|x−)
θ θ
C.3 Derivationof∇ L
θ ExMATE
(cid:34) T (cid:18) T (cid:19)(cid:35)
∇ L =∇ −E 1 (cid:88) logP (y+|x+,y+ )−βexp 1 (cid:88) logP (y−|x−,y− )
θ ExMATE θ D T θ t <t T θ t <t
t=1 t=1
(cid:34) (cid:18) (cid:19)(cid:35)
1 1
=∇ −E logP (y+|x+)−βexp logP (y−|x−)
θ D T θ T θ
(26)
(cid:34) 1 ∇ P (y+|x+) (cid:18) 1(cid:19) (cid:35)
=−E θ θ −βexp ∇ P (y−|x−)
D T P (y+|x+) T θ θ
θ
(cid:34) (cid:35)
1 ∇ P (y+|x+)
=− E θ θ −β′∇ P (y−|x−)
T D P (y+|x+) θ θ
θ
14D DerivationsofSection5.1wheny+ ̸= y−
t t
WithanassumptionthatP (·)=1anddenotethat(1)P+(·):=P (·|x+,y+ ),P−(·):=P (·|x−,y− ),
ref θ <t θ <t
(2)f+ =u∈[0,1],f− =u+ϵ∈[0,1],and(3)∇ ζ ∈R|V|×|θ| := ∂g θ+ ≈ ∂g θ− forbrevity.
θ θ θ ∂θ ∂θ
(cid:20) (cid:18) P (y+|x+) P (y−|x−) (cid:19)(cid:21) (cid:16)∇ P (y+|x+) ∇ P (y−|x−)(cid:17)
∇ L =−E σ −βlog θ +βlog θ β θ θ − θ θ
θ DPO D P (y+|x+) P (y−|x−) P (y+|x+) P (y−|x−)
ref ref θ θ
=−E
(cid:20) σ(cid:18) βlog(u+ϵ)−βlogu(cid:19)(cid:21) β(cid:88)T (cid:16)∇ θP+(y t+)
−
∇ θP−(y t−)(cid:17)
D P+(y+) P−(y−)
t=1 t t
=−E
σ(cid:18) log(u+ϵβ )(cid:19) β(cid:88)T (cid:16)∇ θP+(y t+)
−
∇ θP−(y t−)(cid:17)
D u P+(y+) P−(y−)
t=1 t t
=−E
(u+ϵ)β β(cid:88)T
∇
ζ·  P− +P (+
P
y( t++yt )+
(
(y) 1t+P −)+ P( +z () y− t+))− −P−
P
−( −y Pt−
(
−y)
t−
(P y)− t−( )z P) −, (if yt+z )̸= ,ify zt+, =z y̸= +y t−
D(u+ϵ)β+uβ
t=1
θ 
−P+
P(yP +t++
()
y(
P
t+y +t+ )()
yt−) − P−(yt−
P)P −(1−
(−
y(
tP
−yt−
−
)) (yt−)),ifz=yt
t−
(u+ϵ)β (cid:88)T


−P+(z)+P−(z),ifz̸=y t+,z̸=y t−
=−E β ∇ ζ· (1−P+(y+))+P−(y+),ifz=y+
D(u+ϵ)β+uβ θ t t t
t=1
 −P+(y t−)−(1−P−(y t−)),ifz=y t−
(27)
∇ L =−E
1 (cid:88)T (cid:34) ∇ θP+(y t+) +β−∇ θP−(y t−)(cid:35)
θ UL DT P+(y+) 1−P−(y−)
t=1 t t
=−E
1
(cid:88)T
∇
ζ·  − P+P (+
P
y( t++yt )+
(
(y) 1t+P −)+ P( +z () y− t+))β −−P β1−
−
−( Py P−t− −() (yP yt−− t−)( )z P) −, (if yt+z )̸= ,ify zt+, =z y̸= +y t−
DT
t=1
θ 
−P+
P(yP +t++
()
y(
P
t+y +t+ )()
yt−) −βP−(y 1t−
−1 )− P(1P −−−
(P
y( t−y −t− )(y) t−)),ifz=yt
t−
(28)
=−E
DT1 (cid:88) t=T
1∇
θζ· 

(
−−
1
PP
−
++
P
(( yz +) −(+
)y
−t+β
)
βP
)
P− 1 +−( −y P βt− (−
P
y) ( −P −y 1− )t−
( −y
,( )
Pt−
iz f)
−)
zP, (i
y−
=f t−(z
y
)
yt+̸= −),y it+ f, zz ≠= yy t+t−
t t t
(cid:34) T (cid:18) T (cid:19)(cid:35)
∇ L =∇ −E 1 (cid:88) logP (y+|x+,y+ )−βexp 1 (cid:88) logP (y−|x−,y− )
θ ExMATE θ D T θ t <t T θ t <t
t=1 t=1
≈−1 (cid:88)T
E
(cid:34) ∇ θP+(y t+)
−β∇
P−(y−)(cid:35)
T D P+(y+) θ t
t=1 t
=
1
(cid:88)T
∇
ζ·  − P+P (+
P
y( t++yt )+
( (y
1) t+P −)+ P( +z () y+ t+))βP +− β( Py t− −) (P y−− )( Pz) −, (i yf +z )̸= ,ify
t
z+, =z y̸= +y t−
(29)
T
t=1
θ 
−P+
P(yP +t++
()
y(
P
t+y +t+ )()
yt−) −βP−(y
t−)t (1−P−t
(y
t−)),ifz=t
y t−
1 (cid:88)T


−P+(z)+βP−(y t−)P−(z),ifz̸=y t+,z̸=y t−
= ∇ ζ· (1−P+(y+))+βP−(y−)P−(y+),ifz=y+
T θ t t t t
t=1
 −P+(y t−)−βP−(y t−)(1−P−(y t−)),ifz=y t−
15E DerivationsofSection5.2wheny+ = y− := y
t t t
Wefirstprovethat∇ f+ ≈∇ f−giventhaty+ =y−andP+ ≈P−:
θ θ θ θ t t t t
(cid:26) −P+(y )P+(z),ifz̸=y
∇ f+ = t t
θ θ P+(y )(1−P+(y )),ifz=y
t t t
(cid:26) −P−(y )P−(z),ifz̸=y (30)
≈ t t
P−(y )(1−P−(y )),ifz=y
t t t
=∇ f−.
θ θ
WethenreducethegradientsofDPO,Unlikelihood,andExMATEasfollowing:
(cid:104) (cid:16) f− f+ (cid:17)(cid:16)∇ f+ ∇ f−(cid:17)(cid:105)
∇ L =−βE σ βlog θ −βlog θ θ θ − θ θ
θ DPO D f− f+ f+ f−
ref ref θ θ
(cid:104) (u+ϵ)β (cid:16)∇ f ∇ f (cid:17)(cid:105)
=−βE θ θ − θ θ (31)
D (u+ϵ)β+uβ u u+ϵ
(cid:104) (u+ϵ)β−1ϵ (cid:105)
=−βE ∇ f
D ((u+ϵ)β+uβ)u θ θ
(cid:16)∇ f+ −∇ f−(cid:17)
∇ L =−E θ θ +β θ θ
θ UL D f+ 1−f−
θ θ
(cid:16)∇ f −∇ f (cid:17)
=−E θ θ +β θ θ (32)
D u 1−u−ϵ
(cid:104)1−(1+β)u−ϵ (cid:105)
=−βE ∇ f
D u(1−u−ϵ) θ θ
(cid:16)∇ f+ (cid:17)
∇ L =−E θ θ −β∇ f−
θ ExMATE D f+ θ θ
θ
=−E (cid:16)∇ θf θ −β∇ f (cid:17) (33)
D u θ θ
(cid:16)1 (cid:17)
=−E −β ∇ f
D u θ θ
F ExperimentDetails
WeuseCausalDialoguedataset(https://github.com/Pascalson/CausalDialogue/tree/main/data)
under GNU Free Document License and Anthropic HH-RLHF dataset (https://huggingface.co/
datasets/Anthropic/hh-rlhf)underMITLisense.
WeusesingleNVIDIARTXA6000fortrainingeachmodelonCausalDialogueandcomsumearound20G
GPUmemoryand5-10hours.WeusefourNVIDIAA100tofine-tuneeachPythia2.8BmodelonAnthropic
HH-RLHFandconsumearound39GmemoryperGPUand2-4hours;WeuseeightNVIDIAA100tofine-tune
eachPythia6.9Bmodelandconsumearound35GmemoryperGPUand3-10hours.Thetimeconsumingrange
isduetothatDPOtakesaroundtwicememoryandoperationsthanothermethodsandmayneedtochangeits
usedbatchsize.DPOintheendtakesaround1.5-3timesoftrainingtimepermodel.
Theerrorbarsweshowninfiguresare95%confidenceinterval.
TheGPT4judgementexperimentisconductedusingthegpt-4-turbo-2024-04-09modelversion.Wegivethe
modelaconversationbetweenHumanandAssistantandtwooptionsResponse1andResponse2.Wetheninstruct
themodel“Youarethehumanintheconversation.Tellmewhichresponseyouprefer.”Wefurtherverifythe
GPT4judgementbyaskingparticipatinghumansthesametask.
G Experiments: Sensitivitytohyper-parameters
WefurthertesthowthecoefficientβinallRGPBmethodsimpacttheiroptimizationresults,aswediscussed
inSection5thatβ canimpactthegradientsmuch. TheresultsareplottedinFigure7. Asweraiseβ,the
probabilitiesofExMATEdecaywhiletheagilityincreases.Incontrast,asβrises,theagilityofDPO(regardless
oftheSFTstage)decaysandtheprobabilitiesarelessdeteriorated.Thisisasthegradientanalysisthathigherβ
16(a) ExMATE (b) Unlikelihood (c) DPO (d) DPO with SFT
Figure7: (a)(b)(c)PositiveexamplesprobabilityandATEagilityusingExMATE,Unlikelihood,and
DPOtofinetunePythia-2.8Bmodelwithoutsupervisedfine-tuning. (d)ResultsofSFT+DPO.
switchExMATEfromhavingmostlypositivegradientstohavesomenegativegradient,thereforedecreasing
theprobabilities.Meanwhile,higherβmakesDPOtohavesmallernegativegradient,thusreducingtherateof
decreasingtheprobabilities.
17