[
    {
        "title": "SAM2Point: Segment Any 3D as Videos in Zero-shot and Promptable Manners",
        "authors": "Ziyu GuoRenrui ZhangXiangyang ZhuChengzhuo TongPeng GaoChunyuan LiPheng-Ann Heng",
        "links": "http://arxiv.org/abs/2408.16768v1",
        "entry_id": "http://arxiv.org/abs/2408.16768v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16768v1",
        "summary": "We introduce SAM2Point, a preliminary exploration adapting Segment Anything\nModel 2 (SAM 2) for zero-shot and promptable 3D segmentation. SAM2Point\ninterprets any 3D data as a series of multi-directional videos, and leverages\nSAM 2 for 3D-space segmentation, without further training or 2D-3D projection.\nOur framework supports various prompt types, including 3D points, boxes, and\nmasks, and can generalize across diverse scenarios, such as 3D objects, indoor\nscenes, outdoor environments, and raw sparse LiDAR. Demonstrations on multiple\n3D datasets, e.g., Objaverse, S3DIS, ScanNet, Semantic3D, and KITTI, highlight\nthe robust generalization capabilities of SAM2Point. To our best knowledge, we\npresent the most faithful implementation of SAM in 3D, which may serve as a\nstarting point for future research in promptable 3D segmentation. Online Demo:\nhttps://huggingface.co/spaces/ZiyuG/SAM2Point . Code:\nhttps://github.com/ZiyuGuo99/SAM2Point .",
        "updated": "2024-08-29 17:59:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16768v1"
    },
    {
        "title": "ReconX: Reconstruct Any Scene from Sparse Views with Video Diffusion Model",
        "authors": "Fangfu LiuWenqiang SunHanyang WangYikai WangHaowen SunJunliang YeJun ZhangYueqi Duan",
        "links": "http://arxiv.org/abs/2408.16767v1",
        "entry_id": "http://arxiv.org/abs/2408.16767v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16767v1",
        "summary": "Advancements in 3D scene reconstruction have transformed 2D images from the\nreal world into 3D models, producing realistic 3D results from hundreds of\ninput photos. Despite great success in dense-view reconstruction scenarios,\nrendering a detailed scene from insufficient captured views is still an\nill-posed optimization problem, often resulting in artifacts and distortions in\nunseen areas. In this paper, we propose ReconX, a novel 3D scene reconstruction\nparadigm that reframes the ambiguous reconstruction challenge as a temporal\ngeneration task. The key insight is to unleash the strong generative prior of\nlarge pre-trained video diffusion models for sparse-view reconstruction.\nHowever, 3D view consistency struggles to be accurately preserved in directly\ngenerated video frames from pre-trained models. To address this, given limited\ninput views, the proposed ReconX first constructs a global point cloud and\nencodes it into a contextual space as the 3D structure condition. Guided by the\ncondition, the video diffusion model then synthesizes video frames that are\nboth detail-preserved and exhibit a high degree of 3D consistency, ensuring the\ncoherence of the scene from various perspectives. Finally, we recover the 3D\nscene from the generated video through a confidence-aware 3D Gaussian Splatting\noptimization scheme. Extensive experiments on various real-world datasets show\nthe superiority of our ReconX over state-of-the-art methods in terms of quality\nand generalizability.",
        "updated": "2024-08-29 17:59:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16767v1"
    },
    {
        "title": "A Score-Based Density Formula, with Applications in Diffusion Generative Models",
        "authors": "Gen LiYuling Yan",
        "links": "http://arxiv.org/abs/2408.16765v1",
        "entry_id": "http://arxiv.org/abs/2408.16765v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16765v1",
        "summary": "Score-based generative models (SGMs) have revolutionized the field of\ngenerative modeling, achieving unprecedented success in generating realistic\nand diverse content. Despite empirical advances, the theoretical basis for why\noptimizing the evidence lower bound (ELBO) on the log-likelihood is effective\nfor training diffusion generative models, such as DDPMs, remains largely\nunexplored. In this paper, we address this question by establishing a density\nformula for a continuous-time diffusion process, which can be viewed as the\ncontinuous-time limit of the forward process in an SGM. This formula reveals\nthe connection between the target density and the score function associated\nwith each step of the forward process. Building on this, we demonstrate that\nthe minimizer of the optimization objective for training DDPMs nearly coincides\nwith that of the true objective, providing a theoretical foundation for\noptimizing DDPMs using the ELBO. Furthermore, we offer new insights into the\nrole of score-matching regularization in training GANs, the use of ELBO in\ndiffusion classifiers, and the recently proposed diffusion loss.",
        "updated": "2024-08-29 17:59:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16765v1"
    },
    {
        "title": "Dissecting Out-of-Distribution Detection and Open-Set Recognition: A Critical Analysis of Methods and Benchmarks",
        "authors": "Hongjun WangSagar VazeKai Han",
        "links": "http://arxiv.org/abs/2408.16757v2",
        "entry_id": "http://arxiv.org/abs/2408.16757v2",
        "pdf_url": "http://arxiv.org/pdf/2408.16757v2",
        "summary": "Detecting test-time distribution shift has emerged as a key capability for\nsafely deployed machine learning models, with the question being tackled under\nvarious guises in recent years. In this paper, we aim to provide a consolidated\nview of the two largest sub-fields within the community: out-of-distribution\n(OOD) detection and open-set recognition (OSR). In particular, we aim to\nprovide rigorous empirical analysis of different methods across settings and\nprovide actionable takeaways for practitioners and researchers. Concretely, we\nmake the following contributions: (i) We perform rigorous cross-evaluation\nbetween state-of-the-art methods in the OOD detection and OSR settings and\nidentify a strong correlation between the performances of methods for them;\n(ii) We propose a new, large-scale benchmark setting which we suggest better\ndisentangles the problem tackled by OOD detection and OSR, re-evaluating\nstate-of-the-art OOD detection and OSR methods in this setting; (iii) We\nsurprisingly find that the best performing method on standard benchmarks\n(Outlier Exposure) struggles when tested at scale, while scoring rules which\nare sensitive to the deep feature magnitude consistently show promise; and (iv)\nWe conduct empirical analysis to explain these phenomena and highlight\ndirections for future research. Code:\nhttps://github.com/Visual-AI/Dissect-OOD-OSR",
        "updated": "2024-08-30 02:26:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16757v2"
    },
    {
        "title": "Assessing Large Language Models for Online Extremism Research: Identification, Explanation, and New Knowledge",
        "authors": "Beidi DongJin R. LeeZiwei ZhuBalassubramanian Srinivasan",
        "links": "http://arxiv.org/abs/2408.16749v1",
        "entry_id": "http://arxiv.org/abs/2408.16749v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16749v1",
        "summary": "The United States has experienced a significant increase in violent\nextremism, prompting the need for automated tools to detect and limit the\nspread of extremist ideology online. This study evaluates the performance of\nBidirectional Encoder Representations from Transformers (BERT) and Generative\nPre-Trained Transformers (GPT) in detecting and classifying online domestic\nextremist posts. We collected social media posts containing \"far-right\" and\n\"far-left\" ideological keywords and manually labeled them as extremist or\nnon-extremist. Extremist posts were further classified into one or more of five\ncontributing elements of extremism based on a working definitional framework.\nThe BERT model's performance was evaluated based on training data size and\nknowledge transfer between categories. We also compared the performance of GPT\n3.5 and GPT 4 models using different prompts: na\\\"ive, layperson-definition,\nrole-playing, and professional-definition. Results showed that the best\nperforming GPT models outperformed the best performing BERT models, with more\ndetailed prompts generally yielding better results. However, overly complex\nprompts may impair performance. Different versions of GPT have unique\nsensitives to what they consider extremist. GPT 3.5 performed better at\nclassifying far-left extremist posts, while GPT 4 performed better at\nclassifying far-right extremist posts. Large language models, represented by\nGPT models, hold significant potential for online extremism classification\ntasks, surpassing traditional BERT models in a zero-shot setting. Future\nresearch should explore human-computer interactions in optimizing GPT models\nfor extremist detection and classification tasks to develop more efficient\n(e.g., quicker, less effort) and effective (e.g., fewer errors or mistakes)\nmethods for identifying extremist content.",
        "updated": "2024-08-29 17:43:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16749v1"
    }
]