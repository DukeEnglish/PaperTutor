[
    {
        "title": "A Score-Based Density Formula, with Applications in Diffusion Generative Models",
        "authors": "Gen LiYuling Yan",
        "links": "http://arxiv.org/abs/2408.16765v1",
        "entry_id": "http://arxiv.org/abs/2408.16765v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16765v1",
        "summary": "Score-based generative models (SGMs) have revolutionized the field of\ngenerative modeling, achieving unprecedented success in generating realistic\nand diverse content. Despite empirical advances, the theoretical basis for why\noptimizing the evidence lower bound (ELBO) on the log-likelihood is effective\nfor training diffusion generative models, such as DDPMs, remains largely\nunexplored. In this paper, we address this question by establishing a density\nformula for a continuous-time diffusion process, which can be viewed as the\ncontinuous-time limit of the forward process in an SGM. This formula reveals\nthe connection between the target density and the score function associated\nwith each step of the forward process. Building on this, we demonstrate that\nthe minimizer of the optimization objective for training DDPMs nearly coincides\nwith that of the true objective, providing a theoretical foundation for\noptimizing DDPMs using the ELBO. Furthermore, we offer new insights into the\nrole of score-matching regularization in training GANs, the use of ELBO in\ndiffusion classifiers, and the recently proposed diffusion loss.",
        "updated": "2024-08-29 17:59:07 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16765v1"
    },
    {
        "title": "A Gradient Analysis Framework for Rewarding Good and Penalizing Bad Examples in Language Models",
        "authors": "Yi-Lin TuanWilliam Yang Wang",
        "links": "http://arxiv.org/abs/2408.16751v1",
        "entry_id": "http://arxiv.org/abs/2408.16751v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16751v1",
        "summary": "Beyond maximum likelihood estimation (MLE), the standard objective of a\nlanguage model (LM) that optimizes good examples probabilities, many studies\nhave explored ways that also penalize bad examples for enhancing the quality of\noutput distribution, including unlikelihood training, exponential maximizing\naverage treatment effect (ExMATE), and direct preference optimization (DPO). To\nsystematically compare these methods and further provide a unified recipe for\nLM optimization, in this paper, we present a unique angle of gradient analysis\nof loss functions that simultaneously reward good examples and penalize bad\nones in LMs. Through both mathematical results and experiments on\nCausalDialogue and Anthropic HH-RLHF datasets, we identify distinct functional\ncharacteristics among these methods. We find that ExMATE serves as a superior\nsurrogate for MLE, and that combining DPO with ExMATE instead of MLE further\nenhances both the statistical (5-7%) and generative (+18% win rate)\nperformance.",
        "updated": "2024-08-29 17:46:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16751v1"
    },
    {
        "title": "Statistical and Geometrical properties of regularized Kernel Kullback-Leibler divergence",
        "authors": "Clémentine ChazalAnna KorbaFrancis Bach",
        "links": "http://arxiv.org/abs/2408.16543v1",
        "entry_id": "http://arxiv.org/abs/2408.16543v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16543v1",
        "summary": "In this paper, we study the statistical and geometrical properties of the\nKullback-Leibler divergence with kernel covariance operators (KKL) introduced\nby Bach [2022]. Unlike the classical Kullback-Leibler (KL) divergence that\ninvolves density ratios, the KKL compares probability distributions through\ncovariance operators (embeddings) in a reproducible kernel Hilbert space\n(RKHS), and compute the Kullback-Leibler quantum divergence. This novel\ndivergence hence shares parallel but different aspects with both the standard\nKullback-Leibler between probability distributions and kernel embeddings\nmetrics such as the maximum mean discrepancy. A limitation faced with the\noriginal KKL divergence is its inability to be defined for distributions with\ndisjoint supports. To solve this problem, we propose in this paper a\nregularised variant that guarantees that the divergence is well defined for all\ndistributions. We derive bounds that quantify the deviation of the regularised\nKKL to the original one, as well as finite-sample bounds. In addition, we\nprovide a closed-form expression for the regularised KKL, specifically\napplicable when the distributions consist of finite sets of points, which makes\nit implementable. Furthermore, we derive a Wasserstein gradient descent scheme\nof the KKL divergence in the case of discrete distributions, and study\nempirically its properties to transport a set of points to a target\ndistribution.",
        "updated": "2024-08-29 14:01:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16543v1"
    },
    {
        "title": "Gradient-free variational learning with conditional mixture networks",
        "authors": "Conor HeinsHao WuDimitrije MarkovicAlexander TschantzJeff BeckChristopher Buckley",
        "links": "http://arxiv.org/abs/2408.16429v1",
        "entry_id": "http://arxiv.org/abs/2408.16429v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16429v1",
        "summary": "Balancing computational efficiency with robust predictive performance is\ncrucial in supervised learning, especially for critical applications. Standard\ndeep learning models, while accurate and scalable, often lack probabilistic\nfeatures like calibrated predictions and uncertainty quantification. Bayesian\nmethods address these issues but can be computationally expensive as model and\ndata complexity increase. Previous work shows that fast variational methods can\nreduce the compute requirements of Bayesian methods by eliminating the need for\ngradient computation or sampling, but are often limited to simple models. We\ndemonstrate that conditional mixture networks (CMNs), a probabilistic variant\nof the mixture-of-experts (MoE) model, are suitable for fast, gradient-free\ninference and can solve complex classification tasks. CMNs employ linear\nexperts and a softmax gating network. By exploiting conditional conjugacy and\nP\\'olya-Gamma augmentation, we furnish Gaussian likelihoods for the weights of\nboth the linear experts and the gating network. This enables efficient\nvariational updates using coordinate ascent variational inference (CAVI),\navoiding traditional gradient-based optimization. We validate this approach by\ntraining two-layer CMNs on standard benchmarks from the UCI repository. Our\nmethod, CAVI-CMN, achieves competitive and often superior predictive accuracy\ncompared to maximum likelihood estimation (MLE) with backpropagation, while\nmaintaining competitive runtime and full posterior distributions over all model\nparameters. Moreover, as input size or the number of experts increases,\ncomputation time scales competitively with MLE and other gradient-based\nsolutions like black-box variational inference (BBVI), making CAVI-CMN a\npromising tool for deep, fast, and gradient-free Bayesian networks.",
        "updated": "2024-08-29 10:43:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16429v1"
    },
    {
        "title": "Iterated Energy-based Flow Matching for Sampling from Boltzmann Densities",
        "authors": "Dongyeop WooSungsoo Ahn",
        "links": "http://arxiv.org/abs/2408.16249v1",
        "entry_id": "http://arxiv.org/abs/2408.16249v1",
        "pdf_url": "http://arxiv.org/pdf/2408.16249v1",
        "summary": "In this work, we consider the problem of training a generator from\nevaluations of energy functions or unnormalized densities. This is a\nfundamental problem in probabilistic inference, which is crucial for scientific\napplications such as learning the 3D coordinate distribution of a molecule. To\nsolve this problem, we propose iterated energy-based flow matching (iEFM), the\nfirst off-policy approach to train continuous normalizing flow (CNF) models\nfrom unnormalized densities. We introduce the simulation-free energy-based flow\nmatching objective, which trains the model to predict the Monte Carlo\nestimation of the marginal vector field constructed from known energy\nfunctions. Our framework is general and can be extended to variance-exploding\n(VE) and optimal transport (OT) conditional probability paths. We evaluate iEFM\non a two-dimensional Gaussian mixture model (GMM) and an eight-dimensional\nfour-particle double-well potential (DW-4) energy function. Our results\ndemonstrate that iEFM outperforms existing methods, showcasing its potential\nfor efficient and scalable probabilistic modeling in complex high-dimensional\nsystems.",
        "updated": "2024-08-29 04:06:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.16249v1"
    }
]