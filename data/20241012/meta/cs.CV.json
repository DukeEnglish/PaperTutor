[
    {
        "title": "LatteCLIP: Unsupervised CLIP Fine-Tuning via LMM-Synthetic Texts",
        "authors": "Anh-Quan CaoMaximilian JaritzMatthieu GuillauminRaoul de CharetteLoris Bazzani",
        "links": "http://arxiv.org/abs/2410.08211v1",
        "entry_id": "http://arxiv.org/abs/2410.08211v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08211v1",
        "summary": "Large-scale vision-language pre-trained (VLP) models (e.g., CLIP) are\nrenowned for their versatility, as they can be applied to diverse applications\nin a zero-shot setup. However, when these models are used in specific domains,\ntheir performance often falls short due to domain gaps or the\nunder-representation of these domains in the training data. While fine-tuning\nVLP models on custom datasets with human-annotated labels can address this\nissue, annotating even a small-scale dataset (e.g., 100k samples) can be an\nexpensive endeavor, often requiring expert annotators if the task is complex.\nTo address these challenges, we propose LatteCLIP, an unsupervised method for\nfine-tuning CLIP models on classification with known class names in custom\ndomains, without relying on human annotations. Our method leverages Large\nMultimodal Models (LMMs) to generate expressive textual descriptions for both\nindividual images and groups of images. These provide additional contextual\ninformation to guide the fine-tuning process in the custom domains. Since\nLMM-generated descriptions are prone to hallucination or missing details, we\nintroduce a novel strategy to distill only the useful information and stabilize\nthe training. Specifically, we learn rich per-class prototype representations\nfrom noisy generated texts and dual pseudo-labels. Our experiments on 10\ndomain-specific datasets show that LatteCLIP outperforms pre-trained zero-shot\nmethods by an average improvement of +4.74 points in top-1 accuracy and other\nstate-of-the-art unsupervised methods by +3.45 points.",
        "updated": "2024-10-10 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08211v1"
    },
    {
        "title": "PointOBB-v2: Towards Simpler, Faster, and Stronger Single Point Supervised Oriented Object Detection",
        "authors": "Botao RenXue YangYi YuJunwei LuoZhidong Deng",
        "links": "http://arxiv.org/abs/2410.08210v1",
        "entry_id": "http://arxiv.org/abs/2410.08210v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08210v1",
        "summary": "Single point supervised oriented object detection has gained attention and\nmade initial progress within the community. Diverse from those approaches\nrelying on one-shot samples or powerful pretrained models (e.g. SAM), PointOBB\nhas shown promise due to its prior-free feature. In this paper, we propose\nPointOBB-v2, a simpler, faster, and stronger method to generate pseudo rotated\nboxes from points without relying on any other prior. Specifically, we first\ngenerate a Class Probability Map (CPM) by training the network with non-uniform\npositive and negative sampling. We show that the CPM is able to learn the\napproximate object regions and their contours. Then, Principal Component\nAnalysis (PCA) is applied to accurately estimate the orientation and the\nboundary of objects. By further incorporating a separation mechanism, we\nresolve the confusion caused by the overlapping on the CPM, enabling its\noperation in high-density scenarios. Extensive comparisons demonstrate that our\nmethod achieves a training speed 15.58x faster and an accuracy improvement of\n11.60%/25.15%/21.19% on the DOTA-v1.0/v1.5/v2.0 datasets compared to the\nprevious state-of-the-art, PointOBB. This significantly advances the cutting\nedge of single point supervised oriented detection in the modular track.",
        "updated": "2024-10-10 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08210v1"
    },
    {
        "title": "Emerging Pixel Grounding in Large Multimodal Models Without Grounding Supervision",
        "authors": "Shengcao CaoLiang-Yan GuiYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2410.08209v1",
        "entry_id": "http://arxiv.org/abs/2410.08209v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08209v1",
        "summary": "Current large multimodal models (LMMs) face challenges in grounding, which\nrequires the model to relate language components to visual entities. Contrary\nto the common practice that fine-tunes LMMs with additional grounding\nsupervision, we find that the grounding ability can in fact emerge in LMMs\ntrained without explicit grounding supervision. To reveal this emerging\ngrounding, we introduce an \"attend-and-segment\" method which leverages\nattention maps from standard LMMs to perform pixel-level segmentation.\nFurthermore, to enhance the grounding ability, we propose DIFFLMM, an LMM\nutilizing a diffusion-based visual encoder, as opposed to the standard CLIP\nvisual encoder, and trained with the same weak supervision. Without being\nconstrained by the biases and limited scale of grounding-specific supervision\ndata, our approach is more generalizable and scalable. We achieve competitive\nperformance on both grounding-specific and general visual question answering\nbenchmarks, compared with grounding LMMs and generalist LMMs, respectively.\nNotably, we achieve a 44.2 grounding mask recall on grounded conversation\ngeneration without any grounding supervision, outperforming the extensively\nsupervised model GLaMM. Project page: https://groundLMM.github.io.",
        "updated": "2024-10-10 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08209v1"
    },
    {
        "title": "SPA: 3D Spatial-Awareness Enables Effective Embodied Representation",
        "authors": "Haoyi ZhuHonghui YangYating WangJiange YangLimin WangTong He",
        "links": "http://arxiv.org/abs/2410.08208v1",
        "entry_id": "http://arxiv.org/abs/2410.08208v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08208v1",
        "summary": "In this paper, we introduce SPA, a novel representation learning framework\nthat emphasizes the importance of 3D spatial awareness in embodied AI. Our\napproach leverages differentiable neural rendering on multi-view images to\nendow a vanilla Vision Transformer (ViT) with intrinsic spatial understanding.\nWe present the most comprehensive evaluation of embodied representation\nlearning to date, covering 268 tasks across 8 simulators with diverse policies\nin both single-task and language-conditioned multi-task scenarios. The results\nare compelling: SPA consistently outperforms more than 10 state-of-the-art\nrepresentation methods, including those specifically designed for embodied AI,\nvision-centric tasks, and multi-modal applications, while using less training\ndata. Furthermore, we conduct a series of real-world experiments to confirm its\neffectiveness in practical scenarios. These results highlight the critical role\nof 3D spatial awareness for embodied representation learning. Our strongest\nmodel takes more than 6000 GPU hours to train and we are committed to\nopen-sourcing all code and model weights to foster future research in embodied\nrepresentation learning. Project Page: https://haoyizhu.github.io/spa/.",
        "updated": "2024-10-10 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08208v1"
    },
    {
        "title": "DICE: Discrete Inversion Enabling Controllable Editing for Multinomial Diffusion and Masked Generative Models",
        "authors": "Xiaoxiao HeLigong HanQuan DaoSong WenMinhao BaiDi LiuHan ZhangMartin Renqiang MinFelix Juefei-XuChaowei TanBo LiuKang LiHongdong LiJunzhou HuangFaez AhmedAkash SrivastavaDimitris Metaxas",
        "links": "http://arxiv.org/abs/2410.08207v1",
        "entry_id": "http://arxiv.org/abs/2410.08207v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08207v1",
        "summary": "Discrete diffusion models have achieved success in tasks like image\ngeneration and masked language modeling but face limitations in controlled\ncontent editing. We introduce DICE (Discrete Inversion for Controllable\nEditing), the first approach to enable precise inversion for discrete diffusion\nmodels, including multinomial diffusion and masked generative models. By\nrecording noise sequences and masking patterns during the reverse diffusion\nprocess, DICE enables accurate reconstruction and flexible editing of discrete\ndata without the need for predefined masks or attention manipulation. We\ndemonstrate the effectiveness of DICE across both image and text domains,\nevaluating it on models such as VQ-Diffusion, Paella, and RoBERTa. Our results\nshow that DICE preserves high data fidelity while enhancing editing\ncapabilities, offering new opportunities for fine-grained content manipulation\nin discrete spaces. For project webpage, see\nhttps://hexiaoxiao-cs.github.io/DICE/.",
        "updated": "2024-10-10 17:59:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08207v1"
    }
]