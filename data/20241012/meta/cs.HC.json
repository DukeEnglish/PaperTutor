[
    {
        "title": "SoundScape: A Human-AI Co-Creation System Making Your Memories Heard",
        "authors": "Chongjun ZhongJiaxing YuYingping CaoSongruoyao WuWenqi WuKejun Zhang",
        "links": "http://arxiv.org/abs/2410.08136v1",
        "entry_id": "http://arxiv.org/abs/2410.08136v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08136v1",
        "summary": "Sound plays a significant role in human memory, yet it is often overlooked by\nmainstream life-recording methods. Most current UGC (User-Generated Content)\ncreation tools emphasize visual content while lacking user-friendly sound\ndesign features. This paper introduces SoundScape, a human-AI co-creation\nsystem that allows users to easily create sound memories on mobile devices\nthrough innovative interaction. By integrating sound effects and music with\nvisual scenes, SoundScape encourages users to enrich their creations with\nimmersive sound elements, enhancing the atmosphere of their works. To support\npublic creation, SoundScape incorporates a conversational agent and AI music\ngeneration technology. User studies indicate that our approach is effective for\nsound memory creation, with SoundScape outperforming existing tools in terms of\nuser experience and the perceived quality of produced works.",
        "updated": "2024-10-10 17:19:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08136v1"
    },
    {
        "title": "Crossing Margins: Intersectional Users' Ethical Concerns about Software",
        "authors": "Lauren OlsonTom P. HumbertRicarda Anna-Lena FischerBob WesterveldFlorian KunnemanEmitzá Guzmán",
        "links": "http://arxiv.org/abs/2410.08090v1",
        "entry_id": "http://arxiv.org/abs/2410.08090v1",
        "pdf_url": "http://arxiv.org/pdf/2410.08090v1",
        "summary": "Many modern software applications present numerous ethical concerns due to\nconflicts between users' values and companies' priorities. Intersectional\ncommunities, those with multiple marginalized identities, are\ndisproportionately affected by these ethical issues, leading to legal,\nfinancial, and reputational issues for software companies, as well as\nreal-world harm for intersectional users. Historically, the voices of\nintersectional communities have been systematically marginalized and excluded\nfrom contributing their unique perspectives to software design, perpetuating\nsoftware-related ethical concerns.\n  This work aims to fill the gap in research on intersectional users'\nsoftware-related perspectives and provide software practitioners with a\nstarting point to address their ethical concerns. We aggregated and analyzed\nthe intersectional users' ethical concerns over time and developed a\nprioritization method to identify critical concerns. To achieve this, we\ncollected posts from over 700 intersectional subreddits discussing software\napplications, utilized deep learning to identify ethical concerns in these\nposts, and employed state-of-the-art techniques to analyze their content in\nrelation to time and priority. Our findings revealed that intersectional\ncommunities report \\textit{critical} complaints related to cyberbullying,\ninappropriate content, and discrimination, highlighting significant flaws in\nmodern software, particularly for intersectional users. Based on these\nfindings, we discuss how to better address the ethical concerns of\nintersectional users in software development.",
        "updated": "2024-10-10 16:33:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.08090v1"
    },
    {
        "title": "APOLLO: A GPT-based tool to detect phishing emails and generate explanations that warn users",
        "authors": "Giuseppe DesoldaFrancesco GrecoLuca Viganò",
        "links": "http://arxiv.org/abs/2410.07997v1",
        "entry_id": "http://arxiv.org/abs/2410.07997v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07997v1",
        "summary": "Phishing is one of the most prolific cybercriminal activities, with attacks\nbecoming increasingly sophisticated. It is, therefore, imperative to explore\nnovel technologies to improve user protection across both technical and human\ndimensions. Large Language Models (LLMs) offer significant promise for text\nprocessing in various domains, but their use for defense against phishing\nattacks still remains scarcely explored. In this paper, we present APOLLO, a\ntool based on OpenAI's GPT-4o to detect phishing emails and generate\nexplanation messages to users about why a specific email is dangerous, thus\nimproving their decision-making capabilities. We have evaluated the performance\nof APOLLO in classifying phishing emails; the results show that the LLM models\nhave exemplary capabilities in classifying phishing emails (97 percent accuracy\nin the case of GPT-4o) and that this performance can be further improved by\nintegrating data from third-party services, resulting in a near-perfect\nclassification rate (99 percent accuracy). To assess the perception of the\nexplanations generated by this tool, we also conducted a study with 20\nparticipants, comparing four different explanations presented as phishing\nwarnings. We compared the LLM-generated explanations to four baselines: a\nmanually crafted warning, and warnings from Chrome, Firefox, and Edge browsers.\nThe results show that not only the LLM-generated explanations were perceived as\nhigh quality, but also that they can be more understandable, interesting, and\ntrustworthy than the baselines. These findings suggest that using LLMs as a\ndefense against phishing is a very promising approach, with APOLLO representing\na proof of concept in this research direction.",
        "updated": "2024-10-10 14:53:39 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07997v1"
    },
    {
        "title": "Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic Analysis of Annotators and Targets",
        "authors": "Tommaso GiorgiLorenzo CimaTiziano FagniMarco AvvenutiStefano Cresci",
        "links": "http://arxiv.org/abs/2410.07991v1",
        "entry_id": "http://arxiv.org/abs/2410.07991v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07991v1",
        "summary": "The rise of online platforms exacerbated the spread of hate speech, demanding\nscalable and effective detection. However, the accuracy of hate speech\ndetection systems heavily relies on human-labeled data, which is inherently\nsusceptible to biases. While previous work has examined the issue, the\ninterplay between the characteristics of the annotator and those of the target\nof the hate are still unexplored. We fill this gap by leveraging an extensive\ndataset with rich socio-demographic information of both annotators and targets,\nuncovering how human biases manifest in relation to the target's attributes.\nOur analysis surfaces the presence of widespread biases, which we\nquantitatively describe and characterize based on their intensity and\nprevalence, revealing marked differences. Furthermore, we compare human biases\nwith those exhibited by persona-based LLMs. Our findings indicate that while\npersona-based LLMs do exhibit biases, these differ significantly from those of\nhuman annotators. Overall, our work offers new and nuanced results on human\nbiases in hate speech annotations, as well as fresh insights into the design of\nAI-driven hate speech detection systems.",
        "updated": "2024-10-10 14:48:57 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07991v1"
    },
    {
        "title": "Post-Training Quantization in Brain-Computer Interfaces based on Event-Related Potential Detection",
        "authors": "Hubert CecottiDalvir DhaliwalHardip SinghYogesh Kumar Meena",
        "links": "http://arxiv.org/abs/2410.07920v1",
        "entry_id": "http://arxiv.org/abs/2410.07920v1",
        "pdf_url": "http://arxiv.org/pdf/2410.07920v1",
        "summary": "Post-training quantization (PTQ) is a technique used to optimize and reduce\nthe memory footprint and computational requirements of machine learning models.\nIt has been used primarily for neural networks. For Brain-Computer Interfaces\n(BCI) that are fully portable and usable in various situations, it is necessary\nto provide approaches that are lightweight for storage and computation. In this\npaper, we propose the evaluation of post-training quantization on\nstate-of-the-art approaches in brain-computer interfaces and assess their\nimpact on accuracy. We evaluate the performance of the single-trial detection\nof event-related potentials representing one major BCI paradigm. The area under\nthe receiver operating characteristic curve drops from 0.861 to 0.825 with PTQ\nwhen applied on both spatial filters and the classifier, while reducing the\nsize of the model by about $\\times$ 15. The results support the conclusion that\nPTQ can substantially reduce the memory footprint of the models while keeping\nroughly the same level of accuracy.",
        "updated": "2024-10-10 13:47:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.07920v1"
    }
]