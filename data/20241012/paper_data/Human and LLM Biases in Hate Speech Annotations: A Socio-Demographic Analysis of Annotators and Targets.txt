Human and LLM Biases in Hate Speech Annotations:
A Socio-Demographic Analysis of Annotators and Targets
TommasoGiorgi,2* LorenzoCima,1,2*† TizianoFagni,1 MarcoAvvenuti,2 StefanoCresci1
1IIT-CNR,Italy,[name.surname]@iit.cnr.it
2UniversityofPisa,Italy,
tommaso.giorgi@studenti.unipi.it;lorenzo.cima@phd.unipi.it;marco.avvenuti@unipi.it
Abstract
The rise of online platforms exacerbated the spread of hate
speech, demanding scalable and effective detection. How-
ever, the accuracy of hate speech detection systems heavily
reliesonhuman-labeleddata,whichisinherentlysusceptible
tobiases.Whilepreviousworkhasexaminedtheissue,thein-
terplaybetweenthecharacteristicsoftheannotatorandthose
ofthetargetofthehatearestillunexplored.Wefillthisgapby
leveraginganextensivedatasetwithrichsocio-demographic
information of both annotators and targets, uncovering how
Figure1:Bothhumanandpersona-basedLLMhatespeech
human biases manifest in relation to the target’s attributes.
annotations can be influenced by the interplay between the
Our analysis surfaces the presence of widespread biases,
annotator’sandtarget’ssocio-demographicattributes.This
which we quantitatively describe and characterize based on
theirintensityandprevalence,revealingmarkeddifferences. possiblesourceofbiasishoweverlittle-explored.
Furthermore, we compare human biases with those exhib-
itedbypersona-basedLLMs.Ourfindingsindicatethatwhile
persona-based LLMs do exhibit biases, these differ signifi- biases (Davani et al. 2023). In turn, biases may seep into
cantlyfromthoseofhumanannotators.Overall,ourworkof- theautomateddetectionsystemstrainedonsuchdata,caus-
fersnewandnuancedresultsonhumanbiasesinhatespeech ing severe consequences such as the unwarranted removal
annotations, as well as fresh insights into the design of AI-
of non-hateful content or the disproportionate targeting of
drivenhatespeechdetectionsystems.
marginalizedgroups(Nogaraetal.2025).
Warning: This paper contains examples that may be per-
Large language models (LLMs) have recently extended
ceivedasoffensiveorupsetting.
the set of AI tools in support of platform administrators.
However, LLMs are not immune to the biases embedded
Introduction inthetextualdataonwhichtheyaretrained(Baack2024).
Consequently, also these models may reflect and perpetu-
Theproliferationofhatefulandtoxicspeechononlinesocial
ate stereotypes, prejudices, and biases (Perez et al. 2023).
platformsisamongthemostseverethreatstoinclusiveon-
Despite the development of various techniques aimed at
line spaces (Mathew et al. 2019). The exponential increase
reducing LLM biases, this remains an outstanding chal-
inuser-generatedcontenthasmademanualhatespeechde-
lenge (Ouyang et al. 2022). Interestingly, the capacity of
tection impractical, driving the development of automated
LLMs to produce subjective outputs can also serve benefi-
systems(Gargetal.2023).However,theeffectivenessofau-
cial purposes. LLMs can be personalized and deployed as
tomated detectors is heavily contingent upon the quality of
actors in agent-based simulations of online human interac-
theirtrainingdata,whichistypicallyhuman-labeled.Chal-
tions (To¨rnberg et al. 2023), which represents a promising
lenges arise as hate speech detection is inherently subjec-
way to compensate for the limited availability of platform
tive:whatoneindividualmayperceiveasoffensive,another
datainthepost-APIage.Byaccuratelyreproducingcertain
might consider acceptable or even normal. This subjectiv-
human biases, persona-based LLMs could lay the ground-
ity is shaped by the annotators’ backgrounds, experiences,
workforthenextgenerationofAI-drivensocialsimulations.
andculturalcontexts.Assuch,socio-demographicattributes
For these reasons, investigating biases in both human hate
such as age, gender, race, and others, can influence how
speech annotations and persona-based LLMs is a task of
annotators interpret and label content, possibly leading to
greattheoreticalandpracticalrelevance(Tsengetal.2024).
Despitesignificantefforts,knowledgeofhumanbiasesin
*Equalcontributions
†Correspondingauthor. hatespeechannotationsisstilllimited.First,manyexisting
Copyright©2024,AssociationfortheAdvancementofArtificial studiesrelyonsmalldatasets.Thisresultsinworksthatonly
Intelligence(www.aaai.org).Allrightsreserved. consideranarrowsetofsocio-demographicattributes,stud-
4202
tcO
01
]LC.sc[
1v19970.0142:viXraiedinisolation(Prabhakaran,Davani,andD´ıaz2021).Sim- tors impact the quality of training datasets. Geva, Gold-
ilar limitations also exist in the nascent literature on LLM berg, and Berant (2019) demonstrated that datasets gen-
biases (Wan et al. 2023). Most importantly, so far the re- erated by a small number of high-quality crowd workers
lationship between the socio-demographic attributes of the lack diversity, which affects model generalization. Simi-
annotatorandthoseofthetargetofthehatehavebeenover- larly, Wich, Al Kuwatly, and Groh (2020) identified an-
looked.However,attributesofthetargetcansignificantlyin- notator bias in hate speech detection systems and pro-
fluence the annotator’s perception of what constitutes hate posed using community detection algorithms to group an-
speech. For example, a White, male, conservative-leaning notators and mitigate this bias. They also showed that
annotatormayinterpretanoffensivepostdirectedataBlack, socio-demographicattributessignificantlyaffectbiasinhate
transgenderwomandifferentlythananannotatorwithadif- speech datasets (Al Kuwatly, Wich, and Groh 2020). Par-
ferentbackground,potentiallydownplayingtheharmfulim- maretal.(2023)foundbiasesevenintheinstructionsused
pactofthecontent,assketchedinFigure1.Understanding forcrowdsourcingdataannotationstasks.Finally,Sapetal.
such dynamics is essential for accurately mapping human (2022)highlightedthatannotatoridentityandbeliefsaffect
andLLMbiases,andforensuringthathatespeechdetection toxicityratings,stressingtheneedforcontextualizinglabels
isbothfairandeffectiveacrossdiversepopulations. withsocialvariables,asdoneinourwork.
Research focus. Motivated by this knowledge gap, we Biasinhumanhatespeechannotations.Theimpactthat
investigate the biases that human annotators and persona- biased datasets have on the reliability and fairness of auto-
based LLMs exhibit in a hate speech annotation task. We matedhatespeechdetectionsystemssparkedmuchresearch
analyzeanextensiveandpubliclyavailabledatasetcontain- on the issue (Garg et al. 2023). Waseem and Hovy (2016)
ing136Khatespeechlabelsassignedby8Khumanannota- showedthatculturalandsocio-economicbackgroundslead
tors.Foreachhatefulmessage,wealsoanalyzethetargetof to variability hate speech annotations. Inherent prejudices
theattack.Targetsandannotatorsaredescribedbytensocio- furtherskewlabels,ashumanannotatorsmaybemoresen-
demographicattributes,assummarizedinTable1.Ouranal- sitive to hate speech against familiar groups while under-
ysisallowsustoanswerthefollowingresearchquestions: estimating it against less familiar ones. Others found that
RQ1a Arehumanannotatorsmoresensitivetohatespeech socio-demographicfactorssuchasage,gender,raceandeth-
directed at individuals or groups sharing their own nicity,education,andEnglishproficiency,influenceannota-
socio-demographicattributes? tion outcomes, with younger and minority annotators more
likely to label content as hate speech (Al Kuwatly, Wich,
RQ1b How do annotators with a specific socio-
andGroh2020;Davanietal.2023).Additionally,annotator
demographic attribute differ in their labeling of hate
beliefsanddemographicscanintroducefurtherinconsisten-
speechcomparedtoannotatorswithoutthatattribute?
cies in crowdsourced annotations (Hettiachchi et al. 2023).
InRQ1a,wehypothesizethatidentificationwiththetarget Although much research has explored socio-demographic
group(i.e.,theannotator’sin-group)mayheightentheanno- biases in hate speech annotations (Garg et al. 2023), most
tator’ssensitivitytohate,leadingthemtoflagsuchcontent studiesfocusedonalimitedsetofattributes,primarilyrace
more often (Sachdeva et al. 2022b). Conversely, when the andethnicity,gender,andage.Moreover,noneinvestigated
targetbelongstoanout-group,theannotator’sperceptionof theinterplaybetweenannotatorandtargetattributes.
thehatemaybelessacute.RQ1bbroadenstheanalysisby
Demographic alignment of LLMs. LLM prompting
examiningwhethercertaingroupssystematicallylabelcon-
has been used to experiment with persona-based and role-
tent differently than others, in relation to the targets of the
playing models, to test the alignment of LLM predictions
hate. This analysis reveals possible disparities in how hate
with human opinions (Tseng et al. 2024). Studies involved
speechisperceivedacrossdiversesocio-demographicback-
replicating famous cognitive and social experiments (Aher,
grounds.
Arriaga, and Kalai 2023; Srivastava et al. 2023), as well
RQ2a How do persona-based LLMs with a specific socio- as assessing agreement and personalization capabilities.
demographic attribute differ in their labeling of hate Among the existing works, Beck et al. (2024) and Hu and
speechcomparedtoLLMswithoutthatattribute? Collier (2024) measured the impact of socio-demographic
RQ2b Arepersona-basedLLMbiasesthesameasthosere- prompting on model performance, showing that it can en-
portedbythehumanannotatorstheyimpersonate? hance zero-shot learning in subjective tasks. Additional
studies explored the personalities (e.g., Big5, MBTI) ex-
We move our focus from human biases to LLMs’. RQ2a
hibited by LLMs when prompted to emulate certain hu-
mirrorstheanalysisinRQ1bbyinvestigatingthebiasesex-
mansocio-psychologicaltraits(Rao,Leung,andMiao2023;
hibitedbypersona-basedLLMs.Finally,RQ2bdetermines
LaCava,Costa,andTagarelli2024).Resultsshowedmarked
whetherpersona-basedLLMsreplicatethebiasesobserved
differences between models and a moderate predisposition
in human annotators with matching attributes, thereby as-
to personalization and psychological prompting. However,
sessing the extent to which LLMs reflect or diverge from
Santurkar et al. (2023) measured the alignment between
humanpatternsofbiasinhatespeechannotation.
human and LLM opinions across a wide array of demo-
graphicgroupsandtopics,findingsubstantialmisalignment.
RelatedWork
The misalignment persisted even after explicitly steering
Annotator diversity and data quality. Numerous studies the LLMs towards particular groups. Similarly, Lee, An,
have examined how the characteristics of human annota- andThorne(2023)investigatedthealignmentofinstruction-attribute values annotators targets
age children,teenagers,youngadults,middleaged,seniors,other 7,907 3,522
disability cognitive,hearingimpaired,neurological,physical,unspecific,visuallyimpaired,other – 4,719
education somehighschool,highschool,somecollege,collegeaa,collegeba,master,PhD,professionaldegree 7,911 –
ideology neutral,slightlyliberal,extremelyliberal,slightlyconservative,conservative,extremelyconservative,noopinion 7,910 –
gender men,nonbinary,transgendermen,transgenderunspecified,transgenderwomen,women,other 7,950 51,325
income <10K,10K–50K,50K–100K,100K–200K,>200K 7,906 –
origin immigrant,migrantworker,specificcountry,undocumented,other – 32,503
race asian,black,latinx,middleeastern,nativeamerican,pacificislander,white,other 8,589 67,625
religion atheist,buddhist,christian,hindu,jewish,mormon,muslim,other,nothing 8,053 32,663
sexuality bisexual,gay,lesbian,straight,other 7,886 36,521
Table 1: Dataset overview. Annotators and hate targets are characterized by ten socio-demographic attributes, each assuming
multiplevalues.AppendixTableT1reportsthedetaileddistributionofannotatorsandtargetsacrossattributesandvalues.
tuned LLMs with the disagreement among human annota- where annotators were asked to classify each post as con-
tors, finding that models fail to capture human disagree- taininghate(hatelabel),notcontaininghate(non-hate),
ments. As briefly summarized, there are many contrasting orwhethertheywereuncertain(maybe).Onaverage,each
results on the alignment of persona-based LLMs with hu- annotator labeled ∼17 posts (σ = 3.8) and each post has
man opinions. This gap in understanding underscores the beenlabeledby∼3.5differentannotators(σ =27.0).
needforfurtherresearchtoclarifytheextentandconditions AsshowninTable1,annotatorsaredescribedbyeightat-
underwhichLLMscanmirrorhumanperspectives. tributeswhilehatetargetsbysevenattributes.Fiveattributes
Bias in persona-based LLMs. Multiple recent studies overlap between annotators and targets, while the remain-
examining social bias in LLMs indicated that the models ingareonlyavailableeitherforannotators(education,ideol-
tend to lean towards left-wing views on issues such as im- ogy, income) or targets (disability, origin). The distribution
migration, gun rights, and the political compass test (Perez of annotators across the available attributes is almost uni-
etal.2023;Santurkaretal.2023).Additionally,Argyleetal. form and close to the total number of annotators, meaning
(2023)showedthatGPT-3canemulatecertainhumansub- thateachannotatorprovidedinformationforthemajorityof
populations,revealingdemographically-correlatedalgorith- attributes. Appendix Table T1 however shows that certain
mic biases; while Simmons (2023) found political moral subgroups of annotators are more represented than others.
biases. It was also shown that persona assignment impacts Instead, the distribution of hate targets is very skewed, re-
LLMreasoningandrevealssocio-demographicbiases,with flecting the tendency foronline hate to be directed towards
de-biasing efforts proving largely ineffective (Gupta et al. specificsocio-demographiccharacteristics,suchasraceand
2024).Finallyandmoreakintoourwork,Dasetal.(2024) gender.Nonetheless,theavailabilityofaconsiderablenum-
exploredgender,race,religion,anddisabilitybiasesinhate berofannotationsalsofortheleastfrequenthatetargetsen-
speech annotations made by GPT-3.5 and GPT-4. Unlike ablesstatisticallysignificantanalysesacrossallsubgroups.
current general research on biases in persona-based LLMs, The dataset follows the FAIR principles. It is available
wespecificallyfocusonahatespeechannotationtask,and on a prominent cloud storage service,1 making it findable
weassesshowLLMbiasescomparetohumanbiasesinthis and accessible. It is interoperable in that it is released in a
task,forawidearrayofsocio-demographicattributes. machine-readable format, and reusable due to the informa-
tioncontainedintheoriginalpaper(Sachdevaetal.2022a).
Dataset
Methodology
The MEASURING HATE SPEECH corpus (Sachdeva et al.
2022a) is an extensive and publicly available dataset con- Annotationbias
taining 135,556 hate speech labels assigned by 8,472 hu- Definition.Manydefinitionsofbiashavebeenproposedto
man annotators to 39,565 distinct social media posts. The date(Gargetal.2023).Informedbythese,wedefine“anno-
postswerecollectedbetweenMarchandAugust2019from tationbias”asthesystematicdeviationinthelabelingofhate
threemajoronlineplatforms:Twitter,Reddit,andYouTube. speechthatarisesduetothesocio-demographicattributesof
In addition to hate labels, the dataset includes rich socio- theannotatororthepersona-basedLLM.Thisbiasmanifests
demographicinformationaboutboththeannotatorsandthe whentheannotationsofcontentareinfluencednotsolelyby
targetsofthehatefulmessages.Thissocio-demographicin- the content itself, but also by the attributes of the annota-
formationspanstenattributes,eachwithmultiplevalues,re- tor, persona-based LLM, or the target, resulting in over- or
sulting in a fine-grained and nuanced resource for in-depth underestimationsofhatespeech.
studies of how socio-demographic characteristics influence Operationalization.Ourdefinitionofbiasentailsthesys-
perceptions of hate. Table 1 provides an overview of the tematic comparison between the labels assigned by human
dataset,whileAppendixTableT1providesdetailedinforma-
tion for each attribute and value. The labels were obtained 1https://huggingface.co/datasets/ucberkeley-dlab/measuring-
via a data annotation task on Amazon Mechanical Turk, hate-speech(accessed:15/09/2024)A(t̸=v) callysignificantcomparisonsserveasafoundationaltoolto
non-hate
m
aybe
hate
i td he en mti ofy stb ri ea ls ee vs ai nn td anat da s( eW veic reh ie nt sa tal. n2 ce0 s21 o) f. bT iah se ,n w,t eo ph ri og ph oli sg eh at
setofquantitativeindicatorstomeasurethestrength,direc-
non-hate NN NM NH agreement tion,andprevalenceoftheidentifiedbiases:
Biasintensity(I). I is obtained as the difference between
A(t=v) thesumofvaluesinthelowertriangleandtheuppertri-
maybe MN MM MH
overestimates angleofD,dividedbythesumofboth:
A(t=v) (cid:80) D −(cid:80) D
hate HN HM HH underestimates I = i<j ij i>j ij
(cid:80) (cid:80)
D + D
i<j ij i>j ij
Figure 2: Confusion matrix D used to compare labels as- I ∈[−1,+1],whereI ≈−1indicatesastrongtendency
signed by annotators with a given socio-demographic at- tounderestimatehatebyA(t = v),whileI ≈ +1sug-
tributeA(t=v)versusthosewithoutitA(t̸=v). gestsastrongtendencytooverestimate.I ≈ 0indicates
minimal bias. Thus, I conveys both the polarity of the
bias (i.e., over- or underestimation) and its strength, of-
feringaclearandconcisemeasureofbiasintensity.
or LLM annotators with certain attributes, with the labels
assigned by annotators without such attributes. We opera- Biasprevalence(P). P quantifiestheoveralloccurrenceof
tionalize the comparison by extending the method of Wich biaswithintheannotationprocess,irrespectiveofitsin-
et al. (2021) as follows. Let A = {a ,...,a } be the set tensity. It is calculated as the sum of the values in both
1 n
of annotators and G = {g ,...,g } be the set of targets. the lower and upper triangles of D, divided by the sum
1 m
We first fix one socio-demographic attribute t and a spe- ofallvalues:
cificvalueofthatattributet = v (e.g.,ideology=liberal). (cid:80) D +(cid:80) D
Next,weselectallpostslabeledbyallannotatorsA(t=v) P = i<j (cid:80)ij i>j ij
D
with the fixed attribute and value. Depending on the RQ to i,j ij
answer, we further filter the posts by only selecting those P ∈ [0,1], where P ≈ 0 suggests that the bias mea-
whosetargetG(t′ = v′)featureanother,possiblydifferent, suredbyIoccursinonlyaminorityofcases.Conversely,
attribute and value t′ = v′. Finally, we compare the labels largervaluesofP indicatethatthebiasoccursinamore
assigned by the annotators A(t = v) to those of all other substantial portion of the annotations, reflecting a non-
annotatorsA(t ̸= v)onthesamesetofselectedposts.The negligible prevalence. Unlike I, which measures the di-
comparisonisbasedontheconfusionmatrixDinFigure2. rection and strength of bias, P focuses on the extent to
Rows in D correspond to the labels assigned by the anno- whichthatbiasisprevalentacrosstheentiresetoflabels.
tators A(t = v), while columns correspond to the labels
Agreement(κ). We complement the previous indicators
assigned by A(t ̸= v). The diagonal represents those in-
withastandardmeasureofannotatoragreement.Cohen’s
stancesthatarelabeledequallybythetwogroups.Thelower
κgaugestheextentofagreementbetweentwogroupsof
trianglerepresentsinstanceswheretheannotatorsA(t=v)
annotators,correctingfortheagreementthatcouldoccur
overestimatethehatewithrespecttoA(t̸=v).Conversely,
by chance. Here, it contextualizes findings from I and
theuppertrianglerepresentsinstanceswhereA(t = v)un-
P,highlightingwhethertheobservedbiasesoccurinthe
derestimate hate. We repeat the analysis ∀ t,v. Algorithm
contextofhighorlowagreementbetweenannotators.
A1intheAppendixprovidesthepseudo-codeimplementa-
tionoftheprocessusedtocomputetheconfusionmatrixD. Largelanguagemodels
Statistical significance. Our comprehensive approach to
Model selection. We prioritize open-source models over
investigating bias across a large set of socio-demographic
commercialoptions(e.g.,GPT-4)whenselectingtheLLMs
attributes results in a substantial number of comparisons.
for our experiments. First, open-source models are freely
However,asreportedinAppendixTableT1,certainminor-
available, which enhances transparency and reproducibil-
itygroupsofannotatorsandtargetsareunderrepresentedin
ity of our research. Additionally, many open-source mod-
thedata.Consequently,somecomparisonsarebasedonlim-
els have comparable accuracy to that of leading commer-
ited data, and the observed labeling differences may lack
cial LLMs.2 Open-source LLMs also require less compu-
statisticalsignificance.Weaddressthisissuebyconducting
tational resources, broadening accessibility and easing im-
Mann-Whitney tests to assess the statistical significance of
plementation.Thesechoicesmakeourresultsgeneralizable
the differences between the distributions of labels assigned
andbroadlyapplicable.Drivenbytheseconsiderations,we
by any two groups of annotators being compared. In the
based our model selection on recent results on the perfor-
remainder, we only report results about those comparisons
mance of open-source LLMs, favoring high-performance
wherethelabelingdifferencesarestatisticallysignificantat
and widely-used models (La Cava, Costa, and Tagarelli
the 0.05 level. Due to data imbalance, we were unable to
2024). Our selection includes Llama-3-8B-Instruct (8B),3
obtain any significant result for a small set of particularly
underrepresentedgroups,highlightedingrayinTableT1. 2https://lmarena.ai/?leaderboard(accessed:15/09/2024)
Measures. The confusion matrices derived from statisti- 3https://shorturl.at/0xT0b(accessed:15/09/2024)
)v=t(APhi-3-mini-4k-instruct(3.8B),4 SOLAR-10.7B-Instruct-v1.0
(10B),5andStarling-LM-7B-alpha(7B).6
Role-playing. We adopt a traditional prompt engineer-
ingapproachtoexperimentwithrole-playingLLMs(Tseng
et al. 2024). The prompt includes detailed instructions on
howtocarryoutthehatespeechannotationtask.Addition- gender:
transgender
ally,itdynamicallyspecifiesasetofsocio-demographicat- men
tributes,allowingLLMstoimpersonateannotatorswithcer- age:
tain given attributes. Initially, we experimented with 100 seniors
randomlyselectedpoststomanuallyrefineandevaluatevar- trag ne sn gd ee nr d: er r mel oig rmio on n: gender:
ious prompts. Through iterative prompting and evaluation, unspecified transgender
women
weassessedthequalityoftheLLMs’output.Thefinalver-
sion of the prompt is reported in Appendix Table T2, to-
getherwithsomealternativeversionsthatweevaluated.
Model evaluation. We assess the suitability of persona-
Figure 3: Sensitivity to in-group hate. Points correspond to
based LLMs to act as hate speech annotators based on
thestatisticallysignificantbiasesthatwefound.Mostbiases
two criteria: (i) the model’s ability to accurately perform
havemildintensityandlowprevalence,withafewremark-
hate speech detection, and (ii) its sensitivity to personal-
ableexceptionshighlightedintheplot.
ization, meaning the extent to which the LLM’s output
changeswhenpersonalizedwithspecificsocio-demographic
attributes. To evaluate the first criterion, we used the base
versions (i.e., neither personalized nor fine-tuned) of each tified strong biases (I > ±0.5) in some instances. As ex-
model to label the posts in our dataset. We then compared pected,thestrongestbiasesoccurwhenthereislittleagree-
theirperformanceagainstthatofhumanannotatorstoiden- ment (κ < 0.4) between the annotator groups. Regarding
tify which base models excel at hate speech detection. For prevalence P, some biases are overall marginal since they
thesecondcriterion,weassessedsensitivitytopersonaliza- occur very infrequently. There are situations, however, for
tion by focusing on the 11,810 posts in our dataset (30%) which we foundbiases that are bothintense and with large
where human annotators disagreed about the assigned la-
prevalence(P > 0.4),suchasthoserelatedtotransgender
bel.WepersonalizedeachLLMwithsocio-demographicat- annotatorsandtargets.Othercaseswithconsiderablebiases
tributes and tasked them with labeling this subset of posts.
arethoseannotatorsandtargetsintheseniorsagegroupand
We then measured the agreement κ between the personal- ofMormonreligion.
ized outputs and those of the base, non-personalized ver- Insights.Theseresultsindicatethatwhilein-grouphyper-
sionsofeachmodel.ThisanalysisrevealswhichLLMsare sensitivityisnotuniversal,thereisamildtendencytoover-
mostaffectedbythepersonalizationprocess—indicatedby estimatein-grouphate.Moreover,significantandsometimes
lowerκ—thusidentifyingthemodelforwhichpersonaliza- substantialbiasesarepresentinspecificsocio-demographic
tionismosteffective. contexts.
RQ1b:Humanbiasesinannotator-targetdynamics
AnalysesandResults
Analysis.Webroadenouranalysisofannotationbiasinhate
RQ1a:Sensitivitytoin-grouphate
speechbyexaminingallcombinationsofannotatorandtar-
Analysis. We assess whether annotators are more sensible get attributes. For each iteration, we fix attribute t = v for
tohatedirectedattheirownin-group.Weiterativelyfixan annotators and attribute t′ = v′ for hate targets. We then
attributeandvalue.Then,weperformtheanalysisbyselect- compare the labels assigned by annotators with t = v to
ingalllabelsassignedbyannotatorswiththefixedattribute postsdirectedattargetswitht′ =v′,againstthelabelspro-
andvaluetopoststargetedatthesamegroup,andbycom- videdbyannotatorswitht ̸= v.Thisapproachallowsusto
paringtheseannotationstothosegiventothesamepostsby systematically assess how the interplay between annotator
annotatorswithoutthefixedattributeandvalue. andtargetattributesinfluencehatespeechlabeling.
Results. We found statistically significant labeling dif- Results. Figure 4 shows a heatmap of bias intensity (I)
ferences for 17 of 29 (59%) socio-demographic attributes. andprevalence(P)fortheexploredcombinationsofannota-
For each of these cases, we measured bias intensity (I), torandtargetattributes.Intensityiscolor-codedwhilepreva-
prevalence (P), and annotator agreement (κ). Results are lenceisproportionaltocellarea.Thefigurecaptionprovides
showninFigure3.Regardingthe17significantdifferences, athoroughdescriptionoftheheatmapanditsinterpretation.
11 (65%) were overestimations (I > 0) and 6 were un- Figure 4 provides nuanced results on hate speech anno-
derestimations. Bias intensity was generally mild or mod- tation biases. When observed row-wise, it allows identify-
erate, with I ≤ ±0.25 in most cases. However, we iden- ing groups of annotators with the tendency to systemati-
cally over- or underestimate hate. As a notable example,
4https://shorturl.at/tOs9i(accessed:15/09/2024) our analysis reveals a marked age bias: younger annota-
5https://shorturl.at/udLQZ(accessed:15/09/2024) tors (teenagers and young adults) generally tend to under-
6https://shorturl.at/kqqLq(accessed:15/09/2024) estimate hate, while older ones (middle aged and seniors)target
age gender race religion sexuality disability origin
Figure4:Intensity(I)andprevalence(P)ofhumanbiasesinhatespeechannotations,forallpossiblecombinationsofannota-
tor(yaxis)andtarget(xaxis)socio-demographicattributes.Cellcolorcorrespondstointensitywhilecellareaisproportionalto
prevalence.Statisticalsignificanceisassessedatthe0.05levelwithaMann-Whitneytest.Missingcellsdenotenon-significant
results.Red-coloredcellsmarkstatisticallysignificanthateoverestimations,whileblue-coloredonesdenotesignificantunder-
estimations.Themagenta-coloredtop-leftsquarecorrespondstothesocio-demographicattributesthatareavailableforboththe
annotatorsandtargets(age,gender,race,religion,sexuality),asthoroughlyreportedinAppendixTableT1.Thedasheddiago-
nalcorrespondstoin-groupannotations(i.e.,sameannotatorandtargetattribute).Attributesoutsideofthetop-leftsquareare
onlyavailableeitherforannotators(education,ideology,income)ortargets(disability,origin).Annotatorandtargetattributes
forwhichwedidnotobtainanysignificantresult(e.g.,ideology:extremelyliberal)arenotshown.Whenobservedrow-wise,
the heatmap allows identifying groups of annotators with the tendency to systematically over- or underestimate hate. When
observed column-wise, it allows identifying hate targets that are overall flagged more or less frequently. Single cells show
biasintensityandprevalenceforspecificcombinationsofannotator-targetattributes.Toeasetheexplorationofourresults,an
interactiveversionofthisvisualizationisavailableathttps://webvis.github.io/annotator-target-hate-speech-biases/.
rotatonna
ega
redneg
ecar
noigiler
ytilauxes
noitacude
ygoloedi
emocni
bias
prevalence
(P)
bias
intensity
(I)1.0 1.0
A B
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
-1.0 -0.6 -0.20.0 0.2 0.6 1.0 -1.0 -0.6 -0.20.0 0.2 0.6 1.0
bias intensity (I) bias intensity (I)
Figure 5: Distribution of statistically significant human biases in the intensity-agreement (panel A) and intensity-prevalence
(panelB)space.RedlinesinpanelAdelimitdifferentregionsofthespace.PanelAshowstheexistenceofbiaseswithstrong
intensityandlowagreement,whilepanelBhighlightssomebiasescharacterizedbybothlargeprevalenceandintensity.Some
examplesarehighlightedwithcoloredsymbolsandfurtherdescribedinTable2.
bias fectinggroupsofannotatorsortargets.Nowwedigdeeper
annotator target # I P κ by considering the interplay between annotator and target
■ g:transgenderwomen s:bisexual 511 0.91 0.18 0.596 attributes. Figure 5 shows scatterplots where each statis-
⋆ g:transgenderwomen s:gay 4,880 0.92 0.16 0.661 tically significant case previously identified is mapped in
♦ g:transgenderwomen s:lesbian 639 0.93 0.19 0.598 the I-κ and I-P spaces. Red lines in Figure 5A delimit
■ r:atheist d:hearingimpaired 183 -0.62 0.34 0.143 different regions of the I-κ space. The majority of points
⋆ r:atheist d:visuallyimpaired 209 -0.81 0.35 0.015 lay in the central region, corresponding to cases of moder-
♦ a:youngadults d:hearingimpaired 249 -0.78 0.22 0.190
ate to strong agreement between the groups of annotators
▲ a:youngadults d:visuallyimpaired 240 -1.00 0.20 0.000
(0.35 ≤ κ ≤ 0.75) and to low bias intensity (I ≤ ±0.35).
■ g:nonbinary s:straight 195 -0.75 0.41 0.318
⋆ g:transgenderunspecified s:straight 141 -0.74 0.55 0.238 Anotherdenseareaoftheplotcorrespondstolowagreement
(κ ≤ 0.35)andlowbiasintensity.Thesepointsindicatesit-
g:gender;s:sexuality;r:religion;a:age;d:disability
uationswherethecomparedgroupsofannotatorsfrequently
Table 2: Examples of human biases. For each example we disagreed on labels, yet these disagreements balanced out
reportthenumberofconsideredlabels(#),andthevaluesof withoutresultinginamarkedtrendofover-orunderestima-
biasintensity(I),prevalence(P),andagreement(κ).Allre- tion.Thisfindingunderscoresthedistinctionbetweenagree-
portedexamplesarestatisticallysignificantatthe0.01level, mentandbias,demonstratingthatlowannotatoragreement
asconfirmedbyaMann-Whitneytest.Coloredsymbolsmap doesnotnecessarilyleadtobias.Contrarily,thebottomleft
examplesintothespacesofFigure5. and right corners of Figure 5A correspond to low agree-
ment and strong bias intensity. Table 2 highlights some of
theobservedbiasesthatarisefrominterestingcombinations
ofannotatorandtargetattributes.Forexample,transgender
overestimate.Thisbiasmightbeexplainedbythefamiliar-
womenshowedheightenedsensitivitytohatedirectedatbi-
itythatyoungerannotatorshavewiththetoxiclanguageof
sexuals,gays,andlesbians.Conversely,atheistsandyoung
socialmedia,whichmightdesensitisethemtosuchexpres-
adultsunderestimatedhatetowardshearingandvisuallyim-
sions, contrarily to older annotators who might be less fa-
paired individuals, while transgender unspecified and non-
miliar with it (Sang and Stanton 2022). Other groups who
binarypeopleunderestimatedhatetowardsindividualswith
overestimate hate speech are those with a high school de-
straight sexual orientation. Finally, the top-right corner of
gree; religious people; transgender women; and those who
Figure 5A corresponds to strong agreement (κ > 0.75) but
identifyasgay.Conversely,amongthosewhounderestimate
large bias intensity (I > 0.35). These cases are character-
themostareatheists;thosewithaPhDdegree;men,trans-
izedbylowbiasprevalence(P < 0.42),whichisshownin
gendermen,andnon-binarypeople;andthosewhoidentify
Figure 5B, suggesting that the observed biases occur infre-
as lesbian. These results indicate possible education, reli-
quently.Thestrongagreementunderscoresthatthesebiases
gion,gender,andsexualorientationbiases.Whenobserved
arenotwidespread,makingthemlessrelevantoverall.
column-wise,Figure4allowsidentifyingthetargetsofhate
that are overall flagged more or less frequently. For exam- Insights. Our results reveal multiple biases of the anno-
ple, human annotators show heightened sensitivity to of- tators and the targets independently, as well as others that
fenses based on disability and sexual orientation. Instead, arisefromspecificcombinationsofannotatorandtargetat-
hatedirectedatcertainminorities,suchasmigrantworkers tributes.Thesefindingsshowthatdifferentannotatorgroups
andtransgenderindividuals,tendstobeunderestimated. react distinctly depending on the socio-demographic char-
TheanalysisofFigure4revealsgeneralbiaseseitheraf- acteristics of the hate target. While some of the identified
)k(
tnemeerga ytisned
)P(
ecnelaverp
saib
ytisnedhatespeechdetection personalization
1 1
model macroF1↑ weightedF1↑ accuracy↑ agreement(κ)↓
0.8
Llama3 0.40 0.56 0.59 0.88 0.8 0.6
Phi3 0.43 0.57 0.57 0.79 0.4
Solar 0.53 0.69 0.66 0.72 0.6 0.2
Starling 0.52 0.73 0.75 0.91
0.0
-1.0 -0.6 -0.20.0 0.2 0.6 1.0
Table3:LLMsevaluationresultsintermsofthebasemod- 0.4 bias intensity (I)
els’ capacity to detect hateful posts and their sensitivity to
personalization. Best results in each evaluation metric are 0.2
showninbold,second-bestsareunderlined.
0.0
-1.0 -0.6 -0.20.0 0.2 0.6 1.0
bias intensity (I)
biases are mild, others are pronounced both in terms of in-
tensityandprevalence,highlightingthatboththeindividual Figure6:DistributionofstatisticallysignificantLLMbiases
attributesofannotatorsandtargets,andtheirinteraction,can in the intensity-agreement (inset) and intensity-prevalence
significantlyinfluencehatespeechlabelingdecisions. (outer figure) space. Differently from humans, persona-
based LLMs exhibit strong agreement. No biases exhibit
RQ2a:LLMbiasesinannotator-targetdynamics both high intensity and prevalence, but some display high
intensitywithnotableprevalence.
Bestmodelselection.Weinvestigatethebiasesofpersona-
basedLLMs,similarlytowhatwedidinRQ1bforhuman
annotators.AlthoughweexperimentedwiththefourLLMs
eachsignificantcombinationintheI-κspace(inset)andI-
inTable3,duetospaceconstraintsweonlypresentresults
P space (outer figure). Unlike human annotators, persona-
of the most effective and relevant one. Table 3 reports the
based LLMs largely exhibited substantial agreement (κ ≥
performance of the base LLMs in a hate speech detection
0.6) in their label assignments. In terms of bias intensity,
task, and their sensitivity to personalization. The former is
we identified a range from mild to severe, with both no-
measured with standard machine learning evaluation met-
tableover-andunderestimations.However,thehighlevelof
rics, while greater sensitivity to personalization is reflected
agreement resulted in relatively low bias prevalence, indi-
byloweragreement(i.e.,morediversification)withthenon-
cating that the majority of detected biases were infrequent.
personalized version of the same model. In the remainder
Nevertheless, we identified several biases characterized by
wepresentresultsfromSolar,giventhatitachievedoverall
bothhighintensityandsubstantialprevalence.Forinstance,
second-best performance in hate speech detection and has
LLMsimpersonatingannotatorswhoidentifiedasChristian
thebestsensitivitytopersonalization.
consistentlyunderreportedhatedirectedatbisexualindivid-
Analysis.WeinstructSolartoimpersonatehumananno-
uals(I =−0.84,P =0.27,p<0.01).Similarly,LLMsim-
tators with specific socio-demographic attributes. For each
personating annotators with straight sexual orientation un-
annotationinthedataset,wepersonalizetheLLMwiththe
derreported hate targeted at men (I = −0.91, P = 0.26,
exactsocio-demographicattributesofthecorrespondinghu-
p < 0.01), while those impersonating bisexual (I = 0.91,
manannotatorandaskittolabelthesamepostthatthehu-
P = 0.27, p < 0.01) and gay (I = 0.89, P = 0.23,
manhadlabeled.Werepeatthisprocessforall136Kanno-
p<0.01)annotatorsoverreportedit.
tations,creatinganLLM-labeledcopyofthehuman-labeled
dataset. We then iteratively fix an attribute t = v for the Robustness. We experimented with alternative prompts
LLM and another attribute t′ = v′ for the hate targets. to assess whether prompt variations would yield better
Finally, we compare the labels given by the persona-based LLM personalization. Following Beck et al. (2024) we
LLM with t = v to posts directed at targets with t′ = v′, testedadditionalpromptsbyreducingthenumberofsocio-
against the labels given by the persona-based LLM with- demographicattributespassedtothemodel,andbysimpli-
out t = v, thus investigating the impact of LLM socio- fyingtheinstructionsfortheannotationtask.Thesealterna-
demographicattributesonlabelingbehavior. tive prompts are reported in Appendix Tables T3 and T4.
Results.Weobservedstatisticallysignificantlabelingdif- However, Solar’s sensitivity to personalization decreased
ferences in 122 out of 1,950 (6%) combinations of annota- with both alternative prompts, respectively with κ = 0.81
tor and target attributes when using persona-based LLMs. andκ=0.82,insteadofκ=0.72(lowerisbetter).
In comparison, human-generated annotations showed sig- Insights.Ourfindingssuggestthatpersona-basedLLMs
nificant differences in 1,200 out of 1,950 (62%) combina- exhibit fewer labeling differences and, consequently, fewer
tions. The relatively low number of significant results for biases (according to our definition) than human annota-
persona-basedLLMs,especiallycomparedtohumananno- tors.Thereducedvariabilityinthelabelsassignedbythese
tators,suggeststhatthepersonalizationhadalimitedimpact LLMs, regardless of the socio-demographic attributes they
on the hate speech labels assigned by the models and that werepromptedtoemulate,indicatesthatthemodel’sperson-
thesocio-demographicattributesembeddedintheLLMsdid alizationmaynotfullycapturethenuancedbiasespresentin
notleadtosubstantiallabelingdifferences.Figure6presents humanbehavior.Despitethis,weidentifiedseveralmarked
)P(
ecnelaverp
saib
)k(
tnemeerga
ytisned1.0 1.0 1.0
A B C
0.6 0.8 0.8
0.2 0.6 0.6
r: -0.023 r: 0.388 r: 0.182
-0.2 0.4 0.4
-0.6 0.2 0.2
-1.0 0.0 0.0
-1.0 -0.6 -0.2 0.2 0.6 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
human bias intensity (I) human bias prevalence (P) human agreement (k)
Figure7:ComparisonofhumanandLLMbiasesacrossbiasintensity(panelA),prevalence(panelB),andagreement(panelC).
Eachpointrepresentsacombinationofannotatorandtargetattributeswherebothhumansandpersona-basedLLMsexhibited
astatisticallysignificantbias.RedlinesandannotationsreportPearsoncorrelations.Theintensity(i.e.,strengthandpolarity)
ofLLMbiasesisuncorrelatedwiththatofhumans.However,wemeasuredamoderatepositivecorrelationinprevalence.
biasesagainstminoritygroups,whichcouldnegativelyim- influencingagreementamonghumansarenotfullycaptured
pacttheperformanceofautomatichatespeechdetection. by LLMs. As already observed in Figure 6, this is mainly
due to the large agreement exhibited by the persona-based
RQ2b:ComparisonofhumanandLLMbiases LLMs, which is indicative of strong labeling consistency
Analysis.Weassesstheextenttowhichthebiasesexhibited evenwhenLLMsarepersonalizedwithdifferentattributes.
by persona-based LLMs are similar to those of human an- Robustness. Results obtained with Llama3, Phi3, and
notators.Concretely,wefocusonthestatisticallysignificant Starling qualitatively confirm those of Solar, with correla-
cases in which both humans and persona-based LLMs ex- tionsbetweenhumanandLLMbiasintensityrangingfrom
hibitabiasforthesamecombinationofannotatorandtarget r = 0.108 (Llama3) to r = −0.137 (Phi3). These addi-
attributes. For each of these cases, we compare bias inten- tional figures show invariance of our results to the specific
sity, bias prevalence, and agreement. This approach allows LLM used and reinforce the conclusion that persona-based
us to measure the correlation between the considered bias LLMshavelimitedcapacitytoreproducehumanbiases.
indicatorsofhumansandLLMs,providinginsightsintohow Insights.Thepersona-basedLLMsthatweusedwere,in
closelypersona-basedLLMsreplicatehumanbiases. general,unabletoreplicatehumanbiases.Thiswasparticu-
Results.Wefound86statisticallysignificant(p < 0.01) larlyevidentwithrespecttothestrengthandpolarityofthe
casesinwhichhumanannotatorsandpersona-basedLLMs biases. However, biases exhibited by persona-based LLMs
exhibitedabiasforthesamecombinationofannotatorand occurred with a moderately similar frequency than the cor-
target attributes. As shown in Figure 7A, the Pearson cor- respondinghumanbiases,highlightingsomesimilaritiesbe-
relation r = −0.023 between the intensity of human and tweenthetwo.Theseresultssuggestthat,althoughpersona-
LLM biases suggests the lack of a linear relationship. This based LLMs can mimic some aspects of human bias, they
near-zero correlation indicates that, overall, the biases ex- donotcloselyreplicatethenuancedbiasespresentinhuman
hibitedbypersona-basedLLMsdonotcorrespondtothose hatespeechannotations.
observed in human annotators. A notable exception is the
marked tendency that men annotators—both human and DiscussionandConclusions
LLM-impersonated—exhibit to overestimate hate towards
gay individuals, with bias intensity I = 0.74 and I = Ourresultsrevealthathumanannotatorsexhibitamildten-
0.89forhumansandLLMsrespectively.Instead,Figure7B dencytooverestimatein-grouphate(RQ1a),andthatdiffer-
showsthatthecorrelationbetweentheprevalenceofhuman ent socio-demographic attributes among annotators lead to
and LLM biases is moderately positive, with r = 0.388. diverselabelingpatterns(RQ1b).Furthermore,LLMsshow
This suggests that the biases that are (in)frequent among little sensitivity to personalization via prompting, which
human annotators tend to be relatively (in)frequent among likelyaccountsforthefewbiasesobserved,thoughtheystill
persona-basedLLMannotationsaswell.Finally,thecorrela- display significant biases against certain minority groups
tionr =0.182betweentheagreementmeasuredforhuman (RQ2a). However, persona-based LLMs do not fully repli-
andLLMbiases,showninFigure7C,indicatesaweakpos- cate human biases, demonstrating minimal alignment with
itiverelationship.Thisreflectsaslighttendencyforthelevel humanannotators(RQ2b).
ofagreementamonghumanannotators(withandwithouta Bias intensity and prevalence. Our methodology in-
certain attribute) to correspond with the level of agreement troduces two bias indicators—intensity (I) and prevalence
among LLMs (with and without the same attribute). How- (P)—thatmeasuretheseverityandfrequencyofannotation
ever,theweakcorrelationsuggeststhatwhilethereissome biases. While P indicates the frequency of disagreement,
degreeofsimilarityinhowhumansandLLMsagreewithin I measures the extent of imbalance when such disagree-
andacrosscertainsocio-demographicattributes,thefactors ment occurs. Clearly, the most concerning biases are those
)I(
ytisnetni
saib
MLL
)P(
ecnelaverp
saib
MLL
)k(
tnemeerga
MLLwith both high intensity and prevalence. However, even bi- ing and testing datasets. Moreover, LLMs can be used
aseswithhighintensityandsmallbutnon-negligiblepreva- for automatic data annotation, data augmentation, or di-
lenceareattentionworthy,astheyrepresentrarebuthighly rectly for hate speech detection. Thus, their biases may
skewedannotationsthatmightwarrantfurtherinvestigation. alsocausemultiplenegativedownstreamconsequences.Our
Human biases. We systematically applied our method- work enhances the understanding of these biases and in-
ology to identify and characterize human biases in a hate forms the construction, curation, and rebalancing of hate
speechannotationtask,uncoveringalargenumberofbiases speechdatasets,aswellasthedevelopmentofnewmethods
with varying intensity and prevalence. Therefore our study fortrainingfairhatespeechdetectors(Gargetal.2023).
favorsbreadthoverdepth,anditisbeyondourscopetoin- Generalizability.Whileourpresentstudyfocusesonbias
vestigate the identified biases in detail. However, our work in hate speech annotations, our approach is general and
provides a foundation for domain experts to conduct in- broadly applicable. The methods we employed to uncover
depthstudiesonspecificbiases,exploringtheiroriginsand and characterize human biases can be used to detect bi-
potential solutions. To foster this line of work, we publicly asesinawiderangeofannotateddatasets.Additionally,our
releasethecompletelistofstatisticallysignificantbiasesthat methodology for personalizing LLMs and comparing their
wefound,andtheindicatorsthatcharacterizethem.7 biasestohumanonescanbeeasilycarriedovertoothertasks
LLMbiases.Wealsoinvestigatedthebiasesexhibitedby beyondhatespeechdetection.
persona-based LLMs, and compared them to human ones. Limitations and Future work. Some limitations must
Current research in this area is facing a significant tension: be acknowledged. Although extensive and rich, the social
ononehand,westriveforunbiasedLLMstoavoidpropagat- media data was collected from platforms with a marked
inghumanprejudices(Perezetal.2023).Ontheotherhand, US user-base and the annotations were obtained from US
there is a compelling case for developing LLMs that faith- crowdsourcing contributors. Hence the data may primarily
fullyreproducehumanbehaviors—includingbiases—asthis reflectUSlanguage,culturalnorms,andperspectives.More-
would open new frontiers in social simulations (To¨rnberg over, the data is skewed in that some socio-demographic
etal.2023).Ourresultsinthisregardareconcerning.First, groups are over- or underrepresented and the data collec-
wefoundthatpersona-basedLLMsobtainedviaprompting tionperiodwasrelativelyshort.Thesefactorsmaylimitthe
do exhibit biases. More troubling, however, is the finding generalizability of the biases that we detected to other cul-
thatthereisverylittlealignmentbetweenthebiasesexhib- turalcontexts.Methodologically,ouranalysisofannotation
ited by LLMs and those exhibited by humans. This sug- bias, while informative, does not capture all dimensions of
gests that current persona-based LLMs may fail to accu- bias,suchascontextualfactorsorannotators’subjectiveex-
ratelyreflectthecomplexandnuancedbiasespresentinhu- periences. Moreover, although we experimented with vari-
man behavior. This raises concerns about the reliability of ouspromptsandLLMmodels,thespecificpromptingstrat-
using prompting techniques to create persona-based LLMs egyandLLMversionsusedmayhaveinfluencedtheresults.
for social simulations and underscores the need for further Theselimitationshighlighttheneedformorerepresentative
researchintothesourcesofLLMbiases. datasets, improved methods for LLM personalization such
LLM personalization. Our study is the first to analyze as automatic prompt optimization (Schulhoff et al. 2024),
biasinhatespeechannotationsatsuchafine-grainedlevel. and deeper investigations into specific biases in future re-
Thus,ourresultsabouttheinabilityofpersona-basedLLMs search. Other than these, future work should also focus on
toreproducehumanbiasesmightstemfromtwofactors:the intersectional studies, as combinations of attributes, rather
LLMs’ inability to replicate fine-grained biases, or the in- thanindividualones,cancausenewbiasesorreinforceex-
effectiveness of the personalization process. About the for- isting ones (Kim et al. 2020). To this end, the analyzed
mer,morepowerfulLLMsthanthosetestedhereincouldpo- dataset appears particularly well-suited for comprehensive
tentiallyovercometheissue,resultinginmoreaccuratehu- intersectional analyses given the availability of rich socio-
manrepresentations.Instead,regardingpersonalization,we demographicinformationforbothannotatorsandtargets.
foundalimitedcapacityoftheLLMstomodifytheirbehav-
iorandadapttospecificpersonas.Thisresultreinforcesthe Acknowledgments
current body of work that questions the ability of LLMs to
faithfullyreproducehumanbehaviors(Santurkaretal.2023; This work is partially supported by the European Union
Lee,An,andThorne2023).Nonetheless,althoughweused – Next Generation EU within the PRIN 2022 framework
a widely adopted state-of-the-art approach for personaliza- project PIANO (Personalized Interventions Against Online
tion (Beck et al. 2024), different personalization methods Toxicity),andbytheItalianMinistryofEducationandRe-
(e.g.,modelfine-tuning)mightyieldbetterresults. search (MUR) in the framework of the FoReLab projects
(DepartmentsofExcellence).
Implications. In addition to being a valuable re-
source for scholars studying prejudices against vulnerable
groups(Saha,Chandrasekharan,andDeChoudhury2019), References
our work also contributes to the development of fair hate
Aher,G.V.;Arriaga,R.I.;andKalai,A.T.2023.Usinglarge
speech detection systems. Our work is relevant in this area
languagemodelstosimulatemultiplehumansandreplicate
since human biases hinder the manual annotation of train-
humansubjectstudies. InICML.
7https://doi.org/10.5281/zenodo.13906773 AlKuwatly,H.;Wich,M.;andGroh,G.2020. Identifyingand measuring annotator bias based on annotators’ demo- Parmar,M.;Mishra,S.;Geva,M.;andBaral,C.2023.Don’t
graphiccharacteristics. InWOAH. blame the annotator: Bias already starts in the annotation
Argyle,L.P.;Busby,E.C.;Fulda,N.;Gubler,J.R.;Rytting, instructions. InEACL.
C.; and Wingate, D. 2023. Out of one, many: Using lan- Perez,E.;Ringer,S.;Lukosiute,K.;Nguyen,K.;Chen,E.;
guagemodelstosimulatehumansamples. PoliticalAnaly- Heiner, S.; Pettit, C.; Olsson, C.; Kundu, S.; Kadavath, S.;
sis,31(3). et al. 2023. Discovering language model behaviors with
Baack,S.2024. Acriticalanalysisofthelargestsourcefor model-writtenevaluations. InACL.
generativeAItrainingdata:CommonCrawl.InACMFAccT. Prabhakaran, V.; Davani, A. M.; and D´ıaz, M. 2021. On
Beck, T.; Schuff, H.; Lauscher, A.; and Gurevych, I. 2024. releasingannotator-levellabelsandinformationindatasets.
Sensitivity,performance,robustness:Deconstructingtheef- InLAW-DMR.
fectofsociodemographicprompting. InEACL.
Rao, H.; Leung, C.; and Miao, C. 2023. Can ChatGPT
Das,A.;Zhang,Z.;Jamshidi,F.;Jain,V.;Chadha,A.;Ray- AssessHumanPersonalities?AGeneralEvaluationFrame-
chawdhary, N.; Sandage, M.; Pope, L.; Dozier, G.; and work. InEMNLP.
Seals, C. 2024. Investigating annotator bias in large lan-
Sachdeva,P.;Barreto,R.;Bacon,G.;Sahn,A.;VonVacano,
guagemodelsforhatespeechdetection. arXiv:2406.11109.
C.; and Kennedy, C. 2022a. The measuring hate speech
Davani, A. M.; Atari, M.; Kennedy, B.; and Dehghani, M. corpus:LeveragingRaschmeasurementtheoryfordataper-
2023. Hatespeechclassifierslearnnormativesocialstereo- spectivism. InNLPerspectives.
types. TACL,11.
Sachdeva,P.S.;Barreto,R.;vonVacano,C.;andKennedy,
Garg, T.; Masud, S.; Suresh, T.; and Chakraborty, T. 2023.
C.J.2022b.Assessingannotatoridentitysensitivityviaitem
Handling bias in toxic speech detection: A survey. ACM
response theory: A case study in a hate speech corpus. In
CSUR.
ACMFAccT.
Geva,M.;Goldberg,Y.;andBerant,J.2019. Arewemodel-
Saha, K.; Chandrasekharan, E.; and De Choudhury, M.
ingthetaskortheannotator?Aninvestigationofannotator
2019.Prevalenceandpsychologicaleffectsofhatefulspeech
biasinnaturallanguageunderstandingdatasets.InEMNLP-
inonlinecollegecommunities. InACMWebSci.
IJCNLP.
Sang,Y.;andStanton,J.2022. Theoriginandvalueofdis-
Gupta, S.; Shrivastava, V.; Deshpande, A.; Kalyan, A.;
agreement among data labelers: A case study of individual
Clark, P.; Sabharwal, A.; and Khot, T. 2024. Bias runs
differencesinhatespeechannotation. IniConference.
deep: Implicit reasoning biases in persona-assigned LLMs.
InICLR. Santurkar,S.;Durmus,E.;Ladhak,F.;Lee,C.;Liang,P.;and
Hashimoto, T. 2023. Whose opinions do language models
Hettiachchi, D.; Holcombe-James, I.; Livingstone, S.;
reflect? InICML.
de Silva, A.; Lease, M.; Salim, F. D.; and Sanderson, M.
2023. How crowd worker factors influence subjective an- Sap, M.; Swayamdipta, S.; Vianna, L.; Zhou, X.; Choi, Y.;
notations: A study of tagging misogynistic hate speech in andSmith,N.A.2022. Annotatorswithattitudes:Howan-
tweets. InAAAIHCOMP. notator beliefs and identities bias toxic language detection.
InNAACL-HLT.
Hu,T.;andCollier,N.2024. Quantifyingthepersonaeffect
inLLMsimulations. InACL. Schulhoff, S.; Ilie, M.; Balepur, N.; Kahadze, K.; Liu, A.;
Kim, J. Y.; Ortiz, C.; Nam, S.; Santiago, S.; and Datta, V. Si,C.;Li,Y.;Gupta,A.;Han,H.;Schulhoff,S.;etal.2024.
2020. Intersectional bias in hate speech and abusive lan- ThePromptReport:Asystematicsurveyofpromptingtech-
guagedatasets. InAAAIICWSM. niques. arXiv:2406.06608.
La Cava, L.; Costa, D.; and Tagarelli, A. 2024. Open Simmons,G.2023. Moralmimicry:Largelanguagemodels
models,closedminds?Onagentscapabilitiesinmimicking produce moral rationalizations tailored to political identity.
human personalities through open large language models. InACL.
arXiv:2401.07115.
Srivastava,A.;Rastogi,A.;Rao,A.;Shoeb,A.A.M.;Abid,
Lee,N.;An,N.M.;andThorne,J.2023.Canlargelanguage A.;Fisch,A.;Brown,A.R.;Santoro,A.;Gupta,A.;Garriga-
modelscapturedissentinghumanvoices? InEMNLP. Alonso,A.;etal.2023.BeyondtheImitationGame:Quanti-
Mathew, B.; Dutt, R.; Goyal, P.; and Mukherjee, A. 2019. fyingandextrapolatingthecapabilitiesoflanguagemodels.
Spreadofhatespeechinonlinesocialmedia. InACMWeb- TMLR.
Sci,173–182. To¨rnberg, P.; Valeeva, D.; Uitermark, J.; and Bail, C.
Nogara, G.; Pierri, F.; Cresci, S.; Luceri, L.; To¨rnberg, P.; 2023. Simulating social media using large language
and Giordano, S. 2025. Toxic bias: Perspective API mis- models to evaluate alternative news feed algorithms.
readsGermanasmoretoxic. InAAAIICWSM. arXiv:2310.05984.
Ouyang,L.;Wu,J.;Jiang,X.;Almeida,D.;Wainwright,C.; Tseng, Y.-M.; Huang, Y.-C.; Hsiao, T.-Y.; Hsu, Y.-C.; Foo,
Mishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.; J.-Y.;Huang,C.-W.;andChen,Y.-N.2024.Twotalesofper-
etal.2022. Traininglanguagemodelstofollowinstructions sonainLLMs:Asurveyofrole-playingandpersonalization.
withhumanfeedback. NeurIPS. arXiv:2406.01171.Wan,Y.;Wang,W.;He,P.;Gu,J.;Bai,H.;andLyu,M.R. Appendix
2023. BiasAsker: Measuring the bias in conversational AI
system. InACMESEC/FSE.
Waseem, Z.; and Hovy, D. 2016. Hateful symbols or hate-
fulpeople?Predictivefeaturesforhatespeechdetectionon
Twitter. InNAACL.
Wich,M.;AlKuwatly,H.;andGroh,G.2020. Investigating
annotatorbiaswithagraph-basedapproach. InWOAH.
Wich,M.;Widmer,C.;Hagerer,G.;andGroh,G.2021. In-
vestigating annotator bias in abusive language datasets. In
RANLP.attribute value annotators targets
attribute value annotators targets
age youngadults 4,964(62.78%) 935(26.55%)
teenagers 201(2.54%) 786(22.32%) disability cognitive - 1,735(36.77%)
children 0(0%) 642(18.23%) physical - 1,126(23.86%)
middleaged 2,306(29.16%) 560(15.90%) unspecific - 855(18.12%)
seniors 436(5.52%) 500(14.19%) neurological - 542(11.48%)
other - 99(2.81%) hearingimpaired - 142(3.01%)
visuallyimpaired - 125(2.65%)
gender women 4,426(55.67%) 27,889(54.34%)
other - 194(4.11%)
men 3,392(42.67%) 10,029(19.54%)
transgenderunspecified 67(0.84%) 4,703(9.16%) education collegeba 2,913(36.82%) -
transgendermen - 3,326(6.48%) somecollege 2,064(26.09%) -
transgenderwomen - 2,611(5.09%) collegeaa 1,041(13.16%) -
nonbinary 59(0.74%) 2,116(4.12%) highschool 850(10.74%) -
other 6(0.08%) 651(1.27%) master 729(9.22%) -
professionaldegree 176(2.22%) -
race black 791(9.21%) 22,899(33.86%)
PhD 89(1.13%) -
white 6,373(74.2%) 9,797(14.49%)
somehighschool 49(0.62%) -
middleeastern 51(0.59%) 9,450(13.97%)
latinx 560(6.52%) 8,497(12.56%) ideology liberal 1,968(24.88%) -
asian 552(6.43%) 7,025(10.39%) neutral 1,367(17.28%) -
nativeamerican 154(1.79%) 2,819(4.17%) slightlyliberal 1,242(15.70%) -
pacificislander 28(0.33%) 2,358(3.49%) extremelyliberal 1,065(13.46%) -
other 80(0.93%) 4,780(7.07%) conservative 895(11.31%) -
slightlyconservative 860(10.87%) -
religion muslim 56(0.70%) 12,509(38.30%)
extremelyconservative 264(3.34%) -
christian 3,350(41.89%) 6,982(21.38%)
noopinion 249(3.16%) -
jewish 128(1.60%) 6,924(21.20%)
hindu 37(0.46%) 1,285(3.93%) income 10K-50K 3,321(42.01%) -
atheist 1,608(20.11%) 953(2.92%) 50K-100K 3,079(38.94%) -
mormon 60(0.75%) 953(2.92%) 100K-200K 1,009(12.76%) -
buddhist 126(1.58%) 729(2.23%) <10K 375(4.74%) -
other 497(6.21%) 2,328(7.12%) >200K 122(1.55%) -
nothing 2,191(27.40%) -
origin specificcountry - 14,124(43.45%)
sexuality gay 308(3.91%) 15,465(42.34%) immigrant - 9,525(29.30%)
lesbian - 6,883(18.85%) undocumented - 6,081(18.71%)
bisexual 727(9.22%) 6,631(18.16%) migrantworker - 2,523(7.76%)
straight 6,727(85.30%) 4,438(12.15%) other - 250(0.78%)
other 124(1.57%) 3,104(8.50%)
Table T1: The dataset includes socio-demographic attributes for both annotators and hate targets, with each attribute taking
on multiple values. In this comprehensive overview, we present the distributions of these values. The left column reports
attributesthatareavailableforbothannotatorsandhatetargets,whiletherightcolumnsdisplaysattributesspecifictoonlyone
ofthesecategories.Grey-coloredrowsrepresentattributesthatareexcludedfromtheresults,eitherbecausetheydidnotyield
statisticallysignificantfindings(e.g.,ideology:extremelyliberal)orbecausetheywerenotused,suchasallothervalues.Algorithm A1: Pseudo-code to compute a confusion matrix D for annotators having the socio-demographic attribute t = v,
annotatingpoststargetedatindividualswithattributet′ =v′.
1: functionCOMPUTE MATRIX(t=v,t′ =v′)
2: D ←INIT MATRIX ▷Initmatrixwithzeros
3: forallcomment∈G(t′ =v′)do
4: forallannotator ∈A(t=v)do
5: text analyzed←EXTRACT TEXT(comment) ▷Getthetextassociatedwithcurrentcommentrecord
6: other comments←GET RECORDS FROM TEXT(text analyzed) ▷Getallrecordssharingthistext
7: forallother comment∈other commentsdo
8: ifcomment=other commentthen
9: continue
10: endif
11: An←ANNOTATOR(comment) ▷Getannotatorattribute/value
12: An other ←ANNOTATOR(other comment) ▷Getannotatorattribute/value
13: ifAn̸=An otherthen
14: comment label←GET HATESPEECH LABEL(comment) ▷T,N,orM
15: other comment label←GET HATESPEECH LABEL(other comment) ▷T,N,orM
16: INC CELL MATRIX(D,comment label,other comment label,1) ▷Incby1correspondingmatrixcell
17: endif
18: endfor
19: endfor
20: endfor
21: returnD
22: endfunctionObjective:Youareanexperiencedhumanannotatorwhomustanalyzeasocialnetworktextcommentanddeterminewhether
thecommenttargetsaprotectedgroupanddetermineifitcontainshatespeechornot.
Tostateifthetextcommentcontainshatespeech,youmustanswerthefollowingquestion:
Does this comment contain hate speech, defined as ”bias-motivated, hostile and malicious language targeted at a per-
son/group because of their actual or perceived innate characteristics, especially when the group is unnecessarily labeled
(EXAMPLE:”jewbankers”).”
Also,youmusttakeinmindthat:
The comment contains hate speech if slurs or derogatory language are used toward any person or group regardless of
whetherornotthetargetisamemberoftheslur-targetgroup.(e.g.acommentcallingTrumpafaggot).
Ifyoucan’tdecideifthecommentcontainshatespeech,youmustlabelitas”maybe”
Todeterminethecommenttargets,youmustselectatleastonetargetfromthefollowinglist:{target list}
Inperformingthetask,youmustimpersonateanannotatorwiththefollowinglistedcharacteristics:
• annotator gender:{annotator gender}
• {annotator trans}
• {annotator sexuality}
• {annotator race}
• {annotator religion}
• annotator education:{annotator educ}
• annotator income:{annotator income}
• annotator ideology:{annotator ideology}
• annotator age:{annotator age}years.
YoumustanalyzetheinputtextandpredictanoutputinJSONformatthatindicates:
• hate speech:(yes—no—maybe)
• Alistnamed’targets’includingatleast1ormoretargetslistedabovewithhatespeechexpressedinthetext.
• ’targets’listmustbenotempty
YoumustonlyreturnrawJSONoutputasdescribedabove,inparticular:
• don’tgenerateanyexplanationsoradditionalnotesofthereasonsofsuchresponses
• don’tescapeanycharacter
Theinputtexttoanalyzeis:
{comment text}
TableT2:PrompttemplateusedfordynamicallypersonalizingLLMswithhumanannotators’socio-demographiccharacteris-
ticsandforannotatinghatespeechmessages.Objective:Youareanexperiencedhumanannotatorwhomustanalyzeasocialnetworktextcommentanddeterminewhether
thecommenttargetsaprotectedgroupanddetermineifitcontainshatespeechornot.
Tostateifthetextcommentcontainshatespeech,youmustanswerthefollowingquestion:
Does this comment contain hate speech, defined as ”bias-motivated, hostile and malicious language targeted at a per-
son/group because of their actual or perceived innate characteristics, especially when the group is unnecessarily labeled
(EXAMPLE:”jewbankers”).”
Also,youmusttakeinmindthat:
The comment contains hate speech if slurs or derogatory language are used toward any person or group regardless of
whetherornotthetargetisamemberoftheslur-targetgroup.(e.g.acommentcallingTrumpafaggot).
Ifyoucan’tdecideifthecommentcontainshatespeech,youmustlabelitas”maybe”
Todeterminethecommenttargets,youmustselectatleastonetargetfromthefollowinglist:{target list}
Inperformingthetask,youmustimpersonateanannotatorwiththefollowingfeature:
• annotator feature:{annotator random feature}
YoumustanalyzetheinputtextandpredictanoutputinJSONformatthatindicates:
• hate speech:(yes—no—maybe)
• Alistnamed’targets’includingatleast1ormoretargetslistedabovewithhatespeechexpressedinthetext.
• ’targets’listmustbenotempty
YoumustonlyreturnrawJSONoutputasdescribedabove,inparticular:
• don’tgenerateanyexplanationsoradditionalnotesofthereasonsofsuchresponses
• don’tescapeanycharacter
Theinputtexttoanalyzeis:
{comment text}
TableT3:Alternativepromptobtainedbyreducingthenumberofsocio-demographicattributespassedtothemodel.Objective: You are an experienced annotator tasked with analyzing a social network comment to identify if it targets a
protectedgroupandcontainshatespeech.
Hate speech is defined as ”bias-motivated, hostile, and malicious language aimed at a person/group based on their ac-
tualorperceivedinnatecharacteristics,especiallywhenthegroupisunnecessarilylabeled(e.g.,’jewbankers’).”
A comment qualifies as hate speech if slurs or derogatory language are used, even if the target is not a member of
theslur-targetedgroup(e.g.,callingTrumpafaggot).Ifunsure,labelitas”maybe.”
Todeterminethetargets,selectatleastonefromthefollowinglist:{target list}
Inperformingthetask,youmustimpersonateanannotatorwiththefollowinglistedcharacteristics:
• annotator gender:{annotator gender}
• {annotator trans}
• {annotator sexuality}
• {annotator race}
• {annotator religion}
• annotator education:{annotator educ}
• annotator income:{annotator income}
• annotator ideology:{annotator ideology}
• annotator age:{annotator age}years.
YoumustpredictandreturnthefollowingoutputinJSONformat:
• hate speech:(yes—no—maybe)
• Alistnamed’targets’containingatleastonetargetfromtheabovelist.
ReturnonlytherawJSONoutputwithnoexplanationsornotes.
Theinputtexttoanalyzeis:
{comment text}
TableT4:Alternativepromptobtainedbysimplifyingtheinstructionsforthehatespeechannotationtask.