Noether’s Razor: Learning Conserved Quantities
TychoF.A.vanderOuderaa MarkvanderWilk PimdeHaan
ImperialCollegeLondon UniversityofOxford CuspAI
London,UK Oxford,UK Amsterdam,NL
Abstract
Symmetrieshaveprovenusefulinmachinelearningmodels,improvinggeneralisa-
tionandoverallperformance. Atthesametime,recentadvancementsinlearning
dynamicalsystemsrelyonmodellingtheunderlyingHamiltoniantoguaranteethe
conservationofenergy. Theseapproachescanbeconnectedviaaseminalresult
inmathematicalphysics: Noether’stheorem, whichstatesthatsymmetriesina
dynamicalsystemcorrespondtoconservedquantities. ThisworkusesNoether’s
theoremtoparameterisesymmetriesaslearnableconservedquantities. Wethen
allowconservedquantitiesandassociatedsymmetriestobelearneddirectlyfrom
traindatathroughapproximateBayesianmodelselection,jointlywiththeregular
trainingprocedure. Astrainingobjective,wederiveavariationallowerboundto
themarginallikelihood. TheobjectiveautomaticallyembodiesanOccam’sRazor
effectthatavoidscollapseofconservationlawstothetrivialconstant,withoutthe
needtomanuallyaddandtuneadditionalregularisers. Wedemonstrateaproof-of-
principleonn-harmonicoscillatorsandn-bodysystems. Wefindthatourmethod
correctlyidentifiesthecorrectconservedquantitiesandU(n)andSE(n)symmetry
groups,improvingoverallperformanceandpredictiveaccuracyontestdata.
1 Introduction
Symmetriesprovidestronginductivebiases,effectivelyreducingthevolumeofthehypothesisspace.
Acelebratedexampleofthisistheconvolutionallayerembeddingtranslationequivarianceinneural
networks,whichcanbegeneralisedtoothersymmetrygroups[CohenandWelling,2016].
Meanwhile,physics-informedmachinelearningmodels[Greydanusetal.,2019,Cranmeretal.,2020],
typicallyrelyingonneuraldifferentialequations[Chenetal.,2018],embedconstraintsknownfrom
classicalmechanicsintomodelarchitecturestoimproveaccuracyonphysicaldynamicalsystems.
Ratherthanstrictlyconstrainingamodeltocertainsymmetries,recentworkshaveexploredwhether
invarianceandequivariancesymmetriesinmachinelearningmodelscanalsobeautomaticallylearned
fromdata. Thisoftenreliesonseparatevalidationdata[Maileetal.,2022],explicitregularisers[Finzi
etal.,2021]oradditionalouterloops[Cubuketal.,2018]. Alternatively,wecantakeaBayesian
approachwhereweembedsymmetriesintothepriorandempiricallylearnthemthroughBayesian
modelselection[vanderWilketal.,2018,Immeretal.,2022,vanderOuderaaetal.,2022].
WeproposetouseNoether’stheorem[Noether,1918]toparameterisesymmetriesinHamiltonian
machinelearningmodelsintermsoftheirconservedquantities. Todoso,weproposetosymmetrise
alearnableHamiltonianusingasetoflearnablequadraticconservedquantitites. Bychoosingthe
conservedquantitiestobequadratic,wecanfindclosed-formtransformationsthatcanbeusedto
obtainanunbiasedestimateofthesymmetrisedHamiltonian.
Secondly,wephasesymmetriesimpliedbyconservedquantitiesintheprioroverHamiltoniansan
leveragetheOccam’srazoreffectofBayesianmodelselection[RasmussenandGhahramani,2000,
vanderWilketal.,2018]tolearnconservedquantitiesandtheirimpliedsymmetriesdirectlyfrom
train data. We derive a practical lower bound using variational inference [Hoffman et al., 2013]
Preprint.AcceptedforpublicationaspartoftheNeurIPS2024proceedings.
4202
tcO
01
]GL.sc[
1v78080.0142:viXraresultinginasingleend-to-endtrainingprocedurecapableoflearningtheHamiltonianofasystem
jointlywithitsconservedquantities. Asfarasweknow,thisisthefirstcaseinwhichBayesianmodel
selectionwithvariationalinferenceissuccessfullyscaledtodeepneuralnetworks,anachievementin
itsownright,whereasmostworkssofarhavereliedonLaplaceapproximations[Immeretal.,2021].
Experimentally,weevaluateourNoether’srazormethodonvariousdynamicalsystems,including
n-simpleharmonicoscillatorsandn-bodysystems. Ourresultssuggestthatourmethodisindeed
capableoflearningtheconservedquantitiesthatgiverisetocorrectsymmetrygroupsfortheproblem
at hand. Quantitatively, we find that our method that learns symmetries from data matches the
performanceofmodelswiththecorrectsymmetriesbuilt-inasoracle. Weoutperformvanillatraining,
resultinginimprovedtestgeneralisationandpredictionsthatremainaccurateoverlongertimeperiods.
2 Background
2.1 Hamiltonianmechanics
Hamiltonianmechanicsisaframeworkthatdescribesdynamicalsystemsinphasespace,denoted
M=RM,withM even. Phasespaceelements(q,p)∈MfollowHamiltonianequationsofmotion:
∂H ∂H
q˙ = , p˙ =− (1)
i ∂p i ∂q
i i
wheretheHamiltonianH : M → Risanobservable1,whicharesmoothfunctionsonthephase
space,thatcorrespondstotheenergyofthesystem. Itisoftensimplertowritex=(q,p),sothatwe
have: x˙ =J∇H andJ =(cid:2) 0 I(cid:3) ,whereI istheidentitymatrix,J iscalledthesymplecticform
−I 0
and∇H =∇ H(x)isthegradientofphasespacecoordinates.
x
Example: n-bodyproblemin3d. Ifweconsiderad=3dimensionalEuclideanspacecontainingn
bodies, our position and velocity spaces are each R3n making up phase space M = R2·3n. Our
HamiltonianH :R3n×R3n →R,whichinthiscaseisaseparablefunctionH(q,p)=K(q)+P(p)
ofkineticenergyK(q)=(cid:80)
m
||p||2/2andthepotentialenergyP(p)=(cid:80)
Gm m /||q −q ||
i i i̸=j i j i j
wherem isthemassofabodyiandGisthegravitationalconstant.
i
2.2 LearningHamiltonianmechanicsfromdata
WecanmodeltheHamiltonianfromdata[Greydanusetal.,2019,RossandHeinonen,2023,Tanaka
etal.,2022,Zhongetal.,2019]. Concretely,weareinterestedinaposterioroverfunctionsthatthe
Hamiltoniancantakep(H |D),conditionedontrajectorydataD ={(xn,xn)}N sampledfrom
θ t t′ n=1
phasespaceatdifferenttimepoints(t,t′),ortimedifference∆t=t′−t. Givenanewdatapointx∗,
t
wewouldliketomakepredictionsp(x∗|x∗,H ,D)overphasespacetrajectoriesintothefuturet′.
t′ t θ
Hamiltonianneuralnetworks Hamiltonianneuralnetworks[Greydanusetal.,2019]modelthe
Hamiltonian H using a learnable Hamiltonian H : M → R parameterised by θ ∈ RP. With
θ
a straightforward Gaussian likelihood p(x |x ,θ) = N(x |x +J∇H (x )∆t,σ2 I) with a
t′ t t′ t θ t data
smallobservationnoiseσ2 ,amaximumlikelihoodfitcanbefoundbyminimisingthenegative
data
log-likelihoodθ =argmin (cid:80) (cid:80) −logp(xi |xi,θ)onminibatchesofdatausingstochastic
∗ θ i t t+∆t t
gradientdescent. ThemeanofthislikelihoodrepresentsasingleEulerintegrationstep(Sec. 2.1of
DavidandMéhats[2023]),whichboundsthepossibleaccuracyofthefittothetrueHamiltonianH.
Inpractice,wemayreplacethisbymoreaccuratedifferentiablenumericalintegrators[Kidger,2022].
2.3 Noether’stheorem
Thetheoremof[Noether,1918],herepresentedintheHamiltonianformalism[Baez,2020,Arnold,
1989],linkstheconceptsofanobservablebeingconserved,totheHamiltonianbeinginvarianttothe
symmetriesgeneratedbyanobservable.
1Thetermobservableinclassicalmechanicsshouldnotbeconfusedwiththestatisticalnotionofavariable
beingobservedornot.Infact,wewillmodelobservablesaslatentvariablesthatarenotobserved.
2Conserved quantity Let O be the set of observables, which are smooth real-valued functions
M → R on the phase space. Given a trajectory x(t) generated by the Hamiltonian H, we can
computethevariationofanobservableO ∈OintimeviathechainruleandHamilton’sequationsof
motion(Equation(1))
dO (cid:88)∂O ∂O (cid:88)∂O∂H ∂O∂H
= q˙ + p˙ = − ={O,H}, (2)
dt ∂q i ∂p i ∂q ∂p ∂p ∂q
i i i i i i
i i
wherethelastequalitydefinesthePoissonbracket{·,·}:O×O →O. ThePoissonbracketrelates
tothesymplecticformvia{O,H}(x)= ∇O(x)·J∇H(x). Anobservablethatdoesnotchange
alonganytrajectoryiscalledaconservedquantity. AswecanseefromEquation(2),anobservable
Oisconservedifandonlyif{O,H}=0.
From two conserved quantities O,O′ ∈ O, we can create a new conserved quantity by linear
combinationαO+βO′ ∈Owithcoefficientsforα,β ∈R,whichisconservedbecausethePoisson
bracket is linear in both arguments. Also, we can take the product OO′ ∈ O, with (OO′)(x) =
O(x)O′(x),whichisconservedbecausethePoissonbracketsatisfiesLeibniz’slawofdifferentiation
{OO′,H} = {O,H}O′ +O{O′,H}. Finally, the Poisson bracket of the conserved quantities
{O,O′}∈Oisalsoconserved,becauseoftheJacobiidentity.
Symmetriesgeneratedbyobservables ReferringbackastotheHamiltonianequationsofmotion
inEquation(1),notethattheseequationsworknotjustfortheHamiltonianH ∈Oofthesystem,
butforanyobservableO ∈ O. Sogivenanystartingpointx ,wecangenerateatrajectoryx(τ)
0
satisfying
x(0)=x x˙(τ)=J∇O(x(τ)). (3)
0
WehaveusedadifferentsymboltonotconflatetheODEtimeτ withregulartimetofthetrajectory
generatedbytheHamiltonian. DenotetheflowassociatedtothisODEgeneratedbyobservableOby
Φτ :M→M,mappingx toΦτ (x )=x(τ). NotethatanyODEflowsatisfiesΦ0 =id and
O 0 O 0 O M
Φτ+κ =Φτ ◦Φκ. Hence,theobservableOgeneratesaone-dimensionalgroupG ,parametrizedby
O O O O
τ,thatisasubgroupofthegroupDiff(M)ofdiffeomorphismsM→M.
Theorem1(Noether). TheobservableO ∈Oisaconservedquantityonthetrajectoriesgenerated
byHamiltonianH ∈OifandonlyifH isinvarianttoG ,meaningthatforallτ ∈R,H◦Φτ =H.
O O
Proof. ByreasoninganalogoustothatinEquation(2),thevalueoftheHamiltonianchangesunderthe
flowgeneratedbyobservableOas dH ={H,O}. NotingthatthePoissonbracketisanti-symmetric,
dτ
wehavethat: Oisaconservedquantity ⇐⇒ {O,H}=0 ⇐⇒ {H,O}=0 ⇐⇒ H isinvariant
totheflowgeneratedbyO.
2.4 Automaticsymmetrydiscovery
Symmetries play an important role in machine learning models, most notably group invariance
andequivarianceconstraints[CohenandWelling,2016]. Insteadofhavingtodefinesymmetries
explicitlyinadvance,recentattemptshavebeenmadetolearnsymmetriesautomaticallyfromdata.
Eveniflearnablesymmetriescanbedifferentiablyparameterised,learningthemcanremaindifficult
as symmetries act as constraints on the functions a model can represent and are, therefore, not
encouragedbyobjectivesthatsolelyoptimisetraindatafit. Asaresult,evenifasymmetrywould
leadtobettertestgeneralisation,thetrainingcollapsesintoselectingnosymmetry. Commonways
toovercomethisaredesigningexplicitregularisersthatencouragesymmetry[Bentonetal.,2020,
vanderOuderaaetal.,2022],whichoftenrequiretuning,oruseofvalidationdata[Aletetal.,2021,
Maileetal.,2022,Zhouetal.,2020]. Learningsymmetriesforintegrablesystemswasproposed
in [Bondesan and Lamacraft, 2019], whereas our framework works more generally also for non-
integrable systems, such as the 3-body problem. Recent works have demonstrated effectivity of
Bayesianmodelselectiontolearnsymmetriesdirectlyfromtrainingdata. Thisworksbyoptimising
themarginallikelihood,whichembodiesanOccam’srazoreffectthattradesoffdatafitandmodel
complexity. ForGaussianprocesses,thequantitycanoftenbecomputedinclosed-form[vanderWilk
etal.,2018],andcanbescaledtoneuralnetworksthroughvariationalinference[vanderOuderaa
andvanderWilk,2021]andlinearisedLaplaceapproximations[Immeretal.,2022].
33 SymmetrisingHamiltonianswithConservedQuantities
OurmethodintroducedinthenextsectionwilllearntheHamiltonianofasystemtogetherwitha
setofconservedquantities. First,inthissectionwediscusshowthelearnedconservedquantities
willbeparametrised,andhowwecanmaketheHamiltonianinvarianttothesymmetrygeneratedby
conservedquantities.
3.1 Parameterisingconservedquantities
In this work, we limit ourselves to modelling up to a fixed maximum number of K conserved
quantitiesC1,C2,...,CK :M→Rareobservablesparameterisedbysymmetrisationparameters
η η η
η,todistinguishthemfromthemodelparametersθparameterisingtheHamiltonianscalarfield.
Inthispaper,weconsiderquadraticconservedquantitiesoftheformC (x)=xTAx/2+bTx+c.
η
Asweusetheconservedquantitiesonlythroughtheirgradients,theconstantisarbitraryandcan
beignored. Thelearnablesymmetrisationparametersarethusη ={A,b},forasymmetricmatrix
A. A quadratic conserved quantity C generates a symmetry transformation whose scalar field
x˙ =J∇C(x)=JAx+Jbisaffine,orlinearonthehomogeneouscoordinates(x,1). Itsflowcan
beanalyticallysolved
(cid:20) (cid:21) (cid:18) (cid:20) (cid:21)(cid:19)(cid:20) (cid:21)
Φτ (x) JA Jb x
C =exp τ (4)
1 0T 0 1
using the matrix exponential exp(·) for which efficient numerical algorithms exist [Moler and
VanLoan,2003].Thisequationcanbeverifiedtohavethecorrectscalarfieldandboundarycondition,
andthusformstheuniquesolutiontotheODEinEquation(3).
3.2 Symmetrisingobservables
GivenanobservableC ∈ O,wewanttotransformanobservablef intofˆthatisinvarianttothe
transformationsgeneratedbyC. Thismeansthatfˆ◦Φτ = fˆforallsymmetrytimeτ ∈ R. Via
C
Noether’stheorem,weknowthatthisisequivalenttoC beingconservedinthetrajectoriesgenerated
byf,andalsoequivalentto{C,fˆ}=0. However,thisequationdoesnotprescribehowtoobtain
suchfˆ. Instead,we’llcreatefˆbysymmetrizingoverthesymmetrygroupgeneratedbyC. Thisis
donebyaveragingovertheorbitofthetransformation
(cid:90)
fˆ(x)= f(Φτ (x))µ(τ).
C
R
withameasureµoversymmetrytimeτ. Thismeasureµinducesameasureonthe1-dimensional
subgroup G of the group of diffeomorphisms M → M. If this measure on G is uniform
C C
(specifically,aright-invariantmeasure [Halmos,1950]),thenfˆisindeedinvariant.
Insteadofasinglesymmetrygenerator,wecanalsohaveasetC ={C ,...,C }ofobservablesand
1 K
wewanttomakef invarianttoallofthese. Assumethatthissetspansavectorspaceofobservables
thatisclosedunderthePoissonbracket(i.e. theyformaLiesubalgebra). Inthatcase,thegroupsof
transformationsoftheobservablescombinedgenerateagroupG [Hall,2015,Thm.5.20].Thisgroup
C
isparameterizedbyavectorofsymmetrytimesτ ∈RK. ThecorrespondingflowisΦτ =Φ1 .
C (cid:80) iτiCi
Tomakeanobservablef invarianttothesymmetriesofallconservervedquantitiesC,equivalentlyto
thegroupG ,wesymmetrize
C
(cid:90)
fˆ(x)= f(Φτ(x))µ(τ). (5)
C
RK
with some measure µ over RK. As before, if this induces a uniform measure over G , then this
C
symmetrizationindeedmakesfˆinvarianttoG .
C
However,aprobabilitymeasureµ(τ)thatgivesauniformdistributionoverG mightnotexist,for
C
examplewhenthegroupcontainsanon-compactgroupoftranslations. Evenwhensuchameasure
does exist, it may be hard to construct, and the symmetrisation integral in Equation (5) may be
intractabletocompute. Soinstead,inpractice,weapproximatethisbychoosingµ(τ)tobeaunit
normaldistributionN(0,I )oruniformdistribution. Thisresultsinarelaxednotionofsymmetry
K
4infˆwhichcanbeinterpretedasaformofrobustnesstoactionsofthesymmetrygroupimpliedby
theconservedquantity,bysmoothingthefunctioninthisdirectionarounddata,incontrasttostrict
invariancebydefinitionclosedundergroupactionsalongthefullorbit. Finally,weapproximatethe
integralbyanunbiasedMonteCarloestimatewithS samples.
4 AutomaticSymmetryDiscoveryusingNoether’sRazor
Nowthatwehaveawayofparameterisingsymmetrydifferentiablyasconservationlawsthrough
Noether’stheorem,weneedanobjectivefunctionthatiscapableofselectingtherightsymmetry.
Unfortunately,regulartrainingobjectivesthatonlyrelyondatafitcannotnecessarilydistinguishthe
correctinductivebias,asnotedinpriorwork[vanderWilketal.,2018,Immeretal.,2022,vander
OuderaaandvanderWilk,2021]. Thisisbecause,eveniftraindataoriginatesfromasymmetric
distribution,therecanbebothnon-symmetricandsymmetricsolutionsthatfitthetraindataequally
well,givenasufficientlyflexiblemodel. Consequently,theregularmaximumlikelihoodobjective
thatonlymeasurestraindatafitwillnotnecessarilyfavourasymmetricmodel,evenifweexpect
thistogeneralisebestontestdata. Insteadofhavingtoresorttocross-validationtoselecttheright
symmetryinductivebias,weproposetouseanapproximatemarginallikelihoodonthetraindata.
Thishastheadditionalbenefitofbeingdifferentiable,allowingsymmetrisationtobelearnedwith
back-propagationalongwithregularparametersinasingletrainingprocedure. Inourcase,weuse
Noether’stheoremtoparameterisesymmetriesinourpriorthroughconservedquantities,whichwe
canoptimisewithback-propagationusingadifferentiablelowerboundonthemarginallikelihood.
Thisquantity,alsoknownasthe‘evidence’,differsdistinctlyfrommaximumlikelihoodinthatit
balancesbothtrainfitaswellasmodelcomplexity. TheOccam’srazoreffectencouragessymmetry
andleveragesthesymmetrisationprocessto‘cutaway’priordensityoverHamiltonianfunctions
thatarenotsymmetric,ifthisdoesnotresultinaworsedatafit. Theresultingposteriorpredictions
automaticallybecomessymmetricifobserveddataobeysasymmetry(highevidenceforsymmetry),
butcanbecomenon-symmetricifthisdoesnotmatchthedata(lowevidenceforsymmetry). Hence,
thenameofourproposedmethodforautomaticinductivebiasselectionisNoether’srazor.
4.1 Probabilisticmodelwithsymmetriesembeddedintheprior.
F C
θ η
Tobemoreexplicitaboutourprobabilisticmodel,wecanintroduce
fourvariables,namelyanon-symmetrisedobservableF ,asetof
θ
H
conservedquantitiesC ,whichinduceasymmetrisedHamiltonian
η
H generating the observed trajectory data X. We treat trajectory
dataananobservedvariable,considertheconservedquantitiesas
X
partofanempiricalpriorasweoptimiseoverthem,andintegrate
outtheHamiltonianaslatent. Theconstructioncanbeinterpreteda
placingasophisticatedprioroverthefunctionsthatthesymmetrised
Figure 1: Graphical proba-
HamiltonianH canrepresent,whichisthevariableofprimaryin-
bilisticmodel. Trajectorydata
terest. Theunderlyingnon-symmetrisedF doesnothaveadirect
θ X dependsonasymmetrised
physicalmeaningasHdoes,butdefinesaprioroverneuralnetworks
Hamiltonian H induced by
toflexiblydefineadensityoverarichclassofpossiblefunctions.
non-symmetrised observable
The conserved quantities C control the amount of symmetry in
η F andconservationlawsC.
theeffectiveprioroversymmetrisedHamiltoniansH. Empirically
optimisingC throughBayesianmodelselectionallowsusto‘cut
η
away’ density in the prior over H that correspond to functions that are not symmetric - as the
symmetrisationaveragesfunctionsinF thatlieinthesameorbitandtherebyincreasestherelative
θ
densityofsymmetricfunctionsinH. Wehypothesisethatwewillnotover-fitconservedquantitiesas
η isrelativelylow-dimensional,onlyrepresentingquadraticfunctions,whileweintegrateoutthe
high-dimensionalneuralnetworkmodelparametersθthatparameterisestheobservableF . Infuture
θ
work,itwouldbeinterestingtoexplorearicherfunctionclassesforconservedquantities,suchas
neuralnetworks,althoughwedoexpectthistobemoredifficultandtorequireadditionalpriorsor
regularisationtechniquestoavoidover-fitting.
54.2 Bayesianmodelselectionforsymmetrydiscovery
Tolearntherightsymmetryfromdata,weproposetouseBayesianmodelselectionthroughoptimisa-
tionofthemarginallikelihood. Intheprevioussections,wehavephrasedsymmetriesparameterised
byη aspartoftheprioroverHamiltonians. Thesymmetryparametersη parameterisethespace
ofpossible‘models’thatweconsider,whereasthemodelparametersθ parameterisetheweights
ofasinglemodel. ToperformBayesianmodelselectiononthesymmetries, weareinterestedin
computingthemarginallikelihood:
(cid:90)
p(x|η)= p(x|θ,η)p(θ)dθ (6)
θ
which requires integrating (marginalising) the likelihood over model parameters θ weighted by
theprior,andissometimesreferredtoasthe‘evidence’foraparticularmodel. Unlikemaximum
likelihood, the marginal likelihood has an Occam’s razor effect [Smith and Spiegelhalter, 1980,
Rasmussen and Ghahramani, 2000] that balancing both data fit and model complexity, allowing
optimisationofsymmetryparametersη. Althoughthemarginallikelihoodistypicallyintractable,
certainapproximateBayesianinferencetechniquescanprovidedifferentiableestimates. Inthenext
sections,wewillusevariationalinferencetoderiveatractableanddifferentiablelowerboundtothe
marginallikelihoodthatcanbeusedtofindaposterioroverθandoptimisesymmetriesη.
Whythemarginallikelihoodcanlearnsymmetry Tounderstandwhythemarginallikelihood
objectiveiscapableoflearningtherightsymmetry(tolearnη),Sec. 3.2[vanderWilketal.,2018]
proposedtodecomposeitthroughtheproductrule:
(cid:89)C
p(x|η)=p(x |η)p(x |x ,η)p(x |x ,η) p(x |x ,η) (7)
1 2 1 3 1:2 c 1:c−1
c=4
whichshowsthatthemarginallikelihoodmeasureshowmuchpartsofthedatasetpredictotherparts
of the data - a measure of generalisation that does not require cross-validation. Given a perfect
datafit,themarginallikelihoodwillbehigherwhentherightsymmetryisselected,aspartsofthe
datasetwill resultinbetterand morecertainpredictionson otherpartof thedata. This isunlike
themaximumlikelihood,whichisalwaysmaximisedwithperfectdatafit,withorwithouttheright
symmetry. Forsomeposteriorapproximations,suchaslinearisedLaplaceapproximations,itcan
beanalyticallyshownthatsymmetrymaximisestheapproximatemarginallikelihood(App. G.2of
Immeretal.[2022]). Ourmethodisverysimilar, butusesmoreexpressivevariationalinference
whichcanoptimisetheposteriorglobally,ratherthanrelyingonalocalTaylorexpansion.
4.3 Lowerboundingthemarginallikelihood
ThemarginallikelihoodofanHamiltonianneuralnetworkistypicallynottractableinclosed-form.
However,wecanderivealowerboundtothemarginallikelihoodusingvariationalinference(VI):
logp(x|η)≥E [logp(x|θ,η)]−KL(q (θ)||p(θ)) (8)
θ m,S
(cid:20) (cid:20) (cid:21)(cid:21)
(cid:88)N
≥E
θ
E
τ
logN(xi
t′
|H(cid:98) θτ ,η(xi t),σ d2 ataI) −KL(q m,S(θ)||p(θ |0,σ p2 riorI))
i=1
whereH(cid:98) θτ ,η(xi t)= S1 (cid:80)S s=1H θ,η(Φτ η(s)(xi t))andH(cid:98) isanunbiasedS-sampleMonteCarloestimator
ofthesymmetrisedHamiltonian. WewriteE θ :=E θ∼q m,S andE τ =E τ∼(cid:81)S µ(τ)forwhichwe
s=1
canobtainanunbiasedestimatebytakingMonteCarlosamples. Thefirstinequalityisthestandard
VIlowerbound. ThesecondinequalityfollowsfromapplyingJensen’sinequality(again)whichuses
thefactthattheloglikelihoodisaconvexfunction. Similarlowerboundstoinvariantmodelsthat
averageoverasymmetrygrouphaverecentlyappearedinpriorwork[vanderOuderaaandvander
Wilk,2021,Schwöbeletal.,2022,Nabarroetal.,2022]. FullderivationinAppendixA.1.
4.4 ImprovedvariationalinferenceforscalableBayesianmodelselection
VariationalinferenceisacommontooltoperformBayesianinferenceonmodelswithintractable
marginal likelihoods, including neural networks. In deep learning literature, however, its use is
typicallylimitedtobetterpredictiveuncertaintyestimationandrarelyforBayesianmodelselection.
Meanwhile,linearisedLaplaceapproximationshaverecentlybeensuccessfullyappliedtoBayesian
6modelselection[Immeretal.,2021]andsymmetrylearninginspecific[Immeretal.,2022,vander
Ouderaaetal.,2024],withafewreportedcasesofmodelselectionusingVIonlyinsingleneural
network layers [van der Ouderaa and van der Wilk, 2021, Schwöbel et al., 2021]. Optimising
Bayesianneuralnetworkswithvariationalinferenceismuchlessestablishedthantrainingregular
neural networks, for which many useful heuristics are available. This work, however, provides
evidencethatitisalsopossibletoperformapproximateBayesianmodelselectionusingVIindeep
neuralnetworks,whichwedeemaninterestingobservationinitsownright. Tomakesurethelower
boundonthemarginallikelihoodissufficientlytight,weemployaseriesoftechniques,includinga
richernon-meanfieldfamilyofmatrixnormalposteriors[LouizosandWelling,2016],andclosed-
form updates of the prior precision and output variances derived with expectation maximisation.
Details on how we train a Bayesian neural network using variational inference can be found in
AppendixD.
5 Results
Inthissection,wewilldiscusshowthelearnedsymmetriesareanalysedandthenlistourexperiments
andresults.
5.1 Analyzinglearnedsymmetries
Inourexperiments, wewillfindasetofK conservedquantitiesC : M → R. Asweconsider
k
quadraticconservedquantitiesinparticular,wecanequivalentlyanalyzetheresultinggeneratorsof
theassociatedsymmetriesGˆ (x) = J∇C whichareaffineandthusrepresentablewithamatrix
k k
Gˆ ∈R(M+1)×(M+1)onhomogeneouscoordinates(x,1). InAppendixB,welistforeachsystem
k
theLgroundtruthconservedquantitiesgeneratorsG⋆. Thelearnedandgroundtruthgenerators
l
can be stacked in to the matrices Gˆ ∈ RK×(M+1)2,G∗ ∈ RL×(M+1)2 respectively. As we can
identifythesymmetriesonlyuptolinearcombinations,wehavelearnedthecorrectsymmetriesif
thelearnedgeneratorsspanalinearsubspaceofR(M+1)2 thatcoincideswiththespacespannedby
thegroundtruthgenerators. Toverifythis,wetesttwoproperties. First,weshowthatthematrix
Gˆ hasLnon-zerosingularvalues. Secondly, forthefirstLrightsingularvectorsv ∈ R(M+1)2,
i
wedecomposev = v∥+v⊥ inavectoringroundtruthsubspace,andoneorthogonaltoit. The
i i i
learnedv isacorrectconservedquantityifv⊥ =0,orequivalently,becausethesingularvectorsare
i i
normalized,if∥v∥∥=1. Wecallthismeasurethe“parallelness”.
i
5.2 SimpleHarmonicOscillator HNN HNN
HNN + learned symmetry + fixed SO(2) oracle True H
Data Data Data
Westartwithademonstrationonthesimplehar-
monic oscillator. This text book example has
a 2-dimensional phase space, making learned
Hamiltonians amenable to visualisation. Fur- q q q
ther, ithasaclearrotationalsymmetrySO(2),
relatingtotheconservedphase.Onafinitesetof Figure2: LearnedHamiltoniansonphasespaceof
generatedtraindata,wemodeltheHamiltonian simpleharmonicoscillatorbyHNNmodels.
using a vanilla HNN, our symmetry learning
method,andamodelwithtruesymmetrybuilt-inasreferenceoracle(experimentaldetailsinAp-
pendixB.1). InFigure2,wefindthatoursymmetrylearningmethodresultsinarotationallyinvariant
HamiltonianthatmatchesthefixedrotationalSO(2)symmetry. Furtherawayfromtheorigin,the
learnedHamiltoniandiffersfromthegroundtruthHamiltonian,asthereisnodatainthatregion.
InTable1,wefindthatthelearnedsymmetryhasabetterELBOonthetrainsetandmatchesthe
improvedpredictiveperformanceofthemodelwiththecorrectsymmetrybuilt-in. Thesymmetry
learningmethodoutperformsthevanillamodelintermsofpredictiveperformanceonthetestset.
7
p p pTable1: LearningHamiltoniandynamicsofthesimpleharmonicoscillator. Wecompareavanilla
HNN, our symmetry learning method, and a model with the correct SO(2) symmetry built-in as
referenceoracle. Ourmethodachievesreferenceoracleperformance,indicatingcorrectsymmetry
learning,andoutperformsthevanillamodelbyimprovingpredictiveperformanceonthetestset.
Learneddynamics: Traindata Testdata
simpleharmonicoscillator TrainMSE NLL/N KL/N -ELBO/N(↓) TestMSE(↓)
HNN 0.005 0.3667 3314.374 3314.741 0.005
HNN+learnedsymmetry (ours) 0.002 -2.618 3304.754 3302.136 0.002
HNN+fixedSO(2) (referenceoracle) 0.002 -3.213 3298.357 3295.144 0.002
5.3 n−HarmonicOscillators
Now,weconsidern−harmonicoscillators. Thissys-
tem has as symmetry group the unitary Lie group
U(n) of dimensionality of n2 (see Appendix B.2).
Wesamplerandomtrajectoriesfromphasespaceand
trainaHNNneuralnetworkwithoutandwithsym-
metry learning using variational inference. Again,
we find improved ELBO and test performance for
learnedsymmetriesTable2. Followingtheprotocol Figure3: Singularvalueandparallelnessof
fromSection5.1,weanalyzethelearnedsymmetries. thesingularvectorsofthelearnedgenerators,
In Figure 3 (right), we see that for varying n, we fornoscilators. U(n)iscorrectlylearned.
indeedfindthatthematrixoflearnedsymmetrieshas
n2nonzerosingularvalues. Furthermore,thefirstn2singularvectorslieinthegroundtruthsubspace
ofgeneratorswithmeasuredparallelness∥v∥∥>0.99,asseeninFigure3(left). Thisshowsthatthe
i
U(n)symmetryiscorectlylearned.
Table2: LearningHamiltoniandynamicsof3−foldharmonicoscillators. WecompareHNNwith
symmetrylearningtoavanillaHNNwithoutsymmetrylearningandtothecorrectU(3)symmetry
built-inasfixedreferenceoracle.Wefindthatourmethodcandiscoverthecorrectsymmetry,achieves
referenceoracleperformance,andoutperformsvanillatraininginbothELBOandtestperformance.
Learneddynamics: Traindata Testdata
simpleharmonicoscillator TrainMSE NLL/N KL/N -ELBO/N(↓) TestMSE(↓)
HNN 0.00106 -12.04 5.27 -6.77 0.00002141
HNN+learnedsymmetry (ours) 0.00102 -12.16 2.53 -9.63 0.00000994
HNN+fixedsymmetryU(n) (referenceoracle) 0.00102 -12.15 2.21 -9.94 0.00000898
5.4 n-BodySystem
Toinvestigateperformanceofourmethodonmore
interestingsystems,weconsiderlearningtheHamil-
tonianofann-bodysystemwithgravitationalinter-
action. We use 3 bodies in 2 dimensions so that
trajectoriesandgeneratorsremaineasytovisualise.
AstheHamiltoniandependsonlyonthenormofthe
momentaandonpositionsviatherelativedistances Figure4: Singularvalueandparallelnessof
of the bodies, the three dimensional group SE(2) thesingularvectorsofthelearnedgenerators
of rototranslations is an invariance of the ground forthreebodysystemintwodimensions. The
truth Hamiltonian. However, as explained in Ap- 7-dimensionalLiegroupG ofquadraticcon-
pendixB.3,theHamiltonianhasfourmorequadratic servedquantitiesiscorrectlylearned.
conservedquantities. Theygeneratea7-dimensional
LiegroupG ofsymmetries. ThisgrouphasthesameorbitsonthephasespaceasSE(2). Therefore,a
functionbeinginvarianttoSE(2)isequivalenttoitbeinginvarianttoG. We’llfindthatNoether’s
razordiscoversnotjustSE(2),butallsevengeneratorsofG.
8Generators associated to learned conserved quantities and their singular value decomposition. Example of rotational symmetry associated to generator G9
Figure5: Learnedgeneratorsassociatedbyconservedquantitiesandtheirsingularvaluedecomposi-
tion. Wefindasubspacespannedbythe7lineargeneratorsthatcorrespondtothecorrectsymmetries
(seeAppendixB.3): (1)rotationofthecenterofmassRCOM,(2)rotationaroundtheoriginRABS,
(4+5)translation,(5+6+7)momentum-dependenttranslationsP,Q,(8+9+10)inactive(λ<0.05).
Thefirst7singularvectorslieinthegroundtruthsubspaceofgeneratorswithmeasuredparallelness
||v||||>0.95.
i
In Table 3, we compare performance of a vanilla variational HNN with our symmetry learning
approach and a model that has the appropriate SE(2) symmetry of rototranslations built-in as an
referenceoracle. Wefindthatourmethodisabletoautomaticallydiscovertheconservedquantities
andassociatedgeneratorsthatspanthesymmetrygroup. Themodelachievesthesameperformance
asthemodelwiththesymmetrybuilt-inasreferenceoracle,butwithouthavingrequiredtheprior
knowledge. Compared to the vanilla baseline, our approach improves test accuracy on both in-
distributionasout-of-distributiontestsets.
Table3: LearningHamiltoniandynamicsof2d3-bodysystemwithvariationalHamiltonianneural
networks(HNN).Wecompareoursymmetrylearningmethodtoavanillamodelwithoutsymmetry
learningandamodelwiththecorrectSE(2)symmetrybuilt-inasareferenceoracle. Ourmethod
capableofdiscoveryingsymmetryachievestheoracleperformance,outperformingthevanillamethod.
Learneddynamics: Traindata Testdata Testdata(moved) Testdata(wider)
2d3-bodysystem TrainMSE NLL/N KL/N -ELBO/N(↓) TestMSE(↓) TestMSE(↓) TestMSE(↓)
HNN 0.0028 -13.87 13.34 -9.52 0.0016 0.0035 0.0016
HNN+learnedsymmetry (ours) 0.0017 -20.09 7.28 -12.81 0.0006 0.0004 0.0006
HNN+fixedSE(2) (referenceoracle) 0.0019 -19.27 7.96 -11.32 0.0006 0.0006 0.0006
Aftertraining,wecananalysethelearnedconservedquantitiesandimpliedsymmetriesbyinspecting
their associated generators. In Figure 5, we plot these generators as well as their singular value
decomposition. Weseethatourmethodcorrectlylearns7singularvalueswithλ > 0.05andthe
i
associatedsingularvectorslieinthegroundtruthsubspacewith∥v∥∥>0.95. Thisindicatesthatour
i
methodisinfactcapableofinferringtherightsymmetriesfromtraindata,beyondmerelyimproving
generalisationbyimprovingpredictiveperformanceonthetestset.
6 Conclusion
Inthiswork,weproposetouseNoether’stheoremtoparameterisesymmetriesinmachinelearning
modelsofdynamicalsystemsintermsofconservedquantities. Secondly,weproposetoleverage
theOccam’srazoreffectofBayesianmodelselectionbyphrasingsymmetriesimpliedbyconserved
quantitiesinthepriorandlearningthembyoptimisinganapproximatemarginallikelihooddirectlyon
traindata,whichdoesnotrequirevalidationdataorexplicitregularisationoftheconservedquantities.
Ourapproach,dubbedNoether’srazor,encouragessymmetriesbybalancingbothdatafitandmodel
complexity. Wederiveavariationallowerboundonthemarginallikelihoodprovidingaconcrete
objective capable of jointly learning the neural network as well as the conserved quantities that
symmetrisetheHamiltonian. Asfarasweknow,thisisalsothefirsttimedifferentiableBayesian
modelselectionusingvariationalinferencehasbeendemonstratedondeepneuralnetworks. We
demonstrateourapproachonn-harmonicoscillatorsandn-bodysystems. Wefindthatourmethod
learnsthecorrectconservedquantitiesbyanalysingthesingularvaluesandcorrectnessofthesubspace
spannedbythegeneratorsimpliedbylearnedconservedquantitites. Further,wefindthatourmethod
performson-parwithmodelswiththetruesymmetriesbuilt-inexplicitlyandweoutperformvanilla
model,improvinggeneralisationandpredictiveaccuraciesontestdata.
9References
Ferran Alet, Dylan Doblar, Allan Zhou, Josh Tenenbaum, Kenji Kawaguchi, and Chelsea Finn.
Noethernetworks: meta-learningusefulconservedquantities. AdvancesinNeuralInformation
ProcessingSystems,34,2021.
Jean-PierreAmietandStefanWeigert. Commensurateharmonicoscillators: Classicalsymmetries. J.
Math.Phys.,43(8):4110–4126,August2002. URLhttp://dx.doi.org/10.1063/1.1488672.
VIArnold. Mathematicalmethodsofclassicalmechanics,1989.
JohnCBaez. Gettingtothebottomofnoether’stheorem,2020. URLhttp://arxiv.org/abs/
2006.14741.
GregoryBenton,MarcFinzi,PavelIzmailov,andAndrewGordonWilson. Learninginvariancesin
neuralnetworks. arXivpreprintarXiv:2010.11882,2020.
RobertoBondesanandAustenLamacraft.Learningsymmetriesofclassicalintegrablesystems.arXiv
preprintarXiv:1906.04645,2019.
Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary
differentialequations. Advancesinneuralinformationprocessingsystems,31,2018.
TacoCohenandMaxWelling.Groupequivariantconvolutionalnetworks.InInternationalconference
onmachinelearning,pages2990–2999.PMLR,2016.
MilesCranmer,SamGreydanus,StephanHoyer,PeterBattaglia,DavidSpergel,andShirleyHo.
Lagrangianneuralnetworks. arXivpreprintarXiv:2003.04630,2020.
EkinDCubuk, BarretZoph, DandelionMane, VijayVasudevan, andQuocVLe. Autoaugment:
Learningaugmentationpoliciesfromdata. arXivpreprintarXiv:1805.09501,2018.
MarcoDavidandFlorianMéhats. Symplecticlearningforhamiltonianneuralnetworks. Journalof
ComputationalPhysics,494:112495,2023.
MarcFinzi,GregoryBenton,andAndrewGWilson. Residualpathwaypriorsforsoftequivariance
constraints. AdvancesinNeuralInformationProcessingSystems,34,2021.
SamuelGreydanus,MiskoDzamba,andJasonYosinski. Hamiltonianneuralnetworks. Advancesin
neuralinformationprocessingsystems,32,2019.
RogerGrosseandJamesMartens. Akronecker-factoredapproximatefishermatrixforconvolution
layers. InInternationalConferenceonMachineLearning,pages573–582.PMLR,2016.
BrianCHall. LieGroups, LieAlgebras, andRepresentations. SpringerInternationalPublishing,
2015. URLhttps://link.springer.com/book/10.1007/978-3-319-13467-3.
PaulRHalmos.MeasureTheory.SpringerNewYork,1950.URLhttps://link.springer.com/
book/10.1007/978-1-4684-9440-2.
IrinaHiggins,LoicMatthey,ArkaPal,ChristopherPBurgess,XavierGlorot,MatthewMBotvinick,
Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a
constrainedvariationalframework. ICLR(Poster),3,2017.
MatthewDHoffman,DavidMBlei,ChongWang,andJohnPaisley. Stochasticvariationalinference.
JournalofMachineLearningResearch,2013.
AlexanderImmer,MatthiasBauer,VincentFortuin,GunnarRätsch,andKhanMohammadEmtiyaz.
Scalable marginal likelihood estimation for model selection in deep learning. In International
ConferenceonMachineLearning,pages4563–4573.PMLR,2021.
AlexanderImmer,TychoF.A.vanderOuderaa,VincentFortuin,GunnarRätsch,andMarkvander
Wilk. Invariance learning in deep neural networks with differentiable laplace approximations,
2022.
10PatrickKidger. Onneuraldifferentialequations. arXivpreprintarXiv:2202.02435,2022.
DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. arXivpreprint
arXiv:1412.6980,2014.
ChristosLouizosandMaxWelling. Structuredandefficientvariationaldeeplearningwithmatrix
gaussianposteriors. InInternationalconferenceonmachinelearning,pages1708–1716.PMLR,
2016.
KaitlinMaile,DennisGeorgeWilson,andPatrickForré. Equivariance-awarearchitecturaloptimiza-
tionofneuralnetworks. InTheEleventhInternationalConferenceonLearningRepresentations,
2022.
CleveMolerandCharlesVanLoan. Nineteendubiouswaystocomputetheexponentialofamatrix,
twenty-fiveyearslater. SIAMreview,45(1):3–49,2003.
SethNabarro,StoilGanev,AdriàGarriga-Alonso,VincentFortuin,MarkvanderWilk,andLaurence
Aitchison. Data augmentation in bayesian neural networks and the cold posterior effect. In
UncertaintyinArtificialIntelligence,pages1434–1444.PMLR,2022.
E.Noether. Invariantevariationsprobleme. KöniglichGesellschaftderWissenschaftenGöttingen
NachrichtenMathematik-physikKlasse,1918.
CarlRasmussenandZoubinGhahramani.Occam’srazor.Advancesinneuralinformationprocessing
systems,13,2000.
MagnusRossandMarkusHeinonen. Learningenergyconservingdynamicsefficientlywithhamilto-
niangaussianprocesses. arXivpreprintarXiv:2303.01925,2023.
PolaSchwöbel,MartinJørgensen,SebastianWOber,andMarkvanderWilk. Lastlayermarginal
likelihoodforinvariancelearning. arXivpreprintarXiv:2106.07512,2021.
PolaSchwöbel,MartinJørgensen,SebastianWOber,andMarkVanDerWilk. Lastlayermarginal
likelihood for invariance learning. In International Conference on Artificial Intelligence and
Statistics,pages3542–3555.PMLR,2022.
AdrianFMSmithandDavidJSpiegelhalter. Bayesfactorsandchoicecriteriaforlinearmodels.
JournaloftheRoyalStatisticalSociety: SeriesB(Methodological),42(2):213–220,1980.
YusukeTanaka,TomoharuIwata,etal. Symplecticspectrumgaussianprocesses: Learninghamil-
tonians from noisy and sparse data. Advances in Neural Information Processing Systems, 35:
20795–20808,2022.
TychovanderOuderaa,AlexanderImmer,andMarkvanderWilk.Learninglayer-wiseequivariances
automaticallyusinggradients. AdvancesinNeuralInformationProcessingSystems,36,2024.
TychoFAvanderOuderaaandMarkvanderWilk. Learninginvariantweightsinneuralnetworks.
InWorkshopinUncertainty&RobustnessinDeepLearning,ICML,2021.
Tycho FA van der Ouderaa, David W Romero, and Mark van der Wilk. Relaxing equivariance
constraintswithnon-stationarycontinuousfilters. arXivpreprintarXiv:2204.07178,2022.
MarkvanderWilk,MatthiasBauer,STJohn,andJamesHensman. Learninginvariancesusingthe
marginallikelihood. arXivpreprintarXiv:1808.05563,2018.
YaofengDesmondZhong, BiswadipDey, andAmitChakraborty. Symplecticode-net: Learning
hamiltoniandynamicswithcontrol. arXivpreprintarXiv:1909.12077,2019.
Allan Zhou, Tom Knowles, and Chelsea Finn. Meta-learning symmetries by reparameterization.
arXivpreprintarXiv:2007.02933,2020.
11A Mathematicalderivations
A.1 ELBOofHamiltonianNeuralNetwork
Wecanfindalowerboundonthemarginallikelihoodlogp(x|η)throughvariationalinference,
logp(x|η)≥E [logp(x|θ,η)]−KL(q (θ)||p(θ)) (9)
θ m,S
(cid:20) (cid:21)
(cid:88)N
=E logN(xi |H (xi),σ2 I) −KL(q (θ)||p(θ |0,σ2 I))
θ t′ θ,η t data m,S prior
i=1
(10)
(cid:20) (cid:88)N (cid:104) (cid:105) (cid:21)
=E
θ
logN(xi
t′
|E
τ
H(cid:98) θτ ,η(xi t) ,σ d2 ataI) −KL(q m,S(θ)||p(θ |0,σ p2 riorI))
i=1
(11)
(cid:20) (cid:20) (cid:21)(cid:21)
(cid:88)N
≥E
θ
E
τ
logN(xi
t′
|H(cid:98) θτ ,η(xi t),σ d2 ataI) −KL(q m,S(θ)||p(θ |0,σ p2 riorI))
i=1
(12)
M (cid:34) S (cid:35)
≈ 1 (cid:88) (cid:88)N logN(xi | 1 (cid:88) H (Φτ(s) (xi)),σ2 I) −KL(q (θ)||p(θ |0,σ2 I))
M i=1 t′ S θ,η Cη t data m,S prior
i=1 s=1
(13)
whereweuseM samplestoobtainanunbiasedestimateofE := E ∼q andasinglesample
θ θ m,S
E τ =E τ∼(cid:81)S p(τ)anduseanS-sampledMonteCarloestimateofthesymmetrisedHamiltonian:
s=1
S
H(cid:98)θ,η(xi t)= S1 (cid:88) H θ,η(Φτ Cη(s) (xi t))withsamplesτ(1),τ(2),...,τ(S) ∼µ(τ) (14)
s=1
with the fact that this yields an unbiased estimator of the true symmetrised Hamiltonian
(cid:104) (cid:105)
E
τ
H(cid:98)θ,η(·) = H(cid:98)θ,η(·). where we obtained an unbiased estimate of expectations through S
sampledsymmetrytransformationsandM sampledparameters. Thefirstinequalityisthestandard
VIlowerbound. ThesecondinequalityfollowsfromapplyingJensen’sinequality(again),usingthe
factthattheloglikelihoodisconvex. Similarboundstosymmetrisationbyaveragingoverorbitshave
appearedinpriorwork[vanderOuderaaandvanderWilk,2021,Schwöbeletal.,2022,Nabarro
etal.,2022].
B Groundtruthconservedquantities
Inthissection,we’lldiscusstheconservedquantitiespresentintheground-truthHamiltoniansofthe
systemswediscuss.
AsstatedinSection2.3,wecancombineconservedquantitiesintonewonesbylinearcombinations,
products,andPoissonbrackets. Thuswe’llspeakofthegeneratingsetofconservedquantities,which
combineintoallconservedquantities.
B.1 Simpleharmonicoscillator
Forthesimpleharmonicoscillator,thephasespaceisR×R. ThegroundtruthHamiltonianis
p2 kq2
H(p,q)= + .
2m 2
Wechoosem=k =1,sothatH(p,q)=(p2+q2)/2. Timeevolutionisarotationofphasespace.
TheHamiltonianitselfgeneratesallconservedquantities.
12B.2 n−Simpleharmonicoscillators
ThephasespaceisR2n. Wechooseallk =m=1,sothattheHamiltonianis
H(p,q)=∥p∥2/2+∥q∥2/2.
Timeevolutionrotateseachpair(q ,p ). Theconservedquantitiesaregeneratedbythefollowingset,
i i
fori,j =1,...,n,i̸=j.
H =(q2+p2)/2
i i i
R =q p −q p
ij i j j i
F =q q +p p
ij i j i j
TheconservedquantityH rotatesthepair(q ,p ). R rotatesbothpairs(q ,q )and(p ,p )and
i i i ij i j i j
F rotatesbothpairs(q ,p )and(q ,p ).
ij i j j i
Alternatively,wecaninterpretthephasespaceasCn[Arnold,1989,Sec. 41E],withthepositions
beingtherealpartandthemomentatheimaginarypart. Inthatcase,thesymplecticformJ becomes
simply the complex number −i. Then H(x) = x†x/2. Time evolution is multiplication by the
complexnumbere−it. ConservedquantitiesareH =x∗x /2,whichisreal,andC =x∗x ,whose
i i i ij i j
realpartcorrespondstoF andimaginaryparttoR . TheconservedquantitiesH andC are
ij ij i ij
quadraticandthustheirsymmetriesaregeneratedbylinearmatrices,whichareallskew-Hermitian.
Infact,allskew-Hermitianmatricesarespannedbythesegenerators. Thisshowsthatthecombined
symmetrygroupisinfactU(n)[AmietandWeigert,2002].
B.3 n-body
InDspatialdimensions,withnbodies,thephasespaceisR2nD andtheHamiltonianis
H(p,q)=(cid:88)∥p i∥2 +(cid:88) Gm im j
(cid:112)
i
2m i
i̸=j
∥q i−q j∥2+ϵ2
withasmallϵtomakeitsmooth.
Themainconservedquantitiesare,ford,d′ =1,...D,d̸=d′,
(cid:88)
T = p
d id
i
(cid:88)
RABS = (q ∧p )
dd′ i i dd′
i
RCOM =(q ∧p )
dd′ COM COM dd′
(cid:80) (cid:80) (cid:80) (cid:80)
withq = m q / m andp = m p / m .
COM i i i i i COM i i i i i
Thesegeneratefurtherconservedquantitiesofinterest:
P =T2
d d
Q =T T
dd′ d d′
(cid:88)
RREL = ((q −q )∧(p −p ) =RABS−2RCOM
dd′ i COM i COM dd′ dd′ dd′
i
AsT islinear,wecanstilllearnP andQ asquadraticconservedquantities. AstheRREL is
d d dd′
alinearcombinationofotherconservedquantities, wedisregarditinouranalysisofthelearned
symmetries.
Thecorrespondingsymmetriesare: T translatesallbodiesintheddirection. RCOM rotatesthe
d dd′
centerofmassofallthebodiesintheplanedd′,whilepreservingthepositionsrelativetothecenter
ofmass. RABSrotatesallbodiesrelativetotheorigin. RRELrotatesallbodiesrelativetothecenter
ofmass. P translatesinthe ddirectionproportionalto itsCOMmomentum. Q translatesin
d dd′
directiondproportionaltop andviceversa.
COM,d′
13Thesesymmetriestogethergenerateagroupwe’llcallG. ThisgrouphasasasubgroupSE(D),which
isgeneratedbyT andRABS. ThegroupG hasthesameorbitsasSE(D),aseachelementinG can
beseenasarototranslationconditionalonsomepropertyofphasespace. Becausetheorbitsare
thesame,foranyobservablef : M → R,wehavethatf invarianttoG istrueifandonlyiff is
invarianttoSE(2).
Thissystemcanhavefurtherconservedquantities,suchastheLaplace–Runge–Lenzvectorforn=2.
However, thesearenotexpressibleasaquadraticpolynomial. Asfarasweknow, theconserved
quantitieslistedaboveareallthatareexpressibleasaquadraticpolynomial.
C Experimentaldetails
AllexperimentswererunonasingleNVIDIARTX4090GPUwith24GiBofGPUmemory.
C.1 Simpleharmonicoscillatorexperiment
Data Forthetrainingdata,wesampled7initialconditionsfromunitGaussianandsimulated4
datapointswith∆t=0.2apart. Fortestdata,wesampled100initialconditionsfromunitGaussian
andsimulated20timestepswith∆t=0.2fromeachinitialcondition.
Training WeuseanMLPwith2hiddenlayers,eachconsistingof200hiddenneuronsandalinear
exponentialunitactivationfunctionwithα=2. Forsymmetrisation,weuseS =200samplesfroma
uniformmeasureforµ(τ). Weuse20Eulerstepsfortimeintegration. Weusefixedoutputnoiseand
closed-formpriorvariance(AppendixD).WeoptimisetheELBOinfullbatchwithAdam[Kingma
andBa,2014](β =0.9,β =0.999)trainedfor2000epochswithalearningrateof0.001,cosine
1 2
annealedto0.
C.2 n−Simpleharmonicoscillatorsexperiment
Data For training data, we randomly sampled 200 initial conditions independently from a unit
normal. Foreachinitialcondition,wesimulatedatrajectoryconsistingof50datapointsat0.3time
unitsapart.
Training We use an MLP with 3 hidden layers with 200 hidden units and exponential linear
activationfunctionswithα=1. WeoptimisetheELBOinminibatchesofB =20trajectories,using
S =100symmetrisationsamples,20Eulerstepsfortimeintegration,andM =2weightsamples
usingAdam[KingmaandBa,2014](β = 0.9,β = 0.999)for2000epochswithalearningrate
1 2
startingfrom0.001,cosineannealedto0.
C.3 n-bodyexperiment
Data Fortrainingdata,werandomlysampled200initialconditionsbyindependentlysampling
positionsfromaunitnormal,shiftedbyanormalwithastandarddeviationof3. Fromeachinitial
condition,wesimulatedtrajectoriesconsistingof50datapoints0.3timeunitsapart.
Training WeuseanMLPwith4hiddenlayerswith250hiddenunitunitsandexponentiallinear
unitactivationfunctionswithα=1.WeoptimisetheELBObatchesofB =20trajectories,S =100
symmetrisationsamples,20Eulerstepsfortimeintegration,andM =2weightsamplesusingAdam
[KingmaandBa,2014](β =0.9,β =0.999)for2000epochswithalearningratestartingfrom
1 2
0.001,cosineannealedto0.
14D Ontraininganeuralnetworkwithvariationalinference
D.1 Matrixnormalvariationalposterior
Naively,thecovarianceofaGaussianposterioroverweightsgrowsquadraticallywiththenumber
ofparameters|θ|2. Itisthereforecommontodisregardallcorrelationsbetweenweights,resulting
inadiagonalormean-fieldposterior. AlthoughtheELBOremainsalowerboundforanychoiceof
approximatefamily,morecrudeapproximationscanincreasetheslackinthebound,possiblymaking
ithardertouseestimatesforBayesianmodelselection. We,therefore,proposetousematrixnormal
posteriors[LouizosandWelling,2016]factorisedperlayer,
N
(cid:89)
q(θ)= q(θ ),with q(θ )=N(θ |,m ,S ⊗A ) (15)
l l l l l l
i=1
where θ = (θ ,θ ,...,θ ) denote the weights of each layer l. If we denote the parameters
1 2 L
in terms of weight and bias matrices of each layer with in input and out output dimensions,
l l
vec(θ l)=[W
l
b l]∈Routl×(inl+1),wecanequivalentlywritethisposteriorasafactorisedmatrix
normaldistribution:
N
(cid:89)
q(θ)= q(θ ),with q(θ )=MN([W b ]|,M ,S ⊗A ) (16)
l l l l l l l
i=1
The variational parameters {W ,S ,A } provide the mean M as well as correlations between
l l l l
layerinputsandbiasA
l
∈ R(inl+1),(inl+1) andlayeroutputsS
l
∈ Routl,outl. ForLhiddenlayers
ofwidthH, thenumberofvariationalparametersscalesquadraticallyO(LH2)comparedtothe
quarticnumberofvariationalparametersO(LH4)wewouldneedtorepresentthefullcovariance.
Thisstrikesapracticalbalancebetweentakingimportantcorrelationsintoaccountwhileavoiding
having to make a mean-field assumption. Further, we note that the matrix gaussian posterior of
[LouizosandWelling,2016]isthesameapproximatedistributionasusedinKronecker-factored
Laplaceapproximations[GrosseandMartens,2016]. InLaplaceapproximationsthecovarianceis
theinverseHessian,whereasinvariationalinferencethecovarianceisoptimisedusingtheELBO.
Layer-factoredmatrixnormaldistributionshavebeensuccesfullyappliedtoperformapproximate
BayesianmodelselectionbasedontheLaplaceapproximationin[Immeretal.,2021,2022,vander
Ouderaaetal.,2024]. Thisworkprovidesevidencethatvariationalinferencecanalsobeusedto
obtainapproximateposteriorsofthisformandobtainalowerboundonthemarginallikelihoodthat
issufficientlytighttoperformBayesianmodelselectionindeepneuralnetworks.
D.2 Closed-formoutputvariance
Itcanbeshownthattheoutputvariancethatmaximisesthemarginallikelihoodσˆ2 istheempirical
data
varianceoftheoutput. We,therefore,eitherfixingtheoutputvarianceσˆ2 -typicallytoaverysmall
data
numberinnoise-freesettings, orsettingtheoutputvariance toanempiricaloutputvariance. An
exponentiallyweightedaverageoftheempiricalvarianceovermini-batchescanbeused.
On downscaling the KL term by a β−scalar Many deep learning papers that use variational
inferencedown-scaletheKLtermbyaβ−parameter[Higginsetal.,2017].Wenotethat,forstandard
Gaussianlikelihoods,scalingtheoutputvarianceisequivalenttoinverselyscalingtheKLterm. We
doadviceagainstdownscalingoftheKLterm,asitmakesitlessclearthattheresultingobjectiveis
stillalowerboundtothemarginallikelihood,andhidesthefactthatthelowerboundcorrespondsto
achangedmodelwithalteredoutputvariance. InMAPestimationunderaGaussianlikelihood,the
outputvarianceisarbitraryasitonlyscalestheobjectivenoteffectingtheoptimum,andtheobjective
isoftensimplifiedasthemeansquarederror. Invariationalinference,theoutputvariancedoesplay
animportantroleofbalancingtherelativeimportancebetweentheloglikelihood(datafit)andKL
term(pulltoprior). Inthissetting,usinghalfmeansquarederroreffectivelycorrespondstoanoutput
varianceofσ2 = 1. Inpractice,thisvalueisoftentoohighbecausecommonmachinelearning
data
datasetshavelittlelabelnoise. Asaresult,theloglikelihoodtermistooweakandtheKLtermistoo
strong. Wehypothesisethatthishasledtopractitionerstodown-weightingtheKLtermtoobtain
sensibleposteriorpredictions,withoutnecessarilyrealisingthattheywereeffectivelyalteringthe
outputvarianceofthemodel. Usingautomaticoutputvariance,theoptimalβˆcanbesetto(arunning
estimateof)theinverseoftheempiricalvariance,alsoknownastheempiricalpriorprecision.
15D.3 Closed-formpriorvarianceminimisinginverseKL
ConsiderthesettingofaD-dimensionalGaussianq,parameterisedbymeanmandcovarianceS,
andazeromeanGaussianp withscalarvariancevineachoftheequallymanydimensions:
v
q =N(m,S), p =N(0,vI)
v
where0denotesazerovectorandI anidentitymatrix. Astheloglikelihooddoesnotdependonv,
wecanfindvthatoptimisesthemarginallikelihoodbyfindingtheminimiseroftheinverseKL:
(cid:20) (cid:21)
1 |vI|
argminKL[q||p ]=argmin log −D+Tr((vI)−1S)+(0−m)T(vI)−1(0−m)
v 2 |S|
v v
(cid:20) (cid:21)
1 |vI|
=argmin log −D+Tr(S)/v+mTm/v
2 |S|
v
=argmin(cid:2) Dlog(v)+Tr(S)/v+mTm/v(cid:3)
v
Settingthederivativetozero:
(cid:20) (cid:21)
∂ 1
0= Dlog(v)+ Tr(S)+mTm/v
∂v v
−Dv+Tr(S)+mTm
0=−
v2
Tr(S)+mTm
v = (17)
∗ D
WefoundKL-minimisingvariancev inclosed-formasafunctionofmandS. Verifiednumerically.
∗
D.4 PluggingminimisingvarianceintoKL
Pluggingv backintotheGaussianp andcomputingtheKL:
∗ v∗
(cid:20) (cid:21)
1 |v I|
KL[q||p ]= log ∗ −D+Tr((v I)−1S)+(0−m)T(v I)−1(0−m)
v∗ 2 |S| ∗ ∗
1(cid:20) |v I| DTr(S) DmTm (cid:21) 1(cid:20) |v I|(cid:21)
= log ∗ −D+ + = log ∗
2 |S| Tr(S)+mTm Tr(S)+mTm 2 |S|
1(cid:20) (cid:18) Tr(S)+mTm(cid:19) (cid:21)
= Dlog −log|S|
2 D
showsthattheresultingKLisonlymeasuringtherelativevolume 1log |q| betweenthepriorandthe
2 |p|
posterior,whichcanbefurthersimplifiedas
KL[q||p ]=
1(cid:2) Dlog(Tr(S)+mTm)−Dlog(D)−log|S|(cid:3)
v∗ 2
Inpractice,wemightreparameteriseS =LLT intermsofitstriangularCholeskyfactorLanduse
(cid:89) (cid:88)
log|S|=log|LLT|=log|L|2 =2log|L|=2log L =2 logL
ii ii
i i
(cid:88)
Tr(S)=Tr(LLT)= L2
ij
i,j
ThisgivesthefinalexpressionoftheKL
   
1 (cid:88) (cid:88) (cid:88)
KL[q||p v∗]= 2Dlog L2 ij + m2 i−Dlog(D)−2 logL ii (18)
i,j i i
whichimplicitlyusesthederivedoptimalvariancev =
(cid:80) i,jL2 ij+(cid:80) imi.
∗ D
E CodeandQuestions
Thecodeisavailableathttps://github.com/tychovdo/noethers-razor.
Foranyquestions,pleasecontactthecorrespondingauthor,TychovanderOuderaa,byemail.
16