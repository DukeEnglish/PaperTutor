BENCHMARKING AGENTIC WORKFLOW GENERATION
ShuofeiQiao♠∗,RunnanFang♠∗,ZhisongQiu♠∗,XiaobinWang♢,
NingyuZhang♠†,YongJiang♢†,PengjunXie♢,FeiHuang♢,HuajunChen♠†
♠ZhejiangUniversity ♢AlibabaGroup
{shuofei,zhangningyu}@zju.edu.cn
ABSTRACT
LargeLanguageModels(LLMs),withtheirexceptionalabilitytohandleawide
range of tasks, have driven significant advancements in tackling reasoning and
planningtasks,whereindecomposingcomplexproblemsintoexecutableworkflows
isacrucialstepinthisprocess. Existingworkflowevaluationframeworkseither
focussolelyonholisticperformanceorsufferfromlimitationssuchasrestricted
scenariocoverage,simplisticworkflowstructures,andlaxevaluationstandards. To
thisend,weintroduceWORFBENCH,aunifiedworkflowgenerationbenchmark
withmulti-facetedscenariosandintricategraphworkflowstructures. Additionally,
wepresentWORFEVAL,asystemicevaluationprotocolutilizingsubsequenceand
subgraphmatchingalgorithmstoaccuratelyquantifytheLLMagent’sworkflow
generationcapabilities. Throughcomprehensiveevaluationsacrossdifferenttypes
ofLLMs,wediscoverdistinctgapsbetweenthesequenceplanningcapabilities
and graph planning capabilities of LLM agents, with even GPT-4 exhibiting a
gap of around 15%. We also train two open-source models and evaluate their
generalizationabilitiesonheld-outtasks. Furthermore,weobservethatthegener-
atedworkflowscanenhancedownstreamtasks,enablingthemtoachievesuperior
performancewithlesstimeduringinference1.
“Ifyoucan’tdescribewhatyouaredoingasaprocess,youdon’tknowwhatyou’redoing.”
—W.EdwardsDeming
1 INTRODUCTION
TheremarkableadvancesinLargeLanguageModels(LLMs)(Dubeyetal.,2024;Yangetal.,2024a;
OpenAI,2024)aregraduallyrecoveringtheconsiderablepotentialofLLM-drivenagentstowards
tacklingcomplexreal-worldproblems,suchasfunctioncalls(Qinetal.,2024;Tangetal.,2023;
Quetal.,2024;Liuetal.,2024a),embodiedplanning(Songetal.,2023;Zengetal.,2024a;Xiang
etal.,2023;Songetal.,2024;Qiaoetal.,2024a),codegeneration(Hongetal.,2024;Qianetal.,
2023;Zhangetal.,2024b),etc.,whereindecomposingcomplexproblemsintoexecutable-granularity
subtasksisacrucialcapabilitythatLLMagentsmustpossesstoachievepracticallydeployable.
A set of subtasks with execution dependencies is
Check if the emails admin@site.com and manager @site.com are disposable and
typicallyreferredtoasaworkflow. Workflowscan get their MX records.
serve as an intermediate state for solving complex Planning Steps:
1: Check if the email admin@site.com is disposable
tasks, aiding agents in bridging the gap between 2: Check if the email manager@site.com is disposable
3: Get the MX records for the email admin@site.com
tasks and specific executable actions (Wang et al., 4: Get the MX records for the email manager@site.com.
Linear Workflow Graph Workflow
2024c). Moreover,anexplicitworkflowcanenhance
1 3
theagent’sdebuggabilityandinterpretability,facil- START 1 2 3 4 END START END
itatinghuman-machineinteraction. Manyprevious 2 4
Sequential Execution Inefficient Parallel Execution Efficient
studieshaveexaminedthepotentialofLLMsforau-
tomaticworkflowgeneration(Zengetal.,2023;Ye The Application of Workflow
Chain-of-Thought Augmentation Structured Prior Knowledge
etal.,2023;Xueetal.,2024;Lietal.,2024d).Recent Parallel Planning Steps Shorten Planning Steps
researchersintherealmofLLMagentsemphasize Figure1: Workflowanditsapplication.
workflows as a form of prior knowledge or experi-
ence,guidingagentplanningtoavoidtheoccurrenceofhallucinations(Zhuetal.,2024;Xiaoetal.,
2024;Wangetal.,2024c). Asforbenchmarksinthisfield,mostworksareconfinedtotheend-to-end
∗ EqualContribution.
† CorrespondingAuthor.
1Codeanddatasetwillbeavailableathttps://github.com/zjunlp/WorFBench.
1
4202
tcO
01
]LC.sc[
1v96870.0142:viXraplanningabilityofLLMagents(Qinetal.,2024;Patiletal.,2023;Yangetal.,2023b;Wuetal.,2024;
Liuetal.,2024b;c),whileonlyafewstudiesattempttoevaluatetheworkflowgeneration(problem
decomposition)capabilityfromafinergranularityperspective(Shenetal.,2023b;Chenetal.,2024;
Yeetal.,2024;Zhengetal.,2024a). However,thecurrentevaluationbenchmarksunavoidablysuffer
fromthefollowingissues: 1)Limitedscopeofscenarios. Theyonlyfocusonfunctioncalltasks.
2)Soleemphasisonlinearrelationshipsbetweensubtasks(Figure1left). Real-worldscenarios
ofteninvolvemorecomplexgraphstructures,includingparallelism. 3)Evaluationsheavilyrelyon
GPT-3.5/4,yetLLMthemselvesexhibithallucinationsandambiguity. Theselimitationsmakethe
evaluationofworkflowgenerationlacksomesystematicity. Therefore,toaddresstheaboveissues,
weintroduceWORFBENCH,aunifiedworkflowgenerationbenchmarkwiththefollowingfeatures:
• Multi-facetedscenarios. WecoverfourcomplexscenariosforLLMagents,includingproblem-
solving,functioncalling,embodiedplanning,andopen-groundedplanning. Thedatasetcomprises
18ktrainingsamples,2146testsamples,and723held-outtaskstoevaluategeneralization.
• Complexworkflowstructures. WemodeltheworkflowsasDirectedAcyclicGraphsbasedon
dependenciesbetweensubtasks,enablingamorepreciserepresentationofcomplicatedserialor
parallelstructuresintherealworld(Figure1right).
• Strictqualitycontrolanddatafiltering.Weintroduceanintermediarystructurecallednodechain
betweentheoriginaltaskandtheworkflowgraphandemploytheTopologicalSortingalgorithm
forrigorousvalidationofthegraphstructurefollowedbyhumanevaluation.
• Accuratequantitativeevaluation. WefurtherproposeWORFEVALtoevaluatetheworkflow
generationabilityofLLMagents,whichappliesdevelopedsubsequenceandsubgraphmatching
algorithmsforaccuratequantitativeassessment.
WeevaluatetheperformanceofmostmainstreamLLMswithvariousmodelscalesonWORFBENCH.
We observe that, compared to linear structures, the models’ ability to predict graph-structured
workflowsfallsfarshortofreal-worldrequirements,withevenGPT-4(OpenAI,2023)onlyachieving
aperformanceof52.47%. Furthermore,wetraintwoopen-sourcedmodelsonthetrainingsetand
evaluate their generalization abilities on held-out tasks. Finally, we analyze how workflows can
enhanceend-to-endmodelperformanceasCoT(Weietal.,2022)augmentationandpriorknowledge,
reducingend-to-endinferencetimebyparallelingandshorteningplanningsteps(Figure1bottom).
Tosumup,wesummarizeourmaincontributionsasfollows:
• WeproposeWORFBENCH,aunifiedworkflowgenerationbenchmarkwithmulti-facetedscenarios
andcomplexworkflowstructures. Weconductstrictdatafilteringandhumanevaluationtoensure
thequalityofWORFBENCH.
• We introduce WORFEVAL, using effective subsequence and subgraph matching algorithms to
evaluatetheworkflowgenerationabilityofLLMagentsfrombothchainandgraphstructures.
• Weconductcomprehensiveevaluationonvariousclosed-sourcedandopen-sourcedmodelswith
differentscales. Wefurtherexploitthegeneratedworkflowstofacilitatedownstreamtasksand
achievesuperiorandefficientperformance.
2 WORFBENCH
2.1 TASKFORMULATION
Givenaspecifictaskandacandidateactionlist,ourgoalistoenablethelanguageagentstogenerate
agraph-structuredworkflow,wherethenodesintheworkflowsatisfytheminimumexecutablegranu-
larity. OuractionlistherecanincludefunctionAPIs,tools,andembodiedactionstosimultaneously
adaptvariousscenarios. Formally,giventhetaskdescriptionq,actionlistAandlanguageagents
M ,theworkflowgenerationcanbemodeledas:
θ
G(V,E)←M (q,A), (1)
θ
whereG isaDirectedAcyclicGraph(DAG)withnodesV ={v ,v ,...,v }beingsubtasksand
1 2 |V|
edges E = {(v ,v )},1 ≤ i ̸= j ≤ n representing the execution relationships between nodes
i j
(v must be executed after v ). Note that all nodes V need to be executed according to their
j i
2Sector 1: Benchmark Construction Sector 2: Quality Control
Task Description Action List
“How to pack a carry on bag full of fun for a … 1 4
long plane ride?”(Open-Grounded)
““ WP hu ot a is c tl he ea n p rs eo sa ep n b t a for o in d t moi il ne it s. t” er( E om f tb ho ed sie tad t) e Retrieve 2 3 5
where Kankumbi is located?”(Problem-Solving)
“What is the precipitation forecast for the next Action List Topological Sort
7 days in Seattle?”(Function Call) Sorted (small number first)
GPT-4 Workflow Graph 1 3 … 4 5
GPT-4 1 4 Retrieved
Node Chain Gold
1 2 3 4 5 3 2 5 Gold discard 1 2 … 4 5
Sector 3: WorFEval Sector 4: A Detailed Case in WorFBench
Input
(numbered by index) Gold Graph Task:
1 2 3 4 5 Topological Check if the emails admin@site.com and manager@site.com are disposable and
Sort get their MX records.
1 2 3 4 5 The action list you can select from:
{name: check_for_disposable_emails, description: …, parameters: …}
{name: validate_email, description: …, parameters: …}
1 2 3 4 5 {name: get_the_mx_records, description: …, parameters: …}
Maximum Common {name: normalize, description: …, parameters: …}
1 2 3 4 5 Subgraph Matching Output
Predicted Graph Node:
Longest Increasing 1: Check if the email admin@site.com is disposable. 1
Subsequence Matching 2: Check if the email manager@site.com is disposable. 2
Predicted Node Chain 3: Get the MX records for the email admin@site.com. 3
1 3 2 4 5 4 E: d G ge et : the MX records for the email manager@site.com. 4
(START,1) (START,2) (1,3) (2,4) (3,END) (4,END) 1 3
2 4
Figure2: TheoverviewframeworkofourWORFBENCH.Sector1isthebenchmarkconstructionwhere
wefirstsynthesizethenodechainandthentheworkflowgraph(§2.2). Sector2isourdatafilteringprocess
(§2.3). Sector3describesthealgorithmsinWORFEVALtoevaluatethepredictedworkflowofLLMagents
(§2.4). Sector4isadetaileddatapointofourWORFBENCH. Notethateachnodeinthisfigureisuniquely
identifiedbyitscolor.Numbersonthenodesrepresenttheirindexesinthegoldchain.Nodesmatchedwithgold
chainorgrapharecircledby inSector3.
dependenciesbeforetaskqisconsideredcompleted. Itisevidentlyabitchallengingtodirectly
instructthelanguageagentstogenerateagraphstructure. Toaccommodatethegenerationhabitsof
languagemodels,weintroducethenodechainC(V)beforegraphG(V,E):
G(V,E)←C(V)←M (q,A). (2)
θ
HerenodechainC isoneoftheTopologicalSequencesofgraphG,wheretheorderofnodesinthe
nodechainensurestherelativeorderofnodesinthegraph. Thus,theworkflowgraphgenerationcan
bereformulatedbyfirstcreatinganodechainandthenestablishingedgesforthegeneratednodes.
2.2 BENCHMARKCONSTRUCTION
Wemainlycollectvarioustasksq andthecorrespondingactionlistsAfromexistingwell-known
datasets. Tofacilitateabetterunderstandingofthebenchmarkconstruction,weprovideadetailed
expositionofeachdatasetutilizedinourpaperinAppendixA.1. Toensurethequalityofthedata,
wealsoadheretothestrategyoffirstconstructingthenodechainandthenbuildingtheworkflow
graphduringbenchmarkconstruction. Dependingonthevariedwaysofacquiringworkflownodesin
differentscenarios,wecategorizenodechainC constructionintothethreefollowingtypes:
FunctionCallTasks. OurfunctioncalldataiscollectedfromToolBench(Qinetal.,2024)and
ToolAlpaca(Tangetal.,2023),bothofwhichinvolvecombiningdifferentfunctionstoaccomplish
multi-stepusertasks. Asthetwodatasetsonlyhavegoldenfunctioncallsforeachtask,tomakethe
synthesizednodechainmorereasonable,followingthethought-action-observation REACT (Yao
et al., 2023b) format, we first utilize GPT-4 to reverse-engineer the thought based on the given
functioncall(action)andthenexecutethefunctioncalltoobtaintheobservation. Next,wecarefully
design the few-shot prompt for GPT-4 to generate the node (subtask) for each step based on its
thought-action-observationloop. Byiteratingthroughallthefunctioncallsteps,wecanobtainthe
nodechainforthespecifictask. Inaddition, wealsoincludeSeal-Tools(Wuetal.,2024)asthe
held-outtasks,wheretheconstructionofthenodechainfollowsthesameprocessasdescribedabove.
EmbodiedTasks. WecollecttheREACTformatgoldtrajectoriesofALFWorld(Shridharetal.,
2021)andWebShop(Yaoetal.,2022)fromETO(Songetal.,2024)andOS(Liuetal.,2024b)from
AgentInstruct(Zengetal.,2024a). Unlikefunctioncalltasks,whereeachfunctioncallcorresponds
toanode,embodiedscenariosevolvedynamicallybasedontheenvironment. Itishardtodecompose
tasks into a one-node-per-action granularity solely based on the initial environment. Therefore,
wecanonlydecomposetasksintoafixedgranularitybasedonthetaskandinitialenvironmental
informationavailable. However,theadvantageofembodiedtasksisthattheworkflowsofthesame
kindoftasksaresimilar. Soforeachkindoftask,weseriouslyanalyzeandmanuallydesignfew-shot
3examples,enablingGPT-4tosynthesizenodechainsdirectlybasedonthegoldtrajectories. Similarly,
wealsoincludeInterCodeSQL(Yangetal.,2023a)asheld-outtasksforembodiedscenarios.
Problem-Solvingand Open-GroundedTasks. Wealsointroduceproblem-solvingtasks
likemath,commonsense,andmultimodalreasoningtasksfromLUMOS(Yinetal.,2024),aswell
as a challenging general open-grounded dataset, WikiHow (Koupaee & Wang, 2018). Since the
LUMOS-OversionofLUMOSandtheWikiHowhavealreadycontainedgoldplanningchains,we
directly process them into the unified format we require. Furthermore, since WikiHow does not
providecandidateactionlists,weusethespecifictaskasthequerytoretrievethemostsimilaractions
inthepublicactionlibraryasdistractorsandthenmixthedistractorswiththegoldactionstocreate
theactionlist. Theaimofretrievingsimilaractionsistoincreasethedifficultyofthetask.
AfterobtainingthenodechainC(V), wefurtheruseGPT-4togenerateedgesforthenodechain.
Sinceeachnodeisapieceoftextdescribingasubtask,itisformallydifficulttoaddedgestothetext
directly. Thus,wesequentiallyassignnumberstothenodesinthenodechainandusethesenumbers
insteadofthenodetextwhengeneratingedges(e.g. (i,j)insteadof(v ,v )). Allthegenerated
i j
edgesfinallyformG(V,E). Foreaseofevaluation,weaddtheSTARTandENDnodestorepresent
thebeginningandendofaworkflow. AdetaileddatapointisillustratedinFigure2Sector4. Allthe
detailedpromptsweuseduringthebenchmarkconstructionprocesscanbefoundinAppendixA.6.
2.3 QUALITYCONTROL
AlthoughweuseGPT-4tosynthesizeworkflows,rationalitystillcannotbeguaranteed.Soweconduct
restrictedqualitycontrolsanddatafilteringforbothnodechainandworkflowgraphgeneration.
QualityControlforNodeChain. Duetothelowvariabilityofthenodechainsinembodiedtasks,
our primary focus lies in filtering the node chain of function call tasks. Two critical factors are
whethertheorderofnodesislogicalandwhethereachnodeaccuratelydecomposesthetask. The
formerhasbeenguaranteedwhenconstructingthenodechainbasedonthesequenceofgoldfunction
calls. Forthelatter,weuseeachsynthesizednodeasaquerytoretrievethefunctionlist. Ifonenode
withinthenodechainretrievesafunctionthatdoesnotalignwiththegold,wewilldiscardthistask.
Wefilterout15.36%datathroughthequalitycontrolforthenodechain.
QualityControlforWorkflowGraph. Thekeypriorityistoensurethattheorderofnodesin
theworkflowgraphisconsistentwiththeorderofnodesinthenodechain. Therefore,weperform
aTopologicalSortonthegraphgeneratedbyGPT-4. Sincethetopologicalsortofagraphisnot
unique, toensureuniquenessandfacilitatematchingwiththenodechain, duringthetopological
sortingprocess,whentherearemultiplenodeswithanin-degreeof0inthegraph,wesequentially
removetheminascendingorderbasedontheirnodenumberdefinedinnodechain. Wediscardthe
datapointswithtopologicalsortingresultsnotalignedwithnodechains. Wefilterout29.77%data
throughthequalitycontrolfortheworkflowgraph.
Toensurecomplexity,wefilteroutdatapointswithonly1nodeor1edge. Furthermore,tomaintain
databalanceacrossdifferentscenarios,werandomlysampledatasetswithexcessivedatavolume,
subsequentlydividingthemintotrainingandtestingsets. Thenwemanuallycheckthetestsetfora
fairandeffectiveevaluation(ThedetailedhumanverificationprocesscanbefoundinAppendixA.2).
ThefinaldatastatisticsareoutlinedinAppendixA.3.
2.4 WORFEVAL
Toensureaccuracy,apartfromutilizingGPT-4orsimplesemanticsimilaritymatching,wequantita-
tivelyevaluateboththenodechainandworkflowgraphusingrestrictalgorithms. Assumingthenodes
andedgesofthegoldworkflowaredenotedasVg = [vg,vg,...,vg ]andEg = [eg,eg,...,eg ],
1 2 |Vg| 1 2 |Eg|
while the agent predicted nodes and edges are represented as Vp = [vp,vp,...,vp ] and Ep =
1 2 |Vp|
[ep,ep,...,ep ]. Firstly,wecalculatethesimilaritymatrixS betweenVg andVp:
1 2 |Ep|
(cid:40)
σ(vg,vp), σ(vg,vp)≥β
S = i j i j (3)
i,j 0, σ(vg,vp)<β
i j
Here,σrepresentsthecosinesimilarityfunctionandinthispaper,weencodethesemanticsofnodes
usingSentence-BERT2(Reimers&Gurevych,2019). β servesasathreshold,andweconsidertwo
2all-mpnet-base-v2: https://huggingface.co/sentence-transformers/
all-mpnet-base-v2.Thismodelisalsousedastheretrieverinbenchmarkconstructionprocess.
4nodestobesemanticallymatchedonlywhentheirsimilarityisgreaterthanorequaltoβ. Therefore,
S canbeviewedasabipartitegraph,whereonepartconsistsofgoldnodesandtheotherpartconsists
ofpredictednodes. Sinceapredictednodemaymatchmultiplegoldnodesandagoldnodemay
bematchedbymultiplepredictednodes,weutilizeamax-weightedbipartitematchingalgorithm
(Hopcroft&Karp,1973)tofindthebestmatches. Ultimately,thematchedgoldnodesandpredicted
nodesformtwonewnodesetsVg′ ⊆Vg andVp′ ⊆Vp,wherethenodesinVp′ andthenodesinVg′
correspondone-to-one. Afterobtainingtheaforementionedvariables,weproceedtoevaluateboth
thenodechainandworkflowgraphpredictedbytheagent.
NodeChain. Supposingtheagent’spredictednodechainisC(Vp),basedonthegoldworkflow
graphG(Vg,Eg),wecanobtainallitspossibletopologicalsequences{C(Vg) ,C(Vg) ,...,C(Vg) }3.
1 2 n
AssumingC(Vp′)isapermutationofVp′ suchthatitformsasubsequenceofC(Vp),andconsidering
theone-to-onemappingbetweenVp′ andVg′,wecanderiveasequenceC(Vg′)thatalignsinorder
withC(Vp′). Next,followingChenetal.(2024),basedontheindexesofVg′ inC(Vg) ,1≤i≤n,
i
wecancalculateaLongestIncreasingSubsequence(LIS)4l ofC(Vg′)foreachC(Vg) :
i i
l =LIS(cid:0) C(Vg′ ),C(Vg) (cid:1) (4)
i i
Then,l=max(|l |,|l |,...,|l |)isobtainedtorepresentthelengthofthelongestvalidsubsequence
1 2 n
fortheagent’spredictednodechainC(Vp). Finally,thescoreforthenodechainisdenotedas:
2p r l l
f1 = chain chain ,p = ,r = , (5)
chain p +r chain |Vp| chain |Vg|
chain chain
wherep andr aretheprecisionandrecallofthegeneratednodechainrespectively.
chain chain
Workflow Graph. Based on Vp′, we can derive the subgraph G(Vp′,Ep′), where Ep′ satisfies
Ep′ ⊆Epand∀(v ,v )∈Ep′,v ,v ∈Vp′. Byonceagainleveragingtheone-to-onecorrespondence
i j i j
betweenVp′ andVg′,wecanutilizetheMaximumCommonInducedSubgraph(MCIS)5matching
algorithmtofindthemaximummatchbetweenG(Vp′,Ep′)andG(Vg,Eg):
G (V ,E
)=MCIS(cid:0) G(Vp′ ,Ep′ ),G(Vg,Eg)(cid:1)
(6)
mcis mcis mcis
Assumingthenumberofnodesinthismaximumcommoninducedsubgraphisk =|V |,thescore
mcis
fortheagent-generatedworkflowgraphisdenotedas:
2p r k k
f1 = graph graph ,p = ,r = , (7)
graph p +r graph |Vp| graph |Vg|
graph graph
wherep andr aretheprecisionandrecallofthegeneratedworkflowgraphrespectively.
graph graph
3 EXPERIMENTS
3.1 EXPERIMENTALSETTINGS
TocomprehensivelyevaluatetheworkflowgenerationcapabilitiesofexistingLLMagents,wevalidate
atotalnumberof18modelsonthetestsetofWORFBENCH,including:
1) Three representative closed-sourced language models from OpenAI and Anthropic: GPT-4
(gpt-4-turbo-2024-04-09)(OpenAI,2023),GPT-3.5(gpt-3.5-turbo-0125)(OpenAI,
2022)andClaude-3.5(claude-3.5-sonnet)(Anthropic,2024).
2) Fifteen state-of-the-art open-sourced language models ranging from 7B to 72B: Llama se-
riesmodelsandtheirvariants,Llama-3.1-{8,70}B(Meta-Llama-3.1-{8,70}B-Instruct)
(Dubeyetal.,2024),Llama-2-13B(Llama-2-13b-chat-hf)(Touvronetal.,2023),Vicuna-13B
(vicuna-13b-v1.5)(Zhengetal.,2023),andWizardLM-{13,70}B(WizardLM-13B-V1.2
3Whenthenumberofnodesandedgesisconsiderable,traversingallpossibletopologicalordersofagraph
becomesahightimecomplexityissue.Therefore,welimittheoutputtoamaximumof20topologicalorders.
4https://en.wikipedia.org/wiki/Longest_increasing_subsequence
5https://en.wikipedia.org/wiki/Maximum_common_induced_subgraph
5Table1: MainResults.Weevaluateallthemodelswithidenticalcarefullydesignedinstructionsandtwo-shot
examples.Wecategorizethemodelsbasedonwhetherthemodelsareopen-sourceandtheirscales.Thebest
resultsforeachcategoryaremarkedinbold,andthesecond-bestresultsaremarkedwithunderline.
FunctionCall Problem-Solving Embodied Open-Grounded Average
Model
f1chain f1graph f1chain f1graph f1chain f1graph f1chain f1graph f1chain f1graph
Closed-Sourced
Claude-3.5 65.58 51.93 62.44 45.45 65.04 48.47 61.73 41.49 63.70 46.84
GPT-3.5 73.36 60.32 69.86 54.50 64.57 49.17 47.67 28.10 63.86 48.02
GPT-4 74.87 62.11 67.18 55.24 70.94 56.17 56.30 36.36 67.32 52.47
Open-Sourced
GLM-4-9B 59.27 36.34 58.91 40.15 53.17 36.15 44.04 22.56 53.85 33.80
Phi-3-small 57.66 40.71 55.76 39.75 54.77 37.52 44.65 22.66 53.21 35.16
Llama-3.1-8B 63.30 43.62 64.49 46.79 56.23 36.40 44.58 25.48 57.15 38.08
Mistral-7B 67.30 51.67 61.27 45.35 64.59 48.83 40.97 21.48 58.53 41.83
Qwen-2-7B 70.79 55.50 68.65 52.13 62.83 46.25 39.29 20.89 60.39 43.69
InternLM-2.5-7B 68.43 52.99 72.92 57.80 65.77 48.09 40.84 21.27 61.99 45.03
Llama-2-13B 53.32 34.33 53.74 38.69 44.27 30.55 37.17 23.14 47.12 31.68
WizardLM-13B 55.78 36.94 65.42 49.71 55.41 37.34 37.23 21.66 53.46 36.41
Vicuna-13B 53.75 37.66 64.58 50.25 57.99 42.61 38.93 23.11 53.81 38.41
Qwen-1.5-14B 65.73 46.86 58.80 43.89 60.55 44.14 41.73 21.44 56.70 39.08
Phi-3-medium 67.71 47.26 71.15 54.85 65.11 49.99 42.73 23.77 61.68 43.97
WizardLM-70B 63.47 45.46 63.92 47.93 59.15 42.87 45.27 26.89 57.95 40.79
Mixtral-8×7B 66.13 48.83 71.89 57.58 72.08 54.94 42.96 23.21 63.26 46.14
Llama-3.1-70B 64.41 52.72 70.37 57.05 69.98 55.52 53.64 33.06 64.60 49.59
Qwen-2-72B 71.67 52.31 70.63 58.13 73.24 58.49 53.43 32.89 67.24 50.46
and WizardLM-70B-V1.0) (Xu et al., 2024); Qwen series models, Qwen-2-{7,72}B
(Qwen2-{7,72}B-Instruct)(Yangetal.,2024a)andQwen-1.5-14B(Qwen1.5-14B-Chat)
(Bai et al., 2023); Mistral series models, Mistral-7B (Mistral-7B-Instruct-v0.2) (Jiang
et al., 2023) and Mixtral-8×7B (Mixtral-8x7B-Instruct-v0.1) (Jiang et al., 2024); Phi
seriesmodels,Phi-3-{small,medium}(Phi-3-{small,medium}-128k-instruct)(Abdin
et al., 2024); other models including GLM-4-9B (glm-4-9b-chat) (Zeng et al., 2024b) and
InternLM-2.5-7B(internlm2.5-7b-chat)(Caietal.,2024).
WeevaluateallthemodelsusingtheLlamaFactory(Zhengetal.,2024b)frameworkwithtwo-shot
prompting. Formodelsabove70Bparameters,weutilizevLLM(Kwonetal.,2023)toaccelerate
inference. Thehyperparametersusedduringdecodingareallsettodefaultvaluesexceptforthe
temperature,whichis0.5. Forallthemodels,thesemanticallymatchingthresholdβ issetto0.6.
3.2 MAINRESULTS
Table1displaysourdetailedexperimentalresults. Withthehelpofthistable,weaimtoanalyzethe
followingfourresearchquestions.
Q1: Which is more challenging for LLM agents, linear planning or graph planning? For
eachkindoftask,weshowboththenodechainscoref1 andworkflowgraphscoref1 ,
chain graph
representingthelinearplanningandgraphplanningabilitiesofLLMagentsrespectively. Itcanbe
observedthattheworkflowgraphscoresofallmodelsaresignificantlylowerthanthenodechain
scores,withthelargestdisparityfoundinGLM-4-9B,reachinganaveragedifferenceof20.05%.
Eventhemodelwiththesmallestdifference,Llama-3.1-70B,exhibitsadifferenceof15.01%. Soit
appearsthatgraphplanningismorechallengingthanlinearplanning,andoverall,thegraphplanning
capabilityofmodelsispositivelycorrelatedwiththeirlinearplanningcapability. However,thereare
alsoseveralexceptionstothistrend. Forinstance,inthereasoningtasks,Qwen-2-72Bhasalower
f1 comparedtoMixtral-8×7B,butitleadsinf1 . Ontheotherhand,infunctioncalltasks,
chain graph
althoughQwen-2-72Bhasamoresatisfyingf1 ,itsf1 islowerthanthatofLlama-3.1-70B.
chain graph
Q2: HowisScalingLawmanifestedinworkflowgeneration? Modelsize,trainingdatascale,
andtrainingtimearethreecrucialindicatorsoftheScalingLaw(Kaplanetal.,2020).Foropen-source
models,wecategorizethemintothreegroupsbasedonmodelsize: around7B,13B,and70B.By
observingtheperformanceofmodelswithinthesameseries,wecandiscernthepowerofmodel
sizeforworkflowgeneration. Forinstance,intheQwen-2series,the72Bmodeloutperformsthe
7B model by 6.77% in f1 ; in the Llama-3.1 series, the 70B model surpasses the 8B model
graph
by11.51%. Anunconventionalphenomenonisthatsome7Bmodelsoutperformthemajorityof
13Bmodels. Onereasonforthisisthatmostoftheselected7Bmodelswerereleasedwithinthe
pastsixmonths,andtrainedonagreateramountofhigh-qualitydata,whichiscrucialforworkflow
670 node num
edge num
60 f f1 1_ _c gh raa pin h Table2: GeneralizationResultsoffine-tuned(FT)
modelsonheld-outtaskscomparedtobaselines.
50
40 Held-inTasks Held-outTasks
Model Average Seal-Tools InterCodeSQL
30
fchain fgraph fchain fgraph fchain fgraph
20 GPT-3.5 63.86 48.02 95.91 76.63 65.30 53.07
GPT-4 67.32 52.47 96.58 80.25 66.35 54.36
10 Qwen-2-7B 60.39 43.69 92.68 74.75 54.20 39.72
InternLM-2.5-7B 61.99 45.03 93.07 74.43 55.06 42.20
0 Phi-3-medium 61.68 43.97 94.11 79.45 58.45 46.62
2 3 4 5 6 7 8 9 10 11 12 Llama-3.1-70B 64.60 49.59 94.40 80.11 63.49 53.66
Node & Edge Num Qwen-2-72B 67.24 50.46 94.47 78.90 63.86 52.47
Figure3: PerformanceDistributionofGPT-4.The Qwen-2-7B+FT 79.35 70.38 96.49 82.82 62.37 48.72
InternLM-2.5-7B+FT 78.98 69.33 95.83 83.72 63.78 50.97
distributionoff1 forthenumberofnodesand
chain
thedistributionoff1 forthenumberofedges.
graph
generation. Incontrast,the13Bmodelsweremostlyreleasedlastyear,andtheirtrainingdataand
techniquesmayhavebecomeoutdated.Anotherpossiblereasonisourspeculationthatmodelsaround
13Bmayhavereachedapointwherethetrade-offbetweentrainingcostsandmodeleffectivenesshas
becomechallengingtonavigate. Thiscouldalsoexplainwhytherehavebeenfewmodelsofthis
scalereleasedinthepastsixmonthsintheopen-sourcecommunity.
Q3: How far are existing LLM agents from being real workflow planning experts? Even
though the node chains and workflow graphs are synthesized by GPT-4 in our benchmark, after
weremovethegoldtrajectoriesandletitgeneratedirectly,itsabsoluteperformanceinf1 and
chain
f1 averagesonly67.32%and52.47%,respectively. Inthechallengingopen-groundedplanning
graph
tasks,thetopperformer,Claude-3.5,onlyachieves61.73%and41.49%,whichisfarfromthelevel
ofapracticalanddeployableworkflowplanner. Inaddition,weanalyzetheperformanceofGPT-4
acrossdifferentnumbersofnodesandedgesinworkflow,asshowninFigure3. Withtheincreaseof
nodesandedges,boththef1 andf1 performanceofGPT-4tendtodecline,withoccasional
chain graph
briefspikeslikelycausedbyunevensampledistribution. Therefore,forcomplexplanningtaskswith
moreplanningsteps,theperformanceofGPT-4isunsatisfyingnomatterforlinearplanningorgraph
planning,letaloneothermodels. Thisisclearlyinadequateformanycomplexreal-worldscenarios,
whichiswhymanyagentarchitecturesarecurrentlyonlyatthetheoreticallevel.
Furthermore,weattempttotrainthe7BmodelsQwen-2-7BandInternLM-2.5-7B(bothmodelsshow
excellentperformanceinTable1)onthetrainingset6. Weevaluatethetrainedmodels’capabilities
on both held-in and held-out tasks. Results are presented in Table 2. While the trained models
haveshownthebestperformanceonSeal-Tools(surpassingGPT-4by2.5∼3.5%inf1 ),their
graph
advantages are not as pronounced as on held-in tasks (where they surpass GPT-4 by 10%+ and
15%+inf1 andf1 ,respectively). Moreover,theworkflowsofSeal-Toolsarerelatively
chain graph
simple(about2∼3nodespertaskonaverage),withevenuntrained7Bmodelsachievingaround
74%. When it comes to the more complex InterCodeSQL tasks, the trained models fall slightly
behind,onlyoutperformingmodelsaround7Band13B.Tosumup,althoughthetrainedmodelshave
madesignificantbreakthroughsontheheld-intasks,theirperformancedoesnotexhibitremarkable
generalizationwhenextendedtoheld-outtasks,especiallyonembodiedtasks. Thisimpliesthatthe
structuredworkflowplanningcapabilitycannotbelearnedsolelythroughfittingalargeamountof
data. Sowhetherthroughpromptingortrainingforgeneralization, LLMagentsstillhavealong
distancetoreachrealworkflowplanningexperts.
Q4: Whatshallwedotoenhancetheworkflowgenerationcapability
ofLLMagents? First,weanswerthisquestionbyanalyzingsamples
whereGPT-4scoreslessthan0.5onf1 .Throughmeticulousmanual
graph
checks and categorization, we identify four kinds of typical errors: 1)
Granularity. Thedecompositionofsubtasksdoesnotmeettheminimum
executable granularity. 2) Explicitness. The summary of subtasks is
overlyvague. 3)Graph. Thesubtaskiscorrect,butthegraphstructure
isincorrect. 4)Format. Theoutputdoesnotadheretothespecifiedtext
format. Figure 4 displays the probabilities of different types of errors
thatoccurinallthefailedsamples. WealsotestsomecasesonOpenAI’s
strongestreasoningmodel,o1(OpenAI,2024),whicharepresentedin granularity explicitness
graph format others
AppendixA.5. Amongthem,case(a)representstheerrorofGraph,while
Figure4: ErrorStatistics.
case(b)representstheerrorofGranularity. Wesummarizethatmostof
6ThedetailedtrainingsetupscanbefoundinAppendixA.4
7
hparg_1f
&
niahc_1ftheseerrorscanlikelybeattributedtotheLLMagent’slackofenvironmentalknowledge. Granularity
pertainstoadeficiencyinpriorknowledgeoftheenvironment. Forinstance,incase(b),o1lacks
the knowledge that the fridge can be used for cooling, assuming that a cool potato can only be
achievedinsideafridgeratherthansomewhereelse. Explicitnessreflectsalackofunderstanding
oftheenvironmentaltask, resultingininsufficientlyspecificsubtasks. Graphissuesarisefroma
lackofcomprehensionregardingthedependenciesofenvironmentalactions,leadingtoerrorsinthe
relationshipsbetweensubtasks. Hence,toachieveahigherlevelofintelligence,relyingsolelyon
step-levelfeedback,reinforcementlearning,orinference-timescalingmaynotsuffice. Thekeymay
lieintrulyintegratingworldknowledge(Yuetal.,2024;Qiaoetal.,2024a;Guettaetal.,2024)or
worldmodel(Dawid&LeCun,2023;Hu&Shu,2023;Wongetal.,2023)intoagentstoadvance
theirunderstandingoftherealworld(Sumersetal.,2024;Duranteetal.,2024a;Yangetal.,2024d).
4 THE ROLE OF WORKFLOW FOR AGENT PLANNING
Inthissection,wedelveintohowstructuredworkflowscanaiddownstreamtaskssuchasfunction
invocationorembodiedplanninginachievingmorepreciseandexpeditedresults. Inthissection,we
defaulttoutilizingthetrainedQwen-2-7B(mentionedin§3.2Q3)astheworkflowgenerator.
4.1 ENHANCEEND-TO-ENDPERFORMANCE
Table3:End-to-endPerformanceaugmentedbywork- Workflow as Structured Prior Knowledge.
flowaspriorknowledge. Giventhataworkflowencompassesadetailed
executionprocessforatask,anevidentusecase
Model ALFWorld WebShop is to employ it as prior knowledge to directly
seen unseen guide agent planning. This is particularly ad-
GPT-4 27.14 28.36 55.62 vantageousinembodiedscenarioswhereLLM
GPT-4+W 40.71↑13.57 47.01↑18.65 56.49↑0.87
agentsoftenlackpriorknowledgeoftherealen-
Llama-3.1-8B 1.49 5.00 51.03
Llama-3.1-8B+W 8.21↑6.72 12.14↑7.14 52.28↑1.25 vironmentandrelyonbrainlesstrial-and-error
Qwen-2-72B 53.57 56.72 58.95 (Qiaoetal.,2024a). Therefore,wedirectlyin-
Qwen-2-72B+W 56.43↑2.86 62.29↑5.57 60.55↑1.60
putthegeneratedworkflowalongwiththetask
and design instructions for the LLM agent to plan based on the guidance of the workflow. We
chooseGPT-4,Llama-3.1-8B,andQwen-2-72BastheLLMagentsandreporttheresultsinTable3,
illustratingthatmodelswithvaryingcapabilitiescanbenefitwhenenrichedwithstructuredworkflow
knowledge. ForALFWorldwithgreaterdiversityinenvironmentalchangesandmorecomplextasks,
workflowknowledgeyieldsgreateradvantages. Furthermore,weobservethattheseworkflowsare
generatedbya7Bmodel,providingguidanceeventothesignificantlymorepowerful72Bmodel.
Thisleadsustocontemplatetheweak-guide-strongparadigm,whereinasmallmodelpossessing
specificenvironmentalknowledgesupervisestheplanningofalarger,moregeneralmodel.
WorkflowasCoTAugmentation. Chain-of-Thought I2 Cat.
(CoT)(Weietal.,2022)hasbeenwidelyacknowledged 87.50
forenhancingthereasoningabilitiesofLLMsandplaysa
crucialroleinOpenAI’slatestreasoningmodel,o1(Ope-
83.79
nAI,2024). However,atrickyissueliesinitslong-context
nature,whichmaymisleadLLMagentsinmakingerro- 72.90 82.44
85.33 86.00
neousdecisions,especiallywhentherearemultipleplan- I3 Inst. I2 Inst.
ningstepsinvolved. Basedonourworkflowconstruction
processwhereeachnodecorrespondstoafunctioncall,
GPT-4 Qwen-2-72B ToolLlama Qwen-2-7B+W
wecanleveragethischaracteristictoinduceagentstoen-
Figure5:RelativeFunctionCallAccuracy
gageinmorefocusedplanning. Specifically,weprompt
ofworkflow-augmentedQwen-2-7B(Qwen-
Qwen-2-7BtogenerateaCoTateachstepbasedonthe
2-7B+W)onStableToolBench(Guoetal.,
correspondingnodeandthenusethenodeasaqueryto
2024b)comparedwithvariousbaselines.
retrievethemostsimilarAPIfromtheAPIlistasthefunc-
tionforthatstep. Ultimately,weallowthemodeltodecidehowtoinvokethefunctionbasedon
theCoTandtheselectedfunction. Inthisprocess,theworkflowplaysarolesimilartoaugmenting
CoT,assistingtheagentinthinkingateachstep, servingasthequeryforretrievaltoprovidethe
agentwithmorerelevantAPIs,therebyalleviatingtheagent’sburdenandenablingittofocusmore
onhowtoinvoketoolseffectively. BycomparingtheaccuracyoffunctioncallswithToolLlama
8(Qinetal.,2024)andtwoone-shotbaselines(Qwen-2-72BandGPT-4)onStableToolBench7,we
findthattheaboveprocedureiseffective(showninFigure5). Unlikeakindofexternalknowledge
forreference,theworkflowhereactivelyparticipatesintheplanningprocess,leadingtoimproved
accuracyinfunctioninvocation.
4.2 REDUCEEND-TO-ENDINFERENCE-TIME
Parallel Planning Steps. In a graph-structured workflow, nodes without dependencies can be
executedinparallel. Thiscansignificantlyreducethetimerequiredtocompletetaskscomparedto
linearstep-by-stepexecution. Continuingouranalysison
StableToolbench,foraspecifictask,wecalculatethetime
ToolLlama ToolLlama+W
takenbyToolLlamatocompleteeachnodewhenexecut- 50
ingstepbystep(includinggeneratingthought,generating
40 44.15 18.26%
functioncalls,executingfunctions,andreturningresults). 39.31 39.76
Wethenmarkthenodesintheworkflowgraphbasedon 30 35.49% 32.47% 36.09
theircompletiontimes.Soourobjectivecanbetransferred 25.36 26.85
20
toidentifythelongestpathbetweentheSTARTandEND
nodes,alsoknownastheCriticalPathofthegraph. Fi- 10
nally,wecomparetheaveragetimetakentocompleteall 0
taskswiththelinearToolLlama,asshowninFigure6. It I2 Cat. I2 Inst. I3 Inst
.
canbeobservedthatwithgraph-structuredparallelization, Figure6: AverageTaskExecutionTime
thereisasignificantreductionintheaveragetimetocom- oflinearToolLlamaandparallelToolLlama.
pletetasks,withreductionsrangingfromapproximately
one-fifthtoone-thirdacrossdifferenttestsets. Theparallelizationfeatureofgraphstructuresallows
forsubstantialsavingsininferencetimeinreal-worldapplications. Moreover,theexecutionofa
nodedoesnotnecessarilydependonallpreviousnodes,whichtosomeextentalleviatestheissueof
longcontextsinmulti-stepcomplextasks,therebyenhancingthequalityoftaskcompletion.
Table4: AveragePlanningSteps. Shorten Planning Steps. In addition to the hori-
zontalreductionofinferencetimebroughtbyparallel
ALFWorld subtaskexecution, wealsoobservethatworkflows
Model WebShop
seen unseen canverticallydecreasetheplanningstepsoftheLLM
GPT-4 17.19 17.43 5.80 agent. Thisfindingemergesduringourexperiments
GPT-4+W 15.64↓1.55 15.85↓1.58 5.72↓0.08
on workflow as structured knowledge. When the
Llama-3.1-8B 19.81 19.43 7.08
Llama-3.1-8B+W 19.09↓0.72 18.38↓1.05 6.77↓0.31 LLMagentlackspriorknowledgeoftheenvironment,
Qwen-2-72B 14.39 14.67 3.88 itoftenaccumulatesknowledgethroughrandomtrial-
Qwen-2-72B+W 14.05↓0.34 13.94↓0.73 3.73↓0.15 and-errorintheenvironment,whichmayintroduce
irrelevantnoiseandleadtoadropinlong-textdisaster. Introducingknowledgemakestheagent’s
actionsmorepurposeful,reducingthestepsofblindtrial-and-error. InTable4,wequantitatively
analyzetheaverageplanningstepsrequiredforthemodeltocompletetaskswithorwithoutworkflow
knowledge,whichcorroboratesourdiscoveries.
5 RELATED WORK
5.1 LARGELANGUAGEAGENTS
TheriseofLargeLanguageModels(LLMs)hasestablishedthemattheforefrontofthequestfor
Artificial General Intelligence (AGI), offering substantial support for the advancement of LLM-
centeredAIagents(Wangetal.,2023;Xietal.,2023;Guoetal.,2024a;Yangetal.,2024b;Durante
et al., 2024b; Li et al., 2024b; Zhang et al., 2024a; Yang et al., 2024c). Numerous efforts have
beendedicatedtoemployinglanguageagentsfortoolutilization(Qinetal.,2024;Tangetal.,2023;
Yuanetal.,2024;Yeetal.,2024;Qiaoetal.,2024b;Quetal.,2024),embodiedplanning(Huang
et al., 2022; Yao et al., 2023b; Song et al., 2023; Xiang et al., 2023; Palo et al., 2023), software
engineering(Hongetal.,2024;Qianetal.,2023;2024),etc. ManyLLM-drivenagentsystemshave
7AsdefinedinToolBench(Qinetal.,2024),I2andI3standforintra-categoryandintra-collectionmulti-tool
instructions,respectively,basedonthetoolsbelongingtothesameRapidAPI(https://rapidapi.com/
hub)categoryorcollection.Inst.representsunseeninstructionsforthesamesetoftoolsinthetrainingdata.
Cat.denotesunseentoolsthatbelongtoanunseencategoryoftoolsinthetrainingdata.
9
)s(
emitbeendevelopedforapplicationsinwebinterfaces(Nakanoetal.,2021;Dengetal.,2023;Guretal.,
2024),medical(Lietal.,2024a),coding(Sunetal.,2024;Lietal.,2023;Shenetal.,2023a),or
general-purpose(Wuetal.,2023;Zhouetal.,2024;Wangetal.,2024b)tasks. However,theseagent
methodsorframeworkstendtoconcentrateprimarilyontheend-to-endperformanceofthetaskat
hand,overlookingtheassessmentoftheinherentreasoningandplanningcapabilitiesthatareactually
thebasesforachievingstableandreliableagentperformance.
5.2 WORKFLOWANDAGENTPLANNING
Theabilityofagentstoaddresscomplextaskscanbedividedintotwocategories: reasoning(Qiao
etal.,2023;Yaoetal.,2023a;Lietal.,2023;Bestaetal.,2024;Wangetal.,2024a)andplanning
(Yaoetal.,2023b;Yinetal.,2024;Songetal.,2024;Qiaoetal.,2024a;Mengetal.,2024),basedon
whetherinteractionwiththeexternalenvironmentisrequired. Infact,reasoningabilitycanalsobe
viewedasapartofplanningability(Haoetal.,2023). Anyway,bothabilitiesfundamentallyinvolve
decomposingcomplexproblemsintoworkflowsthatcontainaseriesoffiner-grainedsubtasksto
beeasilycompleted. Ontheonehand,workflowscanserveasanintermediatestateforsolving
complex tasks, aiding agents in bridging the gap between tasks and specific executable actions.
ExplicitworkflowscanenhancetheinterpretabilityoftheLLMagent,facilitatinghumaninvolvement
indebuggingandensuringtheagent’ssecurity(human-machineinteraction). Ontheotherhand,
workflowscanserveasstructuredpriorknowledge,assistingagentsinhandlingknowledge-intensive
taskstoavoidplanninghallucinations(Hongetal.,2024;Zhouetal.,2023;Zengetal.,2023;Ye
etal.,2023;Zhuetal.,2024;Qiaoetal.,2024a;Xiaoetal.,2024).
5.3 WORKFLOWGENERATIONANDEVALUATION
Themoststraightforwardapproachishuman-designedworkflowsthatconstraintheplanningprocess
oflanguageagentsintheformofnaturallanguagepromptsorstatemachinestopreventhallucinations
(Hongetal.,2024;Lietal.,2024c;Guanetal.,2024;Zhuetal.,2024). However,manualdesignis
time-consumingandlacksflexibility. Asaresult,somestudies(Zhouetal.,2023;Yeetal.,2023;
Zengetal.,2023;Lietal.,2024d;Xueetal.,2024)trytoenablelanguageagentstoautomatically
generate workflows. Nevertheless, the propensity of LLMs to generate hallucinations makes it
challengingtoguaranteethequalityoftheworkflows. Therefore,itiscrucialtoeffectivelyevaluate
thequalityofworkflowsgeneratedbylanguageagents. Previousstudieshaveexploredautomating
workflowgenerationevaluationintoollearningscenarios(Chenetal.,2024;Yeetal.,2024;Shen
etal.,2023b),usingfine-grainedmetricstoassesseachstageoftoolutilization. Inparticular,problem
decompositionabilityisevaluatedthroughsemanticsimilaritymatchingorGPT-4scoring. However,
these works suffer from the following limitations: firstly, most of them focus only on function-
callingscenarios,neglectingembodiedinteractivescenarioswiththerealenvironment;secondly,
theirworkflowsareallsinglelinearstructures,makingitdifficulttorepresentmorecomplextasks;
lastly,theymostlyrelyonGPT-4orhumanevaluation. Moreover,Xiaoetal.(2024)alsoexplores
workflow-guided planning, but it is noticed that they mainly examine the workflow compliance
capabilityoflanguageagents,whichcanbeviewedasadownstreamprocedureofourwork. Tothe
bestofourknowledge,wearethefirstbenchmarktoevaluatelanguageagents’abilitytogenerate
complexgraph-structurednaturallanguageworkflows,encompassingmulti-facetedscenarios.
6 CONCLUSION
Inthiswork,weintroduceWORFBENCH,aunifiedagenticworkflowgenerationbenchmarkwith
miscellaneousscenariosandintricategraph-structuredworkflows. Topreciselyassesstheworkflow
generationcapabilityofLLMagents,wefurtherpresentWORFEVAL,whichutilizesquantitative
algorithmstoevaluateboththelinearandgraphworkflows. Throughcomprehensiveexperiments
acrossvariouskindsofLLMs,weinvestigatelargeperformancegapsbetweenthetraditionallinear-
structuredandcomplexgraph-structuredworkflowgeneration. Wealsotrainopen-sourcedmodels
andevaluatetheirgeneralizationabilitiesonheld-outtasks. Finally,weexploretheroleofworkflows
fordownstreamplanningtasksfrommodelperformanceandinferencetimeefficiency.
10LIMITATIONS
Thispaperstillhascertainlimitationsthatmustbeacknowledged: a)Whilewehaveenforcedstrict
qualitycontrolonthenodechainandworkflowgraph,itisinevitablethatsomequeriesthemselves
mayhavequalityissues. Thesynthesisofcomplexqueriesremainsanunresolvedissue,whichwe
leaveforfuturework. b)Allourdataiscollectedfromexistinggeneraldomaindatasets,inevitably
missingscenariosthatarenotcovered. c)Inadditiontonaturallanguage,workflowscanalsobe
representedincodeform(e.g. PDDL)(Zengetal.,2023;Lietal.,2024d;Zuoetal.,2024),whichwe
plantoincorporateinthefuture. d)Ourworkflowcurrentlyfollowsaone-passgenerationparadigm.
In the future, we plan to introduce an iterative paradigm where the workflow can be iteratively
generated and evolve based on environmental feedback. e) Workflows in this paper assume that
allnodesneedtobetraversedtocompletethewholetask. Wedonotcoversomescenarioswith
uncertaintywheretaskcompletionmaynotrequiretraversalofallnodesintheworkflowandeach
nodemayhaveaprobabilityofbeingvisited.
REPRODUCIBILITY STATEMENT
Code and dataset will be available at https://github.com/zjunlp/WorFBench. The
detailed benchmark construction and quality control processes can be found in Section 2.2 and
2.3. Additionaldatasourceinformation,humanverificationprocesses,andbenchmarkstatisticsare
provided in Appendix A.1, A.2 and A.3. Specific test configurations: the code framework used,
themodelversionsused,andtheinferencehyperparametersarementionedinSection3.1. Allthe
trainingsettingsinvolvedinourpaperaredetailedinAppendixA.4.
ETHICS STATEMENT
Thisstudyiscarriedoutinstrictaccordancewithethicalguidelinesandbestpracticesinresearch.
Allthedatautilizedaresourcedfrompubliclyavailabledatasets,andnoproprietaryorconfidential
dataareused. Throughoutthepaper,everymentionoruseofthesedatasourceshasbeenproperly
andaccuratelycited. Westronglyurgealluserstoadheretothehighestethicalstandardswhenusing
ourdataset,ensuringfairness,transparency,andresponsibilityintheirresearch. Anyusageofthe
datasetthatmayleadtoharmorposeadetrimenttosocietyisstrictlyforbidden.
REFERENCES
Marah I Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany
Awadalla,NguyenBach,AmitBahree,ArashBakhtiari,HarkiratS.Behl,AlonBenhaim,Misha
Bilenko,JohanBjorck,SébastienBubeck,MartinCai,CaioCésarTeodoroMendes,WeizhuChen,
VishravChaudhary,ParulChopra,AllieDelGiorno,GustavodeRosa,MatthewDixon,Ronen
Eldan,DanIter,AmitGarg,AbhishekGoswami,SuriyaGunasekar,andetal. Phi-3technical
report: Ahighlycapablelanguagemodellocallyonyourphone. CoRR,abs/2404.14219,2024.
doi: 10.48550/ARXIV.2404.14219. URL https://doi.org/10.48550/arXiv.2404.
14219.
Anthropic. Claude 3.5 sonnet, 2024. https://www.anthropic.com/news/
claude-3-5-sonnet.
Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenbin Ge,
YuHan,FeiHuang,BinyuanHui,LuoJi,MeiLi,JunyangLin,RunjiLin,DayihengLiu,GaoLiu,
ChengqiangLu,KemingLu,JianxinMa,RuiMen,XingzhangRen,XuanchengRen,ChuanqiTan,
SinanTan,JianhongTu,PengWang,andetal. Qwentechnicalreport. CoRR,abs/2309.16609,
2023. doi: 10.48550/ARXIV.2309.16609. URL https://doi.org/10.48550/arXiv.
2309.16609.
MaciejBesta,NilsBlach,AlesKubicek,RobertGerstenberger,MichalPodstawski,LukasGianinazzi,
Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, and Torsten Hoefler.
Graph of thoughts: Solving elaborate problems with large language models. In Michael J.
Wooldridge, Jennifer G. Dy, and Sriraam Natarajan (eds.), Thirty-Eighth AAAI Conference on
11ArtificialIntelligence,AAAI2024,Thirty-SixthConferenceonInnovativeApplicationsofArtificial
Intelligence,IAAI2024,FourteenthSymposiumonEducationalAdvancesinArtificialIntelligence,
EAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 17682–17690. AAAI Press, 2024.
doi: 10.1609/AAAI.V38I16.29720. URL https://doi.org/10.1609/aaai.v38i16.
29720.
Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui
Chen,ZhiChen,PeiChu,XiaoyiDong,HaodongDuan,QiFan,ZhaoyeFei,YangGao,JiayeGe,
ChenyaGu,YuzheGu,andetal. Internlm2technicalreport. CoRR,abs/2403.17297,2024. doi:10.
48550/ARXIV.2403.17297. URLhttps://doi.org/10.48550/arXiv.2403.17297.
Zehui Chen, Weihua Du, Wenwei Zhang, Kuikun Liu, Jiangning Liu, Miao Zheng, Jingming
Zhuo, Songyang Zhang, Dahua Lin, Kai Chen, and Feng Zhao. T-eval: Evaluating the tool
utilization capability of large language models step by step. In Lun-Wei Ku, Andre Martins,
and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August
11-16, 2024, pp.9510–9529. Association for Computational Linguistics, 2024. URL https:
//aclanthology.org/2024.acl-long.515.
AnnaDawidandYannLeCun. Introductiontolatentvariableenergy-basedmodels: Apathtowards
autonomousmachineintelligence. CoRR,abs/2306.02572,2023. doi: 10.48550/ARXIV.2306.
02572. URLhttps://doi.org/10.48550/arXiv.2306.02572.
Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samual Stevens, Boshi Wang, Huan Sun,
and Yu Su. Mind2web: Towards a generalist agent for the web. In Alice Oh, Tristan
Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances
in Neural Information Processing Systems 36: Annual Conference on Neural Informa-
tion Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 -
16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/
hash/5950bf290a1570ea401bf98882128160-Abstract-Datasets_and_
Benchmarks.html.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha
Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,AnirudhGoyal,AnthonyHartshorn,
AoboYang,ArchiMitra,ArchieSravankumar,ArtemKorenev,ArthurHinsvark,ArunRao,Aston
Zhang,AurélienRodriguez,AustenGregerson,andetal. Thellama3herdofmodels. CoRR,
abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. URL https://doi.org/10.
48550/arXiv.2407.21783.
ZaneDurante,QiuyuanHuang,NaokiWake,RanGong,JaeSungPark,BidiptaSarkar,RohanTaori,
YusukeNoda,DemetriTerzopoulos,YejinChoi,KatsushiIkeuchi,HoiVo,LiFei-Fei,andJianfeng
Gao. AgentAI:surveyingthehorizonsofmultimodalinteraction. CoRR,abs/2401.03568,2024a.
doi: 10.48550/ARXIV.2401.03568. URL https://doi.org/10.48550/arXiv.2401.
03568.
ZaneDurante,QiuyuanHuang,NaokiWake,RanGong,JaeSungPark,BidiptaSarkar,RohanTaori,
YusukeNoda,DemetriTerzopoulos,YejinChoi,KatsushiIkeuchi,HoiVo,LiFei-Fei,andJianfeng
Gao. AgentAI:surveyingthehorizonsofmultimodalinteraction. CoRR,abs/2401.03568,2024b.
doi: 10.48550/ARXIV.2401.03568. URL https://doi.org/10.48550/arXiv.2401.
03568.
JianGuan,WeiWu,ZujieWen,PengXu,HongningWang,andMinlieHuang. AMOR:Arecipefor
buildingadaptablemodularknowledgeagentsthroughprocessfeedback. CoRR,abs/2402.01469,
2024. doi: 10.48550/ARXIV.2402.01469. URL https://doi.org/10.48550/arXiv.
2402.01469.
NitzanBittonGuetta,AvivSlobodkin,AviyaMaimon,EliyaHabba,RoyiRassin,YonatanBitton,
IdanSzpektor,AmirGloberson,andYuvalElovici. Visualriddles: acommonsenseandworld
knowledgechallengeforlargevisionandlanguagemodels. CoRR,abs/2407.19474,2024. doi: 10.
48550/ARXIV.2407.19474. URLhttps://doi.org/10.48550/arXiv.2407.19474.
12TaichengGuo,XiuyingChen,YaqiWang,RuidiChang,ShichaoPei,NiteshV.Chawla,OlafWiest,
and Xiangliang Zhang. Large language model based multi-agents: A survey of progress and
challenges. CoRR,abs/2402.01680,2024a. doi: 10.48550/ARXIV.2402.01680. URLhttps:
//doi.org/10.48550/arXiv.2402.01680.
ZhichengGuo,SijieCheng,HaoWang,ShihaoLiang,YujiaQin,PengLi,ZhiyuanLiu,Maosong
Sun,andYangLiu. Stabletoolbench: Towardsstablelarge-scalebenchmarkingontoollearningof
largelanguagemodels. InLun-WeiKu,AndreMartins,andVivekSrikumar(eds.),Findingsof
theAssociationforComputationalLinguistics,ACL2024,Bangkok,Thailandandvirtualmeeting,
August11-16,2024,pp.11143–11156.AssociationforComputationalLinguistics,2024b. URL
https://aclanthology.org/2024.findings-acl.664.
IzzeddinGur,HirokiFuruta,AustinV.Huang,MustafaSafdari,YutakaMatsuo,DouglasEck,and
AleksandraFaust. Areal-worldwebagentwithplanning,longcontextunderstanding,andprogram
synthesis. In The Twelfth International Conference on Learning Representations, ICLR 2024,
Vienna,Austria,May7-11,2024.OpenReview.net,2024. URLhttps://openreview.net/
forum?id=9JQtrumvg8.
Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, and Zhiting
Hu. Reasoningwithlanguagemodelisplanningwithworldmodel. InHoudaBouamor, Juan
Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in
NaturalLanguageProcessing,EMNLP2023,Singapore,December6-10,2023,pp.8154–8173.
AssociationforComputationalLinguistics,2023. doi: 10.18653/V1/2023.EMNLP-MAIN.507.
URLhttps://doi.org/10.18653/v1/2023.emnlp-main.507.
SiruiHong,MingchenZhuge,JonathanChen,XiawuZheng,YuhengCheng,JinlinWang,Ceyao
Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, Chenyu Ran, Lingfeng
Xiao, Chenglin Wu, and Jürgen Schmidhuber. Metagpt: Meta programming for A multi-
agent collaborative framework. In The Twelfth International Conference on Learning Rep-
resentations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL
https://openreview.net/forum?id=VtmBAGCN7o.
5/2
JohnE.HopcroftandRichardM.Karp. Ann algorithmformaximummatchingsinbipartite
graphs. SIAMJ.Comput.,2(4):225–231,1973. doi: 10.1137/0202019. URLhttps://doi.
org/10.1137/0202019.
ZhitingHuandTianminShu. Languagemodels,agentmodels,andworldmodels: TheLAWfor
machinereasoningandplanning.CoRR,abs/2312.05230,2023.doi:10.48550/ARXIV.2312.05230.
URLhttps://doi.org/10.48550/arXiv.2312.05230.
Wenlong Huang, Pieter Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-
shotplanners: Extractingactionableknowledgeforembodiedagents. InKamalikaChaudhuri,
StefanieJegelka,LeSong,CsabaSzepesvári,GangNiu,andSivanSabato(eds.),International
Conference on Machine Learning, ICML 2022, 17-23 July 2022, Baltimore, Maryland, USA,
volume162ofProceedingsofMachineLearningResearch,pp.9118–9147.PMLR,2022. URL
https://proceedings.mlr.press/v162/huang22a.html.
AlbertQ.Jiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSinghChaplot,
Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier,
LélioRenardLavaud,Marie-AnneLachaux,PierreStock,TevenLeScao,ThibautLavril,Thomas
Wang,TimothéeLacroix,andWilliamElSayed. Mistral7b. CoRR,abs/2310.06825,2023. doi:10.
48550/ARXIV.2310.06825. URLhttps://doi.org/10.48550/arXiv.2310.06825.
Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris
Bamford,DevendraSinghChaplot,DiegodeLasCasas,EmmaBouHanna,FlorianBressand,
GiannaLengyel,GuillaumeBour,GuillaumeLample,LélioRenardLavaud,LucileSaulnier,Marie-
AnneLachaux,PierreStock,SandeepSubramanian,SophiaYang,SzymonAntoniak,TevenLe
Scao,ThéophileGervet,ThibautLavril,ThomasWang,TimothéeLacroix,andWilliamElSayed.
Mixtral of experts. CoRR, abs/2401.04088, 2024. doi: 10.48550/ARXIV.2401.04088. URL
https://doi.org/10.48550/arXiv.2401.04088.
13JaredKaplan, SamMcCandlish, TomHenighan, TomB.Brown, BenjaminChess, RewonChild,
ScottGray,AlecRadford,JeffreyWu,andDarioAmodei.Scalinglawsforneurallanguagemodels.
CoRR,abs/2001.08361,2020. URLhttps://arxiv.org/abs/2001.08361.
Mahnaz Koupaee and William Yang Wang. Wikihow: A large scale text summarization dataset.
CoRR,abs/1810.09305,2018. URLhttp://arxiv.org/abs/1810.09305.
WoosukKwon,ZhuohanLi,SiyuanZhuang,YingSheng,LianminZheng,CodyHaoYu,Joseph
Gonzalez,HaoZhang,andIonStoica. Efficientmemorymanagementforlargelanguagemodel
servingwithpagedattention. InJasonFlinn,MargoI.Seltzer,PeterDruschel,AntoineKaufmann,
andJonathanMace(eds.),Proceedingsofthe29thSymposiumonOperatingSystemsPrinciples,
SOSP2023,Koblenz,Germany,October23-26,2023,pp.611–626.ACM,2023. doi: 10.1145/
3600006.3613165. URLhttps://doi.org/10.1145/3600006.3613165.
Chengshu Li, Jacky Liang, Andy Zeng, Xinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey
Levine,LiFei-Fei,FeiXia,andBrianIchter. Chainofcode: Reasoningwithalanguagemodel-
augmentedcodeemulator. CoRR,abs/2312.04474,2023. doi:10.48550/ARXIV.2312.04474. URL
https://doi.org/10.48550/arXiv.2312.04474.
Junkai Li, Siyu Wang, Meng Zhang, Weitao Li, Yunghwei Lai, Xinhui Kang, Weizhi Ma, and
Yang Liu. Agent hospital: A simulacrum of hospital with evolvable medical agents. CoRR,
abs/2405.02957, 2024a. doi: 10.48550/ARXIV.2405.02957. URL https://doi.org/10.
48550/arXiv.2405.02957.
Yuanchun Li, Hao Wen, Weijun Wang, Xiangyu Li, Yizhen Yuan, Guohong Liu, Jiacheng Liu,
WenxingXu,XiangWang,YiSun,RuiKong,YileWang,HanfeiGeng,JianLuan,XuefengJin,
ZilongYe,GuanjingXiong,FanZhang,XiangLi,MengweiXu,ZhijunLi,PengLi,YangLiu,
Ya-QinZhang,andYunxinLiu. PersonalLLMagents: Insightsandsurveyaboutthecapability,
efficiencyandsecurity. CoRR,abs/2401.05459,2024b. doi: 10.48550/ARXIV.2401.05459. URL
https://doi.org/10.48550/arXiv.2401.05459.
ZelongLi,WenyueHua,HaoWang,HeZhu,andYongfengZhang. Formal-llm: Integratingformal
languageandnaturallanguageforcontrollablellm-basedagents. CoRR,abs/2402.00798,2024c.
doi: 10.48550/ARXIV.2402.00798. URL https://doi.org/10.48550/arXiv.2402.
00798.
Zelong Li, Shuyuan Xu, Kai Mei, Wenyue Hua, Balaji Rama, Om Raheja, Hao Wang, He Zhu,
and Yongfeng Zhang. Autoflow: Automated workflow generation for large language model
agents. CoRR, abs/2407.12821, 2024d. doi: 10.48550/ARXIV.2407.12821. URL https:
//doi.org/10.48550/arXiv.2407.12821.
WeiwenLiu,XuHuang,XingshanZeng,XinlongHao,ShuaiYu,DexunLi,ShuaiWang,Weinan
Gan,ZhengyingLiu,YuanqingYu,ZezhongWang,YuxianWang,WuNing,YutaiHou,BinWang,
ChuhanWu,XinzhiWang,YongLiu,YashengWang,DuyuTang,DandanTu,LifengShang,Xin
Jiang,RuimingTang,DefuLian,QunLiu,andEnhongChen. Toolace: Winningthepointsofllm
functioncalling,2024a. URLhttps://arxiv.org/abs/2409.00920.
XiaoLiu, HaoYu, HanchenZhang, YifanXu, XuanyuLei, HanyuLai, YuGu, HangliangDing,
KaiwenMen,KejuanYang,ShudanZhang,XiangDeng,AohanZeng,ZhengxiaoDu,Chenhui
Zhang,ShengShen,TianjunZhang,YuSu,HuanSun,MinlieHuang,YuxiaoDong,andJieTang.
Agentbench: Evaluatingllmsasagents. InTheTwelfthInternationalConferenceonLearning
Representations, ICLR2024, Vienna, Austria, May7-11, 2024.OpenReview.net, 2024b. URL
https://openreview.net/forum?id=zAdUB0aCTQ.
XiaoLiu,TianjieZhang,YuGu,IatLongIong,YifanXu,XixuanSong,ShudanZhang,HanyuLai,
XinyiLiu,HanlinZhao,JiadaiSun,XinyueYang,YuYang,ZehanQi,ShuntianYao,Xueqiao
Sun, Siyi Cheng, Qinkai Zheng, Hao Yu, Hanchen Zhang, Wenyi Hong, Ming Ding, Lihang
Pan, Xiaotao Gu, Aohan Zeng, Zhengxiao Du, Chan Hee Song, Yu Su, Yuxiao Dong, and Jie
Tang. Visualagentbench: Towardslargemultimodalmodelsasvisualfoundationagents. CoRR,
abs/2408.06327, 2024c. doi: 10.48550/ARXIV.2408.06327. URL https://doi.org/10.
48550/arXiv.2408.06327.
14Silin Meng, Yiwei Wang, Cheng-Fu Yang, Nanyun Peng, and Kai-Wei Chang. Llm-a*: Large
languagemodelenhancedincrementalheuristicsearchonpathplanning. CoRR,abs/2407.02511,
2024. doi: 10.48550/ARXIV.2407.02511. URL https://doi.org/10.48550/arXiv.
2407.02511.
ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu,LongOuyang,ChristinaKim,Christopher
Hesse,ShantanuJain,VineetKosaraju,WilliamSaunders,XuJiang,KarlCobbe,TynaEloundou,
GretchenKrueger,KevinButton,MatthewKnight,BenjaminChess,andJohnSchulman. Webgpt:
Browser-assistedquestion-answeringwithhumanfeedback. CoRR,abs/2112.09332,2021. URL
https://arxiv.org/abs/2112.09332.
OpenAI. Chatgpt: Optimizinglanguagemodelsfordialogue, 2022. https://openai.com/
blog/chatgpt/.
OpenAI. GPT-4technicalreport. CoRR,abs/2303.08774,2023. doi: 10.48550/arXiv.2303.08774.
URLhttps://doi.org/10.48550/arXiv.2303.08774.
OpenAI. Introducing openai o1-preview, 2024. https://openai.com/index/
introducing-openai-o1-preview/.
NormanDiPalo,ArunkumarByravan,LeonardHasenclever,MarkusWulfmeier,NicolasHeess,and
MartinA.Riedmiller. TowardsAunifiedagentwithfoundationmodels. CoRR,abs/2307.09668,
2023. doi: 10.48550/ARXIV.2307.09668. URL https://doi.org/10.48550/arXiv.
2307.09668.
ShishirG.Patil,TianjunZhang,XinWang,andJosephE.Gonzalez. Gorilla: Largelanguagemodel
connectedwithmassiveapis. CoRR,abs/2305.15334,2023. doi: 10.48550/ARXIV.2305.15334.
URLhttps://doi.org/10.48550/arXiv.2305.15334.
ChenQian,XinCong,ChengYang,WeizeChen,YushengSu,JuyuanXu,ZhiyuanLiu,andMaosong
Sun. Communicativeagentsforsoftwaredevelopment. CoRR,abs/2307.07924,2023. doi: 10.
48550/ARXIV.2307.07924. URLhttps://doi.org/10.48550/arXiv.2307.07924.
Chen Qian, Jiahao Li, Yufan Dang, Wei Liu, Yifei Wang, Zihao Xie, Weize Chen, Cheng Yang,
Yingli Zhang, Zhiyuan Liu, and Maosong Sun. Iterative experience refinement of software-
developing agents. CoRR, abs/2405.04219, 2024. doi: 10.48550/ARXIV.2405.04219. URL
https://doi.org/10.48550/arXiv.2405.04219.
Shuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen, Yunzhi Yao, Shumin Deng, Chuanqi Tan,
FeiHuang,andHuajunChen. Reasoningwithlanguagemodelprompting: Asurvey. InAnna
Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual
MeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),ACL2023,
Toronto, Canada, July9-14, 2023, pp.5368–5393.AssociationforComputationalLinguistics,
2023. doi: 10.18653/V1/2023.ACL-LONG.294. URLhttps://doi.org/10.18653/v1/
2023.acl-long.294.
Shuofei Qiao, Runnan Fang, Ningyu Zhang, Yuqi Zhu, Xiang Chen, Shumin Deng, Yong Jiang,
PengjunXie,FeiHuang,andHuajunChen. Agentplanningwithworldknowledgemodel. CoRR,
abs/2405.14205, 2024a. doi: 10.48550/ARXIV.2405.14205. URL https://doi.org/10.
48550/arXiv.2405.14205.
ShuofeiQiao,NingyuZhang,RunnanFang,YujieLuo,WangchunshuZhou,YuchenEleanorJiang,
Chengfei Lv, and Huajun Chen. Autoact: Automatic agent learning from scratch for QA via
self-planning. InLun-WeiKu,AndreMartins,andVivekSrikumar(eds.),Proceedingsofthe62nd
AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),ACL
2024,Bangkok,Thailand,August11-16,2024,pp.3003–3021.AssociationforComputational
Linguistics,2024b. URLhttps://aclanthology.org/2024.acl-long.165.
Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong,
Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong, Runchu Tian, Ruobing Xie, Jie Zhou,
MarkGerstein,DahaiLi,ZhiyuanLiu,andMaosongSun. Toolllm: Facilitatinglargelanguage
modelstomaster16000+real-worldapis. InTheTwelfthInternationalConferenceonLearning
Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL
https://openreview.net/forum?id=dHng2O0Jjr.
15Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei Yin, Jun Xu, and
Ji-RongWen. Toollearningwithlargelanguagemodels: Asurvey. CoRR,abs/2405.17935,2024.
doi: 10.48550/ARXIV.2405.17935. URL https://doi.org/10.48550/arXiv.2405.
17935.
Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. Deepspeed: System op-
timizations enable training deep learning models with over 100 billion parameters. In Ra-
jesh Gupta, Yan Liu, Jiliang Tang, and B. Aditya Prakash (eds.), KDD ’20: The 26th ACM
SIGKDD Conference on Knowledge Discovery and Data Mining, Virtual Event, CA, USA,
August 23-27, 2020, pp. 3505–3506. ACM, 2020. doi: 10.1145/3394486.3406703. URL
https://doi.org/10.1145/3394486.3406703.
NilsReimersandIrynaGurevych. Sentence-bert: Sentenceembeddingsusingsiamesebert-networks.
In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (eds.), Proceedings of the 2019
Conference on Empirical Methods in Natural Language Processing and the 9th International
JointConferenceonNaturalLanguageProcessing,EMNLP-IJCNLP2019,HongKong,China,
November 3-7, 2019, pp. 3980–3990. Association for Computational Linguistics, 2019. doi:
10.18653/V1/D19-1410. URLhttps://doi.org/10.18653/v1/D19-1410.
BoShen,JiaxinZhang,TaihongChen,DaoguangZan,BingGeng,AnFu,MuhanZeng,AilunYu,
JichuanJi, JingyangZhao, YuenanGuo, andQianxiangWang. Pangu-coder2: Boostinglarge
languagemodelsforcodewithrankingfeedback. CoRR,abs/2307.14936,2023a. doi: 10.48550/
ARXIV.2307.14936. URLhttps://doi.org/10.48550/arXiv.2307.14936.
YongliangShen,KaitaoSong,XuTan,WenqiZhang,KanRen,SiyuYuan,WeimingLu,Dongsheng
Li,andYuetingZhuang. Taskbench: Benchmarkinglargelanguagemodelsfortaskautomation.
CoRR,abs/2311.18760,2023b. doi: 10.48550/ARXIV.2311.18760. URLhttps://doi.org/
10.48550/arXiv.2311.18760.
MohitShridhar,XingdiYuan,Marc-AlexandreCôté,YonatanBisk,AdamTrischler,andMatthewJ.
Hausknecht. Alfworld: Aligning text and embodied environments for interactive learning. In
9thInternationalConferenceonLearningRepresentations,ICLR2021,VirtualEvent,Austria,
May 3-7, 2021. OpenReview.net, 2021. URL https://openreview.net/forum?id=
0IOX0YcCdTn.
Chan Hee Song, Brian M. Sadler, Jiaman Wu, Wei-Lun Chao, Clayton Washington, and Yu Su.
Llm-planner: Few-shotgroundedplanningforembodiedagentswithlargelanguagemodels. In
IEEE/CVFInternationalConferenceonComputerVision,ICCV2023,Paris,France,October
1-6, 2023, pp.2986–2997.IEEE,2023. doi: 10.1109/ICCV51070.2023.00280. URLhttps:
//doi.org/10.1109/ICCV51070.2023.00280.
Yifan Song, Da Yin, Xiang Yue, Jie Huang, Sujian Li, and Bill Yuchen Lin. Trial and er-
ror: Exploration-based trajectory optimization of LLM agents. In Lun-Wei Ku, Andre Mar-
tins, and Vivek Srikumar (eds.), Proceedings of the 62nd Annual Meeting of the Association
for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, Au-
gust 11-16, 2024, pp. 7584–7600. Association for Computational Linguistics, 2024. URL
https://aclanthology.org/2024.acl-long.409.
Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L. Griffiths. Cognitive
architectures for language agents. Trans. Mach. Learn. Res., 2024, 2024. URL https:
//openreview.net/forum?id=1i6ZCvflQJ.
Qiushi Sun, Zhirui Chen, Fangzhi Xu, Kanzhi Cheng, Chang Ma, Zhangyue Yin, Jianing Wang,
ChengchengHan,RenyuZhu,ShuaiYuan,QipengGuo,XipengQiu,PengchengYin,XiaoliLi,Fei
Yuan,LingpengKong,XiangLi,andZhiyongWu.Asurveyofneuralcodeintelligence:Paradigms,
advancesandbeyond. CoRR,abs/2403.14734,2024. doi: 10.48550/ARXIV.2403.14734. URL
https://doi.org/10.48550/arXiv.2403.14734.
Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun. Toolalpaca:
Generalizedtoollearningforlanguagemodelswith3000simulatedcases. CoRR,abs/2306.05301,
2023. doi: 10.48550/ARXIV.2306.05301. URL https://doi.org/10.48550/arXiv.
2306.05301.
16HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,DanBikel,LukasBlecher,Cristian
Canton-Ferrer, Moya Chen, and et al. Llama 2: Open foundation and fine-tuned chat models.
CoRR,abs/2307.09288,2023. doi: 10.48550/ARXIV.2307.09288. URLhttps://doi.org/
10.48550/arXiv.2307.09288.
KarthikValmeekam,KayaStechly,andSubbaraoKambhampati. Llmsstillcan’tplan;canlrms? a
preliminaryevaluationofopenai’so1onplanbench,2024. URLhttps://arxiv.org/abs/
2409.13373.
ChaojieWang,YanchenDeng,ZhiyiLv, ZengLiang,JujieHe,ShuichengYan,andBoAn. Q*:
Improvingmulti-stepreasoningforllmswithdeliberativeplanning. CoRR,abs/2406.14283,2024a.
doi: 10.48550/ARXIV.2406.14283. URL https://doi.org/10.48550/arXiv.2406.
14283.
LeiWang,ChenMa,XueyangFeng,ZeyuZhang,HaoYang,JingsenZhang,ZhiyuanChen,Jiakai
Tang,XuChen,YankaiLin,WayneXinZhao,ZheweiWei,andJi-RongWen. Asurveyonlarge
languagemodelbasedautonomousagents. CoRR,abs/2308.11432,2023. doi: 10.48550/ARXIV.
2308.11432. URLhttps://doi.org/10.48550/arXiv.2308.11432.
YulongWang,TianhaoShen,LifengLiu,andJianXie. Sibyl: Simpleyeteffectiveagentframework
forcomplexreal-worldreasoning. CoRR,abs/2407.10718,2024b. doi: 10.48550/ARXIV.2407.
10718. URLhttps://doi.org/10.48550/arXiv.2407.10718.
ZoraZhiruoWang,JiayuanMao,DanielFried,andGrahamNeubig. Agentworkflowmemory,2024c.
URLhttps://arxiv.org/abs/2409.07429.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi,
QuocV.Le,andDennyZhou. Chain-of-thoughtpromptingelicitsreasoninginlargelanguage
models. InSanmiKoyejo,S.Mohamed,A.Agarwal,DanielleBelgrave,K.Cho,andA.Oh(eds.),
AdvancesinNeuralInformationProcessingSystems35:AnnualConferenceonNeuralInformation
Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December
9, 2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
9d5609613524ecf4f15af0f7b31abca4-Abstract-Conference.html.
LionelWong,GabrielGrand,AlexanderK.Lew,NoahD.Goodman,VikashK.Mansinghka,Jacob
Andreas,andJoshuaB.Tenenbaum. Fromwordmodelstoworldmodels: Translatingfromnatural
languagetotheprobabilisticlanguageofthought. CoRR,abs/2306.12672,2023. doi: 10.48550/
ARXIV.2306.12672. URLhttps://doi.org/10.48550/arXiv.2306.12672.
MengsongWu,TongZhu,HanHan,ChuanyuanTan,XiangZhang,andWenliangChen. Seal-tools:
Self-instructtoollearningdatasetforagenttuninganddetailedbenchmark. CoRR,abs/2405.08355,
2024. doi: 10.48550/ARXIV.2405.08355. URL https://doi.org/10.48550/arXiv.
2405.08355.
Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li,
LiJiang,XiaoyunZhang,andChiWang. Autogen: Enablingnext-genLLMapplicationsviamulti-
agentconversationframework. CoRR,abs/2308.08155,2023. doi: 10.48550/ARXIV.2308.08155.
URLhttps://doi.org/10.48550/arXiv.2308.08155.
Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Jun-
zheWang,SenjieJin,EnyuZhou,RuiZheng,XiaoranFan,XiaoWang,LimaoXiong,Yuhao
Zhou, Weiran Wang, Changhao Jiang, Yicheng Zou, Xiangyang Liu, Zhangyue Yin, Shihan
Dou, Rongxiang Weng, Wensen Cheng, Qi Zhang, Wenjuan Qin, Yongyan Zheng, Xipeng
Qiu, Xuanjing Huan, and Tao Gui. The rise and potential of large language model based
agents: A survey. CoRR, abs/2309.07864, 2023. doi: 10.48550/ARXIV.2309.07864. URL
https://doi.org/10.48550/arXiv.2309.07864.
Jiannan Xiang, Tianhua Tao, Yi Gu, Tianmin Shu, Zirui Wang, Zichao Yang, and Zhiting Hu.
Language models meet world models: Embodied experiences enhance language models. In
Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine
(eds.),AdvancesinNeuralInformationProcessingSystems36: AnnualConferenceonNeural
17Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 -
16,2023,2023. URLhttp://papers.nips.cc/paper_files/paper/2023/hash/
ee6630dcbcff857026e474fc857aa9f0-Abstract-Conference.html.
Ruixuan Xiao, Wentao Ma, Ke Wang, Yuchuan Wu, Junbo Zhao, Haobo Wang, Fei Huang, and
YongbinLi. Flowbench: Revisitingandbenchmarkingworkflow-guidedplanningforllm-based
agents. CoRR,abs/2406.14884,2024. doi: 10.48550/ARXIV.2406.14884. URLhttps://doi.
org/10.48550/arXiv.2406.14884.
CanXu,QingfengSun,KaiZheng,XiuboGeng,PuZhao,JiazhanFeng,ChongyangTao,Qingwei
Lin,andDaxinJiang.Wizardlm:Empoweringlargepre-trainedlanguagemodelstofollowcomplex
instructions. InTheTwelfthInternationalConferenceonLearningRepresentations,ICLR2024,
Vienna,Austria,May7-11,2024.OpenReview.net,2024. URLhttps://openreview.net/
forum?id=CfXh93NDgH.
XiangyuanXue,ZeyuLu,DiHuang,WanliOuyang,andLeiBai. Genagent: Buildcollaborative
aisystemswithautomatedworkflowgeneration–casestudiesoncomfyui,2024. URLhttps:
//arxiv.org/abs/2409.01392.
An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li,
ChengyuanLi,DayihengLiu,FeiHuang,GuantingDong,HaoranWei,HuanLin,JialongTang,
JialinWang,JianYang,JianhongTu,JianweiZhang,JianxinMa,JianxinYang,JinXu,Jingren
Zhou,andetal. Qwen2technicalreport. CoRR,abs/2407.10671,2024a. doi: 10.48550/ARXIV.
2407.10671. URLhttps://doi.org/10.48550/arXiv.2407.10671.
John Yang, Akshara Prabhakar, Karthik Narasimhan, and Shunyu Yao. Intercode: Stan-
dardizing and benchmarking interactive coding with execution feedback. In Alice Oh,
Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.),
Advances in Neural Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December
10 - 16, 2023, 2023a. URL http://papers.nips.cc/paper_files/paper/
2023/hash/4b175d846fb008d540d233c188379ff9-Abstract-Datasets_
and_Benchmarks.html.
KeYang,JiatengLiu,JohnWu,ChaoqiYang,YiR.Fung,ShaLi,ZixuanHuang,XuCao,Xingyao
Wang,YiquanWang,HengJi,andChengxiangZhai. IfLLMisthewizard,thencodeisthewand:
A survey on how code empowers large language models to serve as intelligent agents. CoRR,
abs/2401.00812, 2024b. doi: 10.48550/ARXIV.2401.00812. URLhttps://doi.org/10.
48550/arXiv.2401.00812.
Rui Yang, Lin Song, Yanwei Li, Sijie Zhao, Yixiao Ge, Xiu Li, and Ying Shan. Gpt4tools:
Teaching large language model to use tools via self-instruction. In Alice Oh, Tristan
Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Ad-
vances in Neural Information Processing Systems 36: Annual Conference on Neural Infor-
mation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,
2023, 2023b. URL http://papers.nips.cc/paper_files/paper/2023/hash/
e393677793767624f2821cec8bdd02f1-Abstract-Conference.html.
ZonghanYang,PengLi,MingYan,JiZhang,FeiHuang,andYangLiu. Reactmeetsactre: When
languageagentsenjoytrainingdataautonomy. CoRR,abs/2403.14589,2024c. doi: 10.48550/
ARXIV.2403.14589. URLhttps://doi.org/10.48550/arXiv.2403.14589.
Zonghan Yang, An Liu, Zijun Liu, Kaiming Liu, Fangzhou Xiong, Yile Wang, Zeyuan Yang,
QingyuanHu,XinruiChen,ZhenheZhang,FuwenLuo,ZhichengGuo,PengLi,andYangLiu.
Position: Towardsunifiedalignmentbetweenagents, humans, andenvironment. InForty-first
InternationalConferenceonMachineLearning,ICML2024,Vienna,Austria,July21-27,2024.
OpenReview.net,2024d. URLhttps://openreview.net/forum?id=DzLna0cFL1.
Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scal-
able real-world web interaction with grounded language agents. In Sanmi Koyejo, S. Mo-
hamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural
18Information Processing Systems 35: Annual Conference on Neural Information Process-
ing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9,
2022, 2022. URL http://papers.nips.cc/paper_files/paper/2022/hash/
82ad13ec01f9fe44c01cb91814fd7b8c-Abstract-Conference.html.
ShunyuYao,DianYu,JeffreyZhao,IzhakShafran,TomGriffiths,YuanCao,andKarthikNarasimhan.
Tree of thoughts: Deliberate problem solving with large language models. In Alice Oh, Tris-
tan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Ad-
vances in Neural Information Processing Systems 36: Annual Conference on Neural Infor-
mation Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16,
2023, 2023a. URL http://papers.nips.cc/paper_files/paper/2023/hash/
271db9922b8d1f4dd7aaef84ed5ac703-Abstract-Conference.html.
ShunyuYao,JeffreyZhao,DianYu,NanDu,IzhakShafran,KarthikR.Narasimhan,andYuanCao.
React:Synergizingreasoningandactinginlanguagemodels.InTheEleventhInternationalConfer-
enceonLearningRepresentations,ICLR2023,Kigali,Rwanda,May1-5,2023.OpenReview.net,
2023b. URLhttps://openreview.net/forum?id=WE_vluYUL-X.
JunjieYe,GuanyuLi,SongyangGao,CaishuangHuang,YilongWu,SixianLi,XiaoranFan,Shihan
Dou,QiZhang,TaoGui,andXuanjingHuang. Tooleyes: Fine-grainedevaluationfortoollearning
capabilities of large language models in real-world scenarios. CoRR, abs/2401.00741, 2024.
doi: 10.48550/ARXIV.2401.00741. URL https://doi.org/10.48550/arXiv.2401.
00741.
YiningYe,XinCong,ShizuoTian,JiannanCao,HaoWang,YujiaQin,YaxiLu,HeyangYu,Huadong
Wang,YankaiLin,ZhiyuanLiu,andMaosongSun. Proagent: Fromroboticprocessautomationto
agenticprocessautomation. CoRR,abs/2311.10751,2023. doi: 10.48550/ARXIV.2311.10751.
URLhttps://doi.org/10.48550/arXiv.2311.10751.
DaYin,FaezeBrahman,AbhilashaRavichander,KhyathiRaghaviChandu,Kai-WeiChang,Yejin
Choi,andBillYuchenLin. Agentlumos: Unifiedandmodulartrainingforopen-sourcelanguage
agents. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Proceedings of the 62nd
AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: LongPapers),ACL
2024,Bangkok,Thailand,August11-16,2024,pp.12380–12403.AssociationforComputational
Linguistics,2024. URLhttps://aclanthology.org/2024.acl-long.670.
Jifan Yu, Xiaozhi Wang, Shangqing Tu, Shulin Cao, Daniel Zhang-Li, Xin Lv, Hao Peng, Zijun
Yao,XiaohanZhang,HanmingLi,ChunyangLi,ZheyuanZhang,YushiBai,YantaoLiu,Amy
Xin, Kaifeng Yun, Linlu Gong, Nianyi Lin, Jianhui Chen, Zhili Wu, Yunjia Qi, Weikai Li,
YongGuan, KaishengZeng, JiQi, HailongJin, JinxinLiu, YuGu, YuanYao, NingDing, Lei
Hou, Zhiyuan Liu, Bin Xu, Jie Tang, and Juanzi Li. Kola: Carefully benchmarking world
knowledge of large language models. In The Twelfth International Conference on Learning
Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL
https://openreview.net/forum?id=AqN23oqraW.
Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Kan Ren, Dongsheng Li, and
DeqingYang. EASYTOOL:enhancingllm-basedagentswithconcisetoolinstruction. CoRR,
abs/2401.06201, 2024. doi: 10.48550/ARXIV.2401.06201. URL https://doi.org/10.
48550/arXiv.2401.06201.
AohanZeng,MingdaoLiu,RuiLu,BowenWang,XiaoLiu,YuxiaoDong,andJieTang. Agent-
tuning: Enablinggeneralizedagentabilitiesforllms. InLun-WeiKu,AndreMartins,andVivek
Srikumar(eds.),FindingsoftheAssociationforComputationalLinguistics,ACL2024,Bangkok,
Thailandandvirtualmeeting,August11-16,2024,pp.3053–3077.AssociationforComputational
Linguistics,2024a. URLhttps://aclanthology.org/2024.findings-acl.181.
AohanZeng,BinXu,BowenWang,ChenhuiZhang,DaYin,DiegoRojas,GuanyuFeng,Hanlin
Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui,
Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie
Huang, and et al. Chatglm: A family of large language models from GLM-130B to GLM-4
all tools. CoRR, abs/2406.12793, 2024b. doi: 10.48550/ARXIV.2406.12793. URL https:
//doi.org/10.48550/arXiv.2406.12793.
19Zhen Zeng, William Watson, Nicole Cho, Saba Rahimi, Shayleen Reynolds, Tucker Balch, and
ManuelaVeloso. Flowmind: Automaticworkflowgenerationwithllms. In4thACMInternational
Conference on AI in Finance, ICAIF 2023, Brooklyn, NY, USA, November 27-29, 2023, pp.
73–81.ACM,2023. doi: 10.1145/3604237.3626908. URLhttps://doi.org/10.1145/
3604237.3626908.
JianguoZhang,TianLan,MingZhu,ZuxinLiu,ThaiHoang,ShirleyKokane,WeiranYao,Juntao
Tan, Akshara Prabhakar, Haolin Chen, Zhiwei Liu, Yihao Feng, Tulika Awalgaonkar, Rithesh
Murthy, Eric Hu, Zeyuan Chen, Ran Xu, Juan Carlos Niebles, Shelby Heinecke, Huan Wang,
SilvioSavarese,andCaimingXiong. xlam: Afamilyoflargeactionmodelstoempoweraiagent
systems,2024a. URLhttps://arxiv.org/abs/2409.03215.
Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. Codeagent: Enhancing code generation
with tool-integrated agent systems for real-world repo-level coding challenges. In Lun-Wei
Ku,AndreMartins,andVivekSrikumar(eds.),Proceedingsofthe62ndAnnualMeetingofthe
AssociationforComputationalLinguistics(Volume1:LongPapers),ACL2024,Bangkok,Thailand,
August11-16,2024,pp.13643–13658.AssociationforComputationalLinguistics,2024b. URL
https://aclanthology.org/2024.acl-long.737.
HuaixiuStevenZheng,SwaroopMishra,HughZhang,XinyunChen,MinminChen,AzadeNova,
LeHou,Heng-TzeCheng,QuocV.Le,EdH.Chi,andDennyZhou. NATURALPLAN:bench-
markingllmsonnaturallanguageplanning. CoRR,abs/2406.04520,2024a. doi: 10.48550/ARXIV.
2406.04520. URLhttps://doi.org/10.48550/arXiv.2406.04520.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao
Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez,
and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In Alice Oh,
Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.),
Advances in Neural Information Processing Systems 36: Annual Conference on Neural
Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December
10 - 16, 2023, 2023. URL http://papers.nips.cc/paper_files/paper/2023/
hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_
Benchmarks.html.
YaoweiZheng,RichongZhang,JunhaoZhang,YanhanYe,ZheyanLuo,andYongqiangMa. Lla-
mafactory: Unifiedefficientfine-tuningof100+languagemodels. CoRR,abs/2403.13372,2024b.
doi: 10.48550/ARXIV.2403.13372. URL https://doi.org/10.48550/arXiv.2403.
13372.
WangchunshuZhou,YuchenEleanorJiang,LongLi,JialongWu,TiannanWang,ShiQiu,Jintian
Zhang, Jing Chen, Ruipu Wu, Shuai Wang, Shiding Zhu, Jiyu Chen, Wentao Zhang, Ningyu
Zhang,HuajunChen,PengCui,andMrinmayaSachan. Agents: Anopen-sourceframeworkfor
autonomouslanguageagents. CoRR,abs/2309.07870,2023. doi: 10.48550/ARXIV.2309.07870.
URLhttps://doi.org/10.48550/arXiv.2309.07870.
WangchunshuZhou,YixinOu,ShengweiDing,LongLi,JialongWu,TiannanWang,JiaminChen,
ShuaiWang,XiaohuaXu,NingyuZhang,HuajunChen,andYuchenEleanorJiang. Symbolic
learningenablesself-evolvingagents. CoRR,abs/2406.18532,2024. doi: 10.48550/ARXIV.2406.
18532. URLhttps://doi.org/10.48550/arXiv.2406.18532.
YuqiZhu,ShuofeiQiao,YixinOu,ShuminDeng,NingyuZhang,ShiweiLyu,YueShen,LeiLiang,
JinjieGu,andHuajunChen. Knowagent: Knowledge-augmentedplanningforllm-basedagents.
CoRR,abs/2403.03101,2024. doi: 10.48550/ARXIV.2403.03101. URLhttps://doi.org/
10.48550/arXiv.2403.03101.
Max Zuo, Francisco Piedrahita Velez, Xiaochen Li, Michael L. Littman, and Stephen H. Bach.
Planetarium: Arigorousbenchmarkfortranslatingtexttostructuredplanninglanguages. CoRR,
abs/2407.03321, 2024. doi: 10.48550/ARXIV.2407.03321. URL https://doi.org/10.
48550/arXiv.2407.03321.
20A APPENDIX
A.1 SOURCEDATAINFORMATION
Inordertofacilitateabetterunderstandingofourpaper,hereweprovideadetailedexpositionofthe
datasetutilizedinourpaper.
Held-inTasks
• ToolBench(Qinetal.,2024). Toolbenchisafunctioncalldatasetgeneratedbyselectingseveral
APIs from an API library and synthesizing instructions using GPT-3.5. Due to the potentially
disparatenatureoftheextractedAPIs,manyinstructionswithinToolbenchmayposechallengesin
logicalcomprehension.Toaddressthisissue,wemanuallyestablishtemplatesandfilterthembased
onthecomplexityoffunctioninvocations. Additionally,thereisanotherversionofToolBench
calledStableToolBench(Guoetal.,2024b)thatutilizesGPT-4andcachingmechanismstoachieve
stableAPIcalls,whichweuseforend-to-endtaskevaluation.
• ToolAlpaca(Tangetal.,2023).TheconstructionmethodofToolAlpacaisakintothatofToolBench.
Thequalityofitsinstructionsisrelativelyhigher,yetmostinstructionscanbeexecutedwithin1-3
functioncalls. Wefilteroutinstructionswithonlyasinglefunctioncall.
• ALFWorld(Shridharetal.,2021).ALFWorldisahouseholddatasetrequiringtheagenttonavigate
throughtheroomandmanipulateobjects. Itincludeshuman-annotatedgoldtrajectories,whichwe
directlyutilizetoconstructnodechainsandworkflowgraphs.
• WebShop(Yaoetal.,2022). WebShopisanonlineshoppingdatasetinawebsiteenvironment. We
usethegoldtrajectoriescollectedthroughGPT-4inSongetal.(2024)toconstructourbenchmark.
• OS (Liu et al., 2024b). OS is an interaction dataset based on operating systems, where agents
arerequiredtocompleteoperationsthroughshellcommands. Wegathergoldtrajectoriesfrom
AgentInstruct(Zengetal.,2024a)toaidintheconstructionofourbenchmark.
• LUMOS(Yinetal.,2024). LUMOSisadatasetthatmodelsagentplanning-relateddatasetsusing
aunifiedformat. Within LUMOS,theOnePassplanningdata(LUMOS-O)alignswellwithour
workflowconstruction. Weselectthemath,commonsense,andmultimodalreasoningcomponents
asthefoundationforbuildingourreasoningtasks’workflow.
• WikiHow(Koupaee&Wang,2018). WikiHowcontainsopen-worldplanningtasksandcomplex
linearprocessdata. Wecollectandfilterdatabasedontasktopicsanddirectlyconstructgraph
workflowsbasedontheprocessdata.
Held-outTasks
• Seal-Tools (Wu et al., 2024). Seal-Tools is a function call dataset obtained entirely through
self-instructionusingChatGPT.
• InterCodeSQL(Yangetal.,2023a). InterCodeSQLisanembodieddatasetthatgeneratesSQL
instructionsbasedonuserintentandinteractswithagraphdatabase.
A.2 HUMANVERIFICATION
WedividethetestdataintofivepartsandinvitefiveNLPvolunteerstoevaluatethequalityofthe
nodechainsandworkflowgraphsbasedonthefollowingprinciples:
1. Granularity.Thedecompositionofthenodechainshouldmeetthesmallestexecutablegranularity
thatcanbederivedfromthetaskdescriptionandactionlist. Thismeansthatnodesshouldnot
combinesubtasks(indicatinggranularityistoolarge)orintroduceinformationthatcannotbe
obtainedfromexistinginformation(modelself-association,indicatinggranularityistoosmall).
2. Logic. Theworkflowgraphshouldfollowlogicalsequencingthatsatisfiestheexecutionrelation-
shipsbetweennodes.
3. TaskQuality. Thetasksthemselvesshouldnotexhibitanyobviousqualityissues.
Wediscarddatathatdoesnotadheretotheabovecriteriaandobtainthefinaltestset.
21Train Test
Average: 4.17
400
9.65%
26.77% 21.85%
33.69% 300
21.67%
200
24.62%
26.77% 12.21% 22.79%
100
Total 18679 Total 2146
0
Function Call Embodied Held-out 0 2 4 6 8 10 12 14 16 18 20
Problem Solving Open Grounded Steps
Figure 7: Statistics of Our Benchmark. We also Figure8: WorkflowStepsDistributiononthe
includeheld-outtasksinthetestset. wholebenchmark.
A.3 BENCHMARKSTATISTICS
Figure 7 illustrates the statistics of our benchmark. Our training set comprises 18,679 instances,
withdataevenlydistributedacrossfourtypes: functioncall,problem-solving,embodied,andopen-
grounded.Thetestsetconsistsof2,146instances,with33.69%dedicatedtoheld-outtaskstoevaluate
thegeneralizationcapabilityofthetrainedmodel. Figure8isthedistributionofourbenchmarkbased
onthenumberofnodesintheworkflow. Themajorityofthedataisintherangeof2to10steps,
withasmallerportionfallingwithinthe10to20stepsrange. Theaveragenumberofnodesacrossall
datapointsis4.17.
A.4 TRAININGSETUPS
Wefine-tuneQwen-2-7BandInternLM-2.5-7BwithfullparametersusingDeepSpeed(Rasleyetal.,
2020). Weincludeatwo-shotpromptintheinputtoenhancethemodel’sgeneralizationduringboth
trainingandtesting. Bothmodelsutilizeidenticalhyperparameters. Thedetailedhyperparameter
settingsareoutlinedinTable5. Alltheexperimentsareconductedon3NVIDIA80GBA100GPUs.
Table5: Detailedtraininghyperparametersusedinourpaper.
Name Value
cutofflen 4,096
epochs 3
batchsize 12
batchsizeperdevice 2
gradientaccumulationsteps 2
learningrate 1e-5
lrschedulertype cosine
warmupratio 0.1
bf16 true
A.5 CASESTUDYOFO1
Below are two cases we test on OpenAI’s current most powerful reasoning model, o1 (OpenAI,
2024). Inthefirstcase,o1successfullyidentifiesvarioussubtasksbutmakesanerrorinpredicting
thedependencybetweennode1andnode2/3whengeneratingthegraph,failingtorecognizetheir
parallel relationship. In the second case, based on environmental priors, the agent needs to first
locate the potato and then cool it using the fridge. Due to lacking this specific knowledge, o1
assumesthepotatoisalreadyinsidethefridge,thusmissingthesubtaskoffindingthepotato. These
observationsconfirmwhathasbeenmentionedinValmeekametal.(2024)thatLargeReasoning
Models(LRM)stillcan’tplan. Weanalyzethatthecomplexityofplanningliesintheinteractionwith
theenvironment,whichrequiresasubstantialamountofenvironmentalcommonsenseknowledge.
22
muN
tnioP
ataDTaskslikemathreasoningorcodegenerationonlyinvolvestep-by-stepdeductionfromtheproblemto
thesolution. Therefore,toenhancetheplanningabilityofLLMagents,integratingworldknowledge
(Yuetal.,2024;Qiaoetal.,2024a;Guettaetal.,2024)orworldmodels(Dawid&LeCun,2023;Hu
&Shu,2023;Wongetal.,2023)intotheagentsystemremainscrucial. Thisisalsowhyacorrect
workflowcanserveaspriorknowledgetoenhanceagentplanning.
CaseStudyofo1(a)
Task: Pleaseanalyzetheaccess_logs.txtfileusingthemachinelearningalgorithmtodetect
any potential malicious activity. Afterwards, retrieve the network policy for the library
withID‘wscDOqa63Giq’regardinginternetaccess. Finally,getthedigitalPRmetricsfor
the‘BeautyRevolution’campaignthattookplacefrom‘2022-01-01’to‘2022-12-31’on
platformssuchasTwitter,Facebook,andInstagram.
Thetoollistyoucanselectfrom: [tool_list]
GoldNodesandEdges
Node:
1: Analyzeaccess_logs.txtforpotentialmaliciousactivityusingmachinelearning.
2: Retrievenetworkpolicyforlibrary‘wscDOqa63Giq’regardinginternetaccess.
3: GetdigitalPRmetricsfor‘BeautyRevolution’campaignfrom‘2022-01-01’to‘2022-12-
31’onTwitter,Facebook,andInstagram.
Edge:
(START,1)(START,2)(START,3)(1,END)(2,END)(3,END)
PredictedNodesandEdges
Node:
a. Analyzetheaccess_logs.txtfileforpotentialmaliciousactivityusingamachinelearning
algorithm.
b. RetrievethenetworkpolicyforthelibrarywithID‘wscDOqa63Giq’regardinginternet
access.
c. GetthedigitalPRmetricsforthe‘BeautyRevolution’campaignfrom‘2022-01-01’to
‘2022-12-31’onTwitter,Facebook,andInstagram.
Edge:
(START,1)(1,2)(1,3)(2,END)(3,END)
CaseStudyofo1(b)
Task: Youareinthemiddleofaroom. Lookingquicklyaroundyou,youseeacabinet20,a
cabinet19,acabinet18,acabinet17,acabinet16,acabinet15,acabinet14,acabinet13,
acabinet12,acabinet11,acabinet10,acabinet9,acabinet8,acabinet7,acabinet6,a
cabinet5,acabinet4,acabinet3,acabinet2,acabinet1,acoffeemachine1,acountertop2,
acountertop1,adiningtable2,adiningtable1,adrawer6,adrawer5,adrawer4,adrawer3,
adrawer2,adrawer1,afridge1,agarbagecan1,amicrowave1,asinkbasin1,astoveburner
4,astoveburner3,astoveburner2,astoveburner1,andatoaster1. Yourtaskisto: puta
coolpotatoingarbagecan.
Theactionlistyoucanselectfrom:
1. gotorecep
2. taskobjfromrecep
3. putobjin/onrecep
4. openrecep
5. closerecep
6. toggleobjrecep
7. cleanobjwithrecep
8. heatobjwithrecep
9. coolobjwithrecep
whereobjandrecepcorrespondtoobjectsandreceptacles.
GoldNodesandEdges
23Node:
1: gotowherethepotatoislocated
2: takepotatofromwhereitislocated
3: gotofridge
4: coolpotatowithfridge
5: gotogarbagecan
6: putpotatoin/ongarbagecan.
Edge:
(START,1)(1,2)(2,3)(3,4)(4,5)(5,6)(6,END)
PredictedNodesandEdges
Node:
Gotofridge1
Takecoolpotatofromfridge1
Gotogarbagecan1
Putpotatoingarbagecan1
Edge:
(START,1)(1,2)(2,3)(3,4)(4,END)
A.6 PROMPTFORBENCHMARKCONSTRUCTION
PromptforNodeChainGeneration
FunctionCallTasks
Hereisamulti-hopqueryandasegmentofitssolutiontrajectoryinconversationformat.
Pleasesummarizethecontentofthelaststepbasedonthe"Action"results, andgenerate
a node task. Node tasks should focus on the API related to the current action, and there
shouldbenoduplicationbetweennodetasks,makesurethetaskisclear,conciseaccurate,
andspecific.
PleasenotethatthetracemaycontainsomeAPIcallerrors.Pleaseignoretheerrorsandfocus
onthelastThoughtandActionstep. Nodetaskfocusesmoreonplanningratherthanthe
specificAPIcallortheresultsafterexecutionsothatspecificAPInamesshouldnotappear.
Herearetwoexamplesforyou:
Examples
Nowit’syourturn.
Query: Query
Trajectory: TrajectorySegment
EmbodiedTasks
Iwillprovideyouwithananalysisofasuccessfultrajectoryofataskthatinteractswiththe
environment. Pleaseidentifythekeyfactorsthatcontributetosuccess. Basedonthisanalysis,
youneedtogenerateworkflowtohelpincreasethesuccessrateoffutureendeavors. The
availableactionsare: ActionList
Youroutputshouldbeasequenceofsubtasksthatyouwouldtaketocompletethetaskandin
theformatof: Workflow: A->B->C
Herearetwoexamplesforyou:
Examples
Nowit’syourturn.
Query: Query
Trajectory: Trajectory
PromptforWorkflowGraphConstruction
Youareaplannerwhoisgoodattaskplanning. Next,Iwillgiveyouataskandsomenode
ofthesubtasks. Somesubtasksmaynotbedependentoneachother,whileothersmayhave
24dependencies. Pleaseconvertthesenodesofsubtasksintoatopologydiagrambasedonthe
taskrelevanceintheworkflow. TheGraphshouldstartwiththeSTARTnode,andendwith
theENDnode.
Herearetwoexamplesforyou:
Examples
Nowit’syourturn.
Task: Task
Nodes: SubtaskNodes
25