Strategic Classification with Externalities
Yiling Chen Safwan Hossain
Harvard University Harvard University
yiling@seas.harvard.edu shossain@g.harvard.edu
Evi Micha∗ Ariel Procaccia
University of Southern California Harvard University
pmicha@usc.edu arielpro@seas.harvard.edu
Abstract
We propose a new variant of the strategic classification problem: a principal reveals a clas-
sifier, and n agents report their (possibly manipulated) features to be classified. Motivated
by real-world applications, our model crucially allows the manipulation of one agent to affect
another;thatis,itexplicitly capturesinter-agentexternalities. Theprincipal-agentinteractions
are formally modeled as a Stackelberg game, with the resulting agent manipulation dynamics
captured as a simultaneous game. We show that under certain assumptions, the pure Nash
Equilibrium of this agent manipulation game is unique and canbe efficiently computed. Lever-
aging this result, PAC learning guarantees are established for the learner: informally, we show
thatitispossibletolearnclassifiersthatminimizelossonthedistribution,evenwhenarandom
number of agents are manipulating their way to a pure Nash Equilibrium. We also comment
on the optimization of such classifiers through gradient-based approaches. This work sets the
theoreticalfoundationsforamorerealisticanalysisofclassifiersthatarerobustagainstmultiple
strategic actors interacting in a common environment.
1 Introduction
Machine learning algorithms are increasingly deployed in high-stakes decision making, including
loan applications, school admissions, hiring, and insurance claims (Bejarano Carbo, 2021; Harwell,
2022; Kumar et al., 2022). Relying on past data as a reference, these algorithms use features of a
candidate to determine their merit for the given task. The interactions in these settings, however,
often involve strategic agents who may manipulate their features if doing so yields a more favorable
outcome. This presents a significant challenge to the algorithm if it is not trained to anticipate
such behavior since the training and test distributions no longer match. Correspondingly, a large
and growing literature on strategic classification has emerged to understand the dynamics of this
behavior and propose strategies for learning in such settings (Hardt et al., 2016).
By and large, the existing literature roughly models the interaction as follows: The learner (or
principal) deploys a classifier, and an agent with feature x observes this and may choose to report
a manipulated feature x to obtain a better outcome. Manipulation is not considered free since it
′
may involve some additional effort or risk. While the classifier may be deployed for any number
of agents, agent interactions with the learner are crucially assumed to be independent; in other
words, one agent’s action has no influence on another. We posit that this ignores a critical aspect
∗
Part of thiswork was donewhile Dr. Micha was a post-doctoral researcher at Harvard University.
1
4202
tcO
01
]TG.sc[
1v23080.0142:viXraof multi-agent interactions in a shared environment: their actions exert externality on one another.
This is not a new observation: indeed, the notion that agents in a system are affected by a common
resource has been a core component of economic models for over a century (Pigou, 1920; Coase,
1960). It has not, however, been formally studied in the strategic classification setting despite its
relevance and applicability.
Consider the example Hardt et al. (2016) used to initiate this literature: the number of books
in the parent’s household may bea valid feature for university admissions given its correlation with
studentsuccess(Evans et al.,2010). Since“booksarerelatively cheap”, thisallows parentstogame
the system by buying“an attic full of unread books”(Hardt et al., 2016). It is immediate, however,
that if all parents do this, then the price of books no longer stays cheap. This is the externality
induced by everyone’s actions. Externality, however, can not only model demand for lucrative
features but also the additional risk or burden associated with manipulation. When a university
is deciding which students to accept from a high school using a classification algorithm, applicants
may report features such as class rank, GPA, volunteering affiliations, and so on. Naturally, only a
handfulofstudentscanbeinthetopoftheirclassorbeaffiliatedwithaspecificorganization. Ifonly
a handful of students manipulate their features, it may not be statistically discernible. However,
if many claim to be at the top of their class or work with the same organization, this can stand
out and lead the university to audit the applications further. Similarly, a few applicants inflating
their credit score on a loan application may be treated as an anomaly; a large pool of applicants
doing so becomes a systematic problem that demands a reexamination of the loan process, causing
a burden on all.
The preceding examples illustrate that the demand for books, the risk faced by a student, or
the burden on the borrower is not solely determined by an individual’s action but also influenced
by the interactions of others within the same system. This naturally affects how, what, and if
an agent manipulates. This phenomenon applies to most settings where strategic classification is
pertinent. As such, it is imperative to correctly model and understand this phenomenon to ensure
that classifiers deployed in sensitive settings act as intended. We specifically study the following
questions:
How should we jointly model the strategic interactions between agents alongside those with the
•
learner?
What is the appropriate notion of equilibrium in this setting and what properties does it have?
•
What learning guarantees can be given for classifiers in such settings?
•
1.1 Our Contributions
We study the problem of deploying classifiers in strategic multi-agent settings with inter-agent
externalitiesfromatheoreticalperspective. Consistentwithpriorliterature,theinteractionbetween
thelearnerandtheagents ismodeledasaStackelberggame: thelearnerfirstcommitstoaclassifier
to which agents can respond. The resulting inter-agent interactions are captured as a simultaneous
game, and theStackelberg-Nash Equilibrium is proposedas thesolution concept for all interactions.
We precisely define these aspects of the multi-agent strategic classification game in Section 2.
Learning in this setting is challenging, not least due to the possible multiplicity of equilibrium,
their computation, and their dynamics due to a changing classifier. Section 3 motivates a set of
structural assumptions on the cost and externality, under which the inter-agent simultaneous game
has a uniquepure Nash equilibrium that is efficiently computable. Building on this insight, Section
4 comments on the regularity of the Nash equilibrium under changing classifiers, and provides
probably approximately correct (PAC) learning guarantees for computing the Stackelberg-Nash
2equilibrium. Intuitively, itispossibletolearnloss-minimizingclassifiersthatgeneralizetoarandom
number of agents manipulating to achieve a Nash Equilibrium. We also differentiate through
the equilibrium solution to explicitly characterize the loss gradient, illustrating the feasibility of
gradient-based optimization algorithms. Section 5 uses our motivating examples to model some
externality functions that can be captured by our general framework. Lastly, in Section 6, we
discuss some limitations of our work alongside technical and conceptual extensions.
1.2 Related Work
Ourworkbuildsonthegrowingliteratureonstrategicclassification(Hardt et al.,2016;Bru¨ckner et al.,
2012; Bru¨ckner & Scheffer, 2009). In thebasic setting, a learner seeks to release a classifier that ac-
counts for the fact that strategic agents may misreporttheir true features to maximize their utility,
which is determined by the likelihood of a positive outcome and the cost incurred for misreporting.
Recent papers have studied extensions of the basic strategic classification setting. For example,
Levanon & Rosenfeld (2022) introduce a setting where agents’ utility functions capture different
intentions beyond simply maximizing for the positive outcome. Others aim to account for limited
information (Ghalme et al.,2021; Bechavod et al.,2022), unknownutilities (Dong et al., 2018) and
causal effects (Miller et al., 2020; Horowitz & Rosenfeld, 2023). These extensions do not handle
externalities or multi-agent behaviour in general.
While motivated by a different aspect of strategic classification, the work of Eilat et al. (2023),
which studies strategic classification in a graph setting, has conceptual similarities with our work.
Node classification on graphs naturally depends on neighboring nodes’ features, which strategic
agents canexploit. Whilethisimplicitly modelsagentinteractions asnotwhollyindependent,there
are several fundamental differences from our work. First, agents in their model are interrelated due
to the classifier outcome of one agent depending on neighboring nodes; in contrast, we consider
the classifier outcome for an agent to only depend on their feature, with the externality explicitly
capturing additional risk or cost to agents due to others also interacting in the system. This better
aligns with our motivation to capture classification dynamics in competitive high-stakes settings.
Furthermore, they consider agents myopically best responding over a sequence of rounds whereas
we consider the performance of a classifier at a Nash equilibrium.
Further afield, machine learning researchers have investigated the effect of strategic behavior
on social welfare (Milli et al., 2019; Haghtalab et al., 2020; Kleinberg & Raghavan, 2020) or on dif-
ferent groups of agents (Hu et al., 2019). However, none of these papers consider the direct impact
that manipulation by others can have on each individual agent. The impact of such externalities
has, nonetheless, beenstudied in computational problemslike auctions (Agarwal et al.,2020), data
markets (Hossain & Chen, 2024), and facility location (Li et al., 2019).
2 Model
Preliminaries: For k N, let [k] = 1,...,k . We consider k strategic agents interacting with
∈ { }
a learner who releases a classifier parametrized by weights ω. Using one of our running examples,
this corresponds to k students applying for admission to a university, which decides to accept or
reject using classifier f . Let x Rd and y 1,+1 denote the feature vector and true class
ω i i
∈ ∈ {− }
of agent i respectively, with (x ,y ) . Let X Rk d denote the k agents’ feature matrix, and
i i ×
∼ D ∈
y 1,+1 k the vector of their true class labels1, sampled independently from ; we use the
∈ {− } D
1In general, we use bold capital letters to refer to matrices, and bold lower case letters to refer to vectors.
3shorthand
(X,y) , k.
∼ D×···×D D
k
We use the notation X
i:
and X
:j
to respectiv|ely r{ezfer to} the ith row and jth column of the matrix
X, and [X ;X ] to denote two matrices concatenated along rows.
1 2
Our focus will be on binary linear classifiers due to their wide-spread practical usage and popu-
laritywithinthestrategicclassificationliterature2 (Hardt et al.,2016;Dong et al.,2018;Bechavod et al.,
2022); that is, ω Ω Rd and f (x) = x,ω denotes the score for positive classification. The
ω
∈ ⊆ h i
learner may use any Lipschitz loss function3 to compute the loss and maximize the accuracy of
their deployed classifier with respect to the true labels.
Utility and Externality: Following the standard strategic classification model, we consider a
Stackelberginteraction betweenthelearnerandtheagents (Hardt et al.,2016). Thatis,thelearner
first releases a classifier f , and thereafter, agents submit their features to be classified. An agent
ω
need not be truthful and may instead submit a manipulated feature vector x to receive a higher
′i
score forthepositive class. Consistent withthestrategic classification literature (Dong et al.,2018;
Bechavod et al.,2022),weassumeagentutilities areproportionaltothescore: f (x )g+(x ),where
ω ′i i
g+(x ) :Rd R is an agent specific gain for positive classification4.
i
→
In line with prior work, we assume agent features lie within a bounded region (without loss of
generality [0,1]d) and they incur a cost c(x ,x ) in modifying their true feature vector. However,
i ′i
in contrast to these earlier models, we also consider agents’ decisions causing externality to others.
Conceptually, externality is theimpactagents’ actions haveon oneanother wheninteracting within
a common system. Formally, the negative externality suffered by agent i due to an agent j is
capturedbythefunctiont(x ,x ,x ,x ). WeuseT(x ,x ,X,X ) = t(x ,x ,x ,x )todenote
i ′i j ′j i ′i ′ j=i i ′i j ′j
the total externality suffered by agent i dueto all other participating age6 nts’ decision. Insummary,
P
the total utility of an agent i in reporting x in our multi-agent strategic classification game is:
′i
u (X,X ,f ) =f (x )g+(x ) c(x ,x ) T(x ,x ,X,X )
i ′ ω ω ′i i i ′i i ′i ′
− −
The standard literature on strategic classification assumes the principal knows the cost function
(Hardt et al., 2016; Milli et al., 2019; Levanon & Rosenfeld, 2021). We extend this assumption to
externality functions as well.
Game-Theoretic Model: Since an agent’s total utility depends on others’ reports, the inter-
action between the agents is naturally modeled as a game. Taking inspiration from prior work
on strategic regression (Chen et al., 2018; Hossain & Shah, 2021) and related settings (Nakamura,
2015), we model this inter-agent interaction as a simultaneous game. Correspondingly, the best
response of agent i given classifier f and reports of others is:
ω
argmaxu (X,X ,f )
i ′ ω
x′ [0,1]d
i∈
For a fixed classifier f , the natural solution concept for the agent game is a pure Nash Equi-
ω
librium (PNE). Formally, a set of reported values Xq = [xq ;...;xq ] is a PNE if for each agent i
i i
2Ourresults can be generalized to multi-class linear classifiers.
3This includesnearly any reasonable loss function including HingeLoss, Cross Entropy Loss, MSE, etc.
4It may be common to associate a type vector θ i with an agent and express gain in terms of g+(θ i,x i). This is
equivalent toour utility model as thefeature vector can include thetype. This typeof dependencecan beextended
to our cost and externality functions as well.
4and fixing the reported values of others, xq is a best response for agent i. Intuitively, no agent can
i
unilaterally improve their total utility at this equilibrium. Since PNE is not necessarily unique, let
NE(X,f ) denote the correspondence from the true features to the set of features reported at an
ω
equilibrium under classifier f . That is: Xq NE(X,f ).
ω ω
∈
Learning Problem: While the relationship between the k agents corresponds to a simultaneous
game, the principal and agent interactions still induce a Stackelberg game since the principal first
commits to a classifier f . The Stackelberg equilibrium in the classical setting is with respect
ω
to a single agent best responding under the released classifier; however, our multi-agent setting
implies the principal must now react according to the Nash Equilibrium induced by their chosen
classifier. Thus, the principal’s goal is to compute the Stackelberg-Nash equilibrium (Xu et al.,
2016), which corresponds to a classifier that performs optimally when sampled features reach a
pure Nash Equilibrium with respect to the chosen classifier.
Since the number of participants in the classification setting (e.g., the number of students
applyingforadmissions)isnotfixed,letk denotethemaximumnumberofsuchparticipants,and
max
let denote a distribution over [k ]. The principal has access to a dataset of n labeled samples
max
K
= (X 1,y 1,k 1),...,(X n,y n,k n) , where sample j consists of k
j
and (X j,y j) kj. That
S { } ∼ K ∼ D
is, the jth samples consist of the features and labels of the k participating agents. The principal
j
uses this training set to learn a classifier that performs well in practice. The optimal classifier is
S
formally defined as follows:
Definition 1. The optimal classifier parameter ω in the multi-agent strategic classification game
∗
with loss function ℓ, participant distribution , and data distribution is given by:
K D
k
ω = argminE E max ℓ(y ,f (xq )) .
∗ k (X,y) k i ω i
ω Ω ∼K ∼D Xq NE(X,fω)" #
∈ ∈ Xi=1
The learning problem in this Stackelberg-Nash game is non-trivial for several reasons. First, it
requires optimizing over the PNE of the sampled feature vectors. We discuss the efficient compu-
tation of this equilibrium in Section 3 under structural assumptions on the cost and externality.
Second, we must not only generalize with respect to the equilibrium of sampled agents but also
when the number of such agents is random. We address this and the optimization of f from a
ω
dataset in Section 4.
S
3 Equilibrium Properties
Coreto our problem is computingthe PNE achieved by the sampled agents since it informsthe loss
suffered by the principal under their chosen classifier. Thus, understanding equilibrium properties
such as existence, uniqueness, and computation is crucial to any learning algorithm. However, such
properties cannot be precisely stated without a structured model of the agent utilities, which in
our game is largely predicated upon the cost and externality. We formally state our assumptions
below:
1. The cost is given by the ℓ norm of the manipulation vector: c(x ,x )= α x x 2
2 i i
||
i
−
′i||2
2. Externalities are pairwise symmetric: i,j, t(x ,x ,x ,x ) = t(x ,x ,x ,x ).
∀
i ′i j ′j j ′j i ′i
3. Total externality faced by any agent i, T(x ,x ,X,X ) is smooth and convex in the manip-
i ′i ′
ulation variables (x ,...,x ).
′1 ′k
5Initial work on strategic classification, including Hardt et al. (2016), assumed the cost function
to be separable, a somewhat restrictive assumption. More recent work models cost as the norm
of the manipulation vector x x as it satisfies natural axioms associated with being a metric
′
−
(Levanon & Rosenfeld, 2022; Horowitz & Rosenfeld, 2023). The most common choice is the ℓ
2
norm, which we adopt (our results do hold for a larger class of cost functions - see the end of this
section).
Structural assumptions on externality models are routine in economic literature. In compet-
itive settings, which describe many high-stakes classification tasks, symmetric externalities are
natural and commonplace (Goeree et al., 2005; Hossain & Chen, 2024). Settings such as data
markets (Agarwal et al., 2020) and facility location (Li et al., 2019) further capture externality
under a linear model, which our model subsumes. To give an example of a non-linear but con-
vex externality, consider the loan application setting where a bank (the learner) is screening
candidates for approval. The more candidates manipulate their reports, the more likely the
bank’s approval process will become stricter, adversely affecting all candidates. The function
t(x ,x ,x ,x ) = ( x x + x x )2, which increases as the manipulation increases can
i ′i j ′j
||
′i
−
i ||2
||
′j
−
j ||2
model this setting. It is convex since the square of a non-negative convex function is convex. While
thisisoneexample, Section 5contains amoredetailed exploration ofexternality models, andshows
that our results hold for a broader class of functions, including non-convex ones.
ReturningtothePNEanalysis ofthemanipulationgame, wefirstshowinTheorem1thatwhen
externalities are pairwise symmetric, the resulting interactions can be characterized by a potential
game. Such games use a single function, the potential function Φ, to encapsulate the difference
in utility to any agent i due to their unilateral change in action (Monderer & Shapley, 1996). We
formally define this below for convenience.
Definition 2. An n player simultaneous game is a potential game if there exists a potential
function Φ : n R mapping from joint action space to the reals such that for any agent i:
A →
u (a ,a ) u (a ,a ) = Φ(a ,a ) Φ(a ,a ), where u is the ith agent’s utility.
i i −i
−
i ′i −i i −i
−
′i −i i
Theorem 1. For any classifier f , if the externalities are pairwise symmetric (Condition 2), the
ω
agent interactions constitute a potential game with the potential function:
k k k
Φ(X,X ,f ) = f (x )g+(x ) c(x ,x ) t(x ,x ,x ,x ).
′ ω ω ′i i i ′i i ′i j ′j
− −
i=1 i=1 i=1 j>i
X X XX
Proof. A potential function encapsulates the change in utility to any agent i due to their unilateral
deviation. For us to claim that the proposed function Φ as a potential function, the following must
hold:
i:u (X,[X ;x ],f ) u (X,[X ;x ],f ) = Φ(X,[X ;x ],f ) Φ(X,[X ;x ],f )
i ′ i ′i ω i ′ i ′i′ ω ′ i ′i ω ′ i ′i′ ω
∀ − − − − − −
Observe that the expression on the left is given by:
[f (x ) f (x )]g+(x ) [c(x ,x ) c(x ,x )] t(x ,x ,x ,x ) t(x ,x ,x ,x )
ω ′i ω ′i′ i ′i i ′i′ i i ′i j ′j i ′i′ j ′j
− − − − −
j=i
X6
We claim this is equivalent to Φ(X,[X ;x ],f ) Φ(X,[X ;x ],f ). Since only x changes
′ i ′i ω − ′ i ′i′ ω ′i
to x , terms involving gain and cost in−the potential function−difference match the above. Simi-
′i′
larly, all externalities terms not involving x remain the same and only the externalities involving
′i
agent i remain. There are exactly k 1 such terms and due to symmetry, this is equivalent to
−
t(x ,x ,x ,x ) t(x ,x ,x ,x ).
j=i i ′i j ′j − i ′i′ j ′j
6
P
6Since all player incentives map to the potential function, PNE strategies exactly correspond to
local maxima of the potential function (Roughgarden, 2010).5 Indeed, if an agent could improve
their utility by modifying their action, the value of the potential function at this updated action
also increases. This has several implications. First, finding an equilibrium is equivalent to finding
such local maxima, and second, properties of local maxima of Φ directly inform the multiplic-
ity/uniqueness of the equilibria. Lastly, suppose agents sequentially play their best response from
any starting point. In that case, the potential function value is strictly increasing, giving a simple
local algorithm for arriving at a PNE.
Our next result shows that the potential function is strictly concave under our set of assump-
tions. Forsuchfunctions,anylocaloptimumisalsotheglobaloptimum,suchanoptimumisunique,
and the PNE must be at this optimum (Neyman, 1997). Correspondingly, the Nash Equilibrium
is unique and can be written as the outcome of a convex optimization problem6. We present this
formally below:
Theorem 2. Under Conditions 1, 2 and 3 the potential function specified in Theorem 1 is strictly
concave. Further, the Nash Equilibrium is unique and given by argmax X′ [0,1]k×dΦ(X,X ′,f ω).
∈
Proof. Duetothepairwisesymmetrycondition, k T(x ,x ,X,X )= 2 k t(x ,x ,x ,x ).
i=1 i ′i ′ i=1 j>i i ′i j ′j
Hence if each T(x ,x ,X,X ) is convex, then k t(x ,x ,x ,x ), the last term in the po-
i ′i ′ Pi=1 j>i i ′i j ′j P P
tential function, is also convex. Combined with the ℓ norm square cost, which is strictly convex,
2
P P
and f which is linear, it immediately follows that that the potential function is strictly concave.
ω
The optimization program for the Nash Equilibrium follows immediately from this.
For concave optimization problems, any local optima is a global optima. Suppose by contra-
diction there are two global optima X ,X , with Φ(X,X ,f ) = Φ(X,X ,f )= m. Then strict
′ ′′ ′ ω ′′ ω
concavity implies: Φ(X,λX +(1 λ)X ,f )> λΦ(X,X ,f )+(1 λ)Φ(X,X ,f ) = m. Since
′ ′′ ω ′ ω ′′ ω
− −
the feasible region is convex, the points on the λX +(1 λ)X are feasible and thus neither X
′ ′′ ′
−
or X could be maximizers.
′′
This implies that NE(X,f ) is now a function mapping true value to the unique PNE. This
ω
resolves the ambiguity of learning in the presence of multiple equilibria. Further, this function is
the outcome of a parameterized convex optimization problem (in terms of the classifier f ).
ω
The result above hinges on the strict concavity of the potential function. While our stated
conditions imply this, we note that Conditions 1 and 3 can be replaced with a weaker one:
1’. c(x ,x )+ t(x ,x ,x ,x ), is smooth and strictly convex7.
i i ′i i j>i i ′i j ′j
We dePnote this as thPe cPumulative impact of manipulation — total cost and the sum of all com-
binations of externality. Due to our pairwise symmetry condition, t(x ,x ,x ,x ) =
i j>i i ′i j ′j
1 n T(x ,x ,X,X ); since convexity is preserved through summation, for any strictly convex
2 i=1 i ′i ′ P P
cost (i.e. Condition 1, ℓ norm squared cost) and any externality where T(x ,x ,X,X ) is convex
P
2 i ′i ′
(Condition 3) the updated condition above is immediately satisfied. This condition however is
more general and allows us to capture a larger class of possibly non-convex externality within our
framework. We have a detailed discussion of some of these richer models in Section 5.
5In this setting, a local maxima refers to a point that is optimal with respect to changes in any one direction.
6Since we are maximizing a concave objective underconvexconstraints, this is a convex optimization problem.
7By strict convexity,we mean theHessian is positive definite.
74 Learning and Generalization
We now tackle the problem of learnability in our setting. Broadly speaking, the learner’s goal
is to ensure that a classifier trained on a finite dataset to anticipate agents misreporting to an
equilibrium,performswell on thebroaderpopulation distributionwhich exhibits similar behaviour.
A unique aspect of our setting is the number of agents participating in a given instance need not
be fixed. Taking university admissions as an example, the number of applicants in a given year
is not constant but rather a random variable. Our generalization result should accommodate this
variability in the number of players in each instance, alongside the equilibrium they reach.
Recall from Section 2 that k denotes the largest number of simultaneous participants with
max
, a distribution over [k ], and , the data distribution that we sample from, i.e., (x ,y ) .
max i i
K D ∼ D
The learner has access to a training dataset of n instances, where each instance involves a sample
k , and then k independent samples from the distribution . Observe that the following is
i i
∼ K D
an equivalent sampling procedure: sample k , draw k independent samples from , and
i max
∼ K D
then only consider the first k elements. We use this latter procedure for the generalization result;
i
formally, we define a joint distribution
D = .
D×···×D×K
kmax
The learner has access to n samples from |this d{zistrib}ution, representing their training set: =
S
(X ,y ,k ),...,(X ,y ,k ) , where X Rkmax d, y Rkmax, and k [k ]. Note that each
1 1 1 n n n i × i i max
{ } ∈ ∈ ∈
sample/instance in this dataset can be seen as holding the attributes of k individuals.
i
For any chosen classifier f , the participating agents reach a PNE and the classifier suffers a
ω
resulting loss based on this. In the preceding section, we show this equilibrium to be thesolution of
a convex optimization problem. To provide formal learning guarantees, however, it is important to
understand how the outcome of this optimization (i.e. the PNE) behaves as the classifier changes.
Indeed, if the PNE drastically changes due to small changes in ω, learning can be challenging.
Let NE(X,f ,k) : Rkmax d Rd [k ] Rkmax d, where the first k rows of the output
ω × max ×
× × →
correspond to the equilibrium solution when k agents are participating. We prove in the following
result that the potential function optimum (and thus the PNE strategy) is Lipschitz continuous in
ω. The theorem statement treats inputs to Φ and NE as vectors. This is without loss of generality
since ourX Rkmax d feature matrix is equivalent to a x Rdkmax vector, with thevector ℓ norm
× 2
∈ ∈
x equivalent to the matrix Frobenius norm X .
2 F
|| || || ||
Lemma 1. Let Φ(x,x,f ) be the potential function for k agents (x,x Rdkmax). Then for
′ ω max ′
η = γ c, where c = 1
2
min x′,ω(λ min( ∇2 x′Φ(x,x ′,f ω))) and γ = max x′,ω ||∇2 ωx′Φ∈ (x,x ′,f ω) ||+1, the
function NE(x,f ,k) is η-Lipschitz in ω (under the norm) for any k.
ω 2
||·||
The proof (deferred to the Appendix) is technical and in fact shows the Lipschitz continuity
for a general class of parametrized convex optimization problems. Noting that local Lipschitz-
ness suffices, we lower bound the difference between an optimal and sub-optimal solution using
properties of strict concavity and smoothness of the potential function. The remainder of the proof
leverages the continuity of the objective in both x and ω along with the insights above to establish
′
theLipschitz-ness ofthemaximizer ofΦ. Torelate thistoNE(X,f ,k), wedefinetheoptimization
ω
objective as summingthepotential function over only thefirstk participants withatrivial function
involving the remaining agents.
Next, foralossfunctionℓ(f (x ),y )definedonclassifyinganindividual’sreportedfeatures,the
ω i i
average loss on a sample consisting of k participating strategic agents is given by L(X,y,f ,k) =
ω
81 k ℓ(f (NE(X,f ,k) ),y )8. We now define the standard learning theory notions using this:
k i=1 ω ω i: i
DPefinition 3. For a classifier f , the true risk R(f ) on distribution D and the empirical risk
ω ω
Rˆ(f ) on a dataset consisting of n independent samples from D is given respectively by:
ω
S
n
1
R(f ω) = E
(X,y,k)
∼DL(X,y,f ω,k) and Rˆ(f ω) =
n
L(X i,y i,f ω,k i).
i=1
X
We want to show that a classifier chosen to minimize the risk on a finite dataset , denoted
by fˆ , will be close in a PAC sense, to the true risk minimizer of the population, dS enoted by
ω
f . Recall that we focus on linear classifiers and as such, our function class is the set of norm-
ω∗
constrained linear functions: ω,x s.t. ω r . We denote Ω = ω s.t. ω r as the
i 2
{h i || || ≤ } { || || ≤ }
corresponding parameter space. We now give our generalization guarantee below. To sketch the
proof, it notes thatthe average loss on thesample (X,y,k) is a scalar regardless of therandomness
of the number of participants. Further, due to the Lipschitz-ness of the NE function, the loss
changes predictably due to a changing classifier. This allows us to apply a covering argument and
bound the discretization error. While the sample complexity itself does not directly depend on
k (due mainly to the sample loss being scaled by the number of participants 1) it does depend
max k
on the Lipschitz constant η. Thisconstant is definedwith respect to thepotential function over the
maximum number of k participants (see Lemma 1). Sample complexity, thus, has an implicit
max
dependence on k .
max
Theorem 3. For any λ-Lipschitz loss function ℓ, linear function class parametrized by Ω =
ω s.t. ω r , agent dynamics captured by a strictly concave potential function Φ(x,x,f ),
2 ′ ω
{ || || ≤ }
and any ε> 0 and δ > 0:
8 e 16(λ(d+ηr)γ
n ln +dln = P R(fˆ ) R(f ) ε δ
≥ ε2 γ ε ⇒ |
ω
−
ω∗
| ≥ ≤
(cid:18) (cid:19) (cid:18) (cid:19)
(cid:16) (cid:17)
where η is the Lipschitz constant from Lemma 1.
Proof. Let = (k ,X ,y ),...,(k ,X ,y ) be the training set where X Rkmax d and
1 1 1 n n n i ×
S { } ∈
y Rkmax. Let L(X ,y ,f ,k ) denote the loss on the i-th sample, where L(X ,y ,f ,k ) =
i i i ω i i i ω i
1
∈ki
ℓ(f (NE(X ,f ,k ) ),y ). Note that the following holds where the expectation is over
sk ai mpj l= es1 fromω D: i ω i j: ij
P
n n
1 1
Rˆ(f ) = L(X ,y ,f ,k ) E[Rˆ(f )] = E[L(X,y,f ,k)] = R(f ) (1)
ω i i ω i ω ω ω
n n
i=1 i=1
X X
Our goal is to show the following generalization: R(fˆ ) R(f ) ε with low probability. To
ω ω∗
| − | ≥
that end, we use the fact that uniform convergence implies generalization. Formally:
R(fˆ ) R(f ) Rˆ(fˆ ) R(fˆ ) + Rˆ(f ) R(f ) 2 sup Rˆ(f ) R(f ) (2)
ω ω∗ ω ω ω∗ ω∗ ω ω
− ≤ | − | | − | ≤ | − |
fω
∈F
wherethe second inequality follows from the triangle inequality and that Rˆ(fˆ ) R(fˆ ) 0. Since
ω ω
− ≤
the function space is continuous, we will fix a ζ-cover ζ of the parameter space Ω. That is, for
ω
any ω Ω, there exists a ω ζ such that ω N ω ζ. Since our parameter space is d
′ ω ′
∈ ∈ N || − || ≤
dimensional ball of norm r, it is well known that such a covering can be achieved with ζ
ω
|N | ≤
8The notation NE(X,fω)i: denotes theequilibrium report of theith agent.
9d
2r√d . For a sample i in our dataset, let z denote the vector of scores received. That is,
ζ i
(cid:16)z ij = h(cid:17)NE(X i,f ω,k i) j:,ω i, and let z i′j denote the score on this sample when classifier f ω′ is used.
Since our loss function ℓ is λ-Lipschitz in the score z , we have that:
ij
1
ki
λ
′
|L(X i,y i,f ω,k i) −L(X i,y i,f ω′,k i) |=
(cid:12)k
ℓ(z ij,y ij) −ℓ(z i′j,y ij)
(cid:12)≤ k
||z
i
−z
i ||1
(cid:12) i j=1 (cid:12) i
(cid:12) X (cid:12)
(cid:12) (cid:12)
Next, by making use of the triangle inequa(cid:12)lity, we have the following: (cid:12)
(cid:12) (cid:12)
λ
′
z z
k || i − i ||1
i
λ
ki
= NE(X i,f ω,k i) j:,ω NE(X i,f ω′,k i) j:,ω
′
k |h i−h i|
i
j=1
X
λ
ki
= NE(X i,f ω′,k i) j:,ω + NE(X i,f ω,k i)
j:
NE(X i,f ω′,k i) j:,ω NE(X i,f ω′,k i) j:,ω
′
k |h i h − i−h i|
i
j=1
X
λ
ki
NE(X i,f ω′,k i) j:,(ω ω ′) + NE(X i,f ω,k i)
j:
NE(X i,f ω′,k i) j:,ω
≤k |h − i| |h − i|
i
j=1
X
λ
ki
λ
ki
NE(X i,f ω′,k i)
j: 1
(ω ω ′) 1+ NE(X i,f ω,k i)
j:
NE(X i,f ω′,k i) j:,ω
≤k || || ·|| − || k |h − i|
i i
j=1 j=1
X X
λ
λd (ω ω ′)
1
+ (NE(X i,f ω,k i) NE(X i,f ω′,k i))ω
1
≤ || − || k || − ||
i
where the last inequality follows from the assumption that the feature vectors are bounded in the
region [0,1]d. Next, we make use of the following 3 relations that hold for any matrix V and vector
ω Rd: (1) ω ω √d ω , (2) submultiplicavity of matrix norms Vω V ω
2 1 2 2 2
∈ || || ≤ || || ≤ || || || || ≤ || |||| ||
and (3) V V , where V denotes the matrix Frobenius norm:
2 F F
|| || ≤ || || || ||
(NE(X i,f ω,k i) NE(X i,f ω′,k i))ω
1
k
i
(NE(X i,f ω,k i) NE(X i,f ω′,k i))ω
1
|| − || ≤ || − ||
pk
i
(NE(X i,f ω,k i) NE(X i,f ω′,k i))
F
ω
2
≤ || − || || ||
pr k
i
(NE(X i,f ω,k i) NE(X i,f ω′,k i))
F
≤ || − ||
p
Next, we appeal to theorem 1 which establishes the η Lipschitz continuity of the NE function in
−
ω under the norm to state the following:
F
||·||
r k
i
(NE(X i,f ω,k i) NE(X i,f ω′,k i))
F
ηr k
i
ω ω
′ 1
|| − || ≤ || − ||
p p
Thus in combining the results here, we can state the following, where the last inequality follows
from the definition of a γ covering:
λ
L(X i,y i,f ω,k i) L(X i,y i,f ω′,k i) λd ω ω
′
1+ ηr k
i
ω ω
′ 1
λ(d+ηr)γ
| − | ≤ || − || k || − || ≤
i
p
This essentially bounds the change in empirical and true risk due to using classifier parameters
ζ
from using parameters in the finite covering . Formally, we expand Equation 2 to state the
ω
N
10following (ω is the closest element in ζ to ω):
′ ω
N
sup R(f ω) Rˆ(f ω) =sup R(f ω′) Rˆ(f ω′)+R(f ω) R(f ω′)+Rˆ(f ω′) Rˆ(f ω)
| − | | − − − |
ω Ω ω Ω
∈ ∈
max R(f ω′) Rˆ(f ω′) +2λ(d+ηr)γ.
≤ ω′ ωζ | − |
∈N
By choosing ζ = ε , we get that
8λ(d+ηr)γ
P (cid:18)ωsu ∈p Ω|R(f ω) −Rˆ(f ω)
|≥
2ε
(cid:19)
≤
P ωm
′
∈a Nx
ωζ
|R(f ω′) −Rˆ(f ω′)
| ≥
4ε
!
Lastly, we can apply Hoeffding’s inequality due to Equation 1 and using union bound over δ
ω
N
at the right-hand side of the inequality above, we have that
nε2
P ωm
′
ax
ωζ
|R(f ω′) −Rˆ(f ω′)
|| ≥
4ε
! ≤
2 |Nωζ |exp (cid:18)−
8
(cid:19)
∈N
16λ(d+ηr)γ d nε2
2 exp −
≤ ε 8
(cid:18) (cid:19) (cid:18) (cid:19)
and the theorem follows by choosing n accordingly.
4.1 Minimizing Empirial Risk
While we have shown PAC learning is possible in our multi-agent strategic classification game, a
cornerstone of learning is minimizing the empirical risk; that is, finding a classifier that minimizes
loss incurred on the training set . Even for a convex loss function ℓ and linear classifier f ,
ω
S
minimizing this across the samples of is non-trivial since the NE function is the solution to an
S
optimization problem. In general, there is no closed-form expression for the NE function and we
cannot hope for this loss minimization problem to be convex.
Machinelearningnonethelesshasavastliteratureonoptimizingnonconvexlossfunctions;these,
however, are largely gradient-based and require computing the loss gradient with respect to the
learning parameter ω. This loss is computed with respect to the equilibrium outcome/score of a
participating agent i: f (NE(X,f ) )9. When f is linear, this is simply NE(X,f ) ,ω . We
ω ω i: ω ω i:
h i
now show when and how this gradient can be explicitly computed.
Let v = NE(X,f ) and z = f (NE(X,f ) ) = f(v ,ω) for i [k], noting that z R and
i ω i: i ω ω i: i i
∈ ∈
v Rd. The loss gradient is given as follows:
i
∈
k k k
1 1 1 ∂ℓ
L(x,y,f ) = ℓ(f(NE(X,f ) ,ω),y ) = ℓ(z ,y ) = z
ω ω ω ω i: i ω i i ω i
∇ k ∇ k ∇ k ∂z ∇
i
i=1 i=1 i=1
X X X
k
1 ∂ℓ
= Tf (NE(x,f ) )+ f
k ∂z ∇vi Jω ω i: ∇ω
i
i=1
X (cid:2) (cid:3)
We use the chain rule here, with representing the Jacobian with respect to ω. ∂ℓ is the
Jω ∂zi
derivative of the loss with respect to the score, a feature of the loss function. Further, the gradients
9Since we are concerned with theloss on a sample, we assume k=k max and drop thedependenceon k.
11Tf and f depend only on the function class we are learning. Indeed, for a linear function
∇vi ∇ω
class, the desired gradient is given by:
k
1 ∂ℓ
L(x,y,f )= ωT (NE(x,f ) )+NE(x,f ) (3)
ω ω ω ω i: ω i:
∇ k ∂z J
i
i=1
X (cid:2) (cid:3)
The remaining and crucial term is the Jacobian of the outcome of the PNE optimizer with
respect to the weights: (NE(x,f ) ). The theorem below proves exactly when this Jacobian
ω ω i:
J
exists, and explicitly characterizes it. Once again, for ease of exposition we interpret the input
and output of the NE function as a vector. To sketch the key ideas, we introduce primal and dual
slack variables to capture the KKT conditions necessary and sufficient in the optimization above in
an implicit equation. We show that the Jacobian of a slightly modified but representative version
of this implicit function has full rank except possibly at the specified points. Aside from these
degenerate points, we can use the implicit function theorem to compute the gradient explicitly.
With this hand, the learner is free to use standard gradient descent based algorithms to solve the
empirical risk minimization problem.
Theorem 4. For NE(x,f ω)= argmax x′ [0,1]kdΦ(x ′,x,f ω)where Φisstrictly concave and smooth,
let λ and µ denote the dual variables corr∈ esponding to the upper and lower bound constraints. Then
x (ω) = NE(x,f ) is differentiable with respect to ω everywhere except possibly if there exists
∗ ω
an x (ω) 0,1 with the corresponding dual variable set to 0; i.e., if there exists i such that
∗i
∈ { }
(x (ω) = 1 λ = 0) (x (ω) = 0 µ = 0).
∗i
∧
∗i
∨
∗i
∧
∗i
Proof. Note that NE is the solution to a strictly convex optimization problem. Let z = kd. We
express the Lagrangian of this problem as follows, with the λ Rz denoting the dual variables for
∈
the upper bound constraint and µ Rz, the duals for the lower bound (recall our feasible region
∈
is a box [0,1]z):
z z
(x,λ,µ,ω;x) = Φ(x,x,f ) λ (x 1)+ µ x
′ ′ ω i ′i i ′i
L − −
i=1 i=1
X X
Since our feasible region is always strictly satisfiable, Slater’s condition is always satisfied.
When our objective is concave, as modeled throughout the paper, the KKT conditions are both
sufficient and necessary for the optimal solution x = NE(x,ω). Since the constraints are affine,
∗
in non-concave setting, the KKT conditions are necessary at a local optimum. Define vectors
s Rz,s Rz,s Rz,s Rz which will be used to denote the primal and dual slacks. We
x+ x λ µ
now∈ define th− e∈ following∈ implicit∈ function G() :R7z d R7z:
×
· →
x′ (x ′,λ,µ,ω;x)
∇ L
x 1+s2 i [m]
 ′i − x+,i ∀ ∈ 
x s2 i [m]
′i− x ,i ∀ ∈
G(x,λ,µ,s ,s ,s ,s ,ω) =  diag(λ− )(1 x) 
′ x+ x λ µ  ′ 
−  diag(µ)(− x) 
 ′ 
 λ s2 i [m] 
  

µ ii −− s2 µλ, ,i
i
∀ ∀i∈ ∈[m]   

 
Let G() = 0. Under this implicit equation, the first row of equations corresponds to the KKT
·
stationarity conditions. The second two rows of equations enforce each x is less than 1 and greater
′i
than 0 respectively - the KKT primal feasibility conditions. The fourth and fifth rows of equations
correspondtothecomplementary slack constraints. Thelast two rows of equations enforcethat the
12dual variables are positive, the KKT dual feasibility condition. Thus, when G() = 0, it means the
·
KKT conditions are satisfied, and with a concave objective this also implies the solution is optimal.
Similarly, for any optimal X ,λ ,µ , since it is feasible, we can always find the corresponding
∗ ∗ ∗
slacks so as to satisfy G() = 0. Therefore, solutions to this implicit equation fully characterizes
·
the optimal solution.
Let p = (x,λ,µ,s ,s ,s ) and we can simplify our equation to G(p,ω) = 0. Any p that
′ x λ µ
satisfies this is the optimal solution for ω. We aim to use the Implicit Function Theorem and to
that end, we first compute the Jacobian (G) as follows (the columns represent the derivative
z
J
with respect to x,λ,µ,s ,s ,s ,s ):
′ x+ x λ µ
−
2 Φ(x,x,f ) −I I 0 0 0 0
∇x′ ′ ω
I 0 0 diag(2)s 0 0 0
x+
 
I 0 0 0 diag( 2)s 0 0
x
− −
z(G) =  diag(λ) Ix 0 0 0 0 0 
J  
 diag(µ) 0 Ix 0 0 0 0 
 − − 
 0 I 0 0 0 diag( 2)s 0 
 − λ 
 0 0 I 0 0 0 diag( 2)s 
 − µ
 
We note that the firstm columns are always linearly independentowing to the block of identity
matrices in the second and third rows of block matrices. Similarly, the second and third sets of m
columns are linearly independent owing to the identity in the first row. The remaining columns
correspond to slack variables, which we now address. A constraint i is active if x 0,1 . Under
∗i
∈ { }
the KKT conditions, the Lagrange multiplier for inactive constraints must be 0. Let [2m]
I ⊆
denote a set of active constraints. Let (ω) = ix (ω) = 0 i+m x (ω) = 1 . We define
I { |
∗i
} ∪ { |
∗i
}
the inverse mapping ω( ) as follows: ω( ) = ω Ω I(ω) = . It is immediate that the set
I I { ∈ | I}
ω( ) [2m] is a partition of the parameter space Ω.
{ I ∀I ∈ }
Fix an and without loss of generality, let i constraints from λ and j constraints from µ be
I
active. Then for ω ω( ), it suffices to consider λ Ri and µ Rj - ie the dual variables
i j
∈ I ∈ ∈
corresponding to the active constraints - since the others will be 0 under KKT complementary
slackness condition. Similarly, we need only consider the slacks for the inactive constraints since
thosearetheonlyonesfree. Thusforω ω( )wecansimplifythegeneralfunctionGtoafunction
∈ I
G : R3m+i+j Rd R3m+i+j by focusing only on the inactive primal slacks and the active dual
′
× →
variables and their corresponding slack.
We note that by definition, the primal slack for inactive constraints cannot be 0 by definition
(otherwise those constraints would be active). Hence the columns corresponding to those are also
independent. Thus, if the dual slacks for the active constraints are non-zero, then the Jacobian for
G has full rank. This allows us to apply the implicit function theorem and state the following:
′
∂NE(x,f )
(NE(x,f )) , ω = 1(G(p,ω)) (G(p,ω))
Jω ω
∂ω
Jp− ′ Jω ′
If however some of these dual slacks are zero, then it suggests that some of these constraints
are redundant and the Jacobian of G may not be invertible. This is formally equivalent to:
′
i (x = 1 λ = 0) (x = 0 µ = 0)
∗i ∗i ∗i ∗i
∃ | ∧ ∨ ∧
Indeed, this is a degenerate situation known as strict complementary failure (Nocedal & Wright,
1999) and represents the threshold or boundary of the w( ) region where one set of constraints is
I
becoming active and another inactive.
13Wealsonotethatthisresultmaybeinsightfulforsettingsevenwhenthepotentialfunctionmay
be non-concave. As we have affine constraints, the KKT conditions are still necessary at local op-
tima(Boyd & Vandenberghe,2004). SinceourapproachreliesonimplicitlycharacterizingtheKKT
conditions,wecanstillusethistocomputethederivativeatlocaloptima,providedwecanfindthem
effectively. Game-theoretically, this would correspond to local Nash Equilibrium (Balduzzi et al.,
2018).
5 Models of Externality
The presented results leverage the strict concavity of the potential function to assert that the PNE
is unique, efficiently computable, and PAC learning guarantees possible for the learning problem.
Section 3 concluded by noting a more general Condition (1) on the cost and externality to en-
′
sure this. This allows us to capture possibly non-convex externality; we give two such examples
motivated by the settings we highlighted.
Proportional Externality: Externality in strategic classification can model an increased risk in
detection/audit dueto others also manipulating. In our admissions example, the risk of the univer-
sity (classifier) suspecting something is amiss is much higher when many students wrongfully claim
to be top of their class, as opposed to a handful. As such, it is natural that one’s externality due to
manipulation increases proportional to the extent to which others in the system also manipulate.
We express the externality suffered by agent i in such a setting as:
d
t(x ,x ,x ,x ) = β (x x )2(x x )2
′i i j ′j k 1 ′iℓ − iℓ ′jℓ − jℓ
−
ℓ=1
X
We scale by k 1 since each agent pays a single cost c() but suffers externality from k 1 agents;
− · −
this allows α (the scale constant for the cost) and β to be on the same scale. Next, observe that
for individuals who do not manipulate, the total externality they incur is 0. This is consistent with
one interpretation of the university admissions setting: even if many claim to be top of their class,
those who truly are, have nothing to fear. We note this externality is pairwise symmetric and we
show below that it satisfies the updated condition (proof in Appendix).
Proposition 1. Under the cost c(x,x) = α x x 2, the cumulative impact of manipulation
′ || − ′ ||2
under the proportional externality model is smooth and strictly convex for β < α.
Congestion Externality: In many decision-making scenarios, reported features map to real re-
sources and have downstream consequences beyond classification. Externality can thus model an
increased cost for manipulated features due to demand from others. Many countries, for example,
deploy immigration policies that favour candidates who pledge to settle in under-populated areas
(Picot et al.,2023). Naturally, aninfluxofapplicants may reportsuchintentions andinitially move
to these areas (with many reneging on this soon after and relocating10); this can significantly in-
crease housing and living costs for new immigrants in these underpopulated communities. In such
cases, individuals suffer from others manipulating even if they are being honest. Externalities of
this nature can be modelled as follows:
d
t(x ,x ,x ,x ) = β exp( (x x )2)
′i i j ′j k 1 − ′iℓ− ′jℓ
−
ℓ=1
X
10The government of Canada which deploys such a policy, reported a 5 years retention rate of only 39% for the
provinceof Prince Edward Island (Picot et al., 2023).
14Under this externality, when reported values (regardless of their veracity) become more similar,
indicating usage of common resources, the externality to agents increases, with the exponential
functionensuringthisissmooth. Again,itispairwisesymmetricandsatisfiesourupdatedconvexity
condition (proof in Appendix).
Proposition 2. Under cost c(x,x) = α x x 2, the cumulative impact of manipulation under
′ || − ′ ||2
the congestion externality model is smooth and strictly convex for β < α .
√2
We note that while these functions capture the spirit of each setting, they are by no means
definitive. Indeed there may be other representations for these settings that satisfy our desiderata.
Choosing the right model for the right context is an important design goal of the decision-maker.
6 Discussion
This paper studies a fundamental question within the strategic classification paradigm: what is the
effect of inter-agent externalities on both the agents and the classifier? It is no longer reasonable
to assume agents simply best respond to the classifier; rather, the Nash Equilibrium of the induced
game becomes the natural solution concept, and we provide a set of conditions whereupon this
equilibrium is unique and efficiently computable. The classifier, on the other hand, must learn an
optimal model with respect to such an induced equilibrium. We show that this Stackelberg-Nash
Equilibrium can be learned in a PAC sense and its loss gradients computable. This paper shows
the possibility of deploying loss-minimizing classifiers robust against rich manipulation dynamics.
Alimitationofourworkisthestructuralassumptionsonexternality. Theirpairwisenaturegives
rise to a potential game, and the convexity assumption ensures this is efficiently computable and
satisfies Lipschitz regularity conditions. This leads to an intriguing open question: what learning
guarantees, if any, can be given for non-concave potential functions where equilibria may not be
unique or well-behaved? Can we precisely specify the necessary conditions (our results give a set of
sufficientconditions)onexternality to ensurebothlearnability androbustness? Understandingthis
is both technically and practically interesting. Another important direction is assuming the learner
doesnot knowthecostandexternality modelandmustlearnthembyobservingequilibriumreports
over multiple interactions. This closely parallels the literature on online strategic classification
(Dong et al., 2018; Chen et al., 2020) and learning in games (Cesa-Bianchi & Lugosi, 2006). Given
the high stakes, characterizing the welfare properties of multi-agent strategic interaction is also
imperative. This can involve parametrizing the price of anarchy (Roughgarden, 2010) in terms of a
given classifier or simultaneously optimizing forwelfare alongside robustness. Lastly, engaging with
pertinentstakeholderstoaccurately capturerealcostsandexternality andassesstheimplicationsof
ourmodelisanimportantfuturestep. Ourworklays thegroundworkforsuchimportantquestions,
allowing us to comprehensively understand and mitigate the real challenges of classification in
strategic multi-agent settings.
15References
Anish Agarwal, Munther Dahleh, Thibaut Horel, and Maryann Rui. Towards data auctions with
externalities. arXiv preprint arXiv:2003.08345, 2020.
David Balduzzi, Sebastien Racaniere, James Martens, Jakob Foerster, KarlTuyls,and ThoreGrae-
pel. The mechanics of n-player differentiable games. In International Conference on Machine
Learning, pp. 354–363. PMLR, 2018.
YahavBechavod,CharaPodimata,StevenWu,andJubaZiani.Informationdiscrepancyinstrategic
learning. In International Conference on Machine Learning, pp. 1691–1715. PMLR, 2022.
Maria Patricia Bejarano Carbo. Machine learning applications in the united states criminal justice
system: A critical content analysis of the compas recidivism risk assessment. 2021.
Claude Berge. Topological spaces. Oliver & Boyd, 1963.
Stephen Boyd and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.
Michael Bru¨ckner and Tobias Scheffer. Nash equilibria of static prediction games. Advances in
neural information processing systems, 22, 2009.
Michael Bru¨ckner, Christian Kanzow, and Tobias Scheffer. Static prediction games for adversarial
learning problems. The Journal of Machine Learning Research, 13(1):2617–2654, 2012.
Nicolo Cesa-Bianchi and Ga´bor Lugosi. Prediction, learning, and games. Cambridge university
press, 2006.
Yiling Chen, Chara Podimata, Ariel D Procaccia, and Nisarg Shah. Strategyproof linear regression
inhighdimensions. InProceedings of the 2018 ACM Conference on Economics and Computation,
pp. 9–26, 2018.
Yiling Chen, Yang Liu, and Chara Podimata. Learning strategy-aware linear classifiers. Advances
in Neural Information Processing Systems, 33:15265–15276, 2020.
RH Coase. The problem of social cost. Journal of Law and Economics, pp. 1–44, 1960.
Jinshuo Dong, Aaron Roth, Zachary Schutzman, Bo Waggoner, and Zhiwei Steven Wu. Strategic
classification from revealed preferences. In Proceedings of the 2018 ACM Conference on Eco-
nomics and Computation, pp. 55–70, 2018.
Itay Eilat, Ben Finkelshtein, Chaim Baskin, and Nir Rosenfeld. Strategic classification with graph
neuralnetworks. InProceeding of the 11-thInternational Conference onLearning Representations
(ICLR), 2023.
MariahDREvans,JonathanKelley,JoannaSikora,andDonaldJTreiman. Familyscholarlyculture
and educational success: Books and schooling in 27 nations. Research in social stratification and
mobility, 28(2):171–197, 2010.
Ganesh Ghalme, Vineet Nair, Itay Eilat, Inbal Talgam-Cohen, and Nir Rosenfeld. Strategic classi-
fication in the dark. In In Proceeding of the 24th International Conference on Machine Learning
(ICML), pp. 3672–3681, 2021.
16JacobKGoeree, EmielMaasland, SanderOnderstal,andJohnLTurner. How(not)toraisemoney.
Journal of Political Economy, 113(4):897–918, 2005.
Nika Haghtalab, Nicole Immorlica, Brendan Lucier, and Jack Z. Wang. Maximizing welfare with
incentive-aware evaluationmechanisms. InProceeding of the 26-th International JointConference
on Artificial Intelligence (IJCAI), pp. 160–166, 2020.
Moritz Hardt, Nimrod Megiddo, Christos Papadimitriou, and Mary Wootters. Strategic classifica-
tion. In Proceedings of the 2016 ACM conference on innovations in theoretical computer science,
pp. 111–122, 2016.
Drew Harwell. A face-scanning algorithm increasingly decides whether you deserve the job. In
Ethics of Data and Analytics, pp. 206–211. Auerbach Publications, 2022.
Guy Horowitz and Nir Rosenfeld. Causal strategic classification: A tale of two shifts. In Interna-
tional Conference on Machine Learning, pp. 13233–13253. PMLR, 2023.
Safwan Hossain and Yiling Chen. Equilibrium of data markets with externality. In Forty-first
International Conference on Machine Learning. PMLR, 2024.
Safwan Hossain and Nisarg Shah. The effect of strategic noise in linear regression. Autonomous
Agents and Multi-Agent Systems, 35(2):21, 2021.
Lily Hu, Nicole Immorlica, and Jennifer Wortman Vaughan. The disparate effects of strategic ma-
nipulation. In Proceedings of the 2nd Conference on Fairness, Accountability, and Transparency
(FAccT), pp. 259–268, 2019.
JonKleinbergandManishRaghavan. How doclassifiersinduceagents toinvest effortstrategically?
ACM Transactions on Economics and Computation (TEAC), 8(4):1–23, 2020.
I Elizabeth Kumar, Keegan E Hines, and John P Dickerson. Equalizing credit opportunity in
algorithms: Aligning algorithmic fairness research with us fair lending regulation. In Proceedings
of the 2022 AAAI/ACM Conference on AI, Ethics, and Society, pp. 357–368, 2022.
SagiLevanonandNirRosenfeld.Strategicclassificationmadepractical. InInternational Conference
on Machine Learning, pp. 6243–6253. PMLR, 2021.
Sagi Levanon and Nir Rosenfeld. Generalized strategic classification and the case of aligned incen-
tives. In International Conference on Machine Learning, pp. 12593–12618. PMLR, 2022.
Minming Li, Lili Mei, Yi Xu, Guochuan Zhang, and Yingchao Zhao. Facility location games with
externalities. In Proceedings of the 18th International Conference on Autonomous Agents and
Multiagent Systems, pp. 1443–1451, 2019.
John Miller, Smitha Milli, and Moritz Hardt. Strategic classification is causal modeling in disguise.
InProceedings of the21st International Conference onMachineLearning (ICML),pp.6917–6926,
2020.
Smitha Milli, John Miller, Anca D Dragan, and Moritz Hardt. The social cost of strategic classifi-
cation. InProceedings of the Conference on Fairness, Accountability, and Transparency (FAccT),
pp. 230–239, 2019.
DovMondererandLloydSShapley. Potentialgames. Gamesand economic behavior,14(1):124–143,
1996.
17Tomoya Nakamura. One-leader and multiple-follower stackelberg games with private information.
Economics Letters, 127:27–30, 2015.
Abraham Neyman. Correlated equilibrium and potential games. International Journal of Game
Theory, 26(2):223–227, 1997.
Jorge Nocedal and Stephen J Wright. Numerical optimization. Springer, 1999.
WG Picot, Eden Crossman, and Feng Hou. The Provincial Nominee Program: Retention in
Province of Landing. Statistics Canada= Statistique Canada, 2023.
AC Pigou. The economics of welfare. 1920.
Tim Roughgarden. Algorithmic game theory. Communications of the ACM, 53(7):78–86, 2010.
Georg Still. Lectures on parametric optimization: An introduction. Optimization Online, pp. 2,
2018.
Stephen J Wright and Benjamin Recht. Optimization for data analysis. Cambridge University
Press, 2022.
Jiuping Xu, Zongmin Li, Zhimiao Tao, Jiuping Xu, Zongmin Li, and Zhimiao Tao. Methodology
from an equilibria viewpoints. Random-Like Bi-level Decision Making, pp. 365–386, 2016.
18Proof of Lemma 1
Proof. We begin by proving the Lipschitz continuity of the maximizer of the following generic
optimization problem:
x (ω) =argmaxΨ(x,ω) (4)
∗ ′
x′
∈X
where Ψ is strictly concave in x for any ω Ω and is convex. We know there is always a
′
∈ X
unique maximizer, and thus the optimal value x (ω) is a well-defined function. We also note that
∗
our optimization problem satisfies the conditions of Berge’s Maximum Theorem (Berge, 1963).
Thus, we can immediately conclude that the set-valued correspondence from any ω to the set of
maximizers is upper-hemicontinuous. Since our maximizer is unique - i.e. the set is a singleton - it
suffices to observe that any set-valued map that is a singleton is upper-hemicontinous if and only
if the corresponding function is continuous.
LetB (ω)referstoanopenballofradiumεcenteredatω. ForLipschitz-ness, wefirstnotethat
ε
since Ω is bounded and compact, it suffices to prove this locally. That is, we wish to show that for
any ω, there exists constants L,ε > 0 such that for all ω B , x (ω) x (ω) L ω¯ ω .
ω
∈
εω
||
∗
−
∗ ||2
≤ || −
||2
Indeed, this means that for any two ω ,ω Ω, we can consider a sequence of distinct point
′ ′′
∈
ω = ω ,ω ,...,ω ,ω = ω lying on the line αω +(1 α)ω such that ω ω ε .
1 ′ 2 n −1 n ′′ ′
−
′′
||
i
−
i −1 ||2
≤
ωi−1
Then we have the following, which means it suffices to focus on local Lipschitz-ness:
n
x (ω ) x (ω ) L ω ω = L ω ω
∗ ′′ ∗ ′ 2 i i 1 2 ′′ ′ 2
|| − || ≤ || − − || || − ||
i=1
X
We next prove a property that holds for the maximizer of our problem under any ω. Fix a
ω, and let x be the maximizer. Next, consider a Taylor expansion of Ψ at x . One formulation
∗ ∗
of this presented in Chapter 2 of Wright & Recht (2022) states for some γ (0,1), we have (the
∈
dependence on ω is dropped for now as it is unchanged):
1
Ψ(x ′, ·) = Ψ(x ∗, ·)+(x
′
−x ∗)T ∇x′Ψ(x ∗, ·)+ 2(x
′
−x ∗)T ∇2 x′Ψ(x ∗+γ(x
′
−x ∗), ·)(x
′
−x ∗)
1
=
⇒
Ψ(x ′, ·)
≤
Ψ(x ∗, ·)+ 2(x
′
−x ∗)T ∇2 x′Ψ(x ∗+γ(x
′
−x ∗), ·)(x
′
−x ∗)
where the second line follows from Lemma 2.7 in Still (2018), which states that at for any x ,
′
∈ X
∇x′Ψ(x ∗,ω) ·(x
′
−x ∗)
≤
011. We also note that ∇2 x′Ψ(x ′, ·) is the Hessian matrix which is (1)
always symmetric and (2) consists of all strictly negative eigenvalues since Ψ(x, ) is always strictly
′
·
concave. Since any symmetric matrix can be diagonalized as QΛQT, where Q is the matrix of
orthonormal eigenvectors and the Λ the eigenvalues, we have that (define p = x x ):
′ ∗
−
n
(x x )T 2Ψ(x +γ(x x ), )(x x ) = pTQΛQTp = λ (vTp)2
′ ∗ x ∗ ′ ∗ ′ ∗ i i
− ∇ − · −
i=1
X
where v is the ith eigenvector. Since we have strictly negative eigenvalues and Q is an orthonor-
i
mal matrix and thus does not affect the norm of vector it matrix multiplies, we have (where the
λ ( 2 Ψ) returns the minimum eigenvalue of the Hessian across ω Ω and x ):
min ∇x′
∈
′
∈ X
(x
′
−x ∗)T ∇2 x′Ψ(x ∗+γ(x
′
−x ∗), ·)(x
′
−x ∗)
≤
λ min( ∇2 x′Ψ) ||QTp ||2
2
= λ min( ∇2 x′Ψ) ||p ||2
2
= Ψ(x, ) Ψ(x , )+ 1λ ( 2 Ψ) x x 2
⇒ ′ · ≤ ∗ · 2 min ∇x′ || ′ − ∗ ||2
11Technically the lemma is for convex functions with a ≥,but it can beeasily massaged for concavefunctions
19Sinceλ mustbestrictly negative, itis evident that thereis astrictly positive constant c, namely
min
c = 21λ min( ∇2 x′Ψ) = 21min x′,ω(λ( ∇2 x′Ψ(x ′,ω))) such that:
Ψ(x ,ω) Ψ(x,ω) c x x 2
∗ − ′ ≥ || ′ − ∗ ||2
We now turn back to local Lipschitz-ness. Fix a ω Ω and denote the unique maximizer by
∈
x (ω) = x. From the relation derived above (Equation 6), there exist constants δ ,c > 0, such
∗ 1
that:
Ψ(x,ω) Ψ(x,ω) c x x 2 x , x x δ (5)
− ′ ≥ || − ′ ||2 ∀ ∈ X || − ||2 ≤ 1
Let Ψ = Ψ(ω,x) denote the optimal value at ω. For δ such that 0 < δ < δ , let 2α =
∗ 1
min x′ s.t. x¯ x′ =δΨ(x,ω) Ψ(x ′,ω). Since xis the global and strict optimizer at ω, it holds that
|| − || −
2α > 0. Therefore, we have that:
Ψ(x,ω) Ψ 2α x such that x x¯ = δ (6)
′ ∗ ′ ′ 2
≤ − ∀ || − ||
We now leverage the continuity of Ψ() with respect to ω at (x,ω). For a chosen value α,
2
there must exist an ε such that for all ω B (ω), we have that Ψ(x,ω) Ψ(x,ω) < α.
1 ∈ ε1 | − | 2
Similarly, the continuity of Ψ() with respect to ω at (ω,x) such that x x = δ, implies that
′ ′ 2
|| − ||
for any such x ′ and chosen value α 2, there exists ε x′ such that for all ω
∈
B εx′(ω), we have that
|Ψ(x ′,ω) −Ψ(x ′,ω)
|
< α 2. Letting ε = min(ε 1,min x′ |δ= ||x′ −x¯ ||ε x′), the following holds:
α α α
ω B (ω), x: Ψ(x,ω) Ψ ,Ψ + = Ψ(x,ω) Ψ (7)
ε ∗ ∗ ∗
∀ ∈ ∈ − 2 2 ⇒ ≥ − 2
(cid:16) (cid:17)
α α
ω B (ω),x s.t x x = δ : Ψ(x,ω) Ψ(x,ω) ,Ψ(x,ω)+ (8)
ε ′ ′ ′ ′ ′
∀ ∈ || − || ∈ − 2 2
(cid:16) (cid:17)
CombiningEquations6and8wehavethatforanyω B (ω)andanyx suchthat x x δ:
ε ′ ′ 2
∈ || − || ≤
α α
Ψ(x,ω) < Ψ(x,ω)+ Ψ 2α+ Ψ α (9)
′ ′ ∗ ∗
2 ≤ − 2 ≤ −
Fix a ω B (ω). We aim to show Lipschitz continuity between ωand ω , and follow a similar
0 ε 0
∈
structure as Theorem 6.2a in Still (2018). We know from Equation 7 that Ψ(x,ω ) Ψ α. We
0 ≥ ∗ − 2
also know from Equation 9 that for any x on the boundary of the B (x) ball, Ψ(x,ω ) Ψ α.
′ δ ′ 0 ∗
≤ −
Since Ψ() is concave in x, this implies that the c= Ψ α super-level set of Ψ(x,ω ) (for a fixed
′ ∗ − 2 ′ 0
ω ) must lie within this B (x) ball. In other words, the maximizer at ω , x (ω ) , x satisfies
0 δ 0 ∗ 0 0
x x δ. Noting that Ψ(x,ω ) Ψ(x ,ω )< 0, we have that:
0 2 0 0 0
|| − || ≤ −
Ψ(x,ω) Ψ(x ,ω)= [Ψ(x,ω) Ψ(x,ω )] [Ψ(x ,ω) Ψ(x ,ω )]+[Ψ(x,ω ) Ψ(x ,ω )]
0 0 0 0 0 0 0 0
− − − − −
[Ψ(x,ω) Ψ(x,ω )] [Ψ(x ,ω) Ψ(x ,ω )]
0 0 0 0
≤ − − −
A B
Consider next the f| unction Ψ{z (x,ω) } Ψ(|x,ω ) , {gz (x). Con} sider g(x) between x and x .
′ ′ 0 ′ ′ 0
−
Indeed g(x) g(x ) = A B Ψ(x,ω) Ψ(x ,ω). By the mean value theorem, there exists a
0 0
− − ≥ −
α [0,1] such that:
∈
Ψ(x,ω) Ψ(x ,ω) g(x) g(x ) = [Ψ(x+α(x x),ω) Ψ(x+α(x x),ω )] (x x )
0 0 x 0 0 0 0
− ≤ − ∇ − − − · −
Recall that for a differentiable function f(ω), we can approximate it near a value ω using the
gradient: f(ω) = f(ω)+ f(ω) (ω ω)+o( ω ω ). So for any x cl(B (x)) (cl denotes the
ω ′ δ
∇ · − || − || ∈
20closure), define h(ω) = Ψ(x,ω). Thus, we have that:
x ′
∇
h(ω ) = h(ω)+ h(ω)(ω ω )+o(1 ω ω )
0 ω 0 0 2
∇ − || − ||
= x′Ψ(x ′,ω 0) = x′Ψ(x ′,ω)+ 2 ωx′Ψ(x ′,ω)(ω ω 0)+o(1 ω
0
ω 2)
⇒ ∇ ∇ ∇ − || − ||
= x′ Ψ(x ′,ω) Ψ(x ′,ω 0) = 2 ωx′Ψ(x ′,ω)(ω ω 0)+o(1 ω 0 ω 2).
⇒ ∇ − ∇ − || − ||
(cid:2) (cid:3)
Going back to Ψ(x,ω) Ψ(x ,ω) we have the following:
0
−
Ψ(x,ω) Ψ(x 0,ω) x′[Ψ(x+α(x
0
x),ω) Ψ(x+α(x
0
x),ω 0)] (x x 0)
− ≤ ∇ − − − · −
max x′ Ψ(x ′,ω) Ψ(x ′,ω 0) (x x 0)
≤ x′ ∈clBδ(x¯)∇ − · −
max (
2(cid:2)
Ψ(x,ω) +1)
(cid:3)
ω ω x x .
ωx′ ′ 2 0 2 0 2
≤ x′ ∈clBδ(x¯) ||∇ || ·|| − || ·|| − ||
Let γ = max x′,ω ||∇2 ωx′Ψ(x ′,ω) ||2 + 1. Then using the relation outlined in Equation 5, and
noting the fact that x B (x), we have that:
0 δ
∈
c x x 2 Ψ(x,ω) Ψ(x ,ω) γ ω ω x x
|| − 0 ||2 ≤ − 0 ≤ || − 0 ||2 ·|| − 0 ||2
γ
= x x ω ω
0 2 0 2
⇒ || − || ≤ c|| − ||
To relate this to the Nash Equilibrium of an arbitrary k participants, observe that the following
is acharacterization of theNE(x,f ,k) function(again wetreat xas aRdkmax dimensionalvector):
ω
kmax
NE(x,f ,k) = argmax Φ(x ,x ,f ) x x 2
ω x′ ∈[0,1]dkmax ′0:k 0:k ω −
j=
Xk+1|| ′j − j ||2
 
Thefirstterm, thepotential function, fits the function signature of Ψ (see theobjective in equation
4) since x (the true features) is simply a constant. Second, we note that the optimization problem
here is separable since the potential function only uses the first k feature vectors and the sum of
norms only involves the remaining k k feature vectors. Further, the latter is independent of
max
−
ω and will always be set to x = x for k < j k in the maximization program. Thus, we
′j j
≤
max
can appeal to the result above for the maximization of Φ and claim the function NE(x,f ,k) to
ω
be γ lipschitz continuous where γ,c are as above but for the largest possible value of k, which is
c
k .
max
21Proof of Proposition 1
Proof. We express the cumulative impact of manipulation under the stated conditions as follows:
k d k d
α (x x )2+ β (x x )2(x x )2
′iℓ − iℓ k 1 ′iℓ − iℓ ′jℓ − jℓ
−
i=1 ℓ=1 i=1 j>i ℓ=1
XX XXX
d n k
= α (x x )2+ β (x x )2(x x )2
 ′iℓ − iℓ k 1 ′iℓ − iℓ ′jℓ − jℓ 
−
ℓ=1 i=1 i=1 j>i
X X XX
 
Since the sum of strictly convex functions is convex, it suffices to show that each inner summand is
strictly convex. For a fixed ℓ, we observe that since since there are exactly k(k −1) pairs satisfying
2
j > i, and in each feature x appears in exactly (k 1) of these pairs, the inner summand can be
i
−
written as follows:
k
ℓ : α (x x )2+ β (x x )2(x x )2+ α (x x )2 (10)
∀ k 1 ′il − il k 1 ′iℓ − iℓ ′jℓ − jℓ k 1 ′jl − jl
− − −
i=1 j>i
XX
Again since convexity is preserved in summation, we only need to show strong convexity with
respect to the summands, each of whom is a function of two variables (x and x since x and x
′iℓ ′jℓ i j
are constants). For an arbitrary summand index by (i,j), let u = (x x ) and u = (x x ).
i ′iℓ− iℓ j ′jℓ− jℓ
Then the Hessian (upon multiplying Equation 10 by k 1, which does not affect convexity), is:
−
2α+2βu2 4βu u
2 = j i j
∇x′ iℓ,x′ jℓ
(cid:20)
4βu iu j 2α+2βu2 i.
(cid:21)
The determinant of this Hessian, when simplified, is given by:
det( ∇2 x′ iℓ,x′ jℓ)= 4α2 +4αβu2 i +4αβu2 j −12β2u2 iu2 j
Sincewe wishto showthe determinantis strictly positive, α2+αβu2+αβu2 > 3β2u2u2. As feature
i j i j
vectors are bounded, u ,u [ 1,1], and our condition is α > β, the following holds:
i j
∈ −
α2+αβu2+αβu2 > β2+β2u2+β2u2 β2u2u2+β2u2+β2u2
i j i j i j i j
≥
β2u2u2+2β2u2u2 = 3β2u2u2,
i j i j i j
≥
where the second last transition uses the fact that 2u2u2 u2+u2 in the feature vector range.
i j ≤ i j
Proof of Proposition 2
Proof. We express the cumulative impact of manipulation under the stated conditions as follows:
k d k d
α (x x )2+ β exp( (x x )2)
′iℓ − iℓ k 1 − ′iℓ − ′jℓ
−
i=1 ℓ=1 i=1 j>i ℓ=1
XX XXX
d n k
= α (x x )2+ β exp( (x x )2)
 ′iℓ − iℓ k 1 − ′iℓ − ′jℓ 
−
ℓ=1 i=1 i=1 j>i
X X XX
 
Since the sum of strictly convex functions is convex, it suffices to show that each inner summand is
strictly convex. For a fixed ℓ, we observe that since since there are exactly k(k −1) pairs satisfying
2
22j > i, and in each feature x appears in exactly (k 1) of these pairs, the inner summand can be
i
−
written as follows:
k
ℓ: α (x x )2+ β exp( (x x )2)+ α (x x )2. (11)
∀ k 1 ′il − il k 1 − ′iℓ − ′jℓ k 1 ′jl − jl
− − −
i=1 j>i
XX
Again, since convexity is preserved in summation, we only need to show strong convexity with
respecttothesummands,eachofwhomisafunctionoftwovariables(x andx ). Foranarbitrary
′iℓ ′jℓ
summand index by (i,j), let u = (x x ). Then the Hessian (upon multiplying Equation 11 by
′iℓ
−
′jℓ
k 1, which does not affect convexity) is given by:
−
2α+2βe u2 [2u2 1] 2βe u2 [1 2u2]
∇2 x′ iℓ,x′
jℓ
=
" 2βe
−u2−
[1
2u2−
]
2α+2β−
e
−u2−
[2u2+1]. #
−
A positive definite matrix is equivalent to a matrix with all positive principal minors. When
β < α < α, we note the the (0,0) index (and thus the first principal minor) is positive. The
√2
second principal minor is the determinant, which for our 2 2 matrix above is given by:
×
det( 2 ) = 4α2+16αβe u2 u2+4β2e 2u2 [4u2 2].
∇x′ iℓ,x′
jℓ
− −
−
Note that the middle term is always positive. The last term is the most negative when u = 0,
which results in it being 8β2. It is evident that can be well compensated for by the first term
− 2
since using our relation between β and α, we have that: 8β2 <8 α = 4α2.
√2
(cid:16) (cid:17)
23