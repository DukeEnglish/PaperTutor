TV-TREES: Multimodal Entailment Trees
for Neuro-Symbolic Video Reasoning
KateSanders NathanielWeir BenjaminVanDurme
JohnsHopkinsUniversity
{ksande25, nweir, vandurme}@jhu.edu
Abstract
Itischallengingtoperformquestion-answering
overcomplex,multimodalcontentsuchastele-
vision clips. This is in part because current
video-languagemodelsrelyonsingle-modality
reasoning,haveloweredperformanceonlong
inputs, and lack interpetability. We propose
TV-TREES, the first multimodal entailment
tree generator. TV-TREES serves as an ap-
proachtovideounderstandingthatpromotesin-
terpretablejoint-modalityreasoningbyproduc-
ingtreesofentailmentrelationshipsbetween
simplepremisesdirectlyentailedbythevideos
and higher-level conclusions. We then intro-
ducethetaskofmultimodalentailmenttreegen-
erationtoevaluatethereasoningqualityofsuch
Figure1: A(a)QApairand(b)correspondingvideo
methods. Our method’s experimental results
clip and dialogue from the TVQA dataset (Lei et al.,
onthechallengingTVQAdatasetdemonstrate
intepretable,state-of-the-artzero-shotperfor- 2018)and(c)amultimodalentailmenttree,recursively
producedbyourapproach(top-down).Treesarecreated
manceonfullvideoclips,illustratingabestof
throughrecursivelyretrievingatomicevidencefromthe
bothworldscontrasttoblack-boxmethods.
transcriptandvideoframesanddecomposingtheQA
1 Introduction pair into compositionally equivalent hypotheses until
eachcanbedirectlyentailedbytheretrievedevidence.
Videosaccountforalargeportionofcontentavail-
ableandconsumedonline,butautomatedreason-
ingoversemanticallycomplexvideo-languagedata performjointvisual-languagereasoningisalsolim-
remains a challenging and under-explored prob- ited,thattheyrelyoneithertextorvisualcontent
lem. A popular task for assessing models’ video but not both (Rawal et al., 2023). Better inter-
understandingisnarrative-centricvideoquestion- pretabilityofthesemodelscouldilluminatethese
answering (VideoQA): Given a natural language reasoningpitfallsandallowresearcherstoidentify
question,avideoclipofamovieorTVshow,and andcorrectsystemissues. However,whileLLMs
a corresponding dialogue transcript, the goal is nowfacilitateincreasinglytransparentexplanation
toreturnacorrectnaturallanguageanswertothe generation alongside outputs (Zhao et al., 2023),
questionusingthevideo-textdata. video-languagemodelslackthisability.
Methodstacklingthistask(Yangetal.,2022;Li Entailmenttrees(Dalvietal.,2021),ortreesof
et al., 2020; Ko et al., 2023) frequently take the entailmentrelationshipsbetweenatomicpremises
formoflarge,joint-modalitytransformermodels. andhigher-levelconclusions,havebeenshownto
Whilethesesystemstypicallyoutperformsmaller, servewellasthestructuralbasisfortext-onlyQA
domain-specificarchitectures,theyinherentlylack tasks by systematically and transparently model-
qualities necessary for robust and reliable video- inglogicalreasoningchains(WeirandVanDurme,
languageunderstanding. Inadditiontomodelper- 2023). Weembracethisapproach: Wedevelop(1)
formance often correlating with the length of the thefirstmultimodalentailmenttreegenerator,TV-
input video clip, analyses suggest their ability to TREES(theTransparentVideo-TextREasoning
4202
beF
92
]LC.sc[
1v76491.2042:viXrawithEntailmentSystem),and(2)thetaskofmul- Incontrasttothesevideo-languagemodels,Khu-
timodal entailment tree generation to assess the rana and Deshpande (2021) highlight altenrative
reasoningabilityofsuchsystems. deep learning strategies for video QA such as
In contrast to existing black-box systems, TV- attention-free methods, attention-based methods,
TREES focuses on the manipulation of atomic memory network methods, and hierarchical rein-
"facts" retrieved from video clips to answer forcedmethods. Notably,Zhaoetal.(2018,2020)
VideoQAquestions. Theapproachjointlyreasons proposeahierarchicalencoder-decodermodelthat
overbothmodalitiesandiscompatiblewithlong uses adaptive video segmentation based on the
videoinputs,andcrucially,theresultingentailment question contents. Related works consider graph
treesprovidehuman-interpretableevidenceandnat- networks for video understanding (Wang et al.,
ural language explanations for each logical oper- 2021a; Gu et al., 2021; Liu et al., 2022). While
ation. Our evaluation method builds on work in these models scale to longer videos more suc-
informallogicandtextualentailmenttreegenera- cessfully, their performance suffers compared to
tion,adaptingtheseideastothemultimodaldomain transformer-basedapproaches.
withanemphasisonreliableevaluation.
2.2 ExplainableMultimodalUnderstanding
Weshowthatourmultimodalreasoningsystem
performscompetitivelyonzero-shotVideoQAfor Traditionaltechniqueslikekernelvisualizationand
thedifficultTVQAdataset(Leietal.,2018),while perturbation have been considered for video ex-
atthesametimeprovidinginterpretablereasoning plainability (Hiley et al., 2019; Li et al., 2021b)
traces. Further,TV-TREESachievesstate-of-the- alongsideotherapproachesthatconsiderlow-level
art performance using full-length video clips as reasoningstepsforsimpletasks(Zhuoetal.,2019;
input. Royetal.,2019;Nouranietal.,2020). Somework
Insummary,ourcontributionsare: focuses on grounded video QA, in which mod-
els are tasked with providing the visual evidence
1. Thefirstmultimodalentailmenttreegenerator, necessaryfor answeringa questionaboutspatial-
afullyexplainablevideounderstandingsystem temporalcontent(Xiaoetal.,2023).
thatemphasizeslogicalreasoningacrossmodal- The approaches most similar to our work are
ities. (Chen and Kong, 2021) and (Mao et al., 2022).
Chen and Kong (2021) tackle the VIOLIN video
2. Thetaskofmultimodalentailmenttreegenera-
entailmentdataset(Liuetal.,2020)bygrounding
tionandacorrespondingmetricforevaluating
the relevant textual entities in the video and tran-
step-by-stepvideo-textreasoningquality.
script and providing a heatmap over the input as
anexplanationfortheproducedoutput. Ourwork
3. Results demonstrating state-of-the-art perfor-
differs in that we show exactly what data pieces
manceonzero-shotTVQAwhenusingfullclips
contributetothefinaloutput,explicitlymodeleach
andtranscriptsasinput.
stepofthereasoningprocess,anddon’trequirefine-
2 RelatedWork tuningonthetargetdatasetordomain. Maoetal.
(2022)usesachain-of-thoughtexplanationsystem
2.1 VideoQA
basedonavideoscenegraphtoanswerquestions
QAoverimagesmakesupalargeportionofmul- aboutactionsandobjectsinshortvideoclipsand
timodal question-answering work (Zou and Xie, GIFs. The primary difference between this and
2020). VideoQAbenchmarksconstituteasmaller ourworkisthelackofdialogueandvisualseman-
portion of this area (Zhong et al., 2022) and of- tic complexity. The chain-of-thought reasoning
tenfocusonsimplecontentandquestions[CITE], primarilyconsiderslogicalandtaxonomy-centric
but some recent videoQA datasets have targeted operationsoveratomic-levelscenegraphcontent
models’ commonsense knowledge and inference instead of complex inference reasoning, and the
ability (Lei et al., 2018; Zadeh et al., 2019). Re- input for their proposed system only spans a few
cently,vision-and-languagetransformershavesub- seconds.
stantiallyimprovedperformanceonthesevideoQA
2.3 EntailmentTreeGeneration
tasks[CITE],andcanoftenreasonovercomplex
contentwithoutanexternalknowledgebase(Kim Thispaperdrawsinspirationfromrecentworkon
etal.,2021;Wangetal.,2021b;Salinetal.,2022). constructing natural language entailment trees to
2explain reasoning. The notion starts with Dalvi 3.1 Taskformulation
et al. (2021), who introduce an expert-annotated
Input Following Dalvi et al. (2021), as input
datasetofcompositionaltreesshowinghowahy-
we consider a collection of possible “evidence"
pothesisfollowsasalogicalconsequenceofase-
anddeclarativeformofaquestion-answerpair,the
riesofmulti-premiseentailmentstepsstartingfrom
hypothesish . Traditionallythisevidencebank
(q,a)
verified support facts. They propose a series of
takestheformofacollectionofnaturallanguage
reconstructiontasks,challengingmodelstorepro-
sentences, but in the multimodal domain, it will
duceexpert-annotatedtreesgivenjustthetop-level
taketheformofavideoclipV andcorresponding
hypothesisandsomeamountofgoldanddistractor
dialoguetranscriptD. Thevideoisanorderedlist
factleaves,andourproposedmultimodalconstruc- ofk imagesV := {v }k ,andthetranscriptisan
i i=0
tiontaskisinspiredbythisformulation.
ordered list of l (dialogue line, timestamp) pairs
More recent work has introduced methods to D := {(d ,s )}l ,wherethetimestampmapsthe
i i i=0
tackleDalvietal.’sreconstructiontask(Bostrom dialoguelinetostartandendframeswithinV.
etal.,2022;NevesRibeiroetal.,2022),andtouse
entailmenttreesasabasisforneuro-symbolicrea- Output Wedefineentailmenttreesasstructures
soning(Tafjordetal.,2022;WeirandVanDurme, whichtaketheformT := (h,e). hisahypothesis
2023). OurworkismostsimilartoWeirandVan andeisevidence,whichtakestheformofeithera
Durme (2023), who introduce a QA system that
1. Leaf: A(possiblyempty)subsetofitemsfrom
reasons by searching via backward chaining for
V orD.
entailmenttreesgroundedinaknowledgesource.
Webuilduponthisnotion,extendingittothemul- 2. Branch: A pair of two distinct entailment
timodalsettingandaddressingthemanyresulting subtrees T := (h ,e ) and T := (h ,e ),
1 1 1 2 2 2
challenges. wheree := (T ,T ).
1 2
Leaveswithemptyevidencesetsarelabeledasnull
2.4 MultimodalEntailment leaves.
Thepurposeofanentailmenttreeistoillustrate
Thereisaselectionofworkthatconsidersentail-
thecompositionalreasoningnecessarytoreacha
mentinimagesandvideo: (Xieetal.,2019)intro-
conclusionfromaninitialevidencebankthrough
duceadatasetofimage-entailmentpairssimilarto
entailment relationships between the parent and
theSNLI(Bowmanetal.,2015a)corpus,and(Do
child nodes. Therefore, in a well-formed tree,
etal.,2020)addnaturallanguageexplanationsto
the evidence at any node (h,e) must explicitly
thepairs. Morespecificvisualentailmenttasksin
entailthehypothesisatthatsamenode. Foraleaf
thisdomainhavebeenproposedaswell(Thomas
node, we posit that e entails h if a human would
et al., 2022; Li et al., 2023b)., and (Suzuki et al.,
reasonably infer that h is true if presented only
2019)introducealogicsystemforidentifyingen-
withevidencee ⊆ V ∪D. Forabranchingnode,e
tailmentbetweenimagesandcaptions.
entailshifahumanwouldreasonablyinferthath
Notably, Liu et al. (2020) introduce VIOLIN, istrueifpresentedwithhypothesesh andh .
1 2
a dataset of videos paired with natural language
inferencesthatareeitherentailedorcontradicted Objective Giveninput(h ,V,D),ourobjec-
(q,a)
by the video content. Typically, standard vision- tive is to return a well-formed entailment tree T
languagetransformersaretrainedforthistask(Li that includes null leaves if and only if a is not a
et al., 2020; Sun et al., 2022), but more tailored correctanswertoquestionq.
approachesexistaswell(Lietal.,2021a;Chenand
3.2 Evaluation
Kong,2021).
Toserveasasecondaryanddistinctobjectivefrom
rawVideoQAperformance,weproposeanevalu-
3 MultimodalEntailmentTrees
ation method for assessing the reasoning quality
of multimodal entailment trees inspired by Weir
We introduce the task of multimodal entailment etal.’sworkonscoringcompositionalentailments
tree generation for the VideoQA domain and the (Weir et al., 2024). Informal logic theory posits
evaluationprocedure. thatnaturallanguageargumentsmaybeevaluated
3Figure2: Themultimodalprooftreegeneratorpipeline,matchingthecontentsofAlgorithm1. Thedashedboxes
dividethepipelineintothethreeprimarymodulesofthesystem: Theyellowboxmarksthe"retrieval"module,the
lightblueboxmarksthe"filter"module,andtheorangeboxmarksthe"decomposition"module. Withrespectto
individualpipelinecells,thelightblueandyellowcellsrepresentimportantpiecesofdatausedorproducedduring
thepipeline,thedarkbluecellsrepresentgenerativetextoperations,thegreencellsrepresentdiscriminativetext
operations,andthepurplecellsrepresentvisualoperations.
intermsoftheiracceptability,relevance,andsuf- positionallyentailh ,or
0
ficiency (Johnson and Blair, 1977). We consider
each node within an entailment tree as an “argu- I(h |h ,h ) = 0 ∀(h ,(T ,T )) ∈ T. (5)
0 1 2 0 1 2
ment"andconsiderthesequaliaasguidelinesfor
comprehensiveentailmenttreeevaluation. Below, Weexplorepracticalimplementationsofthesemet-
we formulate these three qualia through an infor- ricsinSection5.
mationtheoreticlenstoestablishasetofevaluation
metrics. WeusetheShannondefinitionofinforma- 4 TV-TREES
tiongain,
In this section we introduce our proposed multi-
modal entailment tree generator, beginning with
I(x|y) = −logP(x|y),
anoverviewoftheframeworkandthenindividual
whereP(x)istheprobabilitythatnaturallanguage module details. All LLM and VLM prompts are
statementxistrueconditionedonnaturallanguage includedinfullinAppendixA.
statement(s)y.
Algorithm1Treegeneration, GENERATE
Acceptability Hypothesesateverynodeshould Input: Hypothesish, transcriptsampleD′ ⊆ D,
becompleteandverifiablenaturallanguagestate- videosampleV′ ⊆ V,currentdepthk
mentsthatareunderstandabletoahuman,andhy- Output: TreecandidateTˆ := (h,p′)
pothesesatleafnodesshouldbefactuallyaccurate 1: F D ← RETRIEVE(D′ |h)
statementsconditionedontheworldstate(V,D). 2: F D′ ← FILTERD(F,h)
Theseitemsmaybeformalizedas 3: ifF′ ̸= ∅then
D
4: e ← BESTD(F D′ |h)
I(h) ∈ [0,1] ∀h ∈ T (1) 5: elseifk ≥ k′ then
I(h|V ∪D) = 0 ∀h ∈ T leaves. (2) 6: e ← ∅
7: else
Relevance For each branching node T 0 := 8: h 0,h 1 ← DECOMPOSE(h|T′)
(h 0,(T 1,T 2)), hypotheses h 1 and h 2 should both 9: T 0 ← PROVE(h 0,D′,V′,k+1)
beconditionallyrelevanttoh 0,meaningthatthey 10: T 1 ← PROVE(h 1,D′,V′,k+1)
eachintroducedistinctinformationthatcontributes
11: e ← (T 0,T 1)
to the compositional entailment of h . Formally,
0 12: endif
thismetricismetif 13: F V′ ← FILTERV(V′ |h)
14: ifNULL(e)andF′ ̸= ∅then
I(h|h ,h ) < I(h|h ) ∀(h,e) ∈ T (3) V
1 2 2 branches 15: e ← BESTV(F V′ |h)
I(h|h ,h ) < I(h|h ) ∀(h,e) ∈ T (4)
1 2 1 branches 16: endif
17: return(h,e)
Sufficiency For each branching node T :=
0
(h ,(T ,T )), hypotheses h and h should com-
0 1 2 1 2
4toewhereapplicable(15)inthesamemanneras
thetextcontent.
Ifthemaximumdepthisreachedduringrecur-
sion,theevidenceatthatnodeissettotheempty
setandthetreeisincomplete.
Inthefollowingsections,weexplaintheimple-
mentationofthesubroutinescalledbyAlgorithm
1.
4.2 Preprocessing
Figure 3: An example question from TVQA, corre-
spondingdialogueexcerptsampledbyTV-TREES,and Hypothesis Generation The purpose of the
set of inferences generated from these inputs by TV-
hypothesisgenerationistoprovidethedownstream
TREES.Theobjectiveofinferencegenerationistopro-
modules with a single declarative statement that
duceasetoftruenaturallanguagestatementsthatcan
containsthefullsemanticmeaningoftheoriginal
helpprovethehypothesis.
QApair. Forsimplicity,thisgenerativeoperation
iscarriedoutbypromptingGPT-3.5(Brownetal.,
4.1 Systemoverview 2020). Wefindthatlessrobustin-contextlearning
models like FLAN-T5 (Chung et al., 2022) are
TV-TREES is a recursive search algorithm that
pronetoomittingcontextualdetailspresentinthe
involvesthreeprimaryprocedures:
questionandnothandlingtyposappropriately.
1. Retrieval Givenahypothesisandacollection
ofpotentialevidence,thesystemfirstsamples Evidence Localization Given the hypothesis,
relevantevidencefromthiscollectionthatmay TV-TREESattemptstoidentifyatemporalwindow
sufficientlyentailthecurrenthypothesis. to sample evidence from based on the dialogue.
Weuseacross-encodermodeltrainedontheMS
2. Filtering The system tests whether any re-
MARCOpassagerankingtask(Bajajetal.,2016)
trievedevidencefullyentailsthehypothesis. If
to rank six-line transcript passages on their com-
suchevidenceexistsandwasretrieved,itisre-
putedsimilaritywiththegeneratedhypothesis. We
turnedandthecurrentnodebecomesaleaf.
useaslidingwindowtocalculatescoresforevery
potentialsampleandreturnthehighestscoringex-
3. Decomposition If the retrieval and filtering
cerpt. Ifasufficientwindowisidentified,thevision
steps result in insufficient evidence, the sys-
pipelineinheritsthiswindow. Ifnosufficientdia-
tem decomposes the hypothesis into two sub-
logue sample is found, the system uses all video
hypotheses such that proving both indepen-
framesastheevidencebank,omittingtextentirely.
dentlyisequivalenttoprovingtheoriginalhy-
pothesis.
4.3 EvidenceRetrieval
Theinteractionofthesethreepartsisillustrated Existing natural language inference (NLI) mod-
in Algorithm 1. Given a hypothesis h, transcript elsarenotwell-suitedforclassifyingentailments
sample D′ ⊆ D and video sample V′ ⊆ V, the withinhighlycontextualandsocialdialogue,which
system first returns evidence from the transcript ofteninsinuatemeaningnotdirectlystatedwithin
relevanttoh(line1)andidentifieswhetheranyof the text. Instead of producing an entirely new
it entails h (2). If such evidence was retrieved, e dataset for the domain of dialogue NLI, we use
is set to the best sample (4) and the leaf node is GPT-3.5togenerateasetofnaturallanguageinfer-
returned (17). Otherwise, h is decomposed into encesaboutthedialoguesamplewritteninthestyle
sub-hypotheses h and h (8) and the algorithm asdatapointsinadatasetakintoSNLI(Bowman
0 1
is recursively called on these newly constructed et al., 2015b), conditioned on a question form of
sub-problems (9-10), treating the generated sub- the hypothesis, q. Presenting the question under
proofs as explanation e (11). If textual evidence discussion in the interrogative form significantly
cannotbefoundforthecurrentnodenoranyofthe reducesthehallucinationratecomparedtopassing
downstreamnodes(14),thenthevisualevidencein intheoriginalhypothesis. q isalsogeneratedvia
sampleV′ issampled,filtered,(13)andassigned GPT-3.5takingthehypothesishasinput.
5OursystemqueriesGPTforfiveinferencesfrom window (if applicable) sampled at 2 FPS into a
agivenquestionandpassage. Then,werunthese vision-languagemodel. Inourexperiments,weuse
inferencesthroughGPTtoverifythattheyareen- LLaVA-7B(Liuetal.,2023). Toencourageconser-
tailed by the transcript. Examples of generated vativeclassifications,inadditiontoaskingfor“yes"
inferencesareincludedinFigure3. and “no" answers we encourage the model to re-
spondwith“notenoughinformation"ifitisunsure
4.4 EvidenceFiltering
ortheimagedoesnotprovidesufficientevidence.
We use a cross-encoder trained on SNLI and Ifmorethan10%oftheframesinthewindowre-
MultiNLItodeterminewhetheranyoftheretrieved sultinanaffirmativeanswerfromtheVLMmodel,
evidencesufficientlyentailsthehypothesis. Weac- thevisualcontentisconsideredtocontainsufficient
ceptanysamplethatachievesalogitsscoreabove entailingevidenceandtheframewiththehighest
acertainthresholdforthe"entailment"label. logits score is returned. If no frames result in an
Then,weapplyasecondaryentailmentfilterthat affirmativeanswer,noappropriatevisualevidence
ensurestheinferencesareaccuratedescriptionsof entailsthehypothesis. TheLLaVA-7Bpromptis
thecontentpresentedinthedialogue. Thisisimpor- includedalongsidetheGPTpromptsinAppendix
tantas,whileconditioningtheinferencegenerator A.
onaninterrogativeformofthehypothesismitigates WealsouseGPT-3.5toanonymizethequestion
hallucinations,itdoesnoteliminatethementirely. generatedinsection4.1,replacingcharacternames
IdentifyingthesecasesisattemptedthroughaGPT with common nouns such as "person". We query
filter that takes in the inference and the dialogue, LLaVA-7Boneachframeindividually,usingthe
withoutanyhypothesisconditioning. anonymizedquestionastextualinput. Wecompare
Finally, as the cross-encoder tends to ignore theperformanceofthisapproachtoprovidingthe
negation,whichisoftenpresentinthegeneratedin- originalquestioninAppendixB,butfindthatthe
ferences,weadditionallypassthefilteredinference- modificationmakesmarginaldifference(approxi-
hypothesispairstoaGPT-3.5promptthatverifies matelyone-pointlowerperformanceonaverage).
theentailment.
Thesystemonlyretainstheinferencesthatpass 5 EvaluationMethodology
throughallthreefilters.
Traditionally, qualitative natural text evaluations
4.5 Decomposition haveoftenbeenconductedusinghumans(Celikyil-
mazetal.,2021),eitherexpertannotatorsorcrowd-
In the case where no atomic evidence can be re-
sourcedworkers. Recently,researchershavecon-
trieved from the transcript or video that immedi-
sidered whether these human evaluations could
atelyentailsthecurrenthypothesis,thesystemat-
be replaced by high-performing LLMs like GPT-
tempts to break it down into two sub-hypotheses
4 (Naismith et al., 2023). Following this line of
thatare(1)completesentenceswithoutambiguous
thinking,inthissection,wedetailhowweimple-
pronouns or decontextualized references and (2)
ment the evaluation metrics described in Section
compositionallyequivalenttotheoriginalhypothe-
3.2throughhumanannotationsaswellasGPT-4.
sis,i.e.,provingthetwosub-hypothesesastrueis
Wereportevaluationstatisticsforbothmethodsin
approximatelylogicallyequivalenttoprovingthe
Section6.
originalhypothesis.
WepromptGPT-3.5tobreakthecurrenthypoth-
5.1 HumanEvaluations
esisintotwocompositionallyequivalentpiecesof
information, conditioned on the dialogue sample Consideringthethreeevaluationmetricsdescribed
extractedinsection4.2. WeinstructGPTtoonly in Section 3.2 (acceptability, relevance, and suf-
returnadecompositionwhenitissyntacticallypos- ficiency), we evaluate trees along these qualia
sible,toavoidrecursingonsentencefragmentsand throughthreeannotationtasks. Thefirsttaskpro-
hypothesisrepeatsthatthemodelmayerroneously vides annotators with the visual or text evidence
outputifasounddecompositioncannotbefound. assignedtotheleafnodesbythealgorithmandask
themtoassessthecorrectnessoftheleafnodehy-
4.6 VisualReasoning
pothesesonascaleof1-5(acceptability)basedon
WepassinthequestionsgeneratedinSection5.3 thatevidence. Thesecondtaskprovidesannotators
alongsidevideoframesfromthelocalizedevidence with(h ,h′)pairsfrombranchingnodesandasks
0
6Method Zero-Shot FullClips Transparent Dialogue Vision TVQAAcc.
Fine-TunedMethods
STAGE No Yes No Yes Yes 70.5
HERO No No No Yes Yes 74.2
FrozenBiLM No No No Yes Yes 82.0
LLaMA-VQA No No No Yes Yes 82.2
Zero-ShotMethods
FrozenBiLM∗ Yes Yes No Yes Yes 26.3
SeVILA Yes Yes No No Yes 38.2
VideoChat2 Yes Yes No No Yes 40.6
TV-TREES‡ Yes Yes Yes Yes No 44.9
TV-TREES Yes Yes Yes Yes Yes 49.4
Table1: Tablecomparingvariousvision-textunderstandingmodelsacrossasetofcriteriaincludingperformanceon
theTVQAbenchmark. Allzero-shotmethods(Zero-Shot)takeinfullvideoclips(FullClips),butunlikethefine-
tunedapproaches,noneexceptFrozenBiLMoperateoverbothvisionanddialoguemodalities. Notably,TV-TREES
istheonlyinterpretableapproach. ExperimentresultssuggestthatTV-TREESandTV-TREESwithtextinputonly
(TV-TREES‡)outperformexistingzero-shotmethodsonfullclips. Allnumbersforcompetingapproachesareas
theyarereportedintheirrespectivepapersexceptforFrozenBiLM*,whichwere-runonourvalidationsubsetwith
fullclipsasinput. (Ongroundtruthclipfragments,FrozenBiLMreports59.7%accuracy). Resultssuggestthata
morerobustvisualunderstandingmodulecouldfurtherimprovetheperformanceofTV-TREES,seeingthebaseline
resultsachievedbymodelstakinginvisioninputonly.
ifthechildhypothesish′ isrelevanttotheparent weuseGPT-4Vforvisionevaluations,andseparate
h (relevance). Thethirdtaskprovidesannotators bothfromtheremainingchecklistitemsasonlythe
0
with a full hypothesis triplet (h ,h ,h ) from a leavesmustbeevaluatedforevidence-centriccor-
0 1 2
branchingnodewithparenth andchildpremises rectness. We use the same scoring values as in
0
h and h and asks (1) whether h and h each thehumanevaluations,andpassintwelvedecom-
1 2 1 2
introduce distinct information (the other facet of positions per prompt for the text prompts. These
relevance, we also call this distinctness for dis- promptsareincludedinfullinAppendixE.
ambiguation purposes), and (2) if h introduces
0
5.3 TreeScoringParadigm
informationnotprovidedbyh andh together,to
1 2
checkforentailment(sufficiency). Throughthese Weconsiderthemeannormalizedscoreofthethree
tasks, annotatorsarealsoaskedtoindicateifany mainevaluationqualiaacrossallnodesastheover-
of the hypotheses or premises are malformed or all“compositionscore"foreachindividualtree:
otherwise uninterpretable (the other facet of ac-
a+s+0.5(d+r)
ceptability).
S =
3
Everynodeinamultimodalentailmenttreeisas-
signedabinaryscoreforeachassessmentdescribed whereaisthetree’smeanleafacceptabilityscore,
above(exceptforthecorrectnesschecks,whichare d is the tree’s mean distinctness score, r is the
collected on a scale of 1-5). We include all task tree’smeanrelevancescore,andsisthetree’smean
instructionsandlayoutsinAppendixD,alongwith sufficiencyscore.
more formal descriptions of the five quantitative
6 Experiments
acceptabilityscores.
We evaluate TV-TREES on the TVQA dataset,
5.2 GPTEvaluations
comparingitsperformanceagainstatext-onlyver-
WetakethequaliaoutlinedinSection3.2andwrite sion of the architecture and competing zero-shot
threeGPT-4promptsfor(1)correctleavesinthe VideoQAapproaches. Wecompareallapproaches
textdomain,(2)correctleavesinthevisiondomain, intermsofQAaccuracy,andcomparetheentail-
and (3) the remaining three checklist items. Cor- menttreegenerationmethodsintermsoftreequal-
rectnesscheckpromptsaremodalitydependentas ityasdescribedinSection5.
7Trees Acceptability Relevance Distinctness Sufficiency Score
GPT-4Evaluations
TextOnly 58.4 99.6 87.7 88.6 74.3
Multimodal 61.0 99.6 90.6 93.9 77.8
All 59.7 99.6 89.1 91.2 76.0
HumanEvaluations
TextOnly 65.6 93.9 88.8 93.6 78.9
Multimodal 51.8 98.1 91.2 92.8 72.9
All 58.7 96.0 91.7 93.2 75.9
Table 2: Entailment tree quality evaluations using human and LLM evaluators. For the human annotations,
acceptabilitycorrespondstoTask1,relevancetoTask2,anddistinctnessandsufficiencytoTask3. Thesemetrics
areexplicitlylabeledintheGPT-4promptsforevaluation. Thistablereportsmeanscoresaggregatedpertreefor
eachcategory. Inadditiontometricscores,wereportcompositionscoreasdefinedinSection5.3. Wepartition
resultsbymodality: Wereportscoresfortreesusingtextcontentonly, treesthatusevisualevidence, andboth
groupscombined. Asshown,treescoreslargelysufferduetothecorrectnessoftheleafnodes,whichisunsurprising
giventhedifficultyofextractinghigh-levelinferencesfromsocialdialogueandoftenambiguousvideoscreenshots.
Method Acc. Comp. Acc. Comp. % dialogue and visual content of video clips taken
from six TV shows. The clips are approximately
Vision 32.4 51.9 19.7
60-90 seconds long and contain around 30 lines
Dialogue 44.9 53.3 51.5
ofdialogueeach. AnexampleTVQAquestionis
Both 49.4 53.0 69.5
showninFigure1.
Table3: Ablationexperimentresultscomparingperfor-
manceonTVQAwhenusingonlydialogueevidence, Models In the zero-shot setting, in addition to
onlyvisualevidence,andbothmodalitiesasevidence. TV-TREES, we consider zero-shot approaches
Wereporttheoverallaccuracy,theaccuracyofthesys-
FrozenBiLM (Yang et al., 2022), SeVILA (Yu
temonquestionswhereatleastoneproofwascomplete,
et al., 2023), and VideoChat2 (Li et al., 2023a).
andthepercentageofquestionsonwhichatleastone
We also include performance reported by other
proofwascomplete.
systems(notzero-shot)forcontext: STAGE(Lei
etal.,2019),HERO(Lietal.,2020),FrozenBiLM
6.1 Setup (fine-tuned) (Yang et al., 2022), and LLaMA-
WeinstantiateTV-TREESasitisdescribedinSec- VQA(Koetal.,2023).
tion 4, setting the maximum recursion depth to
k = 2, or allowing trees with up to 3 levels. Our Ablations Existingworknotesthatbothexisting
experimentsfocusonthemultiplechoiceVideoQA multimodal models are biased toward the text
domain,andsoweconsideraquestion’scorrectan- modality,oftenrelyingontextdataforreasoning
swertobetheanswerthatresultsinacompletetree. evenforvideo-centricquestions. Inlinewiththis
In the case that the system does not successfully theme, we evaluate our system’s performance
complete any tree for the five answer candidates, conditioned on input modality on a subset of the
weconsidertheanswercandidatewiththe"most TVQAvalidationset. Wefirstevaluatethesystem
complete"treetobethecorrectanswer,breaking whenitisonlyprovidedwithdialoguetranscripts
tiesbytheaverageentailmentscoreateachnode. from the clip and then when it is only provided
When complete trees are generated for multiple withvideoframesfromtheclip.
answers,webreaktiesinthesameway.
Results We report overall accuracy alongside
6.2 EvaluationonTVQA
qualitative comparisons between the approaches
Data Weevaluateoursystemon3,000multiple inTable4. Asshowninthetable,TV-TREESout-
choicequestionsfromthevalidationsetofTVQA performs existing zero-shot methods when using
(Leietal.,2018). TVQAisaVideoQAbenchmark full clips, but still shows significant room for fu-
thatincludes multiplechoice questionsabout the tureimprovements. Notably,thetext-onlymodel
8outperformsjoint-modalitymethods,andthejoint rorsintheproducedtreesstemfromacceptability
modalitymodelonlyimprovesperformancemod- issues. Accordingtohumanevaluations,thevisual
estly,suggestingthatthelanguagemodulesofTV- moduleproduceslowerqualityinferencesthanthe
TREES are more robust and performance could textual modules do. This is not surprising, as we
befurtherincreasedthroughimprovementstothe areabletoincludeadditionalentailmentfiltersfor
visionpipeline. Thisisfurthershownintheabla- thetextualreasoningstepstoremovelowerquality
tionexperimentresultsinTable3,whichsuggests predictionsbeforeconstructingthefinalentailment
that vision evidence alone allows TV-TREES to trees,whereaswedonothavesimilarmethodsin
complete trees for only 19.7% of the questions placeforvisualinference. Basedontheseresults,
compared to 51.5% and 69.5% for text-only and introducingstrongerentailmentclassifiersforboth
joint-modalitymodels,respectively. domains may significantly improve performance
ontreeevaluationaswellasongeneralVideoQA.
6.3 ProofScoring
7 Conclusion
Setup Werandomlysample600completedentail-
menttreesgeneratedbyTV-TREESontheTVQA Weintroducethefirstneuro-symbolicentailment
validationsplit,splitevenlybetweentext-onlyand treegeneratorformultimodalcontenttoimprove
multimodaltreesandsplitevenlyamongtreecom- robustness, reliability, interpretability, and scala-
plexity(rangingfromonetoseventreenodes). bility of video-language understanding systems.
Weevaluatethesesampledtreesusingtheauto- We focus on the application of narrative-driven
maticGPT4approachasdescribedinSection5.2. VideoQA, and show that our approach achieves
Wethensample200proofsfromthisset(evenlydis- state-of-the-art results on the zero-shot TVQA
tributedacrossmodalitiesandcomplexity)andwe benchmarkwithfullvideoclips. Wealsopropose
annotatethissetwithhumanannotatorsfromAma- thetaskofmultimodalentailmenttreegeneration
zonMechanicalTurkasdescribedinSection5.1. fortheassessmentofgeneratedtreereasoningqual-
Forhumanannotations,weidentifycarefulanno- ity, establishing an information-theoretic evalua-
tatorsthroughapreliminarypilottaskwhereeach tionmethodgroundedininformallogictheory. Ex-
annotator’sworkisscoredbyhand,andonlyhigh- perimentalresultssuggestthatsuchinterpretable,
scoringannotatorsareinvitedtoannotatethefull neuro-symbolic approaches to video understand-
proofs. Moreinformationregardingthesecrowd- ingareastrongalternativetoexistingmethodsand
sourcedannotationsareincludedinAppendixC. presentexcitingdirectionsforfutureresearch.
Forscoringacceptability,weprovidethescorer
8 Limitations
withthelocalizeddialogueretrievedbythecross
encoder model described in Section 4.2, and the Weintroduceaninitialexplorationintothetaskof
videoframesthatachievedthehighestlogitsscores multimodalentailmenttreegenerationforvideoun-
duringVQAinference,dependingonthemodality. derstanding,andso,thereareinherentlimitations
WereportresultsinTable2. that we hope to correct in future work. Most no-
tably,ourvisionmoduleunderperformscompared
Results Generally,thereisaclosealignmentbe- tosomesystems-infuturework,wehopetoim-
tweentheGPT-4andhumanscores. Whiletheover- proveupontheexistingend-to-endarchitectureas
allaveragescoreassignedtothetreesiswithina.1 well as explore more compositional approaches.
pointdifferencebetweenthetwoapproaches,GPT- Furthermore, while we consider six lines of di-
4tendedtoscorethetext-onlytreesmoreharshly alogue at a time to ensure sufficient context for
than humans, and the multimodal trees more le- textualinference,wedonotdothesameforvisual
niently. This is shown primarily in the resulting analysis(insteadworkingwithonlyoneframeata
acceptability scores, and more moderately in the time). Extendingtheimmediatecontextforvisual
sufficiency scores. GPT-4 rated relevance scores inference would likely improve performance as
morelenientlyforbothmodalities,whichmaystem well. Finally,itisimportanttoconsiderthedomain
from differences in human interpretations of the thatoursystemisusedin,asmodelperformance
task instructions. In contrast, distinctness scores mayvaryindomainswithlimiteddialogue,etc. We
arealmostidenticalbetweenthetwomethods. hopethatthisworkinspiresfutureresearchinthis
Wefindthat,unsurprisingly,themajorityofer- domaintoimproveuponourproposedpipeline.
9References LiamHiley,AlunPreece,andYuliaHicks.2019. Ex-
plainabledeeplearningforvideorecognitiontasks:
PayalBajaj,DanielCampos,NickCraswell,LiDeng,
A framework & recommendations. arXiv preprint
Jianfeng Gao, Xiaodong Liu, Rangan Majumder,
arXiv:1909.05667.
Andrew McNamara, Bhaskar Mitra, Tri Nguyen,
et al. 2016. Ms marco: A human generated ma-
RalphH.JohnsonandJ.AnthonyBlair.1977. Logical
chinereadingcomprehensiondataset. arXivpreprint
self-defense.
arXiv:1611.09268.
Khushboo Khurana and Umesh Deshpande. 2021.
KajBostrom,ZayneSprague,SwaratChaudhuri,and
Video question-answering techniques, benchmark
Greg Durrett. 2022. Natural language deduction
datasetsandevaluationmetricsleveragingvideocap-
throughsearchoverstatementcompositions. InFind-
tioning: a comprehensive survey. IEEE Access,
ingsoftheAssociationforComputationalLinguistics:
9:43799–43823.
EMNLP2022,pages4871–4883,AbuDhabi,United
ArabEmirates.AssociationforComputationalLin- WonjaeKim,BokyungSon,andIldooKim.2021. Vilt:
guistics. Vision-and-language transformer without convolu-
tion or region supervision. In International Con-
SamuelRBowman,GaborAngeli,ChristopherPotts,
ference on Machine Learning, pages 5583–5594.
andChristopherDManning.2015a. Alargeanno-
PMLR.
tatedcorpusforlearningnaturallanguageinference.
arXivpreprintarXiv:1508.05326.
DohwanKo,JiSooLee,WooyoungKang,Byungseok
Roh, and Hyunwoo J Kim. 2023. Large lan-
SamuelR.Bowman,GaborAngeli,ChristopherPotts,
guage models are temporal and causal reasoners
andChristopherD.Manning.2015b. Alargeanno-
for video question answering. arXiv preprint
tatedcorpusforlearningnaturallanguageinference.
arXiv:2310.15747.
CoRR,abs/1508.05326.
JieLei,LichengYu,MohitBansal,andTamaraLBerg.
Tom Brown, Benjamin Mann, Nick Ryder, Melanie
2018. Tvqa: Localized, compositionalvideoques-
Subbiah,JaredDKaplan,PrafullaDhariwal,Arvind
tionanswering. arXivpreprintarXiv:1809.01696.
Neelakantan,PranavShyam,GirishSastry,Amanda
Askell,etal.2020. Languagemodelsarefew-shot
Jie Lei, Licheng Yu, Tamara L Berg, and Mohit
learners. Advancesinneuralinformationprocessing
Bansal. 2019. Tvqa+: Spatio-temporal ground-
systems,33:1877–1901.
ing for video question answering. arXiv preprint
arXiv:1904.11574.
Asli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.
2021. Evaluationoftextgeneration: Asurvey.
JunchengLi,SiliangTang,LinchaoZhu,HaochenShi,
Xuanwen Huang, Fei Wu, Yi Yang, and Yueting
JunwenChenandYuKong.2021. Explainablevideo
entailment with grounded visual evidence. In Pro- Zhuang.2021a. Adaptivehierarchicalgraphreason-
ceedingsoftheIEEE/CVFInternationalConference ingwithsemanticcoherenceforvideo-and-language
onComputerVision. inference. InProceedingsoftheIEEE/CVFInterna-
tionalConferenceonComputerVision,pages1867–
HyungWonChung,LeHou,ShayneLongpre,Barret 1877.
Zoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi
Wang,MostafaDehghani,SiddharthaBrahma,etal. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li,
2022. Scalinginstruction-finetunedlanguagemodels. Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen,
arXivpreprintarXiv:2210.11416. PingLuo,etal.2023a. Mvbench: Acomprehensive
multi-modalvideounderstandingbenchmark. arXiv
BhavanaDalvi,PeterJansen,OyvindTafjord,Zhengnan preprintarXiv:2311.17005.
Xie,HannahSmith,LeighannaPipatanangkura,and
PeterClark.2021. Explaininganswerswithentail- LinjieLi,Yen-ChunChen,YuCheng,ZheGan,Licheng
menttrees. InProceedingsofthe2021Conference Yu,andJingjingLiu.2020. Hero: Hierarchicalen-
onEmpiricalMethodsinNaturalLanguageProcess- coderforvideo+languageomni-representationpre-
ing,pages7358–7370,OnlineandPuntaCana,Do- training. arXivpreprintarXiv:2005.00200.
minican Republic. Association for Computational
Linguistics. NanLi,PijianLi,DongshengXu,WenyeZhao,YiCai,
and Qingbao Huang. 2023b. Scene-text oriented
VirginieDo,Oana-MariaCamburu,ZeynepAkata,and visualentailment: Task,datasetandsolution. InPro-
Thomas Lukasiewicz. 2020. e-snli-ve: Corrected ceedingsofthe31stACMInternationalConference
visual-textual entailment with natural language ex- onMultimedia,pages5562–5571.
planations. arXivpreprintarXiv:2004.03744.
ZhenqiangLi,WeiminWang,ZuoyueLi,YifeiHuang,
MaoGu,ZhouZhao,WeikeJin,RichangHong,andFei andYoichiSato.2021b. Towardsvisuallyexplaining
Wu. 2021. Graph-based multi-interaction network videounderstandingnetworkswithperturbation. In
forvideoquestionanswering. IEEETransactionson ProceedingsoftheIEEE/CVFWinterConferenceon
ImageProcessing,30:2758–2770. ApplicationsofComputerVision,pages1120–1129.
10HaotianLiu,ChunyuanLi,QingyangWu,andYongJae Yuchong Sun, Hongwei Xue, Ruihua Song, Bei Liu,
Lee.2023. Visualinstructiontuning. HuanYang,andJianlongFu.2022. Long-formvideo-
languagepre-trainingwithmultimodaltemporalcon-
Jingzhou Liu, Wenhu Chen, Yu Cheng, Zhe Gan, trastive learning. Advances in neural information
LichengYu, YimingYang, andJingjingLiu.2020. processingsystems,35:38032–38045.
Violin: Alarge-scaledatasetforvideo-and-language
RikoSuzuki,HitomiYanaka,MasashiYoshikawa,Koji
inference. InProceedingsoftheIEEE/CVFConfer-
Mineshima,andDaisukeBekki.2019. Multimodal
enceonComputerVisionandPatternRecognition,
logicalinferencesystemforvisual-textualentailment.
pages10900–10910.
arXivpreprintarXiv:1906.03952.
YunLiu, XiaomingZhang, FeiranHuang, BoZhang,
OyvindTafjord,BhavanaDalviMishra,andPeterClark.
and Zhoujun Li. 2022. Cross-attentional spatio-
2022. Entailer: Answering questions with faithful
temporalsemanticgraphnetworksforvideoquestion
andtruthfulchainsofreasoning. InProceedingsof
answering. IEEETransactionsonImageProcessing,
the2022ConferenceonEmpiricalMethodsinNat-
31:1684–1696.
uralLanguageProcessing,pages2078–2093,Abu
Dhabi,UnitedArabEmirates.AssociationforCom-
JianguoMao,WenbinJiang,XiangdongWang,Zhifan putationalLinguistics.
Feng,YajuanLyu,HongLiu,andYongZhu.2022.
Dynamicmultistepreasoningbasedonvideoscene Christopher Thomas, Yipeng Zhang, and Shih-Fu
graphforvideoquestionanswering. InProceedings Chang. 2022. Fine-grained visual entailment. In
ofthe2022ConferenceoftheNorthAmericanChap- European Conference on Computer Vision, pages
teroftheAssociationforComputationalLinguistics: 398–416.Springer.
HumanLanguageTechnologies,pages3894–3904.
Jianyu Wang, Bing-Kun Bao, and Changsheng Xu.
BenNaismith,PhoebeMulcaire,andJillBurstein.2023. 2021a. Dualvgr: Adual-visualgraphreasoningunit
Automatedevaluationofwrittendiscoursecoherence forvideoquestionanswering. IEEETransactionson
using gpt-4. In Proceedings of the 18th Workshop Multimedia,24:3369–3380.
onInnovativeUseofNLPforBuildingEducational
ZekunWang,WenhuiWang,HaichaoZhu,MingLiu,
Applications(BEA2023),pages394–403.
Bing Qin, and Furu Wei. 2021b. Distilled dual-
encoder model for vision-language understanding.
Danilo Neves Ribeiro, Shen Wang, Xiaofei Ma, Rui
arXivpreprintarXiv:2112.08723.
Dong, Xiaokai Wei, Henghui Zhu, Xinchi Chen,
PengXu,ZhihengHuang,AndrewArnold,andDan
Nathaniel Weir, Kate Sanders, Orion Weller, Shreya
Roth.2022. Entailmenttreeexplanationsviaitera-
Sharma, Dongwei Jiang, Zhengping Zhang, Bha-
tiveretrieval-generationreasoner. InFindingsofthe
vana Dalvi Mishra, Oyvind Tafjord, Peter Jansen,
AssociationforComputationalLinguistics: NAACL
Peter Clark, et al. 2024. Enhancing systematic de-
2022,pages465–475,Seattle,UnitedStates.Associ-
compositionalnaturallanguageinferenceusinginfor-
ationforComputationalLinguistics.
mallogic. arXivpreprintarXiv:2402.14798.
Mahsan Nourani, Chiradeep Roy, Tahrima Rahman, NathanielWeirandBenjaminVanDurme.2023. Dy-
EricDRagan,NicholasRuozzi,andVibhavGogate. namicgenerationofgroundedlogicalexplanations
2020. Don’texplainwithoutverifyingveracity: an inaneuro-symbolicexpertsystem.
evaluationofexplainableaiwithvideoactivityrecog-
nition. arXivpreprintarXiv:2005.02335. Junbin Xiao, Angela Yao, Yicong Li, and Tat Seng
Chua. 2023. Can i trust your answer? visually
groundedvideoquestionanswering. arXivpreprint
Ishaan Singh Rawal, Shantanu Jaiswal, Basura Fer-
arXiv:2309.01327.
nando,andChestonTan.2023. Revealingtheillusion
ofjointmultimodalunderstandinginvideoqamodels.
Ning Xie, Farley Lai, Derek Doran, and Asim Ka-
arXivpreprintarXiv:2306.08889.
dav. 2019. Visual entailment: A novel task for
fine-grained image understanding. arXiv preprint
Chiradeep Roy, Mahesh Shanbhag, Mahsan Nourani,
arXiv:1901.06706.
Tahrima Rahman, Samia Kabir, Vibhav Gogate,
NicholasRuozzi,andEricDRagan.2019. Explain-
AntoineYang,AntoineMiech,JosefSivic,IvanLaptev,
ableactivityrecognitioninvideos. InIUIWorkshops,
andCordeliaSchmid.2022. Zero-shotvideoques-
volume2.
tionansweringviafrozenbidirectionallanguagemod-
els. AdvancesinNeuralInformationProcessingSys-
Emmanuelle Salin, Badreddine Farah, Stéphane Ay- tems,35:124–141.
ache,andBenoitFavre.2022. Arevision-language
transformerslearningmultimodalrepresentations? a Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit
probingperspective. InProceedingsoftheAAAICon- Bansal.2023. Self-chainedimage-languagemodel
ferenceonArtificialIntelligence,volume36,pages forvideolocalizationandquestionanswering. arXiv
11248–11257. preprintarXiv:2305.06988.
11Amir Zadeh, Michael Chan, Paul Pu Liang, Edmund
Tong,andLouis-PhilippeMorency.2019. Social-iq:
Aquestionansweringbenchmarkforartificialsocial
intelligence. InProceedingsoftheIEEE/CVFCon-
ferenceonComputerVisionandPatternRecognition,
pages8807–8817.
Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu,
HuiqiDeng,HengyiCai,ShuaiqiangWang,Dawei
Yin,andMengnanDu.2023. Explainabilityforlarge
languagemodels: Asurvey. ACMTransactionson
IntelligentSystemsandTechnology.
ZhouZhao,ShuwenXiao,ZehanSong,ChujieLu,Jun
Xiao,andYuetingZhuang.2020. Open-endedvideo
questionansweringviamulti-modalconditionalad-
versarial networks. IEEE Transactions on Image
Processing,29:3859–3870.
ZhouZhao, ZhuZhang, ShuwenXiao, ZhouYu, Jun
Yu, Deng Cai, Fei Wu, and Yueting Zhuang. 2018.
Open-endedlong-formvideoquestionansweringvia
adaptivehierarchicalreinforcednetworks. InIJCAI,
volume2,page8.
Yaoyao Zhong, Junbin Xiao, Wei Ji, Yicong Li, Wei-
hongDeng,andTat-SengChua.2022. Videoques-
tionanswering: Datasets,algorithmsandchallenges.
arXivpreprintarXiv:2203.01225.
Tao Zhuo, Zhiyong Cheng, Peng Zhang, Yongkang
Wong,andMohanKankanhalli.2019. Explainable
videoactionreasoningviapriorknowledgeandstate
transitions. InProceedingsofthe27thacminterna-
tionalconferenceonmultimedia,pages521–529.
Yeyun Zou and Qiyu Xie. 2020. A survey on vqa:
Datasetsandapproaches. In20202ndInternational
Conference on Information Technology and Com-
puterApplication(ITCA),pages289–297.IEEE.
12A TV-TREESLLMPrompts
WeprovidetheLLMandVLMpromptsusedintheTV-TREESpipelineinFigures9-16.
B VisualPromptAnonymizationExperiments
WeconsideranadditionalcomponenttotheTV-TREESsystemoutlinedinSection4thatanonymizesany
referencestocharacterspassedintothevisualentailmentmodule. Wepassanyquestionsthatwillbe
usedforvisualQApromptsthroughaGPTfilterthatreplacesanycharacternameswithcommonnouns
andpronounslike“theman",“they",and“thedoctor". Wereportresultsbelow,comparingthisalternate
systemtothecompetingmethodsandthestandardTV-TREESmethod. Wefindthattheanonymization
paradigm results in a TVQA accuracy score of 48.1% compared to the standard system’s 49.4%. We
providetheanonymizationGPTpromptinFigure13andaresultstableforcomparison(Table4).
C AmazonMechanicalTurkDetails
WeevaluategeneratedtreequalitythroughcrowdsourcedworkersonAmazonMechanicalTurkwiththree
mainannotationtasks. Weidentifyaseparategroupofqualityannotatorsforeachtaskby(1)settingthe
qualificationsforthetasktoworkerslocatedwithintheUnitedStateswithaHITacceptancerateof98%
andover1000completedHITS,and(2)runningapilottaskwithcarefullyselectedquestionstoidentify
annotatorswhoanswerthepreselectedquestionswithhighaccuracy.
We estimate time completion for each version of the task uploaded to Mechanical Turk and set the
paymentvaluestoanestimated$15perhour. Noidentifiableinformationofanyannotatorsispresentin
thispaperorinanyartifactswewillrelease.
D HumanTreeEvaluationTasks
Below,weincludescreenshotsdepictingtheinstructionsandformatofeachtaskprovidedtoannotators.
Wealsoincludeatabledetailingthedescriptionsprovidedtoannotatorsforeachofthefiveacceptability
scores(Table5).
Acceptability: SeeFigures4and5.
Relevance: SeeFigure6.
Sufficiency: SeeFigures7and8.
E GPT-4EvaluationPrompts
PromptsforGPT-4evaluationsareshowninFigures17-19. Figure17showstheprimarydecomposition
evaluation prompt, which accounts for relevancy, distinctness, and sufficiency. Figure 18 shows the
textualacceptabilityfordialogueprompt,andFigure19showsthevisualacceptabilityforscreenshots
prompt,whichwaspassedtoGPT-4V.
13Figure4: AMTacceptabilitytaskinstructionsandexampleforpremiseswithtextualevidence.
Figure5: AMTacceptabilitytaskinstructionsandexampleforpremiseswithvisualevidence.
Figure6: AMTrelevancetaskinstructionsandexample.
14Figure7: AMTsufficiencytaskinstructions.
Figure8: AMTsufficiencytaskexample.
15Method FrozenBiLM SeVILA VideoChat2 TV-TREES‡ TV-TREES TV-TREES*
TVQAAcc. 26.3 38.2 40.6 44.9 49.4 48.1
Table4: TablecontextualizingtheanonymizedVQAinputsablationexperiment(TV-TREES*)bycomparingitto
theotherzero-shotTVQAresults.
Score Description
1 Sentenceiscontradictedbythescreenshotordialogue.
2 Sentenceisunlikelytobetruebasedonthescreenshotordialogue.
3 Sentenceispurelyambiguousgiventhescreenshotordialogue.
4 Sentenceislikelytobetruebasedonthescreenshotordialogue.
5 Sentenceisdirectlysuggestedorshownbythescreenshotordialogue.
Table5: Descriptionsforeachacceptabilityscoreprovidedtoannotatorsaspartoftheslidingbarfunctionalityin
thetask.
HypothesisGenerationPrompt
Convert each of the answer options for the following questions into GRAMMATICAL
ANSWER SENTENCES. Make sure that they are FULL and COMPLETE sentences, not just
words. They should be sentences that you can "prove" by reasoning about the
situation. Proving the sentence should amount to choosing choosing that answer
option over the other one(s).
## Input
QUESTION:
{ICL Q Examples}
## Output
{ICL A Examples}
## Input
QUESTION:
{Questions}
## Output
Figure9: ExamplepromptforgeneratinghypothesesfromQApairsasdescribedinSection4.2.
Hypothesis-To-QuestionGenerationPrompt
Rewrite the following statement into a "yes" or "no" question, and nothing else.
STATEMENT: "{Statement}"
QUESTION:
Figure10: Examplepromptforgeneratinginterrogativeformsofhypothesesforconditioninginferencegeneration
andVQAasdescribedinSection4.3.
16HypothesisDecompositionPrompt
You are a writing system that values clarity above all else. You NEVER uses
pronouns like "he", "they", or "it" to ensure that readers can understand your
sentences in isolation without additional context.
Your task is to break down the following statement into two, simpler sentences.
STATEMENT: "Lauren closed the door after discussing the party with Kelly."
DECOMPOSITION (USING NO PRONOUNS, INCLUDING "THEY" OR "HE" OR "SHE"):
(1) "Lauren closed the door."
(2) "Lauren discussed the party with Kelly."
STATEMENT: "Jason asked about the brown briefcase because he was concerned that it
had been misplaced or stolen."
DECOMPOSITION (USING NO PRONOUNS, INCLUDING "THEY" OR "HE" OR "SHE"):
(1) "Jason asked about the brown briefcase."
(2) "Jason was concerned that the brown briefcase had been misplaced or stolen."
STATEMENT: "{Statement}"
DECOMPOSITION (USING NO PRONOUNS, INCLUDING "THEY" OR "HE" OR "SHE"):
Figure11: ExamplepromptfordecomposingahypothesisintotwodistinctpremisesasdescribedinSection4.5.
InferenceGenerationPrompt
You are a fact-checking expert that uses evidence to answer questions about a TV
show.
For the following question and scene dialogue, write a set of five independent
inferences entailed by some part of the scene. The inferences should resemble
short, factual statements about the scene and should help to answer the question
using component reasoning steps.
Write your facts in JSON format, i.e. {"1": "<answer here>", "2": "<answer
here>", ...} and nothing else.
QUESTION: "Why does Howard say they´re late after walking in?"
SCENE:
{Dialogue}
INFERENCES (5 total):
Figure 12: Example prompt for generating inferences from dialogue samples given an underlying question as
describedinSection4.3.
17Premise-DialogueEntailmentVerificationFilteringPrompt
You are an expert social reasoning system that understands the implied meanings
of complex conversations between TV show characters. Given social inferences made
by other AI systems about transcripts, you score them on whether they are CORRECT or
NOT SUPPORTED by the transcript.
Given the following TV show transcript, write whether each of the following
statements about the TV show are CORRECT or NOT SUPPORTED. A statement is CORRECT
if an average human would agree that it is most likely true based on the transcript,
and is NOT SUPPORTED otherwise.
Write your facts in JSON format, i.e. {"1": <"answer here">, "2": <"answer
here">, ...} and nothing else.
TRANSCRIPT:
{Dialogue}
STATEMENTS:
{Inferences}
OUTPUT:
Figure13: ExamplepromptforfilteringpremisesbasedondialogueentailmentasdescribedinSection4.3.
QuestionAnonymizationPrompt
Anonymize the following questions by replacing all the characters’ names replaced
with ¨the man¨, ¨the woman¨, ¨the person¨, or ¨the people¨. Your output should be formatted
as a serialized JSON list, i.e. {¨q1¨: ¨<answer here>¨, ¨q2¨: ¨<answer here>¨}, ..., and
nothing else.
SENTENCES:
{Questions}
QUESTIONS:
Figure14: Examplepromptforgeneratinganonymizedversionsofinterrogativeversionsofhypothesesasdescribed
inAppendixB.
Premise-HypothesisEntailmentVerificationFilteringPrompt
You are a logical reasoning system that determines whether individual facts are
enough to prove a hypothesis statement.
For each of the following independent facts, answer "YES" if the fact cannot be
true without the hypothesis also being true, and "NO" if the hypothesis can be false
even if the fact is true. Always answer "NO" if the hypothesis is not a complete
sentence (for example "is sitting.". Write your answers in JSON format, i.e. {"1":
"<fact 1 answer here>", "2": "<fact 2 answer here>", ...} and nothing else.
HYPOTHESIS: {Hypothesis}
FACTS:
{Inferences}
OUTPUT:
Figure15: ExamplepromptforfilteringpremisesbasedonhypothesisentailmentasdescribedinSection4.4.
VisualQAPrompt
From this image, can you answer the question {Question}? If so, answer the
question, otherwise, answer ¨NOT ENOUGH INFO¨.
Figure16: PrompttemplateforsolicitingVQAoutputsfromtheLLaVA-7BmodelasdescribedinSection4.6.
18GPT-4Relevance,Distinctness,andSufficiencyEvaluation
You are a reasoning system that searches for proofs of a hypothesis about a video
clip by recursively decomposing it into simpler premises.
Given a hypothesis, you identify entries in a list of possible two-premise
decompositions of the hypothesis that are “well-formed”: Proving the premises
of a well-formed decomposition would amount to proving the hypothesis through
compositional entailment.
You assess decompositions using three metrics: Premise relevancy, premise
distinctness, and decomposition sufficiency. Each decomposition should receive
two relevancy and distinctness scores, one for each premise, but only one single
sufficiency score.
RELEVANCY: Relevancy measures whether a premise contributes information pertaining
to the hypothesis. This is measured on a binary scale. Simply, if the premise
mentions an entity or idea also mentioned by the hypothesis, the relevancy score is
1. Otherwise, it is 0.
DISTINCTNESS: Distinctness measures whether a premise introduces new information not
already entailed by the other premise in the decomposition. This is measured on a
binary scale. If the premise only introduces information already entailed by the
other premise in the decomposition, the distinctness score is 0. Otherwise, it is 1.
If both premises are the same, both receive a score of 0.
SUFFICIENCY: Sufficiency measures whether the two premises cover all the information
introduced by the hypothesis. This is also measured on a binary scale. If, when
considering both premises, the hypothesis introduces new information not covered by
the decompositional premises, the sufficiency score is 0. If the hypothesis does
not introduce new information, the sufficiency score is 1.
For the following decompositions, score each decomposition’s relevancy and
sufficiency. Decompositions will be presented in the form “(<decomposition number>)
H: <hypothesis> & P1: <decomp premise 1> & P2: <decomp premise 2>”. Your answer
should be a list of entries taking the form “(<decomposition number>) RELEVANCY:
(<premise 1 score>, <premise 2 score>), DISTINCTNESS: ((<premise 1 score>, <premise
2 score>), SUFFICIENCY: (<overall score>)”.
DECOMPOSITIONS:
{Decompositions}
JUDGEMENTS (one line per decomposition):
Figure17: GPT-4promptforscoringtherelevance,distinctness,andsufficiencyofdecompositionsinanentailment
tree.
GPT-4TextualAcceptabilityEvaluation
Based on the dialogue from the TV show, how likely is it that the statements below
are true? Score the likelihood of each statement on a 1-5 scale, where 1 indicates
the dialogue contradicts the statement, 2 indicates the statement is unlikely to be
true given the dialogue, 3 indicates the statement is ambiguous given the dialogue,
4 indicates the statement is likely to be true given the dialogue, and 5 indicates
that the statement must be true given the dialogue. Write your numerical scores in
the same order as the listed statements, separated by commas, and nothing else.
Dialogue:
{Dialogue}
Statements:
{Statements}
Figure18: GPT-4promptforscoringtheacceptabilityofentailmenttreeleafnodesthatcitetextualevidence.
19GPT-4VVisualAcceptabilityEvaluation
Based on the screenshot from the TV show, how likely is it that the statement below
is true? Score the likelihood on a 1-5 scale, where 1 indicates the screenshot
contradicts the statement, 2 indicates the statement is unlikely to be true given
the screenshot, 3 indicates the statement is ambiguous given the screenshot, 4
indicates the statement is likely to be true given the screenshot, and 5 indicates
that the statement must be true given the screenshot. Write your numerical score
and nothing else.
Statement: {Statement}
Figure19: GPT-4Vpromptforscoringtheacceptabilityofentailmenttreeleafnodesthatcitevisualevidence. The
top-scoringvideoframeispassedinalongsidetheprompt.
20