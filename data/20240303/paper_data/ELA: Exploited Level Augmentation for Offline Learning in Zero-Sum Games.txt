ELA: Exploited Level Augmentation for Offline Learning in Zero-Sum Games
ShiqiLei*1 KanghoonLee*2 LinjingLi1 JinkyooPark2 JiachenLi3
Abstract
Offline Learning Algorithm
Trained Model
Offlinelearninghasbecomewidelyuseddueto (e.g. IL or offline RL)
itsabilitytoderiveeffectivepoliciesfromoffline
datasetsgatheredbyexpertdemonstratorswith- Offline
outinteractingwiththeenvironmentdirectly. Re- Dataset
cent research has explored various ways to en- Expert vs vs Amateur
hanceofflinelearningefficiencybyconsidering
the characteristics (e.g., expertise level or mul-
tiple demonstrators) of the dataset. However, a 0 Exploited Level
differentapproachisnecessaryinthecontextof
Figure1.IllustrationofELA(ExploitedLevelAugmentation)for
zero-sum games, where outcomes vary signifi-
offlinelearninginzero-sumgames.Itlearnstheexploitedlevelin
cantlybasedonthestrategyoftheopponent. In anunsupervisedmannerandprioritizestrajectoriescreatedbythe
this study, we introduce a novel approach that dominantbehaviorinofflinelearning.
usesunsupervisedlearningtechniquestoestimate
theexploitedlevelofeachtrajectoryfromtheof- theseissues,manymethodshaveemergedtoenableefficient
flinedatasetofzero-sumgamesmadebydiverse learningusingofflinedatasetsgeneratedbydemonstrators.
demonstrators. Subsequently,weincorporatethe Forexample,behaviorcloning(Pomerleau,1988)replicates
estimatedexploitedlevelintotheofflinelearning actions from the offline dataset, assuming demonstrators
tomaximizetheinfluenceofthedominantstrat- areexperts. Itofferseaseofimplementationandlearning
egy. Ourmethodenablesinterpretableexploited butissensitivetosuboptimaldemonstrationsandlimited
levelestimationinmultiplezero-sumgamesand generalization. Incontrast,offlinereinforcementlearning
effectivelyidentifiesdominantstrategydata.Also, (Fujimotoetal.,2019;Kumaretal.,2020)aimstoderive
ourexploitedlevelaugmentedofflinelearningsig- anoptimalpolicyfromthedataset. Whileitexhibitsrobust-
nificantlyenhancestheoriginalofflinelearning nesstosuboptimaldataandgeneralizationissues,itposes
algorithms including imitation learning and of- challengeswhendealingwithsmallorbiaseddatasets.
flinereinforcementlearningforzero-sumgames.
Inreal-worldscenarios,offlinedatasetsexhibitdiversityin
variousaspects. Onenotableaspectconcernsthecoverage
1.Introduction ofthelabels(state,action,andreward)inthedata. Ideally,
datashouldcontainfullpairsofstates,actions,andrewards
Although reinforcement learning has become a powerful
foreffectiveofflinelearning. However, practicaldatasets
techniqueextensivelyappliedfordecisionmakinginvar-
oftencomprisesequencesofstate-actionpairswithoutre-
ious domains such as robotic manipulation, autonomous
wardlabelsor,insomecases,onlystates(Daietal.,2023;
driving,andgameplaying(Andrychowiczetal.,2020;Chen
Ettingeretal.,2021). Toaddressthesechallenges,several
etal.,2019;Vinyalsetal.,2019), conventionalreinforce-
methodshavebeenproposed,includingtechniquesforlabel-
mentlearningdemandssubstantialonlineinteractionswith
ingviainversedynamicmodel(Bakeretal.,2022;Luetal.,
theenvironment,whichisaprocessthatcanbebothcostly
2022)orrewardlabeling(Yuetal.,2022). Anotheraspect
and sample inefficient while potentially leading to safety
ofdiversityisrelatedtothecharacteristicsofthedemonstra-
risks(Berneretal.,2019;Bojarskietal.,2016). Toaddress
torsforgeneratingthedataset. Forexample,demonstrators
*Equalcontribution 1InstituteofAutomation,ChineseAcademy canvaryintermsoftaskexecutionmethodsorskilllevels.
of Sciences, Beijing, China 2Department of Industrial & Sys- Morespecifically,differentdemonstratorsmayexhibitdis-
temsEngineering, KAIST, Daejeon, SouthKorea 3Department tinctpreferencesandemployvariousapproachesevenwhen
of Electrical and Computer Engineering, University of Califor-
performingthesametask,resultinginmulti-modaldatasets
nia,Riverside,California,USA.Correspondenceto: JiachenLi
(Shafiullah et al., 2022). Meanwhile, diversity may arise
<jiachen.li@ucr.edu>.
fromdifferentskilllevelsamongdemonstrators,whichnot
Preprint.Underreview. onlyintroducesmulti-modalchallengesbutalsocomplicates
1
4202
beF
82
]TG.sc[
1v71681.2042:viXraELA:ExploitedLevelAugmentationforOfflineLearninginZero-SumGames
theidentificationofirrelevantdatathathindersthelearning knowledgeoftheenvironmentdynamics,andmanymethods
process(Mandlekaretal.,2021). Tomitigatetheseissues, followitsstructure(Dingetal.,2019).Offlinereinforcement
Pearceetal.(2023)includesmodelsforhandlingdemon- learning (Ernst et al., 2005) emerged and raised with the
stratormulti-modalityandBeliaevetal.(2022)proposesa developmentofreinforcementlearning,andmanymethods
methodtodistinguishhighlyskilleddemonstrators. havebeenproposedinrecentyears(Fujimotoetal.,2019;
Whiletherehasbeensignificantresearchonlearningfrom Kumaretal.,2020). Nevertheless, offlineRLdemandsa
offlinedatainmulti-agentsystems(Panetal.,2022),han- rewardforeachtimestep,imposingsignificantconstraints
dlingtheuniquecharacteristicsofthedemonstratorsisstill onitsapplicability. Sinceouraugmentationmethoddoes
alargelyunderexploredarea. Specifically,incompetitive nothavesuchrequirements,weonlydiscussmethodsthat
environmentssuchaszero-sumgames,thedatadistribution usethetrajectoryinformationandterminalrewardtodeal
isinfluencedbytheattributesandexpertiseofthepartici- withsuboptimaldemonstrationsinthefollowing.
patingplayers. Therefore,unlikesingle-agentofflinedata, IRL-basedmethods. Brownetal.(2019)proposedanIRL-
itiscrucialtoextractasuitablerepresentationbyconsider- basedTrajectory-rankedRewardExtrapolation(T-REX)al-
ingtheindividualcharacteristicsofeachparticipantinthe gorithm,whichextrapolatesapproximatelyrankeddemon-
game. Additionally,becausethestrategiesinsuchgames strations, sothatbetter-designedrewardfunctionscanbe
cantakediverseforms(Czarneckietal.,2020),successful derivedfrompoordemonstrations. Afterthat,Distrubance-
learningfromofflinedatainzero-sumgamesrequiresadeep basedRewardExtrapolation(D-REX)(Brownetal.,2020)
understandingandconsiderationofthesefactors. generatesrankeddemonstrationsbyintroducingnoiseinto
Inthiswork,weintroduceELA(i.e.,ExploitedLevelAug- a BC-learned policy and leveraging T-REX. Chen et al.
mentation)forofflinelearningdesignedtolearnfromlow- (2021)highlightedalimitationofD-REXthatLuce’srule
exploited behavioral data. ELA discerns exploited levels inaccuratelydepictsthenoise-performancerelationshipand
withindiversedemonstrators’offlinedatasetsinzero-sum proposed a method to minimize the effect of suboptimal
games. Figure1illustratesanoverviewofELA. demonstrationsbygeneratingoptimality-parameterizeddata.
WhileIRL-basedmethodsoutperformdemonstrationswith
Themaincontributionsofthispaperareasfollows:
limitedexpertise,theyfacechallengesinzero-sumgames,
• WeproposethePartially-trainable-conditionedVaria-
where agent interactions with the environment and oppo-
tional Recurrent Neural Network (P-VRNN) and an
nentssignificantlyinfluencetheterminalreward.
unsupervisedframeworkforlearningstrategyrepresen-
BC-basedmethods. Sasaki&Yamashina(2020)enhances
tationoftrajectoriesinmulti-agentgames.
BC for noisy demonstrations, while TRAIL (Yang et al.,
• WedefinetheEL(i.e.,ExploitedLevel)ofthestrate-
2021) achieves sample-efficient imitation learning via a
gies,andproposeanunsupervisedmethodforestimat-
factoredtransitionmodel. Play-LMP(Lynchetal.,2020)
ingtheexploitedlevelintheofflinedatasetgenerated
leveragesunsupervisedrepresentationlearninginalatent
byvariousdemonstratorsforzero-sumgames.
planspaceforimprovedtaskgeneralization. However,em-
• WeintroduceELA,atechniqueforofflinelearningthat
ployingavariationalauto-encoder(VAE)withtheencoder
incorporatestheexploitedlevelofeachtrajectory. Itis
outputting latent plans is unsuitable for zero-sum games,
compatiblewithvariousofflinelearningalgorithms.
potentiallyleakingopponentinformationfromtheobserva-
• We demonstrate that our EL estimator serves as an
tionsanddisruptingtheevaluationofthedemonstrator.
effectiveindicatorinzero-sumgames,includingRock-
Paper-Scissors (RPS), Two-player Pong, and Limit IL with representation learning. The work by Beliaev
TexasHold’em. ELAsignificantlyenhancesbothimi- etal.(2022)closelyalignswithourresearch, sharingthe
tationandofflinereinforcementlearningperformance. primary goal of extracting expertise levels of trajectories.
Theyassumedthatthedemonstratorhasavectorindicating
2.RelatedWork theexpertiseoflatentskills,witheachskillrequiringadif-
ferentlevelataspecificstate. Theseelementsjointlyderive
In the early days of learning from demonstrations (LfD)
theexpertiselevel. Themethodalsoconsidersthepolicy
orimitationlearning(IL),behaviorcloning(BC)(Bain&
worse when it is closer to uniformly random distribution.
Sammut,1995)andinversereinforcementlearning(IRL)
However,thisassumptioncannotbesatisfiedeveninsimple
(Russell, 1998) were proposed to learn behavior policies
gameslikeRock-Paper-Scissors(RPS),whereauniformly
fromofflinedata. Inrecentyears,lotsofdifferentmethods
random strategy constitutes a Nash equilibrium. Grover
haveemerged,suchasDAgger(Rossetal.,2011),GAIL
et al. (2018) also studied learning policy representations,
(Ho & Ermon, 2016), and other variants based on BC or
buttheyusedtheinformationofagentidentificationduring
IRL (Ziebart et al., 2008). However, some of them (e.g.,
training,whichenablesthemtoaddalosstodistinguishone
DAgger) require the experts to make real-time decisions
agentfromothers. Inourwork,weproposeamethodfor
whenencounteringnewsituationsratherthansolelyrelying
policyrepresentationsthateffectivelycapturesoptimalityin
onofflinedata. Someothermethods(e.g.,GAIL)require
2ELA:ExploitedLevelAugmentationforOfflineLearninginZero-SumGames
azero-sumgamewithoutrelyingonagentzidentification.
<latexit sha1_base64="lZaAIlWhaB0NITUGnABhE0DC7Dk=">AAAB6HicbZC7SgNBFIbPeo3xFrUUZDEIVmHXQtMZsLFMwFwgWcLs5GwyZnZ2mZkV4pLSysZCEVufIpUPYecz+BJOLoUm/jDw8f/nMOccP+ZMacf5spaWV1bX1jMb2c2t7Z3d3N5+TUWJpFilEY9kwycKORNY1UxzbMQSSehzrPv9q3Fev0OpWCRu9CBGLyRdwQJGiTZW5b6dyzsFZyJ7EdwZ5C8/RpXvh6NRuZ37bHUimoQoNOVEqabrxNpLidSMchxmW4nCmNA+6WLToCAhKi+dDDq0T4zTsYNImie0PXF/d6QkVGoQ+qYyJLqn5rOx+V/WTHRQ9FIm4kSjoNOPgoTbOrLHW9sdJpFqPjBAqGRmVpv2iCRUm9tkzRHc+ZUXoXZWcM8LbsXJl4owVQYO4RhOwYULKME1lKEKFBAe4RlerFvryXq13qalS9as5wD+yHr/Aa9/kUI=</latexit> <latexit sha1_base64="jNC0o7kKNiz99T8b8WcwAFO3q6M=">AAAB/HicbVDLSsNAFJ3UV1tf0W4EN4NFaBeWREG7EYoiuKxgH9CEMJlO26GTSZiZCDFU/BM3LhRx48IPcSf4MU5bF9p64MLhnHu59x4/YlQqy/o0MguLS8sr2Vx+dW19Y9Pc2m7KMBaYNHDIQtH2kSSMctJQVDHSjgRBgc9Iyx+ej/3WDRGShvxaJRFxA9TntEcxUlryzMKB8I5OL0pOREuOQrE3LJc9s2hVrAngPLF/SLG2c/uVu387q3vmh9MNcRwQrjBDUnZsK1JuioSimJFR3okliRAeoj7paMpRQKSbTo4fwX2tdGEvFLq4ghP190SKAimTwNedAVIDOeuNxf+8Tqx6VTelPIoV4Xi6qBczqEI4TgJ2qSBYsUQThAXVt0I8QAJhpfPK6xDs2ZfnSfOwYh9X7CudRhVMkQW7YA+UgA1OQA1cgjpoAAwS8ACewLNxZzwaL8brtDVj/MwUwB8Y79/GPpaG</latexit> r =E(⇡(⌧ ))
3 k
 
3.Preliminaries (<latexit sha1_base64="ehAmZAPoHStRRgyg9SBbUdnpjJA=">AAAB7nicbVA9SwNBEJ2LX0n8itoINotBiBDCnYWmDNpYRvCSQHKEvc1esmRv79jdE+IR8C/YWChia+1PsRP8MW4+Ck18MPB4b4aZeX7MmdK2/WVlVlbX1jeyufzm1vbObmFvv6GiRBLqkohHsuVjRTkT1NVMc9qKJcWhz2nTH15N/OYdlYpF4laPYuqFuC9YwAjWRmqW7LJddk67haJdsadAy8SZk2Lt8P479/BxWe8WPju9iCQhFZpwrFTbsWPtpVhqRjgd5zuJojEmQ9ynbUMFDqny0um5Y3RilB4KImlKaDRVf0+kOFRqFPqmM8R6oBa9ifif1050UPVSJuJEU0Fmi4KEIx2hye+oxyQlmo8MwUQycysiAywx0SahvAnBWXx5mTTOKs55xbkxaVRhhiwcwTGUwIELqME11MEFAkN4hGd4sWLryXq13matGWs+cwB/YL3/AC21kR0=</latexit> 0,0,1)
<latexit sha1_base64="oNX9XhUqO+2KLktfAFLJt+VkPf4=">AAACA3icbVDLSgMxFM3UV62vUXe6CS1CXVgmKtqNUBDBhYsK9gGdYcikmTY08yDJCGUouPET/AU3LhQRBMGfcOdH+A+mrQttPXDhcM693HuPF3MmlWV9GpmZ2bn5hexibml5ZXXNXN+oyygRhNZIxCPR9LCknIW0ppjitBkLigOP04bXOx36jWsqJIvCK9WPqRPgTsh8RrDSkmtu7dm+wCRFg/RwINyDk7OLoq1w4vZ2XbNglawR4DRBP6RQse5Q/vXtq+qaH3Y7IklAQ0U4lrKFrFg5KRaKEU4HOTuRNMakhzu0pWmIAyqddPTDAO5opQ39SOgKFRypvydSHEjZDzzdGWDVlZPeUPzPayXKLzspC+NE0ZCMF/kJhyqCw0BgmwlKFO9rgolg+lZIulhnonRsOR0Cmnx5mtT3S+iohC51GmUwRhZsgzwoAgSOQQWcgyqoAQJuwD14BE/GrfFgPBsv49aM8TOzCf7AeP8Gapeaag==</latexit> 1
ComponentsofanN-playerzero-sumgameareasfollows:
(<latexit sha1_base64="ehAmZAPoHStRRgyg9SBbUdnpjJA=">AAAB7nicbVA9SwNBEJ2LX0n8itoINotBiBDCnYWmDNpYRvCSQHKEvc1esmRv79jdE+IR8C/YWChia+1PsRP8MW4+Ck18MPB4b4aZeX7MmdK2/WVlVlbX1jeyufzm1vbObmFvv6GiRBLqkohHsuVjRTkT1NVMc9qKJcWhz2nTH15N/OYdlYpF4laPYuqFuC9YwAjWRmqW7LJddk67haJdsadAy8SZk2Lt8P479/BxWe8WPju9iCQhFZpwrFTbsWPtpVhqRjgd5zuJojEmQ9ynbUMFDqny0um5Y3RilB4KImlKaDRVf0+kOFRqFPqmM8R6oBa9ifif1050UPVSJuJEU0Fmi4KEIx2hye+oxyQlmo8MwUQycysiAywx0SahvAnBWXx5mTTOKs55xbkxaVRhhiwcwTGUwIELqME11MEFAkN4hGd4sWLryXq13matGWs+cwB/YL3/AC21kR0=</latexit>0,0,1)
 4r 3=EL(⌧ k)
• PlayersetP: P = {1,...,N }; (<latexit sha1_base64="Ml0VKnRc36i3gYw7mID/55aO4AM=">AAACF3icbVDLSsNAFJ3UV62vqLhyEyxCfYVEwbosuHFZwT6gDWEynbRDJw9mJkIJ8SvcuHLtL7hxoYhb3fkJ/oWTpKC2HpjhzDn3cuceJ6SEC8P4VAozs3PzC8XF0tLyyuqaur7R5EHEEG6ggAas7UCOKfFxQxBBcTtkGHoOxS1neJ76rWvMOAn8KzEKseXBvk9cgqCQkq3qla7LIIqPmH2SZPcBs83k0DjM9fTxI+/ZatnQjQzaNDHHpFzb/9qq3j/c1G31o9sLUORhXyAKOe+YRiisGDJBEMVJqRtxHEI0hH3ckdSHHuZWnO2VaLtS6WluwOTxhZapvzti6HE+8hxZ6UEx4JNeKv7ndSLhnlkx8cNIYB/lg9yIaiLQ0pC0HmEYCTqSBCJG5F81NIAyDyGjLMkQzMmVp0nzWDdPdfNSpmGAHEWwDXZABZigCmrgAtRBAyBwCx7BM3hR7pQn5VV5y0sLyrhnE/yB8v4NwhChZA==</latexit>  r3 ,0, r1 )
• States:alltheinformationatacertainstatus,includ(<latexit sha1_base64="63zurEKX6kBVFbwTcZQs7uHcPIM=">AAAB7nicbVA9SwNBEJ2LX0n8itoINotBiBDCnYWmDNpYRvCSQHKEvc1esmRv79jdE+IR8C/YWChia+1PsRP8MW4+Ck18MPB4b4aZeX7MmdK2/WVlVlbX1jeyufzm1vbObmFvv6GiRBLqkohHsuVjRTkT1NVMc9qKJcWhz2nTH15N/OYdlYpF4laPYuqFuC9YwAjWRmqW7LJTtk+7haJdsadAy8SZk2Lt8P479/BxWe8WPju9iCQhFZpwrFTbsWPtpVhqRjgd5zuJojEmQ9ynbUMFDqny0um5Y3RilB4KImlKaDRVf0+kOFRqFPqmM8R6oBa9ifif1050UPVSJuJEU0Fmi4KEIx2hye+oxyQlmo8MwUQycysiAywx0SahvAnBWXx5mTTOKs55xbkxaVRhhiwcwTGUwIELqME11MEFAkN4hGd4sWLryXq13matGWs+cwB/YL3/AC23kR0=</latexit>i0n,g1,0)  r3+r1  r3+r1 (<latexit sha1_base64="FfNuprLHVI9xgS+nvMrJZYYqzUE=">AAACF3icbZDLSgMxFIYz9VbrbVRcuQkWod6GmQrWZcGNywr2Am0ZMmmmDc1cSDJCGcancOPKta/gxoUibnXnI/gWZqYFL/VAwsf/n0NyfidkVEjT/NByM7Nz8wv5xcLS8srqmr6+0RBBxDGp44AFvOUgQRj1SV1SyUgr5AR5DiNNZ3iW+s0rwgUN/Es5CknXQ32fuhQjqSRbN0rmYcflCMdH3D5OsvuA2+Vkoqb4Le7ZetE0zKzgNFgTKFb3P7cqd/fXNVt/7/QCHHnEl5ghIdqWGcpujLikmJGk0IkECREeoj5pK/SRR0Q3zvZK4K5SetANuDq+hJn6cyJGnhAjz1GdHpID8ddLxf+8diTd025M/TCSxMfjh9yIQRnANCTYo5xgyUYKEOZU/RXiAVJ5SBVlQYVg/V15GhplwzoxrAuVhgnGlQfbYAeUgAUqoArOQQ3UAQY34AE8gWftVnvUXrTXcWtOm8xsgl+lvX0Bv2OhZw==</latexit>0,  r3 , r2 )
actionhistoryandimperfectinformation;
y
<latexit sha1_base64="h6fXuTVUMN4vlrE4tJCOOADFWAY=">AAAB6HicbVC7SgNBFL0bXzG+oiltBkPAKuxaaMqAjWUC5gHJEmYnN8mY2Qczs8KypLOzsVDE1o/xA+z0A/wCP8DJo9DEAxcO59zLvfd4keBK2/aHlVlb39jcym7ndnb39g/yh0dNFcaSYYOFIpRtjyoUPMCG5lpgO5JIfU9gyxtfTv3WLUrFw+BaJxG6Ph0GfMAZ1UaqJ7180S7bM5BV4ixIsVoo3X2/fX3Wevn3bj9ksY+BZoIq1XHsSLsplZozgZNcN1YYUTamQ+wYGlAflZvODp2QklH6ZBBKU4EmM/X3REp9pRLfM50+1SO17E3F/7xOrAcVN+VBFGsM2HzRIBZEh2T6NelziUyLxBDKJDe3EjaikjJtssmZEJzll1dJ86zsnJedukmjAnNk4RhO4BQcuIAqXEENGsAA4R4e4cm6sR6sZ+tl3pqxFjMF+APr9QfhRJFr</latexit>
(<latexit sha1_base64="tkBER5FWHhV9LPH+8ogYwIp61Lw=">AAAB7nicbVA9SwNBEJ2LX0n8itoINotBiBDCnYWmDNpYRvCSQHKEvc1esmRv79jdE+IR8C/YWChia+1PsRP8MW4+Ck18MPB4b4aZeX7MmdK2/WVlVlbX1jeyufzm1vbObmFvv6GiRBLqkohHsuVjRTkT1NVMc9qKJcWhz2nTH15N/OYdlYpF4laPYuqFuC9YwAjWRmqWnLJdtk+7haJdsadAy8SZk2Lt8P479/BxWe8WPju9iCQhFZpwrFTbsWPtpVhqRjgd5zuJojEmQ9ynbUMFDqny0um5Y3RilB4KImlKaDRVf0+kOFRqFPqmM8R6oBa9ifif1050UPVSJuJEU0Fmi4KEIx2hye+oxyQlmo8MwUQycysiAywx0SahvAnBWXx5mTTOKs55xbkxaVRhhiwcwTGUwIELqME11MEFAkN4hGd4sWLryXq13matGWs+cwB/YL3/AC25kR0=</latexit>1,0,0)
 r3+r2  r3+r2
• Observationo (s): alltheinformationplayericanget
i (<latexit sha1_base64="63zurEKX6kBVFbwTcZQs7uHcPIM=">AAAB7nicbVA9SwNBEJ2LX0n8itoINotBiBDCnYWmDNpYRvCSQHKEvc1esmRv79jdE+IR8C/YWChia+1PsRP8MW4+Ck18MPB4b4aZeX7MmdK2/WVlVlbX1jeyufzm1vbObmFvv6GiRBLqkohHsuVjRTkT1NVMc9qKJcWhz2nTH15N/OYdlYpF4laPYuqFuC9YwAjWRmqW7LJTtk+7haJdsadAy8SZk2Lt8P479/BxWe8WPju9iCQhFZpwrFTbsWPtpVhqRjgd5zuJojEmQ9ynbUMFDqny0um5Y3RilB4KImlKaDRVf0+kOFRqFPqmM8R6oBa9ifif1050UPVSJuJEU0Fmi4KEIx2hye+oxyQlmo8MwUQycysiAywx0SahvAnBWXx5mTTOKs55xbkxaVRhhiwcwTGUwIELqME11MEFAkN4hGd4sWLryXq13matGWs+cwB/YL3/AC23kR0=</latexit>0,1,0)
atacertainstates S;
∈(<latexit sha1_base64="tkBER5FWHhV9LPH+8ogYwIp61Lw=">AAAB7nicbVA9SwNBEJ2LX0n8itoINotBiBDCnYWmDNpYRvCSQHKEvc1esmRv79jdE+IR8C/YWChia+1PsRP8MW4+Ck18MPB4b4aZeX7MmdK2/WVlVlbX1jeyufzm1vbObmFvv6GiRBLqkohHsuVjRTkT1NVMc9qKJcWhz2nTH15N/OYdlYpF4laPYuqFuC9YwAjWRmqWnLJdtk+7haJdsadAy8SZk2Lt8P479/BxWe8WPju9iCQhFZpwrFTbsWPtpVhqRjgd5zuJojEmQ9ynbUMFDqny0um5Y3RilB4KImlKaDRVf0+kOFRqFPqmM8R6oBa9ifif1050UPVSJuJEU0Fmi4KEIx2hye+oxyQlmo8MwUQycysiAywx0SahvAnBWXx5mTTOKs55xbkxaVRhhiwcwTGUwIELqME11MEFAkN4hGd4sWLryXq13matGWs+cwB/YL3/AC25kR0=</latexit> 1,0,0)
• ActionspaceA (o ): allactionsthatcanbedoneata
certainobservati ioni o i;
x
 <latexit sha1_base64="uhu/tsm010zLlxY2BEF3Wrtlo/4=">AAAB63icbVBNSwMxEJ2tX3W1WvXoJVgKXiy7HrTHgiAeK9hWaJeSTbNtaJJdkqxQlv4FLx4U8ao/xJ/gzX9jtu1BWx8MPN6bYWZemHCmjed9O4W19Y3NreK2u7Nb2tsvHxy2dZwqQlsk5rG6D7GmnEnaMsxwep8oikXIaSccX+V+54EqzWJ5ZyYJDQQeShYxgk0unam+3y9XvJo3A1ol/oJUGqXPtHrtfjT75a/eICapoNIQjrXu+l5iggwrwwinU7eXappgMsZD2rVUYkF1kM1unaKqVQYoipUtadBM/T2RYaH1RIS2U2Az0steLv7ndVMT1YOMySQ1VJL5oijlyMQofxwNmKLE8IklmChmb0VkhBUmxsbj2hD85ZdXSfu85l/U/FubRh3mKMIxnMAp+HAJDbiBJrSAwAge4RleHOE8Oa/O27y14CxmjuAPnPcf8I6QaA==</latexit> r 1  <latexit sha1_base64="IsS6zT5MoUtwoySMiyosgJGzaQA=">AAAB63icbVDLSgNBEOyNr7gajXr0MhgCXgy7OWiOAUE8RjAPSJYwO5lNhszMLjOzQljyC148KOJVP8RP8ObfOHkcNLGgoajqprsrTDjTxvO+ndzG5tb2Tn7X3dsvHBwWj45bOk4VoU0S81h1QqwpZ5I2DTOcdhJFsQg5bYfj65nffqBKs1jem0lCA4GHkkWMYDOTLlS/2i+WvIo3B1on/pKU6oXPtHzjfjT6xa/eICapoNIQjrXu+l5iggwrwwinU7eXappgMsZD2rVUYkF1kM1vnaKyVQYoipUtadBc/T2RYaH1RIS2U2Az0qveTPzP66YmqgUZk0lqqCSLRVHKkYnR7HE0YIoSwyeWYKKYvRWREVaYGBuPa0PwV19eJ61qxb+s+Hc2jRoskIdTOINz8OEK6nALDWgCgRE8wjO8OMJ5cl6dt0VrzlnOnMAfOO8/8hKQaQ==</latexit> r 2
<latexit sha1_base64="J1pP1P6svV5BsCZT/ZKgjFx0tTk=">AAAB6HicbZC7SgNBFIbPeo3xFrUUZDEIVmHXQtMZsLFMwFwgWcLs5GwyZnZ2mZkVw5LSysZCEVufIpUPYecz+BJOLoUm/jDw8f/nMOccP+ZMacf5spaWV1bX1jMb2c2t7Z3d3N5+TUWJpFilEY9kwycKORNY1UxzbMQSSehzrPv9q3Fev0OpWCRu9CBGLyRdwQJGiTZW5b6dyzsFZyJ7EdwZ5C8/RpXvh6NRuZ37bHUimoQoNOVEqabrxNpLidSMchxmW4nCmNA+6WLToCAhKi+dDDq0T4zTsYNImie0PXF/d6QkVGoQ+qYyJLqn5rOx+V/WTHRQ9FIm4kSjoNOPgoTbOrLHW9sdJpFqPjBAqGRmVpv2iCRUm9tkzRHc+ZUXoXZWcM8LbsXJl4owVQYO4RhOwYULKME1lKEKFBAe4RlerFvryXq13qalS9as5wD+yHr/Aax3kUA=</latexit>
• Terminalstatesz Z S: allstatesthatnofurther
∈ ⊂ Figure2.Illustration of EL and exploitability of a strategy in a
actionscanbedone;
two-playerzero-sumgamewiththreepurestrategies.
• Rewardsr
i
:Z R: therewardgiventoplayeri,and
(cid:80) →
i P r i(z)=0, ∀z ∈Z; assume that each trajectory τ can be directly mapped to
• Stra∈tegyπ i(a o i):theprobabilityofplayerichoosing a strategy π(τ). In our setting, where all the players are
|
actionaatobservationo i. competent, each player can be exploited by at most one
Π isthesetofallstrategiesofplayeri. Insymmetriccases, purestrategy. Asfortheoverallstrategydistributionover
i
weuseΠtodenotethestrategyset, i.e. Π i = Π, ∀i. We Π,weassumethe(a 1,a 2,...,a n)hasuniformdistribution
definethereachprobability over(n 1)-dimensionalstandardsimplex.
(cid:89) −
p (s)= π(a o (s)) ThedefinitionofELisasfollows:
π ′ i ′
|
(s′,a′) →s EL(τ)=Eπ[ −r(π,π(τ)) |r(π,π(τ)) ≤0].
astheprobabilityofreachingstateswithstrategyπ,where Foratrajectoryτ ,letr(π ,π(τ )) = r . Byourassump-
k i k i
( os f′, ga o′ i) n→ gtos sm tae ta ens s.th Tat hc eh no ,o wsi eng caa nct nio an tua ra′ la lt ys d′ eis fit nh ee tc hh eoi ec xe - t rion,o 0n ,ly io =ne j.j W∈ e{ c1 a, n2 d, i. r.. e, cn tl} ys sa et eis tfi he as tEth (a πt (r τj < ))0 =,wh ril .e
i k j
pectedrewardofplayeriwithstrategyπas ≥ ∀ ̸ −
Since EL(τ ) is a conditional expectation defined on
(cid:88) k
r (π)= p (z)r (z). Π, we can view it as a conditional expected value
i π i
z Z over an (n 1)-dimensional simplex. When π =
W poe neu nse tsr ti r( aπ te−gi y, ππ i) i.to Ts hp ee bc ei∈ f sy trt eh se pp ol na sy ee or fst or pa pte og ny enπ ti sa tn rad teo gp y- (cid:80) comn i= e1 sa (cid:80)iπ
n
ii =, 1r r− ( iπ a, iπ ≤(τ 0k .)) Th= us,(cid:80) then i= e1 xa pi er ci t, att ih oe nic so sn tid li lt dio en finb ee d-
π is defined as− BR(π ) = argmax r (π ,π ). We over an (n 1)-dimensional simplex, but a smaller one,
ad−di itionallydefinethebe−sti respo (cid:88)nseofsπ ti′ rai tegy−i π iai′ s w iith =v jertices− ({ 0( ,0 ., ... ,.. a,a i == 1,− ..r− .j ,r + 0j r )i, ... T., ha ej n,= w−er cr j ai + nri c, o. n.. s, i0 d) er|
BR(π i)=argmax π −′ i r j(π −′ i,π i), ∀ add̸ ing} an∪ o{ therdimej nsiononthe} simplex,sothatthenew
j P,j=i
∈ ̸ dimensionhasvalue r(π,π(τ)). Duetolinearity,thenew
whichequalstoargmin π −′ ir i(π −′ i,π i)inzero-sumcase.We objectbecomesann-− dimensionalpyramid,andthedesired
definetheexploitabilityofstrategyπas
expectationistheheightofthepyramid’scentroidw.r.t. the
(cid:88)
E(π)= (r (π ,BR(π )) r (π)). surfaceoftheoriginal(n 1)-dimensionalsimplex. From
i −i −i − i calculus,theheightofthe−
centroidofn-dimensionalpyra-
i P
Inzero-sumsymm∈ etriccases,wedefinetheexploitability mid is always 1 of the height of the pyramid w.r.t. its
n+1
ofaplayerstrategyπ as base. Sincetheheightisr ,theexpectationis 1 r . So
i j n+1 j
(cid:88)
E(π )= r (BR(π ),π )= r (BR(π ),π ). 1
i − i i i j i i EL(τ k)= E(π(τ k))
j P,j=i n+1
∈ ̸
alwaysholdsinthiscase,whichshowsthatELisanappro-
4.AnIntuitionofExploitedLevel
priateindicator. Astrategyofagamewiththreedifferent
Inthissection,weprovideanintuitionofExploitedLevel purestrategiesisshownasanexampleinFigure2,withEL
(EL)withatoymodel.Itservesasaproportionalapproxima- andexploitabilityvisualized.
tionofexploitabilitywithacertaindistributiononthestrat- Concretely,consideranRPSgame(seedetailsinAppendix
egyset. Considera2-playerzero-sumsymmetricgamethat C.1)andletπ ,π andπ bethepurestrategiesofchoosing
1 2 3
hasnpurestrategiesπ ,i=1,...,n. Allstrategiesarecon- rock, paper, and scissors, respectively. Let the strategy
i
vexcombinationsofpurestrategies,i.e.,π =
(cid:80)n
a π , of τ be π(τ) = (0,2/3,1/3), i.e. "choosing paper with
where
(cid:80)n
a = 1, 0 a 1, i. For
simplii c= i1 ty,i wi
e 2/3probabilityandchoosingscissorswith1/3probability".
i=1 i ≤ i ≤ ∀
3ELA:ExploitedLevelAugmentationforOfflineLearninginZero-SumGames
si si
zt zt
o1,i o2,i oi
ht-1 φe ht ht-1 φp ht
a1,i a2,i ai
l1 si+1 l2 l1 si+1 l2 l,ot at l,ot at
o
1,i+1
o
2,i+1
oi+1 Trajectory encoder Prior estimator
a1,i+1 a2,i+1 ai+1 zt zt
si+2 si+2
ht-1 φd ht ht-1 φr ht
(a) (b)
Figure3.The basic structures of games with representation-
l,ot at l,ot at
dependentpolicy.(a)Gameswithsimultaneousactions.(b)Games
withsequentialactions. Action decoder Recurrent unit
Thenwecaneasilyderivethat r = r = 1/3, r = Latent variables Trainable parameters Fixed variables
1 2 3
− − − −
2/3. SowehaveE(π(τ))=2/3,whileEL(τ)=1/6.
Figure4. ThenetworkstructureoftheP-VRNNmodel.
5.ProblemFormulation
inoursetting,discerningthestrategydirectlyfromtrajecto-
Considerazero-sumgameandwehaveadatasetofgame riesisnotpossible. Toaddressthislimitation,givenaprior
histories,includingthetrajectoriesofeachplayerandter- distributionofstrategies,wecanderivetheprobabilitydis-
minal rewards. The trajectories are generated by diverse tributionofatrajectory’sstrategy. Consequently,acquiring
players,rangingfromhigh-levelexpertstoamateurs. We representationsoftrajectoriesthatillustrateadistribution
aimtodistinguishtheplayerswithdifferentlevelsandlearn overthestrategyspaceΠemergesasafeasiblesolution. Af-
anexpertpolicyfromthedatasetviaofflinelearning. We terobtainingthestrategyrepresentation,theterminalreward
assumethatwedonothavethedemonstratoridentification. canbeutilizedtoestimatehowwelltheplayerdoesusing
Inourproblem,thetrajectories theproposedestimator. Theseestimations,inturn,canbe
τi,j =((oi,j,ai,j),...,(oi,j ,ai,j )) leveragedtoenhanceofflinelearningalgorithmsbyprior-
0 0 Ti,j Ti,j itizingrelevantdata. Ourmethodconsistsofthreemajor
are collected for each player i and game j, in which oi,j
t procedures,asillustratedinFigure5:
is the observation, ai,j is the action at time t and Ti,j is
t 1. Obtaining the strategy representation with unsuper-
thelengthofthecorrespondingtrajectory. Thedatasetof
visedlearning;
trajectoriesofM gamesisdenotedas
2. Derivingthefunctionofobtainingtheexploitedlevel
Γ= τi,j i=1,...,N,j =1,...,M .
fromthestrategyrepresentation;
{ | }
Weremarkthattheobservationofeachplayerisdifferent 3. Exploitedlevelaugmentedofflinelearning.
in an imperfect information game. The terminal reward
ri,j isrecordedforeachtrajectory. Forsimplicity,welater 6.1.LearningStrategyRepresentation
omitsuperscriptsi,j whenreferringtoasingletrajectory. We propose a Partially-trainable-conditioned Variational
We assume that within a single trajectory, the strategy of Recurrent Neural Network (P-VRNN) where partially-
aplayerisconsistent. Theprobabilityofacertainaction trainable-conditionedmeansthatpartoftheconditiononthe
followingacertainsequenceofobservationsisdenotedas neuralnetworkistrainable, andtheconditionacquisition
π(a l,o 0,...,o t),wherelrepresentsthestrategyrepresen- processisentirelyunsupervised. TheproposedP-VRNN
|
tationvectorofthetrajectory. Thebasicstructuresofgames hasfourmajorcomponentssimilartotheVRNNwithan
withstrategyrepresentationareillustratedinFigure3. additionalcondition,asshowninFigure4. Insteadofusing
thelatentvariableasanindicator(Lynchetal.,2020),we
6.Method
usepartoftheconditionasarepresentationvector.
Evaluatingaplayer’sstrategyfromaspecifictrajectoryin Trajectory encoder. The trajectory encoder obtains the
a dataset of trajectories and terminal rewards of a game latent variable z at time t from the past trajectory infor-
t
is challenging. However, estimating individual strategies mationincludingactiona . Theencoderusesthecurrent
t
becomes feasible with unique player identification in the actiona ,strategyrepresentationl,andallthepastobserva-
t
dataset, consequentlyallowingfortheevaluationofeach tions o t accordingtoFigure3. Itcanbeseenfromthe
{ i }i=1
player’sskilllevelundertheassumptionofaconsistentstrat- computationgraphofP-VRNNthatthelaststeprecurrent
egy. Nevertheless,giventheabsenceofplayeridentification variableh
t
−1containsinformationof {o
i
}it =−11, {a
i
}it =−11and
4ELA:ExploitedLevelAugmentationforOfflineLearninginZero-SumGames
<latexit sha1_base64="Yr1WwOq8Iz3cLBxAvqZ3GnVgixg=">AAAB9HicbVC5TsNAEF2HK4QrHB2NRYREFdkUQEckCigDIoeUWNF6PU5WWR/sjgPBynfQUIAQbSq+hI6SP2FzFJDwpJGe3pvRzDw3FlyhZX0ZmYXFpeWV7GpubX1jcyu/vVNVUSIZVFgkIll3qQLBQ6ggRwH1WAINXAE1t3sx8ms9kIpH4S32Y3AC2g65zxlFLTlNhAdMb+CeSm/QyhesojWGOU/sKSmcfzx+Xw730nIr/9n0IpYEECITVKmGbcXopFQiZwIGuWaiIKasS9vQ0DSkASgnHR89MA+14pl+JHWFaI7V3xMpDZTqB67uDCh21Kw3Ev/zGgn6Z07KwzhBCNlkkZ8IEyNzlIDpcQkMRV8TyiTXt5qsQyVlqHPK6RDs2ZfnSfW4aJ8U7WurULLIBFmyTw7IEbHJKSmRK1ImFcLIHXkiL+TV6BnPxpvxPmnNGNOZXfIHxvAHgCmWPA==</latexit>Reward <latexit sha1_base64="N26my+iYNEnAUJbAt1VQWXgoJaA=">AAAB+nicbVC7SgNBFJ2NryS+NlraDAbBQsKuhVoGbCwj5AXJEmZnZ5Mxsw9m7qrLmsbGr7CxUETs/BI7v0Ynj0ITD1w4nHMv997jxoIrsKwvI7e0vLK6li8U1zc2t7bN0k5TRYmkrEEjEcm2SxQTPGQN4CBYO5aMBK5gLXd4PvZb10wqHoV1SGPmBKQfcp9TAlrqmaUusFvI6pJcMQqRTEc9s2xVrAnwIrFnpFwtPHjv3/dHtZ752fUimgQsBCqIUh3bisHJiAROBRsVu4liMaFD0mcdTUMSMOVkk9NH+EArHvYjqSsEPFF/T2QkUCoNXN0ZEBioeW8s/ud1EvDPnIyHcQIspNNFfiIwRHicA/a41A+LVBNCJde3YjogklDQaRV1CPb8y4ukeVyxTyr2pU7DQlPk0R7aR4fIRqeoii5QDTUQRTfoET2jF+POeDJejbdpa86YzeyiPzA+fgDwkJgT</latexit>Trajectory <latexit sha1_base64="KPIK68mZXadsXOrV+hxhRv3KH2c=">AAAB9HicbVDLSgNBEJyNrxhfUY9eBoPgxbAroh4DXjyFKOYByRJmJ73JkNmHM73BsOQ7vHhQxKsf482/cZLsQRMLGoqqbrq7vFgKjbb9beVWVtfWN/Kbha3tnd294v5BQ0eJ4lDnkYxUy2MapAihjgIltGIFLPAkNL3hzdRvjkBpEYUPOI7BDVg/FL7gDI3kdhCeMK2dNe6r1Um3WLLL9gx0mTgZKZEMtW7xq9OLeBJAiFwyrduOHaObMoWCS5gUOomGmPEh60Pb0JAFoN10dvSEnhilR/1ImQqRztTfEykLtB4HnukMGA70ojcV//PaCfrXbirCOEEI+XyRn0iKEZ0mQHtCAUc5NoRxJcytlA+YYhxNTgUTgrP48jJpnJedy7Jzd1Gq2FkceXJEjskpccgVqZBbUiN1wskjeSav5M0aWS/Wu/Uxb81Z2cwh+QPr8wdhdpHK</latexit>P-VRNN <latexit sha1_base64="PJMR2IT9b0ySm4U0ctqvn3hGvCs=">AAAB/HicbVC7SgNBFJ31GeNrNXY2g0GwCrsWamdAghYWEcwDkiXMTmaTIbOzy8xdMS7xV2wsFNFK/BA7S//EyaPQxAMDh3Pu5Z45fiy4Bsf5submFxaXljMr2dW19Y1Ne2u7qqNEUVahkYhU3SeaCS5ZBTgIVo8VI6EvWM3vnQ392g1TmkfyGvox80LSkTzglICRWnauCewW0tIlLmngIYFIDVp23ik4I+BZ4k5I/vT97vv8bSctt+zPZjuiScgkUEG0brhODF5KFHAq2CDbTDSLCe2RDmsYKknItJeOwg/wvlHaOIiUeRLwSP29kZJQ637om0kTr6unvaH4n9dIIDjxUi7jBJik40NBIjBEeNgEbnPFKIi+IYQqbrJi2iWKUDB9ZU0J7vSXZ0n1sOAeFdwrJ1900BgZtIv20AFy0TEqogtURhVEUR89oCf0bN1bj9aL9ToenbMmOzn0B9bHDwXvmK0=</latexit>ELEstimator <latexit sha1_base64="SvpoXx67d1vx9TTrkOrGMPTC42Q=">AAAB9HicbVBNS8NAEJ34WetX1aOXYCt4KkkP6rEiggcPFewHtKFsttt26WYTdyfFEvo7vHhQxKs/xpv/xk2bg7Y+GHi8N8PMPD8SXKPjfFsrq2vrG5u5rfz2zu7efuHgsKHDWFFWp6EIVcsnmgkuWR05CtaKFCOBL1jTH12nfnPMlOahfMBJxLyADCTvc0rQSF6pg+wJk5u7q2kp3y0UnbIzg71M3IwUIUOtW/jq9EIaB0wiFUTrtutE6CVEIaeCTfOdWLOI0BEZsLahkgRMe8ns6Kl9apSe3Q+VKYn2TP09kZBA60ngm86A4FAveqn4n9eOsX/pJVxGMTJJ54v6sbAxtNME7B5XjKKYGEKo4uZWmw6JIhRNTmkI7uLLy6RRKbvnZfe+Uqw6WRw5OIYTOAMXLqAKt1CDOlB4hGd4hTdrbL1Y79bHvHXFymaO4A+szx9wWJEr</latexit>ELA
O<latexit sha1_base64="6iatD3fh6AMFx78UhLvTBEYFv8A=">AAAB+nicbVC7TsMwFHXKq5RXCiOLRYvEVCUdgLESCxtFog+pjSrHdVqrjhPZN0AV+iksDCDEypew8Tc4bQZoOdKVjs651773+LHgGhzn2yqsrW9sbhW3Szu7e/sHdvmwraNEUdaikYhU1yeaCS5ZCzgI1o0VI6EvWMefXGV+554pzSN5B9OYeSEZSR5wSsBIA7tc7QN7hPQmCLInZtXSwK44NWcOvErcnFRQjubA/uoPI5qETAIVROue68TgpUQBp4LNSv1Es5jQCRmxnqGShEx76Xz1GT41yhAHkTIlAc/V3xMpCbWehr7pDAmM9bKXif95vQSCSy/lMk6ASbr4KEgEhghnOeAhV4yCmBpCqOJmV0zHRBEKJq0sBHf55FXSrtfc85p7W680nDyOIjpGJ+gMuegCNdA1aqIWougBPaNX9GY9WS/Wu/WxaC1Y+cwR+gPr8weFuJN1</latexit> ✏ine
<latexit sha1_base64="xsaj0CIwc/mQJ8rP3aURcYmvKBw=">AAAB6nicbVDLSgNBEOw1PmJ8RcWTl8EgeAq7HtRjwIvHiOYByRJmJ73JkNnZZWZWCEs+wYsHRbz6I/6CB8GTn6KTx0ETCxqKqm66u4JEcG1c99NZyi2vrK7l1wsbm1vbO8XdvbqOU8WwxmIRq2ZANQousWa4EdhMFNIoENgIBpdjv3GHSvNY3pphgn5Ee5KHnFFjpRvV4Z1iyS27E5BF4s1IqZL7+H47+MJqp/je7sYsjVAaJqjWLc9NjJ9RZTgTOCq0U40JZQPaw5alkkao/Wxy6ogcW6VLwljZkoZM1N8TGY20HkaB7Yyo6et5byz+57VSE174GZdJalCy6aIwFcTEZPw36XKFzIihJZQpbm8lrE8VZcamU7AhePMvL5L6adk7K3vXNg0XpsjDIRzBCXhwDhW4girUgEEP7uERnhzhPDjPzsu0dcmZzezDHzivP/bnkfQ=</latexit>ri <latexit sha1_base64="6vEANM/VlEApgPYC41Z+cnWEoOI=">AAACEXicbVDLSsNAFJ34rPUVdaebYBFSKCFxoW6EghsXLlroC9o0TKaTduhkEmYmQin9AsGNv+LGhSK6dOfOv/ATnKRdaOuBuRzOuZc79/gxJULa9pe2tLyyurae28hvbm3v7Op7+w0RJRzhOopoxFs+FJgShuuSSIpbMccw9Clu+sOr1G/eYi5IxGpyFGM3hH1GAoKgVJKnmx0Jky65NM3Ic7qkBNNaLFmWVVJKLVNULRY9vWBbdgZjkTgzUijr1e/3m8O7iqd/dnoRSkLMJKJQiLZjx9IdQy4JoniS7yQCxxANYR+3FWUwxMIdZxdNjBOl9Iwg4uoxaWTq74kxDIUYhb7qDKEciHkvFf/z2okMLtwxYXEiMUPTRUFCDRkZaTxGj3CMJB0pAhEn6q8GGkAOkVQh5lUIzvzJi6RxajlnllNVadhgihw4AsfABA44B2VwDSqgDhC4B4/gGbxoD9qT9qq9TVuXtNnMAfgD7eMH/OWdiQ==</latexit>⌧i=((oi 1,ai 1),...,(oi T,ai T)) o<latexit sha1_base64="NbC+F7/2DAjubON5NEFfCsbeSHo=">AAAB6nicbZC7SgNBFIbPxluMt6iNYDMYhFRh10LtDNhYRjQXSJYwO5lNhsxlmZkVQsgj2FgoYmvhc/gIdj6BTyE4uRSa+MPAx/+fw5xzooQzY33/08ssLa+srmXXcxubW9s7+d29mlGpJrRKFFe6EWFDOZO0apnltJFoikXEaT3qX47z+h3Vhil5awcJDQXuShYzgq2zblTbtvMFv+RPhBYhmEHh4vtdfR28iUo7/9HqKJIKKi3h2Jhm4Cc2HGJtGeF0lGulhiaY9HGXNh1KLKgJh5NRR+jYOR0UK+2etGji/u4YYmHMQESuUmDbM/PZ2Pwva6Y2Pg+HTCappZJMP4pTjqxC471Rh2lKLB84wEQzNysiPawxse46OXeEYH7lRaidlILTUnDtF8pFmCoLh3AERQjgDMpwBRWoAoEu3MMjPHnce/CevZdpacab9ezDH3mvP21VkkM=</latexit>t o<latexit sha1_base64="dXHbUYkUooy040cMhqHOHvodfdw=">AAAB7nicbVA9SwNBEJ2LGmP8ilraHAYhIIQ7C2MZsLGMkC9IQtjb7CVL9naP3TkhHPkHNjYWitj6e+ws/Q3+ATcfhSY+GHi8N8PMvCAW3KDnfTqZjc2t7HZuJ7+7t39wWDg6bhqVaMoaVAml2wExTHDJGshRsHasGYkCwVrB+Gbmt+6ZNlzJOk5i1ovIUPKQU4JWaql+ihf+tF8oemVvDned+EtSrGa/Kt8P9UqtX/joDhRNIiaRCmJMx/di7KVEI6eCTfPdxLCY0DEZso6lkkTM9NL5uVP33CoDN1TalkR3rv6eSElkzCQKbGdEcGRWvZn4n9dJMLzupVzGCTJJF4vCRLio3Nnv7oBrRlFMLCFUc3urS0dEE4o2obwNwV99eZ00L8v+Vdm/s2mUYIEcnMIZlMCHClThFmrQAApjeIRneHFi58l5dd4WrRlnOXMCf+C8/wCPypKm</latexit>t+1 o<latexit sha1_base64="cQwoO6+uWqmUFVFTDHN6sWS6uq8=">AAAB7nicbVDLSgNBEOyNrxhfUY9eFoMQEMJuDsZjwIvHCHlBEsLsZDYZMjuzzPQKYckfePHiQRGvfo83j36DP+DkcdDEgoaiqpvuriAW3KDnfTqZjc2t7Z3sbm5v/+DwKH980jQq0ZQ1qBJKtwNimOCSNZCjYO1YMxIFgrWC8c3Mb90zbbiSdZzErBeRoeQhpwSt1FL9FC/L036+4JW8Odx14i9Jobr9Vfl+qFdq/fxHd6BoEjGJVBBjOr4XYy8lGjkVbJrrJobFhI7JkHUslSRippfOz526F1YZuKHStiS6c/X3REoiYyZRYDsjgiOz6s3E/7xOguF1L+UyTpBJulgUJsJF5c5+dwdcM4piYgmhmttbXToimlC0CeVsCP7qy+ukWS75VyX/zqZRhAWycAbnUAQfKlCFW6hBAyiM4RGe4cWJnSfn1XlbtGac5cwp/IHz/gORT5Kn</latexit>t+2  <latexit sha1_base64="VQLuQu+xOXQszQDxvLrndNtoslM=">AAAB63icbVA9SwNBEJ0zfsT4FRUrm8Mg2BjuLNQyYGMZwXxAEsLeZi5Zsrt37O4J4chfsLFQxNYf4l+wEKz8KbqXpNDEBwOP92aYmRfEnGnjeZ/OUm55ZXUtv17Y2Nza3inu7tV1lCiKNRrxSDUDopEziTXDDMdmrJCIgGMjGF5lfuMOlWaRvDWjGDuC9CULGSUmk05Vl3WLJa/sTeAuEn9GSpXcx/fbwRdWu8X3di+iiUBpKCdat3wvNp2UKMMox3GhnWiMCR2SPrYslUSg7qSTW8fusVV6bhgpW9K4E/X3REqE1iMR2E5BzEDPe5n4n9dKTHjZSZmME4OSTheFCXdN5GaPuz2mkBo+soRQxeytLh0QRaix8RRsCP78y4ukflb2z8v+jU3DgynycAhHcAI+XEAFrqEKNaAwgHt4hCdHOA/Os/MybV1yZjP78AfO6w9gmZIr</latexit> ri <latexit sha1_base64="FLbEXwdWlS8w4Z5tI22QeD/23DY=">AAAB+3icbVBNT8JAEN3iF+JXxaOXRjDxRFoO6pHEiwcPmAiYQEO2yxQ2bLfN7tRAmv4VLx40xqt/xJv/xhZ6UPAlk7y8N5OZeV4kuEbb/jZKG5tb2zvl3cre/sHhkXlc7eowVgw6LBShevSoBsEldJCjgMdIAQ08AT1vepP7vSdQmofyAecRuAEdS+5zRjGThma1PkCYYXIHVEkux2m9MjRrdsNewFonTkFqpEB7aH4NRiGLA5DIBNW679gRuglVyJmAtDKINUSUTekY+hmVNADtJovbU+s8U0aWH6qsJFoL9fdEQgOt54GXdQYUJ3rVy8X/vH6M/rWbcBnFCJItF/mxsDC08iCsEVfAUMwzQpni2a0Wm1BFGWZx5SE4qy+vk26z4Vw2nPtmrWUXcZTJKTkjF8QhV6RFbkmbdAgjM/JMXsmbkRovxrvxsWwtGcXMCfkD4/MHVtqT7A==</latexit>Learning
a<latexit sha1_base64="aJALuZpj0u+V+doAj7lbjt3KaSg=">AAAB6nicbZC7SgNBFIbPxluMt6iNYDMYhFRh10LtDNhYRjQXSJYwO5lNhsxlmZkVQsgj2FgoYmvhc/gIdj6BTyE4uRSa+MPAx/+fw5xzooQzY33/08ssLa+srmXXcxubW9s7+d29mlGpJrRKFFe6EWFDOZO0apnltJFoikXEaT3qX47z+h3Vhil5awcJDQXuShYzgq2zbnDbtvMFv+RPhBYhmEHh4vtdfR28iUo7/9HqKJIKKi3h2Jhm4Cc2HGJtGeF0lGulhiaY9HGXNh1KLKgJh5NRR+jYOR0UK+2etGji/u4YYmHMQESuUmDbM/PZ2Pwva6Y2Pg+HTCappZJMP4pTjqxC471Rh2lKLB84wEQzNysiPawxse46OXeEYH7lRaidlILTUnDtF8pFmCoLh3AERQjgDMpwBRWoAoEu3MMjPHnce/CevZdpacab9ezDH3mvP1gBkjU=</latexit>t z<latexit sha1_base64="Ql4ioYzx60VgZwIfaYZdCZwXd/0=">AAAB6nicbZC7SgNBFIZnvcZ4i9oINoNBSBV2LdTOgI1lRHOBJITZyWwyZC7LzFkhLnkEGwtFbC18Dh/BzifwKQQnl0ITfxj4+P9zmHNOGAtuwfc/vYXFpeWV1cxadn1jc2s7t7NbtToxlFWoFtrUQ2KZ4IpVgINg9dgwIkPBamH/YpTXbpmxXKsbGMSsJUlX8YhTAs66vmtDO5f3i/5YeB6CKeTPv9/11/6bLLdzH82OpolkCqgg1jYCP4ZWSgxwKtgw20wsiwntky5rOFREMttKx6MO8ZFzOjjSxj0FeOz+7kiJtHYgQ1cpCfTsbDYy/8saCURnrZSrOAGm6OSjKBEYNB7tjTvcMApi4IBQw92smPaIIRTcdbLuCMHsyvNQPS4GJ8Xgys+XCmiiDDpAh6iAAnSKSugSlVEFUdRF9+gRPXnCe/CevZdJ6YI37dlDf+S9/gB+F5JO</latexit>t a<latexit sha1_base64="etGMnJ34mXgNe5YVhnmgPOb/lHo=">AAAB7nicbVA9SwNBEJ2LGmP8ilraLAYhIIQ7C2MZsLGMkC9IQtjb7CVL9vaO3TkhHPkHNjYWitj6e+ws/Q3+ATcfhSY+GHi8N8PMPD+WwqDrfjqZjc2t7HZuJ7+7t39wWDg6bpoo0Yw3WCQj3fap4VIo3kCBkrdjzWnoS97yxzczv3XPtRGRquMk5r2QDpUIBKNopRbtp3jhTfuFolt25yDrxFuSYjX7Vfl+qFdq/cJHdxCxJOQKmaTGdDw3xl5KNQom+TTfTQyPKRvTIe9YqmjITS+dnzsl51YZkCDSthSSufp7IqWhMZPQt50hxZFZ9Wbif14nweC6lwoVJ8gVWywKEkkwIrPfyUBozlBOLKFMC3srYSOqKUObUN6G4K2+vE6al2Xvquzd2TRKsEAOTuEMSuBBBapwCzVoAIMxPMIzvDix8+S8Om+L1oyznDmBP3DefwB6PpKY</latexit>t+1 z<latexit sha1_base64="gi9I1te/deo7XtV6HpVNxeqsInc=">AAAB7nicbVA9SwNBEJ2LGmP8ilraLAYhIIQ7C2MZsLGMkC+IIext9pIle3vH7pwQj/wDGxsLRWz9PXaW/gb/gJuPQhMfDDzem2Fmnh9LYdB1P53M2vpGdjO3ld/e2d3bLxwcNk2UaMYbLJKRbvvUcCkUb6BAydux5jT0JW/5o6up37rj2ohI1XEc825IB0oEglG0Uuu+l+KZN+kVim7ZnYGsEm9BitXsV+X7oV6p9Qoft/2IJSFXyCQ1puO5MXZTqlEwySf528TwmLIRHfCOpYqG3HTT2bkTcmqVPgkibUshmam/J1IaGjMOfdsZUhyaZW8q/ud1Egwuu6lQcYJcsfmiIJEEIzL9nfSF5gzl2BLKtLC3EjakmjK0CeVtCN7yy6ukeV72LsrejU2jBHPk4BhOoAQeVKAK11CDBjAYwSM8w4sTO0/Oq/M2b804i5kj+APn/QeguJKx</latexit>t+1 a<latexit sha1_base64="WDvSh8/fK+K/e4DkjXkTXhWIYqQ=">AAAB7nicbVDLSgNBEOyNrxhfUY9eBoMQEMJuDsZjwIvHCHlBEsLsZDYZMju7zPQKYckfePHiQRGvfo83j36DP+DkcdDEgoaiqpvuLj+WwqDrfjqZjc2t7Z3sbm5v/+DwKH980jRRohlvsEhGuu1Tw6VQvIECJW/HmtPQl7zlj29mfuueayMiVcdJzHshHSoRCEbRSi3aT/GyPO3nC27JnYOsE29JCtXtr8r3Q71S6+c/uoOIJSFXyCQ1puO5MfZSqlEwyae5bmJ4TNmYDnnHUkVDbnrp/NwpubDKgASRtqWQzNXfEykNjZmEvu0MKY7MqjcT//M6CQbXvVSoOEGu2GJRkEiCEZn9TgZCc4ZyYgllWthbCRtRTRnahHI2BG/15XXSLJe8q5J3Z9MowgJZOINzKIIHFajCLdSgAQzG8AjP8OLEzpPz6rwtWjPOcuYU/sB5/wF7w5KZ</latexit>t+2 z<latexit sha1_base64="aVavKUclwbmuXUrObtFr8DE4c5U=">AAAB7nicbVA9SwNBEJ2LGmP8ilraHAYhIIS7FMYyYGMZIV+QHGFvs5cs2ds7dueEeOQf2NhYKGLr77Gz9Df4B9x8FJr4YODx3gwz8/xYcI2O82llNja3stu5nfzu3v7BYeHouKWjRFHWpJGIVMcnmgkuWRM5CtaJFSOhL1jbH1/P/PYdU5pHsoGTmHkhGUoecErQSO37fooXlWm/UHTKzhz2OnGXpFjLflW/HxrVer/w0RtENAmZRCqI1l3XidFLiUJOBZvme4lmMaFjMmRdQyUJmfbS+blT+9woAzuIlCmJ9lz9PZGSUOtJ6JvOkOBIr3oz8T+vm2Bw5aVcxgkySReLgkTYGNmz3+0BV4yimBhCqOLmVpuOiCIUTUJ5E4K7+vI6aVXK7mXZvTVplGCBHJzCGZTAhSrU4Abq0AQKY3iEZ3ixYuvJerXeFq0ZazlzAn9gvf8Aoj2Ssg==</latexit>t+2 E<latexit sha1_base64="O4F1YjZkmgg7cwC04/Hc4X+yqmg=">AAAB73icbVDJSgNBEK1xjXEb9aaXxiDES5jxoB4DInjIIQGzQDKEnk4nadKz2F0jhCFf4M2LB0W8+h3+gTf/wk+wsxw08UHB470qqur5sRQaHefLWlpeWV1bz2xkN7e2d3btvf2ajhLFeJVFMlINn2ouRcirKFDyRqw4DXzJ6/7gauzX77nSIgpvcRhzL6C9UHQFo2ikxnUp30KanLbtnFNwJiCLxJ2RXNGufH+UDh/Kbfuz1YlYEvAQmaRaN10nRi+lCgWTfJRtJZrHlA1ojzcNDWnAtZdO7h2RE6N0SDdSpkIkE/X3REoDrYeBbzoDin09743F/7xmgt1LLxVhnCAP2XRRN5EEIzJ+nnSE4gzl0BDKlDC3EtanijI0EWVNCO78y4ukdlZwzwtuxaThwBQZOIJjyIMLF1CEGyhDFRhIeIRneLHurCfr1Xqbti5Zs5kD+APr/Qdyg5KH</latexit> L(⌧)
<latexit sha1_base64="wWMAcLxoPZdtBtVdgG9NM9vAFu4=">AAAB+nicbVC7TsNAEDyHVwgvB0oaiwSJKrJTAGUkGgqKIJGHlFjR+bxOTjmfrbs1EJl8Cg0FCNHyJXT8DU7iAhJGWmk0s6vdHS8WXKNtfxuFtfWNza3idmlnd2//wCwftnWUKAYtFolIdT2qQXAJLeQooBsroKEnoOONr2Z+5x6U5pG8w0kMbkiHkgecUcykgVmu9hEeMb0BqiT402ppYFbsmj2HtUqcnFRIjubA/Or7EUtCkMgE1brn2DG6KVXImYBpqZ9oiCkb0yH0MippCNpN56dPrdNM8a0gUllJtObq74mUhlpPQi/rDCmO9LI3E//zegkGl27KZZwgSLZYFCTCwsia5WD5XAFDMckIZYpnt1psRBVlmKU1C8FZfnmVtOs157zm3NYrDTuPo0iOyQk5Iw65IA1yTZqkRRh5IM/klbwZT8aL8W58LFoLRj5zRP7A+PwBeWiTbQ==</latexit>Learned
h<latexit sha1_base64="nzLXogQ6P+Md0zDvgO3FaI3z9eI=">AAAB7nicbVA9SwNBEJ2LGmP8ilraLAYhjeHOwlgGbCwj5AuSEPY2e8mSvb1jd04IR/6BjY2FIrb+HjtLf4N/wM1HoYkPBh7vzTAzz4+lMOi6n05mY3Mru53bye/u7R8cFo6OmyZKNOMNFslIt31quBSKN1Cg5O1Ycxr6krf88c3Mb91zbUSk6jiJeS+kQyUCwShaqTXqp3jhTfuFolt25yDrxFuSYjX7Vfl+qFdq/cJHdxCxJOQKmaTGdDw3xl5KNQom+TTfTQyPKRvTIe9YqmjITS+dnzsl51YZkCDSthSSufp7IqWhMZPQt50hxZFZ9Wbif14nweC6lwoVJ8gVWywKEkkwIrPfyUBozlBOLKFMC3srYSOqKUObUN6G4K2+vE6al2Xvquzd2TRKsEAOTuEMSuBBBapwCzVoAIMxPMIzvDix8+S8Om+L1oyznDmBP3DefwCIEJKh</latexit>t 1 h<latexit sha1_base64="2mbE0Wc+R+RTkDPRg1zRwow4hgM=">AAAB6nicbZC7SgNBFIbPxluMt6iNYDMYhFRh10LtDNhYRjQXSJYwO5kkQ+ayzMwKYckj2FgoYmvhc/gIdj6BTyE4uRSa+MPAx/+fw5xzopgzY33/08ssLa+srmXXcxubW9s7+d29mlGJJrRKFFe6EWFDOZO0apnltBFrikXEaT0aXI7z+h3Vhil5a4cxDQXuSdZlBFtn3fTbtp0v+CV/IrQIwQwKF9/v6uvgTVTa+Y9WR5FEUGkJx8Y0Az+2YYq1ZYTTUa6VGBpjMsA92nQosaAmTCejjtCxczqoq7R70qKJ+7sjxcKYoYhcpcC2b+azsflf1kxs9zxMmYwTSyWZftRNOLIKjfdGHaYpsXzoABPN3KyI9LHGxLrr5NwRgvmVF6F2UgpOS8G1XygXYaosHMIRFCGAMyjDFVSgCgR6cA+P8ORx78F79l6mpRlv1rMPf+S9/gBiq5I8</latexit>t h<latexit sha1_base64="NVwkLHVWYZJy/rVTxXgD9iCOoLQ=">AAAB7nicbVA9SwNBEJ2LGmP8ilraLAYhIIQ7C2MZsLGMkC9IQtjb7CVL9vaO3TkhHPkHNjYWitj6e+ws/Q3+ATcfhSY+GHi8N8PMPD+WwqDrfjqZjc2t7HZuJ7+7t39wWDg6bpoo0Yw3WCQj3fap4VIo3kCBkrdjzWnoS97yxzczv3XPtRGRquMk5r2QDpUIBKNopdaon+KFN+0Xim7ZnYOsE29JitXsV+X7oV6p9Qsf3UHEkpArZJIa0/HcGHsp1SiY5NN8NzE8pmxMh7xjqaIhN710fu6UnFtlQIJI21JI5urviZSGxkxC33aGFEdm1ZuJ/3mdBIPrXipUnCBXbLEoSCTBiMx+JwOhOUM5sYQyLeythI2opgxtQnkbgrf68jppXpa9q7J3Z9MowQI5OIUzKIEHFajCLdSgAQzG8AjP8OLEzpPz6rwtWjPOcuYE/sB5/wGFBJKf</latexit>t+1 l<latexit sha1_base64="YWUmWjm1sFxeQZDO14KQIFoxXks=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPBi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9xm+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST27nfeeLaiFg94jThfkRHSoSCUbTSgxyIQbXm1t0cZJV4BalBgeag+tUfxiyNuEImqTE9z03Qz6hGwSSfVfqp4QllEzriPUsVjbjxs/zUGTmzypCEsbalkOTq74mMRsZMo8B2RhTHZtmbi/95vRTDGz8TKkmRK7ZYFKaSYEzmf5Oh0JyhnFpCmRb2VsLGVFOGNp2KDcFbfnmVtC/q3lXdu7+sNdwijjKcwCmcgwfX0IA7aEILGIzgGV7hzZHOi/PufCxaS04xcwx/4Hz+AErWjcA=</latexit>i <latexit sha1_base64="9RcF4K3OcDYbD+qt9rlWYy+RQgI=">AAAB+XicbVBNT8JAEN3iF+JX1aOXjWDiibQc1COJF4+YCJhAQ7bLFjZsd5vdKbFp+CdePGiMV/+JN/+NW+Cg4EsmeXlvJjPzwkRwA5737ZQ2Nre2d8q7lb39g8Mj9/ikY1SqKWtTJZR+DIlhgkvWBg6CPSaakTgUrBtObgu/O2XacCUfIEtYEJOR5BGnBKw0cN1aH9gT5C0lOM1mtcrArXp1bw68TvwlqaIlWgP3qz9UNI2ZBCqIMT3fSyDIiQZOBZtV+qlhCaETMmI9SyWJmQny+eUzfGGVIY6UtiUBz9XfEzmJjcni0HbGBMZm1SvE/7xeCtFNkHOZpMAkXSyKUoFB4SIGPOSaURCZJYRqbm/FdEw0oWDDKkLwV19eJ51G3b+q+/eNatNbxlFGZ+gcXSIfXaMmukMt1EYUTdEzekVvTu68OO/Ox6K15CxnTtEfOJ8/3NSTGA==</latexit>Policy
0<latexit sha1_base64="VRXanKDFW1lFuD5AFhvegMwGA1k=">AAAB6nicbZC7SgNBFIbPeo3xFi+dzWAQrJZZwUtnwELLiOYCyRJmJ7PJkNnZZWZWiEsewcZCEVtrK5/EztI3cXIpNPGHgY//P4c55wSJ4Npg/OXMzS8sLi3nVvKra+sbm4Wt7aqOU0VZhcYiVvWAaCa4ZBXDjWD1RDESBYLVgt7FMK/dMaV5LG9NP2F+RDqSh5wSY60b7B63CkXs4pHQLHgTKJ5/3H9fvu9m5Vbhs9mOaRoxaaggWjc8nBg/I8pwKtgg30w1SwjtkQ5rWJQkYtrPRqMO0IF12iiMlX3SoJH7uyMjkdb9KLCVETFdPZ0Nzf+yRmrCMz/jMkkNk3T8UZgKZGI03Bu1uWLUiL4FQhW3syLaJYpQY6+Tt0fwpleeheqR65243jUuljCMlYM92IdD8OAUSnAFZagAhQ48wBM8O8J5dF6c13HpnDPp2YE/ct5+AHn7kO0=</latexit> .5
R<latexit sha1_base64="tde9Utyn6xOfLO6I5FB37PU04Kw=">AAAB/nicbVBNS8NAFNzUr1q/ouLJS7AInkoioh4LXjxWsbXQhrLZvrRLN5uw+yKWUPCvePGgiFd/hzf/jds0B20dWBhm3uy+nSARXKPrflulpeWV1bXyemVjc2t7x97da+k4VQyaLBaxagdUg+ASmshRQDtRQKNAwH0wupr69w+gNI/lHY4T8CM6kDzkjKKRevZBF+ERs1swKQ0Sc3nSs6tuzc3hLBKvIFVSoNGzv7r9mKWRuYEJqnXHcxP0M6qQMwGTSjfVkFA2ogPoGCppBNrP8vUnzrFR+k4YK3MkOrn6O5HRSOtxFJjJiOJQz3tT8T+vk2J46WdcJimCZLOHwlQ4GDvTLpw+V8BQjA2hTHGzq8OGVFGGprGKKcGb//IiaZ3WvPOad3NWrbtFHWVySI7ICfHIBamTa9IgTcJIRp7JK3mznqwX6936mI2WrCKzT/7A+vwBZ4KWXQ==</latexit> epresentation
l<latexit sha1_base64="YWUmWjm1sFxeQZDO14KQIFoxXks=">AAAB6nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE1GPBi8eK9gPaUDbbTbt0swm7E6GE/gQvHhTx6i/y5r9xm+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST27nfeeLaiFg94jThfkRHSoSCUbTSgxyIQbXm1t0cZJV4BalBgeag+tUfxiyNuEImqTE9z03Qz6hGwSSfVfqp4QllEzriPUsVjbjxs/zUGTmzypCEsbalkOTq74mMRsZMo8B2RhTHZtmbi/95vRTDGz8TKkmRK7ZYFKaSYEzmf5Oh0JyhnFpCmRb2VsLGVFOGNp2KDcFbfnmVtC/q3lXdu7+sNdwijjKcwCmcgwfX0IA7aEILGIzgGV7hzZHOi/PufCxaS04xcwx/4Hz+AErWjcA=</latexit>i (<latexit sha1_base64="jUGMVMlujuCb0QKCaaRbObIa7ZQ=">AAACAHicbZDLSgMxFIYz9VbrbdSFCzfBIlQoIVOrtbuCG5cV7AXaoWTSTBuauZBkhFK68VXcuFDErY/hzrcxbWehrT8EPv5zDifn92LBlcb428qsrW9sbmW3czu7e/sH9uFRU0WJpKxBIxHJtkcUEzxkDc21YO1YMhJ4grW80e2s3npkUvEofNDjmLkBGYTc55RoY/XskwJG5atSEaPLarmIEDJUwdWLnp3HCM8FV8FJIQ9S1Xv2V7cf0SRgoaaCKNVxcKzdCZGaU8GmuW6iWEzoiAxYx2BIAqbcyfyAKTw3Th/6kTQv1HDu/p6YkECpceCZzoDooVquzcz/ap1E+zfuhIdxollIF4v8REAdwVkasM8lo1qMDRAqufkrpEMiCdUms5wJwVk+eRWaJeRcI+e+nK/hNI4sOAVnoAAcUAE1cAfqoAEomIJn8ArerCfrxXq3PhatGSudOQZ/ZH3+AJJJkdk=</latexit>0.452,0.394,...,0.709) E<latexit sha1_base64="Vyt0F3W0fUxJIxzHTAIzoWxEbe0=">AAACNnicbVDLSgNBEJz1GeMr6tHLYCJE0LDrQT0KIngRIhgVskuYne1NBmcfzvQKYdmv8uJ3eMvFgyJe/QQnj4MaCxpqqrrp6fJTKTTa9sCamZ2bX1gsLZWXV1bX1isbmzc6yRSHFk9kou58pkGKGFooUMJdqoBFvoRb//5s6N8+gtIiia+xn4IXsW4sQsEZGqlTuazV3Ihhz/fz86KTu6koXAkhtg9U3Tz2TdVdZNnenhuJgE6JEh6o7SrR7aFXq3UqVbthj0CniTMhVTJBs1N5cYOEZxHEyCXTuu3YKXo5Uyi4hKLsZhpSxu9ZF9qGxiwC7eWjswu6a5SAhokyFSMdqT8nchZp3Y980zk8Uf/1huJ/XjvD8MTLRZxmCDEfLwozSTGhwwxpIBRwlH1DGFfC/JXyHlOMo0m6bEJw/p48TW4OG85Rw7k6rJ7akzhKZJvskDpxyDE5JRekSVqEkycyIG/k3Xq2Xq0P63PcOmNNZrbIL1hf324tqmw=</latexit> ⇡[  r(⇡,⇡(⌧)) |r(⇡,⇡(⌧)) 0]
Figure5. TheoveralldiagramofExploitedLevelAugmentation(ELA)forofflinelearning.
lsincez hastheinformationof(h ,o ,l)andh gathers Regularization loss. The prediction of the prior estima-
t t 1 t t
theinformationof(h ,o ,a ,z ,− l). Theinformationof tor is expected to closely align with the result of the tra-
t 1 t t t
a isalsocontained−ino , thuswedefinethetrajectory jectory encoder. Since the output of both ϕ and ϕ are
i 1 i p e
en−coderas normal distributions, we follow the design of VAE that
ϕ (h ,o ,a ,l)=[µz,σz], usesKLdivergence. Theregularizationlossisthusdefined
whereϕ
xz t is| ah tt r− ae i1 n, aot bt− l, e1 a mt, at l p∼ pt iN ng( aµ nz t d,d suit a bg s( ct ( rσ iptz t) x2) c) a,
nbeany
wa 21s h (cid:104)L i (cid:80)cK h
k
iL =,t 1ca (cid:0)= n (loK gbL e
σˆ
t( )N e ix −( pµ l lz t i oc, gid t (li σyag tz( )( icσ a +ltz c) (u2 σˆl) a t) )te −i∥ d 1wN b i(cid:1)( yµˆ −t, L kd K (cid:105)ia ,Lg ,t w(σˆ ht2 e) r=) e,
character. We follow the convention in VAE and assume w = ((µˆ ) (µz) )2 + (σz) and k is the dimension
thatthelatentvariablehasadiagonalcovariancematrix. ofi thelatent ti sp− ace.t i t i
Prior estimator. Without knowing the actual action a t, Finally,thetotallossiswrittenas
the prior of latent variable z can be derived only from
t
therepresentationlandthepastobservations o t . We
{ i }i=1 (cid:88)T
extractinformationfromthepastandobtainanestimateof = ( + ).
recon,t KL,t
thepriorofz . Wedefinethepriorestimatoras L L L
t t=1
ϕ (h ,o ,l)=[µˆ ,σˆ ],
p t 1 t t t
− Whenlearningthestrategyrepresentation,thetrainableran-
z t |h t −1,o t,l ∼N(µˆ t,diag(σˆ t2)). domrepresentationvectorli,j isinitiatedforeachtrajectory
τi,j. TheconditionpartofP-VRNNconsistsofobservation
Actiondecoder.Theactiondecoderworksinverselyandob-
o whichchangesovertimeandtherepresentationvectorl
tainsactiona fromthelatentvariablez ,pastobservations t
t t whichisconsistentduringthewholetrajectoryandtrainable.
o t andrepresentationl,whichcanalsobesubstituted
{ i }i=1 Duringtraining,allthel’sareoptimizedtogetherwiththe
by using h , z , o and l. We obtain the prediction of
actionsbytt h−e1 actt iont
decoder,whichisdefinedformallyas
parametersofϕ p,ϕ e,ϕ d,andϕ r. Thenetworksaretrained
toperformbetterinpredictingthenextstepoftrajectories,
ϕ d(h t 1,z t,o t,l)=[µx t,σ tx], whiletherepresentationsareoptimizeddifferentlyforeach
−
a h ,z ,o ,l (µx,diag((σx)2)). trajectorytoprovidecustomizedpredictions. Consequently,
t | t −1 t t ∼N t t thelshouldbeadjustedbasedonthetendencytoexpress
Recurrentunit. Therecurrentunittakesinallthevariables trajectorystrategiesmoreeffectively. Theentireprocessof
ofthecurrentstepandtheoutputoftherecurrentunitofthe obtaining l is not only unsupervised but also without the
laststep,whichextractsallthepastinformationandpasses informationofplayers’identification.
itontothenextstep. Ateachtimestep,therecurrentunitis
updatedby 6.2.ExploitedLevelEstimator
Intwo-playersymmetriczero-sumgames,itiscommonto
ϕ(h ,a ,z ,o ,l)=h .
r t 1 t t t t
− useexploitabilityasameasureforevaluatingtheeffective-
WedesignthelossfunctionofP-VRNNasfollows: nessofastrategy.However,itisextremelydifficulttoobtain
Reconstruction loss. The encoder-decoder model aims exploitabilitywithasingletrajectorysincewecannot: 1)
tocloselymatchthetruedata. Inourdatasetwhereeach inferormodifythestrategyoftheopponent;or2)makeany
moment has only one one-hot encoded action, the recon- interactionwiththeenvironment. Forastrategyπ ,ifwe
i
structionlossisdefinedas = CE(µz,a ),andthe havemanytrajectoriesthathaveastrategysimilartoitand
Lrecon,t t t
varianceσz isomitted,wherethecrossentropyisexpressed theopponentsusealargevarietyofstrategies(sothatthere
t (cid:82)
asCE(p,q)= p(x)logq(x)dx. isonestrategynearthebestresponse),then ϵ > 0,there
− ∀
5ELA:ExploitedLevelAugmentationforOfflineLearninginZero-SumGames
existsaδ >0whichsatisfiesthefollowingapproximation: reward less than 0 is proportional to exploitability. The
(cid:12) (cid:12) right-handsidevalueisareasonablemeasureofatrajectory,
(cid:12) (cid:12)
(cid:12) (cid:12)E(π i) −d(πm i′,πa ix )<δ[ −r(πˆ −i,π i′)](cid:12) (cid:12)<ϵ, w reh pi rc eh sei ns ts ah tio ow nn spin acth e,e wto ey pm roo vd ie dl e. aT no ae ls tt ei rm na at te ivE eL dew fii nth itil oa nte on ft
wheredisadistanceoverthestrategyspace.
EL :
However, if we have many trajectories so that for each δ
t sr ta raje tec gto iery s, ,t ah ne do tp hp eo tn rae jn et cs tt or ra it ee sgi wes ithca sn imco ilv ae rr rm epo rs et sek nin tad ts ioo nf
EL δ(τ)=
(cid:80) (cid:80)d(f(τ),f(τ′))<δ(
−
1r(πˆ,π(τ ′)))+
.
d(f(τ),f(τ′))<δ r(πˆ,π(τ′)) 0
vectorshavesimilarstrategydistributions,canwestilluse ≤
Itisobviousthatlim EL (τ)=EL(τ). Theproperty
theminimumrewardoftrajectorieswithrepresentationnear δ 0+ δ
of EL satisfies our r→equirement that the trajectories that
itselftoserveasanapproximationofnegativeexploitability?
performsimilarlytoNashEquilibriumcanbedetectedwith
First,wedefinemeasuredπonstrategyspaceΠaccording
anELnear0sincewehavethefollowingproposition.
totheprobabilityofπchoseninthewholedataset:
(cid:90) Proposition6.2. Givenatrajectoryτ anditscorresponding
π
Sdπ =P[τ ∼π,π ∈S, ∀τ ∈Γ],
distributionτ(π)overΠ,π(τ)isϵ 1-Nashequilibrium,and
∈ weassumethatanypurestrategycanexploitanotherstrat-
whereSisanarbitrarysubsetofΠ. Denotethetrajectoryas
τ,therepresentationfunctionlearnedaboveasf(τ),andthe
egybyatmostM. Bythesmoothnessoff,wealsoassume
(cid:82)
rewardofτ asr(τ).Weremarkthatatrajectoryτ shouldbe thatifd(f(τ 1),f(τ 2))<δ,then π Π|τ 1(π) −τ 2(π) |dπ <
mappedtoaprobabilitydistributionofstrategiessuchthat
αδ,whereαisaconstant. Wehave∈thefollowingresult:
(cid:82)
τ(π)dπ = 1,whereτ(π)istheprobabilityofusing EL (τ)<ϵ +αδM.
π Π δ 1
str∈ategy π when having trajectory τ, instead of a single
strategy. Butwecanviewthemixtureofπwithprobability SinceEListheaverageofvaluessatisfyingconditionswith
τ(π)asasinglemixedstrategy(cid:82) πτ(π)dπ,sowecan distance constraints on the representation space, we can
π Π
stillusenotationπ(τ)torepresent∈thestrategyofτ. Using trainanoperatorLtoestimateELfromrepresentation. We
theabovemethod,wecanapproximateE(π(τ)),i.e., have representation l and reward r for each trajectory τ,
(cid:12) (cid:12) and we intend to minimize (cid:80) L(li) ri so that
(cid:12) (cid:12) ri 0|| − ||2
(cid:12)E(π(τ)) max [ r(τ ′)](cid:12)<ϵ. the prediction from L(l) becom≥es close to the mean of
(cid:12) −d(f(τ′),f(τ))<δ − (cid:12)
satisfyingrewardr 0nearby. Weuseatwo-layerMLP
But the E(π(τ)) we are approximating is not what we ≥
as L. After training L, we can directly obtain EL of a
desire. In order to measure the exploitability of τ, we
singletrajectoryτ evenwithouttherewardinformation. By
(cid:82)
should calculate E(τ) := τ(π)E(π)dπ instead of
π Π applyingrepresentationestimatorf andELestimatorLto
E(cid:0)(cid:82) πτ(π)dπ(cid:1) . Infact,we∈havethefollowingresult: thetrajectoryτ,wecangetthedesiredresultL(f(τ)).
π Π
∈
Proposition6.1. Ifτ(π)isadistributionoverΠ,andE is
6.3.ELAugmentationforOfflineLearning
definedasexploitability,thenwehave
(cid:90) (cid:18)(cid:90) (cid:19) As described in Section 6.2, an EL value approaching 0
τ(π)E(π)dπ E πτ(π)dπ . indicates the trajectory is approaching the Nash equilib-
≥
π ∈Π π ∈Π riumbehavior. Consequently,weformulateExploitedLevel
TheproofisprovidedinAppendixA.Giventheproposi-
Augmentation(ELA)fortheofflinelearningobjectiveas
tionabove,therewillbeanunderestimationifweusethis
follows,emphasizingtrajectorieswithasmallEL:
method. Also,usingmaximumaloneabandonsalmostall
theinformationofnearbytrajectories,whichmakestheap-
ELA(π)=Eτ(cid:2)1(EL(τ)<EL
thresh)
OL(π,τ)(cid:3)
,
L ·L
proximationunstable. Toresolvetheseproblems,weuse
where EL is a threshold that specifies the minimum
thresh
meaninsteadofmaximum. Here,werestatethedefinition
value of an EL suitable for training. It provides data by
oftheexploitedlevel(EL)as samplingonlyfortrajectoriessmallerthanthisvalue. OL
L
EL(τ)=Eπ[ r(π,π(τ)) r(π,π(τ)) 0]. representsanarbitrarymethodthatallowsofflinelearningby
− | ≤
Exceptfortheconditionsmentionedabove,thealgorithmis leveragingatrajectorysuchasimitationlearningoroffline
mainlybasedonthefollowingassumption: RLmethods. Forexample,whenincorporatingELAwith
(cid:82) ( r(π,π(τ))+dπ behaviorcloning,theobjectivefunctionisformulatedas:
E(τ) ∝EL(τ)= (cid:82)π ∈Π −
1 dπ
, 
τ

wherer(π,π
′)returnstherewπ ∈ arΠ dor f( aπ, pπ l( aτ y)) e≤ r0
withstrategy
LELA(π)=Eτ1(EL(τ)<EL thresh) ·(cid:88)| | logπ(a
t
|o t).
π bydefault,(x)+ = max x,0 and1 = 1ifandonly t=0
′ c
if condition c is satisfied, o{ therw} ise 1 = 0. The above Note that when employing a maximum value of EL in
c
functionmeansthatgivenatrajectoryτ,themeannegative thedatasetasathreshold,itreducestotheoriginaloffline
rewardofthetrajectorieswitharepresentationnearτ and learningalgorithm.
6ELA:ExploitedLevelAugmentationforOfflineLearninginZero-SumGames
(a)Playerstrategy (b)Exploitedlevel (c)Trajectoryreward (d)3Dvisualization
(e)Exploitedlevel (f)Trajectoryreward (g)Exploitedlevel (h)Trajectoryreward
Figure6. Thetrajectoryrepresentationsofthe(a-d)RPS,(e-f)Two-playerPong,and(g-h)LimitTexasHold’em.
7.Experiments Hold’em, since a player can win with different margins
dependingonacertaingame, wedeterminedtheaverage
7.1.ExperimentSettings
scoreasthedifferencebetweenthetotalchipswonandlost
We use two-player zero-sum games to validate the effec-
dividedbythetotalnumberofgamesplayed.
tivenessofourapproach: Rock-Paper-Scissors(RPS),Two-
playerPong,andLimitTexasHold’em(Zhaetal.,2020), 7.2.RepresentationandELEstimation
whichareintroducedinAppendixC.1. Theimplementation
Thestrategyrepresentationoftrajectoriesisfirstlyreduced
detailsforourmethodareprovidedinAppendixC.2.
totwodimensionsusingt-SNE(PCAisusedinTwo-player
Datasetgeneration.Weemploydifferentmethodstocreate
Pongtopreservescaleforourfurtheranalysis),followedby
training datasets with diverse demonstrators for the envi-
coloringbasedondifferentlabels.
ronments. For RPS, we choose the strategy to generate
RPS. We demonstrate the results in the RPS in Figure 6.
trajectoriesforRPSasarandomstrategywithapreference
In RPS, a more biased strategy deviates further from the
for action a with bias p, where π(a) = (1 p)/3 and
′ − Nashequilibrium,leadingtoworseperformance. InFigure
π(a) = (1+2p)/3 a = a. ForPong, weuseself-play
∀ ′ ̸ 6a, wecolortherepresentationsbytheplayerstrategyof
withopponentsampling(Bansaletal.,2018)withtheProx-
eachtrajectory: thetrajectorieswithbias0.5,0.2,and0are
imalPolicyOptimization(PPO)algorithm(Schulmanetal.,
coloredwithchartreuse,cyan,andpurple,respectively. We
2017). ForLimitTexasHold’em,weuseneuralfictitious
useconsistentcolorsinotherimagesinFigure6toindicate
self-play(Heinrich&Silver,2016)withDeepQ-network
thelabelvalueofeachplot. Figure6bshowstheestimated
(DQN)algorithm(Mnihetal.,2013)togenerateexpertpoli-
ELderivedfromP-VRNNandtheELestimator. Figure6c
cies,givenitscomplexityandtheneedtoadapttovarious
islabeledbytheterminalrewardofeachtrajectory. From
opponents.Behaviormodelsarethenselectedfrommultiple
thethreefigures,itisevidentthatourELestimatorshows
intermediatecheckpointstogeneratetheofflinedata.
similarpatternwithFigure6a, consideredagroundtruth.
Evaluation metrics. We evaluated our method across
Furthermore,adecreaseintheestimatedELisobservedas
threeenvironmentstoestimatetherepresentationandEL
thetrajectoryapproachestheNashpolicy.
oftrajectories. Specifically,weconductedtestsinaRock-
Consideringthestructureofthestrategyspace,wecanan-
Paper-Scissors(RPS)environmenttoprovideinsightinto
alyze an (N + 1)-dimensional space with N dimension
the higher-level geometrical representation of strategies,
oftrajectoryrepresentationandonedimensionofEL.To
aligningwiththehypothesisin(Czarneckietal.,2020). In
simplifytheproblemandmakeitperceptible,weconsider
evaluating ELA for offline learning algorithms, we com-
reducing the dimension to 2+1, as shown in Figure 6d.
paredaveragescoresover500gamesbetweentwoplayers.
Despitenotmatchingprecisely,wecanseeaspinningtop
FortheTwo-playerPong,theaveragescorewascalculated
structureaspredicted. Therearemorestrategieswithhigher
usingtheformula(N N )/N . ForLimitTexas
win − lose game ELthatarenon-transitive.
7ELA:ExploitedLevelAugmentationforOfflineLearninginZero-SumGames
     
 % &                                                         
 % D V H
       : 7  % &  : 7                                                     
     ( / $  % &  ( / $                                                        
 
     % & 4                                                          
       % & 4  : 7                                                              
 % & 4  ( / $                                                         
     
 % &  % & 4  & 4 /  % &  % & 4  & 4 /  & 4 /                                                  
   
 7 Z R  S O D \ H U  3 R Q J  / L P L W  7 H [ D V  + R O G H P
 & 4 /  : 7                                                       
Figure7.Exploitabilitysupportedonthedemonstratorsetgenerat-  & 4 /  ( / $                                                  
ingtheofflinedataset.Lowerisbetter.  % &  % &  % &  % & 4  % & 4  % & 4  & 4 /  & 4 /  & 4 /  $ Y J    
  : 7   ( / $   : 7   ( / $   : 7   ( / $
Two-playerPongandLimitTexasHold’em.Wealsoillus- (a)Two-playerPong
tratetheresultsoftheTwo-playerPongandtheLimitTexas    
 % &                                                      
Hold’em. IntheTwo-playerPong,wechooseeightplayers
 % &  : 7                                                                  
withstrategiestrainedbyPPOwithdifferentcheckpoints.
 % &  ( / $                                                        
AsshowninFigure6eand6f,thestrategyrepresentationis  % & 4                                                            
naturallyseparatedintoeightclusters,revealingbothELand  % & 4  : 7                                                           
rewarddistributions. Figure6edemonstratesthatELbetter  % & 4  ( / $                                                      
   
reflectsthestrengthofeachplayer,asthevalueswithineach  & 4 /                                                       
clusteraremoreconsistent. Additionally,itisobservedthat  & 4 /  : 7                                                        
themostexpansiveclusterwiththelowestdensityhasthe  & 4 /  ( / $                                                      
   
highestEL,suggestingthattheleasttrainedstrategyexhibits  % &  % &  % &  % & 4  % & 4  % & 4  & 4 /  & 4 /  & 4 /  $ Y J
  : 7   ( / $   : 7   ( / $   : 7   ( / $
unstablebehavior. IntheLimitTexasHold’em,thereare (b)LimitTexasHold’em
threeplayers—twoexpertswithslightlydifferentstrategies Figure8.Crossevaluationofofflinelearningalgorithmsinzero-
and one relatively novice player. As shown in Figure 6g sumgames.Higherisbetter.
and 6h, if we use EL as a filter, most of the trajectories
playedbyexpertsareretained. However,inFigure6h,there RLalgorithmsshowbetterperformancethantheimitation
arealotofgreenishpointsintheclusterontheleft,which learningapproachonaveragebecauseoftheofflinedatasets
hasalowreward. Therefore,ifwefiltertherewardwitha frommixeddemonstrators. Notably,ELAconsistentlyout-
neutralvaluenear0, alotofexperttrajectorieswouldbe performsalternativemethods. WhileWTenhancestheper-
erroneously excluded. Figure 6h indicates that there is a formance of the original offline algorithm in some cases,
possibilityofexpertplayersobtaininglowrewards,which it occasionally hinders performance due to Q-value over-
disruptsfilteringwithreward,whileFigure6gdemonstrates estimationstemmingfromdatabiasbyonlyselectingthe
thattheELestimatorcanovercomethischallengebychang- winning trajectory. Furthermore, to illustrate the relative
ingtherewardfiltertoELfilter. performanceamongthetrainedmodels,Figure8displays
the outcomes of cross-evaluating various algorithm com-
7.3.ELAugmentedOfflineLearning binationsinthetwoenvironments. Thevalueofeachcell
InourevaluationoftheEL-augmentedofflinelearningap- signifies the score of the model along the horizontal axis
proach,weconsideredtwomaincategoriesofmethodolo- compared to the model along the vertical axis. The last
gies. For IL, we employed BC, while in the domain of column in each subfigure highlights that ELA enhances
offlineRL,weadoptedrepresentativealgorithmsBCQ(Fu- theperformanceofallofflinelearningalgorithmsinboth
jimotoetal.,2019)andCQL(Kumaretal.,2020). Inour environments.
evaluation,weexcludedmethodsthatrelyononlineinterac-
8.Conclusions
tions(e.g.,GAIL)ornecessitateinteractionswithexperts
(e.g.,DAgger)inofflinelearningapproaches. Inthiswork,weproposedaneffectiveframework,ELA,to
We applied ELA to each algorithm to evaluate its perfor- enhanceofflinelearningmethodsinzero-sumgames. We
manceenhancement. WescaledthelearnedestimatedEL designedaP-VRNNnetwork,whichshowsextraordinary
from 0 to 1 by using the maximum and minimum EL in resultsinidentifyingthestrategydistributionofthetrajec-
the dataset. A hyperparameter search was conducted to tories. Wedefinedtheexploitedlevelforthetrajectoryto
identifytheappropriateEL foreachmodelandenvi- measureproximitytoNashequilibriumandsubsequently
thresh
ronment. As an additional baseline for ELA, we trained proposedELAasauniversalmethodforimprovingtheper-
theofflinelearningalgorithmbyexclusivelyselectingthe formanceofofflinelearningalgorithms. Webuiltasolid
winning trajectory (WT) from the dataset. In Figure 7, a theoreticalfoundationforELA,andtheexperimentsonmul-
comparisonofexploitabilityispresented,supportedonthe tipleenvironmentsandalgorithmsshowedpositiveresults
demonstrator set outlined in Section 3. Basically, offline inaddingELA.Weexploredabroadvarietyofalgorithms
8
 \ W L O L E D W L R O S [ (ELA:ExploitedLevelAugmentationforOfflineLearninginZero-SumGames
withdifferenthyperparameters. Infuturework,weaimto Brown, D. S., Goo, W., and Niekum, S. Better-than-
explore zero-sum games with a larger number of players. demonstratorimitationlearningviaautomatically-ranked
Weconsiderutilizingthestrategyrepresentationindiffer- demonstrations. In Conference on robot learning, pp.
ent ways over a larger variety of environments since the 330–359.PMLR,2020.
P-VRNNdoesnotrequirethegametobezero-sum.
Chen, J., Yuan, B., and Tomizuka, M. Model-free deep
SocialImpacts reinforcementlearningforurbanautonomousdriving. In
2019IEEEintelligenttransportationsystemsconference
Ourpaperintroducesanovelapproachtoenhancetheex-
(ITSC),pp.2765–2771.IEEE,2019.
istingofflinelearningalgorithmsinzero-sumgames. The
improvedefficiencyinidentifyingdominantstrategiesmay Chen,L.,Paleja,R.,andGombolay,M. Learningfromsub-
inadvertentlyamplifystrategicadvantagesincompetitive optimaldemonstrationviaself-supervisedrewardregres-
domains,posingriskstofairness. Ethicalconsiderationsare sion. InConferenceonrobotlearning,pp.1262–1277.
necessary to responsibly deploy the method and mitigate PMLR,2021.
potentialnegativeresultsinreal-worldapplications.
Chung,J.,Gulcehre,C.,Cho,K.,andBengio,Y. Empirical
evaluationofgatedrecurrentneuralnetworksonsequence
References
modeling. arXivpreprintarXiv:1412.3555,2014.
Andrychowicz,O.M.,Baker,B.,Chociej,M.,Jozefowicz,
R.,McGrew,B.,Pachocki,J.,Petron,A.,Plappert,M., Chung,J.,Kastner,K.,Dinh,L.,Goel,K.,Courville,A.C.,
Powell, G., Ray, A., etal. Learningdexterousin-hand and Bengio, Y. A recurrent latent variable model for
manipulation. The International Journal of Robotics sequential data. In Cortes, C., Lawrence, N., Lee, D.,
Research,39(1):3–20,2020. Sugiyama,M.,andGarnett,R.(eds.),AdvancesinNeu-
ralInformationProcessingSystems,volume28.Curran
Bain, M. and Sammut, C. A framework for behavioural
Associates,Inc.,2015.
cloning. InMachineIntelligence15,pp.103–129,1995.
Czarnecki,W.M.,Gidel,G.,Tracey,B.,Tuyls,K.,Omid-
Baker, B., Akkaya, I., Zhokov, P., Huizinga, J., Tang, J.,
shafiei,S.,Balduzzi,D.,andJaderberg,M. Realworld
Ecoffet, A., Houghton, B., Sampedro, R., and Clune,
gameslooklikespinningtops. AdvancesinNeuralInfor-
J. Videopretraining(vpt): Learningtoactbywatching
mationProcessingSystems,33:17443–17454,2020.
unlabeledonlinevideos. AdvancesinNeuralInformation
ProcessingSystems,35:24639–24654,2022. Dai,Y.,Yang,M.,Dai,B.,Dai,H.,Nachum,O.,Tenenbaum,
J., Schuurmans, D., andAbbeel, P. Learninguniversal
Bansal,T.,Pachocki,J.,Sidor,S.,Sutskever,I.,andMor-
policiesviatext-guidedvideogeneration. arXivpreprint
datch,I. Emergentcomplexityviamulti-agentcompeti-
arXiv:2302.00111,2023.
tion. InInternationalConferenceonLearningRepresen-
tations,2018. Ding,Y.,Florensa,C.,Abbeel,P.,andPhielipp,M. Goal-
conditionedimitationlearning. Advancesinneuralinfor-
Beliaev,M.,Shih,A.,Ermon,S.,Sadigh,D.,andPedarsani,
mationprocessingsystems,32,2019.
R. Imitationlearningbyestimatingexpertiseofdemon-
strators. InInternationalConferenceonMachineLearn- Ernst,D.,Geurts,P.,andWehenkel,L. Tree-basedbatch
ing,pp.1732–1748.PMLR,2022. modereinforcementlearning. JournalofMachineLearn-
ingResearch,6,2005.
Berner,C.,Brockman,G.,Chan,B.,Cheung,V.,De˛biak,P.,
Dennison,C.,Farhi,D.,Fischer,Q.,Hashme,S.,Hesse, Ettinger,S.,Cheng,S.,Caine,B.,Liu,C.,Zhao,H.,Prad-
C., et al. Dota 2 with large scale deep reinforcement han,S.,Chai,Y.,Sapp,B.,Qi,C.R.,Zhou,Y.,etal.Large
learning. arXivpreprintarXiv:1912.06680,2019. scaleinteractivemotionforecastingforautonomousdriv-
ing: The waymo open motion dataset. In Proceedings
Bojarski,M.,DelTesta,D.,Dworakowski,D.,Firner,B.,
oftheIEEE/CVFInternationalConferenceonComputer
Flepp,B.,Goyal,P.,Jackel,L.D.,Monfort,M.,Muller,
Vision,pp.9710–9719,2021.
U.,Zhang,J.,etal. Endtoendlearningforself-driving
cars. arXivpreprintarXiv:1604.07316,2016. Fujimoto,S.,Meger,D.,andPrecup,D. Off-policydeep
reinforcementlearningwithoutexploration. InInterna-
Brown,D.,Goo,W.,Nagarajan,P.,andNiekum,S. Extrap-
tionalconferenceonmachinelearning,pp.2052–2062.
olatingbeyondsuboptimaldemonstrationsviainversere-
PMLR,2019.
inforcementlearningfromobservations. InInternational
conference on machine learning, pp. 783–792. PMLR, Grover,A.,Al-Shedivat,M.,Gupta,J.,Burda,Y.,andEd-
2019. wards,H. Learningpolicyrepresentationsinmultiagent
9ELA:ExploitedLevelAugmentationforOfflineLearninginZero-SumGames
systems. InInternationalconferenceonmachinelearn- Russell,S. Learningagentsforuncertainenvironments. In
ing,pp.1802–1811.PMLR,2018. ProceedingsoftheeleventhannualconferenceonCom-
putationallearningtheory,pp.101–103,1998.
Heinrich, J. and Silver, D. Deep reinforcement learning
from self-play in imperfect-information games. arXiv Sasaki,F.andYamashina,R. Behavioralcloningfromnoisy
preprintarXiv:1603.01121,2016. demonstrations. InInternationalConferenceonLearning
Representations,2020.
Ho,J.andErmon,S. Generativeadversarialimitationlearn-
ing. Advancesinneuralinformationprocessingsystems, Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
29,2016. Klimov, O. Proximal policy optimization algorithms.
arXivpreprintarXiv:1707.06347,2017.
Kumar, A., Zhou, A., Tucker, G., and Levine, S. Con-
servative q-learning for offline reinforcement learning.
Shafiullah,N.M.,Cui,Z.,Altanzaya,A.A.,andPinto,L.
AdvancesinNeuralInformationProcessingSystems,33:
Behaviortransformers: Cloningkmodeswithonestone.
1179–1191,2020. Advancesinneuralinformationprocessingsystems,35:
22955–22968,2022.
Lu,Y.,Fu,J.,Tucker,G.,Pan,X.,Bronstein,E.,Roelofs,
B., Sapp, B., White, B., Faust, A., Whiteson, S., et al.
Vinyals,O.,Babuschkin,I.,Czarnecki,W.M.,Mathieu,M.,
Imitationisnotenough: Robustifyingimitationwithre-
Dudzik,A.,Chung,J.,Choi,D.H.,Powell,R.,Ewalds,
inforcementlearningforchallengingdrivingscenarios.
T., Georgiev, P., etal. Grandmasterlevelinstarcraftii
arXivpreprintarXiv:2212.11419,2022.
usingmulti-agentreinforcementlearning. Nature, 575
Lynch,C.,Khansari,M.,Xiao,T.,Kumar,V.,Tompson,J., (7782):350–354,2019.
Levine,S.,andSermanet,P. Learninglatentplansfrom
Yang,M.,Levine,S.,andNachum,O. Trail: Near-optimal
play. InConferenceonrobotlearning,pp.1113–1132.
imitationlearningwithsuboptimaldata. arXivpreprint
PMLR,2020.
arXiv:2110.14770,2021.
Mandlekar, A., Xu, D., Wong, J., Nasiriany, S., Wang,
Yu, T., Kumar, A., Chebotar, Y., Hausman, K., Finn, C.,
C.,Kulkarni,R.,Fei-Fei,L.,Savarese,S.,Zhu,Y.,and
andLevine,S. Howtoleverageunlabeleddatainoffline
Martín-Martín,R. Whatmattersinlearningfromoffline
reinforcementlearning. InInternationalConferenceon
human demonstrations for robot manipulation. In 5th
MachineLearning,pp.25611–25635.PMLR,2022.
AnnualConferenceonRobotLearning,2021.
Zha,D.,Lai,K.-H.,Huang,S.,Cao,Y.,Reddy,K.,Vargas,
Mnih, V., Kavukcuoglu, K., Silver, D., Graves, A.,
J.,Nguyen,A.,Wei,R.,Guo,J.,andHu,X. Rlcard: A
Antonoglou,I.,Wierstra,D.,andRiedmiller,M. Playing
platform for reinforcement learning in card games. In
atari with deep reinforcement learning. arXiv preprint
IJCAI,2020.
arXiv:1312.5602,2013.
Ziebart, B. D., Maas, A. L., Bagnell, J. A., Dey, A. K.,
Pan, L., Huang, L., Ma, T., and Xu, H. Plan better amid
etal. Maximumentropyinversereinforcementlearning.
conservatism: Offlinemulti-agentreinforcementlearning
InAAAI,volume8,pp.1433–1438.Chicago,IL,USA,
withactorrectification. InInternationalConferenceon
2008.
MachineLearning,pp.17221–17237.PMLR,2022.
Pearce,T.,Rashid,T.,Kanervisto,A.,Bignell,D.,Sun,M.,
Georgescu,R.,Macua,S.V.,Tan,S.Z.,Momennejad,I.,
Hofmann,K.,etal. Imitatinghumanbehaviourwithdif-
fusionmodels. InInternationalConferenceonLearning
Representations,2023.
Pomerleau, D. A. Alvinn: An autonomous land vehicle
in a neural network. Advances in neural information
processingsystems,1,1988.
Ross,S.,Gordon,G.,andBagnell,D. Areductionofimita-
tionlearningandstructuredpredictiontono-regretonline
learning. InProceedingsofthefourteenthinternational
conference on artificial intelligence and statistics, pp.
627–635.JMLRWorkshopandConferenceProceedings,
2011.
10ELA:ExploitedLevelAugmentationforOfflineLearninginZero-SumGames
A.TheProofofProposition6.1
Proof. Forsimplicity,weonlyproveina2-playersetting. Bydefinitionofexploitability,E(π)= r(BR(π),π). Sowe
−
have
E(π(τ))= r(BR(π(τ)),π(τ))
−
(cid:16) (cid:17)
= r argmax r(π ,π(τ)),π(τ)
− π−i −i
(cid:90) (cid:16) (cid:17)
= τ(π)r argmax r(π ,π(τ)),π dπ
−
π Π
π−i −i
(cid:90) ∈ (cid:16) (cid:17)
τ(π)r argmax r(π ,π),π dπ
≤−
π Π
π−i −i
(cid:90) ∈
= τ(π)r(BR(π),π)dπ
−
π Π
(cid:90) ∈
= τ(π)E(π)dπ
π Π
∈
Theinequalityisestablishedbythepropertyofargmaxfunction.
B.TheProofofProposition6.2
Proof. Sinceπ(τ)isϵ -Nashequilibrium,theexploitabilityE(π(τ)) ϵ . Thusforanarbitraryπˆ,wehaver(πˆ,π(τ))
1 1
≤ ≥
ϵ . Hence,forallτ satisfyingd(f(τ),f(τ ))<δ,wehave
1 ′ ′
−
(cid:90)
r(πˆ,π(τ ))= τ (π)r(πˆ,π)dπ
′ ′
π Π
(cid:90) ∈ (cid:90)
= τ(π)r(πˆ,π)dπ+ (τ (π) τ(π))r(πˆ,π)dπ
′
−
π Π π Π
∈ (cid:90) ∈
r(πˆ,π(τ)) τ (π) τ(π) r(πˆ,π) dπ
′
≥ − | − || |
π Π
(cid:90) ∈
ϵ M τ (π) τ(π) dπ
1 ′
≥− − | − |
π Π
∈
> ϵ αδM
1
− −
Thus,wehave
(cid:80) ( r(πˆ,π(τ )))+
EL δ(τ)= (cid:80)d(f(τ),f(τ′))<δ −
1
′
d(f(τ),f(τ′))<δ r(πˆ,π(τ′)) 0
≤
max r(πˆ,π(τ ))
′
≤d(f(τ),f(τ′))<δ−
<ϵ +αδM
1
C.TheGamesandImplementationDetails
C.1.OverviewoftheZero-SumGames
Wechoosethefollowingwell-knowngamesinourexperiments:
• Rock-Paper-Scissors(RPS):Playershavethreepotentialactionstotake: rock,paper,andscissors. Theobservationof
eachplayeristheactionoftheopponentinthelastround. Ineachtrajectory,RPSgamesareplayedforT =500times
consecutively. Theplayerwhowinsgets+1point,andtheplayerwholosesgets 1point. Whenthereisadraw,the
−
pointisnotchanged.
• Two-playerPong: Eachplayercontrolsapaddleononesideofthescreen. Thegoalistokeeptheballinplayby
movingthepaddlesupordowntohitit. Ifaplayermisseshittingtheballwiththeirpaddle,itlosesthegame. The
observationofplayersincludesballandpaddlepositionsacrosstwoconsecutivetimestepsandpotentialactionsinclude
movingupordown.
• LimitTexasHold’em: Playersstartwithtwoprivateholecards,andfivecommunitycardsarerevealedineachstage
(theflop,turn,andriver). Eachplayerhastocreatethebestfive-cardhandusingacombinationoftheirholeandthe
11ELA:ExploitedLevelAugmentationforOfflineLearninginZero-SumGames
communitycards. Duringthefourrounds,playerscanselectcall,check,raise,orfold. Theplayersaimtowinthe
gamebyaccumulatingchipsthroughstrategicbettingandbuildingstrongpokerhands. Theobservationofplayersis
a72-elementvector,withthefirst52elementsrepresentingcards(holecardsandcommunitycards)andthelast20
elementstrackingthebettinghistoryinfourrounds.
C.2.ImplementationDetails
IntheactualimplementationofP-VRNN,theactiona andobservationo passthroughneuralnetworksψ andψ first
t t a o
to reduce dimension and extract features. The functions ϕ , ϕ , and ϕ are implemented with multi-layer perceptron
p e d
(MLP)withlatentspacedimensionz =8,hiddenlayerdimensionh =8,recurrencelayerdimensionr =8and
dim dim dim
representationdimensionl = 8. GatedRecurrentUnit(GRU)(Chungetal.,2014)isusedastherecurrencefunction
dim
ϕ. Wetrainedthemodelsfor100epochswithalearningrateof0.0005andabatchsizeof32trajectoriesusingtheAdam
r
optimizer. AsforELestimation,wealsouseGRU,andweusetherecurrentoutputofthefinalstepasfunctionoutput.
Inourofflinelearningexperiments,weutilizeanMLParchitecturefortheactornetwork,withtwohiddenlayersof256
unitseach. Ourofflinedatasetconsistsof45Ktrajectories,eachcontaining100timestepsfortheLimitedTexasHold’em
gameand1KtimestepsfortheTwo-playerPonggame. Duringofflinelearning,wetrainedthemodelsfor300epochswith
alearningrateof0.0005. Wesettheminibatchnumberto50foreachepoch,employingtheAdamoptimizertoensurea
consistentnumberofupdatesforallmethods. WeusedawidelyusedcodebaseforBCQ1andCQL2toensureconsistency
andreproducibility. AllexperimentswereconductedusinganRTX2080TiGPUandanAMDRyzenThreadripper3970X
CPU.
D.RemarksonLearningStrategyRepresentationofTrajectoriesinMulti-AgentGames
Inanimperfectinformationgame,relyingonmodelsthatonlyconsidertheobservationandactioninformationofasingle
time step is insufficient to obtain strategy representations for trajectories. Also, directly inferring representation from
observationandactionusinganoperatorintroducesbiasduetotheinfluenceofopponents’strategiesontheiractions. Unlike
asingleplayerinaspecificenvironment,whereactionsdirectlyaffectobservations,thepresenceofmultipleplayersleads
todiverseobservationsevenwhentheagent’sbehaviorremainsconstant. Forthesamereason,therepresentationshould
remainindependentofthereward,asitisacombinedoutcomeofbothplayers,includingtheopponent. Furthermore,the
representationshouldalsobeinformativeenoughtopredictaplayer’ssubsequentaction. Therefore,weturntovariational
recurrentneuralnetworks(VRNN)(Chungetal.,2015),whichiswidelyusedforsequentialgenerationtoenablesuch
prediction.
1https://github.com/sfujim/BCQ
2https://github.com/BY571/CQL
12