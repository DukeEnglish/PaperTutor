Benchmarking Uncertainty Disentanglement:
Specialized Uncertainties for Specialized Tasks
Ba´lintMucsa´nyi1 MichaelKirchhof1 SeongJoonOh12
Abstract
Uncertaintyquantification,onceasingulartask,
has evolved into a spectrum of tasks, including
Methodranking
abstainedprediction,out-of-distributiondetection,
andaleatoricuncertaintyquantification. Thelat- DeepEnsemble 2 1 1 1 1 4 6
estgoalisdisentanglement: theconstructionof Dropout 1 2 2 2 2 3 2
multipleestimatorsthatareeachtailoredtoone Baseline 6 3 5 3 3 7 9
andonlyonetask. Hence,thereisaplethoraof SNGP 5 8 4 6 7 1 4
recentadvanceswithdifferentintentions—thatof- GP 4 6 3 4 6 2 5
tenentirelydeviatefrompracticalbehavior. This Mahalanobis 11 11 – – 11 – 1
paper conducts a comprehensive evaluation of ShallowEnsemble 3 5 6 5 5 5 3
numerous uncertainty estimators across diverse Laplace 7 7 7 7 9 6 11
tasksonImageNet. Wefindthat,despitepromis- HET-XL 8 4 8 8 4 8 10
ingtheoreticalendeavors,disentanglementisnot CorrectnessPred. 9 9 9 9 8 9 7
yetachievedinpractice. Additionally,wereveal LossPrediction 10 10 – – 10 – 8
whichuncertaintyestimatorsexcelatwhichspe-
cifictasks,providinginsightsforpractitionersand Table1.Differenttaskshavedifferentbest-performinguncertainty
guiding future researchtoward task-centric and quantificationmethodsonImageNet-ReaL.Dropoutanddeepen-
disentangleduncertaintyestimationmethods.Our semblearegoodchoicesacrosstheboard(butexpensive,seeAp-
code is available at https://github.com/ pendixK.3).Best,second-best,andthird-bestmethodhighlighted
ingold,silver,andbronze.Bewarethatdifferencesbetweenranks
bmucsanyi/bud.
canbeverysmall,seetheper-taskplotsfordetails.
1.Introduction
(Lahlouetal.,2023).
When uncertainty quantification methods were first pio-
One limitation of these recent endeavors is that they are
neeredfordeeplearning(Gal&Ghahramani,2016;Laksh-
primarilytheoretical,supportedbytoyorsmall-scaleexper-
minarayananetal.,2017),theirtaskwassimple: givingone
iments(Shaker&Hu¨llermeier,2021;VanAmersfoortetal.,
total uncertainty. The recent demand for trustworthy ma-
2020;Mukhotietal.,2023). Largerscalebenchmarksoften
chinelearning(Mucsa´nyietal.,2023)creatednewrequire-
evaluatetasksforonlyonecomponentanddonottestfor
ments, mostly centering around disentangling the above
undesirablesideeffectsontheothercomponent(Galiletal.,
predictiveuncertaintyintoaleatoric(data-inherentandirre-
2023a;Ovadiaetal.,2019). Whilethisapproachallowsfor
ducible)andepistemic(model-centricandreducible)com-
insightsintotheperformanceofasubsetofmethodsona
ponents(Depewegetal.,2018;Valdenegro-Toro&Mori,
selectionoftasks,thereiscurrentlynostudythatevaluates
2022;Shaker&Hu¨llermeier,2021). Theyservedifferent
whichcomponent(s)eachmethodcapturesinpracticeand
purposes: epistemicuncertaintyiswidelyusedforout-of-
whichitdoesnot.
distributiondetection(VanAmersfoortetal.,2020),andtwo
estimatorsthateachestimateoneandonlyonecomponent Our work establishes an overview of this vast landscape
inadisentangledmannerenabletaskslikeactivelearning ofmethodsandtasks. Wereimplementtwelveuncertainty
quantificationestimatorsinuptoeightwaysandevaluate
1UniversityofTu¨bingen,Germany2Tu¨bingenAICenter,Ger-
each on seven practically defined tasks on ImageNet-1k
many.Correspondenceto:Ba´lintMucsa´nyi<bdothdotmucsanyi
(Deng et al., 2009), ranging from abstained prediction to
atgmaildotcom>.
out-of-distribution detection. We further study if recent
Preliminarywork. uncertaintydecompositionformulasdecomposetheestima-
1
4202
beF
92
]GL.sc[
1v06491.2042:viXra
ssentcerroC ecnenitsbA
.borPgoL
reirB
cirotaelA
ECE DOOBenchmarkingUncertaintyDisentanglement
torsintodisentangledcomponentsastheoreticallyintended overthepre-logitembeddingsandsamplemultipleembed-
(Wimmeretal.,2023;Pfau,2013;Depewegetal.,2018). dingsthatgetturnedintoclassprobabilityvectors.
Wefindthatdisentanglementisunachievedinpracticesince
Dropout(Srivastavaetal.,2014)anddeepensembles(Lak-
mostproposedcombinationsofestimatorsarehighlyinter-
shminarayananetal.,2017)donotconstructdistributions
nallycorrelatedandfailtounmixaleatoricandepistemic
q(f)butdirectlysamplefromthem,eitherbyM repeated
uncertaintyintotwocomponents(Section3.1). However,
forwardpasses,orbytrainingM models,respectively.Shal-
wefindthattherearegroupsofapproachesspecializedon
lowensembles(Leeetal.,2015)arelightweightapproxi-
individualtasks,likedensity-basedapproachesoutperform-
mationsofdeepensembles. Theyuseasharedbackbone
ing the others on out-of-distribution detection (epistemic
andM outputheads(oftenreferredtoas“experts”). Witha
uncertainty,Section3.2)butperformingclosetorandomon
singleforwardpass,oneobtainsM logitvectorsperinput.
aleatoricandpredictiveuncertaintytasks(Section3.3).
Asabaseline,wealsobenchmarkadeterministicnetwork
Thesefindingsemphasizetheimportanceofspecifyingthe thatcorrespondstoaDiracposteriorintheparameterspace.
particulartaskonewantstosolveanddevelopinguncertainty
Practicaltaskslikethreshold-basedrejectionoftenneeda
estimatorstailoredtoit. Weanticipatethatourinsightsinto
scalaruncertaintyoutputu(x)∈Rinsteadofadistribution
thepracticalworkingsofuncertaintyestimatorswilldrive
q(f). Tothisend,aggregatorscompiletheupperdistribu-
the field of uncertainty quantification toward developing
tionsintoscalaruncertaintyestimatesu(x)∈R. Thereare
robustanddisentangleduncertaintyestimators.
severalmethodsforthisaggregation,e.g.,calculatingthe
BayesianModelAverage(BMA)f˜(x):=E [f(x)]and
q(f)
2.BenchmarkedMethods usingitsentropyastheuncertaintyestimateu(x)orquanti-
fyingthevarianceofq(f). Weconsidereightaggregators
This section provides an overview of current uncertainty
detailedinAppendixBand,unlessstatedotherwise,usethe
estimators and disentanglement formulas we benchmark.
best-performingoneforeachdistributionalmethod.
Wereimplementallmethodsasplug-and-playmodulesthat
will be released after the anonymity period. Details are
2.1.2.DETERMINISTICMETHODS
providedinAppendixA.
Deterministicmethods(Postelsetal.,2022)directlyoutput
2.1.UncertaintyEstimators a scalar uncertainty estimate u(x) instead of modeling a
probabilitydistributionoverclassprobabilityvectors.
We consider a classification setting with a discrete label
spaceY ofC classesandmodelsf: X →∆C thatoutput Lossprediction(Yoo&Kweon,2019;Lahlouetal.,2023;
aclassprobabilityvectorontheprobabilitysimplexforany Kirchhofetal.,2023b)employsanadditionalMLPheadfor
inputx∈X. Theuncertaintyestimatorscanbecategorized u(x)thatestimatesthelossofthenetwork’spredictionf(x)
intotwoclasses: distributionalanddeterministicmethods. oneachinputx,assumingthatthelossreflectsanotionof
(in-)correctness. Wealsoimplementaspecialvariantfor
2.1.1.DISTRIBUTIONALMETHODS classification,correctnessprediction,whereu(x)predicts
howlikelythepredictedclassyˆ:=argmax f (x)
c∈{1,...,C} c
Distributional methods output a probability distribution
istobethecorrectclassy,i.e.,p(yˆ=y).
q(f(x) | x)overallpossibleclassprobabilityvectors,ab-
breviatedasq(f). Thisdistributioncan,e.g.,correspondto Deterministic uncertainty quantification (DUQ)
aBayesianhypothesisposteriorp(f |D)inducedbyapa- (Van Amersfoort et al., 2020) learns a latent mixture-of-
rameterposteriorp(θ | D) ∝ p(D | θ)p(θ)whentraining RBF density on the training set and outputs as u(x) how
onadatasetD. closeaninput’sembeddingistothemixturemeans. The
Mahalanobis method (Lee et al., 2018) builds a similar
Spectral-normalized Gaussian processes (SNGP) (Liu
latentmixtureofGaussiansinapost-hocfashion. Italso
etal.,2020)obtainthesedistributionsbyapproximatinga
perturbs the inputs adversarially to train a classifier for
Gaussianprocessovertheclassifieroutput,aidedbyspec-
separating in-distribution (ID) and out-of-distribution
tral normalization on all network parameters. We also
(OOD)samples. Thisistheonlymethodinourbenchmark
benchmark the last-layer Gaussian process without spec-
that requires a validation set for training the logistic
tralnormalization,denotedasGP.TheLaplaceapproxima-
regressionOODdetector.
tion(Daxbergeretal.,2021)approximatesaGaussianposte-
rioroverthenetworkparametersusinganefficientHessian
2.2.UncertaintyDisentanglement
approximation. Thisisapost-hocmethodappliedtoapoint
estimatenetwork,allowingtodrawmultipleoutputsperin- Thepreviousmethodsallgiveonegeneraluncertaintyes-
put. Latentheteroscedasticclassifiers(HET-XL)(Collier timate. Asecondstrainofliteratureoutputsnotonlyone
etal.,2023)predictaheteroscedasticGaussiandistribution estimatebutdecomposesaposteriorq(f)(obtainedbyany
2BenchmarkingUncertaintyDisentanglement
ofthemethodsabove)intomultipleestimators,intending 1.0
toquantifydifferentformsofuncertainty,suchasepistemic 0.8
and aleatoric uncertainty (Hora, 1996). Epistemic uncer-
0.6
taintyiscausedbyalackofdataandcanbereducedasone
gathersmoreinformation. Incontrast,aleatoricuncertainty 0.4
is due to the randomness inherent in the data-generating 0.2
process itself and is irreducible (Mucsa´nyi et al., 2023).
0.0
Theestimatorsforeachsourceshouldbedisentangled: the
aleatoricestimatorshouldonlyreflectaleatoricuncertainty, Figure1.Sixoutofsevendistributionalmethodsexhibitaseverely
andtheepistemicestimatorshouldreflectonlyepistemicun- highrankcorrelationbetweentheinformation-theoreticalaleatoric
certainty. SeeAppendixEformoredetails. Webenchmark and epistemic components when evaluated on ImageNet-ReaL.
twoprominentapproachestoobtainsuchpairsofestimators. Thesemethodsviolateanecessaryconditionofuncertaintydisen-
tanglement.
2.2.1.INFORMATION-THEORETICALDECOMPOSITION
Theinformation-theoretical(IT)decomposition(Depeweg
etal.,2018;Shaker&Hu¨llermeier,2021;Mukhotietal.,
2021;Wimmeretal.,2023)decomposetheentropyofthe
predictivedistributionp(y |x)=(cid:82) p(y |x,f)dq(f)into aleatoric uncertainty is the Bayes risk of the generative
analeatoricandanepistemiccomponent: process,whichisbydefinitionirreducibleandindependent
oftheposteriorq(f).Asthisprocessisunknowninpractice,
H p(y|x)(y)=E q(f)(cid:2)H p(y|x,f)(y)(cid:3) +I p(y,f|x)(y;f), (1) weestimatethealeatorictermbyE (cid:2)H (y)(cid:3) . The
q(f) p(y|f,x)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
epistemicuncertaintyis,similarlytotheITdecomposition,
predictive aleatoric epistemic
the average distance of the posterior members f ∼ q(f)
where H p(y|x)(y) ≡ H(Y |x) is the entropy and fromtheircentroidf¯(x)=argmin E [D [z∥f(x)]].
I (y;f)≡I(Y;F |x)isthemutualinformation. In- z q(f) F
p(y,f|x) This average is calculated in a dual space, but in certain
tuitively, the aleatoric component gives the spread of the
casesisequaltotheBMA(Guptaetal.,2022). Tomake
labels that the plausible models in the posterior have on
thedecompositionequalitycomplete,theBregmandecom-
average, whereas the epistemic component only captures
positionhasathirdterm, thebias. Thisisanuncertainty
thedisagreementofthepredictionp(y |x,f)betweenthe
source that subsumes the uncertainty about the function
modelsf. Sincemostposteriorsq(f)arepracticallyimple-
class(VonLuxburg&Scho¨lkopf,2011).
mented as mixtures of M Diracs q(f) ≈ {f(m)(x)}M ,
m=1
weshowhowthealeatoricandepistemicestimatesarecom- ThepopularDEUPriskdecomposition(Lahlouetal.,2023)
putedinthisspecialcaseofEquation(1)inAppendixC. isaspecialcaseoftheBregmandecomposition. Weprovide
detailsandevaluationinAppendixH.
2.2.2.BREGMANDECOMPOSITIONS
3.Experiments
Bregmandecompositions(Pfau,2013;Guptaetal.,2022;
Lahlouetal.,2023;Gruber&Buettner,2023)usenotonly
With these different estimators and pairs of estimators at
the posterior q(f), which is internally computed by each
hand,wenowinvestigateourmainresearchquestions:Does
method,butalsotaketheground-truthgenerativeprocess
any approach give disentangled uncertainty estimators in
pgt(x,y) into account. Bregman decompositions then de-
practice? Furthermore,whattypeofuncertaintydoeseach
compose the expected loss of a model over all possible
estimatorcaptureintermsofpracticaltasks?
training datasets, where the loss D is a Bregman diver-
F
genceliketheEuclideandistanceortheKullback-Leibler We reimplement and train each approach on a pretrained
divergence. ResNet-50for50epochsonImageNet-1k(Dengetal.,2009)
withatrainingpipelinefollowingTranetal.(2022). Since
E [D [y∥f(x)]]=E [D [y∥f∗(x)]]
q(f),pgt(y|x) F pgt(y|x) F theDUQandMahalanobismethodshavememoryandsta-
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
predictive aleatoric bilityissuesonImageNet,inSection3.7,werepeatexper-
+E (cid:2) D (cid:2) f¯(x)∥f(x)(cid:3)(cid:3) iments on CIFAR-10 (Krizhevsky & Hinton, 2009) with
q(f) F
(cid:124) (cid:123)(cid:122) (cid:125) the Wide ResNet 28-10 architecture, following Liu et al.
epistemic
(2020). We track the validation performance to conduct
+D F
(cid:2) f∗(x)∥f¯(x)(cid:3)
(2) early stopping and to choose further hyperparameters of
(cid:124) (cid:123)(cid:122) (cid:125)
eachmethod. Wereportmean, minimum, andmaximum
bias
performanceacrossfiveseeds. Thebenchmarktook1GPU
Since f∗(x) = E [y] is the Bayes predictor, the yearonRTX2080TiGPUs.
pgt(y|x)
3
noitalerroCknaR ↓
ecalpaL tuoporD
.snEwollahS .snEpeeD
PGNS
PG
LX-TEHBenchmarkingUncertaintyDisentanglement
1.0 Distributional 1.0 Distributional
Deterministic Deterministic
0.9 0.8
0.8 0.6
0.7 0.4 0.6 0.2
0.5 0.0
Figure2.Mahalanobis—adirectOODdetector,dropout,andshal- Figure3.Only deep ensembles and dropout have a higher rank
low ensembles distinguish ID and OOD samples considerably correlation with the ground-truth aleatoric uncertainty than the
better(AUROC≥ 0.728)thanthebaseline(AUROC= 0.674). cross-entropybaseline(rankcorr. = 0.516). Methodsareeval-
OODsamplesareperturbedbyImageNet-Ccorruptionsofseverity uated on the ImageNet validation set using the entropy of the
leveltwo. ImageNet-ReaLlabelsasground-truthaleatoricuncertainty.
3.1.Uncertaintydisentanglementoftenfails
decompositionsproposeasepistemicestimators. So,while
WefirststudyiftheITandBregmandecompositionsyield theITdecompositionofLaplacegivesdecorrelatedestima-
pairs of disentangled estimators. Since they can only de- tors, they are not good estimators of their corresponding
composetheposteriorsq(f)ofdistributionalmethods,de- ground-truthuncertainties(AppendixG.2).
terministicmethodsareexcludedinthissection.
The best OOD detection, and thus the highest alignment
Figure1revealsasimplefailure: forsixofthesevendis- withepistemicuncertainty,isachievedbytheMahalanobis
tributionalmethods,theITdecompositionleadstohighly method. ThisisamethoddevelopedspecificallyforOOD
mutuallycorrelatedaleatoricandepistemicestimates(rank detection. Itisalsotrainedspecificallyforcorruptionsof
corr. ≥ 0.92). ThiscorrelationremainsforBregman(Ap- severitytwo,anditsadvantagevanishesalreadywhenusing
pendix H) and does not considerably lower even when OODsamplesofseveritythree(AppendixI.3).Interestingly,
we add more epistemic uncertainty into the dataset (Ap- the second estimator of this latent density type, DUQ, is
pendixG.1). Therefore,inthemajorityofcases,disentan- theworst-performingmethodonCIFAR-10(AppendixF.4).
glementisviolatedinpractice. This may be because DUQ is developed as a predictive
uncertaintyestimator,notanOODdetector(VanAmersfoort
TheonlyexceptionistheLaplaceposterior,whosealeatoric
etal.,2020). Thisshowsthatuncertaintyestimatorsmust
and epistemic estimators are entirely decorrelated. This
bespecificallytailoredtothetaskapractitionerintendsto
showsthatdisentanglementcannotbethoughtaboutonly
usethemforratherthanrelyingonhigh-levelintuitions.
onthehighlevelofdecompositionsbutneedstotakethe
exactdistributionalmethodintoaccount. However,inthe
3.3.Aleatoricuncertaintyaloneishardtoquantify
followingsections,wewillseethatLaplace’sdecomposed
estimatorsdonotsufficientlycapturethealeatoricandepis- Thepreviousexperimentisolatedtheepistemiccapabilities
temicgroundtruthuncertainties. ofuncertaintyestimates. Letusnowbenchmarkhowwell
theypredictaleatoricuncertainties.
3.2.OOD-nessishardtodetect
WefollowTranetal.(2022);Kirchhofetal.(2023a;b)and
Letusbeginbytestingwhichestimatorsrepresentepistemic usehumanannotatorsasground-truthsforthealeatoricun-
uncertainty. We measure this via an out-of-distribution certainty,inparticulartheirdisagreement: ImageNet-ReaL
(OOD)detectiontask(Gruber&Buettner,2023;Mukhoti (Beyeretal.,2020)(andCIFAR-10H(Petersonetal.,2019))
etal.,2021). WeuseImageNet-C(Hendrycks&Dietterich, queriesmultipleannotatorsforlabelsoneachimage. We
2019)withcorruptionsofseverityleveltwoasOODdata. showcasesomeexamplesinAppendixL.Ifevenhumans
Thisisfarenoughout-of-distributiontodeterioratetheac- disagreeaboutthecontentofanimage,itreflectsthatthe
curacyby26%,seealsoSection3.5. Theground-truthepis- imageisambiguousbyitself.Thismeansthattheentropyof
temicuncertaintyishighonthesesamples,sowemeasure thesoft-labeldistributionperimagegivesanaleatoricuncer-
viaabinaryclassificationAUROCifuncertaintyestimators taintygroundtruth. Wethencalculatetherankcorrelation
arehigherontheseOODsamplesthanonIDsamples.From between an uncertainty estimator and these ground-truth
thispointforward,wealsoconsiderdeterministicmethods. valuesacrossallimages.
AsillustratedinFigure2,Laplaceistheleastabletodetect Figure3showsthatmostmethodsperformbelowthecross-
epistemic uncertainties. Notably, Figure 2 uses the best entropy baseline. Deep ensembles are most aligned with
aggregatoravailable,notjusttheonethattheITorBregman human uncertainties on average. This result shows that
4
CORUA
↑
sibonalahaM tuoporD .snEwollahS PGNS
PG
.snEpeeD .derP.rroC .derPssoL enilesaB LX-TEH ecalpaL
noitalerroCknaR
↑
.snEpeeD tuoporD enilesaB LX-TEH .snEwollahS .derPssoL
PG
PGNS .derP.rroC ecalpaL
sibonalahaMBenchmarkingUncertaintyDisentanglement
1.0 Distributional 1.0 Distributional Deterministic
Deterministic
0.9 0.9
0.8
0.8
0.7 0.7 0.6 0.6
0.5
0.5
Figure4.ID,theperformanceofmethodsonpredictingcorrectness
issaturated,asmeasuredbytheAUROCw.r.t.modelcorrectness Figure5.ID,allbenchmarkedmethodsapartfromaspecialized
ontheImageNetvalidationset.MethodsapartfromMahalanobis OOD detector perform very well on the abstinence task (AUC
arewithina0.023AUROCband.Onlydropoutachievesconsis- ≥0.9)basedontheAUCofthecumulativeabstinenceaccuracy
tently better results. The Mahalanobis method is a specialized curve. However, only dropout and deep ensemble surpass the
OODdetectorthatcannotdistinguishIDsamples. baseline,andthemethodsaresaturated,withalmostallofthem
being within a 0.03 AUC band. Evaluation performed on the
ImageNetvalidationdataset.
even though the ensemble members are initialized to the
samepretrainedmodel,thestochasticityinducedbyindepen-
utilizinganyofthecurrentuncertaintyquantifierstoabstain
dentlytrainingthemfurtherbenefitshumanalignment. On
frompredictiononatinysetofuncertainsamples. While
CIFAR-10,theensemblemembersaretrainedfromscratch,
computationally expensive distributional methods have a
resultinginaconsiderablymoreperformantmethod.
slight edge, the cheaper deterministic methods also give
Ontheothersideofthespectrum,theMahalanobismethod considerableperformance.
isalmostuncorrelatedwithaleatoricuncertainty. Coupled
We would like to highlight the poor performance of the
withanestimatorthatrepresentsonlyaleatoricwithoutepis-
Mahalanobismethod. BeingaspecializedOODdetector,it
temic uncertainties, this could give rise to disentangled
alignswithourexpectationsthatitcannottellthecorrectness
uncertainty estimators in future research. This result is
ofonlyin-distributionsamplesapart. InAppendixF.2,we
surprisingsincelatentdensitymethodsareintendedtocap-
show that DUQ, which likewise models the data density,
turealeatoricuncertaintybyplacingaleatoricallyuncertain
alsofallsbehindthebaselineonthecorrectnessprediction
samplesinbetweenclasscentroidswheredensityandthus
task. This suggests that the predictive uncertainty DUQ
uncertaintyishigh (VanAmersfoortetal.,2020).
is intended to achieve is less aligned with the notion of
correctnessbenchmarkedhere.
3.4.Correctnesspredictionworksacrosstheboard
Let us now broaden the view beyond disentanglement to 3.5.UncertaintiescangeneralizewelltoOODsettings
benchmarkhowwelluncertaintyestimatorssolveotherprac-
Anecessaryconditionforthereliabledeploymentofuncer-
ticallyrelevanttasks. Westartwithcorrectnessprediction,
taintyquantificationmethodsisthattheirestimatesshould
where the AUROC quantifies whether wrong predictions
stayperformantwhenfacinguncertaininputs. Wetestthis
generallyhavehigheruncertaintiesthancorrectpredictions.
by checking if their previous abstinence and correctness
Figure 4 shows that most uncertainty estimators perform performancesremainhighlongerthanthemodel’saccuracy
within±0.014ofthecross-entropybaselinewhenpredict- whenincreasingtheOODperturbationlevel. Onlythencan
ingcorrectness. ModernmethodslikeHET-XLdonotout- wetrustthemandbase,e.g.,theabstinencefromprediction
performoldermethodslikedeepensemblesordropout. We ontheseuncertaintyestimates.
seeasimilarsaturationwhenslightlyalteringthecorrect-
Figure6showsthecorrectnesspredictionAUROC,absti-
nessmetrictoaccountforsoftlabelsinAppendixI.1.
nenceAUC,andmodelaccuracyasweincreasinglyperturb
Thereisarelatedtaskcalledabstainedprediction, where thesamplesandgoOOD.Theresultsshowanalmostcon-
thepredictionsforthex%mostuncertainexamplesareex- stant correctness prediction performance as we go more
cludedandwemeasuretheaccuracyontheremainder. The OOD, whereas the accuracy degrades considerably. Ab-
areaunderthiscurveshowshowmanyerrorsremovingthe stainedpredictionperformancedegradestogetherwithac-
supposedlyuncertainsamplesprevents. Figure5showsthat curacy,whichisafundamentalpropertyofthemetricitself
thesaturationisjustaspronouncedontheabstainedpredic- sincetheareaundertheaccuracycurvedependsonthebase-
tiontask. AlluncertaintymethodsapartfromMahalanobis lineaccuracy. Thisismaintainedevenwhenwenormalize
obtain an AUC score greater than 0.92. Practically, this themetrics(solidlines)accordingtotheirrandompredictive
meansthatonecanobtainahighclassificationaccuracyby performanceusingtheformula(metric−rnd)/(1−rnd)for
5
CORUA
↑
tuoporD .snEpeeD .snEwollahS PG PGNS enilesaB ecalpaL LX-TEH .derPssoL .derP.rroC
sibonalahaM
CUAecnenitsbA
↑
.snEpeeD tuoporD enilesaB LX-TEH .derPssoL .snEwollahS
PG
ecalpaL PGNS .derP.rroC sibonalahaMBenchmarkingUncertaintyDisentanglement
1.0 1
LogProb.
0.8 Brier 0.959 0.5
-ECE(*) 0.9290.898
0.6 Correctness 0.7870.7990.749
0
Abstinence 0.5670.6460.394 0.37
0.4
Accuracy 0.3770.4620.1740.085 0.91
0.5
AUROCCorrectness Aleatoric 0.6610.7110.4970.5470.8970.765 −
0.2
AUCAbstinence
OOD(*) 0.1850.0720.1750.414-0.206-0.498-0.002
Accuracy 1
−
0.0 0 1 S2 everityLev3
el
4 5 LogProb. Brie -r ECE C( o* r) rectne As bs stinence Accuracy Aleatoric OOD(*)
Figure6.Thepredictiveperformanceofuncertaintymethodsde- Figure7.Onlysomeoftheconsideredmetricshaveaveryhigh
gradesmuchsloweronthecorrectnesspredictiontaskthanthe correlationamongmethodsontheImageNetvalidationdataset:
accuracyofthemodelasthesamplesbecomemoreOODbycor- mostcapturedifferentaspectsofuncertaintymethods.Rankcor-
ruptingtheImageNetvalidationimages.Thedisplayedmethodis relationofmetricpairsacrossallmethodsandaggregators. (*)
dropout,whoseresultsarerepresentativeofallothermethods(ex- OOD’sandECE’scorrelationwiththeothermethodsissensitive
ceptMahalanobis).Solidlinescorrespondtometricsnormalized tothechoiceofaggregator,seeAppendixJ.
tothe[0,1]rangew.r.t.therandompredictorontheparticulartask.
Dashedlinescorrespondtounnormalizedvalues.
ofpredictioninone,theyarenotcorrelatedwithaccuracy.
OODdetectiondoesnotbelongtoanyoftheseclusters,un-
directcomparability,whererndisthebasevaluethataran- derliningthatitbenchmarksadifferenttypeofuncertainty,
dompredictorachievesonthatmetric(0.5forAUROC,1/C namely,epistemicuncertainty.
forclassificationaccuracy). Thisholdsacrossallmethods
Astherearedifferentgroupsoftasks,thereisnoone-fits-all
(exceptMahalanobis),seeAppendixI.2. Thisobservation
uncertaintyestimator. Table1demonstratesthisbyranking
underlinesthetrustworthinessofexistinguncertaintyquan-
all methods on all tasks. An uncertainty estimator has to
tificationmethodsonOODcorrectnessprediction.
bechosenordevelopedforthespecifictaskapractitioner
isinterestedin. Ifthetaskisunknown,dropoutanddeep
3.6.Differenttasksrequiredifferentestimators ensemblesofferagoodcompromise,buteventhebaselineis
agoodstartingpointiftheruntimecostsofdeepensembles
Intheprevioussections,wehavehintedatthefactthatthe
aretoohigh(AppendixK.3).
performanceacrossmethodsisverysimilaronsometasks
anddissimilaronothers. Inthissection,weinvestigatethe
3.7.Conclusionsdonotalwaystransferamongdatasets
correlationamongthepreviouspracticaltasksandfurther
popular metrics using a correlation matrix. To construct
We conclude our experiments with a word of caution:
thematrix,weconsiderallbenchmarkedmethodswithall
Appendix F repeats all above experiments on CIFAR-10,
uncertainty aggregators that can be benchmarked on the
which is widely used in the uncertainty quantification lit-
consideredmetrics(seeAppendixB)andcalculatetherank
erature(VanAmersfoortetal.,2020;Mukhotietal.,2021;
correlationondifferentpairsofmetrics.
2023;Gruber&Buettner,2023)butcansometimesleadto
Figure 7 shows two clusters of metrics. The Brier score conclusionsnotinlinewiththelargerscaleImageNet. We
andlogprobabilityproperscoringrules,coupledwiththe sharesomekeydifferencesbelow.
ECE metric, are all recognized as predictive uncertainty
metricsintheliterature(Mucsa´nyietal.,2023). Assuch, Disentanglement. Mostmethodsshowanalmostperfect
theyarealsotightlyconnectedtothecorrectnessprediction rankcorrelationbetweenthecomponentsoftheITdecompo-
task. Thehighrankcorrelationamongthesemetrics(rank sition(seeFigure1forImageNetandFigure8forCIFAR-
corr. ∈ [0.707,0.948]) evidences this claim empirically. 10). However, which of the methods shows a promising
The second cluster is the accuracy, abstinence, aleatoric decorrelationofthecomponentsisseeminglyrandomacross
uncertainty triad: interestingly, methods that are good at thedatasets: onImageNet, Laplaceleadstouncorrelated
capturing human uncertainty are also the most accurate, components,butitshowcasesperfectcorrelationonCIFAR-
eventhoughmodelsaretrainedwithoutaccesstothosehu- 10. TheoppositeholdsforSNGPanditsGPvariant.
manuncertaintysoftlabels. Thereisnonotableconnection
between the two clusters: Although proper scoring rules Aleatoric uncertainty. On CIFAR-10, all methods are
evaluateboththeuncertaintyestimatesandthecorrectness consistently less aligned with human uncertainties (best
6
ycaruccAdnasCORUA ↑BenchmarkingUncertaintyDisentanglement
1.0 0.15
GP SNGP
≈
0.8 Dropout
0.10 DeepEns.
0.6 Baseline
0.4 0.05
0.2
0.00
0.0 ID OODSeverity1
(a)ImageNetcalibrationresults.Methodspreservetheirrankings
Figure8.Fouroutofsevendistributionalmethodsexhibitanal- asthedatasetbecomesmoreOODviaImageNet-Ccorruptions,
mostperfectrankcorrelation(≥0.986)betweentheITaleatoric andmostofthemevenbecomemorecalibrated.
andepistemiccomponentswhentestedonCIFAR-10.TheLaplace
methodthatdisentanglestheITaleatoricandepistemiccompo- 0.10
nentsonImageNetshowsanextremecorrelation. 0.08 G DP ee≈ pES nN seG mP ble
Dropout 0.06
Baseline
0.04
rankcorr. 0.40vs0.54onImageNet,AppendixF.5). Thisis 0.02
peculiar,asthevastnumberofclassesonImageNetcouldin- 0.00
troducemorenoiseintothehumansoftlabels. Forexample, ID OODSeverity1
humanlabelersmightunanimouslyrecognizethattheobject (b)OnCIFAR-10,methodsbecomelesscalibrated,donotpreserve
their rankings, and SNGP breaks down to the baseline level at
inquestionisadog,yettheremaybevariationsinopinion
severitylevelonealready.
regardingitsspecificbreed. Thisindicatesthatthewaysoft
labelsareobtained, whichisdifferentinImageNet-ReaL
Figure9.MethodsdisplaydrasticallydifferentbehavioronIma-
andCIFAR-10H,hasasignificantimpactontheresults.
geNetandCIFAR-10regardingtherobustnessoftheircalibration.
Robustness. Weobservethatcorrectnesspredictorsare
Metric RankCorr.CIFAR-10vsImageNet
muchmorerobustonImageNetthanonCIFAR-10, even
Correctness 0.286
though the drop in accuracy is very similar. Unlike on
Abstinence 0.546
ImageNet,wheretheuncertaintyestimatorsmaintainaclose
LogProb. 0.358
toconstantperformanceinpredictingcorrectnessaswego Brier 0.373
moreandmoreOOD(Figure6),onCIFAR-10,almostall Aleatoric 0.316
correctnessestimatorsdeterioratetogetherwiththemodel’s ECE 0.330
OOD -0.289
accuracy(Figure10). Whilerobustnesswouldappearasa
strikingproblemonCIFAR-10,itgetsresolvedbysimply
Table2.Therankingsofapproachesareconsiderablydifferenton
switchingtoalarger-scaledataset.
CIFAR-10 andImageNet. Correlationsofmethod rankingson
differentmetricsforallcombinationsofmethodsandaggregators.
Calibration. SNGPshowsahighlydifferentperformance
depending on the dataset and task it is trained for. On
CIFAR-10,Figure9bshowsthatSNGP(andGP)provide
rankings(rank. corr≤0.373). Thisisalsoreflectedinthe
thebest-calibrateduncertaintiesID,butalreadyatthelowest
best-performingmethods:Infouroftheseventasks,itisnot
OODperturbationleveldroptothebaselinelevel,withthe
thesameonImageNetasitisonCIFAR-10. Thisindicates
ECE jumping from 0.005 to 0.084. In Appendix I.4, we
thatperformanceonCIFAR-10shouldnotbetakenasan
showthatthebestwaytoaggregateSNGP’sposteriorq(f)
estimateforperformanceonlarger-scaledatasets.
intoanuncertaintyscoreu(x)isdifferentwhenoptimizing
fortheECEversuscorrectnessprediction,showingthatsub- Theseexperimentsunderlinethatmethodsmightshowsub-
tledesignchoicesgreatlyaffectperformance. OnImageNet, stantially different behaviors on large-scale datasets. We
Figure9aevidencesthattheexactoppositehappens: most encourage,asbestpractice,tofirstscaletheapproachesto
methodsbecomemorecalibratedaswegoslightlyOOD, thefinaldeploymentdomain(anddefineaprecisetask)in-
andSNGPvariantsretaintheircalibration. steadofmakingfundamentaldesignchoicesontoydatasets.
Methodrankings. Thesefundamentallydifferentbehav- 4.RelatedWorks
iorsalsochangetherankingoftheapproachesinsometasks.
Table2showsthecorrelationofrankingsonCIFAR-10and Thissectiondiscussespreviousquantitativeuncertaintystud-
ImageNet. 6outof7metricshavesubstantiallydifferent iesandhowtheirfindingsconnecttoours.
7
noitalerroCknaR
↓
PGNS
PG
.snEwollahS
tuoporD
.snEpeeD
ecalpaL LX-TEH
ECE
ECE
↓
↓BenchmarkingUncertaintyDisentanglement
1.0 supportsthesefindingsonCIFAR-10,especiallyinthere-
gionwheretheOOD-nessisonlyslightyetalreadycauses
0.8 degradationofboththemaintaskandtheuncertaintyesti-
mator. Thelatterimpliesthatuncertaintyestimatorseither
0.6
needtobecomemorerobusttodistributionshifts(Kirchhof
et al., 2023b) or be better able to detect subtle epistemic
0.4
uncertainties. However,ourexperimentsonImageNetdo
AUROCCorrectness notshowrobustnessissues,highlightingtheimportanceof
0.2
AUCAbstinence theuseddataset.
Accuracy
0.0
0 1 2 3 4 5 Aleatoric uncertainty, as opposed to epistemic uncer-
SeverityLevel
taintywiththeOODdetectionproxytask,stilllacksastan-
dardizedtestingprotocol. Thecurrentapproachesseemto
Figure10.OnCIFAR-10,modelaccuracyandtheperformanceof
convergetosoftlabels,butnuancesinhowtheyarecollected
theuncertaintymethoddegradetogetherasthesamplesbecome
stillneeddiscussion(compare,forexample,CIFAR-10H
moreOODbycorruptingtheimages. Thedisplayedmethodis
(Petersonetal.,2019)toCIFAR-10S(Collinsetal.,2022)
dropout,whoseresultsarerepresentativeofallothermethods(ex-
ceptMahalanobis).Solidlinescorrespondtometricsnormalized andCIFAR-10N(Weietal.,2022)). Anincreasingnumber
tothe[0,1]rangew.r.t.therandompredictor;dashedlinescorre- ofuncertaintyquantificationapproachescomparetosuch
spondtotheunnormalizedvalues. humanground-truthnotionsofaleatoricuncertainty(Tran
etal.,2022;Kirchhofetal.,2023a;b),indicatingtheinterest
inthefield. Ourbenchmarkshowsthatnomethodcanyet
reliablygivealeatoricuncertaintyestimates,stressingthe
Disentanglement ofaleatoricandepistemicuncertainties needforbenchmarksandmethodstodevelopalong.
viadecompositions(Pfau,2013;Depewegetal.,2018)has
recentlybeenshowntohavefailurecases(Wimmeretal., Predictive uncertainty tasks and calibration, on the
2023;Bengsetal.,2023;Gruberetal.,2023;Valdenegro- otherhand,starttobecomesaturatedandreadyforappli-
Toro&Mori,2022). Thesepapersmaketheirclaimstheo- cation according to our experiments. This corroborates
reticallyorviatoyproblemslikebinaryclassificationor1D recent findings by Galil et al. (2023b). In comparison to
regression. Ourresultssupportthisdiscussionwithaprac- thisbenchmarkthatcomparedmodelarchitectures,wecom-
ticalandquantitativeperspective. Wefindthatuncertainty pareddifferentapproachesonthesamebackbone.
decompositionsdonotworkingeneraland,ifatall,depend
greatlyontheirpracticalimplementation. Ourfindingsthus A limitation of our study is the focus on classification.
encourage us to take a more holistic view of disentangle- Futureresearchshouldaimtoexpandthescopeoftheinves-
ment, includingthedecompositionformula, method, and tigationtoincludeawiderarrayoftaskslikeuncertainties
implementation. Forexample,accordingtoourresults,itis forregression(Upadhyayetal.,2023)orunsupervisedlearn-
promisingtocombineseparatemethods,suchaslosspre- ing(Kirchhofetal.,2023a). Thisexpansionwouldhelpin
dictionandtheMahalanobisdistance,whereeachmethod generalizingourfindingsandunderstandingthedynamics
handlesaspecifictypeofuncertainty,similartoMukhoti ofuncertaintyquantificationindiversereal-worldscenarios.
etal.(2021).
5.Conclusion
Sensitivitytoimplementation. Thefindingthattheper-
formanceofuncertaintyestimatorsdependsgreatlyontheir Westudyhowcurrentuncertaintyestimatorsanddisentan-
implementation and that different design choices are bet- glementformulasperformonawidearrayofuncertainty
terfordifferentmetricsisinlinewithrecentbenchmarks quantificationtasks. Insummary, ourfindingsencourage
(Galiletal.,2023b;Kirchhofetal.,2023b). Ourbenchmark apragmaticreassessmentofuncertaintyquantificationre-
furthershowsthattheaggregatorfunctionofdistributional search. Thereisnogeneraluncertainty;instead,uncertainty
methodsisacrucialcomponentandthatsimplyaveraging quantificationcoversaspectrumoftaskswherethedefini-
outputsisofteninferiorto,e.g.,averaginginthedualspace tionoftheexacttaskheavilyinfluencestheoptimalmethod
(Guptaetal.,2022). andperformance. Suchaprecisedefinitionoftasksperesti-
matorwouldalsobenefitconstructingdisentangleduncer-
Robustness. RecentbenchmarksonOODdetectionand tainties. Thisshiftcouldleadtothealignmentoftheoretical
robustness(Nadoetal.,2021;Ovadiaetal.,2019;Postels developmentandintuitivedescriptionsaboutwhatparticular
et al., 2022; Galil et al., 2023a) have first highlighted ro- typesofuncertaintyamethodaimstocapture,withtangible
bustness issues of uncertainty estimates. Our benchmark improvementsonthebenchmarktasksweconsider.
8
ycaruccAdnasCORUA
↑BenchmarkingUncertaintyDisentanglement
ImpactStatement Galil,I.,Dabbah,M.,andEl-Yaniv,R. Whatcanwelearn
fromtheselectivepredictionanduncertaintyestimation
Thispaperpresentsworkwhosegoalistoadvancethefield
performanceof523imagenetclassifiers? InInternational
of Machine Learning. There are many potential societal
ConferenceonLearningRepresentations(ICLR),2023b.
consequencesofourwork,noneofwhichwefeelmustbe
specificallyhighlightedhere. Gruber, C., Schenk, P. O., Schierholz, M., Kreuter, F.,
and Kauermann, G. Sources of uncertainty in ma-
References chine learning–a statisticians’ view. arXiv preprint
arXiv:2305.16703,2023.
Bengs,V.,Hu¨llermeier,E.,andWaegeman,W. Onsecond-
orderscoringrulesforepistemicuncertaintyquantifica- Gruber, S. and Buettner, F. Uncertainty estimates of pre-
tion. InInternationalConferenceonMachineLearning dictions viaa generalbias-variancedecomposition. In
(ICML),2023. InternationalConferenceonArtificialIntelligenceand
Statistics,pp.11331–11354.PMLR,2023.
Beyer,L.,He´naff,O.J.,Kolesnikov,A.,Zhai,X.,andOord,
A. v. d. Are we done with imagenet? arXiv preprint Gupta, N., Smith, J., Adlam, B., and Mariet, Z. Ensem-
arXiv:2006.07159,2020. blingoverclassifiers: abias-varianceperspective. arXiv
preprintarXiv:2206.10566,2022.
Biewald,L. Experimenttrackingwithweightsandbiases.
2020. URLhttps://www.wandb.com/. Software He,K.,Zhang,X.,Ren,S.,andSun,J. Deepresiduallearn-
availablefromwandb.com. ingforimagerecognition. InProceedingsoftheIEEE
conferenceoncomputervisionandpatternrecognition,
Collier,M.,Jenatton,R.,Mustafa,B.,Houlsby,N.,Berent, pp.770–778,2016.
J.,andKokiopoulou,E.Massivelyscalingheteroscedastic
classifiers. In The Eleventh International Conference Hendrycks, D. and Dietterich, T. Benchmarking neural
on Learning Representations, 2023. URL https:// networkrobustnesstocommoncorruptionsandperturba-
openreview.net/forum?id=sIoED-yPK9l. tions. arXivpreprintarXiv:1903.12261,2019.
Collins, K. M., Bhatt, U., and Weller, A. Eliciting and Hora, S.C. Aleatoryandepistemicuncertaintyinproba-
learning with soft labels from every annotator. In Pro- bilityelicitationwithanexamplefromhazardouswaste
ceedingsoftheAAAIConferenceonHumanComputation management. ReliabilityEngineering&SystemSafety,
andCrowdsourcing(HCOMP),volume10,2022. 54(2-3):217–223,1996.
Daxberger,E.,Kristiadi,A.,Immer,A.,Eschenhagen,R., Kirchhof,M.,Kasneci,E.,andOh,S.J. Probabilisticcon-
Bauer, M., and Hennig, P. Laplace redux-effortless trastivelearningrecoversthecorrectaleatoricuncertainty
bayesian deep learning. Advances in Neural Informa- ofambiguousinputs. InternationalConferenceonMa-
tionProcessingSystems,34:20089–20103,2021. chineLearning(ICML),2023a.
Deng,J.,Dong,W.,Socher,R.,Li,L.-J.,Li,K.,andFei-Fei, Kirchhof,M.,Mucsa´nyi,B.,Oh,S.J.,andKasneci,E.URL:
L. Imagenet: Alarge-scalehierarchicalimagedatabase. Arepresentationlearningbenchmarkfortransferableun-
In2009IEEEconferenceoncomputervisionandpattern certaintyestimates. InThirty-seventhConferenceonNeu-
recognition,pp.248–255.Ieee,2009. ralInformationProcessingSystemsDatasetsandBench-
marks Track, 2023b. URL https://openreview.
Depeweg, S., Hernandez-Lobato, J.-M., Doshi-Velez, F., net/forum?id=e9n4JjkmXZ.
andUdluft,S. Decompositionofuncertaintyinbayesian
deeplearningforefficientandrisk-sensitivelearning. In Krizhevsky,A.andHinton,G. Learningmultiplelayersof
InternationalConferenceonMachineLearning,pp.1184– featuresfromtinyimages. Master’sthesis,Department
1193.PMLR,2018. ofComputerScience,UniversityofToronto,2009.
Gal,Y.andGhahramani,Z. Dropoutasabayesianapprox- Lahlou, S., Jain, M., Nekoei, H., Butoi, V. I., Bertin, P.,
imation: Representingmodeluncertaintyindeeplearn- Rector-Brooks,J.,Korablyov,M.,andBengio,Y. DEUP:
ing. Ininternationalconferenceonmachinelearning,pp. Direct epistemic uncertainty prediction. Transactions
1050–1059.PMLR,2016. on Machine Learning Research, 2023. ISSN 2835-
8856. URLhttps://openreview.net/forum?
Galil, I., Dabbah, M., andEl-Yaniv, R. Aframeworkfor id=eGLdVRvvfQ. ExpertCertification.
benchmarkingclass-out-of-distributiondetectionandits
applicationtoimagenet. InTheEleventhInternational Lakshminarayanan,B.,Pritzel,A.,andBlundell,C. Simple
ConferenceonLearningRepresentations(ICLR),2023a. andscalablepredictiveuncertaintyestimationusingdeep
9BenchmarkingUncertaintyDisentanglement
ensembles. Advancesinneuralinformationprocessing Peterson,J.C.,Battleday,R.M.,Griffiths,T.L.,andRus-
systems,30,2017. sakovsky, O. Human uncertainty makes classification
morerobust. InProceedingsoftheIEEE/CVFInterna-
Lee, K., Lee, K., Lee, H., and Shin, J. A simple unified
tionalConferenceonComputerVision,pp.9617–9626,
frameworkfordetectingout-of-distributionsamplesand
2019.
adversarialattacks. Advancesinneuralinformationpro-
cessingsystems,31,2018. Pfau, D. A generalized bias-variance decomposition for
bregmandivergences. UnpublishedManuscript,2013.
Lee,S.,Purushwalkam,S.,Cogswell,M.,Crandall,D.,and
Batra, D. Why m heads are better than one: Training Postels,J.,Segu`,M.,Sun,T.,Sieber,L.D.,VanGool,L.,Yu,
a diverse ensemble of deep networks. arXiv preprint F.,andTombari,F. Onthepracticalityofdeterministic
arXiv:1511.06314,2015. epistemic uncertainty. In International Conference on
MachineLearning(ICML),2022.
Liu,J.,Lin,Z.,Padhy,S.,Tran,D.,BedraxWeiss,T.,and
Shaker,M.H.andHu¨llermeier,E. Ensemble-baseduncer-
Lakshminarayanan,B. Simpleandprincipleduncertainty
taintyquantification:Bayesianversuscredalinference.In
estimationwithdeterministicdeeplearningviadistance
PROCEEDINGS31.WORKSHOPCOMPUTATIONAL
awareness. AdvancesinNeuralInformationProcessing
Systems,33:7498–7512,2020. INTELLIGENCE,volume25,pp. 63,2021.
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,
Loshchilov, I. and Hutter, F. SGDR: Stochastic gradient
andSalakhutdinov,R. Dropout: asimplewaytoprevent
descentwithwarmrestarts. InInternationalConference
neuralnetworksfromoverfitting. Thejournalofmachine
on Learning Representations, 2017. URL https://
learningresearch,15(1):1929–1958,2014.
openreview.net/forum?id=Skq89Scxx.
Tran,D.,Liu,J.Z.,Dusenberry,M.W.,Phan,D.,Collier,
Loshchilov,I.andHutter,F. Decoupledweightdecayreg-
M., Ren, J., Han, K., Wang, Z., Mariet, Z. E., Hu, H.,
ularization. In International Conference on Learning
Band, N., Rudner, T. G. J., Nado, Z., van Amersfoort,
Representations,2019. URLhttps://openreview.
J.,Kirsch,A.,Jenatton,R.,Thain,N.,Buchanan,E.K.,
net/forum?id=Bkg6RiCqY7.
Murphy, K. P., Sculley, D., Gal, Y., Ghahramani, Z.,
Mucsa´nyi, B., Kirchhof, M., Nguyen, E., Rubinstein, A., Snoek, J., and Lakshminarayanan, B. Plex: Towards
and Oh, S. J. Trustworthy machine learning. arXiv reliability using pretrained large model extensions. In
preprintarXiv:2310.08215,2023. First Workshop on Pre-training: Perspectives, Pitfalls,
andPathsForwardatICML2022,2022. URLhttps:
Mukhoti, J., Kirsch, A., van Amersfoort, J., Torr, P. H., //openreview.net/forum?id=6x0gB9gOHFg.
and Gal, Y. Deep deterministic uncertainty: A simple
baseline. arXivpreprintarXiv:2102.11582,2021. Upadhyay,U.,Kim,J.M.,Schmidt,C.,Scho¨lkopf,B.,and
Akata, Z. Likelihoodannealing: Fastcalibrateduncer-
Mukhoti,J.,Kirsch,A.,vanAmersfoort,J.,Torr,P.H.,and taintyforregression. arXivpreprintarXiv:2302.11012,
Gal, Y. Deepdeterministicuncertainty: Anewsimple 2023.
baseline. InProceedingsoftheIEEE/CVFConference
Valdenegro-Toro,M.andMori,D.S. Adeeperlookinto
onComputerVisionandPatternRecognition,pp.24384–
aleatoricandepistemicuncertaintydisentanglement. In
24394,2023.
Computer Vision and Pattern Recognition Workshops
Nado,Z.,Band,N.,Collier,M.,Djolonga,J.,Dusenberry, (CVPRW),2022.
M.,Farquhar,S.,Filos,A.,Havasi,M.,Jenatton,R.,Jer-
VanAmersfoort,J.,Smith,L.,Teh,Y.W.,andGal,Y. Un-
fel, G., Liu, J., Mariet, Z., Nixon, J., Padhy, S., Ren,
certainty estimation using a single deep deterministic
J., Rudner, T., Wen, Y., Wenzel, F., Murphy, K., Scul-
neuralnetwork. InInternationalconferenceonmachine
ley, D., Lakshminarayanan, B., Snoek, J., Gal, Y., and
learning,pp.9690–9700.PMLR,2020.
Tran, D. Uncertainty Baselines: Benchmarks for un-
certainty&robustnessindeeplearning. arXivpreprint Von Luxburg, U. and Scho¨lkopf, B. Statistical learning
arXiv:2106.04015,2021. theory: Models,concepts,andresults. InHandbookof
theHistoryofLogic,volume10,pp.651–706.Elsevier,
Ovadia, Y., Fertig, E., Ren, J., Nado, Z., Sculley, D.,
2011.
Nowozin, S., Dillon, J., Lakshminarayanan, B., and
Snoek,J. Canyoutrustyourmodel’suncertainty? Evalu- Wei, J., Zhu, Z., Cheng, H., Liu, T., Niu, G., and Liu, Y.
atingpredictiveuncertaintyunderdatasetshift. Advances Learningwithnoisylabelsrevisited: Astudyusingreal-
inNeuralInformationProcessingSystems(NeurIPS),32, worldhumanannotations. InInternationalConference
2019. onLearningRepresentations(ICLR),2022.
10BenchmarkingUncertaintyDisentanglement
Wimmer, L., Sale, Y., Hofman, P., Bischl, B., and
Hu¨llermeier,E. Quantifyingaleatoricandepistemicun-
certaintyinmachinelearning: Areconditionalentropy
andmutualinformationappropriatemeasures? InUncer-
taintyinArtificialIntelligence,pp.2282–2292.PMLR,
2023.
Yoo, D. and Kweon, I. S. Learning loss for active learn-
ing. In Proceedings of the IEEE/CVF Conference on
ComputerVisionandPatternRecognition(CVPR),2019.
Zagoruyko,S.andKomodakis,N. Wideresidualnetworks.
InProcedingsoftheBritishMachineVisionConference
2016.BritishMachineVisionAssociation,2016.
11BenchmarkingUncertaintyDisentanglement
A.BenchmarkedMethods
Weconsideraclassificationsettingwithdiscretelabelspace{1,...,C}ofC classesandmodelsthatoutputaprobability
vectorf(x)∈∆C−1forinputx∈X. The(pre-softmax)logitsofthemodelsaredenotedbylogfˆ(x).
Weevaluatetwoclassesofmethods: directpredictionmethodsanddistributionalmethods.
A.1.DirectPredictionMethods
Directpredictionmethodsoutputanuncertaintyestimateu(x)forinputx,suchasu(x)≈p(f(x)isacorrectprediction).
A.1.1.RISKPREDICTION
Riskprediction(Upadhyayetal.,2023;Lahlouetal.,2023;Kirchhofetal.,2023b)employsanadditionaloutputheadurp
connectedtothepre-logitlayerthatpredictstheriskofthenetwork’spredictiononinputx∈X. Theriskpredictorhead
istrainedinasupervisedfashionbymakingurp(x),thepredictedrisk,closertotheactuallossℓ(f(x),y)=−logfˆ(x).
y
Precisely,weusetheobjective
n
L=(cid:88) −logfˆ (x )+λ(cid:16) urp(x )+logfˆ (x )(cid:17)2 , (3)
yi i i yi i
i=1
wheretheriskpredictorloss(squaredEuclideandistance)istradedoffwiththelabelpredictorloss(cross-entropy)witha
hyperparameterλ∈R+.
Notethatyisarandomvariableinthepresenceofaleatoricuncertainty.Inexpectation,thisencouragesurp(x)toapproximate
thetrueriskR(f,x)=E [ℓ(f(x),y)]ateachinputx.
pgt(y|x)
A.1.2.CORRECTNESSPREDICTION
Correctnesspredictionisavariantofriskpredictionthat,insteadofaimingtopredicttheriskofthenetworkoninputx,
(cid:16) (cid:12) (cid:17)
predictstheprobabilityofcorrectnessp argmax f (x)=y(cid:12)x oninputx. Thisisachievedbyusingasigmoid
c∈{1,...,C} c (cid:12)
correctnesspredictorheadhandusingtheobjective
n
L=(cid:88) −logfˆ (x )−λ(l logh(x )+(1−l )log(1−h(x ))), (4)
yi i i i i i
i=1
(cid:104) (cid:105)
wherel =I argmax logfˆ(x )=y ∀i∈{1,...,n},andthecorrectnesspredictorloss(binarycross-entropy)
i c∈{1,...,C} c i i
is traded off with the label predictor loss (cross-entropy) with a hyperparameter λ ∈ R+. The uncertainty estimate is
ucp(x)=1−h(x)(i.e.,theprobabilityofmakinganerror).
A.1.3.DETERMINISTICUNCERTAINTYQUANTIFICATION
Thedeterministicuncertaintyquantification(DUQ)methodofVanAmersfoortetal.(2020)learnsalatentmixture-of-RBF
density on the training set with a strictly proper scoring rule to capture the uncertainty in the prediction based on the
Euclideandistanceoftheinput’sembeddingtothemixturemeans. Thetrainingobjectiveis
n C
(cid:88)(cid:88)
L=− y logK (x )+(1−y )log(1−K (x ), (5)
ic c i ic c i
i=1c=1
(cid:16) (cid:13) (cid:13)(cid:17)
whereK (x) = exp − 1 (cid:13)logfˆ(x)−m (cid:13) istheRBFvaluecorrespondingtoclassc ∈ {1,...,C}identifiedbyits
c 2γ (cid:13) c(cid:13)
meanvectorm inthelatentspace. Tofacilitateminibatchtraining,VanAmersfoortetal.(2020)employanexponential
c
12BenchmarkingUncertaintyDisentanglement
movingaverage(EMA)tolearnthemeanvectorusingthefollowingupdaterules:
n ←γ·n +(1−γ)|B | (6)
c c c
M ←γ·M +(1−γ) (cid:88) W logfˆ(x) (7)
c c c
(x,y)∈Bc
M
m ← c, (8)
c n
c
whereB isaminibatchofsamplesandB = {(x,y) ∈ B | y = c}∀c ∈ {1,...,C}. γ istheEMAparameterandW
c c
characterizesalinearmappingofthelogitsforeachclass.
Toregularizethelatentdensityandpreventfeaturecollapse,VanAmersfoortetal.(2020)usethefollowinggradientpenalty
addedto∇ L:
θ
(cid:13) (cid:13)2 2
(cid:13) (cid:88)C (cid:13)
λ·(cid:13) (cid:13)∇
x
K c(cid:13)
(cid:13)
−1 (9)
(cid:13) (cid:13)
c=1 2
Each RBF component in the latent space corresponds to one class. The confidence output of the method is the max-
imal RBF value of the input over all classes. Therefore, the uncertainty estimate can be calculated as uduq(x) =
1−max K (x).
c∈{1,...,C} c
Thepredictedclassofthetrainednetworkisargmax K (x).
c∈{1,...,C} c
A.1.4.MAHALANOBIS
The Mahalanobis method (Lee et al., 2018) builds a post-hoc latent density for the training set in the latent space by
calculatingper-classmeansandcovariances,andusingtheinducedmixture-of-Gaussiansasthelatentdensityestimate. Such
latentdensitiesareestimatedinmultiplelayersofthenetwork. Onelayer’sconfidenceestimateisthemaximalMahalanobis
score(Gaussianlog-likelihood)K (x)overallclasses:
ℓ
K (x)=−(f (x)−µ )⊤Σ−1(f (x)−µ ) (10)
ℓ,c ℓ ℓ,c ℓ ℓ ℓ,c
K (x)= max K (x), (11)
ℓ ℓ,c
c∈{1,...,C}
wheref istheℓ-thlayer’soutput,
ℓ
n
1 (cid:88)
µ = I[y =c]f (x ) (12)
ℓ,c n i ℓ i
c
i=1
isthecentroidoftheGaussianforclassc∈{1,...,C}inlayerℓ∈{1,...,L},n isthenumberofsampleswithlabelc,
c
and
C n
Σ = 1 (cid:88)(cid:88) I[y =c](f (x)−µ )(f (x)−µ )⊤ (13)
ℓ n i ℓ ℓ,c ℓ ℓ,c
c=1i=1
isthetiedcovariancematrixusedforallclassesinlayerℓ∈{1,...,L}.
TomakethedifferencesoflatentembeddingsofIDandOODsamplesmorepronounced,allsamplesareadversarially
perturbedw.r.t. themaximalMahalanobisscoreforeachlayer’sconfidencescore:
xˆ(ℓ) =x−ϵsgn(∇ −K (x)). (14)
x ℓ
ThisperturbedsampleisusedtocomputeK (xˆ(ℓ)). Finally,alogisticregressionOODdetectorislearnedonaheld-out
ℓ
validation set of a balanced mix of ID and OOD samples to learn weights w for each layer ℓ ∈ {1,...,L} using the
ℓ
L-dimensionalinputs(cid:2)
K
(cid:0) xˆ(1)(cid:1)
,...,K
(cid:0) xˆ(L)(cid:1)(cid:3)⊤
.
ThefinaluncertaintyestimatebecomesuMah(x)=(cid:80)L
w K (x).
1 L ℓ=1 ℓ ℓ
ThisistheonlymethodinourbenchmarkthatrequiresavalidationsetfortrainingthelogisticregressionOODdetectorand
notjustthehyperparameters.
13BenchmarkingUncertaintyDisentanglement
A.2.DistributionalMethods
Distributionalmethodsoutputaconditionalprobabilitydistributionoverprobabilityvectorsq(f(x)|x),abbreviatedas
q(f).
A.2.1.SPECTRALNORMALIZEDGAUSSIANPROCESS
Spectral normalized Gaussian processes (SNGP) (Liu et al., 2020) give an approximate Bayesian treatment to obtain
uncertaintyestimatesusingspectralnormalizationoftheparametertensorsandalast-layerGaussianprocessapproximated
byFourierfeatures. Foraninputx,itpredictsamultivariateGaussiandistribution
N
(cid:16) βϕ(x),ϕ(x)⊤(cid:0) Φ⊤Φ+I(cid:1)−1 ϕ(x)I(cid:17)
, (15)
whereβ isalearnedparametermatrixthatmapsfromthepre-logitstothelogits,andϕ(x)=cos(Wh(x)+b)isarandom
featureembeddingoftheinputxwithh(x)beingapre-logitembedding,W afixedsemi-orthogonalrandommatrix,andba
fixedrandomvectorsampledfromUniform(0,2π). Φ⊤Φistheempiricalcovariancematrixofthepre-logitsofthetraining
set. Thisiscalculatedduringthelastepoch. ThemultivariateGaussianpresentedabovecanbeMonte-Carlosampledto
obtainM logitvectors.
The method also applies spectral normalization to the hidden weights in each layer in order to satisfy input distance
awareness. Wetreatwhethertoapplyspectralnormalizationthroughthenetworkandwhethertouselayernormalizationin
thelastlayerashyperparameters.
WebenchmarkbothSNGPsandtheirnon-spectral-normalizedvariants(simplydenotedbyGP).
A.2.2.LATENTHETEROSCEDASTICCLASSIFIER
Latentheteroscedasticclassifiers(HET-XL)(Collieretal.,2023)constructaheteroscedasticGaussiandistributioninthe
pre-logitlayertomodelper-inputuncertainties: N(ϕ(x),Σ(x)),where
Σ(x)=V(x)⊤V(x)+diag(d(x)) (16)
is an input-conditional full-rank covariance matrix. Both the low-rank term’s V(x) and the diagonal term’s d(x) are
calculatedasalinearfunctionofthelayer’soutputbeforethepre-logitlayer.
OnecanMonte-Carlosamplethepre-logitsfromtheaboveGaussiandistributionandobtainasetoflogitsbytransforming
eachusingthelastlinearlayerofthenetwork. Duringtraining,thissetisusedtocalculatetheBayesianModelAverage
(BMA)whoseargmaxisthefinalprediction.
HET-XLusesatemperatureparametertoscalethelogitsbeforecalculatingtheBMA.Thisischosenusingavalidationset.
A.2.3.LAPLACEAPPROXIMATION
TheLaplaceapproximation(Daxbergeretal.,2021)approximatesaGaussianposteriorq(θ |D)overthenetworkparameters
foraGaussianpriorp(θ)andlikelihooddefinedbythenetworkarchitecture. Itusesthemaximumaposteriori(MAP)
estimateasthemeanandtheinverseHessianofthelossevaluatedattheMAPasthecovariancematrix:

(cid:18) ∂2L(D;θ)(cid:12)
(cid:12)
(cid:33)−1
N θ MAP,
∂θ ∂θ
(cid:12)
(cid:12)
. (17)
i j θ
MAP
Thisisapost-hocmethodappliedtoapointestimatenetwork. FollowingtherecommendationofDaxbergeretal.(2021),
weemployalast-layerKFACLaplaceapproximationandfindthepriorvarianceusingevidenceoptimization.
A.2.4.DROPOUT
Dropout(Srivastavaetal.,2014)hasbeenshowntobeavariationalapproximationtoadeepGaussianprocess(Gal&
Ghahramani,2016). Dropoutintherealmofuncertaintyquantificationremainsactiveduringinference, andisusedto
sampleM logitsbyperformingM forwardpasses. Therefore,itdirectlysamplesfromq(f)withoutcharacterizingit.
14BenchmarkingUncertaintyDisentanglement
A.2.5.DEEPENSEMBLE
Deepensembles(Lakshminarayananetal.,2017)areapproximatemodeldistributionsthatgiverisetoamixtureofDirac
deltasinparameterspace: q(θ)= 1 (cid:80)M δ(θ−θ(i)). Predominantlyusedtoreducethevarianceinthepredictionsand
M i=1
improvemodelaccuracy,deepensemblescanalsobeusedasapproximatorstothetruedistributionp(θ)inducedbythe
randomnessoverdatasetsD :={(x ,y )|i∈{1,...,n},x ∈X,y ∈Y}inthegenerativeprocessp(x,y).
i i i i
We obtain a set of logits by performing a forward pass over all models. Similarly to dropout, deep ensembles do not
explicitlyparameterizethedistributionoverthepredictions,theyonlysamplefromit. Weensemblefivemodelstrainedwith
cross-entropy.
A.2.6.SHALLOWENSEMBLE
Shallowensembles(Leeetal.,2015)arelightweightapproximationsofdeepensembles. Theyuseasharedbackboneand
M outputheads(oftenreferredtoas“experts”). Withasingleforwardpass,oneobtainsM logitvectorsperinput.
A.2.7.DETERMINISTICBASELINE
Asabaseline,wealsobenchmarkadeterministicnetworktrainedwiththecross-entropylossthatcorrespondstoaDirac
deltainparameterspace: q(θ′)=δ(θ−θ′).
B.Aggregators
Inpracticalapplications,distributionalmethodsoutputadiscretesetofprobabilityvectors{f(m)(x)}M perinputx. This
m=1
setcanbeaggregatedinseveralwaystoconstructanuncertaintyestimateu(x).
TheBayesianModelAverage(BMA)isgivenbyf˜(x)= 1 (cid:80)M f(m)(x). WeconsidertwoaggregatorsfortheBMA:
M m=1
(cid:16) (cid:17)
• Itsentropy: u(x)=H f˜(x) .
• Oneminusitsmaximumprobabilityentry: u(x)=1−max f˜(x).
c∈{1,...,C} c
Thef¯(x)valueintheBregmandecomposition(Section2.2.2)averagesinthelogprobabilityspaceinsteadoftheprobability
(cid:16) (cid:17)
simplex: f¯(x)=softmax 1 (cid:80)M logf(m)(x) . SimilarlytotheBMA,wecantaketheentropyoff¯(x)oroneminus
M m=1
itsmaximumprobabilityentryasanuncertaintyestimate.
Onecanalsocalculatetheentropyoftheindividualprobabilityvectorsandthenaveragethem,leadingto
M
1 (cid:88) (cid:16) (cid:17)
u(x)= H f(m)(x) . (18)
M
m=1
Similarly,onecandeterminetheexpectedmaximumprobabilitiesandconstructtheestimatoras
M
1 (cid:88)
u(x)=1− max f(m)(x). (19)
M c∈{1,...,C} c
m=1
Finally,onecandirectlyusetheepistemiccomponentsoftheBregmanandITdecompositionsastheydonotrequirea
groundtruth. Inparticular,onecanuse
M
u(x)=H(cid:16) f˜(x)(cid:17)
−
1 (cid:88) H(cid:16) f(m)(x)(cid:17)
, f(m) ∼q(f)∀m∈{1,...,M}, (20)
M
m=1
the(discretized)epistemicpartoftheITdecomposition(seeAppendixC),or
u(x)=
1 (cid:88)M (cid:104)
D
(cid:16) f¯(x)(cid:13) (cid:13)f(m)(x)(cid:17)(cid:105)
, f(m) ∼q(f)∀m∈{1,...,M}, (21)
M KL (cid:13)
m=1
15BenchmarkingUncertaintyDisentanglement
the(discretized)epistemicpartoftheBregmandecomposition(seeAppendixD).
Unlessstatedotherwise,weusethebest-performingalternativeforeachdistributionalmethodinthebenchmarks. Forthese
methods,themodel’spredictionisalwaysthemostconfidentclassoftheBMA.Fordirectpredictionmethods,weusetheir
“canonical”uncertaintyestimatorintroducedinAppendixA.1.
C.SpecialFormontheInformation-TheoreticalDecompositionforDiscretePosteriors
Below,weshowthattheinformation-theoretical(IT)decomposition(Depewegetal.,2018)separatestheentropyofthe
BMAintoanexpectedentropytermandaJensen-Shannondivergencetermwhenconsideringdiscreteuniformdistributions
q(f(x)|x)= 1 (cid:80)M δ(f(x)−f(m)(x))abbreviatedasq(f)inthemainpaper.
M m=1
(cid:82)
The IT decomposition treats the entropy of the predictive distribution p(y | x) = p(y | f(x)) dq(f(x) | x) as the
predictiveuncertaintymetricanddecomposesitinto
H (y)=E (cid:2)H (y)(cid:3) +I (y;f(x)), (22)
p(y|x) q(f(x)|x) p(y|x,f) p(y,f(x)|x)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
predictive aleatoric epistemic
whereHistheentropyandIisthemutualinformation.
Underadiscreteuniformapproximateposteriorq(f),thepredictiveuncertaintyisstilltheentropyoftheBMAandthe
aleatoric uncertainty also stays the expected entropy of the probability vectors of non-zero measure. We only have to
showthatthemutualinformationtakestheconvenientformoftheJensen-Shannondivergenceundersuchanapproximate
posterior. Usingp(y,f(x)|x)=p(y |f(x))q(f(x)|x),wehave
(cid:88)C (cid:90) p(y,f(x)|x)
I (y;f(x))= log dp(y,f(x)|x) (23)
p(y,f(x)|x) p(y |x)q(f(x)|x)
y=1
1 (cid:88)M (cid:88)C (cid:16) (cid:17) p(cid:0) y |f(m)(x)(cid:1)
= p y |f(m)(x) log (24)
M p(y |x)
m=1y=1
M C M
1 (cid:88) (cid:16) (cid:17) (cid:88) 1 (cid:88) (cid:16) (cid:17)
=− H f(m)(x) − p y |f(m)(x) logp(y |x) (25)
M M
m=1 y=1 m=1
(cid:32) M (cid:33) M
1 (cid:88) 1 (cid:88) (cid:16) (cid:17)
=H f(m)(x) − H f(m)(x) (26)
M M
m=1 m=1
whichistheJensen-shannondivergenceofthedistributionsp(cid:0)
y
|f(m)(x)(cid:1)
,m∈{1,...,M}.
D.SpecialFormoftheBregmanDecompositionfortheKullback-LeiblerDivergence
Considering a Bregman divergence induced by the strictly convex function F as the loss function L, the Bregman
decompositiondisentanglesitinto
E [D [y∥f(x)]]=E [D [y∥f∗(x)]]+E (cid:2) D (cid:2) f¯(x)∥f(x)(cid:3)(cid:3) +D (cid:2) f∗(x)∥f¯(x)(cid:3) , (27)
q(f),pgt(y|x) F pgt(y|x) F q(f) F F
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
predictive aleatoric epistemic bias
wheref∗(x)=E [y]istheBayespredictorandf¯(x)=argmin E [D [z∥f(x)]]isthecentralpredictor.
pgt(y|x) z∈∆C−1 q(f) F
WhenchoosingF(·)=−H(·),weobtainD [·∥·]=D (·∥·). Considerthepredictiveuncertaintyterm. Notethatwe
F KL
nowtreatsamplesfrompgt(y |x)asone-hotvectors. Therefore,asample’sentropyiszero. Consequently,thepredictive
uncertaintybecomesE [CE(y,f(x))]. Thealeatorictermtakesaconvenientform:
q(f),pgt(y|x)
(cid:34) C (cid:35) C
E [D (y∥f∗(x))]=E (cid:88) y log y i =−(cid:88) f∗(x)logf∗(x)=H(f∗(x)). (28)
pgt(y|x) KL pgt(y|x) i f∗(x) i i i
i=1 i i=1
16BenchmarkingUncertaintyDisentanglement
On datasets with multiple labels per input, this quantity is precisely the entropy of the (normalized) label distribution
correspondingtothelabelervotes.
Tocalculatef¯(x),wecanproceedasfollows.
f¯(x)=argminE [D (z∥f(x))] (29)
q(f) KL
z∈∆C−1
C C
=argmin(cid:88)
z logz
−(cid:88)
z
log(cid:0) exp(cid:0)E
[logf
(x)](cid:1)(cid:1)
(30)
i i i q(f) i
z∈∆C−1
i=1 i=1
 
C C C C
=argmin(cid:88)
z ilogz
i−(cid:88)
z
ilog(cid:0) exp(cid:0)E
q(f)[logf
i(x)](cid:1)(cid:1) +(cid:88)
z
ilog(cid:88) exp(cid:0)E
q(f)[logf
j(x)](cid:1)
 (31)
z∈∆C−1
i=1 i=1 i=1 j=1
(cid:88)C (cid:88)C exp(cid:0)E q(f)[logf i(x)](cid:1)
=argmin z logz − z log (32)
z∈∆C−1
i=1
i i
i=1
i (cid:80)C j=1exp(cid:0)E
q(f)[logf
j(x)](cid:1)
(cid:124) (cid:123)(cid:122) (cid:125)
pi:=
=argminD (z∥p) (33)
KL
z∈∆C−1
=p. (34)
Therefore,f¯(x)=softmax(cid:0)E [logf(x)](cid:1)
,wherelogisappliedelementwise.
q(f)
D.1.DEUPisaSpecialCaseofBregman
AsmentionedinSection2.2.2ofthemainpaper,acloselyrelatedformulatoBregmanistheriskdecompositionofLahlou
etal.(2023)wherethepredictiveuncertaintyisdirectlyequatedtotheriskofadeterministicpredictorf: X →Y,notan
expectationofrisksoverdatasetsorhypothesisdistributions:
R(f,x)=R(f∗,x)+R(f,x)−R(f∗,x) (35)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
predictive aleatoric bias
whereR(f,x)=E [L(f(x),y)]isthepointwiseriskoff onx∈X. WhenchoosingLtobethesquaredEuclidean
p(y|x)
distanceortheKullback-Leiblerdivergence,Equation35becomesaspecialcaseofEquation2foraDiracdistribution
q(f′)=δ(f′−f)atanarbitrarypredictorf. Thisformulationisdesiredwhenonewantsthepredictiveuncertaintytobe
alignedwiththeriskofoneparticularpredictorandnottheexpectedriskoverahypothesisdistribution.
E.GoalsofDisentanglement
Whatdoesitmeantohavedisentangleduncertaintyestimators? Considertwoestimatorsu(a)(x ),u(e)(x )andground-truth
i i
aleatoricandepistemicuncertaintiesU(a)(x ),U(e)(x )foreachinputx . Theestimatorsu(a)andu(e)aredisentangledif
i i i
1. u(a)haslowrankcorrelationwithU(e)and
2. u(e)haslowrankcorrelationwithU(a).
Importantly, u(a) and u(e) having a severely high rank correlation prohibits disentanglement. Further, they are well-
performingif
3. u(a)hashighrankcorrelationwithU(a)and
4. u(e)hashighrankcorrelationwithU(e).
Inspiredbygeneralizedbias-variancedecompositions(Pfau,2013;Gruber&Buettner,2023),onemaytreatthetraining
datasetDasarandomvariablesampledfromthegenerativeprocessp(x,y)andrecordthevariabilityofthetrainedpredictor
underdatasetchange. FollowingtheBregmandecomposition,onemaythendefine
U(e)(x):=E (cid:2) D (cid:2) f¯(x)∥f (x)(cid:3)(cid:3) (36)
p(D) F D
17BenchmarkingUncertaintyDisentanglement
1.0 0.986 0.990 0.991 0.999
0.8
0.6
0.535
0.4 0.383
0.293
0.2
0.0
Figure11.FouroutofsevendistributionalmethodsexhibitanalmostperfectrankcorrelationbetweentheITaleatoricandepistemic
componentsontheCIFAR-10dataset.Thesemethodsviolateanecessaryconditionofuncertaintydisentanglement.
with the corresponding central predictor f¯(x) = argmin E [D [z∥f (x)]]. As this is impossible to obtain in
z p(D) F D
practicalsetups(oristoonoisytoMonteCarloestimate),weinsteadconsidertheproxytaskofOODdetectionforevaluating
thedisentanglementofaleatoricandepistemicuncertainties. Thisisnotneededfor,e.g.,theevaluationoftheBregmanbias
andaleatoriccomponents’disentanglement.
F.CIFAR-10Experiments
ThissectionmirrorsSection3ofthemainpaperontheCIFAR-10dataset. Wewanttounderstandthebehaviorofcurrent
uncertaintyquantificationmethods. Wefirstbenchmarkdisentanglementformulasandthenindividualestimators.
F.1.Disentanglementformulasoftenfail
Figure11revealsasurprisinglysimplefailureofdisentanglementformulas: Infourofthesevendistributionalmethods,the
ITdecompositionleadstoaleatoricandepistemicestimatesthatarehighlymutuallycorrelated(Rank. Corr≥0.99). This
violatesthefirsttwonecessaryconditionsfordisentanglement. Thecorrelationremainsevenwhenweaddmoreepistemic
uncertaintyintothedatasetinAppendixG.1orswitchtoBregmanestimatesinAppendixH.Threeposteriorestimators,
SNGP,GP,andshallowensembles(butnotdeepensembles)showalowerrankcorrelationbetweentheiraleatoricand
epistemiccomponents.Fromthis,itfollowsthatdisentanglementcannotbethoughtaboutonlyonthelevelofdecomposition
formulasbutneedstotaketheexplicitimplementationintoaccount.
Tofindoutwhethertheremainingthreeestimatorpairsarenotonlydisentangledbutalsowell-performing,wecompare
themtotheabove-definedaleatoricandepistemicgroundtruthsinAppendixG.2. Wefindthattheyperformbetterthan
random but worse than specialized deterministic estimators for both tasks. In other words, combining two specialized
estimatorsmaypracticallyperformbetterthanderivingthembydecomposingasingleposterior.
F.2.Correctnesspredictionandabstainedpredictionworkacrosstheboard
Figure12showsthatmostuncertaintyestimatorsperformwithin±0.015ofthebaselinewhenpredictingcorrectnessID,
andmodernmethodslikeHET-XLdonotoutperformoldermethodslikedeepensembles. Weseeasimilarsaturationwhen
slightlyalteringthecorrectnessmetrictoaccountforsoftlabelsinAppendixI.1.
Thesaturationisevenmorepronouncedontheabstainedpredictiontask. AlluncertaintymethodsapartfromMahalanobis
obtain an AUC score greater than 0.99 in Figure 13. Practically, this means that one can obtain a close-to-perfect
classificationaccuracybyabstainingfrompredictiononatinysetofsamples. Inbothtasks,themorecomputationally
expensivedistributionalmethodshaveaslightedgeoverdeterministicmethods.
WewouldliketohighlightthepoorpredictiveperformanceoftheMahalanobismethod. BeingaspecializedOODdetector,
italignswithourexpectationsthatitcannottellthecorrectnessofonlyin-distributionsamplesapart. DUQ,whichalso
modelsthedatadensity,alsofallsbehindthebaselineonthecorrectnesspredictiontask. Thissuggeststhatthenotionof
18
noitalerroCknaR
↓
PGNS
PG
.snEwollahS
tuoporD
.snEpeeD
ecalpaL LX-TEHBenchmarkingUncertaintyDisentanglement
1.0
0.949 0.941 0.940 0.940 0.934 0.933Di 0s .t 9r 3i 2but 0i .o 93n 0al
0.921
Deterministic
0.904
0.9 0.887
0.8
0.7
0.6
0.550
0.5
Figure12.OnIDCIFAR-10samples,theperformanceofmethodsonpredictingcorrectnessissaturated,withonlydeepensembles
achievingconsistentlybetterresults.TheMahalanobismethodisaspecializedOODdetectorthatcannotdistinguishIDsamples.
Distributional Deterministic
1.0 0.997 0.995 0.995 0.995 0.994 0.994 0.994 0.993 0.993 0.991 0.990
0.946
0.9
0.8
0.7
0.6
0.5
Figure13. OnIDCIFAR-10samples,mostmethodssolvetheabstinencetaskalmostperfectly.
predictiveuncertaintyimposedbythelatentdensityofDUQislessalignedwiththenotionofcorrectness.
F.3.Uncertaintiesare(only)asrobustasaccuracy
Anecessaryconditionforthereliabledeploymentofuncertaintyquantificationmethodsisthattheirestimatesshouldstay
performantlongerthanthemodel’spredictiveperformancewhengoingOOD.Onlythencanwetrustthemandbase,e.g.,
theabstinenceofthemodelontheirpredictions. Atfirstglance,whenplottingtherawperformancevaluesasdashedlines
inFigure10, thisisthecase. However, AUROCshavearandomperformancevalueof0.5andtherandompredictor’s
expectedaccuracyistheinverseofthenumberofclasses. NormalizingthesevaluesoutbyFORMULArevealsthatboth
correctnesspredictionandabstainedpredictiondegradeconsistentlywiththemodel’saccuracyasthedatabecomesmore
andmoreOOD(solidlines). ResultspermethodarereportedinAppendixI.2. Thisobservationshowsthatallbenchmarked
methodsareincapableofgeneralizingbetterthanthemodels,makingtheircorrectnesspredictionsnottrustworthyathigher
perturbationlevels.
F.4.SubtleOOD-nessishardtodetect
IfbothaccuracyanduncertaintyestimatesdeteriorateOOD,cancurrentmethodsreliablydetectOODsamples?
We use balanced mixtures of ID and OOD datasets. OOD samples are perturbed ID samples with severity level two
wherethemodels’accuracyalreadyseverelydeterioratesaccordingtoFigure10. Theuncertaintyestimatorsaretasked
topredictwhichsampleisOOD,i.e.,OODinputsshouldhavehigheruncertaintyestimates. AsshowninFigure14,the
Mahalanobismethod,whichintheprevioustaskswasfaroff,isbyfarthemostperformantonaverageintellingapartclean
19
CORUA
CUAecnenitsbA
↑
↑
.snEpeeD
.snEpeeD
PGNS
tuoporD
PG
PGNS
tuoporD
PG
enilesaB
.snEwollahS
ecalpaL
LX-TEH
LX-TEH
enilesaB
.snEwollahS
ecalpaL
.derP.rroC
.derP.rroC
.derPksiR
.derPksiR
QUD
QUD
sibonalahaM
sibonalahaMBenchmarkingUncertaintyDisentanglement
1.0
Distributional
Deterministic
0.9
0.8
0.735
0.7 0.703 0.694 0.692 0.692 0.686 0.686 0.686 0.681 0.675 0.669
0.613
0.6
0.5
Figure14.OnlydeepensemblesandMahalanobis,adirectOODdetector,candistinguishIDandOODsamplesconsiderablybetterthan
thebaselineonCIFAR-10.OODsamplesareperturbedbyCIFAR-10Ccorruptionsofseverityleveltwo.
1.0
Distributional
Deterministic
0.8
0.6
0.4 0.401 0.393 0.380 0.377 0.375 0.369 0.361 0.360
0.300 0.286
0.218
0.2
0.016
0.0
Figure15.OnCIFAR-10,noneofthemethodshaveaconsiderablyhigherrankcorrelationwiththeground-truthaleatoricuncertainty
thanthebaseline.
CIFAR-10samplesfromperturbedones. ThisshowsthattheOODtaskindeedbenchmarksanotablydifferentuncertainty
quantificationcapabilitythanthepreviouspredictiveuncertaintytasks,namelyepistemicuncertainty. Wefurtherinvestigate
thecorrelationsbetweenthetasksinAppendixF.6.
Onemayexpectthatlatent-density-basedmethodsarenaturallygoodatpredictingOOD-nesssince,forawell-behaved
latentdensity,theembeddingsofOODinputswillliefarawayfromtheclasscentroids. Interestingly,theworst-performing
methodalsobuildsalatentdensityestimate: wecannotestablishclearcategoriesofmethodsthatworkandthatdonot. We
hypothesizethatthemainpoweroftheMahalanobismethodliesinitsadversarialperturbationoftheinputsandthatadding
thistoDUQwouldmakeitasgoodasMahalanobis. Weleavethisexperimenttofuturework.
TheuncertaintymethodsshowasteadyincreaseinOODdetectionperformanceasweincreasetheseverityoftheperturbed
halfofthedataset(seeAppendixI.3). TheadvantageoftheMahalanobismethodvanishesastheseverityincreases. At
severity5,mostmethodsbecomesaturated.
Onemayexpectthattherisk/correctnesspredictionmethods’OODdetectionperformanceisworsethandistributional
methods as they are trained only on ID samples and cannot utilize the disagreement among models to modulate their
predictionvectors. Surprisingly,thisisnotthecase. Toresolvethisparadox,wenotethatitdoesnotmatterwhatthese
methods’ exact behavior on OOD inputs is. As long as it is sufficiently different from their ID behavior, they can be
employedtodetectOOD-ness.
20
CORUA
noitalerroCknaR
↑
↑
sibonalahaM
.derP.rroC
.snEpeeD
.snEpeeD
PG
tuoporD
PGNS
PG
.derPksiR
.derPksiR
enilesaB
PGNS
ecalpaL
enilesaB
.derP.rroC
ecalpaL
.snEwollahS
.snEwollahS
LX-TEH
LX-TEH
tuoporD
QUD
QUD
sibonalahaMBenchmarkingUncertaintyDisentanglement
Methodranking
DeepEnsemble 1 1 1 3 1 2 2
Dropout 4 2 2 4 2 3 11
Baseline 5 7 7 7 9 7 6
SNGP 2 3 4 1 5 6 4
GP 3 4 8 2 7 4 3
Mahalanobis 12 12 - - - 12 1
ShallowEnsemble 8 5 6 8 6 9 9
Laplace 6 8 3 5 8 8 7
HET-XL 7 6 5 9 4 10 10
CorrectnessPred. 9 9 10 10 10 1 8
LossPrediction 10 10 - - - 5 5
DUQ 11 11 9 3 6 11 12
Table3.Differenttaskshavedifferentbest-performingmethodsonCIFAR-10.Deepensembleisagood(butexpensive,seeAppendixK.3)
choiceacrosstheboard.Best,second-best,andthird-bestmethodhighlightedingold,silver,andbronze.Notethatdifferencesbetween
rankscanbeverysmall,seetheplotspertaskfordetails.
F.5.Aleatoricuncertaintyaloneishardtoquantify
Thepreviousexperimentisolatedtheepistemiccapabilitiesofuncertaintyestimates. Letusnowbenchmarkhowmuch
theypredictaleatoricuncertainties. Sinceweusetheentropyofhumanannotatorlabeldistributionsasgroundtruths,this
could also be considered the alignment with human uncertainties. Figure 15 shows that most methods perform within
theerrorbarofthebaseline. Correctnesspredictionismostalignedonaveragewithanotablysmallmin-maxerrorbar.
ThisisreasonablesinceID,thealeatoricuncertaintyofthesampledeterminesthenetwork’scorrectnessthemost.1 The
latentdensitymethodsDUQandMahalanobisareparticularoutliers. Eventhoughtheyareintendedtocapturealeatoric
uncertaintybyplacingaleatoricallyuncertainsamplesinbetweenclasscentroids,resultinginalowdensity(VanAmersfoort
etal.,2020),theyperformworsethantheremainingmethods. Mahalanobisevenperformsrandomly. Thisreinforcesthatit
isanepistemic,notanaleatoricuncertaintyestimator. WeobtainsimilarresultsfortherankcorrelationwiththeBregman
biascomponent: nomethodcansignificantlysurpassthebaseline(seeAppendixH.2). Wealsoinvestigatethecorrelationof
methodswithdifferentsourcesofuncertaintyonOODdatasetsinAppendixH.2.
F.6.Differenttasksrequiredifferentestimators
Intheprevioussections,wehavehintedatthefactthattheperformanceacrossmethodsisverysimilaronsometasksand
dissimilaronothers. Inthissection,weinvestigatethecorrelationamongthepreviouspracticaltasksusingacorrelation
matrixdisplayedinFigure16.Toconstructthematrix,weconsiderallbenchmarkedmethodswithalluncertaintyaggregators
(seeAppendixB)andcalculatethecorrelationoftheirrankingsondifferentmetrics.Wefindthatmethodsgoodinpredicting
correctnessaregoodinabstainingfrompredictionandviceversa(rankcorr. 0.894),andbothtasksarealsocorrelatedwith
thelog-likelihood. Thesemethodsformaclusterthatcapturespredictiveuncertaintycapabilities. Thelog-likelihood,a
properscoringrule,isalsohighlycorrelatedwiththeBrierscore,anotherproperscoringrule. Bothmetricsevaluateboththe
uncertaintyestimatesandthecorrectnessofpredictioninone,whichalsoexplainswhytheBrierscoreishighlycorrelated
withaccuracy. Aleatoricuncertainty,OODdetection,andcalibration(viaECE)formclustersontheirown,underliningthat
theybenchmarkdifferentformsofuncertainty.
Astherearedifferentgroupsoftasks,thereisnoone-fits-alluncertaintyestimator. Table3demonstratesthisbyrankingall
methodsonalltasks. Anuncertaintyestimatorhastobechosenordevelopedforthespecifictaskapractitionerisinterested
in. Ifthetaskisunknown,deepensemblesofferagoodcompromise,buteventhebaselineisagoodstartingpointifthe
runtimecostsofdeepensemblesaretoohigh.
1Notethatwetrainwithonlyonelabelperinput.
21
ssentcerroC ecnenitsbA
.borPgoL
reirB ECE
cirotaelA
DOOBenchmarkingUncertaintyDisentanglement
1
LogProb.
Brier 0.73
0.5
-ECE(*) 0.5910.533
Correctness 0.7530.484 0.21
0
Abstinence 0.804 0.59 0.1840.894
Accuracy 0.363 0.8 0.3130.1120.395
0.5
Aleatoric 0.266-0.054-0.0770.6020.467-0.285 −
OOD(*) 0.101-0.197-0.3010.4560.384-0.1830.72
1
−
LogProb. Brie -r ECE C( o* r) rectne As bs stinence Accuracy Aleatoric OOD(*)
Figure16.TherankcorrelationofmetricsonCIFAR-10isnotablydifferentfromthatofImageNet(Figure7).Rankcorrelationofmetric
pairscalculatedoverall(method,aggregator)pairs.(*)OOD’sandECE’scorrelationwiththeothermethodsissensitivetothechoiceof
aggregator,seeAppendixJ.
0.10
GP SNGP
≈
DeepEnsemble
0.08 Dropout
Baseline
0.06
0.04
0.02
0.00
ID OODSeverity1
Figure17.WhilemostmethodspreservetheirrankingsasthedatasetbecomesmoreandmoreOODviaCIFAR-10Ccorruptions,SNGP
variantsbreakdowntothebaselinelevelalreadyforseverityleveloneontheECEmetric.
F.7.Performancedependsonimplementationdetails
Weconcludewithanexamplewheredefiningthespecifictaskisofparticularimportance. SNGPshowsahighlydifferent
performancedependingonthedatasetandtaskitistrainedfor. Figure17showsthatSNGP(andGP)providethebest-
calibrated uncertainties ID, but already at the lowest OOD perturbation level drop to the baseline level, with the ECE
jumpingfrom0.005to0.084. InAppendixI.4, weshowthatthebestwaytoaggregateSNGP’sposteriorq(f)intoan
uncertaintyscoreu(x)isdifferentwhenoptimizingfortheECEversusforcorrectnessprediction. Thisisnotabug;we
implementedSNGPthreetimesfromscratchwiththesameresults. Itrathergoestoshowthatsubtledesignchoicesgreatly
affectperformance. Hence,weencourage,asbestpractice,tofirstdefinetheuncertaintyquantificationtaskathandandthen
developaspecializeduncertaintyestimator.
G.FurtherResultsontheInformation-TheoreticalDecomposition
G.1.OODGeneralizationPerformance
TheIDcorrelationoftheITdecomposition’scomponentsusingdifferentdistributionalmethodsisdiscussedinAppendixF.1
andSection3.1ofthemainpaper. Inthissection,wefocusonhowthiscorrelationchangesaswegomoreandmoreOOD.
22
ECE
↓BenchmarkingUncertaintyDisentanglement
G.1.1.IMAGENET
Figure18showsthegeneralizationperformanceofbenchmarkeddistributionalmethodsusingtheITdecompositionat
severitylevelstwoandfive. OnlySNGPandGPshowareasonablylowrankcorrelationbetweentheITaleatoricand
epistemiccomponentsacrossallseverities. ThesesourcesgenerallybecomelesscorrelatedaswegomoreOOD.Balanced
mixturesofOODsamplesleadtohighercorrelations,buttherankingofmethodsremainsunchanged.
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 0.2
0.0 0.0
(a)OODseverityleveltwo. (b)OODseveritylevelfive.
Figure18.LaplacehasuncorrelatedepistemicandaleatoricITestimatesacrossdifferentOODlevelsontheImageNetvalidationset.
Othermethodsonlyachievealowrankcorrelationatthehighestseveritylevels.OODrankcorrelationresultsoftheITdecomposition
usingdifferentdistributionalmethods.
G.1.2.CIFAR-10
ResultsforseveritylevelstwoandfiveareshowninFigure19. OnlySNGPandGPshowareasonablylowrankcorrelation
betweentheITaleatoricandepistemiccomponentsacrossallseverities. Thesesourcesbecomeslightlylesscorrelatedaswe
gomoreOOD–muchlesssothanonImageNet(Figure18). BalancedmixturesofOODsamplesleadtohighercorrelations,
buttherankingofmethodsremainsunchanged.
1.0 0.958 0.981 0.989 0.999 1.0 0.946 0.998
0.906 0.914
0.8 0.812 0.8 0.800
0.6 0.6
0.450 0.469
0.4 0.4
0.315 0.319
0.2 0.2
0.0 0.0
(a)OODseverityleveltwo. (b)OODseveritylevelfive.
Figure19.OnCIFAR-10,SNGPsshowastablylowrankcorrelationbetweentheBregmanepistemicandaleatoriccomponentsacross
differentOODlevels. OODrankcorrelationresultsoftheITdecomposition’sepistemicandaleatoriccomponentsusingdifferent
distributionalmethods.
G.2.PerformanceofDecorrelatedMethodsusingtheInformation-TheoreticalComponents
G.2.1.IMAGENET
InSection3.1ofthemainpaper,weshowthatusingLaplace,thealeatoricandepistemiccomponentsoftheITdecomposition
canbecomeuncorrelated. Inthissection,weshowthattheydonotperformwellonthetaskstheyaremadefor. Figure20
23
noitalerroCknaR
noitalerroCknaR
↓
↓
ecalpaL
PGNS
tuoporD
PG
.snEwollahS
.snEwollahS
PGNS
.snEpeeD
PG
tuoporD
.snEpeeD
ecalpaL
LX-TEH
LX-TEH
noitalerroCknaR
noitalerroCknaR
↓
↓
ecalpaL
PGNS
tuoporD
PG
PGNS
.snEpeeD
PG
tuoporD
.snEwollahS
.snEwollahS
.snEpeeD
ecalpaL
LX-TEH
LX-TEHBenchmarkingUncertaintyDisentanglement
shows that Laplace does not match the baseline with its best estimator when Laplace uses the estimators of the IT
decomposition—infact,itevenperformsrandomlyontheOODdetectiontask. Thislimitationprohibitsitspracticaluse,
andwecannotbenefitfromthelesscorrelatedcomponents.
1.0 1.0
Distributional Distributional
Deterministic Deterministic
0.8 0.9
0.6 0.8
0.535 0.526 0.516 0.502 0.499 0.491 0.487 0.481 0.468 0.459 0.768 0.752 0.728
0.4 0.7 0.708 0.707 0.700 0.695 0.694
0.674 0.669
0.2 0.6
0.108
0.503
0.0 0.5
(a)Rankcorrelationwiththeground-truthhumanuncertain- (b)AUROCoftheOODdetectiontaskusingseveritylevel
ties. two.
Figure20.OnImageNet,LaplacecannotmatchthebaselinewhenusingtheestimatorsoftheITdecomposition.Allothermethodsare
equippedwiththeirbest-performingestimatorfortherespectivetasks,showingthatspecializedestimatorsworkbetter.
G.2.2.CIFAR-10
InAppendixF.1,wediscussthatwhenusingSNGPvariantsorshallowensemblesonCIFAR-10,thealeatoricandepistemic
componentsoftheITdecompositioncanbecomequiteuncorrelated. Inthissection,weshowthatwhilethesecomponents
mightbeuncorrelated,theyarenotperformantonthetaskstheyaremadefor. Figure21showsthatboththeSNGPvariants
and the shallow ensemble method underperform the baseline with its best estimator when the SNGPs and the shallow
ensembleusetheestimatorsoftheITdecomposition.
1.0 1.0
Distributional Distributional
Deterministic Deterministic
0.8 0.9
0.6 0.8
0.735
0.4 0.401 0.393 0.380 0.375 0.361 0.360 0.7 0.703 0.692 0.686 0.686 0.686 0.675 0.669 0.669
0.308 0.299 0.296 0.286 0.654
0.218 0.617 0.613 0.2 0.6
0.016
0.0 0.5
(a) Rank correlation between methods and the Bregman (b)AUROCofOODdetectionperformanceofmethodsusing
aleatoriccomponent. perturbationsofseverityleveltwo.
Figure21.SNGP,GP,andshallowensembleunderperformthebaselineonCIFAR-10whenusingtheestimatorsoftheITdecomposition.
Allothermethodsareequippedwiththeirbest-performingestimatorfortherespectivetasks,showingthattheITdecompositionisnot
practicallybeneficial.
H.FurtherResultsontheBregmanDecomposition
H.1.CorrelationofComponentsandLimitations
LetuscarryoutthesameexperimentsforBregmanaswedidfortheITdecompositioninAppendixF.1. AstheBregman
andriskdecompositions(Equations2and35)considertheground-truthlabeldistributionasthealeatoriccomponent,we
24
noitalerroCknaR
noitalerroCknaR
↑
↑
.snEpeeD
.derP.rroC
tuoporD
elbmesnEpeeD
tuoporD
enilesaB
noitciderPksiR
LX-TEH
enilesaB
.snEwollahS
ecalpaL
PG
PG
PGNS
.snEwollahS
.derP.rroC
PGNS
ecalpaL
LX-TEH
.derPksiR
QUD
sibonalahaM
sibonalahaM
CORUA
CORUA
↑
↑
sibonalahaM
sibonalahaM
tuoporD
elbmesnEpeeD noitciderPksiR
.snEwollahS
enilesaB
PGNS
ecalpaL
PG
.derP.rroC
.snEpeeD
LX-TEH
.derP.rroC
PG
.derPksiR
tuoporD
enilesaB
PGNS
LX-TEH
.EwollahS
ecalpaL
QUDBenchmarkingUncertaintyDisentanglement
usetheITaleatoricuncertaintyasanestimatorofit.
H.1.1.IMAGENET
Figure22showsthatthereisaconsiderablerankcorrelationbetweentheBregmanground-truthaleatoricandbiascomponents
butisnotsevereenoughsuchthatitpreventsthetheoreticalpossibilityofdisentanglingthemviaestimators. Ontheright,
wecanalsoseethattheITandBregmancorrelationresultsareverysimilar.
1.0 1.0
0.916 0.937 0.944 0.945 0.945
0.823
0.8 0.8
0.671 0.676
0.6 0.598 0.607 0.612 0.617 0.619 0.6
0.4 0.4
0.2 0.2
0.0 0.0 0.003
Figure22.Left. On ImageNet-ReaL, the rank correlation of the Bregman aleatoric and bias terms is between 0.6 and 0.7 for all
distributionalmethodswebenchmark.Right.TheBregmandecompositionshowssimilarrankcorrelationresultstotheITdecomposition
betweentheestimatedaleatoricuncertaintyandtheepistemiccomponentontheImageNetvalidationdataset.
H.1.2.CIFAR-10
WeseeinFigure23thattheresultsusingBregmanarevirtuallythesame: nodistributionalmethodcanprovidedecorrelated
uncertaintyestimatesthatalignwiththeirgroundtruth. Giventhehighground-truthrankcorrelationofthealeatoricand
biascomponentsshowninFigure25,therealsoseemstobeafundamentallimitationindisentanglingthem. Notethatnone
ofthebenchmarkeddistributionalmethodstrulyseparatethealeatoricandbiasterms.
1.0 1.0 0.987 0.993 0.996 0.999
0.847 0.863 0.870 0.872 0.875 0.879 0.879
0.8 0.8
0.6 0.6
0.472
0.4 0.4
0.336
0.2 0.2 0.208
0.0 0.0
Figure23.Left.TherankcorrelationoftheBregmanaleatoricandbiastermsisabove0.84foralldistributionalmethodswebenchmark
onCIFAR-10.Right.OnCIFAR-10,theBregmandecompositionshowssimilarrankcorrelationresultstotheITdecompositionbetween
theestimatedaleatoricuncertaintyandtheepistemiccomponent.
H.2.AlignmentofMethodswiththeBregmanBias
H.2.1.IMAGENET
TherankcorrelationofbenchmarkedmethodswiththebiascomponentoftheBregmandecompositionisshowninFigure24
for ID and OOD with severity two. Only dropout and deep ensembles are notably more correlated with the Bregman
biascomponentthanthebaseline, butmostmethodsexhibitahighrankcorrelation(≥ 0.8)(unlikeCIFAR-10below).
25
noitalerroCknaR
noitalerroCknaR
↓
↓
tuoporD
ecalpaL
ecalpaL
tuoporD
PGNS
PG
PG
PGNS
.snEpeeD
.snEwollahS
.snEwollahS
.snEpeeD
LX-TEH
LX-TEH
noitalerroCknaR
noitalerroCknaR
↓
↓
ecalpaL
PGNS
.snEwollahS
PG
tuoporD
.snEwollahS
LX-TEH
tuoporD
PGNS
.snEpeeD
.snEpeeD
ecalpaL
PG
LX-TEHBenchmarkingUncertaintyDisentanglement
ThissuggeststhatuncertaintyestimatorsaremostalignedwiththebiascomponentofBregmanoutofthethree.
ConsideringanOODdatasetwithseverity-twoperturbations,allmethodsbecomelesscorrelatedwithbias,unlikeCIFAR-10
below.
1.0 1.0
Distributional Deterministic Distributional Deterministic
0.8 0.886 0.866 0.854 0.841 0.840 0.830 0.815 0.806 0.783 0.743 0.8 0.836 0.833 0.813 0.789 0.762 0.741 0.739
0.691
0.659 0.633
0.6 0.6
0.4 0.4
0.2 0.2
0.068 0.048
0.0 0.0
(a)IDrankcorrelationofmethodswiththeBregmandecom- (b)OODrankcorrelationofmethodswiththeBregmanbias
position’sbiascomponent. usingseverity-twoperturbations.
Figure24.OnlydropoutanddeepensemblesarenotablymorecorrelatedwiththeBregmanbiascomponentthanthebaselineonImageNet.
Mostmethodsexhibitahighrankcorrelation(≥0.8).WhengoingmoreOOD,allmethodsbecomelesscorrelatedwithbias.
H.2.2.CIFAR-10
TherankcorrelationofbenchmarkedmethodswiththebiascomponentoftheBregmandecompositionisshowninFigure25
forIDandOODwithseveritytwo. NoneofthebenchmarkedmethodsarenotablymorecorrelatedwiththeBregmanbias
componentthanthebaseline. ConsideringanOODdatasetwithseverity-twoperturbations,allmethodsbecomebetter
correlatedwithbias.
1.0 1.0
Distributional Distributional
Deterministic Deterministic
0.8 0.8
0.6 0.6 0.591 0.580 0.577 0.575 0.573 0.569 0.566 0.556 0.539 0.531
0.461 0.455 0.455 0.453 0.453 0.449 0.449 0.448 0.471
0.4 0.394 0.375 0.4
0.238 0.207 0.2 0.2
0.035
0.0 0.0
(a)IDrankcorrelationofmethodswiththeBregmandecom- (b)OODrankcorrelationofmethodswiththeBregmanbias
position’sbiascomponent. usingseverity-twoperturbations.
Figure25.NoneofthebenchmarkedmethodsaresignificantlymorecorrelatedwiththeBregmanbiascomponentthanthebaselineon
CIFAR-10.WhengoingmoreOOD,allmethodsbecomebettercorrelatedwithbias.
I.FurtherPracticalResults
I.1.CorrectnessPrediction
I.1.1.IMAGENET
WeshowthecorrectnesspredictionperformanceofmethodsonOODandmixedID+OODdatasetsinFigure26. OOD,
nomethodhasanedgeoverthebaseline. Weobserveaconsistentbutnotsignificantdegradationofperformanceacross
26
noitalerroCknaR
noitalerroCknaR
↑
↑
tuoporD
PG
.snEpeeD
.snEpeeD
PGNS
enilesaB
ecalpaL
PGNS
tuoporD
PG
.derPksiR
ecalpaL
.derP.rroC
.derPssoL
enilesaB
.snEwollahS
.snEwollahS
LX-TEH
LX-TEH
.derP.rroC
QUD
sibonalahaM
sibonalahaM
noitalerroCknaR
noitalerroCknaR
↑
↑
.snEpeeD
ecalpaL
tuoporD
enilesaB
.snEpeeD
enilesaB
PGNS
ecalpaL
PG
.derPssoL
.derPksiR
PGNS
tuoporD
PG
.snEwollahS
LX-TEH
.derP.rroC
.snEwollahS
LX-TEH
.derP.rroC
QUD
sibonalahaM
sibonalahaMBenchmarkingUncertaintyDisentanglement
methodsonbothdatasettypes. Interestingly,theperformanceofMahalanobisdoesnotincreaseonmixeddatasets,even
thoughmodelsperformworseonOODimagesthanonIDones,anditisasuitableOODdetector.
1.0 1.0
Distributional Deterministic Distributional Deterministic
0.9 0.9
0.879 0.878 0.877 0.876 0.875 0.875 0.871 0.867 0.866 0.869 0.867 0.863 0.861 0.860 0.859 0.859 0.858 0.857
0.822
0.8 0.8 0.794
0.7 0.7
0.6 0.6
0.515 0.526
0.5 0.5
(a)MixedIDandOODseverityleveltwo. (b)OODseverityleveltwo.
1.0 1.0
Distributional Deterministic Distributional Deterministic
0.9 0.898 0.895 0.895 0.893 0.892 0.892 0.891 0.887 0.886 0.9
0.866 0.864 0.861 0.860 0.859 0.857
0.844 0.844 0.843 0.841
0.8 0.8 0.780
0.7 0.7
0.607
0.6 0.6
0.526
0.5 0.5
(c)MixedIDandOODseveritylevelfour. (d)OODseveritylevelfour.
Figure26.OnImageNet,thecorrectnesspredictionperformanceofthemethodsconsistentlydropsbothoncompletelyOODdatasets
(toprightandbottomright)andonbalancedmixturesofIDandOODdatasets(topleftandbottomleft)asweincreasetheseverity.The
performanceofMahalanobisincreasesonmixeddatasets,asmodelsperformworseonOODimagesthanonIDones,anditisasuitable
OODdetector.
Aswehaveaccesstovalidationsetswithmultiplelabelsperinput,thenotionofa“correct”predictionbecomesunclear. In
themainpaperandthepreviousplots,wefocusonthecanonicalnotionofcorrectness: whetherthemodelpredictsthemost
likelyclass. Arelatednotionofcorrectnessissoftcorrectness: here,themodeldoesnotreceiveabinaryrewardforits
predictionbutratheracontinuousnumberc∈[0,1]: thisgivestheground-truthprobabilityofthepredictedclassbeingthe
correctone. WecancalculateasimilarAUROCcorrectnessscoreasbefore,butastheAUROCrequiresbinarylabels,one
hastounrollthecontinuouscorrectnessvalue: e.g.,ifc=0.8foraninputx,onecanrepresentthiswith8correctand2
incorrectpredictionsonthesameinputx. TheresultingAUROChasatheoreticallimitstrictlylessthanonewhenc<1.
ThecorrespondingresultsareshowninFigure27thatfollowsthesamestructureasFigure26. Thesaturationofmethodsis
unchanged;however,theorderingisaffected.
I.1.2.CIFAR-10
WeshowthecorrectnesspredictionperformanceofmethodsonOODandmixedID+OODdatasetsinFigure28.Weobserve
aconsistentdegradationofperformanceacrossmethodsonbothdatasettypes. TheonlyexceptionistheMahalanobis
method: here, we observe an increase in performance. For the mixed datasets, it can be explained by the fact that the
modelsperformworseonOODimagesthanonIDones,makingtheMahalanobismethodasuitableestimatorofcorrectness.
However,theresultisunexpectedforsolelyOODsamples.
Aswehaveaccesstovalidationsetswithmultiplelabelsperinput,thenotionofa“correct”predictionbecomesunclear. In
themainpaperandthepreviousplots,wefocusonthecanonicalnotionofcorrectness: whetherthemodelpredictsthemost
likelyclass. Arelatednotionofcorrectnessissoftcorrectness: here,themodeldoesnotreceiveabinaryrewardforits
predictionbutratheracontinuousnumberc∈[0,1]: thisgivestheground-truthprobabilityofthepredictedclassbeingthe
27
CORUA
CORUA
↑
↑
.snEpeeD
tuoporD
.snEwollahS
enilesaB
tuoporD
.snEwollahS
enilesaB
PGNS
PGNS
.snEpeeD
PG
PG
ecalpaL
ecalpaL
LX-TEH
.derPssoL
.derPssoL
LX-TEH
.derP.rroC
.derP.rroC
sibonalahaM
sibonalahaM
CORUA
CORUA
↑
↑
.snEpeeD
enilesaB
enilesaB
ecalpaL
ecalpaL
LX-TEH
LX-TEH
.snEpeeD
.derPssoL
tuoporD
tuoporD
.derPssoL
.snEwollahS
PGNS
PGNS
PG
PG
.snEwollahS
.derP.rroC
.derP.rroC
sibonalahaM
sibonalahaMBenchmarkingUncertaintyDisentanglement
correctone. WecancalculateasimilarAUROCcorrectnessscoreasbefore,butastheAUROCrequiresbinarylabels,one
hastounrollthecontinuouscorrectnessvalue: e.g.,ifc=0.8foraninputx,onecanrepresentthiswith8correctand2
incorrectpredictionsonthesameinputx. TheresultingAUROChasatheoreticallimitstrictlylessthanonewhenc<1.
ThecorrespondingresultsareshowninFigure29thatfollowsthesamestructureasFigure28. Thesaturationofmethodsis
unchanged;however,theorderingisaffected.
I.2.PerformanceTendencyforIncreasingSeverity
InSection3.5ofthemainpaperandAppendixF.3,weshowtheperformanceofdropoutwhengoingOOD,claimingthatit
isprototypicalforothermethods. Figure30andFigure31showforCIFAR-10andImageNet,respectively,thatdropoutis
notanoutlierandothermethodsshowverysimilargeneralizationcapabilities.
I.3.OODDetection
I.3.1.IMAGENET
InAppendixF.4,wehintatthefactthatnearlyallmethodsshowasteadyincreaseinOODdetectionperformanceaswe
increasetheseverityoftheperturbedhalfofthedataset. Figure32showshowtheperformanceofeachmethodchangesas
weincreasetheseveritylevel. WecanseeasteadyincreaseinOODdetectionperformanceforallmethods. However,the
specializedOODdetector,Mahalanobis,benefitslessthantheothermethods. Inparticular,atseveritylevelthree,dropout
becomesbestonaverage,andshallowensemblesalsoovertaketheMahalanobismethod. Atseveritylevelfive,Mahalanobis
becomestheworstOODdetectoroutofthebenchmarkedmethods. ThismaybebecauseMahalanobiswastrainedtodetect
samplesatseverityleveltwoandcannotgeneralizeaswelltohigherseveritylevelsastheothermethods.
I.3.2.CIFAR-10
Figure33showshowtheOODdetectionperformanceofeachmethodchangesasweincreasetheseveritylevel. Wecansee
asteadyincreaseinOODdetectionperformanceforallmethods. However,thespecializedOODdetector,Mahalanobis,
benefitslessthantheothermethods. Inparticular,atseveritylevelfour,deepensemblesbecomebestonaverage,andSNGP
variantsalsoovertaketheMahalanobismethodatseveritylevelfive.
I.4.SensitivitytotheChoiceofAggregator
Thissectiondemonstratesthatthechoiceofaggregatorisofcrucialimportanceforspecificmethodsandtasks,showing
resultsontheCIFAR-10dataset. Sometasks,suchascorrectnesspredictionandabstainedpredictiontasks(ID),havehighly
correlatedperformancesacrossmethods. Thisisanintuitiveresult,asbothtasksfundamentallyrequiretellingapartcorrect
andincorrectsamples. However,thisisnotalwaysthecase. OODdetectionandtheECEscorearetwosuchtasks/metrics.
Considering that the ECE is closely connected to correctness but on a much more fine-grained scale than the binary
correctnesspredictiontask,onewouldexpectthatahighECEscoretranslatesovertoahighcorrectnessAUROCscore.
Surprisingly,thisisoftennotthecasefordistributionalmethods,asshowninFigure34.Importantly,thechoiceofaggregator
(seeAppendixB)perdistributionalmethodhasahighinfluenceonpredictivepowerandthecorrelationofperformances:
SNGPvariants,Laplace,deepensemble,andHET-XLallhavedifferentoptimalaggregatorsforcorrectnessandECE,and
dependingonwhichtaskweoptimizetheestimatorfor,wecanobtainbothpositiveandnegativerankcorrelationamong
thetaskperformances. SNGPvariantsareparticularoutliers: theyhavethelargesttrade-offbetweenthebestpossible
performancesinthetwotasks.
I.5.ECEOODGeneralization
InFigure17oftheappendix,weshowthatSNGPvariantsarethemostcalibratedIDontheCIFAR-10dataset,buttheyalso
breakdowntothebaselinelevelfromseveritylevelone. Thefourmethodsthatbringconsiderableimprovementscompared
tothebaselineIDareSNGP,GP,deepensemble,anddropout,intheorderofincreasingECE.Figure35showsthataswe
increasetheseveritylevel,onlydeepensembleanddropoutarecapableofperformingconsiderablybetter(≥ .05ECE
improvement)thanthebaselineonaverage.
28BenchmarkingUncertaintyDisentanglement
I.6.LogProbabilityProperScoringRule
InFigure36,wepresentthemethods’resultsonthelogprobabilityproperscoringruleconsideringtheCIFAR-10dataset.
Wefindthatdeepensemble,dropout,andLaplacearetheonlymethodsthatconsistentlyoutperformthebaselineonaverage,
bothIDandOODforallseveritylevels. Still,theirperformanceadvantageiswithintheerrorbarsofthebaseline.
J.SensitivityofCorrelationMatrix
Figure37showsrankcorrelationresultsacrossmetricsusingdifferentestimatorsontheImageNetdataset. Notably,as
foreshadowedinthecaptionofFigure7,theOODandECEmetricsexhibitdifferentrankcorrelationscoresdependingon
theestimatorwechoose.
K.TrainingandImplementationDetails
Forbothdatasets,wetrainandevaluateonanNVIDIAGeForceRTX2080Ti. WeonlyuseanNVIDIAA100TensorCore
GPUfortheconstructionoftheLaplaceapproximationonImageNet,owingtotheVRAMrequirementsofthismethod.
K.1.CIFAR-10
ForCIFAR-10,wefollowtheaugmentationsandtrainingschedulesoftheuncertainty baselinesGitHubrepository. In
particular,wetrainaWideResNet28-10(Zagoruyko&Komodakis,2016)for200epochswithastepdecayscheduleat
[60,120,160]epochswithdecayrate0.2. Weusestochasticgradientdescentwithmomentum0.9andabatchsizeof128.
Ourtrainingaugmentationcomprisesarandomcropusingpadding2andarandomflipontheverticalaxiswithprobability
0.5. ThelearningrateandweightdecayhyperparametersarechosenbytheBayesianoptimizationschemeofWeightsand
Biases(Biewald,2020). Theadditionalhyperparametersofbenchmarkedmethodsaredeterminedbyeitherusingvalues
suggestedbytheoriginalauthorsorincludingtheseinthehyperparametersweep.
K.2.ImageNet
OnImageNet,wefine-tuneapretrainedResNet50(Heetal.,2016)usingtheresnet50.a1 in1kparametersfromthe
timmlibraryasinitialization. Wefine-tunefor50epochsfollowingacosinelearningrateschedule(Loshchilov&Hutter,
2017)usingtheAdamWoptimizer(Loshchilov&Hutter,2019)andalearningratewarmupperiodof5epochs. Weusea
batchsizeof128with16accumulationsteps,resultinginaneffectivebatchsizeof1024. Thehyperparametersarechosen
identicallytothoseonCIFAR-10(seeAppendixK.1).
K.3.Runtime
Table4andTable5showstatisticsoftheper-epochruntimeforeachmethodonImageNetandCIFAR-10,respectively.
AsLaplace,Mahalanobis,anddeepensemblearepost-hocmethods,theirreportedtimecomprisestheconstructionofthe
methodanditsevaluation.
L.VisualizationofImagesandLabelDistributions
Thissectiondisplaysbotheasy(lowhumanuncertainty)ImageNetsamplesinFigure38andhard(highhumanuncertainty)
onesinFigure39usingtheImageNet-ReaLlabelsandImageNet-Cperturbations.
Figure40andFigure41givesummarystatisticsofthelabeldistributionsofImageNet-ReaLandCIFAR-10H,respectively.
29BenchmarkingUncertaintyDisentanglement
Method Mean(s) Min(s) Max(s) StdDev(s)
Deterministic 90.1829 83.7423 122.5249 6.0592
GP 91.2036 83.9509 236.0735 9.5304
CorrectnessPrediction 91.8449 82.6371 133.5970 6.4087
ShallowEnsemble 91.9881 83.0224 119.2658 5.1190
RiskPrediction 94.7748 83.2490 123.5470 8.8957
SNGP 96.8125 88.4623 129.1145 6.4576
HET-XL 99.0390 89.4465 161.1186 8.7700
Dropout 134.0979 126.2741 188.3551 7.0563
DUQ 148.6540 137.8707 197.6619 7.9503
Laplace 273.2982 250.8563 307.6892 22.9335
Mahalanobis 370.4277 360.0912 376.3072 5.7191
DeepEnsemble 865.0582 835.1872 904.2822 22.8003
Table4.Summaryofper-epochtimesforthebenchmarkedmethodsonCIFAR-10.AsLaplace,Mahalanobis,andDeepEnsembleare
post-hocmethods,theirreportedtimecomprisestheconstructionofthemethodanditsevaluation.Methodsaresortedbyincreasingmean
per-epochruntime,separatelyfortrainedandpost-hocmethods.
Method Mean(s) Min(s) Max(s) StdDev(s)
Deterministic 2646.6512 2566.9703 2899.1143 44.7298
RiskPrediction 2692.1484 2637.5598 2908.5478 33.4657
CorrectnessPrediction 2732.9235 2562.6328 3284.8763 230.3123
ShallowEnsemble 2803.4469 2671.9823 3268.5573 181.0415
GP 3059.7266 2645.5470 3880.5432 399.7528
Dropout 3145.4667 3034.7784 3307.8100 80.4335
SNGP 3233.9454 3081.7184 3742.6995 145.4325
HET-XL 4018.7616 3915.7693 4214.7061 55.4737
Mahalanobis 33929.2063 32972.1129 35235.9114 956.6842
Laplace 52836.5020 52298.8008 53949.9782 588.1313
DeepEnsemble 161492.2153 161492.2153 161492.2153 0.0000
Table5.Summaryofper-epochtimesforthebenchmarkedmethodsonImageNet.AsLaplace,Mahalanobis,andDeepEnsembleare
post-hocmethods,theirreportedtimecomprisestheconstructionofthemethodanditsevaluation.Methodsaresortedbyincreasingmean
per-epochruntimeseparatelyfortrainedandpost-hocmethods.
30BenchmarkingUncertaintyDisentanglement
1.0
Distributional Deterministic
0.9
0.878 0.872 0.872 0.870 0.870 0.866 0.856 0.854 0.854
0.827
0.8
0.7
0.6
0.567
0.5
1.0 1.0
Distributional Deterministic Distributional Deterministic
0.9 0.9
0.874 0.872 0.872 0.872 0.871 0.871 0.865 0.862 0.861 0.864 0.863 0.859 0.857 0.857 0.856 0.856 0.856 0.855
0.819
0.8 0.8 0.793
0.7 0.7
0.6 0.6
0.525
0.512
0.5 0.5
(a)MixedIDandOODseverityleveltwo. (b)OODseverityleveltwo.
1.0 1.0
Distributional Deterministic Distributional Deterministic
0.9 0.894 0.892 0.891 0.890 0.889 0.887 0.887 0.883 0.882 0.9
0.864 0.863 0.859 0.858 0.857 0.856
0.841 0.843 0.843 0.840
0.8 0.8 0.780
0.7 0.7
0.6 0.602 0.6
0.526
0.5 0.5
(c)MixedIDandOODseveritylevelfour. (d)OODseveritylevelfour.
Figure27.SlightlyalteringtheevaluationcriterionhasaninfluenceontherankingofmethodsontheImageNetvalidationset,evidencing
thatthemethodsaresaturatedonthecorrectnesspredictiontask.Variantof26wherecorrectnessiscalculatedw.r.t.thesoftlabelsof
ImageNet-ReaL.IDresultsareaddedontop.
31
CORUAtfoS
CORUAtfoS
↑
↑
.snEwollahS
tuoporD
.snEpeeD
.snEwollahS
tuoporD
enilesaB
PGNS
PGNS
PG
PG
enilesaB
.snEpeeD
CORUAtfoS
ecalpaL
ecalpaL
↑
LX-TEH
.derPssoL
.derPssoL
LX-TEH
tuoporD
.derP.rroC
.derP.rroC
.snEpeeD
sibonalahaM
sibonalahaM
.snEwollahS
PG
PGNS
enilesaB ecalpaL
CORUAtfoS
CORUAtfoS
↑
↑
.derPssoL
LX-TEH
.snEpeeD
enilesaB
.derP.rroC
enilesaB
ecalpaL
sibonalahaM
ecalpaL
LX-TEH
LX-TEH
.snEpeeD
.snEwollahS
tuoporD
.derPssoL
.derPssoL
tuoporD
PGNS
PGNS
PG
PG
.snEwollahS
.derP.rroC
.derP.rroC
sibonalahaM
sibonalahaMBenchmarkingUncertaintyDisentanglement
1.0 1.0
Distributional Distributional
0.926 0.919 0.918 0.918 0.910 0.910 0.909 0.909 Deterministic Deterministic
0.9 0.895 0.882 0.868 0.9 0.894 0.888 0.886 0.886 0.878 0.877 0.875 0.875 0.858
0.844 0.842
0.8 0.8
0.7 0.687 0.7
0.667
0.6 0.6
0.5 0.5
(a)MixedIDandOODseverityleveltwo. (b)OODseverityleveltwo.
1.0 1.0
Distributional Distributional
Deterministic Deterministic
0.9 0.908 0.901 0.898 0.896 0.894 0.893 0.891 0.889 0.9
0.876 0.870
0.845 0.841 0.834 0.832 0.828 0.827 0.824 0.822 0.817
0.8 0.8 0.802 0.794 0.790
0.757
0.705
0.7 0.7
0.6 0.6
0.5 0.5
(c)MixedIDandOODseveritylevelfour. (d)OODseveritylevelfour.
Figure28.OnCIFAR-10,theperformanceofthemethodsconsistentlydropsonthecorrectnesspredictiontask,bothoncompletelyOOD
datasets(toprightandbottomright)andonbalancedmixturesofIDandOODdatasets(topleftandbottomleft).Anotableexceptionis
theMahalanobismethod:here,weobserveanincreaseinperformanceasweincreasethelevelofseverity.Forthemixeddatasets,this
canbeexplainedbythefactthatthemodelsperformworseonOODimagesthanonIDones,makingtheMahalanobismethodasuitable
estimatorofcorrectness.However,theresultisunexpectedforsolelyOODsamples.
32
CORUA
CORUA
↑
↑
.snEpeeD
.snEpeeD
PGNS
PGNS
PG
PG
tuoporD
tuoporD
LX-TEH
.snEwollahS
.snEwollahS
LX-TEH
enilesaB
ecalpaL
ecalpaL
enilesaB
.derP.rroC
.derP.rroC
.derPksiR
.derPksiR
QUD
QUD
sibonalahaM
sibonalahaM
CORUA
CORUA
↑
↑
.snEpeeD
.snEpeeD
tuoporD
PGNS
PGNS
tuoporD
PG
PG
LX-TEH
.snEwollahS
.snEwollahS
LX-TEH
ecalpaL
ecalpaL
enilesaB
enilesaB
.derP.rroC
.derP.rroC
.derPksiR
.derPksiR
QUD
QUD
sibonalahaM
sibonalahaMBenchmarkingUncertaintyDisentanglement
1.0
Distributional
Deterministic
0.9
0.853 0.852 0.850 0.850 0.849 0.848 0.844 0.838 0.829 0.827
0.8 0.786
0.7
0.6
0.526
0.5
1.0 1.0
Distributional Distributional
Deterministic Deterministic
0.9 0.9
0.873 0.873 0.872 0.867 0.867 0.866 0.860 0.858 0.858 0.849 0.819 0.862 0.859 0.859 0.858 0.850 0.850 0.848 0.848 0.836 0.826 0.814
0.8 0.8
0.7 0.7
0.655 0.649
0.6 0.6
0.5 0.5
(a)MixedIDandOODseverityleveltwo. (b)Severityleveltwo.
1.0 1.0
Distributional Distributional
Deterministic Deterministic
0.9 0.9
0.876 0.873 0.871 0.866 0.866 0.865 0.864 0.863 0.854 0.849
0.818 0.829 0.823 0.820 0.818 0.816 0.813 0.813 0.808 0.8 0.8 0.794 0.787 0.780
0.730
0.7 0.7 0.695
0.6 0.6
0.5 0.5
(c)MixedIDandOODseveritylevelfour. (d)OODseveritylevelfour.
Figure29.Slightlyalteringtheevaluationcriteriononthecorrectnesspredictiontaskchangestherankingofmethodsconsiderably,
evidencingthatthemethodsaresaturatedonCIFAR-10.VariantofFigure28w.r.t.softlabelcorrectnesswiththeIDresultsaddedontop.
33
CORUAtfoS
CORUAtfoS
↑
↑
PG
.snEpeeD
PGNS
PGNS
.snEpeeD
PG
tuoporD
tuoporD
ecalpaL
ecalpaL
enilesaB
.snEwollahS
.snEwollahS
enilesaB
CORUAtfoS
↑
LX-TEH
LX-TEH
.derP.rroC
.derP.rroC
PG
.derPksiR
.derPksiR
.derP.rroC
QUD
QUD
PGNS
sibonalahaM
sibonalahaM
ecalpaL tuoporD enilesaB
.snEpeeD
CORUAtfoS
CORUAtfoS
.derPksiR
↑
↑
.snEwollahS
LX-TEH
.snEpeeD
.snEpeeD
QUD
PG
PGNS
sibonalahaM
PGNS
tuoporD
tuoporD
PG
ecalpaL
.snEwollahS
enilesaB
ecalpaL
.snEwollahS
LX-TEH
LX-TEH
enilesaB
.derP.rroC
.derP.rroC
.derPksiR
.derPksiR
QUD
QUD
sibonalahaM
sibonalahaMBenchmarkingUncertaintyDisentanglement
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 AUROCCorrectness 0.2 AUROCCorrectness
AUCAbstinence AUCAbstinence
Accuracy Accuracy
0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5
SeverityLevel SeverityLevel
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 AUROCCorrectness 0.2 AUROCCorrectness
AUCAbstinence AUCAbstinence
Accuracy Accuracy
0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5
SeverityLevel SeverityLevel
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 AUROCCorrectness 0.2 AUROCCorrectness
AUCAbstinence AUCAbstinence
Accuracy Accuracy
0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5
SeverityLevel SeverityLevel
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 AUROCCorrectness 0.2 AUROCCorrectness
AUCAbstinence AUCAbstinence
Accuracy Accuracy
0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5
SeverityLevel SeverityLevel
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 AUROCCorrectness 0.2 AUROCCorrectness
AUCAbstinence AUCAbstinence
Accuracy Accuracy
0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5
SeverityLevel SeverityLevel
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 AUROCCorrectness 0.2 AUROCCorrectness
AUCAbstinence AUCAbstinence
Accuracy Accuracy
0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5
SeverityLevel SeverityLevel
Figure30.OnCIFAR-10,allmethods’performancedeterioratesatthesamerateasthemodel’saccuracyonthecorrectnessandabstinence
tasks.TheonlyexceptionisMahalanobis,whichisaspecializedOODdetector.
34
.ccAdnasCORUAenilesaB
.ccAdnasCORUA.derP.rroC
.ccAdnasCORUAtuoporD
.ccAdnasCORUAPG
.ccAdnasCORUAecalpaL
.ccAdnasCORUA.derPksiR
↑
↑
↑
↑
↑
↑
.ccAdnasCORUAPGNS
.ccAdnasCORUA.snEpeeD
.ccAdnasCORUAQUD
.ccAdnasCORUALX-TEH
.ccAdnasCORUAsibonalahaM
.ccAdnasCORUA.snEwollahS
↑
↑
↑
↑
↑
↑BenchmarkingUncertaintyDisentanglement
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 AUROCCorrectness 0.2 AUROCCorrectness
AUCAbstinence AUCAbstinence
Accuracy Accuracy
0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5
SeverityLevel SeverityLevel
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 AUROCCorrectness 0.2 AUROCCorrectness
AUCAbstinence AUCAbstinence
Accuracy Accuracy
0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5
SeverityLevel SeverityLevel
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 AUROCCorrectness 0.2 AUROCCorrectness
AUCAbstinence AUCAbstinence
Accuracy Accuracy
0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5
SeverityLevel SeverityLevel
1.0 1.0
0.8 0.8
0.6 0.6
0.4 0.4
0.2 AUROCCorrectness 0.2 AUROCCorrectness
AUCAbstinence AUCAbstinence
Accuracy Accuracy
0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5
SeverityLevel SeverityLevel
1.0 1.0
AUROCCorrectness
AUCAbstinence
0.8 0.8 Accuracy
0.6 0.6
0.4 0.4
0.2 AUROCCorrectness 0.2
AUCAbstinence
Accuracy
0.0 0.0
0 1 2 3 4 5 0 1 2 3 4 5
SeverityLevel SeverityLevel
1.0
0.8
0.6
0.4
0.2 AUROCCorrectness
AUCAbstinence
Accuracy
0.0
0 1 2 3 4 5
SeverityLevel
Figure31.OnImageNet,theestimateforpredictivecorrectnessismuchmorerobusttoOODperturbationsthanthemodel’saccuracyfor
allmethodsexceptMahalanobis(whichisaspecializedOODdetector).TheAUCabstinencescoredeterioratesatthesamerateasthe
model’saccuracy.
35
.ccAdnasCORUAenilesaB
.ccAdnasCORUA.derP.rroC
.ccAdnasCORUAtuoporD
.ccAdnasCORUAPG
.ccAdnasCORUAecalpaL
↑
↑
↑
↑
↑
.ccAdnasCORUA.derPksiR
↑
.ccAdnasCORUAPGNS
.ccAdnasCORUA.snEpeeD
.ccAdnasCORUA.snEwollahS
.ccAdnasCORUALX-TEH
.ccAdnasCORUAsibonalahaM
↑
↑
↑
↑
↑BenchmarkingUncertaintyDisentanglement
1.0 1.0
Distributional Deterministic Distributional Deterministic
0.9 0.9
0.812
0.8 0.768 0.752 0.8 0.791 0.781 0.772 0.771 0.761 0.747 0.747
0.728 0.732 0.726 0.725
0.7 0.708 0.707 0.700 0.695 0.694 0.7
0.674 0.669 0.666
0.6 0.6
0.5 0.5
(a)AUROCOOD-nesswithOODseverityleveltwo. (b)AUROCOOD-nesswithOODseveritylevelthree.
1.0 1.0
Distributional Deterministic Distributional Deterministic
0.918
0.9 0.9 0.896 0.891 0.888 0.887
0.874 0.855 0.865 0.861 0.860 0.860 0.859
0.842 0.842 0.839
0.8 0.822 0.810 0.810 0.806 0.803 0.801 0.8 0.822
0.7 0.7
0.6 0.6
0.5 0.5
(c)AUROCOOD-nesswithOODseveritylevelfour. (d)AUROCOOD-nesswithOODseveritylevelfive.
Figure32.TheOODdetectionperformanceofallmethodsincreasessteadilyasweincreasetheseverityoftheperturbedhalfofthe
mixeddatasetontheImageNetvalidationdataset.However,thespecializedOODdetector,Mahalanobis,generalizesworsethantheother
methods.
36
CORUA
CORUA
↑
↑
sibonalahaM
tuoporD
tuoporD
.snEwollahS
.snEwollahS
PGNS
PGNS
PG
PG
.snEpeeD
.snEpeeD
.derPssoL
.derP.rroC
.derP.rroC
.derPssoL
enilesaB
enilesaB
sibonalahaM
LX-TEH
ecalpaL
ecalpaL
LX-TEH
CORUA
CORUA
↑
↑
tuoporD
tuoporD
.snEwollahS
.snEwollahS
sibonalahaM
PG
PGNS
PGNS
PG
.snEpeeD
.snEpeeD
enilesaB
.derPssoL
.derPssoL
.derP.rroC
LX-TEH
enilesaB
ecalpaL
LX-TEH
.derP.rroC
ecalpaL
sibonalahaMBenchmarkingUncertaintyDisentanglement
1.0 1.0
Distributional Distributional
Deterministic Deterministic
0.9 0.9
0.8 0.8
0.767
0.735 0.754 0.746 0.744 0.743 0.734 0.734 0.734 0.729 0.725 0.718
0.7 0.703 0.694 0.692 0.692 0.686 0.686 0.686 0.681 0.675 0.669 0.7
0.648
0.613
0.6 0.6
0.5 0.5
(a)OOD-nessAUROCwithseverityleveltwo. (b)OOD-nessAUROCwithseveritylevelthree.
1.0 1.0
Distributional Distributional
Deterministic Deterministic
0.9 0.9
0.853 0.850 0.847 0.843 0.838 0.828 0.826 0.826 0.825 0.825 0.819
0.8 0.801 0.800 0.790 0.787 0.784 0.776 0.775 0.773 0.773 0.771 0.767 0.8
0.741
0.7 0.687 0.7
0.6 0.6
0.5 0.5
(c)OOD-nessAUROCwithseveritylevelfour. (d)OOD-nessAUROCwithseveritylevelfive.
Figure33.OnCIFAR-10,theOODdetectionperformanceofallmethodsincreasessteadilyasweincreasetheseverityoftheperturbed
halfofthemixeddataset.Mahalanobisgeneralizesworsethantheothermethods.
0.950
0.925
0.900 GP
HET-XL
0.875 Baseline
Dropout
0.850 SNGP
DUQ
0.825 ShallowEnsemble
CorrectnessPrediction
0.800 DeepEnsemble
Laplace
0.01 0.02 0.03 0.04 0.05
ECECorrectness(rcorr=0.101)
Figure34.Dependingonwhattaskweoptimizetheaggregatorfor,weobtainnotablydifferentresultsforSNGPvariants,deepensembles,
andLaplacenetworksonCIFAR-10.Eachcolorcorrespondstoonemethod.Thepointpairspermethodshowtheperformanceofthe
methodoptimizedforECEandthatoptimizedforthecorrectnessAUROC.Therankcorrelationvaluesontheaxesarewithrespectto
allfiveseedsforallmethodswhenoptimizedforECEorthecorrectnessAUROC.Thesignofthecorrelationcanflipbasedonwhat
aggregatorwechoose.
37
CORUA
CORUA
↑
↑
sibonalahaM
.snEpeeD
.snEpeeD
sibonalahaM
PG
PG
PGNS
PGNS
.derPksiR
.derPksiR
)35.0-=rrocr(ssentcerroCCORUA
enilesaB
ecalpaL
ecalpaL
enilesaB
.derP.rroC
.snEwollahS
.snEwollahS
.derP.rroC
LX-TEH
LX-TEH
tuoporD
tuoporD
QUD
QUD
CORUA
CORUA
↑
↑
sibonalahaM
.snEpeeD
.snEpeeD
PG
PG
PGNS
.derPksiR
sibonalahaM
PGNS
.derPksiR
enilesaB
.snEwollahS
.derP.rroC
LX-TEH
ecalpaL
ecalpaL
.snEwollahS
enilesaB
LX-TEH
.derP.rroC
tuoporD
tuoporD
QUD
QUDBenchmarkingUncertaintyDisentanglement
0.35 Baseline
GP SNGP
0.30 ≈
Dropout
DeepEnsemble
0.25
0.20
0.15
0.10
0.05
0.00
0 1 2 3 4 5
SeverityLevel
Figure35.OnCIFAR-10,asweincreasetheseveritylevel,onlydeepensembleanddropoutarecapableofperformingconsiderablybetter
thanthebaselineonaverage.SNGPisomittedasitbehavesidenticallytoGP.Thebaselineisincludedforreference.
38
ssentcerroCECEBenchmarkingUncertaintyDisentanglement
0.00
0.25 -0.105 -0.142 -0.156 -0.168 -0.174 -0.174 -0.177 -0.177 -0.181
− -0.266
0.50 −
0.75
−
1.00
−
1.25
−
1.50
−
1.75 Distributional
− Deterministic
2.00
−
0.00 0.00
− 00 .. 52 05 -0.208 -0.267 -0.299 -0.349 -0.357 -0.359 -0.360 -0.363 -0.385 − 00 .. 52 05 -0.311 -0.391
-0.443 − -0.543 − -0.531 -0.539 -0.541 -0.543 -0.552 -0.590
0.75 0.75
− −
-0.819
1.00 1.00
− −
1.25 1.25
− −
1.50 1.50
− −
1.75 Distributional 1.75 Distributional
− Deterministic − Deterministic
2.00 2.00
− −
(a)MixedIDandOODseverityleveltwo. (b)OODseverityleveltwo.
0.00 0.00
0.25 0.25
− −
-0.314
−0.50 -0.432 -0.450
-0.532 -0.558 -0.560 -0.560 -0.564
−0.50
-0.522
-0.626 −0.75 −0.75 -0.722 -0.744
-0.847
−1.00 −1.00 -0.896 -0.941 -0.943 -0.946 -0.952
-1.071
1.25 1.25
− −
1.50 1.50 -1.428
− −
1.75 Distributional 1.75 Distributional
− Deterministic − Deterministic
2.00 2.00
− −
(c)MixedIDandOODseveritylevelfour. (d)OODseveritylevelfour.
Figure36.OnCIFAR-10,deepensemble,dropout,andLaplacearetheonlymethodsthatconsistentlyoutperformthebaselineonaverage,
bothIDandOODforallseveritylevelswhenevaluatingonthelogprobabilityproperscoringrule.
39
ytilibaborPgoL
ytilibaborPgoL
↑
↑
.snEpeeD
.snEpeeD
tuoporD
tuoporD
ecalpaL
ecalpaL
PGNS
PGNS
LX-TEH
LX-TEH
ytilibaborPgoL
enilesaB
PG
↑
PG
.snEwollahS
.snEwollahS
enilesaB
.snEpeeD
QUD
QUD
tuoporD
.derP.rroC
.derP.rroC
ecalpaL PGNS LX-TEH .snEwollahS
ytilibaborPgoL
ytilibaborPgoL
↑
↑
enilesaB PG QUD
.snEpeeD
.snEpeeD
.derP.rroC
tuoporD
tuoporD
ecalpaL
ecalpaL
PGNS
PGNS
LX-TEH
LX-TEH
enilesaB
PG
PG
.snEwollahS
.snEwollahS
enilesaB
QUD
QUD
.derP.rroC
.derP.rroCBenchmarkingUncertaintyDisentanglement
1
LogProb.
Brier 0.965
0.5
-ECE(*) 0.86 0.734
Correctness 0.7760.7550.734
0
Abstinence 0.4830.6150.0140.217
Accuracy 0.28 0.399-0.182-0.1470.881
0.5
Aleatoric 0.72 0.8040.3640.4340.916 0.72 −
OOD(*) 0.4130.3430.5450.755-0.056-0.49 0.14
1
−
LogProb. Brie -r ECE C( o* r) rectne As bs stinence Accuracy Aleatoric OOD(*)
(a)Resultsusingoneminusthemaximumprobabilityoff˜
astheuncertaintyestimator.
1
LogProb.
Brier 0.986
0.5
-ECE(*) 0.9650.951
Correctness 0.8530.8250.734
0
Abstinence 0.545 0.58 0.462 0.35
Accuracy 0.35 0.4060.3010.133 0.93
0.5
Aleatoric 0.5240.5520.3920.5240.8320.783 −
OOD(*) 0.1680.1120.0280.559-0.252-0.524-0.014
1
−
LogProb. Brie -r ECE C( o* r) rectne As bs stinence Accuracy Aleatoric OOD(*)
(b)Resultsusingoneminusthemaximumprobabilityoff¯
astheuncertaintyestimator.
1
LogProb.
Brier 0.923
0.5
-ECE(*) 0.8950.972
Correctness 0.8110.902 0.86
0
Abstinence 0.587 0.58 0.5240.483
Accuracy 0.5240.4690.3780.3360.944
0.5
Aleatoric 0.6780.6710.6010.6430.8950.832 −
OOD(*) -0.056-0.084-0.0140.028-0.238-0.469-0.112
1
−
LogProb. Brie -r ECE C( o* r) rectne As bs stinence Accuracy Aleatoric OOD(*)
(c)Resultsusingoneminustheexpectedmaximumprobabilityastheuncertaintyestimator.
Figure37.RankcorrelationresultsacrossmetricsusingdifferentestimatorsonImageNet. TheOODandECEmetricsexhibithighly
differentrankcorrelationscoresdependingontheestimatorwechoose.
40BenchmarkingUncertaintyDisentanglement
OriginalSamples PerturbedSamples
Figure38. EasyImageNet-ReaLcaseswithnohumandisagreementonthelabels.OODsamplesareofseveritytwo.
41BenchmarkingUncertaintyDisentanglement
OriginalSamples LabelDistributions PerturbedSamples
0.08
0.06
0.04
0.02
0.00
0 200 400 600 800 1000
0.12
0.10
0.08
0.06
0.04
0.02
0.00
0 200 400 600 800 1000
0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00
0 200 400 600 800 1000
0.14
0.12
0.10
0.08
0.06
0.04
0.02
0.00
0 200 400 600 800 1000
Figure39.HardImageNet-ReaLcaseswithhighhumanuncertainty(i.e.,highdisagreementamongannotatorsonthecorrectlabel).OOD
samplesareofseveritytwo.
42BenchmarkingUncertaintyDisentanglement
35000
25000
30000
20000 25000
20000
15000
15000
10000
10000
5000
5000
0 0
1 2 3 4 5 6 7 8 9 1011121314151617181920 1 2 3 4 5 6 7 8 9 1011121314151617181920
Number of Annotations per Sample Number of Unique Annotations per Sample
Figure40. HistogramsofthelabeldistributionsoftheImageNet-ReaLvalidationset.
3500 2500
3000
2000
2500
2000 1500
1500
1000
1000
500
500
0 0
48 50 52 54 56 58 60 2 4 6 8 10
Number of Annotations per Sample Number of Unique Annotations per Sample
Figure41. HistogramsofthelabeldistributionsoftheCIFAR-10Hvalidationset.
43