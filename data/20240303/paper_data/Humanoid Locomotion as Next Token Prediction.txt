Humanoid Locomotion as Next Token Prediction
IlijaRadosavovic1 BikeZhang1 BaifengShi1 JathushanRajasegaran1 SarthakKamat1
TrevorDarrell1 KoushilSreenath1 JitendraMalik1
Abstract tionsinasimilarfashion? Indeed,wehaveseenthatwecan
learngoodrepresentationsofhigh-dimensionalvisualdata
We cast real-world humanoid control as a next
byautoregressivemodeling(6)andrelatedmaskedmodel-
tokenpredictionproblem,akintopredictingthe
ingapproaches(13). Whiletherehasbeenpositivesignal
next word in language. Our model is a causal
onlearningsensorimotorrepresentationsinthecontextof
transformertrainedviaautoregressiveprediction
manipulation(32),thisarearemainslargelyunexplored.
ofsensorimotortrajectories. Toaccountforthe
multi-modalnatureofthedata,weperformpre- Inthispaper,wecasthumanoidcontrolasdatamodeling
dictioninamodality-alignedway,andforeach of large collections of sensorimotor trajectories. Like in
inputtokenpredictthenexttokenfromthesame language,wetrainageneraltransformermodeltoautore-
modality. Thisgeneralformulationenablesusto gressively predict shifted input sequences. In contrast to
leveragedatawithmissingmodalities,likevideo language, the nature of data in robotics is different. It is
trajectorieswithoutactions. Wetrainourmodel high-dimensional and contains multiple input modalities.
onacollectionofsimulatedtrajectoriescoming Differentmodalitiesincludesensors,likejointencodersor
frompriorneuralnetworkpolicies,model-based inertial measurement units, as well as motor commands.
controllers, motion capture data, and YouTube Thesegiverisetosensorimotortrajectorieswhichweview
videosofhumans. Weshowthatourmodelen- asthesentencesofthephysicalworld. Adoptingthisper-
ablesafull-sizedhumanoidtowalkinSanFran- spective suggests a simple instantiation of the language
cisco zero-shot. Our model can transfer to the modelingframeworkintheroboticcontext. Wetokenize
real world even when trained on only 27 hours theinputtrajectoriesandtrainacausaltransformermodel
ofwalkingdata,andcangeneralizetocommands topredictshiftedtokens. Importantly,wepredictcomplete
notseenduringtraininglikewalkingbackward. inputsequences,includingbothsensoryandmotortokens.
Thesefindingssuggestapromisingpathtoward Inotherwords,wearemodelingthejointdatadistribution
learningchallengingreal-worldcontroltasksby asopposedtotheconditionalactiondistribution.
generativemodelingofsensorimotortrajectories.
This has several benefits. First, we are training the neu-
ralnetworktopredictmorebitsofinformationandconse-
quentlyacquirearichermodeloftheworld. Second, we
1.Introduction
canleveragenoisyorimperfecttrajectoriesthatmaycontain
The last decade of artificial intelligence (AI) has shown suboptimalactions.Third,wecangeneralizeourframework
thatlargeneuralnetworkstrainedondiversedatasetsfrom tolearningfromtrajectorieswithmissinginformation.
theInternetcanleadtoimpressiveresultsacrossdifferent
Ourcoreobservationisthatifatrajectoryisincomplete,i.e.,
settings. The core enablers of this wave of AI have been
someofthesensoryormotorinformationismissing, we
largetransformermodels(42)trainedbygenerativemodel-
canstilllearnfromitbypredictingwhateverinformation
ingofmassivequantitiesoflanguagedatafromtheInter-
ispresentandreplacingthemissingtokenswithlearnable
net (29, 8, 30, 31, 4). By predicting the next word, these
masktokens.Theintuitionisthatifthemodelhaslearnedto
modelsacquirerichrepresentationsoflanguagethatcanbe
makegoodpredictions,evenintheabsenceofinformation,
transferred to downstream tasks (29), perform multi-task
itwillhaveacquiredabettermodelofthephysicalworld. A
learning(30,31),andlearninafew-shotmanner(4).
veryimportantsourceofsuchdataarehumanvideosfrom
Aresuchmodelingtechniquesexclusivetolanguage? Can theInternet. Namely,wecanobservehumanmovementin
welearnpowerfulmodelsofsensoryandmotorrepresenta- videosbutwedonotgetaccesstothemotorcommandsor
completesensoryinputs. Wedemonstratethatourmethod
1UniversityofCalifornia,Berkeley.Correspondenceto:Ilija
canlearnfromsuchdatasourceseffectively.
Radosavovic<ilija@berkeley.edu>.
4202
beF
92
]OR.sc[
1v96491.2042:viXraHumanoidLocomotionasNextTokenPrediction
Figure1: AhumanoidthatwalksinSanFrancisco. WedeployourpolicytovariouslocationsinSanFranciscoover
thecourseofoneweek. Pleaseseeourprojectpageforvideos. Weshowthatourpolicycanwalkoverdifferentsurfaces
includingwalkways,concrete,asphalt,tiledplazas,andsandedroads. Wefindthatourpolicyfollowsomnidirectional
velocitycommandswellandenablesdeploymentinachallengingcityenvironmentlikeSanFrancisco.HumanoidLocomotionasNextTokenPrediction
Tovalidateourmethod,weapplyittothechallengingtask modelsinrobotics. Wehaveseenseveralworksshowing
ofreal-worldhumanoidlocomotion. Weusethefull-sized that transformers can be effective with behavior cloning.
DigithumanoidrobotdevelopedbyAgilityRobotics. We For example, (38) learns multi-task transformer policies
firstcollectadatasetofsensorimotortrajectoriesinsimu- withlanguage,and(2)trainslanguage-conditionedmanipu-
lation. Theseincludecompletetrajectoriesfromaneural lationpoliciesfromlarge-scaledata. (10)trainslanguage
networkpolicytrainedwithreinforcementlearning(33),as modelswithembodieddata. Wehavealsoseenthattrans-
wellasincompletetrajectoriesfromthreedifferentsources: former policies can be effective for large-scale reinforce-
(i) Agility Robotics controller based on model predictive mentlearning(33).(32)learnssensorimotorrepresentations
control,(ii)motioncaptureofhumans,and(iii)YouTube with masked prediction. (1) trains goal-conditioned poli-
videosofhumans. Wereconstructhumanvideosbyusing ciesarelearnedfromdemonstrations. Likewise,weshare
computervisiontechniquesandretargetbothmotioncapture the goal of using transformer models for robotics but fo-
andYouTubetrajectoriesviainversekinematics. Wethen cusonautoregressivemodelingofdiversetrajectoriesfor
trainatransformermodeltoautoregressivelypredicttrajec- real-worldhumanoidlocomotion.
tories. Attesttime,weexecutetheactionsautoregressively
Humanoid locomotion. Mastering the ability for robots
andignorethesensorypredictions.
towalkhasbeenalong-standingchallengeinrobotics. In
Wedemonstratethatourpolicycanbedeployedinthereal the past several decades, roboticists have built a variety
worldzero-shotandwalkondifferentsurfaces. Specifically, of humanoid robots (20, 15, 26, 40, 7) to explore human-
deploy our model across a range of different locations in likelocomotionskills. Stablelocomotionbehaviorshave
San Francisco over the course of one week. Please see been achieved through model-based control approaches
Figure1forexamplesandourprojectpageforvideos. To (34, 18), and optimization-based methods further enable
quantitatively evaluate different aspects of our approach, highlydynamichumanoidmotions(22). Althoughsignifi-
weperformanextensivestudyinsimulation. Wefindthat cantprogresshasbeenmadewiththesestrategiesandcom-
ourautoregressivepoliciestrainedfromofflinedataalone biningthemwithlearning(5),learning-basedapproaches
arecomparabletothestate-of-the-artapproachesthatuse aregainingattentionfortheirabilitytoimproveandadaptto
reinforcementlearning(33)intestedsettings. Wefurther awiderangeofenvironments. Recently,wehaveseenthat
findthatourapproachcanreadilybenefitfromincomplete apurelylearningbasedapproachtrainedwithlarge-scale
trajectoriesandhasfavorablescalingproperties. reinforcementlearninginsimulationcanenablereal-world
humanoidlocomotion(33). Likeinpriorwork,ourmodel
These findings suggest a promising path toward learning
is a causal transformer. Unlike prior work, we perform
challenging real-world robot control tasks by generative
autoregressivemodelinginsteadofreinforcementlearning.
modelingoflargecollectionsofsensorimotortrajectories.
3.Approach
2.RelatedWork
Inthissection,weassumethatadatasetDofsensorimotor
Generativemodeling. Thestudyofdatahasbeenexten-
trajectoriesT isgivenanddescribeourapproachbelow.
sive,rangingfromShannon’sfoundationalwork(37)tothe
recent era of large language models. Various such mod-
3.1.Objective
els emerged over the last decade. Notable such models
includes,GAN(12)andDiffusionmodels(39,16)forgen- Eachsensorimotortrajectoryisasequenceofsensoryob-
erating pixels, LSTM (17) and GPT (29) for generating servations and actions: T = (o ,a ,o ,a ,...,o ,a ).
1 1 2 2 T T
languagetokens. Thesemodelshavebeenadoptedforother We first tokenize the trajectory into K tokens to obtain
modalitiesaswell(27,11,43). Amongthese,autoregres- t = (t ,t ,t ,...,t ). Our goal is to train a neural net-
1 2 3 K
sive transformer models became the front runner, due to worktomodelthedensityfunctionp(t)autoregressively:
theimpressivescalingbehaviours(19)andabilitytolearn
fromin-contextexamples(3). Thisbehaviorisevenshown K
(cid:89)
toextendtoothermodalitiessuchaspixels(6),language- p(t)= p(t k|t k−1,...,t 1) (1)
pixels(36),andlanguage-pixels-audio(21). Weexploreau- k=1
toregressivegenerativemodelsinthecontextofreal-world
We train our model by minimizing the negative log-
humanoidlocomotion.
likelihoodoverourtrajectorydataset:
Transformersinrobotics. Followingthesuccessoftrans-
(cid:88)
formermodels(42)innaturallanguageprocessing(29,8, L= −logp(t) (2)
30,3)andcomputervision(9,13),overthelastfewyears, t∈D
therehasbeenanincreasedinterestedinusingtransformerHumanoidLocomotionasNextTokenPrediction
Data Training Deployment
Neural networkpolicy Model basedcontroller
Transformer
Motioncapture Internetvideos
Figure2: Humanoidlocomotionasnexttokenprediction. Wecollectadatasetontrajectoriesfromvarioussources,such
asfromneuralnetworkpolicies,model-basedcontrollers,humanmotioncapture,andYouTubevideosofhumans. Thenwe
usethisdatasettotrainatransformerpolicybyautoregressivemodelingofobservationsandactions. Ourtransformerallows
ahumanoidtowalkzero-shotonvariousterrainsaroundSanFrancisco. Pleaseseeourprojectpageforvideoresults.
WeassumeaGaussiandistributionwithconstantvariance 3.4.Jointtraining
andtrainaneuralnetworktominimizethemeansquared
We have two options for training on collections that con-
errorbetweenthepredictedandthegroundtruthtokens:
taindiversetrajectoriesintermsofnoiselevelsormodality
K subsets. Wecaneithertrainjointlywithalldataatonce,in-
1 (cid:88)
L=
K
((cid:98)t k−t k)2 (3) cludingcompleteandincompletetrajectories. Alternatively,
k=1 wecanfirstpre-trainonnoisyandincompletetrajectories.
This can be viewed as providing a good initialization for
Insteadofregressingtherawtokenvalues,wecouldquantiz-
then training on complete trajectories. We find that both
ingeachdimensionintobinsorperformvectorquantization.
approachesworkcomparablyinoursettingandoptforjoint
However,wefoundtheregressionapproachtoworkreason-
traininginthemajorityoftheexperimentsforsimplicity.
ablywellinpracticeandoptforitforsimplicity.
3.5.Modelarchitecture
3.2.Missingmodalities
Ourmodelisavanillatransformer(42). Giventhetrajec-
In the discussion so far we have assumed that each tra-
tories from either complete or incomplete data, we first
jectory is a sequence of observations and actions. Next,
tokenizethetrajectoriesintotokens. Welearnseparatelin-
we show how our framework can be generalized to se-
ear projection layers for each modality but shared across
quenceswithmissingmodalities,liketrajectoriesextracted
time. Toencodethetemporalinformationweusepositional
fromhumanvideosthatdonothaveactions. Supposewe
embeddings. Let’sassumeo ∈Rmanda ∈Rn,then:
i i
are given a trajectory of observations without the actions
T = (o ,o ,...,o ). Ourkeyinsightisthatwecantreat
1 2 T t =concat(o ,a ), (4)
a trajectory without actions like a regular trajectory with i i i
actionsmasked. Namely,wecaninsertmasktokens[M] h0
i
=Wt i, (5)
toobtainT =(o ,[M],o ,[M],...,o ,[M]). Thistrajec-
1 2 T
torynowhasthesameformatasourregulartrajectoriesand whereW ∈Rd×(m+n)isalinearprojectionlayertoproject
thuscanbeprocessedinaunifiedway. Weignoretheloss concatenatedobservationandactionmodalitiestoddimen-
for the predictions that correspond to the masked part of sional embedding vector. The superscript 0 indicates the
inputs. Notethatthisprincipleisnotlimitedtoactionsand embeddingat0-thlayer,i.e.,theinputlayer. Whenactionis
appliestoanyothermodalityaswell. unavailable,weuseamasktoken[M]∈Rntoreplacea ,
i
and[M]isinitializedasarandomvectorandlearnedend-
3.3.Alignedprediction to-endwiththewholemodel. Themodeltakesthesequence
ofembeddingvectorsH ={h0,h0,...,h0}asinput.
Ratherthanpredictingthenexttokeninamodality-agnostic 0 1 2 t
way, we make predictions in a modality-aligned way. ThetransformerarchitecturecontainsLlayers, eachcon-
Namely, for each input token we predict the next token sistingofamulti-headself-attentionmoduleandanMLP
ofthesamemodality. PleaseseeFigure3fordiagrams. module. Assume the output of the layer l is H , then the
lHumanoidLocomotionasNextTokenPrediction
Training with complete data Training with missing data
Transformer Transformer
M M M M
Figure 3: A general framework for training with different data sources. Our data modeling allows us to train our
transformerwithmultiplemodesoftraining. Inthecaseofobservation-actionpairsbeingavailable,wetrainourtransformer
topredictthenextpairofobservation-action. Whenthereisnoactiondataavailable,withMoCapandinternetdata,we
onlytrainourtransformertopredictthenextobservationsbymaskingtheactionswithamasktoken. Thesetwomodelsof
trainingallowourmodeltoutilizebothtypesofdata,andthisenablesustoscaleourtrainingintermsofdata.
layerl+1outputiscomputedasfollows: 4.Dataset
H˜ =LayerNorm(H ) (6) Ourapproachmotivatesbuildingadatasetoftrajectoriesfor
l l
trainingourmodel. Ourdatasetincludestrajectoriesfrom
H˜ =H˜ +MHSA(H˜ ) (7)
l l l different sources: (i) neural network policies, (ii) model-
H =H˜ +MLP(H˜ ) (8) basedcontrollers,(iii)humanmotioncapture,and(iv)hu-
l+1 l l
manvideosfromYouTube. Anillustrationofdifferentdata
sourcesisshowninFigure4. Wedescribeeachinturnnext.
Here, the multi-head self-attention has causal masking,
wherethetokenonlyattendstoitselfandthepasttokens.
4.1.Neuralnetworktrajectories
Once the tokens are processed through all the layers, we
projecttheembeddingtopredictedstatesandactions, by As the first source of training trajectories, we use a neu-
learningalinearprojectionlayerW(cid:99) ∈R(m+n)×d: ralnetworkpolicytrainedwithlarge-scalereinforcement
learning (33). Specifically, this policy was trained with
(cid:98)t i+1 =W(cid:99)hL i (9) billionsofsamplesfromthousandsofrandomizedenviron-
mentsinIsaacGym(25). WerunthispolicyintheAgility
o
(cid:98)i+1
=((cid:98)t i+1)
0:m
(10)
Robotics’simulatorandcollect10ktrajectoriesof10seach
(cid:98)a
i+1
=((cid:98)t i+1)
m:(m+n)
(11)
onflatground,withoutdomainrandomization. Eachtrajec-
toryisconditionedonavelocitycommandsampledfroma
Thenwetrainthetransformerwiththeobjectivein(3). In clippednormaldistributionasfollows: linearvelocityfor-
thecaseswherethetokenismasked,wedonotapplyany ward[0.0,1.0]m/s,linearvelocitysideways[−0.5,0.5]m/s,
losses. Wetrainourtransformerwithbothtypesofdata,as andturningangularvelocity[−0.5,0.5]rad/s.
showninFigure3. Thisallowsustousevarioussourcesof
Since we have access to the data generation policies, we
data,thusenablingscalingintermsofdata.
are able to record complete observations as well as the
exactactionsthatthemodelpredicted. Weusethissetas
3.6.Modelinference
oursourceofcompletesensorimotortrajectoriesthathave
Atinferencetime,ourtransformermodelwillalwayshave completeobservationsaswellasgroundtruthactions.
accesstoobservation-actionpairs. Inthissetting,weapply
ourtransformermodelautoregressivelyforeachobservation- 4.2.Model-basedtrajectories
actionpairtoken. Byconditioningonpastobservationsand
Asthesecondsourceoftrajectories,weusethemodel-based
actions,wepredictthenextactions(orobservation-action
controllerdevelopedbyAgilityRobotics. Itisthecontroller
pair)andexecutetheaction. Thenwetaketheobservations
thatisdeployedontheDigithumanoidrobotandavailable
fromtherobotanddiscardthepredictedobservations. We
intheAgilityRobotics’simulatoraswell. Wecollecttwo
use the observed observation and predicted action as the
setsof10ktrajectoriesofwalkingonaflatgroundof10s
nextsetoftokensandconcatenatethemwithpastpairsto
each. In both cases, we sample the velocity commands
predictthenextobservation-actionpair.HumanoidLocomotionasNextTokenPrediction
Neural Net Controller Model based Controller MoCap Internet Videos
Figure4: Trainingdataset. Totrainourmodel,weconstructadatasetoftrajectoriescomingfromfourdifferentsources. (i)
neuralnetworkpolicy: providestrajectorieswithcompleteobservationsandactions. (ii)model-basedcontroller: produces
trajectories without actions. (iii) motion capture of humans: does not contain actions and is approximately retargeted
ontotherobot. (iv)internetvideosofhumans: noisyhumanposesarefirstreconstructedvia3Dreconstructionandthen
approximatelyretargetedontotherobot.
asfollows: linearvelocityforward[−1.0,1.0]m/s,linear The optimization variables include q, q˙. For constraints,
velocitysideways[−1.0,1.0]m/s,andturningangularve- (12b)istheEulerintegrationofpostureq,(12c)constrains
locity [−1.0,1.0] rad/s. We use the default model-based therangeofqandq˙ totheiradmissiblesetsQandV. Inthe
configurationsforonesetandrandomizetheleglength,step cost function, φtraj tracks keypoint locations from human
clearance,andbouncinessofthefloorfortheother. trajectories, and φreg represents the regularization costs,
suchasjointvelocityminimizationandsmoothness.
Asthiscontrolleroutputsjointtorques,whicharenotcon-
sistentwithourjointpositionactionspace. Weonlyrecord
4.4.TrajectoriesfromYouTubevideos
the observations without the actions. This data serves as
asourceoftrajectorieswithreasonablygoodobservations Internetvideosofpeopledoingvariousactivitiesarepoten-
fromthesamemorphologybutwithouttheactions. tiallyavatsourceofdataforlearninghumanlocomotion.
However,therawpixelshavenoinformationaboutthestate
4.3.Humanmotioncapturetrajectories andactionsofthehuman. Torecoverthis,wefirstweruna
computervisiontrackingalgorithmPHALP(35)toextract
Asthenextsourceoftrajectories,weusethemotioncapture
humantrajectoriesin3D.Thisprovidesanestimateofthe
(MoCap)recordingsofhumansfromtheKITdataset(28)
3D joints of the human body SMPL (23) parameters and
distributedviatheAMASSrepository(24). Thisdatawas
a noisy estimate of the human joints in the world coordi-
recordedusingopticalmarker-basedtrackinginalaboratory
nates. Weusethehumanbodyjointpositionstoretargetthe
setting. Thedatasetconsistsof∼4ktrajectories. Weusea
motiontothehumanoidrobotusingtheinversekinematics
subsetof∼1kstanding,walking,andrunningtrajectories.
optimizationdescribedabove. Onceweretargetthemotion
Inadditiontonotcontainingthegroundtruthactions,the fromtheInternetvideostohumanoidtrajectories,wefilter
MoCaptrajectoriescomewithanadditionalchallenge: dif- thetrajectorieswiththelowoptimizationcost. Notethatthe
ferent morphology. Namely, MoCap trajectories capture scaleofthisdatacomeswiththecostofbeingnoisy.
humankeypointpositionsin3D.Inordertousethesetrajec-
toriesfortrainingarobot,wesolveaninversekinematics
5.Experiments
problemtofindthecorrespondingrobotposes.
We evaluate our approach on the challenging task of hu-
Weformulateaninversekinematicsoptimizationproblem:
manoidlocomotion. Weperformoutdoorexperimentson
N realhardwareandsystematicevaluationsinsimulation.
(cid:88)
min φtraj[t]+φreg[t] (12a)
q[t],q˙[t]
t=1 5.1.Experimentalsetup
q˙[t+1]+q˙[t]
s.t.q[t+1]=q[t]+ dt, (12b) Robotplatform. Digitisahumanoidrobotplatformdevel-
2
opedbyAgilityRobotics. Itisafull-sizedhumanoidthat
q∈Q,q˙ ∈V (12c)
is1.6mtallandweighs45kilograms. Ithas30degreesof
freedomofwhich20areactuated. Duetoitshighdimen-
whereqistherobotstateinthegeneralizedcoordinates,and
sionality and four-bar linkage structure, it is challenging
N anddtaretheoptimizationhorizonandsamplingtime.HumanoidLocomotionasNextTokenPrediction
Figure5: Comparisontostateoftheart,trajectoryad- Figure6: Trackingerrorcomparisons. Wemeasurethe
herence. The robot is commanded to walk starting from trackingerrorofourpolicyagainstastate-of-the-artbench-
the origin with a fixed heading command of 0.5 m/s and mark(left),aswellastheimprovementproducedbycom-
varying yaw commands in [−0.4,0.4] rad/s. We plot the plementingaction-labeledRLtrajectorieswithaction-free
desired(dotted)andactual(solid)trajectoriesforourpolicy trajectories(right).
andareinforcement-learningtrainedpolicy(RL).
tooptimizefastwhichmakesitparticularlyinterestingfor metricswithdetailsasfollowsandshowthattwometrics
learningapproachesthatcanlearnefficientlyfromtrajectory canconsistentlypredictlocomotionperformance.
collectionslikeours.
Trackingerror.Inallexperiments,therobotstartsfromrest
Model. Our model has a hidden size of 192 dimensions, inasimulatedenvironmentandisissuedaconstantnatural
with4layersofself-attentionlayersandMLPlayers. Each walking command consisting of a desired heading veloc-
self-attentionhas4heads. WeuseLayerNormbeforeeach ity sampled in [0.35,0.70] m/s, angular velocity sampled
attention layer and ReLU activation after the MLP layer. in[−0.4,0.4]rad/s,andzerolateralvelocity. Wecompute
WeuseaBatchNormlayertoprocesstheinputbeforethe x∗(t),theidealrobotbasepositiontrajectorythatfullysat-
transformermodel. Whenpredictingatokenattimek,to isfies the velocity command v∗(t) at all time steps. To
keepthecontextlengthatareasonablesize,weonlykeep measuretheaccuracyofcommandtracking,wedefinethe
thepast16stepsininput.InSection5.9,weshowthemodel positiontrackingerroras 1 (cid:80)T ∥x(t)−x∗(t)∥. Weuse
T t=0
isabletoscaleuptomoreparametersandlongercontext theMuJoCosimulator(41)forevaluations,andalltrajecto-
lengthandachievehigherperformance. rieslastforadurationof10seconds.
Predictionerror. Sincethemodelistrainedwiththenext
5.2.Real-worlddeployment
tokenprediction,wetestthepredictionerroronasetofvali-
Webeginbyreportingtheresultsofdeployingourpolicy dationdatathatisseparatedfromtrainingdataandcontains
intherealworld. Specifically,weevaluatedeployingour state-actiontrajectoriescollectedfromtheRLpolicy. This
robotatvariouslocationsinSanFranciscooverthecourse issimilartothelanguagemodelingevaluationforlargelan-
ofoneweek. PleaseseeFigure1forexamplesandproject guagemodels(14). Wetestbothstateandactionprediction
pageforvideos. Wefindthatourpolicyisabletowalkover errorsandaddthemtogetherasthefinalerrormetric.
avarietyofsurfacesincludingwalkways,concrete,asphalt,
tiledplazas,anddirtroads. Notethatthedeploymentina 5.4.Comparisontothestateoftheart
largecityenvironment,likeSanFrancisco,isconsiderably
Trajectory Adherence. We compare our policy to a
morechallengingthaninconstrainedenvironments.Thecity
neuralfig:tracking network controller trained with rein-
environmentismuchmorecrowded,lesspatient,andnot
forcement learning (RL) (33). Figure 5 presents a vi-
forgiving. Thismakestheerrortolerancelowandrequiresa
sual comparison of the trajectory adherence of our con-
policythatworksconsistentlywell.
troller against these state-of-the-art baselines. Starting
with a robot at the origin, we plot the actual trajec-
5.3.EvaluationMetrics
tory of the robot with eleven different yaw commands
Weevaluatelocomotionpolicieswithtwometrics: tracking selectedfrom{0.00,±0.05,±0.10,±0.20,±0.30,±0.40}
error andpredictionerror. Trackingerrormeasureshow rad/s. Foreachpolicy,wejointlyplotthedesiredandactual
accuratelytherobotfollowsaspecificlocomotioncommand. pathtracedbytherobotbase. Ourmodelexhibitssuperior
Thepredictionerroristhenexttokenpredictionlossmea- trackingtotheRLcontrolleratallturningspeeds,andhas
suredonaseparatesetofvalidationdata. Weintroducetwo near-perfecttrackingforstraight-linewalking.HumanoidLocomotionasNextTokenPrediction
0.28
0.30
0.32
0.34
0.36
r=0.87
0.38
1.3 1.2 1.1 1.0 0.9
Prediction Loss (10−2)
Figure7: Predictionerrorcorrelateswithperformance.
Figure8: Gaitquality. Wecommandtherobotwithahead-
Weplotthetrackingerrorandpredictionerrorfor14models.
ingvelocityof0.5m/sandplottheresultingphaseportrait
Thepredictionerrorlinearlycorrelateswithtasktracking
oftheleftkneejoint. ComparedtotheRLpolicy,ourpolicy
error with r = 0.87, which means lower prediction loss
featuresfewerirregularitiesandasmoother,cyclicgait.
likelyindicatesmoreaccuratecommandfollowing.
tainstheoverallshapeoftheRLpolicywhilehavingfewer
QuantitativeEvaluation. InFigure6,left,werepeatthe aberrations. Thissupportsourqualitativeassessmentofthe
above comparison to the RL controller (N = 245), with moreregularizedbehaviorseenonourpolicy.
the full range of heading and yaw velocities mentioned
in Section 5.3. We plot the mean position tracking error,
5.7.Generalizationtounseencommands
binnedbythecommandedangularyaw. Whilebothmodels
havelowertrackingerrorsatloweryaw,oursconsistently Wefindthatourpolicyalsoextrapolatesnewskillssuchas
outperformsthebaselineRLpolicy. Thisisaninteresting walking backward, which was not included in the action-
result,sinceourmodelwastrainedonnexttokenprediction labeledtrainingdata. AsFigure9illustrates,byprompting
ontrajectoriesproducedbythisverypolicy. our controller with negative values for the heading com-
mand,wefindthattherobotnaturallyperformsbackward
walkingatspeedsupto0.5m/swithoutfalling.
5.5.Predictionerrorcorrelateswithperformance
Wecollect14modelstrainedwithdifferenttrainingrecipes,
5.8.Trainingwithaction-freedata
modelarchitectures,datasizeandtypes,andtesttracking
error and prediction error for each one of them. We plot Oneofthebenefitsofourapproachisthatitcanbeapplied
the tracking and prediction errors of all the models into totrajectoriesfromdiversesources,includingmissingin-
a single scatter plot, as shown in Figure 7. We can see formation like actions in the case of human videos from
thattrackingandpredictionerrorarehighlycorrelatedwith YouTube. InFigure6,right,wecomparetheperformance
Pearson coefficient r = 0.87, which means models with oftrainingonlywithcompletetrajectoriestojointtraining
lower prediction error on the validation set likely follow onbothcompleteandincompletetrajectories. Weobserve
different commands with higher accuracy. This suggests thatincludingincompletetrajectoriesconsistentlyleadsto
thatthepredictionerrorispredictivetaskperformance. betterperformance. Thisisapromisingsignalforscaling
ourapproachtoalargecollectionofdiversetrajectories.
5.6.Gaitquality
Inhumanoidlocomotion,thesmoothnessintherobot’sgait
is contingent on the rhythmic functioning of its actuated
knee joints. One way to measure this is a phase portrait,
whichisaparametricplotofajoint’sgeneralizedposition
andvelocityovertime. Patternsintheplotcanrevealinfor-
mationaboutthetypeofmovementthejointisundergoing.
Forexample,acyclicpatternmayindicaterepetitivemotion,
while irregular patterns might suggest complex or varied
movements,suchasstumbling. InFigure8,wecommand
Figure9: Unseencommands. Ourpolicyisabletofollow
therobottowalkforwardat0.5m/s,andplottheassociated
backwardcommandsattesttime,unseenduringtraining.
phaseportraitofitsleftkneejoint. Noticethatourpolicyre-
)m(
rorrE
gnikcarT
.soPHumanoidLocomotionasNextTokenPrediction
Figure10: Scalingstudies. Wefindthatourapproachscaleswiththenumberoftrajectoriesinthetrainingdataset(left),
contextlength(middle),andlargermodels(right).
5.9.Scalingstudies when predicting action of (t + 1)-th step, since there is
no alignment, we need to first predict o and use this
Training data. In Figure 10, left, we study the scaling (cid:98)i+1
prediction as input to predict a . If the predicted o
of our model’s performance by increasing the size of the
(cid:98)i+1 (cid:98)i+1
is not accurate compared to real o (which is used to
i+1
trainingdataset. Wefindthattrainingonmoretrajectories
predict a during training), there will be a discrepancy
reducespositiontrackingerror,whichisapositivesignal
(cid:98)i+1
between test and training data which will cause error in
forincreasedperformancewhentrainingonlargerdatasets.
actionprediction.
Contextlength. Westudytheeffectofincreasingthenum-
Jointtrainingvs.stagedtraining. Givenbothcomplete
beroftokensusedinthecontextwindowofthetransformer
datawithactionandincompletedatawithoutaction,wecan
policy, varying it between 16, 32, and 48 steps in Figure
eitherjointlytrainonbothdataasdescribedinSection3,
10middle. Largercontextwindowsproducebetterpolicies,
orwecanfirstpre-trainthemodelonallthedatawithstate
whichsuggeststhatourgenerativepolicyperformsaform
predictiononly,thenfine-tunethemodeloncompletedata
ofin-contextadaptationthatimproveswithscale.
withactionprediction. Wecomparethesetwoapproaches
Modelsize. Wecomparemodelswithincreasingnumber inTable1c. Weobservenosignificantdifferencebetween
of parameters (1M, 2M, 8M) by varying the embedding these two, which indicates that pre-training on state pre-
dimension(144,192,384),numberofattentionheads(3,4, diction then fine-tuning on action prediction also gives a
12),andnumberoftransformerblocks(4,4,6)respectively. reasonablelocomotionpolicy.
Trackingerrormonotonicallydecreaseswithmodelsize.
State-action prediction vs. action-only prediction. We
comparetheperformanceofourpolicywhentrainedwith
5.10.Ablationstudies
onlypredictingactions,versuswhentrainedwithpredicting
Concatenatedvs.separatetokens. Fortheinputoftrans- bothstatesandactions. TheresultsinTable1dshowthat
former,wecaneitherconcatenateobservationandactionat thestate-actionpredictionimprovesmodelperformanceon
eachstepintoasingletoken,orembedthemintotwosepa- trajectorytracking.Wehypothesizethattheadditionallearn-
ratetokens. WecomparethesetwochoicesinTable1a. We ingsignalenablesthemodeltolearnricherrepresentations
canseethatconcatenationhaslowerpredictionerrorwhile oftheworldthatarebeneficialforthelocomotiontask.
separatingtokenshaslowertrackingerror.Overallthesetwo
performcomparablywhileusingseparatetokensdoubles 6.Discussion
theinputlengthandintroducescomputationoverhead.
We present a self-supervised approach for real-world hu-
Modality-alignedvs.non-alignedprediction. Whenwe
manoidlocomotion. Ourmodelistrainedonacollection
useseparatetokensforobservationandactionsasinput,we
ofsensorimotortrajectories,whichcomefrompriorneural
can either predict o from o and a from a , which
(cid:98)i+1 i (cid:98)i+1 i networkpolicies,model-basedcontrollers,humanmotion
aligns modality between prediction and input, or we can
capture,andYouTubevideosofhumans. Weshowthatour
predicto froma anda fromo ,whichdoesnot
(cid:98)i+1 i (cid:98)i+1 i+1 model enables a full-sized humanoid to walk in the real-
havealignment. FromTable1b,wecanseethatmodality
worldzero-shot. Thesefindingssuggestapromisingpath
alignmenthasclearlybetterperformancethannoalignment.
towardlearningchallengingreal-worldrobotcontroltasks
We suspect this is because, at t-th step during inference,
bygenerativemodelingoflargecollectionsoftrajectories.HumanoidLocomotionasNextTokenPrediction
TrackErr. Pred.Err. TrackErr. Pred.Err.
Concat 0.310 0.88 Align 0.299 0.98
Separate 0.299 0.98 Non-align 0.338 1.05
(a)Concatenatedvs.separatetokensforstatesandaction.Two (b)Alignmentvs.non-alignmentofstatesoractionsfornext
modelingdesignshavecomparableperformancewhileconcatenat- tokenprediction.Predictionwithalignedmodalityperformsbetter
ingstateandactiongivesshorterinputlengthandfasterinference. onbothetrackingerrorandnexttokenpredictionerror.
TrackErr. Pred.Err. TrackErr. Pred.Err.
Jointtraining 0.310 0.88 State-action 0.305 0.97
Stagedtraining 0.311 - Action-only 0.335 -
(c)Jointvs.stagedtrainingondatawithandwithoutactions. (d) State-action vs. action-only prediction. Predicting both
Stagedtrainingwhichpre-trainsonstatepredictionandfinetunes statesandactionsleadstolowertrackingerrorthanonlypredict-
onactionpredictionhassimilarperformanceasjointtraining. ingactionasinvanillabehaviorcloning.
Table1: Ablationsondifferentdesignchoicesinmodelingandtraining. Foreachablationwecomparetheaverage
trackingerroronasetofcommands,aswellasthenexttokenpredictionerroronthetestset. Forafaircomparison,wedo
notreportnexttokenpredictionerrorformodelsthatonlypredictactions.
Acknowledgements [5] Castillo,G.A.,Weng,B.,Zhang,W.,andHereid,A.
Robustfeedbackmotionpolicydesignusingreinforce-
ThisworkwassupportedinpartbyDARPAMachineCom-
ment learning on a 3d digit bipedal robot. In IROS,
monSenseprogram,ONRMURIprogram(N00014-21-1-
2021.
2801),NVIDIA,HongKongCentreforLogisticsRobotics,
TheAIInstitute,andBAIR’sindustrialallianceprograms. [6] Chen, M., Radford, A., Child, R., Wu, J., Jun, H.,
WethankSanerCakirandVikasUmmadisettyforhelpwith Luan, D., and Sutskever, I. Generative pretraining
theinversekinematicssimulationexperiments. frompixels. InICML,2020.
References [7] Chignoli, M., Kim, D., Stanger-Jones, E., and Kim,
S. Themithumanoidrobot: Design,motionplanning,
[1] Bousmalis,K.,Vezzani,G.,Rao,D.,Devin,C.,Lee, and control for acrobatic behaviors. In Humanoids,
A.X.,Bauza,M.,Davchev,T.,Zhou,Y.,Gupta,A., 2021.
Raju,A.,etal. Robocat: Aself-improvingfoundation
agent for robotic manipulation. arXiv:2306.11706, [8] Devlin,J.,Chang,M.-W.,Lee,K.,andToutanova,K.
2023. Bert: Pre-trainingofdeepbidirectionaltransformers
forlanguageunderstanding. InNAACL-HCT,2019.
[2] Brohan, A., Brown, N., Carbajal, J., Chebotar, Y.,
Dabis,J.,Finn,C.,Gopalakrishnan,K.,Hausman,K., [9] Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weis-
Herzog,A.,Hsu,J.,etal. Rt-1: Roboticstransformer senborn,D.,Zhai,X.,Unterthiner,T.,Dehghani,M.,
for real-world control at scale. arXiv:2212.06817, Minderer, M., Heigold, G., Gelly, S., et al. An im-
2022. age is worth 16x16 words: Transformers for image
recognitionatscale. InICLR,2020.
[3] Brown,T.,Mann,B.,Ryder,N.,Subbiah,M.,Kaplan,
J.D.,Dhariwal,P.,Neelakantan,A.,Shyam,P.,Sastry, [10] Driess,D.,Xia,F.,Sajjadi,M.S.,Lynch,C.,Chowd-
G.,Askell,A.,etal. Languagemodelsarefew-shot hery,A.,Ichter,B.,Wahid,A.,Tompson,J.,Vuong,
learners. InNeurIPS,2020. Q., Yu, T., etal. Palm-e: Anembodiedmultimodal
languagemodel. arXiv:2303.03378,2023.
[4] Brown,T.B.,Mann,B.,Ryder,N.,Subbiah,M.,Ka-
plan, J., Dhariwal, P., Neelakantan, A., Shyam, P., [11] Engel,J.,Agrawal,K.K.,Chen,S.,Gulrajani,I.,Don-
Sastry, G., Askell, A., et al. Language models are ahue, C., and Roberts, A. Gansynth: Adversarial
few-shotlearners. NeurIPS,2020. neuralaudiosynthesis. arXiv:1902.08710,2019.HumanoidLocomotionasNextTokenPrediction
[12] Goodfellow,I.,Pouget-Abadie,J.,Mirza,M.,Xu,B., [26] Nelson, G., Saunders, A., Neville, N., Swilling, B.,
Warde-Farley,D.,Ozair,S.,Courville,A.,andBengio, Bondaryk, J., Billings, D., Lee, C., Playter, R., and
Y. Generativeadversarialnets. InNeurIPS,2014. Raibert, M. Petman: A humanoid robot for testing
chemicalprotectiveclothing. JournaloftheRobotics
[13] He,K.,Chen,X.,Xie,S.,Li,Y.,Dolla´r,P.,andGir-
SocietyofJapan,2012.
shick, R. Masked autoencoders are scalable vision
learners. arXiv:2111.06377,2021. [27] Oord,A.v.d.,Dieleman,S.,Zen,H.,Simonyan,K.,
Vinyals,O.,Graves,A.,Kalchbrenner,N.,Senior,A.,
[14] Hendrycks, D., Burns, C., Basart, S., Zou, A.,
andKavukcuoglu,K. Wavenet: Agenerativemodel
Mazeika, M., Song, D., and Steinhardt, J. Measur-
forrawaudio. arXiv:1609.03499,2016.
ingmassivemultitasklanguageunderstanding. arXiv
preprintarXiv:2009.03300,2020. [28] Plappert, M., Mandery, C., andAsfour, T. TheKIT
motion-languagedataset. BigData,2016.
[15] Hirai,K.,Hirose,M.,Haikawa,Y.,andTakenaka,T.
Thedevelopmentofhondahumanoidrobot. InICRA, [29] Radford, A., Narasimhan, K., Salimans, T., and
1998. Sutskever, I. Improving language understanding by
generativepre-training. 2018.
[16] Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusion
probabilisticmodels. InNeurIPS,2020.
[30] Radford,A.,Wu,J.,Child,R.,Luan,D.,Amodei,D.,
Sutskever,I.,etal. Languagemodelsareunsupervised
[17] Hochreiter,S.andSchmidhuber,J. Longshort-term
multitasklearners. 2019.
memory. Neuralcomputation,1997.
[31] Radford,A.,Kim,J.W.,Hallacy,C.,Ramesh,A.,Goh,
[18] Kajita,S.,Kanehiro,F.,Kaneko,K.,Yokoi,K.,and
G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P.,
Hirukawa,H. The3dlinearinvertedpendulummode:
Clark, J., etal. Learningtransferablevisualmodels
Asimplemodelingforabipedwalkingpatterngenera-
tion. InIROS,2001. fromnaturallanguagesupervision. InICML,2021.
[19] Kaplan, J., McCandlish, S., Henighan, T., Brown, [32] Radosavovic,I.,Shi,B.,Fu,L.,Goldberg,K.,Darrell,
T.B.,Chess,B.,Child,R.,Gray,S.,Radford,A.,Wu, T., and Malik, J. Robot learning with sensorimotor
J.,andAmodei,D. Scalinglawsforneurallanguage pre-training. InCoRL,2023.
models. arXiv:2001.08361,2020.
[33] Radosavovic,I.,Xiao,T.,Zhang,B.,Darrell,T.,Malik,
[20] Kato, I. Development of wabot 1. Biomechanism, J.,andSreenath,K. Real-worldhumanoidlocomotion
1973. withreinforcementlearning. arXiv:2303.03381,2023.
[21] Kondratyuk,D.,Yu,L.,Gu,X.,Lezama,J.,Huang,J., [34] Raibert,M.H. Leggedrobotsthatbalance. MITpress,
Hornung,R.,Adam,H.,Akbari,H.,Alon,Y.,Birod- 1986.
kar,V.,etal. Videopoet: Alargelanguagemodelfor
[35] Rajasegaran,J.,Pavlakos,G.,Kanazawa,A.,andMa-
zero-shotvideogeneration. arXiv:2312.14125,2023.
lik,J. Trackingpeoplebypredicting3dappearance,
[22] Kuindersma,S. Recentprogressonatlas,theworld’s locationandpose. InProceedingsoftheIEEE/CVF
mostdynamichumanoidrobot,2020. URLhttps: ConferenceonComputerVisionandPatternRecogni-
//youtu.be/EGABAx52GKI. tion,pp.2740–2749,2022.
[23] Loper,M.,Mahmood,N.,Romero,J.,Pons-Moll,G., [36] Ramesh,A.,Pavlov,M.,Goh,G.,Gray,S.,Voss,C.,
andBlack,M.J. Smpl: Askinnedmulti-personlinear Radford, A., Chen, M., and Sutskever, I. Zero-shot
model. In Seminal Graphics Papers: Pushing the text-to-imagegeneration. InICML,2021.
Boundaries,Volume2,2023.
[37] Shannon, C. E. Prediction and entropy of printed
[24] Mahmood,N.,Ghorbani,N.,Troje,N.F.,Pons-Moll, english. Bellsystemtechnicaljournal,1951.
G., and Black, M. J. AMASS: Archive of motion
captureassurfaceshapes. InICCV,2019. [38] Shridhar, M., Manuelli, L., and Fox, D. Perceiver-
actor: Amulti-tasktransformerforroboticmanipula-
[25] Makoviychuk,V.,Wawrzyniak,L.,Guo,Y.,Lu,M.,
tion. InCoRL,2022.
Storey,K.,Macklin,M.,Hoeller,D.,Rudin,N.,All-
shire, A., Handa, A., etal. Isaacgym: Highperfor- [39] Sohl-Dickstein,J.,Weiss,E.,Maheswaranathan,N.,
mancegpu-basedphysicssimulationforrobotlearning. and Ganguli, S. Deep unsupervised learning using
InNeurIPS,2021. nonequilibriumthermodynamics. InICML,2015.HumanoidLocomotionasNextTokenPrediction
[40] Stasse,O.,Flayols,T.,Budhiraja,R.,Giraud-Esclasse,
K.,Carpentier,J.,Mirabel,J.,DelPrete,A.,Soue`res,
P., Mansard, N., Lamiraux, F., et al. Talos: A new
humanoidresearchplatformtargetedforindustrialap-
plications. InHumanoids,2017.
[41] Todorov,E.,Erez,T.,andTassa,Y.Mujoco:Aphysics
engineformodel-basedcontrol. InIROS,2012.
[42] Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J.,
Jones,L.,Gomez,A.N.,Kaiser,Ł.,andPolosukhin,I.
Attentionisallyouneed. InNeurIPS,2017.
[43] Wu,J.,Zhang,C.,Xue,T.,Freeman,B.,andTenen-
baum,J. Learningaprobabilisticlatentspaceofob-
jectshapesvia3dgenerative-adversarialmodeling. In
NeurIPS,2016.