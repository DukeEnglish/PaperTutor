DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models
MuyangLi1* TianleCai2* JiaxinCao3 QinshengZhang4 HanCai1
JunjieBai3 YangqingJia3 Ming-YuLiu4 KaiLi2 SongHan1,4
1MIT 2Princeton 3LeptonAI 4NVIDIA
Original Naïve Patch (4 Devices) Ours (4 Devices) Ours (8 Devices)
MACs: 907T MACs Per Device: 190T (4.8× Less) MACs Per Device: 227T (4.0× Less) MACs Per Device: 113T (8.0× Less)
Latency: 12.3s Latency: 3.14s (3.9× Faster) Latency: 4.16s (3.0× Faster) Latency: 2.74s (4.5× Faster)
Prompt: Ethereal fantasy concept art of an elf, magnificent, celestial, ethereal, painterly, epic, majestic, magical, fantasy art, cover art, dreamy.
Prompt: Romantic painting of a ship sailing in a stormy sea, with dramatic lighting and powerful waves.
Figure1. WeintroduceDistriFusion,atraining-freealgorithmtoharnessmultipleGPUstoacceleratediffusionmodelinferencewithout
sacrificingimagequality.NaïvePatch(Figure2(b))suffersfromthefragmentationissueduetothelackofpatchinteraction.Thepresented
examplesaregeneratedwithSDXL[47]usinga50-stepEulersampler[19]at1280×1920resolution,andlatencyismeasuredonA100GPUs.
Abstract rent step. Therefore, our method supports asynchronous
communication,whichcanbepipelinedbycomputation. Ex-
Diffusion models have achieved great success in syn- tensiveexperimentsshowthatourmethodcanbeapplied
thesizing high-quality images. However, generating high- torecentStableDiffusionXLwithnoqualitydegradation
resolutionimageswithdiffusionmodelsisstillchallenging andachieveuptoa6.1×speeduponeightNVIDIAA100s
duetotheenormouscomputationalcosts,resultinginapro- comparedtoone. Ourcodeispubliclyavailableathttps:
hibitivelatencyforinteractiveapplications. Inthispaper, //github.com/mit-han-lab/distrifuser.
weproposeDistriFusiontotacklethisproblembyleveraging
parallelism across multiple GPUs. Our method splits the
1.Introduction
modelinputintomultiplepatchesandassignseachpatchto
aGPU.However,naïvelyimplementingsuchanalgorithm
TheadventofAI-generatedcontent(AIGC)representsa
breaks the interaction between patches and loses fidelity,
seismicshiftintechnologicalinnovation. ToolslikeAdobe
whileincorporatingsuchaninteractionwillincurtremen-
Firefly,MidjourneyandrecentSorashowcaseastonishingca-
douscommunicationoverhead. Toovercomethisdilemma,
pabilities,producingcompellingimageryanddesignsfrom
weobservethehighsimilaritybetweentheinputfromadja-
simpletextprompts. Theseachievementsarenotablysup-
centdiffusionstepsandproposedisplacedpatchparallelism,
portedbytheprogressionindiffusionmodels[13,61]. The
which takes advantage of the sequential nature of the dif-
emergenceoflargetext-to-imagemodels,includingStable
fusion process by reusing the pre-computed feature maps
Diffusion[55],Imgen[57],eDiff-I[2],DALL·E[3,49,50]
from the previous timestep to provide context for the cur-
andEmu[6],furtherexpandsthehorizonsofAIcreativity.
*indicatesequalcontributions. Trained on diverse open-web data, these models can
4202
beF
92
]VC.sc[
1v18491.2042:viXraablefordiffusionmodels,ascommunicationcostsoutweigh
U-Net … U-Net savingsfromdistributedcomputation. Thus,evenwhenmul-
tipleGPUsareavailable,theycannotbeeffectivelyexploited
(a) Original
tofurtheracceleratesingle-imagegeneration.Thismotivates
U-Net U-Net … U-Net thedevelopmentofamethodthatcanutilizemultipleGPUs
tospeedupsingle-imagegenerationwithdiffusionmodels.
A naïve approach would be to divide the image into
U-Net U-Net … U-Net severalpatches,assigningeachpatchtoadifferentdevicefor
generation,asillustratedinFigure2(b). Thismethodallows
(b) Naïve Patch
for independent and parallel operations across devices.
U-Net U-Net U-Net … U-Net However, it suffers from a clearly visible seam at the
boundariesofeachpatchduetotheabsenceofinteraction
…
⨁ ⨁ ⨁ between the individual patches. However, introducing
interactionsamongpatchestoaddressthisissuewouldincur
U-Net U-Net U-Net … U-Net
excessivesynchronizationcostsagain,offsettingthebenefits
(c) DistriFusion ofparallelprocessing.
Computation Use Activation Synchronous/Asynchronous Communication In this work, we present DistriFusion, a method that
On Device 1 On Device 2 Synchronous/Asynchronous AllGather enables running diffusion models across multiple devices
⨁⨁
inparalleltoreducethelatencyofsingle-samplegeneration
Figure2.(a)Originaldiffusionmodelrunningonasingledevice.
withouthurtingimagequality. AsdepictedinFigure2(c),
(b)Naïvelysplittingtheimageinto2patchesacross2GPUshas
anevidentseamattheboundaryduetotheabsenceofinteraction our approach is also based on patch parallelism, which
acrosspatches.(c)DistriFusionemployssynchronouscommunica- divides the image into multiple patches, each assigned to
tionforpatchinteractionatthefirststep.Afterthat,wereusethe a different device. Our key observation is that the inputs
activationsfromthepreviousstepviaasynchronouscommunica- across adjacent denoising steps in diffusion models are
tion.Inthisway,thecommunicationoverheadcanbehiddeninto similar. Therefore,weadoptsynchronouscommunication
thecomputationpipeline. solely for the first step. For the subsequent steps, we
reusethepre-computedactivationsfromthepreviousstep
generatephotorealisticimagesfromtextdescriptionsalone. to provide global context and patch interactions for the
Suchtechnologicalrevolutionunlocksnumeroussynthesis currentstep. Wefurtherco-designaninferenceframework
andeditingapplicationsforimagesandvideos,placingnew to implement our algorithm. Specifically, our framework
demands on responsiveness: by interactively guiding and effectively hides the communication overhead within
refiningthemodeloutput,userscanachievemoreperson- the computation via asynchronous communication. It
alizedandpreciseresults. Nonetheless,acriticalchallenge also sparsely runs the convolutional and attention layers
remains–highresolutionleadingtolargecomputation. For exclusivelyontheassignedregions,therebyproportionally
example, the original Stable Diffusion [55] is limited to reducing per-device computation. Our method, distinct
generating512×512images. Later, SDXL[47]expands fromdata,tensor,orpipelineparallelism,introducesanew
thecapabilitiesto1024×1024images. Morerecently,Sora parallelizationopportunity: displacedpatchparallelism.
furtherpushestheboundariesbyenablingvideogeneration
DistriFusiononlyrequiresoff-the-shelfpre-traineddiffu-
at 1080 × 1920 resolution. Despite these advancements,
sionmodelsandisapplicabletoamajorityoffew-stepsam-
theincreasedlatencyofgeneratinghigh-resolutionimages
plers. WebenchmarkitonasubsetofCOCOCaptions[5].
presentsatremendousbarriertoreal-timeapplications.
Without loss of visual fidelity, it mirrors the performance
Recent efforts to accelerate diffusion model inference of the original Stable Diffusion XL (SDXL) [47] while
havemainlyfocusedontwoapproaches: reducingsampling reducingthecomputation*proportionallytothenumberof
steps [21, 33, 34, 37, 58, 62, 71, 74] and optimizing neu- useddevices. Furthermore,ourframeworkalsoreducesthe
ral network inference [24, 26, 27]. As computational re- latencyofSDXLU-Netforgeneratingasingleimagebyup
sources grow rapidly, leveraging multiple GPUs to speed to1.8×,3.4×and6.1×with2,4,and8A100GPUs,respec-
upinferenceisappealing. Forexample,innaturallanguage tively.Whencombinedwithbatchsplittingforclassifier-free
processing(NLP),largelanguagemodelshavesuccessfully guidance[12],weachieveintotal3.6×and6.6×speedups
harnessedtensorparallelismacrossGPUs,significantlyre- using4and8A100GPUsfor3840×3840images,respec-
ducing latency. However, for diffusion models, multiple tively. SeeFigure1forsomeexamplesofourmethod.
GPUsareusuallyonlyusedforbatchinference. Whengen-
eratingasingleimage,typicallyonlyoneGPUisinvolved
*Followingpreviousworks,wemeasurethecomputationalcostwiththe
(Figure2(a)).Techniquesliketensorparallelismarelesssuit- numberofMultiply-Accumulateoperations(MACs).1MAC=2FLOPs.2.RelatedWork bysplittingtheinputintosmallpatches. Comparedtotensor
parallelism,suchaschemehassuperiorindependenceand
Diffusionmodels.Diffusionmodelshavesignificantlytrans- reduced communication demands. Additionally, it favors
formedthelandscapeofcontentgeneration[2,13,42,47]. theuseofAllGatheroverAllReducefordatainterac-
At its core, these models synthesize content through an tion,significantlyloweringoverhead(seeSection5.3forthe
iterativedenoisingprocess. Althoughthisiterativeapproach fullcomparisons). Drawinginspirationfromthesuccessof
yields unprecedented capabilities for content generation, asynchronouscommunicationinparallelcomputing[68],we
itrequiressubstantiallymorecomputationalresourcesand furtherreusethefeaturesfromthepreviousstepascontext
results in slower generative speed. This issue intensifies forcurrentsteptooverlapcommunicationandcomputation,
withthesynthesisofhigh-dimensionaldata,suchashigh- calleddisplacedpatchparallelism. Thisrepresentsthefirst
resolution [9, 14] or 360◦ images [76]. Researchers have parallelismstrategytailoredtothesequentialcharacteristics
investigatedvariousperspectivestoacceleratethediffusion ofdiffusionmodelswhileavoidingtheheavycommunication
model. Thefirstlineliesindesigningmoreefficientdenois- costsoftraditionaltechniquesliketensorparallelism.
ingprocesses. Rombachetal.[55]andVahdatetal.[67]
Sparsecomputation. Sparsecomputationhasbeenexten-
propose to compress high-resolution images into low-
sivelyresearchedinvariousdomains,includingweight[10,
resolutionlatentrepresentationsandlearndiffusionmodelin
16,22,32],input[54,65,66]andactivation[7,18,24,25,
latentspace. Anotherlineliesinimprovingsamplingviade-
43, 53, 53, 59]. In the activation domain, to facilitate on-
signingefficienttraining-freesamplingalgorithms. Alarge
hardwarespeedups,severalstudiesproposetousestructured
categoryofworksalongthislineisbuiltupontheconnection
sparsity. SBNet[53]employsaspatialmasktosparsifyacti-
between diffusion models and differential equations [63],
vationsforaccelerating3Dobjectdetection. Thismaskcan
and leverage a well-established exponential integra-
bederivedeitherfrompriorproblemknowledgeoranauxil-
tor[33,74,75]toreducesamplingstepswhilemaintaining
iarynetwork. Inthecontextofimagegeneration,SIGE[24]
numerical accuracy. The third strategy involves distilling
leveragesthehighlystructuredsparsityofuseredits,selec-
fastergenerativemodelsfrompre-traineddiffusionmodels.
tivelyperformingcomputationattheeditedregionstospeed
Despitesignificantprogressmadeinthisarea,aqualitygap
upGANs[8]anddiffusionmodels. MCUNetV2[30]adopts
persists between these expedited generators and diffusion
apatch-basedinferencetoreducememoryusageforimage
models[20,37,58]. Inadditiontotheaboveschemes,some
classificationanddetection. Inourwork,wealsopartition
worksinvestigatehowtooptimizetheneuralinferencefor
theinputintopatches,eachprocessedbyadifferentdevice.
diffusion models [24, 26, 27]. In this work, we explore
However,wefocusonreducingthelatencybyparallelism
a new paradigm for accelerating diffusion by leveraging
forimagegenerationinstead.Eachdevicewillsolelyprocess
parallelismtotheneuralnetworkonmultipledevices.
theassignedregionstoreducetheper-devicecomputation.
Parallelism. Existingworkhasexploredvariousparallelism
3.Background
strategiestoacceleratethetrainingandinferenceoflargelan-
guagemodels(LLMs),includingdata,pipeline[15,28,39],
Togenerateahigh-qualityimage,adiffusionmodeloften
tensor [17, 40, 72, 73, 79], and zero-redundancy paral-
trains a noise-prediction neural model (e.g., U-Net [56])
lelism[48,51,52,78]. Tensorparallelisminparticularhas
ϵ . Starting from pure Gaussian noise x ∼ N(0,I), it
θ T
beenwidelyadoptedforacceleratingLLMs[29],asLLM
involvestenstohundredsofiterativedenoisingstepstoget
inference tends to be memory-bound. In such scenarios,
the final clean image x , where T is the total number of
0
the communication overhead introduced by tensor paral-
steps. Specifically,giventhenoisyimagex attimestept,
t
lelismisrelativelyminorcomparedtothesubstantiallatency
themodelϵ takesx ,tandanadditionalconditionc(e.g.,
θ t
benefits brought by increased memory bandwidth. How-
text)asinputstopredictthecorrespondingnoiseϵ within
t
ever, the situation differs for diffusion models, which are
x . Ateachdenoisingstep, x canbederivedfromthe
t t−1
compute-bound. Fordiffusionmodels,thecommunication
followingequation:
overheadfromtensorparallelismbecomesasignificantfac-
tor,overshadowingtheactualcomputationtime. Asaresult, x =Update(x ,t,ϵ ), ϵ =ϵ (x ,t,c). (1)
t−1 t t t θ t
only data parallelism has been used thus far for diffusion
model serving, which provides no latency improvements. Here,‘Update’referstoasampler-specificfunctionthattyp-
TheonlyexceptionisParaDiGMS[60],whichusesPicard icallyincludeselement-wiseadditionsandmultiplications.
iteration to run multiple steps in parallel. However, this Therefore,theprimarysourceoflatencyinthisprocessis
sampler tends to waste much computation, and the gener- theforwardpassesthroughmodelϵ . Forexample,Stable
θ
atedresults exhibitsignificantdeviationfromthe original DiffusionXL[47]requires6,763GMACspersteptogen-
diffusionmodel. Ourmethodisbasedonpatchparallelism, erate a 1024×1024 image. This computational demand
whichdistributesthecomputationacrossmultipledevices escalatesmorethanquadraticallywithincreasingresolution,makingthelatencyforgeneratingasinglehigh-resolution patchonthei-thdevice,denotedasAl,(i). Thispatchisfirst
t
imageimpracticallyhighforreal-worldapplications. Fur- scattered into the stale activations from the previous step,
thermore,giventhatx dependsonx ,parallelcomputa- Al ,atitscorrespondingspatiallocation(themethodfor
t−1 t t+1
tionofϵ andϵ ischallenging. Hence,evenwithmulti- obtainingAl willbediscussedlater). Here,Al isinfull
t t−1 t+1 t+1
pleidleGPUs,acceleratingthegenerationofasinglehigh- spatialshape. IntheScatteroutput,onlythe 1 regions
N
resolutionimageremainstricky. Recently,Shihetal.intro- whereAl,(i) isplacedarefreshandrequirerecomputation.
t
ducedParaDiGMS[60],employingPicarditerationstoparal-
We then selectively apply the layer operation F (linear,
l
lelizethedenoisingstepsinadata-parallelmanner.However,
convolution,orattention)tothesefreshareas,therebygener-
ParaDiGMSwastesthecomputationonspeculativeguesses
atingtheoutputforthecorrespondingregions. Thisprocess
that fail quality thresholds. It also relies on a large total
isrepeatedforeachlayer. Finally,theoutputsfromalllayers
stepcountT toexploitmulti-GPUdataparallelism,limiting
aresynchronizedtogethertoapproximateϵ (x ). Through
θ t
itspotentialapplications. Anotherconventionalmethodis thismethodology,eachdeviceisresponsibleforonly 1 of
N
sharding the model on multiple devices and using tensor
thetotalcomputations,enablingefficientparallelization.
parallelismforinference. However,thismethodsuffersfrom
Therestillremainsaproblemofhowtoobtainthestale
intolerablecommunicationcosts,makingitimpracticalfor
activationsfromthepreviousstep. AsshowninFigure3,at
real-world applications. Beyond these two schemes, are
eachtimestept,whendeviceiacquiresAl,(i),itwillthen
therealternativestrategiesfordistributingworkloadsacross t
broadcasttheactivationstoallotherdevicesandperformthe
multipleGPUdevicessothatsingle-imagegenerationcan
AllGatheroperation. ModernGPUsoftensupportasyn-
alsoenjoythefree-lunchspeedupsfrommultipledevices?
chronous communication and computation, which means
thatthisAllGatherprocessdoesnotblockongoingcom-
4.Method
putations. Bythetimewereachlayerlinthenexttimestep,
eachdeviceshouldhavealreadyreceivedareplicateofAl.
ThekeyideaofDistriFusionistoparallelizecomputation t
Such an approach effectively hides communication over-
acrossdevicesbysplittingtheimageintopatches. Naïvely,
headswithinthecomputationphase,asshowninFigure4.
this can be done by either (1) independently computing
However,thereisanexception: theveryfirststep(i.e.,x ).
patchesandstitchingthemtogether, or(2)synchronously T
Inthisscenario,eachdevicesimplyexecutesthestandard
communicating intermediate activations between patches.
synchronous communication and caches the intermediate
However,thefirstapproachleadstovisiblediscrepanciesat
activationsforthenextstep.
theboundariesofeachpatchduetotheabsenceofinteraction
betweenthem(seeFigure1andFigure2(b)). Thesecond Sparseoperations. Foreachlayerl,wemodifytheoriginal
approach,ontheotherhand,incursexcessivecommunica- operatorF toenablesparsecomputationselectivelyonthe
l
tionoverheads,negatingthebenefitsofparallelprocessing. fresh areas. Specifically, if F is a convolution, linear, or
l
Toaddressthesechallenges,weproposeanovelparallelism cross-attentionlayer,weapplytheoperatorexclusivelyto
paradigm,displacedpatchparallelism,whichleveragesthe thenewlyrefreshedregions,ratherthanthefullfeaturemap.
sequentialnatureofdiffusionmodelstooverlapcommunica- Thiscanbeachievedbyextractingthefreshsectionsfrom
tionandcomputation. Ourkeyinsightisreusingslightlyout- thescatteroutputandfeedingthemintoF . Forlayers
l
dated,or‘stale’activationsfromthepreviousdiffusionstep where F is a self-attention layer, we transform it into a
l
tofacilitateinteractionsbetweenpatches,whichwedescribe cross-attentionlayer,similartoSIGE[24]. Inthissetting,
asactivationdisplacement. Thisisbasedontheobservation onlythequerytokensfromthefreshareasarepreservedon
thattheinputsforconsecutivedenoisingstepsarerelatively thedevice,whilethekeyandvaluetokensstillencompass
similar. Consequently, computing each patch’s activation the entire feature map (the scatter output). Thus, the
atalayerdoesnotrelyonotherpatches’freshactivations, computationalcostforF isexactlyproportionaltothesize
l
allowingcommunicationtobehiddenwithinsubsequentlay- ofthefresharea.
ers’computation.Wewillnextprovideadetailedbreakdown
CorrectedasynchronousGroupNorm. Diffusionmodels
ofeachaspectofouralgorithmandsystemdesign.
often adopt group normalization (GN) [41, 69] layers in
Displacedpatchparallelism. AsshowninFigure3,when the network. These layers normalize across the spatial
predictingϵ θ(x t)(weomittheinputsoftimesteptandcon- dimension, necessitating the aggregation of activations to
ditionchereforsimplicity),wefirstsplitx t intomultiple restoretheirfullspatialshape. InSection5.3,wediscover
patchesx(1),x(2),...,x(N),whereN isthenumberofde- that either normalizing only the fresh patches or reusing
t t t
vices. Forexample,weuseN =2inFigure3. Eachdevice stalefeaturesdegradesimagequality. However,aggregating
has a replicate of the model ϵ θ and will process a single all the normalization statistics will incur considerable
patchindependently,inparallel. overheadduetothesynchronouscommunication. Tosolve
For a given layer l, let’s consider the input activation thisdilemma,weadditionallyintroduceacorrectiontermtoComputation Asynchronous Communication
… … … …
A1
t+1
Al
t+1
AL
t+1
OP
Scatter
F
l
x( t1) Al t,(1) ≈F l(Al t)(1)
Layer 1 … … Layer L
xt
Al t,(2) Al
t+1 ≈ϵ θ(xt)
OP
Scatter
F
l
x( t2) AllGather Layer l ≈F l(Al t)(2)
Update
…xt−1 …A1 t …Al t …AL t
Figure3.OverviewofDistriFusion.Forsimplicity,weomittheinputsoftandc,anduseN =2devicesasanexample.Superscripts(1)and
(2)representthefirstandthesecondpatch,respectively.Staleactivationsfromtheprevioussteparedarkened.Ateachstept,wefirstsplit
theinputx intoN patchesx(1),...,x(N).Foreachlayerlanddevicei,upongettingtheinputactivationpatchesAl,(i),twooperations
t t t t
thenprocessasynchronously:First,ondevicei,Al,(i)isscatteredbackintothestaleactivationAl fromthepreviousstep.Theoutput
t t+1
ofthisScatteroperationisthenfedintothesparseoperatorF (linear,convolution,orattentionlayers),whichperformscomputations
l
exclusivelyonthefreshregionsandproducesthecorrespondingoutput.Meanwhile,anAllGatheroperationisperformedoverAl,(i)
t
topreparethefullactivationAl forthenextstep.Werepeatthisprocedureforeachlayer.Thefinaloutputsarethenaggregatedtogether
t
toapproximateϵ (x ),whichisusedtocomputex .Thetimelinevisualizationofeachdeviceforpredictingϵ (x )isshowninFigure4.
θ t t−1 θ t
Layer 1 Layer 2 Layer L Warm-upsteps. AsobservedineDiff-I[2]andFastCom-
D 1e -v Nice Scatter Sparse Op F
1
Scatter Sparse Op F
2
Scatter Sparse Op FL poser [70], the behavior of diffusion synthesis undergoes
… qualitative changes throughout the denoising process.
Comm. AllGather AllGather AllGather Specifically, the initial steps of sampling predominantly
shapethelow-frequencyaspectsoftheimage,suchasspatial
Figure4. Timelinevisualizationoneachdevicewhenpredicting
ϵ (x ). Comm. means communication, which is asynchronous layoutandoverallsemantics. Asthesamplingprogresses,
θ t
with computation. The AllGather overhead is fully hidden thefocusshiftstorecoveringlocalhigh-frequencydetails.
withinthecomputation. Therefore, to boost image quality, especially in samplers
withareducednumberofsteps,weadoptwarm-upsteps. In-
thestalestatistics. Specifically,foreachdeviceiatagiven
steadofdirectlyemployingdisplacedpatchparallelismafter
stept,everyGNlayercancomputethegroup-wisemeanof
itsfreshpatchA(i),denotedasE[A(i)]. Forsimplicity,we thefirststep,wecontinuewithseveraliterationsofthestan-
t t dardsynchronouspatchparallelismasapreliminaryphase,
omitthelayerindexlhere. Italsohascachedthelocalmean
orwarm-up. AsdetailedinSection5.3,thisintegrationof
E[A(i) ] and aggregated global mean E[A ] from the
t+1 t+1 warm-upstepssignificantlyimprovesperformance.
previousstep. ThentheapproximatedglobalmeanE[A ]
t
forcurrentstepondeviceicanbecomputedas
5.Experiments
E[A ]≈ E[A ] +(E[A(i)]−E[A(i) ]). (2)
t t+1 t t+1 We first describe our experiment setups, including our
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
staleglobalmean correction benchmark datasets, baselines, and evaluation protocols.
Thenwepresentourmainresultsregardingbothqualityand
WeusethesametechniquetoapproximateE[(A )2],then
t efficiency. Finally,wefurthershowsomeablationstudiesto
the variance can be approximated as E[(A )2] − E[A ]2.
t t verifyeachdesignchoice.
WethenusetheseapproximatedstatisticsfortheGNlayer
andinthemeantimeaggregatethelocalmeanandvariance 5.1.Setups
tocomputethepreciseonesusingasynchronouscommuni-
cation. Thus,thecommunicationcostcanalsobepipelined Models. As our method only requires off-the-shelf pre-
intothecomputation. Weempiricallyfindthismethodyields traineddiffusionmodels,wemainlyconductexperimentson
comparable results to the direct synchronous aggregation. thestate-of-the-artpublictext-to-imagemodelStableDif-
However,therearesomerarecaseswheretheapproximated fusionXL(SDXL)[47]. SDXLfirstcompressesanimage
varianceisnegative. Forthesenegativevariancegroups,we to an 8× smaller latent representation using a pre-trained
willfallbacktousethelocalvarianceofthefreshpatch. auto-encoderandthenappliesadiffusionmodelinthislatentOriginal Naïve Patch (2 Devices) ParaDiGMS (8 Devices) Ours (2 Devices) Ours (4 Devices) Ours (8 Devices)
Latency: 5.02s Latency: 2.83s (1.8× Faster) Latency: 1.80s (2.8× Faster) Latency: 3.35s (1.5× Faster) Latency: 2.26s (2.2× Faster) Latency: 1.77s (2.8× Faster)
FID: 24.0 FID: 33.6 FID: 25.1 FID: 24.0 FID: 24.2 FID: 24.3
Prompt: A multi-colored parrot holding its foot up to its beak.
Prompt: A kid wearing headphones and using a laptop
Prompt: A pair of parking meters reflecting expired times.
PPrroommpptt:: AA ddoouubbllee ddeecckkeerr bbuuss ddrriivviinngg ddoowwnn tthhee ssttrreeeett..
Prompt: A brown dog laying on the ground with a metal bowl in front of him.
Figure5.Qualitativeresults.FIDiscomputedagainsttheground-truthimages.OurDistriFusioncanreducethelatencyaccordingtothe
numberofuseddeviceswhilepreservingvisualfidelity.
space. Italsoincorporatesmultiplecross-attentionlayersto fromthevalidationset,whichcontains5Kimageswithone
facilitatetextconditioning. ComparedtotheoriginalStable captionperimage.
Diffusion [55], SDXL adopts significantly more attention
Baselines. WecompareourDistriFusionagainstthefollow-
layers,resultinginamorecomputationallyintensivemodel.
ingbaselinesintermsofbothqualityandefficiency:
Datasets. We use the HuggingFace version of COCO • NaïvePatch. Ateachiteration,theinputisdividedrow-
Captions2014[5]datasettobenchmarkourmethod. This wiseorcolumn-wisealternately. Thesepatchesarethen
dataset contains human-generated captions for images processed independently by the model, without any in-
from Microsoft Common Objects in COntext (COCO) teraction between them. The outputs are subsequently
dataset[31]. Forevaluation,werandomlysampleasubset concatenatedtogether.LPIPS(↓) FID(↓) Latency
#Steps #Devices Method PSNR(↑) MACs(T)
w/G.T. w/Orig. w/Orig. w/G.T. Value(s) Speedup
1 Original – 0.797 – 24.0 – 338 5.02 –
NaïvePatch 28.2 0.812 0.596 33.6 29.4 322 2.83 1.8×
2
Ours 31.9 0.797 0.146 24.2 4.86 338 3.35 1.5×
NaïvePatch 27.9 0.853 0.753 125 133 318 1.74 2.9×
50 4
Ours 31.0 0.798 0.183 24.2 5.76 338 2.26 2.2×
NaïvePatch 27.8 0.892 0.857 252 259 324 1.27 4.0×
8 ParaDiGMS 29.3 0.800 0.320 25.1 10.8 657 1.80 2.8×
Ours 30.5 0.799 0.211 24.4 6.46 338 1.77 2.8×
1 Original – 0.801 – 23.9 – 169 2.52 –
25 ParaDiGMS 29.6 0.808 0.273 25.8 10.4 721 1.89 1.3×
8
Ours 31.5 0.802 0.161 24.6 5.67 169 0.93 2.7×
Table1.Quantitativeevaluation.MACsmeasurescumulativecomputationacrossalldevicesforthewholedenoisingprocessforgenerating
asingle1024×1024image.w/G.T.meanscalculatingthemetricswiththeground-truthimages,whilew/Orig.meanswiththeoriginal
model’ssamples.ForPSNR,wereportthew/Orig.setting.Ourmethodmirrorstheresultsoftheoriginalmodelacrossallmetricswhile
maintainingthetotalMACs.ItalsoreducesthelatencyonNVIDIAA100GPUsinproportiontothenumberofuseddevices.
• ParaDiGMS[60]isatechniquetoacceleratepre-trained Original Ours (2 Devices) Ours (4 Devices) Ours (8 Devices)
6.0 28 160
diffusionmodelsbydenoisingmultiplestepsinparallel. It
usesPicarditerationstoguessthesolutionoffuturesteps 5.02 23.7 140
4.5 21 120
a sin zd ei 8te fr oa rti Pve al ry aDre ifi Gn Mes Sit toun alt ii gl nco wnv ite hrg Te an bc lee. 4W ine tu hs ee oa rib ga it nc ah
l 3.0
1 3. .5 35× 2.2×2.8×
14
1.8×
3.1×4.9× 80
1.8×
3.4× 6.1×
13.3 76.6
paper[60]. Weempiricallyfindthissettingyieldsthebest
2.26
performanceinbothqualityandlatency. 1.5 1.77 7 7.60 40 41.3
4.81 22.9
Metrics. Following previous works [23, 24, 38, 44], we 0.0 0 0
1024×1024 2048×2048 3840×3840
evaluatetheimagequalitywithstandardmetrics: PeakSig-
Figure6.MeasuredtotallatencyofDistriFusionwiththe50-step
nalNoiseRatio(PSNR,higherisbetter),LPIPS(loweris DDIMsampler[62]forgeneratingasingleimageacrossdifferent
better)[77],andFréchetInceptionDistance(FID,loweris resolutions on NVIDIA A100 GPUs. When scaling up the res-
better)[11]†. WeemployPSNRtoquantifytheminornu- olution,theGPUdevicesarebetterutilized. Remarkably,when
mericaldifferencesbetweentheoutputsofthebenchmarked generating3840×3840images,DistriFusionachieves1.8×,3.4×
method and the original diffusion model outputs. LPIPS and6.1×speedupswith2,4,and8A100s,respectively.
isusedtoevaluateperceptualsimilarity. Additionally,the
FIDscoreisusedtomeasurethedistributionaldifferences 5.2.MainResults
betweentheoutputsofthemethodandeithertheoriginal
outputsortheground-truthimages. Quality results. In Figure 5, we show some qualitative
visual results and report some quantitative evaluation in
Implementationdetails. Bydefault,weadoptthe50-step
Table 1. with G.T. means computing the metric with the
DDIM sampler [62] with classifier-free guidance scale 5
ground-truthCOCO[31]images,whereasw/Orig. refers
to generate 1024×1024 images, unless otherwise speci-
tocomputingthemetricswiththeoutputsfromtheoriginal
fied. Inadditiontothefirststep,weperformanother4-step
model. ForPSNR,wereportonlythew/Orig. setting,as
synchronouspatchparallelism,servingasawarm-upphase.
thew/G.T.comparisonisnotinformativeduetosignificant
We use PyTorch 2.2 [46] to benchmark the speedups
numericaldifferencesbetweenthegeneratedoutputsandthe
of our method. To measure latency, we first warm up the
ground-truthimages.
devices with 3 iterations of the whole denoising process,
AsshowninTable1,ParaDiGMS[60]expendsconsid-
then run another 10 iterations and calculate the average
erablecomputationalresourcesonguessingfuturedenoising
latencybydiscardingtheresultsofthefastestandslowest
steps, resulting in a much higher total MACs. Besides, it
runs. Additionally,weuseCUDAGraphtooptimizesome
alsosuffersfromsomeperformancedegradation. Incontrast,
kernellaunchingoverheadforboththeoriginalmodeland
our method simply distributes workloads across multiple
ourmethod.
GPUs,maintainingaconstanttotalcomputation. TheNaïve
†WeuseTorchMetricstocalculatePSNRandLPIPS,anduseClean- Patchbaseline,whilelowerintotalMACs,lacksthecrucial
FID[45]tocalculateFID. inter-patchinteraction,leadingtofragmentedoutputs. This
)s(
ycnetaL1024×1024 2048×2048 3840×3840 Step 9 Input x9 Step 8 Input x8 Input Difference |x9−x8|
Method
Comm. Latency Comm. Latency Comm. Latency
Original – 5.02s – 23.7s – 140s
Sync.TP 1.33G 3.61s 5.33G 11.7s 18.7G 46.3s
Sync.PP 0.42G 2.21s 1.48G 5.62s 5.38G 24.7s
DistriFusion(Ours) 0.42G 1.77s 1.48G 4.81s 5.38G 22.9s
NoComm. – 1.48s – 4.14s – 21.3s
Figure7.Visualizationoftheinputsfromsteps9and8andtheir
Table2. Communicationcostcomparisonswith8A100sacross
difference.Allfeaturemapsarechannel-wiseaveraged.Thediffer-
different resolutions. Sync. TP/PP: Synchronous tensor/patch
enceisnearlyallzero,exhibitinghighsimilarity.
parallelism. NoComm.: AnidealnocommunicationPP.Comm.
measuresthetotalcommunicationamount.PPonlyrequiresless
than 1 communicationamountscomparedtoTP.OurDistriFusion
3 PP eliminates the need for communication within cross-
furtherreducesthecommunicationoverheadby50∼60%.
attentionandlinearlayers. Forconvolutionallayers,commu-
limitationsignificantlyimpactsimagequality,asreflected nicationisonlyrequiredatthepatchboundaries,whichrep-
across all evaluation metrics. Our DistriFusion can well resentaminimalportionoftheentiretensor. Moreover,PP
preserveinteraction. Evenwhenusing8devices,itachieves utilizes AllGather over AllReduce, leading to lower
comparablePSNR,LPIPS,andFIDscorescomparableto communicationdemandsandnoadditionaluseofcomputing
thoseoftheoriginalmodel. resources. Therefore,PPrequires60%fewercommunica-
tion amounts and is 1.6 ∼ 2.1× faster than TP. We also
Speedups. Comparedtothetheoreticalcomputationreduc-
includeatheoreticalPPbaselinewithoutanycommunica-
tion,on-hardwareaccelerationismorecriticalforreal-world
tion(NoComm.) todemonstratethecommunicationover-
applications.Todemonstratetheeffectivenessofourmethod,
headinSync. PPandDistriFusion. ComparedtoSync. PP,
wealsoreporttheend-to-endlatencyinTable1on8NVIDIA
DistriFusionfurthercutssuchoverheadbyover50%. The
A100GPUs. Inthe50-stepsetting, ParaDiGMSachieves
remainingoverheadmainlycomesfromourcurrentusage
anidenticalspeedupof2.8×toourmethodatthecostof
ofNVIDIACollectiveCommunicationLibrary(NCCL)for
compromisedimagequality(seeFigure5). Inthemorecom-
asynchronouscommunication. NCCLkernelsuseSMs(the
monlyused25-stepsetting,ParaDiGMSonlyhasamarginal
computingresourcesonGPUs),whichwillslowdownthe
1.3×speedupduetoexcessivewastedguesses,whichisalso
overlappedcomputation. Usingremotememoryaccesscan
reportedinShihetal.[60]. However,ourmethodcanstill
bypassthisissueandclosetheperformancegap.
mirrortheoriginalqualityandacceleratethemodelby2.7×.
Whengenerating1024×1024images,ourspeedupsare Inputsimilarity. Ourdisplacedpatchparallelismrelieson
limitedbythelowGPUutilizationofSDXL.Tomaximize theassumptionthattheinputsfromconsecutivedenoising
deviceusage,wefurtherscaletheresolutionto2048×2048 stepsaresimilar. Tosupportthisclaim, wequantitatively
and 3840×3840 in Figure 6. At these larger resolutions, calculatethemodelinputdifferenceacrossallconsecutive
the GPU devices are better utilized. Specifically, for steps using a 50-step DDIM sampler. The average differ-
3840×3840 images, DistriFusion reduces the latency by enceisonly0.02,withintheinputrangeof[−4,4](about
1.8×, 3.4×and6.1×with2, 4and8A100s, respectively. 0.3%). Figure 7 further qualitatively visualizes the input
Note that these results are benchmarked with PyTorch. differencebetweensteps9and8(randomlyselected). The
With more advanced compilers, such as TVM [4] and differenceisnearlyallzero,substantiatingourhypothesisof
TensorRT[1],weanticipateevenhigherGPUutilizationand highsimilaritybetweeninputsfromneighboringsteps.
consequentlymorepronouncedspeedupsfromDistriFusion,
asobservedin SIGE[24]. Inpracticaluse, thebatchsize Few-stepsamplingandwarm-upsteps. Asstatedabove,
often doubles due to classifier-free guidance [12]. We ourapproachhingesontheobservationthatadjacentdenois-
can first split the batch and then apply DistriFusion to ing steps share similar inputs, i.e., x t ≈ x t−1. However,
eachbatchseparately. Thisapproachfurtherimprovesthe as we increase the step size and thereby reduce the num-
total speedups to 3.6× and 6.6× with 4 and 8 A100s for berofsteps,theapproximationerrorescalates,potentially
generatingasingle3840×3840image,respectively. compromisingtheeffectivenessofourmethod. InFigure8,
wepresentresultsusing10-stepDPM-Solver[33,34]. The
5.3.AblationStudy 10-stepconfigurationisthethresholdforthetraining-free
samplerstomaintaintheimagequality. Underthissetting,
Communication cost. In Table 2, we benchmark our la- naïveDistriFusionwithoutwarm-upstrugglestopreserve
tencywithsynchronoustensorparallelism(Sync. TP)and theimagequality. However,incorporatinganadditionaltwo-
synchronous patch parallelism (Sync. PP), and report the stepwarm-upsignificantlyrecoverstheperformancewith
correspondingcommunicationamounts. ComparedtoTP, onlyslightlyincreasedlatency.LatO enr cig yi :n 1a .l 01s Ou Lrs L a t( P ew I nP/ co S y : :W 0 0.a .4 3r 0m 74 4- sup) Our Ls L a(1 tP e- I nS P ct Se yp : : 0 0W . .2 3a 8 8r 8m 8s-up) Our Ls L a(2 P te- I nS P ct Se yp : : 00W .. 41a 09rm 06 s-up) LatO enr cig yi :n 5a .l 02s LLS aPe tep IP na cSra y:t :e 0 1 .G 3 .61N 47 s LL aP tS eIt P na cSle y: :G 0 1.N 2 .74 67 s LL aPS teIy P nn cSc y:. : 0G 1.2N .80 57 s LL aP teIP nO cSu y: r :0 s 1.2 .71 71 s
Prompt: A kitchen with a microwave, stove, cutlery and fruits.
Prompt: A small boat in the blue and green water.
Prompt: An old clock reading two twenty on a gloomy day.
Figure9.QualitativeresultsofdifferentGNschemeswith8A100s.
LPIPSiscomputedagainsttheoriginalsamplesoverthewhole
Prompt: A motorcylce sits on the pavement on a cloudy day. COCO[5]dataset.SeparateGNonlyutilizesthestatisticsfromthe
Figure8.Qualitativeresultsonthe10-stepDPM-Solver[33,34] on-devicepatch. StaleGNreusesthestalestatistics. Theysuffer
with different warm-up steps. LPIPS is computed against the fromqualitydegradation. Sync. GNsynchronizesdatatoensure
samplesfromtheoriginalSDXLovertheentireCOCO[5]dataset. accurate statistics at the cost of extra overhead. Our corrected
Naïve DistriFusion without warm-up steps has evident quality asynchronousGN,bycorrectingstalestatistics,avoidstheneedfor
degradation.Addinga2-stepwarm-upsignificantlyimprovesthe synchronizationandeffectivelyrestoresquality.
performancewhileavoidinghighlatencyrise.
siontomaximizethespeedup. However,NVLinkhasbeen
GroupNorm. AsdiscussedinSection4,calculatingaccu- widelyusedrecently. Moreover,quantization[26]canalso
rategroupnormalization(GN)statisticsiscrucialforpre- reducethecommunicationworkloadsforourmethod. Be-
servingimagequality.InFigure9,wecomparefourdifferent sides,DistriFusionhaslimitedspeedupsforlow-resolution
GNschemes. ThefirstapproachSeparateGN usesstatistics imagesasthedevicesareunderutilized. Advancedcompil-
fromtheon-devicefreshpatch. Thisapproachdeliversthe ers[1,4]wouldhelptoexploitthedevicesandachievebetter
bestspeedatthecostoflowerimagefidelity. Thiscompro- speedups. Ourmethodmaynotworkfortheextremely-few-
miseisparticularlysevereforlargenumbersofuseddevices, stepmethods[35–37,58,64],duetotherapidchangesofthe
duetoinsufficientpatchsizeforprecisestatisticsestimation. denoisingstates. Yetourpreliminaryexperimentsuggests
ThesecondschemeStaleGN computesstatisticsusingstale thatslightlymoresteps(e.g.,10)areenoughforDistriFusion
activations.However,thismethodalsofacesqualitydegrada- toobtainhigh-qualityresults.
tion,becauseofthedifferentdistributionsbetweenstaleand
Societalimpacts. Inthispaper,weproposeanewmethod
freshactivations,oftenresultinginimageswithafog-like
toacceleratediffusionmodelsbyleveragingmultipleGPUs,
noiseeffect. ThethirdapproachSync. GN usesynchronized
which can enable more responsive interactions for users.
communication to aggregate accurate statistics. Though
Byreducinglatency,ouradvancementsleadtoasmoother
achievingthebestimagequality,itsuffersfromlargesyn-
creativeprocessinapplicationslikeimageediting.
chronizationoverhead. Ourmethodusesacorrectiontermto
However,aswithmanygenerativemodels,thereispoten-
closethedistributiongapbetweenthestaleandfreshstatis-
tialformisuse. Werecognizetheneedforthoughtfulgover-
tics. It achieves image quality on par with Sync. GN but
nance. Followingtheguidelinesinpreviouswork[24,38],
withoutincurringsynchronouscommunicationoverhead.
wewillclearlyspecifyauthorizedusesinourlicensetomiti-
gatepotentialharms. Bykeepingthepublicinterestinmind
6.Conclusion&Discussion
asthistechnologyevolves,wehopetoencouragecreativity
In this paper, we introduce DistriFusion to accelerate andaccessibilitywhilepromotingresponsibleinnovation.
diffusionmodelswithmultipleGPUsforparallelism. Our
method divides images into patches, assigning each to a Acknowledgments
separateGPU.Wereusethepre-computedactivationsfrom
previous steps to maintain patch interactions. On Stable WethankJun-YanZhuandLigengZhufortheirhelpful
DiffusionXL,ourmethodachievesuptoa6.1×speedupon discussionandvaluablefeedback. Theprojectissupported
8NVIDIAA100s. Thisadvancementnotonlyenhancesthe byMIT-IBMWatsonAILab,Amazon,MITScienceHub,
efficiencyofAI-generatedcontentcreationbutalsosetsa andNationalScienceFoundation.
newbenchmarkforfutureresearchinparallelcomputingfor
AIapplications. References
Limitations. To fully hide the communication overhead [1] NVIDIA/TensorRT. 2023. 8,9
within the computation, NVLink is essential for DistriFu- [2] YogeshBalaji,SeungjunNah,XunHuang,ArashVahdat,Ji-amingSong,KarstenKreis,MiikaAittala,TimoAila,Samuli [18] Patrick Judd, Alberto Delmas, Sayeh Sharify, and An-
Laine,BryanCatanzaro,etal. ediffi:Text-to-imagediffusion dreas Moshovos. Cnvlutin2: Ineffectual-activation-and-
modelswithanensembleofexpertdenoisers. arXivpreprint weight-freedeepneuralnetworkcomputing. arXivpreprint
arXiv:2211.01324,2022. 1,3,5 arXiv:1705.00125,2017. 3
[3] JamesBetker,GabrielGoh,LiJing,TimBrooks,Jianfeng [19] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine.
Wang,LinjieLi,LongOuyang,JuntangZhuang,JoyceLee, Elucidatingthedesignspaceofdiffusion-basedgenerative
YufeiGuo,etal. Improvingimagegenerationwithbettercap- models. NeurIPS,2022. 1
tions. ComputerScience.https://cdn.openai.com/papers/dall-
[20] Gwanghyun Kim and Jong Chul Ye. Diffusionclip: Text-
e-3.pdf,2023. 1
guidedimagemanipulationusingdiffusionmodels. arXiv
[4] TianqiChen,ThierryMoreau,ZihengJiang,LianminZheng,
preprintarXiv:2110.02711,2021. 3
EddieYan,HaichenShen,MeghanCowan,LeyuanWang,
[21] ZhifengKongandWeiPing. Onfastsamplingofdiffusion
YuweiHu,LuisCeze,etal. {TVM}:Anautomated{End-to-
probabilisticmodels. InICMLWorkshoponInvertibleNeu-
End}optimizingcompilerfordeeplearning. InOSDI,2018.
ralNetworks,NormalizingFlows,andExplicitLikelihood
8,9
Models,2021. 2
[5] XinleiChen,HaoFang,Tsung-YiLin,RamakrishnaVedan-
[22] HaoLi,AsimKadav,IgorDurdanovic,HananSamet,and
tam,SaurabhGupta,PiotrDollár,andCLawrenceZitnick.
HansPeterGraf. Pruningfiltersforefficientconvnets. ICLR,
Microsoft coco captions: Data collection and evaluation
2016. 3
server. arXivpreprintarXiv:1504.00325,2015. 2,6,9
[6] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang [23] MuyangLi,JiLin,YaoyaoDing,ZhijianLiu,Jun-YanZhu,
Wang,RuiWang,PeizhaoZhang,SimonVandenhende,Xiao- andSongHan. Gancompression:Efficientarchitecturesfor
fangWang,AbhimanyuDubey,etal. Emu:Enhancingimage interactiveconditionalgans. InCVPR,2020. 7
generationmodelsusingphotogenicneedlesinahaystack. [24] MuyangLi,JiLin,ChenlinMeng,StefanoErmon,SongHan,
arXivpreprintarXiv:2309.15807,2023. 1 and Jun-Yan Zhu. Efficient spatially sparse inference for
[7] XuanyiDong,JunshiHuang,YiYang,andShuichengYan. conditionalgansanddiffusionmodels. InNeurIPS,2022. 2,
Moreisless:Amorecomplicatednetworkwithlessinference 3,4,7,8,9
complexity. InCVPR,2017. 3 [25] XiaoxiaoLi,ZiweiLiu,PingLuo,ChenChangeLoy,and
[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing XiaoouTang. Notallpixelsareequal: Difficulty-awarese-
Xu,DavidWarde-Farley,SherjilOzair,AaronCourville,and manticsegmentationviadeeplayercascade. InCVPR,2017.
YoshuaBengio. Generativeadversarialnets. NeurIPS,2014. 3
3
[26] Xiuyu Li, Long Lian, Yijiang Liu, Huanrui Yang, Zhen
[9] JiataoGu,ShuangfeiZhai,YizheZhang,JoshSusskind,and
Dong, Daniel Kang, Shanghang Zhang, and Kurt Keutzer.
NavdeepJaitly. Matryoshkadiffusionmodels. arXivpreprint Q-diffusion: Quantizing diffusion models. arXiv preprint
arXiv:2310.15111,2023. 3 arXiv:2302.04304,2023. 2,3,9
[10] SongHan,JeffPool,JohnTran,andWilliamDally. Learning
[27] YanyuLi,HuanWang,QingJin,JuHu,PavloChemerys,Yun
bothweightsandconnectionsforefficientneuralnetwork.
Fu,YanzhiWang,SergeyTulyakov,andJianRen.Snapfusion:
NeurIPS,2015. 3
Text-to-imagediffusionmodelonmobiledeviceswithintwo
[11] MartinHeusel,HubertRamsauer,ThomasUnterthiner,Bern-
seconds. NeurIPS,2023. 2,3
hardNessler,andSeppHochreiter. Ganstrainedbyatwo
[28] ZhuohanLi,SiyuanZhuang,ShiyuanGuo,DanyangZhuo,
time-scaleupdateruleconvergetoalocalnashequilibrium.
HaoZhang,D.Song,andI.Stoica. Terapipe: Token-level
NeurIPS,2017. 7
pipelineparallelismfortraininglarge-scalelanguagemodels.
[12] Jonathan Ho and Tim Salimans. Classifier-free diffusion
ICML,2021. 3
guidance. InNeurIPS2021WorkshoponDeepGenerative
ModelsandDownstreamApplications,2021. 2,8 [29] ZhuohanLi, LianminZheng, YinminZhong, VincentLiu,
[13] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu- YingSheng,XinJin,YanpingHuang,Z.Chen,HaoZhang,
sionprobabilisticmodels. NeurIPS,2020. 1,3 Joseph E. Gonzalez, and I. Stoica. Alpaserve: Statistical
multiplexingwithmodelparallelismfordeeplearningserv-
[14] EmielHoogeboom,JonathanHeek,andTimSalimans.simple
ing. USENIXSymposiumonOperatingSystemsDesignand
diffusion: End-to-enddiffusionforhighresolutionimages.
arXivpreprintarXiv:2301.11093,2023. 3 Implementation,2023. 3
[15] YanpingHuang,YoulongCheng,AnkurBapna,OrhanFirat, [30] JiLin,Wei-MingChen,HanCai,ChuangGan,andSongHan.
Dehao Chen, Mia Chen, HyoukJoong Lee, Jiquan Ngiam, Mcunetv2:Memory-efficientpatch-basedinferencefortiny
QuocVLe,YonghuiWu,etal. Gpipe:Efficienttrainingof deeplearning. InAnnualConferenceonNeuralInformation
giantneuralnetworksusingpipelineparallelism. NeurIPS, ProcessingSystems(NeurIPS),2021. 3
2019. 3 [31] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,
[16] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. PietroPerona,DevaRamanan,PiotrDollár,andCLawrence
Speeding up convolutional neural networks with low rank Zitnick. Microsoft coco: Common objects in context. In
expansions. InBMVC,2014. 3 ComputerVision–ECCV2014: 13thEuropeanConference,
[17] ZhihaoJia,MateiZaharia,andAlexAiken. Beyonddataand Zurich,Switzerland,September6-12,2014,Proceedings,Part
modelparallelismfordeepneuralnetworks. MLSys,2019. 3 V13,pages740–755.Springer,2014. 6,7[32] BaoyuanLiu,MinWang,HassanForoosh,MarshallTappen, Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: an
andMariannaPensky. Sparseconvolutionalneuralnetworks. imperativestyle,high-performancedeeplearninglibrary. In
InCVPR,2015. 3 NeurIPS,2019. 7
[33] ChengLu,YuhaoZhou,FanBao,JianfeiChen,Chongxuan [47] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,
Li,andJunZhu. Dpm-solver: Afastodesolverfordiffu- TimDockhorn,JonasMüller,JoePenna,andRobinRombach.
sionprobabilisticmodelsamplinginaround10steps. arXiv Sdxl:Improvinglatentdiffusionmodelsforhigh-resolution
preprintarXiv:2206.00927,2022. 2,3,8,9 imagesynthesis. InICLR,2024. 1,2,3,5
[34] ChengLu,YuhaoZhou,FanBao,JianfeiChen,Chongxuan [48] SamyamRajbhandari,JeffRasley,OlatunjiRuwase,andYux-
Li, and Jun Zhu. Dpm-solver++: Fast solver for guided iongHe.Zero:Memoryoptimizationstowardtrainingtrillion
samplingofdiffusionprobabilisticmodels. arXivpreprint parametermodels. Sc20:InternationalConferenceForHigh
arXiv:2211.01095,2022. 2,8,9 PerformanceComputing,Networking,StorageAndAnalysis,
[35] SimianLuo,YiqinTan,LongboHuang,JianLi,andHang 2019. 3
Zhao. Latent consistency models: Synthesizing high- [49] AdityaRamesh, MikhailPavlov, GabrielGoh, ScottGray,
resolutionimageswithfew-stepinference. arXivpreprint ChelseaVoss,AlecRadford,MarkChen,andIlyaSutskever.
arXiv:2310.04378,2023. 9 Zero-shottext-to-imagegeneration. InICML,2021. 1
[36] SimianLuo,YiqinTan,SurajPatil,DanielGu,Patrickvon
[50] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu,
Platen,ApolinárioPassos,LongboHuang,JianLi,andHang
andMarkChen. Hierarchicaltext-conditionalimagegenera-
Zhao. Lcm-lora: Auniversalstable-diffusionacceleration
tionwithcliplatents. arXivpreprintarXiv:2204.06125,2022.
module. arXivpreprintarXiv:2311.05556,2023.
1
[37] ChenlinMeng,RuiqiGao,DiederikPKingma,StefanoEr-
[51] JeffRasley,SamyamRajbhandari,OlatunjiRuwase,andYux-
mon, Jonathan Ho, and Tim Salimans. On distillation of
iongHe. Deepspeed:Systemoptimizationsenabletraining
guideddiffusionmodels. arXivpreprintarXiv:2210.03142,
deeplearningmodelswithover100billionparameters.InPro-
2022. 2,3,9
ceedingsofthe26thACMSIGKDDInternationalConference
[38] ChenlinMeng,YutongHe,YangSong,JiamingSong,Jiajun
onKnowledgeDiscovery&DataMining,pages3505–3506,
Wu,Jun-YanZhu,andStefanoErmon.SDEdit:Guidedimage
2020. 3
synthesisandeditingwithstochasticdifferentialequations.
[52] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi,
InICLR,2022. 7,9
OlatunjiRuwase,ShuangyanYang,MinjiaZhang,DongLi,
[39] DeepakNarayanan,AaronHarlap,AmarPhanishayee,Vivek
andYuxiongHe. Zero-offload:Democratizingbillion-scale
Seshadri,NikhilRDevanur,GregoryRGanger,PhillipBGib-
modeltraining. In2021USENIXAnnualTechnicalConfer-
bons,andMateiZaharia. Pipedream: Generalizedpipeline
ence,USENIXATC2021,July14-16,2021,pages551–564.
parallelismfordnntraining. InSOSP,2019. 3
USENIXAssociation,2021. 3
[40] D.Narayanan,M.Shoeybi,J.Casper,P.LeGresley,M.Pat-
[53] MengyeRen,AndreiPokrovsky,BinYang,andRaquelUr-
wary,V.Korthikanti,DmitriVainbrand,PrethviKashinkunti,
tasun. Sbnet: Sparseblocksnetworkforfastinference. In
J. Bernauer, Bryan Catanzaro, Amar Phanishayee, and M.
CVPR,2018. 3
Zaharia. Efficient large-scale language model training on
[54] GernotRiegler,AliOsmanUlusoy,andAndreasGeiger. Oct-
gpuclustersusingmegatron-lm. InternationalConference
net:Learningdeep3drepresentationsathighresolutions. In
forHighPerformanceComputing,Networking,Storageand
CVPR,2017. 3
Analysis,2021. 3
[55] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
[41] AlexanderQuinnNicholandPrafullaDhariwal. Improved
denoisingdiffusionprobabilisticmodels. InICML,2021. 4 Patrick Esser, and Björn Ommer. High-resolution image
synthesiswithlatentdiffusionmodels. InCVPR,2022. 1,2,
[42] AlexanderQuinnNichol,PrafullaDhariwal,AdityaRamesh,
3,6
PranavShyam,PamelaMishkin,BobMcgrew,IlyaSutskever,
andMarkChen. Glide:Towardsphotorealisticimagegenera- [56] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-
tionandeditingwithtext-guideddiffusionmodels. InICML, net:Convolutionalnetworksforbiomedicalimagesegmenta-
2022. 3 tion. InMedicalImageComputingandComputer-Assisted
[43] BowenPan,WuweiLin,XiaolinFang,ChaoqinHuang,Bolei Intervention–MICCAI2015:18thInternationalConference,
Zhou, and Cewu Lu. Recurrent residual module for fast Munich,Germany,October5-9,2015,Proceedings,PartIII
inferenceinvideos. InCVPR,2018. 3 18,pages234–241.Springer,2015. 3
[44] TaesungPark,Ming-YuLiu,Ting-ChunWang,andJun-Yan [57] ChitwanSaharia,WilliamChan,SaurabhSaxena, LalaLi,
Zhu. Semanticimagesynthesiswithspatially-adaptivenor- JayWhang,EmilyLDenton,KamyarGhasemipour,Raphael
malization. InCVPR,2019. 7 GontijoLopes,BurcuKaragolAyan,TimSalimans,etal.Pho-
[45] Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On torealistictext-to-imagediffusionmodelswithdeeplanguage
aliasedresizingandsurprisingsubtletiesinGANevaluation. understanding. NeurIPS,2022. 1
InIEEE/CVFConferenceonComputerVisionandPattern [58] TimSalimansandJonathanHo. Progressivedistillationfor
Recognition,CVPR2022,NewOrleans,LA,USA,June18-24, fastsamplingofdiffusionmodels. InICLR,2021. 2,3,9
2022,pages11400–11410.IEEE,2022. 7 [59] ShaohuaiShiandXiaowenChu. Speedingupconvolutional
[46] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, neuralnetworksbyexploitingthesparsityofrectifierunits.
JamesBradbury,GregoryChanan,TrevorKilleen,Zeming arXivpreprintarXiv:1704.07724,2017. 3[60] AndyShih,SuneelBelkhale,StefanoErmon,DorsaSadigh, [78] YanliZhao,AndrewGu,RohanVarma,LiangLuo,Chien-
and Nima Anari. Parallel sampling of diffusion models. ChinHuang,MinXu,LessWright,HamidShojanazeri,Myle
NeurIPS,2023. 3,4,7,8 Ott,SamShleifer,etal. Pytorchfsdp:experiencesonscaling
[61] JaschaSohl-Dickstein,EricWeiss,NiruMaheswaranathan, fullyshardeddataparallel. arXivpreprintarXiv:2304.11277,
and Surya Ganguli. Deep unsupervised learning using 2023. 3
nonequilibriumthermodynamics. InICML,2015. 1 [79] LianminZheng,ZhuohanLi,HaoZhang,YonghaoZhuang,
[62] JiamingSong,ChenlinMeng,andStefanoErmon. Denoising ZhifengChen,YanpingHuang,YidaWang,YuanzhongXu,
diffusionimplicitmodels. InICLR,2020. 2,7 DanyangZhuo,EricPXing,etal. Alpa: Automatinginter-
[63] YangSong,JaschaSohl-Dickstein,DiederikPKingma,Ab- and{Intra-Operator}parallelismfordistributeddeeplearning.
hishekKumar,StefanoErmon,andBenPoole. Score-based In16thUSENIXSymposiumonOperatingSystemsDesign
generativemodelingthroughstochasticdifferentialequations. andImplementation(OSDI22),pages559–578,2022. 3
InICLR,2020. 3
[64] YangSong,PrafullaDhariwal,MarkChen,andIlyaSutskever.
Consistencymodels. 2023. 9
[65] HaotianTang,ZhijianLiu,XiuyuLi,YujunLin,andSong
Han. Torchsparse:Efficientpointcloudinferenceengine. In
MLSys,2022. 3
[66] HaotianTang,ShangYang,ZhijianLiu,KeHong,Zhong-
mingYu,XiuyuLi,GuohaoDai,YuWang,andSongHan.
Torchsparse++: Efficienttrainingandinferenceframework
forsparseconvolutionongpus. InMICRO,2023. 3
[67] ArashVahdat,KarstenKreis,andJanKautz. Score-based
generativemodelinginlatentspace. 34:11287–11302,2021.
3
[68] LeslieG.Valiant. Abridgingmodelforparallelcomputation.
Commun.ACM,33(8):103–111,1990. 3
[69] YuxinWuandKaimingHe. Groupnormalization. InECCV,
2018. 4
[70] GuangxuanXiao,TianweiYin,WilliamT.Freeman,Frédo
Durand,andSongHan. Fastcomposer: Tuning-freemulti-
subject image generation with localized attention. arXiv,
2023. 5
[71] ZhishengXiao,KarstenKreis,andArashVahdat. Tackling
the generative learning trilemma with denoising diffusion
GANs. InICLR,2022. 2
[72] YuanzhongXu,HyoukJoongLee,DehaoChen,BlakeHecht-
man,YanpingHuang,RahulJoshi,MaximKrikun,Dmitry
Lepikhin, Andy Ly, Marcello Maggioni, Ruoming Pang,
NoamShazeer,ShiboWang,TaoWang,YonghuiWu,and
ZhifengChen. Gspmd:Generalandscalableparallelization
formlcomputationgraphs.arXivpreprintarXiv:2105.04663,
2021. 3
[73] JinhuiYuan,XinqiLi,ChengCheng,JunchengLiu,RanGuo,
Shenghang Cai, Chi Yao, Fei Yang, Xiaodong Yi, Chuan
Wu,HaoranZhang,andJieZhao. Oneflow: Redesignthe
distributed deep learning framework from scratch. arXiv
preprintarXiv:2110.15032,2021. 3
[74] QinshengZhangandYongxinChen. Fastsamplingofdiffu-
sionmodelswithexponentialintegrator. InICLR,2022. 2,
3
[75] Qinsheng Zhang, Molei Tao, and Yongxin Chen. gddim:
Generalizeddenoisingdiffusionimplicitmodels. 2022. 3
[76] QinshengZhang,JiamingSong,XunHuang,YongxinChen,
andMingyuLiu. Diffcollage: Parallelgenerationoflarge
contentwithdiffusionmodels. InCVPR,2023. 3
[77] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,
andOliverWang. Theunreasonableeffectivenessofdeep
featuresasaperceptualmetric. InCVPR,2018. 7