[
    {
        "title": "The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?",
        "authors": "Alex GuWen-Ding LiNaman JainTheo X. OlaussonCeline LeeKoushik SenArmando Solar-Lezama",
        "links": "http://arxiv.org/abs/2402.19475v1",
        "entry_id": "http://arxiv.org/abs/2402.19475v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19475v1",
        "summary": "While language models are increasingly more proficient at code generation,\nthey still frequently generate incorrect programs. Many of these programs are\nobviously wrong, but others are more subtle and pass weaker correctness checks\nsuch as being able to compile. In this work, we focus on these counterfeit\nsamples: programs sampled from a language model that 1) have a high enough\nlog-probability to be generated at a moderate temperature and 2) pass weak\ncorrectness checks. Overall, we discover that most models have a very shallow\nunderstanding of counterfeits through three clear failure modes. First, models\nmistakenly classify them as correct. Second, models are worse at reasoning\nabout the execution behaviour of counterfeits and often predict their execution\nresults as if they were correct. Third, when asking models to fix counterfeits,\nthe likelihood of a model successfully repairing a counterfeit is often even\nlower than that of sampling a correct program from scratch. Counterfeits also\nhave very unexpected properties: first, counterfeit programs for problems that\nare easier for a model to solve are not necessarily easier to detect and only\nslightly easier to execute and repair. Second, counterfeits from a given model\nare just as confusing to the model itself as they are to other models. Finally,\nboth strong and weak models are able to generate counterfeit samples that\nequally challenge all models. In light of our findings, we recommend that care\nand caution be taken when relying on models to understand their own samples,\nespecially when no external feedback is incorporated.",
        "updated": "2024-02-29 18:59:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19475v1"
    },
    {
        "title": "Lifelong Benchmarks: Efficient Model Evaluation in an Era of Rapid Progress",
        "authors": "Ameya PrabhuVishaal UdandaraoPhilip TorrMatthias BethgeAdel BibiSamuel Albanie",
        "links": "http://arxiv.org/abs/2402.19472v1",
        "entry_id": "http://arxiv.org/abs/2402.19472v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19472v1",
        "summary": "Standardized benchmarks drive progress in machine learning. However, with\nrepeated testing, the risk of overfitting grows as algorithms over-exploit\nbenchmark idiosyncrasies. In our work, we seek to mitigate this challenge by\ncompiling ever-expanding large-scale benchmarks called Lifelong Benchmarks. As\nexemplars of our approach, we create Lifelong-CIFAR10 and Lifelong-ImageNet,\ncontaining (for now) 1.69M and 1.98M test samples, respectively. While reducing\noverfitting, lifelong benchmarks introduce a key challenge: the high cost of\nevaluating a growing number of models across an ever-expanding sample set. To\naddress this challenge, we also introduce an efficient evaluation framework:\nSort \\& Search (S&S), which reuses previously evaluated models by leveraging\ndynamic programming algorithms to selectively rank and sub-select test samples,\nenabling cost-effective lifelong benchmarking. Extensive empirical evaluations\nacross 31,000 models demonstrate that S&S achieves highly-efficient approximate\naccuracy measurement, reducing compute cost from 180 GPU days to 5 GPU hours\n(1000x reduction) on a single A100 GPU, with low approximation error. As such,\nlifelong benchmarks offer a robust, practical solution to the \"benchmark\nexhaustion\" problem.",
        "updated": "2024-02-29 18:58:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19472v1"
    },
    {
        "title": "Humanoid Locomotion as Next Token Prediction",
        "authors": "Ilija RadosavovicBike ZhangBaifeng ShiJathushan RajasegaranSarthak KamatTrevor DarrellKoushil SreenathJitendra Malik",
        "links": "http://arxiv.org/abs/2402.19469v1",
        "entry_id": "http://arxiv.org/abs/2402.19469v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19469v1",
        "summary": "We cast real-world humanoid control as a next token prediction problem, akin\nto predicting the next word in language. Our model is a causal transformer\ntrained via autoregressive prediction of sensorimotor trajectories. To account\nfor the multi-modal nature of the data, we perform prediction in a\nmodality-aligned way, and for each input token predict the next token from the\nsame modality. This general formulation enables us to leverage data with\nmissing modalities, like video trajectories without actions. We train our model\non a collection of simulated trajectories coming from prior neural network\npolicies, model-based controllers, motion capture data, and YouTube videos of\nhumans. We show that our model enables a full-sized humanoid to walk in San\nFrancisco zero-shot. Our model can transfer to the real world even when trained\non only 27 hours of walking data, and can generalize to commands not seen\nduring training like walking backward. These findings suggest a promising path\ntoward learning challenging real-world control tasks by generative modeling of\nsensorimotor trajectories.",
        "updated": "2024-02-29 18:57:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19469v1"
    },
    {
        "title": "Curiosity-driven Red-teaming for Large Language Models",
        "authors": "Zhang-Wei HongIdan ShenfeldTsun-Hsuan WangYung-Sung ChuangAldo ParejaJames GlassAkash SrivastavaPulkit Agrawal",
        "links": "http://arxiv.org/abs/2402.19464v1",
        "entry_id": "http://arxiv.org/abs/2402.19464v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19464v1",
        "summary": "Large language models (LLMs) hold great potential for many natural language\napplications but risk generating incorrect or toxic content. To probe when an\nLLM generates unwanted content, the current paradigm is to recruit a\n\\textit{red team} of human testers to design input prompts (i.e., test cases)\nthat elicit undesirable responses from LLMs. However, relying solely on human\ntesters is expensive and time-consuming. Recent works automate red teaming by\ntraining a separate red team LLM with reinforcement learning (RL) to generate\ntest cases that maximize the chance of eliciting undesirable responses from the\ntarget LLM. However, current RL methods are only able to generate a small\nnumber of effective test cases resulting in a low coverage of the span of\nprompts that elicit undesirable responses from the target LLM. To overcome this\nlimitation, we draw a connection between the problem of increasing the coverage\nof generated test cases and the well-studied approach of curiosity-driven\nexploration that optimizes for novelty. Our method of curiosity-driven red\nteaming (CRT) achieves greater coverage of test cases while mantaining or\nincreasing their effectiveness compared to existing methods. Our method, CRT\nsuccessfully provokes toxic responses from LLaMA2 model that has been heavily\nfine-tuned using human preferences to avoid toxic outputs. Code is available at\n\\url{https://github.com/Improbable-AI/curiosity_redteam}",
        "updated": "2024-02-29 18:55:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19464v1"
    },
    {
        "title": "Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for Specialized Tasks",
        "authors": "Bálint MucsányiMichael KirchhofSeong Joon Oh",
        "links": "http://arxiv.org/abs/2402.19460v1",
        "entry_id": "http://arxiv.org/abs/2402.19460v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19460v1",
        "summary": "Uncertainty quantification, once a singular task, has evolved into a spectrum\nof tasks, including abstained prediction, out-of-distribution detection, and\naleatoric uncertainty quantification. The latest goal is disentanglement: the\nconstruction of multiple estimators that are each tailored to one and only one\ntask. Hence, there is a plethora of recent advances with different intentions -\nthat often entirely deviate from practical behavior. This paper conducts a\ncomprehensive evaluation of numerous uncertainty estimators across diverse\ntasks on ImageNet. We find that, despite promising theoretical endeavors,\ndisentanglement is not yet achieved in practice. Additionally, we reveal which\nuncertainty estimators excel at which specific tasks, providing insights for\npractitioners and guiding future research toward task-centric and disentangled\nuncertainty estimation methods. Our code is available at\nhttps://github.com/bmucsanyi/bud.",
        "updated": "2024-02-29 18:52:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19460v1"
    }
]