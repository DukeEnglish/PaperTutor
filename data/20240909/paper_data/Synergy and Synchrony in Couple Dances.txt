Synergy and Synchrony in Couple Dances
VonganiMaluleke1 LeaMu¨ller1 JathushanRajasegaran1 GeorgiosPavlakos2
ShiryGinosar1 AngjooKanazawa1 JitendraMalik1
1UCBerkeley2UTAustin
Figure1. TowhatextentdoesBob’sbehavioraffectAlice’sbehavior?Westudythisquestioninacouple’sdance-anexampleof
full-bodydyadicphysicalsocialinteraction.Wepredictthefullbodymotionofadancer,Alice(orange),giventheirownpastmotion(gray)
andtheirpartner,Bob’s(blue),motion.
Abstract wild,coupledancevideodatasettoenablefutureresearch
inthisdirection.Videoresultsareavailableontheproject
website:https://von31.github.io/synNsync.
This paper asks to what extent social interaction influ-
ences one’s behavior. We study this in the setting of two
dancersdancingasacouple.Wefirstconsiderabaselinein 1.Introduction
whichwepredictadancer’sfuturemovesconditionedonly
ontheirpastmotionwithoutregardtotheirpartner.Wethen Whatisthecounterpartinvideounderstandingofnext-word
investigatetheadvantageoftakingsocialinformationinto prediction,thebedrockpre-trainingtaskoflargelanguage
accountbyconditioningalsoonthemotionoftheirdancing models? Is a “word” in video a pixel? Or a patch? Or an
partner.WefocusouranalysisonSwing,adancegenrewith entitylikeanobjectoraperson?Whilethesearedebatable
tightphysicalcouplingforwhichwepresentanin-the-wild questions,thispaperwillfocusonahumanasthetokenof
videodataset.Wedemonstratethatsingle-personfuturemo- interest, with an associated “state”, the counterpart of the
tion prediction in this context is challenging. Instead, we wordembeddingvectorusedinnaturallanguageprocessing.
observe that prediction greatly benefits from considering Unlikenext-tokenpredictioninlanguage,thedynamics
theinteractionpartners’behavior,resultinginsurprisingly ofaperson’sstateareconditionedonmuchmorethanthe
compellingcoupledancesynthesisresults(seesupp.video). paststatehistoryoftheperson,aswealsohavetoconsider
Ourcontributionsareademonstrationoftheadvantagesof theirinteractionswithotherpeopleduringsocialsituations.
sociallyconditionedfuturemotionpredictionandanin-the- For example, when one partner raises their arm during a
1
4202
peS
6
]VC.sc[
1v04440.9042:viXracoupledance,theotherpartnerislikelytotwirl. 3Dmesh In-the-wild Futurepred.
Thispaperinvestigateshowsocialinformationfromin- Qianhuietal.[3] ✗ ✗ ✗
teraction partners impacts human future behavior predic- ReMoS[17] ✗ ✗ ✗
tion.Weconsiderthisquestioninthecontextofphysically Murchanaetal.[4] ✗ ✗ ✓
InterFormer[9] ✗ ✓ ✗
close social interactions such as couple dance, where we
Duolando[46] ✓ ✗ ✗
predictthefuturemotionofoneofthedancingpartners.In ExPI[22] ✓ ✗ ✓
social situations, single-person future behavior prediction Ours ✓ ✓ ✓
ispossibletosomeextent,thankstothesynergycreatedby
the coupling body parts resulting in a lower dimensional Table1.Relatedworkonmulti-personmotiongenerationandes-
manifold of human motion than the available degrees of timation.Wearethefirsttodemonstratehowinternetdatacanbe
leveragedtotrainamodelforpredicting3Dhumanmotioncaptur-
freedom[5].However,interactingpartnersdynamicallyand
ingthefullbodysurface.
reciprocallyadaptthetemporalstructureoftheirbehavior.
Thissynchrony[11,27,42]enforcesconstraintsoncoordi-
natedmotionthatareunpredictablefromeachpartneralone
coupledancevideos,whichweintroduceandreleaseaspart
andthatwemusttakeintoaccountinsocialsituations.We
ofthiswork.Ourresultsshowthatsocialinformationfrom
therefore examine how the dancers’ prediction improves
aninteractionpartnergreatlyenhancesthepredictabilityof
whenconsideringtheirdancingpartner’smotion.
humanbehavior.Ourcontributionsareademonstrationof
We represent continuous dance motion as a series of
the advantages of socially conditioned future motion pre-
atomic motion elements. To understand why this might
diction and an in-the-wild, annotated couple dance video
be useful, we can look at Labanotation [56], a notation
datasettoenablefutureresearchinthisdirection.
invented for capturing dance motion like a musical score.
TheideabehindLabanotationistorecordthepositionsof
2.RelatedWork
the body, the pattern of the steps, and places of emphasis.
Labanotationthusbreaksdancedownintoatomicmotions
HumanMotionPredictioninvolvesgeneratingfuture
with associated discrete notations that are easier to write
humanmotionfrompastdata.Thewell-studiedcaseofunary
down on paper and analyze than the continuous motion
humanmotionpredictionaimstopredictaperson’sfuture
becausethereisasetdictionaryofthem.
motionformtheirownpastmotion.Forthistask,tradition-
Similarly, we learn a discrete dictionary of quantized
ally,RecurrentNeuralNetworks(RNNs)wereused[14,34],
atomic motion elements with a motion-encoding VQ-
whilerecentworkleveragesattentionmechanisms[7,55].
VAE[54].Critically,ourmotionrepresentationreliesona
Somestudiespredictunarymotionininteractionswithob-
parametric3Dhumanbodymodel,acomputationallycrucial
jectsorscenes[8,10,33],basedoninputpose[47,48,63],
choicesinceitallowsustoignorealltheirrelevantpixels
orconditionedonscenecontact[41]oraudiosignals[36].
and focus on predicting behavior directly. Moreover, the
Recently,Radosavovicetal.[40]usednexttokenprediction
parametricbodymodelenablesustodecouplemotioninto
forhumanoidlocomotion.
itsconstituents:pose,orientation,andtranslation,andlearn
DyadicHumanMotionPredictionextendsunarypredic-
a separate atomic dictionary for each—a necessary detail
tion by incorporating past motion from a second person
becausedancersmovefreelyaroundthestage.
[4, 22]; please see Table 1 where we refer to this task as
The synchrony between interacting partners hints that
“Futurepred.”.
thereisenoughtemporalcorrelationbetweentheirbehav-
Existingmethodsformulti-personmotionpredictionare
iors for predicting the motion of one agent based on that
typicallytrainedondatacollectedincontrolledsettings,us-
of the other – but to what extent? We study this question
ingMoCapsystems[22]ordepthsensors[4].Whilein-lab
with an autoregressive transformer [55] as our prediction
data can produce clean motion sequences, it suffers from
mechanism,astransformersexcelatcapturinglong-range
limitations in scalability and naturalness. In contrast, we
temporalcorrelationsintime-seriesdata.Thediscretemo-
demonstrate that the problem of social interaction can be
tionrepresentationlearnedbytheVQ-VAEiswell-suited
studiedusingrealvideodatafromtheInternet,allowingus
fortransformersandenablescomputationallyefficientpre-
toanalyzethemotionofprofessionalswingdancersthrough
dictionusingdiscreteclassificationratherthancontinuous
state-of-the-art4Dhumanreconstructionmethods.Ourap-
regression.Italsoallowsfornondeterministicpredictionas
proachinprinciplecanbeappliedtoanycollectionofInter-
theoutputoftheautoregressivetransformerisamultinomial
netvideos.
distributionoverthenexttimestepofmotion,fromwhich
wecansamplemultipletrajectories. MotionGenerationaddressestheproblemofsynthesizing
Wedemonstratetheefficacyofleveragingsocialinforma- humanmotionfromscratch.Whilemanystudiesfocuson
tioninasetofexperimentsonadatasetofin-the-wildSwing generatingsingle-personmotionsfromvariousmodalities,
2e.g.,speechaudio[1,18,32,59],text[2,15,21,39,52,61], gaged in couple dance. We study two aspects of motion
scene[23,25,57],action[20,38,51],music[28–30,45,49, duringphysicalinteraction:howaperson’spastmotionpre-
53],researchongeneratingdyadichumanmotionsisstill dictstheirfutureandhowindividualsmodulatetheirmotions
emerging.Afewpriorworkshaveexploreddyadichuman whenengagedinclose,coordinatedactivities.Fornotation
mesh generation during close social interactions [31, 35, purposes, we introduce Alice and Bob, a pair of fictional
44]. Closest to our approach, previous work has explored characterscommonlyusedasplaceholdersincryptography
predictingalistener’s3Dmotionfromaspeaker’smotion literature[43],whomweassigntorepresentthetwopeople
andaudioinconversationalsettings[26,36,64].Multiple engagedinthedance.WerefertoAliceandBob’sspecific
works generate a person’s motion conditioned on the full componentsviathesuperscriptsAandB.e.g.,MArefersto
(past,current,andfuture)sequenceofanotherperson[3,9, Alice’smotion,andMB referstoBob’smotion.
17,46];wemarkthesemethodswithan“✗”inTable1under We define the following two tasks: (1) Unary motion
“Futurepred.”.Unlikethesemethods,wherethemodelhas prediction:givenAlice’spast3D-bodymotion,weautore-
accessthefullconditioningsignal,ourgoalistopredicttwo gressivelypredictherfuturemoves.;and(2)Dyadicmotion
people’sfuturemotiononlyfromtheirpastmotion. prediction:givenAliceandBob’spast3D-bodymotion,we
autoregressivelypredictthefuturemotionofAlice,whois
VQ-VAEsforHumanMotionRepresentations.Theidea
dancingtogetherwithBob.
oflearningadiscretedictionaryofatomicmotionelements
Torepresenttheflowofmotion,wedefineatransformer-
for the purpose of motion generation was first introduced
based predictor, P, that learns to model temporally long-
by[36]forfacialmotionofstationaryindividualscaptured
rangepatternsintheinputsequence.Attesttime,thepre-
byastaticfrontalcameraandrecentlyexpandedtofull-body
dictortakesasinputeitherAlice’spastmotion(intheunary
motion[12,61].
case)orAliceandBob’spastmotion(inthedyadiccase)and
Thesepreviousworksaretrainedtocompressmultiple
autoregressivelypredictsAlice’snextdancesteps(Figure2).
motionparameters,i.e.,3Djointlocations[46,62],SMPL
Werepresentthecontinuousdancemotionusingadictio-
[6]bodyposeparameters[12],andfacialmotion[36],into
naryofatomicmotionelements,thatwelearnusingaset
asinglecodebook.
ofmotionVQ-VAEs(Section3.2).Asdancersmayfreely
While some body poses naturally correlate with a per-
movearoundspace,weextendmotionVQ-VAEmodeling
son’sorientation,mostposescanoccurinanyorientation
by disentangling the three full-body motion constituents:
ortranslation.Aunifiedcodebookentanglestheseelements,
bodypose,orientationandtranslation.Thediscreterepre-
limitingitsabilitytocapturediversehumanposesparticu-
sentationlearnedbytheVQ-VAEsenablesustopredicta
larly in the context of social interaction. In this work, we
multinomialdistributionoverthenexttimestepofmotion.
proposetolearnthreeseparatedictionariesforbodypose,
Thus,theoutputoftheautoregressivepredictor(Section3.3)
orientation, and translation parameters of a human body
isadistributionoverAlice’spossiblerealisticfuturedance
modelforeachperson,andshowtheeffectivenessofthis
moves,fromwhichwecansamplemultipletrajectories.
approachcomparedtoasinglecodebookthroughanablation
experiment. 3.1.ProblemDefinitions
Datasets of People in Close Social Interaction. Various
UnaryMotionPrediction.LetM = {m }T beapose
labs have captured data of two people interacting closely. i i=1
trajectory representing motion m . For each timestep t ∈
Hi4D[60],CHI3D[13]containsocialinteractionsbetween i
[t ,T],wheret ∈[1,T]isthetimeinwhichwestartpre-
twopeoplesuchas“hugging”or“shakinghands”withvideo π π
dicting,wetakeasinputAlice’spastgroundtruthmotionup
andground-truthbodyshapedfrommotioncapture,respec-
tot ,MA andanyofherpreviouslypredictedpastmotion
tively.InterGen[31]isadatasetofinteractingpeoplecap- π 1:tπ
turedviamotioncaptureandannotatedwithtextdescriptions. MˆA tπ+1:t−1,ifavailable.Ourpredictor,P,autoregressively
ReMoS[16]isamotioncapturedatasetofpeopleperform- predictsAlice’sfuturemotiononetime-stepatatime:
ing martial arts and couple dance, respectively. ExPi [22]
mˆA =P(MA ,MˆA ), (1)
containssequencesoftwopeoplein“extremeposeinterac- t 1:tπ tπ+1:t−1
tions”ofprofessionaldancerscapturedinamotioncapture
where P learns to model the distribution over the next
studio.
timestepofAlice’smotion:
3.MotionPredictioninCoupleDance p(mˆA|MA ). (2)
t 1:t−1
Ouraimistoinvestigatethepracticalbenefitsofincorporat-
ingsocialinformationintofuturebehaviorpredictionduring DyadicMotionPrediction.LetM = {m }T beapose
i i=1
physical interaction. We focus on motion prediction as a trajectory representing motion m . For each timestep t ∈
i
proxy of behavior and predict the motion of a dancer en- [t ,T], where t ∈ [1,T] is the time in which we start
π π
3Figure2.Test-timeautoregressivepredictionintheunary(left)anddyadic(right)tasks.Intheunarypredictiontask,westartfromAlice’s
pastmotionuptotimet ,andpredictthenexttimestepineachiteration.Inthedyadicpredictiontask,westartfromAliceandBob’s
π
motionuptotimet .Ineachstep,wepredictAlice’snexttokenfromherpastgroundtruthmotionuntilt ,herpredictedmotionfort>t ,
π π π
andBob’spastgroundtruthmotion
predicting,wetakeasinputBob’spastgroundtruthmotion representationstodiscretecodesinthecodebook.Givena
MB alongwithAlice’spastgroundtruthmotionupto sequenceofcodes,thedecoderreconstructsthecorrespond-
1:t−1
t ,MA andanyofherpreviouslypredictedpastmotion ingcontinuousinputdata.
π 1:tπ
MˆA ,ifavailable.Ourpredictor,P,autoregressively ThediscretecodebookZ ={z }K ofaVQ-VAEcon-
pret dπ i+ c1 ts:t A−1
lice’sfuturemotiononetime-stepatatime: sistsofK codebookentriesz
k
∈Rk dzk .= G1
ivenaninputsignal
X ∈ RT×dx, we encode X via zˆ = E(x) to a latent vari-
mˆA t =P(MB 1:t−1,MA 1:tπ,MˆA tπ+1:t−1), (3) able of size wT ×d z, where w is a temporal window size
usuallymuchsmallerthanT.Aquantizationfunctionq(·)
where P learns to model the distribution over the next thenmapseachelementzˆ,i=1,...,T,totheindex,z¯,of
timestepofAlice’smotion,givenBob’smotion: i w
itsclosestentryinZ via:
p(mˆA|MB ,MA ). (4)
t 1:t−1 1:t−1
z¯=q(zˆ):=argmin(∥zˆ −z ∥).
i k
3.2.LearningQuantizedMotionCodebooks zk∈Z
Giventheindexz¯ofacodebookentry,thedecoder’stask
MotionRepresentation.Animportantquestionishowto
istorecovertheoriginalinputsignal:
represent the human body in the world. Ideally, the body
representationenablesustogeneralizeacrossindividualsand
xˆ=D(z¯)=D(q(E(x))).
cameraviewpoints.Unlikepreviousmethodsthat,among
others,relyon3Djointlocations,wechoosearepresentation
thatisinvarianttochangesinbodyshape,scale,andcamera Tomodelthefullrangeofdancemotion,wemustcon-
pose. In particular, we use the SMPL [6] body model to sideralltheconstituentsofmotion:bodypose,orientation,
represent human bodies in 3D. SMPL is a differentiable andtranslation.Whiletheseparametersmaycorrelateinsim-
function that maps pose, θ ∈ RJ×3 and shape, β ∈ RB pletrajectories(e.g.,walkingforwardorwalkinginacircle),
parameterstoa3DmeshwithJ =23jointsandB <300. in most cases, one can strike the same pose at any global
Throughϕ∈R3andγ ∈R3,thebodycanbeorientedand orientationandtranslation.Therefore,weextenduponthe
translatedinthe3Dworld.Torepresentmotionovertime commonlyusedunifiedcodebookapproach,whichentangles
T,weextendtheparametersrelatedtotheposetrajectory thedifferentaspectsofmotion[36,62].Instead,wefactorize
viaΘ ∈ RT×J×3,Φ ∈ RT×3,andΓ ∈ RT×3.Wefurther thedictionarylearningandlearnthreeseparatecodebooks,
denoteaperson’sposetrajectoryofdurationT via: oneforeachbodymodelparameter(seeFig.3).
WerefertothesecodebooksviaZ ,Z ,andZ forpose,
Θ Φ Γ
P={m i}T
i=1
={[θ,ϕ,γ]}T
i=1
=[Θ,Φ,Γ].
orientation,andtranslation,respectively.Theposetrajectory,
ormotionsequences,ofasinglepersongiveninbodymodel
VQ-VAE’s for Full-Body Motion. VQ-VAE’s [54] are parameterspacecanthenbeapproximatedthroughatriplet
encoder-decodermodelstrainedtocompressinputdatainto ofVQ-VAEcodebookindices,bycombiningindicesfrom
a lower dimensional discrete latent space, i.e., “the code- eachcodebook,i.e.,
book”.Theencodermapsinputdataintoacontinuouslatent
space, and a quantization process maps these continuous Pˆ =s=(z¯ ,z¯ ,z¯ ).
Θ Φ Γ
4Figure 3. Illustration of the VQ-VAEs. We learn three separate
codebooks,oneforeachbodymodelparameter.Anencoder,E,
·
mapsthebodyparametertothecodebook,Z.Thedecoder,D
· ·
Figure4.Transformertrainingprocedureinthedyadiccase.The
bringsthecodebooklatentvectorsbackintobodymodelparameter
major part of our network is a transformer-decoder block with
space.Toobtain3Dmeshes,wejointlypasstheparametersthrough
causalmasking,suchthatAliceandBobcanonlyattendtotheir
thebodymodelfunction.
past motion. Input to our model are Alice and Bob’s codebook
indicesforbodypose,Θ,orientation,Φ,andtranslation,Γ.We
3.3.ConditionalAutoregressivePrediction embed the tokens into a latent space and add time, person, and
parameterencoding.Thefinallayerinournetworkgeneratesprob-
Ourgoalistocapturelong-rangetemporalcorrelationsinthe abilityscoresovercodebookindices,representingthelikelihoodof
inputdatatoautoregressivelypredictAlice’sfuturemotion. anindexbeingthenextmotions .
tπ+1
Todothis,weatrainatransformer-decoderbasednetwork
directlyonthelearnedcodebookindices.
Wetrainthetransformerwithacrossentropylossonthe Then,wetrainZ ΦB onΦBwrt.A andZ ΓA onΓBwrt.A.Z ΘB is
codebookindices,
trainedonΘB,andforAliceweusethecodebooksfromthe
unarytask,ZA,ZA,andZA.
Θ Φ Γ
Inthedyadicscenario,insteadofonlyusingAlice’smo-
(cid:88)T (cid:88)K tionasinputtothetransformer,weconcatenateBob’smo-
L =E [−log(p(sA )]=− sAlog(p(sA ),
P y∼p(y) τ+1 t τ+1 tion along the token dimension. Specifically, given Alice
τ=1k=1 and Bob’s motion sequences in VQ-VAE indices, sA ∈
wherethetargetcodebookindexatτ +1iscomputedfrom {1,...,K}(3× wT) and sB ∈ {1,...,K}(3× wT), the predic-
Alice’sgroundtruthfuturemotiony =MA . tor’sinputbecomes:
t+1:t+1+w
UnaryAutoregressivePrediction.Intheunaryprediction sAB =[sA|sB]∈{1,...,K}(3×2 wT).
scenario,theinputsequenceconsistsoftheindicesofAlice’s
bodymodeltriplets:
GiveninitialmotionsequencesofAliceandBob:
sA
1:τ
=(z¯ ΘA 1:τ,z¯ ΦA 1:τ,z¯ ΓA 1:τ), sA
1:τ
=(z¯ ΘA 1:τ,z¯ ΦA 1:τ,z¯ ΓA 1:τ)andsB
1:τ
=(z¯ ΘB 1:τ,z¯ ΦB 1:τ,z¯ ΓB 1:τ),
where τ is the length of the encoded sequence. The net-
thenetworkstaskistopredictthenexttokentripletforAlice:
work’staskistopredictthemultinomialdistributionofthe
codebookindicesofthenexttokentriplet:
P(sA andsB )=sA =(z¯A ,z¯A ,z¯A ).
1:τ 1:τ τ+1 Θτ+1 Φτ+1 Γτ+1
P(sA )=sA =(z¯A ,z¯A ,z¯A ).
1:τ τ+1 Θτ+1 Φτ+1 Γτ+1
TransformerArchitecture.Thepredictor,P,isdesigned
DyadicAutoregressivePrediction.TocompressBob’smo- with embedding and encoding layers, a transformer de-
tion,astraightforwardwaywouldbetore-usetheVQ-VAE coderblock,andafinallayertopredictlogits(seeFig.4).
codebooksfromtheunarypredictiontask.Thisrepresenta- Specifically, we first embed the input dictionary indices
tion,however,doesnotmodeltheinteractionandrelative s ∈ {1,...K}(3× wT) into d-dimensional latent variables
motionbetweenAliceandBob.Instead,wetrainthreenew D∈R3 wT×dviaanembeddinglayer.Notethatweshiftthe
codebooksonBob’sbodymodelparametersinAlice’srefer- codebookindicesofz¯ byK andthoseofz¯ by2K such
Φ Γ
enceframe.Inparticular,givenAlice’sbodymodelparame- thatindicesofeachcodebookaredistinct.BeforepassingD
tersattimet=0,i.e.,bodyorientation,ΦA,andtranslation, tothetransformerdecoder,weaddtime,person,andparam-
0
ΓA,wemodelBob’sorientation,ΦB,andtranslation,ΓB, eterencodinglayers.Weemploycausalmaskinginthede-
0
w.r.t.Alicebytransforming: codersuchthateachtokencanonlyattendtoprevioustime
steps. Finally, the predictor P outputs p(MA) ∈ {RK}3,
ΓBwrt.A = (ΦA)−1(ΓB −ΓA) t
0 0 i.e., the multinomial distributions of Alice’s motion next
(5)
ΦBwrt.A = (ΦA)−1(ΦB)−1. codebookindicesacrosseachcodebook’sK entries.
0
53.4.ImplementationDetails 5.Evaluation
Ourmodelsaretrainedwithabatchsizeof128andalearn- We evaluate our motion prediction results on metrics de-
ingrateof1e−5. signedtomeasurethequalityofAlice’spredictedmotion
intheunaryanddyadicsetups.Inthedyadiccase,wead-
Our VQ-VAE codebooks have 1024 indices and 256-
ditionallyevaluatethemotioncoordinationbetweenAlice
dimensionallatentvectors.Eachvideosequenceoflength
andBobasacouple.Sinceweuseallavailableinformation
100frames isdownsampled bya factorof w = 4. Attest
frombothinteractionpartnerstopredictthedyadiccase,we
time, we use teacher forcing on Alice’s input motion se-
furthercompareAlice’sdyadicmotionpredictiontoseveral
quence,M .Toselectcodebookindicesfrompredicted
1:τ
strongbaselines,asoutlinedbelow.
logits, we use top-k sampling with k = 103. For all our
experiments,weuse48framesasinput/querymotionand
EvaluationMetrics.Quantifyingmotionrealismisadiffi-
predicttheremaining52frames.
cultproblemthatcannotbereducedtoasinglemetric.We
thus evaluate our predictions along multiple axes follow-
4.3DHumanCoupleDancingDataset ing[36,37].Ourevaluationsuiteisbasedonthenotionthat
humansshoulddisplay(1)realisticand(2)diversemotion
WecollectedcompetitiveSwingdancevideosfromthe2014- thatis(3)synchronouswiththemotionoftheirinteraction
2021InternationalLindyHopChampionshipcompetitions partnerwhentheyareinteractingsocially.WeassessAlice’s
(https://ilhc.com/)thatwerepubliclydistributedon generatedmotionaccordingtothesethreepillarsagainsther
YouTube.Duringthistimeframe,ILHCcompetitionswere rawgroundtruthmotiony.
filmed with one mostly static frontal camera. We filtered • FD (unary & dyadic): Motion realism measured by the
thevideostoonlyonesthatcontaincoupledances(rather Frechetdistance[24]betweengeneratedandgroundtruth
thanasingledancer)anddonotshowtheaudience.Wealso motiondistributions.WedirectlycalculatetheFDonthe3D
removed introductory slides, including text related to the jointlocationsofthepredictedmeshes.
competition’sordancers’names.Thefinaldatasetcontains •Div(unary&dyadic):Motiondiversity.Temporalvariance
30hoursofcleancoupledancefootage.Fortheexperiments acrossasequenceofposes.Measurestheamountofmotion
inthispaper,wesegmenteachvideointo4-secondchunks. inasequence.
Thedatasetwillbepubliclyreleased. • Paired FD (dyadic): The synchrony between Alice and
To lift the motion of the couple to 3D, we use Bob’smotiondynamicsasacouplemeasuredbytheFrechet
SLAHMR[58],whichrecoversthe3Dmotionofthecouple distributiondistancesonAlice-Bobpairs(P-FD).Calculated
inaworldcoordinateframebyconsideringanddecoupling FDonconcatenatedAliceandBobpose(jointlocations).
themotionofthecamera[50]fromthemotionofthepeople.
Toachievethebestresults,weuse4DHumans[19]totrack Baselines.Wecompareourdyadicpredictionresultstothe
thepeopleandestimatetheir3DposetoinitializeSLAHMR. followingbaselines:
One challenge we faced here is related to estimating the • Nearest Neighbor (NN) on Bob’s input motion: A
heightofthetwopartners,whichisdifficultfrommonocular segment-search method commonly used for synthesis in
observations.Errorsinrelativeheightcansignificantlydete- graphics.GivenBob’sinputmotion,wefinditsnearestneigh-
rioratethe3Doutputsincetherelativepositionsofthetwo borfromthetrainingsetaccordingtoourlearnedVQ-VAE
peopleareincorrect.Toremedythat,weassumethatoneof embeddingvectorsanduseitscorrespondinggroundtruth
thetwopartnerswillbeslightlytallerthantheother(since Alicesegmentastheprediction.
weprimarilyhavemale-femalecouples).Thisassumption •Randomly-ChosenTrainingAlice(RandomTrain.Al-
introducedanextraconstraintfortherelativeheightsofthe ice): Return a randomly-chosen Alice sequence from the
twopeopleintheSLAHMRoptimization(seeSupp.),which trainingset.
improvestherelativeplacementofthetwodancers. •RandomSample:Randomsampleofthelearnedcodebook
SLAHMRrepresentsmotioninanarbitraryworldcoor- indices.
dinateframe,similartoobservingthemotionasamember
5.1.VQ-VAECodebookAblation
oftheaudience.Wecanonicalizethemotionsequencesby
introducing an extra transformation to make our analysis WeexperimentwithdifferentVQ-VAEcodebookmodelvari-
invarianttotheobservedviewpoint.Effectively,theperson ants by evaluating the effect on the reconstructed motion.
of interest at time t = 0 is at the origin of the coordinate ThefullresultsarepresentedinTable2.Morespecifically,
frame,withacanonicalorientation(i.e.,thetorsofacingin with“Sep-Person”weindicatethatAliceandBobhavesep-
the+zdirectionandtheheadinthe+ydirection).Moreover, aratecodebooks,while“Joint-Person”meansthattheyshare
as a form of data augmentation, we mirror the recovered thesamecodebook.Moreover,for“Sep-Feat”welearnthree
motionsalongtheyzplane. separate codebooks for the three factors of motion (body
6pose, orientation, translation), while for “Joint-Feat” we Realism Diversity Synchrony
learnaunifiedcodebookthatentanglesthesethreefactors.
FD↓ Div P-FD↓
Forthedifferentvariantswereporttheerrorinthemotion
GT - 0.694 -
reconstruction,asmeasuredbytheMPJPE,PA-MPJPE,and
NNonBob 0.023 0.773 0.079
FDmetrics.Weobservethatlearningthreeseparatecode- RandomTrain.Alice 0.027 0.791 0.107
booksforbodypose,orientation,translationleadstoclear RandomSample 0.212 0.902 0.405
Ours(unary) 0.025 0.697 -
improvementinthereconstructionmetricswhichconfirms
Ours(dyadic) 0.012 0.688 0.026
our design choice. Moreover, whether the codebooks for
AliceandBobareseparatehassmallereffectinthemotion
Table3.Quantitativeresults.Comparisonagainstgroundtruth
reconstruction,butweobservedsmallerstandarddeviation
annotations(GT)onaheld-outtestset.↓indicateslowerisbetter.
whenwekeepthemseparate,sothisisthedesignweadopt. Fordiversity,closertoGTisbetter.
VQ-VAEModel MPJPE↓ PA-MPJPE↓ FD↓
Sep-PersonSep-Feat 0.112±0.009 0.038±0.000 0.004±0.001 overallrealism(FD)ofthepredictedunarymotionissimilar
Sep-PersonJoint-Feat 0.272±0.007 0.052±0.001 0.014±0.008
to that of the baselines containing real motion sequences,
Joint-PersonSep-Feat 0.112±0.011 0.038±0.001 0.006±0.001
Joint-PersonJoint-Feat 0.261±0.011 0.051±0.001 0.116±0.090 NN on Bob and Random Train. Alice. However, in com-
parison,therealism(FD)ofourdyadicpredictionismuch
Table2.VQ-VAECodebookAblation.Quantitativecomparison higher(i.e.lowerFD)sinceconditioningAlice’smotionon
ofjointandseparateVQ-VAEmodelvariantsintermsofMPJPE, that of her dance partner constrains the set of moves that
PA-MPJPE,andFDmetrics. shemightperformandpushesthedistributionoverallher
possibleconditionalmotiontrajectoriesclosertotheground
truthAlicewhodanceswithBob.
5.2.QuantitativeResults
Lookingatthevariousbaselines,weconfirmthatRandom
We start by observing the temporal dynamics of our mo- Sampleisindeedtheleastrealistic(FD),themostdiverse
tionpredictionmethodbygraphingAlice’sMeanPerJoint (Div),andtheleastsynchronouswithBob(P-FD).Incom-
PositionError(MPJPE,PA-MPJPE)overtimeinFigure5. parison, choosing Alice randomly from the training data
MPJPEmeasuresEuclideandistancebetweengroundtruth resultsinarealisticmotiontrajectory(FD)thatissimilarin
and predicted 3D joint locations averaged over 14 major diversity (Div) to the ground truth but is not synchronous
body joints (no root alignment is performed). PA-MPJPE with her partner, Bob (P-FD). Doing NN on Bob, outper-
isthesamelossafterProcrustesAlignmentoneachframe, formstheotherbaselinesonallfronts,butitisstillinferior
whichignoresanytranslation,rotationorscaleerrors.Ef- comparedtoourpredictions.
fectively,MPJPEjointlycaptureslocationandposeerrors,
Thatsaid,Alice’smotionismostsynchronouswithBob
whilePA-MPJPEonlycaptureslocalposeerrors.Wenote
(P-FD)whenweconditionhermotiononBob’sinthedyadic
that, as expected, while our predicted motion is accurate
case,comparedtoallthebaselines,includingNNonBob,
initially, it quickly digresses from the ground truth trajec-
which looks for the nearest neighbor of Bob’s motion tra-
toryovertime.Thefactthatthishappensmorerapidlyfor
jectoryanduseshisgroundtruthpartnerastheprediction.
the unary versus the dyadic case suggests that our dyadic
Thisfindingconfirmsourhypothesisthatconditioningon
predictionsuccessfullycapturesinformativesocialsignals
aninteractionpartner’sbehaviornotonlyimprovesmotion
fromAlice’sdancepartner.Weobserveslowerincreasefor
realismanddiversityofanindividualbutalsoenhancesthe
PA-MPJPEbecauseitonlycaptureserrorsinthelocalpose,
synchronyofthecoupleasawhole.
andunliketrajectories,whichcandivergequickly,werarely
makegrosserrorsinthepredictedposes.
Our expectation that the additional information from 5.3.QualitativeResults
Bob’smotionshouldimproveAlice’spredictedmotionis
corroboratedbyourexperimentsassummarizedinTable3. We provide extensive qualitative results of our approach
As the table shows, the diversity (Div) of our predictions in the Supp. Video. Moreover, we present a few example
ofAlice’smotionissimilartothegroundtruthforboththe results of our dyadic motion prediction in Figure 6. We
unaryandthedyadiccases.However,takingBob’smotion comparemotionpredictioninthedyadicandunarycasesto
intoaccountduringdyadicpredictionimprovestherealism thegroundtruthandNNonBobinthedyadiccase.While
(FD)ofAlice’spredictedmotioncomparedtotheunarycase. ourdyadicpredictionofAlice’smotionresultsinaplausible
In the unary case, we note that while Figure 5 demon- posetrajectorythatfitsBob’smotion,thetrajectorypredicted
strated that the future predicted motion diverges from the bytheNNbaseline,whilerealistic-looking,doesnotmatch
specific single corresponding ground truth sequence, the herpartner.
7Figure5.Predictionerrorgrowsovertime,moresoforunaryprediction.GraphsoftheMPJPEandPA-MPJPEmetricsovertime
computedforoursvs.baselines(incomparisonwithgroundtruth)startingfromt=t ,thepointinwhichwestartpredicting.Whileour
π
predictedfuturemotioniscorrectinitiallyinbothconditions,predictionerrorgrowsovertimefasterintheunarythaninthedyadiccase.
Thisincreaseisexpectedsincemotionduringphysicalinteractionhighlydependsonone’sinteractionpartner.Incontrast,allbaselinesstart
fromahigherroratt .
π
Figure6.QualitativeresultsofAlice’sautoregressivemotionprediction.Ontheleft,weinputAliceandBob’spastmotionuptoframe
t andthenstartpredictingAlice’sfuturemotion.Wecolorthe“predicted”Alicesinyellow.Ontherightistheresultoftheunaryprediction
π
task.WeinputAlice’spastmotion(inblue)andpredictherfuturemotion(inyellow).Ourmodelcanpredictplausiblefuturemotionof
Aliceincludingcomplexmotionsincoupledancelikerotations.
5.4.Limitations method[58]forhumanmotionrecovery,westillencounter
issuessuchaserrorsintherelativepositionsofthepartners,
Unlikethemajorityofpreviouswork,whichreliesonmotion potentialmeshinterpenetration,andimprecisecontactesti-
capturedatathatisoftenlimitedbyheavyinstrumentation mation.Yet,abenefitofourapproachisthatitcanleverage
andscalabilityissues,westudyhumanmotionsrecovered rapidadvancementsin4Dhumanmotionreconstructiontech-
from Internet videos, extending the scope of our analysis. niques.Combiningthedyadicsynchronywehaveidentified
However,thisapproachmeansthatthequalityofourdata as crucial for motion prediction with these improvements
heavily depends on the success of the video reconstruc- presentsanintriguingavenueforfuturework.
tion method. Although we have utilized a state-of-the-art
86.Conclusion withscenecontext. InComputerVision–ECCV2020:16th
European Conference, Glasgow, UK, August 23–28, 2020,
Thispaperinvestigatesthebenefitofconsideringsocialin-
Proceedings,PartI16,pages387–404.Springer,2020. 2
formation for behavior prediction during physical social
[9] Baptiste Chopin, Hao Tang, Naima Otberdout, Mohamed
interaction,suchascoupledance.Wedemonstratehowto Daoudi,andNicuSebe. Interactiontransformerforhuman
quantizefull-bodymotionbydisentanglingmotionintoits reactiongeneration. IEEETransactionsonMultimedia,2023.
constituentpose,orientation,andtranslationviaaparametric 2,3
bodymodel.Wetrainanautoregressivenon-deterministic [10] Enric Corona, Albert Pumarola, Guillem Alenya, and
transformerpredictordirectlyontheparametersofthepara- FrancescMoreno-Noguer. Context-awarehumanmotionpre-
metric body model. We release a dataset of Swing dance diction. InCVPR,pages6992–7001,2020. 2
[11] EmilieDelaherche,MohamedChetouani,AmmarMahdhaoui,
in-the-wildvideoswith3Dpseudogroundtruthonwhich
Catherine Saint-Georges, Sylvie Viaux, and David Cohen.
weperformourexperiments.Ourresultsdemonstratethat
Interpersonal synchrony: A survey of evaluation methods
while we can predict a person’s immediate future motion
acrossdisciplines.IEEETransactionsonAffectiveComputing,
frompastdancemoves,consideringtheinteractionpartner
3(3):349–365,2012. 2
providesamuchrichercontextandresultsinmoreaccurate
[12] SaiKumarDwivedi,YuSun,PriyankaPatel,YaoFeng,and
predictions. While we focused on the dyadic scenarios in
MichaelJ.Black.TokenHMR:Advancinghumanmeshrecov-
this work, our framework can be directly generalized and erywithatokenizedposerepresentation. InIEEE/CVFCon-
appliedtomultipleinteractionpartnersinfuturework. ferenceonComputerVisionandPatternRecognition(CVPR),
2024. 3
[13] Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut
Acknowledgements WethankEvonneNgandmembersof
Popa,VladOlaru,andCristianSminchisescu. Reconstruct-
theBAIRcommunityforhelpfuldiscussions.Thisworkwas
ingthree-dimensionalmodelsofinteractinghumans. arXiv
supportedbyBAIR/BDDsponsors,ONRMURI(N00014-
preprintarXiv:2308.01854,2023. 3
21-1-2801),andtheDARPAMCSprogram.
[14] KaterinaFragkiadaki,SergeyLevine,PannaFelsen,andJiten-
draMalik. Recurrentnetworkmodelsforhumandynamics.
References
2015 IEEE International Conference on Computer Vision
(ICCV),pages4346–4354,2015. 2
[1] SimonAlexanderson,GustavEjeHenter,TarasKucherenko,
[15] AninditaGhosh,NoshabaCheema,CennetOguz,Christian
andJonasBeskow. Style-controllablespeech-drivengesture
Theobalt,andPhilippSlusallek. Synthesisofcompositional
synthesisusingnormalisingflows. InComputerGraphics
animationsfromtextualdescriptions. InICCV,pages1396–
Forum,pages487–496,2020. 3
1406,2021. 3
[2] NikosAthanasiou,MathisPetrovich,MichaelJ.Black,and
[16] AninditaGhosh,RishabhDabral,VladislavGolyanik,Chris-
Gu¨lVarol. Teach:Temporalactioncompositionsfor3dhu-
tianTheobalt,andPhilippSlusallek. Remos:Reactive3d
mans. In3DV,pages414–423,2022. 3
motionsynthesisfortwo-personinteractions. arXivpreprint
[3] MurchanaBaruahandBonnyBanerjee. Amultimodalpre-
arXiv:2311.17057,2023. 3
dictive agent model for human interaction generation. In
[17] AninditaGhosh,RishabhDabral,VladislavGolyanik,Chris-
ProceedingsoftheIEEE/CVFconferenceoncomputervision
tian Theobalt, and Philipp Slusallek. Remos: 3d motion-
andpatternrecognitionworkshops,2020. 2,3
conditionedreactionsynthesisfortwo-personinteractions. In
[4] MurchanaBaruahandBonnyBanerjee. Amultimodalpre- EuropeanConferenceonComputerVision(ECCV),2024. 2,
dictive agent model for human interaction generation. In
3
2020IEEE/CVFConferenceonComputerVisionandPattern
[18] ShiryGinosar,AmirBar,GefenKohavi,CarolineChan,An-
RecognitionWorkshops(CVPRW),2020. 2
drewOwens,andJitendraMalik. Learningindividualstyles
[5] NikolaiBernstein. OPoestroeniiDvizhenni,”OntheCon- ofconversationalgesture. InProceedingsoftheIEEE/CVF
structionofMovements”. medghiz,1947. 2 Conference on Computer Vision and Pattern Recognition,
[6] FedericaBogo,AngjooKanazawa,ChristophLassner,Peter pages3497–3506,2019. 3
Gehler,JavierRomero,andMichaelJ.Black. KeepitSMPL: [19] ShubhamGoel,GeorgiosPavlakos,JathushanRajasegaran,
Automaticestimationof3Dhumanposeandshapefromasin- AngjooKanazawa*,andJitendraMalik*. Humansin4D:
gleimage. InECCV,pages561–578.SpringerInternational Reconstructingandtrackinghumanswithtransformers. In
Publishing,2016. 3,4 ICCV,2023. 6
[7] YujunCai,LinHuang,YiweiWang,Tat-JenCham,Jianfei [20] ChuanGuo,XinxinZuo,SenWang,ShihaoZou,Qingyao
Cai,JunsongYuan,JunLiu,XuYang,YihengZhu,Xiaohui Sun, Annan Deng, Minglun Gong, and Li Cheng. Ac-
Shen,DingLiu,JingLiu,andNadiaMagnenat-Thalmann. tion2motion:Conditionedgenerationof3dhumanmotions.
Learning progressive joint propagation for human motion InACMMM,pages2021–2029,2020. 3
prediction. In European Conference on Computer Vision, [21] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji,
2020. 2 XingyuLi,andLiCheng. Generatingdiverseandnatural3d
[8] ZheCao,HangGao,KarttikeyaMangalam,Qi-ZhiCai,Minh humanmotionsfromtext. InCVPR,pages5152–5161,2022.
Vo,andJitendraMalik. Long-termhumanmotionprediction 3
9[22] WenGuo,XiaoyuBie,XavierAlameda-Pineda,andFrancesc [37] EvonneNg,JavierRomero,TimurBagautdinov,ShaojieBai,
Moreno-Noguer. Multi-personextrememotionprediction. TrevorDarrell,AngjooKanazawa,andAlexanderRichard.
InProceedingsoftheIEEE/CVFConferenceonComputer Fromaudiotophotorealembodiment:Synthesizinghumans
VisionandPatternRecognition,pages13053–13064,2022. 2, inconversations. InArXiv,2024. 6
3 [38] MathisPetrovich,MichaelJBlack,andGu¨lVarol. Action-
[23] MohamedHassan,DuyguCeylan,RubenVillegas,JunSaito, conditioned3dhumanmotionsynthesiswithtransformervae.
JimeiYang,YiZhou,andMichaelJBlack. Stochasticscene- InProceedingsoftheIEEE/CVFInternationalConference
awaremotionprediction.InICCV,pages11374–11384,2021. onComputerVision,pages10985–10995,2021. 3
3 [39] MathisPetrovich,MichaelJ.Black,andGu¨lVarol. TEMOS:
[24] MartinHeusel,HubertRamsauer,ThomasUnterthiner,Bern- Generatingdiversehumanmotionsfromtextualdescriptions.
hardNessler,andSeppHochreiter. Ganstrainedbyatwo InECCV,pages480–497,2022. 3
time-scaleupdateruleconvergetoalocalnashequilibrium. [40] IlijaRadosavovic,BikeZhang,BaifengShi,JathushanRa-
arXivpreprintarXiv:1706.08500,2017. 6 jasegaran,SarthakKamat,TrevorDarrell,KoushilSreenath,
[25] SiyuanHuang,ZanWang,PuhaoLi,BaoxiongJia,Tengyu and Jitendra Malik. Humanoid locomotion as next token
Liu,YixinZhu,WeiLiang,andSong-ChunZhu. Diffusion- prediction. arXiv:2402.19469,2024. 2
basedgeneration,optimization,andplanningin3dscenes. [41] DavisRempe,TolgaBirdal,AaronHertzmann,JimeiYang,
arXivpreprintarXiv:2301.06015,2023. 3 SrinathSridhar,andLeonidasJGuibas. Humor:3dhuman
motionmodelforrobustposeestimation. InICCV,pages
[26] Patrik Jonell, Taras Kucherenko, Erik Ekstedt, and Jonas
11488–11499,2021. 2
Beskow. Learning non-verbal behavior for a social robot
fromyoutubevideos. InICDL-EpiRobWorkshoponNatu- [42] MichaelARiley,MichaelJRichardson,KevinShockley,and
ralisticNon-VerbalandAffectiveHuman-RobotInteractions, Vero´nicaCRamenzoni. Interpersonalsynergies. Frontiersin
Oslo,Norway,August19,2019,2019. 3 psychology,2:38,2011. 2
[43] Ronald L Rivest, Adi Shamir, and Leonard Adleman. A
[27] Gu¨nther Knoblich, Stephen Butterfill, and Natalie Sebanz.
methodforobtainingdigitalsignaturesandpublic-keycryp-
Psychologicalresearchonjointaction:theoryanddata. Psy-
tosystems. Communications of the ACM, 21(2):120–126,
chologyoflearningandmotivation,54:59–101,2011. 2
1978. 3
[28] NhatLe,ThangPham,TuongDo,ErmanTjiputra,QuangD.
[44] YonatanShafir,GuyTevet,RoyKapon,andAmitHBermano.
Tran,andAnhNguyen. Music-drivengroupchoreography.
Humanmotiondiffusionasagenerativeprior. arXivpreprint
CVPR,2023. 3
arXiv:2303.01418,2023. 3
[29] JiamanLi,YihangYin,HangChu,YiZhou,TingwuWang,
[45] LiSiyao,WeijiangYu,TianpeiGu,ChunzeLin,QuanWang,
SanjaFidler,andHaoLi. Learningtogeneratediversedance
Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando:
motionswithtransformer. arXivpreprintarXiv:2008.08171,
3ddancegenerationviaactor-criticgptwithchoreographic
2020.
memory. InCVPR,2022. 3
[30] Ruilong Li, Shan Yang, David A. Ross, and Angjoo
[46] LiSiyao,TianpeiGu,ZhitaoYang,ZhengyuLin,ZiweiLiu,
Kanazawa. Aichoreographer:Musicconditioned3ddance
HenghuiDing,LeiYang,andChenChangeLoy. Duolando:
generationwithaist++,2021. 3
Followergptwithoff-policyreinforcementlearningfordance
[31] Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, and
accompaniment. InICLR,2024. 2,3
Lan Xu. Intergen: Diffusion-based multi-human motion
[47] SebastianStarke,HeZhang,TakuKomura,andJunSaito.
generation under complex interactions. arXiv preprint
Neuralstatemachineforcharacter-sceneinteractions. ACM
arXiv:2304.05684,2023. 3
Trans.Graph.,38(6):209–1,2019. 2
[32] Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng,
[48] OmidTaheri,VasileiosChoutas,MichaelJBlack,andDim-
ZhengqingLi,YouZhou,ElifBozkurt,andBoZheng. Beat:
itriosTzionas. Goal:Generating4dwhole-bodymotionfor
Alarge-scalesemanticandemotionalmulti-modaldatasetfor
hand-objectgrasping. InProceedingsoftheIEEE/CVFCon-
conversationalgesturessynthesis. ECCV,2022. 3
ferenceonComputerVisionandPatternRecognition,pages
[33] WeiMao,miaomiaoLiu,RichardHartley,andMathieuSalz- 13263–13273,2022. 2
mann. Contact-awarehumanmotionforecasting. InNeurIPS, [49] TaoranTang,JiaJia,andHanyangMao. Dancewithmelody:
2022. 2 Anlstm-autoencoderapproachtomusic-orienteddancesyn-
[34] JulietaMartinez,MichaelJ.Black,andJavierRomero. On thesis. InACMMM,pages1598–1606,2018. 3
humanmotionpredictionusingrecurrentneuralnetworks. In [50] ZacharyTeedandJiaDeng. Droid-slam:Deepvisualslam
CVPR,2017. 2 formonocular,stereo,andrgb-dcameras.Advancesinneural
[35] LeaMu¨ller,VickieYe,GeorgiosPavlakos,MichaelJ.Black, informationprocessingsystems,34:16558–16569,2021. 6
andAngjooKanazawa.Generativeproxemics:Apriorfor3D [51] GuyTevet,SigalRaab,BrianGordon,YoniShafir,Daniel
socialinteractionfromimages. arXivpreprint2306.09337v2, Cohen-or,andAmitHaimBermano.Humanmotiondiffusion
2023. 3 model. InICLR,2023. 3
[36] EvonneNg,HanbyulJoo,LiwenHu,HaoLi,,TrevorDarrell, [52] GuyTevet,SigalRaab,BrianGordon,YoniShafir,Daniel
Angjoo Kanazawa, and Shiry Ginosar. Learning to listen: Cohen-or,andAmitHaimBermano.Humanmotiondiffusion
Modeling non-deterministic dyadic facial motion. CVPR, model.InTheEleventhInternationalConferenceonLearning
2022. 2,3,4,6 Representations,2023. 3
10[53] JonathanTseng,RodrigoCastellon,andKarenLiu. Edge:
Editable dance generation from music. In Proceedings of
theIEEE/CVFConferenceonComputerVisionandPattern
Recognition,pages448–458,2023. 3
[54] AaronvandenOord,OriolVinyals,andKorayKavukcuoglu.
Neuraldiscreterepresentationlearning. InProceedingsof
the31stInternationalConferenceonNeuralInformationPro-
cessingSystems,pages6309–6318,2017. 2,4
[55] AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-
lia Polosukhin. Attention is all you need. arXiv preprint
arXiv:1706.03762,2017. 2
[56] R.vonLaban. Schrifttanz. Numberv.1inUniversal-Edition.
Universal-Edition,1928. 2
[57] JingboWang,YuRong,JingyuanLiu,SijieYan,DahuaLin,
and Bo Dai. Towards diverse and natural scene-aware 3d
human motion synthesis. In CVPR, pages 20460–20469,
2022. 3
[58] VickieYe,GeorgiosPavlakos,JitendraMalik,andAngjoo
Kanazawa. Decoupling human and camera motion from
videosinthewild. InProceedingsoftheIEEE/CVFCon-
ferenceonComputerVisionandPatternRecognition,pages
21222–21232,2023. 6,8
[59] HongweiYi,HualinLiang,YifeiLiu,QiongCao,Yandong
Wen,TimoBolkart,DachengTao,andMichaelJBlack. Gen-
eratingholistic3dhumanmotionfromspeech. InIEEECon-
ferenceonComputerVisionandPatternRecognition(CVPR),
pages469–480,2023. 3
[60] YifeiYin,ChenGuo,ManuelKaufmann,JuanZarate,Jie
Song,andOtmarHilliges. Hi4d:4dinstancesegmentation
ofclosehumaninteraction. InComputerVisionandPattern
Recognition(CVPR),2023. 3
[61] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli
Huang,YongZhang,HongweiZhao,HongtaoLu,andXi
Shen. T2m-gpt:Generatinghumanmotionfromtextualde-
scriptionswithdiscreterepresentations. InProceedingsof
theIEEE/CVFConferenceonComputerVisionandPattern
Recognition(CVPR),2023. 3
[62] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli
Huang,YongZhang,HongweiZhao,HongtaoLu,andXi
Shen. T2m-gpt: Generating human motion from textual
descriptions with discrete representations. arXiv preprint
arXiv:2301.06052,2023. 3,4
[63] Xiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke,
Vladimir Guzov, and Gerard Pons-Moll. Couch: towards
controllablehuman-chairinteractions. InECCV,pages518–
535,2022. 2
[64] MohanZhou,YalongBai,WeiZhang,TingYao,TiejunZhao,
andTaoMei. Responsivelisteningheadgeneration:Abench-
markdatasetandbaseline. InECCV,2022. 3
11