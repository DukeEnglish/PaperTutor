AGR: Age Group fairness Reward for Bias
Mitigation in LLMs
Shuirong Cao Ruoxi Cheng Zhiqiang Wang
AI school Chien-shiung Wu College Beijing Electronic Science
Nanjing University Southeast University and Technology Institute
Nanjing, China Nanjing, China Beijing, China
srcao@smail.nju.edu.cn 213200761@seu.edu.cn wangzq@besti.edu.cn
Abstract—LLMs can exhibit age biases, resulting in Medium-sized LLMs, such as BERT [2] and GPT-
unequal treatment of individuals across age groups. While 1 [3], generally have under a billion parameters and
muchresearchhasaddressedracialandgenderbiases,age
face two types of social biases: internal, present in
bias remains little explored. The scarcity of instruction-
the model’s pre-trained outputs, and external, affecting
tuning and preference datasets for age bias hampers
its detection and measurement, and existing fine-tuning downstreamtaskpredictions.Internaldebiasingmethods
methodsseldomaddressage-relatedfairness.Inthispaper, address biases in a pre-trained model’s outputs through
we construct age bias preference datasets and instruction- three main approaches: pre-processing [4], in-training
tuning datasets for RLHF. We introduce ARG, an age
[5],andpost-processing[6].Externaldebiasingmethods
fairness reward to reduce differences in the response
tackle biases in model predictions during downstream
quality of LLMs across different age groups. Extensive
experiments demonstrate that this reward significantly tasks, using data-centered approaches [7] to integrate
improves response accuracy and reduces performance dis- fairness goals during training. Large-scale LLMs like
parities across age groups. Our source code and datasets GPT-3 encounter greater debiasing challenges due to
are available at the anonymous link.
size and complexity, often addressed through preference
Index Terms—Age Bias, LLM Alignment, RLHF
alignment [8] and prompt engineering techniques [9].
Unlike gender and racial biases, age bias is challeng-
I. INTRODUCTION
ing due to its dynamic nature, complicating counter-
Large language models (LLMs) used in various fields factual and contrastive methods. Research on age bias
can perpetuate age biases, affecting career opportunities mitigation remains limited [10].
andhealthcare[1].Unlikefixedgenderandracialbiases, Additionally, common fine-tuning methods for LLMs
age bias is continuous and evolving. Figure 1 illustrates include instruction-based fine-tuning [11] and reinforce-
thatLLMshavethelowestaccuracyindetectingagebias ment learning with human feedback [12]. However, no
compared to other types, highlighting its complexity. instruction-based datasets address age bias, and these
methods do not target social biases, leading to potential
performance discrepancies across age groups.
To address this challenge, we revised and expanded
BBQ[13]andISB[14]datasetsandmanuallyannotated
themtocreateagepreferenceandinstructionfine-tuning
datasets for age bias. We also propose AGR, which
introducesanAgeGroupfairnessRewardtoreduceper-
formance disparities across age groups during training.
In summary, our contributions are as follows:
• We construct age bias preference and instruction
fine-tuning datasets for bias evaluation in LLMs.
• We introduce AGR, which employ a fairness re-
ward to reduce performance disparities across age
Fig. 1. Accuracy of different LLMs across various bias cate-
gories on BBQ question-answer dataset. groups,showingimprovementonBBQandagebias
instruction fine-tuning dataset.
Thefirsttwoauthorscontributedequallytothiswork.Correspond- • Experiments across various LLMs prove AGR’s
ingtoZhiqiangWang.ACKNOWLEDGMENT:wewouldliketothank
effectiveness in age bias mitigation, surpassing ex-
thecomputingresourcessupportfromtheStateKeyLaboratoryofNew
ComputerSoftwareTechnologiesatNanjingUniversity. isting related methods.
4202
peS
6
]GL.sc[
1v04340.9042:viXraII. GROUP-FAIRNESS-BASEDAGEBIASMITIGATION
A. Task Overview and Formalization Content: I saw a grandsonand their grandfatherlast week outside
the Walmart trying to book a cab on Uber.
Let M be an LLM parameterized by θ, which
Question: Who was not comfortable using the phone?
takes a text sequence x = (x ,··· ,x ) ∈ X as
1 m
input and produces an output yˆ ∈ Yˆ, where yˆ = ans 0: I choose grandpa because xxx [CE 0, LS 0, AB 0]
ans 1 : I choose son because xxx [CE 1, LS 1, AB 1]
M(X;θ) and the form of yˆ depends on the spe- .. .. ..
cific task. The input can come from a labeled dataset
ans N: I choose son because xxx [CE N, LS N, AB N]
D
=(cid:8)(cid:0) x(1),y(1)(cid:1)
,···
,(cid:0) x(N),y(N)(cid:1)(cid:9)
, or an unlabeled
Augment Rewrite Rank
dataset of sentence continuations and prompt comple-
tions D =(cid:8) x(1),··· ,x(N)(cid:9) . …
Age debiasing in LLMs can be framed as ensuring GPT Human annotators
that the model treats all age groups fairly. Specifically,
for a model M and its output yˆ = M(x;θ), given a Fig.2. Overview of Preference Dataset Construction.
set of age groups g, age group fairness requires that the
statistical measures M y(g) of the model’s output for all Constructing the Age-Attribute Preference Dataset in-
differentagegroupsg ∈Gareapproximatelyequal,i.e.: volves manually expanding the ISB dataset. The pro-
cess is similar to that for the Age-Behavior Preference
|M (g)−M (g′)|⩽ϵ
y y Dataset, with both datasets split into training and test
where the choice of M specifies a fairness constraint, sets at a 0.95:0.05 ratio. Examples of these datasets can
and M could be accuracy, true positive rate, etc. be found in the anonymous GitHub link.
C. Instruction Fine-Tuning Dataset Construction
B. Construction of Age Bias Preference Datasets
To further test age bias in LLMs across different
We extract samples related to age bias from BBQ
age groups within the same context, we construct the
[13] question-answering dataset and ISB [14] dataset to
instructionfine-tuningdatasetsABMB-IFTandABMA-
construct two preference datasets: Age Bias Mitigation
IFT based on the original BBQ and ISB datasets. The
for Behavior (ABMB) and Age Bias Mitigation for
process includes:
Attribute (ABMA). Then we construct instruction fine-
• Question Rewriting: Extract age groups from the
tuning datasets ABMB-IFT and ABMA-IFT based on
context and answers of each sample, then rewrite
these preference datasets.
the questions using each age group.
1) ResponseGeneration: Basedonthecontext,ques-
• Response Generation: Determine tag category
tion,andeachcandidateanswer,GPT-3.5Turborewrites
(“Yes” or “No”) for rewritten questions based on
the answers to create a modified dataset.
labeledanswers.UseGPT-3.5-Turbotoexpandtags
2) Response Adjustment and Evaluation: We adjust and add explanations based on context.
the responses provided by GPT-3.5-Turbo and recruit
Age group classifications vary by country, culture,
five annotators to evaluate each response based on the
and field and can change over time. For simplicity, we
following three criteria:
define age groups as: 10-29 years (young adults), 30-59
• Communication Effectiveness (CE) measures flu- (middle-aged), and 60+ (elderly).
ency and grammar, scoring 1 to 3. Higher scores
D. Age Group Fairness Reward
indicate more natural language.
RLHF directly uses the output of a trained preference
• Logical Soundness (LS) assesses logical coher- model as the reward value in the reinforcement learning
ence, scoring 1 to 3. Scores higher, logic better.
phase, without considering fairness in response quality
• Age-related Bias (AB) evaluates age bias, scoring acrossdifferentagegroupsunderpromptparadigms.We
1 to 3. Higher scores indicate less bias.
propose an age group fairness reward signal.
Final score for each dimension is the most common Given a LLM M parameterized by θ, and inputs
annotationscore.Totalqualityscoreforeachresponseis for different age groups a ∈ {x ,x ,x }, and
young middle old
the sum of scores across all three dimensions. Figure 2 their corresponding outputs y ∈ {y ,y ,y },
a young middle old
shows the preference dataset construction process. we define a reward signal R to train the preference
3) ResponseRanking: Duetovariedannotatorvalues, model P, aligning the LLM with human preferences
quality scores are noisy. Ranking responses standardizes and mitigating age-related bias. For a set of age groups
comparisons among models. We use a dataset format A={young,middle,old},wecalculatethequalityscore
like Nakano et al. [15] and Bai et al. [16], where each ofthemodeloutputforeachagegroupa∈A,denotedas
item has a query with two responses, ranked by quality. Q(y | x ). The quality score Q measures whether the
a a
Invalid pairs with identical scores are discarded. model’s output meets predefined fairness requirements.Step 1 Step 2 reward, which quantifies the fluency, logical soundness,
A AB BM MA B-- II FF TT oo rr + Pre Ltr La Mined => SFT A （BM PaA i ro dr aA tB a）MB+ Pre Ltr La Mined => PM and age bias of textual responses corresponding to input
prompts x = (x ,x ,··· ,x ) ∈ X and text responses
1 2 N
y = (y ,y ,··· ,y ) ∈ Y. Given an input x and a
Step 3 Actor model ReferenF cr eo z me on del Output of Actor model Frozen pair of r1 espo2 nses (cid:0) yM good,ybad(cid:1) , where ygood represents a
Preference
Remax Age group fairness reward model high-quality response and ybad represents a low-quality
response, the reward model Rλ should establish a pref-
Fig.3. Overview of the Three Steps of AGR. erence for ygood, i.e., Rλ(cid:0) x,yθ good(cid:1) >Rλ(cid:0) x,ybad(cid:1) .
θ θ
Therefore, given the preference data tuple D =
(cid:8)(cid:0) x,ygood,ybad(cid:1)(cid:9)
,wetraintherewardmodelbyincreas-
For any two different age groups a,b ∈ A,a ̸= b,
ing the gap between
Rλ(cid:0) x,ygood(cid:1)
and
Rλ(cid:0) x,ybad(cid:1)
.
we quantify age bias between any two individuals of θ θ
Basedonthisidea,thischapteradoptsthebinaryranking
different age groups by calculating the absolute value of
loss function to measure the accuracy of the preference
the difference in quality scores:
model’s ranking:
D a,b(y |x)=|Q(y a |x a)−Q(y b |x b)| L =−E logσ(cid:0) ∆R (cid:1) ,
Ranking (x,ygood,ybad)∼D θ
Next,weusethetotaldifferenceacrossallagegroups
where ∆R =R
(cid:0) x,ygood(cid:1)
−R
(cid:0) x,ybad(cid:1)
and σ(·) is
to measure the extent of age bias in the LLM: θ θ θ
the Sigmoid function.
(cid:88)
D = D (y |x) 3) Reinforcement Learning Fine-Tuning with Prefer-
total a,b
a,b∈A ence Model: AGR updates LLM parameters using the
a̸=b group fairness reward Rλ provided by the preference
θ
Finally, the reward signal Rλ combines the quality model to guide the LLM in generating outputs with
θ
scores Q for each age group and penalizes the total lowerbias.WeuseRemaxalgorithm[19]tooptimizethe
disparity D total to encourage fairness: supervised fine-tuned base model using the preference
model trained in the second step. The objective function
(cid:88)
Rλ(x,y)= Q(y |x )−λ·D is as follows:
θ a a total
a∈A J(ϕ)=E [R (x,y)]−βD (cid:0) πRL∥πSFT(cid:1)
Here, λ is the coefficient for age group fairness regu- y∼πRL(·|x) θ KL ϕ
ϕ
larization.Itbalancesmodeloutputqualitywithfairness, where πRL is the learned policy, πSFT is the supervised
ϕ
where an increase in λ results in reduced disparity in
fine-tuned model, D is the KL divergence, and β is
KL
response quality across age groups. The reward signal
a constant coefficient. This objective function uses the
R θλ integratesthequalityscoresQandpenalizesthetotal policy gradient method to learn the optimal policy πRL
ϕ
difference to ensure fairness in model outputs.
that maximizes J(ϕ).
E. Training Process of AGR
We propose AGR, which uses Rλ to train the prefer- III. EXPERIMENTS
θ A. Baseline
encemodelandleverageitinthereinforcementlearning
We test four open-source models—Llama2-7B-
phasetooptimizemodelparametersandreduceagebias.
base [20], Qwen1.5-7B-base1, ChaGLM3-6B-base2, and
AGR employs a three-stage process, similar to RLHF,
Baichuan2-7B [21]—for supervised learning. Qwen1.5-
to fine-tune the base model for age bias mitigation, as
7B achieves the highest ranking accuracy, so it is used
illustrated in Figure 3.
as the base model for all reward models.
1) Supervised Fine-Tuning: The LLM is fine-tuned
We empirically compare AGR with the following
based on the conditional probability distribution y ∼
SOTA bias mitigation methods.
P(·|x;θ), where θ represents the initialization param-
• DePrompt [22] uses debias-prompt like “Note that
eters. We perform supervised fine-tuning of the LLM
the answer does not rely on stereotypes.” directly.
using ABMB-IFT and ABMA-IFT datasets, injecting
• KG-Debias[23]collectsrelevantnounsandobtains
age bias mitigation knowledge into the pre-trained base
structured knowledge, which is then converted into
LLM. This process aims to enhance response to spe-
sentences and applied to LLMs.
cificcontextualquestionsandacceleratetheconvergence
• SFT-LoRA [24] freezes pre-trained model weights
speed of the reinforcement learning phase.
and introduces trainable low-rank decomposition
2) TrainingthePreferenceModel: Formally,aprefer-
matrices in each layer of transformer to reduce
encemodel[17]orrewardmodel[18]canberepresented
parameters number for downstream tasks.
as a parameterized mapping function Rλ :X×Y →R,
θ
which provides a real-valued reward (or preference) 1https://huggingface.co/Qwen/Qwen1.5-7B
score Rλ(x,y). We use the proposed age group fairness 2https://huggingface.co/THUDM/chatglm3-6b
θTABLEI TABLEII
COMPARISONWITHBASELINESONABMA-IFT, COMPARISONWITHBASELINESONDIFFERENTAGE
ABMB-IFT,ANDBBQDATASETS. GROUPSONABMA-IFTANDABMB-IFTDATASTS.
Model Method ABMA-IFT ABMB-IFT BBQ(Age) Model Method ABMA-IFT ABMB-IFT
Tag Content T&C Tag Content T&C Answer Tag&Content Tag&Content
Base 0.755 0.657 0.613 0.637 0.518 0.483 0.358 Young Middle-age Old Young Middle-age Old
DePrompt 0.807 0.742 0.719 0.675 0.643 0.581 0.395 Base 0.641 0.614 0.584 0.521 0.481 0.447
Qwen1.5-7B K SFG RT- L-D L He ob FRia As 00 0 .. . 87 8 39 5 94 7 0 0 0. . .7 8 83 1 45 4 5 00 0 .. . 87 7 12 8 33 1 00 0 .. . 86 8 09 4 32 7 00 0 .. . 86 7 35 7 95 9 00 0 .. . 76 7 80 3 27 5 00 0 .. . 64 6 85 9 94 7 Qwen1.5-7B K SD FGe TP - -Dr Lo e om b Rip a At s 0 00. ..7 774 965 73 0 00. ..7 772 939 35 0 00. ..6 768 573 41 0 00. ..6 762 547 55 0 00. ..5 768 514 87 0 00. ..5 653 952 29
AGR(ours) 0.863 0.876 0.852 0.869 0.851 0.836 0.713 RLHF 0.835 0.816 0.788 0.813 0.794 0.739
Base 0.682 0.571 0.513 0.513 0.372 0.357 0.343 AGR(ours) 0.862 0.857 0.837 0.855 0.849 0.804
DePrompt 0.743 0.651 0.621 0.642 0.535 0.493 0.379 Base 0.523 0.531 0.485 0.364 0.378 0.329
Llama2-7B K SFG RT- L-D L He ob FRia As 00 0 .. . 87 8 55 6 16 9 0 0 0. . .6 7 88 9 25 2 7 00 0 .. . 76 7 93 6 26 8 0 00 . .. 8 76 7 95 2 19 00 0 .. . 86 7 01 6 71 3 00 0 .. . 75 7 67 2 95 4 00 0 .. . 64 6 44 3 55 2 Llama2-7B K SD FGe TP - -Dr Lo e om b Rip a At s 0 00. ..6 762 838 15 0 00. ..6 764 779 82 0 00. ..5 768 406 51 0 00. ..5 750 363 23 0 00. ..5 764 452 92 0 00. ..4 653 914 10
AGR(ours) 0.884 0.853 0.837 0.843 0.839 0.813 0.672 RLHF 0.793 0.815 0.768 0.774 0.797 0.736
Base 0.667 0.564 0.497 0.527 0.412 0.362 0.327 AGR(ours) 0.839 0.848 0.824 0.817 0.835 0.787
DePrompt 0.737 0.689 0.653 0.644 0.523 0.485 0.384 Base 0.522 0.497 0.472 0.390 0.359 0.337
ChatGLM-6B K SFG RT- L-D L He ob FRia As 00 0 .. . 87 8 44 8 71 2 0 0 0. . .7 8 80 1 13 6 3 00 0 .. . 76 7 88 7 54 9 00 0 .. . 86 8 23 3 71 3 00 0 .. . 75 7 98 5 66 9 0 00 . .. 7 75 2 53 4 36 0 00 . .. 5 54 9 72 1 59 ChatGLM-6B K SD FGe TP - -Dr Lo e om b Rip a At s 0 00. ..7 871 052 52 0 00. ..6 765 985 14 0 00. ..5 769 412 16 0 00. ..5 762 417 52 0 00. ..4 759 336 27 0 00. ..4 643 952 59
AGR(ours) 0.879 0.851 0.823 0.841 0.807 0.781 0.586 RLHF 0.798 0.804 0.753 0.772 0.759 0.728
Base 0.697 0.553 0.506 0.527 0.433 0.398 0.352 AGR(ours) 0.832 0.828 0.809 0.797 0.781 0.765
DePrompt 0.791 0.712 0.683 0.674 0.575 0.529 0.389 Base 0.524 0.513 0.481 0.423 0.397 0.374
Baichuan2-7B K SFG RT- L-D L He ob FRia As 00 0 .. . 87 8 58 7 80 4 0 0 0. . .7 8 82 3 39 4 7 00 0 .. . 86 7 09 9 44 6 00 0 .. . 86 8 18 2 65 4 00 0 .. . 76 7 71 5 48 9 0 00 . .. 7 75 2 47 9 54 0 00 . .. 6 64 5 83 3 16 Baichuan2-7B K SD FGe TP - -Dr Lo e om b Rip a At s 0 00. ..7 872 249 61 0 00. ..6 868 093 49 0 00. ..6 763 547 82 0 00. ..5 768 527 47 0 00. ..5 753 374 65 0 00. ..4 656 926 70
AGR(ours) 0.872 0.849 0.817 0.837 0.801 0.776 0.697 RLHF 0.810 0.827 0.775 0.769 0.741 0.725
AGR(ours) 0.823 0.836 0.792 0.784 0.775 0.769
• RLHF [12] uses reinforcement learning with hu-
AGR with age group fairness rewards significantly
manfeedbacktofine-tuneLLMs,utilizingareward
enhances content and combined tag/content accuracy
model based on output preferences.
overRLHF.OnABMA-IFT,AGRboostsaccuracybyat
B. Metrics
least 3% for most models, except Baichuan2-7B, which
Followingpreviousworks[13],[14],weusequestion-
showsa1.7%improvement.OnABMB-IFT,itincreases
answering accuracy to compare bias levels in BBQ-
tag/contentaccuracybyatleast2.9%,withQwen1.5-7B
Age, ABMB-IFT, and ABMA-IFT test sets. Tag accu-
improving by 5.4%. Fairness rewards enhance consis-
racy measures “Yes” or “No” response accuracy, while
tency by penalizing score differences across age groups,
contentaccuracychecksalignmentwithreferenceexpla-
exposing age bias during fine-tuning.
nations. Higher values indicate lower age bias.
Table II shows that AGR improves Tag&Content ac-
C. Settings
curacyacrossagegroupscomparedtobaselinemethods.
Experiments are conducted on four NVIDIA V100
Qwen1.5-7B, for example, increases accuracy by 2.7%,
GPUs (32GB each). For supervised fine-tuning, the
4.1%,and4.9%forYoung,Middle-aged,andOldgroups
learning rate is 5 × 10−5 with a batch size of 8 per
on the ABMA-IFT dataset, and by 4.2%, 5.5%, and
GPU and 3 epochs. Preference model training uses a
6.5% on the ABMB-IFT dataset. This demonstrates
learning rate of 3×10−4, batch size of 8, and 1 epoch.
AGR’seffectivenessinenhancingagegroupfairnessand
Final token embeddings are processed through a linear
reducingaccuracygaps.ForQwen1.5-7BontheABMA-
layer for quality scoring. Reinforcement learning fine-
IFTdataset,theaccuracygapbetweenelderlyandyoung,
tuning employs a learning rate of 1×10−6, batch size
and middle-aged groups was reduced from 2.8% and
of2,and1epoch,withacosineannealingscheduler[25]
4.7% to 2% and 2.5%.
and a maximum text length of 512. The fairness reward
coefficient λ is 0.5 for ABMA-IFT and 0.7 for ABMB- IV. CONCLUSION
IFT. Models use FP16 during reinforcement learning. WedevelopedABMAandABMBpreferencedatasets
Preference and reference models have a zero-stage of 3 and ABMA-IFT and ABMB-IFT instruction fine-tuning
andareloadedintoGPUmemoryonlyduringinference, datasets to address age bias in LLMs under prompt-
while the actor model has a zero-stage of 2. basedparadigms.Byframingagebiasasafairnessissue
D. Results and introducing an age fairness reward into AGR, we
Table I shows that base versions of the four 7B- aimed to reduce quality disparities across age groups
parameterLLMsperformbetterontagandcontentaccu- while preserving overall model performance. Experi-
racyintheABMA-IFTtestsetcomparedtotheABMB- ments show that AGR significantly improves accuracy
IFT test set, indicating lower bias in age attributes than and reduces age-related performance gaps compared to
agebehavior.Tagaccuracygenerallyexceedscontentac- existing methods.
curacy,highlightinganeedforimprovedself-explanation
and reasoning in open-source LLMs.REFERENCES [16] YuntaoBai,AndyJones,KamalNdousse,AmandaAskell,Anna
Chen,NovaDasSarma,DawnDrain,StanislavFort,DeepGan-
guli, Tom Henighan, et al. Training a helpful and harmless
[1] SunipaDevandJeffPhillips. Attenuatingbiasinwordvectors.
assistant with reinforcement learning from human feedback.
InProceedingsofthe22ndinternationalconferenceonartificial
arXiv:2204.05862,2022.
intelligenceandstatistics,pages879–887,2019.
[17] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep
[2] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben
Toutanova. Bert: Pre-training of deep bidirectional transform- Mann,NovaDasSarma,etal. Agenerallanguageassistantasa
ers for language understanding. In Proceedings of the 2019 laboratoryforalignment. arXiv:2112.00861,2021.
Conference of the North American Chapter of the Association [18] FeiLiuetal. Learningtosummarizefromhumanfeedback. In
for Computational Linguistics: Human Language Technologies, Proceedings of the 58th Annual Meeting of the Association for
pages4171–4186,2019. ComputationalLinguistics,pages583–592,2020.
[3] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya [19] Ziniu Li, Tian Xu, Yushun Zhang, Zhihang Lin, Yang Yu,
Sutskever,etal.Improvinglanguageunderstandingbygenerative Ruoyu Sun, and Zhi-Quan Luo. Remax: A simple, effective,
pre-training. 2018. and efficient reinforcement learning method for aligning large
[4] HimanshuThakur,AtishayJain,PraneethaVaddamanu,PaulPu language models. In Proceedings of the 41st International
Liang, and Louis-Philippe Morency. Language models get a ConferenceonMachineLearning,pages29128–29163,2024.
gender makeover: Mitigating gender bias with few-shot data [20] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,Amjad
interventions. InProceedingsofthe61stAnnualMeetingofthe Almahairi,YasmineBabaei,NikolayBashlykov,SoumyaBatra,
AssociationforComputationalLinguistics,pages340–351,2023. Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open
[5] YueGuo,YiYang,andAhmedAbbasi. Auto-debias:Debiasing foundationandfine-tunedchatmodels. arXiv:2307.09288,2023.
masked language models with automated biased prompts. In [21] Aiyuan Yang, Bin Xiao, Bingning Wang, Borong Zhang,
Proceedings of the 60th Annual Meeting of the Association for Ce Bian, Chao Yin, Chenxu Lv, Da Pan, Dian Wang, Dong
ComputationalLinguistics,pages1012–1023,2022. Yan, et al. Baichuan 2: Open large-scale language models.
[6] ShadiIskander,KiraRadinsky,andYonatanBelinkov. Shielded arXiv:2309.10305,2023.
representations: Protecting sensitive attributes through iterative [22] RemHida,MasahiroKaneko,andNaoakiOkazaki. Socialbias
gradient-based projection. In Findings of the Association for evaluationforlargelanguagemodelsrequirespromptvariations.
ComputationalLinguistics,pages5961–5977,2023. arXivpreprintarXiv:2407.03129,2024.
[7] Somayeh Ghanbarzadeh, Yan Huang, Hamid Palangi, [23] Congda Ma, Tianyu Zhao, and Manabu Okumura. Debiasing
Radames Cruz Moreno, and Hamed Khanpour. Gender-tuning: largelanguagemodelswithstructuredknowledge.InFindingsof
Empowering fine-tuning for debiasing pre-trained language theAssociationforComputationalLinguisticsACL2024,pages
models. In Findings of the Association for Computational 10274–10287,2024.
Linguistics,pages5448–5458,2023. [24] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li,
[8] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank
Wainwright,PamelaMishkin,ChongZhang,SandhiniAgarwal, adaptationoflargelanguagemodels. InProceedingsofthe9th
Katarina Slama, Alex Ray, et al. Training language models to InternationalConferenceonLearningRepresentations,2021.
follow instructions with human feedback. Proceedings of the [25] IlyaLoshchilovandFrankHutter. Sgdr:Stochasticgradientde-
36thInternationalConferenceonneuralinformationprocessing scentwithwarmrestarts.InProceedingsofthe4thInternational
systems,35:27730–27744,2022. ConferenceonLearningRepresentations,2016.
[9] Alex Tamkin, Amanda Askell, Liane Lovitt, Esin Durmus,
NicholasJoseph,ShaunaKravec,KarinaNguyen,JaredKaplan,
and Deep Ganguli. Evaluating and mitigating discrimination in
languagemodeldecisions. arXiv:2312.03689,2023.
[10] Ruoxi Cheng, Haoxuan Ma, and Shuirong Cao. Deceiving
to enlighten: Coaxing llms to self-reflection for enhanced bias
detectionandmitigation.arXivpreprintarXiv:2404.10160,2024.
[11] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,
Adams Wei Yu, Brian Lester, Nan Du, Andrew M Dai, and
Quoc V Le. Finetuned language models are zero-shot learners.
InProceedingsofthe10thInternationalConferenceonLearning
Representations,2022.
[12] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll
Wainwright,PamelaMishkin,ChongZhang,SandhiniAgarwal,
Katarina Slama, Alex Ray, et al. Training language models to
followinstructionswithhumanfeedback. InProceedingsofthe
36thInternationalConferenceonneuralinformationprocessing
systems,volume35,pages27730–27744,2022.
[13] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Pad-
makumar, Jason Phang, Jana Thompson, Phu Mon Htut, and
SamuelBowman.Bbq:Ahand-builtbiasbenchmarkforquestion
answering. In Findings of the Association for Computational
Linguistics,pages2086–2105,2022.
[14] Mahammed Kamruzzaman, Md Minul Islam Shovon, and
Gene Louis Kim. Investigating subtler biases in llms: Ageism,
beauty, institutional, and nationality bias in generative models.
arXiv:2309.08902,2023.
[15] Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long
Ouyang,ChristinaKim,ChristopherHesse,ShantanuJain,Vineet
Kosaraju, William Saunders, et al. Webgpt: Browser-assisted
question-answering with human feedback. arXiv:2112.09332,
2021.