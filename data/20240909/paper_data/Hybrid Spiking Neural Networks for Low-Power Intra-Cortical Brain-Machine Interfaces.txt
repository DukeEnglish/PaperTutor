Hybrid Spiking Neural Networks for Low-Power
Intra-Cortical Brain-Machine Interfaces
Alexandru Vasilache∗,1,2 Jann Krausse∗,1,3
Klaus Knobloch,3 Juergen Becker1
1 Karlsruhe Institute of Technology, Karlsruhe, Germany
2 FZI Research Center for Information Technology, Karlsruhe, Germany
3 Infineon Technologies, Dresden, Germany
Abstract—Intra-cortical brain-machine interfaces (iBMIs) First, implants are connected via bulky wiring to the oper-
have the potential to dramatically improve the lives of people atingequipment,severelyrestrictingthepatient’smovement
with paraplegia by restoring their ability to perform daily
[6]. Second, permanently opening the skull to allow wiring
activities. However, current iBMIs suffer from scalability and
increases the risk of infection [7]. In hopes of mitigating
mobility limitations due to bulky hardware and wiring. Wire-
lessiBMIsofferasolutionbutareconstrainedbyalimiteddata this, research is moving towards wireless iBMIs [6], [8].
rate. To overcome this challenge, we are investigating hybrid The Grand Challenge on Neural Decoding for Motor
spiking neural networks for embedded neural decoding in
Control of non-Human Primates of IEEE BioCAS 2024
wirelessiBMIs.Thenetworksconsistofatemporalconvolution-
calls for solutions to the scalability issues of such wireless
based compression followed by recurrent processing and a
final interpolation back to the original sequence length. As BMIs.Sincedataratesarelimitedduetobit-errorrates,heat
recurrentunits,weexploregatedrecurrentunits(GRUs),leaky dissipation, and battery lifetime, an optimal solution should
integrate-and-fire (LIF) neurons, and a combination of both - handle the trade-off between high-quality neural decoding,
spiking GRUs (sGRUs) and analyze their differences in terms
data compression, and resource management. As the devel-
of accuracy, footprint, and activation sparsity. To that end,
opment of techniques for embedded artificial intelligence
we train decoders on the "Nonhuman Primate Reaching with
Multichannel Sensorimotor Cortex Electrophysiology" dataset progresses,neuralnetworkspresentpromisingcandidatesfor
and evaluate it using the NeuroBench framework, targeting wireless low-power neural decoders [9], [10]. Additionally,
both tracks of the IEEE BioCAS Grand Challenge on Neural biologically inspired spiking neural networks (SNNs) ben-
Decoding. Our approach achieves high accuracy in predicting
efit from high temporal sparsity, single-bit communication
velocities of primate reaching movements from multichannel
facilitated by spikes, and an intrinsic recurrence due to their
primary motor cortex recordings while maintaining a low
numberofsynapticoperations,surpassingthecurrentbaseline statefulness [11]. Consequently, participants of the Grand
modelsintheNeuroBenchframework.Thisworkhighlightsthe Challenge on Neural Decoding are tasked with training a
potentialofhybridneuralnetworkstofacilitatewirelessiBMIs neural network on the Primate Reaching dataset [12] for
with high decoding precision and a substantial increase in the
predicting the velocities of cursor movements. The network
number of monitored neurons, paving the way toward more
isthenevaluatedusingtheNeuroBenchframeworktoobtain
advanced neuroprosthetic technologies.
Index Terms—spiking neural network, neural decoding, metrics regarding accuracy and resources [13]. Results are
brain machine interface, neurobench judged based on two challenge tracks: track 1 assesses
sole accuracy optimization, while track 2 targets the co-
I. INTRODUCTION
optimization of accuracy, memory footprint, and number of
Tens of millions of lives worldwide are suffering from compute operations, as defined in [13].
paralysis [1], [2]. Those affected experience an impaired
Our work presents a hybrid network architecture of tem-
ability to direct their movements, which, in severe cases,
poral convolutions in combination with recurrent processing
leadstoacompletelossofmotorcontrol.Thismotivatesthe
andasubsequentinterpolationbacktotheoriginalsequence
development of technology that can decode patients’ brain
length.WhileGRUsareveryeffectiveinsequencemodeling
activity and accordingly control assistive prostheses. Such
[14], networks based on spiking neurons like the LIF model
devices are called brain machine interfaces (BMIs) [3] and
profit from the advantages of SNNs regarding resourceful-
have been very successful with restoring motor control [4],
ness mentioned above [15]. Hence, we investigate recurrent
sensory information [5], or even emotional responses [4].
processing by GRUs, LIF units, and a combination of both
Usually, BMIs are directly placed on the surface of a
and discuss the differences in their results.
patient’sbraintoensurethemaximalqualityoftherecorded
Furthermore, we motivate the chosen architecture via a
brain signals (iBMIs). However, this raises two problems.
few experiments before presenting the results of all three
The project on which this report is based was sponsored by the Ger- types of recurrence. All three network types beat the base-
man Federal Ministry of Education and Research under grant number lines given by [13] in at least one of the challenge tracks by
16ME0801.Theresponsibilityforthecontentofthispublicationlieswith
agoodmargin.However,thedifferentrecurrencetypesshow
theauthor.
*Theseauthorscontributedequallytothiswork. evident differences in accuracy and resourcefulness. Based
4202
peS
6
]GL.sc[
1v82440.9042:viXraon that, we will discuss the implications of using spiking 0.05 originaldata
elements. Finally, we point out the possibilities of the real- interpolateddata keypoints
timedeploymentofthesenetworksandareasoffuturework. 0
II. RELATEDWORK
-0.05
The authors of [16] used SNNs to predict a rhesus -0.1 -0.05 0 0.05 0.1 0.15
monkey’s arm velocity accurately. However, the network vx/ms−1
was not trained directly on the data. Instead, they mapped a
Fig. 1: Linear interpolation of discretized cursor velocities (8 steps) visu-
Kalman filter onto the network. alizedaboveoriginalcursorvelocities.
In[17],theauthorstrainSNNsontwodatasetsforoffline
finger velocity decodings. They achieve high accuracy and Input
compare their approach to the artificial neural networks
(ANNs) baseline, even specifying numbers for total opera- Convolutional
tions and memory accesses. Still, their network represents a MaxPool
simplefeed-forwardarchitectureandistrainedonadifferent Dropout
×n
dataset than this work.
ConvolutionalOutput
The clear baseline for this work is given by [13]. Among
otherdatasets,theauthorsmakethedatasetof[12]available Recurrent Recurrent
for deep learning approaches and subsequently train neural
Linear Linear
networks as baselines. They differentiate between ANNs interpolation
Keypoint Keypoint
and SNNs as well as between networks that target pure
reconstruction accuracy (track 1) and those that co-optimize Output
accuracyandresourcedemands(track2).Theusednetworks Fig.2:Illustrationofthegeneralnetworkarchitectureusedinthiswork.
are of relatively simple architecture. Their work aims to
enable others to benchmark respective datasets easily. We
by recurrent units and a fully connected layer to determine
willmakeuseoftheirworkandsurpasstheirbaselineusing
output velocities as keypoints. We apply linear interpola-
a different network architecture in both challenge tracks.
tion between the determined keypoints to scale the output
III. METHODS sequence back to the original sequence length.
A. Motivating an Interpolation-based Approach Here, we compare three types of recurrent units for the
architecture described above. Those comprise GRU and LIF
Our interpolation approach is inspired by observing pri-
units, as well as a fusion of both, which we call the sGRU.
mate cursor movements. In the video, a new target appears
We define the sGRU as
each time the previous one is reached, prompting a rapid,
goal-directed movement toward it. This suggests that the
r =LIF(W x +U h ), (1)
movement can be approximated by discrete, target-locked t r t r t−1
actions rather than fine-grained continuous adjustments. z =LIF(W x +U h ), (2)
t z t z t−1
Based on this, we hypothesize that capturing a few
h˜ =LIF(W x +U ((1−r )⊙h )), (3)
keypoints along the velocity trajectory and interpolating t h t h t t−1
between them can effectively approximate the whole move- h =(1−z )⊙h +z ⊙h˜ , (4)
t t t−1 t t
ment velocity. Fig. 1 illustrates this concept by comparing
the original movement with a simplified version, where where r t, z t, h˜ t, and h t denote reset gate, update gate, can-
the velocity at every eighth point is retained, and linear didate hidden state, and hidden state at time t, respectively.
interpolation is used between them. We argue that the W r, W z, W h, U r, U z, and U h are learnable parameters.
resulting error is negligible, assuming that the keypoint x t denotestheinput.LIFreferstotheimplementationofthe
prediction is of high quality, as the R2 score between the LIF spiking neuron model presented in [15].
interpolated test set and the original test set is 0.998 with 4- Fig.3displaystheintermediarystatesofallthreenetwork
step interpolation, 0.988 with 8-step interpolation and 0.955 types for visualization. Based on this general architecture,
with 16-step interpolation. wepresent2modelsizes,targetingtrack1(GRU-t1,sGRU-
t1, LIF-t1) and track 2 (GRU-t2, sGRU-t2, LIF-t2) of the
B. Model Architecture
Neural Decoding Challenge. The LIF networks additionally
The general architecture of the model (Fig. 2) involves use recurrent weights. Track 1 models employ three con-
temporal convolutions to reduce the number of time steps volutional blocks with 32 channels, kernel sizes of 3, 6,
in a sequence of neuron recordings from the input size of and 12, and padding sizes of 5, 3, and 6, targeting 8-step
1024 to the desired number of keypoints and efficiently interpolation with 127 keypoints. All max pooling layers
extracttemporalfeatures.Tocreatesufficientkeypointpairs, use a kernel size and stride of 2. The size of the recurrent
convolutional blocks reduce the sequence to a length of blocks is 64. Track 2 models use two convolutional blocks
number of keypoints + 1. These features are then processed with 10 channels and a kernel size of 3, which reduce the
1−sm/yvTableII:ComparisonofR2scoresfordifferentnumberofkeypoints.The
Input networksarebasedontheGRUunitanddonotdifferinmemoryfootprint.
Conv1
Dense Effective
Pool1 Keypoints(Interpolation) R2
Operations MACs
Conv2
1025(1-step) 46400.3 37184.3 0.736
Pool2 513(2-step) 27808.3 18592.3 0.766
257(4-step) 20054.3 10838.3 0.764
GRU sGRU LIF 129(8-step) 17734.3 8518.3 0.779
Fig.3:LayervisualisationsforGRU-t2,sGRU-t2,LIF-t2.
TableI:TradeoffbetweenmodelsizeandR2Score Input
Conv1
ConvChannelsxGRUHiddenSize R2Score
Pool1
10x20 0.667
32x64 0.692 Conv2
64x128 0.687
128x256 0.661 Pool2
Fig.4:ReceptiveFieldVisualisation
input size to 257 keypoints, effectively targeting a 4-step
interpolation. To achieve the number of keypoints, the first increased from 0.615 to 0.707 compared to B-ANN3, while
convolutional layer uses a padding of 3, while the second using 26% fewer MACs and 34% less Dense operations.
convolutional layer employs a padding size of 1. The max
For track 2, we compare GRU-t2 with B-ANN2; it has
pooling layers both use a kernel size and stride of 2. The
only roughly 13% of the MACs and an increased R2 score
size of the recurrent blocks is 20. (+0.045). sGRU-t2 uses 60% of the ACs, with the same R2
IV. EXPERIMENTSANDOBSERVATIONS score when compared to B-SNN2 and 8% of the MACs for
the same R2 score when compared to B-ANN2. The LIF-t2
Tounderstandtherelationshipbetweenmodelsizeandthe
model achieves the same R2 score, with roughly the same
R2scoreandthetradeoffthatcomeswithit,wetrainedfour
activation sparsity, while only using 63% of the Dense and
networks of different hidden sizes. Due to time limitations,
60% of the ACs when compared to B-SNN2.
we performed this experiment only for the sGRU model,
training only on the indy2016062201 file with fewer data
B. Recurrence Comparison
samples. Table I displays the respective results.
By far, the best performance has been achieved by the
Additionally, we study the influence of the number of
GRUrecurrentunitforbothinvestigatedsizes.Furthermore,
keypoints on the R2 score by training four networks with
the GRU also gives the best trade-off between footprint and
1025to129keypoints(1-stepto8-stepinterpolation).Again,
R2 score. Across both sizes, the lowest number of synaptic
we trained the networks only on the indy2016062201 file
operations (Dense and MACs) is achieved by the LIF recur-
withfewerdatasamples.TableIIpresentstheresultsforthe
rence,whichreachesthehighestactivationsparsity,ascanbe
GRU model. Note that fewer keypoints directly translate to
visually confirmed in Fig. 3. The sGRU recurrence achieves
ahigherR2score.ThistrendwasalsoconfirmedforsGRU-
higher activation sparsity and lower MACs than the GRU
and LIF-based networks.
forthesamenumberoftotalsynapticoperationsandACsat
Wealsoranexperimentstoevaluatethetestperformances
thecostofahigherfootprintandlowerR2score.Compared
of models trained on all three recordings for each primate.
to the LIF, sGRU consistently displays a slightly higher
Interestingly, the R2 score decreases when using aggregated
R2, hinting at improved memory management, compared to
data, contrasting the expected increase in generalizability
solely using LIF neurons.
due to a more representative training set. This hints at a
possible change or degradation of the signal recording from
VI. DISCUSSION
the intracortical electrodes across time.
We hypothesize that the reason for the higher achieved
V. RESULTS
R2 score, given a sufficiently large receptive field (as seen
A. Baseline Comparison inourmodelsproposedfortrack1),maybethatthefiltering
WepresentthebestresultsweobtainedforGRU-,sGRU-, operationsperformedbytheconvolutionallayersofferbetter
andLIF-basednetworksforchallengetracks1and2inTable informationaggregationacrosstime,comparedtothesimple
III,onthemetricsdefinedin[13].Comparingourmodelsto summing aggregation used by the baseline model B-SNN3.
those provided by the baselines in [13], we notice a larger
footprint due to the increased input buffer size required for Theproposedmodelsuseaninputbufferwindowof1024
aninputofsize1024andtheconvolutionalblocks.However, steps provided by the NeuroBench [13] Primate Reaching
ourmodelspresentfewersynapticoperations,judgingbythe Dataset, where each step represents 4 ms. This results in a
Dense, MACs, and ACs values. total buffer window and a latency of 4.096 s. The models
All our track 1 models achieve equal or higher R2 scores are executed for non-overlapping windows of size 1024,
thanthebaselines,withGRU-t1reachinganR2scorethatis meaning that the model execution rate is 0.244 Hz.a
0.125
0
-0.125
b
target
GRU
0.125
sGRU
LIF
0
-0.125
c
0.125
TableIII:Resultsofthetrainednetworksandtheirrespectivebaselines.NetworksprefixedwithaBrefertothebaselinesgivenby[13].SectionIII-B
describes thecorresponding network architectures. Theexact definitions of eachmetric are defined in[13]. The values for Dense,MACs and ACs are
computedbyaveragingthetotaloverthelengthoftheinput(1024),asimplementedbytheNeurobenchbenchmarkingtool[13].
Connection Activation
Track Model Footprint Dense MACs ACs R2
Sparsity Sparsity
B-ANN3 137752 0 0.681 33888 11507 0 0.615
B-SNN3 33996 0 0.788 43680 32256 5831 0.633
Track1 GRU-t1 352904±0 0±0 0±0 22342±0 8518±0 793±0 0.707±0.012
sGRU-t1 425924±0 0±0 0.651±0.017 22318±0 7238±24 797.7±0.8 0.656±0.013
LIF-t1 302492±0 0±0 0.939±0.008 20766±0 6414±0 825±4 0.648±0.022
B-ANN2 27160 0 0.676 6237 4970 0 0.576
B-SNN2 29248 0 0.998 7300 0 414 0.581
Track2 GRU-t2 174104±0 0±0 0±0 4947±0 627±0 248±0 0.621±0.014
sGRU-t2 180716±0 0±0 0.69±0.07 4932±0 379±23 250.2±0.8 0.577±0.013
LIF-t2 168596±0 0±0 0.946±0.009 4631±0 201±0 254±0.8 0.566±0.016
Our current approach comes with a high flexibility in the a
possible latency and execution rate that it can achieve, as 0.125
both the convolutional and the recurrent layers allow for
iterative data processing. Models GRU-t2, sGRU-t2, and 0
LIF-t2 use a kernel size of 3 applied in two convolutional
blocks. The receptive field determined by this structure can -0.125
be visualized in Fig. 4. With the sizes mentioned above, the b
target
0 computationofonekeypointrequiresaneffectivebufferwin- 0.125 GRU
sGRU
dowof10steps,whichoffersalatencyof40ms.Thiswould LIF
also reduce the input buffer size from 1024 to 10, reducing 0
the model footprints by a sizable amount. The stride of the
receptive field is 4 steps, or 16 ms, which translates to an -0.125
execution rate of 62.5 Hz. The theoretical upper limit of the c
latency of our models (40 ms) is well under the time delay 0.125
between stimulus and voluntary muscle movement reported
bytheneuroscienceliterature[18],whichistypicallygreater 0
than100ms.Assumingnofurtherlatenciesarisefromsignal
transmission and ignoring computation time, our approach -0.125
would be suitable for deployment in the real world, given -0.375 -0.25 -0.125 0 0.125 0.25 0.375
an appropriate real-time implementation of the networks. vx/ms−1
Fig. 5: Visualization of the velocity outputs of all three model types for
VII. CONCLUSIONANDOUTLOOK three exemplary samples each. a displays a sample learned well by all
threenetworks(R2≈0.9).bshowstheoutputforasampleforwhichthe
This work targets both tracks of the Grand Challenge on networks display average accuracy (R2 ≈ 0.7). For the sample shown by
NeuralDecodingforMotorControlofnon-HumanPrimates c,thenetworkscouldnotaccuratelyreconstructthetarget(R2≪0).
ofIEEEBioCAS2024.Thisincludestrack1,whichfocuses
onmaximizingtaskaccuracy,andtrack2,whichaimsatco-
optimizing accuracy and resource demand, which is critical based models consistently achieve a higher R2 than solely
for wireless iBMIs. The networks presented in this work using LIF-neurons, suggesting that such spiking neuron
surpassthebaselinesin[13]bygoodmarginsforbothtracks. models could benefit from improved memory management.
For track 1, GRU- and sGRU-based networks beat the Our work does not yet leverage some SNN-centered
baselines by up to 7.4% in terms of R2 while the LIF-based methods to improve their resourcefulness. This includes
networksperformequal.Fortrack2,consideringthemargin spike regularization, pruning, and event-triggered updating
of error, all networks are at least equal in R2 but show of the neural units, which will be included in future work.
an improvement in the double-digit percentages in terms Finally, three of the six recordings in the dataset consist of
of compute operations. Only the footprint is increased by motor cortex and somatosensory cortex recordings. We do
a rough factor of 6. We explain that this difference is due to not yet distinguish between the two different data types and
largedatabuffersinourcurrentmodelimplementation.This expect an improved regression if done so.
gap could be eliminated in real-world deployment by taking Our work enhances the baseline for the primate reaching
advantage of the iterative nature of convolutional filters and datasetanddemonstratesthepotentialofusinghybridneural
recurrent units. Generally, the GRU-based networks score networks for efficient neural decoders. This advances the
thehighestinbothtracks.However,thetotalamountofoper- field of wireless iBMIs to eventually improve the lives of
ations is the fewest for the LIF-based networks. Our sGRU- millions of humans suffering from paralysis.
-0.125
-0.375 -0.25 -0.125 0 0.125 0.25 0.375
−1
v m s
/
x
1−
s
m
v
/
y
1−sm/
yvREFERENCES embeddedneuromorphicai,”in2024IEEE37thInternationalSystem-
on-ChipConference(SOCC),IEEE,2024.
[1] B. S. Armour, E. A. Courtney-Long, M. H. Fox, H. Fredine, and
[11] W. Maass, “Networks of spiking neurons: the third generation of
A.Cahill,“Prevalenceandcausesofparalysis—unitedstates,2013,”
neuralnetworkmodels,”NeuralNetworks,vol.10,no.9,pp.1659–
Americanjournalofpublichealth,vol.106,no.10,pp.1855–1857,
1671,1997.
2016.
[12] J.E.O’Doherty,M.M.Cardoso,J.G.Makin,andP.N.Sabes,“Non-
[2] World Health Organization (WHO), “Spinal cord injury.,” 2024.
humanprimatereachingwithmultichannelsensorimotorcortexelec-
[Accessed:Sep.2,2024].
trophysiology,” Zenodo http://doi. org/10.5281/zenodo, vol. 583331,
[3] M. A. Lebedev and M. A. Nicolelis, “Brain–machine interfaces:
2017.
past, present and future,” TRENDS in Neurosciences, vol. 29, no. 9,
[13] J. Yik, S. H. Ahmed, Z. Ahmed, B. Anderson, A. G. Andreou,
pp.536–546,2006.
C. Bartolozzi, A. Basu, D. den Blanken, P. Bogdan, S. Buckley,
[4] M. M. Shanechi, “Brain–machine interfaces from motor to mood,”
et al., “Neurobench: Advancing neuromorphic computing through
Natureneuroscience,vol.22,no.10,pp.1554–1564,2019.
collaborative,fairandrepresentativebenchmarking,”2023.
[5] M. Lebedev, “Brain-machine interfaces: an overview,” Translational
Neuroscience,vol.5,pp.99–110,2014. [14] J.Chung,C.Gulcehre,K.Cho,andY.Bengio,“Empiricalevaluation
[6] C. Libedinsky, R. So, Z. Xu, T. K. Kyar, D. Ho, C. Lim, L. Chan, of gated recurrent neural networks on sequence modeling,” arXiv
Y.Chua,L.Yao,J.H.Cheong,etal.,“Independentmobilityachieved preprintarXiv:1412.3555,2014.
through a wireless brain-machine interface,” PLoS One, vol. 11, [15] G. Bellec, F. Scherr, A. Subramoney, E. Hajek, D. Salaj, R. Legen-
no.11,p.e0165773,2016. stein,andW.Maass,“Asolutiontothelearningdilemmaforrecurrent
[7] C. Pandarinath, P. Nuyujukian, C. H. Blabe, B. L. Sorice, J. Saab, networksofspikingneurons,”Naturecommunications,vol.11,no.1,
F. R. Willett, L. R. Hochberg, K. V. Shenoy, and J. M. Henderson, p.3625,2020.
“Highperformancecommunicationbypeoplewithparalysisusingan [16] J.Dethier,P.Nuyujukian,C.Eliasmith,T.Stewart,S.Elasaad,K.V.
intracorticalbrain-computerinterface,”elife,vol.6,p.e18554,2017. Shenoy,andK.A.Boahen,“Abrain-machineinterfaceoperatingwith
[8] J.D.Simeral,T.Hosman,J.Saab,S.N.Flesher,M.Vilela,B.Franco, a real-time spiking neural network control algorithm,” Advances in
J. N. Kelemen, D. M. Brandman, J. G. Ciancibello, P. G. Rezaii, neuralinformationprocessingsystems,vol.24,2011.
et al., “Home use of a percutaneous wireless intracortical brain- [17] J.Liao,L.Widmer,X.Wang,A.DiMauro,S.R.Nason-Tomaszewski,
computerinterfacebyindividualswithtetraplegia,”IEEETransactions C. A. Chestek, L. Benini, and T. Jang, “An energy-efficient spiking
onBiomedicalEngineering,vol.68,no.7,pp.2313–2325,2021. neural network for finger velocity decoding for implantable brain-
[9] X.Zhang,Z.Ma,H.Zheng,T.Li,K.Chen,X.Wang,C.Liu,L.Xu, machine interface,” in 2022 IEEE 4th International Conference on
X.Wu,D.Lin,etal.,“Thecombinationofbrain-computerinterfaces Artificial Intelligence Circuits and Systems (AICAS), pp. 134–137,
and artificial intelligence: applications and challenges,” Annals of IEEE,2022.
translationalmedicine,vol.8,no.11,2020. [18] I. L. Kurtzer, “Long-latency reflexes account for limb biomechanics
[10] J. Krausse, M. Neher, I. Fuerst-Walter, C. Weigelt, T. Harbaum, through several supraspinal pathways,” Frontiers in Integrative Neu-
K. Knobloch, and J. Becker, “On metric-driven development of roscience,vol.8,Jan.2015. Publisher:Frontiers.