1
MixNet: Joining Force of Classical and Modern
Approaches toward The Comprehensive Pipeline in
Motor Imagery EEG Classification
Phairot Autthasan , Rattanaphon Chaisaen , Huy Phan , Maarten De Vos , and Theerawit Wilaiprasitporn
Abstract—Recent advances in deep learning (DL) have sig- resolution [2], [3]. Motor imagery (MI)-based BCI systems are
nificantly impacted motor imagery (MI)-based brain-computer increasingly popular in BCI paradigms due to their advantage
interface (BCI) systems, enhancing the decoding of electroen-
of not requiring external stimuli [4], [5]. Within such an
cephalography (EEG) signals. However, most studies struggle
MI-based BCI system, participants are instructed to mentally
to identify discriminative patterns across subjects during MI
tasks, limiting MI classification performance. In this paper, we imagine the movement of different body parts, evoking neural
propose MixNet, a novel classification framework designed to activity in specific cerebral regions associated with those
overcome this limitation by utilizing spectral-spatial signals from movements. Throughout the MI process, the co-occurrence
MI data, along with a multi-task learning architecture named
of event-related desynchronization (ERD) and event-related
MIN2Net, for classification. Here, the spectral-spatial signals
synchronization (ERS) patterns in EEG signals reflects event-
are generated using the filter-bank common spatial patterns
(FBCSP) method on MI data. Since the multi-task learning relatedmodulationsinbrainactivitywithinparticularfrequency
architecture is used for the classification task, the learning in bands, such as mu (9–13 Hz) and beta (22–29 Hz), over the
each task may exhibit different generalization rates and potential motor cortex region [6], [7]. The ERD/ERS patterns generated
overfitting across tasks. To address this issue, we implement
by imagined movements are matched with actual movements
adaptive gradient blending, simultaneously regulating multiple
and then decoded to determine the user’s intent. Furthermore,
loss weights and adjusting the learning pace for each task based
on its generalization/overfitting tendencies. Experimental results the output signal can be used to control external devices [8].
onsixbenchmarkdatasetsofdifferentdatasizesdemonstratethat Nevertheless, MI-based BCI systems encounter notable
MixNet consistently outperforms all state-of-the-art algorithms challenges. Firstly, the EEG signals have a low signal-to-noise
in subject-dependent and -independent settings. Finally, the low-
ratio(SNR),makingthemhighlysensitivetonoiseinterference
density EEG-MI classification results show MixNet’s superiority
[9], [10]. Another limitation is the considerable variability
over state-of-the-art algorithms, offering promising implications
for Internet of Thing (IoT) applications such as lightweight and observedinEEGsignalsacrossdifferentsubjects,posingasub-
portable EEG wearable devices based on low-density montages. stantialchallengetoaccuratelydecodingtheseoscillatoryneural
activities. Recently, researchers have developed calibration-
Index Terms—Deep learning (DL), brain-computer interface
(BCI),motor-imagery(MI),adaptivegradientblending,multi-task free or subject-independent methods to tackle the issue of
learning EEG data variability across subjects [11], [12]. These methods
are trained and tested using data from different groups of
subjects, enabling new users to utilize the BCI system without
I. INTRODUCTION
needing a calibration phase. For the MI-based BCI system to
BRAIN-computer interfaces (BCIs) are a transformative help capture and learn generalized MI-EEG features across
technology that establishes a direct communication chan- subjects, it is essential to develop powerful feature extraction
nel between the human brain and external devices [1]. Among and classification algorithms. Deep learning (DL) has recently
the numerous neural acquisition techniques used in BCIs, elec- emerged as an up-and-coming technique and demonstrated
troencephalography (EEG) has attracted considerable interest remarkable success in numerous fields, including computer
due to its non-invasiveness, affordability, and high temporal vision, bio-signals, speech recognition, and natural language
processing [13]. In contrast to conventional methods, which
This work was supported by PTT Public Company Limited, The SCB
rely on manually hand-crafted features, DL models can learn
PublicCompanyLimited,ThailandandNationalResearchCouncilofThailand
(N41A640131) (Phairot Autthasan and Rattanaphon Chaisaen contributed complex patterns directly from multi-dimensional data. This
equallytothiswork)(∗Correspondingauthor:TheerawitWilaiprasitporn). capability has attracted numerous researchers’ interest in BCIs,
P. Autthasan, R. Chaisaen, and T. Wilaiprasitporn are with Bio-
resulting in the development of advanced DL architectures
inspired Robotics and Neural Engineering (BRAIN) Lab, School
of Information Science and Technology (IST), Vidyasirimedhi thathavedemonstratedsignificantimprovementsinEEG-based
Institute of Science & Technology (VISTEC), Rayong, 21210, Thailand motor imagery (MI) classification [14].
(e-mail:theerawit.w@vistec.ac.th).
Convolutionalneuralnetworks(CNNs)havebeenextensively
H.PhaniswithAmazonAGI,Cambridge,MA02142USA.Theworkdoes
notrelatetoH.P.’spositionatAmazon. applied in EEG-MI classification due to their ability to learn
M.DeVosiswiththeDepartmentofEngineeringandwiththeDepartment temporal and spatial features from EEG signals effectively
ofDevelopmentandRegeneration,KULeuven,Leuven,Belgium,3001.
[15]–[17]. By leveraging the power of 2D-CNNs, these models
Code examples, and other supporting materials are available on
https://github.com/Max-Phairot-A/MixNet can capture local and global connectivity patterns within
4202
peS
6
]GL.sc[
1v40140.9042:viXra2
EEG data. The combination of CNNs and long short-term and robot control [22].
memory units (LSTMs) has been extensively employed to The remainder of this paper is organized as follows. Section
capture long-term dependencies in temporal features [18], II presents an overview of related work on MI classification
[19]. This fusion permits extracting relevant temporal and topics. Section III details the pre-processing steps applied to
spatial features, enhancing the overall performance of EEG-MI EEGdataandoutlinesthearchitectureoftheproposedmethod.
classification. Despite the notable successes of existing deep Section IV presents the experimental results, indicating the
learning approaches in decoding EEG signals in various MI performance of the proposed method. Section V provides a
datasets, their performance is limited to the subject-dependent comprehensive discussion of the experimental results and their
task, where the methods are trained and tested using data from implications. Finally, the main findings and potential avenues
the same subject. In other words, these models have difficulty for future research are summarized in Section VI.
adapting and performing well when evaluated using data from
unseen users. This limitation hinders their applicability and
II. RELATEDWORK
potential real-world deployment in BCI systems.
In recent years, our previous work [12] introduced a deep This section thoroughly reviews the evolution and progress
learning architecture known as MIN2Net, explicitly developed in EEG-based MI classification. Subsequently, we address the
to capture generalized MI-EEG features across subjects in existing limitations in the current research on MI-based BCI.
the subject-independent task. This architecture demonstrated Furthermore, we provide a detailed exposition of the concepts
acceptable performance in subject-independent learning over a of autoencoders (AE) and deep metric learning (DML), both
large-scaledataset.However,itseffectivenesswascompromised of which bear relevance to our study. Lastly, we outline the
when attempting to recognize generalized EEG-MI features rationale behind undertaking this research endeavor.
with limited training data, such as using a small data size and
training data from only a single subject in subject-dependent
A. The Progressive Development of MI based on BCI
learning.Anotherdrawbackisincorporatingmulti-tasklearning
into MIN2Net (e.g., unsupervised, supervised, and deep metric The progressive evolution of machine learning has inspired
learning) for EEG-MI classification proved challenging, given BCI researchers to propose more advanced intelligent algo-
the differing generalization rates and potential overfitting rithms employing subject-dependent paradigms to improve
across tasks. MIN2Net was the reliance on an exhaustive motor imagery (MI) decoding performance based on EEG
parameter search to identify the optimal set of loss weights. signals. Common spatial pattern (CSP) is one of the most
This time-consuming process yielded varying optimal weight prominent approacheswidely usedfor feature extraction in MI-
configurations when applied to different datasets. basedBCI[23].CSPefficientlyextractsdiscriminativefeatures
This paper proposes MixNet, a novel framework that builds by maximizing the variance differences between the different
upon the foundational principles of MIN2Net, addressing classes of EEG signals. Moreover, several methods are built
limitations in network learning with small data sizes and time- based on extensions of CSP, such as common spatiospectral
consuming loss weights optimization. MixNet utilizes filter- pattern (CSSP) [24], filter bank common spatial pattern
bankcommonspatialpatterns(FBCSP)fordatapre-processing, (FBCSP) [25], and bayesian spatio-spectral filter optimization
extracting discriminative patterns from EEG-MI data. This (BSSFO) [26].
resultsinspatiallyfilteredorspectral-spatialsignals,preserving Among advanced CSP algorithms, FBCSP is the most
meaningful information across EEG classes. We adjusted and widely used, which extends the approach by incorporating
adopted the adaptive gradient blending concept, previously multiple frequency bands instead of being limited to a specific
applied in sleep staging tasks by Huy et al. [20], which is band. It has demonstrated state-of-the-art performance in EEG-
incorporated to regulate multiple loss weights concurrently. based MI classification as seen in [27]. Upon processing
This approach alleviates the need for manual parameter tuning, EEG signals through FBCSP, meaningful brain features are
leading to a more efficient and effective optimization process. extracted. Subsequently, a feature selection method such as
The proposed method achieves remarkable performance in mutual information-based best individual feature (MIBIF) [25]
subject-dependent and subject-independent MI classification is employed to identify the most discriminative features. In
tasks by leveraging the spectral-spatial signals and employ- the realm of classification, numerous conventional algorithms,
ing an adaptive gradient blending approach for loss weight such as support vector machine (SVM) and linear discriminant
optimization. analysis (LDA), have been extensively utilized to decode
Additionally, MixNet outperforms state-of-the-art algorithms the extracted features, as seen in [11], [27]. Furthermore,
in handling MI classification on low-density EEG-MI signals. severalalgorithmshaveextendedtheCSPparadigm,promising
As the MixNet developed for improving low- and high- performance improvements [28]–[30]. Despite the significant
density EEG classification, this advancement has promising achievements of existing CSP and its modified algorithms
implications for Internet of Thing (IoT) applications such as in EEG decoding for various Motor Imagery (MI) datasets,
the EEG classification on EEG signals that are from light- it is evident that their efficacy is primarily restricted to the
weight and portable EEG wearable devices based on low- subject-specific task. Regrettably, their effectiveness in the
density montages; most of these devices are more comfortable subject-independent task is a topic of ongoing research and
for users, reducing the effort to set up and also expanding their requires additional refinement to improve their performance
usage in the real world, including stroke identification [21] and generalizability.3
4–8 Hz CSP
Nc Nb 8-12 Hz CSP U U×Nb
t 36-40 Hz CSP t
EEG recordings Raw EEG FBCSP projection Spectral-spatial signals
(a)
Encoder Decoder
p
n
a
Deep Metric Learning Classifier
(b)
Fig. 1: Overall visualization of MixNet framework. (a) exhibits an overview of the preparation process for spectral-spatial signals. (b)
demonstrates the multi-task autoencoder (AE) of MixNet, designed to simultaneously address three tasks: autoencoder, deep metric learning,
and supervised learning. This framework is employed for EEG-MI classification. More in-depth insights into the network architecture can be
found in Table II.
Recently,advanceddeeplearningtechniques[11],[12],[31], metric learning network. Classical similarity metric functions,
[32] have demonstrated the ability to effectively utilize large- such as Euclidean distance, Mahalanobis distance, and Cosine
scale EEG data and knowledge from analogous or related distance, are explicitly engaged as the distance metric between
subjects/sessions, aiming to reduce the need for calibration. In pairs of data points within the DML framework.
a more recent paper, Kwon et al. [11] proposed a subject- In recent years, there has been significant advancement in
independent framework based on CNN architectures that developing several loss functions designed explicitly for DML.
incorporate spectral-spatial feature representation to enhance These loss functions are created with the primary objective of
subject-independent MI classification, achieving state-of-the- improvingthediscriminationoffeatures.Severallossfunctions
art performance on the OpenBMI dataset. Subsequently, in have been identified as significant in this context, including
our previous work [12], we introduced MIN2Net, a novel contrastive loss [34], triplet loss [35], quadruplet loss [36],
end-to-end neural network architecture based on a multi-task and multi-similarity loss [37]. By adeptly utilizing these loss
autoencoder,whichachievedstate-of-the-artperformanceonthe functions, similarity measures are computed on correlated
SMR-BCI and OpenBMI datasets in the subject-independent samples, compelling samples belonging to the same class to
setting. However, MIN2Net’s performance was limited in converge while concurrently inducing samples from distinct
identifying generalized EEG-MI features under conditions classes to diverge. One notable differentiation between DML
of limited training data, such as with small dataset sizes or losses and traditional loss functions, such as cross-entropy loss,
when relying on training data from a single subject in subject- istheirdependencyoncontrastivepairs,triplets,orquadruplets
dependent learning. of data for gradient determination.
More recently, DML has been extensively utilized in EEG-
B. Deep Metric Learning in MI based on BCI
BCI research, providing novel opportunities for decoding
Deep metric learning (DML) is an advanced and effective human brain activity more precisely and efficiently [12],
techniquebasedonthefundamentalconceptofdistancemetrics [38]–[40]. Specifically, integrating the DML technique into
[33]. Its primary objective is to acquire highly informative data advanceddeeplearningarchitectures,suchasautoencodersand
representationsthatenableprecisedatasimilaritymeasurement. convolutional neural networks (CNNs), enables the model to
This is accomplished using embedded features obtained from a automatically learn and extract meaningful features from high-
Spectral-spatial
signals
FC
Softmax
Reconstructed
signals4
dimensional embeddings and optimize the entire system for the model cannot extract discriminative patterns from raw
the task of capturing similarities and differences between EEG EEG signals when the number of training samples is relatively
classes. The combination of deep metric learning in multi-task scarce. Therefore, from all angles, this study aims to develop
autoencoder-based CNN architectures indicates great promise an enhanced version of MIN2Net capable of achieving high
for advancing EEG-BCI technology and has demonstrated performance regardless of the size of the training dataset.
promising results in addressing the long-standing problem of
inter-subject variability, as seen in our previous work [12].
III. METHODS
C. Autoencoders in MI based on BCI
TABLE I: Description of all benchmark datasets
The Autoencoder (AE) concept, a widely recognized compo-
nent in unsupervised learning algorithms, was first introduced
Datasets #Subjects MITask #Channels SelectedChannellocation Frequency(Hz.)
in 1986 [41]. Autoencoders (AEs) have been widely used and FC3,FC1,FCz,FC2,FC4,
have demonstrated their versatility in various domains, such BCICIV2a 9 vs.L Ref it ghh tan hd and 20 C4C ,C5, 6C ,C3, PC 31 ,, CC Pz 1, ,C C2 P, z, 250
CP2,CP4,P1,Pz,P2
as data compression, denoising, dimensionality reduction, and BCICIV2b 9 vs.L Ref it ghh tan hd and 3 C3,Cz,C4 250
feature extraction [42]–[44]. The AE network architecture can
efficiently extract crucial features from labeled and unlabeled BNCI2015-001 12 R vig sh .t Fh ea en td 13 FC C3 1 C, ,F PCC 3z ,z , C, CF P2C z, ,C4 C, 4C , PC5 4, 6C ,3, 512
input data, forming a latent representation. The latent represen- SmallLaplacianelectrodewere
tation plays a fundamental role in the later reconstruction SMR-BCI 14 R vig sh .t Fh ea en td 15 Dip sl ta ac ne cd esat beC tw3, eeC nz n, ea in gd hbC or4 i. ng 512
electrodeswere2.5cm.
of the original input data. The training objective for this
FC5,FC3,FC1,FC2,FC4,
network architecture is centered on reducing the reconstruction High-Gamma 14 vs.L Ref it ghh tan hd and 20 CF 2C ,6 C, 4C ,5 C, 6C ,3 C, PC 51 ,, CC Pz 3, , 500
loss corresponding to the input data. Moreover, significant CP1,CPz,CP2,CP4,CP6
FC5,FC3,FC1,FC2,FC4,
research findings suggest that the efficacy of the obtained OpenBMI 54 vs.L Ref it ghh tan hd and 20 CF 2C ,6 C, 4C ,5 C, 6C ,3 C, PC 51 ,, CC Pz 3, , 1,000
latent representation is much enhanced when the reconstructed CP1,CPz,CP2,CP4,CP6
datacloselymirrorstheprominentfeaturesoftheoriginalinput
data.
One such advanced AE architecture is the denoising sparse
autoencoder (DSAE) [45], which is proposed to improve A. Databases
EEG-based epileptic seizure detection. The main objective of
DSAE is to improve the accuracy of epileptic seizure detection The proposed method, along with all baseline methods,
by efficiently reconstructing the original EEG signals from was assessed using various datasets, including BCIC IV
corrupted inputs. The sparsity constraint integrated into the 2a [49], BCIC IV 2b [50], BNCI2015-001 [51], SMR-BCI
DSAEiscrucialinachievingthisefficiency,offeringsignificant [52], High-Gamma [15], and OpenBMI [53] datasets. The
potential for more reliable and precise results. Another area BCIC IV 2a, BCIC IV 2b, BNCI2015-001, and SMR-BCI
where AE-based methods have shown remarkable potential datasets are widely recognized as benchmark datasets for MI
is in handling biopotentials and telemonitoring systems. A classification, originating from Graz University of Technology.
technique based on compressed sensing (CS) and AE [46] The High-Gamma dataset, which has been made available by
has demonstrated outstanding results in data compression and the University of Freiburg, is recognized as the second largest
accurate classification of electrocardiogram (ECG) and EEG MI dataset. Lastly, the evaluation also included the OpenBMI
signals. dataset,whichisthelargestpublicMIdatasettodate,provided
In more recent papers, the utilization of DL-based AE by Korea University.
architectureshasgrowninpopularity.IntheworksofParashiva In the high-density EEG motor imagery (EEG-MI) classifi-
et al. [47] and Mammone et al. [48], researchers have shown cationtask,weconsideredallEEGchannelsfromtheBCICIV
this trend through integrating AEs with FBCSP technique. 2a, BNCI2015-001, and SMR-BCI datasets, as these datasets
These studies have proven that combining AEs and FBCSP directly provide comprehensive EEG channels associated with
can extract crucial features, resulting in excellent EEG-based the motor cortex area. Following previous studies [11], [12],
MI classification results. One limitation of these approaches we selected 20 EEG channels in the motor cortex region from
is that they do not offer end-to-end learning paradigms. Even the High-Gamma and OpenBMI datasets for our analysis. We
though latent representations were successfully compressed utilized the BCIC IV 2b dataset for the low-density EEG-MI
during the encoding-decoding process, further classification classification task, which provides EEG data from 3 channels
is still required to accurately distinguish and classify various (C 3, Cz, and C 4) placed over the motor cortex region. Further
classes or types of EEG patterns. details about these databases are provided in Table I.
In our previous work [12], we introduced MIN2Net, a novel Additionally, all EEG data in the considered datasets were
end-to-end neural network architecture based on a multi-task downsampled to a sampling frequency of 100 Hz, and the
autoencoder. MIN2Net simultaneously learns deep features MI period for all datasets was selected as the time interval
fromunsupervisedEEG-MIreconstructionandsupervisedEEG- between 0 s and 4 s after stimulus onset. The EEG signals
MI classification. It demonstrated excellent performance in utilized in this study are time-domain signals with a duration
extracting discriminative features for MI decoding. However, of four seconds.5
TABLE II: MixNet’s architecture, where U is the number of spatial filters, t is the number of time points, N is the number of frequency
b
bands, z is the size of latent vector and N is the number of classes. Noted that the data format of Conv2D is “channels last”
class
Blocks Layer Filter Size Stride Activation Options Output
Input (1,t,U×Nb) (1,t,U×Nb)
Conv2D U×Nb (1,64) 1 ELU padding=same (1,t,U×Nb)
BatchNormalization (1,t,U×Nb)
Encoder A Cv oe nr va 2g DePooling2D (U×Nb)/2 ( (1 1, ,t 3/ 21 )00) 1 ELU padding=same ( (1 1, ,1 10 00 0, ,U U× ×N Nb b) /2)
BatchNormalization (1,100,U×Nb/2)
Autoencoder AveragePooling2D (1,4) (1,25,U×Nb/2)
Flatten (25×U×Nb/2)
Latent FC (z) (z)
FC (25×U×Nb/2) (25×U×Nb/2)
Decoder
R Ce os nh va 2p De
Transpose U×Nb/2
( (1 1, ,2 65 4, )U×Nb/2)
4 ELU padding=same
(( 11 ,, 12 05 0, ,U U× ×N Nb/ b/2 2)
)
Conv2DTranspose U×Nb (1,32) t/100 ELU padding=same (1,t,U×Nb)
DeepMetriclearning Latent (z)
Latent (z)
SupervisedLearning FC Nclass softmax (Nclass)
B. Generation of Spectral-spatial Signals EEG trials N , the covariance matrix for each class of the b
i k
filtered EEGs is then given by:
In this study, the raw EEG signals are time-domain signals
thatchangeovertimewith4secondslong.Formally,wedenote
X
i
∈RNc×t as a single-trial EEG data from the ith trial and
Σ =
1 (cid:88)Ni E b( ky ,) iE b( ky ,) iT
(3)
l Nety ii s∈ the{1 n, u2 m,. b. e., rN ofcl ca hss a} nnd ee ln s,ot te ii sts thc eor nr ue msp bo en rd oin fg sala mb pe ll e, dw th imer ee bk,y N i i=1 tr(E b( ky ,) iE b( ky ,) iT)
c
The spatially filtered EEG signal or spectral-spatial EEG
points, and N is the number of classes.
class
signal Z in Equation 1 using first and last U columns of
Using a fifth-order Butterworth bandpass filter, a filter bank bk,i 2
spatialfilterW fromEquation2canmaximizethedifferences
was used to decompose the single-trial EEG data X into bk
in the variance of the two classes of filtered EEG signals,
multiple frequency bands. Here, the filter bank is defined
as B = {b k} kN =b 1 ∈ {[4,8],[8,12],...,[36,40]}, where N b is t fih lu tes rsd Ueno =te 4d mas eaW n¯ sb tk h. atFo thr ee tx wa om lp al re g, est the ann du tm hebe tr woof smsp aa llt eia sl
t
the number of frequency bands, is predefined from 4 to 40
eigenvalues of the spatial filter W are selected. The spectral-
Hz. Each bandpass filter has a range of 4 Hz. While some bk
spatial signals are finally computed as follows:
variations in the filter bank configurations are effective, these
specific bandpass frequencyranges are used due to their ability Z¯ =W¯TE , (4)
to provide stable frequency response, spanning the range of 4 –
bk,i bk bk,i
40 Hz, as observed in the work by [27]. Filtered EEG signals Where Z¯
bk,i
∈RU×t and W¯
bk
∈RNc×U.
will be denoted as E
bk,i
∈RNc×t.
After filtering the EEG signals by the filter bank, the CSP
C. Classification With MixNet
algorithm was used for spatial filtering [25], [27]. The CSP
algorithm is well-known for its efficacy in estimating and In this study, the proposed method builds upon the the
applying a projection matrix to transform EEG signals linearly, idea of our previous paper presented MIN2Net and still keeps
maximizing the variance of signals belonging to one class and the network architecture the same as the original MIN2Net.
minimizing the variance of signals belonging to the other. The MixNet consists of three main modules: autoencoder, deep
spatial filtering procedure involves linearly transforming EEG metric learning, and supervised learning, and its architecture
data using the CSP algorithm as follows: is optimized by minimizing three different loss functions
simultaneously: reconstruction, cross-entropy, and triplet loss
Z bk,i =W bT kE bk,i, (1) functions.
1) Autoencoder: The autoencoder module, which is a part
Where Z
bk,i
∈ RNc×t denotes E
bk,i
after spatial filtering,
of the MixNet framework, has two main components: the
W
bk
∈RNc×Nc denotes the CSP projection matrix; T denotes
encoder, expressed as z =q(x), and the decoder, represented
the transpose operator.
as xˆ = p(z). The input signal x is subjected to encoding
TheCSPalgorithmcomputesthetransformationmatrixW
bk within the encoder component, transforming x into a latent
through the solution of the eigenvalue decomposition problem
vector z via dimensionality reduction. In contrast, the decoder
as shown in Equation 2, thereby producing features with
module receives a given latent vector z and continues to
optimized variances to discriminate between the two classes
decode it, resulting in the reconstruction of the input signal xˆ.
of EEG signals effectively:
The primary objective of the AE module is to learn how to
map data into a latent space that retains pertinent information
Σ W =(Σ +Σ )W D , (2)
bk,1 bk bk,1 bk,2 bk bk
for reconstructing the data. By utilizing spectral-spatial EEG
Where Σ and Σ are the estimation of the covariance signals from FBCSP as inputs for the AE module in this
bk,1 bk,2
matrices of the filtered EEG signals for motor imagery class study, the AE can effectively recognize instances and preserve
1 and class 2, respectively. D is the diagonal matrix that instances’ discriminative patterns corresponding to different
bk
containstheeigenvaluesofΣ .Byconsideringallthefiltered classes.
bk,16
True Loss
Training Loss L (za,zp,zn)= 1(cid:2) ∥za−zp∥2−∥za−zn∥2+α(cid:3) .
triplet 2 +
(6)
tanθ(n) where [z] + = max(z,0). The parameter α regulates as the ⋄ 0
margin threshold that imposes a constraint on the Euclidean
distance ∥za−zp∥2 between positive pairings, ensuring that it
tanθ train(n 0) tanθ ⋄(n) islessthantheEuclideandistance∥za−zn∥2 betweennegative
pairs. Determining the optimal value for the triplet loss margin
tanθ train(n)
is crucial to training the DML module.
3) Supervised Learning: The supervised learning module
Epoch
employsaconventionalsoft-maxclassifiertoclassifythelatent
Fig. 2: The tangent slope of training and true losses at epoch n and
representations of the input spectral-spatial EEG signals. The
n .
0 latent representation z is inputted into the fully connected (FC)
layer using the softmax activation function to calculate the
importance weight for each class. This can be represented as
The encoder module consists of two separate CNN blocks.
follows:
Each block is built of a Conv2D layer, followed by a batch
yˆ(z)=softmax(Wz+b) (7)
normalization (BN) layer, an exponential linear unit (ELU),
and an average pooling layer (AveragePooling2D). The final
Where W and b are the weight matrix and the bias vector,
output of the last CNN block serves as the input to a fully
respectively. Then, the cross-entropy (CE) loss is used to
connected (FC) layer. This FC layer aids in the process of
measure the difference between the predicted value and true
mapping the latent representation. The decoder module has a
label, given by:
symmetrical construction to that of the encoder module. The
latent vector is passed via an FC layer to preserve consistency 1
(cid:88)Ni
L (y,yˆ)=− y logyˆ. (8)
in the input dimensions of the CNN blocks. It then proceeds to CE N i i
i
a reshape layer that arranges the data into a suitable dimension. i=1
The decoder module consists of two CNN blocks, each using a wherey representsthetruelabel,yˆrepresentstheclassification
Conv2DTranspose layer with a stride value of 4. Subsequently, probabilities, and N i represents the total number of input
an ELU layer is used to achieve its intended purpose. More signals. The class with the highest classification probability is
details on the dimensions of the input size at each layer in the the predicted class of the single-trial EEG signal.
architecture of MixNet can be found in Table II.
The primary goal of the AE module’s training objective is D. Training Procedure for MixNet Using Adaptive Gradient
to minimize the difference in reconstruction between the input Blending
and the reconstructed output. This study uses the mean square
The training objective of the proposed method is optimized
error (MSE) as the objective function for this module, which
through the combination of the three loss functions: L in
is given by: MSE
Equation 5, L in Equation 6, and L in Equation 8. The
triplet CE
1
(cid:88)Nc final loss function of MixNet L
MixNet
is represented as:
L (x,xˆ)= ∥x −xˆ∥2. (5)
MSE N
c
j=1
j j
L (x,xˆ,za,zp,zn,y,yˆ)=
1
(cid:88)Ni
{w(1)L (x ,xˆ)
MixNet N MSE i i
Where x j denotes the input signals and xˆ j is the reconstructed i i=1
signals of the channel j. +w(2)L (za,zp,zn)+w(3)L (y ,yˆ)}.
triplet i i i CE i i
2) Deep Metric Leaning: Deep metric learning (DML) has (9)
proven its efficacy in retaining discriminative patterns for the Where N denotes the total number of input signals, w(1),
i
latent representation of the autoencoder (AE), as indicated by w(2), and w(3) represent the hyperparameters to weigh the
our prior research [12]. Hence, this research article employs contribution of each loss function. As a result of integrating
a triplet loss approach, specifically using a semi-hard triplet the three loss functions, both the unsupervised and the DML
constraintasproposedintheworkofSchroffetal.[35],inside can influence the learning process when supervised learning
theDMLmodule.Theaimistomaximizetherelativedistances occurs.
between distinct classes of latent vectors. During the training Manually tuning these weights {w(1),w(2),w(3)} is a
procedure, a triplet set comprising three samples, represented problematic and expensive operation, making multi-tasking
as {xa,xp,xn}, is randomly selected from the training dataset. learningimpractical.Inspiredbytheadaptivegradientblending
Therelationshipbetweentheanchorsamplexa andthepositive concept, previously demonstrated in work by Huy et al.
samplexp isensuredtobelargerthanthedistancebetweenthe [20], it has shown great success in regulating multiple loss
anchor sample xa and the negative sample xn. Subsequently, weightssimultaneouslyforjointmulti-viewlearning.Thisstudy
the encoder component processes the set of three input signals presents a systematic solution for multi-task learning, where
concurrently, generating their corresponding latent vectors za, the weighting of multiple loss functions is determined based
zp, and zn. The loss function can, therefore, be expressed as: on the adaptive gradient blending of each task. The concept
ssoL7
of multi-task learning is designed to tackle the challenge of Algorithm 1 Process of Calculating MixNet’s Loss Weights
optimizingamodelwhenfacedwithmultipleobjectives,which Using Adaptive Gradient Blending Approximation
is a significant problem in many deep learning architectures. 1: procedure MIXNET WEIGHT(L train,L ⋄,W,l)
Acquiring access to the gradient flows of network streams 2: input: L train[1...n]: array of training loss values
becomes crucial for effectively regulating the learning pace. 3: L ⋄[1...n]: array of true loss values
4: W: warm-up period
MixNethasthreemainlearningtasks:autoencoder,deepmetric
5: l: window size for line fitting
learning, and supervised learning, optimizing three objective
6: output: w(n): weight at the training epoch n
functions simultaneously. Conducting a thorough investigation 7: if n<W then n 0 =n; return
of the tendencies towards generalization and overfitting in the 8: tanθ train(n)←LineFit(L train[(n−l)...n])
learning tasks allows us to provide suitable weights to the 9: tanθ ⋄(n)←LineFit(L ⋄[(n−l)...n])
gradient flows of these tasks. This means providing a higher
10: G(n)=tanθ⋄(n)−tanθ⋄(n0)
11: O(n)=[tanθ⋄(n)−tanθtrain(n)]−[tanθ⋄(n0)−tanθtrain(n0)]
weight to the task that exhibits excellent generalization while 12: w(n)= 1 Gn
ZOn
a lower weight to the task with tendencies towards overfitting. 13: iftanθ⋄(n0)>tanθ⋄(n)then
14: n0=n ▷Trainingepochw.r.t.thetrueloss’ssmallesttangent
In this approach, the gradients are blended in a way that
considers the overall behavior of multi-task learning in terms
of generalization and overfitting. Additionally, the learning L(m)(0) may vary considerably with various random initializa-
⋄
progress of the network tasks is adjusted for each individual
tions. Alleviate these drawbacks; the powerful approximation
task.
approach is proposed to alleviate these drawbacks based on
This paper applies adaptive loss weighting to regulate the
the tangents of losses over the line fitting of loss curves
learning process across the three learning tasks to reach a
(training and validation) [20]. This approximation method
balance in the generalization and overfitting rates inside the
utilizes two fundamental observations: the smooth yet noisy
multi-task learning of MixNet. We determine the loss weights
patternoflosscurvesandtheinformativecharacteristicsofloss
using a mechanism similar to that described in reference [20]
tangent directions at particular training iterations to determine
based on the ratio of generalization and overfitting metrics
the generalization or overfitting tendencies of the network.
(a theoretical justification is provided in the supplementary
The approximation approach based on the tangents of losses
material), as can be seen below:
throughout our experiments where G and O at the training
m m
1 G process at epoch n can be given by
w(m) = m (10)
Z O2
m G (n)≈tanθ(m)(n)−tanθ(m)(n ), (14)
m ⋄ ⋄ 0
where m ∈ {1,2,3} and Z is a normalization factor. The
generalizationmetricisthegapbetweenthegainininformation (cid:16) (cid:17)
learned about the target distribution. We define the overfitting O m(n)≈ tanθ ⋄(m)(n)−tanθ t( rm a) in(n)
(15)
metric as the gap between the gain on the training set and the (cid:16) (cid:17)
− tanθ(m)(n )−tanθ(m) (n )
target distribution. The weighted loss used through the training ⋄ 0 train 0
process at epoch n is then expressed as follows:
Where tanθ(m) (n) and tanθ(m)(n) are the training and
train ⋄
(cid:88) true loss tangents through the training process at epoch n and
L(n)= w(m)(n)L(m)(n) (11)
tanθ(m) (n ) and tanθ(m)(n ) are their references, respec-
m∈{1,2,3} train 0 ⋄ 0
tively. The relationship between all above variables can be
The learning behavior of MixNet can be influenced by the summarized in Figure 2. Moreover, let θ(m) (n), and θ(m)(n),
train ⋄
weighted loss mentioned above, relying on how G m and O m where −90◦ ≤ θ(m) (n) ≤ 0◦ and −90◦ ≤ θ(m)(n) ≤ 90◦,
are approximated. train ⋄
denotes the angles made by the tangent lines of the training
Generally, in the work [54], G and O at the training
m m and true loss curves with the horizontal axis, respectively. The
process at epoch n can be simply approximated as follows: network is generalizing when −90◦ ≤ θ(m)(n) < 0◦ (i.e.,
⋄
negative tangent) and overfitting when 0◦ < θ(m)(n) ≤ 90◦
G (n)≈L(m)(n )−L(m)(n), (12) ⋄
m ⋄ 0 ⋄ (i.e., positive tangent).
Theprocessofdeterminingtheadaptivelossweight,denoted
O m(n)≈∆L(n)−∆L(n 0) as w(m), for a specific network task labeled as m in MixNet,
(cid:16) (cid:17)
≈ L(m)(n)−L(m) (n) is detailed in Algorithm 1. This involves estimating the loss
⋄ train (13) tangent (tanθ(m) (n) and tanθ(m)(n)) at epoch n. This is
(cid:16) (cid:17) train ⋄
− L(m)(n )−L(m) (n ) done by analyzing the slope of lines fitted to a loss curve
⋄ 0 train 0
segmentspanninglengthl epoch,startingfromepochn−l and
Where L( trm a)
in
and L( ⋄m) are defined as the training loss extending to epoch n. This results in initiating the network’s
and true loss, respectively. Epoch n is the training step and training with a warm-up period lasting W training epochs,
n is reference step, where epoch n < n. Nonetheless, the during which the task weights are uniformly set, where l ≤
0 0
approximation above approach has two major drawbacks: the W. Furthermore, for each learning task, the training epoch
loss curves of training and validation are highly frustrated corresponding to the smallest authentic loss tangent (i.e., the
due to minibatch training, and the references L(m) (0) and steepest slope of the fitted line) is identified as the reference
train8
BCIC IV 2a/BCIC IV 2b SMR-BCI High Gamma BCIC IV 2a/BCIC IV 2b SMR-BCI High Gamma
Offline 1 Offline 2 Offline Online Train Session Test Session Offline 1 Offline 2 Offline Online Train Session Test Session
S01 A/B01T A/B01E S01 S01T S01E S01 offline offline S01 A/B01T A/B01E S01 S01T S01E S01 offline offline
S02 A/B02T A/B02E S02 S02T S02E S02 offline offline S02 A/B02T A/B02E S02 S02T S02E S02 offline offline
S09 A/B09T A/B09E S14 S14T S14E S14 offline offline S09 A/B09T A/B09E S14 S14T S14E S14 offline offline
OpenBMI BNCI2015-001 OpenBMI BNCI2015-001
Session 1 Session 2 Session1 Session2 Session 1 Session 2 Session1 Session2
S01 offline online offline online S01 0A 1B S01 offline online offline online S01 0A 1B
S02 offline online offline online S02 0A 1B S02 offline online offline online S02 0A 1B
S54 offline online offline online S09 0A 1B S54 offline online offline online S09 0A 1B
a) Subject-dependent b) Subject-independent
Prediction
Testing set Pre-processing Testing set Pre-processing
5-fold cross-validation Testing 5-fold cross-validation Testing
Validation set Validation set
Learning set
#(1/5)(Ni−1)
Learning set
#(1/5)(Ns−1)
#(Ni−1) Training set Pre-processing #(Ns−1) Training set Pre-processing
#(4/5)(Ni−1) Training #(4/5)(Ns−1) Training
Training set Testing set Ni The number of trials Training set Testing set Ns The number of subjects
Fig. 3: Frameworks of a) subject-dependent and b) subject-independent with stratified k-fold cross-validation for the classification models.
Note that the pre-processing procedure comprises both downsampling and FBCSP methods.
epoch n . In the practical context of MixNet, the true loss Butterworth bandpass filtering. Furthermore, a support vector
0
tanθ(m)(n) remains unknown. Therefore, we approximate it machine (SVM) with a grid search considered kernel options
⋄
withthelossmeasuredonaheld-outvalidationsettanθ(m)(n). (linear, radial bias function (RBF), sigmoid), C values (0.001,
val
0.01, 0.1, 1, 10, 100, 1000), and, particularly for RBF, gamma
values(0.01,0.001).Thegridsearchoptimizedhyperparameters
E. Network Parameters
throughvalidationsetpredictions.TheresultingSVMclassifier,
The proposed method was implemented using TensorFlow
with optimal parameters, was then used for testing.
framework version 2.8.2. An NVIDIA Tesla V100 GPU with
2) DeepConvNet: The DeepConvNet, introduced initially as
32GB of memory was utilized during the training process. We
adeeplearningmodelbasedontwoCNNarchitectures[15],has
optimized the loss function in each training iteration using
demonstrated considerable efficacy in EEG-MI classification.
the Adam optimizer, with a learning rate schedule ranging
In this current study, we faithfully employed the DeepConvNet
from 10−3 to 10−4 throughout our experiments. The learning
with optimal parameters as outlined in [15]. Moreover, to
rate was reduced by a factor of 0.5 when no improvement
prepare the raw EEG data for analysis, we applied a bandpass
in validation loss was observed for five consecutive epochs.
filter spanning the frequency range of 8 to 30 Hz, utilizing a
A batch size of 32 samples was employed for the subject-
5th order non-causal Butterworth filter.
dependent classification setting, while 100 samples were used
3) EEGNet-8,2: Inspired by the FBCSP method, EEGNet-
forthesubject-independentclassificationsetting.Theparameter
8,2 was introduced as a compact CNN architecture to capture
l inAlgorithm1,whichdefinesthewindowsizeforlinefitting,
distinctive EEG features, yielding remarkable performance
was established at 1.5 epochs. Additionally, an l-point moving
across various BCI paradigms [17]. In this context, we
average was applied to the loss curves for smoothing purposes
reproduced the EEGNet-8,2 architectureto achieve comparable
prior to conducting line fitting. Finally, we employed the early
results. The network’s hyperparameters were maintained in the
stoppingstrategytodeterminethenumberoftrainingiterations.
optimal configuration recommended in the original publication
The training process was halted if there was no reduction in
[17]. Moreover, the raw EEG data performed similar pre-
validation loss for 20 consecutive epochs.
processing to the DeepConvNet model, ensuring consistent
input preparation for the EEGNet-8,2 training process.
F. Baseline Approaches 4) Spectral-spatial CNN: The spectral-spatial CNN frame-
To validate the effectiveness of MixNet, we implemented work, built upon CNN architectures, was introduced by [11],
five state-of-the-art methods for performance comparison. demonstrating outstanding performance in subject-independent
1) FBCSP-SVM: FBCSP builds on the original CSP al- MI decoding. This framework learned the spectral-spatial
gorithm [25], extracting discriminative EEG features from input, capturing discriminative features from EEG signals
multiple frequency bands. We implemented FBCSP using across multiple frequency bands. This paper transformed the
MNE-Pythonpackage(version0.20)[55],applyingfourspatial raw EEG signals into a similar spectral-spatial representation
filters to decompose EEG signals into nine 4 Hz-wide bands as conducted in [11]. The spectral-spatial CNN model was
(4–8 Hz, 8–12 Hz, ..., 36–40 Hz) via 5th order non-causal implemented using the optimal parameters defined in the9
TABLE III: Classification performance (Accuracy ± SD and F1-
original paper.
score±SD)in%ofMixNetusingthesubject-dependentandsubject-
5) MIN2Net: MIN2Net, introduced in our previous research
independent manners comparisons on five different CSP components
[12], was developed as an advanced end-to-end neural network (U). Bold denotes the best numerical values.
architecture based on a multi-task autoencoder framework.
Within this architecture, MIN2Net concurrently accomplishes Datasets #ofcomponent Subject-dependent Subject-independent
Accuracy F1-score Accuracy F1-score
unsupervised EEG-MI reconstruction and supervised EEG-MI
2 77.35±14.41 76.36±15.62 67.11±11.46 64.31±14.19
classification tasks, leading to the capture of meaningful fea- 4 76.16±12.81 75.30±13.54 69.07±12.10 67.24±14.38
tures. The demonstrated performance of MIN2Net is excellent BCICIV2a 6 76.08±14.01 75.03±15.24 67.93±12.73 65.13±15.77
8 75.71±13.24 74.43±14.64 68.30±12.75 66.01±15.05
in extracting discriminative features for subject-independent 10 74.81±13.15 73.55±14.53 68.38±11.72 66.05±14.44
MI decoding. 2 75.66±15.85 73.59±18.51 64.67±10.81 60.46±14.60
4 78.38±14.18 76.90±16.29 63.78±10.20 59.69±13.63
BNCI2015-001 6 77.88±15.94 76.20±18.41 62.34±9.79 57.96±13.00
G. Performance Evaluation 8 77.46±15.16 75.91±17.17 61.39±9.71 57.06±12.96
10 75.78±15.77 73.77±18.59 61.39±9.23 56.71±12.94
To validate MixNet’s performance in MI classification, we 2 75.24±16.45 73.76±18.33 61.02±14.30 55.67±18.13
conducted experiments using subject-dependent and subject- 4 74.69±17.49 73.52±18.83 65.17±14.01 60.89±17.82
SMR-BCI 6 72.00±16.73 70.42±18.33 66.02±14.69 61.56±18.72
independent approaches on six benchmark datasets: BCIC IV 8 71.62±14.90 70.31±15.99 64.81±13.62 59.05±18.97
2a, BCIC IV 2b, BNCI2015-001, SMR-BCI, High-Gamma, 10 69.26±16.22 67.58±17.83 63.79±14.49 57.37±19.97
2 77.64±16.71 77.05±17.33 65.32±12.62 62.50±15.31
and OpenBMI. Figure 3 illustrates how we divided the training
4 79.45±16.14 79.05±16.54 66.86±11.02 64.75±13.00
and testing sets for both manners. High-Gamma 6 80.23±15.28 79.87±15.67 69.04±12.06 67.14±14.02
8 79.66±15.87 79.00±16.76 69.57±11.62 67.92±13.52
The training and testing sets were sourced from the same
10 78.82±16.31 78.24±16.91 69.84±10.82 67.97±12.98
subject but in different sessions in the subject-dependent setup. 2 67.94±16.98 67.31±17.40 71.09±14.29 70.12±15.23
For subject-independent evaluation, we utilized leave-one- 4 68.56±16.70 67.73±17.37 70.41±13.82 69.78±14.34
OpenBMI 6 68.07±16.78 67.25±17.43 69.45±13.48 68.89±13.88
subject-out cross-validation (LOSO-CV). In both scenarios, we 8 67.47±16.74 66.40±17.60 69.62±13.69 69.04±14.15
10 66.78±16.61 65.82±17.32 70.14±13.78 69.54±14.28
applied stratified 5-fold cross-validation (CV) to partition the
training set into new training and validation subsets, ensuring
balanced class representation. Lastly, we evaluated all the
considered methods’ performance using Accuracy, F1-score, 3) Experiment III: Low-density EEG data collection needs
and area under the curve (AUC), offering a comprehensive reduced sensors or electrodes, resulting in less expensive
assessment of their effectiveness [56]. setup and equipment. This makes deploying EEG systems in
This paper consists of three main experiments as follows: various real-world scenarios, such as home monitoring, remote
1) Experiment I: In pursuit of optimal hyperparameter healthcare, and mobile applications, more feasible and cost-
settings for MixNet, we conducted an ablation study to refine effective [57], [58]. However, the low-density EEG system
parameter choices. Initially, we explored the influence of the might sacrifice some spatial resolution, resulting in suboptimal
number of spatial filters, denoted as U in Equation 4. A grid classification performance.
search was conducted over {2,4,6,8,10} to determine the TodemonstratethepracticalityofMixNetindevelopingreal-
optimal value of U. worldapplications,weconductedabinaryclassificationtaskon
As evident from Equation 6, the margin parameter α three-channel EEG-MI in both subject-dependent and subject-
significantly impacts MixNet’s training. We systematically independent manners. The objective of this experiment was
investigated α via experiments in MixNet, seeking the optimal to evaluate the effectiveness of MixNet compared to all other
value from the set {0.1,0.5,1.0,5.0,10.0,100.0}. baseline methods in a low-density EEG system scenario. By
Thelatentvectorsizez withinMixNet’sautoencodermodule considering the three-channel EEG-MI dataset, it is important
is vital in training, preserving essential features in high- to note that the number of spatial filters U was set to 2.
dimensional EEG-MI signals. We performed a grid search over Additionally, the optimal MI classification performance of
{8,U×N b,64,128,256} to ascertain an optimal latent vector MixNet across all hyperparameter searches (α, z, W) for a
size. The optimal latent vector size aims to reduce dimensions three-channelEEG-MIdatasetisreportedinthesupplementary
while maintaining the quality of data representation. materials.
The parameter W, representing the warm-up period, was
integral to Algorithm 1. We conducted a grid search over the
set {2,3,5,7,9} to determine the optimal W size.
2) Experiment II: A comparative analysis was conducted H. Visualization
to evaluate the classification performance of MixNet and
all baseline methods. The study centered on binary MI To compare the efficacy of deep learning techniques in
classification using high-density EEG recordings. It aimed extracting highly discriminative features from EEG signals,
toinvestigatetheeffectivenessofalltheapproachesconsidered we used the t-SNE method [59] to visually represent the
across five benchmark datasets (BCIC IV 2a, BNCI2015-001, generalized brain features learned by different deep learning
SMR-BCI, High-Gamma, and OpenBMI), covering subject- methods within a two-dimensional embedding space. Across
dependent and subject-independent scenarios. We assessed all all trained models, the t-SNE algorithm was used to visualize
methods using identical training, validation, and testing sets to the high-dimensional embedding space at the input of the FC
ensure a fair comparison. layer.10
TABLE IV: Classification performance (Accuracy ± SD and F1-
Table IV depicts the classification results obtained when
score±SD)in%ofMixNetusingthesubject-dependentandsubject-
utilizing different values of the margin parameter (α) in the
independent manners comparisons on six different margins (α). Bold
denotes the best numerical values. DMLmoduleofMixNet.Wefoundthatthevalueofthemargin
parameter significantly impacts the classification performance.
Datasets Margins Subject-dependent Subject-independent Utilizing a margin value of 1.0 for the BCIC IV 2a and High-
Accuracy F1-score Accuracy F1-score
Gamma datasets, 0.5 for the SMR-BCI dataset, 5 for the
0.1 76.48±15.17 75.48±16.38 69.23±11.80 67.42±13.88
OpenBMIdataset,andamarginvalueof10fortheBNCI2015-
0.5 77.30±15.22 76.32±16.37 68.09±12.08 65.79±14.52
1 77.35±14.41 76.36±15.62 69.07±12.10 67.24±14.38 001dataset,MixNetdemonstrateditsoptimalsubject-dependent
BCICIV2a
5 75.65±14.53 74.36±16.08 67.69±11.90 65.81±13.90
performance. Furthermore, MixNet demonstrated its most
10 76.40±14.50 75.30±15.72 67.55±11.56 65.61±13.57
100 75.23±14.40 74.43±15.15 67.85±12.70 65.87±14.74 effective subject-independent performance by utilizing margin
0.1 77.08±14.48 75.58±16.16 64.67±11.45 60.44±15.08 values of 0.1 for the BCIC IV 2a dataset, 1.0 for the High-
0.5 78.10±15.00 76.31±18.26 64.76±10.11 60.88±13.27 Gamma dataset, 5 for the SMR-BCI dataset, and 100 for the
1 78.38±14.18 76.90±16.29 64.67±10.81 60.46±14.60
BNCI2015-001 5 79.00±13.27 77.73±15.36 65.64±11.25 61.46±15.04 BNCI2015-001 and OpenBMI datasets.
10 79.03±13.29 77.97±14.93 64.98±10.68 61.15±13.84 Table V presents the averaged classification performance
100 76.20±13.86 74.56±16.04 65.85±11.23 62.08±14.64
correspondingtodifferentlatentvectorz sizeswithinMixNet’s
0.1 75.45±16.49 73.84±18.58 65.93±15.88 60.80±20.71
0.5 75.40±16.33 74.14±17.95 64.98±14.99 60.09±19.53 autoencoder module. It can be observed that the latent vector’s
SMR-BCI 1 75.24±16.45 73.76±18.33 66.02±14.69 61.56±18.72 dimensions significantly impact the averaged classification
5 74.38±16.94 72.65±19.14 66.14±14.00 62.14±17.73
performance. Notably, when the latent vector size is defined
10 73.36±16.58 71.74±18.43 65.31±13.97 61.06±17.88
100 74.88±16.83 73.32±18.83 63.36±13.93 58.06±18.18 as z = U ×N , MixNet achieves its optimal performance
b
0.1 79.95±15.73 79.59±16.12 68.73±11.73 66.41±14.41 in a subject-dependent manner across all the considered
0.5 79.61±15.80 79.02±16.59 69.21±10.54 67.04±13.22
datasets. MixNet achieves its highest performance in a subject-
1 80.23±15.28 79.87±15.67 69.84±10.82 67.97±12.98
High-Gamma
5 79.66±16.56 79.16±17.07 68.98±11.51 66.77±14.00 independent manner by configuring z =U ×N for the SMR-
b
10 79.84±16.09 79.46±16.52 69.57±11.08 67.85±12.89
BCI, High-Gamma, and OpenBMI datasets. Conversely, the
100 77.96±16.87 77.57±17.25 69.41±11.90 66.91±14.75
best MixNet performance on the BCIC IV 2a and BNCI2015-
0.1 68.47±16.57 67.83±17.06 70.59±14.11 69.65±14.97
0.5 68.57±16.36 67.86±16.91 70.83±14.00 69.97±14.76 001 datasets is reached when z =128 is employed.
1 68.56±16.70 67.73±17.37 71.09±14.29 70.12±15.23
OpenBMI Table VI presents the average classification performance
5 68.95±16.75 68.04±17.54 71.25±14.33 70.32±15.21
10 68.70±16.69 67.84±17.36 71.25±14.33 70.36±15.14 across different warm-up period sizes (W) within adaptive
100 68.17±16.21 67.12±17.12 71.95±14.18 71.13±14.88 gradientblending. Thesizeofthe warm-upperiodsignificantly
influences the average classification performance. Optimal
subject-dependent performance was observed with a warm-
IV. RESULTS
up size of 2 for the BNCI2015-001 and SMR-BCI datasets,
This section presents the findings and statistical analysis of 5 for the High-Gamma and OpenBMI datasets, and 7 for the
Experiments I, II, and III, which aim to establish the efficacy BCIC IV 2a dataset. Remarkably, MixNet achieves its optimal
of MixNet. Moreover, we employ visualization techniques subject-independent performance with a warm-up period size
to illustrate the discriminative pattern of the EEG features of 5 across all evaluated datasets.
acquired by the MixNet model compared to other baseline
methods.Theresultsofeachexperimentwerereportedinterms
B. Experiment II: Binary MI Classification
of accuracy, F1-score, and AUC, along with their respective
standard deviations (Accuracy ± SD, F1-score ± SD, AUC ± The overall classification performance of MixNet and five
SD). baseline methods, using high-density EEG-MI across all five
datasets in both subject-dependent and subject-independent set-
tings, is presented in Table VII. In subject-dependent analyses,
A. Experiment I: Ablation Study
MixNet exhibited superior accuracy and F1-score across the
TableIIIpresentstheclassificationresultsforvaryingvalues BCIC IV 2a, BNCI2015-001, High-Gamma, and OpenBMI
ofthenumberofspatialfilters,denotedasU,incorporatedinto datasets. Remarkably, MixNet achieved its best performance
the spectral-spatial signals of MixNet. The impact of different on the SMR-BCI dataset in terms of F1-score. Furthermore,
valuesofspatialfiltersonthefinalclassificationperformanceis MixNet’s performance in terms of AUC outperformed that of
evident.Notably,thechoiceofU valuessignificantlyinfluenced other baseline methods on the High-Gamma and OpenBMI
the outcomes. MixNet demonstrated its highest performance datasets.
in a subject-dependent manner by utilizing U = 2 for the Thesubject-independentanalysisresultsindicatethatMixNet
BCIC IV 2a and SMR-BCI datasets, U =4 for the BNCI2015- displayed superior performance in accuracy, F1-score, and
001 and OpenBMI datasets, and U =6 for the High-Gamma AUC on the BCIC IV 2a and High-Gamma datasets. For the
dataset. In a subject-independent scenario, MixNet achieved BNCI2015-001 dataset, MixNet achieved its highest perfor-
optimal performance with U =2 on the BNCI2015-001 and mance in accuracy and F1-score, and the second-highest in
OpenBMI datasets, while on the BCIC IV 2a dataset, U =4 AUC. In evaluating the SMR-BCI dataset, MixNet surpassed
yielded the best results. For the SMR-BCI and High-Gamma all baseline methods in F1-score, while it secured the second-
datasets,themosteffectiveU valueswere6and10,respectively, highestperformanceinaccuracyandAUC.Additionally,onthe
contributing to MixNet’s superior performance. OpenBMI dataset, MixNet outperformed all baseline methods11
TABLEV:Classificationperformance(Accuracy±SDandF1-score
± SD) in % of MixNet using the subject-dependent and subject-
independent manners comparisons on five different sizes of latent
vector (z). Bold denotes the best numerical values.
#oflatent Subject-dependent Subject-independent
Datatsets
vector Accuracy F1-score Accuracy F1-score
8 76.93±15.03 75.74±16.45 68.64±12.27 66.40±14.80
U×Nf 77.35±14.41 76.36±15.62 69.23±11.80 67.42±13.88
BCICIV2a 64 76.60±14.95 75.63±16.19 68.43±12.32 65.89±15.40
128 76.62±14.26 75.64±15.38 69.35±11.79 67.54±13.86
256 76.37±14.58 75.58±15.43 68.92±11.27 67.08±13.46
8 78.50±14.03 77.06±16.03 64.53±11.52 59.93±15.30
U×Nf 79.03±13.29 77.97±14.93 65.85±11.23 62.08±14.64
BNCI2015-001 64 79.12±13.78 77.64±15.88 65.47±11.04 61.64±14.50
128 77.88±14.55 76.03±17.59 66.22±11.70 62.55±14.95
256 77.25±14.89 75.18±18.08 64.55±11.90 60.49±15.57
8 75.62±16.11 73.86±18.33 63.90±13.77 59.02±18.35
U×Nf 75.40±16.33 74.14±17.95 66.14±14.00 62.14±17.73
SMR-BCI 64 75.14±16.32 73.81±18.02 66.17±14.17 62.06±18.31
128 74.17±16.52 72.95±18.02 64.52±13.22 59.76±17.36
256 73.71±16.88 72.52±18.33 64.26±12.97 59.69±17.01
8 79.57±15.77 79.02±16.38 68.91±11.15 66.93±13.42 Fig. 4: Impact of the number of training samples on binary classifi-
U×Nf 80.23±15.28 79.87±15.67 69.84±10.82 67.97±12.98
cation performance, measured by AUC score, across two considered
High-Gamma 64 79.79±15.55 79.19±16.48 69.18±11.51 67.31±13.62
128 78.45±16.14 77.82±16.76 69.18±11.58 67.02±14.13 methods.
256 79.21±16.19 78.60±17.08 69.55±10.78 67.70±12.79
8 68.52±16.54 67.73±17.12 71.62±14.25 70.75±15.04 TABLE VI: Classification performance (Accuracy ± SD and F1-
U×Nf 68.95±16.75 68.04±17.54 71.95±14.18 71.13±14.88
score±SD)in%ofMixNetusingthesubject-dependentandsubject-
OpenBMI 64 68.40±16.98 67.67±17.56 71.82±14.47 70.88±15.38
128 68.69±16.81 67.86±17.49 71.75±14.27 70.91±15.00 independent manners comparisons on five different sizes of warm-up
256 68.77±16.97 68.04±17.55 71.68±14.49 70.66±15.49 period (W). Bold denotes the best numerical values.
Subject-dependent Subject-independent
Datatsets Warm-up
in accuracy, F1-score, and AUC, with the exception of the Accuracy F1-score Accuracy F1-score
2 76.20±15.21 74.99±16.65 69.23±11.97 67.47±13.95
MIN2Net approach.
3 77.07±14.60 76.17±15.77 68.94±11.66 67.13±13.73
Figure4illustratesthevariationinclassificationperformance, BCICIV2a 5 77.35±14.41 76.36±15.62 69.35±11.79 67.54±13.86
7 77.64±15.08 76.71±16.22 68.89±11.86 67.09±13.93
measuredbytheAUCmetric,relativetothenumberoftraining
9 76.85±14.74 75.67±16.46 68.72±11.91 66.71±14.17
samples. The x-axis denotes the number of training samples
2 80.03±13.17 79.25±14.30 65.32±11.18 61.60±14.23
across all datasets, while the y-axis represents the binary 3 78.73±13.73 77.68±15.30 65.12±11.02 61.21±14.64
BNCI2015-001 5 79.03±13.29 77.97±14.93 66.22±11.70 62.55±14.95
classificationAUCforMixNetandSpectral-SpatialCNNmeth-
7 78.34±13.66 76.72±16.17 65.77±10.94 62.16±13.78
ods. It can be seen that MixNet consistently outperforms the 9 77.47±13.68 75.78±16.38 64.93±11.36 61.05±14.60
Spectral-Spatial CNN method in terms of AUC in both subject- 2 76.26±16.53 75.17±17.99 64.86±13.74 60.03±17.86
3 75.83±16.47 74.36±18.42 66.14±13.95 62.06±17.69
dependent and subject-independent scenarios, particularly with
SMR-BCI 5 75.40±16.33 74.14±17.95 66.14±14.00 62.14±17.73
larger datasets. Therefore, the volume of training samples 7 75.88±16.34 74.65±17.87 65.33±14.39 61.12±18.36
is a key factor in enhancing MixNet’s classification efficacy, 9 75.57±16.49 74.47±18.01 64.60±14.65 59.61±19.26
2 78.98±16.19 78.43±16.78 68.25±11.68 65.67±14.34
demonstrating its robustness with extensive training data.
3 79.70±16.44 79.12±17.10 69.54±11.04 67.54±13.31
HighGamma 5 80.23±15.28 79.87±15.67 69.84±10.82 67.97±12.98
7 79.95±15.47 79.52±15.86 69.45±11.23 67.56±13.30
C. Experiment III: Low-density EEG-MI Classification 9 79.55±15.55 79.04±16.08 68.48±10.76 66.43±13.12
2 68.57±16.79 67.74±17.50 71.83±14.29 70.93±15.16
TableVIIIillustratesthebinaryclassificationperformanceof
3 68.52±17.12 67.59±17.89 71.69±14.33 70.87±15.03
MixNet and five baseline methods using low-density EEG (3- OpenBMI 5 68.95±16.75 68.04±17.54 71.95±14.18 71.13±14.88
channel)acrosstheBCICIV2bdataset.Thisevaluationcovers 7 69.09±16.86 68.41±17.37 71.52±14.33 70.78±14.91
9 69.16±16.45 68.37±17.13 71.59±14.20 70.81±14.83
both subject-dependent and subject-independent scenarios. It
can be seen that MixNet outperforms all baseline methods
in terms of accuracy and F1-score in the subject-dependent
for the multi-task learning architecture. The effectiveness
setting. In the subject-independent manner, MixNet achieved
of MixNet is demonstrated through its performances, as
the highest performance in terms of accuracy, F1-score, and
presented in both Table VII and Table VIII. MixNet indicates
AUC. This result indicates the promising potential of MixNet
outstanding performance across various data sizes, including
in developing real-world applications.
small, medium, and large, in both subject-dependent and
subject-independent settings. Notably, MixNet achieved the
V. DISCUSSION
highest F1-score for all evaluated datasets in both settings,
MixNet is developed in this paper to address the unstable indicating its effectiveness in reducing false positives and
learningbehaviorencounteredinMIN2Net,particularlyevident negatives. Furthermore, MixNet consistently outperforms all
whendealingwithlimiteddatasizesandtheneedforexhaustive state-of-the-art methods in terms of accuracy, F1-score, and
parametersearchestodeterminetheoptimalsetoflossweights AUC in both scenarios, particularly with large-scale datasets12
TABLE VII: Classification performance (Accuracy ± SD, F1-score ± SD, and AUC ± SD) in % for the subject-dependent and subject-
independent MI classification (using high-density EEG) on BCIC IV 2a, BNCI2015-001, SMR-BCI, High-Gamma, and OpenBMI compared
to six different methods. The numerical values where MixNet’s performance surpasses that of all the baseline approaches are denoted in bold.
Subject-dependent Subject-independent
Dataset Models
Accuracy F1-score AUC Accuracy F1-score AUC
FBCSP-SVM 75.93±14.93 74.60±16.98 80.29±23.75 58.06±9.85 52.97±13.87 62.21±16.17
EEGNet-8,2 65.93±18.44 61.81±22.41 69.83±21.33 64.26±11.03 61.32±13.28 73.35±14.07
DeepConvNet 63.72±17.18 62.66±18.09 67.21±20.40 56.34±8.86 47.25±14.27 66.50±17.48
BCICIV2a
Spectral-spatialCNN 76.91±13.75 75.93±14.63 86.77±13.70 66.05±13.70 63.18±15.97 75.27±16.53
MIN2Net 65.23±16.14 64.68±16.58 70.45±19.97 60.03±9.24 55.75±12.84 68.61±12.22
MixNet 77.64±15.08 76.71±16.22 85.44±15.02 69.35±11.79 67.54±13.86 77.49±12.06
FBCSP-SVM 78.36±14.10 77.40±15.21 76.18±28.68 57.13±10.58 48.70±15.30 53.31±28.76
EEGNet-8,2 67.41±18.03 65.51±19.87 71.76±20.36 61.78±12.16 56.59±16.26 71.35±16.90
DeepConvNet 70.55±18.18 69.16±19.60 75.56±21.17 61.16±13.94 54.82±18.26 72.37±18.65
BNCI2015-001
Spectral-spatialCNN 79.82±12.55 78.61±13.95 90.82±9.21 63.77±13.74 57.10±18.84 78.58±18.29
MIN2Net 74.06±16.13 73.45±16.74 79.27±18.25 60.75±8.17 56.98±10.90 70.26±13.58
MixNet 80.03±13.17 79.25±14.30 87.88±11.50 66.22±11.70 62.55±14.95 78.56±15.76
FBCSP-SVM 74.19±17.28 72.70±18.79 67.39±21.61 62.71±15.42 55.47±21.23 65.59±24.24
EEGNet-8,2 67.76±18.09 64.55±21.33 71.79±21.40 58.07±11.45 48.24±17.04 72.52±18.35
DeepConvNet 61.40±15.66 59.28±17.17 63.58±19.12 65.26±16.83 59.03±22.02 76.48±20.18
SMR-BCI
Spectral-spatialCNN 76.76±16.66 74.39±20.39 83.07±16.86 66.21±15.15 60.21±20.61 78.94±17.73
MIN2Net 65.90±16.50 64.98±16.96 70.87±19.83 59.79±13.72 51.17±19.07 77.53±19.60
MixNet 76.26±16.53 75.17±17.99 82.37±17.65 66.14±14.00 62.14±17.73 75.76±17.27
FBCSP-SVM 76.29±16.35 75.98±16.66 76.45±19.77 62.86±10.87 58.65±14.17 72.21±14.73
EEGNet-8,2 70.64±19.20 69.67±20.23 74.87±20.04 63.52±11.44 59.59±14.64 74.91±14.93
DeepConvNet 71.05±19.95 69.04±22.02 75.49±22.87 66.82±12.49 63.19±16.09 78.72±15.15
HighGamma
Spectral-spatialCNN 77.89±14.69 76.91±16.00 84.25±13.56 65.29±12.49 61.01±16.31 76.91±13.60
MIN2Net 73.48±18.23 73.33±18.35 77.77±19.56 68.60±11.79 66.83±13.26 77.74±15.89
MixNet 80.23±15.28 79.87±15.67 85.90±15.15 70.00±9.53 68.71±10.58 79.60±12.78
FBCSP-SVM 66.20±16.43 65.12±17.32 59.75±25.22 64.96±12.70 62.99±14.24 70.82±16.64
EEGNet-8,2 60.41±17.12 57.77±19.04 62.19±19.58 68.84±13.87 66.48±16.36 76.77±15.06
DeepConvNet 60.31±16.76 58.60±17.85 62.10±19.03 68.33±15.33 65.71±17.80 75.55±16.96
OpenBMI
Spectral-spatialCNN 65.19±15.94 62.20±19.19 69.36±17.47 68.24±13.54 67.06±14.45 75.06±16.17
MIN2Net 61.03±14.47 59.78±15.29 64.56±17.52 72.03±14.04 71.17±14.80 78.92±14.97
MixNet 68.95±16.75 68.04±17.54 73.30±18.44 71.95±14.18 71.13±14.88 78.05±15.97
TABLE VIII: Classification performance (Accuracy ± SD, F1-score ± SD, and AUC ± SD) in % for the subject-dependent and subject-
independent MI classification (using 3-channel EEG) on BCIC IV 2b dataset compared to six different methods. Bold denotes the best
MixNet’s performance when it outperforms all the baseline methods.
Subject-dependent Subject-independent
Dataset Models
Accuracy F1-score AUC Accuracy F1-score AUC
FBCSP-SVM 74.80±14.27 74.30±14.63 78.91±20.23 66.31±10.15 63.68±12.07 73.70±16.52
EEGNet-8,2 73.91±14.00 72.10±16.63 81.70±15.85 72.46±10.34 71.48±11.15 82.32±14.10
DeepConvNet 75.30±15.29 73.95±16.64 84.06±17.56 73.96±12.11 72.91±12.97 83.78±14.26
BCIC2b
Spectral-spatialCNN 75.99±13.51 75.16±14.34 85.29±14.77 72.72±10.80 71.06±13.25 83.16±12.96
MIN2Net 74.55±14.45 74.23±14.56 81.52±16.80 74.67±10.90 74.17±11.14 83.43±14.30
MixNet 77.07±14.59 76.84±14.68 83.90±16.92 75.66±10.49 75.23±10.78 84.03±12.93
such as the High-Gamma and OpenBMI datasets. This finding datasets.Thisfindingsuggeststhatsettingthelatentvectorsizes
exhibits the importance of the volume of training samples in equal to MixNet’s input dimensions enables MixNet to capture
enhancing MixNet’s classification efficacy, demonstrating its generalized features and provide optimal MI classification
robustness with massive training data. performance.
Based on the findings from Table III and Table IV, it is Based on the findings illustrated in Figure 5, it can be
obvious that the optimal number of spatial filters and margins seen that the latent embedding features produced by MixNet
vary significantly between subject-dependent and subject- exhibitahigherdegreeofcompactnessinthesettingofsubject-
independent settings. The difference can be explained by the dependentMIclassification,incomparisontotheotherbaseline
substantial differences in the distribution of EEG data between methods.Furthermore,theyareextendingtheirpresenceacross
the two settings. In other words, there is a high variability the projection space for each class. The finding indicates that
observed in EEG signals among different subjects when MixNet has superior results in the learning process when the
considering subject-independent analysis, whereas this issue data size is either large or small, owing to the high-quality
does not arise when considering subject-dependent analysis. representationoftheMIsignalsinthelearnedlatentembedding
The margin values in the subject-independent are greater than features.
those in the subject-dependent. This is because the margin In a subject-independent setting, as illustrated in Figure 6,
must be present to ensure that the learned embeddings have a MixNetgenerateshighlydiscriminativepatternsoverthesmall-
structured space where similar samples are clustered together andlarge-scaledatasets(i.e.,BCICIV2aandOpenBMI,respec-
and there is a clear differentiation between different classes. tively) compared to the other baseline methods. Consequently,
In contrast, Table V demonstrates that configuring MixNet MixNetoutperformsothersduetoitssuperiorMIrepresentation
with latent vector dimensions z =U ×N results in optimal in the learned latent embedding features.
b
performanceinbothsubject-dependentandsubject-independent To gain a deeper understanding of the internal workings of
scenarios across the SMR-BCI, High-Gamma, and OpenBMI the optimization process of MixNet, we analyzed the changes13
Datasets Raw EEG DeepConvNet EEGNet-8,2 Spectral-spatial CNN MIN2Net MixNet a) Mean Square Error Loss b) Triplet Loss
BCIC IV
2a
OpenBMI
Left hand MI Right hand MI
Fig. 5: RepresentationofrawandlearnedEEGfeaturesgeneratedby
c) Cross-entropy Loss d) Total Loss
eachconsideredmethodusingt-SNEprojection.Theimagecompares
two-dimensional t-SNE projections designed for subject-dependent
binary classification.
Datasets Raw EEG DeepConvNet EEGNet-8,2 Spectral-spatial CNN MIN2Net MixNet
BCIC IV
2a
Fig. 7: The training and validation losses during the OpenBMI
dataset’s training period for subject-dependent binary classification.
OpenBMI The graphs depict the averaged losses for 54 subjects, including a)
mean square error, b) triplet loss, c) cross-entropy loss, and d) total
loss.
Left hand MI Right hand MI
Fig. 6: RepresentationofrawandlearnedEEGfeaturesgeneratedby
eachconsideredmethodusingt-SNEprojection.Thepicturecompares
MixNet’s subject-dependent classification, can help alleviate
two-dimensional t-SNE projections designed for subject-independent
binary classification. overfitting when dealing with a small size of data.
To further assess the feasibility of MixNet for online BCI
systems, we conducted an experiment using the OpenBMI and
in training and validation losses for binary classification. This BCIC IV 2b datasets, comparing MixNet’s time complexity
analysis was performed on the OpenBMI dataset, utilizing with that of established baseline methods. According to
both subject-dependent (Figure 7) and subject-independent the results shown in Table IX, MixNet has lower trainable
(Figure 8) manners. The study monitored four distinct types of parameters than Spectral-spatial CNN and DeepConvNet for
losses in the MixNet model: mean squared error (MSE), triplet thelow-densityEEG-MIdatasetandlowerthanSpectral-spatial
loss, cross-entropy (CE) loss, and total loss. This monitoring CNN for the high-density EEG-MI dataset. MixNet’s GPU
was conducted throughout 80 epochs for all subjects. memory usage is significantly lower than that of DeepConvNet
In the subject-dependent task, it can be observed that three and Spectral-spatial CNN for both datasets, although it is
validation losses did not exhibit signs of overfitting, except for slightlyhigherthanthatofMIN2NetandEEGNet.Additionally,
the cross-entropy (CE) loss, which showed indications of rapid MixNet demonstrates training and prediction speeds slightly
overfitting. This suggests that the supervised learning module different from those of compact baseline models, such as
in MixNet might be less effective when dealing with small EEGNet-8,2 and DeepConvNet. These findings suggest that
datasizes.However,theunsupervisedanddeepmetriclearning MixNetcouldbeeffectivelyusedinonlineBCIsystemsbecause
modules play vital roles in helping MixNet alleviate overfitting it offers compactness and maintains latency or prediction times
during its training. As a result, MixNet’s total loss continues below 0.5 seconds during testing sessions.
to perform well without showing signs of overfitting. In the While MixNet demonstrates promising classification results,
subject-independent task, it can be seen that the convergence there remains plenty of room for further enhancement. EEG
process of MixNet in this learning task is remarkably robust. multi-class classification poses a significant challenge for
We noticed that the cross-entropy (CE) loss starts displaying MixNet due to the requirement of transforming the inputs
signs of overfitting after around ten epochs, whereas the other using FBCSP. However, this transformation technique does
losses do not show any indications of overfitting. not sufficiently capture the characteristics of multi-class EEG
ToobservehowMixNet’slossweights(w(1),w(2),andw(3)) data. Finally, we can investigate the application of MixNet to
were adapted for adaptive gradient blending during MixNet’s additional EEG measurements, including steady-state visual
training, we examined the progression of adaptive loss weights evoked potentials (SSVEP), movement-related cortical poten-
throughout all cross-validation folds on the OpenBMI dataset, tials (MRCPs), and event-related potentials (ERP). MixNet can
as depicted in Figure 9. The graphs exhibit that all weights help obtain the most distinguishing features for classification.
decrease rapidly, which helps to impede learning and prevent
regular overfitting of the data. Interestingly, the weight ratio VI. CONCLUSION
forthecross-entropy(CE)lossinsubject-dependentlearningis This paper presents a novel framework known as MixNet,
muchlowerthaninsubject-independentlearning.Thisindicates which builds upon the core ideas of MIN2Net and efficiently
that reducing the cross-entropy loss weight in a multi-tasking addresses difficulties faced in subject-specific learning, multi-
architecture while increasing other loss weights, as seen in task learning, and loss weight optimization. The development14
TABLEIX:Timecomplexityoftraining(T )andprediction(T )
train pred
a) Mean Square Error Loss b) Triplet Loss insecondsperepochforallmethods,numberoftrainableparameters,
and GPU memory usage in (MB) for all approaches on low- and
high-density EEG-MI datasets.
Dataset ComparisonModel # ptr aa ri an mab sle G UP sU agM e(e Mm Bor )y Subject-dependent Subject-independent
Ttrain Tpred Ttrain Tpred
FBCSP-SVM - - - 0.0043 - 0.0405
IB VCI 2C b SpecDE treE ae lG p -SCN poe ant tv- i8 aN l,2 e Ct NN 161 ,4 4 4, 2 78 , 59 8 ,0 0 62 34 250 2 3. . .1 6 62 2 61 3 08 0 15 00 0 .. . 81 1 14 5 71 6 85 8 00 0 .. . 82 3 76 1 57 2 31 0 20 0 .. . 46 6 15 2 88 2 94 3 00 0 .. . 82 3 35 0 76 0 72 7
MIN2Net 10,709 0.8391 0.2148 0.2875 0.7857 0.3052
MixNet 44,777 1.3862 0.2573 0.3150 1.2081 0.3262
c) Cross-entropy Loss d) Total Loss FBCSP-SVM - - 0.0020 - 0.1906
EEGNet-8,2 5,162 0.1199 0.1882 0.1439 3.0951 0.1372
OpenBMI DeepConvNet 153,427 2.6079 0.1804 0.1618 1.7497 0.4734
Spectral-SpatialCNN 77,577,714 1210.0701 2.2476 1.0934 11.9067 0.8560
MIN2Net 55,232 1.3086 0.3527 0.2851 1.3626 0.1043
MixNet 178,328 1.0491 0.1614 0.3160 3.9670 0.3201
healthcare, and mobile applications, more feasible and cost-
effective.
Fig. 8: The training and validation losses during the OpenBMI REFERENCES
dataset’s training period for subject-independent binary classification.
[1] J.R.Wolpaw,N.Birbaumer,D.J.McFarland,G.Pfurtscheller,andT.M.
The plots exhibit the averaged losses for 54 subjects, including a)
Vaughan,“Brain–computerinterfacesforcommunicationandcontrol,”
mean square error, b) triplet loss, c) cross-entropy loss, and d) total ClinicalNeurophysiology,vol.113,no.6,pp.767–791,2002.
loss. [2] J.Vidal,“Real-timedetectionofbraineventsineeg,”Proceedingsof
theIEEE,vol.65,no.5,pp.633–641,1977.
[3] S.Bozinovski,M.Sestakov,andL.Bozinovska,“Usingeegalpharhythm
tocontrolamobilerobot,”inProceedingsoftheAnnualInternational
ConferenceoftheIEEEEngineeringinMedicineandBiologySociety,
1988,pp.1515–1516vol.3.
[4] K. P. Thomas, C. Guan, C. T. Lau, A. P. Vinod, and K. K. Ang, “A
newdiscriminativecommonspatialpatternmethodformotorimagery
brain–computerinterfaces,”IEEETransactionsonBiomedicalEngineer-
ing,vol.56,no.11,pp.2730–2733,2009.
[5] K.K.AngandC.Guan,“Eeg-basedstrategiestodetectmotorimagery
for control and rehabilitation,” IEEE Transactions on Neural Systems
andRehabilitationEngineering,vol.25,no.4,pp.392–401,2017.
[6] C.S.Nam,Y.Jeon,Y.-J.Kim,I.Lee,andK.Park,“Movementimagery-
(a) (b)
relatedlateralizationofevent-related(de)synchronization(erd/ers):Motor-
Fig. 9: ThechangeoftheadaptivelossweightsduringtheOpenBMI imagerydurationeffects,”ClinicalNeurophysiology,vol.122,no.3,pp.
dataset’s training period, specifically for binary classification. The 567–577,2011.
graphs illustrate the averaged loss weights for all cross-validation [7] C.Zich,S.Debener,C.Kranczioch,M.G.Bleichner,I.Gutberlet,and
M. De Vos, “Real-time eeg feedback during simultaneous eeg–fmri
folds using 54 subjects, including (a) the subject-dependent scheme
identifiesthecorticalsignatureofmotorimagery,”NeuroImage,vol.114,
and (b) the subject-independent scheme.
pp.438–447,2015.
[8] H.-I.SukandS.-W.Lee,“Subjectandclassspecificfrequencybands
selection for multiclass motor imagery classification,” International
of MixNet involves the integration of spectral-spatial signals, JournalofImagingSystemsandTechnology,vol.21,no.2,pp.123–130,
2011.
adaptive gradient blending, and a multi-task autoencoder.
[9] J. Wolpaw, N. Birbaumer, W. Heetderks, D. McFarland, P. Peckham,
The integration of this fusion allows MixNet to effectively G. Schalk, E. Donchin, L. Quatrano, C. Robinson, and T. Vaughan,
compress and extract distinctive patterns from EEG-MI data “Brain-computerinterfacetechnology:areviewofthefirstinternational
meeting,”IEEETransactionsonRehabilitationEngineering,vol.8,no.2,
while concurrently performing the classification of MI classes.
pp.164–173,2000.
To evaluate MixNet’s performance, we conducted binary [10] G.Wang,C.Teng,K.Li,Z.Zhang,andX.Yan,“Theremovalofeog
classificationcomparisonswithfivebaselinemethodsacrosssix artifacts from eeg signals using independent component analysis and
multivariateempiricalmodedecomposition,”IEEEJournalofBiomedical
benchmark datasets. The results of the classification analysis
andHealthInformatics,vol.20,no.5,pp.1301–1308,2016.
demonstrated that MixNet outperformed all state-of-the-art [11] O.-Y.Kwon,M.-H.Lee,C.Guan,andS.-W.Lee,“Subject-independent
methods in subject-dependent and subject-independent MI brain–computerinterfacesbasedondeepconvolutionalneuralnetworks,”
IEEETransactionsonNeuralNetworksandLearningSystems,vol.31,
classification tasks over six benchmark datasets in terms of the
no.10,pp.3839–3852,2020.
F1-score metric. Notably, the MI classification performance [12] P. Autthasan, R. Chaisaen, T. Sudhawiyangkul, P. Rangpong, S. Ki-
on the low-density EEG database BCIC IV 2b stands out atthaveephong, N. Dilokthanakul, G. Bhakdisongkhram, H. Phan,
C. Guan, and T. Wilaiprasitporn, “Min2net: End-to-end multi-task
among the six experimental datasets. The proposed method
learningforsubject-independentmotorimageryeegclassification,”IEEE
demonstrated promising experimental results in this context, TransactionsonBiomedicalEngineering,vol.69,no.6,pp.2105–2118,
outperforming state-of-the-art algorithms in classifying EEG- 2022.
[13] J.-S.Bang,M.-H.Lee,S.Fazli,C.Guan,andS.-W.Lee,“Spatio-spectral
MI signals with three EEG channels. This finding indicates the
featurerepresentationformotorimageryclassificationusingconvolutional
possibilityandpracticabilityofusingthismodelfordeveloping neuralnetworks,”IEEETransactionsonNeuralNetworksandLearning
a low-density EEG system, such as low-density wearable Systems,vol.33,no.7,pp.3038–3049,2022.
[14] A.Al-Saegh,S.A.Dawwd,andJ.M.Abdul-Jabbar,“Deeplearningfor
devices. This system could make deploying EEG systems in
motorimageryeeg-basedclassification:Areview,”BiomedicalSignal
various real-world scenarios, such as home monitoring, remote ProcessingandControl,vol.63,p.102172,2021.15
[15] R.T.Schirrmeister,J.T.Springenberg,L.D.J.Fiederer,M.Glasstetter, Conference on Computer Vision and Pattern Recognition (CVPR’06),
K.Eggensperger,M.Tangermann,F.Hutter,W.Burgard,andT.Ball, vol.2,2006,pp.1735–1742.
“Deeplearningwithconvolutionalneuralnetworksforeegdecodingand [35] F. Schroff, D. Kalenichenko, and J. Philbin, “Facenet: A unified
visualization,”HumanBrainMapping,vol.38,no.11,pp.5391–5420, embeddingforfacerecognitionandclustering,”in2015IEEEConference
2017. onComputerVisionandPatternRecognition(CVPR),2015,pp.815–823.
[16] S.Sakhavi,C.Guan,andS.Yan,“Learningtemporalinformationfor [36] W.Chen,X.Chen,J.Zhang,andK.Huang,“Beyondtripletloss:adeep
brain-computer interface using convolutional neural networks,” IEEE quadrupletnetworkforpersonre-identification,”inProceedingsofIEEE
TransactionsonNeuralNetworksandLearningSystems,vol.29,no.11, InternationalConferenceonComputerVisionandPatternRecognition.
pp.5619–5629,2018. IEEE,2017,pp.1320–1329.
[17] V.J.Lawhern,A.J.Solon,N.R.Waytowich,S.M.Gordon,C.P.Hung, [37] X.Wang,X.Han,W.Huang,D.Dong,andM.R.Scott,“Multi-similarity
andB.J.Lance,“EEGNet:acompactconvolutionalneuralnetworkfor losswithgeneralpairweightingfordeepmetriclearning,”inProceedings
EEG-basedbrain–computerinterfaces,”JournalofNeuralEngineering, oftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,
vol.15,no.5,p.056013,jul2018. 2019,pp.5022–5030.
[18] T. Wilaiprasitporn, A. Ditthapron, K. Matchaparn, T. Tongbuasirilai, [38] R.Thiyagarajan,C.Curro,andS.Keene,“Alearnedembeddingspace
N.Banluesombatkul,andE.Chuangsuwanich,“Affectiveeeg-basedper- foreegsignalclustering,”in2017IEEESignalProcessinginMedicine
sonidentificationusingthedeeplearningapproach,”IEEETransactions andBiologySymposium(SPMB),2017,pp.1–4.
onCognitiveandDevelopmentalSystems,vol.12,no.3,pp.486–496, [39] H.Alwasiti,M.Z.Yusoff,andK.Raza,“Motorimageryclassification
2020. forbraincomputerinterfaceusingdeepmetriclearning,”IEEEAccess,
[19] P. Zhang, X. Wang, W. Zhang, and J. Chen, “Learning spa- vol.8,pp.109949–109963,2020.
tial–spectral–temporaleegfeatureswithrecurrent3dconvolutionalneural [40] K.Liu,M.Yang,Z.Yu,G.Wang,andW.Wu,“Fbmsnet:Afilter-bank
networksforcross-taskmentalworkloadassessment,”IEEETransactions multi-scaleconvolutionalneuralnetworkforeeg-basedmotorimagery
onNeuralSystemsandRehabilitationEngineering,vol.27,no.1,pp. decoding,”IEEETransactionsonBiomedicalEngineering,vol.70,no.2,
31–42,2019. pp.436–445,2023.
[20] H.Phan,O.Y.Che´n,M.C.Tran,P.Koch,A.Mertins,andM.DeVos, [41] D.E.Rumelhart,G.E.Hinton,andR.J.Williams,LearningInternal
“Xsleepnet:Multi-viewsequentialmodelforautomaticsleepstaging,” RepresentationsbyErrorPropagation. Cambridge,MA,USA:MIT
IEEETransactionsonPatternAnalysisandMachineIntelligence,vol.44, Press,1986,p.318–362.
no.9,pp.5903–5915,2022. [42] G.E.HintonandR.R.Salakhutdinov,“Reducingthedimensionalityof
[21] M. Gottlibe, O. Rosen, B. Weller, A. Mahagney, N. Omar, A. Khuri,
datawithneuralnetworks,”Science,vol.313,no.5786,pp.504–507,
I. Srugo, and J. Genizi, “Stroke identification using a portable eeg 2006.
device–apilotstudy,”NeurophysiologieClinique,vol.50,no.1,pp. [43] G.AntoniolandP.Tonella,“Eegdatacompressiontechniques,”IEEE
21–25,2020.
TransactionsonBiomedicalEngineering,vol.44,no.2,pp.105–114,
1997.
[22] Y.Zhang,K.Qian,S.Q.Xie,C.Shi,J.Li,andZ.-Q.Zhang,“Ssvep-
[44] P. Vincent, H. Larochelle, I. Lajoie, Y. Bengio, and P.-A. Manzagol,
basedbrain-computerinterfacecontrolledroboticplatformwithvelocity
“Stackeddenoisingautoencoders:Learningusefulrepresentationsina
modulation,”IEEETransactionsonNeuralSystemsandRehabilitation
deep network with a local denoising criterion,” J. Mach. Learn. Res.,
Engineering,vol.31,pp.3448–3458,2023.
vol.11,p.3371–3408,Dec.2010.
[23] H.Ramoser,J.Muller-Gerking,andG.Pfurtscheller,“Optimalspatial
[45] Y. Qiu, W. Zhou, N. Yu, and P. Du, “Denoising sparse autoencoder-
filtering of single trial eeg during imagined hand movement,” IEEE
basedictaleegclassification,”IEEETransactionsonNeuralSystemsand
TransactionsonRehabilitationEngineering,vol.8,no.4,pp.441–446,
RehabilitationEngineering,vol.26,no.9,pp.1717–1726,2018.
2000.
[46] A.Gogna,A.Majumdar,andR.Ward,“Semi-supervisedstackedlabel
[24] S. Lemm, B. Blankertz, G. Curio, and K.-R. Muller, “Spatio-spectral
consistent autoencoder for reconstruction and analysis of biomedical
filters for improving the classification of single trial eeg,” IEEE
signals,”IEEETransactionsonBiomedicalEngineering,vol.64,no.9,
TransactionsonBiomedicalEngineering,vol.52,no.9,pp.1541–1548,
pp.2196–2205,2017.
2005.
[47] P.K.ParashivaandA.P.Vinod,“Anewchannelselectionmethodusing
[25] KaiKengAng,ZhengYangChin,HaihongZhang,andCuntaiGuan,
autoencoderformotorimagerybasedbraincomputerinterface,”in2019
“Filterbankcommonspatialpattern(fbcsp)inbrain-computerinterface,”
IEEEInternationalConferenceonSystems,ManandCybernetics(SMC).
in2008IEEEInternationalJointConferenceonNeuralNetworks(IEEE
IEEE,2019,pp.3641–3646.
WorldCongressonComputationalIntelligence),2008,pp.2390–2397.
[48] N.Mammone,C.Ieracitano,H.Adeli,andF.C.Morabito,“Autoencoder
[26] H.-I.SukandS.-W.Lee,“Anovelbayesianframeworkfordiscriminative
filterbankcommonspatialpatternstodecodemotorimageryfromeeg,”
featureextractioninbrain-computerinterfaces,”IEEETransactionson
IEEEJournalofBiomedicalandHealthInformatics,2023.
PatternAnalysisandMachineIntelligence,vol.35,no.2,pp.286–299,
[49] M. Tangermann, K.-R. Mu¨ller, A. Aertsen, N. Birbaumer, C. Braun,
2013.
C.Brunner,R.Leeb,C.Mehring,K.Miller,G.Mueller-Putz,G.Nolte,
[27] K.K.Ang,Z.Y.Chin,C.Wang,C.Guan,andH.Zhang,“Filterbank
G.Pfurtscheller,H.Preissl,G.Schalk,A.Schlo¨gl,C.Vidaurre,S.Waldert,
commonspatialpatternalgorithmonbcicompetitionivdatasets2aand
and B. Blankertz, “Review of the bci competition iv,” Frontiers in
2b,”FrontiersinNeuroscience,vol.6,p.39,2012.
Neuroscience,vol.6,p.55,2012.
[28] F.LotteandC.Guan,“Regularizingcommonspatialpatternstoimprove [50] R.Leeb,F.Lee,C.Keinrath,R.Scherer,H.Bischof,andG.Pfurtscheller,
bcidesigns:Unifiedtheoryandnewalgorithms,”IEEETransactionson
“Brain–computercommunication:Motivation,aim,andimpactofexplor-
BiomedicalEngineering,vol.58,no.2,pp.355–362,2011. ing a virtual apartment,” IEEE Transactions on Neural Systems and
[29] Y.Jiao,Y.Zhang,X.Chen,E.Yin,J.Jin,X.Wang,andA.Cichocki, RehabilitationEngineering,vol.15,no.4,pp.473–482,2007.
“Sparsegrouprepresentationmodelformotorimageryeegclassification,” [51] J. Faller, C. Vidaurre, T. Solis-Escalante, C. Neuper, and R. Scherer,
IEEEJournalofBiomedicalandHealthInformatics,vol.23,no.2,pp. “Autocalibration and recurrent adaptation: Towards a plug and play
631–641,2019. onlineerd-bci,”IEEETransactionsonNeuralSystemsandRehabilitation
[30] Y.Zhang,T.Zhou,W.Wu,H.Xie,H.Zhu,G.Zhou,andA.Cichocki, Engineering,vol.20,no.3,pp.313–319,2012.
“Improvingeegdecodingviaclustering-basedmultitaskfeaturelearning,” [52] D.Steyrl,R.Scherer,O.Fo¨rstner,andG.Mu¨ller-Putz,“Motorimagery
IEEETransactionsonNeuralNetworksandLearningSystems,pp.1–11, brain-computerinterfaces:Randomforestsvsregularizedlda-non-linear
2021. beatslinear,”inProceedingsofthe6thInternationalBrain-Computer
[31] K. Zhang, N. Robinson, S.-W. Lee, and C. Guan, “Adaptive transfer InterfaceConferenceGraz2014,2014,pp.061–1–061–4.
learningforeegmotorimageryclassificationwithdeepconvolutional [53] M.-H.Lee,O.-Y.Kwon,Y.-J.Kim,H.-K.Kim,Y.-E.Lee,J.Williamson,
neuralnetwork,”NeuralNetworks,vol.136,pp.1–10,2021. S.Fazli,andS.-W.Lee,“EEGdatasetandOpenBMItoolboxforthree
[32] A. Nagarajan, N. Robinson, and C. Guan, “Relevance-based channel BCIparadigms:aninvestigationintoBCIilliteracy,”GigaScience,vol.8,
selectioninmotorimagerybrain–computerinterface,”JournalofNeural no.5,012019.
Engineering,vol.20,no.1,p.016024,2023. [54] W. Wang, D. Tran, and M. Feiszli, “What makes training multi-
[33] J.Lu,J.Hu,andJ.Zhou,“Deepmetriclearningforvisualunderstanding: modalclassificationnetworkshard?”inProceedingsoftheIEEE/CVF
An overview of recent advances,” IEEE Signal Processing Magazine, conferenceoncomputervisionandpatternrecognition,2020,pp.12695–
vol.34,no.6,pp.76–84,2017. 12705.
[34] R. Hadsell, S. Chopra, and Y. LeCun, “Dimensionality reduction [55] A.Gramfort,M.Luessi,E.Larson,D.Engemann,D.Strohmeier,C.Brod-
by learning an invariant mapping,” in 2006 IEEE Computer Society beck, R. Goj, M. Jas, T. Brooks, L. Parkkonen, and M. Ha¨ma¨la¨inen,16
“Megandeegdataanalysiswithmne-python,”FrontiersinNeuroscience, MaartenDeVoshasajointappointmentasProfessor
vol.7,p.267,2013. in the Departments of Engineering and Medicine
[56] S.Bhattacharyya,A.Konar,D.N.Tibarewala,andM.Hayashibe,“A at KU Leuven after being Associate Professor at
generictransferableeegdecoderforonlinedetectionoferrorpotential the University of Oxford, United Kingdom, and
intargetselection,”FrontiersinNeuroscience,vol.11,p.226,2017. Junior Professor at the University of Oldenburg,
[57] S. Venkatesh, E. R. Miranda, and E. Braund, “Ssvep-based brain– Germany. He obtained an MSc (2005) and PhD
computerinterfaceformusicusingalow-densityeegsystem,”Assistive (2009)inElectricalEngineeringfromKULeuven,
Technology,pp.1–11,2022. Belgium.HisacademicworkfocusesonAIforhealth,
[58] C.He,Y.-Y.Chen,C.-R.Phang,C.Stevenson,I.-P.Chen,T.-P.Jung,and innovativebiomedicalmonitoringandsignalanalysis
L.-W.Ko,“Diversityandsuitabilityofthestate-of-the-artwearableand fordailylifeapplications,inparticularthederivation
wirelesseegsystemsreview,”IEEEJournalofBiomedicalandHealth ofpersonalisedbiosignaturesofpatienthealthfrom
Informatics,2023. dataacquiredviawearablesensorsandtheincorporationofsmartanalytics
[59] L.vanderMaatenandG.Hinton,“Visualizingdatausingt-sne,”Journal into unobtrusive systems. His pioneering research in the field of mobile
ofMachineLearningResearch,vol.9,no.86,pp.2579–2605,2008. real-life brain-monitoring has won several innovation prices, among which
the prestigious Mobile Brain Body monitoring prize in 2017. In 2019, he
was awarded the Martin Black Prize for the best paper in Physiological
Measurements. In 2023, he was also elected as Laureate of the Flemish
AcademyofSciences,disciplineTechnicalSciences.HeisAssociateEditor
ofJournalofBiomedicalandHealthInformatics,andontheeditorialboard
ofJournalofNeuralEngineeringandNatureDigitalMedicine.
Phairot Autthasan received the B.Sc. degree in
ChemistryDepartment,KingMongkut’sUniversity
ofTechnologyThonburi,Bangkok,Thailandin2017,
and the Ph.D. degree in Information Science and
Technology,VidyasirimedhiInstituteofScienceand
Technology(VISTEC),Thailand,in2023.Hewasa
visitingPh.D.researcherinDepartmentofElectrical
Engineering(ESAT),KatholiekeUniversiteitLeuven
(KULeuven),Belgium,in2022.Heiscurrentlya
postdoctoralresearcherattheVidyasirimedhiInstitute
ofScienceandTechnology,Thailand.Hisresearch
interestsincludeAIforhealthcare,biosignalanalysis,brain-computerinterfaces,
andmachinelearningforbiosignals.
TheerawitWilaiprasitpornisascientistspecializing
inmedicalAIandapassionateadvocatefordeeptech
startups;foundedInterfaces(AIinHealthResearch
TeamatVISTEC)andSensAI(AI-DrivenAnomaly
RattanaphonChaisaenreceivedtheB.Sc.degreein
SensingforBetterHealth).Hiseffortshaveplayed
computersciencefromKhonKaenUniversity,Khon
acrucialroleinbuildingremotehealthmonitoring
Kaen, Thailand, in 2018. He is currently working
systems,benefittingover30,000peopleduringthe
towardthePh.D.degreeininformationscienceand
COVID-19pandemic.Thisearnedhima2022IEEE
technologywiththeBio-InspiredRoboticsandNeu-
R10HumanitarianTechnologyActivitiesOutstanding
ralEngineeringLaboratory,SchoolofInformation
VolunteerAwardnomination.Dr.Theerawitserves
ScienceandTechnology,VidyasirimedhiInstituteof
asasupervisorforpostgraduatestudentsatVISTEC,
Science and Technology (VISTEC), Thailand. His
guidingresearchanddevelopmentefforts.Heisactivelyinvolvedinestablishing
researchinterestsincludedeeplearningapproaches
auniversityspinoffcompanythatwillcreateawarenessofThailand’spotential
appliedtobiosignals,brain-computerinterfaces,and
asahubforresearchanddevelopment,contributingtothesustainablegrowth
assistivetechnology.
oftheThaieconomyandindustry.DrTheerawitremainsactiveintheInstitute
ofElectricalandElectronicsEngineers(IEEE),furthersolidifyinghisimpact
onadvancingtechnologyforthebenefitofhumanity.
HuyPhanreceivedtheDr.-Ing.degree(summacum
laude) in computer science from the University of
Lu¨beck, Germany, in 2017. From 2017 to 2018,
he was a postdoctoral research assistant with the
UniversityofOxford,U.K.From2019to2020,he
wasalecturerwiththeUniversityofKent,U.K.From
2020-2022,hewasalecturerinartificialintelligence
Queen Mary University of London, U.K. and a
Turing Fellow at The Alan Turing Institute, U.K.
In 2023, he joined Amazon where he is currently
a senior research scientist. His research interests
includemachinelearningandsignalprocessing,withafocusonaudioand
biosignalanalysis.HewastherecipientoftheBerndFischerAwardforthe
best PhD thesis from the University of Lu¨beck in 2018 and the Benelux’s
IEEE-EMBSBestPaperAward2019-20.HeisanIEEEseniormember.