Provable Hyperparameter Tuning for Structured Pfaffian Settings
Maria-FlorinaBalcan AnhTuanNguyen
CarnegieMellonUniversity CarnegieMellonUniversity
ninamf@cs.cmu.edu atnguyen@cs.cmu.edu
DravyanshSharma
ToyotaTechnologicalInstituteatChicago
dravy@ttic.edu
August7,2024
Abstract
Data-drivenalgorithmdesignautomaticallyadaptsalgorithmstospecificapplicationdomains,achieving
betterperformance.Inthecontextofparameterizedalgorithms,thisapproachinvolvestuningthealgorithm’s
parametersusingprobleminstancesdrawnfromtheproblemdistributionofthetargetapplicationdomain.
This can be achieved by maximizing empirical utilities that measure the algorithms’ performance as a
functionoftheirparameters,usingprobleminstances.
Whileempiricalevidencesupportstheeffectivenessofdata-drivenalgorithmdesign,providingtheoreti-
calguaranteesforseveralparameterizedfamiliesremainschallenging. Thisisduetotheintricatebehaviors
oftheircorrespondingutilityfunctions,whichtypicallyadmitpiece-wiseanddiscontinuitystructures. In
thiswork,wepresentrefinedframeworksforprovidinglearningguaranteesforparameterizeddata-driven
algorithmdesignproblemsinbothdistributionalandonlinelearningsettings.
Forthedistributionallearningsetting, weintroducethePfaffianGJframework, anextensionofthe
classicalGJframework,thatiscapableofprovidinglearningguaranteesforfunctionclassesforwhichthe
computationinvolvesPfaffianfunctions. UnliketheGJframework,whichislimitedtofunctionclasseswith
computationcharacterizedbyrationalfunctions,ourproposedframeworkcandealwithfunctionclasses
involvingPfaffianfunctions,whicharemuchmoregeneralandwidelyapplicable. Wethenshowthatfor
manyparameterizedalgorithmsofinterest,theirutilityfunctionpossessesarefinedpiece-wisestructure,
whichautomaticallytranslatestolearningguaranteesusingourproposedframework.
Fortheonlinelearningsetting,weprovideanewtoolforverifyingdispersionpropertyofasequenceof
lossfunctions,asufficientconditionthatallowsno-regretlearningforsequencesofpiece-wisestructured
lossfunctionswherethepiece-wisestructureinvolvesPfaffiantransitionboundaries. Weuseourframework
toprovidenovellearningguaranteesformanychallengingdata-drivendesignproblemsofinterest,includ-
ingdata-drivenlinkage-basedclustering,graph-basedsemi-supervisedlearning,andregularizedlogistic
regression.
1
4202
peS
6
]GL.sc[
1v76340.9042:viXraContents
1 Introduction 4
2 Relatedworks 6
3 Preliminaries 7
4 PfaffianGJframeworkfordata-drivenalgorithmdesign 8
4.1 Pfaffianfunctions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
4.2 PfaffianGJAlgorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
5 Pfaffianpiece-wisestructurefordata-drivendistributionallearning 11
5.1 Priorgeneralizationframeworkforpiece-wisestructuredutilityfunctionsindata-drivenalgo-
rithmdesignanditslimitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
5.2 Arefinedpiece-wisestructurefordata-drivenalgorithmdesign . . . . . . . . . . . . . . . . . 13
5.3 Analyzingviaapproximationfunctionclass . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
6 ApplicationsofthePfaffianpiece-wisestructureframework 15
6.1 Data-drivenagglomerativehierarchicalclustering . . . . . . . . . . . . . . . . . . . . . . . . 15
6.1.1 Problemsetting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
6.1.2 Generalizationguaranteesfordata-drivenhierarchicalclustering . . . . . . . . . . . . 17
6.2 Data-drivengraph-basedsemi-supervisedlearning . . . . . . . . . . . . . . . . . . . . . . . . 19
6.2.1 Problemsettings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
6.2.2 Generalizationguaranteefordata-drivensemi-supervisedlearningwithGaussianRBF
kernelandmultipledistancefunctions . . . . . . . . . . . . . . . . . . . . . . . . . . 20
7 OnlineLearning 21
7.1 Onlinelearningfordata-drivenagglomerativehierarchicalclustering . . . . . . . . . . . . . . 22
7.2 Onlinelearningfordata-drivenregularizedlogisticregression . . . . . . . . . . . . . . . . . 23
8 Conclusionandfuturedirections 24
A Preliminaries 30
A.1 Classicalgeneralizationresults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
A.2 UniformconvergenceandPAC-learnability . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
B OmittedproofsfromSection4 31
B.1 SomefactsaboutPfaffianfunctionswithelementaryoperators . . . . . . . . . . . . . . . . . 31
B.2 ProofforTheorem4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
B.2.1 Preliminariesontheconnectionbetweenpseudo-dimensionandsolutionsetconnected
componentsbound . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
B.2.2 Pfaffianformulae . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
B.2.3 ProofforTheorem4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
C AdditionaldetailsandomittedproofforSection5 34
C.1 OmittedproofforSection5.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
C.2 OmittedproofsforSection5.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34
C.3 Adetailedcomparisonbetweenthepiece-wisestructurebyBalcanetal.[BDD+21]andour
refinedPfaffianpiece-wisestructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
2D AdditionalresultsandomittedproofsforSection6 36
D.1 OmittedproofsforSection6.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
D.2 Additionalresult: Data-drivenregularizedlogisticregression . . . . . . . . . . . . . . . . . . 37
D.2.1 Problemsetting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
D.2.2 Generalizationguaranteefordata-drivenregularizedlogisticregression . . . . . . . . 38
E AdditionalbackgroundandomittedproofsforSection7 40
E.1 Onlinelearningfordata-drivenagglomerativehierarchicalclustering . . . . . . . . . . . . . . 42
E.2 Onlinelearningfordata-drivenregularizedlogisticregression . . . . . . . . . . . . . . . . . 46
31 Introduction
Data-drivenalgorithmdesign[GR16,Bal20]isamodernapproachthatdevelopsandanalyzesalgorithmsbased
on the assumption that problem instances come from an underlying application domain. Unlike traditional
worst-caseoraverage-caseanalyses,thisapproachleveragesobservedprobleminstancestodesignalgorithms
that achieve high performance for specific problem domains. In many application domains [BNVW17,
BDSV18,BKST22],algorithmsareparameterized,meaningtheyareequippedwithtunablehyperparameters
whichsignificantlyinfluencetheirperformance. Wedevelopgeneraltechniquesapplicabletoalargevariety
of parameterized algorithm families for the selection of domain-specific good algorithms by learning the
hyperparametersfromtheprobleminstancescomingfromthedomain.
Typically,theperformanceofanalgorithmisevaluatedbyaspecificutilityfunction. Inmoreconcreteterms,
foranalgorithmparameterizedbya ∈ A,considertheutilityfunctionclassU = {u : X → [0,H] | a ∈ A},
a
whereu (x)gaugestheperformanceofthealgorithmwithparametersawheninputtingaprobleminstance
a
x. In the data-driven algorithm design setting, we assume an unknown underlying distribution D over X,
representing the application domain on which the algorithm operates. Consequently, designing algorithms
tailoredtoaspecificdomaincorrespondstotheoptimalselectionofparametersaforthegivendomain.
Applications of data-driven algorithm design span various domains, including low-rank approximation
[IVY19, IWW21, LLL+23], sparse linear systems solvers [LGM+20], dimensionality reduction [ALN21],
amongothers. Itsempiricalsuccess[IVY19,ACC+11,IWW21]underscoresthenecessityforatheoretical
understanding of this approach. Intensive efforts have been made towards theoretical understanding for
data-drivenalgorithmdesign,includinglearningguaranteesfornumericallinearalgebramethods[BIW22],
tuningregularizationparametersforregressionproblems[BKST22,BNS24],unsupervisedandsemi-supervised
learning[BDW18,BDL20,BS21],applicationinintegerandmixed-integerprogramming[BDSV18,BPSV21],
tonamebutafew.
Priortheoreticalworkondata-drivenalgorithmdesignfocusesontwomainsettings: distributional(also
known as statistical/batch) learning [BDD+21, BIW22] and online learning [BDV18]. In the distributional
learning setting, there is a learner trying to optimize hyperparameters for the algorithm within a specific
domain,givenaccesstoprobleminstancesfromthatdomain. Inthiscase,thequestionisaboutthesample
complexity: Howmanyprobleminstancesarerequiredtolearngoodparametersthatguaranteethealgorithm’s
performanceinthatdomain? Intheonlinelearningsetting,thereisasequenceofprobleminstanceschosen
byanadversaryarrivingovertime. Thegoalnowistodesignano-regretlearningalgorithm: adjustingthe
algorithm’sparametersontheflysothatthedifferencebetweentheaverageutilityandtheutilitycorresponding
tothebestparametersinhindsightdiminishesovertime.
Themainchallengeinestablishinglearningguaranteesfortheutilityfunctionclassesliesinthecomplex
structure of the utility function. In other words, even a very small perturbation in a can lead to a drastic
changeintheperformanceofthealgorithm,makingtheanalysisofsuchclassesofutilityfunctionsparticularly
challenging. In response to this challenge, prior works take an alternative approach by analyzing the dual
utility functions U∗ = {u : A → [0,H] | x ∈ X}, which often admits piece-wise structured behavior
x
[BDD+21,BNVW17,BIW22].
Building upon this observation, in the distributional learning setting, Balcan et al. [BDD+21] propose
a general approach that analyzes the learnability of the utility function class via the piece and boundary
functionsclassinducedbythepiece-wisestructure. Targetingamorerestrictedscenario,Bartlettetal. [BIW22]
introducedarefinementfortheGJframework[GJ93],whichestablishestighterguaranteesforutilityfunction
classes of which the piece-wise structure involves only rational functions. In the online learning setting,
Balcan et al. [BDV18] introduce the dispersion condition, which serves as a sufficient condition allowing
no-regretlearningforpiece-wiseLipschitzfunctions. Essentially,thedispersionpropertyemphasizesthatifthe
discontinuityofutilityfunctionsequencesdoesnotdenselyconcentrateinanysmallregionofparameterspace,
no-regretlearningispossible. Despitetheirbroadapplicability,thesegeneralizedframeworksexhibitinherent
4limitations. Inthedistributionallearningsetting,thepiece-wisestructureframeworkintroducedbyBalcanet
al. [BDD+21]oftenresultsinsub-optimalguarantees,especiallywhentheparametersaremulti-dimensional.
Moreover,theframeworkrequiresanalyzingthepieceandboundaryfunctionclassesasanintermediatestep,
whichisnottrivialandsometimesleadstolooseorvacuousbounds. TherefinedGJframeworkinstantiatedby
Bartlettetal. [BIW22]islimitedtothecaseswherethecomputationofutilityfunctionsonlyinvolvesrational
functionsoftheparameters. Fortheonlinelearningsetting,thedispersionpropertyisgenerallychallengingto
verify[BDV18,BDP20,BS21],andrequiresmanyextraassumptionsonthediscontinuityofutilityfunctions
sequence. Moreover, when the form of discontinuity goes beyond affinity and rational functions, no prior
verifyingtechniquecanbeapplied.
Motivatedbythelimitationsofpriorresearch,partofthisworkaimstopresentrefinedtheoreticalframe-
worksfordata-drivenalgorithmdesignwhentheutilityfunctionadmitsaspecificstructure. Inthedistributional
learning setting, we introduce a powerful Pfaffian GJ framework that can establish learning guarantees for
functionclasseswhosediscontinuityinvolvesPfaffianfunctions. Roughlyspeaking,Pfaffianisaverygeneral
classoffunctionsthatcapturesawiderangeoffunctionsofinterest,includingrational,exponentiation,and
combinationsofthose,amongothers. Furthermore,wedemonstratethatmanydata-drivenalgorithmdesign
problemsexhibitaspecificrefinedPfaffianpiece-wisestructure,which,whencombinedwiththePfaffianGJ
framework,canestablishlearningguaranteesforsuchproblems. Intheonlinelearningsetting,weintroduce
anoveltooltoverifythedispersionproperty,wherethediscontinuityofutilityfunctionsequencesinvolves
Pfaffianfunctions,whichgobeyondaffinityandrationalfunctions.
Another aim of this work is to provide learning guarantees for several under-investigated data-driven
algorithmdesignproblems,wherethepiece-wisestructureoftheutilityfunctionsinvolvesPfaffianfunctions.
The problems we consider have been investigated in simpler settings, including data-driven agglomerative
hierarchical clustering [BNVW17, BDL20], data-driven semi-supervised learning [BS21], and data-driven
regularizedlogisticregression[BNS24]. However,previousinvestigationshavelimitations: theyeitherhave
missingresultsfornaturalextensionsofthesettingsunderstudy[BNVW17,BS21],requirestrongassumptions
[BDL20],orsolelyconsiderdistributionallearningsettings[BNS24]. Moreover,wenotethatthetechniques
usedinpriorworkareinsufficientandcannotbeappliedinoursettings,whichinvolvePfaffiananalysis.
Bycarefullyanalyzingtheutilityfunctionsassociatedwiththeseproblems,weuncovertheirunderlying
PfaffianstructuresandcontrolthePfaffiancomplexities,whichallowsustoleverageourproposedframeworks
toestablishlearningguarantees. Itisimportanttonotethatanalyzingthosespecificproblemsposesasignificant
challenge: alooseestimationofthePfaffianfunctioncomplexitywhencombinedwithourproposedframeworks
wouldstillleadtolooseorvacuouslearningguarantees.
Contributions. Inthiswork,weprovidearefinedframeworkfortheoreticalanalysisofdata-drivenalgorithm
designproblems. Wetheninvestigatemanyunder-investigateddata-drivenalgorithmdesignproblems,analyzing
theirunderlyingproblemstructure,andthenleveragingournewlyproposedframeworkstoprovidelearning
guaranteeforthoseproblems. Concretely,ourcontributionsare:
• We present the Pfaffian GJ framework (Definition 5, Theorem 4.2), a general approach for analyzing the
pseudo-dimensionofvariousfunctionclassesofinterest. Thisframeworkdrawsinspirationfromtherefined
version of the GJ framework introduced by Bartlett et al. (2022) [BIW22]. However, in contrast to the
conventionalGJframeworkwhichisonlycapableofhandlingcomputationrelatedtorationalfunctions,the
PfaffianGJframeworkcanhandlecomputationsthatinvolvePfaffianfunctions—amuchbroaderfunction
class—significantly increasing its applicability. We note that our proposed Pfaffian GJ framework is of
independentinterestandcanbeappliedtootherproblemsbeyonddata-drivenalgorithmdesign.
• Fordistributionallearning(statistical/batch)data-drivenalgorithmdesign,weintroducearefinedpiece-wise
structure(Definition8)forthedualutilityfunctionclass,whichapplieswheneverthepieceandboundary
functionsarePfaffian. Incontrasttothepriorpiece-wisestructureproposedbyBalcanetal. [BDD+21],our
5frameworkassumesthepieceandboundaryfunctionsbelongtothebroadclassofPfaffianfunctionsand
yields refined learning guarantees for this broad class of utility functions. We then show how the refined
piece-wisestructurecanbecombinedwiththenewlyproposedPfaffianGJframeworktoprovidelearning
guarantees(Theorem5.2)forproblemsthatsatisfythisproperty.
• Foronlinelearningdata-drivenalgorithmdesign,weintroduceageneralapproach(Theorem7.1,Theorem7.2)
for verifying the dispersion property [BDV18]—a sufficient condition for online learning in data-driven
algorithmdesign. Priorwork[BDP20,BS21]providestechniquesforverifyingdispersiononlywhenthe
pieceboundariesarealgebraicfunctions. Wesignificantlyexpandtheclassoffunctionsforwhichonline
learning guarantees may be obtained by establishing a novel tool which applies for Pfaffian boundary
functions.
• Wederivenovellearningguaranteesforavarietyofunderstudieddata-drivenalgorithmdesignproblems,
including data-driven agglomerative clustering (Theorem 6.1), data-driven graph-based semi-supervised
learning(Theorem6.4),aswellasrecovertheguaranteefordata-drivenregularizedlogisticregressionin
previouswork(TheoremD.3). Bycarefullyanalyzingtheunderlyingstructuresoftheutilityfunctionsfor
theseproblems,wecanconvertthemintotheformofourproposedframeworks,whichautomaticallyyield
learningguarantees.
2 Related works
Data-driven algorithm design. Data-driven algorithm design [GR16, Bal20] is a modern approach for
automaticallyconfiguringalgorithmsandmakingthemadaptivetospecificapplicationdomains. Incontrast
toconventionalalgorithmdesignandanalysis,whichpredominantlyfocusesonworst-caseoraverage-case
scenarios,data-drivenalgorithmdesignpositstheexistenceofan(unknown)underlyingproblemdistribution
thatdictatestheprobleminstancethatalgorithmsencounter. Theprimaryobjectiveshiftstoidentifyingoptimal
configurationsforsuchalgorithms,leveragingavailableprobleminstancesathandfromthesameapplication
domain. Empiricalworkshaveconsistentlyvalidatedtheeffectivenessofdata-drivenalgorithmapproaches
invariousdomains,includingmatrixlow-rankapproximation[IVY19,IWW21],matrixsketching[LLL+23],
mixed-integerlinearprogramming[CKFB24],amongothers. Thesefindingsunderscoretheapplicationofthe
data-drivenalgorithmdesignapproachinreal-worldapplications.
Distributionallearningguaranteefordata-drivenalgorithmdesign. Motivatedbytheempiricalsuccess
of data-driven algorithm design, there is an emerged line of work that focuses on theoretically analyzing
theunderlyingmechanism,mostlyfocusingonprovidinggeneralizationguarantees. Thisincludeslearning
guaranteefordata-drivenalgorithmdesignsoflow-rankapproximationandsketching[BIW22],learningmetrics
forclustering[BDL20,BS21,BDW18],integerandmixed-integerlinearprogramming[BPSV21,BPSV22a,
BDSV18],hyperparametertuningforregularizedregression[BKST22,BNS24],decisiontreelearning[BS24],
robustnearest-neighbors[BBSZ23]amongothers.
Remarkably,Balcanetal. [BS21]introducedageneralframeworkforestablishinglearningguaranteesfor
problems that admit a specific piece-wisestructure. Despite its broad applicability, the framework exhibits
limitations: (1)itrequiresanintermediatetaskofanalyzingthedualofpieceandboundaryfunctions,whichis
nottrivial,and(2)itfailstoincorporateextrausefulinformationaboutpieceandboundaryfunctions,sometimes
leadingtosub-optimalbounds. Buildingonthisinsight,Bartlettetal. [BIW22]instantiatesarefinementofthe
classicalGJframework[GJ93],offeringanimprovedlearningguaranteefordata-drivenalgorithmdesignwhere
thepiece-wisestructureinvolvesonlyrationalfunctions. However,itisessentialtonotethattheirframework
cannotbeappliedbeyondrationalstructures.
6Onlinelearningguaranteefordata-drivenalgorithmdesign. Anotherlineofworkfocusesonproviding
no-regret learning guarantees for data-driven algorithm design problems in online learning settings. This
includesonlinelearningguaranteesforheuristicknapsack,SDP-roundingforintegerquadraticprogramming
[BNVW17],anddata-drivenlinkage-basedclustering[BDP20],amongothers.
Onlinelearningfordata-drivenalgorithmdesignisgenerallyachallengingtaskduetothediscontinuity
andpiece-wisestructureoftheutilityfunctionsencountered. Mostworksprovidelearningguaranteesinthis
settingbyverifyingthedispersionoftheutilityfunctionsequence,asufficientconditionproposedbyBalcan
etal. [BDV18]statingthatthediscontinuityofthesequenceisnothighlyconcentratedinanysmallregion.
However,verifyingthedispersionpropertyisgenerallychallenging. Balcanetal. [BDP20]providesatoolfor
verifyingthedispersionproperty,targetingcaseswherethediscontinuityisdescribedbytherootsofrandom
polynomialsforone-dimensionalhyperparametersoralgebraiccurvesinthetwo-dimensionalcase. Balcan
andSharma[BS21]thengeneralizethis,providingatoolfordiscontinuitiesdescribedbyalgebraicvarietiesin
higher-dimensionalcases.
Algorithmswithpredictions. Anothermodernapproachfordesignalgorithmsisalgorithmswithpredictions
[MV22],inwhichpredictionisintegratedatcertainstagesofalgorithmstoenhancetheirperformance. Typically,
thisapproachassumestheexistenceofamachinelearningmodelcapableofreceivingprobleminstancesasinput
andgeneratingpredictionsspecifictoeachinstance. Theperformanceanalysisofthesealgorithmsiscloselytied
tothequalityofpredictionsgeneratedbythemachinelearningmodels. Ahigherqualityofpredictionsgenerally
correlateswithimprovedalgorithmicperformance. Hence,thealgorithm,nowintegratedwithpredictivemodels,
is analyzed based on its inherent algorithmic performance and the quality of predictions. algorithms with
predictionshaveprovedtheirefficacyinvariousclassicalproblems,includingsupportestimation[EIN+21],
pagemigration[IMTMR22],onlinematching,flows,andloadbalancing[LMRX20],amongothers[KBTV22,
LV21,WZ20].
While having many similarities and overlapping traits, there is a fundamental distinction between the
two approaches. Data-driven algorithm design primarily seeks to optimize algorithmic parameters directly
forspecificapplicationdomains. Ontheotherhand, algorithmswithpredictions. Besides, algorithmswith
predictions also have to decide what qualities or quantities of the problem instance to predict, and how to
predict,whichheavilyaffectstheperformanceandpropertiesofalgorithms. Itisworthnotingthatthesetwo
directionscancomplementeachotherandbeintegratedintothesamesystem,asexploredinnumerousprior
studies.
3 Preliminaries
We adhere closely to the problem setting and notation introduced by Balcan et al. [BDD+21]. Our focus
is on parameterized algorithms, where the parameter belongs to a set A ⊆ Rd. Let X represent the set of
input problem instances. The performance of the algorithm corresponding to parameter a ∈ A on input
x ∈ X isgivenbytheutilityfunctionu(x,a)whereu : X ×A → [0,H]. Theutilityfunctionclassofthat
parameteristhendefinedasU = {u : X → [0,H] | a ∈ A}. Wethendefinethedualutilityfunctionclass
a
U∗ = {u∗ : A → [0,H] | x ∈ X},whereu∗(a) := u(x,a),whichconsistsofutilityfunctionsobtainedby
x x
varyingtheparameteraforfixedprobleminstancesfromX. Thedual-classU∗ playsanimportantroleinour
analysis,whichwewilldiscusslater.
Distributionallearning. Incontrasttotraditionalworst-caseoraverage-casealgorithmanalysis,weassume
the existence of an underlying problem distribution D over X, which encapsulates information about the
relevantapplicationdomain. Undersuchanassumption,ourgoalistodeterminehowmanyprobleminstances
are sufficient to learn the near-optimal parameters of the algorithm for such application-specific problem
7distribution. Tothisend,itsufficestoboundthepseudo-dimension[Pol12]ofthecorrespondingutilityfunction
class.
Definition1(Pseudo-dimension,[Pol12]). Considerareal-valuedfunctionclassF,ofwhicheachfunction
takesinputinX. GivenasetofinputsS = (x ,...,x ),wesaythatS isshatteredbyF ifthereexistsaset
1 N
ofreal-valuedthresholdr ,...,r ∈ Rsuchthat|{(sign(f(x )−r ),...,sign(f(x )−r )) | f ∈ F}| =
1 N 1 1 N N
2N. Thepseudo-dimensionofF,denotedasPdim(F),isthemaximumsizeN ofainputsetthatF canshatter.
IfthefunctionclassF isbinary-valued,thiscorrespondstothewell-knownVC-dimension[VC74]. Itiswide-
knownintraditionallearningtheorythataboundonthepseudo-dimensionimpliesaboundonthegeneralization
error (See Appendix A.1 for further background), which is the difference between the performance of the
algorithmonthetrainingprobleminstancesandtheexpectedperformanceovertheapplicationdomainfrom
whichtheinstancesaredrawn.
Theorem3.1([AB09]). Givenareal-valuedfunctionclassF whoserangeis[0,H],andassumethatPdim(F)
isfinite. Then,givenanyδ ∈ (0,1),andanydistributionD overtheinputspaceX,withprobabilityatleast
1−δ overthedrawnofS ∼ Dn,wehave
(cid:12) (cid:12)1 (cid:88)N (cid:12) (cid:12) (cid:32) (cid:115) 1 (cid:18) 1(cid:19)(cid:33)
(cid:12) f(x )−E [f(x)](cid:12) ≤ O H Pdim(F)+ln .
(cid:12)n i x∼D (cid:12) N δ
(cid:12) (cid:12)
i=1
Online learning. In the online learning setting, there is a sequence of utility functions corresponds to a
sequence of problem instances (x ,...,x ), coming over T rounds. The task is to design a sequence of
1 T
parameters(a ,...,a )forthealgorithmthatminimizetheexpectedregretregret
1 T
T T
(cid:88) (cid:88)
Regret = max u(x ,a)− u(x ,a ).
T t t t
a∈A
t=1 t=1
Ourgoalistodesignasequenceofparametersathatachievesub-linearregret.
4 Pfaffian GJ framework for data-driven algorithm design
In a classical work, Goldberg and Jerrum [GJ93] introduced a comprehensive framework for bounding the
VC-dimension(orpseudo-dimension)ofparameterizedfunctionclassesexhibitingaspecificproperty. They
proposedthatifanyfunctionwithinagivenclasscanbecomputedviaaspecifictypeofcomputation,nameda
GJalgorithm,consistingoffundamentaloperatorssuchasaddition,subtraction,multiplication,division,and
conditionalstatements,thenthepseudo-dimensionofsuchafunctionclasscanbeeffectivelyupperbounded.
Thebounddependsontherunningtimeofthealgorithm,offeringaconvenientapproachtoreducethetaskof
boundingthecomplexityofafunctionclassintothemoremanageabletaskofcountingthenumberofoperators.
However,aboundbasedonruntimecanoftenbeoverlyconservative. Recently,Bartlettetal. [BIW22]
instantiatedarefinementfortheGJframework. NotingthatanyintermediatevaluestheGJalgorithmcomputes
are rational functions of parameters, Bartlett et al. proposed more refined complexity measures of the GJ
framework, namely the predicate complexity and the degree of the GJ algorithm. Informally, the predicate
complexityandthedegreearethenumberofdistinctrationalfunctionsinconditionalstatementsandthehighest
orderofthoserationalfunctions,respectively. Remarkably,basedontherefinedcomplexitymeasures,Bartlett
etal. showedarefinedbound,demonstratingitsefficacyinvariouscases,includingapplicationsondata-driven
algorithmdesignfornumericallinearalgebra.
It is worth noting that the GJ algorithm has limitations as it can only accommodate scenarios where
intermediatevaluesarerationalfunctions. Specifically,itdoesnotcapturemoreprevalentclassesoffunctions,
8such as the exponential function. Building upon the insights gained from the refined GJ framework, we
introduceanextendedframeworkcalledthePfaffianGJFramework. Ourframeworkcanbeusedtoboundthe
pseudo-dimensionoffunctionclassesthatcanbecomputednotonlybyfundamentaloperatorsandconditional
statements but also by a broad class of functions called Pfaffian functions, which includes exponential and
logarithmicfunctions. Technically,ourresultisarefinementoftheanalyticalapproachintroducedby[Kho91,
KM97, MW97] which is directly applicable to data-driven algorithm design. An important part of our
contributionisacarefulinstantiationofourmainresultforseveralimportantalgorithmicproblems,asanaive
applicationcouldresultinsignificantlylooserboundsonthesamplecomplexity.
4.1 Pfaffianfunctions
WepresentthefoundationalconceptsofPfaffianchains,Pfaffianfunctions,andtheirassociatedcomplexity
measures. IntroducedbyKhovanskii[Kho91],Pfaffianfunctionanalysisisatoolforanalyzingtheproperties
ofsolutionsetsofPfaffianequations. Wenotethatthesetechniqueshavebeenpreviouslyusedtoderivean
upperboundontheVC-dimensionofsigmoidalneuralnetworks[KM97].
WefirstintroducethenotionofaPfaffianchain. Intuitively,aPfaffianchainconsistsofanorderedsequence
offunctions,inwhichthederivativeofeachfunctioncanberepresentedasapolynomialofthevariablesand
previousfunctionsinthesequence.
Definition2(PfaffianChain,[Kho91]). Afinitesequenceofcontinuouslydifferentiablefunctionsη ,...,η :
1 q
Rd → R and variables a = (a ,...,a ) ∈ Rd form a Pfaffian chain C(a,η ,...,η ) if there are real
1 d 1 q
polynomialsP (a,η ,...,η )ina ,...,a ,η ,...,η ,forforalli ∈ [d]andj ∈ [q],suchthat
i,j 1 j 1 d 1 j
∂η
j
= P (a,η ,...,η ).
i,j 1 j
∂a
i
WenowdefinetwocomplexitynotationsforPfaffianchains,termedthelengthandPfaffiandegree,thatdictate
thecomplexityofaPfaffianchain. ThelengthofaPfaffianchainisthenumberoffunctionsthatappearonthat
chain,whilethePfaffiandegreeofachainisthemaximumdegreeofpolynomialsthatcanbeusedtoexpress
thepartialderivativeoffunctionsonthatchain. FormaldefinitionsofPfaffianchainlengthandPfaffiandegree
arementionedinDefinition3.
Definition3(ComplexityofPfaffianchain). GivenaPfaffianchainC(a,η ,...,η ),asdefinedinDefinition
1 q
2,wesaythatthelengthofC isq,andPfaffiandegreeofC ismax deg(P ).
i,j i,j
GivenaPfaffianchain, onecandefinethePfaffianfunction, whichissimplyapolynomialofvariablesand
functionsonthatchain.
Definition4(Pfaffianfunctions,[Kho91]). GivenaPfaffianchainC(a,η ,...,η ),asdefinedinDefinition2,
1 q
aPfaffianfunctionoverthechainC isafunctionoftheformg(a) = Q(a,η ,...,η ),whereQisapolynomial
1 q
invariablesaandfunctionsη ,...,η inthechainC.
1 q
TheconceptsofthePfaffianchain,functions,andcomplexitiesmaybeabitabstracttounfamiliarreaders. To
helpreadersbettergrasptheconceptsofPfaffianchainsandPfaffianfunctions,herearesomesimpleexamples.
Example1. ConsiderthechainC(a,ea)consistingofthevariableaandthefunctionea,wherea ∈ R. Then
C isaPfaffianchainsince d ea = ea,whichisapolynomialofdegree1inea. Hence,thechainC haslength
da
q = 1andPfaffiandegreeM = 1. Now,considerthefunctionf(a) = (ea)2+a3. Weobservethatf(a)isa
polynomialinaandea. Therefore,f(a)isaPfaffianfunctionoverthechainC.
9Example2. Thefollowingexampleisusefulwhenanalyzingthelearnabilityofclusteringalgorithms. Let
β = (β ,...,β ) ∈ Rk andα ∈ Rbevariables,andletd(β) = (cid:80)k d β > 0,whered (i ∈ [k])aresome
1 k i=1 i i i
fixed real coefficients. Consider the functions f(α,β) := 1 , g(α,β) := lnd(β), and h(α,β) := d(β)α.
d(β)
Thenf,g,andharePfaffianfunctionsfromthechainC(α,β,f,g,h)oflength3andPfaffiandegree2,since
∂f ∂f
= 0, = −d ·f2,
i
∂α ∂β
i
∂g ∂g
= 0, = d ·f,
i
∂α ∂β
i
∂h ∂h
= g·h, = d ·f ·h.
i
∂α ∂β
i
4.2 PfaffianGJAlgorithm
WenowpresentaformaldefinitionofthePfaffianGJalgorithm,whichsharessimilaritieswiththeGJalgorithm
butextendsitscapabilitiestocomputePfaffianfunctionsasintermediatevalues,inadditiontobasicoperators
andconditionalstatements. ThisimprovementmakesthePfaffianGJalgorithmsignificantlymoreversatile
comparedtotheclassicalGJframework.
Definition5(PfaffianGJalgorithm). APfaffianGJalgorithmΓoperatesonreal-valuedinputsa ∈ A ⊆ Rd,
andcanperformthreetypesofoperations:
• Arithmeticoperatorsoftheformv′′ = v⊙v′,where⊙ ∈ {+,−,×,÷}.
• Pfaffianoperatorsoftheformv′′ = η(v),whereη : R → RisaPfaffianfunction.
• Conditionalstatementsoftheform“ifv ≥ 0... else ...”.
Herev andv′ areeitherinputsor(intermediate)valuespreviouslycomputedbythealgorithm.
Definition6(PfaffianchainassociatedwithPfaffianGJalgorithm). GivenaPfaffianGJalgorithmΓoperating
onreal-valuedinputsa ∈ A ⊆ Rd,wesaythataPfaffianchainC isassociatedwithΓifalltheintermediate
valuescomputedbyΓisaPfaffianfunctionfromthechainC.
ThemaindifferencebetweentheclassicalGJalgorithm[GJ93,BIW22]andthePfaffianGJalgorithmisthat
thelatterallowsPfaffianoperatorsinitscomputation. ByleveragingthefundamentalpropertiesofPfaffian
chainsandfunctions,wecaneasilyshowthatallintermediatefunctionscomputedbyaspecificPfaffianGJ
algorithmcomefromthesamePfaffianchain(seeAppendixB.1fordetails). Thisremarkablepropertyenables
ustocontrolthecomplexityofthePfaffianGJalgorithmbycontrollingthecomplexityofthecorresponding
Pfaffianchain. Weformalizethisclaiminthefollowinglemma.
Lemma4.1. ForanyPfaffianGJalgorithmΓinvolvingafinitenumberofoperations,thereisaPfaffianchain
C offinitelengthassociatedwithΓ.
ProofSketch. ThePfaffianchainC canbeconstructedrecursivelyasfollows. Initially,wecreateaPfaffian
chainofvariablesawithlength0. UsingthebasicpropertiesofPfaffianfunctionsdiscussedinSectionB.1,
each time Γ computes a new value v, one of the following cases arises: (1) v is a Pfaffian function on the
currentchainC,or(2)wecanextendthechainC byaddingnewfunctions,increasingitslength(butstillfinite),
suchthatv becomesaPfaffianfunctiononthemodifiedchainC. □
Remark1. WenotethateachPfaffianGJalgorithmΓcanbeassociatedwithvariousPfaffianchains,with
differentcomplexities. Inanyspecificapplication,designingthecorrespondingPfaffianchainC forΓwitha
smallcomplexityisacrucialtaskthatrequirescarefulanalysis.
10We now present our main technical tool, which can be used to bound the pseudo-dimension of a function
classbyexpressingthefunctioncomputationaPfaffianGJalgorithmandgivingaboundoncomplexityofthe
associatedPfaffianchain. ThedetailedproofispresentedinAppendixB.2.
Theorem4.2. Considerareal-valuedfunctionclassLwithdomainX,ofwhicheachalgorithmL ∈ Lis
a
parameterizedbya ∈ A ⊆ Rd. Supposethatforeveryx ∈ X andr ∈ R, thereisaPfaffianGJalgorithm
Γ ,withassociatedPfaffianchainC oflengthatmostq andPfaffiandegreeatmostM,thatgivenL ∈ L,
x,r x,r a
checkwhetherL (x) > r. Moreover,assumethatvaluescomputedatintermediatestepsofΓ arefromthe
a x,r
PfaffianchainC ,eachofdegreeatmost∆;andthefunctionscomputedintheconditionalstatementsareof
x,r
atmostK Pfaffianfunctions. ThenPdim(L) ≤ d2q2+2dqlog(∆+M)+4dqlogd+2dlog∆K +16d.
Proof Sketch. The overall idea is that given N input problem instances x ,...,x and N thresholds
1 N
r ,...,r ,wewanttoboundΠ (N)thenumberofdistinctsignpatterns
1 N L
(sign(L(x ,a)−r > 0),...,sign(L(x ,a)−r > 0))
N 1 N N
obtainedbyvaryinga ∈ A,whereL(x ,a) = L (x ). Thenwesolvetheinequality2N ≤ Π (N)toobtain
i a i L
anupperboundforPdim(L).
Basedontheassumption,Π (N)canbeupperboundedbythenumberofdistinctsignpatterns
L
(sign(τ (x ,a)−r ),...,sign(τ (x ,a)−r ),...,sign(τ (x ,a)−r )),...,sign(τ (x ,a)−r )).
1 1 1 K 1 1 1 N N K N N
ByusingaresultbyKarpinskiandMacintyre[KM97],wecanturnthetaskaboveintothetaskofbounding
thesolutionsetconnectedcomponentsofasystemofatmostdequationsθ (a) = 0,...,θ (a) = 0,where
1 d
θ : A → Rfori = 1,...,dischosenfrom{r (x ,·)} . Notethatalsofromassumption,wecan
i j k j∈[K],k∈[N]
constructaPfaffianchainoflengthdq thatcontainsallthePfaffianfunctionsθ . Then,applyingaresultby
i
Khovanskii[Kho91]yieldsthedesiredresult. □
Remark 2. For the case q = 0, meaning that the functions computed in the conditional statements are
merelyrationalfunctions,Theorem4.2givesanupperboundofO(dlog(∆K)),whichmatchestherateofGJ
algorithmbyBartlettetal. [BIW22].
5 Pfaffian piece-wise structure for data-driven distributional learning
Inthissection,weproposethePfaffianpiece-wisestructureforfunctionclasses,arefinementofthepiece-wise
decomposablestructureintroducedbyBalcanetal. [BDD+21]. Comparedtotheirpiece-wisedecomposable
structure, ourproposedPfaffianpiece-wisestructureincorporatesadditionalinformationaboutthePfaffian
structuresofpieceandboundaryfunctions,aswellasthemaximumnumberofformsthatthepiecefunctions
cantake. Wearguethattheadditionalinformationcanbederivedasaby-productinmanydata-drivenalgorithm
designproblems,butbeingleftoutintheframeworkbyBalcanetal. [BDD+21]. Wethenshowthatifthedual
utilityfunctionclassofaparameterizedalgorithmadmitsthePfaffianpiece-wisestructure,wecanestablish
animprovedlearningguarantee,comparedtotheframeworkbyBalcanetal. [BDD+21],forthealgorithm
byleveragingourproposedPfaffianGJframework. Additionally,weproposeafurtherrefinedargumentfor
thecasewherealldualutilityfunctionssharethesameboundarystructures,whichleadstofurtherimproved
learningguarantee.
5.1 Priorgeneralizationframeworkforpiece-wisestructuredutilityfunctionsindata-driven
algorithmdesignanditslimitations
Inthissection,wediscusstheutilityofthePfaffianGJframeworkinprovidinglearningguaranteesforproblems
relatedtodata-drivenalgorithmdesign. Manyparameterizedalgorithms, suchascombinatorialalgorithms
11andinteger/mixed-integerprogramming,exhibitvolatileutilityfunctionswithrespecttotheirparameters. In
otherwords,evenminorchangesinparameterscanleadtosignificantalterationsinthebehavioroftheutility
function. Analyzingsuchvolatileutilityfunctionclassesposessignificanttechnicalchallenge.
Fortunately,manydata-drivenalgorithmdesignproblemsstillpossessacertaindegreeofstructure. Prior
studies[BNVW17,BDSV18,BKST22,BNS24]havedemonstratedthatthedualutilityfunctionsassociated
withdata-drivenalgorithmdesignproblemsoftenexhibitapiece-wisestructure. Inotherwords,theparameter
spaceofthedualutilityfunctioncanbepartitionedintoregions,eachseparatedbydistinctboundaryfunctions.
Within each region, the dual utility function corresponds to a piece function that exhibits well-behaved
properties,suchasbeinglinear,rational,orPfaffianinnature. Buildinguponthisinsight,workbyBalcanetal.
[BDD+21]proposedaformaldefinitionofapiece-wise-structureddualutilityfunctionclassandestablisheda
generalizationguaranteeapplicabletoanydata-drivenalgorithmdesignproblemconformingtosuchstructures.
Formally, let us revisit the definition of the utility function class U = {u : X → [0,H] | a ∈ A} for
a
an parameterized algorithm, where A ⊆ Rd. This class represents functions that evaluate the performance
of the algorithm, with u : X → [0,H] denoting the utility function corresponding to parameter a. For a
a
giveninputprobleminstancex ∈ X,u (x)yieldstheperformanceevaluationofthealgorithmonx. Notably,
a
foreachinputx ∈ X,wecandefinethedualutilityfunctioncorrespondingtoxasu∗ : A → [0,H],where
x
u∗(a) := u (x)measurestheperformanceforaspecificprobleminstancexasafunctionoftheparametera.
x a
Consequently,wecanalsodefinethedualutilityfunctionclassU∗ = {u∗ : A → R | x ∈ X}. Itwasshown
x
inpriorworks[BDD+21,BKST22,BPSV22b,BNS24]thatinmanydata-drivenalgorithmdesignproblems,
everyfunctioninthedualfunctionclassU∗ adherestoaspecificpiece-wisestructure,whichcanbeprecisely
definedasfollows:
Definition7(Piece-wisedecomposable,[BDD+21]). AfunctionclassH ⊆ RY thatmapsadomainY toR
is(F,G,k)-piece-wisedecomposableforaclassG ⊆ {0,1}Y andaclassF ⊆ RY ofpiecefunctionsifthe
following holds: for every h ∈ H, there are k boundary functions g(1),...,g(k) ∈ G and a piece function
f ∈ F foreachbitvector{0,1}ksuchthatforally ∈ Y,h(y) = f (y)whereb = (g(1)(y),...,g(k)(y)) ∈
b by y
{0,1}k.
Aintuitiveillustrationofthepiece-wisestructurecanbefoundinFigure1. Roughlyspeaking,ifafunction
classsatisfiesthepiece-wisestructureasdefinedinDefinition7,thenforeachfunctioninsuchaclass,theinput
domainispartitionedintomultipleregionsbyk boundaryfunctions. Withineachregion,whichcorresponds
toak-elementbinaryvectorindicatingitspositionrelativetothek boundaryfunctions,theutilityfunction
is a well-behaved piece function. Based on this observation, Balcan et al. [BDD+21] showed that for any
algorithm,ifthedualutilityfunctionclasssatisfiesthepiece-wisestructureasdefinedinDefinition7,thenthe
pseudo-dimensionoftheutilityfunctionclassisbounded.
Theorem5.1([BDD+21]). ConsidertheutilityfunctionclassU = {u : X → [0,H] | a ∈ A}. Supposethat
a
thedualfunctionclassU∗ is(F,G,k)-piece-wisedecomposablewithboundaryfunctionsG ⊆ {0,1}A and
piecefunctionsF ⊆ RA. Thenthepseudo-dimensionofU isboundedasfollows
Pdim(U) = O((Pdim(F∗)+VCdim(G∗))log(Pdim(F∗)+VCdim(G∗))+VCdim(G∗)logk),
whereF∗ andG∗ isthedualclassofF andG,respectively.
Intuitively,Theorem5.1allowsustoboundthepseudo-dimensionoftheutilityfunctionclassU,whichisnot
well-behaved,byalternativelyanalyzingthedualboundaryandpiecefunctionclassesF∗ andG∗. However,
Bartlettetal. [BIW22]demonstratethatforcertainproblemsinwhichthepiece-wisestructureinvolvesonly
rationalfunctions,applyingTheorem5.1canyieldlooserboundscomparedtotheapproachbasedontheGJ
framework [GJ93]. Furthermore, applying Theorem 5.1 requires the non-trivial task of analyzing the dual
classesF∗ andG∗ ofpieceandboundaryfunctions. Thismightcausetrouble,suchasleadingtoloosebounds
(see [BDL20], Lemma 7) or vacuous bounds (see [BIW22], Appendix E.3). This situation arises when the
pieceandboundaryfunctionsinvolvePfaffianfunctions,motivatingtheneedtodesignarefinedapproach.
12Figure1: Anexampleoftheoriginalpiece-wisestructure(Definition7)andourproposedPfaffianpiece-wise
structure(Definition8). Inthisexample,fixedaninputx,theparameterdomainAispartitionedintosixregions
bythreeboundaryfunctionsI(g(1)(a) ≥ 0),I(g(2)(a) ≥ 0),andI(g(3)(a) ≥ 0),whereIdenotestheindicator
function. Ineachregion,theutilityfunctionu (a)correspondstoapiecefunctionf (a)inwhichthebinary
x b
vectorbencodestherelativepositionoftheregionconcerningtheboundaryfunctions. However,whatisnot
capturedbypiece-wisestructureisthat: (1)thepiecefunctionsherecanonlytakethreeanalyticalforms(each
correspondingtoacolor),and(2)allthepieceandboundaryfunctions(g(i) andf )arePfaffianfunctionsfrom
b
thesamePfaffianchain.
5.2 Arefinedpiece-wisestructurefordata-drivenalgorithmdesign
Inthissection,weproposeamorerefinedapproachtoderivelearningguaranteesfordata-drivenalgorithm
designproblemswheretheutilityfunctionsexhibitaPfaffianpiece-wisestructure. Thekeydifferencebetween
our proposed frameworks and the framework by Balcan et al. [BDD+21] is that we consider the following
additionalfactors: (1)BothpieceandboundaryfunctionsarePfaffianfunctionsfromthesamePfaffianchain,
and(2)themaximumnumberofthedistinctformsthatthepiecefunctioncanadmit. Later,wewillarguethat
byleveragingthoseextrastructures,wecangetabetterpseudo-dimensionupperboundbyalogarithmicfactor,
comparedtousingtheframeworkby[BDD+21]. ThePfaffianpiece-wisestructureisformalizedasbelow.
Definition 8 (Pfaffian piece-wise structure). A function class H ⊆ RY that maps domain Y ⊆ Rd to R is
said to be (k ,k ,q,M,∆,d) piece-wise structured if the following holds: for every h ∈ H, there are at
F G
most k boundary functionsof the
formsI(g(1)
≥
0),...,I(g(k)
≥ 0), wherek ≤ k , and apiece function
G h h G
f foreachbinaryvectorb ∈ {0,1}k suchthatforally ∈ Y, h(y) = f (y)whereb = (I(g(1) (y) ≥
h,b h,by y h
0),...,I(g(k) (y) ≥ 0)) ∈ {0,1}k. Moreover, the piece functions f can take on of at most k forms, i.e.
(cid:12) (cid:12)(cid:8)
f h,b | b
∈h {0,1}k(cid:9)(cid:12)
(cid:12) ≤ k
F,andallthepieceandboundaryfunctionh s,b arePfaffianfunctionsofdeF
greeatmost
∆overaPfaffianchainC oflengthatmostq andPfaffiandegreeatmostM.
h
An intuitive illustration of Pfaffian piece-wise structure and its comparison with the piece-wise structure
by Balcan et al. [BDD+21] can be found in Figure 1. For a data-driven algorithm design problem with
correspondingutilityfunctionclassU = {u : X → [0,H] | a ∈ A}whereA ⊆ Rd,wecanseethatifitsdual
a
utilityfunctionclassU∗ = {u∗ : A → R | x ∈ X}admitsthePfaffianpiece-wisestructureasinDefinition8,
x
thenitcanbecomputedusingthePfaffianGJalgorithm(seeFigure2foravisualization). Therefore,wecan
13) such that ,
Calculate
Calculate
where are polynomials,
Calculate
Figure 2: A demonstration of how the computation of a dual utility function satisfying Pfaffian piece-wise
structure can be described by Pfaffian GJ algorithm. Given an input x ∈ X and a threshold r ∈ R, the
functionu∗ ispiece-wisestructuredwithboundaryfunctionsg(i) (fori = 1,...k),andpiecefunctionsf
x h h,b
(b ∈ {0,1}k). Notethat,thepiecefunctionsf cantakeatmostk formsandallthepieceandboundary
h,b F
functionsarePfaffianfunctionsfromthechainC.
usetheestablishedresultsforthePfaffianGJalgorithm(Theorem5)toderivelearningguaranteesforsuch
problems. Weformalizethisclaiminthefollowingtheorem.
Theorem5.2. ConsidertheutilityfunctionclassU = {u : X → [0,H] | a ∈ A}whereA ⊆ Rd. Suppose
a
that the dual function class U∗ = {u∗ : A → [0,H] | x ∈ X} is (k ,k ,q,M,∆,d)-Pfaffian piece-wise
x F G
structured,thenthepseudo-dimensionofU isboundedasfollows
Pdim(U) ≤ d2q2+2dqlog(∆+M)+4dqlogd+2dlog∆(k +k )+16d.
F G
Remark3. ThedetailsofthedifferencesbetweenourrefinedPfaffianpiece-wisestructureandthepiece-wise
structurebyBalcanetal. [BDD+21]canbefoundinAppendixC.3. Inshort,comparedtotheframeworkby
Balcanetal. [BDD+21],ourframeworkoffers: (1)animprovedupperboundonthepseudo-dimensionbya
logarithmicfactor(TheoremC.1),and(2)amorestraightforwardmethodforproblemsadmittingaPfaffian
piece-wise structure, without invoking dual piece and boundary function classes, which are non-trivial to
analyze. Thismightleadtoloosebounds(see[BDL20],Lemma7,or[BIW22],AppendixE.3).
5.3 Analyzingviaapproximationfunctionclass
Inmanyapplications,wemightwanttoanalyzetheutilityfunctionclassU indirectlybystudyingasurrogate
utilityfunctionclassV thatis"sufficientlyclose"toU. Thereareseveralreasonsforthisapproach. Forinstance,
if we do not know the closed form of U, it becomes challenging to analyze its pseudo-dimension [BNS24].
Insuchcases,wemayconsiderasurrogatefunctionclassV withaclosed-formexpressionforconvenience.
AnotherscenarioariseswhenthefunctionclassU istoocomplex,andanalyzingasimplersurrogatefunction
classV mightyieldbetterbounds[BSV20].
In this section, we consider a scenario where we approximate the parameterized utility function over a
predefinedpartitionA ,...,A oftheparameterspaceA. Formally,apartitionP ofasetAisacollection
1 n
{A ,...,A }isacollectionofnon-emptysubsetsA ofAthatarepairwisedisjointandwhoseunionisthe
1 n i
entiresetA. Thisisaspecialcase,asforeveryprobleminstancex,thedualfunctionu∗ exhibitsthesame
x
discontinuity structure, which can be leveraged to obtain a tighter bound. The following lemma highlights
14thatifthedualfunctionv∗ exhibitsthesamediscontinuitystructureacrossallx,andwithineachregion,itis
x
aPfaffianfunctionarisingfromaPfaffianchainofboundedcomplexity, thenthefunctionclassU admitsa
refinedupperboundonitspseudo-dimension. ThedetailedproofisprovidedinAppendixC.2.
Lemma5.3. ConsiderafunctionclassV = {v : X → [0,H] | a ∈ A}whereA ⊆ Rd. Assumethereisa
a
partitionP = {A ,...,A }oftheparameterspaceAsuchthatforanyprobleminstancex ∈ X,thedual
1 n
utilityfunctionu∗ isaPfaffianfunctionofdegreeatmost∆inregionA fromaPfaffianchainC oflengthat
x i Ai
mostq andPfaffiandegreeM. Thenthepseudo-dimensionofV isupperboundedasfollows
Pdim(V) = O(q2d2+qdlog(∆+M)+qdlogd+logn).
Remark4. Essentially,Lemma5.3simplifiestheanalysisbyrestrictingthecomplexityofthePfaffianchainto
individualregionsA ,ratherthanacrosstheentirepartitionP. Thisoftensignificantlyreducesthelengthof
i
thePfaffianchain,whichisthedominanttermintheupperboundinmostcases.
6 Applications of the Pfaffian piece-wise structure framework
Inthissection,wedemonstratehowtoleverageourproposedframeworktoestablishnewdistributionallearning
guarantees,significantlyexpandingthescopeofalgorithmicfamiliesdata-drivenalgorithmdesignproblems.
6.1 Data-drivenagglomerativehierarchicalclustering
Agglomerative hierarchical clustering (AHC) [MC12] is a versatile, two-stage clustering approach widely
employed across various domains. In the first stage, data is organized into a hierarchical clustering tree,
determiningtheorderinwhichdatapointsaremergedintoclusters. Subsequently, inthesecondstage, the
clustertreeisprunedaccordingtoaspecificobjectivefunction,ofwhichsomecommonchoicesarek-mean
k-median,andk-center[Llo82,XW05],amongothers,toobtainthefinalclusters.
The first stage of AHC involves carefully designing linkage functions, which measure the similarity
betweenclustersanddeterminethepairofclusterstobemergedateachstep. Theselinkagefunctionsrequire
apairwisedistancefunctiondbetweendatapointsandcalculatethedistancebetweenclustersbasedonthe
distancesoftheirconstituentpointsinaspecificmanner. Commonlinkagefunctionsincludesingle-linkage,
complete-linkage,andaverage-linkage,withnumerousvariantsinterpolatingbetweenthesesimplefunctions
employedinpracticalapplications[ABV17,SMB+03,WNN+10]. Itisimportanttonotethatiftwolinkage
functionsgeneratethesamehierarchicalclustertree,theywillyieldthesamefinalclusters,irrespectiveofthe
objectivefunctionusedinthesubsequentpruningstage.
AlthoughlinkagefunctionsareacrucialcomponentofAHC,theyaregenerallychosenheuristicallywithout
anytheoreticalguarantees. Recently,Balcanetal. [BNVW17]proposedadata-driven,provableapproachfor
designinglinkagefunctions. Similartootherdata-drivensettings,theiranalysisoperatesundertheassumption
that there exists an unknown, application-specific distribution for clustering instances. They then provide
learningguaranteesforsomesimplefamiliesoflinkagefunctions,parameterizedbyasingleparameter,that
interpolatesamongsingle,complete,andaveragelinkage. However,theyassumethatthepairwisedistance
functiondisfixed,whereasinpractice,multipledistancefunctions,eachwithdistinctpropertiesandbenefits,
arecombinedtoachievebetterperformance[BBC+05]. SubsequentworkbyBalcanetal. [Bal20]proposes
combining multiple pairwise distance functions by jointly learning their weights and the parameters of the
linkage function. However, their analysis holds only under a strict assumption on the linkage function and
distance functions. In particular, it requires that the merge function m is 2-point-based, meaning that the
distancem (A,B)betweentwoclustersA,B onlydependsonthedistanceδ(a,b)betweentwopointsa ∈ A
δ
andb ∈ B. Moreover,thatpairofpointsmustdependonlyontheorderingofpairwisedistances,inthesense
15thatforanytwodistancesδ,δ′,andtwopairofpoints(a,b),(a′,b′) ∈ A×B,wehaveδ(a,b) ≤ δ(a′,b′)iff
δ′(a,b) ≤ δ′(a′,b′),thenm (A,B) = δ(a,b)impliesthatm (A,B) = δ′(a,b). Thatrestrictioneffectively
δ δ′
rulesoutmanymergefunctionsofinterestsandcompatibilityofdistancefunctionslearned.
In this section, we instantiate theoretical guarantees for novel data-driven AHC settings in which we
near-optimallylearntheparameterofthemergefunctionfamilyandthecombinationofmultiplepoint-wise
distance functions. Our analysis requires no restriction on the considered point-wise distances, extends to
clusterfamiliesthatarenot2-point-based,andappliestoanyobjectivefunctionsusedfortree-pruning.
6.1.1 Problemsetting
GivenasetofnpointsS ∈ Xn,whereX denotesthedatadomain,andadistancefunctionδ : X ×X → R ,
≥0
thegoalistopartitionS intoclusterssuchthattheintra-clusterdistanceisminimized,andtheinter-cluster
distanceismaximized. IntheAHCapproach,wefirstdesignalinkagefunctionm basedonδ,wherem (A,B)
δ δ
specifies the distance between two clusters A and B. The cluster tree construction algorithm starts with n
singletonclusters,eachcontainingasingledatapoint,andsuccessivelymergesthepairofclustersA,B that
minimizesthecluster-wisedistancem (A,B). Thissequenceofmergesyieldsahierarchicalclustertree,with
δ
therootcorrespondingtotheentiresetS,leafnodescorrespondingtoindividualpointsinS,andeachinternal
noderepresentingasubsetofpointsinS obtainedbymergingthepointsetscorrespondingtoitstwochild
nodes. Subsequently,theclustertreeisprunedtoobtainthefinalclustersviaadynamicprogrammingprocedure
that optimizes a chosen objective function. Common objective functions include k-means, k-medians, and
k-centers,amongothers. Importantly,givenafixedobjectivefunction,iftwolinkagefunctionsgeneratethe
sameclustertreeforagivendataset,theywillyieldthesamefinalclusters.
Asdiscussedpreviously,thepoint-wisedistancefunctionδ canbeaconvexcombinationofseveraldistance
functionschosenfromδ = {δ ,...,δ },i.e.,δ = δ =
(cid:80)L
β δ ,whereβ = (β ,...,β ) ∈ ∆(L). Here
1 L β i=1 i i 1 L
∆(L)denotesthe(L−1)-dimensionalsimplex. Thecombineddistancefunctionδ isthenusedinthelinkage
β
function. Inthiswork,weconsiderthefollowingparameterizedfamiliesoflinkagefunctions:
(cid:40) (cid:41)
(cid:18) (cid:19)1/α
M = m1,α : (A,B) (cid:55)→ min (δ(a,b))α+ max (δ(a,b))α ,α ∈ R∪{−∞,∞}\{0} ,
1 δ
a∈A,b∈B a∈A,b∈B
(cid:40)
 1/α
(cid:41)
M 2 = m2 δ,α : (A,B) (cid:55)→  |A|1
|B|
(cid:88) (δ(a,b))α  ,α ∈ R∪{−∞,∞}\{0} ,
a∈A,b∈B
(cid:40)  1/(cid:80) iαi (cid:41)
M 3 = m3 δ,α : (A,B) (cid:55)→  |A|1
|B|
(cid:88) Π i∈[L](δ i(a,b))αi ,α i ∈ R∪{−∞,∞}\{0} .
a∈A,b∈B
ThemergefunctionfamilyM interpolatesbetweensingleandcompletelinkage. TheversatilelinkageM
1 2
isanotherrichlinkagefamily,whichinterpolatesamongsingle,complete,andaveragelinkage[FG20]. The
family M is another generalization of single, complete, and average linkages that incorporates multiple
3
pairwisedistancefunctions. InM ,wedonotcombineallthegivendistancefunctionsδ = {δ ,...,δ }into
3 1 L
onebuttreatthemseparately. Precisely,ifwesetα = 1,andα = 0forallj ̸= i,wehaveaveragelinkage
i j
withrespecttothedistancefunctionδ ;ifwesetα = ∞,andα = 0forallj ̸= i,wehavecompletelinkage
i i j
withrespecttothedistancefunctionδ ;andifwesetα = −∞,andα = 0forj ̸= i,wehavethewell-known
i i j
singlelinkagewithrespecttoδ .
i
Inthedata-drivensetting,wearegivenmultipleprobleminstancesP ,...,P ,whereeachinstanceP
1 N i
consistsofasetofpointsS andasetofdistancefunctionsδ. Assumingthatthereexistsanunderlyingproblem
distributionthatrepresentsaspecificapplicationdomain,weaimtodeterminehowmanyprobleminstancesare
sufficienttolearntheparametersofthelinkagefunctionfamiliesandtheweightsofthedistancefunctions(for
16thefamiliesM andM ). Thesequestionsareequivalenttoanalyzingthepseudo-dimensionofthefollowing
1 2
classesofutilityfunctions.
Fori ∈ 1,2,letAα,β denotethealgorithmthattakesaprobleminstanceP = (S,δ)asinputandreturnsa
i
clustertreeAα,β(S,δ) ∈ T,whereT isthesetofallpossibleclustertrees,byusingtheinterpolatedmerge
i
function m1,α . Then given an objective function, for example the k-means objective, the pruning function
δ
β
µ : T → S takesasinputaclusteringtree,andreturnsak-partition{P ,...,P }ofS thatminimizessuch
k k 1 k
objectivefunction. Then,givenatargetclusterY = {C ,...,C },theperformanceofthealgorithmAα,β is
1 k i
givenbytheHammingdistancebetweentheproducedclustersµ (Aα,β(S,δ)) = {P ,...,P }andthetarget
k i 1 k
clustersY = {C ,...,C }
1 k
k
ℓ(µ (Aα,β(S,δ)),Y) = min 1 (cid:88) |C \P |.
k i σ∈S |S| i σi
k i=1
Here, the minimum is taken over the set of all permutations of {1,...,k}. We can clearly see that ℓ takes
valuein{0, 1,..., n−1,1}. However,notethatgivenanobjectivefunction,theclustertreeisequivalentto
n n
theproducedclusters. Thus,theperformanceofthealgorithmiscompletelydeterminedbytheclustertreeit
produces,andforsimplicity,wecanexpresstheperformanceofAα,β asuα,β : (S,δ) (cid:55)→ v(Aα,β(S,δ)),where
i i i
v isafunctionthatmapsaclustertreetoavaluein[0,1].
Similarly,wedenoteAα astheclustertreebuildingalgorithmthattakesP = (S,δ)astheinputandreturns
3
aclustertreeAα(S,δ)byusingthelinkagefunctionm3,α . Again,wehavethatuα : (S,δ) (cid:55)→ v(Aα(S,δ))
3 d 3 3
represents the performance of the algorithm. We consider the following function classes that measure the
performanceoftheabovealgorithmfamilies:
H = {uα,β : (S,δ) (cid:55)→ u(Aα,β(S,δ)) | α ∈ R∪{−∞,+∞},β ∈ ∆([L])},
1 1 1
H = {uα,β : (S,δ) (cid:55)→ u(Aα,β(S,δ)) | α ∈ R∪{−∞,+∞},β ∈ ∆([L])},
2 2 2
H = {uα : (S,δ) (cid:55)→ u(Aα(S,δ)) | α ∈ R∪{−∞,∞}\{0}}.
3 3 3 i
Inthenextsection,weanalyzethepseudo-dimensionofthefunctionclassesdescribedabove,whichprovides
insightsintothenumberofprobleminstancesrequiredtolearnnear-optimalAHCparametersforaspecific
applicationdomain.
6.1.2 Generalizationguaranteesfordata-drivenhierarchicalclustering
Inthissection,wewillleverageourproposedframework(Theorem5.2)toestablishtheupperboundsforthe
pseudo-dimensionofH describedabove.
1
Theorem6.1. LetH beaclassoffunctions
1
H = {uα,β : (S,δ) (cid:55)→ u(Aα,β(S,δ)) | α ∈ R∪{−∞,+∞},β ∈ ∆([L])}
1 1 1
mapping clustering instances (S,δ)) to [0,1] by using merge functions from class M and an arbitrary
1
objectivefunction. ThenPdim(H ) = O(n4L2).
1
Proof. The high-level idea is to show that the utility function exhibits a piece-wise structure: its parameter
spaceispartitionedbymultipleboundaryfunctions,andwithineachregion,theclustertreeremainsunchanged,
implyingthattheutilityfunctionisconstant. Wethencharacterizethenumberandcomplexityoftheboundary
functions,whichweshowbelongtoaPfaffiansystem. Subsequently,wecanutilizeourframeworktoobtaina
boundonthepseudo-dimensionofH .
1
17Fix a problem instance (S,δ). Suppose A,B ⊆ S and A′,B′ ⊆ S are two candidate clusters at some
mergestepofthealgorithm. ThenA,B ispreferredformergingoverA′,B′ iff
(cid:18) (cid:19)1/α (cid:18) (cid:19)1/α
min (δ (a,b))α+ max (δ (a,b))α ≤ min (δ (a,b))α+ max (δ (a,b))α ,
β β β β
a∈A,b∈B a∈A,b∈B a∈A′,b∈B′ a∈A′,b∈B′
orequivalently,
(δ (a ,b ))α+(δ (a ,b ))α ≤ (δ (a′,b′))α+(δ (a′,b′))α,
β 1 1 β 2 2 β 1 1 β 2 2
where we have set a ,b = argmin (δ (a,b))α, a ,b = argmax (δ (a,b))α, a′,b′ =
1 1 a∈A,b∈B β 2 2 a∈A,b∈B β 1 1
argmin (δ (a,b))α, and a′,b′ = argmax (δ (a,b))α. For distinct choices of the points
a∈A′,b∈B′ β 2 2 a∈A′,b∈B′ β
a ,b ,a ,b ,a′,b′,a′,b′,wegetatmostn8 distinctboundaryconditionsacrosswhichthemergedecisionat
1 1 2 2 1 1 2 2
anystepofthealgorithmmaychange.
We next show that the boundary functions constitute a Pfaffian system in α,β ,...,β and bound its
1 L
complexity. For each pair of points a,b ∈ S, define f (α,β) := 1 , g (α,β) := lnδ (a,b) and
a,b δ (a,b) a,b β
β
h (α,β) := δ (a,b)α. Recall δ (a,b) = (cid:80)L β δ (a,b). Consider the chain C(α,β,f ,g ,h ) of
a,b β β i=1 i i a,b a,b a,b
lengthq = 3n2,fora,b ∈ S. WewillshowthatthesefunctionsformaPfaffianchainofPfaffiandegreeM = 2.
Indeed,wehaveforeacha,b ∈ S,
∂f ∂f
a,b = 0, a,b = −δ (a,b)f2 ,
∂α ∂β i a,b
i
∂g ∂g
a,b a,b
= 0, = δ (a,b)f ,
i a,b
∂α ∂β
i
∂h ∂g
a,b a,b
= g h , = δ (a,b)f h ,
a,b a,b i a,b a,b
∂α ∂β
i
which establishes the claim. The boundary conditions can clearly be all written in terms of the functions
{h } ,meaningthatthedegree∆ = 1. Notethatthepiecefunctionsareconstantandcanonlytakevalue
a,b a,b∈S
in{0, 1,...,1},meaningthatk = n+1. ThenweconcludethatH∗ admits(n+1,n8,3n2,2,1,L+1)-
n F 1
Pfaffian piece-wise structure. Applying Theorem 5.2 now gives that Pdim(H ) = O(n4L2 +n2LlogL+
1
Llog(n8+n+1)) = O(n4L2).
Similarly,wealsohavetheupper-boundforthepseudo-dimensionofH ,andH .
2 3
Theorem6.2. LetH beaclassoffunctions
2
H = {uα,β: (S,δ) (cid:55)→ u(Aα,β(S,δ)) | α ∈ R∪{−∞,+∞},β ∈ ∆([L])}
2 2 2
mappingclusteringinstances(S,δ)to[0,1]byusingmergefunctionsfromclassM andanarbitrarymerge
2
function. ThenPdim(H ) = O(n4L2).
2
Theorem6.3. LetH beaclassoffunctions
3
H = {uα : (S,δ) (cid:55)→ u(Aα(S,δ)) | α ∈ R∪{−∞,∞}\{0}}
3 3 3 i
mapping clustering instances (S,δ) to [0,1] by using merge functions from class M . Then Pdim(H ) =
3 3
O(n4L2).
The detailed proofs of Theorem 6.2, 6.3 for the function classes H , and H can be found in Appendix
2 3
D.1. Althoughthesefunctionclassessharethesamepseudo-dimensionupper-bound,theirstructuresdiffer,
necessitatingseparateanalysesandleadingtodistinctPfaffianpiece-wisestructures. Toshowthatthedual
functionclassesofH ,H ,andH admitPfaffianpiece-wisestructure,weanalyzethetransitioncondition
1 2 3
18on the hyperparameters when the preference for merging two candidate clusters A,B switches to merging
a different pair of clusters A′,B′ instead, at any merge step of the hierarchical clustering algorithm. The
transitionconditioncorrespondstoanequalityinvolvingPfaffianfunctionsoftheparametersαandβ. Allof
suchequationscorrespondingtoeachtuple(A,B,A′,B′) ⊂ S4 willdividetheparameterspaceintoregions,
in each of which the AHC algorithm produces the same clustering tree, leading to the same performance.
Afterthisstep,weconstructthePfaffianchainsforeachfunctioninfunctionclassesH ,H ,andH ,where
1 2 3
thedifferencenaturallyliesintheformoffunctionsinthosefunctionclasses. Wethencarefullyanalyzethe
complexitiesofthePfaffianchainofthosePfaffianfunctionstoobtaintheabovebounds.
6.2 Data-drivengraph-basedsemi-supervisedlearning
Semi-supervisedlearning[CSZ10]isalearningparadigmwherelabeleddataisscarceduetoexpensivelabeling
processes. Thisparadigmleveragesunlabeleddatainadditiontoasmallsetoflabeledsamplesforeffective
learning. Acommonsemi-supervisedlearningapproachisthegraph-basedmethod[ZG09,CSZ10],which
capturesrelationshipsbetweenlabeledandunlabeleddatausingagraph. Inthisapproach, nodesrepresent
datapoints,andedgesareconstructedbasedonthesimilaritybetweendatapointpairs,measuredbyagiven
distancefunction. Optimizingalabelingfunctiononthisgraphhelpspropagatelabelsfromthelabeleddatato
theunlabeleddata.
Alargebodyofresearchfocusesonhowtolearnsuchlabelingfunctionsgiventhegraph,includingusing
st-mincuts [BC01], optimizing harmonic objective with soft mincuts [ZGL03], label propagation [ZG02],
amongothers. However,undertheassumptionthatthegraphwelldescribestherelationshipamongstdata,itis
knownthatallalgorithmsforlearningthelabelingfunctionaboveareequivalent[ZG09]. Thisalsohighlights
theimportanceofthegraphingraph-basedsemi-supervisedlearning.
Despite its significance, the graph is usually considered given or constructed using heuristic methods
withouttheoreticalguarantees[Zhu05,ZCP04]. Recently,BalcanandSharma[BS21]proposedanoveldata-
drivenapproachforconstructingthegraph,bylearningtheparametersofthegraphkernelunderlyingthegraph
construction,fromtheprobleminstancesathand. EachprobleminstanceP consistsofsetsoflabeledLand
unlabeleddataU andadistancemetricd. Assumingthatallprobleminstancesaredrawnfromanunderlying,
potentiallyunknowndistribution,theyprovideguaranteesforlearningnear-optimalgraphkernelparameters
forsuchadistribution.
Nonetheless,theyconsideronlyasingledistancefunction,whereasinpracticalapplications,combining
multipledistancefunctions,eachwithitsuniquecharacteristics,canimprovethegraphqualityandtypically
result in better outcomes compared to utilizing a single metric [BBC+05]. In this section, we consider a
novel and more practical setting for data-driven graph-based semi-supervised learning, where we learn the
parametersofthecommonly-usedGaussianRBFkernelw (u,v) = exp(−δ(u,v)/σ2)andtheweightsβ of
σ,β
δ =
(cid:80)L
β δ isacombinationmultipledistancefunctionsforconstructingthegraph.
i=1 i i
6.2.1 Problemsettings
Inthegraph-basedsemi-supervisedlearningwithGaussianRBFkernel,wearegivenafewlabeledsamples
L ⊂ X ×Y,alargenumberofunlabeledpointsU ⊂ X,andasetofdistancefunctionsδ = δ ,...,δ ,where
1 L
δ : X ×X → R≥ 0fori ∈ [L]. Here,X denotesthedataspace,andY = 0,1denotesthebinaryclassification
i
label space. To extrapolate labels from L to U, a graph Gσ,β is constructed with the node set L ∪ U and
edgesweightedbytheGaussianRBFgraphkernelw (u,v) = exp(−d(u,v)/σ2),whereσ isthebandwidth
σ,β
parameter,andδ =
(cid:80)L
β δ isaconvexcombinationofthegivendistancefunctions. Afterconstructingthe
i=1 i i
graphGσ,β,apopulargraphlabelingalgorithmcalledtheharmonicmethod[ZGL03]isemployed. Itassigns
softlabelsbyminimizingthefollowingquadraticobjective:
191 (cid:88)
w(u,v)(f(u)−f(v))2 = fT(D−W)f,
2
u,v
where f ∈ [0,1]n, n = |U|+|L|, W denotes the graph adjacency matrix W = w (u,v), and D is the
uv α,β
(cid:80)
diagonalmatrixwithentriesD = W . Thefinalpredictionsareobtainedbyroundingf foru ∈ U,i.e.
ii j ij u
predictingI{f ≥ 1},denotedbyGσ,β(L,U,δ). Letvσ,β : (L,U,δ) (cid:55)→ [0,1]denotetheaverage0-1binary
u 2
classificationlossofthepredictionsoftheabovealgorithmwhenthegraphisbuiltwithparametersσ,β.
In the data-driven setting, we are given multiple problem instances P ,...,P , each P is represented
1 i i
as a tuple (L ,U ,δ ). Assuming that there is an underlying problem distribution, that represents a specific
i i i
applicationdomain,wewanttoknowhowmanyproblemsinstanceissufficienttolearnthebestparameters
α,β thatisnear-optimalforsuchproblemdistribution. Todothat,wewanttoanalyzethepseudo-dimensionof
thefollowingfunctionclass:
G = {vσ,β : (L,U,δ) (cid:55)→ v(Gσ,β(L,U,δ)) | σ ∈ R\{0},β ∈ ∆([L])}.
6.2.2 Generalizationguaranteefordata-drivensemi-supervisedlearningwithGaussianRBFkernel
andmultipledistancefunctions
Wenowinstantiatethemainresultinthissection,whichestablishesanupperboundforthepseudo-dimension
ofthefunctionclassG.
Theorem 6.4. Let G be a class of functions mapping semi-supervised learning instances (L,U,δ) to [0,1].
ThenPdim(G) = O(n4L2),wheren = |L|+|U|isthetotalnumberofsamplesineachprobleminstance,and
L = |δ|isthenumberofdistancefunctions.
Proof. Fixaprobleminstance(L,U,δ),wewillshowthattheduallossfunctionv (σ,β) := vσ,β(L,U,δ)
L,U,δ
ispiece-wiseconstantandcharacterizethenumberandcomplexityoftheboundaryfunctionswhichwewill
showbelongtoaPfaffiansystem. Ourmainresultimpliesaboundonthepseudo-dimensionofG. Thequadratic
objectiveminimizationhasaclosed-formsolution[ZGL03],givenby
f = (D −W )−1W f ,
U UU UU UL L
wheresubscriptsU,Lrefertotheunlabeledandlabeledpoints,respectively. Heref ∈ {0,1}|L| istheground
L
truthlabelofsamplesinthelabeledsetL.
Thekeychallengenowistoanalyzetheformula(D −W )−1 ofwhichtheelementwillbeshownto
UU UU
bePfaffianfunctionsofσ,β. Recallthatw = exp(−δ (u,v)/σ2)foru,v ∈ L∪U,andD = (cid:80)n w .
u,v β ii j=1 ij
First,werecalltheidentityA−1 = adj(A) ,foranyinvertiblematrixA,whereadj(A)anddetAaretheadjoint
det(A)
anddeterminantofmatrixA,respectively. Therefore,wecanseethatanyelementof(D −W )−1 isa
UU UU
rationalfunctionofw ofdegreeatmost|U|.
u,v
Now, consider the Pfaffian chain C(σ,β, 1,w ,...,w ) with L+1 variables σ,β, and of length q =
σ 11 nn
n2+1. ToseethePfaffiandegreeofC,notethatforanypairofnodesu,v ∈ U ∪L,wehave
(cid:18) (cid:19)
∂w δ (u,v) δ (u,v)
u,v = − i exp − β = −δ (u,v)g2w ,
∂β σ2 σ2 i u,v
i
(cid:18) (cid:19)
∂w 2δ (u,v) δ (u,v)
u,v = β exp − β = 2δ (u,v)g3w .
∂α σ3 σ2 β u,v
Thus,thePfaffianchainC hasPfaffiandegreeM = 5.
20Now,considertheduallossfunctionv (σ,β). Notethat
L,U,δ
1 (cid:88)
v (σ,β) = I(I(f ≥ 1/2) = g ),
L,U,δ u u
|U|
u∈U
where g is the ground-truth label of unlabeled node u, using for evaluation purpose. We can see that, for
u
(cid:18) (cid:19)
eachu ∈ U, I(f ≥ 1/2) = I fu(1) ≥ 1/2 = I(cid:16) f(1) ≥ 1/2f(2)(cid:17) isa boundaryfunction. Bothfunctions
u fu(2) u u
(1) (2)
f ,f arePfaffianfunctionsfromchainC andofdegree|U|. Ineachregiondeterminedbythesignvector
u u
b ,whereb = I(f ≥ 1/2)foru ∈ U,theduallossfunctionv (σ,β)isaconstantfunctions. Wecansee
U u u L,U,δ
thatthereareatmost|U|+1suchconstantfunctionsthatv (σ,β)cantake. Therefore,byDefinition8,
L,U,δ
thedualfunctionclassG∗ is(|U|+1,|U|+1,n2 +1,5,|U|)-Pfaffainpiece-wisestructured. Wecanapply
Theorem5.2toget
Pdim(G) = O(n4L2+n2Llog(|U|)+n2LlogL+Llog|U|).
Noting|U| ≤ nandsimplifyingyieldstheclaimedbound.
The solution to the harmonic objective of Zhu et al. [ZGL03] has a closed-form expresion f = (D −
U UU
W )−1W f . Hereeachelementof(D −W )−1 canbeshowntobeaPfaffianfunctionofthegraph
UU UL L UU UU
hyperparameter σ over a Pfaffian chain of length n2 +1. We then apply Theorem 5.2 to obtain the stated
pseudo-dimensionbound.
7 Online Learning
Inthissection,weintroducenewgeneraltechniquesforestablishingno-regretlearningguaranteefordata-driven
algorithmsintheonlinelearningsetting. Balcanetal. [BDP20]providedatoolforverifyingdispersionwhen
non-Lipschitzness(ordiscontinuity)occursalongrootsofpolynomialsinoneandtwodimensions,andBalcan
etal. [BS21]extendedtheresulttoalgebraicvarietiesinhigherdimensions. Here,wefurthergeneralizethe
resultstocaseswherenon-LipschitznessoccursalongPfaffianhypersurfaces.
We first establish a constant upper bound on the VC-dimension of a class of Pfaffian functions with
boundedchainlengthandPfaffiandegreewhenlabeledbyaxis-alignedsegments. Thisboundisinstrumental
inextendingtheframeworkforestablishingdispersiondevelopedinpriorwork[BDP20,BS21]. Thenecessary
backgroundfromthetheoryofPfaffianfunctionstocomprehendtheseresultsisprovidedinAppendixE.
Theorem 7.1. There is a constant K depending only on M, q, d, and p such that axis-aligned line
M,q,d,p
segmentsinRp cannotshatteranycollectionofK Pfaffianhypersurfacesconsistingoffunctionsfroma
M,q,d,p
commonchainoflengthq,degreeatmostdandPfaffiandegreeatmostM.
Proof. Let C(x,f ,...,f ), where x ∈ Rp, is a Pfaffian chain of length q and of Pfaffian degree at most
1 q
M. LetP = {P ,...,P }denoteacollectionofK PfaffianhypersurfacesP (x,f (x),...,f (x)) = 0for
1 K i 1 q
i = 1,...,K,whereeachP isaPfaffainfunctionofdegreeatmostdfromthePfaffianchainC. Wesaythata
i
subsetP′ ⊆ P ishitbyalinesegmentυ ifforanyP ∈ P′,thehypersurfaceP = 0intersectwithυ. Wesay
thatP′ ⊆ P ishitbyalineΥifforanyP ∈ P′,thereexistalinesegmentυ ⊂ Υsuchthatthehypersurface
P = 0intersectswithυ. WeseektoupperboundthenumberofsubsetsofP whichmaybehitbyaxis-aligned
linesegments. Wewillfirstconsidershatteringbylinesegmentsinafixedaxialdirectionx = x forj ∈ [p].
j
LetΥ bealinealongtheaxialdirectionx . ThesubsetsofP whichmaybehitbysegmentsalongΥ are
j j j
determinedbythepatternofintersectionsofΥ withthehypersurfacesinP. WecannowuseTheoremE.1to
j
boundthenumberofintersectionsbetweenΥ andanyhypersurfaceP = 0forP ∈ P as
j i i
R := 2q(q−1)/2d(M min{q,p}+d)q,
21using the fact that the straight line Υ is given by p−1 equations x = 0 for k ̸= j which form a Pfaffian
j k
system with P of chain length q, Pfaffian degree M, and degrees deg(P ) = d, deg(x ) = 1 for k ̸= j.
i i k
ThereforeΥ intersectshypersurfacesinP atmostKRpoints,resultinginatmostKR+1segmentsofΥ .
j j
Thus,Υ mayhitatmost(cid:0)KR+1(cid:1) = O(K22q(q−1)d2(M min{q,p}+d)2q)subsetsofP.
j 2
Wewillnowboundthenumberofdistinctsubsetsthatmaybehitastheaxis-alignedΥ isvaried(whileitis
j
stillalongthedirectionx ). DefinetheequivalencerelationL ∼ L ifthesamesequenceofhypersurfaces
j x1 x2
in P intersect L and L (in the same order, including with multiplicities). To obtain these equivalence
x1 x2
classes, we will project the hypersurfaces in P onto a hyperplane orthogonal to the x -direction. By the
j
generalizationoftheTarski-SeidenbergtheoremtoPfaffianmanifolds,wegetacollectionofsemi-Pfaffiansets
[vdD86], and these sets form a cell complex with at most
O(K(p!)2(2p(2p+q))p(M +d)qO(p3))
cells [GV01].
Thisisalsoaboundonthenumberofequivalenceclassesfortherelation∼.
Putting together, for each axis-parallel direction x , the number of distinct subsets of P hit by any line
j
segmentalongthedirectionx isatmostO(K22q(q−1)d2(M
min{q,p}+d)2qK(p!)2(2p(2p+q))p(M+d)qO(p3)).
j
Thus,forallaxis-paralleldirections,wegetthatthetotalnumberofsubsetsofP thatmaybehitisatmost
(cid:18) (cid:19)
C = O p2q(q−1)d2(M min{q,p}+d)2qK(p!)2(2p(2p+q))p+2(M +d)qO(p3) .
K
ForfixedM,q,d,p,thisgrowssub-exponentiallyinK,andthereforethereisanabsoluteconstantK
M,q,d,p
suchthatC < 2K providedK ≥ K . ThisimpliesthatacollectionofK Pfaffianhypersurfaces
K M,q,d,p M,q,d,p
cannotbeshatteredbyaxis-alignedlinesegmentsandestablishesthedesiredclaim.
We can now use the above theorem to establish the following general tool for verifying dispersion of
piece-wise-LipschitzfunctionswherethepieceboundariesaregivenbyPfaffianhypersurfaces. Afullproofof
thefollowingresultislocatedinAppendixE.
Theorem 7.2. Let l ,...,l : Rp → R be independent piece-wise L-Lipschitz functions, each having
1 T
discontinuities specified by a collection of at most K Pfaffian hypersurfaces of bounded degree d, Pfaffian
degreeM andlengthofcommonPfaffianchainq. LetLdenotethesetofaxis-alignedpathsbetweenpairs
of points in Rp, and for each s ∈ L define D(T,s) = |{1 ≤ t ≤ T | l hasadiscontinuityalongs}|. Then
√ t
wehavethatE[sup D(T,s)] ≤ sup E[D(T,s)]+O˜( T logT),wherethesoft-Onotationsuppresses
s∈L s∈L
constantsind,p,M,q.
ProofSketch. We relate the number of ways line segments can label vectors of K Pfaffian hypersurfaces
of degree d, Pfaffian degree M and common chain length q, to the VC-dimension of line segments (when
labelingPfaffianhypersurfaces),whichfromTheorem7.1isconstant. Toverifydispersion,weneedauniform-
convergenceboundonthenumberofLipschitznessviolationsbetweentheworstpairofpointsρ,ρ′ atdistance
≤ ϵ,butthedefinitionallowsustoboundtheworstrateofdiscontinutiesalonganypathbetweenρ,ρ′ ofour
choice. WecanboundtheVC-dimensionofaxisalignedsegmentsagainstPfaffianhypersurfacesofbounded
complexity,whichwillallowustoestablishdispersionbyconsideringpiece-wiseaxis-alignedpathsbetween
pointsρandρ′.
Theaboveresultreducestheproblemofverifyingdispersiontoshowingthattheexpectednumberofdisconti-
nuitiesE[D(T,s)]alonganyaxis-alignedpathsisO˜(ϵT),whichtogetherwithTheorem7.2impliesthatthe
sequenceoffunctionsis1/2-dispersed.
7.1 Onlinelearningfordata-drivenagglomerativehierarchicalclustering
Weillustratehowthiscanbeusedforlearningtheparametersonlineinagglomerativehierarchicalclustering.
WerefertoAppendixEforproofdetails.
22Theorem 7.3. Consider an adversary choosing a sequence of clustering instances where the t-th instance
hasasymmetricdistancematrixD(t) ∈ [0,R]n×n andforalli ≤ j,d(t) isκ-smooth. Forthesub-familyof
ij
(t)
M withα > α for0 < α < ∞,wehavethatthecorrespondingsequenceofutilityfunctionsu are
1 min min 1
1/2-dispersed.
ProofSketch. ByusingageneralizationoftheDescartes’ruleofsigns,wefirstshowthatthereisatmostone
positiverealsolutionfortheequation
(d(a ,b ))α+(d(a ,b ))α = (d(a′,b′))α+(d(a′,b′))α.
1 1 2 2 1 1 2 2
Wethenuseκboundednessofthedistancesd toshowthattheprobabilityofhavingazeroα∗ ∈ I inany
ij
intervalofwidthI isatmostO˜(ϵ). Usingthatwehaveatmostn8 suchboundaryfunctions,wecanconclude
thatE[D(T,s)] = O˜(n8ϵT)anduseTheorem7.2. □
We define another family of linkage clustering algorithms that uses L parameters α = (α ,...,α ) to
1 L
interpolateseveralmetricsd ,d ,...,d asfollows
1 2 L
(cid:40)  1/(cid:80) iαi (cid:41)
M 3 = m3 d,α : (A,B) (cid:55)→  |A|1
|B|
(cid:88) Π i∈[L](d i(a,b))αi | α i ∈ [α min,α max] ,
a∈A,b∈B
where0 < α < α ≤ 1. Forthisfamily,wehavethefollowingresult.
min max
Theorem7.4. Consideranadversarychoosingasequenceofclusteringinstanceswherethet-thinstancehas
asymmetricdistancematrixD(t) ∈ [0,R]n×n andforalli ≤ j,d(t) isκ-smooth. Forthefamilyofclustering
ij
(t)
algorithmsM ,wehavethatthecorrespondingsequenceofutilityfunctionsu asafunctionoftheparameter
3 3
α = (α ,...,α )are1/2-dispersed.
1 L
ProofSketch. Itissufficienttoshowdispersionforeachα keepingtheremainingparametersfixed. Thisis
i
because,thedefinitionofdispersionallowsustoconsiderintersectionsofdiscontinuitieswithaxis-aligned
paths. WLOG,assumeα ,...,α arefixed. Theboundaryfunctionsaregivenbysolutionsofexponential
2 L
equationsinα oftheform
1
1 (cid:88) 1 (cid:88)
Π (d (a,b))αi = Π (d (a′,b′))αi,
|A||B| i∈[L] i |A′||B′| i∈[L] i
a∈A,b∈B a′∈A′,b′∈B′
forA,B,A′,B′ ⊆ S. WecannowuseTheorem7.2togetherwithTheorem25of[BS21]toconcludethatthe
discontinuitiesofu(t) are 1-dispersed. □
3 2
7.2 Onlinelearningfordata-drivenregularizedlogisticregression
We consider an online learning setting for regularized logistic regression from Section D.2. The problem
instancesP =
(X(t),y(t),X(t) ,y(t)
)arenowpresentedonlineinasequenceofroundst = 1,...,T,andthe
t val val
onlinelearnerpredictstheregularizationcoefficientλ ineachround. Theregretofthelearnerisgivenby
t
T
(cid:88)
R = E h(λ ,P )−h(λ∗,P ),
T t t t
t=1
where λ∗ = argmin E(cid:80)T h(λ,P ). Our main result is to show the existence of a no-regret
λ∈[λmin,λmax] t=1 t
learnerinthissetting.
23Theorem7.5. Considertheonlinelearningproblemfortuningthelogisticregressionregularizationcoefficient
(cid:112)
λ statedabove. ThereexistsanonlinelearnerwithexpectedregretboundedbyO( T log[(λ −λ )T]).
t max min
ProofSketch. Thekeyideaistouseanappropriatesurrogatelossfunctionwhichwellapproximatesthetrue
lossfunction,butisdispersedandthereforecanbelearnedonline. Wethenconnecttheregretofthelearner
withrespecttothesurrogatelosswithitsregretw.r.t.thetrueloss.
Weconsideranϵ-gridofλvalues. Foreachroundt,toconstructthesurrogatelossfunction,wesamplea
(ϵ)
uniformlyrandompointfromeachinterval[λ +kϵ,λ +(k+1)ϵ]andcomputethesurrogatelossh (P)
min min λ
atthatpoint,andusealinearinterpolationbetweensuccessivepoints. ByTheoremD.1,thishasanerrorof
at most O(ϵ2), which implies the regret gap with the true loss is at most O(ϵ2T) when using the surrogate
function.
We show that the surrogate function is f-point-dispersed (Definition 4, [BDP20]) with f(T,r) = rT.
ϵ
(cid:112)
UsingTheorem5of[BDP20],wegetthatthereisanonlinelearnerwithregretO( T log((λ −λ )/r)+
max min
f(T,r)+Tr+ϵ2T) = O((cid:112) T log((λ −λ )/r)+ Tr +ϵ2T). Settingϵ = T−1/4 andr = T−3/4,we
max min ϵ
gettheclaimedregretupperbound. □
8 Conclusion and future directions
In this work, we introduce the Pfaffian GJ framework for the data-driven distributional learning setting,
providing learning guarantees for function classes whose computations involve the broad class of Pfaffian
functions. Additionally,weproposearefinedPfaffianpiece-wisestructurefordata-drivenalgorithmdesign
problemsandestablishimprovedlearningguaranteesforproblemsadmittingsucharefinedpiece-wisestructure.
Weapplyourframeworktoavarietyofpreviouslystudieddata-drivenalgorithmdesignproblems(including
hierarchicalclustering,graph-basedsemi-supervisedlearningandregularizedlogisticregression)undernovel
andmorechallengingsettings,whereknowntechniquesdonotyieldconcretelearningguarantees. Bycarefully
analyzingtheunderlyingPfaffianstructurefortheseproblems,weleverageourproposedframeworktoestablish
novel statistical learning guarantees. We further study the data-driven online learning setting, where we
introduceanewtoolforverifyingthedispersionproperty,applicablewhenthetransitionboundariesofthe
utilityfunctionsinvolvePfaffianfunctions.
References
[AB09] Martin Anthony and Peter L. Bartlett. Neural Network Learning: Theoretical Foundations.
CambridgeUniversityPress,USA,1stedition,2009.
[ABM08] PeterArmitage,GeoffreyBerry,andJohnNigelScottMatthews. Statisticalmethodsinmedical
research. JohnWiley&Sons,2008.
[ABV17] PranjalAwasthi,MariaFlorinaBalcan,andKonstantinVoevodski. Localalgorithmsforinterac-
tiveclustering. JournalofMachineLearningResearch,18(3):1–35,2017.
[ACC+11] NirAilon,BernardChazelle,KennethLClarkson,DingLiu,WolfgangMulzer,andCSeshadhri.
Self-improvingalgorithms. SIAMJournalonComputing,40(2):350–375,2011.
[ALN21] Nir Ailon, Omer Leibovich, and Vineet Nair. Sparse linear networks with a fixed butterfly
structure: Theoryandpractice,2021.
[Bal20] Maria-Florina Balcan. Book chapter Data-Driven Algorithm Design. In Beyond Worst Case
AnalysisofAlgorithms,T.Roughgarden(Ed).CambridgeUniversityPress,2020.
24[BBC+05] Maria-Florina Balcan, Avrim Blum, Patrick Pakyan Choi, John Lafferty, Brian Pantano, Mu-
giziRobertRwebangira,andXiaojinZhu. Personidentificationinwebcamimages: Anapplica-
tionofsemi-supervisedlearning. InICML2005WorkshoponLearningwithPartiallyClassified
TrainingData,volume2,2005.
[BBSZ23] Maria-FlorinaBalcan,AvrimBlum,DravyanshSharma,andHongyangZhang. Ananalysisof
robustness of non-Lipschitz networks. Journal of Machine Learning Research, 24(98):1–43,
2023.
[BC01] AvrimBlumandShuchiChawla. Learningfromlabeledandunlabeleddatausinggraphmincuts.
InternationalConferenceonMachineLearning,2001.
[BDD+21] Maria-Florina Balcan, Dan DeBlasio, Travis Dick, Carl Kingsford, Tuomas Sandholm, and
EllenVitercik. Howmuchdataissufficienttolearnhigh-performingalgorithms? generalization
guaranteesfordata-drivenalgorithmdesign. InProceedingsofthe53rdAnnualACMSIGACT
SymposiumonTheoryofComputing,pages919–932,2021.
[BDL20] Maria-Florina Balcan, Travis Dick, and Manuel Lang. Learning to link. In International
ConferenceonLearningRepresentation,2020.
[BDP20] Maria-FlorinaBalcan,TravisDick,andWesleyPegden.Semi-banditoptimizationinthedispersed
setting. InConferenceonUncertaintyinArtificialIntelligence,pages909–918.PMLR,2020.
[BDSV18] Maria-FlorinaBalcan,TravisDick,TuomasSandholm,andEllenVitercik. Learningtobranch.
InInternationalconferenceonmachinelearning,pages344–353.PMLR,2018.
[BDV18] Maria-Florina Balcan, Travis Dick, and Ellen Vitercik. Dispersion for data-driven algorithm
design, online learning, and private optimization. In 2018 IEEE 59th Annual Symposium on
FoundationsofComputerScience(FOCS),pages603–614.IEEE,2018.
[BDW18] Maria-FlorinaBalcan,TravisDick,andColinWhite. Data-drivenclusteringviaparameterized
Lloyd’sfamilies. AdvancesinNeuralInformationProcessingSystems,31,2018.
[BIW22] PeterBartlett, PiotrIndyk, andTalWagner. Generalizationboundsfordata-drivennumerical
linearalgebra. InConferenceonLearningTheory,pages2013–2040.PMLR,2022.
[BKST22] Maria-Florina Balcan, Mikhail Khodak, Dravyansh Sharma, and Ameet Talwalkar. Provably
tuning the ElasticNet across instances. Advances in Neural Information Processing Systems,
35:27769–27782,2022.
[BM02] PeterLBartlettandShaharMendelson. Rademacherandgaussiancomplexities: Riskbounds
andstructuralresults. JournalofMachineLearningResearch,3(Nov):463–482,2002.
[BNS24] Maria-FlorinaBalcan,AnhNguyen,andDravyanshSharma. Newboundsforhyperparameter
tuning of regression problems across instances. Advances in Neural Information Processing
Systems,36,2024.
[BNVW17] Maria-FlorinaBalcan,VaishnavhNagarajan,EllenVitercik,andColinWhite. Learning-theoretic
foundationsofalgorithmconfigurationforcombinatorialpartitioningproblems. InConference
onLearningTheory,pages213–274.PMLR,2017.
[BPSV21] Maria-FlorinaBalcan,SiddharthPrasad,TuomasSandholm,andEllenVitercik. Samplecom-
plexityoftreesearchconfiguration: Cuttingplanesandbeyond. AdvancesinNeuralInformation
ProcessingSystems,34:4015–4027,2021.
25[BPSV22a] Maria-Florina Balcan, Siddharth Prasad, Tuomas Sandholm, and Ellen Vitercik. Improved
samplecomplexityboundsforbranch-and-cut. In28thInternationalConferenceonPrinciples
andPracticeofConstraintProgramming,2022.
[BPSV22b] Maria-Florina Balcan, Siddharth Prasad, Tuomas Sandholm, and Ellen Vitercik. Structural
analysis of branch-and-cut and the learnability of Gomory mixed integer cuts. Advances in
NeuralInformationProcessingSystems,35:33890–33903,2022.
[BS21] Maria-FlorinaBalcanandDravyanshSharma. Datadrivensemi-supervisedlearning. Advances
inNeuralInformationProcessingSystems,34:14782–14794,2021.
[BS24] Maria-FlorinaBalcanandDravyanshSharma. Learningaccurateandinterpretabledecisiontrees.
ConferenceonUncertaintyinArtificialIntelligence,2024.
[BSV20] Maria-Florina Balcan, Tuomas Sandholm, and Ellen Vitercik. Refined bounds for algorithm
configuration: The knife-edge of dual class approximability. In International Conference on
MachineLearning,pages580–590.PMLR,2020.
[CKFB24] Hongyu Cheng, Sammy Khalife, Barbara Fiedorowicz, and Amitabh Basu. Data-driven al-
gorithm design using neural networks with applications to branch-and-cut. arXiv preprint
arXiv:2402.02328,2024.
[CLW16] MichaelChichignoud,JohannesLederer,andMartinJWainwright. Apracticalschemeandfast
algorithmtotunethelassowithoptimalityguarantees. JournalofMachineLearningResearch,
17(229):1–20,2016.
[CSZ10] OlivierChapelle,BernhardSchlkopf,andAlexanderZien. Semi-SupervisedLearning. TheMIT
Press,1stedition,2010.
[EIN+21] TalyaEden,PiotrIndyk,ShyamNarayanan,RonittRubinfeld,SandeepSilwal,andTalWagner.
Learning-basedsupportestimationinsublineartime. InInternationalConferenceonLearning
Representations,2021.
[FG20] AlbertoFernándezandSergioGómez. Versatilelinkage: afamilyofspace-conservingstrategies
foragglomerativehierarchicalclustering. JournalofClassification,37(3):584–597,2020.
[Gab95] AndreiGabrielov. MultiplicitiesofPfaffianintersections,andthełojasiewiczinequality. Selecta
Mathematica,1:113–127,1995.
[GJ93] Paul Goldberg and Mark Jerrum. Bounding the Vapnik-Chervonenkis dimension of concept
classes parameterized by real numbers. In Proceedings of the sixth annual conference on
Computationallearningtheory,pages361–369,1993.
[GR16] RishiGuptaandTimRoughgarden. APACapproachtoapplication-specificalgorithmselection.
InProceedingsofthe2016ACMConferenceonInnovationsinTheoreticalComputerScience,
pages123–134,2016.
[GV01] AndreiGabrielovandNicolaiVorobjov.Complexityofcylindricaldecompositionsofsub-pfaffian
sets. JournalofPureandAppliedAlgebra,164(1-2):179–197,2001.
[HT11] Pentti Haukkanen and Timo Tossavainen. A generalization of Descartes’ rule of signs and
fundamentaltheoremofalgebra. AppliedMathematicsandComputation,218(4):1203–1207,
2011.
26[IMTMR22] PiotrIndyk,FrederikMallmann-Trenn,SlobodanMitrovic,andRonittRubinfeld. Onlinepage
migrationwithmladvice. InInternationalConferenceonArtificialIntelligenceandStatistics,
pages1655–1670.PMLR,2022.
[IVY19] PiotrIndyk,AliVakilian,andYangYuan. Learning-basedlow-rankapproximations. Advances
inNeuralInformationProcessingSystems,32,2019.
[IWW21] Piotr Indyk, Tal Wagner, and David Woodruff. Few-shot data-driven algorithms for low rank
approximation. AdvancesinNeuralInformationProcessingSystems,34:10678–10690,2021.
[Jam06] GrahamJOJameson. Countingzerosofgeneralisedpolynomials: Descartes’ruleofsignsand
Laguerre’sextensions. TheMathematicalGazette,90(518):223–234,2006.
[JWH+13] Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, et al. An introduction to
statisticallearning,volume112. Springer,2013.
[KBTV22] MikhailKhodak, Maria-FlorinaBalcan, Ameet Talwalkar, andSergei Vassilvitskii. Learning
predictionsforalgorithmswithpredictions. AdvancesinNeuralInformationProcessingSystems,
35:3542–3555,2022.
[Kho91] AskoldGKhovanski. Fewnomials,volume88. AmericanMathematicalSoc.,1991.
[KM97] MarekKarpinskiandAngusMacintyre. PolynomialboundsforVCdimensionofsigmoidaland
general Pfaffian neural networks. Journal of Computer and System Sciences, 54(1):169–176,
1997.
[LB11] Gordon S Linoff and Michael JA Berry. Data mining techniques: for marketing, sales, and
customerrelationshipmanagement. JohnWiley&Sons,2011.
[LGM+20] Ilay Luz, Meirav Galun, Haggai Maron, Ronen Basri, and Irad Yavneh. Learning algebraic
multigridusinggraphneuralnetworks,2020.
[LLL+23] YiLi, HonghaoLin, SiminLiu, AliVakilian, andDavidWoodruff. Learningthepositionsin
countsketch. InTheEleventhInternationalConferenceonLearningRepresentations,2023.
[Llo82] Stuart Lloyd. Least squares quantization in PCM. IEEE transactions on information theory,
28(2):129–137,1982.
[LMRX20] ThomasLavastida,BenjaminMoseley,RRavi,andChenyangXu. Learnableandinstance-robust
predictions for online matching, flows and load balancing. arXiv preprint arXiv:2011.11743,
2020.
[LV21] ThodorisLykourisandSergeiVassilvitskii. Competitivecachingwithmachinelearnedadvice.
JournaloftheACM(JACM),68(4):1–25,2021.
[MC12] FionnMurtaghandPedroContreras. Algorithmsforhierarchicalclustering: anoverview. Wiley
InterdisciplinaryReviews: DataMiningandKnowledgeDiscovery,2(1):86–97,2012.
[MRT18] MehryarMohri,AfshinRostamizadeh,andAmeetTalwalkar. Foundationsofmachinelearning.
MITpress,2018.
[Mur12] KevinPMurphy. Machinelearning: aprobabilisticperspective. MITpress,2012.
27[MV22] MichaelMitzenmacherandSergeiVassilvitskii. Algorithmswithpredictions. Communications
oftheACM,65(7):33–35,2022.
[MW97] JohnWillardMilnorandDavidWWeaver. Topologyfromthedifferentiableviewpoint,volume21.
Princetonuniversitypress,1997.
[PD09] SanjayKumarPaleiandSamirKumarDas. Logisticregressionmodelforpredictionofrooffall
risksinbordandpillarworkingsincoalmines: Anapproach. Safetyscience,47(1):88–96,2009.
[Pol12] DavidPollard. Convergenceofstochasticprocesses. SpringerScience&BusinessMedia,2012.
[Ros04] SaharonRosset. Followingcurvedregularizedoptimizationsolutionpaths. AdvancesinNeural
InformationProcessingSystems,17,2004.
[SMB+03] MehreenSaeed,OnaizaMaqbool,HaroonAtiqueBabri,SyedZahoorHassan,andSMansoor
Sarwar. Softwareclusteringtechniquesandtheuseofcombinedalgorithm. InSeventhEuropean
ConferenceonSoftwareMaintenanceandReengineering,2003.Proceedings.,pages301–306.
IEEE,2003.
[VC74] VladimirVapnikandAlexeyChervonenkis. Theoryofpatternrecognition,1974.
[vdD86] LouvandenDries. AgeneralizationoftheTarski-Seidenbergtheorem,andsomenondefinability
results. BulletinoftheAmericanMathematicalSociety,15(2):189–193,1986.
[Wai19] Martin J Wainwright. High-dimensional statistics: A non-asymptotic viewpoint, volume 48.
Cambridgeuniversitypress,2019.
[War68] HughEWarren. Lowerboundsforapproximationbynonlinearmanifolds. Transactionsofthe
AmericanMathematicalSociety,133(1):167–178,1968.
[WNN+10] JamesRWhite,SaketNavlakha,NiranjanNagarajan,Mohammad-RezaGhodsi,CarlKingsford,
andMihaiPop. Alignmentandclusteringofphylogeneticmarkers-implicationsformicrobial
diversitystudies. BMCbioinformatics,11:1–10,2010.
[WZ20] Alexander Wei and Fred Zhang. Optimal robustness-consistency trade-offs for learning-
augmentedonlinealgorithms. AdvancesinNeuralInformationProcessingSystems,33:8042–
8053,2020.
[XW05] Rui Xu and Donald Wunsch. Survey of clustering algorithms. IEEE Transactions on neural
networks,16(3):645–678,2005.
[ZCP04] Richard Zemel and Miguel Carreira-Perpiñán. Proximity graphs for clustering and manifold
learning. Advancesinneuralinformationprocessingsystems,17,2004.
[ZG02] Xiaojin Zhu and Zoubin Ghahramani. Learning from labeled and unlabeled data with label
propagation. ProQuestnumber: informationtoallusers,2002.
[ZG09] Xiaojin Zhu and Andrew B Goldberg. Introduction to semi-supervised learning. Synthesis
lecturesonartificialintelligenceandmachinelearning,3(1):1–130,2009.
[ZGL03] XiaojinZhu,ZoubinGhahramani,andJohnDLafferty. Semi-supervisedlearningusingGaussian
fieldsandharmonicfunctions. InProceedingsofthe20thInternationalconferenceonMachine
learning(ICML-03),pages912–919,2003.
28[Zha09] TongZhang. SomesharpperformanceboundsforleastsquaresregressionwithL1regularization.
AnnalsofStatistics,2009.
[Zhu05] XiaojinZhu. Semi-supervisedlearningwithgraphs. Doctoralthesis,CarnegieMellonUniversity,
2005.
29A Preliminaries
A.1 Classicalgeneralizationresults
Forcompleteness,alongwiththedefinitionofpseudo-dimension(Definition1),wenowgiveaformaldefinition
oftheVC-dimension,whichisacounterpartofpseudo-dimensionwhenthefunctionclassisbinary-valued.
Definition9(ShatteringandVC-dimension). LetF isaclassofbinary-valuedfunctionsthattakeinputfrom
X. LetS = {x ,...,x },wesaythatS isshatteredbyF if
1 N
|{(f(x ),...,f(x )) | f ∈ F}| = 2N.
1 N
TheVC-dimensionofF,denotedVCdim(F),isthelargestsizeofthesetS thatcanbeshatteredbyF.
Rademacher complexity [BM02] is another classical learning-theoretic complexity measure for obtaining
data-dependentlearningguarantees.
Definition10(Rademachercomplexity,[BM02,Wai19]). LetF beareal-valuedfunctionclasswhichtakes
inputfromdomainX. LetS = {x ,...,x } ⊂ X beafinitesetofinputs. ThentheempiricalRademacher
1 n
complexityR (F)ofF withrespecttoS isdefinedas
S
(cid:34) (cid:12) n (cid:12)(cid:35)
Rˆ (F) = E sup(cid:12) (cid:12)(cid:88) σ f(x )(cid:12) (cid:12) .
S σ (cid:12) i i (cid:12)
f∈F(cid:12) (cid:12)
i=1
TheRademachercomplexityofF fornsamplesisthendefinedas
(cid:34) (cid:12) n (cid:12)(cid:35)
R (F) = E [Rˆ (F)] = E E sup(cid:12) (cid:12)(cid:88) σ f(x )(cid:12) (cid:12) .
n S S S σ (cid:12) i i (cid:12)
f∈F(cid:12) (cid:12)
i=1
ThefollowingclassicallemmaallowsustoboundtheRademachercomplexityofareal-valuedfunctionclass
viaitspseudo-dimension.
LemmaA.1. LetF beareal-valuedfunctionclassandboundedbyH. Thenwehave
(cid:32) (cid:114) (cid:33)
Pdim(F)
R (F) ≤ O H .
n
H
A.2 UniformconvergenceandPAC-learnability
Wenowremindclassicalnotionsinlearningtheory,whichformalizesthelearnabilityofafunctionclass.
Definition11(Uniformconvergence). LetF isareal-valuedfunctionclasswhichtakesvaluefromX,andD
isadistributionoverX. Ifforanyϵ > 0,andanyδ ∈ [0,1],thereexistsN(ϵ,δ)s.t. w.p. atleast1−δ over
thedrawofn ≥ N(ϵ,δ)samplesX ,...,X ∼ D,wehave
1 n
(cid:12) (cid:12)
n
(cid:12)1 (cid:88) (cid:12)
∆ = sup(cid:12) f(X )−E [f(X)](cid:12) ≤ ϵ,
n (cid:12)n i X∼D (cid:12)
f∈F(cid:12) (cid:12)
i=1
thenwesaythat(theempiricalprocessof)F uniformlyconverges. Thequantity∆ iscalledtheempirical
n
processoffunctionclassF.
30Thefollowingclassicalresultdemonstratestheconnectionbetweenuniformconvergenceandlearnabilitywith
anERMlearner.
Theorem A.2 ([MRT18]). If F has a uniform convergence guarantee with s(ϵ,δ) samples then it is PAC
learnablewithERMands(ϵ/2,δ)samples.
Proof. ForS = {x ,...,x },letL (f) = 1 (cid:80)n f(x ),andL (f) = E [f(X)]forf ∈ F. SinceF
1 N S n i=1 i D X∼D
isuniformconvergencewiths(ϵ,δ)samples,w.p. atleast1−δ forallf ∈ F,wehave|L (f)−L (f)|for
S D
anysetS withcardinalm ≥ s(ϵ,δ). Letf ∈ argmin L (f)bethehypothesisoutputtedbytheERM
ERM f∈F S
learner,andf∗ ∈ argmin L (f)bethebesthypothesis. Wehave
f∈F D
ϵ ϵ
L (f ) ≤ L (f )+ ≤ L (f∗)+ ≤ L (h∗)+ϵ,
D ERM S ERM S D
2 2
whichconcludestheproof.
B Omitted proofs from Section 4
Inthissection,wewillpresentbasicpropertiesofPfaffianchain/functionsandaproofforTheorem4.2.
B.1 SomefactsaboutPfaffianfunctionswithelementaryoperators
Inthissection,weformalizesomebasicpropertiesofPfaffianfunctions. Essentially,thefollowingresultssay
thatifadding/subtracting/multiplying/dividingtwoPfaffianfunctionsfromthesamePfaffianchain,weendup
withafunctionthatisalsothePfaffianfunctionfromthatPfaffianchain. Moreover,evenifthetwoPfaffian
functionsarenotfromthesamePfaffianchain,wecanstillconstructanewPfaffianchainthatcontainsboththe
functionsandthenewlycomputedfunctionaswell(bysimplycombiningthetwochains).
Fact1(Addition/Subtraction). Letg,hbePfaffianfunctionsfromthePfaffianchainC(a,η ,...,η )(a ∈ Rd).
1 q
Thenwehaveg(a)±h(a)arealsoPfaffianfunctionsfromthechainC.
Proof. Foranya ,wehave ∂ (g±h)(a) = ∂ g(a)± ∂ h(a). Since ∂g(a) and ∂h(a) arepolynomialofa
i ∂ai ∂ai ∂ai ∂ai ∂ai
andη , ∂ (g±h)(a)arealsopolynomialofaandη ,whichconcludestheproof.
i ∂ai i
Fact2(Multiplication). Letg, hbePfaffianfunctionsfromthePfaffianchainC(a,η ,...,η )(a ∈ Rd)of
1 q
lengthqandPfaffaindegreeM. Thenwehaveg(a)h(a)isaPfaffianfunctionfromthechainC′(a,η ,...,η ,g,h).
1 q
Proof. Foranya ,wehave ∂ (g(a)h(a)) = g∂h(a) +h∂g(a) ,whichisapolynomialofa,η ,...,η ,g,h.
i ∂ai ∂ai ∂ai 1 q
Fact3(Division). Letg,hbePfaffainfunctionsfromthePfaffianchainC(a,η ,...,η )(a ∈ Rd)oflengthq
q q
andPfaffiandegreeM. Assumethath(a) ̸= 0,thenwehave g(a) isaPfaffianfunctionfromthePfaffianchain
h(a)
C′(a,η ,...,η ,, 1, g).
1 q h h
(cid:16) (cid:17)
Proof. Foranya ,wehave ∂ g(a) = ∂g(a) 1 − g(a)∂h(a)2 isapolynomialofa,η ,...,η , 1, g.
i ∂ai h(a) ∂ai h(a) h(a) ∂ai 1 q h h
Fact4(Composition). LethbeaPfaffianfunctionfromthePfaffianchainC(a,η ,...,η )(a ∈ Rd),andg be
1 q
PfaffianfunctionfromthePfaffainchainC′(b,η′,...,η′ )(b ∈ R). Theng(h(a))isaPfaffianfunctionfrom
1 q′
thePfaffianchainC′′(a,η ,...,η ,h,η′(h),...,η′ (h)).
1 q 1 q′
Proof. Foranya ,wehave ∂g(h(a)) = ∂h(a)∂g(h(a)) . Notethat ∂g(h(a)) = P(h(a),η′(h(a)),...,η′ (h(a)),
i ∂ai ∂ai ∂h(a) ∂h(a) 1 q′
whereP issomepolynomial. Therefore,g(h(a))isaPfaffianfunctionfromthePfaffianchain
C′′(a,η ,...,η ,h,η′(h),...,η′ (h)).
1 q 1 q′
31B.2 ProofforTheorem4.2
In this section, we will present the proof of the Pfaffian GJ algorithm guarantee (Theorem 4.2). To begin
with,wefirstrecallsomepreliminaryresultsaboutastandardtechniqueforestablishingpseudo-dimension
upper-boundbyanalyzingthesolutionsetconnectedcomponentsbound.
B.2.1 Preliminariesontheconnectionbetweenpseudo-dimensionandsolutionsetconnectedcompo-
nentsbound
Wefirstintroducethenotionofaregularvalue. Roughlyspeaking,givenasmoothmapF : Rd → Rr,where
r ≤ d,aregularvalueϵ ∈ Rr isthepointintheimagespacesuchthatthesolutionofF(a) = ϵiswell-behaved.
Definition12(Regularvalue,[MW97]). ConsiderC∞ functionsΘ ,...,Θ : A → Rwherer ≤ d,A ⊆ Rd,
1 r
and the mapping F : A → Rr given by F(a) = (Θ (a),...,Θ (a)). Then (ϵ ,...,ϵ ) ∈ Rr is a regular
1 r 1 r
valueofFifeither: (1)F−1((ϵ ,...,ϵ )) = ∅,or(2)F−1((ϵ ,...,ϵ ))isa(d−r)-dimensionalsub-manifold
1 r 1 r
ofRd.
It is widely known that by Sard’s Lemma [MW97], the set of non-regular values of the smooth map F has
Lebesgue measure 0. Based on this definition, we now present the definition of the solution set connected
componentsbound.
Definition13(Solutionsetconnectedcomponentsbound,[KM97]). Considerfunctionsτ ,...,τ : X×A →
1 K
R, where A ⊆ Rd. Given x ,...,x , assume that τ (x ,·) : Rd → R is a C∞ function for i ∈ [K] and
1 N i j
j ∈ [N]. ForanyF : Rd → Rr,whereF(a) = (Θ (a),...,Θ (a))withΘ chosenfromtheNK functions
1 r i
τ (x ),ifthenumberofconnectedcomponentsofF−1(ϵ),whereϵisaregularvalue,isupperboundedbyB
i j
independentlyonx ,r,andϵ,thenwesaythatB isthesolutionsetconnectedcomponentsbound.
i
ThesolutionsetconnectedcomponentsboundoffersanalternativewayofanalyzingVC-dimension(orpseudo-
dimension)ofrelatedparameterizedfunctionclasses,whichisformalizedinthefollowinglemma[KM97]. We
includeahigh-levelproofsketchforcomprehensiveness.
Lemma B.1 ([KM97]). Consider the binary-valued function Φ(x,a), for x ∈ X and a ∈ Rd constructed
usingthebooleanoperatorsANDandOR,andbooleanpredicatesinoneofthetwoforms“τ(x,a) > 0”or
“τ(x,a) = 0”. Assumethatthefunctionτ(x,a)canbeoneofatmostK forms(τ ,...,τ ),whereτ (x,·)
1 K i
(i ∈ [K])isaC∞ functionofaforanyfixedx. LetC = {Φ : X → {0,1} | x ∈ Rd}whereΦ = Φ(·,a).
Φ a a
Then
VCdim(C ) ≤ 2log B+(16+2logK)d,
Φ 2
whereB isthesolutionsetconnectedcomponentsbound.
Proof Sketch. The key idea involved in proving the following lemma is a combinatorial argument due to
Warren(Theorem1,[War68])whichboundsthenumberofconnectedcomponentsinducedbyacollectionof
boundaryfunctionsby(cid:80)d
b ,whereb isthenumberofconnectedcomponentsinducedbyintersectionsof
j=0 j j
anyj functions. ThiscanbecombinedwiththesolutionsetconnectedcomponentsboundB,togetabound
(cid:80)d 2j(cid:0)NK(cid:1)
B ≤
B(cid:0)2NKe(cid:1)d
onthetotalnumberofconnectedcomponents. Theresultfollowsfromnoting
j=0 j d
2N ≤ B(cid:0)2NKe(cid:1)d iftheN instancesx ,...,x aretobeshattered. □
d 1 N
WenowrecallaresultwhichisespeciallyusefulforboundingthesolutionsetconnectedcomponentsboundB
forequationsrelatedtoPfaffianfunctions.
Lemma B.2 ([Kho91]). Let C be a Pfaffian chain of length q and Pfaffian degree M, consists of functions
f ,...,f in a ∈ Rd. Consider a non-singular system of equations Θ (a) = ··· = Θ (a) = 0 where
1 q 1 r
r ≤ d, in which Θ (a) (i ∈ [r]) is a polynomial of degree at most ∆ in the variable a and in the Pfaffian
i
32functions f ,...,f . Then the manifold of dimension k = d − r determined by this system has at most
1 q
2q(q−1)∆mSd−r[(r−d+1)S −(r−d)]q connectedcomponents,whereS = r(∆−1)+dM +1.
ThefollowingcorollaryisthedirectconsequenceofLemmaB.2.
Corollary B.3. Consider the setting as in Lemma B.1. Assume that for any fixed x, τ (x,·) (i ∈ [K]) is a
i
PfaffianfunctionfromaPfaffianchainC withlengthq andPfaffaindegreeM. Then
B ≤ 2dq(dq−1)/2∆d[(d2(∆+M)]dq.
The following lemma gives a connection between the number of sign patterns and number of connected
components.
Lemma B.4 (Section 1.8, [War68]). Given N real-valued functions h ,...,h , the number of distinct
1 N
sign patterns
(cid:12)
(cid:12){(sign(h 1(a)),...,sign(h N(a))) | a ∈
Rd}(cid:12)
(cid:12) is upper-bounded by the number of connected
componentsRd−∪ {a ∈ Rd | g (a) = 0}.
i∈[N] i
Thefollowingresultisabouttherelationbetweentheconnectedcomponentsandthesolutionsetconnected
components.
LemmaB.5(Lemma7.9,[AB09]). Let{f ,...,f }beasetofdifferentiablefunctionsthatmapfromRd → R,
1 N
withregularzero-setintersections. Foreachi,defineZ thezero-setoff : Z = {a ∈ Rd : f (a) = 0}. Then
i i i i
 
(cid:32) (cid:33)
(cid:91) (cid:88) (cid:92)
CCRd− Z i ≤ CC Z i .
i∈[N] S⊆[N] i∈S
CombiningwithDefinition13,theRHSbecomesB(cid:80)d (cid:0)N(cid:1)
≤
B(cid:0)eN(cid:1)d
forN ≥ d.
i=0 i d
B.2.2 Pfaffianformulae
WenowintroducethebuildingblockofthePfaffianGJframework,namedPfaffianformula. Roughlyspeaking,
aPfaffianformulaisabooleanformulathatincorporatesPfaffianfunctions.
Definition14(Pfaffianformulae). APfaffianformulaef : Rd → {True,False}isadisjunctivenormalform
(DNF)formulaoverbooleanpredicatesoftheformg(a,η ,...,η ) ≥ 0,whereg isaPfaffianfunctionofa
1 q
PfaffianchainC(a,η ,...,η ).
1 q
Thefollowinglemmaessentiallyclaimsthatforanyfunctionclass,ifitscomputationcanbedescribedbya
PfaffianformulaandthecorrespondingPfaffianchainexhibitsboundedcomplexities,thenthefunctionclass
alsopossessesboundedpseudo-dimension.
LemmaB.6. SupposethateachalgorithmL ∈ Lisparameterizedbya ∈ Rd. Supposethatforeveryx ∈ X
andr ∈ R,thereisaPfaffianformulaf ofaPfaffianchainC withlengthqandPfaffiandegreeM,thatgiven
x,r
L ∈ L,checkwhetherL(x) > r. Supposethatf hasatmostK distinctPfaffianfunctionsinitspredicates,
x,r
eachofdegreeatmost∆. Then,
Pdim(L) ≤ d2q2+2dqlog(∆+M)+4dqlogd+2dlog∆K +16d.
Proof. ThislemmaisadirectconsequenceofLemmaB.1,andCorollaryB.3.
33B.2.3 ProofforTheorem4.2
WearenowreadytopresenttheproofofTheorem4.2.
Theorem4.2(restated). Considerareal-valuedfunctionclassLwithdomainX,ofwhicheachalgorithm
L ∈ Lisparameterizedbya ∈ A ⊆ Rd. Supposethatforeveryx ∈ X andr ∈ R,thereisaPfaffianGJ
a
algorithmΓ ,withassociatedPfaffianchainC oflengthatmostqandPfaffiandegreeatmostM,thatgiven
x,r x,r
L ∈ L,checkwhetherL (x) > r. Moreover,assumethatvaluescomputedatintermediatestepsofΓ are
a a x,r
fromthePfaffianchainC ,eachofdegreeatmost∆;andthefunctionscomputedintheconditionalstatements
x,r
areofatmostK Pfaffianfunctions. ThenPdim(L) ≤ d2q2+2dqlog(∆+M)+4dqlogd+2dlog∆K+16d.
Proof. WecantransformsthecomputationofT toaPfaffianformulabyconvertingtheconditionalstatements
x,r
intobooleancombinationsofPfaffianformulae
A ← ifv ≥ 0 : X elseY ⇒ A ← ((v ≥ 0)∧X)∨(¬(v ≥ 0)∧Y)
wherev,X,andY arevaluescomputedbyPfaffianformulae. Thus,wecanapplyLemmaB.6toobtainthe
desiredresultPdim(L) ≤ d2q2+2dqlog(∆+M)+4dqlogd+2dlog∆K +16d.
C Additional details and omitted proof for Section 5
C.1 OmittedproofforSection5.2
Lemma5.2(restated). ConsidertheutilityfunctionclassU = {u : X → [0,H] | a ∈ A}whereA ⊆ Rd.
a
SupposethatthedualfunctionclassU∗ = {u∗ : A → R | x ∈ X}is(k ,k ,q,M,∆,d)-Pfaffianpiece-wise
x F G
structured,thenthepseudo-dimensionofU isboundedasfollows
Pdim(U) ≤ d2q2+2dqlog(∆+M)+4dqlogd+2dlog∆(k +k )+16d.
F G
Proof. AnintuitiveexplanationofthistheoremcanbefoundatFigure2. SinceU∗admits(k ,k ,q,M,∆,d)-
F G
Pfaffianpiece-wisestructure,thenforanyprobleminstancex ∈ X correspondingtothedualutilityfunctionu∗,
x
thereareatmostI(g(1)
≥
0),...,I(g(k)
≥ 0)wherek ≤ k ,andapiecefunctionf foreachbinaryvector
x x G x,b
b ∈ {0,1}k such that for all a ∈ A, u∗(a) = f (a), where b = (I(g(1) (a) ≥ 0),...,I(g(k) (a) ≥ 0)).
x x,ba a x x
Therefore,foranyprobleminstancexandrealthresholdr ∈ R,thebooleanvalueofu∗(a)−r ≥ 0forany
x
a ∈ AcanbecalculatedanalgorithmΓ describedasfollow: firstcalculatingthebooleanvectorb ,and
x,r a
thencalculatethebooleanvaluef (a)−r ≥ 0.
x,ba
(i)
Fromassumption,notethatg andf areaPfaffianfunctionsofdegreeatmost∆fromPfaffianchainC
x x,b x
oflengthatmostq andPfaffiandegreeatmostM. ThismeansthatΓ isaPfaffianGJalgorithm. Therefore,
x,r
combiningwithTheorem4.2,wehavetheaboveclaim.
C.2 OmittedproofsforSection5.3
Lemma5.3(restated). ConsiderafunctionclassV = {v : X → R | a ∈ A}whereA ⊆ Rd. Assumethere
a
isapartitionP = {A ,...,A }oftheparameterspaceAsuchthatforanyprobleminstancex ∈ X, the
1 n
dualutilityfunctionu∗ isaPfaffianfunctionofdegreeatmost∆inregionA fromaPfaffianchainC of
x i Ai
lengthatmostq andPfaffiandegreeM. Thenthepseudo-dimensionofV isupper-boundedasfollows
Pdim(V) = O(q2d2+qdlog(∆+M)+qdlogd+logn).
34Proof. ConsiderN probleminstancesx ,...,x withN correspondingthresholdsτ ,...,τ ,wefirstwantto
1 N 1 N
boundthenumberofdistinctsignpatternsΠ (N) = |{(sign(v (x )−τ ),...,sign(v (x )−τ )) | a ∈ A}|.
V a 1 1 a N N
Denote ΠAi(N) = |{(sign(v (x ) − τ ),...,sign(v (x ) − τ )) | a ∈ A }|, we have Π (N) ≤
V a 1 1 a N N i V
(cid:80)n ΠAi(N). Foranyi ∈ [n],fromtheassumptions,Lemma5.2andusingSauer’sLemma,wehave
i=1 V
(cid:18) eN(cid:19)S
Π (N) ≤ ,
V
S
whereS = C(q2d2+qdlog(∆+M)+qdlogd)forsomeconstantC. ThereforeΠ (N) ≤ n(cid:0)eN(cid:1)S . Solving
V S
theinequality2N ≤ Π (N),weconcludethatPdim(V) = O(q2d2+qdlog(∆+M)+qdlogd+logn)as
V
expected.
C.3 Adetailedcomparisonbetweenthepiece-wisestructurebyBalcanetal.[BDD+21]and
ourrefinedPfaffianpiece-wisestructure
Inthissection,wewilldoadetailedcomparisonbetweenourrefinedPfaffianpiece-wisestructure,andthe
piece-wisestructureproposedbyBalcanetal.[BDD+21]. First,wederiveconcretepseudo-dimensionbounds
impliedby[BDD+21]whentherefinedPfaffianpiece-wisestructureispresent. Wenotethatthisimplication
isnotimmediateandneedsacarefulargumenttoboundthelearning-theoreticcomplexityofPfaffianpieceand
boundaryfunctionswhichappearintheirbounds.
Theorem C.1. Consider the utility function class U = {u : X → R | a ∈ A}, where A ⊆ Rd. Suppose
a
that U∗ admits (k ,k ,q,M,∆,d)-Pfaffian piece-wise structure. Then using Theorem 5.1 by Balcan et al.
F G
[BDD+21],wehave
Pdim(U) = O((d2q2+dqlog(∆+M)+dqlogd+d)·log[(d2q2+dqlog(∆+M)+dqlogd+d)k ]).
G
Proof. ConsideranutilityfunctionclassU = {u : X → R | a ∈ A}, whereA ⊆ Rd, weassumethatU∗
a
admits(k ,k ,q,M,∆,d)-Pfaffianpiece-wisestructure. Then,U∗alsoadmits(F,G,k )piece-wisestructure
F G G
followingDefinition7. Here,HisthesetofPfaffianfunctionsofdegree∆fromaPfaffianchainoflengthq
andPfaffiandegreeM,andG isthesetofthresholdfunctionswhicharealsoPfaffianfunctionsofdegree∆
fromthesamePfaffianchainoflengthq andPfaffiandegreeM.
ThefirstchallengeinusingtheframeworkofBalcanetal. isthatitonlyreducestheproblemofbounding
thelearning-theoreticcomplexityofthepiecewise-structuredutilityfunctiontothatofboundingthecomplexity
ofthepieceandboundaryfunctionsinvolved,whichisnon-trivialinthecaseofPfaffianfunctions. Thatis,
we still need to bound Pdim(F∗) and VCdim(G∗), where F∗ and G∗ are dual function classes of F and G,
respectively. ToboundVCdim(G∗),wefirstconsiderthesetofN inputinstancesS = {g ,...,g } ⊂ G,and
1 N
boundthenumberofdistinctsignpatterns,
Γ (N) = |{(g∗(g ),...,g∗(g )) | a ∈ A}| = |{(g (a),...,g (a)) | a ∈ A}|.
S a 1 a N 1 N
FromLemmaB.5andB.4,andB.2,wecanboundΓ (N)as
S
(cid:18) eN(cid:19)d
Γ (N) = O(2dq(dq−1)/2∆d[(d2(∆+M)])dq .
S
d
Solving the inequality 2N = O(2dq(dq−1)/2∆d[(d2(∆+M)]dq(cid:0)ek(cid:1)d ), we have VCdim(G∗) ≤ dq(dq−1) +
d 2
dlog∆+dqlog(d2(∆+M)).
35We now bound Pdim(F∗). By definition, we have F∗ = {f∗ : F → R | a ∈ A}. Again, to bound
a
Pdim(F∗),wefirstconsiderthesetS = {f ,...,f } ⊂ F andasetofthresholdsT = {r ,...,r },andwe
1 N 1 N
wanttoboundthenumberofdistinctsignpatterns
Γ (N) = |{(sign(f∗(f )−r ),...,sign(f∗(f )−r )) | a ∈ A}|
S,T a 1 1 a N N
= |{(sign(f (a)−r ),...,sign(f (a)−r )) | a ∈ A}|.
1 1 N N
UsingsimilarargumentasforG∗,wehavePdim(F∗) = O(d2q2+dqlog(∆+M)+dqlogd+d). Combining
withTheorem3.3byBalcanetal. [BDD+21],weconcludethat
Pdim(U) = O((d2q2+dqlog(∆+M)+dqlogd+d)log[(d2q2+dqlog(∆+M)+dqlogd+d)k ]).
G
Remark5. ComparedtoourboundsinTheorem5.2,TheoremC.1(impliedby[BDD+21])hastwonotable
differences. First,ourboundissharperbyalogarithmicfactorO(log(d2q2+dqlog(∆+M)+dqlogd+d)),
whichisaconsequenceofusingasharperformoftheSauer-Shelahlemma. Second,wehaveadependence
onalogarithmictermk whichisasymptoticallydominated bytheotherterms, andcorrespondstobetter
F
multiplicativeconstantsthanTheoremC.1.
D Additional results and omitted proofs for Section 6
We include complete proofs for upper bounds on the learning-theoretic complexity of linkage clustering
algorithmfamiliesfromSection6.1. InSectionD.2,wewillleverageourproposedframeworkLemma5.3to
recoverarecentresultbyBalcanetal.[BNS24].
D.1 OmittedproofsforSection6.1
Theorem6.2. LetH beaclassoffunctions
2
H = {uα,β: (S,δ) (cid:55)→ u(Aα,β(S,δ)) | α ∈ R∪{−∞,+∞},β ∈ ∆([L])}
2 2 2
mappingclusteringinstances(S,δ)to[0,1]byusingmergefunctionsfromclassM andanarbitrarymerge
2
function. ThenPdim(H ) = O(n4L2).
2
Proof. Fix the clustering instance (S,δ). Suppose A,B ⊆ S and A′,B′ ⊆ S are two candidate clusters at
somemergestepofthealgorithm. ThenA,B ispreferredformergingoverA′,B′ iff
 1/α  1/α
1 (cid:88) 1 (cid:88)

|A||B|
(δ β(a,b))α  ≤ 
|A′||B′|
(δ β(a,b))α  ,
a∈A,b∈B a∈A′,b∈B′
orequivalently,
1 (cid:88) 1 (cid:88)
(δ (a,b))α ≤ (δ (a,b))α.
|A||B| β |A′||B′| β
a∈A,b∈B a∈A′,b∈B′
FordistinctchoicesofthepointsetsA,B,A′,B′,wegetatmost(cid:0)2n(cid:1)2
≤ 24n distinctboundaryconditions
2
acrosswhichthemergedecisionatanystepofthealgorithmmaychange.
We next show that the boundary functions constitute a Pfaffian system in α,β ,...,β and bound
1 L
its complexity. For each pair of points a,b ∈ S, define f (α,β) := 1 , g (α,β) := lnδ (a,b)
a,b δ (a,b) a,b β
β
36and h (α,β) := δ (a,b)α. Similar to the proof of Theorem 6.1, these functions form a Pfaffian chain
a,b β
C(α,β,f ,g ,h )fora,b ∈ S. WecanseethatC isoflengthq = 3n2 andPfaffiandegreeM = 2. The
a,b a,b a,b
boundary conditions can all be written in terms of the functions {h } , meaning k = n8 and degree
a,b a,b∈S G
∆ = 1. Again,notethatk = n+1. Therefore,H∗admits(n+1,n8,3n2,2,1,L+1)-Pfaffianpiece-wisestruc-
F 2
ture. ApplyingTheorem5.2nowgivesthatPdim(H ) = O(n4L2+n2LlogL+Llog24n) = O(n4L2).
2
Theorem6.3. LetH beaclassoffunctions
3
H = {uα : (S,δ) (cid:55)→ u(Aα(S,δ)) | α ∈ R∪{−∞,∞}\{0}}
3 3 3 i
mapping clustering instances (S,δ) to [0,1] by using merge functions from class M . Then Pdim(H ) =
3 3
O(n4L2).
Proof. Fix the clustering instance (S,δ). Suppose A,B ⊆ S and A′,B′ ⊆ S are two candidate clusters at
somemergestepofthealgorithm. ThenA,B ispreferredformergingoverA′,B′ iff

1 (cid:88)
1/(cid:80) iαi
(cid:18) 1 (cid:19)1/(cid:80) iαi

|A||B|
Π i∈[L](δ i(a,b))αi ≤ |A′||B′|Π i∈[L](δ i(a,b))αi ,
a∈A,b∈B
orequivalently,
1 (cid:88) 1 (cid:88)
Π (δ (a,b))αi ≤ Π (δ (a,b))αi.
|A||B| i∈[L] i |A′||B′| i∈[L] i
a∈A,b∈B a∈A′,b∈B′
FordistinctchoicesofthepointsetsA,B,A′,B′,wegetatmost(cid:0)2n(cid:1)2
≤ 24n distinctboundaryconditions
2
acrosswhichthemergedecisionatanystepofthealgorithmmaychange.
We next show that the boundary functions constitute a Pfaffian system in α ,...,α and bound its
1 L
complexity. For each pair of points a,b ∈ S, define h a,b(α 1,...,α L) := Π i∈[L](δ i(a,b))αi. Note that
∂h := lnδ (a,b)h (α ,...,α ), and thus these functions form a Pfaffian chain of chain length n2 and
∂αi i a,b 1 L
Pfaffiandegree1. Theboundaryconditionscanbeallwrittenintermsofthefunctions{h } ,meaning
a,b a,b∈S
thatk = 24n,and∆ = 1. Again,notethatk = n+1. ThereforeH∗admits(n+1,24n,n2,1,1,L)-Pfaffian
G F 3
piece-wisestructure. ApplyingTheoremB.6nowgivesthatPdim(H ) = O(n4L2).
2
D.2 Additionalresult: Data-drivenregularizedlogisticregression
Logisticregression[JWH+13]isafundamentalstatisticaltechniqueandwidelyutilizedclassificationmodel
withnumerousapplicationsacrossdiversedomains, includinghealthcarescreening[ABM08], marketfore-
casting [LB11], and engineering safety control [PD09], among others. To mitigate overfitting and enhance
robustness, regularizations, such as sparsity (ℓ ) and shrinkage (ℓ ) constraints, are commonly employed
1 2
in logistic regression [MRT18, Mur12]. In regularized logistic regression and regularized linear models in
general, the regularization parameters, which control the magnitude of regularization, play a crucial role
[MRT18,Mur12,JWH+13]andmustbecarefullychosen. Excessivelyhighregularizationparametersmay
leadtounderfitting,whilesettingthemtoolowmaynullifytheeffectivenessofregularization. Inpractice,a
commonstrategyforselectingregularizationparametersiscross-validation,whichisknowntolacktheoretical
guarantees,ingeneral,[Zha09,CLW16].
Recently,Balcanetal.[BNS24]proposedadata-drivenapproachforselectingregularizationparametersin
regularizedlogisticregression. Theirmethodologyconsiderseachregressionproblem,comprisingtraining
andvalidationsets,asaprobleminstancedrawnfromanunderlyingproblemdistribution. Theobjectiveisto
leveragetheavailableprobleminstancestodetermineregularizationparametersthatminimizethevalidation
37lossforanyfutureprobleminstancesampledfromthesamedistribution. Inthissection,wewillleverageour
proposedrefinedpiece-wisestructuretorecoverlearningguaranteesfortheproblemofdata-drivenlogistic
regressionconsideredby[BNS24].
D.2.1 Problemsetting
Werecallthedata-drivenregularizedlogisticregressionsettingbyBalcanetal. [BNS24]. Givenaproblem
instanceP = (X,y,X ,y ) ∈ R = Rm×p×{±1}m×Rm′×p×{±1}m′ where(X,y)and(X ,y )
val val m,p,m′ val val
arethetrainingandvalidationset,respectively,andaregularizationparameterλ ∈ [λ ,λ ],theregularized
min max
logisticregressionsolvesforthecoefficientsβˆ thatisthebestfitforthetrainingset(X,y)
(X,y)
βˆ (λ) = arg min l(β,(X,y))+λR(β),
(X,y)
β∈Rp
where l(β,(X,y)) = 1 (cid:80)m log(1+exp(−y x⊤β)) is the logistic loss, and R(β) is either ℓ (∥β_1|) or
m i=1 i i 1
ℓ (∥β∥2). Afterthat,βˆ (λ)isevaluatedbyh(λ,P)whichisthevalidationloss
2 2 (X,y)
h(λ,P) = l(βˆ (λ),(X ,y )).
(X,y) val val
Inthedata-drivensetting,weassumethatthereisanunknownproblemdistributionDoverR thatthe
m,p,m′
probleminstancesaredrawnfrom. Intuitively,theproblemdistributionDrepresentstheapplicationdomainthat
theregularizedlogisticregressionoperateson. Ourgoalistolearnthebestλ ∈ [λ ,λ ]s.t. itminimizes
min max
theexpectedvalidationlossE [h(λ,P)].
P∼D
However,theproblemdistributionD isunknown. SoinsteadwewanttominimizeE [h(λ,P)]using
P∼D
ERM:assumethatwearegivenN probleminstancesP(1),...,P(N) andweminimizetheempiricalvalidation
loss
m
(cid:88)
λˆ = argmin h(λ,P(i)).
λ∈[λ min,λmax]
i=1
The question now is: how many problem instances do we need to learn a good parameter
λˆ
so that the
performanceofE [h(λˆ,P)iscomparabletotheperformanceofbestparameterE [h(λ∗,P)],where
P∼D P∼D
λ∗ = argmin E [h(λ,P)]. Define H = {h : R → R | λ ∈ [λ ,λ ]}. The goal
λ∈[λ ,λ P∼D λ m,p,m′ + min max
min max]
aboveisequivalenttogivingalearningguaranteeforthefunctionclassH.
D.2.2 Generalizationguaranteefordata-drivenregularizedlogisticregression
Directly analyzing the learnability of H is challenging due to the unknown explicit form of h(λ,P) as a
functionofλ. Instead,Balcanetal. [BNS24]proposeanalyzingthefunctionclassHviaasurrogatevalidation
lossfunctionclassH(ϵ),whichismorewell-behavedandsufficientlyclosetoH.
ConstructingthesurrogatefunctionclassH(ϵ). ToconstructsuchasurrogatelossfunctionclassH(ϵ),for
anyprobleminstanceP = (X,y,X ,y ),Balcanetal. firstapproximatetheregularizedlogisticregression
val val
estimator βˆ(X,y)(λ) by β(ϵ)(X,y)(λ), calculated using Algorithm 1 if R(β) = ∥β∥ (or Algorithm 2 if
1
R(β) = ∥β∥ ). Intuitively,forasufficientlysmallϵ,thesearchspace[λ ,λ ]canbedividedinto λmax−λ min
2 min max ϵ
pieces. Withineachpiece,β(ϵ)(X,y)(λ)isalinearfunctionofλ,andthereexistsauniformerrorboundof
O(ϵ2)fortheapproximationerror∥β(ϵ)(X,y)(λ)−βˆ (λ)∥ (formalizedinTheoremD.1). Thesurrogate
(X,y) 2
validationlossfunctionclasscannowbedefinedas
H(ϵ) = {h(ϵ) : Π → R | λ ∈ [λ ,λ ]},
λ m,p + min max
38where
m′ (cid:32)m′ (cid:33)
h(ϵ)
(P) =
1 (cid:88)
log(1+exp(−y
x⊤β(ϵ)
(λ))) =
1
log
(cid:89)
(1+exp(−y
x⊤β(ϵ)
(λ))) ,
λ m′ i i (X,y) m′ i i (X,y)
i=1 i=1
(ϵ)
andineachregion[λ +tϵ,λ +(t+1)ϵ],β (λ)) = λa +b ,wherea ,b isdefinedinTheoremD.1.
min min (X,y) t t t t
AnalyzingH(ϵ). WecannowproceedtoanalyzeH(ϵ). Moreover,wecansimplifytheanalysisbyanalyzing
thepseudo-dimensionofG(ϵ) = {g(ϵ) : Π → R | λ λ ]},where
λ m,p ≥0 min max
m′
g(ϵ)
(P) =
(cid:89)
(1+exp(−y
x⊤β(ϵ)
(λ))).
λ i i (X,y)
i=1
FixingaprobleminstanceP andathresholdτ ∈
R,bythepropertyofβ(ϵ)
(λ)),weknowthatthecalculation
(X,y)
of sign(g(ϵ) (P)−τ) can be described by λmax−λmin boundary functions and λmax−λmin piece functions. In
λ ϵ ϵ
(ϵ)
eachinterval[λ +tϵ,λ +(t+1)ϵ],g (P)takestheform
min min λ
m′
g(ϵ) (P) = (cid:89) (1+exp(−y x⊤(λa +b ))).
λ i i t t
i=1
(ϵ)
Thisimpliesthat,ineachinterval[λ +tϵ,λ +(t+1)ϵ],g (P)isaPfaffianfunctionofdegreeatmost
min min λ
mfromaPfaffianchainC oflengthatmostmandofPfaffiandegreeatmostm. FromLemma5.3,weconclude
thatPdim(H(ϵ)) = Pdim(G(ϵ)) = O(m2+log(1/ϵ)).
RecovertheguaranteeforH. Notethatwejusthavethelearningguaranteefortheapproximationfunction
classH(ϵ). TorecoverthelearningguaranteeforH,weneedtoleveragetheapproximationerrorguarantee
betweenHandH(ϵ) andthefollowingtheorem.
Algorithm1ApproximateincrementalquadraticalgorithmforRLRwithℓ penalty,[Ros04]
1
Setβ(ϵ) = βˆ (λ ),t = 0,smallconstantδ ∈ R ,andA = {j | [βˆ (λ )] ̸= 0}.
0 (X,y) min >0 (X,y) min j
whileλ < λ do
t max
λ = λ +ϵ
t+1 t
(cid:16) (cid:17) (cid:16) (cid:17) (cid:104) (cid:16) (cid:17) (cid:105)−1 (cid:104) (cid:16) (cid:17) (cid:16) (cid:17) (cid:105)
β(ϵ) = β(ϵ) − ∇2l β(ϵ) ,(X,y) · ∇l β(ϵ) ,(X,y) +λ sgn β(ϵ)
t+1 t t t t+1 t
A A A A A
(cid:16) (cid:17)
β(ϵ) =⃗0
t+1
−A
(ϵ)
A = A∪{j ̸= A | ∇l(β ,(X,y)) > λ }
t+1 t+1
(cid:12) (cid:12)
A = A\{j ∈ A | (cid:12)β(ϵ) (cid:12) < δ}
(cid:12) t+1,j(cid:12)
t = t+1
Theorem D.1 (Theorem 1, [Ros04], [BNS24]). Given a problem instance P = (X,y,X ,y ) ∈ Π ,
val val m,p
for small enough ϵ, if we use Algorithm 1 (2) to approximate the solution βˆ (λ) of RLR under ℓ (ℓ )
(X,y) 1 2
constraint byβ(ϵ) (λ)thenthereis auniformboundO(ϵ2)onthe error ∥βˆ (λ)−β(ϵ) (λ)∥ forany
(X,y) (X,y) (X,y) 2
λ ∈ [λ ,λ ].
min max
39Algorithm2ApproximateincrementalquadraticalgorithmforRLRwithℓ penalty,[Ros04]
2
Setβ(ϵ) = βˆ (λ ),t = 0.
0 (X,y) min
whileλ < λ do
t max
λ = λ +ϵ
t+1 t
(cid:104) (cid:16) (cid:17) (cid:105)−1 (cid:104) (cid:16) (cid:17) (cid:105)
β(ϵ)(λ) = β(ϵ) − ∇2l β(ϵ) ,(X,y) +2λ I · ∇l β(ϵ) ,(X,y) +2λ β(ϵ)
t t t+1 t t+1 t
t = t+1
Foranyλ ∈ [λ ,λ ],whereλ = λ +kϵ,theapproximatesolutionβ(ϵ)(λ)iscalculatedby
t t+1 k min
(cid:104) (cid:16) (cid:17) (cid:105)−1 (cid:104) (cid:16) (cid:17) (cid:16) (cid:17) (cid:105)
β(ϵ) (λ) = β(ϵ) − ∇2l β(ϵ) ,(X,y) · ∇l β(ϵ) ,(X,y) +λsgn β(ϵ) = a λ+b ,
(X,y) t t t t t t
A A A
ifweuseAlgorithm1forRLRunderℓ constraint,or
1
(cid:104) (cid:16) (cid:17) (cid:105)−1 (cid:104) (cid:16) (cid:17) (cid:105)
β(ϵ) (λ) = β(ϵ) − ∇2l β(ϵ) ,(X,y) +2λ I · ∇l β(ϵ) ,(X,y) +2λβ(ϵ) = a′λ+b′,
(X,y) t t t+1 t t t t
ifweuseAlgorithm2forRLRunderℓ constraint.
2
Remark 6. Compared to [BNS24], our framework provides a significant generalization. The techniques
usedforanalyzingthesurrogatelogisticlossin[BNS24]involvesboundingthecomplexityofPfaffianpiece
functions,whilethepieceboundariesofthesurrogatefunctionarenicelinearfunctions. Incontrast,inthis
work we develop general techniques that can handle Pfaffian boundary functions as well, and we recover
resultsfrom[BNS24]asaspecialcase.
LemmaD.2([BSV20]). LetF = {f | r ∈ R}andG = {g | r ∈ R}consistoffunctionmappingX to[0,1].
r r
ForanyS ⊆ X, Rˆ(F,S) ≤ Rˆ(G,S)+ 1 (cid:80) ∥f∗ −g∗∥, whereRˆ denotestheempiricalRademacher
|S| x∈S x x
complexity.
Combining with Lemma A.1, we then recover the following theorem, which establishes the learning
guaranteeforH.
TheoremD.3([BNS24]). Considertheproblemofregularizedlogisticregressionwithℓ (orℓ )regularization.
1 2
(cid:18)(cid:113) (cid:19)
LetHthevalidationlossfunctionclass,thenwehaveR (H) = O m2+log(1/ϵ) +ϵ2 ,whereR isthe
n n n
Rademachercomplexity.
E Additional background and omitted proofs for Section 7
We record here some fundamental results from the theory of Pfaffian functions (also known as Fewnomial
theory) which will be needed in establishing our online learning results. The following result is a Pfaffian
analogueofBezout’stheoremfromalgebraicgeometry,usefulinboundingthemultiplicityofintersectionsof
Pfaffianhypersurfaces.
TheoremE.1([Kho91,Gab95]). Considerasystemofequationsg (x) = ··· = g (x) = 0whereg (x) =
1 n i
P (x,f (x),...,f (x)) is a polynomial of degree at most d in x ∈ Rn and f ,...,f are a sequence of
i 1 q i 1 q
functions that constitute a Pfaffian chain of length q and Pfaffian degree at most M. Then the number of
non-degeneratesolutionsofthissystemdoesnotexceed
2q(q−1)/2d ...d (min{q,n}M +d +···+d −n+1)q.
1 n 1 n
40Below,weprovideadetailedproofofTheorem7.2.
Theorem 7.2. Let l ,...,l : Rp → R be independent piece-wise L-Lipschitz functions, each having
1 T
discontinuities specified by a collection of at most K Pfaffian hypersurfaces of bounded degree d, Pfaffian
degreeM andlengthofcommonPfaffianchainq. LetLdenotethesetofaxis-alignedpathsbetweenpairs
of points in Rp, and for each s ∈ L define D(T,s) = |{1 ≤ t ≤ T | l hasadiscontinuityalongs}|. Then
√ t
wehavethatE[sup D(T,s)] ≤ sup E[D(T,s)]+O˜( T logT),wherethesoft-Onotationsuppresses
s∈L s∈L
constantsind,p,M,q.
Proof. The proof is similar to analogous results in [BDP20, BS21]. The main difference is that instead of
relatingthenumberofwaysintervalscanlabelvectorsofdiscontinuitypointstotheVC-dimensionofintervals,
weinsteadrelatethenumberofwayslinesegmentscanlabelvectorsofK Pfaffianhypersurfacesofbounded
complexitytotheVC-dimensionoflinesegments(whenlabelingPfaffianhypersurfaces),whichfromTheorem
7.1isconstant. Toverifydispersion,weneedauniform-convergenceboundonthenumberofLipschitzfailures
betweentheworstpairofpointsα,α′ atdistance≤ ϵ,butthedefinitionallowsustoboundtheworstrateof
discontinuitiesalonganypathbetweenα,α′ ofourchoice. WecanboundtheVCdimensionofaxisaligned
segmentsagainstboundedcomplexityPfaffianhypersurfaces,whichwillallowustoestablishdispersionby
consideringpiecewiseaxis-alignedpathsbetweenpointsαandα′.
LetP denotethesetofallPfaffianhypersurfacesofdegreed,fromaPfaffianchainoflengthatmostq and
PfaffiandegreeatmostM. Forsimplicity,weassumethateveryfunctionhasitsdiscontinuitiesspecifiedbya
collectionofexactlyK Pfaffianhypersurfaces(K couldbeanupperbound,andwecouldsimplyduplicate
hypersurfacesasneededwhichdoesnotaffectourargumentbelow). Foreachfunctionl ,letπ(t) ∈ PK denote
t
theorderedtupleofPfaffianhypersurfacesinP whoseentriesarethediscontinuitylocationsofl . Thatis,l
t t
(t) (t)
hasdiscontinuitiesalong(π ,...,π ),butisotherwiseL-Lispchitz.
1 K
Foranyaxisalignedpaths,definethefunctionf : PK → {0,1}by
s
(cid:40)
1 ifforsomei ∈ [K],π intersectss,
i
f (π) =
s
0 otherwise,
where π = (π ,...,π ) ∈ PK. The sum (cid:80)T f (π(t)) counts the number of vectors (π(t) ,...,π(t) ) that
1 K t=1 s 1 K
intersectsor,equivalently,thenumberoffunctionsl ,...,l thatarenotL-Lipschitzons. WewillapplyVC-
1 T
dimensionuniformconvergenceargumentstotheclassF = {f : PK → {0,1} | sisanaxis-alignedpath}.
s
(t) (t)
Inparticular,wewillshowthatforanindependentsetofvectors(π ,...,π ),withhighprobabilitywehave
1 K
that 1 (cid:80)T f (π(t))isclosetoE[1 (cid:80)T f (π(t))]forallpathss.
T t=1 s T t=1 s
Now,Theorem7.1impliesthatVCdimensionofF isatmostK . AstandardVC-dimensionuniform
M,q,d,p
convergenceargumentfortheclassF implythatwithprobabilityatleast1−δ,forallf ∈ F
s
(cid:12) T (cid:34) T (cid:35)(cid:12) (cid:32)(cid:114) (cid:33)
(cid:12) (cid:12)1 (cid:88)
f (π(t))−E
1 (cid:88)
f (π(t))
(cid:12)
(cid:12) ≤ O
K M,q,d,p+log(1/δ)
,or
(cid:12)T s T s (cid:12) T
(cid:12) (cid:12)
t=1 t=1
(cid:12) T (cid:34) T (cid:35)(cid:12)
(cid:12) (cid:12)(cid:88)
f (π(t))−E
(cid:88)
f (π(t))
(cid:12)
(cid:12) ≤
O˜(cid:16)(cid:112)
T
log(1/δ)(cid:17)
.
(cid:12) s s (cid:12)
(cid:12) (cid:12)
t=1 t=1
NowsinceD(T,s) = (cid:80)T f (π(t)),wehaveforallsandδ,withprobabilityatleast1−δ,sup D(T,s) ≤
t=1 s √ s∈L
sup
E[D(T,s)]+O˜((cid:112)
T log(1/δ)). Takingexpectationandsettingδ = 1/ T implies
s∈L
(cid:18) (cid:19)
E[supD(T,s)] ≤ (1−δ)
supE[D(T,s)]+O˜((cid:112)
T log(1/δ)) +δ·T
s∈L s∈L
(cid:113) √ √
≤ supE[D(T,s)]+O˜( T log( T))+ T,
s∈L
41whichimpliesthedesiredbound. HerewehaveupperboundedtheexpectedregretbyT inthelowprobability
failureeventwheretheuniformconvergencedoesnothold.
E.1 Onlinelearningfordata-drivenagglomerativehierarchicalclustering
We will now present some useful lemmas for establishing Theorem 7.3. The following lemma generalizes
Lemma25of[BDP20].
LemmaE.2. LetX ,...,X beafinitecollectionofindependentrandomvariableseachhavingdensities
1 n
upperboundedbyκ. TherandomvariableY =
(cid:80)n
β X forsomefixedscalarsβ ,...,β
with(cid:80)n
β = 1
i=1 i i 1 n i=1 i
hasdensityf satisfyingf (y) ≤ κforally.
Y Y
Proof. Weproceedbyinductiononn. Firstconsidern = 1. ClearlyY = X andtheconclusionfollowsfrom
1
theassumptionthatX hasdensityupperboundedbyκ.
1
Now suppose n > 1. We have Y = β X + β X + ··· + β X = β X + (1 − β )X′, where
1 1 2 2 n n 1 1 1
X′ = 1 (cid:80)n β X . Bytheinductivehypothesis,X′ hasadensityf whichisupperboundedbyκ. Let
1−β1 i=2 i i X′
f denotethedensityofX . NotingX′ = Y−β1X1 andusingtheindependenceofX′ andX ,weget
X1 1 1−β1 1
(cid:90) ∞ (cid:18) y−β x(cid:19) (cid:90) ∞
1
f (y) = f f (x)dx ≤ κf (x)dx = κ.
Y X′
1−β
X1 X1
−∞ 1 −∞
Thiscompletestheinductionstep.
WewillnowpresentausefulalgebraiclemmaforestablishingTheorem7.3.
LemmaE.3. Ifα > 0,a,b,c > 0,andaα+bα−cα > 0,then
aαlna+bαlnb−cαlnc 1
≤ +lnmax{a,b,c}.
aα+bα−cα α
Proof. Weconsidertwocases. Firstsupposethatc > aandc > b. Wehavethat
aαlna+bαlnb−cαlnc aαlnc+bαlnc−cαlnc
≤ ≤ lnc
aα+bα−cα aα+bα−cα
inthiscase.
Nowsupposec ≤ a(thecasec ≤ bissymmetric). Observethatforα > 0,thefunctionf(x) = xαln K is
x
monotonicallyincreasingforx ≤ Ke−1/α whenK,α,x > 0. ThisimpliesforK = max{a,b}e1/α,
K K K K
cαln ≤ aαln ≤ aαln +bαln ,
c a a b
or,equivalently,
aαlna+bαlnb−cαlnc ≤ lnK(aα+bα−cα).
Since,aα+bα−cα > 0,wefurtherget
aαlna+bαlnb−cαlnc 1
≤ lnK = +lnmax{a,b}.
aα+bα−cα α
42Theorem7.3(restated). Consideranadversarychoosingasequenceofclusteringinstanceswherethet-th
instance has a symmetric distance matrix D(t) ∈ [0,R]n×n and for all i ≤ j,d(t) is κ-smooth. For the
ij
sub-family of C with α > α for 0 < α < ∞, we have that the corresponding sequence of utility
1 min min
(t)
functionsu are1/2-dispersed.
1
Proof. AsnotedintheproofofTheorem6.1,theboundariesofthepiecewise-constantdualutilityfunctions
aregivenby
(d(a ,b ))α+(d(a ,b ))α = (d(a′,b′))α+(d(a′,b′))α, (1)
1 1 2 2 1 1 2 2
forsome(notnecessarilydistinct)pointsa ,b ,a ,b ,a′,b′,a′,b′ ∈ S.
1 1 2 2 1 1 2 2
WeusethegeneralizedDescartes’ruleofsigns[Jam06,HT11],whichimpliesthatthenumberofrealzeros
inα(forwhichboundaryconditionissatisfied)isatmostthenumberofsignchangeswhenthebaseofthe
exponentsarearrangedinanascendingorder(since,thefamily{ax | a ∈ R }isDescartesadmissibleonR),
+
toconcludethattheboundaryofthelossfunctionoccursforatmostonepointα∗ ∈ Rapartfromα = 0. We
considerthefollowingdistinctcases(uptosymmetry):
• Cased (a ,b ) ≥ d (a ,b ) ≥ d (a′,b′) ≥ d (a′,b′). Thenumberofsignchangesisone,andthe
β 1 1 β 2 2 β 1 1 β 2 2
conclusionisimmediate.
• Cased (a ,b ) ≥ d (a′,b′) ≥ d (a ,b ) ≥ d (a′,b′). Theonlypossibilityisd (a ,b ) = d (a′,b′)
β 1 1 β 1 1 β 2 2 β 2 2 β 1 1 β 1 1
andd (a ,b ) = d (a′,b′). Butthen,theconditionholdsforallαandwedonotgetacriticalpoint.
β 2 2 β 2 2
This corresponds to the special case of tie-breaking when merging clusters, and we assume ties are
brokenfollowingsomearbitrarybutfixedorderingofthepairofpoints.
• Case d (a′,b′) ≥ d (a ,b ) ≥ d (a ,b ) ≥ d (a′,b′). The number of sign changes is two. Since
β 1 1 β 1 1 β 2 2 β 2 2
α = 0isasolution,thereisatmostoneα∗ ∈ R ∈ \{0}correspondingtoacriticalpoint.
Now,letϵ > 0. ConsideranintervalI = [α ,α ]withα > α suchthatthewidthα −α ≤ ϵ. Ifequation
1 2 1 min 2 1
(1)hasasolutionα∗ ∈ I (guaranteedtobeuniqueifitexists),wehavethat
(cid:16) (cid:17)1/α∗
d(a ,b ) =
(d(a′,b′))α∗ +(d(a′,b′))α∗
−(d(a ,b
))α∗
1 1 1 1 2 2 2 2
Letp denotethedensityofd(a ,b )whichisatmostκbyassumption,andletf(α∗) := (d(a′,b′))α∗ +
1 1 1 1 1
(d(a′,b′))α∗ − (d(a ,b ))α∗ and d (α∗) := (f(α∗))1/α∗ . We seek to upper bound Pr {d (α∗) | α∗ ∈
2 2 2 2 1 p1 1
I,f(α∗) ≥ 0}togetaboundontheprobabilitythatthereisacriticalpointinI.
Nowweconsidertwocases. Iff(α) ≤ ϵα forallα ∈ I,wehavethatd (α) = f(α)1/α ≤ ϵforallα ∈ I.
1
Thus,theprobabilityofhavingacriticalpointinI isO(κϵ)inthiscase.
Else,wehavethatf(α∗) > ϵα∗ > 0forsomeα∗ ∈ I. UsingTaylorseriesexpansionford (α)aroundα∗,
1
wehave
d (α∗+ϵ) = d (α∗)+d′(α∗)ϵ+O(ϵ2).
1 1 1
Thus,theset{d (α∗) | α∗ ∈ I,f(α∗) ≥ 0}iscontainedinanintervalofwidthatmost|d′(α∗)|O(ϵ),implying
1 1
aupperboundofκ|d′(α∗)|O(ϵ)ontheprobabilityofhavingacriticalpointinI.
1
Thusitissufficienttogiveaboundon|d′(α∗)|. Wehave
1
43(cid:18)
1 1
g(α∗)(cid:19)
d′(α∗) = d (α∗) − lnf(α∗)+
1 1 α∗2 α∗f(α∗)
1 (cid:18) g(α∗) (cid:19)
= −d (α∗)lnd (α∗) ,
α∗ f(α∗) 1 1
where
g(α) :=
lnd(a′,b′)(d(a′,b′))α∗ +lnd(a′,b′)(d(a′,b′))α∗
−lnd(a ,b )(d(a ,b
))α∗
.
1 1 1 1 2 2 2 2 2 2 2 2
Thus,
(cid:12) (cid:12)
(cid:12) 1 (cid:18) g(α∗) (cid:19)(cid:12)
|d′(α∗)| = (cid:12) −d (α∗)lnd (α∗) (cid:12)
1 (cid:12)α∗ f(α∗) 1 1 (cid:12)
(cid:12) (cid:12)
(cid:12) 1 (cid:12) (cid:18)(cid:12)g(α∗)(cid:12) (cid:19)
≤ (cid:12) (cid:12)· (cid:12) (cid:12)+|d (α∗)lnd (α∗)|
(cid:12)α∗(cid:12) (cid:12)f(α∗)(cid:12) 1 1
(cid:12) 1 (cid:12) (cid:18)(cid:12)g(α∗)(cid:12) (cid:19)
≤ (cid:12) (cid:12)· (cid:12) (cid:12)+RlnR .
(cid:12)α (cid:12) (cid:12)f(α∗)(cid:12)
min
NowusingLemmaE.3,wegetthat
(cid:12)g(α∗)(cid:12) 1
(cid:12) (cid:12) ≤ +lnR,
(cid:12)f(α∗)(cid:12) α∗
andthus,
(cid:18) (cid:19)
RlnR
|d′(α∗)| ≤ O .
1 α2
min
Using that we have at most n8 boundary functions of the form (1), we can conclude that E[D(T,s)] =
O˜(RlnRκn8ϵT)andusingTheorem7.2wecanconcludethatthesequenceofutilityfunctionsare 1-dispersed.
α2 2
min
ToestablishTheorem7.4,wefirstpresentausefullemma,andrestateausefulresultfrom[BS21].
Lemma E.4. Suppose X is a real-valued random variable taking values in [0,M] for some M ∈ R+ and
supposeitsprobabilitydensityisκ-bounded. Then,Y = Xα forα ∈ [α ,1]forsomeα > 0takesvalues
min min
in[0,M]withaκ′-boundeddensitywithκ′ ≤ κ max{1,Mαm1 in−1 }.
αmin
Proof. ThecumulativedensityfunctionforY isgivenby
F (y) = Pr[Y ≤ y] = Pr[X ≤ y1/α]
Y
(cid:90) y1/α
= f (x)dx,
X
0
44wheref (x)istheprobabilitydensityfunctionforX. UsingLeibniz’srule,wecanobtainthedensityfunction
X
forY as
d
f (y) = F (y)
Y Y
dy
d
(cid:90) y1/α
= f (x)dx
X
dy
0
d
≤ κ y1/α
dy
κ
=
yα1−1.
α
Nowfory ≤ 1,yα1−1 ≤ y ≤ 1andthereforef Y(y) ≤ αmκ in. Else,y ≤ M,andf Y(y) ≤ αmκ inMαm1 in−1 .
Wewillalsoneedthefollowingresultwhichisusefultoestablishdispersionwhenthediscontinuitiesofthe
lossfunctionaregivenbyrootsofanexponentialequationintheparameterwithrandomcoefficients.
TheoremE.5([BS21]). Letϕ(x) = (cid:80)n i=1a iebix bearandomfunction,suchthatcoefficientsa
i
arerealand
ofmagnitudeatmostR,anddistributedwithjointdensityatmostκ. ThenforanyintervalI ofwidthatmostϵ,
Pr(ϕhasazeroinI) ≤ O˜(ϵ)(dependenceonb ,n,κ,Rsuppressed).
i
Wewillnowre-stateandproveouronlinelearningresultforlearningtheparameterαintheclusteringlinkage
familyC .
3
Theorem7.4(restated). Consideranadversarychoosingasequenceofclusteringinstanceswherethet-th
instancehasasymmetricdistancematrixD(t) ∈ [0,R]n×n andforalli ≤ j,d(t) isκ-smooth. Forthefamily
ij
(t)
ofclusteringalgorithmsC ,wehavethatthecorrespondingsequenceofutilityfunctionsu asafunctionof
3 3
theparameterα = (α ,...,α )are1/2-dispersed.
1 L
Proof. Wewillshowdispersionforeachα keepingtheremainingparametersfixed. Thisissufficientbecause,
i
thedefinitionofdispersion[BDP20]allowsustoconsiderdiscontinuitiesalongaxis-alignedpathsbetween
pairsofpointsα,α′ intheparameterspace. WLOG,assumethatα ,...,α arefixed. Theboundaryfunctions
2 L
aregivenbysolutionsofexponentialequationsinα oftheform
1
1 (cid:88) 1 (cid:88)
Π (d (a,b))αi = Π (d (a′,b′))αi,
|A||B| i∈[L] i |A′||B′| i∈[L] i
a∈A,b∈B a′∈A′,b′∈B′
for A,B,A′,B′ ⊆ S. We can rewrite this equation in the form (cid:80)n j=1a jebjx = 0 where, x = α 1, b
j
=
ln(d 1(a,b)) for a,b ∈ A × B or a,b ∈ A′ × B′ and a
j
= |A|1 |B|Π i∈{2,...,Ld i(a,b)αi for a,b ∈ A × B or
a
j
= |A− ′||1 B′|Π i∈{2,...,L}d i(a′,b′)αi fora′,b′ ∈ A′ ×B′. Thecoefficientsa
j
arerealwithmagnitudeatmost
RL−1. ByLemmaE.4,wehavethatd i(a,b)αi isκ′-boundedforκ′ ≤
α
(cid:18)mκ
in
max{1,Rαm1 in− (cid:19)1 },andtherefore
(cid:16) (cid:17)L−1
thecoefficientsa haveaκ′′-boundeddensityforκ′′ ≤ n2κ′L−1 = O n2 κR1/αmin (usingLemma
j αmin
8from[BDV18]). UsingTheoremE.5,theprobabilitythereisadiscontinuityalonganysegmentalongthe
direction α of width ϵ is p = O˜(ϵ). Thus, for any axis-aligned path s between points α,α′ ∈ RL, the
1 1
expectednumberofdiscontinuitiesforanyutilityfunctionu(t) (t ∈ [T])isatmostLp = O˜(ϵ). Wecannow
3 1
applyTheorem7.2toget
45E[supD(T,s)] ≤
supE[D(T,s)]+O˜((cid:112)
T logT)
s∈L s∈L
=
O˜(ϵT)+O˜((cid:112)
T logT),
where L denotes the set of axis-aligned paths between pairs of points α,α′ ∈ RL. It then follows that
√
E[sup D(T,s)] = O˜( T)forϵ ≥ T−1/2,establishingthatthesequenceofutilityfunctionsu(1) ,...,u(T)
s∈L 3 3
is1/2-dispersed.
E.2 Onlinelearningfordata-drivenregularizedlogisticregression
Theorem 7.5 (restated). Consider the online learning problem for tuning the logistic regression regu-
larization coefficient λ stated above. There exists an online learner with expected regret bounded by
t
(cid:112)
O( T log[(λ −λ )T]).
max min
Proof. Weconsideranϵ-gridofλvaluesgivenbyintervals[λ +kϵ,λ +(k+1)ϵ]fork = 0,...,⌊λmax−λmin⌋.
min min ϵ
(k)
Foreachroundt,toconstructthesurrogatelossfunction,wesampleauniformlyrandompointλ fromeach
t
interval[λ +kϵ,λ +(k+1)ϵ]andcomputethesurrogatemodelβˆ (λ)atthatpointusingAlgorithm
min min (X,y)
(k) (k+1)
1 (2) for ℓ (ℓ ), and use a linear interpolation between successive points λ ,λ which are at most 2ϵ
1 2 t t
apart. ByTheoremD.1,wehavethat∥βˆ (λ)−β(ϵ) (λ)∥ = O(ϵ2)foranyλ ∈ [λ ,λ ],where
(Xt,yt) (Xt,yt) 2 min max
(ϵ)
β (λ)isthetruemodel(thatexactlyoptimizesthelogisticloss)for(X ,y ). ByLemma4.2of[BNS24]
(Xt,yt) t t
wecanconcludethatthecorrespondingsurrogatelosshˆ(ϵ) (P )alsosatisfies||hˆ(ϵ) (P )−h (P )|| ≤ O(ϵ2)for
λ t λ t λ t
anyλ ∈ [λ ,λ ].
min max
ThisimpliestheregretwithrespecttothetruelossisatmostO(ϵ2T)morethantheregretwhenusingthe
surrogatefunction. Supposeλ∗ = argmin (cid:80)T E[h (P )]. Wehave,
λ∈[λmin,λmax] t=1 λ t
T
(cid:88)
R = E[h (P )−h (P )]
T λt t λ∗ t
t=1
T T
(cid:88) (cid:88)
= E[h (P )−hˆ (P )]+ E[hˆ (P )−h (P )]
λt t λ∗ t λ∗ t λ∗ t
t=1 t=1
T
(cid:88)
≤ E[h (P )−hˆ (P )]+O(ϵ2T)
λt t λ∗ t
t=1
T
(cid:88)
≤ E[h (P )−hˆ (P )]+O(ϵ2T),
λt t λˆ t
t=1
whereλˆ = argmin (cid:80)T E[hˆ (P )].
λ∈[λmin,λmax] t=1 λ t
Wenextshowthatthesurrogatefunctionisf-point-dispersed(Definition4,[BDP20])withf(T,r) = rT.
ϵ
Indeed,let0 < r < ϵ. LetI = [λ ,λ ] ⊂ [λ ,λ ]beanintervaloflengthr. Thentheprobabilityhˆ (P )
1 2 min max λ t
hasacriticalpointinI isatmostr/ϵ.
UsingTheorem5of[BDP20]combinedwiththeaboveargument,wenowgetthatthereisanonlinelearner
withregretO((cid:112) T log((λ −λ )/r)+f(T,r)+Tr+ϵ2T) = O((cid:112) T log((λ −λ )/r)+Tr+ϵ2T).
max min max min ϵ
Settingϵ = T−1/4 andr = T−3/4,wegettheclaimedregretupperbound.
46