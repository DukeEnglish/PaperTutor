PARCO: Learning Parallel Autoregressive Policies for
Efficient Multi-Agent Combinatorial Optimization
FedericoBerto∗1,3,ChuanboHua∗1,3,LaurinLuttmann∗2,
JiwooSon3,JunyoungPark1,KyureeAhn3,
ChanghyunKwon1,3,LinXie2,4,JinkyooPark1,3
1KAIST 2LeuphanaUniversity 3OMELET 4TwenteUniversity AI4CO‡
Abstract
Multi-agent combinatorial optimization problems such as routing and schedul-
ing have great practical relevance but present challenges due to their NP-hard
combinatorial nature, hard constraints on the number of possible agents, and
hard-to-optimize objective functions. This paper introduces PARCO (Parallel
AutoRegressiveCombinatorialOptimization), anovelapproachthatlearnsfast
surrogatesolversformulti-agentcombinatorialproblemswithreinforcementlearn-
ingbyemployingparallelautoregressivedecoding. Weproposeamodelwitha
MultiplePointerMechanismtoefficientlydecodemultipledecisionssimultane-
ouslybydifferentagents,enhancedbyaPriority-basedConflictHandlingscheme.
Moreover,wedesignspecializedCommunicationLayersthatenableeffectiveagent
collaboration,thusenrichingdecision-making. WeevaluatePARCOinrepresenta-
tivemulti-agentcombinatorialproblemsinroutingandschedulinganddemonstrate
thatourlearnedsolversoffercompetitiveresultsagainstbothclassicalandneural
baselinesintermsofbothsolutionqualityandspeed. Wemakeourcodeopenly
availableathttps://github.com/ai4co/parco.
1 Introduction
Combinatorialoptimization(CO)problems,suchasroutingandschedulingproblems,involvedeter-
mininganoptimalsequenceofactionsinacombinatorialspaceandhaveapplicationsrangingfrom
warehouseoperatons(Xieetal.,2023)tonetworkdesign(Chabareketal.,2008)andsafety-critical
systems(Girardeyetal.,2010). COproblemsarenotoriouslyhardtosolveandcannotgenerallybe
solvedoptimallyinpolynomialtime,i.e.,theyareNP-hard(Jüngeretal.,1995). Multi-agentsettings
gainedsignificantinterestduetotheirapplicabilityinrealisticscenarios,suchaspathfinding(Reijnen
etal.,2020),dronerouting(Annetal.,2015),disastermanagement(Bektas,2006;Cheikhrouhou
andKhoufi,2021)andorderdelivery(YakıcıandKarasakal,2013;ArchettiandBertazzi,2021),
butpresentevenmorechallengesduetoadditionalconstraintsanddifferentoptimizationobjectives,
including minimizing a global lateness objective or the makespan (Mahmoudinazlou and Kwon,
2024).
Whiletraditionalalgorithmicmethodshavesignificantlycontributedtosolvingarangeofproblems
(Laporte and Osman, 1995; Hejazi and Saghafian, 2005), these approaches often concentrate on
single-agentscenarios. Withtheadventofmoderncomputationaltechniques,neuralnetwork-based
approacheshavestartedtoyieldpromisingresultsforcomplexCOproblemsinafieldknownas
NeuralCombinatorialOptimization(NCO).Inparticular,ReinforcementLearning(RL)hasshown
promiseduetoitsabilitytolearndirectlyfrominteractionswithenvironmentsinsteadofrelyingon
∗Equalcontribution.
‡AuthorsaremembersoftheAI4COopenresearchcommunity.
4202
peS
5
]AM.sc[
1v11830.9042:viXra(costly)labeleddatasetsintheshapeofoptimalsolutions(Belloetal.,2016;Kwonetal.,2020;Kim
etal.,2023a).
AmongNCOmethods,Autoregressive(AR)sequencegenerationhasgainedattentionforitsability
to manage hard constraints (Kool et al., 2018; Kwon et al., 2021). In the context of NCO, this
capabilityiscrucialforaddressingcomplexproblemswithmultipleconstructionconstraints,suchas
theprecedenceconstraintsprevalentinscheduling(Zhangetal.,2020a)aswellaspickupanddelivery
problems(SavelsberghandSol,1995;Parraghetal.,2008). However,oneissueofautoregressive
sequencegenerationisthehighlatencyassociatedwithit,especiallywhenconsideringlargeproblem
instances. TheproblemofhighinferencelatencyofARmethodsisespeciallyprevalentinthedomain
oflargelanguagemodels,asmultiplestacksoftransformerlayershavetobecomputedsequentially
foreachindividualtoken(Baeetal.,2023).
Motivated by recent studies on LLMs demon-
stratingparalleldecodingcannotonlytacklethe Autoregressive Policy
Action: {2} Action: {6} Solution
highgenerationlatencyproblemofnext-token 4 3Continue? 2 4 3 2
predictorsbutalsoimprovetheiranswerquality 1 1
… Back? … …
(Qietal.,2020;Ningetal.,2023;Gloeckleetal., Number of Agent: 3 5 6 Agent 1 5 6 Agent 1 Agent 3 Agent 1
2 A0 u2 to4 R) eth gi rs esp sa ivp eer Coin mtr bo id nu atc oe rs iaP lA OR ptC imO iz( aP ta iora nl )l ,e al 4 3 2 1 Step 2 … StA eg pen t 6 2 … Step A 9g e [n Dt o2 ne]
novelapproachdesignedtoaddressmulti-agent 5 Depot City Actions: {1, 4, 6}Parallel A A ctu iot no sr :e {g 2r ,e 3ss , i 5v }e Policy Solution
6
c toom apb pin lyat po ari ra al llp er lo db el ce om ds ine gffi ic nie tn ht ely N. CW Oep dr oo mpo as ine
Instance
Ag 4ent 33 2
1
Ag 4ent 3 3 2
1
Agent 3
toincreasesolutionconstructionefficiencyand 5 Agent 1 5 Agent 1 Agent 1
Communicate 6 Agent 2
effectiveness. IncontrasttomostpreviousNCO 6 Agent 2 Agent 2
methods, PARCO constructs solutions in par- Step 1 Step 2 Step 3 [Done]
allel for multiple agents, made feasible by a Figure 1: Solution construction with Autoregressive
priority-basedconflicthandlertomanagecon- (top)andParallelAutoregressivepolicies(bottom).
flicting decisions of different agents. Impor-
tantly,weenablethecollaborativebehaviorofagentsthroughspecializedinter-agentCommunication
Layers,essentialforfindinghigh-qualitysolutionstomulti-agentcombinatorialproblems.
Contributions. Wesummarizeourcontributionsasfollows:
• WeintroducePARCO,anovelapproachfortacklingmulti-agentCOproblemsbyeffectively
constructingsolutionsinparallelviaaMultiplePointerMechanism.
• WeintroduceanefficientPriority-basedConflict-Handlingschemetoresolveconflictbe-
tweenagentsandenhancedecision-making.
• WedesignaspecialCommunicationLayertohelpagentscommunicateandcollaborateto
obtainabetterstate-spacerepresentationofpartialsolutions.
• Weshowcompetitiveperformanceagainstclassicalsolversandneuralbaselinesinrouting
andschedulingproblems,bothintermsofsolutionqualityandcomputationalefficiency.
2 RelatedWork
NeuralCombinatorialOptimization RecentadvancementsinNCOhaveshownpromisingend-
to-endsolutionsforcombinatorialoptimizationproblems,ashighlightedbyBengioetal.(2021)and
YangandWhinston(2023). NCOhasledtothedevelopmentofalignedneuralarchitectures(Bietal.,
2022;Jinetal.,2023;Luoetal.,2023;Kimetal.,2023c),hybridmethodswithORsolvers(Lietal.,
2021b;Kimetal.,2024a;YanandWu,2024;Yeetal.,2024a;Kimetal.,2024b),multi-levelsolution
pipelines(Maetal.,2023a;Lietal.,2023;Xiaoetal.,2023;Yeetal.,2023),alongsideimproved
trainingalgorithms(Kimetal.,2023c;Jiangetal.,2023;Drakulicetal.,2023;SunandYang,2023;
Gaoetal.,2023;Xiaoetal.,2023;Lietal.,2024b;Wangetal.,2024)toenhancetheheuristicsearch.
TheseinnovationshaveexpandedNCO’sapplicationacrossawidearrayofproblems(Chenetal.,
2023;Kimetal.,2023b;Zhouetal.,2023;Maetal.,2023b;LuttmannandXie,2024). However,
integratingappropriateinductivebiasesrequiresmanualtuningofmodelarchitecturesandtraining
algorithms,presentingchallengessuchascomputationallyintensivetraining,theneedforspecialized
hardware,andissueswithinterpretabilityandgeneralizability(Liuetal.,2023).
2Multi-agentConstructiveMethodsforNCO WhileseveralseminalworksasVinyalsetal.(2015);
Kooletal.(2018);Kwonetal.(2020)proposemodelsthatcanbeusedinloosemulti-agentsettings,
suchmethodscannotbeemployeddirectlytomodelheterogeneousagentswithdifferentattributes,
suchasdifferentcapacitiesorstartinglocationsinroutingproblems.Zhangetal.(2020b);Falknerand
Schmidt-Thieme(2020);Lietal.(2022);Liuetal.(2024c)introducemodelstoconstructsolutions
ofdifferentagentssimultaneously. However,theaboveparallelconstructionis,infact,sequential
interms ofthe modeland environmentstepping itself, thusnot benefittingfrom truemodel-side
paralleldecodingandoftenlackinginagentcollaboration. Ontheotherhand, Sonetal.(2024);
Zhengetal.(2024)proposetomodelmulti-agentmin-maxroutingassequentialdecision-making,
allowingforswitchingtheagentsafterasingle-agentsolutioniscomplete. MostrelatedtoPARCOis
Zongetal.(2022),whoproposeamulti-agentmodelforthemin-summulti-agentPDPwithdifferent
decoders. However,thisapproachisnotonlytailoredtoaspecificCOproblembutalsoaspecific
numberofagentsduetobothdifferentdecodersforeachagentandanon-flexiblecontextembedding,
whichis dependentonthe agentnumber, whilePARCOismore flexibleand hasamore general
communicationrepresentation. Moreover,whileZongetal.(2022)managesconflictsbyrandomly
givingprecedencetooneagent,PARCOlearnshowtoprioritizeagentswithhigherprobabilitiesof
selectingaspecificnode.
3 Background
3.1 MarkovDecisionProcesses
COproblemscanbeframedasMarkovDecisionProcesses(MDPs). Inthisformulation,theproblem
isdefinedbyasetofstatesS,whereeachstates ∈S representstheconfigurationoftheproblemat
t
timestept. Ateachstep,anagentselectsanactiona fromtheactionspaceAaccordingtoapolicy
t
π : S → A,whichmapsstatestoactions. Thesystemthentransitionsfromstates tostates
t t+1
accordingtoatransitionfunctionT :S×A→S. Theagentreachestheterminalstateonceithas
generatedaviablesolutionfortheprobleminstancex. Typically,arewardRisonlyobtainedinthe
terminalstateandtakestheformoftheobjectivefunctionoftherespectiveCOproblem.
3.2 AutoregressiveMethodsforNCO
GiventhesequentialnatureofMDPs,autoregressive(AR)methodsposeanaturalchoiceforthe
policyπ. ARmethodsconstructaviablesolutionbysequentiallygeneratingactionsbasedonthe
currentstateandpreviouslyselectedactions. Formally,theprocesscanberepresentedas:
T
(cid:89)
p (a|x)≜ g (a |a ,...,a ,h) (1)
θ θ t t−1 0
t=1
where h = f (x) is the encoding of problem instance x via encoder network f, which is used
θ
to decode actions autoregressively via decoder g . p is a solver that maps x to a solution a.
θ θ
a = (a ,...,a )representsthe(optimal)actionsexecutedinT constructionsteps, resulting ina
1 T
feasiblesolutiontoCOproblems.
Notably,thesameARschemehasbeenappliedtosolvemulti-agentCOproblems,suchasvehicle
routingproblems(Kooletal.,2018;Kwonetal.,2020),inwhichasingleagenttaskedwithvisiting
allnodesunderspecificroutelengthconstraints,distinguishingitfromarealmulti-agentscenarioby
focusingonoptimizingasolitaryagent’spathwithinsetlimits. Totacklesuchproblems,ARmethods
sequentiallyconstructmultipleroutesasifhandlingasingleagentatatime. However, inducing
collaborativebehavioramongagentsusingtheARmethodposesachallenge,asitinherentlyfocuses
ononeagentatatime. Thislimitationrestrictsthemethod’sabilitytodirectlyfosterinteractions
amongtheagents,whichiscrucialforoptimizingcollectiveoutcomesinmulti-agentscenarios.
3.3 TrainingARModelsviaReinforcementLearning
Inthiswork,wefocusonReinforcementLearning(RL)solversfortrainingp astheycanbetrained
θ
withoutrelyingon(oftenhard-to-obtain)labeledsolutions.
3Wecanthuscasttheoptimizationproblemasfollows:
θ∗ =argmax(cid:104) E (cid:2)E R(a,x)(cid:3)(cid:105) , (2)
x∼P(x) a∼pθ(a|x)
θ
where P(x) is problem distribution, R(a,x) is reward of a given x. We can solve Eq. (2) by
employingvariousRLmethodssuchasvariationsofpolicygradients(REINFORCE)(Kwonetal.,
2020;Kimetal.,2022).
4 Methodology
4.1 ParallelMulti-AgentEnvironments
Weproposetostepmultipleactionsovertheenvironmentsimultaneouslytoenhanceefficiencyin
practice,whichcanreducethenumberofstepsrequiredcomparedtosingle-agentstepping. Inthe
generalmulti-agentCOsetting,agentsk =1,...,mareselectingactionsa =(a1,...,am)froma
t t t
sharedactionspaceAinparallelatagivendecodingstept.Giventheagentactionsa ,thestateofthe
t
probleminstancetransitionsfroms tos viasometransitionfunctionT :S×A ×...×A →S,
t t+1 1 m
whichusuallyfollowsdeterministicrulesincaseofnon-stochasticCOproblems. Afterreaching
the terminal state, the agents receive a shared reward R(A,x), with x the problem instance and
A=(a ,...,a )thesequenceofagentsactions,whichistheobjectiveoftherespectiveCOproblem.
1 T
Fig.1illustratesanexampleofARandParallelARsolutionconstruction.
Inthiswork,weproposeamodelarchitecture,PARCO,tosolvesuchcooperativemulti-agentCO
problemsefficiently. Drawingonstudiesonmulti-agentRL(MARL)whichpromotesparameter
sharingincooperativeMARLsettingswithasharedactionspace(Yuetal.,2022),PARCOestablishes
acentralauto-regressivepolicyπ : S → P(A ×...×A )todecodethenextactionforthem
θ 1 m
agentsinparallel. ThisnotonlymakesPARCOagnostictothenumberofagentsprevalentinthe
respectiveCOproblem,butalsoenhancesbothsolutionqualityandconstructionspeed.
4.2 ParallelAutoregressiveModel
Inthissection,wepresentourgeneralarchitecturetosolveCOproblemsefficientlycastasmulti-agent
sequential decision-making problems. PARCO follows the general encoder-decoder architecture
prevalentinautoregressiveNCO(Kooletal.,2018;Belloetal.,2016;Vinyalsetal.,2015).
Encoder Decoder
Action Probabilities
Initial Encode Hidden States Context
Embedding Layers Agents Embedding Norm K
Graph Features Embedding + V
Q
N No o …d de e 1 2 … … xL EmN bo ed de ds i ng MLP K Agent 1
Node i … Norm V
… Q
Node N … AA gg. ee. nn. tt m1 curr Ae Sn g tt ae_ tns ett ssate=i M+
HA
K
Age …nt 2 Actions …
reset()
NN EPoo. nadd. ree.
va l
ilN1
e rl o U np mdat ein ng t S
SN
t
to
a
ad
t
te
e
es
s s ED my bn ea dm dii nc g [ Cb oa
mtcQ
mh_ us ni iz
ceK
a, t im o, n
V
h L_ ad yi em r]
QV
Mu MltA eig p ce l hn e at P n m io si mnter CoP nri fo lir cit ty s - Hba as ne dd ler
step()
Actions …
Figure2:OverviewofthePARCOmodel.
Encoder ThejoboftheencoderinautoregressiveNCOapproachesistolearnamappingfromthe
graphrepresentingtheCOproblemtoahigherdimensionalembeddingspace. Thistypicallyinvolves
aproblemdependingontheinitialembeddinglayer,projectingthegraphnodesusingtheirfeature
representations. ThisisalsothecaseforPARCO,whoseencoderfirstmapsfeaturesofthegraph
instancetoanembeddingspacetolearndeeprelationshipsbetweenthenodesthroughastackof
transformerblocks,similartoKooletal.(2018). Thecoreofeachtransformerblockisamulti-head
attention(MHA)layer,whichenablesmessagepassingbetweenthenodesinthegraphandcanbe
4definedasfollows(Vaswanietal.,2017):
(cid:32) (cid:33)
h
MHA(Q,K,V)= (cid:13) (cid:13) Attn(QWQ,KWKVWV) WO
i i i
i=1
where
(cid:18) QK⊤(cid:19)
Attn(Q,K,V)=softmax √ V .
d
k
where(cid:13) (cid:13)istheconcatenationoperator;WO ∈Rhdk×dh,withd
k
=d h/h,combinestheoutputsof
differentattentionheads,andWQ,WK,WV ∈Rdh×dk areprojectionmatricesforthequeriesQ,
h h h
keysK andvaluesV. InatypicalNCOsetting,queries,keys,andvaluescomefromthesameinput
sequenceh0 ∈RN×dh thatrepresentstheinitialembeddingsofthegraphnodes.
PARCOadditionallydefinesanagentprojectionlayerthatmapsagentsintothesameembedding
space as the problem nodes, whose exact definition heavily depends on the problem. In routing
problems,forexample,agentembeddingscanberepresentedbythedepotlocationtheyarestarting
fromandtheircapacities,whileanagentinschedulingproblemscanberepresentedbythemachine
whosescheduleitisconstructing. Lastly,encodedactionsandagentscanfurtherbeenhancedby
performingcross-attentionbetweenthem,dependingonthestructureoftheproblem(Kwonetal.,
2021). Wedescribethespecificencoderdetailsforthedifferentproblemscoveredinthisworkinthe
Appendix.
Having a separate agent encoder contrasts other multi-agent (NCO) approaches like Zhang et al.
(2020b),whichuseseparatepoliciesπk foreachagent. Thishasasubstantialdrawback,asamodel
θm
trainedforagivennumberofagentscannotbeusedtosolveproblemswithadifferentnumberof
agents. ThePARCOencoder,ontheotherhand,cangeneraterepresentationsforarbitraryagents,
makingitagnostictotheproblemsize.
Decoder ThedecoderisthecoreofthePARCOarchitecture. Weadoptthepointermechanism
fromKooletal.(2018),whichyieldsunnormalizedlog-probabilitiesz ∈RN foreachaction/node:
(cid:18) u⊤L(cid:19)
z =C·tanh √ (3)
d
h
whereC isascaleparameter,Lisaprojectionofthenodeembeddingshanduistheoutputofan
MHAlayer,inwhichthequeryisdefinedbyasinglecontextembeddingqc ∈Rdh,specifyingthe
t
currentstateoftheproblematdecodingsteptandkeysandvaluesarethenodeembeddingsfrom
theencoder. EachattentionheadoftheMHAlayercanfurtherfusetheprojectionoftheencoder
embeddingswithdynamicembeddingsrepresentingcurrentinformationabouttheparticularnode,
i.e.
Attn(qcWQ, hWK +WKδ , hWV +WVδ ) (4)
t i i δ t i δ t
withδ tencapsulatingthedynamicelementsoftheproblemnodesandW δK,W δV ∈R|δ|×dm project-
ingthemintoembeddingspace.
ForPARCOweintroduceanovelMultiplePointerMechanismbasedonEq.(3)ofthefollowing
form:
(cid:18) UL⊤(cid:19)
Z =C·tanh √ (5)
d
h
Here,U ∈Rm×dh istheoutputofanequivalentMHAlayerthantheoneusedinKooletal.(2018).
However, instead of a single context embedding, PARCO uses the m agent embeddings of the
encoder,enrichedbythecurrentstate’scontextandindividualagent’sstateembeddings,asqueries
Qc ∈ Rm×dh. TheresultingZ ∈ Rm×N log-probabilitiesareusedtosamplemactions-onefor
t
eachagent-inparallel.
CommunicationLayers Toenableeffectiveagentcoordination,weintroduceacommunication
layerbeforetheMultiplePointerMechanism,whichupdatestheagentembeddingsthroughmessages
exchangedwithotheragents. Thiscomponentisvitalforparalleldecodingofmultipleactions,asthe
5qualityofanactionchosenbyoneagentissignificantlyinfluencedbyhowitaffectstheactionsof
otheragents.
Giventhemagentqueriesatdecodingstept,thecommunicationlayerappliesatransformerblock
(Vaswanietal.,2017)tocaptureintra-agentrelationshipsandspotpotentialconflictsbetweenagents:
H′ =Norm(MHA(Qc,Qc,Qc)+Qc) (6)
t t t t
Q′ =Norm(MLP(H′)+H′) (7)
t
whereNormdenotesanormalizationlayer(IoffeandSzegedy,2015;ZhangandSennrich,2019)
andMLPrepresentsthemulti-layerperceptron. Self-attentionintheMHAlayerenablesmessage
passingbetweenagentsbasedontheirembeddings. Byapplyingthecommunicationlayerafterfusing
theagentembeddingswiththedynamiccontextembeddings,itcancapturedynamicrelationships
thatcouldnotberesolvedduringencoding,therebyenablingthedecodertomakemoreinformed
decisions. TheupdatedqueryQ′ isfinallypassedintotheMultiplePointerMechanism.
s
4.2.1 ConflictsHandlers
WhensamplingfromtheprobabilitydistributiongeneratedbytheMultiplePointerMechanism,itis
possibleformultipleagentstoselectthesamenodesimultaneously,resultinginaconflict. These
conflictsmustberesolvedtoensurethegenerationofafeasiblesolution.Toaddressthis,weintroduce
aPriority-basedConflictHandlerH,whichresolvessuchconflictsbyleveragingapredefinedpriority
scheme.TheconflicthandlercanbedefinedasafunctionH:Nm×Rm →Nmwithnumberofagents
m. Theprioritiesaredeterminedbasedonthecurrentactionsandstates. Givenaninputvectorof
actionsa=(a ,a ,...,a )wherea ∈N,thecorrespondingpriorityvectorp=(p ,p ,...,p )
1 2 m i 1 2 m
wherep ∈Randthefallbackactions(i.e.,thepreviousnode)r =(r ,r ,...,r )wherer ∈N,
i 1 2 m i
theoutputisanothervectorofactionsa′ = (a′,a′,...,a′ )suchthatanyconflictsareresolved
1 2 m
basedonthepriorityorderasillustratedinAlgorithm1.
Algorithm1Priority-basedConflictHandler
Require: Actionsa∈Nm,Prioritiesp∈Rm,Fallbackactionsr ∈Nm
Ensure: ResolvedActionsa′ ∈Nm
1: σ ←argsort(p,descending=True){Sortindicesbypriority}
2: aˆ ←a[σ]{Reorderactionsaccordingtopriority}
3: InitializeconflictmaskM ←0m
4: fori=2tomdo
5: ifaˆ i ∈{aˆ 1,...,aˆ i−1}then
6: M i ←1{Identifyconflicts}
7: endif
8: endfor
9: aˆ ←(1−M)⊙aˆ+M ·r{Resolveconflictsbyassigningfallbackactionr}
10: a′ ←aˆ[σ−1]{Reorderactionsbacktooriginalsequence}
11: return a′
4.2.2 StepDefinition
WedefineasingleparallelstepofthemodelsimilarlytoEq.(1)asfollows:
m
(cid:89)
p (a|x)= g (a′ |a′ ,...,a′ ,h) (8)
θ θ t,i t−1,i 0,i
i=1
Here,a′ denotestheactionexecutedbyagentiattimet,afterpassingthroughtheconflicthandler
t,i
H:
a′ =H(a ,a ,...,a ;p ,r ),
t,i t,1 t,2 t,m t t
wherep andr areprioritiesandfallbackactions,respectively,atthecurrentdecodingstept. The
t t
conflicthandlerHensuresthatmultipleagentsdonotselectthesamenode,thusmaintainingthe
integrityandfeasibilityofthesolution.
64.3 Training
PARCOisacentralizedmulti-agentsequentialdecision-makingmodel,withasharedpolicyπ for
θ
allagentsandaglobalrewardR. Thus,PARCOcanbetrainedusinganyofthetrainingalgorithms
adoptedinthesingle-agentNCOliterature. WetrainPARCOviatheREINFORCEgradientestimator
(Williams,1992)withasharedbaselineasoutlinedbyKwonetal.(2020)andKimetal.(2022):
B L
1 (cid:88)(cid:88)
∇ L≈ G ∇ logp (A |x ) (9)
θ B·L ij θ θ ij i
i=1j=1
whereBisthesizeofthemini-batchandG istheadvantageR(A ,x )−bshared(x )ofasolution
ij ij i i
A comparedtothesharedbaselinebsharedofprobleminstancex .
ij i i
5 Experiments
Inthissection,wepresenttheexperimentalresultsofPARCOintworoutingproblems,themin-max
heterogenouscapacitatedvehicleroutingproblem(HCVRP)andtheopenmulti-depotcapacitated
pickupanddeliveryproblem(OMDCPDP),andaschedulingproblem,namelytheflexibleflowshop
problem(FFSP).WeprovidemoredetailsabouttheproblemandexperimentalsetupsinAppendixA
andAppendixBrespectively.
5.1 ProblemDescriptions
HCVRP Themin-maxHCVRPconsistsofmagentssequentiallyvisitingcustomerstosatisfytheir
demands,withconstraintsincludingeachcustomercanbevisitedexactlyonceandtheamountof
demandsatisfiedbyasinglevehicleinatripcannotexceeditscapacity,whichcanbereloadedby
goingbacktothedepot.Thegoalistominimizethemakespan,i.e.,theworstroute.Baselinesinclude
SISR(ChristiaensandVandenBerghe,2020),GeneticAlgorithm(GA)(Karakaticˇ andPodgorelec,
2015),SimulatedAnnealing(SA)(˙Ilhan,2021),theAttentionModel(AM)(Kooletal.,2018),Equity
Transformer(ET)(Sonetal.,2024),themodelfromLietal.(2022)(DRL ),andthestate-of-the-art
Li
neuralbaseline2D-Ptr(Liuetal.,2024c).
OMDCPDP TheOMDCPDPproblemisapracticalvariantofthepickupanddeliveryproblem
inwhichagentshaveastackinglimitofordersthatcanbecarriedatanygiventime. Pickupand
deliverylocationsarepaired,andpickupsmustbevisitedbeforedeliveries. Multipleagentsstart
fromdifferentdepotswithnoneedtogoback(open). Thegoalistominimizethesumofarrival
timestodeliverylocations,i.e. minimizingthecumulativelateness. WeincludeORTools(Furnonand
Perron,2024)asaclassicalbaseline,theHeterogeneousAttentionModel(HAM)(Lietal.,2021a)
forsequentialdecision-makingandMAPDP(Zongetal.,2022)forparalleldecision-making.
FFSP Intheflexibleflowshopproblem(FFSP),N jobsmustbeprocessedacrossS stages,each
withmultiplemachines(m>1). Jobsfollowaspecifiedsequencethroughthesestages,butwithin
eachstage,anyavailablemachinecanprocessthejob,withthekeyconstraintthatnomachinecan
handlemorethanonejobsimultaneously. Thegoalistoschedulethejobssothatalljobsarefinished
intheshortesttimepossible. NotablebenchmarksincludetheMatNetmodel(Kwonetal.,2021),
theRandomandShortestJobFirst(SJF)dispatchingrules,aswellastheevolutionaryalgorithms
ParticleSwarmOptimization(PSO),andGeneticAlgorithm(GA)(HejaziandSaghafian,2005).
5.2 ExperimentalSetup
WeperformallexperimentsonamachineequippedwithtwoINTEL(R)XEON(R)GOLD6338CPU
@2.00GHZCPUswithatotal128threadsand8NVIDIARTX4090graphiccardswith24GBof
VRAM.TrainingrunsofPARCOtakelessthan24hourseach. Duringinference,weemployonly
oneCPUandasingleGPU.Wereportkeymetricssuchassolutioncost,inferencetimes,andgaps
inbest-knownsolutions. WekeepmostsettingsofPARCOconsistentacrossexperiments,i.e.,we
useaCommunicationLayerandPriority-basedConflictHandlingbasedonthehighestprobability
actionfromthemodeloutput. Weprovideadditionaldetailsregardingtrainingandtestingsetupsin
theAppendix.
7Table1: Benchmarksandresultsofourmodelformin-maxHCVRPproblemsofvaryingsizesandagent
numbers.Highlightingcost(↓)andaveragegaps(↓)fromthebest-knownsolutionsofclassicalheuristicsolvers.
Inferencetimesareshowninsecondsinparentheses(·).(g.)referstogreedyperformancewhile(s.)refersto
sampling1280solutions.
N 60 80 100 Gap(%)
m 3 5 7 3 5 7 3 5 7 avg.
SISR 6.57(271) 4.00(274) 2.91(276) 8.52(425) 5.10(430) 3.69(434) 10.29(615) 6.17(623) 4.45(625) 0.00
GA 9.21(233) 6.89(320) 5.98(405) 12.32(347) 8.95(465) 7.58(578) 15.33(479) 10.93(623) 9.10(772) 74.90
SA 7.04(130) 4.39(289) 3.30(362) 9.17(318) 5.61(417) 4.17(515) 11.13(434) 6.80(557) 5.01(678) 10.21
AM(g.) 8.49(0.08) 5.51(0.08) 4.15(0.09) 10.81(0.10) 6.87(0.11) 5.18(0.10) 12.68(0.14) 8.10(0.13) 6.13(0.13) 33.80
ET(g.) 7.58(0.15) 4.76(0.17) 3.58(0.16) 9.76(0.21) 6.01(0.20) 4.43(0.23) 11.74(0.25) 7.25(0.25) 5.23(0.26) 17.67
DRLLi(g.) 7.43(0.19) 4.71(0.22) 3.60(0.25) 9.64(0.25) 5.97(0.30) 4.52(0.33) 11.44(0.32) 7.06(0.37) 5.38(0.43) 17.08
2D-Ptr(g.) 7.20(0.11) 4.48(0.11) 3.31(0.11) 9.24(0.15) 5.65(0.15) 4.14(0.14) 11.12(0.18) 6.75(0.18) 4.92(0.17) 10.54
PARCO(g.) 7.12(0.12) 4.40(0.11) 3.25(0.11) 9.14(0.15) 5.53(0.15) 4.04(0.15) 10.98(0.19) 6.61(0.18) 4.79(0.18) 8.56
AM(s.) 7.62(0.14) 4.82(0.13) 3.63(0.14) 9.92(0.20) 6.19(0.21) 4.64(0.22) 11.82(0.29) 7.45(0.28) 5.58(0.28) 20.69
ET(s.) 7.14(0.21) 4.46(0.22) 3.33(0.22) 9.19(0.30) 5.64(0.30) 4.17(0.31) 11.20(0.41) 6.85(0.38) 4.98(0.40) 10.89
DRLLi(s.) 6.97(0.30) 4.34(0.36) 3.25(0.43) 9.10(0.45) 5.54(0.55) 4.13(0.65) 10.90(0.60) 6.65(0.76) 4.98(0.92) 8.78
2D-Ptr(s.) 6.82(0.13) 4.20(0.13) 3.09(0.14) 8.85(0.17) 5.36(0.18) 3.90(0.19) 10.71(0.22) 6.46(0.23) 4.68(0.24) 4.84
PARCO(s.) 6.82(0.31) 4.17(0.31) 3.06(0.31) 8.79(0.36) 5.28(0.36) 3.83(0.35) 10.61(0.40) 6.36(0.40) 4.58(0.40) 3.65
6
4.5
5 4.0 1.50 PARCO-10agents
PARCO-20agents
1.25 PARCO-50agents 3.5 4 1.00 E ET T- -1 20 0a ag ge en nt ts s
3.0 0.75 ET-50agents
3
2.5 0.50
0.25
2 2.0
N=50 N=100 N=200 N=50 N=100 N=200 0.00
Nocomm. MLPcomm. Random Smallest HighProb. 200 400 600 800 1000
MHAcomm. Comm. First Cloest Scale
(a)EffectofCommunicationLayers. (b)EffectofConflictHandlers. (c)PARCOvsARdecodingspeed.
Figure3:AblationstudiesonPARCOcomponents.PARCOscalesbetterthanARmodelsasET.
Table2:BenchmarksandresultsofourmodelforOMDCPDPproblemsofvaryingsizesandagentnumbers.
TrainingDistribution TestDistribution Gap(%)
N 50 100 200 1000 avg.
m 10 18 25 20 35 50 40 70 100 100 250 500
OR-Tools 23.73 18.64 16.92 43.34 34.80 31.89 78.35 64.93 60.16 562.11 333.01 310.55 0.00%
HAM(g.) 34.85 24.79 18.52 65.95 51.56 35.41 124.16 98.32 67.45 740.34 475.76 326.07 33.52%
MAPDP(g.) 27.95 20.13 17.54 52.44 38.23 33.33 - - - - - - 10.80%
PARCO(g.) 24.70 19.19 17.44 44.88 35.51 32.79 81.14 66.95 61.95 531.95 344.67 302.09 1.96%
HAM(s.) 32.84 25.45 18.12 63.59 49.12 34.90 122.77 96.32 67.01 739.77 471.09 321.50 31.02%
MAPDP(s.) 25.49 20.02 17.17 49.55 37.37 33.25 - - - - - - 7.04%
PARCO(s.) 24.27 18.71 17.00 43.36 34.82 32.19 79.23 65.83 61.12 526.52 338.78 298.71 -0.01%
Table3:FFSPresults.PARCOimprovestheMatNetperformanceandisalsomorethan4×fasteroverall.
FFSP20 FFSP50 FFSP100
Method
MS Gap Time(training) MS Gap Time(training) MS Gap Time(training)
CPLEX 46.4 21.0 60s - -
CPLEX 36.6 6.4 600s - -
Random 47.8 22.9 0.1s 93.2 44 0.2s 167.2 78.0 0.3s
ShortestJobFirst 31.3 6.4 0.1s 57.0 7.8 0.1s 99.3 10.1 0.2s
GeneticAlgorithm 30.6 5.7 25.4s 56.4 7.2 57.8s 98.7 9.5 104.5s
ParticleSwarmOpt. 29.1 4.2 46.8s 55.1 5.9 92.9s 97.3 8.1 173.4s
MatNet 27.26 3.0 0.9s(2h) 51.52 0.5 2.1s(5.25h) 91.58 0.0 4.9s(28h)
PARCO(w/oCL) 27.02 2.1 0.2s(0.25h) 52.29 2.0 0.4s(1.25h) 92.15 2.0 0.9s(5.25h)
PARCO 26.47 0.0 0.2s(0.5h) 51.25 0.0 0.5s(2h) 91.55 0.0 1.1s(6.25h)
8
)%(paG )%(paG )s(emiT5.3 ExperimentalResults
Mainexperiments ThemainexperimentsshowcasingtheperformanceofPARCOareinTable1,
Table2,Table3forHCVRP,OMDCPDP,andFFSP,respectively. OurPARCOconsistentlyoutper-
formsSotAneuralbaselinesacrossavarietyofexperiments. Weadditionallynoteanotherproperty
ofPARCO:unlikeneuralbaselinesinHCVRPandMAPDPintheOMDCPDP,whicharetrained
specificallyforasinglesizeandnumberofagents,PARCOisasinglemodeltrainedonmultiple
location and agent distributions at the same time; nonetheless, our single model can outperform
baselinestrainedadhoconspecificdistributions,demonstratingourmethod’sflexibility.
Generalization WeadditionallystudythegeneralizationperformanceinTable2ontheright,which
arethesizesandnumberofagentsunseenduringtraininginOMDCPDP.UnlikeMAPDP,whichis
constrainedtoaspecificnumberofagents,PARCOcansuccessfullygeneralizetounseensizesand
m. Notably,PARCOcanevenoutperformGoogleORToolsforlarger-scaleinstances,demonstrating
remarkablescalability.
EffectofCommunicationLayers Weshowcase
theimportanceofCommunicationLayersinFig.3 Table4:Effectofdifferent#ofagentsinFFSP.
(a). Webenchmark1)NoCommunication(onlycon-
textfeatures),2)communicationviaanMLP,3)com- J×S×m Metric MatNet PARCO
municationviaanMHAlayer,and4)OurCommuni-
Obj. 35.69 33.58
cationlayer. OurCommunicationLayerconsistently Gap 6.3% 0.0%
50×3×6
outperformsothermethods. Duration 1.1h 10.3m
Avg.Steps 200.05 19.10
EffectofConflictHandlers Fig.3(b)showsthe Obj. 28.59 25.62
effect of the Priority-based Conflict Handler with Gap 11.6% 0.0%
50×3×8
differentprioritiesmethodspintheOMDCPDP.We Duration 1.5h 9.4m
Avg.Steps 205.95 16.80
consider the following: 1) Random: the priority is
chosenrandomlyasinZongetal.(2022),2)First:the Obj. 25.05 21.46
priorityischosenbytheagentindexk =1,...,m,3) 50×3×10 Gap 16.7% 0.0%
Duration 1.8h 7.5m
Smallest: givesprioritytotheagentthathastraveled
Avg.Steps 215.85 13.45
theleast,4)Closest: priorityisgiventotheclosest
agent and 5) High Probability: priority is given to the agent whose probability of selecting the
conflictingactionisthehighest. HighProbabilitycanconsistentlyoutperformothermethods. We
notethatthisisalsothemostgeneralandproblem-agnosticsincemethods2-4areproblem-specific.
Scalability Finally,weshowcasePARCO’sscalabilityintermsofspeedinlarge-scaleinstancesin
Fig.3(c)comparedtotheconstructiveautoregressiveET(Sonetal.,2024). Interestingly,PARCO’s
gapgrowswithmoreagents. Thisisbecauseourmethodcanfullyexploitagentparallelismandcan
thusbeastrongcandidateforlarge-scalereal-timeCOapplications.
6 Conclusion
Inthispaper, weintroducedanovelapproach, PARCO-ParallelAutoRegressiveCombinatorial
Optimization-thatutilizesparallelautoregressive(AR)policiestotacklemulti-agentNCOproblems.
WeintroducedanewCommunicationLayertoallowfortheagentstoeffectivelycoordinatetheir
nextstepsduringdecoding,alongsideaMultiplePointerMechanismcoupledwithaPriority-based
ConflictsHandlerthatcangeneratefeasiblesolutionsefficiently. Wevalidatetheeffectivenessof
PARCOthroughextensiveexperimentswithdifferentCOproblemsfromtheroutingandscheduling
domainanddemonstrateitscompetitiveperformanceagainstclassicalheuristicandneuralbaselines.
WhilePARCOisalreadyfasterthanmostevaluatedneuralbaselinesanddecreasesthenumberof
requireddecodingstepsbymorethan90%inFFSP50instanceswith10agentsasshowninTable4,
itstilldoesnotachievethepotentialreductionfromO(N)toO(N)inmanycases. Thisismainly
m
attributedtothefactthatagentscanhaveconflictsthatwecurrentlyresolvewiththepriority-based
conflicthandler. Givingoneagentprecedenceandforcingotherstostayintheircurrentposition
resultsinmorethannecessarydecodingsteps. Infuturework,wewilltrytoaddressthislimitation
byconstructingacustomlossfunction,whichpenalizesconflictsandmakesagentslearntoavoid
themduringtraining. Further,weplantoextendourworkwithlearnableimprovementmethodsto
9enablefastfirstsolutionscoupledwithpowerfullocalsearch. GivenPARCO’ssolutionqualityand
efficiency,afurtherexcitingavenueoffutureresearchistocoupleourmodelwithpopulation-based
approaches(Grinsztajnetal.,2022;Chalumeauetal.,2023;Hottungetal.,2024)thatcouldsearch
thelatentspacefordiverseandbettersolutions.
Acknowledgements
WegratefullyacknowledgeOMELETforsupportinguswithcomputing. Wealsothankpeopleinthe
AI4COopenresearchcommunitywhohavecontributedtoRL4CO,whichthisworkisbasedon.
References
S.Ann,Y.Kim,andJ.Ahn. Areaallocationalgorithmformultipleuavsareacoveragebasedon
clusteringandgraphmethod. IFAC-PapersOnLine,48(9):204–209,2015.
C.ArchettiandL.Bertazzi. Recentchallengesinroutingandinventoryrouting: E-commerceand
last-miledelivery. Networks,77(2):255–268,2021.
S.Bae,J.Ko,H.Song,andS.-Y.Yun. Fastandrobustearly-exitingframeworkforautoregressive
languagemodelswithsynchronizedparalleldecoding. arXivpreprintarXiv:2310.05424,2023.
T. Bektas. The multiple traveling salesman problem: an overview of formulations and solution
procedures. omega,34(3):209–219,2006.
I.Bello,H.Pham,Q.V.Le,M.Norouzi,andS.Bengio. Neuralcombinatorialoptimizationwith
reinforcementlearning. arXivpreprintarXiv:1611.09940,2016.
Y.Bengio,A.Lodi,andA.Prouvost. Machinelearningforcombinatorialoptimization: amethod-
ologicaltourd’horizon. EuropeanJournalofOperationalResearch,290(2):405–421,2021.
F. Berto, C. Hua, J. Park, L. Luttmann, Y. Ma, F. Bu, J. Wang, H. Ye, M. Kim, S. Choi, N. G.
Zepeda,A.Hottung,J.Zhou,J.Bi,Y.Hu,F.Liu,H.Kim,J.Son,H.Kim,D.Angioni,W.Kool,
Z.Cao,J.Zhang,K.Shin,C.Wu,S.Ahn,G.Song,C.Kwon,L.Xie,andJ.Park. RL4CO:an
ExtensiveReinforcementLearningforCombinatorialOptimizationBenchmark. arXivpreprint
arXiv:2306.17100,2024a. https://github.com/ai4co/rl4co.
F.Berto,C.Hua,N.G.Zepeda,A.Hottung,N.Wouda,L.Lan,K.Tierney,andJ.Park. Routefinder:
Towardsfoundationmodelsforvehicleroutingproblems. arXivpreprintarXiv:2406.15007,2024b.
J.Bi,Y.Ma,J.Wang,Z.Cao,J.Chen,Y.Sun,andY.M.Chee. Learninggeneralizablemodelsfor
vehicleroutingproblemsviaknowledgedistillation. AdvancesinNeuralInformationProcessing
Systems,35:31226–31238,2022.
J.Chabarek,J.Sommers,P.Barford,C.Estan,D.Tsiang,andS.Wright. Powerawarenessinnetwork
designandrouting. InIEEEINFOCOM2008-The27thConferenceonComputerCommunications,
pages457–465.IEEE,2008.
F. Chalumeau, S. Surana, C. Bonnet, N. Grinsztajn, A. Pretorius, A. Laterre, and T. D. Barrett.
Combinatorial optimization with policy adaptation using latent space search. arXiv preprint
arXiv:2311.13569,2023.
O.CheikhrouhouandI.Khoufi. Acomprehensivesurveyonthemultipletravelingsalesmanproblem:
Applications,approachesandtaxonomy. ComputerScienceReview,40:100369,2021.
J.Chen,J.Wang,Z.Zhang,Z.Cao,T.Ye,andS.Chen. Efficientmetaneuralheuristicformulti-
objectivecombinatorialoptimization. arXivpreprintarXiv:2310.15196,2023.
J.Choo,Y.-D.Kwon,J.Kim,J.Jae,A.Hottung,K.Tierney,andY.Gwon. Simulation-guidedbeam
searchforneuralcombinatorialoptimization. AdvancesinNeuralInformationProcessingSystems,
35:8760–8772,2022.
J.ChristiaensandG.VandenBerghe.Slackinductionbystringremovalsforvehicleroutingproblems.
TransportationScience,54(2):417–433,2020.
A.Corsini,A.Porrello,S.Calderara,andM.Dell’Amico. Self-labelingthejobshopscheduling
problem. arXivpreprintarXiv:2401.11849,2024.
10D.Drakulic,S.Michel,F.Mai,A.Sors,andJ.-M.Andreoli. Bq-nco: Bisimulationquotientingfor
efficientneuralcombinatorialoptimization. InThirty-seventhConferenceonNeuralInformation
ProcessingSystems,2023.
D.Drakulic,S.Michel,andJ.-M.Andreoli. Goal: Ageneralistcombinatorialoptimizationagent
learner. arXivpreprintarXiv:2406.15079,2024.
W.FalconandThePyTorchLightningteam. PyTorchLightning,32019. URLhttps://github.
com/Lightning-AI/lightning.
J.K.FalknerandL.Schmidt-Thieme. Learningtosolvevehicleroutingproblemswithtimewindows
throughjointattention. arXivpreprintarXiv:2006.09100,2020.
V.FurnonandL.Perron. Or-toolsroutinglibrary,2024. URLhttps://developers.google.
com/optimization/routing/.
C.Gao,H.Shang,K.Xue,D.Li,andC.Qian. Towardsgeneralizableneuralsolversforvehicle
routingproblemsviaensemblewithtransferrablelocalpolicy. arXivpreprintarXiv:2308.14104,
2023.
R.Girardey,M.Hübner,andJ.Becker. Safetyawareplaceandrouteforon-chipredundancyinsafety
criticalapplications. In2010IEEEComputerSocietyAnnualSymposiumonVLSI,pages74–79.
IEEE,2010.
F.Gloeckle,B.Y.Idrissi,B.Rozière,D.Lopez-Paz,andG.Synnaeve. Better&fasterlargelanguage
modelsviamulti-tokenprediction. arXivpreprintarXiv:2404.19737,2024.
N.Grinsztajn,D.Furelos-Blanco,andT.D.Barrett. Population-basedreinforcementlearningfor
combinatorialoptimization. arXivpreprintarXiv:2210.03475,2022.
C.He,T.Duhan,P.Tulsyan,P.Kim,andG.Sartoretti. Socialbehaviorasakeytolearning-based
multi-agentpathfindingdilemmas. arXivpreprintarXiv:2408.03063,2024.
S.R.HejaziandS.Saghafian. Flowshop-schedulingproblemswithmakespancriterion: areview.
InternationalJournalofProductionResearch,43(14):2895–2929,2005.
A. Hottung, Y.-D. Kwon, and K. Tierney. Efficient active search for combinatorial optimization
problems. arXivpreprintarXiv:2106.05126,2021.
A.Hottung,M.Mahajan,andK.Tierney. Polynet: Learningdiversesolutionstrategiesforneural
combinatorialoptimization. arXivpreprintarXiv:2402.14048,2024.
˙I.˙Ilhan. Animprovedsimulatedannealingalgorithmwithcrossoveroperatorforcapacitatedvehicle
routingproblem. SwarmandEvolutionaryComputation,64:100911,2021.
S. Ioffe and C. Szegedy. Batch normalization: Accelerating deep network training by reducing
internalcovariateshift. InInternationalconferenceonmachinelearning,pages448–456.pmlr,
2015.
X.Jiang,Y.Wu,Y.Wang,andY.Zhang. Unco: Towardsunifyingneuralcombinatorialoptimization
throughlargelanguagemodel. arXivpreprintarXiv:2408.12214,2024.
Y. Jiang, Z. Cao, Y. Wu, W. Song, and J. Zhang. Ensemble-based deep reinforcement
learning for vehicle routing problems under distribution shift. In A. Oh, T. Naumann,
A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural In-
formation Processing Systems, volume 36, pages 53112–53125. Curran Associates, Inc.,
2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/
file/a68120d2eb2f53f7d9e71547591aef11-Paper-Conference.pdf.
Y.Jin,Y.Ding,X.Pan,K.He,L.Zhao,T.Qin,L.Song,andJ.Bian. Pointerformer: Deepreinforced
multi-pointertransformerforthetravelingsalesmanproblem. arXivpreprintarXiv:2304.09407,
2023.
M.Jünger,G.Reinelt,andG.Rinaldi. Thetravelingsalesmanproblem. Handbooksinoperations
researchandmanagementscience,7:225–330,1995.
C.Kahraman,O.Engin,I.Kaya,andM.K.Yilmaz. Anapplicationofeffectivegeneticalgorithmsfor
solvinghybridflowshopschedulingproblems.InternationalJournalofComputationalIntelligence
Systems,1(2):134–147,2008.
S. Karakaticˇ and V. Podgorelec. A survey of genetic algorithms for solving multi depot vehicle
routingproblem. AppliedSoftComputing,27:519–532,2015.
11H.Kim,M.Kim,S.Ahn,andJ.Park. Symmetricexplorationincombinatorialoptimizationisfree!
arXivpreprintarXiv:2306.01276,2023a.
H.Kim,M.Kim,F.Berto,J.Kim,andJ.Park. Devformer: Asymmetrictransformerforcontext-
awaredeviceplacement. 2023b.
H.Kim,J.Park,andC.Kwon. Aneuralseparationalgorithmfortheroundedcapacityinequalities.
INFORMSJournalonComputing,2024a.
M.Kim,J.Park,andJ.Park. Sym-nco: Leveragingsymmetricityforneuralcombinatorialoptimiza-
tion. AdvancesinNeuralInformationProcessingSystems,35:1936–1949,2022.
M.Kim,T.Yun,E.Bengio,D.Zhang,Y.Bengio,S.Ahn,andJ.Park. Localsearchgflownets. arXiv
preprintarXiv:2310.02710,2023c.
M.Kim,S.Choi,J.Son,H.Kim,J.Park,andY.Bengio. Antcolonysamplingwithgflownetsfor
combinatorialoptimization. arXivpreprintarXiv:2403.07041,2024b.
D.P.KingmaandJ.Ba.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980,
2014.
D.Kong,Y.Ma,Z.Cao,T.Yu,andJ.Xiao. Efficientneuralcollaborativesearchforpickupand
deliveryproblems. 2024.
W.Kool,H.VanHoof,andM.Welling. Attention,learntosolveroutingproblems! arXivpreprint
arXiv:1803.08475,2018.
Y.-D. Kwon, J. Choo, B. Kim, I. Yoon, Y. Gwon, and S. Min. Pomo: Policy optimization with
multipleoptimaforreinforcementlearning. AdvancesinNeuralInformationProcessingSystems,
33:21188–21198,2020.
Y.-D.Kwon,J.Choo,I.Yoon,M.Park,D.Park,andY.Gwon. Matrixencodingnetworksforneural
combinatorialoptimization. AdvancesinNeuralInformationProcessingSystems,34:5138–5149,
2021.
G.LaporteandI.H.Osman. Routingproblems: Abibliography. Annalsofoperationsresearch,61:
227–262,1995.
J.Li,L.Xin,Z.Cao,A.Lim,W.Song,andJ.Zhang.Heterogeneousattentionsforsolvingpickupand
deliveryproblemviadeepreinforcementlearning.IEEETransactionsonIntelligentTransportation
Systems,23(3):2306–2315,2021a.
J.Li,Y.Ma,R.Gao,Z.Cao,L.Andrew,W.Song,andJ.Zhang. Deepreinforcementlearningfor
solvingtheheterogeneouscapacitatedvehicleroutingproblem. IEEETransactionsonCybernetics,
52(12):13572–13585,2022. doi: 10.1109/TCYB.2021.3111082.
J. Li, C. Hua, H. Ma, J. Park, V. Dax, and M. J. Kochenderfer. Multi-agent dynamic relational
reasoningforsocialrobotnavigation. arXivpreprintarXiv:2401.12275,2024a.
S.Li,Z.Yan,andC.Wu. Learningtodelegateforlarge-scalevehiclerouting. AdvancesinNeural
InformationProcessingSystems,34:26198–26211,2021b.
Y.Li,J.Guo,R.Wang,andJ.Yan. T2t: Fromdistributionlearningintrainingtogradientsearch
intestingforcombinatorialoptimization. InThirty-seventhConferenceonNeuralInformation
ProcessingSystems,2023.
Y.Li,J.Guo,R.Wang,andJ.Yan. Fromdistributionlearningintrainingtogradientsearchintesting
forcombinatorialoptimization. AdvancesinNeuralInformationProcessingSystems,36,2024b.
Q.LinandH.Ma. Sacha: Softactor-criticwithheuristic-basedattentionforpartiallyobservable
multi-agentpathfinding. IEEERoboticsandAutomationLetters,2023.
Z.Lin,Y.Wu,B.Zhou,Z.Cao,W.Song,Y.Zhang,andS.Jayavelu. Cross-problemlearningfor
solvingvehicleroutingproblems. IJCAI,2024.
F. Liu, X. Lin, Q. Zhang, X. Tong, and M. Yuan. Multi-task learning for routing problem with
cross-problemzero-shotgeneralization. arXivpreprintarXiv:2402.16891,2024a.
F.Liu,X.Tong,M.Yuan,X.Lin,F.Luo,Z.Wang,Z.Lu,andQ.Zhang. Evolutionofheuristics:
Towards efficient automatic algorithm design using large language model. In International
ConferenceonMachineLearning,2024b.
12Q.Liu,C.Liu,S.Niu,C.Long,J.Zhang,andM.Xu. 2d-ptr: 2darraypointernetworkforsolving
theheterogeneouscapacitatedvehicleroutingproblem. InProceedingsofthe23rdInternational
ConferenceonAutonomousAgentsandMultiagentSystems,pages1238–1246,2024c.
S.Liu,Y.Zhang,K.Tang,andX.Yao. Howgoodisneuralcombinatorialoptimization? asystematic
evaluationonthetravelingsalesmanproblem. IEEEComputationalIntelligenceMagazine,18(3):
14–28,2023.
F. Luo, X. Lin, F. Liu, Q. Zhang, and Z. Wang. Neural combinatorial optimization with heavy
decoder: Towardlargescalegeneralization. arXivpreprintarXiv:2310.07985,2023.
F.Luo,X.Lin,Z.Wang,T.Xialiang,M.Yuan,andQ.Zhang. Self-improvedlearningforscalable
neuralcombinatorialoptimization. arXivpreprintarXiv:2403.19561,2024.
L.LuttmannandL.Xie. Neuralcombinatorialoptimizationonheterogeneousgraphs: Anapplication
tothepickerroutingprobleminmixed-shelveswarehouses. InProceedingsoftheInternational
ConferenceonAutomatedPlanningandScheduling,volume34,pages351–359,2024.
Y.Ma,Z.Cao,andY.M.Chee.Learningtosearchfeasibleandinfeasibleregionsofroutingproblems
withflexibleneuralk-opt. arXivpreprintarXiv:2310.18264,2023a.
Z. Ma, H. Guo, J. Chen, Z. Li, G. Peng, Y.-J. Gong, Y. Ma, and Z. Cao. Metabox: A bench-
mark platform for meta-black-box optimization with reinforcement learning. arXiv preprint
arXiv:2310.08252,2023b.
S.MahmoudinazlouandC.Kwon. Ahybridgeneticalgorithmforthemin–maxmultipletraveling
salesmanproblem. Computers&OperationsResearch,162:106455,2024.
X.Ning,Z.Lin,Z.Zhou,H.Yang,andY.Wang. Skeleton-of-thought: Largelanguagemodelscan
doparalleldecoding. arXivpreprintarXiv:2307.15337,2023.
S.N.Parragh, K.F.Doerner, andR.F.Hartl. Asurveyonpickupanddeliveryproblems: Parti:
Transportationbetweencustomersanddepot. JournalfürBetriebswirtschaft,58:21–51,2008.
A.Paszke,S.Gross,F.Massa,A.Lerer,J.Bradbury,G.Chanan,T.Killeen,Z.Lin,N.Gimelshein,
L.Antiga,etal. Pytorch: Animperativestyle,high-performancedeeplearninglibrary. Advances
inneuralinformationprocessingsystems,32,2019.
J.PirnayandD.G.Grimm. Self-improvementforneuralcombinatorialoptimization:Samplewithout
replacement,butimprovement. arXivpreprintarXiv:2403.15180,2024.
W.Qi,Y.Yan,Y.Gong,D.Liu,N.Duan,J.Chen,R.Zhang,andM.Zhou. Prophetnet: Predicting
futuren-gramforsequence-to-sequencepre-training. arXivpreprintarXiv:2001.04063,2020.
R.Reijnen,Y.Zhang,W.Nuijten,C.Senaras,andM.Goldak-Altgassen. Combiningdeepreinforce-
mentlearningwithsearchheuristicsforsolvingmulti-agentpathfindinginsegment-basedlayouts.
In2020IEEEsymposiumseriesoncomputationalintelligence(SSCI),pages2647–2654.IEEE,
2020.
M.W.SavelsberghandM.Sol. Thegeneralpickupanddeliveryproblem. Transportationscience,29
(1):17–29,1995.
M.R.SinghandS.Mahapatra. Aswarmoptimizationapproachforflexibleflowshopscheduling
withmultiprocessortasks. TheInternationalJournalofAdvancedManufacturingTechnology,62:
267–277,2012.
J.Son,M.Kim,S.Choi,H.Kim,andJ.Park. Equity-transformer: Solvingnp-hardmin-maxrouting
problemsassequentialgenerationwithequitycontext. InProceedingsoftheAAAIConferenceon
ArtificialIntelligence,volume38,pages20265–20273,2024.
Z.SunandY.Yang. Difusco: Graph-baseddiffusionsolversforcombinatorialoptimization. arXiv
preprintarXiv:2302.08224,2023.
H.Tang,F.Berto,Z.Ma,C.Hua,K.Ahn,andJ.Park. Himap: Learningheuristics-informedpolicies
forlarge-scalemulti-agentpathfinding. arXivpreprintarXiv:2402.15546,2024a.
H.Tang,F.Berto,andJ.Park. Ensemblingprioritizedhybridpoliciesformulti-agentpathfinding. In
2024IEEE/RSJInternationalConferenceonIntelligentRobotsandSystems(IROS).IEEE,2024b.
https://github.com/ai4co/eph-mapf.
D.Ulyanov,A.Vedaldi,andV.Lempitsky. Instancenormalization: Themissingingredientforfast
stylization. arXivpreprintarXiv:1607.08022,2016.
13A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N.Gomez,Ł.Kaiser,andI.Polosukhin.
Attentionisallyouneed. Advancesinneuralinformationprocessingsystems,30,2017.
O.Vinyals,M.Fortunato,andN.Jaitly. Pointernetworks. Advancesinneuralinformationprocessing
systems,28,2015.
C. Wang, Z. Yu, S. McAleer, T. Yu, and Y. Yang. Asp: Learn a universal neural solver! IEEE
TransactionsonPatternAnalysisandMachineIntelligence,2024.
Y.Wang,B.Xiang,S.Huang,andG.Sartoretti. Scrimp: Scalablecommunicationforreinforcement-
andimitation-learning-basedmulti-agentpathfinding. In2023IEEE/RSJInternationalConference
onIntelligentRobotsandSystems(IROS),pages9301–9308.IEEE,2023.
R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement
learning. Machine Learning, 8(3-4):229–256, May 1992. ISSN 0885-6125, 1573-0565. doi:
10.1007/BF00992696.
Y.Xiao,D.Wang,B.Li,M.Wang,X.Wu,C.Zhou,andY.Zhou. Distillingautoregressivemodels
toobtainhigh-performancenon-autoregressivesolversforvehicleroutingproblemswithfaster
inferencespeed. arXivpreprintarXiv:2312.12469,2023.
L.Xie,H.Li,andL.Luttmann. Formulatingandsolvingintegratedorderbatchingandroutingin
multi-depotagv-assistedmixed-shelveswarehouses. EuropeanJournalofOperationalResearch,
307(2):713–730,2023.
E.YakıcıandO.Karasakal.Amin–maxvehicleroutingproblemwithsplitdeliveryandheterogeneous
demand. OptimizationLetters,7:1611–1625,2013.
Z. Yan and C. Wu. Neural neighborhood search for multi-agent path finding. In The Twelfth
InternationalConferenceonLearningRepresentations,2024.
Y.YangandA.Whinston. Asurveyonreinforcementlearningforcombinatorialoptimization. In
2023IEEEWorldConferenceonAppliedIntelligenceandComputing(AIC),pages131–136.IEEE,
2023.
H. Ye, J. Wang, H. Liang, Z. Cao, Y. Li, and F. Li. Glop: Learning global partition and local
constructionforsolvinglarge-scaleroutingproblemsinreal-time.arXivpreprintarXiv:2312.08224,
2023.
H.Ye,J.Wang,Z.Cao,H.Liang,andY.Li. Deepaco:neural-enhancedantsystemsforcombinatorial
optimization. AdvancesinNeuralInformationProcessingSystems,36,2024a.
H. Ye, J. Wang, Z. Cao, and G. Song. Reevo: Large language models as hyper-heuristics with
reflectiveevolution. arXivpreprintarXiv:2402.01145,2024b.
C.Yu,A.Velu,E.Vinitsky,J.Gao,Y.Wang,A.Bayen,andY.Wu. Thesurprisingeffectivenessof
ppoincooperativemulti-agentgames. AdvancesinNeuralInformationProcessingSystems,35:
24611–24624,2022.
B.ZhangandR.Sennrich. Rootmeansquarelayernormalization. AdvancesinNeuralInformation
ProcessingSystems,32,2019.
C. Zhang, W. Song, Z. Cao, J. Zhang, P. S. Tan, and X. Chi. Learning to dispatch for job shop
schedulingviadeepreinforcementlearning. Advancesinneuralinformationprocessingsystems,
33:1621–1632,2020a.
K. Zhang, F. He, Z. Zhang, X. Lin, and M. Li. Multi-vehicle routing problems with soft time
windows: A multi-agent reinforcement learning approach. Transportation Research Part C:
EmergingTechnologies,121:102861,2020b.
Z.Zheng,S.Yao,Z.Wang,X.Tong,M.Yuan,andK.Tang.Dpn:Decouplingpartitionandnavigation
forneuralsolversofmin-maxvehicleroutingproblems. arXivpreprintarXiv:2405.17272,2024.
J. Zhou, Y. Wu, Z. Cao, W. Song, J. Zhang, and Z. Chen. Learning large neighborhood search
for vehicle routing in airport ground handling. IEEE Transactions on Knowledge and Data
Engineering,2023.
J.Zhou,Z.Cao,Y.Wu,W.Song,Y.Ma,J.Zhang,andC.Xu. Mvmoe: Multi-taskvehiclerouting
solverwithmixture-of-experts. arXivpreprintarXiv:2405.01029,2024.
Z. Zong, M. Zheng, Y. Li, and D. Jin. Mapdp: Cooperative multi-agent reinforcement learning
to solve pickup and delivery problems. In Proceedings of the AAAI Conference on Artificial
Intelligence,volume36,pages9980–9988,2022.
14PARCO: Learning Parallel Autoregressive Policies for
Efficient Multi-Agent Combinatorial Optimization
Appendix
Table of Contents
A ProblemDescriptions 15
A.1 HCVRP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.1.1 ProblemDefinition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
A.1.2 MathematicalFormulation . . . . . . . . . . . . . . . . . . . . . . . . 16
A.2 OMDCPDP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.2.1 ProblemDefinition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
A.2.2 MathematicalFormulation . . . . . . . . . . . . . . . . . . . . . . . . 17
A.3 FFSP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.3.1 ProblemDefinition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
A.3.2 MathematicalFormulation . . . . . . . . . . . . . . . . . . . . . . . . 18
B ExperimentalDetails 19
B.1 HCVRP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.1.1 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.1.2 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
B.1.3 PARCONetworkHyperparameters . . . . . . . . . . . . . . . . . . . . 21
B.1.4 PARCOTrainingHyperparameters . . . . . . . . . . . . . . . . . . . . 21
B.2 OMDCPDP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.2.1 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.2.2 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.2.3 PARCONetworkHyperparameters . . . . . . . . . . . . . . . . . . . . 23
B.2.4 PARCOTrainingHyperparameters . . . . . . . . . . . . . . . . . . . . 23
B.3 FFSP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
B.3.1 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
B.3.2 Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
B.3.3 PARCONetworkHyperparameters . . . . . . . . . . . . . . . . . . . . 24
B.3.4 PARCOTrainingHyperparameters . . . . . . . . . . . . . . . . . . . . 25
B.3.5 DiagramforMatNetDecodingvs.PARCODecodingfortheFFSP . . . 25
B.4 HardwareandSoftware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
B.4.1 Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
B.4.2 Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
B.5 SourceCode . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
C AdditionalDiscussion 25
A ProblemDescriptions
A.1 HCVRP
A.1.1 ProblemDefinition
Themin-maxHCVRP(HeterogeneousCapacitatedVehicleRoutingProblem)consistsofmagents
sequentiallyvisitingcustomerstosatisfytheirdemands,withconstraintsincludingeachcustomer
canbevisitedexactlyonceandtheamountofdemandsatisfiedbyasinglevehicleinatripcannot
exceeditscapacity,whichcanbereloadedbygoingbacktothedepot. Thegoalistominimizethe
makespan,i.e.,theworstroute.
15A.1.2 MathematicalFormulation
ConsideraproblemwithN +1nodes(includingN customersandadepot)andmvehicles. The
depotisindexedas0,andcustomersareindexedfrom1toN.
Indices
i,j Nodeindices,wherei,j =0,...,N (0representsthedepot)
k Vehicleindex,wherek =1,...,m
Parameters
N Numberofcustomernodes(excludingdepot)
m Numberofvehicles
X Locationofnodei
i
d Demandofnodei(d =0forthedepot)
i 0
Q Capacityofvehiclek
k
f Speedofvehiclek
k
c Distancebetweennodesiandj
ij
DecisionVariables
(cid:26)
1 ifvehiclektravelsdirectlyfromnodeitonodej
x
ijk 0 otherwise
l Remainingloadofvehiclekbeforetravellingfromnodeitonodej
ijk
ObjectiveFunction:
 
N N
(cid:88)(cid:88)c
ij
min max  x ijk (10)
k=1,...,m f k
i=0j=0
Subjectto:
m N
(cid:88)(cid:88)
x =1 i=1,...,N (11)
ijk
k=1j=0
N N
(cid:88) (cid:88)
x − x =0 j =0,...,N, k =1,...,m (12)
ijk jhk
i=0 h=0
m N m N
(cid:88)(cid:88) (cid:88)(cid:88)
l − l =d j =1,...,N (13)
ijk jhk j
k=1i=0 k=1h=0
d x ≤l ≤(Q −d )·x i,j =0,...,N, k =1,...,m (14)
j ijk ijk k i ijk
x ∈{0,1} i,j =0,...,N, k =1,...,m (15)
ijk
l ≥0,d ≥0 i,j =0,...,N, k =1,...,m (16)
ijk i
ConstraintExplanations: Theformulationissubjecttoseveralconstraintsthatdefinethefeasible
solutionspace. Equation(11)ensuresthateachcustomerisvisitedexactlyoncebyonevehicle. The
flowconservationconstraint(12)guaranteesthateachvehiclethatentersanodealsoleavesthatnode,
maintainingroutecontinuity. Demandsatisfactionisenforcedbyconstraint(13),whichensuresthat
thedifferenceinloadbeforeandafterservingacustomerequalsthecustomer’sdemand. Thevehicle
capacityconstraint(14)ensuresthattheloadcarriedbyavehicledoesnotexceeditscapacityandis
sufficienttomeetthenextcustomer’sdemand.
A.2 OMDCPDP
A.2.1 ProblemDefinition
TheOMDCPDP(OpenMulti-DepotCapacitatedPickupandDeliveryProblem)isapracticalvariant
of the pickup and delivery problem in which agents have a stacking limit of orders that can be
16carried at any given time. Pickup and delivery locations are paired, and pickups must be visited
beforedeliveries. Multipleagentsstartfromdifferentdepotswithoutreturning(open). Thegoalisto
minimizethesumofarrivaltimestodeliverylocations,i.e.,minimizingthecumulativelateness.
A.2.2 MathematicalFormulation
Indices
i,j Nodeindices,wherei,j =1,...,2N
k Vehicleindex,wherek =1,...,m
Sets
P Setofpickupnodes,P ={1,...,N}
D Setofdeliverynodes,D ={N +1,...,2N}
Parameters
N Numberofpickup-deliverypairs
m Numberofvehicles
c Traveltimebetweennodesiandj
ij
Q Capacity(stackinglimit)ofvehiclek
k
o Initiallocation(depot)ofvehiclek
k
DecisionVariables
(cid:26)
1 ifvehiclektravelsdirectlyfromnodeitonodej
x
ijk 0 otherwise
(cid:26)
1 ifvehiclekvisitsnodei
y
ik 0 otherwise
t Arrivaltimeatnodei
i
l Loadofvehiclekaftervisitingnodei
ik
ObjectiveFunction:
2N
(cid:88)
min t (17)
i
i=N+1
Subjectto:
m
(cid:88)
y =1 i=1,...,2N (18)
ik
k=1
2N
(cid:88)
x =1 k =1,...,m (19)
ok,j,k
j=1
2N 2N
(cid:88) (cid:88)
x − x =0 j =1,...,2N, k =1,...,m (20)
ijk jhk
i=1 h=1
2N
(cid:88)
y = x i=1,...,2N, k =1,...,m (21)
ik ijk
j=1
t +c −M(1−x )≤t i,j =1,...,2N, k =1,...,m (22)
i ij ijk j
t ≤t i∈P (23)
i i+N
l +1−M(1−x )≤l i∈P,j ̸=i+N,k =1,...,m (24)
ik ijk jk
l −1+M(1−x )≥l i∈D,j ̸=i−N,k =1,...,m (25)
ik ijk jk
0≤l ≤Q i=1,...,2N, k =1,...,m (26)
ik k
x ,y ∈{0,1} i,j =1,...,2N, k =1,...,m (27)
ijk ik
t ≥0 i=1,...,2N (28)
i
17ConstraintsExplanations: Equation(18)ensuresthateachnodeisvisitedexactlyonce. Constraint
(19)guaranteesthateachvehiclestartsfromitsdesignateddepot. Theflowconservationconstraint
(20)ensuresroutecontinuityforeachvehicle. Equation(21)definestherelationshipbetweenxandy
variables. Timeconsistencyisenforcedbyconstraint(22),while(23)ensuresthatpickupsarevisited
beforetheircorrespondingdeliveries. Constraints(24)and(25)managetheloadchangesduring
pickupanddeliveryoperations. Finally,thevehiclecapacityconstraint(26)ensuresthattheload
neverexceedsthevehicle’sstackinglimit.
A.3 FFSP
A.3.1 ProblemDefinition
Theflexibleflowshopproblem(FFSP)isachallengingandextensivelystudiedoptimizationproblem
inproductionscheduling,involvingN jobsthatmustbeprocessedacrossi=1...S stages,each
withmultiplemachines(m >1). Jobsfollowaspecifiedsequencethroughthesestages,butwithin
i
eachstage,anyavailablemachinecanprocessthejob,withthekeyconstraintthatnomachinecan
handlemorethanonejobsimultaneously. TheFFSPcannaturallybeviewedasamulti-agentCO
problem by considering each machine as an agent that constructs its own schedule. Adhering to
autoregressiveCO,agentsconstructtheschedulesequentially,selectingonejob(ornojob)atatime.
Thejobselectedbyamachine(agent)ataspecificstageinthedecodingprocessisscheduledatthe
earliestpossibletime,thatis,themaximumofthetimethejobbecomesavailableintherespective
stage(i.e.,thetimethejobfinishedonpriorstages)andthemachinebecomingidle. Theprocess
repeatsuntilalljobsforeachstagehavebeenscheduled,andtheultimategoalistominimizethe
makespan,i.e.,thetotaltimerequiredtocompletealljobs.
A.3.2 MathematicalFormulation
WeusethemathematicalmodeloutlinedinKwonetal.(2021)todefinetheFFSP:
Indices
i Stageindex
j,l Jobindex
k Machineindexineachstage
Parameters
N Numberofjobs
S Numberofstages
m Numberofmachinesinstagei
i
M Averylargenumber
p Processingtimeofjobj instageionmachinek
ijk
Decisionvariables
C Completiontimeofjobj instagei
ij
(cid:26)
1 ifjobj isassignedtomachinekinstagei
X
ijk 0 otherwise
(cid:26)
1 ifjoblisprocessedearlierthanjobj instagei
Y
ilj 0 otherwise
Objective:
(cid:18) (cid:19)
min max{C } (29)
Sj
j=1..n
18Subjectto:
(cid:88)mi
X =1 i=1,...,S; j =1,...,N (30)
ijk
k=1
Y =0 i=1,...,S; j =1,...,N (31)
iij
 
(cid:88)N (cid:88)N (cid:88)mi (cid:88)n
Y
ilj
= max (X ijk−1),0 i=1,...,S (32)
j=1l=1 k=1 j=1
(cid:18) (cid:19)
Y ≤max max {X +X }−1,0 i=1,...,S; j,l=1,2,...,N
ilj ijk ilk
k=1...mi
(33)
N
(cid:88)
Y ≤1 i=1,2,...,S; j =1,2,...,N
ilj
l=1
(34)
N
(cid:88)
Y ≤1 i=1,2,...,S; l=1,2,...,N
ilj
j=1
(35)
(cid:88)m1
C ≥ p ·X j =1,2,...,N (36)
1j 1jk 1jk
k=1
(cid:88)mi
C ≥C + p ·X i=2,3,...,S; j =1,2,...,N
ij i−1j ijk ijk
k=1
(37)
(cid:88)mi
C +M(1−Y )≥C + p ·X i=1,2,...,S; j,l=1,2,...,N
ij ilj il ijk ijk
k=1
(38)
ConstraintExplanations: Here, theobjectivefunctionEq.(29)minimizesthemakespanofthe
resulting schedule, that is, the completetion time of the job that finishes last. The schedule has
toadheretoseveralconstraints: First,constraintsetEq.(30)ensuresthateachjobisassignedto
exactlyonemachineateachstage. ConstraintsetsEq.(31)throughEq.(35)definetheprecedence
relationships between jobs within a stage. Specifically, constraint set Eq. (31) ensures that a job
hasnoprecedencerelationshipwithitself. ConstraintsetEq.(32)ensuresthatthetotalnumberof
precedence relationships in a stage equals N −m minus the number of machines with no jobs
i
assigned. ConstraintsetEq.(33)dictatesthatprecedencerelationshipscanonlyexistamongjobs
assignedtothesamemachine. Additionally,constraintsetsEq.(34)andEq.(35)restrictajobto
havingatmostoneprecedingjobandonefollowingjob.
Movingon,constraintsetEq.(36)specifiesthatthecompletiontimeofajobinthefirststagemust
beatleastaslongasitsprocessingtimeinthatstage. Therelationshipbetweenthecompletiontimes
ofajobinconsecutivestagesisdescribedbyconstraintsetEq.(37). Finally,constraintsetEq.(38)
ensuresthatnomorethanonejobcanbeprocessedonthesamemachinesimultaneously.
B ExperimentalDetails
B.1 HCVRP
B.1.1 Baselines
SISR TheSlackInductionbyStringRemovals(SISR)approach(ChristiaensandVandenBerghe,
2020)offersaheuristicmethodforaddressingvehicleroutingproblems(VRPs),focusingonsimplify-
19ingtheoptimizationprocess. Itcombinestechniquesforroutedismantlingandreconstruction,along
withvehiclefleetminimizationstrategies. SISRisappliedacrossvariousVRPscenarios,including
thosewithspecificpickupanddeliverytasks. Inourexperiments,weadheretothehyperparameters
provided in the original paper with c¯ = 10,Lmax = 10,α = 10−3,β = 10−2,T = 100,T =
0 f
1,iter=3×105×N.
GA The Genetic Algorithm (GA) (Karakaticˇ and Podgorelec, 2015) is used to address vehicle
routingproblems(VRPs)andotherNP-hardchallengesbysimulatingnaturalevolutionaryprocesses.
ParticularlyeffectiveinMulti-DepotVehicleRoutingProblems(MDVRP),GAquicklygenerates
adequatesolutionswithreasonablecomputationalresources. Inourexperiment,wefollowthesame
carefully tuned hyperparameters from Liu et al. (2024c) with n = 200,iter = 40×N,P =
m
0.8,P =1.
c
SA TheSimulatedAnnealing(SA)method(˙Ilhan,2021)targetsthecapacitatedvehiclerouting
problem(CVRP)usingapopulation-basedapproachcombinedwithcrossoveroperators. Itincorpo-
rateslocalsearchandtheimproved2-optalgorithmtorefineroutesalongsidecrossovertechniquesto
speedupconvergence. Inourexperiment,wefollowthesamecarefullytunedhyperparametersfrom
Liuetal.(2024c)withT =100,T =10−7,L=20×N,α=0.98.
0 f
AM The Attention Model (AM) (Kool et al., 2018) applies the attention mechanism to tackle
combinatorialoptimizationproblemsliketheTravelingSalesmanandVehicleRoutingProblems. It
utilizesattentionlayersformodelimprovementandtrainsusingREINFORCEwithdeterministic
rollouts. Inourstudies,weadoptadjustmentsfromtheDRLLiframework,whichinvolvesselecting
vehiclessequentiallyandthenchoosingthenextnodeforeach. Additionally,vehicle-specificfeatures
areincorporatedintothecontextvectorgenerationtodistinguishbetweendifferentvehicles.
ET The Equity-Transformer (ET) approach (Son et al., 2024) addresses large-scale min-max
routingproblemsbyemployingasequentialplanningapproachwithsequencegeneratorslikethe
Transformer. Itfocusesonequitableworkloaddistributionamongmultipleagents, applyingthis
strategy to challenges like the min-max multi-agent traveling salesman and pickup and delivery
problems. Inourexperiments,wemodifythedecodermaskinETtogeneratefeasiblesolutionsfor
HCVRP,andintegratevehiclefeaturesintoboththeinputlayerandthecontextencoder.
DRL TheDRLapproachforsolvingHCVRPbyLietal.(2022)employsatransformerarchitec-
Li
turesimilartoKooletal.(2018)inwhichthevehicleandnodeselectionhappensintwostepsviaa
twoselectiondecoder,thusrequiringtwoactions. Weemploytheiroriginalmodelwithadditional
contextofvariablevehiclespeeds,notingthatintheoriginalsettingeachmodelwastrainedona
singledistributionofnumberofagentsm,eachwithalwaysthesamecharacteristics.
2D-Ptr The2DArrayPointernetwork(2D-Ptr)(Liuetal.,2024c)addressestheheterogeneous
capacitatedvehicleroutingproblem(HCVRP)byusingadual-encodersetuptomapvehiclesand
customernodeseffectively. Thisapproachfacilitatesdynamic,real-timedecision-makingforroute
optimization. Itsdecoderemploysa2Darraypointerforactionselection,prioritizingactionsover
vehicles. The model is designed to adapt to changes in vehicle and customer numbers, ensuring
robustperformanceacrossdifferentscenarios. Forourstudy,2D-Ptrwasconfiguredfollowingthe
originalstudy’sparametersforconsistentcomparisonwith8heads,128hiddendimensionsformodel
structure;lr=10−4,λ =0.995,κ =3forturning;50epochs,2500batchesperepoch,512
Adam L2
instancesperbatchfortraining.
B.1.2 Datasets
Traindatageneration NeuralbaselinesweretrainedwiththespecificnumberofnodesN and
numberofagentsmtheyweretestedon. InPARCO,weselectavaryingsizeandnumberofcustomer
trainingschemes: ateachtrainingstep,wesampleN ∼U(60,100)andm∼U(3,7). Asweshow
intheTable1,asinglePARCOmodelcanoutperformbaselinemodelsevenwhentheywerefitted
onaspecificdistribution. Thecoordinatesofeachcustomerlocation(x ,y ),wherei=1,...,N,
i i
are sampled from a uniform distribution U(0.0,1.0) within a two-dimensional space. The depot
locationissimilarlysampledusingthesameuniformdistribution. Thedemandd foreachcustomer
i
iisalsodrawnfromauniformdistributionU(1,10),withthedepothavingnodemand,i.e.,d =0.
0
20Eachvehiclek,wherek =1,...,m,isassignedacapacityQ sampledfromauniformdistribution
k
U(20,41). Thespeedf ofeachvehicleisuniformlydistributedwithintherangeU(0.5,1.0).
k
Testing Testing is performed on the 1280 instances per test setting from Liu et al. (2024c). In
Table1,(g.)referstothegreedyperformanceofthemodel,i.e.,takingasingletrajectorybytakingthe
maximumactionprobability;(s.) referstosampling1280solutionsinthelatentspaceandselecting
theonewiththelowestcost(i.e.,highestreward). WereportoptimalitygapstoORToolsandsolution
timeinsecondsinparentheses.
B.1.3 PARCONetworkHyperparameters
Encoder Initial Embedding. This layer projects initial raw features to hidden space. For the
depot,theinitialembeddingisthepositionalencodingofthedepot’slocationX . Foragents,the
0
initialembeddingistheencodingfortheinitiallocation,capacity,andspeed. MainEncoder. we
employL=3attentionlayersintheencoder,withhiddendimensiond =128,8attentionheads
h
intheMHA,MLPhiddendimensionsetto512, withRMSNorm(ZhangandSennrich,2019)as
normalizationbeforetheMHAandtheMLP.
Decoder Context Embedding. This layer projects dynamic raw features to hidden space. The
contextistheembeddingforthedepotstates,currentnodestates,currenttime,remainingcapacities,
time of backing to the depot, and number of visited nodes. These features are then employed to
updatemultiplequeriesq , k =1,...,msimultaneously. MainDecoder. Similarlytotheencoder,
k
we employ the same hidden dimension and number of attention heads for the Multiple Pointer
Mechanism.
CommunicationLayer Weemployasingletransformerlayerwithhiddendimensiond =128,8
h
attentionheadsintheMHA,MLPhiddendimensionsetto512,withRMSNorm(ZhangandSennrich,
2019)asnormalizationbeforetheMHAandtheMLP.Unliketheencoderlayer,whichactsbetween
allm+N nodes,communicationlayersarelighterbecausetheycommunicatebetweenmagents.
AgentHandler WeusetheHighProbabilityHandlerformanagingconflicts: priorityisgivento
theagentwhose(log-)probabilityofselectingtheconflictingactionaisthehighest. Inotherwords,
prioritiesp =logp (a |x)fork =1,...,m.
k θ k
B.1.4 PARCOTrainingHyperparameters
Foreachproblemsize,wetrainasinglePARCOmodelthatcaneffectivelygeneralizeovermultiple
sizeandagentdistributions. WetrainPARCOwithRLviaSymNCO(Kimetal.,2022)withK =10
symmetricaugmentationsassharedREINFORCEbaselinefor100epochsusingtheAdamoptimizer
(Kingma and Ba, 2014) with a total batch size 512 (using 4 GPUs in Distributed Data Parallel
configuration)andaninitiallearningrateof10−4withastepdecayfactorof0.1afterthe80thand
95thepochs. Foreachepoch,wesample4×105randomlygenerateddata. Trainingtakesaround15
hoursinourconfiguration.
B.2 OMDCPDP
B.2.1 Datasets
Traindatageneration Neuralbaselinesweretrainedwiththespecificnumberofnodes2N and
numberofagentsmtheyweretestedon. InPARCO,weselectavaryingsizeandnumberofcustomer
training schemes: at each training step, we sample 2N ∼ U(50,100) and m ∼ U(10,50). As
weshowintheTable2,asinglePARCOmodelcanoutperformbaselinemodelsevenwhenthey
were fitted on a specific distribution. The coordinates of each customer location (x ,y ), where
i i
i=1,...,2N,aresampledfromauniformdistributionU(0.0,1.0)withinatwo-dimensionalspace.
Similarly,wesampleminitialvehiclelocationsfromthesamedistribution. Wesetthedemandd for
i
eachcustomerto1andthecapacityofeachvehicleto3. Thisemulatesrealisticsettingsinwhicha
singlepackagepercustomerwillbepickedupanddelivered.
21Testing Testingisperformedon100newinstancesforeachsettingofN andminTable2withthe
distributionsfromthetrainingsettings. WefinallytestPARCOonlarge-scale,real-worlddatafrom
complex,realisticdistributionswith32instancesforeachvehicle,pickup,anddeliverylocationin
SeoulCity,SouthKorea. Thisdatasetisprovidedinoursourcecode. Fig.4providesanexample
ofsuchdata. InTable2, (g.) referstothegreedyperformanceofthemodel, i.e., takingasingle
trajectorybytakingthemaximumactionprobability;(s.) referstosampling1280solutionsinthe
latentspaceandselectingtheonewiththelowestcost(i.e.,highestreward).
Figure4:Real-worldinstancefortheOMDCPDPprobleminSeoulCity,SouthKorea,withN =1000locations
andm=100agents(green)showingrelations(gray)ofpickups(redtriangles)andtheirrespectivedeliveries
(blackcrosses).
B.2.2 Baselines
OR-Tools TheOR-Tools(FurnonandPerron,2024)isopen-sourcesoftwaredesignedtoaddress
variouscombinatorialoptimizationproblems. Thistoolkitoffersacomprehensivesuiteofsolvers
suitableforlinearprogramming,mixed-integerprogramming,constraintprogramming,androuting
andschedulingchallenges. SpecificallyforroutingproblemsliketheOMDCPDP,OR-Toolscan
integrateadditionalconstraintstoenhancesolutionaccuracy.DocumentationforthePythonOR-Tools
library,alongwithexamplecodesfordiverseoptimizationchallenges,isreadilyaccessibleontheir
officialwebsite. Forourexperiments,wemaintainedconsistentparametersacrossvariousproblem
sizes and numbers of agents. We configured the global span cost coefficient to 10,000, selected
PATH_CHEAPEST_ARCastheinitialsolutionstrategy, followedbyGUIDED_LOCAL_SEARCH
forlocaloptimization. Thesolvingtimewascappedat300secondstostandardizecomparisonand
measureefficiency.
HAM The Heterogeneous Attention Model (HAM) (Li et al., 2021a) utilizes a neural network
integratedwithaheterogeneousattentionmechanismthatdistinguishesbetweentherolesofnodes
andenforcesprecedenceconstraints,ensuringthecorrectsequenceofpickupanddeliverynodes.
Thisapproachhelpsthedeepreinforcementlearningmodeltomakeinformednodeselectionsduring
routeplanning. WeusedthesamehyperparametersettingsfromtheoriginalHAMexperiments.
MAPDP TheMulti-AgentReinforcementLearning-basedFrameworkforCooperativePickupand
DeliveryProblem(MAPDP)(Zongetal.,2022)pioneerstheuseofmulti-agentreinforcementlearning
(MARL)forthecooperativePDPwithmultiplevehicleagents. Thisinnovativeframeworkintroduces
acentralizedMARLarchitecturetogeneratecooperativedecisionsamongagents,incorporatinga
pairedcontextembeddingtocapturetheinter-dependencyofheterogeneousnodes. Weadaptedthe
MAPDPtofitourOMDCPDPtask,utilizingthesameencoderasPARCOtoensureafaircomparison.
Forthedecoderandtrainingphases,wekeptthesamerandomconflicthandlerandretainedthesame
hyperparametersasdetailedintheoriginalstudywith8heads,128hiddendimensionsformodel
structure;lr=10−3,L =3forturning.
Adam
22B.2.3 PARCONetworkHyperparameters
MosthyperparametersarekeptsimilartoAppendixB.1.3;wereportallofthemnonethelessbelow.
Encoder InitialEmbedding. Thislayerprojectsinitialrawfeaturestohiddenspace. Fordepots,
theinitialembeddingsencodethelocationo andtherespectivevehicle’scapacityQ . Forpickup
k k
nodes,theinitialembeddingsencodethelocationandpaireddeliverynodes’location. Fordelivery
nodes,theinitialembeddingsencodethelocationandpairedpickupnodes’location. MainEncoder.
Weemployl=3attentionlayersintheencoder,withhiddendimensiond =128,8attentionheads
h
intheMHA,MLPhiddendimensionsetto512, withRMSNorm(ZhangandSennrich,2019)as
normalizationbeforetheMHAandtheMLP.
Decoder Context Embedding. This layer projects dynamic raw features to hidden space. The
context is the embedding for the depot states o , current node states, current length, remaining
k
capacity,andnumberofvisitednodes. Thesefeaturesarethenemployedtoupdatemultiplequeries
q , k = 1,...,msimultaneously. MainDecoder. Similarlytotheencoder,weemploythesame
k
hiddendimensionandnumberofattentionheadsfortheMultiplePointerMechanism.
CommunicationLayer Weemployasingletransformerlayerwithhiddendimensiond =128,8
h
attentionheadsintheMHA,MLPhiddendimensionsetto512,withRMSNorm(ZhangandSennrich,
2019)asnormalizationbeforetheMHAandtheMLP.Notethatunliketheencoderlayer,whichacts
betweenallm+N nodes,communicationlayersarelighterbecausetheycommunicatebetweenm
agents.
AgentHandler WeusetheHighProbabilityHandlerformanagingconflicts: priorityisgivento
theagentwhose(log-)probabilityofselectingtheconflictingactionaisthehighest. Inotherwords,
prioritiesp =logp (a |x)fork =1,...,m.
k θ k
B.2.4 PARCOTrainingHyperparameters
Foreachproblemsize,wetrainasinglePARCOmodelthatcaneffectivelygeneralizeovermultiple
sizeandagentdistributions. WetrainPARCOwithRLviaSymNCO(Kimetal.,2022)withK =8
symmetricaugmentationsassharedREINFORCEbaselinefor100epochsusingtheAdamoptimizer
(KingmaandBa,2014)withatotalbatchsize128onasingleGPUandaninitiallearningrateof
10−4withastepdecayfactorof0.1afterthe80thand95thepochs. Foreachepoch,wesample105
randomlygenerateddata. Trainingtakeslessthan4hoursinourconfiguration.
B.3 FFSP
B.3.1 Baselines
MatNet ThecomparisonofPARCOwithotherFFSPbaselinesisshownintableTable3. Beinga
neuralsolver,webenchmarkPARCOmainlyagainstMatNet(Kwonetal.,2021),theSotANCO
architecture for the FFSP. MatNet is an encoder-decoder architecture, which is inspired by the
attentionmodel(Kooletal.,2018). Itextendstheencoderoftheattentionmodelwithadualgraph
attentionlayer,ahorizontalstackoftwotransformerblocks,capabletoencodenodesofdifferent
typesinabipartitegraph-likemachinesandjobsintheFFSP.Kwonetal.(2021)trainMatNetusing
POMO(Kwonetal.,2020).
CPLEX BesidestheaforementionedMatNetmodel,weimplementthemathematicalmodelde-
scribedaboveintheexactsolverCPLEXwithatimebudgetof60and600secondsperinstance.
However,withbothtimebudgets,CPLEXisonlycapableofgeneratingsolutionstotheFFSP20
instances.
RandomandShortestJobFirst Third,weusedifferent(meta-)heuristicstobenchmarkPARCO
ontheFFSP.TheRandomandShortestJobFirst(SJF)heuristicsaresimpleconstructionstrategies
that build valid schedules in an iterative manner. Starting from an empty schedule, the Random
constructionheuristiciteratesthroughtimestepst=0,...T andstagesi=1...S andrandomly
assignsjobsavailableatthegiventimetoanidlemachineoftherespectivestageuntilalljobsare
23scheduled. Likewise,theSJFproceedsbyassigningjob-machinepairswiththeshortestprocessing
timefirst.
Genetic Algorithm The Genetic Algorithms (GA) are metaheuristics widely used by the OR
community to tackle the FFSP (Kahraman et al., 2008). The GA iteratively improves multiple
candidatesolutionscalledchromosomes. EachchromosomeconsistsofS×N realnumbers,where
S isthenumberofstagesandN isthenumberofjobs. Foreachjobateachstage,theintegerpartof
thecorrespondingnumberindicatestheassignedmachineindex,whilethefractionalpartdetermines
jobprioritywhenmultiplejobsareavailablesimultaneously. Childchromosomesarecreatedthrough
crossover,inheritingintegerandfractionalpartsindependentlyfromtwoparents. Mutations,applied
witha30%chance,useoneoffourrandomlyselectedmethods: exchange,inverse,insert,orchange.
Theimplementationuses25chromosomes. OneinitialchromosomeissettotheShortestJobFirst
(SJF)heuristicsolutionandthebest-performingchromosomeispreservedacrossiterations. Each
instancerunsfor1,000iterations.
ParticleSwarmOptimization Finally,ParticleSwarmOptimization(PSO)iterativelyupdates
multiplecandidatesolutionscalledparticles,whichareupdatedbytheweightedsumoftheinertial
value, the local best, and the global best at each iteration (Singh and Mahapatra, 2012). In this
implementation,particlesusethesamerepresentationasGAchromosomes. Thealgorithmemploys
25particles,withaninertialweightof0.7andcognitiveandsocialconstantssetto1.5. Oneinitial
particlerepresentstheSJFheuristicsolution. LikeGA,PSOrunsfor1,000iterationsperinstance.
B.3.2 Datasets
Traindatageneration WefollowtheinstancegenerationschemeoutlinedinKwonetal.(2021)
sampleprocessingtimesforjob-machinepairsindependentlyfromauniformdistributionwithin
thebounds[2,10]. ForthemainexperimentsontheFFSPshowninTable3,wealsousethesame
instancesizesasKwonetal.(2021)withN =20,50and100jobs(denotedFFSP20,FFSP50and
FFSP100,respectively)thatneedtobeprocessedinS = 3stagesonm = 4machineseach(i.e.
i
m=12machinesintotal). FortheagentsensitivityanalysisshowninTable4,wefixthenumberof
jobsto50butalterthenumberofagents. Still,weuseS =3forthisexperiment,butalterthenumber
ofmachinesperstagetom =6,8and10,yieldingatotalof18,24and30agents,respectively.
i
Testing Testingisperformedon1,000testinstancesgeneratedaccordingtotheabovesampling
scheme. WeusethesameinstancesasKwonetal.(2021)fortheexperimentsofTable3andgenerate
newonesfortheexperimentsofTable4.
B.3.3 PARCONetworkHyperparameters
Encoder TosolvetheFFSPwithourPARCOmethod,weuseasimilarencoderasKwonetal.
(2021). The MatNet encoder generates embeddings for all machines of all stages and the jobs
they need to process, plus an additional dummy job embedding, which can be selected by any
machineineachdecodingsteptoskiptothenextstep. TocomparePARCOwithMatNet,weuse
similarhyperparametersforbothmodels. WeuseL=3encoderlayers,generatingembeddingsof
dimensionalityd =256,whicharesplitoverh=16attentionheadsintheMHAlayers. Further,
h
we employ Instance Normalization (Ulyanov et al., 2016) and a feed-forward network with 512
neuronsinthetransformerblocksoftheencoder.
Decoder The machines are regarded as the agents in our PARCO framework. As such, their
embeddingsareusedasqueriesQsintheMultiplePointerMechanismEq.(5),whilejobembeddings
t
areusedasthekeysandvalues. Ineachdecodingstep,themachineembeddingsarefusedwitha
projectionofthetimetherespectivemachinebecomesidle. Similarly,jobembeddingsareaugmented
withalineartransformationofthetimetheybecomeavailableintherespectivestagebeforeentering
theattentionheadinEq.(4).
CommunicationLayer Weemployasingletransformerblockwithhiddendimensiond =256
h
andh=16attentionheadsintheMHA,anMLPwith512hiddenunitsandInstanceNormalization.
24AgentHandler WeusetheHighProbabilityHandlerformanagingconflicts:priorityisgiventothe
agentwhose(log-)probabilityofselectingtheconflictingactionisthehighest. Formally,priorities
p =logp (a |x)fork =1,...,m.
k θ k
B.3.4 PARCOTrainingHyperparameters
Regardingthetrainingsetup,eachtraininginstanceiisaugmentedbyafactorof24,andtheaverage
makespanovertheaugmentedinstancesisusedasasharedbaselinebshared fortheREINFORCE
i
gradientestimatorofEq.(9). WeusetheADAMoptimizer(KingmaandBa,2014)withalearning
rateof4×10−4,whichwealterduringtrainingusingacosineannealingscheme. Wetrainseparate
models for FFSP20, FFSP50 and FFSP100 instances for 100, 150 and 200 epochs, respectively,
using1,000randomlygeneratedinstancesperepochsplitintobatchesofsize50.3 Forthelearned
methods, thetimesgiveninparenthesisindicatethetimerequiredtotraintherespectivemodels,
whereastheothertimeinformationindicatesthetimerequiredtosolveasingleinstance. Forthe
agentsensitivityanalysisinTable4,Matnet,aswellasourPARCOmodel,aretrainedfor20epochs
on 1,000 instances per epoch. Duration in this table indicates the time required for training the
respectivemodelsonthehardwarespecifiedinAppendixB.4.
B.3.5 DiagramforMatNetDecodingvs. PARCODecodingfortheFFSP
ThefollowingfiguresvisualizethedecodingforthemachinesofagivenstageusingMatNetand
PARCO. As one can see in Fig. 5, MatNet requires a decoder forward pass for each machine to
scheduleajoboneachofthem. Incontrast,asdetailedinFig.6,PARCOcanschedulejobsonall
machinessimultaneouslythroughitsMultiplePointerMechanismandAgentHandler,leadingto
significantefficiencygains.
B.4 HardwareandSoftware
B.4.1 Hardware
ExperimentswerecarriedoutonamachineequippedwithtwoINTEL(R) XEON(R) GOLD 6338
CPU@2.00GHZCPUswithatotal128threadsand8NVIDIARTX4090graphiccardswith24
GBofVRAM.Duringinference,weemployonlyoneCPUandasingleGPU.
B.4.2 Software
WeusedPython3.12,PyTorch2.4(Paszkeetal.,2019)coupledwithPyTorchLightning(Falcon
andThePyTorchLightningteam,2019)withmostcodebasedontheRL4COlibrary(Bertoetal.,
2024a). ThemainOSisUbuntu22.04.4LTS.Therewerenomajordifficultieswhentryingthecode
onotherplatforms,soweexpectexperimentstobereproducibleinotherarchitectures.
B.5 SourceCode
Wevalueopenreproducibility. WeprovidethesourcecodetoreproduceourexperimentsonGithub
athttps://github.com/ai4co/parco.
C AdditionalDiscussion
FollowingSection6,weoutlinefurtherdiscussionhere. Weadditionallyremarkthatseveralareasof
multi-agentplanning,includingmulti-agentpathfinding(LinandMa,2023;Wangetal.,2023;Tang
etal.,2024a)andsocialrobotnavigation(Lietal.,2024a;Heetal.,2024)employcommunicationas
atoolwithgreatimportance,fromwhichwealsoborrowideassuchasprioritizedconflictresolution
(Tangetal.,2024b).
Anexcitingfuturedirectiongoingforwardisthedevelopmentoffoundationmodels,inwhichPARCO
couldaidinimprovingtheefficiencyandperformanceinsolvingmulti-agentCOproblems. Recent
examplesintheliteraturehavedemonstratedthepossibilityofscalingupsolverswith(self)supervision
3Note:toavoidOOMs,ForFFSP100instances,batchesarefurthersplitintomini-batchesofsize25whose
gradientsareaccumulated.
25𝑡=0 5 10
𝑘=1 𝑗=4
𝑘=2 𝑗=1
𝑘=3 𝑗=5
𝑘=4 𝑗=3
𝑗!)* "+=4 𝑗!)* #+=1 𝑗!)* $+=5 𝑗!)* %+=3
𝑝&",𝑝&#,⋯,𝑝&’,𝑝&’(" 𝑝&",𝑝&#,⋯,𝑝&’,𝑝&’(" 𝑝&",𝑝&#,⋯,𝑝&’,𝑝&’(" 𝑝&",𝑝&#,⋯,𝑝&’,𝑝&’("
𝑡=0
Encoder Decoder Decoder Decoder Decoder
𝑒 𝑒 𝑒𝑚 𝑚 𝑚𝑏 𝑏 𝑏! ! !" #
$
𝑒 𝑒 𝑒𝑚 𝑚 𝑚𝑏 𝑏 𝑏& & &" #
$
𝑒𝑚 𝑞𝑏!" 𝑒 𝑒 𝑒𝑚 𝑚 𝑚𝑏 𝑏 𝑏& & &" #
$
∞0
0
𝑒𝑚 𝑞𝑏!# 𝑒 𝑒 𝑒𝑚 𝑚 𝑚𝑏 𝑏 𝑏& & &" #
$
∞0
0
𝑒𝑚 𝑞𝑏!$ 𝑒 𝑒 𝑒𝑚 𝑚 𝑚𝑏 𝑏 𝑏& & &" #
$
∞ ∞
0
𝑒𝑚 𝑞𝑏!% 𝑒 𝑒 𝑒𝑚 𝑚 𝑚𝑏 𝑏 𝑏& & &" #
$
∞0
0
𝑒𝑚𝑏!% ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮ ⋮
𝑒𝑚𝑏&’ 𝑒𝑚𝑏&’ 0 𝑒𝑚𝑏&’ 0 𝑒𝑚𝑏&’ 0 𝑒𝑚𝑏&’ 0
𝑒𝑚𝑏&’(" 𝑒𝑚𝑏&’(" 0 𝑒𝑚𝑏&’(" 0 𝑒𝑚𝑏&’(" 0 𝑒𝑚𝑏&’(" 0
𝐾,𝑉 𝑚𝑎𝑠𝑘 𝐾,𝑉 𝑚𝑎𝑠𝑘 𝐾,𝑉 𝑚𝑎𝑠𝑘 𝐾,𝑉 𝑚𝑎𝑠𝑘
𝑡=𝑡+1
Figure5:AFFSPDecodingStepwithMatNet
𝑡=0 5 10
𝑝!% $$ 𝑝!% &$ ⋯ 𝑝!% "$ 𝑝!% "$ #$ 𝑗!)* "+=4 𝑗=4 𝑘=1
𝑝!% $& 𝑝!% && ⋯ 𝑝!% "& 𝑝!% "& #$ 𝑗!)* #+=1 𝑗=1 𝑘=2
𝑝!% $’ 𝑝!% &’ ⋯ 𝑝!% "’ 𝑝!% "’ #$ 𝑗!)* $+=5 𝑗=5 𝑘=3
𝑝!% $( 𝑝!% &( ⋯ 𝑝!% "( 𝑝!% "( #$ 𝑗!)* %+=3 𝑗=3 𝑘=4
𝑡=0
Encoder Decoder
𝑒 𝑒 𝑒𝑚 𝑚 𝑚𝑏 𝑏 𝑏! ! !" #
$
𝑒 𝑒 𝑒𝑚 𝑚 𝑚𝑏 𝑏 𝑏& & &" #
$
𝑒 𝑒 𝑒𝑚 𝑚 𝑚𝑏 𝑏 𝑏! ! !" #
$
𝑒 𝑒 𝑒𝑚 𝑚 𝑚𝑏 𝑏 𝑏& & &" #
$
∞0
0
𝑒𝑚𝑏!% ⋮ 𝑒𝑚𝑏!% ⋮ ⋮
𝑒𝑚𝑏&’ 𝑒𝑚𝑏&’ 0
𝑒𝑚𝑏&’(" 𝑄 𝑒𝑚𝑏&’(" 0
𝐾,𝑉 𝑚𝑎𝑠𝑘
Figure6:AFFSPDecodingStepwithPARCO
(Drakulicetal.,2023;Luoetal.,2023;Corsinietal.,2024;PirnayandGrimm,2024;Luoetal.,
2024),developingmulti-tasksolversforVRPs(Liuetal.,2024a;Linetal.,2024;Zhouetal.,2024;
Bertoetal.,2024b)aswellasmoregeneralCOtasks(Drakulicetal.,2024;Jiangetal.,2024). Such
approaches model CO problems as sequential autoregressive decision-making; parallelizing this
processwithPARCOcouldenrichsolutionspeed/quality. WenotethatmostNCOapproachesine.g.,
VRPs,mostlyfocusonsolvingwithoutexplicitlymodeling(heterogeneous)agentsasinPARCO,
whichisofpracticalrelevanceinseveralreal-worldtasks. Betterheuristicsforprioritizedplanning
mightalsobefoundutilizingautomateddiscoveryviarecentLLMapproachesforCO(Liuetal.,
2024b;Yeetal.,2024b). Finally,integratingPARCOinonlineoptimization(Hottungetal.,2021;
Chooetal.,2022)andend-to-endpipelinesforconstructionandimprovement,asdonerecentlyin
(Bertoetal.,2024a;Kongetal.,2024),wouldfurtherimprovetheperformanceofPARCOgiven
largercomputationaltimebudgets.
26
Agent
Handler