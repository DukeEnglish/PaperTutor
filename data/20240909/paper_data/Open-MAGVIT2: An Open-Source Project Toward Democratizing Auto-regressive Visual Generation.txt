TechnicalReport
OPEN-MAGVIT2:
AN OPEN-SOURCE PROJECT TOWARD DEMOCRATIZ-
ING AUTO-REGRESSIVE VISUAL GENERATION
ZhuoyanLuo1,2˚ FengyuanShi1,3˚ YixiaoGe1: YujiuYang2 LiminWang3 YingShan1
1ARCLab,TencentPCG 2TsinghuaUniversity 3NanjingUniversity
https://github.com/TencentARC/Open-MAGVIT2
Figure 1: Reconstruction and generation samples of Open-MAGVIT2. We show 1024ˆ1024 recon-
structedsamples(top)and256ˆ256generatedsamples(middleandbottom).
ABSTRACT
WepresentOpen-MAGVIT2,afamilyofauto-regressiveimagegenerationmod-
elsrangingfrom300Mto1.5B.TheOpen-MAGVIT2projectproducesanopen-
source replication of Google’s MAGVIT-v2 tokenizer, a tokenizer with a super-
large codebook (i.e., 218 codes), and achieves the state-of-the-art reconstruction
performance (1.17 rFID) on ImageNet 256ˆ256. Furthermore, we explore its
applicationinplainauto-regressivemodelsandvalidatescalabilityproperties. To
assistauto-regressivemodelsinpredictingwithasuper-largevocabulary,wefac-
torize it into two sub-vocabulary of different sizes by asymmetric token factor-
ization, and further introduce “next sub-token prediction” to enhance sub-token
interactionforbettergenerationquality. Wereleaseallmodelsandcodestofoster
innovationandcreativityinthefieldofauto-regressivevisualgeneration.
˚EqualContribution.WorkdoneduringaninternshipatARCLab,TencentPCG.
:Correspondingauthorandprojectlead.
1
4202
peS
6
]VC.sc[
1v01440.9042:viXraTechnicalReport
Table1: ModelconfigurationsofOpen-MAGVIT2. Wepartiallyfollowthescalingruleproposed
inthepreviousworks(Sunetal.,2024;Tianetal.,2024).
Model Parameters Inter-BlocksN Intra-BlocksL Widthsw Headsh
Open-MAGVIT2-B 343M 24 2 1024 16
Open-MAGVIT2-L 804M 36 3 1280 20
Open-MAGVIT2-XL 1.5B 48 4 1536 24
1 INTRODUCTION
LargeLanguageModels(LLMs),builtuponauto-regressivetransformer(Vaswanietal.,2017;Ope-
nAI,2023;Chowdheryetal.,2022;Touvronetal.,2023),havedemonstrateddominanceinnatural
languagegenerationduetotheincrediblecontextmodelingandscalability. Inspiredbythis,emer-
gent works introduce auto-regressive models into visual generation (Van Den Oord et al., 2017;
Esser et al., 2021; Yu et al., 2022; Lee et al., 2022; Sun et al., 2024). These approaches first uti-
lizeavectorquantizerforimagetokenizationandde-tokenization,thenemployanauto-regressive
transformerfordiscreteimagetokensequencemodeling.
Although great processes are achieved, the quality of visual generation still falls behind the
diffusion-based methods. The main factor is limited tokenizer performance. Tokenizers are gen-
erallypositedastheupperboundofthevisualgeneration,andinferioroff-the-shelftokenizers(e.g.,
VQ-VAE(VanDenOordetal.,2017))willleadtopoorgenerationquality.Althoughsomeimprove-
ments are done (Yu et al., 2022; Lee et al., 2022; Sun et al., 2024), current tokenizers are limited
by the codebook size and utilization, and the reconstruction performance is still far worse than
VAE(Kingma, 2013; Rombach et al., 2022b) used in diffusion models. To unlock the potential of
tokenizers,MAGVIT-v2(Yuetal.,2024a)proposesLookup-FreeQuantizertoenableahighlycode-
activated and super-large codebook, and achieves better generation quality than diffusion models.
However,suchapowerfulvisualtokenizeriscompletelyclosed-sourceandwehavenoaccess
tothissofar,limitingthedevelopmentoftheacademiccommunity.
Inthiswork,wepushforwardtheauto-regressivevisualgenerationintwofolds: 1)Replicationof
thevisualtokenizer:Were-implementtheadvancedLookup-FreeQuantizerproposedbyMAGVIT-
v2. Toourbestknowledge,ouropen-sourcereplicationachievestheclosestreconstructionperfor-
mancestatedinMAGVIT-v2(1.18vs. 1.15rFIDonImageNet128ˆ128)andoutperformsallother
methods on the hallmark Imagenet benchmark (Deng et al., 2009). 2) Integrating a super-large
codebookwithARvisualgeneration: InsteadofsimplyfollowingMAGVIT-v2thatleveragesthe
vision-oriented design (i.e., mask generative methods (Chang et al., 2022) for visual synthesis),
weseektoexploitthepotentialofsuchalargecodebookinvanillaauto-regressivegeneration. To
assist auto-regressive models in predicting with a super-large vocabulary, we factorize it into two
sub-vocabulary of different sizes by asymmetric token factorization, and further introduce “next
sub-token prediction” to enhance sub-token interaction for better generation quality. Our experi-
mentsonthestandardvisualgenerationdatasetImageNetsuggestthat,withthepowerfultokenizer,
theplainauto-regressivemodelexhibitssuperiorityandscalability.
2 METHOD
2.1 OVERVIEW
Open-MAGVIT2iscomposedoftwosignificantstages.Oneisapowerfulvisualtokenizerthatmaps
theinputvisualsignalintothediscretetokenrepresentations.Subsequently,thevector-quantizedse-
quencewillbefedintotheauto-regressivetransformerforintra-andinter-tokenrelationshipmod-
eling,eventuallyforvisualsynthesis.
2.2 VISUALTOKENIZER
Preliminary. Visual tokenization is fundamentally deemed as the crucial component in multi-
modal large language models (MLLMs) to understand the visual signal input. The CNN-based
2TechnicalReport
...
1 𝑇𝑇12 3 𝑇𝑇24 5 𝑇𝑇36 2t-1𝑇𝑇𝑡𝑡2t
MAGVIT2 Intra- Intra- Intra- S Ph .aa r.r a.e md . Intra-
Encoder Block Block Block Block
×L ×L ×L ×L
...
Image C 1 C 3 C 5 C 2t-1
𝑇𝑇𝑡𝑡
LFQ 𝑇𝑇1 𝑇𝑇2 𝑇𝑇3 𝑇𝑇𝑡𝑡
Llama Inter-Block
×N
M DA eG coV dI eT r2 S 1 + 2 3 + 4 ... 2t-3+2t-2
Class Token
+1 +1 ... -1 𝑇𝑇1 Subtok𝑇𝑇e2n C Intra𝑇𝑇-T𝑡𝑡o−k1en
Modeling
Reconstruction
𝑚𝑚2
𝑚𝑚𝐾𝐾 +1 -1 𝒌𝒌... +1 𝐹𝐹
𝐹𝐹
Subtoken C Ton ot ke ex nt In Mte or
d
– eT lio nk gen
𝑚𝑚0
𝑲𝑲−𝒌𝒌
Figure 2: Overview of Open-MAGVIT2. There are two crucial stages in Open-MAGVIT2. In
StageI: theimageisfirstencodedbyMAGVIT-v2Encoderandsubsequentlytransformedintobits
format by Lookup-Free Quantizer (LFQ). In Stage II: The quantized features are further mapped
intodiscretevisualtokensandinputintotheLlama-basedauto-regressiveframeworkforintra-and
inter-tokenrelationshipmodeling.
encoder-quantizer-decoder architecture first proposed in VQVAE (Van Den Oord et al., 2017) is
well adoptedas the visualtokenizer, which maps inputpixels into discreterepresentations and re-
constructsimagesfromquantizedfeatures. Specifically,givenanimageI PR3ˆHˆW,theencoder
projectsitintothefeaturemapZ PRDˆH1ˆW1,whereH1 “H{p,W1 “W{p,andpisthedown-
sample ratio. The quantizer containing a learnable codebook E P R2KˆD then selects the closest
entry zˆ P RD from the codebook for each feature vector z P RD. And we can use discrete token
indicesX “tx uH1ˆW1 torepresentthecontinuousfeaturemapZ. Fordecoding,eachcodeindex
i i“1
willbemappedbacktothequantizedfeaturevectorandinputintothedecoderforpixel-levelimage
reconstruction.
Review of Lookup-Free Quantization. Motivated by the relationship between the size of the
codebook and the dimension of code embeddings, MAGVIT-v2 (Yu et al., 2024a) eliminates the
needforembeddinglookupbyreducingthedimensionofcodeembeddingtozero. Specifically,the
codebook is shrunk into an integer set where the latent space of each entry is decomposed as the
Ś
Cartesianproductofsingle-dimensionalvariables(i.e.,Cˆ “ K t´1,1u,|Cˆ |“2K). Asshownin
i“1
Fig.2,thetokenizationprocesscanbesimplifiedas:
zˆ “signpz q“´1tz ď0u`1tz ą0u, (1)
i i i i
wherezˆ denotesthequantizedrepresentationofthefeaturevectorz . Andthetokenindexforz is
i i i
givenby:
ÿK
Indexpz q“ 2k´11tzˆ ą0u. (2)
i ik
k“1
Toencouragetheconfidentassignmentofeachcodebookentryandutilizationofthewholecodebook
simultaneously,MAGVIT-v2furtherintroducesentropyloss:
ÿ ÿ
1 1
L “ Hpfpzqq´Hp fpzqq, (3)
entropy BH1W1 BH1W1
whereHp¨qdenotestheentropy,Bisbatchsizes,andfp¨qisamappingfunctionfromlatentspaceto
acategoricaldistributionspecifyingtheprobabilityofassignmenttoeachentry. Inourexperiment,
weobservethatreplacingtraditionalcodeassignment(i.e.,pair-wisedistance)withthislookup-free
quantizationenablestrainingasuper-largecodebook(i.e.,218codes)ofhighutilization(100%).
3TechnicalReport
Original LPIPS = 0.315 0.256 0.236
↓
Original LPIPS = 0.194 0.148 0.134
↓
Original LPIPS = 0.085 0.082 0.059
VQGAN LlamaGen Open-MAGVIT2
↓
Figure 3: Reconstruction comparison with different tokenizers. We compare VQGAN (Esser
etal.,2021),LlamaGen(Sunetal.,2024)andourmodelstrainedonImageNet. (Bestviewedwith
zoomingin. TheoriginalimagesarefromUnsplash).
ReviewofArchitectureimprovements. Intuitively,sinceeachcontinuousfeaturevectorwillbe
quantized into K bits, it poses a significant challenge to both the encoder and decoder. There-
fore,were-implementthearchitectureimprovementstechniqueillustratedin(Yuetal.,2024a). 1)
Downsamplersintheencoderarestridedconvolutionswithlearnedkernelswhileupsamplersinthe
decoder are the depth-to-space operator. 2) Following (Karras et al., 2019; Peebles & Xie, 2023;
Huang&Belongie,2017),were-implementtheAdaptiveGroupNormLayer,whichintegratesthe
quantizedvectorwiththeoutputofeachresidualblockinthedecoder.
2.3 AUTO-REGRESSIVETRANSFORMER
Preliminary. Given a sequence of discrete tokens X “ tx uT ,T “ H1 ˆW1 from the visual
i i“1
tokenizer, the auto-regressive transformer predicts the next token x conditioned on the previous
t
tokenstx ,x ,¨¨¨ ,x u:
1 2 t´1
źT
ppx ,x ,¨¨¨ ,x q“ ppx |x ,x ,¨¨¨ ,x q. (4)
1 2 T t 1 2 t´1
t“1
Auto-regressive Architecture. Considering the different scales of auto-regressive transformer
(i.e., from „300M to 1B) and the limited training academic data, directly optimizing such a large
vocabulary(i.e.,218codes)isimpractical.Therefore,weproposetheasymmetrictokenfactorization
technique to assist models in performing “next-token prediction” within concatenated codebooks.
Specifically, the LFQ token’s latent space is factorized into M subspaces tx1uT , tx2uT , ¨¨¨,
i i“1 i i“1
4TechnicalReport
txMuT , each of which contains 2km tokens. As shown in Fig. 2, each subspace is embedded
i i“1
individuallyandtheirsummationisusedasthetransformerinputs. Conventionally,anintuitiveso-
lutiontoperformauto-regressivewithinsubspacesisleveragingM separateheadsforindependent
categoricaldistributionmodeling. However,sincebothsub-tokensarederivedfromthesamelatent
spaces,suchasimpleoperationmayignoretheirintra-correlation. Consequently,inspiredby(Lee
etal.,2022),wereformulatetheautoregressionparadigmintomodelingbothintra-andinter-token
dependency, which is essentially “next sub-token prediction”. In this manner, the representational
capacityofthesuper-largecodebookcanexhibitgreatpotentialinauto-regressivegenerationwith
betterscalability.
1) Inter-token Relationship: Given a set of sub-tokens from the visual tokenizers, a stacked of
LlamablockswithN layersandwwidthareleveragedtocapturethein-contextinformationbetween
tokens. Theprocesscanbeformulatedas:
ÿM ÿM
C “LlamaBlockps,p xiq,¨¨¨ ,p xi qq, (5)
t 1 t´1
i“1 i“1
wheres denotestheconditionaltokens,C
t
PRTˆws isthet-thcontexttoken.
2)Intra-tokenRelationship:WefurtherutilizeatransformerwithLintra-blockstoautoregressively
predicttheeachsub-token(x1,x2,¨¨¨ ,xM)atthepositiont. Byassociatingthesub-tokencondi-
t t t
tionedwithcontextual-enrichedvectorC,theintra-dependencywithintokenscanbewellmodeled.
Formally, at t position, the autoregression of predicting the conditional distribution of each sub-
tokenis:
p “LlamaBlockpC ,x1¨¨¨ ,xm´1q. (6)
tm t t t
Therefore,theauto-regressivelikelihoodisformulatedas:
źT
ppX ,X ,¨¨¨ ,X q“ ppX |X ,X ,¨¨¨ ,X q
1 2 T t 1 2 t´1
t“1
(7)
źT źM
“ ppxm|pX ,X ,¨¨¨ ,X q,px1,x2,¨¨¨xm´1qq,
t 1 2 t´1 t t t
t“1m“1
whereX specifiesasetofsub-tokentx1,x2,¨¨¨ ,xMuateachpositiont.
t t t t
3 EXPERIMENTS
3.1 DATASETANDMETRICS
The training of the visual tokenizer and auto-regressive transformer are both on ImageNet (Deng
etal.,2009). Specifically,wetrainthetokenizerin128ˆ128and256ˆ256resolutions.
Forvisualreconstruction,thereconstruction-FID,denotedasrFID(Heuseletal.,2017),codebook
utilization, the use percentage of codes, and PSNR on ImageNet 50k validation set are adopted to
measurethequalityofreconstructedimages. Simultaneously,wemeasurethequalityofimagegen-
erationbytheprevalentmetricsFID,IS(Salimansetal.,2016)andPrecision/Recall(Kynka¨a¨nniemi
etal.,2019).
3.2 IMPLEMENTATIONSDETAILS
Visual Tokenizer Setup. Open-MAGVIT2 follows the same architecture of the visual tokenizer
proposedin(Yuetal.,2024a). Forcomputationalefficiency, weremovethegradientpenaltyloss,
and adopt PatchGAN (Isola et al., 2017) as the discriminator instead of StyleGAN (Karras et al.,
2019). All models corresponding to different resolutions are trained with similar settings: an ini-
tial 1e ´ 4 learning rate, an Adam Optimizer with β “ 0.5, β “ 0.9, a total 256 batch size
1 2
from 270 to 350 epochs, a combination of reconstruction, GAN, perceptual (Zhang et al., 2018),
entropypenalty(Yuetal.,2024a),commitmentlosses,LeCAMregularization(Tsengetal.,2021)
fortrainingstability,and32ˆNvidiaV100/Ascend910BwithPytorch.
5TechnicalReport
Table2: ModeldesignsandreconstructionperformancecomparisonwiththeoriginalMAGVIT-v2
on128ˆ128ImageNet50kvalidationset,followingtheMAGVIT-v2paper.
Train Large Up/Down Deeper Adaptive
Method Tokens LFQ rFID
Resolution Codebook Sampler Model GroupNorm
Open-MAGVIT2 16ˆ16 128ˆ128 ✓ ✓ ✓ ✓ ✓ 1.18
MAGVIT2(Yuetal.,2024a) 16ˆ16 128ˆ128 ✓ ✓ ✓ ✓ ✓ 1.15
Table3: Reconstructionperformanceofdifferenttokenizerson256ˆ256ImageNet50kvalidation
set. Open-MAGVIT2achievesSOTAresultsondifferentdownsamplingrates. :specifiesthatthe
trainingisonOpenImages. ˚denotesthattheresultsarefromthedirectinferenceusingthemodel
trainedwith128ˆ128resolutionwithoutfine-tuning.
Token Train Codebook Codebook
Method Tokens Ratio rFIDÓ PSNRÒ
Type Resolution Size UsageÒ
VQGAN(Esseretal.,2021) 2D 16ˆ16 16 256ˆ256 1024 7.94 19.4 ´
SD-VQGAN(Rombachetal.,2022a) 2D 16ˆ16 16 256ˆ256 16384 5.15 ´ ´
MaskGIT(Changetal.,2022) 2D 16ˆ16 16 256ˆ256 1024 2.28 ´ ´
LlamaGen(Sunetal.,2024) 2D 16ˆ16 16 256ˆ256 16384 2.19 20.79 97%
Open-MAGVIT2 2D 16ˆ16 16 256ˆ256 262144 1.17 21.90 100%
ViT-VQGAN(Yuetal.,2022) 2D 32ˆ32 8 256ˆ256 8192 1.28 ´ ´
VQGAN:(Esseretal.,2021) 2D 32ˆ32 8 256ˆ256 16384 1.19 23.38 ´
SD-VQGAN:(Rombachetal.,2022a) 2D 32ˆ32 8 256ˆ256 16384 1.14 ´ ´
OmiTokenizer-VQ(Wangetal.,2024) 2D 32ˆ32 8 256ˆ256 8192 1.11 ´ ´
LlamaGen(Sunetal.,2024) 2D 32ˆ32 8 256ˆ256 16384 0.59 24.45 ´
Open-MAGVIT2˚ 2D 32ˆ32 8 128ˆ128 262144 0.34 26.19 100%
Titok-L(Yuetal.,2024b) 1D 32 ´ 256ˆ256 4096 2.21 ´ ´
Titok-B(Yuetal.,2024b) 1D 64 ´ 256ˆ256 4096 1.70 ´ ´
Titok-S(Yuetal.,2024b) 1D 128 ´ 256ˆ256 4096 1.71 ´ ´
Auto-regressiveTransformerSetup. Asillustrated, weproposeasymmetrictokenfactorization
toassisttheauto-regressivetransformermodelsinmakingtheprecisepredictionwithalargecode-
book. Note that, we empirically set M “ 2 and k “ 6, k “ 12. Since our main focus is on
1 2
democratizing scalable auto-regressive visual generation, the plain auto-regressive transformer is
utilizedwhilethe techniquesthatintroduceinductive biassuchasAdaLn (Karrasetal.,2020) are
excluded.Specifically,weadopttheLlama-based(Touvronetal.,2023)architecture(e.g.,RoPE(Su
et al., 2024), SwiGLU (Shazeer, 2020), RMSNorm (Zhang et al., 2022) technique, each of which
hasbeenproveneffectivein(Sunetal.,2024)). Theclassembeddingwhichisindexedfromasetof
learnableembeddingsservesasthestarttoken. Open-MAGVIT2followsthesimplescalingprinci-
pleproposedin(Sunetal.,2024),whichisinTab.1. Allmodelsaretrainedwithsimilarsettings:
abaselearningrateof1e´4per256batchsize,anAdamWoptimizerwithβ “ 0.9,β “ 0.95,
1 2
weightdecay=5e´2,atotal768batchsizeand300„350trainingepochs,gradientclippingof1.0,
0.1dropoutrateforinputembedding,FFNmoduleandconditionalembedding,32„96ˆNvidia
V100/Ascend910BfordifferentscalesofthemodelwithPytorch.
3.3 MAINRESULTS
VisualReconstruction. AsshowninTab.2,byincorporatingallusefuldesignsproposedin(Yu
etal.,2024a),Open-MAGVIT2matchesMAGVIT-v2performanceswithmerely0.03FIDmargin
onImageNet128ˆ128. Further,wealsocompareourOpen-MAGVIT2withpreviousvisualtok-
enizersonImageNet256ˆ256inTab.3.Benefitingfromthesuper-largecodebookwithlookup-free
quantization,Open-MAGVIT2outperformsallpreviousimagetokenizersunderfairsettings.More-
over,weprovideanillustrativevisualcomparisoninFig.3. Asindicated,ourvisualtokenizergains
moresuperiorityindetailperceptionaswellasprecisefacialandtextreconstruction.
VisualGeneration. MAGVIT-v2leveragesthenon-autoregressiveframeworkforimagesynthesis
andachievescompetitiveperformance.Consideringthescalabilityofauto-regressivemodelsandthe
remarkable success of the auto-regressive paradigm in MLLM (Team, 2024), we instead focus on
exploringthepotentialofincorporatingasuper-largecodebookforauto-regressivevisualgeneration.
6TechnicalReport
Table4: Class-conditionalgenerationon256ˆ256ImageNet. ˚specifiesthegeneratedimagesare
384ˆ384andareresizedto256×256forevaluation. Theevaluationprotocolandimplementation
arethesamewithADM.
Type Model #Para. FIDÓ ISÒ PrecisionÒ RecallÒ
ADM(Dhariwal&Nichol,2021) 554M 10.94 101.0 0.69 0.63
CDM(Hoetal.,2022) ´ 4.88 158.7 ´ ´
Diffusion
LDM-4(Rombachetal.,2022a) 400M 3.60 247.7 ´ ´
DiT-XL/2(Peebles&Xie,2023) 675M 2.27 278.2 0.83 0.57
VQGAN(Esseretal.,2021) 227M 18.65 80.4 0.78 0.26
VQGAN(Esseretal.,2021) 1.4B 15.78 74.3 ´ ´
VQGAN-re(Esseretal.,2021) 1.4B 5.20 280.3 ´ ´
AR ViT-VQGAN(Yuetal.,2022) 1.7B 4.17 175.1 ´ ´
ViT-VQGAN-re(Yuetal.,2022) 1.7B 3.04 227.4 ´ ´
RQTran.(Leeetal.,2022) 3.8B 7.55 134.0 ´ ´
RQTran.-re(Leeetal.,2022) 3.8B 3.80 323.7 ´ ´
VAR-d16(Tianetal.,2024) 310M 3.30 274.4 0.84 0.51
VAR-d20(Tianetal.,2024) 600M 2.57 302.6 0.83 0.56
VAR
VAR-d24(Tianetal.,2024) 1.0B 2.09 312.9 0.82 0.59
VAR-d30(Tianetal.,2024) 2.0B 1.92 323.1 0.82 0.59
LlamaGen-L˚ (Sunetal.,2024) 343M 3.07 256.06 0.83 0.52
LlamaGen-XL˚ (Sunetal.,2024) 775M 2.62 244.08 0.80 0.57
LlamaGen-XXL˚ (Sunetal.,2024) 1.4B 2.34 253.90 0.80 0.59
LlamaGen-L(Sunetal.,2024) 343M 3.80 248.28 0.83 0.51
LlamaGen-XL(Sunetal.,2024) 775M 3.39 227.08 0.81 0.54
AR LlamaGen-XXL(Sunetal.,2024) 1.4B 3.09 253.61 0.83 0.53
Open-MAGVIT2-B 343M 3.08 258.26 0.85 0.51
Open-MAGVIT2-L 804M 2.51 271.70 0.84 0.54
Open-MAGVIT2-XL 1.5B 2.33 271.77 0.84 0.54
As shown in Tab. 4, Open-MAGVIT2 outperforms all previous image generation models using a
plain auto-regressive approach. This benefits from the increased representational capacity of the
largescaleofthecodebook. However,webelievethatthestrengthofsuchalargecodebookisstill
underestimated because of the data bottleneck and the model size. We hope our effort in building
suchapowerfulvisualtokenizerhelpsmeritfutureresearchinunifiedMLLMforimagegeneration.
3.4 QUALITATIVERESULTS
WepresentthequalitativeresultsonImagenetBenchmarkintermsofvisualreconstruction(seein
Fig.4)andvisualgeneration(seeinFig.5),respectively.
4 RELATED WORKS
4.1 VISUALTOKENIZER
Visual tokenizer is to map an image into compact discrete tokens, which are subsequently fed
into the generative models for sequence modeling. Early pioneer VQVAE (Van Den Oord et al.,
2017)firstintroduceslearnablecodebookmechanismfor2Dtokensgeneration. Subsequently,ViT-
VQGAN(Yuetal.,2022)andRQ-VAE(Leeetal.,2022)improveVQVAEthroughnormalizedand
multi-scalequantizationrespectively. Recently,LlamaGen(Sunetal.,2024)reexaminesthedesign
of vanilla tokenizer (Esser et al., 2021) and reveals the conflict between the fidelity of the synthe-
sized image and the size of codebook. Therefore, following the simple intuition (Yu et al., 2022)
that reducing code dimension limits the representational capacity of individual tokens, MAGVIT-
2(Yuetal.,2024a)proposesanadvancedvisualtokenizerwhichsignificantlyenlargesthesizeof
codebookto218withLookup-FreeQuantization.
7TechnicalReport
4.2 VISUALGENERATION
Givenasetofcompactdiscreteimagetokens, thereexisttwoprevalentframeworksforthesubse-
quentimagesynthesis,includingNon-autoregressiveandAuto-regressivegeneration.
Non-autoregressive frameworks. MaskGIT (Chang et al., 2022) utilizes BERT-style trans-
former (Devlin et al., 2018) to parallelly generate all visual tokens via masked-prediction mech-
anism. MAGVIT (Yu et al., 2023; 2024a) adopts the same architecture but includes an additional
embeddingmaskforbettergenerationquality.
Auto-regressive frameworks. Autoregressive-based Multi-Modal Large Language Models (Liu
et al., 2024; Li et al., 2024) has achieved remarkable success in versatile visual understanding.
Incontrast,theprogressincounterpartvisualgenerationstillremainsunsatisfactory. Thesimplest
approachVQGAN(Esseretal.,2021)employstinyGPT2(Radfordetal.,2019)(„300M)fornext-
tokenprediction.VAR(Tianetal.,2024)reformulatestheimagegenerationapproachintonext-scale
prediction and unveils the scaling principle simultaneously. Subsequently, LlamaGen (Sun et al.,
2024)extendsVQGANwithLlama(Touvronetal.,2023)architecture,showcasingsignificantim-
provement in fidelity. However, the limited codebook size (e.g., 214) in existing auto-regressive
models may incur the representational bottleneck. Therefore, considering that the capacity of the
visual tokenizer is highly correlated with the quality of visual synthesis (Yu et al., 2024a), we de-
mocratizetheplainauto-regressiveapproachwithasuper-largecodebook.
5 CONCLUSION
In this work, we re-implement the powerful visual tokenizer, which achieves state-of-the-art per-
formance compared with previous methods, and make it available to the community. Instead of
simplyfollowing(Yuetal.,2024a)thatleveragesmasked-generativetransformerforvisualgener-
ation, wedelve intoamorepromising manner(i.e., auto-regressivevisual synthesis). To excavate
the potential of the large vocabulary, we introduce the “next sub-token prediction” paradigm with
theasymmetrictokenfactorizationtechnique. Theexperimentsuggeststhatwiththepowerfultok-
enizer,theplainauto-regressivemodelexhibitssuperiorityandscalability.Wehopeourcontribution
totheopen-sourcecommunitycanfacilitatemoreinnovativeandcreativeworksinthefieldofauto-
regressivevisualgeneration,eventuallymakingadifferenceinbuildinganomnipotentmulti-modal
framework.
Limitationsandfuturework. Weexpectthattheeffectivenessofsuchasuper-largecodebook,
(i.e.,218codes),isstillunderestimatedduetothelimiteddatascaleandthesacrificeoftherepresen-
tationalcapacitywiththetokenfactorizationtechnique. Webelievethatbyamplifyingthetaskwith
more training data (e.g., text-conditional image generation, video generation, etc.), and enlarging
the model size to 7B or even larger, the potential of AR generation with a super-large codebook
canbedramaticallyexploited. Therefore,extendingOpen-MAGVIT2intomorebroadmulti-modal
generationapplicationswillbeahighpriorityinourfutureexploration.
ACKNOWLEDGMENTS
WesincerelythankLijunYuforhisencouragingdiscussionsandsupport. WealsothankTianheng
ChengandYuxinChenfortheirhelpfulsuggestionsonthisproject.
8TechnicalReport
(a)
(b)
(a)
(b)
(a)
(b)
Figure 4: Visualization of the Open-MAGVIT2 tokenizer. The upper part illustrates the model
trainedat128ˆ128resolutionandtestedat512ˆ512resolution. Thesecondpartshowcasesthe
tokenizertrainedat256ˆ256resolutionandtestedat256ˆ256resolution.(a)indicatestheoriginal
imageswhile(b)specifiesthereconstructionimages.
9TechnicalReport
Figure5: VisualizationofOpen-MAGVIT2auto-regressivegenerations. Class-conditionalgen-
erationonImageNet256ˆ256.
10TechnicalReport
REFERENCES
HuiwenChang,HanZhang,LuJiang,CeLiu,andWilliamT.Freeman.Maskgit:Maskedgenerative
imagetransformer. InCVPR,pp.11305–11315,2022. 2,6,8
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. PaLM:
Scalinglanguagemodelingwithpathways. arXivpreprintarXiv:2204.02311,2022. 2
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale
hierarchicalimagedatabase. InCVPR,pp.248–255,2009. 2,5
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep
bidirectionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018. 8
PrafullaDhariwalandAlexanderNichol. Diffusionmodelsbeatgansonimagesynthesis. NeurIPS,
34:8780–8794,2021. 7
PatrickEsser,RobinRombach,andBjo¨rnOmmer. Tamingtransformersforhigh-resolutionimage
synthesis. InCVPR,pp.12873–12883,2021. 2,4,6,7,8
Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter.
GANstrainedbyatwotime-scaleupdateruleconvergetoalocalnashequilibrium. InNeurIPS,
volume30,2017. 5
Jonathan Ho, Chitwan Saharia, William Chan, David J Fleet, Mohammad Norouzi, and Tim Sali-
mans. Cascaded diffusion models for high fidelity image generation. JMLR, 23(1):2249–2281,
2022. 7
XunHuangandSergeJ.Belongie. Arbitrarystyletransferinreal-timewithadaptiveinstancenor-
malization. InICCV,pp.1510–1519,2017. 4
Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A. Efros. Image-to-image translation with
conditionaladversarialnetworks. InCVPR,pp.5967–5976,2017. 5
Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative
adversarialnetworks. InCVPR,pp.4401–4410,2019. 4,5
TeroKarras,SamuliLaine,MiikaAittala,JanneHellsten,JaakkoLehtinen,andTimoAila. Analyz-
ingandimprovingtheimagequalityofstylegan. InCVPR,pp.8110–8119,2020. 6
DiederikPKingma. Auto-encodingvariationalbayes. arXivpreprintarXiv:1312.6114,2013. 2
TuomasKynka¨a¨nniemi,TeroKarras,SamuliLaine,JaakkoLehtinen,andTimoAila. Improvedpre-
cisionandrecallmetricforassessinggenerativemodels. InHannaM.Wallach,HugoLarochelle,
Alina Beygelzimer, Florence d’Alche´-Buc, Emily B. Fox, and Roman Garnett (eds.), NeurIPS,
pp.3929–3938,2019. 5
DoyupLee,ChiheonKim,SaehoonKim,MinsuCho,andWook-ShinHan. Autoregressiveimage
generationusingresidualquantization. InCVPR,pp.11513–11522,2022. 2,5,7
YanweiLi,YuechenZhang,ChengyaoWang,ZhishengZhong,YixinChen,RuihangChu,Shaoteng
Liu,andJiayaJia. Mini-gemini: Miningthepotentialofmulti-modalityvisionlanguagemodels.
arXivpreprintarXiv:2403.18814,2024. 8
HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. Advances
inneuralinformationprocessingsystems,36,2024. 8
OpenAI. GPT-4technicalreport. arXivpreprintarXiv:2303.08774,2023. 2
William Peebles and Saining Xie. Scalable diffusion models with transformers. In CVPR, pp.
4195–4205,2023. 4,7
AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal.Language
modelsareunsupervisedmultitasklearners. OpenAIBlog,2019. 8
11TechnicalReport
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,pp.10684–10695,2022a. 6,
7
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjo¨rn Ommer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InCVPR,pp.10674–10685,2022b. 2
Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen.
Improvedtechniquesfortraininggans. InNeurIPS,volume29,2016. 5
NoamShazeer. Gluvariantsimprovetransformer. arXivpreprintarXiv:2002.05202,2020. 6
Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: En-
hancedtransformerwithrotarypositionembedding,2024. 6
Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan.
Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint
arXiv:2406.06525,2024. 2,4,6,7,8
Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint
arXiv:2405.09818,2024. 6
KeyuTian,YiJiang,ZehuanYuan,BingyuePeng,andLiweiWang.Visualautoregressivemodeling:
Scalableimagegenerationvianext-scaleprediction. arXivpreprintarXiv:2404.02905,2024. 2,
7,8
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Niko-
lay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher,
Cristian Canton-Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy
Fu,WenyinFu,BrianFuller,CynthiaGao,VedanujGoswami,NamanGoyal,AnthonyHartshorn,
Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel
Kloumann,ArtemKorenev,PunitSinghKoura,Marie-AnneLachaux,ThibautLavril,JenyaLee,
Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,
Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,
AlanSchelten,RuanSilva,EricMichaelSmith,RanjanSubramanian,XiaoqingEllenTan,Binh
Tang,RossTaylor,AdinaWilliams,JianXiangKuan,PuxinXu,ZhengYan,IliyanZarov,Yuchen
Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aure´lien Rodriguez, Robert Stojnic,
Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models.
arXivpreprintarXiv:2307.09288,2023. 2,6,8
Hung-YuTseng,LuJiang,CeLiu,Ming-HsuanYang,andWeilongYang. Regularizinggenerative
adversarialnetworksunderlimiteddata. InCVPR,pp.7921–7931,2021. 5
AaronVanDenOord,OriolVinyals,etal.Neuraldiscreterepresentationlearning.volume30,2017.
2,3,7
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,
Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Isabelle Guyon, Ulrike von
Luxburg, Samy Bengio, Hanna M. Wallach, Rob Fergus, S. V. N. Vishwanathan, and Roman
Garnett(eds.),NeurIPS,pp.5998–6008,2017. 2
JunkeWang,YiJiang,ZehuanYuan,BinyuePeng,ZuxuanWu,andYu-GangJiang.Omnitokenizer:
Ajointimage-videotokenizerforvisualgeneration. arXivpreprintarXiv:2406.09399,2024. 6
JiahuiYu,XinLi,JingYuKoh,HanZhang,RuomingPang,JamesQin,AlexanderKu,Yuanzhong
Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved VQ-
GAN. InICLR,2022. 2,6,7
Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose´ Lezama, Han Zhang, Huiwen Chang, Alexander G.
Hauptmann,Ming-HsuanYang,YuanHao,IrfanEssa,andLuJiang. MAGVIT:maskedgenera-
tivevideotransformer. InCVPR,pp.10459–10469,2023. 8
12TechnicalReport
LijunYu,JoseLezama,NiteshBharadwajGundavarapu,LucaVersari,KihyukSohn,DavidMinnen,
Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G Hauptmann, Boqing Gong, Ming-Hsuan
Yang,IrfanEssa,DavidARoss,andLuJiang. Languagemodelbeatsdiffusion-tokenizeriskey
tovisualgeneration. InICLR,2024a. 2,3,4,5,6,7,8
Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen.
Animageisworth32tokensforreconstructionandgeneration.arXivpreprintarXiv:2406.07550,
2024b. 6
RichardZhang,PhillipIsola,AlexeiA.Efros,EliShechtman,andOliverWang. Theunreasonable
effectivenessofdeepfeaturesasaperceptualmetric. InCVPR,pp.586–595,2018. 5
Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christo-
pherDewan, MonaT.Diab, XianLi, XiVictoriaLin, TodorMihaylov, MyleOtt, SamShleifer,
Kurt Shuster, Daniel Simig, Punit Singh Koura, Anjali Sridhar, Tianlu Wang, and Luke Zettle-
moyer. OPT:openpre-trainedtransformerlanguagemodels. arXivpreprintarXiv:2205.01068,
2022. 6
13