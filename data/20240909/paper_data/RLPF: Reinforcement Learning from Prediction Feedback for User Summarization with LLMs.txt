RLPF: Reinforcement Learning from Prediction Feedback for User
Summarization with LLMs
JiaxingWu1* LinNing1 LuyangLiu1 HarrisonLee1 NeoWu1 ChaoWang1 SushantPrakash2
ShawnO’Banion1 BradleyGreen1 JunXie1
1GoogleDeepMind,2Google
Abstract language space are reusable across any LLM for down-
stream tasks without needing to re-train the LLM. In addi-
LLM-powered personalization agent systems employ Large tion, natural language summaries are interpretable and ed-
Language Models (LLMs) to predict users’ behavior from
itable, offering users more control over their personalized
theirpastactivities.However,theireffectivenessoftenhinges
experiences.
ontheabilitytoeffectivelyleverageextensive,longuserhis-
Generatingusersummaries,however,facesuniqueobsta-
toricaldataduetoitsinherentnoiseandlengthofsuchdata.
cles. Unlike traditional text summarization, user summary
ExistingpretrainedLLMsmaygeneratesummariesthatare
concisebutlackthenecessarycontextfordownstreamtasks, qualityissubjectiveanddifficulttodefine,andthereareno
hinderingtheirutilityinpersonalizationsystems.Toaddress readilyavailabledatasetstotrainon.Consequently,training
thesechallenges,weintroduceReinforcementLearningfrom ausersummarizationmodelisnon-trivial.
PredictionFeedback(RLPF).RLPFfine-tunesLLMstogen- To overcome the challenges of generating natural lan-
erate concise, human-readable user summaries that are op- guage user summaries, we propose RLPF: Reinforcement
timized for downstream task performance. By maximizing
LearningfromPredictionFeedback(illustratedinFigure1),
theusefulnessofthegeneratedsummaries,RLPFeffectively
whichincludesthreecomponents:
distills extensive user history data while preserving essen-
tialinformationfordownstreamtasks.Ourempiricalevalu- • Summarization Model: A model is trained to generate
ation demonstrates significant improvements in both extrin- succinctusersummariesfromrawactivitydata.
sic downstream task utility and intrinsic summary quality,
• Prediction-based Reward Model: To compute a re-
surpassing baseline methods by up to 22% on downstream
ward,wemeasuretheeffectivenessofthegeneratedsum-
task performance and achieving an up to 84.59% win rate
mariesindownstreampredictiontasks.
on Factuality, Abstractiveness, and Readability. RLPF also
achievesaremarkable74%reductionincontextlengthwhile • Feedback Loop: The reward is then used to update the
improvingperformanceon16outof19unseentasksand/or summarizationmodelwithRL,withanadditionalreward
datasets,showcasingitsgeneralizability.Thisapproachoffers toencourageshorterlengths.Thisfeedbackloopguides
apromisingsolutionforenhancingLLMpersonalizationby
thesummarizationmodeltocontinuouslyrefineitsabil-
effectivelytransforminglong,noisyuserhistoriesintoinfor-
ity to produce summaries that are not only concise but
mativeandhuman-readablerepresentations.
alsohighlyeffectivefortheirintendedapplications.
RLPFoffersawin-winsolution:itenablesthecreationof
1 Introduction
high-qualityusersummarieswithouttheneedforresource-
LargeLanguageModels(LLMs)haveshowngreatpromise intensive and potentially privacy-compromising human in-
forpersonalizedpredictionbyleveraginghistoricalactivity tervention. By directly optimizing the summarization pro-
data(Liuetal.2023;Lyuetal.2024;Lietal.2023).How- cessfordownstreampredictionperformance,weensurethat
ever,theinherentnoiseandlengthofuserhistorydatapose thegeneratedsummariesarebothcompactanddirectlyrel-
obstaclestotheireffectiveutilizationinLLM-poweredagent evant to the tasks they are meant to support. Furthermore,
systems. compared to prevailing Reinforcement Learning (RL) ap-
Naturallanguageusersummariesofferseveraladvantages proaches relying on feedback from a dedicated trained re-
overusingrawuseractivitydata.First,theyimproveinfer- ward LLM (Ouyang et al. 2022; Bai et al. 2022; Lee et al.
ence efficiency over using raw user data due to their com- 2024; Yang et al. 2023), RLPF eliminates the overhead of
pactnature.Second,theyofferthepotentialtoimproveper- trainingaseparaterewardmodel.
formance on downstream tasks by distilling user activities Through extensive experiments on four public datasets
andreducingnoise.Representingusercontextthroughnatu- groundedinreal-worlduserinteractions-MovieLens2015
rallanguagealsooffersseveraladvantagesoverembedding- and2003(HarperandKonstan2015),AmazonReview(He
based representations. User representations in the natural and McAuley 2016), and Google Local Review (Yan et al.
2022), we demonstrate that RLPF summaries outperform
*Correspondenceto:JiaxingWu{jxwu@google.com}. baselinesintermsofpredictivepoweronbothseenandun-
4202
peS
6
]LC.sc[
1v12440.9042:viXraEvaluation
Training
Figure1:OverviewofRLPF.Left:TrainingprocessofRLPF,inwhichfutureactivitywillbeusedtowardsrewardcomputation.
Right:WeassessRLPFonunseendownstreampredictiontaskstodemonstrateitsgeneralizabilityandadaptability.
seen tasks, as well as on intrinsic summary quality evalua- the partially generated summary, while actions correspond
tions. totheselectionoftokensfromtheentirevocabulary.Ateach
Ourcontributionsarefour-fold: step, the policy model maps these states to probability dis-
tributionsoverthevocabulary,facilitatingautoregressiveto-
• We introduce the novel task of generating natural lan-
kenselection.Thisselectionprocessisguidedbythecurrent
guage user summaries for user modeling and personal-
contextandtheoverarchingobjectiveofmaximizingcumu-
izationsystems.Thisoffersaninterpretablealternativeto
lativerewards.
traditional embedding-based representations and allows
WithinthisRLframework,weformalizeRLPFinthecon-
utilizationbyarbitraryLLMswithoutfurthertraining.
textofusersummarizationasfollows:
• We introduce RLPF, a novel and easy-to-implement
method for training user summarizers. RLPF elimi- • State:ThesetofusercontextsU ={u i}M i=1,whereeach
nates the need for reference summaries or hand-crafted u i isasinglestringrepresentingthetextualfeaturesofa
prompts,whilesafeguardinguserprivacy. user’sN pastactivities.
• WedemonstratethatRLPFsummariesoutperformbase- • Action:ThesetofusersummariesS = {s i}M
i=1
gener-
lines on both the training task and unseen tasks across atedbasedonthecorrespondingusercontexts.
fourdatasetsanddomains. • Policy Model: The summarizer model, denoted by π ,
θ
• WeevaluateRLPFsummariesintrinsicallyandfindsig- whichmapsusercontexts(states)tousersummaries(ac-
nificant improvements in factuality, abstractiveness, and tions):π(u i;θ)→s i.
readability. • Reward:Weleverageafrozen,pre-trainedLLMtogen-
erate predictions P(s ) for one or more specified tasks
i
2 Methodology basedonusersummariess .Thenascalarrewardvalue
i
ProblemStatement iscomputedbycomparingthepredictionP(s i)withits
correspondinggroundtruthlabely ofthespecifictask.
Consider a set of users U = {u }M , where each user i i
i i=1
has an associated chronologically ordered sequence of in- TheobjectiveofRLPFistolearnapolicyπ∗ thatmaxi-
teractions, denoted as {v1,v2,...,vN}. Each vj within this mizestheexpectedcumulativereward:
i i i i
sequence (where 1 ≤ j ≤ N) comprises one or more tex- π∗ =argmaxE [r(π(u ;θ))]
tual features that describe a specific item, such as the titles π ui∼U i
orratingsofmovieswatchedbytheuser.Foreachuseri,we
Reward Computation RLPF provides the flexibility to
concatenate all of their interactions {vj}N into a single
i j=1 leverage any task for reward derivation, tailored to specific
stringtoformtheusercontextu .
i downstream application requirements. Moreover, it seam-
A summarizer model π takes as input the user context
θ lesslyaccommodatesthecombinationofrewardsfrommul-
and generates a summary s = π (u ). The summary is
i θ i tipletasksifnecessary.Inourimplementation,weoptedto
thenprovidedtooff-the-shelfLLMtoproduceaprediction
utilizefutureactivitypredictionasthesoletaskforgenerat-
yˆ = P(s )foraspecificdownstreamtask.Weoptimizeπ
i i θ ingrewardsignals,givenitssignificanceinpersonalization
to generate summaries {s }M that minimize the expected
i i=1 and recommender systems. Rather than training a summa-
error between the predictions {yˆ i}M i=1 and the ground truth rizermodelonmultipledownstreamtasks,wechosetotrain
tasklabels{y i}M i=1. itbasedonthissingletask,subsequentlyevaluatingitsgen-
eralizationandtransferabilityonotherunseentasks.
ReinforcementLearningfromPredictionFeedback Foreachuseri,wespecifythesummaryrewardr(s )as
i
In the context of RL, we formulate summary generation follows:
as a Contextual Markov Decision Process (CMDP). In this
framework, the state encompasses both the input text and r(s )=rpred(s , y )+w·rlen(s )
i i i iwhererpred(.)isthepredictionfeedbackreward,rlen(.)is where α is a hyperparameter controlling the balance be-
thelengthreward,andwisaweightthatcontrolsthebalance tween the reward maximization and policy regularization
betweenthetwoterms. objectives.
Prediction Feedback Reward: Recall that each user con-
textu consistsofthetextualfeaturesofN pastuseractiv- 3 ExperimentalDetails
i
ities. We employ the subsequent (N +1)-th activity (e.g., Dataset
watched movie title, rated product or place name) as the
Weconductexperimentsonfourpublicdatasetsgroundedin
groundtruthlabely forpredictingthefutureactivity.Given
i real-worlduserinteractions,encompassingproductreviews,
theusersummarys ,wecalculateabinaryrewardbycom-
i movie watching behavior, and location data. We perform
paringtheLLM’spredictionbasedons totheactualfuture
i trainingonAmazonBooks(HeandMcAuley2016),Google
activityv iN+1:
Local Review (Yan et al. 2022), MovieLens 2015(Harper
and Konstan 2015). Additionally, we utilized another four
rpred(s i, y i)=1(P(s i)=y i), wherey i =v iN+1 Amazon Review datasets with different product categories,
as well as MovieLens 2003, which features distinct users
However,sincetherewardmodeloperatesinazero-shot
andmoviecatalogscomparedtoMovieLens2015).Seeap-
setting, predicting item names with exact matches without
pendixCfordatasetdetails.
any additional context is challenging due to the vast num-
ber of possibilities and potential variations. This hinders Data Generation For each user’s interaction data, pre-
the policy model’s ability to receive positive feedback and sentedasachronologicallyorderedlistofactivitiesu ∈U,
i
learn effectively. To tackle this issue, we adopt a multiple- werandomlyselectoneitemasthetargetforfutureactivity
choice approach, providing four answer choices for each prediction,denotedasy .WeutilizetheN activitiespreced-
i
summarybasedprediction, includingthegroundtruth. The ingthistargetasthepastactivities{vj}N .vj representsan
i j=1 i
reward model is then prompted to select the correct option item name and rating pair, where item name correspond to
from the given choices. It should be noted that our method movie title for MovieLens, product name for Amazon Re-
isadaptabletodifferentclosed-endedquestionformats.See view, and place name + city name for Google Local Re-
AppendixIformoredetailsaboutthepromptusedforpre- view,respectively.Aspreviouslymentioned,weconcatenate
diction. {vj}N to construct the user context u . To prevent label
Length Reward: Furthermore, to promote concise sum- i j=1 i
leakage, the last item in each user’s data is reserved as the
marygeneration,weincorporatealengthreward:
targetiteminthetestset.Unlessotherwisespecified,weset
N =50inourexperiments.
rlen(s )=min[C, β∗(L−l )]
i i
Evaluation
where l represents the token length of summary s , and
i i
thehyperparametersM,β,andLdenotetheupperbound, We evaluate user summaries along the following three di-
magnitude, and target length of the summary, respectively. mensions:
We set the target length to the average length of Zero
Shotsummariesinourexperiments.PleaserefertotheAp- Predictiveness We gauge the predictiveness of the sum-
pendixDforspecificvaluesofthesevariables. maries based on their prediction performance in various
downstream tasks. Extending beyond Future Activity Pre-
Training Process The absence of reference summaries diction which is used as feedback during training, we in-
prevents the application of supervised fine-tuning to either corporated additional tasks of various types to gauge the
the policy or reward model. Unlike the standard RLHF transferability and generalization capabilities of the gener-
pipeline,whichsequentiallyinvolvessupervisedfine-tuning, ated summaries. These included 19 tasks include user in-
reward modeling, and policy optimization, RLPF directly terestreasoning,historyactivityretrieval,ratingprediction,
optimizesthepolicyinasingleRLtrainingstep.Byleverag- user demographic prediction and open text review genera-
ingLLMs’inherentzero-shotsummarizationandprediction tion.PleaserefertoAppendixGfordetailedtaskdefinitions
capabilities, RLPF eliminates the need for intricate prompt aswellastheirabbreviationusedinthepaper.
engineering, generating feedback for the RL process based A frozen instruction tuned Gemini 1.0 Pro model was
onpredictedfutureactivities.WhileRLPFisnottiedtoany employed to generate predictions for all downstream tasks.
specific RL algorithm, we utilize REINFORCE (Williams Eachsummarys wasfedintothemodel,andtheresulting
i
1992)withabaselinetoupdatethepolicymodelgiventhat predictionswereevaluatedagainstgroundtruthlabels.
itissimpleryetstilleffectiveforourtasks.Bothpolicyand
valuemodelsareinitializedfromafrozenmodel. Factuality,Abstractiveness,andReadability Tofurther
assess the intrinsic quality of the generated summaries, we
To preserve the LLM’s original summarization capabil-
utilize automated evaluation to compare summaries before
ity and mitigate reward hacking, we introduce a KL diver-
gencetermbetweenthecurrentpolicyπ andtheinitialpol- and after training. This assessment focuses on aspects not
θ
icy π . Consequently, the policy parameters are updated explicitly covered by downstream task performance, in-
init
cludingFactuality,Abstractiveness,ReadabilityandOverall
accordingtothefollowingrule:
quality.Foreachcriterionandoverallquality,theAutoRater
θ ←θ+[(1−α)∇ E[r ]−αE[∇ KL(π ||π )]] compares a pair of summaries, with their relative positions
θ i θ θ initRLPF ZS-CP
ZS-nano2 ZS-Pro RLPF RLAIF RLPF All Activities RLPF All Activities
1000
0.7 0.7 0.7
750
0.6 0.6 0.6
500
0.5 0.5 0.5
250
0.4 0.4 0.4 0
Amazon Google ML2015 Amazon Google ML2015 Amazon Google ML2015 Amazon Google ML2015
(a) (b) (c) (d)
Figure2:RLPFsummariesconsistentlydemonstratesuperiorperformanceinFutureActivityPrediction,surpassingbothother
summarizationtechniquesandthefullusercontext(”AllActivities”),whilesignificantlyreducingtherequiredcontextlength.
ZS-nano2:GeminiNano-2Zero-Shot;ZS-CP:GeminiNano-2withCraftedPrompts;ZS-Pro:GeminiProZero-Shot.
randomlyassignedtoeliminatepotentialbiasintheevalua- – Gemini1.0Nano-2withCraftedPrompt:Usessum-
tion.WeharnessedthemostpowerfulmodelintheGemini maries from Gemini 1.0 Nano-2, but with custom-
family, Gemini 1.5 Pro (GeminiTeam et al. 2024a), as the designedpromptsoptimizedfordownstreamtasks.We
AutoRater.SeeAppendixAforthefullprompt. showthepromptinAppendixB.
– RLAIF: User summaries trained with Direct
TrainingDetails RLAIF(Leeetal.2024),usingGemini1.0Nano-2as
the policy model. The reward signal is derived from
The summarizer model, or policy model π , is initial-
θ
scores provided by an LLM (Gemini 1.0 Pro) based
ized from Gemini 1.0 (GeminiTeam et al. 2024b) Nano-
on Coherence, Accuracy, Coverage, and Overall
2(instruction tuned) and fine-tuned using RLPF. During
quality.Furtherdetailsonthepromptingtechniqueare
training, reward computation (P(s )) is performed by a
i
availableintheAppendixI.
frozen,instruction-tunedGemini1.0Promodel,whichpre-
dicts future activity based on the generated summary s . • Activity-BasedBaselines:Theusercontextu isdirectly
i i
Gemini1.0Prowasselectedforitsoptimalbalanceofper- fedasinputtoafrozeninstructiontunedmodel(Gemini
formanceandinferenceefficiency.Inadditiona,wealsoem- 1.0Pro)togeneratepredictions(Lyuetal.2024):
ployed PaLM-2 XS (Anil et al. 2023) to showcase RLPF’s
– FirstX Activities:UsesonlytheearliestXactivities
applicabilityacrossdiversepolicymodels.
(X < N) for downstream task predictions, ensuring
For all three training datasets (Amazon Books, Google
comparabletokenlengthtoRLPFsummaries.
LocalReview,andMovieLens2015),wetrainedthepolicy
– Random X Activities: Similar to the above, but se-
modelwithabatchsizeof64,utilizingtheAdafactoropti-
lectsX activitiesrandomly.
mizer(ShazeerandStern2018)withalearningrateof1e-6.
Training was conducted for 15,000 steps on each dataset, – LastX Activities:UsesthemostrecentX activities.
andevaluationwasperformedonthefinalcheckpoint.More – AllActivities:UsesthefullusercontextN activities.
hyper-parametervaluesarelistedinAppendixD.
4 Results
Baselines
TargetTaskPerformance
Wecomparetheperformanceofusersummarygeneratedby
Figure2comparesRLPFperformanceontheFutureActiv-
RLPF against two categories of baselines: summary-based
ityPredictiontask.Acrossallthreedatasets,RLPFdemon-
andactivity-based.Astheevaluatormodelmakeszero-shot
stratessuperiororcomparableperformancetovarioussum-
predictions for all inputs, any performance differences are
marizers,includingcraftedprompting,alargersummarizer
attributedtotheinformativenessoftheinput,assumingcon-
model, and RLAIF. Overall, RLPF outperforms Nano-2
sistentpredictioncapability.
zero-shot summaries by +13.4% improvement, and outper-
forms RLAIF by +22% on average. Compared to utilizing
• Summary-BasedBaselines:Weemployfrozeninstruc-
the full user context (all activities), RLPF achieves an av-
tion tuned or fine-tuned models to generate summaries
erage context length compression of -73.8% while still ex-
andassesstheirdownstreamperformance.
hibiting a +12.4% performance gain. Further comparisons
– Gemini1.0Nano-2Zero-Shot:Usessummariesgen- withotherbaselinesareprovidedintheAppendixF,under-
erated by Gemini 1.0 Nano-2 in a zero-shot manner. scoring exceptional capability of RLPF summaries to cap-
This represents the anchor model before RLPF train- turebothshort-termandlong-termusercontextinformation.
ing. For comparison, we conducted supervised fine-tuning of
– Gemini 1.0 Pro Zero-Shot: Uses summaries gener- a Gemini 1.0 Pro model on the same task, reaching 94%
atedbyGemini1.0Proinazero-shotmanner,alarger accuracy. However, this fine-tuned model exhibited zero
andmorepowerfulmodelthantheanchormodel. performance on other tasks, highlighting its overfitting to
ycaruccA ycaruccA ycaruccA
Token
LengthTraining Evaluation Evaluation vs vs
0-Shot RLAIF RLPF
Dataset Dataset Task 0-Shot RLAIF
MovieLens2015 MovieLens2015 FavGenre 0.774 0.776 0.818 5.68% 5.48%
MovieLens2015 MovieLens2015 Rating 0.225 0.229 0.232 3.11% 1.31%
AmazonBooks AmazonBooks FavCategory 0.594 0.613 0.605 1.85% -1.27%
Task AmazonBooks AmazonBooks Rating 0.244 0.147 0.255 4.51% 73.47%
Transfer AmazonBooks AmazonBooks ReviewGen 13.52 13.68 13.46 -0.41% -1.58%
GoogleLocal GoogleLocal FavCategory 0.487 0.513 0.559 14.78% 8.90%
GoogleLocal GoogleLocal Rating 0.118 0.118 0.111 -5.93% -5.93%
GoogleLocal GoogleLocal CommonCity 0.765 0.791 0.901 17.73% 13.93%
MovieLens2015 MovieLens2003 FutureAct 0.468 0.447 0.509 8.82% 13.93%
MovieLens2015 AmazonMovies FutureAct 0.572 0.579 0.606 5.94% 4.66%
Dataset AmazonBooks AmazonMovies FutureAct 0.645 0.573 0.663 2.73% 15.68%
Transfer AmazonBooks AmazonCDs FutureAct 0.397 0.447 0.573 44.33% 28.22%
AmazonBooks AmazonToys FutureAct 0.620 0.585 0.644 3.94% 10.14%
AmazonBooks AmazonGames FutureAct 0.688 0.631 0.713 3.60% 12.90%
MovieLens2015 MovieLens2003 FavGenre 0.808 0.801 0.843 4.35% 5.26%
Task& MovieLens2015 MovieLens2003 UserAge 0.274 0.341 0.246 -10.22% -27.86%
Dataset MovieLens2015 MovieLens2003 UserGender 0.723 0.738 0.729 0.90% -1.15%
Transfer MovieLens2015 MovieLens2003 UserOccupancy 0.146 0.13 0.162 11.20% 24.89%
MovieLens2015 MovieLens2003 Rating 0.228 0.224 0.245 7.50% 9.38%
Table 1: RLPF, trained exclusively on future activity prediction, exhibits remarkable transferability and generalization across
diverse unseentasks and datasets.Evaluation metrics: recall@3for Favorite Genre/Category,Common City, and UserOccu-
pancy;ROUGE-LsumforReviewGen;andaccuracyfortheremainingtasks.
the specific training task. Conversely, RLPF showcased re- RLPFWinRate
Dataset Criteria
markable transferability and generalization capabilities, as vsZero-Shot vsRLAIF
demonstratedinthesubsequentsection.
Factuality 61.32% 62.53%
MovieLens Abstractiveness 62.54% 56.09%
TransferabilityandGeneralization 2015 Readability 62.42% 56.36%
Overall 62.47% 56.10%
ToevaluatethegeneralizabilityandadaptabilityofRLPFfor
variouspersonalizationagentsystems,weconductedacom- Factuality 72.93% 40.09%
prehensivetransferabilityassessmentacrossadiversesetof Amazon Abstractiveness 70.14% 39.20%
Books Readability 71.28% 35.47%
unseentasksanddatasets.AsshowninTable1,RLPFsum-
Overall 70.08% 39.17%
maries consistently exhibited superior transferability com-
paredtozero-shotandRLAIFbaselines,demonstratingim- Factuality 77.58% 49.97%
Google
provementsin16and14outof19totalevaluationcases,re- Local Abstractiveness 84.59% 54.56%
Readability 83.73% 46.02%
spectively.TheseresultshighlightRLPF’sexceptionaltrans- Review
Overall 84.46% 54.22%
ferabilityanditspotentialtobeeffectivelyappliedtoawide
range of personalization scenarios, particularly when train- Table2:IntrinsicEvaluationwithAutoRater.
ingdataisscarce.
Amazon CDs&Vinyl data, highlighting its strong domain
Task Transfer RLPF summaries demonstrated a slight
adaptationabilities.
improvement on an unseen retrieval task, common city re-
trievalonGoogleLocalReview,andperformedonparwith
Task and Dataset Transfer Furthermore, we evaluated
zero-shot summary on an unseen personalized text genera-
RLPF model performance on unseen tasks from unseen
tiontask,reviewgenerationonAmazonBooks.
datasets. RLPF model trained with MovieLens 2015 with
future activity prediction showed improvement on Movie-
Dataset and Domain Transfer We also evaluated
Lens 2003 dataset in favorite genre prediction and user de-
whether an RLPF trained model can generalize to an un-
mographicreasoning.
seen dataset, either in same domain or a different domain.
We used the policy model trained with MovieLens 2015
IntrinsicEvaluation
to generate summaries on MovieLens 2003 and Amazon
Movies&TVs dataset and evaluated future movie predic- Table2demonstratesthatRLPFsummariesconsistentlyout-
tionwiththegeneratedsummaries.Fromtheresults,RLPF perform zero-shot summaries on all three datasets, as eval-
model trained on MovieLens 2015, showed improvements uated by the automated rater across all criteria: Factuality,
onbothunseendatasets.Furthermore,themodeltrainedon Abstractiveness, and Readability, as well as in the Overall
Amazon Books achieved significant performance gains on evaluation.PaLM-2XS Dataset Task zero-shot RLPF
Dataset Task
zero-shot RLPF
MovieLens FutureAct 0.578 0.674
MovieLens FutureAct 0.638 0.741 2015 FavCategory 0.822 0.84
2015 FavCategory 0.86 0.849
Amazon FutureAct 0.689 0.734
Amazon FutureAct 0.626 0.675 Books FavCategory 0.543 0.567
Books FavCategory 0.557 0.565
GoogleLocal FutureAct 0.502 0.532 Table4:EvaluatedusingPaLM-2S,withrewardsignalsde-
Review FavCategory 0.454 0.477 rivedfromGemini1.0Produringtraining.
Table3:RLPFwithPaLM-2XSasthepolicymodel. Future Activity Fav Genre Rating Pred
0.0%
This finding is noteworthy given that RLPF was trained
50.0%
solelyonrewardsignalsfromfutureactivityprediction.De- -2.0%
spitethisfocusedtraining,RLPFsummariesnotonlyavoid -4.0% 40.0%
degradation or overfitting to a single goal but also exhibit -6.0% 30.0%
significantimprovementsinothercrucialaspects.Thissug-
-8.0%
gests that when employing RLPF for user summarization, 20.0%
-10.0%
designing explicit reward signals for each criterion, which
10.0%
can be challenging to obtain, may not be necessary. In- -12.0%
AutoEval(Overall)
stead,futureactivitypredictionperformanceappearstopro- -14.0% 0.0%
vide correlated and implicit signals for these criteria. Intu- L=150 L=268 No Length Reward
itively, to make accurate future activity predictions, a sum- Target Summary Length
mary needs to be factually consistent and distill key user Figure3:ImpactofDifferentTargetLengthsonMovieLens
information.Whilereadabilitymightnotbeastrictprereq- 2015.Percentagechangesarecalculatedrelative“NoLength
uisiteforfutureactivityprediction,it’snoteworthythatthis Reward” condition (no maximum length constraint). Data
criterionalsocorrelateswiththisdownstreamtask. on the right axis pertains to AutoEval, while the left axis
correspondstotheremainingtasks.
Interestingly,RLPF’sperformanceonparwithRLAIFin
thisevaluation,eventhoughRLAIFwasspecificallytrained
with reward signals more aligned with the intrinsic evalua-
tioncriteria,highlightstheeffectivenessofRLPF.
0.6
Analysis
Prompt 1
AlternativePolicyModel Additionally,weappliedRLPF 0.4
Prompt 2
to a policy model initialized from the PaLM-2 XS model,
with results presented in Table 3. Mirroring the observa-
tions with Gemini 1.0 Nano-2, RLPF summaries based on
PaLM-2XSalsoexhibitedimprovementsinboththetrain- 0.6
ingtask(futureactivityprediction)andtheunseentask(fa-
voritegenre/categoryprediction)acrossallthreedatasets.A Prompt 1
0.4
slightdropinperformancewasnotedforfavoritegenrepre- Prompt 2
dictionontheMovieLens2015dataset.
0 2000 4000 6000 8000 10000
Robustness to Model that Uses Summaries To further Training Step
ensure that RLPF summaries are not overly tailored to the Figure 4: RLPF is robust with various prompts. Top:
specific reward model used during training, we employed Evaluation metric with different prompts for Summariza-
anadditionalevaluatormodelPaLM-2Stoassesstheirper- tion, Bottom: Evaluation metric with different prompts for
formance. As in previous experiments, RLPF summaries Predictionduringrewardcomputation.Predictiontask:Fu-
weretrainedusingrewardsignalsderivedfromGemini1.0 tureactivitypredictiononMovieLens2015.
Pro. Table 5 demonstrates that the improvements achieved
withRLPFsummariestransfereffectivelytothesedifferent Robustness to Prompts We investigated the impact of
evaluatormodels,highlightingthegeneralizabilityofRLPF varyingpromptsforsummarygenerationandpredictiondur-
summariesacrossvariousLLM-poweredsystems. ingrewardcomputation.AsillustratedinFigure4,taskre-
turns converge to a similar level despite initial differences
Impact of Summary Length Figure 3 illustrates our ex-
in zero-shot performance, demonstrating the robustness of
periments on MovieLens 2015, where we varied the tar-
RLPFtodiverseprompts.SeefullpromptsinAppendixI.
get length(L) in the length reward term. Generally, longer
summariesledtoimprovedtaskperformancebutdecreased QualitativeObservation Ingeneral,zero-shotsummaries
scores in automated evaluation metrics, suggesting a trade- tend to mimic the structure of the input, which may ei-
offbetweenextrinsicutilityandintrinsicqualities. therbedirectlycopiedfromtheinputactivitiesorrepresent
ecnamrofreP
evitaleR
ycaruccA
Summarization
Predictionhallucinations (e.g., mentioning popular movies like “The Ourworkdivergesfromtraditionaltextsummarizationby
Godfather” despite their absence in the user history). Af- focusingongeneratingabstractiveandfactualuserinsights
ter RLPF training, summaries become more coherent and for downstream personalization tasks. We leverage LLMs
distill user information effectively, though some repetition toproducenovelsummariesbeyondsimpleextractionfrom
or hallucination may still occur occasionally. This also ex- userhistory.
plainsthebetterFactualityandAbstractivenessscoresfrom
theAutomatedEvaluation.Although,wenoticedbothRLPF UserModeling
andRLAIFsummariessometimesexhibitrepetitivepatterns UsermodelinghasbenefitedsignificantlyfromLLM-driven
(e.g.,”Iamalwaysupforagoodrecommendation”),while advancements,notablyinpersonalizedrankingandretrieval.
thecorecontentremainsuser-specific.SeetheAppendixH ManyexistingLLMapproachesemploypromptingorfine-
forexamplesummaries. tuning for specific tasks (Bao et al. 2023; Wu et al. 2024b;
Liu et al. 2023; Lyu et al. 2024; Li et al. 2023; Salemi
5 Discussion et al. 2024). However, the length and complexity of user
interaction data, such as extended histories, can introduce
ResponsibleDeployment WhileRLPFshowspromisefor
noiseandhinderprocessingefficiency.Toaddresstheissue,
enhancingpersonalization,itsuseofuserdataraisesprivacy
some work has focused on using pre-trained or co-trained
and data leakage concerns. Offline training of user sum-
userembeddingstocondenseuserhistoryinformation(Ning
mariesandemployingafrozenLLMforonlineservingcan
et al. 2024; Doddapaneni et al. 2024). In contrast to these
mitigatesomerisks.However,athoroughanalysisofpoten-
embedding-basedrepresentations,ourmethodemployscon-
tialvulnerabilitiesiscrucialbeforereal-worlddeployment.
cise, natural language-based user summaries. These sum-
maries are human-readable, promoting better interpretabil-
6 RelatedWork
ity,explainability,andreusabilityacrossdiverseLLMs.
TextSummarization User summaries are essentially user profiles extracted
Leveraging language models to summarize extensive doc- from extensive activity history, capturing user personality
uments has become a prominent area. Two major chal- andpreferences.Priorstudieshaveprimarilyfocusedonin-
lenges in this field are the availability of high-quality ref- vestigatingdifferentpromptingtechniques(Rao,Leung,and
erence summaries and effective summary evaluation. Tra- Miao2023;Jietal.2023;Wuetal.2024a),evaluatingper-
ditionalsummarizationapproachesofteninvolvesupervised formance directly against ground truth user profile labels.
fine-tuning of a model using reference summaries as pre- Whileourworkfocusesongeneratingcomprehensivesum-
diction labels (Cohan et al. 2018; He et al. 2022, 2023; mariesthatbenefitabroadrangeofdownstreamtasks,rather
Krys´cin´skietal.2021).Generatedsummariesaretheneval- thanasingleusercharacteristic.
uatedagainstthesereferencesthroughlexicalmatching(Lin
ReinforcementLearningfromAIFeedback
2004) or embedding-based similarity metrics (Zhang et al.
2020).However,thesemethodsareinapplicablewhenrefer- RL from Human Feedback (RLHF) (Ouyang et al. 2022)
encesummariesareunavailableorofsubparquality,acom- aligns language models with human values through rein-
monissueinmanypopularsummarizationdatasets. forcementlearningguidedbyextensivehumanfeedback.To
Previousworkalsoexploredmetricsintheabsenceofre- addresstherelianceonhigh-qualityhumanlabelsinRLHF,
liable reference summaries. This work leveraged question- recently, RL from AI Feedback(RLAIF) (Bai et al. 2022)
answering(QA) (Durmus, He, and Diab 2020; Fabbri et al. anditsvariants(Yangetal.2023)haveemergedaspromis-
2022; Deutsch, Bedrax-Weiss, and Roth 2021), pre-trained ing alternatives. These approaches levarage a powerful off-
model(Kryscinskietal.2020;GoyalandDurrett2020),and the-shelfLLMtogeneratepreferencesinlieuofhumanan-
human eval (Goyal, Li, and Durrett 2023) to assess factu- notatorandachievesuperiorperformancetoRLHFontasks
ality,coherence,andabstractiveness.However,thesemeth- such as summarization (Lee et al. 2024). Along this direc-
odsexhibitedlowcorrelationwithhumanjudgments(Fabbri tion, there is also previous work (Kwon et al. 2023) that
etal.2021)orwereprohibitivelyexpensive. demonstrated the use of a frozen LLM as a proxy reward
There is also prior work that leverages RL to enhance functionforfew-shotin-contextlearningtofine-tuneanon-
the summary quality by designing reward signals aligned LLM-basedRLagent.
with specific summarization objectives. Existing RL meth-
7 Conclusions
ods have explored using similarity scores between answers
generated from reference and generated summaries as re- We introduced RLPF, a novel method for generating con-
ward signals (Gunasekara et al. 2021), or directly incorpo- ciseandhuman-readableusersummariesfromrawactivity
ratinghumanannotationasfeedback(Stiennonetal.2020). data. RLPF leverages readily available LLMs and down-
Yet, these approaches remain reliant on either reference stream task performance as reward signals, overcoming
summariesorhumanlabeling.Othertechniqueshaveaimed challengesintraditionalsummarizationapproaches.Ourex-
toimprovespecificsummarycriteria,likefactuality,through periments demonstrate superior performance, context com-
specialized reward models (Roit et al. 2023). It is worth pression, and generalizability across unseen tasks. Future
pointingoutthesemethodsofteninvolveasupervisedfine- work will extend RLPF to more complex scenarios, incor-
tuning stage with reference summaries, thus not being en- porateadditionalfeedbackmechanisms,andexploreitsap-
tirelyreference-free. plicationtootherLLMcapabilities.References Gunasekara,C.;Feigenblat,G.;Sznajder,B.;Aharonov,R.;
andJoshi,S.2021.Usingquestionansweringrewardstoim-
Anil, R.; Dai, A. M.; Firat, O.; Johnson, M.; Lepikhin, D.;
proveabstractivesummarization.InFindingsoftheAssocia-
Passos,A.;Shakeri,S.;Taropa,E.;Bailey,P.;Chen,Z.;etal.
tionforComputationalLinguistics:EMNLP2021,518–526.
2023. PaLM2TechnicalReport. arXiv:2305.10403.
Harper, F. M.; and Konstan, J. A. 2015. The MovieLens
Bai, Y.; Kadavath, S.; Kundu, S.; Askell, A.; Kernion, J.;
Datasets:HistoryandContext. ACMTrans.Interact.Intell.
Jones,A.;Chen,A.;Goldie,A.;Mirhoseini,A.;McKinnon,
Syst.,5(4).
C.; et al. 2022. Constitutional AI: Harmlessness from AI
He,J.;Kryscinski,W.;McCann,B.;Rajani,N.;andXiong,
Feedback. arXiv:2212.08073.
C. 2022. CTRLsum: Towards Generic Controllable Text
Bao, K.; Zhang, J.; Zhang, Y.; Wang, W.; Feng, F.; and Summarization. InProceedingsofthe2022Conferenceon
He,X.2023. TALLRec:AnEffectiveandEfficientTuning EmpiricalMethodsinNaturalLanguageProcessing,5879–
Framework to Align Large Language Model with Recom- 5915.
mendation. InProceedingsofthe17thACMConferenceon He,P.;Peng,B.;Wang,S.;Liu,Y.;Xu,R.;Hassan,H.;Shi,
RecommenderSystems.ACM.
Y.; Zhu, C.; Xiong, W.; Zeng, M.; Gao, J.; and Huang, X.
Cohan, A.; Dernoncourt, F.; Kim, D. S.; Bui, T.; Kim, S.; 2023.Z-Code++:APre-trainedLanguageModelOptimized
Chang, W.; and Goharian, N. 2018. A Discourse-Aware for Abstractive Summarization. In Proceedings of the 61st
Attention Model for Abstractive Summarization of Long Annual Meeting of the Association for Computational Lin-
Documents. In Proceedings of the 2018 Conference of the guistics(Volume1:LongPapers).
North American Chapter of the Association for Computa- He, R.; and McAuley, J. 2016. Ups and Downs: Modeling
tional Linguistics: Human Language Technologies, Volume theVisualEvolutionofFashionTrendswithOne-ClassCol-
2(ShortPapers),615–621. laborativeFiltering.InProceedingsofthe25thInternational
ConferenceonWorldWideWeb.
Deutsch,D.;Bedrax-Weiss,T.;andRoth,D.2021. Towards
Question-AnsweringasanAutomaticMetricforEvaluating Ji, Y.; Wu, W.; Zheng, H.; Hu, Y.; Chen, X.; and He, L.
theContentQualityofaSummary. TransactionsoftheAs- 2023. Is chatgpt a good personality recognizer? a prelim-
sociationforComputationalLinguistics,9. inarystudy. arXivpreprintarXiv:2307.03952.
Kryscinski, W.; McCann, B.; Xiong, C.; and Socher, R.
Doddapaneni, S.; Sayana, K.; Jash, A.; Sodhi, S.; and
2020. Evaluating the Factual Consistency of Abstractive
Kuzmin,D.2024. UserEmbeddingModelforPersonalized
Text Summarization. In Proceedings of the 2020 Confer-
Language Prompting. In Proceedings of the 1st Workshop
enceonEmpiricalMethodsinNaturalLanguageProcessing
on Personalization of Generative AI Systems (PERSONAL-
(EMNLP).
IZE2024).
Krys´cin´ski, W.; Rajani, N.; Agarwal, D.; Xiong, C.; and
Durmus,E.;He,H.;andDiab,M.2020. FEQA:AQuestion
Radev, D. 2021. BookSum: A Collection of Datasets for
Answering Evaluation Framework for Faithfulness Assess-
Long-formNarrativeSummarization.
ment in Abstractive Summarization. In Proceedings of the
Kwon, M.; Xie, S. M.; Bullard, K.; and Sadigh, D. 2023.
58th Annual Meeting of the Association for Computational
Reward design with language models. arXiv preprint
Linguistics.
arXiv:2303.00001.
Fabbri, A.; Wu, C.-S.; Liu, W.; and Xiong, C. 2022.
Lee, H.; Phatale, S.; Mansoor, H.; Mesnard, T.; Ferret, J.;
QAFactEval: Improved QA-Based Factual Consistency
Lu, K.; Bishop, C.; Hall, E.; Carbune, V.; Rastogi, A.; and
EvaluationforSummarization. InProceedingsofthe2022
Prakash,S.2024. RLAIFvs.RLHF:ScalingReinforcement
Conference of the North American Chapter of the Associa-
LearningfromHumanFeedbackwithAIFeedback. InPro-
tionforComputationalLinguistics:HumanLanguageTech-
ceedings of the 41st International Conference on Machine
nologies.
Learning(ICML).
Fabbri, A. R.; Krys´cin´ski, W.; McCann, B.; Xiong, C.; Li,R.;Deng,W.;Cheng,Y.;Yuan,Z.;Zhang,J.;andYuan,
Socher,R.;andRadev,D.2021. SummEval:Re-evaluating F.2023.ExploringtheUpperLimitsofText-BasedCollabo-
SummarizationEvaluation. rativeFilteringUsingLargeLanguageModels:Discoveries
andInsights.
GeminiTeam; et al. 2024a. Gemini 1.5: Unlocking multi-
modal understanding across millions of tokens of context. Lin,C.-Y.2004. ROUGE:APackageforAutomaticEvalu-
arXiv:2403.05530. ation of Summaries. In Text Summarization Branches Out,
74–81. Barcelona, Spain: Association for Computational
GeminiTeam; et al. 2024b. Gemini: A Family of Highly
Linguistics.
CapableMultimodalModels. arXiv:2312.11805.
Liu, J.; Liu, C.; Zhou, P.; Lv, R.; Zhou, K.; and Zhang, Y.
Goyal, T.; and Durrett, G. 2020. Evaluating Factuality in 2023. Is ChatGPT a Good Recommender? A Preliminary
GenerationwithDependency-levelEntailment. InFindings Study.
of the Association for Computational Linguistics: EMNLP
Lyu, H.; Jiang, S.; Zeng, H.; Xia, Y.; Wang, Q.; Zhang, S.;
2020.
Chen,R.;Leung,C.;Tang,J.;andLuo,J.2024. LLM-Rec:
Goyal,T.;Li,J.J.;andDurrett,G.2023. NewsSummariza- Personalized Recommendation via Prompting Large Lan-
tionandEvaluationintheEraofGPT-3. guageModels.Ning, L.; Liu, L.; Wu, J.; Wu, N.; Berlowitz, D.; Prakash,
S.; Green, B.; O’Banion, S.; and Xie, J. 2024. User-LLM:
EfficientLLMContextualizationwithUserEmbeddings.
Ouyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright,
C.L.;Mishkin,P.;Zhang,C.;Agarwal,S.;Slama,K.;Ray,
A.; Schulman, J.; Hilton, J.; Kelton, F.; Miller, L.; Simens,
M.; Askell, A.; Welinder, P.; Christiano, P.; Leike, J.; and
Lowe,R.2022. Traininglanguagemodelstofollowinstruc-
tionswithhumanfeedback.
Rao,H.;Leung,C.;andMiao,C.2023. Canchatgptassess
humanpersonalities?ageneralevaluationframework.arXiv
preprintarXiv:2303.01248.
Roit, P.; Ferret, J.; Shani, L.; Aharoni, R.; Cideron, G.;
Dadashi,R.;Geist,M.;Girgin,S.;Hussenot,L.;Keller,O.;
et al. 2023. Factually consistent summarization via rein-
forcementlearningwithtextualentailmentfeedback. arXiv
preprintarXiv:2306.00186.
Salemi, A.; Mysore, S.; Bendersky, M.; and Zamani, H.
2024. LaMP:WhenLargeLanguageModelsMeetPerson-
alization.
Shazeer,N.;andStern,M.2018. Adafactor:Adaptivelearn-
ingrateswithsublinearmemorycost. InInternationalCon-
ferenceonMachineLearning,4596–4604.PMLR.
Stiennon, N.; Ouyang, L.; Wu, J.; Ziegler, D.; Lowe, R.;
Voss, C.; Radford, A.; Amodei, D.; and Christiano, P. F.
2020. Learning to summarize with human feedback.
Advances in Neural Information Processing Systems, 33:
3008–3021.
Williams, R. J. 1992. Simple statistical gradient-following
algorithms for connectionist reinforcement learning. Ma-
chinelearning,8(3-4):229–256.
Wu,B.;Shi,Z.;Rahmani,H.A.;Ramineni,V.;andYilmaz,
E. 2024a. Understanding the Role of User Profile in the
PersonalizationofLargeLanguageModels.
Wu, L.; Zheng, Z.; Qiu, Z.; Wang, H.; Gu, H.; Shen, T.;
Qin, C.; Zhu, C.; Zhu, H.; Liu, Q.; Xiong, H.; and Chen,
E.2024b. ASurveyonLargeLanguageModelsforRecom-
mendation.
Yan, A.; He, Z.; Li, J.; Zhang, T.; and McAuley, J.
2022. Personalized Showcases: Generating Multi-Modal
Explanations for Recommendations. arXiv preprint
arXiv:2207.00422.
Yang, K.; Klein, D.; Celikyilmaz, A.; Peng, N.; and Tian,
Y. 2023. Rlcd: Reinforcement learning from contrast dis-
tillation for language model alignment. arXiv preprint
arXiv:2307.12950.
Zhang,T.;Kishore,V.;Wu,F.;Weinberger,K.Q.;andArtzi,
Y. 2020. BERTScore: Evaluating Text Generation with
BERT. InInternationalConferenceonLearningRepresen-
tations.A AutomatedEvaluationDetails B DirectRLAIFBaseline
Table 5 shows the prompt we used for automated evalua- Followingthepreviouswork(Leeetal.2024),weusedthe
tion, which assesses the summary qualify from three axes: following prompt to compute reward scores for generated
(1) Factuality, (2) Abstractiveness, and (3) Readability. We summariesduringRLAIFtraining.
also asked auto raters for an overall assessment. The order
ofthetwosummariesinthepromptisselectedrandomlyto
avoidevaluationbiascausedbyposition.Detailedresultsare Agoodsummaryisashorterpieceoftextthathas
showninTable6andTable7. the essence of the original. It tries to accomplish
thesamepurposeandconveysthekeyinformation
from the original input text. Below we define four
Youareanevaluatortojudgewhichusersummary
evaluation axes for summary quality: coherence,
is better. The user summary will be used in agent
accuracy, coverage, and overall quality.
systems for personalization.
You will be provided with two user summaries to
Coherence: This axis answers the question
compare with, along with the input user activity
“how coherent is the summary on its own?” A
history that was used to generate those two sum-
summary is coherent if it’s easy to understand
maries.
when read on its own and free of English errors.
Your job is to evaluate the summaries based on
A summary is not coherent if it’s difficult to
following three criteria:
understand what the summary is trying to say.
Generally, it’s more important that the summary
1. Factuality: Does the user summary only
is understandable than it being free of grammar
contain facts supported by the activity his-
errors.
tory?
Accuracy: This axis answers the question “does
2. Abstractiveness: Does the user summary
the factual information in the summary accurately
capture the key insights of the user based
match the input user history?” A summary is ac-
on their activity history in a concise manner,
curate if it doesn’t say things that aren’t in the
instead of repeating the activity history?
input user hisory and generally is not misleading.
3. Readability: Is the user summary fluent
Coverage:Thisaxisanswersthequestion“howwell
and readable? And based on these, also give
doesthesummarycovertheimportantinformation
an overall evaluation.
in the input user history?” A summary has good
coverage if it mentions the main information from
Here is the user activity history (between
the input user history text that’s important to un-
<history> and </history>):
derstandthesituationdescribedintheuserhistory.
<history> {input activities} </history>
A summary has poor coverage if someone reading
onlythesummarywouldbemissingseveralimpor-
Here is the first user summary (between
tant pieces of information about the situation in
<summary1> and </summary1>):
the input text.
<summary1> SUMMARY 1 </summary1>
Overall quality: This axis answers the question
“how good is the summary overall at representing
Here is the second user summary (between
the input user history?” This can encompass all
<summary2> and </summary2>):
of the above axes of quality, as well as others you
<summary2> SUMMARY 2 </summary2>
feelareimportant.Ifit’shardtofindwaystomake
the summary better, the overall quality is good. If
For each criteria and the overall comparison:
there are lots of different ways the summary can
Respond 0 if the first summary is better; Respond
be made better, the overall quality is bad.
1 if two summaries are equally good; Respond 2 if
the second summary is better.
You are an expert user summary rater. Given
Please structure your response in the following
a USER HISTORY TEXT as user past activity
format with no extra explanation.
history in text and a SUMMARY that summarizes
Factuality: Your choice of 0/1/2
users’preferencebasedonthepastactivityhistory,
Abstractiveness: Your choice of 0/1/2
your role is to provide a SCORE from 1 to 10
Readability: Your choice of 0/1/2
thatratesthequalityoftheSUMMARYgiventhe
Overall: Your choice of 0/1/2
USER HISTORY TEXT, with 1 being awful and
10 being a perfect SUMMARY.
Response:
USER HISTORY TEXT: {input activities}
Table 5: Evaluation Criteria and Prompt Used for SUMMARY: {summary}
AutomatedEvaluation. SCORE:Dataset Criteria 0-ShotWin Equal RLPFWin EqualRatio RLPFWinRate
Factuality 2908 1 4611 0.0% 61.3%
MovieLens Abstractiveness 2814 8 4698 0.1% 62.5%
2015 Readability 2441 1024 4055 13.6% 62.4%
Overall 2822 0 4698 0.0% 62.5%
Factuality 2043 644 5505 7.9% 72.9%
Amazon Abstractiveness 2414 107 5671 1.3% 70.1%
Books Readability 2133 764 5295 9.3% 71.3%
Overall 2426 83 5683 1.0% 70.1%
Factuality 1835 6 6351 0.1% 77.6%
Google
Abstractiveness 1262 5 6925 0.1% 84.6%
Local
Readability 1264 425 6503 5.2% 83.7%
Review
Overall 1273 0 6919 0.0% 84.5%
Table6:AutomatedEvaluationResultsonComparingGemini1.0Nano-2zeroshotv.s.RLPF.Notethatthetotalnumberof
examplesmayfluctuateasweexcludeinstanceswhereeitherresponsedoesn’tadheretotheguidelines.
Dataset Criteria RLAIFWin Equal RLPFWin EqualRatio RLPFWinRate
Factuality 2814 0 4696 0.0% 62.5%
MovieLens Abstractiveness 3298 0 4212 0.0% 56.1%
2015 Readability 3217 138 4155 1.8% 56.4%
Overall 3297 0 4213 0.0% 56.1%
Factuality 4890 23 3272 0.3% 40.1%
Amazon Abstractiveness 4960 27 3198 0.3% 39.2%
Books Readability 3848 2222 2115 27.1% 35.5%
Overall 4972 12 3201 0.1% 39.2%
Factuality 4088 21 4083 0.3% 50.0%
Google
Abstractiveness 3708 31 4453 0.4% 54.6%
Local
Readability 3154 2349 2689 28.7% 46.0%
Review
Overall 3750 0 4442 0.0% 54.2%
Table7:AutomatedEvaluationResultsonComparingRLAIFv.s.RLPF.Notethatthetotalnumberofexamplesmayfluctuate
asweexcludeinstanceswhereeitherresponsedoesn’tadheretotheguidelines.
C DatasetDetails D TrainingHyperparameters
• AmazonReview(HeandMcAuley2016)encompasses During training, we set the policy temperature to 1.0 for
avastcollectionofproductreviewsfromAmazon,span- Gemini1.0Nano-2and0.9forPaLM-2XS,whilemaintain-
ning various categories, offering insights into customer ingapredictiontemperatureof0.1duringrewardcomputa-
preferences.WeuseddomainBooksfortrainingandad-
tion. We employed a learning rate of 1e-6 with 100 warm-
ditionalourdomainsfordatasettransferevaluation. upsteps,duringwhichonlythevaluenetworkwasupdated
• Google Local Review (Yan et al. 2022) contains user while the policy network remained frozen. The weight w
reviews and ratings for local businesses across different that controls the balance between prediction feedback re-
locations,reflectinguserexperiencesandsentimentsto- wardandlengthrewardwassetto1andtheKLdivergence
wardslocalservices.WechoseNewYorkstateinourex- coefficientαwassetto0.01,andthelengthrewardparam-
periments. eterswereconfiguredasC =1e-4andβ =3e-3.Thetarget
• MovieLens2015(HarperandKonstan2015)commonly length L in the length reward was set to 358, 242, and 268
known as MovieLens20M, a widely used benchmark for Amazon Books, Google Local Review, and MovieLens
datasetforpersonalizationandrecommendersystems. 2015,respectively,tomatchtheZero-Shotsumamries.The
input lengths for the policy models were 1600, 1280, and
• MovieLens2003(HarperandKonstan2015)alsoknown
1024forAmazonBooks,GoogleLocalReview,andMovie-
as MovieLens1M. Unlike MovieLens 2015, this dataset
Lens 2015, respectively, with a consistent output length of
has different users, movies, and collection times, plus
512 across all datasets. Training was conducted for 15,000
userdemographicdata.
stepsforeachdataset,andallreportedevaluationresultsare
SeeTable8fordetaileddatasetstatistics. basedonthefinalcheckpoint.#Total #Users #Train #Test Sequence
Dataset Abbreviation Usage
Items inTrain Examples Examples LengthN
MovieLens2015 ML2015 Train&Eval 27,278 82,977 13,821,405 8,192 50
MovieLens2003 ML2003 EvalOnly 3,882 4,247 716,003 4,247 50
GoogleLocalReview GoogleLocal Train&Eval 270,721 80,080 3,387,375 8,192 50
AmazonReview
AmazonBooks Train&Eval 2,935,525 64,191 4,936,920 8,192 50
-Books
AmazonReview
AmazonMovies EvalOnly 203,970 5,516 356,329 5,516 50
-MoviesandTV
AmazonReview
AmazonCDs EvalOnly 544,442 3,558 273,231 3,558 50
-CDsandVinyl
AmazonReview
AmazonToys EvalOnly 634,414 1,242 41,741 1,242 50
-ToysandGames
AmazonReview
AmazonGames EvalOnly 84,893 447 16,589 447 50
-VideoGames
Table8:DetailsofDatasetsUsedinExperiments.
E StatisticalSignificance F DetailedResults
We ran a paired t-test in Table 9 comparing the future ac- SeeTable10formoredetailedresultsforfutureactivitypre-
tivity prediction performance of RLPF and the Gemini 1.0 dictionandfavoritegenre/categorypredction.SeeTable11
Nano-2 Zero-Shot model. The results demonstrate that the forinputtokenlengthcomparisonbetweensummariesaand
improvementsachievedbyRLPFarestatisticallysignificant. fullusercontext.
G TaskDefinitionDetails
Dataset t-statistic p-value Detailed definitions and information about tasks in experi-
AmazonBooks -9.4155 6.01e-21 mentsarelistedinTable12.Promptsthatareusedforpre-
GoogleLocal -10.0829 9.05e-24 dictionsarelistedinTable14inAppendixI.
ML2015 -15.3415 2.15e-52
H ExampleSummaries
Table9:Pairedt-testresultscomparingGemini1.0Nano-2 ExamplesummariesaredisplayedinFigure5,6,7,8,9,10.
Zero-Shot summary and RLPF summary in future activity
prediction.AmazonBooks GoogleLocalReview MovieLens2015 Overall
PredictionInput
Future Fav Future Fav Future Fav
Average
Activity Category* Activity Category* Activity Category*
FirstX Activities 0.497 0.586 0.451 0.467 0.64 0.763 0.567
RandomX Activities 0.489 0.59 0.451 0.473 0.668 0.798 0.578
LastX Activities 0.582 0.587 0.492 0.472 0.738 0.753 0.604
AllN Activities 0.530 0.596 0.507 0.494 0.733 0.910 0.628
Gemini1.0Nano-20-Shot 0.664 0.594 0.483 0.487 0.607 0.774 0.601
Gemini1.0Pro0-Shot 0.647 0.585 0.504 0.488 0.595 0.843 0.610
Gemini1.0Nano-2CraftedPrompt 0.684 0.541 0.523 0.419 0.653 0.899 0.620
Gemini1.0Nano-2RLAIF 0.654 0.613 0.458 0.513 0.519 0.776 0.589
Gemini1.0Nano-2RLPF(ours) 0.709 0.605 0.560 0.559 0.719 0.818 0.662
ImprovoverNano-20-shot 6.78% 1.85% 15.93% 14.78% 18.46% 5.68% 10.00%
ImprovoverAllNActivities 33.77% 1.51% 10.44% 13.15% -1.91% -10.11% 5.30%
Table 10: Evaluation Results on Future Activity Prediction and Favorite Category Prediction Tasks between Different Ap-
proaches.*denotedasunseentasksfromtraining.X =16,10,13forAmazonBooks,GoogleLocalReview,MovieLens2015
respectivelytokeepthetokenlengthcomparablewithRLPFsummaries.
AmazonBooks GoogleLocal ML2015 Average
AllNActivities 812 642 791 748
Gemini1.0Nano-20-Shot 349 260 272 293
Gemini1.0Nano-2RLPF(ours) 267 133 198 200
RLPFCompressionRatio
67.0% 79.3% 74.9% 73.8%
OverAllNActivities
Table11:AverageTokenLengthofDifferentApproaches.TaskName Dataset Metric Abbreviation DetailedDescription
Predict the item name (e.g., movie, product, or
AmazonBooks place)theuserwillinteractwithinthefuture,pre-
F Pu ret du ir ce tiA oc ntivity A Am ma az zo on nM CDov sies Accuracy FutureAct s de on mte ld yi sn ela ecm teu dlt nip el ge a-c tih vo eic oe ptf io or nm s.a Ttw hii sth tat sh kre se err van es-
AmazonToys as the sole task used in both training and evalua-
AmazonGames tion.
GoogleLocal
ML2015
ML2003
Predict the user’s favorite genre or category. The
label is determined as the most frequent genre/-
categoryappearingintheentireuser’shistory.For
MovieLens, the model predicts three out of 19
genres.ForAmazonBooksandGoogleLocalRe-
view,duetotheirlargercategoryvocabularies,we
F orav Co ar ti ete goG ryenre A Gm ooa gz lo en LB oo cao lks Recall@3 F Fa av vG Cae tn er ge oo ryr l fi rm eqi ut eth ne tcm ao ted ge ol’ rs iec sh ,o ai gc ae is nt po roth me pu ts iner g’s foto rp th1 re0 em po res -t
Prediction ML2015 dictions.Importantly,thegenre/categoryinforma-
ML2003 tionforeachitemisnotincludedintheusercon-
text provided for summary generation. This task
evaluatesthesummarizationmodel’sabilitytoin-
fer high-level user insights based on real-world
knowledge.
Predict the rating a user would assign to a spec-
R Pra et din icg
tion
A Gm ooa gz lo en LB oo cao lks Accuracy Rating i ifi tee md ait sem th. eI tn argo eu tr ae cx tip ve ir ti ym ie nn Fts u, tuw re
e
Asp ce tc ivifi ite yd Pt rh ee
-
ML2015 diction.
ML2003
Predict the city the user has visited most fre-
quently on Google Local Review dataset. This
Common
GoogleLocal Recall@3 CommonCity taskassesseswhetherausersummarycanfurnish
Activity
sufficientinformationforusercontextretrieval,a
Retrieval
crucialfunctionalityforagentsystems.
Given two exemplar reviews written by the user
Review AmazonBooks ROUGE- ReviewGen for other items, generate a review for a specified
Generation Lsum itemthatalignswiththeuser’swritingstyle.
UserAge ML2003 Accuracy UserAge Predicttheuser’sagerange.
Prediction
UserGender ML2003 Accuracy UserGender Predicttheuser’sgender.
Prediction
UserOccupancy ML2003 Recall@3 User Predicttheuser’soccupancy.
Prediction Occupancy
Table12:PredictionTaskInformationandDefinitions.Figure5:ExampleSummaryComparisononAmazonBooks.Thereareduplicateparts(orange)inZero-Shotsummary,and
hallucinationsinRLAIFsummary(red),whilesummarygeneratedbyRLPFadherestothefactshighlightedingreen.Activity
datastatistics(outof50bookreviewactivities):Fiction:34,Thrillers:12,Writtenbywoman:20.
Figure6:ExampleSummaryComparisononAmazonBooks.Thereareduplicateparts(orange)inZero-Shotsummary,andtoo
generalsummaryinRLAIF(blue),whilesummarygeneratedbyRLPFadherestothefactshighlightedingreen.Activitydata
statistics(outof50bookreviewactivities):Fiction:25,RomanceNovels:11,Historical:4,BooksaboutNatureorEnvironment:
4.Figure 7: Example Summary Comparison on Google Local Review. There are duplicate parts (orange) and hallucination in
both Zero-Shot summary. RLAIF’s summary also has the issue of being too general summary (blue). Activity data statistics
(outof50placereviewactivities):Italian:7,Chinese:2,Mexican:2,PopularTouristDestinations:3.
Figure8:ExampleSummaryComparisononGoogleLocalReview.Zero-Shotsummaryistoogeneral(blue)whileRLAIF’s
summary contains of duplication (orange). Activity data statistics (out of 50 place review activities): Italian Restaurants: 9,
Mexico:1,American:3,Restaurantschain:7.Figure9:ExampleSummaryComparisononMovieLens2015.Therearehallucination,whichdoesn’texistintheinputactiv-
ities(red)inZero-Shotsummary.RLAIF’ssummaryalsohastheissueofbeingtoogeneralsummary(blue),whilesummary
generatedbyRLPFadherestothefacts(green).Activitydatastatistics(outof50moviewatchactivities):Drama:19,Action:
11,Comedy:10,Thriller:9,ScienceFiction:5,Romance:5,Crime:5,Sports:4,Historical:4,Horror:2.Figure 10: Example Summary Comparison on MovieLens 2015. There are hallucination, which doesn’t exist in the input
activities (red) as well as simple repetition of the input activities (orange) in Zero-Shot summary. RLAIF’s summary also
sufferstheissueofbeingtoogeneral(blue)orrepetition(orange).WealsoobservethatsummarygeneratedbyRLPFhassome
hallucination (red). Activity data statistics (out of 50 movie watch activities): Comedies: 5, Dramas: 7, Horror: 13, Science
Fiction:3.UnitedKingdomMovie:1,FranceMovie:0,ItalyMovie:0,JapanMovie:1.I Prompts • UserAgePrediction:
Based on this information, what is
PromptforSummaryGenerationandFuture
my age range? Choose one from the
ActivityPrediction
following age ranges:
SeeTable13andforfullpromptsweusedtogeneratesum- Under 18, 18-24, 25-34, 35-44, 45-49,
maries during training and evaluation baselines, and Table 50-55, 56+.
14forfutureactivitypredictionpromptusedduringreward Structure the answer with one age
computationandevaluation. range only without extra words.
Answer:
DifferentSummarizationandPredictionPrompts • UserOccupationPrediction:
toAssessPromptRobustness Based on this information, what is
my occupation? Choose three from the
Different summarization and prediction prompts used in
following occupations:
Figure “Prompt Robustness” are shown in Table 15. In ad-
academic, artist, clerical, college
dition, all the prediction prompts for unseen tasks used in
student, customer service, health
experimentsarelistedbelow.
care, executive, farmer, homemaker,
K-12 student, lawyer, programmer,
PromptsforUnseenPredictionTasks
retired, sales, scientist,
MovieLens2015andMovieLens2003 self-employed, technician, craftsman,
Allunseenpredictiontasksforthesetwodatasetssharethe unemployed, writer, other.
samepromptpreamble: From the list above, select three
occupations that are an exact match
and structure the answer in the
Here is a summary of my movie
format of "OCCUPATION1→OCCUPATION2→
watch preference (between
OCCUPATION3".
<summary> and </summary>).
Answer:
<summary>SUMMARY</summary>
AmazonReview
Foreachpredictiontask,thepromptsuffixisasfollowing: All unseen prediction tasks for Amazon Review dataset
sharethesamepromptpreamble:
• FavoriteGenrePrediction:
Based on this information, what are
Here is a summary of my interest
my top 3 favorite movie genre? Choose
three from the following genres:
about {domain} products based on
my product review history (between
Drama, Comedy, Thriller, Action,
Romance, Adventure, Crime, Sci-Fi,
<summary> and </summary>).
Horror, Fantasy, Mystery, Children,
<summary>SUMMARY</summary>
War, Animation, Musical, Documentary,
Western, IMAX, Film-Noir. Foreachpredictiontask,thepromptsuffixisasfollowing:
Structure the answer in the format of
• FavoriteCategoryPrediction:
"GENRE1→ GENRE2→GENRE3".
Based on this information, what
Answer:
are my top 3 favorite categories
• RatingPrediction: of {domain}? Choose three from
Based on this information, what is my the following categories that are
rating on the following movie? separated by "-->":
Answer a score between 1 to 5 {choices}
inclusive, using half-point(0.5) From the list above, select
increments. Structure the answer with three categories that are an
"[Your choice from (1.0, 1.5, 2.0, exact match and structure
2.5, 3.0, 3.5, 4.0, 4.5, 5.0)]." the answer in the format of
Answer: "CATEGORY1-->CATEGORY2-->CATEGORY3".
Answer:
• UserGenderPrediction:
Based on this information, what is my • RatingPrediction:
gender? Choose one from the following Based on this information, what is
genders: my rating on the following {domain}
Male, Female. product?
Structure the answer with one gender Answer a score between 1 to 5
only without extra words. inclusive, using full-point(1.0)
Answer: increments.Structure the answer with ’[Your the following city names that are
choice from (1.0, 2.0, 3.0, 4.0, separated by "-->":
5.0)].’ {choices}
Answer: From the list above, select three
city names that are an exact match
• ReviewGeneration:
In addition, here is a list of few and structure the answer in the
example reviews I wrote for each format of "CITY1-->CITY2-->CITY3".
product in {domain}, in format of Answer:
[PRODUCT NAME: "REVIEW TEXT"]:
Based on this information, write a
review based on my preference and
writing style for the following
product in {domain}:
Structure the answer with ’[Your
review text for this product]’.
Answer:
GoogleLocalReview
AllunseenpredictiontasksforGoogleLocalReviewdataset
sharethesamepromptpreamble:
Here is a summary of my place
visit pattern and preference in
{state} state based on my place
visit history (between <summary>
and </summary>).
<summary>SUMMARY</summary>
Foreachpredictiontask,thepromptsuffixisasfollowing:
• FavoriteCategoryPrediction:
Based on this information, what are
my top 3 favorite categories for
places to visit in {state} state?
Choose three from the following
categories that are separated by
"-->":
{choices}
From the list above, select
three categories that are an
exact match and structure
the answer in the format of
"CATEGORY1-->CATEGORY2-->CATEGORY3".
Answer:
• RatingPrediction:
Based on this information, what is
my rating on the following place in
{state} state?
Answer a score between 1 to 5
inclusive, using full-point(1.0)
increments.
Structure the answer with "[Your
choice from (1.0, 2.0, 3.0, 4.0,
5.0)]."
Answer:
• CommonActivity(City)Retrieval:
Based on this information, what are
the top 3 cities I visited most in
{state} state? Choose three fromDataset Zero-Shot/RLPFPrompt CraftedPrompt
Summarizemymoviewatchhistoryfromthreeaspects.
First,listthreeofmyfavoritemoviegenres.
Second,summarizethetimeperiodsofmoviesIlike,e.g.1980s,1990setc.
Here is a list of movies
Third,providefivemoviesbelongingtomyfavoritegenres.
that I have watched in the
format of [MOVIE NAME
Forexample:
(YEAR), MY RATING],
Favoritemoviegenresare<GENRE1>,<GENRE2>,<GENRE1>
MovieLens where RATING=1 is the Timeperiodsoflikedmoviesare<TIME1>,<TIME2>
2015 lowest and RATING=5 is Example movies are <MOVIE1>, <MOVIE2>, <MOVIE3>,
MovieLens thehighest: <MOVIE4>,<MOVIE5>
2003 {input activities}
Please summarize my
HereisalistofmoviesthatIhavewatchedintheformatof[MOVIE NAME
movie watch preference
(YEAR), MY RATING], where RATING=1 is the lowest and RATING=5 is
within200words.
thehighest:
{input activities}
Pleasesummarizemymoviewatchpreferencewithin200words.
Summarizemyinterestabout{domain}productsfromtwoaspects.
Here is a list of {domain} First,listthreeofmyfavoritecategoriesof{domain}.
products that I have Second, provide five products belonging to my favorite categories of
purchased and ratings {domain}.
I gave for each prod-
uct in the format of Forexample:
[PRODUCT NAME, Favoritecategoriesof{domain}are<CATEGORY1>,<CATEGORY2>,
Amazon MY RATING], where <CATEGORY3>
Review MY RATING=1isthelow- Exampleproductsare<PRODUCT1>,<PRODUCT2>,<PRODUCT3>,
est and MY RATING=5 is <PRODUCT4>,<PRODUCT5>
thehighest:
{input activities} Here is a list of {domain} products that I have purchased and ratings I gave
Pleasesummarizemyinter- foreachproductintheformatof[PRODUCT NAME,MY RATING],where
est about {domain} within MY RATING=1isthelowestandMY RATING=5isthehighest:
200words. {input activities}
Pleasesummarizemyinterestabout{domain}within200words.
Summarizemylocationhistorypatternandplacevisitpreferencefromthree
aspects.
First,listthreeofmyfavoritecategoriesforplacestovisitin{state}state.
Hereisalistofplacesthat
Second,listthreecommoncitiesIvisitedin{state}state.
I have visited in {state}
Third, provide five places belonging to my favorite categories for places to
state and ratings I gave for
visit.
each place in the format
of [PLACE NAME,
Forexample:
CITY NAME,
Favorite categories for places to visit are <CATEGORY1>, <CATE-
MY RATING], where
Google GORY2>,<CATEGORY3>
MY RATING=1isthelow-
Local CommoncitiesIvisitedare<CITY1>,<CITY2>,<CITY3>
est and MY RATING=5 is
Review Example places are <PLACE1>, <PLACE2>, <PLACE3>,
thehighest:
<PLACE4>,<PLACE5>
{input activities}
Please summarize my
HereisalistofplacesthatIhavevisitedinNewYorkstateandratingsIgave
location history pattern
foreachplaceintheformatof[PLACE NAME,CITY NAME,MY RATING],
and place visit preference
whereMY RATING=1isthelowestandMY RATING=5isthehighest:
within200words.
{input activities}
Pleasesummarizemylocationhistorypatternandplacevisitpreferencewithin
200words.
Table13:Differentsummarizationprompts.Zero-Shot/RLPFPrompt:Thispromptisutilizedforbothtrainingthesummarizer
model and evaluating the ”Gemini 1.0 Nano-2 Zero-Shot” baseline. Crafted Prompt: Employed for baseline evaluation with
”Gemini1.0Nano-2withCraftedPrompts.”Thehighlightedtextwithinthesepromptsrepresentsadditionalinstructionstailored
tospecificdownstreamtaskpredictions.ForAmazonReview,{domain}isreplacedwithBooks,MoviesandTV,CDsandVinyl,
ToysandGames,VideoGamesetc.{domain}isreplacedwithNewYork.Dataset FutureActivityPredictionPrompt
Hereisasummaryofmymoviewatchpreference(between<summary>and</summary>).
<summary>{summary}</summary>.
MovieLens Baseonthisinformation,whichmoviewillIwatchnext?Chooseonefromthefollowingoptions:
2015 {choices}
Structuretheanswerwith’[Indexofyourchoice(A/B/C/D)]’.
Answer:
Hereisasummaryofmyproductpurchasepreference(between<summary>and</summary>).
<summary>{summary}</summary>.
Amazon Baseonthisinformation,whichproductwillIpurchasenext?Chooseonefromthefollowingoptions:
Review {choices}
Structuretheanswerwith’[Indexofyourchoice(A/B/C/D)]’.
Answer:
Hereisasummaryofmyplacevisitpreference(between<summary>and</summary>).
<summary>{summary}</summary>.
Google
Baseonthisinformation,whichplacewillIvisitnext?Chooseonefromthefollowingoptions:
Local
{choices}
Review
Structuretheanswerwith’[Indexofyourchoice(A/B/C/D)]’.
Answer:
Table14:PromptforFutureActivityPrediction,usedinbothRLPFtrainingduringrewardcomputationandevaluation.
Usage Prompt1 Prompt2
Here is a list of movies that I have watched in the format of
Here is a list of movies
[MOVIE NAME (YEAR), MY RATING], where RATING=1 is
that I have watched in the
thelowestandRATING=5isthehighest:
format of [MOVIE NAME
Summarization {input activities}
(YEAR), MY RATING],
Please summarize my movie watch preference within 200 words.
where RATING=1 is the
Agoodsummaryisashorterpieceoftextthathastheessence
lowest and RATING=5 is
of users’ preference based on their past movie watch history.
thehighest:
Italsoconveysthekeyinformationaboutthepersonbasedon
{input activities}
theirpastwatchhistory.Thesummaryshouldbecoherentand
Please summarize my
easytounderstandwhenreadonitsown.Thefactualinforma-
movie watch preference
tioninthesummaryshouldaccuratelymatchtheperson’spast
within200words.
watchhistory.
Summary:
Here is a summary of
mymoviewatchpreference
(between<summary>and
</summary>). Hereisasummaryofmymoviewatchpreference(between<sum-
Prediction <summary>{summary}< mary>and</summary>).
/summary>. <summary>{summary}</summary>.
Base on this information, Baseonthisinformation,whichmoviewillIwatchnext?Choose
which movie will I watch onefromthefollowingoptions:{choices}
next? Choose one from Structuretheanswerwith’[Indexofyourchoice(A/B/C/D)]’.
the following options: Answer:
{choices}
Let’s think it step by
stepandstructurethean-
swer with the following
format: Rationale: [Why
do you choose this op-
tion] and Answer: [Your
Choice(A/B/C/D)].
Table15:PromptsusedinFigure“PromptRobustness”.Top:Differentsummarizationpromptsduringtraining.Bottom:Dif-
ferentpredictionpromptsduringrewardcomputation.Theadditionaltextsarehighlighted.