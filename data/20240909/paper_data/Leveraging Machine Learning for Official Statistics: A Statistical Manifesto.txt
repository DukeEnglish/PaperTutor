Leveraging Machine Learning for Official
Statistics: A Statistical Manifesto
Marco J.H. Puts1, David Salgado2, and Piet J.H. Daas1
1Department of Methodology, Statistics Netherlands, CBS-weg 11,
6412 EX, Heerlen, The Netherlands
2Department of Methodology, Statistics Spain, Avenida de
Manoteras 50-52, 28050 Madrid, Spain
February 2024
1 Introduction
1.1 Production of official statistics and machine learning
Evaluating the impact and utilization of machine learning (ML) in the pro-
duction of official statistics presents an ongoing challenge. ML is a subfield of
Artificial Intelligence, which aims at “not just to understand but also to build
intelligententities”(RussellandNorvig,2010). Itisthussimilartoassessingthe
impact of intelligent human activity on a production system, which is limitless.
ML itself is composed of different subfields about how the process of learning is
carriedout: supervisedlearning, unsupervisedlearning, reinforcementlearning,
etc. (Alpaydin, 2020; Murphy, 2013).
DespitethewidespreadadoptionofML,implementationstillhasmanychal-
lenges(O’Neil,2016,discussesthissubject). Despitethedangerandcomplexity
of ML, the compelling ’datafication’ of our society forces us to look at ML as
an addition to our (official) statistical toolbox. Datasets get larger, are more
detailed, and become more and more complex. Because of this, it becomes
increasingly difficult to perform (classical) statistical analysis on these kinds
of data. The need for algorithmic-based approaches, which can handle larger,
more complex, and unstructured datasets, is necessary to be able to perform
successful analysis; see Breiman (2001). Without it, we would not be able to
extract statistical information from many big data sources, like web scraped
data (Daas and van der Doef, 2020) and aerial images (De Jong et al., 2020).
ML encompasses various techniques, often referred to as ’a methodology’
within the realm of data science. However, most statisticians, as well as most
scientists, would disagree with the usage of this term. In social sciences and
econometrics, ”methodology,” ”methods,” and ”techniques” carry specific and
1
4202
peS
6
]LM.tats[
1v56340.9042:viXradistinct meanings, which we advocate for adhering to, at least when talking
about ML from an (official) statistical point of view.
When we look at the term ’methods’ this becomes most apparent. We have
observed that the term ’methods’ is incorrectly used in many fields that apply
ML.Itisoftenusedtomerelydescribethechosenenvironmentinwhichastudy
is performed. So, when summarizing the ML algorithms and hyperparameters
used, maybe with some kind of rationale, many data scientists assume this
describes the ’method’ used. Such a description, however, falls short when
viewed from the statistical standpoint of a methodologist.
So let’s start with the basis. Techniques, and how we combine them, are
primarilydeterminedbythe’why’and’what’questionsoftheapplication, a.o.:
• why are we doing it?
• why do we choose certain techniques?
• what are we going to do?
• what is our ground material?
• what is the context?
Itisthe’how’questionthatisatthecoreofthetechniquesthemselves: how
do we go about performing certain steps? In addition to the algorithmic de-
scription,itdescribesthe(pre-andpost-)conditionsforapplyingthetechnique.
From this, we can define a method as:
Amethodisasystematicprocedureoftechniquesforaccomplishing
a certain goal. Most of the time these methods are established.
and a technique as:
A technique is a way of carrying out a task. Most of the time these
techniques are described as algorithms.
For example, preparing a meal involves cutting vegetables, boiling eggs, and
grilling steaks. Recipes can be considered methods. it is also possible to con-
sider a method that takes into account the context in which one prepares a
meal, as well as the circumstances that may arise (for example, a guest may be
vegan or allergic to certain ingredients) when preparing a meal. What is the
bestwaytouseaspecifictechniquewithaspecificsetofingredientsunderwhat
circumstances? Methodology is the subject of this area. When it comes to ML,
this implies that we must pay greater attention to the context of methodology
when discussing it in relation to its application.
Admittedly, sometimes it is hard to determine if something is a method or
a technique. Since methods are procedural, they might be interpreted as an al-
gorithm and the other way around. It’s therefore easy to understand why there
is confusion about methods and techniques. By definition, they are so closely
2related that it is easy to confuse them, but for reasons that will become clearer
through the rest of this chapter, it is crucial to keep them separate.
Apriorieveryproductiontaskissusceptibletobeingimpingedbythegrow-
ing success of ML and deep learning techniques. At least we distinguish two
broad groups of production activities potentially affected by these techniques.
On the one hand, we have the inference problem providing us with a set of
statistics and indicators describing some fragment of social or economic reality.
This is, in our view, the core of the business of official statistical production.
On the other hand, complementarily important, we have numerous additional
tasks improving the quality of the first goal such as data collection, coding,
statistical dissemination, etc. In both cases, by and large, the use of ML boils
down to building a predictive model to be applied to new data, thus constitut-
ing a process step in the whole production cycle (See chapters 23-35 in Snijkers
et al. (2023): Dumpert, 2023; Measure, 2023; Moscardi and Schultz, 2023). For
example,predictingvaluesofacontinuoustargetvariablewillbeusefulinbuild-
ing model-assisted estimators (as a concrete illustrative example of a regression
task in the first group). Also, an automatic coding machine will provide a pre-
dicted category for a given statistical unit (as a concrete illustrative example of
a classification task in the second group).
In this chapter, we shall focus on supervised learning so that different al-
gorithms will be trained, validated, and tested on a given dataset to be then
applied to new data (see e.g. Hastie et al., 2009).
1.2 Production of official statistics and quality
Quality has been the spinal cord of the production of official statistics making
it possible to be used for highly relevant policy-making actions in all countries
and the international community (see e.g. United Nations, 2019, and references
therein). Nowadays, quality is a multidimensional concept (Karr et al., 2006)
and strongly oriented towards users’ needs and purpose (European Statistical
System Committee, 2022). Measuring and determining quality is thus very im-
portant. As a consequence, there exist complementary frameworks developed
to assess different aspects of the quality of the production of official statistics
(see e.g. Gootzen et al., 2023, and references therein).
We may cite the output quality approach of many statistical systems (see
e.g. European Statistical System Committee, 2022, in the context of the Eu-
ropean Statistical System), which focuses on the assurance of multiple quality
dimensions for the statistical outputs. In the light of the use of ML techniques,
Puts and Daas (2021) describe how their usage can influence the quality of offi-
cial statistics in several quality dimensions, namely relevance, accessibility and
clarity, coherence and comparability, and accuracy and reliability (see Euro-
pean Statistical System Committee, 2022, for definitions, specifically principles
11, 12, 14, and 15). Puts and Daas (2021) argue that the following topics must
be worked on regarding the methodology of ML in official statistics:
3• Accuracy and reliability
– Methodology concerning the human annotation of data
– Sampling the population to obtain representative training sets
– Using stratification in the context of ML
– Correcting the bias caused by the ML model
• Accessibility and clarity
– Datastructureengineeringandselectiontoincreasethetransparency
of models
– Explainability of ML models (explainable AI)
• Coherence and comparability
– Reducing spurious correlations
– Methodology for studying causation
– Dealing with concept drift (representativity over time)
Taking the European Statistics Code of Practice (European Statistical Sys-
tem Committee, 2022) as a starting point, Saidani et al. (2023) provide an
overview of all the attempts made to create an ML quality framework for of-
ficial statistics. Similar to Puts and Daas (2021), they also conclude that the
ESCoPprinciples11-15(statisticaloutput)arecrucialindefiningthequalityof
ML algorithms, with the additional consideration of principles 7-10 (statistical
processes). They add to this an extra quality dimension, called robustness. As
producers of official statistics, we need to make sure that the models are robust
for changes in the population.
However, in the following, we shall focus on the Total Survey Error Model
(TSEM henceforth) (Groves and Lyberg, 2010) as the starting point for our
proposal to adapt this framework to the use of ML in the production of official
statistics. As stated above, we shall simplify the use of these techniques to the
constructionofapredictivemodeleitherofcontinuous,semi-continuous,orcat-
egoricalvariablestrained,validated,andtestedonagivendatasettobeapplied
tonewdataforanypurpose(thuscoveringboththecoreandadditionalgroups
of tasks).
The TSEM is a comprehensive framework used in survey research and prac-
tice to understand and quantify the various sources of error that can affect the
accuracy of survey estimates (thus we are focusing on the reliability quality
dimension). The approach acknowledges that no survey is perfect, and errors
can arise at different stages of the survey process. The goal of the model is to
identify, measure, and minimize these errors to improve the overall quality of
survey data. This framework implicitly assumes that design-based inference is
4used to construct estimators and their accuracy assessment (confidence inter-
vals, variance estimation, ...).
IntheTSEM,theconceptsofpopulationunitandtargetvariablearecentral,
depicted by the so-called representation line and measurement line (see figure
1). Assuch,themodelidentifiesmeasurement-relatederrorsandrepresentation-
related errors. The measurement-related errors are associated to what is being
measured, whereas the representation-related errors are about the population
and its units and how they are included in the sample.
Figure 1: The total survey error model (source: Groves and Lyberg 2010)
We give a brief overview of the definitions of the different sources of error in
the TSEM: The measurement-related errors are:
• (Construct) Validity error:
Construct validity errors are related to whether a survey instrument is
effectivelymeasuringthetheoreticalconstructorconceptitisintendedto
measure. It assesses the degree to which the survey questions align with
the underlying construct of interest.
It is caused by an inadequate alignment between survey questions and
the conceptual framework being studied can lead to construct validity
concerns.
• Measurement error:
Measurement errors occur when there is a discrepancy between the true
value of a variable and the value obtained through measurement. This
can result from respondent misunderstanding, misreporting, or errors in
the survey instrument.
5It is caused by respondent biases, unclear survey questions, or issues with
data collection instruments.
• Processing error:
Processing errors involve mistakes made during the data collection and
dataprocessingstages,includingerrorsindataentry,coding,andediting.
Itiscausedbyhumanerrorinthehandlingandprocessingofsurveydata.
The representation-related errors are:
• Coverage error:
Coverage errors arise when the sampling frame does not accurately repre-
sent the target population. It includes individuals who should be in the
population but are not in the frame (undercoverage) or individuals in the
frame who are not in the population (overcoverage). It is caused by an
incomplete or inaccurate sampling frame.
• Sampling Error:
Sampling errors occur due to the inherent variability that arises when a
sample is used to estimate the characteristics of a population. It is the
difference between a sample statistic and the true population parameter.
It is caused by random chance in the selection of the sample.
• Nonresponse Error:
Nonresponse errors result from differences between respondents and non-
respondents. It occurs when individuals chosen for the survey do not
participate, andtheircharacteristicsdifferfromthosewhodoparticipate.
It is caused by refusals, inability to reach respondents, or other factors
leading to nonparticipation.
Putsetal.(2022)arguedthatsuchamodelcanalsobevalidforML.Firstof
all, the training set is a sample of the population, the measurements are imper-
fect (e.g. the result of annotations) and can contain errors, and the parameters
of a trained model can be seen as estimates. They argued that, due to the
presence of both measurement errors and representation errors in the training
datasetwithrespecttothepopulation, themodelwillgenerateerrorsandmost
of these errors will result in biases when applied to new data.
In contrast, Saidani et al. (2023) claim translation of TSEM errors into ML
presents a challenge. This observation is fundamentally accurate. TSEM was
originallydesignedforevaluatingsurveymethodologyerrors,butadaptingitto
ML presents a number of challenges. Moreover, quantifying the various errors
outlined in the TSEM is difficult, and achieving this task would, of course,
exceed the scope of this chapter.
In this document, we propose including a TSEM-like view on the construc-
tionofasupervisedlearningmodelasameanstoassessthequalityofthemodel.
It will be referred to as the Total Machine Learning Error model. While the
framework will be completed to the greatest extent possible for the time being,
6there might be some gaps that will need to be filled, thus providing further
opportunities for research and refinement in the future.
1.3 Outline of the chapter
The remainder of this chapter is structured as follows. After the introduction,
the next section focuses on populations and samples. Here, the viewpoint of
Deming on this topic provides valuable insights. This is followed by a descrip-
tionoftheTotalMachineLearningError(TMLE)Modelandthevariousphases
discerned. In section 4 an overview is given of the consequences of the TMLE-
model which provides valuable insights. This section is followed by an overview
ofsomeMLapplicationsthatbothinspiredandbenefitedfromthedevelopment
of the TMLE-model. The chapter ends with a discussion in which the most im-
portant topics for future research are listed.
2 Deming´s machine: populations and samples
For our purposes, let’s start with an example used by Deming (1942a) (later
on suggested as an exercise in his well-known textbook about survey sampling
(Deming, 1950)). An industrial business person owns an industrial machine
producing bolts with a set of technical specifications (weight, size, lengths, re-
sistance to temperature, etc.). Every, say, N = 100 units are packed up in a
box to be sold to retailers. For evident reasons, this person can pose two com-
plementary and different questions in this situation: (a) how many defective
bolts (failing technical specifications) exist in each box of N = 100 bolts? and
(b) how many defective bolts are produced on average by the machine? Notice
thattheseareaprioriindependentconcernsrightfullyrelevantinthissituation.
This simple example will allow us to introduce relevant statistical and mathe-
matical concepts which will be the basis for our proposed TMLE-model for the
production of official statistics.
Firstly, notice that in question (a) no random element exists in the formu-
lation of the concern: for each box, say U , with a specific known number of
i
bolts, say N , there exists a fixed, but unknown number of defective bolts, say
i
N (D) ≤ N . All we want is to know N (D) to monitor the output quality of
i i i
our production system. There is no reference to any probability distribution
underlying the box and quantities such as total, mean, and variance must be
understoodasnumericalaggregationfiguresmuchinthelineofexploratorydata
analysis. Thecharacteristicsofeachbolt,i.e.thetargetvariables1 arefixedbut
unknown numbers.
However,question(b)containsanimplicitreferencetoanunderlyingrandom
process or random experiment every time a bolt is produced by the machine.
1Inoursimplifiedcase,thedefectivenessbinaryindicatorδ (D)∈{0,1}.
k
7The question is meaningful only when randomness is recognized to be present
in the generation process of each bolt so that the result may be different, i.e.
sometimes defective sometimes working. The interest is not only focused on
the generation mechanism in the past but genuinely on (immediately) future
instances of the generation process.
Thisdistinctionallowsustoformalizeandmotivatethefollowingdefinitions,
which are implicitly used at all times in similar related statistical analyses. In
the context of question (a) a finite population U is a set of identifiable units u
k
which we usually denote by their labels k so that U ={1,...,N}. In this line,
a sample s is just a subset of units selected from U, i.e. s ⊂ N (see e.g. Cas-
sel et al., 1977). Ordered and/or with-replacement samples (Koop, 1974) can
be further expressed in rigorous mathematical language but the key underlying
concept is the same: a finite population is a collection of population units; no
probability measure is involved in the definition; it is a set-theoretic concept.
The characteristics of interest y of these population units are just numeric
k
variables, i.e. y ∈ D(q) for each variable q = 1,...,Q, where D(q) stands for
kq
the numerical set of possible values of variable y(q).
In the context of question (b), more subtleties are needed. The underlying
randomness (and probability space) enters into play by defining the (infinite)
population as the probability distribution function F of variables y, thus now
θ
turned into random variables Y. This distribution function F basically con-
θ
centrates the random generation mechanism of the values y for each bolt k, in
k
particular, for the defectiveness indicator variable δ(D). The concept of popu-
lation unit in this context is much subtler since the mathematical definition of
a random variable does not make any reference to such a concept. It is a mod-
ellingassumption. Inoursimplifiedmodellingscenarioforthemachine,wemay
conceiveofeachvariableδ (D)asarealizationofthesamebinaryrandomvari-
k
able δ(D) ≃ F . In this way, every time the random experiment is conducted
θ
(generation of a bolt), the random variable is realized (as when we toss a coin)
andanewvalueδ (D)isgenerated. Thismotivatesthefollowingdefinitionofa
k
sample (Casella and Berger, 2002): “the random variables Y ,...,Y are called
1 n
a random sample of size n from the population F if Y ,...,Y are mutually
θ 1 n
independent random variables and the marginal distribution function of each
variable Y is the same function F ”. Alternatively, they are called indepen-
k θ
dent and identically distributed random variables. Following these definitions,
a population unit in the context of question (b) amounts to each instance the
random experiment is conducted giving rise to a new bolt, thus identified with
this bolt. Notice how the concept of infinite population naturally fits in this
description: wehave,ontheonehand,alreadygeneratedboltsbut,ontheother
hand,wemayalsogenerateasmanyaswewantsincetheycomefromarandom
experiment.
In sum, the difference between questions (a) and (b) stems from the mod-
ellingassumptionofconceivingtargetvariablesvaluesy asrealizationsofran-
k
8dom variables or not, i.e. whether there exists an underlying random gener-
ation mechanism or not. In this line of reasoning, we can observe the two
fundamental concepts to assess quality in the production of statistics using the
TSEM, namely, population and variable. Assessing the quality of both con-
cepts amounts to assessing the quality of the final statistics. This is what the
TSEM does for the production of official statistics in the context of question
(a), thus motivating the design-based inference paradigm (see e.g. Till´e, 2020,
andmultiplereferencestherein),wherenoassumptionforanunderlyingrandom
generation mechanism is made for the target variables.
Thisanalysisdoesnotmeanthatmorecomplexmodellingassumptionscan-
notbemadeoralternativeinferentialparadigmscannotbefollowed(Chambers
and Clark, 2012; Little, 2012). Our proposal concentrates on how to fit the
increasing use of ML models into the classical quality assessment framework
provided by the TSEM.
3 The total machine learning error model
ToproposeaTMLE-model, weshallassumethatwearestillprovidinganswers
to question (a) in Deming’s machine scenario, which we understand as the tra-
ditional realm of the production of official statistics (typically by statistical
offices). This in contrast to question (b), which we understand as the natural
realm of the analysis of official statistics (typically by policy-makers, analysts,
researchers, and stakeholders in general). This is our reading of the difference
between enumerative vs. analytic surveys by Deming and Stephan (1941) and
Deming (1942b, 1953).
The challenge we face is two-fold. Firstly, we intend to provide a proposed
TMLE-model for supervising statistical learning models independently of their
specificuseinabusinessfunctionintheproductionprocess(seeBarrag´anetal.,
2024,inthisbookforsomeexamples). Secondly,weproposeacombinationwith
the TSEM to produce statistics in a general fashion.
LiketheTSEM,theTMLE-modeldealswiththetotalerroronanMLmodel
asaresultofdifferenterrorsintroducedduringtheprocessofcreatingthetrain-
ing set and assessing the model with the test set to be subsequently applied to
ourtargetpopulationtoproducethestatisticsofinterest. IntheTSEMtheba-
sicassumptionistheexistenceoftruevaluesforthetargetvariables(Grovesand
Lyberg, 2010). Likewise, for statistical learning models we shall assume the ex-
istenceofatruestatisticalmodel expressingtherandomgenerationmechanism
ofatargetvariableY fromauxiliaryvariablesX,sothatwithlittlelossofgener-
ality,wemaywriteY =f(X;Θ)+ϵ,whereΘrepresentanyparametrizationof
the functional dependence f. The model output will basically be an estimated
(cid:16) (cid:17)
function fˆwith a estimated set of parameters Θ(cid:98) so that Y(cid:98) = fˆ X;Θ(cid:98) . The
9choice of fˆ, and the resulting parameters Θ(cid:98) =Θ+ϵ Θ, will lead to errors in the
final predictions of the model. Given the model fˆ, it is in our opinion essential
to identify the error ϵ , since it will deliver a substantive contribution to the
Θ
total error of the estimation of Y.
In the TMLE-model, we try to identify all the sources of errors that will
occur during this process impinging on the quality of all predicted values y
(cid:98)k
for the units k ∈ U in our target population U and their inclusion in the
final statistics for the target population of interest. To accomplish this, the
model is defined in two phases. The first phase, the training phase, tries to
estimate the optimal set of parameters, Θ(cid:98), based on the selected training set
(see paragraph 3.1), whereas, during the application phase, the model is used
to find an estimation of the target variable Y(cid:98) (see figure 4 for an overview of
the total model; paragraph 3.3). But first, we will focus on the estimation of
the parameters Θ(cid:98) with its error term ϵ .
Θ(cid:98)
3.1 The training phase
The model also makes use of a measurement line and a representation line, as
in the TSEM. The basic scheme for the training phase is depicted in figure 2.
Figure 2: The Total Machine Learning Error model: training phase.
103.1.1 The measurement line
The measurement line represents the evolution of variable values from its con-
ception to its influence in the final model. It consists of two important steps
within the training of an ML algorithm.
Case The first block at the upper left corner references each instance in the
training set. The case still does not have any features, since these will be
describedinthenextblock(measurement). Acaseisbestdescribedasapointer
toaneventorobjectintherealworldthatwehavetomakeapredictionabout.
In the next step, we will observe and measure this object or event.
Measurement In the process of measuring, the first errors will occur. We
state here that these measurement errors are often neglected in data science,
and can therefore be an important source of errors. We often hear many data
scientists refer to the training set as the ground truth (true values in the orig-
inal TSEM’s terminology), and this is - by definition - wrong. In the case
of supervised learning, for example, a training set has features and a target
variable. Firstly,sincethefeatures(X)canhavemeasurementerrors,theycan-
not be equal to their true values. We can write with little loss of generality
x = x(0) +ϵ , where x(0) denotes the true values (ground truth), x denotes
x
the actually measured, distorted, values, and ϵ are the measurement error of
x
variables x.
Secondly,themodeltargetvariableY canalsohaveerrors. Wecanalsowrite
y =y(0)+ϵ , wherey(0) standsforthetruevalue(groundtruth), y denotesthe
y
actually measured, distorted, values and ϵ stands for the measurement error
y
of variable y. In a classification task, this leads to misclassifications (e.g. by
a human annotator), which can be expressed using, e.g., one-hot encoding by
y ̸=y(0), where y,y(0) ∈{0,1}×n:
 y(0)
1
y(0)
y(0) =  2 .  , (1)
 . . 
 
y(0)
n
where
(cid:40)
y(0) =1 if the case belongs to class i
i
y(0) =0 otherwise.
i
Let the transformation matrix T, where T describes the transformation
ij
probabilities from class i to class j. The error in classifications can now be
expressed in terms of the expected values of y and this transformation matrix:
E(y)=E(y(0)+ϵ )=T ·y(0) (2)
y
ThistransformationmatrixT is,infact,equaltoanormalizedconfusionmatrix.
11Model(measurementperspective) Basedontheprecedingdistorteddata-
set (x,y), we have to form the model. At the moment that we have measure-
ments (even with its measurement errors), we can start to train the model.
Anothersourceoferrorswecanidentifyisbestdescribedasthemodelassump-
tions. As is usual for models, the ML model is an abstraction of reality. It
will describe how certain variables are related to each other. This is based on
the way the ML model assumes how these variables (X,Y) are related and how
the features X are located in the feature space. Also, the choices made by the
data scientist about which features he/she selects and how they are coded or
transformed belong to the realm of model assumptions.
Hence, errors due to model assumptions are:
• errors due to the assumptions on how the features X are represented in
the feature space (e.g. encodings);
• errors due to the selection or actual availability of features X ̸= X(0),
where X(0) stands for the true features generating the target variable Y
and X denotes the actual set of features used in the modelling exercise;
• errors due to the assumptions in the functional dependence f, i.e. f ̸=f˜,
where f stands for the true functional dependence and f˜ denotes the
actually assumed dependence (to be estimated).
All these assumptions degrade the model, but one should not forget that
the idea of modelling is to generalize and the only way we can generalize is
by making assumptions. The famous quote from George Box is highly relevant
in this context. Box stated ”All models are wrong, but some are useful. The
question you need to ask is not ’Is the model true?’ (it never is) but ’Is the
model good enough for this particular application?’” (Box et al., 2009, p. 61).
In the end, it is all about the usefulness of the model, and a model with fewer
erroneous assumptions is expected to be better.
Afterthemodelisdefinedandafunctionaldependencefˆischosen,aparam-
eterspaceisdeterminedanderrors,duetotheoptimizationofmodelparameters
Θ, i.e. Θ(cid:98) ̸=Θ(0), where Θ(0) stands for the values for the parameters generat-
ing the smallest errors in target variable Y from the features X and Θ(cid:98) denotes
the estimated (by optimization search, usually) values of these parameters. In
many cases, a model is under-determined, leading to many equal optimal solu-
tions(Θ(cid:98) isinfactoneinstancefromasetofsolutions). Thismakesthesolution
not unique and could mean that, although the error is equal for different Θ(cid:98),
there are more stable points in the parameter space. A nice study about the
fractalbehaviorofneuralnetworkshasbeenprovidedbySohldicksteinandetal.
(2024a,b).
3.1.2 The representation line
The representation line deals with errors regarding the concepts of population,
frame, and sample (specific dataset for training, testing, and application). For
12MLmodels, theseconceptsaremuchsubtlerthanintheTSEM,wherethecon-
ceptions of set-theoretic population, frame, and sample are used (see section
2). ML models are statistical models and the focus is on the joint distribution
F(Y,X) of the target variable and the features, which are usually decomposed
as F(Y,X) = F (Y|X)F (X) so that the focus is actually placed on the con-
Θ
ditional dependence F (Y|X) (see e.g. Hastie et al., 2009). In consequence,
Θ
the conception of population, frame, and sample must be stated in terms of the
generating distribution function, i.e. as an infinite population in the traditional
jargon.
The dichotomy in these concepts lies much in line with the dichotomic dis-
tinctionbetweenenumerativeandanalyticsurveysbyDeming(1942a). Indeed,
inthisworkheintroducedtheconceptofquestionAandquestionBtypes,notic-
ing that each type has distinct implications for inference and decision-making.
The production of official statistics engages mainly in type A questions, which
involve actions based on existing data, intended to characterize and make deci-
sionsaboutknownpopulations(set-theoreticconception). Conversely, question
B types go beyond the limitations of current data, focusing on future measure-
ments of unknown entities (statistical conception). To understand the inherent
challenges of making inferences about unseen data, which is also an essential
aspect of ML, we must first understand this dichotomy.
This dichotomy is not even novel in the current practice. Model-assisted
estimation (see e.g. S¨arndal et al., 1992) makes an intelligent and profuse use
of the dichotomy to introduce linear regression models (question B type) in the
design-based inference paradigm (question A type).
Understandingthedynamicsofpopulationsisessentialtounderstandingthe
difference between these two types of questions. Deming’s 1941 paper, ”On the
InterpretationofCensusesasSamples”(DemingandStephan,1941),challenged
the conventional view of a census as a complete enumeration of a population
(set-theoretic definition). According to him, even a 100% census represents a
sample from a broader, infinite population. He argued that, if one needs to
make an inference on a very small cohort of the complete population, even a
100% sample could be too small to make that inference. Even though the sam-
pling error will be zero, the more general type B questions cannot be answered
based on this data. We need to consider the broader context when answering
type B questions, i.e. we need to focus on the generating distribution function
underlying the random phenomena behind the data generation.
In this line of thought, ML models and algorithms are generally designed
to assist in solving problems similar to Deming’s type B questions, aiming to
predict(oftenfuture)instancesofunobservedunitsbasedonobserveddata. As
opposed to type A questions, which focus on actions based on existing informa-
tion, type B questions involve anticipation and inference. Having understood
this distinction, one can better understand the essence of ML, where predictive
13models are trained not simply to describe past events (although they can be
used in this sense (see e.g. Barrag´an et al., 2024, in this same book)), but also
toextrapolatepatternsandrelationshipsforinformeddecisionsaboutdatathat
has not yet been seen. As we shall argue, the problem of concept drift, data
drift, or model drift originates here.
To illustrate our proposal let us focus on a concrete example where the di-
chotomy between set-theoretic and statistical concepts can be clearly observed.
Let us try to construct a model-assisted estimator like the GREG estimator
(see e.g. S¨arndal et al., 1992) but using a CART-type regression tree (see e.g.
Murphy,2013)insteadofalinearregressionmodel2. Basically,weneedtobuild
aregressiontreemodel(whichinvolvesbothtrainingandtesting)tobeapplied
to a concrete dataset to provide estimates for population totals of a continuous
target variable Y. To be more specific, we may think of Y as the turnover
target variable in a periodic business statistics and the features X as the set
of auxiliary variables available for the whole target population of interest (set-
theoretic), as in the GREG scenario.
In terms of Deming’s machine analogy, each box corresponds to actual data
from a given reference time period. The machine corresponds to the coun-
try’s economy producing these data for all periods. To build such a CART-
assistedestimatoramountstoinvestigatingthestatisticalfunctionaldependence
Y = f(X;Θ)+ϵ behind the data generation mechanism F (Y|X) to provide
Θ
estimates for, say, the population totals of the target population U at a given
referencetimeperiodt. Asusualintheproductionofofficialstatistics, wehave
a (set-theoretic) probabilistic sample s selected according to a sampling design
t
p(·) from the (set-theoretic) target frame U . For simplicity, we shall assume
Ft
U = U so there are no coverage errors affecting the target sample. We also
Ft t
consider there is no non-response. Errors from the measurement line are also
considered non-present so that we can focus on the representation errors in the
ML model. The CART-assisted estimator will be
Y(cid:98) dC tART = (cid:88) (cid:98)y (cid:98)k+ (cid:88) y k π−(cid:98)y (cid:98)k, (3)
k
k∈Udt k∈sd
where π
k
stands for the first-order inclusion probability of unit k and (cid:98)y
(cid:98)k
is the
design-based estimate of the CART-predicted value of variable Y for unit k.
The CART-predicted value y can be written as (see e.g. Murphy, 2013)
(cid:98)k
M
(cid:88)
y = w I (x ),
(cid:98)k m R(cid:98)m k
m=1
where {R(cid:98)m}M
m=1
denotes the binary disjointly split regions of the feature space
2Theargumentremainsvalidforalinearregressionmodel,butbyusingCARTswehope
tounderlinethedifferentconceptsinvolved,usuallyawayfromusualpractice.
14according to the estimated model and w = 1 (cid:80) y .
m nm k∈R(cid:98)m k
Now since we are taking a (set-theoretic) probabilistic sample s , as in the
t
GREG estimation, we need to provide the design-based estimator so that
(cid:98)y
(cid:98)k
=
m(cid:88)M =1(cid:32)(cid:80)
(cid:80)k k∈ ∈R(cid:98) R(cid:98)m
my
1k
// ππ kk(cid:33)
I R(cid:98)m(x k).
We shall use this example to introduce the concepts of training and target
populations, training and target frames, and training, test, and target samples.
Training Population Let us denote by Ftr(Y|X) the data generation dis-
Θ
tribution function for those statistical units Utr used for training the model.
This infinite population Ftr(Y|X) will be, in our TMLE-model, the training
Θ
population. In our analogy using Deming’s machine, the training population is
a machine generating data later to be used just for training.
To assess its effect on the final statistics to be produced, we need to investi-
gateitsrelationshipwiththefinaltargetpopulationU ofinterest. Thedifficulty
t
arises because we need to compare this (set-theoretic) target population U of
t
interestwiththe(statistical)conceptoftrainingpopulationFtr(Y|X). Wecan
Θ
consider U as the realization of the underlying data generation distribution
t
function F (Y|X) for the target population so that the finite population U
Θ t
can indeed be conceived as a sample of this infinite target population (i.e. the
superpopulation approach (see e.g. Cassel et al., 1977)).
Inthisline,themoredifferentFtr(Y|X)isfromF (Y|X),thelessaccurate
Θ Θ
the final statistics will be. Although representativity as a comparison between
two sets for estimating purposes constitutes a slippery concept (Kruskal and
Mosteller,1979a,b,c,1980),amathematicaldefinitionintheset-theoreticrealm
(Bethlehem, 2009) can be provided in terms of the difference between (empiri-
cal)distributionfunctionsforagivenvariable. Inthissame(tobemadeprecise)
we can talk of the representativity of the training population with respect to
the target population in the context of an ML model in terms of the distance
betweenFtr(Y|X)andF (Y|X). Thedifferencesarisebecauseofthechanging
Θ Θ
dynamics of populations. For example, if data from the past are used, a judge-
ment is implicitly made about the stability in time in the relationship between
Y and X for the target population at stake.
Training Frame OncethedatageneratingdistributionfunctionFtr(Y|X)is
Θ
in place, datamustbe actuallygenerated (Deming’s machine must produce the
bolts) so that we have a (set-theoretic) frame population Utr from which sta-
F
tistical units (instances) will be taken to train the model. This data-generating
mechanismmaybeaffectedbydifferentfactorsproducingover-coverage,under-
coverage,imbalance,etc. ErrorscanarisewhentherelationshipbetweenY and
X is not extensively and properly covered throughout all generated instances.
15In mathematical terms, this means that it is impossible to obtain an accurate
estimationofthetrainingdatagenerationdistributionfunctionFtr(Y|X)from
Θ
the frame population Utr.
F
Target Sample The actual dataset used for training is composed of a se-
lection of instances from Utr. This selection may have been executed in many
F
differentways,eitherproducinganon-probabilityor(ideally)aprobabilitysam-
ple s .
tr
Notice that even selecting a representative sample s with respect to the
tr
target frame Utr, i.e. even having a small distance between the (empirical) dis-
F
tribution functions of the target variable in Utr and s this does not guarantee
F tr
the final quality of the model if yet Ftr(Y|X) is very different to F (Y|X).
Θ Θ
However, to meet this final requirement, this notion of (set-theoretic) represen-
tativity of s with respect to Utr is necessary.
tr F
Model(representationperspective) Inourprevioussections,wediscussed
boththecoverageerrorandthesamplingerrorforthetrainingphase. Tocreate
the optimal training set from the representation perspective to build the model
we need to ensure that Ftr(Y|X) is very close to F (Y|X). The concept of
Θ Θ
representativity, as we have seen, is indeed slippery and involves diverse sub-
tleties.
Asaresultofthisprocedure,webelievethatproblemslikeconceptdriftwill
be minimized to the best of our ability, as the final model will be robust to the
dynamics occurring in the finite population under consideration.
3.2 The testing phase
For the testing phase, the model also makes use of a measurement line and a
representation line, as in the training phase. The basic scheme for the combi-
nation of the training and testing phases is depicted in figure 3.
A cautious reader may think that an infinite population Ftest(Y|X) may be
Θ
needed for the testing phase so that both the measurement and representation
lines can be defined in a similar fashion. However, this is in contrast to usual
(good)practice. Theavailabledataaredividedfortrainingandtesting, sothat
indeedweareensuringFtest(Y|X)=Ftr(Y|X)(unlessthisdivisionisexecuted
Θ Θ
in a highly non-ignorable way). This has a direct consequence for the scope
of the validity of the tested models. Validity can be described as internal or
external Onwuegbuzie (2000). The internal validity refers to the fact that the
modellearnedtherelationshipsthatwewerepresentinthetrainingset,whereas
the external validity refers to a validity beyond that point: the model is able to
predict outside the context of the given population. This kind of validity is not
reachable at the moment that Ftest(Y|X)=Ftr(Y|X).
Θ Θ
16Figure3: TheTotalMachineLearningErrormodel: trainingandtestingphases.
Thetrainingphaseishighlightedinred;thetestingphaseishighlightedinblue.
17It also has immediate consequences for both the measurement and the rep-
resentation lines in the testing phase, as well as consequences for the scope of
the validity of the models tested in this fashion.
3.2.1 The measurement line
The measurement line in the testing phase still represents the evolution of vari-
able values since its generation to its influence in the final model, but now
focused on those units used in the testing phase.
Measurement Whenmeasuringvariablesforthetestingphase,thesamesit-
uation occurs as in the training phase: measurement of variables takes place
witherrors. Measuredvaluesarenotingeneralequaltothecorrespondingtrue
values.
Internalvalidityisindeedensuredwhenbothtrainingandtestingdatacome
from the same training target frame, as stated out above. This may not be the
case, for example, when training data are taken from preceding time periods
and testing data are used for the next period in the time series. The splitting
into training and testing should be executed with extreme care to ensure that
both underlying infinite populations are indeed the same!
Target variable Once the model has been trained and tested, even with
measurement errors, we can produce the predicted values y since the functional
(cid:98)
dependence f(cid:98)has been estimated as well as the parameters Θ(cid:98).
Metrics (measurement perspective) Once predicted values can be pro-
duced, then model performance evaluation can be undertaken with the corre-
sponding metrics. The basis for these kinds of validation measures lies in the
well-known confusion matrix. It is important to underline that the model qual-
ity in its final application to the target population of interest will depend on
thechoiceofmetrics. Thebestreferencetoanoverviewoftheconfusionmatrix
and all its derived metrics can be found on Wikipedia (Wikipedia contributors,
2024).
3.2.2 The representation line
Once the training/testing splitting is executed ensuring that both underlying
infinite populations are the same, we can assume that the testing population
andtestingframearethesameasinthetrainingphasesothatonlythesamples
will indeed be different: this is the unique novel element.
Test sample Theactualdatasetusedfortestingiscomposedofaselectionof
instances from Utr according to the mentioned training/testing splitting strat-
F
egy. This selection may have been also executed in many different ways, either
18producing a non-probability or an (ideally) probability sample s .
test
Under these assumptions, representativity properties of s and s are
tr test
shared, thus sampling errors are equally present in the testing phase, although
it is common that the test set is much smaller than the training set. Particular
training/testing splitting strategies may introduce differences between s and
tr
s . Cross-validation,out-of-bagprocedures,andsimilartechniquesarehighly
test
convenient and adequate in this sense.
Metrics (representation perspective) Model performance indicators and
metrics are then computed for s . Notice that, when performance indica-
test
tors and metrics on the test set are considered adequate, the whole model is
retrained in the whole training/testing dataset, thus for the same underlying
infinite population.
3.3 The application phase
Ultimately, one of the goals of creating an ML model is to use it on a set of
new data. This is shown in the TMLE-model in figure 4. The training phase
(implicitly including the testing phase) is highlighted in red, whereas the pre-
diction/application phase is highlighted in blue.
The central aspect of error assessment is that the model now includes two
infinite populations, namely the training population Ftr(Y|X) and the infinite
Θ
target population F (Y|X) and the realised finite target population U, which
Θ
is the population for statistical analysis.
At this point, the original TSEM (figure 1) should be taken into account
both for the variables y (measurement line) and for the units k ∈U (represen-
tation line). As a final step, following equation (3), we will make an estimate
basedoncollectedtargetvariablevaluesy andthepredictionsy thathavebeen
(cid:98)
made by the model.
Itisimportanttonotethatwearenowdiscussingtheexternalvalidityofthe
model on the measurement side of the model. This is because we are applying
themodeltoapopulationthatiscompletelydifferentfromtheoneonwhichwe
trainedandtestedit. Ifthemodeldoesnotperformaswellasitdidonthetest
set, it is not externally valid. As a form of validity, external validity is stronger
than internal validity, and when applying ML algorithms, we should strive for
anasgoodaspossibleexternalvalidity. Inouropinion,thisisalsoabetterway
to test the model. Instead of taking a sample of the same population as the
training set, we should take a sample from a different (part of the) population.
This would provide us with a much better assessment of the model.
19Figure 4: The Total Machine Learning Error model: training/testing and ap-
plication phases. The training/testing phases are highlighted in red; the appli-
cation/prediction phase is highlighted in blue.
204 Summary of the model
Having a comprehensive understanding of and meticulous rectification of inher-
ent measurement errors prevalent in a variety of tasks is a critical aspect of
MLendeavors. Inadditiontoinaccuraciesinfeaturemeasurement, theseerrors
can also be attributed to the complexities associated with misclassifications of
targetvariables, whichcansignificantlyimpairMLaccuracyandreliability. By
meticulously acknowledging and adeptly mitigating these multifaceted errors,
practitionerscansubstantiallyenhancetherobustnessandeffectivenessoftheir
models, resultingingreatertrustandutilityintheirapplicationsacrossvarious
domains and scenarios.
A pivotal juncture occurs in the course of the Representation Dimension, as
the focus shifts toward the intricacies of creating training populations, frames,
and samples. Deming’s seminal contributions to population analysis are under-
pinning this transition, whose distinction between type A and type B questions
provides invaluable insight into population analysis’s nuance. It is evident from
Deming’sdistinctionthatitisparamountthattrainingdataaccuratelycapture
the subtle nuances and complexities inherent in the infinite population being
studied. In addition, a growing emphasis is being placed on cultivating repre-
sentativetrainingframesthatreflectthetrueessenceofthebroaderpopulation
landscapeasaresoundingcalltoactionamongdiscussionsregardingfinitepop-
ulations and their broad-ranging implications for model generalization.
Our focus is increasingly on rigorously evaluating model performance by
applying a dedicated test dataset as we proceed through the testing phase. As
practitionersnavigatethelabyrinthinecomplexitiesofmeasurementerrors,they
face a multitude of challenges, ranging from internal validity concerns to nav-
igating the labyrinthine nature of internal validity concerns. In light of these
challenges, robust evaluation methodologies that go beyond mere performance
metrics,examiningtheintricatenuancesofmodelbehaviorandefficiencyacross
a variety of contexts and scenarios are imperative. Practitioners can increase
their confidence and reliability in their ML endeavors by embracing the imper-
ative of assessing external validity and ensuring that models can be applied to
entirely new datasets.
In applying a model to previously unobserved data, we are presented with
the ultimate frontier of ML deployment, wherein real-world applications and
insights are derived from unobserved data, resulting in the fruits of labor. It-
erative learning culminates in this phase, when models move from theory to
practical application, enabling them to impact diverse domains and industries
tangibly. Thereisastrongemphasisonexternalvalidityinthiscontext, under-
lining the importance of models demonstrating robustness and generalizability
across a variety of scenarios and contexts. In contrast to the confines of finite
populations, Deming’s principles promote models that transcend the confines
of finite populations and embrace the dynamic complexity of the broader land-
21scape by drawing parallels with the TSEM. For ML to reach greater heights of
innovationandimpact,practitionersmustmaintainacommitmenttoexcellence
andasteadfastcommitmenttoadvancingthefrontiersofknowledgetonavigate
the rapidly evolving landscape with confidence and efficacy.
Based on these considerations, we would like to propose the following best
practices:
• Understand Measurement Errors: Thoroughly investigate and address
measurementerrorsinfeaturevariablesandtargetvariablesduringmodel
training.
• ConstructRepresentativeTrainingFrames: Strivetocreatetrainingframes
that accurately represent the characteristics and complexities of the infi-
nite population, considering diversity and coverage issues.
• Evaluate Model Performance: Use test sets to evaluate model perfor-
mance, focusing on the external validity. Having a test set that is not
part of the training population helps in determining the external validity
• Continuously Monitor and Refine Models: keep checking the validity of
thetrainingpopulationthatwasusedtocreatethemodel. Makesurethat
the target population is still a sample of the assumed infinite population.
Iterate, when necessary on model development, incorporating feedback
from evaluations on test sets and real-world applications to improve per-
formance and robustness.
In the context of ML pipelines, we would like to emphasize the importance
of integrating these insights into every stage of the development lifecycle. From
data collection and preprocessing to model training and evaluation, incorporat-
ing robust mechanisms for addressing measurement errors and ensuring repre-
sentativenessintrainingdataisparamount. Thisentailsimplementingrigorous
validation protocols, leveraging diverse datasets to capture the full spectrum
of population characteristics, and continually refining models to enhance their
generalizability and reliability.
Moreover,fosteringacultureoftransparencyandaccountabilityisessential,
wherein practitioners actively document and communicate the limitations and
assumptionsunderlyingtheirmodels. Bypromotingopendialogueandcollabo-
ration, we can collectively identify and address potential biases and distortions,
thereby fostering greater trust and confidence in ML applications.
Furthermore,investinginongoingresearchanddevelopmenteffortsaimedat
elucidating the intricate dynamics of population analysis and model evaluation
is crucial. This entails exploring novel methodologies for assessing external va-
lidity,refiningsamplingstrategiestominimizebiasandvariance,andadvancing
techniques for quantifying and mitigating measurement errors.
22Ultimately, by integrating these best practices into ML pipelines, we can
pave the way for the development of more robust, reliable, and ethical models
that not only excel in performance but also uphold the highest standards of in-
tegrity and accountability. Through a steadfast commitment to excellence and
a relentless pursuit of innovation, we can harness the transformative potential
of ML to address some of the most pressing challenges facing society today,
driving progress and prosperity for all.
5 Applying Machine Learning models: some clas-
sification examples
As described above, creating a good externally valid ML model is essential
when applying this kind of algorithm in the context of official statistics. Here,
we will first describe the approach followed in the study performed to detect
innovativecompanies,followedbythedetectionofonlineplatforms. Intheend,
some recent insights gained during a detailed study of the creative industry are
discussed. All studies use website text to identify different types of companies
intheNetherlands. Thesestudieswereperformedbysomeofthecoauthorsand
nicely illustrate the insights gained during our study of ML-methodology.
5.1 Detecting Innovative Companies
Producing an overview of innovative companies in a country is a challenging
task. Traditionally, this is done by sending a questionnaire to a sample of com-
panies. This approach, however, focuses on large companies and completely
misses small companies, such as startups. Therefore, an alternative approach
wasinvestigatedbydeterminingifacompanyisinnovativebystudyingthetext
on its website. An ML model was developed based on the texts of the websites
ofcompaniesincludedintheCommunityInnovationSurveyoftheNetherlands.
The latter is a survey carried out every two years that focuses on the detection
of innovative companies with 10 or more working persons. All websites of the
innovative companies were included (a total of 3340) in addition to a similar
sized-randomsampleofthenon-innovativecompanies(3302)(Daasandvander
Doef, 2020). This provided the training frame which, according to the popula-
tion topics discussed above, is very likely representative. It was found that the
MLmodeldevelopedwasabletoreproducetheresultsfromtheCommunityIn-
novationSurvey, withamaximumaccuracyof93%, andwasalsoabletodetect
innovative companies with less than 10 employees, such as startups Daas and
vanderDoef(2020). Manualcheckingwasperformedtodeterminetheaccuracy
of the model on the classification of small companies and confirmed its external
validity (regarding this subpopulation).
In a separate study, focused on misclassification errors, the model was ap-
pliedtoaverylarge,morerepresentative,datasetofwebsites;ofaround400.000.
23When a new model was trained on a random sample of 20.000 websites in the
newly classified dataset, it became clear that even though the new model was
trainedonamorerepresentativepartofthe(finite)population, theaccuracyof
the model had a decreased accuracy of 88%. This demonstrated that misclassi-
fication errors build up during model development.
Thedownsideoftheoriginalmodelwasitsstabilityovertime. Here,afairly
rapiddeclinewasobservedonthesamesetofwebsitesscrapedatvariouspoints
in time Daas and Jansen (2020). This is referred to as concept drift. Studies
to reduce this issue were performed and found to be a challenging topic. Appli-
cation of Large Language-based models provided the most successful ’solution’
thus far with an accuracy of 86-87% for the various data sets over time. How-
ever,theexternalvalidityofthatmodel,onnewunseendata,wasnotthathigh.
It had an accuracy of 72%.
5.2 Detecting Online Platforms
Obtainingreliableinformationfromasmallorraresubpopulationisachalleng-
ing topic. Approaches commonly used to find rare or so-called hard-to-identify
groups are a screening survey, network sampling, area sampling, or a combi-
nation. An example of a rare subpopulation is online platform businesses. To
get a complete overview of this subpopulation, an ML model was developed to
identify these kind of platforms. This required a training frame. Hence, busi-
ness statistics experts of Statistics Netherlands were asked to manually provide
examples of such websites. This resulted in a set of 569 online platforms and
303 non-platform organizations, with very similar characteristics, that were ad-
ditionally identified during this process. To the latter, a random sample of 266
non-platform organizations, from the websites linked to the Business Register,
were additionally added.
Thecombinedframecontained50%platformand50%non-platformwebsites
used for model development. This resulted in an ML model that was able to
identify online platforms with an accuracy of 82% on the test set (Daas et al.,
2024). After applying the model to the entire population of websites linked
to the business register and manual inspection of random samples, it became
clearthatthemodelseriouslyoverestimatedthenumberofonlineplatforms. In
other words, the external validity of the model was low and its findings were
biased. By sending questionnaires to a large sample of businesses this problem
was initially solved and the answers obtained were used to validate the ML
model-based findings Daas et al. (2024).
The initial study revealed that online platforms merely compose 0.22% of
thetotalpopulationofbusinesseswithawebsiteintheNetherlands. Thismade
clear why false positives were such a huge problem. Subsequently, various steps
were studied with the aim to considerably reduce the number of false positives
detected by the ML model (Gubbels et al., 2024). The combination of steps
24Table1: Effectofvariousmodel-basedapproachesononlineplatformdetection.
Type of model True Pos. Est. Pos. Bias Accuracy
1 Log. reg. 69 2991 0.098 0.901
2 Log. reg. prob. 69 7657 0.255 0.901
3 Log. reg. cal. prob. 69 637 0.019 0.985
4 Ensemble cal. prob. 69 306 0.007 0.993
that worked well are listed in Table 1.
Apart from producing binary labels, the model used could also produce the
probability of a website being an online platform. These probabilities have a
value between 0 and 1. Using the ‘probabilities’ of the model was found to in-
creasetheonlineplatformestimates;hence,increasingthebias(Table1,row2).
This makes clear that, under these circumstances, the model did not produce
actual probabilities. This did not affect the accuracy.
Subsequently, a method was applied to correctly calibrate the probabilities
of the model used. This method corrects for the intrinsic prevalence of the
model; i.e. the prevalence caused by the ratio of positives and negatives on
which the ML model was originally trained (Puts and Daas, 2021). Since the
number of online platforms is very low in the population and the model was
trained on a much higher ratio of positive and negative cases (either 50-50% or
30-70%), it can be expected that correcting for this prevalence may seriously
reducethenumberofpositivecasesestimated. Applyingthecalibrationmethod
revealed that this was indeed the case; see Table 1, row 3. As a consequence,
the accuracy considerably increased.
When the results of multiple trained and calibrated models, up to 10, were
combined, the bias was reduced even further; see Table 1, row 4. The accuracy
also increased somewhat. The bias is, however, not completely removed by the
combination of correction methods applied. This is not unexpected for a model
detecting rare events. We think there are two ways to even further improve
this approach. The first is by increasing the number of models included in the
ensemble. The second is by improving the representativeness of the websites
included in the training frame used and those in the dataset used to produce
thefindingsshowninTable1. Thisobviouslyrelatestothediscussiononinfinite
and finite populations mentioned above.
5.3 Detecting the Creative Industry
Next, we discuss an ML study on the detection of businesses belonging to the
creative industry in the Dutch municipality of Eindhoven. Since the creative
industry is very difficult to define, it was interesting to study the topic with a
data-driven approach. The big question in this study was - initially - if such
25businesses could be identified with an ML model trained in the texts on the
websites of positive and negative cases. This required examples and, hence,
local experts were asked to provide a list of websites of businesses belonging to
the creative industry.
At the start of the study, a list of 110 positive websites was provided. Con-
sideringthenumberofpositivecasesincludedinthetrainingframeinthecases
discussed above, this is a very low amount. However, assuming that ’a website
ofabusinessbelongingtothecreativeindustry’isarareevent,theideaemerged
that a (small) random sample of the websites linked to the business register,
excluding the websites already in the positive set, could provide a nearly per-
fect list of non-creative industry examples. Such an approach is referred to as
Positive and Unknown (PU)learning (Bekker and Davis, 2020) and might pro-
vide a solution. Subsequently, random samples were drawn from the websites
of the business register (of various sizes), combined with the 110 positive cases
and various models were trained. Be aware that the selection procedure used
assured that no websites already included in the positive set were selected from
the business register linked list of websites. After some trial and error, an ML
modelwasfoundthatseemedtoproducefairlyaccurateresults, onthetestset,
with an accuracy of 86%.
Applying the model to the population, resulted in a probability distribu-
tion that was composed essentially of two clearly distinct peaks: a large one
of non-creative websites (with an average of around 0.05) and a smaller group
of potential creative websites (with an average probability of 0.99). However,
manual inspection of a sample of 370 relatively high-scoring websites revealed
thatonly52%ofthosewebsitesactuallybelongedtothecreativeindustry. The
external validity of the model was obviously poor.
Since random samples were drawn for manual inspections, the idea emerged
to add the findings for the websites to the training frame. Subsequently, in the
next iteration, the combination of 110 positive and the 370 manually classified
cases (including both negative and positive cases) was combined with various
amountsofrandomlysampledwebsitesfromthewebsiteslinkedtothebusiness
register. Here again, the selection procedure used ensured that no websites al-
readyincludedinthepositiveandmanuallyclassifiedsetwereselectedfromthe
business register list of websites. This procedure resulted in a model with an
accuracy of 85% on the test set.
Applying this second model to the population revealed a probability distri-
bution composed of three distinct groups: a very large group with an average
probability of 0), a small group with an average probability of 0.1) and a small
group with an average probability of 1). Manual inspecting random samples
fromeachgrouprevealedthateachgroupcontained,respectively,2%,40%,and
87% websites belonging to the creative industry. Hence, the second model was
much better able to discern creative industry websites compared to the first
26model. This leads to the conclusion that including the 370 manually inspected
websites in the training frame, the 100 positive cases, and a random sample
of unknown cases, made the resulting frame much more representative of the
populationstudied. Wethinkthatfollowingsuchaniterativeapproachisavery
interestingwaytocreatehigh-quality(andmorerepresentative)trainingframes
withafairlylowmanualeffort. ItremindsusoftheDemingCycleofcontinuous
improvement. Becausethisstudywasperformedduringthedevelopmentofthe
TMLE model described above, this also indicated that having such a frame in
mind while performing an ML based study also helps to improve the quality of
its findings.
6 Discussion
Fromtheabove,itisclearthatthemethodologyofapplyingMLinofficialstatis-
tics is just in its infancy. Compared to the questions posed in Puts and Daas
(2021) this document already sheds some light on the methodology concerning
thehumanannotationofdata,samplingthepopulationtoobtainrepresentative
training sets, dealing with concept drift, and correcting the bias caused by the
ML model. However, there are several additional and important considerations
- from a methodological perspective - that became apparent while writing this
document. These are at various stages of development and are described in
the paragraphs below. Some of them are quite fundamental and all should be
thoroughly investigated.
(1) The terminology used by statisticians and data scientists dif-
fers. Historically, emerging fields use their own terminology. This is usually
not problematic, since this terminology is typically only used within the newly
defined paradigm. However, if the emerging field is adopted in another field,
with its own paradigm, it will result in a loss of common ground. This does
not necessarily have to result in a total misunderstanding between the fields.
Thedivergingterminologywithindifferentfieldshasastrikingsimilaritytothe
term ”false friends”; which are words written exactly the same in two differ-
ent languages but with a divergent meaning. Even though multilingual families
have to deal with ’false friends’ on a daily basis, they can function in peace and
harmony without many misunderstandings. Why would this be a problem in
science? The reason is evident: the older field (in this case statistics) will con-
sider the terms used in the younger field (in this case ML) as incorrect. To deal
with ’false friends’, however, we should acknowledge their different meanings
and keep them under consideration when communicating (like in bilingual fam-
ilies). Partofthepurposeofthischapterwastoacknowledgethatmethodology
in statistics means something different compared to ML, and subsequently de-
scribethefieldofMLintheterminologyusedwithinthefieldofofficialstatistics.
(2) Ensure homogeneity in the construct measured. Developing an
MLmodelstartswithatrainingframeincludingcasesofthebestpossiblequal-
27ity. To enable this, there usually is a ”human in the loop”, for instance, to
ensure that the cases included are correctly classified orto check the findingsof
the model on new data. Including human checking is challenging because the
findings of multiple humans need to be consistent, the so-called inter-annotator
agreement, which requires considerable effort. Efficient ways to verify the con-
struct measured by ML models need to be developed.
(3)RepresentativityinthecontextofML.Thisfundamentalandchal-
lenging question touches the heart of statistical inference. There is a definitive
needtodevelopmoretheoryinthisareatoensurethatMLmodels,asaccurately
as possible, measure the concept of interest on new cases. Which steps should
be taken, in which order, to enable this? In many of the questions included in
this list, the notion of representativity in the context of ML is paramount and
should therefore be resolved.
(4) How to deal with ML and type A and type B questions? Dem-
ing distinguishes these types of questions. Type A questions are action-based
on existing information and more in line with the production of official statis-
tics, while type B questions aim to predict instances of unobserved units and
go beyond the limitations of current data. Here, one could simply decide to
just focus on ML and type A questions, but one needs to be aware that ML
models developed for answering type B questions are expected (when properly
developed) to better deal with new data and changing conditions. Both types
are highly relevant and ML models that can deal with both should be studied.
(5) Bias(es) resulting from errors made during training of the
model (especially for type B questions). Many of the errors occurring
during model development will result in biased estimates. Reducing the errors
asmuchaspossibleduringmodeldevelopmentisawaytodecreasetheirdisrup-
tiveeffect. TheTMLE-modelisagoodstartingpointhereandthefocusshould
now shift to measure each of the errors identified. Of course, one should bear
in mind that this is not as evident as it seems. For instance, errors introduced
by model assumptions are almost impossible to quantify.
(6) Representativity problems resulting from differences in the
population composition of the training frame and the infinite popula-
tion. We saw that machine learning involves answering a type B question (see
previous point). Thus, the training set should not be representative with re-
specttothefinitepopulation,butratherwithrespecttotheinfinitepopulation.
Procedures are needed to ensure that the cases in the training frame include
therelevantfeaturesoccurringintheinfinitepopulationaswellaspossible. It’s
obviousthatrandomsamplingwillbeanimportantstephere. However,whatis
the best approach to obtain such a training frame? certainly for topics focused
on rare events.
(7) How to approximate the infinite population? A possible approx-
28imation of the infinite population can be done by taking one simple random
sample: the finite population. We will, however, introduce parts in the feature
space that are not well represented in this final population. The ill-covered
finite population (with respect to the infinite population) can be seen as a cov-
erage error, but also as a sampling error. Acknowledging this error is already
an important step. However, the question remains how we could approximate
the infinite population? To answer this question, we need to have a better
understanding of representativity in the context of machine learning. Deming
suggestedapproximatingtheinfinitepopulationbytakingseveralfinitepopula-
tions, separated in time to ensure independence between the ’samples’, but this
approach is not always feasible. Consequently, the question remains and needs
to be answered.
(8) Should we sample with replacement or not? When reading pa-
pers of Deming on the subject of infinite populations, it becomes clear that,
in order to minimize the errors on the estimators (in our case: the parameters
of the model), we need to sample with replacement. This is not an approach
commonly applied in ML. Here, the training set is sampled from the training
frame without replacement in order to minimize the overlap between elements
in the feature space. Its clear that there is a problem here. Future work is
needed to better understand this fundamental question.
(9) Develop a procedure that ensures that only the most impor-
tantfeatures(variables)areincludedinthemodel. Whenlargertraining
framesarebeingusedinMLmodeldevelopment,wehaveobservedthatincreas-
ingnumbersoffeaturesbecomeincludedinmodels. Thereisadefiniteneedfor
an approach that reduces this effect and ensures that only the ’best’ features
are selected. Such a procedure could also improve the accuracy and stability of
the model over time.
(10) Develop a procedure that ensures that the ML model is both
internally and externally valid and as stable as possible over time.
From the above, it has become obvious that the ultimate goal when developing
an ML model is creating a model with a high external validity. This is linked
to topic (6). There is a definitive need for a procedure that ensures a model is
obtained with both a high internal and external validity. The findings of topic
(6) will certainly help here.
These topics go beyond the focus of many traditional ML practitioners and
highlight the importance of a statistical view on ML and the need for ML
methodology. WeareconvincedthattheTMLEmodel,inparticularthesources
or errors identified, will help users to get a better grip on the challenging appli-
cation of ML in a statistical context but also when applying ML in general. In
addition, the document gives an overview of the topics that need to be studied
in more detail and, as such, sets the stage for future research in this interesting
and challenging area.
29Acknowledgements
First of all, we would like to extend our heartfelt thanks to Yvonne Gootzen
for her invaluable contributions to this work. We are deeply grateful for her
involvement.
WealsowishtoacknowledgeLuukGubbelsandSannePeereboom,whosein-
ternshipworkprovidedvaluableinsightsthatcontributedtosection5. Delorian
Canlon, and Sourav Bhattacharjee are gratefully acknowledged for stimulating
discussions that considerably improved the document.
References
Alpaydin, E. (2020). Introduction to Machine Learning. MIT press.
Barrag´an, S., C. S´aez, D. Salgado, and L. Sanguiao (2024). Streamlining
Business Functions in Official Statistical Production with Machine Learning.
Chapter in this book.
Bekker, J. and J. Davis (2020). Learning from positive and unlabeled data: a
survey. Journal of Machine Learning 109, 719–760.
Bethlehem, J. (2009). Applied Survey Methods: A Statistical Perspective. New
York: Wiley.
Box, G., A. Lucen˜o, and M. Paniagua-Quin˜ones (2009). Statistical Control By
Monitoring and Adjustment. John Wiley & Sons.
Breiman, L. (2001). Statistical modeling: the two cultures. Statistical Sci-
ence 16(3), 199–215.
Casella, G. and R. Berger (2002). Statistical Inference. Duxbury Press.
Cassel, C.-M., C.-E.S¨arndal, andJ.Wretman(1977). Foundations of Inference
in Survey Sampling. New York: Wiley.
Chambers, R. and R. Clark (2012). An introduction to model-based survey
sampling with applications. Oxford University Press.
Daas, P., W. Hassink, and B. Klijs (2024). On the validity of using webpage
texts to identify the target population of a survey: An application to detect
online platforms. Journal of Official Statistics 40(1), in press.
Daas, P. and J. Jansen (2020). Model degradation in web derived text-based
models. In Paper for the 3rd International Conference on Advanced Research
Methods and Analytics (CARMA), pp. 77–84.
30Daas, P. and S. van der Doef (2020). Detecting innovative companies via their
website. Statistical Journal of IAOS 36(4), 1239–1251.
DeJong,T.,S.Bromuri,X.Chang,M.Debusschere,N.Rosenski,C.Schartner,
K. Strauch, M. Boehmer, and L. Curier (2020, 09). Monitoring spatial sus-
tainable development: semi-automated analysis of satellite and aerial images
forenergytransitionandsustainabilityindicators.Technicalreport,Eurostat.
Deming,W.(1942a). Onaclassificationoftheproblemsofstatisticalinference.
Journal of the American Statistical Association 37(218), 173–185.
Deming,W.(1942b). Onaclassificationoftheproblemsofstatisticalinference.
Journal of the American Statistical Association 37(218), 173–185.
Deming, W. (1950). Some theory of sampling. New York: Wiley.
Deming, W. (1953). On the distinction between enumerative and analytic sur-
veys. Journal of the American Statistical Association 48(262), 244–255.
Deming, W. and F. F. Stephan (1941). On the interpretation of censuses as
samples. Journal of the American Statistical Association 36(213), 45–49.
Dumpert, F. (2023). Machine learning in german official statistics. In G. Snijk-
ers,M.Bavdaˇz,S.Bender,J.Jones,S.MacFeely,J.Sakshaug,K.Thompson,
andA.vanDelden(Eds.),AdvancesinBusinessStatistics, MethodsandData
Collection, Chapter 23, pp. 537–560. Wiley.
European Statistical System Committee (2022). Quality Assurance Framework
of the European Statistical System, version 2.0. https://ec.europa.eu/
eurostat/documents/64157/4392716/ESS-QAF-V1-2final.pdf.
Gootzen, Y., P. Daas, and A. van Delden (2023). Quality framework for com-
bining survey, administrative and big data for official statistics. Statistical
Journal of the IAOS 392, 439–446.
Groves, R. M. and L. Lyberg (2010). Total survey error: Past, present, and
future. Public Opinion Quarterly 74(5), 849–879.
Gubbels, L., M. Puts, and P. Daas (2024). Bias correction in machine learning-
basedclassificationofrareevents. InSymposium on Data Science and Statis-
tics.
Hastie, T., R. Tibshirani, and J. Friedman (2009). The Elements of Statistical
Learning (Second ed.). New York: Springer.
Karr, A. F., A. P. Sanil, and D. L. Banks (2006). Data quality: A statistical
perspective. Statistical Methodology 3(2), 137–173.
Koop, J. (1974). Notes for a unified theory of estimation for sample surveys
taking into account response errors. Metrika 21, 19–39.
31Kruskal,W.andF.Mosteller(1979a).Representativesampling,i: Non-scientific
literature. Int. Stat. Rev. 47, 13–24.
Kruskal, W. and F. Mosteller (1979b). Representative sampling, ii: scientific
literature, excluding statistics. International Statistical Review 47, 111–127.
Kruskal, W.andF.Mosteller(1979c). Representativesampling, iii: thecurrent
statistical literature. International Statistical Review 47, 245–265.
Kruskal, W. and F. Mosteller (1980). Representative sampling, iv: The history
of the concept in statistics, 1895-1939. International Statistical Review 48,
169–195.
Little, R. (2012). Calibrated Bayes, an Alternative Inferential Paradigm for
Official Statistics. Journal of Official Statistics 28, 309–334.
Measure, A. (2023). Six years of machine learning in the bureau of labor statis-
tics. InG.Snijkers,M.Bavdaˇz,S.Bender,J.Jones,S.MacFeely,J.Sakshaug,
K. Thompson, and A. van Delden (Eds.), Advances in Business Statistics,
Methods and Data Collection, Chapter 24, pp. 561–572. Wiley.
Moscardi,C.andB.Schultz(2023). Usingmachinelearningtoclassifyproducts
forthecommodityflowsurvey.InG.Snijkers,M.Bavdaˇz,S.Bender,J.Jones,
S.MacFeely,J.Sakshaug,K.Thompson,andA.vanDelden(Eds.),Advances
inBusinessStatistics,MethodsandDataCollection,Chapter25,pp.573–591.
Wiley.
Murphy, K. (2013). Machine learning: a probabilistic perspective. MIT Press.
O’Neil, C. (2016). Weapons of Math Destruction: How Big Data Increases
Inequality and Threatens Democracy. USA: Crown Publishing Group.
Onwuegbuzie,A.J.(2000,11). Expandingtheframeworkofinternalandexter-
nalvalidityinquantitativeresearch. InAnnualMeetingoftheAssociationfor
the Advancement of Educational Research (AAER), Number 120 in 1, Ponte
Vedra, FL, pp. 62. ERIC. Paper presented at the Annual Meeting of the
Association for the Advancement of Educational Research (AAER).
Puts, M. and P. Daas (2021, July). Machine learning from the perspective of
official statistic. The Survey Statistician 84, 12–17.
Puts,M.J.,A.daSilva,L.D.Consiglio,I.Choi,D.Salgado,C.Clarke,S.Jones,
and A. Baily (2022). ONS-UNECE Machine Learning Group 2022. Quality
of Training Data. Theme Group Report. Technical report, UNECE.
Russell, S. and P. Norvig (2010). Artificial Intelligence: A Modern Approach
(Third ed.). Prentice Hall.
Saidani, Y., F. Dumpert, C. Borgs, et al. (2023). Qualit¨atsdimensionen
maschinellenlernensinderamtlichenstatistik. AStAWirtschafts-undSozial-
statistisches Archiv 17, 253–303.
32S¨arndal, C.-E., B. Swensson, and J. Wretman (1992). Model assisted survey
sampling. New York: Springer.
Snijkers, G., M. Bavdaˇz, S. Bender, J. Jones, S. MacFeely, J. Sakshaug,
K. Thompson, and A. van Delden (2023). Advances in Business Statistics,
Methods and Data Collection. Wiley.
Sohldickstein,N.andetal.(2024a). Boundary-optimizedneuralnetworks. Neu-
ral Computation 36(5), 101–120.
Sohldickstein, N.andet al.(2024b). Fractalbehaviorof neural networks. Jour-
nal of Machine Learning Research 25(1), 212–227.
Till´e, Y. (2020). Sampling and estimation from finite populations. Wiley.
United Nations (2019). UN National Quality Assurance Frameworks Manual
for Official Statistics. New York: United Nations.
Wikipedia contributors (2024). Confusion matrix — Wikipedia, the free ency-
clopedia. [Online; accessed 21-February-2024].
33