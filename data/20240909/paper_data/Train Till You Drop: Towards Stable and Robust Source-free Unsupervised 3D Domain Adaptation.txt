Train Till You Drop: Towards Stable and Robust
Source-free Unsupervised 3D Domain Adaptation
Björn Michele1,2 , Alexandre Boulch1 , Tuan-Hung Vu1 , Gilles Puy1,
Renaud Marlet1,3 , and Nicolas Courty2
1 valeo.ai, Paris, France
2 CNRS, IRISA, Univ. Bretagne Sud, Vannes, France
3 LIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS, Marne-la-Vallée, France
Abstract. Wetacklethechallengingproblemofsource-freeunsupervised
domain adaptation (SFUDA) for 3D semantic segmentation. It amounts
toperformingdomainadaptationonanunlabeledtargetdomainwithout
anyaccesstosourcedata;theavailableinformationisamodeltrainedto
achieve good performance on the source domain. A common issue with
existing SFUDA approaches is that performance degrades after some
trainingtime,whichisaby-productofanunder-constrainedandill-posed
problem.Wediscusstwostrategiestoalleviatethisissue.First,wepropose
asensiblewaytoregularizethelearningproblem.Second,weintroducea
novelcriterionbasedonagreementwithareferencemodel.Itisused(1)to
stopthetrainingwhenappropriateand(2)asvalidatortoselecthyperpa-
rameterswithoutanyknowledgeonthetargetdomain.Ourcontributions
are easy to implement and readily amenable for all SFUDA methods,
ensuring stable improvements over all baselines. We validate our findings
on various 3D lidar settings, achieving state-of-the-art performance. The
project repository (with code) is: github.com/valeoai/TTYD
Keywords: source-freeunsuperviseddomainadaptation·3Dlidarpoint
cloud · robustness
1 Introduction
The goal of domain adaptation (DA) is to transfer knowledge learned from a
source domain, typically with abundant or cheap annotated data, into a model
suited for a target domain, typically with less data or data more expensive to
annotate, thus saving acquisition or annotation costs. Concretely, DA studies
learning schemes to adapt networks to different forms of shifts between source
and target data distributions. If no annotation is available for the target domain,
the problem is referred to as unsupervised domain adaptation (UDA).
The traditional UDA setup requires the presence of both source and target
data during training. However, this is less desirable in practical scenarios for two
reasons:(i)sourceandtargetdataarenotalwaysaccessibleatthesametimedue
to data development cycles or to data privacy constraints, and (ii) many models
4202
peS
6
]VC.sc[
1v90440.9042:viXra2 B. Michele et al.
TTYD (ours) TENT[60] SHOT[30] URMDA[46]
core
NS→SK NS→WO
10 10
50 55
45
50
40
45
35
30 40
2 6 10 14 18 2 6 10 14 18
Fig.1: Evolutionoftheperformanceofbaselineswithoutdegradationpreventionstrate-
gies as they train over 20k iterations. Our method (TTYD ) uses an unsupervised
core
criterion to stop training. The horizontal dotted line illustrates that we keep the model
obtained at the stopping point (marked with a cross). Models are trained on nuScenes
(NS)andunsupervisedly adaptedtoSemanticKITTI(SK )andWaymoOpen(WO ).
10 10
have already been trained on existing source data, and retraining on both source
and target data is suboptimal in terms of consumed resources.
Inthiswork,weaddresssource-freeunsuperviseddomainadaptation (SFUDA)
for 3D semantic segmentation. In this setting, target adaptation is carried out
using unlabeled target data and without any access to source data; a model
trained on source data is however available. As opposed to vanilla UDA, SFUDA
cannot rely on source supervision to prevent the training process from drifting
towards collapse [23,71]. It is illustrated in Fig. 1 for baseline methods, where
training first benefits to the models before being detrimental. This phenomenon
is often mitigated in papers by rules of thumb, such as qualitative assessment or
early stopping based on ground-truth target labels, which are however supposed
to be unavailable. Though widely used in existing work, such practices obscure
quantitative comparisons and raise concerns about their actual applicability. Our
method departs from these practices: it totally ignores any target ground truth.
While widely exploited on image datasets, domain adaptation has recently
gainedattractionregardingpointclouds[70].Thistaskisparticularlychallenging
because domain shifts are multiple, including specific covariate shifts due to
sensors, acquisition conditions heterogeneity, and differences of class proportions
between domains [70]. Techniques like self-training and mixing [49], object size
adaptation [61] and surface regularization [38] have been proven effective in UDA
for 3D semantic segmentation. The SFUDA setup has also been studied for 3D
object detection, leveraging the temporal consistency of objects [51].
For SFUDA in 3D segmentation, we resort to a straightforward yet highly
effectivetrainingschemeinvolvingtwolosses:oneistoencouragemodelcertainty
on target samples and the other is to regularize the divergence in class distri-
)%(
UoImTTYD: Towards Stable and Robust 3D SFUDA 3
bution between source and target. To avoid the degradation issue, we propose
an unsupervised criterion that indicates when to stop the training. For this
criterion, the agreement of the trained model with a reference model is measured.
The red curve in Fig. 1 visualizes the evolution of our model’s performance
during training; the red cross marks the point when training is halted using our
criterion. Furthermore, we repurpose the stopping criterion as an unsupervised
validator, in the sense of Musgrave et al. [40]. We thus can unsupervisedly tune
all hyperparameters used in our base SFUDA framework, making it completely
hyperparameter-free. To summarize, our contributions are the following:
– We propose an unsupervised stopping criterion targeting the degradation
issue of 3D SFUDA.
– To achieve hyperparameter-freedom, we repurpose the stopping criterion as
an unsupervised model validator.
– WeintroduceaSFUDAtrainingschemethatworksfor3Dlidardatasemantic
segmentation and show promising results for image semantic segmentation.
– Extensive experiments (real-to-real and synthetic-to-real) show that our
method outperforms the SOTA of 3D SFUDA.
2 Related work
2.1 SFUDA in Computer Vision
Traditional Unsupervised Domain Adaptation techniques rely on a variety of
approaches to handle potential discrepancies between source and target do-
mains [63]. While some approaches look for Domain-invariant features by mini-
mizing statistical divergences between source and target feature representations
(e.g., [11,15,34,36,55,62]), or through adversarial training (e.g., [16,35,58]),
another line of work considers finding a Mapping between domains [6,19]. Based
on the assumption that both domains are not too different, other strategies
were proved efficient, such as reducing prediction uncertainty on target samples
in Self-supervised methods [59,60], relying on Pseudo-labeling [8,48,73,76] or
Self-ensembling [21,28,56,57], that maintains a teacher model using a temporal
exponential moving average of the student to ensure training stability.
In SFUDA, also called unsupervised model adaptation, and contrary to
previous methods, source data is no longer available at adaptation time [31,46].
Some abstract source information is however sometimes used, e.g., adapting
the target statistics of batches to those of the source [24,29,39,41,53,60]. The
seminal work SHOT [30] freezes the classification layer of the source model and
finetunes the remaining parameters by leveraging an information maximization
loss,composedofentropyminimizationatthesampleleveltoenforceunambiguous
predictions, while promoting global diversity by constraining predicted class
proportions [22,26,54]. Without any prior knowledge, diversity turns into an
objective of producing a balanced class distribution. Also, SHOT uses pseudo-
labeling based on prototypes obtained by clustering classes in the target domain.4 B. Michele et al.
TENT [60] freezes the model trained on source data but learns affine transfor-
mations in each normalization layer, whose parameters are trained to minimize
classificationentropy.Benefitslieinthereducedcomplexityofthelinearadapters,
which enforce simple changes in the normalization layer. However, as highlighted
in Fig. 1, it is not sufficient to prevent the model from drifting towards collapse.
To prevent this behavior, a possibility is to freeze the trainable weights of the
source network and work only on batch norm statistics. AdaBN [29] replaces
the running statistics (mean and variance) of the source dataset by the running
statistics of the target dataset. Rather than computing the running statistics on
test data once and for all, PTBN [41] solely relies on the batch statistics of test
data at inference time. In a similar spirit, MixedBN in [38], which is not per se a
SFUDA method, mixes at training time both source and target statistics of the
combined source-target dataset, but requires the source data. We showcase in
the remainder a small adaptation of it to the SFUDA case.
Performing adaptation at test time, those methods do not show the patholog-
ical drift exhibited in Fig. 1. However, their performances compare unfavorably
to methods that train a model, e.g., [30,60]. In this work, we propose to use
these non-learned models as guardrails for the optimization process.
Semantic segmentation. URMDA [46] is one of the first methods tackling
semantic segmentation in SFUDA, by minimizing an uncertainty loss to make
the feature representation more robust to noise, and by exploiting class-balanced
pseudo-labeling [76]. Self-training, especially with pseudo-labels is also a popular
approach [8,23,27,33,69,75]. In [23], the self-training stability is enforced by
constraining the current model using consistency with previous models.
3D-specific SFUDA. Applying UDA to 3D data has recently received a
lot of attention, with a focus on detection [37,44,51,61,66–68,72,74] and
segmentation [38,49,70]. But there are only a few works on SFUDA. Some
are specifically focusing on object detection [18,51], leveraging the trackability of
carsoverseveralframes[51],orimprovetheidentificationofregions-of-interestby
using attentive class prototypes [18]. Others target online SFUDA for semantic
segmentation [50], relying on spatio-temporal sequential lidar data, as well as on
an additional point cloud processing network to produce geometric features.
2.2 Mitigating the drift in SFUDA
Addressing model drift during adaptation is a significant challenge in SFUDA. It
is typically done by parameter tuning or early stopping based on target scores.
While it offers insight into the upper-bound performance of a method, it does not
account for real-world scenarios where target performance is not readily available.
Using validators. Validators have been introduced in UDA as methods for
selecting hyperparameters without any access to target labels [12,40]. In [38],
target entropy, information maximization (IM), and source validation have been
proventobereliableinanUDAsemanticsegmentationtaskon3Ddata.SND[47]
is used in [75] as a criterion to guide the update rate of the EMA teacher.TTYD: Towards Stable and Robust 3D SFUDA 5
RankME [17] assesses the quality of self-supervised representations without
labeled downstream data, and can thus also be used to select models.
Learning stabilization. Another approach is to improve the training stability,
e.g., modulating the learning rate or the update rate of the EMA teacher for
pseudo-labeling [75]. In DT-ST [75], the update interval of the EMA teacher
is selected based on the evolution of the SND [47] or entropy values. In [71],
the degradation is explained for pseudo-labeling approaches with the impact of
noisy-labels, and an early-learning regularization term is introduced, putting
more weight on the early predictions of the network in the training process.
3 Method
Our approach is mostly model-agnostic. We consider a model f, with trainable
parameters θ, that takes as input a point cloud P and that outputs, for each
point p∈P, a probabilistic classification prediction f[θ](P) ∈[0,1]K among K
p
classes (generally after a softmax as final layer). Without loss of generality, we
considerthatP canalsobeabatchofpointclouds,thatareprocessedinparallel.
We assume we are in the more usual white-box SFUDA setting [14]: we know the
architectureandhaveaccesstotheweights.Wedenotebyf[θs]themodeltrained
on source data Xs. (Xs is unavailable at domain adaptation time.) Finally, we
assumeweknowthesourceclassdistributionDs=D(Xs)∈[0,1]K.Ourgoalisto
find, without any ground-truth knowledge of the target data Xt, new parameters
θt such that the model f[θt] performs well on Xt.
The framework, coined as TTYD, is composed of three elements that can
be used independently: (i) a training scheme to regularize the adaptation of the
source-only model to target data, (ii) a stopping criterion (TTYD ) to halt
stop
training and prevent performance degradation, which is additionally repurposed
asavalidator (TTYD )tounsupervisedlytunetraininghyperparameters,and
valid
(iii) a self-training module using the initially-adapted model (i)+(ii) (TTYD )
core
as a starting point.
3.1 Training scheme
Ratherthantraininganewmodelfromscratch,weassumethatthetargetdomain
is not widely different from the source domain and adapt the already-trained
model f[θs] by fine-tuning it on target data Xt, without any label supervision.
General idea. To train on unlabeled target data, we need a guidance that
does not require ground-truth knowledge. To that end, we consider two training
objectives. First, and quite classically, the trained (adapted) model should be
discriminative, i.e., points should be classified with a large margin, which is one
way to promote certainty in the predictions. Second, and more originally, the
predicted class distribution of the target data should not only be diverse but in
fact similar enough to the source class distribution.
As already noted, previous SFUDA work only considers perfect class balanc-
ing [30], while autonomous driving data contains severe class imbalance, with6 B. Michele et al.
factors of proportion up to three orders of magnitude [32]. Besides, blindly bal-
ancing the classes ignores information that is readily available in the distribution
of the source data. Additionally, favoring the alignment of the predicated class
distribution onto source data is consistent with the fine-tuning strategy, which
consists in finding θt in the neighborhood of θs. Conversely, if target data is
actually very different from source data, domain adaptation makes little sense in
the first place. While the first objective (discriminability) is neither particular to
the task nor to the target domain, the second one (distribution similarity with
source data) is specific both to the task and to the target data.
Formal description. Concretely,toperformthetrainingontargetdata,weuse
a loss that does not require ground-truth knowledge. This new loss is composed
of two terms, which correspond to the two objectives mentioned above.
The first term penalizes ambiguity in the probabilistic class predictions. To
that end, we classically [30,60] measure the entropy of predictions:
1 (cid:88)
L (P)= H(f[θt](P) ) (1)
discrim |P| p
p∈P
where |P| is the number of points in P, and H is the entropy function.
The second term penalizes the discrepancy between the known class distri-
bution in the source data Ds, which we assume is not widely different from the
(unknown) class distribution in the target data Dt, and the predicted class distri-
bution of Dt, estimated as the average on the current point cloud (or batch) P:
1 (cid:88)
L (P)=KL(D(P)||Ds), where D(P)= f[θt](P) (2)
simsrc |P| p
p∈P
and KL is the Kullback–Leibler divergence. Of note, our approach differs from
priorwork[30],whichtriestoenforcesimilaritywiththeuniformclassdistribution.
In urban scene segmentation, while the source’s class distribution is not perfectly
aligned with the target’s, it still serves as a more accurate prior than uniform.
While an explicit class distribution prior has already been used in UDA [5,20],
we develop it here in the specific context of SFUDA: whereas source data is
inaccessible, we assume the source class distribution remains available.
Ourfinallossisthesumofthesetwoterms.Wedonotintroduceanybalancing
factorasthetwolossessomehowhaveasimilarnatureandrangeofvalues.Indeed,
like L , L can also be expressed with (cross-)entropies:
discrim simsrc
L (P)=KL(D(P)||Ds)=H(D(P),Ds)−H(D(P)). (3)
simsrc
Yet, to prevent overconfidence in discriminability, we consider a hinge-loss-like
variantofL thatignoressampleswithverylowentropy.Similarly,toprevent
discrim
the adapted model from following exactly the estimated distribution of classes in
the target set, we clip L under a certain threshold. Our actual loss is:
simsrc
L(P)=max(0,L (P)−λ)+max(0,L (P)−λ). (4)
discrim simsrcTTYD: Towards Stable and Robust 3D SFUDA 7
We use the same margin λ for both losses, which is set to 0.02 in all experiments.
The 3D source model is trained from scratch. Once trained, the Batch Nor-
malization (BN) layers [24] within the 3D model profoundly embody the char-
acteristics of the source domain. It results in significant covariate shifts when
the model is applied to the target domain. Competitive results in 3D UDA are
reported [38] by simply altering BN statistics, with AdaBN [29], PTBN [41] or
MixedBN[38].Despiteitslowoperationalcost,theeffectivenessofBNadaptation
in 3D perception is intriguing. Here, we explore this idea for 3D SFUDA.
We conducted an extensive study to determine which parameters (the entire
network, the classifier, or the BN layers) are better to finetune. Our finding is
that most parameter schemes yield similar results. (See supp.mat. for details.)
As it is sufficient to only alter few parameters, we keep the model f[θs]
completely frozen and replace BN layers by optimizable linear layers initialized
with BN statistics, scale and bias. A similar affine transformation is also used,
but at inference time, in [60].
3.2 Unsupervised stopping criterion and model validator
As discussed above, training in current SFUDA methods starts to provide gain
over the source-only model, before degrading (Fig. 1). Workarounds include
adding hyper-parameters that are hard to set without peeking at the inaccessible
target ground truth, e.g., fixed number of iterations or learning-rate scheduling.
A rightful solution is to rely on a validator, which scores adapted models
to choose the best one [40]. Using such a validator to tune hyperparameters
(includingthenumberoftrainingiterations),isawaytomakedomainadaptation
methods truly unsupervised [38,75]. A constraint is to use a validator that is
not based on the same principle as the validated domain adaptation method. As
an example, using the minimization of entropy both as a validator and as an
objective model optimization would lead to an infinite training. As validators
tend to measure the same kind of aspects that DA methods try to optimize, i.e.,
class discriminability and class diversity, this situation is not uncommon.
In fact, as illustrated in the experiment Sec. 4.2, existing validators are not
well suited for our method and fail to select a good model. The reason is that, as
a gauge of discriminability, a number of validators are also based or inspired by a
measure of entropy, as is our method. Regarding diversity, as existing validators
are designed to be general and target-set agnostic, they tend to measure how
uniform the class distribution is, which is basically the only thing one can do
without any prior on the target set. But as explained above, it is not appropriate
forautonomousdrivingdata,whichfeatureshighly-imbalancedclasses.Aspecific
validator-like criterion is needed with our SFUDA training.
Objective. Our goal here is to try to capture the best performance achieved by
a model as it trains, without any label knowledge on target data. More precisely,
we aim to identify the point when the unknown, underlying performance of a
model being trained starts to degrade.
General idea. In a supervised setting, a validation dataset is used to stop
training when the performance on this data starts to drop, thus reducing the risk8 B. Michele et al.
of over-fitting up to a certain extent. In UDA, the source data can be used either
during the training for stabilization purposes, or as a validator [40] to find an
optimal hyperparameter setting or an optimal point to stop the training. But in
SFUDA, source data is not available; we can thus only use a model trained using
source data as a basis to construct a validator or a stopping criterion.
For this construction, rather than just using model f[θs], we propose to use
an additional auxiliary model g that is already adapted to the target data in an
SFUDA fashion, and which is thus better than f[θs]. The idea however remains
to explore the space of domain adaptations using our SFUDA training, starting
from f[θt]=f[θs]. The auxiliary model g only acts as a kind of anchor to detect
0
when the model being trained strays too much and degrades. It does not alter
the training of f[θt] in any way, and definitely does not act as an upper bound in
terms of performance. It merely helps to identify when training f[θt] should stop.
Formal description. Given two models f,g that classify (among K classes)
the points x of a dataset X, we measure their class assignment agreement A(f,g)
by counting the number of times they make identical predictions:
1 (cid:88)
A(f,g)= 1(argmaxf(x) = argmaxg(x) ). (5)
|X| k k
k∈[[1,K]] k∈[[1,K]]
x∈X
As alternatives to this hard counting, we experimented with various divergences
to measure the agreement (symmetric KL divergence, L1 and L2 norms). All
options gave similar results (see supp.mat.) and we kept the simplest one.
The measure A(f,g), which is also a metric, can be used to define a model
validator in the sense of Musgrave et al. [40], i.e., as a way to select the best
model among a set of choices. Given a reference model g and a set of models F,
the best model f∗ is the one that agrees the most with g:
f∗ =argmaxA(f,g). (6)
f∈F
We now consider a model f being trained, with parameters θ at iteration i.
i
In its training trajectory from θ , the closest agreement point of f with another
0
model g is the smallest iteration i∗ that maximizes the agreement, i.e., i∗ =
minargmax A(f[θ ],g). Given that most on-going trainings tend to improve the
i i
performance, before the performance starts to drop continuously (cf. Fig. 1), we
consider as stopping point the first reversal in the increasing agreement phase,
i.e., the first iterationˆı after which the agreement starts to decrease:
ˆı=argminA(f[θ ],g)≥A(f[θ ],g). (7)
i i+1
i
The advantage of this first disagreement trend is that it does not have any
parameter and is quick to compute, whereas the closest agreement requires a
maximum training horizon. In theory, the stopping could be sub-optimal if there
are local maxima in the evolution of the class assignment agreement. However,
in practice, we do not check the agreement after processing each batch but after
a significant number of iterations (typically 1000), which has a smoothing effect.TTYD: Towards Stable and Robust 3D SFUDA 9
In our experiments, even checking the agreement as often as every 100 iterations,
which is practically useless on our context, yields similar results.
Empirically, the training of model f stops when reaching the maximum
agreement of 60-80% with g, but at a performance much higher than g by a
large margin. Though using an auxiliary model g as anchor can be seen as a
limitation in that it does not favor a disruptive improvement of f, we argue, as
shown in our experiments, that the remaining slack of 20-40% is sufficient to
provide substantial benefits, while preventing catastrophic outcomes in SFUDA.
Note that taking g=f[θs] would lead to a degenerate case because the closest
agreement point for A(f[θt],g) is then reached with i∗=0, i.e., θt =θs, meaning
i 0
that there is no adaptation on target data from the model trained on source data.
Therefore, we have to take as training starting point f[θt] a model close to f[θs],
0
but not equal to it. We have several possible choices, among the pure SFUDA
methods, as discussed below.
Selection of a reference model g. Intheory,thereferencemodelg canbeany
modelathandandreliable,provideditisnotbasedonthesameprinciplesasthe
training scheme. However, as a practical guideline, low-cost, hyperparameter-free
and training-free reference models are more favorable for SFUDA.
Recent 3D UDA SOTA [38] reveals the intriguing effectiveness of low-cost BN
adaptationmethods.WerevisitthesemethodsintheSFUDAcontextandobserve
competitive performance. Interestingly, BN adaptation methods do not require
training, hence they do not suffer from the degradation issue of training-based
methods. In addition, methods like AdaBN or PTBN are hyperparameter-free,
which is ideal for the unsupervised setting of SFUDA. BN-adapted models hence
become our primary choices to select a reference model g.
In the following, we use PTBN as our default reference model. It gives similar
resultsasAdaBNbutcanbeevaluatedonthefly,thusrequiringlesscomputation.
And we denote by TTYD the corresponding stopping criterion.
stop
Model validator. The stopping criterion TTYD , can serve as a model val-
stop
idator, referred to as TTYD , whose score is simply defined as the agreement
valid
level at the stopping point, i.e., A(f[θ ],g). The validator helps unsupervisedly
ˆı
tune the hyperparmeters to obtain the best model f∗, thanks to Eq. (6).
3.3 Self-training module
TheproposedtrainingschemealongwiththecriterionTTYD andthevalida-
stop
torTTYD allowustoadaptthepretrainedsourcemodeltoatargetdomain
valid
using only target data; more importantly the entire process is hyperparameter-
free. As later demonstrated in Sec. 4.4, this adapted model alone, referred to as
TTYD , performs better or is on par with SOTA baselines.
core
To obtain the final TTYD model, we conduct the second phase of self-
training [8,23,27,33,69,75]. Specifically, starting from TTYD , pseudo-labels
core
are computed on the fly for unlabeled target data, and then are used to self-
train the network with the standard cross-entropy loss. To stabilize training, the
EMA teacher model is used for pseudo-labeling [1]. Additionally, we employ the10 B. Michele et al.
Table 1: Datasets used in our domain adaptation experiments.
Dataset Lidar beams cls. Regionoftheworld Adaptationpairs
nuScenes [3] (NS) HDL-32E/32 16 Boston,Singapore
SynLiDAR [64] (SL) synthetic /64 22 UnrealEngine4
PandaSet [65] (PD) Pandar64/64 37 2UScities NS→PD 8[52]
WaymoOpen [13] (WO) L.B.H. /64 23 3UScities NS→WO 10[25]
SemanticPOSS [42] (SP) Pandora /40 14 PekingUniversity NS→SP 6[52],SL→SP 13[49]
SemanticKITTI[2] (SK) HDL-64E/64 19 Karlsruhe NS→SK 10[70],SL→SK 19[49]
Inadaptationpairs,subscriptnumberontargetindicatethenumberofmappedclasses(cls.).
Dynamic Teacher Update (DTU) [75] to adjust the update rate of the teacher
model dynamically, further stabilizing SFUDA self-training.
4 Experiments
4.1 Experimental setup
Datasets. The datasets we use for evaluation are listed in Tab. 1. It is worth
noting the variety of (rotating) lidar sensors (in particular number of beams),
labeled classes, and world scenes. Besides, one of the six datasets is synthetic.
Class mapping. The SFUDA setting assumes that source and target domains
sharesemanticclasses.Inpractice,whencomparingexistingdatasetswithground-
truthdata,notallclassesaresharedandtherearesometimespartialclassoverlaps.
For each source-target pair, we therefore have to select and aggregate common
classes in the two datasets to evaluate the quality of the domain adaptation.
However, we do not train source-only models based on class mappings; we use
the official classes of each dataset. The class mapping (Tab. 1 and supp.mat.) is
only used at evaluation time, to map source-domain classes inferred on target
data onto common classes that can be compared based on target ground truth.
Adapted domains. The different domain adaptation settings we experiment
with are summarized in Tab. 1. In the following, we write as subscript the
number of aggregated common classes that we use to evaluate the quality of the
adaptation.Weaddressdifferenttypesofdomainshifts:real-to-realandsparse-to-
dense (NS→SK , NS→SP , NS→PD , NS→WO ), as well as synthetic-to-real
10 6 8 10
(SL→SK ) including dense-to-sparse (SL→SP ).
19 13
Network setting. For all evaluated methods, we use the same sparse-voxel
Minkowski U-Net [7] with 10cm voxel size. It is a commonly used model for auto-
motive lidar semantic segmentation. The model contains 49 batch normalization
layers, thus, adapted parameters represent 0.06% of the model parameters.
As in [49,70], we do not use lidar intensity as input feature. Lidar intensities
are difficult to synthesize in simulated datasets and, for real datasets, reflectance
calibration may vary a lot from one sensor to another.
To train our method, we use AdamW with a learning rate of 10−5, a weight
decay of 0.01, and a batch size of 4. We use λ=0.02 in all settings and train forTTYD: Towards Stable and Robust 3D SFUDA 11
NS→SK
10
SL→SK
19
Learn.rateη(NS→SK10) Marginλ(NS→SK10)
SL→SP
13
NS→SP
6
10−1,−2,−3,−4,−6,−7 0,0.04,0.06,0.08
NS→PD
8
NS→WO
10
10−5 0.02
60 40 45
30 40
40
20 35
20 10 30
80
80 80
60
60 75
40
40 70
2 6 10 14 18 2 6 10 14 18 2 6 10 14 18
Fig.2: Performance%mIoU(top),asreference,andclassagreementin%(bottom),for
training over 20k iterations. (1st column) the crosses indicate when TTYD stops
stop
thetrainingindifferentSFUDAsetups.Dashedlinesafterthecrossesjustillustratethe
expected degradation issue. In reality, we do not continue training once the criterion is
triggered. (2nd and 3rd columns) the red curves correspond to the hyperparameters
η and λ selected using TTYD in NS→SK , showing we pick the best ones.
valid 10
at most 20k iterations on target data, creating checkpoints every 1k iterations
to test our stopping criterion. The source-only models are trained to achieve
high performance on the source validation set, regardless of the target data and
without considering class mapping.
We show in our ablation study and the application to image modality (both
see supp. mat.) that a wide range of models can serve as reference. However
BN adaptation models are the most readily available and remain competitive in
performance.
Evaluation. We measure performance with the classwise intersection over
union (IoU) and the mean IoU (mIoU) over all classes, as done in the official SK
benchmark [2], i.e., computed over the whole evaluation dataset.
4.2 Stopping criterion TTYD
stop
In this section, we evaluate the quality of our stopping criterion TTYD .
stop
First,Fig.2(left)showsthatourtrainingscheme,whilebeingrelativelystable
(little performance gap between the last and maximal mIoUs) on the majority of
adaptation scenarios, can still suffer from a sharp drop of performance: -38.0 pp.
on NS→SP . This highlights the need for using a good stopping criterion.
6
)%(
UoIm
)%(A12 B. Michele et al.
Table 2: Unsupervised stopping criteria to select the best checkpoint in 20k training
iterations (one checkpoint every 1k iterations). Oracle w/ GT gives the upper bound.
Stop. Criterion NS→SK SL→SK SL→SP NS→SP NS→WO NS→PD
10 19 13 6 10 8
Entropy [40] 41.4 27.8 29.7 23.7 47.8 60.9
SND [47] 41.4 22.3 30.5 23.7 47.8 60.9
IM [40] 42.4 27.8 34.8 57.5 51.1 63.0
BNM [10] 43.9 27.8 32.1 59.9 51.1 63.0
RankME [17] 42.4 28.2 32.1 57.5 51.0 63.3
TTYD 44.5 28.2 35.9 61.1 51.4 63.3
stop
Oracle w/ GT 44.7 28.2 36.0 61.4 51.4 64.9
Second, on the six practical domain adaptation cases we study, our stop
criterion TTYD is able to identify a model reaching a performance close to
stop
the best achievable one. This highlights the effectiveness of our method. In none
oftheobservedrunswasTTYD misledbyalocalmaximaofA.Computation
stop
is thus saved without giving up performance.
Third, we benchmark TTYD in Tab. 2. It outperforms other validators
stop
used as stopping criteria by a significant margin. RankMe, which is designed to
score feature quality, always chooses a model close to the source-only model. As
expected, the ‘Entropy’ validator selects suboptimal models as it relies on one of
the ingredients that we also use for our actual domain adaptation (cf. Eq. (1).
In conclusion, we see that our stopping criterion TTYD systematically
stop
selects high-performing models. We however do not claim it is applicable beyond
SFUDA, but that it is well suited for that problem.
4.3 Model validator TTYD
valid
Fig. 2(right) shows performance curves on NS→SK , for a range of learning
10
ratesη andmarginsλ,alignedwiththeiragreementscoreA.Weobservethatthe
agreement, which we can easily compute, is a good proxy for the actual mIoU,
which cannot be known for selecting the highest one as the ground truth is not
accessible. The weighted Spearman correlation (as in [40]) between performance
andagreementis0.95forthelearningratesand0.75forthemargins.Selectingthe
highestagreementthusisclosetoselectingthehighestmIoU.Infact,TTYD
valid
selects η=10−5 and λ=0.02.
4.4 3D-SFUDA benchmark
Strict SFUDA setting (hyperparameter free). We consider here a strict
SFUDA setting: any hyperparameter, if it exists, must be tuned without any
access to target scores, thus, e.g., using to a SFUDA validator.
WecompareTTYD tomethodsthatdonothaveanyhyperparameterand
core
that can thus be used in a pure SFUDA setting: Source-only, which is the modelTTYD: Towards Stable and Robust 3D SFUDA 13
Table 3: Performance(mIoU%)ontargetvalidationsetsintwoSFUDAsettings:strict
(withouthyperparameters,orwithhyperparameterstunedwithavalidator)andvanilla
(with hyperparameters set using target ground truth). For additional comparison, we
provide UDA results (using source data at adaptation time).
Domains Src. H.P. NS→ SL→ SL→ NS→ NS→ NS→
Method free free SK SK SP SP WO PD
10 19 13 6 10 8
Source-only ✓ ✓ 34.4 22.3 25.6 60.4 46.1 60.4
AdaBN [29] ✓ ✓ 39.9 24.6 25.4 57.7 47.7 59.6
PTBN [41] ✓ ✓ 39.4 22.4 23.7 54.7 42.3 60.2
MeanBN [38] ✓ ✓ 41.7 26.9 27.7 60.9 50.3 61.3
TTYD ✓ ✓ 44.5 28.2 35.9 61.1 51.4 63.3
core
SHOT [30] ✓ ✗ 34.9 18.4 21.7 42.4 37.3 43.7
TENT [60] ✓ ✗ 37.9 24.5 28.3 45.1 40.4 59.1
URMDA [46] ✓ ✗ 29.4 25.4 24.5 30.8 42.7 56.9
SHOT+ELR [71] ✓ ✗ 40.5 27.1 36.9 59.4 49.5 60.9
DT-ST [75] ✓ ✚ 35.6 23.5 36.8 63.1 51.8 62.5
TTYD ✓ ✚ 45.4 32.4 39.1 64.5 55.5 65.7
CoSMix [49] ✗ ✗ 38.3 28.0 40.8 65.2 – –
SALUDA [38] ✗ ✗ 46.2 31.2 42.9 65.8 – –
H.P.free(nohyperparameterorselectedwithvalidator):✓=Yes;✗=Noandparametersetsspecific
toeachsettingeitherreportedinliterature[38,49]orre-runbyourselveswhendefaultparameterdo
notperformcorrectly[30,46,60];✚=Nobutusingonesinglesetofparametersforallsettingstaken
fromimageSFUDAliterature[71,75]).Src.free:notusinganysourcedataatadaptationtime.
f[θs] trained on source data without any adaptation, AdaBN [29], PTBN [41]
and MeanBN, which is a simple source-free adaptation of MixedBN [38] (see
supp. mat. for a detailed description).
ThestrictSFUDAsettingispresentedintheupperpartofTab.3.TTYD
core
systematically outperforms all other parameterless approaches, sometimes with a
large margin (up to +8.2 pp. on SL→SP ).
13
Loose SFUDA setting. In this setting, we allow the use of hyperparameters
tuned by looking at the target performances. These hyperparameters may be
specific to each adaptation pair (indicated by ✗ in Tab. 3) or tuned once and for
all (indicated by ✚ in Tab. 3), possibly on other modalities, e.g., images.
As the default hyperparameters of SHOT [30], TENT [60], and URMA [46]
do not transfer to 3D SFUDA, we retrained these approaches with various sets of
hyperparameters and selected the best performing ones for each adaptation pair.
Regarding SHOT+ELR [71], we used a grid-searched hyperparameter for
SHOT and the two default hyperparameters for ELR, which are described
as robust [71]. As DT-ST [75] is designed for stability and robustness in the
SFUDA setting, we used its default set of hyperparameters (experimented on
images), which we also use for the DTU self-training module of TTYD. Last,
we report UDA scores (use of source data at adaptation time) for CosMix [49]
and SALUDA [38], as expected upper-bounds exploiting extra information.
ADUFStcirts
ADUFS)esool(
ADU14 B. Michele et al.
The results obtained in the common “vanilla” SFUDA setting are presented
in the middle part of Tab. 3. First, we observe that TTYD reaches state-of-the-
art performance on all adaptation scenarios. Second, comparing the results of
TTYD and TTYD highlights the interest of using a self-training scheme
core
for SFUDA. Third, if not for TTYD, TTYD ranks first or second in the
core
vanilla benchmark, which shows that hyperparameter-less or hyperparameter-
validated approaches are competitive. Last, TTYD closes the gap between
SFUDA methods and UDA approaches with an average gap of 1.2 mIoU point
on four adaptation pairs.
4.5 Application to image modality
The formulation of TTYD appears to be general enough to be used for other
modalities than 3D lidar data. To study this aspect, we conducted experiments
on image segmentation and obtained promising results. Please refer to the supp.
material for more details.
4.6 Ablations
Loss terms. Ablation of the two loss terms are Table 4: Loss and distribu-
presented in Tab. 4, showing the relevance of each tion study (NS→SK ).
10
ingredient. L L max
discrim simsrc
Prior class distribution. In Tab. 4, we compare unif. src mIoU%
the performance obtained with a uniform prior, to
34.4
the one obtained using the source class statistics. It
✓ 34.4
clearly shows the advantage of taking into account ✓ 34.4
the strong class imbalances in the data, even though ✓ 40.9
they are approximated by the source statistics. ✓ ✓ 44.7
5 Conclusion
In this work, we propose simple and effective strategies to stabilize the perfor-
mance of Source-Free Unsupervised Domain Adaptation in 3D semantic segmen-
tation. Our contributions include a novel stopping criterion that measures an
agreement with a reference model, and prevents catastrophic drifting of perfor-
mance due to the under-constrained nature of the optimization problem. We also
provide an easy to apply, yet efficient training scheme, that is well suited for the
task of semantic segmentation in autonomous driving scenarios. We demonstrate
the effectiveness of our proposal through extensive comparisons with state-of-
the-art methods in 3D semantic segmentation, which is a challenging SFUDA
instance, and we show its applicability in the image domain.TTYD: Towards Stable and Robust 3D SFUDA 15
Acknowledgements
WealsoacknowledgethesupportoftheFrenchAgenceNationaledelaRecherche
(ANR), under grants ANR-21-CE23-0032 (project MultiTrans), ANR-20-CHIA-
0030 (OTTOPIA AI chair), and the European Lighthouse on Secure and Safe AI
funded by the European Union under grant agreement No. 101070617. This work
was performed using HPC resources from GENCI–IDRIS (2022-AD011013839,
2023-AD011013839R1).
References
1. Araslanov, N., Roth, S.: Self-supervised augmentation consistency for adapting
semantic segmentation. In: CVPR (2021)
2. Behley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke, S., Stachniss, C., Gall,
J.: SemanticKITTI: A dataset for semantic scene understanding of lidar sequences.
In: ICCV (2019)
3. Caesar, H., Bankiti, V., Lang, A.H., Vora, S., Liong, V.E., Xu, Q., Krishnan, A.,
Pan,Y.,Baldan,G.,Beijbom,O.:nuScenes:Amultimodaldatasetforautonomous
driving. In: CVPR (2020)
4. Chen, M., Xue, H., Cai, D.: Domain adaptation for semantic segmentation with
maximum squares loss. In: ICCV (2019)
5. Chen, Y.H., Chen, W.Y., Chen, Y.T., Tsai, B.C., Frank Wang, Y.C., Sun, M.: No
more discrimination: Cross city adaptation of road scene segmenters. In: ICCV
(2017)
6. Choi,Y.,Choi,M.,Kim,M.,Ha,J.W.,Kim,S.,Choo,J.:Stargan:Unifiedgenerative
adversarialnetworksformulti-domainimage-to-imagetranslation.In:CVPR(2018)
7. Choy, C., Gwak, J., Savarese, S.: 4d spatio-temporal convnets: Minkowski convolu-
tional neural networks. In: CVPR (2019)
8. Corbiere, C., Thome, N., Saporta, A., Vu, T.H., Cord, M., Perez, P.: Confidence
estimation via auxiliary models. IEEE TPAMI (2021)
9. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,
Franke, U., Roth, S., Schiele, B.: The cityscapes dataset for semantic urban scene
understanding. In: CVPR (2016)
10. Cui, S., Wang, S., Zhuo, J., Li, L., Huang, Q., Tian, Q.: Towards discriminability
and diversity: Batch nuclear-norm maximization under label insufficient situations.
In: CVPR (2020)
11. Damodaran, B.B., Kellenberger, B., Flamary, R., Tuia, D., Courty, N.: Deepjdot:
Deep joint distribution optimal transport for unsupervised domain adaptation. In:
ECCV (2018)
12. Ericsson, L., Li, D., Hospedales, T.M.: Better practices for domain adaptation. In:
AutoML (2023)
13. Ettinger, S., Cheng, S., Caine, B., Liu, C., Zhao, H., Pradhan, S., Chai, Y., Sapp,
B., Qi, C.R., Zhou, Y., Yang, Z., Chouard, A., Sun, P., Ngiam, J., Vasudevan, V.,
McCauley, A., Shlens, J., Anguelov, D.: Large scale interactive motion forecasting
for autonomous driving: The waymo open motion dataset. In: ICCV (2021)
14. Fang, Y., Yap, P.T., Lin, W., Zhu, H., Liu, M.: Source-free unsupervised domain
adaptation: A survey. arXiv preprint arXiv:2301.00265 (2022)
15. Fatras, K., Séjourné, T., Courty, N., Flamary, R.: Unbalanced minibatch optimal
transport; applications to domain adaptation. In: ICML (2021)16 B. Michele et al.
16. Ganin, Y., Ustinova, E., Ajakan, H., Germain, P., Larochelle, H., Laviolette, F.,
Marchand, M., Lempitsky, V.: Domain-adversarial training of neural networks.
JMLR (2016)
17. Garrido, Q., Balestriero, R., Najman, L., Lecun, Y.: Rankme: Assessing the down-
stream performance of pretrained self-supervised representations by their rank. In:
ICML (2023)
18. Hegde, D., Patel, V.M.: Attentive prototypes for source-free unsupervised domain
adaptive 3d object detection. In: WACV (2024)
19. Hoffman,J.,Tzeng,E.,Park,T.,Zhu,J.Y.,Isola,P.,Saenko,K.,Efros,A.,Darrell,
T.: Cycada: Cycle-consistent adversarial domain adaptation. In: ICLR (2018)
20. Hoffman,J.,Wang,D.,Yu,F.,Darrell,T.:FCNsinthewild:Pixel-leveladversarial
and constraint-based adaptation. arXiv preprint arXiv:1612.02649 (2016)
21. Hoyer, L., Dai, D., Van Gool, L.: Daformer: Improving network architectures and
training strategies for domain-adaptive semantic segmentation. In: CVPR (2022)
22. Hu, W., Miyato, T., Tokui, S., Matsumoto, E., Sugiyama, M.: Learning discrete
representations via information maximizing self-augmented training. In: ICML
(2017)
23. Huang, J., Guan, D., Xiao, A., Lu, S.: Model adaptation: Historical contrastive
learning for unsupervised domain adaptation without source data. In: NeurIPS
(2021)
24. Ioffe, S., Szegedy, C.: Batch normalization: Accelerating deep network training by
reducing internal covariate shift. In: ICML. PMLR (2015)
25. Kim, H., Kang, Y., Oh, C., Yoon, K.J.: Single domain generalization for lidar
semantic segmentation. In: CVPR (2023)
26. Krause, A., Perona, P., Gomes, R.: Discriminative clustering by regularized infor-
mation maximization. In: NeurIPS (2010)
27. Kundu, J.N., Kulkarni, A., Singh, A., Jampani, V., Babu, R.V.: Generalize then
adapt: Source-free domain adaptive semantic segmentation. In: ICCV (2021)
28. Laine, S., Aila, T.: Temporal ensembling for semi-supervised learning. In: ICLR
(2017)
29. Li,Y.,Wang,N.,Shi,J.,Hou,X.,Liu,J.:Adaptivebatchnormalizationforpractical
domain adaptation. PR 80 (2018)
30. Liang, J., Hu, D., Feng, J.: Do we really need to access the source data? Source
hypothesis transfer for unsupervised domain adaptation. In: ICML (2020)
31. Liang, J., Hu, D., Feng, J.: Do we really need to access the source data? source
hypothesis transfer for unsupervised domain adaptation. In: ICML (2020)
32. Liu, M., Zhou, Y., Qi, C.R., Gong, B., Su, H., Anguelov, D.: LESS: Label-efficient
semantic segmentation for lidar point clouds. In: ECCV (2022)
33. Liu, Y., Zhang, W., Wang, J.: Source-free domain adaptation for semantic segmen-
tation. In: CVPR (2021)
34. Long, M., Cao, Y., Wang, J., Jordan, M.: Learning transferable features with deep
adaptation networks. In: ICML (2015)
35. Long, M., Cao, Z., Wang, J., Jordan, M.I.: Conditional adversarial domain adapta-
tion. In: NeurIPS (2018)
36. Long, M., Zhu, H., Wang, J., Jordan, M.I.: Deep transfer learning with joint
adaptation networks. In: ICML (2017)
37. Luo, Z., Cai, Z., Zhou, C., Zhang, G., Zhao, H., Yi, S., Lu, S., Li, H., Zhang, S.,
Liu, Z.: Unsupervised domain adaptive 3d detection with multi-level consistency.
In: ICCV (2021)
38. Michele, B., Boulch, A., Puy, G., Vu, T.H., Marlet, R., Courty, N.: SALUDA:
Surface-based automotive lidar unsupervised domain adaptation. In: 3DV (2024)TTYD: Towards Stable and Robust 3D SFUDA 17
39. Mirza,M.J.,Micorek,J.,Possegger,H.,Bischof,H.:Thenormmustgoon:Dynamic
unsupervised domain adaptation by normalization. In: CVPR (2022)
40. Musgrave,K.,Belongie,S.,Lim,S.N.:Threenewvalidatorsandalarge-scalebench-
markrankingforunsuperviseddomainadaptation.arXivpreprintarXiv:2208.07360
(2022)
41. Nado, Z., Padhy, S., Sculley, D., D’Amour, A., Lakshminarayanan, B., Snoek, J.:
Evaluatingprediction-timebatchnormalizationforrobustnessundercovariateshift.
arXiv preprint arXiv:2006.10963 (2020)
42. Pan,Y.,Gao,B.,Mei,J.,Geng,S.,Li,C.,Zhao,H.:SemanticPOSS:Apointcloud
dataset with large quantity of dynamic instances. In: IV (2020)
43. Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen,
T., Lin, Z., Gimelshein, N., Antiga, L., et al.: Pytorch: An imperative style, high-
performance deep learning library. In: NeurIPS (2019)
44. Peng, X., Zhu, X., Ma, Y.: Cl3d: Unsupervised domain adaptation for cross-lidar
3d detection. In: AAAI (2023)
45. Richter, S.R., Vineet, V., Roth, S., Koltun, V.: Playing for data: Ground truth
from computer games. In: ECCV (2016)
46. S, P.T., Fleuret, F.: Uncertainty reduction for model adaptation in semantic
segmentation. In: CVPR (2021)
47. Saito, K., Kim, D., Teterwak, P., Sclaroff, S., Darrell, T., Saenko, K.: Tune it the
right way: Unsupervised validation of domain adaptation via soft neighborhood
density. In: ICCV (2021)
48. Saito,K.,Ushiku,Y.,Harada,T.:Asymmetrictri-trainingforunsuperviseddomain
adaptation. In: ICML (2017)
49. Saltori, C., Galasso, F., Fiameni, G., Sebe, N., Ricci, E., Poiesi, F.: Cosmix:
Compositional semantic mix for domain adaptation in 3d lidar segmentation. In:
ECCV (2022)
50. Saltori,C.,Krivosheev,E.,Lathuiliére,S.,Sebe,N.,Galasso,F.,Fiameni,G.,Ricci,
E., Poiesi, F.: Gipso: Geometrically informed propagation for online adaptation in
3d lidar segmentation. In: ECCV (2022)
51. Saltori, C., Lathuiliére, S., Sebe, N., Ricci, E., Galasso, F.: Sf-uda 3d: Source-free
unsuperviseddomainadaptationforlidar-based3dobjectdetection.In:3DV(2020)
52. Sanchez, J., Deschaud, J.E., Goulette, F.: Domain generalization of 3d semantic
segmentation in autonomous driving. In: ICCV (2023)
53. Schneider, S., Rusak, E., Eck, L., Bringmann, O., Brendel, W., Bethge, M.: Im-
proving robustness against common corruptions by covariate shift adaptation. In:
NeurIPS (2020)
54. Shi, Y., Sha, F.: Information-theoretical learning of discriminative clusters for
unsupervised domain adaptation. In: ICML (2012)
55. Sun,B.,Saenko,K.:Deepcoral:Correlationalignmentfordeepdomainadaptation.
In: ECCV (2016)
56. Tarvainen, A., Valpola, H.: Mean teachers are better role models: Weight-averaged
consistency targets improve semi-supervised deep learning results. In: NeurIPS
(2017)
57. Tranheden, W., Olsson, V., Pinto, J., Svensson, L.: Dacs: Domain adaptation via
cross-domain mixed sampling. In: WACV (2021)
58. Tzeng, E., Hoffman, J., Saenko, K., Darrell, T.: Adversarial discriminative domain
adaptation. In: CVPR (2017)
59. Vu, T.H., Jain, H., Bucher, M., Cord, M., Pérez, P.: Advent: Adversarial entropy
minimization for domain adaptation in semantic segmentation. In: CVPR (2019)18 B. Michele et al.
60. Wang, D., Shelhamer, E., Liu, S., Olshausen, B., Darrell, T.: Tent: Fully test-time
adaptation by entropy minimization. In: ICLR (2021)
61. Wang, Y., Chen, X., You, Y., Li, L.E., Hariharan, B., Campbell, M., Weinberger,
K.Q., Chao, W.L.: Train in germany, test in the usa: Making 3d object detectors
generalize. In: CVPR (2020)
62. Wang, Y., Li, W., Dai, D., Van Gool, L.: Deep domain adaptation by geodesic
distance minimization. In: CVPRW (2017)
63. Wilson, G., Cook, D.J.: A survey of unsupervised deep domain adaptation. ACM
TIST (2020)
64. Xiao, A., Huang, J., Guan, D., Zhan, F., Lu, S.: Transfer learning from synthetic
to real lidar point cloud for semantic segmentation. In: AAAI (2022)
65. Xiao, P., Shao, Z., Hao, S., Zhang, Z., Chai, X., Jiao, J., Li, Z., Wu, J., Sun, K.,
Jiang, K., et al.: Pandaset: Advanced sensor suite dataset for autonomous driving.
In: ITSC (2021)
66. Xu, Q., Zhou, Y., Wang, W., Qi, C.R., Anguelov, D.: Spg: Unsupervised domain
adaptation for 3d object detection via semantic point generation. In: ICCV (2021)
67. Yang, J., Shi, S., Wang, Z., Li, H., Qi, X.: St3d: Self-training for unsupervised
domain adaptation on 3d object detection. In: CVPR (2021)
68. Yang, J., Shi, S., Wang, Z., Li, H., Qi, X.: St3d++: Denoised self-training for
unsupervised domain adaptation on 3d object detection. IEEE TPAMI (2022)
69. Ye, M., Zhang, J., Ouyang, J., Yuan, D.: Source data-free unsupervised domain
adaptation for semantic segmentation. In: ACM MM (2021)
70. Yi,L.,Gong,B.,Funkhouser,T.:Complete&Label:Adomainadaptationapproach
to semantic segmentation of lidar point clouds. In: CVPR (2021)
71. Yi, L., Xu, G., Xu, P., Li, J., Pu, R., Ling, C., McLeod, A.I., Wang, B.: When
source-free domain adaptation meets learning with noisy labels. In: ICLR (2023)
72. You, Y., Diaz-Ruiz, C.A., Wang, Y., Chao, W.L., Hariharan, B., Campbell, M.,
Weinbergert, K.Q.: Exploiting playbacks in unsupervised domain adaptation for 3d
object detection in self-driving cars. In: ICRA (2022)
73. Zhang, P., Zhang, B., Zhang, T., Chen, D., Wang, Y., Wen, F.: Prototypical
pseudo label denoising and target structure learning for domain adaptive semantic
segmentation. In: CVPR (2021)
74. Zhang,W.,Li,W.,Xu,D.:Srdan:Scale-awareandrange-awaredomainadaptation
network for cross-dataset 3d object detection. In: CVPR (2021)
75. Zhao, D., Wang, S., Zang, Q., Quan, D., Ye, X., Jiao, L.: Towards better stability
and adaptability: Improve online self-training for model adaptation in semantic
segmentation. In: CVPR (2023)
76. Zou,Y.,Yu,Z.,Kumar,B.,Wang,J.:Unsuperviseddomainadaptationforsemantic
segmentation via class-balanced self-training. In: ECCV (2018)TTYD: Towards Stable and Robust 3D SFUDA 19
Train Till You Drop: Towards Stable and Robust
Source-free Unsupervised 3D Domain Adaptation
— Supplementary Material —
Overview
In this document, we provide: experiments on the application of TTYD to
the image modality (Sec. A), additional implementation details (Sec. B), a
guarantee of the soundness (Sec. C), and additional ablations: on the parameters
toadapt(Sec.D),onalternativedistancesfortheconsistencyvalidatorTTYD
stop
(Sec. E) and on other reference models (Sec. F). We also report the performance
of TTYD with other training schemes (Sec. G) and discuss the SFUDA
stop
hypothesis for our training scheme (Sec. H). Additionally, we also provide the
per-class results and comparison to non-SF UDA approaches (Sec. I), qualitative
results (Sec. J), and more details on the datasets and class mappings (Sec. K).
A Application to image modality
Whiledevelopedfor3DSFUDA,theformulationof TTYDappearstobegeneral
enough to be used for other modalities. To study this aspect, we conducted
experiments on image segmentation. We used the GTA5 dataset [45] as source,
and the Cityscapes (City) dataset [9] as target.
This is also an opportunity to eval-
Table 5: SFUDA for image modality.
uate if different models can be used as
reference models for the validation. We Valid. GTA5 →
Method ref. model City
remark, nevertheless, that it is common
practice for image semantic segmentation Source-only 36.8
to keep the ImageNet-pretrained batch- URMDA [46] 45.1
norm frozen during training on the source SFDA [33] 45.8
SDF [69] 49.4
dataset. We cannot directly use a PTBN
HCL [23] 48.1
versionofsuchsource-onlymodelsasrefer-
DT-ST [75] 52.1
ence for TTYD , in particular because
stop TTYD PTBN 53.4
theImageNet-pretrainedbatchnormstatis-
TTYD DT-ST 53.2
tics differ too much from those we would
have obtained on the source training set. Therefore, we use a PTBN model built
using a source-only model trained without freezing the BN layers [4]. We also
test the DT-ST model from [75].
Our results are presented in Tab. 5. We also reach SOTA performances for
theGT5→Cityadaptationpair.Asweusetheself-trainingmoduleofDT-ST,we
can conclude that, as for 3D SFUDA, the final performance relies on the quality
of the self-training starting point, which is provided here by TTYD .
core20 B. Michele et al.
B Additional implementation details
WeusePyTorchforourimplementation[43].ThemodelsforNS→SK ,SL→SK ,
10 19
and NS→PD are trained on a single NVIDIA GeForce RTX 2080 Ti (11 GB)
8
GPU.ForSL→SP ,NS→SP ,andNS→PD ,weuseasplitNVIDIAA100-40GB
13 6 8
GPU with 20 GB memory.
Code. AdaBN [29] and PTBN [41] were not designed specifically for 3D point
clouds; we implemented them. MeanBN is derived from the idea of MixedBN [38]
(ratherthanthecodeofMixedBN,whichrequiressourcedata,seejustbelow);we
implemented it ourselves. AdaBN, PTBN and MeanBN are hyperparameter-free.
For DT-ST [75], we used the official code repository and default parameters, as
recommended. Code for SHOT [30], TENT [60] and URMDA [46] was taken
from their official repository, with parameters set as described below.
Note on MixedBN and MeanBN. In the main paper, we introduce MeanBN
as a SFUDA version of MixedBN [38]. Indeed, MixedBN computes the average
running statistics of the source and target datasets by mixing them during the
training, which cannot be done in an SFUDA setting. MeanBN just averages
(with equal weight) the running statistics from source training and from passing
the target data through the source-trained network: it is the average of the
running statistics of Source-only and AdaBN.
Parameters selected for SHOT, TENT and URMDA. For SHOT [30], we
obtained the best results on the target validation set with a learning rate of 10−6
and a balancing hyperparameter of β=10−5. For TENT [60] and URMDA [46],
we used a learning rate of 10−5. Additionally, as URMDA relies on [76] for
setting per-class confidence thresholds, we achieved optimal results with sig-
nificantly different values for the target portion p, depending on source and
target domains: p=0.01 (NS→SK , NS→WO ), p=0.9 (SL→SK , SL→SP ,
10 10 19 13
NS→PD ), p=0.1 (NS→SP ).
8 6
Self-training (ST) propagates and somehow denoises uncertain pseudo-labels.
IthasbeensuccessfullyusedinUDA[38,49]andSFUDA[75].Table3inthemain
paper shows the benefits of adding self-training in our context (line TTYD
core
vs line TTYD).
We used the self-training from [75], which we adapted for point clouds, e.g.,
regarding augmentations. This self-training handles a confidence level for each
class, making sure to also promote rare classes. This allows us to train on the
target data, selecting a mostly-correct set of labels while keeping a sufficient
balance of rare classes, also preventing collapse, which may occur when focusing
mainly on most frequent classes.
Training time. Our stopping criterion TTYD saves a lot of time and
stop
computationatthetrainingstage.Forexample,westopthetrainingforNS→SK
10
after 1.1 hr, compared to 6 hrs for a full 20k-iteration training.
The design of our training scheme itself makes it also faster, as there is no
costlycentroidgenerationaftereachepochlikeinSHOT[30],where20kiterations
require 30 hrs, or time-consuming surface reconstruction regularization like inTTYD: Towards Stable and Robust 3D SFUDA 21
SALUDA [38], which is reported to run in 120 hrs. The self-training step then
takes about 10 hrs.
GPU memory footprint. Our training scheme is also memory efficient at
training time, as only one semantic segmentation network is needed. This is in
contrast, e.g., to DT-ST [75], where an additional teacher network is used, or to
SALUDA [38], which uses an additional geometric regularization head during
training.
C Soundness guarantee
WecanshowthatTTYD issound becausetheagreementA(f,g)(cf.Eq.(5)),
stop
which is bounded by 1, can only take at most |X|+1 different values. Hence, the
number of iterations, as defined by Eq.(7), is bounded by |X|. Also, to check
the stopping criterion efficiently, we actually only evaluate Eq.(7) after a fixed
number N of iterations (typically, N=1000). Even so, the number of iterations
remains bounded, by N|X|. In our experiments, the number of iterations at the
stopping point is however much smaller than N|X|, typically between 5 and 10k.
However, it is to be noted that we have no performance guarantees, as most
UDA and SFUDA methods, including validators [40,47], whose performance
results are generally empirical.
D Ablation: Model parameters to adapt
In Tab. 6, we explore a wide range of possible options concerning the parameters
to adapt, some of which are already proposed in the literature [29,30,38,60,75].
Please note that reported values represent the maximum performance over a
training for 20k iterations; a stopping criterion is to be used on top of that.
Although they differ in terms of maximum performance, most adaptation
strategies make sense, except adapting the classification layer only (Tab. 6.a). On
the contrary, adapting the features in the backbone, including before each layer,
is key to the performance, to obtain linearly separable features. Adapting the
runningstatisticsonlinebothattrainandevaltimealsoisdetrimental(Tab.6.b),
probably because it does not “see” enough target data. In the end, we adopt for
our method the affine transformations before each batch normalization layer as
it performs the best, although adapting the backbone is on average nearly as
good. Besides, it reduces the memory footprint as fewer parameters have to be
updated (although not reducing gradient computation) and it could facilitate
investigations for a deeper understanding of the adaptation.22 B. Michele et al.
Table 6: Ablation study
(a) Parameters to adapt. Assuming frozen statistics, parameters to update can be
replacement of BN by linear layer, or the backbone weights only (without the
classification layer) for different learning rates, or the classification layer only, or the
complete network (backbone + classification layer).
BN→ Lin. Backbone only Classif. Backbone+classif.
Adaptation w/o w/ 10−5 10−6 10−7 layer 10−5 10−6 10−7
bias bias
NS→SK 44.0 44.7 40.3 42.1 42.0† 34.4 41.5 41.4† 35.7†
10
SL→SK 27.9† 28.2 27.9 28.5 26.7† 22.4 28.1 28.0† 23.3†
19
SL→SP 36.1 36.0 31.3 36.6 36.9† 34.1 36.6 36.9 30.0†
13
NS→SP 61.5 61.4 60.5 61.5† 60.9† 60.4 61.5 61.4 61.0†
6
(b) Choice of running statistics for BN layers, (c) Class distribution to
either fixed or variable (per-instance norm. at train target, uniform or obtained
and eval time, or only at train and fixed at eval). from source data.
Fixed statistics Online statistics
Distribution
Adaptation source target mean train train
Adaptation uniform source
+eval
NS→SK 35.0 44.7
NS→SK 44.7 43.4 45.9 39.1 43.7∗ 10
10 SL→SK 23.8 28.2
SL→SK 28.2 26.2 27.4 22.2 26.7∗ 19
19 SL→SP 25.6 36.0
SL→SP 36.0 30.9 34.4 23.7 26.8∗ 13
13 NS→SP 60.7 61.4
NS→SP 61.4 59.3 61.1 54.7 60.4∗ 6
6
Maximum mIoU% over 20k iterations, learning rate 10−5 unless otherwise stated.
∗: performance strongly fluctuating. †: maximum reached at 20k iterations.
Table 7: Performance of our criterion TTYD and other using soft measurements
stop
to select a model being trained over 20k iterations (one model for each 1k iteration
increment).
Adaptation NS→
Validator SK
10
TTYD (i.e., hard choice A) 44.5
stop
TTYD L2 44.5
stop
TTYD L1 44.5
stop
TTYD Symmetric KL 44.5
stop
E Ablation: Other distances for consistency validator
We show in Tab. 7 the results of our stopping criterion using various divergences
to measure the agreement (symmetric KL divergence, L1 and L2 norms), instead
of the default hard counting of identical predictions. As all different optionsTTYD: Towards Stable and Robust 3D SFUDA 23
Table 8: Performance of our TTYD with different reference models to select a
stop
model being trained over 20k iterations (one model for each 1k iteration increment).
Adaptation NS→ SL→ SL→ NS→ NS→ NS→
Validator SK SK SP SP WO PD
10 19 13 6 10 8
Source-only 34.4 22.3 25.6 60.4 46.1 60.4
TTYD-train (last iter.) 39.2 27.8 28.1 23.4 47.7 60.8
TTYD-train (max. value) 44.7 28.2 36.0 61.4 51.4 64.9
TTYD (i.e., w/ PTBN) 44.5 28.2 35.9 61.1 51.4 63.3
stop
TTYD w/ AdaBn 44.5 28.2 36.0 61.1 51.4 63.3
stop
TTYD w/ MeanBN 39.0 26.9 32.3 61.1 49.8 60.4
stop
TTYD w/ SHOT [30] 43.8 22.3 29.8 60.4 46.1 63.3
stop
TTYD w/ TENT [60] 43.0 27.4 35.9 61.4 50.2 64.5
stop
TTYD w/ URMDA [46] 39.0 24.7 25.6 60.4 46.1 60.4
stop
TTYD w/ SHOT+ELR [71] 44.6 28.1 32.3 60.4 51.0 63.3
stop
TTYD w/ DT-ST [75] 42.4 26.9 32.3 60.4 49.8 63.3
stop
give the same results we keep the simplest one, the hard counting of identical
predictions.
F Ablation: Other reference models
In Tab. 8, we compare the performance of the model selected by TTYD
stop
using PTBN as a reference model, against the selection of models using AdaBN
and MeanBN as reference models. It can be seen that using PTBN or AdaBN as
referencemodelaremostlyequivalent.UsingMeanBNisclearlyinferior,probably
because it is too close to the source-only model: it always selects a model trained
for less iterations than our proposed alternatives.
Wealsotestedothermodelsaspotentialreferencemodels:DT-ST,SHOT+ELR,
SHOT, TENT and URMDA. We use the model obtained after 20k iterations
as reference model for all these methods. DT-ST and and SHOT-ELR are able
to select competitive checkpoints, improving performance over the source-only
one in 5 out of the 6 domain adaptation scenarios. Although SHOT suffered
from a strong performance degradation during training, and therefore would
not be a natural choice as reference model, SHOT allows selection a better
performing model that the source-only model in half of the domain adaptation
settings, and never select a model performing worse than the source-only one.
It is to be noted that PTBN, AdaBN, MeanBN are hyperparameter-free. We
use default hyperparameters for DT-ST. For SHOT, TENT, URMDA, we use
target-validated hyperparameters to study their potential.24 B. Michele et al.
Table 9: Performance of our criterion TTYD to select a SHOT or URMDA model
stop
being trained over 20k iterations (one model for each 1k iteration increment).
Adaptation NS→ SL→ SL→ NS→ NS→ NS→
Validator SK SK SP SP WO PD
10 19 13 6 10 8
Source-only 34.4 22.3 25.6 60.4 46.1 60.4
TTYD-train (last iter.) 39.2 27.8 28.1 23.4 47.7 60.8
TTYD-train (max. value) 44.7 28.2 36.0 61.4 51.4 64.9
TTYD 44.5 28.2 35.9 61.1 51.4 63.3
core
SHOT last iter. 34.9 18.4 21.7 42.4 37.3 43.7
SHOT max. 42.7 27.9 36.7 61.2 50.1 62.9
SHOT w/ TTYD 40.7 27.9 35.9 61.2 50.1 62.9
stop
URMDA last iter. 29.4 25.4 24.5 30.8 42.7 56.9
URMDA max. 37.5 25.5 33.4 63.0 48.4 60.4
URMDA w/ TTYD 37.2 25.6 25.6 60.4 46.1 60.4
stop
G TTYD for other training schemes
stop
In Tab. 9 we also apply TTYD to SHOT and URMDA, as both methods
stop
are facing strong model degradation during training. We report the maximal
achieved performance during training (max.), the performance reached after 20k
iterations (last iter.), and the performance reached using our stopping criterion
(TTYD ). We see that our stopping criterion is able to pick a model whose
stop
performance is close to the best achieved performance during training (max.).
The application of our stop criterion on TENT does not make sense as the
starting point for the TENT method is identical to the reference model.
H SFUDA hypothesis
For our training scheme, we use no source data. Besides a source-only trained
model f[θs], we only use global statistics Ds=D(Xs) on source data, i.e., a few
class frequencies. These class-wise point ratios are in fact often already provided
on dataset datasheets, e.g., SemanticKITTI [2], nuScenes [3]. This very minor
requirement complies with motivations of source-free approaches, e.g., privacy,
lostaccessorcomputationsaving.AsitcanbeseeninTab.10:alternativestoour
prior (DS) in Eq.(2) (main paper) do not perform well on NS→SK . However,
10
the correct target class data distribution (DT), which of course is not available,
but could be seen of a kind of oracle, helps to further improve the performance.
I Classwise results and related approaches
In this section, we detail classwise results of semantic segmentation after domain
adaptation. We also compare to UDA methods.TTYD: Towards Stable and Robust 3D SFUDA 25
Table10:ComparisonofdifferentpriorsinEq.(2)onNS→SK .Foreasiercomparison
10
we report the maximal obtained performance with our training scheme without the
selection of TTYD .
stop
KL(D(P)|| ? ) unif. D(f[θs](Xt)) Ds(ours) Dt (oracle)
Ours (mIoU%) 35.0 34.4 44.7 47.0
Per-classresults. WeprovideinTabs.11to16theclasswiseresultsformethods
and domain adaptation settings reported in Tab. 2 of the main paper. It can be
seen that the gain in performance (mIoU) achieved by our TTYD originates,
core
on all dataset settings, from a consistent improvement over a broad range of
classes, not just a few of them.
UDA (with source data) as a kind of SFUDA upper bound. General
UDA is privileged over the SFUDA setting because it has access to the source
data at training time. UDA resutls thus represents a kind of upper bound to
SFUDA’s. To analyze this aspect, we compare to two state-of-the-art UDA
methods, namely CoSMix [49] and SALUDA [38], on the domain adaptation
settings we experimented with and for which UDA results are available, i.e.,
NS→SK , SL→SK , SL→SP and NS→SP .
10 19 13 6
Please note that CoSMix has hyperparameters, which have to be (and are)
optimized for each setting on the ground-truth target validation set (which
somewhat detracts from the lack of supervision). On the contrary, SALUDA uses
an unsupervised validator (Entropy [40]), like we do with our own unsupervised
stopping criterion and validator.
As can be seen in Tabs. 11 to 12, although CoSMiX and SALUDA do have a
bettermIoUonaverage,ourmethodTTYD stilloutperformsCoSMixon2/4
core
domain adaptations and is only 1.8 to 4.7 percentage points behind SALUDA,
except on SL→SP , where SALUDA remains 7.0 p.p. ahead. TTYD reduces
13
the gaps with SALUDA down to 0.8 to 3.8 p.p., and even outperforms SALUDA
by 1.2 p.p. on SL→SK .
19
Please note that we compare to values reported in the SALUDA paper [38],
including for CoSMix [49], as the evaluation protocol in [49] for mIoU calculation
differs from the official evaluation metric [2], which we use instead. Furthermore,
[38] report results as an average over 3 runs, whereas we provide here only the
results of a single run.26 B. Michele et al.
Table 11: Classwise results for NS→SP . † from [38].
6
NS→ (%S IP o6
U) %
mIoU Person
Bike Car
Ground Vegetatio Mn anmade
Strict SFUDA
Source-only 60.4 56.1 7.5 65.0 79.4 79.0 75.7
AdaBN [29] 57.7 58.8 14.9 42.8 76.8 79.2 73.7
PTBN [41] 54.7 55.2 10.5 41.0 75.7 74.8 70.9
MeanBN [38] 60.9 58.6 12.4 60.7 78.0 80.0 75.5
TTYD (ours) 61.1 57.0 11.3 64.2 79.0 80.6 74.4
core
Loose SFUDA
SHOT [30] 42.4 19.0 0.0 13.3 78.7 71.6 72.1
TENT [60] 45.1 36.0 0.1 35.9 76.1 62.0 60.5
URMDA [46] 30.8 36.2 7.7 2.6 71.1 26.2 41.1
SHOT+ELR [71] 59.4 54.0 1.2 67.0 79.9 78.3 75.9
DT-ST [75] 63.1 59.8 7.6 72.9 81.0 79.2 78.2
TTYD (ours) 64.5 61.0 10.4 74.5 80.9 81.6 78.8
UDA methods with src data and (for CoSMix) parameters
CoSMix† [49] 65.2 60.3 24.1 66.4 80.4 81.4 78.3
SALUDA† [38] 65.8 59.0 20.5 70.6 82.6 81.4 81.0
Table 12: Classwise results for NS→SK . † from [38].
10
NS→ (%S IK oU10
) %
mIoU
Car
Bicycle Motorcycl Tre
uck
Otherve Phi ec dl ee strian Driveabl Sie ds eu wrf a. lk Terrain Vegetation
Strict SFUDA
Source-only 34.4 77.5 8.8 18.3 5.7 4.6 52.0 38.8 25.6 29.7 83.2
AdaBN [29] 39.9 80.8 14.5 16.7 8.6 3.8 23.8 75.0 38.9 52.9 84.0
PTBN [41] 39.4 80.0 14.7 27.0 7.3 5.5 23.2 71.3 35.4 48.8 80.6
MeanBN [38] 41.7 87.0 17.6 29.6 12.1 4.4 43.8 61.3 33.3 40.2 87.5
TTYD (ours) 44.5 87.4 7.8 30.1 16.6 8.3 50.1 71.9 33.2 51.9 87.3
core
Loose SFUDA
SHOT[30] 34.9 90.2 1.2 8.6 20.9 6.2 1.2 68.9 19.0 60.4 72.3
TENT[60] 37.9 58.4 0.1 4.6 43.1 10.2 41.6 66.1 20.3 57.8 76.4
URMDA[46] 29.4 72.0 1.4 3.4 3.3 3.1 18.3 36.4 36.8 41.4 78.0
SHOT+ELR[71] 40.5 90.1 2.8 18.2 16.2 10.6 44.9 69.3 15.8 51.2 86.1
DT-ST [75] 35.6 88.6 0.0 26.3 9.1 4.1 54.9 39.9 17.2 29.2 87.2
TTYD(ours) 45.4 92.4 0.0 37.0 26.9 2.1 49.0 72.8 27.7 56.3 89.7
UDA methods with source data and (for CoSMix) hyperparameters
CoSMix† [49] 38.3 77.1 10.4 20.0 15.2 6.6 51.0 52.1 31.8 34.5 84.8
SALUDA† [38] 46.2 89.8 13.2 26.2 15.3 7.0 37.6 79.0 50.4 55.0 88.3TTYD: Towards Stable and Robust 3D SFUDA 27
Table 13: Classwise results for SL→SK . † from [38].
19
SL→ (%SK Io1 U9
) %
mIoU Car Bicycle Motorcy Tc rle uck Other Pv ee dh ei sc tl re ia Bin cyclist Motor Rc oy acl dist Parking Sidewalk Other Bug ir lo diu nn gd Fence Vegetati To rn unk Terrain Pole Trafficsign
StrictSFUDA
Source-only 22.3 40.7 7.6 9.6 1.5 1.7 21.0 47.1 1.6 21.9 4.7 34.0 0.0 36.3 22.2 62.3 28.3 48.5 28.8 5.6
AdaBN[29] 24.6 64.2 8.5 9.1 2.9 3.3 20.8 27.0 0.4 56.5 6.8 30.5 0.0 64.9 17.8 59.2 19.2 36.6 28.0 11.5
PTBN[41] 22.4 53.5 6.5 11.2 4.7 3.5 18.8 30.4 0.3 52.4 3.9 33.2 0.0 58.5 14.4 45.3 20.2 32.7 25.7 10.4
MeanBN[38] 26.9 59.6 9.1 9.8 2.4 3.1 23.6 37.3 1.2 42.5 6.8 34.0 0.1 60.2 28.8 68.9 29.3 42.3 38.0 14.5
TTYDcore(ours) 28.2 63.9 11.1 11.0 3.6 3.0 26.5 33.0 1.763.2 5.9 32.3 0.267.4 19.1 72.630.5 35.4 40.915.2
LooseSFUDA
SHOT[30] 18.4 49.5 1.0 2.1 4.5 4.2 13.7 8.0 0.5 60.0 4.2 24.0 0.5 46.5 16.7 38.0 22.8 15.1 37.4 0.9
TENT[60] 24.5 57.8 3.3 9.5 12.4 2.5 11.7 20.3 0.0 52.0 0.3 34.2 0.0 60.8 15.6 66.9 29.9 44.4 40.6 3.5
URMDA[46] 25.4 52.0 3.3 6.3 1.3 1.1 14.7 52.0 1.2 26.2 5.6 37.0 0.1 46.3 32.3 65.3 35.8 51.6 45.8 4.7
SHOT+ELR[71] 27.1 56.7 4.1 10.0 3.3 1.7 31.4 32.7 1.0 62.1 2.8 33.7 0.1 64.9 7.6 71.9 32.3 40.0 46.2 12.2
DT-ST[75] 23.5 34.9 2.1 10.9 2.3 2.0 29.2 66.7 1.0 20.6 3.2 35.1 0.0 27.8 5.4 60.4 30.7 52.9 48.8 12.6
TTYD(ours) 32.4 77.0 5.0 12.8 8.7 2.940.0 43.6 1.267.4 5.5 34.8 0.0 70.8 8.4 77.540.4 38.6 52.828.1
UDAmethodswithsourcedataand(forCoSMix)hyperparameters
CoSMix†[49] 28.0 63.9 5.6 11.4 5.7 7.9 20.0 40.3 3.8 56.4 13.2 37.9 0.1 42.6 29.5 66.9 27.9 29.6 46.0 22.5
SALUDA†[38] 31.2 65.4 7.5 13.6 3.2 5.9 23.9 43.7 1.7 52.9 11.6 39.8 0.3 67.8 28.2 74.2 37.6 43.6 47.5 22.7
Table 14: Classwise results for SL→SP .† from[38]andusesavoxelsizeof5cm.
13
n
SL→ (%SP Io1 U3
) %
mIoU Person Rider Car Trunk Plants Trafficsi Pg oln e Garbage Buc ila ding Cone Fence Bike Ground
StrictSFUDA
Source-only 25.6 43.2 31.4 22.5 20.8 65.8 1.0 4.5 14.9 53.9 7.0 21.5 3.0 43.4
AdaBN[29] 25.4 38.4 17.8 22.4 23.6 55.9 13.0 7.8 8.8 61.1 6.9 14.9 9.3 50.9
PTBN[41] 23.7 36.3 20.4 27.0 19.9 43.4 10.6 6.8 8.2 58.8 5.2 15.3 8.5 47.7
MeanBN[38] 27.7 38.9 23.2 22.5 26.2 69.5 6.1 7.0 15.6 63.2 9.4 21.2 5.2 52.2
TTYDcore (ours) 35.9 46.1 37.2 43.5 31.3 71.3 4.8 20.5 21.8 69.1 11.5 25.4 4.3 79.9
LooseSFUDA
SHOT[30] 21.7 31.1 5.7 11.8 32.9 37.1 8.0 18.5 4.6 52.3 6.2 18.1 0.1 55.3
TENT[60] 28.3 39.1 30.0 33.4 20.0 63.3 0.0 21.4 3.0 60.0 16.8 31.6 0.7 48.7
URMDA[46] 24.5 42.0 37.7 50.3 23.5 46.1 0.0 21.5 0.0 41.9 0.0 51.7 0.0 3.4
SHOT+ELR[71] 36.9 59.8 29.1 47.7 30.4 71.1 1.3 23.1 12.1 70.9 18.4 34.4 0.4 81.9
DT-ST[75] 36.8 64.1 57.1 47.3 21.5 65.3 3.6 23.6 28.3 58.5 6.2 35.1 0.3 67.1
TTYD(ours) 39.1 64.1 54.8 48.9 27.8 73.0 8.8 29.4 14.1 73.6 5.9 36.8 0.5 70.7
UDAmethodswithsourcedataand(forCoSMix)hyperparameters
CoSMix† [49] 40.8 50.9 54.5 34.9 33.6 71.1 19.4 35.6 26.8 65.2 30.4 24.0 6.0 78.5
SALUDA† [38] 42.9 59.9 54.6 59.2 33.7 69.8 14.9 40.9 30.8 64.5 26.2 22.1 2.7 78.028 B. Michele et al.
Table 15: Classwise results for NS→WO .
10
NS→ (%W IoO U1 )0
%
mIoU
Car
Bicycle Motorcy Tc rle uck Otherve Pehi dc el se trian Drivea Sb ile des wur af l. k Walkable Vegetation
Strict SFUDA
Source-only 46.1 72.2 6.2 14.0 24.9 24.5 68.1 70.8 47.8 43.8 88.6
AdaBN [29] 47.7 70.5 8.9 9.1 27.6 33.2 58.8 82.2 51.5 46.4 89.0
PTBN [41] 42.3 65.1 4.5 7.7 21.7 22.1 51.8 80.3 46.4 40.4 83.3
MeanBN [38] 50.3 75.2 9.6 12.8 30.0 37.2 67.5 78.5 52.2 48.9 91.5
TTYD (ours) 51.4 77.5 7.6 17.3 27.5 36.1 74.2 80.3 53.8 48.4 91.1
core
Loose SFUDA
SHOT [30] 37.3 56.2 0.8 7.6 15.2 21.7 36.9 61.7 45.9 41.1 85.7
TENT [60] 40.4 56.5 0.4 10.9 18.3 23.8 52.1 82.2 47.8 35.5 76.2
URMDA [46] 42.7 71.9 1.7 1.3 26.2 20.6 60.2 64.9 52.1 41.5 86.5
SHOT+ELR [71] 49.5 79.5 2.2 24.0 26.2 29.0 67.6 76.5 51.9 50.0 88.1
DT-ST [75] 51.8 81.0 6.8 18.9 33.1 42.9 77.6 72.1 47.5 45.7 92.7
TTYD (ours) 55.5 83.1 8.4 20.4 33.1 46.0 79.5 82.2 55.4 53.0 93.5
Table 16: Classwise results for NS→PD .
8
d
n
NS→ (%P ID
oU8
) %
mIoU 2-wheeled Pedestrian
Drivea
Sibl de eg wr
alo ku
Othergr Mo au nn
md
ade Vegetati 4o -n wheeled
Strict SFUDA
Source-only 60.4 27.6 64.2 71.6 45.1 24.2 88.1 75.0 87.2
AdaBN [29] 59.6 31.3 51.6 77.3 44.5 28.5 86.0 73.1 84.3
PTBN [41] 60.2 32.4 52.3 76.1 46.0 28.3 86.9 74.1 85.6
MeanBN [38] 61.3 31.3 61.6 75.0 44.8 27.0 87.8 75.0 87.5
TTYD (ours) 63.3 28.8 65.3 78.1 49.0 30.5 88.2 76.2 90.4
core
Loose SFUDA
SHOT [30] 43.7 0.7 38.4 27.7 40.1 17.1 84.5 67.8 72.5
TENT [60] 59.1 14.8 50.5 83.6 50.8 25.8 85.5 72.7 89.2
URMDA [46] 56.9 17.0 62.2 68.9 40.1 22.6 88.5 71.9 84.9
SHOT+ELR [71] 60.9 15.2 58.5 78.1 48.3 30.0 88.8 77.4 90.8
DT-ST [75] 62.5 32.7 64.2 75.9 43.8 26.6 89.1 77.5 90.4
TTYD (ours) 65.7 35.2 64.2 81.7 49.5 35.9 88.4 78.3 92.9TTYD: Towards Stable and Robust 3D SFUDA 29
J Qualitative results
Methods with no degradation prevention. We illustrate in Fig. 3 the
performance degradation when training is too long for TENT [60], SHOT [30]
and URMDA [46]. Note that, for these methods, we select the best trained model
by looking at the ground-truth target validation set. It highlights the difference
between what can be achieved in theory and what actually happens if training is
not stopped with a criterion like ours.
One can observe that the TENT model, which estimates the normalization
parameters of the batch norm layers on the target dataset, starts from a better
source-only model, although it has not been trained on target data yet. After 20k
iterations, the motorcycle, the truck, and part of the vegetation are not correctly
classified, although they were correctly classified in the source-only model. A
similar degradation behavior can be seen for the SHOT method. The URMDA
method does not perform as well as the others. After 20k iterations, it also shows
a significant degradation with respect to both the source-only starting point and
the best model: while the source-only model correctly segments the vegetation
and the truck, the final model incorrectly labels part of the vegetation using
various other classes, and wrongly predicts the class on the top of the truck.
Our stopping criterion. In Fig. 4, we show qualitative results for each domain
adaptation setting: ground-truth labels (GT), the source-only result, the result
obtained by our training scheme with TTYD , and the result obtained after
stop
20kiterations.Theserepresentationshighlightthatthestoppingcriterionachieves
a significant, qualitatively visible improvement.
As can be seen, the improvements of our training scheme in combination with
our stopping criterion over the source-only model are dominated by changes in
the “Road”, “Sidewalk”, and “Terrain” classes. If the training is pushed to 20k
iterations,theselargeclassesarelittledegraded,whileobjectsofotherclasseslike
cars or pedestrians can be totally misclassified. One exception is the NS→SP
6
setting, where we can observe a total collapse into a binary classification after
training for 20k iterations.30 B. Michele et al.
GT Src.-only (start point) Best model After 20k iterations
GT Src.-only (start point) Best model After 20k iterations
GT Src.-only (start point) Best model After 20k iterations
Fig.3:ExamplesofresultswithTENT[60],SHOT[30]andURMDA[46]onNS→SK :
10
ground truth (GT), initial model trained only on source data, best model as upper
bound (using ground-truth knowledge of the target validation set), and “full” training
for 20k iterations. “Ignore” points are removed for a better visualisation. Notable errors
due to degradation are marked with a dashed rectangle.
TNET
TOHS
ADMRUTTYD: Towards Stable and Robust 3D SFUDA 31
GT Src.-only (start point) TTYD After 20k iterations
stop
GT Src.-only (start point) TTYD After 20k iterations
stop
GT Src.-only (start point) TTYD After 20k iterations
stop
GT Src.-only (start point) TTYD After 20k iterations
stop
GT Src.-only (start point) TTYD After 20k iterations
stop
GT Src.-only (start point) TTYD After 20k iterations
stop
Fig.4: Examples of results with TTYD : ground truth (GT), initial model trained
stop
only on source data, training with our training scheme when using our stopping
criterion, and “full” training for 20k iterations. “Ignore” points are removed for a better
visualisation. Notable errors due to degradation are marked with a dashed rectangle.
Due to different class mappings, coloring can vary between the different settings.
KS→SN
KS→LS
PS→LS
PS→SN
OW→SN
DP→SN
01
91
31
6
01
832 B. Michele et al.
K Datasets and class mappings
Tab. 17 summarizes the main characteristics of the datasets we used in experi-
ments, including details about the lidars used for data capture. As can be seen,
there is a lot of variety among the lidar sensors, not counting variations that
are not even reported here, such as sensor height or laser range. This sensor gap
yields significant dissimilarities at point cloud level. Considering on top of that
the geographical variety of the driving landscapes over 3 continents, including
synthetic scenery, the total domain gap between most of these datasets can be
considered as severe.
Notethatthenumberofclasseswereportisthenumberusedforthestandard
benchmarking of semantic segmentation on each dataset, which may be lower
than the number of finer-grained classes actually annotated in the ground-truth
data. Also, for SemanticKITTI, the class of a moving object is merged with the
class of the same static object.
In Tabs. 18 to 23, we provide the exact class mapping. Unnamed classes are
mapped to ‘Ignore’.
Table 17: Datasets used in our domain adaptation experiments. For each dataset, we
provide: abbreviation in the paper, main reference, lidar sensor used for data capture,
numberofbeams,verticalfieldofview(V.FoV),verticalresolution(V.res.),horizontal
resolution (H.res.), number of classes used for standard benchmarking (which may be
lower than the number of finer-grained actually annotated classes), number of frames
for training and/or testing, and region of the world where the data was captured. The
V.FoV of the Pandora (Pandar40) lidar is variable, denser when closer to horizontality:
0.33°fortheFoV-6°to+2°,and1°fortheFoV-16°to-6°and+2°to+7°.TheV.FoV
of the Pandar64 is even more variable: 0.167° (-6° to +2°), 1° (-14° to -6°, +2° to +3°),
2° (+3° to +5°), 3° (+5° to +11°), 4° (+11° to +15°), 5° (-19° to -14°), 6° (-25° to -19°).
Dataset Ref.Lidar Beams V.FoV V.res.H.res.Classes Train Test Regionoftheworld
nuScenes (NS) [3]VelodyneHDL-32E 32 -30.7°to+10.7°1.33° 0.33° 16 28,130 –Boston,Singapore
SynLiDAR (SL) [64]synthetic 64 -25.0°to+3.0° 22 19,840 –3Dexpertsusing
UnrealEngine4
SemanticPOSS(SP) [42]Pandora(Pandar40) 40 -16.0°to+7.0°0.20° 0,33°/1° 14 2,484 499PekingUniversity
(manydynamicobjects)
SemanticKITTI(SK) [2]VelodyneHDL-64E 64 -24.8°to+2.0°0.42° 0.18° 19 19,1304,071Karlsruhe
Pandaset (PD) [65]Pandar64 64 -25.0°to+15.0°0.17° 0.20°/6° 37 3,8002,280SanFrancisco,
ElCaminoReal
WaymoOpen (WO)[13]LaserBearHoneycomb 64 -17.6°to+2.4° 23 23,6915,976Phoenix,SanFrancisco,
MountainViewTTYD: Towards Stable and Robust 3D SFUDA 33
Table 18: Class mapping Table 19: Class mapping
for NS→SK (from [70]). for NS→SP (from [52]).
10 6
nuScenes NS→SK 10 SemanticKITTI nuScenes NS→SP 6 SemanticPOSS
Car Car Car Pedestrian Person Person
Bicycle Bicycle Bicycle Bicycle,Motorcycle Bike Rider,Bike
Motorcycle Motorcycle Motorcycle Car,Bus,
Truck Truck Truck Constrictionvehicle, Car Car
Trailer,Truck
Construction Other-vehicle,
vehicle,Bus Othervehicle Bus Driveablesurface, Ground Ground
Otherflat,
Pedestrian Pedestrian Person Sidewalk,Terrain
Driveable Driveable Road, Vegetation Vegetation Plants
Surface surface LanP ear mki an rg k, ing Barrier, Trafficsign,Pole,
Manmade, ManmadeGarbagecan,Building,
Sidewalk Sidewalk Sidewalk Trafficcone Cone/Stone,Fence
Terrain Terrain Terrain
Vegetation Vegetation Vegetation,Trunk
Table 20: Class mapping Table 21: Class mapping
for NS→WO (from [25]). for NS→PD (from [52]).
10 8
nuScenes NS→WO
10
WaymoOpen nuScenes NS→PD
8
Pandaset
Pedestrian Person Person Bicycle,Motorcycle,
Bicycle, Motorizedscooter
Bicycle,Motorcycle Bike Rider,Bike 2-wheeled
Motorcycle Pedicab,
Car,Bus, PersonalMobilityDevice
ConstrictionVehicle, Car Car
Pedestrian,
Trailer,Truck Pedestrian Pedestrian
Pedestrianw/objects
DriveableSurface,
Ground Ground Driveway,Road,
OtherFlat, Driveableground Driveableground
Roadmarking
Sidewalk,Terrain
Sidewalk Sidewalk Sidewalk
Vegetation Vegetation Vegetation,Plant
Otherflat,Terrain Otherground Ground
Barrier, TrafficSign,Pole,
Manmade, Manmade GarbageCan,Building, Building,Cones,
TrafficCone Cone/Stone,Fence Barrier, ConstructionBarriers/Signs,
Manmade, Manmade Otherstaticobject,
Trafficcone Pylons,RoadBarriers,
Rollingcontainers,Signs
Vegetation Vegetation Vegetation
Car,Constructionvehicle,
Bus,Car, Emergencyvehicle,
Constructionvehicle, 4-wheeled Bus,Towedobject,
Trailer,Truck Truck(allkindsof)
Uncommonvehicle34 B. Michele et al.
Table 22: Class mapping for Table 23: Class mapping for
SL→SK (from [49]). SL→SP (from [49]).
19 13
SynLiDAR SL→SK 19&SemanticKITTI SynLidar SL→SP 13&SemanticPOSS
Car Car Person Person
Bicycle Bicycle Bicyclist,Motorcyclist Rider
Motorcycle Motorcycle Car Car
Truck Truck Trunk Trunk
Bus,Othervehicle Othervehicle Vegetation Plants
Person Pedestrian Trafficsign Trafficsign
Bicyclist Bicyclist Pole Pole
Motorcyclist Motorcyclist Garbagecan Garbagecan
Road Road Building Building
Parking Parking Traffic-cone Cone
Sidewalk Sidewalk Fence Fence
Otherground Otherground Bicycle Bike
Building Building Road Ground
Fence Fence
Vegetation Vegetation
Trunk Trunk
Terrain Terrain
Pole Pole
Trafficsign Trafficsign