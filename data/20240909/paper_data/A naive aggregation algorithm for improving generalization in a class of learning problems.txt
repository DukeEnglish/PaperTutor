ANAIVEAGGREGATIONALGORITHMFORIMPROVINGGENERALIZATION
INACLASSOFLEARNINGPROBLEMS
GETACHEWK.BEFEKADU
Abstract.Inthisbriefpaper,wepresentanaiveaggregationalgorithmforatypicallearningproblemwithex-
pertadvicesetting,inwhichthetaskofimprovinggeneralization,i.e.,modelvalidation,isembeddedinthelearning
processasasequentialdecision-makingproblem. Inparticular,weconsideraclassoflearningproblemofpoint
estimationsformodelinghigh-dimensionalnonlinearfunctions, whereagroupofexpertsupdatetheirparameter
estimatesusingthediscrete-timeversionofgradientsystems,withsmalladditivenoiseterm,guidedbythecorre-
spondingsubsampledatasetsobtainedfromtheoriginaldataset. Here,ourmainobjectiveistoprovideconditions
underwhichsuchanalgorithmwillsequentiallydetermineasetofmixingdistributionstrategiesusedforaggregat-
ingtheexperts’estimatesthatultimatelyleadingtoanoptimalparameterestimate,i.e.,asaconsensussolutionfor
allexperts,whichisbetterthananyindividualexpert’sestimateintermsofimprovedgeneralizationorlearningper-
formances.Finally,aspartofthiswork,wepresentsomenumericalresultsforatypicalcaseofnonlinearregression
problem.
Keywords.Decision-makingproblem,expertsystems,generalization,learningproblem,modelingofnonlinear
functions,pointestimations,randomperturbations
1. Introduction. Themainobjectiveofthisbriefpaperistopresentanaggregational-
gorithmforatypicallearningproblemwithexpertadvicesetting,inwhichthetaskofimprov-
ing generalization, i.e., model validation, is embedded in the learning process as a sequen-
tial decision-making problem with dynamic allocation scenarios. In particular, the learning
framework that we propose here can be viewed as an extension for enhancing the learning
performance in a typical empirical risk minimization-based learning problem of point esti-
mations for modeling of high-dimensional nonlinear functions, when we are dealing with
largedatasetsasaninstanceofdivide-and-conquerparadigm. Tobemorespecific,wehave
agroupofexpertsthatupdatetheirparameterestimatesusingadiscrete-timeversionofgra-
dientsystemswithsmalladditivenoiseterm,guidedbyasetofsubsampledatasetsobtained
fromtheoriginaldatasetbymeansofbootstrappingwith/withoutreplacementorotherrelated
resampling-based techniques. Here, our interest is to provide conditions under which such
an algorithm will determine a set of mixing distribution strategies used for aggregating the
experts’parameterestimatesthatultimatelyleadingtoanoptimalparameterestimate,i.e.,as
aconsensussolutionforallexperts, whichisbetterthananyindividualexpert’sestimatein
termsofimprovedgeneralizationorlearningperformances.
This brief paper is organized as follows. In Section 2, we present our main results, where
we provide an aggregation algorithm that can be viewed as an extension for enhancing the
learning performance and improving generalization in a typical learning problem with the
help of a group of experts. In Section 3, we present numerical results for a typical case of
nonlinearregressionproblem,andSection4containssomeconcludingremarks.
2. Main results. In this section, we present a learning framework with expert advice
setting, that can be viewed as an extension for enhancing the learning performances in a
typical empirical risk minimization-based learning problem, where the task of improving
generalizationisembeddedinthelearningprocessasasequentialdecision-makingproblem
withdynamicallocationscenarios.
In what follows, the learning framework that we propose consists of (K + 1) subsample
datasets of size m (where m is much less than the total dataset points d) that are generated
1
4202
peS
6
]GL.sc[
1v25340.9042:viXrafrom a given original dataset Zd = (cid:8) (x ,y )(cid:9)d by means of bootstrapping with/without
i i i=1
replacementorotherrelatedresampling-basedtechniques,i.e.,
Zˆ(k) =(cid:8) (xˆ(k),yˆ(k))(cid:9)m , (2.1)
i i i=1
where (xˆ(k),yˆ(k)) ∈ Zd with i ∈ {1, 2,...,d} and k = 1,2,...,K + 1. Moreover,
i i
we use the datasets Zˆ(k), k = 1, 2,...,K, for parameter estimation, i.e., model training
purpose, corresponding to each of the K experts, (i.e., a group of K experts, numbered
k = 1,2,...,K), while the last dataset Zˆ(K+1) = (cid:8) (xˆ(k),yˆ(k))(cid:9)m will be used for im-
i i i=1
provinggeneralizationinthelearningprocess(i.e.,howwelleachexpert’sestimateperforms
aspartofthemodelvalidationprocess). Here,eachexpertistaskedtosearchforaparameter
θ ∈Γ,fromafinite-dimensionalparameterspaceRp,suchthatthefunctionh (x)∈H,i.e.,
θ
fromagivenclassofhypothesisfunctionspaceH,describesbestthecorrespondingdataset
usedduringthemodeltrainingandvalidationprocesses.
In terms of mathematical optimization construct, searching for an optimal parameter θ∗ ∈
Γ ⊂ Rp can be associated with a steady-state solution to the following gradient system,
whosetime-evolutionisguidedbythecorrespondingsubsampleddatasetZˆ(k)
θ˙(k)(t)=−∇J (θ(k)(t),Zˆ(k)), θ(k)(0)=θ , k =1,2,...,K, (2.2)
k 0
withJ (θ(k),Zˆ(k)) = 1 (cid:80)m ℓ(cid:0) h (xˆ(k)),yˆ(k)(cid:1) , whereℓisasuitablelossfunctionthat
k m i=1 θ(k) i i
quantifiesthelack-of-fitbetweenthemodel(e.g.,see[1]forgeneraldiscussionsonlearning
via dynamical systems). Here, we specifically allow each expert to update its parameter
estimateusinga discrete-timeversionoftheaboverelateddifferentialequationswith small
additivenoiseterm,i.e.,
dΘ(k) =−∇J (Θ(k),Zˆ(k))dt+(cid:16) ϵ/(cid:112) log(t+2)(cid:17) I dW(k), Θ(k) =θ(k),
t k t p t 0 0
k =1,2,...,K, (2.3)
where ϵ > 0 is very small positive number, I is a p × p identity matrix, and W(k) is a
p t
p-dimensionalstandardWienerprocess.
Moreover, we remark that if ∇J (θ,Zˆ(k)), for each k ∈ {= 1,2,...,K}, is uniformly
k
Lipschitzandfurthersatisfiesthefollowinggrowthcondition
(cid:12) (cid:12)∇J k(θ,Zˆ(k))(cid:12) (cid:12)2 ≤L Lip(cid:0) 1+|θ|2(cid:1) , ∀θ ∈Γ⊂Rp, (2.4)
forsomeconstantL >0.Then,thedistributionofΘ(k)convergestothelimitoftheGibbs
Lip t
(cid:16) (cid:17)
densitiesproportionaltoexp −J (θ,Zˆ(k))/T astheabsolutetemperatureT tendstozero,
k
i.e.,
(cid:16) (cid:112) (cid:17)2
T = ϵ/ log(t+2) →0 as t→∞, (2.5)
whichisexpectedtobeconcentratedattheglobalminimumθ∗ ∈Γ⊂RpofJ (θ,Zˆ(k)).
k
Notethat,foranequidistantdiscretizationtimeδ =τ −τ =T/N,n=0,1,2,...,N−
n+1 n
1, with 0 = τ < τ < ... < τ < ... < τ = T, of the time interval [0,T], the Euler-
0 1 n N
Maruyamaapproximationforthecontinuous-timestochasticprocessesΘ(k) = (cid:8) Θ(k), 0 ≤
t
2(cid:9)
t≤T ,k =1,2,...,K,satisfyingthefollowingiterativescheme
Θ(k) =Θ(k)−δ∇J (Θ(k),Zˆ(k))+(cid:16) ϵ/(cid:112) log(τ +2)(cid:17) I ∆W(k), Θ(k) =θ(k),
n+1 n k n n+1 p n 0 0
n=0,1,...,N −1, (2.6)
where we have used the notation Θ(k) = Θ(k) and the increments ∆W(k) = (W(k) −
n τn n n+1
W(k))areindependentGaussianrandomvariableswithmeanE(∆W(k)) = 0andvariance
n n
Var(∆W(k))=δI (e.g.,see[2]or[3]).
n p
Then,weformalizeourdecision-makingparadigmwithdynamicallocationscenariosthatcan
beviewedasanextensionforenhancingthelearningperformanceaswellasimprovinggener-
alizationinthelearningframework. Here,ateachiterationtimestepn=0,1,2,...,N −1,
we decide on a mixing distribution π = (π (1),π (2),...,π (K)) over strategies, with
n n n n
π (k) > 0, for all k ∈ {1,2,...,K}, and sum to one, i.e.,
(cid:80)K
π (k) = 1, that will
n k=1 n
be used for dynamically apportioning the weighting coefficients with respect to the current
parameterestimates. Moreover,foreachexpert’scurrentparameterestimate,weassociatea
risk measure r (k) ∈ [0,1], based on an exponential function, which is determined by the
n
currentestimateΘ(k)togetherwiththevalidatingdatasetZˆ(K+1),i.e.,
n
r
(k)=1−exp(cid:16) −γ (cid:88)m ℓ(cid:16)
h
(xˆ(K+1)),yˆ(K+1)(cid:17)(cid:17)
, n=0,1,2,...,N −1,
n m i=1 Θ( nk) i i
γ >0. (2.7)
REMARK1. Notethattheriskmeasurer n(k),ateachiterationtimestepn=0,1,2,...,N−
1,issimplyanempiricalvaluebetween0and1foranappropriatelychosenlossfunctionℓ
thatquantifiesthelack-of-fitbetweenthemodel.
Notethattheaverage(ormixture)lossstrategiesateachiterationtimeisgivenby
(cid:88)K
L = π (k)r (k), n=0,1,2,...,N −1. (2.8)
n n n
k=1
Then,ourobjectiveistopresentanaggregationalgorithm,thathasadecision-theoreticmin-
imization interpretation with dynamic allocation scenarios, which also guarantees an upper
boundforthetotaloverallmixturelossL,i.e.,
(cid:88)N−1
min→L= L
n
n=0
(cid:88)N−1(cid:88)K
= π (k)r (k). (2.9)
n n
n=0 k=1
Moreover,suchanaggregationalgorithmalsoensuresthefollowingadditionalproperties:
(i) r (k)tendsto0asn→∞forallk ∈{1,2,...,K}.
n
(ii) Θ¯ =(cid:80)K π (k)Θ(k)tendstotheoptimalparameterestimateθ∗ ∈Γ⊂Rp.
N k=1 N N
In order to accomplish the above properties, we use a simple dynamic allocation strategy
coupled with an iterative update scheme for computing the mixing distribution strategies
π (k),foreachk =1,2,...,K,i.e.,
n
ω (k)
π (k)= n , (2.10)
n (cid:80)K
ω (k)
k=1 n
3whiletheweightingcoefficientsareupdatedaccordingto
ω (k)=ω (k)exp(r (k)log(β)), β ∈(0,1), (2.11)
n+1 n n
forn=0,1,2,...,N −1.
REMARK 2. Notethatthemixingdistributionstrategiesπ n(k),fork = 1,2,...,K,canbe
interpretedasameasureofqualitycorrespondingtoeachofexperts’currentparameteresti-
mates. Moreover,wecanassigntheinitialweightsω (k),fork =1,2,...,K,arbitraryval-
0
ues,butmustbenonnegativeandsumtoone(seetheAlgorithmpartinthissection).
Then, we state the following proposition that provides an upper bound for the total overall
mixtureloss.
PROPOSITION 2.1. Let L
n
=
(cid:80)K
k=1π n(k)r n(k), n = 0,1,2,...,N −1, be a sequence
oflossesassociatedwiththemixingdistributionstrategiesπ =(π (1),π (2),...,π (K))
n n n n
and risk measures r (k), for k = 1,2,...,K. Then, the total overall mixture loss L, i.e.,
n
L=(cid:80)N−1(cid:80)K
π (k)r (k),satisfiesthefollowingupperboundcondition
n=0 k=1 n n
(cid:18) (cid:19)
1 (cid:88)K
L≤− log ω (k) . (2.12)
1−β k=1 N
Proof. Note that for any β ∈ (0,1) and r (k) ∈ [0,1] for k = 1,2,...,K, we have the
n
followinginequalityrelation
exp(r (k)log(β))≤1−(1−β)r (k), (2.13)
n n
duetotheconvexityargumentforexp(r n(k)log(β)) ≡ βrn(k) ≤ 1−(1−β)r n(k). Then,
ifwecombineEquations(2.10)and(2.11),wewillhave
(cid:88)K (cid:88)K
ω (k)= ω (k)exp(r (k)log(β))
n+1 n n
k=1 k=1
(cid:88)K
≤ ω (k)(1−(1−β)r (k))
n n
k=1
(cid:88)K (cid:88)K
= ω (k)−(1−β) ω (k)r (k)
n n n
k=1 k=1
(cid:18) (cid:19)(cid:18) (cid:19)
(cid:88)K (cid:88)K
= ω (k) 1−(1−β) π (k)r (k) . (2.14)
n n n
k=1 k=1
Moreover,ifweapplyrepeatedlyforn = 0,1,2,...,N −1totheaboveequation,thenwe
have
(cid:18) (cid:19)
(cid:88)K (cid:89)N−1 (cid:88)K
ω (k)≤ 1−(1−β) π (k)r (k)
N n n
k=1 n=0 k=1
(cid:18) (cid:19)
(cid:88)N−1(cid:88)K
≤exp −(1−β) π (k)r (k) . (2.15)
n n
n=0 k=1
Noticethat(duetotheinequalityrelation1+t ≤ exp(t)forallt)theright-handsideofthe
aboveequationsatisfiesthefollowinginequality
(cid:18) (cid:19)
(cid:88)N−1(cid:88)K (cid:88)N−1(cid:88)K
1−(1−β) π (k)r (k)≤exp −(1−β) π (k)r (k) ,
n n n n
n=0 k=1 n=0 k=1
(2.16)
4thatfurthergivesusthefollowingresult
(cid:18) (cid:19)
(cid:88)K (cid:88)N−1(cid:88)K
ω (k)≤exp −(1−β) π (k)r (k) . (2.17)
N n n
k=1 n=0 k=1
Hence,thestatementinthepropositionfollowsimmediately.
NotethatthedynamicallocationstrategyinEquation(2.9)togetherwiththeiterativeupdate
schemeinEquation(2.9)provideconditionsunderwhichsuchanalgorithmdeterminesaset
of mixing distribution strategies used for aggregating the experts’ estimates that ultimately
leadingtoanoptimalparameterestimate,i.e.,asaconsensussolutionforallexperts,which
isbetterthananyindividualexpert’sestimateintermsofimprovedgeneralizationorlearning
performances.
Here, it is worth remarking that the risk measure r (k) tends to 0 as n → ∞ for all k ∈
n
{1,2,...,K}. Moreover,ifwealloweachexperttoupdateitsnextparameterestimatewith
amodifiedinitialconditionΘ¯ =(cid:80)K π (k)Θ(k) (i.e.,theaveragedvaluefortheexperts’
n k=1 n n
currentestimates)asfollows
Θ(k) =Θ¯ −δ∇J (Θ¯ ,Zˆ(k))+(cid:16) ϵ/(cid:112) log(τ +2)(cid:17) I ∆W(k). (2.18)
n+1 n k n n+1 p n
Then,Θ¯ =(cid:80)K π (k)Θ(k)tendstotheoptimalparameterestimateθ∗ ∈Γ⊂Rp.
N k=1 N N
Inwhatfollows, weprovideouralgorithmthatimplementssuchanaggregationschemefor
improvinggeneralizationinatypicalclassoflearningproblems.
ALGORITHM:ImprovingGeneralizationinaClassofLearningProblems
Input: TheoriginaldatasetZd =(cid:8) (xi,yi)(cid:9)d i=1;K+1numberofsubsampleddatasets;msubsample
datasize. Then,bymeansofbootstrappingtechniquewith/withoutreplacement,generateK+1
subsampledatasets:
Zˆ(k)=(cid:8) (xˆ(k),yˆ(k))(cid:9)m
, k=1,2,...,K+1,
i i i=1
with(xˆ i(k),yˆ i(k)) ∈ Zdandi ∈ {1,2,...,d};anequidistantdiscretizationtimeδ = τn+1−
τn =T/N,forn=0,1,2,...,N−1,with0=τ0 <τ1 <...<τn <...<τN =T,of
thetimeinterval[0,T];γ>0andβ∈(0,1).
0. Initialize:Startwithn=0,andsetπ0(k)=ω0(k)=1/K,Θ( 0k)=θ0forallk=1,2,...,K.
1. UpdateParameterEstimates:
Θ¯ n=(cid:88)K πn(k)Θn(k)
k=1
Θ( nk +) 1=Θ¯ n−δ∇J k(Θ¯ n,Zˆ(k))+(cid:16) ϵ/(cid:112) log(τn+1+2)(cid:17) Ip∆Wn(k), k=1,2,...,K
2. UpdatetheallocationDistributionStrategiesandtheWeightingCoefficients:
i. Computetheriskmeasureassociatewitheachexperts’sestimate:
rn(k)=1−exp(cid:16) − mγ (cid:88)m i=1ℓ(cid:16) h
Θ(
nk)(xˆ( iK+1)),yˆ i(K+1)(cid:17)(cid:17)
ii. Updatetheweightingcoefficients:
ωn+1(k)=ωn(k)exp(rn(k)log(β))
iii. Updatetheallocationdistributionstrategies:
πn+1(k)=
(cid:80)K
kω =n 1+ ω1 n( +k)
1(k)
fork=1,2,...,K.
3. Incrementnby1and,thenrepeatSteps1and2untilconvergence,i.e.,∥Θ¯ n+1−Θ¯ n∥≤tol,or
n=N−1.
Output: AnoptimalparametervalueΘ¯ N =θ∗.
5Finally, it is worth mentioning that such a learning framework could be interesting to in-
vestigate from game-theoretic perspective with expert advice (e.g., see [4] and [5] for an
interestingstudyincomputerscienceliterature).
3. Numerical simulation results. In this section, we presented numerical results for
a simple nonlinear regression problem. In our simulation study, the numerical data for the
populationofParameciumcaudatum,whichisaspeciesofunicellularorganisms,grownina
nutrientmediumover24days(includingthestartingdayoftheexperiment),weredigitized
usingtheSoftware:WebPlotDigitizer[7]fromthefiguresinthepaperbyF.G.Gause[5](see
also[6]). Here,ourinterestistoestimateparametervaluesforthepopulationgrowthmodel,
ontheassumptionthatthemodelobeysthelogisticlaw,i.e.,
N N
N (t)= 0 e , θ =(N ,N ,r),
θ N +(N −N )exp(−rt) 0 e
0 e 0
whereN (t)isthenumberofParameciumcaudatumpopulationattimetin[Days],andN ,
θ 0
N andr(i.e.,θ =(N ,N ,r))aretheparameterstobeestimatedusingthedigitizeddataset
e 0 e
obtainedfromGause’spaper,i.e.,theoriginaldatasetZd = (cid:8) (t ,N )(cid:9)d ,whered = 23is
i i i=1
the total digitized dataset points, t is the time in [Days] and N is the corresponding num-
i i
berofParameciumcaudatumpopulation. Moreover,wegeneratedatotalof26subsampled
datasetsofsizem = 23fromthedigitizedoriginaldatasetbymeansofbootstrappingwith
replacement,i.e.,thedatasetsZˆ(k) =(cid:8) (tˆ(k),Nˆ(k))(cid:9)23 ,withk =1,2,...,25,willbeused
i i i=1
for model training purpose, while the last dataset Zˆ(26) = (cid:8) (tˆ(26),Nˆ(26))(cid:9)23 for general-
i i i=1
izationormodelvalidation.
Fig.3.1: Plotsfortheoriginaldatasetandthepopulationgrowthmodel.
Note that we used a simple Euler–Maruyama time discretization approximation scheme to
solve numerically the corresponding system of SDEs (cf. Equation (2.3)), with an equidis-
tancediscretizationtimeδ =1×10−5ofthetimeinterval[0,1]. Forbothmodeltrainingand
6modelvalidationprocesses,weusedtheusualquadraticlossfunction
J
(θ,Zˆ(k))=(1/23)(cid:88)23 (cid:16)
N
(tˆ(k))−Nˆ(k)(cid:17)2
, k =1,2,...,K+1,
k θ i i
i=1
thatquantifiesthelack-of-fitbetweenthemodelandthecorrespondingdatasets. Figures3.1
showsboththedigitizedoriginaldatasetfromGause’spaperandthepopulationgrowthmodel
N(t),withglobaloptimalparametervaluesN∗ =2.1070,N∗ =219.0527andr∗ =0.7427,
0 e
versustimetin[Days]onthesameplot.Inoursimulation,weusedanoiselevelofϵ=0.001,
andparametervaluesofγ =0.01andβ =0.5(i.e.,satisfyingγ >0andβ ∈(0,1),seethe
AlgorithminSection2). Here,weremarkthattheproposedlearningframeworkdetermined
theparametervaluesforN whichisclosetotheinitialexperimentalpopulationsizeattime
0
t=0,i.e.,twoParameciumcaudatumorganisms.
4. Concluding remarks. In this brief paper, we presented an algorithm that can be
viewed as an extension for enhancing the learning performances in a typical empirical risk
minimization-basedlearningproblem,wherethetaskofimprovinggeneralizationisembed-
dedinthelearningprocessasasequentialdecision-makingproblemwithdynamicallocation
scenarios.Moreover,wealsoprovidedconditionsunderwhichsuchanalgorithmsequentially
determinesasetofdistributionstrategiesusedforaggregatingacrossagroupofexperts’es-
timatesthatultimatelyleadingtoanoptimalparameterestimate,i.e.,asaconsensussolution
for all experts, which is better than any individual expert’s estimate in terms of improved
generalizationorperformances. Finally, aspartofthiswork, wepresentedsomenumerical
resultsforatypicalnonlinearregressionproblem.
REFERENCES
[1] G.K.Befekadu.Embeddinggeneralizationwithinthelearningdynamics:Anapproachbased-onsamplepath
largedeviationtheory.arXivpreprint,arXiv:2408.02167,2024.
[2] D.J.Higham.Analgorithmicintroductiontonumericalsimulationofstochasticdifferentialequations.SIAM
Rev.,43(3),525–546.2001.
[3] E.Platen.Anintroductiontonumericalmethodsforstochasticdifferentialequations.ActaNumer.,8,197–
246,1999.
[4] V.G.Vovk.Agameofpredictionwithexpertadvice.J.Comput.Syst.Sci.,56(2),153–173,1998.
[5] N.Littlestone&M.Warmuth.Theweightedmajorityalgorithm.Inf.Comput.,108(2),212–261,1994.
[6] G.F.Gause.ExperimentalanalysisofVitoVolterra’smathematicaltheoryofthestruggleforexistence.Sci-
ence,79(2036),16–17,1934.
[7] G.F.Gause.Thestruggleforexistence:Aclassicofmathematicalbiologyandecology.UnitedStates:Dover
Publications,2019.
[8] A.Rohatgi.Software: WebPlotDigitizerVersion5.May14,2024.Availableat: https://automeris.
io.
7