UI-JEPA: Towards Active Perception of User Intent through
Onscreen User Activity
YichengFu∗ RavitejaAnantha† PrabalVashisht†
StanfordUniversity Apple Apple
Stanford,CA,USA Seattle,WA,USA Seattle,WA,USA
JianpengCheng EtaiLittwin
Apple Apple
Seattle,WA,USA Cupertino,CA,USA
Abstract thatsummarizesuserinteractionhistorywithasmartdevice.How-
Generatinguserintentfromasequenceofuserinterface(UI)ac- ever,understandingUIsposessignificantchallenges.Applications
tionsisacorechallengeincomprehensiveUIunderstanding.Recent varyinvisualrepresentation,whichcanchangedynamicallybased
advancementsinmultimodallargelanguagemodels(MLLMs)have on user actions. This necessitates using cross-modal features —
ledtosubstantialprogressinthisarea,buttheirdemandsforexten- images,naturallanguage,andstructuralmetadata—tograspthe
sivemodelparameters,computingpower,andhighlatencymakes temporalrelationshipsinUIsequences.
themimpracticalforscenariosrequiringlightweight,on-device RecentadvancesinMultimodalLargeLanguageModels(MLLMs)
solutions with low latency or heightened privacy. Additionally, havemadeprogressinbuildingperceptionagentscapableofgener-
thelackofhigh-qualitydatasetshashinderedthedevelopmentof atinguserintentfromUIactionsequences.However,moststate-of-
suchlightweightmodels.Toaddressthesechallenges,wepropose the-artMLLMs[2,12]arecomputationallyintensiveandrequire
UI-JEPA,anovelframeworkthatemploysmaskingstrategiesto server-sideprocessing,leadingtoextremelyhighcosts.Givencon-
learnabstractUIembeddingsfromunlabeleddatathroughself- nectivity,latency,andprivacyconcerns,anon-devicesolutionis
supervisedlearning,combinedwithanLLMdecoderfine-tuned needed—onethatislightweightyetmaintainscomparableaccuracy.
foruserintentprediction.WealsointroducetwonewUI-grounded Whileeffortsareunderwaytoreducethesizeofthesemodels[13],
multimodaldatasets,“IntentintheWild”(IIW)and“Intentinthe
theyarestillfarfromtheoptimalsize(∼3Bparameters)needed
Tame”(IIT),designedforfew-shotandzero-shotUIunderstanding forreliableoperationonadvancedmobiledevices.Additionally,
tasks.IIWconsistsof1.7Kvideosacross219intentcategories,while thesemodelsstillrequiresubstantialdataandcomputeresources
IITcontains∼900videosacross10categories.Weestablishthefirst fortraining.
baselinesforthesedatasets,showingthatrepresentationslearned Inspiredbythesuccessofself-supervisedlearning(SSL)tech-
usingaJEPA-styleobjective,combinedwithanLLMdecoder,can niqueslikeJointEmbeddingPredictiveArchitectures(JEPA)[10]
achieveuserintentpredictionsthatmatchtheperformanceofstate- anditsvariants[3,5],weproposeUI-JEPA,alightweightvideo-to-
of-the-artlargeMLLMs,butwithsignificantlyreducedannotation textmodelspecializedforUIactivities.UI-JEPAcomprisesaJEPA-
anddeploymentresources.Measuredbyintentsimilarityscores,UI- basedencoder,trainedwithnoveltemporalmaskingstrategieson
JEPAoutperformsGPT-4TurboandClaude3.5Sonnetby10.0%and unlabeledUIvideodata,tolearnabstractfeaturerepresentations,
7.2%respectively,averagedacrosstwodatasets.Notably,UI-JEPA andanLLMdecoderthatpredictsuserintentfromthesefeatures.
accomplishestheperformancewitha50.5xreductionincomputa- Ourkeyinsight,inspiredbythePredictiveFeaturePrinciple[15],
tionalcostanda6.6ximprovementinlatencyintheIIWdataset. isthatpredictingfullymaskedframesusingunmaskedframesal-
TheseresultsunderscoretheeffectivenessofUI-JEPA,highlighting lowsthemodeltoeffectivelycapturetemporalrelationshipsand
itspotentialforlightweight,high-performanceUIunderstanding. understandtaskmeanings.Wedemonstratethatfine-tuningan
LLMdecoderconditionedonUI-JEPArepresentationsrequiresonly
Keywords afractionofthepairedvideo-textdataandcomputationalresources
asrequiredbystate-of-the-artMLLMs.Thisframeworkisparticu-
UIUnderstanding,Self-SupervisedLearning,JEPA,MLLMs
larlyvaluablewhenhigh-qualitylabeleddataarescarce.
Thelackofhigh-quality,task-specificlabeledUIdatasetshas
1 Introduction
alsohinderedthedevelopmentoflightweightMLLMsforUIun-
Astheuseofsmartdevicesfordailytasksincreases,theuserin- derstanding.Toaddressthis,weintroducetwonewmultimodal
terface(UI)remainstheprimarymediumthroughwhichhumans datasets:“IntentintheWild”and“IntentintheTame.”Weestablish
interactwithapplications,eitherdirectlyorviadialogueagents. thefirstbenchmarksonthesedatasetsusingtheUI-JEPAframe-
AccuratelyperceivingUIactionsandpredictinguserintentcan workinbothfew-shotandzero-shotsettings.Ourcontributions
significantlyenhancedialogagents,providingvaluablefeedbackon are:
thesuccessorfailureoftheiractions.Moreover,aneffectivepercep-
tionmodelcanenable“MultimodalIntentStateTracking”(MIST)
• Benchmarks:Weintroducetwonewbenchmarks,“Intent
∗WorkdoneasaninternatApple. intheWild”and“IntentintheTame,”bothevaluatedin
†EqualContributions. few-shotandzero-shotsettingsforUIunderstanding.The
4202
peS
6
]LC.sc[
1v18040.9042:viXraYichengFu,RavitejaAnantha,PrabalVashisht,JianpengCheng,andEtaiLittwin
(a)IntentintheWildDataset. (b)IntentintheTameDataset.
Figure1:3DScatterPlotsComparingBenchmarkScoreswithModelSizeandLatencyinIntentintheWildandIntentinthe
Tamedatasetrespectively:(a)therelationshipbetweenmodelsize(inbillionsofparameters),latency(inmilliseconds),and
Intentsimilarityscores;(b)thesamerelationshipbutforIntentintheTamedataset.Eachpointrepresentsadifferentmodel.
taskinvolvesgeneratinguserintentfromasequenceofUI 2.1 UIUnderstanding
actionscapturedinvideoformat.1
Variousmachinelearningmodelshavebeenproposedtoenhance
• Framework:WeproposeUI-JEPA,anovelframeworkthat UI understanding. Early efforts [4, 8] primarily focused on pre-
employsvariousmaskingstrategiestolearnabstractUI trainingtransformer-basedmodelsusinglarge-scale,unlabeledUI
embeddingsfromunlabeleddatathroughSSL. datatolearngenericfeaturerepresentationsattheUIcomponent
• Model:WepresentalightweightJEPA-tunedMLLM(JEPA- level.Otherapproaches[24]haveaugmentedmodeltrainingwith
based video encoder + fine-tuned auto-regressive head) semanticinformationandheuristicstoimproveUIdetection.How-
designedtogenerateuserintentfromUIactionsequences, ever,thesemethodsoftenfallshortinunderstandingtheconcept
showcasingtheintegrationofJEPAwithLLMdecodersfor ofataskandfailtolearncomprehensivevisualrepresentations,as
userintentprediction. theyarelimitedtoindividualUIcomponents.Someapproaches[22]
• Comparison:Wecompareourlightweightmodelagainst utilizecrawlermodelstailoredtospecifictasks,butthesemodels
state-of-the-artMLLMs[2,12],demonstratingthatourap- strugglewithscalabilityacrossalargenumberoftasksandexhibit
proachachievesparitywhileusingonlyafractionofthe limitedgeneralizationtounseentasks.Additionally,methods[23]
data,time,andcomputationalresources.SeeFigure1fora thatintegrateimageencoderswithLLMsaregenerallyconfinedto
comparison.2
basicUItasks,suchasiconrecognitionandwidgetlisting,andop-
erateonstaticimages,whichhinderstheirabilitytolearntemporal
2 RelatedWork
relationshipsandtheconceptofatask.
OurworkwithUI-JEPAisfocusedonadvancingUIunderstand- Incontrast,ourapproachprocessesvideosthatcapturesequences
ingbysignificantlyimprovinggeneralizationcapabilitiesanden- ofUIactionsduringtaskexecution.ByusingaJEPA-basedencoder,
ablingautomaticinferenceofuserintentfromUIinteractionse- welearnvideorepresentationsthroughself-supervisedlearning,
quenceswithminimalrelianceonannotation,memory,andcom- andanLLMdecodertotextualrepresentationsoftheuserintents.
putationalresourcesduringbothtrainingandinference.Byusing ThismethodcapturesnotonlythetemporaldynamicsofUIin-
JEPA,westrivetoachieveperformancematchinglargestate-of- teractions,butalsooffersamoreholisticunderstandingofuser
the-artMLLMswhileutilizinglessvideo-textalignedtrainingdata. tasks.
ThisworksitsattheintersectionofUIunderstanding,MLLMs,
andself-supervisedlearning,pushingthecapabilitiesofwhatis
2.2 MultimodalLargeLanguageModels
possiblewithlightweightMLLMs.
RecentMultimodalLargeLanguageModels[2,12](MLLMs)have
1Thedatasetswillbemadepubliclyavailableshortly. ledtosignificantadvancementsinUIunderstanding,combining
2Estimatedparametercountbasedonpubliclyavailableinformation[19]. variousdatamodalitiesformorepreciseintentpredictions.DespiteUI-JEPA:TowardsActivePerceptionofUserIntentthroughOnscreenUserActivity
(a)TrainingProcess. (b)InferenceProcess.
Figure2:(a)TrainingProcessofUI-JEPA:Thetrainingprocessconsistsoftwostages:(1)JEPAtuningStage:Thepre-trained
x-encoder,y-encoder,andpredictorarefurtherfine-tunedonourUIdatasetsusingvariousmaskingtechniques.(2)LLM
Fine-tuningStage:Theparametersofthex-encoderfromthepreviousstageisfrozen.Thevideoembeddingiscombinedwith
texttokensembeddings,andfedtogetherasinputstothelargelanguagemodeltogenerateanoutputembedding.Thefinalloss
iscomputedbasedonlyonthetextportionoftheoutput,excludingthevideoportion;(b)InferenceProcessofUI-JEPA:During
inference,thevideoembeddingandtextembeddingsareinputintothelanguagemodeltogenerateapredictionofuserintent.
theseimprovements,thehighcomputationaldemandsofMLLMs topretrainvisiontransformers[7](ViTs)bymaskingandrecon-
oftennecessitateserver-sideprocessing,whichdrivesupcostsand structingrandomvideocubes.Similarly,JointEmbeddingPredic-
limitstheirscalability,whilepotentiallysacrificingprivacyand tiveArchitecture[10](JEPA)anditsderivatives,I-JEPA[3]and
imposingconnectivityconstraints. V-JEPA[5],focusonlearningsemanticrepresentationsbyignoring
Recentresearch[13]hasfocusedonreducingmodelsizewhile irrelevantdetailsandpredictingmaskedspatio-temporalregions.
striving to maintain performance. Although these efforts have OurapproachbuildsonexistingJEPA-basedmethodsbyemploy-
achievedsomesuccess,theresultingmodelsstillrequiresubstantial ingatemporalmaskingstrategy,whereentireframesaremasked
dataandremaintoolargeforefficientdeploymentonadvanced ratherthanjustpatches.Additionally,weintegrateaJEPA-based
mobiledevices. encoderwithanLLMdecoder,projectingvideoembeddingsfrom
Incomparison,ourapproachseekstoaddresstheselimitationsby ViTintotheLLMinputspaceusingadenselayerandgenerating
furthercompressingthemodelsizeandreducingdatarequirements. intentdescriptionswithafine-tunedLoRA[9]adapter.
Weachievethiswithoutcompromisingperformance,demonstrating
that our method performs competitively with larger models in
bothfew-shotandzero-shotscenarios,makingitmoresuitablefor 3 TheUI-JEPAFramework
mobileenvironments.
Ourgoalistotrackamobileuser’sintentbyanalyzingtheirin-
teractionswiththeUIandrepresentingthatuserintentasatext
2.3 SelfSupervisedLearning
summary.Weoptfortextoverastructuredformatbecausenat-
Obtaininglarge-scalelabeleddatasetsforUIunderstanding,espe- urallanguageservesasageneral-purposeandscalablesemantic
ciallythosethatpairvideosofUIactionswithuserintentlabels, representation,whichlanguagemodelsexcelatgenerating.
iscostlyanddifficulttoscale.Moreover,suchdatasetscannotcap- Thistaskrequirescomprehendingthetextual,spatial,andtempo-
tureeverypossiblevisualvariation,makingitessentialtodevelop raldimensionsofscreenactivitiesandtransformingtheirabstract
approachesthatuseunlabeleddataandgeneralizewellbylearning meaningsintoacoherenttextdescription.Toaddressthis,wede-
robustabstractrepresentations. velopedUI-JEPA,aframeworkconsistingoftwokeycomponents:
Self-supervisedlearning(SSL)hasemergedasapromisingso- avideotransformerandadecoder-onlylanguagemodel(LM).The
lutiontothesechallenges.Forinstance,VideoMAE[20]usesSSL videotransformerprocessesvideosofcontinuousscreenactivitiesYichengFu,RavitejaAnantha,PrabalVashisht,JianpengCheng,andEtaiLittwin
Figure3:2DVisualizationofVideoEmbeddings:Theleftpanelshowsthe2Dembeddingrepresentationofvideosfromthe
“IntentintheWild”datasetusingarandomencoder,whiletherightpaneldisplaystheembeddingsgeneratedbytheUI-JEPA
encoder.
intovideoembeddings,whicharethenfedintothedecoder-only JEPAfine-tuning,weapplyamaskingstrategy:maskedtokensare
LMtogenerateacorrespondingtextdescriptionoftheuserintent. removedfromtheinputtothe𝑥-encoder,butareusedasinputs
tothe𝑦-encoder,alongwiththeunmaskedtokens(wediscussthe
3.1 NetworkParameterization differentmaskingschemesemployedin6.2).Thepredictorreceives
asequenceofembeddingsfromthe𝑥-encoderconcatenatedwith
WeemploytheVisionTransformer[7](ViT)asourvideoencoder.
learnablemasktokens,whichincludepositionalembeddingsrepre-
Toobtainthevideoembeddings,weprocesstheentirevideoby
sentingthe3Dspatio-temporalpositionsofthemaskedtokens.The
sampling16evenlyspacedframes,resizingthemto384×384pixels, predictoristhentaskedwithpredictingthe𝑦-encoder’sembeddings
anddividingthemintoa3Dgridofspatial-temporalpatches.Each foreachmasktoken.The𝑦-encoderweightsstartasadeepcopy
patchmeasures16×16pixelsandspans2frames.Thesepatches, ofthe𝑥-encoderweightsandareupdatedusinganexponential
orvisualtokens,arefedintothevideoencodertogeneratethe movingaverage(EMA)[18]ofthe𝑥-encoderweights.
videoembeddings.AsillustratedinFigure2(b),theseembeddings
Inthesecondfine-tuningstage,wefreezetheViTweights(the
areprojectedintotheLM’sinputspaceusingadenselayer. 𝑥-encoderfromUI-JEPA)andupdatetheadaptersofapre-trained
FortheLM,weusealightweightvariant,MicrosoftPhi-3[1],
languagemodel.ThevideoembeddingsproducedbytheViTare
withapproximately3billionparameters.Thischoiceofalight-
projectedintotheLM’sinputspaceviaadenselayer.Theparam-
weightmodelfacilitateson-deviceexperimentationanddeploy-
etersupdatedinthisstageincludethedenseprojectionlayerand
ment.TheLMprocessesboththevideoembeddingsandthetext
theLM’sLoRAadapter[6,9].Anoverviewofthetrainingprocess
embeddings,withpositionalembeddingsappliedonlytothetext
isshowninFigure2(a).
inputs.AnoverviewofUI-JEPAinferencearchitectureispresented
inFigure2(b). 3.3 UI-JEPADataStrategy
There are two key differences in the fine-tuning data between
3.2 Training
UI-JEPAanditsparentV-JEPA.First,weintentionallyavoiddata
Weusepre-trainedweightsfromaVisionTransformer(ViT)andan augmentationonUIvideos.Unlikegeneralvideodata,UIvideos
LMtoperformfine-tuningintwostages:first,fine-tuningtheViT containbothspatialandtemporalactivitiesaswellastextualinfor-
onunlabeledUIvideos,andsecond,fine-tuningtheLMonvideos mationthataccuratelydescribesappfunctionalityanduserinput.
labeledwithuserintentdescriptions. Ourexperimentsdemonstratethatrandomdataaugmentationcan
Intheinitialfine-tuningstagefortheViT,weaddressthechal- resultinmalformedUIvideos,whichnegativelyimpactsmodel
lengeofannotatinguserintentdescriptionsbytrainingtheViTon performance.Second,whileUI-JEPAincorporatesthepatch-based
solelyvideosinaself-supervisedmanner.Followingtheapproach temporal-spatialmaskingstrategyusedinV-JEPA,italsointro-
ofV-JEPA[5],weuseamaskedpredictiontaskwheretheunmasked ducesanoveltemporalmaskingapproachwhereentireframesare
videoservesascontext,andthemaskedpartispredictedasthe maskedratherthanjustpatches.Thisnewstrategyenhancesthe
target.AsillustratedinFigure(𝑥-encoder),atargetmomentum model’sabilitytolearnframedependencies,addressingthedra-
encoder (𝑦-encoder), and a predictor. The context encoder uses maticchangesthatoftenoccurinUIvideoswhenusersopennew
theViTweights,whicharethefocusofourfine-tuning.During appsornavigatebetweendifferentscreens.UI-JEPA:TowardsActivePerceptionofUserIntentthroughOnscreenUserActivity
Figure4:ExamplesofinputsandcorrespondinglabelsfromtheIIWandIITdatasets.IntheIIWdataset,theinputisasequence
ofUIactionsinasinglevideo,labeledwithahigh-level,delexicalizeddescriptionofuserintent.Incontrast,theIITdataset
useslexicalizedintentasthelabel.Inaddition,itincludesOCRtextsconvertedfromthefinalvideoframe.
Table 1: Performance Metrics for Different Encoders: P- 4 TheUI-JEPABenchmarks
correlation(Pearsoncorrelationcoefficient),S-correlation
CurrentUIunderstandingbenchmarksfallshortincapturingUIac-
(Spearman’srankcorrelationcoefficient),andS-Score(Sil-
tionsassequenceswheretemporalrelationshipscanbelearned.Ad-
houetteScore).Thesemetricsevaluatetheperformanceof
ditionally,existingdatasetsfocusprimarilyonlearningmultimodal
variousencoders.
representationsfromseparateimages,whichmaynotsufficiently
capturethecomplexityoftasksorenablemodelstoaccuratelypre-
Encoder P-Correlation S-Correlation S-Score dictuserintent.Theselimitationshighlighttheneedfordatasets
thatrecordtaskexecutionasasequenceofUIactionsinvideo
Random 0.0334 0.0155 -0.1230
format,witheachvideolabeledtodescribetheuser’sintent.
VideoMAE 0.0861 0.0291 0.0081
For such datasets to be useful practically, they must possess
V-JEPA 0.1158 0.0435 0.0094
certaincharacteristics.First,variationsinappversionscanlead
UI-JEPA 0.1267 0.0427 0.0212
todifferentpresentationsandbehaviors,andthesequenceofUI
actionsmayvarydependingontheintendedtaskandappversion.
3.4 VisualizationofUI-JEPAembeddings Furthermore,toreflectreal-worldusageandtotestmodelrobust-
ness,thedatasetshouldincludenaturalnoise,suchasirrelevantUI
ToevaluatetheeffectivenessoftheUI-JEPAencoderinextracting
actionsinthevideos.Regardingchallenges,scalingthecollection
abstractrepresentations,wefirstselectthetop10mostfrequentapp
ofsuchadatasetischallenging,requiringalargenumberofannota-
typesfromtheIIWdatasetandcreateda2Dembeddingvisualiza-
torsandmobiledevicestoperformtasksaccordingtoinstructions,
tionusingthet-SNEmethod[21].Wecomparethisrepresentation
which demands significant investment. Finally, it is impractical
tothatofarandomlyinitializedvideoencoder,asillustratedin
tocaptureallpossiblevisualrepresentationvariationsofallapps
Figure3.Toassessclusterquality,wecomputethesilhouettescore,
andUIactionsequences.Toensuremodelsarebuiltandtestedfor
which measures how closely objects in a cluster resemble each
goodgeneralization,thedatasetmustsupportevaluationinboth
othercomparedtoobjectsinotherclusters.AsshowninFigure3,
few-shotandzero-shotsettings.
UI-JEPAachieveshighersilhouettescoresthanbaselines,indicating
Toaddressthesechallenges,wecreatedtwomultimodaldatasets:
thattheUI-JEPAembeddingsaremoretightlypackedwithintheir
“IntentintheWild”whichcapturesopen-endedsequencesofUI
clusters.
actionswithambiguoususerintent,and“IntentintheTame”which
Additionally, we calculate the cosine similarity for each pair
focusesonmorecommontaskswithrelativelyclearerintent.We
ofvideoembeddingsandcomparethesetothecosinesimilarity
believethesedatasetswillcontributetothedevelopmentofmore
scoresofcorrespondingnaturallanguageintentpairs.Thisanalysis
powerfulandlightweightMLLMs,aswellastrainingparadigms
investigatesthecorrelationbetweenthesesimilarityscores,with
withenhancedgeneralizationcapabilities.Figure4providesexam-
theexpectationthatvideoswithsimilarintentswillhavesimilar
plesfromeachdataset,andTable2providesstatisticsabouteach
embeddings.Ahighercorrelationsuggeststhatourvideoencoder
dataset.
effectivelycaptureshigh-levelrepresentationsofuserintent.As
showninTable1,UI-JEPAleadstohighervideo-textcorrelation
scorescomparedtoV-JEPAandotherbaselinesintroducedinsec-
tion5.YichengFu,RavitejaAnantha,PrabalVashisht,JianpengCheng,andEtaiLittwin
Table2:Datastatistics.
NumberofVideos
DatasetName Resolution Avg.Frames Categories
Train Few-shotEval Zero-shotEval
IIW 1170×2532 1274 344 87 723 219
IIT 334×720 682 187 45 826 10
4.1 IntentintheWild
TotrainandevaluateourproposedUI-JEPAmodel,state-of-the-art
MLLMs,andothercomparableself-supervisedlearningapproaches
ontaskswhereUIactionsareopen-endedanduserintentischal-
lengingtopredictorambiguous,wedevelopedthe“Intentinthe
Wild”(IIW)dataset.ThecreationoftheIIWdatasetinvolvedthree
keysteps:
• Recording UI Interactions: We manually recorded UI
interactionsforcomplextasks,suchasbookingavacation
rental,ensuringthatsomeintentcategoriesappearonly
once.Wefilteredoutanyvideosshorterthan16framesto
maintaindataquality.
• AnnotatingUserIntent:Weannotatedtheuserintent
basedontherecordedUIinteractions,providinghigh-level,
delexicalizeddescriptionsofintent,asillustratedinFig-
ure4.
• DatasetSplitting:Thedatasetwassplitintotwosettings:
afew-shotsplitwithatleasttwoinstancesofvideosfor
eachintentcategory,andazero-shotsplitwhereeachintent
categoryappearsonlyonce,withnooverlapwiththefew-
shotsplit.
(a)Top20mostfrequentappcategoriesintheIIWDataset.
TheIIWdatasetconsistsof219intentcategories,with135in
thefew-shotsplitand84inthezero-shotsplit.Intotal,thedataset
contains1.6Kvideos,eachaveraging749framesandapproximately
∼25secondsinduration.
Figure5ashowsthetop20mostfrequentintentcategories.
4.2 IntentintheTame
TheIntentintheTame(IIT)datasetwasdevelopedtorecordau-
thenticUIinteractionsexecutedbyindividualsintheprocessof
taskcompletion.Forscalingthesize,whilemaintainingrealism,
datasetcreationwasautomatedbyframingthenavigationasa
directedgraph⟨𝑉,𝐸⟩traversal,wheresetofvertices𝑉 represents
differentstateofthescreenandsetofedges𝐸arethedifferentUI
macros(actions).Theapproachcanbesummedupintwosteps,
• Setup:AniOSframeworkwasusedtorecorddiversesetof
UImacrosacrossapps.AnLLMwasutilizedtosynthetically
generatestagingdataforthedevice.Macros,inconjunc-
tionwiththedataproducedbyanLLM,werethenused
toautomaticallystagethedevicetoaspecificstate.For
instance,recordedmacrosfortheiOSContactsappwere
usedtocreatethecontacttobeedited(withthenameand
generatedbytheLLM)fortheEditContactintent. (b)10intentcategoriesintheIITtrainingsplit.
• Execution:Acomprehensivegraphwascreatedforeach
intentwhichencompassedallpotentialexecutionpaths. Figure5
Parametersfortheintentweresyntheticallycreatedviaan
LLM(toexemplify,thenameofthenewcontactfor“EditUI-JEPA:TowardsActivePerceptionofUserIntentthroughOnscreenUserActivity
Table3:PerformanceMetricsforDifferentModelsonIIWDatasetintheFew-shotSplit
LanguageModel VideoEncoder Img.Size Param. SBERT ROUGE-1 ROUGE-2 ROUGE-L IntentSim.
Random 384 52.46 58.10 31.94 57.48 55.94
MAE 224 62.82 63.77 39.33 62.97 61.87
V-JEPA 384 60.10 62.45 37.15 61.46 60.28
Phi Random+JEPAtuning 384 4.4B 44.02 52.37 24.12 51.78 50.07
MAE+JEPAtuning 224 59.27 61.51 36.95 60.68 59.69
UI-JEPA 224 64.99 65.38 41.84 64.73 63.61
UI-JEPA 384 66.51 66.33 42.94 65.48 64.50
Claude3.5Sonnet — Arbitrary >70B 76.58 65.12 42.06 63.58 64.76
GPT-4Turbo — Arbitrary 880B 74.79 63.91 39.61 62.54 63.36
Table4:PerformanceMetricsforDifferentModelsonIIWDatasetintheZero-shotSplit
LanguageModel VideoEncoder Img.Size Param. SBERT ROUGE-1 ROUGE-2 ROUGE-L IntentSim.
Random 384 42.90 50.16 20.67 48.85 47.78
MAE 224 49.57 53.17 22.31 51.60 50.47
V-JEPA 384 46.82 52.48 22.93 51.21 50.01
Phi Random+JEPAtuning 384 4.4B 41.18 49.78 20.71 49.26 47.59
MAE+JEPAtuning 224 47.21 52.08 22.25 50.48 49.60
UI-JEPA 224 49.98 54.43 23.63 52.12 51.29
UI-JEPA 384 50.68 55.08 24.71 53.52 52.16
Claude3.5Sonnet — Arbitrary >70B 73.74 61.15 34.75 58.63 60.35
GPT-4Turbo — Arbitrary 880B 71.84 58.77 32.03 56.26 58.24
Contact”intent).Finally,eachdatapointwasgenerated • Claude-3.5-Sonnet:Aclosed-sourcemultimodalmodelby
usingastageddeviceandarandomlyguidedtraversalof Anthropic.
thegraph(algorithmmadesuretheintentiscompleted). Theseclosed-sourcemodelsarepromptedtoconverttheUItest
ThisapproachensuresthattheIITdatasetencapsulatesfour splitvideosintotextsummariesofuserintentsforevaluation.
primarycharacteristics:realism,diversity,intentfulfillmentand Forallbaselines,weapplythesamedatapreprocessingtech-
labeled.Initsfirstversion,IITdatasetconsistsof914labeledvideos nique:sampling16evenlyspacedframesfromtheentirevideo.
spanningacross10intentcategoriesasshowninFigure5b.Outof SinceVideoMAEwasoriginallypre-trainedwitha224×224image
914videos,45videoswereevaluatedinazero-shotsetting. size,wealsoreportscoresfortheUI-JEPAencoderusingthesame
224×224imagesizetoensureafaircomparison.
5 Baselines Weevaluatetheoutputsusingseveralestablishedmetrics:the
WecompareUI-JEPAwithseveralbaselines,focusingprimarilyon SBERT(Sentence-BERT)cosinesimilarityscore[16],andtheROUGE-
differentmodelweightsforthevideoencoder,allbasedontheViT 1,ROUGE-2,andROUGE-Lscores[11].TheSBERTscoremeasures
architecture[7]: semanticsimilaritybyembeddingsentencesintoavectorspaceand
• RandomEncoder:AViTencoderinitializedwithrandom computingcosinesimilarity.ROUGEscoresevaluatesummaryqual-
itythroughunigramoverlap(ROUGE-1),bigramoverlap(ROUGE-
weights,whicharethenfine-tunedusinglabeleddatain
2),andthelongestcommonsubsequence(ROUGE-L),reflecting
thesecondstage.
• V-JEPA:AViTencoderpre-trainedonvideodatasetsusing sentencestructure.Additionally,weintroduceanewmetric,Intent
Similarity, calculated by averaging the four normalized similar-
afeaturepredictionobjective.
• VideoMAE:AViTencoderpre-trainedonvideodatasets ityscores,eachscaledtotherange[0,1].Together,thesemetrics
provideacomprehensiveevaluationofbothlexicalandsemantic
withvideotubemasking.
qualityinthegeneratedintents.
ThesebaselinevideoencodersarepairedwiththesameLLM
foruserintentgeneration.Wealsoincludeclosed-sourcemodels
6 Results
withpotentiallydifferentend-to-endarchitecturesforvideo-to-text
generation: 6.1 MainResults
• GPT-4Turbo:Aclosed-sourcemodelwithmultimodalun- UI-JEPAoutperformsallotherbaselinesonthefew-shotsplit. Ta-
derstandingcapabilitiesbyOpenAI. bles3,4,5,and6presenttheperformanceofUI-JEPAalongsideYichengFu,RavitejaAnantha,PrabalVashisht,JianpengCheng,andEtaiLittwin
Table5:PerformanceMetricsforDifferentModelsonIITDatasetintheFew-shotSplit
OCR LanguageModel VideoEncoder Param. SBERT ROUGE-1 ROUGE-2 ROUGE-L IntentSim.
Random 59.47 58.25 39.83 55.93 58.44
Phi V-JEPA 4.4B 69.69 70.40 51.72 68.06 68.76
No UI-JEPA 73.26 73.47 54.86 71.26 71.55
Claude3.5Sonnet — >70B 81.39 59.33 42.35 56.47 62.21
GPT-4Turbo — 880B 79.73 59.65 36.38 55.34 60.31
Random 82.68 78.78 62.38 75.84 77.09
Phi V-JEPA 4.4B 85.85 82.57 68.02 80.34 80.96
Yes UI-JEPA 87.43 83.73 69.17 81.51 82.03
Claude3.5Sonnet — >70B 80.67 59.05 41.78 56.63 61.95
GPT-4Turbo — 880B 79.29 57.88 36.65 54.61 59.70
Table6:PerformanceMetricsforDifferentModelsonIITDatasetintheZero-shotSplit
OCR LanguageModel VideoEncoder Param. SBERT ROUGE-1 ROUGE-2 ROUGE-L IntentSim.
Phi UI-JEPA 4.4B 44.03 36.12 12.74 31.65 38.13
No
Claude3.5Sonnet — >70B 79.52 53.21 32.74 49.38 56.27
GPT-4Turbo — 880B 78.26 51.75 27.43 46.43 53.69
Phi UI-JEPA 4.4B 41.91 29.81 11.66 27.55 34.99
Yes
Claude3.5Sonnet — >70B 79.69 53.67 33.92 49.85 56.82
GPT-4Turbo — 880B 75.77 50.81 25.97 46.1 52.69
Table7:PerformanceMetricsforDataAugmentationTechniques
Split Augmentation SBERT ROUGE-1 ROUGE-2 ROUGE-L IntentSim. IntentSim.Δ
NoAugmentation 64.55 65.42 41.84 64.55 63.52 0.00%
Flipping 65.66 65.45 42.06 64.82 63.79 0.42%
Few-shot
Cropping 63.44 64.13 40.76 63.21 62.46 -1.68%
Flipping+Cropping 61.43 62.50 37.84 61.66 60.68 -4.47%
NoAugmentation 50.68 55.08 24.71 53.52 52.16 0.00%
Flipping 47.71 52.95 22.49 51.66 50.24 -3.69%
Zero-shot
Cropping 49.59 54.33 25.06 51.81 39.43 -0.68%
Flipping+Cropping 49.13 52.70 23.20 50.39 38.02 -3.40%
Table8:PerformanceMetricsforAddingPositionalIDstoVideoEmbeddings
Split AddPositionalEmbeddings SBERT ROUGE-1 ROUGE-2 ROUGE-L IntentSim.
No 64.55 65.42 41.84 64.55 63.52
Few-shot
Yes 62.95 63.64 39.61 62.65 61.84
No 50.68 55.08 24.71 53.52 52.16
Zero-shot
Yes 46.89 53.08 22.64 51.44 50.15
other baselines on the few-shot and zero-shot splits of the IIW leadingclosed-sourcemodel,Claude3.5Sonnet,UI-JEPAdemon-
andtheIITdatasets,respectively.Amongallvideoencoders,UI- stratessuperiorperformanceonfew-shottaskswhileshowinga
JEPA consistently achieves the highest scores in both few-shot performancegapinzero-shotsettingsespeciallyfortheIITdataset.
andzero-shotscenarios.Whencomparingintentsimilaritytothe ThisindicatesthatwhileUI-JEPAexcelsintasksinvolvingfamiliar
applications,itfaceschallengeswithunfamiliarones.However,itUI-JEPA:TowardsActivePerceptionofUserIntentthroughOnscreenUserActivity
(a)NoMasking. (b)SpatialMasking. (c)TemporalMasking,
Figure6:ExampleofMaskingStrategies:(a)OriginalFrameSequences;(b)ConsistentRegionMaskingAcrossAllFrames;(c)
FullRegionMaskinginSelectedFrames
isimportanttonotethatClaude3.5Sonnetcomeswithsignificant performance.Thecombinationofflippingandcroppingfurtherex-
trade-offs,includingcoststhatare50.5timeshigherandlatency acerbatesperformancedegradationinbothfew-shotandzero-shot
thatis6.6timesgreaterthanUI-JEPAasamorelightweightmodel. scenarios.Asaresult,wedecidedtoeliminatealldataaugmentation
IncorporatingOCR-extractedtextfromthefinalframeenhances methodsinoursubsequentexperiments.
performanceonthefew-shotsplitofIITdatasetforallJEPA-based
models, with UI-JEPA outperforming other baselines. However, 6.2.2 PositionalEmbedding. Duringtheintegrationofvideoand
closed-sourcemodelsandthezero-shotsplitshownoimprovement textembeddingsbeforepassingthesehybridembeddingstotheLM,
fromthisaddition.ForUI-JEPAspecifically,theOCRextraction weexaminedtheimpactofaddingpositionalembeddingstothe
stepintroducesa13.5%latencyincreasewhiledeliveringa14.4% videoembeddings,similartothoseusedintextembeddings.Wealso
performanceboost. consideredthealternativeofomittingtheseadditionalpositional
embeddings.Giventhatthevideoembeddingsalreadycontain3D
6.2 AblationStudies positionalinformationfromtheencoder,whichrepresentsspatio-
temporalpositions,weevaluatedwhetheraddingextrapositional
Weperformablationstudiestoassesstheimpactofdataaugmenta-
embeddingswouldofferanyadvantage(seeTable8).
tion,positionalembeddings,maskingstrategy,maskingratio,and
Ourresultsshowthatomittingadditionalpositionalembeddings
JEPA-tuningdatasize.Allexperimentsareconductedusingthe
consistently improves performance, while their inclusion tends
MicrosoftPhi-3modelandthe"IntentintheWild"dataset.
todegradeit.Basedonthesefindings,wechosenottoincorpo-
6.2.1 DataAugmentation. DuringthepretrainingstageofV-JEPA, rateextrapositionalembeddingsintothevideoembeddingsinour
dataaugmentationtechniquessuchasrandomflippingandcrop- subsequentexperiments.
ping are applied to each video frame. However, unlike natural
videodatasets,smartphonescreenshaveafixedorientation,mak- 6.2.3 JEPA-tuningDataSize. Inthissection,weexaminetheeffect
ingimageflippingineffectiveandpotentiallyintroducingnoise. ofvaryingthesizeoftheJEPA-tuningdatasettoassesswhether
Additionally,crucialsignalslikenotificationsareoftenlocatedat addingmoreunlabeledUIvideosenhancesvideorepresentation
thetoporbottomofthescreen,socroppingriskslosingsignificant andoverallmodelperformance.Weexperimentedwithusing25%,
information.Toevaluatetheimpactofexcludingtheselesseffec- 50%,75%,and100%oftheavailabledataduringtheJEPA-tuning
tivedataaugmentationmethods,weconductedexperiments(see phasewhileusingthefulldatasetforfine-tuning.Theresultsare
Table7). summarizedinFigure7.
Ourresultsshowthatwhileflippingslightlyimprovesperfor- AsthesizeoftheJEPA-tuningdataincreases,modelperformance
manceonthefew-shotsplit,itsignificantlydegradesperformance improvesconsistentlyinthefew-shotscenario.Forthezero-shot
onthezero-shotsplit.Thisdeclinecouldbeduetothevideoen- scenario,performanceinitiallyremainsstablebutshowsasignifi-
coder’sdifficultyinlearningaconsistentorientationwithinthe cantincreasewhenthefulldatasetisusedforJEPA-tuning.This
UI video dataset, which negatively affects generalization to the indicatesthatevenwhenthenumberoflabeledexamplesisfixed,
zero-shotset.Similarly,thecroppingmethodalsoleadstoreduced increasingthenumberofunlabeledexamplesduringJEPA-tuningYichengFu,RavitejaAnantha,PrabalVashisht,JianpengCheng,andEtaiLittwin
Figure7:PerformanceofDifferentJEPA-tuningDataSize Figure8:PerformanceComparisonAcrossDifferentMasking
Settings
enhances the model’s feature extraction capabilities, leading to resultsforboththefew-shotandzero-shotscenariosareachieved
betterperformanceindownstreamtasks. withsixmaskeddiscreteframes.
7 ConclusionandApplications
6.2.4 MaskingStrategy. IntheoriginalJEPApre-trainingprocess,
7.1 Conclusion
optimalresultsareachievedusingamulti-blockmaskingstrategy,
whererandomspatio-temporalblocksfromtheentirevideoare Inthiswork,weintroducedUI-JEPA,aframeworkthatusesself-
masked. However, applying additional masking to the last few supervisedlearningtogenerateabstractUIembeddingsand,when
framescanreduceperformance.Giventheuniquecharacteristicsof combinedwithasmallLLM,canperformuserintentprediction.
ourUI-groundedmultimodaldatasets,theseconventionalmasking UI-JEPAmatchestheperformanceofstate-of-the-artMLLMswhile
strategiesmaynotbeideal.Toexploretheeffectsofmaskingduring reducingcomputationalcostsby50.5xandlatencyby6.6x,makingit
JEPA-tuning,weaddresstwokeyresearchquestions: idealforlightweight,on-deviceapplications.Ournewlyintroduced
Q1:Whattypesofmaskingyieldoptimalperformance? datasets,“IntentintheWild”(IIW)and“IntentintheTame”(IIT),
Q2:Whatratioandstrategyoftemporalmaskingshouldbeused establishabenchmarkforfew-shotandzero-shotUIunderstanding.
forthebestresults? ThesefindingshighlightUI-JEPA’spotentialforadvancingefficient
ToanswerQ1,weexperimentedduringtheJEPA-tuningstage andprivacy-preservingUIunderstanding.
withthreemaskingsettings:(1)short-rangemasking,(2)short-
range+long-rangemasking(asusedinV-JEPA),and(3)short-range 7.2 ApplicationsofUI-JEPA
+long-range+temporalmasking.Figure6providesanoverviewof
Userintentunderstandinghasatleasttwokeyapplications:User
thesemaskingstrategies,andFigure8comparestheirperformance.
FeedbackLearningandMultimodalIntentStateTracking.
Ourresultsshowthatprogressivelyaddingmoremaskingim-
provesperformanceinboththefew-shotandzero-shotscenarios, 7.2.1 UserFeedbackLearning. Smartphoneusersworldwideinter-
indicatingthatadditionaltemporalmaskinghelpsthemodelbetter actdailywithdigitalassistants,generatingavastarrayofqueries.
captureserialdependenciesbetweenframes,leadingtoenhanced Thisdataisinvaluableforrefiningthereasoningcapabilitiesof
outcomes. digitalassistantsandaligningtheirresponseswithuserpreferences.
ForQ2,wetestedtwotemporalmaskingstrategiesduringJEPA- However,privacyandsecurityconcernsaside,asignificantchal-
tuning:(1)ContiguousTemporalMasking,whereasingleblock lengeisthatmanydigitalassistantscurrentlystruggletoaddress
ofconsecutiveframesismaskedinFigure9;and(2)DiscreteTem- userrequestseffectively,resultinginalargevolumeofdisorganized,
poralMasking,wherearbitraryframesareselectedformasking low-qualitydata.ByusingUI-JEPA,wecanaccuratelyinferuser
inFigure10.Sinceeachpatchspanstwovideoframes,masking intentfromon-screenactivityandassesstheeffectivenessofthe
occursatthelevelofthesepatches,with“frame”herereferringto digitalassistant’sperformance.Thisincludesdeterminingwhether
ahyper-frameconsistingoftwovideoframes. userscontinuewiththeappopenedbytheassistantorswitchto
Fromourexperiments,weobservedthatinthefew-shotscenario, adifferentone.IfUI-JEPApredictssuccessfulexecution,thedata
performanceimprovesasmoreframesaremasked.However,inthe pointcanbedirectlyaddedtoourhigh-qualitydataset.Conversely,
zero-shotscenario,thistrendislesspronounced,withperformance ifthepredictionindicatesafailure,UI-JEPAcanstillcapturethe
remainingrelativelystableacrossdifferentmaskingratios.Thebest user’strueintent,contributingvaluabledatatoourhigh-qualityUI-JEPA:TowardsActivePerceptionofUserIntentthroughOnscreenUserActivity
Figure9:PerformanceAcrossDifferentNumbersofMaskedFramesUsingContiguousTemporalMasking.
Figure10:PerformanceAcrossDifferentNumbersofMaskedFramesUsingDiscreteTemporalMasking.
Figure11:UserFeedbackLearning:UI-JEPAautomaticallyfiltersandlabelshigh-qualitydataforconstructingdatasetstotrain
digitalassistants.
dataset,asshowninFigure11.Thesehigh-qualitydatasetscanthen storingtheseintentsinamemorystore.Whenauserinteractswith
beusedtofurtherfine-tuneUI-JEPA,enhancingitsperformance adigitalassistant,thesystemretrievesthemostrelevantintent
andenrichingthedatasetforfutureUIunderstandingresearch. basedonthequeryandgeneratestheappropriateAPIcalltofulfill
theuser’srequest,asillustratedinFigure12.
7.2.2 Multimodal Intent State Tracking. Another promising ap-
plicationofUI-JEPAisitsintegrationintoanagenticframework
8 Limitations
[14,17]designedtoactivelytrackuserintentacrossvariousappli-
cationsandmodalities.Inthisframework,UI-JEPAfunctionsasthe WhileUI-JEPAhasshownpromisingresultsontheIIWandIIT
perceptionagent,capturinguserintentatdifferenttimepointsand datasets,severallimitationsremain:YichengFu,RavitejaAnantha,PrabalVashisht,JianpengCheng,andEtaiLittwin
toSrinivasChappidi,SophieOstlund,LuisaMendozaGonzalez,
GabrielaLopez,JoshElman,TimKolecke,DianeWhite,SteveSmith,
LindsayHislop,andJohnGiannandreafortheirinvaluablesupport
throughoutthiswork.
References
[1] MarahAbdin,SamAdeJacobs,AmmarAhmadAwan,JyotiAneja,Ahmed
Awadallah,HanyAwadalla,NguyenBach,AmitBahree,ArashBakhtiari,Harki-
ratBehl,etal.2024.Phi-3technicalreport:Ahighlycapablelanguagemodel
locallyonyourphone.arXivpreprintarXiv:2404.14219(2024).
[2] Anthropic.2024.AnthropicClaude3.5Sonnet. https://www.anthropic.com/
news/claude-3-5-sonnet
[3] MahmoudAssran,QuentinDuval,IshanMisra,PiotrBojanowski,PascalVincent,
MichaelRabbat,YannLeCun,andNicolasBallas.2023.Self-SupervisedLearning
FromImagesWithaJoint-EmbeddingPredictiveArchitecture.InProceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).
15619–15629.
[4] ChongyangBai,XiaoxueZang,YingXu,SrinivasSunkara,AbhinavRastogi,
JindongChen,andBlaiseAguerayArcas.2021. UIBert:LearningGeneric
MultimodalRepresentationsforUIUnderstanding.InProceedingsoftheThirtieth
InternationalJointConferenceonArtificialIntelligence(IJCAI-21).1705–1711.
https://www.ijcai.org/proceedings/2021/0235.pdf
[5] AdrienBardes,QuentinGarrido,JeanPonce,XinleiChen,MichaelRabbat,Yann
LeCun,MidoAssran,andNicolasBallas.2024.V-JEPA:LatentVideoPrediction
forVisualRepresentationLearning.(2024). https://openreview.net/forum?id=
WFYbBOEOtv
Figure12:Multi-modalIntentStateTracking:Thisframe-
[6] TimDettmers,ArtidoroPagnoni,AriHoltzman,andLukeZettlemoyer.2023.
workintegratesUI-JEPAasaperceptionagentthatactively QLoRA:EfficientFinetuningofQuantizedLLMs. arXiv:2305.14314[cs.LG]
monitorsandcapturesuserintentacrossvariousapplica- https://arxiv.org/abs/2305.14314
[7] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,
tionsandmodalities.
XiaohuaZhai,ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,
GeorgHeigold,SylvainGelly,JakobUszkoreit,andNeilHoulsby.2021. An
ImageisWorth16x16Words:TransformersforImageRecognitionatScale.
arXiv:2010.11929[cs.CV] https://arxiv.org/abs/2010.11929
• GranularityofUserIntentPrediction:JEPAembeddings
[8] ZechengHe,SrinivasSunkara,XiaoxueZang,YingXu,LijuanLiu,NevanWich-
aloneoftenfallshortforgranularuserintentprediction, ers,GabrielSchubiner,RubyLee,JindongChen,andBlaiseAgüerayArcas.2021.
especiallyintheIITdataset,wheretheencoderprimarily ActionBert:LeveragingUserActionsforSemanticUnderstandingofUserInter-
faces.InTheThirty-FifthAAAIConferenceonArtificialIntelligence(AAAI-21).
captureshigh-levelvideorepresentations.Thislimitation 5931–5938. https://cdn.aaai.org/ojs/16741/16741-13-20235-1-2-20210518.pdf
affectstasksrequiringdetailedtextrecognitionanddescrip- [9] EdwardHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,Shean
Wang,LuWang,andWeizhuChen.2022.LoRA:Low-RankAdaptationofLarge
tion. To address this, we incorporate OCR; however, its
LanguageModels.InInICLR,2022. https://iclr.cc/virtual/2022/poster/6319
effectivenessiscontingentonthequalityoftheOCRand [10] YannLeCun.2022.APathTowardsAutonomousMachineIntelligence.(2022).
thepresenceoftextualinformationintheframes.Further https://openreview.net/pdf?id=BZ5a1r-kVsf
[11] Chin-YewLin.2004.ROUGE:APackageforAutomaticEvaluationofSummaries.
researchisneededtoenhanceJEPArepresentationstocap-
InTextSummarizationBranchesOut.AssociationforComputationalLinguistics,
turemoredetailedcontent. Barcelona,Spain,74–81. https://aclanthology.org/W04-1013
• Pre-trainingRequirements:Experimentalresultsrevealthat [12] OpenAI.2024.GPT-4TechnicalReport. arXiv:2303.08774[cs.CL] https://arxiv.
org/abs/2303.08774
JEPA-tuningarandomlyinitializedvideoencoderyields [13] OpenAI.2024.GPT-4oMini. https://openai.com/index/gpt-4o-mini-advancing-
poorperformance.EffectiveJEPA-tuningnecessitatesex- cost-efficient-intelligence/
[14] ShuofeiQiao,NingyuZhang,RunnanFang,YujieLuo,WangchunshuZhou,
tensivepre-trainingofthevideoencoder,whichrestricts
YuchenJiang,ChengfeiLv,andHuajunChen.2024.AutoAct:AutomaticAgent
theapplicabilityofJEPA-tuningtoscenarioswithample LearningfromScratchforQAviaSelf-Planning.InProceedingsofthe62nd
videodata. AnnualMeetingoftheAssociationforComputationalLinguistics(Volume1:Long
Papers),Lun-WeiKu,AndreMartins,andVivekSrikumar(Eds.).Associationfor
• Performance in Zero-Shot Scenarios: Although UI-JEPA
ComputationalLinguistics,Bangkok,Thailand,3003–3021. https://aclanthology.
performscompetitivelywithlargeMLLMslikeClaude3.5 org/2024.acl-long.165
SonnetandGPT-4Turboinfew-shotsettings,itsperfor- [15] RajeshPNRaoandDanaHBallard.1999.Predictivecodinginthevisualcortex:a
functionalinterpretationofsomeextra-classicalreceptive-fieldeffects.InNature
mancelagssignificantlyinzero-shotscenarios.Thisindi- Neuroscience.79–87. https://doi.org/10.1038/4580
catesthatUI-JEPA’sabilitytogeneralizefromfamiliarto [16] NilsReimersandIrynaGurevych.2019.Sentence-BERT:SentenceEmbeddings
usingSiameseBERT-Networks. arXiv:1908.10084[cs.CL] https://arxiv.org/abs/
unfamiliarappsneedsimprovement.
1908.10084
• AudioModality:UI-JEPAhasnotbeentestedwithaudio [17] MiracSuzgunandAdamTaumanKalai.2024. Meta-Prompting:Enhancing
modalities, and its performance in this domain remains LanguageModelswithTask-AgnosticScaffolding. arXiv:2401.12954[cs.CL]
https://arxiv.org/abs/2401.12954
unexamined.
[18] AnttiTarvainenandHarriValpola.2018.Meanteachersarebetterrolemodels:
Weight-averagedconsistencytargetsimprovesemi-superviseddeeplearning
Acknowledgments results. arXiv:1703.01780[cs.NE] https://arxiv.org/abs/1703.01780
[19] DrAlanD.Thompson.2024.TheMemo-Specialedition:Claude3Opus.Substack.
WewouldliketothankAmandaSwearngin,BarryTheobald,Chen LifeArchitect.ai.
[20] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. 2023. VideoMAE:
Huang,EldonSchoop,ArturoArgueta,andVimalThilakfortheir
MaskedAutoencodersareData-EfficientLearnersforSelf-SupervisedVideo
insightfulfeedbackandcommentsonthedraft.Wearealsograteful Pre-Training. In 36th Conference on Neural Information Processing SystemsUI-JEPA:TowardsActivePerceptionofUserIntentthroughOnscreenUserActivity
(NeurIPS 2022). https://proceedings.neurips.cc/paper_files/paper/2022/file/ B.1.4 LoRAConfiguration. WeapplytheLoRAtuningtechnique
416f9cb3276121c42eebb86352a4354a-Paper-Conference.pdf duringthefine-tuningoftheLM.Fordetailedinformationonthe
[21] LaurensVanderMaatenandGeoffreyHinton.2008. Visualizingdatausing
LoRAtuningconfiguration,pleaserefertoTable10.
t-SNE.Journalofmachinelearningresearch9,11(2008).
[22] JasonWu,RebeccaKrosnick,EldonSchoop,AmandaSwearngin,JeffreyP
Bigham, and Jeffrey Nichols. 2023. Never-ending Learning of User Inter- C Inference
faces.InProceedingsofthe36thAnnualACMSymposiumonUserInterface
SoftwareandTechnology(SanFrancisco,CA,USA)(UIST’23).Associationfor C.1 Prompts
ComputingMachinery,NewYork,NY,USA,Article113,13pages. https:
//doi.org/10.1145/3586183.3606824 Forclosed-sourceMLLMs,includingGPT-4TurboandClaude3.5
[23] KeenYou,HaotianZhang,EldonSchoop,FlorisWeers,AmandaSwearngin, Sonnet,thepromptsusedtogenerateresultsaredetailedinTable11.
JeffreyNichols,YinfeiYang,andZheGan.2024. Ferret-UI:GroundedMobile
UIUnderstandingwithMultimodalLLMs. arXiv:2404.05719[cs.CV] https: Whenthebytesizeof16imagesintheClaude3.5Sonnetdataset
//arxiv.org/abs/2404.05719 exceedsprocessinglimits,weuseonlyhalfoftheimages(specifi-
[24] XiaoyiZhang,LiliandeGreef,AmandaSwearngin,SamuelWhite,KyleMurray,
cally,images1,3,5,...,15)toobtainresults.Additionally,ifClaude
LisaYu,QiShan,JeffreyNichols,JasonWu,ChrisFleizach,AaronEveritt,and
JeffreyP.Bigham.2021.ScreenRecognition:CreatingAccessibilityMetadatafor 3.5Sonnetfailstoprovidedescriptionsofuserintentorapplication
MobileApplicationsfromPixels.InCHI’21:Proceedingsofthe2021CHIConference typeduetoethnicorprivacyconsiderations,wemanuallyexclude
onHumanFactorsinComputingSystems(Yokohama,Japan).Associationfor
theseresponsesfromtheperformancemetricscalculation.
ComputingMachinery,NewYork,NY,USA.
A DatasetProcessingDetails
IntheV-JEPApaper,encodersprocessvideoclipsconsistingof16
frames,withatemporalstrideof4betweenconsecutiveframes.For
theK400dataset,8clipsaresampledpervideo,whilefortheSSv2
dataset,only1clipissampledpervideo.However,thisapproach
reliesonpriorknowledgeofvideodistributionandmaynotbe
effectiveforvideosofvaryinglengths.Toaddressthis,wepropose
samplingasingle16-frameclipusingaflexibletemporalstride,
withframesevenlysampledfromthebeginningtotheendofthe
video.Thismethodaccommodatesvariationsinvideolengthmore
effectively.
During the JEPA-tuning and fine-tuning stages, we exclude
videoswithfewerthan16frames,assuchbriefvideosareunlikely
tocapturesignificantuseractivities.
B TrainingDetails
Inthissection,weprovidedetailsonthetrainingprocessforUI-
JEPA,includingboththeJEPA-tuningandfine-tuningstages.Ta-
ble9summarizesthekeyhyperparametersusedduringpretraining.
B.1 Fine-tuning
B.1.1 SeparatorandEndingToken. Duringfine-tuning,weinsert
aseparatortokenbetweenthevideoembeddingandtheOCRtext,
aswellasbetweentheOCRtextandtheuserintent.Anending
tokenisalsousedtodistinguishbetweendifferentinputtypes.For
theMicrosoftPhi-3model,theseparatortokenis<|placeholder1|>,
andtheendingtokenis<|endoftext|>.
B.1.2 PositionalEmbedding. Forpositionalembeddings,wedonot
applyadditionalpositionalembeddingstothevideoembeddingor
theinitialseparatortoken.Instead,weusethestandardpositional
IDstartingfromthefirstOCRtexttoken.IfnoOCRtextispresent,
thepositionalIDbeginswiththefirstintenttexttoken.
B.1.3 OCR. In our study, we use the Apple Vision Service for
OpticalCharacterRecognition(OCR),employingtheVNRecognize-
TextRequesttoextracttextfromimages.Toimprovetherelevance
oftheextracteddata,wefilteroutsingle-lettertextsandspecific
stringssuchas"123","space",and"return."Theseelementsgenerally
correspondtokeyboardinputsdisplayedontheuser’sscreenand
donotprovidemeaningfulinformation.YichengFu,RavitejaAnantha,PrabalVashisht,JianpengCheng,andEtaiLittwin
Table9:Hyper-parametersandTheirValues
IIW IIT
Hyper-parameter
JEPA-tuning Fine-tuning JEPA-tuning Fine-tuning
Data
Resolution 384 384 384 384
NumFrames 16 16 16 16
TemporalStride Flexible Flexible Flexible Flexible
DataAugmentation False False False False
Short-RangeMasking
BlockAspectRatio (0.75,1.5) — (0.75,1.5) —
NumBlocks 8 — 8 —
SpatialScale 0.15 — 0.15 —
TemporalScale 1 — 1 —
Long-RangeMasking
BlockAspectRatio (0.75,1.5) — (0.75,1.5) —
NumBlocks 2 — 2 —
SpatialScale 0.7 — 0.7 —
TemporalScale 1 — 1 —
TemporalMasking
BlockAspectRatio (1.0,1.0) — (1.0,1.0) —
NumBlocks 1 — 1 —
SpatialScale 1 — 1 —
TemporalScale 0.75 — 0.75 —
Optimization
BatchSize 4 1 4 1
TotalNumberofIterations 4000 6000 2000 3000
WarmupIterations 100 300 50 150
LR 3×10−4 3×10−4 3×10−4 3×10−4
StartLR 2×10−4 2×10−4 2×10−4 2×10−4
FinalLR 1×10−6 1×10−6 1×10−6 1×10−6
StartMomentum 0.998 0.998 0.998 0.998
FinalMomentum 1.0 1.0 1.0 1.0
StartWeightDecay 0.04 0.04 0.04 0.04
FinalWeightDecay 0.4 0.4 0.4 0.4
SchedulerScaleFactor 1.25 1.25 1.25 1.25
Architecture
PatchSize 16 16 16 16
TubeletSize 2 2 2 2
PredDepth 12 12 12 12
PredEmbedDim 12 12 12 12
Hardware
DataType bfloat16 bfloat16 bfloat16 bfloat16
Accelerator A10080G A10080G A10080G A10080GUI-JEPA:TowardsActivePerceptionofUserIntentthroughOnscreenUserActivity
Table10:LoRATuningConfiguration
Hyper-parameter ParameterValue
LoRA
LoRAAlpha 16
LoRADropout 0.05
LoRARank 16
TargetModules 𝑞_𝑝𝑟𝑜𝑗,𝑘_𝑝𝑟𝑜𝑗,𝑣_𝑝𝑟𝑜𝑗,𝑜_𝑝𝑟𝑜𝑗,𝑔𝑎𝑡𝑒_𝑝𝑟𝑜𝑗,𝑢𝑝_𝑝𝑟𝑜𝑗,𝑑𝑜𝑤𝑛_𝑝𝑟𝑜𝑗
Quantization
QuantizationType nf4
DoubleQuantization true
ComputationType bfloat16YichengFu,RavitejaAnantha,PrabalVashisht,JianpengCheng,andEtaiLittwin
Table11:Promptsusedforbaselines
Prompt
PromptforIIWDataset:
Hereisarecordingofauser’soperationonaniPhone.Pleasesummarizetheuser’sintentinadelexicalizedandconcisemanner,removing
specifictextdetail,andspecifytheapptypefollowingtheformatbelowusingplaintext:
Intent:
AppType:
Herearesomeexamples:
Intent:Theuserchecksweather.
AppType:weather
Intent:Theuserbrowsesproducts.
AppType:shopping
Intent:Theuserlooksforaccommodations.
AppType:travel
Hereistherecording:
{image1}{image2},...,{image16}.
PromptforIITDataset:
Hereisarecordingofauser’siPhoneactivity.Pleasesummarizetheirintentions.Whiletheintentmayinvolvemultipleactivities,itmust
endwithoneofthefollowingactivitycategories:
ExampleEndingActivityCategories:
UsercallsAbigailfromtheircontacts.
UserupdatesacontactnametoAlex.
UsersendsamessagetoAndrewviatheiMessageappsaying’WatchingTV.’.
Usercreatesanalarmfor11:09AM.
UseraddsMastercardstocktotheirwatchlist.
UseraddsanewcontactnamedRavi.
Useraddsareminder’Doctorappointment’.
Usercreatesanotetitled’Presentationnotes’withinthe’Resolutions’folder.
Usercreatesatimerfor6minutesand52seconds.
Exampleoutput:
Useropensstockapp,andthencallsLilyfromtheircategory.
UserenablesDoNotDisturb,searchesforcontacts,andaddsanewcontactnamedJackson.
UsersearchesforNetflix,opensapp,andcreatesanalarmfor2:19AM.
Hereistherecording:
{image1}{image2},...,{image16}.
(Optional)HereistheOCRtextfromthefinalframe:{OCR_text}.