VILA-U: a Unified Foundation Model Integrating
Visual Understanding and Generation
YechengWu1,2∗ ZhuoyangZhang2,3∗ JunyuChen1,2 HaotianTang2,3
DachengLi3,4 YunhaoFang3,5 LigengZhu3 EnzeXie3
HongxuYin3 LiYi1 SongHan2,3 YaoLu3
TsinghuaUniversity1 MIT2 NVIDIA3
UCBerkeley4 UCSanDiego5
Abstract
VILA-UisaUnifiedfoundationmodelthatintegratesVideo,Image,Language
understandingandgeneration. Traditionalvisuallanguagemodels(VLMs)use
separatemodulesforunderstandingandgeneratingvisualcontent,whichcanlead
tomisalignmentandincreasedcomplexity. Incontrast,VILA-Uemploysasingle
autoregressivenext-tokenpredictionframeworkforbothtasks,eliminatingtheneed
foradditionalcomponentslikediffusionmodels. Thisapproachnotonlysimplifies
themodelbutalsoachievesnearstate-of-the-artperformanceinvisuallanguage
understandingandgeneration. ThesuccessofVILA-Uisattributedtotwomain
factors: the unified vision tower that aligns discrete visual tokens with textual
inputsduringpretraining,whichenhancesvisualperception,andautoregressive
imagegenerationcanachievesimilarqualityasdiffusionmodelswithhigh-quality
dataset. This allows VILA-U to perform comparably to more complex models
usingafullytoken-basedautoregressiveframework.
1 Introduction
Inrecentyears,largelanguagemodels(LLMs)havedemonstratedsuperiorcapabilitiesinvarious
languagetasks. Theirappealingpropertieslikeinstructionfollowing,zero-shotgeneralization,and
few-shotin-contextlearningmotivateresearcherstocombinethemwithvisionmodelstobuildvisual
languagemodels(VLMs)formulti-modaltasks. Manyefforts[15,51,45]havebeenputintothis
field,achievingremarkableperformanceonvisuallanguageunderstandingbenchmarks. Inthese
works,visualinputsareprojectedontoLLMs’semanticspacethroughavisionfoundationmodellike
CLIP[58]tobridgetwomodalitiesbyincludingtext-imagealignmenttrainingobjectives.
Inaddition tovisual understanding, anotheressentialresearch directionincombining visualand
languagemodalitiesisvisualgeneration. Therearetwopopularapproachesfortext-guidedimage
generation.Oneapproachemploysdiffusionmodels[60],apowerfultoolforvariousgenerationtasks.
Theotherlineofworkconvertsvisualcontentintodiscretetokensthroughvectorquantization(VQ)
andthenleveragingautoregressivetransformersforhigh-qualityanddiversegeneration[21,73,33].
Witnessingtherapidadvancementsinbothvisualunderstandingandgeneration,anemergingtrend
istounifythesetechniquesintoasinglemulti-modalframework. Therearetwomainapproaches
to achieving such unification. Many VLMs [31, 41, 64, 63] maintain an understanding-oriented
frameworkandoffloadthegenerationtasktoanexternaldiffusionmodel. Thisdisjointapproach
addscomplexitytoinfrastructuredesign. Availablelarge-scalefoundationmodeltrainingpipelines
anddeploymentsystemshavealreadybeenhighlyoptimizedforlanguagemodelingwithnext-token
prediction. Designinganewstacktosupportdiffusionmodelswouldincursignificantengineering
costs.Tocircumventsuchcosts,itisdesirabletodesignasingleend-to-endautoregressiveframework
∗equalcontribution
4202
peS
6
]VC.sc[
1v92440.9042:viXrafor both image understanding and generation. There is a trend in VLMs [48, 75] that adopt VQ
encoderstoconvertvisualinputsintodiscretetokensandtreattheminthesamenext-tokenprediction
manneraslanguagedata. However,replacingcontinuoustokenswithVQtokensinVLMsusually
resultsinasevereperformancedropindownstreamvisualperceptiontasks. Otherworks[52,65]
haveto makevariousarchitecturalmodifications andconduct multi-modaltrainingfrom scratch,
whichiscomputationallyexpensive.
Inthiswork,wepresentVILA-U,anend-to-endautoregressiveframeworkwithaunifiednext-token
prediction objective for both visual and text inputs that can achieve competitive performance on
bothvisuallanguageunderstandingandgenerationtasks,withoutthehelpofexternalcomponents
likediffusionmodels. Weidentifytwocriticalprinciplestounifyvisionandlanguagemodalities
effectivelyandefficiently. (1)Existingend-to-endautoregressiveVLMscannotachievecompetitive
visual understanding performance because the discrete VQ tokens are trained solely on image
reconstructionlossandarenotalignedwithtextualinputs. Therefore,itiscrucialtointroducetext
alignmentduringVQvisiontowerpretrainingtoenhanceperceptioncapabilities. (2)Autoregressive
imagegenerationcanattainsimilarqualityasdiffusionmodelsiftrainedonahigh-qualitydatacorpus
withsufficientsize. Guidedbytheseinsights,VILA-Ufeaturesaunifiedfoundationvisiontowerthat
convertsvisualinputsintodiscretetokensthroughvectorquantizationandalignsthesetokenswith
textualinputsusingcontrastivelearning. Themulti-modaltrainingofVILA-Utakesadvantageofa
unifiednext-tokenpredictionobjectiveforbothvisualandtextualtokensonasmall-sizehigh-quality
image-textcorpus.
WeevaluateVILA-Uoncommonvisuallanguagetasks,includingimage-languageunderstanding,
video-languageunderstanding,imagegenerationandvideogeneration. VILA-Usignificantlynar-
rowsthegapinvisualunderstandingperformancebetweenend-to-endautoregressivemodelsand
continuous-tokenVLMs,whileintroducingcompetitivenativevisualgenerationcapabilities.
2 RelatedWork
LargeLanguageModels(LLMs). LLMsbasedonpre-trainedlarge-scaletransformers[68]has
drastically revolutionized natural language processing field. Featuring gigantic model size and
pre-training data corpus, LLM has achieved remarkable performance on various linguistic tasks.
The development of open-source LLMs such as LLaMA [67], Mixtral [29] and Vicuna [13] has
furtherednourishedresearchonhowtoadoptLLMforcomplexlanguagetasks. Besidesexcellent
zero-shotgeneralizabilitytodiversedomains,LLMiscommonlyfinetunedoncustomdatasetsfor
betterperformanceonspecifictasks. Instructiontuning[55,14,56]alsostandsasakeystepforbetter
outputsinapplyingLLMs. Inthiswork,weadopttheLLaMA-2-7B[67]modelasourbasicLLM.
VisualLanguageModels(VLMs). Combiningcomputervisionandnaturallanguageprocessing
givesrisetoVLMinthisLLMera. InVLMs,researchersleveragevisionfoundationmodelssuch
asCLIP[58],BLIP[38]andCoCa[74]toextractvisualfeatures,alignwithtexts,andfeedthem
into LLM to achieve the cross-modality understanding between texts and visual content. Build-
inguponsuchprogress,manyVLMs[3,36,51,45]havebeendesignedandtrainedonextensive
vision-languagedatatoachieveremarkableperformanceonvisualunderstandingandreasoningtasks.
VLMsaredividedintotwotypes. (1)BLIP-styleVLMs[4,3,39,37,16,26]utilizescrossattention
mechanismtofuselanguageandvisualinformationandoptionallyapplyperceivers[28]todownsam-
plevisualtokens. (2)LLaVA-styleVLMs[50,20,11,1,80,72,5,2,12,49,45,79]convertsvisual
inputstotokens(patches)andpassthemthroughViTs.TheoutputofViTsundergoesMLPlayersand
getsalignedtothelanguagespace. Inthiswork,weaimtodevelopaVLMwithvisualunderstanding
capacitiescomparabletopriorworks,whilealsopossessingthenewcapacityofvisualgeneration.
Unified Visual Language Models. Numerous efforts have been made to develop unified visual
languagemodelscapableofgeneratingbothtextandvisualcontent,includingimagesandvideos.
TherearetwomainstreammethodstogeneratevisualcontentinVLMs. Manyworks[64,63,31,
30, 41] combine VLMs with diffusion models like Stable Diffusion [60] for high-quality image
generation. Otherworks[48,75,52,65,70]adoptVQGAN-basedvisionencoderstoconvertvisual
inputs into discrete tokens and make LLMs learn to predict them. In this work, we design our
frameworkbasedontheautoregressivenext-tokenpredictionmethodforvisualgenerationandmake
ourVLMlearntogeneratevisualcontenteffectivelyandefficiently.
2Multi-modal Multi-modal Token Multi-modal Token Multi-modal
Inputs Sequence In Sequence Out Inference Outputs
Text 📄 Text … Text The man is
Encoder Decoder skating
Text Tokens
Generative
Image 🖼 Multi-modal Model
Text-aligned
…………… Vision … Vision ……………
Decoder
Encoder Text-aligned Discrete
Video 🎞 Visual Tokens Next-token Prediction
Figure1: Anoverviewofourframework’smulti-modaltrainingandinferenceprocess. Visual
inputsaretokenizedintodiscretetokensandconcatenatedwithtextualtokenstoformamulti-modal
tokensequence. Alltokensareinvolvedinournext-tokenpredictionprocess, enablingaunified
trainingobjective. Duringinference,theoutputtokensaredecodedbyourtextdetokenizerorvision
towerdecodertoyieldmulti-modalcontent.
3 Methods
This work proposes a multi-modal framework that aims to unify visual and language modalities
efficientlyandeffectively. Thekeycomponentsenablingsuchunificationareaunifiedfoundation
visiontowerthatconvertsvisualinputsintodiscretetokensalignedwithtext,andaunifiedmulti-
modalgenerativetrainingprocedure. Anoverviewofthemainmulti-modaltrainingandinference
processwithinourframeworkisdepictedinFigure1.
3.1 UnifiedFoundationVisionTower
Tosupportdiversevisualunderstandingandgenerationtasks, wefirstbuildaunifiedfoundation
visiontowertoprovideappropriatevisualfeatures. Weproposetoincludetext-imagecontrastiveloss
andVQ-basedimagereconstructionlossinourvisiontowertraining,empoweringthetextalignment
anddiscretetokenizationabilitiesforourvisiontower. AsdepictedinFigure2,thefeaturesextracted
fromimagesareprimarilydiscretizedthroughresidualquantization. Theninoneroute,thediscrete
visualfeaturesarefedintoadecodertoreconstructtheimageandcomputethereconstructionloss;
ontheotherroute,wecomputetheimage-textcontrastivelossbetweenthediscretevisualfeatures
andthetextualfeaturesprovidedbyatextencoder. Withthistrainingprocedure,thevisiontower
learnstoextractdiscretefeaturessuitableforbothunderstandingandgenerationinourVLM.
UnifiedTrainingRecipe. Astraightforwardcombinationofcontrastiveandreconstructionloss
cannotconverge. Thisisbecausealignmentandreconstructiontasksrequirehigh-levelsemanticand
low-levelappearancefeatures,respectively. Trainingtheentirevisiontowerfromscratchwithboth
objectivescouldinduceconflictinggoals. Inpractice,weobservethattrainingthevector-quantized
visiontowerfromscratchwithbothimagereconstructionandcontrastivelossresultsinamere5%
Top-1accuracyforzero-shotimageclassificationonImageNet[17]afterseveralepochsoftraining.
Toaddressthisissue,weexperimentwithdifferenttrainingrecipesandfindthefollowingsolutionto
bemosteffective. Insteadoflearningbothobjectivessimultaneously,ourtrainingrecipesuggests
firstequippingthemodelwithtext-imagealignmentabilityandthenlearningreconstructionwhile
maintaining alignment ability. We initialize the vision encoder and text encoder with pretrained
weights from the CLIP model to ensure good text-image alignment. Next, we freeze the text
encoderandkeepallvisioncomponentstrainableusingbothcontrastiveandreconstructionloss. The
contrastivelossmaintainsalignmentability,whilethereconstructionlossdevelopsreconstruction
ability. Thistrainingapproachconvergesquicklyandyieldsstrongperformance. Thepre-trained
CLIPweightscontainlearnedhigh-levelpriors,whicharedifficultandcomputationallyexpensiveto
learnfromscratch. Initializingwiththeseweightsenablesthebindingoflow-levelandhigh-level
featuresmuchfasterandmoretractablyforthevisionencoder. Withthistrainingrecipe,wecan
efficiently train a vision tower that exhibits both good text alignment and image reconstruction
abilities. We use weighted sum to combine the text-image contrastive loss and VQ-based image
reconstructionloss:
3
…
…
…
…
…
…Reconstruction
Loss
🔥 🔥
Vision Residual Vision
Encoder Quantizer🔥 … Decoder
Pretrained Discrete Visual Tokens
This is an image Text Encoder … Contrastive Frozen weights 🔥 Trainable weights
of a cute cat Pretrained Loss
Text Tokens
Figure 2: Overview of our unified foundation vision tower. Given input images the features
extractedbythevisionencoderarediscretizedusingresidualquantization. Thenthediscretevision
featuresaremeanwhileputintothevisiondecodertoreconstructimagesandusedtoperformthe
text-imagealignment. Duringthisprocess,thereconstructionlossandcontrastivelossarecomputed
toupdatethevisiontower,endowingittoproducediscretevisualfeatureswithtextalignment.
L =w L +w L (1)
total contra contra recon recon
Inourexperiments,wepickw =1andw =1.
contra recon
Discussion:FailedTrainingRecipes. Weexperimentwithnumeroustrainingrecipesandfindnone
tobeaseffectiveasourfinalapproach. Welistfouralternativerecipesanddiscusstheirshortcomings
comparedtoourfinalrecipe: (1)Loadpre-trainedCLIPweightsintothetextencoderonly;(2)Load
pre-trained RQ-VAE weights for the vision encoder and decoder while training other parts from
scratch;(3)Freezethevisionencoder;(4)Makethetextencodertrainable.
Recipes1)and2)failduetothelackofpre-trainedCLIPweightsforthevisionencoder. Traininga
CLIPmodelfromscratchtypicallyrequiresnumerousGPUdayswithalargeglobalbatchsize(e.g.,
32k). However,VQ-basedreconstructiontrainingnecessitatesarelativelysmallglobalbatchsize
(e.g.,512)forsteadyimprovement. Withsuchasmallbatchsize,trainingatext-alignedvisiontower
fromscratchwouldbeprohibitivelytime-consumingandresource-intensive.
Recipe3)failsbecausefreezingthevisionencoderpreventsitfromlearningthelow-levelfeatures
essentialforreconstruction. Inthiscase, theburdenofreconstructionfallsentirelyonthevision
decoder,butitisimpossibletoreconstructimageswellusingonlysemanticfeatures.
Recipe4)failsbecausethequantizedfeaturesarechaoticduringtheinitialtrainingsteps,andthe
contrastivelossdisruptsthetextencoderweights,slowingdowntheentiretrainingprocess.
In contrast, our final training recipe leverages pre-trained CLIP weights for the vision encoder,
enablingittomaintainlearnedsemanticfeaturesratherthangraspingthemfromscratch. Thisallows
ustotrainwithasmallbatchsizewhilekeepingthevisionencodertrainable,facilitatingthelearning
oflow-levelfeaturesforreconstructionduringtraining.
ResidualVectorQuantization. Ourvisualfeaturesarediscretelyquantized,sotheirrepresentation
ability heavily depends on the code size used in our quantizer. Since we hope they contain both
high-levelandlow-levelfeatures,weneedmorecapacitiesintheirvectorfeaturespace,makinga
largercodesizenecessaryforgoodperformanceindownstreamtasks. However,toomanycodes
foreachimagewillresultintoomanytokensforLLMtoproduceinthevisualgenerationprocess,
incurring much latency. So in an attempt to increase the vector feature capacity and meanwhile
maintainareasonablenumberoftokensforLLM,weadoptaresidualvectorquantizationmethod
followingRQ-VAE[33]todiscretizeavectorzasDdiscretecodes:
RQ(z;C,D)=(k ,··· ,k )∈[K]D, (2)
1 D
where C is the codebook, K = |C| and k is the code of z at depth d. Starting with r = z, we
d 0
recursivelyperformvectorquantizationby
k =Q(r ,C),
d d−1
(3)
r =r −e(k ),
d d−1 d
foreachdepthd = 1,2,··· ,D,whereeisthecodebookembeddingtableandQisthestandard
vectorquantization:
Q(z;C)=argmin∥z−e(k)∥2.
2 (4)
k∈[K]
4Thequantizedvectorforzisthesumoverthedepthdim:
z=(cid:80)D
e(k ). Intuitively,ineachdepth
(cid:98) i=1 i
wechooseacodetoreducethequantizationerror. Socomparedtothestandardvectorquantization
methods,wehaveDcodestoquantizeonevector,allowingforfinerapproximationandlargerfeature
space. Duringmulti-modaltrainingandinference,LLMonlyneedstopredictthecodeembedding,
withcodesindifferentdepthsequentiallyproducedbyadepthtransformertakingthecodeembedding
astheinitialinput,aswewillintroduceinSection3.2. Sowiththisresidualquantization,wecan
enhancetherepresentationcapabilityofourvisiontowerwhileincurringlittlelatency.
3.2 UnifiedMulti-modalGenerativePre-training
Figure1presentsanoverviewofourunifiedmulti-modalpre-trainingprocess. Ourvisiontower
encoderprocessesvisualinputssequentially,generatinga1Dtokensequence. Thissequenceisthen
concatenatedwithtexttokenstoformamulti-modalsequence. Todistinguishbetweenmodalities
andenablevisualcontentgeneration,weinsertspecialtokens: <image_start>and<image_end>
atthestartandendofimagetokens,and<video_start>and<video_end>atthestartandend
ofvideotokens. Videotokensarethedirectconcatenationofmulti-frameimagetokens.
Pre-trainingdataform. Intermsofunifiedpre-trainingdata,weleveragedifferentconcatenation
formsbetweentextandvisualtokenstofacilitatebothunderstandingandgeneration.Weuse[image,
text],[text, image],and[text, video]forms,withsupervisionlossaddedonlyonthelatter
modalityineachpairtoavoidunconditionalcontentgenerationandpromotemodalityalignment.
Wealsoemployaninterleavedtextandimageconcatenationformforenhancedunderstanding,with
supervisionlossappliedsolelytothetext. Notably,weexcludethe[video, text]formduring
pre-trainingforefficiencyreasons,aswefindincorporatingitduringsupervisedfine-tuningeffectively
yieldsexcellentvideounderstandingability.
TrainingObjective. Sincebothvisualtokensandtexttokensarediscrete,wecantrainourLLMwith
thegenerallanguagemodelingnext-tokenpredictionobjective. However,duetotheuseofresidual
quantizationforvisualtokens,thetrainingobjectivesfortextandvisualtokensdifferslightly. For
texttokens,thenegativelog-likelihoodlossiscalculatedas
T
(cid:88)
L =− logP (y |y ), (5)
text θ i <i
i=1
whereT isthelengthofthemulti-modalsequenceandionlycountswhenthetexttokenappearsat
positioni. Forvisualtokens,residualquantizationintroducesadepth-stackedstructureofcodesat
eachvisualpositionj. Toaddressthis,weleveragethedepthtransformerintroducedinRQ-VAE
[33]. Specifically,giventhecodeembeddingh generatedbytheLLMforvisualtokensatpositionj,
j
thedepthtransformerautoregressivelypredictsDresidualtokens(k ,...,k ). Duringtraining,the
j1 jD
inputofthedepthtransformerv atdepthdisdefinedasthesumofthecodeembeddingsofupto
jd
depthd−1ford>1suchthat
d−1
(cid:88)
v = e(k ), (6)
jd jd′
d′=1
andv =h . Thus,thedepthtransformerpredictsthenextcodeforafinerestimationofthefeature
j1 j
zˆ basedonthepreviousestimationsuptod−1. Thenthenegativelog-likelihoodlossforvisual
j
tokensis
T D
(cid:88)(cid:88)
L =− logP (k |k ), (7)
visual δ jd j,<d
j=1d=1
whereT isthelengthofthemulti-modalsequenceandj onlycountswhenavisualtokenappearsat
positionj. Duringthemulti-modalpre-training,theweightsofthedepthtransformerarerandomly
initializedandupdatedtogetherwiththeLLM.
4 Experiments
Inthissection,weintroducecomprehensiveexperimentstoevaluatetheperformanceofourmethod
onvariousvisualunderstandingandgenerationtasks. Firstly,weoutlineourexperimentalsetup,
5includingthemodelarchitecture,trainingdatasets,andevaluationbenchmarks. Subsequently,we
evaluatetheperformanceofourunifiedfoundationvisiontower. Then,wecompareourmethodwith
otherpopularVLMsonvariousvisualunderstandingandgenerationbenchmarks. Finally,wegive
somequalitativeresults.
4.1 ExperimentalSetup
In our experiments, we employ LLaMA-2-7B [66] as our base language model. For the vision
tower, we choose SigLIP-Large-patch16-256 / SigLIP-SO400M-patch14-384 [77] as our vision
encoder architecture, and adopt the residual quantizer, depth transformer as well as the decoder
architecturefromRQ-VAE[33]. Thequantizercodebooksizeis16384. Allimagesandvideosare
resizedtoaresolutionof256×256/384×384,witheachimageorvideoframeconvertedintoa
16×16×4/27×27×16codewiththeresidualdepthD =4/D =16. Wetrainourvisiontower
onCOYO-700M[6]andevaluateitforzero-shotclassificationandreconstructionperformanceon
ImageNet[18]. Forvisualunderstanding,weleverage1M[image, text]datafromShareGPT4V
[10],6MinterleavedtextandimagedatafromMMC4[81]. Forvisualgeneration,weincorporate
15Mhigh-quality[text, image]datacuratedfromourinternaldatasetand1M[text, video]
datafromOpenVid[54]datasets. Classifier-freeguidance[25]isemployedforvisualgeneration
withaCFGvalueof3.
Forexaminingvisualunderstandingability,weevaluateourmodelonthewidelyadoptedzero-shot
image-basedvisual-languagebenchmarksincludingVQA-v2[24],GQA[27],TextVQA[62],POPE
[42],MME[23],SEED[34],MM-Vet[76]andvideo-basedvisual-languagebenchmarksincluding
ActivityNet[7],MSVD[8],MSRVTT[71],TGIF[43].
Toevaluatethevisualgenerationcapability,weuseMJHQ-30K[35]andGenAI-Bench[46]asour
benchmarks. TheformeradoptstheFIDbetweengeneratedimagesand30Khigh-qualityimagesto
reflecttheoverallcapabilityofimagegeneration. Thelatterisachallengingimage-to-textgeneration
benchmarkthatreflectsthecomprehensivegenerativeabilitiesofvisualgenerationmodels. This
benchmarkisdividedintotwocategoriesofprompts: basicskills,whichincludeattribute,scene,and
relationunderstandingintextinputs,andadvancedskills,whichencompasscounting,differentiation,
comparison,andlogicalrelationunderstandingintextinputs.
4.2 UnifiedFoundationVisionTower
WepresentthecommonlyusedmetricsreconstructionFID(rFID)andTop-1accuracyforzero-shot
imageclassificationonImageNettomeasurethereconstructionandtextalignmentcapabilitiesofthe
unifiedfoundationvisiontowerinTable1. Ourmodelachievessignificantlybetterreconstruction
resultsthanVQ-GAN.OurrFIDisslightlyinferiortothatofRQ-VAEwhenusingthesamecode
shape. Thisisexpectedastheintroductionofcontrastivelossduringtraining,aimedatenhancing
imageunderstanding,ledtoadecreaseinreconstructionquality. Forthetextalignmentcapability,
ourunifiedvisiontowerachievesaTop-1accuracyof73.3/78.0under256/384resolution. This
demonstratestheexceptionaltextalignmentcapabilityofourunifiedvisiontower. However,itis
worthnotingthatboththerFIDandTop-1accuracyofthevisiontoweronlyservesasamedium
indicator instead of directly linear correlated to the final performance of our whole multi-modal
framework. Theperformanceonvisualunderstandingandgenerationtaskspresentedinthefollowing
sectionsismoreimportant.
Table1: ThereconstructionFID(rFID)andTop-1accuracyforzero-shotimageclassificationofour
unifiedvisiontoweronImageNet.
Model PretrainedWeights Resolution ShapeofCode rFID↓ Top-1Accuracy↑
VQ-GAN[22] – 256×256 16×16 4.98 –
RQ-VAE[33] – 256×256 8×8×4 3.20 –
RQ-VAE[33] – 256×256 16×16×4 1.30 –
Ours SigLIP-Large 256×256 16×16×4 1.80 73.3
Ours SigLIP-SO400M 384×384 27×27×16 1.25 78.0
6Table 2: Comparison with leading methods on image-based visual language benchmarks. Our
performanceisclosetoleadingVLMs,surpassingmanymethodsbyalargemarginunderthesame
LLMsize,evenwithadiscretevisualtokentype. *indicatesthatimagesinthetrainingsplitofthese
datasetsareobservedduringVLMtraining.
Method LLM VisualToken Res. VQAv2 GQA TextVQA POPE MME SEED MM-Vet
LLaVA-1.5[51] Vicuna-1.5-7B Continuous 336 78.5∗ 62.0∗ 58.2 85.9 1510.7 58.6 30.5
VILA[45] LLaMA-2-7B Continuous 336 79.9∗ 62.3∗ 64.4 85.5 1533.0 61.1 34.9
Unified-IO2[52] 6.8Bfromscratch Continuous 384 79.4∗ – – 87.7 – 61.8 –
InstructBLIP[15] Vicuna-7B Continuous 224 – 49.2 50.1 – – 53.4 26.2
IDEFICS-9B[32] LLaMA-7B Continuous 224 50.9 38.4 25.9 – – – –
Emu[64] LLaMA-13B Continuous 224 52.0 – – – – – –
LaVIT[31] LLaMA-7B Continuous 224 66.0 46.8 – – – – –
DreamLLM[19] Vicuna-7B Continuous 224 72.9∗ – 41.8 – – – 36.6
Video-LaVIT[30] LLaMA-2-7B Continuous 224 80.2∗ 63.6∗ – – 1581.5 64.4 35.0
CM3Leon-7B[75] 7Bfromscratch Discrete 256 47.6 – – – – – –
LWM[48] LLaMA-2-7B Discrete 256 55.8 44.8 18.8 75.2 – – 9.6
Show-o[70] Phi-1.5B Discrete 256 59.3∗ 48.7∗ – 73.8 948.4 – –
Ours LLaMA-2-7B Discrete 256 75.3∗ 58.3∗ 48.3 83.9 1336.2 56.3 27.7
Ours LLaMA-2-7B Discrete 384 79.4∗ 60.8∗ 60.8 85.8 1401.8 59.0 33.5
Table 3: Comparison with leading methods on video-based visual language benchmarks. The
performanceofourmethodisclosetostate-of-the-artVLMs,surpassingmanymethodsunderthe
sameLLMsize,evenwithadiscretevisualtokentype.
Method LLM VisualToken Res. MSVD-QA MSRVTT-QA TGIF-QA ActivityNet-QA
Unified-IO2[52] 6.8Bfromscratch Continuous 384 52.1 42.5 – –
Emu[64] LLaMA-13B Continuous 224 – 18.8 8.3 –
VideoChat[40] Vicuna-7B Continuous 224 56.3 45 34.4 –
Video-LLaMA[78] LLaMA-2-7B Continuous 224 51.6 29.6 – –
Video-ChatGPT[53] LLaMA-2-7B Continuous 224 64.9 49.3 51.4 35.2
Video-LLava[44] Vicuna-7B Continuous 224 70.7 59.2 70.0 45.3
Video-LaVIT[30] LLaMA-2-7B Continuous 224 73.5 59.5 – 50.2
LWM[48] LLaMA-2-7B Discrete 256 55.9 44.1 40.9 –
Ours LLaMA-2-7B Discrete 256 73.4 58.9 51.3 51.6
Ours LLaMA-2-7B Discrete 384 75.3 60.0 51.9 52.7
4.3 QuantitativeEvaluation
VisualUnderstandingTasks. Table2andTable3summarizethecomparisonbetweenourmethod
andotherleadingVLMsontheimage-languageandvideo-languagebenchmarksrespectively. Com-
paredtothemainstreamchoiceofcontinuousvisualtokensproducedbyfoundationmodelslike
CLIP,theVQGAN-baseddiscretevisualtokenshavelessalignmentwithtext,thusharmingVLMs’
performanceonvisualunderstandingtasks. Withourunifiedfoundationvisiontower,ourmodelcan
haveaperformanceclosetoleadingVLMsevenwithdiscretevisualtokens.
Visual Generation Tasks. As shown in
Table4,VILA-UcanachieveabetterFID
Method Type #Images FID↓
than other autoregressive methods and
SD-XL[57] Diffusion 2000M 9.55
have comparable performance with some
PixArt[9] Diffusion 25M 6.14
diffusionbasedmethods. Thisresultshows Playgroundv2.5[35] Diffusion – 4.48
thefeasibilityofourmethodforvisualgen- LWM[48] Autoregressive – 17.77
eration. Table5summarizesthequantitative Show-o[70] Autoregressive 36M 15.18
resultsofourmethodandothervisualgener- Ours(256) Autoregressive 15M 12.81
ationmethodsonGenAI-Bench. Although Ours(384) Autoregressive 15M 7.69
Our method is inferior to diffusion-based
Table4: Comparisonwithothervisualgenerationmethods
visual generation methods that have been
onMJHQ-30Kevaluationbenchmark.
trained on billions-level image-text pairs,
ourmethodhascomparableperformancewithSDv2.1[61]andSD-XL[57]onadvancedprompts
eventrainedwithmagnitude-levellessdata. ThisfurthershowsthatVILA-Ucanlearnthecorrelation
amongvisualandtextualmodalitieseffectivelyandefficientlywithourunifiedtrainingframework.
7Table5: ComparisonwithothervisualgenerationmethodsonGenAI-Bench[46]. Theresultsshow
that our method outperforms previous autoregressive visual generation methods. For advanced
promptsthatrequirebettertextfollowingabilitytogenerate,ourmethodcanhavearelativelysmall
performancegapwithdiffusion-basedmethods,evenwithmuchlesstrainingdataandtime.
Relation↑
Method Type #TrainingImages Attribute↑ Scene↑ Overall↑
Spatial Action Part
SDv2.1[60] Diffusion 2000M 0.80 0.79 0.76 0.77 0.80 0.78
SD-XL[57] Diffusion 2000M 0.84 0.84 0.82 0.83 0.89 0.83
Midjourneyv6[59] Diffusion – 0.88 0.87 0.87 0.87 0.91 0.87
DALL-E3[47] Diffusion – 0.91 0.90 0.92 0.89 0.91 0.90
LWM[48] Autoregressive – 0.63 0.62 0.65 0.63 0.70 0.63
Show-o[70] Autoregressive 36M 0.72 0.72 0.70 0.70 0.75 0.70
Ours(256) Autoregressive 15M 0.78 0.78 0.77 0.78 0.79 0.76
Ours(384) Autoregressive 15M 0.75 0.76 0.75 0.73 0.75 0.73
(a)VQAScoresonbasicpromptsofGenAI-Bench
Logical↑
Method Type #TrainingImages Count↑ Differ↑ Compare↑ Overall↑
Negate Universal
SDv2.1[60] Diffusion 2000M 0.68 0.70 0.68 0.54 0.64 0.62
SD-XL[57] Diffusion 2000M 0.71 0.73 0.69 0.50 0.66 0.63
Midjourneyv6[59] Diffusion – 0.78 0.78 0.79 0.50 0.76 0.69
DALL-E3[47] Diffusion – 0.82 0.78 0.82 0.48 0.80 0.70
LWM[48] Autoregressive – 0.59 0.58 0.54 0.49 0.52 0.53
Show-o[70] Autoregressive 36M 0.70 0.62 0.71 0.51 0.65 0.60
Ours(256) Autoregressive 15M 0.70 0.71 0.74 0.53 0.66 0.64
Ours(384) Autoregressive 15M 0.68 0.67 0.71 0.51 0.64 0.61
(b)VQAScoresonadvancedpromptsofGenAI-Bench
4.4 QualitativeEvaluation
Visual Understanding. To validate the effectiveness of VILA-U in comprehensive visual
understandingtasks,weapplyitinseveralunderstandingandreasoningtasks,assomeexamples
showninFigure3andFigure4. Fromtheresults,wecanseetheversatilityofVILA-Uinvarious
tasksincludingvisualcaptioningandvisualquestionanswering. Besides,ourmodelhasinherited
some important capabilities from VILA [45] including multi-image understanding, in-context
learning,asshowninFigure5andFigure6.
VisualGeneration. WepresentsomeexamplesofthevisualgenerationresultswithVILA-Uin
Figure7. Wecanseethatourmodelcanbeemployedinbothimagegenerationandvideogeneration,
eventrainedwitharelativelysmalldatacorpus. Inthegivenexamples,ourmethodcangenerate
nice-lookingimagesandcontinuousvideosadheringtousers’input.
5 AblationStudy
Inthissection,wedelveintosomekeydesigncomponentsofourframeworkanddesignablation
experimentstoshowhowtheywillinfluencetheperformance.
Prompt: <video>\n
Elaborate on the visual and narrative elements of the video in detail.
VILA-U(Ours): The video features a person wearing a blue t-shirt, khaki pants, and a cap, executing a series of breakdancing moves on
a concrete road. The background includes a clear sky, a bridge, and a train passing by. The dancer's movements are dynamic, withspins,
flips, and handstands. The lighting is bright, suggesting daytime, and the shadows indicate the sun is high. The dancer's attireis casual,
and the setting appears to be a quiet road with minimal traffic.
Figure3: VILA-Ucancorrectlycaptionvideosandcoverallthedetails,thankstothetextalignment
ofourvisionencoder.
8Input image Question Output Input few shot examples + target image Output
Is there water visible? Yes
is home to camels
What are flying in the sky? Kites
is home to polar bears is home to pandas ?
How many motorcycles are
2
in the picture? is snowy
How many people are on each
0
motorcycle?
is sunny is rainy ?
Figure 4: VILA-U has good visual question Figure 5: VILA-U has good in-context learning
answeringcapability.Theimagesandquestions capability. Wefeedtwoimage-textpairsandathird
arefromthetestsplitofVQAv2dataset. imageasthecontexttoprompttheVLM.
Prompt: Image 1: <img1>\nImage 2: <img2>\n
What is the common thing and the difference between image 1 and image 2?
VILA-U(Ours): The common thing in both images is the presence of a
panda bear. However, the difference between image 1 and image 2 is that
the panda bear in image 1 is lying down on a rock, while the panda bear in
image 2 is lying down on a log.
Prompt: Image 1: <img1>\nImage 2: <img2>\n
What is the common thing and the difference between image 1 and image 2?
VILA-U(Ours): The common thing in both images is the presence of trees,
but the difference between the two images is the angle of the sunlight. In
image 1, the sun is shining through the trees, creating a bright and lively
atmosphere. In contrast, the sun is not visible in image 2, which shows a
darker and more subdued forest scene.
Figure6: VILA-Ucancorrectlyreasonovermultipleimages.
5.1 ImpactofContrastiveLosstoVisualUnderstanding
Weincludecontrastivelossinvisiontowertraining,whichendowsitwiththetextalignmentability.
Duringourmulti-modaltraining,suchtextalignmentabilityiscrucialinenhancingmodalityfusion
andperformanceondownstreamvisuallanguagetasks. Wevalidatetheimportanceofthisalignment
bytrainingthevisiontowerwithandwithoutthecontrastiveloss,evaluatingitsimpactonvisual
languageunderstandingperformance. Forthisablationstudy,werandomlysample25Mdatafrom
COYO-700Mtotrainthevisiontower. Formulti-modaltraining,weuseShareGPT4VandMMC4
withouttext-imageandtext-videodata. TheresultsofthefirsttwolinesinTable6demonstratethe
crucialroleoftextalignmentinachievingstrongvisuallanguageunderstandingperformance. Scaling
thedatasetsizefrom25Mto700Mfurtherenhancesperformance,highlightingtheimportanceof
learningtextalignmentonalarge-scaledataset.
Table6: Impactofcontrastivelosstovisualunderstanding.
PretrainedWeights Datasize LossType Top-1Accuracy VQAv2 POPE MME SEED MM-Vet
SigLIP-Large 25M Recon. – 57.7 75.1 937.7 38.7 15.3
SigLIP-Large 25M Recon.+Contra. 62.9 68.0 83.7 1219 50.4 20.8
SigLIP-Large 700M Recon.+Contra. 73.3 75.3 83.9 1336.2 56.3 27.7
5.2 ImpactofContrastiveLosstoVisualGeneration
Weconducttwoexperimentstodemonstratetheinfluenceofcontrastivelosstogenerationperfor-
mance. Forefficiency,weconductonlytext-to-imagepretrainingandutilizeSheared-LLaMA-1.3B
[69]insteadofLLaMA-2-7BastheLLM.Inthefirstexperiment,weusetheRQ-VAEasthevision
tower,whichhasanrFIDof1.30. Inthesecondexperiment,weemployourunifiedvisiontower.
ResultsareshowninTable7. OurUnifiedVisionToweryieldedslightlyworseFIDresultsthan
9Happy dreamy owl monster sitting on a A cute orange kitten sliding down an aqua
tree branch, colorful glittering particles, A black dog: Crocodile in a sweater: slide, happy excited. Vibrant colors, water
forest background, detailed feathers: splashing on the lens:
Selfie of a w ono m tha en p a lan id n sh :er lion cub wea m a h ri ia d nn d gd l e es yo w em wi t g ihe tl h as2 sk4 asy ny e ice msoa , er l io ts ' sr so t b ysld la u e c p :b k eo g ry r doin eu tnt ah d ie l e d sA nN or oe wra t yhl i e ms rti onc u Ll na ig tn ahd it ns s c rd aa a np n ge c es i nh ingo It o c o v ef e l at rh n ae d : mirA ro d re ee dp p fi f o lo ln er de ds r nt e ic gfll hee tca t sr ii knn ygg : aw git ah l aa x y-
Waves rolling on the sea:
Fireworks exploding in the sky:
Figure7: VILA-Ucangeneratehigh-qualityimagesandvideosgiventextinput.
the RQ-VAE on MJHQ-30K, possibly due to its inferior rFID resulting from the introduction of
contrastiveloss.
Table8: ImpactofCFG.
Table7: Impactofcontrastivelosstovisualgeneration.
CFGValue FID↓
VisionTower LLM Resolution rFID↓ FID↓
1.0 14.1
RQ-VAE[33] Sheared-LLaMA-1.3B 256×256 1.30 12.0 2.0 13.0
Ours Sheared-LLaMA-1.3B 256×256 1.80 13.2 3.0 12.8
5.0 13.2
5.3 ImpactofClassifier-freeGuidance
Weadoptclassifier-freeguidanceduringthevisualcontentgeneration. Weinvestigatetheimpactof
theCFGvalueonour256-resolutionmodel. ResultspresentedinTable8indicatethataCFGvalue
of3.0yieldsthebestFIDscore.
6 Conclusion
WepresentVILA-U,anovelandunifiedvisuallanguagemodelthatintegratesvideo, imageand
languageunderstandingandgenerationtasksintooneautoregressivenext-tokenpredictionframework.
Our method is not only more concise than most VLMs that leverage additional components like
diffusion models for unifying visual generation and understanding, but also demonstrates that
autoregressivemethodscanachievecomparableperformancetostate-of-the-artVLMs. Oursuccess
isduetobothaunifiedfoundationvisiontowerthatalignsdiscretevisualfeatureswithtextsduring
10pre-trainingandahigh-qualitydatasetsuitableforvisualunderstandingandgenerationtraining. We
believeVILA-Ucanserveasageneral-purposeframeworkfordiversevisuallanguagetasks.
Limitations.ThereisstillaperformancegapinthevisualunderstandingabilitybetweenVILA-Uand
state-of-the-artVLMsleveragingcontinuousvisualfeature. Besides,thevisualgenerationqualityis
relativelylowcomparedtostate-of-the-artdiffusionmodels. Infuturework,wewillbecommittedto
overcomingtheselimitationstobuildanadvancedVLMthatcanachievestate-of-the-artperformance
inallkindsofvisuallanguagetasks.
References
[1] ADEPT AI. Fuyu-8B: A multimodal architecture for AI agents. https://www.adept.ai/blog/
fuyu-8b,2023.
[2] Emanuele Aiello, Lili Yu, Yixin Nie, Armen Aghajanyan, and Barlas Oguz. Jointly training large
autoregressivemultimodalmodels. arXivpreprintarXiv:2309.15564,2023.
[3] Jean-BaptisteAlayrac,JeffDonahue,PaulineLuc,AntoineMiech,IainBarr,YanaHasson,KarelLenc,
ArthurMensch,KatherineMillican,MalcolmReynolds,etal. Flamingo: avisuallanguagemodelfor
few-shotlearning. AdvancesinNeuralInformationProcessingSystems,35:23716–23736,2022.
[4] AnasAwadalla,IrenaGao,JoshuaGardner,JackHessel,YusufHanafy,WanrongZhu,KalyaniMarathe,
YonatanBitton,SamirGadre,JeniaJitsev,SimonKornblith,PangWeiKoh,GabrielIlharco,Mitchell
Wortsman,andLudwigSchmidt. Openflamingo,March2023.
[5] JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,XiaodongDeng,YangFan,WenbinGe,YuHan,
FeiHuang,etal. Qwentechnicalreport. Technicalreport,AlibabaGroup,2023. https://arxiv.org/
abs/2303.08774.
[6] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim.
Coyo-700m:Image-textpairdataset. https://github.com/kakaobrain/coyo-dataset,2022.
[7] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A
large-scalevideobenchmarkforhumanactivityunderstanding. InProceedingsoftheieeeconferenceon
computervisionandpatternrecognition,pages961–970,2015.
[8] DavidChenandWilliamBDolan.Collectinghighlyparalleldataforparaphraseevaluation.InProceedings
ofthe49thannualmeetingoftheassociationforcomputationallinguistics:humanlanguagetechnologies,
pages190–200,2011.
[9] JunsongChen, JinchengYu, ChongjianGe, LeweiYao, EnzeXie, YueWu, ZhongdaoWang, James
Kwok,PingLuo,HuchuanLu,etal. Pixart-alpha:Fasttrainingofdiffusiontransformerforphotorealistic
text-to-imagesynthesis. arXivpreprintarXiv:2310.00426,2023.
[10] LinChen,JisongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahuaLin.
Sharegpt4v:Improvinglargemulti-modalmodelswithbettercaptions. arXivpreprintarXiv:2311.12793,
2023.
[11] XiChen,JosipDjolonga,PiotrPadlewski,BasilMustafa,SoravitChangpinyo,JialinWu,CarlosRiquelme
Ruiz,SebastianGoodman,XiaoWang,YiTay,etal. Pali-x: Onscalingupamultilingualvisionand
languagemodel. arXivpreprintarXiv:2305.18565,2023.
[12] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,ZhongMuyan,QinglongZhang,
XizhouZhu,LeweiLu,etal. Internvl: Scalingupvisionfoundationmodelsandaligningforgeneric
visual-linguistictasks. arXivpreprintarXiv:2312.14238,2023.
[13] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,Siyuan
Zhuang,YonghaoZhuang,JosephEGonzalez,etal. Vicuna:Anopen-sourcechatbotimpressinggpt-4
with90%*chatgptquality. Seehttps://vicuna.lmsys.org(accessed14April2023),2(3):6,2023.
[14] HyungWonChung,LeHou,ShayneLongpre,BarretZoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instruction-finetuned language models.
JournalofMachineLearningResearch,25(70):1–53,2024.
[15] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,Boyang
Li,PascaleNFung,andStevenHoi. Instructblip:Towardsgeneral-purposevision-languagemodelswith
instructiontuning. AdvancesinNeuralInformationProcessingSystems,36,2024.
[16] WenliangDai,JunnanLi,DongxuLi,AnthonyMengHuatTiong,JunqiZhao,WeishengWang,BoyangAl-
bertLi,PascaleFung,andStevenC.H.Hoi.Instructblip:Towardsgeneral-purposevision-languagemodels
withinstructiontuning. ArXiv,abs/2305.06500,2023.
[17] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehierarchical
imagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,pages248–255.
Ieee,2009.
11[18] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehierarchical
imagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,pages248–255.
Ieee,2009.
[19] RunpeiDong,ChunruiHan,YuangPeng,ZekunQi,ZhengGe,JinrongYang,LiangZhao,JianjianSun,
HongyuZhou,HaoranWei,etal. Dreamllm:Synergisticmultimodalcomprehensionandcreation. arXiv
preprintarXiv:2309.11499,2023.
[20] DannyDriess,FeiXia,MehdiSMSajjadi,CoreyLynch,AakankshaChowdhery,BrianIchter,Ayzaan
Wahid,JonathanTompson,QuanVuong,TianheYu,etal. Palm-e:Anembodiedmultimodallanguage
model. arXivpreprintarXiv:2303.03378,2023.
[21] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image
synthesis. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages
12873–12883,2021.
[22] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image
synthesis. InProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition,pages
12873–12883,2021.
[23] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,JinruiYang,XiawuZheng,
KeLi,XingSun,YunshengWu,andRongrongJi. Mme: Acomprehensiveevaluationbenchmarkfor
multimodallargelanguagemodels,2024.
[24] YashGoyal,TejasKhot,DouglasSummers-Stay,DhruvBatra,andDeviParikh. MakingtheVinVQA
matter: Elevating the role of image understanding in Visual Question Answering. In Conference on
ComputerVisionandPatternRecognition(CVPR),2017.
[25] JonathanHoandTimSalimans. Classifier-freediffusionguidance. arXivpreprintarXiv:2207.12598,
2022.
[26] WenyiHong,WeihanWang,QingsongLv,JiazhengXu,WenmengYu,JunhuiJi,YanWang,ZihanWang,
Yuxiao Dong, Ming Ding, et al. Cogagent: A visual language model for gui agents. arXiv preprint
arXiv:2312.08914,2023.
[27] DrewAHudsonandChristopherDManning. Gqa: Anewdatasetforreal-worldvisualreasoningand
compositionalquestionanswering. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pages6700–6709,2019.
[28] AndrewJaegle,FelixGimeno,AndyBrock,OriolVinyals,AndrewZisserman,andJoaoCarreira.Perceiver:
Generalperceptionwithiterativeattention. InInternationalconferenceonmachinelearning,pages4651–
4664.PMLR,2021.
[29] AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,ChrisBamford,
DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,FlorianBressand,etal. Mixtralof
experts. arXiv:2401.04088,2024.
[30] YangJin,ZhichengSun,KunXu,LiweiChen,HaoJiang,QuzheHuang,ChengruSong,YuliangLiu,
DiZhang, YangSong, etal. Video-lavit: Unifiedvideo-languagepre-trainingwithdecoupledvisual-
motionaltokenization. arXivpreprintarXiv:2402.03161,2024.
[31] YangJin,KunXu,LiweiChen,ChaoLiao,JianchaoTan,QuzheHuang,CHENBin,ChengruSong,
DiZHANG,WenwuOu,etal. Unifiedlanguage-visionpretraininginllmwithdynamicdiscretevisual
tokenization. InTheTwelfthInternationalConferenceonLearningRepresentations,2023.
[32] HugoLaurençon,DanielvanStrien,StasBekman,LeoTronchon,LucileSaulnier,ThomasWang,Sid-
dharthKaramcheti,AmanpreetSingh,GiadaPistilli,YacineJernite,etal. Introducingidefics:Anopen
reproductionofstate-of-the-artvisuallanguagemodel,2023. URLhttps://huggingface.co/blog/idefics.
Accessed,pages09–18,2023.
[33] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image
generationusingresidualquantization. InProceedingsoftheIEEE/CVFConferenceonComputerVision
andPatternRecognition,pages11523–11532,2022.
[34] BohaoLi,RuiWang,GuangzhiWang,YuyingGe,YixiaoGe,andYingShan. Seed-bench:Benchmarking
multimodalllmswithgenerativecomprehension. arXivpreprintarXiv:2307.16125,2023.
[35] Daiqing Li, Aleks Kamko, Ehsan Akhgari, Ali Sabet, Linmiao Xu, and Suhail Doshi. Playground
v2.5: Threeinsightstowardsenhancingaestheticqualityintext-to-imagegeneration. arXivpreprint
arXiv:2402.17245,2024.
[36] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.Blip-2:Bootstrappinglanguage-imagepre-training
withfrozenimageencodersandlargelanguagemodels. InInternationalconferenceonmachinelearning,
pages19730–19742.PMLR,2023.
[37] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.Blip-2:Bootstrappinglanguage-imagepre-training
withfrozenimageencodersandlargelanguagemodels. arXivpreprintarXiv:2301.12597,2023.
12[38] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip:Bootstrappinglanguage-imagepre-training
forunifiedvision-languageunderstandingandgeneration. InICML,2022.
[39] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip:Bootstrappinglanguage-imagepre-training
for unified vision-language understanding and generation. In International Conference on Machine
Learning,pages12888–12900.PMLR,2022.
[40] KunChangLi,YinanHe,YiWang,YizhuoLi,WenhaiWang,PingLuo,YaliWang,LiminWang,and
YuQiao. Videochat:Chat-centricvideounderstanding. arXivpreprintarXiv:2305.06355,2023.
[41] YanweiLi, YuechenZhang, ChengyaoWang, ZhishengZhong, YixinChen, RuihangChu, Shaoteng
Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision language models.
arXiv:2403.18814,2023.
[42] YifanLi,YifanDu,KunZhou,JinpengWang,WayneXinZhao,andJi-RongWen. Evaluatingobject
hallucinationinlargevision-languagemodels. arXivpreprintarXiv:2305.10355,2023.
[43] YunchengLi,YaleSong,LiangliangCao,JoelTetreault,LarryGoldberg,AlejandroJaimes,andJieboLuo.
Tgif:Anewdatasetandbenchmarkonanimatedgifdescription. InProceedingsoftheIEEEConference
onComputerVisionandPatternRecognition,pages4641–4650,2016.
[44] BinLin,BinZhu,YangYe,MunanNing,PengJin,andLiYuan. Video-llava: Learningunitedvisual
representationbyalignmentbeforeprojection. arXivpreprintarXiv:2311.10122,2023.
[45] JiLin,HongxuYin,WeiPing,YaoLu,PavloMolchanov,AndrewTao,HuiziMao,JanKautz,Mohammad
Shoeybi,andSongHan. Vila:Onpre-trainingforvisuallanguagemodels,2023.
[46] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and
Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. arXiv preprint
arXiv:2404.01291,2024.
[47] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and
Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. arXiv preprint
arXiv:2404.01291,2024.
[48] HaoLiu,WilsonYan,MateiZaharia,andPieterAbbeel.Worldmodelonmillion-lengthvideoandlanguage
withringattention. arXivpreprintarXiv:2402.08268,2024.
[49] HaotianLiu,ChunyuanLi,YuhengLi,andYongJaeLee. Improvedbaselineswithvisualinstruction
tuning. arXivpreprintarXiv:2310.03744,2023.
[50] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. InNeurIPS,2023.
[51] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning.Advancesinneural
informationprocessingsystems,36,2024.
[52] JiasenLu,ChristopherClark,SanghoLee,ZichenZhang,SavyaKhosla,RyanMarten,DerekHoiem,and
AniruddhaKembhavi. Unified-io2: Scalingautoregressivemultimodalmodelswithvision,language,
audio,andaction. arXivpreprintarXiv:2312.17172,2023.
[53] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFahadShahbazKhan. Video-chatgpt:Towards
detailedvideounderstandingvialargevisionandlanguagemodels. arXivpreprintarXiv:2306.05424,
2023.
[54] KepanNan,RuiXie,PenghaoZhou,TiehanFan,ZhenhengYang,ZhijieChen,XiangLi,JianYang,and
YingTai. Openvid-1m: Alarge-scalehigh-qualitydatasetfortext-to-videogeneration. arXivpreprint
arXiv:2407.02371,2024.
[55] OpenAI. Chatgpt. https://openai.com/blog/chatgpt/,2023.
[56] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,ChongZhang,
SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollowinstructionswith
humanfeedback. Advancesinneuralinformationprocessingsystems,35:27730–27744,2022.
[57] DustinPodell,ZionEnglish,KyleLacey,AndreasBlattmann,TimDockhorn,JonasMüller,JoePenna,
andRobinRombach. Sdxl:Improvinglatentdiffusionmodelsforhigh-resolutionimagesynthesis. arXiv
preprintarXiv:2307.01952,2023.
[58] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalconferenceonmachinelearning,pages8748–8763.PMLR,
2021.
[59] ArMoheshRadhakrishnan. Ismidjourney-aithenewanti-heroofarchitecturalimagery&creativity? GSJ,
11(1):94–104,2023.
[60] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages10684–10695,2022.
13[61] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-resolution
imagesynthesiswithlatentdiffusionmodels. InProceedingsoftheIEEE/CVFconferenceoncomputer
visionandpatternrecognition,pages10684–10695,2022.
[62] AmanpreetSingh,VivekNatarajan,MeetShah,YuJiang,XinleiChen,DhruvBatra,DeviParikh,and
MarcusRohrbach. Towardsvqamodelsthatcanread. InProceedingsoftheIEEE/CVFconferenceon
computervisionandpatternrecognition,pages8317–8326,2019.
[63] QuanSun,YufengCui,XiaosongZhang,FanZhang,QiyingYu,ZhengxiongLuo,YuezeWang,Yongming
Rao,JingjingLiu,TiejunHuang,andXinlongWang.Generativemultimodalmodelsarein-contextlearners.
arXivpreprintarXiv:2312.13286,2023.
[64] QuanSun,QiyingYu,YufengCui,FanZhang,XiaosongZhang,YuezeWang,HongchengGao,Jingjing
Liu, Tiejun Huang, and Xinlong Wang. Generative pretraining in multimodality. arXiv preprint
arXiv:2307.05222,2023.
[65] ChameleonTeam. Chameleon:Mixed-modalearly-fusionfoundationmodels,2024.
[66] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama:Openandefficientfoundation
languagemodels. arXivpreprintarXiv:2302.13971,2023.
[67] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,
BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,AurelienRodriguez,ArmandJoulin,Edouard
Grave,andGuillaumeLample. Llama:Openandefficientfoundationlanguagemodels. arXiv:2302.13971,
2023.
[68] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser,andIlliaPolosukhin.Attentionisallyouneed.InI.Guyon,U.VonLuxburg,S.Bengio,H.Wallach,
R.Fergus,S.Vishwanathan,andR.Garnett,editors,AdvancesinNeuralInformationProcessingSystems,
volume30.CurranAssociates,Inc.,2017.
[69] MengzhouXia,TianyuGao,ZhiyuanZeng,andDanqiChen.Shearedllama:Acceleratinglanguagemodel
pre-trainingviastructuredpruning. arXivpreprintarXiv:2310.06694,2023.
[70] JinhengXie,WeijiaMao,ZechenBai,DavidJunhaoZhang,WeihaoWang,KevinQinghongLin,Yuchao
Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify
multimodalunderstandingandgeneration. arXivpreprintarXiv:2408.12528,2024.
[71] DejingXu,ZhouZhao,JunXiao,FeiWu,HanwangZhang,XiangnanHe,andYuetingZhuang. Video
questionansweringviagraduallyrefinedattentionoverappearanceandmotion. InProceedingsofthe25th
ACMinternationalconferenceonMultimedia,pages1645–1653,2017.
[72] QinghaoYe,HaiyangXu,GuohaiXu,JiaboYe,MingYan,YiyangZhou,JunyangWang,AnwenHu,
Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large language models with
multimodality. arXivpreprintarXiv:2304.14178,2023.
[73] JiahuiYu,XinLi,JingYuKoh,HanZhang,RuomingPang,JamesQin,AlexanderKu,YuanzhongXu,
JasonBaldridge,andYonghuiWu. Vector-quantizedimagemodelingwithimprovedvqgan. arXivpreprint
arXiv:2110.04627,2021.
[74] JiahuiYu,ZiruiWang,VijayVasudevan,LeggYeung,MojtabaSeyedhosseini,andYonghuiWu. Coca:
Contrastivecaptionersareimage-textfoundationmodels,2022.
[75] LiliYu,BowenShi,RamakanthPasunuru,BenjaminMuller,OlgaGolovneva,TianluWang,ArunBabu,
BinhTang,BrianKarrer,ShellySheynin,etal. Scalingautoregressivemulti-modalmodels:Pretraining
andinstructiontuning. arXivpreprintarXiv:2309.02591,2(3),2023.
[76] WeihaoYu,ZhengyuanYang,LinjieLi,JianfengWang,KevinLin,ZichengLiu,XinchaoWang,and
LijuanWang. Mm-vet: Evaluatinglargemultimodalmodelsforintegratedcapabilities. arXivpreprint
arXiv:2308.02490,2023.
[77] XiaohuaZhai,BasilMustafa,AlexanderKolesnikov,andLucasBeyer. Sigmoidlossforlanguageimage
pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
11975–11986,2023.
[78] HangZhang,XinLi,andLidongBing. Video-llama:Aninstruction-tunedaudio-visuallanguagemodel
forvideounderstanding. arXivpreprintarXiv:2306.02858,2023.
[79] PanZhang,XiaoyiDongBinWang,YuhangCao,ChaoXu,LinkeOuyang,ZhiyuanZhao,Shuangrui
Ding,SongyangZhang,HaodongDuan,HangYan,etal. Internlm-xcomposer:Avision-languagelarge
modelforadvancedtext-imagecomprehensionandcomposition. arXivpreprintarXiv:2309.15112,2023.
[80] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing
vision-languageunderstandingwithadvancedlargelanguagemodels. arXivpreprintarXiv:2304.10592,
2023.
14[81] WanrongZhu,JackHessel,AnasAwadalla,SamirYitzhakGadre,JesseDodge,AlexFang,YoungjaeYu,
LudwigSchmidt,WilliamYangWang,andYejinChoi. Multimodalc4:Anopen,billion-scalecorpusof
imagesinterleavedwithtext. AdvancesinNeuralInformationProcessingSystems,36,2024.
15