Improved Parallel Algorithm for Non-Monotone Submodular Maximization
under Knapsack Constraint
TanD.Tran1, CanhV.Pham1∗, DungT.K.Ha2, PhuongN.H.Pham3
1ORLab,FacultyofComputerScience,PhenikaaUniversity,Hanoi,Vietnam
2 FacultyofInformationTechnology,VNUUniversityofEngineeringandTechnology,Hanoi,Vietnam
3 FacultyofInformationTechnology,HoChiMinhCityUniversityofIndustryandTrade,HoChiMinh,
Vietnam
canh.phamvan@phenikaa-uni.edu.vn,{22027005,20028008}@vnu.edu.vn,phuongpnh@huit.edu.vn
Abstract Formally, a SMK problem can be defined such as given
a ground set V of size n, a budget B > 0, and a non-
This work proposes an efficient parallel algorithm negative submodular set function (not necessary monotone)
for non-monotone submodular maximization un- f : 2V (cid:55)→ R . Every element e ∈ V has its positive cost
+
deraknapsackconstraintproblemovertheground c(e). The problem SMK asks to find S ⊆ V subject to
set of size n. Our algorithm improves the best c(S)=(cid:80) c(e)≤Bthatmaximizesf(S).
e∈S
approximation factor of the existing parallel one OneofthemainchallengesofSMKisaddressingbigdata
from 8+ϵ to 7+ϵ with O(logn) adaptive com-
in which the sizes of applications can grow exponentially.
plexity. The key idea of our approach is to cre-
Themodernapproachistodesignapproximationalgorithms
ate a new alternate threshold algorithmic frame-
with low query complexity representing the total number of
work. Thisstrategyalternatelyconstructstwodis- queries to the oracle of f. However, required oracles of f
jointcandidatesolutionswithinaconstantnumber
are often expensive and may take a long time to process on
ofsequencerounds. Then,thealgorithmboostsso-
themachinewithinasinglethread.Therefore,peoplethinkof
lutionqualitywithoutsacrificingtheadaptivecom-
designingefficientparallelalgorithmsthatcanleverageparal-
plexity.Extensiveexperimentalstudiesonthreeap-
lelcomputerarchitecturestoobtainagoodsolutionpromptly.
plications,RevenueMaximization,ImageSumma- Thismotivatestheadaptivecomplexityoradaptivity[Balkan-
rization, and Maximum Weighted Cut, show that skiandSinger,2018]tobecomeanimportantmeasurementof
ouralgorithmnotonlysignificantlyincreasessolu-
parallelalgorithms. Itisdefinedasthenumberofsequential
tion quality but also requires comparative adaptiv-
roundsneededifthealgorithmcanexecutepolynomialinde-
itytostate-of-the-artalgorithms.
pendentqueriesinparallel. Therefore,thelowertheadaptive
complexityofanalgorithmis,thehigheritsparallelismis.
Intheeraofbigdatanow,severalalgorithmsthatachieve
1 Introduction
near-optimal solutions with low adaptive complexities have
A wide range of instances in artificial intelligence and ma- beendevelopedrecently(SeeTable1foranoverviewoflow
chine learning have been modeled as a problem of Sub- adaptivealgorithms). Ascanbeseen,althoughrecentstudies
modular Maximization under Knapsack constraint (SMK) make an outstanding contribution by significantly reducing
such as maximum weighted cut [Amanatidis et al., 2020; the adaptive complexity of a constant factor approximation
Han et al., 2021], data summarization [Han et al., 2021; algorithm from O(log2n) to O(logn), there are two draw-
Mirzasoleimanetal., 2016], revenuemaximizationinsocial backs, including(1)thehighquerycomplexitiesmakethem
networks [Han et al., 2021; Cui et al., 2023a; Cui et al., become impractical in some instances [Ene and Nguyen,
2021], recommendation systems [Amanatidis et al., 2021; 2020]and(2)thereisahugegapbetweenthehighapproxi-
Amanatidis et al., 2020]. The attraction of this problem mationfactorsoflowadaptivityalgorithms,e.g. [Amanatidis
comes from the diversity of submodular utility functions et al., 2021; Cui et al., 2023a; Cui et al., 2023b], compared
and the generalization of the knapsack constraint. The sub- to the best one, e.g. [Buchbinder and Feldman, 2019]. This
modular function has a high ability to gather a vast amount raisestousaninterestingquestion: Isitpossibletoimprove
of information from a small subset instead of extracting a the factor of an approximation algorithm with near-optimal
whole large set, while the knapsack constraint can represent adaptivecomplexityofO(logn)?
the budget, the cardinality, or the total time limit for a re-
Our contributions. In this work, we address the above
source. Hence, peopleareinterestedinproposingexpensive
question by introducing the AST algorithm for the non-
algorithms for SMK these years [Amanatidis et al., 2021;
monotone SMK problem. AST has an approximation factor
Han et al., 2021; Cui et al., 2023a; Pham et al., 2023;
Amanatidisetal.,2020]. of 7+ϵ, within a pair of O(logn) adaptivity, O˜(nk) query
complexity, where ϵ is a constant input. Therefore, our al-
∗Correspondingauthor gorithm improves the best factor of the near-optimal adap-
4202
peS
6
]IA.sc[
1v51440.9042:viXraReference ApproximationFactor AdaptiveComplexity QueryComplexity
[BuchbinderandFeldman,2019] 2.6 poly(n) poly(n)
[Hanetal.,2021] 4+ϵ O(nlogk) O(nlogk)
[Phametal.,2023] 4+ϵ O(n) O(n)
[Eneetal.,2019] e+ϵ O(log2n) O˜(n2)
[Amanatidisetal.,2021] 9.465+ϵ O(logn) O˜(n2)
[Cuietal.,2023a](Alg.3) 8+ϵ O(logn) O˜(nk)
√
[Cuietal.,2023a](Alg.5) 5+2 2+ϵ≈7.83+ϵ O(log2n) O˜(nk)
AST(Algorithm1,thiswork) 7+ϵ O(logn) O˜(nk)
Table1:AlgorithmsforSMKproblem.WeusetheO˜notationthroughoutthepapertohidepoly(logn)factorsandkisthelargestcardinality
ofanyfeasiblesolution.Boldfontindicatesthebestresult(s)ineachsetting.
tive complexity algorithm in [Cui et al., 2023a]. We inves- carefully analyze the role of the highest cost element in the
tigatetheperformanceofouralgorithmonthreebenchmark optimalsolutiontodeservemoretightnessfortheproblem.
applications: RevenueMaximization,ImageSummarization,
and Maximum Weighted Cut. The results show that our al- 2 RelatedWorks
gorithm not only significantly improves the solution quality
This section focuses on the related works for the non-
butalsorequirescomparativeadaptivitytoexistingpractical
monotonecaseoftheSMKproblem.
algorithms.
Firstly,regardingthenon-adaptivealgorithms,thefirstal-
New technical approach. It is noted that one popular ap-
gorithm for the non-monotone SMK problem was due to
proach to designing parallel algorithms with near-optimal [Lee et al., 2010] with the 5 + ϵ factor and polynomial
adaptivity of O(logn) is based on making multiple guesses
query complexity. Later, several works concentrated on
of the optimal solution in parallel and adapting a thresh-
improving both approximation factor and query complex-
old sampling method1, which selects a batch of elements ity [Buchbinder and Feldman, 2019; Gupta et al., 2010;
whose density gains, i.e., the ratio between the marginal
Mirzasoleiman et al., 2016; Li, 2018; Sun et al., 2022;
gain of an element per its cost, are at least a given thresh- Pham et al., 2023; Han et al., 2021]. In this line of works,
old within O(logn) adaptivity [Amanatidis et al., 2021; algorithm of [Buchbinder and Feldman, 2019] archived the
Cui et al., 2023a]. By making the guesses of the optimal
best approximation factor of 2.6 but required a high query
along with calling the threshold sampling multiple times in complexity; the fastest algorithm was proposed by [Pham
parallel,theexistingalgorithmscouldkeeptheadaptivecom- et al., 2023] with 4 + ϵ factor in linear queries. For the
plexityofO(logn)andobtainsomeapproximationratios.
non-monotone Submodular Maximization under Cardinality
From another view, we introduce a novel algorithmic
(SMC) problem, which finds the best solution that does not
framework named “alternate threshold” to improve the ap-
exceedkelementstomaximizeasubmodularobjectivevalue,
proximation factor to 7 + ϵ but keep the same adaptivity the best factor of 2.6 of the algorithm in [Buchbinder and
and query complexity with the best one [Cui et al., 2023a]. Feldman, 2019] still held. Besides, a few algorithmic mod-
Firstly,weadaptanexistingadaptivealgorithmtofindanear- elshavebeenproposedforimprovingrunningtime[Badani-
optimalsolutionwithinO(logn)adaptivityandgiveaO(1)
diyuru and Vondra´k, 2014; Kuhnle, 2021b; Li et al., 2022;
numberofguessesoftheoptimalsolution. Then,thecoreof Buchbinderetal.,2015]. Amongthem,thefastestalgorithm
our framework consists of a constant number of iterations. belonged to [Buchbinder et al., 2015] that provided a e+ϵ
It initiates two disjoint candidate sets and then adapts the factorwithinO(nlog(1/ϵ)/ϵ2)queries. However,theabove
thresholdsamplingtoupgradethemalternatelyduringitera-
approaches couldn’t be parallelized efficiently by the high
tions:oneisupdatedatodditerations,andanotherisupdated
adaptivecomplexityofΩ(n).
ateveniterations.Thankstothisstrategy,wecanfindthecon-
The adaptive complexity was first proposed by [Balka-
nection between two solutions for supporting each other in nski and Singer, 2018] for the SMC problem. Regarding
evaluatingthe“utilityloss”aftereachiteration. Attheendof
adaptivity-basedalgorithmsfornon-monotoneSMK,thefirst
thisstage,weenhancethesolutionqualitybyfindingthebest one belonged to [Ene and Nguyen, 2019] with e + ϵ and
elementtobeaddedtoeachcandidate’ssubsets(pre-fixesof O(log2n) adaptive complexity. However, due to the high
ielements)withoutviolatingthebudgetconstraint.
querycomplexityofaccessing,themulti-linearextensionofa
It must be noted that our method differs from the Twin
submodularfunctionanditsgradientintheirmethodbecomes
Greedy-based algorithms [Han et al., 2020; Pham et al.,
impractical in real applications [Amanatidis et al., 2021;
2023; Sun et al., 2022], which update both candidate sets
Fahrbach et al., 2019]. After that, [Amanatidis et al.,
at the same iterations but do not allow the integration of the
2021]deviseda(9.465+ϵ)-approximationalgorithmwithin
thresholdsamplingalgorithmforparallelization. Besides,we
O(logn),whichwasoptimaluptoaΘ(loglog(n))factorby
1WerefertothresholdsamplingmethodsasThreshSeqin[Ama- adopting the lower bound in [Balkanski and Singer, 2018].
natidis et al., 2021] and RandBatch in [Cui et al., 2023a] with It is noted that improving the adaptive complexity of a con-
O(logn)adaptivity. stant factor algorithm from O(log2n) to O(logn) made anoutstandingcontributionsinceitgreatlyreducedthenumber set S ⊆ V is defined as f(e|S) = f(S ∪{e})−f(S) and
of sequential rounds in practical implementation [Cui et al., f({e})iswrittenasf(e)foranye∈V.
2023a;EneandNguyen,2020;Fahrbachetal.,2019]. More In this paper, we design a parallel algorithm based on
recently,[Cuietal.,2023a]createdabigstepwhencontribut- Adaptive complexity or Adaptivity, which is defined as fol-
inganefficientparallelone,whichresultedinafactorof8+ϵ lows:
within a pair of O(logn) adaptivity and O˜(nk) query com- Definition1(AdaptivecomplexityorAdaptivity[Balkanski
plexity. Nevertheless,thisfactorstillhasahugegapwiththe and Singer, 2018]). Given a value of oracle of f, the adap-
bestfactorof2.6[BuchbinderandFeldman,2019].Theyalso tivityoradaptivecomplexityofanalgorithmistheminimum
provided an enhanc √ed version of increasing the approxima- number of rounds needed such that in each round, the algo-
tionfactorto5+2 2+ϵ ≈ 7.83+ϵ. However,itrequired rithm makes O(poly(n)) independent queries to the evalua-
a higher adaptivity of O(log2n). Thus, from this view, the tionoracle.
aboveresultof[Cuietal.,2023a]isthebestoneuntilnow. In the following, we recap two sub-problems which our
People have also focused on developing parallel algo- algorithm need to solve: Unconstrained Submodular Maxi-
rithms for non-monotone SMC these years [Kuhnle, 2021a; mizationandDensityThreshold.
EneandNguyen,2020;Fahrbachetal.,2019],etc. Thefore-
Unconstrained Submodular Maximization (UnSubMax)
mentionedcontributionsof[EneandNguyen,2020]wasalso
ThisproblemrequirestofindasubsetS ⊆V thatmaximizes
appliedforSMCtogetthebestapproximationfactorofe+ϵ,
f(S) without any constraint. The problem was shown NP-
however it used multi-linear extension and thus had a high
hard[Feigeetal.,2011a].
query complexity. Next, [Kuhnle, 2021a] and [Fahrbach et
To obtain mentioned approximation factor, our algorithm
al.,2019]triedtoreducetheadaptivecomplexitytoO(logn)
adapts the low adaptivity algorithm in [Chen et al., 2019]
with25.64+ϵand6+ϵfactors.However,[ChenandKuhnle,
that achieves an approximation factor of (2 + ϵ) in con-
2022] claimed that both [Kuhnle, 2021a] and [Fahrbach et
stantadaptiveroundsofO(log(1/ϵ)/ϵ)andlinearqueriesof
al.,2019]hadanon-trivialerrorbecausetheyusedthesame
O(nlog3(1/ϵ)/ϵ4).
threshold sampling subroutine which did not work for the
non-monotoneobjectivefunction. [ChenandKuhnle, 2022] DensityThreshold(DS). Theproblemreceivesaninstance
furthertriedtofixedthepreviousworkandrecoveredthe6+ϵ (f,V,B),afixedthresholdτ andaparameterϵ>0asinputs,
factor in O(logn). Recently, [Amanatidis et al., 2021] im- it asks to find a subset S ⊆ V satisfies two conditions: (1)
(cid:80)
proved the factor to 5.83+ϵ in O(logn) adaptivity. Later, f(S)≥c(S)·τ;(2) e∈V\Sf(e|S)≤ϵ·OPT.
theworkof[Cuietal.,2023a]archivedthefactorof8+ϵin Twoalgorithmsintheliteraturesatisfytheaboveconditions,
O(logn)adaptivityor4+ϵfactorinO(log2n). including those in [Amanatidis et al., 2021] and [Cui et al.,
After all, our algorithm overcome the existing drawbacks 2023a]. In this work, we adapt the RandBatch algorithm in
byanimprovedparallelversionwiththeapproximationfactor [Cuietal.,2023a]. RandBatchrequiresthesetI,asubmod-
increasingto7+ϵwithinO(logn)roundstoparallelO˜(nk) ular function f(·), and parameters ϵ,M to control the solu-
queries. tion’s accuracy and complexities. RandBatch is combined
withtheaforementioneddensitythresholdstosetupsievesin
3 Preliminaries parallel for SMK. Due to the space limitations, Pseudocode
forRandBatchisgivenintheappendix.
GivenagroundsetV ={e ,...,e }andanutilitysetfunc-
1 n Foraninstance(V,f,B)ofSMK,twosubsetsI,M ofV,
tionf :2V (cid:55)→R tomeasurethequalityofasubsetS ⊆V,
+ a fixed threshold θ and input parameter ϵ. The performance
weusethedefinitionofsubmodularitybasedonthediminish-
ofRandBatchisprovidedinthefollowingLemmas.
ing return property: f : 2V (cid:55)→ R . f is submodular iff for
+ Lemma 1 (Lemma 1 in [Cui et al., 2023a]). The sets
anyA⊆B ⊆V ande∈V \B,wehave
A, L output by RandBatch(θ,I,M,ϵ,f(·),c(·)) satisfy
f(e|A)≥f(e|B). E[f(A)]≥(1−ϵ)2θ·E[c(A)]andϵ·M ·(cid:80) f(u|A)≤
u∈L
OPT.
Eachelemente∈V isassignedapositivecostc(e)>0. Let
c : 2V (cid:55)→ R+ beacostfunction. Assumethatcismodular, Lemma2(Lemma2in[Cuietal.,2023a]). RandBatchhas
(cid:80) O(1(log(|I|·β(I))+M)) adaptivity, and its query com-
i.e.,c(S)= c(e)suchthatc(S)=0iffS =∅. ϵp
e∈S
TheproblemSMKaskstofindS ⊆ V subjecttoc(S) = plexity is O(|I|k) times of its adaptive complexity, where
(cid:80) c(e) ≤ B thatmaximizesf(S). Wedenotebyatuple β(I) = max c(u). If we use binary search in Line 10 of
e∈S u,v c(v)
(f,V,B) an instance of SMK. Without loss of generality, f RandBatch, then it has O(1(log(|I|·β(I))+M)log(k))
isassumednon-negative,i.e.,f(X) ≥ 0forallX ⊆ V and ϵp
adaptivity, and its query complexity is O(|I|) times of its
normalized, i.e., f(∅) = 0. We also assume there exists an
adaptivity.
oraclequery,which,whenqueriedwiththesetS returnsthe
Interestingly, we further explore a useful property of
valuef(S).
RandBatchwhenapplyingittoouralgorithm.
For convenience, we denote by S ∪e as S ∪{e}. Next,
we denote by O an optimal solution with the optimal value Lemma 3. The sets A, L output by
OPT = f(O)andr = argmax o∈Oc(o). Wealsodefinethe RandBatch(θ,I,M,ϵ,f(·),c(·)) satisfy E[f(a i|A i−1)] ≥
contribution gain of a set T to a set S as f(T|S) = f(T ∪ (1 − ϵ)2E[c(a i)]θ, where A = {a 1,a 2,...,a |A|},A i =
S)−f(S). Also, thecontributiongainofanelementetoa {a ,a ,...,a }.
1 2 i4 ProposedAlgorithm
Algorithm1:ASTAlgorithm
In this section, we introduce AST (Algorithm 1), a Input: Aninstance(f,V,B),parametersα,ϵ,δ
(7+ϵ)-approximation algorithm in O(logn) adaptivity and 1: V 0 ←{e∈V :c(e)≤ϵB/n},V 1 ←V \V 0,I ←V 1,
O(n2log2n)querycomplexity. p←1
AST receives an instance (f,V,B), constant parameters 2:
S
0
←ParSKP1( 41,δ,f(·),c(·)),Γ← (8 1α −f 8( δS )ϵ0 B)
δ,ϵ,α as inputs. It contains two main phases. At the first X ←∅,Y ←∅,∆←⌈log 8α ⌉+1,
3: 1 ϵ2(1−8δ)
phase(Lines1-14),itfirstdividesthegroundsetV intotwo 1−ϵ
M ← 1(∆ +1)
subsets: V contains elements with small costs, and V con- ϵ2 2
0 1 fori=1to∆do
tains the rest. AST then calls ParSKP1 [Cui et al., 2023a] 4:
ifiisoddthen
asasubroutinewhichreturnsa(1/8−δ)-approximationso- 5:
θX ←Γ(1−ϵ)i
lution within O(logn) adaptive rounds. Based on that, the 6:
(A ,U ,L )←
algorithmcanofferO(log(1/ϵ)/ϵ)guessesoftheoptimalso- 7: i i i
lutionforthemainloop(Lines4-14). Themainloopconsists
RandBatch(θX,I,M,p,ϵ,f(·|X),c(·))
of O(log(1/ϵ)/ϵ) iterations; each corresponds to a guess of 8: X ←X∪A i,I ←I\X
theoptimal. Itsequentiallyconstructstwodisjointsolutions, 9: else
X and Y, one at odd iterations and the other at even itera- 10: θY ←Γ(1−ϵ)i
tions. Theworkoftheoddandtheevenisthesame. Atthe 11: (B i,U i,L i)←
odd (or even) ones, it sets the threshold θ (θ ) and calls RandBatch(θY,I,M,p,ϵ,f(·|Y),c(·))
X Y
the RandBatch routine with the ground set I and the func- Y ←Y ∪B ,I ←I\Y
12: i
tionf(·|X)(f(·|Y))asinputstoprovidethenewsetA i(B i)
13:
end
(Lines8, 12). ItthenupdatesX (Y)andI astheremaining end
14:
elements(Line8or12). ForT ∈{X,Y},define: T isT aftertheiterationiof
15: i
The second phase (Lines 15-24) is to improve the quality thefirstloop,Tiisthesetoffirstielementsadded
of solutions. If c(X 1 ∪ V 0) ≤ ϵB, this phase first adapts intoT.
UnSubMax algorithm [Chen et al., 2019] for unconstrained ifc(X ∪V )≤ϵBthen
16: 1 0
submodular maximization over X 1 ∪ V 0 to get a candidate 17: S 1 ←UnSubMax(X 1∪V 0)
solutionS (Lines15-16). Thisstepisbasedonanobserva-
1 end
18:
tionthatX isimportantinanalyzingthealgorithm’sperfor-
1 fori=1to|X|do
19:
mance. It then selects the sets of the first i elements added
a ←argmax f(Xi∪{e}),
into X and Y and finds the best elements without violating 20: i e∈V:c(Xi∪e)≤B
X′i ←Xi∪{a }
thetotalcostconstraint(Lines19-24). Finally,thealgorithm i
end
returnsthebestcandidatesolution(Lines25-26). Thedetails 21:
fori=1to|Y|do
ofASTaredepictedinAlgorithm1. 22:
b ←argmax f(Yi∪{e}),
At the high level, AST works follow a novel framework 23: i e∈V:c(Yi∪e)≤B
Y′i ←Yi∪{a }
that combines an alternate threshold greedy algorithm with i
end
the boosting phase. The term “alternate” means that candi- 24:
S ←argmax f(T)
datesolutionsareupdatedalternatelywitheachotherinmul- 25: T∈{X′i}| iX =1|∪{Y′i}| iY =| 1∪{X,Y,S1}
tipleiterations. Ateachiteration,onlyonepartialsolutionis returnS
26:
updatedbasedontwofactors: oneguessoftheoptimalsolu-
tionandtheremainingelementsofthegroundsetthatdonot
belongtotheothersolution. • X and Y are X and Y after the iteration i of the first
i i
Itshouldbeemphasizedthatthealternatethresholdgreedy loop(Lines4-14)andX =Y =∅.
0 0
differs from recent works [Cui et al., 2023a; Amanatidis et
• O is an optimal solution of SMK over instance
al., 2021] where two candidate solutions for each guess are 1
(f,V ,B).
constructed after only one adaptive round. Alternate thresh- 1
oldgreedyalsodiffersfromthetwingreedymethodin[Han • O′ =O \X ,Or =O \{r}andO′r =O′\{r}.
1 1 1
etal.,2020],whichallowsupdatingbothdisjointsetsineach
• Foranelemente ∈ X ∪Y, wedenote: X , Y , θX
iteration. Forthetheoreticalanalysis, thekeytoobtaininga and θY as X, Y, θX and θY right before< ee is s< ele ectee d
tighterapproximationfactorlinesinaspects: (1)theconnec- e
intoX orY,respectively;l(e)istheiterationwheneis
tions between X and Y after each iteration of the first loop
addedintoX orY.
and(2)carefullyconsideringtheroleofr toeliminateterms
thatworsentheapproximationfactor. Lemma 4 makes a connection between X and Y after each
iteration.
WenowanalyzetheperformanceguaranteesofAST. We
considerX andY afterendingthefirstloop. Wefirstintro- Lemma4. Afteranyiterationiofthefirstloop(Lines4-14)
ducesomenotationsregardingASTasfollows. ofAST,wehave:
• Xi,YiisthesetoffirstielementsaddedintoX andY, a) Ifi≥1,iisoddandc(X i)≤B−c(r).LetT ⊆Y i−1∩
respectively. O , we have
(cid:80)
f(e|X ) <
(cid:80) E[f(e|Y<e)]
+ϵ·
1 e∈T i e∈T (1−ϵ)3OPT. T1 = {e ∈ T : f(e|Y<e) < θY ,c(Y )+c(e) ≤ B} and
j j c(e) <e <e
b) OIf ′i ,≥ we2, hi ai vs eev (cid:80)en e∈a Tnd fc (( eY |Yi) i)≤ <B (cid:80)−c e( ∈r T). EL [fe ((t
1e
−T
|X
ϵ)⊆
<
3e)X
]
i +−1 ϵ∩
·
T sij m2 i= lar{ ae rg∈ umT j en: tf t( oe c| t(Y he< ) ee p) re≥ viθ o<Y ue s, cc a( sY e< ,we) e+ hac v( ee :) ≤ B}. Bya
OPT. (cid:88) (cid:88) (cid:88)
f(e|Y )= f(e|Y )+ f(e|Y ) (9)
<e <e <e
Proof. Prove a) If i = 1, Y
0
= ∅, the Lemma holds. We e∈Tj e∈T j1 e∈T j2
considertheothercase. WedivideT intoseveralsubsetsin- (cid:88) OPT
< c(e)θY + (10)
cludingT =T 2∪T 4∪...∪T i−1,whereT j isasetofallele- e ϵM
mentsinT thatareaddedintoY atiterationj ≤i−1. Since e∈T1
i j
T j ⊆Y i−1∩O 1andc(X i)≤B−c(r)soc(X <e)+c(e)≤ (cid:88) θX OPT
c(X )+c(e) ≤ B foralle ∈ T . Wethereforeclassifythe = c(e) e + (11)
i j 1−ϵ ϵM
elements in T j into two disjoint sets T j = T j1 ∪T j2, where e∈T j1
T j1 = {e ∈ T j : f(e c| (X e)<e) < θ eX,c(X <e)+c(e) ≤ B}and
≤
(cid:88) E[f(e|X <e)]
+
OPT
(12)
T2 ={e∈T : f(e|X<e) ≥θX,c(X )+c(e)≤B}. Since (1−ϵ)3 ϵM
j j c(e) e <e e∈T1
(1−ϵ)θX =θY ∀e∈T1,wehave: j
e l(e) j whichimpliesthat
(cid:88) (cid:88) (cid:88)  
f(e|X )= f(e|X )+ f(e|X ) (1)
<e <e <e (cid:88) (cid:88) (cid:88)
e∈Tj e∈T j1 e∈T j2 f(e|Y i)=  f(e|Y i) (13)
(cid:88) OPT
e∈T j=3,5,...,i−1 e∈Tj
< c(e)θX + (2)  
e ϵM (cid:88) (cid:88)
e∈T j1 ≤  f(e|Y <e) (14)
(cid:88) θ lY (e) OPT j=3,5,...,i−1 e∈Tj
= c(e) + (3)  
e∈T j1
1−ϵ ϵM
<
(cid:88) (cid:88) E[f(e|X <e)]
+
OPT

(1−ϵ)3 ϵM
≤
(cid:88) E[f(e|Y <e)]
+
OPT
(4)
j=3,5,...,i−1 e∈T j1
e∈T j1
(1−ϵ)3 ϵM
≤
(cid:88) E[f(e|X <e)] +(∆
+1)·
OPT
(15)
(1−ϵ)3 2 ϵM
whereinequality(2)isduetoLemma1,inequality(4)isdue e∈T
to applying Lemma 3: E[f(e|Y <e)] ≥ (1−ϵ)2E[c(e)]θ lY (e).
≤
(cid:88) E[f(e|X <e)]
+ϵOPT. (16)
Itfollowsthat (1−ϵ)3
  e∈T
Theproofiscompleted.
(cid:88) (cid:88) (cid:88)
f(e|X i)=  f(e|X i) (5)
ByusingLemma4,wefurtherprovidetheboundoff(O′∪
e∈T j=2,4,...,i−1 e∈Tj
T)forT isasubsetofX orY inLemma5whenc(r)isvery
 
large,i.e.,c(r)≥(1−ϵ)B.
(cid:88) (cid:88)
≤  f(e|X <e) (6) Lemma5. Ifc(r) ≥ (1−ϵ)B,oneoftwofollowingpropo-
j=2,4,...,i−1 e∈Tj sitionshappens
 
<
(cid:88) (cid:88) E[f(e|Y <e)]
+
OPT

ba )) E Th[f er( eS e) x] i≥ sts(1 a− subϵ) s5 eα tXOP
′
⊆T.
X sothat
(1−ϵ)3 ϵM
j=2,4,...,i−1 e∈T1 1 1
j f(X′∪O′)<(1+ )E[f(S)]+(2ϵ+ )OPT.
≤
(cid:88) E[f(e|Y <e)] +(∆
+1)·
OPT
(7)
(1−ϵ)3 ϵM
(1−ϵ)3 2 ϵM Similarly,oneoftwofollowingpropositionshappens:
e∈T
≤
(cid:88) E[f(e|Y <e)]
+ϵOPT (8)
c) E[f(S)]≥(1−ϵ)5αOPT.
(1−ϵ)3 d) ThereexistsasubsetY′ ⊆Y sothat
e∈T
1 1
where inequation (6) is due to the submodularity, inequa- f(Y′∪O′)<(1+ )E[f(S)]+(2ϵ+ )OPT.
(1−ϵ)3 ϵM
tion(8)isduetosettingofM.
Prove b). If i = 2, X 1 ∩ O = ∅, the Lemma holds. If Whenc(X 1)<ϵBandc(Y 2)<ϵB,it’seasytoobtainthe
i > 2, we only consider set T ⊆ X i−1 ∩ O′ since we approximationfactorduetof(S) ≥ max{f(X 1),f(Y 2)} ≥
can not bound the f(e|Y <e) if e ∈ X 1. We also divide T ϵBαΓ(1−ϵ)2.Otherwise,wecombineLemma5andthefact
into subsets: T = T 3 ∪ T 5 ∪ ... ∪ T i−1, where T j is a that f(O′) ≤ f(O′ ∪X)+f(O′ ∪Y) to get the bound of
set added into X j at the iteration j ≤ i−1. We also clas- f(O′)inLemma6. Theproofsofthemcanbefoundinthe
sify the elements in T into two sets T = T1 ∪T2, where Appendix.
j j j jLemma6. Ifc(X )<ϵBandc(Y )<ϵB,wehave: Therefore
1 2
• Ifc(r)<(1−ϵ)B,thenf(O′)≤ 5 (E 1[ −f( ϵS )4)] +ϵOPT E[f(S)]≥ (1−ϵ)5 OPT> 1−5ϵ OPT>(1 −ϵ)OPT.
7 7 7
• Ifc(r)≥(1−ϵ)B,oneoftwothingshappens:
Ifc(r)≥(1−ϵ)B,weconsidertwocases:
a) E[f(S)]≥(1−ϵ)5αOPT. -Ifa)inLemma6happens,then
b) f(O′)≤ 5 (E 1[ −f( ϵS )3)] +2(2ϵ+ ϵM1 )OPT.
E[f(S)]≥
(1−ϵ)5 OPT>(1
−ϵ)OPT.
7 7
Finally,putLemmata4,5,6togetheranddivideO intoap-
propriate subsets, we state the performance’s algorithm in -Ifb)inLemma6happens,then
Theorem4.1. 5E[f(S)] 2
f(O)≤ +(4ϵ+ )OPT+(2+ϵ)E[f(S)]
Theorem4.1. Forα= 1,ϵ∈(0,1),δ ∈(0,1),Algorithm1 (1−ϵ)3 ϵM
7 7 8
needsO(logn)adaptivecomplexityandO(nklog2n)query 7E[f(S)] 29
complexity and returns a solution S satisfying E[f(S)] ≥ < + ϵOPT. (22)
(1−ϵ)3 7
(1/7−ϵ)OPT.
wheretheinequality(22)isduetoϵM = 1(∆+1)> 14 for
ϵ 2 ϵ
Proof. AST first calls ParSKP1 to find a candidate so- ϵ∈(0,1),δ ∈(0,1). Itfollowsthat
lution S . This task takes O(1log(n)) adaptivity and 7 8
O(nklog0 2n) query complexity [Cδ ui etδ al., 2023b]. For E[f(S)]≥ 1 (1−ϵ)3(1− 29 ϵ)OPT>(1 −ϵ)OPT
the first loop, it calls ThreshSeq ∆ = O(1log(1)) 7 7 7
ϵ ϵ
times. Each time, RandBatch needs O(1log(n) + M) = whichcompletestheproof.
ϵ ϵ
O(1log(n) + 1 log(1)) = O(logn) adaptive complex-
ϵ ϵ ϵ3 ϵ 5 ExperimentalEvaluation
ity and O(nk log(n) + nk log(1)) = O(nklogn) query
ϵ ϵ ϵ3 ϵ
complexity. In the second phase, the algorithm may need This section evaluates our AST’s performance by compar-
O(1log(1)) adaptivity and O(n log3(1)) query complex- ing our algorithm with state-of-the-art algorithms for non-
ϵ ϵ ϵ4 ϵ
ity to call UnSubMax algorithm of [Chen et al., 2019] monotoneSMKincluding:
(Lines 16-17). Then, it only has two adaptive rounds and • ParSKP1: The parallel algorithm in [Cui et al., 2023a]
takes O(kn) query complexity to find X′i and Y′i (Lines thatrunsinO(logn)adaptivityandreturnsasolutionS
19-24). Therefore, the adaptive complexity of the algo- satisfyingE[f(S)]≥(1/8−ϵ)OPT.
rithm is O(1logn) + O(1log(1)logn) + O(1log(1)) +
δ δ ϵ ϵ ϵ ϵ • ParSKP2: The algorithm in [Cui et al., 2023a] that
2 = O(logn) and its query complexity is O(nklog2n) + runs in O(log2n) adaptivity and returns a solution of
O(nklogn)+O( ϵn
4
log3(1 ϵ))+O(nk)=O(nklog2n). E[f(S)]≥(cid:0) 1/(5+2√ 2)−ϵ(cid:1)
OPT.
For the approximation factor, we consider the following
cases: • ParKnapsack: Theparallelalgorithmin[Amanatidiset
Case1. Ifc(X )≥ϵBorc(Y )≥ϵB,wehave al.,2021]achievesanapproximationfactorof(9.465+
1 2
ε)withinO(logn).
f(S)≥max{f(X ),f(Y )}≥ϵBαΓ(1−ϵ)2
1 2 • SmkRanAcc:Thenon-adaptivealgorithmin[Hanetal.,
(1−ϵ)2 1 2021]thatachievesanapproximationfactorof4+ϵin
> OPT>( −ϵ)OPT.
7 7 querycomplexityofO(nlog(k/ϵ)/ϵ).
• RLA:Thenon-adaptivealgorithmin[Phametal.,2023]
Case2. Ifc(X )<ϵBandc(Y )<ϵB,wehave
1 2 with a factor of 4 + ϵ in linear query complexity of
f(O)≤f(O∩(V \X ))+f(O∩(V ∪X )) (17) O(nlog(1/ϵ)/ϵ).
1 1 0 1
=f(O′)+f(O∩(V ∪X )) (18) Weexperimentedwiththefollowingthreeapplications:
2 1
≤f(O′)+(2+ϵ)E[f(S)] (19) Revenue Maximization (RM). Given a network G =
(V,E) where V is a set of nodes and E is a set of edges.
where inequality (17) is due to the submodularity of f and Each edge (u,v) in E is assigned a positive weight w(u,v)
(V 1\X 1)∩(V 0∩X 1)=∅,equality(18)isduetothedefini- sampleduniformlyin[0,1]andeachnodei √sassignedapos-
tionofO′andinequality(19)isduetoapplyingAlgorithmin itive cost c(u) defined as c(u) = 1 − e (cid:80) (u,v)∈Ew(u,v).
[Chenetal.,2019]. WenowapplyLemma6toboundf(O′).
The revenue of any subset S ⊆ V is defined as f(S) =
Ifc(r)<(1−ϵ)B,then (cid:80) (cid:112)(cid:80)
w . Given a budget of B, the goal of
v∈V\S u∈S u,v
5E[f(S)] the problem is to select a set S with the cost at most B to
f(O)≤ +ϵOPT+(2+ϵ)E[f(S)] (20)
(1−ϵ)4 maximize f(S). As in the prior work, [Amanatidis et al.,
2021], wecanconstructthegraphGusingaYouTubecom-
7E[f(S)]
≤ +ϵOPT. (21) munitynetworkdataset[Hanetal.,2021]with39,841nodes
(1−ϵ)4
and224,235edges.MaximumWeightedCut(MWC). ConsideragraphG = theothersinbothsolutionperformanceandthequantitiesof
(V,E)whereeachedge(u,v)∈Ehasanon-negativeweight adaptivity.
w(u,v). For a node subset S ⊂ V, define the weighted cut
(cid:80) (cid:80)
function f(S) = w(u,v). The maximum
u∈V\S v∈S
weightedcutproblemseeksasubsetS ⊆ V thatmaximizes
f(S). As in recent work [Amanatidis et al., 2020], we gen-
Youtube (n=39,841) Youtube (n=39,841)
erateanErdo˝s-Re´nyirandomgraphwith5,000nodesandan 5K
20K
edge probability of 0.2. The node costs c(u) are randomly
uniformlysampledfrom(0,1). 15K 3K
ImageSummarization(IS). ConsideragraphG=(V,E) 10K
where each node u ∈ V represents an image, and each
1K
edge (u,v) ∈ E has a weight w(u,v) showing the sim- 5K
ilarity between images u and v. Let c(u) be the cost
0.001 0.005 0.01 0.015 0.001 0.005 0.01 0.015
to acquire image u. The goal is to find a representa- B B
tive subset S ⊆ V under the budget B that maximizes a (a) (b)
valuef(S)=(cid:80) u∈V max v∈Sw u,v− |V1 |(cid:80) u∈V (cid:80) v∈Sw u,v
450K
CIFAR-10 (n=500) 1.5K CIFAR-10 (n=500)
[Mirzasoleiman etal., 2016;Han etal., 2021]. Asinrecent
works[Hanetal.,2021;Mirzasoleimanetal.,2016],wecre- 1.0K
ate an instance as follows: First, randomly sample 500 im-
ages from the CIFAR dataset [Krizhevsky, 2019] of 10,000 0.5K
images,thenmeasurethesimilaritybetweenimagesuandv
using the cosine similarity of their 3,072-dimensional pixel
445K
vectors.
0.01 0.05 0.1 0.15 0.01 0.05 0.1 0.15
B B
Experiment setting. In our experiments, we set the accu-
(c) (d)
racy parameter ϵ = 0.1 for all algorithms evaluated, and
ER (n=5000) ER (n=5000)
for AST, we set δ = 0.12. We used OpenMP to pro- 40K
gram with C++ language. Besides, we experimented on a
400K 30K
high-performance computing (HPC) server cluster with the
following parameters: partition=large, #threads(CPU)=128, 300K 20K
node=4, max memory = 3,073 GB. For UnSubMax, we 200K
use setting of previous works [Amanatidis et al., 2021; 10K
100K
Cui et al., 2023a], i.e, we adapt Algorithm in [Feige et al.,
2011b]returning1/4−ϵratioinoneadaptiveroundandO(n)
0.01 0.05 0.1 0.15 0.01 0.05 0.1 0.15
B B
querycomplexity.
(e) (f)
Experimental Result. In Figures 1(a), (c), and (e), we
compare the objective values between different algorithms. Figure 1: Performance of algorithms for non-monotoneSMK on
threeinstances:(a),(b)RevenueMaximization;(c),(d)ImageSum-
TheresultsshowthatourASTachievesthebestobjectiveval-
marizationand(e),(f)MaximumWeightedCut. Thebudgetvalues
uesforboththeRMandMWCapplications. InRM,theob-
representfractionsofthetotalcostofallelements.
jectivevaluesachievedbyRLA, SmkRanAcc, andParSKP1
are the same, ParKnapsack attains lower objective values
while ParSKP2 hits the lowest objective values among the 6 Conclusions
algorithms.Especially,ouronemarksthehighestvaluewhen
B =0.015,about1.3timeshigherthantheothersinRM. In Motivated by the challenge of the large scale of input data,
IS, ParKnapsack reaches the highest values while ParSKP2 in this work, we focus on parallel approximation algorithms
hitsthelowest. Ouralgorithmresultsinbestvaluesatsome basedon theconceptof adaptivecomplexity. Moreover, the
points and drops at others. As shown in Figure 1 (c), most requirementofimprovingtheapproximationfactorwhilede-
algorithms fluctuate widely. The variation in the quality of creasing the adaptivity down to log(n) motivates us to pro-
these algorithms might be due to the characteristics of this pose a competitive new algorithm. We have proposed an
dataset. efficient parallel algorithm AST based on a novel alternate
In Figures 1(b), (d), and (f), we make the comparisons thresholdgreedystrategy. Toourknowledge,ourASTalgo-
about the number of adaptive rounds. The results show that rithmisthefirsttoachieveaconstantfactorapproximationof
SmkRanAcc and RLA always require the highest number of 7+ϵfortheaboveproblemintheaforementionedadaptivity.
adaptive rounds across all three applications. For the AST, Ouralgorithmalsoexpressesthesuperiorityinsolutionqual-
thenumberofadaptiveroundsisequivalenttoParSKP1for ityandcomputationcomplexitycomparedtostate-of-the-art
RM and MWC. Besides, for IS, the adaptive number of algorithms via some illustrations in the experiment in three
rounds for AST is higher than that of ParSKP1, which has real-world applications. In the future, we will address an-
thelowestnumberofrounds. However,thehighernumberin othervaluablequestion: canwereducethequerycomplexity
thiscaseisinsignificant. Overall,ouralgorithmoutperforms ofparallelizedalgorithmsfortheSMKproblem?
eulav
evitcejbO
eulav
evitcejbO
eulav
evitcejbO
evitpadA
evitpadA
evitpadAAcknowledgments [Cuietal.,2023a] Shuang Cui, Kai Han, Jing Tang,
HeHuang,XueyingLi,andAakasZhiyuli. Practicalpar-
Thefirstauthor(TanD.Tran)wasfundedbytheMaster,PhD
allel algorithms for submodular maximization subject to
Scholarship Programme of Vingroup Innovation Foundation
a knapsack constraint with nearly optimal adaptivity. In
(VINIF),codeVINIF.2023.TS.105. Thisworkhasbeencar-
Thirty-SeventhAAAIConferenceonArtificialIntelligence,
riedoutpartlyattheVietnamInstituteforAdvancedStudyin
AAAI 2023, Thirty-Fifth Conference on Innovative Appli-
Mathematics (VIASM). The second author (Canh V. Pham)
cations of Artificial Intelligence, IAAI 2023, Thirteenth
would like to thank VIASM for its hospitality and financial
Symposium on Educational Advances in Artificial Intelli-
support.
gence,EAAI2023,Washington,DC,USA,February7-14,
2023,pages7261–7269.AAAIPress,2023.
References
[Cuietal.,2023b] Shuang Cui, Kai Han, Jing Tang,
[Amanatidisetal.,2020] Georgios Amanatidis, Federico
He Huang, Xueying Li, Aakas Zhiyuli, and Hanxiao Li.
Fusco,PhilipLazos,StefanoLeonardi,andRebeccaReif-
Practicalparallelalgorithmsfornon-monotonesubmodu-
fenha¨user. Fastadaptivenon-monotonesubmodularmaxi- larmaximization. CoRR,abs/2308.10656,2023.
mizationsubjecttoaknapsackconstraint. InAnnualCon-
[EneandNguyen,2019] Alina Ene and Huy L. Nguyen.
ferenceonNeuralInformationProcessingSystems,2020.
Submodular maximization with nearly-optimal approxi-
[Amanatidisetal.,2021] Georgios Amanatidis, Federico mation and adaptivity in nearly-linear time. In Proceed-
Fusco,PhilipLazos,StefanoLeonardi,AlbertoMarchetti- ings of the Thirtieth Annual ACM-SIAM Symposium on
Spaccamela, and Rebecca Reiffenha¨user. Submodular DiscreteAlgorithms, SODA2019, SanDiego, California,
maximizationsubjecttoaknapsackconstraint: Combina- USA,January6-9,2019,pages274–282.SIAM,2019.
torial algorithms with near-optimal adaptive complexity.
[EneandNguyen,2020] AlinaEneandHuyL.Nguyen.Par-
InInternationalConferenceonMachineLearning,volume
allel algorithm for non-monotone dr-submodular maxi-
139ofProceedingsofMachineLearningResearch,pages
mization. In Proceedings of the 37th International Con-
231–242,2021.
ference on Machine Learning, ICML 2020, 13-18 July
[BadanidiyuruandVondra´k,2014] Ashwinkumar Badani- 2020, Virtual Event, volume 119 of Proceedings of Ma-
diyuru and Jan Vondra´k. Fast algorithms for maximizing chineLearningResearch,pages2902–2911.PMLR,2020.
submodularfunctions. InAnnualACM-SIAMSymposium
[Eneetal.,2019] Alina Ene, Huy L. Nguyen, and Adrian
onDiscreteAlgorithms,pages1497–1514,2014.
Vladu. Submodularmaximizationwithmatroidandpack-
[BalkanskiandSinger,2018] Eric Balkanski and Yaron ingconstraintsinparallel. InProceedingsofthe51stAn-
Singer. The adaptive complexity of maximizing a sub- nual ACM SIGACT Symposium on Theory of Computing,
modular function. In Annual ACM SIGACT Symposium STOC 2019, Phoenix, AZ, USA, June 23-26, 2019, pages
onTheoryofComputing,pages1138–1151,2018. 90–101.ACM,2019.
[BuchbinderandFeldman,2019] Niv Buchbinder and [Fahrbachetal.,2019] Matthew Fahrbach, Vahab S. Mir-
Moran Feldman. Constrained submodular maximization rokni,andMortezaZadimoghaddam. Non-monotonesub-
viaanonsymmetrictechnique. MathematicalOperations modularmaximizationwithnearlyoptimaladaptivityand
Research,44(3):988–1005,2019. query complexity. In Kamalika Chaudhuri and Ruslan
[Buchbinderetal.,2015] Niv Buchbinder, Moran Feldman, Salakhutdinov, editors, Proceedings of the 36th Interna-
andRoySchwartz. Comparingapplesandoranges:Query tional Conference on Machine Learning, ICML 2019, 9-
tradeoff in submodular maximization. In Proceedings of 15June2019,LongBeach,California,USA,volume97of
the 26th Annual ACM-SIAM Symposium on Discrete Al- ProceedingsofMachineLearningResearch,pages1833–
gorithms2015,pages1149–1168,2015. 1842.PMLR,2019.
[ChenandKuhnle,2022] Yixin Chen and Alan Kuhnle. [Feigeetal.,2011a] UrielFeige,VahabS.Mirrokni,andJan
Vondra´k. Maximizing non-monotone submodular func-
Practical and parallelizable algorithms for non-
tions. SIAM Journal on Computing, 40(4):1133–1153,
monotonesubmodularmaximizationwithsizeconstraint.
https://arxiv.org/abs/2009.01947,2022. 2011.
[Chenetal.,2019] Lin Chen, Moran Feldman, and Amin [Feigeetal.,2011b] UrielFeige,VahabS.Mirrokni,andJan
Vondra´k. Maximizing non-monotone submodular func-
Karbasi. Unconstrained submodular maximization with
tions. SIAM Journal on Computing, 40(4):1133–1153,
constant adaptive complexity. In Proceedings of the 51st
2011.
Annual ACM SIGACT Symposium on Theory of Comput-
ing,STOC2019,page102–113,2019. [Guptaetal.,2010] Anupam Gupta, Aaron Roth, Grant
Schoenebeck, and Kunal Talwar. Constrained non-
[Cuietal.,2021] Shuang Cui, Kai Han, Jing Tang,
monotone submodular maximization: Offline and secre-
He Huang, Xueying Li, and Zhiyu Li. Streaming
taryalgorithms.InInternationalWorkshoponInternetand
algorithms for constrained submodular maximization.
NetworkEconomics,2010.
Proceedings of the ACM SIGMETRICS conference
on Measurement and Analysis of Computer Systems, [Hanetal.,2020] KaiHan, ZongmaiCao, ShuangCui, and
6(3):54:1–54:32,2021. Benwei Wu. Deterministic approximation for submodu-lar maximization over a matroid in nearly linear time. In
Advances in Neural Information Processing Systems 33:
AnnualConferenceonNeuralInformationProcessingSys-
tems2020,NeurIPS2020,2020.
[Hanetal.,2021] KaiHan,ShuangCui,TianshuaiZhu,En-
pei Zhang, Benwei Wu, Zhizhuo Yin, Tong Xu, Shao-
jie Tang, and He Huang. Approximation algorithms for
submodular data summarization with a knapsack con-
straint. Proceedings of the ACM SIGMETRICS confer-
enceonMeasurementandAnalysisofComputerSystems,
5(1):05:1–05:31,2021.
[Krizhevsky,2019] AlexKrizhevsky. Learningmultiplelay-
ersoffeaturesfromtinyimages. TechnicalReports,Uni-
versityofToronto,2019.
[Kuhnle,2021a] AlanKuhnle.Nearlylinear-time,paralleliz-
ablealgorithmsfornon-monotonesubmodularmaximiza-
tion. InProceedingsofthe30thAAAIConferenceonArti-
ficialIntelligence2021,pages8200–8208,2021.
[Kuhnle,2021b] Alan Kuhnle. Quick streaming algorithms
for maximization of monotone submodular functions in
lineartime. InProceedingsofthe24thInternationalCon-
ferenceonArtificialIntelligenceandStatistics2021, vol-
ume 130 of Proceedings of Machine Learning Research,
pages1360–1368,2021.
[Leeetal.,2010] Jon Lee, Vahab S. Mirrokni, Viswanath
Nagarajan, and Maxim Sviridenko. Maximizing non-
monotone submodular functions under matroid or knap-
sackconstraints. SIAMJournalonDiscreteMathematics,
23(4):2053–2078,2010.
[Lietal.,2022] WenxinLi,MoranFeldman,EhsanKazemi,
andAminKarbasi.Submodularmaximizationincleanlin-
ear time. In Advances in Neural Information Processing
Systems,pages7887–7897,2022.
[Li,2018] Wenxin Li. Nearly linear time algorithms and
lower bound for submodular maximization. preprint,
arXiv:1804.08178,2018.
[Mirzasoleimanetal.,2016] Baharan Mirzasoleiman, Ash-
winkumar Badanidiyuru, and Amin Karbasi. Fast con-
strained submodular maximization: Personalized data
summarization. In International Conference on Machine
Learning,volume48ofJMLRWorkshopandConference
Proceedings,pages1358–1367,2016.
[Phametal.,2023] CanhV.Pham,TanD.Tran,DungT.K.
Ha, and My T. Thai. Linear query approximation algo-
rithmsfornon-monotonesubmodularmaximizationunder
knapsackconstraint. InProceedingsoftheThirty-Second
International Joint Conference on Artificial Intelligence,
IJCAI2023,19th-25thAugust2023,Macao,SAR,China,
pages4127–4135.ijcai.org,2023.
[Sunetal.,2022] XiaomingSun,JialinZhang,ShuoZhang,
and Zhijie Zhang. Improved deterministic algorithms for
non-monotonesubmodularmaximization.InYongZhang,
DongjingMiao,andRolfH.Mo¨hring,editors,Computing
and Combinatorics - 28th International Conference, CO-
COON2022Proceedings,volume13595ofLectureNotes
inComputerScience,pages496–507.Springer,2022.Appendix
A RandBatchAlgorithm
Inthissection,werecapRandBatch[Cuietal.,2023a](Algorithm2ofthatpaper),afrequentlyusedsubroutineintheproposed
algorithms. ForadiscussionoftheintuitionbehindRandBatchandrigorousproofofLemma1,wereferthereaderto[Cuiet
al.,2023a]anditsfullversion[Cuietal.,2023b].
Algorithm2:RandBatch
Input: θ,I,M,ϵ,f(·),c(·)
A←∅,count←0
1:
L←{u∈I : f(u|A) ≥ρ∧c(A∪{u})≤B};
2: c(u)
whileL̸=∅∧count<M do
3:
{v ,v ,...,v }←GetSEQ(A,L,c(·));
4: 1 2 d
foreachi∈{0,1,...,d}do
5:
V ←{v ,v ,...,v },G ←A∪V ;
6: 1 2 i i i
E+ ←{u∈L: f(u|Gi) ≥ρc(G ∪{u})≤B};
7: i c(u) i
E− ←{u∈L:f(u|G )<0};
8: i i
D ←{v :j ∈[i]∧f(v |A∪V )<0};
9: i j j j−1
end
10:
Findt ←min {c(E+)≤(1−ϵ)c(L)},
11: 1 i≤d
(cid:80) (cid:80) (cid:80)
t ←min {ϵ }f(u|G )≤ |f(u|G )|+ |f(v |A∪V )|
2 i≤d u∈E i+ i u∈E i− i vj∈Di j j−1
t∗ ←min{t ,t };U ←U{V };
12: 1 2 t∗
Withprobabilitypdo:
13:
A←A∪V
14: t∗
Ift ≤t thencount←count+1
15: 2 1
L←{u∈L\U : f(u|A) ≥ρ∧c(A∪{u})≤B}
16: c(u)
end
17:
return(A,U,L)
18:
Algorithm3:GetSEQ(A,I,c(·))
Input: θ,I,M,ϵ,f(·),c(·)
A←∅,count←0
1:
whileX ̸=∅do
2:
Drawa uniformlyatrandomfromX
3: i
A←[a ,a ,...,a ]
4: 1 2 i
X ←{e∈X\a :c(e)+c(A)+c(S)≤B}
5: i
i←i+1
6:
end
7:
returnA
8:
A.1 ProofofLemma3
TheproofofLemma3impliesfromtheproofofLemma1.Wewritedownthedetailsoftheproofforthesakeofcomplete-
ness. WefirstrecaptheLemma2in[Cuietal.,2023b]forsupportingtheproofofLemma3.
Lemma 7 (Lemma 2 in [Cui et al., 2023b]). For any V found in Lines 11-12 of RandBatch and any V , let λ(u) denote
t∗ t∗
the set of elements in V selected before u (note that V is an ordered list according to Line 6 of RandBatch), and λ(u)
t∗ t∗
does not include u. Let A and U be the sets returned by RandBatch(ρ,I,M,p,ϵ,f(·),c(·)), where the elements in U are
{u ,u ,...,u }(listedaccordingtotheordertheyareaddedintoU). Letu bea“dummyelement”withzeromarginalgain
1 2 s j
andzerocostforalls<j ≤|I|. Thenwehave:
∀j ∈[|I|]:E[f(u |{u ,...,u }∩A∪λ(u ))|F ]≥(1−ϵ)2ρ·E[c(u )|F ] (23)
j 1 j−1 j j−1 j j−1
whereF denotesthefiltrationcapturingalltherandomchoicesmadeuntilthemomentrightbeforeu isselected.
j−1 jConsiderthesequence{u ,u ,...,u ,...,u }tobedefinedinLemma7. Foreachj ∈[|I|],definetherandomvariable
1 2 s+1 |I|
δ (u ) = f(u |{u ,...,u }∩A∪λ(u ))ifu ∈ Aandδ (u ) = 0otherwise,andalsodefineδ (u ) = c(u )ifu ∈ A
1 j j 1 j−1 j j 1 j 2 j j j
andδ (u ) = 0otherwise. Sowehavef(A) ≥
(cid:80)|I|
andc(A) =
(cid:80)|I|
δ (u ). Duetothelinearityofexpectationandthe
2 j j=1 j=1 2 j
lawoftotalexpectation,weonlyneedtoprove
∀j ∈[|I|],∀F :E[f(δ (u )|{u ,...,u }∩A∪λ(u ))|F ]≥(1−ϵ)2ρ·E[δ (u )|F ] (24)
j−1 1 j 1 j−1 j j−1 2 j j−1
whereF isthefiltrationdefinedinLemma7. Thistriviallyholdsforj >s. Foranyj ≤s,wehave
j−1
E[δ (u )|F ]=E[E[δ (u )|F ,u ]|F ] (25)
1 j j−1 1 j j−1 j j−1
=pE[f(u |{u ,...,u ∩A∪λ(u )})|F ], (26)
j 1 j−1 j j−1
wherethesecondequalityisbecauseeachu ∈U isaddedintoAwithprobabilityp,whichisindependentoftheselectionof
j
u . Similarity,wecanproveE[δ (u )|F ]=p·E[c(u )|F ]. Combinetwocaseswithp=1andLemma7wehave:
j 2 j j−1 j j−1
E[f(u |{u ,...,u }∩A∪λ(u )))]=(1−ϵ)2ρE[c(u )] (27)
j 1 j−1 j j
whichimpliestheproof.
B MissingproofsofSection4
Wefirstprovidesomenotationsusedfortheproofs.
• X,Y arethesetsafterthethefirstloopofAST.
• Supposing X and Y ordered: X = {x ,x ,...,x }, Y = {y ,y ,...,y }, we conduct: Xi = {x ,x ,...,x },
1 2 |X| 1 2 |Y| 1 2 i
Yi ={y ,y ,...,y }.
1 2 i
• t=max{i∈N:c(Xi)+c(r)≤B−c(r)},u=max{i∈N:c(Yi)+c(r)≤B−c(r)}.
• X andY areX andY afteriterationi,respectively.
i i
• Forx∈X∪Y,weassumethatxisaddedintoX orY atiterationl(x).
• θX isθX atiterationi,θY isθY atiterationi.
i i
• θX andθY areθX andθY atthelastiterationwhenX andY areconsideredtoupdate.
last last
• Foranelemente∈X ∪Y,wedenoteX ,Y ,θX andθY asX,Y,θX andθY rightbeforeeisselectedintoX orY,
<e <e e e
respectively.
• Oisanoptimalsolution,O isanoptimalsolutionofSMKproblemovertheinstance(V ,f,B).
1 1
• O′ =O \X ,Or =O′\{r}.
1 1
B.1 ProofofLemma5
Provea),b). Inthiscasewehavec(Xt)≤B−c(r)≤ϵB,weconsiderfollowingcases:
Case 1. If Xt = X, denote Y = Y , i.e., the set Y right before the algorithm obtains Xt. For any element e ∈
(t) <xt
O′ \(X ∪Y ), we have c(Xt)+c(e) ≤ B. Therefore we consider O′ \(X ∪Y ) = O′1 ∪O′2, where O′1 = {e ∈
(t) (t)
O′\(X∪Y ):f(e|X)<c(e)θX },O′2 ={e∈O′\(X∪Y ):f(e|X)≥c(e)θX }. Bysetting∆,atanyiterationiwe
(t) last (t) last
have
8αf(S )(1−ϵ) ϵf(S )(1−ϵ)
0 =Γ(1−ϵ)≥Γ(1−ϵ)i ≥Γ(1−ϵ)∆ ≥ 0 (28)
(1−8δ)ϵB BandthereforeθX ≤ Γ(1−ϵ)∆ ≤ ϵOPT. ByapplyingLemma4andLemma1wehave
last 1−ϵ B
(cid:88)
f(X∪O′)−f(X)≤ f(e|X) (Bythesubmodularityoff) (29)
e∈O′\X
(cid:88) (cid:88)
= f(e|X)+ f(e|X) (30)
e∈O′∩Y(t) e∈O′\(X∪Y(t))
(cid:88) (cid:88) (cid:88)
= f(e|X)+ f(e|X)+ f(e|X) (31)
e∈O′∩Y(t) e∈O′1 e∈O′2
≤
(cid:88) E[f(e|Y <e)]
+ϵOPT+c(O′1)θX +
OPT
(ByapplyingLemma1andLemma4) (32)
(1−ϵ)3 last ϵM
e∈O′∩Y(t)
E[f(Y )] OPT
≤ (t) +ϵOPT+BθX + (33)
(1−ϵ)3 last ϵM
E[f(Y)] OPT
< +ϵOPT+ϵOPT+ . (34)
(1−ϵ)3 ϵM
Itfollowsthat
E[f(Y)] 1
f(X∪O′)≤f(X)+ +(2ϵ+ )OPT (35)
(1−ϵ)3 ϵM
1 1
<(1+ )E[f(S)]+(2ϵ+ )OPT. (36)
(1−ϵ)3 ϵM
Case2. IfXt ̸=X,wenowconsiderfollowingcases. Considertheiterationz,z ≥1that
(1−ϵ)αOPT αΓ(1−ϵ)z αOPT
≤θ = ≤ . (37)
B z ϵB B
Wedefineanoddintegerlasfollows
(cid:26)
z,ifzisodd
l= (38)
z+1,otherwise.
Itfollowsthat
(1−ϵ)2αOPT αOPT
≤θ ≤θX ≤θ ≤
B z l z B
Weconsidertwofollowingcases:
Case2.1. IfXt+1 ⊆X ,wefurtherconsidertwosub-cases
l
• Ifc(X )≥(1−ϵ)B,byLemma1wehave
l
E[f(S)]≥E[f(X )]≥E[c(X )](1−ϵ)2θX ≥(1−ϵ)5αOPT.
l l l
• If c(X ) < (1−ϵ)B. Since c(Or) ≤ ϵB, c(X )+c(e) < B, for all e ∈ Or. Combine this with the Lemma 4 and
l l
Lemma1,wehave
(cid:88)
f(X ∪O′)−f(X ∪{r})≤ f(e|X ) (39)
l l l
e∈O′r\Xl
(cid:88) (cid:88)
≤ f(e|X )+ f(e|X ) (40)
l l
e∈O′r∩Yl−1 e∈O′r\(Xl∪Yl−1)
<
(cid:88) E[f(e|Y <e)]
+ϵOPT+c(O′r)θX +
OPT
(41)
(1−ϵ)3 l ϵM
e∈O′r∩Yl−1
E[f(Y )] OPT
≤ l−1 +ϵαOPT+αϵOPT+ (42)
(1−ϵ)3 ϵM
E[f(Y)] OPT
≤ +2αϵOPT+ . (43)
(1−ϵ)3 ϵMCombinethiswiththeselectionruleoffinalsolution: f(X ∪{r})≤f(S),wehave
l
E[f(Y)] OPT
f(X ∪O′)<f(X ∪{r})+ +2αϵOPT+ (44)
l l (1−ϵ)3 ϵM
1 OPT
≤(1+ )E[f(S)]+2αϵOPT+ . (45)
(1−ϵ)3 ϵM
Case2.2. IfX ⊂Xt+1,thenc(X )+c(e)≤Bforalle∈O . Wethusalsohave
l l 1
(cid:88)
f(X ∪O′)−f(X ∪{r})≤ f(e|X ) (46)
l l l
e∈O′r\Xl
(cid:88) (cid:88)
= f(e|X )+ f(e|X ) (47)
l l
e∈O′r∩Yl−1 e∈O′r\(Xl∪Yl−1)
<
(cid:88) E[f(e|Y <e)]
+ϵOPT+c(O′r)θX +
OPT
(48)
(1−ϵ)3 l ϵM
e∈O′r∩Xl−1
E[f(Y )] OPT
≤ l−1 +ϵαOPT+αϵOPT+ (49)
(1−ϵ)3 ϵM
E[f(Y)] OPT
≤ +2αϵOPT+ . (50)
(1−ϵ)3 ϵM
Notethatf(X ∪{r})≤f(S)duetotheselectionruleofthesecondloop. Therefore
l
1 OPT
f(X ∪O′)≤(1+ )E[f(S)]+2αϵOPT+ . (51)
l (1−ϵ)3 ϵM
Combineallabovecaseswithnotethatα<1,weobtaintheproof.
Provec),d). WeprovethiscasebysimilarargumentaspreviouscaseandapplyingLemma4withiisodd.
Case 1. If Yu = Y, denote X = Y , i.e., the set X right before the algorithm obtains Yu. For any element e ∈
(u) <yu
O′ \(Y ∪X ), we have c(Yu)+c(e) ≤ B. Therefore we consider O′ \(Y ∪X ) = O′ ∪O′, where O′ = {e ∈
(u) (u) 1 2 1
O′\(Y ∪X ):f(e|Y)<c(e)θY },O′ ={e∈O′\(Y ∪X ):f(e|Y)≥c(e)θY }.Wehave:θY ≤ Γ(1−ϵ)∆ ≤ ϵOPT.
(u) last 2 (u) last last 1−ϵ B
ByapplyingLemma4andLemma1wehave
(cid:88)
f(Y ∪O′)−f(Y)≤ f(e|Y) (Bythesubmodularityoff) (52)
e∈O′\Y
(cid:88) (cid:88)
= f(e|Y)+ f(e|Y) (53)
e∈O′∩X(u) e∈O′\(Y∪X(u))
(cid:88) (cid:88) (cid:88)
= f(e|Y)+ f(e|Y)+ f(e|Y) (54)
e∈O′∩X(u) e∈O 1′ e∈O 2′
<
(cid:88) E[f(e|X <e)]
+ϵOPT+c(O′)θX +
OPT
(55)
(1−ϵ)3 1 last ϵM
e∈O′∩X(u)
E[f(e|X )] OPT
≤ <e +ϵOPT+BθX + (56)
(1−ϵ)3 last ϵM
E[f(X)] OPT
≤ +ϵOPT+ϵOPT+ . (57)
(1−ϵ)3 ϵM
Itfollowsthat
E[f(X)] 1
f(Y ∪O′)<f(Y)+ +(2ϵ+ )OPT (58)
(1−ϵ)3 ϵM
1 1
≤(1+ )E[f(S)]+(2ϵ+ )OPT. (59)
(1−ϵ)3 ϵM
Case2. IfYu ̸=Y,Considertheiterationz,z ≥1that
(1−ϵ)αOPT αΓ(1−ϵ)z αOPT
≤θ = ≤ . (60)
B z ϵB BWedefineanevenintegerlasfollows
(cid:26)
z,ifziseven
l= (61)
z+1,otherwise.
Itfollowsthat
(1−ϵ)2αOPT αOPT
≤θ ≤θX ≤θ ≤
B z l z B
Weconsidertwofollowingcases:
Case2.1. IfYu+1 ⊆Y ,wefurtherconsidertwosub-cases:
l
• Ifc(Y )≥(1−ϵ)B,byLemma1wehave
l
E[f(S)]≥E[f(Y )]≥E[c(Y )](1−ϵ)2θY ≥(1−ϵ)5αOPT.
l l l
• If c(Y ) < (1−ϵ)B. Since c(O′r) ≤ ϵB, c(Y )+c(e) < B, for all e ∈ O′r. Combine this with the Lemma 4 and
l l
Lemma1,wehave
(cid:88)
f(Y ∪O′)−f(Y ∪{r})≤ f(e|Y ) (62)
l l l
e∈O′r\Yl
(cid:88) (cid:88)
= f(e|Y )+ f(e|Y ) (63)
l l
e∈O′r∩Xl−1 e∈O′r\(Yl∪Xl−1)
<
(cid:88) E[f(e|X <e)]
+ϵOPT+c(O′r)θY +
OPT
(64)
(1−ϵ)3 l ϵM
e∈O′r∩Xl−1
E[f(X )] OPT
≤ l−1 +ϵαOPT+αϵOPT+ (65)
(1−ϵ)3 ϵM
E[f(X)] OPT
≤ +2αϵOPT+ (66)
(1−ϵ)3 ϵM
Combinethiswiththeselectionruleoffinalsolution,wehave
E[f(X)] OPT
f(Y ∪O′)<f(Y )+ +2αϵOPT+ (67)
l l (1−ϵ)3 ϵM
1 OPT
≤(1+ )E[f(S)]+2αϵOPT+ (68)
(1−ϵ)3 ϵM
Case2.2. IfY ⊂Yu+1,thenc(Y )+c(e)≤Bforalle∈O′. Wethusalsohave
l l
(cid:88)
f(Y ∪O′)−f(Y ∪{r})≤ f(e|Y ) (69)
l l l
e∈O′r\Yl
(cid:88) (cid:88)
= f(e|Y )+ f(e|Y ) (70)
l l
e∈O′r∩Xl−1 e∈O′r\(Yl∪Xl−1)
<
(cid:88) E[f(e|X <e)]
+ϵOPT+c(Or)θX +
OPT
(71)
(1−ϵ)3 l ϵM
e∈O′r∩Xl−1
E[f(X )] OPT
≤ l−1 +ϵαOPT+αϵOPT+ (72)
(1−ϵ)3 ϵM
E[f(X)] OPT
≤ +2αϵOPT+ . (73)
(1−ϵ)3 ϵM
Notethatf(Y ∪{r})≤f(S)duetotheselectionruleofthesecondloop. Therefore
l
1 OPT
f(Y ∪O′)<(1+ )E[f(S)]+2αϵOPT+ . (74)
l (1−ϵ)3 ϵM
Combineallcases,weobtaintheproof.B.2 ProofofLemma6
WeprovetheLemmabycarefullyconsideringthefollowingcases:
Case1. Ifc(r)<(1−ϵ)B,thenB−c(r)>ϵB.
Case1.1.Ifc(X)andc(Y)arebothgreaterthanB−c(r).Sincec(Xt+1)>ϵBandc(Yu+1)>ϵB,thealgorithmmustobtain
Xt+1 andYu+1 afteri-thiteration,i≥3. WeprovethiscasewhenthealgorithmobtainsXt beforeYu. Whenconsidering
thesubsetT ⊆O′ inLemma4,therolesofX andY arethesame. Thuswecanprovesimilarlyfortheremainingcase. Since
theelementsinX areaddedattheodditerations,anyelemente∈O′\XtwasnotaddedintoX atiterations=l(x )−2.
t+1
ItiseasytofindthatX ⊆Xt. ByapplyingLemma4,wehave
s
(cid:88)
f(O′∪Xt)−f(Xt∪{r})≤ f(e|Xt) (75)
e∈O′r\Xt
(cid:88) (cid:88)
= f(e|Xt)+ f(e|Xt) (76)
e∈Y<xt∩O′r e∈O′r\(Xt∪Y<xt)
<
(cid:88) E[f(e|Y <e)]
+ϵOPT+
(cid:88)
f(e|Xt). (77)
(1−ϵ)3
e∈Y<xt∩O′r e∈O′r\(Xt∪Y<xt)
Besides,applyingLemma4againgives:
(cid:88) (cid:88)
f(O′∪Yu)−f(Yu∪{r})= f(e|Yu)+ f(e|Yu) (78)
e∈O′r∩X<yu e∈O′r\(X<yu∪Yu)
≤
(cid:88) E[f(e|X <e)]
+ϵOPT+
(cid:88)
f(e|Yu). (79)
(1−ϵ)3
e∈O′r∩X<yu e∈O′r\(X<yu∪Yu)
Bythesubmodularityoff andXt∩Yu =∅,wehave
f(O′)≤f(O′∪Xt)+f(O′∪Yu). (80)
Fromnowon,itisnecessarytoboundsometermsoftherighthandof(77)and(79).
a)
Boundof(cid:80) f(e|Xt)+(cid:80) E[f(e|X<e)].
e∈O′r\(Xt∪Y<xt) e∈O′r∩X<yu (1−ϵ)3
-Ifx ∈/ O′r,O′r\(Xt∪Y )=O′r\(Xt+1∪Y )andO′r∩Xt =O′r∩Xt+1.Anyelemente∈O′r\(Xt+1∪Y )
t+1 <xt <xt <xt
wasnotaddedintoX atiterations=l(x )−2. ByapplyingLemma1,wehave
t+1
(cid:88) (cid:88)
f(e|Xt)= f(e|Xt) (81)
e∈O′r\(Xt∪Y<xt) e∈O′r\(Xt+1∪Y<xt)
(cid:88)
≤ f(e|X ) (82)
s
e∈O′r\(Xt+1∪Y<xt)
θX
OPT
<c(O′r\(Xt+1∪Y )) (t+1) + . (83)
<xt (1−ϵ)2 ϵM
Ontheotherhand,sincec(Xt+1)>B−c(r)≥c(O′r)soc(Xt+1\O′r)>c(O′r\Xt+1)≥c(O′r\(Xt+1∪Y )).
<xt
Combinethiswith(83),wehave
(cid:88)
f(e|X )≥c(Xt+1\O′r)θX (84)
<e (t+1)
e∈Xt+1\O′r
 
c(Xt+1\O′r) (cid:88) OPT
≥(1−ϵ)2  f(e|Xt)−  (85)
c(O′r\(Xt∪Y )) ϵM
<xt
e∈O′r\(Xt∪Y<xt)
 
(cid:88) OPT
>(1−ϵ)2  f(e|Xt)− . (86)
ϵM
e∈O′r\(Xt∪Y<xt)From(86)and(83)withanotethatX ⊆Xtweobtain
<yu
(cid:88)
f(e|Xt)+
(cid:88) E[f(e|X <e)]
(87)
(1−ϵ)3
e∈O′r\(Xt∪Y<xt) e∈O′r∩X<yu
(cid:80)
≤
e∈Xt+1\O′rf(e|X <e)
+
(cid:88) E[f(e|X <e)]
+
OPT
(88)
(1−ϵ)2 (1−ϵ)3 ϵM
e∈O′r∩Xt
(cid:80)
≤
e∈Xt+1\O′rf(e|X <e)
+
(cid:88) E[f(e|X <e)]
+
OPT
(89)
(1−ϵ)2 (1−ϵ)3 ϵM
e∈O′r∩Xt+1
(cid:32)(cid:80) (cid:33)
<E
e∈Xt+1\Orf(e|X <e)
+
(cid:88) f(e|X <e)
+
OPT
(90)
(1−ϵ)3 (1−ϵ)3 ϵM
e∈Or∩Xt+1
E[f(Xt+1)] OPT
< + (DuetoXt+1 =(Xt+1\Or)∪(Or∩Xt+1)) (91)
(1−ϵ)3 ϵM
E[f(S)] OPT
≤ + . (92)
(1−ϵ)3 ϵM
-Ifx
t+1
∈O′r,O′r∩Xt+1 =(O′r∩Xt)∪{x t+1}andO′r\(Xt∪Y<xt)=O′r\(Xt+1∪Y<xt)∪{x t+1}. Therefore
(cid:88)
f(e|Xt)+
(cid:88) E[f(e|X <e)]
(93)
(1−ϵ)3
e∈O′r\(Xt∪Y<xt) e∈O′r∩X<yu
≤
(cid:88)
f(e|Xt)+f(x |Xt)+
(cid:88) E[f(e|X <e)]
(94)
t+1 (1−ϵ)3
e∈O′r\(Xt+1∪Y<xt) e∈O′r∩Xt
≤
(cid:88)
f(e|Xt)+
(cid:88) E[f(e|X <e)]
(95)
(1−ϵ)3
e∈O′r\(Xt+1∪Y<xt) e∈O′r∩Xt+1
(cid:80)
e∈Xt+1\O′rf(e|X <e) (cid:88) E[f(e|X<e)] OPT
< + + (96)
(1−ϵ)2 (1−ϵ)3 ϵM
e∈O′r∩Xt+1
E[f(Xt+1)] OPT
< + (97)
(1−ϵ)3 ϵM
E[f(S)] OPT
≤ + , (98)
(1−ϵ)3 ϵM
wherethetransformfrom(95)to(96)isduetoapplying(86).
b) Boundof(cid:80) f(e|Yu). Anyelemente ∈ O′r \Yu wasnotaddedintoYu attheiterationl(y )−2. By
e∈O′r\(X<yu∪Yu) u
applyingLemma1withanotethatc(O′r\(X ∪Yu))≤B−r ≤c(Yu+1),weobtain
<yu
(cid:88) f(e|Yu)≤c(O′r\(X ∪Yu)) θ (Y u+1) + OPT (99)
<yu (1−ϵ)2 ϵM
e∈O′r\(X<yu∪Yu)
θY
OPT
≤c(Yu+1) (u+1) + (100)
(1−ϵ)2 ϵM
f(Yu+1) OPT
≤ + . (101)
(1−ϵ)4 ϵMPuttheabovebounds(aandb)into(77),(79),wehave
f(O′)≤f(O′∪Xt)+f(O′∪Yu) (102)
≤f(Xt∪{r})+f(Yu∪{r})+
E[f(S)]
+
(cid:88) f(e|Y <e)
+
f(Yu+1)
+
4OPT
(103)
(1−ϵ)3 1−ϵ (1−ϵ)4 ϵM
e∈Y<xt∩O′r
E[f(S)] f(S) f(S) 4OPT
≤2f(S)+ + + + (104)
(1−ϵ)3 1−ϵ (1−ϵ)4 ϵM
(cid:18) 3 (cid:19) 4OPT 5E[f(S)]
< 2+ E[f(S)]+ < +ϵOPT (105)
(1−ϵ)4 ϵM (1−ϵ)4
wherethelastinequalityduetoϵM >4/ϵ.
Case2: Ifc(r)≥(1−ϵ)B. BasedonLemma5,weconsidersub-cases. IfoneofaorcinLemma5happens,thenLemma5
holds. Ifbothbanddhappen,wehave
2 1
f(O′)≤f(O′∪X′)+f(O′∪Y′)≤(2+ )E[f(S)]+2(2ϵ+ )OPT (106)
(1−ϵ)3 ϵM
whichcompletestheproof.