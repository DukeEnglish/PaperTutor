Empirical Bayesian image restoration by Langevin
sampling with a denoising diffusion implicit prior
Charlesquin Kemajou Mbakam1*, Jean-Francois Giovannelli2† and
Marcelo Pereyra1†
1*Mathematics and Computer Sciences, Heriot-Watt University,
Edinburgh, United Kingdom.
2Laboratoire d’Int´egration du Mat´eriau au Syst`eme, Universit´e de
Bordeaux, Bordeaux, France.
*Corresponding author(s). E-mail(s): cmk2000@hw.ac.uk;
Contributing authors: giova@ims-bordeaux.fr; mp71@hw.ac.uk;
†These authors contributed equally to this work.
Abstract
Score-baseddiffusionmethodsprovideapowerfulstrategytosolveimagerestora-
tion tasks by flexibly combining a pre-trained foundational prior model with
a likelihood function specified during test time. Such methods are predom-
inantly derived from two stochastic processes: reversing Ornstein-Uhlenbeck,
whichunderpinsthecelebrateddenoisingdiffusionprobabilisticmodels(DDPM)
and denoising diffusion implicit models (DDIM), and the Langevin diffusion
process. The solutions delivered by DDPM and DDIM are often remarkably
realistic, but they are not always consistent with measurements because of
likelihoodintractabilityissuesandtheassociatedrequiredapproximations.Alter-
natively, using a Langevin process circumvents the intractable likelihood issue,
but usually leads to restoration results of inferior quality and longer computing
times. This paper presents a novel and highly computationally efficient image
restorationmethodthatcarefullyembedsafoundationalDDPMdenoiserwithin
an empirical Bayesian Langevin algorithm, which jointly calibrates key model
hyper-parameters as it estimates the model’s posterior mean. Extensive experi-
mental results on three canonical tasks (image deblurring, super-resolution, and
inpainting)demonstratethattheproposedapproachimprovesonstate-of-the-art
strategies both in image estimation accuracy and computing time.
Keywords:EmpiricalBayesian,Diffusionmodels,Inverseproblems,Generativemodels
1
4202
peS
6
]VC.sc[
1v48340.9042:viXra1 Introduction
Score-baseddenoisingdiffusionmodelshaverecentlyemergedasapowerfulprobabilis-
tic generative modelling strategy with great potential for computer vision [1–5]. For
example, denoising diffusion models have been successfully applied to many challeng-
ing image restoration tasks, where they deliver solutions of outstanding high quality
[6–11]. Image restoration methods based on denoising diffusion models can either be
specialised for a specific problem of interest, or rely on a foundational pre-trained dif-
fusion model as an implicit image prior that is combined with a data fidelity model
specified during test time. The latter, so-called Plug-and-Play (PnP) image restora-
tion methods, can be deployed flexibly and are the focus of intense research efforts
[6, 12–17].
LeveragingapretraineddiffusionmodelasanimplicitPnPpriorforimagerestora-
tion is difficult because it requires operating with a likelihood function that is
computationally intractable [6]. A variety of approximations have been proposed in
the literature to address this fundamental likelihood intractability issue [6, 12–15].
However, the use of likelihood approximations can lead to restoration results that are
not always consistent with the observed data [13]. As a result, existing PnP denoising
diffusion methods for image restoration are less robust than alternative approaches
and typically require heavy fine-tuning.
This paper fully circumvents the intractable likelihood issue by embedding a
foundational pretrained denoising diffusion prior within a PnP unadjusted Langevin
algorithm [18]. This allows operating directly with the original likelihood function,
which is tractable, as well as leveraging empirical Bayesian techniques to automati-
cally adjust model regularisation parameters [19]. The performance of the proposed
Bayesian image restoration method is demonstrated through experiments related to
image deblurring, super-resolution, and inpainting, and through comparisons with
competing approaches from the state of the art. In these experiments, our proposed
method outperforms the state of the art in terms of accuracy (see visual examples in
Figure 1), and achieves highly competitive computing times.
2 Background
2.0.1 Denoising diffusion models
Consider a target distribution of interest π on Rd represented by a training dataset
{x′}M . Denoising diffusion models are a class of stochastic generative models that
i i=1
drawnewsamplesfromπ byapproximatelyreversingtheOrnstein-Uhlenbeckprocess
[20].
1 (cid:112)
dX =− β X dt+ β dW , X ∼π, (1)
t 2 t t t t 0
whereW isad−dimensionalBrownianmotionandβ :t(cid:55)→β(t)isapositiveweighting
t t
function [21]. Under mild assumptions on π, the backwards process associated with
(1) is given by [5, 22]
dX =[−1 β X −β ∇logp (X )]dt+(cid:112) β dW¯ , (2)
t 2 t t t t t t t
2Measurement y Ours Ground truth x⋆
Fig. 1: Restoration examples of our method: we present the restored images, corre-
sponding measurements, and ground truth for three common image restoration tasks.
where p is the marginal density of X , W¯ is a d−dimensional Brownian motion, and
t t t
t now flows backwards from infinity to t=0.
To use (2) for generative modelling, we leverage that (1) and (2) are stochastic
transport maps between the target X ∼ π and a standard normal random variable
0
X ∼N(0,I ).Moreprecisely,denoisingdiffusionmodelsseektousethetrainingdata
∞ d
{x }M anddenoisingscore-matchingtechniques[23]toobtainestimatorsofthescore
i i=1
functions x(cid:55)→∇logp (x), and generate new samples from π by drawing a sample x
t ∞
from X ∼N(0,I ) and solving (2) to t=0.
∞ d
Inpractice,(2)issolvedapproximatelybyusingatime-discretenumericalmethod,
and the process is initially with X ∼ N(0,I ) for some large finite T. Denoising
T d
diffusion probabilistic models (DDPM) [2] and denoising diffusion implicit models
(DDIM) [3] are the two main methodologies to implement this strategy. Accurate
estimationofthescores∇logp (x)underpinning(2)hasbeencrucialtothesuccessof
t
DDPM and DDIM. This has been achieved using large training datasets, specialised
network architectures, weighted denoising score-marking techniques [23], and high-
performance computing, [2, 3].
3
-repuS
gnirrulbeD
gnitniapnI
)4×(noituloser2.0.2 Bayesian image restoration with a denoising diffusion prior.
We now consider the recovery of an unknown image x⋆ ∈ Rd from some mea-
surement y ∈ Rn. We assume that x⋆ is related to y by a statistical observation
model with likelihood function x (cid:55)→ p(y|x), which we henceforth denote by ℓ (x).
y
Throughout the paper, we pay special attention to Gaussian likelihoods of the form
ℓ (x)∝exp(−||y−Ax||2)/(2σ2), where A∈Rn×d models deterministic instrumental
y 2
aspects of the measurement process and σ2 the measurement noise.
Suchimagerestorationproblemsareusuallyill-conditionedorill-posedandrequire
regularisationinordertodelivermeaningfulsolutions[24].IntheBayesianframework,
this is achieved by modelling x⋆ as a realisation of a random variable x with an
informative marginal distribution (the so-called prior) and y as a realisation of the
conditional random variable (y|x = x⋆) with density p(y|x⋆), and applying Bayes’
theorem to derive the posterior distribution of (x|y = y) [24]. Choosing a suitable
prior for x is the crux to delivering accurate results.
Following the remarkable success of DDPM and DDIM for generative tasks, there
has been significant interest in leveraging denoising diffusion models for Bayesian
image restoration. This would allow generating approximate samples from the poste-
rior distribution for (x|y = y) that has π as prior for x [6, 12, 13]. In principle, this
can be achieved by using a conditional variant of (2), given by
(cid:20) (cid:21)
dX = −β tX −β ∇ logp (X |y) dt+(cid:112) β dW¯ . (3)
t 2 t t xt t t t t
The conditional score ∇ logp (x |y) can be learned offline by using an augmented
xt t t
training dataset {x′,y′}M , but the resulting methods are highly problem-specific. A
i i i=1
flexible alternative is to use the decomposition ∇ logp (x |y) = ∇ logp (y|x )+
xt t t xt t t
∇ logp (x )inordertocombineafoundationalpre-trainedDDPMorDDIMencod-
xt t t
ing the prior score ∇ logp (x ) with a likelihood function specified during test time.
xt t t
However, the score ∇ logp (y|x ) is computational intractable, as calculating the
(cid:82)
xt t t
integralp (y|x )= ℓ (x )p (x |x )dx isgenerallynotpossibleevenifthelikelihood
t t y 0 t 0 t 0
ℓ associated with t=0 is known.
y
To address this challenge, several approximations to ∇ logp (y|x ) have been
xt t t
proposed recently. Notably, DPS [6] modifies a DDPM targeting π by incorporating a
gradientsteponlogℓ (xˆ (x ))thatseekstopromoteconsistencywithy,wherexˆ (x )
y 0 t 0 t
is an estimator of x derived from the score estimates underpinning the DDPM. DPS
0
can deliver highly accurate results, but has a high computational cost and requires
considerable fine-tuning to deliver solutions that are consistent with y. DDRM [14]
and ΠGDM [12] improve on DPS by improving the approximation of ∇ logp (y|x )
xt t t
for linear Gaussian inverse problems and by replacing the foundational DPS with a
DDIM which is significantly faster.
More recently, DiffPIR [13] modifies a foundational DDIM by introducing a half-
quadratic splitting that decouples the likelihood term and the prior. This relaxation
leads to a modified DDIM in which, at each iteration, the prior score ∇ logp (x ) is
xt t t
corrected towards ∇ logp (x |y) through an implicit gradient step on logℓ (xˆ (x ))
xt t t y 0 t
that promotes consistency with y, with the weight of the correction increasing across
4iterations. DiffPIR is significantly more robust than DPS and delivers state-of-the-art
performance in less iterations. Moreover, SGS[15] also uses half-quadratic splitting to
modify a DDIM foundational prior, but relies on a stochastic sampling step to intro-
duce consistency with y rather than a gradient step, leading to a DDIM-within-Gibbs
sampling algorithm. MCGdiff [25] also focuses on linear Gaussian inverse problems
and exploits a sequential Monte Carlo strategy to deal with the intractable score
∇logp (y|x ) in a manner that is theoretically rigorous, but more computationally
t t
expensive by comparison.
2.0.3 Image restoration by Plug-and-Play Langevin sampling
Langevin sampling algorithms provide an alternative Bayesian strategy to leverage
data-driven priors obtained by denoising score-matching. In the context of plug-and-
play image restoration, these algorithms are derived from the Langevin diffusion [18,
26]
dX = 1 ∇logℓ (X )+ 1 ∇logp (X )dt+(cid:112) β dW¯ , (4)
s 2 y s 2 λ s s s
where we note that the likelihood ℓ appears explicitly rather than within an
y
intractable integral. The prior density p above, parametrised by λ>0, is equivalent
λ
to p in (2) for some t = β−1(λ) (i.e., p is the marginal density of the random
t⋆ ⋆ λ
variable x ∼N(x,λI ) when x∼π).
λ d
To use (4) to approximately sample from the posterior for (x|y = y) that has
π as prior, one should solve (4) for a long integration time and use a small value
of λ such that p is close to π. In practice, (4) is solved approximately by using
λ
a discrete-time numerical integration scheme and by replacing ∇logp (x) with an
λ
estimate ∇logp (x) ≈ (D (x) − x)/λ, where D a denoising operator trained to
λ λ λ
restore x ∼ π from a noisy realisation contaminated with Gaussian noise of variance
λ. For example, considering an Euler-Maruyama approximation of (4) leads to the
PnP-ULA [18], defined by the following recursion,
(cid:112)
X =X +γ∇ logℓ (X )+γ[D (X )−X ]/λ+ 2γζ , (5)
k+1 k x y k λ k k k+1
where γ > 0 is the step size and {ζ ,k ∈N} is a family of i.i.d. standard Gaussian
k
random variables. Although in principle PnP-ULAs could use the same state-of-the-
art denoisers that underpin foundational DDPM and DDIM, for stability reasons,
PnP-ULAs often rely on denoisers that are Lipschitz-regularised and that have been
trainedforaspecificvalueofλthatachievesasuitabletrade-offbetweenaccuracyand
computing speed. As a result, while PnP-ULAs are better than DDPM and DDIM
strategies at promoting consistency with the measurement y because they involve
ℓ directly, they struggle to deliver solutions that exhibit the amount of fine detail
y
achieved by DDPM or DDIM.
With regards to computational efficiency, DDPM and DDIM can produce an indi-
vidual Monte Carlo sample from the posterior distribution in significantly less neural
function evaluations (NFE)s than existing PnP-ULA, particularly DDIM. However,
Bayesian computation often requires producing a large number samples, e.g., to com-
pute Bayesian estimators and other posterior quantities of interest. Unlike DDPM or
5DDIM,PnP-ULAcanproduceadditionalsampleswitharelativelylowadditionalcost
once it attains stationarity. In our experience, in image restoration problems, DDIM
and PnP-ULA are broadly comparable in terms of NFE cost if at least a hundred
nearly-independent samples are required.
3 Proposed method
We are now ready to present our proposed method to leverage a foundational pre-
trained DDPM as image prior to perform image restoration tasks. Our method is
formulated within an empirical Bayesian framework, which allows the method to self-
calibrate a key regularisation parameter as it draws Monte Carlo samples to compute
theposteriormean.ThisempiricalBayesianstrategyisimplementedinahighlycom-
putationally efficient manner by exploiting a formulation of PnP-ULA that operates
in a latent space, which significantly accelerates the convergence speed of PnP-ULA
and improves the accuracy of the delivered solutions.
3.1 Embeding an implicit DDPM prior within PnP-ULA
Modern foundational score-based models are trained without Lipschitz regularisation
to avoid degrading their performance. The lack of regularisation makes using these
models within PnP-ULA difficult (i.e., small errors in the scores manifest as recon-
struction artefacts that can cause PnP-ULA to diverge, see [18, 27]). We address this
challenge by adopting an equivariant PnP approach [18].
More precisely, to embed a pre-trained DDPM within PnP-ULA we: i) use the
final iterations of DDPM as a denoiser, and ii) postulate that the underlying prior
π is invariant to a group of transformations and force the DDPM denoiser to be
equivariant to these transformations by averaging it over the group. For example,
possible transformations are reflections, small translations, or rotations.
TheresultingPnPpriorisconstructedasfollows.LetG beacompactgroupacting
on Rd, whose action is represented by the invertible linear mappings T [28]. Assume
g
thatπ isdistributionG-invariant;i.e.,forallg ∈G,wehavethatT x∼π (asaresult,
g
the posterior mean of (x|x+λϵ=x) with ϵ∼N(0,I) is G-equivariant). Moreover, let
Ψ denote the Markov kernel associated with a single DDPM iteration evaluated at
βt
the noise schedule β , i.e.,
t
1 (cid:18) ηβ (cid:19) (cid:113)
Ψ (x)= √ x− √ t ϵ (x,t) + ηβ˜ϵ, ϵ∼N(0,I). (6)
βt α 1−α¯ θ t
t t
where α = 1−β , α¯ = (cid:81)t α , β˜ = β (1−α¯ )/(1−α¯ ) and ϵ (x,t) is the
t t t s=0 s t t t−1 t θ
underlying score estimate (e.g., obtained by weighted denoising score-matching [23]).
To use (6) to construct a G-equivariant denoiser D for Gaussian noise of variance λ,
λ
we find t =β−1(λ) and define D as follows:
⋆ λ
D (x)=T−1Ψ ◦...◦Ψ T x, g ∼U . (7)
λ g β0 βt⋆ g G
6where U is the uniform distribution on G. Note that (6) has an additional parameter
G
η >0 to control the stochasticity of D (η =1 reduces (6) to the original DDPM, we
λ
find that η =2 improves stability and PSNR performance).
3.2 Latent-space PnP-ULA
We are now ready to embed the equivariant PnP denoiser D into ULA. We use a
λ
latent space formulation of ULA that explicitly acknowledges that D is imperfect.
λ
This leads to improved accuracy, stability, and convergence speed.
In a manner akin to [13, 15], we use splitting to decouple the prior from the
likelihood function. This is achieved by introducing an auxiliary random variable z∈
Rd related to x by the conditional distribution x|z∼N(z,ρI ), where ρ>0 controls
d
∥x−z||2. We assign the equivariant PnP prior to z by assuming that
2
∇logp(z)≈[D (z)−z]/λ,
λ
for all z ∈ Rd. The likelihood function ℓ relates x to y. To derive the likelihood for
y
z we marginalise x. More precisely, we seek to implement a PnP-ULA targeting the
marginal posterior (z|y=y), which requires the likelihood score z (cid:55)→∇ logp(y|z,ρ).
z
From Fisher’s identity, this marginal score is given by [29]
∇ zlogp(y|z;ρ)=E x|y,z,ρ[∇ zlogp(x|z;ρ)]. (8)
Note that (8) is analytically tractable in image restoration problems where ℓ (x) is
y
a Gaussian likelihood function, as in that case (x|y,z) is also Gaussian. In addition,
∇ logp(x|z;ρ) is linear in x. Therefore, in restoration problems of the form ℓ (x) ∝
z y
exp(−||y−Ax||2)/(2σ2) (8) becomes
2
∇ zlogp(y|z;ρ)=[z−E x|y,z,ρ[x]]/ρ. (9)
The resulting PnP-ULA algorithm to target (z|y = y) is given by the following
recursion: for any Z ∈Rd and k ∈N
0
(cid:112)
Z =Z +γ∇ logp(y|Z ,ρ)+γ[D (Z )−Z )]/λ+ 2γζ , (10)
k+1 k z k λ k k k+1
where γ >0 represents the step size, {ζ ,k ∈N} is a sequence of i.i.d. d-dimensional
k
standard Gaussian random variables.
We then use the Monte Carlo samples generated by (10), targeting (z|y = y), to
compute the expectation of functions of interest ϕ w.r.t. the correct posterior (x|y=
y). This is efficiently achieved by using the estimator [29]
K
1 (cid:88)
E x|y,ρ[ϕ(x)]≈
K
E x|y,Zk,ρ[ϕ(x)], (11)
k=1
where we note again that z (cid:55)→ E x|y,z,ρ[ϕ(x)] is tractable analytically for most ϕ of
interest because (x|y,z,ρ) is Gaussian. In particular, in our experiments, we use (11)
7to compute the posterior mean of (x|y=y,ρ). Lastly, it is worth noting that because
of the action of ρ, the marginal likelihood z (cid:55)→ p(y|z,ρ) is strongly log-concave and
oftensignificantlybetterconditionedthantheoriginallikelihoodℓ (x).Consequently,
y
targeting (z|y = y,ρ) instead of (x|y = y,ρ) often leads to significant improvements
in convergence speed [29, 30].
3.3 Maximum likelihood estimation of ρ.
The performance of the proposed PnP-ULA depends critically on the choice of ρ. We
adoptanempiricalBayesianstrategyandproposetosetρautomaticallybymaximum
marginal likelihood estimation (MMLE) [24], i.e.
ρˆ(y)∈argmaxp(y|ρ). (12)
ρ∈R
+
where the marginal likelihood p(y|ρ)=E x,z|y,ρ[ℓ y(x)] for all y ∈Rn and ρ>0.
To solve (12) and simultaneously obtain samples from the calibrated posterior
distribution (z|y = y,ρˆ), we embed (10) within a stochastic approximation proximal
gradient (SAPG) scheme that iteratively optimises ρ and guides PnP-ULA towards
(z|y = y,ρ) [19]. Given the samples (z|y = y,ρˆ), we compute the posterior mean of
(x|y=y,ρˆ) by using (11). The SAPG scheme is given by the following recursion: for
any ρ >0, Z ∈Rd and k ∈N
0 0
Z =Z +γ[Z −X¯ ]/ρ +γ[D (Z )−Z )]/λ+(cid:112) 2γζ ,
k+1 k k k k λ k k k+1
(13)
ρ
k+1
=ΠR +(cid:2) ρ k+δ k+1∇ ρlogp(X¯ k+1,Z k+1|y,ρ k)(cid:3) ,
whereX¯ k+1 =Σ− ρ1[A⊤y/σ2+Z k+1/ρ k+1]istheexpectationof(x|Z k+1,y,ρ k),(δ k) k∈N
is a non-increasing positive sequence, and ΠR
+
is the projection on R +.
Algorithm 1 below summarises the proposed methodology. For efficiency, we rec-
ommend setting the total number of iterations N such that the algorithm stops when
the posterior mean stabilises (we use N =100 in our experiments).
Algorithm 1 Equivariant-DDPM Latent-Space PnP-ULA
1: Initialization: {ρ 0,X 0,Z 0}, define γ,τ,λ, {δ n} n∈N and N.
2: for k =0:N −1 do
3: Sample ζ k+1 ∼N(0,Id), √
4: Z k+1 =Z k+γ[Z k−X¯ k]/ρ k+γ[D λ(Z k)−Z k]/λ+ 2γζ k+1,
5: X¯ k+1 =Σ− ρ1[A⊤y/σ2+Z k+1/ρ k],
6: ρ n+1 =ΠR +(cid:2) ρ k+δ k+1∇ ρlogp(X¯ k+1,Z k+1|y,ρ k)(cid:3) ,
7: end for
8: X¯ MMSE = N1 (cid:80)N l=− 01X¯ l.
8Fig. 2: Sample images from FFHQ 256×256 dataset [31].
Fig. 3: Sample images from ImageNet 256×256 dataset [32].
4 Experiments
4.1 Experimental setup.
We evaluate our proposed method on 2 datasets: FFHQ 256×256 [31] and ImageNet
256×256 [32]; Figure 2 and Figure 3 depicts sample images from these datasets. We
implementourmethodswiththefoundationalDDPM1,andsetη =2andλ=1.5/255
(inthiscase,evaluatingD requiresthreeevaluationsofDDPM).Wesetγandδ auto-
λ k
matically for each experiment by following the guidelines [18, 19] (see supplementary
material).
Wedemonstratetheproposedapproachonthreecanonicalimagerestorationtasks:
Image deblurring, inpainting and super-resolution (SR). For image deblurring, we
consider a Gaussian blur operator of size 7×7 pixels with bandwidth 3 pixels and a
motion blur operator of size 25×25. For inpainting, we randomly mask 50% and 70%
of the pixels. For SR, we consider 4× bicubic downsampling and mitigate aliasing
by using a Gaussian anti-aliasing filter of bandwidth 3 pixels. In all experiments, we
consider additive Gaussian noise with a standard deviation σ = 1/255. Furthermore,
we also conducted deblurring experiments with relatively high noise variance σ =
2.55/255.
We report comparisons with three strategies from the state-of-the-art, imple-
mented by using the same foundational DDPM: DPS2[6], DiffPIR3 [13], SGS[15]. For
completeness, we also include comparison with DPIR [33], which addresses image
inverse problems in an optimisation framework. For each method, we report peak
signal-to-noiseratio(PSNR),structuresimilaritymeasureindex(SSIM)[34],Learned
Perceptual Image Patch Similarity (LPIPS) [35], and Fr´echet Inception Distance
1Modelcheckpoints
2DPSimplementation
3DiffPIRimplementation
9Table1:QuantitativeresultsonFFHQ256×256datasetforGaussiandeblurringexperiments:
average PSNR (dB), SSIM, LPIPS, and computing time (s) for our method, DPS [6], SGS
[15], DPIR [33], and DiffPIR [13].
σ=1/255 σ=2.25/255
Methods
PSNR↑ SSIM↑ LPIPS↓ FID↓ PSNR↑ SSIM↑ LPIPS↓ FID↓ Time(s)
DPS 28.02 0.8 0.19 0.44 26.92 0.75 0.24 0.46 143
SGS 31.36 0.87 0.18 0.20 24.42 0.82 0.3 0.22 29
DPIR 34.97 0.91 0.12 0.16 32.52 0.85 0.19 0.31 −
DiffPIR 32.35 0.90 0.11 0.13 30.85 0.87 0.14 0.13 27
Oursη=2 36.27 0.94 0.09 0.12 34.21 0.92 0.14 0.20 33
Ours,η=1 35.98 0.94 0.08 0.23 33.80 0.91 0.13 0.28 33
Table 2: Quantitative results on FFHQ 256×256 dataset for Motion deblurring
experiments: average PSNR (dB), SSIM, LPIPS, and computing time (s) for our
method, DPS [6], SGS [15], DPIR [33], and DiffPIR [13].
σ=1/255 σ=2.25/255
Methods
PSNR↑ SSIM↑ LPIPS↓ FID↓ PSNR↑ SSIM↑ LPIPS↓ FID↓
DPS 23.92 0.59 0.48 0.53 23.0 0.55 0.49 0.55
SGS 30.04 0.82 0.17 0.42 23.87 0.77 0.34 0.20
DPIR 36.02 0.93 0.09 0.11 32.10 0.84 0.20 0.25
DiffPIR 31.76 0.91 0.10 0.13 30.06 0.87 0.14 0.15
Oursη=2 37.97 0.96 0.09 0.14 34.74 0.93 0.12 0.21
Ours,η=1 37.80 0.95 0.10 0.18 34.61 0.92 0.13 0.18
(FID). We emphasise that the performance of each method, as measured by PSNR,
SSIM, or LPIPS, can be marginally improved by fine-tuning their parameters and
NFE budget. In this work, we prioritise PSNR and SSIM, and implement all methods
in a manner that delivers the state-of-the-art PSNR and SSIM performance with a
competitive computing time. Accordingly, we implement DPS with 1000 DDPM iter-
ations, DiffPIR also with 1000 DDPM iterations, SGS with 100 Gibbs iterations, and
our proposed method also with 100 ULA iterations and a 30% burn-in period. It is
worth noting that DiffPIR can deliver good results with significantly less than 1000
DDPM iterations at the expense of a deterioration in consistency with y and PSNR
[13].
4.2 Quantitative and qualitative results.
For the quantitative results, we used metrics such as PSNR, SSIM, LPIPS, FID and
computingtimestoevaluateourmethodalongsideDPS,SGS,DPIRandDiffPIR.For
10Table3:QuantitativeresultsforGaussiandeblurringexperimentswith12testimages
from ImageNet 256×256 dataset: average PSNR (dB), SSIM, LPIPS and FID for our
method, DPS [6], SGS [15], DPIR [33] and DiffPIR [13].
σ=1/255 σ=2.25/255
Methods
PSNR↑ SSIM↑ LPIPS↓ FID↓ PSNR↑ SSIM↑ LPIPS↓ FID↓
DPS 25.09 0.63 0.41 3.30 24.80 0.63 0.41 3.70
SGS 29.45 0.80 0.19 0.88 28.15 0.76 0.26 1.71
DiffPIR 28.02 0.82 0.13 1.10 26.84 0.76 0.20 1.72
DPIR 31.47 0.85 0.16 0.97 29.24 0.77 0.25 1.32
Oursη=2 31.94 0.87 0.13 0.79 30.31 0.83 0.18 1.40
Table 4: Quantitative results for Motion deblurring experiments with 12 test images
from ImageNet 256×256 dataset: average PSNR (dB), SSIM, LPIPS and FID for our
method, DPS [6], SGS [15], DPIR [33] and DiffPIR [13].
σ=1/255 σ=2.25/255
Methods
PSNR↑ SSIM↑ LPIPS↓ FID↓ PSNR↑ SSIM↑ LPIPS↓ FID↓
DPS 22.68 0.58 0.48 3.92 22.45 0.57 0.47 4.05
SGS 30.37 0.79 0.15 0.83 28.09 0.76 0.25 1.15
DiffPIR 27.38 0.87 0.08 0.58 25.96 0.76 0.16 1.05
DPIR 33.30 0.89 0.10 0.84 29.63 0.79 0.20 1.04
Oursη=2 33.28 0.90 0.09 0.57 29.88 0.80 0.14 0.87
completeness, we report our method with η = 1((D coincides with an equivariant
λ
DDPM)forthedeblurringexperiments.Inallourexperiments,weused100testimages
fromtheFFHQ256×256datasetand12imagesfromtheImageNet256×256dataset.
We evaluate the FID after extracting 25 patches of size 64×64 from each image.
Table 1, Table 2, Table 3 and Table 4 summarise the results for the deblurring
problems on FFHQ and ImageNet datasets. It can be seen that the proposed method
achievessuperiorsperformancecomparedtoallothercomparisonmethodsintermsof
PSNR, SSIM and LPIPS on both datasets. The only exception is FID, where DiffPIR
sometimes outperforms our method. In Figure 4 and Figure 5 depict the qualitative
results of Gaussian and motion deblurring problems.
Table 5 and Table 6 summarises the experimental results for the inpainting exper-
iment on both FFHQ and ImageNet datasets. we observe that our proposed method
outperformstheothermethodsintermsofPSNRandSSIM.Conversely,inthisexper-
iment DiffPIR achieves a better LPIPS and FID. Also, notice that SGS was unstable
11for the inpainting experiments despite extensive fine-tuning. For inpainting experi-
ment, we excluded DPIR since it lacks initialisation support for random masks (see
[13]formoredetails).Figure7showsthequalitativeresultsoftheinpaintingproblem.
Finally, for the super-resolution problem, Table 7 summarise the results of the
experiments on FFHQ 256×256 dataset. It can be seen that our method significantly
outperforms other methods in terms of PSNR and SSIM values. Conversely, the pro-
posed method achieves comparable results with DiffPIR in terms of LPIPS and FID.
Figure 7 shows the qualitative results of the inpainting problem.
Our findings reveal that our approach delivers high quality reconstructions across
most experiments. However, since we are computing a posterior mean, our solutions
have less fine detail than the results of DiffPIR, which do not average over a posterior
distribution. Although DiffPIR delivers excellent restoration results with more fine
detailcomparedtoourmethod,thisdetailisnotgenerallyconsistentwiththeground
truth. The results delivered by SGS in the Gaussian and motion debluring and SR
experiments have some small residual noise and background artefacts from numerical
instability. DPIR delivers high quality reconstructions with fine details, but cannot
beusedtogeneratesamplefromtheposteriorbecauseitisanoptimisationapproach.
Lastly, the results from DPS are less accurate and can differ significantly from the
ground truth because of likelihood consistency issues.
x y DPS(26.5) DiffPIR(31.5) DPIR(32.4) SGS(30.6) Ours(33.14)
x y DPS(23.9) DiffPIR(27.0) DPIR(29.6) SGS(28.7) Ours(30.1)
Fig. 4: Qualitative results on FFHQ 256×256 (first row) and ImageNet 256×256
(secondrow)datasets-Gaussiandeblurringexperiment:truthx,measurementy,DPS
[6], DiffPIR[13]. From the left to the right, we have truth x⋆, measurement y, DPS
[6], DiffPIR[13], DPIR [33], SGS [15], and our method (η = 2). We also report the
reconstruction PSNR (dB). Observation noise variance σ =1/255.
4.3 Qualitative results.
In Figure 4 and Figure 5 we compare our approach with DiffPIR, SGS, DPIR and
DPS on Gaussian and motion deblurring. In Figure 7 we compare our approach with
DiffPIR, SGS and DPS on image inpainting with random mask. Moreover, we com-
pare in Figure 6 our approach with DiffPIR, DPIR, SGS and DPS on 4× SR. Our
12x y DPS(24.1) DiffPIR(33.0) DPIR(34.2) SGS(31.5) Ours(35.5)
x y DPS(21.5) DiffPIR(27.3) DPIR(31.8) SGS(29.8) Ours(32.3)
Fig. 5: Qualitative results on FFHQ 256×256 (first row) and ImageNet 256×256
(second row) datasets - Motion deblurring experiment. From the left to the right, we
have truth x⋆, measurement y, DPS [6], DiffPIR[13], DPIR [33], SGS [15], and our
method (η = 2). We also report the reconstruction PSNR (dB). Observation noise
variance σ =1/255.
findings reveal that our approach delivers high quality reconstructions across most
experiments. However, since we are computing a posterior mean, our solutions have
less fine detail than the results of DiffPIR, which do not average over a posterior dis-
tribution.However,whileDiffPIRdeliversexcellentrestorationresultsthathavemore
fine detail than our method, this detail is not generally consistent with the ground
truth. The results delivered by SGS in the Gaussian and motion debluring and SR
experiments have some small residual noise and background artefacts from numerical
instability. DPIR delivers high quality reconstructions with fine details, but cannot
beusedtogeneratesamplefromtheposteriorbecauseitisanoptimisationapproach.
Lastly, the results from DPS are less accurate and can differ significantly from the
ground truth because of likelihood consistency issues.
Table 5:Quantitativeresultswith100testimagesfromFFHQ256×256-Inpainting
experiment: average PSNR (dB), SSIM, LPIPS and FID for our method, DPS [6],
SGS [15], DPIR [33] and DiffPIR[13]. The measurement noise σ is set to 1/255.
Rate = 70% Rate = 50%
Methods
PSNR↑ SSIM↑ LPIPS↓ FID↓ PSNR↑ SSIM↑ LPIPS↓ FID↓
DPS 30.88 0.87 0.16 0.48 32.92 0.90 0.14 0.48
SGS 17.70 0.71 0.35 3.07 22.60 0.82 0.25 1.22
DiffPIR 34.0 0.92 0.08 0.11 37.2 0.95 0.06 0.07
Ours 34.20 0.93 0.10 0.25 37.4 0.96 0.05 0.15
13Table 6: Quantitative results with 100 test images from ImageNet 256×256 datasets
-Inpaintingexperiment:averagePSNR(dB),SSIM,LPIPSandFIDforourmethod,
DPS [6], SGS [15], DPIR [33] and DiffPIR[13]. The measurement noise σ is set to
1/255.
Rate = 70% Rate = 50%
Methods
PSNR↑ SSIM↑ LPIPS↓ FID↓ PSNR↑ SSIM↑ LPIPS↓ FID↓
DPS 26.51 0.68 0.39 3.27 27.66 0.71 0.36 2.99
SGS 17.49 0.64 0.37 3.66 21.54 0.75 0.25 2.28
DiffPIR 29.98 0.85 0.15 0.91 32.4 0.90 0.08 0.54
Ours 29.74 0.84 0.17 1.34 32.47 0.91 0.08 0.75
Table 7: Quantitative results with 100 test images from FFHQ 256×256 dataset -
4× super-resolution experiment: average PSNR (dB), SSIM, LPIPS and FID for our
method, DPS [6], SGS [15], DPIR [33] and DiffPIR[13]. The measurement noise is set
to 1/255.
Methods PSNR↑ SSIM↑ LPIPS↓ FID↓
DPS 24.28 0.70 0.26 0.67
SGS 25.99 0.80 0.24 0.80
DPIR 31.0 0.87 0.21 0.47
DiffPIR 29.07 0.83 0.17 0.20
Ours 31.14 0.87 0.21 0.50
x y DPS(23.0) DiffPIR(28.2) DPIR(28.2) SGS(28.4) Ours(28.8)
Fig. 6: Qualitative results - 4× super-resolution experiment on FFHQ 256×256
dataset. From the left to the right, we have truth x⋆, measurement y, DPS [6],
DiffPIR[13], DPIR [33], SGS [15], and our method (η =2). We also report the recon-
struction PSNR (dB). Observation noise variance σ =1/255.
4.4 Ablation study
4.4.1 Effect of modifying DDPM by using η ̸= 1.
To assess the influence of the parameter η, we performed an additional Gaussian
deblurring experiment using different values of η ∈ [0.1,5]. We observe in Figure 8a
thattheproposedmethodimplementedwiththeconventionalDDPMdenoiser(η =1)
14x y DPS(30.28) DiffPIR(34.31dB) SGS(14.82) Ours(34.72)
x y DPS(25.0) DiffPIR(27.4) SGS(23.2) Ours(30.0)
Fig. 7: Qualitative results - image inpainting experiment on FFHQ 256×256 (first
row) and ImageNet 256×256 (second row). From the left to the right, we have truth
x⋆, measurement y, DPS [6], DiffPIR[13], SGS [15], and our method (η =2). We also
report the reconstruction PSNR (dB). Observation noise variance σ =1/255.
performsstronglyinperceptualquality,asmeasuredbyLPIPS,butthatbetterPSNR
performance is achieved by taking η ≈2.
(a) Effect of η (b) Effect of NFEs
Fig. 8: Ablation study: effect on the image restoration performance (PSNR and
LPIPS) of η and of the number of neural function evaluations (NFE)s, controlled
through λ.
4.4.2 Effect of λ and NFEs.
The parameter λ controls the noise level of the PnP denoiser D . Because D is
λ λ
implemented through a DDPM scheme from t⋆ =β−1(λ) to t=0, λ also controls the
number of DDPM iterations required to evaluate D and therefore the NFE cost of
λ
each PnP-ULA iteration. In our previous experiments, we used λ = 1.5/255, which
corresponds to 3 NFEs of the foundational DDPM prior per PnP-ULA iteration.
15Figure8bshowstheimpactofusingadifferentvalueofλfortheGaussiandeblurring
experiment,asafunctionoftheNFEcost.Weobservethatusingalargevalueofλ,in
additiontoanadditionalNFEcost,alsodeterioratesthequalityofthereconstruction
resultsinPSNRandLPIPSbecauseitintroducesexcessivepriorsmoothing(see[18]).
Setting λ too small also leads to poor reconstruction results because of significant
errors in the estimated scores (accurately estimating the scores becomes harder as
λ→0). See the supplementary material for more details.
4.4.3 Effects of equivariance and of the latent-space formulation of
ULA.
WenowanalysetheeffectofoperatingwiththelatentspaceformulationofPnP-ULA,
as opposed to running PnP-ULA directly in the ambient space (i.e., ρ = 0 to force
x=z), as well as the effect of equivariance in PnP-ULA.
To illustrate the estimation accuracy benefit of the latent space formulation of
PnP-ULA, Figure 9a shows the estimation PSNR achieved by the equivariant latent-
space PnP-ULA for different values of ρ. We observe that decoupling x from z by
allowing ρ>0 can significantly improve estimation performance, especially if ρ is set
carefully. Fortunately, the proposed SAPG scheme is able to estimate ρ remarkably
well, as evidenced by the fact that the MMLE ρˆ(y) is very close to the optimal value
ρ thatmaximisestherestorationPSNR.Inaddition,weobserveinFigure9bthatthe
†
SAPG scheme converges to the MMLE ρˆ(y) very quickly, in roughly 100 iterations.
This is consistent with other SAPG results and stems from the fact that the marginal
likelihood is very concentrated [19].
Operating in the latent space also significantly improves the convergence speed
of ULA schemes [29]. To evidence this phenomenon, Figure 9c shows the evolution
of the PSNR of the posterior mean as estimated by an equivariant PnP-ULA imple-
ment directly in the ambient space (ρ = 0). Observe that ambient space PnP-ULA
requiresthousandsofiterationstoconverge,whereaslatentspacePnP-ULAconverges
in approximately 100 iterations (see Figure 9d).
Equivariance plays a central role in stabilising PnP-ULA by mitigating the effect
of errors in the learnt score functions [27]. This is illustrated in Figure 9d, where we
see that a latent-space PnP-ULA without equivariance does not stabilise properly.
This is related to the fact that the errors in the score functions lead to resonance
modesinposteriordistribution,whichdonotappearduringthefirstiterations,butare
eventually identified as PnP-ULA explores the solution space more fully (see [18, 27]
for more details about artefacts in PnP-ULA and the role of equivariance).
5 Conclusion, limitations, and future work
Thispaperpresentedanovelplug-and-playimagerestorationmethodologythatrelies
onafoundationaldenoisingdiffusionprobabilisticmodelasimageprior.Theproposed
methodology is formulated within an empirical Bayesian framework. This allows to
automatically calibrate a key regularisation parameter in the model simultaneously
as we draw Monte Carlo samples from the model’s posterior distribution to com-
pute the posterior mean. This empirical Bayesian strategy is implemented in a highly
16(a) Calibration (b) SAPG iterates ρ
k
(c) Ambient space PnP-ULA (ρ=0) (d) Effect of equivariance
Fig. 9: Ablation study: a) effect of ρ on image restoration PSNR (dB), highlighting
the optimal value ρ† and the obtained estimate ρˆ(y); (b) evolution of the iterates
(ρ ) generatedbySAPGasitsolves(12);c)EvolutionoftheestimationPSNRfor
k k∈N
an equivariant PnP-ULA operating directly on the ambient space, without a latent
space representation (ρ = 0); (d) Evolution of the estimation PSNR for PnP-ULA
with equivariance and without equivariance, as a function of the iterates k.
computationally efficient manner by intimately combining an equivariant plug-and-
play unadjusted Langevin algorithm [18, 27] formulated in a latent space [29], with
a stochastic approximation proximal gradient scheme [19]. We demonstrate through
experiments and comparisons with competing approaches that our proposed method
outperforms the state of the art performance across a range of canonical image
restoration tasks, also achieving highly competitive computing times.
With regards to the main limitations of our method and perspective for future
work, although we have detailed theoretical results for both the plug-and-play unad-
justed Langevin algorithm [18] and the stochastic approximation proximal gradient
17scheme [36], these results are not currently integrated and therefore we cannot pro-
vide convergence guarantees for the proposed scheme. Moreover, the performance of
the method is highly sensitive the choice of λ, which is currently adjusted by cross-
validation.Futureresearchshouldexploretheautomaticcalibrationofλtogetherwith
ρbyjointmaximummarginallikelihoodestimation.Inaddition,ourmethodcurrently
reports the minimum-mean-squared-error Bayesian estimator given by the posterior
mean.However,itiswellknownthatthisestimatordoesnotaccuratelypreservesome
of the fine detail in the posterior distribution. It would be interesting to explore other
Bayesian estimators that are better aligned with visual perception quality criteria.
Also, throughout this paper we assume that the forward operator and the noise level
specifying the likelihood function are perfectly known. Future research could consider
generalisations of the proposed methodology to image restoration problems that are
blind or semi-blind [16, 17, 37]. Lastly, many important image restoration problems
encounteredincomputervisioninvolvenon-Gaussiannoise.Itwouldbeinterestingto
extendtheproposedmethodtohandlemoregeneralstatisticalnoisemodels,especially
Poisson and other forms of low-photon noise [38].
Appendix A Implementation guidelines.
In this section, we provide some recommendations and guidelines for setting the
parameters of Algorithm 1.
A.0.1 Setting γ
As suggested in [36], it is recommended to set 0 < γ < (L +1/λ)−1, where L =
y y
||A||2σ−2 +ρ−1 is the Lipschitz constant of ∇ logp(y|z,ρ2) and λ > 0 is the noise
2 0 z
parameter of the denoiser D . In our experiments, we set γ = 0.5(1/λ+L )−1, and
λ y
λ=1.5/255 as explained in Section 4 of the main paper.
A.0.2 Setting δ
k
It is suggested in [36] to set the step size of the SAPG scheme by δ = c k−p, where
k 0
p ∈ [0.6,0.9]. In the experiments carried out in this work, we choose p = 0.85 and
c =ρ d−1.
0 0
A.0.3 Setting a stopping criterion
In the proposed method, it is recommended to supervise the evolution of the relative
errors |ρ¯ −ρ¯ |/ρ¯ and |X¯ −X¯ |/X¯ until they reach a tolerance level tol and
k+1 k k k+1 k k ρ
tol ,respectively.Inourresults,weobservedthatsettingtol =10−3 andtol =10−4
x ρ x
isenoughtoachievestabilityinsmallcomputationaltimes.FigureA1aandFigureA1a
depict the evolution of relative errors |ρ¯ −ρ¯ |/ρ¯ and |X¯ −X¯ |/X¯ from the
k+1 k k k+1 k k
deblurring experiments.
18(a) Relative error of ρ (b) Relative error of X¯
Fig. A1: Qualitative results - Image deblurring experiment: relative errors of the
sequences of iterates (ρ ) and (cid:0) X¯ (cid:1) .
k k∈N k k∈N
Appendix B Effect of λ
WerecallthattheparameterλcontrolsthenoiselevelofthePnPdenoiserD ,which
λ
was implemented based on the DDPM scheme from t⋆ = β−1(λ) to t = 0. This
parameter also controls the number of NFEs of each PnP-ULA iterations. During
our experiments, we observe that using a large value of λ significantly deteriorates
the quality of the reconstruction results in PSNR and LPIPS, because D introduces
λ
excessive prior smoothing into the model, see Figure B2 for illustration. Conversely,
setting λ too small also deteriorate the quality of the reconstruction results, because
estimatingthescorewhenλ→0isveryhard,seeFigureB2forillustration.TableB1
reports the average PSNR (dB) and LPIPS for the proposed method with different
NFEs cost.
Table B1: Quantitative results - image deblurring experiment: average
PSNR (dB) and LPIPS for our proposed method (η =2).
NFEs NFEs
←−−−−−−−−−−−−−− −−−−−−−−−−−−−−−−−−−→
NFEs 1 2 3 6 10 20 50
PSNR 21.1 29.8 34.0 31.8 29.0 22.9 20.1
LPIPS 0.48 0.21 0.12 0.20 0.26 0.36 0.65
Appendix C Denoising results with D
λ
Inthissection,wedemonstratetheeffectivenessofthedenoisingoperatorD torestore
λ
x from a noisy realisation x˜ =x+ω contaminated with zero mean Gaussian noise ω
ofvarianceλ.WeevaluateD undertwodifferentnoisevariancesettings,λ=0.1and
λ
0.2. Figure C4 and ?? illustrate the results - the first raw displays the ground true
19NFE=1 NFE=2 NFE=3 NFE=6 NFE=10 NFE=20 NFE=50
(21.1dB,0.48) (29.9dB,0.21) (34.1dB,0.11) (32.1dB,0.19) (29.3dB,0.22) (24.6dB,0.31) (19.0dB,0.42)
Fig. B2: Qualitative results - Image deblurring experiment: Reconstruction with
different NFEs from {1,2,3,6,10,20,50} using our proposed method (η =2). Recon-
struction PSNR (dB) and LPIPS of the last raw.
images,thesecondrawshowsthenoisyimages,andthedenoisedimagesaredisplayed
in the last raw.
References
[1] Song, Y., Ermon, S.: Generative modeling by estimating gradients of the data
distribution. Advances in Neural Information processing systems 32 (2019)
[2] Ho, J., Jain, A., Abbeel, P.: Denoising diffusion probabilistic models. Advances
in Neural Information processing systems 33, 6840–6851 (2020)
[3] Song, J., Meng, C., Ermon, S.: Denoising Diffusion Implicit Models. In: Interna-
tional Conference on Learning Representations (2021). https://openreview.net/
forum?id=St1giarCHLP
[4] Song, Y., Ermon, S.: Improved techniques for training score-based generative
models. Advances in Neural Information processing systems 33, 12438–12448
(2020)
[5] Song, Y., Sohl-Dickstein, J.N., Kingma, D.P., Kumar, A., Ermon, S., Poole,
B.: Score-Based Generative Modeling through Stochastic Differential Equations.
ICLR (2021)
20(a) 31.31dB (b) 32.06dB (c) 33.5dB (d) 31.43dB (e) 31.73dB
Fig. C3:Qualitativeresults-denoisingexperimentwithλ=0.1(NFEs=6).Recon-
struction PSNR (dB). From the top to the bottom, we have true image, noisy image
and denoised image respectively.
[6] Chung,H.,Sim,B.,Ye,J.C.:Come-closer-diffuse-faster:Acceleratingconditional
diffusionmodelsforinverseproblemsthroughstochasticcontraction.In:Proceed-
ingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,
pp. 12413–12422 (2022)
[7] Li, H., Yang, Y., Chang, M., Chen, S., Feng, H., Xu, Z., Li, Q., Chen,
Y.: Srdiff: Single image super-resolution with diffusion probabilistic models.
Neurocomputing 479, 47–59 (2022)
[8] O¨zbey, M., Dalmaz, O., Dar, S.U., Bedel, H.A., O¨zturk, S¸., Gu¨ng¨or, A., C¸ukur,
T.: Unsupervised medical image translation with adversarial diffusion models.
IEEE Transactions on Medical Imaging (2023)
[9] Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pp. 10684–10695
(2022)
[10] Saharia, C., Chan, W., Chang, H., Lee, C., Ho, J., Salimans, T., Fleet, D.,
Norouzi, M.: Palette: Image-to-image diffusion models. In: ACM SIGGRAPH
2022 Conference Proceedings, pp. 1–10 (2022)
[11] Saharia, C., Ho, J., Chan, W., Salimans, T., Fleet, D.J., Norouzi, M.: Image
21(a) 28.96dB (b) 29.35dB (c) 31.35dB (d) 28.6dB (e) 29.19dB
Fig. C4:Qualitativeresults-denoisingexperimentwithλ=0.1(NFEs=6).Recon-
struction PSNR (dB). From the top to the bottom, we have true image, noisy image
and denoised image respectively.
super-resolutionviaiterativerefinement.IEEETransactionsonPatternAnalysis
and Machine Intelligence 45(4), 4713–4726 (2022)
[12] Song, J., Vahdat, A., Mardani, M., Kautz, J.: Pseudoinverse-guided diffu-
sion models for inverse problems. In: International Conference on Learning
Representations (2022)
[13] Zhu, Y., Zhang, K., Liang, J., Cao, J., Wen, B., Timofte, R., Van Gool, L.:
DenoisingDiffusionModelsforPlug-and-PlayImageRestoration.In:Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.
1219–1229 (2023)
[14] Kawar,B.,Elad,M.,Ermon,S.,Song,J.:Denoisingdiffusionrestorationmodels.
Advances in Neural Information Processing Systems 35, 23593–23606 (2022)
[15] Coeurdoux, F., Dobigeon, N., Chainais, P.: Plug-and-Play split Gibbs sam-
pler: embedding deep generative priors in Bayesian inference. arXiv preprint
arXiv:2304.11134 (2023)
[16] Laroche, C., Almansa, A., Coupete, E.: Fast diffusion EM: A diffusion model
for blind inverse problems with application to deconvolution. In: Proceedings
of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp.
5271–5281 (2024)
22[17] Chung, H., Kim, J., Kim, S., Ye, J.C.: Parallel diffusion models of operator and
image for blind inverse problems. In: Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pp. 6059–6069 (2023)
[18] Laumont, R., Bortoli, V.D., Almansa, A., Delon, J., Durmus, A., Pereyra, M.:
BayesianimagingusingPlug&Playpriors:whenLangevinmeetsTweedie.SIAM
Journal on Imaging Sciences 15(2), 701–737 (2022)
[19] Vidal, A.F., De Bortoli, V., Pereyra, M., Durmus, A.: Maximum likelihood
estimation of regularization parameters in high-dimensional inverse problems:
An empirical Bayesian approach. Part I: Methodology and experiments. SIAM
Journal on Imaging Sciences 13(4), 1945–1989 (2020)
[20] Chen, S., Daras, G., Dimakis, A.: Restoration-degradation beyond linear dif-
fusions: A non-asymptotic analysis for DDIM-type samplers. In: International
Conference on Machine Learning, pp. 4462–4484 (2023). PMLR
[21] Benton, J., Bortoli, V.D., Doucet, A., Deligiannidis, G.: Linear Convergence
BoundsforDiffusionModelsviaStochasticLocalization.In:TheTwelfthInterna-
tional Conference on Learning Representations (2024). https://openreview.net/
forum?id=r5njV3BsuD
[22] Anderson,B.D.:Reverse-timediffusionequationmodels.StochasticProcessesand
their Applications 12(3), 313–326 (1982)
[23] Li,H.,Xu,Z.,Taylor,G.,Studer,C.,Goldstein,T.:Visualizingthelosslandscape
of neural nets. Advances in Neural Information processing systems 31 (2018)
[24] Robert, C.P.: The Bayesian Choice: from Decision-theoretic Foundations to
Computational Implementation vol. Second edition. Springer, ??? (2007)
[25] Cardoso, G., Janati El Idrissi, Y., Moulines, E., Corff, S.L.: Monte Carlo
guided Denoising Diffusion models for Bayesian linear inverse problems. In: The
Twelfth International Conference on Learning Representations (2024). https:
//openreview.net/forum?id=nHESwXvxWK
[26] Durmus, A., Moulines, E., Pereyra, M.: Efficient Bayesian computation by prox-
imal Markov chain Monte Carlo: when Langevin meets Moreau. SIAM Journal
on Imaging Sciences 11(1), 473–506 (2018)
[27] Terris, M., Moreau, T., Pustelnik, N., Tachella, J.: Equivariant Plug-and-Play
image reconstruction. arXiv preprint arXiv:2312.01831 (2023)
[28] Serre,J.-P.:Linearrepresentationsoffinitegroups,volume42of.Graduatetexts
in Mathematics, 80 (1977)
[29] Pereyra, M., Vargas-Mieles, L.A., Zygalakis, K.C.: The split Gibbs sampler
23revisited: Improvements to its algorithmic structure and augmented target
distribution. SIAM Journal on Imaging Sciences 16(4), 2040–2071 (2023)
[30] Vono, M., Dobigeon, N., Chainais, P.: Split-and-augmented Gibbs sam-
pler—Applicationtolarge-scaleinferenceproblems.IEEETransactionsonSignal
Processing 67(6), 1648–1661 (2019)
[31] Karras,T.,Laine,S.,Aila,T.:Astyle-basedgeneratorarchitectureforgenerative
adversarialnetworks.In:ProceedingsoftheIEEE/CVFConferenceonComputer
Vision and Pattern Recognition, pp. 4401–4410 (2019)
[32] Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L.: Imagenet: A large-
scalehierarchicalimagedatabase.In:2009IEEEConferenceonComputerVision
and Pattern Recognition, pp. 248–255 (2009). Ieee
[33] Zhang, K., Li, Y., Zuo, W., Zhang, L., Van Gool, L., Timofte, R.: Plug-and-
play image restoration with deep denoiser prior. IEEE Transactions on Pattern
Analysis and Machine Intelligence 44(10), 6360–6376 (2021)
[34] Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality assess-
ment: from error visibility to structural similarity. IEEE transactions on image
processing 13(4), 600–612 (2004)
[35] Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectivenessofdeepfeaturesasaperceptualmetric.In:ProceedingsoftheIEEE
Conference on Computer Vision and Pattern Recognition, pp. 586–595 (2018)
[36] De Bortoli, V., Durmus, A., Pereyra, M., Vidal, A.F.: Maximum likelihood esti-
mation of regularization parameters in high-dimensional inverse problems: An
empirical Bayesian approach. Part II: Theoretical analysis. SIAM Journal on
Imaging Sciences 13(4), 1990–2028 (2020)
[37] Kemajou Mbakam, C., Pereyra, M., Giovannelli, J.-F.: A stochastic optimisa-
tionunadjustedLangevinmethodforempiricalBayesianestimationinsemi-blind
image deblurring problems. SIAM Journal on Imaging Sciences. to appear
[38] Melidonis, S., Dobson, P., Altmann, Y., Pereyra, M., Zygalakis, K.: Efficient
Bayesian computation for low-photon imaging problems. SIAM Journal on
Imaging Sciences 16(3), 1195–1234 (2023)
24