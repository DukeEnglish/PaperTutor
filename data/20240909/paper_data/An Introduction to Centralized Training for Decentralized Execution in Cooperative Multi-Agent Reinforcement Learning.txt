An Introduction to
Centralized Training for Decentralized Execution in
Cooperative Multi-Agent Reinforcement Learning
Christopher Amato, Northeastern University
September 6, 2024
Contents
1 ThecooperativeMARLproblem: TheDec-POMDP 3
2 CTDEoverview 6
3 Valuefunctionfactorizationmethods 6
3.1 Backgroundonvalue-basedRL . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
3.2 VDN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
3.3 QMIX . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
3.4 QTRAN . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
3.5 QPLEX . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
3.6 Theuseofstateinfactorizationmethods . . . . . . . . . . . . . . . . . . . . . . . 16
4 Centralizedcriticmethods 16
4.1 Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
4.2 Abasiccentralizedcriticapproach . . . . . . . . . . . . . . . . . . . . . . . . . . 17
4.3 MADDPG . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
4.4 COMA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
4.5 MAPPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
4.6 State-basedcritics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
4.7 Choosingdifferenttypesofdecentralizedandcentralizedcritics . . . . . . . . . . 24
4.8 Methodsthatcombinepolicygradientandvaluefactorization . . . . . . . . . . . . 25
4.9 Othercentralizedcriticmethods . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
5 OtherformsofCTDE 25
5.1 Addingcentralizedinformationtodecentralizedmethods . . . . . . . . . . . . . . 25
5.2 Decentralizingcentralizedsolutions . . . . . . . . . . . . . . . . . . . . . . . . . 26
5.3 Topicsnotdiscussed . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
6 Acknowledgements 27
1
4202
peS
4
]GL.sc[
1v25030.9042:viXraMulti-agentreinforcementlearning(MARL)hasexplodedinpopularityinrecentyears. Many
approacheshavebeendevelopedbuttheycanbedividedintothreemaintypes: centralizedtraining
and execution (CTE), centralized training for decentralized execution (CTDE), and Decentralized
trainingandexecution(DTE).
CTE methods assume centralization during training and execution (e.g., with fast, free and
perfect communication) and have the most information during execution. That is, the actions of
each agent can depend on the information from all agents. As a result, a simple form of CTE
canbeachievedbyusingasingle-agentRLmethodwithcentralizedactionandobservationspaces
(maintainingacentralizedaction-observationhistoryforthepartiallyobservablecase). CTEmeth-
ods can potentially outperform the decentralized execution methods (since they allow centralized
control)butarelessscalableasthe(centralized)actionandobservationspacesscaleexponentially
withthenumberofagents. CTEistypicallyonlyusedinthecooperativeMARLcasesincecentral-
ized control implies coordination on what actions will be selected by each agent. CTDE methods
are the most common as they can use centralized information during training but execute in a de-
centralizedmanner—usingonlyinformationavailabletothatagentduringexecution. CTDEisthe
only paradigm that requires a separate training phase where any available information (e.g., other
agent policies, underlying states) can be used. As a result, they can be more scalable than CTE
methods, do not require communication during execution, and can often perform well. CTDE fits
most naturally with the cooperative case, but can be potentially applied in competitive or mixed
settings depending on what information is assumed to be observed. Decentralized training and
execution methods make the fewest assumptions and are often simple to implement. In fact, any
single-agentRLmethodcanbeusedforDTEbyjustlettingeachagentlearnseparately. Ofcourse,
there are pros and cons to such approaches [Amato, 2024]. It is worth noting that DTE is required
if no centralized training phase is available (e.g., though a centralized simulator), requiring all
agentstolearnduringonlineinteractionswithoutpriorcoordination. DTEmethodscanbeapplied
incooperative,competitive,ormixedcases.
MARLmethodscanbefurtherbrokenupintovalue-basedandpolicygradientmethods. Value-
based methods (e.g., Q-learning) learn a value function and then choose actions based on those
values. Policy gradient methods learn an explicit policy representation and attempt to improve the
policyinthedirectionofthegradient. BothclassesofmethodsarewidelyusedinMARL.
This text is an introductionto CTDE MARL. It is meant to explain the setting, basic concepts,
andcommonmethods. ItdoesnotcoverallworkinCTDEMARLasthesubareaisquiteextensive.
IhaveincludedworkthatIbelieveisimportantforunderstandingthemainconceptsinthesubarea
andapologizetothosethatIhaveomitted.
I will first give a brief description of the cooperative MARL problem in the form of the Dec-
POMDP. Then, I present an overview of CTDE and the two main classes of CTDE methods:
value function factorization methods and centralized critic actor-critic methods. Value function
factorization methods include the well-known VDN [Sunehag et al., 2017], QMIX [Rashid et al.,
2018], and QPLEX [Wang et al., 2021a] approaches, while centralized critic methods include
MADDPG [Lowe et al., 2017], COMA [Foerster et al., 2018b], and MAPPO [Yu et al., 2022].
Finally, I discuss other forms of CTDE such as adding centralized information to decentralized
(i.e.,independent)learners(suchasparametersharing)anddecentralizingcentralizedsolutions.
The basics of reinforcement learning (in the single-agent setting) are not presented in this
text. Anyone interested in RL should read the book by Sutton and Barto [2018]. Similarly, for a
broaderoverviewofMARL,therecentbookbyAlbrecht,ChristianosandScha¨ferisrecommended
[Albrechtetal.,2024].
2r
a
1
o
a1
Environment
n
o
n
Figure1: AdepictionofcooperativeMARL—aDec-POMDP.
1 The cooperative MARL problem: The Dec-POMDP
Thecooperativemulti-agentreinforcementlearning(MARL)problemcanberepresentedasaDec-
POMDP [Oliehoek and Amato, 2016, Bernstein et al., 2002]. Dec-POMDPs generalize POMDPs
[Kaelblingetal.,1998](andMDPs[Puterman,1994])tothemulti-agent,decentralizedsetting. As
depictedinFigure1,multipleagentsoperateunderuncertaintybasedonpartialviewsoftheworld,
withexecutionunfoldingovertime. Ateachstep,everyagentchoosesanaction(inparallel)based
purely on locally observable information, resulting in each agent obtaining an observation and the
team obtaining a joint reward. The shared reward function makes the problem cooperative, but
theirlocalviewsmeanthatexecutionisdecentralized.
Formally, a Dec-POMDP is defined by tuple ⟨I,S,{A },T,R,{O },O,H,γ⟩. For simplicity,
i i
Idefinethefiniteversionbutiteasilyextendstothecontinuouscase:
• Iisafinitesetofagentsofsize|I| = n;
• Sisafinitesetofstateswithdesignatedinitialstatedistributionb ;1
0
• A isafinitesetofactionsforeachagentiwithA = × A thesetofjointactions;
i i i
• T is a state transition probability function, T : S × A × S → [0,1], that specifies the
probability of transitioning from state s ∈ S to s′ ∈ S when the actions a ∈ A are taken by
theagents(i.e.,T(s,a,s′) = Pr(s′|a,s));
• R is a reward function: R : S×A → R, the immediate reward for being in state s ∈ S and
takingtheactionsa ∈ A;
• O is a finite set of observations for each agent, i, with O = × O the set of joint observa-
i i i
tions;
• O is an observation probability function: O : O×A×S → [0,1], the probability of seeing
observations o ∈ O given actions a ∈ A were taken and resulting in state s′ ∈ S (i.e.,
O(a,s′,o) = Pr(o|a,s′));
• H isthenumberofstepsuntiltermination,calledthehorizon;
• andγ ∈ [0,1]isthediscountfactor.
1Somepapersrefertootherinformationsuchasjointobservationsorhistoriesas‘state.’ Forclarity,Iwillonlyuse
thistermtorefertothetrueunderlyingstate.
3Ialsoincludeboththehorizonanddiscountinthedefinitionbut,typically,onlyoneisused. When
H isfinite,γ canbesetto1andwhenH = ∞,γ ∈ [0,1).2
AsolutiontoaDec-POMDPisajointpolicy,denotedπ—asetofpolicies,oneforeachagent,
each of which is denoted π . Because the state is not directly observed, it is typically beneficial
i
for each agent to remember a history of its observations (and potentially actions). Then, a local
policy,π ,foranagentisamappingfromlocalaction-observationhistoriestoactionsorprobability
i
distributions over actions. A local deterministic policy for agent i, π (h ) = a , maps H → A ,
i i i i i
whereH isthesetoflocalobservationhistories,h = {a ,o ,...,a ,o }3,byagentiup
i i i,0 i,0 i,t−1 i,t−1
to the current time step, t. Note that histories have implicit time steps due to their length which
we do not include in the notation (i.e., we always assume a history starts on the first time step and
the last time step is defined by the number of action-observation pairs). We can denote the joint
histories for all agents at a given time step as h = ⟨h ,...,h ⟩. A joint deterministic policy is
1 n
denoted π(h) = a = ⟨π (h ),...,π (h )⟩ = ⟨a ,...,a ⟩. A stochastic local policy for agent
1 1 n n 1 n
i is π (a |h ), representing the probability of choosing action a in history h . A joint stochastic
i i i i i
(cid:81)
policy is denoted π(a|h) = π (a |h ). Because one policy is generated for each agent and
i∈I i i i
thesepoliciesdependonlyonlocalobservations,theyoperateinadecentralizedmanner.
Manyresearchersjustuseobservationhistories(withoutincludingactions),whichissufficient
for deterministic policies but may not be for stochastic policies [Oliehoek and Amato, 2016]. De-
terministic policies will be used in the value-based methods in Section 3 while stochastic policies
will be used in the policy gradient gradient methods in Section 4. There always exists an opti-
mal deterministic joint policy in Dec-POMDPs [Oliehoek and Amato, 2016], but stochastic (or
continuous)policiesareneededforpolicygradientmethods.
The value of a joint policy, π, at joint history h can be defined for the case of discrete states
andobservationsas
(cid:88) (cid:104) (cid:88) (cid:88) (cid:105)(cid:12)
Vπ(h) = P(s|h,b ) R(s,a)+γ P(s′|a,s) P(o|a,s′)Vπ(hao) (cid:12) (1)
0 (cid:12)
a=π(h)
s s′ o
where P(s|h,b ) is the probability of state s after observing joint history h starting from state
0
distribution b and a = π(h) is the joint action taken at the joint history. Also, hao represents h′,
0
the joint history after taking joint action a in joint history h and observing joint observation o. In
the RL context, algorithms do not iterate over states and observations to explicitly calculate this
expectation but approximate it through sampling. For the finite-horizon case, Vπ(h) = 0 when
the length of h equals the horizon H, showing the value function includes the time step from the
history.
Wewilloftenwanttoevaluatepoliciesstartingfromthebeginning—startingattheinitialstate
distribution before any action or observation. Starting from this initial, null history we denote the
value of a policy as Vπ(h ). An optimal joint policy beginning at h is argmax Vπ(h ), where
0 0 π 0
theargmaxheredenotesenumerationoverdecentralizedpolicies. Theoptimaljointpolicyisthen
thesetoflocalpoliciesforeachagentthatprovidesthehighestvalue,whichisdenotedV∗.
Reinforcement learning methods often use history-action values, Q(h,a), rather than just his-
tory values V(h). Qπ(h,a) is the value of choosing joint action a at joint history h and then
2The episodic case [Sutton and Barto, 2018] is typically indefinite-horizon with termination to a set of terminal
stateswithprobability1andtheinfinite-horizoncaseissometimescalled‘continuing.’
3Sometimes,o isalsoincludedasanobservationgeneratedbytheinitialstatedistribution
i,0
4continuingwithpolicyπ,
(cid:88) (cid:104) (cid:88) (cid:88) (cid:0) (cid:1)(cid:105)
Qπ(h,a) = P(s|h,b ) R(a,s)+γ P(s′|a,s) P(o|a,s′)Qπ hao,a′)| ,
0 a′=π(hao)
s s′ o
(2)
whileQ∗(h,a)isthevalueofchoosingactionaathistoryhandthencontinuingwiththeoptimal
policy,π∗.
Policies that depend only on a single observation It is somewhat popular to define policies
that depend only on a single observation rather than the whole observation history. That is, π :
i
O → A , rather than H → A . This type of policy is often referred to as reactive or Markov
i i i i
since it just depends on (or reacts from) the past observation. These reactive policies are typically
not desirable since they can be arbitrarily worse than policies that consider history information
[Murphy, 2000] but can perform well or even be optimal in simpler subclasses of Dec-POMDPs
[Goldman and Zilberstein, 2004]. In general, reactive policies reduce the complexity of finding a
solution (since many fewer policies need to be considered) but only perform well in limited cases
such as when the problem is not really partially observable or when the observation history isn’t
helpfulfordecision-making.
The fully observable case It is worth noting that if the cooperative problem is fully observable,
it becomes much simpler. Specifically, a multi-agent MDP (MMDP) can be defined by the tuple
⟨I,S,{A },T,R,H,γ⟩[Boutilier, 1996]. Each agent can observe the full state in this case. In the
i
CTDE context, solving an MMDP can be done by standard MDP methods where the action space
is the joint action space, A = × A . The resulting policy, S → A, can be trivially decentralized as
i i
S → A foreachagenti. AnumberofmethodshavebeendevelopedforlearninginMMDPs(and
i
othermulti-agentmodels)[Bus¸oniuetal.,2008].
Finding solutions in Dec-POMDPs is much more challenging. This can even be seen by the
(finite-horizon) complexity for finite problems—optimally solving an MDP is P-complete (poly-
nomial in the size of the system) [Papadimitriou and Tsitsiklis, 1987, Littman et al., 1995] and
optimally solving a POMDP is PSPACE (essentially exponential in the actions and observations),
[Papadimitriou and Tsitsiklis, 1987], while optimally solving a Dec-POMDP is NEXP-complete
(essentially doubly exponential) [Bernstein et al., 2002]. This can be intuitively understood by
considering the simplest possible solution method for each class of problems. For an MDP, you
could search all policies mapping states to (joint) actions: |A||S| of them. For a POMDP, you need
to consider histories so centralized control (mapping joint histories to joint actions) would result
in searching over |A||O|H possibilities.4 In the Dec-POMDP case, each agent would have |A ||O i|H
i
possiblepolicies,sothepossiblenumberofpossiblejointpolicieswouldbe|A ||O
i|H|I|
ifallagents
i
have the same number of actions and observations as agent i. Of course, state-of-the-art methods
aremoresophisticatedthanthisandthecomplexityisfortheworstcase. Mostreal-worldproblems
are intractable to solve optimally anyway. Regardless, these numbers make it clear that searching
thejointspaceofhistory-basedpoliciesisenormousandtheresultingpartiallyobservableproblem
ismuchmorechallengingthanthefullyobservablecase.
4Since we are assuming deterministic policies, the actions are fixed but we would still have to search over all
possibleobservationhistoriesuptothegivenhorizon,resultinginOHpossiblehistories.
52 CTDE overview
The idea of centralized training for decentralized execution is quite vague. The general idea of
CTDE the Dec-POMDP case originated (with the Dec-POMDP itself) in the planning literature
[Bernstein et al., 2002], where planning would be centralized but execution was decentralized.
This idea goes back further to team decision making more generally since it is natural to think
of deriving a solution for the team as a whole and then assigning corresponding parts to the team
members [Marschak, 1955, Radner, 1962, Ho, 1980]. This concept was carried over to the rein-
forcementlearningcaseandhascometomeanthatthereisan‘offline’5 trainingphasewheresome
amount of centralization is allowed (such as in [Kraemer and Banerjee, 2016]). After this cen-
tralized training phase is complete, agents must then act in a decentralized manner based on their
own histories (as given by the Dec-POMDP definition above). Any information that would not
be available during decentralized execution can be used in the centralized training phase ranging
from neural network parameters, policies of the other agents, a joint value estimate, etc. Never-
theless, CTDE has never been formally defined (to the best of my knowledge) and I will not do so
here. I will take a wide view of CTDE and consider it to include any centralized information or
coordinationduringlearning—anythingthatisnotavailableduringdecentralizedexecution.
CTDE methods for Dec-POMDPs vary widely. Some methods add centralized information
to DTE methods. These approaches should be scalable but use limited centralized information.
Other methods are at the other extreme, where they learn a centralized policy and then attempt
to decentralize it so it can be executed without the centralized information. Most CTDE methods
fall somewhere in between these two ideas. The main class of value-based CTDE methods learns
a set of decentralized Q-functions (one for each agent) by factoring a joint value function, as in
value function factorization methods. The main class of policy gradient CTDE methods learns a
centralizedcriticthatapproximatesthejointvaluefunctionforallagentsandthatcentralizedcritic
is used to update a set of decentralized actors (one per agent). I discuss these ideas in more detail
below.
3 Value function factorization methods
There is a long history of learning factored value functions in multi-agent reinforcement learning,
but many previous methods were focused on more efficiently learning a joint value function (e.g.,
[Kok and Vlassis, 2006]). Other methods have also been developed that share Q-functions dur-
ing training to improve coordination [Schneider et al., 1999]. Modern value function factorization
methods learn Q-functions for each agent by combining them into a joint Q-function and calcu-
lating a loss based on the joint Q-function. In this way, the joint Q-function is factored into local,
decentralizedQ-valuesthatcanbeusedduringdecentralizedexecution. Thisideaisappealingbe-
causeajointQ-functioncanbelearnedtoapproximatetheoneinEquation2butagentscanchoose
actions based on their decentralized Q-values. Additional details and the most popular methods
arebelow.
5Offlinedoesnotrefertohavingafixedtrainingset[Levineetal.,2020]butratherinadifferentsettingthanwhat
isusedforexecution(whichwouldbeconsideredonline).
6(a) DQN (b) DRQN
Figure2: DQNandDRQNdiagrams.
3.1 Background on value-based RL
I will first provide background on the value-based methods that the value function factorization
methods build upon. Therefore, I will first introduce Deep Q-Networks (DQN), and the extension
topartialobservability,DRQN.
DeepQ-networks(DQN) [Mnihetal.,2015]isanextensionofQ-learning[WatkinsandDayan,
1992] to include a neural net as a function approximator. Since DQN was designed for the fully
observable (MDP) case, it learns Q (s,a), parameterized with θ (i.e., θ represents the parameters
θ
oftheneuralnetwork),byminimizingtheloss:
(cid:104) (cid:105)
L(θ) = E (cid:0) y −Q (s,a)(cid:1)2 ,where y = r+γmaxQ (s′,a′) (3)
<s,a,r,s′>∼D θ θ−
a′
which is just the squared TD error—the difference between the current estimated value, Q (s,a),
θ
and the new value gotten from adding the newly seen reward to the previous Q-estimate at the
next state, Q (s′,a′). Because learning neural networks can be unstable, a separate target action-
θ−
value function Q and an experience replay buffer D [Lin, 1992] are implemented to stabilize
θ−
learning. The target network is an older version of the Q-estimator that is updated periodically
with ⟨s,a,r,s′⟩ sequences stored in the experience replay buffer and single ⟨s,a,r,s′⟩ tuples are
i.i.d. sampled for updates. As shown in Figure 2(a), the neural network (NN) outputs values for
all actions (a ∈ A) to make maximizing over all states possible with a single forward pass (rather
thaniteratingthroughtheactions).
Deep recurrent Q-networks (DRQN) [Hausknecht and Stone, 2015] extends DQN to handle
partialobservability,wheresomenumberofrecurrentlayers(e.g.,LSTM[HochreiterandSchmid-
huber,1997])areincludedtomaintainaninternalhiddenstatewhichisanabstractionofthehistory
(asshownash˜ inFigure2(b)). Becausetheproblemispartiallyobservable,o,a,r,o′sequencesare
storedinthereplaybufferduringexecution. TheupdateequationisverysimilartothatofDQN:
(cid:104) (cid:105)
L(θ) = E (cid:0) y −Q (h,a)(cid:1)2 ,where y = r+γmaxQ (h′,a′) (4)
<h,a,r,o>∼D θ θ−
a′
butsincetherecurrentneuralnetwork(RNN)isused,theinternalstateoftheRNNcanbethought
of as a history representation. RNNs are trained sequentially, updating the internal state at each
˜
timestep. Asaresult,thefigureshowsonlyoastheinputbutthisassumestheinternalstatehhas
already been updated using the history up to this point ht−1. Therefore, rather than just sampling
single updates (o,a,r,o′) i.i.d, like in DQN, histories are sampled from the buffer. The internal
7Figure3: VDNarchitecturediagram
state can be updated incrementally and the Q-values learned starting from the first time step and
going until the end of the history (i.e., the horizon or episode). An example of training using
DRQN is shown in Algorithm 1. Technically, a whole history (e.g., episode) should be sampled
from the replay buffer to train the recurrent network but it is common to use a fixed history length
(e.g., sample o,a,o′,r sequences of length 10). Also, as mentioned above, many implementations
just use observation histories rather than full action-observation histories. Note that I will still
˜
write h rather than h in the equations (e.g., Q (h,a) above) to be more general. The resulting
θ
equations are not restricted to recurrent models (e.g., use a non-recurrent representation such as
the full history or a Transformer-based representation [Esslinger et al., 2022, Ni et al., 2023]).
With the popularity of DRQN, it has become common to add recurrent layers to standard (fully
observable)deepreinforcementlearningmethodswhensolvingpartiallyobservableproblems.
3.2 VDN
Value decomposition networks (VDN) [Sunehag et al., 2017] began the popular trend in value
decomposition methods for solving multi-agent reinforcement learning. The main contribution in
VDN is a decomposition of the joint Q-function into additive Q-functions per agent.6 This idea
is relatively simple but very powerful. It allows training to take place in a centralized setting but
agents can still execute in a decentralized manner because they learn individual Q-functions per
agent,whichcanthenbemaxedovertoselectanaction.
Inparticular,itassumesthefollowingapproximatefactorizationoftheQ-function:
n
(cid:88)
Q(h,a) ≈ Q (h ,a ) (5)
i i i
i∈I
That is, the joint Q-function is approximated by a sum over each agent’s individual Q-function.
This is a form of factorization—the joint Q-function is factored as a sum of the local Q functions
i
[Guestrin et al., 2001]. It is worth noting that each Q is technically a utility and not a value
i
function since it is not estimating a particular expected return but can be any arbitrary value in the
sum.
6Thepaperalsousesdueling[Wangetal.,2016]andinvestigatesformsofweightsharingandcommunicationbut
weviewtheseasorthogonaltothemainvaluedecompositioncontribution.
8The architecture diagram is given in Figure 3. Each agent learns an individual set of Q-values
that depend only on local information. These Q-networks take the current observation as input to
a recurrent network, which updates the history representation and then outputs Q-values for that
agent. During training, these individual Q-values are summed to generate the joint Q-value which
canthenbeusedtocalculatethelosslike(acentralizedformof)DRQN:
n n
L(θ) = E (cid:104) (cid:0) y −(cid:88) Qθ(h ,a )(cid:1)2(cid:105) ,where y = r+γ(cid:88) maxQθ−(h′,a′) (6)
<h,a,r,o>∼D i i i i i i
a′
i i i
whereh′ ish a o forh takenfromh,a froma,ando fromo.
i i i i i i i
Since the loss uses the sum of the agent’s Q-functions, the result is a set of Q-values whose
sumapproximatesthejointQ-function. Thisapproximationwillbelosslessinextremecases(e.g.,
fullindependence)butitmaybecloseorstillpermittheagentstoselectthebestactionsevenifthe
approximation is poor. The approach is also scalable since the max used to create the target value
y is not over all agent actions as in the centralized case (A), but done separately for each agent in
Equation6.
A simple version of VDN is given in Algorithm 1. The approach is very similar to standard
D(R)QN [Mnih et al., 2015, Hausknecht and Stone, 2015] as well as multi-agent DTE extensions
[Omidshafieietal.,2017,Tampuuetal.,2017]. Thekeydifferenceisthecalculationofthelosson
Line20,whichapproximatesthejointQ-valuebyusingthesumoftheindividualQ-values.
Inmoredetail,theQ areestimatedusingRNNsforeachagentparameterizedbyθ ,alongwith
i i
usingatargetnetwork,θ−,andareplaybuffer,D. First,episodesaregeneratedbyinteractingwith
i
the environment (e.g., ϵ-greedy exploration), which are added to the replay buffer and indexed by
episodenumberandtimestep(De(t)). Thisbuffertypicallyhasafixedsizeandnewdatareplaces
olddatawhenthebufferisfull. Episodescanthenbesampledfromthereplaybuffer(eithersingle
episodes or as a minibatch of episodes). In order to have the correct internal state, RNNs are
trained sequentially. Training is done from the beginning of the episode until the end, calculating
the internal state and the loss at each step and updating each θ using gradient descent. The target
i
networks θ− are updated periodically (every C episodes in the code) by copying the parameters
i
fromθ.
While the algorithm is presented for the finite-horizon case (for simplicity), it can easily ex-
tendedtotheepisodicandinfinite-horizoncasesbyincludingterminalstatesorremovingtheloop
overepisodes.
After training, each agent keeps its own recurrent network, which can output the Q-values
for that agent. The agent can then select an action by argmaxing over those Q-values: π (h ) =
i i
argmax Q (h ,a ).
ai i i i
VDNisoftenusedabaselinebutbyusingasimplesumtocombineQ-functions,itcanperform
poorlycomparedtomoresophisticatedmethods.
3.3 QMIX
QMIX[Rashidetal.,2018,2020b]extendsthevaluefactorizationideaofVDNtoallowmoregen-
eraldecompositions. Inparticular,insteadofassumingthejointQ-function,Q(h,a),factorsintoa
sum of local Q-values, Q (h ,a ), QMIX assumes the joint Q-function is a monotonic function of
i i i
the individual Q-functions. The sum used in VDN is also a monotonic function but QMIX allows
moregeneral(possiblynonlinear)monotonicfunctionstobelearned.
9Algorithm1Aversionofvaluedecompositionnetworks(VDN)(finite-horizon)
1: setα,ϵ,andC (learningrate,exploration,andtargetupdatefrequency)
2: Initializenetworkparametersθ foreachQ (denotedQθ)
i i i
3: foralli,θ− ← θ
i i
4: D ← ∅
5: e ← 1 {episodeindex}
6: forallepisodesdo
7: forallh ← ∅ {initialhistoryisempty}
i
8: fort = 1toH do
9: foralli,takea ath fromQθ(h ,·)withexploration(e.g.,ϵ-greedy)
i i i i
10: Seejointrewardr ,andobservationso
t t
11: appenda,o,r toDe
12: forallh ← h a o {updateRNNstateofthenetwork}
i i i i
13: endfor
14: sampleanepisodefromD
15: fort = 1toH do
16: foralli,h ← ∅
i
17: a,o,r ← De(t)
18: foralli,h′ ← h a o
i i i i
19: y = r+γ(cid:80) imax a′ iQθ i−(h′ i,a′ i)
20: foralli,dogradientdescentonθ withlearningrateα andloss(cid:0) y −(cid:80) Qθ(h ,a )(cid:1)2
i i i i i
21: foralli,h ← h′
i i
22: endfor
23: ife mod C = 0then
24: foralli,θ− ← θ
i i
25: endif
26: e ← e+1
27: endfor
28: return allQ
i
10Figure4: QMIXdiagram
Specifically,QMIXassumesthefollowingapproximatefactorizationoftheQ-function:
Q(h,a) ≈ f (Q (h ,a ),...,Q (h ,a )) (7)
mono i 1 1 n n n
ThismonotonicassumptionmeanstheargmaxoverthelocalQ-functionsisalsoanargmaxover
the joint Q-function, This property allows agents to choose actions from their local Q-functions
insteadofhavingtochoosethemfromajointQ-function—ensuringthechoicewouldbethesame
in both cases. Like in VDN, it also makes the training more efficient since calculating the argmax
intheupdateEquation9islinearinthenumberofagentsratherthanexponential.
Sonetal.[2019]formalizedthispropertyastheIndividual-Global-Max(IGM)principal. IGM
states that the argmax over the joint Q-function is the same as argmaxing over each agent’s indi-
vidual Q-function. The definition below is for a particular history but it should ideally hold for all
histories(oratleasttheonesvisitedbythepolicy).
Definition1(Individual-Global-Max(IGM)Sonetal.[2019]) Forajointaction-valuefunction
Q(h,a), where h = ⟨h ,...,h ⟩ is a joint action-observation history, if there exist individual
1 n
functions[Q ],suchthat:
i
 
argmax Q (h ,a )
a1
.
1 1 1
argmaxQ(h,a) =  . . , (8)
 
a
argmax Q (h ,a )
an n n n
then,[Q ]satisfyIGMforQath.
i
AsimplifiedversionoftheQMIXarchitectureisshowninFigure4. LikeVDN,eachagenthas
an RNN that takes in the current observation and can output Q-values for the updated history over
allactions.7 Aparticularactionisthenchosen(e.g.,usingϵ-greedyactionselection)usingeachQ
i
7QMIXtypicallyalsoinputsthepreviousactionbutthisisomittedhereforclarityandconsistency.
11and each Q (h ,a ) is fed into the mixing network. The mixing network is made to be monotonic
i i i
by restricting the weights (but not the bias terms) to be non-negative. The mixing network can
also receive the state to potentially improve performance. In practice, the state is given as input to
hypernetworks [Ha et al., 2017] which generate the weights for the layers of the mixing network.
Becausetheoutputdependsonthestate,itisQ(h,s,a),andisoftencalledQ .
tot
The network is trained end-to-end, like VDN by calculating a loss between Q(h,s,a) and the
jointreturn. Thecorrespondinglossis:
(cid:104) (cid:105)
L(θ) = E (cid:0) y −Qθ(h,s,a)(cid:1)2 ,where y = r+γQθ−(h′,s′,a˜′),
<h,s,a,r,o,s′>∼D
and a˜′ = ⟨argmaxQ (h′,a′),...,argmaxQ (h′ ,a′ )⟩ (9)
1 1 1 n n n
a′ a′
1 n
where the value on the next time step, Qθ−(h′,s′,a˜′), is gotten by inputting the argmax over each
agent’slocal Q-valueusingh′ = ⟨h′,...,h′ ⟩aswell asthecorresponding states′ thatis sampled
1 n
fromthebufferath′.
The mixing network is only needed during training. Agents can retain their RNNs (which are
justDRQNs)forselectingactionsduringdecentralizedexecutionjustlikeVDN.
Again, such a factorization may not be possible in all problems so QMIX may not be able to
approximate the true joint Q-function in all problems accurately. In particular, as discussed in the
paper, this factorization should fail when agents’ action choices depend on (at least some) other
agents’actionsatthesametimestep.
Weighted QMIX extends QMIX in an attempt to improve its expressiveness [Rashid et al.,
2020a]. The main idea is to weigh the actions at different histories differently so the Q-values
for some actions can be accurately represented but other action values can be less accurate. For
instance, if you knew the optimal policy, you could set the weight highest for the optimal action
in each history. You don’t need to accurately represent the other action values but you do need to
makesuretheyarelowerthantheoptimalaction(sothepolicywillchoosetheoptimalaction!).
Specifically, weighted QMIX uses weights w(h,a) ∈ (0,1] to adjust the importance of joint
history-action pairs in the loss (as formalized below).8 It turns out that there always exists some
weighingthatwillallowtheoptimalpolicytoberecoveredbyindependentlyargmaxingovereach
agent’sQ . Anidealizedformofthealgorithmcouldalsoconvergeinthefullyobservablecasebut
i
this proof doesn’t extend to the partially observable case or when approximations are used (i.e.,
thedeepRLcase).
Themethodhastwomaincomponents,aQMIXnetworkwheretheoutputisweighted,andan
approximation of the optimal Q-value called
Qˆ∗.
The first network takes the output of QMIX (as
seen in Figure 4) and weighs it as seen in Equation 10. For easier differentiation with the other
Q network, we call the output of the first (QMIX-style) network Q . This network is exactly the
tot
same as the one in QMIX but the output is weighted in the loss to prioritize different actions at
differenthistories.
TheQˆ∗
networkissimilartotheoneinQMIXbutitdoesnotlimittheweights
tobenon-negativeanddoesn’tuseahypernetwork(justdirectlyinputtingthestateintothemixing
network). Thesenetworksdon’tshareparametersandaretrainedusingthelossesgiveninEquation
8The paper discusses much of the method and theory in terms of the fully observable case but we consider the
partiallyobservablecasehere. Inthepartiallyobservablecase,theweightscantakethehistoryorthehistoryandthe
state: w(h,s,a).
1210. The same target (y) is used in both cases, which now uses the unconstrained output
Qˆ∗.
The
argmax is done the same way as in QMIX, where the actions are chosen using each agent’s Q .
i
As a result, the argmax is tractable but a potentially more accurate value of those actions can be
providedfromQˆ∗
ratherthanQ .
tot
Inparticular,thelossesusesare:
(cid:104) (cid:105)
L(θ ) = E (cid:0) y −Qˆ∗(h,s,a)(cid:1)2 ,
cent <h,s,a,r,o,s′>∼D
(cid:104) (cid:105)
L(θ ) = E w(h,a)(cid:0) y −Q (h,s,a)(cid:1)2 ,
tot <h,s,a,r,o,s′>∼D tot
(10)
where y = r+γQˆ∗(h′,s′,a˜′), and a˜′ = ⟨argmaxQ (h′,a′),...,argmaxQ (h′ ,a′ )⟩
1 1 1 n n n
a′ a′
1 n
Twodifferentweightingfunctionsareconsidered. Centrally-WeightedQMIX(CW)uses:
(cid:26) 1 if y > Qˆ∗(h,s,a˜∗) or a = a˜∗
w(h,a) =
α otherwise
which is an approximation of the optimal action a˜∗ using the individual argmax as in Equation 10
and an approximation of the value function,
Qˆ∗,
using the unconstrained network. The weight is
set to 1 when the action is already known to be optimal (a = a˜∗) or when the action has a higher
valuethanthecurrentbestaction. Optimistically-WeightedQMIX(OW)uses:
(cid:26)
1 if y > Q (h,s,a)
w(h,a) = tot
α otherwise
which just sets the weight to 1 when the current estimate using Q is less than the target value,
tot
whichusesQˆ∗.
ThisinequalitysuggestserrorinQ andacould(optimistically)beoptimalath.
tot
α isahyperparamterthatcanbesetduringtraining.
After training, the Q from the constrained network (outputting Q ) can be used for decen-
i tot
tralizedactionselection(justlikeinQMIX).Therefore,Qˆ∗
isonlyusedtohelpguidethelearning
so better (hopefully, optimal) actions will have higher values in each agent’s Q . While weighted
i
QMIX can outperform QMIX in some cases, it isn’t as widely used as more recent methods (e.g.,
QPLEX[Wangetal.,2021a]andMAPPO[Yuetal.,2022]).
3.4 QTRAN
QTRAN [Son et al., 2019] provides a different way to factor the joint Q-function into individual
Q-functions. The idea generalizes VDN but the intuition is that the sum does not have to equal
the true Q-function but some transformed Q-function Q′. A separate V term9 can then be used to
account for the error between the transformed Q′ and the true Q. The approach cannot represent
all IGM-able functions but it is more general than VDN.10 QPLEX (which I talk about below) is
moregeneralandtherelationshiptoQMIXandweightedQMIXisunclear.
Specifically,QTRANusesEquation11asthebasisfortheirarchitectureandlosses.
9Whilethepapercallsthisastatevalue,thevaluetakeshistoriesasinput,notstates.
10WhilesomehavetakenthetextofthepapertoshowthatQTRANcanrepresentallIGM-ablefunctions,theproofs
onlyshowthatthemethodsatisfiesIGM(notthatanyIGM-ablefunctioncanberepresentedbythemethod).
13(cid:26)
(cid:88) 0 a = a˜
Q (h ,a )−Q(h,a)+V(h) = (11)
i i i ≥ 0 a ̸= a˜
i
where
(cid:88)
V(h) = maxQ(h,a)− Q (h ,a˜),
i i i
a
i
a˜ = argmaxQ (h ,a ), and a˜ = ⟨a˜,...,a˜⟩
i i i i 1 n
ai
Here, V(h) has a particular form where it is the difference between the (centralized) max over
the joint Q-function, Q, and the sum of the maxes over the individual Q-functions, Q . Equation
i
11 is constrained to be 0 when the actions are the argmax over the individual Q-functions, a˜, and
greaterthanorequalto0forotheractions. IfEquation11holds,theQ satisfyIGMfortheQbut
i
it isn’t clear how general it is. That is, unlike QPLEX, the proof does not show that all functions
thatsatisfyIGMcanberepresentedinthisform.
InsteadofenforcinganetworkstructuretogetIGMlikeVDNandQMIX,QTRANdoessoby
using losses based on Equation 11. The architecture outputs Q′(h,a) = (cid:80) Q (h ,a ), like VDN,
i i i i
as well as an unconstrained Q(h,a) and V(h). As shown below, there is the standard TD loss for
learningthetruejointQ-function,Q,alossforlearningthetransformedQ-function,Q′,andvalue
offset, V, for the given Q when the actions are currently the maximizing ones, a˜, in L (θ) and
opt
¯
for other actions in L (θ). The notation Q is used to note that Q isn’t trained from L (θ) and
nopt opt
L (θ).
nopt
(cid:104) (cid:105)
L (θ) = E (cid:0) y −Q(h,a)(cid:1)2 ,
td <h,a,r,o>∼D
(cid:104) (cid:105)
L ) = E (cid:0) Q′(h,a˜)−Q¯ (h,a˜)+V(h)(cid:1)2 ,
opt(θ <h,a,r,o>∼D
L (θ) = E
(cid:104)(cid:16)
min(cid:2) Q′(h,a)−Q¯
(h,a)+V(h),0(cid:3)(cid:17)2(cid:105)
, (12)
nopt <h,a,r,o>∼D
where y = r+γQ (h′,a˜′), and a˜′ = ⟨argmaxQ (h′,a′),...,argmaxQ (h′ ,a′ )⟩
θ− 1 1 1 n n n
a′ a′
1 n
ThelossesaboveareusedforthestandardformofQTRANcalledQTRAN-base. Analternate
form,calledQTRAN-alt,replacesL(θ )withtheaveragecounterfactuallossbelow,forcingthe
nopt
valuetobe0forsomeactionratherthanrelyingontheinequalityconstraintabove.
n
(cid:104)1 (cid:88)(cid:16) (cid:17)2(cid:105)
L(θ ) = E minQ′(h,a ,a )−Q¯ (h,a ,a )+V(h) ,
nopt−min <h,a,r,o>∼D i −i i −i
n
i
UsingthisconditionalsosatisfiesIGM.
TheversionsofQTRAN(QTRAN-baseandQTRAN-alt)canoutperformVDNandQMIXon
somedomainsbuttheyareoftenoutperformedbymorerecentmethods.
3.5 QPLEX
QPLEX[Wangetal.,2021a]furtherextendsthevaluefactorizationideato(provably)includemore
general decentralized policies. In particular, QPLEX can potentially learn any set of Q-functions
thatsatisfytheIGMprinciple.
14Figure5: QPLEXdiagram
First, QPLEX extends the IGM principle to an advantage-based case. They define joint values
andadvantagesfromthejointQ-function: V(h) = max Q(h,a),andA(h,a) = Q(h,a)−V(h)
a
as well as individual values and advantages from each agent’s individual Q-function: V (h ) =
i i
max Q (h ,a ), and A (h ,a ) = Q (h ,a ) − V (h ). Note that this notion of advantage is a
ai i i i i i i i i i i i
bit different than the typical notion (such as [Wang et al., 2016]) since the values and advantages
are generated from the Q-values, rather than the other way around. As a result, the advantages
will have the property that they will be 0 for optimal actions and negative otherwise. That is,
A (h ,a∗) = 0 (since Q (h ,a∗) − max Q (h ,a ) = 0) and for some non-optimal action a†,
i i i i i i ai i i i i
A (h ,a†) < 0(sinceQ (h ,a†) < max Q (h ,a )). Thesameholdstrueforjointadvantages.
i i i i i i ai i i i
Advantage-based IGM extends IGM to the advantage case and states that the argmax over the
jointadvantagefunctionisthesameasargmaxingovereachagent’sindividualadvantagefunction:
Definition2(Advantage-based(IGM)Wangetal.[2021a]) Forajointaction-valuefunction,if
thereexistindividualfunctions[Q ],suchthat:
i
 
argmax A (h ,a )
a1
.
1 1 1
argmaxA(h,a) =  . . . (13)
 
a
argmax A (h ,a )
an n n n
whereA(h,a)andA aredefinedabove,then[Q ]satisfyadvantage-basedIGMforQath.
i i
Thatis,advantage-basedIGMisequivalenttotheQ-basedIGMinDefinition1.
QPLEX uses advantage-based IGM to extend QMIX and QTRAN to represent the full IGM
function class. The architecture is shown in Figure 5. Like QMIX (and VDN and QTRAN),
QPLEX first takes as input each agent’s observation o (and previous action at−1 but that is not
i i
included here) and outputs the individual Q-values, Q (h ,a ), for all a using a DRQN-style
i i i i
15network. After an action is selected (e.g., using ϵ-greedy exploration), the value and advantage
are extracted from the Q-values for each agent as V (h ) = max Q (h ,a ) and A (h ,a ) =
i i ai i i i i i i
Q (h ,a )−V (h ). Next,thetransformationnetworksgenerateV (h ,s)andA (h ,s,a )foreach
i i i i i i i i i i
agent with the given V (h ) and A (h ,a ) along with s as V (h ,s) = w (s)V (h ) + b (s) and
i i i i i i i i i i i
A (h ,s,a ) = w (s)A (h ,a ). Finally, the joint Q-value is output based on the output of each
i i i i i i i
(cid:80)
agent’stransformationnetworkandthestateasQ(h,s,a) = V (h ,s)+λ (s,a)A (h ,s,a ).11
i i i i i i i
While there aren’t constraints on V due to IGM (because it doesn’t include the actions), QPLEX
uses a sum to combine the local V’s. All the weights, w and λ (but not necessarily the biases b )
i i i
are positive to maintain monotonicity (like QMIX) and thus satisfy the advantage IGM principle.
ThearchitectureistrainedusingtheRLloss,justlikeQMIXinEquation9.
While positive weights are used (like in QMIX), there are separate weights for A and V, and
the weights for V don’t depend on the action. As a result, QPLEX is more general than previous
methods with the capacity to represent all Q-functions that satisfy IGM. QPLEX also performs
well, often outperforming other value factorization methods and it is currently one of the best-
performingCTDEmethods.
3.6 The use of state in factorization methods
Manyofthecurrentmethodsuseinconsistentnotationabouttheinclusionofstate(exceptQMIX).
As a result, the theory is often developed without considering the state input. It turns out that,
unlike the policy gradient case discussed in Section 4.6, the using the state doesn’t introduce bias
andisthustheoreticallysound[Marchesinietal.,2024]. Theintuitionisthatthestateisadditional
information in these cases. It doesn’t replace the history, but potentially augments it (similar to a
history-statecriticinSection4.6). Furthermore,actionsarenotchosenbasedonthestatebutonly
thelocal,history-dependentQ-values,Q . Nevertheless,inimplementationsofalgorithmssuchas
i
QPLEX and weighted QMIX, the weights only take the state as input and not the history. This
replacementofhistorywithstateinthesecontextslimitstherepresentationalpoweroftheweights
in partially observable problems. As a result, using the state as input in QPLEX (instead of the
joint history) prevents it from being able to represent the full IGM class of functions. It still may
be beneficial to use the state instead of the joint history but exploring what additional information
touseandinwhatwayisaninterestingopenquestion.
4 Centralized critic methods
Concurrently, Multi-Agent Deep Deterministic Policy Gradient (MADDPG) [Lowe et al., 2017]
and COunterfactual Multi-Agent policy gradient (COMA) [Foerster et al., 2018b] popularized the
use of centralized critics in MARL. I will first discuss the general class of algorithms that MAD-
DPG and COMA represent—multi-agent actor-critic methods with centralized critics—and then
give the details of MADDPG and COMA. I will then describe the very popular PPO-based exten-
sion, MAPPO [Yu et al., 2022]. Finally, I will discuss the (incorrect) use of state in centralized
criticsaswellasthetheoreticalandpracticaltradeoffsinthevarioustypesofcritics.
11Attentionisusedtotraintheλweightsmoreefficiently.
16(a) (b)
Figure6: Decentralizedcritics(a)vs.acentralizedcritic(b).
4.1 Preliminaries
Single-agentpolicygradientandactor-criticmethodshavealsobeenextendedtotheDec-POMDP
case. In the multi-agent case, there is one actor per agent and either one critic per agent (as
shown in 6(a) and discussed in [Amato, 2024]) or a shared, centralized critic (as shown in 6(b)
anddiscussedbelow). ThecentralizedcriticcanbeusedduringCTDEbuttheneachagentcanact
in a decentralized manner by using its actor. The basic motivation is to leverage the centralized
informationthatisavailableduringtrainingandpotentiallycombatnonstationaritythatcanhappen
in decentralized training. All the policy gradient approaches can generally scale to larger action
spacesandhavestronger(local)convergenceguaranteesthanthevalue-basedcounterpartsabove.
Policygradientmethodsusecontinuous-actionorstochasticpolicies. Unlessstatedotherwise,I
willassumeastochasticpolicyforeachagentparameterizedbyψ ,whereπψi(a |h )representsthe
i i i i
probabilityagentiwillchooseactiona giventhehistoryh andparameters,ψ ,Pr(a |h ,ψ ). This
i i i i i i
is in contrastto the value-based methods inSection 3 where deterministic policieswere generated
based on the learned value functions. Like the value-based methods, the policy gradient methods
alsoassumealgorithmsreceiveagentactions,a,theresultingobservations,o,andthejointreward,
r ateachtimestep.
4.2 A basic centralized critic approach
In the simplest form, we can consider a class of centralized critic methods where the joint history
critic, which we denote as Qˆ (h,a), estimates Qπ(h,a) with parameters θ. Qˆ (h,a) is then used
to update each decentralized policy π . We call this approach independent actor with centralized
i
critic(IACC)anditisdescribedinAlgorithm2. Learningisoverasetof(on-policy)episodesand
the histories are initialized to be empty. Agent i’s history at time step t is denoted h , while the
i,t
set of histories for all agents is denoted h . All agents choose an initial action from their policies
t
a ∼ π (a |h ) and then the learning takes place over a fixed horizon of H. At each step, the
i,0 i i i,0
agents take the corresponding current action a , receive the joint reward r and see observations
t t
o . The histories for the next step are updated to include the action taken and observation seen,
t
h a o , and actions are sampled for the next step, a ∼ π (a |h ). Then, the TD error
i,t i,t i,t i,t+1 i i i,t+1
can be computed with the current Q-values, and then is used to update the actors and critic. Each
agent’sactorisupdatedusingtheTDerrorfromthejointQ-function(movinginthedirectionthat
improves the joint Q-value). The centralized critic is updated using a standard on-policy gradient
update. The action for the next step, t + 1, becomes the action for the current step, t, and the
process continues until the end of the horizon and for each episode. The algorithm is written for
the finite-horizon case but it can be adapted to the episodic or infinite-horizon case by including
terminalstatesorremovingtheloopoverepisodes. Notethatthehistoriesmakethevaluefunctions
basedonaparticulartimestepasthehistorylengthprovidesthetimestep.
17While a sample is used in the algorithm, the full gradient associated with each actor in IACC
isrepresentedas:
∇ J = E [Qπ(h,a)∇ logπ (a |h )] ,12 (14)
ψi <h,a>∼D ψi i i i
ˆ
where the approximation of the joint Q-function, Q(h,a), is used to update the policy parameters
of agent i’s actor, π (a |h ). Histories (i.e., observations) and actions are sampled according to the
i i i
on-policy(discounted)visitationprobability.
The objective, J, is to maximize expected (discounted) return starting from the initial state
distributionb asdiscussedinSection1.
0
Thecriticisupdatedusingtheon-policyloss:
(cid:104) (cid:105)
L(θ) = E (cid:0) y −Qˆ (h,a)(cid:1)2 ,where y = r+γQˆ (h′,a′)
<h,a,r,h′>∼D
Algorithm2IndependentActorCentralizedCritic(IACC)(finite-horizon)
1: Initializeindividualactormodelsπ (a |h ),parameterizedbyψ
i i i i
ˆ
2: InitializecentralizedcriticmodelQ(h,a),parameterizedbyθ
3: forallepisodesdo
4: h ← ∅ {Emptyinitialhistory}
i,0
5: Denoteh as⟨h ,...,h ⟩ {Notationforjointvariables}
t 1,0 n,0
6: foralli,choosea ath fromπ (a |h )
i,0 i,0 i i i,0
7: Storea as⟨a ,...,a ⟩
t 1,0 n,0
8: fort = 0toH−1do
9: Takejointactiona ,seejointrewardr ,andobservationso
t t t
10: foralli,h ← h a o {Appendnewactionandobstoprevioushistory}
i,t+1 i,t i,t i,t
11: foralli,choosea ath fromπ (a |h )
i,t+1 i,t+1 i i i,t+1
12: Storea as⟨a ,...,a ⟩
t+1 1,t+1 n,t+1
ˆ ˆ
13: δ ← r +γQ(h ,a )−Q(h ,a ) {ComputecentralizedTDerror}
t t t+1 t+1 t t
ˆ
14: Computecriticgradientestimate: δ ∇ Q(h ,a )
t θ t t
ˆ
15: Update critic parameters θ using gradient estimate (e.g., θ ← θ + βδ ∇ Q(h ,a ) for
t θ t t
learningrateβ)
16: foreachagentido
17: Computeactorgradientestimate:
γtQˆ
(h ,a )∇ logπ (a |h )
t t ψi i i,t i,t
18: Update actor parameters ψ using gradient estimate (e.g., ψ ← ψ +
i i i
αγtQˆ
(h,a)∇ logπ (a |h )forlearningrateα)
ψi i i,t i,t
19: endfor
20: endfor
21: endfor
Wecanextendthisalgorithmtothemorecommonlyusedadvantageactor-critic(A2C)case,as
shown in Algorithm 3. The advantage is defined as A(h,a) = Q(h,a)−V(h), representing the
difference between the Q-value for a given action and the V-value at that history. Because V(h)
doesn’t depend on the action, it is called a baseline and becomes a constant from the perspective
of the policy gradient. As a result using Q(h,a) −V(h) in place of Q(h,a) doesn’t change the
12Forsimplicityandtomatchcommonimplementations,weignorediscountingbutthetruegradientwouldinclude
it[NotaandThomas,2020].
18convergence properties of the method. Nevertheless, using a baseline (e.g., subtracting V(h)) can
reducethevarianceoftheestimateandimproveperformanceinpractice. Inthealgorithm,A(h,a)
ˆ
isn’t explicitly stored since the TD error is used as an approximation for the advantage—Q(h,a)
ˆ
isapproximatedwithr +γV(h ),whichistrueinexpectation. δ canbethoughtofasasample
t t+1 t
ˆ ˆ ˆ
ofQ(h ,a )−V(h ) = A(h ,a ).
t t t t t
Algorithm3IndependentAdvantageActorCentralizedCritic(IA2CC)(finite-horizon)
1: Initializeindividualactormodelsπ (a |h ),parameterizedbyψ
i i i i
ˆ
2: InitializecentralizedcriticmodelV(h),parameterizedbyθ
3: forallepisodesdo
4: foralli,h ← ∅ {Emptyinitialhistory}
i,0
5: Denoteh as⟨h ,...,h ⟩ {Notationforjointvariables}
t 1,0 n,0
6: foralli,choosea ath fromπ (a |h )
i,0 i,0 i i i,0
7: Storea as⟨a ,...,a ⟩
t 1,0 n,0
8: fort = 0toH−1do
9: Takejointactiona ,seejointrewardr ,andobservationso
t t t
10: foralli,h ← h a o {Appendnewactionandobstoprevioushistory}
i,t+1 i,t i,t i,t
11: foralli,choosea ath fromπ (a |h )
i,t+1 i,t+1 i i i,t+1
12: Storea as⟨a ,...,a ⟩
t+1 1,t+1 n,t+1
ˆ ˆ
13: δ ← r +γV(h )−V(h ) {Computecentralizedadvantageestimates(TDerror)}
t t t+1 t
ˆ
14: Computecriticgradientestimate: δ ∇V(h )
t t
ˆ
15: Updatecriticparametersθ usinggradientestimate(e.g.,θ ← θ+βγδ ∇V(h ))
t t
16: foreachagentido
17: Computeactorgradientestimate: γtδ ∇logπ (a |h )
t i i,t i,t
18: Update actor parameters ψ using gradient estimate (e.g., ψ ← ψ +
i i i
αγtδ ∇logπ (a |h ))
t i i,t i,t
19: endfor
20: endfor
21: endfor
While the critic is called centralized because it uses centralized information, it estimates the
joint value function of the decentralized policies. That is, the centralized critic estimatesthe value
of the current set of decentralized policies (not a centralized policy) [Lyu et al., 2023]. This is the
correct thing to do since the critic’s job is to evaluate the policies, which are decentralized in this
case.
Withappropriateassumptions(e.g.,onexploration,learningrate,andfunctionapproximation),
IACC(andIA2CC)willconvergetoalocaloptimum[Peshkinetal.,2000,Lyuetal.,2023,2024].
These results put policy gradient methods on solid theoretical ground and I discuss details of the
variousmethodsaswellassometheoreticalshortcomings.
This idea of learning a centralized critic to update decentralized actors is very general. It can
be (and has been) used with different types of critics, actors, and updates. I present some of the
mostpopularmethodsbelow.
194.3 MADDPG
Multi-Agent DDPG (MADDPG) [Lowe et al., 2017] considers the more general case of possibly
competitive agents as well as continuous actions. To deal with the different reward functions of
different agents in the competitive case, separate centralized critics are learned for each agent. As
we are only concerned with the cooperative case, we assume a single shared critic among agents,
do not have to learn policy models of the other agents (since these are assumed to be accessible
in a cooperative CTDE setting), and do not consider ensembles of other agent policies to improve
robustness. MADDPG is also an off-policy method, unlike the previous algorithms discussed, so
it makes use of replay buffer similar to DQN-based approaches. Nevertheless, MADDPG for the
cooperativecaseisverysimilartoAlgorithm2withthemainchangesnotedbelow.
To deal with continuous actions, MADDPG extends the continuous action single-agent deep
actor-critic method DDPG [Lillicrap et al., 2016] to the multi-agent case. I denote the determin-
istic continuous-action policies for each agent as µ . In the cooperative case, MADDPG uses the
i
followingpolicygradient:
∇ J =. (1−γ)E (cid:2) ∇ µ (o )∇ Qπ(x,a) | (cid:3) , (15)
ψi x,a∼ρ(x,a) ψi i i a ai=µi(oi)
where x and a are sampled according to the (discounted) visitation probability with x discussed
below and a = ⟨a ,...,a ⟩. Since we can can no longer sum over actions and weigh their value
1 n
by their probability as is done in the stochastic policy case, we have to consider the change in the
Q-valuefunctionevaluatedatthechosenactiona fortheagent[Silveretal.,2014].
i
Note that MADDPG considers reactive policies that only map from the last observation to
an action (as seen in µ ). As noted in Section 1, the policy, µ , should choose actions based on
i i
histories, h , rather than single observations, o . The Q-value is from a centralized critic so the
i i
paper states x “could consist of the observations of all agents [...] however, we could also include
additional state information if available.” More generally, x should be history representations for
each agent and we discuss the use of states in more detail in 4.6. These issues are easily fixable
andresultinthefollowingpolicygradient(wecanincorporatethestateinformationlater):
∇ J =. (1−γ)E (cid:2) ∇ µ (h )∇ Qπ(h,a) | (cid:3) . (16)
ψi x,a∼ρ(h,a) ψi i i a ai=µi(hi)
BecauseMADDPGisoff-policy,itmaintainsareplaybufferlikeDQN-basedapproachesalong
with ‘target’ copies of both the actor and critic networks, µ and Q . The Q-update (for the
ψ− θ−
i
history-basedcase)isthen:
(cid:104) (cid:105)
L(θ) = E (cid:0) y −Q (h,a)(cid:1)2 ,where y = r+γQ (h′,a′) | (17)
<h,a,r,h′>∼D θ θ− ai=µ−(hi)∀i∈I
MADDPG executes a stochastic exploration policy (e.g., by adding Gaussian noise to the current
deterministic policy) while estimating (and optimizing) the value of the deterministic policy. This
can be seen because while the action, a, is sampled from the (behavior policy) dataset, the next
action,a′ isselectedbythetargetpolicy,µ−.
Algorithm2canbeupdatedtoincorporatethesechanges. Sincethecontinuous-actionpolicyis
deterministic,(Guassian)noiseisaddedwhenselectingactionstoaidinexploration. Theapproach
isoff-policy,sotheexperiencesarefirststoredinareplaybuffer(likeDQNandothervalue-based
methodsin Section3). Similarly, episodesare sampledfrom thereplay bufferand targetnetworks
used(again,likeDRQN-basedapproaches)fortheactorandthecritic.
MADDPG is no longer widely used but the ideas (such as the centralized critic) have been
adoptedextensively.
204.4 COMA
The main contributions of Counterfactual Multi-Agent Policy Gradients (COMA) were the intro-
duction of the centralized critic along with a counterfactual baseline [Foerster et al., 2018b]. As
mentioned above, baselines are common in policy gradient methods since they are high variance.
The baseline value is subtracted from the Q-value and it can be anything that is not dependent on
theagent’saction.
In the case of COMA, the motivation for the baseline is not only variance reduction but also
bettercreditassignmentbysubtractingofftheperceivedcontributiontotheQ-valuefromtheother
agents. Specifically, it marginalizes out the agent actions from the Q-function to get an estimate
of what the (counterfactual) Q-function would be while holding the other agent actions fixed. The
result is an agent-specific advantage function that subtracts the baseline for agent i from the joint
Q-function:
(cid:88)
A (h,a) = Q(h,a)− π (a′|h )Q(h,a′,a )
i i i i i −i
a′
i
Thisbaselinenolongerdependsonagenti’saction(inexpectation)soitwillnotbiasthegradient.
Note that the COMA paper uses a state-based critic and advantage, Q(s,a) and A (s,a), but this
i
isincorrectasI’lldiscussinSection4.6.
Rather than have separate baseline networks for each agent, there is a single centralized critic
that takes in the other agent actions, a , the joint observation, o, an agent id, i, the proposed
−i
action a , and policy probabilities π (a′|h ), and outputs the advantage for that agent A (h,a)
i i i i i
usingtheequationabove. ThisnetworkcanalsooutputthejointQ-valuesforallagenti’sactions,
Q(h,·,a ),giventheotheragentactions,a ,thejointobservation,o,andanagentid,i. Bothof
−i −i
thesevaluesareneededforthealgorithm.
Then, the COMA algorithm can be an extension of Algorithm 2. The critic update can be
calculated in the same way (using the Q-values from the network described above) but the agent-
specific advantage is incorporated in the actor update. That is, during the actor update (starting on
line 16), the agent-advantage A (h,a) is calculated for the given agent and A is used in the actor
i i
ˆ
gradientestimateinsteadofQ.
While COMA has been very influential, it isn’t widely used since newer methods tend to out-
performit.
4.5 MAPPO
Multi-Agent PPO (MAPPO) extends PPO [Schulman et al., 2017] to the centralized critic MARL
case [Yu et al., 2022]. The motivation behind PPO is to adjust the magnitude of the policy up-
date to learn quickly without becoming unstable. PPO does this using a simple clipped loss that
approximatesthemorecomplextrustregionupdate[Schulmanetal.,2015].
LikeIA2CC(Algorithm3),MAPPOusesanadvantage-basedupdate(butnottheagent-specific
counterfactual one used in COMA). In particular, instead of the loss being γtδ logπ (a |h ) as
t i i,t i,t
reflectedonline17,whichisanapproximationofγtA logπ (a |h ),thelossbecomes:
t i i,t i,t
(cid:16) (cid:17)
LMAPPO(ψ ) = min r A,clip(r ,1−ϵ,1+ϵ)A , (18)
clip i ψi,i ψi,i
21where r = π ψi(ai|hi) .13 Maximizing π ψi(ai|hi) A seeks to maximally improve the new
ψi,i π ψi,old(ai|hi) π ψi,old(ai|hi)
policy, π , compared to the old policy, π , by reweighing the advantage using an importance
ψi ψ
i,old
sampling ratio (considering the advantage was calculated using the old policy). It can be difficult
to estimate this value from a small number of samples and it may result in too large of an update
(resulting in parameters that perform worse). As a result, PPO also considered a term using a
clippedratio,limitingr soitcan’tbetoofaraboveorbelow1(whichwouldbethevaluewhen
ψi,i
thenewpolicyisequaltotheoldone). Byminimizingovertheunclippedandclippedvalues,PPO
limitschangesinthepolicy. Notetheratioisalwayspositivebuttheadvantagecouldbepositiveor
negative. When the advantage is positive, the loss will be clipped if the ratio is too large, limiting
how much more likely the action can be in the policy. Similarly, when the advantage is negative,
thelosswillbeclippedwhentheratioisnear0,limitinghowmuchlesslikelytheactioncanbein
ˆ ˆ
thepolicy. LikeinIA2CC,Acanstillbeapproximatedasr +γV(h )−V(h ).
t t+1 t
ThecriticlossforMAPPOalsoincludesclippingandisgivenby:
(cid:20) (cid:21)
(cid:16) (cid:17)2
LMAPPO(θ) = max (V(h )−Rˆ )2, clip(V(h),V (h)−ϵ,V (h)+ϵ)−Rˆ (19)
t t old old t
where
Rˆ
are Monte Carlo returns starting from the current history (i.e.,
Rˆ
=
(cid:80)H
γr ) and
t j=t j
V represents the value function from the previous time step but TD can also be used and value
old
clippingdoesnothavetobeused.
All agents use the same centralized critic and parameter sharing can also used so all agents
sharethesameactornetwork.
Independent PPO (IPPO) is a version of MAPPO where each agent uses its own local advan-
tageratherthanthejointadvantage. Asaresult,theIPPOactorlossbecomes:
(cid:16) (cid:17)
LIPPO(ψ ) = min r A ,clip(r ,1−ϵ,1+ϵ)A , (20)
clip i ψi,i i ψi,i i
ˆ ˆ ˆ
withA = r +γV (h )−V (h ). TheIPPOcriticlossbecomes:
i t i i,t+1 i i,t
(cid:20) (cid:21)
(cid:16) (cid:17)2
LIPPO(θ) = max (V (h ))−Rˆ )2, clip(V (h )),V (h ))−ϵ,V (h ))+ϵ)−Rˆ
i i,t t i i,t i,old i,t i,old i,t t
(21)
ˆ
whereRareMonteCarloreturnsstartingfromthecurrentlocalhistory,V (h ). Parametersharing
i i,t
isusedwithIPPOsoallagentssharetheactorandcriticnetwork,makingitaCTDEapproach. As
aresult,thereisonlyoneactorandonecritic,butitisupdatedusingthedatafromallagents.
MAPPO and IPPO perform very well on the standard benchmark domains. Both algorithms
typically perform similarly, but a version of MAPPO where the information in the critic was hand
designed to only include features that are relevant to the task could outperform the standard ver-
sionsofMAPPOandIPPO.Thismakessenseastheagentsdidnothavetolearnwhatinformation
wasnecessary,whichisdifficultinhigh-dimensionalsettings.
13Here,Iconsiderthestochastic(notcontinuous)actioncaseandforsimplicitydonotincludeGAE,mini-batching,
orapolicyentropyterm(whichiscommoninpolicygradientmethodstoimproveexploration). Also,thepaperuses
a local advantage loss, A , but this value would not be different than A in a general Dec-POMDP if it is updated
i
usingAandrsincethesequantitieswouldbethesameforallagents. Finally,policiesoversingleobservationsrather
than histories are used, which as noted before is typically not sufficient for partially observable problems (general
Dec-POMDPs)andstate-basedcriticsareused,whichisincorrectasdiscussedbelow.
224.6 State-based critics
Critics that use state (and not history) are unsound in partially observable environments (i.e., gen-
eral Dec-POMDPs). That is, they are incorrect and could be arbitrarily bad. They can work well
indomainsthatarenearlyfullyobservable(suchasSMAC[Samvelyanetal.,2019])anddomains
inwhichrememberinghistoryinformationisnothelpful(observationsarenotveryinformativeor
the task is too hard to solve). This result has been shown theoretically and empirically [Lyu et al.,
2022,2023].
In particular, MADDPG [Lowe et al., 2017] and COMA [Foerster et al., 2018b] popularized
the idea of using state-based critics using the intuition that this ground truth information that is
availableduringtrainingcanhelpaddresspartialobservability. Intruth,itistypicallyabadideato
eliminatepartialobservabilityinthecritic. ADec-POMDPtypicallyispartiallyobservableandthe
actorisusingpartiallyobservableinformation(i.e.,histories). Thestate-basedcriticcanbebiased
(i.e.,incorrect)becausethereisamismatchbetweentheactorandthecritic. Theactorneedstouse
history information to choose actions. The state-based critic uses state values instead. As a result,
the state values are averaged over the histories that are visited and can’t possibly have the correct
history values unless there is a one-to-one mapping from states to histories (which would happen
in the fully observable case). That is, the critic loses the information about partial observability,
whichisnecessaryfortheactortomakegoodchoices.
For example, in the class Dec-Tiger problem [Nair et al., 2003, Oliehoek and Amato, 2016],
thereareonly2states(thetigerbeingontheleftorontheright)butnoisyobservationsarereceived
whenanagentlistens(anagentcanalsochoosetoopenoneofthetwodoors). Astate-basedcritic
would only have two values V(s1) and V(s2) but there are an exponential number of histories
that depends on the horizon. As more information is gathered (i.e., the same observation about
the tiger’s location is heard multiple times) the value of the history, V(h), should increase (so
V(h ) > V(h ), the initial history). This can’t be reflected in the state-
tiger−left−10−times−in−a−row 0
basedcritic. Furthermore,information-gatheringactions(i.e.,listeninginthistigerproblem)won’t
improve the value according to the critic so they shouldn’t be taken. As a result, the state-based
criticvalueswillbeincorrectintheDec-Tigerproblemandtheagentswouldlearnapolicytoopen
oneofthedoorsandhopeforagoodoutcome(assumingthetigerisrandomlyinitialized).
ˆ
TheupdatestoAlgorithm3forthestate-basedcriticcasearestraightforward—V(s)isusedin
ˆ ˆ ˆ
place of V(h). Then, the TD error calculation becomes δ ← r + γV(s ) − V(s ), which is
t t t+1 t
usedintheactorandcriticupdates.
Itturnsoutthathistory-statecritics,thosethattakeboththestateandhistoryasinput(Q(h,s,a)
or V(h,s) are unbiased and often perform the best [Lyu et al., 2022, 2023, Yu et al., 2022]. This
result has been theoretically and empirically shown for the general single and multi-agent cases
[BaiseroandAmato,2022,Lyuetal.,2022,2023]aswellasempiricallywithvariantsofMAPPO
[Yu et al., 2022]. The intuition for correctness of the history-state critic is that no history infor-
mation is lost (as it is in the simpler state-based critic). That is, the correct history value func-
tion can be (and is) recovered from the history-state value: Qπ(h,a) = E [Qπ(h,s,a)]. The
s|h
MAPPOpaperalsoshowedthatifyoucanhandcraftthefeaturesofthehistory-statecritictoretain
only the relevant information, the performance can be the highest since the agent doesn’t need
to learn which information is relevant and which is not. Unfortunately, determining which infor-
mation is relevant and properly separating it can be difficult to do. The algorithmic updates for
ˆ ˆ
the history-state critic case are also straightforward—V(h,s) is used in place of V(h). The TD
ˆ ˆ
error calculation becomes δ ← r +γV(h,s )−V(h,s ), which is used in the actor and critic
t t t+1 t
23updates.
4.7 Choosing different types of decentralized and centralized critics
WhileCTDEmethodsare(byfar)themostpopularformofMARL,theydonotalwaysperformthe
best. Infact,decentralizedtrainingandexecutionmethods(DTE)[Amato,2024]areactuallyquite
close to CTDE methods. This is because DTE methods typically make the concurrent learning
assumption where all the agents learn using the same algorithm, making updates at the same time
onthejointdata. Thisequivalenceisseenexplicitlyinthepolicygradientcaseasthegradientofthe
jointupdateisthesameasthedecentralizedgradient[Peshkinetal.,2000]. Thisphenomenonhas
been also been studied theoretically and empirically with modern actor-critic methods [Lyu et al.,
2021, 2023]. It turns out that while centralized critic actor-critic methods are often assumed to be
better than DTE actor-critic methods, they are theoretically the same (with mild assumptions) and
oftenempiricallysimilar(andsometimesworse). Toomuchcentralizedinformationcansometimes
beoverwhelming,harmingscalabilityofcentralized-criticmethodsandleadingtohighervariance
[Yuetal.,2022,Lyuetal.,2023].
The theory assumes learned critics (or sufficient Monte Carlo estimates) but different types of
critics may be easier to learn in different cases. As a result, it is not straightforward to determine
when to use each critic in practice. Centralized critics often cause higher variance actor updates
since information from the other agents needs to be marginalized out (e.g., through sampling)
while information from the other agents is already removed from the decentralized critics. Cen-
tralizedcriticscanalsobemoredifficulttolearninproblemswithlargenumbersofagentsorwith
large action or observation sets, making decentralized critics more scalable. Decentralized critics
may be difficult to learn due to agents changing their policies (i.e., nonstationarity), making critic
estimates stale. State-based critics are often the easiest to learn (because no history representation
needs to be learned) but are biased in partially observable settings and will have high variance
(again, because of the mismatch between the actor and the critic). In general, learning a history
representation is difficult. Agents need to figure out what history information is relevant and what
is not from a noisy and often sparse RL signal. Using a recurrent network (or even a transformer)
isusuallynotsufficientforthistask. History-statecriticsfurtherincreasethevarianceinthepolicy
update (because the state information needs to also be marginalized out) but can often be easier
to learn than centralized critics with only history information.14 As a result the choice of critic is
often a bias-variance tradeoff since, in practice, a centralized critic should have a lower bias than
decentralized critics with more stable values that are more easily updated when policies change
buthighervariancebecausethepolicyupdatesneedtobeaveragedoverotheragents.
There are many different ways of performing CTDE, and current work has only scratched the
surface. Studying the differences and similarities between DTE and CTDE variants of algorithms
seems like a promising direction for understanding current methods and developing improved ap-
proaches for both cases. Determining what centralized information to use and how to best use
it is another key question. Lastly, to the best of my knowledge, there are no globally optimal
model-freeMARLmethodsforDec-POMDPs. Thisissurprisingsincethereareoptimalplanning
methodswherethemodelisassumedknown[OliehoekandAmato,2016]andmanygloballyopti-
mal model-free methods for single-agent RL [Sutton and Barto, 2018]. Developing such methods
(even for the tabular case) would be interesting and may lead to new, better methods that could be
14Theexactreasonwhythisisthecaseisunclearandagreattopicforfutureresearch!
24approximatedinthedeepcase.
4.8 Methods that combine policy gradient and value factorization
Severalmethodscombineusingacentralizedcriticwithvaluefactorization. Onenotablemethodis
FACtoredMulti-AgentCentralizedpolicygradients(FACMAC)[Pengetal.,2021]whichextends
MADDPG to include a QMIX-style factored centralized critic, but since the local Q-values don’t
need to be used for action selection (the actor does that), the factorization can be nonmonotonic.
Also, rather than sampling the actions of other agents from the off-policy dataset, as MADDPG
does, actions for other agents are sampled from the current policies during the actor updates. De-
composedOff-Policypolicygradient(DOP)[Wangetal.,2021b]usesafactoredcentralizedcritic
that is a weighted sum of the local Q-values and a multi-agent extension of tree backup [Precup
et al., 2000] to more efficiently calculate off-policy updates. While a centralized critic is learned,
each agent’s local Q-value estimate is used to update its actor and with mild assumptions the
methodisshowntoconvergetoalocaloptimumevenwiththesimplecritic.
4.9 Other centralized critic methods
Many other methods use centralized critics. For instance, Iqbal and Sha [2019] use attention for
determiningwhichotheragentinformationtoincludeinthecentralizedcritic. Qatten[Yangetal.,
2020] extends QMIX (and weighted QMIX) to add structure to the mixing network, where the
structure is inspired by a linear approximation of the Q-function and one set of weights is leaned
usingattention. Awiderangeofotherapproacheshavebeendevelopedbutarenotincludedinthis
introductiontothearea.
5 Other forms of CTDE
While the methods above are the ones that are most widely used, there are several other forms of
CTDE. I discuss common approaches and include a note on the many other topics that are not in
thistextinordertofocusoncoreissuesofCTDE.
5.1 Adding centralized information to decentralized methods
Asnotedintheintroduction,decentralizedtrainingandexecution(DTE)methodsareindependent
learning methods [Claus and Boutilier, 1998] where each agent learns on its own using only its
owninformation. Moredetailsabout decentralizedtraining methodscan befoundin acompanion
text [Amato, 2024]. DTE fit well when there is no offline training phase or when scalability is the
key factor. DTE methods can be augmented with centralized information to potentially improve
performance. Theseextensionsarediscussedbelow.
Parametersharing ManycooperativeMARLmethodsuseparametersharing. AsnotedinSec-
tion 4.5, the idea behind parameter (or weight) sharing is instead of each agent using a different
networktoestimatethevaluefunctionorpolicy,allagentssharethesamenetworks. Thedatafrom
eachagentcanbeusedtoupdateasinglevaluenetworkforanalgorithmsuchasDRQN,speeding
up learning. Agents can still perform differently due to observing different histories and agent
25indices can be added to increase specialization [Gupta et al., 2017, Foerster et al., 2016]. While
parametersharingistypicallyusedwithhomogeneousagents(i.e.,thosewiththesameactionand
observation spaces), it can also be used with heterogeneous agents [Terry et al., 2020]. Since pa-
rameter sharing requires agents to share networks during training, it couldn’t be used for online
training in a decentralized fashion. Nevertheless, decentralized algorithms can be extended to use
parameter sharing (such as the case with IPPO). Parameter-sharing implementations may be more
scalable than other forms of CTDE, are often simple to implement, and can perform well [Gupta
etal.,2017,Foersteretal.,2018b,Yuetal.,2022].
Alternating learning A number of methods allow agents to take turns learning. For instance,
decentralized learning methods [Amato, 2024] can be used but all agents are fixed (not learning)
exceptforone. Thelearningagentlearnsuntilconvergence,generatingabest-responsetotheother
agentpolicies. Ifthisprocesscontinuesuntilnoagentscanfurtherimprovetheirpolices,theresult
is a Nash equilibrium [Banerjee et al., 2012, Su et al., 2024]. In limited settings with additional
strong assumptions (e.g., deterministic environments, full observability, additional coordination
mechanisms to ensure coordinated policies), such methods can potentially converge to an optimal
solutions[LauerandRiedmiller,2000,JiangandLu,2023,2022]. Whilethesemethodsaresome-
times called ‘decentralized’ since coordination and communication is need during learning that is
notavailableduringexecution,IconsiderthemtobeCTDE.
Kuba et al. [2022] use sequential agent updates (i.e., one agent updates at a time while hold-
ing the others fixed) in Heterogeneous-Agent Trust Region Policy Optimisation (HATRPO) and
Heterogeneous-Agent Proximal Policy Optimisation (HAPPO). HATRPO and HAPPO build off
of TRPO [Schulman et al., 2015] and PPO [Schulman et al., 2017] and remove parameter sharing
usedinMAPPO[Yuetal.,2022](leadingtotheheterogeneous-agentname). Usingthissequential
updatescheme, theycan theoretically provemonotonic improvement forthe fullyobservablecase
andtheapproachescanworkwellinpractice.
Addressing nonstationarity Decentralized learning methods face a nonstationary problem due
to changing policies of other agents. Some methods try to address this challenge by directly mod-
eling these changes. For example, Foerster et al. [2017] propose using importance sampling to
correct for the probability differences and decay old data or simpler ‘fingerprints’ (e.g., episode
number) to mark the data’s age. Other methods try to model the other agent learning updates as
well[Foersteretal.,2018a,Willietal.,2022].
5.2 Decentralizing centralized solutions
Another potential approach is to learn a centralized solution during training and then (attempt to)
decentralize it before execution. This is much harder than it seems. A centralized policy can map
H → A without the constraint (in the stochastic case) that π(a|h) = (cid:81) π (a |h ). That is,
i∈I i i i
(cid:81)
each agent’s policy can depend on other agent histories: π(a|h) = π (a |h). The centralized
i∈I i i
policyclassismuchlargerandricherthanthedecentralizedpolicyclass.
For example, just considering deterministic policies of horizon H, there would be |A ||O
i|H|I|
i
possibledecentralizedpolicies(assumingallagentshavethesamesizeactionandobservationsets)
vs. |A|OH possible centralized policies. Plugging in 4 actions per agent, 5 observations, a horizon
26(history-length) of 10 and 4 agents gives
45104
= 1.024 × 1043 vs.
(44)(54)10
= 2.56 × 1062520
policies. Thisisamassivedifference.
Asaresult,mostcentralizedpolicieswillnotbedirectlydecentralizeableinthesensethatthey
will be equivalent.15 Policies can only be decentralized when they don’t depend on other agent
information but centralized agents that make joint action choices based on the information of all
agent can often perform much better. In fact, this centralized partially observable case is called a
multi-agentPOMDPandcanbesolvedusingsingle-agentmethods[OliehoekandAmato,2016].
As a result of these issues, approaches that decentralize centralized solutions are not common.
Nonetheless, there are some instances. For example, Li et al. [2023] attempt to reconstruct the
global information using a reconstruction loss, other methods attempt to mimic a centralized con-
troller [Lin et al., 2022], and others allow communication during training but reduce or remove it
during execution [Foerster et al., 2016, Wang et al., 2020]. FOP (for Factorizes the Optimal joint
Policy) [Zhang et al., 2021] makes the assumption that centralized solutions are decentralizeable
andthenseekstolearnsolutionsusingamaxentropyformulationalongwithavaluefactorization
scheme that is similar to methods in Section 3. While FOP (and other methods) could be applied
ingeneralDec-POMDPsettings,theyarelikelytoperformpoorlywhenthecentralizedsolutionis
sufficientlydifferentfromthedecentralizedsolution.
5.3 Topics not discussed
TherearemanyotherissuesthatareimportanttoCTDEbutarenotdiscussedinthistext,including
exploration and communication as well as other topics such as hierarchical methods, role decom-
position, ad-hoc (or zero-shot) coordination, and multi-task approaches. While these (and other)
topicsareveryimportant,Idonotincludethemforthesakeofbrevityandsimplicity.
6 Acknowledgements
IthankAndreaBaisero,ShuoLiu,andtheothermembersofmyLabforLearningandPlanningin
Robotics(LLRP)forreadingmy(very)roughdraftsandprovidingcommentsthathelpedimprove
thedocument.
References
S. V. Albrecht, F. Christianos, and L. Scha¨fer. Multi-Agent Reinforcement Learning: Foundations
andModernApproaches. MITPress,2024. https://www.marl-book.com.
C. Amato. An introduction to decentralized training and execution in cooperative multi-agent
reinforcementlearning. arXivpreprintarXiv:2405.06161,2024.
A.BaiseroandC.Amato. Unbiasedasymmetricreinforcementlearningunderpartialobservability.
InProceedingsoftheInternationalConferenceonAutonomousAgentsandMultiagentSystems,
2022.
15NotethatthisisnottrueinthefullyobservablecaseasdiscussedinSection1.
27B. Banerjee, J. Lyle, L. Kraemer, and R. Yellamraju. Sample bounded distributed reinforcement
learning for decentralized POMDPs. In Proceedings of the National Conference on Artificial
Intelligence,2012.
D. S. Bernstein, R. Givan, N. Immerman, and S. Zilberstein. The complexity of decentralized
control of Markov decision processes. Mathematics of Operations Research, 27(4):819–840,
2002.
C.Boutilier. Planning,learningandcoordinationinmultiagentdecisionprocesses. InProceedings
ofthe6thConferenceonTheoreticalAspectsofRationalityandKnowledge,1996.
L. Bus¸oniu, R. Babusˇka, and B. De Schutter. A comprehensive survey of multi-agent reinforce-
mentlearning. IEEETransactionsonSystems,Man,andCybernetics,PartC:Applicationsand
Reviews,38(2):156–172,Mar.2008.
C. Claus and C. Boutilier. The dynamics of reinforcement learning in cooperative multiagent
systems. In Proceedings of the National Conference on Artificial Intelligence, pages 746–752,
1998.
K. Esslinger, R. Platt, and C. Amato. Deep transformer Q-networks for partially observable rein-
forcementlearning. arXivpreprintarXiv:2206.01078,2022.
J. Foerster, I. A. Assael, N. De Freitas, and S. Whiteson. Learning to communicate with deep
multi-agent reinforcement learning. Advances in Neural Information Processing Systems, 29,
2016.
J. Foerster, N. Nardelli, G. Farquhar, T. Afouras, P. H. Torr, P. Kohli, and S. Whiteson. Stabilising
experience replay for deep multi-agent reinforcement learning. In Proceedings of the Interna-
tionalConferenceonMachineLearning,pages1146–1155,2017.
J. Foerster, R. Y. Chen, M. Al-Shedivat, S. Whiteson, P. Abbeel, and I. Mordatch. Learning with
opponent-learning awareness. In Proceedings of the International Conference on Autonomous
AgentsandMultiagentSystems,2018a.
J. Foerster, G. Farquhar, T. Afouras, N. Nardelli, and S. Whiteson. Counterfactual multi-agent
policygradients. InProceedingsoftheNationalConferenceonArtificialIntelligence,2018b.
C. V. Goldman and S. Zilberstein. Decentralized control of cooperative systems: Categorization
andcomplexityanalysis. JournalofAIResearch,22:143–174,2004.
C. Guestrin, D. Koller, and R. Parr. Multiagent Planning with Factored MDPs. In Advances in
NeuralInformationProcessingSystems,2001.
J. K. Gupta, M. Egorov, and M. Kochenderfer. Cooperative multi-agent control using deep rein-
forcementlearning. InAdaptiveandLearningAgentsWorkshopatAAMAS,2017.
D. Ha, A. M. Dai, and Q. V. Le. Hypernetworks. In Proceedings of the International Conference
onLearningRepresentations,2017.
M. Hausknecht and P. Stone. Deep recurrent Q-learning for partially observable MDPs. arXiv
preprintarXiv:1507.06527,2015.
28Y.-C. Ho. Team decision theory and information structures. Proceedings of the IEEE, 68(6):
644–654,1980.
S. Hochreiter and J. Schmidhuber. Long short-term memory. Neural computation, 9(8):1735–
1780,1997.
S.IqbalandF.Sha. Actor-attention-criticformulti-agentreinforcementlearning. InInternational
conferenceonmachinelearning,2019.
J. Jiang and Z. Lu. I2Q: A fully decentralized q-learning algorithm. In Advances in Neural Infor-
mationProcessingSystems,pages20469–20481,2022.
J.JiangandZ.Lu. BestpossibleQ-learning. arXivpreprintarXiv:2302.01188,2023.
L. P. Kaelbling, M. L. Littman, and A. R. Cassandra. Planning and acting in partially observable
stochasticdomains. ArtificialIntelligence,101(1-2):99–134,1998.
J. R. Kok and N. Vlassis. Collaborative multiagent reinforcement learning by payoff propagation.
JournalofMachineLearningResearch,7:1789–1828,2006.
L. Kraemer and B. Banerjee. Multi-agent reinforcement learning as a rehearsal for decentralized
planning. Neurocomputing,190:82–94,2016.
J. G. Kuba, R. Chen, M. Wen, Y. Wen, F. Sun, J. Wang, and Y. Yang. Trust region policy optimi-
sationinmulti-agentreinforcementlearning. InProceedingsoftheInternationalConferenceon
LearningRepresentations,2022.
M.LauerandM.A.Riedmiller. Analgorithmfordistributedreinforcementlearningincooperative
multi-agent systems. In Proceedings of the International Conference on Machine Learning,
2000.
S. Levine, A. Kumar, G. Tucker, and J. Fu. Offline reinforcement learning: Tutorial, review, and
perspectivesonopenproblems. arXivpreprintarXiv:2005.01643,2020.
D.Li,Z.Xu,B.Zhang,G.Zhou,Z.Zhang,andG.Fan.Fromexplicitcommunicationtotacitcoop-
eration: Anovelparadigmforcooperativemarl. InProceedingsoftheInternationalConference
onAutonomousAgentsandMultiagentSystems,2023.
T.P.Lillicrap,J.J.Hunt,A.Pritzel,N.Heess,T.Erez,Y.Tassa,D.Silver,andD.Wierstra. Contin-
uous control with deep reinforcement learning. In Proceedings of the International Conference
onLearningRepresentations,2016.
A. T. Lin, M. Debord, K. Estabridis, G. Hewer, G. Montufar, and S. Osher. Decentralized multi-
agents by imitation of a centralized controller. In Mathematical and Scientific Machine Learn-
ing,pages619–651,2022.
L.-J. Lin. Self-improving reactive agents based on reinforcement learning, planning and teaching.
Machinelearning,8(3-4):293–321,1992.
M. L. Littman, T. L. Dean, and L. P. Kaelbling. On the complexity of solving Markov decision
problems. InProceedingsofUncertaintyinArtificialIntelligence,1995.
29R. Lowe, Y. I. Wu, A. Tamar, J. Harb, O. Pieter Abbeel, and I. Mordatch. Multi-agent actor-critic
formixedcooperative-competitiveenvironments.InAdvancesinNeuralInformationProcessing
Systems,2017.
X.Lyu,Y.Xiao,B.Daley,andC.Amato.Contrastingcentralizedanddecentralizedcriticsinmulti-
agent reinforcement learning. In Proceedings of the International Conference on Autonomous
AgentsandMultiagentSystems,2021.
X. Lyu, Y. Xiao, A. Baisero, and C. Amato. A deeper understanding of state-based critics in
multi-agent reinforcement learning. In Proceedings of the National Conference on Artificial
Intelligence,2022.
X. Lyu, A. Baisero, Y. Xiao, B. Daley, and C. Amato. On centralized critics in multi-agent rein-
forcementlearning. JournalofAIResearch,77:235–294,2023.
X. Lyu, A. Baisero, Y. Xiao, B. Daley, and C. Amato. On centralized critics in multi-agent rein-
forcementlearning(updatedversion). arXivpreprintarXiv: 2408.14597,2024.
E. Marchesini, A. Baisero, R. Bathi, and C. Amato. On stateful value factorization in multi-agent
reinforcementlearning. arXivpreprintarXiv: 2408.15381,2024.
J.Marschak. Elementsforatheoryofteams. ManagementScience,1:127–137,1955.
V.Mnih,K.Kavukcuoglu,D.Silver,A.A.Rusu,J.Veness,M.G.Bellemare,A.Graves,M.Ried-
miller,A.K.Fidjeland,G.Ostrovski,S.Petersen,C.Beattie,A.Sadik,I.Antonoglou,H.King,
D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis. Human-level control through deep rein-
forcementlearning. Nature,518(7540):529,2015.
K. P. Murphy. A survey of POMDP solution techniques. Technical report, University of British
Columbia,2000.
R. Nair, M. Tambe, M. Yokoo, D. V. Pynadath, and S. Marsella. Taming decentralized POMDPs:
Towardsefficientpolicycomputationformultiagentsettings.InProceedingsoftheInternational
JointConferenceonArtificialIntelligence,pages705–711,2003.
T. Ni, M. Ma, B. Eysenbach, and P.-L. Bacon. When do transformers shine in RL? decoupling
memoryfromcreditassignment. InAdvancesinNeuralInformationProcessingSystems,2023.
C. Nota and P. S. Thomas. Is the policy gradient a gradient? In Proceedings of the International
ConferenceonAutonomousAgentsandMultiagentSystems,2020.
F.A.OliehoekandC.Amato. AConciseIntroductiontoDecentralizedPOMDPs. Springer,2016.
S. Omidshafiei, J. Pazis, C. Amato, J. P. How, and J. Vian. Deep decentralized multi-task multi-
agent reinforcement learning under partial observability. In Proceedings of the International
ConferenceonMachineLearning,2017.
C. H. Papadimitriou and J. N. Tsitsiklis. The complexity of Markov decision processes. Mathe-
maticsofoperationsresearch,12(3):441–450,1987.
30B. Peng, T. Rashid, C. Schroeder de Witt, P.-A. Kamienny, P. Torr, W. Bo¨hmer, and S. Whiteson.
Facmac: Factored multi-agent centralised policy gradients. Advances in Neural Information
ProcessingSystems,2021.
L. Peshkin, K.-E. Kim, N. Meuleau, and L. P. Kaelbling. Learning to cooperate via policy search.
InProceedingsofUncertaintyinArtificialIntelligence,2000.
D. Precup, R. S. Sutton, and S. P. Singh. Eligibility traces for off-policy policy evaluation. In
ProceedingsoftheInternationalConferenceonMachineLearning,2000.
M. L. Puterman. Markov Decision Processes—Discrete Stochastic Dynamic Programming. John
Wiley&Sons,Inc.,1994.
R.Radner. Teamdecisionproblems. AnnalsofMathematicalStatistics,33:857–881,1962.
T.Rashid,M.Samvelyan,C.Schroeder,G.Farquhar,J.Foerster,andS.Whiteson. QMIX:Mono-
tonic value function factorisation for deep multi-agent reinforcement learning. In Proceedings
oftheInternationalConferenceonMachineLearning,2018.
T. Rashid, G. Farquhar, B. Peng, and S. Whiteson. Weighted QMIX: Expanding monotonic value
function factorisation for deep multi-agent reinforcement learning. In Advances in Neural In-
formationProcessingSystems,volume33,2020a.
T. Rashid, M. Samvelyan, C. S. De Witt, G. Farquhar, J. Foerster, and S. Whiteson. Monotonic
value function factorisation for deep multi-agent reinforcement learning. Journal of Machine
LearningResearch,21(1):7234–7284,2020b.
M. Samvelyan, T. Rashid, C. S. de Witt, G. Farquhar, N. Nardelli, T. G. J. Rudner, C.-M. Hung,
P.H.S.Torr,J.Foerster,andS.Whiteson. TheStarCraftMulti-AgentChallenge. arXivpreprint
arXiv:1902.04043,2019.
J.Schneider,W.-K.Wong,A.Moore,andM.Riedmiller. Distributedvaluefunctions. InProceed-
ingsoftheInternationalConferenceonMachineLearning,1999.
J. Schulman, S. Levine, P. Abbeel, M. Jordan, and P. Moritz. Trust region policy optimization. In
ProceedingsoftheInternationalConferenceonMachineLearning,2015.
J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov. Proximal policy optimization
algorithms. arXivpreprintarXiv:1707.06347,2017.
D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller. Deterministic policy
gradient algorithms. In Proceedings of the International Conference on Machine Learning,
2014.
K. Son, D. Kim, W. J. Kang, D. E. Hostallero, and Y. Yi. QTRAN: Learning to factorize with
transformation for cooperative multi-agent reinforcement learning. In Proceedings of the Inter-
nationalConferenceonMachineLearning,2019.
K.Su,S.Zhou,J.Jiang,C.Gan,X.Wang,andZ.Lu. MA2QL:Aminimalistapproachtofullyde-
centralized multi-agent reinforcement learning. In Proceedings of the International Conference
onAutonomousAgentsandMultiagentSystems,2024.
31P. Sunehag, G. Lever, A. Gruslys, W. M. Czarnecki, V. Zambaldi, M. Jaderberg, M. Lanctot,
N. Sonnerat, J. Z. Leibo, K. Tuyls, and T. Graepel. Value-decomposition networks for coopera-
tivemulti-agentlearning. arXiv:1706.05296,2017.
R. S. Sutton and A. G. Barto. Reinforcement Learning: An Introduction (second edition). The
MITPress,2018.
A. Tampuu, T. Matiisen, D. Kodelja, I. Kuzovkin, K. Korjus, J. Aru, J. Aru, and R. Vicente.
Multiagent cooperation and competition with deep reinforcement learning. PloS one, 12(4),
2017.
J. K. Terry, N. Grammel, S. Son, B. Black, and A. Agrawal. Revisiting parameter sharing in
multi-agentdeepreinforcementlearning. arXivpreprintarXiv:2005.13625,2020.
J.Wang,Z.Ren,T.Liu,Y.Yu,andC.Zhang. QPLEX:Duplexduelingmulti-agentQ-learning. In
ProceedingsoftheInternationalConferenceonLearningRepresentations,2021a.
T. Wang, J. Wang, C. Zheng, and C. Zhang. Learning nearly decomposable value functions via
communicationminimization. InProceedingsoftheInternationalConferenceonLearningRep-
resentations,2020.
Y. Wang, B. Han, T. Wang, H. Dong, and C. Zhang. DOP: Off-policy multi-agent decomposed
policy gradients. In Proceedings of the International Conference on Learning Representations,
2021b.
Z. Wang, T. Schaul, M. Hessel, H. Hasselt, M. Lanctot, and N. Freitas. Dueling network archi-
tectures for deep reinforcement learning. In Proceedings of the International Conference on
MachineLearning,2016.
C.J.C.H.WatkinsandP.Dayan. Q-learning. MachineLearning,8(3):279–292,May1992.
T. Willi, A. H. Letcher, J. Treutlein, and J. Foerster. COLA: Consistent learning with opponent-
learningawareness. InProceedingsoftheInternationalConferenceonMachineLearning,2022.
Y. Yang, J. Hao, B. Liao, K. Shao, G. Chen, W. Liu, and H. Tang. Qatten: A general framework
forcooperativemultiagentreinforcementlearning. arXivpreprintarXiv:2002.03939,2020.
C. Yu, A. Velu, E. Vinitsky, J. Gao, Y. Wang, A. Bayen, and Y. Wu. The surprising effectiveness
ofPPOincooperativemulti-agentgames. AdvancesinNeuralInformationProcessingSystems,
35,2022.
T. Zhang, Y. Li, C. Wang, G. Xie, and Z. Lu. FOP: Factorizing optimal joint policy of maximum-
entropy multi-agent reinforcement learning. In Proceedings of the International Conference on
MachineLearning,2021.
32