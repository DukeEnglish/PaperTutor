Can LLMs Generate Novel Research Ideas?
ALarge-ScaleHumanStudywith100+NLPResearchers
ChengleiSi,DiyiYang,TatsunoriHashimoto
StanfordUniversity
{clsi, diyiy, thashim}@stanford.edu
Abstract
Recentadvancementsinlargelanguagemodels(LLMs)havesparkedoptimismabouttheirpotentialto
acceleratescientificdiscovery,withagrowingnumberofworksproposingresearchagentsthatautonomously
generateandvalidatenewideas.Despitethis,noevaluationshaveshownthatLLMsystemscantakethevery
firststepofproducingnovel,expert-levelideas,letaloneperformtheentireresearchprocess.Weaddressthisby
establishinganexperimentaldesignthatevaluatesresearchideagenerationwhilecontrollingforconfounders
andperformsthefirsthead-to-headcomparisonbetweenexpertNLPresearchersandanLLMideationagent.
Byrecruitingover100NLPresearcherstowritenovelideasandblindreviewsofbothLLMandhumanideas,
weobtainthefirststatisticallysignificantconclusiononcurrentLLMcapabilitiesforresearchideation: we
findLLM-generatedideasarejudgedasmorenovel(p<0.05)thanhumanexpertideaswhilebeingjudged
slightlyweakeronfeasibility.Studyingouragentbaselinesclosely,weidentifyopenproblemsinbuildingand
evaluatingresearchagents,includingfailuresofLLMself-evaluationandtheirlackofdiversityingeneration.
Finally,weacknowledgethathumanjudgementsofnoveltycanbedifficult,evenbyexperts,andpropose
anend-to-endstudydesignwhichrecruitsresearcherstoexecutetheseideasintofullprojects,enablingusto
studywhetherthesenoveltyandfeasibilityjudgementsresultinmeaningfuldifferencesinresearchoutcome.1
1 Introduction
TherapidimprovementofLLMs,especiallyincapabilitieslikeknowledgeandreasoning,hasenabled
manynewapplicationsinscientifictasks,suchassolvingchallengingmathematicalproblems(Trinh
etal.,2024),assistingscientistsinwritingproofs(Collinsetal.,2024),retrievingrelatedworks(Ajith
etal.,2024,Pressetal.,2024),generatingcodetosolveanalyticalorcomputationaltasks(Huangetal.,
2024,Tianetal.,2024),anddiscoveringpatternsinlargetextcorpora(Lametal.,2024,Zhongetal.,
2023).Whiletheseareusefulapplicationsthatcanpotentiallyincreasetheproductivityofresearchers,
itremainsanopenquestionwhetherLLMscantakeonthemorecreativeandchallengingpartsof
theresearchprocess.
WefocusonthisproblemofmeasuringtheresearchideationcapabilitiesofLLMsandask:arecurrent
LLMscapableofgeneratingnovelideasthatarecomparabletoexperthumans?Althoughideation
isonlyonepartoftheresearchprocess,thisisakeyquestiontoanswer,asitistheveryfirststeptothe
scientificresearchprocessandservesasalitmustestforthepossibilityofautonomousresearchagents
thatcreatetheirownideas.Evaluatingexpert-levelcapabilitiesofLLMsystemsischallenging(Bakhtin
1Interestedresearcherscansignupforthisend-to-endstudyat:https://tinyurl.com/execution-study.We
releaseouragentimplementationandallhumanreviewscoresat:https://github.com/NoviScl/AI-Researcher.
∗Thelasttwoauthorsadvisedthisprojectequally.
1
4202
peS
6
]LC.sc[
1v90140.9042:viXraIdea Generation Blind Review by Experts (N=79)
7 NLP Human
Condition 1 : Human Ideas (N=49) Novelty Score: 4.84
Topics

 Experts
Bias

Coding

Condition 2 : AI Ideas (N=49) Novelty Score: 5.64
Safety

Multilingual

Factuality
 AI
Math

Agent
Uncertainty Condition 3 : AI Ideas + Human Rerank (N=49) Novelty Score: 5.81
Figure1:Overviewofourstudy:werecruit79expertresearcherstoperformblindreviewof49ideas
fromeachofthethreeconditions:expert-writtenideas,AI-generatedideas,andAI-generatedideas
rerankedbyahumanexpert.Westandardizetheformatandstyleofideasfromallconditionsbefore
theblindreview.WefindAIideasarejudgedassignificantlymorenovelthanhumanideas(p<0.05).
Novelty Excitement Feasibility Effectiveness Overall
7 7 7 7 7
*
6 * 6 * 6 6 6 *
*
5 5 5 5 5
4 4 4 4 4
3 3 3 3 3
Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank
Figure2: Comparisonofthethreeexperimentconditionsacrossallreviewmetrics. Redasterisks
indicatethattheconditionisstatisticallybetterthantheHumanbaselinewithtwo-tailedWelch’st-tests
andBonferronicorrection.Allscoresareona1to10scale.MoredetailedresultsareinSection5.
etal.,2022,Collinsetal.,2024),andresearchideationtakesthistoanextreme. Qualifiedexpertre-
searchersaredifficulttorecruitatscale,evaluationcriteriacanbehighlysubjective,anditisdifficult
foreventhebestexpertstojudgethequalityofanidea(Beygelzimeretal.,2021,Simseketal.,2024).
Weaddressthesechallengesdirectly,recognizingthatforimportant,high-stakestaskslikeresearch
ideation,thereisnosubstituteforalarge-scaleexpertevaluation.Wedesignacarefullycontrolled
comparisonofhumanandLLMideasthatovercomessamplesizeandbaselineproblemspresent
inearliersmall-scaleevaluationstudies.Ourstudyrecruitedalargepoolofover100highlyqualified
NLPresearcherstoproducehumanbaselineideasandperformblindreviewsofhumanandLLM
ideas.Toreducethepossibilitythatconfoundingvariablesaffectouroutcomemeasures,weenforce
strictcontrolsthatstandardizethestylesofhumanandLLMideasandmatchtheirtopicdistribution.
WecompareourhumanexpertbaselinewithasimpleandeffectiveLLMagentthatincorporates
retrievalaugmentationandadoptsrecentideasininference-timescaling,suchasovergeneratingand
rerankingLMoutputs.Thesemeasuresallowustomakestatisticallyrigorouscomparisonsbetween
humanexpertsandstate-of-the-artLLMs(Figure1).
2
erocSOurevaluation-centricapproachcomplementsmanyrecentmethods-centricworksthatattemptto
instantiateresearchagents(Baeketal.,2024,Lietal.,2024,Luetal.,2024,Wangetal.,2024,Yangetal.,
2024).Themajorityoftheseworksrelyonfastandlower-costevaluationsurrogates–eitherbydecreas-
ingthenumberofexpertreviewers(Baeketal.,2024,Lietal.,2024,Wangetal.,2024,Yangetal.,2024),
constrainingthelengthanddetailednessoftheideas(Wangetal.,2024,Yangetal.,2024),orrelyingon
LLM-as-a-judge(Luetal.,2024).Theydonotperformthelarge-scalehumancomparisonstudiesthat
areneededtoanswerthemotivatingquestionofourwork.Ourworktakestheoppositeapproach,
performingayear-longandhigh-costevaluationthatprovideshumanexpertbaselinesandastan-
dardizedevaluationprotocoltoserveasafoundationforfuturefollow-upstudiesandmethodswork.
Throughnearly300reviewsacrossallourconditions,wefindthatAI-generatedideasarejudged
asmorenovelthanhumanexpertideas(p<0.05),whichholdsrobustlyundermultiplehypothesis
correctionandacrossdifferentstatisticaltests. Wefindsomesignsthatthesegainsarecorrelated
withexcitementandoverallscore,andmaycomeattheslightexpenseoffeasibility,butourstudy
sizedidnothavesufficientpowertoconclusivelyidentifytheseeffects(Figure2).
Qualitativeanalysisoffree-textresponsesinourreviewcorroboratesthesefindingsonnoveltyand
feasibility.Apartfromevaluatingtheideas,wealsoanalyzetheLLMagent,showinglimitationsand
openproblems–despiteexcitementaboutinference-timescalingofLLMs,wefindthattheylackidea
diversitywhenwescaleupideageneration,andtheycannotcurrentlyserveasreliableevaluators.
2 ProblemSetup
Thecentralexperimentofourworkisacomparisonofhuman-andLLM-generatedideas. While
thisgoalissimple,thereisnoexistingconsensusonhowtoformulatethetaskofresearchideation
andevaluation,andwebeginbydefiningthekeyaspectsofourexperimentdesign.
Wethinkofresearchideaevaluationasconsistingofthreeseparatecomponents: 1). theideaitself,
generatedinresponsetoourinstructions,2).thewriteupwhichcommunicatestheidea,and3).the
evaluationofthewriteupbyexperts.Weoutlineourexperimentdesignineachofthesethreeparts
withparticularfocusonpotentialconfounders,suchastheareaofresearch,theformatofaresearch
idea,andtheevaluationprocess.
IdeationScopeandInstructions Researchideascantakemanydifferentforms.Theycanbesimple
trickstoimprovemodelperformance,ortheymaybelarge-scaleresearchprogramsthatformthe
basis of a Ph.D. thesis. Any experiment on ideation must carefully balance the realisticness and
interestingnessofaresearchideawiththepracticalrealitiesofelicitingideasfromalargepopulation.
Inourcase,thesetradeoffsareevenmorepronounced,aswehavedesignedourideationexperiments
sothattheresultingideascanbeexecutedbyexpertsinafollow-upsetofexperiments.
Theseconstraintshaveledustostudyprompting-basedNLPresearchasatestbedforourstudy.
PromptingresearchhasbeenpopularinrecentyearsofNLPandAIresearch(e.g.,Chenetal.,2023,
Diaoetal.,2024,Madaanetal.,2023,Qinetal.,2024,Schulhoffetal.,2024,Sietal.,2023,Wangetal.,
2023,Weietal.,2022,Yaoetal.,2023,Yasunagaetal.,2024,Zhouetal.,2023,interalia). Thisclass
of projects strikes a reasonable trade-off among our constraints. The most impactful prompting
projectslikechain-of-thoughthavehadamajorinfluenceonLLMperformance(Weietal.,2022),and
promptingprojectsareexecutablewithminimalcomputinghardware.
Wefurtherstructureourideationprocesstoavoidselection-bias-basedconfoundersinideation.If
wesimplyaskLLMsandhumanstoproduceideason‘promptingtopics’,wemayfindthatLLMs
andhumansdifferinthetypesofresearchideastheyproduce(forexample,LLMsmaynaturally
suggestmoreprojectsonsafertopics,whichmightbejudgedaslessexcitingbyhumans).Thiswould
3leadustosimplymeasuremisalignmentinresearchtopicpreferencebetweenLLMsandhumans,
whichisnotthegoalofourstudy.Toaddressthispossibility,wedefineasetofsevenspecificresearch
topicsextractedfromtheCallForPaperspageofrecentNLPconferencessuchasCOLM.2Specifically,
our topics include: Bias, Coding, Safety, Multilinguality, Factuality, Math, and Uncertainty (see
AppendixAforacompletedescriptionofthesetopics).
EachhumanandLLMparticipantoftheideationexperimentreceivesthesamesetofnaturallanguage
instructions including the same topic description, idea template, and demonstration example to
ensureafaircomparison.Forhumanparticipants,weadditionallyallowthemtoselectapreferred
topicfromthelist,andforeachselectedtopic,wegenerateacorrespondingLLMidea.Thisexactly
matchestheideatopicdistributionbetweentheLLMandhumanparticipants,whileensuringthat
humanexpertsareabletoselecttopicsaccordingtotheirexpertise.
IdeaWriteup Anideacanonlybeevaluatedifitiswrittenuptobecommunicated,butthiswriting
processintroducesmanyadditionalpotentialconfounders.Humanresearchersmaywriteinways
thatsubtlysignalqualityresearch,suchasincludingmoreexamplesandimplementationdetails.
Theformatofthewriteupfunctionsasawaytoscaffoldwhatcontentsshouldbeincludedandthe
levelofdetailedness.Ideally,wewantbothhumanandLLMparticipantstoprovideallthenecessary
implementationdetailsfortheirgeneratedideas.
Wetakeinspirationfromguidelinesusedingrantsubmissionsandintroduceatemplatetospecifythe
structureanddetailednessofideaproposals.Specifically,weconstructatemplatethatincludesfields
forthetitle,problemstatement,motivation,proposedmethod,step-by-stepexperimentplan,test
caseexamples,andthefallbackplan.BoththeLLMagentandthehumanideawritersareinstructed
tofollowthistemplateandourprovideddemonstrationexamplestoproduceaprojectproposalas
theoutput(seeAppendixBforthefulltemplateandAppendixCforthedemoexample).
Evenwiththesetemplates,theremaybesubtlewritingstylecuesthataffecttheoutcomemeasure.For
example,humansmaytendtowriteinamoreengagingandinformaltone.Toreducethispossibility
further,wedevelopedastylenormalizationmodulethatusesanLLMtoconvertallideasintothe
samewritingandformattingstylewithoutchangingtheoriginalcontent. Oursmall-scalehuman
studyshowsthatsuchanormalizationapproachleadstoa50%accuracyforexperthumanjudges
whoareaskedtodistinguishAIideasfromhumanideas.Finally,theuseofanLLMstyleanonymizer
hasthepossibilityofsubstantivelychangingthecontentoftheideas.Torulethisout,thefirstauthor
ofthispapermanuallyverifiedeachhumanideaproposaltoensureallcontentsoftheoriginalideas
werepreserved.WepresentthefullpromptusedinAppendixD.
ReviewandEvaluation Reviewingresearchideasisnotoriouslysubjective,sowewanttodesign
areviewformthatdefinesallreviewcriteriaclearlytostandardizeandanchortheevaluationsas
muchaspossible.Atthesametime,wewantourreviewcriteriaandmeasuredvariablestocapture
allthedesiderataofhigh-qualityresearchideas.
WefollowbestpracticesfromAIconferencereviewing(e.g.,ICLRandACL)whendesigningthe
reviewform,wherewedefinefourbreakdownmetricsincludingnovelty,excitement,feasibility,and
expectedeffectiveness,apartfromtheoverallscore. Foreachmetric,weaskforanumericalscore
ona1-10scalealongwithafree-textrationale.Weprovidecleardefinitionsandgroundingforeach
numericalscaletocalibrateallreviewers’standards(seeAppendixEforthefullreviewform).
Ourblindreviewevaluationwillcompareideasfromthreedifferentconditions:
1. Human Ideas:Ideaproposalswrittenbyourrecruitedexpertresearchers.
2https://colmweb.org/cfp.html
42. AI Ideas:IdeaproposalsgeneratedbyourLLMagent.Wedirectlytakethetop-rankedideas
fromtheagent’soutput.
3. AI Ideas + Human Rerank:IdeaproposalsgeneratedbyourLLMagent.Thefirstauthorof
thispapermanuallyselectedthetop-rankedideasoutofalltheLLMagent’sgenerationsrather
thanrelyingontheLLMrankerinordertobetterestimatetheupper-boundqualityofAIideas.
Inthenexttwosections, weinstantiatehowourLLMagentgeneratesideasandhowourexpert
participantsgenerateandreviewtheideas.
3 IdeaGenerationAgent
We build a simple but effective LLM ideation agent to compare with the human expert baseline.
Rather than focusing on innovating the agent itself, we adhere to a minimalist design principle,
aimingtounderstandthecurrentcapabilitiesofLLMsinideageneration.Ourresearchideationagent
hasthreeessentialcomponents: paperretrieval,ideageneration,andidearanking,whichwewill
describeindetailbelow.
3.1 PaperRetrievalforRAG
To ground idea generation, the agent needs to retrieve papers related to the given research
topic, so that it will be aware of related works when generating new ideas. To do so, we lever-
age retrieval-augmented generation (RAG), which has demonstrated effectiveness on many
knowledge-intensivetasks (Lewisetal.,2020,Shietal.,2024). Concretely,givenaresearchtopic
(e.g., “novel prompting methods that can improve factuality and reduce hallucination of large
language models"), we prompt an LLM to generate a sequence of function calls to the Semantic
ScholarAPI.Weuseclaude-3-5-sonnet-20240620asthebackbonemodelforouragentbut
the pipeline should generalize to other LLMs as well. The paper retrieval action space includes:
{KeywordQuery(keywords), PaperQuery(paperId), GetReferences(paperId)}. Each
actiongenerationisgroundedonthepreviousactionsandexecutedresults.Wekeepthetopk=20
papersfromeachexecutedfunctioncallandstoptheactiongenerationwhenamaxofN=120papers
havebeenretrieved. WethenusetheLLMtoscoreandrerankallretrievedpapersbasedonthree
criteria: 1)thepapershouldbedirectlyrelevanttothespecifiedtopic; 2)thepapershouldbean
empiricalpaperinvolvingcomputationalexperiments;33)thepaperisinterestingandcaninspire
newprojects.TheLLMispromptedtoscoreeachretrievedpaperonascaleof1to10basedonthese
criteriaandweusethetop-rankedpapersforthenextstepofideageneration.
3.2 IdeaGeneration
Ourkeyinsightforideagenerationistogenerateasmanycandidateideasaspossible.Ourintuition
isthatonlyasmallfractionofallgeneratedideasmightbehigh-quality,andweshouldbewilling
toexpendinference-timecomputetogeneratemorecandidatessothatwecanlaterusearerankerto
discoverthe"diamondintherough".Thisalignswithexistingresultsshowingthatscalinginference
compute with repeated sampling can boost LLM performance on various coding and reasoning
tasks (Brown et al., 2024, Li et al., 2022). Specifically, we prompt the LLM to generate 4000 seed
ideasoneachresearchtopic.Theideagenerationpromptincludesthedemonstrationexamplesand
theretrievedpapers. Wecraftk=6demonstrationexamplesbymanuallysummarizingexemplar
3Notethatweexcludepositionpapers,surveypapers,andanalysispapersthroughoutthisstudysincetheirevaluation
tendstobeverysubjective.
5papers(Dhuliawalaetal.,2023,Madaanetal.,2023,Welleretal.,2023,WestonandSukhbaatar,2023,
Yasunagaetal.,2024,Zhengetal.,2024)intoourdesiredideaformat. Forretrievalaugmentation,
werandomlyselectk=10papersfromthetop-rankedretrievedpapersandconcatenatetheirtitles
andabstractstoprependtotheideagenerationprompt.Wealsoappendthetitlesofallpreviously
generatedideastotheprompttoexplicitlyasktheLLMtoavoidrepetitions.
Toremoveduplicatedideasfromthislargepoolofcandidateideas,wefirstperformaroundofdedu-
plicationbyencodingallseedideaswithall-MiniLM-L6-v2fromSentence-Transformers(Reimers
andGurevych,2020)andthencomputingpairwisecosinesimilarities.Wesetasimilaritythreshold
of0.8fortheideadeduplicationbasedonmanualinspection.4 Thisleavesabout5%non-duplicated
ideasoutofallthegeneratedseedideas.WeexpandmoreonthisduplicationissuelaterinSection7.1.
3.3 IdeaRanking
Thenextstepisforourideationagenttorankalltheremainingideassothatwecanfindthebest
onesamongthem. Tobuildsuchanautomaticidearanker,weusepublicreviewdataasaproxy.
Specifically,wescraped1200ICLR2024submissionsrelatedtoLLMs(withkeywordfiltering)along
with their review scores and acceptance decisions. We explored multiple ways of predicting the
scoresanddecisionsofthesesubmissionsandfoundthatLLMsarepoorlycalibratedwhenasked
directlytopredictthefinalscoresordecisions,butcanachievenon-trivialaccuracywhenaskedto
judgewhichpaperisbetterinpairwisecomparisons.
We converted the ICLR submissions into our stan-
dard project proposal format and randomly paired N Top-10 Bottom-10 Gap
up accepted and rejected papers and asked LLMs 1 6.28 5.72 0.56
to predict which one is accepted. On this task, 2 6.14 5.24 0.90
Claude-3.5-Sonnet achieves an accuracy of 71.4% 3 5.83 4.86 0.97
with zero-shot prompting. For comparison, GPT-4o 4 5.94 4.99 0.95
achieves 61.1% and Claude-3-Opus achieves 63.5%, 5 6.42 4.69 1.73
and we do not observe significant gains from addi- 6 6.11 4.81 1.30
tional prompting techniques like few-shot or chain- Table 1: Average ICLR review scores of
of-thought prompting. We therefore choose the top-andbottom-10papersrankedbyour
Claude-3.5-Sonnetzero-shotranker.
LLMranker,withdifferentrounds(N)of
Inordertoobtainreliablescoresforallprojectproposals pairwisecomparisons.
basedonpairwisecomparisons,weadoptaSwisssystem
tournamentwhereallprojectproposalsarepairedwith
thosewhoseaccumulatedscoresaresimilar,andiftheproposalsarejudgedtobebetter,theygainan
additionalpoint.WerepeatthisforN roundssothetotalscoreofeachprojectproposalwillbewithin
the[0,N]range.Asasanitycheck,weusetheClaude-3.5-Sonnetrankertorankthe1.2KICLR
LLM-relatedsubmissionsandcomparetheaveragereviewscoresofthetop10rankedpapersandthe
bottom10rankedpapersinTable1. Weseeaclearseparationbetweenthetopandbottomranked
papers,indicatingtheeffectivenessoftheLLMranker.WechooseN=5forallourexperimentssince
itgivesthebestrankingresultonthisvalidationset.Thetop-rankedprojectproposalsfromtheagent
willbedirectlyusedfortheAI Ideasconditionofthehumanstudy.
SinceourAIrankerisstillfarfromperfect,wealsointroduceanotherexperimentconditionwhere
thefirstauthorofthispapermanuallyrerankedthegeneratedprojectproposalsinsteadofrelying
ontheLLMranker, andwecallthistheAI Ideas + Human Rerankcondition. Asweshowin
4WeproviderandomlysampledideapairsandtheirsimilaritiesinAppendixH.Wealsoprovideadditionalimplementa-
tiondetailsabouttheideationagentinAppendixF.
6Table12,17outofthe49ideasintheAI Ideas + Human RerankconditionoverlapwiththeAI
Ideascondition,whiletheother32aredifferent,indicatingthediscrepancybetweentheLLMranker
andthehumanexpertreranking.
4 ExpertIdeaWritingandReviewing
Inthissection,weshiftfocustothehumanbranchofideagenerationcomparison. Wepresentthe
details of our human study, including information about the recruited experts, the human idea
generationtask,andthesubsequentreviewprocess.
4.1 ExpertRecruitment
Werecruitourexpertparticipants(includingforideawritingandreviewing)bysendingsign-up
formstoseveralchannels,including:1)theOpenNLPSlackchannelwith1426NLPresearchersfrom
71institutions(withconsentfromthechannelmanager);2)Twitter(X);3)Slackchannelsofvarious
NLPgroupsbydirectcommunicationwiththegroupmembers;and4)officialchatappoftheNAACL
2024conference. Wealsoconductedin-personrecruitmentbygivingoutnamecardsandwearing
T-shirts5withsign-uplinksattheNAACL2024conferenceaswellasvariousotherlocalNLPsocial
events.OurstudyhasbeenapprovedbytheStanfordIRB(ID74246).
WeperformedscreeningonalltheUSparticipants6basedontheirprovidedGoogleScholarprofiles.
WesetaminimumrequirementofhavingpublishedatleastonepaperatamajorAIvenue.7 We
reachedouttoallparticipantswhosatisfiedthisrequirementwiththeconsentformandfollowedup
withtheannotationdocumentsforthosewhoconsentedtoparticipate.
Intheend,werecruitedN=49expertsforwritingideas,andN=79expertsforreviewingideas.Note
that24outofthe79reviewersalsoparticipatedintheideawriting,andwemadesurenoreviewer
wouldreviewtheirownidea. ThisresultsinN =104totalparticipantsacrossthetwotasks. Each
ideawriterisaskedtowriteoneideawithin10daysandwecompensate$300foreach,witha$1000
bonusforthetop5ideasasscoredbytheexpertreviewers.Eachideareviewerisassigned2to7ideas
toreviewandwecollectedN=298uniquereviewsintotal. Theyaregivenoneweektofinishthe
reviewsandwecompensated$25foreachreviewwrittenbytheideareviewers.
4.2 ExpertQualifications
Our pool of participants is highly qualified and diverse.
The49ideawriterscomefrom26differentinstitutions(Ta-
Other Postdoc
ble15)andthemajorityofthemarecurrentPhDstudents
Other
8% 8%
(Figure3left).The79reviewerscomefrom32institutions Master 5% Master
18% 6%
(Table16)andaremostlyPhDstudentsandPostdocs(Fig-
ure3right).WeusetheirGoogleScholarprofilestoextract
several proxy metrics, including the number of papers, 73% 79%
PhD
citations,h-index,andi10-indexatthetimeoftheirsubmis- PhD
sion.Table2showsthatourideawritershaveanaverage
of12papersand477citations,whileeveryreviewerhas Figure 3: Positions of our idea writer
publishedatleasttwopapersandhasanaveragecitation (left)andreviewer(right)participants.
of635andh-indexof7. Moreover,basedontheirsurvey
5https://x.com/ChengleiSi/status/1804273510656749649
6WehavetorecruitparticipantslocatedintheUSduetologisticalreasons.
7E.g.,*ACL,NeurIPS,ICLR,ICML,AAAI.
7IdeaWritingParticipants(N=49) IdeaReviewingParticipants(N=79)
Metric Mean Median Min Max SD Mean Median Min Max SD
papers 12 10 2 52 9 15 13 2 52 10
citations 477 125 2 4553 861 635 327 0 7276 989
h-index 5 4 1 21 4 7 7 0 21 4
i10-index 5 4 0 32 6 7 5 0 32 6
Table2:Researchprofilemetricsoftheideawritingandreviewingparticipants.Dataareextracted
fromGoogleScholaratthetimeofideaorreviewsubmission.
Metric Mean Median Min Max SD
HumanIdeas
Familiarity(1-5) 3.7 4.0 1.0 5.0 1.0
Difficulty(1-5) 3.0 3.0 1.0 5.0 0.7
Time(Hours) 5.5 5.0 2.0 15.0 2.7
Length(Words) 901.7 876.0 444.0 1704.0 253.5
AIIdeas
Length(Words) 1186.3 1158.0 706.0 1745.0 233.7
AI + Human RerankIdeas
Length(Words) 1174.0 1166.0 706.0 1708.0 211.0
Table3:Statisticsofthe49ideasfromeachcondition.
responses,72outofthe79reviewershavepreviouslyreviewedformajorAIconferencesorjournals.
These statistics indicate that our participants are highly qualified and have substantial research
experience.8
4.3 IdeaWriting
Wereportstatisticsofourideawriters’ideastomeasuretheir
quality.AsshowninTable3,ideawritersindicateamoderately Topic Count
highfamiliaritywiththeirselectedtopic(3.7ona1to5scale), Bias 4
andindicatethetaskasmoderatelydifficult(3ona1to5scale). Coding 9
Theyspentanaverageof5.5hoursonthetaskandtheirideas Safety 5
are902wordslongonaverage.Theseindicatethatparticipants Multilingual 10
areputtingsubstantialeffortintothistask.9 Wealsoshowthe Factuality 11
distributionoftheirselectedtopicsinTable4. Math 4
Uncertainty 6
Total 49
4.4 IdeaReviewing
Table4:Ideatopicdistribution.
ReviewAssignmentWeletallreviewerparticipantsselecttheir
toptwopreferredtopicsaswellastheirpreferredreviewing
load(from2to7).Wethenrandomlyassignthemtoideaswithin
theirselectedtopicsandallideasareanonymized.Intheassignment,webalancethenumberofideas
fromeachconditionforeachreviewerandensurethateachreviewergetsatleastonehumanidea
andoneAIidea.Everyideaisreviewedby2to4differentreviewers.Wealsoavoidassigningideas
writtenbyauthorsfromthesameinstitutiontoavoidanypotentialcontamination.Table5showsthat
eachreviewerwroteanaverageof3.8reviewsfrom2or3conditions,across1to3topics.
8DetailedbreakdownofparticipantpositionsisinAppendixK.
9SeeAppendixJformoredetailsonthequalitycontrolofhumanideas.
8Metric Mean Median Min Max SD
Ours
Familiarity(1-5) 3.7 3.0 1.0 5.0 0.9
Confidence(1-5) 3.7 4.0 1.0 5.0 0.7
Time(Minutes) 31.7 30.0 5.0 120.0 16.8
Length(Word) 231.9 208.0 41.0 771.0 112.1
ICLR2024
Confidence(1-5) 3.7 4.0 1.0 5.0 0.8
Length(Word) 421.5 360.0 14.0 2426.0 236.4
Length(Word;Strengths&Weaknesses) 247.4 207.0 2.0 2010.0 176.4
Table6:Statisticsofourcollectedreviews,withICLR2024reviewsasabaseline(forthe1.2Ksubmis-
sionsthatmentionedthekeyword“languagemodels").
ReviewQualityCheckApartfromensuringreviewer
Metric Mean Min Max SD
qualifications,wealsocomputestatisticstomeasure #Reviews 3.8 2.0 7.0 1.3
thequalityofthereviewsinTable6.Onaverage,the #Conditions 2.5 2.0 3.0 0.5
reviewersindicatedafamiliarityof3.7(outof5)in #Topics 1.5 1.0 3.0 0.6
theirselectedtopicandaconfidenceof3.7(outof5)in
Table5:Statisticsofthereviewassignment.
theirreviews.Thisiscomparablewiththe1.2KICLR
2024submissionsrelatedtolanguagemodels,where
thereviewersalsohaveanaverageconfidenceof3.7outof5.Moreover,reviewersspentanaverageof
32minutesoneachreview,witheachreviewbeingabout232wordslong.
SinceourreviewformsaredifferentfromtheICLRreviewforms,wecomparethemwiththeICLR
reviewswhereweremovethesummaryandquestionsectionsandonlycountthelengthsofthe
strengthsandweaknessessections.Thisway,theICLRreviewshaveanaveragelengthof247,similar
toourcollectedreviews.Asanadditionalmeasureofreviewquality,outofthe298uniquereviews
thatwehavecollected,80ofthemprovidedlinkstoexistingpapersintheirrationalestojustifywhy
theproposedmethodisnotnovel.Theseresultsfurthervalidatethehighqualityofourreviewdata.
5 MainResult: AIIdeasAreRatedMoreNovelThanExpertIdeas
Inthissection,wepresentourmainfindingonwhetherLLMscangeneratebetterresearchideasthan
experts.Consistentlyacrossthreedifferentstatisticaltestsaccountingforthepossibleconfounders,
wefindthatAIideashavehighernoveltyscoresthanhumanideaswhilebeingcomparableonall
othermetrics.
5.1 Test1: TreatingEachReviewasanIndependentDatapoint
InTest1,wetreateachreviewasanindependentdatapointandaggregateallreviewsfromthesame
condition.WetreattheHuman IdeasasthebaselineconditionandcompareitwithAI IdeasandAI
Ideas + Human Rerankusingtwo-tailedWelch’st-testswithBonferronicorrection.Weshowthe
barplotinFigure2andthedetailednumericalresultsinTable7.BothAI Ideas(µ=5.64±σ=1.76)
andAI Ideas + Human Rerank(µ=5.81±σ=1.66)aresignificantlybetterthanHuman Ideas
(µ = 4.84±σ = 1.79) on the novelty score (p < 0.01). In this particular test, the AI ideas in both
conditionsarealsosignificantlybetterthanhumanideasontheexcitementscore(p<0.05),andthe
AI Ideas + Human RerankconditionisalsosignificantlybetterthanHuman Ideasintermsof
9Condition Size Mean Median SD SE Min Max p-value
NoveltyScore
Human Ideas 119 4.84 5 1.79 0.16 1 8 –
AI Ideas 109 5.64 6 1.76 0.17 1 10 0.00**
AI Ideas + Human Rerank 109 5.81 6 1.66 0.16 2 10 0.00***
ExcitementScore
Human Ideas 119 4.55 5 1.89 0.17 1 8 –
AI Ideas 109 5.19 6 1.73 0.17 1 9 0.04*
AI Ideas + Human Rerank 109 5.46 6 1.82 0.17 1 9 0.00**
FeasibilityScore
Human Ideas 119 6.61 7 1.99 0.18 1 10 –
AI Ideas 109 6.34 6 1.88 0.18 2 10 1.00
AI Ideas + Human Rerank 109 6.44 6 1.63 0.16 1 10 1.00
ExpectedEffectivenessScore
Human Ideas 119 5.13 5 1.76 0.16 1 8 –
AI Ideas 109 5.47 6 1.58 0.15 1 10 0.67
AI Ideas + Human Rerank 109 5.55 6 1.52 0.15 1 9 0.29
OverallScore
Human Ideas 119 4.68 5 1.90 0.17 1 9 –
AI Ideas 109 4.85 5 1.70 0.16 1 9 1.00
AI Ideas + Human Rerank 109 5.34 6 1.79 0.17 1 9 0.04*
Table 7: Scores across all conditions by treating each review as an independent datapoint (Test
1). Size is the number of reviews for each condition and the p-values are computed with two-
tailedWelch’st-testswithBonferronicorrection. Weboldresultsthatarestatisticallysignificant
(∗p<0.05;∗∗p<0.01;∗∗∗p<0.001). AIideasarejudgedassignificantlybetterthanhumanideasin
termsofnoveltyandexcitementwhilebeingcomparableonallothermetrics.
theoverallscore(p<0.05).WedonotobservesignificantdifferencesbetweenAI-generatedideasand
human-writtenideasontheothermetrics.
5.2 Test2: TreatingEachIdeaasanIndependentDatapoint
Sincewecollectmultiplereviewsforeachidea,onecouldarguethatweshouldnottreateachreview
asanindependentdatapoint. Toaccountforthispotentialconfounder, weperformTest2where
we average the scores of each idea and treat each idea as one datapoint. This way, the sample
sizeforeveryconditionwillbeN =49, namelythenumberofideas. WetreattheHuman Ideas
asthebaselineconditionandcompareitwithAI IdeasandAI Ideas + Human Rerankusing
two-tailedWelch’st-testswithBonferronicorrection. AsshowninTable8,westillseesignificant
results (p < 0.05) where both AI Ideas (µ = 5.62±σ = 1.39) and AI Ideas + Human Rerank
(µ=5.78±σ=1.07)havehighernoveltyscoresthanHuman Ideas(µ=4.86±σ=1.26).
5.3 Test3: TreatingEachReviewerasanIndependentDatapoint
Anotherpossibleconfounderisthatdifferentreviewersmighthavedifferentbiases,forexample,
somereviewersmaybemorelenientthanothers.Toaccountforsuchreviewerbiases,weperformTest
10Condition Size Mean Median SD SE Min Max p-value
NoveltyScore
Human Ideas 49 4.86 5.00 1.26 0.18 1.50 7.00 –
AI Ideas 49 5.62 5.50 1.39 0.20 1.50 8.33 0.03*
AI Ideas + Human Rerank 49 5.78 6.00 1.07 0.15 3.00 8.33 0.00**
ExcitementScore
Human Ideas 49 4.56 4.33 1.16 0.17 2.00 7.00 –
AI Ideas 49 5.18 5.50 1.33 0.19 2.50 7.33 0.08
AI Ideas + Human Rerank 49 5.45 5.50 1.36 0.19 1.00 7.33 0.00**
FeasibilityScore
Human Ideas 49 6.53 7.00 1.50 0.21 3.00 9.00 –
AI Ideas 49 6.30 6.00 1.27 0.18 2.50 8.50 1.00
AI Ideas + Human Rerank 49 6.41 6.50 1.06 0.15 4.00 9.00 1.00
ExpectedEffectivenessScore
Human Ideas 49 5.10 5.33 1.14 0.16 3.00 7.00 –
AI Ideas 49 5.48 5.50 1.23 0.18 2.00 7.50 0.58
AI Ideas + Human Rerank 49 5.57 5.50 0.99 0.14 3.00 7.50 0.17
OverallScore
Human Ideas 49 4.69 4.67 1.16 0.17 2.00 6.67 –
AI Ideas 49 4.83 5.00 1.34 0.19 1.50 7.50 1.00
AI Ideas + Human Rerank 49 5.32 5.50 1.24 0.18 2.00 7.50 0.06
Table8:Scoresacrossallconditionsbyaveragingthescoresforeachideaandtreatingeachideaasone
datapoint(Test2).Sizeisthenumberofideasforeachcondition,andthep-valuesarecomputedwith
two-tailedWelch’st-testswithBonferronicorrection.Weboldresultsthatarestatisticallysignificant
(∗p<0.05;∗∗p<0.01).AIideasarejudgedassignificantlybetterthanhumanideasintermsofnovelty
whilebeingcomparableonallothermetrics.
3wherewetreateachreviewerasonedatapointandcomputetheiraveragescoreoneachcondition.
Thenforeachreviewer,wegettheirmeanscoredifferencebetweentheAI Ideasconditionand
theHuman Ideascondition, aswellasthedifferencebetweentheAI Ideas + Human Rerank
conditionandtheHuman Ideascondition. Thisway,weonlyanalyzethedifferencesamongthe
differentconditions.Thatis,ifthedifferencesaresignificantlyhigherthanzeroundertheone-sample
t-test,thatindicatesreviewersaregivinghigherscorestooneconditioncomparedtotheother.The
resultsareshowninTable9,andweseesignificantresults(p<0.05)thatAIideasinboththeAI Ideas
andAI Ideas + Human RerankconditionsareratedmorenovelthanHuman Ideas.Therefore,
weconcludethatAIideasgeneratedbyourideationagentarejudgedasmorenovelthanhuman
expertgeneratedideas,consistentlyacrossallthreedifferentstatisticaltests.10
6 In-DepthAnalysisoftheHumanStudy
WhiletheabovemainresultshighlightthepromiseofLLMsingeneratingnovelresearchideas,there
aresomeadditionalnuances.Inthissection,wemovebeyondthestatisticalcomparisonsanddive
10Wealsoincluderesultsoffittinglinearmixed-effectsmodelsinAppendixN,whichreinforcesourconclusions.Addi-
tionally,weplotthebreakdownofallmetricsbytopicinAppendixO.
11N MeanDiff p-value
NoveltyScore
AI IdeasvsHuman Ideas 70 0.94 0.00**
AI Ideas + Human RerankvsHuman Ideas 65 0.86 0.00**
ExcitementScore
AI IdeasvsHuman Ideas 70 0.73 0.01*
AI Ideas + Human RerankvsHuman Ideas 65 0.87 0.00**
FeasibilityScore
AI IdeasvsHuman Ideas 70 -0.29 0.36
AI Ideas + Human RerankvsHuman Ideas 65 -0.08 0.74
EffectivenessScore
AI IdeasvsHuman Ideas 70 0.42 0.16
AI Ideas + Human RerankvsHuman Ideas 65 0.39 0.16
OverallScore
AI IdeasvsHuman Ideas 70 0.24 0.36
AI Ideas + Human RerankvsHuman Ideas 65 0.66 0.01*
Table9:MeanscoredifferencesbetweenAIideasandhumanideasbytreatingeachreviewerasadata
point(Test3).Allp-valuesarecomputedwithone-samplet-testswithBonferronicorrection.Webold
resultsthatarestatisticallysignificant(∗p<0.05;∗∗p<0.01).
intootheraspectsofourcollecteddata.Specifically,wefocusonthequalityofhumanideas,reviewer
preferences,andtheextentofrevieweragreement.
6.1 HumanExpertsMayNotBeGivingTheirBestIdeas
We first investigate whether human experts are submitting their best ideas to us. We did a post-
studysurveytounderstandhowidea-writingparticipantscameupwiththeirideas. Outofthe49
participants,37ofthemcameupwiththeideaonthespot,whiletheother12alreadyhadtheidea
beforethestudy.Furthermore,weaskedthesurveyquestion:“Howdoesthisideacomparetoyourpast
researchideas(ideasthatyouactuallyworkedon)?Pleaseanswerwithapercentile.E.g.,thisideaisoneofmy
top10%ideas.”Ourparticipantsindicatedthatonaveragetheirsubmittedideasareaboutthetop43%
ofalltheirpastideas.Thisimpliesthatourcollectedideasarelikelythemedian-levelideasfromthese
expertresearchers,whichisreasonablegiventhatmostofthemcameupwiththeideawithinthe
10-daytimeconstraintofthetask.
6.2 ReviewersTendtoFocusMoreonNoveltyandExcitement
Togainadeeperunderstandingofthedynamicsbetweenthedifferentmetricsinthereviewprocess,
weexplorewhetherreviewersfocusonspecificaspectswhenevaluatingtheideas.Wecomputethe
pairwisecorrelationbetweendifferentmetricsinTable10.Theoverallscoremostlycorrelateswiththe
noveltyscore(r=0.725)andexcitementscore(r=0.854),whilehavingalmostnocorrelation(r<0.1)
withthefeasibilityscore.Thisimpliesthatreviewersmightbepayingmoreattentiontothenovelty
andexcitementaspectsoftheideaswhentheyarereviewing.
12Overall Novelty Excitement Feasibility Effectiveness
Overall – 0.725 0.854 0.097 0.642
Novelty 0.725 – 0.719 -0.073 0.357
Excitement 0.854 0.719 – -0.031 0.565
Feasibility 0.097 -0.073 -0.031 – 0.251
Effectiveness 0.642 0.357 0.565 0.251 –
Table10:Pairwisecorrelationbetweendifferentmetrics(symmetricmatrix).
6.3 ReviewingIdeasisInherentlySubjective
Finally, we acknowledge that reviewing is inherently subjective, and reviewing based on ideas
ratherthanexecutedpapersmightbeevenmoresubjective.Weinvestigatethisusinginter-reviewer
agreement.Specifically,werandomlysplitreviewersofeachpaperintohalf,useonehalftorankthe
topandbottom25%ofallideas,andthenmeasureagreementwiththeheld-outsetofreviewers.11 As
showninthefirstblockofTable11,reviewershavearelativelylowagreement(56.1%)despitethe
factthatwehaveprovideddetailedexplanationsforeachmetricinourreviewform.Asabaseline
comparison,theNeurIPS2021reviewerconsistencyexperimentfound66.0%accuracyusingthis
revieweragreementmetricinthebalancedsetting(Beygelzimeretal.,2021,Luetal.,2024).Wealso
computedtherevieweragreementusingthesamemetriconthe1.2KICLR2024submissionsrelated
tolanguagemodels,whichhasabalancedaccuracyof71.9%.Whileourrevieweragreementishigher
thanrandom(50%),itisgenerallylowerthanconferencereviewing,mostlikelyduetothehigher
subjectivityinvolvedwhenevaluatingideaswithoutseeingtheactualexperimentresults.
7 LimitationsofLLMs
With our findings from the human study in mind, we now turn to LLM performance to provide
insightsthatcouldinformfuturemethodsforimprovingideagenerationsystems.Ourideationagent
ismotivatedbytwopotentialstrengthsofLLMs:theirabilitytoscalebygeneratingavastnumber
ofideas-farmorethananyhumancould-andthepossibilityoffilteringtheseideastoextractthe
bestonesfromthelargepool.Intheory,thisapproachcouldleadtohigh-qualityideasbyleveraging
inferencescaling.However,wepresentempiricalevidencethatthisnaiveassumptionaboutscaling
ideagenerationhassignificantlimitations.
7.1 LLMsLackDiversityinIdeaGeneration
Weadoptedanover-generateandrankparadigminideageneration.Thisraisesthequestion:isthere
anupperlimittohowmanynewideasLLMscangenerate?Toanswerthisquestion,wetakeacloser
lookat4000generatedseedideasforeachtopic.
Weencodeallrawideaswithall-MiniLM-L6-v2fromSentence-Transformers.Foreachidea,we
computeitscosinesimilaritywithallpreviouslygeneratedideasonthesametopic. Weconsider
anideaasaduplicateifithasasimilarityofabove0.8withanyofthepreviouslygeneratedideas.
In Figure 4, we show that as the agent keeps generating new batches of ideas, the percentage of
non-duplicatesinnewlygeneratedbatcheskeepsdecreasing,andtheaccumulatednon-duplicate
ideaseventuallyplateau.Infact,outofthe4000generatedseedideas,thereareonly200non-duplicate
11ThismetricfollowsthebalancedaccuracymetricasusedinLuetal.(2024)andavoidsthelimitationsofotheragreement
metricslikeKrippendorff’salpha,whichrequireoverlappingreviewsandwouldresultinasparsematrixduetothe
non-overlappingnatureofourreviewerassignments.Wedotherandomsplitting20timesandreporttheaveragetoreduce
variances.
13Evolution of Non-Duplicates (%) Across Generations Accumulation of Non-Duplicate Ideas Across Generations
100
200
90
175 80
70 150
60 125
50 % Non-Duplicates 100 Accumulated Non-Duplicates
40 75
30
50
20
10 25
0 0
0 500 1000 1500 2000 2500 3000 3500 4000 0 500 1000 1500 2000 2500 3000 3500 4000
Total Number of Ideas Generated Total Number of Ideas Generated
Figure4: MeasuringduplicationofAI-generatedideas: theleftfigureplotsthepercentageofnon-
duplicateideasineachnewbucketofgeneratedideas;therightfigureplotstheaccumulatednon-
duplicateideasastheagentkeepsgeneratingnewideas.Alldatapointsareaveragedacrossalltopics.
uniqueideas. Thissetsabottleneckonourinference-timescalingsinceincreasingthenumberof
generatedideassimplyleadstorepeatingduplicateideas.
7.2 LLMsCannotEvaluateIdeasReliably
MostpriorworkshaveadoptedLLM-as-a-judgeforevaluating
researchideas(Luetal.,2024)motivatedbytheobservationthat Consistency
LLMscanhaveahigheragreementwithhumanevaluatorsthan Random 50.0
theinter-humanagreement.However,weoffersomeempirical NeurIPS’21 66.0
evidencethatLLMscannotevaluateideasreliablyyet. ICLR’24 71.9
Ours 56.1
Concretely,weusetheaveragereviewscoreofeachideatorank
GPT-4oDirect 50.0
thetopandbottom25%ofallourcollectedhumanandAIideas,
GPT-4oPairwise 45.0
andusethistobenchmarkvariousLLMevaluators.Specifically,
Claude-3.5Direct 51.7
weobtaintheLLMpredictedscoresofallideasandsetthemedian
Claude-3.5Pairwise 53.3
scoreasthethresholdtomeasuretheiraccuracyonourbalanced
“AIScientist”Reviewer 43.3
idearankingdata.
Table 11: Review score consis-
InthesecondblockofTable11,wecompareseveraldifferentLLM
tency among human reviewers
evaluators:1)directlygivingthereviewcriteriaandprompting
(firstblock)andbetweenhumans
forafinalscore(Baeketal.,2024,Lietal.,2024,Yangetal.,2024);
andAI(secondblock).
2) our pairwise ranker as described in Section 3.3; and 3) the
“AIScientist”revieweragent(Luetal.,2024).AlloftheseLLM
evaluatorshavealoweragreementthanourexpertreviewers’scores.EventhebestLLMevaluator
—ourownClaude-3.5pairwiseranker—onlyachievesanaccuracyof53.3%,lowerthanourinter-
reviewerconsistencyof56.1%.
Even if AI-human agreement eventually matches or exceeds human-human agreement, simply
meetingthisbaselinedoesnotimplythatAI-as-a-reviewerismeaningful,sincewemaybetrading
varianceforbias,whereAIreviewersaremoreconsistentbutrelyonspuriouscorrelations(Durmus
et al., 2022). Our findings in Table 11 are consistent with these brittleness concerns, as we find a
significantdropinAI-humanagreementscoresunderourstudycomparedtotheoriginalstudies.
Finally,eventhoughClaude-3.5pairwiseagreementsmayseemclosetohumanagreement,many
otherpiecesofevidencethroughoutthepaperleadsustobecautiousabouttheuseofLLM-as-a-judge
14
)%(
egatnecreP
etacilpuD-noN
saedI
etacilpuD-noN
detalumuccAinsuchacomplexandsubjectivetask. Theseincludeourfindingsonthesignificantdiscrepancy
betweentheagent’stop-rankedideasandthehumanexpert’stop-rankedideas(AppendixI)andhow
theAI Ideas + Human RerankconditiontendstoscorehigherthantheAI Ideasconditionon
allmetricsinSection5.TheselimitationsofLLMauto-evaluationnotonlyconstraintheeffectiveness
ofourover-generate-and-rankparadigmforideagenerationbutalsoraiseconcernsabouttrusting
conclusionsthatarebasedprimarilyonLLMevaluators.
8 QualitativeAnalysisandExamples
Inthissection,weoffersomequalitativeanalysisofhuman-andAI-generatedideasbasedonour
collectedreviewsandpresentfourpairsofrandomlysampledhumanandAIideasascasestudies.
8.1 AnalysisofFree-TextReviews
FollowingrecentpracticesofusingLLMstoextractpatternsfromtextcorpora(Zhongetal.,2022,
2023),weuseClaude-3.5toextractandclusterthemainpointsfromallreviews.Wethenmanually
verifiedandlabeledeachcluster.
ManyreviewsreinforceourquantitativefindingthatAIideastendtobemorenovel.Forexample,
reviewersnoted: “Theideaof[...] isquitenovelinanin-contextlearningsetting.”, “Theideaof
exploring[...]usinganLLM-basediterativeapproachisnovel.”,“Theideaof[...]whenconstructing
promptstoimprovecross-lingualtransferisonethatIhavenotheardofbefore.”,“Iliketheideato
[...],andthinkitwillbehelpfulforotherresearchersinthecommunity.”,“Combining[...]isaunique
wayofattemptingtopreservethegistoftheinformationwhilelikelylosingspecificidentifiers.”,and
“Safeguardingusing[...]isclearlynovel.Similarideashavenotbeenseenintherelatedwork.”.
Next,wesummarizesomecommonfailuremodesofAIideas:
1. Being too vague on implementation details. For example, one reviewer noted: “I’m not
superclearonthedetailsofthislatticeandhowthemodelwillbeprompted,soI’mnotsuper
sure how well the model will complete these subtasks and how well-suited this particular
structureistocompletingtheoveralltask.” andanotherreviewernoted: “"Foranalyzingthe
effectivenessofthemethod,theproposalonlyprovidesaveryad-hoc+hand-waveysuggestion
tocompareresponsesacrosspredefinedquestions.”Inanothercase,theAIideaiscriticizedfor
notconsideringpracticalimplementationdetails:“Ithinkineachofthesteps,thereissomething
hardtoexecute. Forexample,instepConstellationFormation,howdowedotheweighted
sum?”Similarly,otherreviewsnoted:“It’sunclearhowCLIPisconnectedtothelanguagemodel
andhowtrainingaCLIPmodelwouldenabletheLMtounderstandimages.”,and“There’s
no mentioning on how to prompt the model to generate defensive strategies and refine the
model’sresponsesusingthesestrategies.”Suchvaguenessoftenmakesitdifficultforreviewers
tomakeconfidentjudgments:“Becausethisideaistoogeneralandvague,Ican’treallyanswer
thepreviousquestion. Anideaneedsacertainlevelofdetailstobedeterminedifitfitsfora
conference/journalbutthisonemissesthem.”
2. Misuseofdatasets.Forexample:“I’mnotsureaboutthedatasetspicked.StereoSetisnotaQA
dataset;itsimplycontainsstatements.Also,Idon’tunderstandwhyDialogueNLIresponses
requireempathy.”,“I’mconcernedthedatasetsproposedaretherighttestcasesforsecurityofthe
code(sincetheyarereallyjustML/programmingproblems,notsystem-levelprogramming).”,
and“thechoiceofdatasetsmightnotbethebesttoshowtheeffectofincorporatingmultiple
perspectives, especially TruthfulQA and ScienceQA, which seems to have a single correct
interpretationandanswer.”Inanotherexample,thebenchmarkdatasetschosenareconsidered
15too easy by the reviewer: “none of the chosen datasets (MATH, GSM8K, and MMLU) uses
complicatedmathconcepts”.
3. Missingorinappropriatebaselines.Forexample:“Theproposedmethodneedstobecompared
tosimplyaskingthemodeltothinkofone(orseveral)factsaboutthequestionbeforeanswering
usingmoreturns.Thiscouldbeanadditionalbaselinetoverifythescoringprocessismeaningful.”
and“Althoughtheproposalincludessomebaselinesthatshouldbecomparedto,itdoesnot
mention some methods which seem to do quite well with LLMs.” Sometimes, “the chosen
baselinesmaynotbesuitable”,forexample,becausetheyarenotdirectlycomparablewiththe
proposedmethod.
4. Making unrealistic assumptions. For example: “The assumption that model can (mostly)
accuratelyflagitsownhallucinationsisquitetricky.”,“thereisapresupposedassumptionthat
hallucinationsinLLMsareungroundedandindependentofthedatatheyaretrainedon,which
isgenerallynotconsideredtrue”,“Thebigissuefortheeffectivenessoftheproposedmethodis
that,itassertsverystrongassumptionsondownstreamtasks,suchastheremustexistonlytwo
extremes.”,“Someassumptions(e.g.,[...]) areunlikelytobetrueinpractice,especiallywhen
low-resourcelanguagesandlessrepresentedculturesareincludedinthestudy.”,and“Amajor
assumptioninthisapproachisthatthemodelisableto[...].However,[...]”.
5. Being too resource-demanding. Despite the fact that we explicitly prompted the agent to
considerfeasibilitywhengeneratingideas,someofthegeneratedideasarestilltooresource-
demanding.Forexample,onereviewernoted:“ThebiggestissuetofeasibilityIseeisthatthe
projectcallsforfine-tuningBLOOM(Seestep5).BLOOMhas176Bparameterssoit’sgoingto
takequitealotofGPUstofine-tune.Fromasystemsperspective,Iseethisascausingdelays.”
Insomeothercases,manualdataannotationisbeingcriticizedforfeasibility:“Thebottleneck
seemstobethedatasetcollectionprocessiftherearenoexistingdatasetsthatfittherequirements
of the paper.”, and “the manual evaluation by native speakers or cultural experts could be
time-consumingandresource-intensive”.
6. Notwell-motivated.Forexample:“Notwell-motivatedandthereisnotaclearintuitionthat
thisworkcanworktoincreasethefactuality.”,“Andingeneralthemethodisnotwell-motivated
andneedsreasonswhyretrievingfrommodelitselfismeaningfulbyusecasesorspecifictasks.”,
and“Theideasimplydoesn’tmakesensetome. GivencurrentLLMs’ability,I’mprettysure
theycansimplyrecitecodelikeinsertingdatatoabinarysearchtree.”
7. Notadequatelyfollowingexistingbestpractices.Forexample:“Theproposaldoesnotseem
to include awareness of what has been previously tried, or more strategic ways to evaluate
success/failures...”
Wecontrastthesewithsomeoftheuniquestrengthsandweaknessesofhumanideas:
1. Humanideasaregenerallymoregroundedinexistingresearchandpracticalconsiderations,
butmaybelessinnovative. Forexample,theseideasmightbeapplyingexistingtechniques
tonewproblems: “Multilingualityasadebiasingmethodhasalreadybeenconsideredinthe
literature,althoughnotnecessarilyinthepromptengineeringframework.”Sometimespeople
applyincrementalchangestoexistingtechniques:“Theoverallideasharesquiteasimilaridea
withprogram-of-thought(PoT).Theonlydifferenceisthatthereisanadditionalstepwherean
LLMispromptedtodecidewhethertousecodeornot.” Someideastrytocombineexisting
techniques:“QuerydecompositionandRAGseparatelyarewellstudied,ifthereisnoexisting
workthatcombinesboth(whichI’mnotawareof),thenit’sreasonablynovel.”Assomereviewers
16noted,humanideastendtobuildonknownintuitionsandresults:“Therearealreadyexisting
worksonusingavailablelexiconstoimprovethetranslationcapabilitiesofLLMsingeneral.”
2. Human ideas tend to be more focused on common problems or datasets in the field. For
example:“Theproblemofmodelsnothandlingnegationproperlyisaverycommonproblem,
especiallyamongproprietyLMssuchasclaude-3-5-sonnet.”,“Thedataexist.Thisprojectmainly
entailsplugginginthesedatasetstoaprompttemplateandfinetuningforabit.Thereislittleleft
unspecified,anditshouldbequitesimpletoexecuteon.”,“Ihaven’tfoundanyworkusingthis
ideatosolvethisspecificproblem,but[...] isdefinitelynotnew.”,and“Whileexistingworks
haveexploredtheproblemofcalibrationinlong-formanswers(e.g.[...]),thespecificmethodfor
calibrationisdifferent.”
3. Human ideas sometimes prioritize feasibility and effectiveness rather than novelty and
excitement.Forexample,reviewersnoted:“Idon’tthinkthiswillbeagroundbreakingfinding,
but it will probably work.” and “while the idea is promising and could lead to significant
improvements,itmaynotbegroundbreakingenoughtobeconsideredtransformativeorworthy
ofabestpaperaward”.
8.2 RandomlySampledHumanandAIIdeaswithReviews
Werandomlysamplefourpairsofideasfromdifferenttopicstogroundournumericalresultswith
actualexamples.Ineachpair,thereisoneAIideaandonehumanidea.Tosavespace,weincludethe
fullprojectproposalofeachideaalongwiththefullsetofreviewsintheAppendix,butwelisttheir
titles,topics,andaveragescoreshereforquickreference(werevealwhethereachideaisAI-generated
orhuman-writteninAppendixX):
1. ModularCalibrationforLong-formAnswers:AppendixP
Topic:Uncertainty;AverageOverallScore:5.5
2. SemanticResonanceUncertaintyQuantification:CalibratingLLMConfidencethroughMulti-
PathReasoning:AppendixQ
Topic:Uncertainty;AverageOverallScore:6
3. TranslationwithLLMsthroughPromptingwithLong-FormContext:AppendixR
Topic:Multilingual;AverageOverallScore:4
4. LinguisticPivotConstellation:EnhancingCross-LingualTransferforLow-ResourceLanguages
andDialects:AppendixS
Topic:Multilingual;AverageOverallScore:6.7
5. LLMDirectedRetrievalQueryingforImprovingFactuality:AppendixT
Topic:Factuality;AverageOverallScore:4.7
6. Semantic Divergence Minimization: Reducing Hallucinations in Large Language Models
throughIterativeConceptGrounding:AppendixU
Topic:Factuality;AverageOverallScore:3.3
7. Autoprompting:GenerateDiverseFew-shotExamplesforAnyApplication:AppendixV
Topic:Coding;AverageOverallScore:5
8. TemporalDependencyUnfolding:ImprovingCodeGenerationforComplexStatefulSystems:
AppendixW
Topic:Coding;AverageOverallScore:6.7
179 RelatedWork
Researchideagenerationandexecution. Severalpriorworksexploredmethodstoimproveidea
generation,suchasiterativenoveltyboosting(Wangetal.,2024),multi-agentcollaboration(Baeketal.,
2024),andmulti-moduleretrievalandrevision(Yangetal.,2024).Whilesomeofthemsharesimilar
componentsasourideationagent,theseworksfocusonimprovingtheideagenerationmethodsover
vanillapromptingbaselines,withoutcomparisonstoanyhumanexpertbaselines.Beyondideation,
anotherlineofworkusesLLMsforexecutingexperimentsbygeneratingcodegiventheresearch
problems(Huangetal.,2024,Tianetal.,2024),orcombiningideagenerationwithcodegeneration
todirectlyimplementAI-generatedideas(Lietal.,2024,Luetal.,2024). Theseworkseitheruse
automaticevaluationonapre-definedsetofproblemsandbenchmarks,settingaconstrainedproblem
space;orrelyonproxymetricslikeLLMevaluators,whichareoftenunreliable.
LLMforotherresearch-relatedtasks.LLMshavealsobeenusedforseveralotherresearch-related
tasks,suchasgeneratingcodetoperformdata-drivendiscovery(Guetal.,2024,Guoetal.,2024,Hu
etal.,2024,Ifarganetal.,2024,Majumderetal.,2024),automaticreviewgeneration(D’Arcyetal.,
2024,Liangetal.,2024),relatedworkcuration(Ajithetal.,2024,KangandXiong,2024,Lehretal.,
2024,Pressetal.,2024),experimentoutcomeprediction(Hewittetal.,2024,Lehretal.,2024,Manning
etal.,2024,Zhangetal.,2024),andfutureworkrecommendation(Zhangetal.,2024).Unlikethese
works,wetacklethemorecreativeandopen-endedtaskofresearchideation.
Computationalcreativity. OurworkalsoconnectstothelineofworkonexaminingAI’snovelty
and diversity in creative tasks. Chakrabarty et al. (2024) found that AI writings are less creative
than professional writers, while we show LLM-generated ideas can be more novel than experts
onthetaskofresearchideation. AnotherlineofworkfoundthatLLMgenerationslackcollective
diversity(Andersonetal.,2024,Zhouetal.,2024),whichmatchesourfindingsonideageneration.
Lastly,severalotherworksconductedhumanevaluationtostudytheimpactofAIexposureorhuman-
AIcollaborationonnoveltyanddiversity(Ashkinazeetal.,2024,Liuetal.,2024,PadmakumarandHe,
2024)withmixedconclusions.Whilewealsoconductahumanevaluationofideanovelty,wefocus
onthehuman-AIcomparisononthechallengingtaskofresearchideationwithexpertparticipants.
10 Discussion
Tosummarize,wecomparedresearchideasgeneratedbyourAIagentwithideaswrittenbyexpert
researchers,andobservedtherobustfindingthatexpertreviewersrateAIideasasstatisticallymore
novelthanexpertideas.Inthissection,wediscusssomehigh-levelquestionsreadersmighthaveand
suggestsomewaystoaddressthem.
Question1:Dothesecollectedexpertideasrepresenttheirbestideas?Onemightarguethatthese
ideassubmittedbyouridea-writingparticipantsmightnotrepresenttheirbestideasaswediscussed
inSection6.1,sincemostofthemcameupwiththeideaonthespotwithinashortperiod.Inorderto
addressthisconcern,wehavedesignedanexperimentwherewewillcompareAIideaswithpapers
acceptedattop-tierAIconferences. Toavoidanypossiblecontamination,wetargettheupcoming
EMNLP2024conference,whichwillreleasetheacceptedpapersinOctober2024.Wehavegenerated
AIideaswithouragenton23topicsfromtheEMNLPCallForPaperspageinJuly2024andcached
them.Wepre-registeredouranalysisplanwhichalsoincludesthelinktothecachedideas.12 Apart
fromcomparingthequalityoftheseideas,wewillalsocomputetheoverlapbetweenAI-generated
ideasandacceptedpapersonthesametopics.
12https://osf.io/z6qa4
18Question2: Areevaluationsbasedsolelyonideassubjective? Inthiscurrentstudy,wefocused
solelyonevaluatingtheideasthemselves.Ideasthatsoundnovelandexcitingmightnotnecessarily
turnintosuccessfulprojects,andourresultsindeedindicatedsomefeasibilitytrade-offsofAIideas.
WeviewthecurrentstudyasapreliminaryevaluationofAI-generatedideas.Inthenextphase,we
willrecruitresearcherstoexecutesomeAIandhuman-generatedideasintofullprojects. Thiswill
enablereviewerstoassessthecompleteexperimentaloutcomes,providingamorereliablebasisfor
evaluation.Furthermore,itwillallowustoanalyzewhetherourinitialideaevaluationsalignwith
theassessmentsoftheactualprojectoutcomes.
Question3:Whydoyoufocusonlyonprompting-basedresearchinNLP?Thescopeofourstudy
islimitedtopromptingresearchideaswithinNLP.Wechosethisdesigntofacilitatethenextphase
ofourexecutionexperiment,wherewepreferresearchideasthatarelessresource-demandingand
canbeexecutedrelativelyquickly.Webelievethattheevaluationprotocolsweestablishedshouldbe
applicabletootherresearchdomainsaswell,althoughtheconclusionscouldbedifferentdepending
ontheresearchfields.Futureworkshouldconsiderextendingsuchhumanstudytootherresearch
domainsanditwouldbeinterestingtocomparehowtheconclusionsdiffer.
Question4: Canyouautomateideaexecutionaswell? Itistemptingtoenvisionanend-to-end
automatedresearchpipelinewhereAIagentscanimplementAI-generatedideastodirectlyevaluate
theireffectiveness.Apartfromspeedingupscientificdiscovery,onecouldalsoimagineusingsuch
executionagentstoautomaticallyverifyexperimentresultsinexistingpapersornewsubmissions.
WehavealsoexploredbuildinganLLMagenttogeneratecodetoimplementthegeneratedideas.
Specifically,weprovideatemplatecodebasethatconsistsof:(1)loadingdatasetsfromHuggingface
orgeneratingsynthetictestexamples; (2)implementingbaselinemethods; (3)implementingthe
proposedmethod;(3)loadingorimplementingtheevaluationmetrics;(4)runningexperimentson
thetestsetwiththebaselinesandtheproposedmethod,sothattheoutputoftheagentwillbeareport
ofthebaselineperformanceaswellastheproposedmethod’sperformance. Whilethisagentcan
generatecodethatcompilesandexecutes,wefindthattheautomatedexperimentscanbemisleading
becausetheagentoftenskipsormodifiesstepsinthebaselinesorproposedmethods.Insomecases,
themetricfunctionsarealsonotcorrectlydefined.Thishighlightsthecorechallenge:justcomparing
thefinalexperimentresultsisnotenough;wehavetoverifythefaithfulnessoftheimplementations
aswell. Performingsuchimplementationverificationisnotatrivialtask,andweleaveittofuture
work.WeprovidedetaileddescriptionofourideaexecutionagentinAppendixY.
11 EthicalConsiderations
PublicationPolicy.ThegrowinguseofAItogenerateresearchideasraisesseriousconcernsaboutthe
potentialabuseofthesetechnologiesbystudentsorresearcherswhomayfloodacademicconferences
withlow-qualityorpoorlythought-outsubmissions. TheavailabilityofLLM-generatedcontent
couldleadtoadeclineintheoverallqualityofacademicdiscourse,assomeindividualsmighttakea
lazyapproach,relyingonAItobothgenerateideasandreviewsubmissions.Thiswouldundermine
thecredibilityandintegrityofthereviewprocess.Therisksarereal.Withoutproperoversight,we
couldseeadelugeofsubmissionsthatlackdepthorintellectualmerit.Topreventthis,itisessentialto
holdresearchersaccountablefortheoutputsgeneratedthroughAItools.Rigorousstandardsmustbe
appliedequallytobothAI-assistedandhuman-generatedresearchtoensurethattheuseofLLMs
doesnotresultinmisleading,superficial,orunethicalacademiccontributions.
IntellectualCredit. TheuseofLLMstogenerateresearchideasintroducessignificantambiguity
aroundtheconceptofintellectualcredit. Traditionalframeworksforattributingcreditinresearch,
basedonhumanauthorshipandcontribution,becomelessclearwhenAIplaysasignificantrole
19inideageneration. Questionsarisearoundhowtodistributecreditbetweenthedevelopersofthe
LLM,theresearcherswhodesignedtheframeworksforitsuse,andtheresearcherswhointegrate
AI-generatedideasintotheirwork.Furthermore,itbecomesincreasinglydifficulttotracetheorigins
ofAI-generatedcontributions,especiallywhentheydrawfromvastdatasetscomposedofnumerous
sources. This complexity calls for a broader rethinking of how intellectual credit is assigned in
AI-drivenresearch. Whileacompleteoverhauloflegalandacademicnormsisbeyondthescope
ofthisproject,weadvocatefortheadoptionoftransparentdocumentationpractices. Researchers
shouldclearlydisclosetheroleAIplayedintheideagenerationprocess,specifyingwhichmodels,
datasources,andframeworkswereused,andoutliningthelevelofhumaninvolvement.Thiscould
ensurethatthecreditdistributioninAI-supportedresearchisastransparentandfairaspossible.
PotentialforMisuse.AI-generatedresearchideas,especiallythosethatintroducenovelconcepts,
havethepotentialtobemisusedinwaysthatcouldleadtoharmfulordestabilizingoutcomes.For
instance,ideationagentscouldbeleveragedtogenerateadversarialattackstrategiesorotherunethical
applications. Thisconcernalignswithbroaderargumentsfromthosefocusedonexistentialrisk
(X-risk),whoarguethatAI-driveninnovationcouldbeaprimaryroutetodestabilizingthestatus
quo, posing risks at a societal or even global level. Our stance is that such discussions on safety
shouldbeevidence-basedtotheextentthatitispossible,andcarefulevaluationworkisanimportant
componentofkeepingthesediscussionsgroundedinactual,measuredcapabilitiesofthesesystems.
Weadvocateforcontinuedsafetyresearchspecificallytargetingthesetypesofconcerns—suchasthe
developmentofReinforcementLearningfromHumanFeedback(RLHF)systemsoranti-jailbreak
mechanismsforresearchideationagents.Additionally,webelieveitwouldbemeaningfultocreate
safetybenchmarksthatassesstheethicalandsafeapplicationofAI-generatedideas.
IdeaHomogenization. OuranalysisshowedthatcurrentLLMslackdiversityinideageneration.
ThisraisesimportantconcernsthatwideadoptionofLLMscanresultinideahomogenization,where
thegeneratedideasonlyreflectanarrowsetofperspectivesorhavesystematicbiases. Overtime,
thiscouldleadtoareductionintherichnessanddiversityofresearchoutputsglobally.Futurework
shoulddevelopwaystoeitherimproveLLMsthemselvesorrefineourideagenerationmethodsto
promoteideadiversity.It’salsoimportanttonotethatourevaluationprimarilyassessesthequalityof
thetypicalideasbeinggenerated,andmaynotfullycapturethelongtailofuniqueornovelideasthat
wouldbetrulytransformative.
ImpactonHumanResearchers. TheintegrationofAIintoresearchideagenerationintroducesa
complexsociotechnicalchallenge,asresearchisfundamentallyacommunity-driven,collaborative
effort.ByintroducingAI,particularlyLLMs,intothissocialsystem,weriskunforeseenconsequences.
OverrelianceonAIcouldleadtoadeclineinoriginalhumanthought,whiletheincreasinguseof
LLMsforideationmightreduceopportunitiesforhumancollaboration,whichisessentialforrefining
andexpandingideas.Tomitigatetheserisks,futureworksshouldexplorenewformsofhuman-AI
collaboration, and our results on human reranking of AI ideas show that even naive human-AI
collaborationapproachescanbeeffective.Beyondreranking,humanscanplayacriticalroleinthe
ideationprocessbyprovidingintermediatefeedback,takingAI-generatedideasasinspirationfor
furtherdevelopment,andbringingtheiruniqueexpertiseintotheprocess.Understandinghowto
integrateLLMsintothiscollaborativeprocesswithoutdisruptingthesocialfabricofresearchwill
be an important ongoing problem, requiring careful consideration of the broader sociotechnical
implications.
20PositionalityStatement
Wedisclosetheauthors’anticipatedoutcomesofthehumanstudybeforetheexperimentwascon-
ductedtobetransparentaboutexperimenterbiases.Amongthethreeauthors,TatsuandDiyiwere
expectinganullresultfromthestudywhileChengleiexpectedAItobebetterthanhumans.
Acknowledgement
Wethankallparticipantswhowroteandreviewedideasforus.Manyofthemalsoprovidedinsightful
feedbackonvariousaspectsofthisstudy. Thisprojectwouldnothavebeenpossiblewithouttheir
support. To ensure the integrity and fairness of phase II of our study, we leave our participants
anonymousbutwillupdatethismanuscriptwithadetailedacknowledgmentofallparticipantsinthe
project’sfinalreport.
WethankRoseWang,DoraZhao,IrenaGao,IsabelGallegos,KenLiu,AryamanArora,HarshitJoshi,
ShiFeng,TianyuGao,XinranZhao,YangjunRuan,XiYe,MertYuksekgonul,andmembersofTatsu
LabandSALTLabfortheirhelpfulfeedbackontheearlyversionofthisdraft.
WethankourundergraduateinternIshaGoswamiandfacultyadministratorEricAlejandroPineda
forassistingwithreviewdatacollectionandfinanciallogistics.
ThisworkwassupportedbygiftsfromOpenPhilanthropy,TianqiaoandChrissyChenInstitute,
Meta,IBM,andAmazon,andgrantsfromONR,NSFIIS-2247357,andCNS-2308994.
References
AnirudhAjith,MengzhouXia,AlexisChevalier,TanyaGoyal,DanqiChen,andTianyuGao.LitSearch:
ARetrievalBenchmarkforScientificLiteratureSearch. ArXiv,abs/2407.18940,2024.
Barrett R Anderson, Jash Hemant Shah, and Max Kreminski. Homogenization Effects of Large
LanguageModelsonHumanCreativeIdeation. InProceedingsofthe16thConferenceonCreativity&
Cognition,2024.
JoshuaAshkinaze,JuliaMendelsohn,QiweiLi,CerenBudak,andEricGilbert.HowAIIdeasAffectthe
Creativity,Diversity,andEvolutionofHumanIdeas:EvidenceFromaLarge,DynamicExperiment.
ArXiv,abs/2401.13481,2024.
Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. ResearchAgent: Iter-
ativeResearchIdeaGenerationoverScientificLiteraturewithLargeLanguageModels. ArXiv,
abs/2404.07738,2024.
AntonBakhtin,NoamBrown,EmilyDinan,GabrieleFarina,ColinFlaherty,DanielFried,Andrew
Goff,JonathanGray,HengyuanHu,AthulPaulJacob,MojtabaKomeili,KarthikKonath,Minae
Kwon, Adam Lerer, Mike Lewis, Alexander H. Miller, Sandra Mitts, Adithya Renduchintala,
StephenRoller,DirkRowe,WeiyanShi,JoeSpisak,AlexanderWei,DavidJ.Wu,HughZhang,and
MarkusZijlstra. Human-levelplayinthegameofdiplomacybycombininglanguagemodelswith
strategicreasoning. Science,378:1067–1074,2022.
Alina Beygelzimer, Yann Dauphin, Percy Liang, and Jennifer Wortman Vaughan. The
neurips 2021 consistency experiment. https://blog.neurips.cc/2021/12/08/
the-neurips-2021-consistency-experiment, 2021. Neural Information Processing
Systemsblogpost.
21BradleyBrown,JordanJuravsky,RyanEhrlich,RonaldClark,QuocV.Le,ChristopherR’e,andAzalia
Mirhoseini. LargeLanguageMonkeys:ScalingInferenceComputewithRepeatedSampling. ArXiv,
abs/2407.21787,2024.
TuhinChakrabarty,PhilippeLaban,DivyanshAgarwal,SmarandaMuresan,andChien-ShengWu.
ArtorArtifice?LargeLanguageModelsandtheFalsePromiseofCreativity. InCHI,2024.
WenhuChen,XueguangMa,XinyiWang,andWilliamW.Cohen. ProgramofThoughtsPrompting:
DisentanglingComputationfromReasoningforNumericalReasoningTasks. TMLR,2023.
KatherineM.Collins,AlbertQiaochuJiang,SimonFrieder,LiSiangWong,MiriZilka,UmangBhatt,
ThomasLukasiewicz,YuhuaiWu,JoshuaB.Tenenbaum,WilliamHart,TimothyGowers,Wenda
Li, Adrian Weller, and Mateja Jamnik. Evaluating language models for mathematics through
interactions. ProceedingsoftheNationalAcademyofSciencesoftheUnitedStatesofAmerica,121,2024.
MikeD’Arcy,TomHope,LarryBirnbaum,andDougDowney. MARG:Multi-AgentReviewGenera-
tionforScientificPapers. ArXiv,abs/2401.04259,2024.
ShehzaadDhuliawala,MojtabaKomeili,JingXu,RobertaRaileanu,XianLi,AsliCelikyilmaz,and
JasonWeston. Chain-of-VerificationReducesHallucinationinLargeLanguageModels. ArXiv,
abs/2309.11495,2023.
Shizhe Diao, Pengcheng Wang, Yong Lin, Xiang Liu, and Tong Zhang. Active Prompting with
Chain-of-ThoughtforLargeLanguageModels. InACL,2024.
EsinDurmus,FaisalLadhak,andTatsunoriB.Hashimoto. SpuriousCorrelationsinReference-Free
EvaluationofTextGeneration. InAnnualMeetingoftheAssociationforComputationalLinguistics,2022.
URLhttps://api.semanticscholar.org/CorpusID:248300077.
KenGu,RuoxiShang,RuienJiang,KeyingKuang,Richard-JohnLin,DongheLyu,YueMao,Youran
Pan,TengWu,JiaqianYu,YikunZhang,TianmaiM.Zhang,LanyiZhu,MikeA.Merrill,Jeffrey
Heer,andTimAlthoff. BLADE:BenchmarkingLanguageModelAgentsforData-DrivenScience.
ArXiv,abs/2408.09667,2024.
SiyuanGuo,ChengDeng,YingWen,HechangChen,YiChang,andJunWang. DS-Agent:Automated
DataSciencebyEmpoweringLargeLanguageModelswithCase-BasedReasoning. InICML,2024.
LukeHewitt,AshwiniAshokkumar,IsaiasGhezae,andRobbWiller. PredictingResultsofSocial
ScienceExperimentsUsingLargeLanguageModels. Preprint,2024. URLhttps://docsend.
com/view/ity6yf2dansesucf.
XueyuHu,ZiyuZhao,ShuangWei,ZiweiChai,GuoyinWang,XuwuWang,JingSu,JingjingXu,
MingZhu,YaoCheng,JianboYuan,KunKuang,YangYang,HongxiaYang,andFeiWu. InfiAgent-
DABench:EvaluatingAgentsonDataAnalysisTasks. InICML,2024.
Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. MLAgentBench: Evaluating Language
AgentsonMachineLearningExperimentation. InICML,2024.
TalIfargan, LukasHafner, MaorKern, OriAlcalay, andRoyKishony. AutonomousLLM-driven
researchfromdatatohuman-verifiableresearchpapers. ArXiv,abs/2404.17605,2024.
HaoKangandChenyanXiong.ResearchArena:BenchmarkingLLMs’AbilitytoCollectandOrganize
InformationasResearchAgents. ArXiv,abs/2406.10291,2024.
22MichelleS.Lam,JaniceTeoh,JamesLanday,JeffreyHeer,andMichaelS.Bernstein.ConceptInduction:
AnalyzingUnstructuredTextwithHigh-LevelConceptsUsingLLooM. InCHI,2024.
StevenA.Lehr,AylinCaliskan,SuneragiriLiyanage,andMahzarinR.Banaji. ChatGPTasResearch
Scientist:ProbingGPT’sCapabilitiesasaResearchLibrarian,ResearchEthicist,DataGeneratorand
DataPredictor. ProceedingsoftheNationalAcademyofSciencesoftheUnitedStatesofAmerica,12135,
2024.
PatrickLewis,EthanPerez,AleksandaraPiktus,FabioPetroni,VladimirKarpukhin,NamanGoyal,
HeinrichKuttler,MikeLewis,WentauYih,TimRocktäschel,SebastianRiedel,andDouweKiela.
Retrieval-AugmentedGenerationforKnowledge-IntensiveNLPTasks. InNeurIPS,2020.
Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. MLR-Copilot: Autonomous Machine
LearningResearchbasedonLargeLanguageModelsAgents. ArXiv,abs/2408.14033,2024.
YujiaLi,DavidChoi,JunyoungChung,NateKushman,JulianSchrittwieser,RémiLeblond,Tom,
Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, Thomas Hubert, Peter Choy, Cyprien
de, Masson d’Autume, Igor Babuschkin, Xinyun Chen, Po-Sen Huang, Johannes Welbl, Sven
Gowal,Alexey,Cherepanov,JamesMolloy,DanielJayminMankowitz,EsmeSutherlandRobson,
PushmeetKohli,Nandode,Freitas,KorayKavukcuoglu,andOriolVinyals. Competition-level
codegenerationwithAlphaCode. Science,378:1092–1097,2022.
WeixinLiang,YuhuiZhang,HanchengCao,BingluWang,DaisyYiDing,XinyuYang,KailasVo-
drahalli,SiyuHe,DanielScottSmith,YianYin,DanielA.McFarland,andJamesZou. CanLarge
LanguageModelsProvideUsefulFeedbackonResearchPapers?ALarge-ScaleEmpiricalAnalysis.
NEJMAI,1(8),2024.
YirenLiu,SiChen,HaocongCheng,MengxiaYu,XiaoRan,AndrewMo,YiliuTang,andYunHuang.
HowAIProcessingDelaysFosterCreativity:ExploringResearchQuestionCo-Creationwithan
LLM-basedAgent. InCHI,2024.
ChrisLu,CongLu,RobertTjarkoLange,JakobFoerster,JeffClune,andDavidHa. TheAIScientist:
TowardsFullyAutomatedOpen-EndedScientificDiscovery. ArXiv,abs/2408.06292,2024.
AmanMadaan,NiketTandon,PrakharGupta,SkylerHallinan,LuyuGao,SarahWiegreffe,UriAlon,
NouhaDziri,ShrimaiPrabhumoye,YimingYang,SeanWelleck,BodhisattwaPrasadMajumder,
ShashankGupta,AmirYazdanbakhsh,andPeterClark. Self-Refine: IterativeRefinementwith
Self-Feedback. InNeurIPS,2023.
Bodhisattwa Prasad Majumder, Harshit Surana, Dhruv Agarwal, Bhavana Dalvi, Abhijeetsingh
Meena,AryanPrakhar,TirthVora,TusharKhot,AshishSabharwal,andPeterClark. Discovery-
Bench: TowardsData-DrivenDiscoverywithLargeLanguageModels. ArXiv,abs/2407.01725,
2024.
BenjaminS.Manning,KehangZhu,andJohnJ.Horton. AutomatedSocialScience:LanguageModels
asScientistandSubjects. SSRNElectronicJournal,2024.
VishakhPadmakumarandHeHe. DoesWritingwithLanguageModelsReduceContentDiversity?
InICLR,2024.
OriPress,AndreasHochlehnert,AmeyaPrabhu,VishaalUdandarao,OfirPress,andMatthiasBethge.
CiteME:CanLanguageModelsAccuratelyCiteScientificClaims? ArXiv,abs/2407.12861,2024.
23ZhenQin,RolfJagerman,KaiHui,HongleiZhuang,JunruWu,JiamingShen,TianqiLiu,JialuLiu,
DonaldMetzler,XuanhuiWang,andMichaelBendersky. LargeLanguageModelsareEffectiveText
RankerswithPairwiseRankingPrompting. InACL,2024.
NilsReimersandIrynaGurevych. MakingMonolingualSentenceEmbeddingsMultilingualusing
KnowledgeDistillation. InEMNLP,2020.
SanderSchulhoff,MichaelIlie,NishantBalepur,KonstantineKahadze,AmandaLiu,ChengleiSi,Yin-
hengLi,AayushGupta,HyoJungHan,SevienSchulhoff,PranavSandeepDulepet,SauravVidyad-
hara,DayeonKi,SwetaAgrawal,ChauPham,GersonC.Kroiz,FeileenLi,HudsonTao,Ashay
Srivastava,HevanderDaCosta,SaloniGupta,MeganL.Rogers,InnaGoncearenco,GiuseppeSarli,
IgorGalynker,DenisPeskoff,MarineCarpuat,JulesWhite,ShyamalAnadkat,AlexanderMiserlis
Hoyle, andPhilipResnik. ThePromptReport: ASystematicSurveyofPromptingTechniques.
ArXiv,abs/2406.06608,2024.
WeijiaShi,SewonMin,MichihiroYasunaga,MinjoonSeo,RichJames,MikeLewis,LukeZettlemoyer,
andWentauYih. REPLUG:Retrieval-AugmentedBlack-BoxLanguageModels. InNAACL,2024.
ChengleiSi,ZheGan,ZhengyuanYang,ShuohangWang,JianfengWang,JordanL.Boyd-Graber,and
LijuanWang. PromptingGPT-3ToBeReliable. InICLR,2023.
MügeSimsek,MathijsdeVaan,andArnoutvandeRijt. Dograntproposaltextsmatterforfunding
decisions?afieldexperiment. Scientometrics,129:2521–2532,2024.
MinyangTian, LuyuGao, ShizhuoDylanZhang, XinanChen, CunweiFan, XuefeiGuo, Roland
Haas,PanJi,KittithatKrongchon,YaoLi,ShengyanLiu,DiLuo,YutaoMa,HaoTong,KhaTrinh,
ChenyuTian,ZihanWang,BohaoWu,YanyuXiong,ShengzhuYin,MinZhu,KilianLieret,Yanxin
Lu,GenglinLiu,YufengDu,TianhuaTao,OfirPress,JamieCallan,E.A.Huerta,andHaoPeng.
SciCode:AResearchCodingBenchmarkCuratedbyScientists. ArXiv,abs/2407.13168,2024.
TrieuH.Trinh,YuhuaiWu,QuocV.Le,HeHe,andThangLuong.Solvingolympiadgeometrywithout
humandemonstrations. Nature,625:476–482,2024.
QingyunWang,DougDowney,HengJi,andTomHope. SciMON:ScientificInspirationMachines
OptimizedforNovelty. InACL,2024.
Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Huai hsin Chi, and Denny Zhou. Self-
ConsistencyImprovesChainofThoughtReasoninginLanguageModels. InICLR,2023.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed Huai hsin Chi, F. Xia, Quoc Le,
andDennyZhou. ChainofThoughtPromptingElicitsReasoninginLargeLanguageModels. In
NeurIPS,2022.
OrionWeller,MarcMarone,NathanielWeir,DawnJLawrie,DanielKhashabi,andBenjaminVan
Durme. “Accordingto...”:PromptingLanguageModelsImprovesQuotingfromPre-Training
Data. InEACL,2023.
JasonWestonandSainbayarSukhbaatar. System2Attention(issomethingyoumightneedtoo).
ArXiv,abs/2311.11829,2023.
ZonglinYang,XinyaDu,JunxianLi,JieZheng,SoujanyaPoria,andE.Cambria. LargeLanguage
ModelsforAutomatedOpen-domainScientificHypothesesDiscovery. ACLFindings,2024.
24Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik
Narasimhan. TreeofThoughts: DeliberateProblemSolvingwithLargeLanguageModels. In
NeurIPS,2023.
MichihiroYasunaga,XinyunChen,YujiaLi,PanupongPasupat,JureLeskovec,PercyLiang,EdHuai
hsinChi,andDennyZhou. LargeLanguageModelsasAnalogicalReasoners. InICLR,2024.
XingjianZhang,YutongXie,JinHuang,JingeMa,ZhaoyingPan,QijiaLiu,ZiyangXiong,TolgaErgen,
DongsubShim,HonglakLee,andQiaozhuMei. MASSW:ANewDatasetandBenchmarkTasksfor
AI-AssistedScientificWorkflows. ArXiv,abs/2406.06357,2024.
HuaixiuStevenZheng,SwaroopMishra,XinyunChen,Heng-TzeCheng,EdHuaihsinChi,QuocV.
Le,andDennyZhou. TakeaStepBack: EvokingReasoningviaAbstractioninLargeLanguage
Models. InICLR,2024.
RuiqiZhong,CharlesBurtonSnell,DanKlein,andJacobSteinhardt. DescribingDifferencesbetween
TextDistributionswithNaturalLanguage. InICML,2022.
RuiqiZhong, PeterZhang, SteveLi, JinwooAhn, DanKlein, andJacobSteinhardt. GoalDriven
DiscoveryofDistributionalDifferencesviaLanguageDescriptions. InNeurIPS,2023.
DennyZhou,NathanaelScharli,LeHou,JasonWei,NathanScales,XuezhiWang,DaleSchuurmans,
OlivierBousquet,QuocLe,andEdHuaihsinChi. Least-to-MostPromptingEnablesComplex
ReasoninginLargeLanguageModels. InICLR,2023.
Yilun Zhou, Caiming Xiong, Silvio Savarese, and Chien-Sheng Wu. Shared Imagination: LLMs
HallucinateAlike. ArXiv,abs/2407.16604,2024.
25A ListofResearchTopics
Weselectedthefollowinglistofresearchtopicsforourresearchideationtask:
1. Bias:novelpromptingmethodstoreducesocialbiasesandstereotypesoflargelanguagemodels
2. Coding:novelpromptingmethodsforlargelanguagemodelstoimprovecodegeneration
3. Safety:novelpromptingmethodstoimprovelargelanguagemodels’robustnessagainstadver-
sarialattacksorimprovetheirsecurityorprivacy
4. Multilingual: novelpromptingmethodstoimprovelargelanguagemodels’performanceon
multilingualtasksorlow-resourcelanguagesandvernacularlanguages
5. Factuality:novelpromptingmethodsthatcanimprovefactualityandreducehallucinationof
largelanguagemodels
6. Math:novelpromptingmethodsforlargelanguagemodelstoimprovemathematicalproblem
solving
7. Uncertainty: novelpromptingmethodsthatcanbetterquantifyuncertaintyorcalibratethe
confidenceoflargelanguagemodels
WeusethesetopicsdescriptionstoelicitideasfrombothhumanparticipantsandourLLMagent.
26B ProjectProposalTemplate
WegivethefollowingprojectproposaltemplatetoboththeAIagentandhumanideawriters.
1.Title:Aconcisestatementofthemainresearchquestiontobeusedasthepapertitle.
2.ProblemStatement:Clearlydefinetheproblemyourresearchintendstoaddress.Explainclearly
whythisproblemisinterestingandimportant.
3.Motivation:Explainwhyexistingmethodsarenotgoodenoughtosolvetheproblem,andexplain
the inspiration behind the new proposed method. You should also motivate why the proposed
methodwouldworkbetterthanexistingbaselinesontheproblem.
4.ProposedMethod:Explainhowtheproposedmethodworks,describealltheessentialsteps.
5.Step-by-StepExperimentPlan:Breakdowneverysinglestepoftheexperiments,makesureevery
stepisexecutable.Coverallessentialdetailssuchasthedatasets,models,andmetricstobeused.If
theprojectinvolvesprompting,givesomeexamplepromptsforeachstep.
6.TestCaseExamples:Giveatleasttwoconcreteexamples.Thefirstexampleshouldshowhowthe
baselinemethodfailsonthetestcase. Iftherearemultiplebaselines,giveexamplesforallofthem.
Thesecondexampleshouldshowhowtheproposedmethodsucceedsonthetestcase.Foreachtest
case,includetheinput(testexampleandthefullprompt)andtheexpectedoutput.Youshouldalso
provideanexplanationforwhytheoutputsfromtheproposedpromptarebetter. Iftheproposed
methodhasmultiplesteps,breakthemdownintointermediatesteps.
7. FallbackPlan: Proposesomealternativeplansforwhatshouldthestudentsdoiftheproposed
method doesn’t manage to satisfy the success criteria. For example, you can suggest additional
analysistohelpdebugwhytheproposedmethoddidn’twork,whichcouldinformalternativenew
methods,orjustturntheprojectintoananalysispaperinsteadbyofferingsomeinterestingablation
andinsights.
27C ProjectProposalDemoExample
Wepresentamanuallywrittendemonstrationexampleusedforprojectproposalgeneration. The
exampleissummarizedfromanexistingpaper(Dhuliawalaetal.,2023).Thissameexampleisgiven
toboththeAIagentaswellastheidea-writingexperts.
1.Title:
Chain-of-VerificationReducesHallucinationinLargeLanguageModels
2.ProblemStatement:
Generationofplausibleyetincorrectfactualinformation,termedhallucination,isanunsolvedissue
inlargelanguagemodels.
3.Motivation:
Amajorityofthemethodsforreducinghallucinationcanbedividedintoroughlythreecategories:
training-timecorrection,generation-timecorrection,andviaaugmentation(tool-use). Wewantto
takeasimplerapproachthatfullyleveragesthepowerofLLMitself.Ourkeymotivationisthatlarge
languagemodels,whensuitablyprompted,canbothgenerateandexecuteaplanofhowtoverify
themselvesinordertochecktheirownwork,andfinallyincorporatethisanalysisintoanimproved
response.
4.ProposedMethod:
Ouroverallprocess,whichwecallChain-of-Verification(CoVe),thusperformsfourcoresteps:
(1) GenerateBaselineResponse:Givenaquery,generatetheresponseusingtheLLM.
(2) Plan Verifications: Given both query and baseline response, generate a list of verification
questionsthatcouldhelptoself-analyzeifthereareanymistakesintheoriginalresponse.
(3) ExecuteVerifications:Answereachverificationquestioninturn,andhencechecktheanswer
againsttheoriginalresponsetocheckforinconsistenciesormistakes.
(4) GenerateFinalVerifiedResponse: Giventhediscoveredinconsistencies(ifany),generatea
revisedresponseincorporatingtheverificationresults.
EachofthesestepsisperformedbypromptingthesameLLMindifferentwaystoobtainthedesired
response.
5.Step-by-StepExperimentPlan:
1: Gather Datasets: We choose datasets that evaluate factual correctness, including the Multi-
SpanQAdatasetonclosed-bookQAandtheFactScoredatasetongeneratingbiographies.
2: ConstructPrompts:Forthebaseline,weusedirectpromptingwhere,givenaquery,wegenerate
left-to-rightasusualusingtheLLM,withnospecialtricks.Giventhatsuchbaselinegenerations
aretypicallypronetohallucination,CoVeattemptstoidentifythesehallucinationsandcorrect
theminthefollowingsteps:
(1) PlanVerifications: Conditionedontheoriginalqueryandthebaselineresponse, the
modelispromptedtogenerateaseriesofverificationquestionsthattestthefactualclaims
intheoriginalbaselineresponse.
(2) ExecuteVerifications:Giventheplannedverificationquestions,thenextstepistoanswer
theminordertoassessifanyhallucinationsexist.Theplanningpromptconditionsonthe
baselineresponseinthefirststep.Theverificationquestionsgeneratedfromplanningare
answeredinthesecondstep,wherecruciallythecontextgiventotheLLMpromptonly
containsthequestionsandnottheoriginalbaselineresponse,hencepreventingtheLLM
fromrepeatingthoseanswersdirectly.
28(3) GenerateFinalVerifiedResponse: Finally,theimprovedresponsethattakesverifica-
tionintoaccountisgenerated. Thisisexecutedbyafinalfew-shotpromptwherethe
contexttakesintoaccountallofthepreviousreasoningsteps,thebaselineresponse,and
verificationquestion-answerpairs,sothatthecorrectionscantakeplace.
3: SelectModels:WetestGPT-3.5(Text-Davinci-003)andGPT-4fromtheOpenAIAPI,aswellas
theopen-sourceLLaMA-2-70B-chat.
4: GetResults:Getanswerpredictionsfromthemodelsonthesedatasetswithboththebaselines
andtheproposedmethod.
5: AnalyzeResults: ComparewhetherthenewmethodimprovestheperformanceofLLMsin
thesetasksascomparedtothebaselines.
6.TestCaseExamples:
• BaselinePromptInput(DirectPrompting):NamesomepoliticianswhowereborninNY,New
York.
• BaselinePromptExpectedOutput(DirectPrompting): Herearesomepoliticianswhowere
borninNY,NewYork:
1.HillaryClinton-formersecretaryofstateandformerDemocraticpresidentialnominee.
2.DonaldTrump-formerpresidentoftheUnitedStates.
3. MichaelBloomberg-formerMayorofNewYorkCityandformerDemocraticpresidential
candidate.
• ProposedPromptInput(Chain-of-VerificationPrompting;Step1:PlanVerifications):Generate
asetofverificationquestionstoverifyeachanswer.
• ProposedPromptExpectedOutput(Chain-of-VerificationPrompting;Step1:PlanVerifica-
tions):
– WherewasHillaryClintonborn?
– WherewasDonaldTrumpborn?
– WherewasMichaelBloombergborn?
• Proposed Prompt Input (Chain-of-Verification Prompting; Step 2: Execute Verifications):
Answereachoftheverificationquestions.
• ProposedPromptExpectedOutput(Chain-of-VerificationPrompting;Step2:ExecuteVerifica-
tions):
1. HillaryClintonwasborninChicago,Illinois,UnitedStatesonOctober26,1947.
2. DonaldTrumpwasbornonJune14,1946,inQueens,NewYorkCity,NewYork,United
States.
3. MichaelBloombergwasbornonFebruary14,1942,inBoston,Massachusetts,UnitedStates.
• ProposedPromptInput(Chain-of-VerificationPrompting;Step3:FinalResponse):(Prepend
theoriginalquestionandthebaselineresponseandtheverificationQApairs)Giventheverifica-
tionanswers,generatearevisedresponseincorporatingtheverificationresults.
• ProposedPromptExpectedOutput(Chain-of-VerificationPrompting;Step3:FinalResponse):
HerearesomepoliticianswhowereborninNY,NewYork:
1.DonaldTrump-formerpresidentoftheUnitedStates.
2.AlexandriaOcasio-Cortez-DemocraticmemberoftheU.S.HouseofRepresentatives.
• Explanation: Givenauserquery,alargelanguagemodelwithdirectpromptinggeneratesa
baselineresponsethatmaycontaininaccuracies,e.g.,factualhallucinations. Toimprovethis,
Chain-of-Verificationfirstgeneratesaplanofasetofverificationquestionstoask, andthen
29executesthatplanbyansweringthemandhencecheckingforagreement.Wefindthatindividual
verificationquestionsaretypicallyansweredwithhigheraccuracythantheoriginalaccuracyof
thefactsintheoriginallongformgeneration.Finally,therevisedresponsetakesintoaccountthe
verifications.
7.FallbackPlan:
Iftheproposedmethoddoesnothelpascomparedtothebaseline,analyzeeachstepoftheCoVe
processtoseeiftheverificationquestionsarerelevant,iftheanswerstotheverificationquestions
arecorrect,andwhetherthegeneratedfinalverifiedresponseisindeedimprovedoverthebaseline
responsebyconsideringtheverificationQApairs.Thiscanhelpusdebugtheproposedmethodor
turnthisintointerestinganalysisonthemodel’sabilitytoverifyandcorrectitsownresponses.
30D StyleStandardizationPrompt
StyleStandardizationPrompt
Youareawritingassistantspecializedineditingacademicwriting.Iwillgiveyouastudent’sresearchideaandan
ideatemplate.Yourtaskistoeditthestudent’sideatofollowthetemplate’sformat.
Studentidea:(Insertthestudent’sideahere)
Template:(Insertthetemplateideahere)
Make sure that you only edit the wording and formatting, including things like punctuation, capitalization,
linebreaks,andbulletpoints.Alsomakesuretoeditanyinformalwordingandphrasingtousevocabularythat
soundslikethetemplate’swritingstyle.Nootherchangesareallowedbeyondthese.
Themainsectionsshouldbeindexedclearlywithoutindentationatthebeginning.Thetitlesectiondoesnotneed
indexing;othersections,includingproblemstatement,motivation,proposedmethod,step-by-stepexperiment
plan,testcaseexamples,andfallbackplan,shouldbeindexed1to6. Eachsectioncanthenhavesub-bulletsfor
sub-sectionsifapplicable.Leaveanemptylineaftereachsection.
Youshouldusetabasindentationandmakesuretouseappropriatenestedindentationforsub-bullets.Allbullets
shouldhaveaclearhierarchysopeoplecaneasilydifferentiatethesub-bullets.Onlyleaveemptylinesbetween
sectionsandremoveanyextralinebreaks.Ifmanybulletpointsareclusteredtogetherinaparagraph,separate
themclearlywithindentationandappropriatebulletpointmarkers.Changetoanewlineforeachnewbulletpoint.
Forthefallbackplan,donotlistabunchofbulletpoints.Instead,condensethemintoonecoherentparagraph.
Forlinebreaks,avoidRawStringLiteralsorDoubleBackslasheswhenusing"\n",andchangethemtospacesor
tabs.
Forin-linecitations,ifthecitationmentionedtheauthor’slastname(like"(Sietal.,2023)"or"(Anetal.,2024)"),you
shouldkeepthemthere;butifthecitationisjustanumber(like"[1]"or"[3,4,5]"),youshouldjustremoveitanddo
somenecessaryrephrasingtomakethesentencestillsoundcoherentwithoutthereferences.
Apartfromminorrephrasingandchangingformatting,donotchangeanycontentoftheidea.Youmustpreserve
theexactmeaningoftheoriginalidea,donotchange,remove,oraddanyotherdetails.Donotdropanysections
(includingtestcaseexamples). Donotrenameanymodels,datasets,ormethods. Donotdropclarificationor
examplesinbracketsanddonotdropanydatasourcementions(e.g.,ChatbotArenaorWildchat)! Notethat
whenindexingtestcaseexamples,eachtestcaseexamplecouldhavemultiplestepsofinputsandoutputsandyou
shouldn’tgiveseparateindicestothem.Eachtestcaseexampleshouldbeawholesetofinput-outputpairsforthe
baseline(s)andproposedmethod.
Fortheproposedmethodsection,avoidanybigchanges.Ifthesectioncomesinasacoherentparagraph,youdon’t
havetobreakitdownintobulletpoints.Ifthesectionisalreadyinbulletpoints,youshouldkeepitthatway.Ifthe
sectionisamixofboth,youshouldkeepthebulletpointsandthecoherentparagraphastheyare.
Keepalltheclarificationandexamplesmentionedinallthesectionsanddonotremoveanyofthem(includingthose
inbrackets).
Formodelselection,ifanyversionofClaudeismentioned,changeittothelatestversionofClaude(Claude-3.5);if
anyversionofLLaMAismentioned,changeittothelatestversionLLaMA-3.Donotmakeanyothermodelchanges.
Nowdirectlygeneratetheeditedstudentideatomatchtheformatofthetemplate.
31E IdeaReviewForm
Weusethefollowingreviewformtoelicitreviewsfromallexpertreviewers. Reviewershaveone
weekoftimetofinisheachreview.
1.Name
2.Institution
3.Email
4.Consent
5. HonorCode:IconfirmthatIwillnotuseChatGPT,Claude,Gemini,oranyotherAItoolswhen
writingmyreviews.
6.Familiarity:Beforereviewingtheidea,pleaseindicatehowfamiliaryouarewiththegiventopicon
ascaleof1-5(thisisjustforustounderstandpotentialconfounders).
1. Youhaveneverreadaboutthistopicbefore
2. Youhavereadatleastonepaperonthistopic
3. Youhavereadmultiplepapersonthistopicbuthavenotpublishedanypaperonit
4. Youhaveco-authoredatleastonepaperonthistopic
5. Youhaveco-authoredmultiplepapersonthistopicorhavepublishedatleastonefirst-author
paperonthistopic
7. Experience: HaveyoureviewedformajorNLPorAIconferencesbefore(e.g.,*ACL,COLING,
NeurIPS,ICLR,ICML,AAAI)?
8.FullResearchIdeaProposal
9. NoveltyScore: Whethertheideaiscreativeanddifferentfromexistingworksonthetopic,and
bringsfreshinsights.Youareencouragedtosearchforrelatedworksonline.Youshouldconsiderall
papersthatappearedonlinepriortoJuly2024asexistingworkwhenjudgingthenovelty.
1. Notnovelatall-therearemanyexistingideasthatarethesame
2.
3. Mostlynotnovel-youcanfindverysimilarideas
4.
5. Somewhatnovel-therearedifferencesfromexistingideasbutnotenoughtoturnintoanew
paper
6. Reasonablynovel-therearesomenotabledifferencesfromexistingideasandprobablyenough
toturnintoanewpaper
7.
8. Clearlynovel-majordifferencesfromallexistingideas
9.
10. Verynovel-verydifferentfromallexistingideasinaveryinterestingandcleverway
10.NoveltyRationale:Shortjustificationforyourscore.Ifyougivealowscore,youshouldspecify
similarrelatedworks.(Yourrationaleshouldbeatleast2-3sentences.)
3211. FeasibilityScore: Howfeasibleitistoimplementandexecutethisideaasaresearchproject?
Specifically,howfeasibletheideaisforatypicalCSPhDstudenttoexecutewithin1-2monthsoftime.
YoucanassumethatwehaveabundantOpenAI/AnthropicAPIaccess,butlimitedGPUcompute.
1. Impossible:theideadoesn’tmakesenseortheproposedexperimentsareflawedandcannotbe
implemented
2.
3. Verychallenging:thereareflawsintheproposedmethodorexperiments,ortheexperiments
requirecompute/humanresourcesbeyondanyacademiclab
4.
5. Moderatelyfeasible:Itcanprobablybeexecutedwithinthegiventimeframebutwouldrequire
carefulplanning,efficientuseofAPIsorsomeadvancedcomputationalstrategiestoovercome
thelimitedGPUresources,andwouldrequiresomemodificationstotheoriginalproposalto
makeitwork
6. Feasible:Canbeexecutedwithinthegivenconstraintswithsomereasonableplanning
7.
8. HighlyFeasible:Straightforwardtoimplementtheideaandrunalltheexperiments
9.
10. Easy:Thewholeproposedprojectcanbequicklyexecutedwithinafewdayswithoutrequiring
advancedtechnicalskills
12.FeasibilityRationale:Shortjustificationforyourscore.Ifyougivealowscore,youshouldspecify
whatpartsaredifficulttoexecuteandwhy.(Yourrationaleshouldbeatleast2-3sentences.)
13. ExpectedEffectivenessScore: Howlikelytheproposedideaisgoingtoworkwell(e.g.,better
thanexistingbaselines).
1. ExtremelyUnlikely:Theideahasmajorflawsanddefinitelywon’tworkwell
2.
3. LowEffectiveness: Theideamightworkinsomespecialscenariosbutyoudon’texpectitto
workingeneral
4.
5. Somewhatineffective:Theremightbesomechancethattheproposedideacanworkbetterthan
existingbaselinesbuttheimprovementwillbemarginalorinconsistent
6. Somewhateffective:Thereisadecentchancethattheproposedideacanbeatexistingbaselines
bymoderatemarginsonafewbenchmarks
7.
8. ProbablyEffective:Theideashouldoffersomesignificantimprovementovercurrentmethods
ontherelevantbenchmarks
9.
10. DefinitelyEffective: Youareveryconfidentthattheproposedideawilloutperformexisting
methodsbysignificantmarginsonmanybenchmarks
3314.ExpectedEffectivenessRationale:Shortjustificationforyourscore.(Yourrationaleshouldbeat
least2-3sentences.)
15. ExcitementScore: Howexcitingandimpactfulthisideawouldbeifexecutedasafullproject.
Wouldtheideachangethefieldandbeveryinfluential.
1. Poor: Youcannotidentifythecontributionsofthisidea, orit’snotinterestingatallandyou
wouldfighttohaveitrejectedatanymajorAIconference
2.
3. Mediocre:thisideamakesmarginalcontributionsandisveryincremental
4.
5. Leaningnegative:ithasinterestingbitsbutoverallnotexcitingenough
6. Learningpositive:excitingenoughtobeacceptedatamajorAIconference,butstillhassome
weaknessesorsomewhatincremental
7.
8. Exciting:woulddeepenthecommunity’sunderstandingormakemajorprogressinthisresearch
direction
9.
10. Transformative:wouldchangetheresearchfieldprofoundlyandworthabestpaperawardat
majorAIconferences
16. ExcitementRationale: Shortjustificationforyourscore. (Yourrationaleshouldbeatleast2-3
sentences.)
17. OverallScore: Overallscore: Apartfromtheabove,youshouldalsogiveanoverallscorefor
theideaonascaleof1-10asdefinedbelow(MajorAIconferencesinthedescriptionsbelowreferto
top-tierNLP/AIconferencessuchas*ACL,COLM,NeurIPS,ICLR,andICML.):
1. Criticallyflawed,trivial,orwrong,wouldbeawasteofstudents’timetoworkonit
2. StrongrejectionformajorAIconferences
3. ClearrejectionformajorAIconferences
4. Okbutnotgoodenough,rejectionformajorAIconferences
5. Decentideabuthassomeweaknessesornotexcitingenough,marginallybelowtheacceptance
thresholdofmajorAIconferences
6. MarginallyabovetheacceptancethresholdofmajorAIconferences
7. Goodidea,wouldbeacceptedbymajorAIconferences
8. Top50%ofallpublishedideasonthistopicatmajorAIconferences,clearaccept
9. Top15%ofallpublishedideasonthistopicatmajorAIconferences,strongaccept
10. Top5%ofallpublishedideasonthistopicatmajorAIconferences,willbeaseminalpaper
18. OverallRationale: Youshouldalsoprovidearationaleforyouroverallscore. (Yourrationale
shouldbeatleast2-3sentences.)
3419.Confidence:Additionally,weaskforyourconfidenceinyourreviewonascaleof1to5definedas
following:
1. Yourevaluationisaneducatedguess
2. Youarewillingtodefendtheevaluation,butitisquitelikelythatyoudidnotunderstandcentral
partsofthepaper
3. Youarefairlyconfidentthattheevaluationiscorrect
4. Youareconfidentbutnotabsolutelycertainthattheevaluationiscorrect
5. You are absolutely certain that the evaluation is correct and very familiar with the relevant
literature
20.Time:Howmanyminutesdidyouspendonthistask?
35F IdeaGenerationAgent: AdditionalImplementationDetails
SeedIdeaGeneration DuetothemaxoutputlengthlimitoftheLLMAPI,wefirstgeneratealarge
numberofshorterseedideas. Wekeeptheseedideasshortsothatwecanexploremoredifferent
ideasgiventhesameoutputtokenbudget.Weprovideademonstrationexampleoftheseedideain
AppendixG.Then,weperformduplicationandexpandeachremainingseedideaintoafullproject
proposalfollowingourstandardtemplateinAppendixB.
RetrievalAugmentation Weapplyretrievalaugmentationtotheideagenerationpromptinorderto
increasediversityintheideageneration. Tomaximizediversity,weapplyretrievalaugmentation
halfofthetimewhengeneratingseedideas,andwerandomlyselectk=10papersfromthetop20
retrievedpaperswhenapplyingretrievalaugmentation.
IdeaFiltering Afterexpandingseedideasintofullprojectproposals,wedidsomebasicfilteringto
removeanyprojectproposalsthatfailedthenoveltyandfeasibilitychecks:
1. Novelty:Weusetheliteraturereviewmoduletoretrievethetop10mostrelevantpaperstothe
generatedideaandasktheLLMtocompareeachofthemtothegeneratedidea.Theideawillbe
filteredaslongasanyoneoftheretrievedpapersisjudgedasequivalent.
2. Feasibility:Theideawillbefilteredifitrequiresextensivemanuallabororhardwareresources
beyondthecapacityofatypicalacademiclab. Theideawillalsobefilteredifitinvolvesany
inconsistency in the experimental setups or assumptions. For example, if the idea assumes
onlyblack-boxAPIaccessoftheLLMs,thenitshouldn’tinvolveexperimentsthatneedinternal
weightaccess.
Thisfilteredoutabout1%ofthegeneratedprojectproposals.
36G DemonstrationExample: SeedIdeaGeneration
Wepresentademonstrationexampleusedforseedideageneration.Theexampleissummarizedfrom
anexistingpaper(Dhuliawalaetal.,2023).
Title:
Chain-of-VerificationPrompting
Problem:
Generationofplausibleyetincorrectfactualinformation,termedhallucination,isanunsolvedissue
inlargelanguagemodels.
ExistingMethods:
Amajorityofthemethodsforreducinghallucinationcanbedividedintoroughlythreecategories:
training-timecorrection;generation-timecorrection;andviaaugmentation(tool-use).
Motivation:
Akeyobservationisthatlargelanguagemodels,whensuitablyprompted,canbothgenerateand
executeaplanofhowtoverifythemselvesinordertochecktheirownwork,andfinallyincorporate
thisanalysisintoanimprovedresponse.
ProposedMethod:
Ouroverallprocess,whichwecallChain-of-Verification(CoVe),thusperformsfourcoresteps:
(1) GenerateBaselineResponse:Givenaquery,generatetheresponseusingtheLLM.
(2) Plan Verifications: Given both query and baseline response, generate a list of verification
questionsthatcouldhelptoself-analyzeifthereareanymistakesintheoriginalresponse.
(3) ExecuteVerifications:Answereachverificationquestioninturn,andhencechecktheanswer
againsttheoriginalresponsetocheckforinconsistenciesormistakes.
(4) GenerateFinalVerifiedResponse: Giventhediscoveredinconsistencies(ifany),generatea
revisedresponseincorporatingtheverificationresults.
EachofthesestepsisperformedbypromptingthesameLLMindifferentwaystoobtainthedesired
response.
ExperimentPlan:
Comparewithzero-shotprompting,Chain-of-Thought,andfew-shotpromptingontheMultiSpanQA
datasetonclosed-bookQAandFactScoredatasetongeneratingbiographies.
37H GeneratedSeedIdeasandTheirNearestNeighbors
Wepresentseveralrandomlysampledgeneratedseedideas(seeAppendixFforthedefinitionofseed
ideas)onthetopicof“novelpromptingmethodsthatcanbetterquantifyuncertaintyorcalibrate
theconfidenceoflargelanguagemodels”. Foreachidea,weshowthemostsimilaridea(nearest
neighbor)basedontheembeddingsimilarity,alongwiththesimilarityscore. Inpractice,weseta
thresholdthresholdof0.8fordeterminingwhethertwoideasareduplicates.
Idea1:
Title:AdaptivePrecisionBoundaryProbing
Problem:LLMsoftenprovideuncertaintyestimatesthatareeithertoocoarse-grainedorinappropri-
atelyprecise,failingtoadapttotheinherentambiguityorprecisionrequirementsofdifferentqueries.
ExistingMethods:Existinguncertaintyquantificationmethodstypicallyusefixedprecisionscales
orcalibrationtechniquesthatdon’tadapttothespecificcontextandprecisionrequirementsofeach
query.
Motivation:Humanexpertsadjusttheprecisionoftheiruncertaintyestimatesbasedonthenatureof
thequestionandtheavailableevidence.WecanincorporatethisadaptiveapproachtoimproveLLM
uncertaintyquantification.
ProposedMethod:WeintroduceAdaptivePrecisionBoundaryProbing(APBP),adynamicprompt-
ingtechniquethatiterativelyrefinestheprecisionofuncertaintyestimates. Givenaquery,APBP
startswithacoarse-grainedconfidenceinterval. Itthenpromptsthemodeltoassesswhetherthis
intervalisappropriatelyprecisegiventhequery’scontextandthemodel’sknowledge.Ifthemodel
determinesthatgreaterprecisioniswarranted,APBPiterativelynarrowstheinterval,prompting
themodelateachsteptojustifytheincreasedprecision. Conversely,ifthemodelrecognizeshigh
ambiguityorlimitedknowledge,APBPwidenstheinterval.Throughoutthisprocess,themodelis
askedtoexplicitlyreasonaboutthefactorsinfluencingtheappropriatelevelofprecision,suchasthe
specificityofthequery,thereliabilityofrelevantknowledge,andpotentialsourcesofambiguity.The
finaloutputisanuncertaintyestimatewithaprecisionleveltailoredtothespecificqueryandthe
model’sknowledgestate.
ExperimentPlan:WewillevaluateAPBPonadiversesetoftaskswithvaryinginherentprecision
requirements,includingnumericalestimation,dateprediction,andopen-endedtextgeneration.We’ll
compareAPBPagainstfixed-precisionuncertaintyestimationmethods,measuringbothcalibration
accuracyandtheappropriatenessofprecisionlevelsasjudgedbyhumanexperts.
NearestNeighborofIdea1:
Title:ContextualConfidenceOscillation
Problem:Currentmethodsforquantifyinguncertaintyinlargelanguagemodelsoftenfailtocapture
thedynamicnatureofconfidenceacrossdifferentcontextswithinasinglequery.
ExistingMethods:Mostexistingapproachesusestaticconfidencescoresorcalibrationtechniques
thatdon’taccountforintra-querycontextualshifts.
Motivation:Humanconfidenceoftenfluctuatesasweprocessdifferentpartsofacomplexquestion
or task. By mimicking this oscillation, we can potentially capture a more nuanced and accurate
representationofmodeluncertainty.
ProposedMethod:WeproposeContextualConfidenceOscillation(CCO),anovelpromptingtech-
niquethatencouragesthemodeltocontinuouslyre-evaluateandexpressitsconfidenceasitprocesses
aquery.Thepromptisstructuredasaseriesofcheckpoints,wherethemodelmustpauseitsreasoning,
reflectonitscurrentconfidencelevel,andexplainanychangessincethelastcheckpoint.Thiscreatesa
confidencetrajectorythatcanbeanalyzedforpatterns,suddendrops,orgradualincreases. Addi-
38tionally,weintroduce’confidencedisruptors’-intentionallyambiguousorchallengingsub-queries
insertedatvariouspointstotestthemodel’sabilitytorecognizeandexpressincreaseduncertainty
whenappropriate.
ExperimentPlan: WewillevaluateCCOagainststandarduncertaintyquantificationmethodson
arangeoftasks,includingmulti-stepreasoningproblems,ambiguousqueries,andlong-formtext
analysis. We’llmeasurenotjustoverallaccuracyofuncertaintyestimates,butalsothecorrelation
between confidence oscillations and human-annotated difficulty levels of different parts of each
query.We’llalsoanalyzehowwellthemodel’sexpressedconfidencetrajectoryalignswithitsactual
performanceacrossdifferentsegmentsofcomplextasks.
Similarity:0.70
Idea2:
Title:QuantumSuperpositionConfidencePrompting
Problem:CurrentLLMsstruggletoaccuratelyquantifyuncertaintyacrossmultiplepossibleanswers,
oftendefaultingtooverconfidenceinasingleresponse.
ExistingMethods:Existingapproachestypicallyinvolvesingle-pathreasoningorlimitedbranching,
failingtocapturethefullspectrumofuncertainty.
Motivation:Inspiredbyquantummechanics,whereparticlescanexistinmultiplestatessimultane-
ously,weproposeamethodthatallowsLLMstoconsidermultipleanswerpossibilitiesconcurrently.
ProposedMethod:WeintroduceQuantumSuperpositionConfidencePrompting(QSCP),wherethe
LLMisinstructedtogeneratemultiplepotentialanswerssimultaneously,assigningconfidencescores
toeach.Thepromptencouragesthemodelto’existinmultiplestates,’exploringcontradictoryan-
swersandtheirimplicationsconcurrently.Forexample:’Imagineyouareinaquantumsuperposition
ofmultipleexpertpersonas.Eachpersonawillprovideananswertothefollowingquestion,along
withaconfidencescore(0-100%).Ensurethepersonasexplorecontradictoryviewpoints.Question:
[INSERTQUESTION]’.TheLLMthengeneratesresponsesfrommultiplepersonas,eachwithitsown
confidencescore.Thefinaluncertaintyisderivedfromthedistributionofthesescores,providinga
morenuancedunderstandingofthemodel’sconfidenceacrosspossibleanswers.
ExperimentPlan:CompareQSCPagainststandardprompting,chain-of-thought,andotheruncer-
taintyquantificationmethodsondiversequestion-answeringdatasets.Evaluateusingmetricssuch
ascalibrationerror,Brierscore,andanovel’quantumuncertaintyscore’thatmeasuresthespreadand
coherenceofthegeneratedanswersuperposition.
NearestNeighborofIdea2:
Title:QuantumSuperpositionPrompting
Problem:Traditionalmethodsforuncertaintyquantificationinlargelanguagemodelsoftenfailto
capturethefullrangeofpossibleinterpretationsandoutcomes,especiallyforquerieswithinherent
ambiguityormultiplevalidperspectives.
Existing Methods: Current approaches typically focus on generating a single response with an
associatedconfidencescore,oratbest,asmallsetofdiscretealternatives.
Motivation:Drawinginspirationfromtheprincipleofsuperpositioninquantummechanics,wepro-
poseamethodtorepresentandreasonaboutmultiplepossibleoutcomessimultaneously,providinga
richerandmorenuanceduncertaintyquantification.
ProposedMethod: WepresentQuantumSuperpositionPrompting(QSP),anovelframeworkfor
exploringandquantifyinguncertaintyinlanguagemodeloutputs. QSPbeginsbypromptingthe
modeltogeneratea’superposition’ofpossibleinterpretationsorapproachestothegivenquery.Each
element in this superposition is assigned a complex amplitude, representing both its probability
39anditsrelationshiptootherelements.Themodelisthenguidedthroughaseriesof’measurement’
prompts,designedtocollapsethissuperpositionalongdifferentbasesofinterpretation.Thesemea-
surementsyieldprobabilitydistributionsoveroutcomes,capturingdifferentfacetsofuncertainty.
QSPemploystechniquesinspiredbyquantumcomputing,suchasinterferenceandentanglement,to
modelhowdifferentinterpretationsinteractandinfluenceeachother.Thefinaluncertaintyquantifi-
cationisderivedfromthefullsetofmeasurements,providingamulti-dimensionalrepresentationof
themodel’suncertaintythatcapturesambiguity,conflictingevidence,andtheinterdependenceof
differentinterpretations.
ExperimentPlan:WewillevaluateQSPontasksthatinherentlyinvolvemultiplevalidperspectives
orambiguousinterpretations,suchasethicaldilemmas,creativewritingprompts,andopen-ended
analyticalquestions.Metricswillincludethediversityandcoherenceofgeneratedsuperpositions,the
abilitytocapturehuman-judgedambiguities,andimprovementsinuncertaintycalibrationcompared
toclassicalmethods.
Similarity:0.77
Idea3:
Title:FractalUncertaintyDecomposition
Problem:LLMsoftenprovideoverlysimplisticuncertaintyestimatesthatfailtocapturethehierarchi-
calandnestednatureofuncertaintyincomplexknowledgedomains.
Existing Methods: Current uncertainty quantification methods typically produce flat, single-
dimensionalconfidencescoresthatdon’treflectthemulti-layeredstructureofknowledgeanduncer-
tainty.
Motivation: Byrecursivelydecomposingaqueryintosub-componentsandassessinguncertainty
atmultiplelevelsofgranularity,wecanconstructamorecomprehensiveandstructurallyinformed
uncertaintyestimate.
ProposedMethod:WeintroduceFractalUncertaintyDecomposition(FUD),apromptingtechnique
thatrecursivelybreaksdownaqueryintoahierarchicalstructureofsub-queries,assessinguncertainty
ateachlevel. Givenaninitialquery, FUDpromptsthemodeltoidentifykeysub-componentsor
aspectsofthequestion. Foreachsub-component,themodelprovidesananswerandaconfidence
estimate.Iftheconfidenceforasub-componentisbelowacertainthreshold,FUDrecursivelyapplies
thesamedecompositionprocesstothatsub-component. Thiscontinuesuntileitheramaximum
depthisreachedorallsub-componentshavehighconfidence. Theresultingstructureisatreeof
nestedconfidenceestimates.FUDthenaggregatestheseestimatesbottom-up,usingacombination
ofstatisticalmethodsandpromptedmeta-analysisbythemodel.Thefinaloutputisbothanoverall
uncertaintyestimateandadetailedmapoftheuncertaintystructure,showinghowconfidencevaries
acrossdifferentaspectsandlevelsofthequery.
ExperimentPlan:WewillevaluateFUDoncomplex,multi-facetedtaskssuchasscientificexplanation,
historicalanalysis,andtechnicaltroubleshooting.Wewillcompareitsperformancetoflatconfidence
estimationmethodsandotherhierarchicalapproaches.Evaluationmetricswillincludetraditional
calibrationmeasures,aswellasnewmetricsdesignedtoassessthequalityandinformativenessofthe
uncertaintydecomposition.WewillalsoconductcasestudiestodemonstratehowFUDcanprovide
moreactionableandinterpretableuncertaintyinformationinreal-worldscenarios.
NearestNeighborofIdea3:
Title:SemanticFractalDecomposition
Problem:Currentuncertaintyquantificationmethodsforlargelanguagemodelsoftenfailtocapture
thehierarchicalandself-similarnatureofconceptualunderstanding,leadingtoinconsistentconfi-
40denceestimatesacrossdifferentlevelsofabstraction.
ExistingMethods:Existingapproachestypicallyfocusonflat,single-leveluncertaintyestimatesor
simplehierarchicaldecompositionsthatdon’tfullycapturethecomplex,nestednatureofsemantic
understanding.
Motivation:Drawinginspirationfromfractalgeometry,wherepatternsrepeatatdifferentscales,we
proposeamethodthatrecursivelydecomposesconceptsandqueriesintoself-similarsub-components,
allowingforamorenuancedandscale-invariantapproachtouncertaintyquantification.
ProposedMethod:WepresentSemanticFractalDecomposition(SFD),apromptingtechniquethat
guidesthemodeltorecursivelybreakdownagivenqueryorconceptintosmaller,self-similarcom-
ponents.Ateachlevelofdecomposition,themodelisaskedtoprovideaconfidenceestimate.The
processcontinuesuntilapredefineddepthisreachedorthemodelindicatesitcannolongermeaning-
fullydecomposetheconcept.Thefinaluncertaintyestimateisthenconstructedbyaggregatingthese
multi-levelconfidencescoresusinganovelfractaldimension-inspiredalgorithm. Thisapproach
allowsforcapturinguncertaintythatmaybepresentatdifferentsemanticscalesandprovidesamore
robustandconsistentmeasureofthemodel’sconfidenceacrossvaryinglevelsofabstraction.
ExperimentPlan:WewillevaluateSFDonadiversesetoftasksrangingfromsimplefactualqueries
tocomplex,multi-facetedquestionsindomainslikephilosophy,science,andlaw.Wewillcompare
itsperformanceagainsttraditionalflatconfidenceestimationtechniquesandsimplerhierarchical
methods.Keymetricswillincludetheconsistencyofuncertaintyestimatesacrossrelatedqueriesat
differentlevelsofabstraction,thecorrelationbetweenfractal-aggregatedconfidencescoresandactual
modelperformance,andtheinterpretabilityofthedecompositionprocess.
Similarity:0.81
41I OverlapBetweenAIRankingandExpertReranking
WeshowtheoverlapbetweentheAI IdeasconditionandtheAI Ideas + Human Rerankcondi-
tionsinTable12.Wenotethat17outofthe49ideasintheAI Ideas + Human Rerankcondition
arealsorankedastopideasintheAI IdeasconditionbytheAIranker,whiletheother32arenot.
Topic Overlap New
Bias 2 2
Coding 4 5
Safety 2 3
Multilingual 5 5
Factuality 2 9
Math 2 2
Uncertainty 1 5
Total 18 31
Table12:OverlapofideasbetweenAI + Human RerankandAIconditions,brokendownbytopic.
J QualityControlofHumanExpertIdeas
Eachexpertisinstructedtochooseoneofthesevenspecifiedtopicsandwriteoneideaonitwithin10
days,followingthegiventemplateintheannotationdocument.Weincludedanhonorcodestatement
toasktheparticipantstonotuseanyAItoolsintheirideawriting.WecollectedN=50ideasoriginally
andmanuallycheckedallofthemforqualitycontrol.Wefilteredoutoneofthemasbeingessentiallya
paraphraseofanexistingpaper’sabstract.Wecompensatedtheparticipantneverthelessbutexcluded
themfromthereviewtask.
42K BreakdownofParticipantPositions
Weshowthedetailedpositionbreakdownofour49idea-writingparticipantsinTable13andthe
positionsofour79reviewerparticipantsinTable14.
Position Count
Postdoc 1
PhD 36
Master 9
Undergraduate 1
ResearchScientist 1
MachineLearningEngineer 1
Table13:Positionsofthe49ideawritingparticipants.
Position Count
Postdoc 7
PhD 63
Master 5
ResearchScientist 3
MachineLearningEngineer 1
Table14:Positionsofthe79ideareviewingparticipants.
43L InstitutionsoftheIdeaWritingParticipants
Institution Count
StanfordUniversity 11
UniversityofSouthernCalifornia 6
UniversityofMaryland 3
UniversityofIllinoisUrbana-Champaign 3
JohnsHopkinsUniversity 3
ColumbiaUniversity 2
CarnegieMellonUniversity 2
UniversityofPennsylvania 1
PrincetonUniversity 1
PennStateUniversity 1
PortlandStateUniversity 1
StonyBrookUniversity 1
UniversityofChicago 1
UniversityofWashington 1
UCBerkeley 1
UCSD 1
MassachusettsInstituteofTechnology 1
GeorgeWashingtonUniversity 1
YaleUniversity 1
UniversityofToronto 1
GeorgiaInstituteofTechnology 1
NationalUniversityofSingapore 1
PekingUniversity 1
TsinghuaUniversity 1
LinkedIn 1
NormAI 1
Table15:Institutionsofthe49ideawritingparticipants.
44M InstitutionsoftheIdeaReviewingParticipants
Institution Count
StanfordUniversity 25
UCBerkeley 4
UTAustin 4
UniversityofMaryland 4
PrincetonUniversity 3
UniversityofWashington 3
UniversityofSouthernCalifornia 3
CarnegieMellonUniversity 3
UniversityofChicago 2
JohnsHopkinsUniversity 2
UCLA 2
GeorgiaInstituteofTechnology 2
UniversityofIllinoisUrbana-Champaign 2
TsinghuaUniversity 2
StonyBrookUniversity 1
OhioStateUniversity 1
NationalUniversityofSingapore 1
UniversityofMichigan 1
DartmouthCollege 1
MassachusettsInstituteofTechnology 1
UniversityofPennsylvania 1
UniversityofToronto 1
PortlandStateUniversity 1
PennStateUniversity 1
NewYorkUniversity 1
ColumbiaUniversity 1
UCSantaBarbara 1
BrownUniversity 1
Amazon 1
LinkedIn 1
NormAI 1
AMD 1
Table16:Institutionsofthe79reviewerparticipants.
45N Mixed-EffectsModels
Onewaytocombineallthestatisticaltestsaboveistofitalinearmixed-effectsmodelwherewetreat
theconditionasthefixedeffectandotherfactorsincludingreviewerandideaasrandomeffects,while
alsoaccountingforthedifferencesamongdifferenttopics.Thisway,wecanrelyontheregressionto
accountforallthepossibleconfoundersastherandomeffects.Specifically,foreachmetric,wefitthe
followinglinearmixed-effectsmodel:
model = smf.mixedlm("Score ~ Condition", df,
groups=df["Topic"],
re_formula="~Condition",
vc_formula={"ReviewerID": "0 + C(ReviewerID)",
"IdeaID": "0 + C(IdeaID)"})
Thismixed-effectsmodelanalyzestherelationshipbetweenScoreandCondition,whileaccounting
forthehierarchicalstructureofthedata. FixedeffectsestimatetheaverageeffectofConditionon
Score.RandominterceptsforTopicallowforvaryingbaselinescoresacrosstopics,andrandomslopes
forConditionwithineachtopicallowtheeffectofConditiontovarybytopic.Additionally,variance
componentsforReviewerIDandIdeaIDaccountforvariabilityinscoresspecifictoindividualreviewers
andideas,respectively.
TheresultsareshowninTable17.Theinterceptsinthemixed-effectsmodelsrepresenttheestimated
meanscoreofthebaselinecondition,whichinthiscontextistheHuman Ideas.Thecoefficientsfor
Condition[AI Ideas]andCondition[AI Ideas + Human Rerank]inthemixed-effectsmodels
represent the difference in the mean score for each metric between the AI ideas and the baseline
(humanideas).Forexample,apositivecoefficientof0.761forthenoveltyscoremeansthatAI Ideas,
onaverage,score0.761pointshigherthanHuman Ideasonthenoveltyscoremetric;conversely,a
negativecoefficientof-0.330forthefeasibilityscoremeansthatAI Ideas,score0.330pointslower
thanHuman Ideasonfeasibilityonaverage.Thetopic(group)varianceinthemixed-effectsmodel
representsthevariabilityintheoutcomemetricthatcanbeattributedtodifferencesbetweenthe
topics, whichisrelativelysmallingeneral. Similarly, theideavarianceandreviewervariancein
the mixed-effects model represent the variability in the outcome metric that can be attributed to
differencesbetweenindividualideasandbetweenreviewers,respectively.Thereviewervariances
arehighingeneral,suggestingthatthereissubstantialvariabilityinhowdifferentreviewersratethe
sameideas.Thisimpliesthatreviewerdifferencesplayasignificantroleintheobservedscores,with
somereviewersconsistentlygivinghigherorlowerratings.
Overall,theresultsfromthemixed-effectsmodelsconfirmourmainconclusionthatAIideasarerated
assignificantlymorenovelthanhumanideas.
46Coef. SE p
NoveltyScore
Intercept 4.826 0.217 0.000***
Condition[AI Ideas] 0.756 0.331 0.023*
Condition[AI Ideas + Human Rerank] 0.902 0.305 0.003**
IdeaVar 0.412 0.178
ReviewerVar 0.803 0.202
ExcitementScore
Intercept 4.493 0.212 0.000***
Condition[AI Ideas] 0.626 0.303 0.039*
Condition[AI Ideas + Human Rerank] 0.879 0.298 0.003**
IdeaVar 0.495 0.227
ReviewerVar 0.782 0.167
FeasibilityScore
Intercept 6.595 0.224 0.000***
Condition[AI Ideas] -0.300 0.294 0.307
Condition[AI Ideas + Human Rerank] -0.183 0.314 0.561
IdeaVar 0.476 0.188
ReviewerVar 1.035 0.261
ExpectedEffectivenessScore
Intercept 5.156 0.211 0.000***
Condition[AI Ideas] 0.310 0.140 0.027*
Condition[AI Ideas + Human Rerank] 0.383 0.242 0.114
IdeaVar 0.200 0.151
ReviewerVar 0.469 0.141
OverallScore
Intercept 4.660 0.242 0.000***
Condition[AI Ideas] 0.137 0.294 0.640
Condition[AI Ideas + Human Rerank] 0.610 0.320 0.056
IdeaVar 0.262 0.154
ReviewerVar 1.071 0.225
Table17: Resultsoflinearmixed-effectsmodels. Weboldresultsthatarestatisticallysignificant
(∗p<0.05;∗∗p<0.01;∗∗∗p<0.001). OurmainconclusiononAIideasbeingmorenovelthanhuman
ideasstillholdshere.
47O ScoreBreakdownbyTopic
Weshowthebreakdownofallscoresacrossallconditionsbytopic. Notethatduetothesmaller
samplesizesfortheper-topicbreakdown,mostresultsarenotstatisticallysignificantandonlyoffer
anintuitiveunderstandingofthetrends.
Novelty Excitement Feasibility Effectiveness Overall
8 *
* * *
6
4
2
0 Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank
8 Novelty Excitement Feasibility Effectiveness Overall
6
4
2
0 Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank
Novelty Excitement Feasibility Effectiveness Overall
8
6
4
2
0 Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank
Novelty Excitement Feasibility Effectiveness Overall
7
6
5
4
3
2
1
0 Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank
Novelty Excitement Feasibility Effectiveness Overall
8
6
4
2
0 Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank
Novelty Excitement Feasibility Effectiveness Overall
8
6
4
2
0 Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank
Novelty Excitement Feasibility Effectiveness Overall
7
6
5
4
3
2
1
0 Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank Human AI AI+Rerank
Figure5:Breakdownofallscoresbytopic.
48
laugnilitluM
ytilautcaF
saiB
ytniatrecnU
ytefaS
htaM
gnidoCP ExampleIdea: ModularCalibrationforLong-formAnswers
ModularCalibrationforLong-formAnswers(Part1)
1.ProblemStatement:CalibratingtheconfidenceofLargeLanguageModels(LLMs)whengeneratinglong-form
answers,suchasessaysandcode,remainsanopenchallengeinthefieldofnaturallanguageprocessing.
2. Motivation: While numerous methods have been developed to calibrate the performance of LLMs on
multiple-choicequestionsoropen-domainquestionswithshortanswers,extendingtheseapproachestotasks
requiring lengthy responses presents significant difficulties. For instance, in code generation tasks (e.g., the
HumanEvaldataset),traditionalconfidenceextractionmethodslikeperplexitymayproveinadequatedueto
thesubstantialvariationinanswerlengthacrossquestions.Verbalizedconfidencecanbeaffectedbyinstruction
tuningartifactsorunclearscope,whilethereliabilityofmetricssuchasExpectedCalibrationError(ECE)and
Macro-averagedCalibrationError(MacroCE)maybecompromisedbydifferencesintasksettings. Ouraimis
toproposeanovelpipelineforconfidenceextractionandcalibrationofLLMsforlong-formanswers,drawing
inspirationfrommethodsusedforshortorfixed-setanswers.Thisapproachwillenableustomonitorthemodel’s
long-formanswergenerationprocessandapplytargetedexternalaugmentationwhennecessary,therebyenhancing
bothperformanceandefficiency.
3.ProposedMethod:WeintroduceModularCalibration,aprocesscomprisingfourcoresteps:
1. Extend:Promptthemodeltoelaborateontheoriginalquestioninrelationtotheanswer,identifyingwhich
componentsofthequestionareaddressedinthelong-formresponse.
2. Decompose:InstructtheLLMtobreakdowntheextendedquestionandlong-formanswerintomultiple
modules.
3. ExtractConfidence:Utilizeverbalizedconfidenceorperplexitytodeterminetheconfidencelevelforeach
module.
4. Merge:Basedontherelationshipsbetweenthemodularquestions/answersandtheoverallquestions/an-
swers,promptthemodeltocombinethemodularconfidencescoresintoanoverallscorerepresentingthe
confidenceinthelong-formanswer.
EachofthesestepsisexecutedbypromptingthesameLLMindifferentwaystoelicitthedesiredresponse.
4.Step-by-StepExperimentPlan:
1. GatherDatasets:Selectdatasetsfeaturinglonganswerswithcorrectnessannotations.Potentialcandidates
includeGSM8K,CodeGen,andEssayWriting.
2. ConstructPrompts:
(a) Establishabaselineusingdirectprompting,whereaqueryispresentedwithoutspecialtechniques.
(b) AnalyzeoutputstorefinepromptsfortheExtendandDecomposesteps.
(c) FortheConfidencestep,employvanillaperplexityorverbalizedconfidenceextraction.Ifperformance
isunsatisfactory,exploreadvancedmethodsbuiltuponthesetechniques,suchasthosepresentedin
recentresearch(e.g.,FaRpaper).
3. SelectModels:EvaluateGPT-3.5(Text-Davinci-003)andGPT-4fromtheOpenAIAPI,aswellastheopen-
sourceLLaMA-3-70B-chat.
4. GetResults: Obtainconfidencepredictionsfromthemodelsontheselecteddatasetsusingbothbaseline
methodsandtheproposedModularCalibrationapproach.
5. AnalyzeResults:ComparethecalibrationperformanceofLLMsusingthenewmethodagainstthebaselines
(e.g.,theperplexityoftheentirelong-formanswer).Conductqualitativeandquantitativeanalysesoneach
componentoftheModularCalibrationprocess.
49ModularCalibrationforLong-formAnswers(Part2)
5.TestCaseExamples:
• TestCase1:VerbalizedConfidencePrompting
– Input:<Q><A>Confidence(0-1)
– Output:[Modelgeneratesaconfidencescorebetween0and1]
• TestCase2:ModularCalibrationStep1(Extend)
– Input:Giventheanswer,canyouextendthequestionandelaborateonwhatpointsarecoveredinthe
answer?
– Output:Theanswercoversthesepointsofthequestion:(1)howfastAruns;(2)howfastBruns;(3)ifA
isfasterthanB.
• TestCase3:ModularCalibrationStep2(Decompose)
– Input:Pleasedecomposetheaboveextendedquestionandanswersintomodules.
– Output:
* HowfastAruns:[relevantexcerptfromtheoriginalanswer]
* HowfastBruns:[relevantexcerptfromtheoriginalanswer]
[Additionalmodulesasneeded]
• TestCase4:ModularCalibrationStep3(Extract)
– Input:HowfastAruns:[relevantexcerptfromtheoriginalanswer]Confidence(0-1)
– Output:1.0.9;2.0.6[Additionalconfidencescoresforothermodules]
• TestCase5:ModularCalibrationStep4(Merge)
– Input:ForeachofthesepointsrelatedtoquestionX,theconfidenceis:0.9,0.6,...Whatistheoverall
confidenceforthewholeproblem?
– Output:[Modelgeneratesanoverallconfidencescore]
6.FallbackPlan:IftheproposedModularCalibrationmethoddoesnotdemonstrateimprovementoverthebaseline,
wewillexecuteeachsub-questionandmoduleindividuallytoassesswhethercalibrationisenhancedforeach
component. Thisapproachwillfacilitatedebuggingoftheproposedmethodandpotentiallyyieldinteresting
insightsintotherelationshipsbetweenperformance/calibrationofdecomposedmodulesandoverallproblems.
Alternatively,wemayanalyzethemodel’sabilitytoeffectivelydecomposequestionsandanswersintoappropriate
modules. Theseanalyseswillinformpotentialrefinementstothemethodorprovidevaluableinsightsintothe
limitationsandcapabilitiesofLLMsinhandlingcomplex,long-formresponses.
50Reviewer1
Novelty:6(reasonablynovel-therearesomenotabledifferencesfromexistingideasandprobablyenoughtoturn
intoanewpaper)
Rationale: Focusonthelong-formsettingisnovelatthemoment. Theideaofobtainingmodularconfidence
estimatesfordifferentclaimsinalong-formoutput,andsynthesizingthemintoasingleuncertaintyestimateisnot
thatcomplicated,butitdoesseemtobeunderexplored.
Feasibility:8(HighlyFeasible:Straightforwardtoimplementtheideaandrunalltheexperiments.)
Rationale:Theonlypartoftheprojectthatseemschallengingisobtainingcorrectnessannotationsforoneofthe
datasets(e.g.,EssayWriting).GSM8KandcodedatasetslikeHumanEvalseemlikeverynaturallong-formoutput
settingstotryouttheidea.Otherthanthis,iteratingonthepromptsfordecomposition/verbalizedUQforeachof
themoduleswillbeimportant,buttheauthormentionsthis.
ExpectedEffectiveness:6(Somewhateffective:Thereisadecentchancethattheproposedideacanbeatexisting
baselinesbymoderatemarginsonafewbenchmarks.)
Rationale:It’spossiblethatfirstobtainingverbalizeduncertaintyestimatesforeachmodule,andthensynthesizing
intoasinglescore,willoutperformthestandardbaselinesofself-consistencyovertheentirelong-formoutput(using
majorityvoteastheconfidencescore).However,Idon’texpectthistobedramaticallybetter.Ifthepaperinsteadset
outwiththegoalofactuallyproducingtheUQestimatesforeachclaim,thenalmostnopriorworkdoesthis,andthe
baselineswouldbelessstrong.
Excitement:5(Leaningnegative:ithasinterestingbitsbutoverallnotexcitingenough)
Rationale:Thisseemslikethemoststraightforwardpossiblewaytoobtainuncertaintyestimatesforalong-form
generationwithanLLM.Thismeanstheprojectcouldproducesomeusefulengineeringartifacts,butitdoesn’t
reallypushtheideatoitslogicalconclusion.ThereforeIdon’tconsiderit"excitingenough".Thereissomemention
of"usingtheuncertaintyestimatestopossiblyconditiononmoreinformation"butthisisnotfleshedout–itcould
bemoreinteresting.Forexample,studyinghowthefine-graineduncertaintyestimatescouldbeusedtoselectively
retrievefactualinformationfromWikipediaetc.onaknowledge-intensivetask.
OverallScore:5(Decentideabuthassomeweaknessesornotexcitingenough,marginallybelowtheacceptance
thresholdofmajorAIconferences)
Rationale:Ilikethefocusonlong-formgenerations. However,thisproposalisaverystraightforwardbaseline
andextensionofexistingworktothelong-formgenerationsetting(justproducethelonggeneration,decomposeit,
applyverbalizeduncertaintyoneachclaim,andfinallyaggregatethem).Icouldseethepaperbeingwell-cited,butI
don’tseeaninteresting/novelanglehere.
Confidence:5(Youareabsolutelycertainthattheevaluationiscorrectandveryfamiliarwiththerelevantliterature)
51Reviewer2
Novelty:6(reasonablynovel-therearesomenotabledifferencesfromexistingideasandprobablyenoughtoturn
intoanewpaper)
Rationale: While existing works have explored the problem of calibration in long-form answers (e.g.
https://arxiv.org/abs/2402.06544),thespecificmethodforcalibrationisdifferent.AlsoseemsrelatedtoFactScore
(https://arxiv.org/abs/2305.14251)wherethetaskwasdifferent(gettingafactualityscore)buttheideaofbreaking
longformgenerationsintosmallerunits,evaluatingeachseparatelyandthencombingdoesseemrelated.
Feasibility:8(HighlyFeasible:Straightforwardtoimplementtheideaandrunalltheexperiments.)
Rationale:TheideaseemssimpleenoughtoimplementwithAPIaccess,consideringallthestepsinvolvedinthe
methodcanbedoneviapromptingwithAPI.TheproposaldoesmentionusingLLaMA3-70Basanadditional
model,whichwouldrequireGPUsIguess.
ExpectedEffectiveness:6(Somewhateffective:Thereisadecentchancethattheproposedideacanbeatexisting
baselinesbymoderatemarginsonafewbenchmarks.)
Rationale:SinceithasbeenshownthatLLMsarequitewellcalibratedwhenaskedtoverbalizetheconfidence
forshortanswers,I’mguessingthecalibrationscoreswouldbeprettygoodforindividualmodules.AlsoLLMs
mightbedecentatcombiningconfidencescores(especiallywithdetailedinstructionsandsomeexamplesinthe
prompt),sooverallthemethodmightworkwell.Butit’sunclearifitwoulddobetterthanthemethodsproposedin-
https://arxiv.org/abs/2402.06544.
Excitement: 6(Learningpositive: excitingenoughtobeacceptedatamajorAIconference,butstillhassome
weaknessesorsomewhatincremental)
Rationale:Ifthemethoddoesworkwellingettingcalibrationforlong-formanswers,Ithinkthatwouldbepretty
exciting.Onethingwhichismissingfromtheproposal(andwhythescorewasnothigher)wasthatitdoesnottouch
upontheissuethatforlong-formanswerswewon’thaveabinarycorrect/incorrectdecisionbutanswerscanbe
partiallycorrect.
OverallScore:6(MarginallyabovetheacceptancethresholdofmajorAIconferences)
Rationale:Theoverallideamakessensetome,butthescoreisnothigherrightnowbecause:(a)it’sunclearwhat
exactlyismeantby’modules’especiallyforessaywritingwhichtheproposalmentionsasoneofthetasks;(b)the
issueforpartialcorrectnesswhichwasmentionedabove.
Confidence:3(Youarefairlyconfidentthattheevaluationiscorrect)
52Q ExampleIdea: SemanticResonanceUncertaintyQuantification
SemanticResonanceUncertaintyQuantification(SRUQ)(Part1)
1. ProblemStatement: CurrentuncertaintyquantificationmethodsforLargeLanguageModels(LLMs)often
relyonsimplestatisticalmeasuresormodel-specificattributes,whichmaynotcapturethenuancedsemantic
uncertaintiesincomplexreasoningtasks. Thislimitationcanleadtooverconfidentorpoorlycalibratedmodel
outputs,potentiallyresultinginunreliabledecision-makingincriticalapplications.
2. Motivation: Existing approaches typically use softmax probabilities, entropy measures, or ensemble
disagreementtoquantifyuncertainty. However,thesemethodsoftenfailtocapturethesemanticnuancesand
reasoningcomplexitiesintasksthatrequiredeepunderstandingandmulti-stepreasoning. Humanexperts,on
theotherhand,gaugetheiruncertaintybyconsideringhowwelltheirreasoning’resonates’withtheirbroader
knowledgeandexperience. BymimickingthisprocessinLLMs,wecanpotentiallydevelopamorerobustand
semanticallygroundedapproachtouncertaintyquantification.
3.ProposedMethod:WeproposeSemanticResonanceUncertaintyQuantification(SRUQ),whichpromptsthe
LLMtogeneratemultipleindependentreasoningpathsforagivenproblem,thenquantifiesuncertaintybasedon
thesemanticcoherenceandmutualreinforcementamongthesepaths.Theprocessinvolvesfivekeysteps:
1. Generatingdiversesolutionattemptsusingdifferentpromptingstrategies.
2. Cross-evaluatingeachsolutionattemptagainsttheothers,assessinglogicalconsistencyandmutualsupport.
3. Constructinga’resonancegraph’wherenodesaresolutionattemptsandedgesrepresentsemanticreinforce-
ment.
4. Computingaresonancescorebasedongraphpropertieslikeconnectivityandcentrality.
5. Mappingtheresonancescoretoacalibrateduncertaintyestimate.
53SemanticResonanceUncertaintyQuantification(SRUQ)(Part2)
4.Step-by-StepExperimentPlan:
1. DatasetPreparation
• Utilizethreedatasetscoveringdifferentreasoningtasks:
(a) GSM8Kformathematicalproblem-solving
(b) EntailmentBankforlogicaldeduction
(c) HotpotQAformulti-hopquestionanswering
• Spliteachdatasetintotrain,validation,andtestsetsifnotalreadydone.
2. BaselineImplementation
• Implementthreebaselineuncertaintyquantificationmethods:
(a) Softmaxprobabilities
(b) MonteCarloDropout
(c) Ensembledisagreement(usingdifferentfew-shotprompts)
• Generatepredictionsanduncertaintyestimatesonthevalidationandtestsetsforeachbaseline.
3. SRUQImplementation
(a) Generate5diversesolutionattemptsusingdifferentfew-shotpromptsandtemperaturesettings.
(b) Foreachpairofsolutions,prompttheLLMtoevaluatetheirconsistencyandmutualsupport.
(c) Constructtheresonancegraphusingthepairwiseevaluations.
(d) Computetheresonancescoreusinggraphcentralitymeasures(e.g.,PageRank).
(e) Maptheresonancescoretoacalibrateduncertaintyestimateusingisotonicregressiononthevalidation
set.
4. Evaluation
• CompareSRUQagainstthebaselinesusingthefollowingmetrics:
(a) ExpectedCalibrationError(ECE)
(b) Brierscore
(c) AreaUnderthePrecision-RecallCurve(AUPRC)foruncertaintyranking
• Evaluatethecorrelationbetweenuncertaintyestimatesandactualerrors.
5. Analysis
• Visualizetheresonancegraphsforhighandlowuncertaintyexamples.
• Analyzetherelationshipbetweengraphpropertiesandpredictionaccuracy.
• InvestigatecaseswhereSRUQsignificantlyoutperformsorunderperformscomparedtobaselines.
6. AblationStudies
• Varythenumberofsolutionattempts.
• Comparedifferentgraphcentralitymeasures.
• Evaluatetheimpactofthecross-evaluationstep.
7. GeneralizationTest
• TestthegeneralizationofSRUQonout-of-distributionsamplesbyapplyingthemethodtrainedonone
datasettoexamplesfromtheotherdatasets.
54SemanticResonanceUncertaintyQuantification(SRUQ)(Part3)
5.TestCaseExamples:
• BaselineExample:
– Input:Q:Ifatraintravelsat60milesperhour,howfarwillittravelin2.5hours?
– SoftmaxOutput:Thetrainwilltravel150milesin2.5hours.(Confidence:0.92)
– Explanation:Thesoftmaxprobabilityishigh,butitdoesnotcapturethereasoningprocessorpotential
uncertaintiesinthecalculation.
• SRUQExample:
– Input:Q:Ifatraintravelsat60milesperhour,howfarwillittravelin2.5hours?
– SolutionAttempts:
1. Distance=Speed×Time
* Distance=60miles/hour×2.5hours
* Distance=150miles
2. In1hour,thetraintravels60miles
* In2hours,it’s120miles
* In0.5hours,it’s30miles
* Total:120+30=150miles
3. Let’sbreakitdown:
* 1hour:60miles
* 1hour:60miles
* 0.5hour:30miles
* Sum:60+60+30=150miles
– Cross-Evaluation: Allthreesolutionsareconsistentandmutuallysupportive. Theyusedifferent
approachesbutarriveatthesameresult.
– ResonanceGraph:Fullyconnectedgraphwithhighedgeweights
– ResonanceScore:0.95
– CalibratedUncertainty:0.05
– FinalOutput:Thetrainwilltravel150milesin2.5hours.(Uncertainty:0.05)
– Explanation: SRUQgeneratesmultiplesolutionpaths, evaluatestheirconsistency, andquantifies
uncertaintybasedontheirsemanticresonance. Thehighresonancescoreindicateslowuncertainty,
whichisthencalibratedtoprovideafinaluncertaintyestimate.
6.FallbackPlan:IfSRUQdoesnotsignificantlyoutperformbaselines,wecanpivottoananalysispaperexploring
whysemanticresonancemightnotcaptureuncertaintyeffectively.Wecouldinvestigatethequalityanddiversityof
generatedsolutionattempts,potentiallyimprovingthepromptingstrategies.Additionally,wecouldexaminethe
effectivenessofthecross-evaluationstep,possiblyincorporatingexternalknowledgeormorestructuredreasoning.
Furthermore,wecouldexploretherelationshipbetweengraphpropertiesandactualuncertainty,whichmight
revealinsightsabouthowLLMsrepresentconfidenceinternally.WecouldalsoconsidercombiningSRUQwith
traditionaluncertaintyquantificationmethods,creatingahybridapproachthatleveragesbothstatisticaland
semanticinformation.
55Reviewer1
Novelty:6(reasonablynovel-therearesomenotabledifferencesfromexistingideasandprobablyenoughtoturn
intoanewpaper)
Rationale:Ihaven’tseen(andcouldn’tfind)anypriorworkwhichexactlyhasthesameideaasinthisproposal.
Theproposedideaisdefinitelyrelatedtousingconsistencyamongmultiplesolutionstoestimateuncertainty(e.g.
https://arxiv.org/abs/2405.18711doesthisacrosssolutionsdecodedfromdifferentlayers)butIhavenotseenthe
ideaofconstructingresonancegraphandusinggraphpropertiestoestimateuncertainty.
Feasibility:8(HighlyFeasible:Straightforwardtoimplementtheideaandrunalltheexperiments.)
Rationale: Theproposedmethod, SRUQ,shouldbeprettyeasytoimplementgiventhatLLMAPIaccessis
abundant.SRUQinvolvesmultiplestepsallofwhichcanbedonethroughpromptingviaAPI—gettingmultiple
solutions,promptingLLMstogetaconsistencyscorebetweeneachpairofsolutionsetc.Thepartswhichcannot
beimplementedthroughAPIarethebaselinese.g.MonteCarlodropout,andwouldrequireGPUs.Todoafair
comparisontothebaselines,IimagineSRUQwillalsohavetobedoneonopenmodelswhichcouldalsorequire
GPUs.
ExpectedEffectiveness:6(Somewhateffective:Thereisadecentchancethattheproposedideacanbeatexisting
baselinesbymoderatemarginsonafewbenchmarks.)
Rationale: Althoughtheproposalincludessomebaselinesthatshouldbecomparedto, itdoesnotmention
somemethodswhichseemtodoquitewellwithLLMs(especiallygettingbetterwithscale)–e.g. methodslike
P(True)(https://arxiv.org/abs/2207.05221)orverbalizedconfidence(https://arxiv.org/abs/2305.14975).It’snot
clear/obvioustomethattheproposedmethodshoulddobetterthanthesebaselines.
Excitement: 6(Learningpositive: excitingenoughtobeacceptedatamajorAIconference,butstillhassome
weaknessesorsomewhatincremental)
Rationale:Whilethemethodisnovelandfeasible,I’mnottooexcitedbyitsincesomeoftheotherexistingmethods
outtherementionedabove(likehttps://arxiv.org/abs/2207.05221,https://arxiv.org/abs/2305.14975)aremuch
simplerandworkquitewell.ComparedtothatSRUQismorecomplex,andhencemaybehaslesschanceofbeing
veryimpactful(unlessitworksreallybetter).
OverallScore:6(MarginallyabovetheacceptancethresholdofmajorAIconferences)
Rationale:Theaboveacceptscoreisassumingtheideadoesworkbetterthanthebaselinesonsomecategoryof
tasks.Overall,giventhattheideaisnovel,theproposalincludescomparisontootherbaselinesaswellanalysis&
ablations,IthinkthatcouldbeenoughtogetacceptedintoanAIconference.
Confidence:4(Youareconfidentbutnotabsolutelycertainthattheevaluationiscorrect)
56Reviewer2
Novelty:6(reasonablynovel-therearesomenotabledifferencesfromexistingideasandprobablyenoughtoturn
intoanewpaper)
Rationale:Theproposedapproachsharessomesimilarideaswithself-consistency(whichsuggeststheconsistency
ofsampledLLMsoutputsisrelativelywellcalibrated).Buttheapproachismoregeneralizedandfine-grainedthan
existingworkiftheapproachusesmoreadvanced‘mutualsupportevaluation‘beyondsimplycomparingthefinal
answers.
Feasibility:5(Moderatelyfeasible:Itcanprobablybeexecutedwithinthegiventimeframebutwouldrequire
carefulplanning,efficientuseofAPIsorsomeadvancedcomputationalstrategiestoovercomethelimitedGPU
resources,andwouldrequiresomemodificationstotheoriginalproposaltomakeitwork.)
Rationale:Therelackssomeimportantdetailsintermsofthecross-evaluationpart.Howisthemutualsupport
evaluated(bypromptingorsomeothermethods?).Thispartiscrucialforimplementingthewholepipelineofthis
approach.
ExpectedEffectiveness:6(Somewhateffective:Thereisadecentchancethattheproposedideacanbeatexisting
baselinesbymoderatemarginsonafewbenchmarks.)
Rationale: Ithinkithassomechancestobeattheproposedbaselines. Ifthecross-evaluationpartisproperly
executed.Again,thesuccessofthisproposalishighlydependentonthatpart.
Excitement: 6(Learningpositive: excitingenoughtobeacceptedatamajorAIconference,butstillhassome
weaknessesorsomewhatincremental)
Rationale:Ifthisideaactuallyworks,atleastittellssomethingnewabouthowtousemultiplesamplestoprovide
betterconfidenceestimationthansimpleconsistency. Buttheideaitselfisstillsomewhatincrementalgiventhe
existenceofcurrentconsistency-basedcalibrators.
OverallScore:6(MarginallyabovetheacceptancethresholdofmajorAIconferences)
Rationale:Overalltherearesomeincrementalcontributions,butnottooexciting.Thealgorithmitselfcanbeneat.I
thinkitcanbeworthaborderlineacceptance.
Confidence:4(Youareconfidentbutnotabsolutelycertainthattheevaluationiscorrect)
57Reviewer3
Novelty:6(reasonablynovel-therearesomenotabledifferencesfromexistingideasandprobablyenoughtoturn
intoanewpaper)
Rationale:Ithinktheideaisreasonableandindeedidentifiessomelimitationsofcurrentworksonuncertainty
estimation.However,theconsistencybetweenreasoningpathsissomehowsimilartoself-consistencyreasoning
fromGoogleandSelfCheckGPT.
Feasibility:7
Rationale:IthinkitcouldbeeasytoimplementandquicklybetriedbyPhDstudentsorevenundergrads.Also,in
thetestcaseexample,thesettingisstraightforwardandwell-defined.
ExpectedEffectiveness:6(Somewhateffective:Thereisadecentchancethattheproposedideacanbeatexisting
baselinesbymoderatemarginsonafewbenchmarks.)
Rationale:Basedonmyexperience,theconsistency-basedmethods,althoughnotfullytheoreticallygrounded,can
workprettywellincurrentuncertaintyestimationquestions.Ibelieveworkingthisonthereasoningpathlevel
couldalsoworktosomeextent.
Excitement: 6(Learningpositive: excitingenoughtobeacceptedatamajorAIconference,butstillhassome
weaknessesorsomewhatincremental)
Rationale:Overall,thisideaidentifiedagoodresearchquestion,althoughthemethodmightnotbeveryexcitingto
me.
OverallScore:6(MarginallyabovetheacceptancethresholdofmajorAIconferences)
Rationale:Thenoveltyandtheactualapplicationofthismethodintheareaislimited,butcouldbeaninspiringidea.
Confidence:4(Youareconfidentbutnotabsolutelycertainthattheevaluationiscorrect)
58R ExampleIdea: TranslationwithLLMsthroughPromptingwith
Long-FormContext
TranslationwithLLMsthroughPromptingwithLong-FormContext(Part1)
1.ProblemStatement:Stablegenerationoftextinlow-resourcelanguagesisanunsolvedissueinlargelanguage
models.
2. Motivation: While LLMs can often produce surprisingly good translations despite not being explicitly
trained for this task, this does not hold for lower-resource languages. LLMs are both more likely to gener-
ate off-target text (text in another language than intended) when prompted to translate to a lower-resource
language,andshowincreasedinstabilityintranslationqualityacrossprompttemplatesinlower-resourcelanguages.
3. ProposedMethod:Ourproposedmethodinvestigatestheuseoflong-formtemplatestoimprovegenerated
translationqualityandreduceoff-targettranslationsinlower-resourcelanguages.Weproposetoprovideadditional
promptcontextbytranslatingmulti-sentenceinput,withadditionalviewsofthetargetlanguagewiththelangid
templateprovidedascontext.Wedosoinmultiplestages:
1. Queryingthelanguagemodeltofirstgenerateaparagraphcontainingthesourcesentencetobetranslated.
2. Prependingmonolingualtextinthetargetlanguage,withlangid:tags,abovethetranslationprompt.
3. Presentingboththeseadditionalsourcesofcontent,promptingtheLLMforatranslation.
4.Step-by-StepExperimentPlan:
1. Choosedatasets: EvaluateontheFLORES-200datasets,whichallowforwidelanguagecoverageonthe
Wikipediadomain,aswellastheWMT-21testsetsfornewsandlaw/medicaldomain.
2. Chooselanguages:OptforEnglish-centrictranslationwith:
• 5high-resourcelanguageswithdifferentscripts(French,German,Russian,Chinese,Japanese)
• 5mid-resourcelanguages(Farsi,Vietnamese,Arabic,Korean,Hebrew)
• 5low-resourcelanguageswithconsiderablylowerlikelihoodofincidentalbilingualism(Gujarati,Thai,
Tajik,Sindhi,Pashto)
3. Choosemodels: IncludetheAPI-basedGPT-3.5(Text-Davinci-003)andGPT-4modelfromOpenAIand
GeminifromGoogle,aswellastheopen-weightLLaMA-3,Gemma,andAyamodelswhichenableadditional
analysis.
4. Gathertranslationresults:SystematicallycomparestandardMTprompttemplatestoourproposedmethod
acrossdifferentmodelsandlanguagepairs.Additionallyablatethestepsofthenewmethod(removinglangid
templates;replacinglangidtemplateswithendonymiclangidtags;provideonlythegeneratedparagraph;
onlythemonolingualcontent).
5. Performanalysis:EvaluatewhetherthenewmethodimprovestheperformanceofLLMsinthesetasksas
comparedtothebaselinesusingmultiplestandardautomaticmetricsforMT(chrF,COMET,BLEU)andtoken-
levelLIDtomeasureoff-targettranslations.Assesswhichcomponent(s)arenecessaryforthisimprovement
andwhetherornottherearechangesacrosslanguagepairdirectionandlanguageresourcelevelsorscripts.
59TranslationwithLLMsthroughPromptingwithLong-FormContext(Part2)
5.TestCaseExample:
• BaselinePromptTemplate:
[English] This is an example.
[Spanish]
• BaselinePromptExpectedOutput:
[Spanish] Este es un ejemplo.
• ProposedPromptInput(step1):
Write a paragraph containing the following sentence:
[English] This is an example.
• Expectedmodeloutput(step1):
This is an example link. Italics and Bold: use asterisks and underscores
to indicate spans of emphasis. Use one asterisk (or underscore) for
italics and two asterisks (or two underscores) for bold. For example...
• ProposedPromptInput(step2):
[Spanish] Computadoras simples son lo suficientemente pequeñas
para residir en los dispositivos móviles.
[Spanish] Las computadoras portátiles, tales come tabletas,
netbooks, notebooks, ultrabooks, pueden ser alimentadas por
pequeñas baterías.
[Spanish] Las computadoras personales en sus diversas formas son
iconos de la llamada era de la información y son lo que la mayoría
de la gente considera como «computadora».
Translate this paragraph from English to Spanish:
[English] This is an example link. Italics and Bold: use asterisks
and underscores to indicate spans of emphasis. Use one asterisk
(or underscore) for italics and two asterisks (or two underscores)
for bold. For example...
[Spanish]
• ProposedPromptExpectedOutput:
[Spanish] Este es un enlace de ejemplo. Cursiva y negrita: utilice
asteriscos y guiones bajos para indicar intervalos de énfasis.
Utilice un asterisco (o guión bajo) para cursiva y dos asteriscos
(o dos guiones bajos) para negrita. Por ejemplo...
60TranslationwithLLMsthroughPromptingwithLong-FormContext(Part3)
5.TestCaseExample(Continued):
• ProposedPromptInput(step3):
Which of the following sentences are translations of the following
English sentence? Multiple sentences can be chosen.
[English] This is an example.
1. Este es un enlace de ejemplo.
2. Cursiva y negrita: utilice asteriscos y guiones bajos para indicar
intervalos de énfasis.
3. Utilice un asterisco (o guión bajo) para cursiva y dos asteriscos
(o dos guiones bajos) para negrita.
4. Por ejemplo...
• ProposedPromptExpectedOutput:
The sentence "This is an example." can be translated to Spanish as:
1. Este es un ejemplo.
2. Por ejemplo...
These two options correctly translate the meaning of
"This is an example." into Spanish.
6. FallbackPlan: Iftheproposedmethoddoesnothelpascomparedtothebaseline,analyzingtheresultsof
step3wouldlikelyprovidefurtherinsightsintohowthetemplateshouldbemodified.Inadditiontopotentially
identifyingoff-targeterrors,itmaybethatthemodelisunabletoidentifycorrecttranslationseveniftheyhave
beengenerated,andresultsarelikelytovaryacrosslanguagesbasedontheirtrainingdata.Usingthegenerated
paragraphasprovidedcontextandstillqueryingthemodeltotranslateatonlythesentencelevelcouldbecompared.
Restrictingmonolingualtexttoberetrievedtextwithinthedomainofthesourcesentencecouldbeexplored.Adding
few-shotexamplesinthepromptandcomparingotherMTprompttemplatesmayalsohelpdebugtheproposed
method.Includinganadditionalquerywherethemodelisfirstaskedtolabeleachgeneratedtokenbylangidand
thenaskedtore-translatethesourceincludingthosetokenswhicharecorrectlylabelledintargetmayreinforce
langidandguidegenerationinthetargetlanguage.Performinglayer-wiseanalysesoflikelihoodofgeneratingthe
nexttokenin-languageandin-scriptforopen-weightmodelsmayalsohelpdebugwhereandwhyoff-targetissues
persist.
61Reviewer1
Novelty:5(somewhatnovel-therearedifferencesfromexistingideasbutnotenoughtoturnintoanewpaper)
Rationale: WhileI’mnotawareofpapersthathaveusedthisexactpromptingstrategy,Idon’tthinkthatthis
proposalwillbeenoughtojustifyapublication.Ithinkthatthereshouldbeavarietyofstrategiessuggested+an
analysisofmultiplepromptingstrategiesratherthansuggestingonestrategy.Ithinkthatathoroughanalysisofthe
effectsofadditionalcontext/langidscouldpotentiallyturnthisintoapaper.
Feasibility:9
Rationale: SuchaprojectthatonlyusesLLMAPIscouldbeexecutedveryquicklywithoutmuchexpertisein
coding/architecture.Theonlytime-consumingpartmightbeiteratingandadjustingthepromptsintheablation
studies.
ExpectedEffectiveness:7
Rationale:IthinkthatthisproposalcouldworkwelltoguideLLMstotranslateinthedesiredtargetlanguage,since
thisisaknownproblemwithcurrentprompt-basedMTstrategies(asthewritershavesuggested).
Excitement:5(Leaningnegative:ithasinterestingbitsbutoverallnotexcitingenough)
Rationale:I’mnotsurehowwellthismethodwilltransfertofuturemodels,andthiscouldbealimitingfactorinthe
longevityofthisresearch.(Butthisisalimitationofallpromptingresearch...)
OverallScore:5(Decentideabuthassomeweaknessesornotexcitingenough,marginallybelowtheacceptance
thresholdofmajorAIconferences)
Rationale: Ithinkthattheworkshouldfocusontheablationstudiesandcomparisonofmultipleprompting
strategies/analysis,ratherthanfocusingononenewstrategy.
Confidence:3(Youarefairlyconfidentthattheevaluationiscorrect)
62Reviewer2
Novelty:1(notnovelatall-therearemanyexistingideasthatarethesame)
Rationale: TherearemultipleexistingworksonpromptingLLMsonlow-resourcetranslation,usuallyusing
few-shotdemo.https://proceedings.mlr.press/v202/garcia23a/garcia23a.pdfhttps://arxiv.org/pdf/2305.14857
Alsoworkexplainingwhyfew-shotpromptwouldwork:https://arxiv.org/pdf/2305.10266
Feasibility:5(Moderatelyfeasible:Itcanprobablybeexecutedwithinthegiventimeframebutwouldrequire
carefulplanning,efficientuseofAPIsorsomeadvancedcomputationalstrategiestoovercomethelimitedGPU
resources,andwouldrequiresomemodificationstotheoriginalproposaltomakeitwork.)
Rationale:ThepromptingexperimentismostlyfeasiblegivenonecanaffordtheAPIcalls.Themodel,prompts,
andevaluationmetricsareconcrete,althoughuncleariftheproposedexperimentisusefulforprovingtheresearch
idea,e.g.,afewhigh-resourcelanguagesarelistedforaresearchideathatfocusesonlow-resourcelanguages.
ExpectedEffectiveness:3(LowEffectiveness:Theideamightworkinsomespecialscenariosbutyoudon’texpectit
toworkingeneral.)
Rationale:Theproposedexperimentcanhelpfindasetofrelativelyhigh-performingprompts,butitisunclear
amongthepromptsproposedifanyofthemwillbringanyimprovement.
Excitement:3(Mediocre:thisideamakesmarginalcontributionsandisveryincremental)
Rationale: The ability to do prompting/few-shot translation is fundamentally tied to the training data, see
https://arxiv.org/pdf/2305.10266,sotryingtosolvethisproblemfromthepromptingspaceisinherentlylimited.
OverallScore:3(ClearrejectionformajorAIconferences)
Rationale:ThereissimilarworkonpromptingLLMstogeneratetranslationinlow-resourcelanguages,hencethe
ideaisnotverynovel.Moreover,intermsofthegoaltogeneratehigh-qualitylow-resourcetranslation,thegains
likelyarenotgoingtocomefromprompting.
Confidence:4(Youareconfidentbutnotabsolutelycertainthattheevaluationiscorrect)
63S ExampleIdea: LinguisticPivotConstellation: Enhancing
Cross-LingualTransferforLow-ResourceLanguagesandDialects
LinguisticPivotConstellation(LPC):EnhancingCross-LingualTransferforLow-Resource
LanguagesandDialects(Part1)
1. ProblemStatement:Largelanguagemodelsstrugglewithcross-lingualtransfer,especiallyforlow-resource
languagesanddialects.Thislimitationhindersthemodels’abilitytoperformwellonmultilingualtasksinvolving
theselanguages,potentiallyexacerbatingdigitallanguagedivides.
2.Motivation:Currentapproachesoftenrelyonparalleldataormultilingualpretraining,whicharelimitedfor
manylanguagepairs.Inspiredbyhowpolyglotsleveragesimilaritiesbetweenknownlanguagestolearnnewones,
weproposecreatinganetworkofconceptualbridgesacrosslanguages.Thismethodcouldpotentiallyovercome
thelimitationsofexistingapproachesbyleveragingthemodel’sbroadknowledgetocreateconnectionsbetween
knownandunknownlinguisticterritories.
3. ProposedMethod: WeintroduceLinguisticPivotConstellation(LPC),anovelpromptingtechniquethat
constructsadynamicnetworkoflinguisticpivotpoints.Foragiventask,LPCfirstidentifiesconceptuallysimilar
languagesordialectstothetargetlanguage.Itthengeneratesaconstellationofpromptsinthesepivotlanguages,
eachcapturingadifferentaspectofthetask.Themodelisguidedto’triangulate’thecorrectresponsebyconsidering
thesemultipleperspectives.Forexample,totranslateararedialect,LPCmightusepromptsinrelatedlanguages,
regionallinguafrancas,andevenetymologicallyconnectedlanguages.
4.Step-by-StepExperimentPlan:
1. DataCollection
• Gatherdatasetsfortranslationandquestion-answeringtasksacrossadiversesetoflow-resource
languagesanddialects.
• UtilizetheFLORES-101datasetformachinetranslationandtheTyDiQAdatasetforquestionanswering.
2. BaselineImplementation
• Implementstandardfew-shotpromptingandexistingcross-lingualtransfermethods(e.g.,zero-shot
cross-lingualtransfer)asbaselines.
3. LPCImplementation
(a) Createalanguagesimilaritymatrixbasedonlanguagefamiliesandgeographicalproximity.
(b) Implementafunctiontoselectthemostrelevantpivotlanguagesforagiventargetlanguage.
(c) Designpromptsforeachpivotlanguagethatcapturedifferentaspectsofthetask.
4. PromptConstruction
(a) Select3-5pivotlanguagesbasedonthesimilaritymatrix.
(b) Generatetask-specificpromptsineachpivotlanguage.
(c) Combinethesepromptsintoa’constellation’promptthatincludestheoriginaltaskinthetargetlanguage.
5. ModelSelection
• UseGPT-4astheprimarymodelforexperiments.
• TestwithGPT-3.5-turboforcomparison.
6. ExperimentExecution
(a) Runthebaselinemethods.
(b) RuntheLPCmethodwithvaryingnumbersofpivotlanguages(1,3,and5).
(c) Recordthemodeloutputsandperformancemetrics.
64LinguisticPivotConstellation(LPC):EnhancingCross-LingualTransferforLow-Resource
LanguagesandDialects(Part3)
4.Step-by-StepExperimentPlan(Continued):
7. Evaluation
• Evaluatetheresultsusingtask-specificmetrics:
– BLEUscorefortranslationtasks
– F1scoreforquestionansweringtasks
8. Analysis
• Analyzetheeffectivenessofdifferentpivotlanguagecombinationsandthemethod’sscalabilityto
extremelylow-resourcescenarios.
• CompareLPCperformanceagainstbaselinesacrossdifferentlanguagefamiliesandresourcelevels.
5.TestCaseExamples:
• TestCase1:
– BaselinePromptInput:TranslatethefollowingSiciliansentencetoEnglish:’Unnic’èfumuc’èfocu.’
– BaselinePromptExpectedOutput:Wherethere’ssmoke,there’sfire.
– ProposedPromptInput:WewilltranslateaSiciliansentencetoEnglish.Tohelpwiththistask,consider
thefollowingrelatedphrases:
In Italian: ’Dove c’è fumo c’è fuoco.’
In Neapolitan: ’Addò ce sta ’o fummo ce sta ’o ffuoco.’
In Latin: ’Ubi fumus, ibi ignis.’
Now,translatetheSiciliansentencetoEnglish:’Unnic’èfumuc’èfocu.’
– ProposedPromptExpectedOutput:Wherethere’ssmoke,there’sfire.
– Explanation:TheLPCmethodprovidescontextfromrelatedlanguages(Italian,Neapolitan,andLatin),
whichcanhelpthemodelbetterunderstandandtranslatetheSicilianphrase.Thisisespeciallyuseful
forlow-resourcelanguageslikeSicilian,wheredirecttranslationdatamightbelimited.
6.FallbackPlan:IftheLPCmethoddoesnotsignificantlyoutperformbaselines,wewillpivottheprojecttowards
anin-depthanalysisofcross-lingualtransfermechanisms.Wewillinvestigatetherelationshipbetweenlanguage
similarityandtransfereffectiveness,theimpactofpivotlanguageselectiononperformance,andhowdifferent
aspectsoflanguage(lexical,syntactic,semantic)transferacrosstheconstellation. Thisanalysiscouldprovide
valuableinsightsintothestrengthsandlimitationsoflargelanguagemodelsincross-lingualtasks,potentially
informingfutureresearchdirectionsinmultilingualNaturalLanguageProcessing.
65Reviewer1
Novelty:9
Rationale:Theideaofusingalinguisticsimilaritymatrixtoformconceptualbridgeswhenconstructingprompts
toimprovecross-lingualtransferisonethatIhavenotheardofbefore.Ithinkthiscouldbeaninterestingwayof
leveragingexistinginformationaboutrelatedlanguagesforNLPtasksingeneral.
Feasibility:8(HighlyFeasible:Straightforwardtoimplementtheideaandrunalltheexperiments.)
Rationale:Ithinktheideamakessense,butmoredetailsshouldbesharedabouthowexactlythislanguagesimilarity
matrixisconstructedandwhatalgorithmswillbeusedfordetermininglanguagesimilarity.Moredetailsshouldbe
providedonhowthepromptsfordifferentlanguageswillbeobtainedandhowthedatawillbecollected,which
mightbeatimebottleneck.
ExpectedEffectiveness:6(Somewhateffective:Thereisadecentchancethattheproposedideacanbeatexisting
baselinesbymoderatemarginsonafewbenchmarks.)
Rationale: Ithinkthatthisideacouldworkwelljustbyprovidingmorecontextindifferentlanguages. The
effectivenesssoundslikeitmightbehighlyvariableontheselectionofpivotlanguages,though.
Excitement:7
Rationale:Ithinkthatthiscouldbeinterestingbeyondthecontextofprompting,suchastheuseofpivotlanguages
intraditionalmachinetranslation.
OverallScore:7(Goodidea,wouldbeacceptedbymajorAIconferences)
Rationale:Ithinkthattheideaissufficientlynovel,andifitisexecutedwellwithgoodresults,couldproducea
qualitypaperatatopNLPconference.
Confidence:3(Youarefairlyconfidentthattheevaluationiscorrect)
66Reviewer2
Novelty:8(clearlynovel-majordifferencesfromallexistingideas)
Rationale:TheLPCmethodintroducesanovelwayofleveragingrelatedlanguagesanddialectstoimprovecross-
lingualtransfer.Whilecross-lingualtransferandlanguagesimilarityhavebeenexplored,theideaofdynamically
creatingaconstellationofpromptsusingpivotlanguagesforspecifictasksisafreshandinnovativeapproach.
Feasibility:5(Moderatelyfeasible:Itcanprobablybeexecutedwithinthegiventimeframebutwouldrequire
carefulplanning,efficientuseofAPIsorsomeadvancedcomputationalstrategiestoovercomethelimitedGPU
resources,andwouldrequiresomemodificationstotheoriginalproposaltomakeitwork.)
Rationale:ImplementingLPCcouldbechallengingduetothecomplexitiesinvolvedinselectingoptimalpivot
languagesanddesigningeffectivepromptsforeach.Whiletheconceptissound,thepracticalexecution—suchas
buildingthelanguagesimilaritymatrixanddynamicallygeneratingprompts—mayrequiresubstantialeffortand
experimentation.
ExpectedEffectiveness:6(Somewhateffective:Thereisadecentchancethattheproposedideacanbeatexisting
baselinesbymoderatemarginsonafewbenchmarks.)
Rationale:TheLPCmethodhasthepotentialtoimprovecross-lingualperformance,especiallyinlow-resource
languages.Byleveraginglinguisticsimilarities,themodelmightbetterunderstandandtranslatelanguageswith
limitedtrainingdata.
Excitement:7
Rationale: TheLPCmethodisexcitingbecauseittacklesacriticalchallengeinmultilingualNLP—improving
performanceforlow-resourcelanguages.Ifsuccessful,itcouldsignificantlyenhancetheaccessibilityandusability
ofAImodelsacrossdiverselinguisticcontexts,particularlyinunderrepresentedlanguages.
OverallScore:6(MarginallyabovetheacceptancethresholdofmajorAIconferences)
Rationale:TheideaisapromisingcandidateforexplorationinthefieldofmultilingualNLP.Itintroducesanovel
approachthatcouldpotentiallyimprovecross-lingualtransfer,particularlyforlow-resourcelanguagesanddialects.
However,thechallengesinimplementationandtheuncertaineffectivenessofthemethodwarrantacautiousoverall
rating.
Confidence:4(Youareconfidentbutnotabsolutelycertainthattheevaluationiscorrect)
67Reviewer3
Novelty:8(clearlynovel-majordifferencesfromallexistingideas)
Rationale:Leveraginglanguagesimilarityisoftenquitewellstudiedinmachinetranslation,buttherehasn’tbeen
onestudyingusingsimilarlanguageasdemonstrationinmultilingualin-contextlearning.Itwouldbeinterestingto
seehowthemodelbehaviorchangewithdifferentpivots.
Feasibility:8(HighlyFeasible:Straightforwardtoimplementtheideaandrunalltheexperiments.)
Rationale:Theimplementationwillmostlyinvolvebuildingthesimilaritymatrixandformattingtheprompts.The
similaritymatrixshouldbeabletogetfromsomeexistingworks.Thepromptformattingandexperimentspart
shouldbeprettystraightforwardwithenoughAPIquota.
ExpectedEffectiveness:6(Somewhateffective:Thereisadecentchancethattheproposedideacanbeatexisting
baselinesbymoderatemarginsonafewbenchmarks.)
Rationale:Theideaisprettyinteresting,butit’snotexactlysurewhethersimilarlanguagesareinformativeenough
forthemodel,sinceitstillrequiresthemodeltounderstandthesimilaritybetweenlanguagesandreasonoverthe
relationshipbetweentargetlanguageandthegivenlanguages.
Excitement:8(Exciting:woulddeepenthecommunity’sunderstandingormakemajorprogressinthisresearch
direction)
Rationale:Itwouldbeinformativetothecommunitytoseewhethersuchdemonstrationcanleadtogoodperfor-
manceforin-contextlearning.Evenifthisideadoesn’twork,theanalysiswillbequiteinformative.
OverallScore:7(Goodidea,wouldbeacceptedbymajorAIconferences)
Rationale:Thisworkstudiesanimportantproblemforthemultilingualcommunity.Theexperimentresultsand
analysiswillbequiteinformativeformultilingualin-contextlearning.
Confidence:4(Youareconfidentbutnotabsolutelycertainthattheevaluationiscorrect)
68T ExampleIdea: LLMDirectedRetrievalQueryingforImproving
Factuality
LLMDirectedRetrievalQueryingforImprovingFactuality(Part1)
1. Problem Statement: Large language models can generate flexible, long-form language generations, but
LLM-generatedresponsesoftencontainhallucinatedorfactuallyinconsistentcontent. Particularlyinhigh-risk
settings,thereisaneedformethodstoimprovethefactualityofLLMs.
2. Motivation: AcommonframeworkforimprovingthefactualityofLLMgenerationsisretrievalaugmented
generation(RAG).InaRAGframework,aretrievertakesaqueryasinputandretrievesexternalknowledgefrom
ahigh-qualityknowledgebasefromreliablesources. Theretrievedcontentisincorporatedintothepromptfor
generatingtheresponse.Oneissuewiththisapproachisthatthequalityofthegenerationcanbebottleneckedbythe
qualityoftheretrievedcontent.Retrievalcanbechallengingfortaskswherethequeryobjectiveisunderspecifiedor
additionalreasoning(ormulti-stepreasoning)onthequeryisrequiredtoretrievecontentthatsupportsthequery.
3.ProposedMethod:OurmethodrefinesthequerybyusinganLLMtodecomposetheproblemintosub-questions
andgeneratecandidateanswerstoexpandeachsub-question.Thekeystepsinclude:
1. Decomposingtheoriginalquestionintosub-questionsusinganLLM.
2. Generatingcandidateanswersforeachsub-questionusingtheLLM.
3. Expandingeachsub-questionwithgeneratedcandidateanswerstocreateretrievalqueries.
4. Retrievingpassagesforeachexpandedquery.
5. Filteringretrievedpassagesbasedonretrievalmodelscore.
6. Aggregatingfilteredpassagesacrosssub-questions.
7. PromptingthegenerativeLLMwiththeaggregatedpassagesascontexttoanswertheoriginalquestion.
4.Step-by-StepExperimentPlan:
1. ChooseRAGdatasetswheretheretrievaltaskhasunderspecified/uniqueobjectivesorrequiresmulti-hop
reasoning,suchasBIRCOandHotpotQA.
2. Selectaretriever,suchasanE5orBGEmodel,andagenerativeLLM,suchasGPTorLLaMA-3.
3. EstablishBaseline:
(a) Usetheexamplequestionasthequerytotheretrievertoretrieverelevantcontentfromtheretrieval
passagepool.
(b) Constructapromptthatprovidestheretrievedcontextpassagesandthequestion.
(c) PromptthegenerativeLLMtoanswerthequestionusingthecontext.
4. ImplementProposedMethod:
(a) PromptthegenerativeLLMtodecomposethequestionintosub-questions.
(b) Foreachsub-question,promptthegenerativeLLMtogeneratecandidateanswers.
(c) Usesemanticsimilaritytoclusterthegeneratedcandidateanswersandsampleforsemanticdiversity.
(d) Constructretrievalqueriesbyexpandingeachsub-questionwithsampledcandidateanswers.
(e) Retrievepassagesusingeachqueryandaggregateresultsforeachsub-question.
(f) Deduplicateretrievedpassagesandfilterbasedonretrievalmodelscore.
(g) PromptthegenerativeLLMwithfilteredpassagesascontexttoanswertheoriginalquestion.
69LLMDirectedRetrievalQueryingforImprovingFactuality(Part2)
5.TestCaseExamples:
• TestCase1:
– OriginalQuestion:Inwhichregionisthevillageafterwhichlager"FuckingHell"isnamed?
– Baseline:
* RetrievalQuery:Inwhichregionisthevillageafterwhichlager"FuckingHell"isnamed?
* RetrievedPassage: FuckingHellisaGermanpalelager, aPilsner,withanalcoholcontentof
4.9%.ItisnamedafterFucking,thepreviousnameofthevillageofFugginginAustria;hellisthe
Germanwordfor’pale’andatypicaldescriptionofthiskindofbeer.Thebeer’snamewasinitially
controversial. BoththelocalauthoritiesinFuckingandtheEuropeanUnion’sTradeMarksand
DesignsRegistrationOfficeinitiallyobjectedtothename.Itwaseventuallyacceptedandthelager
issoldinternationally.
* Prompt:Giventheretrievedpassage(s)ascontextandthequestion,answerthequestionusingthe
context.
* Answer:Thevillageafterwhichthelager"FuckingHell"isnamedislocatedinAustria.
– ProposedMethod:
* Sub-Questions:
1. Whatvillageisthelager"FuckingHell"namedafter?
2. Inwhichcountryisthisvillagelocated?
3. Inwhichspecificregionorstatewithinthatcountryisthevillagelocated?
* ExampleRetrievalQuery:Whatvillageisthelager"FuckingHell"namedafter?Thelager"Fucking
Hell"isnamedafterthevillagepreviouslyknownasFucking,whichisnowcalledFugging,in
Austria.
* RetrievedPassages:
1. FuckingHellisaGermanpalelager,aPilsner,withanalcoholcontentof4.9%.Itisnamedafter
Fucking,thepreviousnameofthevillageofFugginginAustria;hellistheGermanwordfor
’pale’andatypicaldescriptionofthiskindofbeer.Thebeer’snamewasinitiallycontroversial.
Both the local authorities in Fucking and the European Union’s Trade Marks and Designs
RegistrationOfficeinitiallyobjectedtothename.Itwaseventuallyacceptedandthelagerissold
internationally.
2. Fugging,spelledFuckinguntil2021,isanAustrianvillageinthemunicipalityofTarsdorf,located
intheInnviertelregionofwesternUpperAustria.Itis33km(21mi)northofSalzburgand4km
(2.5mi)eastoftheInnriver,whichformspartoftheGermanborder.
* Prompt:Giventheretrievedpassage(s)ascontextandthequestion,answerthequestionusingthe
context.
* Answer:Thevillageafterwhichthelager"FuckingHell"isnamedislocatedintheInnviertelregion
ofwesternUpperAustria.
6. FallbackPlan: Iftheproposedmethoddoesnotsatisfythesuccesscriteria,alternativeapproachescouldbe
explored.Thesemayincludequantifyingthedifficultyofvariousexamplesandanalyzingwhetherthiscorrelates
withmethodimprovement.Themethodislikelytobemoreeffectiveforquestionsaboutesotericfacts,wherethe
modelislesslikelytohaveinternalknowledgeoftheanswer,oritsgeneratedanswersaremorelikelytodisagree.
Additionally,themethodmaybemorebeneficialforquestionsrequiringinformationfrommultiplepassages.
Furtheranalysiscouldhelpdebugwhytheproposedmethoddidnotwork,informingalternativenewmethodsor
transformingtheprojectintoananalysispaperbyofferinginterestingablationsandinsights.
70Reviewer1
Novelty:1(notnovelatall-therearemanyexistingideasthatarethesame)
Rationale:Ifindthisideaisextremelysimilarto"GenDec:ArobustgenerativeQuestion-decompositionmethodfor
Multi-hopreasoning"byWuetal.(2024).Link:https://arxiv.org/html/2402.11166v1
Feasibility:8(HighlyFeasible:Straightforwardtoimplementtheideaandrunalltheexperiments.)
Rationale: Technically,thisideacanbequicklyre-producedbasedontheaforementionedpaper. Thoughthe
motivationsandevaluationsaredifferentfromtheexistingwork,itshouldn’ttaketoolongtofigurethemout.
ExpectedEffectiveness:3(LowEffectiveness:Theideamightworkinsomespecialscenariosbutyoudon’texpectit
toworkingeneral.)
Rationale:Giventhattheideaistoosimilartoanexistingone,theauthormayneedtocreateanewbutrelatedideaas
afollow-upstudyoftheaforementionedpaper.Thisideadoeshaveadifferentmotivationfromtheaforementioned
one,soitusesdifferentevaluationmethods,though.
Excitement:2
Rationale:Reviewersmayarguetheoriginalityandnoveltyofthisideaifit’ssubmittedtoavenue.Theymaynot
finditexciting,either.
OverallScore:1(Criticallyflawed,trivial,orwrong,wouldbeawasteofstudents’timetoworkonit)
Rationale:Thestudentsshouldprobablythinkone-step-furtheroftheexistingstudyandtheymayeventuallyfind
awaytoimprovetheexistingsystem.
Confidence:5(Youareabsolutelycertainthattheevaluationiscorrectandveryfamiliarwiththerelevantliterature)
Reviewer2
Novelty:6(reasonablynovel-therearesomenotabledifferencesfromexistingideasandprobablyenoughtoturn
intoanewpaper)
Rationale:QuerydecompositionandRAGseparatelyarewellstudied,ifthereisnoexistingworkthatcombines
both(whichI’mnotawareof),thenit’sreasonablynovel.
Feasibility:10(Easy:Thewholeproposedprojectcanbequicklyexecutedwithinafewdayswithoutrequiring
advancedtechnicalskills.)
Rationale:It’sjustaseriesofpromptingwhichshouldbeeasyforaCSPhDstudent.
ExpectedEffectiveness:8(ProbablyEffective:Theideashouldoffersomesignificantimprovementovercurrent
methodsontherelevantbenchmarks.)
Rationale: Thismethodinvolvesmultiplefine-grainedretrievaloperationsandshouldnaturallyoutperform
existingretrievalmethodswithoutdecomposition.
Excitement: 6(Learningpositive: excitingenoughtobeacceptedatamajorAIconference,butstillhassome
weaknessesorsomewhatincremental)
Rationale:AlthoughIbelieveintheeffectivenessoftheproposedmethod,thehighlatencycomparedtobaselinesis
aconcern—traininganend-to-endmodeltoreducelatencymightbeagoodadd-on.
OverallScore:7(Goodidea,wouldbeacceptedbymajorAIconferences)
Rationale: Thisisagoodidea. Ifthereisnoidenticalexistingworkandtheauthorsconductcomprehensive
experiments,itwouldbeagoodpaper.
Confidence:4(Youareconfidentbutnotabsolutelycertainthattheevaluationiscorrect)
71Reviewer3
Novelty:5(somewhatnovel-therearedifferencesfromexistingideasbutnotenoughtoturnintoanewpaper)
Rationale:TheideaaimstotackleaquestionbybreakingitdownandsolvingitonebyonewithRAG.Butitseems
tobeamorespecializedwayofCoTwithRAG.
Feasibility:5(Moderatelyfeasible:Itcanprobablybeexecutedwithinthegiventimeframebutwouldrequire
carefulplanning,efficientuseofAPIsorsomeadvancedcomputationalstrategiestoovercomethelimitedGPU
resources,andwouldrequiresomemodificationstotheoriginalproposaltomakeitwork.)
Rationale:Theideaassumesaquestioncanbebrokendownintosubquestionswhereeachsubquestionisindepen-
dentoftheothers.Incaseswheretheyarenotindependent,themethodmightsufferfromissuesorinefficiency.But
maybethedistributionofthesequestionsismorelikealongtailandpredominantlyquestionsthatcanbeeasily
brokendown.Andisthereacasewherethequestionishigh-levelmathematicsanddifficulttothepointwhereit
breaksdownintoanon-linearscaleofthequestiontexttoken?
ExpectedEffectiveness:5(Somewhatineffective:Theremightbesomechancethattheproposedideacanwork
betterthanexistingbaselinesbuttheimprovementwillbemarginalorinconsistent.)
Rationale:Themainquestionishowthesub-questionsarecreated.Wecanbreakthequestionintoconditioned
partsfromp(q 0|q 0,...q n)...p(q n|q 0,...q n−1)whereweassumethemtobedependent,orwecanuseLLMtoreason
abouttheirdependency.Wecanalsoaskthequestionbyaskingleveledsub-questionslike"whereisthisperson
from"into"whichcountryisthispersonfrom","whichcityisthispersonfrom","whichdistrictisthispersonfrom".
Theconcernisthatdifferentmethodsmightaffecttheperformancedifferently.
Excitement: 6(Learningpositive: excitingenoughtobeacceptedatamajorAIconference,butstillhassome
weaknessesorsomewhatincremental)
Rationale:TheideaseemsexcitingasitpreventsLLMfromshortcuttingthequestionandhallucinating.Butitneeds
moremethodformulationonhowthequestionshouldbebrokendown.Theverybaselineimplementationwilljust
degradetoaCoTreasoningwithRAGforeachstep.BecausethiscouldjustbeasubsetofCoTmethodsinsome
sense.
OverallScore:6(MarginallyabovetheacceptancethresholdofmajorAIconferences)
Rationale: IbelievetherecouldbemorecomparisonwithCoTasmotivation. Whyshouldthisbebetterwith
promptingthemodelstepbystepusingRAG,andwhyaretheydifferent?Andforproblemformulation,itwould
begreatifwecanlistmoreedgyexamplesofhowquestionscanbedividedtohelppilotthepromptingmethods.
Confidence:4(Youareconfidentbutnotabsolutelycertainthattheevaluationiscorrect)
72U ExampleIdea: SemanticDivergenceMinimization: Reducing
HallucinationsinLargeLanguageModelsthroughIterativeConcept
Grounding
SemanticDivergenceMinimization:ReducingHallucinationsinLargeLanguageModels
throughIterativeConceptGrounding(Part1)
1.ProblemStatement:Largelanguagemodelsoftengeneratehallucinationsbydivergingfromthecoresemantic
content of the input, especially in complex reasoning tasks. This problem undermines the reliability and
trustworthinessofLLMsincriticalapplicationsthatrequireaccurateandfactualresponses.
2.Motivation:Currentapproacheslikechain-of-thoughtpromptingfocusongeneratingintermediatestepsbut
donotexplicitlyconstrainsemanticdrift.Bycontinuouslygroundinggeneratedcontenttotheoriginalsemantic
spaceoftheinput,wecanreducehallucinationswhilepreservingreasoningcapabilities.Thismethodleveragesthe
LLM’sownabilitytoextractandcomparesemanticconcepts,creatingaself-correctingmechanismthatdoesnot
requireexternalknowledgebasesorcomplexarchitectures.
3.ProposedMethod:WeintroduceSemanticDivergenceMinimization(SDM)prompting.Foreachreasoningstep,
wepromptthemodelto:
1. Generateacandidatenextstep.
2. Extractkeysemanticconceptsfromtheoriginalinput.
3. Measuresemanticsimilaritybetweenthecandidatestepandextractedconcepts.
4. Ifsimilarityisbelowathreshold,regeneratethestepwithexplicitinstructionstoincorporatemorerelevant
concepts.
5. Repeatuntilconvergenceormaximumiterations.
Thiscreatesasemantic’gravitywell’thatkeepsreasoningtetheredtotheinput’sconceptualcore.
73SemanticDivergenceMinimization:ReducingHallucinationsinLargeLanguageModels
throughIterativeConceptGrounding(Part2)
4.Step-by-StepExperimentPlan:
1. DatasetPreparation:
• Usetwodatasets:HotpotQAformulti-hopreasoningandGSM8Kforcomplexmathwordproblems.
• ForHotpotQA,utilizethedevset(7,405questions).
• ForGSM8K,employthetestset(1,319problems).
2. BaselineImplementation:
• Implementtwobaselines:
– Standardprompting:directlyaskingthemodeltoanswerthequestion.
– Chain-of-thought(CoT)prompting:askingthemodeltoshowitsworkstep-by-stepbeforegiving
thefinalanswer.
3. SDMImplementation:
• ImplementtheSDMmethodwiththefollowingsub-stepsforeachreasoningiteration:
– Generatenextstep.
– Extractkeyconceptsfrominput.
– Measuresemanticsimilarity.
– Regenerateifbelowthreshold.
– Repeatuntilconvergenceormaximumiterations.
4. PromptEngineering:
• DesignpromptsforeachstepofSDM.Forexample:
– "Generatethenextstepinsolvingthisproblem:"
– "Extractkeyconceptsfromtheoriginalquestion:"
– "Ratethesemanticsimilaritybetweentheseconceptsandthegeneratedsteponascaleof0-10:"
– "Regeneratethestep,focusingmoreonthesekeyconcepts:"
5. HyperparameterTuning:
• Experimentwithdifferentsimilaritythresholds(e.g.,5,6,7outof10)andmaximumiterationlimits(e.g.,
3,5,7)tofindtheoptimalbalancebetweenperformanceandcomputationalcost.
6. ModelSelection:
• UseGPT-4astheprimarymodelduetoitsadvancedreasoningcapabilities.
• TestGPT-3.5-turboforcomparison.
7. Evaluation:
• ForHotpotQA,usetheofficialevaluationscripttocomputeExactMatch(EM)andF1scores.
• ForGSM8K,useaccuracyasthemetric.
• Manuallyreviewasubsetofresponsestoassessqualitativeimprovementsinreasoningandreductionof
hallucinations.
8. Analysis:
• CompareSDMresultswithbaselines.
• Analyzethecorrelationbetweensemanticsimilarityscoresandanswercorrectness.
• ExaminecaseswhereSDMsignificantlyimprovesorfailstoimproveoverbaselines.
74SemanticDivergenceMinimization:ReducingHallucinationsinLargeLanguageModels
throughIterativeConceptGrounding(Part3)
5.TestCaseExamples:
• Example1(HotpotQA):Input:Whowasbornfirst,AlecGuinnessorBillieHoliday?
BaselineOutput:AlecGuinnesswasbornfirst.HewasbornonApril2,1914,whileBillieHolidaywasborn
onApril7,1915.
SDMOutput:TheSDMmethodprovidesamorestructuredandtransparentreasoningprocess,reducingthe
riskofhallucinationbygroundingeachstepinthekeyconcepts(birthdates)fromtheoriginalquestion.
• Example2(GSM8K):Input:Janet’sduckslay16eggsperday.Sheeatsthreeforbreakfasteverymorningand
sellstherestatthefarmers’marketdailyfor$2peregg.Howmuchmoneydoesshemakeeveryweek?
BaselineOutput:Let’ssolvethisstepbystep:Janetmakes$182everyweek.
SDMOutput:TheSDMmethodnotonlyprovidesaclearstep-by-stepsolutionbutalsoexplicitlyverifies
thatallkeyconceptsfromtheoriginalquestionareincorporated,reducingtheriskofintroducingirrelevant
informationorhallucinatingfacts.
6.FallbackPlan:IftheproposedSDMmethoddoesnotsignificantlyoutperformbaselines,wecanpivottheproject
inseveralways.First,wecouldconductanin-depthanalysisofwhereandwhySDMfails,potentiallyuncovering
insightsaboutLLMreasoningprocesses.WemightfindthatSDMworksbetterforcertaintypesofquestionsor
reasoningtasks,whichcouldleadtoamorenuancedapplicationofthemethod.Second,wecouldexplorevariations
ofSDM,suchasusingdifferentpromptsforconceptextractionorsimilaritymeasurement,orincorporatinga
dynamicthresholdthatadjustsbasedonthecomplexityofthequestion.Third,wecouldcombineSDMwithother
promptingtechniqueslikechain-of-thoughtorself-consistencytocreateahybridapproach.Finally,ifthesemantic
groundingaspectproveschallenging,wecouldshiftfocustoanalyzinghowLLMsinterpretandmaintainsemantic
consistencythroughoutmulti-stepreasoning,whichcouldprovidevaluableinsightsforfutureworkonreducing
hallucinations.
75Reviewer1
Novelty:8(clearlynovel-majordifferencesfromallexistingideas)
Rationale:TheuseofsemanticsimilaritytoconstrainCoT-styledgenerationisverynew.Ihavenotseensimilar
workonit.
Feasibility:5(Moderatelyfeasible:Itcanprobablybeexecutedwithinthegiventimeframebutwouldrequire
carefulplanning,efficientuseofAPIsorsomeadvancedcomputationalstrategiestoovercomethelimitedGPU
resources,andwouldrequiresomemodificationstotheoriginalproposaltomakeitwork.)
Rationale:Thepipelineisfeasibletome.Themajorchallengewouldbefindingthesimilaritythresholdforeach
dataset.
ExpectedEffectiveness:3(LowEffectiveness:Theideamightworkinsomespecialscenariosbutyoudon’texpectit
toworkingeneral.)
Rationale:Iseesomedrawbacksinthispipeline.First,manuallytuningthesimilaritythresholdseemsnotthebest
practiceforscalableapplications.TheGSM8Kmathdatasetcontainsprettyelementarymathproblems.Inthatcase,
thesemanticsimilaritythresholdshouldbesetveryhigh,sincethesebasicmathconceptsinvolvedintheprompt
andtheCoTbreakdownwouldbedeterminedashighlysimilarbymostexistingembeddingmethods.Thisbrings
thequestionofwhetherthissimilaritythresholdisnon-trivialatallforsometasks.
Excitement: 6(Learningpositive: excitingenoughtobeacceptedatamajorAIconference,butstillhassome
weaknessesorsomewhatincremental)
Rationale:ConstrainingCoTbreakdownsisanovelideaanddeservesmoreworkandexploration.Whiletheuseof
semanticsimilarityhasmanydrawbacks(suchastuningthethreshold,task-sensitive,non-scalable),itcanstillshow
ussomevaluableresultsaboutconstrainingCoTbreakdowns.
OverallScore:5(Decentideabuthassomeweaknessesornotexcitingenough,marginallybelowtheacceptance
thresholdofmajorAIconferences)
Rationale:Therearesomecleardrawbacksinherenttothemethod,asdiscussedearlier.Iftheauthorscanovercome
theselimitations,thisideacouldyieldsomeinterestingfindingsusefulforourunderstandingofCoTbehaviorand
couldpassaboveamajorconferencethreshold.
Confidence:3(Youarefairlyconfidentthattheevaluationiscorrect)
76Reviewer2
Novelty:4
Rationale: Generallythismethodisawayofrejectionsamplingtoimprovefactuality. Itissomewhatnottoo
differentfrompreviousliteraturefor"constraineddecoding"forimprovingfactuality:-ConstrainedAbstractive
Summarization:PreservingFactualConsistencywithConstrainedGeneration-Don’tSayWhatYouDon’tKnow:
ImprovingtheConsistencyofAbstractiveSummarizationbyConstrainingBeamSearch
Feasibility:9
Rationale:Simplepromptingapproachthatiseasytoimplement.Evaluationissimple.
ExpectedEffectiveness:3(LowEffectiveness:Theideamightworkinsomespecialscenariosbutyoudon’texpectit
toworkingeneral.)
Rationale:1.RightnowmostLLMshallucinateinasubtleway:theysaythingsinsemanticallycorrectorreasonable
ways,buttheprecisefactisincorrect.Usingsemanticsimilarityasameasurementtogauge/controlhallucination
mightnotbeabletosolvetheproblem.2.TherejectionsamplingisbasedonanotherLLM—whatiftheLLMalso
hallucinates?
Excitement:3(Mediocre:thisideamakesmarginalcontributionsandisveryincremental)
Rationale:ThemethodisnotthatnovelandIthinkthemethodisnotthateffectiveandmightnotsolvetheproblem
atall.
OverallScore:3(ClearrejectionformajorAIconferences)
Rationale:Theexperimentdesigniskindofsimpleandtheevaluationisnotcomprehensive.Ithinktheideaisin
therangeof4buttheexperimentplanfurtherreducesmyscore.
Confidence:5(Youareabsolutelycertainthattheevaluationiscorrectandveryfamiliarwiththerelevantliterature)
77Reviewer3
Novelty:3(mostlynotnovel-youcanfindverysimilarideas)
Rationale:Theideaofextractingkeysemanticconcepts,measuringtherelevanceofthecandidatenextstep,and
possiblyrejecting/revisingthestepisverysimilartoincorporatingself-critiqueintomulti-stepreasoningproblems.
Differentversionsofthisarealreadycommonlyused,especiallyforsolvingmathproblems.
Feasibility:8(HighlyFeasible:Straightforwardtoimplementtheideaandrunalltheexperiments.)
Rationale:Theproposedapproachshouldbestraightforwardtoimplement:itonlyrequirespromptengineeringto
extractsemanticconceptsandevaluatetherelevanceofacandidatenextstep.
ExpectedEffectiveness:3(LowEffectiveness:Theideamightworkinsomespecialscenariosbutyoudon’texpectit
toworkingeneral.)
Rationale:Comparedtochain-of-thoughtprompting,there’sareasonablechancethismethodcouldworkbetter:
itcouldhelpidentifywhenareasoningstepbecomesirrelevanttotheoriginalquestion. However,sincesuch
self-critiquemethodshavealreadybeenexplored,it’sunlikelythatthisinstantiationwillworksignificantlybetter
thanpreviousones. Also,theproposedideaofextractingrelevantsemanticconceptsandmeasuringsemantic
similarityseemsabitvague,andit’snotreflectedintheprovidedexamples.
Excitement:2
Rationale: Theproposedmethodistoosimilartoexistingworks;itdoesn’tcontainnovelinsightsthatwould
meaningfullyboostcurrentLMperformanceorintroducenewideasworthbuildingon.Itwouldnotbeanexciting
paper.
OverallScore:2(StrongrejectionformajorAIconferences)
Rationale:Similartothereasoningabove:theproposalistoosimilartoexistingworks,itdoesn’tintroducenew
ideasorinsights,andisunlikelytomeaningfullyimprovecurrentLMperformance.
Confidence:4(Youareconfidentbutnotabsolutelycertainthattheevaluationiscorrect)
78V ExampleIdea: Autoprompting: GenerateDiverseFew-shotExamples
forAnyApplication
Autoprompting:GenerateDiverseFew-ShotExamplesforAnyApplication(Part1)
1. ProblemStatement: Addingnaturallanguagecapabilitiestoexistingsoftwarerequiresmanuallycrafting
few-shotprompts,whichistediousanddoesnotguaranteehighcoverage.
2. Motivation:Integratingnaturallanguagecapabilitiesintosoftwareapplicationsoftennecessitatesmanually
creatingfew-shotprompts,aprocessthatistime-consumingandmaynotensurecomprehensivecoverage. An
"Autoprompting"systemcapableofautomaticallygeneratingdiverseandrelevantfew-shotexamplestailoredto
specificapplicationswouldsignificantlyreducemanualeffort,improvecoverageandversatility,andenablerapid
prototypinganditerationofnaturallanguagecapabilities.LargeLanguageModelscaniterativelytestdifferent
functionalitiesofanapplicationandmakeadjustmentstofew-shotpromptsakintoahumandeveloper. This
approachwouldultimatelydemocratizetheintegrationofsuchcapabilitiesacrossawiderangeofapplicationsand
industries.
3.ProposedMethod:ThismethodleveragesaLargeLanguageModel(LLM)withcodingcapabilities.Itinvolves
thefollowingcoresteps:
1. Extractalluser-facingfunctionsandgathertheirdocumentationandunittests,ifavailable.
2. Generatediversenaturallanguagepromptstoutilizeeachfunction,definingtheexpectedoutput.
3. Generatecodefromthenaturallanguagepromptsandexecutethecorrespondingfunctions.
4. Ifthecodefails:
• Updatethecodeandretry
• Ifthecoderunsbutproducesanincorrectresult,updateitusinginsightsfromunittestsorgeneral
reasoning.
5. Onceyouhaveafewexemplarpromptsforall(ordesired)functions,generatepromptsthatcomposemultiple
functionstogetherandrepeatstep4.
Byiterativelyrefiningcodegenerationfromnaturallanguageandleveragingavailabledocumentationandtests,this
processaimstocreateanLLMcapableofcorrectlyimplementingfunctionsbasedonnaturallanguageinstructions.
4.Step-by-StepExperimentPlan:
• Applications:WhencollectingapplicationsfromGitHub,prioritizethosewithclear,well-writtendocumenta-
tionandcomprehensivetestsuites.Includeapplicationsfromdifferentdomainsandwithvaryinglevelsof
complexitytoensureadiversedataset.
• Fewshotsandfeasibility:Createmanualfew-shotexamplestounderstandthecomplexityofthefunctions
andthequalityofthedocumentation.Beginbycreating4-5examplesforanyfunction,whichcouldalsoserve
asastartingpointfortheLLM.
• Extractfunctionsandmetadata: Utilizestaticcodeanalysistoolstoensureaccurateandcomprehensive
extractionoffunctions,documentation,andtestcases. Considerextractingadditionalmetadata,suchas
functionsignatures,dependencies,andcomments,astheycanprovidevaluablecontext.
• NLModule:Generatediverseuserutterancesandincorporatetechniquestohandlevariationsinnatural
language.Foreachuserutterance,generatetheexpectedoutcome.Considergeneratingnegativetestcasesto
improvethemodel’sabilitytohandleinvalidorambiguousinputs.
• ExecutionModule:Incorporatesandboxingorcontainerizationtechniquestoensureasecureandisolated
executionenvironmentwhenexecutingthegeneratedcode.Implementloggingandreportingmechanismsto
captureandanalyzeerrorsandunexpectedbehavior.
79Autoprompting:GenerateDiverseFew-ShotExamplesforAnyApplication(Part2)
4.Step-by-StepExperimentPlan(Continued):
• Exploration:Incorporatetechniquessuchascodesummarization,callgraphanalysis,andtypeinference
toprovidemorecontextualinformationtotheagent. Specifically,inanycodesnippet,ifthereareother
user-definedfunctions,retrievetheirmetadataanduseitinthenextiterationofpromptgeneration.
• Store:Utilizeavectordatabaseorotherstructuredstoragemechanismthatsupportsefficientretrievaland
queryingforstoringfew-shotexamplesandtheiroutputs. Incorporatemechanismsforversioningand
updatingthestoreddataasthecodebaseandtheunderlyingmodelsevolve.
• Experiments: Oncefew-shotexamplesfordifferentfunctionalitiesandtheircompositionsareobtained,
simulatedifferentuserswithvariousintentsandcalculategoalcompletionanderrorratesusingdifferent
models.Initially,startwithastrongmodel,andoncefew-shotexamplesareavailable,testwithweakerand
open-sourcemodels.
5.TestCaseExamples:SelectatoyapplicationfromGitHubimplementedinPythonorJavaScript.
• Directprompting:Providethefew-shotexamplescreatedandcheckthegoalcompletionanderrorratesfor
thefollowingscenarios.
• Toyexample:Calculatorappanddifferentutterancestotry.
– Provideacompleteuserutterancewithnoambiguity.Forexample:
* Canyouadd4to8.
* Divide6by9andmultiplyitby6.
– Provideauserutterancewithsomeambiguity.Forexample:
* Take6and9,addthem,andthensubtract8. Also,add2tothefirstone. –herethe"first"oneis
ambiguousasitcouldbe6ortheintermediateanswer(6+9=15).
– Provideauserutterancethatisnotrelatedtothefunction.Forexample:
* PleaseaddAandJ.Thecorrectresultwouldberefusingtoanswerinsteadofgeneratingadd("A",
"J").
6. FallbackPlan: Iftheproposedmethodologydoesnotyieldsatisfactoryresults, thereareseveralareasto
investigate. First,examinethedocumentationtoensureitadequatelyexplainsthebasicfunctionalityofeach
function.Then,assessthecodingstyletoconfirmitalignswithrecommendedpractices.Subsequently,evaluate
eachmoduleseparately.FortheNLmodule,verifythattheexamplesarediverseandthatthegeneratedtestcases
arealigned.Fortheexecutionmodule,ensurethatthecorrecterrormessagesarebeingpassedandexploreways
toenhancethem.Theexplorationmoduleisthemostchallengingaspect;ifanyfunctionhasahighdependency
onotherfunctions,traversingitwillbedifficult.Therefore,initiallyfocusonexampleswithlimitedtonofunction
dependencyandgraduallyincreasethecomplexity.
80Reviewer1
Novelty:4
Rationale:Theproposedmethodissimilartohttps://arxiv.org/abs/2210.03493;
https://aclanthology.org/2023.findings-acl.216/
Feasibility:6(Feasible:Canbeexecutedwithinthegivenconstraintswithsomereasonableplanning.)
Rationale:TheexperimentscanbedonewithsufficientAPIaccess.Thedatasetcollectionneedssomeplanningbut
isingeneralfeasibletodo.Settingupthevectordatabasemaytakeextratime.
ExpectedEffectiveness:5(Somewhatineffective:Theremightbesomechancethattheproposedideacanwork
betterthanexistingbaselinesbuttheimprovementwillbemarginalorinconsistent.)
Rationale:Theproposalisvagueasitdoesn’tmentionwhat’sthefinalevaluationmetric,anddoesnotprovide
sufficientdescriptionofthecomparedbaseline.Thepromptinthedirectpromptbaselineisconfusingtomeaswell.
Overallit’shardtodiscusstheeffectiveness.
Excitement:4
Rationale:Giventhattheproposedmethodisvague,Iamunsureaboutitscontributionsandeffectiveness,and
thereforeIfeellessexcitedaboutit.
OverallScore:4(Okbutnotgoodenough,rejectionformajorAIconferences)
Rationale:ThedescriptionsareconfusingandI’mnotreallysurewhat’sthefocusorcontribution.Thetitleproblem
statementmentionedensuring"diversity"/"highcoverage"asthegoalbutdoesn’tdescribehowthisisensuredin
latersections.The"TestCaseExamples"doesn’texplainhowthecomponentsinthe"Step-by-StepExperimentPlan"
areused.
Confidence:3(Youarefairlyconfidentthattheevaluationiscorrect)
81Reviewer2
Novelty:7
Rationale: Mappingnaturallanguagetocustomapplicationsisahugelyimpactfulcapability, anddoingso
automaticallyisreallyinteresting. Ilikethefocusonautopromptingforthesetypesoftranslations,asthetask
isfeasiblesinceitbuildsoffsomeofthe"few-shotprompting"thatdevelopersmightnormallydotoaddNL
functionality,withamoreautomaticprocessthathasrealsystemchecks/verifications(e.g.,runningtheapplications
throughcontainers).ArelatedworkfromHCItriestoenableindividualdeveloperstoaddsuchNLfunctionality
totheirownapplicationsviaaDSL+NLprogramsignatures(https://jackieyang.me/reactgenie/).Thisworkis
distinguished,asitwouldempoweraddingsuchNLfunctionalitytoanyapplication,withoutchangingthecode.
Feasibility:4
Rationale:Theprojectinfrastructureseemsmoredifficultthansimplychoosingsomepromptingmethods.Itwould
beaniterativeprocesschoosingrealexampleapplicationsfromGithub,anddevelopingthefew-shotprompts
manuallytogetafeelforthistask. Then,someofthemodulesseemlike1-2weektasks(ExecutionModule,
Exploration,Storage)whichIestimatewouldmaketheprojectmorelike3-4monthstocompleteallmodulesAND
todotheevaluations.
ExpectedEffectiveness:7
Rationale:Thebaselinehereisazero-shotprompt,askingtodotheNLintentandfeedinginallthedocumentation
oftheAPI.AssumingtheauthoriscorrecttosaythatsuchNLfunctionmappingrequiresgoodfew&diverse
few-shotexamples,Iexpectthemethodtoworkwell. Itusesanumberofexternalsystemstoenrichthecode
datasettogivetheLLMcontextandusessystemerrorstoinform.Soinsomeways,Autopromptingisallowingan
agenttomakeuseofalltheseSWEtoolsforunderstandingthesoftware,whichthenwillallowittomaximizeits
understandingandbetterretrievegoodfew-shotexamplesforthetaskathand.
Excitement:7
Rationale:Seemslikeanimpactfulandambitiousoutcomeifcompleted.Iamcurioushowsuchanapproachfits
intotheconversationaboutgeneralagents,whichcanleverageAPI/tool/functionscalls.It’salittleunclearfrom
thetoyexamplewhyexistingfunction-callingmodelscan’ttranslateNLintentsinto.
OverallScore:6(MarginallyabovetheacceptancethresholdofmajorAIconferences)
Rationale: TheresultswouldbereallyexcitingandthetechnicalinfrastructuretoenabletheAutoprompting
agentwouldbeimpressive.However,I’mmissingabitofwhichcaseswillbereallydifficultforothergeneralist
web/systemagents,butwherefindingthefew-shotexamplesforthistaskisreallyneeded.Thus,thecoreideaofthe
methoddoesn’tseemclarifiedenoughtoresultinareallycleartakeawayonthemethod.
Confidence:3(Youarefairlyconfidentthattheevaluationiscorrect)
82W ExampleIdea: TemporalDependencyUnfolding: ImprovingCode
GenerationforComplexStatefulSystems
TemporalDependencyUnfolding:ImprovingCodeGenerationforComplexStatefulSys-
tems(Part1)
1. ProblemStatement: Generatingcodeforcomplex,statefulsystemsorapplicationswithintricatetemporal
dependencies remains challenging for current code generation models. Most existing approaches focus on
generatingindividualfunctionsorsmallcodesnippetswithoutfullyconsideringthetemporalaspectsandstate
changesinlargersystems.ThislimitationhinderstheapplicabilityofAI-assistedprogramminginareassuchas
distributedsystems,gamedevelopment,andreal-timeapplications.
2. Motivation: Manyreal-worldapplicationsrequirecarefulmanagementofstateovertime. Existingcode
generationmodelsstrugglewithcapturingthefullcomplexityoftemporaldependenciesandstatechangesin
largersystems.Amethodthatcaneffectivelyreasonaboutandgeneratecodeforsystemswithcomplextemporal
dependenciescouldsignificantlyimprovetheapplicabilityofAI-assistedprogrammingincriticalareas. Our
proposedTemporalDependencyUnfoldingmethodisinspiredbyhowhumandevelopersapproachcomplex
systemdesign,firstidentifyingkeystatesandtheirrelationshipsbeforeimplementingthedetailedlogic.
3.ProposedMethod:WeproposeTemporalDependencyUnfolding,anovelpromptingtechniquethatguidesthe
modeltogeneratecodebyexplicitlyreasoningaboutstatechangesandtemporalrelationships.Themethodconsists
offivesteps:
1. StateIdentification:Promptthemodeltoidentifykeystatesandvariablesthatchangeovertimeinthetarget
system.
2. TemporalGraphConstruction:Guidethemodeltocreateaconceptualgraphofhowthesestatesevolveand
interactovertime.
3. StagedCodeGeneration:Generatecodeinstages,focusingondifferenttemporalslicesorstatetransitionsin
eachstage.
4. ConsistencyVerification: Aftereachstage, promptthemodeltoverifytemporalconsistencyandmake
necessaryadjustments.
5. Integration: Finally,guidethemodeltointegratethestage-wisegeneratedcodeintoacohesivesystem,
ensuringproperhandlingofalltemporaldependencies.
4.Step-by-StepExperimentPlan:
1. DatasetPreparation:
• Createadatasetofprogrammingtasksthatinvolvecomplextemporaldependencies.
• Includetasksfromthreedomains:1)Multi-threadedapplications,2)Gamelogic,and3)Distributed
systems.
• Foreachdomain,prepare50taskdescriptions,eachwithaclearspecificationofthedesiredfunctionality
andtemporalrequirements.
2. BaselineImplementation:
• Implementtwobaselinemethods:
– Directprompting:Simplyprovidethetaskdescriptiontothemodelandaskittogeneratethecode.
– Chain-of-Thought(CoT)prompting:Append’Let’sapproachthisstep-by-step:’tothetaskdescrip-
tion.
• UseGPT-4forbothbaselines.
83TemporalDependencyUnfolding:ImprovingCodeGenerationforComplexStatefulSys-
tems(Part2)
4.Step-by-StepExperimentPlan(Continued):
3. TemporalDependencyUnfoldingImplementation:
• Implementourproposedmethodwiththefollowingsub-stepsforeachtask:
(a) StateIdentification:PromptGPT-4with’Identifythekeystatesandvariablesthatchangeovertime
inthissystem:’.
(b) TemporalGraphConstruction:Promptwith’Createaconceptualgraphshowinghowtheidentified
statesevolveandinteractovertime:’.
(c) StagedCodeGeneration:Foreachmajorstateortransitionidentified,promptwith’Generatecode
forthefollowingstate/transition:[state/transition]’.
(d) ConsistencyVerification: Aftereachstage,promptwith’Verifythetemporalconsistencyofthe
generatedcodeandsuggestanynecessaryadjustments:’.
(e) Integration:Finally,promptwith’Integratethegeneratedcodesegmentsintoacohesivesystem,
ensuringproperhandlingofalltemporaldependencies:’.
4. EvaluationMetrics:
• Correctness:Percentageofgeneratedcodethatpassespredefinedtestcases.
• TemporalConsistency:Manualevaluationofhowwellthecodehandlestemporaldependencies(scale
1-5).
• CodeQuality:Automatedmetricslikecyclomaticcomplexityandmaintainabilityindex.
• ExecutionEfficiency:Runtimeperformanceonbenchmarkinputs.
5. HumanEvaluation:
• Recruit5experienceddeveloperstoreviewasubsetof30generatedsolutions(10fromeachdomain).
• Theywillratethecodeonascaleof1-5forreadability,maintainability,andcorrecthandlingoftemporal
dependencies.
6. ExperimentExecution:
• Foreachtaskinthedataset:
(a) GeneratesolutionsusingbothbaselinemethodsandourTemporalDependencyUnfoldingmethod.
(b) Applyallevaluationmetricstothegeneratedsolutions.
(c) Collecthumanevaluationsforthesubsetofsolutions.
7. Analysis:
(a) ComparetheperformanceofTemporalDependencyUnfoldingagainstthebaselinesacrossallmetrics.
(b) Analyzetheeffectivenessofeachstepinourmethod(StateIdentification,TemporalGraphConstruction,
etc.)byexaminingintermediateoutputs.
(c) Identifypatternsintaskswhereourmethodshowssignificantimprovementorunderperforms.
(d) Correlateautomatedmetricswithhumanevaluationstovalidatetheirreliability.
84TemporalDependencyUnfolding:ImprovingCodeGenerationforComplexStatefulSys-
tems(Part3)
5.TestCaseExamples:
• TestCase1:
– BaselinePromptInput(DirectPrompting):GeneratePythoncodeforasimplemulti-threadedproducer-
consumersystemwithasharedbuffer. Theproducershouldgeneraterandomnumbersandadd
themtothebuffer,whiletheconsumershouldremoveandprocessthesenumbers.Implementproper
synchronizationtoavoidraceconditions.
– BaselinePromptExpectedOutput(DirectPrompting):[Pythoncodeforasimpleproducer-consumer
system]
– ProposedPromptInput(TemporalDependencyUnfolding;Step1:StateIdentification):Foramulti-
threadedproducer-consumersystemwithasharedbuffer,identifythekeystatesandvariablesthat
changeovertimeinthissystem:
– ProposedPromptExpectedOutput(TemporalDependencyUnfolding;Step1:StateIdentification):[List
ofkeystatesandvariables]
– ProposedPromptInput(TemporalDependencyUnfolding;Step2: TemporalGraphConstruction):
Createaconceptualgraphshowinghowtheidentifiedstatesevolveandinteractovertimeforthe
producer-consumersystem:
– ProposedPromptOutput(TemporalDependencyUnfolding;Step2:TemporalGraphConstruction):
[Conceptualgraphofstateevolutionandinteractions]
– ProposedPromptInput(TemporalDependencyUnfolding;Step3:StagedCodeGeneration):Generate
codefortheproducerfunctionalityintheproducer-consumersystem,focusingonitsinteractionwith
thebufferandsynchronizationmechanisms:
– ProposedPromptOutput(TemporalDependencyUnfolding;Step3:StagedCodeGeneration):[Python
codeforproducerfunctionality]
– ProposedPromptInput(TemporalDependencyUnfolding;Step4:ConsistencyVerification):Verifythe
temporalconsistencyofthegeneratedproducercodeandsuggestanynecessaryadjustments:
– ProposedPromptOutput(TemporalDependencyUnfolding;Step4:ConsistencyVerification):[Verifica-
tionandadjustmentsuggestions]
– ProposedPromptInput(TemporalDependencyUnfolding;Step5:Integration):Integratethegenerated
producercodewithaconsumerandmaincontrollogictocreateacompleteproducer-consumersystem,
ensuringproperhandlingofalltemporaldependencies:
– ProposedPromptOutput(TemporalDependencyUnfolding;Step5:Integration):[CompletePython
codeforproducer-consumersystem]
– Explanation: TheTemporalDependencyUnfoldingmethodproducesamorecomprehensiveand
robustsolutioncomparedtothebaseline.Itexplicitlyhandlestemporaldependencies,includesproper
synchronization,andprovidesmechanismsforgracefultermination.Thestagedapproachallowsfor
betterhandlingofedgecasesandimprovedoverallsystemdesign.
6.FallbackPlan:IftheTemporalDependencyUnfoldingmethoddoesnotshowsignificantimprovementoverthe
baselines,wecanpivottheprojectinseveralways.First,wecouldconductanin-depthanalysisofwhereandwhy
themethodfails,whichcouldprovidevaluableinsightsintothelimitationsofcurrentlanguagemodelsinhandling
temporalreasoningtasks. Thisanalysiscouldinvolveexaminingtheintermediateoutputs(stateidentification,
temporalgraphs)tounderstandwherethereasoningbreaksdown.Second,wecouldexplorecombiningourmethod
withothertechniques,suchasretrieval-augmentedgeneration,toseeifprovidingrelevantexamplesimproves
performance.Third,wecouldfocusondevelopinganewevaluationframeworkspecificallydesignedtoassess
temporalreasoningincodegeneration,whichcouldbeavaluablecontributiontothefieldevenifourprimary
methoddoesn’toutperformbaselines.Lastly,wecouldinvestigatewhetherthemethodperformsbetteroncertain
typesoftemporaldependenciesorspecificprogrammingdomains,whichcouldleadtoamoretargetedapproach
forimprovingcodegenerationinthoseareas.
85Reviewer1
Novelty:6(reasonablynovel-therearesomenotabledifferencesfromexistingideasandprobablyenoughtoturn
intoanewpaper)
Rationale:TheconstructionofTemporalGraphsoundsnovel.Theresearchquestionisalsorelativelyunderexplored,
butnecessaryforcodingindomainslikedistributedsystems.
Feasibility:6(Feasible:Canbeexecutedwithinthegivenconstraintswithsomereasonableplanning.)
Rationale:Thedatacollectionpartshouldbethemostchallengingpart.Collectinghigh-qualitycodingproblems
thatinvolvecomplextemporaldependenciescouldbehard.Also,thehumanevaluationmightalsotaketimeto
execute.
ExpectedEffectiveness:6(Somewhateffective:Thereisadecentchancethattheproposedideacanbeatexisting
baselinesbymoderatemarginsonafewbenchmarks.)
Rationale: Withspecificpromptingtechniques,theproposedmethodshouldoutperformbaselinesintermsof
temporaldependencies.
Excitement:7
Rationale:Ithinkthisshouldbemoreexcitingthanmostoftheborderlinepaperssinceweareworkingonanew
problem.Thecollecteddatashouldalsobesuperuseful.
OverallScore:7(Goodidea,wouldbeacceptedbymajorAIconferences)
Rationale:Again,workingonanovelproblemmakesitbetterthanmostofthepromptingpapers.
Confidence:4(Youareconfidentbutnotabsolutelycertainthattheevaluationiscorrect)
86Reviewer2
Novelty:5(somewhatnovel-therearedifferencesfromexistingideasbutnotenoughtoturnintoanewpaper)
Rationale:AlthoughIamnotentirelyfamiliarwiththefieldofgeneratingtemporallyadaptiveprograms,Isuspect
somesimilarideascanbefoundinsoftwareengineeringworks(e.g.,ICSE).Moreconcretelyonthemethod,itis
rathersimilartocodegenerationwithintermediatestatereasoning,whichhasbeenexploredinseveralmulti-step,
conversationalcodegenerationworks,e.g:
1.Zheng,Tianyu,etal."Opencodeinterpreter:Integratingcodegenerationwithexecutionandrefinement."
2.Cao,Liuwen,etal."BeyondCode:EvaluateThoughtStepsforComplexCodeGeneration."Proceedingsofthe2024
JointInternationalConferenceonComputationalLinguistics,LanguageResourcesandEvaluation(LREC-COLING
2024).2024.
3.Nijkamp,Erik,etal."Codegen:Anopenlargelanguagemodelforcodewithmulti-turnprogramsynthesis."
Feasibility:3(Verychallenging:thereareflawsintheproposedmethodorexperiments,ortheexperimentsrequire
compute/humanresourcesbeyondanyacademiclab)
Rationale:Itwouldbeprettyhardtocollectsuchdatasets(e.g.,wouldmostlyrequireawholerepository),further,it
wouldbedifficulttogenerateexecutabletestcasestoverifythemultipleproblemscreated.Especiallybecausethe
tasktargetstemporally-dependentmodulesintheprogram,itmaynecessitatedomainexpertstocarefullyconstruct
examplesandtests,whichwoulddemandalotoftimeandcosts.
ExpectedEffectiveness:5(Somewhatineffective:Theremightbesomechancethattheproposedideacanwork
betterthanexistingbaselinesbuttheimprovementwillbemarginalorinconsistent.)
Rationale:Iamnotveryconfidentthatthemodelcansolvethiscomplextemporally-dependentprogramming
problemswithreasonablecorrectness.Furthermore,becausethecurrentmethodisbasicallyprompting,whichmay
haveaverylowperformanceupperbound.Therefore,Idon’texpecttheproposedmethodtoimprovesignificantly
oncodegeneration.
Excitement:4
Rationale:Overall,Idon’texpectthismethodtobringsubstantialimprovements,henceamlessexcitedaboutthe
potentialofthismethod.Itwouldstillbeaninterestingproblemtosolve,particularlyinbringingmorechallenging
codingproblemsandproposedcorrespondingmethods.Withthisbeingsaid,giventhecurrentperformanceof
models,buildingasolidbenchmarkregardingthistemporalcodegenerationproblemmaybemoreexcitingthan
proposingamethodthatisexpectedlynotworking.
OverallScore:4(Okbutnotgoodenough,rejectionformajorAIconferences)
Rationale:Thetaskoftemporalcodegenerationisnotthemosturgentissueofcurrentcodegenerationmodels,and
theproposedmethodisexpectedtonotbringmuchimprovement.Themethodneedstobefurtherrefinedandgo
beyondsimplepromptingtoconvincetheaudienceofthepotentialofthisthreadofmethods.
Confidence:3(Youarefairlyconfidentthattheevaluationiscorrect)
87Reviewer3
Novelty:10(verynovel-verydifferentfromallexistingideasinaveryinterestingandcleverway)
Rationale:ThisideastudiesaverynovelprobleminLLM-basedcodegeneration.Temporaldependenciesincode
generationshouldbespecificallystudiedintheeraofLLMs.
Feasibility:5(Moderatelyfeasible:Itcanprobablybeexecutedwithinthegiventimeframebutwouldrequire
carefulplanning,efficientuseofAPIsorsomeadvancedcomputationalstrategiestoovercomethelimitedGPU
resources,andwouldrequiresomemodificationstotheoriginalproposaltomakeitwork.)
Rationale:Constructingareasonabledatasetischallengingwithinashorttime.Also,humanevaluationmighttake
moretime.WhetherLLMcanconstructhigh-qualitygraphsinthiscaseisalsotobeexamined.
ExpectedEffectiveness:6(Somewhateffective:Thereisadecentchancethattheproposedideacanbeatexisting
baselinesbymoderatemarginsonafewbenchmarks.)
Rationale:Oneneedstobuildreasonablemetricstoshoweffectiveness. Also,onemightneedtotuneprompts
carefullytoconstructhigh-qualitygraphsinthiscase.
Excitement:8(Exciting:woulddeepenthecommunity’sunderstandingormakemajorprogressinthisresearch
direction)
Rationale:Thisisnovelandcouldhaveahugeimpactonthosecodegenerationcasesrequiringtemporaldependen-
cies.Butoneneedstojustifywhysuchusecasesareimportant,andwhytemporaldependencyisthecoreproblem
insuchusecases.
OverallScore:9(Top15%ofallpublishedideasonthistopicatmajorAIconferences,strongaccept)
Rationale:Consideringitsnovelty,valuabledataset,andcomprehensivenessofexperimentandevaluationdesign,
thiscouldbeanimpactfulwork.Butoneneedstomakeexperimentresultsconcretebyre-examiningwhethereach
stepworkswellinpractice.
Confidence:4(Youareconfidentbutnotabsolutelycertainthattheevaluationiscorrect)
88X IdentitiesofExampleIdeas
WerevealwhethereachexampleideaisAI-generatedorhuman-written:
• Humanideas:ExampleP,ExampleR,ExampleT,ExampleV
• AIideas:ExampleQ,ExampleS,ExampleU,ExampleW
89Y AttemptonIdeaExecutionAgent
Forourexecutionagent,theinputisthegenerateidea(thefullprojectproposal),andtheoutputisa
Pythonfilethatcanbeexecutedwithourspecifiedcommand.Sincethereisoftenacommonpipeline
ofimplementingprompting-basedresearchideas,weprovideamanuallycraftedcodefileexampleas
template.Weattachthefulltemplatebelow:
import random
1
from tqdm import tqdm
2
from utils import call_api, load_model
3
import random
4
random.seed(2024)
5
6
## Step 1: Generate synthetic test examples
7
def generate_testset():
8
test_data = [
9
{
10
"input": "Natalia sold clips to 48 of her friends in April, and then
11
she sold half as many clips in May. How many clips did Natalia sell
altogether in April and May?",
"output": "Natalia sold 48/2 = <<48/2=24>>24 clips in May. Natalia sold
12
48+24 = <<48+24=72>>72 clips altogether in April and May. #### 72"
},
13
{
14
"input": "Weng earns $12 an hour for babysitting. Yesterday, she just
15
did 50 minutes of babysitting. How much did she earn?",
"output": "Weng earns 12/60 = $<<12/60=0.2>>0.2 per minute. Working 50
16
minutes, she earned 0.2 x 50 = $<<0.2*50=10>>10. #### 10"
},
17
{
18
"input": "Tim has 30 less apples than Martha, and Harry has half as
19
many apples as Tim. If Martha has 68 apples, how many apples does
Harry have?",
"output": "Tim has 68-30 = <<68-30=38>>38 apples. Harry has 38/2 =
20
<<38/2=19>>19 apples. #### 19"
},
21
{
22
"input": "Four people lost a total of 103 kilograms of weight. The
23
first person lost 27 kilograms. The second person lost 7 kilograms
less than the first person. The two remaining people lost the same
amount. How many kilograms did each of the last two people lose?",
"output": "Second person = 27 - 7 = <<27-7=20>>20 kg 103 - 27 - 20 =
24
<<103-27-20=56>>56 kg 56/2 = <<56/2=28>>28 kg The last two people
each lost 28 kilograms of weight. #### 28"
}
25
]
26
27
return test_data
28
29
30
## Step 2: Implement the baseline method
31
def baseline_method(client, model_name, seed, question):
32
## zero-shot chain-of-thought
33
prompt = "Answer the following question: {}\n".format(question)
34
prompt += "Think step by step."
35
prompt_messages = [{"role": "user", "content": prompt}]
36
response, _ = call_api(client, model_name, prompt_messages, temperature=0.,
37
max_tokens=2000, seed=seed, json_output=False)
return response.strip()
38
39
9040
## Step 3: Implement the proposed method
41
def proposed_method(client, model_name, seed, question, print_all=False):
42
intermediate_outputs = ""
43
44
if print_all:
45
print ("question:\n", question)
46
47
## collaborative reasoning step 1: task decomposition
48
prompt = "Please break down the following task into smaller sub-tasks or
49
steps:: {}".format(question)
prompt_messages = [{"role": "user", "content": prompt}]
50
decomposition, _ = call_api(client, model_name, prompt_messages,
51
temperature=0., max_tokens=2000, seed=seed, json_output=False)
intermediate_outputs += "task decomposition:\n" + decomposition + "\n"
52
if print_all:
53
print ("decomposition:\n", decomposition)
54
55
## collaborative reasoning step 2: sub-task information generation
56
prompt = "For each of the following sub-tasks, please generate relevant
57
information or intermediate results: \n{}".format(decomposition)
prompt_messages = [{"role": "user", "content": prompt}]
58
intermediate, _ = call_api(client, model_name, prompt_messages, temperature=0.,
59
max_tokens=2000, seed=seed, json_output=False)
intermediate_outputs += "sub-task results:\n" + intermediate + "\n"
60
if print_all:
61
print ("intermediate:\n", intermediate)
62
63
## collaborative reasoning step 3: result combination
64
prompt = "Given the following intermediate results: \n{}, please combine them
65
to generate the final answer for the task: \n{}".format(intermediate,
question)
prompt_messages = [{"role": "user", "content": prompt}]
66
answer, _ = call_api(client, model_name, prompt_messages, temperature=0.,
67
max_tokens=2000, seed=seed, json_output=False)
intermediate_outputs += "result combination:\n" + answer + "\n"
68
if print_all:
69
print ("initial answer:\n", answer)
70
71
## collaborative reasoning step 4: reflection and refinement
72
prompt = "Given the task: {}\nPlease reflect on the generated
73
answer:\n{}.\n\nAre there any gaps or inconsistencies in the answer? If so,
please identify and address them and give me an improved answer. If not, you
don’t have to edit anything and can just return the original
answer.\n".format(question, answer)
prompt_messages = [{"role": "user", "content": prompt}]
74
final_answer, _ = call_api(client, model_name, prompt_messages, temperature=0.,
75
max_tokens=2000, seed=seed, json_output=False)
intermediate_outputs += "reflection and refinement:\n" + final_answer
76
if print_all:
77
print ("final answer:\n", final_answer)
78
79
return final_answer.strip(), intermediate_outputs
80
81
82
## Step 4: Define the style evaluator
83
def style_evaluator(client, model_name, seed, question, baseline_prediction,
84
proposed_prediction):
## define all the components that the proposed method outputs should have
85
## and the advantages of the proposed method over the baseline method
86
## just need to check the style is correct
87
prompt = "Given the task: {}\n".format(question)
88
91prompt += "The baseline method produced the following
89
output:\n{}\n\n".format(baseline_prediction)
prompt += "The proposed new method produced the following
90
output:\n{}\n\n".format(proposed_prediction)
prompt += "Now determine if the proposed method is better by checking if it has
91
satisfied the following criteria:\n"
prompt += "1. The proposed method’s output should produce all the intermediate
92
components including: task decomposition, sub-task information generation,
result combination, and reflection and refinement.\n"
prompt += "2. The proposed method should provide a more detailed and
93
comprehensive answer than the baseline method.\n"
prompt += "Just tell me ’yes’ or ’no’ for whether the criteria are met, nothing
94
else is needed."
prompt_messages = [{"role": "user", "content": prompt}]
95
response, _ = call_api(client, model_name, prompt_messages, temperature=0.,
96
max_tokens=1, seed=seed, json_output=False)
97
judgment = False
98
if response.strip().lower() == "yes":
99
return True
100
101
return judgment
102
103
104
## Step 5: Define the output evaluator
105
def output_evaluator(client, model_name, seed, question, gold_label, prediction):
106
## check if the prediction is correct given the gold label
107
prompt = "Given the following question and reference answer, determine if the
108
prediction is correct. Just tell me ’yes’ or ’no’, nothing else is
needed.\n\nQuestion: {}\n\nReference Answer: {}\n\nPrediction:
{}\n\n".format(question, gold_label, prediction)
prompt_messages = [{"role": "user", "content": prompt}]
109
response, _ = call_api(client, model_name, prompt_messages, temperature=0.,
110
max_tokens=1, seed=seed, json_output=False)
111
judgment = False
112
if response.strip().lower() == "yes":
113
return True
114
115
return judgment
116
117
118
## Step 6: Define the function that runs the experiments to obtain model
119
predictions and performance
## you shouldn’t need to modify this function in most cases
120
def run_experiment(client, model_name, seed, testset):
121
sample_size = len(testset)
122
baseline_predictions = []
123
proposed_predictions = []
124
125
baseline_correctness = []
126
proposed_correctness = []
127
128
style_check = []
129
130
for i in tqdm(range(sample_size)):
131
question = testset[i]["input"].strip()
132
gold_label = testset[i]["output"].strip()
133
134
baseline_prediction = baseline_method(client, model_name, seed, question)
135
proposed_prediction_final, proposed_prediction_intermediate =
136
proposed_method(client, model_name, seed, question)
92baseline_predictions.append(baseline_prediction)
137
proposed_predictions.append(proposed_prediction_final)
138
139
baseline_correctness.append(output_evaluator(client, model_name, seed,
140
question, gold_label, baseline_prediction))
proposed_correctness.append(output_evaluator(client, model_name, seed,
141
question, gold_label, proposed_prediction_final))
142
style_check.append(style_evaluator(client, model_name, seed, question,
143
baseline_prediction, proposed_prediction_intermediate))
144
return baseline_correctness, proposed_correctness, style_check
145
146
147
## Step 7: Execute the experiments and compare performance
148
if __name__ == "__main__":
149
testset = generate_testset()
150
print ("simulated {} test examples for evaluation.".format(len(testset)))
151
152
model_name = "claude-3-opus-20240229"
153
seed = 2024
154
client = load_model(model_name)
155
print ("using model: ", model_name)
156
157
## output correctness
158
baseline_correctness, proposed_correctness, style_check =
159
run_experiment(client, model_name, seed, testset)
print ("baseline correctness: ", sum(baseline_correctness) /
160
len(baseline_correctness))
print ("proposed correctness: ", sum(proposed_correctness) /
161
len(proposed_correctness))
print ("style check pass rate: ", sum(style_check) / len(style_check))
162
As seen above, we have defined two different evaluator functions. The style_evaluator()
checks whether all components mentioned in the proposed method are implemented, and the
output_evaluator()checkswhetherthemodelpredictionsarecorrect.
Weprovidedthistemplatealongwithinstructionstotelltheexecutionagenttomodifyallnecessary
functionsfortheprovidedidea.Wetestedtheagenton30safetypromptingideasand30factuality
promptingideas.
Safety Factuality
Total 30 30
Executed 17 18
PassedStyleEvaluator 13 14
ProposedMethodWorksBetter 5 1
Table18:Successrateofourexecutionagenton30randomlysampledAIideasonsafetyandfactuality.
AsshowninTable18,overhalfofthegeneratedcodefilescanbesuccessfullyexecuted,andtheresults
indicatethatonlyahandfuloftheseproposedmethodsactuallyoutperformthebaselines.Whilethis
showssomepromisethattheexecutionagentscanautomaticallyimplementandidentifyeffective
methodsoutofallthegeneratedideas,acarefulmanualcheckrevealsmanypitfalls.
93Forexample,theagentimplementedthefollowingbaselineforthetaskofidentifyingmaliciousand
adversarialprompts,whichisakeyword-basedapproachthatistrivialtobeatwithanyLLM-based
methods.
## Step 2: Implement the baseline method
1
def baseline_method(client, model_name, seed, prompt):
2
# Keyword-based heuristic
3
keywords = ["ignore", "pretend", "roleplay", "disregard", "override"]
4
for keyword in keywords:
5
if keyword in prompt.lower():
6
return "Adversarial"
7
return "Benign"
8
Inanotherexample,forthesametaskofdetectingadversarialprompts,theagentimplementedthe
followingevaluatorfunction:
## Step 5: Define the output evaluator
1
def output_evaluator(client, model_name, seed, input_text, gold_label, prediction):
2
prompt = "Given the following text and reference sentiment classification,
3
determine if the predicted classification is correct. Just tell me ’yes’ or
’no’, nothing else is needed.\n\nText: {}\n\nReference: {}\n\nPrediction:
{}\n\n".format(input_text, gold_label, prediction)
prompt_messages = [{"role": "user", "content": prompt}]
4
response, _ = call_api(client, model_name, prompt_messages, temperature=0.,
5
max_tokens=1, seed=seed, json_output=False)
6
judgment = False
7
if response.strip().lower() == "yes":
8
return True
9
10
return judgment
11
Theagentissupposedtoinjectadversarialtriggersintosentimentclassificationdatatotestwhether
theproposedmethodcandetectthoseadversarialpromptswhilemaintainingsentimentclassification
accuracy.However,theagentonlyevaluatestheaccuracyontheoriginalsentimentclassificationtask
butnotthetaskofadversarialpromptdetection.
Giventheseerrors,webelievemoreworkisneededtocarefullyverifythecodeimplementations
producedbytheexecutionagentratherthanblindlytrustingtheirexecutedresults,andweleavesuch
attemptstofuturework.
94