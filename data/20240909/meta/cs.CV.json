[
    {
        "title": "Synergy and Synchrony in Couple Dances",
        "authors": "Vongani MalulekeLea MüllerJathushan RajasegaranGeorgios PavlakosShiry GinosarAngjoo KanazawaJitendra Malik",
        "links": "http://arxiv.org/abs/2409.04440v1",
        "entry_id": "http://arxiv.org/abs/2409.04440v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04440v1",
        "summary": "This paper asks to what extent social interaction influences one's behavior.\nWe study this in the setting of two dancers dancing as a couple. We first\nconsider a baseline in which we predict a dancer's future moves conditioned\nonly on their past motion without regard to their partner. We then investigate\nthe advantage of taking social information into account by conditioning also on\nthe motion of their dancing partner. We focus our analysis on Swing, a dance\ngenre with tight physical coupling for which we present an in-the-wild video\ndataset. We demonstrate that single-person future motion prediction in this\ncontext is challenging. Instead, we observe that prediction greatly benefits\nfrom considering the interaction partners' behavior, resulting in surprisingly\ncompelling couple dance synthesis results (see supp. video). Our contributions\nare a demonstration of the advantages of socially conditioned future motion\nprediction and an in-the-wild, couple dance video dataset to enable future\nresearch in this direction. Video results are available on the project website:\nhttps://von31.github.io/synNsync",
        "updated": "2024-09-06 17:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04440v1"
    },
    {
        "title": "VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation",
        "authors": "Yecheng WuZhuoyang ZhangJunyu ChenHaotian TangDacheng LiYunhao FangLigeng ZhuEnze XieHongxu YinLi YiSong HanYao Lu",
        "links": "http://arxiv.org/abs/2409.04429v1",
        "entry_id": "http://arxiv.org/abs/2409.04429v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04429v1",
        "summary": "VILA-U is a Unified foundation model that integrates Video, Image, Language\nunderstanding and generation. Traditional visual language models (VLMs) use\nseparate modules for understanding and generating visual content, which can\nlead to misalignment and increased complexity. In contrast, VILA-U employs a\nsingle autoregressive next-token prediction framework for both tasks,\neliminating the need for additional components like diffusion models. This\napproach not only simplifies the model but also achieves near state-of-the-art\nperformance in visual language understanding and generation. The success of\nVILA-U is attributed to two main factors: the unified vision tower that aligns\ndiscrete visual tokens with textual inputs during pretraining, which enhances\nvisual perception, and autoregressive image generation can achieve similar\nquality as diffusion models with high-quality dataset. This allows VILA-U to\nperform comparably to more complex models using a fully token-based\nautoregressive framework.",
        "updated": "2024-09-06 17:49:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04429v1"
    },
    {
        "title": "Exploring Foundation Models for Synthetic Medical Imaging: A Study on Chest X-Rays and Fine-Tuning Techniques",
        "authors": "Davide Clode da SilvaMarina Musse BernardesNathalia Giacomini CerettaGabriel Vaz de SouzaGabriel Fonseca SilvaRafael Heitor BordiniSoraia Raupp Musse",
        "links": "http://arxiv.org/abs/2409.04424v1",
        "entry_id": "http://arxiv.org/abs/2409.04424v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04424v1",
        "summary": "Machine learning has significantly advanced healthcare by aiding in disease\nprevention and treatment identification. However, accessing patient data can be\nchallenging due to privacy concerns and strict regulations. Generating\nsynthetic, realistic data offers a potential solution for overcoming these\nlimitations, and recent studies suggest that fine-tuning foundation models can\nproduce such data effectively. In this study, we explore the potential of\nfoundation models for generating realistic medical images, particularly chest\nx-rays, and assess how their performance improves with fine-tuning. We propose\nusing a Latent Diffusion Model, starting with a pre-trained foundation model\nand refining it through various configurations. Additionally, we performed\nexperiments with input from a medical professional to assess the realism of the\nimages produced by each trained model.",
        "updated": "2024-09-06 17:36:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04424v1"
    },
    {
        "title": "Open-MAGVIT2: An Open-Source Project Toward Democratizing Auto-regressive Visual Generation",
        "authors": "Zhuoyan LuoFengyuan ShiYixiao GeYujiu YangLimin WangYing Shan",
        "links": "http://arxiv.org/abs/2409.04410v1",
        "entry_id": "http://arxiv.org/abs/2409.04410v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04410v1",
        "summary": "We present Open-MAGVIT2, a family of auto-regressive image generation models\nranging from 300M to 1.5B. The Open-MAGVIT2 project produces an open-source\nreplication of Google's MAGVIT-v2 tokenizer, a tokenizer with a super-large\ncodebook (i.e., $2^{18}$ codes), and achieves the state-of-the-art\nreconstruction performance (1.17 rFID) on ImageNet $256 \\times 256$.\nFurthermore, we explore its application in plain auto-regressive models and\nvalidate scalability properties. To assist auto-regressive models in predicting\nwith a super-large vocabulary, we factorize it into two sub-vocabulary of\ndifferent sizes by asymmetric token factorization, and further introduce \"next\nsub-token prediction\" to enhance sub-token interaction for better generation\nquality. We release all models and codes to foster innovation and creativity in\nthe field of auto-regressive visual generation.",
        "updated": "2024-09-06 17:14:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04410v1"
    },
    {
        "title": "Train Till You Drop: Towards Stable and Robust Source-free Unsupervised 3D Domain Adaptation",
        "authors": "Björn MicheleAlexandre BoulchTuan-Hung VuGilles PuyRenaud MarletNicolas Courty",
        "links": "http://arxiv.org/abs/2409.04409v1",
        "entry_id": "http://arxiv.org/abs/2409.04409v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04409v1",
        "summary": "We tackle the challenging problem of source-free unsupervised domain\nadaptation (SFUDA) for 3D semantic segmentation. It amounts to performing\ndomain adaptation on an unlabeled target domain without any access to source\ndata; the available information is a model trained to achieve good performance\non the source domain. A common issue with existing SFUDA approaches is that\nperformance degrades after some training time, which is a by product of an\nunder-constrained and ill-posed problem. We discuss two strategies to alleviate\nthis issue. First, we propose a sensible way to regularize the learning\nproblem. Second, we introduce a novel criterion based on agreement with a\nreference model. It is used (1) to stop the training when appropriate and (2)\nas validator to select hyperparameters without any knowledge on the target\ndomain. Our contributions are easy to implement and readily amenable for all\nSFUDA methods, ensuring stable improvements over all baselines. We validate our\nfindings on various 3D lidar settings, achieving state-of-the-art performance.\nThe project repository (with code) is: github.com/valeoai/TTYD.",
        "updated": "2024-09-06 17:13:14 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04409v1"
    }
]