[
    {
        "title": "SPACE: A Python-based Simulator for Evaluating Decentralized Multi-Robot Task Allocation Algorithms",
        "authors": "Inmo Jang",
        "links": "http://arxiv.org/abs/2409.04230v1",
        "entry_id": "http://arxiv.org/abs/2409.04230v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04230v1",
        "summary": "Swarm robotics explores the coordination of multiple robots to achieve\ncollective goals, with collective decision-making being a central focus. This\nprocess involves decentralized robots autonomously making local decisions and\ncommunicating them, which influences the overall emergent behavior. Testing\nsuch decentralized algorithms in real-world scenarios with hundreds or more\nrobots is often impractical, underscoring the need for effective simulation\ntools. We propose SPACE (Swarm Planning and Control Evaluation), a Python-based\nsimulator designed to support the research, evaluation, and comparison of\ndecentralized Multi-Robot Task Allocation (MRTA) algorithms. SPACE streamlines\ncore algorithmic development by allowing users to implement decision-making\nalgorithms as Python plug-ins, easily construct agent behavior trees via an\nintuitive GUI, and leverage built-in support for inter-agent communication and\nlocal task awareness. To demonstrate its practical utility, we implement and\nevaluate CBBA and GRAPE within the simulator, comparing their performance\nacross different metrics, particularly in scenarios with dynamically introduced\ntasks. This evaluation shows the usefulness of SPACE in conducting rigorous and\nstandardized comparisons of MRTA algorithms, helping to support future research\nin the field.",
        "updated": "2024-09-06 12:38:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04230v1"
    },
    {
        "title": "Multi-agent Path Finding for Mixed Autonomy Traffic Coordination",
        "authors": "Han ZhengZhongxia YanCathy Wu",
        "links": "http://arxiv.org/abs/2409.03881v1",
        "entry_id": "http://arxiv.org/abs/2409.03881v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03881v1",
        "summary": "In the evolving landscape of urban mobility, the prospective integration of\nConnected and Automated Vehicles (CAVs) with Human-Driven Vehicles (HDVs)\npresents a complex array of challenges and opportunities for autonomous driving\nsystems. While recent advancements in robotics have yielded Multi-Agent Path\nFinding (MAPF) algorithms tailored for agent coordination task characterized by\nsimplified kinematics and complete control over agent behaviors, these\nsolutions are inapplicable in mixed-traffic environments where uncontrollable\nHDVs must coexist and interact with CAVs. Addressing this gap, we propose the\nBehavior Prediction Kinematic Priority Based Search (BK-PBS), which leverages\nan offline-trained conditional prediction model to forecast HDV responses to\nCAV maneuvers, integrating these insights into a Priority Based Search (PBS)\nwhere the A* search proceeds over motion primitives to accommodate kinematic\nconstraints. We compare BK-PBS with CAV planning algorithms derived by\nrule-based car-following models, and reinforcement learning. Through\ncomprehensive simulation on a highway merging scenario across diverse scenarios\nof CAV penetration rate and traffic density, BK-PBS outperforms these baselines\nin reducing collision rates and enhancing system-level travel delay. Our work\nis directly applicable to many scenarios of multi-human multi-robot\ncoordination.",
        "updated": "2024-09-05 19:37:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03881v1"
    },
    {
        "title": "PARCO: Learning Parallel Autoregressive Policies for Efficient Multi-Agent Combinatorial Optimization",
        "authors": "Federico BertoChuanbo HuaLaurin LuttmannJiwoo SonJunyoung ParkKyuree AhnChanghyun KwonLin XieJinkyoo Park",
        "links": "http://arxiv.org/abs/2409.03811v1",
        "entry_id": "http://arxiv.org/abs/2409.03811v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03811v1",
        "summary": "Multi-agent combinatorial optimization problems such as routing and\nscheduling have great practical relevance but present challenges due to their\nNP-hard combinatorial nature, hard constraints on the number of possible\nagents, and hard-to-optimize objective functions. This paper introduces PARCO\n(Parallel AutoRegressive Combinatorial Optimization), a novel approach that\nlearns fast surrogate solvers for multi-agent combinatorial problems with\nreinforcement learning by employing parallel autoregressive decoding. We\npropose a model with a Multiple Pointer Mechanism to efficiently decode\nmultiple decisions simultaneously by different agents, enhanced by a\nPriority-based Conflict Handling scheme. Moreover, we design specialized\nCommunication Layers that enable effective agent collaboration, thus enriching\ndecision-making. We evaluate PARCO in representative multi-agent combinatorial\nproblems in routing and scheduling and demonstrate that our learned solvers\noffer competitive results against both classical and neural baselines in terms\nof both solution quality and speed. We make our code openly available at\nhttps://github.com/ai4co/parco.",
        "updated": "2024-09-05 17:49:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03811v1"
    },
    {
        "title": "Non-stationary and Sparsely-correlated Multi-output Gaussian Process with Spike-and-Slab Prior",
        "authors": "Wang XinmingLi YongxiangYue XiaoweiWu Jianguo",
        "links": "http://arxiv.org/abs/2409.03149v1",
        "entry_id": "http://arxiv.org/abs/2409.03149v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03149v1",
        "summary": "Multi-output Gaussian process (MGP) is commonly used as a transfer learning\nmethod to leverage information among multiple outputs. A key advantage of MGP\nis providing uncertainty quantification for prediction, which is highly\nimportant for subsequent decision-making tasks. However, traditional MGP may\nnot be sufficiently flexible to handle multivariate data with dynamic\ncharacteristics, particularly when dealing with complex temporal correlations.\nAdditionally, since some outputs may lack correlation, transferring information\namong them may lead to negative transfer. To address these issues, this study\nproposes a non-stationary MGP model that can capture both the dynamic and\nsparse correlation among outputs. Specifically, the covariance functions of MGP\nare constructed using convolutions of time-varying kernel functions. Then a\ndynamic spike-and-slab prior is placed on correlation parameters to\nautomatically decide which sources are informative to the target output in the\ntraining process. An expectation-maximization (EM) algorithm is proposed for\nefficient model fitting. Both numerical studies and a real case demonstrate its\nefficacy in capturing dynamic and sparse correlation structure and mitigating\nnegative transfer for high-dimensional time-series data. Finally, a\nmountain-car reinforcement learning case highlights its potential application\nin decision making problems.",
        "updated": "2024-09-05 00:56:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03149v1"
    },
    {
        "title": "An Introduction to Centralized Training for Decentralized Execution in Cooperative Multi-Agent Reinforcement Learning",
        "authors": "Christopher Amato",
        "links": "http://arxiv.org/abs/2409.03052v1",
        "entry_id": "http://arxiv.org/abs/2409.03052v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03052v1",
        "summary": "Multi-agent reinforcement learning (MARL) has exploded in popularity in\nrecent years. Many approaches have been developed but they can be divided into\nthree main types: centralized training and execution (CTE), centralized\ntraining for decentralized execution (CTDE), and Decentralized training and\nexecution (DTE).\n  CTDE methods are the most common as they can use centralized information\nduring training but execute in a decentralized manner -- using only information\navailable to that agent during execution. CTDE is the only paradigm that\nrequires a separate training phase where any available information (e.g., other\nagent policies, underlying states) can be used. As a result, they can be more\nscalable than CTE methods, do not require communication during execution, and\ncan often perform well. CTDE fits most naturally with the cooperative case, but\ncan be potentially applied in competitive or mixed settings depending on what\ninformation is assumed to be observed.\n  This text is an introduction to CTDE in cooperative MARL. It is meant to\nexplain the setting, basic concepts, and common methods. It does not cover all\nwork in CTDE MARL as the subarea is quite extensive. I have included work that\nI believe is important for understanding the main concepts in the subarea and\napologize to those that I have omitted.",
        "updated": "2024-09-04 19:54:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03052v1"
    }
]