[
    {
        "title": "Accelerating Training with Neuron Interaction and Nowcasting Networks",
        "authors": "Boris KnyazevAbhinav MoudgilGuillaume LajoieEugene BelilovskySimon Lacoste-Julien",
        "links": "http://arxiv.org/abs/2409.04434v1",
        "entry_id": "http://arxiv.org/abs/2409.04434v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04434v1",
        "summary": "Neural network training can be accelerated when a learnable update rule is\nused in lieu of classic adaptive optimizers (e.g. Adam). However, learnable\nupdate rules can be costly and unstable to train and use. A simpler recently\nproposed approach to accelerate training is to use Adam for most of the\noptimization steps and periodically, only every few steps, nowcast (predict\nfuture) parameters. We improve this approach by Neuron interaction and\nNowcasting (NiNo) networks. NiNo leverages neuron connectivity and graph neural\nnetworks to more accurately nowcast parameters by learning in a supervised way\nfrom a set of training trajectories over multiple tasks. We show that in some\nnetworks, such as Transformers, neuron connectivity is non-trivial. By\naccurately modeling neuron connectivity, we allow NiNo to accelerate Adam\ntraining by up to 50\\% in vision and language tasks.",
        "updated": "2024-09-06 17:55:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04434v1"
    },
    {
        "title": "Provable Hyperparameter Tuning for Structured Pfaffian Settings",
        "authors": "Maria-Florina BalcanAnh Tuan NguyenDravyansh Sharma",
        "links": "http://arxiv.org/abs/2409.04367v1",
        "entry_id": "http://arxiv.org/abs/2409.04367v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04367v1",
        "summary": "Data-driven algorithm design automatically adapts algorithms to specific\napplication domains, achieving better performance. In the context of\nparameterized algorithms, this approach involves tuning the algorithm\nparameters using problem instances drawn from the problem distribution of the\ntarget application domain. While empirical evidence supports the effectiveness\nof data-driven algorithm design, providing theoretical guarantees for several\nparameterized families remains challenging. This is due to the intricate\nbehaviors of their corresponding utility functions, which typically admit\npiece-wise and discontinuity structures. In this work, we present refined\nframeworks for providing learning guarantees for parameterized data-driven\nalgorithm design problems in both distributional and online learning settings.\nFor the distributional learning setting, we introduce the Pfaffian GJ\nframework, an extension of the classical GJ framework, capable of providing\nlearning guarantees for function classes for which the computation involves\nPfaffian functions. Unlike the GJ framework, which is limited to function\nclasses with computation characterized by rational functions, our proposed\nframework can deal with function classes involving Pfaffian functions, which\nare much more general and widely applicable. We then show that for many\nparameterized algorithms of interest, their utility function possesses a\nrefined piece-wise structure, which automatically translates to learning\nguarantees using our proposed framework. For the online learning setting, we\nprovide a new tool for verifying dispersion property of a sequence of loss\nfunctions. This sufficient condition allows no-regret learning for sequences of\npiece-wise structured loss functions where the piece-wise structure involves\nPfaffian transition boundaries.",
        "updated": "2024-09-06 15:58:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04367v1"
    },
    {
        "title": "Leveraging Machine Learning for Official Statistics: A Statistical Manifesto",
        "authors": "Marco PutsDavid SalgadoPiet Daas",
        "links": "http://arxiv.org/abs/2409.04365v1",
        "entry_id": "http://arxiv.org/abs/2409.04365v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04365v1",
        "summary": "It is important for official statistics production to apply ML with\nstatistical rigor, as it presents both opportunities and challenges. Although\nmachine learning has enjoyed rapid technological advances in recent years, its\napplication does not possess the methodological robustness necessary to produce\nhigh quality statistical results. In order to account for all sources of error\nin machine learning models, the Total Machine Learning Error (TMLE) is\npresented as a framework analogous to the Total Survey Error Model used in\nsurvey methodology. As a means of ensuring that ML models are both internally\nvalid as well as externally valid, the TMLE model addresses issues such as\nrepresentativeness and measurement errors. There are several case studies\npresented, illustrating the importance of applying more rigor to the\napplication of machine learning in official statistics.",
        "updated": "2024-09-06 15:57:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04365v1"
    },
    {
        "title": "A naive aggregation algorithm for improving generalization in a class of learning problems",
        "authors": "Getachew K Befekadu",
        "links": "http://arxiv.org/abs/2409.04352v1",
        "entry_id": "http://arxiv.org/abs/2409.04352v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04352v1",
        "summary": "In this brief paper, we present a naive aggregation algorithm for a typical\nlearning problem with expert advice setting, in which the task of improving\ngeneralization, i.e., model validation, is embedded in the learning process as\na sequential decision-making problem. In particular, we consider a class of\nlearning problem of point estimations for modeling high-dimensional nonlinear\nfunctions, where a group of experts update their parameter estimates using the\ndiscrete-time version of gradient systems, with small additive noise term,\nguided by the corresponding subsample datasets obtained from the original\ndataset. Here, our main objective is to provide conditions under which such an\nalgorithm will sequentially determine a set of mixing distribution strategies\nused for aggregating the experts' estimates that ultimately leading to an\noptimal parameter estimate, i.e., as a consensus solution for all experts,\nwhich is better than any individual expert's estimate in terms of improved\ngeneralization or learning performances. Finally, as part of this work, we\npresent some numerical results for a typical case of nonlinear regression\nproblem.",
        "updated": "2024-09-06 15:34:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04352v1"
    },
    {
        "title": "Amortized Bayesian Workflow (Extended Abstract)",
        "authors": "Marvin SchmittChengkun LiAki VehtariLuigi AcerbiPaul-Christian BürknerStefan T. Radev",
        "links": "http://arxiv.org/abs/2409.04332v1",
        "entry_id": "http://arxiv.org/abs/2409.04332v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04332v1",
        "summary": "Bayesian inference often faces a trade-off between computational speed and\nsampling accuracy. We propose an adaptive workflow that integrates rapid\namortized inference with gold-standard MCMC techniques to achieve both speed\nand accuracy when performing inference on many observed datasets. Our approach\nuses principled diagnostics to guide the choice of inference method for each\ndataset, moving along the Pareto front from fast amortized sampling to slower\nbut guaranteed-accurate MCMC when necessary. By reusing computations across\nsteps, our workflow creates synergies between amortized and MCMC-based\ninference. We demonstrate the effectiveness of this integrated approach on a\ngeneralized extreme value task with 1000 observed data sets, showing 90x time\nefficiency gains while maintaining high posterior quality.",
        "updated": "2024-09-06 15:09:04 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04332v1"
    }
]