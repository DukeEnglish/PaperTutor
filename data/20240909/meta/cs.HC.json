[
    {
        "title": "Virtual Reality-Based Preoperative Planning for Optimized Trocar Placement in Thoracic Surgery: A Preliminary Study",
        "authors": "Arash HarirpoushGeorge RakovichMarta Kersten-OertelYiming Xiao",
        "links": "http://arxiv.org/abs/2409.04414v1",
        "entry_id": "http://arxiv.org/abs/2409.04414v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04414v1",
        "summary": "Video-assisted thoracic surgery (VATS) is a minimally invasive approach for\ntreating early-stage non-small-cell lung cancer. Optimal trocar placement\nduring VATS ensures comprehensive access to the thoracic cavity, provides a\npanoramic endoscopic view, and prevents instrument crowding. While established\nprinciples such as the Baseball Diamond Principle (BDP) and Triangle Target\nPrinciple (TTP) exist, surgeons mainly rely on experience and patient-specific\nanatomy for trocar placement, potentially leading to sub-optimal surgical plans\nthat increase operative time and fatigue. To address this, we present the first\nvirtual reality (VR)-based pre-operative planning tool with tailored data\nvisualization and interaction designs for efficient and optimal VATS trocar\nplacement, following the established surgical principles and consultation with\nan experienced surgeon. In our preliminary study, we demonstrate the system's\napplication in right upper lung lobectomy, a common thoracic procedure\ntypically using three trocars. A preliminary user study of our system indicates\nit is efficient, robust, and user-friendly for planning optimal trocar\nplacement, with a great promise for clinical application while offering\npotentially valuable insights for the development of other surgical VR systems.",
        "updated": "2024-09-06 17:17:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04414v1"
    },
    {
        "title": "Can LLMs Generate Novel Research Ideas? A Large-Scale Human Study with 100+ NLP Researchers",
        "authors": "Chenglei SiDiyi YangTatsunori Hashimoto",
        "links": "http://arxiv.org/abs/2409.04109v1",
        "entry_id": "http://arxiv.org/abs/2409.04109v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04109v1",
        "summary": "Recent advancements in large language models (LLMs) have sparked optimism\nabout their potential to accelerate scientific discovery, with a growing number\nof works proposing research agents that autonomously generate and validate new\nideas. Despite this, no evaluations have shown that LLM systems can take the\nvery first step of producing novel, expert-level ideas, let alone perform the\nentire research process. We address this by establishing an experimental design\nthat evaluates research idea generation while controlling for confounders and\nperforms the first head-to-head comparison between expert NLP researchers and\nan LLM ideation agent. By recruiting over 100 NLP researchers to write novel\nideas and blind reviews of both LLM and human ideas, we obtain the first\nstatistically significant conclusion on current LLM capabilities for research\nideation: we find LLM-generated ideas are judged as more novel (p < 0.05) than\nhuman expert ideas while being judged slightly weaker on feasibility. Studying\nour agent baselines closely, we identify open problems in building and\nevaluating research agents, including failures of LLM self-evaluation and their\nlack of diversity in generation. Finally, we acknowledge that human judgements\nof novelty can be difficult, even by experts, and propose an end-to-end study\ndesign which recruits researchers to execute these ideas into full projects,\nenabling us to study whether these novelty and feasibility judgements result in\nmeaningful differences in research outcome.",
        "updated": "2024-09-06 08:25:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04109v1"
    },
    {
        "title": "MixNet: Joining Force of Classical and Modern Approaches Toward the Comprehensive Pipeline in Motor Imagery EEG Classification",
        "authors": "Phairot AutthasanRattanaphon ChaisaenHuy PhanMaarten De VosTheerawit Wilaiprasitporn",
        "links": "http://dx.doi.org/10.1109/JIOT.2024.3402254",
        "entry_id": "http://arxiv.org/abs/2409.04104v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04104v1",
        "summary": "Recent advances in deep learning (DL) have significantly impacted motor\nimagery (MI)-based brain-computer interface (BCI) systems, enhancing the\ndecoding of electroencephalography (EEG) signals. However, most studies\nstruggle to identify discriminative patterns across subjects during MI tasks,\nlimiting MI classification performance. In this article, we propose MixNet, a\nnovel classification framework designed to overcome this limitation by\nutilizing spectral-spatial signals from MI data, along with a multitask\nlearning architecture named MIN2Net, for classification. Here, the\nspectral-spatial signals are generated using the filter-bank common spatial\npatterns (FBCSPs) method on MI data. Since the multitask learning architecture\nis used for the classification task, the learning in each task may exhibit\ndifferent generalization rates and potential overfitting across tasks. To\naddress this issue, we implement adaptive gradient blending, simultaneously\nregulating multiple loss weights and adjusting the learning pace for each task\nbased on its generalization/overfitting tendencies. Experimental results on six\nbenchmark data sets of different data sizes demonstrate that MixNet\nconsistently outperforms all state-of-the-art algorithms in subject-dependent\nand -independent settings. Finally, the low-density EEG MI classification\nresults show that MixNet outperforms all state-of-the-art algorithms, offering\npromising implications for Internet of Thing (IoT) applications, such as\nlightweight and portable EEG wearable devices based on low-density montages.",
        "updated": "2024-09-06 08:14:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04104v1"
    },
    {
        "title": "What Guides Our Choices? Modeling Developers' Trust and Behavioral Intentions Towards GenAI",
        "authors": "Rudrajit ChoudhuriBianca TrinkenreichRahul PanditaEirini KalliamvakouIgor SteinmacherMarco GerosaChristopher SanchezAnita Sarma",
        "links": "http://arxiv.org/abs/2409.04099v1",
        "entry_id": "http://arxiv.org/abs/2409.04099v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04099v1",
        "summary": "Generative AI (genAI) tools, such as ChatGPT or Copilot, are advertised to\nimprove developer productivity and are being integrated into software\ndevelopment. However, misaligned trust, skepticism, and usability concerns can\nimpede the adoption of such tools. Research also indicates that AI can be\nexclusionary, failing to support diverse users adequately. One such aspect of\ndiversity is cognitive diversity -- variations in users' cognitive styles --\nthat leads to divergence in perspectives and interaction styles. When an\nindividual's cognitive style is unsupported, it creates barriers to technology\nadoption. Therefore, to understand how to effectively integrate genAI tools\ninto software development, it is first important to model what factors affect\ndevelopers' trust and intentions to adopt genAI tools in practice?\n  We developed a theoretical model to (1) identify factors that influence\ndevelopers' trust in genAI tools and (2) examine the relationship between\ndevelopers' trust, cognitive styles, and their intentions to use these tools.\nWe surveyed software developers (N=238) at two major global tech organizations\nand employed Partial Least Squares-Structural Equation Modeling (PLS-SEM) to\nevaluate our model. Our findings reveal that genAI's system/output quality,\nfunctional value, and goal maintenance significantly influence developers'\ntrust in these tools. Furthermore, developers' trust and cognitive styles\ninfluence their intentions to use these tools. We offer practical suggestions\nfor designing genAI tools for effective use and inclusive user experience.",
        "updated": "2024-09-06 08:05:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04099v1"
    },
    {
        "title": "UI-JEPA: Towards Active Perception of User Intent through Onscreen User Activity",
        "authors": "Yicheng FuRaviteja AnanthaPrabal VashishtJianpeng ChengEtai Littwin",
        "links": "http://arxiv.org/abs/2409.04081v1",
        "entry_id": "http://arxiv.org/abs/2409.04081v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04081v1",
        "summary": "Generating user intent from a sequence of user interface (UI) actions is a\ncore challenge in comprehensive UI understanding. Recent advancements in\nmultimodal large language models (MLLMs) have led to substantial progress in\nthis area, but their demands for extensive model parameters, computing power,\nand high latency makes them impractical for scenarios requiring lightweight,\non-device solutions with low latency or heightened privacy. Additionally, the\nlack of high-quality datasets has hindered the development of such lightweight\nmodels. To address these challenges, we propose UI-JEPA, a novel framework that\nemploys masking strategies to learn abstract UI embeddings from unlabeled data\nthrough self-supervised learning, combined with an LLM decoder fine-tuned for\nuser intent prediction. We also introduce two new UI-grounded multimodal\ndatasets, \"Intent in the Wild\" (IIW) and \"Intent in the Tame\" (IIT), designed\nfor few-shot and zero-shot UI understanding tasks. IIW consists of 1.7K videos\nacross 219 intent categories, while IIT contains 914 videos across 10\ncategories. We establish the first baselines for these datasets, showing that\nrepresentations learned using a JEPA-style objective, combined with an LLM\ndecoder, can achieve user intent predictions that match the performance of\nstate-of-the-art large MLLMs, but with significantly reduced annotation and\ndeployment resources. Measured by intent similarity scores, UI-JEPA outperforms\nGPT-4 Turbo and Claude 3.5 Sonnet by 10.0% and 7.2% respectively, averaged\nacross two datasets. Notably, UI-JEPA accomplishes the performance with a 50.5x\nreduction in computational cost and a 6.6x improvement in latency in the IIW\ndataset. These results underscore the effectiveness of UI-JEPA, highlighting\nits potential for lightweight, high-performance UI understanding.",
        "updated": "2024-09-06 07:44:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04081v1"
    }
]