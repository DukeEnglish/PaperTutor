[
    {
        "title": "RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs",
        "authors": "Jiaxing WuLin NingLuyang LiuHarrison LeeNeo WuChao WangSushant PrakashShawn O'BanionBradley GreenJun Xie",
        "links": "http://arxiv.org/abs/2409.04421v1",
        "entry_id": "http://arxiv.org/abs/2409.04421v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04421v1",
        "summary": "LLM-powered personalization agent systems employ Large Language Models (LLMs)\nto predict users' behavior from their past activities. However, their\neffectiveness often hinges on the ability to effectively leverage extensive,\nlong user historical data due to its inherent noise and length of such data.\nExisting pretrained LLMs may generate summaries that are concise but lack the\nnecessary context for downstream tasks, hindering their utility in\npersonalization systems. To address these challenges, we introduce\nReinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to\ngenerate concise, human-readable user summaries that are optimized for\ndownstream task performance. By maximizing the usefulness of the generated\nsummaries, RLPF effectively distills extensive user history data while\npreserving essential information for downstream tasks. Our empirical evaluation\ndemonstrates significant improvements in both extrinsic downstream task utility\nand intrinsic summary quality, surpassing baseline methods by up to 22% on\ndownstream task performance and achieving an up to 84.59% win rate on\nFactuality, Abstractiveness, and Readability. RLPF also achieves a remarkable\n74% reduction in context length while improving performance on 16 out of 19\nunseen tasks and/or datasets, showcasing its generalizability. This approach\noffers a promising solution for enhancing LLM personalization by effectively\ntransforming long, noisy user histories into informative and human-readable\nrepresentations.",
        "updated": "2024-09-06 17:30:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04421v1"
    },
    {
        "title": "Empirical Bayesian image restoration by Langevin sampling with a denoising diffusion implicit prior",
        "authors": "Charlesquin Kemajou MbakamJean-Francois GiovannelliMarcelo Pereyra",
        "links": "http://arxiv.org/abs/2409.04384v1",
        "entry_id": "http://arxiv.org/abs/2409.04384v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04384v1",
        "summary": "Score-based diffusion methods provide a powerful strategy to solve image\nrestoration tasks by flexibly combining a pre-trained foundational prior model\nwith a likelihood function specified during test time. Such methods are\npredominantly derived from two stochastic processes: reversing\nOrnstein-Uhlenbeck, which underpins the celebrated denoising diffusion\nprobabilistic models (DDPM) and denoising diffusion implicit models (DDIM), and\nthe Langevin diffusion process. The solutions delivered by DDPM and DDIM are\noften remarkably realistic, but they are not always consistent with\nmeasurements because of likelihood intractability issues and the associated\nrequired approximations. Alternatively, using a Langevin process circumvents\nthe intractable likelihood issue, but usually leads to restoration results of\ninferior quality and longer computing times. This paper presents a novel and\nhighly computationally efficient image restoration method that carefully embeds\na foundational DDPM denoiser within an empirical Bayesian Langevin algorithm,\nwhich jointly calibrates key model hyper-parameters as it estimates the model's\nposterior mean. Extensive experimental results on three canonical tasks (image\ndeblurring, super-resolution, and inpainting) demonstrate that the proposed\napproach improves on state-of-the-art strategies both in image estimation\naccuracy and computing time.",
        "updated": "2024-09-06 16:20:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04384v1"
    },
    {
        "title": "AGR: Age Group fairness Reward for Bias Mitigation in LLMs",
        "authors": "Shuirong CaoRuoxi ChengZhiqiang Wang",
        "links": "http://arxiv.org/abs/2409.04340v1",
        "entry_id": "http://arxiv.org/abs/2409.04340v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04340v1",
        "summary": "LLMs can exhibit age biases, resulting in unequal treatment of individuals\nacross age groups. While much research has addressed racial and gender biases,\nage bias remains little explored. The scarcity of instruction-tuning and\npreference datasets for age bias hampers its detection and measurement, and\nexisting fine-tuning methods seldom address age-related fairness. In this\npaper, we construct age bias preference datasets and instruction-tuning\ndatasets for RLHF. We introduce ARG, an age fairness reward to reduce\ndifferences in the response quality of LLMs across different age groups.\nExtensive experiments demonstrate that this reward significantly improves\nresponse accuracy and reduces performance disparities across age groups. Our\nsource code and datasets are available at the anonymous\n\\href{https://anonymous.4open.science/r/FairRLHF-D445/readme.md}{link}.",
        "updated": "2024-09-06 15:18:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04340v1"
    },
    {
        "title": "Learning vs Retrieval: The Role of In-Context Examples in Regression with LLMs",
        "authors": "Aliakbar NafarKristen Brent VenableParisa Kordjamshidi",
        "links": "http://arxiv.org/abs/2409.04318v1",
        "entry_id": "http://arxiv.org/abs/2409.04318v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04318v1",
        "summary": "Generative Large Language Models (LLMs) are capable of being in-context\nlearners. However, the underlying mechanism of in-context learning (ICL) is\nstill a major research question, and experimental research results about how\nmodels exploit ICL are not always consistent. In this work, we propose a\nframework for evaluating in-context learning mechanisms, which we claim are a\ncombination of retrieving internal knowledge and learning from in-context\nexamples by focusing on regression tasks. First, we show that LLMs can perform\nregression on real-world datasets and then design experiments to measure the\nextent to which the LLM retrieves its internal knowledge versus learning from\nin-context examples. We argue that this process lies on a spectrum between\nthese two extremes. We provide an in-depth analysis of the degrees to which\nthese mechanisms are triggered depending on various factors, such as prior\nknowledge about the tasks and the type and richness of the information provided\nby the in-context examples. We employ three LLMs and utilize multiple datasets\nto corroborate the robustness of our findings. Our results shed light on how to\nengineer prompts to leverage meta-learning from in-context examples and foster\nknowledge retrieval depending on the problem being addressed.",
        "updated": "2024-09-06 14:46:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04318v1"
    },
    {
        "title": "Using Large Language Models to Generate Authentic Multi-agent Knowledge Work Datasets",
        "authors": "Desiree HeimChristian JilekAdrian UlgesAndreas Dengel",
        "links": "http://arxiv.org/abs/2409.04286v1",
        "entry_id": "http://arxiv.org/abs/2409.04286v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04286v1",
        "summary": "Current publicly available knowledge work data collections lack diversity,\nextensive annotations, and contextual information about the users and their\ndocuments. These issues hinder objective and comparable data-driven evaluations\nand optimizations of knowledge work assistance systems. Due to the considerable\nresources needed to collect such data in real-life settings and the necessity\nof data censorship, collecting such a dataset appears nearly impossible. For\nthis reason, we propose a configurable, multi-agent knowledge work dataset\ngenerator. This system simulates collaborative knowledge work among agents\nproducing Large Language Model-generated documents and accompanying data\ntraces. Additionally, the generator captures all background information, given\nin its configuration or created during the simulation process, in a knowledge\ngraph. Finally, the resulting dataset can be utilized and shared without\nprivacy or confidentiality concerns.\n  This paper introduces our approach's design and vision and focuses on\ngenerating authentic knowledge work documents using Large Language Models. Our\nstudy involving human raters who assessed 53% of the generated and 74% of the\nreal documents as realistic demonstrates the potential of our approach.\nFurthermore, we analyze the authenticity criteria mentioned in the\nparticipants' comments and elaborate on potential improvements for identified\ncommon issues.",
        "updated": "2024-09-06 13:53:28 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04286v1"
    }
]