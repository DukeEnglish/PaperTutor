[
    {
        "title": "Accelerating Training with Neuron Interaction and Nowcasting Networks",
        "authors": "Boris KnyazevAbhinav MoudgilGuillaume LajoieEugene BelilovskySimon Lacoste-Julien",
        "links": "http://arxiv.org/abs/2409.04434v1",
        "entry_id": "http://arxiv.org/abs/2409.04434v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04434v1",
        "summary": "Neural network training can be accelerated when a learnable update rule is\nused in lieu of classic adaptive optimizers (e.g. Adam). However, learnable\nupdate rules can be costly and unstable to train and use. A simpler recently\nproposed approach to accelerate training is to use Adam for most of the\noptimization steps and periodically, only every few steps, nowcast (predict\nfuture) parameters. We improve this approach by Neuron interaction and\nNowcasting (NiNo) networks. NiNo leverages neuron connectivity and graph neural\nnetworks to more accurately nowcast parameters by learning in a supervised way\nfrom a set of training trajectories over multiple tasks. We show that in some\nnetworks, such as Transformers, neuron connectivity is non-trivial. By\naccurately modeling neuron connectivity, we allow NiNo to accelerate Adam\ntraining by up to 50\\% in vision and language tasks.",
        "updated": "2024-09-06 17:55:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04434v1"
    },
    {
        "title": "Theory, Analysis, and Best Practices for Sigmoid Self-Attention",
        "authors": "Jason RamapuramFederico DanieliEeshan DhekaneFloris WeersDan BusbridgePierre AblinTatiana LikhomanenkoJagrit DiganiZijin GuAmitis ShidaniRuss Webb",
        "links": "http://arxiv.org/abs/2409.04431v1",
        "entry_id": "http://arxiv.org/abs/2409.04431v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04431v1",
        "summary": "Attention is a key part of the transformer architecture. It is a\nsequence-to-sequence mapping that transforms each sequence element into a\nweighted sum of values. The weights are typically obtained as the softmax of\ndot products between keys and queries. Recent work has explored alternatives to\nsoftmax attention in transformers, such as ReLU and sigmoid activations. In\nthis work, we revisit sigmoid attention and conduct an in-depth theoretical and\nempirical analysis. Theoretically, we prove that transformers with sigmoid\nattention are universal function approximators and benefit from improved\nregularity compared to softmax attention. Through detailed empirical analysis,\nwe identify stabilization of large initial attention norms during the early\nstages of training as a crucial factor for the successful training of models\nwith sigmoid attention, outperforming prior attempts. We also introduce\nFLASHSIGMOID, a hardware-aware and memory-efficient implementation of sigmoid\nattention yielding a 17% inference kernel speed-up over FLASHATTENTION2 on H100\nGPUs. Experiments across language, vision, and speech show that properly\nnormalized sigmoid attention matches the strong performance of softmax\nattention on a wide range of domains and scales, which previous attempts at\nsigmoid attention were unable to fully achieve. Our work unifies prior art and\nestablishes best practices for sigmoid attention as a drop-in softmax\nreplacement in transformers.",
        "updated": "2024-09-06 17:53:26 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04431v1"
    },
    {
        "title": "VILA-U: a Unified Foundation Model Integrating Visual Understanding and Generation",
        "authors": "Yecheng WuZhuoyang ZhangJunyu ChenHaotian TangDacheng LiYunhao FangLigeng ZhuEnze XieHongxu YinLi YiSong HanYao Lu",
        "links": "http://arxiv.org/abs/2409.04429v1",
        "entry_id": "http://arxiv.org/abs/2409.04429v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04429v1",
        "summary": "VILA-U is a Unified foundation model that integrates Video, Image, Language\nunderstanding and generation. Traditional visual language models (VLMs) use\nseparate modules for understanding and generating visual content, which can\nlead to misalignment and increased complexity. In contrast, VILA-U employs a\nsingle autoregressive next-token prediction framework for both tasks,\neliminating the need for additional components like diffusion models. This\napproach not only simplifies the model but also achieves near state-of-the-art\nperformance in visual language understanding and generation. The success of\nVILA-U is attributed to two main factors: the unified vision tower that aligns\ndiscrete visual tokens with textual inputs during pretraining, which enhances\nvisual perception, and autoregressive image generation can achieve similar\nquality as diffusion models with high-quality dataset. This allows VILA-U to\nperform comparably to more complex models using a fully token-based\nautoregressive framework.",
        "updated": "2024-09-06 17:49:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04429v1"
    },
    {
        "title": "Hybrid Spiking Neural Networks for Low-Power Intra-Cortical Brain-Machine Interfaces",
        "authors": "Alexandru VasilacheJann KrausseKlaus KnoblochJuergen Becker",
        "links": "http://arxiv.org/abs/2409.04428v1",
        "entry_id": "http://arxiv.org/abs/2409.04428v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04428v1",
        "summary": "Intra-cortical brain-machine interfaces (iBMIs) have the potential to\ndramatically improve the lives of people with paraplegia by restoring their\nability to perform daily activities. However, current iBMIs suffer from\nscalability and mobility limitations due to bulky hardware and wiring. Wireless\niBMIs offer a solution but are constrained by a limited data rate. To overcome\nthis challenge, we are investigating hybrid spiking neural networks for\nembedded neural decoding in wireless iBMIs. The networks consist of a temporal\nconvolution-based compression followed by recurrent processing and a final\ninterpolation back to the original sequence length. As recurrent units, we\nexplore gated recurrent units (GRUs), leaky integrate-and-fire (LIF) neurons,\nand a combination of both - spiking GRUs (sGRUs) and analyze their differences\nin terms of accuracy, footprint, and activation sparsity. To that end, we train\ndecoders on the \"Nonhuman Primate Reaching with Multichannel Sensorimotor\nCortex Electrophysiology\" dataset and evaluate it using the NeuroBench\nframework, targeting both tracks of the IEEE BioCAS Grand Challenge on Neural\nDecoding. Our approach achieves high accuracy in predicting velocities of\nprimate reaching movements from multichannel primary motor cortex recordings\nwhile maintaining a low number of synaptic operations, surpassing the current\nbaseline models in the NeuroBench framework. This work highlights the potential\nof hybrid neural networks to facilitate wireless iBMIs with high decoding\nprecision and a substantial increase in the number of monitored neurons, paving\nthe way toward more advanced neuroprosthetic technologies.",
        "updated": "2024-09-06 17:48:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04428v1"
    },
    {
        "title": "RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs",
        "authors": "Jiaxing WuLin NingLuyang LiuHarrison LeeNeo WuChao WangSushant PrakashShawn O'BanionBradley GreenJun Xie",
        "links": "http://arxiv.org/abs/2409.04421v1",
        "entry_id": "http://arxiv.org/abs/2409.04421v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04421v1",
        "summary": "LLM-powered personalization agent systems employ Large Language Models (LLMs)\nto predict users' behavior from their past activities. However, their\neffectiveness often hinges on the ability to effectively leverage extensive,\nlong user historical data due to its inherent noise and length of such data.\nExisting pretrained LLMs may generate summaries that are concise but lack the\nnecessary context for downstream tasks, hindering their utility in\npersonalization systems. To address these challenges, we introduce\nReinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to\ngenerate concise, human-readable user summaries that are optimized for\ndownstream task performance. By maximizing the usefulness of the generated\nsummaries, RLPF effectively distills extensive user history data while\npreserving essential information for downstream tasks. Our empirical evaluation\ndemonstrates significant improvements in both extrinsic downstream task utility\nand intrinsic summary quality, surpassing baseline methods by up to 22% on\ndownstream task performance and achieving an up to 84.59% win rate on\nFactuality, Abstractiveness, and Readability. RLPF also achieves a remarkable\n74% reduction in context length while improving performance on 16 out of 19\nunseen tasks and/or datasets, showcasing its generalizability. This approach\noffers a promising solution for enhancing LLM personalization by effectively\ntransforming long, noisy user histories into informative and human-readable\nrepresentations.",
        "updated": "2024-09-06 17:30:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04421v1"
    }
]