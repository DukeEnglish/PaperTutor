[
    {
        "title": "Accelerating Training with Neuron Interaction and Nowcasting Networks",
        "authors": "Boris KnyazevAbhinav MoudgilGuillaume LajoieEugene BelilovskySimon Lacoste-Julien",
        "links": "http://arxiv.org/abs/2409.04434v1",
        "entry_id": "http://arxiv.org/abs/2409.04434v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04434v1",
        "summary": "Neural network training can be accelerated when a learnable update rule is\nused in lieu of classic adaptive optimizers (e.g. Adam). However, learnable\nupdate rules can be costly and unstable to train and use. A simpler recently\nproposed approach to accelerate training is to use Adam for most of the\noptimization steps and periodically, only every few steps, nowcast (predict\nfuture) parameters. We improve this approach by Neuron interaction and\nNowcasting (NiNo) networks. NiNo leverages neuron connectivity and graph neural\nnetworks to more accurately nowcast parameters by learning in a supervised way\nfrom a set of training trajectories over multiple tasks. We show that in some\nnetworks, such as Transformers, neuron connectivity is non-trivial. By\naccurately modeling neuron connectivity, we allow NiNo to accelerate Adam\ntraining by up to 50\\% in vision and language tasks.",
        "updated": "2024-09-06 17:55:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04434v1"
    },
    {
        "title": "A Survey on Knowledge Organization Systems of Research Fields: Resources and Challenges",
        "authors": "Angelo SalatinoTanay AggarwalAndrea MannocciFrancesco OsborneEnrico Motta",
        "links": "http://arxiv.org/abs/2409.04432v1",
        "entry_id": "http://arxiv.org/abs/2409.04432v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04432v1",
        "summary": "Knowledge Organization Systems (KOSs), such as term lists, thesauri,\ntaxonomies, and ontologies, play a fundamental role in categorising, managing,\nand retrieving information. In the academic domain, KOSs are often adopted for\nrepresenting research areas and their relationships, primarily aiming to\nclassify research articles, academic courses, patents, books, scientific\nvenues, domain experts, grants, software, experiment materials, and several\nother relevant products and agents. These structured representations of\nresearch areas, widely embraced by many academic fields, have proven effective\nin empowering AI-based systems to i) enhance retrievability of relevant\ndocuments, ii) enable advanced analytic solutions to quantify the impact of\nacademic research, and iii) analyse and forecast research dynamics. This paper\naims to present a comprehensive survey of the current KOS for academic\ndisciplines. We analysed and compared 45 KOSs according to five main\ndimensions: scope, structure, curation, usage, and links to other KOSs. Our\nresults reveal a very heterogeneous scenario in terms of scope, scale, quality,\nand usage, highlighting the need for more integrated solutions for representing\nresearch knowledge across academic fields. We conclude by discussing the main\nchallenges and the most promising future directions.",
        "updated": "2024-09-06 17:54:43 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04432v1"
    },
    {
        "title": "Hybrid Spiking Neural Networks for Low-Power Intra-Cortical Brain-Machine Interfaces",
        "authors": "Alexandru VasilacheJann KrausseKlaus KnoblochJuergen Becker",
        "links": "http://arxiv.org/abs/2409.04428v1",
        "entry_id": "http://arxiv.org/abs/2409.04428v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04428v1",
        "summary": "Intra-cortical brain-machine interfaces (iBMIs) have the potential to\ndramatically improve the lives of people with paraplegia by restoring their\nability to perform daily activities. However, current iBMIs suffer from\nscalability and mobility limitations due to bulky hardware and wiring. Wireless\niBMIs offer a solution but are constrained by a limited data rate. To overcome\nthis challenge, we are investigating hybrid spiking neural networks for\nembedded neural decoding in wireless iBMIs. The networks consist of a temporal\nconvolution-based compression followed by recurrent processing and a final\ninterpolation back to the original sequence length. As recurrent units, we\nexplore gated recurrent units (GRUs), leaky integrate-and-fire (LIF) neurons,\nand a combination of both - spiking GRUs (sGRUs) and analyze their differences\nin terms of accuracy, footprint, and activation sparsity. To that end, we train\ndecoders on the \"Nonhuman Primate Reaching with Multichannel Sensorimotor\nCortex Electrophysiology\" dataset and evaluate it using the NeuroBench\nframework, targeting both tracks of the IEEE BioCAS Grand Challenge on Neural\nDecoding. Our approach achieves high accuracy in predicting velocities of\nprimate reaching movements from multichannel primary motor cortex recordings\nwhile maintaining a low number of synaptic operations, surpassing the current\nbaseline models in the NeuroBench framework. This work highlights the potential\nof hybrid neural networks to facilitate wireless iBMIs with high decoding\nprecision and a substantial increase in the number of monitored neurons, paving\nthe way toward more advanced neuroprosthetic technologies.",
        "updated": "2024-09-06 17:48:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04428v1"
    },
    {
        "title": "RLPF: Reinforcement Learning from Prediction Feedback for User Summarization with LLMs",
        "authors": "Jiaxing WuLin NingLuyang LiuHarrison LeeNeo WuChao WangSushant PrakashShawn O'BanionBradley GreenJun Xie",
        "links": "http://arxiv.org/abs/2409.04421v1",
        "entry_id": "http://arxiv.org/abs/2409.04421v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04421v1",
        "summary": "LLM-powered personalization agent systems employ Large Language Models (LLMs)\nto predict users' behavior from their past activities. However, their\neffectiveness often hinges on the ability to effectively leverage extensive,\nlong user historical data due to its inherent noise and length of such data.\nExisting pretrained LLMs may generate summaries that are concise but lack the\nnecessary context for downstream tasks, hindering their utility in\npersonalization systems. To address these challenges, we introduce\nReinforcement Learning from Prediction Feedback (RLPF). RLPF fine-tunes LLMs to\ngenerate concise, human-readable user summaries that are optimized for\ndownstream task performance. By maximizing the usefulness of the generated\nsummaries, RLPF effectively distills extensive user history data while\npreserving essential information for downstream tasks. Our empirical evaluation\ndemonstrates significant improvements in both extrinsic downstream task utility\nand intrinsic summary quality, surpassing baseline methods by up to 22% on\ndownstream task performance and achieving an up to 84.59% win rate on\nFactuality, Abstractiveness, and Readability. RLPF also achieves a remarkable\n74% reduction in context length while improving performance on 16 out of 19\nunseen tasks and/or datasets, showcasing its generalizability. This approach\noffers a promising solution for enhancing LLM personalization by effectively\ntransforming long, noisy user histories into informative and human-readable\nrepresentations.",
        "updated": "2024-09-06 17:30:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04421v1"
    },
    {
        "title": "Improved Parallel Algorithm for Non-Monotone Submodular Maximization under Knapsack Constraint",
        "authors": "Tan D. TranCanh V. PhamDung T. K. HaPhuong N. H. Pham",
        "links": "http://arxiv.org/abs/2409.04415v1",
        "entry_id": "http://arxiv.org/abs/2409.04415v1",
        "pdf_url": "http://arxiv.org/pdf/2409.04415v1",
        "summary": "This work proposes an efficient parallel algorithm for non-monotone\nsubmodular maximization under a knapsack constraint problem over the ground set\nof size $n$. Our algorithm improves the best approximation factor of the\nexisting parallel one from $8+\\epsilon$ to $7+\\epsilon$ with $O(\\log n)$\nadaptive complexity.\n  The key idea of our approach is to create a new alternate threshold\nalgorithmic framework. This strategy alternately constructs two disjoint\ncandidate solutions within a constant number of sequence rounds. Then, the\nalgorithm boosts solution quality without sacrificing the adaptive complexity.\nExtensive experimental studies on three applications, Revenue Maximization,\nImage Summarization, and Maximum Weighted Cut, show that our algorithm not only\nsignificantly increases solution quality but also requires comparative\nadaptivity to state-of-the-art algorithms.",
        "updated": "2024-09-06 17:17:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.04415v1"
    }
]