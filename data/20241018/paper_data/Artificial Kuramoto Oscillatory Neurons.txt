ARTIFICIAL KURAMOTO OSCILLATORY NEURONS
TakeruMiyato1,SindyLo¨we2,AndreasGeiger1,MaxWelling2
1UniversityofTu¨bingen,Tu¨bingenAICenter 2UniversityofAmsterdam
ABSTRACT
IthaslongbeenknowninbothneuroscienceandAIthat“binding”betweenneu-
ronsleadstoaformofcompetitivelearningwhererepresentationsarecompressed
inordertorepresentmoreabstractconceptsindeeperlayersofthenetwork. More
recently, it was also hypothesized that dynamic (spatiotemporal) representations
playanimportantroleinbothneuroscienceandAI.Buildingontheseideas, we
introduceArtificialKuramotoOscillatoryNeurons(AKOrN)asadynamicalalter-
nativetothresholdunits, whichcanbecombinedwitharbitraryconnectivityde-
signssuchasfullyconnected,convolutional,orattentivemechanisms. Ourgener-
alizedKuramotoupdatesbindneuronstogetherthroughtheirsynchronizationdy-
namics.Weshowthatthisideaprovidesperformanceimprovementsacrossawide
spectrum of tasks such as unsupervised object discovery, adversarial robustness,
calibrateduncertaintyquantification,andreasoning. Webelievethattheseempir-
icalresultsshowtheimportanceofrethinkingourassumptionsatthemostbasic
neuronallevelofneuralrepresentation,andinparticularshowtheimportanceof
dynamicalrepresentations.
1 INTRODUCTION
Before the advent of modern deep learning architectures, artificial neural networks were inspired
bybiologicalneurons. IncontrasttotheMcCulloch-Pittsneuron(McCulloch&Pitts,1943)which
wasdesignedasanabstractionofanintegrate-and-fireneuron(Sherrington,1906),recentbuilding
blocksofneuralnetworksaredesignedtoworkwellonmodernhardware(Hooker,2021). Asour
understandingofthebrainisimprovingoverrecentyears,andneuroscientistsarediscoveringmore
about its information processing principles, we can ask ourselves again if there are lessons from
neurosciencethatcanbeusedasdesignprinciplesforartificialneuralnets.
Inthispaper,wefollowamoremoderndynamicalviewofneuronsasoscillatoryunitsthatarecou-
pledtootherneurons(Mulleretal.,2018).SimilartohowthebinarystateofaMcCulloch-Pittsneu-
ronabstractsthefiringofarealneuron,wewillabstractanoscillatingneuronbyanN-dimensional
unitvectorthatrotatesonthesphere(Lo¨weetal.,2024a). Webuildanewneuralnetworkarchitec-
ture that has iterative modules that update N-dimensional oscillatory neurons via a generalization
ofthewell-knownnon-lineardynamicalmodelcalledtheKuramotomodel(Kuramoto,1984).
The Kuramoto model describes the synchronization of oscillators; each Kuramoto update applies
forcestoconnectedoscillators,encouragingthemtobecomealignedoranti-aligned. Thisprocessis
similartobindinginneuroscienceandcanbeunderstoodasdistributedandcontinuousclustering.
Thus,networkswiththismechanismtendtocompresstheirrepresentationsviasynchronization.
We incorporate the Kuramoto model into an artificial neural network, by applying the differential
equationthatdescribestheKuramotomodeltoeachindividualneuron. TheresultingartificialKu-
ramoto oscillatory neurons (AKOrN) can be combined with layer architectures such as fully con-
nectedlayers,convolutions,andattentionmechanisms.
We explore the capabilities of AKOrN and find that its neuronal mechanism drastically changes
the behavior of the network. AKOrN strongly binds object features with competitive performance
to slot-based models in object discovery, enhances the reasoning capability of self-attention, and
increases robustness against random, adversarial, and natural perturbations with surprisingly good
calibration.
Code:https://github.com/autonomousvision/akorn.
1
4202
tcO
71
]GL.sc[
1v12831.0142:viXrac
i
x
i
-4000 J ij
-4500
-5000
-5500
0 50 100 150 200 250
t
Figure1: OurproposedartificialKuramotooscillatoryneurons(AKOrN).Theseriesofpictureson
theleftare64×64KuramotooscillatorsevolvingbytheKuramotoupdates(Eq.(2)),alongwitha
plotoftheenergiescomputedbyEq.(3). Eachsingleoscillatorx isanN-dimensionalvectoron
i
the sphere and is influenced by (1) connected oscillators through the weights J , (2) conditional
ij
stimulic ,and(3)Ω thatdeterminesthenaturalfrequencyofeachoscillator. SeeFig10fordetails
i i
onCandJ.
2 MOTIVATION
It was recognized early on that neurons interact via lateral connections (Hubel & Wiesel, 1962;
Somersetal.,1995). Infact, neighboringneuronstendtoclustertheiractivities, andclusterstend
to compete to explain the input. This “competitive learning” has the advantage that information
is compressed as we move through the layers, facilitating the process of abstraction by creating
aninformationbottleneck. Additionally,thecompetitionencouragesdifferenthigher-levelneurons
to focus on different aspects of the input (i.e. they specialize). This process is made possible by
synchronization: like fireflies in the night, neurons tend to synchronize their activities with their
neighbors’, which leads to the compression of their representations. This idea has been used in
artificialneuralnetworksbeforetomodel“binding”betweenneurons,whereneuronsrepresenting
features such as square, blue, and toy are bound by synchronization to represent a square blue
toy (Reichert & Serre, 2013; Lo¨we et al., 2022). In this paper, we will use an N-dimensional
generalizationofthefamousKuramotomodel(Kuramoto,1984)tomodelthissynchronization.
Our model has the advantage that it naturally incorporates spatiotemporal representations in the
formoftravelingwaves(Kelleretal.,2024), forwhichthereisampleevidenceintheneuroscien-
tificliterature. Whiletheirroleinthebrainremainspoorlyunderstood, ithasbeenpostulatedthat
theyareinvolvedinshort-termmemory,long-rangecoordinationbetweenbrainregions,andother
cognitivefunctions(Rubinoetal.,2006;Lubenov&Siapas,2009;Fell&Axmacher,2011;Zhang
etal.,2018;Robertsetal.,2019;Mulleretal.,2016;Davisetal.,2020;Benignoetal.,2023). For
example, Muller et al. (2016) finds that oscillatory patterns in the thalamocortical network during
sleepareorganizedintocircularwave-likepatterns,whichcouldgiveanaccountofhowmemories
areconsolidatedinthebrain. Davisetal.(2020)suggestthatspontaneoustravelingwavesinthevi-
sualcortexmodulatesynapticactivitiesandthusactasagatingmechanisminthebrain. Inthegen-
eralizedKuramotomodel,travelingwavesnaturallyemergeasneighboringoscillatorsstarttosyn-
chronize(seeontheleftinFig1,andFig10intheAppendix).
Another advantage of using dynamical neurons is that they can perform a form of reasoning. Ku-
ramotooscillatorshavebeensuccessfullyusedtosolvecombinatorialoptimizationtaskssuchask-
SATproblems(Heisenberg,1985;Wang&Roychowdhury,2017). Thiscanbeunderstoodbythe
fact that Kuramoto models can be viewed as continuous versions of discrete Ising models, where
phasevariablesreplacethediscretespinstates. Manyauthorshavearguedthatthemodernarchitec-
turesbasedon,e.g.,transformerslackthisintrinsiccapabilityof“neuro-symbolicreasoning”(Dziri
etal.,2024;Bounsietal.,2024). WeshowthatAKOrN cansuccessfullysolveSudokupuzzles,il-
lustratingthiscapability.Additionally,AKOrNrelatestomodelsinquantumphysicsandactivemat-
ter(seeappendixA).
In summary, AKOrN combines beneficial features such as competitive learning (i.e., feature bind-
ing),reasoning,robustnessanduncertaintyquantification,aswellasthepotentialadvantagesoftrav-
elingwavesobservedinthebrain,whilebeingfirmlygroundedinwell-understoodphysicsmodels.
2
E3 THE KURAMOTO MODEL
TheKuramotomodel(Kuramoto,1984)isanon-lineardynamicalmodelofoscillators,thatexhibits
synchronizationphenomena. Evenwithitssimpleformulation, themodelcanrepresentnumerous
dynamicalpatternsdependingontheconnectionsbetweenoscillators(Breakspearetal.,2010;Heit-
mannetal.,2012).
IntheoriginalKuramotomodel,eachoscillatoriisrepresentedbyitsphaseinformationθ ∈[0,2π).
i
ThedifferentialequationoftheKuramotomodelis
θ˙ =ω +(cid:80) J sin(θ −θ ), (1)
i i j ij j i
whereω ∈ RisthenaturalfrequencyandJ ∈ Rrepresentstheconnectionsbetweenoscillators:
i ij
ifJ >0theiandj-thoscillatortendtoalign,andifJ <0,theytendtoopposeeachother.
ij ij
While the original Kuramoto model describes one-dimensional oscillators, we introduce a multi-
dimensional vector version of the model (Cumin & Unsworth, 2007; Chandra et al., 2019; Lipton
etal.,2021)withanovelsymmetry-breakingtermintoneuralnetworks. Wedenoteoscillatorsby
X={x }C ,whereeachx isanN-dimensionalunitvectorx ∈RN, ∥x ∥ =1. Whileeachx
i i=1 i i i 2 i
istime-dependent,weomittforclarity. Theoscillatorindeximayhavemultipledimensions: ifthe
inputisanimage,forexample,eachoscillatorisrepresentedbyx withchwindicatingchannel,
chw
heightandwidthpositions,respectively.
Thedifferentialequationofourvector-valuedKuramotomodeliswrittenasfollows:
(cid:88)
x˙ =Ω x +Proj (c + J x )whereProj (y )=y −⟨y ,x ⟩x (2)
i i i xi i ij j xi i i i i i
j
Here, Ω is an N ×N anti-symmetric matrix and Ω x is called the natural frequency term that
i i i
determineseachoscillator’sownrotationfrequencyandangle.Thesecondtermgovernsinteractions
betweenoscillators,whereProj isanoperatorthatprojectsaninputvectorontothetangentspace
xi
ofthesphereatx . c ∈ RN,C = {c }C isadata-dependentvariable,whichiscomputedfrom
i i i i=1
the observational input or the activations of the previous layer. In this paper, every c is set to be
i
constantacrosstime,butitcanbeatime-dependentvariable.c canbeseenasanotheroscillatorthat
i
hasaunidirectionalconnectiontox . Sincec isnotaffectedbyanyoscillators,c stronglybinds
i i i
x tothesamedirectionasc ,i.e. itactsasabiasdirection(seeFig10intheAppendix). Usually,
i i
theKuramotomodelisstudiedwithoutsuchconditionalstimuli,butwefoundthattheuseofCis
necessaryforstabletraining. Inphysicslingo,Cisoftenreferredtoasa“symmetrybreaking”field.
TheKuramotomodelisLyapunovifweassumecertainsymmetricpropertiesinJ andΩ (Aoyagi,
ij i
1995;Wang&Roychowdhury,2017). Forexample,ifJissymmetricanddifferentoscillatorsshare
the same natural frequencies: J = JT, Ω = Ω, and Ωc = 0, each update is guaranteed to
ij ji i i
minimizethefollowingenergy:
(cid:88) (cid:88)
E =− xTJ x − cTx (3)
i ij j i i
i,j i
Fig 1 on the left shows how the Kuramoto oscillators and the corresponding energy evolve with a
simple Gaussian kernel as the connectivity matrix. Here, we set C a silhouette of a fish, where
c = 1 on the outer silhouette and c = 0 on the inner silhouette. The oscillator state is initially
i i
disordered, but gradually exhibits collective behavior, eventually becoming a spatially propagat-
ing wavy pattern. We include animations of visualized oscillators, including oscillators of trained
AKOrN modelsusedinourexperiments,intheSupplementaryMaterial.
Wewouldliketonotethatwefoundthatevenwithoutsymmetricconstraints,theenergyvaluede-
creasesrelativelystably,andthemodelsperformbetteracrossalltaskswetestedcomparedtomod-
elswithsymmetricJ. AsimilarobservationismadebyEffenbergeretal.(2022)whereheteroge-
neousoscillatorssuchasthosewithdifferentnaturalfrequenciesarehelpfulforthenetworktocon-
trolthelevelofsynchronizationandincreasethenetworkcapacity. Fromhere,weassumenosym-
metric constraints on J and Ω. Having asymmetric (a.k.a. non-reciprocal) connections is aligned
withthebiologicalneuronsinthebrain,whichalsodonothavesymmetricsynapses.
3X(0) X(L)
xT
...
...
C(0) C(L)
Figure2: OurproposedKuramoto-basednetwork(here,forimageprocessing). Eachblockconsists
of a Kuramoto-layer and a readout module described in Sec 4. C(L) is used to make the final
predictionofourmodel.
4 NETWORKS WITH KURAMOTO OSCILLATORS
WeutilizetheartificialKuramotooscillatorneurons(AKOrN)asabasicunitofinformationprocess-
inginneuralnetworks(Fig2). First,wetransformanobservationwitharelativelysimplefunction
tocreatetheinitialC. Next,XisinitializedbyeitherC,afixedlearnedembedding,randomvec-
tors,oramixtureoftheseinitializationschemes. Theblockiscomposedoftwomodules: theKu-
ramotolayerandthereadoutmodule,whichtogetherprocessthepair{X,C}. TheKuramotolayer
updatesXwiththeconditionalstimuliC,andthereadoutlayerextractsfeaturesfromthefinalos-
cillatorystatestocreatenewconditionalstimuli. Wedenotel-thlayer’soutputofthel-thblockby
{X(l),C(l)}.
Kuramoto layer Starting with X(l,0) := X(l) as initial oscillators, where the second superscript
denotesthetimestep,weupdatethembythediscreteversionofthedifferentialequation(2):
∆x(l,t) =Ω x(l,t)+Proj (c(l)+(cid:88) J x(l,t)) (4)
i i i x(l,t) i ij j
i
j
(cid:104) (cid:105)
x(l,t+1) =Π x(l,t)+γ∆x(l,t) , (5)
i i i
where Π is the normalizing operator x/∥x∥ that ensures that the oscillators stay on the sphere.
2
γ > 0 is a scalar controlling the step size of the update, which is learned in our experiments. We
callthisupdateaKuramotoupdateoraKuramotostepfromhere. WeoptimizebothΩandJgiven
thetaskobjective.
WeupdatetheoscillatorsT times. WedenotetheoscillatorsatT byX(l,T). Thisoscillatorstateis
usedastheinitialstateofthenextblock: X(l,T) :=X(l+1,0).
Readoutmodule Wereadoutpatternsencodedintheoscillatorstocreatenewconditionalstimuli
C(l+1) forthesubsequentblock. Sincetheoscillatorsareconstrainedontothe(unit)hyper-sphere,
alltheinformationisencodedintheirdirections. Inparticular,therelativedirectionbetweenoscil-
latorsisanimportantsourceofinformationbecausepatternsaftercertainKuramotostepsonlydif-
feringlobalphaseshifts(seethelasttwopatternsinFig10intheAppendix). Tocapturephasein-
variantpatterns,wetakethenormofthelinearlyprocessedoscillators:
C(l+1) =g(m)∈RC′×N,m =∥z ∥ , z =(cid:80) U x(l,T) ∈RN′, (6)
k k 2 k i ki i
whereU ∈RN′×N isalearnedweightmatrix,gisalearnedfunction,andm=[m ,...,m ]T ∈
ki 1 K
RK. N′ is typically set to the same value as N. In this work, g is just the identity function, a
linearlayer,oratmostathree-layerneuralnetworkwithresidualconnections. Becausethemodule
computesthenormof(weighted)X(l,T), thisreadoutmoduleincludesfunctionsthatareinvariant
to the global phase shift in the solution space. Unless otherwise specified, we set C′ = C and
K =C×N inallourexperiments.
4
Preprocess
K-Layer
Readout4.1 CONNECTIVITIES
We implement artificial Kuramoto oscillator neurons (AKOrN) within convolutional and self-
attentionlayers.Wewritedowntheformalequationsoftheconnectivityforcompleteness,however,
theysimplyfollowtheconventionaloperationofconvolutionorself-attentionappliedtooscillatory
neuronsflattenedw.r.ttherotatingdimensionN. Inshort, convolutionalconnectivityislocal, and
attentiveconnectivityisdynamicinput-dependentconnectivity.
Convolutionalconnectivity ToimplementAKOrN inaconvolutionallayer,oscillatorsandcondi-
tionalstimuliarerepresentedas{x ,c }wherec,h,warechannel,heightandwidthpositions,
chw chw
andtheupdatedirectionisgivenby:
(cid:88) (cid:88)
y :=c + J x , (7)
chw chw cdh′w′ d(h+h′)(w+w′)
d h′,w′∈R[H′,W′]
whereR[H′,W′] = [1,...,H′]×[1,...,W′]istheH′×W′ rectangleregion(i.e. kernelsize)and
J ∈RN×N arethelearnedweightsintheconvolutionkernelwhere(c,d),(h′,w′)areoutput
cdh′w′
andinputchannels,andheightandwidthpositions.
Attentiveconnectivity SimilartoBahdanauetal.(2014);Vaswanietal.(2017),weconstructthe
internalconnectivityintheQKV-attentionmanner. Inthiscase,oscillatorsandconditionalstimuli
are represented by {x ,c } where l and i are indices of tokens and channels, respectively. The
li li
updatedirectionbecomes:
(cid:88) (cid:88)(cid:88)
y :=c + J x =c + WO A (l,m)WV x (8)
li li lmij mj li h,ik h h,kj mj
m,j m,j k,h
(cid:42) (cid:43)
A (l,m)=
edh(l,m)
, d
(l,m)=(cid:88) (cid:88)
WQ x
,(cid:88)
WK x (9)
h (cid:80) edh(l,m) h h,ai li h,ai mi
m a i i
whereWO ,WV ,WQ ,WK ∈RN×N arelearnedweightsofheadh.Sincetheconnectivity
h,ik h,kj h,ai h,ai
isdependentontheoscillatorvaluesandthusnotstaticduringtheupdates,itisunclearwhetherthe
energydefinedinEq,(3)isproper. Nonetheless,inourexperiments,theenergyandoscillatorstates
are stable after several updates (see the Supplementary Material, which includes visualizations of
theoscillatorsoftrainedAKOrN modelsandtheircorrespondingenergiesovertimesteps).
5 RELATED WORKS
TheKuramotomodelisrarelyseeninmachinelearning,especiallyindeeplearning. However,sev-
eralworksmotivateustousetheKuramotomodelasamechanismforlearningbindingfeatures.For
example,althoughtestedonlyinfairlysyntheticsettings,Libonietal.(2023)showthatclusterfea-
turesemergeintheoscillatorsoftheKuramotomodelwithlateralconnectionswithoutoptimization.
Also,alineofworksonneuralsynchrony(Reichert&Serre,2013;Lo¨weetal.,2022;Stanic´ etal.,
2023; Lo¨we et al., 2024a; Gopalakrishnan et al., 2024) shares the same philosophy with AKOrN.
Lo¨we et al. (2024a) extend the complex-valued neurons used by Reichert & Serre (2013); Lo¨we
etal.(2022)tomultidimensionalneuronsandshowsthat, togetherwithaspecificactivationfunc-
tion called χ-binding that implements the ‘winner-take-all’ mechanism at the single neuron level
(Lo¨weetal.,2024b),themultidimensionalneuronslearntoencodebindinginformationintheirori-
entations. Themechanismitselfisintriguinginitsownrightbutstrugglestoscaletonaturalimages
withoutpre-trainedmodels. Additionally,itsintegrationbeyondlinearandconvolutionlayers,such
asintoattentionmechanisms,remainsunclear.
Slot-based models (Le Roux et al., 2011; Burgess et al., 2019; Greff et al., 2019; Locatello et al.,
2020)arethemost-usedmodelforobject-centric(OC)learning. Theirdiscretenatureofrepresenta-
tionsisshowntobeagoodinductivebiastolearnsuchOCrepresentations,butthesemodelsstrug-
gleonnaturalimages,andarethereforeoftencombinedwithpowerful,pre-trainedself-supervised
learning(SSL)modelssuchasDINO(Caronetal.,2021). OurproposedcontinuousKuramotoneu-
rons can be a building block of the SSL network itself, and we show that they learn better object-
centricfeaturesthanwell-knownSSLmodels. AKOrNsperformparticularlywellonobjectdiscov-
erytaskswhenimplementedinself-attentionlayers. Self-attentionupdateswithnormalizationhave
5Figure3: Objectdiscoveryperformanceonsyntheticdatasets. Bindicatesthenumberofblocks.
Input ItrSA AKOrN GTmask Input ItrSA AKOrN GTmask
Figure4: AKOrN learnsmoreobject-boundfeaturesthanthenon-Kuramotomodelcounterpart.
CLEVRTex OOD CAMO
Model FG-ARI MBO FG-ARI MBO FG-ARI MBO
∗MONet(Burgessetal.,2019) 19.78 - 37.29 - 31.52 -
SLATE(Singhetal.,2021) 44.19 50.88 - - - -
∗Slot-Attention(Locatelloetal.,2020) 62.40 - 58.45 - 57.54 -
Slot-diffusion(Wuetal.,2023) 69.66 61.94 - - - -
Slot-diffusion+BO(Wuetal.,2023) 78.50 68.68 - - - -
∗DTI(Monnieretal.,2021) 79.90 - 73.67 - 72.90 -
∗I-SA(Changetal.,2022) 78.96 - 83.71 - 57.20 -
BO-SA(Jiaetal.,2023) 80.47 - 86.50 - 63.71 -
ISA-TS(Bizaetal.,2023) 92.9 - 84.4 - 86.2 -
AKOrNattn 89.24 60.02 88.00 60.96 77.18 53.43
Table1: ObjectdiscoveryperformanceonCLEVRTexanditsvariants(OOD,CAMO).AKOrN is
comparedamongmodelstrainedfromscratch. ∗NumberstakenfromJiaetal.(2023).
beenshownmathematicallytoclustertokenfeatures(Geshkovskietal.,2024). Ourworkcombines
this clustering behavior of transformers with the clustering induced by the synchronization of the
Kuramotoneurons,resultinginAKOrNbeingthefirstcompetitivemethodtoslot-basedapproaches.
Finally, thereexistseveralworksoninterpretingself-attentionasenergy-basedmodels(Ramsauer
etal.,2020;Hooveretal.,2024). OurKuramotomodel-basedmodelsdifferfromtheseapproaches
in various aspects such as the implementation of unit-norm-constrained neurons with asymmetric
connections,andtheirsymmetrybreakingterm.
6 EXPERIMENTS
6.1 UNSUPERVISEDOBJECTDISCOVERY
Unsupervisedobjectdiscoveryisthetaskoffindingobjectsinanimagewithoutsupervision. Here,
we test AKOrN on four synthetic datasets (Tetrominoes, dSprites, CLEVR (Kabra et al., 2019),
CLEVRTex (Karazija et al., 2021)) and two real image datasets (PascalVOC (Everingham et al.,
2010), COCO2017 (Lin et al., 2014)) (see Appendix C for details). Among the four synthetic
datasets,CLEVRTexhasthemostcomplexobjectsandbackgrounds.Wefurtherevaluatethemodels
trained on the CLEVRTex dataset on two variants (OOD, CAMO). The materials and shapes of
objects in OOD differ from those in CLEVRTex, while CAMO (short for camouflage) features
sceneswhereobjectsandbackgroundssharesimilartextureswithineachscene.
6Input DINO AKOrN GTMask Input DINO AKOrN GTmask
Figure5: Visualizationofclusterson(Left)PascalVOCand(Right)COCO2017.
Asbaselines,wetrainmodelsthataresimilartoResNet(Heetal.,2016)andViT(Dosovitskiyetal.,
2021), but iterate the convolution or self-attention layers multiple times with shared parameters.
Thisallowsustoevaluatetheimpactofourproposed,Kuramoto-basediterativeupdates. Wedenote
thesebaselinesasIterativeConvolution(ItrConv)andIterativeSelf-Attention(ItrSA),respectively.
Fig11intheAppendixshowsdiagramsofeachnetwork.
In AKOrN, C is initialized by the patched features of the images, while each x is initialized by
i
randomoscillatorssampledfromtheuniformdistributiononthesphere. WetraintheAKOrNmodel
withtheself-supervisedSimCLR(Chenetal.,2020)objective.
We train each model from scratch on the four synthetic datasets. For the two real image datasets,
we first train AKOrN on ImageNet (Krizhevsky et al., 2012) and directly evaluate that ImageNet-
pretrainedmodelonbothdatasetswithoutfine-tuning. Whenevaluating,weapplyclusteringtothe
finalblock’soutputfeatures(InAKOrN,itisC(L)). Weuseagglomerationclusteringwithaverage
linkage, which we found to outperform K-means for both the baseline models and AKOrN. We
evaluatetheclusteringresultsbyforegroundadjustedrandindex(FG-ARI)andMean-Best-Overlap
(MBO).FG-ARImeasuresthesimilaritybetweenthegroundtruthmasksandthecomputedclusters,
onlyforforegroundobjects. MBOfirstassignseachclustertothehighestoverlappinggroundtruth
maskandthencomputestheaverageintersection-over-union(IoU)ofallpairs. ForPascalVOCand
COCO2017,weshowinstance-levelMBO(MBO )andclass-level(MBO )segmentationresults.
i c
AKOrNbindsobjectfeatures Fig3showsthatAKOrNsimprovetheobjectdiscoveryperformance
over their non-Kuramoto counterparts in almost every dataset (except for Tetrominoes). Interest-
ingly,weobservethatconvolutionislesseffectivethanattentioninmostdatasets. InFig4,wesee
thattheKuramotomodels’clustersarewell-alignedwiththeindividualobjects,whileclustersofthe
ItrSAmodeloftenspanacrossobjectsandbackground,andaresensitivetothetextureoftheback-
ground(moreclusteringresultsareshowninFig20-22intheAppendix).
Tab1showsacomparisontoexistingworksonCLEVRTexanditsvariants. Allothermethodsare
slot-based. Amongthedistributedrepresentationmodels, AKOrNisthefirstmethodthatisshown
tobecompetitivewithslot-basedmodelsonthecomplexCLEVRTexdataset.
7PascalVOC COCO2017
Model MBO MBO MBO MBO
i c i c
(slot-basedmodels)
Slot-attention(Locatelloetal.,2020) 22.2 23.7 24.6 24.9
SLATE(Singhetal.,2021) 35.9 41.5 29.1 33.6
(DINO+slot-basedmodel)
DINOSAUR(Seitzeretal.,2023) 44.0 51.2 31.6 39.7
Slot-diffusion(Wuetal.,2023) 50.4 55.3 31.0 35.0
SPOT(Kakogeorgiouetal.,2024) 48.3 55.6 35.0 44.7
(transformer+SSL)
MAE(Heetal.,2022) 34.0 38.3 23.1 28.5
MoCoV3(Chenetal.,2021) 47.3 53.0 28.7 36.0
DINO(Caronetal.,2021) 47.2 53.5 29.4 37.0
AKOrN 52.0 60.3 31.3 40.3
Table2: ObjectdiscoveryonPascalVOCandCOCO2017.
AKOrN scales to natural images Fig 5 shows AKOrN binds object features on natural images
much better than DINO (Caron et al., 2021). We show a benchmark comparison on Pascal VOC
andCOCO2017inTab2. TheproposedAKOrNmodeloutperformsexistingSSLmodelsincluding
DINO,MoCoV3,andMAEonbothdatasets,showingthatitlearnsmoreobject-boundfeaturesthan
conventionaltransformer-basedmodels. OnPascal, AKOrN isfarbetterthanothermodelstrained
fromscratchandisbetterthanmethodstrainedonfeaturesofapretrainedDINOmodel. OnCOCO,
AKOrN againoutperformsmethodsthataretrainedfromscratchandiscompetitivetoDINOSAUR
andSlot-diffusion,butisoutperformedbytherecentSPOTmodel.
6.2 SOLVINGSUDOKU
TotestAKOrN’sreasoningcapability,weapplyitontheSudokupuzzledatasets(Wangetal.,2019;
Palmetal.,2018). Thetrainingsetcontainsboardswith31-42givendigits. Wetestmodelsinin-
distribution (ID) and out-of-distribution (OOD) scenarios. The ID test set contains 1,000 boards
sampledfromthesamedistribution,whileboardsintheOODsetcontainmuchfewergivendigits
(17-34)thanthetrainset.
To initialize C, we use embeddings of the digits 0-9 (0 for blank, 1-9 for given digits). x takes
i
the value c when a digit is given, and is randomly sampled from the uniform distribution on the
i
sphereforblanksquares. ThenumberofKuramotostepsduringtrainingissetto16. Wealsotrain
atransformermodelwith8blocks.
AKOrN solvesSudokupuzzles AKOrN perfectlysolvesallpuzzlesintheIDtestset, whileonly
RecurrentTransformer(R-Transformer (Yangetal.,2023))achievesthis(Tab3). OntheOODset,
AKOrN achieves61.1±14.7accuracywhichisonparwithIRED(Duetal.,2024),anenergy-based
diffusionmodel,andvastlybetterthanallotherexistingapproaches(includingtheR-Transformer).
AKOrN againstronglyoutperformsitsnon-Kuramotocounterparts,ItrSAandTransformer.
Test-timeextensionoftheKuramotosteps Justaswehumansusemoretimetosolveharderprob-
lems,AKOrN’sperformanceimprovesasweincreasethenumberofKuramotosteps. Asshownin
Fig6(a,b),ontheIDtestset,theenergyfluctuatesbutroughlyconvergestoaminimumafteraround
32steps. OntheOODtestset,however,theenergycontinuestodecreasefurther. Fig6(c)shows
thatincreasingthenumberofKuramotostepsattesttimeimprovesaccuracysignificantly(17%to
52%),whileincreasingthestepcountofstandardself-attentionprovidesalimitedimprovementon
theOODset(14%to34%)andleadstolowerperformanceontheIDset(99.3%to95.7%).
The energy value tells the correctness of the boards The energy value is a good indicator of
the solution’s correctness. In fact, we observe that predictions with low-energy oscillator states
tend to be correct (see Fig 18). We utilize this property to improve the performance. For each
givenboard,wesamplemultiplepredictionswithdifferentinitialoscillatorsandselectthelowest-
energy prediction as the model’s answer, which we call Energy-based voting (E-vote). We see in
8(a) (b) (c)
Figure6: (a)TransitionoftheenergyinEq.(3)over#KuramotostepsontheSudokudatasets. The
semi-transparentlinesareactualenergyvaluesaveragedacrossexamples,andthesolidonesconnect
thetroughs. Thedottedverticallineindicates#Kuramotostepssetduringtraining. (b)Azoomed-
inversionofeachplot. (c)Theeffectoftest-timeextensionon#Kuramotosteps.
Model ID OOD
60 SAT-Net(Wangetal.,2019) 98.3 3.2
Diffusion(Duetal.,2024) 66.1 10.3
58
IREM(Duetal.,2022) 93.5 24.6
E-vote
56 RRN(Palmetal.,2018) 99.8 28.6
Avg
R-Transformer(Yangetal.,2023) 100.0 30.3
54
IRED(Duetal.,2024) 99.4 62.1
1 5 20 100 Transformer 98.6±0.3 5.2±0.2
#Random sampling ItrSA 95.7±8.5 34.4±5.4
AKOrNattn 100.0±0.0 61.1±14.7
Figure 7: Improvement of board ac-
curacybythepost-selectionofpredic-Table3: BoardaccuracyonSudokuPuzzles. Weshowthe
tions based on the E values describedmeanandstdoftheaccuracyofmodelswith5differentran-
inSec6.2. T evalissetto128. ‘E-vote’domseedsfortheweightinitialization. TheAKOrNresults
and ‘Avg’ stand for energy-based vot-areobtainedwithT = 128andtheenergy-basedvoting
eval
ingandmajorityvoting,respectively. with100samplesofinitialoscillators.
Fig7thatbyincreasingthenumberofsampledpredictions,themodel’sboardaccuracyimproves.
Interestingly, just averaging the predictions of different states (i.e., majority voting) does not give
betteranswers.
6.3 ROBUSTNESSANDCALIBRATIONIMAGECLASSIFICATION
obustnesstoadversarialattacksanduncertaintyquantificitsationperformanceonthenetworkwith
the and CIFAR10 with common corruptions (CC, Hendrycks & Dietterich (2019)). We train two
types of networks: a convolutional AKOrN (AKOrNconv) and AKOrN with both convolution and
self-attention(AKOrNmix).TheformerhasthreeconvolutionalKuramotoblocks.Thelatterreplaces
thelastblockwithanattentiveKuramotoblock. WeuseAutoAttack(Caronetal.,2021)toevaluate
themodel’sadversarialrobustness.
AKOrNs are resilient against gradient-based attacks The model is heavily regularized and
achievesbothgoodadversarialrobustnessandrobustnesstonaturalcorruptions(Tab4). Thisisre-
markable,sinceconventionalneuralmodelsneedadditionaltechniquessuchasadversarialtraining
and/oradversarialpurificationtoachievegoodadversarialrobustness. Incontrast,AKOrN isrobust
bydesign,evenwhentrainedononlycleanexamples.
K-Netsarewell-calibratedandrobusttostrongrandomnoise
WefoundthatAKOrNsarerobusttostrongrandomnoise(Fig8)andgivegooduncertaintyestima-
tion(onthebottomrightinFig9). Surprisingly,thereisanalmostlinearrelationshipbetweencon-
fidenceandactualaccuracy. Thisissimilartoobservationsingenerativemodels(Grathwohletal.,
2020; Jaini et al., 2024), where conditional generative models give well-calibrated outputs. Since
AKOrN’senergyisnotlearnedtomodelinputdistribution,wecannottightlyrelateourstosuchgen-
erativemodels. However, wespeculatethatAKOrNs’energyroughlyapproximatesthelikelihood
9
)%(
ccA
draoBFigure 8: Robustness performance on random noise examples. Each bar plot shows classification
accuracy on CIFAR10 with strong random noise (∥ϵ∥ = 64/255). The left two pictures are
∞
examplesofimageswiththatϵ. GreenbarsshowaccuracywhenweablateeachelementofAKOrN.
↑Accuracy ↓ECE
Bartoldson’24 Diffenderfer’21
Model Clean Adv CC CC
Bartoldsonetal.(2024) 93.68 73.71 75.9 20.5
Diffenderferetal.(2021) 96.56 0.00 92.8 4.8
ViT 91.44 0.00 81.0 9.6
ResNet-18 94.41 0.00 81.5 8.9
AKOrNconv 88.91 ∗58.91 83.0 1.3
AKOrNmix 91.23 ∗51.56 86.4 1.4 ResNet-18 AKOrNmix
Table 4: Robustness to adversarial examples by Au-
toAttack (Adv) and common corruptions (CC) on CI-
FAR10. ∗The attack is done by AutoAttack with
EoT(Athalyeetal.,2018). ∥ϵ∥ issetto8/255. Ex-
∞
pectedCalibrationError(ECE)measuresthealignment
between confidence of the prediction and accuracy.
The top two methods are selected from the highest-Figure9: ConfidencevsAccuracyplotson
rankedmethodsonhttps://robustbench.github.io/. CIFAR10withcommoncorruptions.
oftheinputexamples,andthustheoscillatorstatefluctuatesaccordingtotheheightoftheenergy,
whichwouldresultingoodcalibration.
7 DISCUSSION & CONCLUSION
WeproposeAKOrN,whichintegratestheKuramotomodelintoneuralnetworksandscalestocom-
plexobservations,suchasnaturalimages. AKOrNslearnstronglyobject-bindingfeatures,canrea-
sonandarerobusttoadversarialandnaturalperturbationswithwell-calibratedpredictions. Webe-
lieveourworkprovidesafoundationforexploringafundamentalshiftinthecurrentneuralnetwork
paradigm.
Ourmodelsstillhavealotofphenomenathatarenotfullyuncovered.Forexample,AKOrNexhibits
quitedifferentbehaviorsdependingontherotatingdimensionN. AKOrN withN = 2isstrongly
regularized, whichpositivelyinfluencesitsrobustness, butnegativelyimpactsoptimization. Addi-
tionally,theperformancewithN = 2forobjectdiscoveryandSudokusolvingismuchworsethan
N = 4. Furtherexperimentalandmathematicalanalysisisneededtounderstandwhythisoccurs,
whichcouldprovideinsightsintohowwecanleveragebothadvantages.
The oscillator is constrained onto the sphere and each single oscillator cannot represent the ‘pres-
ence’ofthefeaturesliketherotatingfeaturesinLo¨weetal.(2024a).Becauseofthat,AKOrNwould
not perform well on memory tasks, where the model needs to remember the presence of events.
Thisnormconstraintalsodoesnotalignwithrealbiologicalneuronsthathavefiringandnon-firing
states. Relaxingthehardnormconstraintoftheoscillatorwouldbeaninterestingfuturedirectionin
termsofbothbiologicalplausibilityandapplicabilitytoamuchwiderrangeoftaskssuchaslong-
termtemporalprocessing.
108 ACKNOWLEDGEMENT
TakeruMiyatoandAndreasGeigerweresupportedbytheERCStartingGrantLEGO-3D(850533)
andtheDFGEXCnumber2064/1-projectnumber390727645. WethankAndyKeller,Jun-nosuke
Teramae, Bernhard Jaeger, Madhav Iyengar, and Daniel Dauner for their insightful feedback and
comments. Takeru Miyato acknowledges his affiliation with the ELLIS (European Laboratory for
LearningandIntelligentSystems)PhDprogram.
REFERENCES
ToshioAoyagi. Networkofneuraloscillatorsforretrievingphaseinformation. Physicalreviewletters,74(20):
4075,1995.
Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security:
Circumventingdefensestoadversarialexamples. InProc.oftheInternationalConf.onMachinelearning
(ICML),pp.274–283,2018.
JimmyLeiBa,JamieRyanKiros,andGeoffreyEHinton. Layernormalization. arXiv.org,2016.
DzmitryBahdanau, KyunghyunCho, andYoshuaBengio. Neuralmachinetranslationbyjointlylearningto
alignandtranslate. arXiv.org,2014.
BrianRBartoldson,JamesDiffenderfer,KonstantinosParasyris,andBhavyaKailkhura.Adversarialrobustness
limitsviascaling-lawandhuman-alignmentstudies.InProc.oftheInternationalConf.onMachinelearning
(ICML),2024.
GabrielBBenigno,RobertoCBudzinski,ZacharyWDavis,JohnHReynolds,andLyleMuller. Wavestravel-
ingoveramapofvisualspacecanigniteshort-termpredictionsofsensoryinput. NatureCommunications,
14(1):3409,2023.
ChristopherMBishopandNasserMNasrabadi.Patternrecognitionandmachinelearning,volume4.Springer,
2006.
Ondrej Biza, Sjoerd Van Steenkiste, Mehdi SM Sajjadi, Gamaleldin F Elsayed, Aravindh Mahendran, and
ThomasKipf. Invariantslotattention: Objectdiscoverywithslot-centricreferenceframes. InProc.ofthe
InternationalConf.onMachinelearning(ICML),2023.
WilfriedBounsi,BorjaIbarz,AndrewDudzik,JessicaBHamrick,LarisaMarkeeva,AlexVitvitskyi,Razvan
Pascanu,andPetarVelicˇkovic´. Transformersmeetneuralalgorithmicreasoners. arXiv.org,2024.
MichaelBreakspear,StewartHeitmann,andAndreasDaffertshofer. Generativemodelsofcorticaloscillations:
neurobiologicalimplicationsofthekuramotomodel. Frontiersinhumanneuroscience,4:190,2010.
ChristopherPBurgess, LoicMatthey, NicholasWatters, RishabhKabra, IrinaHiggins, MattBotvinick, and
AlexanderLerchner. MONet:Unsupervisedscenedecompositionandrepresentation. arXiv.org,2019.
Mathilde Caron, Hugo Touvron, Ishan Misra, Herve´ Je´gou, Julien Mairal, Piotr Bojanowski, and Armand
Joulin.Emergingpropertiesinself-supervisedvisiontransformers.InProc.oftheIEEEInternationalConf.
onComputerVision(ICCV),pp.9650–9660,2021.
Sarthak Chandra, Michelle Girvan, and Edward Ott. Continuous versus discontinuous transitions in the d-
dimensionalgeneralizedkuramotomodel:Odddisdifferent. PhysicalReviewX,9(1):011002,2019.
MichaelChang,TomGriffiths,andSergeyLevine. Objectrepresentationsasfixedpoints: Trainingiterative
refinementalgorithmswithimplicitdifferentiation. InAdvancesinNeuralInformationProcessingSystems
(NeurIPS),pp.32694–32708,2022.
TingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton. Asimpleframeworkforcontrastive
learning of visual representations. In Proc. of the International Conf. on Machine learning (ICML), pp.
1597–1607,2020.
XinleiChen,SainingXie,andKaimingHe.Anempiricalstudyoftrainingself-supervisedvisiontransformers.
InProc.oftheIEEEInternationalConf.onComputerVision(ICCV),pp.9640–9649,2021.
DavidCuminandCP2296240Unsworth. Generalisingthekuramotomodelforthestudyofneuronalsynchro-
nisationinthebrain. PhysicaD:NonlinearPhenomena,226(2):181–196,2007.
11ZacharyWDavis,LyleMuller, JulioMartinez-Trujillo, TerrenceSejnowski, andJohnHReynolds. Sponta-
neoustravellingcorticalwavesgateperceptioninbehavingprimates. Nature,587(7834):432–436,2020.
BhishmaDedhiaandNirajKJha. Neuralslotinterpreters:Groundingobjectsemanticsinemergentslotrepre-
sentations. arXiv.org,2024.
JamesDiffenderfer,BrianBartoldson,ShreyaChaganti,JizeZhang,andBhavyaKailkhura. Awinninghand:
Compressingdeepnetworkscanimproveout-of-distributionrobustness. InAdvancesinNeuralInformation
ProcessingSystems(NeurIPS),pp.664–676,2021.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Un-
terthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszkoreit,andNeil
Houlsby. Animageisworth16x16words: Transformersforimagerecognitionatscale. InProc.oftheIn-
ternationalConf.onLearningRepresentations(ICLR),2021.
Yilun Du, Shuang Li, Joshua Tenenbaum, and Igor Mordatch. Learning iterative reasoning through energy
minimization. InProc.oftheInternationalConf.onMachinelearning(ICML),pp.5570–5582,2022.
YilunDu,JiayuanMao,andJoshuaBTenenbaum. Learningiterativereasoningthroughenergydiffusion. In
Proc.oftheInternationalConf.onMachinelearning(ICML),2024.
NouhaDziri,XimingLu,MelanieSclar,XiangLorraineLi,LiweiJiang,BillYuchenLin,SeanWelleck,Peter
West,ChandraBhagavatula,RonanLeBras,etal.Faithandfate:Limitsoftransformersoncompositionality.
InAdvancesinNeuralInformationProcessingSystems(NeurIPS),2024.
FelixEffenberger,PedroCarvalho,IgorDubinin,andWolfSinger. Thefunctionalroleofoscillatorydynamics
inneocorticalcircuits:acomputationalperspective. bioRxiv,pp.2022–11,2022.
MarkEveringham,LucVanGool,ChristopherKIWilliams,JohnWinn,andAndrewZisserman. Thepascal
visualobjectclasses(voc)challenge. Internationaljournalofcomputervision,88:303–338,2010.
JuergenFellandNikolaiAxmacher. Theroleofphasesynchronizationinmemoryprocesses. Naturereviews
neuroscience,12(2):105–118,2011.
Michel Fruchart, Ryo Hanai, Peter B Littlewood, and Vincenzo Vitelli. Non-reciprocal phase transitions.
Nature,592(7854):363–369,2021.
BorjanGeshkovski,CyrilLetrouit,YuryPolyanskiy,andPhilippeRigollet. Theemergenceofclustersinself-
attentiondynamics. InAdvancesinNeuralInformationProcessingSystems(NeurIPS),2024.
IanJGoodfellow,JonathonShlens,andChristianSzegedy. Explainingandharnessingadversarialexamples.
InProc.oftheInternationalConf.onLearningRepresentations(ICLR),2014.
Anand Gopalakrishnan, Aleksandar Stanic´, Ju¨rgen Schmidhuber, and Michael Curtis Mozer. Recurrent
complex-weightedautoencodersforunsupervisedobjectdiscovery. arXiv.org,2024.
SvenGowal, ChongliQin, JonathanUesato, TimothyMann, andPushmeetKohli. Uncoveringthelimitsof
adversarialtrainingagainstnorm-boundedadversarialexamples. arXiv.org,2020.
Sven Gowal, Sylvestre-Alvise Rebuffi, Olivia Wiles, Florian Stimberg, Dan Andrei Calian, and Timothy A
Mann. Improvingrobustnessusinggenerateddata. InAdvancesinNeuralInformationProcessingSystems
(NeurIPS),pp.4218–4233,2021.
WillGrathwohl,Kuan-ChiehWang,Jo¨rn-HenrikJacobsen,DavidDuvenaud,MohammadNorouzi,andKevin
Swersky. Yourclassifierissecretlyanenergybasedmodelandyoushouldtreatitlikeone. InProc.ofthe
InternationalConf.onLearningRepresentations(ICLR),2020.
KlausGreff,Raphae¨lLopezKaufman,RishabhKabra,NickWatters,ChristopherBurgess,DanielZoran,Loic
Matthey, MatthewBotvinick, andAlexanderLerchner. Multi-objectrepresentationlearningwithiterative
variationalinference.InProc.oftheInternationalConf.onMachinelearning(ICML),pp.2424–2433,2019.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Identitymappingsindeepresidualnetworks. In
Proc.oftheEuropeanConf.onComputerVision(ECCV),pp.630–645,2016.
KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDolla´r,andRossGirshick. Maskedautoencoders
arescalablevisionlearners. InProc.IEEEConf.onComputerVisionandPatternRecognition(CVPR),pp.
16000–16009,2022.
WernerHeisenberg. Zurtheoriedesferromagnetismus. Springer,1985.
12Stewart Heitmann, Pulin Gong, and Michael Breakspear. A computational role for bistability and traveling
wavesinmotorcortex. Frontiersincomputationalneuroscience,6:67,2012.
DanHendrycksandThomasDietterich. Benchmarkingneuralnetworkrobustnesstocommoncorruptionsand
perturbations. InProc.oftheInternationalConf.onLearningRepresentations(ICLR),2019.
DanHendrycks,NormanMu,EkinDCubuk,BarretZoph,JustinGilmer,andBalajiLakshminarayanan. Aug-
mix: Asimpledataprocessingmethodtoimproverobustnessanduncertainty. InProc.oftheInternational
Conf.onLearningRepresentations(ICLR),2020.
SaraHooker. Thehardwarelottery. CommunicationsoftheACM,64(12):58–65,2021.
Benjamin Hoover, Yuchen Liang, Bao Pham, Rameswar Panda, Hendrik Strobelt, Duen Horng Chau, Mo-
hammedZaki,andDmitryKrotov. Energytransformer. InAdvancesinNeuralInformationProcessingSys-
tems(NeurIPS),2024.
JohnJHopfield. Neuralnetworksandphysicalsystemswithemergentcollectivecomputationalabilities. Pro-
ceedingsofthenationalacademyofsciences,79(8):2554–2558,1982.
DavidHHubelandTorstenNWiesel. Receptivefields,binocularinteractionandfunctionalarchitectureinthe
cat’svisualcortex. TheJournalofphysiology,160(1):106,1962.
Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing
internalcovariateshift. InProc.oftheInternationalConf.onMachinelearning(ICML),2015.
PriyankJaini,KevinClark,andRobertGeirhos. Intriguingpropertiesofgenerativeclassifiers. InProc.ofthe
InternationalConf.onLearningRepresentations(ICLR),2024.
BaoxiongJia,YuLiu,andSiyuanHuang. Improvingobject-centriclearningwithqueryoptimization. InProc.
oftheInternationalConf.onLearningRepresentations(ICLR),2023.
JindongJiang,FeiDeng,GautamSingh,andSungjinAhn.Object-centricslotdiffusion.InAdvancesinNeural
InformationProcessingSystems(NeurIPS),2023.
WhieJung,JaehoonYoo,SungjinAhn,andSeunghoonHong. Learningtocompose:Improvingobjectcentric
learning by injecting compositionality. In Proc. of the International Conf. on Learning Representations
(ICLR),2024.
RishabhKabra,ChrisBurgess,LoicMatthey,RaphaelLopezKaufman,KlausGreff,MalcolmReynolds,and
AlexanderLerchner. Multi-objectdatasets. https://github.com/deepmind/multi-object-datasets/,2019.
IoannisKakogeorgiou,SpyrosGidaris,KonstantinosKarantzalos,andNikosKomodakis. Spot: Self-training
withpatch-orderpermutationforobject-centriclearningwithautoregressivetransformers. InProc.IEEE
Conf.onComputerVisionandPatternRecognition(CVPR),pp.22776–22786,2024.
LaurynasKarazija,IroLaina,andChristianRupprecht. Clevrtex: Atexture-richbenchmarkforunsupervised
multi-objectsegmentation. arXiv.org,2021.
TAndersonKeller,LyleMuller,TerrenceJSejnowski,andMaxWelling.Aspacetimeperspectiveondynamical
computationinneuralinformationprocessingsystems. arXiv.org,2024.
DiederikKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. InProc.oftheInternational
Conf.onLearningRepresentations(ICLR),2015.
KlimKireev,MaksymAndriushchenko,andNicolasFlammarion. Ontheeffectivenessofadversarialtraining
againstcommoncorruptions. InUncertaintyinArtificialIntelligence,pp.1012–1021.PMLR,2022.
AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton.Imagenetclassificationwithdeepconvolutionalneural
networks. InAdvancesinNeuralInformationProcessingSystems(NeurIPS),2012.
YoshikiKuramoto. Chemicalturbulence. Springer,1984.
AlinaKuznetsova,HassanRom,NeilAlldrin,JasperUijlings,IvanKrasin,JordiPont-Tuset,ShahabKamali,
StefanPopov, MatteoMalloci, AlexanderKolesnikov, etal. Theopenimagesdatasetv4: Unifiedimage
classification,objectdetection,andvisualrelationshipdetectionatscale. Internationaljournalofcomputer
vision,128(7):1956–1981,2020.
YaLeandXuanYang. Tinyimagenetvisualrecognitionchallenge. CS231N,7(7):3,2015.
13NicolasLeRoux,NicolasHeess,JamieShotton,andJohnWinn. Learningagenerativemodelofimagesby
factoringappearanceandshape. NeuralComputation,23(3):593–650,2011.
KiminLee,HonglakLee,KibokLee,andJinwooShin. Trainingconfidence-calibratedclassifiersfordetecting
out-of-distributionsamples. arXiv.org,2017.
AlexanderCLi,MihirPrabhudesai,ShivamDuggal,EllisBrown,andDeepakPathak. Yourdiffusionmodel
issecretlyazero-shotclassifier. InProc.oftheIEEEInternationalConf.onComputerVision(ICCV),pp.
2206–2217,2023.
LuisaHBLiboni,RobertoCBudzinski,AlexandraNBusch,SindyLo¨we,ThomasAKeller,MaxWelling,and
LyleEMuller. Imagesegmentationwithtravelingwavesinanexactlysolvablerecurrentneuralnetwork.
arXiv.org,2023.
Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays,PietroPerona,DevaRamanan,PiotrDolla´r,and
CLawrenceZitnick. Microsoftcoco: Commonobjectsincontext. InComputerVision–ECCV2014: 13th
EuropeanConference, Zurich, Switzerland, September6-12, 2014, Proceedings, PartV13, pp.740–755.
Springer,2014.
MaxLipton,RenatoMirollo,andStevenHStrogatz. Thekuramotomodelonasphere: Explainingitslow-
dimensionaldynamicswithgrouptheoryandhyperbolicgeometry. Chaos: AnInterdisciplinaryJournalof
NonlinearScience,31(9),2021.
Francesco Locatello, Dirk Weissenborn, Thomas Unterthiner, Aravindh Mahendran, Georg Heigold, Jakob
Uszkoreit,AlexeyDosovitskiy,andThomasKipf. Object-centriclearningwithslotattention. InAdvances
inNeuralInformationProcessingSystems(NeurIPS),pp.11525–11538,2020.
SindyLo¨we,PhillipLippe,MajaRudolph,andMaxWelling.Complex-valuedautoencodersforobjectdiscov-
ery. arXivpreprintarXiv:2204.02075,2022.
SindyLo¨we,PhillipLippe,FrancescoLocatello,andMaxWelling. Rotatingfeaturesforobjectdiscovery. In
AdvancesinNeuralInformationProcessingSystems(NeurIPS),2024a.
Sindy Lo¨we, Francesco Locatello, and Max Welling. Binding Dynamics in Rotating Features. ICLR 2024
Workshop:BridgingtheGapBetweenPracticeandTheoryinDeepLearning,2024b.
EvgueniyVLubenovandAthanassiosGSiapas. Hippocampalthetaoscillationsaretravellingwaves. Nature,
459(7246):534–539,2009.
AleksanderMadry,AleksandarMakelov,LudwigSchmidt,DimitrisTsipras,andAdrianVladu. Towardsdeep
learningmodelsresistanttoadversarialattacks. arXiv.org,2017.
DanielCMattis. ThetheoryofmagnetismI:StaticsandDynamics,volume17. SpringerScience&Business
Media,2012.
WarrenSMcCullochandWalterPitts.Alogicalcalculusoftheideasimmanentinnervousactivity.Thebulletin
ofmathematicalbiophysics,5:115–133,1943.
TakeruMiyato,Shin-ichiMaeda,MasanoriKoyama,andShinIshii. Virtualadversarialtraining: aregulariza-
tionmethodforsupervisedandsemi-supervisedlearning. IEEEtransactionsonpatternanalysisandma-
chineintelligence,41(8):1979–1993,2018.
TakeruMiyato,BernhardJaeger,MaxWelling,andAndreasGeiger. Gta:Ageometry-awareattentionmecha-
nismformulti-viewtransformers. InProc.oftheInternationalConf.onLearningRepresentations(ICLR),
2024.
TomMonnier, ElliotVincent, JeanPonce, andMathieuAubry. Unsupervisedlayeredimagedecomposition
intoobjectprototypes.InProc.IEEEConf.onComputerVisionandPatternRecognition(CVPR),pp.8640–
8650,2021.
Lyle Muller, Giovanni Piantoni, Dominik Koller, Sydney S Cash, Eric Halgren, and Terrence J Sejnowski.
Rotatingwavesduringhumansleepspindlesorganizeglobalpatternsofactivitythatrepeatpreciselythrough
thenight. Elife,5:e17267,2016.
LyleMuller,Fre´de´ricChavane,JohnReynolds,andTerrenceJSejnowski. Corticaltravellingwaves: mecha-
nismsandcomputationalprinciples. NatureReviewsNeuroscience,19(5):255–268,2018.
AndrewNgandMichaelJordan. Ondiscriminativevs.generativeclassifiers:Acomparisonoflogisticregres-
sionandnaivebayes. InAdvancesinNeuralInformationProcessingSystems(NeurIPS),2001.
14RasmusPalm,UlrichPaquet,andOleWinther. Recurrentrelationalnetworks. Advancesinneuralinformation
processingsystems,31,2018.
HubertRamsauer,BernhardScha¨fl,JohannesLehner,PhilippSeidl,MichaelWidrich,ThomasAdler,Lukas
Gruber,MarkusHolzleitner,MilenaPavlovic´,GeirKjetilSandve,etal. Hopfieldnetworksisallyouneed.
arXiv.org,2020.
DavidPReichertandThomasSerre. Neuronalsynchronyincomplex-valueddeepnetworks. arXivpreprint
arXiv:1312.6115,2013.
James A Roberts, Leonardo L Gollo, Romesh G Abeysuriya, Gloria Roberts, Philip B Mitchell, Mark W
Woolrich,andMichaelBreakspear. Metastablebrainwaves. Naturecommunications,10(1):1056,2019.
DougRubino,KayARobbins,andNicholasGHatsopoulos. Propagatingwavesmediateinformationtransfer
inthemotorcortex. Natureneuroscience,9(12):1549–1557,2006.
BrunoSauvalleandArnauddeLaFortelle. Unsupervisedmulti-objectsegmentationusingattentionandsoft-
argmax. InProc.oftheIEEEWinterConferenceonApplicationsofComputerVision(WACV),pp.3267–
3276,2023.
Maximilian Seitzer, Max Horn, Andrii Zadaianchuk, Dominik Zietlow, Tianjun Xiao, Carl-Johann Simon-
Gabriel,TongHe,ZhengZhang,BernhardScho¨lkopf,ThomasBrox,etal. Bridgingthegaptoreal-world
object-centriclearning. InProc.oftheInternationalConf.onLearningRepresentations(ICLR),2023.
CharlesScottSherrington. Theintegrativeactionofthenervoussystem. YaleUniversityPress,1906.
GautamSingh,FeiDeng,andSungjinAhn. Illiteratedall-elearnstocompose. arXiv.org,2021.
Gautam Singh, Yi-Fu Wu, and Sungjin Ahn. Simple unsupervised object-centric learning for complex and
naturalisticvideos. pp.18181–18196,2022.
DavidCSomers,SachaBNelson,andMrigankaSur.Anemergentmodeloforientationselectivityincatvisual
corticalsimplecells. Journalofneuroscience,15(8):5448–5465,1995.
Aleksandar Stanic´, Anand Gopalakrishnan, Kazuki Irie, and Ju¨rgen Schmidhuber. Contrastive training of
complex-valuedautoencodersforobjectdiscovery. InAdvancesinNeuralInformationProcessingSystems
(NeurIPS),2023.
JianlinSu,YuLu,ShengfengPan,AhmedMurtadha,BoWen,andYunfengLiu. Roformer: Enhancedtrans-
formerwithrotarypositionembedding. arXiv.org,2021.
Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and
Rob Fergus. Intriguing properties of neural networks. In Proc. of the International Conf. on Learning
Representations(ICLR),2014.
FlorianTramer,NicholasCarlini,WielandBrendel,andAleksanderMadry. Onadaptiveattackstoadversarial
exampledefenses.InAdvancesinNeuralInformationProcessingSystems(NeurIPS),pp.1633–1645,2020.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanN.Gomez,LukaszKaiser,
and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems
(NIPS),pp.5998–6008,2017.
Po-WeiWang,PriyaDonti,BryanWilder,andZicoKolter.Satnet:Bridgingdeeplearningandlogicalreasoning
usingadifferentiablesatisfiabilitysolver. InProc.oftheInternationalConf.onMachinelearning(ICML),
pp.6545–6554,2019.
TianshiWangandJaijeetRoychowdhury. Oscillator-basedisingmachine. arXiv.org,2017.
YuxinWuandKaimingHe.Groupnormalization.InProc.oftheEuropeanConf.onComputerVision(ECCV),
pp.3–19,2018.
ZiyiWu,JingyuHu,WuyueLu,IgorGilitschenski,andAnimeshGarg.Slotdiffusion:Object-centricgenerative
modeling with diffusion models. In Advances in Neural Information Processing Systems (NeurIPS), pp.
50932–50958,2023.
ZhunYang,AdamIshay,andJoohyungLee. Learningtosolveconstraintsatisfactionproblemswithrecurrent
transformer. arXiv.org,2023.
HonghuiZhang,AndrewJWatrous,AnshPatel,andJoshuaJacobs. Thetaandalphaoscillationsaretraveling
wavesinthehumanneocortex. Neuron,98(6):1269–1281,2018.
15HongyangZhang,YaodongYu,JiantaoJiao,EricXing,LaurentElGhaoui,andMichaelJordan. Theoretically
principledtrade-offbetweenrobustnessandaccuracy. InProc.oftheInternationalConf.onMachinelearn-
ing(ICML),pp.7472–7482,2019.
16Appendix
Table of Contents
A Relationtophysicsmodels 18
B RelatedworksontheNNrobustness 19
C Experimentalsettings 19
C.1 Unsupervisedobjectdiscovery . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.2 Sudokusolving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.3 RobustnessandcalibrationonCIFAR10 . . . . . . . . . . . . . . . . . . . . . 25
D Additionalexperimentalresults 25
D.1 Positionalencodingfortheattentiveconnectivity . . . . . . . . . . . . . . . . 25
D.2 Unsupervisedobjectdiscovery . . . . . . . . . . . . . . . . . . . . . . . . . . 26
D.3 Sudokusolving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
D.4 RobustnessandcalibrationonCIFAR10 . . . . . . . . . . . . . . . . . . . . . 30
17C X
Figure10: Thetransitionofthe64×64oscillatorneurons(N =2). (Left)VisualiaztionofC. c on
i
thewhiteregionis1andtheblackregionis0. (Right)Oscillators’timeevolution. Similarcolors
indicateoscillatorsdirectingsimilardirections. TheconnectivityJisthefollowing9×9Gaussian
kernel: J cdhw = k hwI ∈ R2×2,k hw = exp(−∥h−4.5∥2 2 3+ .0∥ 2w−4.5∥2 2). The oscillators on the white
regionofCarefixedinthesamedirectionastheconditionalstimuliandalmoststayconstantacross
time. Theoscillatorsontheblackregionarelargelyinfluencedbytheneighboringoscillatorsand
exhibitwavypatterns.
RO
MLP
MLP
×T
×T
×T
Conv
SA K-Layer
Conv/Attn
GELU
LN
GN
(a)Iterativeconv (b)Iterativeself-attention (c)AKOrN
Figure 11: The single block of each (a) ItrConv (b) ItrSA, and (c) AKOrN. GN and LN stand for
GroupNormalization(Wu&He,2018)andLayernormalization(Baetal.,2016).TheMLPin(a)or
(b)iscomposedofastackofGNorLNfollowedbyLinear,GELU,andLinear. Thehiddendimof
MLPis4×(channelsize). ThenumberofheadsinSAandtheK-Layerwithattentiveconnectivity
issetto8throughoutourexperiments.
A RELATION TO PHYSICS MODELS
Similar to how the Ising model is the basis for recurrent neural models, such as the Hopfield
model(Hopfield,1982),theKuramotomodelwithsymmetriclateralinteractionscanalsobestud-
iedbyviewingitasamodelfromstatisticalphysicscalledtheHeisenbergmodel(Mattis,2012). In
fact, wewillbeusingamoregeneralversionoftheKuramotomodelwhichinvolvesasymmetry-
breakingterm(akintoamagneticfieldinteraction)andasymmetricconnectionsbetweentheneu-
rons. Thisnotonlyisbiologicallyplausible(synapsesarenotsymmetric),italsoleadstomuchbet-
terresultsinourexperiments.
Non-equilibriumsoftmatterphysicshasstudiedmodelswithnonreciprocalinteractions,forinstance
inthefieldof“activematter”. Theyhavedevelopedaccuratecoarse-grainedhydrodynamicsmodels
toapproximatethemicroscopicdynamicsandobservedveryinterestingbehavior,suchassymmetry-
breaking phase transitions and resultant traveling waves representing so called Goldstone modes
(Fruchartetal.,2021). Wehopethatthisopensthedoortoadeeperunderstandingofthesemodels
whenemployedasneuralnetworks.
18Tetrominoes dSprites CLEVR
Trainingexamples 60,000 60,000 50,000
Testexamples 320 320 320
Imagesize 32 64 128
Maximumnumberofobjects 3 6 6
Patchsize 4 4 8
Patchresolution 8 16 16
Channelsize 128 128 256
#internalsteps(T) 8
#Epochs 50 50 300
Batchsize 256
Learningrate 0.001
Augmentations Randomresizeandcrop+colorjittering
#clusterssetforeval 4 7 11
Table5: ExperimentalsettingsonTetrominoes,dSprites,andCLEVR.
B RELATED WORKS ON THE NN ROBUSTNESS
Experimental proof of the conventional NNs’ limited OOD generalization is represented by the
vulnerability to adversarial examples (Szegedy et al., 2014; Goodfellow et al., 2014). The most
effective way to resist such examples is training the model on adversarial examples generated by
the model itself, which is called adversarial training (Goodfellow et al., 2014; Madry et al., 2017;
Miyatoetal.,2018;Zhangetal.,2019). Manyotherdefenseshavebeenproposed,butmostofthem
werefoundtobenotafundamentalsolution(Trameretal.,2020).
One framework that can produce more human-algined predictions is a generative classifier (Ng
& Jordan, 2001; Bishop & Nasrabadi, 2006), where we train a model with both generative and
discriminative objectives or turn a label conditional generative model into a discriminative model
basedonBayestheorem.Interestingly,differentgenerativeclassifierstrainedwithdifferentmethods
share similar robust and calibration properties (Lee et al., 2017; Grathwohl et al., 2020; Li et al.,
2023;Jainietal.,2024). Generativeclassifiersarerobustbutinvolvecostlygenerativetrainingsuch
asdenoisingdiffusion(Lietal.,2023;Jainietal.,2024),MCMC(Grathwohletal.,2020)togenerate
negative samples, or unstable min-max optimization as GANs training (Lee et al., 2017). AKOrN
sharessimilarrobustnesspropertiesbutwithoutanygenerativeobjectives.
C EXPERIMENTAL SETTINGS
WeobservethatboththereadoutmoduleandconditionalstimuliCareessentialforstabletraining,
especiallywhenN =2.WealsoseethatAKOrNwithN =2exhibitsastrongregularity,whichacts
positivelyonrobustnessperformancewhilehavingnegativeeffectsonunsupervisedobjectdiscovery
andtheSudoku-solvingexperiments. WeshowresultsofAKOrNwithN =4inthoseexperiments.
WedonotobserveimprovementbyincreasingN above4.
Tab5-Tab8showexperimentalsettingsoneachdataset(e.g. hyperparametersonmodelsandopti-
mization,thenumberoftrainingandtestexamples,datasetstatistics,etc...). ForAKOrN,thechan-
nel size is set to (thechannelsizeshowninthetable)/N, so the memory consumption and FLOPs
are effectively the same between AKOrNs and their non-Kuramoto counterpart baselines we test.
AllmodelsaretrainedwithAdam(Kingma&Ba,2015)withoutweightdecay.
19CLEVRTex OOD CAMO
Trainingexamples 40,000 - -
Testexamples 5,000 10,000 2,000
Imagesize 128
Maximumnumberofobjects 10
Patchsize 8
Patchresolution 16
Channelsize 256
#internalsteps(T) 8
#epochs 500 - -
Batchsize 256 - -
Learningrate 0.0005 - -
Augmentations Randomresizeandcrop+colorjittering
#clusterssetforeval 11
Table6: ExperimentalsettingsonCLEVRTexanditsvariants(OOD,CAMO).Wealsotrainalarge
AKOrN model that is trained with the doubled channel size, epochs, and epochs. We denote that
modelbyLargeAKOrN.
C.1 UNSUPERVISEDOBJECTDISCOVERY
We test on 4 synthetic datasets (Tetrominoes, dSprites, CLEVR, CLEVRTex) and 2 real image
datasets(PascalVOC,COCO2017).ThekernelsizeofconvolutionlayersinAKOrNconvandItrConv
issetto5,7,and9onTetrominoes,dSprites,andCLEVR,respectively. InadditiontoItrConvand
ItrSA,wealsotrainaViTmodel(Dosovitskiyetal.,2021)asanotherbaselinemodel.
AllnetworksprocessimagessimilarlytoViT(Dosovitskiyetal.,2021). First,wepatcheachimage
into H/P ×W/P patches where H,W are the height and width of the image and P is the patch
size. Wethenapplythestackofblocks. Theoutputofthefinalblockisfurtherprocessedbyglobal
max-poolingfollowedbyasinglehiddenlayerMLP,whoseoutputisusedtocomputetheSimCLR
loss. We used a conventional set of augmentations for SSL training: random resizing, cropping,
and color jittering. We also apply horizontal flipping for the ImageNet pretraining. All models
includingbaselinemodelshaveroughlythesamenumberofparametersandaretrainedwithshared
hyperparameterssuchaslearningratesandtrainingepochs. SeeTab5-7forthosehyperparameter
details.
InAKOrN,Cisinitializedbythepatchedfeaturesofimages,whileeachx isinitializedbyrandom
i
oscillatorssampledfromtheuniformdistributiononthesphere. Weusetheidentityfunctionforg
ineachreadoutmodule. Inmulti-blockmodels, weapplyGroupNormalization(Wu&He,2018)
toCexceptforC(L).
For the Tetrominoes, dSprites, and CLEVR datasets, we train single-block models with T = 8.
Weobservethatstackingmultipleblocksdoesnotyieldimprovementsonthosethreedatasets. On
CLEVRTex,wetrainsingle-andtwo-blockmodelswithattentiveconnectivityandT =8,whileon
ImageNet,wetrainathree-blockAKOrN modelwithattentiveconnectivityandT =4.
Followingtheliterature,weexcludethebackgroundmaskfromtheMBOevaluation.
20ImageNet PascalVOC COCO2017
Trainingexamples 1,281,167 - -
Testexamples - 1,449 5,000
Imagesize 256 256 256
Patchsize 16
Patchresolution 16
Channelsize 768
#Blocks 3
#internalsteps(T) 4
#epochs 400 - -
Batchsize 512 - -
Learningrate 0.0005 - -
#clusterssetforeval - 4 7
Table 7: Experimental settings on ImageNet pratraining and on the PascalVOC and COCO2017
evaluation.ForSimCLRtrainingaugmentations,weuserandomresizeandcrop,colorjittering,and
horizontalflipping.
Sudoku(ID)(Wangetal.,2019) Sudoku(OOD)(Palmetal.,2018)
9 1 5 3 6 5 1
3 6 2 1 8 6 9 4 3
2 7 4 6 9 7 2
4 7 2 5 5 8 9
1 9 3 8 4 7 6
7 8 9 3
6 5 4 1 9 2
4 8 9 3 7 9
5 1 2 7
Trainingexamples 9,000 -
Testexamples 1,000 18,000
Channelsize 512
#epochs 100 -
Batchsize 100 -
Learningrate 0.0005 -
Table8: Sudokupuzzledatasets.
21① ② ①① ②
① ② ① ②
Tiling
③ ④ ③ ④
③ ④ ③③ ④ ① ② ① ②
③ ④ ③ ④
Figure 12: 2× up-tiling. First, we create horizontally or/and vertically shifted images with stride
equal to (patchsize/2) and compute the model’s output on each shifted image. We then interleave
eachtokenfeaturetomakea2×upsampledfeaturemap.
C.1.1 UPSAMPLEFEATURESBYUP-TILING
Whenwecomputetheclusterassignment, weupsampletheoutputfeaturesbyup-tilingwherewe
letthemodelseeasetofpicturesthatareslightlyshiftedbothonthehorizontalor/andverticalaxes
and make the higher resolution feature map by interleaving those features. This up-tiling enables
ustogetfinerclusterassignmentsandsubstantiallyimprovestheobjectdiscoveryperformanceof
our AKOrN. We show a pictorial explanation in Fig 12 and PyTorch code below. We also show
a comparison to the original features and bilinear upsampling in Fig 13 and examples of up-tiled
features in Fig 14. We apply up-tiling with the scale factor of 4 for producing numbers on Tab 1
and 2 as well as for cluster visualization in Fig 4,5 and Fig 20-24. Unless otherwise stated, no
upsamplingisperformedwhencomputingtheclusterassignment.
Code1: PyTorchcodeforup-tiling
def create_shifted_imgs(img, psize, stride):
H, W = img.shape[-2:]
img = F.interpolate(img,
(H+psize-stride, W+psize-stride),
mode=’bilinear’, align_corners=False)
imgs = []
for h in range(0, psize, stride):
for w in range(0, psize, stride):
imgs.append(img[:, :, h:h+H, w:w+W])
return imgs
def uptiling(model, images, psize=16, s=2):
"""
Args:
model: a function that takes [B,C,H,W]-shaped tensor
and outputs [B,C,H/psize,W/psize]-shaped tensor.
images: a tensor of shape [B, C, H, W].
psize: the patch size of the model.
s: scale factor. The resulting features will
be upscaled to [R*H/psize, R*W/psize]
where (H, W) are the original image size.
Must be equal to or less than the patch size.
Returns:
nimgs: [B, C, R*H/psize, R*W/psize]
"""
B = images.shape[0]
stride = psize // s
# Create shifted images.
shifted_imgs = create_shifted_imgs(images, psize, stride)
# Compute a feature map on each shifted image.
outputs = []
for i in range(len(shifted_imgs)):
with torch.no_grad():
output = model(shifted_imgs[i].cuda())
22
NetworkPCA1-3 PCA4-6 PCA6-9
Input
Bilinear
Up-tiling
(a)Theoriginalfeatures(top)andfeaturesupsampledbyup-tiling(bottom).
PCA1-3 PCA4-6 PCA6-9
Original
Input
features
Up-tiled
features
(b)Featuresupsampledbybilinearupsampling(top)andbyup-tiling(bottom).
Figure13: ComparisonofAKOrN’soutputfeaturesupsampledbydifferentmethods. PCA{i−j}
indicates that the corresponding column’s panels represent the features’ i-th to j-th PCA compo-
nents. Thescalingfactorofup-tilingissetto8.
outputs.append(output.detach().cpu())
# Tile the output feature maps.
oh, ow = outputs[0].shape[-2:]
nimgs = torch.zeros(B, outputs[0].shape[1], oh, s, ow, s)
for h in range(R):
for w in range(R):
nimgs[:, :, :, h, :, w] = outputs[h*R+w]
# Reshape into [B, C, (H/psize)*R, (W/psize)*R]
nimgs = nimgs.view(, -1, oh*nh, ow*nw)
return nimgs
C.2 SUDOKUSOLVING
The task is to fill a 9×9 grid, given some initial digits from 1 to 9, so that each row, column, and
3×3 subgrid contains all digits from 1 to 9. While the task may be straightforward if the game’s
rulesareknown,themodelmustlearntheserulessolelyfromthetrainingset. Exampleboardsare
showninTab.8”.
23PCA1-3 PCA4-6 PCA7-9 Input
(a)CLEVRTex
PCA1-3 PCA4-6 PCA7-9 Input
(b)PascalVOC
Figure14: Up-tiliedfeaturemapsonCLEVRTexandPascalVOC.Thescalefactorsaresetto8and
16forCLEVRTexandPascalVOC,respectively.
24WetrainAKOrNwithattentiveconnections,theItrSAmodel,andaconventionaltransformermodel.
We denote them by AKOrNattn, ItrSA, and Transformer, respectively. AKOrNattn has almost the
same architecture used in the object discovery task except for g in the readout module, which is
composed of the norm computation layer followed by a stack of BatchNormalization, ReLU, and
linearlayer.
Theinputforeachmodelis9×9digitsfrom0to9(0forblank,1-9forgivendigits).Wefirstembed
each digit into a 512-dimensional token vector. The 9×9 tokens are then flattened into 81 tokens.
Weapplyeachmodeltothistokensequenceandcomputethepredictiononeachsquarebyapplying
thesoftmaxlayertoeachoutputtokenofthefinalblock. Allmodelsaretrainedtominimizecross-
entropylossfor100epochs.
ThenumberofblocksofbothItrSAandAKOrNissettoone. Wetestedmodelswithmorethanone
blocksbutfoundnoimprovementontheIDtestsetandadeclineinOODperformance. Similarto
the object discovery experiments, a transformer results in even worse performance than the ItrSA
model(Tab13).
The readout module is composed of the norm computation followed by the Batch Normalization
layer,ReLU,andalinearlayer.
C.3 ROBUSTNESSANDCALIBRATIONONCIFAR10
We train two types of networks: a convolution-based AKOrN and AKOrN with a combination of
convolution and attention. The former has three proposed blocks, and all the Kuramoto layer’s
connectivitiesareconvolutionalconnectivity. Thekernelsizesare9,7,and5fromshallowtodeep,
andT issetto3forallblocks. Betweenconsecutiveblocks,asingleconvolutionwithastridebeing
2isappliedtoeachofCandX.Thus,thefeatureresolutionofthefinalblock’soutputis8×8.Each
readoutmodule’sgisBatchNormalization(Ioffe&Szegedy,2015)followedbyReLU,anda3×3
convolution. C(3)isaverage-pooledfollowedbythesoftmaxlayerthatmakescategorypredictions.
Thelatternetworkisidenticaltotheformeroneexceptforthethirdblock, whichwereplacewith
the block with attentive connectivity. Different timesteps T are set across different blocks, which
are[6,4,2]fromshallowtodeep.
ForResNet-18andAKOrN,wefirstconductpre-trainingontheTiny-imagenet(Le&Yang,2015)
datasetwiththeSimCLRlossfor50epochswithbatchsizeof512. Weobservethatthispre-training
iseffectiveforAKOrNandimprovestheCIFAR10cleanaccuracycomparedtotrainingfromscratch
(from 87% to 91%). The ImageNet pretraining slightly improves ResNet’s clean accuracy (from
94.1% to 94.4%). Each model is then trained on CIFAR10 for 400 epochs. We apply augmenta-
tions, including random scaling and cropping, color jittering, and horizontal flipping, along with
AugMix (Hendrycks et al., 2020), as commonly used in robustness benchmarks. Both models are
trainedtominimizecross-entropyloss.
D ADDITIONAL EXPERIMENTAL RESULTS
D.1 POSITIONALENCODINGFORTHEATTENTIVECONNECTIVITY
We need a positional encoding (PE) for AKOrN with attentive connectivity. We found GTA-type
PE(Miyatoetal.,2024)iseffectiveandusedforAKOrN throughoutourexperiments. Comparison
toabsolutepositionalencoding(APE)(Vaswanietal.,2017)andRoPE(Suetal.,2021)isshownin
Tab9. GTAdoesnotimprovethebaselineItrSAmodels.
25CLEVRTex
PE Sudoku(OOD)
PE FG-ARI MBO
APE 34.37±5.40
APE 66.87 42.15 ItrSA
ItrSA GTA 24.32±7.81
GTA 66.07 43.41
APE 48.13±9.08
APE 71.96 51.35
AKOrN RoPE 48.43±5.60
AKOrN RoPE 65.70 50.22
GTA 51.72±3.26
GTA 75.79 54.08
(b)Sudoku(OODTest)
(a)CLEVRTex
Table 9: Coparison of positional encoding schemes. The number of blocks is one for all models.
TheSudokuresultsareobtainedwithtest-timeextensionsoftheKuramotosteps(T = 128)but
eval
withouttheenergy-basedvoting.
D.2 UNSUPERVISEDOBJECTDISCOVERY
Figure15: MBOonTetrominoes,dSprites,andCLEVR.
D.2.1 MBO
i
VS#CLUSTERS
0.300
0.50
0.48 0.295
0.46 0.290
0.44 0.285
0.42
0.280
0.40
AKOrN 0.275 AKOrN
0.38 DINO DINO
0.36 MoCoV3 0.270 MoCoV3
0.34 0.265
2 3 4 5 7 10 15 45 7 10 15 20 26
Number of clusters Number of clusters
(a)PascalVOC (b)COCO2017
Figure 16: MBO vs the number of clusters used for evaluation. AKOrN outperforms DINO and
i
MoCoV3acrossawiderangeofclusternumbers.
26
iOBM iOBMD.2.2 FULLTABLESOFOBJECTDISCOVERYPERFORMANCE
Model Tetrominoes dSprites CLEVR
FG-ARI MBO FG-ARI MBO FG-ARI MBO
ItrConv 55.56 48.82 20.46 31.25 56.41 33.98
AKOrNconv 75.37 53.88 73.35 56.80 65.08 47.03
ItrSA 86.81 51.74 69.42 64.67 80.15 36.76
AKOrNattn 86.19 55.06 79.98 65.57 90.93 43.55
(+up-tiling(×4))
AKOrNattn 92.72 56.40 88.57 62.81 93.64 43.76
(Distributedrepresentationmodels)
CAE(Lo¨weetal.,2022) 78 - 51 - 27 -
CtCAE(Stanic´ etal.,2023) 84 - 56 - 54 -
SynCx(Gopalakrishnanetal.,2024) 89 - 82 - 59 -
RotatingFeatures(Lo¨weetal.,2024a) 42 - 88.8 86.3 66.4 60.8
(Slot-basedmodel)
Slot-Attnetion(Locatelloetal.,2020) 99.5 - 91.3 - 98.8 -
Table10: Objectdiscoveryresultsonsyntheticdatasets.
Model CLEVRTex OOD CAMO
FG-ARI MBO FG-ARI MBO FG-ARI MBO
ViT 46.37 23.77 43.60 27.01 31.40 15.75
ItrSA(B =1,T =8,) 66.07 43.41 65.70 44.50 49.02 29.48
ItrSA(B =2,T =8) 75.33 48.44 73.91 45.69 60.38 36.72
AKOrNattn(B =1,T =8) 75.79 54.94 73.11 55.05 59.70 43.28
AKOrNattn(B =2,T =8) 81.50 54.08 80.15 55.02 68.73 44.98
(+up-tiling(×4))
AKOrNattn(B =2,T =8) 87.28 55.40 86.41 56.32 74.85 45.95
LargeAKOrNattn(B =2,T =8) 89.24 60.02 88.00 60.96 77.18 53.43
∗MONet(Burgessetal.,2019) 19.78 - 37.29 - 31.52 -
SLATE(Singhetal.,2022) 44.19 50.88 - - - -
∗Slot-Attetion(Locatelloetal.,2020) 62.40 - 58.45 - 57.54 -
Slot-diffusion(Wuetal.,2023) 69.66 61.94 - - - -
†SLATE+(Singhetal.,2022) 70.71 54.90 - - - -
†LSD(Jiangetal.,2023) 76.44 72.44 - - - -
Slot-diffusion+BO(Wuetal.,2023) 78.50 68.68 - - - -
∗DTI(Monnieretal.,2021) 79.90 - 73.67 - 72.90 -
∗I-SA(Changetal.,2022) 78.96 - 83.71 - 57.20 -
BO-SA(Jiaetal.,2023) 80.47 - 86.50 - 63.71 -
‡NSI(Dedhia&Jha,2024) 89.89 44.86 - - - -
ISA-TS(Bizaetal.,2023) 92.9 - 84.4 - 86.2 -
†Jungetal.(2024) 93.06 75.36 - - - -
pSauvalle&deLaFortelle(2023) 94.77 - 83.14 - 87.27 -
Table11: ObjectdiscoveryonCLEVRTex(Karazijaetal.,2021). †UseOpenimages(Kuznetsova
et al., 2020)-pretrained encoder. Numbers are from Jung et al. (2024). ‡Use ImageNet-pretrained
DINO.∗NumberstakenfromJiaetal.(2023). pUseImagenet-pretrainedbackbonemodels.
27PascalVOC COCO2017
Model MBO MBO MBO MBO
i c i c
(slot-basedmodels)
Slot-attention(Locatelloetal.,2020) 22.2 23.7 24.6 24.9
SLATE(Singhetal.,2021) 35.9 41.5 29.1 33.6
(DINO+slot-basedmodel)
DINOSAUR(Seitzeretal.,2023) 44.0 51.2 31.6 39.7
Slot-diffusion(Wuetal.,2023) 50.4 55.3 31.0 35.0
SPOT(Kakogeorgiouetal.,2024) 48.3 55.6 35.0 44.7
(transformer+SSL)
MAE(Heetal.,2022) 33.8 37.7 22.9 28.3
DINO(Caronetal.,2021) 44.3 50.0 28.8 35.8
MoCoV3(Chenetal.,2021) 47.3 53.0 28.7 36.0
AKOrNattn 50.3 58.2 30.2 38.2
(transformer+SSL+up-tiling(×4))
MAE 34.0 38.3 23.1 28.5
DINO 47.2 53.5 29.4 37.0
MoCoV3 44.6 50.5 29.0 35.9
AKOrNattn 52.0 60.3 31.3 40.3
Table12: ObjectdiscoveryonPascalVOCandCOCO2017.
D.2.3 TRAININGEPOCHSVSMBO
Fig.17showsthatMBO andMBO scoresonPascalandCOCOimproveasImageNetpretraining
i c
progresses. Similar observations are made on CLEVRTex datasets, where larger AKOrNs give
better object discovery performance (see Fig 20-22 and Tab 11). These results indicate that there
is an alignment between the SSL training with AKOrN and learning object-binding features and
that increasing parameters and computational resources can further enhance the object discovery
performance.
Figure17: MBO andMBO vs. trainingepochs. (Left)PascalVOC(Right)COCO2017.
i c
28D.3 SUDOKUSOLVING
Model ID OOD
ItrSA(B =1,T =16) 99.7±0.3 14.1±2.7
Transformer 98.6±0.3 5.2±0.3
AKOrNattnwoΩ(B =1,T =16) 99.8±0.1 16.6±2.2
AKOrNattn(B =1,T =16) 99.8±0.1 16.6±2.1
(+Testtimeextensionsofinternalsteps)
ItrSA(T =32) 95.7±8.5 34.4±5.4
eval
AKOrNattnwoΩ(T =128) 100.0±0.0 49.6±3.3
eval
AKOrNattn(T =128) 100.0±0.0 51.7±3.3
eval
(T =128,Energy-basedvoting(K =100))
eval
AKOrNattnwoΩ 100.0±0.0 46.8±9.0
AKOrNattn 100.0±0.0 61.1±14.7
SAT-Net(Wangetal.,2019) 98.3 3.2
Diffusion(Duetal.,2024) 66.1 10.3
IREM(Duetal.,2022) 93.5 24.6
RRN(Palmetal.,2018) 99.8 28.6
R-Transformer(Yangetal.,2023) 100.0 30.3
IRED(Duetal.,2024) 99.4 62.1
Table13:BoardaccuracyonSudokuPuzzles.Theharderdataset(OOD)hasfewerconditionaldigits
perexamplethanthetrainset(17-34intheharderdatasetwhile31-42inthetrainset). Weshowthe
meanandstdoftheaccuracyofmodelswithdifferentrandomseedsfortheweightinitialization.
D.3.1 EFFECTOFTHENATURALFREQUENCYTERMINENERGY-BASEDVOTING
Interestingly,themodelwithouttheΩtermdoesnotgiveimprovementwiththispost-selection,as
the energy value and correctness are inconsistent (Fig 18). This implies the asymmetric term Ω
preventstheoscillatorsfrombeingstuckinbadminima.
20 Correct 20 20 10 Wrong 10 10 10 10
0 0 0 0 0
25800 25700 25600 25800 25700 25600 25500 26800 26600 26400 24000 23900 23800 23700 24600 24400 24200
E E E E E
10 10 20 20 10
5 10 10 5
0 0 0 0 0
26200 26000 25800 25600 24400 24200 24000 23800 26200 26000 25800 24400 24300 24200 25900 25800 25700 25600
E E E E E
(a)withoutΩ
Correct 40 40 20 Wrong 20 20 20 20
0 0 0 0 0
21650 21600 21550 21500 22400 22200 22000 23000 22800 22600 22200 22100 22000 21900 22600 22400 22200 22000
E E E E E
20 50
10 10 20 25 20
0 0 0 0 0
23300 23200 23100 23000 22900 23500 23250 23000 22750 21000 20900 20800 22500 22400 22300 22200 23100 23000 22900 22800 22700
E E E E E
(b)withΩ
Figure18:EnergydistributionoftheK-NetwithorwithouttheΩterm.Ineachpanel,givenasingle
board,wecomputeenergiesofthefinaloscillatorystatesthatstartfromdifferentrandomoscillators
and show the histogram of these energies, color-coded by the correctness of the predictions made
onthecorrespondingfinaloscillatorystates. Notethatnotforallboardsdoesthemodelyieldthose
mixedpredictions: onapproximately30%boards,allpredictionswithrandominitialoscillatorsare
wrong.
29
tnuoC
tnuoCD.4 ROBUSTNESSANDCALIBRATIONONCIFAR10
↑Accuracy ↓ECE
Model Clean Adv CC CC
Gowaletal.(2020) 85.29 57.14 69.1 13.2
Gowaletal.(2021) 88.74 66.10 70.7 5.6
Bartoldsonetal.(2024) 93.68 73.71 75.9 20.5
Kireevetal.(2022) 94.75 0.00 83.9 9.0
Diffenderferetal.(2021) 96.56 0.00 89.2 4.8
ViT 91.44 0.00 81.0 9.6
ResNet-18 94.41 0.00 81.5 8.9
AKOrNconv (N =2) 88.91 ∗58.91 83.0 1.3
AKOrNmix(N =2) 91.23 ∗51.56 86.4 1.4
AKOrNmix(N =4) 93.51 ∗0.00 84.0 6.4
Table 14: (An extended version of Tab 4) Robustness to adversarial attack (Adv) and Common
Corruptions (CC) on CIFAR10 with the most severe corruption level (5). ∗The adversarial attack
isdonebyAutoAttackwithEoT(Athalyeetal.,2018). Themaxnormconstraintoftheadversrial
perturbtionsissetto8/255. WithN = 4,theperformancetendencyofAKOrN isalmostthesame
asResNetexceptfortheaccuracyanduncertaintycalibrationonCIFAR10withnaturalcorruptions,
whicharemoderatelybetterwithAKOrNmix.
Figure19: AKOrN’sadversarialexamplesareinterpretable. Eachpairofimagesisanoriginaland
theadversariallyperturbedimage(∥ϵ∥ =64/255). Thetextaboveeachimageindicatestheclass
∞
predictionmadebytheAKOrN model.
30Input ItrSA AKOrN LargeAKOrN GTmask
Figure20: VisualizationofclustersonCLEVRTex. Thenumberofblocksinallmodelsistwo.
Input ItrSA AKOrN LargeAKOrN GTmask
Figure 21: Visualization of clusters on CLEVRTex-OOD. The number of blocks in all models is
two.
31Input ItrSA AKOrN LargeAKOrN GTmask
Figure22: VisualizationofclustersonCLEVRTex-CAMO.Thenumberofblocksinallmodelsis
two.
32Input DINO MoCoV3 AKOrN GTmask
Figure23: VisualizationofclustersonPascalVOC.Thenumberofclustersissetto4.
33Input DINO MoCoV3 AKOrN GTmask
Figure24: VisualizationofclustersonCOCO2017. Thenumberofclustersissetto7.
34