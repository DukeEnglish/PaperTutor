Preprintundersubmission
RETROSPECTIVE LEARNING FROM INTERACTIONS
ZizhaoChen,MustafaOmerGul,YiweiChen,GloriaGeng,AnneWu&YoavArtzi
DepartmentofComputerScienceandCornellTech,CornellUniversity
czz,momergul,annewu,yoav @cs.cornell.edu yc833,gcg46 @cornell.edu
{ } { }
ABSTRACT
Multi-turninteractionsbetweenlargelanguagemodels(LLMs)andusersnaturally
includeimplicitfeedbacksignals. IfanLLMrespondsinanunexpectedwayto
aninstruction,theuserislikelytosignalitbyrephrasingtherequest,expressing
frustration,orpivotingtoanalternativetask. Suchsignalsaretask-independentand
occupyarelativelyconstrainedsubspaceoflanguage,allowingtheLLMtoidentify
them even if it fails on the actual task. This creates an avenue for continually
learningfrominteractionswithoutadditionalannotations. WeintroduceRESPECT,
amethodtolearnfromsuchsignalsinpastinteractionsviaretrospection.Wedeploy
RESPECTinanewmultimodalinteractionscenario,wherehumansinstructanLLM
tosolveanabstractreasoningtaskwithacombinatorialsolutionspace. Through
thousandsofinteractionswithhumans,weshowhowRESPECTgraduallyimproves
taskcompletionratefrom31%to82%,allwithoutanyexternalannotation.
1 INTRODUCTION
Languagemodels(LMs)oftenengageinmulti-turninteractionswithhumanusers. Similartohuman-
humaninteractions,theseinteractionsarenaturallyrichwithimplicitlearningsignals. IftheLMfails
torespondappropriately,theuserislikelytofollowwithanexpressionoffrustration,arephraseof
theirintent,ormaybeevencompletelypivotwhattheyaskfor. Similarly,iftheLMdoeswell,the
usermayexpressapprovalorsimplycontinuetotheirnextobjective. Suchresponsescaninformthe
LMofitsperformance,therebycreatinganopportunitytolearnthroughretrospection.
Westudytheefficacyofsuchsignals,andhowtheycanleadtoasystemthatimprovesovertime.
WeintroduceRESPECT,asimpleapproachtolearnfromsignalsthemodelitselfderivesaboutits
ownpastactionsthroughretrospectionofpastinteractionswithhumanusers. WedeployRESPECT
in MULTIREF,anewmulti-turngroundedinteractionscenario,whichrequiresmodelstodisplay
θ ρ+1←θ
ρ
Deploy π θρ(a|x) Retrospect Train on D
≤ρ
……
Was my response good?
Could you … ……
Could you …
Next …
Interactions
……
Figure1: LearningviaRESPECT. WedeployanLLMpolicyπ θρ(a |x)inroundsρ,tointeractwith
usersinmulti-turninteractions. Followingeachround,theLLMreasonsretrospectivelyabouteachof
itsactions(highlightedinblue)todecodefeedbackgiventheinteractioncontext,includingfollowup
utterances. Aftereachround,themodelisretrainedusingalldataaggregatedsofarD . TheLLM
≤ρ
improvesovertimewithoutanyexternalannotations. Theplotontherightshowstheperformance
curveinourexperiments–theLLMimprovesfrom31%to82%taskcompletionrateoversixrounds.
1
4202
tcO
71
]LC.sc[
1v25831.0142:viXra
ecnamrofrePPreprintundersubmission
complexabstractreasoning,andhumanstograduallyinstructmodelstoaccomplishsequencesof
goalstocompletetheirtasks.
The key insight underlying RESPECT is that conversational implicit feedback signals occupy a
relativelyconstrainedsubspaceofnaturallanguage. Suchsignalscanincludedirectapprovals(e.g.,
great!) orsignsoffrustration(e.g.,notagain),andalsomoresubtlecues,suchaswhentheuser
rephrasestheirrequest.Critically,itisrelativelysimpletodisentanglethemfromtaskperformance. A
humancaneasilyfigureoutfromsuchcuesiftheydowellornot,eveniftheyhavelittleunderstanding
aboutwhattheyareaskedfor. Itisthisconstrainednaturethatmakesreasoningaboutsuchsignalsto
bewithinthecapacitiesoflargelanguagemodels(LLMs),eveniftheyfailatthetaskathand.
RESPECTutilizesthissignalinaprocesswherethemodelinteractswithhumans,andafterinteraction
decodesfeedbackforeachofitsactionsfromtheinteractioncontextincludingthefollowuputterances.
Figure1illustratesthisprocess. Themodelinteractswithhumanstoaccomplishtasks,retrospectively
examinesitsownpastinteractions,andthenre-trains. Thisprocessprogressesinrounds,alternating
betweeninteractionandtraining,withthemodelimprovingovertime. Critically,unlikecommon
recipesfortrainingfromhumanfeedback,RESPECTdoesnotrequireanyexternalannotation(Ouyang
etal.,2022,RLHF)orevensolicitingfeedbackfromtheusersthemselves(Suhr&Artzi,2023).
WedeployRESPECTinMULTIREFovermultipleroundsofgroundedinteractionswithhumanuseand
re-training.WeuseIDEFICS2-8B(Laurenc¸onetal.,2024)asourLLM,andexperimentwithmultiple
learningmethods,includingsupervisedlearning,REINFORCE-stylepolicygradient(Williams,1992;
Kojima et al., 2021), and KTO (Ethayarajh et al., 2024). Across our experiments, we observe
thatIDEFICS2-8Beffectivelydecodesfeedback, evenasitinitiallyperformspoorlyinthesame
interactions. Inourlongestrunningexperiment,weobservemodeltaskcompletionrateimproves
from31%to82%. Ourcode,data,andmodelsareathttps://lil-lab.github.io/respect.
2 TECHNICAL OVERVIEW AND NOTATION
WeconductcontinuallearningstudiesbydeployingourapproachinMULTIREF,anewmulti-turn
groundedinteractionscenario(Section3). Overall,thestudyprogressesinrounds,wheretheLLM
policyisfirstdeployedtointeractwithusersandcompletetasks,andtheinteractionsarethenusedto
re-trainthepolicy. Ourstudyinvolvesmultiplerounds,andourgoalistoobserveandevaluatethe
long-termdynamicsoftheprocess. Thisincludestherobustnessofourawarddecodingandtraining
methodstothechangingdistributionofthedatalikelytobeseeninanadaptivesysteminthewild.
Section3describesourinteractionscenarioindetail,andSection4ourlearningmethod. First,we
outlineourproblemofinterestanditsnotationinabstractterms.
TaskNotation Thepolicy’staskistorespondeffectivelytohumanutterancesgiveninconversa-
tionalcontext. Formally, letπ(a x )bethepolicythatcontrolsthelistenerbehavior, witha an
t t t
|
action string that represents the model response and x being the context on which the policy is
t
conditioned,bothattimetintheinteraction. Thecontextincludestheinstructionhistoryuptoand
excludingtimet,includingcurrent(i.e.,attimet 1)andpastspeakerutterances,aswellasany
−
otherrelevantcontextinwhichtheinteractiontakesplace. Asourlearningprogressesinrounds,we
denoteθ asthemodelparametersinroundρ,andπ astheparameterizedpolicy.
ρ θρ
Learning and Deployment We study a continual learning setup, where the learning signal is
acquiredfrominteractionsofthedeployedmodelwithhumanspeakers. Ourstudyprogressesin
rounds(Figure1). Eachroundρincludesadeployment,followedbytraining. Duringdeployment
atroundρ,themodelπ interactswithusers. Foreachmodelactionaˆ π (ax ),werecorda
tuple(x ,aˆ ,p
,f¯),whθ eρ
rex
isthecontextgiventothemodelattimet t∼ topθ rρ edic| tt
actionaˆ ,p is
t t t t t t t
theprobabilityofaˆ atthetimeofprediction,andf¯ istheremainderoftheinteractionfollowingaˆ .
t t t
Critically,theseinteractiontuplescontainnoexplicitfeedback. Wecomputetheimplicitfeedbackγˆ
t
usingafeedbackdecoderϕ(x ,aˆ ,f¯),toobtaintuples(x ,aˆ ,γˆ ,p ). Weexperimentwithmultiple
t t t t t t t
learningobjectivesusingthisfeedback: supervisedlearning(SFT),policygradient,andKTO.
Evaluation Wemeasurethequalityofthelistenermodelπ (a x )ateachroundρprimarilyby
θρ t
|
t
interactionsuccessratesfromlivehuman-botdeployments. Thesameinteractionsareusedtotrain
themodelforthenextround. Wetrackvariouscharacteristicsofmodelbehavior,suchasnumber
2Preprintundersubmission
A B C D Targets: A E G I
A diamond head man without legs
Select I
E F G H
Another diamond head man
Select C
I J No find another one with diamond head
and triangle body and triangle leg Deselect C Select G
……
Figure2: Theinteractionscenarioweuseinourexperiments. MULTIREFisamulti-turnreference
game. Aspeakerandalistenerbothobserveasharedsetoftangramshapes,butindifferentorder.
Thegoalofthespeakeristodescribeasubsetoftargetsforthelistenertoselect. Becausethetarget
requiresmultipleabstractshapes,humansoftencommunicatethetargetsgraduallyovermultiple
turns. Asaninteractionprogressesnaturally,thespeakerproducesimplicitfeedbacksignalsthat
validateorrejectthelistener’sactions.
ofturnsperinteractionasanefficiencymeasure. Wealsodopost-hocannotationofasubsetofthe
interactionstomeasureutterance-levelpolicysuccessrateandfeedbackdecoderaccuracy.
3 MULTIREF: A MULTI-TURN GROUNDED INTERACTION SCENARIO
Keytoourstudyisthattasksarerelayedgraduallyacrossmultipleturns,ascommonlyhappensin
humaninteractions. WecreateMULTIREF,aconversationalinteractionscenariowheretwopartners,
aspeakerandalistener,coordinateontheselectionofasetofitems. Inourstudies,thespeakeris
alwaysahuman,andthelistenerisamodel.
MULTIREFgeneralizesthecommonlystudiedreferencegamescenario. Itsdesignandourchoiceof
stimuliaregroundedinexistingworkfrombothcognitivescienceandcomputationallanguagemod-
eling(Rosenberg&Cohen,1964;Clark&Wilkes-Gibbs,1986;Schober&Clark,1989;Goodman&
Frank,2016). Figure2illustratesthescenario. Bothpartnersobserveasharedsetofimages,butin
differentorder. Thespeakerisgivenasubsetoftheimagesastargets,withthegoalofcommunicating
thetargetstothelistener,sothelatterselectstheexactsubset. Onlythespeakercanwritemessages,
andonlythelistenercanselectordeselectimages. Theinteractionconcludessuccessfullyonceall
andonlytargetsareselected,orfailsifthepartnersrunoutofturns,20inourstudies.
Theinteractionprogressesinturnst,alternatingbetweenspeakerandlistenerturns. Ateachspeaker
turn,theyprovideasingleunrestrictednaturallanguageutterance. Itmaydirectthelistenertoselect
oneormoreitems,asktodeselectitemsifthelistenerpreviouslymadeamistake,orincludewhatever
othercontenttheydesire. Thisutteranceaswellasthehistoryoftheinteraction,thesetofimages,
andtheirselectionstatuscomposethecontextx forthefollowingmodelturnattimet. Thefollower
t
respondswithanactiona ,whichincludesoneormoreimageselectsordeselectsaccordingtotheir
t
understandingofthespeakerintention. Theactionspaceconsistsofallpossiblelegalsequencesof
theformDeselect E select ForSelect D Gassumingimagesarecode-namedalphabetically.
Themotivationbehindthedesignof MULTIREF istocreateatask-orientedscenariothatisboth
accessibletonon-experthumansandencouragesconstructingasolutioninmultipleturns,thereby
creatingmulti-turninteractionsandelicitingthelearningsignalsweaimtostudy. Therulesofthe
interactionaresimple: thespeakerneedstodescribetargetstoselect,andthelistenerhastoselect
whatthespeakerisreferringto. ThismakesMULTIREFeasilyaccessibletocrowdsourcingworkers.
Atthesametime,thesolutionthespeakercommunicatestothelistenerisrelativelycomplex,because
of the enormous solution space: Consider a conventional reference games, where the goal is to
select a single image. The number of possible solutions is the number of images in the context.
InMULTIREF,thegoalistoselectasubsetofunknownsize,sothecombinatorialsolutionspace
thelistenerfacesisexponentialinthenumberofimages. Atthesametime, thesolutionitselfis
structured,creatingeasyopportunitiestodecomposeittomultiplesteps.
3Preprintundersubmission
Key to making MULTIREF work well is the choice of images. We use tangram shapes from the
diverse KILOGRAM dataset (Ji et al., 2022). Tangrams are abstract shapes that are designed to
elicitcommonconceptsinhumans. Thisabstractnessoftenleadstoambiguousdescriptionsopen
tointerpretation. Forexample, ShapeAinFigure2canbedescribedasabat, alowercasew, or
evenastarwarsstarfighter. Weselecttangramsbecausetheynaturallyprovideanambiguousand
challengingstimuliforhumaninteraction(Clark&Wilkes-Gibbs,1986;Schober&Clark,1989;
FoxTree,1999;Hawkinsetal.,2020b),therebyleadingtohighlydiverselanguage. Theyalsoremain
challengingforcontemporarymultimodalLLMs(MLLMs)toreasonabout(Jietal.,2022),leaving
significantroomforlearning.
Thefree-formnaturallanguagehumanspeakersproduceinMULTIREFisverydiverse,andbalances
betweencompetingpressures. First,itoftenrequirescomplexpragmaticreasoning(Clark&Wilkes-
Gibbs,1986;Schober&Clark,1989;Horton&Gerrig,2002),becauseoftheabstractnessoftangrams.
Thisiscompoundedbyhowthecombinatorialsolutionspacedriveshumanstobalancebetween
relayingasmuchinformationaspossible,andrelayingclearobjectivestomakegradualprogress.
ThisisabalancebetweentwoGriceanmaxims: quantityandmanner.1 Speakersmayormaynot
includeexplicitfeedbacksuchasgood,ordeselectthelastone;thespeakermaydescribemorethan
onetargetinasingleutterance,forexample,selecttwomen;speakersmayrefertopreviousselections
withoutdirectlydescribingtargets,forexample,theotherone,ortryagain. Incombinationwiththe
abstractstimulitangramsprovide,thiscreatesachallengingreasoningproblemforthelistenermodel.
MULTIREFisnotdesignedtoincreasecomplexityinarbitraryways,buttoprovideanenvironment
forhumanstonaturallyexposecoreaspectsofhumancommunication. Atthesametime,thescenario
isbothcontrolledandscoped,allowingforeasymeasurementoftaskcompletionandprogress,as
wellasmakinglearningfeasiblewithrelativelylimiteddata. ThismakesMULTIREFparticularly
suitabletoresearchinacademiaorotherlow-resourcesettings.
4 RESPECT: RETROSPECTIVE LEARNING FROM PAST INTERACTIONS
RESPECThastwocomponents:decodingimplicitfeedbackfrompastinteractions(retrospection)and
learningfromthedecodedfeedbacksignals(learning). WedeployRESPECTinaniterativecontinual
learningscenario,whereeachroundincludesbothsteps. Thisdeploymentallowsustoobservethe
dynamicsofRESPECTovertime. However,themethoditselfisnotlimitedtocontinuallearning,and
canbeappliedasingletimeaswell.
The goal of RESPECT is to re-estimate the parameters of a model given interactions that were
collectedbythemodelitself,orpreviousversionsofit. WeassumeaccesstoarawdatasetDraw =
(x(i),aˆ(i),p(i),f¯(i)) N ,wherex(i) isthepolicycontext,aˆ(i) isthepredictedaction,p(i) isthe
{ probabilityofthisacti} oni= ,1 andf¯(i)istheremainderoftheinteractionfollowingaˆ(i).2
Inourcontinual
learningsetup,Drawisaunionofalldatacollectedfrompastrounds.
Thefeedbackdecoderϕcomputesacategoricalfeedbackγˆ(i) positive,neutral,negative
foreachactionaˆ(i)holisticallybaseditscontextx(i),actiontak∈ en{ aˆ(i),followuputterancesf¯(i).}
3
ThisprocesstransformsDrawtoD = (x(i),aˆ(i),p(i),γˆ(i)) N . Weusethisdatasetfortraining.
{ }i=1
4.1 DECODINGIMPLICITFEEDBACKTHROUGHRETROSPECTION
Weimplementthefeedbackdecoderϕbypromptingthemodeltoanalyzepastinteractiontuples
(x,aˆ,p,f¯)tocomputefeedbackγˆ =ϕ(x,aˆ,f¯).
Thegoalisaprocesswherethemodelbootstraps
fromitsowninteractions. OurhypothesisisthatLLMshavetheabilitytoreasonabouttherelatively
constrained space of implicit signals, even if they fail at the task. We show this empirically in
our experiments. Critically, this process does not rely on a stronger LLM for critique or on past
interactionscreatedbyotherLLMs. Figure3showsthedecoderprompt. Weexperimentwithbinary
1Grice’smaximofquantity:onetriestobeasinformativeasonepossiblycan,andgivesasmuchinformation
asisneeded,andnomore;Grice’smaximofmanner:onetriestobeasclear,asbrief,andasorderlyasonecan
inwhatonesays,andwhereoneavoidsobscurityandambiguity.(Grice,1975)
2Forsimplicityofnotation,weomitthetimestepinthissection.
3Wedonotcomputefeedbackforthelastactionineachinteractionbecausethereisnotfollowupinteraction.
Forsimplicity,Drawdoesnotincludethem.
4Preprintundersubmission
FeedbackDecoderPrompt
User:Pleasecarefullyreadthefollowingconversationandanswer:Istheverylastutterance
fromthespeakerpositiveornegativepositive,neutral,ornegativefeedback?Oftennegative
feedbackincludecorrectionsandkeywordslikeno,not,undo,don’t,withgenerallynegative
sentiment,whilepositivefeedbackoftenincludesgood,yes,correct,okay,orsimplymove
ontothenextstage.Leantowardsnegativeifitsoundsneutral.
(startoftheconversation)
Listener:DeselectFselectG
Speaker:yes,pickthethinpersonwithatrianglehead
Listener:SelectA ......................................(Actiontofocuson)
Speaker:yes,pickthehousewithchimney .......................(Feedback)
(endoftheconversation)
Answerasingleword,Positive,orNegativePositive,NeutralorNegative.
Assistant:Positive
Figure 3: The prompt used to decode feedback from past interactions. The figure combines the
promptsforbothbinaryandternaryfeedbackdecoding. Thepartsthatbelongtothebinarycaseonly
arecoloredgreen,whilepartsthatbelongtheternarycasearecoloredorange. Theverbalfeedback
generatedbythemodelisinbold. Additionalcommentsforreadabilityareinmagentaitalics.
orternaryfeedback. Ternaryaddsneutralontopofthepositiveandnegativebinaryoptions.
Thefeedbackdecoderisdesignedtoidentifygenerallinguisticcues,andnotforthespecifictaskwe
study. Weassumenoaccesstoanyauxiliaryannotationorprivilegedinformation(e.g.,notinferring
basedonwhetherthepolicyselectsagroundtruthtargetinaturn,orwhetheranentireinteraction
endsearly),althoughtheyarelikelytobeusefulsignalsasexploredinPangetal.(2023).
4.2 LEARNING
ThefeedbackdecodingprocesstransformersthedatasetfromDrawtoD = (x(i),aˆ(i),p(i),γˆ(i)) N .
{ }i=1
We study several learning approaches using this data: supervised learning, offline reinforcement
learning(RL),ortheKTO-styleutilitymaximization(Ethayarajhetal.,2024).
SupervisedLearning Wefine-tuneonpositivedatapoints(γˆ(i) = positive)anddiscarddata
pointspredictedasneutralornegative. Weusecrossentropylosswithadditionallabelsmoothing
topreventoverfittingandencourageexploration. Oursetupisdistinctfromconventionalsupervised
learninginthatthedataiscomingfromthemodelinteractions(i.e.,on-policy),andnotfromagiven
dataset. Also,werunthelearningprocessiteratively,eachtimewithmoredata. Wedonotdesignthe
supervisedapproachinanyspecialwaytofitthesechanges,butthisisapotentialavenueforfuture
work,whichcanfurtherimproveperformance.
ReinforcementLearning Wefollowpriorwork(Kojimaetal.,2021)andusesimpleREINFORCE-
stylepolicygradient(Williams,1992). Thecategoricalfeedbackγ(i)(i.e.,thetextgeneratedbythe
promptedLLM)ismappedtoanumericalvaluewithasimplerewardfunction:

1, γ =positive

R(γ)= 0, γ =neutral . (1)
 0.1, γ =negative
−
Droppingthei-superscriptsforsimplicity,thegradientestimatorforasingleexampleis:
(cid:40)
1, ifR(γˆ) 0
∆=cR(γˆ) logP(aˆx;θ ) c= ≥ (2)
∇ | ρ+1 P(aˆ|x;θρ+1), ifR(γˆ)<0 ,
p
where the coefficient c downweights examples with negative reward by their inverse propensity
score(Kojimaetal.,2021). Thisiscriticalbecauselim logP()= . Inpractice,wealso
P(·)→0
· −∞
discarddatapointswithpredictedneutralfeedback(R(γˆ)=0).
5Preprintundersubmission
WechooseREINFORCEforitssimplicity. Thepositivecasereducestobemathematicallyequivalent
tothegradientofsupervisedfine-tuning(SFT),whoseoptimizationisrelativelywellunderstood.
Asopposedtoothermethods,suchasPPO(Schulmanetal.,2017),REINFORCEdoesnotrequire
a reward model and has relatively few hyperparameters. This is critical with human-in-the-loop
experiments,wherebroadparametersweepsarenotpossible. Recentwork(Ahmadianetal.,2024)
alsosuggestsREINFORCEcanproduceon-parresultsinLLMswithPPOdespiteitssimplicity.
UtilityMaximization Toexperimentwithutilitymaximization,weuseKahneman-TverskyOp-
timization(Ethayarajhetal.,2024). KTOwasdevelopedtolearnfromper-examplebinaryhuman
feedback,ascenariothatfitsourswell. Weconsiderexampleswithdecodedpositivefeedback
asdesiredutterances,thosewithdecodednegativefeedbackasundesired,anddiscardthosewith
neutralfeedback. WereferreaderstoEthayarajhetal.(2024)forthedefinitionoftheobjective.
5 EXPERIMENTAL SETUP
InteractionInstantiation WeusetheKILOGRAM(Jietal.,2022)tangramimages,followingGul
&Artzi(2024). KILOGRAMcontains1,013images. Werandomlysplitthemintoamainsplit(912
tangrams) and a development split (101 tangrams). We create interaction contexts by randomly
sampling10tangrams,andrandomlyselect3–5astargets. Thedevelopmentsplitisexclusivelyused
forseedingtheinitiallistenerpolicyπ ,andallhuman-botinteractionsareconductedonimages
θ0
fromthemainsplit,i.e.,tangramsthattheseedpolicyπ hasneverseenbefore.
θ0
ModelandInitialization WeuseIDEFICS2-8B(Laurenc¸onetal.,2024)asourmodelforboththe
policyandfeedbackdecoder.Wefine-tunewithLoRA(Huetal.,2022).Weseedtheinitialpolicyπ
θ0
byfine-tuningthepretrainedIDEFICS2-8Bweightsonasmallsuperviseddatasetof90successful
turnsfrom25human-humangamesconstructedwiththedevelopmentsplittangrams,augmentedwith
12syntheticallygenerateddeselectionturns,becausewhilenecessaryforhuman-modelinteractions,
deselectionsarerareinhuman-humaninteractions(AppendixB.2). D isreusedincontinualtraining
0
viarehearsal.Wevalidateourdesignonlinewith30main-splithuman-botpilotinteractions,oroffline
withavalidationsetof344successfulmain-splithuman-humanturns(AppendixA). Weusethe
originalIDEFICS2-8Bforfeedbackdecoding,becausethenarrowfocusofourdataislikelytoinhibit
somegenerallinguisticknowledge. Thismeanswecannotseeimprovementinthemodelfeedback
decodingcapability,likelylow-ballingthepotentialoftheapproach.Itremainsanimportantdirection
forfutureworktokeepthedecodermodelinsyncwiththepolicy. Thisrequiresdeploymentsthat
includehighdomaindiversity. WeobservetheoriginalIDEFICS2-8Btoproviderobustfeedback
decodingoutofthebox,confirmingourhypothesis,andprovidingasolidgroundforourexperiments.
SystemVariants Westudysixsystemvariantsbasedontwodimensions: (a)feedbackdecoder
configuration(binaryvs. ternary);(b)optimizationmethods(supervisedvs. REINFORCEvs. KTO):
• B-SUP and T-SUP binary (B) / ternary (T) that only trains on positive data points with a
supervisedfine-tuningobjective(SUP).
• B-RLandT-RLtrainsonbothpositiveandnegativedatapointsusingREINFORCE.
• B-KTOandT-KTOarelikeB-RLandT-RL,butusingKTO.
Forvariantsinvolvingnegativedatapoints(B-RL,T-RL,B-KTO,andT-KTO),wesubsamplenegative
onestokeepthepositive:negativeratiocloseto5:4 (Ethayarajhetal.,2024).
Deployment Weconductthreeroundsoftraining-deploymentforallsixsystemsandthreemore
rounds for B-SUP. We select B-SUP for another three rounds because it is the most promising
variant after three rounds, and we want to observe its progress over a longer period. The reason
forthiscascadeddesignisthehighcostofexperiments. Wedonotdistinguishbetweentraining
andevaluationinthetraditionalsense. Instead,alllistenerpoliciesareevaluatedliveonMTurkon
about330human-botinteractionseachroundcontainingroughly2400turns. Thenthesamedatais
usedtotrainthenextiterationofpoliciesrespectively. Thepoliciesinthesameroundaredeployed
concurrentlyinarandomizedexperimentonthesamesetofgamestomitigatehumanbiasesand
variancesduetogamedifficulty. MoredetailsoncrowdsourcingareinAppendixA.3.
6Preprintundersubmission
Learning Implementation Details We use the validation set for model selection throughout
continuallearning. Followingpriorwork(Misraetal.,2017;Mu¨lleretal.,2019;Liuetal.,2022),we
addanentropytermandlengthnormalizationtoallthreeobjectivestoreduceover-fittinggiventhe
relativelysmallamountofdata. AppendixBprovidesadditionalreproducibilitydetails. Unlikewith
REINFORCE,wherewetrainfromscratcheachround,whenusingKTO,wecontinuallyfine-tune
fromapreviousmodelcheckpointθ toobtainθ withdataaccumulation. Thiswasshownto
ρ ρ+1
outperformtrainingfromscratchinpilotstudies.
Evaluation We evaluate each system variant at each round by the success rate during the live
deployment. We report both interaction- and utterance-level success rates. The interaction level
successrateisstraightforward-whetherthegameendedwithalltargetsselectedbythelistenerand
nothingelse. Theutterancelevelsuccessrateismorenuancedbecausewedonothaveaccesstothe
groundtruth,i.e.,theintendedaction. Wesample1,000utterancesperroundfromB-SUPtoannotate
byMTurkworkersposthoc. Wereporttwomeasures: exactmatchbetweentheannotationandmodel
actionandsimilarityscore,whichisbasedonthecomputedsimilaritybetweenthetangramsselected
ordeselectedduringtheturnbythehumanannotatorandthesystem. Wealsoevaluatethequalityof
thefeedbackdecoderbycomparingitspredictionswithhumaninterpretationscollectedduringthe
post-hocannotation. Becauseofcost,wecannotdopost-hocannotationforallsystemvariants,so
wealsoreportclickaccuracy,whichapproximatesutterance-levelperformancewell. Itmeasures
theratioofthemodelactionsthatleadtoselectionstatusesthatdonotviolatethesetoftargets(i.e.,
selectionsoftargettangramsaregood,deselectionofnon-targettangramsaregood). Lastly,wetrack
thenumberofturnsperinteraction. AppendixB.4providesfulldefinitionsofourmetrics.
6 RESULTS AND ANALYSIS
Wedeployourmodelsforthreerounds,withadditionalthreeroundsforB-SUP,thebest-performing
variant,tobetterunderstandlong-termdynamics. Allourresultsarefromconcurrentrandomized
deployment, where the models interact with humans in real time. We collected a total of 7,230
interactionsconsistingof55,004utterancesover4weeks,atacostof$11,180USD.
Figure4showsthedeploymentstatisticsforallsixsystemvariants,aswellascontroldeployments
fortheinitialpolicyandhuman-humangames.4 Figure5showsutterance-levelstatisticsforB-SUP
fromthepost-hocannotationswecollected. Theinteractionsuccessrateofallsystemsimproves
monotonicallyinthefirstthreerounds,exceptfor B-KTO inroundthree. Weconductthreemore
roundswithB-SUP,theleadingsystemafterthefirstthreerounds. B-SUPthenplateaus,andeven
showsatemporarydecreaseinperformance,beforeresumingitsimprovement.5
Overall, B-SUP improvesinteraction-levelsuccessrateby51%(31% 82%)andutterance-level
→
exactmatchby22%(31% 53%). Atthelastround,followingtheplateau,B-SUPinteractionsuccess
→
rateimprovesby5%(77% 82%). Thenumberofturnsfollowsthesetrends. Asthepolicygets
→
better,moregamesarecompletedwithintheallottednumberofturns,andevenfaster. B-SUPstarts
with8.9turnspergame,andconcludeswith6.7pergame. ThecenterpanelofFigure5showsthat
actionstakenbythepolicyincreasinglyresemblehumanactions,evenmistakes(actionsthatreceive
negativefeedback)becomemoresimilartohumanactions. Allotherstatisticslargelytrackthese,
exceptsomeoftheutterance-levelstatisticsaroundwhenB-SUPplateaus. Whileallshowadeviation
fromthemonotonousearliertrend,someshowatemporarydecreaseandnotjustastagnation,but
delayedbyoneround. Thisillustratesthecomplexdynamicsofcontinuallearning,whichweexplore
inmoredetailbelow.
ThereremainsasignificantgapbetweenB-SUP(ourleadingsystem)andHH(human-humaninterac-
tions),whichshowsperfecttasksuccessrateandalmostdoubleefficiency(i.e.,tasksarecompleted
4Wepresentresultsinroundsforsimplicity.AppendixCconnectsroundstocumulativenumberofinterac-
tions.
5Thereasonsbehindtheplateauarehardtoinfer.Onehypothesisweconsideredisthatchangesintheamount
ofdataovertimemadesomesettingssub-optimal.Specifically,weconsideredourLoRAadaptersettings,as
theyimpacttheexpressivenessoffine-tuning.Weconductedaseparateddeployment,branchingoutfromround
threefortworounds(fourandfive)withB-SUPandmoreexpressiveadapters. Weobservedthisincreasein
expressivityallowsthemodeltocontinueitsmonotonousimprovement.AppendixDprovidesthedetails.This
miniexperimentillustratesthecomplexitiesofcontinuallearningwithcurrentlearningsystems.
7Preprintundersubmission
B-SUP B-RL B-KTO CONTROL
T-SUP T-RL T-KTO HH
100%
90%
8
80%
80%
60%
6 70%
40%
60%
4
0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6
Round Round Round
Figure4: Taskperformanceandefficiencyimproveasthepolicylearnsfrommorepastinteractions.
Wepresentdeploymentresultsacrossthreeroundsforsixconcurrentsystems,andthreemorerounds
forthetopsystemB-SUP,togetherwithhuman-humanreferences(HH)andaredeploymentofthe
initial policy π
θ0
(CONTROL). Left: interaction-level success rate ( ↑, higher is better). Center:
interaction-levelefficiencyby#turnsperinteractions( ). Right: micro-levelperformancebyclick
↓
accuracy( ). Shadesare95%confidenceintervalsbybootstrappingwith10,000resamples.
↑
B-SUP CONTROL NEGFB
0.25 50% 50%
40% 40%
30% 0.00 30%
0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6
Round Round Round
Figure5: Turn-levelperformanceofB-SUPevaluatedbypost-hochumanannotations. Left: %turns
wherethepolicy’sactionaˆmatchesexactlythehumanlistener’sactiona∗ ( ). Center: similarity
↑
betweenthepolicy’sactionandthehumanlistener’saction( ). Evenactionsthatreceivenegative
↑
feedbackindeployment(NEG FB)areincreasinglysimilartohumanactions. Right: %turnsthat
annotatedtohavereceivedpositiveimplicitfeedbackfromhumanlisteners( ).
↑
withmuchshorterinteractions). Ourintuitionisthatthegapisduetothelackoflong-termcreditas-
signmentinourlearningmethod. Thisisespeciallyinfluentialinlearningtoreasonsaboutlaterturns.
Laterturnsshowsmuchstrongerdependenceonearlierturns,creatingamorecomplexreasoning
problemandahardercreditassignmentproblemwiththelongercontext. Thislearningchallenge
iscompoundedbythedatascarcity: wehavesignificantlylessdataforlaterturns,becausenotall
interactionsincludethem. Thiscanpotentiallybeaddressedbynotincludingallpastturnsinthe
context(i.e.,slidingwindowapproach).
UserAdaptation Apotentialconfounderisuseradaptation:theimprovementininteractionsuccess
ratecouldhavebeenattributedtousersadaptingtotheinteractionscenarioandthesystem,insteadof
policyimprovement(Hawkinsetal.,2020a). Weredeploytheinitialpolicyπ concurrentlytofinal
θ0
B-SUProundtotestthis(CONTROLinFigure4). TheinteractionsuccessrateofCONTROLremains
relativelystableovertime(31% 33%),suggestingthatspeakerfamiliarityandadaptationdonot
→
explaintheoverall51%absoluteimprovementinB-SUPinteractionsuccessrate.
Positive Only vs. All Data The difference between systems that use positive learning signals
only(B-SUP, T-SUP)andthesethatuseall(B-RL, T-RL, B-KTO, T-KTO)isinlearningobjectives
(supervisedvs. RL/KTO)Overall,thesystemsbasedonpositivesignalsonlyperformbetter. Itis
expectedthatpositivesignalswillbemoreinformativeforlearning. Ourpolicyactsinalargeaction
space. Negativerewardssuppressaspecificaction,butwithoutmoreinformationaboutwhatisa
goodaction,theysimplyencourageauniformdistribution. Thishasbeenshowntohaveahelpful
regularizingeffectinpastwork(Kojimaetal.,2021). However,notonlynegativefeedbackdoesnot
helpinameaningfulway,itseemstoconfusethelearner. Thepositive-onlysystemsthat,ineffect,
8
etarsseccusnoitcaretnI
hctamtcaxE )∗a,aˆ(miS
noitcaretnirepsnrut#
ycaruccakcilC
kcabdeefsoPPreprintundersubmission
Pred
neg neu pos
0.46 - 0.02 0.46 - 0.01 0.44 - 0.02 0.43 - 0.01 0.40 - 0.01 0.41 - 0.01 0.38 - 0.01
0.12 - 0.07 0.08 - 0.06 0.06 - 0.05 0.04 - 0.05 0.05 - 0.05 0.05 - 0.03 0.06 - 0.04
0.16 - 0.17 0.15 - 0.24 0.14 - 0.29 0.15 - 0.32 0.15 - 0.34 0.17 - 0.33 0.16 - 0.34
0.38 0.09 0.01 0.42 0.04 0.01 0.41 0.04 0.01 0.39 0.04 0.00 0.33 0.07 0.01 0.35 0.05 0.01 0.34 0.05 0.01
0.07 0.08 0.04 0.05 0.05 0.04 0.04 0.03 0.04 0.03 0.03 0.03 0.03 0.04 0.04 0.03 0.03 0.03 0.03 0.04 0.03
0.07 0.12 0.14 0.07 0.12 0.20 0.06 0.13 0.24 0.06 0.15 0.27 0.04 0.17 0.28 0.03 0.20 0.26 0.04 0.19 0.27
ρ=0 ρ=1 ρ=2 ρ=3 ρ=4 ρ=5 ρ=6
Figure6: Confusionmatricesofthebinary(toprow)andternary(bottomrow)feedbackdecoders
overrounds. Thefeedbackdecoderyieldsprecisepositivesignals,eveninearlyrounds.
haveaccesstofewerlearningsignalsperformbetter. Utilizingnegativesignalsbetterisanimportant
directionforfuturework.
Feedback Decoder Evaluation We evaluate the quality of the feedback decoder through our
annotationtask. Foreachturn,workersannotateifthespeakerwassatisfiedwiththeanswergiven
theirfollowuputterances. Figure6showsfeedbackdecodingconfusionmatricesovertime. The
feedback decoder performance is relatively stable throughout the rounds, showing robustness to
changesinthedatadistribution. Ifwecollapsetogetheractualpositivesandneutrals,weobserve
above90%precisionconsistently. Theternaryfeedbackdecoderismoreconservativecomparedto
thebinaryoneandlabelsmorepositiveturnsasneutrals. Thisisatask-dependenttrade-off. Thezero
feedbackofneutralsessentiallyeliminatestheexamples,butallowsforslightlycleanerdata. Here
weempiricallyobserveitisbeneficialtohaveslightlynoisydatabutmoreofit.
Supervised vs. REINFORCE vs. KTO Overall, the supervised variants (B-SUP and T-SUP)
performbest. TheKTOvariants(B-KTO and T-KTO)trailaftertheREINFORCEvariants(B-RL,
T-RL). B-KTO evendivergesatsomepointandstartslosingperformancefast. Wesuspectthisis
becausetheKTOrecipedoesnotworkwellinthechallengingoptimizationscenarioofcontinual
learning,wherethemodelisfine-tunedmultipletimes. WeobservethatB-KTOdeterioratesinrounds
twoandthree,andstartsgeneratingillegaloutputs(e.g.,Deselect select). AppendixB.3describes
aquickinterventionweappliedtotrytomitigatethisissue. Althoughiteliminatedtheillegaloutputs,
thequalityremainedlow. ItispossiblethatfurtherrefinementofhowKTOisusedorfurthertuning
ofitshyperparamterswillhelp. However,thisisacomplexprocessinalivedeployment.
LanguageAnalysis Weanalyzethehumaninstructionsandhowtheychangeasthepolicylearns
frommoreinteractions(Figure7). Weobserveareductioninvocabularysizeandutterancelength
earlyon. Thisisexpected,andfollowsknownobservationsinhowhumansadapttoreducecognitive
costs (e.g., Clark & Wilkes-Gibbs, 1986; Effenberger et al., 2021). However, in later rounds, B-
SUP witnessesanincreaseinvocabularysizeandutterancelength. Thissurprisingtrendreversal
isattributedtoonlythreeoutlierworkers, sodoesnotexpressasignificantchangeinpopulation
behavior. The number of reset signals drops, another reflection of improved collaborated task
performance. Suchtrendsarefairlyconsistentacrosssystemvariants,exceptforB-KTO,whichalso
showsdivergenceinperformance. WeobservethatinitiallyworkerstendtouseTryagaininsteadof
directlydescribingatarget,orrequestaresetwithinstructionslikeDeselecteverything(Figure15
andFigure16). Theoccurrencesofbothdecreaseinlaterrounds. Eventhoughtheworkerschange
theirlanguage,thisdoesnotreallyhelptheinitialpolicyπ ,whichremainspoor(Figure4).
θ0
7 RELATED WORK
LearningfromFeedback LearningfromfeedbackforLLMsisbeingstudiedextensively. RLfrom
humanfeedback(RLHF)ismaybethemostcommontechnique(Ouyangetal.,2022). Itrelieson
solicitingpair-wisepreferencesfromannotators,whichissignificantlydifferentthanourrelianceon
signalsfromtheinteractionitself. Learningfromfeedbackonasinglesystemoutputhasalsobeen
studied,eitherintheformofbinaryfeedback(Ethayarajhetal.,2024;Suhr&Artzi,2023;Gaoetal.,
9
tcA
gen
uen
sopPreprintundersubmission
B-SUP T-SUP B-RL T-RL B-KTO T-KTO
40
1400 9
25 20
1200 8
0
0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6 0 1 2 3 4 5 6
Round Round Round Round
Figure 7: Language analysis of human instructions. All systems show a decrease in instruction
complexityinthefirstthreerounds,exceptforB-KTO,suggestingadaptationandimprovedefficiency
onthespeaker’sside. Keyword-basedanalysisrevealsthatthenumberofreset/frustrationsignals
drops,areflectionofthemodellearningandcollaborationimproving.
2023)orthroughmoreexpressiveediting(Gaoetal.,2024)orcommentingandrefinement(Lietal.,
2017;Sumersetal.,2021;Scheureretal.,2023). Hancocketal.(2019)trainsaseparatesupervised
modeltocontinuallypredictsatisfactionlevels,whichisthenusedtopauseinteractionsandsolicit
explicitfeedback. Unlikethislineofwork,wedonotsolicitfeedback,butrelyonnaturalsignalsthat
arisefromthefollowupinteraction. Someoftheseutterancesincludeexplicitfeedback,butmanydo
not. Mostimportant,ourapproachreliesonthenaturalcourseofhumaninteractions,anddoesnot
addsolicitationoffeedback.
LearningfromNaturallyOccurringSignals Kojimaetal.(2021)presentsanapproachtolearn
togenerateinstructionsbyobservinghowhumansfollowthem,acomplementarymodeoflearning
toourfocusongeneralresponse. Pangetal.(2023)maximizesheuristics, suchasthechanceof
long responses from humans, in a chatbot scenario. Artzi & Zettlemoyer (2011) studied the use
of naturally occurring recovery efforts (i.e., when the user switches to simpler language to relay
information)totrainasymbolicsemanticparserfromacorpusofdialogueinteractions. Incontrast,
weoptforageneralapproachtoinferfeedbackfromnaturallanguageinteractionsofthemodelitself.
Concurrentlytoourwork,Don-Yehiyaetal.(2024),aswellas Petraketal.(2023),proposesan
approach that uses naturally occurring feedback in conversations to filter a large conversational
corpus. The linguistic cues they rely on are similar to ours. Unlike our study, the model they
improve is not the model that generated the interactions, creating a distillation-like setup, where
improvementisnotcomingfromthemodel’sowninteraction,butfromothermodels. Wefocuson
modelself-improvement,whereitiscriticalthatnostrongermodelisinvolved,andthemodeldrives
itsownimprovement. Aseconddistinguishingfactorisourinterestincontinualdeploymentwith
humans,whereastheyfollowastandardtrain-testbenchmarkingrecipe. Thisallowsourworkto
exposedynamicsthatareotherwisehidden. Takentogether,ourworkandDon-Yehiyaetal.(2024)
complementeachother,andstrengthenourconclusions. Theirworkshowsthesignalcanbederived
fromlarge-scalediversedata,whereasoursshowshowasingle-modelloopcanworkoveralong
periodoftime,andthedynamicsitcreates.
LLMsthatSelf-improve AcommonapproachtoimprovemodelsisviaAIfeedback,solicited
fromthemodelitselforanothermodel(Baietal.,2022;Burnsetal.,2023;Madaanetal.,2023;
Kumaretal.,2024;Quetal.,2024;Yuanetal.,2024;Lietal.,2024a). AsopposedtoAIfeedback,
weelicitrealhumanfeedbackautomaticallyfromtheinteractionthemodelhaswhileitisdeployed.
Thissignalismoreon-the-job,andlesslikelytobeinfluencedbymodelbiases. Wealsousethe
samemodelforinteractionandinferringfeedback,rulingoutconcernsaboutdistillation. Ourworkis
somewhatanalogoustothegenerator-verifierlineofwork(Aniletal.,2021;Lietal.,2024b;Kirchner
etal.,2024),whichreliesonautomaticverifiers(e.g.,unittests).Weretrospectandlearnfromimplicit
humanfeedbackthatnaturallyoccursininteractions,asanalternativeverificationsignal.
8 DISCUSSION
WeintroduceRESPECT: retrospectivelearningfrominteractions,anannotation-freeapproachby
leveragingsignalsfromnaturallyoccurringfeedbackininteractions. Wedemonstrateitseffectiveness
inlong-termdeploymentsandrobustnesstosystemvariants. Asopposedtoevaluatingonastatic
10
ezisyralubacoV htgnelecnarettU
teseR#
niagayrT#Preprintundersubmission
benchmark,wedesignMULTIREFtostudyrealinteractionsoveraperiodoftime.Wemaketrade-offs
betweenthegeneralityofthetask,andtheabilitytoiterateonaprototypefast,andwithouthighcosts.
Itisimportanttoexpandthistypeofstudytoothertasks,suchassummarizationorconversational
questionanswering,wheresimilarsignalsmaybemorecomplex,farapart,ordemandlong-term
creditassignment.Anotherinterestingdirectionisexpandingtheexpressivityofthefeedbackdecoder,
suchthatitrecoversamoreexpressivesignal(e.g.,anaturallanguageexplanation).
AUTHORCONTRIBUTIONS
ZC developed and implemented the methods, trained models, conducted pilot studies, and the
continual learning study. MOG contributed code for tangram reference games and designed the
human evaluation study. MOG, AW, YC, and GG designed the MULTIREF scenario. YC and
GGimplementedtheMULTIREFinterface,recruitedMTurkworkers,andcollectedhuman-human
interactions,withMOGandAW’smentoring. ZC,MOG,andAWcontributedtothewriting. YA
advisedandoversawallpartsoftheproject.
ACKNOWLEDGMENTS
ThisresearchwassupportedbyNSFundergrantsNo. 1750499andOAC-2311521,NASAunder
award No. 20-OSTFL20-0053, a gift from Open Philanthropy, a gift from Apple, the National
ArtificialIntelligenceResearchResource(NAIRR)Pilot,theFronterasupercomputersupportedby
theNationalScienceFoundation(awardNSF-OAC1818253)attheTexasAdvancedComputing
Center(TACC)atTheUniversityofTexasatAustin,andtheDeltaadvancedcomputinganddata
resourcewhichissupportedbytheNationalScienceFoundation(awardNSF-OAC2005572). GG
was supported by an NSF REU supplement for NSF grant No. 175049. YC was supported by
BowersUndergraduateResearchExperienceprogram. Anyopinions,findingsandconclusionsor
recommendationsexpressedinthismaterialarethoseoftheauthor(s)anddonotnecessarilyreflect
theviewsoftheNationalScienceFoundation,NASA,ortheotherfunders.
REFERENCES
ArashAhmadian,ChrisCremer,MatthiasGalle´,MarziehFadaee,JuliaKreutzer,OlivierPietquin,
AhmetU¨stu¨n,andSaraHooker. Backtobasics: RevisitingREINFORCE-styleoptimizationfor
learningfromhumanfeedbackinLLMs. InLun-WeiKu,AndreMartins,andVivekSrikumar
(eds.),Proceedingsofthe62ndAnnualMeetingoftheAssociationforComputationalLinguistics
(Volume1: LongPapers),pp.12248–12267,Bangkok,Thailand,August2024.Associationfor
ComputationalLinguistics. doi: 10.18653/v1/2024.acl-long.662. URLhttps://aclanthology.
org/2024.acl-long.662. 6
AbdullahAlmaatouq,JoshuaBecker,JamesP.Houghton,NicolasPaton,DuncanJ.Watts,andMarkE.
Whiting. Empirica: Avirtuallabforhigh-throughputmacro-levelexperiments. BehaviorResearch
Methods,53(5):2158–2171,October2021. ISSN1554-3528. doi: 10.3758/s13428-020-01535-9.
URLhttps://doi.org/10.3758/s13428-020-01535-9. 16
CemAnil,GuodongZhang,YuhuaiWu,andRogerGrosse. LearningtoGiveCheckableAnswers
withProver-VerifierGames,August2021. URLhttp://arxiv.org/abs/2108.12099. 10
YoavArtziandLukeZettlemoyer.Bootstrappingsemanticparsersfromconversations.InProceedings
oftheConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.421–432,Edinburgh,
Scotland,UK.,July2011.AssociationforComputationalLinguistics. URLhttp://www.aclweb.
org/anthology/D11-1039. 10
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones,
AnnaChen,AnnaGoldie,AzaliaMirhoseini,CameronMcKinnon,CarolChen,CatherineOlsson,
ChristopherOlah,DannyHernandez,DawnDrain,DeepGanguli,DustinLi,EliTran-Johnson,
EthanPerez,JamieKerr,JaredMueller,JeffreyLadish,JoshuaLandau,KamalNdousse,Kamile
Lukosuite,LianeLovitt,MichaelSellitto,NelsonElhage,NicholasSchiefer,NoemiMercado,Nova
DasSarma,RobertLasenby,RobinLarson,SamRinger,ScottJohnston,ShaunaKravec,SheerEl
Showk,StanislavFort,TameraLanham,TimothyTelleen-Lawton,TomConerly,TomHenighan,
Tristan Hume, Samuel R. Bowman, Zac Hatfield-Dodds, Ben Mann, Dario Amodei, Nicholas
11Preprintundersubmission
Joseph,SamMcCandlish,TomBrown,andJaredKaplan. ConstitutionalAI:Harmlessnessfrom
AIFeedback,December2022. URLhttp://arxiv.org/abs/2212.08073. 10
CollinBurns,PavelIzmailov,JanHendrikKirchner,BowenBaker,LeoGao,LeopoldAschenbrenner,
YiningChen,AdrienEcoffet,ManasJoglekar,JanLeike,IlyaSutskever,andJeffWu. Weak-to-
Strong Generalization: Eliciting Strong Capabilities With Weak Supervision, December 2023.
URLhttp://arxiv.org/abs/2312.09390. 10
HerbertHClarkandDeannaWilkes-Gibbs. Referringasacollaborativeprocess. Cognition,22(1):
1–39,1986. 3,4,9
Shachar Don-Yehiya, Leshem Choshen, and Omri Abend. Learning from Naturally Occurring
Feedback,July2024. URLhttp://arxiv.org/abs/2407.10944. 10
AnnaEffenberger,RhiaSingh,EvaYan,AlaneSuhr,andYoavArtzi. Analysisoflanguagechangein
collaborativeinstructionfollowing. InMarie-FrancineMoens,XuanjingHuang,LuciaSpecia,and
ScottWen-tauYih(eds.),FindingsoftheAssociationforComputationalLinguistics:EMNLP2021,
pp.2803–2811,PuntaCana,DominicanRepublic,November2021.AssociationforComputational
Linguistics. doi: 10.18653/v1/2021.findings-emnlp.239. URL https://aclanthology.org/
2021.findings-emnlp.239. 9
KawinEthayarajh,WinnieXu,NiklasMuennighoff,DanJurafsky,andDouweKiela. KTO:Model
AlignmentasProspectTheoreticOptimization,September2024. URLhttp://arxiv.org/abs/
2402.01306. 2,5,6,9,19
JeanEFoxTree. Listeninginonmonologuesanddialogues. Discourseprocesses,27(1):35–53,
1999. 4
Ge Gao, Hung-Ting Chen, Yoav Artzi, and Eunsol Choi. Continually improving extractive QA
viahumanfeedback. InHoudaBouamor,JuanPino,andKalikaBali(eds.),Proceedingsofthe
2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.406–423,Singapore,
December2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.emnlp-main.
27. URLhttps://aclanthology.org/2023.emnlp-main.27. 9
GeGao,AlexeyTaymanov,EduardoSalinas,PaulMineiro,andDipendraMisra. AligningLLM
AgentsbyLearningLatentPreferencefromUserEdits,June2024. URLhttp://arxiv.org/
abs/2404.15269. 10
NoahDGoodmanandMichaelCFrank. Pragmaticlanguageinterpretationasprobabilisticinference.
Trendsincognitivesciences,20(11):818–829,2016. 3
HerbertPaulGrice. Logicandconversation. Syntaxandsemantics,3:43–58,1975. 4,25
MustafaOmerGulandYoavArtzi. CoGen: LearningfromFeedbackwithCoupledComprehension
andGeneration,August2024. URLhttps://arxiv.org/abs/2408.15992v1. 6,15,16
Braden Hancock, Antoine Bordes, Pierre-Emmanuel Mazare, and Jason Weston. Learning from
dialogueafterdeployment: Feedyourself,chatbot! InAnnaKorhonen,DavidTraum,andLlu´ıs
Ma`rquez(eds.),Proceedingsofthe57thAnnualMeetingoftheAssociationforComputational
Linguistics,pp.3667–3684,Florence,Italy,July2019.AssociationforComputationalLinguistics.
doi: 10.18653/v1/P19-1358. URLhttps://aclanthology.org/P19-1358. 10
Robert Hawkins, Minae Kwon, Dorsa Sadigh, and Noah Goodman. Continual Adaptation for
Efficient Machine Communication. In Raquel Ferna´ndez and Tal Linzen (eds.), Proceedings
of the 24th Conference on Computational Natural Language Learning, pp. 408–419, Online,
November2020a.AssociationforComputationalLinguistics. doi: 10.18653/v1/2020.conll-1.33.
URLhttps://aclanthology.org/2020.conll-1.33. 8
RobertD.Hawkins, MichaelC.Frank, andNoahD.Goodman. CharacterizingtheDynamicsof
LearninginRepeatedReferenceGames.CognitiveScience,44(6):e12845,2020b.ISSN1551-6709.
doi: 10.1111/cogs.12845. URLhttps://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.
12845. 4
12Preprintundersubmission
WilliamS.HortonandRichardJ.Gerrig. Speakers’experiencesandaudiencedesign: Knowing
whenandknowinghowtoadjustutterancestoaddressees. JournalofMemoryandLanguage,47
(4):589–606,November2002. ISSN0749-596X. doi: 10.1016/S0749-596X(02)00019-0. URL
https://www.sciencedirect.com/science/article/pii/S0749596X02000190. 4
EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen. LoRA:Low-rankadaptationoflargelanguagemodels. InInternationalConference
onLearningRepresentations,2022. URLhttps://openreview.net/forum?id=nZeVKeeFYf9.
6,18
Anya Ji, Noriyuki Kojima, Noah Rush, Alane Suhr, Wai Keen Vong, Robert Hawkins, and
Yoav Artzi. Abstract Visual Reasoning with Tangram Shapes. In Yoav Goldberg, Zornitsa
Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods
in Natural Language Processing, pp. 582–601, Abu Dhabi, United Arab Emirates, December
2022.AssociationforComputationalLinguistics. doi: 10.18653/v1/2022.emnlp-main.38. URL
https://aclanthology.org/2022.emnlp-main.38. 4,6,15,20
Jan Hendrik Kirchner, Yining Chen, Harri Edwards, Jan Leike, Nat McAleese, and Yuri Burda.
Prover-VerifierGamesimprovelegibilityofLLMoutputs,August2024. URLhttp://arxiv.
org/abs/2407.13692. 10
NoriyukiKojima,AlaneSuhr,andYoavArtzi. ContinualLearningforGroundedInstructionGenera-
tionbyObservingHumanFollowingBehavior. TransactionsoftheAssociationforComputational
Linguistics,9:1303–1319,December2021. ISSN2307-387X. doi: 10.1162/tacl a 00428. URL
https://doi.org/10.1162/tacl a 00428. 2,5,8,10
AviralKumar,VincentZhuang,RishabhAgarwal,YiSu,JohnDCo-Reyes,AviSingh,KateBaumli,
ShariqIqbal,ColtonBishop,RebeccaRoelofs,etal. Traininglanguagemodelstoself-correctvia
reinforcementlearning. arXivpreprintarXiv:2409.12917,2024. 10
Hugo Laurenc¸on, Le´o Tronchon, Matthieu Cord, and Victor Sanh. What matters when building
vision-languagemodels?,May2024. URLhttp://arxiv.org/abs/2405.02246. 2,6
JiweiLi,AlexanderH.Miller,SumitChopra,Marc’AurelioRanzato,andJasonWeston. Dialogue
learningwithhuman-in-the-loop. InInternationalConferenceonLearningRepresentations,2017.
URLhttps://openreview.net/forum?id=HJgXCV9xx. 10
MingLi,LichangChen,JiuhaiChen,ShwaiHe,JiuxiangGu,andTianyiZhou. Selectivereflection-
tuning: Student-selected data recycling for LLM instruction-tuning. In Lun-Wei Ku, Andre
Martins,andVivekSrikumar(eds.),FindingsoftheAssociationforComputationalLinguistics
ACL2024,pp.16189–16211,Bangkok,Thailandandvirtualmeeting,August2024a.Association
for Computational Linguistics. URL https://aclanthology.org/2024.findings-acl.958.
10
XiangLisaLi,VaishnaviShrivastava,SiyanLi,TatsunoriHashimoto,andPercyLiang.Benchmarking
andimprovinggenerator-validatorconsistencyoflanguagemodels. InTheTwelfthInternational
ConferenceonLearningRepresentations,2024b. URLhttps://openreview.net/forum?id=
phBS6YpTzC. 10
Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and
ColinARaffel. Few-shotparameter-efficientfine-tuningisbetterandcheaperthanin-context
learning. AdvancesinNeuralInformationProcessingSystems,35:1950–1965,2022. 7
AmanMadaan,NiketTandon,PrakharGupta,SkylerHallinan,LuyuGao,SarahWiegreffe,UriAlon,
NouhaDziri,ShrimaiPrabhumoye,YimingYang,ShashankGupta,BodhisattwaPrasadMajumder,
KatherineHermann,SeanWelleck,AmirYazdanbakhsh,andPeterClark. Self-refine: Iterative
refinementwithself-feedback. InThirty-seventhConferenceonNeuralInformationProcessing
Systems,2023. URLhttps://openreview.net/forum?id=S37hOerQLB. 10
DipendraMisra,JohnLangford,andYoavArtzi. Mappinginstructionsandvisualobservationsto
actionswithreinforcementlearning. InMarthaPalmer,RebeccaHwa,andSebastianRiedel(eds.),
Proceedingsofthe2017ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.
1004–1015,Copenhagen,Denmark,September2017.AssociationforComputationalLinguistics.
doi: 10.18653/v1/D17-1106. URLhttps://aclanthology.org/D17-1106. 7
13Preprintundersubmission
Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang,
MelihElibol,ZonghengYang,WilliamPaul,MichaelI.Jordan,andIonStoica. Ray: Adistributed
framework for emerging AI applications. In Proceedings of the 13th USENIX Conference on
Operating Systems Design and Implementation, OSDI’18, pp. 561–577, USA, October 2018.
USENIXAssociation. ISBN978-1-931971-47-8. 16
RafaelMu¨ller,SimonKornblith,andGeoffreyEHinton. Whendoeslabelsmoothinghelp? Advances
inneuralinformationprocessingsystems,32,2019. 7
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,PamelaMishkin,Chong
Zhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal. Traininglanguagemodelstofollow
instructionswithhumanfeedback. Advancesinneuralinformationprocessingsystems,35:27730–
27744,2022. 2,9
RichardYuanzhePang,StephenRoller,KyunghyunCho,HeHe,andJasonWeston. Leveraging
ImplicitFeedbackfromDeploymentDatainDialogue,July2023. URLhttp://arxiv.org/abs/
2307.14117. 5,10
DominicPetrak,NafiseMoosavi,YeTian,NikolaiRozanov,andIrynaGurevych. Learningfrom
free-text human feedback – collect new datasets or extend existing ones? In Proceedings of
the2023ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pp.16259–16279,
Singapore,December2023.AssociationforComputationalLinguistics. doi: 10.18653/v1/2023.
emnlp-main.1011. URLhttps://aclanthology.org/2023.emnlp-main.1011. 10
YuxiaoQu,TianjunZhang,NamanGarg,andAviralKumar. Recursiveintrospection: TeachingLLM
agentshowtoself-improve. InICML2024WorkshoponFoundationModelsintheWild,2024.
URLhttps://openreview.net/forum?id=g5wp1F3Dsr. 10
Seymour Rosenberg and Bertram D. Cohen. Speakers’ and Listeners’ Processes in a Word-
CommunicationTask. Science,145(3637):1201–1203,September1964. doi: 10.1126/science.145.
3637.1201. URLhttps://www.science.org/doi/abs/10.1126/science.145.3637.1201. 3
Je´re´myScheurer,JonAnderCampos,TomaszKorbak,JunShernChan,AngelicaChen,Kyunghyun
Cho,andEthanPerez. TrainingLanguageModelswithLanguageFeedbackatScale,April2023.
URLhttp://arxiv.org/abs/2303.16755. 10
MichaelFSchoberandHerbertHClark. Understandingbyaddresseesandoverhearers. Cognitive
Psychology,21(2):211–232,April1989. ISSN0010-0285. doi: 10.1016/0010-0285(89)90008-X.
URLhttps://www.sciencedirect.com/science/article/pii/001002858990008X. 3,4
JohnSchulman,FilipWolski,PrafullaDhariwal,AlecRadford,andOlegKlimov. ProximalPolicy
OptimizationAlgorithms,August2017. URLhttp://arxiv.org/abs/1707.06347. 6
AlaneSuhrandYoavArtzi. Continuallearningforinstructionfollowingfromrealtimefeedback.
In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https:
//openreview.net/forum?id=ez6Cb0ZGzG. 2,9
TheodoreR.Sumers,MarkK.Ho,RobertD.Hawkins,KarthikNarasimhan,andThomasL.Griffiths.
LearningRewardsFromLinguisticFeedback. ProceedingsoftheAAAIConferenceonArtificial
Intelligence,35(7):6002–6010,May2021. ISSN2374-3468. doi: 10.1609/aaai.v35i7.16749. URL
https://ojs.aaai.org/index.php/AAAI/article/view/16749. 10
RonaldJWilliams. Simplestatisticalgradient-followingalgorithmsforconnectionistreinforcement
learning. Machinelearning,8:229–256,1992. 2,5
WeizheYuan,RichardYuanzhePang,KyunghyunCho,XianLi,SainbayarSukhbaatar,JingXu,
andJasonEWeston. Self-rewardinglanguagemodels. InForty-firstInternationalConferenceon
MachineLearning,2024. URLhttps://openreview.net/forum?id=0NphYCmgua. 10
14Preprintundersubmission
A THE MULTIREF GAME DESIGN AND DATA COLLECTION
A.1 INTERACTIONDESIGN
MULTIREFisamulti-target,multi-turnreferencegamebetweentwoplayers,aspeakerandalistener.
Eachgamestartswith10tangramsasthecontext, with3–5tangramsdesignatedastargets. The
targetdesignationsarerevealedtothespeakerbuthiddentothelistener. Thegoalistoselectall
targets without selecting any non-targets. The speaker can only communicate with the listener
throughasequenceofutterances,andonlythelistenercantakeselectionanddeselectionactions. The
interactionstartswithaspeakerturn. Turnsalternatebetweenspeakerandlistener,withamaximum
of20turns. Ineachspeakerturn,theytypeanutterancetosendtothelistener. Speakerturnsare
limitedto25seconds. Ineachlistenerturn,theyhave45secondstoselectordeselectimagesas
instructedtobythespeaker. Thegameconcludeswhenthelistenerselectsonlyandalltargets,orthe
whenthepartnersrunoutofturns. AppendixA.3showsscreenshotsoftheinterface.
ContextConstruction WefollowGul&Artzi(2024)andconstructgamecontextsusing1,013
tangramimagesfrom KILOGRAM Jietal.(2022). Wegrouptangramsrandomlyintotwosplits:
developmentsplit(101tangrams)andmainsplit(912tangrams). Thedevelopmentsplitisexclusively
usedforseedingtheinitiallistenerpolicyπ . Allhuman-botinteractionsareconstructedfromthe
0
mainsplit,i.e.,tangramsthattheseedpolicyπ hasneverseenbefore. Weconstructallgameswith
0
3–5targettangrams. Moretargetsaregenerallyharder,giventhesamemaximumnumberofturns
perinteraction.
A.2 HUMANEVALUATIONDESIGN
Automaticallyevaluatingturn-levelpolicyperformanceishard,becausewehavenogroundtruth
(i.e.,theselectionanddeselectionactionsintendedbythespeakerineachturn)tocompareagainst.
Similarly,wehavenogroundtruthtosystematicallyassessthefeedbackdecoderquality. Weconduct
humanevaluationsurveystoaddresstheseproblems. WeannotateasubsetofB-SUPinteractions,
roughly120interactionsor1,000turnspersystem-turn.
Weshowhumanannotatorsacompleteinteractionturnbyturn,withoutrevealingtheunderlying
targets. Foreachturn,theannotationconsistsfortwophases:
1. Ground-truth: weshowcontext,currentlyselectedtangrams,andinstructiongivenbythe
speaker. Weasktheannotatortoannotatethelisteneraction. Theannotatoractiona∗ is
consideredasgroundtruthactionforthisturn. Weusetheselabelsfortune-levelevaluation.
Aftertheactionannotation,werevealtheactionaˆactuallytakenbythelistener(i.e.,the
model)duringtheinteraction.
2. Satisfaction: wepresentthefollow-uputterance. Weasktheannotatortorateifthespeaker
issatisfiedwiththelistener’saction,basedonthefollow-uputterance. Theychooseoneof
thefollowingoptions:
a. Yes.
b. Yes,eventhoughthelistenerdidnotperformallrequiredselections/deselections.
c. Yes,eventhoughthelistenermadeincorrectselections/deselections.
d. No.
Thethirdoptionaccountsforthelisteneraccidentallyselectingatargettangramnotintended
bythespeaker,butthespeakerchoosingtomoveonwithoutcorrectionorevenvalidating
theselection. Wetreattheselabelsasgroundtruthforevaluatingfeedbackdecoders.
Weannotate5%oflong-termhuman-botinteractionsannotationsbythreedifferentannotators,to
estimatehowreliabletheannotationsare. Weobserve85%agreementonthecorrectness(whether
aˆ=a∗)ongroundtruthstage,6and65%agreementontheground-truthactiona∗acrossworkers.7
Forsatisfactionannotation,weobserve93%agreementrate,illustratingtherelativesimplicityof
extractingthesignalthatdriveourlearningprocess.
6Thepercentageofcaseswhereallannotatorsagreethatthebotdidrightorwrong.
7Thepercentageofcaseswhereallthreeannotatorsprovidedexactlythesamesetofactions.
15Preprintundersubmission
Figure8: TheMULTIREFinterfaceforthespeakerinturn1. Predefinedtargetsarerevealedtothe
speakerinblackboxes.
A.3 MTURKDETAILS
Worker Recruitment We follow Gul & Artzi’s (2024) worker recruitment recipe. We require
workerstohaveaminimum98%approvalrate,atleast1,000approvedHITs(HumanIntelligence
Task),andbelocatedinEnglish-majoritylocales. Allworkersmustwatchavideotutorialandpassa
quizbeforegainingqualificationtoworkonMULTIREFinteractions. Theymustreadathorough
guidelineandpassanotherquizbeforegrantedaccesstohumanevaluationsurveys. Werecruit33
expertworkerstointeractwithLLMsinthemainstudyandannotatebycompletingsurveysafterthe
mainstudy. ThisstudyisexemptedbyCornell’sInstitutionalReviewBoard.
Payment Wepayworkers$0.81USDperMULTIREFgame,andabonusifthegameissuccessful.
Overall the estimated hourly wage is $13.00 USD, and closer to $23.00 USD by the end of the
continual study when the LLM is fairly good at the game. On average a human-bot game takes
under2minutes. Wepayworkers$0.06USDperturnforhumanevaluationsurveys,or$0.08USD
iftheturnannotationinvolveserrormodes. Theestimatedhourlywageis$16.00USDforhuman
evaluationsurveys. Onaverageittakesunder2.5minutestoannotateonegame. Wesetthepayment
schemethroughpilotstudiesandaimat$15.00USDhourlywage.
InterfaceandServing WeimplementMULTIREFusingEmpirica(Almaatouqetal.,2021)andon
topofthecodebaseofGul&Artzi(2024). Thespeakerhas25secondstotypeintoachatboxeach
turnandhitEnterorsubmit,andthelistenerhas45secondstoclickonthetangramstoselectorto
deselect. Thegameendsifonepartyidlesforoneturn,andthepartyidlingisnotcompensated. We
serveonanEC2instance. WeserveLLMpolicieswiththeRayframework(Moritzetal.,2018). We
walkthroughthefirstturnsofasampleinteractioninFigure8,Figure9,andFigure10.
16Preprintundersubmission
Figure9: TheMULTIREFinterfaceforthelistenerinturn2,followingthespeakerturninFigure8.
Targetsarehiddenforthelistener,andthecontexttangramsareinadifferentorder. Herethelistener
hasselectedatangramgiventheinstructionselectthebutterfly.
Figure10: TheMULTIREFinterfaceforthespeakerinturn3,followingthelistenerturninFigure9
Thelistenerselectedanon-targettangram,showninredtothespeaker.
17Preprintundersubmission
Policyprompt
System: <img>A <img>B <img>C <img>D <img>E <img>F <img>G <img>H
<img>I<img>J .....................(Contexttangramstaggedbycodenames)
System: noneisselected ......................................(Turn1state)
User: SelecttheletterSwiththelargetrianglebackside .....(Turn1utterance)
Assistant: SelectJ ...........................................(Turn1action)
System: Jcurrentlyselected
User: unselectthatoneandtryagain
Assistant: DeselectJselectA
System: Acurrentlyselected
User: almost,unselectthatoneandtryonemoretime
Assistant: DeselectAselectG
System: Gcurrentlyselected
User: correct. Nowselecttherabbitfacingright
Assistant: SelectF
System: FGcurrentlyselected
User: nothatisabird,tryagain
Assistant: DeselectFselectH
System: GHcurrentlyselected
User: nothatisanotherbird,tryagain
Assistant: DeselectHselectF ......................................(Action)
Figure 11: Policy prompt example with a model predicted action and additional comments for
readability.
B LEARNING DETAILS
B.1 INTERACTIONREPRESENTATION
WeencodethecontextxasinFigure11. Westandardizeactionrepresentationbyorderingactions,
forexample,alwaysproduceSelect A CratherthanSelect C A.Weshufflethecontextimages
duringtrainingastheorderofcontexttangramsshouldnothaveanyimpactontheinteractionlogic.
B.2 POLICYINITIALIZATION
Weseedtheinitialpolicyπ byfine-tuningthemodelonasmalldatasetof90turnsD ,whereboth
0 0
thespeakerandthelistenerarehumans. Wealsoexperimentedwithpromptingtoinitializethepolicy.
Wefindearlythatfew-shotpromptingyieldsarandompolicyatbest,likelybecausereasoningwith
abstractshapessuchastangramsisvisuallyout-of-distributionforthemodel.
Thereisasignificantdistributionshiftbetweenhuman-humaninteractions,andhuman-policyinterac-
tion,especiallyearlyonwhenthemodelperformspoorly. Inpractice,twomajordifferencesarethe
lengthofinteractionsandtheprevalenceofdeselectioninstructions,whicharerareinhuman-human
interactions. Weaddressthedeselectionissuewithdataaugmentation. Wesyntheticallygenerate
turnswherethespeakerasksfordeselections,andthelistenercomplies. Weaugmentthedatawith
theseataratioof1:12totheexistingdata. ThishelpstheLLMpolicylearntodeselectandrecover
frommistakes. ThisaugmentationisonlyusedforD andsuchdistributionshiftisnotpresentin
0
alterrounds,whenlearningfromactualhuman-botinteractions.
B.3 HYPERPARAMETERSANDOTHERIMPLEMENTATIONDETAILS
We use the instruction-tuned IDEFICS2-8B model for all policies. We fine-tune with LoRA
adapters (Hu et al., 2022) (α=r=8, dropout=0.1) due to compute constraints. Appendix D pro-
videsmoreLoRAdetails. WetraineachmodelwithasingleGPU,RTXA6000, NVIDIAA100
40GBor80GB.Thetimetotrainrangesbetween2–24hours,longerinlaterroundsasmoredata
accumulates. Forstoppingcriteria,wepickcheckpointsbyhighestaccuracy(exactmatch)among
18Preprintundersubmission
Policypromptwithdeselectionaugmentation
[Previousturnsomitted]
System: noneisselected ....................................(Previousturns)
Speaker: Maninahat
Listener: SelectA
System: Acurrentlyselected ..............................(Augmentedstate)
Speaker: Wrong,undowhatyouselected ...............(Augmentedutterance)
Listener: DeselectA ....................................(Augmentedaction)
Figure12: Anexampleofdeselectionaugmentationwithaugmentedactionandcomments.
three seeds on a hold-out validation set of 344 turns DHH. The validation set is curated from 92
val
human-humangamesthemainsplitoftangrams. WesummarizehyperparametersinTable1.
Hyperparameter SearchSpace Supervised REINFORCE KTO
Optimizer AdamW AdamW RMSProp
Learningrate 1e-6,1e-5,1e-4,2e-4 1e-4 1e-4 1e-5
{ }
Learningratedecay no,cosine,linear cosine cosine no
{ }
Epochs 5,10,20,40 20 20 20
{ }
Warm-upsteps 0,10,50 10 10 10
{ }
Weightdecay 0,0.01,0.1 0.01 0.01 0.01
{ }
Effectivebatchsize 16,32,48,64,128 64 64 64
{ }
Entropyweight 0,0.01,0.5,0.1 0.01 0.01 0.1
{ }
β 0.01,0.1,0.5 0.5
KTO
{ }
Temperature 1 1 1
Table1: Hyperparametersettings.
DataImbalance Thedecodedfeedbackisimbalanced,withmorenegativeexamplesthanpositive
examples(3:1to2:1),especiallyatearlyroundsofcontinuallearning. Weaddressthisbyweighing
the loss by the absolute value of the reward, i.e., 0.1 for RL or λ and λ for KTO, and by
d u
−
downsamplingnegativeexamplesperbatch,suchthatthenumberofpositiveexamplesandnegative
examplesisroughly5:4.
KTOStability DeviationfromtheoriginalKTOimplementationbyhigherlearningrate,higher
β,moreepochs,producebetterresultsempiricallyonthevalidationsetinpilotandroundρ = 1.
However,inroundρ=2,B-KTOpolicystarttodegeneratebyproducingnonsensicalactionssuchas
Deselect A select A BorDeselect select select. Weattempttomitigatethisissueduring
trainingroundρ=3byswitchingfromweighingλ =4andλ =1asrecommendedinEthayarajh
d u
etal.(2024)toλ =λ =1,plusdownsamplingnegativeexamples. Wealsointroduceregex-based
d u
constraineddecodingtopreventnonsensicalactionsforB-KTOandT-KTOpoliciesinroundρ=3.
Despitethat,theKTOgroupperformsworseinliveinteractions(Figure4). WesuspectKTOismore
challengingtooptimizeforiterativecontinuallearning,butwesuspectfurthertuning(withhigher
computationalcosts)canreduceoreveneliminatetheseissues.
B.4 EVALUATIONMETRICS
Interaction-levelMetrics Interactionperformanceandstatisticsarecomputedautomaticallyfrom
livedeploymentinteractions. Theydonotrequirefurtherannotation.
1. Successrate=#successfulinteractions/#allinteractions. Aninteractionissuccessfulif
thelistenerselectsallandonlytargetsbeforerunningoutof10turns. Thisistheprimary
metricweusetoevaluatetheperformanceoftheLLMpolicy.
2. #Turnsperinteraction. Thisisameasureofcollaborativeefficiency.
19Preprintundersubmission
Turn-levelMetricswithReferencetoHumanAnnotation Wecomputeturn-levelmetricseither
withrespecttoHHgameswhereweconsiderhumanlisteneractionasgroundtruth(e.g.,DHH),orwith
val
respecttoB-SUPgameswhereweconsideractionsa∗annotatedinpost-hocsurveysasgroundtruth.
Whencomputedwithliveinteractions,thesemetricsarebiasedtowardslongerorfailedinteractions
becausetheyhavemoreturnsthansuccessfulinteraction.
1. Exactmatch=#exactmatch/#allturns. Anexactmatchiswhentheactiontakenbythe
policymatchesexactlytheactionlabeled/takenbyhumanlisteners(aˆ=a∗).
2. Similarity=Sim(aˆ,a∗)isacompositemetric. Letf(p,q): Rbeafunctionthat
I×I →
evaluatesthesimilarityofbetweentwoimagesp,q . Lettheactiontakenbypolicybe
∈I
aˆ= pˆ ,pˆ ,...,pˆ ,qˆ ,qˆ ,...,qˆ whereparetheselectedtangramsandqarethedeselected
1 2 nˆ 1 2 mˆ
tangr{ ams. Denotethegroundtru} thactionsasa∗ = p∗,p∗,...,p∗ ,q∗,q∗,...,q∗ . The
{ 1 2 n∗ 1 2 m∗ }
similaritybetweentwoactionsisdefinedas:
1 (cid:16) (cid:17)
Sim(aˆ,a∗)= Σnˆ Σn∗ f(pˆ,p∗)+Σmˆ Σm∗ f(qˆ,q∗) .
nˆn∗+mˆm∗ i=1 j=1 i j i=1 j=1 i j
If only one of nˆ and n∗ is zero, we rewrite Σnˆ Σn∗ f(pˆ,p∗) with max(nˆ,n∗), and
i=1 j=1 i j −
nˆn∗inthedenominatorwithmax(nˆ,n∗),intuitivelyassigning-1foreachmissedselection.
Thisedgecaseissimilarlytreatedformˆ,m∗anddeselection. Wecomputesimilaritiesusing
embeddingsfromthetangramfine-tunedCLIPmodelofJietal.(2022).
3. Positivefeedback=#turnsreceivingpositivefeedback/#allturns. Anactionreceives
positivefeedbackifspeakerissatisfiedwiththelistener’sactioninthefollowupinteraction.
Thisislabelledinhumanevaluationsurvey.
Micro-levelMetricwithReferencetoGroundTruthTargets Wecomputeclickaccuracywith
respecttothegroundtruthtargets(insteadofthetargetsintendedbythespeaker). Thisischeaper
becauseitdoesnotrequirehumanannotation,sowecancomputeitforallsystemvariantsandall
interactions. However, this measure produces false positives when an action selects a target not
intendedbythespeaker. Inpractice,though,wefinditcorrelateswellwithourhuman-annotated
evaluation.
Wecomputeclickaccuracyforaturngivenitscontextxandactionaˆ. Wedenotethesetofground
truthtargetsinthisinteractionas ,thesetofcurrentlyselectedcontexttangramsas ,thenforeach
T S
clickcinaˆ(selectordeselect),wecomputetheclickaccuracyas:
(cid:26)
1 if(c c / ) (c / c )
Clickaccuracy(c, , )= ∈T ∧ ∈S ∨ ∈T ∧ ∈S
T S 0 otherwise
Intuitively,aclickisapproximatelyaccurateifitselectsatargetordeselectsanon-target.Wecompute
thisforallclicksfromallinteractionsinaroundforallsystemsinFigure4.
Corpus-levelMetrics Weanalyzespeakerinstructionspersystem-round. Thekeywordusedto
generatetheanalysisinFigure7are:
1. #Reset=occurrencesofphrasesin reset,restart,fromscratch,allover,startover,deselect
{
everything,deselectall,removeeverything,removeall,cleareverything,clearall,unselect
everything,unselectall,dropeverything,dropall
}
2. #Tryagain=occurrencesofphrasesin tryagain,tryonemoretime,theotherone
{ }
C CUMULATIVE NUMBER OF INTERACTIONS OBSERVED
Themaintextincludesresultsbyround. Wecollectroughly330interactionsperpolicyperround.
Duetotheuncertaintyoflivedatacollection,wedonotalwayshitthisexactnumberforeachvariant
andround. Figure13showsthecumulativenumberofhuman-botinteractionseenbyapolicyvariant
byeachround.
20Preprintundersubmission
2000
B-SUP
1500
T-SUP
B-RL
1000 T-RL
B-KTO
500 T-KTO
0
0 1 2 3 4 5 6
Round
Figure13: Cumulativenumberofhuman-botinteractionsusedtotrainthepolicyeachround.
D ADDITIONAL ENHANCED LORA LAUNCH
Wesuspecttheplateauof B-SUP inFigure4ispartiallyduetothelimitedexpressivityofLoRA
adaptersweused. Wetestthishypothesisbydeployingroundρ=4andρ=5againwithenhanced
LoRAadapters. WeusethesamehyperparametersasinSectionB.3exceptadditionaladapters. The
originaladapterplacementisonthetextmodel,themodalityprojector,andtheperceiverresampler.
Adapters include the down projection layers, the gate projection layers, the up projection layers,
andthekey/query/valueprojectionlayers. Incomparison,theenhancedlaunchaddsadaptersonthe
visionmodel,includingtheoutprojection,thefirstandthesecondfullyconnectedlayers,besidesthe
projectionlayersontextmodels. Figure14showstheresultsfromthiscomplementarydeployment.
TheenhancedLoRAadaptersyieldasmallimprovementininteractionsuccessratecomparedto
theoriginallaunch,yettheoverallslowdownisevident. ThissuggestsLoRAexpressivityhassome
effect,butothereffectsarealsolimitingtheLLMpolicyfromcontinuingitsearlierimprovement
trends.
80%
60%
LoRA
40%
LoRAEnhanced
0 1 2 3 4 5 6
Round
Figure14: SuccessrateofB-SUPwithadditionalLoRAadaptersinround4and5.
E FEEDBACK DECODER DESIGN
The prompt design is minimal, general, and task-agnostic. We validate the prompt with manual
inspectionpriortocontinuallearninglaunchandhumansurveys. Consideringonlythemostrecent
twoaction-utteranceturns aˆ ,u ,aˆ ,u issufficienttoproducesatisfactorydecodingresults,
i−1 i i i+1
⟨ ⟩
andmorehistoryseemstodistractthedecoder.
Wealsoexperimentedwithnumericalreward(i.e.,decodingarealnumber),experimentingwitha
discretizedrewardspaceof .0,.1,.5,.9 . Ourexperimentsshowthemodelisnotwellcalibrated
{ }
forsuchdecoding.
21
snoitcaretnifo#evitalumuC
etarsseccusnoitcaretnIPreprintundersubmission
F INTERACTION CASE STUDIES
Figures 15–18 show case studies that illustrate the diversity of MULTIREF interaction scenarios.
Blackbordersindicatetargets. Yellowdotsindicateactionstakenbythelistener. Greenborders
indicatecorrectselections,whileredbordersindicatewrongselection.
Initialcontext
Turn1:getthebirdflying
Turn2:gettheflattablewithpointedbottomunderneath
Turn3:gettheshapethatisflatonthebottomandjaggedontop
Turn4:great.nowgettheshapethatlookslikeaJ
Turn5:no,notthatone.thisoneistallandskinny.itlookslikeaJwithasquareontop.
Turn6:no,notthatone.deselectitandtryagain.
Figure 15: The speaker is left with the last target at Turn 4. Failing, they provide an additional
descriptioninTurn5,andeventuallyresortto“tryagain”withoutdescribingthetargetinTurn6.
Theinitialturnsillustratehowfeedbackisimplied,ratherthanspecifiedexplicitly. Theinteraction
concludessuccessfully.
1
22Preprintundersubmission
Initialcontext
Turn1:lookslikeaspaceshipwithapointontop
Turn2:nodeselectlookslikeafigurebentover
Turn3:nodeselecteverything
Turn4:yeslookslikrabottlewithasquarelid
Turn5:yeslookslikeasmalltriangleonaflatsurface
Turn6:nodeselectlookslikeahousewithanopendoor
Turn7:yeslookslikeafgurebentoverfacingright
Figure16: ThespeakeraskstodeselecteverythinginTurn3toreset,anexpressionoffrustration.
Theinteractionconcludessuccessfully.
1
23Preprintundersubmission
Initialcontext
Turn1:aWshapeontopofatable
Turn2:Yes.Atrianglenexttoatallpole
Turn3:Yes.Thefrontofadog.Wecan’tseehistail.
Turn4:No,wecan’tseehistail.Justtheheadandfrontpaw.
Turn5:No,that’snotit
Turn6:Youhavethreecorrect.Selectthebearfront
Turn7:No,that’snotit.hisearisstickingup.
Figure17: Theabstractnessandambiguityoftangramslendtocomplexinteractions. Therearetwo
dogsinthecontext,andthelistenerstrugglestodisambiguateoridentifythetarget. Theinteraction
concludessuccessfully.
1
24Preprintundersubmission
Initialcontext
Turn1:Findthetwopeople.
Turn2:Welldone.Nowselectthestackedsquareswithaspoutontheleftside
Figure18: ThespeakerasksfortwotargetsinTurn1,exemplifyingGrice’sMaximsofQuantity-
onetriestobeasinformativeasonepossiblycan,andgivesasmuchinformationasisneeded,and
nomore(Grice,1975). Theinteractionconcludessuccessfully.
25
1