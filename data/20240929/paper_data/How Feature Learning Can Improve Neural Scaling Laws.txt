Preprint
HOW FEATURE LEARNING CAN IMPROVE NEURAL
SCALING LAWS
BlakeBordelon∗,AlexanderAtanasov∗,CengizPehlevan
ABSTRACT
Wedevelopasolvablemodelofneuralscalinglawsbeyondthekernellimit. The-
oretical analysis of this model shows how performance scales with model size,
training time, and the total amount of available data. We identify three scaling
regimes corresponding to varying task difficulties: hard, easy, and super easy
tasks. For easy and super-easy target functions, which lie in the reproducing
kernelHilbertspace(RKHS)definedbytheinitialinfinite-widthNeuralTangent
Kernel(NTK),thescalingexponentsremainunchangedbetweenfeaturelearning
and kernel regime models. For hard tasks, defined as those outside the RKHS
oftheinitialNTK,wedemonstratebothanalyticallyandempiricallythatfeature
learningcanimprovescalingwithtrainingtimeandcompute,nearlydoublingthe
exponentforhardtasks.Thisleadstoadifferentcomputeoptimalstrategytoscale
parametersandtrainingtimeinthefeaturelearningregime. Wesupportourfind-
ingthatfeaturelearningimprovesthescalinglawforhardtasksbutnotforeasy
and super-easy tasks with experiments of nonlinear MLPs fitting functions with
power-lawFourierspectraonthecircleandCNNslearningvisiontasks.
1 INTRODUCTION
Deeplearningmodelstendtoimproveinperformancewithmodelsize,trainingtimeandtotalavail-
abledata. Thedependenceofperformanceontheavailablestatisticalandcomputationalresources
areoftenregularandwell-capturedbyapower-law(Hestnessetal.,2017;Kaplanetal.,2020). For
example,theChinchillascalinglaw(Hoffmannetal.,2022)forthelossL(t,N)ofaN-parameter
modeltrainedonlinefortsteps(orttokens)follows
L(t,N)=c t−rt +c N−rN +L , (1)
t N ∞
where the constants c ,c and exponents r ,r are dataset and architecture dependent and L
t N t N ∞
representsthelowestachievablelossforthisarchitectureanddataset. Thesescalinglawsenablein-
telligentstrategiestoachieveperformanceunderlimitedcomputebudgets(Hoffmannetal.,2022)or
limiteddatabudgets(Muennighoffetal.,2023). Abetterunderstandingofwhatpropertiesofneural
networkarchitectures,parameterizationsanddatadistributionsgiverisetotheseneuralscalinglaws
couldbeusefultoselectbetterinitializationschemes,parameterizations,andoptimizers(Yangetal.,
2021;Achiametal.,2023;Everettetal.,2024)anddevelopbettercurriculaandsamplingstrategies
(Sorscheretal.,2022).
Despitesignificantempiricalresearch, apredictivetheoryofscalinglawsfordeepneuralnetwork
models is currently lacking. Several works have recovered data-dependent scaling laws from the
analysis of linear models (Spigler et al., 2020; Bordelon et al., 2020; Bahri et al., 2021; Maloney
etal.,2022;Simonetal.,2021;Bordelonetal.,2024a;Zavatone-Veth&Pehlevan,2023;Paquette
et al., 2024; Lin et al., 2024). However these models are fundamentally limited to describing the
kernelorlazylearningregimeofneuralnetworks(Chizatetal.,2019). Severalworkshavefound
thatthisfailstocapturethescalinglawsofdeepnetworksinthefeaturelearningregime(Fortetal.,
2020;Vyasetal.,2022;2023a;Bordelonetal.,2024a). Atheoryofscalinglawsthatcancapture
consistentfeaturelearningeveninaninfiniteparameterN → ∞limitisespeciallypressinggiven
the success of mean field and µ-parameterizations which generate constant scale feature updates
acrossmodelwidthsanddepths(Meietal.,2019;Geigeretal.,2020;Yang&Hu,2021;Bordelon
&Pehlevan,2022;Yangetal.,2022;Bordelonetal.,2023;2024b). Thetrainingdynamicsofthe
∗EqualContribution
1
4202
peS
62
]LM.tats[
1v85871.9042:viXraPreprint
infinite width/depth limits in such models can significantly differ from the lazy training regime.
InfinitelimitswhichpreservefeaturelearningarebetterdescriptorsofpracticalnetworksVyasetal.
(2023a). Motivatedbythis,weaskthefollowing:
Question: Under what conditions can feature learning improve the scaling law exponents of
neuralnetworkscomparedtolazytrainingregime?
1.1 OURCONTRIBUTIONS
Inthiswork,wedevelopatheoreticalmodelofneuralscalinglawsthatallowsforimprovedscaling
exponentscomparedtolazytrainingundercertainsettings. Ourcontributionsare
1. Weproposeasimpletwo-layerlinearneuralnetworkmodeltrainedwithaformofprojected
gradientdescent. Weshowthatthismodelreproducespowerlawscalingsintrainingtime,
model size and training set size. The predicted scaling law exponents are summarized in
termsoftwoparametersrelatedtothedataandarchitecture(α,β),whichwerefertoasthe
sourceandcapacityexponentsfollowingCaponnetto&Vito(2005).
2. Weidentifyaconditiononthedifficultyofthelearningtask, measuredbythesourceex-
ponentβ, underwhichfeaturelearningcanimprovethescalingofthelosswithtimeand
withcompute. Foreasytasks,whichwedefineastaskswithβ >1wheretheRKHSnorm
of the target is finite, there is no improvement in the power-law exponent while for hard
tasks(β < 1)thatareoutsidetheRKHSoftheinitiallimitingkernel,therecanbeanim-
provement. Forsuper-easytasksβ > 2− 1,whichhaveverylowRKHSnorm,variance
α
from stochastic gradient descent (SGD) can alter the scaling law at large time. Figure 1
summarizestheseresults.
3. Weprovideanapproximatepredictionofthecomputeoptimalscalinglawsforhard,easy
tasksandsuper-easytasks. Eachoftheseregimeshasadifferentexponentforthecompute
optimalneuralscalinglaw. Table1summarizestheseresults.
4. Wetestourpredictedfeaturelearningscalingsbytrainingdeepnonlinearneuralnetworks
fitting nonlinear functions. In many cases, our predictions from the initial kernel spectra
accuratelycapturethetestlossofthenetworkinthefeaturelearningregime.
Dynamics, 𝜸→𝟎 Dynamics, 𝜸≫𝟎
Rich Regime
1.2 Lazy Limit
1.0 𝑡−1+1/𝛼
→𝑁−𝛼𝛽
𝑡−1+1/𝛼
→𝑁−𝛼𝛽
0.8 𝑁 𝑁 𝜶 𝑡−𝛽→𝑁−𝛼𝛽 𝜶 𝑡−2𝛽/1+𝛽 →𝑁−𝛼𝛽
0.6
SGD Noise SGD Noise
0.4
𝑡−2+1/𝛼 𝑡−1+1/𝛼 𝑡−2+1/𝛼 𝑡−1+1/𝛼
0.2 1 𝐵 → 𝑁 1 𝐵 → 𝑁
0.0
0.0 0.2 0.4 0.6 0.8 1.0 1.2 0 1 0 1
Hard 𝜷 Easy Hard 𝜷 Easy
(a) lim L(t,N)∼t−χ(β) (b) LazyLimit (c) RichRegime
N→∞
Figure 1: Our model changes its scaling law exponents for hard tasks, where the source is suf-
ficiently small β < 1. (a) The exponent χ(β) which appears in the loss scaling L(t) ∼ t−χ(β)
of our model. (b)-(c) Phase plots in the α,β plane of the observed scalings that give rise to the
compute-optimaltrade-off. Arrows(→)representatransitionfromonescalingbehaviortoanother
ast→∞,wherethebalancingofthesetermsatfixedcomputeC =Ntgivesthecomputeoptimal
scalinglaw. Inthelazylimitγ → 0,werecoverthephaseplotforα > 1ofPaquetteetal.(2024).
Atnonzeroγ,however,weseethatthesetof“hardtasks”,asgivenbyβ <1exhibitsanimproved
scalingexponent. Thecomputeoptimalcurvesfortheeasytaskswithβ >1areunchanged.
1.2 RELATEDWORKS
Our work builds on the recent results of Bordelon et al. (2024a) and Paquette et al. (2024) which
analyzedtheSGDdynamicsofastructuredrandomfeaturemodel. Staticsofthismodelhavebeen
analyzedbymanypriorworks(Atanasovetal.,2023;Simonetal.,2023;Zavatone-Veth&Pehlevan,
2023; Bahri et al., 2021; Maloney et al., 2022). These kinds of models can accurately describe
2
)
(Preprint
networksinthelazylearningregime. However,theempiricalstudyofVyasetal.(2022)andsome
experimentsinBordelonetal.(2024a)indicatethatthepredictedcomputeoptimalexponentswere
smallerthanthosemeasuredinnetworksthatlearnfeaturesonrealdata.Theselatterworksobserved
thatnetworkstrainfasterintherichregimecomparedtolazytraining. Wedirectlyaddressthisgap
inperformance betweenlazyandfeature learningneuralnetworks byallowingthekernel features
toadapttothedata. WerevisitthecomputervisionsettingsofBordelonetal.(2024a)andshowthat
ournewexponentsmoreaccuratelycapturethescalinglawinthefeaturelearningregime.
Otherworkhasinvestigatedwhenneuralnetworksoutperformkernels(Ghorbanietal.,2020). Ba
etal.(2022)andAbbeetal.(2023)haveshownhowfeaturelearningneuralnetworkscanlearnlow
rankspikesinthehiddenlayerweights/kernelstohelpwithsparsetaskswhilelazynetworkscannot.
Targetfunctionswithstaircaseproperties,wherelearningsimplercomponentsaidlearningofmore
complexcomponentsalsoexhibitsignificantimprovements(withrespecttoalargeinputdimension)
due to feature learning (Abbe et al., 2021; Dandi et al., 2023; Bardone & Goldt, 2024). Here, we
consideradifferentsetting. Weaskwhetherfeaturelearningcanleadtoimprovementsinpowerlaw
exponentsfortheneuralscalinglaw. TheworkofPaccolatetal.(2021)asksasimilarquestionin
thecaseofasimplestripemodel. Hereweinvestigatewhetherthepowerlawscalingexponentcan
be improved with feature learning in a model that only depends on properties of the initial kernel
andthetargetfunctionspectra.
2 SOLVABLE MODEL OF SCALING LAWS WITH FEATURE LEARNING
Westartbymotivatinganddefiningourmodel. Ourgoalsistobuildasimplemodelthatexhibits
feature learning in the infinite-width limit but also captures finite network size, finite batch SGD
effects,andsamplesizeeffectsthatcansignificantlyalterscalingbehavior.
Following the notation of Bordelon et al. (2024a), we introduce our model from the perspective
of kernel regression. We first assume a randomly initialized neural network in an infinite-width
limitwheretheneuraltangentkernel(NTK)concentrates. Wethendiagonalizetheinitialinfinite-
width NTK. The resulting eigenfunctions ψ (x) ∈ RM have an inner product that define the
∞
infinite-widthNTKK (x,x′)=ψ (x)·ψ (x′). Theseeigenfunctionsareorthogonalunderthe
∞ ∞ ∞
probabilitydistributionofthedatap(x)with
(cid:10)
ψ (x)ψ
(x)⊤(cid:11)
=Λ=diag(λ ,...,λ ). (2)
∞ ∞ x∼p(x) 1 M
WewilloftenconsiderthecasewhereM →∞firstsothatthesefunctionsψ (x)formacomplete
∞
basisforthespaceofsquareintegrablefunctions.
WenextconsiderafinitesizedmodelwithN parameters.Weassumethismodel’sinitialparameters
aresampledfromthesamedistributionastheinfinitemodelandthattheN →∞limitrecoversthe
samekernelK (x,x′). ThefiniteN-parametermodel,atinitializationt=0,hasN eigenfeatures
∞
ψ˜(x,0)∈RN. Inthefeaturelearningregime,thesefeatureswillevolveduringtraining.
The finite network’s learned function f is expressed in terms of the lower dimensional features,
while the target function y(x) can be decomposed in terms of the limiting (and static) features
ψ (x)withcoefficientsw⋆. Theinstantaneousfinitewidthfeaturesψ˜(x,t)canalsobeexpanded
∞
asalinearcombinationofthebasisfunctionsψ (x)withcoefficientmatrixA(t) ∈ RN×M. We
∞
canthereforeviewourmodelasthefollowingstudent-teachersetup
1
f(x,t)= w(t)·ψ˜(x,t), ψ˜(x,t)=A(t)ψ (x)
N ∞ (3)
y(x)=w⋆·ψ (x).
∞
IfthematrixA(t)israndomandstaticthengradientdescentonthisrandomfeaturemodelrecovers
thestochasticprocessanalyzedbyBordelonetal.(2024a);Paquetteetal.(2024). Inthiswork,we
extendtheanalysistocaseswherethematrixA(t)isalsoupdated. Weconsideronlinetrainingin
themaintextbutdiscussandanalyzethecasewheresamplesarereusedinAppendixD.
Wealloww(t)toevolvewithstochasticgradientdescent(SGD)andA(t)evolvebyprojectedSGD
onameansquareerrorwithbatchsizeB. LettingΨ (t) ∈ RB×M representarandomlysampled
∞
batch of B points evaluated on the limiting features {ψ (x )}B and η to be the learning rate,
∞ µ µ=1
3Preprint
ourupdatestaketheform
(cid:18) (cid:19)
1 1
w(t+1)−w(t)=ηA(t) Ψ (t)⊤Ψ (t) v0(t), v0(t)≡w − A(t)⊤w(t)
B ∞ ∞ ⋆ N
(cid:18) (cid:19)(cid:18) (cid:19)
1 1
A(t+1)−A(t)=ηγ w(t)v0(t)⊤ Ψ (t)⊤Ψ (t) A(0)⊤A(0) . (4)
B ∞ ∞ N
Thefixedrandomprojection(cid:0)1A(0)⊤A(0)(cid:1)
presentinA(t)’sdynamicsensurethatthefeatures
N
cannot have complete access to the infinite width features ψ but only access to the initial N-
∞
dimensionalfeaturesA(0)ψ .Ifthistermwerenotpresentthentherewouldbenofiniteparameter
∞
bottlenecksinthemodelandevenamodelwithN =1couldfullyfitthetargetfunction,leadingto
trivialparameterscalinglaws1. Inthissense, thevectorspacespannedbythefeaturesψ˜ doesnot
changeoverthecourseoftraining,butthefinite-widthkernelHilbertspacedoeschangeitskernel:
ψ˜(x,t)·ψ˜(x′,t). Featurelearninginthisspaceamountstoreweighingthenormsoftheexisting
features. WehavechosenA(t)tohavedynamicssimilartothefirstlayerweightmatrixofalinear
neuralnetwork. Aswewillsee,thisisenoughtoleadtoanimprovedscalingexponent.
The hyperparameter γ sets the timescale of A’s dynamics and thus controls the rate of feature
evolution. The γ → 0 limit represents the lazy learning limit Chizat et al. (2019) where features
arestaticandcoincideswitharandomfeaturemodeldynamicsofBordelonetal.(2024a);Paquette
etal.(2024). ThetesterroraftertstepsonaN parametermodelwithbatchsizeBis
(cid:28)(cid:104) (cid:105)2(cid:29)
L(t,N,B,γ)≡ ψ ∞(x)·w∗−ψ˜(x,t)·w(t) =v0(t)⊤Λv0(t). (5)
x∼p(x)
In the next sections we will work out a theoretical description of this model as a function of the
spectrumΛandthetargetcoefficientsw⋆. Wewillthenspecializetopower-lawspectraandtarget
weightsandstudytheresultingscalinglaws.
3 DYNAMICAL MEAN FIELD THEORY OF THE MODEL
WecanconsiderthedynamicsforrandomA(0)andrandomdrawsofdataduringSGDinthelimit
of M → ∞ and N,B ≫ 12. This dimension-free theory is especially appropriate for realistic
traceclasskernelswhere⟨K (x,x′)⟩ = (cid:80) λ < ∞(equivalenttoα > 1),whichisourfocus.
∞ x k k
Define w⋆,v (t) to be respectively the components of w⋆,v0(t) in the kth eigenspace of Λ. The
k k
errorvariablesv0(t)aregivenbyastochasticprocess,andyielddeterministicpredictionfortheloss
k
L(t,N,B,γ),analogoustotheresultsofBordelonetal.(2024a).
Sincetheresultingdynamicsforv0(t)atγ > 0arenonlinearandcannotbeexpressedintermsof
k
a matrix resolvent, we utilize dynamical mean field theory (DMFT), a flexible approach for han-
dling nonlinear dynamical systems driven by random matrices (Sompolinsky & Zippelius, 1981;
Helias & Dahmen, 2020; Mannelli et al., 2019; Mignacco et al., 2020; Gerbelot et al., 2022;
Bordelon et al., 2024a). Most importantly, the theory gives closed analytical predictions for
L(t,N,B,γ). We defer the derivation and full DMFT equations to the Appendix C. The full
set of closed DMFT equations are given in Equation equation 26 for online SGD and Equation
equation 41 for offline training with data repetition. Informally, this DMFT computes a closed
setofequationsforthecorrelationandresponsefunctionsforacollectionoftime-varyingvectors
V ={v0(t),v1(t),v2(t),v3(t),v4(t)} including,
t∈{0,1,...}
1 (cid:20) δv0(t) (cid:21)
C (t,s)=v0(t)⊤Λv0(s), C (t,s)= v3(t)·v3(s), R (t,s)=TrΛ ,
0 3 N 0,2 δv2(s)⊤
where δvk(t) representstheresponseofvk attimettoasmallperturbationtovℓ attimes. The
δvℓ(s)⊤
testlosscanbecomputedfromL(t) = C (t,t). Thistheoryisderivedgenerallyforanyspectrum
0
λ andanytargetw⋆. Inthecomingsectionswewillexamineapproximatescalingbehaviorofthe
k k
losswhenthespectrumfollowsapowerlaw.
1Wecouldalsosolvethisproblembytrainingamodeloftheformf = w(t)⊤B(t)Aψwherew(t)and
B(t)aredynamicalwithinitialconditionB(0) = I andAfrozenandthematrixB(t)followinggradient
descent.WeshowthatthesetwomodelsareactuallyexhibitequivalentdynamicsinAppendixB.
2Alternatively,wecanoperateinaproportionallimitwithN/M,B/Mapproachingconstants.Thisversion
ofthetheorywouldbeexactwithnofinitesizefluctuations.
4Preprint
4 POWER LAW SCALINGS FROM POWER LAW FEATURES
We consider initial kernels that satisfy source and capacity conditions as in (Caponnetto & Vito,
2005; Pillaud-Vivien et al., 2018; Cui et al., 2021; 2023). These conditions measure the rate of
decayofthespectrumoftheinitialinfinitewidthkernelK (x,x′)andtargetfunctiony(x)inthat
∞
basis. Concretely,weconsidersettingswiththefollowingpowerlawscalings:
(cid:88)
λ ∼k−α, λ (w∗)2 ∼λ−β ∼k−αβ. (6)
k ℓ ℓ k
ℓ>k
Theexponentαiscalledthecapacityandmeasurestherateofdecayoftheinitialkerneleigenvalues.
Wewillassumethisexponentisgreaterthanunityα > 1sincethelimitingN → ∞kernelshould
be trace class. The exponent β is called the source and quantifies the difficulty of the task under
kernelregressionwithK .3 TheRKHSnorm|·|2 ofthetargetfunctionisgivenby:
∞ H
(cid:40)
(cid:88) (cid:88) 1 β >1
|y|2 = (w⋆)2 = k−α(β−1)−1 ≈ α(β−1) (7)
H k ∞ β <1.
k k
While the case of finite RKHS norm (β > 1) is often assumed in analyses of kernel methods that
relyonnorm-basedbounds, suchas(Bartlett&Mendelson,2002;Bach,2024), theβ < 1caseis
actuallymorerepresentativeofrealdatasets. Thiswaspointedoutin(Weietal.,2022). Thiscanbe
seenbyspectraldiagonalizationsperformedonrealdatasetsin(Bahrietal.,2021;Bordelonetal.,
2024a)aswellasinexperimentsinSection5.2. Westressthispointsincethebehavioroffeature
learningwithβ >1andβ <1willbestrikinglydifferentinourmodel.
General Scaling Law in the Lazy Limit For the purposes of deriving compute optimal scaling
laws, the works of Bordelon et al. (2024a) and Paquette et al. (2024) derived precise asymptotics
for the loss curves under SGD. For the purposes of deriving compute optimal scaling laws, these
asymptoticscanbeapproximatedasthefollowingsumofpowerlawsatlarget,N
1 η
lim L(t,N,B,γ)≈ t−β +N−αmin{2,β}+ t−(1− α1) + t−(2− α1). (8)
γ→0 (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125) N B
LimitingGradientFlow ModelBottleneck (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
FiniteNTransient SGDTransient
The first terms represent bottleneck/resolution-limited scalings in the sense that taking all other
quantities to infinity and studying scaling with the last (Bahri et al., 2021). The first term gives
the loss dynamics of population gradient flow (N,B → ∞) while the second (model bottleneck)
termdescribest → ∞limitofthelosswhichdependsonN. Thethirdandfourthtermsaremixed
transientsthatarisefromtheperturbativefinitemodelandbatchsizeeffects. WhileBordelonetal.
(2024a)focusedonhardtaskswhereβ < 1wherethefirsttwotermsdominatewhenconsidering
computeoptimalscalinglaws,Paquetteetal.(2024)alsodiscussedtwootherphasesoftheeasytask
regimewherethefinaltwotermscanbecomerelevantforthecomputeoptimalscaling.
GeneralScalingLawintheFeatureLearningRegime Forγ >0,approximationstoourprecise
DMFTequationsunderpowerlawspectragivethefollowingsumofpowerlaws
1 η
L(t,N,B,γ)≈ t−βmax{1, 1+2 β} +N−αmin{2,β}+ t−(1− α1)max{1, 1+2 β} + t−(2− α1)max{1, 1+2 β} .
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) N B
LimitingGradientFlow ModelBottleneck (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
FiniteNTransient SGDTransient
(9)
Weseethatintherichregime,allexponentsexceptforthemodelbottleneckareeitherthesameor
areimproved. Foreasytasksandsuper-easytaskswhereβ > 1,werecoverthesameapproximate
scaling laws as those computed in the linear model of Bordelon et al. (2024a) and Paquette et al.
(2024). For hard tasks, β < 1, all exponents except for the model bottleneck term are improved.
Belowwewillexplainwhyeachofthesetermscanexperienceanimprovementintheβ < 1case.
Weexhibitaphasediagramallofthecaseshighlightedinequation8,equation9inFigure1.
3Thesourceexponentrusedin(Pillaud-Vivienetal.,2018)andotherworksisgivenby2r=β.
5Preprint
Accelerated Training in Rich Regime The key distinction between our model and the random
feature model (γ = 0) is the limiting gradient flow dynamics, which allow for acceleration due
to feature learning. For nonzero feature learning γ > 0, our theory predicts that in the N → ∞
limit,thelossscalesasapowerlawL(t) ∼ t−χ wheretheexponentχsatisfiesthefollowingself-
consistentequation
(cid:34) (cid:35) (cid:26) (cid:27)
χ(β)=− lim 1 ln (cid:88) (w⋆)2λ exp(cid:0) −λ (cid:2) t+γt2−χ(cid:3)(cid:1) =βmax 1, 2 . (10)
t→∞lnt k k k 1+β
k
WederivethisequationinAppendixE.Weseethatifβ >1thenwehavethesamescalinglawasa
lazylearningmodelL(t)∼t−β. However,ifthetaskissufficientlyhard(β <1),thentheexponent
isincreasedtoχ= 2β >β.
1+β
This acceleration is caused by the fact that the effective dynamical kernel K(t) defined by the
dynamicsofourfeaturesψ˜(x,t)divergesasapowerlawK(t) ∼ t1−χ whenβ < 1(seeAppendix
E).Asaconsequence,attimet,themodelislearningmodek (t)∼t(2−χ)/αwhichgivesaloss
⋆
L(t)∼ (cid:88) (w k⋆)2λ
k
∼t−β(2−χ) =t−βmax{1, 1+2 β} . (11)
k>k⋆
Whileourmodelpredictsthatthescalingexponentonlychangesforhardtaskswhereβ <1,italso
predictsanoveralldecreaseintraininglossasγ increasesforeithereasyorhardtasks. InFigure2
(a)-(b)weshowthetheN,B →∞limitofourtheoryatvaryingvaluesofγ. Foreasytasksβ >1,
themodelswillalwaysfollowL∼t−β atlatetime,butwithapotentiallyreducedconstantwhenγ
islarge. Forhardtasks(Fig. 2(b))thescalingexponentimprovesL∼t− 12 +β β forγ >0.
ModelBottleneckScalings OurtheorycanbeusedtocomputefiniteN effectsintherichregime
duringSGDtraining. Inthiscase,thedynamicssmoothlytransitionbetweenfollowingthegradient
descenttrajectoryatearlytimetoanasymptotethatdependsonN ast → ∞. InFigure2(c)-(d)
weillustratetheselearningcurvesfromourtheoryandfromfiniteN simulations,showingagood
matchofthetheorytoexperiment.
We derive the asymptotic scaling of N−αmin{2,β} in Appendix E.3. Intuitively, at finite N, the
dynamics only depend on the filtered signal
(cid:0)1A(0)⊤A(0)(cid:1)
w . Thus the algorithm can only
N ⋆
estimate,atbest,thetopN componentsofw ,resultinginthefollowingt→∞loss
⋆
(cid:88)
L(N)∼ k−αβ−1 ∼N−αβ. (12)
k>N
SGD Noise Effects The variance in the learned model predictions due to random sampling of
minibatchesduringSGDalsoaltersthemeanfieldpredictionoftheloss. InFigure3,weshowSGD
noiseeffectsfromfinitebatchsizeBforhardβ <1andsupereasyβ >2−1/αtasks.
Compute Optimal Scaling Laws in Feature Learning Regime At a fixed compute budget
C = Nt, one can determine how to allocate compute towards training time t and model size N
using our derived exponents from the previous sections. Choosing N,t optimally, we derive the
followingcomputeoptimalscalinglawsL (C)inthefeaturelearningregimeγ >0. Thesearealso
⋆
summarizedinFigure1. 4
1. Hard task regime (β < 1): the compute optimum balances the population gradient flow
termt− 12 +β
β
andthemodelbottleneckN−αβ.
2. Easytasks(1 < β < 2− 1): thecomputeoptimumcomparesgradientflowtermt−β to
α
finiteN transientterm 1t−1+1/α
N
3. Supereasytasks(β >2− 1):computeoptimumbalancesthefiniteNtransient 1t−1+1/α
α N
andSGDtransientterms 1t−2+ α1.
B
4The three regimes of interest correspond to Phases I,II,III in Paquette et al. (2024). These are the only
(cid:80)
relevantregimesfortrace-class⟨K (x,x)⟩ = λ <∞(finitevariance)kernels(equivalenttoα>1).
∞ x k k
6Preprint
100
100
10 1
10 2 =0
=0 10 1 =2 3
10 3 =2 3 =2 2
=2 2 =2 1
10 4 =2 1 =20
=20 =21
10 5 =21 10 2 t
t t 2/(1+ )
10 6
100 101 102 103 104 100 101 102 103 104
t t
(a) EasyTaskβ =1.2withN,B →∞ (b) HardTaskβ =0.4withN,B →∞
100 100
N=24 N=24
N=25 N=25
10 1
N=26 N=26
N=27 N=27
N=28 10 1 N=28
10 2 N=29 N=29
N=210 N=210
N=211 N=211
t t 2/(1+ )
10 3 Theory Theory
10 2
100 101 102 100 101 102
t t
(c) EasyTaskβ =1.2withfiniteN (d) HardTaskβ =0.4withfiniteN
Figure2: Thelearningdynamicsofourmodelunderpowerlawfeaturesexhibitspowerlawscaling
with an exponent that depends on task difficulty. Dashed black lines represent solutions to the
dynamicalmeanfieldtheory(DMFT)whilecoloredlinesandshadedregionsrepresentmeansand
errorbars over 32 random experiments. (a) For easy tasks with source exponent β > 1, the loss
is improved with feature learning but the exponent of the power law is unchanged. We plot the
approximationL ∼ t−β inblue. (b)Forhardtaskswhereβ < 1, thepowerlawscalingexponent
improves. An approximation of our learning curves predicts a new exponent L ∼ t− 12 +β β which
matchestheexactN,B →∞equations. (c)-(d)Themeanfieldtheoryaccuratelycapturesthefinite
N effectsinboththeeasyandhardtaskregimes. AsN →∞thecurveapproachest−βmax{1, 1+2 β}.
TaskDifficulty Hardβ <1 Easy1<β <2−1/α Super-Easyβ >2−1/α
Lazy(γ =0) αβ αβ 1− 1
α+1 αβ+1 2α
Rich(γ >0) 2αβ αβ 1− 1
α(1+β)+2 αβ+1 2α
Table 1: Compute optimal scaling exponents r
C
for the loss L ⋆(C) ∼ C−rC for tasks of varying
difficulty in the feature learning regime. For β > 1, the exponents coincide with the lazy model
analyzedbyBordelonetal.(2024a);Paquetteetal.(2024),whileforhardtaskstheyareimproved.
We work out the complete compute optimal scaling laws for these three settings by imposing the
constraintC = Nt,identifyingtheoptimalchoiceofN andtatfixedtandverifyingtheassumed
dominantbalance. WesummarizethethreepossiblecomputescalingexponentsinTable1.
InFigure4wecomparethecomputeoptimalscalinglawsinthehardandeasyregimes. Weshow
thatthepredictedexponentsareaccurate.InFigure3weillustratetheinfluenceofSGDnoiseonthe
learningcurveinthesupereasyregimeanddemonstratethatthelargeC computeoptimalscaling
lawisgivenbyC−1+ 21 α.
7
)t(
)t(
)t(
)t(Preprint
100 100
B = 1 B = 1
B = 2 101 B = 2
B = 4 B = 4
B = 8 102 B = 8
B = 16 B = 16
101 B B = = 3 62 4 103 B B = = 3 62 4
t Th2 eo/(1 ry+) 104 t t 2+1/
t 1+1/
105 Theory
102 106
100 101 102 100 101 102
t t
(a) SGDTransientsHardβ =0.5 (b) SGDTransientsSuper-Easyβ =3.6
Figure3: SGDTransientsinfeaturelearningregime. (a)Inthehardregime,theSGDnoisedoes
notsignificantlyalterthescalingbehavior, butdoesaddsomeadditionalvariancetothepredictor.
AsB → ∞,thelossconvergestothet−2β/(1+β) scaling. (b)Inthesuper-easyregime,themodel
transitionsfromgradientflowscalingt−β toaSGDnoiselimitedscaling 1t−2+1/αandfinallytoa
B
finiteN transientscaling 1t−1+1/α.
N
100 C + 100 C +1 100 C +1
Theory Theory C1+21
Theory
101 101
102 102
101
103
103
104
102 103 104 105 102 103 104 105 106 102 103 104 105 106
Compute C Compute C Compute C
(a) HardTaskScalingβ =0.5 (b) EasyTaskβ =1.25 (c) SuperEasyβ =1.75>2− 1.
α
Figure 4: Compute optimal scalings in the feature learning regime (γ = 0.75). Dashed black
lines are the full DMFT predictions. (a) In the β < 1 regime the compute optimal scaling law
is determined by a tradeoff between the bottleneck scalings for training time t and model size N,
giving L ⋆(C) ∼ C− αα ββ +χ χ where χ = 12 +β
β
is the time-exponent for hard tasks inthe rich-regime.
(b) In the easy task regime 1 < β < 2− 1, the large C scaling is determined by a competition
α
between the bottleneck scaling in time t and the leading order 1/N correction to the dynamics
L ⋆(C) ∼ C− αα β+β 1. (c)Inthesuper-easyregime, thescalingexponentatlargecomputeisderived
bybalancingtheSGDnoiseeffectswiththe1/N transients.
5 EXPERIMENTS WITH DEEP NONLINEAR NEURAL NETWORKS
Whileourtheoryaccuratelydescribessimulationsofoursolvablemodel,wenowaimtotestifthese
newexponentsarepredictivewhentrainingdeepnonlinearneuralnetworks.
5.1 SOBOLEVSPACESONTHECIRCLE
We first consider training multilayer nonlinear MLPs with nonlinear activation function ϕ(h) =
[ReLU(h)]qϕ inthemeanfieldparameterization/µP(Geigeretal.,2020;Yang&Hu,2021;Borde-
lon&Pehlevan,2022)withdimensionless(width-independent)featurelearningparameterγ . We
0
considerfittingtargetfunctionsy(x)withx=[cos(θ),sin(θ)]⊤onthecircleθ ∈[0,2π].Theeigen-
functionsforrandomlyinitializedinfinite widthnetworksaretheFourierharmonics. Weconsider
targetfunctionsy(θ)withpower-lawFourierspectrawhilethekernelsatinitializationK(θ,θ′)also
admitaFouriereigenexpansion
∞ ∞
(cid:88) (cid:88)
y(θ)= k−qcos(kθ), K(θ,θ′)= λ cos(k(θ−θ′)). (13)
k
k=1 k=1
8
)C(
)t(
)C(
)t(
)C(Preprint
Weshowthattheeigenvaluesofthekernelatinitializationdecayasλ
k
∼k−2qϕ intheAppendixA.
Thecapacityandsourceexponentsα,β requiredforourtheorycanbecomputedfromqandq as
ϕ
2q−1
α=2q , β = (14)
ϕ 2q
ϕ
Thus task difficulty can be manipulated by altering the target function or the nonlinear activation
function of the neural network. We show in Figure 5 examples of online training in this kind
of network on tasks and architectures of varying β. In all cases, our theoretical prediction of
t−βmax{1, 1+2 β}providesaveryaccuratepredictionofthescalinglaw.
100 100
10 1
10 1
10 2 =0.10
10 3 = =0 0. .2 55 0 10 2
=0.75 =0.50
10 4 =1.00 10 3 =0.75
=1.25 =1.00
10 5 =1.50 =1.25
t max{1,2/(1+ )} 10 4 t max{1,2/(1+ )}
10 6
101 102 103 104 101 102 103 104
t t
(a) ReLU(q =1.0)withvaryingβ = 2q−1 (b) Varyingq withfixedtarget(q=1.4)
ϕ 2qϕ ϕ
Figure5: Changingthetargetfunction’sFourierspectrumordetailsoftheneuralarchitecturecan
changethescalinglawinnonlinearnetworkstrainedonline. (a)Ourpredictedexponentsarecom-
paredtoSGDtraininginaReLUnetwork. Theexponentβ isvariedbychangingq,thedecayrate
for the target function’s Fourier spectrum. The scaling laws are well predicted by our toy model
t−βmax{1, 1+2 β}. (b) The learning exponent for a fixed target function can also be manipulated by
changingpropertiesofthemodelsuchastheactivationfunctionq .
ϕ
5.2 COMPUTERVISIONTASKS(MNISTANDCIFAR)
We next study networks trained on MNIST and CIFAR image recognition tasks. Our motivation
is to study networks training in the online setting over several orders of magnitude in time. To
this end, we adopt larger versions of these datasets: “MNIST-1M” and CIAFR-5M. We generate
MNIST-1Musingthedenoisingdiffusionmodel(Hoetal.,2020)inPearce(2022). WeuseCIFAR-
5MfromNakkiranetal.(2021). EarlierresultsinRefinettietal.(2023)showthatnetworkstrained
onCIFAR-5MhaveverysimilartrajectoriestothosetrainedonCIFAR-10withoutrepetition. The
resulting scaling plots are provided in Figure 6. MNIST-1M scaling is very well captured by the
ourtheoreticalscalingexponents. TheCIFAR-5Mscalinglawexponentatlargeγ firstfollowsour
0
predictions,butlaterentersaregimewithascalingexponentlargerthanwhatourtheoreticalmodel
predicts.
6 DISCUSSION
Weproposedasimplemodeloflearningcurvesintherichregimewheretheoriginalfeaturescan
evolve as a linear combination of the initial features. While the theory can give a quantitatively
accuratefortheonlinelearningscalingexponentintherichregimeforhardtasks, theCIFAR-5M
experimentsuggeststhatadditionaleffectsinnonlinearnetworkscanoccuraftersufficienttraining.
However,therearemanyweakerpredictionsofourtheorythatwesuspecttoholdinawidersetof
settings,whichweenumeratebelow.
Source Hypothesis: Feature Learning Only Improves Scaling For β < 1. Our model makes
a general prediction that feature learning does not improve the scaling laws for tasks within the
9
)t( )t(Preprint
10 1
0=1e-02
0=3e-02
0=1e-01
0=3e-01 Linearized
0=1e+00 0=0.25
10 2 t t 2/(1+ ) 10 1 0 0= =0 0. .3 53 0
t
t 2/(1+ )
100 101 102 103 100 101 102 103 104 105
t Steps
(a) CNNsonMNIST-1M,β =0.30 (b) CNNsonCIFAR-5Mβ =0.075
Figure 6: The improved scaling law with training time gives better predictions for training deep
networks on real data, but still slightly underestimate improvements to the scaling law for CNNs
trained on CIFAR-5M, especially at large richness γ . (a)-(b) Training on MNIST-1M is well de-
0
scribedbythenewpowerlawexponentfromourtheory. (c)CNNtrainingonCIFAR-5Misinitially
welldescribedbyournewexponent,buteventuallyachievesabetterpowerlaw.
RKHS of the initial infinite width kernel. Our experiments with ReLU networks fitting functions
in different Sobolev spaces with β > 1 support this hypothesis. Since many tasks using real data
appear to fall outside the RKHS of the initial infinite width kernels, this hypothesis suggests that
lazy learning would not be adequate to describe neural scaling laws on real data, consistent with
empiricalfindings(Vyasetal.,2022).
InsignificanceofSGDforHardTasks RecentempiricalworkhasfoundthatSGDnoisehaslittle
impactinonlinetrainingofdeeplearningmodels(Vyasetal.,2023b;Zhaoetal.,2024). Ourtheory
suggests this may be due to the fact that SGD transients are always suppressed for realistic tasks
which are often outside the RKHS of the initial kernel. The regime in which feature learning can
improvethescalinglawinourmodelispreciselytheregimewhereSGDtransientshavenoimpact
onthescalingbehavior.
Ordering of Models in Lazy Limit Preserved in Feature Learning Regime An additional in-
terestingpredictionofourtheoryisthattheorderingofmodelsbyperformanceinthelazyregime
ispreservedisthesameastheorderingofmodelsinthefeaturelearningregime. IfmodelAoutper-
formsmodelBonataskinthelazylimit(β > β ),thenmodelAwillalsoperformbetterinthe
A B
richregimeχ(β )>χ(β )(seeFigure1).Thissuggestsusingkernellimitsofneuralarchitectures
A B
for fast initial architecture search may be viable, despite failing to capture feature learning (Park
etal.,2020). Thispredictiondeservesagreaterdegreeofstresstesting.
Limitations and Future Directions There are many limitations to the current theory. First, we
study mean square error loss with SGD updates, while most modern models are trained on cross-
entropy loss with adaptive optimizers (Everett et al., 2024). Understanding the effect of adaptive
optimizers or preconditioned updates on the scaling laws represents an important future direction.
In addition, our model treats the learned features as linear combinations of the initial features, an
assumptionwhichmaybeviolatedinfinitewidthneuralnetworks. Lastly,whileourtheoryisvery
descriptive of nonlinear networks on several tasks, we did identify a noticeable disagreement on
CIFAR-5M after sufficient amounts of training. Versions of our model where the learned features
arenotwithinthespanoftheinitialfeaturesorwherethematrixAundergoesdifferentdynamics
mayprovideapromisingavenueoffutureresearchtoderiveeffectivemodelsofneuralscalinglaws.
ACKNOWLEDGEMENTS
We would like to thank Jacob Zavatone-Veth, Jascha Sohl-Dickstein, Courtney Paquette, Bruno
Loureiro,andYasamanBahriforusefuldiscussions.
10
ssoL
tseT
ssoL
tseTPreprint
B.B. is supported by a Google PhD Fellowship. A.A. is supported by a Fannie and John Hertz
Fellowship. C.P. is supported by NSF grant DMS-2134157, NSF CAREER Award IIS-2239780,
andaSloanResearchFellowship. ThisworkhasbeenmadepossibleinpartbyagiftfromtheChan
Zuckerberg Initiative Foundation to establish the Kempner Institute for the Study of Natural and
ArtificialIntelligence.
REFERENCES
EmmanuelAbbe,EnricBoix-Adsera,MatthewSBrennan,GuyBresler,andDheerajNagaraj. The
staircaseproperty: Howhierarchicalstructurecanguidedeeplearning. AdvancesinNeuralIn-
formationProcessingSystems,34:26989–27002,2021.
EmmanuelAbbe,EnricBoixAdsera,andTheodorMisiakiewicz. Sgdlearningonneuralnetworks:
leapcomplexityandsaddle-to-saddledynamics.InTheThirtySixthAnnualConferenceonLearn-
ingTheory,pp.2552–2623.PMLR,2023.
Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Ale-
man,DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technical
report. arXivpreprintarXiv:2303.08774,2023.
Alexander Atanasov, Blake Bordelon, and Cengiz Pehlevan. Neural networks as kernel learners:
The silent alignment effect. In International Conference on Learning Representations, 2022.
URLhttps://openreview.net/forum?id=1NvflqAdoom.
Alexander Atanasov, Blake Bordelon, Sabarish Sainathan, and Cengiz Pehlevan. The onset of
variance-limited behavior for networks in the lazy and rich regimes. In The Eleventh Interna-
tional Conference on Learning Representations, 2023. URL https://openreview.net/
forum?id=JLINxPOVTh7.
AlexanderBAtanasov,JacobAZavatone-Veth,andCengizPehlevan. Scalingandrenormalization
inhigh-dimensionalregression. arXivpreprintarXiv:2405.00592,2024.
Jimmy Ba, Murat A Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-
dimensionalasymptoticsoffeaturelearning: Howonegradientstepimprovestherepresentation.
AdvancesinNeuralInformationProcessingSystems,35:37932–37946,2022.
FrancisBach. Learningtheoryfromfirstprinciples. MITpress,2024.
Yasaman Bahri, Ethan Dyer, Jared Kaplan, Jaehoon Lee, and Utkarsh Sharma. Explaining neural
scalinglaws. arXivpreprintarXiv:2102.06701,2021.
Lorenzo Bardone and Sebastian Goldt. Sliding down the stairs: how correlated latent variables
acceleratelearningwithneuralnetworks. arXivpreprintarXiv:2404.08602,2024.
PeterLBartlettandShaharMendelson. Rademacherandgaussiancomplexities: Riskboundsand
structuralresults. JournalofMachineLearningResearch,3(Nov):463–482,2002.
BlakeBordelonandCengizPehlevan. Self-consistentdynamicalfieldtheoryofkernelevolutionin
wideneuralnetworks. arXivpreprintarXiv:2205.09653,2022.
BlakeBordelon,AbdulkadirCanatar,andCengizPehlevan. Spectrumdependentlearningcurvesin
kernelregressionandwideneuralnetworks. InInternationalConferenceonMachineLearning,
pp.1024–1034.PMLR,2020.
Blake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris Hanin, and Cengiz Pehlevan. Depthwise
hyperparametertransferinresidualnetworks: Dynamicsandscalinglimit,2023.
BlakeBordelon,AlexanderAtanasov,andCengizPehlevan. Adynamicalmodelofneuralscaling
laws. arXivpreprintarXiv:2402.01092,2024a.
BlakeBordelon,HamzaTahirChaudhry,andCengizPehlevan. Infinitelimitsofmulti-headtrans-
formerdynamics. arXivpreprintarXiv:2405.15712,2024b.
AndreaCaponnettoandErnestoDeVito. Fastratesforregularizedleast-squaresalgorithm. 2005.
11Preprint
LenaicChizat,EdouardOyallon,andFrancisBach. Onlazytrainingindifferentiableprogramming.
Advancesinneuralinformationprocessingsystems,32,2019.
Hugo Cui, Bruno Loureiro, Florent Krzakala, and Lenka Zdeborova´. Generalization error rates
in kernel regression: The crossover from the noiseless to noisy regime. Advances in Neural
InformationProcessingSystems,34:10131–10143,2021.
HugoCui,BrunoLoureiro,FlorentKrzakala,andLenkaZdeborova´. Errorscalinglawsforkernel
classificationundersourceandcapacityconditions. MachineLearning: ScienceandTechnology,
4(3):035033,2023.
YatinDandi,FlorentKrzakala,BrunoLoureiro,LucaPesce,andLudovicStephan. Howtwo-layer
neural networks learn, one (giant) step at a time. In NeurIPS 2023 Workshop on Mathemat-
ics of Modern Machine Learning, 2023. URL https://openreview.net/forum?id=
iBDcaBLhz2.
SimonSDu,WeiHu,andJasonDLee. Algorithmicregularizationinlearningdeephomogeneous
models: Layersareautomaticallybalanced. Advancesinneuralinformationprocessingsystems,
31,2018.
Katie Everett, Lechao Xiao, Mitchell Wortsman, Alexander A Alemi, Roman Novak, Peter J Liu,
Izzeddin Gur, Jascha Sohl-Dickstein, Leslie Pack Kaelbling, Jaehoon Lee, et al. Scaling expo-
nentsacrossparameterizationsandoptimizers. arXivpreprintarXiv:2407.05872,2024.
Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M. Roy,
and Surya Ganguli. Deep learning versus kernel learning: an empirical study of loss landscape
geometryandthetimeevolutionoftheneuraltangentkernel. InHugoLarochelle,Marc’Aurelio
Ranzato,RaiaHadsell,Maria-FlorinaBalcan,andHsuan-TienLin(eds.),AdvancesinNeuralIn-
formationProcessingSystems33:AnnualConferenceonNeuralInformationProcessingSystems
2020,NeurIPS2020,December6-12,2020,virtual,2020.
Mario Geiger, Stefano Spigler, Arthur Jacot, and Matthieu Wyart. Disentangling feature and lazy
trainingindeepneuralnetworks. JournalofStatisticalMechanics:TheoryandExperiment,2020
(11):113301,2020.
CedricGerbelot,EmanueleTroiani,FrancescaMignacco,FlorentKrzakala,andLenkaZdeborova.
Rigorous dynamical mean field theory for stochastic gradient descent methods. arXiv preprint
arXiv:2210.06591,2022.
Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, and Andrea Montanari. When do neural
networksoutperformkernelmethods? AdvancesinNeuralInformationProcessingSystems,33:
14820–14830,2020.
MoritzHeliasandDavidDahmen.Statisticalfieldtheoryforneuralnetworks,volume970.Springer,
2020.
JoelHestness,SharanNarang,NewshaArdalani,GregoryDiamos,HeewooJun,HassanKianinejad,
Md Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. Deep learning scaling is predictable,
empirically. arXivpreprintarXiv:1712.00409,2017.
JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. Advancesin
neuralinformationprocessingsystems,33:6840–6851,2020.
Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza
Rutherford,DiegodeLasCasas,LisaAnneHendricks,JohannesWelbl,AidanClark,etal.Train-
ingcompute-optimallargelanguagemodels. arXivpreprintarXiv:2203.15556,2022.
Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child,
Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language
models. arXivpreprintarXiv:2001.08361,2020.
LicongLin,JingfengWu,ShamMKakade,PeterLBartlett,andJasonDLee.Scalinglawsinlinear
regression: Compute,parameters,anddata. arXivpreprintarXiv:2406.08466,2024.
12Preprint
AlexanderMaloney,DanielARoberts,andJamesSully. Asolvablemodelofneuralscalinglaws.
arXivpreprintarXiv:2210.16859,2022.
StefanoSaraoMannelli,FlorentKrzakala,PierfrancescoUrbani,andLenkaZdeborova. Passed&
spurious: Descentalgorithmsandlocalminimainspikedmatrix-tensormodels. Ininternational
conferenceonmachinelearning,pp.4333–4342.PMLR,2019.
SongMei, TheodorMisiakiewicz, andAndreaMontanari. Mean-fieldtheoryoftwo-layersneural
networks:dimension-freeboundsandkernellimit. InConferenceonLearningTheory,pp.2388–
2464.PMLR,2019.
Francesca Mignacco, Florent Krzakala, Pierfrancesco Urbani, and Lenka Zdeborova´. Dynamical
mean-fieldtheoryforstochasticgradientdescentingaussianmixtureclassification. Advancesin
NeuralInformationProcessingSystems,33:9540–9550,2020.
Niklas Muennighoff, Alexander M Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Noua-
maneTazi,SampoPyysalo,ThomasWolf,andColinRaffel. Scalingdata-constrainedlanguage
models. arXivpreprintarXiv:2305.16264,2023.
Preetum Nakkiran, Behnam Neyshabur, and Hanie Sedghi. The deep bootstrap framework: Good
onlinelearnersaregoodofflinegeneralizers. InInternationalConferenceonLearningRepresen-
tations,2021.
Jonas Paccolat, Leonardo Petrini, Mario Geiger, Kevin Tyloo, and Matthieu Wyart. Geometric
compressionofinvariantmanifoldsinneuralnetworks. JournalofStatisticalMechanics: Theory
andExperiment,2021(4):044001,2021.
CourtneyPaquette,KiwonLee,FabianPedregosa,andElliotPaquette. Sgdinthelarge: Average-
caseanalysis,asymptotics,andstepsizecriticality. InConferenceonLearningTheory,pp.3548–
3626.PMLR,2021.
ElliotPaquette,CourtneyPaquette,LechaoXiao,andJeffreyPennington. 4+3phasesofcompute-
optimalneuralscalinglaws. arXivpreprintarXiv:2405.15074,2024.
Daniel S Park, Jaehoon Lee, Daiyi Peng, Yuan Cao, and Jascha Sohl-Dickstein. Towards nngp-
guidedneuralarchitecturesearch. arXivpreprintarXiv:2011.06006,2020.
Tim Pearce. Conditional diffusion mnist. https://github.com/TeaPearce/
Conditional_Diffusion_MNIST,2022. Accessed: 2024-05-14.
LoucasPillaud-Vivien,AlessandroRudi,andFrancisBach. Statisticaloptimalityofstochasticgra-
dientdescentonhardlearningproblemsthroughmultiplepasses.AdvancesinNeuralInformation
ProcessingSystems,31,2018.
MariaRefinetti,AlessandroIngrosso,andSebastianGoldt. Neuralnetworkstrainedwithsgdlearn
distributions of increasing complexity. In International Conference on Machine Learning, pp.
28843–28863.PMLR,2023.
AndrewMSaxe,JamesLMcClelland,andSuryaGanguli. Exactsolutionstothenonlineardynam-
icsoflearningindeeplinearneuralnetworks. arXivpreprintarXiv:1312.6120,2013.
JamesBSimon,MadelineDickens,DhruvaKarkada,andMichaelRDeWeese. Theeigenlearning
framework:Aconservationlawperspectiveonkernelregressionandwideneuralnetworks.arXiv
preprintarXiv:2110.03922,2021.
James B Simon, Dhruva Karkada, Nikhil Ghosh, and Mikhail Belkin. More is better in modern
machine learning: when infinite overparameterization is optimal and overfitting is obligatory.
arXivpreprintarXiv:2311.14646,2023.
HaimSompolinskyandAnnetteZippelius.Dynamictheoryofthespin-glassphase.PhysicalReview
Letters,47(5):359,1981.
Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neu-
ral scaling laws: beating power law scaling via data pruning. Advances in Neural Information
ProcessingSystems,35:19523–19536,2022.
13Preprint
StefanoSpigler,MarioGeiger,andMatthieuWyart. Asymptoticlearningcurvesofkernelmethods:
empirical data versus teacher–student paradigm. Journal of Statistical Mechanics: Theory and
Experiment,2020(12):124001,2020.
NikhilVyas,YaminiBansal,andPreetumNakkiran. Limitationsofthentkforunderstandinggen-
eralizationindeeplearning. arXivpreprintarXiv:2206.10012,2022.
NikhilVyas,AlexanderAtanasov,BlakeBordelon,DepenMorwani,SabarishSainathan,andCen-
giz Pehlevan. Feature-learning networks are consistent across widths at realistic scales. arXiv
preprintarXiv:2305.18411,2023a.
Nikhil Vyas, Depen Morwani, Rosie Zhao, Gal Kaplun, Sham Kakade, and Boaz Barak. Beyond
implicitbias:Theinsignificanceofsgdnoiseinonlinelearning.arXivpreprintarXiv:2306.08590,
2023b.
AlexanderWei,WeiHu,andJacobSteinhardt.Morethanatoy:Randommatrixmodelspredicthow
real-worldneuralrepresentationsgeneralize. InInternationalConferenceonMachineLearning,
pp.23549–23588.PMLR,2022.
BlakeWoodworth,SuriyaGunasekar,JasonDLee,EdwardMoroshko,PedroSavarese,ItayGolan,
Daniel Soudry, and Nathan Srebro. Kernel and rich regimes in overparametrized models. In
ConferenceonLearningTheory,pp.3635–3673.PMLR,2020.
GregYangandEdwardJHu.Tensorprogramsiv:Featurelearningininfinite-widthneuralnetworks.
InInternationalConferenceonMachineLearning,pp.11727–11737.PMLR,2021.
Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick
Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tuning large neural networks via
zero-shot hyperparameter transfer. In A. Beygelzimer, Y. Dauphin, P. Liang, and J. Wortman
Vaughan (eds.), Advances in Neural Information Processing Systems, 2021. URL https:
//openreview.net/forum?id=Bx6qKuBM2AD.
Greg Yang, Edward J Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ry-
der, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural
networksviazero-shothyperparametertransfer. arXivpreprintarXiv:2203.03466,2022.
JacobA.Zavatone-VethandCengizPehlevan. Learningcurvesfordeepstructuredgaussianfeature
models,2023.
RosieZhao,DepenMorwani,DavidBrandfonbrener,NikhilVyas,andShamKakade. Deconstruct-
ingwhatmakesagoodoptimizerforlanguagemodels. arXivpreprintarXiv:2407.07972,2024.
14Preprint
APPENDIX
A ADDITIONAL EXPERIMENTS
100 100
10 2 10 1
10 4 q =0.75 10 2
q =1.0
=1.0
q =1.25 =1.2
10 6 q =1.5 10 3 =1.5
q =1.75 =1.8
k 2q 10 4 t
10 8
100 101 102 100 101 102 103 104
k t
(a) Kerneleigenspectraα=2q (b) EasySobolevTasks(ReLUq =1)
ϕ ϕ
Figure7: Additionalexperimentsinthesettingwithdatadrawnfromthecircle. (a)Thespectraof
kernels across different nonlinearities ϕ(h) = ReLU(h)qϕ which scale as λ
k
∼ k−2qϕ. (b) More
experimentsintheeasytaskregime,showthatfeaturelearningdoesnotalterthelongtimescaling
behaviorforβ >1.
8×100
7×100
=0.0001
=0.01
6×100 =0.05
=0.1
t
t (2 )
105 106 107 108 109
Tokens
Figure 8: A depth L = 4 decoder-only transformer (16 heads with d = 128) trained on next-
head
wordpredictionwithSGDontheC4datasettokenizedwiththeSentencePiecetokenizer. Weplot
cross entropy loss and fit a powerlaw t−β to lazy learning curve γ = 10−4 over the interval from
106 to 3×109 tokens. We then compute the new predicted exponent t−β(2−β) and compare to a
simulationatγ =0.1. Thoughourtheoreticalpredictionofadoublingofthescalingexponentwas
derivedinthecontextofMSE,thenewscalingexponentfitsthedatasomewhatwellforthissetting
atearlytimes.
B FURTHER DISCUSSION OF MODELS
Weseekamodelthatincorporatesboththebottle-neckingeffectsoffinitewidthobservedinrecent
linear random feature models of scaling laws while still allowing for a notion of feature learning.
Exactlysolvablemodelsoffeaturelearningnetworksarerelativelyrare. Here,wetakeinspiration
fromthelinearneuralnetworkliteratureSaxeetal.(2013).Linearneuralnetworksexhibitbothlazy
andrichregimesoflearningWoodworthetal.(2020),inwhichtheycanlearnusefultask-relevant
featuresinawaythatcanbeanalyticallystudiedAtanasovetal.(2022). Inourwork,wegobeyond
thesemodelsoflinearneuralnetworksandshowthatlinearneuralnetworkstrainedondataunder
source and capacity conditions can improve the convergence rate compared to that predicted by
kerneltheory.
15
k
ssoL
yportnE
ssorC
)t(Preprint
𝑓 𝒙 𝑓 𝒙 𝑓 𝒙
𝑁 𝑁 𝑁 𝑁
𝐴0 𝑤 𝑡 𝐴𝑡 𝑤 𝑡 𝐴0 𝐵 𝑡 𝑤 𝑡
𝑀 𝑀 𝑀
(a) (b) (c)
Figure 9: Three different models studied in this work and prior work. Black weights are frozen
whileorangeweightsaretrainable. a)Alinearrandomfeaturemodelwithonlythereadoutweights
trainable. ThismodelwasstudiedinMaloneyetal.(2022);Bordelonetal.(2024a);Paquetteetal.
(2024)asasolvablemodelofneuralscalinglaws. b)Atwolayerlinearnetworkwithbothweights
trainable. Thismodeldoesnotincurabottleneckduetofinitewidthbutundergoesfeaturelearning,
whichimprovesthescalingofthelosswithtime. WestudypurelinearneuralnetworksinAppendix
G.Inthemaintext,wetrainthismodelwithaprojectedversionofgradientdescent. Thisisequiv-
alent to c) and gives both finite-parameter bottlenecks as well as improvements to scaling due to
featurelearning.
Themodelintroducedinsection2isgivebyatwo-layerlinearnetworkactingontheψ (x):
∞
1
f(x)= w⊤Aψ (x). (15)
N ∞
There, weconstrainedittoupdateitsweightsbyaformofprojectedgradientdescentasgivenby
Equation4. Here,weshowthatrunningthisprojectedgradientdescentisequivalenttorunningor-
dinarygradientdescentonatwolayerlinearnetworkafterpassingψ throughrandomprojections.
∞
DefineA =A(0)andB(t)=A(t)A+where+denotestheMoore-Penrosepsuedoinverse.Then
0 0
assumingN <M andA isrankN,wehave
0
B(t)A =A(t), B(0)=I . (16)
0 N×N
Nowconsidertakingψ andpassingitthroughA ,andthentrainingB,wwithordinarygradient
∞ 0
descent. Wehaveupdateequations:
(cid:18) (cid:19)
1
w(t+1)−w(t)=ηB(t)A Ψ⊤Ψ v0(t)
0 B ∞ ∞
(17)
(cid:18) (cid:19)
1 1
B(t+1)−B(t)=ηw(t)v0(t)⊤ Ψ⊤Ψ A⊤
B ∞ ∞ N 0
MultiplyingthesecondequationbyA ontherightrecoversequation4. Here,γ actsasarescaling
0
ofthelearningratefortheBupdateequations.Weillustratethismodel,aswellasthelinearrandom
featureandlinearneuralnetworkmodelsinFigure9.
SeveralpapersMaloneyetal.(2022);Atanasovetal.(2023);Bordelonetal.(2024a);Atanasovetal.
(2024)havestudiedthemodelgiveninequation15withfrozenAunderthefollowinginterpretation.
Thesamplesψ ∈RD correspondtothedatasetasexpressedinthespaceofaninfinitelywideNTK
atinitialization. ThesearepassedthroughasetoffrozenrandomweightsW ,whicharethoughtof
1
astheprojectionfromtheinfinitewidthnetworktothefinite-widthempiricalNTK,corresponding
to a lazy network. From there, the final layer weights are not frozen and perform the analog of
regressionwiththefinite-widthNTK.InBordelonetal.(2024a),thismodelwasshowntoreproduce
many of the compute optimal scaling laws observed in practice. It was also however shown there
that the scaling laws for lazy networks are very different from those observed for feature-learning
networks.
16Preprint
Ourmotivationistodevelopasimpleandsolvablemodelofhowthefinite-widthnetworkfeatures
ψ˜(x;t) = A(t)ψ (x) might evolve to learn useful features. The projected linear model defined
∞
above states that the ψ˜ recombine themselves in such a way so that the empirical neural tangent
kernelψ˜(x;t)·ψ˜(x′;t)isbetteralignedtothetask. Thesimplemodelofalinearneuralnetworkis
richenoughtoyieldanimprovedpowerlaw,whilestillbeinganalyticallytractable.
C DERIVATION OF THE MEAN FIELD EQUATIONS
Inthissetting,wederivethemeanfieldequationsforthetypicaltestlossL(t,N,B)asafunctionof
trainingtime. Toaccomplishthis,wehavetoperformdisorderaveragesovertherandommatrices
A(0)and{Ψ(t)}∞ . Westartbydefiningthefollowingcollectionoffields
t=0
1
v0(t)=w⋆− A(t)⊤w(t)
N
1
v1(t)=Ψ(t)v0(t), v2(t)= Ψ(t)⊤v1(t)
B
1
v3(t)=A(0)v2(t), v4(t)= A(0)⊤v3(t)
N
1
vw(t)= A(0)⊤w(t) (18)
N
Fromtheseprimitivefields,wecansimplifythedynamicsofA,w(t)
(cid:88)
A(t)=A(0)+ηγ w(s)v4(s)⊤
s<t
w(t+1)=w(t)+ηA(t)v2(t)
=w(t)+ηv3(t)+η2γ(cid:88) w(s)(cid:2) v4(s)·v2(t)(cid:3)
s<t
(cid:88)
=w(t)+ηv3(t)+η2γ w(s)C (t,s) (19)
3
s<t
whereweintroducedthecorrelationfunctionC (t,s)≡ 1v3(t)·v3(s)=v2(t)·v4(s). Similarly
3 N
forv0(t)andvw(t)wehave
(cid:88)
v0(t)=w⋆−vw(t)−ηγ v4(s)C (t,s)
w
s<t
(cid:88)
vw(t+1)=vw(t)+ηv4(t)+η2γ vw(t)C (t,s) (20)
3
s<t
whereweintroducedC (t,s)≡ 1w(t)·w(s). Weseethat,conditionalonthecorrelationfunction
w N
C (t,s), the vector vw(t) can be interpreted as a linear filtered version of {v4(s)} and is thus
3 s<t
redundant. Inaddition,wenolongerhavetoworkwiththerandommatrixA(t)butcanrathertrack
projections of this matrix on vectors of interest. Since all dynamics only depend on the random
variablesV ={v0,v1,v2,v3},wethereforeonlyneedtocharacterizethejointdistributionofthese
variables.
Disorder Averages We now consider the averages over the random matrices which appear in
the dynamics {Ψ(t)} t∈N and A(0). This can be performed with either a path integral or a cavity
derivationfollowingthetechniquesofBordelonetal.(2024a).Afteraveragingover{Ψ(t)}∞ ,one
t=0
obtainsthefollowingprocessforv1(t)andv2(t)
k
v1(t)=u1(t), u1(t)∼N(0,δ(t−s)C (t,t))
0
v2(t)=u2(t)+λ v0(t), u2(t)∼N (cid:0) 0,B−1δ(t−s)λ C (t,t)(cid:1) . (21)
k k k k k k 1
17Preprint
wherethecorrelationfunctionsC andC havetheforms
0 1
C
(t,s)=(cid:88)
λ
(cid:10) v0(t)v0(s)(cid:11)
, C
(t,s)=(cid:10) v1(t)v1(s)(cid:11)
(22)
0 k k k 1
k
TheaverageoverthematrixA(0)couplesthedynamicsforv3(t),v4(t)resultinginthefollowing
1 (cid:88)
v3(t)=u3(t)+ R (t,s)v3(s), u3(t)∼N(0,C (t,s))
N 2,4 2
s<t
v4(t)=u4(t)+(cid:88) R (t,s)v2(s), u4 ∼N (cid:0) 0,N−1C (t,s)(cid:1) (23)
k k 3 k k 3
s<t
where
C
(t,s)=(cid:88)(cid:10) v2(t)v2(s)(cid:11)
, C
(t,s)=(cid:10) v3(t)v3(s)(cid:11)
(24)
2 k k 3
k
Lastly,wehavethefollowingsinglesiteequationforw(t)whichcanbeusedtocomputeC (t,s)
w
(cid:88)
w(t+1)−w(t)=ηv3(t)+η2γ C (t,s)w(s). (25)
3
s<t
FinalDMFTEquationsforourModelforOnlineSGD Thecompletegoverningequationsfor
thetestlossevolutionafteraveragingovertherandommatricescanbeobtainedfromthefollowing
stochasticprocesseswhicharedrivenbyGaussiannoisesources{u2(t),u3(t),u4(t)}. Letting⟨·⟩
k k
representaveragesoverthesesourcesofnoise,theequationscloseas
(cid:88)
v0(t)=w⋆−vw(t)−ηγ C (t,s)v4(s)
k k k w k
s<t
(cid:88)
vw(t+1)=vw(t)+ηv4(t)+η2γ C (t,s)vw(s)
k k k 3 k
s<t
v2(t)=u2(t)+λ v0(t), u2(t)∼N (cid:0) 0,B−1λ δ(t−s)C (t,t)(cid:1)
k k k k k k 0
1 (cid:88)
v3(t)=u3(t)+ R (t,s)v3(s), u3(t)∼N(0,C (t,s))
N 2,4 2
s<t
(cid:88)
w(t+1)=w(t)+ηv3(t)+η2γ C (t,s)w(s)
3
s<t
(cid:88)
v4(t)=u4(t)+ R (t,s)v2(s), u4(t)∼N(0,N−1C (t,s))
k k 3 k k 3
s<t
(cid:88)(cid:28) ∂v2(t)(cid:29) (cid:28) ∂v3(t)(cid:29)
R (t,s)= k , R (t,s)=
2,4 ∂u4(s) 3 ∂u3(s)
k k
C
(t,s)=(cid:88)
λ
(cid:10) v0(t)v0(s)(cid:11)
, C
(t,s)=(cid:88)(cid:10) v2(t)v2(s)(cid:11)
(26a)
0 k k k 2 k k
k k
Closing the DMFT Correlation and Response as Time × Time Matrices We can try using
these equations to write a closed form expression for v0(t) which determines the generalization
k
error. First, we start by solving the equations for v kw = Vec{v kw(t)} t∈N. We introduce a step
functionmatrixwhichisjustlowertriangularmatrix[Θ] =ηΘ(t−s)
t,s
vw =[I−ηγΘC ]−1Θv4 ≡Hwv4
k 3 k k k
Hw ≡[I−ηγΘC ]−1Θ (27)
k 3
Now,combiningthiswiththeequationforv0 =Vec{v0(t)}wefind
k k
v0 =H0(cid:2) w⋆1−(Hw+ηγC )(cid:0) u4 +R u2(cid:1)(cid:3)
k k k k w k 3 k
H0 =[I+λ (Hw+ηγC )R ]−1 (28)
k k k w 3
18Preprint
ThekeyresponsefunctionisR withentries[R ] =R (t,s)satisfiesthefollowingequation
3 3 t,s 3
M
1 (cid:88)
R =I+ R R , R =− λ H0(Hw+ηγC )
3 N 2,4 3 2,4 k k k w
k=1
M
1 (cid:88)
=⇒R =I− λ H0(Hw+ηγC )R
3 N k k k w 3
k=1
M
=I− 1 (cid:88) λ [I+λ (Hw+ηγC )R ]−1(Hw+ηγC )R (29)
N k k k w 3 k w 3
k=1
Lastly,wecancomputethecorrelationmatrixC fromthecovarianceC
w 3
C =[I−ηγC ]−1ΘC Θ⊤[I−ηγC ]−1⊤ (30)
w 3 3 3
Theremainingcorrelationfunctionsaredefinedas
(cid:20) (cid:18) (cid:19) (cid:21)
C =(cid:88) λ H0 (w⋆)211⊤+(Hw+ηγC ) 1 C + λ kR diag(C )R⊤ (Hw+ηγC )⊤ [H0]⊤
0 k k k k w N 3 B 3 0 3 k w k
k
C = 1 (cid:88) λ (cid:0) I−λ H0(Hw+ηγC )R (cid:1) diag(C )(cid:0) I−λ H0(Hw+ηγC )R (cid:1)⊤
2 B k k k k w 3 0 k k k w 3
k
(cid:20) (cid:21)
+(cid:88) H0 11⊤(w⋆)2+ 1 (Hw+ηγC )C (Hw+ηγC )⊤ (cid:2) H0(cid:3)⊤
k k N k w 3 k w k
k
C =R C R⊤ (31)
3 3 2 3
D OFFLINE TRAINING: TRAIN AND TEST LOSS UNDER SAMPLE REUSE
Our theory can also handle the case where samples are reused in a finite dataset Ψ ∈ RP×M. To
simplify this setting we focus on the gradient flow limit (this will preserve all of the interesting
finite-P effectswhilesimplifyingtheexpressions)
(cid:18) (cid:19)
d 1
w(t)=A(t) Ψ⊤Ψ v0(t)
dt P
(cid:18) (cid:19)(cid:18) (cid:19)
d 1 1
A(t)=γ w(t)v0(t)⊤ Ψ⊤Ψ A(0)⊤A(0) (32)
dt P N
Weintroducethefields
1
v1(t)=Ψv0(t), v2(t)= Ψ⊤v1(t) (33)
P
1
v3(t)=A(0)v2(t), v4(t)= A(0)⊤v3(t) (34)
N
sothatthedynamicscanbeexpressedas
d
w(t)=A(t)v2(t)
dt
d
A(t)=γw(t)v4(t)⊤ (35)
dt
Asbeforewealsointroducethefollowingfieldwhichshowsupinthev0(t)dynamics
1
vw(t)= A(0)⊤w(t) (36)
N
DataAverage TheaverageoverthefrozendatamatrixΨ∈RP×M
(cid:90) t
v2(t)=u2(t)+λ dsR (t,s)v0(s), u2(t)∼N(0,P−1λ C (t,s)) (37)
k k k 1 k k k 1
0
1 (cid:90) t
v1(t)=u1(t)+ dsR (t,s)v1(s), u1(t)∼N(0,C (t,s)) (38)
P 0,2 0
0
19Preprint
FeatureProjectionAverage NextweaverageoverA(0)∈RN×M withN/M =ν whichyields
(cid:90) t
v4(t)=u4(t)+ dsR (t,s)v2(s), u4(t)∼N(0,N−1C (t,s)) (39)
k k 3 k k 3
0
1 (cid:90) t
v3(t)=u3(t)+ dsR (t,s)v3(s), u3(t)∼N(0,C (t,s)) (40)
N 2,4 2
0
Wecannowsimplyplugtheseequationsintothedynamicsofw(t),vw(t),v0(t)toobtainthefinal
DMFTequations.
FinalDMFTEquationsforDataReuseSetting Thecompletegoverningequationsforthetest
lossevolutionafteraveragingovertherandommatrices{Ψ,A}canbeobtainedfromthefollowing
stochasticprocesseswhicharedrivenbyGaussiannoisesources{u2(t),u3(t),u4(t)}. Letting⟨·⟩
k k
representaveragesoverthesesourcesofnoise,theequationscloseas
(cid:90) t
v0(t)=w⋆−vw(t)−γ dsC (t,s)v4(s)
k k k w k
0
(cid:90) t
∂ vw(t)=v4(t)+γ dsC (t,s)vw(s)
t k k 3 k
0
1 (cid:90) t
v1(t)=u1(t)+ dsR (t,s)v1(s), u1(t)∼N(0,C (t,s))
P 0,2 0
0
(cid:90)
v2(t)=u2(t)+λ dsR (t,s)v0(t), u2(t)∼N (cid:0) 0,P−1λ C (t,s)(cid:1)
k k k 1 k k k 1
1 (cid:88)
v3(t)=u3(t)+ R (t,s)v3(s), u3(t)∼N(0,C (t,s))
N 2,4 2
s<t
(cid:88)
w(t+1)=w(t)+ηv3(t)+η2γ C (t,s)w(s)
3
s<t
(cid:88)
v4(t)=u4(t)+ R (t,s)v2(s), u4(t)∼N(0,N−1C (t,s))
k k 3 k k 3
s<t
(cid:88)
(cid:28) ∂v0(t)(cid:29) (cid:28) ∂v1(t)(cid:29)
R (t,s)= λ k , R (t,s)=
0,2 k ∂u2(s) 1 ∂u1(s)
k k
(cid:88)(cid:28) ∂v2(t)(cid:29) (cid:28) ∂v3(t)(cid:29)
R (t,s)= k , R (t,s)=
2,4 ∂u4(s) 3 ∂u3(s)
k k
C
(t,s)=(cid:88)
λ
(cid:10) v0(t)v0(s)(cid:11)
, C
(t,s)=(cid:10) v1(t)v1(s)(cid:11)
0 k k k 1
k
C
(t,s)=(cid:88)(cid:10) v2(t)v2(s)(cid:11)
, C
(t,s)=(cid:10) v3(t)v3(s)(cid:11)
(41a)
2 k k 3
k
The γ → 0 limit of these equations recovers the DMFT equations from Bordelon et al. (2024a)
whichanalyzedtherandomfeature(staticA)case.
E BOTTLENECK SCALINGS FOR POWER LAW FEATURES
Inthissetting,weinvestigatethescalingbehaviorofthemodelunderthesourceandcapacitycon-
ditionsdescribedinthemaintext:
λ ∼k−α, (w⋆)2λ ∼k−βα−1 (42)
k k k
E.1 TIMEBOTTLENECK
InthissectionwecomputethelossdynamicsinthelimitofN,B →∞.Westartwithaperturbative
argument that predicts a scaling law of the form L(t) ∼ t−β(2−β) for β < 1. We then use this
20Preprint
approximationtobootstrapmoreandmorepreciseestimatesoftheexponent. Thefinalpredictionis
thelimitofinfinitelymanyapproximationstepswhichrecoversL(t) ∼ t− 12 +β β. Wethenprovidea
self-consistencyderivationofthisexponenttoverifythatitisthestablefixedpointoftheexponent.
E.1.1 WARMUP: PERTURBATIONEXPANSIONOFTHEDMFTODERPARAMETERS
First, in this section, we investigate the N,B → ∞ limit of the DMFT equations and study what
happensforsmallbutfiniteγ. ThisperturbativeapproximationwillleadtoanapproximationL ∼
t−β(2−β). In later sections, we will show how to refine this approximation to arrive at our self-
consistentlycomputedexponent 2β . IntheN,B →∞limit,theDMFTequationssimplifyto
1+β
R →I , u4 →0, u2 →0, v4 →λ v0 , C →C . (43)
3 k k k k k 3 2
Thedynamicsinthislimithavetheform
(cid:88)
v0(t)=w⋆−vw(t)−ηγλ C (t,s)v0(s)
k k k k w k
s<t
(cid:88)
vw(t+1)−vw(t)=ηλ v0(t)+ηγ C (t,s)vw(s)
k k k k 2 k
s<t
(cid:88)
w(t+1)−w(t)=ηv3(t)+ηγ C (t,s)w(s)
2
s<t
C
(t,s)=(cid:88) λ2(cid:10) v0(t)v0(s)(cid:11)
, C (t,s)=⟨w(t)w(s)⟩ (44)
2 k k k w
k
TheseexactdynamicscanbesimulatedaswedoinFigure2.However,wecanobtainthecorrectrate
of convergence by studying the following Markovian continuous time approximation of the above
dynamicswhereweneglecttheextraO(γ)terminthevw(t)dynamics
k
d
v0(t)≈λ v0(t)−γλ C (t)v0(t), C (t)≡C (t,t)
dt k k k k w k w w
∂ C (t)≈C (t)+O(γ), C (t)≡C (t,t) (45)
t w 2 2 2
Thesolutionfortheerroralongthek-theigenfunctionwilltaketheform
(cid:18) (cid:90) t (cid:19)
v0(t)≈exp −λ t−γλ dsC (s) w⋆ (46)
k k k w k
0
We next solve for the dynamics of C (t) and C (t) in the leading order γ → 0 limit (under the
2 w
lineardynamics)
(cid:90)
C 2(t)∼(cid:88) λ2 k(w k⋆)2e−2λkt ∼ dkk−α−βα−1e−k−αt ∼t−β
k
(cid:26) t1−β β <1
C (t)∼ (47)
w C (∞) β >1
w
whereC (∞)isalimitingfinitevalueofC (t).
w w
v0(t) (cid:26) exp(cid:0) −λ t−γλ t2−β(cid:1) β <1
k ≈ k k (48)
w⋆ exp(−λ [1+γC (∞)]t) β >1
k k w
For β < 1, the feature learning term will eventually dominate. The mode k (t) which is being
⋆
learnedattimetsatisfies
k (t)∼t(2−β)/α (49)
⋆
whichimpliesthatthe
(cid:88)
L≈ (w⋆)2λ ∼t−β(2−β) (50)
k k
k>k⋆
However,thissolutionactuallyover-estimatestheexponent. Toderiveabetterapproximationofthe
exponent,weturntoaMarkovianperspectiveonthedynamicswhichholdsasN,B →∞.
21Preprint
E.1.2 MATRIXPERTURBATIONPERSPECTIVEONTHETIMEBOTTLENECKSCALINGWITH
MARKOVIANDYNAMICS
The limiting dynamics in the N,B → ∞ limit can also be expressed as a Markovian system in
termsofthevectorv0(t)andamatrixM(t)whichpreconditionsthegradientflowdynamics
(cid:20) (cid:21)
d 1 γ
v0(t)=−M(t)Λv0(t), M(t)= A(t)⊤A(t)+ |w(t)|2I
dt N N
d M(t)=γ(cid:0) w −v0(t)(cid:1) v0(t)⊤Λ+γΛv0(t)(cid:0) w −v0(t)(cid:1)⊤ +2γ(w −v0(t))⊤Λv0(t)I
dt ⋆ ⋆ ⋆
(51)
Wecanrewritethissystemintermsofthefunction∆(t) = Λ1/2v0(t)withy = Λ1/2w andthe
⋆
HermitiankernelmatrixK =Λ1/2M(t)Λ1/2then
d
∆(t)=−K(t)∆(t).
dt
d
K(t)=γ(y−∆(t))∆(t)⊤Λ+γΛ∆(t)(y−∆(t))⊤+2γ(y−∆(t))·∆(t)Λ (52)
dt
ThetestlosscanbeexpressedasL(t)=|∆(t)|2.
Loss Dynamics Dominated by the Last Term in Kernel Dynamics We note that the loss dy-
namicssatisfythefollowingdynamicsatlargetimet
d
L(t)=−∆(t)⊤K(t)∆(t)
dt
(cid:90) t
=−∆(t)Λ∆(t)−2γ ds[(y−∆(s))·∆(t)]∆(t)⊤Λ∆(s)
0
(cid:90) t
−2γ(∆(t)⊤Λ∆(t)) ds(y−∆(s))·∆(s)
0
(cid:90) t (cid:90) t
∼−∆(t)⊤Λ∆(t)−2γ[y·∆(t)] ds∆(t)⊤Λ∆(s)−2γ(∆(t)⊤Λ∆(t)) dsy·∆(s)
(cid:124) (cid:123)(cid:122) (cid:125) 0 0
LazyLimit (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
Subleading Dominant
(53)
(cid:20) (cid:90) t (cid:21)
≈−∆(t)⊤Λ∆(t) 1+2γ dsy·∆(s) . (54)
0
One can straightforwardly verify that the middle term is subleading compared to the final term is
thatundertheansatzthat∆ (t) ∼
exp(cid:0)
−λ
t2−χ(cid:1)
whereβ ≤ χ ≤ 1forβ < 1. Wecantherefore
k k
focusonthelasttermwhenderivingcorrectionstothescalinglaw.
IntuitionPump: PerturbativeLevel1Approximation Inthelazylimitγ → 0,K(t) = Λfor
all t. However, for γ > 0 this effective kernel matrix K(t) evolves in a task-dependent manner.
TocomputeK willapproximateM(t)withitsleadingorderdynamicsinγ,whichareobtainedby
evaluatingthev0(t)dynamicswiththelazylearningγ →0solution. Wecanthusapproximatethe
kernelmatrixK(t)dynamicsas
K(t)≈Λ+γyy⊤(I−exp(−Λt))⊤+γ(I−exp(−Λt))yy⊤
+2γ(cid:2) y⊤Λ−1(I−exp(−Λt))y(cid:3)
Λ (55)
Fromthisperspectiveweseethatthekernelhastwodynamicalcomponents. First,alowrankspike
growsinthekernel,eventuallyconvergingtotherankonematrixyy⊤. Inaddition,thereisascale
growthoftheexistingeigenvaluesduetothelastterm(cid:2) y⊤Λ−1(I−exp(−Λt))y(cid:3)
Λ,whichwill
approachthevalueoftheRKHSnormofthetargetfunctionast→∞.Theeigenvalues{K (t)}∞
k k=1
ofthekernelK(t)evolveatleadingorderasthediagonalentries. Assumingthatβ <1theseterms
22Preprint
increasewithtas
K k(t)∼λ k+2γy k2(cid:0) 1−e−λkt(cid:1) +2γλ k(cid:88)y λℓ2 (1−e−λℓt)
ℓ
ℓ
∼λ k+2γy k2(cid:0) 1−e−λkt(cid:1) +2γλ kt1−β (56)
Thedynamicsfortheerrorscanbeapproximatedas
∂
∆ (t)∼−K (t)∆ (t)
∂t k k k
(cid:18) (cid:90) t (cid:19)
(cid:112)
=⇒ ∆ (t)∼exp − dsK (s) λ w⋆
k k k k
0
∼exp(cid:0) −λ t−2γλ (w⋆)2t−2γλ t2−β(cid:1)(cid:112) λ w⋆ (57)
k k k k k k
Forsufficientlylarget,thefinaltermdominatesandthemodek (t)whichisbeinglearnedattime
⋆
tis
2−β
k ⋆(t)∼t α (58)
Thetestlossissimplythevarianceintheunlearnedmodes
(cid:88)
L(t)∼ (w⋆)2λ ∼t−β(2−β). (59)
k k
k>k⋆
Bootstrapping a More Accurate Exponent From the previous argument, we started with the
lazylearninglimitingdynamicsforv0(t) ∼ e−λktw⋆ andusedthesedynamicstoestimatetherate
k k
at which M(t) (or equivalently K(t)) changes. This lead to an improved rate of convergence for
the mode errors v0(t), which under this next order approximation decay as v0(t) ∼ e−λkt2−βw⋆.
k k k
Supposingthattheerrorsdecayatthisrate,wecanestimatethedynamicsofM(t)
dd tK k(t)≈λ k(cid:88) (w ℓ⋆)2λ ℓe−λℓt2−β ≈λ kt−β(2−β), (60)
ℓ
(cid:16) (cid:17)
=⇒ v0(t)∼exp −λ t2−β(2−β) w⋆ , (Level2Approximation) (61)
k k k
Wecanimaginecontinuingthisapproximationschemetohigherandhigherlevelswhichwillyield
aseriesofbetterapproximationstothepowerlaw

t−β Level0Approximation
t−β(2−β)
Level1Approximation
L(t)∼ t−β[2−β(2−β)] Level2Approximation (62)
.t .− .β[2−β(2−β(2−β))] Level3Approximation
WeplotthefirstfewoftheseinFigure10,showingthattheyapproachalimitasthenumberoflevels
diverges. Asn→∞,thisgeometricserieswilleventuallyconvergetot− 12 +β β.
E.2 SELF-CONSISTENTDERIVATIONOFTHEINFINITELEVELSCALINGLAWEXPONENT
Fromtheaboveargument,itmakessensetowonderwhetherornotthereexistsafixedpointtothis
seriesofapproximationsthatwillactuallyyieldthecorrectexponentinthelimitofinfinitelymany
steps. Indeedinthissection,wefindthatthislimitcanbecomputedself-consistently
d (cid:88)
M(t)≈γλ w⋆λ v0(t) (63)
dt k ℓ ℓ ℓ
ℓ
(cid:90) t
(cid:88)
=⇒ M(t)=1+γ ds w⋆λ v0(t) (64)
ℓ ℓ ℓ
0 ℓ
(cid:18) (cid:90) t (cid:19)
v (t)∼exp −λ dt′M(t′) w⋆ (65)
k k k
0
23Preprint
1.2
1.0
0.8 Level 0
Level 1
0.6 Level 2
Level 3
0.4
Level 4
Level 5
0.2
Level 6
Level
0.0
0.0 0.2 0.4 0.6 0.8 1.0 1.2
Figure10: Predictionsforthelossscalingexponentχ (β)atvaryinglevelsnoftheapproximation
n
scheme. Our final prediction is the infinite level limit which gives 2 . This agrees with a self-
1+β
consistencyargument.
Wecandefinetheintermediatevariable
(cid:88)
B(t)= w⋆λ v0(t) (66)
k k k
k
whichhasself-consistentequation
(cid:32) (cid:90) t (cid:34) (cid:90) t′ (cid:35)(cid:33)
(cid:88)
B(t)= λ (w⋆)2exp −λ dt′ 1+γ dsB(s) (67)
k k k
k 0 0
WecanseekasolutionoftheformB(t)∼t−χ. Thisyields
(cid:26) (cid:27)
2
t−χ ≈t−max{β,β(2−χ)} =⇒ χ=βmax 1, . (68)
β+1
UsingthesolutionforB(t),wecanalsoderivethescalingforthelosswhichisidenticalL(t)∼t−χ.
Wenotethatthisargumentalsoleadstoanapproximatedoublingoftheexponentcomparedtothe
lazy case for β < 1, however this is slightly disagreement with the perturbative approach which
yieldsβ(2−β)forβ <1.
Thisargumentiswherewedevelopedthegeneralexpressionthatdeterminesχwhichwasprovided
inthemaintext
(cid:34) (cid:35)
χ=− lim 1 ln (cid:88) (w⋆)2λ exp(cid:0) −λ (cid:2) t+γt2−χ(cid:3)(cid:1) . (69)
t→∞lnt k k k
k
E.3 FINITEMODELSIZEBOTTLENECK
Inthelimitoft → ∞,thedynamicsfor{v0(t)}willconvergetoafixedpointthatdependsonN.
k
Toascertainthevalueofthisfixedpoint,wefirstmustcomputetheasymptotics. First,wenotethat
24
)
(
nPreprint
correlationandresponsefunctionsreachthefollowingfixedpointbehaviors
(cid:90) t
lim dt′R (t′,s)∼ r δ(t−s)
3 3
t,s→∞
0
lim R (t,s)=r Θ(t−s)
2,4 2,4
t,s→∞
lim C (t,s)=c
w w
t,s→∞
(cid:90) t
lim dt′C (t′,s)=0 (70)
3
t,s→∞
0
whichgivesthefollowinglongtimebehavior
(cid:90) t (cid:90) t (cid:90) t′
v0(t)∼w⋆− dt′u4(t′)−λ dt′ dsR (t′,s)v0(s) (71)
k k k k 3 k
0 0 0
(cid:90) t
∼w⋆− dt′u4(t′)−λ r v0(t) (72)
k k k 3 k
0
=⇒ r ∼−(cid:88) λ k (73)
2,4 1+λ r
k 3
k
Usingtheasymptoticrelationshipbetween 1r r = ν,wearriveatthefollowingself-consistent
N 3 2,4
equationforr
3
1= 1 (cid:88) λ kr 3 (74)
N 1+λ r
k 3
k
Forpowerlawfeaturesthisgives
(cid:90) k−αr
N ≈ dk 3 ≈[r ]1/α =⇒ r ∼Nα (75)
k−αr +1 3 3
3
whichrecoversthecorrectscalinglawwithmodelsizeN.
lim C
(t,s)=(cid:88) λ k(w k⋆)2
t,s→∞ 0 (1+λ kNα)2
k
(cid:90) N (cid:90) ∞
≈N−2α dkk−αβ−1+2α+ dkk−βα−1 ∼N−αmin{2,β}
1 N
(76a)
E.4 FINITEDATABOTTLENECK(DATAREUSESETTING)
ThedatabottleneckwhentrainingonrepeatedP trainingexamplesisverysimilar. Inthiscase,the
relevantresponsefunctionstotrackareR (t,s)andR (t,s)whichhavethefollowinglargetime
1 0,2
propertiesast,s→∞
(cid:90) t
dt′R (t′,s)∼ r δ(t−s)
1 1
0
R (t,s)∼r Θ(t−s)
0,2 0,2
Underthisansatz,wefindthefollowingexpressionforr ast→∞
1
1∼ 1 (cid:88) λ kr 1 . (77)
P 1+λ r
k 1
k
25Preprint
Followinganidenticalargumentabovewefindthatr ∼Pα,resultinginthefollowingasymptotic
1
testloss
lim C
(t,s)=(cid:88) λ k(w k⋆)2
t,s→∞ 0 (1+λ kPα)2
k
(cid:90) P (cid:90) ∞
≈P−2α dkk−αβ−1+2α+ dkk−βα−1 ∼P−αmin{2,β}.
1 P
(78a)
F TRANSIENT DYNAMICS
To compute the transient 1/N and 1/B effects, it suffices to compute the scaling of a response
function/Volterrakernelatleadingorder.
F.1 LEADINGBIASCORRECTIONATFINITEN
Atleadingorderin1/N thebiascorrectionsfromfinitemodelsizecanbeobtainedfromthefollow-
ingleadingorderapproximationoftheresponsefunctionR (t,s)
3
R 3(t,s)∼δ(t−s)−
N1 (cid:88)
λ
ke−λk(tχ/β−sχ/β)+O(N−2)
k
1
∼δ(t−s)+ (tχ/β −sχ/β)−1+1/α (79)
N
(cid:110) (cid:111)
whereχ=βmax 1, 2 .FollowingPaquetteetal.(2024),wenotethatthescalingofR (t,s)−
1+β 3
δ(t−s) determines the scaling of the finite width transient. Thus the finite width effects can be
approximatedas
1
L(t,N)∼t−βmax{1, 1+2 β}+N−αmin{2,β}+ t−(1−1/α)max{1, 1+2 β}. (80)
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) N
LimitingDynamics ModelBottleneck (cid:124) (cid:123)(cid:122) (cid:125)
FiniteModelTransient
Inthecasewhereβ >1thisagreeswiththetransientderivedinPaquetteetal.(2024). Howeverfor
β <1,thefeaturelearningdynamicsacceleratethedecayrateofthisterm.
F.2 LEADINGSGDCORRECTIONATFINITEB
WestartbycomputingtheN →∞limitoftheSGDdynamicscanbeapproximatedas
C (t)≈(cid:88) (w⋆)2λ exp(cid:0) −2λ (t+γt2+χ)(cid:1) + η (cid:90) t dsK(t,s)C (s) (81)
0 k k k B 0
k 0
(cid:110) (cid:111)
whereχ=βmax 1, 2 andK(t,s)istheVolterra-kernelforSGDPaquetteetal.(2021;2024),
1+β
whichinourcasetakestheform
K(t,s)=(cid:88)
λ
exp(cid:0)
−2λ
(cid:2) (t+γt2−χ)−(s−γs2−χ)(cid:3)(cid:1)
k k
k
(cid:16)
max(1,2−χ)
max(1,2−χ)(cid:17)−(α−1)
∼ t α −s α (82)
Since the transient dynamics are again generated by the long time behavior of K(t,0), we can
approximatetheSGDdynamicsas
η
L(t,B)≈t−βmax{1, 1+2 β}+ t−(1−1/α)max{1, 1+2 β} . (83)
(cid:124) (cid:123)(cid:122) (cid:125) B
GradientFlow (cid:124) (cid:123)(cid:122) (cid:125)
SGDNoise
26Preprint
Source Cap a Mc =ity 1 L 0i 0n 0e 0a ,r NN =etw 10o 0rk , , B == 12 2. 800, =1.20 Source Cap a Mc =ity 1 L 0i 0n 0e 0a ,r NN =etw 10o 0rk , , B == 12 2. 800, =0.20
11 00 10 t0 0 0 0= = = =1 1 1 1e e e e+ - - -0 0 00 1 2 30 100
t
t0 0 0 0 2= = = = /(1 1 1 1 1e e e e ++ - - -0 0 0 )0 1 2 30
102
103
100 101 102 103 104 100 101 102 103 104
Steps Steps
(a) (b)
Figure11: LinearNetworksa) β > 1, whereacrossvaluesofγ, weobservethesameasymptotic
scalinggoingast−β aspredictedbykerneltheory. b)β <1,wherefeaturelearninglinearnetworks
achieveanimprovedscalingaspredictedbyourtheory.
Asbefore,theβ > 1caseisconsistentwiththeestimatefortheVolterrakernelscalinginPaquette
etal.(2024).
G LINEAR NETWORK DYNAMICS UNDER SOURCE AND CAPACITY
Inthissection,weshowhowinasimplelinearnetwork,theadvantageinthescalingpropertiesof
the loss due to larger γ is evident. Here, we consider a simple model of a two-layer linear neural
network trained with vanilla SGD online. We explicitly add a feature learning parameter γ. We
study when this linear network can outperform the rate of t−β given by linear regression directly
from input space. Despite its linearity, this setting is already rich enough to capture many of the
powerlawsbehaviorsobservedinrealisticmodels.
G.1 MODELDEFINITION
FollowingChizatetal.(2019),thenetworkfunctionisparameterizedas:
1
f(x;t)= (f˜(x;t)−f˜(x;0)), f˜(x;t)=w⊤Ax, (84)
γ
Weletx∈RM andtakehiddenlayerwidthN sothatA∈RN×M,w ∈RN,asinthemaintext.
Wetrainthenetworkonpowerlawdataofthefollowingform
x∼N(0,Λ), y =w∗·x. (85)
WeimposetheusualsourceandcapacityconditionsonΛandw∗asinequation6.
G.2 IMPROVEDSCALINGSONLYBELOWβ <1
Weempiricallyfindthatwhenβ > 1, largeγ networksdonotachievebetterscalingthansmallγ
ones. By contrast, when β < 1 we see an improvement to the loss scaling. We illustrate both of
thesebehaviorsinFigure11.Empirically,weobservearateoft−2β/(1+β)fortheselinearnetworks.
Thisisthesameimprovementderivedfortheprojectedgradientdescentmodelstudiedinthemain
text.
G.3 TRACKINGTHERANKONESPIKE
In the linear network setting, one can show that this improved scaling is due to the continued the
growthofarankonespikeinthefirstlayerweightsAofthelinearnetwork. Bybalancing,asinDu
27
ssoL ssoLPreprint
Linear Network, =2.00, =1.20 Linear Network, =2.00, =0.20
M=10000, N=100, B=128 M=10000, N=100, B=128
102
0=1e-03
103
0=1e-03
0=1e-02 0=1e-02
0=1e-01 0=1e-01
0=1e+00
102
t10=1e+00
t1 2/(1+ )
101
101
100 100
100 101 102 103 104 100 101 102 103 104
t t
(a) (b)
Figure12: Westudythegrowthofthespikeasmeasuredby|w|2. (a)Foreasytasks,wherew∗ is
finite,thespikegrowstofinitesize,andthenplateaus. Thisleadstoamultiplicativeimprovementin
theloss,butdoesnotchangethescalingexponent. (b)Whenthetaskishard,wcontinuestogrow
withoutbound. Boththeperturbativescalingoft1−β andthescalingt1−2β(1+β) obtainedfromthe
self-consistentequation10areplotted. Weseeexcellentagreementwiththelatterscaling.
etal.(2018),thisismatchedbythegrowthof|w|2. Thiswhichwillcontinuetogrowextensivelyin
timeDonlywhenβ <1. WeillustratethesetwocasesinFigure12.
28
2|w| 2|w|