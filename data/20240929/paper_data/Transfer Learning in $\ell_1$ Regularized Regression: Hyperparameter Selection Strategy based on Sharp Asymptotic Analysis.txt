Transfer Learning in ℓ Regularized Regression: Hyper-
1
parameter Selection Strategy based on Sharp Asymptotic
Analysis
Koki Okajima darjeeling@g.ecc.u-tokyo.ac.jp
Graduate School of Science, Department of Physics
The University of Tokyo
Tomoyuki Obuchi obuchi@i.kyoto-u.ac.jp
Graduate School of Informatics, Department of Informatics
Kyoto University
Abstract
Transfer learning techniques aim to leverage information from multiple related datasets to
enhancepredictionqualityagainstatargetdataset. Suchmethodshavebeenadoptedinthe
context of high-dimensional sparse regression, and some Lasso-based algorithms have been
invented: Trans-Lasso and Pretraining Lasso are such examples. These algorithms require
the statistician to select hyperparameters that control the extent and type of information
transfer from related datasets. However, selection strategies for these hyperparameters,
as well as the impact of these choices on the algorithm’s performance, have been largely
unexplored. To address this, we conduct a thorough, precise study of the algorithm in a
high-dimensionalsettingviaanasymptoticanalysisusingthereplicamethod. Ourapproach
reveals a surprisingly simple behavior of the algorithm: Ignoring one of the two types of
information transferred to the fine-tuning stage has little effect on generalization perfor-
mance, implyingthateffortsforhyperparameterselectioncanbesignificantlyreduced. Our
theoretical findings are also empirically supported by real-world applications on the IMDb
dataset.
1 Introduction
Theincreasingavailabilityoflarge-scaledatasetshasledtoanexpansionofmethodssuchaspretrainingand
transfer learning (Torrey & Shavlik, 2010; Weiss et al., 2016; Zhuang et al., 2021), which exploit similarities
between datasets obtained from different but related sources. By capturing general patterns and features,
which are assumed to be shared across data collected from multiple sources, one can improve the general-
ization performance of models trained for a specific task, even when the data for the task is too limited to
make accurate predictions. Transfer learning has demonstrated its efficacy across a wide range of applica-
tions, such as image recognition (Zhu et al., 2011), text categorization (Zhuang et al., 2011), bioinformatics
(Petegrosso et al., 2016), and wireless communications (Pan et al., 2011; Wang et al., 2021). It is reported
to be effective in high-dimensional statistics (Banerjee et al., 2020; Takada & Fujisawa, 2020; Bastani, 2021;
Li et al., 2021), with clinical analysis (Turki et al., 2017; Hajiramezanali et al., 2018; McGough et al., 2023;
Craigetal.,2024)beingamajorapplicationareaduetothescarcityofpatientdataandtheneedtocomply
with privacy regulations. In such applications, one must handle high-dimensional data, where the number
of features greatly exceeds the number of samples. This has sparked interest in utilizing transfer learning to
sparse high-dimensional regression, which is the standard method for analyzing such data.
Aclassicalmethodforstatisticallearningunderhigh-dimensionalsettingsisLasso(Tibshirani,1996),which
aims at simultaneously identifying the sparse set of features and their regression coefficients that are most
relevant for predicting the target variable. Its simplicity and interpretability of the results, as well as
1
4202
peS
62
]LM.tats[
1v40771.9042:viXraits convex property as an optimization problem, has made Lasso a popular choice for sparse regression
tasks, leading to a flurry of research on its theoretical properties (Candes & Tao, 2006; Zhao & Yu, 2006;
Meinshausen & Bühlmann, 2006; Wainwright, 2009; Bellec et al., 2018).
ModificationstoLassohavebeenproposedtoincorporateandtakeadvantageofauxiliarysamples. Thedata-
sharedLasso(Gross&Tibshirani,2016)andstratifiedLasso(Ollier&Viallon,2017)arebothmethodsbased
on multi-target learning, where one solves multiple regression problems with related data in a single fitting
procedure. Two more recent approaches are Trans-Lasso (Bastani, 2021; Li et al., 2021) and Pretraining
Lasso (Craig et al., 2024), both of which require two-stage regression procedures. Here, the first stage is
focused on identifying common features shared across multiple related datasets, while the second stage is
dedicated to fine-tuning the model on a specific target dataset. To simplify the explanation, we introduce
a generalized algorithm that encompasses both methods, which is referred to as Generalized Trans-Lasso
hereafter. Given a set of datasets of size K, D = {D(k)}K , each consisting of an observation vector of
k=1
dimension M k, y(k) ∈RMk, and its corresponding covariate matrix A(k) ∈RMk×N, generalized Trans-Lasso
performs the following two steps:
1. Pretraining procedure: In the first stage, all datasets are used to identify a common feature vector
xˆ which captures shared patterns and relevant features across the datasets:
1
1 K
X
xˆ(1st) =argminL(1st)(x;D), L(1st)(x;D)= ∥y(k)−A(k)x∥2+λ ∥x∥ . (1)
2 2 1 1
x∈RN
k=1
2. Fine-tuning: Here, the interest is only on a specific target dataset, which is designated by k = 1
without loss of generality. The second stage incorporates the learned feature vector’s configuration
and its support set into a modified sparse regression model. This is done by first offsetting the
observation vector y(1) by a common prediction vector deduced from the first stage, i.e. A(1)xˆ(1st).
Further, the penalty factor is also modified such that variables not selected in the first stage is
penalized more heavily than those selected. The above procedure is summarized by the following
optimization problem:
xˆ(2nd) =argminL(2nd)(x;xˆ(1st),D(1)),
x∈RN
(2)
L(2nd)(x;xˆ(1st),D(1))=
1 ∥y(1)−κA(1)xˆ(1st)−A(1)x∥2+XN (cid:16)
λ +∆λI[xˆ(1st)
=0](cid:17)
|x |,
2 2 2 i i
i=1
where I is the indicator function. Here, κ,∆λ ≥ 0 are hyperparameters that determine the extent
of information transfer from the first stage.
It is crucial to note that the performance of this algorithm is highly dependent on the choice of hyperpa-
rameters, which control the strength of knowledge transfer. Suboptimal hyperparameter selection can lead
to insufficient knowledge transfer, or negative transfer (Pan et al., 2011), where the model’s performance
is degraded by the transfer of adversarial information. Thus, the choice of appropriate hyperparameters
should be addressed with equal importance as the algorithm itself. In fact, a distinctive difference between
Trans-Lasso and Pretraining Lasso is whether one should inherit support information from the first stage
or not. While this adds an additional layer of complexity to hyperparameter selection, it also provides an
opportunity to inherit significant information from the first stage, if the support of the feature vectors be-
tween the sources are similar. Empirical investigation of the qualitative effects of hyperparameters on the
algorithm’s performance can be computationally intensive, especially in high-dimensional settings.
To address the issue of hyperparameter selection without resorting to extensive empirical research, one can
turn to the field of statistical physics, which provides a set of tools to analyze the performance of high-
dimensional statistical models in a theoretical manner. While these theoretical studies are restricted to
inevitably simplified assumptions on the statistical properties of the dataset, they often provide valuable
qualitative insights generalizable to more realistic settings. The replica method (Mezard et al., 1986; Char-
bonneau et al., 2023), in particular, has been widely used to precisely analyze high-dimensional statistical
2modelsincludingLasso(Kabashimaetal.,2009;Ranganetal.,2009;Obuchi&Kabashima,2016;Takahashi
& Kabashima, 2018; Obuchi & Kabashima, 2019; Okajima et al., 2023), and has been shown to provide
accurate predictions in many cases.
In this work, we conduct a sharp asymptotic analysis of the generalized Trans–Lasso’s performance under a
high-dimensional setting using the replica method. Our study reveals a simple heuristic strategy to select
hyperparameters. More specifically, it suffices to use either the support information or the actual value
of the feature vector xˆ(1st) obtained from the pretraining stage to achieve near-optimal performance after
fine-tuning. The theoretical findings are complemented by an application of the generalized Trans-Lasso to
a real-world dataset called the IMDb dataset, which highlights the effectiveness of this heuristic.
1.1 Related Works
Trans-Lasso, first proposed by Bastani (2021) and further studied by Li et al. (2021), is a special case of
the two-stage regression algorithm given by equation 1 and equation 2. In this variant, the second stage
omits information transfer regarding the support of xˆ(1st) (i.e., ∆λ = 0) and sets κ to unity. This method
has demonstrated efficacy in enhancing the generalization performance of Lasso regression, and has since
been extended to various regression models, such as generalized linear models (Tian & Feng, 2023; Li et al.,
2024), Gaussian graphical models (Li et al., 2023), and robust regression (Sun & Zhang, 2023). Moreover,
the method also embodies a framework to determine which datasets are adversarial in transfer learning,
allowing one to avoid negative transfer. While the problem of dataset selection is a practically important
issueintransferlearning, thepresentstudywillnotconsiderthisaspect,butratherfocusonthealgorithm’s
performance given a fixed set of datasets.
On the other hand, Pretraining Lasso controls both κ and ∆λ using a tunable interpolation parameter
s∈[0,1] as
1−s
κ=1−s, ∆λ= λ . (3)
s 2
Thisformulationallowsforacontinuoustransitionbetweentwoextremes: Settings=1reducestheproblem
toastandardsingle-datasetregression,whiles=0reducestoregressionconstrainedonthesupportofxˆ(1st).
Note that there is no choice of s which reduces Pretraining Lasso to Trans-Lasso.
Previous research has not yet established a clear understanding of the optimal hyperparameter selection
for these methods. Moreover, the introduction of additional hyperparameters, such as those proposed in
this study, has been largely unexplored in existing literature, possibly due to the computational demands
involved. A more thorough investigation into the performance of these methods is necessary to reevaluate
current,potentiallysuboptimalhyperparameterchoicesandtoprovideamorecomprehensiveunderstanding
of the algorithm’s behavior.
The analysis of our work is based on the replica method, which is a non-rigorous mathematical tool used
in a wide range of theoretical studies, such as those on spin-glass systems and high-dimensional statistical
models. The results from such analyses have been shown to be consistent with empirical results, and in
some cases later proved by mathematically rigorous methods, such as approximate message passing theory
(Bayati & Montanari, 2011; Javanmard & Montanari, 2013), adaptive interpolation (Barbier et al., 2019;
Barbier&Macris,2019),Gordoncomparisoninequalities(Stojnic,2013;Thrampoulidisetal.,2018;Miolane
& Montanari, 2021), and second-order Stein formulae (Bellec & Zhang, 2021; Bellec & Shen, 2022). The
application of the replica method to multi-stage procedures in machine learning has also been done in the
contextofknowledgedistillation(Saglietti&Zdeborová,2022),self-training(Takahashi,2024),anditerative
algorithms (Okajima & Takahashi, 2024). Our analysis can be seen as an extension of these works to this
generalized Trans-Lasso algorithm.
2 Problem Setup
As given in the Introduction, we consider a set of K datasets, each consisting of an observation vector
y(k) ∈RMk and covariate matrices A(k) ∈RMk×N. The observation vector y(k) is assumed to be generated
3from a linear model with Gaussian noise, i.e.
y(k) =A(k)r(k)+e(k), e(k) ∼N(0,(σ(k))2I ). (4)
Mk
Here, e(k) ∈ RMk is a Gaussian-distributed noise term, while r(k) ∈ RN is the true feature vector of the
underlying linear model. We consider the common and individual support model (Craig et al., 2024), where
{r(k)}K isassumedtohaveoverlappingsupport. Moreconcretely,let{I(k)}K beasetofdisjointindices,
k=1 k=0
where I(k)⊂{1,2,...,N}, |I(k)|=N , for all k =0,··· ,K, and PK N ≤N. Then, r(k) is given by
k k=0 k
r I(k ()
0)
=x( ⋆0) ∈RN0, r I(k ()
k)
=x( ⋆k) ∈RNk, r \(k {)
I(k)∪I(0)}
=0∈RN−Nk−N0, (5)
and
y(k) =A(k) x(0)+A(k) x(k)+e(k). (6)
I(0) ⋆ I(k) ⋆
Here,A(k) denotesthesubmatrixofA(k) withcolumnsindexedbyI,r(k) denotesthesubvectorofr(k) with
I I
indices in I, and \I is the shorthand of {1,2,...,N}\I . Therefore, each linear model is always affected by
the variables in the common support set I(0), with additional variables in the unique support set I(k) for
each dataset k =1,··· ,K. We refer to x(0) as the common feature vector, while x(k) as the unique feature
⋆ ⋆
vector for class k. The implicit aim of Trans-Lasso can be seen as estimating the common feature vector
x(0) in the first stage, treating the unique feature vectors in each class as noise. On the other hand, the
second stage is dedicated to estimating the unique feature vector x(1) given a noisy, biased estimator of the
⋆
common feature vector x(0), i.e. xˆ(1st).
⋆
We evaluate the performance of the generalized Trans-Lasso using the generalization error in each stage,
which is defined as the expected mean square error of the model prediction made against a set of new
observations D˜ ={(y˜(k),A˜ (k))}K :
k=1
ϵ(1st) =
1 XK
E
h ∥y˜(k)−A˜(k)xˆ(1st)∥2i
, (7)
N D,D˜ 2
k=1
for the first stage, while the generalization error for the second stage, with specific interest on class k = 1
without loss of generality, is given by
ϵ(2nd) =
1
E
h ∥y˜(1)−A˜(1)(κxˆ(1st)+xˆ(2nd))∥2i
. (8)
N D,D˜ 2
The objective of our analysis is to determine the behavior of the generalization error given the choice of
hyperparameters (λ ,λ ,κ,∆λ).
1 2
High-dimensional setup We focus on the asymptotic behavior in a high-dimensional limit, where the
length of the common and unique feature vectors, as well as the size of the datasets for each class, tend to
infinity at the same rate, i.e.
N N M
0 →π(0) =O(1), k →π(k) =O(1), k →α(k) =O(1), k =1,··· ,K, as N →∞. (9)
N N N
Also, define the proportion of the number of variables not included in any of the support sets as π(neg.) =
1−PK π(k). The covariate matrices are all assumed to have i.i.d. entries with zero mean and variance
k=0
1/N, while the true feature vectors are also assumed to have i.i.d. standard Gaussian entries. While such
a setup is a drastic simplification of real-world scenarios, it should be noted that these conditions can be
further generalized to more complex settings. For instance, the analysis can be further extended to the case
where the random matrix A(k) inherits rotationally invariant structure (Vehkaperä et al., 2016; Takahashi
&Kabashima,2018), ortothecasewheretheobservationsarecorruptedbyheavy-tailednoise(Adomaityte
et al., 2023).
43 Results of Sharp Asymptotic Analysis via the replica method
Here, we state the result of our replica analysis. The series of calculations to obtain our results is similar to
thewell-establishedprocedureusedintheanalysesformulti-stagedprocedures(Saglietti&Zdeborová,2022;
Takahashi, 2024; Okajima & Takahashi, 2024). While a brief overview of the derivation is given in the next
subsection, we refer the reader to Appendix A for a detailed calculation. Although our theoretical analysis
lacks a standing proof from the non-rigorousness of the replica method, we conjecture that the results are
exact in the limit of large N. This is supported numerically via experiments on finite-size systems, which
will be shown to be consistent with the theoretical predictions.
In fact, since the first stage of the algorithm is a standard Lasso optimization problem under Gaussian
design, it is already known that at least up to the first stage of the algorithm, the replica method yields
the same formula derivable from Approximate Message Passing (Bayati & Montanari, 2011) or Convex
Gaussian Minmax Theorem (CGMT) (Thrampoulidis et al., 2018; Miolane & Montanari, 2021). Moreover,
CGMT can be further applied to multi-stage optimization problems with independent data in each stage
(Chandrasekheretal.,2023). Theextensionofthisprooftothesecondstageofouralgorithm,whichcontains
data dependence among the two stages, is an unsolved problem that we leave for future work.
3.1 Overview of the calculation
Here, we briefly outline the derivation of the generalization errors ϵ(1st) and ϵ(2nd) using the replica method.
Readers not interested in the derivation may skip this subsection and proceed to the next subsection for the
main results.
GiventhetrainingdatasetD ={D(k)}K ,withD(k) =(A(k),y(k)), k =1,··· ,K,definethefollowingjoint
k=1
probability measure for x ∈RN and x ∈RN as
1 2
h i
P (x ,x ;D)∝w (x ,x ;D)=exp −β L(1st)(x ;D)−β L(2nd)(x ;x ,D(1)) , (10)
β1,β2 1 2 β1,β2 1 2 1 1 2 2 1
Under this measure, the generalization error ϵ(1st) and ϵ(2nd) can be expressed as
" #
ϵ(1st) = β2li →m ∞β1li →m ∞tl 1i →m 0N1 E D,D˜ ∂∂
t
1
logZ dx 1dx 2w β1,β2(x 1,x 2;D) et1PK k=1∥y˜(k)−A˜(k)x1∥2 2 , (11)
" #
1 ∂ Z
ϵ(2nd) = β2li →m ∞β1li →m ∞tl 2i →m 0NE D,D˜
∂t
2
log dx 1dx 2w β1,β2(x 1,x 2;D) et2∥y˜(1)−A˜(1)(κx1+x2)∥2 2 . (12)
Such observation stems from the fact that, in the successive limit of β → ∞ followed by β → ∞, the
1 2
joint probability measure P (x ,x ;D) concentrates around (xˆ(1st),xˆ(2nd)), the solution to the two-stage
β1,β2 1 2
Trans-Lasso procedure. The above expression requires one to evaluate the average of the logarithm of the
integral of w (x ,x ;D), which can be alternatively expressed as
β1,β2 1 2
!
Z ∂ hZ in
E log dx dx w (x ,x ;D)= lim log E dx dx w (x ,x ;D) . (13)
D 1 2 β1,β2 1 2 n→+0∂n D 1 2 β1,β2 1 2
Although the right-hand side of the above expression is not directly computable for arbitrary n ∈ R , it
≥0
turns out that under plausible assumptions, this can be evaluated for n ∈ N in the limit N → ∞. The
replica method is based on the assumption that one can analytically continue this formula, defined only for
n∈N, to n∈R . On this continuation, the limit n→+0 can be taken formally.
≥0
Intheprocessofcalculatingequation13,acrucialobservationisthatfromGaussianityofthedesignmatrices
A(k),theproductsbetweenthedesignmatricesandthefeaturevectorsx ,x areGaussianrandomvariables.
1 2
This not only simplifies the high-dimensional nature of the expectation over D, but it also reduces the
complexity of the analysis to consider only up to the second moments of these Gaussian random variables.
These moments are characterized by the inner product of the feature vectors, which happen to concentrate
to deterministic values in the limit N → ∞. One is then left with an integral over this finite number of
concentrating scalar variables, which can then be evaluated using the standard saddle-point method, again
in the limit N →∞.
53.2 Equations of State and Generalization Error
The precise asymptotic behavior of the generalization error is given by the solution of a set of non-linear
equations, which we refer to as the equations of state. They are provided as follows.
Definition 1 (Equations of statefor the First Stage). Let z ,{x(k)}K be i.i.d. standard Gaussian random
1 ⋆ k=0
variables,anddenotetheexpectationwithrespecttotheserandomvariablesasE . ForfinitevariablesΘ =
1 1
{{m(k),mˆ(k)}K ,q ,qˆ ,χ ,χˆ }, define {x(k)}K ,x(neg.) as the solution to random optimization problems as
1 1 k=0 1 1 1 1 1 k=0 1
follows:
x(k) =argminE(k)(x), E(k)(x)=
qˆ 1x2−(cid:16)p
χˆ z
+mˆ(k)x(k)(cid:17)
x+λ |x| (14)
1 1 1 2 1 1 1 ⋆ 1
x
x(neg.) =argminE(neg.)(x), E(neg.)(x)=
qˆ
1x2−p χˆ z x+λ |x|. (15)
1 1 1 2 1 1 1
x
Then, Θ is given by the solution of the following set of equations:
1
q
=XK
π(k)E
h(cid:0) x(k)(cid:1)2i
+π(neg.)E
h(cid:0) x(neg.)(cid:1)2i
, qˆ =mˆ(0) =
α(tot)
,
1 1 1 1 1 1 1 1+χ
1
k=1
h i α(k)
m(k) =π(k)E x(k)x(k) , mˆ(k) = ,
1 1 1 ⋆ 1 1+χ
1
(16)
χˆ
=XK α(k)q 1−2(m( 10)+m( 1k))+ρ(k)
,
1 (1+χ )2
1
k=1
K " # " #
χ =X π(k)E ∂ x(k) +π(neg.)E ∂ x(neg.) ,
1 1 ∂z 1 1 ∂z 1
1 1
k=1
where we have defined ρ(k) =π(k)+π(0)+(σ(k))2, and α(tot) =PK α(k).
k=1
Definition 2 (Equations of state of the Second Stage). Let z ,{x(k)}K and {x(k)}K as defined in Def-
1 ⋆ k=0 1 k=0
inition 1. For finite variables Θ = {{m(k),mˆ(k)}K ,q ,qˆ ,q ,qˆ ,χ ,χˆ ,χ ,χˆ }, and Θ predefined in
2 2 2 k=0 2 2 r r 2 2 r r 1
Definition 1, let z be a Gaussian random variable with conditional distribution:
2
!
χˆ χˆ2
z |z ∼N √ r z ,1− r , (17)
2 1 χˆ χˆ 1 χˆ χˆ
1 2 1 2
and furthermore, denote the joint expectation with respect to z ,z ,{x(k)}K as E . Let {x(k)}K ,x(neg.)
1 2 ⋆ k=0 2 2 k=0 2
be the solution to random optimization problems as follows:
x(k) =argminE(k)(x|x(k)), E(k)(x|x(k))=
qˆ 2x2−(cid:16)p
χˆ z +mˆ(k)x(k)+qˆ
x(k)(cid:17)
x+r(x|x(k)), (18)
2 2 1 2 1 2 2 2 2 ⋆ r 1 1
x
x(neg.) =argminE(neg.)(x|x(neg.)), E(neg.)(x|x(neg.))=
qˆ 2x2−(cid:16)p
χˆ z +qˆ
x(neg.)(cid:17)
x+r(x|x(neg.)), (19)
2 2 1 2 1 2 2 2 r 1 1
x
6where r(x|y)=(λ +∆λI[y =0])|x|. Then, Θ is given by the solution of the following set of equations:
2 2
q
=XK
π(k)E
h(cid:0) x(k)(cid:1)2i
+π(neg.)E
h(cid:0) x(neg.)(cid:1)2i
, qˆ =
α(1)
,
2 2 2 2 2 2 1+χ
2
k=1
q
=XK
π(k)E
h x(k)x(k)i
+π(neg.)E
h x(neg.)x(neg.)i
, qˆ
=−α(1)A
,
r 2 2 1 2 2 1 r 1+χ
2
k=1
h i α(1)B
m(k) =π(k)E x(k)x(k) , mˆ(k) = ,
2 2 2 ⋆ 2 1+χ
2
K
χ =X π(k)E h ∂ x(k)i +π(neg.)E h ∂ x(neg.)i , (20)
2 2 ∂z 2 2 ∂z 2
2 2
k=1
q +A2q +2Aq +B2ρ(1)−2B(m(0)+m(1))−2AB(m(0)+m(1))
χˆ =α(1) 2 1 r 2 2 1 1 ,
2 (1+χ )2
2
K " # " #
χ =X π(k)E ∂ x(k) +π(neg.)E ∂ x(neg.) ,
r 2 ∂z 2 2 ∂z 2
1 1
k=1
Bρ(1)−m(0)−m(1)−A(m(0)+m(1))
χˆ =−α(1) 2 2 1 1 ,
r (1+χ )(1+χ )
1 2
where A= κ−χ1,B =1− χr+κχ1.
1+χ1 1+χ1
Given these equations of state, the generalization error is given from the following claims:
Claim 1 (Generalizationerrorofthefirststage). GivenΘ , theexpectedgeneralizationerrorϵ(1st) converges
1
in the large N limit as
K
lim ϵ(1st)
=X α(k)h
q
−2(m(0)+m(k))+ρ(k)i
. (21)
1 1 1
N→∞
k=1
Claim 2 (Generalization error of the second stage). Given Θ and Θ , the expected generalization error
1 2
ϵ(2nd) converges in the large N limit as
h i
lim ϵ(2nd) =α(1) q +κ2q +2κq +ρ(1)−2(m(0)+m(1))−2κ(m(0)+m(0)) . (22)
2 1 r 2 2 1 1
N→∞
Claims 1 and 2 indicate that for a given set of hyperparameters, the asymptotic generalization error can be
computed by solving a finite set of non-linear equations in Definitions 1 and 2. Therefore, one can obtain
the optimal choice of hyperparameters minimizing the generalization error via a numerical root-finding and
optimization procedure in finite dimension.
4 Comparison with synthetic finite-size data simulations
To verify our theoretical results, we conduct a series of synthetic numerical experiments to compare the
results obtained from Claim 2 and the empirical results for the generalization error obtained from datasets
of finite-size systems.
The synthetic data is specified as follows. We consider the case of K = 2 classes, with α(1) = 0.2,0.4, or
0.6, and α(2) = 0.8. Two types of settings for the common and unique feature vectors are considered; one
with π(0) = 0.10,π(1) = π(2) = 0.09, and π(0) = 0.15,π(1) = π(2) = 0.04. These two settings are chosen
to represent the cases where the common feature vector is relatively small compared to the unique feature
vectors, and the case where the common feature vector is relatively large, but with the total number of
non-zero features underlying each class being the same (0.19N). Finally, the noise level of each dataset
is set to σ(1) = σ(2) = 0.1. The numerical simulations are performed on 64 random realizations of data
with size N =16,000. The second stage generalization error is evaluated by further generating 256 random
7(0)=0.10, (1)= (2)=0.09
( (1), (2))=(0.2,0.8) ( (1), (2))=(0.4,0.8) ( (1), (2))=(0.6,0.8)
=0.0 0.032 =0.0 0.030
0.031 =0.2 =0.2
=0.5 =0.5
0.030 =1.0 =1.0 0.028
0.030
0.029 0.026
0.028 0.028 0.024
0.027
0.022
0.026 0.026
0.025 0.020 =0.0
=0.2
0.024 0.024 0.018 =0.5
=1.0
104 103 102 101 100 104 103 102 101 100 104 103 102 101 100
(0)=0.15, (1)= (2)=0.04
( (1), (2))=(0.2,0.8) ( (1), (2))=(0.4,0.8) ( (1), (2))=(0.6,0.8)
0.0195
0.028 =0.0 =0.0
0.022 =0.2 0.0190 =0.2
0.026 =0.5 =0.5
=1.0 0.0185 =1.0
0.021
0.024
0.0180
0.022 =0.0 0.020
=0.2 0.0175
0.020 =0.5 0.019
=1.0 0.0170
0.018
0.018 0.0165
0.016
0.017 0.0160
0.014
0.0155
0.016
0.012
104 103 102 101 100 104 103 102 101 100 104 103 102 101 100
Figure 1: Comparison of generalization error obtained from Claim 2 (solid line) with finite size simulations
(markers). Error bars represent standard error obtained from 64 realizations of data.
realizations of (A˜ (1),y˜(1)) for each dataset and calculating the average empirical generalization error over
these realizations. The hyperparameters λ and λ are chosen such that they minimize the generalization
1 2
error given in Claim 1 and 2 for each stage respectively. The finite size simulations are also performed using
the same values of λ and λ used in the theory. In figure 1, we plot the generalization error of the second
1 2
stage as a function of ∆λ for κ=0.0,0.2,0.5, and 1.0. As evident from the plot, the generalization error is
in fairly good agreement with the theoretical predictions.
Case with scarce target data Let us look closely at the left column of figure 1 where the target sample
size is relatively scarce (α(1) = 0.2). For both settings of (π(0),π(1),π(2)), the generalization error is insen-
sitive to the choice of ∆λ when κ is tuned properly. Another important observation is that Trans-Lasso’s
hyperparameter choice (κ=1,∆λ=0) seems comparable with the optimal choice regarding the generaliza-
tion error. The same trend is also seen in the case where α(1) =0.4 and (π(0),π(1),π(2))=(0.15,0.04,0.04)
(bottom center subfigure of figure 1).
Case with abundant target data Next, we turn our attention to the right column of figure 1 where
the target sample size is relatively abundant (α(1) = 0.6). In this case, the generalization error is sensitive
to the choice of ∆λ but is insensitive to the choice of κ around its optimal value (κ=0.0), for both settings
of (π(0),π(1),π(2)). This indicates that under abundant target data, transferring the knowledge with respect
to the support of the pretrained feature vector xˆ(1st) is more crucial than the actual value of xˆ(1st) itself.
The same trend can be seen for the case when α(1) =0.4 and (π(0),π(1),π(2))=(0.15,0.04,0.04) (top center
subfigure of figure 1).
8
rorrE
noitazilareneG
rorrE
noitazilareneG5 Hyperparameter Selection Strategy
The experiments from the last section indicate not only the accuracy of our theoretical predictions but also
the practical implications for hyperparameter selection. The observations made from the two cases suggest
that there are roughly two behaviors for the optimal choice of hyperparameters: One where the choice of
κ is more crucial, and the other where the choice of ∆λ is more important. This observation suggests a
simple strategy for hyperparameter selection, where one fixes ∆λ=0 for the former case and κ=0 for the
latter case. To further investigate the validity of this strategy, we conduct a comparative analysis of the
generalization error among the following four strategies:
• κ=0 strategy Here, we fix κ to zero, while letting ∆λ to be a tunable parameter. This reflects on
theinsightobtainedinsubsection4fortheabundanttargetdatacase. Theregularizationparameters
λ and λ are both chosen such that they minimize the generalization error in the first and second
1 2
stages, respectively.
• ∆λ=0 strategy Here, we fix ∆λ to zero, while letting κ to be a tunable parameter. This reflects
the insight obtained in subsection 4 for the scarce target data case. The regularization parameters
λ and λ are both chosen in the same way as the κ=0 strategy.
1 2
• Locally Optimal (LO) strategyHere,bothκand∆λaretunableparameters. Theregularization
parameters λ and λ are both chosen in the same way as the κ=0 strategy.
1 2
• Globally Optimal (GO) strategy Here, all four parameters (λ ,λ ,κ,∆λ) are tuned such that
1 2
the second stage generalization error is minimized.
The last strategy requires retraining the first stage estimator xˆ(1st) for each choice of (λ ,λ ,κ,∆λ), which
1 2
is computationally demanding. Even the LO strategy can be problematic when the dataset size is large.
Basically, these two strategies are regarded as a benchmark for the other two strategies: κ=0 and ∆λ=0
strategies. Below,wecomparethesefourstrategiesintermsofthesecond-stagegeneralizationerrorcomputed
by our theoretical analysis. For simplicity, the case (π(0),π(1),π(2)) = (0.10,0.09,0.09) is investigated. In
figure 2, we plot the second stage generalization error, as well as the optimal choice of hyperparameters for
eachstrategyasafunctionofσ,for(α ,α )=(0.2,0.8),(0.3,0.8),and(0.4,0.8). Notethattheshadedgreen
1 2
region indicates the range of σ where the κ=0 strategy outperforms the ∆λ=0 strategy, while the shaded
red region indicates the opposite.
Let us first examine the behavior of optimal ∆λ across different strategies. With the exception of the case
(α ,α ) = (0.2,0.8), the optimal ∆λ values for each strategy (excluding the ∆λ = 0 strategy) are approx-
1 2
imately equal in the green region. Notably, for (α(1),α(2)) = (0.4,0.8), the κ = 0 strategy demonstrates
generalizationperformancecomparabletotheLOstrategyinthisregion,whilethe∆λ=0strategyexhibits
significantly lower performance. Above a certain noise threshold, the optimal value of ∆λ for both the LO
and GO strategies becomes strictly zero. At this point, the LO strategy and the ∆λ=0 strategy coincide,
indicating that the ∆λ=0 strategy can potentially be equivalent to the LO strategy.
Next, let us see the behavior of the optimal κ in each strategy. In the green region, where the κ = 0
strategy outperforms the ∆λ = 0 strategy, both the LO and GO strategies actually exhibit finite values of
κ. This means that the κ = 0 strategy is suboptimal. However, it still exhibits a comparable performance
with the LO strategy and gives a significant improvement from the ∆λ = 0 strategy. This underscores the
importanceof solely transferringsupport information; transferringthe first stage featurevectoritself can be
less beneficial. This may be due to the crosstalk noise induced by different datasets: Using the regression
results on other datasets than the target can induce noises on the feature vector components specific to the
target. Consequently, it may be preferable to transfer more robust information, in this case the support
configuration, rather than the noisy vector itself obtained in the first stage.
TheaboveobservationsindicatethattheperformancecloselycomparabletotheLOstrategycanbeachieved
across a wide range of noise levels by simply considering either the κ = 0 or ∆λ = 0 strategy; the κ = 0
strategy is preferred in low noise scenarios while the ∆λ = 0 strategy is better in high noise scenarios. For
practical purposes, these two strategies should be examined and the best hyperparameters should be chosen
based on comparison. This is the main message for practitioners in this paper.
91=0.2, 2=0.8 1=0.3, 2=0.8 1=0.4, 2=0.8
0.8
1.0 1.0
0.6
0.4
0.5 0.5
0.2
0.0 0.0 0.0
10 1 10 1 10 1
10 2
10 2 10 2
10 3
10 3 10 3 10 4
2.0 2.0 2.0
1.5 1.5 1.5
1.0 1.0 1.0
0.5 0.5 0.5
0.0 0.0 0.0
=0 Strategy =0 Strategy =0 Strategy
=0 Strategy =0 Strategy =0 Strategy
LO Strategy LO Strategy LO Strategy
GO Strategy GO Strategy GO Strategy
10 1 10 1
10 1
10 2
10 2 10 1 100 10 2 10 1 100 10 2 10 1 100
Figure2: Generalizationerrorandoptimalhyperparametersforeachstrategyasafunctionofσfor(α ,α )=
1 2
(0.2,0.8),(0.3,0.8),(0.4,0.8). The region shaded in red indicates the range of σ where the ∆λ = 0 strategy
outperforms the κ=0 strategy, while the region shaded in green indicates the opposite.
Comparison with the original Trans-Lasso The ∆λ = 0 strategy provides a lower bound for the
generalization error of the Trans-Lasso, as the latter constrains κ to unity instead of treating it as a tun-
able parameter. Our results demonstrate that the Trans-Lasso is suboptimal compared to strategies that
incorporate support information in the second stage, particularly in the green region of figure 2. Notably,
for (α ,α ) = (0.4,0.8) and σ = 0.01, the ∆λ = 0 strategy reduces the generalization error by over 30%
1 2
compared to the Trans-Lasso.
Ubiquityofthesimplifiedhyperparameterselection Tofurtherconfirmtheefficacyofthetwosimple
strategies,weinvestigatethedifferenceingeneralizationerrorbetweentheLOstrategyandκ=0or∆λ=0
strategy. Morespecifically,letϵ ,ϵ andϵ bethegeneralizationerrorofthecorrespondingstrategies.
LO κ=0 ∆λ=0
Figure3showstheminimumratiomin{ϵ ,ϵ }/ϵ forvariousvaluesof(α ,α ). Forcomparison, the
∆λ=0 κ=0 LO 1 2
samevaluesarecalculatedforthePretrainingLassoandTrans-Lasso, i.e. ϵ /ϵ andϵ /ϵ , with
Pretrain LO Trans LO
ϵ andϵ beingthegeneralizationerrorwiththeparameterschosenoptimally(notethatλ andλ
Pretrain Trans 1 2
arechoseninthesamewayastheLOstrategy). Theresultsindicatethat,acrossabroadrangeofparameters,
atleastoneofthesimplifiedstrategies(either∆λ=0orκ=0)achievesageneralizationerrorwithin10%of
that obtained by the more complex LO strategy. Notably, substantial relative differences are observed only
within a narrow region of the parameter space (α(1),α(2)), highlighting the effectiveness of the simplified
approach across a wide range of problem settings. We also report that for the case σ = 0.5, the ∆λ = 0
strategy always coincided with the LO strategy in the region (α(1),α(2)) ∈ [0.05,0.45]×[0.1,0.9]. Results
consistent with the above observations were also seen for other settings of (π(0),π(1),π(2)); see Appendix B.
10
.rrE
noitazilareneG
1=0.01, ( (0), (1), (2))=(0.1,0.09,0.09)
Pretraining Lasso Trans-Lasso Generalized Trans-Lasso
0.45 0.45 2.0 0.45 1.06
1.8
1.7 1.8 1.05
0.35 0.35 0.35
1.6
1.04
1.5 1.6
0.25 1.4 0.25 0.25 1.03
1.4
1.3 1.02
0.15 0.15 0.15
1.2
1.2
1.01
1.1
0.05 1.0 0.05 1.0 0.05 1.00
0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9
2 2 2
=0.1, ( (0), (1), (2))=(0.1,0.09,0.09)
Pretraining Lasso Trans-Lasso Generalized Trans-Lasso
0.45 0.45 0.45
1.35
1.05 1.04
1.30
0.35 0.35 0.35
1.04 1.25 1.03
0.25 1.03 0.25 1.20 0.25
1.02
1.15
1.02
0.15 0.15 1.10 0.15
1.01
1.01
1.05
0.05 1.00 0.05 1.00 0.05 1.00
0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9
2 2 2
Figure 3: Ratios min{ϵ ,ϵ }/ϵ ,ϵ /ϵ and ϵ /ϵ under setting (π(0),π(1),π(2)) =
∆λ=0 κ=0 LO Pretrain LO Trans LO
(0.1,0.09,0.09)andnoiselevelσ =0.01(topfigure)and0.1(bottomfigure)forvariousvaluesof(α(1),α(2)).
Note that each heatmap has its own color scale bar; see Appendix B for a direct comparison with shared
color scale bars between the κ=0 or ∆λ=0 strategy with Pretraining-Lasso and Trans-Lasso.
Theroleofλ Forlownoisescenarios,theGOstrategytendstopreferlargerλ inthefirststagecompared
1 1
to the other strategies. This observation may be beneficial when the noise level of each dataset is known a
priori: Wemaytunetheλ valuetoalargervaluethantheoneobtainedfromtheLOoritsproxystrategies
1
(κ=0 or ∆λ=0 strategies). This offers an opportunity to enhance our prediction beyond the LO strategy
while avoiding the complicated retraining process necessary for the GO strategy.
6 Real Data Application: IMDb Dataset
Toverifywhethertheaboveinsightscanbegeneralizedtoreal-worldscenarios,wehereconductexperiments
on the IMDb dataset. The IMDb dataset comprises a pair of 25,000 user-generated movie reviews from the
Internet Movie Database, each being devoted to training and test datasets. The dataset is evenly split into
highly positive or negative reviews, with negative labels assigned to ratings given 4 or lower out of 10, and
positive labels to ratings of 7 or higher. Movies labeled with only one of the three most common genres
in the dataset, drama, comedy, and horror, are considered. By representing the reviews as binary feature
vectorsusingbag-of-wordswhileignoringwordswithlessthan5occurrences,weareleftwith27743features
in total, with 8286 drama, 5027 comedy, and 3073 horror movies in the training dataset, and 9937 drama,
4774comedy,and3398horrormoviesinthetestdataset. ThedatasetscanbeacquiredfromSupplementary
Materials of Gross & Tibshirani (2016).
For sake of completeness, we calculate the test error for (κ,∆λ/λ ) ∈ {0.0,0.1,0.2,··· ,1.5} ×
2
{0.0,0.1,0.2,··· ,1.0} to see if our insights are qualitatively consistent with experiments on real data. For
each pair of (κ,∆λ/λ ), λ and λ are determined by a 10-fold cross-validation procedure, where the train-
2 1 2
ing dataset is further split into a learning dataset and a validation dataset. Since λ (and consequently
1
xˆ(1st)) must be selected oblivious to the validation dataset, we perform Approximate Leave-One-Out Cross-
Validation(Obuchi&Kabashima,2016;Rad&Maleki,2020;Stephenson&Broderick,2020)onthelearning
11
1
1Drama Comedy Action
5.7
5.90
6.05
5.85 5.6
6.00
5.80 5.5
5.95
5.75 5.4
5.90
5.70 5.3
5.65 5.85 5.2
5.60 5.80
5.1
Figure4: SecondstagetesterrorforeachIMDBgenreplottedagainstvaryingvaluesofhyperparametersκ
and∆λ/λ . Theplotsdemonstratethattheeffectof∆λongeneralizationperformanceistinyacrossgenres.
2
Method Drama Comedy Action
Gen. Trans-Lasso, ∆λ=0 5.58 5.78 5.07
Gen. Trans-Lasso, κ=0 5.67 5.90 5.48
Gen. Trans-Lasso, LO 5.58 5.78 5.19
Pretraining Lasso 5.63 5.78 5.18
Trans-Lasso 5.71 5.78 5.17
Table 1: Comparison of test errors for various Lasso-based methods and hyperparameter strategies applied
to the IMDb dataset.
dataset, which allows one to estimate the leave-one-out cross-validation error deterministically without ac-
tually performing the computationally expensive procedure. For the sake of comparison, we also compare
with the performance of Pretraining Lasso and Trans-Lasso, with λ and λ chosen using the same method
1 2
as above.
In Figure 4, we present the second stage test error for each genre across various values of (κ,∆λ/λ ). The
2
plot reveals that ∆λ is essentially a redundant hyperparameter having almost no influence on generalization
performance. This observation strongly supports the efficacy of the ∆λ=0 strategy in the present dataset.
We compare the test errors obtained from three approaches within our generalized Trans-Lasso framework:
The ∆λ = 0 strategy, the κ = 0 strategy, and LO strategy. Additionally, we include test errors from
Pretraining Lasso and the original Trans-Lasso for comparison, whose results are summarized in Table 1.
Recall that Trans-Lasso fixes κ to unity, while Pretraining Lasso employs (κ,∆λ/λ )=(1−s,(1−s)/s) for
2
hyperparameter s ∈ [0,1]. In the experiments, we choose s from {0.0,0.1,0.2,··· ,1.0} via cross-validation.
Our analysis demonstrates that the hyperparameter choices made by both the original Trans-Lasso and
PretrainingLassoaresuboptimalcomparedtothe∆λ=0strategyofourgeneralizedTrans-Lassoalgorithm.
Note that the LO strategy can potentially select hyperparameters exhibiting higher test error compared to
the ∆λ = 0 strategy (as seen in the Action genre), as they are chosen via cross-validation and the true
generalization error is not directly minimized.
7 Conclusion
In this work, we have conducted a sharp asymptotic analysis of the generalized Trans-Lasso algorithm,
precisely characterizing the effect of hyperparameters on its generalization performance. Our theoretical
calculations reveal that near-optimal generalization in this transfer learning algorithm can be achieved by
focusingonjustoneoftwomodesofknowledgetransferfromthesourcedataset. Thefirstmodetransfersonly
the support information of the features obtained from all source data, while the second transfers the actual
configurationofthefeaturevectorobtainedfromallsources. ExperimentsontheIMDbdatasetconfirmthat
this simple hyperparameter strategy is effective, outperforming conventional Lasso-type transfer algorithms.
12
2/
0.19.08.07.06.05.04.03.02.01.00.0
0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 0.1 1.1 2.1 3.1 4.1 5.1
rorrE
tseT
2/
0.19.08.07.06.05.04.03.02.01.00.0
0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 0.1 1.1 2.1 3.1 4.1 5.1
rorrE
tseT
2/
0.19.08.07.06.05.04.03.02.01.00.0
0.0 1.0 2.0 3.0 4.0 5.0 6.0 7.0 8.0 9.0 0.1 1.1 2.1 3.1 4.1 5.1
rorrE
tseTReferences
Urte Adomaityte, Leonardo Defilippis, Bruno Loureiro, and Gabriele Sicuro. High-dimensional robust re-
gression under heavy-tailed data: Asymptotics and universality. arXiv preprint arXiv:2309.16476, 2023.
Trambak Banerjee, Gourab Mukherjee, and Wenguang Sun. Adaptive sparse estimation with side informa-
tion. Journal of the American Statistical Association, 115(532):2053–2067, 2020.
Jean Barbier and Nicolas Macris. The adaptive interpolation method: a simple scheme to prove replica
formulas in bayesian inference. Probability theory and related fields, 174:1133–1185, 2019.
Jean Barbier, Florent Krzakala, Nicolas Macris, Léo Miolane, and Lenka Zdeborová. Optimal errors and
phase transitions in high-dimensional generalized linear models. Proceedings of the National Academy of
Sciences, 116(12):5451–5460, 2019.
Hamsa Bastani. Predicting with proxies: Transfer learning in high dimension. Management Science, 67(5):
2964–2984, 2021.
Mohsen Bayati and Andrea Montanari. The Lasso risk for gaussian matrices. IEEE Transactions on Infor-
mation Theory, 58(4):1997–2017, 2011.
Pierre C Bellec and Yiwei Shen. Derivatives and residual distribution of regularized m-estimators with
application to adaptive tuning. In Conference on Learning Theory, pp. 1912–1947. PMLR, 2022.
Pierre C. Bellec and Cun-Hui Zhang. Second-order Stein: SURE for SURE and other applications in high-
dimensional inference. The Annals of Statistics, 49(4):1864 – 1903, 2021.
PierreC.Bellec, GuillaumeLecué, andAlexandreB.Tsybakov. SlopemeetsLasso: Improvedoraclebounds
and optimality. The Annals of Statistics, 46(6B):3603 – 3642, 2018.
Emmanuel J. Candes and Terence Tao. Near-optimal signal recovery from random projections: Universal
encoding strategies? IEEE Transactions on Information Theory, 52(12):5406–5425, 2006.
Kabir Aladin Chandrasekher, Ashwin Pananjady, and Christos Thrampoulidis. Sharp global convergence
guarantees for iterative nonconvex optimization with random data. The Annals of Statistics, 51(1):179 –
210, 2023.
Patrick Charbonneau, Enzo Marinari, Marc Mézard, Giorgio Parisi, Federico Ricci-Tersenghi, Gabriele Si-
curo, and Francesco Zamponi. Spin Glass Theory and Far Beyond. WORLD SCIENTIFIC, 2023.
Erin Craig, Mert Pilanci, Thomas Le Menestrel, Balasubramanian Narasimhan, Manuel Rivas, Roozbeh
Dehghannasiri,JuliaSalzman,JonathanTaylor,andRobertTibshirani. PretrainingandtheLasso. arXiv
preprint arXiv:2401.1291, 2024.
Cedric Gerbelot, Alia Abbara, and Florent Krzakala. Asymptotic errors for teacher-student convex gener-
alized linear models (or: How to prove Kabashima’s replica formula). IEEE Transactions on Information
Theory, 69(3):1824–1852, 2023.
Samuel M. Gross and Robert Tibshirani. Data shared Lasso: A novel tool to discover uplift. Computational
Statistics and Data Analysis, 101:226–235, 2016.
Ehsan Hajiramezanali, Siamak Zamani Dadaneh, Alireza Karbalayghareh, Mingyuan Zhou, and Xiaoning
Qian. Bayesianmulti-domainlearningforcancersubtypediscoveryfromnext-generationsequencingcount
data. In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.
AdelJavanmardandAndreaMontanari.Stateevolutionforgeneralapproximatemessagepassingalgorithms,
with applications to spatial coupling. Information and Inference: A Journal of the IMA, 2(2):115–144,
2013.
13Yoshiyuki Kabashima, Tadashi Wadayama, and Toshiyuki Tanaka. A typical reconstruction limit for com-
pressedsensingbasedonℓ -normminimization. JournalofStatisticalMechanics: TheoryandExperiment,
p
2009(09):L09003, 2009.
Sai Li, T. Tony Cai, and Hongzhe Li. Transfer Learning for High-Dimensional Linear Regression: Predic-
tion, Estimation and Minimax Optimality. Journal of the Royal Statistical Society Series B: Statistical
Methodology, 84(1):149–173, 11 2021.
Sai Li, T Tony Cai, and Hongzhe Li. Transfer learning in large-scale gaussian graphical models with false
discovery rate control. Journal of the American Statistical Association, 118(543):2171–2183, 2023.
SaiLi,LinjunZhang,TTonyCai,andHongzheLi.Estimationandinferenceforhigh-dimensionalgeneralized
linear models with knowledge transfer. Journal of the American Statistical Association, 119(546):1274–
1285, 2024.
Sarah F. McGough, Svetlana Lyalina, Devin Incerti, Yunru Huang, Stefka Tyanova, Kieran Mace, Chris
Harbron, Ryan Copping, Balasubramanian Narasimhan, and Robert Tibshirani. Prognostic pan-cancer
and single-cancer models: A large-scale analysis using a real-world clinico-genomic database. medRxiv
preprint 2023.12.18.23300166, 2023.
Nicolai Meinshausen and Peter Bühlmann. High-dimensional graphs and variable selection with the Lasso.
The Annals of Statistics, 34(3):1436 – 1462, 2006.
M Mezard, G Parisi, and M Virasoro. Spin Glass Theory and Beyond. WORLD SCIENTIFIC, 1986.
Léo Miolane and Andrea Montanari. The distribution of the Lasso: Uniform control over sparse balls and
adaptive parameter tuning. The Annals of Statistics, 49(4), 2021.
Tomoyuki Obuchi and Yoshiyuki Kabashima. Cross validation in LASSO and its acceleration. Journal of
Statistical Mechanics: Theory and Experiment, 2016(5):053304, 2016.
TomoyukiObuchiandYoshiyukiKabashima. Semi-analyticresamplinginLasso. Journal of Machine Learn-
ing Research, 20(70):1–33, 2019.
KokiOkajimaandTakashiTakahashi. Asymptoticdynamicsofalternatingminimizationforbilinearregres-
sion. arXiv preprint arXiv:2402.04751, 2024.
Koki Okajima, Xiangming Meng, Takashi Takahashi, and Yoshiyuki Kabashima. Average case analysis of
Lasso under ultra sparse conditions. In Proceedings of The 26th International Conference on Artificial
Intelligence and Statistics, volume 206 of Proceedings of Machine Learning Research, pp. 11317–11330.
PMLR, 25–27 Apr 2023.
E. Ollier and V. Viallon. Regression modelling on stratified data with the Lasso. Biometrika, 104(1):83–96,
2017.
Sinno Jialin Pan, Ivor W. Tsang, James T. Kwok, and Qiang Yang. Domain adaptation via transfer compo-
nent analysis. IEEE Transactions on Neural Networks, 22(2):199–210, 2011.
Raphael Petegrosso, Sunho Park, Tae Hyun Hwang, and Rui Kuang. Transfer learning across ontologies for
phenome–genome association prediction. Bioinformatics, 33(4):529–536, 11 2016.
Kamiar Rahnama Rad and Arian Maleki. A scalable estimate of the out-of-sample prediction error via
approximate leave-one-out cross-validation. Journal of the Royal Statistical Society Series B: Statistical
Methodology, 82(4):965–996, 2020.
SundeepRangan,VivekGoyal,andAlysonKFletcher. Asymptoticanalysisofmapestimationviathereplica
method and compressed sensing. In Advances in Neural Information Processing Systems, volume 22.
Curran Associates, Inc., 2009.
14Luca Saglietti and Lenka Zdeborová. Solvable model for inheriting the regularization through knowledge
distillation. In Mathematical and Scientific Machine Learning, pp. 809–846. PMLR, 2022.
William Stephenson and Tamara Broderick. Approximate cross-validation in high dimensions with guaran-
tees. InProceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics,
volume 108 of Proceedings of Machine Learning Research, pp. 2424–2434. PMLR, 26–28 Aug 2020.
Mihailo Stojnic. A framework to characterize performance of Lasso algorithms. arXiv preprint
arXiv:1303.7291, 2013.
Fei Sun and Qi Zhang. Robust transfer learning of high-dimensional generalized linear model. Physica A:
Statistical Mechanics and its Applications, 618:128674, 2023.
Masaaki Takada and Hironori Fujisawa. Transfer learning via ℓ regularization. Advances in Neural Infor-
1
mation Processing Systems, 33:14266–14277, 2020.
Takashi Takahashi. The role of pseudo-labels in self-training linear classifiers on high-dimensional gaussian
mixture data. arXiv preprint arXiv:2205.07739, 2024.
TakashiTakahashiandYoshiyukiKabashima.Astatisticalmechanicsapproachtode-biasinganduncertainty
estimation in Lasso for random measurements. Journal of Statistical Mechanics: Theory and Experiment,
2018(7):073405, 2018.
ChristosThrampoulidis,EhsanAbbasi,andBabakHassibi.Preciseerroranalysisofregularizedm-estimators
in high dimensions. IEEE Transactions on Information Theory, 64(8):5592–5628, 2018.
Ye Tian and Yang Feng. Transfer learning under high-dimensional generalized linear models. Journal of the
American Statistical Association, 118(544):2684–2697, 2023.
RobertTibshirani. RegressionshrinkageandselectionviatheLasso. Journal of the Royal Statistical Society.
Series B (Methodological), 58(1):267–288, 1996.
Lisa Torrey and Jude Shavlik. Transfer Learning. In: Handbook of Research on Machine Learning Applica-
tions and Trends: Algorithms, Methods, and Techniques. IGI Global, 2010.
Turki Turki, Zhi Wei, and Jason T. L. Wang. Transfer learning approaches to improve drug sensitivity
prediction in multiple myeloma patients. IEEE Access, 5:7381–7393, 2017.
Mikko Vehkaperä, Yoshiyuki Kabashima, and Saikat Chatterjee. Analysis of regularized LS reconstruction
and random matrix ensembles in compressed sensing. IEEE Transactions on Information Theory, 62(4):
2100–2124, 2016.
Martin J. Wainwright. Sharp thresholds for high-dimensional and noisy sparsity recovery using ℓ -
1
constrainedquadraticprogramming(Lasso). IEEETransactionsonInformationTheory,55(5):2183–2202,
2009.
Meiyu Wang, Yun Lin, Qiao Tian, and Guangzhen Si. Transfer learning promotes 6G wireless communica-
tions: Recent advances and future challenges. IEEE Transactions on Reliability, 70(2):790–807, 2021.
KarlWeiss,TaghiMKhoshgoftaar,andDingDingWang. Asurveyoftransferlearning. Journal of Big data,
3:1–40, 2016.
Peng Zhao and Bin Yu. On model selection consistency of Lasso. Journal of Machine Learning Research, 7
(90):2541–2563, 2006.
Yin Zhu, Yuqiang Chen, Zhongqi Lu, Sinno Jialin Pan, Gui-Rong Xue, Yong Yu, and Qiang Yang. Hetero-
geneous transfer learning for image classification. In Proceedings of the Twenty-Fifth AAAI Conference
on Artificial Intelligence, AAAI’11, pp. 1304–1309. AAAI Press, 2011.
15Fuzhen Zhuang, Ping Luo, Hui Xiong, Qing He, Yuhong Xiong, and Zhongzhi Shi. Exploiting associations
between word clusters and document classes for cross-domain text categorization. Statistical Analysis and
Data Mining: The ASA Data Science Journal, 4(1):100–114, 2011.
Fuzhen Zhuang, Zhiyuan Qi, Keyu Duan, Dongbo Xi, Yongchun Zhu, Hengshu Zhu, Hui Xiong, and Qing
He. A comprehensive survey on transfer learning. Proceedings of the IEEE, 109(1):43–76, 2021.
A Detailed derivation of Asymptotic analysis
Here, we provide a detailed derivation for the asymptotic formulae for the generalization errors ϵ(1st) and
ϵ(2nd). Recall that the objective of our analysis is to calculate the data average of the n−th power of
Φ(D,D˜):=Z
dx 1dx
2e−β1L(1st)(x1;D)−β2L(2nd)(x2;x1,D(1))+t1PK
k=1∥y˜(k)−A˜(k)x1∥2 2+t2∥y˜(1)−A˜(1)(κx1+x2)∥2 2, (23)
in the successive limit lim lim . For n∈N, one can express this as
β2→∞ β1→∞
E
h Φn(D,D˜)i =Z Yn
dx dx
! exph −Xn (cid:16)
β λ ∥x ∥ +β r(x |x
)(cid:17)i
L,
D,D˜ 1,a 2,a 1 1 1,a 1 2 2,a 1,a
a=1 a=1
M n " K K
L=E Y Y exp − β 1 X(cid:0) y(k)−A(k)·x (cid:1)2 −t X(cid:0) y˜(k)−A˜(k)·x (cid:1)2 (24)
D,D˜ 2 µ µ,: 1,a 1 µ µ,: 1,a
µ=1a=1 k=1 k=1
#
− β 2(cid:0) y(1)−A(1)·(κx +x )(cid:1)2 −t (cid:0) y˜(1)−A˜(1)·(κx +x )(cid:1)2 .
2 µ µ,: 1,a 2,a 2 µ µ,: 1,a 2,a
Conditionedon{x ,x }n ,thedistributionover{A(k)·x ,A˜(k)·x ,A(1)·x ,A˜(1)·x ,y ,y˜ }M
1,a 2,a a=1 µ,: 1,a µ,: 1,a µ,: 2,a µ,: 2,a µ µ µ=1
is i.i.d. with respect to µ, whose profile is identical to that of the random variables
h(k) :=a(k)·x , h :=a(1)·x , h˜(k) :=a˜(k)·x , h˜ :=a˜(1)·x , (25)
1,a 1,a 2,a 2,a 1,a 1,a 2,a 2,a
H(k) :=a(k) ·x(k)+a(k) ·x(0)+ξ(k), H˜(k) :=a˜(k) ·x(k)+a˜(k) ·x(0)+ξ˜(k), (26)
I(k) ⋆ I(0) ⋆ I(k) ⋆ I(0) ⋆
with {a(k),a˜(k)}K being a set of N-dimensional centered Gaussian vectors with i.i.d. elements of variance
1/N, and ξ(k),ξ˜ (k k= )1 being independent centered Gaussian random variables with variance (σ(k))2 for k =
1,··· ,K. Note that the random variables with different index k are independent of each other. Following
this observation, L can be expressed alternatively as L:=QK (EL(k))M(k)(EL˜ (k))M(k), where
k=1
n
L(k) = Y exph − β 1(H(k)−h(k))2− δ 1kβ 2(H(1)−κh(1) −h )2i , (27)
2 1,a 2 1,a 2,a
a=1
n
L˜(k) = Y exph −t (H˜(k)−h˜(k))2−δ t (H˜(1)−κh˜(1) −h˜ )2i . (28)
1 1,a 1k 2 1,a 2,a
a=1
Here,δ =I[i=j]denotestheKroneckerdelta,whichisunityifi=jandzerootherwise. FromtheGaussian
ij
natureof{a(k),a˜(k)}K and{ξ(k),ξ˜ (k)}K ,therandomvariables{h(k)}n,K ,{h }n ,and{H(k)}K are
k=1 k=1 1,a a,k=1 2,a a=1 k=1
16all centered Gaussians with the following covariance structure:
1 1 K !
E[h(k)h(l)]=δ Qab, Qab := x ·x = X x(k′)·x(k′)+x(neg.)·x(neg.) ,
1,a 1,b kl 1 1 N 1,a 1,b N 1,a 1,b 1,a 1,b
k′=0
1 1 K !
E[h h ]=Qab, Qab := x ·x = X x(k′)·x(k′)+x(neg.)·x(neg.) ,
2,a 2,b 2 2 N 2,a 2,b N 2,a 2,b 2,a 2,b
k′=0
1 1 K !
E[h(k)h ]=δ Qab, Qab := x ·x = X x(k′)·x(k′)+x(neg.)·x(neg.) ,
1,a 2,a 1k r r N 1,a 2,b N 1,a 2,b 1,a 2,b (29)
k′=0
1
E[h(k)H(l)]=δ (cid:0) m(k)+m(0)(cid:1) , m(k′) := x(k′)·x(k′) (k′ =0,1,··· ,K),
1,a kl 1 1 1,a N 1,a ⋆
1
E[h H(k)]=δ (cid:0) m(k)+m(0)(cid:1) , m(k′) := x(k′)·x(k′) (k′ =0,1,··· ,K),
2,a k1 2 2 2,a N 2,a ⋆
" #
1 1
E[H(k)H(l)]=δ ∥x(k)∥2+ ∥x(0)∥2+(σ(k))2 =δ ρ(k),
kl N ⋆ 2 N ⋆ 2 kl
for 1 ≤ k,l ≤ K and 1 ≤ a,b ≤ n. Here, x(k) and x(k) are defined as the subvectors of x and
1,a 2,a 1,a
x indexed by I(k) respectively, while x(neg.) and x(neg.) are both subvectors of x and x whose
2,a 1,a 2,a 1,a 2,a
indices are not included in SK I(k). The same covariance structure also follows for the random variables
k=0
{h˜(k)}n,K ,{h˜ }n , and {H˜ (k)}K . Define the average with respect to the random variables given in
1,a a,k=1 2,a a=1 k=1
equation 29, conditioned on Ω={Qab,Qab,Qab,m(k),m(k)}, as E . Inserting the trivial identities based on
1 2 r 1,a 2,a |Ω
the delta function corresponding to the definitions of Ω given in equation 29, we can rewrite equation 24 up
to a trivial multiplicative constant as
Z dΩZ Yn YK dx(k)dx(k)! exp" −Xn (cid:16)
β λ ∥x ∥ +β r(x |x
)(cid:17) +XK
M(k)log(E L(k))(E
L˜(k))#
1,a 2,a 1 1 1,a 1 2 1,a 2,a |Ω |Ω
a=1k=0 a=1 k=1
n K ! K !
×Y δ NQab− X x(k′)·x(k′)−x(neg.)·x(neg.) δ NQab− X x(k′)·x(k′)−x(neg.)·x(neg.)
1 1,a 1,b 1,a 1,b 2 2,a 2,b 2,a 2,b
a≤b=1 k′=0 k′=0
n K ! n K
×Y δ NQab− X x(k′)·x(k′)−x(neg.)·x(neg.) E Y Y δ(cid:0) Nm(k)−x(k)·x(k)(cid:1) δ(cid:0) Nm(k)−x(k)·x(k)(cid:1) ,
r 1,a 2,b 1,a 2,b ⋆ 1,a 1,a ⋆ 2,a 2,a ⋆
a,b=1 k′=0 a=1k=0
(30)
where E is the average with respect to the set of ground truth vectors {x(k)}K , whose elements are i.i.d.
⋆ ⋆ k=0
according to a standard normal distribution. Note that the delta functions can be expressed alternatively
using its Fourier representation as
δ NQa 1b− XK x( 1k ,a′)·x( 1k ,b′)−x( 1n ,aeg.)·x 1(n ,beg.)! =Z dQˆa 1beQˆa 1b(cid:0) NQa 1b−PK k′=0x( 1k ,a′)·x( 1k ,b′)−x( 1n ,e ag.)·x( 1n ,e bg.)(cid:1) ,
C
k′=0
δ NQa 2b− XK x( 2k ,a′)·x( 2k ,b′)−x( 2n ,aeg.)·x 2(n ,beg.)! =Z dQˆa 2beQˆa 2b(cid:0) NQa 2b−PK k′=0x( 2k ,a′)·x( 2k ,b′)−x( 2n ,e ag.)·x( 2n ,e bg.)(cid:1) ,
C
k′=0
δ NQa rb− XK x( 1k ,a′)·x( 2k ,b′)−x( 1n ,aeg.)·x 2(n ,beg.)! =Z dQˆa rbeQˆa rb(cid:0) NQa rb−PK k′=0x( 1k ,a′)·x( 2k ,b′)−x( 1n ,e ag.)·x( 2n ,e bg.)(cid:1) , (31)
C
k′=0
Z (cid:0) (cid:1)
δ(cid:0) Nm( 1k ,a)−x( 1k ,a)·x( ⋆k)(cid:1)= dmˆ( 1k ,a)emˆ( 1k ,a) Nm( 1k ,a)−x( 1k ,a)·x( ⋆k) ,
C
Z (cid:0) (cid:1)
δ(cid:0) Nm( 2k ,a)−x( 2k ,a)·x( ⋆k)(cid:1)= dmˆ( 2k ,a)emˆ( 2k ,a) Nm( 2k ,a)−x( 2k ,a)·x( ⋆k) ,
C
17up to a trivial multiplicative constant.
Toobtainanexpressionthatisanalyticallycontinuableton→0,weintroducethereplicasymmetricansatz
(Charbonneau et al., 2023), which assumes that the integral over Ω is dominated by the contribution of the
subspace where Ω satisfy the following constraints:
χ χ χ
Qab =q +(1−δ ) 1, Qab =q +(1−δ ) 2, Qab =q +(1−δ ) r,
1 1 ab β 1 2 2 ab β 2 r r ab β 1 (32)
m(k) =m(k), m(k) =m(k),
1,a 1 2,a 2
for 1 ≤ a,b ≤ n and 0 ≤ k ≤ K. Although a general proof of this ansatz itself is still lacking, calculations
based on this assumption are known to be asymptotically exact in the limit N →∞ for convex generalized
linear models (Gerbelot et al., 2023) and Bayes-optimal estimation (Barbier et al., 2019). Consequently, the
conjugate variables {Qˆ ab,Qˆ ab,Qˆ ab,mˆ(k),mˆ(k)} are also assumed to be replica symmetric:
1 2 r 1,a 2,a
Qˆab =β qˆ −(1−δ )β2χˆ , Qˆab =β qˆ −(1−δ )β2χˆ , Qˆab =−β qˆ +(1−δ )β β χˆ ,
1 1 1 ab 1 1 2 2 2 ab 2 2 r 2 r ab 1 2 r (33)
mˆ(k) =−β mˆ(k), mˆ(k) =−β mˆ(k).
1,a 1 1 2,a 2 2
Inserting equation 32 and equation 33 to equation 30, and rewriting E as E given this simplified profile
|Ω |Θ
of Ω offers
Z Z n K ! " K K #
E dΘ dΘ Y Y dx(k)dx(k) exp X M(k)log(E L(k))+X M(k)log(E L˜(k))
⋆ 1 2 1,a 2,a |Θ |Θ
C
a=1k=0 k=1 k=1
×expnN"
β
(cid:16)q 1qˆ 1−χ 1χˆ
1
−XK m(k)mˆ(k)(cid:17)
+β
(cid:16)q 2qˆ 2−χ 2χˆ
2 −q qˆ −χ χˆ
−XK m(k)mˆ(k)(cid:17)#
1 2 1 1 2 2 r r r r 2 2
k=0 k=0
×exp"
−β
Xn XK (cid:16)qˆ
1∥x(k)∥2−mˆ x(k)·x(k)+λ ∥x(k)∥
(cid:17)
−β
Xn (cid:16)qˆ
1∥x(neg.)∥2+λ ∥x(neg.)∥
(cid:17)
1 2 1,a 2 1 ⋆ 1,a 1 1,a 1 1 2 1,a 2 1 1,a 1
a=1k=0 a=1 (34)
−β
Xn XK (cid:16)qˆ
2∥x(k)∥2−(mˆ x(k)+qˆ
x(k))·x(k)+r(x(k)|x(k))(cid:17)
2 2 2,a 2 2 ⋆ r 1,a 2,a 2,a 1,a
a=1k=0
−β
Xn (cid:16)qˆ
2∥x(neg.)∥2−qˆ
x(neg.)·x(neg.)+r(x(neg.)|x(neg.))(cid:17)
2 2 2,a 2 r 1,a 2,a 2,a 1,a
a=1
+
β 12χˆ 1(cid:13) (cid:13)Xn
x
(cid:13) (cid:13)2
+
β 22χˆ 2(cid:13) (cid:13)Xn
x
(cid:13) (cid:13)2
+β β χˆ
(cid:16)Xn
x
(cid:17) ·(cid:16)Xn
x
(cid:17)#
.
2 (cid:13) 1,a(cid:13) 2 (cid:13) 2,a(cid:13) 1 2 r 1,a 2,a
2 2
a=1 a=1 a=1 a=1
The last equation can be further simplified using the equality
exp" β 12χˆ 1(cid:13) (cid:13)Xn
x
(cid:13) (cid:13)2
+
β 22χˆ 2(cid:13) (cid:13)Xn
x
(cid:13) (cid:13)2
+β β χˆ
(cid:16)Xn
x
(cid:17) ·(cid:16)Xn
x
(cid:17)#
(35)
2 (cid:13) 1,a(cid:13) 2 (cid:13) 2,a(cid:13) 1 2 r 1,a 2,a
2 2
a=1 a=1 a=1 a=1
" n n #
=E exp β p χˆ X z ·x +β p χˆ X z ·x (36)
z1,z2 1 1 1 1,a 2 2 2 2,a
a=1 a=1
where z ,z ∈ RN are random Gaussian vectors with elements i.i.d. according to z ∼ z ,z ∼ z for
1 2 1,i 1 2,i 2
all i = 1,··· ,N. From this decomposition, the integrals over x and x decouple over both vector
1,a 2,a
coordinates i=1,··· ,N and replica indices a=1,··· ,n :
Z dΘ 1dΘ 2Z ePK k=1M(k)logE |ΘL(k)+PK k=1M(k)logE |ΘL˜(k) YK n E 2hZ dx 1dx 2e−β1E( 1k)(x1)−β2E( 2k)(x2|x1)inoN(k)
k=0
×n
E
2hZ
dx 1dx 2e−β1E( 1neg.)(x1)−β2E(
2neg.)(x2|x1)inoN−PK k=0N(k)
.
(37)
18Inthesuccessivelimitofβ →∞andβ →∞,theintegralsoverx andx canbeevaluatedusingLaplace’s
1 2 1 2
method, yielding the asymptotic form:
Z
dΘ 1dΘ
2
YK
eM(k)logE |ΘL(k)+M(k)logE |ΘL˜(k)
YK n
E
2h
e−β1E( 1k)(x 1(k))−β2E( 2k)(x 2(k)|x
1(k))inoN(k)
k=1 k=0 (38)
×n
E
2h
e−β1E 1(neg.)(x 1(neg.))−β2E( 2neg.)(x 2(neg.)|x
1(neg.))inoN−PK
k=0N(k)
.
where the definitions of E and {x(k),x(k)}K ,x(neg.),x(neg.) follow from Definitions 1 and 2.
2 1 2 k=0 1 2
LetusproceedwiththecalculationofE L(k). DefinethecenteredGaussianvariables{H(k),h(k),h(k)}and
|Θ 1 2
{u ,u }n as
1,a 2,a a=1
    
H(k) ρ(k) m(k) m(k)
1 2 (cid:18) u (cid:19) (cid:18) (cid:18) χ /β χ /β (cid:19)(cid:19)
 h( 1k) ∼N  0 3, m( 1k) q
1
q
r
  , u1,a ∼N 0 2, χ1 /β1 χr /β1 , (39)
h(k) m(k) q q 2,a r 1 2 2
2 2 r 2
where {u ,u } are independent for all a = 1,··· ,n. Then, we can see that the random variables admit
1,a 2,a
the decomposition h(k) =h(k)+u ,h =h(k)+u , offering the expression
1,a 1 1,a 2,a 2 2,a
n " #
E L(k) =E Y E exp −β 1(H(k)−h(k)−u )2−β 2δ 1k(cid:16) H(k)−κ(h(k)+u )−h(k)−u (cid:17)2 .
|Θ h(k),h(k),H(k) u1,a,u2,a 2 1 1,a 2 1 1,a 2 2,a
1 2
a=1
(40)
Taking into account that β ≫β , the Gaussian measure over (u ,u ) can be written as
1 2 1,a 2,a
" #
β β (cid:16) χ (cid:17)2
C exp − 1 (1+c )u2 − 2 u − ru du du , (41)
β1,β2 2χ β1 1,a 2χ 2,a χ 1,a 1,a 2,a
1 2 1
where C is a number subexponential in β and β , and c is a real number which converges to zero as
β1,β2 1 2 β1
β →∞. Using this expression to equation 40 yields
1
( "
Z β β
E L(k) =E du du exp − 1 (1+c )u2− 1(H(k)−h(k)−u )2
|Θ h(k),h(k),H(k) 1 2 2χ β1 1 2 1 1
1 2 1
(42)
#)n
β β δ (cid:16) χ (cid:17)2
− 2 u2− 2 1k H(k)−κ(h(k)+u )−h(k)−u − ru +logC .
2χ 2 2 1 1 2 2 χ 1 β1,β2
2 1
The same procedure can be applied to L˜ (k), this time accounting for t ,t ≪β , offering
1 2 2
" #
E L˜(k) =E expn −t (cid:16) H˜(k)−h˜(k)(cid:17)2 −t δ (cid:16) H˜(k)−κh˜(k)−h˜(k)(cid:17)2 +logC . (43)
|Θ h˜(k),h˜(k),H˜(k) 1 1 2 1k 1 2 β1,β2
1 2
Note that the expressions for L˜ (k) and L(k) are simple Gaussian integrals that can be computed explicitly.
Now that all the terms in equation 30 have been expressed in analytical form with respect to n, the limit
n → 0 can be taken formally. Evaluating equation 38, equation 42 and equation 43 up to first order of n,
and neglecting subleading terms with respect to β and β , we finally obtain an expression of the form
1 2
E h Φn(D,D˜)i =Z dΘ dΘ expN(cid:2) nG(Θ ,Θ )+O(n2)(cid:3) , (44)
D,D˜ 1 2 1 2
where
G(Θ ,Θ )=β G (Θ )+β G (Θ |Θ )+t ϵ +t ϵ , (45)
1 2 1 1 1 2 2 2 1 1 1 2 2
19G (Θ )= q 1qˆ 1−χ 1χˆ 1 −XK m(k)mˆ(k)−XK π(k)E h E(k)(cid:0) x(k)(cid:1)i −π(neg.)E h E(neg.)(cid:0) x(neg.)(cid:1)i
1 1 2 1 1 1 1 1 1 1 1
k=0 k=0
−XK α(k)q 1−2(m( 10)+m( 1k))+ρ(k)
, (46)
2 1+χ
1
k=1
G (Θ |Θ )=
q 2qˆ 2−χ 2χˆ
2 −q qˆ −χ χˆ
−XK
m(k)mˆ(k)
2 2 1 2 r r r r 2 2
k=0
K
−X π(k)E h E(k)(cid:0) x(k)|x(k)(cid:1)i −π(neg.)E h E(neg.)(cid:0) x(neg.)|x(neg.)(cid:1)i
2 2 2 1 2 2 2 1
k=0
α(1)q +A2q +2Aq +B2ρ(1)−2B(m(0)+m(1))−2AB(m(0)+m(1))
− 2 1 r 2 2 1 1 , (47)
2 1+χ
2
and
K
ϵ
=X π(k)h
q
−2(m(0)+m(k))+ρ(k)i
, (48)
1 1 1 1
k=1
h i
ϵ =π(1) q +κ2q +2κq +ρ(1)−2(m(0)+m(1))−2κ(m(0)+m(1)) . (49)
2 2 1 r 2 2 1 1
ForlargeN andfiniten,theintegraloverΘ andΘ canbeevaluatedusingthesaddlepointmethod,where
1 2
the integral is dominated by the stationary point of the exponent. This finally yields
lim lim
1 ∂
logE
h Φn(D,D˜)i
= ExtrG(Θ ,Θ ), (50)
n→0N→∞N ∂n D,D˜ Θ1,Θ2 1 2
where Extr denotes the extremum operation of the function. The stationary conditions for Θ and Θ , in
1 2
the successive limit of β →∞ and β →∞ are given by the equations of state in Definitions 1 and 2. Note
1 2
that the term t ϵ +t ϵ does not contribute to the stationary condition in the limit t ,t → 0, and thus
1 1 2 2 1 2
can be neglected until one takes the derivative with respect to t and t , as in equation 11 and equation 12.
1 2
B Additional numerical experiments
Here, we provide more numerical experiments highlighting the difference between the Generalized Trans-
Lasso,PretrainingLassoandTrans-Lasso. Infigures5and6,weshowtheratiosofmin{ϵ ,ϵ },ϵ
∆λ=0 κ=0 Pretrain
and ϵ against ϵ for the cases (π(0),π(1),π(2)) = (0.15,0.04,0.04) and (0.05,0.14,0.14), respectively.
Trans LO
The former case represents the problem setting where the underlying common feature vector among the
datasetsislarge,whilethelatterrepresentstheproblemsettingwheretheunderlyingcommonfeaturevector
is small. In both cases, the minimum of the κ=0 or ∆λ=0 strategy can obtain generalization errors close
to the one from the LO strategy for both low or moderate noise levels. This is not a property exhibited in
the Trans-Lasso or Pretraining Lasso, where both algorithms have relatively low generalization performance
when σ =0.01 and (π(0),π(1),π(2))=(0.15,0.04,0.04). Note that when (π(0),π(1),π(2))=(0.05,0.14,0.14),
all three algorithms yield comparable generalization performance.
To directly compare the κ = 0 or ∆λ = 0 strategy with the Pretraining Lasso and Trans-Lasso, in figures
7, 8 and 9, we also plot the same ratios with shared color scale bars. As evident from the plot, the κ = 0
or ∆λ=0 strategy exhibits a clear improvement in generalization performance over Trans-Lasso. While the
difference between Pretraining Lasso and κ=0 or ∆λ=0 strategy is comparable for σ =0.1, we can see a
consistent advantage for lower noise levels (σ =0.01).
20=0.01, ( (0), (1), (2))=(0.15,0.04,0.04)
Pretraining Lasso Trans-Lasso Generalized Trans-Lasso
0.45 4.0 0.45 0.45 1.12
6
3.5 1.10
0.35 0.35 5 0.35
3.0 1.08
4
0.25 2.5 0.25 0.25 1.06
3
2.0 1.04
0.15 0.15 0.15
1.5 2 1.02
0.05 1.0 0.05 1 0.05 1.00
0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9
2 2 2
=0.1, ( (0), (1), (2))=(0.15,0.04,0.04)
Pretraining Lasso Trans-Lasso Generalized Trans-Lasso
0.45 0.45 2.6 0.45 1.040
1.05 2.4 1.035
0.35 0.35 2.2 0.35 1.030
1.04
2.0 1.025
0.25 1.03 0.25 1.8 0.25 1.020
1.02 1.6 1.015
0.15 0.15 1.4 0.15 1.010
1.01
1.2 1.005
0.05 1.00 0.05 1.0 0.05 1.000
0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9
2 2 2
Figure 5: Ratios min{ϵ ,ϵ }/ϵ ,ϵ /ϵ and ϵ /ϵ under setting (π(0),π(1),π(2)) =
∆λ=0 κ=0 LO Pretrain LO Trans LO
(0.15,0.04,0.04)andnoiselevelσ =0.01(topfigure)and0.1(bottomfigure)forvariousvaluesof(α(1),α(2)).
=0.01, ( (0), (1), (2))=(0.05,0.14,0.14)
Pretraining Lasso Trans-Lasso Generalized Trans-Lasso
0.45 1.12 0.45 0.45
1.12 1.025
1.10
1.10
0.35 0.35 0.35 1.020
1.08
1.08
1.015
0.25 1.06 0.25 1.06 0.25
1.010
1.04 1.04
0.15 0.15 0.15
1.02 1.02 1.005
0.05 1.00 0.05 1.00 0.05 1.000
0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9
2 2 2
=0.1, ( (0), (1), (2))=(0.05,0.14,0.14)
Pretraining Lasso Trans-Lasso Generalized Trans-Lasso
0.45 1.025 0.45 0.45 1.025
1.08
1.020 1.07 1.020
0.35 0.35 0.35
1.06
1.015 1.05 1.015
0.25 0.25 0.25
1.04
1.010 1.010
1.03
0.15 0.15 1.02 0.15
1.005 1.005
1.01
0.05 1.000 0.05 1.00 0.05 1.000
0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9
2 2 2
Figure 6: Ratios min{ϵ ,ϵ }/ϵ ,ϵ /ϵ and ϵ /ϵ under setting (π(0),π(1),π(2)) =
∆λ=0 κ=0 LO Pretrain LO Trans LO
(0.05,0.14,0.14)andnoiselevelσ =0.01(topfigure)and0.1(bottomfigure)forvariousvaluesof(α(1),α(2)).
21
1
1
1
1=0.01, ( (0), (1), (2))=(0.15,0.04,0.04)
Pretraining Lasso Generalized Trans-Lasso Trans-Lasso Generalized Trans-Lasso
0.45 0.45 4.0 0.45 0.45
6
3.5
0.35 0.35 0.35 0.35 5
3.0
4 0.25 0.25 2.5 0.25 0.25
3
2.0
0.15 0.15 0.15 0.15
1.5 2
0.05 0.05 1.0 0.05 0.05 1
0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9
2 2 2 2
=0.1, ( (0), (1), (2))=(0.15,0.04,0.04)
Pretraining Lasso Generalized Trans-Lasso Trans-Lasso Generalized Trans-Lasso
0.45 0.45 0.45 0.45 2.6
1.05 2.4
0.35 0.35 0.35 0.35 2.2
1.04
2.0
0.25 0.25 1.03 0.25 0.25 1.8
1.02 1.6
0.15 0.15 0.15 0.15 1.4
1.01
1.2
0.05 0.05 1.00 0.05 0.05 1.0
0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9
2 2 2 2
Figure 7: Comparisons between ratios min{ϵ ,ϵ }/ϵ , ϵ /ϵ and ϵ /ϵ under setting
∆λ=0 κ=0 LO Pretrain LO Trans LO
(π(0),π(1),π(2))=(0.15,0.04,0.04).
=0.01, ( (0), (1), (2))=(0.1,0.09,0.09)
Pretraining Lasso Generalized Trans-Lasso Trans-Lasso Generalized Trans-Lasso
0.45 0.45 0.45 0.45 2.0
1.8
1.7 1.8
0.35 0.35 0.35 0.35
1.6
1.5 1.6
0.25 0.25 0.25 0.25
1.4
1.4
1.3
0.15 0.15 1.2 0.15 0.15 1.2
1.1
0.05 0.05 1.0 0.05 0.05 1.0
0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9
2 2 2 2
=0.1, ( (0), (1), (2))=(0.1,0.09,0.09)
Pretraining Lasso Generalized Trans-Lasso Trans-Lasso Generalized Trans-Lasso
0.45 0.45 0.45 0.45
1.35
1.05
1.30
0.35 0.35 0.35 0.35
1.04 1.25
0.25 0.25 1.03 0.25 0.25 1.20
1.15
1.02
0.15 0.15 0.15 0.15 1.10
1.01
1.05
0.05 0.05 1.00 0.05 0.05 1.00
0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9
2 2 2 2
Figure 8: Comparisons between ratios min{ϵ ,ϵ }/ϵ , ϵ /ϵ and ϵ /ϵ under setting
∆λ=0 κ=0 LO Pretrain LO Trans LO
(π(0),π(1),π(2))=(0.10,0.09,0.09).
22
1
1
1
1=0.01, ( (0), (1), (2))=(0.05,0.14,0.14)
Pretraining Lasso Generalized Trans-Lasso Trans-Lasso Generalized Trans-Lasso
0.45 0.45 1.12 0.45 0.45
1.12
1.10
1.10
0.35 0.35 0.35 0.35
1.08 1.08
0.25 0.25 1.06 0.25 0.25 1.06
1.04 1.04
0.15 0.15 0.15 0.15
1.02 1.02
0.05 0.05 1.00 0.05 0.05 1.00
0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9
2 2 2 2
=0.1, ( (0), (1), (2))=(0.05,0.14,0.14)
Pretraining Lasso Generalized Trans-Lasso Trans-Lasso Generalized Trans-Lasso
0.45 0.45 1.025 0.45 0.45
1.08
1.020 1.07
0.35 0.35 0.35 0.35
1.06
1.015 1.05
0.25 0.25 0.25 0.25
1.04
1.010
1.03
0.15 0.15 0.15 0.15 1.02
1.005
1.01
0.05 0.05 1.000 0.05 0.05 1.00
0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9 0.1 0.3 0.5 0.7 0.9
2 2 2 2
Figure 9: Comparisons between ratios min{ϵ ,ϵ }/ϵ , ϵ /ϵ and ϵ /ϵ under setting
∆λ=0 κ=0 LO Pretrain LO Trans LO
(π(0),π(1),π(2))=(0.05,0.14,0.14).
23
1
1