Robot See Robot Do: Imitating Articulated Object
Manipulation with Monocular 4D Reconstruction
JustinKerr ChungMinKim MingxuanWu BrentYi
QianqianWang KenGoldberg AngjooKanazawa
UCBerkeley
https://robot-see-robot-do.github.io
Abstract: Humanscanlearntomanipulatenewobjectsbysimplywatchingoth-
ers; providing robots with the ability to learn from such demonstrations would
enable a natural interface specifying new behaviors. This work develops Robot
See Robot Do (RSRD), a method for imitating articulated object manipulation
from a single monocular RGB human demonstration given a single static multi-
view object scan. We first propose 4D Differentiable Part Models (4D-DPM), a
methodforrecovering3Dpartmotionfromamonocularvideowithdifferentiable
rendering. Thisanalysis-by-synthesisapproachusespart-centricfeaturefieldsin
an iterative optimization which enables the use of geometric regularizers to re-
cover 3D motions from only a single video. Given this 4D reconstruction, the
robotreplicatesobjecttrajectoriesbyplanningbimanualarmmotionsthatinduce
the demonstrated object part motion. By representing demonstrations as part-
centric trajectories, RSRD focuses on replicating the demonstration’s intended
behaviorwhileconsideringtherobot’sownmorphologicallimits, ratherthanat-
tempting to reproduce the hand’s motion. We evaluate 4D-DPM’s 3D tracking
accuracyongroundtruthannotated3DparttrajectoriesandRSRD’sphysicalex-
ecutionperformanceon9objectsacross10trialseachonabimanualYuMirobot.
Each phase of RSRD achieves an average of 87% success rate, for a total end-
to-end success rate of 60% across 90 trials. Notably, this is accomplished using
only feature fields distilled from large pretrained vision models — without any
task-specifictraining,fine-tuning,datasetcollection,orannotation. Projectpage:
https://robot-see-robot-do.github.io
Keywords: Visual Imitation, 4D Reconstruction, Articulated Objects, Feature
Fields
1 Introduction
Considerteachingarobottomanipulateanarticulatedobjectinyourhousesuchasapairofscissors
or sunglasses. The most natural way to do this is simply to pick up the object, show it to the
robot,andthendemonstratehowtouseitwithyourownhands. Thisishowchildrenlearn—from
observing adults — despite the cross-morphology gap between the large hands of adults and the
smallhandsofachild. Akeyinsightthatenablesvisualimitationacrossamorphologygapisnotto
focusontheexactmotionofthemanipulator(i.e.,hand),butobservetheconsequenceoftheaction
attheobject level. Ifthe3Dmotionofanobjectanditspartscanbeperceived, onecouldplanto
manipulatetheobjectsuchthattheperceived3Dmotiontobereplicated.
ThispaperproposesRobotSeeRobotDo, anobject-centricmethodformanipulatingobjectswith
moveablepartsfromasinglehumandemonstrationgiven1)astaticmulti-viewobjectscanand2)
amonocular humaninteractionvideo. Theseinputsareeasilycapturedwithanysmartphone. The
Seephasebuildsamodeloftheobject,groupsitintomovableparts,andrecoverstheir3Dmotion
trajectories.DuringrobotdeploymentintheDophase,therobotispresentedwiththesameobjectin
anunknownposeintheworkspace. Therobotregisterstherecovered3Dobjecttrajectoryfromthe
4202
peS
62
]OR.sc[
1v12181.9042:viXraFigure 1: Robot See Robot Do. To visually imitate articulated object motion RSRD first recon-
structsapart-awarefeaturefield. Givenaninputdemonstrationvideo,wethentracktheobjectpart
motion using the feature field. Next, the robot recognizes the object in its workspace and plans a
bimanualtrajectorytoachievethedemonstratedobjectmotion.
demonstrationtotheposeoftheobjectintheworld, thenplansabimanualend-effectortrajectory
toinducethesame3Dmotionontheobjectasperceivedinthedemonstrationvideo. Becausethe
demonstrationsarerecoveredinanobject-centricmanner,thesamedemonstrationcanbereusedfor
differentrobots,grippers,andre-orientationsoftheobject.
Recovering the 3D movement of an object and its parts from a monocular video is challenging
duetotheunder-constrainednatureleadingtodegeneratesolutions. Inthispaper,wepropose4D-
DifferentiablePartModels(4D-DPM),amethodwhichusesadecomposed3Dfeaturefieldtore-
coverpartandobjectmotionfrommonocularvideos. 4D-DPMleveragesananalysis-by-synthesis
paradigm, where a model of 3D part motion is iteratively compared to visual observations and
fit through optimization. 4D-DPM first processes the multi-view static video of an object with
GARField[1]toconstructa3DGaussianSplat[2]segmentedintoparts. Then,itembedsDINO[3]
featurefieldsintoeachobjectpart,whichenablestrackingtheobjectmotioninamonocularvideoby
comparingittovideo-computedDINOfeaturesthroughdifferentiablerendering. Byleveragingthe
0-shotperformanceofvisualrepresentationsinlargepretrainedmodels,4D-DPMenablestracking
a wide variety of objects without any fine-tuning or task-specific dataset collection. Additionally,
4D-DPMcannaturallyincorporateanyprioronecanrepresentwithadifferentiablelossfunction;
forexampletemporalsmoothnessandas-rigid-as-possibleprior.
Duringdeployment,RSRDgeneratesasetofcandidategraspsformovingeachdesiredsubpart,then
findsacollision-freebimanualmotionthatrigidlytracksthemotionofgraspedobjectsthroughout
the trajectory. To determine which parts to grasp, we recognize hand-part contacts in the demon-
strationvideoandsoftlybiasrobotgraspstowardstheseparts. Notably,RSRDdoesnotattemptto
copythemotionofhumanhands, allowingittofindrobotmotionswhichachievethesameobject
trajectorywithdifferentembodiment.
We evaluate RSRD on a variety of 9 articulated objects, ranging from tools to plushies, assessing
itsflexibilitytofunctiononadiverserangeofobjects. Notably,severaldemonstrationsareaccom-
plishedwithbimanualmanipulation,fullyliftingtheobjectofftheworkspace. Fortracking,RSRD
achievesanaveragedistanceerrorof7.5mmcomparedtogroundtruthpartposes; ablationshigh-
light the importance of both as-rigid-as-possible regularization and DINO for successful tracking.
Forreal-worldrobotexperiments,weemployabimanualYuMirobottomeasuresuccessratesacross
fourdistinctphasesoftheRSRDrobotexecutionpipeline, witheachobjectplacedin10different
orientations within the robot’s workspace. The results demonstrate a success rate of 94% for ini-
tialposeregistrationand87%fortrajectoryplanning. Initialgraspsandmotionexecutionrecorded
successratesof83%and85%respectively;end-to-end,thismeansthatRSRDachievessuccessful
2Figure 2: 4D Reconstruction of Articulated Objects. Keyframes from the motion trajectories
overlaidovermonocularRGBdemonstrationswithpartscolorized,andalongwithtwoviewpoints.
imitationfor60%ofinitialobjectpositions. Importantly,theseresultsareachievedsolelythrough
featurefieldsderivedfrompretrainedvisionmodels, withoutrelyingonanytask-specifictraining,
fine-tuning,datacollection,orannotation.
2 RelatedWork
Recovering 3D Motion for Objects with Moving Parts. Reconstructing the 3D motion trajec-
toryofarticulatedobjectfromasinglevideoisextremelychallengingasitinvolvesdetectingand
reconstructingindividualpartsandrecoveringtheirposesacrossspaceandtime. Asubstantialbody
ofworkbypassesthereconstructionproblemandutilizespointcloudstoperceivearticulatedparts,
with inputs ranging from sequences of articulated motions of objects [4, 5, 6, 7] to single point
clouds[8,9,10,11,12,13]. Moresimilartoourapproachareworksthattakeinvisualobservations
as input for joint reconstruction and part segmentation. Given the challenging nature of model-
ingmovingobjects,mostworkrequireeitherRGB-Dvideosormulti-viewobservationsatmultiple
states[14,15,16,17]asinput.Thereexistmonocularbasedarticulatedobjecttrackingmethods,but
theytypicallyrequireknownkinematicchains[18,19]orcategory-specificpriors[20,21]. Incon-
trast,afterseeingtheobjectonce,RSRDfunctionsfrompurelymonocularinteractioninputvideo.
Inaddition,unlikepreviousworkthatreliesontraininginsmall-scaledatasetswithpart-levelanno-
tations [22], we distill the segmentation of the parts from the Segment Anything (SAM) [23] into
3D using GARField [1] for 3D part understanding, which generalizes well to objects in the wild.
GARFieldreturnsmultiplehypothesisofgroupedparts,inourworkwespecifythelevelthatismost
relevanttothedemonstrationonce. Thiscombinedwith4D-DPM’suseofaDINO[3]featurefield
allowsustoestimate3Dmotionofpartsfromjustmonocularvideos.
Learningfromonedemonstration. Humanvideosarevaluableresourcesforlearningobjectin-
teraction behaviors. Extensive research [24, 25, 26, 27] has leveraged human video data to learn
robotmanipulation,buttechniqueslargelystillrequireadditionalrobotteleoperationdatafortarget
tasks or paired human-robot data to bridge the morphology gap. Also related to our method are
works that learn manipulation policies from a single demonstration [28, 29, 30, 31, 32, 33], many
of which learn from humans [34, 35, 36]. While these methods enable a robot to perform tasks
fromonedemonstration,theyrequireextensivein-domaindataandwell-curatedmeta-trainingtasks
fortraining, whichlimitsthegeneralizationofthelearnedpolicy. Incontrast, ourmethodenables
manipulationfromasinglehumanvideousinganobject-centricformulation,withoutrequiringex-
tensive training with in-domain data. Our setting only requires an additional multiview capture to
obtainthe3Dscanofthemanipulatedobject,whichcanbeachievedwithasmartphonecamera.
3Figure3: 4DDifferentiablePartModels(4D-DPM).Left: DINOfeaturesanddeptharerendered
from per-timestep optimizable part pose parameters, and compared with extracted DINO features
and monocular depth from the input frame. Right: an ARAP loss penalizes gaussians from devi-
ating too far from their initial configuration with respect to neighbors. Together these losses flow
backwardsintothepartposesandareoptimizedwithgradientdescenttorecover3Dpartmotion.
Object-centricrepresentationsforrobotmanipulation. Object-centricrepresentationshaveen-
abled a broad range of capabilities for robot manipulation. Many existing works have focused on
how representations for objects can be learned from specific classes of data, for example by us-
ing 3D structure as a signal for contrastive learning [37], intermediate object properties as a bot-
tleneck for image prediction [38], or canonical object views as conditioning [39]. Others have
shown how object-centric approaches can be used to improve generalization in robot manipula-
tion via imitation [40, 41, 32, 33, 42, 43] or reinforcement [44, 45] learning, as well as to aug-
ment robots with semantic [46], relational [47, 48, 49, 50], uncertainty-based [51, 52, 53], sym-
bolic[54,55,56,57,58,59], andpart-aware[60,61]reasoning. InRSRD,weshowhowrelative
motiontrackedviapart-centricrepresentationscanbeusedforsingle-shotimitation. Importantly,
theserepresentationsdonotrequirefixedobjectcategoriesortask-specificdata. Instead,werelyon
andreaptheopen-worldbenefitsoflargepretrainedmodels.
Feature fields for robotics. 3D neural fields have recently been explored in robotics, beginning
with exploring leveraging Neural Radiance Fields [62] (NeRFs) as as high-quality visual recon-
struction for grasping [63, 64] and navigation [65], and more recently by leveraging its ability to
embedhigherdimensionalfeaturesforlanguage-guidedmanipulation[66,67]. Acorelimitationof
neural fields is their slow training speed, an issue which is ameliorated by 3D Gaussian Splatting
(3DGS) [2], a technique for representing radiance fields as a collection of oriented 3D gaussians
which can be differentiably rasterized quickly on modern GPUs. Concurrent works transfer high-
dimensional feature fields to 3DGS for rapid training and rendering, as well as language-guided
robotgrasping [68,69]. Oneremaining corelimitationisthat theserepresentationsare static, and
must be re-scanned after moving the environment. Wang et al. [70] showed promising results on
tracking DINO embedded object keypoints in 3D from multi-view cameras. In this work, we de-
velopamethodforrecovering3Dmotionof3DGSfeaturefieldsfromamonocularvideo.
3 ProblemandAssumptions
Givenabimanualrobotwithparallel-jawgrippers, amulti-viewobjectscanofanobjectwithtwo
ormoremovableparts,andamonocularhumandemonstration,thegoalistomanipulatetheobject
throughthesameconfigurationchangestartingfromanunknownlocationintherobot’sworkspace.
Wefocusonarticulatedobjectswithoneormorerotaryorprismaticjoints,andassumethatinternal
partdeformationisnegligible. Wealsoassumetheobjectscanistakeninthesamestartingconfig-
urationasthedemonstration,andthattheinputvideohasastaticviewpointwithclearvisibilityof
thesubpartbeingmanipulated.
4Figure4: HandAlignment: RSRDusesHaMeR[74]todetectandalignhumanhandposestothe
demonstrations. Detectionsareusedtorankpartpairsforgrasping(Sec4.3).
4 Method
RSRDfirstbuildsa4DDifferentiablePartModeloftheobjectsegmentedintopartsembeddedwith
feature descriptors (Sec 4.1), and uses these dense part descriptors to recover 3D motion from a
monocularvideo(Sec4.2). Next,duringdeploymenttherobotrecognizestheposeoftheobjectin
its workspace and plans actions which emulate the human’s to bring it through the same range of
motion(Sec.4.3).
4.1 Constructing4DDifferentiablePartModels
Givenastaticmulti-viewcaptureoftheobjectofinterest,wefirstconstructa3Dmodelfortheob-
jectusingGaussianSpatting(3DGS).Weleverage3DGSbecauseithasbeenshowntobeordersof
magnitudefasterinreconstructingandrenderingbothvisualappearanceandhigh-dimensionalfea-
turefields[2,69],whileinadditionprovidinganexplicitrepresentationthatcanbeeasilysegmented
into objects and subparts. In parallel, we train a GARField [1] from the same capture. GARField
canclusterthe3DGaussiansintodiscretegroupsofvaryinggranularities,controlledbyascalepa-
rameter. This allows manual segmentation of the 3D object from the background by clicking it in
thescene,andmanualdecompositionoftheobjectintopartsbyselectingascaleparameteratwhich
tobreakaparttheobjectwhereallrelevantpartsareseparated.
Next,wetrainadensefeaturefieldoverthisobjecttofacilitatetrackinginthelaterstagesupervising
the feature field using each view’s DINO feature map as in prior work [67, 66, 71, 69]. Each
GaussianisembeddedwithafeaturevectorofdimensionD,whichcanbeprojectedontotheDINO
spacewithasmallMLPappliedper-pixel. Thepart-centricfeaturefieldistrainedwithaper-pixel
MSElossfor6000steps,around3minutes.Theresultisanobjectmodelwhichcanbedifferentiably
rendered at high framerate (30fps HD), separated into parts with dense feature descriptors whose
posescanbedifferentiatedthroughwithrespecttopixelsandtime. WebuildonNerfstudio’s[72]
Splatfactovariantof3DGS,whichincludesimprovementslikecameraposeoptimizationandfeature
rasterization, using the gsplat [73] rasterization backend. For more implementation details please
seetheAppendix.
4.2 Monocular3DPartMotionRecovery
Recovering 3D motion of object parts from a single RGB video is a highly underconstrained and
challengingproblem. Totackleit, weproposeananalysis-by-synthesisapproach(Fig.3). Instead
of feed-forward inference we optimize, or synthesize, a model of the object parts over time, to
understand(i.eanalyze)theirmotion. 4DpartposeovertimeisrepresentedasatrajectoryofSE(3)
posespertime-step,whereeachconsecutivetimestepisinitializedfromtheprevious.Thisapproach
leverages the differentiable rendering of feature fields to backpropagate pixel errors into 3D pose
deltas.Thisisappealingoverfeedforwardtrackingmethodsasitcanbeintegratedwithsophisticated
regularizationslikegeometricrigidityortemporalsmoothingthroughoptimization,asweleverage
in this work. In addition, 4D-DPM’s use of large pretrained vision models lends robustness to a
largediversityofobjectszero-shot.
PartMotionOptimizationfromVideo Toobtainposeupdatesforanobject’spartsgivenanew
frame, we render the 4D-DPM at a virtual camera with the same intrinsics as the input video to
obtain rendered outputs including RGB images, depth maps, and DINO feature maps. In parallel,
we extract DINO features from the input video frame, and compare them directly to the rendered
5Figure5: ARAPAblation. ARAPisasimplebuteffectivepriorforimproving3Dmotionrecovery
bypreventingsmallorunder-observedpartsfromdrifting.
featureswithanMSEloss. Becauserenderingisfullydifferentiablewithrespecttothepartposes,
by backpropagating through the entire rendering process the poses of individual object parts can
be optimized with gradient descent (Fig 3). We experimentally validate that DINO features offer
a much more robust optimization target than photometric loss, and so all experiments use DINO
insteadofphotometric(Tab2).
To reduce high-frequency noise in the rendered DINO features, we blur them with a kernel equal
totheViTpatchsize,andclipDINOfeatureswhichcorrespondtolowalphaintherenderedview,
which can correspond to stray floating gaussians. Pose optimization is first done per-frame with
notemporalsmoothing,iterating50stepswiththeAdam[75]optimizer. Weoptimizeposeoffsets
represented asquaternions and translations, suchthat transforms apply relativeto each object part
centroid. Over the course of the optimization for each frame, the learning rate is initialized high,
thendecayedbyafactorof5xtoallowposestosettle.Toimprovethetemporalcoherenceofmotion
recovery,afterper-frametrackingwejointlyoptimizeallframesatoncewithaLaplaciantemporal
smoothness loss on neighboring poses. See Fig 3 for an illustration and the Appendix for more
details.
3DRegularizationPriors ReliablytrackingposefrompuremonocularRGBisasignificantchal-
lenge because of depth ambiguity. 4D-DPM uses the fact that all object parts can be jointly op-
timized, allowing for regularizing the optimization with external 3D priors using auxiliary losses.
Weusetwo3Dpriors: aregularizationfromamono-depthpredictionnetworkandalocalas-rigid-
as-possible (ARAP) penalty. These losses are added to the primary DINO loss described above,
please see the Appendix for hyperparameters. The first regularization L imposes a soft con-
mono
strainttowardsoutputsfromDepthAnything[76]. Toaccountforthefactthatmono-depthoutput
isnon-metric,weusetheranking-basedlossproposedinSparse-NeRF[77]. Specifically,wesam-
plepairsofpointswithintherenderedobjectmaskandenforcetheirrelativedepthordersbetween
ourrendereddepthandthemono-depthtomatch. Theobjectmaskiserodedby5pixelstoreduce
sensitivitytomisalignment.
Thesecondlossisanadaptationofas-rigid-as-possible(ARAP)[78]loss,whichisonlyappliedto
boundarygaussiansbetweenparts.WecomputeL byfindingboundarygaussiansbetweeneach
ARAP
pairofparts,definedbyaradiusthresholdof2.5mmontheircenters,andstoringtheinitialdistance
between neighboring pairs d . During optimization we impose a loss penalizing gaussians from
init
driftingawayfromthatinitialdistance(cid:80) ρ(dij −dij ). Weusetherobustlossρproposedin
ij init current
Barron [79], setting α = 1.0 for objects whose parts stay attached, and α = 0.1 for objects con-
tainingseparableparts(nerfgun,USBcable),whichdecreasesthestrengthofthelossforgaussians
whichdeviatefaraway. L doesnotpenalizeneighboringgaussiansfromrotatingwithrespect
ARAP
tooneanother,allowinghingingmovementeasily.
Initialization During robot execution and for the first frame of each demonstration video, 4D-
DPMmustestimatetheobject’sSE(3)poseeitherformanipulatingtheobject,orinitializingthe3D
motionestimationforsubsequentframes.Toinitializetheobjectposeforasingleframe,RSRDfirst
6approximately locates the object in the 2D image. To do this, we compare the object’s 3D DINO
features to the frame’s 2D DINO features, finding mutual nearest neighbors between 3D gaussian
andpixelfeatures. Thepixelcentroidofthesematchescreatesarayin3Dspace, alongwhichwe
placethecentroidoftheobjectin3Datafixeddistancefromthecamera. 4D-DPMthenexecutes
8seedsofobjectposeoptimizationfor200iterations, rotatingabouttheobject’sgravityaxis, and
select the pose with lowest loss. During robot execution, the exact same procedure is used with
stereodepthinsteadofmonoculardepth.
4.3 ObjectMotionandGraspPlanning
Once the poses of the object parts are registered in the world frame of the robot, we plan feasible
robottrajectoriestoimpartthedesiredmotionontotheobject. Thistakesplaceinthreestages: 1)
part selection, to decide which parts should be moved; 2) grasp planning, to decide which parts
are kinematically reachable by the robot; and 3) trajectory planning, to decide which parts can be
manipulatedthroughthefullparttrajectoryperformedinthedemonstration.
Hand-Guided Part Selection We first create a list of candidate parts for the robot to interact
with,byestimatingwhichonesthehumanhandinteractswith. Notethenaivemethodofchoosing
the maximally-traveled part will fail if object parts are coupled. For example, for “opening the
scissors”, where all parts move the same distance, the motion cannot actuate the scissors if the
chosen parts are the scissor blade and handle rigidly connected to each other. Biasing with the
handhelpsavoidthesedegeneratepairs. Weemphasizethesedetectionsareonlyusedtobiaspart
selection,butnotgrasps,sincehandscanperformgraspsimpossiblewithaparallel-jawgripper.
Todetectthehumanhandsin3D,weuseHaMeR[74]todetecthandposemeshes. Wecalculatethe
hand’smetricsizeandposebymatchingittotheestimatedmetricdepthofthehand,whichwecal-
culatebyscalingandshiftingtheimagemonodepthbytherenderedobjectgaussiandepth.Then,we
register part-hand interactions by computing the part-hand assignments which globally minimizes
thumb and index finger distance to the given parts across the trajectory. Finally, a ranked list of
actuatedobjectpartsiscalculatedbasedonthepartdistancemetric. Forbimanualdemonstrations,
webuildtwolists(oneforeachhand).
Part-CentricGraspPlanning Beforeweaskarobottoreliablyexecuteobjectmotions,wemust
firstaccountforthelimitationsofarobotparallel-jawgripper. Mainly,thehumanhandcanflexibly
slidearoundanobjectpartwithcontrolledcontact,oruseprehensilemotionorwidegrabs–which
mightnotbepossiblewitharobot. Thurs,itisimportantthattherobotstaysrigidlyattachedtothe
objectpartthroughoutthetrajectory,andeverypartmusthaveviablegrasps,shouldthatpartneed
tobemanipulated.
Weuseananalyticgraspgenerationmethodtoguaranteegraspsoneverypart,becauseoff-the-shelf
graspplannerstendtofocusonholisticobjectgraspingratherthanpart-level. Itfirstconvertseach
group of gaussian centers to a mesh by taking its alpha shape, then smooths and decimates it to
produce smooth normals. We sample 20 antipodal grasps axes per part using the grasp procedure
describedinMahleretal.[80],thenaugmentthemwithrotationandandgraspaxistranslationinto
480graspswhicharestoredinthepartframeforlaterusage.
Robot Trajectory Planning Given the candidate list of parts, part-centric grasps, and part mo-
tions, we exhaustively search for collision-free, kinematically feasible robot trajectories. We first
create a list of parts [p ,p ,...], or a list of part-part pairs [(p ,p ),...] for bimanual tasks. Then,
1 2 1 2
foreachcandidatepart(s),wegeneratealistofrobotend-effectorposemotions,eachstartingfrom
one of the 480 part-centric grasps. We implement a sparse Levenberg-Marquardt solver that per-
formstrajectoryoptimizationforeachpartmotion. Trajectoriesthatdeviatetoofarfromthetarget
end-effector poses are rejected. For the remaining trajectories, we use cuRobo [81] for collision
avoidancechecksandtoplanrobotapproachmotions. Wereturnthefirstsuccessfultrajectoryfor
7Objects Init. Traj. Grasp Exec.
RedBox 10/10 9/10 8/9 4/8
NerfGun 10/10 10/10 8/10 7/8
Scissors 9/10 7/9 5/7 5/5
Sunglasses 8/10 8/8 7/8 7/7
Bear 10/10 7/10 6/7 6/6
Stapler 9/10 9/9 7/9 7/7
Light 10/10 10/10 8/10 6/8
Wirecutter 10/10 10/10 7/10 6/7
USBPlug 8/10 7/8 7/7 6/7
Total 84/90 77/84 63/77 54/63
Table1: PhysicalTrials. Wereportsuccessofindi-
vidualstagesoftheRSRDpipeline: objectposein-
Figure 6: Example Robot Executions. Ar- tialization,trajectoryplanning,graspexecution,and
rowsindicatedirectionofmotion. motionexecution.
physicalexecution. Inbimanualexperiments,therobotliftstheobjectwithbothhands2cmoffthe
workspacetoavoidtablecollisions.
5 ExperimentalResults
Forphysicalexecution,weuseanABBYuMirobotbecauseofits7-DoFbimanualarms,andequip
itwithsoft3D-printedparallel-jawgrippersfromElgeneidyetal.[82]. Extracompliancefromsoft
caginggraspsishelpfulformakingrobotexecutionlesssensitivetoerrorinobjecttracking. Weuse
aZED2stereocameraforprovidingdepthestimatesformoreaccurateobjectregistration.
TocapturedatafordemonstrationsweusethePolycamphonescanner,whichprovidesposedcam-
eras with metric scale. We collect demonstrations for articulated objects (Fig. 2) which consist of
a human demonstrating a degree of freedom to actuate with one or both hands clearly visible. In
thisworkweusedemonstrationvideoswhichclearlyshowtheobject-handinteractionwithsimple
backgroundsandleavecomplicatedin-the-wildvideostofuturework.
5.1 DemonstrationExecution
TotesthowwellRSRDcantransferhumandemonstrationstoarobot,weselect9articulatedobjects
for the robot to actuate, listed in Tab. 1 and detailed in the Appendix. We run 10 trials for each
objectontherobot,varyingthezorientationoftheobject360◦ aboutitscentroidwhileremaining
centeredintherobotworkspace. Thismeansinhalfoftheexperimentstheinitialposeoftheobject
is flipped as compared to the demonstration video, necessitating 3D object-centric reasoning to
performthetask.Wemeasurethesuccessrateofeachstageofthepipeline:poseinitialization,grasp
planning,physicallygrasping,andexecutingthemotion. Thefinalstateisevaluatedqualitativelyby
anexperimenterwhowatchestherobot’sfullmotion,andchecksiftherobotimpartedsemantically
similarpartmotionasthedemonstrationvideo(e.g.,“closethesunglasses”,“wigglethebear’sright
arm”). Seethewebsiteforexamplerobotexecutions.
Full results are reported in Table 1. Overall, RSRD can reliably register objects in the correct
pose in 84 of 90 trials, after which it plans feasible robot motions for 77 of 84 registered poses.
ThishighlightsRSRD’sflexibilitytoobjectre-orientationwithintheworkspace,reproducingobject
motionseventhoughtheobjectismirroredwithrespecttotheinputdemo.Whenphysicallygrasping
and executing these motions, RSRD succeeds for 63 of 77 final plans. This performance dropoff
comes primarily from the very low grasp error tolerance needed to accomplish tasks, with many
cases narrowly missing grasps after grazing the object during approach. For the red box, subtle
trackingshiftcausesthedemonstrationtoliftupwardsbyaround3cmintheworkspace,liftingthe
boxintheairandsometimesdroppingitonitssideratherthanupright.
8Method RedBox NerfGun ToyDrawer Sunglasses Frog Average
4D-DPM 8.16±0.77 3.37±0.64 5.85±1.52 4.58±0.39 10.10±2.02 6.41±1.07
NoDepth 9.20±3.40 3.71±0.53 6.87±1.40 4.66±0.69 21.43±3.00 9.17±1.80
NoARAP 13.05±2.55 4.06±0.49 7.74±1.45 13.45±2.13 17.27±3.00 11.11±1.92
Photometric 47.14±4.93 58.87±7.39 74.23±6.02 56.34±2.11 47.09±7.15 56.73±5.52
Table 2: Object Part Pose Tracking Evaluation. We report object part pose tracking accuracy
measuredbyaveragepoint-distance(ADD).RSRDsignificantlyoutperformsphotometrictracking,
owingtoitsuseofDINOfeaturesasarobustoptimizationtarget.
.
5.2 MotionRecoveryAblations
Weevaluateparttrackingperformanceon5objectsbycapturingtheirdemonstrationswithastereo
camera,thenmanuallyannotatingthegroundtruthpartposeatselectkeyframestomatchthestereo
depthobtainedfromRAFT-Stereo[83]. Theannotationprocessinvolvesusingthe3Dvisualization
tool viser, which allows us to interact with the gaussians representing the parts of the objects. By
aligning these gaussians with the depth obtained from the Zed camera, we ensure the correspon-
dence between the annotated poses and actual depth information. Following Wen et al. [84], we
reportaveragepoint-wisedistance(ADD)andcompareagainst3ablationsofourmethod:4D-DPM
withoutARAP,4D-DPMwithoutdepthregularization,andPhotometriconlytracking.
ResultsarereportedinTable2. OnaverageRSRDachievesamean7.5mmaveragepointdistance
on tracking the manipulated 3D part pose. Removing ARAP results in on average 2.1mm worse
position tracking performance for the primary manipulated part, mattering especially for objects
withmanypartslikethefrogplushie. Inaddition,ARAPmattersmoreforsmallpartsofobjects,as
shown by the degradation in object cohesion in Fig. 5. Motion recovery with DINO feature fields
strongly outperforms photometric tracking, which diverges severely in most cases because of an
inabilitytodistinguishbetweenforegroundandbackground.
Qualitative 4D reconstruction results are best viewed in the supplementary video. See Fig. 2 for
exampleframesoftrajectoryoutputsforsometestobjects. RSRDcanfunctiononavarietyoflong-
tailandcommonobjects,owingtoitsusageoffeaturefieldsfromlargepretrainedmodels. Thiscan
extendeventohighlyjointedobjectslikethewoodendoll,whereeveryarmlinkisaseparatepart.
RSRD can struggle on cases where an object is highly symmetric, like the arms of the glasses or
doll’sarm. Thesehaveatendencytojitteralongtheirmajoraxis,aneffectwhichcanbestbeseen
invideos.
6 Discussion
LimitationsandFutureWork ThemainlimitationofRSRDistheassumptionthatobjectstart
configurations match their demonstration, and thus is sensitive to small amounts of difference in
initialconfigurations. Futureworkwillstudyhowtoadapttothesecases. Inaddition,theexistence
of manual segmentation phase should ideally be automated to increase scalability of the method,
perhaps based on the perceived motion in the demonstration. Because the method uses monocu-
lar RGB input only, it is also sensitive to the quality of the object scan and the viewing angle of
thedemonstrationvideo,sometimesstrugglingincaseswherethebackgroundinthedemonstration
is too complicated, overpowering the DINO features on the object. It also struggles with partial
object self-occlusions which can result in gaussian rasterization z-fighting and hence noisy pose
updates. Finally, the tracker struggles with highly symmetric or featureless objects where DINO
doesn’t provide enough motion cues, or objects with small parts. Robot execution in RSRD cur-
rentlyassumesrigidparallel-jawgrasps,anassumptionwhichwouldbeinterestingtoliftinfuture
workforplanningnon-prehensilemotions. Futureworkwillalsostudyextendingthistofew-shot
demonstrations, where multiple different demonstrations or viewpoints could be fused into more
robustobject-centricmanipulation.
9Conclusion This paper presents Robot See Robot Do, a method for teaching articulated object
motionswithasinglemonocularhumandemonstration,andreplicatingthemonabimanualrobot.
It takes advantages of neural feature fields for building a part-aware model of the object which
canbetrackedinaninputmonocularvideowithouttheneedforlabeledpartdatasets. Becauseof
itsobject-centricnature,RSRDcanapplylearneddemonstrationstoarbitraryreorientationsofthe
objectwhiletransferingdemonstrationmorphology.
Acknowledgement This project was funded in part by NSF:CNS-2235013, IARPA DOI/IBC No.
140D0423C0035, andDARPANo. HR001123C0021; JustinKerr, ChungMinKim, andBrentYi
aresupportedbytheNSFResearchFellowshipProgram,GrantDGE2146752.
10A Appendix
A.1 ImplementationDetails
Part-CentricFeatureFields OurimplementationisbuiltonNerfstudio’sSplatfactomodel,taking
advantage of the same splitting and culling logic. We represent DINOv2 ViT-B/14 features by
taking the PCA across all input image features to compress them to 64 dimensions, then assign
everygaussianalearnable32-dimensionvector.Thesevectorscanberasterizedwiththeexactsame
rendering equations as RGB, using the N-D rasterization implementation from the gsplat library.
Afterrasterization, pixelvaluesarepassedthrougha4-layer, 64-wideMLPtooutputthefinal64-
dimensionfeatures. TheoutputsaresupervisedwithasimpleMSElossagainsttheimagefeatures.
We additionally apply a nearest-neighbors total-variation loss, which at each step minimizes the
standarddeviationofagaussianwithits3neighbors,encouragingfeatureembeddingstobespatially
smooth. TorefinecameraposesfromtheirpotentiallynoisyinitializationfromPolycam,weenable
cameraoptimizationfromviewmatrixgradientspropagatedfromRGBrasterization.
Tracking During loss calculation we weight the three optimization objectives with λ =
ARAP
0.2, λ = 0.5, λ = 1beforesumming. Adam’slearningrateisdecreasedfrom0.005
MONO DINO
to 0.0005 over the course of 50 steps each frame with an exponential decay. During tracking we
sample 30,000 random pairs within the object mask to use with the sparse depth loss, where the
objectmaskisdefinedbypixelswithrenderedalphavaluesover0.9(mostlyopaque).
Speed We train the part-centric feature field for 6000 steps, which takes about 3 minutes on an
RTX4090GPU.Priortothis,wetrainaGARFieldmodelto10000steps,whichtakesaround5-10
minutesdependingonthenumberofinputimages. Duringtracking,ourtrajectoriesconsistof3-4
second video clips at 30fps, and tracking runs at approximately 1.4sec/frame, taking 2-3 minutes
totalpertrajectory. Notably, handdetectionaccountforasubstantialportionofthetimespentper
frame,andwithouthandtrackingcanrunatapproximately1.2fps. Detectingtheobjectposeinthe
robot’s workspace takes 30 seconds to run the multi-seed optimization search. Computing grasps
andmotionplanningcollision-freetrajectoriestakesaround1minuteforsinglehanddemonstrations
and 3 minutes for bimanual demonstrations. Speeding up and streamlining the pipeline is a clear
opportunityforfutureworktostudy.
GraspandMotionPlanning Asdescribedinmaintext’ssection4.3,partcontactselectionout-
putsarankedlistofcandidateobjectpartstointeractwith,fromhumanhanddetection. Then,the
planner attempts to find the first set of parts where the motion is executable. For bimanual tasks
thelistiscomposedoflength-twotuples[(p ,p ),...],onepartforeachhand,andweexhaustively
1 2
check over both arms i.e., left arm to p and right to p , and vice versa. We first optimize for the
1 2
robotmotionfollowingtheposeofthedesiredobjectusingatrajectoryoptimizerimplementedvia
sparse Levenberg-Marquardt in JAX [85], optimizing for smooth joint positions given a set of 6D
robot gripper poses, as cuRobo does not provide a waypoint-based trajectory optimization. Then,
forthesuccessfultrajectories,weusecuRobotoplancollision-freetrajectoriestothepre-graspand
graspposeforeachpart.
A.2 ExperimentDetails
Robot Trials Please see the supplemental video for example executions of these motions on the
robot,aswellasfailurecasevideos.
Theexperimentmotionsforeachobjectsareasfollows:
1. RedBox: Closingthebox,byloweringthelid
2. NerfGun: Slidingbackthefiringmechanismofthegun
3. Scissors: Closing,thenopeningthescissors
4. Sunglasses: Foldingbacktheleftlegofthesunglasses
115. Bear: Wavingtherightarmofthebear
6. LEDLight: un-foldingtheLEDlightpanel90degreesup
7. Wirecutter: closingandopening
8. Stapler: foldingthestaplerclosedfromanopenposition
9. USBPlug: unpluggingtheusbcablefromapowerbrick
TrackingEvaluation 3Dposeforparttrajectoriesismanuallyannotatedforkeyframesbyvisu-
alizing the dense RGB-pointcloud obtained from the depth camera in a 3D viewer, then manually
movingtherenderedgaussiansplatoftheobjectparttoalignwiththispointcloud.
12References
[1] C. M. Kim, M. Wu, J. Kerr, M. Tancik, K. Goldberg, and A. Kanazawa. Garfield: Group
anythingwithradiancefields. InarXiv,2024.
[2] B. Kerbl, G. Kopanas, T. Leimku¨hler, and G. Drettakis. 3d gaussian splatting for real-time
radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023. URL https:
//repo-sam.inria.fr/fungraph/3d-gaussian-splatting/.
[3] M.Caron,H.Touvron,I.Misra,H.Je´gou,J.Mairal,P.Bojanowski,andA.Joulin. Emerging
properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF interna-
tionalconferenceoncomputervision,pages9650–9660,2021.
[4] H. Li, G. Wan, H. Li, A. Sharf, K. Xu, and B. Chen. Mobility fitting using 4d ransac. In
ComputerGraphicsForum,volume35,pages79–88.WileyOnlineLibrary,2016.
[5] Y. Shi, X. Cao, and B. Zhou. Self-supervised learning of part mobility from point cloud
sequence. In Computer Graphics Forum, volume 40, pages 104–116. Wiley Online Library,
2021.
[6] A. Jain, R. Lioutikov, C. Chuck, and S. Niekum. Screwnet: Category-independent articu-
lation model estimation from depth images using screw theory. In 2021 IEEE International
ConferenceonRoboticsandAutomation(ICRA),pages13670–13677.IEEE,2021.
[7] N. Heppert, T. Migimatsu, B. Yi, C. Chen, and J. Bohg. Category-independent articulated
objecttrackingwithfactorgraphs. In2022IEEE/RSJInternationalConferenceonIntelligent
RobotsandSystems(IROS),pages3800–3807.IEEE,2022.
[8] X. Wang, B. Zhou, Y. Shi, X. Chen, Q. Zhao, and K. Xu. Shape2motion: Joint analysis of
motionpartsandattributesfrom3dshapes. InProceedingsoftheIEEE/CVFConferenceon
ComputerVisionandPatternRecognition,pages8876–8884,2019.
[9] Z.Yan,R.Hu,X.Yan,L.Chen,O.VanKaick,H.Zhang,andH.Huang. Rpm-net: recurrent
predictionofmotionandpartsfrompointcloud. arXivpreprintarXiv:2006.14865,2020.
[10] X. Li, H. Wang, L. Yi, L. J. Guibas, A. L. Abbott, and S. Song. Category-level articulated
objectposeestimation. InProceedingsoftheIEEE/CVFconferenceoncomputervisionand
patternrecognition,pages3706–3715,2020.
[11] X. Liu, J. Zhang, R. Hu, H. Huang, H. Wang, and L. Yi. Self-supervised category-
level articulated object pose estimation with part-level se (3) equivariance. arXiv preprint
arXiv:2302.14268,2023.
[12] C. Deng, J. Lei, W. B. Shen, K. Daniilidis, and L. J. Guibas. Banana: Banach fixed-point
networkforpointcloudsegmentationwithinter-partequivariance. AdvancesinNeuralInfor-
mationProcessingSystems,36,2024.
[13] Z. Mandi, Y. Weng, D. Bauer, and S. Song. Real2code: Reconstruct articulated objects via
codegeneration,2024. URLhttps://arxiv.org/abs/2406.08474.
[14] A. Noguchi, U. Iqbal, J. Tremblay, T. Harada, and O. Gallo. Watch it move: Unsupervised
discovery of 3d joints for re-posing of articulated objects. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages3677–3687,2022.
[15] J.Liu,A.Mahdavi-Amiri,andM.Savva. Paris: Part-levelreconstructionandmotionanalysis
forarticulatedobjects.InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
Vision,pages352–363,2023.
13[16] F. Wei, R. Chabra, L. Ma, C. Lassner, M. Zollho¨fer, S. Rusinkiewicz, C. Sweeney, R. New-
combe,andM.Slavcheva. Self-supervisedneuralarticulatedshapeandappearancemodels. In
ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition,pages
15816–15826,2022.
[17] J.Prankl,A.Aldoma,A.Svejda,andM.Vincze.Rgb-dobjectmodellingforobjectrecognition
and tracking. In 2015 IEEE/RSJ international conference on intelligent robots and systems
(IROS),pages96–103.IEEE,2015.
[18] Y.LiuandA.Namiki. Articulatedobjecttrackingbyhigh-speedmonocularrgbcamera. IEEE
SensorsJournal,21(10):11899–11915,2020.
[19] Z.Pezzementi,S.Voros,andG.D.Hager. Articulatedobjecttrackingbyrenderingconsistent
appearanceparts. In2009IEEEInternationalConferenceonRoboticsandAutomation,pages
3940–3947.IEEE,2009.
[20] F.KokkinosandI.Kokkinos. Learningmonocular3dreconstructionofarticulatedcategories
from motion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition,pages1737–1746,2021.
[21] J.Romero,H.Kjellstro¨m,andD.Kragic. Monocularreal-time3darticulatedhandposeesti-
mation. In20099thIEEE-RASInternationalConferenceonHumanoidRobots,pages87–92.
IEEE,2009.
[22] H.Geng, H.Xu, C.Zhao, C.Xu, L.Yi, S.Huang, andH.Wang. Gapartnet: Cross-category
domain-generalizable object perception and manipulation via generalizable and actionable
parts. InProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion,pages7081–7091,2023.
[23] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao, S. Whitehead,
A.C.Berg,W.-Y.Lo,etal. Segmentanything. InProceedingsoftheIEEE/CVFInternational
ConferenceonComputerVision,pages4015–4026,2023.
[24] F. Torabi, G. Warnell, and P. Stone. Behavioral cloning from observation. arXiv preprint
arXiv:1805.01954,2018.
[25] Y.-T.A.Sun,H.-C.Lin,P.-Y.Wu,andJ.-T.Huang. Learningbywatchingviakeypointextrac-
tionandimitationlearning. Machines,10(11):1049,2022.
[26] J.Yang,J.Zhang,C.Settle,A.Rai,R.Antonova,andJ.Bohg. Learningperiodictasksfrom
humandemonstrations.In2022InternationalConferenceonRoboticsandAutomation(ICRA),
pages8658–8665.IEEE,2022.
[27] C.Wen,X.Lin,J.So,K.Chen,Q.Dou,Y.Gao,andP.Abbeel. Any-pointtrajectorymodeling
forpolicylearning. arXivpreprintarXiv:2401.00025,2023.
[28] Y.Duan,M.Andrychowicz,B.Stadie,O.JonathanHo,J.Schneider,I.Sutskever,P.Abbeel,
and W. Zaremba. One-shot imitation learning. Advances in neural information processing
systems,30,2017.
[29] C. Finn, T. Yu, T. Zhang, P. Abbeel, and S. Levine. One-shot visual imitation learning via
meta-learning. InConferenceonrobotlearning,pages357–368.PMLR,2017.
[30] N.DiPaloandE.Johns. Learningmulti-stagetaskswithonedemonstrationviaself-replay. In
ConferenceonRobotLearning,pages1180–1189.PMLR,2022.
[31] S. Bahl, A. Gupta, and D. Pathak. Human-to-robot imitation in the wild. arXiv preprint
arXiv:2207.09450,2022.
14[32] E. Valassakis, G. Papagiannis, N. Di Palo, and E. Johns. Demonstrate once, imitate imme-
diately (dome): Learning visual servoing for one-shot imitation learning. In 2022 IEEE/RSJ
InternationalConferenceonIntelligentRobotsandSystems(IROS),pages8614–8621.IEEE,
2022.
[33] D.-A.Huang,D.Xu,Y.Zhu,A.Garg,S.Savarese,L.Fei-Fei,andJ.C.Niebles. Continuous
relaxationofsymbolicplannerforone-shotimitationlearning.In2019IEEE/RSJInternational
ConferenceonIntelligentRobotsandSystems(IROS),pages2635–2642.IEEE,2019.
[34] T. Yu, C. Finn, A. Xie, S. Dasari, T. Zhang, P. Abbeel, and S. Levine. One-shot imitation
fromobservinghumansviadomain-adaptivemeta-learning.arXivpreprintarXiv:1802.01557,
2018.
[35] A. Bonardi, S. James, and A. J. Davison. Learning one-shot imitation from humans without
humans. IEEERoboticsandAutomationLetters,5(2):3533–3539,2020.
[36] X.B.Peng,A.Kanazawa,J.Malik,P.Abbeel,andS.Levine. Sfv: Reinforcementlearningof
physicalskillsfromvideos. ACMTrans.Graph.,37(6),Nov.2018.
[37] P.R.Florence,L.Manuelli,andR.Tedrake. Denseobjectnets: Learningdensevisualobject
descriptorsbyandforroboticmanipulation. arXivpreprintarXiv:1806.08756,2018.
[38] M. Janner, S. Levine, W. T. Freeman, J. B. Tenenbaum, C. Finn, and J. Wu. Reason-
ing about physical interactions with object-oriented prediction and planning. arXiv preprint
arXiv:1812.10972,2018.
[39] W.Yuan,C.Paxton,K.Desingh,andD.Fox. Sornet:Spatialobject-centricrepresentationsfor
sequentialmanipulation. InConferenceonRobotLearning,pages148–157.PMLR,2022.
[40] Y.Zhu,A.Joshi,P.Stone,andY.Zhu. Viola:Imitationlearningforvision-basedmanipulation
withobjectproposalpriors. 6thAnnualConferenceonRobotLearning(CoRL),2022.
[41] Y. Zhu, Z. Jiang, P. Stone, and Y. Zhu. Learning generalizable manipulation policies with
object-centric3drepresentations. arXivpreprintarXiv:2310.14386,2023.
[42] M.Sieb,Z.Xian,A.Huang,O.Kroemer,andK.Fragkiadaki. Graph-structuredvisualimita-
tion. InConferenceonRobotLearning,pages979–989.PMLR,2020.
[43] M. Sieb and K. Fragkiadaki. Data dreaming for object detection: Learning object-centric
staterepresentationsforvisualimitation. In2018IEEE-RAS18thInternationalConferenceon
HumanoidRobots(Humanoids),pages1–9.IEEE,2018.
[44] C. Diuk, A. Cohen, and M. L. Littman. An object-oriented representation for efficient rein-
forcementlearning. InProceedingsofthe25thinternationalconferenceonMachinelearning,
pages240–247,2008.
[45] C.Devin,P.Abbeel,T.Darrell,andS.Levine. Deepobject-centricrepresentationsforgener-
alizablerobotlearning. In2018IEEEInternationalConferenceonRoboticsandAutomation
(ICRA),pages7111–7118.IEEE,2018.
[46] J.Tremblay,T.To,B.Sundaralingam,Y.Xiang,D.Fox,andS.Birchfield.Deepobjectposees-
timationforsemanticroboticgraspingofhouseholdobjects.arXivpreprintarXiv:1809.10790,
2018.
[47] R. Li, A. Jabri, T. Darrell, and P. Agrawal. Towards practical multi-object manipulation us-
ing relational reinforcement learning. In 2020 ieee international conference on robotics and
automation(icra),pages4051–4058.IEEE,2020.
15[48] D.Driess,Z.Huang,Y.Li,R.Tedrake,andM.Toussaint.Learningmulti-objectdynamicswith
compositionalneuralradiancefields.InProceedingsofThe6thConferenceonRobotLearning,
volume205ofProceedingsofMachineLearningResearch,pages1755–1768.PMLR,14–18
Dec2023.
[49] V. Zambaldi, D. Raposo, A. Santoro, V. Bapst, Y. Li, I. Babuschkin, K. Tuyls, D. Reichert,
T. Lillicrap, E. Lockhart, et al. Relational deep reinforcement learning. arXiv preprint
arXiv:1806.01830,2018.
[50] C. Paxton, C. Xie, T. Hermans, and D. Fox. Predicting stable configurations for semantic
placementofnovelobjects. InConferenceonrobotlearning,pages806–815.PMLR,2022.
[51] M. A. Lee, C. Florensa, J. Tremblay, N. Ratliff, A. Garg, F. Ramos, and D. Fox. Guided
uncertainty-aware policy optimization: Combining learning and model-based strategies for
sample-efficient policy learning. In 2020 IEEE International Conference on Robotics and
Automation(ICRA),pages7505–7512.IEEE,2020.
[52] M.A.Lee,B.Yi,R.Mart´ın-Mart´ın,S.Savarese,andJ.Bohg. Multimodalsensorfusionwith
differentiable filters. In 2020 IEEE/RSJ International Conference on Intelligent Robots and
Systems(IROS),pages10444–10451.IEEE,2020.
[53] K. Desingh, S. Lu, A. Opipari, and O. C. Jenkins. Factored pose estimation of articulated
objectsusingefficientnonparametricbeliefpropagation. In2019InternationalConferenceon
RoboticsandAutomation(ICRA),pages7221–7227.IEEE,2019.
[54] D. Driess, F. Xia, M. S. Sajjadi, C. Lynch, A. Chowdhery, B. Ichter, A. Wahid, J. Tompson,
Q. Vuong, T. Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint
arXiv:2303.03378,2023.
[55] K.Lin,C.Agia,T.Migimatsu,M.Pavone,andJ.Bohg. Text2motion: Fromnaturallanguage
instructionstofeasibleplans. AutonomousRobots,47(8):1345–1365,2023.
[56] T.MigimatsuandJ.Bohg. Object-centrictaskandmotionplanningindynamicenvironments.
IEEERoboticsandAutomationLetters,5(2):844–851,2020.
[57] Y.Zhu,J.Tremblay,S.Birchfield,andY.Zhu. Hierarchicalplanningforlong-horizonmanip-
ulationwithgeometricandsymbolicscenegraphs. In2021IEEEInternationalConferenceon
RoboticsandAutomation(ICRA),pages6541–6548.IEEE,2021.
[58] L. P. Kaelbling and T. Lozano-Pe´rez. Hierarchical task and motion planning in the now. In
2011 IEEE International Conference on Robotics and Automation, pages 1470–1477. IEEE,
2011.
[59] R. Alami, J.-P. Laumond, and T. Sime´on. Two manipulation planning algorithms. In WAFR
ProceedingsoftheworkshoponAlgorithmicfoundationsofrobotics,pages109–125.AKPe-
ters,Ltd.Natick,MA,USA,1994.
[60] W.Liu,J.Mao,J.Hsu,T.Hermans,A.Garg,andJ.Wu.Composablepart-basedmanipulation.
arXivpreprintarXiv:2405.05876,2024.
[61] D.Hadjivelichkov,S.Zwane,L.Agapito,M.P.Deisenroth,andD.Kanoulas.One-shottransfer
of affordance regions? affcorrs! In Conference on Robot Learning, pages 550–560. PMLR,
2023.
[62] B.Mildenhall,P.P.Srinivasan,M.Tancik,J.T.Barron,R.Ramamoorthi,andR.Ng. NeRF:
Representingscenesasneuralradiancefieldsforviewsynthesis. InEuropeanConferenceon
ComputerVision,pages405–421.Springer,2020.
16[63] J. Kerr, L. Fu, H. Huang, Y. Avigal, M. Tancik, J. Ichnowski, A. Kanazawa, and K. Gold-
berg. Evo-neRF: Evolving neRF for sequential robot grasping of transparent objects. In 6th
AnnualConferenceonRobotLearning,2022. URLhttps://openreview.net/forum?id=
Bxr45keYrf.
[64] J.Ichnowski*,Y.Avigal*,J.Kerr,andK.Goldberg. Dex-NeRF:Usinganeuralradiancefield
tograsptransparentobjects. InConferenceonRobotLearning(CoRL),2020.
[65] M.Adamkiewicz,T.Chen,A.Caccavale,R.Gardner,P.Culbertson,J.Bohg,andM.Schwa-
ger. Vision-onlyrobotnavigationinaneuralradianceworld. IEEERoboticsandAutomation
Letters,7(2):4606–4613,2022.
[66] A.Rashid,S.Sharma,C.M.Kim,J.Kerr,L.Y.Chen,A.Kanazawa,andK.Goldberg. Lan-
guageembeddedradiancefieldsforzero-shottask-orientedgrasping.In7thAnnualConference
onRobotLearning,2023. URLhttps://openreview.net/forum?id=k-Fg8JDQmc.
[67] W.Shen,G.Yang,A.Yu,J.Wong,L.P.Kaelbling,andP.Isola. Distilledfeaturefieldsenable
few-shotlanguage-guidedmanipulation. In7thAnnualConferenceonRobotLearning,2023.
[68] Y.Zheng,X.Chen,Y.Zheng,S.Gu,R.Yang,B.Jin,P.Li,C.Zhong,Z.Wang,L.Liu,etal.
Gaussiangrasper: 3dlanguagegaussiansplattingforopen-vocabularyroboticgrasping. arXiv
preprintarXiv:2403.09637,2024.
[69] M.Qin, W.Li, J.Zhou, H.Wang, andH.Pfister. Langsplat: 3dlanguagegaussiansplatting.
arXivpreprintarXiv:2312.16084,2023.
[70] Y.Wang, Z.Li, M.Zhang, K.Driggs-Campbell, J.Wu, L.Fei-Fei, andY.Li. D3fields: Dy-
namic 3d descriptor fields for zero-shot generalizable robotic manipulation. arXiv preprint
arXiv:2309.16118,2023.
[71] S.Kobayashi,E.Matsumoto,andV.Sitzmann. Decomposingnerfforeditingviafeaturefield
distillation. In NeurIPS, volume 35, 2022. URL https://arxiv.org/pdf/2205.15585.
pdf.
[72] M. Tancik, E. Weber, E. Ng, R. Li, B. Yi, J. Kerr, T. Wang, A. Kristoffersen, J. Austin,
K. Salahi, et al. Nerfstudio: A modular framework for neural radiance field development.
arXivpreprintarXiv:2302.04264,2023.
[73] V. Ye, R. Li, J. Kerr, M. Turkulainen, B. Yi, Z. Pan, O. Seiskari, J. Ye, J. Hu, M. Tancik,
andA.Kanazawa. gsplat: Anopen-sourcelibraryforgaussiansplatting,2024. URLhttps:
//arxiv.org/abs/2409.06765.
[74] G.Pavlakos,D.Shan,I.Radosavovic,A.Kanazawa,D.Fouhey,andJ.Malik. Reconstructing
handsin3Dwithtransformers. InCVPR,2024.
[75] D.P.Kingma,J.A.Ba,andJ.Adam. Amethodforstochasticoptimization.arxiv2014. arXiv
preprintarXiv:1412.6980,106:1,2020.
[76] L. Yang, B. Kang, Z. Huang, X. Xu, J. Feng, and H. Zhao. Depth anything: Unleashing the
poweroflarge-scaleunlabeleddata. InCVPR,2024.
[77] Guangcong,Z.Chen,C.C.Loy,andZ.Liu. Sparsenerf: Distillingdepthrankingforfew-shot
novelviewsynthesis. TechnicalReport,2023.
[78] O.SorkineandM.Alexa. As-rigid-as-possiblesurfacemodeling. InProceedingsofEURO-
GRAPHICS/ACMSIGGRAPHSymposiumonGeometryProcessing,pages109–116,2007.
[79] J.T.Barron. Ageneralandadaptiverobustlossfunction.2019ieee. InCVFConferenceon
ComputerVisionandPatternRecognition(CVPR),pages4326–4334,2017.
17[80] J.Mahler,J.Liang,S.Niyaz,M.Laskey,R.Doan,X.Liu,J.A.Ojea,andK.Goldberg. Dex-
net 2.0: Deep learning to plan robust grasps with synthetic point clouds and analytic grasp
metrics. arXivpreprintarXiv:1703.09312,2017.
[81] B.Sundaralingam,S.K.S.Hari,A.Fishman,C.Garrett,K.VanWyk,V.Blukis,A.Millane,
H.Oleynikova,A.Handa,F.Ramos,N.Ratliff,andD.Fox.Curobo:Parallelizedcollision-free
robotmotiongeneration. In2023IEEEInternationalConferenceonRoboticsandAutomation
(ICRA),pages8112–8119,2023. doi:10.1109/ICRA48891.2023.10160765.
[82] K. Elgeneidy, P. Lightbody, S. Pearson, and G. Neumann. Characterising 3d-printed soft fin
ray robotic fingers with layer jamming capability for delicate grasping. In 2019 2nd IEEE
International Conference on Soft Robotics (RoboSoft), pages 143–148, 2019. doi:10.1109/
ROBOSOFT.2019.8722715.
[83] L.Lipson,Z.Teed,andJ.Deng. Raft-stereo: Multilevelrecurrentfieldtransformsforstereo
matching. In2021InternationalConferenceon3DVision(3DV),pages218–227.IEEE,2021.
[84] B.Wen,W.Yang,J.Kautz,andS.Birchfield. Foundationpose:Unified6dposeestimationand
trackingofnovelobjects. arXivpreprintarXiv:2312.08344,2023.
[85] B.Yi,M.Lee,A.Kloss,R.Mart´ın-Mart´ın,andJ.Bohg. Differentiablefactorgraphoptimiza-
tionforlearningsmoothers. In2021IEEE/RSJInternationalConferenceonIntelligentRobots
andSystems(IROS),2021.
18