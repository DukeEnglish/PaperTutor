Infering Alt-text For UI Icons With Large Language Models
During App Development
SABRINAHAQUE,
UniversityofTexasatArlington,USA
CHRISTOPHCSALLNER, UniversityofTexasatArlington,USA
Ensuringaccessibilityinmobileapplicationsremainsasignificantchallenge,particularlyforvisuallyimpaired
userswhorelyonscreenreaders.Userinterfaceiconsareessentialfornavigationandinteractionandoftenlack
meaningfulalt-text,creatingbarrierstoeffectiveuse.Traditionaldeeplearningapproachesforgeneratingalt-
textrequireextensivedatasetsandstrugglewiththediversityandimbalanceoficontypes.MorerecentVision
LanguageModels(VLMs)requirecompleteUIscreens,whichcanbeimpracticalduringtheiterativephasesof
appdevelopment.Toaddresstheseissues,weintroduceanovelmethodusingLargeLanguageModels(LLMs)
toautonomouslygenerateinformativealt-textformobileUIiconswithpartialUIdata.Byincorporatingicon
context,thatincludeclass,resourceID,bounds,OCR-detectedtext,andcontextualinformationfromparent
andsiblingnodes,wefine-tuneanoff-the-shelfLLMonasmalldatasetofapproximately1.4kicons,yielding
IconDesc.InanempiricalevaluationandauserstudyIconDescdemonstratessignificantimprovementsin
generatingrelevantalt-text.ThisabilitymakesIconDescaninvaluabletoolfordevelopers,aidingintherapid
iterationandenhancementofUIaccessibility.
CCSConcepts:•Softwareanditsengineering→Softwareusability;•Human-centeredcomputing→
Accessibilitysystemsandtools;Empiricalstudiesinaccessibility.
AdditionalKeyWordsandPhrases:Mobileappdevelopment,developersupport,visualimpairment,fine-tuning,
iconcontext,iconalt-text
ACMReferenceFormat:
SabrinaHaqueandChristophCsallner.2024.InferingAlt-textForUIIconsWithLargeLanguageModels
DuringAppDevelopment. 1,1(September2024),22pages.https://doi.org/XXXXXXX.XXXXXXX
1 INTRODUCTION
Manymobileappsassumethattheiruserswillinteractwiththeappbasedonthevisualappearance
oftheapp’ssmallinteractivegraphicalUIelements,i.e.,itsiconsandimagebuttons.Tosupport
alternative(non-visual)user-appinteractionstyles,themajormobileplatformsalsoalloweach
apptodescribethemeaningofitsiconsviatext(i.e.,accessibilitymetadatasuchasalt-text).But
manyappsdonotprovidemeaningfuldescriptionsforalloftheirrelevantGUIelements[3,64].
Recentworkhasmadegreatprogresstowardautomaticallyinferringsuchtextualdescriptionsvia
deeplearningandlargelanguagemodels.Butmanychallengesremain,includingsupportingthe
longtailofrareGUIelementsandsupportingengineersduringdevelopmentwherefullscreen
informationmaynotyetbeavailable.
Today billions of people rely on mobile apps for a wide variety of tasks. Governments have
enactedregulationssuchastheAmericanswithDisabilitiesAct(ADA)andSection508oftheU.S.
RehabilitationAct[2,70].Theseregulationsmandateaccessibilitystandards.TheScreenReader
Authors’addresses:SabrinaHaque,DepartmentofComputerScienceandEngineering,UniversityofTexasatArlington,
Arlington,Texas,USA,sxh3912@mavs.uta.edu;ChristophCsallner,DepartmentofComputerScienceandEngineering,
UniversityofTexasatArlington,Arlington,Texas,USA,csallner@uta.edu.
Permissiontomakedigitalorhardcopiesofpartorallofthisworkforpersonalorclassroomuseisgrantedwithoutfee
providedthatcopiesarenotmadeordistributedforprofitorcommercialadvantageandthatcopiesbearthisnoticeand
thefullcitationonthefirstpage.Copyrightsforthird-partycomponentsofthisworkmustbehonored.Forallotheruses,
contacttheowner/author(s).
©2024Copyrightheldbytheowner/author(s).
XXXX-XXXX/2024/9-ART
https://doi.org/XXXXXXX.XXXXXXX
,Vol.1,No.1,Article.Publicationdate:September2024.
4202
peS
62
]CH.sc[
1v06081.9042:viXra2 SabrinaHaqueandChristophCsallner
User Survey conducted by WebAIM in 2017 reported that 90.9% of blind or visually impaired
respondents relied on screen readers on smartphones [78]. Yet research highlights persistent
accessibilityissuesinmobileapps,creatingbarriersthatimpedetheirefficientusebypeoplewith
variousinjuriesanddisabilities[3,63,64,81].Centraltothesechallengesarethegraphicaluser
interface(GUI)elementslackingmeaningfulalt-texts.
Inferringthemeaningofanarbitraryiconremainsafundamentallyhardproblem.Wecanonly
makealimitednumberofobservationsyethavetosummarizeandgeneralizetheseobservations
toadescriptionthatcapturestheicon’smeaninginallpossibleappusagescenarios.Tomakethis
generalization,priorworkhasleveragedavarietyoftypesofobservation(includingscreenshots,
view hierarchy information, and app runtime behavior). We are not aware of work however
thatsupportsanengineerduringdevelopmentwithhigh-qualityalt-textswhenthefullscreen
informationisnotavailableyet.Thisadditionallackofinformationmakesthistaskevenharder.
Atahighlevel,recentworkfallsintooneofthefollowingtwobroadcategories.Inthefirst
categoryaretraditionaldeep-learningtechniquesthatarespecializedforinferringGUIelement
alt-texts[18,54].Whiletheypavedthewayforsubsequentwork,thesepioneeringapproachesare
trainedonrelativelysmalldatasets.Inthesecondcategoryaremodernmodelsincludinglarge
multi-modalmodelsandlargelanguagemodels[1,30,56].Whilethesemodelshavebeentrained
onmuchlargerdatasets,theyeitherarenotfine-tunedondomain-specificUItasks[40]ordonot
performwellonpartialscreens.
Toaddressthechallengespreviousapproachesface,thispaperintroducesanovelapplication
of off-the-shelf large language models to generate context-aware alt-texts—without requiring
completescreeninformation.Thisapproachisespeciallybeneficialduringtheearlyphasesofapp
development,wherescreendata(suchasscreenshots,UIelements,viewhierarchies,andUIcode)
areoftenincomplete.ThispaperimplementsthisapproachintheIconDesctoolsandcompares
IconDescwithstate-of-the-art(SOTA)approaches.
BesidesimprovingonSOTAapproachesintermsofcommonly-usedautomatedmetricsand
inauserstudy,IconDescalsoonlyrequiresasmalldataset(forfine-tuning),whichcompares
favourablytothe(atleastanorderofmagnitude)largerdatasetsthatareneededtotrainspecialized
deep-learningapproaches.
Tosummarize,thepapermakesthefollowingcontributions.
• Thepaperintroducesanovelpipelinetechniqueforgeneratingalt-textforUIiconsfrom
partial screen data. The pipeline infers an icon-only description via a large multi-modal
modelandfine-tunesalargelanguagemodeltogeneratealt-texts.
• ThepaperimplementsthetechniqueinIconDescandcomparesIconDescempiricallywith
specializeddeep-learningapproachesandmodernSOTAmodels,achievingtopscoresin
bothautomatedmetricsandauserstudy.
• Allcode,data,andconfigurationsweused(includingforfine-tuning)areavailable[33].
2 BACKGROUND
Mobileappsconsistofvariousscreensoractivitiesfilledwithuserinterface(UI)elementsdesigned
to facilitate user interaction. Among these elements, interactive components such as buttons,
checkboxes,clickableimages,andiconsprovideconcreteengagementpointsforuserstoinitiate
actionsandnavigateappfeatures.Icons’widespreaduseinmobileappscanbeattributedtotheir
abilitytoconveyinformationeffectivelywhileconsumingminimalscreenspace.
,Vol.1,No.1,Article.Publicationdate:September2024.InferingAlt-textForUIIconsWithLargeLanguageModelsDuringAppDevelopment 3
2.1 AccessibilityChallenges:InferringtheMeaningofSmallUIElements
Despitetheirutility,asignificantchallengewiththesesmallinteractiveUIelementsisthatthey
oftenlackameaningfulalt-textlabel[29,64].Meaningfulalt-textlabelsarecrucialforaccessibility
technologies such as screen readers [34, 38], so not having meaningful alt-texts impedes the
navigationandinteractionforusersrelyingonassistivetechnologies.Forexample,researchhas
highlightedthepervasiveissuesofmissingalt-textacrossAndroidapps,underscoringtheneedfor
automatedgenerationofdescriptivealt-texts[29,64].
Forinferringalt-text,asamplechallengeisthatagivenkindoficonmayrepresentdifferent
functionalitiesdependingonthecontext.AsanexamplefromtheRicodataset[25],Figure1shows
twoappscreens.Theleftscreenusesa“minus”iconforzoomingoutwhiletherightscreenusesa
similar“minus”iconfordecreasingthevolume.Tosupportsuchcaseswethusneedanalt-text
inferencetooltointerpretanicon’spixelsintheicon’sscreencontext.
2.2 LargeLanguageandVisionModels
Large language models (LLMs) have become
powerfultoolsinnaturallanguageprocessing,
demonstratingexceptionalperformanceona
varietyoftext-basedtasks.Thesetransformer-
based [71] models, equipped with billions of
parametersandtrainedonextensivedatasets,
cancomprehendandgeneratehumanlanguage
proficiently[24,56].
Beyond processing text, vision-language
models (VLMs) are trained on both text and
images, letting them address complex tasks
suchascaptioningandscene-textunderstand-
ing [23, 48, 50]. Recent more general models
supportmultiplemodalities(text,images,etc.)
and are sometimes also referred to as multi-
modalmodels[1,30].
Thekeyattributeofallofthesetransformer- Fig.1. Examplezoomout(left)vs.lowervolume(right)
basedmodelsistheamount(anddiversity)of minusbuttonsinRico.
datausedintheirtraining.Sowedistinguish
large and state-of-the-art models (the latter
nowrequirelargeamountsoftrainingdata)fromthebroadergroupofmodelsofallsizes.
In any case, a model is trained on diverse data sets, which equips a model to handle a wide
varietyoftasksacrossdomains.
Thereareseveraloptionsforadaptingsuchamodeltonewtasks.First,zero-shotlearningallows
themodeltoperformataskithasneverseenbeforepurelybasedontheinstructionsgivenin
theprompt,whichcanbedifficultforunseentasks.Specifically,promptinginvolvesusingnatural
languageinstructionswithinthemodelinput,toguidetheLLMtowardthedesiredoutput.These
promptscanrangefromsimplequestionformulationstoelaboratedemonstrationsandbackground
information.Buildingonprompting,in-contextlearning(akafew-shotlearning)incorporatesa
fewexamplesofthedesiredtaskwithintheprompt[13].ThisprovidestheLLMwithconcrete
demonstrationsofthedesiredbehavior,enhancingitstask-specificunderstanding.
Whilepromptingandin-contextlearningarepowerfulmethodsforadaptingmodelstonewtasks,
therearecaseswheretheseapproachesmightnotsuffice.Fine-tuningisatraditionalapproachto
,Vol.1,No.1,Article.Publicationdate:September2024.4 SabrinaHaqueandChristophCsallner
adaptingamodelforspecifictasksviaadditionaltrainingonasmalltask-specificdataset,thereby
updatingthemodel’smanyparameters.Fine-tuningcanbeverycostlyandisthussometimesnot
worththeeffort.PlatformssuchasOpenAIprovideAPIsforfine-tuningoncustomdatasets[58].
2.3 Alt-textGeneration:LabelDroid,Coala,WidgetCaptioning,etc.
LabelDroid[18]markedthefirstattemptbasedondeeplearningforautomatingalt-textgeneration
forUIicons.LabelDroidutilizesonlyiconimagestogeneratealt-textinthestyleofimagecaptioning.
Whilegroundbreaking,LabelDroiddidnottakeintoaccountcontextinformation.Thesubsequent
Coaladeep-learningmodel[54]addedbothvisualandcontextualinformationfromtheapp’sview
hierarchyduringtraining.Thisapproachresultedinimprovementsintherelevanceandaccuracy
ofgeneratedalt-texts.
Inasimilardeep-learningapproach,Lietal.introducedWidgetCaptioning[47],whichnot
onlyfocusesoniconsbutbroadensthescopetoincludeotherUIelements(buttons,checkboxes,
image views, etc.). By utilizing both visual and structural data of UI components, this method
efficientlygeneratesdescriptivecaptions.Thisbroaderapproachwassupportedbyadatasetof
human-annotatedwidgetcaptions,whichservesasafoundationforourresearch.
Insteadofapplyingtraditionaldeeplearning,recentworkadaptsVLMstothistask.Modelssuch
asPix2StructandPaliGemmahaveshownnotablesuccessinwidgetcaptioningtasks,inferring
descriptivetextforUIelementsbasedsolelyonvisualinput.Forexample,Pix2Structleveragesthe
richvisualandtextualdatafromwebpagesbyparsingmaskedscreenshotsintosimplifiedHTML,
whichfacilitatesthemodel’sabilitytointegratevisualandtextualinformationduringpretrain-
ing[44].Themodelistrainedon80millionscreenshots,enablingittogeneralizeacrossdiverse
domainsincludingthetaskofwidget-captioningofmobileUIs.Pix2Structachievessignificant
improvementsinthewidgetcaptioningtaskcomparedtosimilarapproaches[43,48].
Asanotherrecentwell-knownexample,thePaliseriesofvision-languagemodelsusesavision-
basedapproachtoanalyzethestructuralcomponentsofUIs[21,22].Thesemodelsareadeptat
(amongothers)visually-situatedtextunderstanding,whichiscrucialforcomprehensivewidget
captioningtasks.Toefficientlyhandlenoisyimage-textdata,theymakeuseofrobustpretraining
methods, such as contrastive pretraining using SigLIP. PaliGemma extends this capability by
integratingtheSigLIPvisionencoderwiththeGemmalanguagemodel,optimizingperformance
acrossdiversevisualandlinguistictasks[12].
Thoughthedeeplearningapproachesusepartialiconcontext,therecentvisuallanguagemodels
(such as Pix2Struct and PaliGemma) require the complete visual context of the UI (such as a
completescreenshot)foroptimalperformance.Requiringfullcontextinformationimpliescertain
limitations,especiallyduringtheiterativeanddynamicphasesofUIdevelopment.Intheseearly
stages,completeandfinalizedUIdesignsmaynotalwaysbeavailable,makingitchallengingto
utilizethesemodelseffectively.
3 GROUNDTRUTHANALYSIS
Wesplitasetofannotatediconsintotwo(non-overlapping)subsets.Weusethefirstsubsetto
fine-tunea(text-to-text)largelanguagemodelandthesecondoneasground-truthforcomparing
variousannotationapproaches.Tocloselysimulatehowathird-partyhumanuserwouldannotate
icons,weavoiddatasetsannotatedbytheoriginalappdevelopersorviaAImodels.Wethussurvey
existingdatasets,lookingforlargepublicUIelementdatasetsthataremobileappfocusedand
annotatedbythird-partyhumans.
MostlargeexistingdatasetsofappUIelementsareeitherproprietary[27,84]orareopenbut
lackthird-partyhumanannotations[14–16,20,46,62,83].Followingaregoodcandidateswedo
notuse.IconNetprovides3rd-partyannotationsbutlumpsalllong-tailicons(beyond38classes)
,Vol.1,No.1,Article.Publicationdate:September2024.InferingAlt-textForUIIconsWithLargeLanguageModelsDuringAppDevelopment 5
intoasingle“other”class[66].UICaptionautomaticallydistillsannotationsfromhow-tomanuals,
whichdoesnotalwaysyieldgoodresults[10].
Groundtruth Train Valid. Test Total
WC20widgets 52,178 4,559 4,548 61,285
WC20captions 138,342 12,275 12,242 162,859
Ouricons 18,176 1,503 1,635 21,314
Ourcaptions(all) 48,528 4,084 4,419 57,301
Ourcaptions(1atrandom) 18,176 1,503 1,635 21,314
LabelDroidcaptions 15,595 1,759 1,879 19,233
Table1. WC20(top)hasuptothree3rd-partyhumancaptionsperwidget,ofwhichwerandomlyselectone
pericon(middle).LabelDroidhasasimilarnumberofcaptions,whicharedeveloper-provided(bottom)
The closest we found is the Widget Captioning data set [47] (“WC20”). WC20 adds to the
foundationalRicodataset[25]12kmobileUIscreenscapturedviarandomlyusingmobileapps(for
atotalof25kdistinctscreensfrom7kapps).EachWC20screencontainsbothscreenshotanda
correspondingviewhierarchy(akaaJSONDOMtree).In22kscreensWC20annotatedatotalof
61kUIelementswithupto3annotationseachbydistincthumanusers,yielding163k3rd-party
human-producedcaptions.
Groundtruth Unique Top-3 ≤4occ. 1occ.
LabelDroid[17] 3,523 52% 14% 12%
Our(allcaptions)[47] 24,556 8% 41% 35%
Our(1atrandom)[47] 11,140 8% 50% 43%
Table2. Iconlabeldistribution:Uniquelabels,shareofthetop-3labels,andlabelsoccurring≤4times/once.
Of the 22k human-annotated screens 17k are avail-
able (via Rico), containing 26k icons (ImageButton or
ImageView).Weremoveabnormallylargeornarrowele-
ments(asinCoala[54]),yielding21,3143rd-partyhuman
annotatediconsin8,201screens.Foreachiconweran-
domlypickoneofWC20’sannotations,yielding21,314
annotations. The screen splits we have are consistent
withtheonesproposedbyWC20.Table1summarizes
thegroundtruthsplitsinWC20[47],ourwork,andLa-
belDroid[18].
WhiledevelopersoftendonotprovideUIelementan- Fig.2. DistributionoficoncaptionsinLa-
notations,wealsocomparewiththediversityofthean- belDroidandourWC20subset.LabelDroid
notationstheydoprovide.Awell-knownbaselinearethe ismorerepetitiveandlessdiverse.
developer-providedUIelementlabelscuratedbytheLa-
belDroidwork[18].Whileour“pick-one-WC20-caption-
per-icon-at-random”WC20subsetandLabelDroidhaveasimilarnumberoftotalcaptions(21kvs
19k),Table2andFigure2showthatoverallLabelDroidislessdiversethanourgroundtruth,with
3.5k(LabelDroid)vs.11.1k(ours)uniquecaptions.Thethreemostcommoncaptionsmakeup8%of
ourgroundtruth,whereas52%ofLabelDroidconsistsofitstop-3captions.
,Vol.1,No.1,Article.Publicationdate:September2024.6 SabrinaHaqueandChristophCsallner
ThediversityinthedatasetsisalsoapparentfromtheCumulativeDistributionFunction(CDF)
plotshowninFigure2.Here,thex-axisrepresentstherankofcaptionswhensortedbyfrequency,
withlowerrankscorrespondingtomorefrequentcaptionsandthey-axisrepresentsthecumulative
proportionofthetotalcaptionoccurrencesuptothatrank.TheCDFplotshowsthatLabelDroid
captionshaveahigherconcentrationoffrequentcaptions,withasmallnumberaccountingfora
largeportionofthedataset.Incontrast,both"Our(allcaptions)"and"Our(1atrandom)"datasets
exhibitgreaterdiversity,withmorecaptionsneededtocoverthesamecumulativeproportion,
indicatinglessbiasandamoreevenlydistributedsetofcaptions.
4 OURAPPROACH
Figure3givesanoverviewofourapproach.IconDesc’sinputsareaUIscreenshotandaview
hierarchy(akaDOMtree).Bothoftheseinputsmaybeunderdevelopmentandhencetheymaybe
incomplete,e.g.,thescreenshotmaymissingthefinalshapeofotherUIelementsandtheDOM
treemaymissseveralsub-trees.
ViatheDOMtreeIconDescfirstextractsthecurrenticoncontext(i.e.,theavailabletextual
propertiesoftheicon’ssurroundingUIelements)andtheicon’spixels.Whileallowingfine-tuning
withtextinputs,atthetimeofwritingnoneoftheOpenAIGPTmodelsnorGeminiallowvision
fine-tuning.Whenusingfine-tuning,thesemodelsthusonlyaccepttextinput.
IconDesc therefore first passes the icon’s
pixelstoaLargemulti-modalModel(LM3)(to
VLM Icon-only
maptheicon’svisualfeaturestoaicon-only
label
label). IconDesc similarly extracts from the
OCR
icon’spixelsanyin-icontextviaOCR.Icon- In-icon text
Desccombinesthesethreekindsoftextsintoa
DOM
LLMprompt,yieldinganalt-textfortheicon. Surrounding
reader
UI elements’
4.1 Icon-onlyLabelviaLM3 text prop.
IconDescfirstripsaniconoutofits(screen) LLM
Icon label Prompt
contextandinfersadescription(orlabel)for
theiconfromjusttheicon’spixels.Duringapp
Fig. 3. Overview: Via the DOM tree IconDesc ex-
development this could mean just using the tractstextpropertiesofanicon’ssurroundingelements
icon’s file. For our experiments, we cropped (yellow-dashedbox),mapstheicon’spixelstoanicon-
eachiconfromitsscreenshotviaourdataset’s only label, and extracts in-icon text. On this Strobe
DOMtreeboundingboxes. Lightappscreen(fromRico)baselinesinfer“selectthe
In this work we focus on (clickable) icons <UNK>”(Coala)and“powerbutton”(PaliGemma).Icon-
viacommonheuristics.First,wefocusonthe Descinfers“turnonthemusic”(WC20ground-truth
labels:“togglemusic”and“turnonthemusic”).
UI classes containing the substrings IMAGE-
BUTTONandIMAGEVIEW,astheyaremost
commonlyusedforclickableiconsinAndroid.
During our ground truth selection (and as in Coala [54]), we skip abnormally large or narrow
UIelements,sincetheyarelikelynotclickableicons.Forourdataset,wetherebyremove17%of
candidateiconsforabnormalshapes,yieldingtheTable1iconcounts.
Theseheuristicsmayremovesomeclickableicons(fromotherUIclassesorforoddshapes)and
failtoremoveclickableimagesthatusersmaynotconsidertobeaclickableicon.Wemanually
reviewedarandomsampleof100oftheoddlyshaped(large/narrow)4,340iconsweremoved,
findingthat40ofthemcouldreasonablyappeartobeaclickableicon.Thismeansweignoresome
7%oftotalcandidateicons(forbothtrainingandevaluation)tomakeourresultseasiertocompare
withCoala’sevaluation[54].
,Vol.1,No.1,Article.Publicationdate:September2024.InferingAlt-textForUIIconsWithLargeLanguageModelsDuringAppDevelopment 7
Wesimilarlymanuallyreviewedarandomsampleof100oftheresulting21,314Table1icons,
finding (within the context of their screen) 8/100 rather appear to be a clickable image than a
clickableicon.Wesimilarlykeeptheseimages(bothfortrainingandevaluation)toenableeasier
comparisonwithCoala’sresults.
Croppingfromscreenshotsyieldsiconsofvariousresolutions.Asalowresolutionmayhurt
propericonlabeling,weapplySuper-Resolution(i.e.,Real-ESRGAN[76]),yieldingafixed128x128
pixelresolution.InanearlyversionweinferredIconDesc’sicon-onlylabelswithdeeplearning,
i.e.,wetrainedanEfficientNetmodel[68]onabalanced100-classicondatasetwecuratedfrom
Flaticon [28], the Noun Project [69], and LabelDroid [18]. While this classifier achieved a 98%
trainingaccuracyanda94.4%testingaccuracyitwasnotcompetitivewiththelatestLargemulti-
modalmodels(LM3s),i.e.,Gemini.(Whereasourdeeplearningclassifierused99classes+1“other”
class,Geminiproducesmeaningfuldescriptionsevenformanyofthelong-tail(“other”)icons.)
ThecurrentIconDescversionthusappliesSuper-Resolutiontotheiconandthensendstheicon
totheGeminiLM3withthefollowingprompt:“Youareanimageclassifier.Whatistheclassofthis
UIicon?Onlyprovidetheclassasresponse.”InmostcasesGemini’sanswerconsistsofaone-word
ortwo-wordphrase,whichweuseastheicon-onlylabel.
4.2 In-iconTextviaOCR
Someiconscontaintextthatmayprovideinformationabouttheicon’s
purpose. Such text is however usually not directly available in the
DOM tree (e.g., via the icon’s “text” property). We thus heuristically
retrievesuchin-icontext,viaOpticalCharacterRecognition(OCR)on
the(cropped)iconpixels,specificallyusingthelatestversion(1.7.1)of
EasyOCR[31]. Fig. 4. In-icon text exam-
Figure 4 shows two examples icons containing text that would be plesfromRico.
helpfulforalt-textgeneration.FortheFigure4examplesIconDesc’s
OCRcorrectlyinferredthein-icontextstrings“Live”and“Quote”.
4.3 SurroundingUIElements’TextualPropertiesFromtheDOMTree
Toprovideanicon’scontextinaLLMquery,pre-
viousworks[75,79]encodedascreen’sentireview
{
hierarchy. While this captures a lot of context in- "app activity name": "com.ape.apps.
strobe.StrobeActivity",
formation,thisapproachhastwochallenges.First,
"UI element info": {
manymobilescreenshaveacomplexhierarchy,lead- "class_name": "AppCompatImageView",
"resource_id": "ivTechnoPower",
ingtolargequeries,whichmaybemoreexpensiveto
"bounds": [957, 878, 1202, 1123],
process(forbothfine-tuningandinference).Second, "OCR detected text": ['d)']
},
encodinganentireDOMtreemaynotbepossible,
"parent node": {
asduringsoftwaredevelopmentthefulltreemay "class": "LinearLayout"
},
notyetbeavailable.
"sibling nodes": [
Weconductedinitialexperimentswithzero-shot {
"resource_id": "tvTechno",
prompting,comparingfullDOM-treecontextwith
"text": "Music",
asmallcontextsubset(Table3).Sinceweobserved "class": "AppCompatTextView"
}
similarperformance,wepursueamorelimitedcon-
]
textandsupportpartialDOMtreesduringsoftware }
development. Listing1. IconcontextIconDescextractsfrom
Listing 1 shows an example icon context Icon- theDOMtree&OCRfortheFigure3example.
Desc extracts for the Figure 3 motivating exam-
plefromtheDOMtree.Thefirstcomponentisthe
,Vol.1,No.1,Article.Publicationdate:September2024.8 SabrinaHaqueandChristophCsallner
screen’sname(“StrobeActivity”),followedbypropertiesoftheicon(classname,resource-id,bound-
ingboxlocation,andon-screentext).TheFigure3exampleicondidnotcomewithaon-screen
textproperty(andourresultingDOMextractjustomitsanysuchmissingelements).Atthispoint
wealsoaddanyin-icontextIconDescinferredviaOCR(i.e.:“d)”).
Besidesscreennameandtheicon,weextractsimilarinformation(class,resource-id,andon-
screentext)abouttheicon’sparentnodeintheviewhierarchyaswellastheicon’ssiblingnodes
(sharingtheicon’sparentnode).OurintuitionisthatthedevelopergroupingtheseUIelements
togetherintothesamecontainerlikelycapturesthattheyareconceptuallyrelated,yieldingvaluable
contextinformation.
ThroughoutthisDOMextraction,ifapropertysuchasanon-screentextismissingorblankin
theDOMtreethenwedonotaddacorrespondingkey-valuepairtoourDOM-extract.IconDesc
currentlyalsodoesnotextractanyalt-textsthatmayalreadybeinthescreen’sDOMtreebutwe
couldeasilyaddthisinafutureversion(whichwouldlikelyimproveIconDesc’sinferredalt-texts).
4.4 PromptGenerationforAlt-textLabeling
IconDescwrapsitsicon-
onlylabelandiconcontext
"You are an accessibility assistant to a mobile app Developer.
in a LLM query template. A mobile app UI element that looks like an icon that developers
often use with icon tag '{icon-only label}' has view hierarchy
Wecraftedthistemplateto
content as below:
convey IconDesc’s goals {icon context}
Generate a short (within 2-7 words), descriptive alt-text for the
and briefly describe Icon-
UI element. Provide only the alt-text as output, nothing else.
Desc’sinferredtexttothe Describe the element as if you were the app developer to help
vision-impaired user understand its functionality and purpose.
LLMinnaturallanguage.
Avoid generic words like 'button', 'image', 'icon' etc."
Listing 2 shows the the
Listing2. PrompttemplateIconDescusesforitsLLMqueries.
template we used to eval-
uate IconDesc. IconDesc
replaces‘{icon-onlylabel}’
withthelabelitinferredfromtheiconpixelsviaaLM3(Section4.1)and{iconcontext}withthe
sub-treeIconDescinferredfromthescreen’sDOMtreeandOCR(Section4.3).
5 QUANTITATIVE&QUALITATIVEEVALUATION
Inourevaluationofalt-textgenerationforUIicons,weassessedtheperformanceofourapproach
usingoff-the-shelflargelanguagemodels(LLMs)andcomparedtheseresultsagainstbothtraditional
deep-learningmodels,vision-transformers,andadvancedvision-languagemodels(VLMs).Specifi-
cally,wecomparewithdeep-learningmodelssuchasCoala[54]andLabelDroid[18],andnewer
approachessuchasthevisiontransformerbasedPix2Struct[44]andthePaliGemmaVLM[12].
WhilePaliGemmaisaversatileVLMthatintegratesvisualandlinguisticdataforenhancedtext
generation,Pix2Structisamodelfocusedonparsingvisualdatafromscreenshotsandgenerating
correspondingtextualdescriptions.BothPaliGemmaandPix2Structarefine-tunedonfullUIscreen
imagesforthewidget-captioningtask,witheachmodeldemonstratingitsstrengthsinhandling
visualandcontextualinformationwithinuserinterfaces.
Foramoretargetedcomparison,weconsideredtwoinputconfigurationsforthemodernap-
proaches(vision-transformersandVLMs):full-screeninputsandmaskedinputs-whichcontain
onlytheiconanditsneighboringelements(Figure5.Thisapproachalignswithourgoaltoevaluate
performanceunderconditionswherethecompleteUIscreenmightnotbeavailable,suchasduring
theearlystagesofappdevelopment.
Weseektoanswerthefollowingresearchquestions(LM3referstoalargemulti-modalmodel).
,Vol.1,No.1,Article.Publicationdate:September2024.InferingAlt-textForUIIconsWithLargeLanguageModelsDuringAppDevelopment 9
RQ0 Foriconalt-textgeneration,howdotraditionaldeep-learningapproachescomparewith
largemodernout-of-the-boxmulti-modalmodelsandIconDesc(i.e.,withoutfine-tuning)?
RQ1 Foriconalt-textgeneration,canavision-then-textLM3-LLMpipelineapproachtheresults
oftheLM3?
RQ2 Foriconalt-textgenerationwithaLM3-LLMpipeline,canalocaliconcontextyieldsimilar
resultsasfullscreencontext?
RQ3 Foriconalt-textgeneration,howdomodernfine-tunedvision-transformersandVLMs
comparewiththeIconDescLM3-LLMpipeline?
RQ4 Foriconalt-textgeneration,howwelldomodernfine-tunedmodelssupportpartialinput
screens?
RQ5 Howdousersratetheiconalt-textsofthemodelsscoringhighestonautomatedmetrics?
5.1 ExperimentalSetup
WeempiricallycompareIconDescwithrepre-
sentative tools from the most-closely related
approaches,i.e.,Coala&LabelDroidfortradi-
tionaldeep-learningapproaches,Pix2Structfor
therecentvision-transformers,andPaliGemma
forVLMs.Tomakethiscomparisonasfairas
possible, we evaluated all tools on the same
1,635WC20-derivedtesticons(Table1row“our
captions(1atrandom)”).Wesimilarlytrained
andfine-tunedalltoolsonthesamedataset,i.e.,
WC20[47].
Specifically,wedownloadedtheLabelDroid
code1andtraineditontheLabelDroidtraining
dataweobtainedfromZenodo.Thisallowedus
toreproducetheresultsreportedintheLabel-
Droidpaper.WethenfurthertrainedthisLabel-
Droidmodelonouricondataset,i.e.,usingthe Fig.5. InputformatsforPix2StructandPaliGemma
icon-labelpairsintheexactsplitdescribedin
theWidgetCaptioningpaper(theTable1row
labeled“ourcaptions(1atrandom)”).
Similarly,weacquiredtheCoalacode2andreplicatedtheirresultsusingthedefaultconfiguration
ontheirUI30KdatasetThereplicationresultsforbothDLapproachescanbefoundhere[33].Similar
toourwork,Coalautilizesboththeiconimageanditsusagecontextfromtheviewhierarchy.Since
theviewhierarchyinformationinourdata(theTable1rowlabeled“ourcaptions(1atrandom)”)is
inJSONformatandCoala’scoderequiredtheminXML,wefirstconvertedourdatatoXML.We
alsorearrangedthemintheformatrequiredbyCoala-i.e.intofoldersbasedontheapppackage.
Welatertrainedtheircontext-awaremodelwithoutattention-asreportedintheirpapertoachieve
thebestresultonthisdatasetandobtainedthescoresinTable4.BoththeDLmodelsreporthigher
imagecaptioningscoresontheirdatasetthanours-possiblybecauseofthegenericnatureofthe
iconlabelsintheirdatasettothemoreinformativeanddiversedatasetweobtainedfromWC20[47].
Theseweretrainedona16GRAMintelcorei7-11700FmachinewithNVIDIAGeForceRTX3060
TiGPU.
1https://github.com/chenjshnn/LabelDroid,accessedSept.2024
2https://github.com/fmehralian/COALA,accessedSept.2024
,Vol.1,No.1,Article.Publicationdate:September2024.10 SabrinaHaqueandChristophCsallner
Forthevision-transformerandVLMapproaches,weutilizedmodelsfromHuggingFacethatwere
alreadyfine-tunedontheWC20dataset,specificallyPix2Struct3 andPaliGemma4.Bothmodels
weretrainedonthefullWC20dataset,whichincludesvariousUIelements,notjusticons.Asa
result,theresultsreportedinthispapermaydifferfromthoseintheoriginalpapersforthese
models.
Tofine-tunethelargelanguagemodel(LLM)forourtool,IconDescondiverseicons,wetookthe
99commoniconclassesLiuetal.[51]identifiedinRico,addedaclass“other”,andsampledupto15
iconsperclassfromour18,176trainingicons,yielding1,425icons.Forfine-tuningweprovidedthe
LLMwithWC20’shuman-inferredlabelandIconDesc-inferredicon-onlylabelandiconcontext.
Forzero-shotpromptingandLLMfine-tuningweusedtheOpenAI[57]andGemini[35]APIs,
stayingwithinatotalbudgetofUSD100.
5.2 Metrics:DistancetoaSetofHuman-writtenGround-truthIconAlt-texts
Foraninitialautomatedevaluationweusethe1,635ground-truthtestpairs(iconwithinascreen
plusuptothreehuman-writteniconalt-texts)fromTable1.Followingcommonpracticeweuse
theMSCOCO[32]implementationtocomputethefollowingwidely-usedtextdistancemetricsto
capturehowwellthetool-generatediconalt-textmatchestheup-tothreehuman-writtenreference
texts.Overall,higherscoresacrossthesemetricsarebetter,signifyingaclosermatchbetween
generatedtextsandtheground-truthdata.Whilewidelyusedinthecurrentresearchliterature,
eachofthesemetricshaslimitationsandthusdoesnotfullyencapsulatehumanjudgment.
BLEU[60]computesaprecisionmeasureonhowcloseatool-generatedcandidatesentenceisto
theupto3human-writtensentences.Tocomputeprecision,atitscoreBLEU[60]countsexact
matchesofwordsequences(orn-grams).Asiconlabelsareoftenshort,wefocusonBLEU-1and
BLEU-2(whichmatches1-gramsand2-grams).Amongthelimitations,BLEUpenalizesspelling
differences,synonyms,andparaphrases.
ROUGE[49]countswordsequencematchessimilartoBLEUbutfocusesonrecall,assessinghow
muchinformationinthereferencetextsthecandidatetextcaptures.Whileatitscoreitgeneralizes
fromBLEU’scontinuous-sequencematchestolongestcommonsubsequences(e.g.,3for(ABC)
and(AXBYZ),ROUGEstillpenalizesspellingdifferences,synonyms,andparaphrases.
METEOR[11]combinesfeaturesfrombothBLEUandROUGE(includingprecisionandrecall).
Whilegeneralizingfromtheseearliermetrics’exactword-levelmatchingtoalsoincludestemming
andsynonymsbasedonWordNet[55],METEORstillpenalizesphrasesthataresemanticallyrelated
but not synonyms. METEOR also generalizes ROUGE’s subsequence matching to out-of-order
alignments,whichmaynotfullycapturethenuancesofwordordercriticaltohumanperception.
CIDEr[72]goesbeyondn-gramprecisionbyincorporatingTF-IDFweighting[61],assigning
higherscorestoinformativen-gramslessfrequentinthereferenceset.Itconsidersoverlapwith
multiplereferencecaptions,reflectingthevariabilityinhumandescriptionswhilestillcapturing
theessenceoftheimage.Thismethodprioritizesrarebutinformativephrasesthataresignificant
inthegivencontext,makingithighlyeffectivefortaskslikeourswherethedistinctivenessof
eachdescriptioniscrucial.However,CIDErsometimescanresultinunbalancedTF-IDFweighting,
causingunimportantdetailstobeweightedmore,resultinginineffectivecaptionevaluation[42].
We also integrate SPICE [4] to address semantic accuracy by examining the agreement of
propositionswithinthetexts,providingadeeperassessmentthatmorecloselymirrorshuman
evaluationsofrelevanceandaccuracy.ItcomplementsCIDErbyensuringthegenerateddescriptions
arenotonlyinformativebutalsosemanticallyalignedwithhumanjudgments.SPICEbuildson
3https://huggingface.co/google/pix2struct-widget-captioning-large,accessedSept.2024
4https://huggingface.co/google/paligemma-3b-ft-widgetcap-448,accessedSept.2024
,Vol.1,No.1,Article.Publicationdate:September2024.InferingAlt-textForUIIconsWithLargeLanguageModelsDuringAppDevelopment 11
scenegraphsthatparsesemantictokensfromcandidateandreferencesentences,andthusincorrect
parsingresultsmayleadtoinaccurateevaluation[42].Besides,thoughscenegraphshaveproven
highlyeffectiveforcompleximageretrieval[41],alt-texttoUIiconsareoftenmuchsimplerand
theirfunctionalitymaynotalwaysmapwelltodetailedscenegraphs.
Whileeachmetricoffersvaluableperspectives,CIDErandSPICEaremostsuitedforourtaskof
evaluatingalt-textssincetheyaligntheclosesttohumanjudgement[4].
5.3 GaugingMulti-modalModel&LM3-LLMCapabilitiesviaZero-shotPrompting
DLvs.0-shotlargemodels BLEU-1 BLEU-2 ROUGE METEOR CIDEr SPICE
Coala(noattention) 35.3 24.1 34.0 14.2 66.3 9.4
LabelDroid 28.4 17.7 31.7 12.5 59.4 10.6
Gemini1.5Flash(vision) part 44.8 28.0 46.6 22.8 94.4 19.9
IconDesc(GPT4o) part 37.8 19.4 40.3 19.3 70.1 15.7
IconDesc(GPT3.5) part 30.0 15.0 34.7 18.2 56.3 14.5
IconDesc(GPT3.5) full 26.7 13.2 32.6 18.5 51.7 14.3
Table3. Traditionaldeep-learningapproaches(top)vszero-shotuse(withoutfine-tuning):multi-modalmodel
(middle)&LM3-LLMpipelines(bottom);part=partialiconcontext;full=fullhierarchy;bold=bestscore;
underlined=runner-up.Withzero-shotprompting,Geminiscoreshighest,followedbyIconDesc(GPT4o).
ExceptonMETEOR,IconDesc(GPT3.5)withpartialiconcontextperformsbetterthanwithfullhierarchy.
5.3.1 RQ0:LargeModernModelsYieldBetterAlt-textsEvenWithZero-shotPrompting. Recent
advances have produced very large multi-modal models such as recent OpenAI GPT models,
Gemini[7],andGemini1.5[30],whichhavebeentrainedon(amongothers)textandimages.While
theserecentmulti-modalmodelsallowfine-tuningwithtextinputs,atthetimeofwritingnoneof
theseGPTnorGeminimodelsallowvisionfine-tuning.Giventheirextensivetraining,itisstillan
interestingbaselinetocomparetheirperformanceout-of-the-box(i.e.,withoutanyfine-tuning)
versustraditionaldeep-learningapproachesthatarespecializedonthistask(i.e.,LabelDroidand
Coala).
Forthisexperiment,wesubmittedpromptstoGemini1.5Flash(vision)thatmimictheprompts
IconDescgenerates(Listing2).Specifically,theGeminipromptsincludetheiconcontextIconDesc
extractsfromtheDOMtree(includingtheOCR-inferredin-icontext)butreplacestheLM3-inferred
icon-onlylabelwiththepixelsofthecroppedicon.TheresultsinTable3showhowmuchbetter
thiszero-shotuseofGeminiperformedvs.thetraditionalspecializeddeep-learningapproaches
LabelDroidandCoala,especiallyonthemostrelevantmetricsCIDErandSPICE.
Atahighlevel,weconcludethatforourtaskourcustomuseofamulti-modalmodelislikelyto
outperformaspecializeddeep-learningapproach(asobservedinTable3).Wefurtherconclude
thatthisoutperformanceislikelytobecomeevenlargeroncefine-tuningbecomesavailablefor
largemulti-modalmodels(asfine-tuninglargemodelshasimprovedperformanceinthepast).
5.3.2 RQ1:ALM3-LLMPipelineCanProduceSimilarResultstoaMulti-ModalModel. Wedonot
knowiforwhenmulti-modalfine-tuningbecomesavailableandhowexpensiveitmaybe.Inthe
meantimewethusworkwithlargemodelsthatallowfine-tuning,i.e.,basedoninputtext.Based
onourTable3experiment,weconcludethatapromisingapproachtodescribeaniconimageisto
usealargemulti-modalmodel(vs.trainingadeep-learningmodel).Thisalignswiththeintuition
,Vol.1,No.1,Article.Publicationdate:September2024.12 SabrinaHaqueandChristophCsallner
thatalargemulti-modalmodelhasbeentrainedonamuchlargersetofimages,yieldingbetter
imageunderstandingandmorenuancedimagedescriptions.
Asabaselinewethuscomparealargezero-shotmulti-modalmodel(LM3)withazero-shotLLM
thatinapre-processing(phase1)stepdescribestheiconimagewiththesamepowerfulzero-shot
LM3.Suchapipelinedsetupthenallowsustofine-tunethe(phase2)LLMcomponent.
ForthisexperimentweuseddifferentversionsofIconDescinazero-shotconfiguration(without
anyfine-tuning).First,weexperimentedwithbothGPT3.5andthemorerecentGPT4o.Table3
showsthat,asexpected,justswitchingIconDescfromGPT3.5toGPT4oimprovesresults(across
allmetrics).ItisnotablethattheGPT4oversiongetsrelativelyclosetothemulti-modalGemini
results,despitetheGPT4oversiononlyoperatingontextinput.
5.3.3 RQ2:LocalIconContextCanYieldSimilarResultsasFullScreenContext. Finally,weare
curiousifprovidingalargeramountofcontextimprovesIconDesc’sresults.Wethuscompare
IconDescwithaversionthatreplacesIconDesc’sDOM-extractediconcontextwiththescreen’sfull
DOMtree(incondensedHTMLformat).Table3resultsshowthat(exceptforaminorimprovement
intheMETEORscore)overallthisadditionalcontextdidnotimproveresults,maybeburyingthe
mostrelevantcontextinformationinalargeamountofnoise.
5.4 ComparingwithStateoftheArtApproachesIncludingFine-tuning
Model BLEU-1 BLEU-2 ROUGE METEOR CIDEr SPICE
G-1.5(0-shot,partialcontext) 44.8 28.0 46.6 22.8 94.4 19.9
IconDesc(0-shot,partialcontext) 30.0 15.0 34.7 18.2 56.3 14.5
Pix2Struct(large)(fullscreen) 41.9 28.1 41.7 18.4 89.5 14.8
Pix2Struct(large)(maskedscreen) 35.6 23.9 35.1 15.7 73.7 10.7
PaliGemma(fullscreen) 56.5 42.1 56.5 25.1 127.9 21.0
PaliGemma(maskedscreen) 52.4 38.6 55.0 23.6 121.5 19.6
IconDesc(partialcontext) 55.2 39.0 54.2 26.1 129.6 21.9
Table4. Zero-shotlargemodels(top,fromTable3)vs.vision-transformer,VLM,andIconDesc(allfine-
tuned,bottom).IconDescscoreshighest(bold)inthemost-relevantmetricsCIDEr&SPICE.Therunner-up
(underlined)needsfull-screeninputs.
WhileTable3hasshownbetterresultsonGPT4o,forIconDescwepickedGPT3.5toensurethat
fine-tuningremainswithinourfinancialbudget.Specifically,forIconDescwefine-tunedOpenAI’s
gpt-3.5-turbo-0125model[58]for3epochsonour1,425icons(sampledfromour18,176training
icons).Onthepositiveside,itshouldbestraight-forwardtoimproveIconDesc’sresultsbyreplacing
IconDesc’sfine-tunedGPT3.5withGPT4ofine-tunedonour1,425icons.
5.4.1 RQ3:IconDescOutperformsSOTAModelsinCIDEr&SPICE. WecompareIconDescwith
thestate-of-the-art(SOTA)approachesPix2StructandPaliGemma.TheseSOTAapproachesare
fine-tuned on the entire WC20 dataset. (WC20 is a superset of our training set, which in turn
is a superset of the 1,425 icons IconDesc uses for fine-tuning.) We follow their experimental
setup[12,44],byprovidingPix2StructandPaliGemmaasinputascreenshotwherethetargeticon
ismarkedwitharedboundingbox.Tocapturehowwelltheyperformonpartialscreensthatmay
occurduringsoftwaredevelopment,werantheseSOTAapproachesbothontheentirescreenshot
andonamaskedscreenthatblacksouteverythingexceptfortheiconanditssiblingUIelements.
,Vol.1,No.1,Article.Publicationdate:September2024.InferingAlt-textForUIIconsWithLargeLanguageModelsDuringAppDevelopment 13
Table4firstshowsthatthe(fine-tuned)IconDescoutperformsourTable3zero-shotexperiments
(whichinturnoutperformedtraditionaldeep-learningapproaches).Asexpected,Table4shows
fine-tuninghasabigimpactonIconDesc(e.g.,increasingCIDErscoresfrom56.3to129.6).Table4
furthershowsthatIconDescalsooutperformsSOTAmodelsinthemost-relevantmetrics(CIDEr
andSPICE)inallconfigurations.
5.4.2 RQ4:IconDesc’sImprovementonSOTAisLargerforPartialScreens. Thisoutperformance
becomeslargerwhencomparingwiththeSOTAonthepartialscreensthatmaybefoundinour
targettaskduringsoftwaredevelopment.Followingareconcreteexamplestoillustratedifferences
betweenIconDescandthenext-highestscoringapproaches,i.e.,PaliGemmafullandPaliGemma
masked.
(a) “search for numbers”, “search (b)“cancel”,“clearoutphonenum- (c) “submit name button”, “the
phonenumber”(ref);“search”(P ber”,“deleteinput”(ref);“close”(P namehasbeenapproved”(ref);“se-
f f
&P );“searchfornumber”(ours). &P );“clearsearchbox”(ours). lectname”(P);“openmenu”(P );
m m f m
“confirmname”(ours).
Fig. 6. Sample screens with icons marked via red box and alt-texts inferred by humans and highest-
scoringmodels;greenbox=iconcontext(greenmarkupnotpartoftoolimageinput);ref=groundtruth;
P =PaliGemmafull;P =PaliGemmamasked;ours=IconDesc.
f m
5.4.3 ExplainingIconsWithASibling. TheiconinFigure6acombinestheloopsymbolcommonly
usedforsearchwithaglobesymbol.BothPaliGemmaversionsseemtoignoretheiconcontextand
produce“search”.WithinIconDesc,Geminisimilarlyinfers“GlobalSearch”.WhiletheIconDesc-
inferredDOMcontextisnoisywithseveralgenericidentifiers,italsocontainsthesiblingsearch
boxstring“Searchover1billionnumbers”,whichallowsIconDesctoinfer“searchfornumber”.
5.4.4 ExplainingCommonIconShapesWithLocalDOMContext. TheFigure6bexampleisinterest-
ingbecausethetarget[x]iconlookslikeacommon“close”button.Thepurely-visualPaliGemma
struggledtocombinethisinformationwiththeicon’scontext,justinferring“close”(bothonthe
fullandmaskedscreen).WithinIconDesc,Geminisimilarlyinfersicon-onlylabel“CloseButton”.
,Vol.1,No.1,Article.Publicationdate:September2024.14 SabrinaHaqueandChristophCsallner
Ontheotherhand,thehumangroundtruth(“cancel”,“clearoutphonenumber”,“deleteinput”)
alignswiththeicon’sDOMcontext,whichcontains(amongothers)theterm“search”fivetimes.
One of these is in the icon’s resource id (search_card__clearTextButton), which also contains
“clear”.Thecontextfurthercontains“edit”fourtimesand“text”fivetimes.Takentogetherthese
makeiteasierforIconDesctoinfertheiconlabel“clearsearchbox”.
5.4.5 ExplainingIconsWithLimitedContextInformation. TheFigure6ciconlookslikeacheck-
markandIconDesc’sGeminiinfers“Checkmark”.OnthefullscreenPaliGemmainfersareasonable
“selectname”.Butwhenonlyrunontheiconsiblings(masked)PaliGemmainfers“openmenu”,
likelybecausemanydrop-downmenususeasimilarcheck-markstyle.
While IconDesc’s DOM-tree extract only contains a few context elements, it contains the
resourceidsofboththeicon(nickname_check)anditssibling(nickname_input).Togetherthiswas
enoughforIconDesctoinfer“confirmname”,whichissimilartothegroundtruth(“submitname
button”,“thenamehasbeenapproved”).
5.5 ForPartialScreensUsersRateIconDesc’sIconAlt-textsHighest(RQ5)
Tocomplementourautomatedmetric-basedevaluation,wealsoconductedasmalluserstudy.The
goalofthestudyistosimulatehowusefulanappuserwouldrateagiveniconannotation.As
smartphonesareubiquitouswesampledparticipantsfromourlocalstudentpopulation.All10
participantswereadults(7male,3female).Participationwasvoluntaryandunpaid.
Forthisstudywerandomlyselectfromourtestset50screen/iconpairs.Foreachscreenwe
markthetargeticonviaaboundingboxandpresentarandomizedlistoffouralt-texts(PaliGemma
fullandmasked,ours,andarandomly-selectedgroundtruth),allinaGoogleform.Participants
wereinstructedtoassesseachalt-textforitsaccuracyindescribingtheicon’sfunctionalityand
purposewithinitsUIcontext,byratingthemonascaleof1(worst)to5(best).Thispromptaims
todetermineifanalt-textsissufficientforuserswithvisualimpairments.
Eachresponsewascomplete,rating4annotationsperscreen,yieldingatotalof2kannotation
ratings.Onaverage,participantsratedthealt-textofthegroundtruth3.6,full-screenPaliGemma3.8,
maskedPaliGemma3.4,andIconDesc3.8.Theseresultsareconsistentwithourautomatedmetric
studyinthat(1)full-screenPaliGemmaisratedsimilarlyasIconDescand(2)bothapproachesrate
higherthanmaskedPaliGemma.
Ratingvariability,expressedviastandarddeviation(SD),washighestformaskedPaliGemma
andgroundtruth(SD=1.5),indicatingdiverseperceptionsamongtheparticipants.Conversely,
IconDescdemonstratedthelowestvariability(SD=1.3),suggestingamoreconsistentreception
amongtheevaluators.
Comparison p-value RejectNullHyp.
IconDescvsPaliGemma-full 0.6700 No
IconDescvsPaliGemma-masked 3.54×10−8 Yes
Table5. HypothesistestingresultsusingtheWilcoxonsigned-ranktest.
GiventhefailureoftheShapiro-Wilktestfornormality[65],weoptedfortheWilcoxonsigned-
ranktest[80],anon-parametricmethodsuitableforoursamples.Weusedthistesttostatistically
assessthedifferencesbetweenthequalityofalt-textgeneratedbyIconDescandtheothermethods,
with a 0.05 significance threshold. The Table 5 results show no significant difference between
,Vol.1,No.1,Article.Publicationdate:September2024.InferingAlt-textForUIIconsWithLargeLanguageModelsDuringAppDevelopment 15
IconDescandfull-screenPaliGemma.Ontheotherhand,thereisastatisticallysignificantdiffer-
encebetweenIconDescandmaskedPaliGemma,indicatingIconDesc’ssuperiorperformancein
generatingcontextuallyappropriatealt-textsforpartialscreens.
5.6 CausesofError
(a) “dislike comment”,(b)“dollars”,“enterfunds (c)“escucharmusica”,“se-(d)refseebelow;“decrease
“vote against this com-orpaymentoption”,“show lecthuman”,“specificim-thequantityoftheitem”
ment”, “vote down” (ref); total balance” (ref); “buy ageselection”(ref);“error (ours).
“downloadoption”(ours). hint”(ours). icon”(ours).
Fig.7. ExampleswhereIconDescinfersasub-optimaliconalt-text;ref=upto3WC20ground-truths.7d’s
groundtruthis:“deleteitem”,“thisbuttonishereifyouwantmorethanoneitemselectquantity”.
TounderstandmoredeeplywhereIconDescfailstogenerateappropriateiconalt-texts,we
randomlychoseandmanuallyreview50exampleswheretheIconDesc-generatedalt-textdidnot
matchreferenceannotations.Followingarethefourmajorgroupsweidentified.
5.6.1 SmallIconDifferencesCanTriggerMisleadingIcon-onlyLabels. Insomecasesaniconlooks
verysimilartoothercommonicons,whichmisleadsIconDesc’sicon-onlylabelgeneration.For
example,theFigure7adown-voteicon( )looksverysimilartoacommondownloadicon( ).
Gemini’sresultingicon-onlylabelinthiscaseconfusedIconDesc’scontext-basedinferenceenough
thatitcouldnotrecovertoamoremeaningfulalt-text.
5.6.2 DOM-derivedIconContextMayContainMisleadingTerms. InsomecasestheDOM-basedicon
contextisverynoisy.Forexample,developersmayusenamingstrategiesinvolvingmanygeneric
ormisleadingterms.IntheFigure7bexample,fourUIelements’resource-idscontaintheterm
“hint”(imgHintBuy,imgHint50,imgHintAndroid,andimgHintFreeze),whichtriggeredIconDescto
alsoinclude“hint”initsalt-text.
5.6.3 Non-icons. InsomecasesouridentificationheuristicsclassifiedotherUIelementsasicons
(astheyareanormallyshapedImageVieworImageButton).Forexample,theFigure7cscreen
showsagridofclickablethumbnailimages.Sinceinthisworkwefocusonicons,IconDescoften
producesanunhelpfulalt-textinsuchcases.
,Vol.1,No.1,Article.Publicationdate:September2024.16 SabrinaHaqueandChristophCsallner
5.6.4 DubiousGroundTruth. Somegroundtruthsseemdubious.Forexample,thesecondFigure7d
groundtruth(“thisbuttonishereifyouwantmorethanoneitemselectquantity”)shouldmaybe
besplit,separating“selectquantity”fromtherest.Evenaftersuchahypotheticalsplittheresulting
alt-textsappearimpreciseorwrong.Inanycase,forthisscreentheIconDesc-inferredalt-text
appearsmoreappropriatethanthegroundtruth(whilereceivinglowscoresonautomatedmetrics).
6 RELATEDWORK
6.1 NecessityandStateofUIAccessibilityinMobileApplications
Accessibilityisdefinedas“[t]hequalityofbeingeasilyreached,entered,orusedbypeoplewithdis-
ability”[39].Accessibilityinmobileapplicationsisfundamentaltoensuringthatthesetechnologies
areinclusiveandcanbeeffectivelyusedbyindividualswithdisabilities,includingthosewithvisual,
auditory,motor,orcognitiveimpairments.Forinstance,visuallyimpairedindividualsmayrelyon
screenreaders[34,38]tonavigatemobileinterfacesaudibly,whilethosewithmotorimpairments
mightrequirealternativeinputmethodssuchasvoicecommandsorgestures.Despitesignificant
advancementsintechnology,mobileapplicationsoftenfallshortofbeingfullyaccessible.
Researchhasconsistentlyhighlightedarangeofaccessibilitybarriersthatpreventtheseapplica-
tionsfrombeingfullyaccessible[3,63,73].Anepidemiology-inspiredframeworkbyRossetal.
identifiedmultipledeterminantsthatrenderappsinaccessible,suggestingthatevenwell-designed
appscouldinadvertentlyincludebarriersthatlimituserengagement[63].Alshaybanetal.founda
widerangeofaccessibilityissuesinAndroidappsacross33differentcategories,includingmissing
textlabels,touchtargetsize,andlowcontrast[3].Thesestudieshighlightthepersistentchallenges
inmobileappaccessibilityandtheneedfordeveloperstobemoreawareofaccessibilityguidelines
andbestpractices.Studieshavebeenconductedtounderstanddevelopers’perspectivesontheissue
andhowtheydealwithaccessibilityinpractice[26].Theirsurveyreflectedthatwhilethedevelop-
ersconsidertheaccessibilityguidelineseasytoimplement,theyarestillreluctanttoimplement
them.Thisreluctancesuggeststhattheproblemisconnectedtoeitheralimitedunderstandingof
thesignificanceoftheseguidelinesforuserswithdisabilitiesoralackofwillingnesstoimplement
them.
Several international standards and guidelines, such as those from the W3C’s Web Content
AccessibilityGuidelines(WCAG),provideaframeworkforcreatingmoreaccessibledigitalcon-
tent[74,77].Variouscompanieshavealsodevelopedtheirdeveloperguidelinesbasedonestablished
standards,includingtheAndroidAccessibilityDeveloperGuidelines[5],Apple’sAccessibilityDe-
veloperGuidelines[8],andtheIBMaccessibilitychecklist[37].Thesestandardsofferarangeof
recommendationsaimedatimprovingsupportforindividualswithdiversedisabilities.Developers
anddesignersareencouragedtoadheretotheseguidelinestoenhancetheaccessibilityoftheirap-
plications.ToolslikeLint[6]forstaticanalysiswithindevelopmentenvironmentsandAccessibility
Scanner[36]fordynamicanalysisondevicesofferbroaderdetectioncapabilitiesforaccessibility
issues.Thesetools,alongsidedevelopertrainingandawareness,playavitalroleinbridgingthe
gapbetweencurrentpracticesandtheidealstateoffullyaccessiblemobileapplications.
6.2 LLMsandVLMsinUIUnderstanding
TheintegrationofLargeLanguageModels(LLMs)intoUItaskshastransformedhowusersinteract
withmobileandwebapplications,withconversationalinterfacesbecomingincreasinglycommon.
Wangetal.demonstratedthepotentialofLLMsbytransformingthemobileUI’sviewhierarchyinto
structuredHTML,enablingthemodelstogeneratedetailedscreensummariesandrespondtouser
queriesabouton-screenelements,therebysimplifyingnavigationforallusers,includingthosewith
disabilities[75].ThismarkedafoundationalstepinusingLLMstoenhancemobileappusability
,Vol.1,No.1,Article.Publicationdate:September2024.InferingAlt-textForUIIconsWithLargeLanguageModelsDuringAppDevelopment 17
andaccessibility.Buildingonthis,AutoDroidbyWenetal.showcasedtheadvancedcapabilitiesof
LLMsinmobiletaskautomationbyemployingthecommonsenseknowledgeinherentinLLMs
alongwithdomain-specificinsightsfromautomatedappanalysis[79].
LLMshavealsoextendedtheirutilitytoUIandaccessibilitytesting,acriticalareaforensuring
applicationsareaccessibletouserswithdisabilities.AXNav,introducedbyTaebetal.,combines
LLMswithpixel-basedUIanalysistoautomateaccessibilitytesting.Thissystemenhancesthe
efficiencyofmanualtestingmethodsbyexecutingtestsbasedonnaturallanguageinstructions,
therebybroadeningthetestingscopeandscalability[67].Othmanetal.haveexploredhowLLMs,
particularlyChatGPT,canaugmenttraditionalwebaccessibilityevaluations,ensuringthorough
andaccuratetestingprocedures[59].Similarly,GPTDroidbyLiuetal.utilizesGPT-3togenerate
dynamictestingscriptsthatsignificantlyimprovebugdetectionandactivitycoverage.Bytreating
GUItestingasaquestion-and-answertask,GPTDroidoffersanovelapproachtoidentifyingand
resolving UI issues [53]. Additionally, Liu et al.’s QTypist leverages LLMs for automated text
inputgenerationinmobileGUItesting,increasingtestingaccuracyandcoverageacrossdiverse
applicationsbyintegratingwithexistingGUItestingtools[52].
Moreover,therecentadvancementsinVision-LanguageModels(VLMs)introduceanewdimen-
siontoUIunderstandingbycombiningvisualandtextualinformation.ToaddresstraditionalVLMs’
underperformanceonUI-specifictasksduetoalackoftrainingdatatailoredtotheUIdomain,
Jiangetal.generatedalargedatasetofUIscreenshotspairedwithtextdescriptionsandquestions
usingacombinationofUIelementdetectionandaLargeLanguageModel(LLM).ILuvUI,created
byfine-tuningLLAVA[50]withthisdatasetshowcasesenhancedcapabilitiesinunderstandingand
interactingwithUIsthroughnaturallanguage.Similarly,AMEXdatasetbyChaietal.focuseson
mobileGUI-controlagentsandemploysGPT-4otogeneratedetaileddescriptionsofGUIscreens
andelementfunctionality[15].BothILuvUIandAMEXemployLLMsinamannersimilartoour
taskofalt-textgeneration,emphasizingthepotentialofthesemodelstoenrichthedatasetcreation
processandultimatelyenhancetheaccessibilityandusabilityofmobileapplications.
TheseworksdemonstratethegrowingcapabilityofLLMsandVLMstounderstandandinteract
withcomplexUIs,provingtheirpotentialtosignificantlyimprovemobileappusability,accessibility,
andtestingworkflows.
6.3 EnhancingMobileUIAccessibilityforUserswithVisualImpairments
Forindividualswithvisualimpairments,informativealt-textlabelsarecrucialastheyofferauditory
descriptionsofimage-basedUIelements,enhancingtheirabilitytoeffectivelynavigateapps.Despite
theirimportance,theprovisionofmeaningfulalt-textsisoftenneglectedbydevelopers,leadingto
significantaccessibilitybarriers[3].Researchesunderscorethewidespreadissueofmissingand
uninformativelabelsinAndroidapps,emphasizingtheurgentneedforautomated,informative
descriptiongenerationforUIelements[29,64].
Several pioneering studies have aimed to address this by focusing on labeling image-based
UIelements.Icons,whichareusedinmostimage-basedUIelementstoconveyinformationand
functionality,havebeenaprimaryfocus.Chenetal.analyzedalargedatasetoficonsextracted
from iPhone apps, highlighting the imbalance between common and long-tail icon types [19].
Theyfurtherproposedacomprehensivelabelingmethodthatcombinesimageclassificationand
few-shotlearning,furtherenhancedbyincorporatingcontextualinformationfromnearbytext
andmodifiers.Theirapproachprimarilycategorizesiconsbasedontheirclassandanydetected
modifiers,focusingonstructuralclassification.Similarly,Zangetal.introducedadeeplearning-
based multimodal approach that combines pixel and view hierarchy features to improve icon
annotationinmobileapplications.[82].Theirmodelsignificantlyenhancesicondetectionand
classificationbyincorporatingbothpixelfeaturesandtextualattributesfromviewhierarchies.In
,Vol.1,No.1,Article.Publicationdate:September2024.18 SabrinaHaqueandChristophCsallner
contrast,ourworkextendsbeyondsimpleclassificationtogeneratedescriptivealt-textforeachicon,
providingricher,contextualinformationthatenhancesusabilityforvisuallyimpairedusers.The
mostrelevantDLapproachesofautomatedalt-textgenerationareLabelDroid[18]andCoala[54].
WhileLabelDroidandCoalafocusedoniconlabelingthroughimagecaptioningtechniquesand
incorporatedviewhierarchydata,theirrelianceonbalanceddatasetslimitedtheirperformance.
Ourapproachovercomestheselimitationsbyfunctioningeffectivelywithaverylimiteddataset
usedforfine-tuning,makingitmoreadaptabletofuturetransitionsinUIdesign.
RecentadvancementsinVision-LanguageModels(VLMs)havefurtherexpandedthepotential
for accessibility in mobile UIs. Spotlight advances mobile UI understanding by utilizing VLMs
inavision-onlyapproachtoaddresstaskssuchaswidgetcaptioningandscreensummarization,
bypassingtheoftenincompleteorinaccurateviewhierarchies[45].Chenetal.introducedPaLI,
amultimodalmodelcapableofhandlingbothvisualandtextualinputs,achievingstate-of-the-
artperformanceacrossavarietyoftaskssuchasimagecaptioning,visualquestionanswering
(VQA),andimage-textretrieval[23].PaLIistrainedonlarge-scaleimage-textpairsacrossmultiple
languagesandperformsexceptionallywellonavarietyoftaskssuchasimagecaptioning,visual
questionanswering(VQA),andimage-textretrieval.ThePaLIfamily’slatestmodel,PaLI-3[22],
hasshownoutstandingresultsinwidgetcaptioning,surpassingevenitslargerpredecessor,PaLI-
X[21].ScreenAI[9]takesinspirationfromthesemodelsandcombinesthemwithPix2Struct’s[44]
mechanisms to excel in UI-related tasks like widget captioning and screen summarization. In
comparison to these approaches, our model specifically addresses the challenge of partial UI
screens,makingitmoreflexibleandadaptabletoincompleteUIinformation,whichisespecially
valuableduringearlydevelopmentstageswhentheentireUImaynotbefullyavailable.
WeevaluateourworkagainsttherelevantavailablemodelsPix2Struct[44]andPaliGemma[12].
7 CONCLUSIONS
Userinterfaceiconsareessentialfornavigationandinteractionandoftenlackmeaningfulalt-
text,creatingbarrierstoeffectiveuse.Traditionaldeeplearningmethods,whileeffectiveinsome
contexts,generallyrequireextensiveandbalanceddatasetsandoftenstrugglewhenfacedwith
UIiconsdiversity.Moreover,currentVision-LanguageModels(VLMs),whilesophisticated,do
notperformoptimallywithpartialUIscreens.WeintroducedIconDesc,anovelapproachthat
efficientlygeneratesinformativealt-textfrompartialUIscreens,usingarelativelysmalldataset
ofabout1.4kicons.Ourresultsdemonstratedsignificantimprovementsingeneratingrelevant
alt-textusingpartialUIinformation.IntegratingIconDescintowidelyuseddevelopmenttools
couldfurtherenhanceitsutilitybymakingalt-textgenerationastandardpartofthedevelopment
workflow,therebypromotingbroaderadoptionandimpact.
8 DATAAVAILABILITY
AllcodeanddataareavailableonbothGitHub[33]andFigshare5.
REFERENCES
[1] JoshAchiametal.2024.GPT-4TechnicalReport. arXiv:2303.08774[cs.CL] https://arxiv.org/abs/2303.08774
[2] ADA.gov.2023.TheAmericanswithDisabilitiesAct(ADA). https://www.ada.gov/.
[3] AbdulazizAlshayban,IftekharAhmed,andSamMalek.2020. AccessibilityissuesinAndroidapps:Stateofaffairs,
sentiments,andwaysforward.InProc.ACM/IEEE42ndInternationalConferenceonSoftwareEngineering(ICSE).
1323–1334.
[4] PeterAnderson,BasuraFernando,MarkJohnson,andStephenGould.2016. SPICE:SemanticPropositionalImage
CaptionEvaluation.InProc.14thEuropeanConferenceonComputerVision(ECCV).Springer,382–398.
5https://figshare.com/s/610e319cb0d4ed663332
,Vol.1,No.1,Article.Publicationdate:September2024.InferingAlt-textForUIIconsWithLargeLanguageModelsDuringAppDevelopment 19
[5] Android.2024.Buildaccessibleapps. https://developer.android.com/guide/topics/ui/accessibility.
[6] Android.2024. Lint:Improveyourcodewithlintchecks. AccessedMarch1,2024.https://developer.android.com/
studio/write/lint?hl=en.
[7] RohanAniletal.2024. Gemini:Afamilyofhighlycapablemultimodalmodels. arXiv:2312.11805[cs.CL] https:
//arxiv.org/abs/2312.11805
[8] Apple.2024.Buildingaccessibleapps. https://developer.apple.com/accessibility/.
[9] GillesBaechler,SrinivasSunkara,MariaWang,FedirZubach,HassanMansoor,VincentEtter,VictorCărbune,JasonLin,
JindongChen,andAbhanshuSharma.2024.Screenai:Avision-languagemodelforuiandinfographicsunderstanding.
arXivpreprintarXiv:2402.04615(2024).
[10] PratyayBanerjee,ShwetiMahajan,KushalArora,ChittaBaral,andOrianaRiva.2022.Lexi:Self-SupervisedLearning
oftheUILanguage.InProc.ConferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP).Associationfor
ComputationalLinguistics,6992–7007. https://doi.org/10.18653/V1/2022.FINDINGS-EMNLP.519
[11] SatanjeevBanerjeeandAlonLavie.2005.METEOR:AnautomaticmetricforMTevaluationwithimprovedcorrelation
withhumanjudgments.InProc.ACLWorkshoponIntrinsicandExtrinsicEvaluationMeasuresforMachineTranslation
and/orSummarization.AssociationforComputationalLinguistics,65–72.
[12] LucasBeyeretal.2024.PaliGemma:Aversatile3BVLMfortransfer. arXiv:2407.07726[cs.CV] https://arxiv.org/abs/
2407.07726
[13] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,
PranavShyam,GirishSastry,AmandaAskell,etal.2020.Languagemodelsarefew-shotlearners.Advancesinneural
informationprocessingsystems33(2020),1877–1901.
[14] AndreaBurns,KateSaenko,andBryanA.Plummer.2024. TellMeWhat’sNext:TextualForesightforGenericUI
Representations.InProc.FindingsoftheAssociationforComputationalLinguistics(ACL). https://arxiv.org/abs/2406.
07822
[15] Yuxiang Chai et al. 2024. AMEX: Android Multi-annotation Expo Dataset for Mobile GUI Agents.
arXiv:2407.17490[cs.HC] https://arxiv.org/abs/2407.17490
[16] Dongping Chen et al. 2024. GUI-World: A Dataset for GUI-oriented Multimodal LLM-based Agents.
arXiv:2406.10819[cs.CV] https://arxiv.org/abs/2406.10819
[17] JieshanChen.2020.DatasetsandTrainedModelsfor"UnblindYourApps:PredictingNatural-LanguageLabelsfor
MobileGUIComponentsbyDeepLearning". https://zenodo.org/records/8100245
[18] JieshanChen,ChunyangChen,ZhenchangXing,XiweiXu,LimingZhu,GuoqiangLi,andJinshuiWang.2020.Unblind
YourApps:PredictingNatural-LanguageLabelsforMobileGUIComponentsbyDeepLearning.InProc.ACM/IEEE
42ndInternationalConferenceonSoftwareEngineering(ICSE).ACM,322–334. https://doi.org/10.1145/3377811.3380327
[19] JieshanChen,AmandaSwearngin,JasonWu,TitusBarik,JeffreyNichols,andXiaoyiZhang.2022.Towardscomplete
iconlabelinginmobileapplications.InProc.CHIConferenceonHumanFactorsinComputingSystems.ACM,1–14.
https://doi.org/10.1145/3491102.3502073
[20] Wentong Chen et al. 2024. GUICourse: From General Vision Language Models to Versatile GUI Agents.
arXiv:2406.11317[cs.AI] https://arxiv.org/abs/2406.11317
[21] XiChen,JosipDjolonga,PiotrPadlewski,BasilMustafa,SoravitChangpinyo,JialinWu,CarlosRiquelmeRuiz,Sebastian
Goodman,XiaoWang,YiTay,SiamakShakeri,MostafaDehghani,DanielSalz,MarioLucic,MichaelTschannen,Arsha
Nagrani,HexiangHu,MandarJoshi,BoPang,CesleeMontgomery,PaulinaPietrzyk,MarvinRitter,AJPiergiovanni,
MatthiasMinderer,FilipPavetic,AustinWaters,GangLi,IbrahimAlabdulmohsin,LucasBeyer,JulienAmelot,Kenton
Lee,AndreasPeterSteiner,YangLi,DanielKeysers,AnuragArnab,YuanzhongXu,KeranRong,AlexanderKolesnikov,
MojtabaSeyedhosseini,AneliaAngelova,XiaohuaZhai,NeilHoulsby,andRaduSoricut.2023.PaLI-X:OnScalingup
aMultilingualVisionandLanguageModel. arXiv:2305.18565[cs.CV] https://arxiv.org/abs/2305.18565
[22] XiChen,XiaoWang,LucasBeyer,AlexanderKolesnikov,JialinWu,PaulVoigtlaender,BasilMustafa,Sebastian
Goodman,IbrahimAlabdulmohsin,PiotrPadlewski,DanielSalz,XiXiong,DanielVlasic,FilipPavetic,KeranRong,
TianliYu,DanielKeysers,XiaohuaZhai,andRaduSoricut.2023. PaLI-3VisionLanguageModels:Smaller,Faster,
Stronger. https://doi.org/10.48550/ARXIV.2310.09199arXiv:2310.09199[cs.CV]
[23] XiChen,XiaoWang,SoravitChangpinyo,AJPiergiovanni,PiotrPadlewski,DanielSalz,SebastianGoodman,Adam
Grycner,BasilMustafa,LucasBeyer,etal.2022. Pali:Ajointly-scaledmultilinguallanguage-imagemodel. arXiv
preprintarXiv:2209.06794(2022).
[24] AakankshaChowdhery,SharanNarang,JacobDevlin,MaartenBosma,GauravMishra,AdamRoberts,PaulBarham,
HyungWonChung,CharlesSutton,SebastianGehrmann,etal.2023.Palm:Scalinglanguagemodelingwithpathways.
JournalofMachineLearningResearch24,240(2023),1–113.
[25] BiplabDeka,ZifengHuang,ChadFranzen,JoshuaHibschman,DanielAfergan,YangLi,JeffreyNichols,andRanjitha
Kumar.2017.Rico:Amobileappdatasetforbuildingdata-drivendesignapplications.InProceedingsofthe30thannual
ACMsymposiumonuserinterfacesoftwareandtechnology.845–854.
,Vol.1,No.1,Article.Publicationdate:September2024.20 SabrinaHaqueandChristophCsallner
[26] MariannaDiGregorio,DarioDiNucci,FabioPalomba,andGiulianaVitiello.2022.ThemakingofaccessibleAndroid
applications:Anempiricalstudyonthestateofthepractice.EmpiricalSoftwareEngineering27,6(2022),145.
[27] ShirinFeiz,JasonWu,XiaoyiZhang,AmandaSwearngin,TitusBarik,andJeffreyNichols.2022.UnderstandingScreen
RelationshipsfromScreenshotsofSmartphoneApplications.InProc.27thInternationalConferenceonIntelligentUser
Interfaces(IUI).ACM,447–458. https://doi.org/10.1145/3490099.3511109
[28] FLATICON.2024.Access14.8M+vectoricons&stickers. https://www.flaticon.com/.
[29] RaymondFok,MingyuanZhong,AnneSpencerRoss,JamesFogarty,andJacobOWobbrock.2022. ALarge-Scale
LongitudinalAnalysisofMissingLabelAccessibilityFailuresinAndroidApps.InProceedingsofthe2022CHIConference
onHumanFactorsinComputingSystems.1–16.
[30] PetkoGeorgievetal.2024. Gemini1.5:Unlockingmultimodalunderstandingacrossmillionsoftokensofcontext.
arXiv:2403.05530[cs.CL] https://arxiv.org/abs/2403.05530
[31] GitHub.2023.EasyOCR. https://github.com/JaidedAI/EasyOCR.
[32] GitHub.2023.MicrosoftCOCOCaptionEvaluation. https://github.com/tylin/coco-caption.
[33] GitHub.2024.QuicklyInferringIconLabelsFromPartialScreens. https://anonymous.4open.science/r/alt-text-code-
4BB7/README.md.
[34] Google.2024. GetstartedonAndroidwithTalkBack. https://support.google.com/accessibility/android/answer/
6283677?hl=en.
[35] GoogleGemini.2024.AIforeverydeveloper. https://ai.google.dev/.
[36] GoogleLLC.2024. AccessibilityScanner. https://play.google.com/store/apps/details?id=com.google.android.apps.
accessibility.auditor&hl=en_US.
[37] IBM.2024.IBMAccessibility. AccessedMarch1,2024.https://www.ibm.com/able/.
[38] iOS.2024.VoiceOveroniPhone.https://support.apple.com/guide/iphone/turn-on-and-practice-voiceover-iph3e2e415f/
ios.
[39] SusanneIwarssonandAgnetaStåhl.2003.Accessibility,usabilityanduniversaldesign—positioninganddefinitionof
conceptsdescribingperson-environmentrelationships.Disabilityandrehabilitation25,2(2003),57–66.
[40] YueJiang,EldonSchoop,AmandaSwearngin,andJeffreyNichols.2023.ILuvUI:Instruction-tunedLangUage-Vision
modelingofUIsfromMachineConversations. arXiv:2310.04869[cs.HC] https://arxiv.org/abs/2310.04869
[41] JustinJohnson,RanjayKrishna,MichaelStark,Li-JiaLi,DavidShamma,MichaelBernstein,andLiFei-Fei.2015.
Imageretrievalusingscenegraphs.InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.
3668–3678.
[42] MertKilickaya,AykutErdem,NazliIkizler-Cinbis,andErkutErdem.2016.Re-evaluatingautomaticmetricsforimage
captioning.arXivpreprintarXiv:1612.07600(2016).
[43] GeewookKim,TeakgyuHong,MoonbinYim,JeongYeonNam,JinyoungPark,JinyeongYim,WonseokHwang,Sangdoo
Yun,DongyoonHan,andSeunghyunPark.2022. OCR-FreeDocumentUnderstandingTransformer.InProc.17th
EuropeanConferenceonComputerVision(ECCV).Springer,498–517. https://doi.org/10.1007/978-3-031-19815-1_29
[44] KentonLee,MandarJoshi,IuliaRalucaTurc,HexiangHu,FangyuLiu,JulianMartinEisenschlos,UrvashiKhandelwal,
PeterShaw,Ming-WeiChang,andKristinaToutanova.2023. Pix2Struct:ScreenshotParsingasPretrainingfor
VisualLanguageUnderstanding.InProc.InternationalConferenceonMachineLearning(ICML).PMLR,18893–18912.
https://proceedings.mlr.press/v202/lee23g.html
[45] GangLiandYangLi.2023.Spotlight:MobileUIUnderstandingusingVision-LanguageModelswithaFocus.InProc.
11thInternationalConferenceonLearningRepresentations(ICLR).OpenReview.net. https://openreview.net/pdf?id=
9yE2xEj0BH7
[46] LinlinLi,RuifengWang,XianZhan,YingWang,CuiyunGao,SinanWang,andYepangLiu.2023.WhatYouSeeIsWhat
YouGet?ItIsNottheCase!DetectingMisleadingIconsforMobileApplications.InProc.32ndACMSIGSOFTInternational
SymposiumonSoftwareTestingandAnalysis(ISSTA).ACM,538–550. https://doi.org/10.1145/3597926.3598076
[47] YangLi,GangLi,LuhengHe,JingjieZheng,HongLi,andZhiweiGuan.2020.WidgetCaptioning:GeneratingNatural
LanguageDescriptionforMobileUserInterfaceElements.InProc.ConferenceonEmpiricalMethodsinNaturalLanguage
Processing(EMNLP).AssociationforComputationalLinguistics,5495–5510. https://doi.org/10.18653/V1/2020.EMNLP-
MAIN.443
[48] YangLi,GangLi,XinZhou,MostafaDehghani,andAlexeyGritsenko.2021. Vut:Versatileuitransformerfor
multi-modalmulti-taskuserinterfacemodeling.arXivpreprintarXiv:2112.05692(2021).
[49] Chin-YewLin.2004.ROUGE:APackageforAutomaticEvaluationofSummaries.InProc.Textsummarizationbranches
out.AssociationforComputationalLinguistics,74–81.
[50] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.2024. Visualinstructiontuning. Advancesinneural
informationprocessingsystems36(2024).
[51] ThomasFLiu,MarkCraft,JasonSitu,ErsinYumer,RadomirMech,andRanjithaKumar.2018. Learningdesign
semanticsformobileapps.InProceedingsofthe31stAnnualACMSymposiumonUserInterfaceSoftwareandTechnology.
,Vol.1,No.1,Article.Publicationdate:September2024.InferingAlt-textForUIIconsWithLargeLanguageModelsDuringAppDevelopment 21
569–579.
[52] ZheLiu,ChunyangChen,JunjieWang,XingChe,YuekaiHuang,JunHu,andQingWang.2023. Fillintheblank:
Context-awareautomatedtextinputgenerationformobileguitesting.In2023IEEE/ACM45thInternationalConference
onSoftwareEngineering(ICSE).IEEE,1355–1367.
[53] ZheLiu,ChunyangChen,JunjieWang,MengzhuoChen,BoyuWu,XingChe,DandanWang,andQingWang.2023.
Chattingwithgpt-3forzero-shothuman-likemobileautomatedguitesting.arXivpreprintarXiv:2305.09434(2023).
[54] ForoughMehralian,NavidSalehnamadi,andSamMalek.2021. Data-drivenaccessibilityrepairrevisited:Onthe
effectivenessofgeneratinglabelsforiconsinAndroidapps.InProc.29thACMJointMeetingonEuropeanSoftware
EngineeringConferenceandSymposiumontheFoundationsofSoftwareEngineering(ESEC/FSE).ACM,107–118. https:
//doi.org/10.1145/3468264.3468604
[55] GeorgeAMiller.1995.WordNet:alexicaldatabaseforEnglish.Commun.ACM38,11(1995),39–41.
[56] OpenAI.2022.ChatGPT. https://openai.com/index/chatgpt/.
[57] OpenAI.2024.. https://openai.com/.
[58] OpenAI.2024.Fine-tuning-OpenAIAPI. https://platform.openai.com/docs/guides/fine-tuning.
[59] AchrafOthman,AmiraDhouib,andAljaziNasserAlJabor.2023.Fosteringwebsitesaccessibility:Acasestudyon
theuseoftheLargeLanguageModelsChatGPTforautomaticremediation.InProceedingsofthe16thInternational
ConferenceonPErvasiveTechnologiesRelatedtoAssistiveEnvironments.707–713.
[60] KishorePapineni,SalimRoukos,ToddWard,andWei-JingZhu.2002.BLEU:Amethodforautomaticevaluationof
machinetranslation.InProc.40thAnnualMeetingoftheAssociationforComputationalLinguistics(ACL).311–318.
[61] JuanRamosetal.2003. Usingtf-idftodeterminewordrelevanceindocumentqueries.InProceedingsofthefirst
instructionalconferenceonmachinelearning,Vol.242.Citeseer,29–48.
[62] ChristopherRawles,AliceLi,DanielRodriguez,OrianaRiva,andTimothyP.Lillicrap.2023.AndroidintheWild:A
Large-ScaleDatasetForAndroidDeviceControl.InProc.AnnualConferenceonNeuralInformationProcessingSystems
2023(NeurIPS). http://papers.nips.cc/paper_files/paper/2023/hash/bbbb6308b402fe909c39dd29950c32e0-Abstract-
Datasets_and_Benchmarks.html
[63] AnneSpencerRoss,XiaoyiZhang,JamesFogarty,andJacobOWobbrock.2017.Epidemiologyasaframeworkfor
large-scalemobileapplicationaccessibilityassessment.InProceedingsofthe19thinternationalACMSIGACCESS
conferenceoncomputersandaccessibility.2–11.
[64] AnneSpencerRoss,XiaoyiZhang,JamesFogarty,andJacobOWobbrock.2018. Examiningimage-basedbutton
labelingforaccessibilityinAndroidappsthroughlarge-scaleanalysis.InProceedingsofthe20thInternationalACM
SIGACCESSConferenceonComputersandAccessibility.119–130.
[65] PatrickRoyston.1992.ApproximatingtheShapiro-WilkW-testfornon-normality.Statisticsandcomputing2(1992),
117–119.
[66] SrinivasSunkara,MariaWang,LijuanLiu,GillesBaechler,Yu-ChungHsiao,JindongChen,AbhanshuSharma,and
JamesW.Stout.2022. TowardsBetterSemanticUnderstandingofMobileInterfaces.InProc.29thInternational
ConferenceonComputationalLinguistics(COLING).InternationalCommitteeonComputationalLinguistics,5636–5650.
https://aclanthology.org/2022.coling-1.497
[67] MaryamTaeb,AmandaSwearngin,EldonSchoop,RuijiaCheng,YueJiang,andJeffreyNichols.2024.Axnav:Replaying
accessibilitytestsfromnaturallanguage.InProceedingsoftheCHIConferenceonHumanFactorsinComputingSystems.
1–16.
[68] MingxingTanandQuocLe.2019. Efficientnet:Rethinkingmodelscalingforconvolutionalneuralnetworks.In
Internationalconferenceonmachinelearning.PMLR,6105–6114.
[69] TheNounProject.2024.IconsandPhotosForEverything. https://thenounproject.com/.
[70] U.S.AccessBoard.2018. AbouttheICTAccessibility508Standardsand255Guidelines. https://www.access-
board.gov/ict/.
[71] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
Polosukhin.2017.Attentionisallyouneed.Advancesinneuralinformationprocessingsystems30(2017).
[72] RamakrishnaVedantam,CLawrenceZitnick,andDeviParikh.2015. CIDEr:Consensus-basedimagedescription
evaluation.InProc.IEEEConferenceonComputerVisionandPatternRecognition(CVPR).4566–4575.
[73] ChristopherVendome,DianaSolano,SantiagoLiñán,andMarioLinares-Vásquez.2019.Caneveryoneusemyapp?
AnempiricalstudyonaccessibilityinAndroidapps.In2019IEEEInternationalConferenceonSoftwareMaintenance
andEvolution(ICSME).IEEE,41–52.
[74] W3C.[n.d.].W3CAccessibilityStandardsOverview. AccessedMarch1,2024.https://www.w3.org/WAI/standards-
guidelines/.
[75] BryanWang,GangLi,andYangLi.2023.EnablingConversationalInteractionwithMobileUIusingLargeLanguage
Models.InProc.ConferenceonHumanFactorsinComputingSystems(CHI).ACM,432:1–432:17. https://doi.org/10.
1145/3544548.3580895
,Vol.1,No.1,Article.Publicationdate:September2024.22 SabrinaHaqueandChristophCsallner
[76] XintaoWang,LiangbinXie,ChaoDong,andYingShan.2021.Real-esrgan:Trainingreal-worldblindsuper-resolution
withpuresyntheticdata.InProceedingsoftheIEEE/CVFinternationalconferenceoncomputervision.1905–1914.
[77] WCAG.[n.d.].WCAG2Overview. AccessedMarch1,2024.https://www.w3.org/WAI/standards-guidelines/wcag/.
[78] WebAIM.2017.ScreenReaderUserSurvey. https://webaim.org/projects/screenreadersurvey7/.
[79] HaoWen,YuanchunLi,GuohongLiu,ShanhuiZhao,TaoYu,TobyJia-JunLi,ShiqiJiang,YunhaoLiu,YaqinZhang,
andYunxinLiu.2024. AutoDroid:LLM-poweredTaskAutomationinAndroid.InProc.30thAnnualInternational
ConferenceonMobileComputingandNetworking(MobiCom).ACM,543–557. https://doi.org/10.1145/3636534.3649379
[80] FrankWilcoxon.1992.Breakthroughsinstatistics.Individualcomparisonsbyrankingmethods(1992),196–202.
[81] ShunguoYanandPGRamachandran.2019.Thecurrentstatusofaccessibilityinmobileapps.ACMTransactionson
AccessibleComputing(TACCESS)12,1(2019),1–31.
[82] XiaoxueZang,YingXu,andJindongChen.2021.Multimodaliconannotationformobileapplications.InProc.23rd
InternationalConferenceonMobileHuman-ComputerInteraction(MobileHCI).ACM,1–11. https://doi.org/10.1145/
3447526.3472064
[83] JiwenZhang,JihaoWu,YihuaTeng,MinghuiLiao,NuoXu,XiaoXiao,ZhongyuWei,andDuyuTang.2024.Android
intheZoo:Chain-of-Action-ThoughtforGUIAgents. arXiv:2403.02713[cs.CL] https://arxiv.org/abs/2403.02713
[84] XiaoyiZhang,LiliandeGreef,AmandaSwearngin,SamuelWhite,KyleI.Murray,LisaYu,QiShan,JeffreyNichols,
JasonWu,ChrisFleizach,AaronEveritt,andJeffreyP.Bigham.2021. ScreenRecognition:CreatingAccessibility
MetadataforMobileApplicationsfromPixels.InCHIConferenceonHumanFactorsinComputingSystems.ACM,
275:1–275:15. https://doi.org/10.1145/3411764.3445186
,Vol.1,No.1,Article.Publicationdate:September2024.