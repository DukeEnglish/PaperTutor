DiffSSC: Semantic LiDAR Scan Completion using Denoising Diffusion
Probabilistic Models
Helin Cao and Sven Behnke
Abstract—Perception systems play a crucial role in au-
tonomous driving, incorporating multiple sensors and corre-
sponding computer vision algorithms. 3D LiDAR sensors are
widely used to capture sparse point clouds of the vehicle’s
surroundings. However, such systems struggle to perceive oc-
cluded areas and gaps in the scene due to the sparsity of
these point clouds and their lack of semantics. To address
these challenges, Semantic Scene Completion (SSC) jointly (a)SparseLiDARInput (b)DenseSemanticEstimation
predicts unobserved geometry and semantics in the scene
Fig. 1: DiffSSC estimates unseen points with semantics (b) from
given raw LiDAR measurements, aiming for a more complete
raw LiDAR point clouds (a). The unknown areas, as defined by
scenerepresentation.Buildingonpromisingresultsofdiffusion
ground truth, are visualized at 20% opacity in (b).
models in image generation and super-resolution tasks, we
proposetheirextensiontoSSCbyimplementingthenoisingand
training. During inference, new partial inputs captured from denoising diffusion processes in the point and semantic spaces
individually. To control the generation, we employ semantic the scene serve as the likelihood, P(observation|scene), and
LiDAR point clouds as conditional input and design local and themodelfinallyestimatesareasonableposteriorresult.No-
global regularization losses to stabilize the denoising process. tably,thefinalestimationisnotauniqueanswerbutrathera
Weevaluateourapproachonautonomousdrivingdatasetsand
samplefromtheposteriordistribution,P(scene|observation).
our approach outperforms the state-of-the-art for SSC.
This aligns with human intuition, since humans also infer
I. INTRODUCTION plausible results from partial inputs, while unobserved parts
remain subject to infinite possibilities.
Perception systems collect low-level attributes of the
However, most traditional SSC methods are limited to
surrounding environment, such as depth, temperature, and learning the prior distribution of data directly, i.e., training
color, through various sensor technologies. These systems
a network to estimate the target output directly from par-
leverage machine learning algorithms to achieve high-level
tial inputs. Another approach to learning prior distributions
understanding, such as object detection and semantic seg-
is to estimate residuals. Denoising Diffusion Probabilistic
mentation. 3D LiDAR is widely used in self-driving cars to
Models (DDPMs) gradually introduce noise into the data
collect 3D point clouds. However, 3D LiDAR has inherent
in the forward diffusion process and employ a denoiser to
limitations, such as unobservable occluded regions, gaps
learn how to remove these noise residuals. The denoiser
between sweeps, non-uniform sampling, noise, and outliers,
iterativelypredictsandremovesnoise,allowingthemodelto
which present significant challenges for high-level scene
recover high-quality data from pure noise. This mechanism
understanding.
effectivelylearnsthepriordistributionofthedata,whichhas
To provide dense and semantic scene representations for
the potential to be applied in SSC tasks.
downstream decision-making and action systems, Semantic
In this work, we propose DiffSSC, a novel SSC approach
SceneCompletion(SSC)hasbeenproposed,aimedatjointly
leveraging DDPMs. As shown in Fig. 1, our method jointly
predicting missing points and semantics from raw LiDAR
estimates missing geometry and semantics from a scene
point clouds. Given its potential to significantly improve
using raw sparse LiDAR point clouds. During training, the
scenerepresentationquality,thistaskhasgarneredsignificant
model learns the prior distribution by predicting residuals at
attention in the robotics and computer vision communities.
different noise intensity levels. These multi-level noisy data
Understanding3Dsurroundingsisaninherenthumanability,
are generated from ground truth using data augmentation.
developed from observing a vast number of complete scenes
In the inference stage, the sparse semantic logits serve as
in daily life. When humans observe scenes from a single
conditional input, and the model generates a dense and
view, they can leverage prior knowledge to estimate geome-
semantic scene from pure Gaussian noise through a multi-
try and semantics. Drawing inspiration from this capability,
stepMarkovprocess.Wemodelboththepointandsemantic
the SSC model learns prior knowledge of scenes, P(scene),
spaces,designingtheforwarddiffusionandreversedenoising
by estimating the complete scene from partial inputs during
processes to enable the model to learn the scene prior to the
semantic point cloud representation. In summary, our key
ThisresearchhasbeensupportedbyMBZIRCprizemoney.Allauthors
are with the Autonomous Intelligent Systems group, Computer Science contributions are:
InstituteVI–IntelligentSystemsandRobotics–andtheCenterforRobotics
and the Lamarr Institute for Machine Learning and Artificial Intelligence, • We utilize DDPMs for the SSC task, introducing a
UniversityofBonn,Germany;caoh@ais.uni-bonn.de residual-learning mechanism compared to traditional
4202
peS
62
]VC.sc[
1v29081.9042:viXraapproaches that directly estimate the complete scene C. Denoising Diffusion Probabilistic Models
from partial input.
Althoughdiffusionmodelswereoriginallydiscoveredand
• We separately model the point and semantic spaces to proposed in the field of physics, DDPMs [19] was the first
adapt to the diffusion process.
to apply this method to generative models. In subsequent
• Our approach operates directly on the point cloud, research, Rombach et al. [20] introduced latent diffusion
avoiding quantization errors and reducing memory us-
models,wherethediffusionprocessisperformedinthelatent
age,whilemakingitamoreefficientmethodforLiDAR
space of the image. This significantly improved computa-
point clouds.
tionalefficiencyandreducedresourceconsumption,enabling
• We design local and global regularization losses to the generation of high-quality and high-resolution images,
stabilize the learning process.
marking a breakthrough in the field of artistic creation.
Beyond artistic applications, several works [21], [22] have
II. RELATEDWORK
also adapted diffusion models for LiDAR perception. These
A. LiDAR Perception
approaches typically project 3D data onto image-based rep-
LiDAR is widely used in various autonomous agents resentations, such as range images, allowing methods devel-
for collecting 3D point clouds from the environment. In oped for image domains to be directly applied. Notably, due
the past, extensive research was dedicated to employing to the higher demands for accuracy in robotics, controlling
LiDAR for odometry [1] and mapping [2], [3]. Given the the generative process to achieve realistic results remains
inherentchallengesofLiDAR,includingdatasparsity,noise, a significant challenge when applying diffusion models in
andoutliers,researchersconcentratedondevelopingfiltering this field. The recent LiDiff [23] directly applies diffusion
algorithms [4] and robust point cloud registration [5] to models to 3D point clouds for scene completion. However,
achieve accurate and efficient LiDAR-SLAM systems. With it still lacks the capability to model and process semantics
the advent of deep learning, LiDAR data began to be lever- simultaneously. In this work, we apply DDPM to semantic
aged for object detection [6] and semantic segmentation [7]. scene completion, to generate dense and accurate semantic
Additionally, unlike dense representations such as images, scenes.
the sparse nature of LiDAR point clouds presents unique
challenges for models. To address these challenges, some
III. METHODOLOGY
researchersfocusonestimatingthegapsbetweensweepsand Given a raw LiDAR point cloud, our objective is to
occluded regions from sparse point clouds. This has led to estimate a more complete semantic point cloud, including
the development of semantic scene completion, an emerging unobserved points with associated semantic labels within
technique in LiDAR perception. gaps and occluded regions. As illustrated in the Fig. 2, we
achieve this through a diffusion model as the core compo-
B. Semantic Scene Completion (SSC)
nent, supported by a pre-processing semantic segmentation
The task of completion has a long research history. Early module and a post-processing refinement module. First, the
efforts in this field focused on filling small holes in shapes raw LiDAR point cloud is semantically segmented using a
to enhance model quality, typically employing continuous Cylinder3D [7] to generate initial semantic logits. Next, we
energy minimization techniques [8]. With the advent of upsample the semantic point cloud to increase point density
deep learning, approaches evolved to enable networks to for the diffusion process. The duplicated semantic points
learn extensive geometric shape properties [9], allowing to undergo a forward diffusion and a reverse denoising process
estimate of entire models from partial inputs. In contrast to toadjusttheirpositionsandsemantics.Notably,thesemantic
shape completion, semantic scene completion (SSC) [10] pointcloudalsoservesasaconditionalinputforthediffusion
presents a significantly more complex challenge. Scenes model, guiding the generation process. The generated scene
exhibit more intricate geometric structures and encompass includessemanticpointslocatedingapsandoccludedareas.
a wider range of semantic categories. SSCNet [11] repre- To further enhance the quality of the generated scene, we
sents the pioneering work that formally defined this task. designed a refinement model based on MinkUnet [24]–[27]
Since its introduction, various input data modalities, such to densify the point cloud.
as occupancy grids [12], images [13], and LiDAR-camera
A. Denoising Diffusion Probabilistic Models (DDPMs)
fusion [14], have been explored. Additionally, a wide ar-
ray of methodologies, including point-voxel mapping [15], Ho et al. [19] introduced DDPMs to produce high-quality
transformers[16],bird’s-eyeview(BEV)assistance[17],and images through iterative denoising from Gaussian noise.
knowledge distillation [18], have been employed to advance This promising capability is driven by a residual learning
thestateoftheartinthisdomain.However,theseapproaches mechanism that efficiently captures the data distribution.
generally operate on voxelized grids, which poses specific Specifically,theprocessbeginswithaforwarddiffusionstep,
challenges for LiDAR point clouds, as voxelization can duringwhichnoiseisgraduallyaddedtothetargetdataover
introduce quantization errors, leading to a loss of resolution T steps.Themodelisthentrainedtopredictthenoiseadded
and increased memory usage. In this work, we operate at each step. By predicting and removing noise at time step
directly on point clouds, offering a more efficient method t, the model generates results that closely approximate the
for handling LiDAR data. raw data distribution.Fig. 2: The overall pipeline of DiffSSC. The raw LiDAR point cloud is semantically segmented using Cylinder3D [7] to generate initial
semantic logits. The semantic point cloud is then upsampled, increasing point density for the diffusion process. These duplicated points
undergoforwarddiffusionandreversedenoising,refiningtheirpositionsandsemanticlabels.Theoriginalsemanticpointcloudservesas
a conditional input, guiding the generation of points in gaps and occluded areas. To further enhance the generated scene, we introduce a
refinement model based on MinkUNet [24]–[27], which increases the density of the point cloud.
1) Forward Diffusion Process: Assuming a sample x ∼ x . Due to the denoiser effectively learning the high quality
0 0
q(x) from a target data distribution, the diffusion process of the data distribution q(x ), the generated samples are of
T
gradually adds noise to x over T steps, producing a similarly high quality.
0
sequence x ,...,x . When T is large enough, q(x ) is While the denoising process generates samples with qual-
1 T T
approximately equal to a normal distribution N(0,I). The ity similar to the dataset, it only produces random samples.
intensity of noise added at each step is defined by the Hence, the denoising process cannot control the generation
noise factors β ,...,β , which significantly influences the of specific desired data, which poses challenges for certain
1 T
performance of the diffusion model. Specifically, at step downstream applications. [28] addresses this issue by in-
t, Gaussian noise amplified by β is sampled and added troducingconditionalinputstoguidethegenerationprocess.
t
to x . In [19], the noise parameter β is determined This advancement allows us to apply diffusion models to
t−1 t
using a linear schedule, starting from an initial value β tasks like SSC.
0
and linearly increasing over T steps to a final value β .
T
B. Diffusion Semantic Scene Completion
Subsequently, several improved noise schedules have been
proposed, such as the cosine schedule [28] and the sigmoid Regarding the principles of DDPMs, we introduce its
schedule [29]. Due to the inefficiency of adding noise step application in SSC. To focus on the main components,
by step, especially during training, where the noise from we assume that primary semantic segmentation has been
different steps can be shuffled, one can simplify this process obtained using Cylinder3D. In the context of the diffusion
bysamplingx t fromx 0 withoutcomputingtheintermediate model, the input is a partial semantic point cloud X =
steps x 1,...,x t−1. To achieve this, Ho et al. [19] define {x1,...,xN}, where each semantic point xi is a tuple of
α t =1−β t and α¯ t
=(cid:81)t
i=1α i, allowing x t to be sampled a point position and a semantic probability vector (pi,si).
as: Here, pi ∈ R3 represents the 3D coordinates, and si ∈
√ √ ∆C−1 = {s ∈ RC | (cid:80)C sj = 1,sj ≥ 0} lies in
x = α¯ x + 1−α¯ ϵ (1) j=1
t t 0 t the standard (C −1)-dimensional simplex, assuming there
where ϵ ∼ N(0,I). It is important to note that as T is are C classes in total. The output estimates complete point
large enough, q(x ) approaches N(0,I) because α¯ tends cloud Yˆ = {yˆ1,...,yˆM}. We generate the reference Y =
T T
to zero. {y1,...,yM} by fusing multiple frames with ground-truth
2) Reverse Denoising Process: The denoising process semantic labels and then taking the corresponding region as
reverses diffusion and aim to recover the original sample x the input scan X. Our goal is to make the estimated Yˆ as
0
from Gaussian noise. This is accomplished by a denoiser, close as possible to the ground truth Y.
which predicts and removes the noise at each step. The AsmentionedinSec.I,bylearningscenepriors,themodel
reverse diffusion step can be formulated as: gains the ability to estimate a complete scene (posterior)
from partial observations (likelihood). The diffusion model
1−α 1−α¯
x =x − √ t ϵ (x ,t)+ t−1β N(0,I) (2) efficiently learns the distribution of the ground truth data,
t−1 t 1−α¯ θ t 1−α t
t t acquiring knowledge of the scene prior. To achieve this,
where ϵ (x ,t) is the noise predicted from x at step t. The we gradually add noise to the ground truth Y, resulting in
θ t t
processofgeneratingtheoriginaldatacanbeformulatedasa Y ,...,Y , until Y approximates a Gaussian distribution.
1 T T
Markovprocessthatrepeatedlycallsthedenoiseruntilt=0. This form of data augmentation can be simplified using
At this point, the model generates a result that approximates Eq. 1. However, this approach does not directly apply tosemantic scene completion. In Eq. 1, the diffusion process
is defined as a combined distribution of the sample x
√ 0
and global noise ϵ ∼ N(0,I), with coefficients α¯ and
√ t
1−α¯ controllingtheratioofnoiseandsampleatdifferent
t
time steps. This mechanism was originally designed for
images, which are flattened and normalized before global
noiseisadded.Similarly,thisapproachhasalsoshowngood
resultsinshapecompletion,astheshapesaregenerallyclose
toa3DGaussiandistribution.However,inlarge-scalescenes,
the data distribution deviates significantly from a Gaussian,
particularly due to varying data ranges across different point
cloud axes. Applying global noise to the entire scene point
cloudasasingleentitycanobscureimportantdetails,failing Fig. 3: Architecture of our MinkUNet Denoiser. The conditional
to account for the unique characteristics and distribution of input, detailed in the red area, is inserted between each layer of
denoiser to guide the generation of point cloud.
each point. Therefore, we reformulate the diffusion process
as a noise offset ϵ ∈ R3+C added locally to each point
ym ∈Y, as shown in the following equation: a closest point algorithm is employed to effectively align
√ the conditional input with the features. Simultaneously, the
y tm =ym+ 1−α¯ tϵ (3) step t is encoded as τ using sinusoidal positional encodings.
After passing through an MLP individually, the conditional
where ϵ is not an isotropic Gaussian distribution, because of
input and step information are concatenated to form the
different scaling for 3D positions and semantics.
weightW.ToalignthedimensionswiththefeatureF,W is
Although training on ground truth allows the model to
processed through an MLP to produce W′. Finally, W′ and
generate high-quality results, the process remains inherently
F are element-wise multiplied to form the refined feature
uncontrollable. The goal of SSC is to predict a complete
F′, which is then passed to the next layer.
scene from partial input, rather than randomly generating
Since noise is added to local points during the diffusion
scenes. Therefore, during training, we also use the partial
process, we employ the commonly used L loss between
semantic point cloud X as a conditional input, feeding it 2
the added noise and the model’s predictions. Additionally,
into the model to guide the point cloud generation. During
wedesignaregularizationtermfortheglobalcharacteristics
training, we load a random step t ∈ [0,T] at each iteration
of the noise, by specifically focusing on the mean µ and
and compute the corresponding Y using Eq. 3. The model ϵ
t
variance σ . Thus, the loss can be formulated as follows:
istrainedtoestimatethenoiseatvariousintensitieswiththe ϵ
following loss: L=L +λ(L +L )=L +λ(µ2+(σ −1)2) (5)
2 mean var 2 ϵ ϵ
L (Y ,X,t)=∥ϵ−ϵ (Y ,X,t)∥2 (4)
2 t θ t where L is the regularization term focused on local fea-
diff
In traditional loss design, the predicted result is directly tures, commonly used in DDPM models, while L mean = µ2 ϵ
compared to the ground truth. However, our approach uses andL var =(σ ϵ−1)2 aredesignedtoensuretheoverallnoise
residual learning, where the model’s output is compared distribution aligns with a Gaussian distribution.
to the residual. Therefore, to generate the final scene, the
D. Refinement
estimated noise must be removed from the noised scene.
Inspired by Lyu et al. [30], we design a refinement and
During inference, the model begins denoising from Gaus-
upsampling scheme based on MinkUNet to further enhance
sian noise and iteratively predicts and removes the noise in
the density of the diffusion model’s output. This module
the samples, ultimately generating a dense semantic scene.
predicts k bias b ∈ R3 for each point position in the
Sincegroundtruthisnotavailableduringinference,Gaussian k
completed scene, while the semantics are propagated to
noise is generated from X. We duplicate the points in X to
the biased points. The refinement module offers a marginal
matchthequantityinthegroundtruth,ensuringthatthereare
improvement in scene quality, but it functions more like in-
enough points to perform the diffusion process using Eq. 3.
terpolating points in the gaps, rather than learning to predict
Note that the denoiser also takes the partial semantic point
missing geometry and semantics. The main contribution is
cloud X as a condition to guide the generation process.
made by the diffusion model, as will be demonstrated in the
C. Denoiser Design and Regularization ablation study.
AsshowninFig.3,thedenoiserisbasedontheMinkUNet
IV. EXPERIMENT
architecture [24]–[27]. Given the feature F extracted from a
A. Experiment Setup
layer of MinkUNet, we integrate the conditional input and
step information between layers to obtain the fused feature 1) Datasets: We evaluate our approach using the Se-
F′. The raw semantic point cloud X is encoded as a condi- manticKITTI [31] and SSCBench-KITTI360 [32] datasets.
tional input C using the same MinkUNet encoder. To embed SemanticKITTIisawidelyusedautonomousdrivingdataset
the most relevant conditional input into the feature space, that provides point-wise annotations on raw LiDAR pointclouds, extending the KITTI dataset to semantic study. to ensure a fair comparison. In the raw SSC setting of
Additionally, it builds the SSC benchmark by accumulating SemanticKITTI,thesceneislimitedtoacuboidregion,rep-
annotated scans within sequences. SSCBench-KITTI360 is resented in the LiDAR’s local coordinate system as: V =
kitti
another SSC benchmark derived from KITTI-360 [33], fea- {(x,y,z) | x ∈ [0,51.2] m,y ∈ [−25.6,+25.6] m,z ∈
turing LiDAR scans encoded the same as SemanticKITTI. [−3.2,+3.2] m}, which corresponds to a region associ-
This consistency allows SSC methods evaluated on Se- ated only with the LiDAR’s [−90◦,+90◦] FoV. To cover
manticKITTI to be seamlessly transferred to the KITTI-360 the full [−180◦,+180◦] panoramic range while preserving
scenario.However,theseSSCbenchmarksonlyusethefront spatial symmetry, we selected an evaluation region within
half of the LiDAR scan (180° LiDAR field-of-view (FoV)) the LiDAR’s local coordinate system defined as: V =
ours
as input, which is not ideal for LiDAR-centered point cloud {(x,y,z) | x ∈ [−51.2,51.2] m,y ∈ [−25.6,+25.6] m,z ∈
data. To address this, we additionally incorporate the rear [−3.2,+3.2] m}.
half of the point cloud, facilitating the evaluation of SSC We directly used the baselines’ official code and check-
approaches on LiDAR-centered data. Our model is trained points to predict the front and rear parts of the scene, with
and validated purely on SemanticKITTI, using sequences eachpartoftheLiDARsweepinputseparately.Althoughthe
00-06 for training and sequences 09-10 for validation. We baselinesweretrainedonlyonthefrontpartofthescene,the
evaluate our model using the official validation sets of both statisticalcharacteristicsofLiDARdatainthefrontandrear
datasets:sequence08ofSemanticKITTIandsequence07of regions are similar, suggesting that a model trained using
SSCBench-KITTI360. only the front half of the data remains effective for the rear
2) Training and Inference: To train DiffSSC on the 360◦ region as well. Although our ground truth generation covers
LiDAR FoV, we generate the ground truth following the asphericalregionwitharadiusof60meterscenteredonthe
guidelines of SemanticKITTI. First, given the pose of each LiDAR,welimitedourevaluationtotheregionpredictedby
frame, we construct the global map by aggregating the se- thebaselines.Additionally,theunknownareasdefinedbythe
manticLiDARsweepswithinthesequence.Next,weextract raw dataset were mapped into V using known poses, and
ours
the neighboring region around the key frame, specifically, a these unknown areas were excluded from the evaluation.
spherical area centered on the LiDAR with a radius of 60 Although our method operates directly on point clouds,
meters. The model is trained on an NVIDIA A6000 GPU point clouds cannot represent continuous regions in space,
for 20 epochs. For the diffusion parameters, we employ a whichmakesdirectevaluationusingtraditionalIoUchalleng-
cosine schedule to modulate the intensity of noise at each ing. Therefore, we voxelized our results and used traditional
step. Specifically, we set β =3.5×10−5 and β =0.007, IoUforscenecompletionandmIoUforsemanticscenecom-
0 T
with the number of diffusion steps T = 1000, and define pletion evaluation. While this introduces quantization error
β ,...,β using the following equation. and potentially degrades our model’s performance, it aligns
1 T−1
(cid:18) (cid:18) (cid:19)(cid:19) with the baseline settings and preserves their performance
1 t
β =β + 1+cos ·π ·(β −β ) (6) for a fair comparison.
t 0 2 T T 0
B. Main Results
We also set the ratio of global regularization to λ=5.0.
3) Baselines: We compare our approach against LMSC- Based on the experimental setting described above, we
Net [12], JS3C-Net [15], and LODE [34]. Both LMSCNet present the results in Tab. I. Our results include the direct
and JS3C-Net take the front half of the quantized LiDAR predictionsfromDiffSSCandtheoutcomesafterrefinement.
sweep as input and are evaluated in the SSC benchmark Wedotheablationstudywithoutthediffusionmodel,which
of SemanticKITTI. LODE primarily focuses on geome- means directly refining the output of Cylinder3D. We also
try completion using implicit representation. However, to report the results of baselines as a comparison.
demonstrate its flexibility, the authors also report results
TABLE I: Quantitative results on SemanticKITTI and SSCBench-
withextendedsemanticparsing.Acommonlimitationamong
KITTI360
thesebaselinesisthattheyaretrainedonpointcloudswithin
a 180◦ LiDAR field of view. To fairly compare with our SemanticKITTI SSCBench-KITTI360
Method Reference
method, we split the 360◦ LiDAR point cloud input into IoU(SC)mIoU(SSC)IoU(SC) mIoU(SSC)
two halves and feed them separately into the baselines.
LMSCNet[12] 3DV’20 48.24 15.43 33.64 13.47
The outputs from these two halves are then concatenated JS3C-Net[15] AAAI’21 51.32 21.38 35.57 16.95
to obtain a 360◦ result. Additionally, while these baselines LODE[34] ICRA’23 50.61 18.22 38.24 15.39
have only been tested on SemanticKITTI, we also ran them
Cylinder3D(refined) - 23.36 7.63 20.66 7.21
on SSCBench-KITTI360 as a supplementary experiment. DiffSSC - 49.38 22.67 36.76 17.34
SincethesemanticlabelsandoverallpipelineinSSCBench- DiffSSC(refined) - 57.03 25.87 40.72 18.51
KITTI360 are consistent with SemanticKITTI, the baselines
Bestandsecondbestresultsarehighlighted.
can be seamlessly applied to this dataset.
4) Evaluation Metrics and Pipeline: Despite our task QualitativeresultsarepresentedinFig.4.Toemphasizethe
being set in a 360◦ LiDAR point cloud completion context, advantagesofourapproach,whichoperatesdirectlyonpoint
weaimedtoretainthebaselinesettingsascloselyaspossible clouds, we visualize samples from both the SemanticKITTIcar bicycle motorcycle truck other-vehicle person bicyclist motorcyclist road parking sidewalk
other-ground building fence vegetation trunk terrain pole traffic-sign
Fig. 4: Qualitative results on SemanticKITTI and SSCBench-KITTI360. The 19 classes are shown without empty spaces. The estimated
points that are located at the unknown region are visualized with 20% opacity.
and SSCBench-KITTI360 datasets in point cloud form. For greater potential. As a result, the cosine schedule converges
voxel-based methods, the point cloud is generated by sam- significantlyfasterthanthelinearone.Althoughthesigmoid
pling the center point of each occupied voxel. As shown in scheduledoesnotconvergeasquicklyasthecosineschedule,
Fig.4, our DiffSSC model predicts more accurate semantic it is still noticeably faster than the linear schedule. They
segmentationofthebackgroundandprovidesamoreprecise performcomparably,thoughthesigmoidscheduleisslightly
representation of the foreground shapes. Furthermore, the weaker than the cosine schedule. Therefore, in our main
voxel-based baselines, which estimate the scene using two results, we adopted the cosine schedule.
halves of a LiDAR sweep, exhibit discontinuous predictions
at the boundary between the front and rear segments. TABLEII:Model’sperformancebasedondifferentnoiseschedule
functions
C. Model Analysis
SemanticKITTI SSCBench-KITTI360
NoiseSchedule
IoU(SC)mIoU(SSC)IoU(SC) mIoU(SSC)
Linear 45.26 19.02 33.32 14.99
Sigmoid 48.29 22.48 36.24 16.55
Cosine 49.38 22.67 36.76 17.34
Bestandsecondbestresultsarehighlighted.
2) Regularization: We also investigated the model’s per-
formance on SemanticKITTI under different ratios of global
regularization, as shown in Fig. 5b. When λ=0, indicating
(a)Noiseschedule (b)Ratioofglobalregularization the use of only local L regularization without global regu-
2
larization, the model exhibited the worst performance, high-
Fig. 5: Hyperparameter study for the model’s performance
lighting the benefits of incorporating global regularization.
1) Noise Schedule: As mentioned in Sec.III, the noise
As the ratio increases, the model’s performance improves,
scheduledeterminestheintensityofnoiseaddedateachstep,
peaking at λ = 5 before declining. This suggests that
commonly including linear, cosine, and sigmoid schedules.
excessive global regularization can constrain the model’s
We conducted a series of experiments to identify the most
ability to generate finer details.
effective noise schedule for the SSC task. In Fig.5a, we
presentthetrainingcurvessampledateachepoch,highlight- V. CONCLUSIONSANDOUTLOOK
ingtheconvergencepatternsforeachschedule.Additionally,
We proposed DiffSSC, a novel SSC approach based on a
we compare the output of DiffSSC without refinement using
diffusion model. It takes raw LiDAR point clouds as input
each noise schedule in Tab.II, providing insights into their
andjointlypredictsmissingpointsalongwiththeirsemantic
impact.Thelinearschedule,thesimplest,wasprimarilyused
labels,therebyextendingtheapplicationboundariesofdiffu-
in early research. It shows slow and stable convergence,
sion models. We evaluated our method on two autonomous
and its performance is significantly lower than that of the
driving datasets, achieving performance that surpasses the
other two schedules. The cosine schedule, an improved
state-of-the-art. In future work, we will explore methods
function, introduces noise more gradually at the beginning
to enhance inference speed by streamlining the step-by-
andend,withafasterincreaseinthemiddle,balancingfaster
step inference process, enabling the application of diffusion
convergence with high final generation quality. The sigmoid
models [35]. Regarding the impact of noise schedules,
schedule shares similarities with the cosine schedule, fea-
we will also explore more complex yet efficient scheduling
turing an S-shaped curve but offering more precise control
mechanisms, such as adaptive schedules [36].
over the noise introduction, theoretically providing evenREFERENCES [20] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer,
“High-resolution image synthesis with latent diffusion models,” in
[1] J.QuenzelandS.Behnke,“Real-timemulti-adaptive-resolution-surfel ProceedingsoftheIEEEConferenceonComputerVisionandPattern
6Dlidarodometryusingcontinuous-timetrajectoryoptimization,”in Recognition(CVPR),2022,pp.10684–10695.
ProceedingsoftheIEEE/RSJInternationalConferenceonIntelligent
[21] K. Nakashima and R. Kurazume, “Lidar data synthesis with denois-
RobotsandSystems(IROS). IEEE,2021,pp.5499–5506. ingdiffusionprobabilisticmodels,”arXivpreprintarXiv:2309.09256,
[2] D.DroeschelandS.Behnke,“Efficientcontinuous-timeslamfor3D
2023.
lidar-basedonlinemapping,”inProceedingsoftheIEEEInternational
[22] V. Zyrianov, X. Zhu, and S. Wang, “Learning to generate realistic
Conference on Robotics and Automation (ICRA). IEEE, 2018, pp. lidar point clouds,” in Proceedings of the European Conference on
5000–5007. ComputerVision(ECCV). Springer,2022,pp.17–35.
[3] X. Zhong, Y. Pan, J. Behley, and C. Stachniss, “SHINE-Mapping:
[23] L.Nunes,R.Marcuzzi,B.Mersch,J.Behley,andC.Stachniss,“Scal-
Large-Scale 3D Mapping Using Sparse Hierarchical Implicit Neural
ingDiffusionModelstoReal-World3DLiDARSceneCompletion,”in
Representations,”inProceedingsoftheIEEEInternationalConference
ProceedingsoftheIEEEConferenceonComputerVisionandPattern
onRoboticsandAutomation(ICRA),2023.
Recognition(CVPR),2024.
[4] R. B. Rusu and S. Cousins, “3D is here: Point cloud library (pcl),”
[24] C. Choy, J. Gwak, and S. Savarese, “4d spatio-temporal convnets:
inProceedingsoftheIEEEInternationalConferenceonRoboticsand
Minkowski convolutional neural networks,” in Proceedings of the
Automation(ICRA). IEEE,2011,pp.1–4. IEEE Conference on Computer Vision and Pattern Recognition
[5] I. Vizzo, T. Guadagnino, B. Mersch, L. Wiesmann, J. Behley, and (CVPR),2019,pp.3075–3084.
C.Stachniss,“KISS-ICP:InDefenseofPoint-to-PointICP–Simple,
[25] C. Choy, J. Park, and V. Koltun, “Fully convolutional geometric
Accurate, and Robust Registration If Done the Right Way,” IEEE features,” in Proceedings of the IEEE International Conference on
RoboticsandAutomationLetters(RA-L),vol.8,no.2,pp.1029–1036,
ComputerVision(ICCV),2019,pp.8958–8966.
2023.
[26] C.Choy,J.Lee,R.Ranftl,J.Park,andV.Koltun,“High-dimensional
[6] L. Nunes, L. Wiesmann, R. Marcuzzi, X. Chen, J. Behley, and convolutional networks for geometric pattern recognition,” in Pro-
C.Stachniss,“TemporalConsistent3DLiDARRepresentationLearn- ceedings of the IEEE Conference on Computer Vision and Pattern
ingforSemanticPerceptioninAutonomousDriving,”inProceedings
Recognition(CVPR),2020.
oftheIEEEConferenceonComputerVisionandPatternRecognition
[27] J. Gwak, C. B. Choy, and S. Savarese, “Generative sparse detection
(CVPR),2023.
networks for 3D single-shot object detection,” in Proceedings of the
[7] X. Zhu, H. Zhou, T. Wang, F. Hong, Y. Ma, W. Li, H. Li, and EuropeanConferenceonComputerVision(ECCV),2020.
D. Lin, “Cylindrical and asymmetrical 3D convolution networks for
[28] A.Q.NicholandP.Dhariwal,“Improveddenoisingdiffusionproba-
lidarsegmentation,”arXivpreprintarXiv:2011.10033,2020.
bilisticmodels,”inProceedingsoftheIEEEInternationalConference
[8] M.Kazhdan,M.Bolitho,andH.Hoppe,“Poissonsurfacereconstruc- onMachineLearning(ICML). PMLR,2021,pp.8162–8171.
tion,” in Proceedings of the Eurographics Symposium on Geometry
[29] D. Kingma, T. Salimans, B. Poole, and J. Ho, “Variational diffusion
Processing(SGP),vol.7,no.4,2006.
models,”AdvancesinNeuralInformationProcessingSystems(NIPS),
[9] A.Dai,C.RuizhongtaiQi,andM.Nießner,“Shapecompletionusing
vol.34,pp.21696–21707,2021.
3D-encoder-predictor cnns and shape synthesis,” in Proceedings of
[30] Z. Lyu, Z. Kong, X. Xu, L. Pan, and D. Lin, “A conditional point
the IEEE Conference on Computer Vision and Pattern Recognition
diffusion-refinementparadigmfor3Dpointcloudcompletion,”arXiv
(CVPR),2017,pp.5868–5877.
preprintarXiv:2112.03530,2021.
[10] L. Rolda˜o, R. de Charette, and A. Verroust-Blondet, “3D semantic
[31] J.Behley,M.Garbade,A.Milioto,J.Quenzel,S.Behnke,C.Stach-
scenecompletion:Asurvey,”InternationalJournalofComputerVision
niss, and J. Gall, “SemanticKITTI: A dataset for semantic scene
(IJCV),vol.130,no.8,pp.1978–2005,2022.
understanding of LiDAR sequences,” in Proceedings of the IEEE
[11] S.Song,F.Yu,A.Zeng,A.X.Chang,M.Savva,andT.Funkhouser, InternationalConferenceonComputerVision(ICCV),2019.
“Semantic scene completion from a single depth image,” in Pro-
[32] Y.Li,S.Li,X.Liu,M.Gong,K.Li,N.Chen,Z.Wang,Z.Li,T.Jiang,
ceedings of the IEEE Conference on Computer Vision and Pattern
F. Yu et al., “Sscbench: Monocular 3D semantic scene completion
Recognition(CVPR),2017,pp.1746–1754.
benchmarkinstreetviews,”arXivpreprintarXiv:2306.09001,2023.
[12] L. Roldao, R. de Charette, and A. Verroust-Blondet, “LMSCNet:
[33] Y. Liao, J. Xie, and A. Geiger, “Kitti-360: A novel dataset and
Lightweight multiscale 3D semantic completion,” in Proceedings of benchmarks for urban scene understanding in 2D and 3D,” IEEE
theInternationalConferenceon3DVision(3DV),2020,pp.111–119.
Transactions on Pattern Analysis and Machine Intelligence, vol. 45,
[13] A.-Q.CaoandR.deCharette,“MonoScene:Monocular3Dsemantic
no.3,pp.3292–3310,2022.
scenecompletion,”in Proceedingsofthe IEEEConferenceonCom-
[34] P.Li,R.Zhao,Y.Shi,H.Zhao,J.Yuan,G.Zhou,andY.-Q.Zhang,
puterVisionandPatternRecognition(CVPR),2022,pp.3991–4001.
“Lode: Locally conditioned eikonal implicit scene completion from
[14] H.CaoandS.Behnke,“SLCF-Net:SequentialLiDAR-camerafusion sparselidar,”inProceedingsoftheIEEEInternationalConferenceon
for semantic scene completion using a 3D recurrent U-Net,” in RoboticsandAutomation(ICRA). IEEE,2023,pp.8269–8276.
Proceedings of the IEEE International Conference on Robotics and
[35] C.Lu,Y.Zhou,F.Bao,J.Chen,C.Li,andJ.Zhu,“Dpm-solver:A
Automation(ICRA),2024,pp.2767–2773.
fast ode solver for diffusion probabilistic model sampling in around
[15] X.Yan,J.Gao,J.Li,R.Zhang,Z.Li,R.Huang,andS.Cui,“Sparse 10steps,”AdvancesinNeuralInformationProcessingSystems(NIPS),
singlesweepLiDARpointcloudsegmentationvialearningcontextual
vol.35,pp.5775–5787,2022.
shapepriorsfromscenecompletion,”inProceedingsoftheNational
[36] A. Jabri, D. Fleet, and T. Chen, “Scalable adaptive computation for
ConferenceonArtificialIntelligence(AAAI),vol.35,no.4,2021,pp.
iterativegeneration,”arXivpreprintarXiv:2212.11972,2022.
3101–3109.
[16] Y.Li,Z.Yu,C.Choy,C.Xiao,J.M.Alvarez,S.Fidler,C.Feng,and
A. Anandkumar, “Voxformer: Sparse voxel transformer for camera-
based 3D semantic scene completion,” in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (CVPR),
2023,pp.9087–9098.
[17] R. Cheng, C. Agia, Y. Ren, X. Li, and L. Bingbing, “S3cnet: A
sparsesemanticscenecompletionnetworkforlidarpointclouds,”in
ProceedingsofMachineLearningResearch(PMLR). PMLR,2021,
pp.2148–2161.
[18] X. Fan, H. Luo, X. Zhang, L. He, C. Zhang, and W. Jiang, “Scp-
net: Spatial-channel parallelism network for joint holistic and partial
person re-identification,” in Proceedings of the Asian Conference on
ComputerVision(ACCV). Springer,2019,pp.19–34.
[19] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic
models,”arXivpreprintarxiv:2006.11239,2020.