Multi-View and Multi-Scale Alignment for Contrastive
Language-Image Pre-training in Mammography
YuexiDu1, JohnOnofrey1,2,3, NichaC.Dvornek1,2
1DepartmentofBiomedicalEngineering,
2DepartmentofRadiology&BiomedicalImaging,3DepartmentofUrology,
YaleUniversity,NewHaven,CT,USA
Abstract
ContrastiveLanguage-ImagePre-training(CLIP)showspromiseinmedicalimage
analysisbutrequiressubstantialdataandcomputationalresources. Duetothese
restrictions,existingCLIPapplicationsinmedicalimagingfocusmainlyonmodal-
itieslikechestX-raysthathaveabundantimage-reportdataavailable,leavingmany
otherimportantmodalitiesunder-explored. Here,weproposethefirstadaptationof
thefullCLIPmodeltomammography,whichpresentssignificantchallengesdueto
labeleddatascarcity,high-resolutionimageswithsmallregionsofinterest,anddata
imbalance. Wefirstdevelopaspecializedsupervisionframeworkformammogra-
phythatleveragesitsmulti-viewnature. Furthermore,wedesignasymmetriclocal
alignmentmoduletobetterfocusondetailedfeaturesinhigh-resolutionimages.
Lastly, we incorporate a parameter-efficient fine-tuning approach for large lan-
guagemodelspre-trainedwithmedicalknowledgetoaddressdatalimitations. Our
multi-viewandmulti-scalealignment(MaMA)methodoutperformsstate-of-the-art
baselinesforthreedifferenttasksontwolargereal-worldmammographydatasets,
EMBEDandRSNA-Mammo,withonly52%modelsizecomparedwiththelargest
baseline. Thecodeisavailableathttps://github.com/XYPB/MaMA. 1
1 Introduction
Contrastivelearning[5,17,16]hasbecomeoneofthemostpopularself-supervisedrepresentation
learningparadigmsduetoitsintuitiveconceptandrobustperformance. Contrastivelearningremoves
the reliance on a supervised signal by optimizing the semantic distance for similar pairs in the
representationspaceinacontrastivemanner. Morerecently,theintroductionofnaturallanguage
signalstocontrastivelearning[38]hasgivenrisetomodernvisual-languagemodels[26,25,29].
ContrastiveLanguage-ImagePre-training(CLIP)[38]hasalsobeenwidelyappliedinthemedical
imagingdomain[53,19,51,56,54,55,15]andshowspromisingimprovementinmedicalimage
understandingwhenlarge-scalemedicalimagingdatasetsareavailable[22,20,15,55]. However,the
CLIPmodelinthenaturalimagedomainusuallydemandsmorethanhundredsofmillionsofimage-
textpairstobeproperlytrained[38,44,46,45],whichisalmostimpossibleinthemedicaldomain
duetoprivacyandsecurityconcerns. ExistingmedicalCLIPmethodseitherbuildgeneral-purpose
CLIPmodelswithmultipleanatomicalsitesandmodalitiesfrompubliconlinedatabases[15,55]
orfocusonimagingmodalitieswithlarge-scale(lessthanamillion)datasets,e.g.,chestX-rayor
pathologyimages[56,19,51,54,53,60,52,50,24]. Thismeansotherimagingmodalities,suchas
mammography,haveyettofullybenefitfromsuchvisual-languagepre-trainedmodels.
Mammography is a critical medical imaging modality for breast cancer screening and diagnosis,
as breast cancer is one of the most commonly diagnosed cancers globally and a leading cause
1ThisworkisalsothebasisoftheoverallbestsolutionfortheMICCAI2024CXR-LTChallenge.
Preprint.Underreview.
4202
peS
62
]VC.sc[
1v91181.9042:viXraℒ
𝑉𝑇
ℒ ℒ ℒ
𝑉𝑇 𝑉𝑉 𝑉𝑇
ℒ ℒ
𝑉𝑉 𝑉𝑇
𝑣 𝑡 𝑣 𝑣 𝑣 𝑡
1 2 𝑣 𝑣෤ 𝑡
𝑓 𝑓 𝑓 𝑓 𝑓 𝑓
𝑉 𝑇 𝑉 𝑇 𝑉 𝑇
𝒯1 𝒯2 𝒯1 𝒯2
Image Text Image Text Multi-view Images Text
(a) CLIP Style (b) SLIP (c) MaMA (Ours)
Figure 1: Comparison of Three Visual-Language Contrastive Learning Frameworks. (a)
CLIP[38]style;(b)SLIP[34]style;(c)ProposedMaMAthatalignsimage-imageandimage-text
features,exploitingthemulti-viewnatureofmammographyandaligningimagesfromthesamestudy.
of cancer-related mortality in women [47]. While visual-language pre-training (VLP) has the
potentialtoimprovemammographyinterpretation,therearetwomajorobstacles: 1)Limiteddata
andannotation: Recentworkhasintroducedalarge-scalemammographyimageandtabulardataset
ofmorethan110,000patients,i.e.,EMBED[21],butnocorrespondingclinicalreportsareavailable.
2) Nature of mammography: Different from the single view natural image or chest X-ray, each
mammographystudyusuallycontainsfourhigh-resolution(∼2,000-by-2,000pixels)viewsofthe
same patient: left and right side, each with craniocaudal (CC) and mediolateral oblique (MLO)
views. Suchmulti-viewmammographyhasthecriticalpropertiesofbilateralasymmetry[12]and
ipsilateralcorrespondence[32]. Bilateralasymmetrymeansimagesfromdifferentsidesofthesame
patientcancontaindifferentinformation,e.g.,density,calcification,andmassfindings. Ipsilateral
correspondence means different views of the same side share similar information from different
viewpoints. Clinicians consider both properties and all four images at once as a cross reference
when reading a study. Meanwhile, lesions of interest are often relatively small compared with
high-resolutionmammograms,whichfurtherchallengesamodel’sabilitytofocusonlocaldetails.
This pixel-level imbalance compounds the problem of image-level imbalance, in which the vast
majorityofmammogramswillnotcontaincancer. Whileonerecentwork[6]attemptstoaddress
these issues by leveraging VLP, they take a fine-tuning approach in which they simply adopt a
pre-trained CLIP model and perform supervised fine-tuning to a zero-shot classification task for
multi-viewmammography, ratherthancapitalizingonthemammographydomaininformationto
performcontrastivelanguage-imagepre-trainingformammographyrepresentationlearning[58].
Toaddressthesechallenges,weproposeanovelMulti-viewandMulti-scaleAlignmenti.e.,MaMA,
contrastivelanguage-imagepre-trainingframeworkthatexploitsthemulti-viewpropertyofmam-
mographyandalignsmulti-scalefeaturessimultaneously. Wefirstproposeanintuitivemethodfor
template-basedreportconstructionfromtabulardatatoresolvethelackofclinicalreportsandenable
VLP.Theproposedmodelthenoptimizesbothmulti-viewimage-imageandsymmetrictext-image
contrastivelosssimultaneously(Fig.1),learningthecorrespondencebetweenthemulti-viewimages
andimage-reportrelationshipfromthesamemammographystudy. Wethenproposeanovelsymmet-
riclocalalignmentmodulethatactivelylearnssentence-patchrelationshipsbycomputingsimilarity
scoresforeachimage-textpair. Wealsoincorporateparameter-efficientfine-tunedlarge-language
models(LLMs)pre-trainedwithmedicaldomainknowledgetoimprovetheunderstandingofthe
reportwhileaddressingdatascarcity. Wevalidateourmethodontwolarge-scalemammography
datasets,EMBED[21]andRNSA-Mammo[4],withmultiplesettingscomparedwithstate-of-the-art
medicalCLIPmethods. Theproposedmethodsurpassesallthebaselineswithaconsiderablegap
withonly52%modelsize,showingpromiseonmultiplemammography-relatedtasks.
2 RelatedWorks
Medical Visual-Language Pre-training Existing medical VLP methods can be divided into
two types depending on the training data. The first type is the general-purpose medical CLIP
modeltrainedwithalarge-scalemedical-imagedatasetwithmultipleanatomicalsitesandimaging
modalitiesderivedfrom PubMed[15, 55]. This approachmainlyfocuseson scalingdatasetsize
while using a vanilla CLIP design [38]. These models show promising generalization ability on
multiplesitesbutareoftensuboptimalcomparedwithmodality-specificmodelsduetothelackofa
specificdesignfortheindividualimagemodality. TheothertypeofVLPmodelsmainlyfocuseson
2Multi-view Images 𝑥(cid:3556)(cid:3036)
#Patches
𝑔(cid:3023)(⋅) Avg.
𝑣(cid:3556)(cid:3036)
𝑠(cid:3036) ⋯ 𝑝(cid:3036)
Image
Encoder ℒ(cid:3023)(cid:3023)
𝑓(cid:3023)
𝑔(cid:3023)(⋅) Avg.
𝑣(cid:3036)
TabuS lt au rd y A- nl ne ov te al tion 𝑥(cid:3036) ℎ(cid:3023)(⋅) ⋯ SLA ⋯ ℒ(cid:3023)(cid:3021) localT ie zx at t ion 𝐶(cid:3036),(cid:3036) Avg.
ℒ(cid:3023)(cid:3021) ⋯
RS et pr ou rc tt u Cr oe nd s tM ra um cm to o. r ℎ(cid:3021)(⋅) ⋯ Avg.
𝑦(cid:3036) 𝑐(cid:3036)(cid:3021) ,(cid:3036) 𝑐(cid:3036)(cid:3023) ,(cid:3036)
‘[BOS]Procedure reported: MG Diagnostic Bilateral
w [ B/ S r E eC P aA ] sD t P[ caS otE miP pe] on stR i e tia ins ofo no: :n … …fo [ [S Sr E E P Pp ] ]roc I Fe m id a nu g dr e ie n: i gn s :d f i o …a :g [Sn … Eo [ Ps S ]t Ei Pc ] EnT ce ox dt er 𝑔(cid:3021)(⋅) Avg. 𝑡(cid:3036) ℒ(cid:3039)(cid:3042)(cid:3030)(cid:3028)(cid:3039)=1 2(ℒ (cid:3039)(cid:3021) (cid:3042)(cid:3030)(cid:3028)(cid:3039)+ℒ (cid:3039)(cid:3023) (cid:3042)(cid:3030)(cid:3028)(cid:3039)) I om fp r me as ls ii go nn an:
c
yB I [- SR EA PD ]S C Oa vt ee rg ao llry A5 ss:
e
h si smg ehl ny
t :
su Hg ig ge hs lt yi ve 𝑓(cid:3021)
suggestive of malignancy [SEP][EOS]’
(a) MaMA (b) Symmetric Local Alignment (SLA)
Figure 2: Proposed Multi-view and Multi-scale (MaMA) VLP Framework. (a) We utilize
themulti-viewinformationofmammographytoconductsymmetricimage-imageandimage-text
contrastivelearning. (b)Welocalizethemostrelevantsentenceforeachimagepatchandthemost
relevantpatchforeachsentenceandalignthesematchedlocalfeaturesviasymmetriclocalalignment.
chestX-ray[56,19,51,54,53,60,52,50]duetotheavailabilityoflargedatasets,trainedoneither
MIMIC-CXR[22]orCheXpert[20]datasets. Whilethesemethodsshowimpressiveperformance
onchest-specifictasks,theyarespeciallydesignedforsingle-viewimageslikeregularCLIP[38].
Someofthemethodsfurtherrequirefullclinicalreportspairedwiththeimage[51,50,60],which
makesthemhardertoadopt. Recently,Chenetal. [6]proposedafirstattempttointroduceCLIPto
mammography. Itfine-tunesapre-trainedCLIPmodelwithanaddedmulti-viewimageaggregation
moduletoazero-shotclassificationtask. However,themethoddoesnotperformcontrastivepre-
training,ignorespixel-leveldataimbalance,andcannotcorrelatethemedicalreportwithfine-grained
ROIs. Furthermore,theyonlyfine-tunedapre-trainedCLIPmodelwithafewthousandprivatecases.
Multi-viewContrastiveLearning Toobtainamorerobustself-supervisedcontrastivelearning
framework,methodslikeSLIP[34](Fig.1(b))andDeCLIP[27]exploitimage-imagecontrastive
learningalongwithimage-textcontrastivelearningsimultaneously. Suchideashavebeenappliedto
3Dshaperecognition[9,42]byexploitingthenatureof3Dshapesfromdifferentviewpointsandalso
totheactionrecognitiontaskintherealworld[40]. Thesemethodsallexploitthemulti-viewnature
ofthespecificimagemodality,whereimagesofthesameobjectfromdifferentviewpointssharethe
samesemanticmeaningwhilehavingdifferentappearances. Multi-viewcontrastivelearninghas
alsobeenutilizedinmammography[28,14,43],wherethemulti-viewconsistencyisleveragedto
activelylearnhigh-levelsharedinformationwithinthemulti-viewmammography. However,tothe
bestofourknowledge,noneoftheexistingworkscombinemulti-viewmammographycontrastive
learningwithCLIPtofullyutilizethesupervisingsignalfromthemultimodaldata.
UnsupervisedLocalContrastiveLearning Correlatingadensevisualrepresentationwithfine-
grainedsemanticmeaningisnotonlyhelpfulforimageunderstandingbutvitaltotaskslikesemantic
segmentation. Recentworkaddressthisprobleminthechallengingunsupervisedscenario[19,51,
59,52,31,57,40,30]. Somemethodsrelyonapre-trainedobjectdetectororsegmentationmodelto
extracttheregionofinterest[57]. Othermethodseitheraggregatedensesimilarityscoresandconduct
image-levelcontrastivelearning[59,52,30],whichmayignoretoomuchvisualinformationduring
training, or exhaustively conduct token-level language-image matching and optimize patch-level
contrastiveloss[19,51,40],withthecostofadditionalcomputation.
3
Random
Sampling
Intra-Study
Paired
Meta
Masking Random
Data
Augmentation
snekoT]PES[
⋯
⋯
⋯
⋯
secnetneS#
⋯
localization
Visual
⋯3 Method
Inthissection,weintroducetheproposedMaMA(Fig.2). Webeginwiththeconstructionofthe
structuredmammographyreportfromthetabulardata. Wethenintroducethemulti-viewcontrastive
image-textpre-trainingframework,followedbytheproposedsymmetriclocalalignment(SLA).
3.1 StructuredReportConstruction
DifferentfromchestX-raydatasetsthatprovidepairedimageswithcorrespondingclinicalreports,
e.g.,MIMIC-CXR[22],large-scalemammographydatasetswiththefullreportavailablearerare.
Rather, existing datasets in this domain [21, 4, 35] mainly provide a tabular structure annotation
includingboththeanonymizedmetainformationaswellastheclinicalfindings,e.g.,breastdensity
type, calcification findings, tumor description, and Breast Imaging Reporting and Data System
(BI-RADS)assessmentcategory[41]. Clinicalfindingsserveascross-validationevidenceforthe
finaldiagnosis. UsingaCLIP-style[58]captionwithonlythesimpleclasslabelforcancerwillresult
inahighlysimplifiedcaptionandlimitthemodel’sunderstandingoftheimageduetomissingdetails.
We propose a template-based caption construction method following the standard clinical report
structure[36](Fig.2(a)). Wefirstcreateareporttemplatewithsegmentsdescribingstudyprocedure,
patientmeta-information,imagemeta-information,breastcomposition,findings,clinicalimpression
andthefinaloverallassessmentinanaturallanguagereportstyle. Eachsegmentcontainskeywords
that can be replaced with the corresponding meta-information in the tabular data. By replacing
thesekeywordsandconcatenatingthesesegments,wecanbuildacompleteclinicalreportforeach
specificimage,andprovidemoredetailsforlanguage-imagecontrastivelearning. Weprovidethefull
templateandafewimage-captionexamplesintheappendix.
Meta-InfoMasking Theincreasedinformationfrompatientandimage-specificmeta-datamay
bememorizedbythemodelduringthecontrastivetrainingandresultinlearningshortcutsforthe
modeldecision. Tofocusmoreonthediagnosisanddisease-relatedinformation,weproposeadata
augmentationmethodthatrandomlymaskseachpatientorimagemeta-informationkeywordwitha
probabilityofmwhenconstructingthecaption.
3.2 Multi-viewVLP
Weintroducethemulti-viewcontrastiveVLPframeworkhere. LetD ={(x ,y ), i=0,1,...,N}
i i
beamultimodaldataset,wherethereareN individualimagesx andcorrespondingtextcaptionsy .
i i
Ourframeworkoptimizesbothimage-to-imageandsymmetricimage-to-textcontrastiveloss.
Multi-viewVisualContrastiveLoss Wefirstoptimizethecontrastivelosswithinthemulti-view
images(Fig.2(a)). Wedefineastudytoincludethedatafromthesameimagingsessionforapatient,
includingoneormoreimage-textpairs. Forarandomimage-textpair(x ,y )fromthedatasetD,we
i i
uniformlysampleanotherimagex˜ fromthesamestudythatx belongstoasthepositivesample
i i
of x . Note that x˜ could be x as the augmented view of the same image is naturally a positive
i i i
sample. We augment both images with random data augmentation and then feed into the vision
encoderf andd-dimensionalglobalembeddingprojectionheadg followedbyaveragepoolingto
V V
getcorrespondingvisualembeddingv ,v˜ ∈Rd,i.e.,v =avg(g (f (x ))). Wethencomputethe
i i i V V i
cosinesimilarityforeachpairofvisualembeddingsandoptimizetheInfoNCE[5]lossforv ina
i
mini-batchofsizeB:
exp(sim(v ,v˜)/τ ) vTv
L (v ,v˜)=log i i 1 , wheresim(v ,v )= i j , (1)
VV i i (cid:80)B j=1exp(sim(v i,v j)/τ 1) i j ∥v i∥∥v j∥
whereτ isthevisualtemperatureconstantandv isthej-thvisualembeddinginthebatch. Since
1 j
twoviewsofthesamesideofastudyhaveipsilateralcorrespondence,itisnaturaltotreatthemas
positivesamplesofeachother,asthefeatures,liketumors,presentinoneview,areoftenpresent
intheotherviewaswell. Ontheotherhand,evenifconsideringbilateralasymmetryforimages
fromdifferentsides,theystillsharemuchhigh-levelinformationsuchaspatient-levelfeatures(e.g.,
globalbreastshapesimilarity,age)andsimilarbreastdensity. Introducingmulti-viewmammography
contrastivelearningforcesthemodeltolearnsemanticallysimilarfeaturesfromimageswithinthe
samestudy.Thisalsoprovidesastrongerself-supervisedsignalthanusingrandomaugmentedimages.
Ourimage-to-imagecontrastivelearningframeworkfollowsthedesignofSimCLR[5]forsimplicity.
4SymmetricVisual-TextContrastiveLoss WhileexistingmethodslikeSLIP[34]alsooptimize
bothimage-imageandimage-textcontrastiveloss,wenotethereisapotentialcontradictionbetween
image-imageandimage-textobjectiveswhencomputedfordifferentexamples(Fig.1(b)),i.e.,L
VV
andL areindependentandtheextraimagewillintroduceunnecessarymemorycost. Toaddress
VT
this,weproposere-usingv whenoptimizingL andsymmetricallyoptimizingthisloss.
i VT
Wefeedcaptiony tothetokenizerandtextencoderf andthenthetextglobalprojectionheadg
i T T
withaveragepoolingtogetthetextembeddingt ∈Rd. WeoptimizetheCLIP[38]loss(Fig.2(a)):
i
1 exp(sim(v ,t )/τ ) exp(sim(t ,v )/τ )
L (v ,t )=− (log i i 2 +log i i 2 ), (2)
VT i i 2 (cid:80)B exp(sim(v ,t )/τ ) (cid:80)B exp(sim(t ,v )/τ )
j=1 i j 2 j=1 i j 2
whereτ isthelearnablelanguagetemperatureconstant. WecomputeL forbothv andv˜ forthe
2 VT i i
samet symmetrically. Namely,weminimizethesemanticdistancebetweentwoimagesfromthe
i
sameviewandthecorrespondingreportsimultaneously. Wenotethateveniftheinformationiny is
i
notcompletelymatchedwithx˜ ,e.g.,differentsideandviewcaption,theystillsharealargeoverlap
i
inpatient-levelinformation. Thisencouragesthemodeltominethehigh-levelsharedpatient-related
featuresviaminimizingL (v˜,t )whilefocusingondiagnosis-relatedinformationbyminimizing
VT i i
L (v ,t ).
VT i i
3.3 SymmetricLocalAlignment(SLA)
Mammographyusuallycontainshigh-frequencydetailsandtheregionofinterestisusuallyverysmall.
Thesepropertiesrequireahigherimageresolutionforthedeeplearningmethodtoworkproperly. It
alsochallengesthemodel’sabilitytoextractimportantlocalinformationandfilteroutlessmeaningful
backgroundandtissueunrelatedtodiagnosis. Toaddressthesechallenges,weproposeasymmetric
localalignment(SLA)module. Specifically,theSLAmoduleallowsthemodeltodeterminethelocal
correspondencerelationshipbetweeneachsentenceandimagepatch(Fig.2(b)).
Westartwith extractinglocal featuresfrom input(x ,y ). We feedthe imageandcaption tothe
i i
visionencoderf andtextencoderf respectively,followedbycorrespondinglocalprojectionhead
V T
h
V
andh
T
withoutpoolingtoproduceoutputfeaturesequencev ilocal ∈RNV×dandtl iocal ∈RNT×d,
where N and N are the length of visual tokens and text tokens, respectively. We then extract
V T
sentence-levelfeaturesbyselectingtheembeddingcorrespondingtothe[SEP]token,whichresults
inasequenceofsentenceembeddingss ∈RS×d,whereS isthenumberofsentences. Weextract
i
theimagepatch-levelfeaturesbyremovingtheextrafunctionaltokenslike[CLS]tokens,resulting
inasequenceofpatchembeddingsp ∈RP×d,whereP isthenumberofpatches. Wethencompute
i
the sentence-patch correspondence matrix C ∈ RS×P in the form of cosine similarity, which
i,i
revealstherelationshipbetweenlocalpatchesandeachsentenceinthereport. However,wecannot
directlysupervisethelearningofthismatrixsincewehavenoaccesstothelocalcorrespondence
betweentheimageandthereport.Thus,weaggregatethepatch-sentencelevelcorrespondencematrix
C toanimage-reportlevelsimilarityscore. Westartbylocalizingthepatchthathasthehighest
i,i
correspondenceforeachsentence. Namely,wefindthemostrelevantregionintheimageforeach
sentence. WecallthisprocessVisualLocalization. Wethenaveragethesimilarityscoreforeach
sentencetoobtainacorrespondencescorewhichdescribesthesimilarityofthemostrelevantpatch
forthewholereportcV = 1 (cid:80) max C (j,k),whereC (j,k)isthesimilaritybetweenthej-th
i,i S j k i,i i,i
sentenceandthek-thpatch. Similarly,weconductTextLocalizationbyfindingthemostsimilar
sentencefeatureforeachpatchandaveragingittogetascoreforthesimilarityofthemostrelevant
sentenceforthewholeimagecT = 1 (cid:80) max C (j,k). Wecomputetheaggregatedvisualand
i,i P k j i,i
textlocalizationscoresforallpandsinthemini-batchandoptimizetheInfoNCE[17]loss:
sLV
(i)=−1
(
exp(cV i,i/τ local)
+
exp(cV i,i/τ local)
), (3)
local 2 (cid:80)B exp(cV /τ ) (cid:80)B exp(cV /τ )
j=1 i,j local j=1 j,i local
andLT isdefinedsimilarly,whereτ isthelocaltemperatureconstant. Thefinallocalloss
local local
willthenbeL = 1(LV +LT ). Wenotethatintroducingthislocallossfromthebeginning
local 2 local local
ofthetrainingcanleadtounstablebehaviorastheinitialvisualandlanguageembeddingsarenot
aligned. Thus,weaddthislossafterkstepsoftraining.
Theintuitionbehindthisdesignistomimictheprocessofradiologicinterpretationofamedical
imageintherealworld. Ontheonehand,inmammography,theclinicianwilllookfortheimage
5Table1: LinearClassificationResultsonEMBED[21]Weevaluatelinearclassificationresults
withdifferentamountsoffine-tuningdataforbothBI-RADSanddensitypredictiontasks. Wereport
bothbalancedaccuracy(bACC)andAUCmetrics. Thebestandsecond-bestresultsarehighlighted
inboldandunderlined,respectively. Ourmethodisshadedingray.
EMBEDBI-RADS[21] EMBEDDensity[21]
Models
bACC(%) AUC(%) bACC(%) AUC(%)
1% 10% 100% 1% 10% 100% 1% 10% 100% 1% 10% 100%
Visiononly
Random-ViT[13] 20.84 20.68 22.10 57.15 61.54 61.76 45.81 45.11 47.01 72.83 72.62 72.92
DiNOv2-ViT[37] 22.63 25.17 29.33 61.83 66.00 70.11 66.71 70.80 71.20 89.18 90.46 90.47
DeiT-based[48]
CLIP[38] 19.33 21.97 22.26 55.52 61.02 61.65 48.95 50.33 50.77 75.41 76.31 76.92
ConVIRT[56] 25.08 27.63 29.56 65.43 70.49 71.54 72.66 73.46 73.53 91.69 92.11 92.10
MGCA[51] 24.17 27.28 28.09 65.18 71.08 71.49 74.03 74.49 74.53 91.80 92.25 92.21
DiNOv2-based[37]
CLIP[38] 26.66 31.65 34.35 70.35 74.98 74.11 74.64 75.00 75.97 91.50 90.62 92.39
SLIP[34] 22.94 27.86 30.93 64.43 69.48 71.95 73.24 74.79 75.23 91.56 92.37 92.46
MM-MIL[52] 25.85 30.94 35.11 67.16 71.99 76.12 74.23 76.69 75.77 91.96 93.34 91.65
ConVIRT[56] 24.62 30.38 31.27 65.09 73.33 74.03 74.34 74.95 74.74 92.21 92.56 92.58
MGCA[51] 23.66 30.11 30.27 64.19 72.24 72.54 71.43 72.25 72.20 90.83 91.21 91.24
MaMA 28.46 35.12 39.75 70.63 75.98 77.50 76.26 78.11 78.09 93.11 93.62 93.65
regionsandlocalfeaturesthatappearmostsuspiciousforcancer. Ontheotherhand,theclinician
willwritetheradiologyreportinafewsentencesbasedonthefindingsacrossthewholeimage,while
matchingeachdescriptionwithaspecificfeatureoftheimage. OurproposedSLAgivesthemodel
theabilitytoperceivefine-grainlocalimagedetailwithsentence-leveldescription. Thederivedlocal
similaritymapcouldalsobeusedasaguideoftherelevancebetweenspecificimagedetailsandeach
sentenceintheprovidedreportandthereforeimprovetheinterpretabilityofthemodel.
3.4 LLMwithPEFTasTextEncoder
Lastly,weincorporateparameter-efficientfine-tuning(PEFT)ofapre-trainedlargelanguagemodel
(LLM)asourtextencoder(e.g.,BioMedLM[3])ratherthanuseasmallpre-trainedBERTencoder[2].
Usingapre-trainedLLMwithstrongdomainknowledgecanhelpimprovethemodel’sunderstanding
ofthetextcaptionandprovideamorerobustsupervisedsignalforthevisual-languagepre-training.
Moreover,PEFT(e.g.,LoRA[18])cangreatlyreducethecostofadaptingLLMtoscenarioswitha
shortageofcomputingresourceswhilemaintainingastrongperformanceafterfine-tuning. Adapting
anLLMwithPEFTthushasthepotentialtogreatlyimproveperformancewhilereducingtrainable
parametersandGPUmemorycostscomparedtolearningthecommonlyadoptedBERT-styleencoder.
4 Experiments
4.1 Pre-trainingSettings
Dataset We pre-trained our model on the Emory EMBED [21] dataset, which is a large-scale
mammography dataset with public access and one of the largest open mammography datasets
available. Thecurrentreleasecontains72,768multi-viewmammographystudiesfor23,356patients
collectedfrom4hospitals. Wefocuson2Dmammography,whichhas364,564individualimages
intotal. Thedatasetprovidestabularannotationaboutthepatient,imagingmeta-information,and
correspondingimage-levelfindingsincludingbreastdensity,BI-RADSassessment,andcalcification
findings. Wesplitthedatasetbypatientintotrain/validation/testpartitions,eachwith70%/10%/20%
images. Alltheimagesareresizedandpaddedto518×518withoutchangingtheaspectratio.
Implementation Details We choose to use DiNOv2-ViT-B-14 [37] and BioMedLM [3] as our
imageandtextencoderrespectively. WeadaptLoRA[18]tothetextencodertofine-tuneitefficiently.
We choose DiNOv2 [37] ViT as it is pre-trained with a larger image size which is suitable for
mammography. Notethatourmethoddoesnotdependonaspecifictextencoderdesign. Wealso
reporttheperformanceofourmodelwithamorecommonBioClincialBERT[2]encoder. Themeta
maskingratiomis0.8duringtraining. WetrainourmodelwiththeAdamWoptimizer[33]usinga
learningrateof4E-5,weight-decayof0.1,andcosineannealingschedulerfor40ksteps. Wealso
adaptwarm-upfrom1E-8for4ksteps. TheSLAlossisaddedafterk =8ksteps. Weuseabatch
6Table2: Zero-shotandFullFine-tuningResultsonEMBED[21]Weevaluatezero-shotandfully
fine-tunedclassificationresultsforbothBI-RADSanddensitypredictiontasks. Wereportbalanced
accuracy(bACC)andAUC.Thebestandsecond-bestresultsarehighlightedinboldandunderlined,
respectively. Ourmethodisshadedingray.
EMBEDBI-RADS[21] EMBEDDensity[21]
Models
Zero-shot FullFine-tune Zero-shot FullFine-tune
bACC(%) AUC(%) bACC(%) AUC(%) bACC(%) AUC(%) bACC(%) AUC(%)
DeiT-based[48]
CLIP[38] 23.86 67.08 25.05 63.43 71.72 91.52 71.90 89.74
ConVIRT[56] 23.72 62.85 31.80 72.82 64.61 86.62 77.07 93.34
MGCA[51] 22.73 62.24 33.05 74.20 68.47 87.86 77.29 93.47
DiNOv2-based
CLIP[38] 23.05 59.81 34.25 71.61 73.56 92.37 77.47 93.69
SLIP[34] 24.14 67.47 21.75 61.96 75.45 92.17 64.72 86.37
MM-MIL[52] 21.78 62.41 33.05 71.26 69.73 89.07 75.92 92.59
ConVIRT[56] 25.27 65.13 34.54 74.05 64.85 87.66 77.93 93.60
MGCA[51] 26.55 63.76 34.15 73.89 69.00 88.36 77.74 93.64
MaMA 31.04 74.83 40.31 77.36 75.40 93.46 78.02 93.65
sizeof144andtrainthemodelon4RTXA5000GPUswithBFloat-16precision. Wesetd=512
andτ =τ =τ =0.07. Weprovidemoredetailsforhyper-parametersintheappendix.
1 2 local
4.2 DownstreamEvaluationSettings
Tasks and Datasets We primarily evaluate our method on the EMBED [21] dataset for both
BI-RADSassessmentcategory(7classes)andbreastdensity(4classes)predictiontasks. Notethat
the real-world distribution of labels for both tasks is extremely imbalanced. To demonstrate the
behaviorofeachmodelinamorerealisticscenario,wesample7,666imagesforBI-RADSprediction
and7,301imagesforbreastdensitypredictionfromthetestsplitfollowingthedatasetdistribution.
To avoid insufficient test data and possible bias, we use all the images with BIRADS 5 and 6 in
theBIRADSpredictiontestset. Detailedclassdistributionisprovidedintheappendix. Wealso
usetheRSNA-Mammo[4]datasetforout-of-domainevaluationforbinarycancerdetection,which
onlyreleasedatrainingsetwith54kimages. Wesplititintoatrainingsetof85%dataandused
theremainingastheevaluation. Giventheextremelyimbalanceddistributionofbothdatasets,we
choosetoreportbalancedaccuracyandAUCasourprimarymetrics. Wealsoreportthesensitivity
andspecificityoftheRSNA-Mammocancerdetectiontask. Wedonotassesszero-shotclassification
onthisdatasetsinceonlyabinarycancerlabelisavailableforallsamples.
EvaluationSettings Weevaluateallmethodsunderzero-shot,linearclassification,andfullfine-
tuning settings. For zero-shot classification, we provide patient and imaging meta-information
alongwiththeclass-wisecaptions,asthismeta-informationisreadilyavailablewithoutaclinician’s
diagnosis. Forlinearclassification,weattachalinearclassifierandfine-tuneitusing1%,10%,or
100%ofthetrainingdata. Following[56,19,51,53,54,50],weperformthisfulldataefficiency
studywithlinearclassificationandpresentasourprimaryresultssincethisexperimentmainlyfocuses
onthequalityofthepre-trainedembeddinganditcanbestdemonstratethedifferencebetweeneach
VLPmethod. Forfullfine-tuning,weagainattachalinearclassifierandfine-tunethewholemodel
using100%ofthetrainingdata. Ourlearningrateissetto5E-4andweightdecayto1E-3usingthe
SGDoptimizerwithcosineannealingschedulerfor8kstepswithbatchsize36. Awarm-upof100
stepswithaminimumlearningrateof1E-5isapplied. Thefine-tuninguses2RTXA5000GPUs.
Baselines Astheveryfirstattemptatfullcontrastivelanguage-imagepre-trainingformammog-
raphy, we choose to compare with the following baselines: 1) ViT [13, 37]: we compare with
vision-onlybaselineswithbothrandominitializationandDiNOv2[37]pre-training. 2)CLIP[39]:
thevanillaCLIPmodelwithoutotheradditionaldesign;3)SLIP[34]: acontrastivelearningframe-
workthatoptimizesbothimage-imageandimage-textloss;4)MM-MIL[52]: aCLIPmodelthat
learnslocalimage-languagerelationshipviaamultipleinstancelearningparadigm;5)ConVIRT[56]:
oneofthefirstChestX-rayspecificCLIPmodels;6)MGCA[51]: oneoftheSoTACLIPmodels
for Chest X-ray that applies multi-granularity feature alignment. We pre-train and fine-tune all
thesebaselineswiththesamesettingsasourmodel. WealsoreplacedtheoriginalDeiT[48]ViT
withDiNOv2[37]forafaircomparisonsinceDeiT-ViT[48]isonlytrainedwithasmallerimage
size. Allthebaselinemethodsusefullyfine-tunedBioClinicalBERT[2]astextencoder. Whilewe
acknowledgethatthereareotherrecentmedicalVLPmethods[19,54,50,53], theyeitheradapt
7Table3: ClassificationResultsonRSNA-Mammo[4]Weevaluatelinearclassificationandfully
fine-tuned settings for the cancer prediction task. We report balanced accuracy (bACC), AUC,
sensitivity(SEN),andspecificity(SPE).Thebestandsecond-bestresultsarehighlightedinboldand
underlined,respectively. Ourmethodisshadedingray.
RSNA-Mammo[4]
Models
LinearClassification FullFine-tune
bACC(%) AUC(%) SEN(%) SPE(%) bACC(%) AUC(%) SEN(%) SPE(%)
Visiononly
Random-ViT 51.90 56.34 72.60 31.21 56.71 57.62 77.88 35.53
DiNOv2-ViT 63.23 68.59 59.62 66.84 55.12 58.18 70.19 40.06
DeiT-based[48]
CLIP[38] 53.97 58.20 85.58 22.37 56.83 61.00 64.42 49.24
ConVIRT[56] 65.96 69.81 66.83 65.10 53.31 69.16 8.65 97.96
MGCA[51] 63.01 69.16 62.50 63.52 53.88 73.04 12.02 95.74
DiNOv2-based
CLIP[38] 63.89 70.28 58.17 69.61 56.86 61.20 69.23 44.49
SLIP[34] 62.48 67.51 78.37 46.60 56.74 60.05 63.94 49.53
MM-MIL[52] 64.02 70.67 58.17 69.86 59.97 65.04 57.21 62.73
ConVIRT[56] 65.89 70.70 66.83 64.96 54.53 69.85 11.06 98.01
MGCA[51] 60.79 67.45 71.15 50.43 55.99 68.67 14.90 97.07
MaMA 67.50 73.99 72.60 62.40 65.20 73.01 67.31 63.10
domain-specific design and require annotations not presented in our dataset [53, 50, 54] or were
showntoperformworseinotherstudiesthanthechosenbaselines[19,60]. Wealsodonotcompare
torelatedworkthathasnoofficialimplementationreleased[30,6].
4.3 Results
LinearClassification WereporttheperformanceofbothEMBEDBI-RADSanddensityclassifi-
cationtasksforeachbaselineinTab.1. WenoteMaMAachievesthebestperformanceoverallunder
differentamountsoftrainingdata. Ourmethodshowsanon-trivialimprovementofmorethan4%
ofbalancedaccuracyontheBI-RADSpredictiontaskwhenfine-tunedwithfulltrainingdata. We
notethatreducingtheamountoftrainingdatahasagreaterinfluenceontheBI-RADSprediction
taskthanthedensitypredictiontask,astheBI-RADSdistributionismoreimbalanced,e.g.,there
areonly6trainingimagesforBI-RADScategory5and2imagesforcategory6whenusing1%
trainingdata. However,ourmethodstillmaintainsthebestoverallperformanceevenwhentrained
withonly1%dataontheBI-RADSpredictiontask. Thisdemonstratesthestronggeneralization
abilityandrobustnessofMaMA.Evenifcomparingwithbaselinesalsowithlocalawareness[52,51],
ourmethodisstillthebest. WealsonoticethattheDiNOv2[37]-basedmodelstendtooutperform
theDeiT[48]-basedmodelsevenifusingthesameVLPmodeldesign. Thisisnotonlybecause
DiNOv2ViT[37]wastrainedonmoredata,butalsoduetotheuseofalargerimagesize,whichis
criticalforhigh-resolutionmammography.
Zero-shotClassification Wereportthezero-shotclassificationperformanceforeachofthemeth-
ods on both EMBED [21] tasks in Tab. 2. While our method still outperforms all the baselines,
we highlight the zero-shot performance of the BI-RADS score prediction task, where our model
outperformsthebestbaselineby∼5%intermsofbalancedaccuracyandmorethan7%inAUC
score. Comparedwithbaselinesusingthefullyfine-tunedsmallBioClinicalBERT[2],ourmethod
withpre-trainedLLMwithPEFTshowsmuchbetterzero-shotperformanceastheLLMcanprovide
atext-supervisedsignalwithhigherquality. Meanwhile,thePEFThelpstopreventtheLLMfrom
collapsing during fine-tuning. As a result, our LLM text encoder with PEFT can provide better
zero-shottextembeddingandimprovethezero-shotperformancegreatly. Meanwhile,wenotethat
theadoptedLLMwithPEFTencoderonlyhas2.6Mtrainableparameters,whichisonly3%ofthe
BioClinicalBERT[2]intermsofsize.
FullFine-tuningClassification Wealsoreporttheclassificationresultsafterfull-fine-tuningfor
EMBED[21]tasksinTab.2. Wenotethatwhilethegapbetweeneachmethodissomewhatreduced
duetofullfine-tuning,ourmodelstillbeatsallotherbaselinesonbothtasks.
Out-of-Domain Data Analysis We report performance of each method on the out-of-domain
RSNA-Mammo dataset in Tab. 3. Since RSNA-Mammo [4] is an extremely imbalanced dataset
(48:1negativetopositiveratio),wereportthesensitivityandspecificityaswell. Wenoteourmodel
8Table4:AblationofModelDesignWeablatedifferentmodeldesignsontheEMBED[21]BI-RADS
prediction task and report balanced accuracy (bACC) and AUC score. The best and second best
resultsarehighlightedinboldandunderlined,respectively. Ourfullmethodisshadedingray.
Methods EMBEDBI-RADS[21]
Zero-shot LinearClassification FullFine-tune
SLA Symm.LVT LVV PEFT-LLM
bACC(%) AUC(%) bACC(%) AUC(%) bACC(%) AUC(%)
✓ ✓ ✓ 29.28 71.16 38.71 77.50 30.55 70.69
✓ ✓ ✓ 31.03 72.79 39.57 77.39 39.47 76.23
✓ ✓ ✓ 27.32 70.18 37.21 77.95 23.78 63.97
✓ ✓ ✓ 23.88 62.84 38.96 77.43 22.29 63.77
✓ ✓ ✓ ✓ 31.04 74.83 39.75 77.50 40.31 77.36
Table5: Multi-viewAblationWeablatedifferent Table 6: Caption Ablation We ablate different
multi-viewcontrastivelearningstrategies. textcaptionconstructionstrategies.
EMBEDBI-RADS[21] EMBEDBI-RADS[21]
Methods Zero-shot LinearClassification FullFine-tune Methods Zero-shot LinearClassification FullFine-tune
bACC(%)AUC(%)bACC(%) AUC(%) bACC(%)AUC(%) bACC(%)AUC(%)bACC(%) AUC(%) bACC(%)AUC(%)
SameImage 30.48 73.95 39.70 77.73 39.35 76.44 CLIP-style 35.99 77.66 37.74 77.25 24.00 65.35
Intra-side 30.71 74.21 39.93 77.41 35.17 76.09 NoMetaMask 27.19 68.20 36.94 76.33 24.06 64.85
Intra-study 31.04 74.83 39.75 77.50 40.31 77.36 Struct.Cap. 31.04 74.83 39.75 77.50 40.31 77.36
performs best in terms of balanced accuracy and AUC with a notable gap. While some of the
baselinesoutperformourmodeloneitherthesensitivityorspecificitymetric,wenotethesemodels
arenotinformative,i.e.,theytendtocollapseandpredictthemajorityofimagestooneoftheclasses.
Thiswillleadtoahighscoreinoneofthesensitivityorspecificitymetricswhileresultinalow
performanceintheother. Incontrast,ourapproachshowsreasonableresultsforbothmetricsandis
theonlymethodwithbothsensitivityandspecificitygreaterthan60%underboththelinearandfull
fine-tuningsettings. Furthermore,theotherfewmethodsthatdemonstratedhighersensitivitythan
oursallresultedinaspecificityof∼45%orworse.
4.4 AblationExperiments
ModelDesign WeablatetheinfluenceofeachcomponentinTab.4.Comparedwiththesebaselines,
wenoteeachcomponenthasanimportantcontributiontotheoverallmodelperformance,asremoving
any one resulted in inferior performance. We note that the baseline without PEFT-LLM instead
employsBioClinicalBERT[2]andshowsacleardropinzero-shotperformance,whichvalidatesthe
importanceofusingaPEFT-LLM.However,thismodelstillperformswellonthelinearclassification
andfullfine-tuningtasks,whichdemonstratestheeffectivenessofourotherdesignchoices.
Multi-viewAblation Weablatethemulti-viewsamplingstrategyherebyusing: 1)thesameimage,
2)anintra-sideimage,and3)thecompleteintra-studyimage(Tab.5). Wecanseethatthemodel
trainedwithonlyoneimagelosesthemulti-viewunderstanding. Themodelusingonlyintra-side
imagesonlyconsidersipsilateralcorrespondenceandalsoresultsinaworseperformance.
Caption Ablation We evaluate the influence of using different caption construction strategies
in Tab. 6. We note that a CLIP style caption that only focuses on class labels shows a better
zero-shotperformance,butdegeneratesgreatlyinthelinearclassificationandfullfine-tuningtasks.
Meanwhile, if simply using the full meta-information during training, the model will fail with
zero-shotclassificationsinceitmaymainlyrelyonthemeta-informationduringthetrainingand
ignore more important clinical information. Our full design of using a structural caption with
meta-informationmaskingshowsthebestperformance.
5 DiscussionandConclusion
Inthiswork,wepresentedacompleteandnovelmulti-viewandmulti-scalealignmentcontrastive
language-imagepre-trainingmethodformammography. Weproposedutilizingthemulti-viewnature
ofmammographyandprovidinglocalimage-sentencecorrespondencetohelpaddressthechallenges
ofsmallROIsandhighimageresolutionandprovidefine-grainedvisualcluesfordecisions. The
proposedmethodgreatlyoutperformsmultipleexistingmedicalCLIPbaselines.
9LimitationandFutureWork Aswemainlyfocusonimagerepresentationlearning,wehaveyetto
evaluateotherdownstreamtaskslikeimage-textretrieval,objectdetection,andsegmentation. While
alsolimitedbyaccessibledatainthisdomain,ourmethodwillbeevaluatedonmoredownstream
tasksinfuturework. Weplantoextendthiscurrentframeworktomoremammographyimaging
modalitiesincludingC-viewanddigitalbreasttomosynthesistofurtherenhanceitsunderstandingof
mammography. Meanwhile,wealsoplantointegratethispre-trainedcomponentintoamulti-modal
question-answeringandgroundingmodel,tofurtherexplorethepotentialofmedicalVLP.
6 Acknowledgement
ThisworkwassupportedbyNIHgrantR21EB032950.
References
[1] AI@Meta. Llama3modelcard,2024. Accessed:2024-05-10.
[2] EmilyAlsentzer,JohnMurphy,WilliamBoag,Wei-HungWeng,DiJin,TristanNaumann,andMatthew
McDermott. PubliclyavailableclinicalBERTembeddings. InProceedingsofthe2ndClinicalNatural
LanguageProcessingWorkshop,pages72–78,Minneapolis,Minnesota,USA,June2019.Associationfor
ComputationalLinguistics.
[3] ElliotBolton,DavidHall,MichihiroYasunaga,TonyLee,ChrisManning,andPercyLiang. Biomedlm,
2023. Accessed:2023-03-02.
[4] ChrisCarrandYanChenet.al. Rsnascreeningmammographybreastcancerdetection,2022.
[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastivelearningofvisualrepresentations. InInternationalconferenceonmachinelearning,pages
1597–1607.PMLR,2020.
[6] Xuxin Chen, Yuheng Li, Mingzhe Hu, Ella Salari, Xiaoqian Chen, Richard LJ Qiu, Bin Zheng, and
XiaofengYang. Mammo-clip: Leveragingcontrastivelanguage-imagepre-training(clip)forenhanced
breastcancerdiagnosiswithmulti-viewmammography. arXivpreprintarXiv:2404.15946,2024.
[7] ZemingChen,AlejandroHernándezCano,AngelikaRomanou,AntoineBonnet,KyleMatoba,Francesco
Salvi,MatteoPagliardini,SiminFan,AndreasKöpf,AmirkeivanMohtashami,etal.Meditron-70b:Scaling
medicalpretrainingforlargelanguagemodels. arXivpreprintarXiv:2311.16079,2023.
[8] TimothéeDarcet,MaximeOquab,JulienMairal,andPiotrBojanowski. Visiontransformersneedregisters.
arXivpreprintarXiv:2309.16588,2023.
[9] AlexandrosDelitzas,MariaParelli,NikolasHars,GeorgiosVlassis,SotiriosAnagnostidis,GregorBach-
mann,andThomasHofmann. Multi-clip:Contrastivevision-languagepre-trainingforquestionanswering
tasksin3dscenes. arXivpreprintarXiv:2306.02329,2023.
[10] JiaDeng,WeiDong,RichardSocher,Li-JiaLi,KaiLi,andLiFei-Fei.Imagenet:Alarge-scalehierarchical
imagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,pages248–255.
Ieee,2009.
[11] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. Bert:Pre-trainingofdeepbidirec-
tionaltransformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
[12] JonDonnelly,LukeMoffett,AlinaJadeBarnett,HariTrivedi,FidesSchwartz,JosephLo,andCynthia
Rudin. Asymmirai:Interpretablemammography-baseddeeplearningmodelfor1–5-yearbreastcancer
riskprediction. Radiology,310(3):e232780,2024.
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal.Animageisworth
16x16words:Transformersforimagerecognitionatscale. arXivpreprintarXiv:2010.11929,2020.
[14] Yuexi Du, Regina J Hooley, John Lewin, and Nicha C Dvornek. Sift-dbt: Self-supervised initializa-
tion and fine-tuning forimbalanced digital breast tomosynthesis image classification. arXiv preprint
arXiv:2403.13148,2024.
[15] SedighehEslami,ChristophMeinel,andGerardDeMelo. Pubmedclip:Howmuchdoesclipbenefitvisual
questionansweringinthemedicaldomain? InFindingsoftheAssociationforComputationalLinguistics:
EACL2023,pages1181–1193,2023.
[16] Jean-BastienGrill,FlorianStrub,FlorentAltché,CorentinTallec,PierreRichemond,ElenaBuchatskaya,
CarlDoersch,BernardoAvilaPires,ZhaohanGuo,MohammadGheshlaghiAzar,etal. Bootstrapyour
ownlatent-anewapproachtoself-supervisedlearning. Advancesinneuralinformationprocessingsystems,
33:21271–21284,2020.
[17] KaimingHe,HaoqiFan,YuxinWu,SainingXie,andRossGirshick. Momentumcontrastforunsupervised
visualrepresentationlearning. arXivpreprintarXiv:1911.05722,2019.
10[18] EdwardJHu,YelongShen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen. Lora:Low-rankadaptationoflargelanguagemodels. arXivpreprintarXiv:2106.09685,
2021.
[19] Shih-ChengHuang,LiyueShen,MatthewPLungren,andSerenaYeung. Gloria:Amultimodalglobal-
localrepresentationlearningframeworkforlabel-efficientmedicalimagerecognition. InProceedingsof
theIEEE/CVFInternationalConferenceonComputerVision,pages3942–3951,2021.
[20] Jeremy Irvin, Pranav Rajpurkar, Michael Ko, Yifan Yu, Silviana Ciurea-Ilcus, Chris Chute, Henrik
Marklund,BehzadHaghgoo,RobynBall,KatieShpanskaya,etal. Chexpert:Alargechestradiograph
datasetwithuncertaintylabelsandexpertcomparison. InProceedingsoftheAAAIconferenceonartificial
intelligence,volume33,pages590–597,2019.
[21] JiwoongJJeong,BriannaLVey,AnanthBhimireddy,ThomasKim,ThiagoSantos,RamonCorrea,Raman
Dutt,MarinaMosunjac,GabrielaOprea-Ilies,GeoffreySmith,etal. Theemorybreastimagingdataset
(embed): Araciallydiverse, granulardatasetof3.4millionscreeninganddiagnosticmammographic
images. Radiology:ArtificialIntelligence,5(1):e220047,2023.
[22] AlistairEWJohnson,TomJPollard,NathanielRGreenbaum,MatthewPLungren,Chih-yingDeng,Yifan
Peng,ZhiyongLu,RogerGMark,SethJBerkowitz,andStevenHorng. Mimic-cxr-jpg,alargepublicly
availabledatabaseoflabeledchestradiographs. arXivpreprintarXiv:1901.07042,2019.
[23] AlistairEWJohnson,TomJPollard,LuShen,Li-weiHLehman,MenglingFeng,MohammadGhassemi,
BenjaminMoody,PeterSzolovits,LeoAnthonyCeli,andRogerGMark. Mimic-iii,afreelyaccessible
criticalcaredatabase. Scientificdata,3(1):1–9,2016.
[24] ZhengfengLai,ZhuohengLi,LucaCernyOliveira,JoohiChauhan,BrittanyNDugger,andChen-Nee
Chuah. Clipath:Fine-tuneclipwithvisualfeaturefusionforpathologyimageanalysistowardsminimizing
datacollectionefforts. InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision,
pages2374–2380,2023.
[25] JunnanLi,DongxuLi,SilvioSavarese,andStevenHoi.Blip-2:Bootstrappinglanguage-imagepre-training
withfrozenimageencodersandlargelanguagemodels. InInternationalconferenceonmachinelearning,
pages19730–19742.PMLR,2023.
[26] JunnanLi,DongxuLi,CaimingXiong,andStevenHoi. Blip:Bootstrappinglanguage-imagepre-training
forunifiedvision-languageunderstandingandgeneration.InInternationalconferenceonmachinelearning,
pages12888–12900.PMLR,2022.
[27] YangguangLi,FengLiang,LichenZhao,YufengCui,WanliOuyang,JingShao,FengweiYu,andJunjie
Yan. Supervisionexistseverywhere:Adataefficientcontrastivelanguage-imagepre-trainingparadigm.
arXivpreprintarXiv:2110.05208,2021.
[28] Zheren Li, Zhiming Cui, Sheng Wang, Yuji Qi, Xi Ouyang, Qitian Chen, Yuezhi Yang, Zhong Xue,
DinggangShen,andJie-ZhiCheng. Domaingeneralizationformammographydetectionviamulti-style
andmulti-viewcontrastivelearning. InMedicalImageComputingandComputerAssistedIntervention–
MICCAI 2021: 24th International Conference, Strasbourg, France, September 27–October 1, 2021,
Proceedings,PartVII24,pages98–108.Springer,2021.
[29] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee.Visualinstructiontuning.Advancesinneural
informationprocessingsystems,36,2024.
[30] Jiarun Liu, Hong-Yu Zhou, Cheng Li, Weijian Huang, Hao Yang, Yong Liang, and Shanshan Wang.
Mlip: Medicallanguage-imagepre-trainingwithmaskedlocalrepresentationlearning. arXivpreprint
arXiv:2401.01591,2024.
[31] ShilongLiu,ZhaoyangZeng,TianheRen,FengLi,HaoZhang,JieYang,ChunyuanLi,JianweiYang,
HangSu,JunZhu,etal. Groundingdino:Marryingdinowithgroundedpre-trainingforopen-setobject
detection. arXivpreprintarXiv:2303.05499,2023.
[32] YuhangLiu, FandongZhang, ChaoqiChen, SiwenWang, YizhouWang, andYizhouYu. Actlikea
radiologist:towardsreliablemulti-viewcorrespondencereasoningformammogrammassdetection. IEEE
TransactionsonPatternAnalysisandMachineIntelligence,44(10):5947–5961,2021.
[33] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017.
[34] NormanMu,AlexanderKirillov,DavidWagner,andSainingXie. Slip:Self-supervisionmeetslanguage-
imagepre-training. InEuropeanconferenceoncomputervision,pages529–544.Springer,2022.
[35] HQNguyen,HHPham,LTLe,MDao,andKVinDr-CXRLam. Anopendatasetofchestx-rayswith
radiologistannotations. PhysioNethttps://doi.org/10.13026/3akn-b287,2021.
[36] MichaelOnken,MarcoEichelberg,JörgRiesmeier,andPeterJensch.Digitalimagingandcommunications
inmedicine. InBiomedicalImageProcessing,pages427–454.Springer,2010.
[37] MaximeOquab,TimothéeDarcet,TheoMoutakanni,HuyV.Vo,MarcSzafraniec,VasilKhalidov,Pierre
Fernandez,DanielHaziza,FranciscoMassa,AlaaeldinEl-Nouby,RussellHowes,Po-YaoHuang,Hu
Xu,VasuSharma,Shang-WenLi,WojciechGaluba,MikeRabbat,MidoAssran,NicolasBallas,Gabriel
11Synnaeve,IshanMisra,HerveJegou,JulienMairal,PatrickLabatut,ArmandJoulin,andPiotrBojanowski.
Dinov2:Learningrobustvisualfeatureswithoutsupervision,2023.
[38] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,Girish
Sastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InInternationalconferenceonmachinelearning,pages8748–8763.PMLR,
2021.
[39] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever,etal. Language
modelsareunsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
[40] KetulShah,AnshulShah,ChunPongLau,CelsoMdeMelo,andRamaChellappa. Multi-viewaction
recognitionusingcontrastivelearning.InProceedingsoftheIEEE/CVFWinterConferenceonApplications
ofComputerVision,pages3381–3391,2023.
[41] E.A.Sickles,C.J.D’Orsi,L.W.Bassett,etal. ACRBI-RADSmammography. InACRBI-RADSAtlas,
BreastImagingReportingandDataSystem.AmericanCollegeofRadiology,Reston,VA,5thedition,
2013.
[42] DanSong,XinweiFu,WeizhiNie,WenhuiLi,andAnanLiu. Mv-clip:Multi-viewclipforzero-shot3d
shaperecognition. arXivpreprintarXiv:2311.18402,2023.
[43] LileiSun, JieWen, JunqianWang, ZhengZhang, YongZhao, GuiyingZhang, andYongXu. Breast
mass classification based on supervised contrastive learning and multi-view consistency penalty on
mammography. IETBiometrics,11(6):588–600,2022.
[44] QuanSun,YuxinFang,LedellWu,XinlongWang,andYueCao. Eva-clip:Improvedtrainingtechniques
forclipatscale. arXivpreprintarXiv:2303.15389,2023.
[45] QuanSun,YuxinFang,LedellWu,XinlongWang,andYueCao. Eva-clip:Improvedtrainingtechniques
forclipatscale. arXivpreprintarXiv:2303.15389,2023.
[46] QuanSun,JinshengWang,QiyingYu,YufengCui,FanZhang,XiaosongZhang,andXinlongWang.
Eva-clip-18b:Scalingclipto18billionparameters. arXivpreprintarXiv:2402.04252,2024.
[47] HyunaSung,JacquesFerlay,RebeccaLSiegel,MathieuLaversanne,IsabelleSoerjomataram,Ahmedin
Jemal,andFreddieBray. Globalcancerstatistics2020:Globocanestimatesofincidenceandmortality
worldwidefor36cancersin185countries. CA:acancerjournalforclinicians,71(3):209–249,2021.
[48] HugoTouvron,MatthieuCord,MatthijsDouze,FranciscoMassa,AlexandreSablayrolles,andHerve
Jegou. Trainingdata-efficientimagetransformersanddistillationthroughattention. InMarinaMeilaand
TongZhang,editors,Proceedingsofthe38thInternationalConferenceonMachineLearning,volume139
ofProceedingsofMachineLearningResearch,pages10347–10357.PMLR,18–24Jul2021.
[49] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov, SoumyaBatra, PrajjwalBhargava, ShrutiBhosale, etal. Llama2: Openfoundationand
fine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[50] Zhongwei Wan, Che Liu, Mi Zhang, Jie Fu, Benyou Wang, Sibo Cheng, Lei Ma, César Quilodrán-
Casas,andRossellaArcucci. Med-unic:Unifyingcross-lingualmedicalvision-languagepre-trainingby
diminishingbias. AdvancesinNeuralInformationProcessingSystems,36,2024.
[51] FuyingWang,YuyinZhou,ShujunWang,VarutVardhanabhuti,andLequanYu. Multi-granularitycross-
modalalignmentforgeneralizedmedicalvisualrepresentationlearning. AdvancesinNeuralInformation
ProcessingSystems,35:33536–33549,2022.
[52] PeiqiWang,WilliamMWells,SethBerkowitz,StevenHorng,andPolinaGolland.Usingmultipleinstance
learningtobuildmultimodalrepresentations. InInternationalConferenceonInformationProcessingin
MedicalImaging,pages457–470.Springer,2023.
[53] ZifengWang, ZhenbangWu, DineshAgarwal, andJimengSun. Medclip: Contrastivelearningfrom
unpairedmedicalimagesandtext. arXivpreprintarXiv:2210.10163,2022.
[54] ChaoyiWu,XiaomanZhang,YaZhang,YanfengWang,andWeidiXie. Medklip:Medicalknowledge
enhancedlanguage-imagepre-training. medRxiv,pages2023–01,2023.
[55] ShengZhang,YanboXu,NaotoUsuyama,HanwenXu,JaspreetBagga,RobertTinn,SamPreston,Rajesh
Rao,MuWei,NaveenValluri,etal. Biomedclip:amultimodalbiomedicalfoundationmodelpretrained
fromfifteenmillionscientificimage-textpairs. arXivpreprintarXiv:2303.00915,2023.
[56] YuhaoZhang,HangJiang,YasuhideMiura,ChristopherDManning,andCurtisPLanglotz. Contrastive
learningofmedicalvisualrepresentationsfrompairedimagesandtext.InMachineLearningforHealthcare
Conference,pages2–25.PMLR,2022.
[57] Yichi Zhang, Ziqiao Ma, Xiaofeng Gao, Suhaila Shakiah, Qiaozi Gao, and Joyce Chai. Groundhog:
Groundinglargelanguagemodelstoholisticsegmentation. arXivpreprintarXiv:2402.16846,2024.
[58] ZihaoZhao,YuxiaoLiu,HanWu,YonghaoLi,ShengWang,LinTeng,DishengLiu,XiangLi,Zhiming
Cui,QianWang,etal.Clipinmedicalimaging:Acomprehensivesurvey.arXivpreprintarXiv:2312.07353,
2023.
12[59] KechengZheng,YifeiZhang,WeiWu,FanLu,ShuaileiMa,XinJin,WeiChen,andYujunShen.Dreamlip:
Language-imagepre-trainingwithlongcaptions. arXivpreprintarXiv:2403.17007,2024.
[60] Hong-YuZhou,ChenyuLian,LianshengWang,andYizhouYu. Advancingradiographrepresentation
learningwithmaskedrecordmodeling. arXivpreprintarXiv:2301.13155,2023.
13A Appendix
Intheappendix,weprovidemoredetailedtrainingsettings,evaluationsettings,modelconfigurations,
andadditionalanalysis.
A.1 FurtherLimitations
The EMBED data comes from the Atlanta, GA region. While the dataset is highly ethnically
diverse,thegeographicfocuscouldlimitgeneralizabilitytootherpopulations,e.g.,thebreastdensity
distributionmaydifferfromdatagatheredinotherregionsoftheworld. Additionally,Thecaption
iscreatedfromthetemplated-basedmethod,whichmaypotentiallyharmthemodelduetolimited
captiondiversity. Futureworksmayconsideraugmentthetemplate-basedpromptwithLLM.
A.2 BroaderImpacts
Thispaperproposedapromisingvisuallanguagepre-trainingschemeformammographythatcanbe
usedforvariousdownstreamtasks. Itcanalsopotentiallyspeedupthereal-worldmammography
screeningordiagnosticprocessbyfilteringoutlow-riskstudiesandhighlightinghigh-riskimagesfor
theclinician. WhiletheEMBEDdatasetisoneofthelargestandmostdiversepublicmammography
datasets available, it is notable that the data were collected from four specific hospitals and thus
thetrainedmodelmayhaveaspecificbiastowardsaspecificgroupofpeopleduetotrainingdata
composition. Anyuserwhowantstousethismodelintheirownresearchmayneedtocarefully
analyzesuchbiasandtheirownapplicationandtasksandavoidusingthemodelinreal-worldclinical
trialswithoutfurtherapproval.
Algorithm1SLALossPseudocode
1: # fp, fs: local patch, sentence projectors
2: # N, tau_local: batch size and SLA loss temperature
3: # patch_feats: patch-wise image feature. (N, num_patch, C)
4: # sent_feats: sentence-wise text feature. list of N tensors, (num_sent, C)
5: def SLA_loss(patch_feats, sent_feats):
6: t2v_scores = []# cV: visual localization correspondence
7: v2t_scores = []# cT: textual localization correspondence
8: patch_feats = normalize(fp(patch_feats))
9: # Each report may have different num_sent
10: for sent in sent_feats:
11: sent = normalize(fs(sent))
12: score = torch.bmm(path_feats, sent.T)# (N, num_patch, num_sent)
13: # Visual localization: Max over patches + Avg over sentences
14: t2v_scores.append(score.max(dim=1, keepdim=True).mean(dim=2).squeeze())
15: # Textual localization: Max over sentences + Avg over patches
16: v2t_scores.append(score.max(dim=2, keepdim=True).mean(dim=1).squeeze())
17: t2v_scores = torch.stack(t2v_scores, dim=0) / tau_local# (N, N)
18: v2t_scores = torch.stack(v2t_scores, dim=0) / tau_local# (N, N)
19: labels = torch.arange(N)
20: loss0 = cross_entropy(t2v_scores, labels)
21: loss1 = cross_entropy(v2t_scores, labels)
22: return 0.5 * (loss0 + loss1)
A.3 Pseudo-CodeofSLAModule
Weprovidethepytorchpseudo-codeoftheSLAmoduleintheAlgorithm1tobetterillustratethe
designoftheSLAmodule.
A.4 Pre-trainingImplementationDetails
DatasetandPre-processing AsmentionedinSec.4.1,weusetheEMBED[21]datasetforpre-
training. Weonlyusethe2Dmammographyandsplitthedatasetinto70%/10%/20%fortraining,
validation, and testing at the patient level. We filter out the studies for males or those that have
missingBI-RADSordensitylabels. WeprovidethedetaileddistributionofBI-RADSscoreand
14Table7: ModelTrainableParametersWeprovidethenumberoftrainableparametersforeach
modelherebelow. Ourmethodasdescribedinthemainpaperisshadedingray.
#TrainableParameters(M)
Models
VisualEncoder LanguageEncoder Total
Visiononly
Random-ViT[13] 89.6 - 86.6
DiNOv2-ViT[37] 89.6 - 86.6
DeiT-based[48]
CLIP[38] 86.6 84.6 172.5
ConVIRT[56] 86.6 84.6 173.2
MGCA[51] 86.6 84.6 174.4
DiNOv2-based[37]
CLIP[38] 89.6 84.6 174.5
SLIP[34] 89.6 84.6 174.8
MM-MIL[52] 89.6 84.6 174.9
ConVIRT[56] 89.6 84.6 176.2
MGCA[51] 89.6 84.6 177.4
MaMA-BioClinicalBERT[2] 89.6 84.6 177.5
MaMA-LoRA-BioMedLM[18,3] 89.6 2.6 92.8
MaMA-LoRA-Meditron[18,7] 89.6 4.2 94.3
MaMA-LoRA-Llama3[18,1] 89.6 3.4 93.4
Breast density in Fig. 3, displaying the extremely imbalanced labels. Each of the sampled splits
sharesroughlythesamedistribution. Moredetailsaboutthedatasetcanbefoundin[21]. Forthe
datapre-processing,wefirstconverteachoriginalDICOMimagefiletoJPEGformatandresizethe
imagebasedonitslongsideto1,024pixelswithoutchangingitsaspectratio. Theseimagesarethen
useddirectlyfortraining.
Pre-trainingDataAugmentation DifferentfromCLIP[39],weuseastrongdataaugmentation
duringthepre-trainingstageforbothimages. WefirstapplytheOTSUthresholdmaskingtocutthe
unnecessarybackgroundregionsandonlykeepthebreasttissue. Thisimageisthenresizedto518
pixelsonitslongsideandpaddedwithzerosontheshortsidetohaveasquareshapeof518,×518.
WethenapplySimCLR[5]styleaugmentationincludingrandomhorizontalandverticalflips,color
jitter,grayscale,andGaussianblur. Duringtesttime,weonlykeeptheresizeoperationanddropall
randomaugmentations.
ModelDetails AsmentionedinSec.4.1,weuseDiNOv2pre-trainedViT-B-reg[13,8]modelwith
imagesize518andpatchsize14asourvisualencoder. WeuseBioMedLM[3],a3MlevelGPT-2
decoder-onlytransformerof32layersasourlanguageencoder. WeadaptLoRA[18]tofine-tune
this encoder. As for the baselines, we choose to experiment with both a DeiT [48]-based and a
DiNOv2-basedvisualencoder. TheDeiT-basedtransformerwaspre-trainedwithapatchsizeof16
andimagesizeof384onImageNet[10]. Theinputforthecorrespondingbaselinesisresizedto384
aswell. FortheDiNOv2[37]visualencoderforthebaselines,thesettingisthesameasourmodel.
AllthebaselinesuseBioClinicalBERT[2],aBERT-styleencoder-onlytransformerwithoutPEFT.
EMBED Dataset Distribution
Figure3: DataDistributionofEMBED[21]Dataset. Wevisualizethedatadistributionofthe
EMBED[21]datasetforbothBI-RADSandDensitylabels.
15Full Fine-tuned Model Confusion Matrix
Figure4: ConfusionMatrixofOurFullFine-tunedModel. Wevisualizetheclass-wiseconfusion
matrixofourmodelfullyfine-tunedwithBI-RADSanddensityclassificationtasks,respectively.
WeusetheonlineimplementationforConVIRT[56]andMGCA[51]2andadjustthevisionencoder
part,andwere-implementtheCLIP[39],SLIP[34],andMM-MIL[52]followingthecorresponding
papersunderourenvironment. WeprovidethemodelsizecomparisoninTab.7. Wecaneasilysee
thatourmodelhasthesmallestnumberoftrainableparameters,only∼52%comparedwithother
baselines. Wechoosetousethelastcheckpointforallmodelsindownstreamevaluations.
PEFT Settings As for the parameter-efficient fine-tuning (PEFT) module, we use the LoRA
implementedbyHuggingFacewithdefaulthyperparameters: r = 8,α = 32,dropout = 0.1. We
choose to use LoRA as it is one of the most popular PEFT methods and has been proven to be
effectiveinpriorresearch.
OverallPre-trainingTarget Asasupplementtothemethodsection,wehereprovidetheoverall
pre-trainingoptimizationtargetinEq.(4).
L(x ,v˜,t )=L (v ,v˜)+L (v ,t )+L (v˜,t )+wL . (4)
i i i VV i i VT i i VT i i local
Wesetw =0.0inthefirst8,000stepsoftrainingandw =1.0inthelatterprocess.
A.5 DownstreamEvaluationDetails
Zero-shotCaption Duringzero-shotevaluation,weprependthemeta-informationtotheclass-wise
descriptionsentence,sincethismeta-informationcanbereadilyobtainedwiththeimageswithout
needingtheclinician’sdiagnosis.Morespecifically,weprependtheinformationincluding:Procedure
reported,Reasonforprocedure,Patientinfo,andImageinfobeforetheclassdescriptionsentence
ofeachBI-RADSordensityclass. Thisimprovesthezero-shotbalancedaccuracyoftheBI-RADS
classificationfrom29.65%to31.04%andimprovesthecorrespondingAUCfrom68.05%to74.83%.
LinearClassifier Weattachalinearclassifiertoeachofthebaselinemodelsforlinearclassification
andfullfine-tunedtasks. Thelinearclassifierusestheaverageofallpatchtokensasinputratherthan
using[CLS]tokensincethe[CLS]tokenisnotusedduringtrainingaswell. Weusethefulltraining
setandbalancedweightedsamplingduringtrainingforallthelinearclassificationandfine-tuning
experiments.
BI-RADS Prediction For EMBED [21] BI-RADS score prediction task, we sample 10% data
randomlyfromthetestset. However,weaddedmoreimagesforBI-RADSscores5and6toensure
these2classesatleasthave200images. Thisistoavoidbiasduetolimitedevaluationsamples. The
finaldistributionofthisdatasetis: [901,4472,1166,517,210,200,200]forBI-RADSscoresfrom0
to6respectively. Thepre-processingisthesameasdescribedinAppendixA.4.
2https://github.com/HKU-MedAI/MGCA
16Table8: AblationwithDifferentMetaMaskingRatioonEMBEDBI-RADS[21]Weevaluate
theinfluenceofusingdifferentmeta-maskingratiosontheinputtextduringpre-trainingandtestthe
modelonzero-shotsettings. Ourmethodasdescribedinthemainpaperisshadedingray.
ModelSettings bACC(%) AUC(%)
m=0.0 27.19 68.20
m=0.2 29.52 71.23
m=0.5 30.37 72.44
m=0.2 31.04 74.83
Table9: AblationwithDifferentVisualContrastiveLearningStyleonEMBED[21]Weevaluate
theinfluenceofusingdifferentvisualcontrastivepre-trainingschemes. Weevaluatethezero-shot
andlinearclassificationperformanceforeachmethod. Ourmethodasdescribedinthemainpaperis
shadedingray.
EMBED[21]BI-RADS EMBED[21]Density
ModelSettings
Zero-shot Linearclassification Zero-shot LinearProbing
bACC(%) AUC(%) bACC(%) AUC(%) bACC(%) AUC(%) bACC(%) AUC(%)
SimCLR[5]style 31.04 74.83 39.75 77.50 75.40 93.46 78.09 93.65
MoCo[17]style 29.04 74.67 36.74 78.16 76.18 92.58 78.03 93.49
DensityPrediction SimilartoBI-RADSprediction,werandomlysampleanother10%datafrom
thetestsetstratifiedbydensitylabeltocreatethedensitypredictionset. Thedistributionofthis
testsetis: [738,3103,3043,417]fordensityfrom1to4. Weusethefulltrainingsetandbalanced
weightedsamplingduringtraining.
RSNA-Mammo[4]CancerDetection SimilartoEMBEDpre-processing,weconverttheDICOM
mammographytoaJPEGimageandresizeitslongsideto1,024withoutchangingtheaspectratio.
Sincethisdatasetdoesnotprovidethecorrespondingmeta-information,weonlyevaluatethelinear
classification and full fine-tuning tasks. We use the full 15% test set for the RSNA-Mammo [4]
evaluation,wherethedistributionofthistestsetis[7979,208]fornormalandcanceroussamples,
respectively. Weusethefulltrainingsetandbalancedweightedsamplingduringtraining.
A.6 ClassificationResultsAnalysis
Wevisualizetheconfusionmatrixforclassificationresultsofthefullyfine-tunedmodelonboth
EMBED[21]predictiontasksinFig.4. WhiletheoverallaccuracyfortheBI-RADSpredictiontask
stillneedsimprovement,wenotethatthemisclassificationmainlyhappensforBI-RADScategories
2,3,and4,whichisreasonablesincetheseclassesaresemanticallyclosetoeachother(“Benign”,
“ProbablyBenign”,and“SuspiciousAbnormality”). Meanwhile,wenoteourmodelshowsahigh
recall for BI-RADS category 6, i.e., “Known biopsy-proven malignancy”, which indicates the
potentialapplicationofthemodeltofilterouthigh-riskabnormalmammographyquickly.
Misclassificationsforthedensitypredictionsarealsoreasonable,asmammographicdensityincreases
withthehigherdensityclasslabel. Notably,mosterrorsforthemiddletwodensityclassesarefor
themoreextremeversionofthatclass(e.g.,3correspondingto"heterogeneouslydense"ismore
oftenmistakenfor4"extremelydense"comparedto2"scattereddensity");thusthebinarydense
(labels3/4)andnon-dense(labels1/2)predictiondoeswell. Thisisimportantaswomenwithdense
breastsarerequiredtobenotifiedbyUSregulations,andthishasramificationsforpotentialfollow-up
screeningrecommendations.
A.7 AdditionalAblationExperiments
MetaMaskingRatio Tobetterunderstandtheinfluenceofmaskingthemeta-information,wehere
provide an extra zero-shot evaluation on different mask ratios m during the pre-training stage in
Tab.8. Asshownabove,thezero-shotperformanceincreasesasthemeta-informationmaskingratio
increases,whichmeansthemodeltendstorelymoreonclinical-relatedinformation,andtherefore,
doesbetterinthezero-shotclassificationtask.
17Table10: AblationwithDifferentMulti-viewContrastiveLearningProbabilityonEMBED[21]
Weevaluatetheinfluenceofusingdifferentmulti-viewcontrastivelearningprobabilitiesponEMBED
BI-RADSprediction. Weevaluatethezero-shotandlinearclassificationperformanceforeachpre-
trainedmodel. Ourmethodasdescribedinthemainpaperisshadedingray.
EMBED[21]BI-RADS
ModelSettings
Zero-shot LinearProbing
bACC(%) AUC(%) bACC(%) AUC(%)
p=0.0 30.48 73.95 39.70 77.23
p=0.2 30.26 73.35 39.37 77.50
p=0.5 31.04 74.83 39.75 77.50
p=0.8 30.76 74.26 39.41 77.45
p=1.0 29.33 73.21 38.20 77.49
Table11: ComparisonwithMedicalPre-trainedVisualEncoderonEMBED[21]Wecompare
ourmethodwithSimCLR[5]pre-trainedvisualencoderontheEMEBD[21]datasetunderlinear
classificationsettings. Ourmethodasdescribedinthemainpaperisshadedingray.
EMBED[21]BIRADS EMBEDDensity
ModelSettings
bACC(%) AUC(%) bACC(%) AUC(%)
SimCLRPre-trained 26.19 65.06 77.06 92.64
MaMA 39.75 77.50 78.09 93.65
Different Visual Contrastive Learning Scheme We here provide additional analysis of the
influence of using different visual contrastive learning schemes by comparing a variation of the
proposed model, i.e., MoCo-style image-to-image contrastive loss [17], where a memory queue
ofsize4096isusedtostorethenegativesamplesduringpre-training. Thiscanproperlyaddress
the sensitivity of the image-to-image contrastive loss to the batch size, as there will always be a
largenumberofnegativeexamplesduringpre-training(seeTab. 2in [17],whereabatchsizeof
256wassufficient). Here,weprovideacomparisonbetweentheproposedmethod(SimCLRstyle
image-to-imageloss)andMoCo-stylevariationinTab.9.
We note that there is no clear difference between the two models. The chosen SimCLR method
is slightly better from a general perspective. This result potentially suggests that the batch size
maynotbe thatimportantinourtask, orthattheused batchsizewaslargeenough. Weprovide
two possible explanations for this result: 1) Different from natural images, where the difference
betweeneachsampleisfairlylarge,theinter-sampledifferenceformammogramsismuchsmaller.
Mammographygenerallyhasverysimilarglobalcontent. Thus,fewernegativesamplesaresufficient
toprovidearobustcontrastivesignalduringimage-to-imagecontrastivepre-training. 2)Apartfrom
theimage-to-imageloss,thesymmetricimage-to-textlossbetweenthecaptionandtwoimagesalso
indirectlyminimizesthedistancebetweenthetwoimages,whichhelpsalleviatethenecessityofa
largebatchsize.
DifferentMulti-viewProbability Additionally,wehereprovidemoreanalysisonthemulti-view
samplingstrategy. Weadjusttheprobabilityofusingintra-studysamplingandtheaugmentedviewof
thesameimageastheextraimagex˜ ,whichisp=0.5intheproposedmethod. Whenp=0.0,the
i
modelalwayssamplesthesameaugmentedimageastheotherviewduringpre-training(equivalent
tothe"SingleImage"baselineinTab. 5). Incontrast, whenp = 1.0, themodelalwayssamples
oneoftheotherimagesfromthesamestudyastheotherview. Wehereprovidetheresultsofthe
Zero-shotandLinearclassificationBI-RADSpredictionevaluationinTab.10. Itisclearthateither
usingnointer-studysampling(p=0.0)orusingonlythemulti-viewsampling(p=1.0)willharm
theperformance. Anequal-weightmixofbothsamplingmethodsshowsthebestperformance,asit
providesamorediversecontrastiveimageandreducesthepotentialcontradictoryimagepairs(by
usingtheaugmentedviewofthesameimage).
VisualConstrastiveOnlyBaseline Wehereincludethelinearclassificationresultsincomparison
totheViTbaselinepre-trainedwiththeSimCLR[5]methodontheEMBEDdatasetinTab.11. The
vision-onlypre-trainedmodelperformsworsecomparedwithourmethodaccordingtotheresults.
18Table12: LinearClassificationResultsonEMBED[21]forDifferentTextEncoderWeevaluate
linearclassificationresultswithdifferentamountsoffine-tuningdataforbothBI-RADSanddensity
predictiontasksofourmodelwithdifferenttextencoder. AllmethodsarebasedonDiNOv2[37]
visionencoderforafaircomparison. Wereportbothbalancedaccuracy(bACC)andAUCmetrics.
Thebestandsecond-bestresultsarehighlightedinboldandunderlinedrespectively. Ourmethodas
describedinthemainpaperisshadedingray.
EMBEDBI-RADS[21] EMBEDDensity[21]
Models
bACC(%) AUC(%) bACC(%) AUC(%)
1% 10% 100% 1% 10% 100% 1% 10% 100% 1% 10% 100%
BioClinicalBERT-based[2]
CLIP[38] 26.66 31.65 34.35 70.35 74.98 74.11 74.64 75.00 75.97 91.50 90.62 92.39
SLIP[34] 22.94 27.86 30.93 64.43 69.48 71.95 73.24 74.79 75.23 91.56 92.37 92.46
MM-MIL[52] 25.85 30.94 35.11 67.16 71.99 76.12 74.23 76.69 75.77 91.96 93.34 91.65
ConVIRT[56] 24.62 30.38 31.27 65.09 73.33 74.03 74.34 74.95 74.74 92.21 92.56 92.58
MGCA[51] 23.66 30.11 30.27 64.19 72.24 72.54 71.43 72.25 72.20 90.83 91.21 91.24
MaMA-BERT 27.81 34.25 38.96 68.99 74.61 77.43 74.77 77.50 78.15 92.90 93.50 93.68
LoRA-LLM-based[18]
MaMA-BioMedLM 28.46 35.12 39.75 70.63 75.98 77.50 76.26 78.11 78.09 93.11 93.62 93.65
MaMA-Meditron 26.94 33.28 38.68 68.93 74.45 77.51 74.48 77.77 78.30 92.65 93.54 93.66
MaMA-Llama3 28.00 34.30 39.99 70.83 75.47 77.50 74.70 77.93 78.13 93.02 93.70 93.72
A.8 BenchmarkDifferentTextEncoders
WeevaluateallmethodswiththesameDiNOv2[37]visionencodersbutcomparetheinfluenceof
usingdifferenttextencodersinTab.12.
TextEncoders 1)BioClinicalBERT[2]:ThestandardtextencoderusedforpreviousmedicalCLIP
models[52,56,19,51,50]andalsoourbaselinemethods,whichisaBERT[11]-styletransformer
pre-trained with MIMIC-III [23] clinical report. 2) BioMedLM [3]: A 2.7B level GPT-2 [39]
transformer pre-trained with PubMed data, which is also one of the best 3B LLM according to
multiplebenchmarks[7]. 3)Meditron-7B[7]: AnewlyreleasedLlama2[49]modelfine-tunedwith
PubMedpapers. 4)Llama3-8B[1]: Recentlyreleased, themostrobustopen-soucedLLM,with
roughlythesamearchitectureasLlama2[49]butpre-trainedwithmuchmoredata. Allthelatter
threeLLMsarefine-tunedwithLoRA[18]
Results WereporttheresultsonlinearclassificationinTab.12. Wenotethatevenourmodelwith
BioClinicalBERT[2]textencoderoutperformsallthebaselinesinthisevaluation;thisdemonstrates
theeffectivenessoftheproposedmulti-viewmammographypre-trainingandsymmetriclocalalign-
mentmodule. ComparingthreedifferentLLMswithLoRA[18],wenotethatBioMedLM[3]and
Llama3-8B[1]roughlyhaveasimilarlevelofperformance,whiletheBioMedLM-basedmodelhasa
smallerGPUmemorycostandfastertrainingspeedduetoitsrelativesize. Meanwhile,wenotice
thattheMeditron[7]-basedmodelisnotasgoodastheothertwoLLMs,butalltheseLLM-based
methodsoutperformthemodelwithsmallerBERT-style[11]encoderingeneral. Overall,ourchoice
ofBioMedLM[3]-basedmodelhasthebestbalancebetweenperformanceandmodelsize.
A.9 LocalSimilarityMapAnalysis
Wevisualizethelearnedlocalpatch-sentencesimilaritymapinFig.5. AsdescribedinSec.3.3,the
localpatch-sentencesimilaritymapindicatestherelationshipbetweeneachregionoftheimageand
thecorrespondinginputsentence. Wevisualizethesimilaritymapforthe“Impression”sentencein
thereport(seeexamplesinFig.6toFig.8),whichincludesthemostimportantdiagnosisinformation.
We also visualize the same similarity map for MM-MIL [52] and a variation of our method that
optimizes local similarity with only visual localization (similar to including the MM-MIL local
branch).
Wenotethatourmethodsgenerallyhaveabetterlocalizationqualitywithmorefine-graineddetails.
The model can accurately locate the high-density and tumor-related regions in the given maps.
Wealsoseefromtheexamplesforpatients3and4thatourmethodhasabettercorrespondence
betweenmammogramsfromdifferentviewsorsides. Especiallyforcolumn3,ourmethodaccurately
identifiedthesameregioninbothviews,whilethebaselinemethodfailedtolocatethetissueinthe
RMLOview(leftimage). TheMM-MIL[52]modelevenfailedtodetectthetumorforpatient4.
Ontheotherhand,thevariationofourmodelthatoptimizesonlyvisuallocalizationlosscanonly
19Patient #1 Patient #2 Patient #3 (two views) Patient #4 (two sides)
Figure5: VisualizationofLocalSimilarityMapsoverInputMammograms. Wevisualizethe
learnedlocalsimilaritymapforthe“Impressions”sentenceonafewtestmammogramsfromthe
EMBED dataset [21] for MM-MIL [52], our method with only visual localization, and our full
methodhere. Alltheheatmapsarenormalizedto[0,1]. Thethirdcolumnshowsmammogramsfrom
thesamesidebutadifferentviewandthefourthcolumnshowsmammogramsfromthesameviewbut
fromadifferentside. ThewhiteboxintheimagerepresentstheROIannotatedfromthedataset[21].
Table 13: Zero-shot Visual Grounding Analysis We report the mean intersection-over-union
(mIoU),meanDICEscore,andROIrecallwith50%coverageformethodswithlocalsentence-region
similaritymapontheEMBED[21]dataset. Ourmethodisshadedingray.
EMBED[21]VisualGrounding
Models
mIoU(%) mDICE(%) Recall(%)
MM-MIL[52] 5.25 9.72 39.23
MaMA 6.22 11.88 47.67
provideavagueandinaccuratesimilaritymap. Webelievethisisbecausetheasymmetricmaxand
averagepoolingoperationdropstoomuchinformationduringtraining,resultinginonlyoneofthe
patchesbeingoptimized.
QuantitativeVisualGroundingAnalysis SimilartotheanalysisinMM-MIL[52],wefurther
conductazero-shotvisualgroundinganalysiswiththepre-trainedmodel. Wecomparethesimilarity
mapextractedfortheimageandthe“Impressions”descriptionwiththeprovidedROIsfromasubset
oftheEMBED[21]dataset,whichcontains841imagesfromthetestsplit,eachwithoneormore
ROIannotations. Wereportthemeanintersection-over-union(mIoU),meanDICE(mDICE)score,
andROIrecallforboththeMM-MIL[52]methodandours. DifferentfromWangetal. [52],we
useasetofthresholdsof[0.5,0.55,0.6,0.65,0.7,0.75,0.8,0.85]sincetheROIisgenerallysmaller
inthemammogramandneedsahigherthresholdtohavebetterdetectionresults. WecomputeIoU
20
LIM‐MM
.lacoL
lausiV
ylnO
)sruO(
AMaMTable14: LinearClassificationBootstrapResultsforBalancedAccuracyonEMBED[21]We
conductthebootstrapevaluationforthelinearclassificationpredictedresultofourmethodonboth
BI-RADSanddensitypredictiontasks. WesampleN =10,000bootstrappedsamplesandcompute
theaveragebalancedAccuracy(bACC)withthecorresponding95%confidenceintervalforeach
setting. Thisillustratesthestatisticalstabilityofourmethod.
bACC(%)
Task
1% 10% 100%
EMBEDBI-RADS[21] 28.46[27.12,29.84] 35.11[33.36,36.86] 39.75[37.81,41.64]
EMBEDDensity[21] 76.25[74.88,77.60] 78.11[73.65,75.66] 78.10[76.82,79.34]
Table 15: Linear Classification Bootstrap Results for AUC on EMBED [21] We conduct the
bootstrapevaluationforthelinearclassificationpredictedresultofourmethodonbothBI-RADSand
densitypredictiontasks. WesampleN = 10,000bootstrappedsamplesandcomputetheaverage
AUCwiththecorresponding95%confidenceintervalforeachsetting. Thisillustratesthestatistical
stabilityofourmethod.
AUC(%)
Task
1% 10% 100%
EMBEDBI-RADS[21] 70.64[69.56,71.69] 75.98[75.09,76.87] 77.50[76.61,78.35]
EMBEDDensity[21] 93.11[92.70,93.52] 93.62[93.23,94.00] 93.65[93.26,94.02]
andDICEscoresforeachthresholdandthenaveragethemtogetmIoUandmDICE.ForROIrecall,
anROIisconsideredsuccessfullypredictedwhentheoverlapbetweenthebinarizedsimilaritymap
(withafixedthresholdof50%)andtheROIisgreaterthan50%. Ourmethodgenerallyshowsabetter
performanceovertheMM-MIL[52]modelandachievesarecallnear50%withouttraining. Wenote
thatthenumberreportedheremaylooklowsincethisisaparameter-freezero-shotevaluation,and
theROIinthemammographyisgenerallysmallcomparedwiththewholeimage,whichmakesthe
taskmorechallenging.
A.10 PerformanceStatisticalAnalysis
Wefurtherevaluatethestabilityoftheproposedmethodbybootstrapsamplingtestsetresultsfrom
linear classification and report the 95% confidence interval in Tab. 14 and Tab. 15. Notably, our
method generally shows a small confidence interval, especially for AUC scores. Comparing our
results with confidence interval with the baselines in Tab. 1, we see that there is still a marked
improvementinperformance.
A.11 ReportConstructionTemplate
Weprovideherethetemplateusedtoconstructourstructuredimagecaptionduringtraining. We
describeeachsegmentbelow,andthekeywordswrappedwith“{{”and“}}”willbereplacedwith
correspondinginformationfromthetabulardata.
1. Procedurereported: {{PROCEDURE}}.
2. Reasonforprocedure: {{SCREENING/DIAGNOSTIC}}.
3. Patientinfo: Thispatientis{{RACE}},{{ETHNIC}},and{{AGE}}yearsold.
4. Imageinfo: Thisisa{{IMAGE_TYPE}}full-fielddigitalmammogramofthe{{SIDE}}
breastwith{{VIEW}}view.
5. Breastcomposition: Thebreastis{{DENSITY_DESC}}.
6. Findings: Themammogramshowsthat{{MASS_DESC}}. Themassis{{SHAPE}}and
{{DENSITY}}. A{{DISTRI}}{{SHAPE}}calcificationispresent.
7. Impressions: BI-RADSCategory{{BIRADS}}: {{BIRADS_DESC}}.
8. OverallAssessment: {{BIRADS_DESC}}
Weprovidemoredetailsandcorrespondingdescriptionstringsinourimplementationfile.
21A.12 ExampleMammographyImageswithCaptions
Weprovide7randomlysampledmammographyimageswithcorrespondingcaptionsforeachofthe
BI-RADScategoriesinFig.6toFig.8.
22• Procedure reported: MG Screen Bilat w/Tomo/CAD Stnd
Protocol.
• Reason for procedure: screening.
• Patient info: This patient is African American or Black,
Non-Hispanic or Latino, and 56 years old.
• Image info: This is a 2D full-field digital mammogram of
the right breast with MLO view.
• Breast composition: The breast is scattered fibro glandular
densities.
• Findings: The mammogram shows that an additional imaging is
recommended.
• Impressions: BI-RADS Category 0: additional imaging
required.
• Overall Assessment: Additional imaging is recommended.
• Procedure reported: MG Screen Bilat w/Tomo/CAD Stnd
Protocol.
• Reason for procedure: screening.
• Patient info: This patient is Caucasian or White, Non-
Hispanic or Latino, and 50 years old.
• Image info: This is a 2D full-field digital mammogram of
the right breast with CC view.
• Breast composition: The breast is heterogeneously dense.
This may lower the sensitivity of mammography.
• Findings: The mammogram shows that no significant masses,
calcification, or other abnormalities are present.
• Impressions: BI-RADS Category 1: negative.
• Overall Assessment: Negative.
• Procedure reported: MG Diagnostic Bilateral w/ CAD.
• Reason for procedure: diagnostic.
• Patient info: This patient is Caucasian or White, Non-
Hispanic or Latino, and 67 years old.
• Image info: This is a 2D full-field digital mammogram of
the left breast with MLO view.
• Breast composition: The breast is scattered fibro glandular
densities.
• Findings: The mammogram shows that a benign finding is
present.
• Impressions: BI-RADS Category 2: benign finding.
• Overall Assessment: Benign.
Figure6: ExampleMulti-viewMammographyBI-RADS0-2withConstructedCaption. We
providerandomsampledmulti-viewmammographywiththecorrespondingcaptionconstructedby
us. Wehighlighttheimagematchexactlywiththecaptioninagreenboundingbox.
23• Procedure reported: MG Diagnostic Left w/CAD.
• Reason for procedure: diagnostic.
• Patient info: This patient is African American or Black,
Non-Hispanic or Latino, and 80 years old.
• Image info: This is a 2D full-field digital mammogram of
the left breast with CC view.
• Breast composition: The breast is scattered fibro glandular
densities.
• Findings: The mammogram shows that a probably benign
finding is present. A Grouped Coarse calcification is
present.
• Impressions: BI-RADS Category 3: probably benign finding.
• Overall Assessment: Probably benign.
• Procedure reported: MG Diagnostic Right w/CAD.
• Reason for procedure: diagnostic.
• Patient info: This patient is Caucasian or White, Non-
Hispanic or Latino, and 41 years old.
• Image info: This is a 2D full-field digital mammogram of
the right breast with MLO view.
• Breast composition: The breast is scattered fibro glandular
densities.
• Findings: The mammogram shows that a suspicious abnormality
is present.
• Impressions: BI-RADS Category 4: suspicious abnormality.
• Overall Assessment: Suspicious abnormality.
• Procedure reported: MG Diagnostic Mammo Bilateral.
• Reason for procedure: diagnostic.
• Patient info: This patient is African American or Black,
Non-Hispanic or Latino, and 59 years old.
• Image info: This is a 2D full-field digital mammogram of
the left breast with MLO view.
• Breast composition: The breast is scattered fibro glandular
densities.
• Findings: The mammogram shows that a highly suggestive of
malignancy is present, a biopsy is recommended.
• Impressions: BI-RADS Category 5: highly suggestive of
malignancy.
• Overall Assessment: Highly suggestive of malignancy.
Figure7: ExampleMulti-viewMammographyBI-RADS3-5withConstructedCaption. We
providerandomsampledmulti-viewmammographywiththecorrespondingcaptionconstructedby
us. Wehighlighttheimagematchexactlywiththecaptioninagreenboundingbox.
24• Procedure reported: MG Diagnostic Bilateral w/ CAD.
• Reason for procedure: diagnostic.
• Patient info: This patient is African American or Black,
Non-Hispanic or Latino, and 68 years old.
• Image info: This is a 2D full-field digital mammogram of
the left breast with CC view.
• Breast composition: The breast is scattered fibro glandular
densities.
• Findings: The mammogram shows that a known biopsy-proven
malignant mass is present.
• Impressions: BI-RADS Category 6: known biopsy-proven
malignancy.
• Overall Assessment: Known biopsy-proven malignancy.
Figure 8: Example Multi-view Mammography BI-RADS 6 with Constructed Caption. We
providerandomsampledmulti-viewmammographywiththecorrespondingcaptionconstructedby
us. Wehighlighttheimagematchexactlywiththecaptioninagreenboundingbox.
25