[
    {
        "title": "Infering Alt-text For UI Icons With Large Language Models During App Development",
        "authors": "Sabrina HaqueChristoph Csallner",
        "links": "http://arxiv.org/abs/2409.18060v1",
        "entry_id": "http://arxiv.org/abs/2409.18060v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18060v1",
        "summary": "Ensuring accessibility in mobile applications remains a significant\nchallenge, particularly for visually impaired users who rely on screen readers.\nUser interface icons are essential for navigation and interaction and often\nlack meaningful alt-text, creating barriers to effective use. Traditional deep\nlearning approaches for generating alt-text require extensive datasets and\nstruggle with the diversity and imbalance of icon types. More recent Vision\nLanguage Models (VLMs) require complete UI screens, which can be impractical\nduring the iterative phases of app development. To address these issues, we\nintroduce a novel method using Large Language Models (LLMs) to autonomously\ngenerate informative alt-text for mobile UI icons with partial UI data. By\nincorporating icon context, that include class, resource ID, bounds,\nOCR-detected text, and contextual information from parent and sibling nodes, we\nfine-tune an off-the-shelf LLM on a small dataset of approximately 1.4k icons,\nyielding IconDesc. In an empirical evaluation and a user study IconDesc\ndemonstrates significant improvements in generating relevant alt-text. This\nability makes IconDesc an invaluable tool for developers, aiding in the rapid\niteration and enhancement of UI accessibility.",
        "updated": "2024-09-26 17:01:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18060v1"
    },
    {
        "title": "HARMONIC: A Framework for Explanatory Cognitive Robots",
        "authors": "Sanjay OrugantiSergei NirenburgMarjorie McShaneJesse EnglishMichael K. RobertsChristian Arndt",
        "links": "http://arxiv.org/abs/2409.18037v1",
        "entry_id": "http://arxiv.org/abs/2409.18037v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18037v1",
        "summary": "We present HARMONIC, a framework for implementing cognitive robots that\ntransforms general-purpose robots into trusted teammates capable of complex\ndecision-making, natural communication and human-level explanation. The\nframework supports interoperability between a strategic (cognitive) layer for\nhigh-level decision-making and a tactical (robot) layer for low-level control\nand execution. We describe the core features of the framework and our initial\nimplementation, in which HARMONIC was deployed on a simulated UGV and drone\ninvolved in a multi-robot search and retrieval task.",
        "updated": "2024-09-26 16:42:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18037v1"
    },
    {
        "title": "Control Industrial Automation System with Large Language Models",
        "authors": "Yuchen XiaNasser JazdiJize ZhangChaitanya ShahMichael Weyrich",
        "links": "http://arxiv.org/abs/2409.18009v1",
        "entry_id": "http://arxiv.org/abs/2409.18009v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18009v1",
        "summary": "Traditional industrial automation systems require specialized expertise to\noperate and complex reprogramming to adapt to new processes. Large language\nmodels offer the intelligence to make them more flexible and easier to use.\nHowever, LLMs' application in industrial settings is underexplored. This paper\nintroduces a framework for integrating LLMs to achieve end-to-end control of\nindustrial automation systems. At the core of the framework are an agent system\ndesigned for industrial tasks, a structured prompting method, and an\nevent-driven information modeling mechanism that provides real-time data for\nLLM inference. The framework supplies LLMs with real-time events on different\ncontext semantic levels, allowing them to interpret the information, generate\nproduction plans, and control operations on the automation system. It also\nsupports structured dataset creation for fine-tuning on this downstream\napplication of LLMs. Our contribution includes a formal system design,\nproof-of-concept implementation, and a method for generating task-specific\ndatasets for LLM fine-tuning and testing. This approach enables a more adaptive\nautomation system that can respond to spontaneous events, while allowing easier\noperation and configuration through natural language for more intuitive\nhuman-machine interaction. We provide demo videos and detailed data on GitHub:\nhttps://github.com/YuchenXia/LLM4IAS",
        "updated": "2024-09-26 16:19:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18009v1"
    },
    {
        "title": "LLM4Brain: Training a Large Language Model for Brain Video Understanding",
        "authors": "Ruizhe ZhengLichao Sun",
        "links": "http://arxiv.org/abs/2409.17987v1",
        "entry_id": "http://arxiv.org/abs/2409.17987v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17987v1",
        "summary": "Decoding visual-semantic information from brain signals, such as functional\nMRI (fMRI), across different subjects poses significant challenges, including\nlow signal-to-noise ratio, limited data availability, and cross-subject\nvariability. Recent advancements in large language models (LLMs) show\nremarkable effectiveness in processing multimodal information. In this study,\nwe introduce an LLM-based approach for reconstructing visual-semantic\ninformation from fMRI signals elicited by video stimuli. Specifically, we\nemploy fine-tuning techniques on an fMRI encoder equipped with adaptors to\ntransform brain responses into latent representations aligned with the video\nstimuli. Subsequently, these representations are mapped to textual modality by\nLLM. In particular, we integrate self-supervised domain adaptation methods to\nenhance the alignment between visual-semantic information and brain responses.\nOur proposed method achieves good results using various quantitative semantic\nmetrics, while yielding similarity with ground-truth information.",
        "updated": "2024-09-26 15:57:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17987v1"
    },
    {
        "title": "Participatory design: A systematic review and insights for future practice",
        "authors": "Peter WacnikShanna DalyAditi Verma",
        "links": "http://arxiv.org/abs/2409.17952v1",
        "entry_id": "http://arxiv.org/abs/2409.17952v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17952v1",
        "summary": "Participatory Design -- an iterative, flexible design process that uses the\nclose involvement of stakeholders, most often end users -- is growing in use\nacross design disciplines. As an increasing number of practitioners turn to\nParticipatory Design (PD), it has become less rigidly defined, with\nstakeholders engaged to varying degrees through the use of disjointed\ntechniques. This ambiguous understanding can be counterproductive when\ndiscussing PD processes. Our findings synthesize key decisions and approaches\nfrom design peers that can support others in engaging in PD practice. We\ninvestigated how scholars report the use of Participatory Design in the field\nthrough a systematic literature review. We found that a majority of PD\nliterature examined specific case studies of PD (53 of 88 articles), with the\ndesign of intangible systems representing the most common design context (61 of\n88 articles). Stakeholders most often participated throughout multiple stages\nof a design process (65 of 88 articles), recruited in a variety of ways and\nengaged in several of the 14 specific participatory techniques identified. This\nsystematic review provides today's practitioners synthesized learnings from\npast Participatory Design processes to inform and improve future use of PD,\nattempting to remedy inequitable design by engaging directly with stakeholders\nand users.",
        "updated": "2024-09-26 15:29:34 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17952v1"
    }
]