[
    {
        "title": "End-to-end guarantees for indirect data-driven control of bilinear systems with finite stochastic data",
        "authors": "Nicolas ChatzikiriakosRobin SträsserFrank AllgöwerAndrea Iannelli",
        "links": "http://arxiv.org/abs/2409.18010v1",
        "entry_id": "http://arxiv.org/abs/2409.18010v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18010v1",
        "summary": "In this paper we propose an end-to-end algorithm for indirect data-driven\ncontrol for bilinear systems with stability guarantees. We consider the case\nwhere the collected i.i.d. data is affected by probabilistic noise with\npossibly unbounded support and leverage tools from statistical learning theory\nto derive finite sample identification error bounds. To this end, we solve the\nbilinear identification problem by solving a set of linear and affine\nidentification problems, by a particular choice of a control input during the\ndata collection phase. We provide a priori as well as data-dependent finite\nsample identification error bounds on the individual matrices as well as\nellipsoidal bounds, both of which are structurally suitable for control.\nFurther, we integrate the structure of the derived identification error bounds\nin a robust controller design to obtain an exponentially stable closed-loop. By\nmeans of an extensive numerical study we showcase the interplay between the\ncontroller design and the derived identification error bounds. Moreover, we\nnote appealing connections of our results to indirect data-driven control of\ngeneral nonlinear systems through Koopman operator theory and discuss how our\nresults may be applied in this setup.",
        "updated": "2024-09-26 16:19:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18010v1"
    },
    {
        "title": "Dimension-independent learning rates for high-dimensional classification problems",
        "authors": "Andres Felipe Lerma-PinedaPhilipp PetersenSimon FriederThomas Lukasiewicz",
        "links": "http://arxiv.org/abs/2409.17991v1",
        "entry_id": "http://arxiv.org/abs/2409.17991v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17991v1",
        "summary": "We study the problem of approximating and estimating classification functions\nthat have their decision boundary in the $RBV^2$ space. Functions of $RBV^2$\ntype arise naturally as solutions of regularized neural network learning\nproblems and neural networks can approximate these functions without the curse\nof dimensionality. We modify existing results to show that every $RBV^2$\nfunction can be approximated by a neural network with bounded weights.\nThereafter, we prove the existence of a neural network with bounded weights\napproximating a classification function. And we leverage these bounds to\nquantify the estimation rates. Finally, we present a numerical study that\nanalyzes the effect of different regularity conditions on the decision\nboundaries.",
        "updated": "2024-09-26 16:02:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17991v1"
    },
    {
        "title": "How Feature Learning Can Improve Neural Scaling Laws",
        "authors": "Blake BordelonAlexander AtanasovCengiz Pehlevan",
        "links": "http://arxiv.org/abs/2409.17858v1",
        "entry_id": "http://arxiv.org/abs/2409.17858v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17858v1",
        "summary": "We develop a solvable model of neural scaling laws beyond the kernel limit.\nTheoretical analysis of this model shows how performance scales with model\nsize, training time, and the total amount of available data. We identify three\nscaling regimes corresponding to varying task difficulties: hard, easy, and\nsuper easy tasks. For easy and super-easy target functions, which lie in the\nreproducing kernel Hilbert space (RKHS) defined by the initial infinite-width\nNeural Tangent Kernel (NTK), the scaling exponents remain unchanged between\nfeature learning and kernel regime models. For hard tasks, defined as those\noutside the RKHS of the initial NTK, we demonstrate both analytically and\nempirically that feature learning can improve scaling with training time and\ncompute, nearly doubling the exponent for hard tasks. This leads to a different\ncompute optimal strategy to scale parameters and training time in the feature\nlearning regime. We support our finding that feature learning improves the\nscaling law for hard tasks but not for easy and super-easy tasks with\nexperiments of nonlinear MLPs fitting functions with power-law Fourier spectra\non the circle and CNNs learning vision tasks.",
        "updated": "2024-09-26 14:05:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17858v1"
    },
    {
        "title": "Enriched Functional Tree-Based Classifiers: A Novel Approach Leveraging Derivatives and Geometric Features",
        "authors": "Fabrizio MaturoAnnamaria Porreca",
        "links": "http://arxiv.org/abs/2409.17804v1",
        "entry_id": "http://arxiv.org/abs/2409.17804v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17804v1",
        "summary": "The positioning of this research falls within the scalar-on-function\nclassification literature, a field of significant interest across various\ndomains, particularly in statistics, mathematics, and computer science. This\nstudy introduces an advanced methodology for supervised classification by\nintegrating Functional Data Analysis (FDA) with tree-based ensemble techniques\nfor classifying high-dimensional time series. The proposed framework, Enriched\nFunctional Tree-Based Classifiers (EFTCs), leverages derivative and geometric\nfeatures, benefiting from the diversity inherent in ensemble methods to further\nenhance predictive performance and reduce variance. While our approach has been\ntested on the enrichment of Functional Classification Trees (FCTs), Functional\nK-NN (FKNN), Functional Random Forest (FRF), Functional XGBoost (FXGB), and\nFunctional LightGBM (FLGBM), it could be extended to other tree-based and\nnon-tree-based classifiers, with appropriate considerations emerging from this\ninvestigation. Through extensive experimental evaluations on seven real-world\ndatasets and six simulated scenarios, this proposal demonstrates fascinating\nimprovements over traditional approaches, providing new insights into the\napplication of FDA in complex, high-dimensional learning problems.",
        "updated": "2024-09-26 12:57:47 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17804v1"
    },
    {
        "title": "Transfer Learning in $\\ell_1$ Regularized Regression: Hyperparameter Selection Strategy based on Sharp Asymptotic Analysis",
        "authors": "Koki OkajimaTomoyuki Obuchi",
        "links": "http://arxiv.org/abs/2409.17704v1",
        "entry_id": "http://arxiv.org/abs/2409.17704v1",
        "pdf_url": "http://arxiv.org/pdf/2409.17704v1",
        "summary": "Transfer learning techniques aim to leverage information from multiple\nrelated datasets to enhance prediction quality against a target dataset. Such\nmethods have been adopted in the context of high-dimensional sparse regression,\nand some Lasso-based algorithms have been invented: Trans-Lasso and Pretraining\nLasso are such examples. These algorithms require the statistician to select\nhyperparameters that control the extent and type of information transfer from\nrelated datasets. However, selection strategies for these hyperparameters, as\nwell as the impact of these choices on the algorithm's performance, have been\nlargely unexplored. To address this, we conduct a thorough, precise study of\nthe algorithm in a high-dimensional setting via an asymptotic analysis using\nthe replica method. Our approach reveals a surprisingly simple behavior of the\nalgorithm: Ignoring one of the two types of information transferred to the\nfine-tuning stage has little effect on generalization performance, implying\nthat efforts for hyperparameter selection can be significantly reduced. Our\ntheoretical findings are also empirically supported by real-world applications\non the IMDb dataset.",
        "updated": "2024-09-26 10:20:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.17704v1"
    }
]