[
    {
        "title": "FlowTurbo: Towards Real-time Flow-Based Image Generation with Velocity Refiner",
        "authors": "Wenliang ZhaoMinglei ShiXumin YuJie ZhouJiwen Lu",
        "links": "http://arxiv.org/abs/2409.18128v1",
        "entry_id": "http://arxiv.org/abs/2409.18128v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18128v1",
        "summary": "Building on the success of diffusion models in visual generation, flow-based\nmodels reemerge as another prominent family of generative models that have\nachieved competitive or better performance in terms of both visual quality and\ninference speed. By learning the velocity field through flow-matching,\nflow-based models tend to produce a straighter sampling trajectory, which is\nadvantageous during the sampling process. However, unlike diffusion models for\nwhich fast samplers are well-developed, efficient sampling of flow-based\ngenerative models has been rarely explored. In this paper, we propose a\nframework called FlowTurbo to accelerate the sampling of flow-based models\nwhile still enhancing the sampling quality. Our primary observation is that the\nvelocity predictor's outputs in the flow-based models will become stable during\nthe sampling, enabling the estimation of velocity via a lightweight velocity\nrefiner. Additionally, we introduce several techniques including a pseudo\ncorrector and sample-aware compilation to further reduce inference time. Since\nFlowTurbo does not change the multi-step sampling paradigm, it can be\neffectively applied for various tasks such as image editing, inpainting, etc.\nBy integrating FlowTurbo into different flow-based models, we obtain an\nacceleration ratio of 53.1%$\\sim$58.3% on class-conditional generation and\n29.8%$\\sim$38.5% on text-to-image generation. Notably, FlowTurbo reaches an FID\nof 2.12 on ImageNet with 100 (ms / img) and FID of 3.93 with 38 (ms / img),\nachieving the real-time image generation and establishing the new\nstate-of-the-art. Code is available at https://github.com/shiml20/FlowTurbo.",
        "updated": "2024-09-26 17:59:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18128v1"
    },
    {
        "title": "EgoLM: Multi-Modal Language Model of Egocentric Motions",
        "authors": "Fangzhou HongVladimir GuzovHyo Jin KimYuting YeRichard NewcombeZiwei LiuLingni Ma",
        "links": "http://arxiv.org/abs/2409.18127v1",
        "entry_id": "http://arxiv.org/abs/2409.18127v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18127v1",
        "summary": "As the prevalence of wearable devices, learning egocentric motions becomes\nessential to develop contextual AI. In this work, we present EgoLM, a versatile\nframework that tracks and understands egocentric motions from multi-modal\ninputs, e.g., egocentric videos and motion sensors. EgoLM exploits rich\ncontexts for the disambiguation of egomotion tracking and understanding, which\nare ill-posed under single modality conditions. To facilitate the versatile and\nmulti-modal framework, our key insight is to model the joint distribution of\negocentric motions and natural languages using large language models (LLM).\nMulti-modal sensor inputs are encoded and projected to the joint latent space\nof language models, and used to prompt motion generation or text generation for\negomotion tracking or understanding, respectively. Extensive experiments on\nlarge-scale multi-modal human motion dataset validate the effectiveness of\nEgoLM as a generalist model for universal egocentric learning.",
        "updated": "2024-09-26 17:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18127v1"
    },
    {
        "title": "LLaVA-3D: A Simple yet Effective Pathway to Empowering LMMs with 3D-awareness",
        "authors": "Chenming ZhuTai WangWenwei ZhangJiangmiao PangXihui Liu",
        "links": "http://arxiv.org/abs/2409.18125v1",
        "entry_id": "http://arxiv.org/abs/2409.18125v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18125v1",
        "summary": "Recent advancements in Large Multimodal Models (LMMs) have greatly enhanced\ntheir proficiency in 2D visual understanding tasks, enabling them to\neffectively process and understand images and videos. However, the development\nof LMMs with 3D-awareness for 3D scene understanding has been hindered by the\nlack of large-scale 3D vision-language datasets and powerful 3D encoders. In\nthis paper, we introduce a simple yet effective framework called LLaVA-3D.\nLeveraging the strong 2D understanding priors from LLaVA, our LLaVA-3D\nefficiently adapts LLaVA for 3D scene understanding without compromising 2D\nunderstanding capabilities. To achieve this, we employ a simple yet effective\nrepresentation, 3D Patch, which connects 2D CLIP patch features with their\ncorresponding positions in 3D space. By integrating the 3D Patches into 2D LMMs\nand employing joint 2D and 3D vision-language instruction tuning, we establish\na unified architecture for both 2D image understanding and 3D scene\nunderstanding. Experimental results show that LLaVA-3D converges 3.5x faster\nthan existing 3D LMMs when trained on 3D vision-language datasets. Moreover,\nLLaVA-3D not only achieves state-of-the-art performance across various 3D tasks\nbut also maintains comparable 2D image understanding and vision-language\nconversation capabilities with LLaVA.",
        "updated": "2024-09-26 17:59:11 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18125v1"
    },
    {
        "title": "Lotus: Diffusion-based Visual Foundation Model for High-quality Dense Prediction",
        "authors": "Jing HeHaodong LiWei YinYixun LiangLeheng LiKaiqiang ZhouHongbo LiuBingbing LiuYing-Cong Chen",
        "links": "http://arxiv.org/abs/2409.18124v1",
        "entry_id": "http://arxiv.org/abs/2409.18124v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18124v1",
        "summary": "Leveraging the visual priors of pre-trained text-to-image diffusion models\noffers a promising solution to enhance zero-shot generalization in dense\nprediction tasks. However, existing methods often uncritically use the original\ndiffusion formulation, which may not be optimal due to the fundamental\ndifferences between dense prediction and image generation. In this paper, we\nprovide a systemic analysis of the diffusion formulation for the dense\nprediction, focusing on both quality and efficiency. And we find that the\noriginal parameterization type for image generation, which learns to predict\nnoise, is harmful for dense prediction; the multi-step noising/denoising\ndiffusion process is also unnecessary and challenging to optimize. Based on\nthese insights, we introduce Lotus, a diffusion-based visual foundation model\nwith a simple yet effective adaptation protocol for dense prediction.\nSpecifically, Lotus is trained to directly predict annotations instead of\nnoise, thereby avoiding harmful variance. We also reformulate the diffusion\nprocess into a single-step procedure, simplifying optimization and\nsignificantly boosting inference speed. Additionally, we introduce a novel\ntuning strategy called detail preserver, which achieves more accurate and\nfine-grained predictions. Without scaling up the training data or model\ncapacity, Lotus achieves SoTA performance in zero-shot depth and normal\nestimation across various datasets. It also significantly enhances efficiency,\nbeing hundreds of times faster than most existing diffusion-based methods.",
        "updated": "2024-09-26 17:58:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18124v1"
    },
    {
        "title": "Robot See Robot Do: Imitating Articulated Object Manipulation with Monocular 4D Reconstruction",
        "authors": "Justin KerrChung Min KimMingxuan WuBrent YiQianqian WangKen GoldbergAngjoo Kanazawa",
        "links": "http://arxiv.org/abs/2409.18121v1",
        "entry_id": "http://arxiv.org/abs/2409.18121v1",
        "pdf_url": "http://arxiv.org/pdf/2409.18121v1",
        "summary": "Humans can learn to manipulate new objects by simply watching others;\nproviding robots with the ability to learn from such demonstrations would\nenable a natural interface specifying new behaviors. This work develops Robot\nSee Robot Do (RSRD), a method for imitating articulated object manipulation\nfrom a single monocular RGB human demonstration given a single static\nmulti-view object scan. We first propose 4D Differentiable Part Models\n(4D-DPM), a method for recovering 3D part motion from a monocular video with\ndifferentiable rendering. This analysis-by-synthesis approach uses part-centric\nfeature fields in an iterative optimization which enables the use of geometric\nregularizers to recover 3D motions from only a single video. Given this 4D\nreconstruction, the robot replicates object trajectories by planning bimanual\narm motions that induce the demonstrated object part motion. By representing\ndemonstrations as part-centric trajectories, RSRD focuses on replicating the\ndemonstration's intended behavior while considering the robot's own\nmorphological limits, rather than attempting to reproduce the hand's motion. We\nevaluate 4D-DPM's 3D tracking accuracy on ground truth annotated 3D part\ntrajectories and RSRD's physical execution performance on 9 objects across 10\ntrials each on a bimanual YuMi robot. Each phase of RSRD achieves an average of\n87% success rate, for a total end-to-end success rate of 60% across 90 trials.\nNotably, this is accomplished using only feature fields distilled from large\npretrained vision models -- without any task-specific training, fine-tuning,\ndataset collection, or annotation. Project page:\nhttps://robot-see-robot-do.github.io",
        "updated": "2024-09-26 17:57:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.18121v1"
    }
]