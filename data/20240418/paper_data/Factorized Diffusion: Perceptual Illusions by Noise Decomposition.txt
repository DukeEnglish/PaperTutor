Factorized Diffusion: Perceptual
Illusions by Noise Decomposition
Daniel Geng*, Inbum Park*, and Andrew Owens
University of Michigan
dgeng@umich.edu
https://dangeng.github.io/factorized_diffusion/
Abstract. Givenafactorizationofanimageintoasumoflinearcompo-
nents, we present a zero-shot method to control each individual compo-
nentthroughdiffusionmodelsampling.Forexample,wecandecompose
animageintolowandhighspatialfrequenciesandconditionthesecom-
ponents on different text prompts. This produces hybrid images, which
change appearance depending on viewing distance. By decomposing an
image into three frequency subbands, we can generate hybrid images
with three prompts. We also use a decomposition into grayscale and
color components to produce images whose appearance changes when
they are viewed in grayscale, a phenomena that naturally occurs under
dim lighting. And we explore a decomposition by a motion blur kernel,
which produces images that change appearance under motion blurring.
Our method works by denoising with a composite noise estimate, built
fromthecomponentsofnoiseestimatesconditionedondifferentprompts.
Wealsoshowthatforcertaindecompositions,ourmethodrecoversprior
approaches to compositional generation and spatial control. Finally, we
show that we can extend our approach to generate hybrid images from
real images. We do this by holding one component fixed and generating
the remaining components, effectively solving an inverse problem.
Keywords: Diffusion models · Perceptual illusions · Hybrid images
1 Introduction
The visual world is full of phenomena that can be understood through image
decompositions. For instance, objects look blurry when seen from a distance,
whileupclosetheirdetailsarehighlysalient—twodistinctperspectivesthatcan
be captured by a decomposition in frequency space [42,49]. During the day, we
see in full color, while in dim light we perceive only luminance—an effect that
can be appreciated with a color space decomposition.
We present a simple method for controlling the factors of such decomposi-
tions, allowing a user to generate images that are perceived differently under
different viewing conditions, yet still globally coherent. We apply this approach
to generating a variety of perceptual illusions (Fig. 1). (i) Inspired by the clas-
sic work of Oliva et al. [42] we generate hybrid images, whose interpretation
4202
rpA
71
]VC.sc[
1v51611.4042:viXra2 D. Geng et al.
(cid:210)a photo of
marilyn
monroe(cid:211)
(cid:210)a photo of
an old man(cid:211)
(cid:210)a photo of
john lennon(cid:211)
(cid:210)a photo of a
(cid:210)a photo of (cid:210)a photo of a rabbit(cid:211) (cid:210)a photo of (cid:210)a photo of rome(cid:211) yin yang(cid:211)
houseplants(cid:211) new york city(cid:211)
(cid:210)a photo of a
teddy bear(cid:211)
(cid:210)a lithograph
of a pig(cid:211)
(cid:210)a lithograph
of a panda(cid:211)
(cid:210)a photo of mountains(cid:211) (cid:210)a wl ai tt eh ro fg ar la lp sh (cid:211) of fl(cid:210) oa w el ri t ah ro rg ar na gp eh m eo nf t s (cid:211) (cid:210) aa sl tki hit e h so alg lor ppa sep (cid:211) h i no f (cid:210) oa f l ai t dh eo eg rr (cid:211)aph
(cid:210)a photo of (cid:210)a photo of
waterfalls(cid:211) flower arrangements(cid:211)
(cid:210)a photo of (cid:210)a photo of a skull(cid:211) a dog(cid:211)
(cid:210)a photo of (cid:210)a photo of
a yin yang(cid:211) a pyramid(cid:211)
(cid:210)a watercolor of (cid:210)a painting of (cid:210)a painting of (cid:210)a photo of the (cid:210)a photo of a (cid:210)a painting of
houseplants(cid:211) a barn(cid:211) a flower arrangement(cid:211) grand canyon(cid:211) sports stadium(cid:211) a beehive(cid:211)
(cid:210)a watercolor of (cid:210)a painting of (cid:210)a painting of (cid:210)a photo of (cid:210)a photo of (cid:210)a painting of
the statue of a bumblebee(cid:211) a teddy bear(cid:211) a car(cid:211) a horse(cid:211) a dog(cid:211)
liberty(cid:211)
High Pass: High Pass:
Low Pass: High Pass: Low Pass: (cid:210)an oil painting Low Pass: (cid:210)a photo of
[Real Image] (cid:210)a photo of a cat(cid:211) [Real Image] of a sunset(cid:211) [Real Image] waterfalls(cid:211)
Fig.1: Illusions by Factorized Diffusion. By conditioning the components of a
generated image with different prompts, we can use off-the-shelf text-conditioned im-
age diffusion models to synthesize hybrid images [42], hybrid images containing three
objects, and new perceptual illusions which we refer to as color hybrids and motion
hybrids, which change appearance when color is added or motion blur is induced. In
addition,wecanextractacomponentfromanexistingimageandgeneratethemissing
components,allowingustoproducehybridimagesfromrealimages,whichwetermin-
verse hybrids.Examplesshownarehand-picked.ForrandomsamplespleaseseeFig.9
andFig.18.Forthehybridimages,weincludeinsetstoaidinvisualization.However,
perception of this effect depends on the resolution of the images, so we highly en-
courage the reader to zoom so that an image fills the screen completely, or
visit our webpage for easier viewing.
segamI
dirbyH
sdirbyH
elpirT
sdirbyH
roloC
sdirbyH
esrevnI
noitarutaseD
Saturation sdirbyH
noitoMFactorized Diffusion: Perceptual Illusions by Noise Decomposition 3
changes with viewing distance, which we achieve by controlling the generated
image’s low and high frequency components. A decomposition into three sub-
bands allows us to produce hybrid images with three different prompts, which
we refer to as triple hybrids. (ii) We generate color images whose appearance
changes when they are viewed in grayscale, a phenomena that naturally occurs
under dim lighting. We call these color hybrids, and achieve this by controlling
image luminance separately from its color. (iii) Finally, we produce images that
change appearance under motion blur, which is achieved by using a blur kernel
to decompose an image. We refer to these as motion hybrids.
Our approach consists of a simple change to the sampling procedure of an
off-the-shelf diffusion model. Given an image decomposition and a text prompt
to control each component, in each step of the reverse diffusion process we es-
timate the noise multiple times: once for each component, conditioned on its
corresponding text prompt. We then assemble a composite noise estimate by
combining components from each individual noise estimate, obtained by ap-
plying the decomposition directly to the noise estimates (Fig. 2). Notably, our
approach does not require finetuning [47,67] or access to auxiliary networks, as
in guidance based methods [1,17,21,31,31,40].
We also show that we can take components from existing images, and gen-
erate the remaining components conditioned on text. This recovers a simple
method to solve inverse problems, and is highly related to prior work on using
diffusion models for solving inverse problems [8–10,28,34,53,54,64]. We apply
this technique to producing hybrid images from real images. Finally, we show
thatusingcertaindecompositionswithourmethodrecoverspriortechniquesfor
spatial [2] and compositional [32] control over text prompts.
In summary, our contributions are as follows:
• Given a decomposition of an image into a sum of components, we propose a
zero-shot adaptation of diffusion models to control these components during
image generation.
• Usingourmethod,weproduceavarietyofperceptualillusions,suchasimages
that change appearance under different viewing distances (hybrid images),
illuminationconditions(colorhybrids),andmotionblurring(motionhybrids).
Each of these illusions corresponds to a different image decomposition.
• We provide quantitative evaluations comparing our hybrid images to those
produced by traditional methods, and show that our results are better.
• We give an analysis and intuition for how and why our method works.
• Weshowasimpleextensionofourmethodallowsustosolveinverseproblems,
and we apply this approach to synthesizing hybrid images from real images.
2 Related Work
Diffusion models. Diffusion models [11,25,51,52,54] are trained to denoise
data corrupted by added Gaussian noise. This is achieved by estimating the
noise in noisy data, potentially with some additional conditioning, such as with4 D. Geng et al.
text embeddings. To sample data from a diffusion model, pure Gaussian noise
is iteratively denoised until a clean image remains. Each denoising step consists
of an update that removes a portion of the predicted noise from the noisy im-
age, such as DDPM [25] or DDIM [52]. One noteworthy application of diffusion
models is for text-conditional image generation [29,40,46,48], which we build
our method on top of.
Diffusion model control. Diffusion models are capable of both generating
and editing images conditioned on text prompts. By modifying the reverse pro-
cess [2,18,35,63,68], finetuning [47,67], performing text inversion [26,37,52,
61,65], swapping attention maps [14,23,59], supplying instructions [4], or us-
ing guidance [1,14,21,31,40,44], modifying the style, location, and appearance
of content in an image has become a relatively accessible task. Another line of
work on compositional generation [2,12,33,63] shows that diffusion models can
generateimagesthatconformtocompositionsoftextprompts.Ourworkbuilds
upon this, and shows that similar techniques can be applied to prompting indi-
vidualcomponentsofanimagetoproduceperceptualillusions.Ourworkisalso
similar to Wang et al. [63], in which a diffusion model is used to generate stacks
of images that emulate a zooming video. However, we focus on generating only
a single image that can be understood at multiple resolutions.
Computational optical illusions. Optical illusions are entertaining, but can
also serve as windows into human and machine perception [13,19,20,24,27,39,
56,62].Assuch,muchworkhasgoneintodevelopingcomputationalmethodsfor
generating optical illusions [5–7,16,18,22,42,43,58,60]. In classic work, Oliva et
al.introducedhybridimages[42],whichareimagesthatchangetheirappearance
dependingonviewingdistanceorduration[49].Theseimagesworkbyexploiting
themultiscaleprocessingofhumanperception[41,50].Byaligninglowfrequency
components of one image and high frequency components of another image, the
observer perceives the image as the former when seen from far away and as the
latter when seen up close. By contrast, our approach generates hybrid images
from scratch with a diffusion model, as opposed to fusing together two existing
images,andthusavoidsmanualalignmentstepsandtheneedtofindappropriate
images, and also leads to fewer artifacts.
Artists and researchers have recently used text-conditioned image diffusion
models to generate optical illusions. For example, a pseudonymous artist [60]
adaptedaQRcodegenerationmodel[30,67]tocreateimagesthatsubtlymatch
a target template. While these are also images with multiple interpretations,
they are restricted tobinary mask templates and require aspecialized finetuned
model. Burgert et al. [5] use score distillation sampling [45] to generate images
that match other prompts when viewed from different orientations or overlaid
ontopofeachother.OthermethodssuchasTancik[58]andGenget al.[18]use
off-the-shelfdiffusionmodels[29,46]togeneratemulti-viewopticalillusionsthat
change appearance upon transformations such as rotations, flips, permutations,
skews,andcolorinversions.Thesemethodsworkbytransformingthenoisyimage
multiple ways during the reverse diffusion process, denoising each transformedFactorized Diffusion: Perceptual Illusions by Noise Decomposition 5
version, then averaging the noise estimates together. However, many types of
transformations, like the multiscale processing considered in hybrid images, fail
becausetheyperturbthenoisedistribution[18].Liketheseapproaches,ourwork
also changes the reverse diffusion process to produce images that have multiple
interpretations. However, our approach manipulates the noise estimate rather
than the noisy image, enabling us to handle illusions that prior work cannot.
Please see Appendix G for additional discussion and results.
3 Method
For a given decomposition of an image into components, our method allows for
control of each of these components through text conditioning. We achieve this
by modifying the sampling procedure of a text-to-image diffusion model.
3.1 Preliminaries: Diffusion Models
Diffusion models sample from a distribution by iteratively denoising noisy data.
Over T timesteps, they denoise pure random Gaussian noise, x , until a clean
T
image, x , is produced at the final step. At intermediate timesteps, a variance
0
schedule is followed such that the noisy image at timestep t is of the form
√ √
x = α x + 1−α ϵ, (1)
t t 0 t
where ϵ ∼ N(0,I) is a sample from a standard Gaussian distribution, and
α is a predetermined variance schedule. To sample x from x the diffu-
t t−1 t
sion model, ϵ (·,·,·), predicts the noise in x , conditioned on the timestep t
θ t
and optionally on context y, such as a text prompt embedding. Afterwards, an
update step, update(·,·), is applied which removes a portion of the estimated
noise, ϵ := ϵ (x ,y,t), from the noisy image x . The exact implementation
θ θ t t
of this step depends on the specifics of the method used, but it is—critically
for our method—often a linear combination of x and ϵ (and possibly noise,
t θ
z∼N(0,I)). For example, DDIM [52] (with σ =0) performs the update as:
t
√
√ x − 1−α ϵ
x =update(x ,ϵ )= α t √ t θ + 1−α ϵ . (2)
t−1 t θ t−1 t−1 θ
α
(cid:18) t (cid:19)
(cid:112)
3.2 Factorized Diffusion
AnoverviewofourmethodcanbefoundinFig.2.Ourmethodworksbymanip-
ulatingthenoiseestimateduringthereversediffusionprocesssuchthatdifferent
components of the estimate are conditioned on different prompts. Given a de-
composition of an image, x∈R3×H×W, into the sum of N components,
N
x= f (x), (3)
i
i
(cid:88)6 D. Geng et al.
D Miff ou dsi eo ln =<latexit sha1_base64="WaL0tWjltrkuxlqV+KuqJEpv8BI=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr4sQ9OIxAfOAZAmzk04yZnZ2mZkVwpIv8OJBEa9+kjf/xkmyB00saCiquunuCmLBtXHdbye3srq2vpHfLGxt7+zuFfcPGjpKFMM6i0SkWgHVKLjEuuFGYCtWSMNAYDMY3U395hMqzSP5YMYx+iEdSN7njBor1W66xZJbdmcgy8TLSAkyVLvFr04vYkmI0jBBtW57bmz8lCrDmcBJoZNojCkb0QG2LZU0RO2ns0Mn5MQqPdKPlC1pyEz9PZHSUOtxGNjOkJqhXvSm4n9eOzH9az/lMk4MSjZf1E8EMRGZfk16XCEzYmwJZYrbWwkbUkWZsdkUbAje4svLpHFW9i7LF7XzUuU2iyMPR3AMp+DBFVTgHqpQBwYIz/AKb86j8+K8Ox/z1pyTzRzCHzifP5ANjMs=</latexit> +<latexit sha1_base64="Age47FzHBvUqYGPEEfH8g0BXt4k=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXfB2DXjwmYB6QhDA76U3GzM4uM7NCWPIFXjwo4tVP8ubfOEn2oIkFDUVVN91dfiy4Nq777eRWVtfWN/Kbha3tnd294v5BQ0eJYlhnkYhUy6caBZdYN9wIbMUKaegLbPqju6nffEKleSQfzDjGbkgHkgecUWOl2lmvWHLL7gxkmXgZKUGGaq/41elHLAlRGiao1m3PjU03pcpwJnBS6CQaY8pGdIBtSyUNUXfT2aETcmKVPgkiZUsaMlN/T6Q01Hoc+rYzpGaoF72p+J/XTkxw0025jBODks0XBYkgJiLTr0mfK2RGjC2hTHF7K2FDqigzNpuCDcFbfHmZNM7L3lX5snZRqtxmceThCI7hFDy4hgrcQxXqwADhGV7hzXl0Xpx352PemnOymUP4A+fzB3TFjLk=</latexit> +<latexit sha1_base64="Age47FzHBvUqYGPEEfH8g0BXt4k=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXfB2DXjwmYB6QhDA76U3GzM4uM7NCWPIFXjwo4tVP8ubfOEn2oIkFDUVVN91dfiy4Nq777eRWVtfWN/Kbha3tnd294v5BQ0eJYlhnkYhUy6caBZdYN9wIbMUKaegLbPqju6nffEKleSQfzDjGbkgHkgecUWOl2lmvWHLL7gxkmXgZKUGGaq/41elHLAlRGiao1m3PjU03pcpwJnBS6CQaY8pGdIBtSyUNUXfT2aETcmKVPgkiZUsaMlN/T6Q01Hoc+rYzpGaoF72p+J/XTkxw0025jBODks0XBYkgJiLTr0mfK2RGjC2hTHF7K2FDqigzNpuCDcFbfHmZNM7L3lX5snZRqtxmceThCI7hFDy4hgrcQxXqwADhGV7hzXl0Xpx352PemnOymUP4A+fzB3TFjLk=</latexit>
(cid:210)waterfalls(cid:211) /<latexit sha1_base64="pYBNhcKG0Xb8t9GDEDvKL+TA+AU=">AAAB8XicbVDLSgNBEOyNrxhfUY9eFoPgKeyKr2PQi8cI5oHJEmYnvcmQ2ZllZlYIS/7CiwdFvPo33vwbJ8keNLGgoajqprsrTDjTxvO+ncLK6tr6RnGztLW9s7tX3j9oapkqig0quVTtkGjkTGDDMMOxnSgkccixFY5up37rCZVmUjyYcYJBTAaCRYwSY6XHLiaacSl6fq9c8areDO4y8XNSgRz1Xvmr25c0jVEYyonWHd9LTJARZRjlOCl1U40JoSMywI6lgsSog2x28cQ9sUrfjaSyJYw7U39PZCTWehyHtjMmZqgXvan4n9dJTXQdZEwkqUFB54uilLtGutP33T5TSA0fW0KoYvZWlw6JItTYkEo2BH/x5WXSPKv6l9WL+/NK7SaPowhHcAyn4MMV1OAO6tAACgKe4RXeHO28OO/Ox7y14OQzh/AHzucPe1GQzg==</latexit>1 f<latexit sha1_base64="1RKDZSmAZbdKeDa4o7dBzVrCacg=">AAAB+HicbVDLSsNAFL3xWeujVZdugkWom5KIr2XRjcsK9gFtCJPpTTt0MgkzE6GGfokbF4q49VPc+TdO2yy09cCFM+fcy9x7goQzpR3n21pZXVvf2CxsFbd3dvdK5f2DlopTSbFJYx7LTkAUciawsPtMItV8bAihkpldbTokklBqtZsnimq2aEEkNkzkFikj5gd2JA656Gztm1XOt/Y/uY7hsS8srV9gZ8s68HjGgCIXckwQTGFgUowWYMUErmq0MkMvd1NwKKAfJbFfFaJw40hQlxdb4ps75u6qsVF++uvOdD+VpniB3nrulZju5DzmCpHQ8Ig6fGXX5/A76q49CWkOnagR=i<g/0l5aUtSepxriuts>k2suI1IxynBR7qcKE0BEZYNdQQSJUXjZbfGKfGKVvh7E0JbQ9U39PZCRSahwFpjMieqgWvan4n9dNdXjtZUwkqUZB5x+FKbd1bE9T1(/1) ftjTTnHYTSXEUcNoJxrczv/N<IlpaWtKexxeiNtC TshhHao1R_HbgaosWeM6o4K=1"kZXoyY7fHxPmr31Eahph8Qmsi1nFH1zMdkMV9O8HuX+RLEUmnRDPnkQ=K"n>FAzAUAoBE+cHTidc/b+V6DgL9SiskNkAZFULa2MpKrx1Uojf3jXbSpb0SMX1YiaEkuZi4lXJR8ab6UqseuKnJFpZiwMT86ZgDD2WDEByUn4boRsDrJL55MowtMPx0Faql6RJBei4icMapGSImWhz0/VFzn9XP/ZjHthMS1KClWJwF9JcjOOHCPOOuvRcWyv9ZJm04gn49Ue9Lp9xXvhqt3ZCc2xvkraGS5alCZrxLu47KSEzwu570djfGtag8O2ypOJaEtEvNYdgOsKGeCpYAGTFKi+cQQTCQzuyIywhITbbIqmRDc5ZNXSbtecy9rF/fnlcZNHkcRjuEEquDCFTTgDprQAgIpPMMrvFlP1ov1bn0sWgtWPnMEf2B9/gCvkpJ5</latexit>2(/1) f<latexit sha1_base64="O87qsQ0rd1We/hx4bue3lUi8etM=">AAAB+HicbVDLTsMwEHTKq5RHAxy5WFRI5VIlvI8VXDgWiT6kNoocd9NadZzIdpBK1C/hwgGEuPIp3Pgb3DYHaBlppfHMrrw7QcKZ0o7zbRVWVtfWN4qbpa3tnd2yvbffUnEqKTRpzGPZCYgCzgQ0NdMcOokEEgUc2sHoduq3H0EqFosHPU7Ai8hAsJBRoo3k2+XQP6v2IFGMm6d74tsVp+bMgJeJm5MKytHw7a9eP6ZpBEJTTpTquk6ivYxIzSiHSamXKkgIHZEBdA0VJALlZbPFJ/jYKH0cxtKU0Him/p7ISKTUOApMZ0T0UC16U/E/r5vq8NrLmEhSDYLOPwpTjnWMpyngPpNANR8bQqhkZldMh0QSqk1WJROCu3jyMmmd1tzL2sX9eaV+k8dRRIfoCFWRi65QHd2hBmoiilL0jF7Rm/VkvVjv1se8tWDlMwfoD6zPH7Eikno=</latexit>3(/1)
D Miff ou dsi eo ln =<latexit sha1_base64="WaL0tWjltrkuxlqV+KuqJEpv8BI=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr4sQ9OIxAfOAZAmzk04yZnZ2mZkVwpIv8OJBEa9+kjf/xkmyB00saCiquunuCmLBtXHdbye3srq2vpHfLGxt7+zuFfcPGjpKFMM6i0SkWgHVKLjEuuFGYCtWSMNAYDMY3U395hMqzSP5YMYx+iEdSN7njBor1W66xZJbdmcgy8TLSAkyVLvFr04vYkmI0jBBtW57bmz8lCrDmcBJoZNojCkb0QG2LZU0RO2ns0Mn5MQqPdKPlC1pyEz9PZHSUOtxGNjOkJqhXvSm4n9eOzH9az/lMk4MSjZf1E8EMRGZfk16XCEzYmwJZYrbWwkbUkWZsdkUbAje4svLpHFW9i7LF7XzUuU2iyMPR3AMp+DBFVTgHqpQBwYIz/AKb86j8+K8Ox/z1pyTzRzCHzifP5ANjMs=</latexit> +<latexit sha1_base64="Age47FzHBvUqYGPEEfH8g0BXt4k=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXfB2DXjwmYB6QhDA76U3GzM4uM7NCWPIFXjwo4tVP8ubfOEn2oIkFDUVVN91dfiy4Nq777eRWVtfWN/Kbha3tnd294v5BQ0eJYlhnkYhUy6caBZdYN9wIbMUKaegLbPqju6nffEKleSQfzDjGbkgHkgecUWOl2lmvWHLL7gxkmXgZKUGGaq/41elHLAlRGiao1m3PjU03pcpwJnBS6CQaY8pGdIBtSyUNUXfT2aETcmKVPgkiZUsaMlN/T6Q01Hoc+rYzpGaoF72p+J/XTkxw0025jBODks0XBYkgJiLTr0mfK2RGjC2hTHF7K2FDqigzNpuCDcFbfHmZNM7L3lX5snZRqtxmceThCI7hFDy4hgrcQxXqwADhGV7hzXl0Xpx352PemnOymUP4A+fzB3TFjLk=</latexit> +<latexit sha1_base64="Age47FzHBvUqYGPEEfH8g0BXt4k=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXfB2DXjwmYB6QhDA76U3GzM4uM7NCWPIFXjwo4tVP8ubfOEn2oIkFDUVVN91dfiy4Nq777eRWVtfWN/Kbha3tnd294v5BQ0eJYlhnkYhUy6caBZdYN9wIbMUKaegLbPqju6nffEKleSQfzDjGbkgHkgecUWOl2lmvWHLL7gxkmXgZKUGGaq/41elHLAlRGiao1m3PjU03pcpwJnBS6CQaY8pGdIBtSyUNUXfT2aETcmKVPgkiZUsaMlN/T6Q01Hoc+rYzpGaoF72p+J/XTkxw0025jBODks0XBYkgJiLTr0mfK2RGjC2hTHF7K2FDqigzNpuCDcFbfHmZNM7L3lX5snZRqtxmceThCI7hFDy4hgrcQxXqwADhGV7hzXl0Xpx352PemnOymUP4A+fzB3TFjLk=</latexit>
x<latexit sha1_base64="im1e/4T9eS0sJniQeODaVSCpcus=">AAAB83icbVDLSsNAFL2pr1pfVZduBovgqiTia1l047KCfUATymQ6aYdOHszciCX0N9y4UMStP+POv3HSZqGtBwYO59zLPXP8RAqNtv1tlVZW19Y3ypuVre2d3b3q/kFbx6livMViGauuTzWXIuItFCh5N1Gchr7kHX98m/udR660iKMHnCTcC+kwEoFgFI3kuiHFkR9kT9M+9qs1u27PQJaJU5AaFGj2q1/uIGZpyCNkkmrdc+wEvYwqFEzyacVNNU8oG9Mh7xka0ZBrL5tlnpITowxIECvzIiQz9fdGRkOtJ6FvJvOMetHLxf+8XorBtZeJKEmRR2x+KEglwZjkBZCBUJyhnBhCmRImK2EjqihDU1PFlOAsfnmZtM/qzmX94v681rgp6ijDERzDKThwBQ24gya0gEECz/AKb1ZqvVjv1sd8tGQVO4fwB9bnD5Mskg0=</latexit> t (cid:210)skull(cid:211) /<latexit sha1_base64="IHXJ3ABAKkPlTBFlvhCDrKqgRTw=">AAAB8XicbVDLSgNBEOz1GeMr6tHLYhA8hd3g6xj04jGCeWCyhNlJbzJkdmaZmRXCkr/w4kERr/6NN//GSbIHTSxoKKq66e4KE8608bxvZ2V1bX1js7BV3N7Z3dsvHRw2tUwVxQaVXKp2SDRyJrBhmOHYThSSOOTYCke3U7/1hEozKR7MOMEgJgPBIkaJsdJjFxPNuBS9aq9U9ireDO4y8XNShhz1Xumr25c0jVEYyonWHd9LTJARZRjlOCl2U40JoSMywI6lgsSog2x28cQ9tUrfjaSyJYw7U39PZCTWehyHtjMmZqgXvan4n9dJTXQdZEwkqUFB54uilLtGutP33T5TSA0fW0KoYvZWlw6JItTYkIo2BH/x5WXSrFb8y8rF/Xm5dpPHUYBjOIEz8OEKanAHdWgABQHP8ApvjnZenHfnY9664uQzR/AHzucPfNWQzw==</latexit>2 f<latexit sha1_base64="XCBIQQ5HYBIivjBWokZd28OSCL4=">AAAB+HicbVDLSsNAFL2pr1ofjbp0M1iEuilJ8bUsunFZwT6gDWEynbRDJ5MwMxFq6Je4caGIWz/FnX/jtM1CWw9cOHPOvcy9J0g4U9pxvq3C2vrG5lZxu7Szu7dftg8O2ypOJaEtEvNYdgOsKGeCtjTTnHYTSXEUcNoJxrczv/NIpWKxeNCThHoRHgoWMoK1kXy7HPputU8Txbh51s98u+LUnDnQKnFzUoEcTd/+6g9ikkZUaMKxUj3XSbSXYakZ4XRa6qeKJpiM8ZD2DBU4osrL5otP0alRBiiMpSmh0Vz9PZHhSKlJFJjOCOuRWvZm4n9eL9XhtZcxkaSaCrL4KEw50jGapYAGTFKi+cQQTCQzuyIywhITbbIqmRDc5ZNXSbtecy9rF/fnlcZNHkcRjuEEquDCFTTgDprQAgIpPMMrvFlP1ov1bn0sWgtWPnMEf2B9/gCvh5J5</latexit>1(/2) ftjTTnHYTSXEUcNoJxrczv/N<IlpaWtKexxeiNtC TshhHao1R_HbgaosWeM6o4K=1"ksX/yu7wHcP6rd1ragpo84mzibniHKzerJJo/S5XdVsUWZpvOOX/O0g=V"e>LAmApAABI+5Hmirc7b9V1DRL/SEsJNIA2FoL02IpRrj1poXfqjubkp20gMv1wi1EIuziwluJm80b1UEs8uVnTFTZAwZT46ygHDtWGESypnwbRRJDWJX5zMRweMfxoFlqO6jJDeF4AcYaSG1INWCzo/7Fnn6Xe/yjLtDMk1VCKWTwK9DcCOdHEPdOYvjctye9zJN0xgP4+U89XpqxrvDqa3yC92jvIrkGk51lFZWxTux7USZzhuy7pdGfMt0g8O2ypOJaEtEvNYdgOsKGeCSwENmKRE84khmEhmdkVkhCUm2mRVMiG4yyevkna95l7WLu7PK42bPI4iHMMJVMGFK2jAHTShBQRSeIZXeLOerBfr3fpYtBasfOYI/sD6/AGxF5J6</latexit>2(/2) f<latexit sha1_base64="t37mmTDNvRltKcNBQxkEBprDpz4=">AAAB+HicbVDLTsMwENzwLOXRAEcuFhVSuVRJeR4ruHAsEn1IbRQ5rtNadZzIdpBK1C/hwgGEuPIp3Pgb3DYHaBlppfHMrrw7QcKZ0o7zba2srq1vbBa2its7u3sle/+gpeJUEtokMY9lJ8CKciZoUzPNaSeRFEcBp+1gdDv1249UKhaLBz1OqBfhgWAhI1gbybdLoX9W6dFEMW6etVPfLjtVZwa0TNyclCFHw7e/ev2YpBEVmnCsVNd1Eu1lWGpGOJ0Ue6miCSYjPKBdQwWOqPKy2eITdGKUPgpjaUpoNFN/T2Q4UmocBaYzwnqoFr2p+J/XTXV47WVMJKmmgsw/ClOOdIymKaA+k5RoPjYEE8nMrogMscREm6yKJgR38eRl0qpV3cvqxf15uX6Tx1GAIziGCrhwBXW4gwY0gUAKz/AKb9aT9WK9Wx/z1hUrnzmEP7A+fwCyp5J7</latexit>3(/2)
D Miff ou dsi eo ln =<latexit sha1_base64="WaL0tWjltrkuxlqV+KuqJEpv8BI=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr4sQ9OIxAfOAZAmzk04yZnZ2mZkVwpIv8OJBEa9+kjf/xkmyB00saCiquunuCmLBtXHdbye3srq2vpHfLGxt7+zuFfcPGjpKFMM6i0SkWgHVKLjEuuFGYCtWSMNAYDMY3U395hMqzSP5YMYx+iEdSN7njBor1W66xZJbdmcgy8TLSAkyVLvFr04vYkmI0jBBtW57bmz8lCrDmcBJoZNojCkb0QG2LZU0RO2ns0Mn5MQqPdKPlC1pyEz9PZHSUOtxGNjOkJqhXvSm4n9eOzH9az/lMk4MSjZf1E8EMRGZfk16XCEzYmwJZYrbWwkbUkWZsdkUbAje4svLpHFW9i7LF7XzUuU2iyMPR3AMp+DBFVTgHqpQBwYIz/AKb86j8+K8Ox/z1pyTzRzCHzifP5ANjMs=</latexit> +<latexit sha1_base64="Age47FzHBvUqYGPEEfH8g0BXt4k=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXfB2DXjwmYB6QhDA76U3GzM4uM7NCWPIFXjwo4tVP8ubfOEn2oIkFDUVVN91dfiy4Nq777eRWVtfWN/Kbha3tnd294v5BQ0eJYlhnkYhUy6caBZdYN9wIbMUKaegLbPqju6nffEKleSQfzDjGbkgHkgecUWOl2lmvWHLL7gxkmXgZKUGGaq/41elHLAlRGiao1m3PjU03pcpwJnBS6CQaY8pGdIBtSyUNUXfT2aETcmKVPgkiZUsaMlN/T6Q01Hoc+rYzpGaoF72p+J/XTkxw0025jBODks0XBYkgJiLTr0mfK2RGjC2hTHF7K2FDqigzNpuCDcFbfHmZNM7L3lX5snZRqtxmceThCI7hFDy4hgrcQxXqwADhGV7hzXl0Xpx352PemnOymUP4A+fzB3TFjLk=</latexit> +<latexit sha1_base64="Age47FzHBvUqYGPEEfH8g0BXt4k=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXfB2DXjwmYB6QhDA76U3GzM4uM7NCWPIFXjwo4tVP8ubfOEn2oIkFDUVVN91dfiy4Nq777eRWVtfWN/Kbha3tnd294v5BQ0eJYlhnkYhUy6caBZdYN9wIbMUKaegLbPqju6nffEKleSQfzDjGbkgHkgecUWOl2lmvWHLL7gxkmXgZKUGGaq/41elHLAlRGiao1m3PjU03pcpwJnBS6CQaY8pGdIBtSyUNUXfT2aETcmKVPgkiZUsaMlN/T6Q01Hoc+rYzpGaoF72p+J/XTkxw0025jBODks0XBYkgJiLTr0mfK2RGjC2hTHF7K2FDqigzNpuCDcFbfHmZNM7L3lX5snZRqtxmceThCI7hFDy4hgrcQxXqwADhGV7hzXl0Xpx352PemnOymUP4A+fzB3TFjLk=</latexit>
(cid:210)yin yang(cid:211) /<latexit sha1_base64="yuNOOHIWamYzXfv78rX9i60CEDg=">AAAB8XicbVDLSgNBEOyNrxhfUY9eFoPgKez6Pga9eIxgHpgsYXbSmwyZnVlmZoWw5C+8eFDEq3/jzb9xkuxBowUNRVU33V1hwpk2nvflFJaWV1bXiuuljc2t7Z3y7l5Ty1RRbFDJpWqHRCNnAhuGGY7tRCGJQ46tcHQz9VuPqDST4t6MEwxiMhAsYpQYKz10MdGMS9E77ZUrXtWbwf1L/JxUIEe9V/7s9iVNYxSGcqJ1x/cSE2REGUY5TkrdVGNC6IgMsGOpIDHqIJtdPHGPrNJ3I6lsCePO1J8TGYm1Hseh7YyJGepFbyr+53VSE10FGRNJalDQ+aIo5a6R7vR9t88UUsPHlhCqmL3VpUOiCDU2pJINwV98+S9pnlT9i+r53Vmldp3HUYQDOIRj8OESanALdWgABQFP8AKvjnaenTfnfd5acPKZffgF5+MbflmQ0A==</latexit>3 f<latexit sha1_base64="AfdVug/1YcGqsRkZESlC7Qwq194=">AAAB+HicbVDLTsMwEHTKq5RHAxy5WFRI5VIlvI8VXDgWiT6kNoocd9NadZzIdpBK1C/hwgGEuPIp3Pgb3DYHaBlppfHMrrw7QcKZ0o7zbRVWVtfWN4qbpa3tnd2yvbffUnEqKTRpzGPZCYgCzgQ0NdMcOokEEgUc2sHoduq3H0EqFosHPU7Ai8hAsJBRoo3k2+XQd6s9SBTj5nl24tsVp+bMgJeJm5MKytHw7a9eP6ZpBEJTTpTquk6ivYxIzSiHSamXKkgIHZEBdA0VJALlZbPFJ/jYKH0cxtKU0Him/p7ISKTUOApMZ0T0UC16U/E/r5vq8NrLmEhSDYLOPwpTjnWMpyngPpNANR8bQqhkZldMh0QSqk1WJROCu3jyMmmd1tzL2sX9eaV+k8dRRIfoCFWRi65QHd2hBmoiilL0jF7Rm/VkvVjv1se8tWDlMwfoD6zPH7EMkno=</latexit>1(/3) fUzPNaSeRFEcBp+1gdDv1249<UlKahtaeLxBizt1 OsqhBaf1h_gbWaAsheI614g=b"y7b6dyLPoGV/+Ar+9wGKilicGqDNfAPwsE13P+flLSj4tfVsZ3wgaM0=T"N>yAcAlACBF+HHwi7ceb/VeDvL2TYspMBwEEVNmznwCLsOVXNRdA1EEcuu1FlhWVGSpuGVORJJ0eURe46rmuiHCASsYEjnP1KIBbdRQQw5WrOtqNPaKdyZ2zeIIdTpdBGKK1UCP/ghpwjgaGUEpuoPNIFpN3/PTg2bQ34DUYmHoacBBlapYpzfwHnMqrorFwr72Qpc+KJZ/0XoT7XzVb4a72WsVrMqJ1KvmbmBgas2wi/tCsl7OuO3dsIlyem/+gpeJUEtokMY9lJ8CKciZoKaA+k5RoPjYEE8nMrogMscREm6yKJgR38eRl0qpV3cvqxf15uX6Tx1GAIziGCrhwBXW4gwY0gUAKz/AKb9aT9WK9Wx/z1hUrnzmEP7A+fwCynJJ7</latexit>2(/3) f<latexit sha1_base64="kqE86tsMnwK627aDV9RGBUwEzCY=">AAAB+HicbVDLTsMwENzwLOXRAEcuFhVSuVQJ5XWs4MKxSPQhtVHkuE5r1XEi20EqUb+ECwcQ4sqncONvcNscoGWklcYzu/LuBAlnSjvOt7Wyura+sVnYKm7v7O6V7P2DlopTSWiTxDyWnQArypmgTc00p51EUhwFnLaD0e3Ubz9SqVgsHvQ4oV6EB4KFjGBtJN8uhX6t0qOJYtw8a6e+XXaqzgxombg5KUOOhm9/9foxSSMqNOFYqa7rJNrLsNSMcDop9lJFE0xGeEC7hgocUeVls8Un6MQofRTG0pTQaKb+nshwpNQ4CkxnhPVQLXpT8T+vm+rw2suYSFJNBZl/FKYc6RhNU0B9JinRfGwIJpKZXREZYomJNlkVTQju4snLpHVWdS+rF/fn5fpNHkcBjuAYKuDCFdThDhrQBAIpPMMrvFlP1ov1bn3MW1esfOYQ/sD6/AG0LJJ8</latexit>3(/3)
x<latexit sha1_base64="E1RRCpatAH7SQJNfmjZSRTu13VA=">AAACDHicbVDLSgMxFM3UV62vqks3wSLUTZkRXxuh6MZlBfuAzjBk0kwbmmSGJCOWoR/gxl9x40IRt36AO//GTDugth4IHM45l9x7gphRpW37yyosLC4trxRXS2vrG5tb5e2dlooSiUkTRyySnQApwqggTU01I51YEsQDRtrB8Crz23dEKhqJWz2KicdRX9CQYqSN5JcrLkd6EITp/RheQFcl3Kcw9Gn1Rz80KbtmTwDniZOTCsjR8Mufbi/CCSdCY4aU6jp2rL0USU0xI+OSmygSIzxEfdI1VCBOlJdOjhnDA6P0YBhJ84SGE/X3RIq4UiMemGS2opr1MvE/r5vo8NxLqYgTTQSefhQmDOoIZs3AHpUEazYyBGFJza4QD5BEWJv+SqYEZ/bkedI6qjmntZOb40r9Mq+jCPbAPqgCB5yBOrgGDdAEGDyAJ/ACXq1H69l6s96n0YKVz+yCP7A+vgFGRpsk</latexit> = fi(x) =<latexit sha1_base64="WaL0tWjltrkuxlqV+KuqJEpv8BI=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoPgKeyKr4sQ9OIxAfOAZAmzk04yZnZ2mZkVwpIv8OJBEa9+kjf/xkmyB00saCiquunuCmLBtXHdbye3srq2vpHfLGxt7+zuFfcPGjpKFMM6i0SkWgHVKLjEuuFGYCtWSMNAYDMY3U395hMqzSP5YMYx+iEdSN7njBor1W66xZJbdmcgy8TLSAkyVLvFr04vYkmI0jBBtW57bmz8lCrDmcBJoZNojCkb0QG2LZU0RO2ns0Mn5MQqPdKPlC1pyEz9PZHSUOtxGNjOkJqhXvSm4n9eOzH9az/lMk4MSjZf1E8EMRGZfk16XCEzYmwJZYrbWwkbUkWZsdkUbAje4svLpHFW9i7LF7XzUuU2iyMPR3AMp+DBFVTgHqpQBwYIz/AKb86j8+K8Ox/z1pyTzRzCHzifP5ANjMs=</latexit> +<latexit sha1_base64="Age47FzHBvUqYGPEEfH8g0BXt4k=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXfB2DXjwmYB6QhDA76U3GzM4uM7NCWPIFXjwo4tVP8ubfOEn2oIkFDUVVN91dfiy4Nq777eRWVtfWN/Kbha3tnd294v5BQ0eJYlhnkYhUy6caBZdYN9wIbMUKaegLbPqju6nffEKleSQfzDjGbkgHkgecUWOl2lmvWHLL7gxkmXgZKUGGaq/41elHLAlRGiao1m3PjU03pcpwJnBS6CQaY8pGdIBtSyUNUXfT2aETcmKVPgkiZUsaMlN/T6Q01Hoc+rYzpGaoF72p+J/XTkxw0025jBODks0XBYkgJiLTr0mfK2RGjC2hTHF7K2FDqigzNpuCDcFbfHmZNM7L3lX5snZRqtxmceThCI7hFDy4hgrcQxXqwADhGV7hzXl0Xpx352PemnOymUP4A+fzB3TFjLk=</latexit> +<latexit sha1_base64="Age47FzHBvUqYGPEEfH8g0BXt4k=">AAAB6HicbVDLSgNBEOyNrxhfUY9eBoMgCGFXfB2DXjwmYB6QhDA76U3GzM4uM7NCWPIFXjwo4tVP8ubfOEn2oIkFDUVVN91dfiy4Nq777eRWVtfWN/Kbha3tnd294v5BQ0eJYlhnkYhUy6caBZdYK2FDNq9iwgIzbNMpUuKCaDecgFLbbfPHqmjZuN6Mn7fLf3ElKXl5esSnQZfRzqDtjxGmbckegTHhkCgIe7chUFWDOyl42hlgmrvcWQHxLXLq7wgAxDkhmGXVg7ZhKzUXGlG0aXqp/x4315e2lPHeLmAnlORyGmiUaPo41Am+3fPzjBU30T3FpjcLpkw=J<n/BlSa6tCeQxaiYt8>pGdIBtSyUNUXfT2aETcmKVPgkiZUsaMlN/T6Q01Hoc+rYzpGaoF72p+J/XTkxw0025jBODks0XBYkgJiLTr0mfK2RGjC2hTHF7
Xi
/˜<latexit sha1_base64="6TzbvKz5CY1Yk/gnh6wB9GQbjUk=">AAAB9XicbVDLSsNAFJ34rPVVdelmsAiuSiK+lkU3LivYBzSxTCY37dDJTJiZKCX0P9y4UMSt/+LOv3HaZqGtBy4czrmXe+8JU860cd1vZ2l5ZXVtvbRR3tza3tmt7O23tMwUhSaVXKpOSDRwJqBpmOHQSRWQJOTQDoc3E7/9CEozKe7NKIUgIX3BYkaJsdKDbxiPwIdUMy5Fr1J1a+4UeJF4BamiAo1e5cuPJM0SEIZyonXXc1MT5EQZRjmMy36mISV0SPrQtVSQBHSQT68e42OrRDiWypYweKr+nshJovUoCW1nQsxAz3sT8T+vm5n4KsiZSDMDgs4WxRnHRuJJBDhiCqjhI0sIVczeiumAKEKNDapsQ/DmX14krdOad1E7vzur1q+LOEroEB2hE+ShS1RHt6iBmogihZ7RK3pznpwX5935mLUuOcXMAfoD5/MH9lKS1A==</latexit> f<latexit sha1_base64="1RKDZSmAZbdKeDa4o7dBzVrCacg=">AAAB+HicbVDLSsNAFL3xWeujVZdugkWom5KIr2XRjcsK9gFtCJPpTTt0MgkzE6GGfokbF4q49VPc+TdO2yy09cCFM+fcy9x7goQzpR3n21pZXVvf2CxsFbd3dvdK5f2DlopTSbFJYx7LTkAUciawqZnm2Ekkkijg2A5Gt1O//YhSsVg86HGCXkQGgoWMEm0kv1wKfbfaw0Qxbp7uqV+uODVnBnuZuDmpQI6GX/7q9WOaRig05USprusk2suI1IxynBR7qcKE0BEZYNdQQSJUXjZbfGKfGKVvh7E0JbQ9U39PZCRSahwFpjMieqgWvan4n9dNdXjtZUwkqUZB5x+FKbd1bE9TsPtMItV8bAihkpldbTokklBtsiqaENzFk5dJ66zmXtYu7s8r9Zs8jgIcwTFUwYUrqMMdNKAJFFJ4hld4s56sF+vd+pi3rlj5zCH8gfX5A64Ckng=</latexit> 1(/1) f<latexit sha1_base64="s/uwc6drgo4zbiKeJoSXVUZvO/0=">AAAB+HicbVDLSsNAFL2pr1ofjbp0M1iEuilJ8bUsunFZwT6gDWEynbRDJ5MwMxFq6Je4caGIWz/FnX/jtM1CWw9cOHPOvcy9J0g4U9pxvq3C2vrG5lZxu7Szu7dftg8O2ypOJaEtEvNYdgOsKGeCtjTTnHYTSXEUcNoJxrczv/NIpWKxeNCThHoRHgoWMoK1kXy7HPr1ap8minHzrJ/5dsWpOXOgVeLmpAI5mr791R/EJI2o0IRjpXquk2gvw1Izwum01E8VTTAZ4yHtGSpwRJWXzRefolOjDFAYS1NCo7n6eyLDkVKTKDCdEdYjtezNxP+8XqrDay9jIkk1FWTxUZhypGM0SwENmKRE84khmEhmdkVkhCUm2mRVMiG4yyevkna95l7WLu7PK42bPI4iHMMJVMGFK2jAHTShBQRSeIZXeLOerBfr3fpYtBasfOYI/sD6/AGxF5J6</latexit> 2(/2) f<latexit sha1_base64="kqE86tsMnwK627aDV9RGBUwEzCY=">AAAB+HicbVDLTsMwENzwLOXRAEcuFhVSuVQJ5XWs4MKxSPQhtVHkuE5r1XEi20EqUb+ECwcQ4sqncONvcNscoGWklcYzu/LuBAlnSjvOt7Wyura+sVnYKm7v7O6V7P2DlopTSWiTxDyWnQArypmgTc00p51EUhwFnLaD0e3Ubz9SqVgsHvQ4oV6EB4KFjGBtJN8uhX6t0qOJYtw8a6e+XXaqzgxombg5KUOOhm9/9foxSSMqNOFYqa7rJNrLsNSMcDop9lJFE0xGeEC7hgocUeVls8Un6MQofRTG0pTQaKb+nshwpNQ4CkxnhPVQLXpT8T+vm+rw2suYSFJNBZl/FKYc6RhNU0B9JinRfGwIJpKZXREZYomJNlkVTQju4snLpHVWdS+rF/fn5fpNHkcBjuAYKuDCFdThDhrQBAIpPMMrvFlP1ov1bn3MW1esfOYQ/sD6/AG0LJJ8</latexit> 3(/3)
Fig.2: Factorized Diffusion.Givenanimage decomposition,wecontrolcompo-
nents of the decomposition through text conditioning during image generation. To do
this, we modify the sampling procedure of a pretrained diffusion model. Specifically,
ateachdenoisingstep,t,weconstructanew noise estimate,ϵ˜,tousefordenoising,
whose components come from components of ϵ , which are noise estimates con-
i
ditioned on different prompts. Here, we show a decomposition into three frequency
subbands, used for creating triple hybrid images, but we consider a number of other
decompositions.
where each f (x) is a component, we can correspond to each component a dif-
i
ferent text prompt y . At each step of the reverse diffusion process, instead
i
of computing a single noise estimate we compute N—one conditioned on each
y —which we denote by ϵ = ϵ (x ,y ,t). We then construct a composite noise
i i θ t i
estimate ϵ˜made up of components from each ϵ :
i
ϵ˜= f (ϵ ). (4)
i i
(cid:88)
Thisnewnoiseestimate,ϵ˜,isusedtoperformthediffusionupdatestep.Ineffect,
each component of the image is denoised while being conditioned by a different
text prompt, resulting in a clean image whose components are conditioned on
the different prompts. We refer to this technique as factorized diffusion.
As noted in Sec. 2, our method is similar to recent work by Tancik [58] and
Geng et al. [18] in that we modify noise estimates with the aim of generating
visual illusions. However, our method differs in that we modify only the noise
estimate, and not the input to the diffusion model, x . As a result, our method
t
produces a different class of perceptual illusions from prior work. Please see
Appendix G for additional discussion and results.
3.3 Analysis of Factorized Diffusion
Togiveintuitionforwhyourmethodworks,wesupposethatourupdatefunction
update(·,·) is a linear combination of the noisy image x and the noise estimate
tFactorized Diffusion: Perceptual Illusions by Noise Decomposition 7
ϵ , as is commonly the case 1 [25,52]. The update function also depends on t,
θ
which we omit for brevity. We may then decompose the update step as
x =update(x ,ϵ ) (5)
t−1 t θ
=update f (x ), f (ϵ ) (6)
i t i θ
= upda(cid:16) t(cid:88) e(f (x ),f(cid:88) (ϵ )) (cid:17) (7)
i t i θ
i
(cid:88)
where the first equality is by definition of the update step, the second equality
is by applying the image decomposition, and the third equality is by linearity of
the update function. Eq. (7) tells us that an update step on x , with ϵ , can be
t θ
interpretedasthesumofupdatesonthecomponentsofx andofϵ .Ourmethod
t θ
can be understood as using different conditioning on each of these components.
Written explicitly, the update our method uses is
N
x = update(f (x ),f (ϵ (x ,y ,t)). (8)
t−1 i t i θ t i
i
(cid:88)
Moreover, let us write out the update step explicitly as
x =update(x ,ϵ )=ω x +γ ϵ , (9)
t−1 t θ t t t θ
for ω and γ determined by the variance schedule and scheduler. Then if the
t t
f ’s are linear, we have
i
f (x )=f (update(x ,ϵ )) (10)
i t−1 i t θ
=f (ω x +γ ϵ ) (11)
i t t t θ
=ω f (x )+γ f (ϵ ) (12)
t i t t i θ
=update(f (x ),f (ϵ )) (13)
i t i θ
meaning that updating the ith component of x with the ith component of ϵ
t θ
will only affect the ith component of x .
t−1
3.4 Decompositions Considered
Wepresentdetailsforthedecompositionsthatweconsiderinthispaper.Results
for all decompositions are presented and discussed in Sec. 4.
Spatial frequencies. We consider factorizing an image into frequency sub-
bands, and conditioning the subbands on different prompts, with the goal of
producing hybrid images [42]. First, we consider a decomposition into two com-
ponents:
x=x−G (x)+G (x), (14)
σ σ
fhigh(x) flow(x)
1 The update may also include ad(cid:124)ding(cid:123)r(cid:122)and(cid:125)om(cid:124)no(cid:123)is(cid:122)e,(cid:125)z ∼ N(0,I), in which case our
analysis still holds with a modification to the argument, discussed in Appendix H.8 D. Geng et al.
where G is a low pass filter implemented as a Gaussian blur with standard
σ
deviation σ, and x−G (x) acts as a high pass of x. For a decomposition into
σ
threesubbands,tomakethetriplehybridimagesinFig.1,componentsarelevels
of a Laplacian pyramid which we define as
x=x−G (x)+G (x)−G (G (x))+G (G (x)) (15)
σ1 σ1 σ2 σ1 σ2 σ1
fhigh(x) fmed(x) flow(x)
where σ
1
and σ 2(cid:124)roug(cid:123)h(cid:122)ly d(cid:125)efin(cid:124)e cutoffs f(cid:123)o(cid:122)r the low,(cid:125)med(cid:124)ium,(cid:123)a(cid:122)nd h(cid:125)igh passes.
Color spaces. We also consider decomposition by color space, with the goal
of creating color hybrids—images with different interpretations when seen in
grayscale or color. Similarly to the CIELAB color space, we decompose an im-
age into a lightness component, L, and a chromaticity component ab. CIELAB
seeks to represent colors in a perceptually uniform space, and therefore requires
nonlineartransformationsofRGBvalues.Instead,weuseasimplelineardecom-
position. Our L component is a channel-wise average of all the pixels
1
f (x)= x , (16)
gray c
3
c∈{R(cid:88),G,B}
where x are the color channels of the image, x, and the resultant f (x) has
c gray
the same shape as x. We define the color component as the residual:
f (x)=x−f (x). (17)
color gray
Motion blurring. Motion blur may be modeled as a convolution with a blur
kernel K [3,15,36,38,57,66]. To produce images that change appearance when
blurred, what we call motion hybrids, we study the following decomposition:
x= K∗x + x−K∗x, (18)
fmotion(x) fres(x)
where we have split an image in(cid:124)to(cid:123)(cid:122)a(cid:125)motio(cid:124)n bl(cid:123)u(cid:122)rred(cid:125) component and a residual
component. We specifically study simple constant velocity motions, in which
K may be modeled as a matrix of zeros with a line of non-zero values. This
may alsobe thoughtof asdecomposing animage into anorientedlow frequency
component, and a residual component.
Spatial decomposition. While our primary focus is on perceptual illusions,
wealsoconsiderspatialmaskingasadecomposition.Givenbinaryspatialmasks
m whose disjoint union covers the entire image, we can use the decomposition
i
x= m ⊙x, (19)
i
(cid:88)i fi(x)
where ⊙ denotes element-wise multiplic(cid:124)ati(cid:123)o(cid:122)n a(cid:125)nd each m i⊙x is a component.
Theeffectofthisdecompositionistoenablecontrolofthepromptsspatially.This
isaspecialcaseofMultiDiffusion[2].WediscussthisconnectioninAppendixE.Factorized Diffusion: Perceptual Illusions by Noise Decomposition 9
Scaling. A final interesting decomposition is of the form x = Na x, for
i i
Na =1.Takinga = 1 recoversthecompositionaldiffusionmethodofLiuet
i i i N (cid:80)
al. [33], in which noise estimates are averaged to sample from conjunctions of
(cid:80)
multiple prompts.
3.5 Inverse Problems
Ifweknowwhatoneofthecomponentsmustbeinourgeneratedimage,perhaps
extracted from some reference image x , we can then fix this component while
ref
generating all other components with our method. This enables us to produce
hybrid images from real images (see Figs. 1 and 8). Without loss of generality,
suppose we want to fix the first component. To do this, we can project x after
t
every reverse process step:
√ √ N
x ←f α x + 1−α ϵ + f (x ) (20)
t 1 t ref t i t
i=2
(cid:0) (cid:1) (cid:88)
whereϵ∼N(0,I),andα isdeterminedbythevarianceschedule.Theargument
t
of f is a sample from the forward process, given the reference image—that is, a
1
noisy version of x with the correct amount of noise for timestep t. Essentially,
ref
weprojectx suchthatitsfirstcomponentmatchesthatofx .Thisamountsto
t ref
solvinga(noiseless)inverseproblemcharacterizedbyy=f (x).Muchworkhas
1
gone into developing methods to solve inverse problems using diffusion models
aspriors,andthisextensionofourmethodcanbeviewedasasimplifiedversion
of prior work [8,28,34,53,64].
4 Results
We provide results organized by decomposition, followed by results on inverse
problems, and then random samples. Additional implementation details can be
found in Appendix A, and additional results can be found in Appendix K.
4.1 Hybrid Images
We show qualitative results in Fig. 1, Fig. 3, and Fig. 4, as well as in Fig. 16
in the appendix. As can be seen, our method produces high quality hybrid im-
ages. Interestingly, we were also able to produce hybrid images with three dif-
ferent prompts (Figs. 1 and 14) by using the Laplacian pyramid decomposition
(Eq. (15)). While prior work [55] has attempted to generate these triple hybrids
withtraditionalmethods,ourmethodfarexceedstheirresultsintermsofquality
and recognizability (for details, see Appendix C).
Effect of blur kernel. In Fig. 3 we show how the strength of the Gaussian
blur,σ,affectsresults.Alowerσ valuecorrespondstoahighercut-offfrequency
onthelow-passfilter,andresultsinthelow-passpromptbeingmoreprominently
featured. Interpolating between σ values gives hybrid images.10 D. Geng et al.
σ<latexit sha1_base64="VbjycshQYH+QMWBZWZ1QN5sjDvQ=">AAAB8XicbVBNSwMxEJ2tX7V+VT16CRbB07IrVr0IRS8eK9gPbJeSTbNtaJJdkqxQlv4LLx4U8eq/8ea/MW33oK0PBh7vzTAzL0w408bzvp3Cyura+kZxs7S1vbO7V94/aOo4VYQ2SMxj1Q6xppxJ2jDMcNpOFMUi5LQVjm6nfuuJKs1i+WDGCQ0EHkgWMYKNlR67mg0Evvbcaq9c8VxvBrRM/JxUIEe9V/7q9mOSCioN4Vjrju8lJsiwMoxwOil1U00TTEZ4QDuWSiyoDrLZxRN0YpU+imJlSxo0U39PZFhoPRah7RTYDPWiNxX/8zqpia6CjMkkNVSS+aIo5cjEaPo+6jNFieFjSzBRzN6KyBArTIwNqWRD8BdfXibNM9e/cKv355XaTR5HEY7gGE7Bh0uowR3UoQEEJDzDK7w52nlx3p2PeWvByWcO4Q+czx95upAl</latexit> =0.5 σ<latexit sha1_base64="d/+78LPVamw6CcfAUH0vf8mGI/k=">AAAB8XicbVBNSwMxEJ2tX7V+VT16CRbB07IrVr0IRS8eK9gPbJeSTbNtaJJdkqxQlv4LLx4U8eq/8ea/MW33oK0PBh7vzTAzL0w408bzvp3Cyura+kZxs7S1vbO7V94/aOo4VYQ2SMxj1Q6xppxJ2jDMcNpOFMUi5LQVjm6nfuuJKs1i+WDGCQ0EHkgWMYKNlR67mg0Evq66Xq9c8VxvBrRM/JxUIEe9V/7q9mOSCioN4Vjrju8lJsiwMoxwOil1U00TTEZ4QDuWSiyoDrLZxRN0YpU+imJlSxo0U39PZFhoPRah7RTYDPWiNxX/8zqpia6CjMkkNVSS+aIo5cjEaPo+6jNFieFjSzBRzN6KyBArTIwNqWRD8BdfXibNM9e/cKv355XaTR5HEY7gGE7Bh0uowR3UoQEEJDzDK7w52nlx3p2PeWvByWcO4Q+czx95xJAl</latexit> =5.0
[low / high] prompt: (cid:210)a photo of a skull(cid:211) / (cid:210)a photo of a snowy mountain village(cid:211)
σ<latexit sha1_base64="cY5d1tx70jrm/R6ByOL9NcPGLX8=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFchaT42ghFNy4r2Ac0oUymk3bozCTMTIQS+htuXCji1p9x5984bbPQ1gMXDufcy733RCln2njet7Oyura+sVnaKm/v7O7tVw4OWzrJFKFNkvBEdSKsKWeSNg0znHZSRbGIOG1Ho7up336iSrNEPppxSkOBB5LFjGBjpSDQbCAwukGeW+tVqp7rzYCWiV+QKhRo9CpfQT8hmaDSEI617vpeasIcK8MIp5NykGmaYjLCA9q1VGJBdZjPbp6gU6v0UZwoW9Kgmfp7IsdC67GIbKfAZqgXvan4n9fNTHwd5kymmaGSzBfFGUcmQdMAUJ8pSgwfW4KJYvZWRIZYYWJsTGUbgr/48jJp1Vz/0r14OK/Wb4s4SnAMJ3AGPlxBHe6hAU0gkMIzvMKbkzkvzrvzMW9dcYqZI/gD5/MHJkKQdg==</latexit> =0.2 σ<latexit sha1_base64="8Ts0+pqCxUCvrhyYihqnV4Lkd9Q=">AAAB83icbVDLSsNAFL3xWeur6tLNYBFchaT42ghFNy4r2Ac0oUymk3bozCTMTIQS+htuXCji1p9x5984bbPQ1gMXDufcy733RCln2njet7Oyura+sVnaKm/v7O7tVw4OWzrJFKFNkvBEdSKsKWeSNg0znHZSRbGIOG1Ho7up336iSrNEPppxSkOBB5LFjGBjpSDQbCAwukE11+tVqp7rzYCWiV+QKhRo9CpfQT8hmaDSEI617vpeasIcK8MIp5NykGmaYjLCA9q1VGJBdZjPbp6gU6v0UZwoW9Kgmfp7IsdC67GIbKfAZqgXvan4n9fNTHwd5kymmaGSzBfFGUcmQdMAUJ8pSgwfW4KJYvZWRIZYYWJsTGUbgr/48jJp1Vz/0r14OK/Wb4s4SnAMJ3AGPlxBHe6hAU0gkMIzvMKbkzkvzrvzMW9dcYqZI/gD5/MHJkaQdg==</latexit> =2.0
[low / high] prompt: (cid:210)a headshot of marilyn monroe(cid:211) / (cid:210)a headshot of albert einstein(cid:211)
Fig.3: Effect of σ. We show a linear sweep over the σ value used in our hybrid
decomposition. A lower σ results in the low pass prompt being more prominent, and
vice-versa. In between lies hybrid images. Best viewed digitally, with zoom.
(a) (cid:210)leopard(cid:211)/(cid:210)elephant(cid:211) (b) (cid:210)stern woman(cid:211)/(cid:210)angry man(cid:211) (c) (cid:210)bicycle(cid:211)/(cid:210)motorcycle(cid:211) (d) (cid:210)sad woman(cid:211)/(cid:210)happy woman(cid:211)
(e) (cid:210)cat(cid:211)/(cid:210)dog(cid:211) (f) (cid:210)einstein(cid:211)/(cid:210)marilyn monroe(cid:211) (g) (cid:210)dolphin(cid:211)/(cid:210)car(cid:211) (h) (cid:210)cat(cid:211)/(cid:210)woman(cid:211)
Fig.4: Comparison to Oliva et al. [42]. We take hybrid images from Oliva et
al. [42], and generate our own versions. Left is from our method, and right is from
Oliva et al.’s. As can be seen, our method produces much more realistic images while
still containing both subjects. Best viewed digitally, with zoom.
Comparisons to Oliva et al. [42]. In Fig. 4 we qualitatively compare our
methodtosamplesfromOlivaetal.[42].Wedirectlytakesamplesfrom[42],and
manually create prompts to generate corresponding hybrids using our method.
As can be seen, our hybrids are considerably more realistic, while containing
thedesiredpromptsatdifferentviewingdistances.Oneadvantageourtechnique
hasisthatthelowfrequencyandhighfrequencycomponentsaregeneratedwith
knowledgeofeachother,asthediffusionmodelisgiventheentireimage.Thisis
in contrast to the hybrid images of Oliva et al., in which frequency components
are extracted from two independent images and combined. Moreover, these two
images must be found and made to align manually, whereas our method simply
generates low and high frequency components that align well.
We also provide quantitative comparisons between our hybrid images and
those of Oliva et al. [42]. In Tab. 1 we show results of a two-alternative forced
choice (2AFC) study, in which human participants are asked to choose between
our hybrid images or Oliva et al.’s. Participants of the study were asked which
image better contained the prompts, and which of the images were of higher
overall quality. For details, please see Appendix B. We find that participants
consistently choose our images as being both higher in quality and better con-
taining the prompts.Factorized Diffusion: Perceptual Illusions by Noise Decomposition 11
Table 1: Human Studies. We compare our hybrid images and Oliva et al.’s with
a two-alternative forced choice test. Participants were shown results from Fig. 4, and
were asked which images better contained the prompts, and which were of higher
overall quality. Percentages denote the proportion that chose our method. Please see
AppendixBforadditionaldetails.Wefindthatourmethodisratedasbothhigherin
quality and better aligned with the prompts. (N =77)
(a) (b) (c) (d) (e) (f) (g) (h) Average
HighPrompt 70.1% 81.8% 63.6% 51.9% 61.0% 53.2% 70.1% 54.5% 63.3%
LowPrompt 83.1% 84.4% 74.0% 87.0% 93.5% 87.0% 75.3% 81.8% 83.3%
Quality 92.2% 87.0% 83.1% 77.9% 85.7% 79.2% 92.2% 33.8% 78.9%
Table2:HybridImageCLIPEvaluation.Weevaluatehybridimagesbyreporting
the maximum clip score over different amounts of blurring. We report the max to
compensate for the fact that different hybrid images may be best viewed at different
resolutions. Please see Fig. 4 for the referenced hybrid images, and Appendix D for
metric implementation details.
Method (a) (b) (c) (d) (e) (f) (g) (h) Average
LowPass
Olivaetal.[42] 0.268 0.258 0.316 0.250 0.237 0.264 0.257 0.241 0.261
Ours 0.286 0.252 0.307 0.273 0.275 0.260 0.244 0.269 0.271
HighPass
Olivaetal.[42] 0.297 0.230 0.306 0.272 0.276 0.306 0.260 0.231 0.272
Ours 0.321 0.242 0.301 0.258 0.292 0.324 0.320 0.277 0.292
Finally, we present CLIP alignment scores in Tab. 2. To account for the fact
that the hybrid images are best viewed at many different resolutions, we report
themaximumCLIPscorebetweenthepromptandtheimageblurredbydifferent
amounts.PleaseseeAppendixDformetricimplementationdetails.Wefindthat
our method generates hybrid images with better alignment to the prompts.
4.2 Other Decompositions
Color hybrids. WeprovidequalitativecolorhybridresultsinFig.1andFig.5,
as well as in Fig. 17 in the appendix. As can be seen, the grayscale image aligns
withoneprompt,whilethecolorimagealignswithanother.Forexample,inthe
"rabbit"/"volcano" image from Fig. 5 the ears of the rabbit are repurposed
as plumes of lava in the grayscale image. Note that it is not sufficient to simply
add arbitrary amounts of color to a grayscale image to achieve this effect, as
the colors added must not change the alignment of the grayscale image with its
prompt. One interesting application of this technique is to produce images that
appear different under bright lighting versus dim lightning, where human vision
has a much harder time discerning color.
Motion hybrids We provide qualitative motion hybrid results in Fig. 1 and
Fig. 6, as well as in Fig. 15 in the appendix. These are images that change
appearance when motion blurred. For all motion hybrids in the paper, we use a
blur kernel of K= 1I∈Rk×k, with k =29, corresponding to a diagonal motion
k
from upper left to bottom right.12 D. Geng et al.
(cid:210)a painting of (cid:210)a painting of (cid:210)oil painting style, (cid:210)a photo of (cid:210)a photo of the (cid:210)a painting of a
a landscape(cid:211) a volcano(cid:211) the grand canyon(cid:211) a bird(cid:211) grand canyon(cid:211) dining table(cid:211)
(cid:210)a p aa i tn it gi en rg (cid:211) of (cid:210)a ap a ri an bt bi in tg (cid:211) of s(cid:210) to yi ll e ,p a ai n bt ii rn dg (cid:211) (cid:210)a photo of a frog(cid:211) (cid:210)a p hh eo at ro t (cid:211)of a (cid:210) aa pp oa li an rt i bn eg a ro (cid:211)f
Fig.5: Color Hybrids. We show additional color hybrid results. These are images
thatchangeappearancewhencolorisaddedorsubtractedaway.Theseimageschange
appearance when moved from bright to dim lighting, in which color is harder to see.
(cid:210)a photo of a van (cid:210)a photo of (cid:210)a photo of a (cid:210)a photo of (cid:210)a photo of a (cid:210)a photo of new
gogh portrait(cid:211) a duck(cid:211) mountain(cid:211) a canyon(cid:211) stadium(cid:211) york city(cid:211)
(cid:210) sa u np fh lo ot wo e ro sf (cid:211) (cid:210)a a rph ao bt bo i t(cid:211)of abr(cid:210) aa h ap mh o lt io n co of l n(cid:211) (cid:210)a a p ph ao nt do a (cid:211)of (cid:210)a photo of a car(cid:211) (cid:210) ta ep dh do y to b eaof r (cid:211)a
Fig.6:MotionHybrids.Weshowadditionalmotionhybridresults.Theseareimages
that change appearance when motion blurred. Here, the motion from upper left to
bottom right.
Spatial decomposition Bydecomposinganimageintodisjointspatialregions
and applying our method, we can recover a technique that is a special case of
MultiDiffusion [2]. Using this method, we can effect fine-grained control over
where the text prompts act spatially, as shown in Fig. 7. For additional discus-
sion, please see Appendix E.
Scaling decomposition By using the scaling decomposition with a = 1, our
i N
method reduces exactly to prior work on compositionality in diffusion models,
by Liu et al. [33]. Specifically, we recover the conjunction operator proposed by
Liu et al. We demonstrate this in Fig. 7, but refer the reader to [33] for more
examples.
4.3 Inverse Problems
As discussed in Sec. 3.5, we can modify our approach to solve inverse problems,
resulting in a technique highly similar to prior work [8–10,28,34,54,64]. While
noitarutaseD SaturationFactorized Diffusion: Perceptual Illusions by Noise Decomposition 13
Spatial Decomposition Scaling Decomposition
(cid:210)yosemite(cid:211)
(cid:210)eggs(cid:211) (cid:210)bacon(cid:211)
(cid:210)dog(cid:211)
(cid:210)(cid:210)
a
pya uo
p
rsp
h
pAeh
o
lNmo
t
eit oD to se
o
k(cid:211)o
f y
f
(cid:211) a
(cid:210) (cid:210)ya bo lsp iAeh zNmo zit D ato re d(cid:211)o
(cid:211)
f
Fig.7: Spatial and Scaling Decompositions. Specialcasesofourmethodreduce
to prior work. (Left) Decomposing images into spatial regions recovers a special case
of MultiDiffusion [2], and allows us to assign prompts to spatial regions. (Right)
Decomposing an image by scaling allows us to compose concepts, and recovers the
method of Liu et al. [33].
Low Pass: High Pass: Low Pass: High Pass: High Pass: Low Pass:
[Real Image] (cid:210) aa lp eh oo pt ao r do (cid:211)f [Real Image] a(cid:210) a l ip gh ho tt bo u lo bf (cid:211) [Real Image] m(cid:210) aa r ih le ya nd s mh oo nt r oo ef (cid:211)
Fig.8: Hybrids from Real Images. We show hybrid images generated from real
images. We take low or high passes of real images, and use our method to fill in the
missing component, conditioned on a prompt. Best viewed digitally, with zoom.
previous work investigates using diffusion priors for solving problems such as
colorization, inpainting, super-resolution, or phase retrieval, we apply the idea
towards generating hybrid images from real images. Specifically, we take low or
high frequency components from a real image and use our method to fill in the
missing components, conditioned on a prompt. Results are shown in Fig. 1 and
Fig. 8. We also provide colorization results in Appendix J.
4.4 Limitations and Negative Impacts
One major limitation of our method is that the success rate is relatively low.
While our method can produce decent images consistently, very high quality
images are rarer. This can be seen in Fig. 9 and Fig. 18, in which we visual-
ize random samples for hybrid images, color hybrids, and motion hybrids. We
attribute this fragility to the fact that our method produces images that are
highly out-of-distribution for the diffusion model. In addition, there is no mech-
anism by which prompts associated with one component are discouraged from
appearing in other components. Another failure case of our method is that the
prompt for one component may dominate the generated image. Empirically, the
success rate of our method can be improved by carefully choosing prompt pairs
(seeAppendixIforadditionaldiscussion),orbymanuallytuningdecomposition
parameters, but we leave improving the robustness of our method in general to
future work.
Theabilitytobettercontrolpowerfulimagesynthesismodelsopensnumerous
societalandethicalconsiderations.Weapplyourmethodtogeneratingillusions,14 D. Geng et al.
High Pass: (cid:210)a photo
of a rabbit(cid:211)
Low Pass: (cid:210)a photo
of an old man(cid:211)
High Pass: (cid:210)a watercolor
of mountains(cid:211)
Low Pass: (cid:210)a watercolor
of a panda(cid:211)
Grayscale: (cid:210)an oil
painting of waterfalls(cid:211)
Full Color: (cid:210)an oil
painting of a tiger(cid:211)
Grayscale: (cid:210)a watercolor
painting of flower
arrangements(cid:211)
Full Color: (cid:210)a watercolor
painting of a duck(cid:211)
Still: (cid:210)a photo
of a duck(cid:211)
Motion: (cid:210)a photo of
a rabbit(cid:211)
Still: (cid:210)a photo
of an old man(cid:211)
Motion: (cid:210)a photo of
a skull(cid:211)
Fig.9: Random Samples. We provide random samples for selected prompts and
decompositions. As can be seen, most random results are of passable quality, with
somecatastrophicfailures,andsomeveryhighqualityillusions.Morerandomsamples
are shown in Fig. 18.
whichinsomesenseseekstodeceiveperception,possiblyleadingtoapplications
inmisinformation.Webelievethisandotherconcernsdeservefurtherstudyand
careful thought.
5 Conclusion
We present a zero-shot method that enables control over different components
of an image through diffusion model sampling and apply it to the task of creat-
ing perceptual illusions. Using our method, we synthesize hybrid images, hybrid
imageswiththreeprompts,andnewclassesofillusionssuchascolorhybridsand
motion hybrids. We give an analysis and provide intuition for why our method
works. For certain image decompositions, we show that our method reduces to
prior work on compositional generation and spatial control of diffusion mod-
els. Finally, we make a connection to inverse problems, and use this insight to
generate hybrid images from real images.
Acknowledgements. We thank Patrick Chao, Aleksander Holynski, Richard
Zhang, Trenton Chang, Utkarsh Singhal, Huijie Zhang, Bowen Song, Jeongsoo
Park,JeongJoonPark,JeffreyFessler,LiyueShen,QingQu,AntonioTorralba,
and Alexei Efros for helpful discussions. Daniel is supported by the National
Science Foundation Graduate Research Fellowship under Grant No. 1841052.Factorized Diffusion: Perceptual Illusions by Noise Decomposition 15
References
1. Bansal, A., Chu, H.M., Schwarzschild, A., Sengupta, S., Goldblum, M., Geiping,
J., Goldstein, T.: Universal guidance for diffusion models (2023) 3, 4
2. Bar-Tal,O.,Yariv,L.,Lipman,Y.,Dekel,T.:Multidiffusion:Fusingdiffusionpaths
for controlled image generation. arXiv preprint arXiv:2302.08113 (2023) 3, 4, 8,
12, 13, 20
3. Brooks,T.,Barron,J.T.:Learningtosynthesizemotionblur.In:Proceedingsofthe
IEEE/CVF Conference on Computer Vision and Pattern Recognition. pp. 6840–
6848 (2019) 8
4. Brooks, T., Holynski, A., Efros, A.A.: Instructpix2pix: Learning to follow image
editing instructions. In: CVPR (2023) 4
5. Burgert,R.,Ranasinghe,K.,Li,X.,Ryoo,M.:Diffusionillusions:Hidingimagesin
plainsight.https://ryanndagreat.github.io/Diffusion-Illusions(Mar2023)
4, 21, 22
6. Chandra, K., Li, T.M., Tenenbaum, J., Ragan-Kelley, J.: Designing perceptual
puzzlesbydifferentiatingprobabilisticprograms.In:ACMSIGGRAPH2022Con-
ference Proceedings. pp. 1–9 (2022) 4
7. Chu, H.K., Hsu, W.H., Mitra, N.J., Cohen-Or, D., Wong, T.T., Lee, T.Y.: Cam-
ouflage images. ACM Trans. Graph. 29(4), 51–1 (2010) 4
8. Chung,H.,Kim,J.,Mccann,M.T.,Klasky,M.L.,Ye,J.C.:Diffusionposteriorsam-
pling for general noisy inverse problems. arXiv preprint arXiv:2209.14687 (2022)
3, 9, 12
9. Chung, H., Sim, B., Ryu, D., Ye, J.C.: Improving diffusion models for inverse
problems using manifold constraints. Advances in Neural Information Processing
Systems 35, 25683–25696 (2022) 3, 12, 24
10. Chung, H., Sim, B., Ye, J.C.: Come-closer-diffuse-faster: Accelerating conditional
diffusionmodelsforinverseproblemsthroughstochasticcontraction.In:Proceed-
ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.
pp. 12413–12422 (2022) 3, 12
11. Dhariwal,P.,Nichol,A.:Diffusionmodelsbeatgansonimagesynthesis.Advances
in neural information processing systems 34, 8780–8794 (2021) 3
12. Du, Y., Li, S., Mordatch, I.: Compositional visual generation with energy based
models.AdvancesinNeuralInformationProcessingSystems33,6637–6647(2020)
4
13. Elsayed, G., Shankar, S., Cheung, B., Papernot, N., Kurakin, A., Goodfellow, I.,
Sohl-Dickstein,J.:Adversarialexamplesthatfoolbothcomputervisionandtime-
limited humans. Advances in neural information processing systems 31 (2018) 4
14. Epstein,D.,Jabri,A.,Poole,B.,Efros,A.A.,Holynski,A.:Diffusionself-guidance
for controllable image generation (2023) 4
15. Fergus, R., Singh, B., Hertzmann, A., Roweis, S.T., Freeman, W.T.: Removing
camera shake from a single photograph. In: Acm Siggraph 2006 Papers, pp. 787–
794 (2006) 8
16. Freeman, W.T., Adelson, E.H., Heeger, D.J.: Motion without movement. ACM
Siggraph Computer Graphics 25(4), 27–30 (1991) 4
17. Geng, D., Owens, A.: Motion guidance: Diffusion-based image editing with differ-
entiablemotionestimators.InternationalConferenceonLearningRepresentations
(2024) 3
18. Geng, D., Park, I., Owens, A.: Visual anagrams: Generating multi-view optical
illusionswithdiffusionmodels.ComputerVisionandPatternRecognition(CVPR)
2024 (2024) 4, 5, 6, 21, 2216 D. Geng et al.
19. Gomez-Villa,A.,Martin,A.,Vazquez-Corral,J.,Bertalmío,M.:Convolutionalneu-
ralnetworkscanbedeceivedbyvisualillusions.In:ProceedingsoftheIEEE/CVF
conference on computer vision and pattern recognition. pp. 12309–12317 (2019) 4
20. Goodfellow, I.J., Shlens, J., Szegedy, C.: Explaining and harnessing adversarial
examples. arXiv preprint arXiv:1412.6572 (2014) 4
21. Gu, Z., Davis, A.: Filtered-guided diffusion: Fast filter guidance for black-box dif-
fusion models. arXiv preprint arXiv:2306.17141 (2023) 3, 4
22. Guo,R.,Collins,J.,deLima,O.,Owens,A.:Ganmouflage:3dobjectnondetection
with texture fields. Computer Vision and Pattern Recognition (CVPR) (2023) 4
23. Hertz, A., Mokady, R., Tenenbaum, J., Aberman, K., Pritch, Y., Cohen-Or, D.:
Prompt-to-prompt image editing with cross attention control (2022) 4
24. Hertzmann, A.: Visual indeterminacy in gan art. In: ACM SIGGRAPH 2020 Art
Gallery, pp. 424–428 (2020) 4
25. Ho,J.,Jain,A.,Abbeel,P.:Denoisingdiffusionprobabilisticmodels.arXivpreprint
arxiv:2006.11239 (2020) 3, 4, 7, 22
26. Huberman-Spiegelglas, I., Kulikov, V., Michaeli, T.: An edit friendly ddpm noise
space: Inversion and manipulations. arXiv preprint arXiv:2304.06140 (2023) 4
27. Jaini, P., Clark, K., Geirhos, R.: Intriguing properties of generative classifiers.
arXiv preprint arXiv:2309.16779 (2023) 4
28. Kawar, B., Elad, M., Ermon, S., Song, J.: Denoising diffusion restoration models.
AdvancesinNeuralInformationProcessingSystems35,23593–23606(2022) 3,9,
12, 24
29. Konstantinov, M., Shonenkov, A., Bakshandaeva, D., Ivanova, K.: If by deepfloyd
labatstabilityai(2023),https://github.com/deep-floyd/IF/,gitHubrepository
4, 19
30. Labs, M.: Controlnet qr code monster v2 for sd-1.5 (July 2023), https://
huggingface.co/monster-labs/control_v1p_sd15_qrcode_monster 4
31. Lee, Y., Kim, K., Kim, H., Sung, M.: Syncdiffusion: Coherent montage via syn-
chronized joint diffusions. In: Thirty-seventh Conference on Neural Information
Processing Systems (2023) 3, 4
32. Liu, N., Li, S., Du, Y., Tenenbaum, J., Torralba, A.: Learning to compose visual
relations. Advances in Neural Information Processing Systems 34, 23166–23178
(2021) 3
33. Liu, N., Li, S., Du, Y., Torralba, A., Tenenbaum, J.B.: Compositional visual gen-
eration with composable diffusion models. In: European Conference on Computer
Vision. pp. 423–439. Springer (2022) 4, 9, 12, 13
34. Lugmayr, A., Danelljan, M., Romero, A., Yu, F., Timofte, R., Van Gool, L.: Re-
paint:Inpaintingusingdenoisingdiffusionprobabilisticmodels.In:Proceedingsof
theIEEE/CVFconferenceoncomputervisionandpatternrecognition.pp.11461–
11471 (2022) 3, 9, 12
35. Meng,C.,He,Y.,Song,Y.,Song,J.,Wu,J.,Zhu,J.Y.,Ermon,S.:SDEdit:Guided
imagesynthesisandeditingwithstochasticdifferentialequations.In:International
Conference on Learning Representations (2022) 4
36. Mildenhall, B., Barron, J.T., Chen, J., Sharlet, D., Ng, R., Carroll, R.: Burst
denoisingwithkernelpredictionnetworks.In:ProceedingsoftheIEEEconference
on computer vision and pattern recognition. pp. 2502–2510 (2018) 8
37. Mokady,R.,Hertz,A.,Aberman,K.,Pritch,Y.,Cohen-Or,D.:Null-textinversion
for editing real images using guided diffusion models (2022) 4
38. Nayar, S.K., Ben-Ezra, M.: Motion-based motion deblurring. IEEE transactions
on pattern analysis and machine intelligence 26(6), 689–698 (2004) 8Factorized Diffusion: Perceptual Illusions by Noise Decomposition 17
39. Ngo, J., Sankaranarayanan, S., Isola, P.: Is clip fooled by optical illusions? (2023)
4
40. Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin, P., McGrew, B.,
Sutskever,I.,Chen,M.:Glide:Towardsphotorealisticimagegenerationandediting
with text-guided diffusion models (2021) 3, 4
41. Oliva, A., Schyns, P.G.: Coarse blobs or fine edges? evidence that information di-
agnosticitychangestheperceptionofcomplexvisualstimuli.Cognitivepsychology
34 (1997) 4
42. Oliva, A., Torralba, A., Schyns, P.G.: Hybrid images. ACM Trans. Graph. 25(3),
527–532 (jul 2006). https://doi.org/10.1145/1141911.1141919, https://doi.
org/10.1145/1141911.1141919 1, 2, 4, 7, 10, 11, 20
43. Owens,A.,Barnes,C.,Flint,A.,Singh,H.,Freeman,W.:Camouflaginganobject
from many viewpoints (2014) 4
44. Parmar,G.,Singh,K.K.,Zhang,R.,Li,Y.,Lu,J.,Zhu,J.Y.:Zero-shotimage-to-
image translation (2023) 4
45. Poole, B., Jain, A., Barron, J.T., Mildenhall, B.: Dreamfusion: Text-to-3d using
2d diffusion. arXiv (2022) 4, 21
46. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition (CVPR) (2022), https:
//github.com/CompVis/latent-diffusionhttps://arxiv.org/abs/2112.10752
4
47. Ruiz, N., Li, Y., Jampani, V., Pritch, Y., Rubinstein, M., Aberman, K.: Dream-
booth: Fine tuning text-to-image diffusion models for subject-driven generation
(2022) 3, 4
48. Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Denton, E., Ghasemipour,
S.K.S., Ayan, B.K., Mahdavi, S.S., Lopes, R.G., Salimans, T., Ho, J., Fleet, D.J.,
Norouzi,M.:Photorealistictext-to-imagediffusionmodelswithdeeplanguageun-
derstanding (2022) 4
49. Schyns, P.G., Oliva, A.: From blobs to boundary edges: Evidence for time-
andspatial-scale-dependentscenerecognition.Psychologicalscience5(4),195–200
(1994) 1, 4
50. Schyns, P.G., Oliva, A.: Dr. angry and mr. smile: when categorization flexibly
modifies the perception of faces in rapid visual presentations. Cognitive 69 (1999)
4
51. Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., Ganguli, S.: Deep unsuper-
vised learning using nonequilibrium thermodynamics. In: Bach, F., Blei, D. (eds.)
Proceedings of the 32nd International Conference on Machine Learning. Proceed-
ings of Machine Learning Research, vol. 37, pp. 2256–2265. PMLR, Lille, France
(07–09 Jul 2015), https://proceedings.mlr.press/v37/sohl-dickstein15.
html 3
52. Song, J., Meng, C., Ermon, S.: Denoising diffusion implicit models.
arXiv:2010.02502 (October 2020), https://arxiv.org/abs/2010.02502 3, 4, 5, 7
53. Song,Y.,Shen,L.,Xing,L.,Ermon,S.:Solvinginverseproblemsinmedicalimag-
ingwithscore-basedgenerativemodels.arXivpreprintarXiv:2111.08005(2021) 3,
9
54. Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Ermon,S.,Poole,B.:Score-
based generative modeling through stochastic differential equations. In: Interna-
tionalConferenceonLearningRepresentations(2021),https://openreview.net/
forum?id=PxTIG12RRHS 3, 12, 2418 D. Geng et al.
55. Sripian, P., Yamaguchi, Y.: Hybrid image of three contents. Visual computing for
industry, biomedicine, and art 3(1), 1–8 (2020) 9, 20
56. Szegedy,C.,Zaremba,W.,Sutskever,I.,Bruna,J.,Erhan,D.,Goodfellow,I.,Fer-
gus, R.: Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199
(2013) 4
57. Takeda,H.,Milanfar,P.:Removingmotionblurwithspace–timeprocessing.IEEE
Transactions on Image Processing 20(10), 2990–3000 (2011) 8
58. Tancik, M.: Illusion diffusion. https://github.com/tancik/Illusion-Diffusion
(Feb 2023) 4, 6
59. Tumanyan,N.,Geyer,M.,Bagon,S.,Dekel,T.:Plug-and-playdiffusionfeaturesfor
text-drivenimage-to-imagetranslation.In:ProceedingsoftheIEEE/CVFConfer-
enceonComputerVisionandPatternRecognition(CVPR).pp.1921–1930(June
2023) 4
60. Ugleh: Spiral town - different approach to qr monster. https://www.reddit.com/
r/StableDiffusion/comments/16ew9fz/spiral_town_different_approach_to_
qr_monster/ (September 2023), https://www.reddit.com/r/StableDiffusion/
comments/16ew9fz/spiral_town_different_approach_to_qr_monster/ 4
61. Wallace,B.,Gokul,A.,Naik,N.:Edict:Exactdiffusioninversionviacoupledtrans-
formations.In:ProceedingsoftheIEEE/CVFConferenceonComputerVisionand
Pattern Recognition. pp. 22532–22541 (2023) 4
62. Wang, X., Bylinskii, Z., Hertzmann, A., Pepperell, R.: Toward quantifying ambi-
guitiesinartisticimages.ACMTransactionsonAppliedPerception(TAP)17(4),
1–10 (2020) 4
63. Wang, X., Kontkanen, J., Curless, B., Seitz, S., Kemelmacher, I., Mildenhall, B.,
Srinivasan, P., Verbin, D., Holynski, A.: Generative powers of ten. arXiv preprint
arXiv:2312.02149 (2023) 4
64. Wang,Y.,Yu,J.,Zhang,J.:Zero-shotimagerestorationusingdenoisingdiffusion
null-space model. arXiv preprint arXiv:2212.00490 (2022) 3, 9, 12
65. Wu,C.H.,DelaTorre,F.:Unifyingdiffusionmodels’latentspace,withapplications
to cyclediffusion and guidance. arXiv preprint arXiv:2210.05559 (2022) 4
66. Yitzhaky,Y.,Mor,I.,Lantzman,A.,Kopeika,N.S.:Directmethodforrestoration
of motion-blurred images. JOSA A 15(6), 1512–1519 (1998) 8
67. Zhang, L., Rao, A., Agrawala, M.: Adding conditional control to text-to-image
diffusion models (2023) 3, 4
68. Zhang, Q., Song, J., Huang, X., Chen, Y., Liu, M.Y.: Diffcollage: Parallel genera-
tionoflargecontentwithdiffusionmodels.arXivpreprintarXiv:2303.17076(2023)
4Factorized Diffusion: Perceptual Illusions by Noise Decomposition 19
A Implementation Details
A.1 Pixel Diffusion Model
For all experiments we use the pixel diffusion model DeepFloyd IF [29], as op-
posed to more common latent diffusion models. This is because the frequency
subband, color space, and motion decompositions are not meaningful in latent
space.Forexample,averagingchannelsinlatentspacedoesnotcorrespondtoan
interpretable image manipulation. Interestingly, using our method to construct
hybrid images with a latent diffusion model, by blurring latent codes, works to
an extent but is easily susceptible to artifacts (see Appendix F), so we opt to
use a pixel diffusion model which is more consistent and principled.
A.2 Hybrid Images
DeepFloydIF[29]generatesimagesintwostages.Firstataresolutionof64×64
andthenat256×256.Becauseofthis,weadopttheconventionthatourσvalues
arespecifiedforthe64×64scale,andarescaledby4×forthe256×256images.
Weusearelativelylargekernelsizeof33atbothscalestominimizeedgeeffects.
We use σ values ranging from σ = 1.0 to σ = 3.0 for all hybrid images except
for those in Fig. 3, in which we sweep the value of σ.
A.3 Triple Hybrids
Triple hybrids are quite difficult to synthesize, and as such we manually select
the sigma values and prompts to generate high-quality samples. Specifically, we
use σ values from σ =0.8 to σ =1.0 and σ values from σ =1.2 to σ =2.0
1 1 1 2 2 2
for all triple hybrids in Fig. 1 and Fig. 14.
A.4 Upscaling
DeepFloyd IF additionally uses a third stage which upscales from 256×256 to
1024×1024. We also use this stage, but because it is a latent model, we do
not apply our method. We upscale using only the prompt corresponding to the
highest frequency component or the color component.
B Human Studies
WeuseAmazonMechanicalTurkforthehumanstudy.77“masterworkers” were
asked the following questions for each hybrid image pair:
• “Which image shows [prompt_1] clearer?”
• “Which image shows [prompt_2] clearer?”
• “Which image is of higher quality?”
For low frequency prompt questions, we downsample the images accordingly
in order to help participants more easily see the content. For the high frequency
prompt questions, as well as the quality questions, we display the images at full
resolution. Participants were shown 8 hybrid image pairs in a random order.20 D. Geng et al.
(cid:210)city scene(cid:211) / (cid:210)a car(cid:211) / (cid:210)a cat(cid:211) (cid:210)text(cid:211) / (cid:210)a clock on a desk(cid:211) / (cid:210)the digit 5(cid:211)
Fig.10: Prior Work on Triple Hybrid Images.Weshowthetriplehybridresults
from prior work [55], which adapts the classic method of [42]. A description of what
shouldbeseenisprovidedunderneatheachimage,goingfromhightolowfrequencies.
As can be seen, these results are of lower quality than our results.
C Prior Triple Hybrid Methods
Prior work [55] attempts to create triple hybrid images by adapting the method
of Oliva et al. [42]. As can be seen in Fig. 10, the results are not of high visual
quality, and it can be hard to identify the three different subjects in the image,
especially when compared to our results. This reflects the difficulty of creating
these images.
D Metrics Implementation
In Tab. 2, we report the max CLIP score over multiple image downsampling
factors. Specifically, for each hybrid image we downsample and then upsample
by a factor f, where we choose f to be a linear sweep of 20 values between 1
and 8. These images are then preprocessed to a size of 224×224, which is the
input resolution of the CLIP ViT-B/32 model which we use. We then take the
normalized dot product between each resulting image embedding, and the text
embedding for the corresponding prompt, and report the max. We report the
maxtoaccountforthefactthatdifferenthybridimagesarebestseenatdifferent
downsampling factors.
E Connection to MultiDiffusion
In Sec. 4.2 we explore Factorized Diffusion with a spatial decomposition, and
show that it allows targeting of prompts to specific spatial regions. We claim
that this is a special case of MultiDiffusion [2]. MultiDiffusion updates a noisy
image of arbitrary size by removing the consensus of multiple noise estimates
overtheimage.FactorizedDiffusion,withaspatialdecomposition,alsoremovesFactorized Diffusion: Perceptual Illusions by Noise Decomposition 21
(cid:210)a photo of marilyn monroe(cid:211) / (cid:210)a photo of albert einstein(cid:211)
Fig.11: Latent Hybrid Images.Weprovidehybridimageresultsusingourmethod
with Stable Diffusion v1.5, a latent diffusion model. As can be seen the results are
passable, but suffer from artifacts, due to applying blurring and bandpass operations
in the latent space.
a consensus of multiple noise estimates. However, in our setup this consensus
is formed specifically by the disjoint union of multiple noise estimates, and our
method operates only at the resolution for which the diffusion model is trained,
as opposed to MultiDiffusion.
F Hybrid Images with Latent Diffusion Models
We show hybrid images resulting from using our method with Stable Diffusion
v1.5, a latent diffusion model, in Fig. 11. As can be seen the results are decent,
buthavesignificantartifacts,duetoapplyingbandpassfiltersinthelatentspace.
We find that pixel diffusion models produce much higher quality samples.
G Synthesizing Hybrid Images with other Methods
We also attempt to generate hybrid images using two recent methods: Visual
Anagrams [18] and Diffusion Illusions [5]. Results can be seen in Fig. 12. Both
methods fail, which we describe and analyze below.
Diffusion Illusions works by minimizing an SDS [45] loss over multiple views
of an image, paired with different prompts. We use the same high and low pass
views as above. As can be seen in Fig. 12 the method produces a decent version
ofthelowpassprompt,butfailstoincorporateanyofthehighpassprompt.We
believe this is because taking the high pass of an image moves it significantly
out-of-distribution,renderingtheSDSgradientsunhelpful.Lowpassinganimage
alters its appearance, but keeps it relatively in-distribution, so as a result the
method can still produce the low pass prompt.
Visual Anagrams works by denoising multiple transformations of an image,
paired with different prompts. We use a high pass and low pass transformation,
but this fails because these operations change the statistics of the noise in the
noisy image. As a result, the diffusion model is being fed out-of-distribution
images, and the reverse process fails to converge, as shown in Fig. 12.22 D. Geng et al.
Visual Anagrams Diffusion Illusions Ours
(cid:210)a lithograph of a skull(cid:211) / (cid:210)a lithograph of waterfalls(cid:211)
Fig.12: Other Illusion Methods.WeattempttocreatehybridimagesusingVisual
Anagrams [18] and Diffusion Illusions [5], two recent methods designed to generate
opticalillusions.Ascanbeseen,bothmethodsfail.PleaseseeAppendixGforanalysis.
Table3:ComparisontoVisualAnagrams[18].Weuse[18]tosynthesizehybrids
andcolorhybrids,andreportthesamemetricsas[18].Weusepromptpairsbuiltfrom
theCIFAR-10classes,with10promptsperpairforatotalof900samples.Ourmethod
performsconsistentlybetter,as[18]isnotdesignedtoproducethesekindsofillusions.
Task Method A↑ A0.9↑ A0.95↑ C↑ C0.9↑ C0.95↑
VisualAnagrams[18] 0.226 0.237 0.240 0.500 0.520 0.525
HybridImages
Ours 0.237 0.263 0.271 0.536 0.630 0.651
VisualAnagrams[18] 0.223 0.232 0.234 0.500 0.537 0.547
ColorHybrids
Ours 0.231 0.260 0.269 0.512 0.562 0.586
Finally, we also quantitatively evaluate hybrid and color hybrids generated
usingGengetal.[18]againstourproposedmethod,withresultsshowninTab.3.
As prompts, we use all pairs of CIFAR-10 classes, and sample 10 images per
prompt pair for a total of 900 samples. We use the same metrics as [18], and
we find that our method does better consistently, as [18] was not designed to
generate these illusions.
H Further Analysis of Factorized Diffusion
As discussed in Sec. 3.3, our analysis assumes that the update step is a linear
combination of the noisy image, x , and the noise estimate, ϵ . However, many
t θ
commonlyusedupdatestepsalsoinvolveaddingrandomnoisez∼N(0,I),such
as DDPM [25]. To deal with this, we can view the update step as a composition
of two steps:
x =update(x ,ϵ ) (21)
t−1 t θ
=update’(x ,ϵ )+σ z. (22)
t θ z
The first is now a linear combination of x and ϵ , and the second adds in the
t θ
noise z. Our analysis then applies to just the update’ function.Factorized Diffusion: Perceptual Illusions by Noise Decomposition 23
I Choosing Prompts
We find that carefully choosing prompts can generate higher quality illusions.
For example, the success rate and quality of samples are much higher when
at least one prompt is of a “flexible” subject, such as "houseplants" or "a
canyon". In addition, we found biases specific to decompositions. Prompts with
thestyle"photo of..."performedbetterforhybridandmotionhybridimages.
We suspect this is because photos tend to have ample amounts of both high
and low frequency content, as opposed to styles such as "oil paintings" or
"watercolors", which tend to lack higher frequency content. For color hybrids,
wefoundthatusingthestyleof "watercolor"producedbetterresults,perhaps
because of the style’s emphasis on color.
(cid:210)a photo of birds(cid:211)
(cid:210)a photo of (cid:210)(cid:201)mossy(cid:211) (cid:210)(cid:201)underwater(cid:211)
vermilion cliffs(cid:211)
(cid:210)a photo of (cid:210)(cid:201)autumn(cid:211) (cid:210)(cid:201)psychedelic(cid:211)
yosemite(cid:211)
Fig.13: Colorization. Our method can also be used to solve inverse problems, such
as colorization. We show grayscale images that we wish to colorize on the left. The
color component is then generated conditioned on the text prompts displayed.
J Colorization
We also show colorization results in Fig. 13, using our method as an inverse
problem solver, as discussed in Sec. 3.5. Specifically, we use the color space
decomposition introduced in Sec. 3.4. During diffusion model sampling we hold24 D. Geng et al.
the grayscale component fixed to the grayscale component of a real image that
we want to colorize, and generate the color component. Note that this is highly
similar to prior work [9,28,54].
K Additional Results
In this section, we provide additional qualitative results. Additional results for
hybrid images and triple hybrids are shown in Fig. 16 and Fig. 14 respectively.
In Fig. 15 and Fig. 17, we provide more examples of motion and color hybrids,
respectively. Finally, we provide more random samples for hybrid images, color
hybrids, and motion hybrids in Fig. 18.
(cid:210)a photo of flower (cid:210)a photo of
arrangements(cid:211) a fish(cid:211)
(cid:210)a photo of (cid:210)a photo of
a rabbit(cid:211) an old man(cid:211)
(cid:210)a photo of (cid:210)a photo of
a yin yang(cid:211) a diamond(cid:211)
(cid:210)a photo of (cid:210)a photo of
the eiffel tower(cid:211) a pyramid(cid:211)
(cid:210)a photo of (cid:210)a photo of
a quokka(cid:211) a rabbit(cid:211)
(cid:210)a photo of (cid:210)a photo of
houseplants(cid:211) flower arrangements(cid:211)
Fig.14: Triple Hybrids. We provide more triple hybrid results. Best viewed digi-
tally, using zoom.
(cid:210)a watercolor of (cid:210)oil painting style, (cid:210)a photo of (cid:210)oil painting style, (cid:210)a photo of (cid:210)a photo of
a bazaar(cid:211) the grand canyon(cid:211) waterfalls(cid:211) a forest(cid:211) ancient ruins(cid:211) an old man(cid:211)
(cid:210)a watercolor of (cid:210)oil painting style, (cid:210)a photo of (cid:210)oil painting style, (cid:210)a photo of (cid:210)a photo of
a dog(cid:211) a heart(cid:211) a bird(cid:211) an old man(cid:211) a teddy bear(cid:211) a skull(cid:211)
Fig.15: Motion Hybrids. We show more motion hybrid results. These are images
that change appearance when motion blurred. Here, the motion is from upper left to
bottom right.Factorized Diffusion: Perceptual Illusions by Noise Decomposition 25
(cid:210)oil painting style,
a bumblebee(cid:211)
(cid:210)oil painting style,
abraham lincoln(cid:211)
(cid:210)a photo of
an old woman(cid:211)
(cid:210)a photo of
audrey hepburn(cid:211)
(cid:210)oil painting style, (cid:210)oil painting style, (cid:210)a photo of (cid:210)a photo of (cid:210)a photo of (cid:210)a photo of
a bazaar(cid:211) a flower arrangement(cid:211) houseplants(cid:211) an english breakfast(cid:211) a library(cid:211) an old woman(cid:211)
(cid:210)oil painting style,
a bird(cid:211)
(cid:210)a photo of
gandhi(cid:211)
(cid:210)a lithograph of
a skull(cid:211)
(cid:210)a lithograph of
a quokka(cid:211)
(cid:210) to ei xl t up ra ei n ot fi n fg e as tt hy el re s, (cid:211) (cid:210)a photo of a forest(cid:211) (cid:210)a wl ai tt eh ro fg ar la lp sh (cid:211) of (cid:210)a hl oi ut sh eo pg lr aa np th s (cid:211)of (cid:210)o ti hl e p ga ri an nt di n cg a ns yt oy nl (cid:211)e, (cid:210)oil p aa i pn at ni dn ag (cid:211) style,
(cid:210)a photo of
an old man(cid:211)
(cid:210)a photo of
a panda(cid:211)
(cid:210)a photo of
abraham lincoln(cid:211)
(cid:210)a watercolor of
king tut(cid:211)
text(cid:210) ua r ep h oo ft o g ro af n i te(cid:211) (cid:210)a photo of a barn(cid:211) (cid:210) aa bp ah zo at ao r (cid:211)of (cid:210)a w aa t se ur nc so el to (cid:211)r of a flow(cid:210) ea r p ah ro rt ao n go ef m ent(cid:211) j(cid:210) oa h np h lo et no n oo nf (cid:211)
(cid:210)a photo of
gandhi(cid:211)
(cid:210)a photo of
an old man(cid:211)
(cid:210)a watercolor of
a panda(cid:211)
(cid:210)a photo of
elvis(cid:211)
(cid:210)a photo of (cid:210)a photo of (cid:210)a watercolor of (cid:210)a photo of (cid:210)a watercolor of (cid:210)a watercolor of
a sunset(cid:211) houseplants(cid:211) a library(cid:211) the grand canyon(cid:211) new york city(cid:211) a teddy bear(cid:211)
(cid:210)a lithograph of
houseplants(cid:211)
(cid:210)oil painting style,
john lennon(cid:211)
(cid:210)a photo of
an old woman(cid:211)
(cid:210)a watercolor of
a bird(cid:211)
(cid:210)a wl ai tt eh ro fg ar la lp sh (cid:211) of (cid:210)o ti hl e p ga ri an nt di n cg a ns yt oy nl (cid:211)e, flowe(cid:210) ra ap rh ro at no g eo mf e nts(cid:211) (cid:210)a w aa t be ar zc ao al ro (cid:211)r of (cid:210)oil mp oa ui nn tt ai in ng s (cid:211)style, (cid:210)oil p aa i pn at ni dn ag (cid:211) style,
(cid:210)a photo of
a teddy bear(cid:211)
(cid:210)a lithograph of
a skull(cid:211)
(cid:210)oil painting style,
a panda(cid:211)
(cid:210)a photo of
abraham lincoln(cid:211)
(cid:210)oil painting style,
(cid:210)a photo of (cid:210)a lithograph of (cid:210)oil painting style, (cid:210)a photo of (cid:210)oil painting style, an old man(cid:211)
the grand canyon(cid:211) houseplants(cid:211) new york city(cid:211) a flower arrangement(cid:211) a bazaar(cid:211)
Fig.16: Hybrid Images.Weshowmorehybrid imageresults.Foreasierviewing,we
provide insets of each hybrid image at lower resolution, along with the corresponding
prompt. Best viewed digitally, with zoom.26 D. Geng et al.
(cid:210)a photo of a car(cid:211) (cid:210) aa n cw ia et ne tr c ro ul io nr s (cid:211)of fl(cid:210) oa w ew ra t ae rr rc ao nl go er m eo nf t s(cid:211) (cid:210)a a p ta hi en at ti en rg (cid:211) of (cid:210)oil mp oa ui nn tt ai in ng s (cid:211)style, (cid:210)an ao ni l o lp da i mn at ni (cid:211)ng of
(cid:210)a photo of a ship(cid:211) (cid:210)a watercolor of (cid:210)a watercolor of (cid:210)a painting of (cid:210)oil painting style, (cid:210)an oil painting of
an old man(cid:211) an old woman(cid:211) a duck(cid:211) a rabbit(cid:211) students in a
classroom(cid:211)
(cid:210)a painting of (cid:210)oil painting style, (cid:210)a watercolor of (cid:210)oil painting style, (cid:210)oil painting style,
city skyscrapers(cid:211) the grand canyon(cid:211) a desert(cid:211) a flower arrangement(cid:211) a volcano(cid:211) (cid:210)a photo of a ship(cid:211)
(cid:210)a painting of (cid:210)oil painting style, (cid:210)a watercolor of (cid:210)oil painting style, (cid:210)oil painting style, (cid:210)a photo of a frog(cid:211)
a rabbit(cid:211) john lennon(cid:211) a camel(cid:211) a bird(cid:211) a duck(cid:211)
(cid:210)oil painting style, (cid:210)a watercolor of (cid:210)a watercolor of (cid:210)a lithograph of (cid:210)a watercolor of (cid:210)a watercolor of
mountains(cid:211) flower arrangements(cid:211) a landscape(cid:211) the grand canyon(cid:211) a rainforest(cid:211) waterfalls(cid:211)
(cid:210)oil painting style, (cid:210)a watercolor of (cid:210)a watercolor of (cid:210)a lithograph of (cid:210)a watercolor of (cid:210)a watercolor of
a tiger(cid:211) a duck(cid:211) a pig(cid:211) a bumblebee(cid:211) a crocodile(cid:211) a skull(cid:211)
Fig.17: Color Hybrids. We show more color hybrid results, with grayscale images
placed above their colorized version.
noitarutaseD
noitarutaseD
noitarutaseD
Saturation
Saturation
SaturationFactorized Diffusion: Perceptual Illusions by Noise Decomposition 27
High Pass: (cid:210)a photo
of waterfalls(cid:211)
Low Pass: (cid:210)a photo
of a skull(cid:211)
High Pass: (cid:210)oil painting
style, a flower
arrangement(cid:211)
Low Pass: (cid:210)oil painting
style, abraham lincoln(cid:211)
High Pass: (cid:210)lithograph
style, a flower
arrangement(cid:211)
Low Pass: (cid:210)lithograph
style, a bumblebee(cid:211)
High Pass: (cid:210)a photo
of a sunset(cid:211)
Low Pass: (cid:210)a photo
of a teddy bear(cid:211)
Grayscale: (cid:210)oil painting
style, a library(cid:211)
Full Color: (cid:210)oil painting
style, a bumblebee(cid:211)
Grayscale: (cid:210)a watercolor
of a flower
arrangement(cid:211)
Full Color: (cid:210)a watercolor
of a bird(cid:211)
Grayscale: (cid:210)an oil
painting of vases(cid:211)
Full Color: (cid:210)an oil
painting of a duck(cid:211)
Grayscale: (cid:210)an oil
painting of a volcano(cid:211)
Full Color: (cid:210)an oil
painting of a rabbit(cid:211)
Still: (cid:210)oil painting
style, a sports
stadium(cid:211)
Motion: (cid:210)oil painting
style, a car(cid:211)
Still: (cid:210)a watercolor
of new york city(cid:211)
Motion: (cid:210)a watercolor
of a dog(cid:211)
Still: (cid:210)a photo
of a bazaar(cid:211)
Motion: (cid:210)a photo of
a teddy bear(cid:211)
Still: (cid:210)oil painting
style, the grand
canyon(cid:211)
Motion: (cid:210)oil painting
style, a heart(cid:211)
Fig.18: Random Samples. We provide random samples of hybrid images, color
hybrids, and motion hybrids for selected prompts.