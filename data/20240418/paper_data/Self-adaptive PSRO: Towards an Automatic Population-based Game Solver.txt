Self-adaptive PSRO: Towards an Automatic Population-based Game Solver
PengdengLi1, ShuxinLi1∗, ChangYang2∗, XinrunWang1†,
XiaoHuang2, HauChan3 and BoAn1
1NanyangTechnologicalUniversity,Singapore
2TheHongKongPolytechnicUniversity,HongKongSAR,China
3UniversityofNebraska-Lincoln,Lincoln,Nebraska,UnitedStates
pengdeng.li,shuxin.li,xinrun.wang,boan @ntu.edu.sg,chang.yang@connect.polyu.hk
{ }
xiaohuang@comp.polyu.edu.hk,hchan3@unl.edu
Abstract Marrisetal.,2021]. Despitetheadvancements,determining
thehyperparametervaluesinPSROisnon-trivial[Smithet
Policy-SpaceResponseOracles(PSRO)asageneral
al.,2021]andtypicallyinvolvesextensivedomainknowledge,
algorithmicframeworkhasachievedstate-of-the-art
whichimpedesitfrombroaderreal-worldapplications.
performanceinlearningequilibriumpoliciesoftwo-
Precisely,oneneedstodeterminethemeta-solverandthe
playerzero-sumgames. However,thehand-crafted
bestresponse(BR)oraclewheninstantiatingPSRO.Onone
hyperparameter value selection in most of the ex-
hand, existing works have suggested various meta-solvers
istingworksrequiresextensivedomainknowledge,
such as uniform [Heinrich and Silver, 2016], Nash equilib-
formingthemainbarriertoapplyingPSROtodiffer-
rium[Lanctotetal.,2017],α-Rank[Mulleretal.,2020],and
entgames. Inthiswork,wemakethefirstattempt
correlatedequilibrium[Marrisetal.,2021]. However,weob-
toinvestigatethepossibilityofself-adaptivelyde-
servethatnoneofthemeta-solverscanconsistentlybeatallthe
terminingtheoptimalhyperparametervaluesinthe
othersintermsoflearningperformanceduringgamesolving.
PSROframework. Ourcontributionsarethree-fold:
Ontheotherhand,theBRpoliciesofaplayeraretypically
(1) Using several hyperparameters, we propose a
obtainedviaadeepreinforcementlearning(RL)oracle,e.g.,
parametricPSROthatunifiesthegradientdescent
DQN[Mnihetal.,2015],whichinvolvestheinitializationand
ascent(GDA)anddifferentPSROvariants. (2)We
thenumberofupdatesoftheBRpolicies. Unfortunately,the
proposetheself-adaptivePSRO(SPSRO)bycasting
determinationofthesehyperparametervaluesinmostofthe
thehyperparametervalueselectionoftheparamet-
existingworksisoftendomain-specific(e.g.,poker,soccer).
ricPSROasahyperparameteroptimization(HPO)
Therefore, an important question is: Can we automatically
problemwhereourobjectiveistolearnanHPOpol-
determinetheoptimalhyperparametervaluesinPSRO?
icythatcanself-adaptivelydeterminetheoptimal
Inthiswork,wemakethefirstattempttoanswerthisques-
hyperparameter values during the running of the
tion. Specifically, wefirstproposeaparametricPSRO(PP-
parametricPSRO.(3)Toovercomethepoorperfor-
SRO)byintroducingtwotypesofhyperparameters: i)game-
manceofonlineHPOmethods,weproposeanovel
freehyperparametersaretheweightsofdifferentmeta-solvers
offlineHPOapproachtooptimizetheHPOpolicy
consideredduringgamesolving,andii)game-based hyper-
basedontheTransformerarchitecture. Experiments
parameters are the initialization and the number of updates
onvarioustwo-playerzero-sumgamesdemonstrate
ofaplayer’sBRpolicies. PPSROprovidesageneralframe-
thesuperiorityofSPSROoverdifferentbaselines.
work to unify the gradient descent ascent (GDA) [Fiez and
Ratliff, 2021] and various PSRO variants [Ho et al., 1998;
1 Introduction Balduzzietal.,2019;Mulleretal.,2020;Marrisetal.,2021].
Then,anaturalproblemishowtodeterminethehyperparam-
Policy-SpaceResponseOracles(PSRO)[Lanctotetal.,2017]
etervaluesofPPSRO.Tosolvethisproblem,weproposea
sinceproposedhasbeenthemainstreamalgorithmicframe-
novelframework, self-adaptivePSRO(SPSRO),bycasting
workforsolvingtwo-playerzero-sumgames. Ateachepoch,
thehyperparametervalueselectionofPPSROasahyperpa-
PSROconstructsameta-gamebysimulatingoutcomesofall
rameteroptimization(HPO)problemwhereourobjectiveisto
match-upsofpoliciesofallplayersandcomputesthemeta-
learnanHPOpolicythatcanself-adaptivelyselecttheoptimal
strategiesforallplayersviaameta-solver. Itthentrainsnew
hyperparameter values of PPSRO during game solving. A
policiesforeachplayeragainsttheopponent’smeta-strategy
straightforwardmethodtooptimizetheHPOpolicyistouse
throughanoracleandappendsthenewpoliciestotheplayer’s
onlineapproachessuchasOptuna[Akibaetal.,2019]. Unfor-
policyspace. Thetwocomponents–meta-solverandoracle
tunately,onlineHPOmethodsonlyuseonlinegenerateddata
–determinethenatureofPSROandvariousPSROvariants
(pastepochsofSPSRO),typicallyconstrainingthetrainingob-
havebeenproposed[Balduzzietal.,2019;Mulleretal.,2020;
jectivestobecheaplycomputable[Chenetal.,2022]andthe
∗Equalcontribution. performancecouldbepoor. Toovercometheselimitations,we
†Correspondingauthor. proposeanofflineHPOapproachtooptimizetheHPOpolicy
4202
rpA
71
]IA.sc[
1v44111.4042:viXrabasedontheTransformerarchitecture[Vaswanietal.,2017; (HPO).ExistingworksonHPOcanberoughlycategorized
Chenetal.,2021a].Specifically,weformulatetheHPOpolicy into online and offline HPO. The classic online HPO meth-
optimizationasasequencemodelingproblemwhereaTrans- odsincludeBayesianoptimization[Snoeketal.,2012]and
formermodelistrainedbyusinganofflinedatasetandthen its variants [Krause and Ong, 2011; Bardenet et al., 2013;
usedtopredictthehyperparametervaluesconditionedonpast Swerskyetal.,2013;Feureretal.,2015;Volppetal.,2019;
epochsofSPSRO.Intuitively,awell-trainedHPOpolicyhas WistubaandGrabocka,2020;Rothfussetal.,2021],andre-
thepotentialtotransfertodifferentgames,reducingtheeffort currentneuralnetworks(RNNs)[Duanetal.,2016;Wanget
neededforresearcherstoconductthecostlyhyperparameter al.,2016;Chenetal.,2017]. However,onlineHPOmethods
tuningwhenapplyingPSROtovariousgames. onlyuseonlinegenerateddata,typicallyconstrainingthetrain-
Insummary,thecontributionsofthisworkarethree-fold: ingobjectivestobecheaplycomputableandtheperformance
(1) By introducing several hyperparameters, we propose a could be poor. Our work is closely related to [Chen et al.,
parametricversionofPSRO(PPSRO)whichunifiesGDAand 2022]whichproposesthefirstofflineTransformer-basedHPO
variousPSROvariants. (2)Weproposeanovelframework, method. Nevertheless,itisnon-trivialtooptimizetheHPO
self-adaptivePSRO(SPSRO),byformulatinganoptimization policyasitinvolvesseveralcriticalchallengessuchashowto
problemwhereourobjectiveistolearnanHPOpolicythatcan generateanofflinedatasetfortraining. Toourknowledge,this
self-adaptivelydeterminetheoptimalhyperparametervalues workisthefirstattempttoexploreanddevelopaself-adaptive
ofPPSRO.(3)Toovercomethepoorperformanceofclassic hyperparametervalueselectoringametheory.
onlineHPOmethods,weproposeanofflineHPOapproachto
optimizetheHPOpolicybasedontheTransformerarchitec- 3 Preliminaries
ture. Weevaluatetheeffectivenessofourapproachthrough
Inthissection,wefirstpresentthegamedefinitionandthen
extensiveexperimentsonasetoftwo-playerzero-sumnormal-
theprocedureofthePSROalgorithm.
formandextensive-formgames,andtheresultsdemonstrate
thatSPSROwithTransformercanachievesignificantlybetter 3.1 Games
learningperformancethandifferentbaselines.
Consideratwo-playerzero-sumgamerepresentedbyatuple
G=( , , ,p, ri , ),whereplayersareindexedby
i
2 RelatedWorks = N 1,2S .A LetN{ =}∈N =T 2. and denotetheplayers’
N { } |N| S A
state and action spaces, respectively. = 0,1, ,T is
PSRO[Lanctotetal.,2017]generalizesthedoubleoraclealgo-
timeindexset. Att
,playeriinstaT tesi{ ta· k· e· san}
ac-
rithm[McMahanetal.,2003]andunifiesvariousmulti-agent tionai andthen∈ cT hangestonewstatest i∈S p( s ,a )
learningmethodsincludingthefictitiousplay(FP)[Robinson, t ∈A t+1 ∼ ·| t t
and receives a reward ri(s ,a ), where s = (si) and
1951;Brown,1951],neuralfictitiousself-play(NFSP)[Hein- t t t t i
a = (ai) arerespectivelyjointstateandjointa∈ctNionof
richandSilver, 2016](anextensionofFPinthecontextof t t i
all players,∈Np : N N ∆( )1 is the transition func-
deepRL),iteratedbestresponse(IBR)[Hoetal.,1998],and tion and ri : NS ×AN →R isS the reward function with
independentreinforcementlearning(InRL)[Matignonetal., ri(s ,aS )=× 0A . Let→ πi : ∆( )denotetheplayer
2012]. Recently, many works have been done toward im- i t t S → A
provingPSRO,includingthescalability[McAleeretal.,2020; i’s ∈pNolicy (strategy)2 with πi Πi where Πi is the policy
Smithetal.,2021],diversityofBRs[Perez-Nievesetal.,2021; (cid:80) space. Giventhejointpolicyo∈ fallplayersπ = (πi) i
Liuetal.,2021a],theintroductionofnovelmeta-solvers,e.g., Π= i
Πi,eachplayeriaimstomaximizehisown∈vNalu∈
e
α-Rank [Muller et al., 2020], correlated equilibrium [Mar- funct× ion∈N Vi(π,s 0) = E T t=0ri(s t,a t) |a t
∼
π,s t+1
∼
risetal., 2021], andneuralmeta-solver[Fengetal., 2021], p ,wheres istheplayers’initialstates.
and application to mean-field games [Muller et al., 2022a]. A
mixed0
strategy σi
(cid:2)(cid:80)
∆(Πi)3 is called a meta-strategy
Moreover,thechallengingstrategyexplorationproblemhas w(cid:3) hich is the probability∈ distribution over the player i’s pol-
alsobeenextensivelyinvestigated[Wellman,2006;Schvartz- icy space Πi. More precisely, suppose that there are c 1
manandWellman,2009b;SchvartzmanandWellman,2009a; policiesinplayeri’spolicyspace,thenthemeta-strategy≥ of
Jordanetal.,2010;Wangetal.,2022]. Despitetheadvance- iisσi = (σi,1, ,σi,c)withσi,j 0and c σi,j = 1.
ments,acriticalobservationweobtainedisthatgivenasetof ··· ≥ j=1
Accordingly,σ =(σi) ∆(Π)isthejointmeta-strategy
meta-solvers,noneofthemcanconsistentlybeat(dominate) of all players. Given i t∈hNe j∈ oint meta-strategy(cid:80) of all players
alltheothersintermsoflearningperformanceduringgame
excepti,σ i,theexpectedpayofftoplayeri’spolicyπi Πi
solving(inthesensethatwejustevaluatePSROasanonline isgivenby− Ri(πi,σ i)= σ i(π i)Vi(πi,∈ π i)
algorithm). Ontheotherhand,theBRsofaplayeraretypi- − π−i Π−i − − −
andthesetofbestresponses(BR∈s)ofplayeriisdefinedas
callyobtainedviaadeepRLoraclesuchasDQN[Mnihet
BRi(σ i) = argmax (cid:80) Ri(πi,σ i). The quality of σ
al.,2015],wherethehyperparameters(e.g.,theinitialization − πi Πi −
can be measured by the∈NashConv [Lanctot et al., 2017].
and the number of updates) are often domain-specific (e.g.,
For player i , the NashConv is defined as i(σ) =
poker,soccer)andmostoftheexistingworksmanuallyselect ∈ N R
thehyperparametervaluesbasedondomainknowledge. In 1∆(X)denotestheprobabilitydistributionoverthespaceX.
thiswork,wedevelopanovelframeworktoself-adaptively 2Weinterchangeablyusepolicyandstrategyinthiswork.
determinetheoptimalhyperparametervaluesinPSRO,which 3InprincipleΠicouldbeaninfiniteset.However,Πiistypically
canbetransferredtodifferentgameswithoutfine-tuning. iterativelyexpandedbylearningalgorithmssuchasPSROandhence,
Anotherlineofrelatedworkishyperparameteroptimization isconsideredfiniteinthiswork.Ri(BRi(σ i),σ i) σi(πi)Ri(πi,σ i). That is, beat(dominate)alltheothersintermsofNashConvduringthe
− − − πi Πi −
theNashConvofplayeriis∈thegainhecanobtainwhenhe PSROprocedure. Forinstance,attheearlystageofthePSRO
unilaterallydeviatesfro(cid:80)mthecurrentjointmeta-strategytoa procedure,Uniformperformsbetterthantheothertwometa-
(pure)BRstrategy. Therefore,thequalityofσismeasuredby solversintermsofNashConv. However,itonlyconvergesto
thetotalNashConvofallplayers (σ)= i(σ). ahighNashConvvalue, whichisalsoobservedinprevious
R i ∈N R works[Mulleretal.,2020]. (2)Byswitchingfromonemeta-
3.2 Policy-SpaceResponseOracles(cid:80)
solvertoanotherduringthePSROprocedure,wecanachieve
Givenagame,PSROfirstinitializesthepolicyspaceofeach betterlearningperformanceintermsofNashConv. Moreover,
playerΠiusingrandomlygeneratedpoliciesandthenexpands the comparison between the two cases: “α-Rank PRD”
the policy space in three iterated phases4 (as shown in Fig- and“PRD α-Rank”,againverifiesthepreviously→ observed
ure 2): (1) Synthesize a meta-game with all match-ups of factthatno→ neofthemeta-solverscanconsistentlybeatallthe
policiesofallplayersandcomputethemissingpayoffentries othersintermsofNashConvduringthePSROprocedure5.
inthepayofftensorM viasimulation. (2)Computethejoint Theaboveobservationsmotivateustothinkaboutanatu-
meta-strategy σ using a meta-solver on the synthesized ralquestion: Howtodeterminethemeta-solversduringthe
M
meta-game M. Different meta-solvers can be used during PSROproceduresuchthatwecanobtainbetterlearningper-
training,e.g.,Nashequilibrium[Lanctotetal.,2017],corre- formance? NotethattheexamplesinFigure1areNFGs. For
latedequilibrium[Marrisetal.,2021],α-Rank[Mulleretal., extensive-formgames(EFGs),inadditiontothemeta-solver,
2020],anduniformdistribution[HeinrichandSilver,2016]. the hyperparameters also include the initialization of a BR
(3)Computeeachplayeri’sBRπi,BRusinganoracle igiven policyπi,BRandthenumberofupdatesK fortrainingtheBR
thejointmeta-strategyσ andaddtheBRπi,BR topO layeri’s policy. Most of the existing works determine the values of
policyspaceΠi. Astheotherplayer’spolicyspacesΠ −iand thesehyperparametersbyhand-craftedtuning,whichtypically
jointmeta-strategyσ −iarefixed,thecomputationofBRisa requiresextensivedomainknowledge.
single-playeroptimizationproblemfromplayeri’sperspec- Thus,acriticalproblemtobeaddressedis: howtoautomat-
tive. Inpractice,thebestresponseistypicallyapproximated icallydeterminetheoptimalhyperparametervaluesduring
byusingdeepRLalgorithms,e.g.,DQN[Mnihetal.,2015]. thePSROrunning? Specifically,ateachepoch,weneedto
Specifically,thebestresponseπi,BRistrainedforK updates, i)chooseoneormultiplemeta-solver(s), ii)determinehow
giventhattheotherplayerusesthepolicyπ −i σ −i, i.e., toinitializethenewBRpolicies,e.g.,randominitialization,
π −iissampledaccordingtoσ −i. ∼ copyfromoneofthepreviousBRs,ormix,andiii)determine
thenumberofupdatesK ofthenewBRpolicyofeachplayer.
4 MotivatingExample
In this work, we make the first attempt to develop a novel
Inthissection,weprovidesomeexamplestobetterillustrate frameworkthatcanself-adaptivelydeterminetheoptimalhy-
themotivationofthiswork. Consideratwo-playerzero-sum perparametervaluesduringthePSROrunning.
normal-formgame(NFG)ofsize . Let denote
1 2
|A |×|A | M
thesetofmeta-solversofinterest. Inthisexample(aswellas 5 Self-adaptivePSRO
thiswork),weconsiderthemostcommonlyusedthreemeta-
Inthissection,weestablishtheSelf-adaptivePSRO(SPSRO)
solvers: Uniform[HeinrichandSilver,2016],α-Rank[Muller
throughtwosteps: (1)WeparameterizethePSROalgorithm
etal.,2020],andPRD[Lanctotetal.,2017]. Weconducttwo
(PPSRO)byintroducingseveralhyperparameters. (2)Wecast
typesofexperiments:i)consistentlyusingasinglemeta-solver
thehyperparametervalueselectionofPPSROasahyperpa-
duringthePSROprocedure,andii)switchingthemeta-solver
rameteroptimization(HPO)problemwhereourobjectiveis
fromonetoanotheratsomeintermediateiterationofthePSRO.
tolearnanHPOpolicythatwillself-adaptivelydeterminethe
TheresultsareshowninFigure1.
optimalhyperparametervaluesofPPSRO.
 1 ) *    ×    1 ) *     ×   
5.1 ParametricPSRO
         
 8 Q L I R U P  8 Q L I R U P
      3 5 '       3 5 ' First, inspired by the observations in the previous section,
       5 D Q N   5 D Q N
 8 Q L I R U P    3 5 '       8 Q L I R U P     5 D Q N in this work, instead of considering a single meta-solver as
             6 Z  5  L W D  F Q  K N    3 5 '       3  6 Z 5 '  L W F   K   5 D Q N mostoftheexistingworks,weusethemeta-solverset α
M
          with m different meta-solvers and associate it with a vec-
          tor α = (α , ,α ) specifying the weight of each meta-
1 m
          solver. Intuitiv· e· l· y,bycombiningmultiplemeta-solvers,we
          couldobtainbetterperformance. Asαisonlydependenton
                                 
 ( S R F K  ( S R F K
5Theseobservationsaredifferentfrom[Wangetal.,2022]which
Figure1:NashConvofdifferentPSROruns. regardsthestrategyexplorationanditsevaluationastwoorthogonal
components.Forexample,onecanusePRDtoguidetheBRpolicies
Fromtheresults,wecanobtainthefollowingobservations. computationbutuseα-Ranktocomputethemeta-distributionfor
(1)Forthethreemeta-solvers,noneofthemcanconsistently decision-makingafterthenewBRpoliciesofplayersareaddedto
theirrespectivepolicyspaces. However,itisworthnotingthatour
4Notethatthroughoutthiswork,ealwaysrepresentstheindexof observationsdonotcauseinconsistencywith[Wangetal.,2022]as
PSROepoch,neitherthepowerofenortheEuler’snumber. wejustevaluatePSROasanonlinealgorithm.
 Y Q R & K V D 1themeta-gamepayofftensorM regardlessoftheunderlying andfinallyformalizethehyperparameteroptimization(HPO)
games(normal-formorextensive-formgames),werefertothe problemwhereourobjectiveistolearnanHPOpolicythat
weightsinitasgame-freehyperparameters. willself-adaptivelyselecttheoptimalhyperparametervalues
Second,weintroduceaparametricBRoracle i(σ;β,K) ofPPSROduringgamesolving.
where the hyperparameters include6: (1) the iO nitialization Letτ ΓdenoteanHPOpolicywhereΓisthepolicyspace.
parameter β [0,1] which determines the initialization of Letue =∈ (αe,βe,Ke) denotethehyperparametervalues
thenewBRp∈ olicyofplayeribymixingtheBRpolicyob- selectedaccordingtothe∈ HU POpolicyτ ateachepoche 1of
tained in the last epoch with a randomly initialized policy SPSRO,where istheadmissiblesetofthehyperpara≥ meter
πi,random, and(2)thenumberK whichdeterminesthenum- values. Thatis,wU ehaveue τ. WerunoneepochofSPSRO
ber of updates needed for training the new BR policy. For- asfollows. (1)Computeth∼ epayofftensorM throughgame
mally,ateachepoche,weinitializeplayeri’sBRpolicyas simulation(Line3). (2)Computethefinaljointmeta-strategy
πi,BR,e =βπi,BR,e −1+(1 β)πi,random,andthenupdatethis σe (Line4). (3)Expandeachplayer’spolicyspace(Line5).
BRpolicyπi,BR,eforK ste− ps. Afterthat,thetrainedBRpol- Specifically,foreachplayeri,weinitializetheBRpolicyas
icyπi,BR,e isaddedtoplayeri’spolicyspaceΠi. Asβ and πi,BR,e =βeπi,BR,e 1+(1 βe)πi,random,andthentrainthe
−
K arehighlydependentontheunderlyinggames(e.g.,poker, BRpolicyπi,BR,eforKeupd− atesandaddittoplayeri’spolicy
soccer),werefertothemasgame-basedhyperparameters. space Πi = Πi πi,BR,e . (4) Compute the performance
Byspecifyingα,βandK,wecanobtainGDAandvarious metricye(ue)of∪ the{ current} selection(Line6). (5)Selectnew
PSROvariants(Table1). Forexample,GDAcanbeinstan- hyperparametervaluesue+1accordingtoτ (Line7).
tiatedasfollows. Supposethat isthemeta-solver“Last-
b
M
One”,thenwesetα =1andα =0,whichimpliesthat
b d=b Meta-solvers Meta-strategy
themeta-strategyofplayeriisσ bi ̸ =(σ bi,1 =0,
···
,σ bi,e −1 = σ1,...,σi,...,σN
1)atepoche. Then,playeriinitializestheBRpolicyπi,BR,e M M (cid:8) (cid:9)
withβ =1andtrainsπi,BR,ewithK =1update.
α
(β,K)
Algorithm α i(σ;β,K) Gamesimulation τ Oβ,K Oracle
M O
GDA Last-One β =1&K =1
InRL Last-One β =1&K =K¯
PSRO Penultimate β [0,1]&K =K¯ Π1,...,Πi,...,ΠN Add π1,...,πi,...,πN
P
PSRO Uniform β ∈ [0,1]&K =K¯ (cid:8) (cid:9) (cid:8) (cid:9)
U
PSRO Nash β ∈ [0,1]&K =K¯ Figure2: IllustrationofSelf-adaptivePolicy-SpaceResponseOra-
N
PSRO RectifiedNash β ∈ [0,1]&K =K¯ cles(SPSRO).PSROandPPSROaretwospecialcasesofSPSRO.
PSROrN α-Rank β ∈ [0,1]&K =K¯ Illustrationinspiredby[Mulleretal.,2020].
α-Rank
PSRO CoarseCorrelated β ∈ [0,1]&K =K¯
CCE
∈
Table1: SpecificationsofPSROvariants. K¯ isthenumberofup- Algorithm1SPSRO
datesneededtoobtainaconvergedBRpolicy. Thereferencesfor 1: InitializeΠiwithrandompolicies, i ,e 1,select
thesemethods: GDA[FiezandRatliff,2021],InRL[Matignonet initialhyperparametervaluesu1 =∀ (α∈1,N β1,K←1),τ Γ
a 20l. 1, 62 ]0 ,1 P2 S] R, OPSR [LO aP n[ cH too tee tt aa ll .,., 21 09 19 78 ],], PP SS RR OO U [B[H ale di un zri zc ih eta an ld .,S 2i 0l 1v 9e ]r,
,
2: forepoche 1,2,3,... do ∈
N rN ∈{ }
PSRO [Mulleretal.,2020],PSRO [Marrisetal.,2021]. 3: UpdatepayofftensorM viagamesimulation
α-Rank CCE 4: Computeσeusing andαe: σe = m αeσe
M b=1 b b
5: Expandpolicyspaces i:Πi Πi i(σe;βe,Ke)
5.2 HPOPolicyOptimization 6: ComputetheperformaO nceme← tricye∪ (uO(cid:80)e)
WiththePPSROintroducedintheprevioussection,anatural 7: Selectnewhyperparametervaluesue+1 τ
∼
problem is how to determine the values of α, β, and K in 8: endfor
PPSRO. To address this problem, we propose a novel algo-
rithmicframework,Self-adaptivePSRO(SPSRO),whichis Givenaselectionofthehyperparametervaluesue,wede-
s ph reo sw en ntin thF eig ou vr ee ra2 lla pn rd ocA el dg uo rr eith om fS1 P. SI Rn Oth ,e thfo el nlo dw ei fin ng e,w the efi pr es rt
-
fineitsperformancemetricasye(ue)= R (( σσ 1e) )+ hh 1e . Roughly
speaking,itconsistsoftwoparts: theNaRshConvofallplayers
formancemetricofagivenselectionofhyperparametervalues,
andtheBRtrainingeffort,whichimpliesthatusinglargerKe
6Therecouldbeotherhyperparameterssuchasbatchsizeinthe couldobtainlowerNashConv (σe)butatthecostoflonger
BRoracle.Nevertheless,asthefirstattempttoexplorethepossibility BRtrainingtimehe,andusingR smallerKecouldshortenthe
ofself-adaptivelydeterminingtheoptimalhyperparametervalues, BRtrainingtimewhileatthecostofhigherNashConv (σe).
wefocusontheonesthatenableustounifyvariousPSROvariants. Giventheperformancemetric,ourobjectiveistoleR arnan
Moreover,itisworthnotingusingthesametypeofBRoracle(DQN
HPOpolicyτ bysolvingthefollowingHPOproblem: e 1,
in this work) with the same configuration for all the other hyper- ∀ ≥
parameters is important to ensure a fair comparison between our
approachandbaselines.SeeAppendixAformorediscussion. argmin ye(ue τ). (1)
τ Γ
∈ ∼6 ANovelOfflineHPOAlgorithm difficultincomplexgames. Oneofthemaindifficultiesisthat
solvingthemeta-gameusingα-RankcouldbeNP-hard[Yang
ThemoststraightforwardmethodtooptimizetheHPOpolicy
etal.,2020]andasitrequiresenumeratingallthejointstrate-
istoemploytheclassicHPOmethodssuchasBayesianopti-
mization[Snoeketal.,2012]. However,mostHPOmethods
giestoconstructtheresponsegraph[Omidshafieietal.,2019],
itistime-consumingastheprogressofthePSROprocedure.
typicallypredicthyperparametervaluesbasedononlinegen-
Tomoreefficientlygeneratethetrainingdataset,weuseasim-
erateddata(historyofpastepochsinthecontextofourwork),
plepruningtechniquetoconstrainthesizeofthesupportsetof
and consequently, the performance could be poor. To over-
themeta-strategywhenusingα-Ranktosolvethemeta-game
comethelimitationsofonlineHPO,weproposeanoveloffline
ineachepochofthePSROalgorithm.
HPOmethodtooptimizetheHPOpolicy,whichpossessesthe
LetC denotethemaximumsizeofthesupportsetofthe
potentialtotransfertodifferentgameswithoutfine-tuning.
meta-strategy. AtthefirstC epochs,wefollowthestandard
6.1 HPOasSequenceModeling PSRO .Afterthat,ateachepoche>C,weconstructthe
α-Rank
meta-gameofsize(C+1) (C+1)whereanewBRpolicy
As presented in Algorithm 1, selecting the hyperparameter
isobtainedbydeepRLalgo× rithmssuchasDQN[Mnihetal.,
values in SPSRO can be naturally regarded as a sequence
2015]. Next,wecomputethemeta-distributionbysolvingthe
modeling problem where we model the probability of the
next token xe conditioned on all prior tokens: P (xe x<e),
meta-gamethroughα-Rank. Letπ mi indenotethepolicywith
similar to the decoder-only sequence models
[Zθ hao|
et al.,
theminimumprobabilityinthemeta-distribution. Then,we
2023; Touvron et al., 2023]. Specifically, we consider the getthefinalmeta-strategybysettingσ αi, -e Rank(π mi in) = 0and
sequenceofhyperparametervaluesuptoe-thepoch: normalizingtheresultingdistribution.
Duringthedatasetgeneration,weemploythewidelyused
He =(
···
,α 1e,
···
,α me ,βe,Ke,ye). (2) tool,Optuna[Akibaetal.,2019],todeterminethevalueofue
ateachepoche. UsingtheterminologyofofflineRL[Levine
Figure3presentstheoverviewofthearchitecture.
et al., 2020; Chen et al., 2021a], Optuna is a behavior pol-
icy to generate the offline training dataset (see Appendix B
α 1e α 2e βe Ke ye
foranexamplecodeofOptunashowinghowtogeneratethe
···
dataset). Furthermore,wedistinguishbetweennormal-form
games(NFGs)andextensive-formgames(EFGs)whengener-
Casual Transformer
··· ··· atingthedataset. Theprimaryreasonisthattheparametersof
interestaredifferent.InEFGs,inadditiontotheweightsofdif-
ye 1 αe αe βe Ke ferentmeta-solversα,Eq. (2)alsoconsistsoftheparameters
− 1 ··· m
relatedtotheBRoracle,β andK. Therefore,thetransformer
modeltrainedontheNFGdatasetcannotbedirectlyapplied
Figure3:HPObasedonTransformer.Ateachepoch,theTransformer
toEFGs. Inaddition,generatingtheNFGdatasetisrelatively
modelpredictsparametervaluesinanautoregressivemannerusinga
easierasthecomputationaldifficultymainlyresultedfromthe
causalself-attentionmask,i.e.,eachpredictedparametervaluewill
befedintothemodeltogeneratethenextparametervalue. meta-gamesolvingusingα-Rank,whichcanbeeffectivelyad-
dressedbythepreviouslyproposedpruningtechnique. Forthe
EFGdataset,obtainingtheBRpoliciesrequiresextracomputa-
6.2 Tokenization tionaloverheadastheBRpoliciesaretypicallyapproximated
viadeepRLalgorithmssuchasDQN[Mnihetal.,2015].
WeconverteachelementinEq.(2)intoasingletoken. The
ideaistonormalizeanddiscretizeeachelementsuchthatit 6.4 LossFunctionandInference
fallsintooneofQbinseachwithsize1. Qisreferredtoas
Given the dataset , we train the Transformer model θ by
thequantizationlevel. Specifically,wehave: maximizingthelogD -likelihoodforeachsequence e¯ :
x¯e =int[xe norm·Q], (3)
(θ; e¯)=
e¯(m+3)
logP (¯n ¯1:n
H
1),
∼D
(5)
θ −
wherexe =(xe x )/(x x ). x andx are L H n=1 H |H
determinn eor dm bythes− pacm ein andm ta hx e− ranm gi en ofym ein isdeterm max
ined
wheree¯isthemaxim(cid:88)umnumberofepochs, ¯n isthen-th
by observed values in
thU
e offline dataset (introduced in the
tokeninEq.(4),and ¯1:n −1isalltokensuptoH the(n 1)-th
H −
tokeninEq.(4). Aftertraining,wecanapplytheTransformer
nextsubsection)ortheunderlyinggames(e.g.,normal-form
andextensive-form). Aftertokenization,wehave:
θtoagivengametopredictthevalueofue. Specifically,we
reversethetokenizationtoobtainthetokendistribution:
¯e =( ,α¯e, ,α¯e ,β¯e,K¯e,y¯e). (4) Q P (x¯ )
H ··· 1 ··· m p (x )= · θ |· . (6)
θ
|· (x x )
6.3 TrainingDataset max min
−
Then,wecansampleue fromthemodel’spriordistribution
TotraintheTransformermodelθ,oneofthecriticalstepsis
andthus,definetheHPOpolicyasfollows:
togeneratetheofflinedataset,whichisnon-trivialduetothe
m
computationalcomplexityofrunningSPSRO.Specifically,to τ(ue e 1)= p (αe e 1,αe, ,αe )
generateadataset consistingof sequencesofEq.(2)or |H − b=1 θ b|H − 1 ··· b −1
Eq.(4),weneedtD oruntheSPSRO|D f| or |D|times. However, (cid:89) ×p θ(βe |He −1, {α be }1 ≤b ≤m) (7)
itiswell-knownthatrunningPSROcanbecomputationally p (Ke e 1, αe ,βe).
×
θ
|H
−
{
b}1 ≤b ≤m 1 ) *     ×     1 ) *     ×     1 ) *     ×     1 ) *     ×     1 ) *     ×   
                   
 * ' $   5 D Q N  * ' $   5 D Q N  * ' $   5 D Q N  * ' $   5 D Q N  * ' $   5 D Q N
 8 Q L I R U P  2 S W X Q D  8 Q L I R U P  2 S W X Q D  8 Q L I R U P  2 S W X Q D  8 Q L I R U P  2 S W X Q D  8 Q L I R U P  2 S W X Q D
 3 5 '  7 U D Q V I R U P H U  3 5 '  7 U D Q V I R U P H U  3 5 '  7 U D Q V I R U P H U  3 5 '  7 U D Q V I R U P H U  3 5 '  7 U D Q V I R U P H U
                   
                                                                                    
 ( S R F K  ( S R F K  ( S R F K  ( S R F K  ( S R F K
 / H G X F  3 R N H U  * R R I V S L H O  / L D U V  ' L F H  1 H J R W L D W L R Q  7 L F  7 D F  7 R H
   
               
   
   
       
 * ' $   5 D Q N  * ' $   5 D Q N  * ' $   5 D Q N  * ' $   5 D Q N  * ' $   5 D Q N
 8 Q L I R U P  2 S W X Q D  8 Q L I R U P  2 S W X Q D  8 Q L I R U P  2 S W X Q D  8 Q L I R U P  2 S W X Q D      8 Q L I R U P  2 S W X Q D
 3 5 '  7 U D Q V I R U P H U  3 5 '  7 U D Q V I R U P H U  3 5 '  7 U D Q V I R U P H U  3 5 '  7 U D Q V I R U P H U  3 5 '  7 U D Q V I R U P H U
                   
                                                                                                                               
 % 5  7 L P H   V   % 5  7 L P H   V   % 5  7 L P H   V   % 5  7 L P H   V   % 5  7 L P H   V 
Figure4:Evaluationperformance.ThetopandbottomrowscorrespondtoNFGsandEFGs,respectively.
That is, at epoch e, τ predicts each parameter value in ue apply the trained Transformer model to the other games to
conditionedon: i)thesequenceofpastepochs e 1,andii) verifythezero-shotgeneralizationabilityofthemodel.
−
H
thevaluesoftheprecedingpredictedparameters.
7.2 Results
7 Experiments TheresultsaresummarizedinFigure4. Fromtheresults,we
candrawseveralconclusionsasfollows.
Inthissection,weevaluatetheeffectivenessofSPSRO.
Bycombiningmultiplemeta-solvers,wecouldobtainbet-
ter performance than using a single meta-solver. In all the
7.1 ExperimentalSetup
NFGs, thefinalNashConvsofOptunaandTransformerare
Allexperimentsareperformedonamachinewitha24-core lowerthanthatoftheclassicPSRObaselinesconsideringa
3.2GHz Intel i9-12900K CPU and an NVIDIA RTX 3060 singlemeta-solver(Uniform,PRD,orα-Rank). ForEFGs,in
GPU.Alltheresultsareaveragedover30independentruns. NegotiationandTic-Tac-Toe,thefinalNashConvsofTrans-
MoreexperimentaldetailscanbefoundinAppendixC. formerarelowerthantheclassicPSRObaselines. Theresults
Games. Weconsiderthefollowinggames. (1)Normal-form clearlyverifythenecessityofsynergisticallyintegratingmul-
games(NFGs)ofsize . Thepayoffmatricesare tiplemeta-solversduringgamesolving.
1 2
|A |×|A |
randomlysampledfromtherange[ 1,1]. Thesetofsizeis Transformer-basedHPOcouldachievebetterperformance.
−
150 150,200 200,250 250,100 200,150 300 . Transformer-basedHPOcanlearnabetterpriordistribution
{ × × × × × }
(2)Extensive-formgames(EFGs): Leduc,Goofspiel,Liar’s ofhyperparametervaluesfromofflinedata,providingabetter
Dice,Negotiation,andTic-Tac-Toe,whichareimplemented scheme for weighting multiple meta-solvers, and therefore,
inOpenSpiel[Lanctotetal.,2019]. PleaserefertoSection6.3 achievingbetterperformancethanOptunawhichisanonline
forthereasonstodistinguishbetweenNFGsandEFGs. method and only relies on past epochs to obtain the prior
Methods. (1)GDA[FiezandRatliff,2021]. Ateachepoch,a distributionofhyperparametervalues. Inaddition,inEFGs,
playeronlybestrespondstotheopponent’snewestBRaction wefoundthattheNashConvofOptunadecreasesquickly,but
(NFGs)orpolicy(EFGs). (2)Uniform[HeinrichandSilver, convergestoahighvalue(alsoshowninFigure5). Incontrast,
2016]. Themeta-distributionistheuniformdistribution. (3) theTransformercanconvergetoalowerNashConv,thoughit
PRD[Lanctotetal.,2017;Mulleretal.,2020],anapproxima- needsalongertimeintermsofBRtraining.
tionofNashequilibrium. WechoosePRDinsteadofanexact Transformerhasthepotentialtoprovideauniversaland
NashsolverasithasbeenwidelyadoptedinPSRO-related plug-and-playhyperparametervalueselector. Asshownin
research. (4)α-Rank[Mulleretal.,2020]. (5)Optuna[Akiba Figure 4, the trained Transformer model can be applied to
etal.,2019]. TheparametervaluesaredeterminedbyOptuna. the games that are different from the training dataset: for
(6)Transformer. Theparametervaluesaredeterminedbythe NFGs,itcanbeappliedtogameswithdifferentsizesofaction
Transformer model. Among these methods, (2) to (4) are (strategy)spaces,andforEFGs,itcanbeappliedtodifferent
classicPSROmethodsthatonlyinvolveasinglemeta-solver, gamesevenwithdifferentrewardscale(e.g.,themaximum
while(5)and(6)areSPSROmethodsthatinvolvemultiple rewardinGoofspielis1whileinNegotiationitis10). This
meta-solversandusedifferentHPOapproaches. correspondstothedesiderataofauniversalandplug-and-play
TrainingandTesting. Wegeneratethetrainingdatasetsfor hyperparametervalueselectorasmentionedinSection6.
NFGsandEFGsseparately.ForNFGs,wegeneratethedataset Givenasetofmeta-solvers,noneofthemcanconsistently
onthegameofsize =200 200. ForEFGs,we beat(dominate)alltheothersduringgamesolving(theob-
1 2
|A |×|A | ×
generatethedatasetontheLeducPoker. Duringtesting, in servation in Section 4). This can be derived by comparing
additiontothegamesusedtogeneratethedataset,wedirectly theperformanceofthethreesingle-solver-basedPSRObase-
 Y Q R & K V D 1
 Y Q R & K V D 1 / H G X F  3 R N H U  * R R I V S L H O  / L D U V  ' L F H  1 H J R W L D W L R Q  7 L F  7 D F  7 R H
              
 2 S W X Q D  2 S W X Q D  2 S W X Q D  2 S W X Q D
   7 U D Q V I R U P H U      7 U D Q V I R U P H U       7 U D Q V I R U P H U     7 U D Q V I R U P H U    
        
                 
                 
          
                 2 S W X Q D
       7 U D Q V I R U P H U
          
                                                                                                   
 ( S R F K  ( S R F K  ( S R F K  ( S R F K  ( S R F K
Figure5:NashConvsofOptunaandTransformerindifferentextensive-formgames.
lines: Uniform, PRD, and α-Rank. For example, consider Optuna Transformer
theNFGofsize200 200. IntheearlystageofPSRO,the 1.0 Uniform -Rank PRD Uniform -Rank PRD
NashConvofUniform× islowerthanPRDandα-Rank. Inthe 0.8 0.36
middlestage, α-RankquicklysurpassesUniformandPRD, 0.6 0.34
butUniformstillperformsbetterthanPRD.However,inthe
0.4
finalstage,UniformisbeatenbyPRDandα-Rank. Moreover, 0.32
0.2
wenotethatinthefinalstage,PRDcouldalsoperformbetter
0.30
thanα-Rank,asshownintheNFGofsize100 200. Similar 0.0
× 555 111000 111555 222000 222555 333000 333555 444000 444555 555 111000 111555 222000 222555 333000 333555 444000 444555
resultsareobservedinEFGs,furtherverifyingtheconclusion. Epoch Epoch
ForEFGs,inFigure4,atfirstglance,onemaycometothe
Figure6:Weightsofthreemeta-solversdeterminedbyOptunaand
conclusion that Optuna is a better option than Transformer.
TransformerintheNFGofsize200×200.
However,wenotethatthex-axisinFigure4istheBRrunning
time(whichisappropriateasinourexperimentsonlyoneBR
policyisaddedtoeachplayer’spolicyspace). Toavoidthis about10epochs. Intuitively,whenK ismuchsmallerthanK¯
misleadingconclusion,weplottheNashConvversusepochin (themaximumnumberofepisodestoobtainaconvergedBR
Figure5. Theresultsclearlyshowthatinsteadofterminating policy),thepolicyobtainedthroughtheBRoraclemaybefar
tooearly,OptunacannotfurtherdecreasetheNashConveven awayfromthetrueBRpolicy,resultinginpoorperformance.
ifitisgivenmoreepochs(10epochsmorethanTransformer). ThisisalsoreflectedinFigure5whereOptunacannotobtain
The primary reason we hypothesize is that, as the SPSRO alowerNashConvevenifitisgivenmoreepochs. Incontrast,
progresses,itisastruggleforOptunatobalancethetwoparts byofflinelearning,Transformercouldbettertrade-offbetween
(NashConvandBRtrainingtime)intheperformancemetric theNashConvandBRtrainingtimeandhence,performsbetter
ye. Specifically,atthelatterstageofSPSRO,thesecondterm thanOptuna. MoreresultscanbefoundinAppendixC.
inye woulddominatethefirstterm, eventhoughtheyhave
beennormalizedbyusingthevaluesobtainedatthefirstepoch, 5000 Optuna Transformer 5000 Optuna Transformer
makingOptunasuggestasmallernumberofupdatesforthe 4000 4000
BRpolicies(seeFigure7inSection7.3),whichonthecontrary
3000 3000
cannot further decrease the NashConv because the quality
2000 2000
oftheBRpolicieswouldbelowwithoutprovidingenough
trainingamount. TheresultsdemonstratethatTransformeris 1000 1000
moreeffectiveinhandlingsuchadilemma. 0 0
55 1100 1155 2200 2255 3300 3355 4400 4455 55 1100 1155 2200 2255 3300 3355 4400 4455
Epoch Epoch
7.3 MoreDiscussion
Figure7:NumbersofBRtrainingepisodesdeterminedbyOptuna
InthissectionandAppendixC,weprovidemorediscussion
andTransformerinEFGs.(Left)Leduc.(Right)Goofspiel.
tofurtherdeepenourunderstandingofourapproach.
Figure6showstheweightsofdifferentmeta-solvers(Uni-
form, PRD,and α-Rank)determinedbyOptunaandTrans-
8 Conclusions
formerduringSPSROrunning.Wecanseethattheweightsde-
terminedbyOptunavarydramaticallythroughoutSPSROrun- Inthiswork,wefirstattempttoexplorethepossibilityofself-
ning,whileTransformer’spredictionsaremorestable(around adaptivelydeterminingtheoptimalhyperparametervaluesin
1/3foreachsolver). Wehypothesizethatsucharelativelysta- thePSROframeworkandprovidethreecontributions: (1)the
bleweightingschemeformultiplemeta-solversisnecessary parametricPSRO(PPSRO)whichunifiesGDAandvarious
toobtainbetterperformance. Inaddition,aninterestingobser- PSROvariants;(2)theself-adaptivePSRO(SPSRO)where
vationisthattheweightsofUniformandPRDchangealmost weaimtolearnaself-adaptiveHPOpolicy;(3)anoveloffline
inamirrorform. MoreresultscanbefoundinAppendixC. HPOapproachtooptimizetheHPOpolicybasedontheTrans-
InFigure7,wefoundthatOptunaandTransformerfollow formerarchitecture. Thewell-trainedTransformer-basedHPO
verydifferentpatternstoselectthecomputingamountused policyhasthepotentialoftransferringtodifferentgameswith-
fortrainingtheBRpolicyateachepoch. ForOptuna,thenum- outfine-tuning. Experimentsondifferentgamesdemonstrate
berofepisodessuddenlydecreasestoaverylowvalueafter thesuperiorityofourapproachoverdifferentbaselines.
 Y Q R & K V D 1
thgieW
sedosipe
RB
fo
.muN
thgieW
sedosipe
RB
fo
.muNReferences Kawakami,GregKochanski,ArnaudDoucet,MarcAurelio
Ranzato,SagiPerel,andNandodeFreitas. Towardslearn-
[Akibaetal.,2019] TakuyaAkiba,ShotaroSano,Toshihiko
inguniversalhyperparameteroptimizerswithtransformers.
Yanase,TakeruOhta,andMasanoriKoyama. Optuna: A
InNeurIPS,pages32053–32068,2022.
next-generationhyperparameteroptimizationframework.
InKDD,pages2623–2631,2019. [Duanetal.,2016] YanDuan,JohnSchulman,XiChen,Pe-
terLBartlett,IlyaSutskever,andPieterAbbeel. RL2: Fast
[Baetal.,2016] JimmyLeiBa,JamieRyanKiros,andGe-
reinforcement learning via slow reinforcement learning.
offrey E Hinton. Layer normalization. arXiv preprint
arXivpreprintarXiv:1611.02779,2016.
arXiv:1607.06450,2016.
[Fengetal.,2021] XidongFeng,OliverSlumbers,ZiyuWan,
[Balduzzietal.,2019] David Balduzzi, Marta Garnelo,
Bo Liu, Stephen McAleer, Ying Wen, Jun Wang, and
YoramBachrach,WojciechCzarnecki,JulienPerolat,Max
YaodongYang. Neuralauto-curriculaintwo-playerzero-
Jaderberg, and Thore Graepel. Open-ended learning in
sumgames. InNeurIPS,pages3504–3517,2021.
symmetric zero-sum games. In ICML, pages 434–443,
2019. [Feureretal.,2015] MatthiasFeurer,JostSpringenberg,and
FrankHutter. InitializingBayesianhyperparameteropti-
[Bardenetetal.,2013] Re´mi Bardenet, Ma´tya´s Brendel,
mization via meta-learning. In AAAI, pages 1128–1135,
Bala´zs Ke´gl, and Michele Sebag. Collaborative hyper-
2015.
parametertuning. InICML,pages199–207,2013.
[FiezandRatliff,2021] TannerFiezandLillianJRatliff. Lo-
[Beltagyetal.,2020] IzBeltagy,MatthewEPeters,andAr-
calconvergenceanalysisofgradientdescentascentwith
manCohan. Longformer: Thelong-documenttransformer.
finitetimescaleseparation. InICLR,2021.
arXivpreprintarXiv:2004.05150,2020.
[HeinrichandSilver,2016] Johannes Heinrich and David
[Brown,1951] GeorgeWBrown. Iterativesolutionofgames
Silver. Deep reinforcement learning from self-
byfictitiousplay. ActivityAnalysisofProductionandAllo-
play in imperfect-information games. arXiv preprint
cation,13(1):374,1951.
arXiv:1603.01121,2016.
[Charton,2021] Franc¸oisCharton. Linearalgebrawithtrans- [Hoetal.,1998] Teck-Hua Ho, Colin Camerer, and Keith
formers. arXivpreprintarXiv:2112.01898,2021. Weigelt. Iterateddominanceanditeratedbestresponsein
[Chenetal.,2017] YutianChen,MatthewWHoffman,Ser- experimental“p-beautycontests”. TheAmericanEconomic
gioGo´mezColmenarejo,MishaDenil,TimothyPLillicrap, Review,88(4):947–969,1998.
MattBotvinick,andNandoFreitas. Learningtolearnwith- [Jainetal.,2013] ManishJain,VincentConitzer,andMilind
outgradientdescentbygradientdescent. InICML,pages Tambe. Securityschedulingforreal-worldnetworks. In
748–756,2017. AAMAS,pages215–222,2013.
[Chenetal.,2021a] Lili Chen, Kevin Lu, Aravind Ra- [Janneretal.,2021] MichaelJanner,QiyangLi,andSergey
jeswaran,KiminLee,AdityaGrover,MishaLaskin,Pieter Levine. Offlinereinforcementlearningasonebigsequence
Abbeel, Aravind Srinivas, and Igor Mordatch. Decision modelingproblem. InNeurIPS,pages1273–1286,2021.
transformer: Reinforcementlearningviasequencemodel-
[Jordanetal.,2010] PatrickRJordan,LJulianSchvartzman,
ing. InNeurIPS,pages15084–15097,2021.
andMichaelPWellman. Strategyexplorationinempirical
[Chenetal.,2021b] MarkChen,JerryTworek,HeewooJun, games. InAAMAS,pages1131–1138,2010.
Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared
[KrauseandOng,2011] Andreas Krause and Cheng Ong.
Kaplan,HarrisonEdwards,YuriBurda,NicholasJoseph,
Contextual Gaussian process bandit optimization. In
GregBrockman,AlexRay,RaulPuri,GretchenKrueger,
NeurIPS,pages2447–2455,2011.
Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela
Mishkin,BrookeChan,ScottGray,NickRyder,Mikhail [LampleandCharton,2019] GuillaumeLampleandFranc¸ois
Pavlov,AletheaPower,LukaszKaiser,MohammadBavar-
Charton.Deeplearningforsymbolicmathematics.InICLR,
ian,ClemensWinter,PhilippeTillet,FelipePetroskiSuch, 2019.
DaveCummings,MatthiasPlappert,FotiosChantzis,Eliz- [Lanctotetal.,2017] Marc Lanctot, Vinicius Zambaldi,
abethBarnes,ArielHerbert-Voss,WilliamHebgenGuss, Audru¯nasGruslys,AngelikiLazaridou,KarlTuyls,Julien
Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Pe´rolat,DavidSilver,andThoreGraepel. Aunifiedgame-
Babuschkin,SuchirBalaji,ShantanuJain,WilliamSaun- theoreticapproachtomultiagentreinforcementlearning. In
ders,ChristopherHesse,AndrewN.Carr,JanLeike,Joshua NeurIPS,pages4193–4206,2017.
Achiam, Vedant Misra, Evan Morikawa, Alec Radford,
[Lanctotetal.,2019] MarcLanctot,EdwardLockhart,Jean-
Matthew Knight, Miles Brundage, Mira Murati, Katie
BaptisteLespiau, ViniciusZambaldi, SatyakiUpadhyay,
Mayer,PeterWelinder,BobMcGrew,DarioAmodei,Sam
Julien Pe´rolat, Sriram Srinivasan, Finbarr Timbers, Karl
McCandlish,IlyaSutskever,andWojciechZaremba. Eval-
Tuyls,ShayeganOmidshafiei,DanielHennes,DustinMor-
uating large language models trained on code. CoRR,
rill, Paul Muller, Timo Ewalds, Ryan Faulkner, Ja´nos
abs/2107.03374,2021.
Krama´r, Bart De Vylder, Brennan Saeta, James Brad-
[Chenetal.,2022] Yutian Chen, Xingyou Song, Chansoo bury,DavidDing,SebastianBorgeaud,MatthewLai,Ju-
Lee, Zi Wang, Qiuyi Zhang, David Dohan, Kazuya lianSchrittwieser,ThomasAnthony,EdwardHughes,IvoDanihelka,andJonahRyan-Davis. OpenSpiel: Aframe- [Mulleretal.,2020] Paul Muller, Shayegan Omidshafiei,
workforreinforcementlearningingames. arXivpreprint MarkRowland,KarlTuyls,JulienPerolat,SiqiLiu,Daniel
arXiv:1908.09453,2019. Hennes,LukeMarris,MarcLanctot,EdwardHughes,Zhe
[Leeetal.,2022] Kuang-Huei Lee, Ofir Nachum, Sherry Wang,GuyLever,NicolasHeess,ThoreGraepel,andRemi
Munos. A generalized training approach for multiagent
Yang, LisaLee, C.DanielFreeman, SergioGuadarrama,
learning. InICLR,2020.
IanFischer,WinnieXu,EricJang,HenrykMichalewski,
andIgorMordatch. Multi-gamedecisiontransformers. In [Mulleretal.,2022a] PaulMuller,MarkRowland,Romuald
NeurIPS,pages27921–27936,2022. Elie,GeorgiosPiliouras,JulienPerolat,MathieuLauriere,
[Levineetal.,2020] Sergey Levine, Aviral Kumar, George RaphaelMarinier,OlivierPietquin,andKarlTuyls. Learn-
Tucker,andJustinFu. Offlinereinforcementlearning: Tu- ingequilibriainmean-fieldgames: Introducingmean-field
torial,review,andperspectivesonopenproblems. arXiv PSRO. InAAMAS,pages926–934,2022.
preprintarXiv:2005.01643,2020. [Mu¨lleretal.,2022b] Samuel Mu¨ller, Noah Hollmann, Se-
[Lietal.,2022] YujiaLi,DavidChoi,JunyoungChung,Nate bastianPinedaArango,JosifGrabocka,andFrankHutter.
Kushman,JulianSchrittwieser,Re´miLeblond,TomEccles, TransformerscandoBayesianinference. InICLR,2022.
JamesKeeling,FelixGimeno,AgustinDalLago,Thomas [Omidshafieietal.,2019] Shayegan Omidshafiei, Christos
Hubert, PeterChoy, CypriendeMassond’Autume, Igor
Papadimitriou,GeorgiosPiliouras,KarlTuyls,MarkRow-
Babuschkin,XinyunChen,Po-SenHuang,JohannesWelbl,
land,Jean-BaptisteLespiau,WojciechMCzarnecki,Marc
SvenGowal,AlexeyCherepanov,JamesMolloy,DanielJ.
Lanctot,JulienPerolat,andRemiMunos. α-rank: Multi-
Mankowitz, Esme Sutherland Robson, Pushmeet Kohli,
agentevaluationbyevolution.ScientificReports,9(1):9937,
NandodeFreitas,KorayKavukcuoglu,andOriolVinyals.
2019.
Competition-levelcodegenerationwithAlphaCode. Sci-
ence,378(6624):1092–1097,2022. [Perez-Nievesetal.,2021] Nicolas Perez-Nieves, Yaodong
Yang, Oliver Slumbers, David H Mguni, Ying Wen, and
[Liuetal.,2021a] XiangyuLiu,HangtianJia,YingWen,Yu-
JunWang. Modellingbehaviouraldiversityforlearningin
jingHu,YingfengChen,ChangjieFan,ZhipengHu,and
open-endedgames. InICML,pages8514–8524,2021.
YaodongYang. Towardsunifyingbehavioralandresponse
diversityforopen-endedlearninginzero-sumgames. In [Robinson,1951] Julia Robinson. An iterative method of
NeurIPS,pages941–952,2021. solvingagame. AnnalsofMathematics,pages296–301,
1951.
[Liuetal.,2021b] Ze Liu, Yutong Lin, Yue Cao, Han Hu,
YixuanWei,ZhengZhang,StephenLin,andBainingGuo. [Rothfussetal.,2021] JonasRothfuss,VincentFortuin,Mar-
Swintransformer: Hierarchicalvisiontransformerusing tinJosifoski,andAndreasKrause. PACOH:Bayes-optimal
shiftedwindows. InICCV,pages10012–10022,2021. meta-learningwithPAC-guarantees. InICML,pages9116–
[Marrisetal.,2021] Luke Marris, Paul Muller, Marc Lanc- 9126,2021.
tot,KarlTuyls,andThoreGraepel. Multi-agenttraining [SchvartzmanandWellman,2009a] L Julian Schvartzman
beyondzero-sumwithcorrelatedequilibriummeta-solvers. andMichaelPWellman. Exploringlargestrategyspaces
InICML,pages7480–7491,2021. inempiricalgamemodeling. AgentMediatedElectronic
[Matignonetal.,2012] LaetitiaMatignon,GuillaumeJLau- Commerce(AMEC2009),page139,2009.
rent,andNadineLeFort-Piat. Independentreinforcement [SchvartzmanandWellman,2009b] L Julian Schvartzman
learnersincooperativeMarkovgames: Asurveyregard- andMichaelPWellman. StrongerCDAstrategiesthrough
ingcoordinationproblems. TheKnowledgeEngineering empiricalgame-theoreticanalysisandreinforcementlearn-
Review,27(1):1–31,2012. ing. InAAMAS,pages249–256,2009.
[McAleeretal.,2020] StephenMcAleer,JohnBLanier,Roy [Smithetal.,2021] Max Smith, Thomas Anthony, and
Fox,andPierreBaldi. PipelinePSRO:Ascalableapproach
Michael Wellman. Iterative empirical game solving via
forfindingapproximateNashequilibriainlargegames. In
singlepolicybestresponse. InICLR,2021.
NeurIPS,pages20238–20248,2020.
[Snoeketal.,2012] Jasper Snoek, Hugo Larochelle, and
[McMahanetal.,2003] H Brendan McMahan, Geoffrey J
Ryan P Adams. Practical Bayesian optimization of ma-
Gordon, and Avrim Blum. Planning in the presence of
chinelearningalgorithms. InNeurIPS,pages2951–2959,
costfunctionscontrolledbyanadversary. InICML,pages
2012.
536–543,2003.
[Swerskyetal.,2013] Kevin Swersky, Jasper Snoek, and
[Mnihetal.,2015] Volodymyr Mnih, Koray Kavukcuoglu,
Ryan P Adams. Multi-task Bayesian optimization. In
DavidSilver,AndreiARusu,JoelVeness,MarcGBelle-
NeurIPS,pages2004–2012,2013.
mare, Alex Graves, Martin Riedmiller, Andreas K Fid-
jeland, Georg Ostrovski, Stig Petersen, Charles Beattie, [Touvronetal.,2023] HugoTouvron,ThibautLavril,Gautier
AmirSadik,IoannisAntonoglou,HelenKing,Dharshan Izacard,XavierMartinet,Marie-AnneLachaux,Timothe´e
Kumaran,DaanWierstra,ShaneLegg,andDemisHassabis. Lacroix, Baptiste Rozie`re, Naman Goyal, Eric Hambro,
Human-levelcontrolthroughdeepreinforcementlearning. FaisalAzhar,etal. LLaMA:Openandefficientfoundation
Nature,518(7540):529–533,2015. languagemodels. arXivpreprintarXiv:2302.13971,2023.[Vaswanietal.,2017] AshishVaswani,NoamShazeer,Niki BeichenZhang,JunjieZhang,ZicanDong,etal. Asurvey
Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, oflargelanguagemodels.arXivpreprintarXiv:2303.18223,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyou 2023.
need. InNeurIPS,pages5998–6008,2017.
[Zhengetal.,2022] QinqingZheng,AmyZhang,andAditya
[Volppetal.,2019] Michael Volpp, Lukas P Fro¨hlich, Grover. Online decision transformer. In ICML, pages
KirstenFischer,AndreasDoerr,StefanFalkner,FrankHut- 27042–27059,2022.
ter,andChristianDaniel. Meta-learningacquisitionfunc-
tionsfortransferlearninginBayesianoptimization. arXiv
preprintarXiv:1904.02642,2019.
[Wangetal.,2016] JaneXWang,ZebKurth-Nelson,Dhruva
Tirumala, Hubert Soyer, Joel Z Leibo, Remi Munos,
CharlesBlundell,DharshanKumaran,andMattBotvinick.
Learning to reinforcement learn. arXiv preprint
arXiv:1611.05763,2016.
[Wangetal.,2019] YufeiWang,ZheyuanRyanShi,Lantao
Yu,YiWu,RohitSingh,LucasJoppa,andFeiFang. Deep
reinforcementlearningforgreensecuritygameswithreal-
timeinformation. InAAAI,pages1401–1408,2019.
[Wangetal.,2022] Yongzhao Wang, Qiurui Ma, and
Michael P Wellman. Evaluating strategy exploration in
empiricalgame-theoreticanalysis. InAAMAS,pages1346–
1354,2022.
[Wellman,2006] MichaelPWellman. Methodsforempirical
game-theoreticanalysis.InAAAI,volume980,pages1552–
1556,2006.
[Wenetal.,2022] MuningWen,JakubGrudzienKuba,Runji
Lin, WeinanZhang, YingWen, JunWang, andYaodong
Yang. Multi-agent reinforcement learning is a sequence
modelingproblem. arXivpreprintarXiv:2205.14953,2022.
[WistubaandGrabocka,2020] Martin Wistuba and Josif
Grabocka. Few-shotBayesianoptimizationwithdeepker-
nelsurrogates. InICLR,2020.
[Xuetal.,2022] Mengdi Xu, Yikang Shen, Shun Zhang,
YuchenLu,DingZhao,JoshuaTenenbaum,andChuang
Gan. Promptingdecisiontransformerforfew-shotpolicy
generalization. InICML,pages24631–24645,2022.
[Yangetal.,2020] YaodongYang,RasulTutunov,PhuSakul-
wongtana,andHaithamBouAmmar. αα-Rank: Practically
scalingα-Rankthroughstochasticoptimisation.InAAMAS,
pages1575–1583,2020.
[Yuanetal.,2021] LiYuan,YunpengChen,TaoWang,Wei-
haoYu,YujunShi,Zi-HangJiang,FrancisEHTay,Jiashi
Feng,andShuichengYan. Tokens-to-tokenViT:Training
visiontransformersfromscratchonImageNet. InICCV,
pages558–567,2021.
[Zahavyetal.,2020] TomZahavy,ZhongwenXu,VivekVee-
riah, Matteo Hessel, Junhyuk Oh, Hado P van Hasselt,
DavidSilver,andSatinderSingh. Aself-tuningactor-critic
algorithm. InNeurIPS,volume33, pages20913–20924,
2020.
[Zhaoetal.,2021] Hengshuang Zhao, Li Jiang, Jiaya Jia,
Philip HS Torr, and Vladlen Koltun. Point transformer.
InICCV,pages16259–16268,2021.
[Zhaoetal.,2023] Wayne Xin Zhao, Kun Zhou, Junyi Li,
TianyiTang,XiaoleiWang,YupengHou,YingqianMin,A FrequentlyAskedQuestions has been explored and demonstrated better performance in
PSRO[Fengetal.,2021]anddoubleoracle(DO)[Wanget
Q1. Whyself-adaptiveHPOpolicyisnecessaryinPSRO? al.,2019],whichprovidessupporttoourmotivation.
PSRO[Lanctotetal.,2017]sincepioneeredhasbeenthe
Q3. DiscussiononthehyperparametersfortheBRoracle.
mainalgorithmforsolvingvariousgames.However,determin-
Inthiswork,weconsidertwohyperparametersintheBR
ingthehyperparametervaluesinPSROcouldbedifficultin
oracle: theinitializationparameterβ andthenumberofBR
complexgames. Ononehand,ateachepoch,itneedstosolve
policy updates K. Here, we provide more explanations on
the meta-game to obtain the meta-strategies of all players,
theparameterK. (1)PSROisadeeplearningextensionof
whichcouldbecomputationallydifficult,e.g.,solvingα-Rank
doubleoracle(DO),whereRLisadoptedtocomputethebest
may be NP-hard [Yang et al., 2020]. On the other hand, at
response. However,RLisnotguaranteedtocomputetheexact
eachepoch,itneedstocomputetheBRsofallplayers,which bestresponse[Lanctotetal.,2017],amoresuitablenotionis
couldbetime-consumingbecause,incomplexgamessuchas
“betterresponse”,i.e.,anewresponsebetterthantheexisting
pokerandsoccer,theBRsareoftenapproximatedbydeepRL responses,whichisalsoconsideredinDO[Jainetal.,2013].
algorithms such as DQN [Mnih et al., 2015] and obtaining
Inthissense,K isahyperparameterdeterminingtheamount
goodapproximatedBRswillrequirealargenumberofupdates.
ofcomputingusedtolearnthebetterresponse. (2)Although
Moreover,existingworkshavesuggestedvariousmeta-solvers
differentmethodscanbeadoptedtolearntheBRpolicy,e.g.,
suchasUniform[HeinrichandSilver,2016],Nash[Lanctot
DQNandPPO,itisessentialtousethesamemethod(DQN
etal.,2017],α-Rank[Mulleretal.,2020],correlatedequilib-
in this work) to ensure a fair comparison when comparing
rium[Marrisetal.,2021],andneuralmeta-solver[Fengetal.,
differentPSROmethods. Inotherwords,thecomparisonbe-
2021]. However,thereisnoexplicitruleofwhichoneisthe
tweenPSRO -DQNandPSRO -PPOwouldnotinduceany
N N
bestintermsoflearningperformance.
validargumentontheparameter K. Furthermore,evenfor
Therefore, there is a challenging dilemma: (1) we must the same BR learning method, keeping the same configura-
determinethesehyperparametervaluesbeforerunningPSRO, tionforalltheotherhyperparameters(e.g.,batchsize)isalso
and (2) evaluating a particular setting is hard because the indispensable. (3)Asthisisthefirstattempttoexplorethe
performanceisunknownuntiltheendofthePSROrunning possibilityofself-adaptivelydeterminingtheoptimalhyper-
due to its nature of iterative policy-space expansion. Thus, parametervaluesinthePSROframework,weonlyfocuson
instantiating and running PSRO typically involve extensive some of the hyperparameters. As for self-adaptively deter-
domain knowledge, rendering it unfriendly to researchers, miningmorehyperparameterssuchaslearningrateandbatch
especiallythosewhoarenotfamiliarwithgametheory. size, more techniques may be required [Chen et al., 2022;
To further boost PSRO-based research and make it more Zahavyetal.,2020]andweleaveforfutureworks.
applicabletoreal-worldproblems,anaturalideaistoestab- Q4. MoreexplanationsontheNashConvvalues.
lish a hyperparameter value selector that can be employed InsomeEFGs,theNashConvvaluesseemrelativelyhigh,
toself-adaptivelydeterminetheoptimalhyperparameterval- e.g.,inTic-Tac-Toe. Nevertheless,wenotethatthisdoesnot
uesfordifferentgames(evenifthegamesareverydifferent violatethegoalachievedbyourframework: achieveabetter
fromeachothersuchasnormal-formgamesversusextensive- balancebetweenminimizingtheNashConvofallplayersand
formgames)withorwithoutfine-tuning,whichisexactlythe theBRtrainingeffort. Inthis sense, Transformerperforms
motivationofthiswork. Ideally,suchaself-adaptivehyperpa- betterthanOptuna(aswellasallotherbaselines). Moreover,
rametervalueselectorcanhelpreducetheeffortrequiredfor though not fully converged, from Figure 5 and Figure 6 in
researcherstoconductthecostlyhyperparametertuningwhen the main text we can see that, as the learning process pro-
applyingPSROtodifferentgames,andhence,webelieveit gresses,theNashConvofTransformershowsacontinuously
willbenefitthegamecommunity. andrelativelyquicklydecreasingtrend,whiletheNashConv
Q2. Moreexplanationsonmixingmultiplemeta-solvers. ofOptunadecreasesmuchmoreslowly.
As PSRO is a general algorithmic framework, any meta-
solvercanbeusedtocomputethemeta-distribution,though
B MoreRelatedWorks
differentmeta-solversaredesignedtooptimizetowarddiffer-
ent solution concepts. However, it is worth noting that the Transformers[Vaswanietal.,2017]havedemonstratedout-
objectivethatPSROachievesisprimarilydeterminedbythe standing performance in various real-world tasks including
evaluationmetric. Forexample,in[Mulleretal.,2020],even natural language processing (NLP) [Vaswani et al., 2017;
ifα-Rankischosenasthemeta-solver,onecanstillevaluate Beltagyetal.,2020],computervision(CV)[Liuetal.,2021b;
theconvergenceofPSROtoNashequilibriumbycomputing Yuan et al., 2021], and 3D point cloud [Zhao et al., 2021].
the NashConv, the metric to measure the distance between Transformershavealsoshowntheabilityofsymbolicmanipu-
the current policy to Nash equilibrium. In this sense, we lation[LampleandCharton,2019;Chenetal.,2021b;Lietal.,
notethattheobservationsinSection4holdintermsofNash- 2022],numericalmanipulation[Charton,2021;Mu¨lleretal.,
Conv,whichsupportsourmotivationofcombiningmultiple 2022b],orboth[Chenetal.,2022]. Recently,Transformers
meta-solversduringgamesolvingusingPSRO.Moreover,as havealsobeenappliedtoofflineRLandachievedstate-of-the-
mentionedinSection4,ourobservationsdonotcauseincon- artperformanceinvariousRLbenchmarks[Chenetal.,2021a;
sistencywith[Wangetal.,2022]aswejustevaluatePSROas Janner et al., 2021; Zheng et al., 2022; Xu et al., 2022;
anonlinealgorithm. Finally,wenotethatcombiningmultiple Lee et al., 2022; Wen et al., 2022]. In this work, it is nat-
meta-solvers or changing from one meta-solver to another uraltomodeltheHPOpolicylearningasasequencemodelingproblemandthus,Transformerscanbeusedtopredicthyper- 1 import optuna
parametervalues. Intuitively,weaimtolearnanHPOpolicy 2 import ⋯
3
thatself-adaptivelydeterminesasequenceofchoicesofthe 4 def objective(trial):
hyperparameter values to optimize the objective, which is 5 # suggest new values of the parameters
similartotheRLproblemwhereanagenttriestomaximize
6 𝛼ത1 = trial.suggest_int # weight of meta-solver 1
7 ⋯
hisreturnthroughasequenceofactions. Therefore,weuse 8 𝛼ത𝑚 = trial.suggest_int # weight of meta-solver m
theDecisionTransformer[Chenetal.,2021a]topredictthe 9 𝛽ҧ = trial.suggest_int # initialization of BR
10 𝐾ഥ = trial.suggest_int # number of updates of BR
hyperparametervaluesinSPSRO. 11
Ourworkisalsorelatedtoempiricalgametheoreticanal- 12 # run one epoch of PSRO, compute the objective
13 𝜶,𝛽,𝐾 = reverse_tokenization(𝜶ഥ,𝛽ҧ,𝐾ഥ)
ysis(EGTA),anempiricalmethodologythatbridgesthegap 14 𝑦 = PSRO(𝜶,𝛽,𝐾)
between game theory and simulation for practical strategic 15 return 𝑦
16
reasoning [Wellman, 2006]. In EGTA, an empirical game
17 study = optuna.create_study(direction=“minimization”)
is estimated via simulation over the combinations of a set 18 study.optimize(lambda trial: objective(trial), n_trials=𝑒ҧ)
of strategies and then can be analyzed with standard meth- 19
20 ℋഥ𝑒ҧ = study.trials_dataframe() # one sequence of Eq.(4)
ods. In iterative approaches to EGTA, the game model is
extended by iteratively generating and adding new strate-
Figure9:AnexamplecodeforgeneratingonesequenceofEq.(4).
gies to the strategy spaces of players. One of the most
challenging problems in EGTA is the strategy exploration
problem(howtodirecttheiterativestrategygenerationpro- tionparametervalueβe,thenumberofBRepisodesKe,and
cess to construct effective game models with minimal it- theobjectiveye tothetokenembeddingswhichwillbefed
eration) and a set of works have been done toward inves- intoaTransformerblockfollowedbylayernormalization[Ba
tigating this problem [Schvartzman and Wellman, 2009b; etal.,2016]. Theoutputsarethenmappedbyalinearlayer
Schvartzman and Wellman, 2009a; Jordan et al., 2010; to the logits which will be used to compute the loss during
Wangetal., 2022]. In[SchvartzmanandWellman, 2009b], trainingorinferthenexttokenduringinference. Moreover,an
tabularRLhasbeenadoptedtoserveastheBRoracletogen- embeddingforeachepochislearnedandaddedtoeachtoken
eratenewstrategies. Investigationofstrategyexplorationwas embedding. Moredetailscanbefoundin[Chenetal.,2021a]
advanced significantly by the introduction of PSRO [Lanc- andthecorrespondingcodebase7.
tot et al., 2017] which is a flexible framework for iterative Hyperparameters. The hyperparameters are provided in
EGTA, where at each epoch, new strategies are generated Table2wherethoseoftheDecisionTransformeraresimilar
throughdeepreinforcementlearning(DRL)algorithmsuchas to[Chenetal.,2021a]. Wegivesomeremarksonsomeofthe
DQN[Mnihetal.,2015]. hyperparametersasfollows. (1)Thecontextlengthusedin
theTransformercontextcorrespondsexactlytothemaximum
numberofepochsofSPSRO.(2)Thereturn-to-goissimply
C MoreExperimentalDetails
set to 0 and the operation of decreasing return-to-go with
Inthissection,weprovidemoreexperimentaldetails. rewardisremoved. Inotherwords,theobjectivevalueateach
Dataset. Weseparatelygeneratethedatasetsfornormal-form intermediateepochisonlyusedtoserveasasignaltoguide
gamesandextensive-formgames. Forthenormal-formgame, theselectionofnewhyperparametervaluesforthenextepoch.
we generate the dataset on the game of size = (3) In this work, we consider three meta-solvers: Uniform,
1 2
200 200,andthedatasetcontains1000sequ|A enc| e× sw|A ith| the PRD,andα-Rank(m = 3), themostcommonlyusedones
form× ofEq. (4)withe¯= 50. Fortheextensive-formgame, inmostoftheexistingworks. Asforconsideringmoremeta-
wegeneratethedatasetontheLeducPoker,andthedataset solverssuchascorrelatedequilibriumandneuralmeta-solver,
consistsofatotalof1359sequenceswiththeformofEq. (4) weleaveitforfutureworks.
withe¯=50.Duringdatasetgeneration,weuseOptuna[Akiba MoreExperimentalResults. ForNFGs 150 150,250
{ × ×
etal., 2019]asthebehaviorpolicy. Usingtheterminology 250,100 200,150 300 , the weights of the three meta-
× × }
ofofflineRL[Levineetal.,2020;Chenetal.,2021a;Leeet solvers(Uniform,PRD,andα-Rank)determinedbyOptuna
al.,2022],eachsequenceisatrajectoryrecordingtheprocess andTransformerareshowninFigure10toFigure13,respec-
ofSPSROunderthebehaviorpolicy. Tomoreclearlyshow tively. ForLeducPoker,Goofspiel,Liar’sDice,Negotiation,
howtogeneratethedataset,wepresentanexamplecodein andTic-Tac-Toe,theweightsofthethreemeta-solversdeter-
Figure9. Ateachepoche,Optunafirstsuggestsnewvalues mined by Optuna and Transformer are shown in Figure 14
forthehyperparameters(Line6–10),includingtheweights toFigure18,respectively. ForLiar’sDice,Negotiation,and
ofmultiplemeta-solversandthehyperparametersrelatedto Tic-Tac-Toe,thenumbersofBRtrainingepisodesdetermined
theBRoracle,thenrunsoneepochofPSROandcomputes byOptunaandTransformerareshowninFigure19.
themetricye (Line13–15). Aftere¯epochs,Optunareturns
thesequenceofEq. (4)(Line17–20). Thisprocesswillbe
repeatedfor timestogeneratethetrainingdataset.
|D|
ModelArchitecture. WeutilizeDecisionTransformer[Chen
etal.,2021a]asourunderlyingmodelarchitecture. Specifi-
cally,4linearlayersareusedtoprojecttheweightsofthethree
meta-solvers(Uniform,PRD,andα-Rank)αe,theinitializa- 7https://github.com/kzl/decision-transformerHyperparameter Value Optuna Transformer
1.0 Uniform -Rank PRD Uniform -Rank PRD
NumberofTransformerblocks 2 0.8 0.36
Numberofattentionheads 4
Embeddingdimension 128 0.6 0.34
Batchsize 64 0.4
0.32
Contextlength(i.e.,e¯) 50
0.2
Return-to-go 0
0.30
0.0
Nonlinearity GeLU
555 111000 111555 222000 222555 333000 333555 444000 444555 555 111000 111555 222000 222555 333000 333555 444000 444555
Transformertrainingepochs 50 Epoch Epoch
Dropout 0.1 Figure13:Weightsofmeta-solversinNFG150×300.
Learningrate 3 10 4
−
∗
Adambetas (0.9,0.95)
Optuna Transformer
Gradnormclip 1.0 1.0 Uniform -Rank PRD Uniform -Rank PRD
Weightdecay 0.1 0.8 0.35
Warmuptokens 5000
0.6 0.34 NFG:2 50 1000 4
Finaltokens EFG:2 ∗ 50 ∗ 1359 ∗ 6 0.4 0.33
∗ ∗ ∗
QuantizationlevelQ 20 0.2 0.32
Numberofmeta-solversm 3 0.0 0.31
SizeofthesupportsetC 10 555 111000 111555 222000 222555 333000 333555 444000 444555 555000 555555 555 111000 111555 222000 222555 333000 333555 444000 444555
Epoch Epoch
MaxnumberofBRepisodesK¯ 5000
Figure14:Weightsofmeta-solversinLeducPoker.
Table2:Hyperparameters.
Optuna Transformer
Optuna Transformer 1.0 Uniform -Rank PRD Uniform -Rank PRD
1.0 Uniform -Rank PRD Uniform -Rank PRD 0.8 0.35
0.8 0.36 0.6 0.34
0.6 0.34 0.4 0.33
0.4 0.2 0.32
0.32
0.2 0.0 0.31
0.30 555 111000 111555 222000 222555 333000 333555 444000 444555 555000 555555 555 111000 111555 222000 222555 333000 333555 444000 444555
0.0 Epoch Epoch
555 111000 111555 222000 222555 333000 333555 444000 444555 555 111000 111555 222000 222555 333000 333555 444000 444555
Epoch Epoch Figure15:Weightsofmeta-solversinGoofspiel.
Figure10:Weightsofmeta-solversinNFG150×150.
Optuna Transformer
Optuna Transformer 1.0 Uniform -Rank PRD Uniform -Rank PRD
1.0 Uniform -Rank PRD Uniform -Rank PRD 0.8 0.35
0.8 0.36
0.6 0.34
0.6 0.34
0.4 0.33
0.4
0.32 0.2 0.32
0.2
0.0 0.31
0.0 0.30 555 111000 111555 222000 222555 333000 333555 444000 444555 555000 555555 555 111000 111555 222000 222555 333000 333555 444000 444555
555 111000 111555 222000 222555 333000 333555 444000 444555 555 111000 111555 222000 222555 333000 333555 444000 444555 Epoch Epoch
Epoch Epoch
Figure16:Weightsofmeta-solversinLiar’sDice.
Figure11:Weightsofmeta-solversinNFG250×250.
1.0 Uniform Optu -Rn aa nk PRD UniformTransfo -Rr am nker PRD 1.0 Uniform Optu -Rn aa nk PRD UniformTransfo -Rr am ne kr PRD
0.8 0.36 0.8 0.35
0.6 0.34 0.6 0.34
0.4 0.4 0.33
0.32
0.2 0.2 0.32
0.0 0.30 0.0 0.31
555 111000 111555 222000 Ep222 o555 ch333000 333555 444000 444555 555 111000 111555 222000 Ep222 o555 ch333000 333555 444000 444555 555 111000 111555 222000 222555 Ep333 o000 ch333555 444000 444555 555000 555555 555 111000 111555 222000 Ep222 o555 ch333000 333555 444000 444555
Figure12:Weightsofmeta-solversinNFG100×200. Figure17:Weightsofmeta-solversinNegotiation.
thgieW
thgieW
thgieW
thgieW
thgieW
thgieW
thgieW
thgieW
thgieW
thgieW
thgieW
thgieW
thgieW
thgieW
thgieW
thgieWOptuna Transformer
1.0 Uniform -Rank PRD Uniform -Rank PRD
0.8 0.35
0.6 0.34
0.4 0.33
0.2 0.32
0.0 0.31
555 111000 111555 222000 222555 333000 333555 444000 444555 555000 555555 555 111000 111555 222000 222555 333000 333555 444000 444555
Epoch Epoch
Figure18:Weightsofmeta-solversinTic-Tac-Toe.
5000 Optuna Transformer 5000 Optuna Transformer
4000 4000
3000 3000
2000 2000
1000 1000
0 0
55 1100 1155 2200 2255 3300 3355 4400 4455 55 1100 1155 2200 2255 3300 3355 4400 4455
Epoch Epoch
(a) Liar’sDice (b) Negotiation
5000 Optuna Transformer
4000
3000
2000
1000
0
55 1100 1155 2200 2255 3300 3355 4400 4455
Epoch
(c) Tic-Tac-Toe
Figure19:NumbersofBRtrainingepisodesdeterminedbyOptuna
andTransformerinLiar’sDice,Negotiation,andTic-Tac-Toe.The
boxesareobtainedbycounting30independentruns.
thgieW
sedosipe
RB
fo
.muN
sedosipe
RB
fo
.muN
thgieW
sedosipe
RB
fo
.muN