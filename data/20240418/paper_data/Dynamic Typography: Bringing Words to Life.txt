Dynamic Typography: Bringing Words to Life
ZICHENLIU∗,TheHongKongUniversityofScienceandTechnology,China
YIHAOMENG∗,TheHongKongUniversityofScienceandTechnology,China
HAOOUYANG,TheHongKongUniversityofScienceandTechnology,China
YUEYU,TheHongKongUniversityofScienceandTechnology,China
BOLINZHAO,TheHongKongUniversityofScienceandTechnology,China
DANIELCOHEN-OR†,Tel-AvivUniversity,Israel
HUAMINQU†,TheHongKongUniversityofScienceandTechnology,China
Fig.1. Givenaletterandatextpromptthatbrieflydescribestheanimation,ourmethodautomaticallysemanticallyreshapesaletter,andanimatesitin
vectorformatwhilemaintaininglegibility.Ourapproachallowsforavarietyofcreativeinterpretationsthatcandynamicallybringwordstolife.
1 INTRODUCTION
Textanimationservesasanexpressivemedium,transformingstaticcom-
municationintodynamicexperiencesbyinfusingwordswithmotionto Textanimationistheartofbringingtexttolifethroughmotion.By
evokeemotions,emphasizemeanings,andconstructcompellingnarratives.
animatingtexttoconveyemotion,emphasizemeaning,andcreate
Craftinganimationsthataresemanticallyawareposessignificantchallenges,
adynamicnarrative,textanimationtransformsstaticmessagesinto
demandingexpertiseingraphicdesignandanimation.Wepresentanau-
vivid,interactiveexperiences[Leeetal.2006,2002b].Thefusion
tomatedtextanimationscheme,termed“DynamicTypography”,which
ofmotionandtext,notonlycaptivatesviewers,butalsodeepens
combinestwochallengingtasks.Itdeformsletterstoconveysemanticmean-
ingandinfusesthemwithvibrantmovementsbasedonuserprompts.Our the message’s impact, making text animation prevail in movies,
techniqueharnessesvectorgraphicsrepresentationsandanend-to-end advertisements,websitewidgets,andonlinememes[Xieetal.2023].
optimization-basedframework.Thisframeworkemploysneuraldisplace- Thispaperintroducesaspecializedtextanimationschemethat
mentfieldstoconvertlettersintobaseshapesandappliesper-framemotion, focusesonanimatingindividualletterswithinwords.Thisanima-
encouragingcoherencewiththeintendedtextualconcept.Shapepreserva- tionisacompoundtask:Thelettersaredeformedtoembodytheir
tiontechniquesandperceptuallossregularizationareemployedtomaintain semanticmeaningandthenbroughttolifewithvividmovements
legibilityandstructuralintegritythroughouttheanimationprocess.We
basedontheuser’sprompt.Werefertoitas“DynamicTypography”.
demonstratethegeneralizabilityofourapproachacrossvarioustext-to-
Forexample,theletter“M”in“CAMEL”canbeanimatedwiththe
videomodelsandhighlightthesuperiorityofourend-to-endmethodology
prompt“Acamelwalkssteadilyacrossthedesert”asillustratedin
overbaselinemethods,whichmightcompriseseparatetasks.Throughquan-
Fig.1.Thisanimationschemeopensupanewdimensionoftextual
titativeandqualitativeevaluations,wedemonstratetheeffectivenessof
ourframeworkingeneratingcoherenttextanimationsthatfaithfullyinter- animationthatenrichestheuser’sreadingexperience.
pretuserpromptswhilemaintainingreadability.Ourcodeisavailableat: However,craftingsuchdetailedandprompt-awareanimationsis
https://animate-your-word.github.io/demo/. challenging,astraditionaltextanimationmethodsdemandconsid-
erableexpertiseingraphicdesignandanimation[Leeetal.2002b],
∗Denotesequalcontribution.
†Denotescorrespondingauthor. making them less accessible to non-experts. The technique we
presentaimstoautomatethetextanimationprocesstomakeit
Authors’addresses:ZichenLiu,TheHongKongUniversityofScienceandTechnology,
moreaccessibleandefficient.Followingpriorresearchinfontgen-
HongKong,China,zliucz@connect.ust.hk;YihaoMeng,TheHongKongUniversityof
ScienceandTechnology,HongKong,China,ymengas@connect.ust.hk;HaoOuyang, erationandstylization[Iluzetal.2023;Lopesetal.2019;Wangand
TheHongKongUniversityofScienceandTechnology,HongKong,China,houyangab@ Lian2021],werepresenteachinputletterandeveryoutputframe
connect.ust.hk;YueYu,TheHongKongUniversityofScienceandTechnology,Hong
asavectorized,closedshapebyacollectionofBéziercurves.This
Kong,China,yue.yu@connect.ust.hk;BolinZhao,TheHongKongUniversityofScience
andTechnology,HongKong,China,bzhaoan@connect.ust.hk;DanielCohen-Or,Tel- vectorrepresentationisresolution-independent,ensuringthattext
AvivUniversity,TelAviv,Israel,cohenor@gmail.com;HuaminQu,TheHongKong remainsclearandsharpregardlessofscale,andbringssubstantial
UniversityofScienceandTechnology,HongKong,China,huamin@ust.hk.
4202
rpA
71
]VC.sc[
1v41611.4042:viXra2 • ZichenLiu,YihaoMeng,HaoOuyang,YueYu,BolinZhao,DanielCohen-Or,andHuaminQu
editabilitybenefitsasuserscaneasilymodifythetext’sappearance stylisticelementsfromasourceimageontotext.Existingworkin-
byadjustingcontrolpoints.However,thisshifttovectorgraph- corporatestexturesynthesis[Fishetal.2020;Yangetal.2016]with
icsintroducesuniquechallengesintextanimation.Mostcurrent generativemodelslikeGANs[Azadietal.2018;Jiangetal.2019;
image-to-videomethods[Guoetal.2024;Nietal.2023;Wangetal. Maoetal.2022;Wangetal.2019].Semantictypographyrefersto
2024;Xingetal.2023]fallshortinthisnewscenarioastheyare techniquesthatblendsemanticunderstandingandvisualrepresenta-
designedtoanimaterasterizedimagesinsteadofvectorizedshapes, tionintypography.Thisencompassesturninglettersorwordsinto
andarehardtorenderreadabletext.Althoughthemostrecentwork, visualformsthatconveytheirmeaningornature,integratingtypog-
LiveSketch[Galetal.2023],introducesanapproachtoanimatear- raphywithsemanticstoenhancethemessage’sclarityandimpact.
bitraryvectorizedsketches,itstrugglestopreservelegibilityand Forinstance,[Iluzetal.2023]leveragesScoreDistillationSampling
consistencyduringanimationwhentheinputbecomesvectorized [Pooleetal.2023]todeformlettersbasedonthepre-traineddiffu-
letters,causingvisuallyunpleasantartifactsincludingflickeringand sionprior[Rombachetal.2022],encouragingtheappearanceofthe
distortion. lettertoconveytheword’ssemanticmeaning.[Tanveeretal.2023]
Toaddressthesechallenges,wedesignedanoptimization-based utilizesalatentdiffusionprocesstoconstructthelatentspaceofthe
end-to-endframeworkthatutilizestwoneuraldisplacementfields, givensemantic-relatedstyleandthenintroducesadiscriminatorto
representedincoordinates-basedMLP.Thefirstfielddeformsthe blendthestyleintotheglyphshape.
originalletterintothebaseshape,settingthestageforanimation. Theseworksonlyproducestaticimages,whichinmanycases
Subsequently,thesecondneuraldisplacementfieldlearnstheper- struggletovividlyandeffectivelycommunicatemeaningfulseman-
framemotionappliedtothebaseshape.Thetwofieldsarejointly ticmessages.Incontrast,ourproposed“DynamicTypograph”in-
optimizedusingthescore-distillationsampling(SDS)loss[Poole fusestextwithvibrantmotions,whichismoreeffectiveincapturing
etal.2023]tointegratemotionpriorsfromapre-trainedtext-to- theuser’sattentionandgivinganaestheticallypleasingimpression
video model [Wang et al. 2023], to encourage the animation to comparedtostatictext[MinakuchiandKidawara2008].
align with the intended textual concept. We encode the control
pointcoordinatesoftheBéziercurveintohigh-frequencyencoding 2.2 DynamicTextAnimation
[Mildenhalletal.2021]andadoptcoarse-to-finefrequencyanneal-
Giventheeffectivenessofanimationsincapturingandretaining
ing[Parketal.2021]tocapturebothminuteandlargemotions.To
audienceattention[ChangandUngar1993],severalstudieshave
preservethelegibilityoftheletterthroughouttheanimation,weap-
embarkedondesigningdynamictextanimations.Anotablearea
plyperceptualloss[Zhangetal.2018]asaformofregularizationon
isdynamicstyletransfer,whichaimstoadaptthevisualstyleand
thebaseshape,tomaintainaperceptualresemblancetotheoriginal
motionpatternsfromareferencevideotothetargettext.Pioneering
letter.Additionally,topreservetheoverallstructureandappear-
workby[Menetal.2019]transferredastylefromasourcevideo
anceduringanimation,weintroduceanovelshapepreservation
displayingdynamictextanimationsontotargetstatictext.[Yang
regularizationbasedonthetriangulation[HormannandGreiner
et al. 2021] further enhanced versatility by using a scale-aware
2000]ofthebaseshape,whichforcesthedeformationbetweenthe
Shape-MatchingGANtohandlediverseinputstyles.
consecutiveframestoadheretotheprincipleofbeingconformal
Kinetictypography[Fordetal.1997]representsanotherinno-
withrespecttothebaseshape.
vativedirectionintextanimation,whichintegratesmotionwith
Ourapproachisdesignedtobedata-efficient,eliminatingtheneed
texttoconveyorenhanceamessage.Creatingkinetictypography
foradditionaldatacollectionorthefine-tuningoflarge-scalemodels.
isalabor-intensiveandchallengingtask,motivatingmanyworks
Furthermore,ourmethodgeneralizeswelltovarioustext-to-video
toreducetheburdenanddemocratizethistechniquetothepublic
models,enablingtheincorporationofupcomingdevelopmentsin
[Fordetal.1997;Forlizzietal.2003;Leeetal.2002a;Minakuchi
thisarea.Wequantitativelyandqualitativelytestedourtextan-
andTanaka2005].RecentadvancementsinAIhaveenabledafully
imationgenerationmethodagainstvariousbaselineapproaches,
automatedgenerationofkinetictypography.Forexample,[Xieetal.
usingabroadspectrumofprompts.Theresultsdemonstratethat
2023]utilizesapre-trainedmotiontransfermodel[Siarohinetal.
thegeneratedanimationnotonlyaccuratelyandaestheticallyin-
2019]toapplyanimationpatternsfromamemeGIFontotext.
terpretstheinputtextpromptdescriptions,butalsomaintainsthe
However,theseapproachesrequirestrictlyspecifieddrivenvideos,
readabilityoftheoriginaltext,outperformingvariousbaselinemod-
whicharedifficulttoobtaininreal-lifescenarios,significantlyre-
elsinpreservinglegibilityandprompt-videoalignment.Overall,
stricting their usability and generalizability. Moreover, they are
ourframeworkdemonstratesitsefficacyinproducingcoherenttext
constrainedtogeneratespecificsimplemotionpatterns,limiting
animationsfromuserprompts,whilemaintainingthereadabilityof
theirabilitytoproduceanimationswitharbitrarycomplexsemantic
thetext,whichisachievedbythekeydesignofthelearnablebase
information.Incontrast,ourmethodisgeneralizabletoarbitrary
shapeandassociatedshapepreservationregularization.
motionpatternsandonlyneedsatextpromptastheinput.
2 RELATEDWORK
2.3 TextandImage-to-VideoGeneration
2.1 StaticTextStylization
Text-to-Videogenerationaimsatautomaticallyproducingcorre-
Textstylizationfocusesonamplifyingtheaestheticqualitiesoftext spondingvideosbasedontextualdescriptions.Recentadvancements
whilemaintainingreadability,includingartistictextstyletransfer indiffusionmodelshavesignificantlyimprovedvideogeneration
andsemantictypography.Artistictextstyletransferaimstomigrate capabilities.MainstreamapproachesleveragethepowerofStableDynamicTypography:BringingWordstoLife • 3
Fig.2. Anoverviewofthemodelarchitecture.Givenaletterrepresentedasasetofcontrolpoints,theBaseFielddeformsittothesharedbaseshape,setting
thestagetoaddper-framedisplacement.Thenweduplicatethebaseshapeacross𝑘framesandutilizetheMotionFieldtopredictdisplacementsforeach
controlpointateachframe,infusingmovementtothebaseshape.Everyframeisthenrenderedbythedifferentiablerasterizer𝑅andconcatenatedas
theoutputvideo.Thebaseandmotionfieldarejointlyoptimizedbythevideoprior(𝐿 )fromfrozenpre-trainedvideofoundationmodelusingScore
𝑆𝐷𝑆
DistillationSampling,underregularizationonlegibility𝐿 𝑙𝑒𝑔𝑖𝑏𝑖𝑙𝑖𝑡𝑦andstructurepreservation𝐿𝑠𝑡𝑟𝑢𝑐𝑡𝑢𝑟𝑒.
Diffusion(SD)[Rombachetal.2022]byincorporatingtemporalin- B-Splinecurves,enablingscalableandflexibletextrendering,which
formationinalatentspace,includingAnimateDiff[Guoetal.2024], weaimtopreserve.Ourmethodoutputseachanimationframein
LVDM[Heetal.2022],MagicVideo[Zhouetal.2022],VideoCrafter thesamevectorrepresentationsasourinput.
[Chenetal.2023]andModelScope[Wangetal.2023].Beyondtext-
to-video,somemethodsattempttogeneratevideosfromagiven
imageandapromptasthecondition,suchasDynamiCrafter[Xing
etal.2023]andMotion-I2V[Shietal.2024].Severalstartupsalso
releasetheirimage-to-videogenerationservices,e.g.,Gen-2[con-
tributors2023a],PikaLabs[contributors2023b],andStableVideo Fig.3. Bźeiercurvesrepresentationofletter“B”
Diffusion(SVD)[Blattmannetal.2023].
Despiteprogress,open-sourcevideogenerationmodelsstruggle Inalignmentwiththesettingoutlinedin[Iluzetal.2023],weuse
tomaintaintextreadabilityduringmotion,letalonecreatevividtext theFreeType[DavidTurner2009]fontlibrarytoextracttheoutlines
animations.Trainingamodelcapableofgeneratinghigh-quality, ofthespecifiedletter.Subsequently,theseoutlinesareconverted
legibletextanimationsusingtheaforementionedmethodswouldre- into a closed curve composed of several cubic Bézier curves, as
quirealargedatasetoftextanimations,whichisdifficulttorequire illustrated in Fig. 3, to achieve a coherent representation across
inpractice.OnerecentworkLiveSketch[Galetal.2023]introduces differentfontsandletters.
anapproachtoanimatearbitraryvectorizedsketcheswithoutex-
3.2 ScoreDistillationSampling
tensivetraining.Thisworkleveragesthemotionpriorfromalarge
pre-trainedtext-to-videodiffusionmodelusingscoredistillation TheobjectiveofScoreDistillationSampling(SDS),originallyin-
sampling[Pooleetal.2023]toguidethemotionofinputsketches. troducedintheDreamFusion[Pooleetal.2023],istoleveragepre-
However,whentheinputbecomesvectorizedletters,LiveSketch traineddiffusionmodels’priorknowledgeforthetext-conditioned
strugglestopreservelegibilityandconsistencyduringanimation, generationofdifferentmodalities[Katziretal.2024].SDSoptimizes
leadingtoflickeringanddistortionartifactsthatseverelydegrade theparameters𝜃 oftheparametricgeneratorG(e.g.,NeRF[Milden-
videoquality.Incontrast,ourproposedmethodsuccessfullygener- hall et al. 2021]), ensuring the output of G aligns well with the
atesconsistentandprompt-awaretextanimationswhilepreserving prompt.Forillustration,assumingGisaparametricimagegenera-
thetextreadability.
tor.First,animage𝑥 =G(𝜃)isgenerated.Next,anoiseimage𝑧 𝜏(𝑥)
isobtainedbyaddingaGaussiannoise𝜖atthediffusionprocess’s
3 PRELIMINARY 𝜏-thtimestep:
3.1 VectorRepresentationandFonts
𝑧 𝜏(𝑥)=𝛼 𝜏𝑥+𝜎 𝜏𝜖, (1)
where𝛼 𝜏,and𝜎
𝜏
arediffusionmodel’snoisingschedule,and𝜖isa
Vectorgraphicscreatevisualimagesdirectlyfromgeometricshapes noisesamplefromthenormaldistributionN(0,1).
likepoints,lines,curves,andpolygons.Unlikerasterimages(like Forapre-traineddiffusionmodel𝜖 𝜙,thegradientoftheSDSloss
PNGandJPEG),whichstoredataforeachpixel,vectorgraphicsare
L𝑆𝐷𝑆 isformulatedas:
nottiedtoaspecificresolution,makingtheminfinitelyscalableand
(cid:20) 𝜕𝑥(cid:21)
moreeditable[Ferraioloetal.2000]. ∇𝜙L𝑆𝐷𝑆 = 𝑤(𝜏)(𝜖 𝜙(𝑧 𝜏(𝑥);𝑦,𝜏)−𝜖)
𝜕𝜃
, (2)
Hence,modernfontformatslikeTrueType[Penny1996]andPost-
Script[AdobeSystemsInc.1990]utilizevectorgraphicstodefine where𝑦istheconditioninginputtothediffusionmodeland𝑤(𝜏)is
glyphoutlines.TheseoutlinesaretypicallycollectionsofBézieror aweightingfunction.Thediffusionmodelpredictsthenoiseadded4 • ZichenLiu,YihaoMeng,HaoOuyang,YueYu,BolinZhao,DanielCohen-Or,andHuaminQu
totheimage𝑥 with𝜖 𝜙(𝑧 𝜏(𝑥);𝑦,𝜏).Thediscrepancybetweenthis
predictionandtheactualnoise𝜖measuresthedifferencebetween
theinputimageandonethatalignswiththetextprompt.Inthis
work,weadoptthisstrategytoextractthemotionpriorfromthe
pre-trainedtext-to-videodiffusionmodel[Wangetal.2023].
SinceSDSisusedwithrasterimages,weutilizeDiffVG[Lietal.
2020]asadifferentiablerasterizer.Thisallowsustoconvertour
vector-definedcontentintopixelspaceinadifferentiablewayfor Fig.4. Illustrationofthepriorknowledgeconflictissue.Theleftisthe
applyingtheSDSloss. deformed“R”forBULLFIGHTERwithprompt“Abullfighterholdsthe
cornersofaredcapeinbothhandsandwavesit”generatedby[Iluzetal.
4 METHOD 2023],therightisgeneratedby[Galetal.2023]toanimatethedeformed
letterwiththesameprompt.Themismatchinpriorknowledgebetween
ProblemFormulation.DynamicTypographyfocusesonanimat- separatemodelsleadstosignificantappearancechangesandsevereartifacts,
ingindividualletterswithinwordsbasedontheuser’sprompt.The ashighlightedbytheredcircles.
letterisdeformedtoembodytheword’ssemanticmeaningandthen
broughttolifebyinfusingmotionbasedontheuser’sprompt. similaritytomaintainletterlegibility(§4.2).Then,weintroducea
TheoriginalinputletterisinitializedasacubicBéziercurvescon- mesh-basedstructurepreservationlosstoensureappearanceand
trolpointsset(Fig.3),denotedas𝑃 𝑙𝑒𝑡𝑡𝑒𝑟 ={𝑝 𝑖} 𝑖𝑁 =1={(𝑥 𝑖,𝑦 𝑖)} 𝑖𝑁 =1 ∈ structureintegritybetweenframes,mitigatingissuessuchasflicker-
R𝑁×2,where𝑥,𝑦referstocontrolpoints’coordinatesinSVGcan- ingartifacts(§4.3).Finally,weutilizefrequency-basedencodingand
vas,𝑁 referstothetotalnumberofcontrolpointsoftheindicated coarse-to-fineannealingtoimprovetherepresentationofgeometry
informationandmotionquality(§4.4).
letter.Theoutputvideoconsistsofkframes,eachrepresentedbya
setofcontrolpoints,denotedas𝑉 ={𝑃𝑡}𝑘
𝑡=1
∈R𝑁·𝑘×2,where𝑃𝑡
4.1 BaseFieldandMotionField
isthecontrolpointsfor𝑡-thframe.
Ourgoalistolearnadisplacementforeachframe,addedonthe Learningtheper-framedisplacementthatdirectlyconvertstheinput
setofcontrolpointcoordinatesoftheoriginalletter’soutline.This letterintoanimationframesischallenging.Thevideopriorderived
displacementrepresentsthemotionofthecontrolpointsovertime, from foundational text-to-video models using Score Distillation
creatingtheanimationthatdepictstheuser’sprompt.Wedenotethe Sampling(SDS)isinsufficientlyrobusttoguidetheoptimization,
displacementfor𝑡-thframeasΔ𝑃𝑡 ={Δ𝑝 𝑖𝑡} 𝑖𝑁 =1={(Δ𝑥 𝑖𝑡,Δ𝑦 𝑖𝑡)} 𝑖𝑁
=1
∈ leadingtosevereartifactsthatdegradethequalityoftheanimation,
R𝑁×2,whereΔ𝑝 𝑖𝑡 referstothedisplacementofthe𝑖-thcontrolpoint includingdistortion,flickering,andabruptappearancechangesin
inthe𝑡-thframe.Thefinalvideocanbederivedas𝑉 ={𝑃 𝑙𝑒𝑡𝑡𝑒𝑟 + theadjacentframe.InspiredbytheCoDeF[Ouyangetal.2023],we
Δ𝑃𝑡}𝑘
𝑡=1.
p fir eo ldp so :s te hem bo ad se eli fin eg ldth ae ndge tn he er mat oed tiov nid fie eo ldin ,tt ow ao ddn re eu sr sa tl hd eis cp ol mac pe lm exe in tyt
Toachieveappealingresults,weidentifythreecrucialrequire-
ofthisdeformation.Bothfieldsarerepresentedbycoordinate-based
mentsforDynamicTypography:(1)TemporalConsistency.The
MultilayerPerceptron(MLP).Tobettercapturehigh-frequencyvari-
deformedlettershouldmovecoherentlywhilepreservingarela-
ationandrepresentgeometryinformation,weprojectthecoordi-
tivelyconsistentappearanceineachanimationframe.(2)Legibility
natesintoahigher-dimensionalspaceusingpositionalencoding,
Preservation.Thedeformedlettershouldremainlegibleineach
whichisthesameastheoneusedinNeRF[Mildenhalletal.2021]:
frameduringanimation.(3)SemanticAlignment.Thelettershould
bedeformedandanimatedinawaythatalignswiththesemantic 𝛾(𝑚)=(cid:16) sin(20𝜋𝑚),cos(20𝜋𝑚),...,sin(2𝐿−1𝜋𝑚),cos(2𝐿−1𝜋𝑚)(cid:17)
(3)
informationinthetextprompt.
Onestraightforwardstrategycanbefirstdeformingthestatic 𝛾(·) isappliedseparatelytoeachdimensionofthecontrolpoint
letterwithexistingmethodslike[Iluzetal.2023],thenutilizing coordinates.
ananimationmodeldesignedforarbitrarygraphicscomposedof Theobjectiveofthebasefield,denotedas𝐵,istolearnashared
Beźiercurveslike[Galetal.2023]toanimatethedeformedletter. shapeforeveryanimationframe,servingasabasetoinfusemotion.
However,thisnon-end-to-endformulationsuffersfromconflicting Itisdefinedbyafunction𝐵 : 𝛾(𝑃 𝑙𝑒𝑡𝑡𝑒𝑟) → 𝑃 𝐵,whichmapsthe
priorknowledge.Thedeformedlettergeneratedbythefirstmodel originalletter’scontrolpointscoordinates𝑃 𝑙𝑒𝑡𝑡𝑒𝑟 intobaseshapes’
maynotalignwiththepriorknowledgeoftheanimationmodel.This coordinates𝑃 𝐵,bothinR𝑁×2.
mismatchcanleadtheanimationmodeltoaltertheappearanceof The motion field, denoted as 𝑀, encodes the correspondence
thedeformedletter,leadingtoconsiderablevisualartifactsincluding between the control points in the base shape and those in each
distortionandinconsistency,seeFig.4. videoframe.InspiredbydynamicNeRFs[Parketal.2021;?]and
Therefore,toensurethecoherenceoftheentireprocess,wepro- CoDeF[Ouyangetal.2023],werepresentthevideoasa3Dvolume
poseanend-to-endarchitecturethatdirectlymapstheoriginalletter space,whereacontrolpointat𝑡-thframewithcoordinate(𝑥,𝑦)is
tothefinalanimation,asillustratedinFig.2.Toaddressthecom- representedby(𝑥,𝑦,𝑡).Specifically,weduplicatethesharedbase
plexityoflearningper-framedisplacementthatconvertstheinput shape𝑘timesandencode𝑥,𝑦,and𝑡 separatelyusingEq.3,writing
letterintoanimation,werepresentthevideoasalearnablebase itas𝑃 𝐵′ :𝛾({(𝑃 𝐵,𝑡)}𝑘 𝑡=1).Themotionfieldisdefinedasafunction
shapeandper-framemotionaddedonthebaseshape(§4.1).Addi- 𝑀 :𝑃 𝐵′ →𝑃 𝑉 thatmapscontrolpointsfromthebaseshapetotheir
tionally,weincorporatelegibilityregularizationbasedonperceptual correspondinglocations𝑃 𝑉 inthe3Dvideospace.DynamicTypography:BringingWordstoLife • 5
Tobettermodelmotion,werepresent𝑃 𝑉 as𝑃 𝐵+Δ𝑃,focusingon process.Specifically,weleverageLearnedPerceptualImagePatch
learningtheper-framedisplacementsΔ𝑃 ={Δ𝑃𝑡}𝑘 𝑡=1tobeapplied Similarity(LPIPS)[Zhangetal.2018]asalosstoregularizethe
onthebaseshape.Following[Galetal.2023],wedecomposethe perceptualdistancebetweentherasterizedimagesofthebaseshape
motionintoglobalmotion(modeledbyanaffinetransformation 𝑅(𝑃 𝐵)andtheoriginalletter𝑅(𝑃 𝑙𝑒𝑡𝑡𝑒𝑟):
matrixsharedbyallcontrolpointsofanentireframe)andlocal
motion(predictedforeachcontrolpointseparately).Considerthe
L legibility=LPIPS(𝑅(𝑃 𝐵),𝑅(𝑃 𝑙𝑒𝑡𝑡𝑒𝑟)) (6)
𝑖-thcontrolpointonthebaseshapewithcoordinate(𝑥 𝐵,𝑖,𝑦 𝐵,𝑖),its Benefitingfromourdesign,weonlyneedtoapplythisLPIPS-based
displacementon𝑡-thframeΔ𝑝 𝑖𝑡 issummedbyitslocalandglobal legibilityregularizationtermtothebaseshape,andthemotionfield
displacement: willautomaticallypropagatethislegibilityconstrainttoeachframe.
Δ𝑝 𝑖𝑡 =Δ𝑝 𝑖𝑡,𝑙𝑜𝑐𝑎𝑙 +Δ𝑝 𝑖𝑡,𝑔𝑙𝑜𝑏𝑎𝑙 (4) 4.3 Mesh-basedStructurePreservationRegularization

𝑠
𝑥
𝑠ℎ 𝑥𝑠 𝑦𝑑
𝑥 
cos𝜃 sin𝜃0 𝑥
𝐵,𝑖
𝑥
𝐵,𝑖
Theoptimizationprocessaltersthepositionsofcontrolpoints,some-
Δ𝑝 𝑖𝑡,𝑔𝑙𝑜𝑏𝑎𝑙 =   𝑠ℎ 𝑦𝑠 𝑥 𝑠 𝑦 𝑑 𝑦 

  −sin𝜃cos𝜃0  ·  𝑦 𝐵,𝑖  −  𝑦 𝐵,𝑖  , (5) t ili lm use ts rale tea ddin ing Fto igc .o 6m d.p Tle hx ein rete nr ds ee rc it nio gn os fb Se ct aw lae be ln eB Vé ez ci te or rc Gur rv ae ps h, ia cs
s
 0 0 1   0 0 1  1   1 
        (SVG)adherestothenon-zeroruleoreven-oddrule[Foley1996],
whereΔ𝑝 𝑖𝑡,𝑙𝑜𝑐𝑎𝑙 andallelementsintheper-frameglobaltransfor- whichdeterminesthefillstatusbydrawinganimaginarylinefrom
mationmatrixarelearnable. thepointtoinfinityandcountingthenumberoftimesthelinein-
Totrainthebasefieldandmotionfield,wedistillpriorknowledge tersectstheshape’sboundary.Thefrequentintersectionsbetween
fromthelarge-scalepretrainedtext-to-videomodel,usingSDSloss beziercurvescomplicatetheboundary,leadingtoalternatingblack
ofEq.2.Ateachtrainingiteration,weuseadifferentiablerasterizer andwhite“holes”withintheimage.Furthermore,theseintersec-
[Lietal.2020],denotedas𝑅,torenderourpredictedcontrolpoints tionsbetweenBéziercurvesvaryabruptlybetweenadjacentframes,
set𝑃 𝑉 intoarasterizedvideo(pixelformatvideo).Weproceedby leadingtosevereflickeringeffectsthatdegradeanimationquality,
selectingadiffusiontimestep𝜏,drawingasamplefromanormal seeFig.6.
distributionfornoise𝜖 ∼N(0,1),andthenaddingthenoisetothe
rasterizedvideo.Thevideofoundationmodeldenoisethisvideo,
basedontheuserpromptdescribingamotionpatterncloselyrelated
totheword’ssemanticmeaning(e.g.“Acamelwalkssteadilyacross
thedesert.”for“M”in“CAMEL”).TheSDSlossiscomputedinEq.2
andguidesthelearningprocesstogeneratevideosalignedwiththe
desiredtextprompt.Wejointlyoptimizethebasefieldandmotion
fieldusingthisSDSloss.Thevisualizedbaseshapedemonstrates (a)frame1 (b)frame2 (c)frame3 (d)frame1vis.
alignmentwiththeprompt’ssemantics,asshowninFig.5. Fig.6. Adjacentframesofanimationforletter“E”in“JET”.Alargeareaof
Tomaintainalegibleandconsistentappearancethroughoutthe alternatingblackandwhite“holes”occurwithineachframe,ashighlighted
animation, we propose legibility regularization and mesh-based withintheredcircles,causingsevereflickeringbetweentheadjacentframes.
structurepreservationregularization,whichwillbedescribedin (d)isthevisualizationofframe1,highlightingthecontrolpointsandthe
latersections. associatedBéziercurves.Theillustrationrevealsfrequentintersections
amongtheBéziercurvesleadingtotheflickeringartifacts.
In addition, the unconstrained degrees of freedom in motion
couldaltertheappearanceofthebaseshape,leadingtonoticeable
discrepanciesinappearancebetweenadjacentframesandtemporal
Fig.5. Baseshapeof“Y”for“GYM”withprompt“Amandoingexerciseby
inconsistency.
liftingtwodumbbellsinbothhands”
Toaddresstheseissues,weadoptDelaunayTriangulation[Barber
andHuhdanpaa1995;Delaunayetal.1934]onthebaseshapebased
4.2 LegibilityRegularization oncontrolpoints(seeFig.7).Bymaintainingthestructureofthe
triangularmesh,weleveragethestabilityoftrianglestoprevent
AcriticalrequirementforDynamicTypographyisensuringthe
frequentintersectionsbetweenBéziercurves,whilealsopreserv-
animationsmaintainlegibility.Forexample,for“M”in“CAMEL”,
ingtherelativeconsistencyoflocalgeometryinformationacross
wehopethe“M”takesontheappearanceofacamelwhilebeing
adjacentframes.
recognizableastheletter“M”.WhenemployingSDSlossfortraining,
Specifically,weemploytheanglevariation[Iluzetal.2023]of
the text-to-video foundation model’s prior knowledge naturally
thecorrespondingtriangularmeshesinadjacentframesasaform
deformstheletter’sshapetomatchthesemanticcontentofthetext
ofregularization:
prompt.However,thissignificantappearancechangecompromises
theletter’slegibilitythroughouttheanimation.
𝑘 𝑚
beT leh gu ibs, lew ,e wp or ro kp ino gse ala or ne gg su idla eri tz ha eti So Dn Ste lorm sst th oa gt ue in df eo tr hc ees ot ph te imle iztt ae tr iot no 𝑘×1
𝑚
∑︁ 𝑡=1∑︁ 𝑖=1∥𝜃(𝑇 𝑖,𝑡+1)−𝜃(𝑇 𝑖,𝑡)∥ 2, (7)6 • ZichenLiu,YihaoMeng,HaoOuyang,YueYu,BolinZhao,DanielCohen-Or,andHuaminQu
where𝑚referstothetotalnumberoftriangularmeshesineach pointsinScalableVectorGraphics(SVG)asinput.Thisallowsthe
frame,𝑇 𝑖,𝑡 referstothe𝑖-thtriangularmeshinthe𝑡-thframe,and MLPsinthebaseandmotionfieldtomoreeffectivelyrepresent
∥𝜃(𝑇 𝑖,𝑡+1)−𝜃(𝑇 𝑖,𝑡)∥ 2 referstothesumofthesquareddifference high-frequencyinformation,correspondingtothedetailedgeomet-
incorrespondinganglesbetweentwotriangles.Particularly,the ricfeatures.Additionally,whenusingcoordinate-basedMLPsto
(𝑘+1)-thframereferstothebaseshape.Duringtraining,wede- modelmotion,asignificantchallengeishowtocapturebothminute
tachthegradientsofthesecondframeineachpair.Thisallows and large motions. Following Nerfies [Park et al. 2021], we em-
ustoregularizethepreviousframe’smeshstructurewiththenext ployacoarse-to-finestrategythatinitiallytargetslow-frequency
frameasareference,andthelastframeisregularizedwiththebase (large-scale)motionandprogressivelyrefinesthehigh-frequency
shapeasareference.Hence,thestructuralconstraintwiththebase (localized)motions.Specifically,weusethefollowingformulato
shapeispropagatedtoeveryframe,allowingthepreservationof applyweightstoeachfrequencyband𝑗 inthepositionalencoding
thegeometricstructurethroughouttheanimation. oftheMLPswithinthemotionfield.
Wefindthatthisangle-basednaiveapproachiscapableofef- 1−cos(𝜋·clamp(𝛼−𝑗,0,1))
fectivelymaintainingthetriangularstructure,therebyalleviating 𝑤 𝑗(𝛼)= , (9)
2
thefrequentintersectionsoftheBeźiercurvesandpreservinga
relativelystableappearanceacrossdifferentframeswithoutsignifi-
where𝛼(𝑡) = 𝐿 𝑁𝑡 ,𝑡 is the current training iteration, and 𝑁 is a
hyper-parameterforwhen𝛼shouldreachthemaximumnumberof
cantlyaffectingthelivelinessofthemotion.Furthermore,toensure
frequencies𝐿.
thatthebaseshapeitselfmitigatesthefrequentintersectionsof
Inourexperiment,thisannealedfrequency-basedencodingre-
Beźiercurvesandcoarsespikes,weapplythesametriangulation-
sultedinhigher-qualitymotionanddetailedgeometricinformation.
basedconstraintsbetweenthebaseshapeandtheinputletter.The
wholeregularizationisillustratedinFig.7.
5 EXPERIMENTS
𝑚 Tocomprehensivelyevaluateourmethod’sability,wecreateda
1 ∑︁
L structure= 𝑚 ∥𝜃(𝑇 𝑖,𝐵)−𝜃(𝑇 𝑖,letter)∥ 2 datasetthatcoversanimationsforalllettersinthealphabet,featur-
𝑖=1 ingavarietyofelementssuchasanimals,humans,andobjects.This
(8)
𝑘 𝑚 datasetcontainsatotalof33DynamicTypographysamples.Each
1 ∑︁∑︁
+ 𝑘×𝑚 ∥𝜃(𝑇 𝑖,𝑡+1)−𝜃(𝑇 𝑖,𝑡)∥ 2 sampleincludesaword,aspecificletterwithinthewordtobeani-
𝑡=1𝑖=1 mated,andaconcisetextpromptdescribingthedesiredanimation.
WeusedKaushanScript-Regularasthedefaultfont.
Weusetext-to-video-ms-1.7bmodelinModelScope[Luoetal.
2023;Wangetal.2023]forthediffusionbackbone.Weapplyaug-
mentationsincludingrandomcropandrandomperspectivetoall
framesoftherenderedvideos.Eachoptimizationtakes1000epochs,
about40minutesonasingleH800GPU.
Toillustrateourmethod’scapabilities,wepresentsomegenerated
resultsinFig.1.Theseanimationsvividlybringthespecifiedletter
tolifewhileadheringtothepromptandmaintainingtheword’s
readability.Forfurtherexploration,westronglysuggestthereaders
gothroughtheadditionalexamplesandfull-lengthvideosonour
projectpage.
5.1 Comparisons
Wecompareourmethodwithapproachesfromtwodistinctcate-
gories:thepixel-basedstrategiesleveragingeithertext-to-videoor
image-to-videomethods,andthevector-basedanimationmethod.
Fig.7. IllustrationoftheMesh-basedstructurepreservation.Wefirstapply Withinthepixel-basedscenario,wecompareourmodelagainst
thisregularizationbetweenthebaseshapeandtheinputletter.Wepropa- theleadingtext-to-videogenerationmodelsGen-2[contributors
gatethestructuralconstrainttoeveryframebyregularizingthelastframe 2023a](rankedfirstintheEvalCrafter[Liuetal.2023]benchmark)
withthebaseshapeandregularizingeveryframewithitsnextframe. – a commercial web-based tool, and DynamiCrafter [Xing et al.
2023],thestate-of-the-artmodelforimage-to-videogenerationcon-
ditionedontext.Fortext-to-videogeneration,weappendtheprompt
4.4 Frequency-basedEncodingandAnnealing
with“whichlookslikealetter𝛽,”where𝛽representsthespecific
NeRF[Mildenhalletal.2021]havehighlightedthataheuristicap- lettertobeanimated.Intheimage-to-videocase,weusethestyl-
plicationofsinusoidalfunctionstoinputcoordinates,knownas izedlettergeneratedbytheword-as-image[Iluzetal.2023]asthe
“positionalencoding”,enablesthecoordinate-basedMLPstocapture conditioningimage.
higherfrequencycontent,asdenotedbyEq.3.Wefoundthatthis Within the vector-based scenario, we utilize LiveSketch [Gal
propertyalsoappliestoourMLPsthatusecoordinatesofcontrol etal.2023]asaframeworktoanimatevectorimages.ToensureaDynamicTypography:BringingWordstoLife • 7
Fig.8. Visualcomparisonsbetweenthebaselinesandourmodel.Text-to-imagemodel(Gen-2)generatescolorfulimagesbutfailstomaintaintheshapeof
theoriginalletter.Thepixel-basedimage-to-videomodel(DynamiCrafter)producesresultswithlittle,sometimesunreasonablemotion.Thegeneralvector
animationmodel(LiveSketch)strugglestopreservelegibilityandmaintainastableappearanceacrossframes.
faircomparison,weconditiontheanimationonthestylizedletter generatedvideosandtheircorrespondingprompts(“text-to-video
generatedbytheword-as-image[Iluzetal.2023]aswell. alignment”),weleveragetheX-CLIPscore[Maetal.2022],which
Qualitative Comparison We present the visual comparison extendsCLIP[Radfordetal.2021]tovideorecognition,toobtain
withbaselinemethodsinFig.8.Whileachievinghighresolution frame-wiseimageembeddingsandtextembeddings.Theaverage
andrealism,Gen-2strugglestogenerateframesthatkeeptheletter’s cosinesimilaritybetweentheseembeddingsreflectshowwellthe
shape,whichgreatlyharmsthelegibility.WithDynamiCrafter,the generatedvideosalignwiththecorrespondingprompts.
“SWAN”animationexhibitsminimalmovement,whilethe“GYM” WhileGen-2achievesthehighesttext-to-videoalignmentscore,
animationfeaturesunrealisticmotionthatdeviatesfromtheuser’s itseverelysuffersinlegibilitypreservation(lowestPICscore).Con-
prompt.AlthoughLiveSketchcandepicttheuser’spromptthrough versely,ourmodelexcelsinPIC(highestscore),indicatingtheeffec-
animation,itsacrificeslegibility.Also,theletter’sappearancedete- tivenessinmaintainingtheoriginalletter’sform.Whileachieving
rioratesthroughouttheanimation,asdemonstratedinthe“SWAN” thesecond-besttext-to-videoalignmentscore,ourmethodstrikesa
example.Ourmodelstrikesabalancebetweenprompt-videoalign- balancebetweenfaithfullyrepresentingboththeanimationconcept
mentandletterlegibility.Itconsistentlygeneratesanimationsthat andtheletteritself.
adheretotheuser’spromptwhilepreservingtheoriginalletter’s
form.Thisallowstheanimationtoseamlesslyintegratewithinthe 5.2 AblationStudy
originalword,asshowcasedbythein-contextresultsinFig.8.
Weconductedanablationstudytoanalyzethecontributionofeach
componentinourproposedmethod:learnablebaseshape,legibil-
Method Perceptual Text-to-Video ityregularization,mesh-basedstructurepreservationregulariza-
InputConformity(↑) Alignment(↑)
tion,andfrequencyencodingwithannealing.VisualresultsinFig.
Gen-2 0.1495 23.3687 9showcasethequalitativeimpactofremovingeachcomponent.
DynamiCrafter 0.5151 17.8124 QuantitativeresultsinTab.2furtherconfirmtheireffectiveness.
LiveSketch 0.4841 20.2402 InadditiontoPerceptualInputConformity(PIC)andText-to-
Ours 0.5301 21.4391 VideoAlignment(X-CLIPscore),weemployedwarpingerrorto
Table1. Quantitativeresultsbetweenthebaselinesandourmodel.The assesstemporalconsistency,followingEvalCrafter[Liuetal.2023].
bestandsecond-bestscoresforeachmetricarehighlightedinredandblue Thismetricestimatestheopticalflowbetweenconsecutiveframes
respectively.
usingthepre-trainedRAFTmodel[TeedandDeng2020]andcalcu-
latesthepixel-wisedifferencebetweenthewarpedimageandthe
QuantitativeComparisonTab.1presentsthequantitativeeval- targetimage.Thelowerwarpingerrorindicatessmootherandmore
uationresults.Weemployedtwometrics,PerceptualInputConfor- temporallyconsistentanimations.
mity(PIC)andText-to-VideoAlignment.FollowingDynamiCrafter BaseShapeThecalculationoflegibilityandstructurepreserva-
[Xingetal.2023],wecomputedPerceptualInputConformity(PIC) tionregularizationinvolvesthebaseshape.Hence,whenremoving
usingDreamSim’s[Pooleetal.2023]perceptualdistancemetric thelearnablebaseshape,thelegibilitylossL𝑙𝑒𝑔𝑖𝑏𝑖𝑙𝑖𝑡𝑦 iscomputed
betweeneachoutputframeandtheinputletter,averagedacrossall betweeneveryoutputframeandtheinputletter,whilethestruc-
frames.Thismetricassesseshowwelltheanimationpreservesthe turepreservationlossL𝑠𝑡𝑟𝑢𝑐𝑡𝑢𝑟𝑒 isappliedbetweeneverypairof
originalletter’sappearance.Toevaluatethealignmentbetweenthe consecutiveframes.8 • ZichenLiu,YihaoMeng,HaoOuyang,YueYu,BolinZhao,DanielCohen-Or,andHuaminQu
AsobservedinFig.9(row2),removingthesharedlearnablebase OpticalFlow Perceptual Text-to-Video
Method
shaperesultsininconsistentanimations.Specifically,ashighlighted WarpingError(↓) InputConformity(↑) Alignment(↑)
by the red circle, the appearance of the bullfighter deviates sig- FullModel 0.01645 0.5310 21.4447
NoBaseShape 0.03616 0.5178 20.0568
nificantlybetweenframes,harminglegibility.Thefindingisalso
NoLegibility 0.01561 0.4924 20.2857
supportedbyTab.2(row2),whereremovingthebaseshaperesults
NoStruc.Pre. 0.01777 0.4906 20.6285
insignificantdegradationunderallthreemetrics. NoFreq. 0.02222 0.5377 20.8280
LegibilityRegularizationWithouttheperceptualregulariza-
Table2. Quantitativeresultsoftheablationstudy.Thebestandsecond-best
tiononthebaseshape,thebaseshapestrugglestopreservelegibility. scoresforeachmetricarehighlightedinredandbluerespectively.
Asaresult,eachanimationframelosestheletter“R”shapeinFig.9
(row3),leadingtolowerPICinTab.2(row3).
StructurePreservationRegularizationRemovingmesh-based
structurepreservationallowsthebaseshape’sstructuretodeviate etal.2024],andZeroScope[Luoetal.2023].Fig.10presentsvisual
fromtheoriginalletter,causingthediscontinuitybetweenthebull- resultsforthesameanimationsample(“Knight”)witheachbase
fighterandcapeinthebaseshapeandallframes,ashighlighted model.
inTab.2(row4).Withoutthisregularizationterm,theanimation Whiletheletter“K”exhibitsdeformationsandanimationstyles
showsinconsistentappearancesacrossdifferentframes,whichde- uniquetoeachmodel,allanimationsaccuratelydepicttheuser’s
gradesthelegibility,leadingtothelowestPICinTab.2(row4). promptandmaintainthebasic“K”shape.Thisshowcasesthegen-
Frequency Encoding and Annealing When removing fre- eralizabilityofourmethod.Hence,futureadvancementsintext-
quencyencodingandcoarse-to-fineannealing,themotionandge- to-videomodelswithstronger priorknowledgewillbenefitour
ometryqualitysuffers.Forexample,thebullfighteranimationin approach.
Fig.9(row5)showsunreasonablemotionandgeometrydetails,
resultinginananimationthatdoesnotaccuratelyrepresentthetext
prompt.Moreover,thedegradationinmotionqualityalsoharms
thetemporalconsistency,Tab.2(row5).
Fig.10. Visualresultsofthesameanimationsampleusingdifferenttext-to-
videobasemodels.
6 CONCLUSION
Weproposeanautomatedtextanimationscheme,termed“Dynamic
Typography,”thatdeformsletterstoconveysemanticmeaningand
animatesthemvividlybasedonuserprompts.Ourmethodisan
end-to-endoptimization-basedapproachandisgeneralizabletoarbi-
Fig.9. Visualcomparisonsofablationstudy.Removingbaseshapeorstruc- trarywordsandmotionpatterns.Nevertheless,thereremainseveral
turepreservationregularizationresultsinshapedeviationandflickering. limitations.First,themotionqualitycanbeboundedbythevideo
Withoutlegibilityregularization,eachanimationframelosestheletter“R” foundationmodel,whichmaybeunawareofspecificmotionsin
shape.Theabsenceoffrequencyencodingandannealingleadstothedegra-
somecases.However,ourframeworkismodel-agnostic,whichfacil-
dationofthemotionqualityandgeometrydetails.
itatesintegrationwithfutureadvancementsindiffusion-basedvideo
foundationmodels.Besides,challengesarisewhenuser-provided
5.3 Generalizability
textpromptsdeviatesignificantlyfromoriginallettershapes,com-
Ouroptimizationframework,leveragingScoreDistillationSampling plicatingthemodel’sabilitytostrikeabalancebetweengenerating
(SDS),achievesgeneralizationacrossvariousdiffusion-basedtext-to- semantic-awarevividmotionandpreservingthelegibilityofthe
videomodels.Todemonstratethis,weapplieddifferentbasemodels originalletter.Wehopethatourworkcanopenthepossibilityfor
forcomputingL𝑆𝐷𝑆,includingthe1.7-billionparametertext-to- furtherresearchofsemantic-awaretextanimationthatincorporates
videomodelfromModelScope[Wangetal.2023],AnimateDiff[Guo therapiddevelopmentofvideogenerationmodels.DynamicTypography:BringingWordstoLife • 9
REFERENCES
Tzu-MaoLi,MichalLukáč,MichaëlGharbi,andJonathanRagan-Kelley.2020.Differen-
AdobeSystemsInc.1990. AdobeType1FontFormat. AddisonWesleyPublishing tiablevectorgraphicsrasterizationforeditingandlearning.ACMTransactionson
Company. Graphics(Dec2020),1–15. https://doi.org/10.1145/3414685.3417871
SamanehAzadi,MatthewFisher,VladimirKim,ZhaowenWang,EliShechtman,and YaofangLiu,XiaodongCun,XueboLiu,XintaoWang,YongZhang,HaoxinChen,Yang
TrevorDarrell.2018. Multi-ContentGANforFew-ShotFontStyleTransfer.In Liu,TieyongZeng,RaymondChan,andYingShan.2023.Evalcrafter:Benchmarking
2018IEEE/CVFConferenceonComputerVisionandPatternRecognition. https: andevaluatinglargevideogenerationmodels.arXivpreprintarXiv:2310.11440(2023).
//doi.org/10.1109/cvpr.2018.00789 RaphaelGontijoLopes,DavidHa,DouglasEck,andJonathonShlens.2019.Alearned
CBarberandHannuHuhdanpaa.1995. Qhull.TheGeometryCenter,Universityof representationforscalablevectorgraphics.InProceedingsoftheIEEE/CVFInterna-
Minnesota. tionalConferenceonComputerVision.7930–7939.
AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,MaciejKilian, ZhengxiongLuo,DayouChen,YingyaZhang,YanHuang,LiangWang,YujunShen,
DominikLorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal.2023. DeliZhao,JingrenZhou,andTieniuTan.2023.VideoFusion:DecomposedDiffusion
Stablevideodiffusion:Scalinglatentvideodiffusionmodelstolargedatasets.arXiv ModelsforHigh-QualityVideoGeneration.InProceedingsoftheIEEE/CVFConference
preprintarXiv:2311.15127(2023). onComputerVisionandPatternRecognition(CVPR).
Bay-WeiChangandDavidUngar.1993.Animation:fromcartoonstotheuserinterface. YiweiMa,GuohaiXu,XiaoshuaiSun,MingYan,JiZhang,andRongrongJi.2022.
InProceedingsofthe6thAnnualACMSymposiumonUserInterfaceSoftwareandTech- X-CLIP:End-to-EndMulti-grainedContrastiveLearningforVideo-TextRetrieval.
nology(Atlanta,Georgia,USA)(UIST’93).AssociationforComputingMachinery, InProceedingsofthe30thACMInternationalConferenceonMultimedia(<conf-
NewYork,NY,USA,45–55. https://doi.org/10.1145/168642.168647 loc>,<city>Lisboa</city>,<country>Portugal</country>,</conf-loc>)(MM’22).
HaoxinChen,MenghanXia,YingqingHe,YongZhang,XiaodongCun,ShaoshuYang, AssociationforComputingMachinery,NewYork,NY,USA,638–647. https://doi.
JinboXing,YaofangLiu,QifengChen,XintaoWang,etal.2023.Videocrafter1:Open org/10.1145/3503161.3547910
diffusionmodelsforhigh-qualityvideogeneration.arXivpreprintarXiv:2310.19512 WendongMao,ShuaiYang,HuihongShi,JiayingLiu,andZhongfengWang.2022.
(2023). Intelligenttypography:Artistictextstyletransferforcomplextextureandstructure.
Gen-2contributors.2023a.Gen-2. https://research.runwayml.com/gen2 IEEETransactionsonMultimedia(2022).
PikaLabscontributors.2023b.Pikalabs. https://www.pika.art/ YifangMen,ZhouhuiLian,YingminTang,andJianguoXiao.2019.DynTypo:Example-
WernerLembergDavidTurner.2009.FreeTypelibrary. RetrievedMar19,2024from BasedDynamicTextEffectsTransfer.In2019IEEE/CVFConferenceonComputer
https://freetype.org/ VisionandPatternRecognition(CVPR). https://doi.org/10.1109/cvpr.2019.00602
BorisDelaunayetal.1934.Surlaspherevide.Izv.Akad.NaukSSSR,OtdelenieMatem- BenMildenhall,PratulPSrinivasan,MatthewTancik,JonathanTBarron,RaviRa-
aticheskiiiEstestvennykaNauk7,793-800(1934),1–2. mamoorthi,andRenNg.2021.Nerf:Representingscenesasneuralradiancefields
JonFerraiolo,FujisawaJun,andDeanJackson.2000.Scalablevectorgraphics(SVG)1.0 forviewsynthesis.Commun.ACM65,1(2021),99–106.
specification.iuniverseBloomington. MitsuruMinakuchiandYutakaKidawara.2008. Kinetictypographyforambient
NoaFish,LilachPerry,AmitBermano,andDanielCohen-Or.2020.SketchPatch.ACM displays.InProceedingsofthe2ndinternationalconferenceonUbiquitousinformation
TransactionsonGraphics(Dec2020),1–14. https://doi.org/10.1145/3414685.3417816 managementandcommunication. https://doi.org/10.1145/1352793.1352805
JamesDFoley.1996.Computergraphics:principlesandpractice.Vol.12110.Addison- MitsuruMinakuchiandKatsumiTanaka.2005.Automatickinetictypographycomposer.
WesleyProfessional. InProceedingsofthe2005ACMSIGCHIInternationalConferenceonAdvancesin
ShannonFord,JodiForlizzi,andSuguruIshizaki.1997.Kinetictypography.InCHI’97 computerentertainmenttechnology. https://doi.org/10.1145/1178477.1178512
extendedabstractsonHumanfactorsincomputingsystemslookingtothefuture-CHI HaomiaoNi,ChanghaoShi,KaiLi,SharonXHuang,andMartinRenqiangMin.2023.
’97. https://doi.org/10.1145/1120212.1120387 ConditionalImage-to-VideoGenerationwithLatentFlowDiffusionModels.In
JodiForlizzi,JohnnyLee,andScottHudson.2003.Thekineditsystem.InProceedings ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
oftheSIGCHIConferenceonHumanFactorsinComputingSystems. https://doi.org/ 18444–18455.
10.1145/642611.642677 HaoOuyang,QiuyuWang,YuxiXiao,QingyanBai,JuntaoZhang,KechengZheng,
RinonGal,YaelVinker,YuvalAlaluf,AmitH.Bermano,DanielCohen-Or,ArielShamir, XiaoweiZhou,QifengChen,andYujunShen.2023.Codef:Contentdeformation
andGalChechik.2023. BreathingLifeIntoSketchesUsingText-to-VideoPriors. fieldsfortemporallyconsistentvideoprocessing.arXivpreprintarXiv:2308.07926
(2023).arXiv:2311.13608[cs.CV] (2023).
YuweiGuo,CeyuanYang,AnyiRao,ZhengyangLiang,YaohuiWang,YuQiao,Maneesh KeunhongPark,UtkarshSinha,JonathanTBarron,SofienBouaziz,DanBGoldman,
Agrawala,DahuaLin,andBoDai.2024.AnimateDiff:AnimateYourPersonalized StevenMSeitz,andRicardoMartin-Brualla.2021. Nerfies:Deformableneural
Text-to-ImageDiffusionModelswithoutSpecificTuning.InTheTwelfthInterna- radiancefields.InProceedingsoftheIEEE/CVFInternationalConferenceonComputer
tionalConferenceonLearningRepresentations. https://openreview.net/forum?id= Vision.5865–5874.
Fx2SbBgcte LaurencePenny.1996. AHistoryofTrueType. RetrievedMar19,2024fromhttps:
YingqingHe,TianyuYang,YongZhang,YingShan,andQifengChen.2022. Latent //www.truetype-typography.com
VideoDiffusionModelsforHigh-FidelityVideoGenerationwithArbitraryLengths. BenPoole,AjayJain,JonathanT.Barron,andBenMildenhall.2023. DreamFusion:
(Nov2022). Text-to-3Dusing2DDiffusion.InTheEleventhInternationalConferenceonLearning
KaiHormannandGüntherGreiner.2000.MIPS:Anefficientglobalparametrization Representations. https://openreview.net/forum?id=FjNys5c7VyY
method.CurveandSurfaceDesign:Saint-Malo1999(2000),153–162. AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,Sandhini
ShirIluz,YaelVinker,AmirHertz,DanielBerio,DanielCohen-Or,andArielShamir. Agarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal.2021.
2023.Word-As-ImageforSemanticTypography.ACMTrans.Graph.42,4,Article Learningtransferablevisualmodelsfromnaturallanguagesupervision.InInterna-
151(jul2023),11pages. https://doi.org/10.1145/3592123 tionalconferenceonmachinelearning.PMLR,8748–8763.
YueJiang,ZhouhuiLian,YingminTang,andJianguoXiao.2019.SCFont:Structure- RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjornOmmer.
GuidedChineseFontGenerationviaDeepStackedNetworks.Proceedingsofthe 2022. High-ResolutionImageSynthesiswithLatentDiffusionModels.In2022
AAAIConferenceonArtificialIntelligence(Sep2019),4015–4022. https://doi.org/10. IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR). https:
1609/aaai.v33i01.33014015 //doi.org/10.1109/cvpr52688.2022.01042
OrenKatzir,OrPatashnik,DanielCohen-Or,andDaniLischinski.2024.Noise-freeScore XiaoyuShi,ZhaoyangHuang,Fu-YunWang,WeikangBian,DasongLi,YiZhang,
Distillation.InTheTwelfthInternationalConferenceonLearningRepresentations. ManyuanZhang,KaChunCheung,SimonSee,HongweiQin,etal.2024.Motion-
https://openreview.net/forum?id=dlIMcmlAdk I2V:ConsistentandControllableImage-to-VideoGenerationwithExplicitMotion
JoonhwanLee,SoojinJun,JodiForlizzi,andScottE.Hudson.2006. Usingkinetic Modeling.arXivpreprintarXiv:2401.15977(2024).
typographytoconveyemotionintext-basedinterpersonalcommunication.InPro- AliaksandrSiarohin,StéphaneLathuilière,SergeyTulyakov,ElisaRicci,andNicuSebe.
ceedingsofthe6thConferenceonDesigningInteractiveSystems(UniversityPark,PA, 2019.FirstOrderMotionModelforImageAnimation.NeuralInformationProcessing
USA)(DIS’06).AssociationforComputingMachinery,NewYork,NY,USA,41–49. Systems,NeuralInformationProcessingSystems(Jan2019).
https://doi.org/10.1145/1142405.1142414 MahamTanveer,YizhiWang,AliMahdavi-Amiri,andHaoZhang.2023.DS-Fusion:
JohnnyC.Lee,JodiForlizzi,andScottE.Hudson.2002a.Thekinetictypographyengine. ArtisticTypographyviaDiscriminatedandStylizedDiffusion.(Mar2023).
InProceedingsofthe15thannualACMsymposiumonUserinterfacesoftwareand ZacharyTeedandJiaDeng.2020.Raft:Recurrentall-pairsfieldtransformsforoptical
technology. https://doi.org/10.1145/571985.571997 flow.InComputerVision–ECCV2020:16thEuropeanConference,Glasgow,UK,August
JohnnyC.Lee,JodiForlizzi,andScottE.Hudson.2002b. Thekinetictypography 23–28,2020,Proceedings,PartII16.Springer,402–419.
engine:anextensiblesystemforanimatingexpressivetext.InProceedingsofthe JiuniuWang,HangjieYuan,DayouChen,YingyaZhang,XiangWang,andShiweiZhang.
15thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology(Paris, 2023.Modelscopetext-to-videotechnicalreport.arXivpreprintarXiv:2308.06571
France)(UIST’02).AssociationforComputingMachinery,NewYork,NY,USA, (2023).
81–90. https://doi.org/10.1145/571985.571997 WenjingWang,JiayingLiu,ShuaiYang,andZongmingGuo.2019.TypographyWith
Decor:IntelligentTextStyleTransfer.In2019IEEE/CVFConferenceonComputer10 • ZichenLiu,YihaoMeng,HaoOuyang,YueYu,BolinZhao,DanielCohen-Or,andHuaminQu
VisionandPatternRecognition(CVPR). https://doi.org/10.1109/cvpr.2019.00604 diffusionpriors.arXivpreprintarXiv:2310.12190(2023).
XiangWang,HangjieYuan,ShiweiZhang,DayouChen,JiuniuWang,YingyaZhang, ShuaiYang,ZhouhuiLian,andZhongwenGuo.2016.AwesomeTypography:Statistics-
YujunShen,DeliZhao,andJingrenZhou.2024. Videocomposer:Compositional BasedTextEffectsTransfer. CornellUniversity-arXiv,CornellUniversity-arXiv
videosynthesiswithmotioncontrollability.AdvancesinNeuralInformationProcess- (Nov2016).
ingSystems36(2024). ShuaiYang,ZhangyangWang,andJiayingLiu.2021.Shape-MatchingGAN++:Scale
YizhiWangandZhouhuiLian.2021.DeepVecFont:SynthesizingHigh-qualityVector ControllableDynamicArtisticTextStyleTransfer. IEEETransactionsonPattern
FontsviaDual-modalityLearning. ACMTransactionsonGraphics40,6(2021), AnalysisandMachineIntelligence(Jan2021),1–1. https://doi.org/10.1109/tpami.
15pages. https://doi.org/10.1145/3478513.3480488 2021.3055211
LiwenhanXie,ZhaoyuZhou,KerunYu,YunWang,HuaminQu,andSimingChen.2023. RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,andOliverWang.2018.The
Wakey-Wakey:AnimateTextbyMimickingCharactersinaGIF.InProceedings unreasonableeffectivenessofdeepfeaturesasaperceptualmetric.InProceedingsof
ofthe36thAnnualACMSymposiumonUserInterfaceSoftwareandTechnology. theIEEEconferenceoncomputervisionandpatternrecognition.586–595.
https://doi.org/10.1145/3586183.3606813 DaquanZhou,WeiminWang,HanshuYan,WeiweiLv,YizheZhu,andJiashiFeng.2022.
JinboXing,MenghanXia,YongZhang,HaoxinChen,XintaoWang,Tien-TsinWong, MagicVideo:EfficientVideoGenerationWithLatentDiffusionModels.(Nov2022).
andYingShan.2023.Dynamicrafter:Animatingopen-domainimageswithvideo