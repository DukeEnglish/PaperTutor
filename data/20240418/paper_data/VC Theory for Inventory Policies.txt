VC Theory for Inventory Policies
Yaqi Xie
BoothSchoolofBusiness,UniversityofChicago,Chicago,IL60637
Yaqi.Xie@chicagobooth.edu
Will Ma
GraduateSchoolofBusinessandDataScienceInstitute,ColumbiaUniversity,NewYork10027
wm2428@gsb.columbia.edu
Linwei Xin
BoothSchoolofBusiness,UniversityofChicago,Chicago,IL60637
Linwei.Xin@chicagobooth.edu
Advances in computational power and AI have increased interest in reinforcement learning approaches to
inventory management. This paper provides a theoretical foundation for these approaches and investigates
the benefits of restricting to policy structures that are well-established by decades of inventory theory. In
particular, we prove generalization guarantees for learning several well-known classes of inventory policies,
includingbase-stockand(s,S)policies,byleveragingthecelebratedVapnik-Chervonenkis(VC)theory.We
applytheconceptsofthePseudo-dimensionandFat-shatteringdimensionfromVCtheorytodeterminethe
generalizability of inventory policies, that is, the difference between an inventory policy’s performance on
trainingdataanditsexpectedperformanceonunseendata.Wefocusonaclassicalsettingwithoutcontexts,
but allow for an arbitrary distribution over demand sequences and do not make any assumptions such as
independence over time. We corroborate our supervised learning results using numerical simulations.
Managerially, our theory and simulations translate to the following insights. First, there is a principle
of “learning less is more” in inventory management: depending on the amount of data available, it may be
beneficialtorestrictoneselftoasimpler,albeitsuboptimal,classofinventorypoliciestominimizeoverfitting
errors. Second, the number of parameters in a policy class may not be the correct measure of overfitting
error:infact,theclassofpoliciesdefinedbyT time-varyingbase-stocklevelsexhibitsageneralizationerror
comparabletothatofthetwo-parameter(s,S)policyclass.Finally,ourresearchsuggestssituationsinwhich
itcouldbebeneficialtoincorporatetheconceptsofbase-stockandinventorypositionintoblack-boxlearning
machines, instead of having these machines directly learn the order quantity actions.
Key words: VC theory, Pseudo-dimension, data-driven algorithm design, generalization bound, estimation
error, sample complexity, shattering, inventory, base-stock policy, (s,S) policy, less is more
History: April 18, 2024
But it is the VC dimension rather than the number of parameters of the set of functions that
defines the generalization ability of a learning machine. — (Vapnik 1998, Page 160)
1
4202
rpA
71
]LM.tats[
1v90511.4042:viXraXie, Ma and Xin: VCTheoryforInventoryPolicies
2
1. Introduction
Inventory management plays a crucial role in businesses, ranging from manufacturing to distri-
bution to retailing, and is arguably the most fundamental topic in operations and supply chain
management. The design of effective inventory algorithms is particularly important in the modern
era, characterized by the rapid proliferation of AI technology, because an increasingly signifi-
cant portion of inventory replenishment decisions has shifted from human intuition to algorithmic
decision-making (e.g., Liu et al. 2023). A classical approach to inventory management involves
estimating a stochastic process for demand over time based on past data and optimizing around
this estimate. However, model selection is notoriously difficult, and it is also unclear whether the
two-step process of estimating and then optimizing leads to the best decisions (e.g., Elmachtoub
and Grigas 2022).
An emerging alternative approach to inventory management involves learning policies directly
from past data. Indeed, under the assumption that demands are uncensored—a common assump-
tion in inventory literature—it becomes possible to accurately evaluate the counterfactual scenario
of how any policy would have performed given any historical demand sequence. This allows for the
adoption of supervised learning or a “hindsight” reinforcement learning approach (Sinclair et al.
2023), which has been demonstrated to be computationally feasible in terms of policy differenti-
ation (Alvo et al. 2023) and has even been deployed in practice (Madeka et al. 2022). The latter
papers highlight the power of black-box neural networks, which do not require explicit knowledge
about the structure of optimal inventory policies to achieve high performance.
By contrast, our paper demonstrates the potential benefits of adhering to policy structures that
have been well-established in 70 years of inventory theory, beginning with Arrow et al. (1951). We
derivelearningtheoryresultsandcomplementthemwithsimulations.Weanalyzethefundamental
classes of base-stock and (s,S) policies, which are optimal in various settings with independent
demandovertime(e.g.,SnyderandShen2019).Weapproachthisasasupervisedlearningproblem
over demand sequences, in the spirit of Madeka et al. (2022), Alvo et al. (2023), but focus on
a classical setting without contexts. We use the celebrated Vapnik-Chervonenkis (VC) theory to
boundthelearning-theoreticcomplexityofthesepolicyclasseswhentrainedofflineonpastdemand
sequences, without assuming they are independent over time as this is often impossible to verify
in practice. We provide statistical learning guarantees for this training procedure, which resembles
the “hindsight” reinforcement learning approaches mentioned earlier.
VC theory is a foundational framework in the field of statistical learning theory (Vapnik and
Chervonenkis1971).Atahighlevel,VCtheoryhelpsusunderstandthecomplexityofahypothesis
class in terms of its ability to generalize to unseen data, which is critical for avoiding overfitting.
The core concept of VC theory centers on the VC dimension, along with its generalizations andXie, Ma and Xin: VCTheoryforInventoryPolicies
3
variations, such as the Pseudo-dimension (Pollard 1984), and the Fat-shattering dimension, also
known as the Pseudo -dimension (Kearns and Schapire 1994). These dimensions can be used to
γ
provide a distribution-free upper bound on the Rademacher complexity, and overall, quantify the
capability of a hypothesis class to learn complex patterns. A hypothesis class with a higher di-
mension generally possesses a greater potential to learn a more complex pattern from data, but
may suffer from reduced generalizability due to overfitting, leading to the need for a larger training
dataset to ensure good performance of the learned hypothesis.
In the context of inventory management, VC theory can be a powerful tool for establishing
rigorous bounds on the generalization error of inventory policy classes, namely, the difference
between an inventory policy’s performance on training data and its expected performance on true
data. This is useful because, as we will show, naive metrics such as the number of parameters do
not necessarily indicate generalizability. We instead use VC theory to indicate generalizability, and
corroborate our theoretical results using simulations. All in all, this reveals how effective each of
these policy class restrictions is in terms of reducing overfitting, complementing the large body of
inventory theory that already characterizes when each of these policy classes are optimal.
1.1. “Learning Less is More” Phenomenon
Let us first illustrate why restricting the inventory policies to a suboptimal class can be desirable.
Consideraclassicalbackloggedinventorymodelwithfixedcostsandzeroleadtime.Itiswellknown
that the optimal policy has an (s,S) structure: when the inventory level reaches the reorder point
s, an order is placed to bring the inventory level up to the order-up-to level S. Given substantial
historical data on demand, it might be plausible to learn the optimal (s∗,S∗) parameters for
the true demand distribution. However, in scenarios where historical data is scant, learning the
optimal (s,S) parameters for the observed demands can lead to overfitting. In such scenarios,
restricting to simpler policy classes like base-stock policies, where s=S, might exhibit superior
performance compared to the more intricate (s,S) policies. We illustrate this idea conceptually
in Figure 1, which is supported by our theoretical and simulation results in the paper. Figure 1
provides important managerial insights into the principle of “learning less is more” in inventory
management: depending on the amount of data available, we may want to restrict the policy space.
The phrase “less is more,” as coined by Theodore R. Sizer in his book “Horace’s Compromise”
(Sizer 1984), embodies one of the Coalition’s Nine Common Principles aimed at guiding school
reform initiatives. According to Sizer, the aphorism highlights the importance of students engaging
deeply and mastering a few subjects, rather than superficially skimming through a broader range
of topics. The principle of less is more has also been investigated and analyzed in machine learning,
sometimes using structural risk minimization (e.g., Chapter 7.2 of Shalev-Shwartz and Ben-DavidXie, Ma and Xin: VCTheoryforInventoryPolicies
4
Figure1 Thisfigureconceptuallydemonstratesthe“learninglessismore”phenomenoninthecontextofinven-
torymanagement.Itsuggeststhatinscenarioswithlimitedhistoricaldata,learningasimplerbase-stock
policy is more advantageous than attempting to learn the more complex optimal (s,S) policy, even
thoughthelatterisrequiredforoptimalityduetofixedcosts.Asthevolumeofavailabledataincreases,
it becomes appropriate to transition towards learning the more complex (s,S) policy.
2014), which balances between in-sample performance and model simplicity. In the context of
inventory management, the principle of “learning less is more” suggests that the employment
of more sophisticated algorithms may lead to analysis paralysis, and the additional algorithmic
complexity does not necessarily translate to better decisions.
Mirroringtheeducationalprocessundertakenwithstudents,weaimtoteachalgorithmseffective
inventorydecision-making.Weadoptthe“learninglessismore”philosophyandprovidegeneraliza-
tion guarantees for simple inventory policies trained on demand sequences by applying VC theory.
Interestingly, we theoretically prove that the generalization error of (s,S) policies is an order of
magnitude higher than that of base-stock policies. This explains why learning a base-stock policy
with one parameter can be preferable to learning an (s,S) policy with two parameters. Overall,
our results provide strong theoretical support for the phenomenon observed in Figure 1.
On the other hand, our results also reveal a surprisingly mild manifestation of the “learning
less is more” phenomenon in other settings. Consider (St) policies, another generalization of the
single-parameter base-stock policies, where we now allow varying base-stock levels {St}T to ac-
t=1
commodate non-stationary demand over a T-period. We show that the generalization error of (St)
policiesisinfactofthesameorderasbase-stockpolicies,despitehavingT parameters.Numerically,
we observe that the generalization error of (St) policies is comparable to that of (s,S) policies,
despite having significantly more parameters.
1.2. Outline of Results
We consider inventory replenishment for a single durable good over a finite, discrete time horizon
of length T. We assume backlogged demand and allow for a deterministic lead time. We defineXie, Ma and Xin: VCTheoryforInventoryPolicies
5
performance by the average inventory cost over the time horizon, which includes fixed costs for
each replenishment, backlogging costs, and holding costs. Demands, cost parameters, and policy
parameters are normalized so that given any T, the performance of any policy on any demand real-
izationisboundedbyaconstant.Thedemandrealizationindicatesthedemandforthegoodduring
each time period, drawn from an arbitrary distribution over sequences of length T (that could
exhibit correlations over time). We assume access to N independent and identically distributed
(i.i.d.) samples of sequences drawn from the same distribution, and investigate how the uniform
generalization error for a policy class Π (difference between expected performance on the true
distribution, and average empirical performance over the N samples, taking the supremum over
policies in Π) scales with N and T. We consider the three different policy classes for Π: base-stock
policies, (s,S) policies, and (St) policies.
We derive the following results:
(cid:112)
1. Base-stock policies (or “S policies”): expected generalization error is Θ( 1/N);
(cid:112) (cid:112)
2. (s,S) policies: expected generalization error is O( logT/N) and Ω( (logT/loglogT)/N);
(cid:112)
3. (St) policies: expected generalization error is Θ( 1/N).
We note that our result for (St) policies assumes no fixed replenishment costs, which is standard
whenever (St) policies are analyzed. Under this assumption, (St) policies have a generalization
error that is constant in T, while having T parameters. In stark contrast, (s,S) policies have a
generalization error that is growing with T, despite having only two parameters (we also note
that our lower bound for (s,S) policies holds even when there are no fixed replenishment costs).
Costsaside,thisprovidesfundamentalintuitionaboutthevolatilityofstatetrajectoriesofdifferent
policy classes, suggesting that the number of parameters is not the right measure of complexity.
Indeed, perturbing the parameters (s,S) can cause a missed inventory replenishment and substan-
tial divergence in the future state trajectory that persists over a long T. In particular, a small
perturbation to the parameter s or S can completely change the set of time periods during which
inventory is replenished. By contrast, the state trajectories of (St) policies are “stable” in the
right measure—exhibiting a Pseudo -dimension that does not grow with T. Intuitively, a small
γ
perturbation to any parameter St can only slightly change the inventory level.
Our results can also be stated in equivalent alternative forms, such as sample complexity. In this
form, our results say that to guarantee an expected generalization error of at most ϵ, the number
of samples required is growing with the horizon length T for (s,S) policies, but not growing with
T for base-stock or (St) policies. All of these expectation bounds can also be converted into high
probability bounds using standard concentration inequalities.
Our theoretical results require a problem-specific analysis of inventory trajectories over time,
as needed to bound Pseudo-dimension and Pseudo -dimension, which to our knowledge is new to
γXie, Ma and Xin: VCTheoryforInventoryPolicies
6
bothinventorytheoryandlearningtheory.Wealsoemphasizethatevenifoneassumeseachperiod
to have its own independent demand distribution, results 2. and 3. above were not previous known.
In particular, existing works (e.g., Levi et al. 2007, Cheung and Simchi-Levi 2019) have analyzed
the sample complexity of (St) policies under the independence assumption (in which case they are
optimal) and shown it to grow with T, in a related “multiplicative” setting. Therefore, we believe
the non-dependence on T in our additive setting to be quite surprising. We elaborate on these
points in Section 4.1.
Finally, we show that the aforementioned surprising distinction between (s,S) and (St) policies
suggested by our worst-case theoretical guarantees do translate to milder numerical settings. We
consider (s,S) policies in a setting with fixed cost (where they are near-optimal), and (St) policies
in a setting without fixed cost but with non-stationary independent demand (where they are
optimal). In both settings we compare the out-of-sample error to that of “learning less”: learning
just a base-stock policy. Overall, the intersection point in Figure 1 emerges further to the right
for (s,S) policies than (St) policies, supporting our theoretical finding that a manager needs more
data samples before they should want to “learn more” in the (s,S) setting.
1.3. The Value of Inventory Theory
Section 1.1 discussed how restricting to suboptimal inventory policy classes can reduce overfitting,
and Section 1.2 explained how the number of parameters is not the right indicator of overfitting.
Finally, we would like to distinguish between our approach of learning policy parameters (e.g.,
s,S,St), and learning order quantities (i.e., “actions”) which is the prevailing approach in papers
that use state-of-the-art deep reinforcement learning for inventory management (e.g., Gijsbrechts
et al. 2022, Madeka et al. 2022, Oroojlooyjadid et al. 2022, Alvo et al. 2023, Liu et al. 2023, Qi
et al. 2023). Although these papers focus on engineering realistic contextual settings, whereas we
focus on a parsimonious theoretical setting, the comparison between learning policy parameters
and learning order quantities can be made irrespective of the presence of contexts.
We make this comparison numerically in a concise non-contextual setting in Section 5.3. We
consider zero fixed replenishment costs, a short positive lead time, and i.i.d. demands. We first
assume that demands are backlogged, and compare learning a base-stock policy, which is provably
optimalfrominventorytheory,tolearninganarbitraryfunctionthatmapsinventorystatetoorder
quantity and is agnostic to inventory theory. Our experiments show that learning a base-stock
policy is much more sample-efficient. Next, we assume that demands are lost, in which case a base-
stock policy is no longer optimal, whereas the arbitrary function can capture the optimal policy.
Nonetheless, our results still demonstrate a significant advantage in learning a base-stock policy at
small sample sizes, i.e., “learning less is more.” We emphasize that in these experiments, the leadXie, Ma and Xin: VCTheoryforInventoryPolicies
7
time is very short, hence the state space to be captured by the arbitrary function is small, which
should only understate the benefits of learning less.
Although not analyzed in this paper, our approach of learning policy parameters can be engi-
neered in a contextual setting. To elaborate, at a time t, let X denote the context vector including
t
all information about upcoming demand, and I denote the vector of on-hand/pipeline inventory.
t
The aforementioned black-box learning approaches would learn an order quantity action a(X ,I )
t t
taking both X and I into the state, whereas we would learn a desired base-stock level S(X )
t t t
and force the order quantity a(X ,I ) to be S(X ) minus the total inventory in I . Of course, we
t t t t
are merely restricting ourselves to a special functional form of their approach, regarding how I
t
should impact a(X ,I ), which can sometimes be rediscovered by the black-box approach given
t t
enough data (Alvo et al. 2023). Nonetheless, it would be interesting to further investigate how this
restriction could potentially improve performance, especially when relevant data is limited. Our
hope is that by combining inventory concepts such as “base-stock” S and “reorder point” s with
machine learning algorithms to handle high-dimensional contexts X , classical inventory theory
t
and modern artificial intelligence can co-exist in harmony.
1.4. Organization of Paper
The rest of the paper is organized as follows. We start by discussing the related work in Section
2. We describe our model setup in Section 3, and define estimation error and approximation
error in Sections 3.1 and 3.2, respectively. We review several well-known results from learning
theory by using the concepts of Rademacher complexity, VC-dimension, Pseudo-dimension, and
Pseudo -dimension in Section 3.3. We present our theoretical results on distribution-free bounds
γ
for the expected generalization errors of three parametric policy classes in Section 4. Numerical
experiments are presented in Section 5 and we conclude the paper in Section 6.
2. Further Related Work
Data-driven inventory management:Mostpapersondata-driveninventorymanagementfocus
ontheNewsvendormodel(e.g.,Levietal.2015,Linetal.2022,BesbesandMouchtaki2023,Besbes
et al. 2023), with some exceptions. Levi et al. (2007) and Cheung and Simchi-Levi (2019) both
consider dynamic models and prove sample complexity bounds on base-stock policies under the
assumption that demands are independent across time but not necessarily identically distributed.
Huh and Rusmevichientong (2009) use stochastic gradient descent to learn inventory policies for
demand that is both independent and identically distributed over time. Ban (2020) considers a
dynamic model with demand censoring and proves the consistency of a non-parametric estimator
of (s ,S ). Yuan et al. (2021) provide an upper bound on the regret of learning the optimal (s,S)
t t
policy.Xie, Ma and Xin: VCTheoryforInventoryPolicies
8
It is quite surprising that the study of VC-type complexity, even for the simplest classes such as
base-stock and (s,S) policies, is missing; this is the gap we aim to fill with this paper. It is worth
noting that Han et al. (2023) apply existing bounds on the Pseudo-dimension of neural networks
directlytosolveaNewsvendorproblem.TheirmodelisbasedonthecontextualNewsvendormodel
that was pioneered by Ban and Rudin (2019). By contrast, our analysis focuses on the dynamics
of inventory state transitions over time and how this relates to learning-theoretic dimensions. We
notethatrecently,GuanandMiˇsi´c(2022)haveaskedsimilarquestionsforoptimalstopping,which
also considers dynamic decision-making over time but is otherwise unrelated to inventory.
The “learning less is more” phenomenon advocates for simplicity. In inventory management, this
phenomenon has been observed in Lyu et al. (2024), when designing UCB-type algorithms in a
lost-salesmodelwithleadtimes,tolearntheoptimalbase-stockpolicywithoneparameterandthe
optimal capped base-stock policy with two parameters. They note that learning the optimal one-
parameter policy is relatively easier, whereas learning the optimal two-parameter policy is more
challenging, despite being potentially more rewarding. However, their finds are purely numerical.
Tothebestofourknowledge,ourpaperisthefirstonedevelopingatheoreticframeworktoformally
analyze this phenomenon in the context of data-driven inventory management.
Data-driven algorithm design: We are tuning the parameters of policy classes based on histor-
ical sequences (instances), a process known as data-driven algorithm design (Balcan 2020, Gupta
and Roughgarden 2020). However, we derive problem-specific results by analyzing the sensitivity
of inventory state transitions to policy parameters, which appear difficult to deduce from general
theory (e.g., Balcan et al. 2021). We also believe that the inventory problem, with its combi-
nation of features—dynamic decision-making, state transitions, and distributions over arbitrary
sequences—is rather underexplored in this area, which could inspire future data-driven algorithm
design for other online algorithms problems. We note that online algorithms can be tuned based on
past arrival sequences by using the so-called “Algorithms with Predictions” framework (Lykouris
andVassilvitskii2021);however,thetheoreticalresultsthereinareofacompletelydifferentnature.
3. Model and Preliminaries
In this paper, we consider a discrete-time backlogged inventory model with lead time over a finite-
horizon. There is a lead time L∈Z between when an order is placed and when it is delivered.
≥0
There are T +L periods in the entire horizon. The demand process follows a stochastic process
with distribution D, which can be non-stationary or correlated across time. A realized demand
sequence d≜(d1,...,dT+L) consists of demand dt for period t=1,...,T +L. Let h and b be the
per-unit holding and backlogging costs, respectively, and let K be the fixed ordering cost. Without
loss of generality, we normalize h,b,K ∈[0,1] and assume D is supported on [0,U]T+L for a givenXie, Ma and Xin: VCTheoryforInventoryPolicies
9
constant U. The decision-maker must specify an inventory control policy π that determines an
order quantity at each time point with the objective of minimizing the expected average loss over
the entire horizon.
Let xt be the inventory level at the beginning of period t. Let (qt−L,...,qt−1) be the pipeline
inventory at the beginning of period t, where qt denotes the inventory ordered in period t that is
to arrive in period t+L. We assume empty initial pipeline: qt=0 for t≤0. Let It be the inventory
t−1
position at the beginning of period t before ordering, namely, It =xt + (cid:80) qt′. Let yt be the
t′=t−L
inventory level after replenishment but before seeing demand in period t, namely, yt=xt+qt−L.
Demands are backlogged and the inventory level in period t+1 is updated according to xt+1 =
yt−dt. The sequence of events in period t is summarized as follows:
1. The decision-maker observes the inventory level xt and pipeline vector (qt−L,...,qt−1);
2. A new order qt is placed based on policy π;
3. The inventory level is updated according to yt=xt+qt−L;
4. Demand dt is realized and the loss ℓt(π,d) is incurred.
In particular, define c(x)≜h[x]++b[−x]+, where [·]+ ≜max{·,0}, and the loss ℓt(π,d)≜c(yt−
dt)+K·I{qt−L>0}. Here, ℓt includes the holding and backlogging costs in period t, as well as the
fixed ordering cost associated with the order placed in period t−L that is arriving during period
T+L
t. Define ℓ(π,d)≜ 1 (cid:80) ℓt(π,d) as the losses from period L+1 to period T +L of a policy π
T
t=L+1
(averaged over time) on a given demand sequence d. Here, we ignore the losses associated with
periods 1 to L because our first action q1 only impacts losses starting from period L+1.
For the data-driven problem, the training dataset contains N demand samples d ,...,d drawn
1 N
i.i.d. from D, and each of the samples d ≜(dt)T+L consists of demand realizations from period 1
i i t=1
to period T +L. Note that this assumption is also made in Ban (2020) and Madeka et al. (2022,
§7). Here, the N samples can be interpreted as the demands of N similar products over (T +L)
periods, or as N records of one product’s demands over (T +L) consecutive periods (e.g., each
sample i=1,...,N represents one year and consists of T +L periods).
3.1. Estimation Error
To take advantage of existing results on inventory management, we aim to learn a policy π from
a well-studied parametric class of policies denoted by Π, based on the training dataset (d )N .
i i=1
Note that the family Π (to be specified in Section 3.2 below) does not necessarily contain the
optimal policy. We define R(π)≜E [ℓ(π,d)] as the true risk (i.e., expected loss) of policy π, and
d∼D
Rˆ(π)≜ 1 (cid:80)N ℓ(π,d ) as its associated empirical risk. Let L(Π)≜{ℓ(π,·):π∈Π} denote a set of
N i=1 i
lossfunctionsinducedbythepolicyclassΠ,whereeachfunctioncorrespondstoapolicyπ∈Π.Let
π∗∈arginf R(π) denote an optimal policy in the policy class Π. Given the dataset (d )N , the
π∈Π i i=1Xie, Ma and Xin: VCTheoryforInventoryPolicies
10
decision-maker uses a procedure to specify a policy π∈Π. One popular procedure is empirical risk
minimization (ERM), which produces an output πˆ∈arginf Rˆ(π). The question of interest is to
π∈Π
bound the expected risk gap between the policy πˆ produced by ERM and the optimal one π∗ in
the policy class Π. This gap is known as the estimation error (EE) of ERM with respect to (w.r.t.)
the function class L(Π) in the literature (e.g., Section 5.2 of Shalev-Shwartz and Ben-David 2014),
denoted by EE(L(Π))≜R(πˆ)−R(π∗). The expected estimation error w.r.t. the N samples (d )N
i i=1
can be bounded from above as follows:
(cid:104) (cid:105)
E [EE(L(Π))]=E R(πˆ)−Rˆ(πˆ)+Rˆ(πˆ)−Rˆ(π∗)+Rˆ(π∗)−R(π∗)
d1,...,dN∼D d1,...,dN∼D
(cid:20) (cid:21)
(cid:110) (cid:111)
≤E sup R(π)−Rˆ(π) , (1)
d1,...,dN∼D
π∈Π
(cid:104) (cid:105)
where the inequality comes from the optimality of πˆ and the fact that E Rˆ(π∗)−R(π∗) =0. Note
that from an upper bound of the expected estimation error, one can directly derive the sample
complexity, i.e., the number of samples required to guarantee a given estimation error with a given
high probability. The supremum term in (1) is referred to as the (uniform) generalization error
(GE)ortherepresentativenessofthepolicyclassΠintheliterature,whichdescribestheworst-case
scenario of overfitting within the policy class. We now formally define the generalization error of
the policy class Π w.r.t. the function class L(Π):
(cid:110) (cid:111)
GE(L(Π))≜sup R(π)−Rˆ(π) . (2)
π∈Π
It quantifies the generalization ability of an algorithm (or a policy), namely, the gap between the
algorithm’sin-sampleperformanceonthetrainingsetanditsout-of-sampleperformance.Notethat
(cid:110)(cid:12) (cid:12)(cid:111)
another widely accepted definition of uniform generalization error is sup (cid:12)R(π)−Rˆ(π)(cid:12) , which
(cid:12) (cid:12)
π∈Π
is commonly used for deriving high-probability sample complexity bounds. The analysis based on
this definition is analogous to that by (2), expect for a constant factor.
In this paper, our primary emphasis is on analyzing the expected generalization error w.r.t. the
samples(d )N ,withtheobjectiveofquantifyingthelearningcomplexityintrinsictothreedifferent
i i=1
policy classes. More specifically, we aim to upper-bound the generalization error w.r.t. the sample
size N, which would then lead to an upper bound on the estimation error. Intuitively, with a larger
horizon length T, the inventory dynamics simulated by the N samples become more unstable and
could lead to greater overfitting. We will investigate how the growth rate of the generalization
error w.r.t. T varies among different policy classes, by leveraging the concepts of VC-Dimension,
Pseudo-Dimension and Pseudo -Dimension.
γXie, Ma and Xin: VCTheoryforInventoryPolicies
11
3.2. Approximation Error
The approximation error of a policy class Π is defined as the gap between the best policy within
the class, π∗ ∈argmin R(π), and the overall optimal policy, πopt ∈argmin R(π). More for-
π∈Π π
mally, AE(L(Π))≜R(π∗)−R(πopt). It depends only on the richness of the policy class Π and is
independent of the sample size N.
The gap between the policy πˆ as determined by the ERM procedure and the overall optimal
policy πopt is equal to the sum of the estimation error EE(L(Π)) and the approximation error
AE(L(Π)). There is a trade-off in balancing the estimation error and approximation error: a richer
policy class results in a lower approximation error but might lead to overfitting and a larger es-
timation error. How to design policy classes with a balanced trade-off between estimation error
and approximation error has been explored in the learning theory literature. The approach in-
volves using prior or domain knowledge about specific problems to initially restrict Π with certain
structures. Moreover, because the approximation error does not depend on the size of the training
data N, this independence motivates an analysis of the estimation error w.r.t. N for specific policy
classes.
In this paper, we mainly focus on three policy classes:
• Π ≜{S:S∈[0,H]}: the class of all base-stock policies with a stationary base-stock level S.
S
These policies order to bring the inventory position up to S whenever the inventory position
falls below S. See Section 4.2 for a formal specification.
• Π ≜{(s,S):H ≤s≤S ≤H,S ≥0}: the class of all (s,S) policies. These policies order
(s,S)
to bring the inventory position up to S whenever the inventory position is below the reorder
point s. See Section 4.3 for a formal specification.
• Π ≜{(St)T+L:(St)T+L∈[0,H]T+L}: the class of all base-stock policies with non-stationary
(St) t=1 t=1
levels(St)T+L.ThesepoliciesordertobringtheinventorypositionuptoSt whentheinventory
t=1
position falls below St in period t. See Section 4.4 for a formal specification.
All values of S and St fall within the interval [0,H] for a fixed constant H >0, and values of s
are lower bounded by H. We assume x1=−∞ to ensure that a replenishment is always ordered in
period t=1. In addition, we assume that the parameter domains are continuous, allowing s,S,St
to assume fractional values.
These three classes of policies have been extensively studied in the inventory literature and are
widely implemented in practice. We next review several classical results for finite-horizon back-
logged inventory models in the stochastic inventory control literature (e.g., Snyder and Shen2019),
demonstrating the conditions under which these policies achieve optimality (i.e., zero approxima-
tion error).Xie, Ma and Xin: VCTheoryforInventoryPolicies
12
• In settings where the fixed cost K =0 and demands are i.i.d. across time, an optimal base-
stock policy exists with a stationary base-stock level for each period, falling within the class
Π with H =(L+1)U.
S
• In settings where K >0 and demands are i.i.d. across time, an (s,S) policy with both sta-
tionary order-up-to level S and reorder point s is asymptotically optimal (as T →∞), falling
withintheclassΠ whereH andH areselectedappropriatelybasedonthecostparameters
(s,S)
(to be further discussed in Section 4.3).
• In settings where the fixed cost K =0 and demands are independently but not necessarily
identically distributed across time, an optimal base-stock policy exists. However, this policy
features a time-dependent base-stock level for period t, which falls within the class Π with
(St)
H =(L+1)U.
3.3. Rademacher Complexity, VC-Dimension, Pseudo-Dimension, and
Pseudo -Dimension
γ
In this section, we review several well-known results from learning theory by using the notion
of Rademacher complexity, VC-dimension, Pseudo-dimension, and Pseudo -dimension. We will
γ
change our notation to generic functions f(π,·), which are usually identical to the loss functions
ℓ(π,·). This change provides flexibility, allowing us to use f(π,·) to also represent the final in-
ventory levels induced by policies π∈Π. Let [m]≜{1,...,m}. We first introduce a fundamental
concept,knownasRademachercomplexity,toboundtheexpectedgeneralizationerror,andpresent
a classical inequality in the literature (e.g., Lemma 26.2 of Shalev-Shwartz and Ben-David 2014).
Definition 1 (Rademacher complexity). The empirical Rademacher complexity of a func-
tion class F(Π)={f(π,·):π∈Π} w.r.t. a dataset (d )N is defined as
i i=1
(cid:34) (cid:35)
N
R(cid:0) F(Π)◦(d )N (cid:1)≜ E sup 1 (cid:88) σ ·f(π,d ) ,
i i=1 σ1,...,σN N i i
π∈Π
i=1
whereσ ,...,σ arerandomvariablesindependentlydrawnfromtheRademacherdistribution,i.e.,
1 n
P(σ =+1)=P(σ =−1)=1/2.
i i
Proposition 1. For any distribution D, sample size N, and function class F(Π) =
{f(π,·):π∈Π}, the following inequality holds:
E [GE(F(Π))]≤ 2E (cid:2) R(cid:0) F(Π)◦(d )N (cid:1)(cid:3) .
d1,...,dN∼D d1,...,dN∼D i i=1
The empirical Rademacher complexity measures the richness and complexity of the function class
F(Π) given the dataset (d )N . However, computing the empirical Rademacher complexity is NP-
i i=1
hard (see, e.g., the second paragraph on page 29 of Mohri et al. 2018). To address this challenge,Xie, Ma and Xin: VCTheoryforInventoryPolicies
13
combinatorial concepts, namely the VC-dimension (Vapnik and Chervonenkis 1971) and Pseudo-
dimension(Pollard1984),areintroduced.ComparedtoRademachercomplexity,VC-dimensionand
Pseudo-dimension are often relatively easier to bound or estimate, providing a simpler alternative
for bounding the expected generalization error. Therefore, sample complexity upper bounds can be
directly inferred using the VC-dimension or Pseudo-dimension, allowing researchers to concentrate
ontheproblem-specifictaskofboundingtheVC-dimensionorPseudo-dimensionforspecificclasses
of algorithms of interest.
Definition 2 (Pseudo-dimension). A dataset (d )m is shattered by a function class F(Π)=
i i=1
{f(π,·):π∈Π} with witnesses τ ,...,τ if there exist τ ,...,τ ∈R such that for any A⊆[m],
1 m 1 m
there exists π∈Π such that f(π,d )>τ ∀i∈A, and f(π,d )≤τ ∀i∈/A. The Pseudo-dimension of
i i i i
F(Π) denoted by P-dim(F(Π)) is the maximum size of a dataset that can be shattered by F(Π).
In Definition 2, if f(π,·)=ℓ(π,·) is the loss function, then A can be interpreted as the subset
of samples i on which π performs “awfully,” i.e., it incurs a loss exceeding the desired threshold
τ . Note that an equivalent definition of shattering is that there exist τ ,...,τ ∈ R such that
i 1 m
|{(I{f(π,d )>τ },...,I{f(π,d )>τ }):π∈Π}|=2m, where |·| denotes the cardinality of a set.
1 1 m m
Here, shattering means being able to realize every possible combination of binary labels on the
set of points I{f(π,d )>τ }. Pseudo-dimension quantifies the richness of the function class to fit
i i
all possible datasets, and it generalizes the idea of VC-dimension that is designed only for binary
classification. Specifically, when the function class is binary-valued, we can trivially set τ =0 for
i
all i∈[m], which results in the Pseudo-dimension reducing to the VC-dimension. A higher Pseudo-
dimensionindicatesamorecomplexclass,whichmayleadtooverfitting.Wereferinterestedreaders
to Section EC.1 in the appendix, which provides some examples to illustrate the calculation of
VC-dimension and Pseudo-dimension.
We next present a classical result in learning theory that uses Pseudo-dimension to derive a
distribution-free bound on the Rademacher complexity (e.g., Section 5.2 of Bousquet et al. 2003).
Proposition 2. For any distribution D, sample size N, and function class F(Π) =
{f(π,·):π∈Π} with f(π,·) bounded in [0,B] for all π∈Π, there exists a positive constant C >0
0
such that
(cid:114)
E (cid:2) R(cid:0) F(Π)◦(d )N (cid:1)(cid:3) ≤C B P-dim(F(Π)) .
d1,...,dN∼D i i=1 0 N
Combining Propositions 1 and 2, an upper bound on the Pseudo-dimension implies an upper
bound on the expected generalization error. In this paper, all the three policy classes that we are
interested are parameterized by real numbers. In such a case, one common technique to bound
P-dim(L(Π)) from above (e.g., Balcan 2020 and Balcan et al. 2021) is to consider its dual class
L∗(Π)≜{ℓ(·,d):d∈[0,U]T+L}, where each function corresponds to a demand sequence d and isXie, Ma and Xin: VCTheoryforInventoryPolicies
14
definedoverthedomainofΠ’sparameters.Werefertoℓ(π,·)andℓ(·,d)astheprimalanddualloss
functions,respectively.Analyzingthedualclasscanofferadvantagesoveranalyzingtheprimalclass
for several reasons. First, the sample space can be more complex than the parameter space, which
typically lies within a Euclidean space of low dimensionality. Second, bounding the VC-dimension
or Pseudo-dimension from above is challenging; it usually involves brute-forcing all possibilities
when analyzing primal loss functions. By contrast, analyzing the dual loss functions provides a
framework that naturally leads to an upper bound. We refer interested readers to Section EC.1
in the appendix for more detailed discussions. We will derive an upper bound on P-dim(L(Π)) by
leveraging the structured nature of the dual class L∗(Π).
The concept of shattering can be extended to γ-shattering, introducing the term Pseudo -
γ
dimension (Kearns and Schapire 1994), which is also referred to as the Fat-shattering dimension in
the literature. This measure, sensitive to the parameter γ≥0, provides a deeper understanding of
thetightnessofboundsontheexpectedgeneralizationerror.InouranalysisofthepolicyclassΠ ,
(St)
we will demonstrate how the Pseudo -dimension enables us to achieve a substantially improved
γ
upper bound, surpassing that achieved by the more traditional Pseudo-dimension approach. It is
worth noting that analyzing the Pseudo -dimension is rare in the literature, because the Pseudo-
γ
dimension typically serves the purpose of bounding the generalization error (e.g., Morgenstern and
Roughgarden 2015, Balcan et al. 2021, 2023).
Definition 3 (Pseudo -dimension). Given γ≥0, a dataset (d )m is γ-shattered by F(Π)=
γ i i=1
{f(π,·):π∈Π}withwitnessesτ ,...,τ ifthereexistτ ,...,τ ∈RsuchthatforanyA⊆[m],there
1 m 1 m
exists π∈Π such that f(π,d )>τ +γ ∀i∈A, and f(π,d )≤τ −γ ∀i∈/A. The Pseudo -dimension
i i i i γ
of F(Π) denoted by P -dim(F(Π)) is the maximum size of a dataset that can be γ-shattered by
γ
F(Π).
When γ = 0, Pseudo -dimension reduces to Pseudo-dimension. We now present a useful
γ
distribution-free bound on the Rademacher complexity, using the Pseudo -dimension (e.g., Theo-
γ
rems 10 and 17 of Mendelson 2003, where the integral in our setting can be bounded between 0
and1,becausethe coveringnumber, witharangeof1 undertheL normforF(Π)thatis bounded
2
within [−1,1], is 1).
Proposition 3. For any distribution D, sample size N, and function class F(Π) =
{f(π,·):π∈Π} with f(π,·) bounded in [−1,1] for all π∈Π, there exist positive constants C ,C >0
1 2
such that
(cid:115)
E (cid:2) R(cid:0) F(Π)◦(d )N (cid:1)(cid:3) ≤ √C 1 (cid:90) 1 log(cid:18) 2(cid:19) ·P -dim(F(Π)) dγ.
d1,...,dN∼D i i=1 N γ C2γ
0Xie, Ma and Xin: VCTheoryforInventoryPolicies
15
4. Theoretical Results
In this section, we utilize the specific structures of loss functions that are induced by the policy
classes Π , Π , and Π to analyze their Pseudo-dimension/Pseudo -dimension. By combining
S (s,S) (St) γ
Propositions 1 to 3, we derive distribution-free bounds on the expected generalization errors of
these policy classes w.r.t. the horizon length T and sample size N. This analysis also allows us to
obtain the sample complexity for these policy classes.
The rest of the section is organized as follows. In Section 4.1, we summarize our technical
contributions and provide an overview of the underlying intuitions and proof sketches for our
results. In Section 4.2, we analyze the class of stationary base-stock policies Π , and demonstrate
S
(cid:112)
an upper bound of O( 1/N) for the expected generalization error of L(Π ) in Corollary 1. In
S
(cid:112)
Section4.3,weanalyzetheclassof(s,S)policiesΠ ,presentinganupperboundofO( logT/N)
(s,S)
(cid:112)
and a lower bound of Ω( logT/(NloglogT)) for the expected generalization error of L(Π )
(s,S)
in Corollaries 2 and 3, respectively. Finally, in Section 4.4, we analyze the class of non-stationary
(cid:112)
base-stock policies Π , and show an upper bound of O( 1/N) for the expected generalization
(St)
error of L(Π ) in Corollary 4.
(St)
4.1. Summary of Our Results and Techniques
In this section, we provide a summary of the ideas behind our technical results in the special case
with L=0 and U =1. For a horizon length T and ϵ>0, we state our results in terms of how many
samples N are required to guarantee that E [GE(L(Π))]≤ϵ, under any instance defined
d1,...,dN∼D
by cost parameters b,h,K ∈[0,1] and distribution D over [0,1]T. Such results can be converted
to say that with probability at least 1−δ, the generalization error R(π)−Rˆ(π) of any policy
π∈Π is at most ϵ, after adding O(log(1/δ)/ϵ2) to the number of samples N (see, e.g., Lemma 26.4
and Theorem 26.5 of Shalev-Shwartz and Ben-David 2014). These results also directly bound the
estimation error R(πˆ)−R(π∗) of an empirical risk minimizer πˆ∈argmin Rˆ(π) based on (1). We
π∈Π
consider the three settings Π=Π =[0,1], Π=Π , and Π=Π =[0,1]T, recalling that the
S (s,S) (St)
policy parameter range depends on the cost parameters b,h if Π=Π (to be specified in Section
(s,S)
4.3), and that K=0 is assumed if Π=Π .
(St)
Base-Stock (or S) Policies: N =O(1/ϵ2). We warm up by showing that the single-parameter
policy class Π indeed has sample complexity that is independent of T, using an argument about
S
the dual class on the parameter space (Balcan et al. 2021), based on the convexity of the loss
function w.r.t. the parameter S. It is also easy to see that N =Ω(1/ϵ2) samples are necessary.
(s,S) Policies: N = O((logT)/ϵ2). We show that the Pseudo-dimension of (s,S) policies is
O(logT), which is sufficient. We first observe that, for a given b,h,K and single demand sequence
d: (i) the minimum order quantity ∆≜S−s fully determines the sequence of time periods (t )
j j∈[J]Xie, Ma and Xin: VCTheoryforInventoryPolicies
16
at which inventory is replenished (i.e., having yt >xt), where J represents the total number of
reordering periods during the horizon; (ii) given a fixed sequence (t ) , the loss function can
j j∈[J]
be re-written to demonstrate that it is convex in S (assuming s changes to keep ∆ the same).
Therefore, for a fixed (t ) , the range of S that leads to a low loss on d is a (possibly empty)
j j∈[J]
interval, and it remains to count the number of possibilities for the sequence (t ) . A priori
j j∈[J]
there would be 2T possible subsequences of 1,...,T, but we show that (t ) can only change
j j∈[J]
when ∆ crosses over the sum of demands in an interval (i.e., crosses over dt+···+dt′ for some
1≤t≤t′≤T). Therefore, as ∆ varies, (t ) can change at most O(T2) times, which means that
j j∈[J]
the policy parameters achieving low loss on d can be defined by O(T2) axis-aligned rectangles in
the (∆,S) plane. Ultimately we apply a counting argument about the dual class on the parameter
space following Balcan et al. (2021) to argue that at most O(logT) samples can be shattered.
ItmaybetemptingtotrytoprovethisresultbydiscretizingtheparameterspaceintoanM×M
grid, which has O((logM)/ϵ2) sample complexity. However, one can show that the approximation
error from discretization remains a constant even for an arbitrary large M, due to the fine-tuning
of ∆ required based on the demand distribution; more formally,
(cid:32) (cid:33)
sup inf R(π) − inf R(π) = Ω(1) ∀M >0. (3)
D π=(s,S)∈{...,− 2 ,− 1 ,0, 1 , 2 ,...}2 π=(s,S)∈Π(s,S)
M M M M
Therefore, it is impossible to establish such a sample complexity via discretization. We defer the
detailed argument of (3) to Section EC.2.1 in the appendix.
(s,S) Policies: N =Ω((logT/loglogT)/ϵ2). We show for a constant γ >0 that the Pseudo -
γ
dimension of (s,S) policies is Ω(logT/loglogT), which again is sufficient due to standard reduc-
tions. Our construction involves demand sequences that start with P −1 1’s followed by one 1/2
and then all 0’s, where P is a product of prime numbers. The performance of an (s,S) policy
depends on whether ∆, which equals S−s and determines the order frequency, divides P. The
number of prime numbers that can be multiplied together without exceeding T determines the
number of samples that can be γ-shattered. We use the celebrated Prime Number Theorem to
demonstrate that this number is Ω(logT/loglogT).
(St) Policies: N =O(1/ϵ2). Again, this is tight because Ω(1/ϵ2) samples are necessary. We first
show that the Pseudo-dimension of (St) policies is Ω(T) (we defer the argument to Section EC.2.2
in the appendix), which would suggest N =Ω(T/ϵ2) and fail to provide the correct answer, so we
instead analyze the stronger notion of Pseudo -dimension. We decompose the loss by time period,
γ
and use Talagrand’s contraction lemma (which is only possible because of the assumption that
K =0) to show that it suffices to bound the Pseudo -dimension of the final inventory states yT
γ
induced by the policy class Π , and show that it is O(1/γ) for all γ>0.
(St)Xie, Ma and Xin: VCTheoryforInventoryPolicies
17
We elaborate on this reduced problem. There are demand sequences d ∈[0,1]T for i=1,...,m.
i
An inventory policy is defined by π=(St)T ∈[0,1]T, and we let yT(π,d ) denote the inventory
t=1 i
level yT when policy π is ran on demand sequence d . The presumption is that for any subset
i
A⊆[m], there exists a policy π such that inventory level yT(π,d ) is “high” for all i∈A, and “low”
i
for all i∈/ A, where the thresholds for “high” and “low” are separated by at least 2γ. We want
to show that m cannot be too large. Indeed, under this presumption, for each i there exists a π
i
that ends with high inventory on d , and low inventory on all other trajectories. We define t as
i i
the last replenishment point, i.e., the last t for which St>xt, when π is ran on d . We essentially
i i
argue that d must exhibit a low total demand from t to the end, compared to any other sequence
i i
d , with the difference being at least 2γ. This gives us a way of directly comparing the demand
j
sequences d ,...,d without considering policies, by identifying for each d a time point t from
1 m i i
which its demand onward is low compared to other sequences. We link these relationships together
to show that m must be O(1/γ), completing the proof.
Ourresult holdsfor arbitrarydistributionsD over [0,1]T, but itis significant evenif oneassumes
independence over time, i.e., D is a product distribution D ×···×D . Indeed, the general theory
1 T
of Guo et al. (2021) implies a sample complexity that grows linearly in the number of marginal
distributions T (although technically their result requires discretization or a strong monotonicity
condition, we believe with some work their result can be applied here). The independent setting
has also been studied in classical data-driven inventory control papers. For example, Levi et al.
(2007) and Cheung and Simchi-Levi (2019) show that the number of samples required to make
the estimation error R(πˆ)−R(π∗) at most ϵR(π∗) is O(T5/ϵ2) and O(T6/ϵ2), respectively. Our
result is not directly comparable because they are providing a multiplicative guarantee, along with
other differences in the assumptions. For example, the nature of multiplicative guarantees is that
one can allow for unbounded demands; however, one now needs to fix cost parameters b,h before
considering asymptotics in N and T. Nonetheless, the fact that existing bounds even assuming
independence grow with T suggests that our T-independent sample complexity is quite surprising.
Beyond inventory, we believe it is interesting to encounter a setting where the stronger notion
of Pseudo -dimension is necessary to establish the correct sample complexity. By contrast, in
γ
the data-driven algorithm design literature, Pseudo-dimension has typically been sufficient (e.g.,
Morgenstern and Roughgarden 2015, Balcan et al. 2021, 2023). Our problem unveils a simple
reason why Pseudo-dimension may not suffice when the loss is averaged over T periods, as will be
explainedatthebeginningofSection4.4.There,wediscussthevariantinvolvingaperishablegood,
whose inventory disappears at the end of each period—in which case, one period has no impact on
the others, and considering the average loss over T periods should be equivalent to considering a
single period. Yet, the naive Pseudo-dimension is still shown to be Ω(T).Xie, Ma and Xin: VCTheoryforInventoryPolicies
18
4.2. The Class of Stationary Base-Stock Policies
In this section, we analyze the class of stationary base-stock policies Π ={S:S∈[0,H]}, where
S
H is set to be (L+1)U. We note that H is the maximum desirable base-stock level needed to cover
the demand over L+1 periods. Each policy in this class is parameterized by a single base-stock
level S. Under such a policy, if the inventory position It falls below S in period t, then an order is
placed to bring the inventory position up to S. That is,
t−1 (cid:26)
qt=S−It=S−xt− (cid:88) qt′ = S, if t=1;
dt−1, if t≥2.
t′=t−L
t−1
It implies that yt=xt+qt−L=S− (cid:80) dt′ for all t≥L+1. The loss function ℓ(S,d) by the S
t′=t−L
policy on demand sequence d is
(cid:32) (cid:32) (cid:33) (cid:33)
T+L t
ℓ(S,d)= 1 (cid:88) c S− (cid:88) dt′ +K·I{qt−L>0} . (4)
T
t=L+1 t′=t−L
Notethatℓ(S,d)isconvexinS foranygivend,becausec(·)isaconvexfunctionandthefixed-cost
component is independent of S. The following result provides an upper bound on the Pseudo-
dimension of L(Π ).
S
Theorem 1 (Proved in Section EC.3.1). P-dim(L(Π ))≤2.
S
We can now combine Proposition 1, Proposition 2 (with B=(L+1)U +1), and Theorem 1, in
that order, to arrive at the following.
Corollary 1. For any distribution D, sample size N, and horizon length T, the expected gen-
eralization error of Π has the following upper bound:
S
(cid:32) (cid:114) (cid:33)
1
E [GE(L(Π ))]=O (L+1)U .
d1,...,dN∼D S N
It is well-known that there exist problem instances such that the expected estimation error is
(cid:112)
lower bounded by Ω( 1/N) (e.g., Proposition 1 of Zhang et al. 2020), which implies the tightness
of the upper bound in Corollary 1.
Remark 1. According to the proof of Theorem 1, the conclusion that P-dim(L(Π ))≤2 es-
S
sentially arises from the convexity of the loss function w.r.t. the parameter. Consequently, this
argument can also be applied to upper-bound the Pseudo-dimension of other single-parameter pol-
icy classes in related inventory models. For instance, it is applicable to both the class of base-stock
policies and the class of constant-order policies for the lost-sales model with lead times. In both
cases, the loss function is known to be convex w.r.t. the policy parameter (e.g., Janakiraman and
Roundy 2004, Xin and Goldberg 2016).Xie, Ma and Xin: VCTheoryforInventoryPolicies
19
Moreover, a base-stock policy with a single S is a special case of an (s,S) policy when s is set
equal to S. A similar analysis can be applied to other (s,S) policies with a pre-specified S −s
difference. For example, we can determine S−s by using the following economic order quantity
(cid:113)
(EOQ) formula that allows backorders: S−s= 2Kµ(h+b) (e.g., Theorem 3.5 of Snyder and Shen
hb
2019). Here, µ represents the mean demand. This enhanced version of the single-parameter policy
may significantly outperform the base-stock policy numerically, as we see in Section 5.
4.3. The Class of (s,S) Policies
In this section, we analyze the class of (s,S) policies Π ={(s,S):H ≤s≤S≤H,S≥0}. Under
(s,S)
such a policy, an order is placed to bring the inventory position up to the order-up-to level S when
the inventory position falls below the reorder point s. That is,
qt=(S−It)·I{It≤s}.
In contrast with base-stock policies, bounding the policy parameters for (s,S) policies is less
clear. For example, if h is small, then due to the fixed cost K, it may be beneficial to set S to be
muchlargerthan(L+1)U.Weboundtheparameterssothatthemaximumholdingorbacklogging
cost paid during any period is at most (L+1)U. The maximum inventory level is still S. Thus, we
restrict S ≤H where H ≜(L+1)U/h to ensure that holding costs cannot exceed (L+1)U. The
minimum inventory level occurs when the current inventory position It is just above s (a negative
number), and then (L+1)U demand occurs, so we pay a backlogging cost of (−s+(L+1)U)b.
Thus, we restrict s≥H where H ≜(L+1)U(1−1/b)≤0 to ensure that backlogging costs cannot
exceed(L+1)U.Therestrictionsofs≤S andS≥0areeasilyseentobewithoutlossofoptimality,
resulting in the policy space Π ={(s,S):H ≤s≤S≤H,S≥0} with H,H defined as above.
(s,S)
The loss of such a policy on any demand sequence cannot exceed (L+1)U +K≤(L+1)U +1.
To simplify the analysis, we focus on an equivalent definition of this policy class Π =
(s,S)
{(∆,S):0≤∆≤H−H,0≤S≤H} by defining ∆≜S−s. To track the inventory dynamics, we
define a sequence of reordering periods (t ) with 1≤t ≤t ≤T, namely, t is in the sequence
j j∈[J] j j+1 j
if and only if qtj >0 and t ≤T. Here, J represents the total number of reordering periods during
j
the horizon, and it depends only on ∆, not on S. We also ignore reordering periods after period T
because these orders will not arrive within the horizon and thus do not affect the loss. Note that
t =1 because x1=−∞ by assumption, and
1
 
t
=min
t>t :
(cid:88)t−1
dt′
≥∆,
t≤T
.
j+1 j
 
t′=tjXie, Ma and Xin: VCTheoryforInventoryPolicies
20
Due to the lead time L, the loss function is
  
ℓ(π,d)=
1
(cid:88)J K+tj+ (cid:88)1+L−1
cS−
(cid:88)tˆ
dt′
,
T
j=1 tˆ=tj+L t′=tj
and we assume t ≜T +1. Based on the structure of the reordering sequence, we are able to
J+1
provide an upper bound on the Pseudo-dimension of L(Π ).
(s,S)
Theorem 2 (Proved in Section EC.3.2). P-dim(cid:0) L(Π )(cid:1) =O(logT).
(s,S)
As discussed earlier in Section 4.1, to our understanding, Theorem 2 cannot be proven by dis-
cretizing the parameter space or directly applying the result of Balcan et al. (2021). We can now
combine Proposition 1, Proposition 2 (with B=(L+1)U +1), and Theorem 2, in that order, to
arrive at the following.
Corollary 2. For any distribution D, sample size N, and horizon length T, the expected gen-
eralization error of Π has the following upper bound:
(s,S)
(cid:32) (cid:114) (cid:33)
E (cid:2) GE(cid:0) L(Π )(cid:1)(cid:3) =O (L+1)U logT .
d1,...,dN∼D (s,S) N
It remains to bound E (cid:2) GE(cid:0) L(Π )(cid:1)(cid:3) from below. Note that for binary function
d1,...,dN∼D (s,S)
classes, the VC-dimension provides both lower and upper bounds on the expected generalization
error (e.g., Theorem 6.8 of Shalev-Shwartz and Ben-David 2014). In other words, the learnability
of a binary-valued function class is equivalent to the finiteness of its VC-dimension. By contrast, a
lower boundonthe Pseudo-dimensionof afunction classesdoes notnecessarily translatetoa lower
bound on the corresponding generalization error. In fact, the learnability of real-valued function
classes is established through the finiteness of Pseudo -dimension, rather than Pseudo-dimension,
γ
and the Pseudo -dimension can provide both lower and upper bounds on the expected generaliza-
γ
tionerrorincertaincontexts(e.g.,KearnsandSchapire1994,Alonetal.1997).Therefore,wehave
to analyze a lower bound on the more complex Pseudo -dimension of L(Π ). Our next result
γ (s,S)
presents such a lower bound on the Pseudo -dimension for some constant γ>0 by constructing an
γ
instance. Our proof involves the celebrated Prime Number Theorem and the asymptotic property
of the first Chebyshev function.
Theorem 3 (Proved in Section EC.3.3). Suppose L≥0, U =1, b∈(0,1/2], and h=K=0.
(cid:0) (cid:1)
For any γ∈[0,b/16], we have P -dim L(Π ) =Ω(logT/loglogT).
γ (s,S)
We note that shattering in Theorem 3 is achieved using (s,S) policies that satisfy the bound-
aries H,H defined above. Note that Pseudo -dimension is non-increasing in γ≥0, implying that
γ
(cid:0) (cid:1) (cid:0) (cid:1) (cid:0) (cid:1)
P -dim L(Π ) ≤P -dim L(Π ) =P-dim L(Π ) . Theorem 3 demonstrates the tight-
γ (s,S) 0 (s,S) (s,S)
(cid:0) (cid:1)
ness of the order O(logT) for P-dim L(Π ) . Finally, we show that Theorem 3 can lead to a
(s,S)
lower bound on the expected generalization error of Π .
(s,S)Xie, Ma and Xin: VCTheoryforInventoryPolicies
21
Corollary 3 (Proved in Section EC.3.4). Suppose L≥0, U =1, b∈(0,1/2], and h=K=
(cid:0) (cid:1)
0. For any γ∈(0,b/16] satisfying P -dim L(Π ) ≥2, there exists a distribution D such that the
γ (s,S)
(cid:0) (cid:1)
expected generalization error of Π has the following lower bound for all N ≥P -dim L(Π ) :
(s,S) γ (s,S)
(cid:115) (cid:0) (cid:1) (cid:32)(cid:115) (cid:33)
E d1,...,dN∼D(cid:2) GE(cid:0) L(Π (s,S))(cid:1)(cid:3) =Ω P γ-dim NL(Π (s,S)) =Ω Nll oo gg lT
ogT
.
TheproofofCorollary3usestheγ-shatteredsamplesfromTheorem3toestablishalowerbound
on the uniform generalization error. This shows that any error bound for (s,S) policies which is
established through uniform convergence must grow with T, unlike the case in S or (St) policies.
It is unclear to us whether an analysis of (s,S) policies that eschews uniform convergence could
avoid this dependence on T. We leave this question for future work.
4.4. The Class of Non-Stationary Base-Stock Policies
In this section, we analyze the class of base-stock policies with non-stationary base-stock levels
(cid:8) (cid:9)
Π = (St)T+L:(St)T+L∈[0,H]T+L for the setting where the fixed cost is zero (K =0). As
(St) t=1 t=1
explained at the start of Section 4.2, we set H =(L+1)U. Under such a policy, the inventory
0
position is replenished to St in period t: qt=max{St−It,0}. Recall that I1=x1+ (cid:80) qt′ =−∞
t′=1−L
and y1=...=yL=0. It is easy to verify that
(cid:40) (cid:41)
t−1
It= max Stˆ−(cid:88) dt′ , ∀t≥2,
tˆ=1,...,t−1
t′=tˆ
(cid:40) (cid:41)
t−1 t−L−1 t−1
yt= max(cid:8) It−L,St−L(cid:9) − (cid:88) dt′ = max Stˆ− (cid:88) dt′ − (cid:88) dt′ , ∀t≥L+1.
tˆ=1,...,t−L
t′=t−L t′=tˆ t′=t−L
Thus, the loss in period t≥L+1 is
(cid:32) (cid:40) (cid:41) (cid:33)
t−L−1 t
ℓt(cid:0) (St)T+L,d(cid:1) = c(yt−dt)= c max Stˆ− (cid:88) dt′ − (cid:88) dt′ .
t=1
tˆ=1,...,t−L
t′=tˆ t′=t−L
(cid:8) (cid:0) (cid:1) (cid:9)
Recall L(Π )= ℓ (St)T+L,· :(St)T+L∈Π is the class of loss functions induced by Π .
(St) t=1 t=1 (St) (St)
As discussed in Section 4.1, the Pseudo-dimension of (St) policies grows linearly in T, leading
(cid:112)
to at best an upper bound of O( T/N) for the expected generalization error. To provide some
intuition on why this upper bound is too loose, we briefly consider a related inventory setting
wheretheproductisperishable,assumingL=0.Intheperishablesetting,leftoverinventorycannot
be carried over to the next period, i.e., xt+1 =min{yt−dt,0} (instead of xt+1 =yt−dt). Under
an (St) policy, inventory at time t+1 would always be replenished to exactly St+1, i.e., yt+1 =
max{St+1,xt+1}=St+1 (because St+1≥0). This means that the loss function can be decomposed
as ℓ((St)T ,d)= 1 (cid:80)T c(St−dt) in the perishable setting, and we derive
t=1 T t=1
E (cid:2) GE(cid:0) L(Π )(cid:1)(cid:3)
d1,...,dN∼D (St)Xie, Ma and Xin: VCTheoryforInventoryPolicies
22
(cid:34) (cid:40) (cid:34) (cid:35) (cid:41)(cid:35)
T N T
1 (cid:88) 1 (cid:88) 1 (cid:88)
= E sup E c(St−dt) − c(St−dt)
d1,...,dN∼D d∼D T N T i
(St)∈[0,1]T
t=1 i=1 t=1
(cid:34) (cid:40) (cid:41)(cid:35)
T N
1 (cid:88) 1 (cid:88)
= E sup E [c(St−dt)]− c(St−dt)
T d1,...,dN∼D d∼D N i
t=1
St∈[0,1]
i=1
T
1 (cid:88) (cid:112) (cid:112)
= O( 1/N)=O( 1/N),
T
t=1
wherethethirdequalitycomesfromCorollary1.Thissimpleargumentestablishesanupperbound
(cid:112)
of O( 1/N) for the perishable setting, showing the Pseudo-dimension bound to be too loose. (It
can be checked from the example in Section EC.2.2 that the Pseudo-dimension is Ω(T) even if the
good is perishable.)
Of course, the simple argument above only works for the perishable setting. To establish an
(cid:112)
upper bound of O( 1/N) for the expected generalization error of (St) policies in the origi-
nal, non-perishable setting, we now analyze the stronger notion of Pseudo -dimension. We use
γ
(cid:16) (cid:17)
yt (St′)T+L,d to denote the inventory level after replenishment but before seeing demand in
t′=1
(cid:16) (cid:17)
period t under policy (St′)T+L on a given demand sequence d. Note that yt (St′)T+L,d =
t′=1 t′=1
(cid:16) (cid:17)
yt (St′)t−L,d ∈[−LU,H]⊆[−(L+1)U,(L+1)U] for any (St′)T+L∈Π and d∈[0,U]T+L. We
t′=1 t′=1 (St)
furtherdefineaclassoffunctionsrepresentingthenormalizedinventorylevelfort=L+1,...,T+L:
(cid:110) (cid:16) (cid:17) (cid:111)
Yt(Π )≜ yt (St′ )t−L,d /((L+1)U):(St′ )t−L ∈[0,H]t−L .
(St) t′=1 t′=1
In other words, the function class Yt(Π ) takes values in the interval [−1,1].
St
We next provide an upper bound on the Pseudo -dimension of Yt(Π ).
γ (St)
Theorem 4 (Proved in Section EC.3.5). For any γ ∈ (0,1] and t = L + 1,...,T + L,
(cid:0) (cid:1)
P -dim Yt(Π ) ≤2/γ+1.
γ (St)
We can now combine Proposition 1, Proposition 3, and Theorem 4, in that order, to arrive at
the following. Note that the proof of Corollary 4 requires applying Talagrand’s contraction lemma
to convert from analyzing lossesto analyzinginventorylevels,and alsoevaluatingthe integralfrom
Proposition 3.
Corollary 4 (Proved in Section EC.3.6). For any distribution D, sample size N, and
horizon length T, the expected generalization error of Π has the following upper bound:
(St)
(cid:32) (cid:114) (cid:33)
E (cid:2) GE(cid:0) L(Π )(cid:1)(cid:3) =O (L+1)U 1 .
d1,...,dN∼D (St) NXie, Ma and Xin: VCTheoryforInventoryPolicies
23
5. Numerical Experiments
Inthissection,weconductnumericalexperimentstoevaluatetheout-of-sampleperformancesofthe
three policy classes. We use the ERM procedure to determine the output πˆ∈arginf Rˆ(π) given
π∈Π
the samples. Specifically, the out-of-sample (OOS) loss of policy class Π by the ERM procedure is
defined as OOS(Π)≜R(πˆ). Recall that the gap between the OOS loss and the expected loss by the
globally optimal policy πopt is exactly the sum of the estimation error and approximation error:
OOS(Π)−R(πopt)=R(πˆ)− inf R(π)+ inf R(π)−R(πopt)=EE(L(Π))+AE(L(Π)).
π∈Π π∈Π
Our goal is to determine the best policy class among Π , Π , and Π based on their OOS
S (s,S) (St)
performances.ThisprovidesinsightonthetradeoffbetweenEE(L(Π))andAE(L(Π))overdifferent
sample sizes, complementing our theoretical results that upper-bound EE(L(Π)).
In the rest of the section, we numerically evaluate the expected OOS loss w.r.t. the sample size
N.Specifically,inSection5.1,wefocusonΠ andΠ whenK>0.InSection5.2,ourevaluation
(s,S) S
concerns Π and Π when K =0. We also demonstrate the benefits of integrating inventory
(St) S
theory into black-box learning machines in Section 5.3 by comparing two approaches: one involves
learning a base-stock policy by leveraging inventory theory, and the other focuses on learning order
quantities without relying on inventory theory. We select N ∈{2,5,10,20,50,100}, and set L=0,
h=1 and b=9, which corresponds to the service level b =90%. Note that we do not normalize
h+b
b and K to lie within the range [0,1] in our numerical experiments. Because EE(L(Π)) decreases
as N increases, R(π∗) can be estimated by using OOS(Π) with a large N, and we set N =300 in
simulations. To approximate OOS(Π)=R(πˆ)=E [ℓ(πˆ,d)], we calculate the average of ℓ(πˆ,d )
d∼D j
for 3,000 randomly generated samples (d ) . To evaluate the expected OOS loss w.r.t. the training
j j
dataset d ,...,d , we repeat the experiments 100 times and record the average value of OOS(Π).
1 N
In our experiments, we do not restrict the ranges of policy parameters using constants such as H
and H; however, we do restrict all demand realizations and order quantities to integer values.
5.1. Π vs. Π when K>0
(s,S) S
In this section, we compare the expected OOS losses of Π and Π when K >0. In our exper-
(s,S) S
iments, we set T =30 and investigate demand sequences that are i.i.d. and drawn from a normal
distribution N(µ,σ2) with µ=9 and σ =4.5. These sequences are then truncated and rounded
into non-negative integers. We select different values of K based on the refined EOQ formula ∆=
(cid:113)
2Kµ(h+b) (as mentioned in Section 4.2). More specifically, we set K(P)= P2µhb =4.05P2 as the
hb 2(h+b)
value of K, so that replenishment occurs approximately every P periods, following from ∆/µ=P.
Recall that Π is asymptotically optimal (as T →∞) under i.i.d. demand distributions. Instead
(s,S)
of plotting the expected OOS losses, we plot the ratio of the expected OOS losses of Π and Π
(s,S) SXie, Ma and Xin: VCTheoryforInventoryPolicies
24
relative to R(π∗), because ratios are easier to interpret. To capture the empirical policies learned
from the training dataset, we calculate the average of the empirical policies obtained across 100
repeated experiments, each using N =300 samples.
We present the results for Π and Π with different values of K in Figure 2. The difference
(s,S) S
betweenthedashedlinesrepresentsa(normalized)approximationerrorforΠ ,since(s,S)policies
S
are near-optimal. We observe that the approximation error of Π increases as K increases. This is
S
because a larger K leads to less frequent replenishments under (s,S) policies, whereas S policies
keepreplenishingalmosteveryperiod,incurringhighfixedcosts.Itfurthersuggeststhatcomparing
OOS(Π )andOOS(Π )ismeaningless,whichmotivatesustoconsideravariantof(s,S)policies.
S (s,S)
This variant would have a pre-specified S−s difference, determined by the refined EOQ formula.
(a) K(1)=4.05×12=4.05 (b) K(2)=4.05×22=16.2
Figure2 The expected OOS loss ratios of Π and Π when K>0 and demands are i.i.d. and drawn from
(s,S) S
a normal distribution N(9,4.52). The averaged empirical policies learned from N=300 samples across
100 experiments are: (a) ((cid:92) s,S)=(11.43,15.02) and Sˆ=15.0; (b) ((cid:92) s,S)=(8.96,23.38) and Sˆ=15.0.
Particularly, let µ¯ denote the empirical mean of the training demand samples, and define ∆¯ ≜
(cid:113)
2Kµ¯(h+b). We consider this policy class defined as Π ≜(cid:8) (s,S):S∈[0,H],s=S−[∆¯](cid:9) , where
hb EOQ
[x] denote the nearest integer to x. Note that the demand is i.i.d. drawn from N(9,4.52), and µ¯
is the sample mean of N ×T i.i.d. demand values. In addition, Π is expected to have much
EOQ
smaller approximation errors than Π due to the fixed costs. Therefore, Π can be regarded as
S EOQ
an enhanced version of Π .
S
We present the results for Π and Π with different values of K in Figure 3. First, we are
(s,S) EOQ
interested in the estimation errors of Π and Π . Because the curves represent the expected
(s,S) EOQ
OOS losses for the policy classes, and the dotted horizontal lines correspond to the simulatedXie, Ma and Xin: VCTheoryforInventoryPolicies
25
(a) K(2)=4.05×22=16.2 (b) K(3)=4.05×32=36.45
(c) K(5)=4.05×52=101.25
Figure3 The expected OOS loss ratios of Π and Π when K >0 and demands are i.i.d. and drawn
(s,S) EOQ
from a normal distribution N(9,4.52). The averaged empirical policies learned from N =300 samples
across 100 experiments are reported as follow: (a) ((cid:92) s,S)=(8.99,26.02) and Sˆ=26.02, [∆¯]=18; (b)
((cid:92) s,S)=(7.36,29.98) and Sˆ=33.96, [∆¯]=27; (c) ((cid:92) s,S)=(4.91,44.72) and Sˆ=49.02, [∆¯]=45.
values of R(π∗), the gap between the OOS curve and the dotted line represents a (normalized)
estimation error. Similar to Figure 2, all parts in Figure 3 show that (s,S) policies have much
larger estimation errors than the single-parameter policies, confirming that learning (s,S) policies
is more challenging. Moreover, we are particularly interested in the intersection points of the OOS
curves, with a special focus on their relation to sample size. This is because the intersection point
represents the sample size at which one would want to switch from the simpler policy class Π
EOQ
(or Π ) to the richer policy class Π . As illustrated in these figures, even with the simplest i.i.d.
S (s,S)
demand distribution, the intersection point is noticeably non-zero due to the large estimation error
of Π . It demonstrates that opting for the simpler policy class Π is preferable to opting for
(s,S) EOQXie, Ma and Xin: VCTheoryforInventoryPolicies
26
Π , which aligns with the principle “learning less is more.” For larger values of K, both the
(s,S)
empiricalgap∆ˆ =Sˆ−sˆobservedinthe(s,S)policiesandthegap[∆¯]observedintheEOQpolicies
increase. However, the difference between these two gaps, [∆¯]−∆ˆ, remains relatively stable. As a
result, as K increases, the fixed cost component dominates the other two cost components, and
the total number of replenishments under Π becomes close to that under Π , leading to a
EOQ (s,S)
relatively smaller difference in approximation error. This is seen in Figure 3, which shows that
the gap between the two dotted lines narrows, and the intersection point shifts rightward as K
increases.
5.2. Π vs. Π when K=0
(St) S
In this section, we compare the expected OOS losses of Π and Π when K=0. We set T =10
S (St)
and investigate demand sequences d=(dt)T that are independently drawn from normal distribu-
t=1
tions with non-stationary means and variances across time, namely, dt∼N(µt,σt) ∀t∈[T]. These
sequences are then truncated and rounded into non-negative integers. Here, µt and σt represent
the mean and standard deviation of the demand at period t, respectively, and we set σt =µt/2
for all t∈[T]. We consider two types of demand sequences: almost stationary demand and highly
non-stationary demand. Specifically, for the almost stationary demand, µt is i.i.d. and drawn from
a uniform distribution U(7,11). For the highly non-stationary demand, µt is i.i.d. and drawn from
a uniform distribution U(1,17). In both settings, we generate 10 sets of mean vectors and record
the average value of OOS(Π). Because Π is optimal under independent and non-stationary de-
(St)
mand distributions, we plot the ratio of the expected OOS losses of Π and Π relative to R(π∗),
S (St)
estimated using N =300 samples.
(a) Almost Stationary (b) Highly Non-Stationary
Figure4 The expected OOS loss ratios of Π and Π when K=0.
(St) SXie, Ma and Xin: VCTheoryforInventoryPolicies
27
We present the results for the two types of demand distributions in Figure 4. First, it is shown
that EE(L(Π )) exceeds EE(L(Π )), confirming that learning multiple parameters can be more
(St) S
challengingthanlearningasingleparameter.However,asN increases,theestimationerrorofΠ
(St)
decreases very quickly, much quicker than that of Π as shown in Figure 3. This complements
(s,S)
our theoretical finding that (St) policies have a better generalization bound and lower estimation
error than (s,S) policies.
In the setting with almost stationary demand, the demand distribution closely resembles that of
the i.i.d. setting, implying that single base-stock policies can effectively approximate the optimal
(St)policy.Ontheotherhand,whendemandbecomeshighlynon-stationary,S policieshavealarge
approximation error that dominates the estimation error of (St) policies. In fact, the intersection
point vanishes (i.e., becomes zero), making it always better to learn the richer class of (St) policies.
Note that Figure 4(a) is designed to closely approximate stationary conditions for illustrative
purposes;however,typicalscenariosareexpectedtoresemblemorecloselythesituationdepictedin
Figure 4(b). This suggests that learning Π should generally be preferred over learning Π , even
(St) S
at small sample sizes, because Π is quite safe from overfitting, complementing our theoretical
(St)
findings.
5.3. Base-Stock vs. Order Quantity
Inventorytheoryhasevolvedovermorethan70years,beginningwiththepioneeringworkofArrow
etal.(1951).Acornerstoneofthisfieldisthedemonstrationoftheoptimalityofabase-stockpolicy
for backlogged models when K=0, a fundamental result in inventory theory. This knowledge has
naturally led to the practice of directly learning base-stock policies based on historical demand
sequences. Without this foundational understanding, one might instead learn a complex function
mapping each inventory state to a corresponding order quantity. The objective of this section is to
highlight the critical role that domain knowledge plays in the field of inventory theory. We support
our argument by comparing the outcomes of learning a base-stock policy with the advantage of
inventory theory versus learning order quantities in its absence. We refer to these two approaches
as “learning-base-stock” and “learning-order-quantity,” respectively.
Let us set K =0, T =30 and L=1. We consider demand sequences that are i.i.d. and drawn
from a discrete uniform distribution U{0,...,D¯}, where dt =0,1,...,D¯ with equal probability.
We test cases where D¯ =2 and D¯ =3. In these settings, learning order quantities is equivalent
to learning a function f(·) that maps the inventory level yt after replenishment at the time t to
an order quantity qt. We plot the ratio of the expected OOS losses to the OOS loss of learning
order quantities using N =300 samples. We investigate both backlogged and lost-sales models: the
former, which is the main focus of this paper, is well known for the optimality of a base-stockXie, Ma and Xin: VCTheoryforInventoryPolicies
28
policy; the latter is notoriously challenging to analyze within the inventory literature (e.g., Zipkin
2008, Xin and Goldberg 2016, Xin 2021), making it particularly interesting to illustrate the power
of domain knowledge in inventory management.
We first compare the two approaches in the backlogged model. Note that the state space can be
condensedtoyt∈{−D¯,...,0,...,2D¯},giventhatdt∈{0,1,...,D¯}.Consequently,thereare3D¯+1
parameterstolearn.Inaddition,therangeoff(yt)canbenarrowedto(cid:8) (−yt)+,...,2D¯ −yt(cid:9) .This
reduction implicitly incorporates some domain knowledge about inventory dynamics. The results
are presented in Figure 5. The estimation error associated with learning a single base-stock level
is significantly lower than that of learning 3D¯ +1 parameters. In addition, given that an optimal
base-stock policy exists, both approaches yield zero approximation error. As a consequence, the
learning-base-stock approach dominates the learning-order-quantity approach for all sample sizes.
Furthermore,theirperformancegapwidensforlargervaluesofD¯,whenthelearning-order-quantity
approach faces an increased number of parameters to learn.
(a) dt∼U{0,1,2} (b) dt∼U{0,1,2,3}
Figure5 The expected OOS loss ratios of learning-base-stock and learning-order-quantity in the backlogged
model.
Next, weconduct a similarcomparison inthe morecomplicated lost-sales model withleadtimes.
The lost-sales assumption is especially appropriate in online retail settings, where customers can
easily switch among different competing retailers. In this lost-sales model, the optimal policy often
involves complex, state-dependent decisions and is challenging to compute due to the curse of
dimensionality. Although base-stock policies may not always be optimal, inventory theory suggests
that they become asymptotically optimal as shortage costs increase (Huh et al. 2009, Wei et al.
2021). These policies are also expected to perform well for short lead times. Similarly, the stateXie, Ma and Xin: VCTheoryforInventoryPolicies
29
space can be condensed to yt∈{0,1,...,2D¯} (i.e., 2D¯ +1 parameters to learn), and the range of
f(yt) can be narrowed to f(yt)∈{0,1,...,2D¯−yt}. The results are presented in Figure 6. Because
the loss function is also convex w.r.t. the base-stock level in the lost-sales model (e.g., Janakiraman
(cid:112)
and Roundy 2004), the estimation error for the learning-base-stock approach is also O( 1/N), as
discussed in Remark 1. This is also evidenced in Figure 6, where it is shown that the estimation
error for the learning-base-stock approach is significantly lower than that for the learning-order-
quantity approach. In addition, the intersection point between the two approaches is relatively
large, suggesting that the simpler learning-base-stock approach is preferable for small sample sizes.
Thisobservationalignswiththeprinciplethat“learninglessismore.”Furthermore,asD¯ increases,
the intersection point shifts rightward, even though the approximation error (distance between the
dotted lines) has grown. This suggests that overfitting becomes a much more dominant issue as the
problem scale increases. We note that the problem size is quite small in the settings studied here,
with L=1 and discrete demand dt∈{0,...,D¯}, involving only 2D¯+1 states. The size of the state
space can be significantly larger for unbounded distributions and also increases exponentially with
L. For example, as illustrated in Table 2 of Zipkin (2008), with a Geometric demand distribution
and the same cost parameters (h=1 and b=9), an increase in L from 1 to 4 results in the size of
theeffectivestatespace—afterapplyingastate-reductiontechnique—growingfrom21to52,722.In
suchsettingswithlargestatespaces,wecanexpecttheintersectionpointtobesignificantlyshifted
to the right, implying that learning-base-stock outperforms learning-order-quantity across a wide
range of sample sizes. This highlights the importance of focusing on well-structured policy classes
whendataislimited,emphasizingthevalueofdomainknowledgederivedfrom70yearsofinventory
theory. Finally, we note that the idea of learning well-structured policies is not unprecedented in
the online learning literature, particularly within the context of the lost-sales inventory model with
lead times and censored demand. Notably, all existing studies focus on learning the optimal base-
stock (or other well-structured) policy, which may not necessarily be optimal (e.g., Zhang et al.
2020, Lyu et al. 2024).
6. Conclusion
This paper adopts a novel approach by analyzing generalization guarantees of several well-known
classes of inventory policies, leveraging the celebrated VC theory. Both our theory and simulations
reveal important managerial insights into the principle of “learning less is more” in inventory
management. Depending on the amount of data available, it may be beneficial to restrict oneself
to a simpler, albeit suboptimal, class of inventory policies. Moreover, the number of parameters
in a policy class may not accurately measure overfitting. For example, the generalization error for
(St) policies is comparable to that of (s,S) policies, despite having significantly more parameters.Xie, Ma and Xin: VCTheoryforInventoryPolicies
30
(a) dt∈{0,1,2} (b) dt∈{0,1,2,3}
Figure6 TheexpectedOOSlossratiosoflearning-base-stockandlearning-order-quantityinthelost-salesmodel.
Our work has the potential for broader, significant impacts. In recent years, many studies have
applied state-of-the-art deep learning approaches to solve large-scale inventory problems. We be-
lieveourstructuredapproachoflearningbase-stocklevelsinsteadoforderquantitiescanpotentially
enhance existing deep learning techniques, which is particularly valuable in retailing where rele-
vant data is often limited. In other words, it is important to leverage the domain knowledge from
70 years of inventory theory. The idea of leveraging academic knowledge and restricting to policy
structures that have been well-established by theory can potentially translate to other problems
too.
Acknowledgments
The authors thank Matias Alvo for providing useful comments on an early draft.
References
Alon N, Ben-David S, Cesa-Bianchi N, Haussler D (1997) Scale-sensitive dimensions, uniform convergence,
and learnability. Journal of the ACM (JACM) 44(4):615–631.
Alvo M, Russo D, Kanoria Y (2023) Neural inventory control in networks via hindsight differentiable policy
optimization. arXiv preprint arXiv:2306.11246 .
ArrowKJ,HarrisT,MarschakJ(1951)Optimalinventorypolicy.Econometrica:JournaloftheEconometric
Society 250–272.
Balcan MF (2020) Data-driven algorithm design. arXiv preprint arXiv:2011.07177 .
Balcan MF, DeBlasio D, Dick T, Kingsford C, Sandholm T, Vitercik E (2021) How much data is suffi-
cient to learn high-performing algorithms? generalization guarantees for data-driven algorithm design.
Proceedings of the 53rd Annual ACM SIGACT Symposium on Theory of Computing, 919–932.Xie, Ma and Xin: VCTheoryforInventoryPolicies
31
Balcan MF, Sandholm T, Vitercik E (2023) Generalization guarantees for multi-item profit maximization:
Pricing, auctions, and randomized mechanisms. Operations Research Forthcoming.
Ban GY (2020) Confidence intervals for data-driven inventory policies with demand censoring. Operations
Research 68(2):309–326.
Ban GY, Rudin C (2019) The big data newsvendor: Practical insights from machine learning. Operations
Research 67(1):90–108.
BerendD,KontorovichA(2013)Asharpestimateofthebinomialmeanabsolutedeviationwithapplications.
Statistics & Probability Letters 83(4):1254–1259.
Besbes O, Ma W, Mouchtaki O (2023) From contextual data to newsvendor decisions: On the actual perfor-
mance of data-driven algorithms. arXiv preprint arXiv:2302.08424 .
Besbes O, Mouchtaki O (2023) How big should your data really be? data-driven newsvendor: learning one
sample at a time. Management Science Forthcoming.
Bousquet O, Boucheron S, Lugosi G (2003) Introduction to statistical learning theory. Summer School on
Machine Learning, 169–207 (Springer).
Broadbent S, Kadiri H, Lumley A, Ng N, Wilk K (2021) Sharper bounds for the chebyshev function θ(x).
Mathematics of Computation 90(331):2281–2315.
CheungWC,Simchi-LeviD(2019)Sampling-basedapproximationschemesforcapacitatedstochasticinven-
tory control models. Mathematics of Operations Research 44(2):668–692.
Elmachtoub AN, Grigas P (2022) Smart “predict, then optimize”. Management Science 68(1):9–26.
Gijsbrechts J, Boute RN, Van Mieghem JA, Zhang DJ (2022) Can deep reinforcement learning improve
inventory management? performance on lost sales, dual-sourcing, and multi-echelon problems. Manu-
facturing & Service Operations Management 24(3):1349–1368.
Guan X, Miˇsi´c VV (2022) Randomized policy optimization for optimal stopping. arXiv preprint
arXiv:2203.13446 .
Guo C, Huang Z, Tang ZG, Zhang X (2021) Generalizing complex hypotheses on product distributions:
Auctions, prophet inequalities, and pandora’s problem. Conference on Learning Theory, 2248–2288
(PMLR).
Gupta R, Roughgarden T (2020) Data-driven algorithm design. Communications of the ACM 63(6):87–94.
Han J, Hu M, Shen G (2023) Deep neural newsvendor. arXiv preprint arXiv:2309.13830 .
Har-PeledS(2011)Geometricapproximationalgorithms (Providence,RhodeIsland:AmericanMathematical
Society).
HuhWT,JanakiramanG,MuckstadtJA,RusmevichientongP(2009)Asymptoticoptimalityoforder-up-to
policies in lost sales inventory systems. Management Science 55(3):404–420.Xie, Ma and Xin: VCTheoryforInventoryPolicies
32
Huh WT, Rusmevichientong P (2009) A nonparametric asymptotic analysis of inventory planning with
censored demand. Mathematics of Operations Research 34(1):103–123.
Janakiraman G, Roundy RO (2004) Lost-sales problems with stochastic lead times: Convexity results for
base-stock policies. Operations Research 52(5):795–803.
Kearns MJ, Schapire RE (1994) Efficient distribution-free learning of probabilistic concepts. Journal of
Computer and System Sciences 48(3):464–497.
Levi R, Perakis G, Uichanco J (2015) The data-driven newsvendor problem: New bounds and insights.
Operations Research 63(6):1294–1306.
LeviR,RoundyRO,ShmoysDB(2007)Provablynear-optimalsampling-basedpoliciesforstochasticinven-
tory control models. Mathematics of Operations Research 32(4):821–839.
Lin M, Huh WT, Krishnan H, Uichanco J (2022) Data-driven newsvendor problem: Performance of the
sample average approximation. Operations Research 70(4):1996–2012.
Liu J, Lin S, Xin L, Zhang Y (2023) Ai vs. human buyers: A study of alibaba’s inventory replenishment
system. INFORMS Journal on Applied Analytics 53(5):372–387.
Lykouris T, Vassilvitskii S (2021) Competitive caching with machine learned advice. Journal of the ACM
(JACM) 68(4):1–25.
Lyu C, Zhang H, Xin L (2024) Ucb-type learning algorithms with kaplan-meier estimator for lost-sales
inventory models with lead times. Operations Research Forthcoming.
Madeka D, Torkkola K, Eisenach C, Luo A, Foster DP, Kakade SM (2022) Deep inventory management.
arXiv preprint arXiv:2210.03137 .
Mendelson S (2003) A few notes on statistical learning theory. Mendelson S, Smola AJ, eds., Advanced Lec-
turesonMachineLearning,1–40,LectureNotesinComputerScience,Volume2600(Berlin,Heidelberg:
Springer).
Mohri M, Rostamizadeh A, Talwalkar A (2018) Foundations of machine learning (Cambridge, MA: MIT
Press), second edition.
Morgenstern JH, Roughgarden T (2015) On the pseudo-dimension of nearly optimal auctions. Advances in
Neural Information Processing Systems 28.
Oroojlooyjadid A, Nazari M, Snyder LV, Tak´aˇc M (2022) A deep q-network for the beer game: Deep re-
inforcement learning for inventory optimization. Manufacturing & Service Operations Management
24(1):285–304.
Pollard D (1984) Convergence of stochastic processes (New York: Springer-Verlag).
Qi M, Shi Y, Qi Y, Ma C, Yuan R, Wu D, Shen ZJ (2023) A practical end-to-end inventory management
model with deep learning. Management Science 69(2):759–773.Xie, Ma and Xin: VCTheoryforInventoryPolicies
33
Shalev-Shwartz S, Ben-David S (2014) Understanding machine learning: From theory to algorithms (New
York: Cambridge University Press).
Sinclair SR, Frujeri FV, Cheng CA, Marshall L, Barbalho HDO, Li J, Neville J, Menache I, Swaminathan
A (2023) Hindsight learning for mdps with exogenous inputs. International Conference on Machine
Learning, 31877–31914 (PMLR).
Sizer T (1984) Horace’s Compromise: The Dilemma of the American High School (New York: Houghton
Mifflin).
Snyder LV, Shen ZJM (2019) Fundamentals of supply chain theory (Hoboken, NJ: John Wiley & Sons),
second edition.
Vapnik V, Chervonenkis AY (1971) On the uniform convergence of relative frequencies of events to their
probabilities. Theory of Probability & Its Applications 16(2):264–280.
Vapnik VN (1998) Statistical Learning Theory (New York: Wiley).
WeiL,JasinS,XinL(2021)Onadeterministicapproximationofinventorysystemswithsequentialservice-
level constraints. Operations Research 69(4):1057–1076.
Xin L (2021) Understanding the performance of capped base-stock policies in lost-sales inventory models.
Operations Research 69(1):61–70.
Xin L, Goldberg DA (2016) Optimality gap of constant-order policies decays exponentially in the lead time
for lost sales models. Operations Research 64(6):1556–1565.
Yuan H, Luo Q, Shi C (2021) Marrying stochastic gradient descent with bandits: Learning algorithms for
inventory systems with fixed costs. Management Science 67(10):6089–6115.
Zhang H, Chao X, Shi C (2020) Closing the gap: A learning algorithm for lost-sales inventory systems with
lead times. Management Science 66(5):1962–1980.
ZipkinP(2008)Oldandnewmethodsforlost-salesinventorysystems.Operations research 56(5):1256–1263.e-companionto Xie, Ma and Xin: VCTheoryforInventoryPolicies ec1
Electronic Companion for “VC Theory for Inventory Policies”
EC.1. Background on VC-Dimension and Pseudo-Dimension
In this section, we present a variety of examples illustrating how to bound the VC-dimension and
Pseudo-dimension. We maintain consistent notation with the main paper. Following conventions in
learning theory, d is interpreted as either a training sample or a problem instance, Π represents a
class of policies or algorithms, and ℓ(π,d) denotes the performance, e.g., loss (or reward), of policy
π on sample d. We focus on contexts involving a policy class Π⊆Rk, which is parameterized by k
real numbers. Recall that the function class L(Π)={ℓ(π,·):π∈Π} consists of loss functions w.r.t.
sample d across different policies π. Our goal is to bound the VC-dimension or Pseudo-dimension
of L(Π). Let us start with examples that directly bound VC-dim(L(Π)) or P-dim(L(Π)). We note
that in machine learning textbooks (e.g., Mohri et al. 2018), the VC-dimension is often introduced
w.r.t. the classifier itself; however, because our focus is on decision problems, we introduce VC-
dimension directly in relation to the loss functions induced by policies.
Example EC.1. Consider d∈R, Π={(a,a¯):a≤a¯}⊆R2, and ℓ(π,d)=I{a≤d≤a¯} for π =
(a,a¯). In this example, a policy π is represented by a closed interval on the real line, a sample d is
represented by a point on the real line, and the loss function is set to one if π includes d, and zero
otherwise. As illustrated in Figure EC.1(a), there exist two samples d and d with d <d , for
1 2 1 2
which there exist four policies π ,π ,π ,π such that (I{ℓ(π,d )>0}, I{ℓ(π,d )>0}) corresponds
1 2 3 4 1 2
tothepairs(1,0),(0,1),(0,0),(1,1),respectively.ItimpliesthatVC-dim(L(Π))≥2.Inaddition,as
illustrated in Figure EC.1(b), for any set of three samples d ,d ,d labeled such that d <d <d ,
1 2 3 1 2 3
no policy π exists such that (I{ℓ(π,d )>0},I{ℓ(π,d )>0},I{ℓ(π,d )>0})=(1,0,1). (It is also
1 2 3
immediately clear that if any two samples are identical, then the losses incurred on them for the
same π cannot be different.) Thus, VC-dim(L(Π))=2.
FigureEC.1 VC-dim(L(Π))=2 in Example EC.1.ec2 e-companionto Xie, Ma and Xin: VCTheoryforInventoryPolicies
Example EC.2. WenowextendtheclassofintervalsasdescribedinExampleEC.1totheclass
of axis-aligned rectangles in R2. Consider d=(d1,d2)∈R2, Π=(cid:8) (a,a¯,b,¯b):a≤a¯, b≤¯b(cid:9) ⊆R4 and
ℓ(π,d)=I{a≤d1≤a¯, b≤d2≤¯b} for π=(a,a¯,b,¯b). In this example, a policy π is represented by
an axis-aligned rectangle in R2, a sample d is represented by a point in R2, and the loss function
is set to one if π includes d, and zero otherwise. As illustrated in Figure EC.2(a), there exists a
set of four samples d ,d ,d ,d that can be shattered by a set of axis-aligned rectangles, implying
1 2 3 4
that VC-dim(L(Π))≥4. In addition, for any set of five samples, there must exist three samples,
denoted by d∗, d′ and d′′, such that d∗ is contained within the axis-aligned rectangle uniquely
defined by d′ and d′′ as opposite corners. For example, in Figure EC.2(b), we have d∗=d , d′=d
5 4
and d′′ =d . It is easy to verify that there does not exist an axis-aligned rectangle π such that
1
(I{ℓ(π,d′)>0},I{ℓ(π,d′′)>0},I{ℓ(π,d∗)>0})=(1,1,0). Thus, VC-dim(L(Π))=4.
FigureEC.2 VC-dim(L(Π))=4 in Example EC.2.
Example EC.3. The loss functions discussed in both Examples EC.1 and EC.2 are binary.
We next discuss a class of loss functions that are real-valued and convex w.r.t. the sample d.
Consider d ∈ R, Π = {a : a ∈ R}, and with some c ,c > 0, ℓ(π,d) = c [d−a]+ +c [a−d]+ for
1 2 1 2
π = a. For each τ ≥ 0 and π ∈ Π, because of the convexity of ℓ(π,d) in d, {d:ℓ(π,d)≤τ} de-
fines an interval in d, as shown in Figure EC.3(a). Similar to Example EC.1, it is easy to
show that P-dim(L(Π))≥2. We next prove P-dim(L(Π))≤2. The argument is not as straight-
forward as in Example EC.1, as illustrated in Figure EC.3(a). Specifically, for any three sam-
ples d < d < d , there exist thresholds τ ,τ ,τ and policy π such that v(π,(d )3 ,(τ )3 ) ≜
1 2 3 1 2 3 i i=1 i i=1
(I{ℓ(π,d )>τ }, I{ℓ(π,d )>τ }, I{ℓ(π,d )>τ })=(0,1,0), due to the additional flexibility in se-
1 1 2 2 3 3
lecting τ . By contrast, this scenario cannot happen in the setting of Example EC.1 without the
i
ability to select τ . However, we can alternatively show that for any τ ,τ ,τ ,π ,π , it is impossible
i 1 2 3 1 2e-companionto Xie, Ma and Xin: VCTheoryforInventoryPolicies ec3
for both v(π ,(d )3 ,(τ )3 ) = (0,1,0) and v(π ,(d )3 ,(τ )3 ) = (1,0,1) to occur simultane-
1 i i=1 i i=1 2 i i=1 i i=1
ously.Thus,P-dim(L(Π))≤2.Itisimportanttonotethatthisargumentisapplicablenotonlytoa
broad class of general convex functions in single variables, but, due to the symmetry of shattering,
it also extends to a class of general concave functions.
The above approach to find an upper bound on the Pseudo-Dimension involves brute-forcing
π =a in R to calculate the values of v(π,(d )3 ,(τ )3 ). Alternatively, we can first determine
i i=1 i i=1
the ranges of π where ℓ(π,d )≤τ holds for each (d ,τ ) pair, as illustrated in Figure EC.3(b),
i i i i
and then brute-force π. It is easy to see that there are at most 7 different values of π resulting
in distinct v(π,(d )3 ,(τ )3 ), which contradicts the fact that, by the definition of shattering,
i i=1 i i=1
the number of distinct vectors |v(π,(d )3 ,(τ )3 ):π∈Π| should be 23=8. This concludes that
i i=1 i i=1
P-dim(L(Π))≤2.
FigureEC.3 P-dim(L(Π))=2 in Example EC.3.
Examples EC.1-EC.3 illustrate the direct approach to calculating VC-dimension and Pseudo-
dimension: this involves first constructing a problem instance to establish a lower bound, followed
by arguments to demonstrate the tightness of this lower bound. This is the traditional approach,
based on the definitions of VC-dimension and Pseudo-dimension. We refer readers to Section 3.3 of
Mohrietal.(2018)foradditionalexamples,includingthoseinvolvinghyperplanesinRn andconvex
polygons in R2. Note that all these examples are based on the structure of the primal loss function
ℓ(π,·) w.r.t. the sample d, where d belongs to a low-dimensional Euclidean space. In general, the
sample space can be quite complex, lacking obvious notions of Lipschitz continuity or smoothness
to rely on, making the traditional approach challenging to implement (Balcan et al. 2021). We now
describe another approach for upper bounding the VC-dimension and Pseudo-dimension, whichec4 e-companionto Xie, Ma and Xin: VCTheoryforInventoryPolicies
is based on the “dual” loss functions ℓ(·,d):Π→R. Recall that the dual function class L∗(Π) of
L(Π) is defined as the set of functions ℓ(·,d) indexed by the possible samples d. This approach can
be desirable if the parameter space Π lies within a low-dimensional Euclidean space. In addition,
many combinatorial problems share a clear-cut, useful structure; for example, for each sample d,
thefunctionℓ(·,d)canbepiecewisestructured(Balcan2020,Balcanetal.2021).Theseadvantages
can make the analysis more tractable compared to a direct analysis of the primal loss function,
as we have already demonstrated in the second part of Example EC.3. Next, we provide another
example that utilizes this dual approach.
Example EC.4. Consider Π⊆R and suppose the dual loss function ℓ(·,d):Π→R is piecewise
constant with at most C+1 pieces for all d. Suppose that there exist m samples d ,...,d that
1 m
are shattered by L(Π) with witnesses τ ,...,τ . We now attempt to count the maximum number
1 m
of distinct vectors (I{ℓ(π,d )>τ },...,I{ℓ(π,d )>τ }) that can be generated by all π∈Π. Note
1 1 m m
that for any fixed d, the function ℓ(·,d) can change value at most C times over R. Thus, given m
suchfunctionsℓ(·,d ),...,ℓ(·,d ),therecanbeatmostmC coordinatesinRatwhichanyofthem
1 m
changesvalue.Therefore,thevector(ℓ(π,d ),...,ℓ(π,d ))cantakeatmostmC+1distinctvalues
1 m
as π ranges over R, and hence |(I{ℓ(π,d )>τ },...,I{ℓ(π,d )>τ })|≤mC +1. We illustrate
1 1 m m
an example of this in Figure EC.4, where m=2, C =3, and value changes occur at π ,...,π . In
1 6
general, this implies that 2m≤mC+1 due to the definition of shattering, and hence m=O(logC),
i.e., P-dim(L(Π))=O(logC). In the example where C=3, we have P-dim(L(Π))≤3.
FigureEC.4 Example EC.4 with C=3.
In summary, the dual approach focusing on the dual function class starts by analyzing the
structure of ℓ(·,d) for a given d. It then considers m copies of this structure corresponding to
m shattered samples d ,...,d . The goal is to count the maximum number of distinct vectors
1 m
(I{ℓ(π,d )>τ },...,I{ℓ(π,d )>τ }) that can be generated by all π ∈ Π. We refer readers to
1 1 m m
Balcanetal.(2021)forbroaderapplicationsofthisapproach,andtoChapter5ofHar-Peled(2011)e-companionto Xie, Ma and Xin: VCTheoryforInventoryPolicies ec5
foranin-depthdiscussionontheVC-dimensionanditsdual.Theapproachisparticularlypowerful
in high-dimensional sample spaces. For example, in our setting, the sample d is T-dimensional and
weareinterestedininvestigatingP-dim(L(Π))forlargeT.Moreover,thisapproachusuallyrequires
a thorough analysis of policies or algorithms, augmented by the application of domain-specific
knowledge, such as inventory theory. We adopt this approach in the proofs of Theorems 1 and 2
in this paper. Specifically, the proof of Theorem 1 relies on the fact that the loss function ℓ(S,d)
is convex w.r.t. the base-stock level S for each d. The proof of Theorem 2 requires a more delicate
analysis,utilizingtheunderlyingstructureofthelossfunctionℓ((s,S),d)w.r.t.(s,S).Forexample,
for a given d, our analysis leads to the conclusion that the condition I{ℓ((∆,S),d)≤τ}=1 defines
a polynomial number of axis-aligned rectangles in the (∆,S) plane with ∆=S−s. Fortunately, in
both cases, this dual approach yields upper bounds for the Pseudo-dimension that are relatively
tight, in terms of dependence on T.
EC.2. Instructive Counterexamples
EC.2.1. Approximation Error from Discretization
In this section, we present a formal proof of (3). Let M be a positive integer. Consider the set-
ting with U =1, L=0, cost parameters b=h=1/2,K =0, a sufficiently large even T, and a
deterministic demand sequence
(cid:18) (cid:19)
1 1 1
d= 1, , 1, , ...,1, .
2M 2M 2M
Note that H =−1 and H =2 in this example. It is straightforward to see that the policy π† =
(cid:0) (cid:1)
(∆,S)= 1+ 1 , 1+ 1 results in a replenishment at the beginning of every odd period, leading
2M 2M
to a loss of 1 .
4M
We now focus on Π , the class of discretized (s,S) policies where S and ∆ are multiples of 1 .
M M
For policies with ∆=0, replenishment occurs in every period. Thus, for any S, the loss incurred
by these policies is
1(cid:18) 1 1(cid:12) (cid:12) 1 (cid:12) (cid:12)(cid:19) 1− 1
|S−1|+ (cid:12)S− (cid:12) ≥ 2M = Ω(1).
2 2 2(cid:12) 2M(cid:12) 4
For policies with 1 ≤∆≤1, replenishment occurs at t=1 and every even period. Thus, for any
M
S, the loss is at least
(cid:20) (cid:18) (cid:12) (cid:12) (cid:12) (cid:12)(cid:19)(cid:21)
1 1 T −2 1(cid:12) 1 (cid:12) 1(cid:12) 1 (cid:12) T −2
|S−1|+ (cid:12)S− (cid:12)+ (cid:12)S− −1(cid:12) ≥ = Ω(1).
T 2 2 2(cid:12) 2M(cid:12) 2(cid:12) 2M (cid:12) 4T
For policies with ∆≥1+ 1 , each replenishment cycle contains at least three periods and the
M
demand realizations over each cycle must have both 1 and 1 . Thus, one can verify that for any
2M
S, the loss is at least (asymptotically)
(cid:18) (cid:12) (cid:12) (cid:12) (cid:12)(cid:19)
1 1 1(cid:12) 1 (cid:12) 1(cid:12) 1 (cid:12)
|S−1|+ (cid:12)S−1− (cid:12)+ (cid:12)S−2− (cid:12)
3 2 2(cid:12) 2M(cid:12) 2(cid:12) 2M(cid:12)
1(cid:18) 1(cid:12) (cid:12) 1 (cid:12) (cid:12) 1(cid:12) (cid:12) 1 (cid:12) (cid:12) 1(cid:12) (cid:12) 1 (cid:12) (cid:12)(cid:19) 1+ 1
= (cid:12)S− (cid:12)+ (cid:12)S−1− (cid:12)+ (cid:12)S−1− (cid:12) ≥ 2M = Ω(1).
3 2(cid:12) 2M(cid:12) 2(cid:12) 2M(cid:12) 2(cid:12) M(cid:12) 6ec6 e-companionto Xie, Ma and Xin: VCTheoryforInventoryPolicies
For example, when ∆=1+ 1 , the loss is
M
⌊T⌋(cid:18) (cid:12)
(cid:12) 1
(cid:12)
(cid:12)
(cid:12)
(cid:12) 1
(cid:12)
(cid:12)
(cid:12)
(cid:12) 1
(cid:12)
(cid:12)
(cid:12)
(cid:12) 1
(cid:12)
(cid:12)
(cid:12)
(cid:12) 1
(cid:12) (cid:12)(cid:19)
6 |S−1|+(cid:12)S−1− (cid:12)+(cid:12)S−2− (cid:12)+(cid:12)S− (cid:12)+(cid:12)S−1− (cid:12)+(cid:12)S−1− (cid:12) = Ω(1).
2T (cid:12) 2M(cid:12) (cid:12) 2M(cid:12) (cid:12) 2M(cid:12) (cid:12) 2M(cid:12) (cid:12) M(cid:12)
Therefore, for any M and any discretized policy π ∈Π , R(π )−R(π†)=Ω(1), which is inde-
M M M
pendent of M. This concludes that the approximation error from discretization remains a constant
even for an arbitrary large M. The proof of (3) is completed.
EC.2.2. Pseudo-Dimension Lower Bound for (St) Policies
In this section, we show that there exists T demand sequences that can be shattered by Π ,
(St)
suggesting the failure of using Pseudo-dimension to bound the expected generalization error of
(St) policies. Consider the instance with b=1, h=0, U =1, and L=0. It follows that all the
base-stock levels St and demand realizations lie within the interval [0,1]. The T demand sequences
(d )T are defined as follows: dt=1 if t=i and dt=1/2 otherwise. For any A⊆[T], consider the
i i=1 i i
following policy (St)T : St =1/2 if t∈A and St =1 otherwise. It is straightforward to see that
A t=1 A A
ℓ((St)T ,d )=1/(2T) if i∈A and ℓ((St)T ,d )=0 otherwise. Thus, with witnesses τ =0 for all
A t=1 i A t=1 i i
(cid:0) (cid:1)
i∈[T], (d )T are shattered by the (St)T policy class, implying P-dim L(Π ) ≥T.
i i=1 t=1 (St)
EC.3. Missing Proofs
EC.3.1. Proof of Theorem 1
Suppose P-dim(L(Π )) = m. By the equivalent definition of Pseudo-dimension, there exist m
S
demand samples d ,...,d and corresponding witnesses τ ,...,τ ∈R such that |E|=2m, where
1 m 1 m
 I{ℓ(S,d )>τ }  
  1 1  
E≜   . . .  :S∈Π S .
  I{ℓ(S,d )>τ }  
m m
From (4), ℓ(S,d)=ℓ(S,d) is convex in S for any given d. Thus, for any give d and τ, the set of S
satisfying I{ℓ(S,d)≤τ}=1 is an interval in Π ⊆R. Thus, I{ℓ(S,d )≤τ } for i∈[m] generate m
S i i
intervals,whichdivideRintoatmost2m+1non-overlappingregions.Consideringanyoneofthese
2m+1 non-overlapping regions in Π , we know (I{ℓ(S,d )>τ },...,I{ℓ(S,d )>τ }) is invariant
S 1 1 m m
for S within this region. Thus, by searching S in R, E has at most 2m+1 different elements. To
have |E|=2m, we must have 2m≤2m+1, implying P-dim(L(Π ))=m≤2. The proof of Theorem
S
1 is completed.
EC.3.2. Proof of Theorem 2
We first investigate the structure of the dual class L∗(Π )={ℓ((·,·),d):d∈[0,U]T+L} in the
(s,S)
(∆,S) space. We claim that for given d and τ ∈R, the region of (∆,S) satisfying I{ℓ((∆,S),d)≤
τ}=1 consists of at most T(T−1)/2+1 non-overlapping axis-aligned rectangles in R2. (Note that
+e-companionto Xie, Ma and Xin: VCTheoryforInventoryPolicies ec7
FigureEC.5 Consider an instance where T =3 and the demand sequence is d=(4,3,1). For ∆∈[0,3], the
reordering sequence is {1,2,3}; for ∆∈(3,4], the reordering sequence is {1,2}; for ∆∈(4,7], the
reorderingsequenceis{1,3};for∆∈(7,∞),thereorderingsequenceis{1}.Thus,thisconfiguration
results in a maximum of 4 non-overlapping axis-aligned rectangles.
the range of (∆,S) is relaxed to R2.) It is straightforward to see that for any given ∆, ℓ((∆,S),d)
+
is convex in S, because the reordering sequence is fixed and c(·) is convex. It implies that for
given ∆, I{ℓ((∆,S),d)≤τ}=1 occurs in an interval of S. Moreover, by the definition of (s,S)
policy, for any given S, the (∆+δ,S) policy or the (∆−δ,S) policy have the same reordering
sequence and loss function with the (∆,S) policy for some small δ>0. Thus, the region of (∆,S)
satisfying I{ℓ((∆,S),d)≤τ}=1 consists of multiple non-overlapping axis-aligned rectangles in
R2. Furthermore, for a given d, the number of rectangles is no greater than the number of different
+
reordering sequences that can be constructed with the demand sequence. Note that a reordering
sequence is determined by ∆ and summations of consecutive demand values. By the definition of
reordering sequence, it only depends on the first T −1 demand values d1,...,dT−1. Because the
number of different consecutive segments in (d1,...,dT−1) with length n is T −n, the number of
different values by summing consecutive demands in first T −1 periods is at most
(cid:80)T−1(T
−n)=
n=1
T(T −1)/2. As ∆ increases from 0 to infinity, the current reordering sequence changes to a new
one only when ∆ reaches one of these T(T −1)/2 values. Thus, the region of (∆,S) satisfying
I{ℓ((∆,S),d)≤τ}=1 consists of at most T(T −1)/2+1 non-overlapping axis-aligned rectangles
in R2. Among these, one rectangle is unbounded as ∆→∞. See an illustration for T =3 in Figure
+
EC.5.
(cid:0) (cid:1)
Suppose P-dim L(Π ) =m. By definition, there exist m samples d ,...,d and correspond-
(s,S) 1 m
ing witnesses τ ,...,τ ∈R such that |E|=2m, where
1 m
 I{ℓ((∆,S),d )>τ }  
  1 1  
E≜   . . .  :(∆,S)∈Π (s,S) .
  I{ℓ((∆,S),d )>τ }  
m Nec8 e-companionto Xie, Ma and Xin: VCTheoryforInventoryPolicies
Similar to the proof of Theorem 1, our goal is to count the maximum number of regions that
I{ℓ((∆,S),d )≤τ }=1 can divide a plane of (∆,S) into for each i∈[m]. By the above claim, the
i i
region of I{ℓ((∆,S),d)≤τ}=1 generates at most T(T −1)/2 vertical lines and 2(T(T −1)/2+1)
horizontal lines. Thus, for all i ∈ [m], I{ℓ((∆,S),d ) ≤ τ } can generate at most mT(T −1)/2
i i
vertical lines and 2m(T(T −1)/2+1) horizontal lines, dividing R2 into at most (mT(T −1)/2+
+
1)(2m(T(T −1)/2+1)+1)=O(m2T4) regions in total. To have |E|=2m, we must have 2m ≤
O(m2T4), implying m=O(logT). The proof of Theorem 2 is completed.
EC.3.3. Proof of Theorem 3
Let {p }m be the first m prime numbers (e.g., 2, 3, 5, 7, 11, 13, 17, ...). Define P ≜ 1 (cid:81)m p
i i=1 i pi j=1 j
for each i∈[m]. We claim that with b∈(0,1/2] and h=K =0, there exists a set of m demand
sequences d ,...,d that can be γ-shattered by L(Π ) for γ∈[0,b/16] if T
=2(cid:81)m
p . In this
1 m (s,S) j=1 j
setting, the range of policy parameters is H ≤−1 and H =∞. We construct the demand sequences
(d )m as follows: for each i∈[m],
i i=1

1, if t<P +L−1;
 i
dt= 1/2, if t=P +L;
i i
0, if t>P +L.
i
We will argue that a policy with S −s=∆ performs poorly on a sequence d if and only if ∆
i
exactly divides P , and ∆ should be a multiple of p to prevent this from occurring. In particular,
i i
(cid:81)
for any subset A⊆[m] that we would like to perform well on, let us consider ∆ = p (note
A i∈A i
that ∆ =1) and S =∆ −1+L (i.e., s =−1+L). It is easy to see i∈/ A if and only if ∆
∅ A A A A
divides P .
i
If i ∈/ A, then the ending inventory level vector (yt − dt)T+L = (∆ − 2,∆ −
t=L+1 A A
3,...,0,−1,∆ − 2,∆ − 3,...,0,−1,...,∆ − 2,∆ − 3,...,0,−1/2,−1/2,...,−1/2),
A A A A
(cid:16) (cid:17)
namely, consisting of exactly Pi −1 cycles (∆ − 2,∆ − 3,...,0,−1) and the
(cid:16) (cid:17)
∆A A A
Pi -th cycle ends with −1, leading to yt − dt = −1 for all t ≥ P + L. Thus,
∆A
(cid:16) (cid:16) (cid:17)
2
(cid:17)
2 i
the loss is 1 b Pi −1 + b(T −P +1) . If i ∈ A, then the vector (yt − dt)T+L =
T ∆A 2 i t=L+1
(∆ −2,∆ −3,...,0,−1,∆ −2,∆ −3,...,0,−1,...,∆ −2,∆ −3,...,r +1,r ,r ,...,r ),
A A A A A A A A A A
namely, consisting of ⌊ Pi ⌋ cycles (∆ −2,∆ −3,...,0,−1) and the ⌈ Pi ⌉-th cycle ends with
(cid:16)
∆A
(cid:17)
A A ∆A
r =∆ −1− P −∆ ⌊ Pi ⌋ + 1 ≥0, leading to yt−dt=r for all t≥P +L. Thus, the loss is
A A i A ∆A 2 A i
no greater than b Pi . In summary, the loss under policy (∆ ,S ) on demand d is
T ∆A A A i
(cid:18) (cid:19) (cid:18) (cid:19)
b P T −P +1 b T −P +1
ℓ((∆ ,S ),d )= i −1+ i ≥ i , if i∈/A,
A A i T ∆ 2 T 2
A
b P b P
ℓ((∆ ,S ),d )< i ≤ i, if i∈A.
A A i T ∆ T p
A ie-companionto Xie, Ma and Xin: VCTheoryforInventoryPolicies ec9
(cid:16) (cid:17)
We then set the witnesses correspondingly: τ = b T−Pi+1 + Pi . Because T =2(cid:81)m p =4P ≥
(cid:16) (cid:17)
i 2T 2 pi j=1 j 1
4P , b T−Pi+1 − Pi > b (cid:0)T−Pi −P (cid:1) ≥b/8. Thus, it is straightforward to verify that for any γ∈
i T 2 pi T 2 i
[0,b/16],
ℓ((∆ ,S ),d )>τ +γ, if i∈/A, and ℓ((∆ ,S ),d )≤τ −γ, if i∈A.
A A i i A A i i
That is, the set of demand sequences d ,...,d can be γ-shattered by L(Π ).
1 m (s,S)
From the celebrated Prime Number Theorem, the number of prime numbers less than or
(cid:16) (cid:17)
equal to x is asymptotically Θ x as x → ∞. In addition, the first Chebyshev function
logx
θ(x) ≜ (cid:80) logp (with the sum over all prime numbers p that are less than or equal to x) is
p≤x
asymptotically Θ(x) as x → ∞ (e.g., Broadbent et al. 2021). In our case, by the first Cheby-
shev function, θ(x) =
(cid:80)m
logp = log(T/2), implying that x = Θ(log(T/2)). Here, m repre-
i=1 i
sents the number of prime numbers that are less than or equal to x. By the Prime Number
(cid:0) (cid:1)
Theorem, m=Θ(log(T/2)/loglog(T/2))=Θ(logT/loglogT), which implies P -dim L(Π ) =
γ (s,S)
Ω(logT/loglogT). The proof of Theorem 3 is completed.
EC.3.4. Proof of Corollary 3
Suppose dˆ ,...,dˆ are γ-shattered by L(Π ) for a constant γ >0, with witnesses τ ,...,τ ,
1 m (s,S) 1 m
where m=P -dim(cid:0) L(Π )(cid:1) . Let D be the uniform distribution over dˆ ,...,dˆ , implying that
γ (s,S) 1 m
R(π)= 1 (cid:80)m ℓ(π,dˆ ) for all π. For the dataset d ,...,d , let M denote the number of times
m i=1 i 1 N i
that dˆ is drawn for each i∈[m], where (cid:80)m M =N. We have
i i=1 i
E (cid:2) GE(cid:0) L(Π )(cid:1)(cid:3)
d1,...,dN∼D (s,S)
(cid:34) (cid:35)
(cid:110) (cid:111)
= E sup R(π)−Rˆ(π)
d1,...,dN∼D
π∈Π(s,S)
(cid:34) (cid:40) (cid:41)(cid:35)
m m
1 (cid:88) 1 (cid:88)
= E sup ℓ(π,dˆ )− M ℓ(π,dˆ )
d1,...,dN∼D m i N i i
π∈Π(s,S)
i=1 i=1
(cid:34) (cid:40) (cid:41)(cid:35)
(cid:88)m 1 (cid:16) (cid:17) (cid:88)m M (cid:16) (cid:17)
= E sup ℓ(π,dˆ )−τ − i ℓ(π,dˆ )−τ
d1,...,dN∼D m i i N i i
π∈Π(s,S)
i=1 i=1
(cid:34) (cid:35)
(cid:88)m (cid:18) 1 M (cid:19) (cid:16) (cid:17)
= E sup − i ℓ(π,dˆ )−τ
d1,...,dN∼D m N i i
π∈Π(s,S)
i=1
(cid:34) m (cid:12) (cid:12) (cid:35)
≥ E (cid:88)(cid:12) (cid:12) 1 − M i(cid:12) (cid:12)γ
d1,...,dN∼D (cid:12)m N (cid:12)
i=1
(cid:20)(cid:12) (cid:18) (cid:19) (cid:12)(cid:21)
= mγ E (cid:12) (cid:12)Bin N, 1 − N(cid:12) (cid:12) .
N (cid:12) m m(cid:12)
Here, Bin(N,1/m) denotes a random variable that follows Binomial distribution with parameters
N and 1/m; the second equality holds because E [M /N]=1/m; the last equality follows
d1,...,dN∼D iec10 e-companionto Xie, Ma and Xin: VCTheoryforInventoryPolicies
from the uniform distribution over these sample dˆ ,∀i∈[m]; the inequality holds because of the
i
definition of γ-shattering, which implies the existence of π∈Π such that
(s,S)
1 M
ℓ(π,dˆ )>τ +γ if − i >0,
i i m N
1 M
ℓ(π,dˆ )≤τ −γ if − i ≤0.
i i m N
By Theorem 1 of Berend and Kontorovich (2013), for each N ≥m≥2,
(cid:115)
(cid:20)(cid:12) (cid:18) (cid:19) (cid:12)(cid:21) (cid:18) (cid:19)
E (cid:12) (cid:12)Bin N, 1 − N(cid:12) (cid:12) ≥ √1 N 1 1− 1 .
(cid:12) m m(cid:12) 2 m m
Thus, combining with Theorem 3 implies
(cid:114) (cid:32)(cid:115) (cid:33)
E (cid:2) GE(cid:0) L(Π )(cid:1)(cid:3) ≥ √γ m−1 =Ω logT .
d1,...,dN∼D (s,S) 2 N NloglogT
The proof of Corollary 3 is completed.
EC.3.5. Proof of Theorem 4
(cid:0) (cid:1)
It suffices to show that the statement holds for t=T+L. Suppose P -dim(YT+L Π ) =m, that
γ (St)
is, there exist demand samples d ,...,d that are γ-shattered with witnesses τ ,...,τ . Note that
1 m 1 m
we must have τ ∈[−1,1] for i∈[m]. For any fixed i∈[m], there exists a (St)T+L policy such that
i i t=1
yT+L((St)T ,d ) yT+L((St)T ,d )
i t=1 i >τ +γ, and i t=1 j ≤τ −γ, ∀j∈[m]\{i}.
(L+1)U i (L+1)U j
Let t denote the last reorder point that affects the inventory level yT+L((St)T ,d ), namely,
i i t=1 i
(cid:110) (cid:111)
t ≜max t′∈[T]:St′−It′((St)T ,d )>0 , where It′((St)T ,d ) denotes the inventory position
i i i t=1 i i t=1 i
at the beginning of period t′ before replenishment by the (St)T+L policy on the demand sequence
i t=1
d . It follows that
i
T+L−1 T−1
yT+L(cid:0) (St)T ,d (cid:1) =Sti− (cid:88) dt, and (cid:88) dt≤H, (EC.1)
i t=1 i i i i
t=ti t=ti
T+L−1 T+L−1
yT+L(cid:0) (St)T ,d (cid:1) ≥max(cid:8) Sti,Iti(cid:0) (St)T ,d (cid:1)(cid:9) − (cid:88) dt ≥Sti− (cid:88) dt, ∀j∈[m]\{i}.
i t=1 j i i t=1 j j i j
t=ti t=ti
Thus, we have
(cid:32) (cid:33) (cid:32) (cid:33)
T+L−1 T+L−1
1 (cid:88) 1 (cid:88)
Sti− dt >τ +γ, and Sti− dt ≤τ −γ, ∀j∈[m]\{i},
(L+1)U i i i (L+1)U i j j
t=ti t=ti
implying that
T+L−1 T+L−1
1 (cid:88) 1 (cid:88)
dt+τ +2γ≤ dt +τ , ∀j∈[m]\{i}. (EC.2)
(L+1)U i i (L+1)U j j
t=ti t=tie-companionto Xie, Ma and Xin: VCTheoryforInventoryPolicies ec11
Without loss of generality, we assume that t ≥t ≥···≥t . We claim that for all i∈[m],
1 2 m
T+L−1
1 (cid:88)
dt+τ ≥2(i−1)γ−1. (EC.3)
(L+1)U i i
t=ti
It is straightforward to see that the claim holds for i=1, because (dt)T+L are non-negative and
1 t=1
τ ≥−1. Now suppose that (EC.3) holds for some i∈[m−1]. It follows from (EC.2) that
1
T+L−1 T+L−1 T+L−1
1 (cid:88) 1 (cid:88) 1 (cid:88)
dt +τ ≥ dt +τ ≥ dt+τ +2γ≥2iγ−1,
(L+1)U i+1 i+1 (L+1)U i+1 i+1 (L+1)U i i
t=ti+1 t=ti t=ti
wherethefirstinequalityholdsbecauset ≥t .Byinduction,theclaimin(EC.3)holds.Moreover,
i i+1
the inequality in (EC.1) implies
(cid:32) (cid:33)
T+L−1 T−1 T+L−1
1 (cid:88) 1 (cid:88) (cid:88) 1
dt= dt+ dt ≤ (H+LU)≤ 2. (EC.4)
(L+1)U i (L+1)U i i (L+1)U
t=ti t=ti t=T
By τ ≤ 1, we have 2(m−1)γ −1 ≤ 3 from (EC.3) and (EC.4). It follows that m ≤ 2/γ +1,
m
completing the proof of Theorem 4.
EC.3.6. Proof of Corollary 4
We first decouple E (cid:2) GE(L(Π ))(cid:3) into the expected generalization error of (St′)t−L
d1,...,dN∼D (St) t′=1
policies for time t=L+1,...,T +L:
E (cid:2) GE(cid:0) L(Π )(cid:1)(cid:3)
d1,...,dN∼D (St)
 
(cid:110) (cid:111)
= E d1,...,dN∼D sup R((St) tT =+ 1L)−Rˆ((St)T t=+ 1L) 
(St)T t=+ 1L∈Π
(St)
 
(cid:40) (cid:34) (cid:35) (cid:41)
= E d1,...,dN∼D sup E d∼D
T1 T (cid:88)+L ℓt(cid:16)
(St′ )T t′=+
1L,d(cid:17)
−
N1 (cid:88)N T1 T (cid:88)+L ℓt(cid:16)
(St′ )T t′=+ 1L,d
i(cid:17)

(St′)T t′+ =L 1∈[0,H]T+L t=L+1 i=1 t=L+1
 
(cid:40) (cid:32) (cid:33)(cid:41)
= E d1,...,dN∼D sup
T1 T (cid:88)+L
E
d∼D(cid:104) ℓt(cid:16)
(St′ )t t− ′=L
1,d(cid:17)(cid:105)
−
N1 (cid:88)N ℓt(cid:16)
(St′ )t t− ′=L 1,d
i(cid:17)

(St′)T t′+ =L 1∈[0,H]T+L t=L+1 i=1
 
(cid:40) (cid:41)
≤ E d1,...,dN∼D
T1 T (cid:88)+L
sup E
d∼D(cid:104) ℓt(cid:16)
(St′ ) tt ′− =L
1,d(cid:17)(cid:105)
−
N1 (cid:88)N ℓt(cid:16)
(St′ )t t− ′=L 1,d
i(cid:17)

t=L+1(St′)t t− ′=L 1∈[0,H]t−L i=1
 
(cid:40) (cid:41)
=
T1 T (cid:88)+L
E d1,...,dN∼D sup E
d∼D(cid:104) ℓt(cid:16)
(St′ ) tt ′− =L
1,d(cid:17)(cid:105)
−
N1 (cid:88)N ℓt(cid:16)
(St′ )t t− ′=L
1,d(cid:17)

t=L+1 (St′)t t− ′=L 1∈[0,H]t−L i=1
T+L
= 1 (cid:88) E (cid:2) GE(cid:0) Lt(Π )(cid:1)(cid:3) , (EC.5)
T d1,...,dN∼D (St)
t=L+1
(cid:110) (cid:16) (cid:17) (cid:111)
where we define function classes Lt(Π ) ≜ ℓt (St′)t−L,· :(St′)t−L ∈[0,H]t−L for t = L+
(St) t′=1 t′=1
1,...,T +L. By Proposition 1,
E (cid:2) GE(cid:0) Lt(Π )(cid:1)(cid:3) ≤2E (cid:2) R(cid:0) Lt(Π )◦(d )N (cid:1)(cid:3) . (EC.6)
d1,...,dN∼D (St) d1,...,dN∼D (St) i i=1ec12 e-companionto Xie, Ma and Xin: VCTheoryforInventoryPolicies
(cid:16) (cid:17) (cid:16) (cid:17)
Note that ℓt (St′)t−L,d is 1-Lipschitz in yt (St′)t−L,d , and thus is ((L+1)U)-Lipschitz in
t′=1 t′=1
(cid:16) (cid:17)
yt (St′)t−L,d /((L+1)U). By the Talagrand’s contraction lemma (e.g., Lemma 26.9 of Shalev-
t′=1
Shwartz and Ben-David 2014), it follows that
E (cid:2) R(cid:0) Lt(Π )◦(d )N (cid:1)(cid:3) ≤ (L+1)UE (cid:2) R(cid:0) Yt(Π )◦(d )N (cid:1)(cid:3) . (EC.7)
d1,...,dN∼D (St) i i=1 d1,...,dN∼D (St) i i=1
By Proposition 3, there exists constants C ,C >0 such that
1 2
(cid:115)
E (cid:2) R(cid:0) Yt(Π )◦(d )N (cid:1)(cid:3) ≤ √C 1 (cid:90) 1 log(cid:18) 2(cid:19) ·P -dim(cid:0) Yt(Π )(cid:1) dγ. (EC.8)
d1,...,dN∼D (St) i i=1 N γ C2γ (St)
0
(cid:0) (cid:1)
Note that by Theorem 4, P -dim Yt(Π ) ≤2/γ+1 for all t=L+1,...,T +L and γ ∈(0,1],
γ (St)
(cid:0) (cid:1)
and it is straightforward to see that P -dim Yt(Π ) =0 for any γ >1, because Yt(Π ) is
γ (St) (St)
bounded in [−1,1]. Therefore, combining (EC.5)-(EC.8), we have
E (cid:2) GE(cid:0) L(Π )(cid:1)(cid:3)
d1,...,dN∼D (St)
(cid:115)
≤ 2(L+ √1)UC 1 (cid:90) 1 log(cid:18) 2(cid:19) ·P -dim(cid:0) Yt(Π )(cid:1) dγ
N γ C2γ (St)
0
(cid:115)
2(L+1)UC (cid:90) 1 (cid:18) 2(cid:19) (cid:18) 2 (cid:19)
≤ √ 1 log · +1 dγ
N γ C γ
0 2
(cid:115)
2(L+1)UC (cid:90) 1 (cid:18) 2(cid:19) (cid:26) 2 (cid:27) 2
≤ √ 1 log ·max ,1 · dγ.
N γ C γ
0 2
By changing the variable γ=2e−t2,
(cid:115)
(cid:90) 01 log(cid:18) γ2(cid:19) · γ2 dγ=4(cid:90) √∞ log2t2e−t 22 dt=4(cid:18) −te−t 22(cid:12) (cid:12) (cid:12)∞
√
log2+(cid:90) √∞ log2e−t 22 dt(cid:19) ≤2(cid:112) 2log2+4√ 2π.
(cid:16) (cid:113) (cid:17)
It implies that E (cid:2) GE(cid:0) L(Π )(cid:1)(cid:3) =O (L+1)U 1 . The proof of Corollary 4 is com-
d1,...,dN∼D (St) N
pleted.