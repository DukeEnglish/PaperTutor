Deep Policy Optimization with Temporal Logic Constraints
AmeeshShah*1 CameronVoloshin*2 ChenxiYang3 AbhinavVerma4 SwaratChaudhuri3 SanjitA.Seshia1
Abstract beenusedseparatelyinavarietyofRLtasks,butfewworks
aimtoconsiderbothrewardsandspecificationsinthesame
Temporal logics, such as linear temporal logic
setting. Thecombinationofthetwoisimportant: anLTL
(LTL),offeraprecisemeansofspecifyingtasks
specificationcandefinethemeaningofachievingatask,and
for(deep)reinforcementlearning(RL)agents. In
arewardfunctioncanbeoptimizedtofindthebestwayof
ourwork,weconsiderthesettingwherethetaskis
achievingthattask. Forexample,inrobotmotionplanning,
specifiedbyanLTLobjectiveandthereisanaddi-
an LTL specification can describe the waypoints a robot
tionalscalarrewardthatweneedtooptimize. Pre-
should reach and obstacles it should avoid, and a reward
viousworksfocuseitheronlearningaLTLtask-
functioncanoptimizeforitsenergyconsumption.
satisfyingpolicyaloneorarerestrictedtofinite
statespaces. Wemaketwocontributions: First, Inourwork,weconsidertheproblemsettingofRL-based
weintroduceanRL-friendlyapproachtothisset- rewardoptimizationunderLTLconstraints. Optimizinga
tingbyformulatingthisproblemasasingleopti- policyunderLTLconstraintsisdifficultbecausefiniteexpe-
mizationobjective. Ourformulationguarantees riencescollectedbyanRLagentcannotprovideawitness
that an optimal policy will be reward-maximal forthesatisfactionofLTLspecificationsingeneral(Yang
from the set of policies that maximize the like- etal.,2022). PreviousworksthatsolveLTLconstraintsat-
lihood of satisfying the LTL specification. Sec- isfactioninareward-maximizingsettingproposeplanning-
ond,weaddressasparsityissuethatoftenarises basedsolutionmethodsthatareonlyapplicabletofinitestate
for LTL-guided Deep RL by introducing Cycle spaces(Voloshinetal.,2022). Tothebestofourknowledge,
Experience Replay (CyclER), a technique that ourworkisthefirsttoapproachthisproblemwithDeepRL
automaticallyguidesRLagentstowardsthesatis- (DRL)toscaletocontinuousstateandactionspaces.
factionofanLTLspecification. Ourexperiments
Ourcontributionistwofold. First,weintroducetheobjec-
demonstratetheefficacyofCyclERinfindingper-
tiveasaconstrainedoptimizationproblemandformulatean
formantdeepRLpoliciesinbothcontinuousand
unconstrainedproxyforit. WedosobycompilingtheLTL
discreteexperimentaldomains.
specificationdowntoarewardfunctionthatcanbedefined
overanaugmentedformoftheenvironment,andtreatthis
1.Introduction asaconstraintterminouroptimizationproblem. Througha
Lagrange-stylerelaxation,wetransformourobjectiveinto
Significant research effort has been exerted in develop- an unconstrained form that can be readily optimized by
ingLinearTemporalLogic(LTL)asanalternativemeans DeepRL(DRL)methods. Weprovideatheoreticalargu-
of specifying objectives for reinforcement learning (RL) mentshowingthatapolicythatoptimizesourunconstrained
agents(Sadighetal.,2014;Hasanbeigetal.,2018;Cama- formulationwillsatisfytheoriginalconstrainedoptimiza-
choetal.,2019;Wangetal.,2020;Vaezipooretal.,2021; tionproblem,underasmallsetofassumptions.
Alur et al., 2022). LTL provides a flexible language for
Oursecondcontributionismotivatedbytheusageofour
definingobjectives,orspecifications,thatareoftennotre-
unconstrained objective in practice. Due to the sparsity
ducible to scalar Markovian rewards (Abel et al., 2021).
of the LTL specification’s compiled reward function, the
Unliketypicalrewardfunctions,objectivesdefinedinLTL
LTL specification is often ignored during policy learning
arecomposable,easilytransferredacrossenvironments,and
and can lead to unsatisfactory behavior. To address this,
offeraprecisenotionofsatisfaction.
weintroduceCycleExperienceReplay(CyclER).CyclER
LTLspecificationsandMarkovianrewardfunctionshave is a novel reward shaping method that exploits the struc-
tureoftheLTLspecificationtoencouragepartialbehavior
*Equal contribution 1UC Berkeley 2Latitude AI 3UT
compliantwiththespecificationwhilemaintainingtheopti-
Austin 4Penn State. Correspondence to: Ameesh Shah
<ameesh@berkeley.edu>. malityguaranteesinouroriginalobjective. Weshowinour
experimentsthatusingCyclERinconjunctionwithourun-
Preliminarywork.Underreview.
1
4202
rpA
71
]GL.sc[
1v87511.4042:viXraDeepPolicyOptimizationwithTemporalLogicConstraints
constrainedobjectivelearnspoliciesthatempiricallysatisfy successivetransitionsunderTB.
thetaskspecificationandachievehighreward,outperform-
Definition 2.1 (Acceptance of ξ). We accept a path ξ =
ingunshapedapproachesthatfailtosatisfythespecification
(b ,b ,...)ifanacceptingstateoftheBuchiautomatonis
0 1
altogether.
visitedinfinitelyoftenbyξ.
2.ProblemStatement 2.1.2.LABELEDMDPS
2.1.Preliminaries WeformulateourenvironmentasalabelledMarkovDeci-
sionProcessM = (S,A,TM,∆(s ),γ,r,LM),contain-
0
2.1.1.LINEARTEMPORALLOGIC(LTL)
ingastatespaceS,anactionspaceA,anunknowntransition
LinearTemporalLogic(Pnueli,1977)isaspecificationlan- function,TM :S×A→∆(S),adistributionoverinitial
guagethatcomposesatomicpropositionswithlogicaland states∆(s 0),adiscountfactor0 < γ < 1,arewardfunc-
temporal operators to precisely define tasks. An atomic tionr : S ×A → [R min,R max],andalabellingfunction
propositionisavariablethattakesonaBooleantruthvalue. LM :S →Σ. Thelabellingfunctionreturnswhichatomic
WedefineanalphabetΣasallpossiblecombinationsovera propositionsinoursetAParetrueforagivenMDPstate.
finitesetofatomicpropositions(AP);thatis,Σ=2AP. For
example,ifAP={a,b},thenΣ={{a,b},{b},{a},{}}. 2.2.ProblemFormulation
Wewillrefertoindividualcombinationsofatomicproposi-
Wewouldliketolearnapolicythatproducessatisfactory
tions,orpredicates,inΣasν. Weusethesymbolφtorefer
(accepting)trajectorieswithrespecttoagivenLTLformula
toanLTLtaskspecification,alsocalledanLTLformula.
φwhilemaximizingr,therewardfunctionfromtheMDP.
In LTL, specifications are constructed using both logical Beforewedefineourformalproblemstatement,weintro-
connectives: not(¬),and(&),andimplies(→);andtem- duceabitmorenotation:
poraloperators: next(X),repeatedly/always/globally(G),
Definition 2.2 (Product MDP). A product MDP syn-
eventually(F),anduntil(U). Formoredetailontheexact
chronizestheMDPwiththeLTLformula’scorresponding
semanticsofLTLoperators,see(Baier&Katoen,2008).
LDBA.Specifically,letMφ beanMDPwithstatespace
Asanexample, considerthe“FlatWorld”environmentin Sφ =S×B. PoliciesoverourproductMDPspacecanbe
Figure 1 (left), where AP = {r,g,b,y}, corresponding definedasΠ:Sφ×A→∆([0,1]),whereactionscombine
to whether the agent is in the red, green, blue, or yellow
A((s,b))=A(s)∪E,toincludethejumptransitionsinB
regionatanypointintime. LTLcaneasilydefinesomesim- aspossibleactions. Thenewprobabilistictransitionrelation
pleobjectives,suchassafetyG(¬b),reachabilityF(g),or ofourproductMDPisdefinedas:
progressF(y)&X(F(r)). Wecanalsocombineoperators
tobringtogethertheseobjectivesintomorecomplexspec-
ifications, such as G(F(r)&F(y)&F(g))&G(¬b), which
instructsanagenttooscillateamongstthered,yellow,and T(s′,b′|s,b,a)=
greenregionsindefinitelywhileavoidingtheblueregion.  TM(s,a,s′) a∈A(s),b′ =TB(b,L(s′))

In order to determine the logical satisfaction of an LTL 1 a∈E,b′ =TB(b,a),s=s′ (1)
specification,wecantransformitintoaspecializedautoma- 0
otherwise
toncalledaLimitDeterministicBuchiAutomaton(LDBA).
Pleasesee(Sickertetal.,2016;Hahnetal.,2013;Kˇret´ınsky`
etal.,2018)fordetailsonhowLTLspecificationscanbe ApolicythatoperatesinourproductMDPwillgeneratetra-
transformedintosemanticallyequivalentLDBA. jectories as sequences τ = ((s 0,b 0,a 0),(s 1,b 1,a 1),...).
WedenotebyR(τ) = (cid:80)∞ γtr(s ,a )theaggregatere-
More precisely, a (de-generalized) LDBA is a tuple B = t=0 t t
wardalongatrajectoryτ.
(B,Σ,TB,B∗,E,b ) with a set of states B, the alphabet
−1
Σthatdefinesdeterministictransitionsintheautomaton,a Definition2.3(Trajectoryacceptance). Atrajectoryissaid
transitionfunctionTB :B×(Σ∪E)→B,asetofaccepting tobeacceptingwithrespecttoφ(τ |=φ,or“φacceptsτ”)
states B∗, and an initial state b . An LDBA may have ifthereexistssomeb∈B∗thatisvisitedinfinitelyoften.
−1
separate deterministic and nondeterministic components Definition2.4(Policysatisfaction). Apolicyπ ∈Πsatis-
B the= nB TD B(∪ b,B xN ), ⊆suc Bh Dt .ha EtB is∗ a⊆ sB etD o, fan “d juf mor pb ”∈ acB tiD on, sx , a∈ lsΣ o fi He es reφ ,1w ii sth anso inm de icp ar too rb vab ari il ait by leP t[ hπ at|= chφ ec] k= swE hτ e∼ thM erφ π o[1 rτ n|= otφ a].
knownasepsilon-transitions,forb∈B N thattransitionsto trajectoryτ isacceptedbyφ,andMφrepresentsthedistri-
π
B D withoutevaluatinganyatomicpropositions. Apathξ = butionofpossibletrajectoriesfromoperatingapolicyπin
(b 0,b 1,...) is a sequence of states in B reached through aproductMDPMφ.
2DeepPolicyOptimizationwithTemporalLogicConstraints
Figure1: Fromlefttoright: (1)TheFlatWorldMDP,whereanagentisrewardedforvisitingthepurpleregions. (2)The
correspondingLDBAfortheLTLformulaφ=G(F(r)&F(y)&F(g))&G(¬b),whichspecifiesoscillatingamongstthered,
yellow,andgreenregionsindefinitelywhileavoidingtheblueregion(someedgesomittedforreadability). Theaccepting
state0iscodedingreen. (3)Top: AtrajectorygeneratedbyapolicytrainedtomaximizetheMDPrewardwhilesatisfying
φwithoutanyLTLrewardshaping. Bottom: AtrajectorygeneratedbyapolicytrainedtomaximizetheMDPrewardwhile
satisfyingφusingtheCyclERrewardshapingmethod.
Definition2.5(Probability-optimalpolicies). Wewillde- basedontheamountoftimebetweenvisitstoanaccepting
noteΠ∗asthesetofpoliciesthatmaximizetheprobability state. A policy that maximizes this eventual discounting
of satisfaction with respect to φ; that is, the policies that rewardisapproximatelyprobability-optimalwithrespectto
havethehighestprobabilityofproducinganacceptedtra- φwhenγisselectedproperly(seeTheorem4.2in(Voloshin
jectory: Π∗ ={π ∈Π|P[π |=φ]=sup P[π′ |=φ]}. etal.,2023)foranexactbound).
π′∈Π
Asaresultofeventualdiscounting,wecanreplaceΠ∗ in
Ouraimistofindthepolicyintheprobability-optimalset
Π∗thatcollectsthelargestexpectedcumulativediscounted objective2withthesetofpoliciesthatmaximizesV π. Let
V =sup V bethemaximalvalue.
reward. We state this constrained objective formally as max π∈Π π
follows:
π∗ =arg max R
π∗ ∈argmax π∈Π∗E τ∼Mφ π(cid:2) R(τ)(cid:3) (2) π∈{π∈Π|Vπ=Vmax} π (4)
WenowformtheLagrangiandualofobjective4:
Fornotationalconvenience,wewillrefertotheMDPvalue
functionasR π ≡E τ∼Mφ π(cid:2) R(τ)(cid:3) . π∗ =minargmaxR π+λ(V π−V max)
λ π∈Π
3.LTL-ConstrainedPolicyOptimization
Intheorem3.3weshowthatbecauseweonlycareabout
3.1.TransformingLTLConstraintsintoRewards constraint-maximizingpolicies, thereexistsλ∗ ∈ Rsuch
thatsolvingtheinnermaximizationofobjective3.1must
Finding a policy within the probability-optimal set Π∗ is
beconstraintoptimalforanyfixedλ>λ∗. Toseethis,the
challenging: anLTLconstraintφisdefinedoverinfinite-
higher λ is, the more our learned policy will account for
length trajectories but policy rollouts in practice produce
V during optimization until the constraint must be satis-
onlyfinite-lengthtrajectories. Weadopteventualdiscount- π
fied. At that point, because we are already achieving the
ing (Voloshin et al., 2023), a common approach in the
maximumpossibleV ,anyadditionalliftwillonlycome
π
existing literature which aims to optimize a proxy value
frommaximizingovertheMDPvalueR,evenifwecon-
functionthatapproximatesthesatisfactionofφ. Eventual
tinuetoincreaseλ. Withthisobservation,wecanforman
discountingisdefinedas:
unconstrainedobjectivefunctionfromobjective4tobethe
following:
(cid:20) ∞ (cid:21)
(cid:88)
V = E Γ r (b )
π t LTL t
τ∼Mφ π t=0 π∗ =argmaxR π+λV π (5)
(cid:40) (3) π∈Π
1 if(b ∈B∗)
Γ =γj, r (b )= t wherewehavedroppedthedependenceonV sinceitis
t LTL t max
0 otherwise aconstantandfixedλ>λ∗.
where j = (cid:80)t r (b ) counts how many times the In the sequel, we show that an exact value for λ∗ can be
k=0 LTL k
set B∗ has been visited (up to and including the current found to ensure that a policy that maximizes eq. 5 will
timestep). Notably,eventualdiscountingdoesnotdiscount certainlymaximizeV .
π
3DeepPolicyOptimizationwithTemporalLogicConstraints
3.2.Derivingλ∗forObjective5 4.RewardShapingforLTL-ConstrainedDRL
Assumption3.1. Thereexistsapositivenonzerogapbe- TodistinguishbetweentheMDP’srewardfunctionandthe
tween the expected reward V π of policies in π ∈ Π∗ and eventual-discountingproxyrewardin3,wewillwritethe
the highest-reward policies that are not; that is, V max − MDP reward function r(s,a) as r MDP(s,a). In Deep RL
sup π∈(Π\Π∗)(V π)=ϵwhereϵ>0. settings,onecantrainapolicytomaximizeobjective5using
the reward function r (s ,b ,a ) = γtr (s ,a )+
Remark3.2. Ifwearerestrictedtostationarypoliciesand DUAL t t t MDP t t
Γ λr (b ).
the spaceof policiesΠ is finite, thenassumption 3.1will t LTL t
alwayshold. Afinitespaceofpoliciescanbeenumerated Inpractice,optimizingourobjectivein5isoftenchallenging
over, and we can take the difference between the optimal duetothesparsityofr . Nonzerorewardisonlygiven
LTL
andnext-bestpoliciestofindϵ. byr onceanacceptingstateinBisvisited,whichmay
LTL
Theorem 3.3. Under assumption 3.1, for any choice of requireaprecisesequenceofactions.
λ > Rmax−Rmin, the solution to objective 5 must be a
ϵ(1−γ) ConsidertheFlatWorldMDPandBuchiautomatoninfig-
solutiontoobjective4.
ure1. Underr ,apolicywillreceivenorewarduntilit
LTL
completestheentiretaskofavoidingblueandvisitingthe
Proof. Considertwopolicies: (1)π ∈Π\Π∗,whichdoes red,yellow,andgreenregionsthroughrandomexploration.
notachieveV max,(2)π˜ ∈ Π∗,achievingV max. LetR max If r MDP is dense, a policy may fall into an unsatisfactory
and R min be upper and lower bounds on the maximum ‘local optimum’ by optimizing for r MDP it receives early
andminimumachievableRinM,respectively. Evaluating during learning, and ignore r entirely. In figure 1, we
LTL
objective5forbothofthesepoliciessatisfiesthefollowing seethatapolicytrainedonr fallsintoalocaloptimum,
DUAL
seriesofinequalities: whereitchoosestomaximizer byvisitingthenearby
MDP
purpleregionsandignoresφ.
(a) R
R +λV ≥ min +λ(V +ϵ)
π˜ π˜ 1−γ π Weseektoaddressthisshortcomingbyautomaticallyshap-
ingr sothatamoredenserewardsignalforφisavailable
(b) R LTL
≥ max +λV duringtraining. Below,wepresentourapproach,whichex-
1−γ π ploitstheknownstructureoftheBuchiautomatonBand
(c) cycleswithinBthatvisitacceptingstates.
≥ R +λV
π π
4.1.RewardingAcceptingCyclesinB
where(a)followsfromassumption3.1andboundingthe BydefinitionofLTLsatisfaction(def.2.3),atrajectorymust
worst-case MDP value, (b) follows from selecting λ > repeatedlyvisitanacceptingstateb∗inaBuchiautomaton.
Rmax−Rmin(≡λ∗),(c)followssincethehighestMDPvalue
Inthecontextoftheautomatonitself, thatmeansthatac-
ϵ(1−γ)
achievable by π must be upper bounded by the best-case ceptingtrajectorieswilltraverseacceptingcyclesofstates
MDPvalue. andtransitionswithinBthatloopacceptingstatesbackto
themselves.
Asaconsequenceof(a−c)weseethatpoliciesachieving
V arepreferredbyobjective5. Considerπ∗ ∈Π∗,the Definition4.1(AcceptingCycle). AnacceptingcycleinB
max
solutiontoobjective5. Thus,sinceπ∗ ∈Π∗,thenπ∗must isasetofvalidtransitions(b i,ν,b j)(i.e., thepredicateν
alsoachieveV
π∗
= V
π˜
= V max. Therefore,incomparing thattransitionsb itob j)inBthatstartandendatthesame
objective5forbothπ∗ andπ˜ itfollowsimmediatelythat acceptingstateb∗ ∈B∗.
k
R ≥ R sinceπ∗ isoptimalforobjective5. Sincethe
π∗ π˜ OurkeyinsightisthatwecanuseanacceptingcycleinBto
choice of π˜ is arbitrary, we have shown that π∗ is also a
shaper . Insteadofonlyprovidingarewardwheneveran
solutiontoobjective4. LTL
acceptingstateinBisvisited,weprovidearewardwhenever
an agent transitions to its next state within an accepting
EmpiricalConsiderations. Sincetheϵinassumption3.1
cycle. Inourexamplefromfig1(right),ifwerewardeach
isoftenunknown,theremaynotbeaverifiablewaytoknow
transitioninthecycle{1,2,3,0},theagentwouldreceive
thatthatourlearnedpolicyismaximizingV . Becauseof
π rewards for visiting the red region, then for visiting the
this,wewilltreatλasatunablehyperparameterthatallows
yellowregion,thengreen,thenforreturningtored,andso
a user to trade off the relative importance of empirically
on.
satisfyingtheLTLconstraint. Thereareanumberofstrate-
giesonecanusetofindanappropriateλ: forexample,one Multiple accepting cycles may exist in B. The cycle that
caniterativelyincreaseλuntilthechangeintheresulting isusedtoshaper cannotbepickedarbitrarily,sincea
LTL
learnedpolicyisminimal. cyclemaybeinfeasibleunderthedynamicsoftheMDP.For
4DeepPolicyOptimizationwithTemporalLogicConstraints
example, thecycle{1,2,0}infigure1cannoteffectively ensurethatrewardswillonlybegivenoncepertransition
shaper becauseitisimpossibletobebothintheyellow untilanacceptingstateisvisited.
LTL
andgreenregionsatthesametime.
NowwedescribetheCyclERalgorithm. Wefirstcollecta
Inthenextsection,weintroduceCycleExperienceReplay completetrajectoryτ inducedbyMφforagivenpolicyπ.
π
(CyclER), an approach that automatically selects a cycle Then,ateachtimesteptfrom0to|τ|−1,wecomputer
C
toshaper basedoncollectedexperienceandusesitto foreveryMACcinC:
LTL
updateapolicy.
r (b ,s ,b ,e,c)=
4.2.CycleExperienceReplay(CyclER) C t t+1 t+1

1/|c| if(b,LM(s ),b )∈c
CyclERisarewardfunctionthatcanbeusedasadirectal-  t+1 t+1
&e[b,LM(s ),b ]=0 (6)
ternativetor infunction3. BeforeweintroduceCyclER, t+1 t+1
wediscusstwLT oL
methodsnecessaryforitsimplementation.
0
otherwise
First,toprovideCyclERwithcyclesthatitcanusetoshape
reward,wefindallminimalacceptingcycles(MACs)inB: Thisfunctionrewardsthefirsttimeatransitioninagiven
MACistaken. Inotherwords,whenanagent“getscloser”
Definition4.2(MinimalAcceptingCycle(MAC)). Amin-
to an accepting state by progressing along the cycle, we
imalacceptingcyclecisanacceptingcycleforanaccept-
rewardthatprogress,butonlyoncepervisittoanaccepting
ingstateb∗ thatdoesnotcontainasubcycleforanynode
state. Notealsothatrewardsgivenarenormalizedbythe
b ∈(b ,ν,b )∈cwhereb ̸=b∗.
i i j i numberoftransitionsinitscorrespondingcycle,toaccount
forthefactthatdifferentcyclesmayhavedifferentlengths.
Finding all MACs in a Buchi automaton can be done us-
ingDepth-FirstSearchwithbacktracking;weprovidethis Ifatransitionvisitsanacceptingstateb∗,orifwereachthe
methodinAlgs.2and3inourappendix. Wewillreferto endofatrajectory,weretroactivelyassignrewardstothe
thesetofallMACsasC. timestepsthatprecededthispoint,uptothepreviousvisit
toanacceptingstate(ifoneexists). Therewardsweassign
Second,wewillmaintainafrontierofvisitedtransitionsin
correspond to the cycle offering the highest total reward
Bforeachtimestepinatrajectory. Wedefineourvisited
forthatpartofthetrajectory. Intuitively,CyclERpicksthe
frontierasabit-vectorewithsizeequaltothenumberof
‘best’ cycle for that partial trajectory and uses it to shape
transitions in B. For a trajectory, we compute e at every
reward. Even if a trajectory does not manage to visit an
timestep,updatingitbythefollowingrules: (1)ifatransi-
acceptingstate,CyclERwillstillproviderewardifitwas
tion(b ,ν,b )istaken,sete[b ,ν,b ]=1. (2)ifatransition
i j i j abletotakeanytransitionalonganyMAC.
(b ,ν,b )istakenwhereb ∈B∗,resetallbitsinetozero.
i j j
The CyclER reward shaping approach is presented in Al-
Tomotivatetheimportanceof
gorithm1. WedenotetherewardsreturnedfromAlg.1as
maintainingavisitedfrontier,
r . r canbeusedinplaceoftheunshapedr in
CyclER CyclER LTL
weshowviaexamplethatthe
function3toprovideamoredenserewardforτ.
existenceofnon-acceptingcy-
clesinBmayallowfortrajec- Theorem 4.1 (Informal). By replacing r LTL with r CyclER
in 3, the solution to problem 5 remains (approximately)
toriesthatinfinitelytaketran-
probabilityoptimalinsatisfyingtheLTLspecificationφ.
sitionsinaMACwithoutever
visitinganacceptingstate.
Proof. SeeappendixA,specificallyLemmaA.4
Consider the accepting cycle
{3,1,2}inthepartialautoma-
Inthenextsection,wewillshowhowCyclERcanleverage
toninfigure2. Althoughthis
quantitativesemanticsinanenvironmenttofurthershape
Figure2:ApartialBuchi
cycle is a MAC, there does
r heuristically.
automaton that necessi- LTL
existaseparatecyclestarting
tatesavisitedfrontier.
andendingatstate1(i.e. the
4.3.CyclERwithQuantitativeSemantics
cycle{1,2,0}.) Ifwegivere-
wardeverytimeatransitioninthecycle{3,1,2}istaken, Insomeenvironments,rewardshapingusingAlg.1maynot
apolicymaybeabletocollectinfiniterewardwithoutever beenoughtohelplearnφ-satisfyingbehavior. Forexample,
visiting an accepting state. For example, in figure 2, a if B has relatively few transitions, and those transitions
path{1,2,0,1,2,0...}wouldinfinitelytaketransitionsin are challenging under the dynamics of M, the CyclER-
aMAC,andthereforecollectinfiniterewardwithoutever shaped reward may remain sparse. To help combat this,
visiting the accepting state 3. Our visited frontier e will we leverage an often-used concept in temporal logic and
5DeepPolicyOptimizationwithTemporalLogicConstraints
introducequantitativesemanticstoeachatomicproposition Algorithm1CycleExperienceReplay(CyclER)
inAPtomoredenselyquantifythesatisfactionofatask. input Trajectoryτ,B∗,cyclesC
Anatomicpropositionx∈APissaidtohaveanassociated 1: initializeR C toamatrixofsize|C|×(|τ|−1);
quantitativesemantics(orrobustnessmeasure)ifthereexists 2: initializer CyclERtoanarrayofsize(|τ|−1);
afunctionρ :S →Rthatquantifieshowclosexistobe- 3: initializej =0;
x
ingsatisfiedatstates. Wesayxevaluatestotrueatagiven 4: fortupto|τ|−1do
stateiffρ x(s) ≥ 0. Forexample,inourFlatworldsetting 5: forCyclec i ∈C do
(fig.1),avalidquantitativesemanticsforeachpropositional 6: R C[i,t]=r C(b t,s t+1,b t+1,e t,c i)
variablewouldbethenegativedistancetoapropositional 7: endfor
variable’scorrespondingregion,andzeroinsidetheregion. 8: ifb t+1 ∈B∗ort+1=|τ|then
9: Selecti=argmax i∈|C|((cid:80)t j′=jR C[i,j′])
Thereisawell-definedsemanticsforallofLTLusingQS, 10: fort′fromj tot+1do
whichallowsustoevaluatethequantitativesatisfactionof 11: r CyclER[t′]=R C[i,t′]
anentireLTLformulausingthatformula’sQSfunctionρ .
φ 12: endfor
Forthefulldefinitionofthesesemantics,see(Fainekos& 13: j =t+1;
Pappas, 2009). However, using ρ as a reward function
φ 14: endif
itselfischallenging: maximizingthecumulativediscounted
15: endfor
sumofρ φdoesnotguaranteethattheresultingpolicywill output r ;
CyclER
satisfyφ.Further,evaluatingρ atagivenstatemayrequire
φ
knowingallfuturestatesintheremainderofthetrajectory
ifφcontainstemporaloperators(Aksarayetal.,2016).
itwillreceiveapositivereward.
Note that the issue of knowing future states does not ex-
istwhencomputingρvaluesoverindividualtransitionsin In our experiments, we show that leveraging quantitative
B. Apredicateofatomicpropositionsassociatedwithany semantics as a heuristic for additional reward shaping of
transitioninaBuchiautomatonwillnotcontaintemporal r LTLleadstoimprovementinempiricalperformance.
operators. Thismeansthatthequantitativesemanticsofthat
transitioncanbecomputedusingthecurrentstatealone. We 5.RelatedWork
willusethequantitativesemanticsassociatedwithindivid-
ualtransitionsinMACstoshaper . LTL-ConstrainedPolicyOptimization. Weareawareof
LTL
fewexistingworksthatexploreoptimizingapolicyunder
Specifically,wedefinethefollowingrewardfunctionfora
LTLconstraintswithaseparaterewardfunction. (Caietal.,
givencyclec. Wewillabusenotationslightlyandusec[b]
2021)and (Dingetal.,2014)explorecost-optimalpolicies
torefertothetransitionincwithparentnodeb:
underLTLconstraintsincontrolsettingswheretheMDP
r (s,b,s′,b′,e,c)=
qs transitionfunctionisknown,usinglinearprogrammingand

1/|c| if(b,LM(s ),b )∈c dynamicprogramming,respectively. Whenthedynamics
(ρ
(s′)−ρ (s))/|c|
if& be ∈[b,
cLM(t+
s
t1 +1),t+
b
t1
+1]=0 a mre ethu on dkn to ow sn o, lve(V tho il sos ph ri on be let mal. i, n2 d0 i2 sc2 r) ep tero sv taid tees spa an cee sxa bc yt
c[b] c[b]) assumingalowerboundontransitionprobabilitiesinM.In
0 o&& th( eeb
r[
w, bL
,
isLM eM(s (t s+ t+1) 1, )b ,t b+ t+1) 1]∈/ =c)
0
c toon sctr aa ls et tt oo dth ee es pe Rw Lor sk es tt, io nu gr s.approachisexplicitlydesigned
RLforLTLSatisfaction. Anumberofeffortshavetreated
(7)
LTLspecificationsasobjectivesinRLsettings.(Yangetal.,
2022)showthatPAC-learningguaranteesfortheLTLsatis-
InthecontextofAlg.1, therewardfunctiondefinedin7
factiontaskinRLisintractableingeneral,and(Aluretal.,
will replace r . Intuitively, we can interpret the reward
C 2022)furtherspecifiesthesettingsthatallowforPACguar-
functiondefinedin7asidenticaltor , withtheaddition
C antees. Approachesthatdoprovidesuchtheoreticalguar-
oftrackingquantitativeprogresstowardstakingatransition
antees on their learning algorithms require discrete state
inacycle. IfanagentisinaBuchistatebthatiswithina
spaces, and either make assumptions on the environment
cyclec ,butdoesnotsuccessfullytakethetransitionfrom
i dynamicsorintroducediscountingstrategiestofinitizethe
bwithinc ,arewardisgiventhatmeasuresthedifference
i horizon (Voloshin et al., 2022; Fu & Topcu, 2014; Wolff
in quantitative semantics for that transition between the
etal.,2012;Aluretal.,2023;Perezetal.,2023).
previous and current states in M. In other words, if the
agentgot‘closer’tosatisfyingthetransitions’spredicateν, Beyond PAC-learning guarantees, (Sadigh et al., 2014;
itwillhaveahigherρvalueinitsnewstates′thanins,and Bozkurtetal.,2020;Fu&Topcu,2014;Wolffetal.,2012)
6DeepPolicyOptimizationwithTemporalLogicConstraints
providesolutionapproacheswithoptimalityexistenceguar- FlatWorld The FlatWorld domain (1) is a two dimen-
anteesindiscretestatespaces. Incontinuousstatespaces, sionalworldwithcontinuousstateandactionspaces. The
(Toro Icarte et al., 2022; Icarte et al., 2022; Camacho agent (denoted by a green dot) starts at (-1, -1). The
et al., 2019; Cai et al., 2023) exploit reward machines as agent’sstate, denotedbyx, isupdatedbyanactionavia
automaton-basedstructuresforlearningsubclassesofLTL x′ =x+a/10wherex∈R2anda∈[0,1]2. InFlatWorld,
specifications. (Vaezipooretal.,2021)exploitsthecompo- thereexistsasetofrandomlygeneratedpurple‘bonusre-
sitionalsyntaxofLTLformulastolearnamulti-taskpolicy gions’,whichofferasmallrewardforeachbonusregionan
forfiniteLTL. (Jothimuruganetal.,2019;Hasanbeigetal., agentvisitsatagiventimestep. TheLTLtaskspecification
2020)proposehierarchicalapproachestotheLTLlearning forthisdomaintaskstheagentwithvisitingthered,green,
problemanddemonstratethemonfinitespecifications. Like andyellowregionscontinuously,whileavoidingtheblue
CyclER, (Kalagarlaetal.,2021;Lietal.,2017;Jothimu- region.
ruganetal.,2019)leveragequantitativesemanticstoshape
reward.
Previousworkshavealsoproposedexploitingthestructure Safety-Gymnasium We use the Point-Button environ-
ofBforLTLrewardshaping(Hasanbeigetal.,2020;Wang
mentfromtheSafety-Gymnasiumsuiteofenvironments(Ji
etal.,2020;Ouraetal.,2020);unlikeCyclER,thesemeth- etal.,2023). Inthisdomain,apointer-shapedrobotmust
odsrelyongeneralizedLDBAswithacceptanceconditions navigatetheenvironment,whichincludesfoursphericalbut-
thatrequirevisitingmultiplestatesinBandpotentiallyrea-
tonsthatcanbepressed,circularbonusregions,andcube-
soning over multiple transitions from a single state. As shaped‘gremlins’thatconstantlymoveinafixedcircular
a result, these approaches cannot easily leverage QS for path. Thebuttonsarerandomlyassignedlabelsonethrough
rewardshaping. fouratthebeginningoftraining. Thepointer-shapedrobot
hastwoactuatorsforaccelerationandsteering,andreceives
Constrained Policy Optimization. The broader con-
anobservationoflidarsensordatathatdetectsthepresence
strainedpolicyoptimizationworksmostlyrelatetothecon-
ofnearbyobjects. TheLTLtaskdescriptioninstructsthe
strainedMarkovDecisionProcess(CMDP)framework(Le
agenttooscillatebetweenpressingthebuttonslabeledone
et al., 2019; Achiam et al., 2017; Altman, 2021), which
andtwoindefinitely,whileavoidingmakingcontactwith
enforcepenaltiesoverexpectedconstraintviolationsrather
gremlins. TheMDPrewardfunctiongivesasmallrewardif
thanabsoluteconstraintviolations. Incontrast, ourwork
anagentisinanybonusregionatagiventimestep.
aimstosatisfyabsoluteconstraintsintheformofLTL.
6.2.ImplementationDetailsandBaselines
6.Experiments
WeusedDDQN(Hasseltetal.,2016)withexplorationin
Wedemonstrateexperimentalresultsinthreedomainswith
the discrete-action Gridworld environment, and entropy-
bothdiscreteandcontinuousstateandactionspacesonLTL
regularizedPPO(Schulmanetal.,2017)withaGaussian
tasksofvaryingcomplexity.
policy over the action space in the continuous-action do-
mains(FlatWorld,SafetyGym)asourpolicyclasses.
6.1.ExperimentalDomainsandTasks
Toevaluateourcontributions,wecompareagainstabase-
In the following, we describe the environments and task line policy trained using the LCER approach introduced
specificationsusedinourexperiments. Fortheadditional in(Voloshinetal.,2023). Additionally,wecompareagainst
detailsofoursetup,pleaseseeAppendix 2. theLCERbaselinepolicy,buttrainedonlyontheLTLre-
wardfunctionr ,inordertoobservetheperformanceofa
LTL
policythatdoesnotget‘distracted’byr duringtraining.
MDP
Our CyclER method also employs the experience replay
Gridworld We use a 10x10 gridworld domain with de-
techniquefrom(Voloshinetal.,2023)duringtraining.
terministicmovement,withcoloredtilescorrespondingto
walls(red),regionsofinterest(purple,yellow,blue),and ToevaluateCyclER’sabilitytoleverageQuantitativeSeman-
randomlyplaced‘penaltyareas’(green). Anagentobserves tics,wedefineabasicsetofquantitativesemanticsfunctions
its(x,y)positioninthegrid,andmaymoveinoneofthe forourFlatWorldandSafetyGymenvironments. Forthese
cardinal directions or stay put. The LTL task for this en- environments,weprovideforeachvariablex ∈ APaQS
vironmentinstructstheagenttovisitthepurple,blue,and function ρ defined as the negative distance between the
x
yellowtilesinthegridcontinuously. Therewardfunction agent and x. We selected our values for λ by iteratively
inthisenvironmentencouragestheagenttoavoidthegreen increasingλuntilallbaselinesshowedminimalchangein
penaltyareas;failingtodosoincursasmallnegativereward. performanceonr . Ourvaluesforλare50,100,and300,
LTL
Anexampleenvironmentisshowninfigure4. forGridworld,FlatWorldandSafety-Gym,respectively.
7DeepPolicyOptimizationwithTemporalLogicConstraints
Gridworld FlatWorld SafetyGymnasium
Figure3: Trainingcurvesshowingtheperformanceofeachbaselineontheunshapedr foreachenvironment.
DUAL
calforadeepRLagenttosatisfycomplexLTLtasksorLTL
tasksincomplexenvironments. Wealsoobservethatinthe
morecomplicatedSafety-Gymenvironment,thepresence
ofQSbecomesimperativetothesuccessofthetask. When
usingCyclERwithoutQSinthisdomain,thelearnedpolicy
isoftenonlyabletolearnpartofthebehavior(i.e.,press-
ingonlyoneofthetwobuttonsduringatrajectory),which
Figure4:ExamplevisualizationsoftheGridworld(left)and preventsitfromfurtheroptimizingr MDP.
SafetyGymnasium(right)environments.
Tofurtherelucidatethe resultsoftraining, wegenerate a
smallnumberoftrajectoriesfromthepolicyeveryktraining
Gridworld FlatWorld Safety-Gym steps, and record the average cumulative r for those
DUAL
r LTL r MDP r LTL r MDP r LTL r MDP trajectories. We present these training curves in figure 3.
CyclER-QS - - 0.56 6.1 0.42 23.41 Notethatthesecurvesshowperformanceontheunshaped
CyclER 1.08 -9.75 0.27 5.4 0.0 9.83
LTLrewardandnottheCyclER-shapedrewardfortheCy-
LCER 0.04 -4.11 0.0 17.1 0.0 100.3
LCER,nor 0.11 -21.27 0.13 1.9 0.0 7.38 clERmethods.Ineachcase,theLCERbaselineconverges
MDP
more quickly by focusing solely on the MDP reward. In
Table 1: Performance results for each reward shaping Safety-Gym, wherethedynamicsallowanagenttositin
method. r identifies the average number of visits to abonusregionfairlyeasilytocollectconstantreward,the
LTL
anacceptingstateinBachievedforatrajectoryfromπ,and policyperformsquitewellinabsolutetermsofr ,but
DUAL
r referstotheaverageMDPrewardcollectedduringa isstilloutperformedbyCyclER.
MDP
trajectory.
7.Conclusion
6.3.Results This paper proposes a novel framework for finding poli-
ciesincontinuousstate-actionspacesthatarebothreward-
We evaluate each method by recording both the average
maximal and probability-optimal with respect to an LTL
rewardcollectedforanagent(r )aswellastheaverage
MDP constraint. Weshowthat,byusingaproxyrewardfunction
number of times an agent visits an accepting state in B
toapproximatethesatisfactionofanLTLformula,thiscon-
duringatrajectory(r ). Wepresentourresultsintable1.
LTL strainedobjectivecanbetransformedintoanunconstrained
Werecordtheaverageperformanceofthebestmodelsat
objectivethatmaintainsguaranteesontheperformanceof
the end of training on 50 trajectory rollouts. Results are
theoptimalpolicy. WethenintroduceCyclER,anexperi-
averagedovertentrialsforGridworldandFlatWorld,and
ence replay technique that automatically shapes the LTL
fivetrialsforSafety-Gymnasium.
proxyreward,alleviatingasparsityissuethatoftenplagues
OurresultsdemonstrateCyclER’ssignificantimprovement LTL-drivenRLapproaches.
inperformanceinsatisfyingtheLTLtaskwhencomparedto
Therearenumerousdirectionsforfuturework.Forexample,
ourbaselinemethods. Ineverydomain,theLCERbaseline
the reward shaping idea behind CyclER can be extended
methodfellintoa‘localminimum’,optimizingfortheMDP
to other classes of logical specifcations, such as Reward
reward while completely ignoring the LTL task. Surpris-
Machines(ToroIcarteetal.,2022). Wearealsointerested
ingly,evenourLCER-no-MDPbaselinemethodfailedto
inapplicationsofCyclERtoreal-worldsafety-criticaltasks,
accomplishtheLTLtasksalmostentirely,eventhoughthere
suchasautonomy.
wasnopossiblilityof‘distraction’fromr . Thisimplies
MDP
thatevenifr isnotpresent,rewardshapingisstillcriti-
MDP
8DeepPolicyOptimizationwithTemporalLogicConstraints
Acknowledgments Baier, C.andKatoen, J.-P. Principlesofmodelchecking.
TheMITPress, Cambridge, Mass, 2008. ISBN978-0-
TheauthorswouldliketothankAdwaitGodbole,Federico
262-02649-9.
MoraandNiklasLaufferfortheirhelpfulfeedback. This
workwassupportedinpartbyanNDSEGFellowship(for
Bozkurt,A.K.,Wang,Y.,Zavlanos,M.M.,andPajic,M.
Shah),ONRAwardNo. N00014-20-1-2115,DARPAcon-
Control synthesis from linear temporal logic specifica-
tractFA8750-23-C-0080(ANSR),C3DTI,ToyotaandNis-
tionsusingmodel-freereinforcementlearning. In2020
san under the iCyPhy center, and by NSF grant 1545126
IEEEInternationalConferenceonRoboticsandAutoma-
(VeHICaL).
tion(ICRA),pp.10349–10355.IEEE,2020.
References Cai,M.,Xiao,S.,Li,Z.,andKan,Z. Optimalprobabilistic
motionplanningwithpotentialinfeasibleltlconstraints.
Abel, D., Dabney, W., Harutyunyan, A., Ho, M. K.,
IEEETransactionsonAutomaticControl,68(1):301–316,
Littman, M., Precup, D., and Singh, S. On
2021.
the expressivity of markov reward. In Ad-
vances in Neural Information Processing Sys-
Cai,M.,Aasi,E.,Belta,C.,andVasile,C.-I. Overcoming
tems, 2021. URL https://proceedings.
exploration: Deepreinforcementlearningforcontinuous
neurips.cc/paper/2021/file/
control in cluttered environments from temporal logic
4079016d940210b4ae9ae7d41c4a2065-Paper.
specifications. IEEERoboticsandAutomationLetters,8
pdf.
(4):2158–2165,2023.
Achiam,J.,Held,D.,Tamar,A.,andAbbeel,P. Constrained
Camacho, A., Toro Icarte, R., Klassen, T. Q., Valenzano,
policyoptimization. InInternationalconferenceonma-
R., and McIlraith, S. A. Ltl and beyond: Formal lan-
chinelearning,pp.22–31.PMLR,2017.
guages for reward function specification in reinforce-
mentlearning. InProceedingsoftheTwenty-EighthIn-
Aksaray, D., Jones, A., Kong, Z., Schwager, M., and ternational Joint Conference on Artificial Intelligence,
Belta, C. Q-learning for robust satisfaction of signal IJCAI-19, pp. 6065–6073. International Joint Confer-
temporal logic specifications. In 55th IEEE Confer-
encesonArtificialIntelligenceOrganization,72019. doi:
ence on Decision and Control, CDC 2016, Las Vegas, 10.24963/ijcai.2019/840. URLhttps://doi.org/
NV,USA,December12-14,2016,pp.6565–6570.IEEE, 10.24963/ijcai.2019/840.
2016. doi: 10.1109/CDC.2016.7799279. URLhttps:
//doi.org/10.1109/CDC.2016.7799279. Ding, X., Smith, S. L., Belta, C., and Rus, D. Optimal
controlofmarkovdecisionprocesseswithlineartempo-
Altman,E. ConstrainedMarkovdecisionprocesses. Rout- ral logic constraints. IEEE Transactions on Automatic
ledge,2021. Control,59(5):1244–1257,May2014. ISSN0018-9286,
1558-2523. doi: 10.1109/TAC.2014.2298143.
Alur,R.,Bansal,S.,Bastani,O.,andJothimurugan,K. A
frameworkfortransformingspecificationsinreinforce- Fainekos,G.E.andPappas,G.J. Robustnessoftemporal
mentlearning. InRaskin,J.,Chatterjee,K.,Doyen,L., logicspecificationsforcontinuous-timesignals. Theor.
andMajumdar,R.(eds.),PrinciplesofSystemsDesign Comput.Sci.,410(42):4262–4291,2009. doi: 10.1016/
-EssaysDedicatedtoThomasA.HenzingerontheOc- J.TCS.2009.06.021. URL https://doi.org/10.
casion of His 60th Birthday, volume 13660 of Lecture 1016/j.tcs.2009.06.021.
NotesinComputerScience,pp.604–624.Springer,2022.
doi: 10.1007/978-3-031-22337-2\ 29. URLhttps:// Fu, J. and Topcu, U. Probably approximately cor-
doi.org/10.1007/978-3-031-22337-2_29. rect MDP learning and control with temporal logic
constraints. In Fox, D., Kavraki, L. E., and Kur-
Alur,R.,Bastani,O.,Jothimurugan,K.,Perez,M.,Somenzi, niawati, H. (eds.), Robotics: Science and Systems
F., andTrivedi, A. Policysynthesisandreinforcement X, University of California, Berkeley, USA, July
learning for discounted LTL. In Enea, C. and Lal, A. 12-16, 2014, 2014. doi: 10.15607/RSS.2014.X.
(eds.),ComputerAidedVerification-35thInternational 039. URLhttp://www.roboticsproceedings.
Conference,CAV2023,Paris,France,July17-22,2023, org/rss10/p39.html.
Proceedings,PartI,volume13964ofLectureNotesin
ComputerScience,pp.415–435.Springer,2023. doi: 10. Hahn,E.M.,Li,G.,Schewe,S.,Turrini,A.,andZhang,L.
1007/978-3-031-37706-8\ 21. URL https://doi. Lazyprobabilisticmodelcheckingwithoutdeterminisa-
org/10.1007/978-3-031-37706-8_21. tion. arXivpreprintarXiv:1311.2928,2013.
9DeepPolicyOptimizationwithTemporalLogicConstraints
Hasanbeig, M., Abate, A., and Kroening, D. Logically- CoRR, abs/2310.12248, 2023. doi: 10.48550/ARXIV.
constrainedreinforcementlearning,2018. URLhttps: 2310.12248. URLhttps://doi.org/10.48550/
//arxiv.org/abs/1801.08099. arXiv.2310.12248.
Hasanbeig, M., Kroening, D., and Abate, A. Deep rein- Pnueli,A. Thetemporallogicofprograms. In18thAnnual
forcementlearningwithtemporallogics. InInternational Symposium on Foundations of Computer Science (sfcs
ConferenceonFormalModelingandAnalysisofTimed 1977),pp.46–57.ieee,1977.
Systems,pp.1–22.Springer,2020.
Sadigh,D.,Kim,E.S.,Coogan,S.,Sastry,S.S.,andSeshia,
Hasselt, H. v., Guez, A., and Silver, D. Deep reinforce- S. A. A learning based approach to control synthesis
mentlearningwithdoubleq-learning. InProceedingsof ofmarkovdecisionprocessesforlineartemporallogic
theThirtiethAAAIConferenceonArtificialIntelligence, specifications. In53rdIEEEConferenceonDecisionand
AAAI’16,pp.2094–2100.AAAIPress,2016. Control,pp.1091–1096.IEEE,2014.
Icarte,R.T.,Klassen,T.Q.,Valenzano,R.,andMcIlraith, Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and
S. A. Reward machines: Exploiting reward function Klimov, O. Proximal policy optimization algorithms.
structureinreinforcementlearning. JournalofArtificial CoRR,abs/1707.06347,2017. URLhttp://arxiv.
IntelligenceResearch,73:173–208,2022. org/abs/1707.06347.
Ji, J., Zhang, B., Zhou, J., Pan, X., Huang, W., Sun, R., Sickert, S., Esparza, J., Jaax, S., andKˇret´ınsky´, J. Limit-
Geng, Y., Zhong, Y., Dai, J., and Yang, Y. Safety- deterministic bu¨chi automata for linear temporal logic.
gymnasium:Aunifiedsafereinforcementlearningbench- InChaudhuri,S.andFarzan,A.(eds.),ComputerAided
mark. arXivpreprintarXiv:2310.12567,2023. Verification,pp.312–332,Cham,2016.SpringerInterna-
tionalPublishing. ISBN978-3-319-41540-6.
Jothimurugan,K.,Alur,R.,andBastani,O. Acomposable
specificationlanguageforreinforcementlearningtasks. ToroIcarte,R.,Klassen,T.Q.,Valenzano,R.,andMcIlraith,
AdvancesinNeuralInformationProcessingSystems,32, S. A. Reward machines: Exploiting reward function
2019. structure in reinforcement learning. J. Artif. Int. Res.,
73, may 2022. ISSN 1076-9757. doi: 10.1613/jair.1.
Kalagarla,K.C.,Jain,R.,andNuzzo,P. Cost-optimalcon- 12440. URLhttps://doi.org/10.1613/jair.
trolofmarkovdecisionprocessesundersignaltemporal 1.12440.
logicconstraints. In2021SeventhIndianControlConfer-
ence(ICC),pp.317–322,2021. doi: 10.1109/ICC54714. Vaezipoor,P.,Li,A.C.,Icarte,R.T.,andMcIlraith,S.A.
2021.9703164. Ltl2action: GeneralizingLTLinstructionsformulti-task
RL. InProceedingsofthe38thInternationalConference
Kˇret´ınsky`, J., Meggendorfer, T., and Sickert, S. Owl: a onMachineLearning,ICML,volume139ofProceedings
libraryforω-words,automata,andltl. InInternational ofMachineLearningResearch,pp.10497–10508,2021.
SymposiumonAutomatedTechnologyforVerificationand URLhttp://proceedings.mlr.press/v139/
Analysis,pp.543–550.Springer,2018. vaezipoor21a.html.
Le,H.,Voloshin,C.,andYue,Y. Batchpolicylearningun- Voloshin, C., Le, H. M., Chaudhuri, S., and Yue, Y. Pol-
derconstraints. InInternationalConferenceonMachine icyoptimizationwithlineartemporallogicconstraints.
Learning,pp.3703–3712.PMLR,2019. In Oh, A. H., Agarwal, A., Belgrave, D., and Cho,
K. (eds.), Advances in Neural Information Processing
Li,X.,Vasile,C.-I.,andBelta,C. Reinforcementlearning
Systems,2022. URLhttps://openreview.net/
with temporal logic rewards. In 2017 IEEE/RSJ Inter-
forum?id=yZcPRIZEwOG.
nationalConferenceonIntelligentRobotsandSystems
(IROS),pp.3834–3839.IEEE,2017. Voloshin,C.,Verma,A.,andYue,Y. Eventualdiscounting
temporallogiccounterfactualexperiencereplay,2023.
Oura, R., Sakakibara, A., and Ushio, T. Reinforcement
learningofcontrolpolicyforlineartemporallogicspeci- Wang, C., Li, Y., Smith, S. L., and Liu, J. Continuous
ficationsusinglimit-deterministicgeneralizedbu¨chiau- motionplanningwithtemporallogicspecificationsusing
tomata. IEEEControl.Syst.Lett.,4(3):761–766,2020. deepneuralnetworks. arXivpreprintarXiv:2004.02610,
doi: 10.1109/LCSYS.2020.2980552. URL https: 2020.
//doi.org/10.1109/LCSYS.2020.2980552.
Wolff,E.M.,Topcu,U.,andMurray,R.M. Robustcontrol
Perez, M., Somenzi, F., and Trivedi, A. A PAC learning of uncertain markov decision processes with temporal
algorithmforLTLandomega-regularobjectivesinmdps. logicspecifications. In2012IEEE51stIEEEConference
10DeepPolicyOptimizationwithTemporalLogicConstraints
onDecisionandControl(CDC),pp.3372–3379,2012.
doi: 10.1109/CDC.2012.6426174.
Yang, C., Littman, M. L., and Carbin, M. On the
(in)tractability of reinforcement learning for ltl objec-
tives. In Raedt, L. D. (ed.), Proceedings of the Thirty-
First International Joint Conference on Artificial Intel-
ligence, IJCAI-22, pp. 3650–3658. International Joint
Conferences on Artificial Intelligence Organization, 7
2022. doi: 10.24963/ijcai.2022/507. URL https:
//doi.org/10.24963/ijcai.2022/507. Main
Track.
11DeepPolicyOptimizationwithTemporalLogicConstraints
A.ProofforTheorem4.1
Westartwithsomenotation. Letrτ representtherewardfunctionforatrajectoryτ thatarereturnedbytheexecutionof
CyclER
Alg.1. LetAτ bethesetoftimestepswhenanacceptingstateinBisvisitedforatrajectory. Let{t ...t }refertothe
π 0 n
timestepsforatrajectoryτ oflengthn+1. WewritethevaluefunctionforCyclER,lettingΓ bethesamefunctionas
t
definedinfunction3:
(cid:20) (cid:12) (cid:21)
AssumptionA.1. SupposeA max =sup π∈ΠE τ∼MP
π
Aτ(cid:12) (cid:12) (cid:12)τ ̸|=φ <M,thereisauniformboundonthelasttimeabad
(non-accepting)trajectoryvisitsanacceptingstateacrossallbadtrajectoriesinducedbyanypolicy.
AssumptionA.2. Supposep =inf P[π |=φ]>0.
min π∈Π
LemmaA.3. UnderAssumptionA.1andAssumptionA.2,foranyπ ∈Πandϵ>0wehave
|(1−γ)Vcyc−P[π |=φ]|≤ϵ
π
whenγ ≥max{1− ϵ ,(1−ϵ)(Amax+1)}ischosenappropriately.
pmin
Proof. WefollowtheproofstyleofLemma4.1from(Voloshinetal.,2023). LetP[π |=φ]=pbetheprobabilitythatπ
satisfiestheLTLspecificationφ. Recallthevaluefunction
(cid:20) ∞ (cid:21)
(cid:88)
Vcyc = E Γ rτ [t]
π t CyclER
τ∼Mφ
π t=0
Itisclearthatrτ [t]≤1soVcyc ≤ 1 .
CyclER π 1−γ
Everytrajectoryτ isdecomposableinto(1)thepartialtrajectoryuptothefirstvisitofanacceptingstateinB(denotedby
thetimestepAτ,whichisarandomvariable),thepartialtrajectorybetweenthefirstandlastvisitofanacceptingstatein
0
B(denotedbythetimestepAτ,whichisarandomvariable),and(3)theremainderofthetrajectory. Fortrajectoriesthat
1
satisfytheLTLspecification,Aτ =∞,otherwiseAτ <∞,finite. Bylinearityofexpectation,wecanrewriteourprevious
1 1
equationas:
(cid:20) (cid:88)Aτ 0 (cid:21) (cid:20) (cid:88)Aτ 1 (cid:21) (cid:20) (cid:88)∞ (cid:21)
Vcyc = E Γ rτ [t] + E Γ rτ [t] + E Γ rτ [t]
π t CyclER t CyclER t CyclER
τ∼Mφ
π t=0
τ∼Mφ
π t=Aτ+1
τ∼Mφ
π t=Aτ+1
0 1
ConsideringtrajectoriesthatsatisfytheLTLformulaφ,thenAτ =∞:
1
(cid:20) ∞ (cid:12) (cid:21)
V πcyc ≥ τ∼E
Mφ
π
t=(cid:88) Aτ+1Γ tr Cτ yclER[t](cid:12) (cid:12) (cid:12)τ |=φ P[π |=φ]= 1p −γ
γ
0
wherewehavedroppedthefirstterm. Ontheotherhand,bythelawoftotalexpectation,foralowerboundwehave:
(cid:20) ∞ (cid:12) (cid:21) (cid:20) ∞ (cid:12) (cid:21)
V πcyc =p τ∼E
Mφ
π
(cid:88) t=0Γ tr Cτ yclER[t](cid:12) (cid:12) (cid:12)τ |=φ +(1−p) τ∼E
Mφ
π
(cid:88) t=0Γ tr Cτ yclER[t](cid:12) (cid:12) (cid:12)τ ̸|=φ
1 1−γAmax+1
≤p +(1−p)
1−γ 1−γ
wherethefirsttermcomesfromtheupperboundonVcyc ≤ 1 andthesecondtermcomesfromboundingAτ witha
π 1−γ 0
uniformupperboundA byAssumptionA.1
max
Combiningtheupperandlowerboundtogetherandsubtractingoffpfrombothsidesandsetting(1−p)≤1,wehave
p(γ−1)≤(1−γ)Vcyc−p≤1−γAmax+1
π
Selectγ ≥max{1− pmϵ in,(1−ϵ)(Amax+1)}whichimpliesthat−ϵ≤p(γ−1)≤(1−γ)V πcyc−p≤1−γAmax+1 <ϵby
assumptionA.2. Hence,
|(1−γ)Vcyc−p|≤ϵ
π
12DeepPolicyOptimizationwithTemporalLogicConstraints
LemmaA.4. Letp∗ =sup P[π |=φ]. UnderAssumptionA.1andAssumptionA.2,thenanypolicyπoptimizingVcyc
π∈Π π
(ie. achievingVcyc }maintains|Vcyc−p∗|≤ϵ.
max π
Proof. ThisfollowsbyanidenticalargumentasinTheorem4.2in(Voloshinetal.,2023),byusingLemmaA.3: Vcyc-
optimizingpolicyπ∗ mustsatisfy|(1−γ)Vcyc −p∗|≤ϵwhenγ isselectedasinLemmaA.3.
cyc max
B.AdditionalAlgorithmicDetails
B.1.FindingMinimumAcceptingCycles
Inalgorithms2and3,weincludethepsuedocodeforfindingminimalacceptingcyclesinagivenB,whichconstitutesthe
setofcyclesC forusageinalgorithm1.
Algorithm2FindMinimalAcceptingCycles(FindMACs)
input BuchiAutomatonB,acceptingstatessetB∗
1: InitializeC toanemptyset;
2: foracceptingstateb∗ ∈B∗do
3: Initializevisitedtoanemptyset;
4: InitializeC toanemptyset;
5: DFS(b∗,{});
6: AddC toC;
7: endfor
output C
Algorithm3DFS(HelperforAlg.2)
input Startingnodeb,Pathp
1: Addnodebtovisited;
2: forOutgoingtransitions(b,ν,b′)frombdo
3: ifb′ =b∗then
4: Addthetransition(b,ν,b′)top;
5: AddptoC;
6: else
7: ifb′ ∈/ B∗andb′ ∈/ visitedthen
8: Addthetransition(b,ν,b′)top;
9: DFS(b′,p)
10: endif
11: endif
12: endfor
13: Removenodebfromvisited;
output C
C.AdditionalExperimentalDetails
For the Minecraft and Flatworld experiments, results are averaged over 10 random seeds. For the Safety-Gymnasium
experiment,resultsareaveragedoverfiverandomseeds. Foreachrandomseed,thelocationsofthefollowingobjectswere
randomizedandfixedforthedurationoftraining: inMinecraft,locationofthepenaltyareas,inFlatWorld,thelocationof
thebonusareas,andinSafety-Gymnasium,thelocationsofthebuttons,bonusareas,andgremlins.
WeprovideadditionalhyperparameterchoicesforeachexperimentinTable2. ForMinecraft,whichusesDQN,thebatch
size refers to the number of transitions, whereas for FlatWorld and Safety-Gym, the batch size refers to a number of
trajectories. TheQ-networkinMinecraftisathreelayerneuralnetworkwith64hiddenunitsandReLUactivations. Our
13DeepPolicyOptimizationwithTemporalLogicConstraints
LTLφ λ CriticLR ActorLR α Updatefreq. γ Batchsize Max|τ|
Minecraft G(F(b)&F(p)&F(y)) 50 0.001 - - 1 0.99 1024 100
FlatWorld G(F(r)&F(y)&F(g))&G(¬b) 100 0.01 0.001 0.3 1 0.99 128 60
Safety-Gym G(F(b1)&F(b2))&G(¬g) 300 0.01 0.002 0.2 5 0.99 256 600
Table2: Datasetdetails.
DDQNimplementationalsousesanexponentiallydecayingexplorationterm,thatstartsatϵ=0.6anddecaysevery25
episodesbyarateof0.95. ForourPPOimplementation,weusea3-layer,64-hiddenunitnetworkastheactorusingReLU
activations,withthesamearchitectureforthecritic. TheactoroutputsthemeanofaGaussian,thevarianceforwhichis
learnedbya3-layer,64-hiddenunitnetworkthatsharesthefirst2layerswiththeactorpolicyitself. Allexperimentswere
doneonanIntelCorei9processorwith10coresequippedwithanNVIDIARTXA4500GPU.WeusetheAdamoptimizer
inallexperiments.
Infigure3,rewardwascomputedbyevaluatingthepolicyeverytentrajectoriesinthecaseofMinecraftandFlatworld,and
every25trajectoriesforSafety-Gym. Ther valuesshownarefromaveragingperformanceoverfiverolloutsforeach
DUAL
datapoint. TheobservationspaceandenvironmentusedinourSafety-Gymexperimentsarethedefaultspacesprovidedthe
ButtonLevel1environmentin(Jietal.,2023),withthefollowingchanges: thelocationsofobjectsintheenvironmentdo
notchangeinbetweentrajectoryrolloutsforagivenrandomtrial,weincludetwogremlinsandsixbonusregions,andwe
placefourstaticcollidablewallsasboundariestoenclosetheagent’senvironmentsattheborderofwhereobjectscanbe
randomlyplaced.
14