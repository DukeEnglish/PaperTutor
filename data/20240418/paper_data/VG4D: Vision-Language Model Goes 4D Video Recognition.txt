VG4D: Vision-Language Model Goes 4D Video Recognition
Zhichao Deng1, Xiangtai Li2, Xia Li3, Yunhai Tong2, Shen Zhao1∗, and Mengyuan Liu4∗
Abstract—Understandingtherealworldthroughpointcloud
video is a crucial aspect of robotics and autonomous driving 4D Label:make victory sign nod head/bow
Embedding FC logits
systems. However, prevailing methods for 4D point cloud Encoder
recognition have limitations due to sensor resolution, which
leads to a lack of detailed information. Recent advances have (a)General Pipeline in Existing Methods
shown that Vision-Language Models (VLM) pre-trained on
web-scale text-image datasets can learn fine-grained visual
concepts that can be transferred to various downstream tasks.
Hand waving
However, effectively integrating VLM into the domain of 4D Reading
point clouds remains an unresolved problem. In this work, … Class:make OK sign vomiting
Clapping
we propose the Vision-Language Models Goes 4D (VG4D)
framework to transfer VLM knowledge from visual-text pre-
trained models to a 4D point cloud network. Our approach im-PSTNet ❄ VLM
involves aligning the 4D encoder’s representation with a VLM
Embedding Embedding Embedding
to learn a shared visual and text space from training on
large-scale image-text pairs. By transferring the knowledge
of the VLM to the 4D encoder and combining the VLM, ❄Frozen logits Dot
our VG4D achieves improved recognition performance. To Class:make victory sign nod head/bow
enhancethe4Dencoder,wemodernizetheclassicdynamicpoint (b)VLM Goes 4D (Ours) (c) Hard cases
cloud backbone and propose an improved version of PSTNet,
im-PSTNet, which can efficiently model point cloud videos. Fig.1. (a)GeneralPipelineinExistingMethods:Inputpointcloudvideo
is processed by a 4D encoder and then a standard classifier to generate
Experiments demonstrate that our method achieves state-of-
prediction scores. (b) Our proposed method harnesses the knowledge of a
the-art performance for action recognition on both the NTU
Visual-Language pre-trained model to enhance action recognition perfor-
RGB+D 60 dataset and the NTU RGB+D 120 dataset. Code is
mance.(c)SomeclassificationhardcasesofpointcloudandRGB.
available at https://github.com/Shark0-0/VG4D.
I. INTRODUCTION These methods either aim to design improved networks for
Recently, robotics and autonomous driving systems have modeling4Dpointcloudsoremployself-supervisedmethods
used real-time depth sensors such as LiDARs to achieve to enhance the efficiency of 4D point cloud representations.
3D perception [1]–[4]. Point clouds from LiDARs can pro- Nonetheless, the recognition of 4D videos remains a
vide rich geometric information and facilitate the machine’s challengeformachines,primarilyduetotheinherentcharac-
comprehension of environmental perception. Early meth- teristics of point cloud data, which lack texture information.
ods [5]–[8] focus on parsing the real world from static point Owing to the lower resolution of LiDARs, point clouds
clouds, neglecting temporal changes. To better understand may lack some details, resulting in the loss of fine-grained
the time-varying world, recent research focuses more on information. The failure recognition case of the traditional
understandingpointcloudvideosin4D,encompassingthree 4D point cloud network is shown on the right side of Fig.1.
spatial dimensions and one temporal dimension. Several We find that recognition failures of 4D point clouds occur
works [9], [10] have been done in 4D point cloud modeling. duetothesmalldifferencesbetweenactions.Whenaperson
executes a specific action with limbs, the limb engaged and
*Correspondingauthor:MengyuanLiuandShenZhao.
1 Zhichao Deng and Shen Zhao are with the School of Intelli- the object being manipulated are vital in distinguishing the
gent Systems Engineering, Sun Yat-sen University, Shenzhen, China action. For example, when differentiating between actions
dengzhch3@mail2.sysu.edu.cn, z-s-06@163.com)
such as “making a victory sign” and “making an OK sign,”
2 Xiangtai Li and Yunhai Tong are with the National Key
the texture characteristics of the hand become critical for
Laboratory of General Artificial Intelligence, Peking University, and
Tong is also with PKU-Wuhan Institute for Artificial Intelligence. precise recognition of the respective actions. On the other
lxtpku@pku.edu.cn hand,imagesfromtheRGBmodalitylackdepthinformation.
3 Xia Li is with the Department of Computer Science, ETH Zurich.
When a person in the video moves deeply, changes in depth
xia.li@inf.ethz.ch
4 Mengyuan Liu is with the National Key Laboratory of Gen- becomechallengingtodiscern,leadingtoconfusionbetween
eral Artificial Intelligence, Peking University, Shenzhen Graduate School actions involving depth movement, such as “nod bow” and
liumengyuan@pku.edu.cn
otherslike“vomiting.”Inconclusion,pointcloudstypically
ThisworkwassupportedbytheNationalNaturalScienceFoundationof
struggle to provide fine-grained information effectively,
China (No. 62203476), the Natural Science Foundation of Shenzhen (No.
JCYJ20230807120801002), the National Key Research and Development and it is difficult for RGB to provide depth information.
Program of China (No. 2023YFC3807600), the interdisciplinary doctoral
Conversely, the rich texture of RGB inherently pro-
grants (iDoc 2021-360) from the Personalized Health and Related Tech-
nologies(PHRT)oftheETHdomain,Switzerland. vides an abundance of fine-grained information, whereas
4202
rpA
71
]VC.sc[
1v50611.4042:viXrathe presence of depth information is inherent to the ing point cloud video understanding abilities of the 4D
point cloud modality. Recently, Vision-Language Models network to enhance multi-modal action recognition.
(VLM), pre-trained on large-scale image-text pairs, were • Our proposed VG4D significantly outperforms the re-
adept at learning fine-grained visual concepts from RGB cent state-of-the-art methods on large-scale RGB+D
images.Someresearches[11]–[15]havesuccessfullyapplied action recognition datasets.
the knowledge gleaned from VLMs to 3D static point cloud
II. RELATEDWORK
and video understanding. However, due to the modality gap,
RGB video and 4D point cloud understanding are treated StaticPointCloudProcessing.Muchprogress[18]hasbeen
as two distinct problems. The RGB video and 4D point made in the field of static point cloud analysis. Point-based
cloudmodelsareconstructedwithdisparatearchitecturesand methodsdirectlyprocessrawpointcloudsasinput.Theyare
trained on varying data types. Currently, the utilization of pioneered by PointNet [19], which models the permutation
RGB video models to recognize 4D point clouds has not invarianceofpointswithsharedMLPsbypoint-wisedfeature
been extensively explored. extraction. PointNet++ [6] enhances PointNet by capturing
Motivated by VLM for point processing and video localgeometricstructures.Lateron,point-basedmethods[8],
understanding [16], [17], we make the first study of VLM [20], [21] aim at designing local aggregation operators for
in 4D action understanding within this work. Specifically, local feature extraction. Recently, several methods have
we propose a new VG4D (VLM Goes 4D) framework that leveraged Transformer-like networks [22], [23] to extract
trains a 4D encoder by leveraging VLM’s knowledge. To information via self-attention. Meanwhile, self-supervised
jointlyfine-tunethis4Dencoder,weputforthacross-modal representationlearningmethods[24]–[27]suchascontrastive
contrastive learning approach, which facilitates harmonious learning and reconstruction masks have attracted significant
alignment of the 3D representation with the VLM feature interestfromthecommunity.However,thesemethodsmainly
domain.Inthismanner,thedeficiencyinfine-graineddetails focus on static point clouds and cannot directly process
within the point cloud modality is effectively compensated dynamic point cloud video due to the lack of the temporal
by rich content information in the RGB modality. Building dynamics of point clouds into account.
on the foundation of VG4D, we synergize the exceptional 4D Point Cloud Modeling. Dynamic 4D point cloud mod-
capabilities of Vision-Language Models (VLMs) in video eling is more challenging than static point cloud processing.
understanding with 4D point cloud representation to en- Previous point cloud video recognition methods rely on
hancemulti-modalactionrecognition.Specifically,asshown convolutional, multi-stream, or vision transformer. Within a
in Fig. 1(b), after aligning across multiple modalities, we convolutionalframework,MeteorNet[28]isthefirstmethod
achieve robust multi-modal action recognition by integrating on deep learning for dynamic raw 4D point cloud, which
multi-modal prediction scores and utilizing text information extends3Dpointsto4Dpointsandthenappendsatemporal
as classifiers. In addition, most VLMs are open source and dimension to PointNet++ to process these 4D points. Mean-
easy to obtain, and we apply different VLMs integrated into while,PSTNet[29],[30]modelsspatio-temporalinformation
our proposed VG4D framework, demonstrating the general- of raw point cloud videos via decomposing space and time
ization of our proposed method. hierarchically. For multi-stream design, the representative
Moreover, we revisit PSTNet, the classical and widely method 3DV [31] integrates 3D motion information into a
used network in dynamic point cloud modeling. We observe regular compact voxel set and then applies PointNet++ to
that a significant portion of the performance improvement extractrepresentationsfromthesetfor3Dactionrecognition.
achieved by the state-of-the-art approach compared to PST- Some other methods [32]–[37] decouple spatial and tem-
Net can be attributed to the enhanced training strategy em- poral information. Recently, P4Transformer [38] and other
ployed. Through comprehensive experimentation with these methods[39],[40]adoptself-attentionmoduletocapturethe
enhancements, we identified pivotal elements contributing long-rangespatial-temporalcontextinformation.Inaddition,
to variations in performance outcomes. Subsequently, we recent methods [41], [42] start exploring the application of
employed these identified components to revitalize the ar- self-supervised or semi-supervised learning in dynamic 4D
chitecture of PSTNet. As a result, we propose the improved pointcloudmodeling.However,owingtotheabsenceoffine-
version of PSTNet dubbed im-PSTNet, and use it as the 4D grained appearance input, the aforementioned methods still
encoder for the VG4D framework. Our contributions can be encounter difficulties in recognition.
summarized as follows: Vision-Language Models. Large vision-language models,
• We propose a novel VG4D framework that aligns 4D comprising an image and a text encoder, are trained on
featurespacewithlanguage-imagerepresentation,facil- extensive image-text pairs in a contrastive manner to learn a
itatingpointcloudvideorecognition.Toourknowledge, shared feature space between images and textual labels. For
we are the first to explore transferring VLM’s pre- example, CLIP is transferable to a variety of downstream
trained knowledge to 4D point cloud understanding. tasks, including point cloud understanding [11], [12], video
• We design a 4D encoder, named im-PSTNet, as the en- understanding [43]–[45], etc. Recently, several studies have
coder of point cloud video, which achieves an effective extended the existing CLIP model to the video domain.
pointcloudvideorepresentation.Wecombinetherobust AIM[44]reusestheCLIPself-attentionasthetemporalones
characterizationcapabilitiesofVLMswiththeoutstand- via an additional adapter module. Vita-CLIP [46] proposedClassification
im-PSTNet head 𝑆𝑝𝑐 𝐿𝑝𝑐 …
Supervised by
Projection Ground Truth
Frame sampling
𝑆𝑝𝑐−𝑡 𝐿(𝑝𝑐,𝑡𝑒𝑥𝑡)
1 Supervised by … …
Ground Truth
2 𝐈𝟏.𝐏𝟏
Hand waving 3 𝐈𝟐.𝐏𝟐 Hand waving Point sampling Point sampling
R …eading Text Encoder 4 𝐈𝟑.𝐏𝟑
𝐈𝟒.𝐏𝟒
𝐿(𝑃,𝑅) Fusion Cross-Grouping Spatial Extractor Grouping
Clapping ❄ 5 … Supervised by MLP MLP
… 𝐈𝐍.𝐏𝐍positive samples Maxpooling im-PSTConvs Maxpooling
K
VLM
Projection
Stop-grad
𝑆𝑟−𝑡 SuP pere rv-t ir sa ei dn e bd
y
𝑝 𝑙(𝑡− 𝑝1 𝑘() 𝑡= )=(𝑥 (𝑙𝑡 𝑥− 𝑘𝑡1 ,, 𝑓𝑓 𝑘𝑡𝑙𝑡 )−1)
(𝑓𝑖𝑡, 𝑥𝑘𝑡 …-𝑥𝑖𝑡, 𝑓𝑘𝑡) MLP
Ground Truth ℎ
❄EV ncid oe do er Clas hs eif aic dation 𝑆𝑟𝑔𝑏 𝐿𝑟𝑔𝑏 𝑝 𝑖(𝑡)=(𝑥𝑖𝑡,𝑓𝑖𝑡) (𝑓𝑖𝑡, 𝑥𝑙𝑡−1- …𝑥𝑖𝑡, 𝑓𝑘𝑡−1) MLP xaM 𝑖(𝑝 )𝑡
…
Learnable ❄Frozen S Score Dot Only train 𝑝 𝑗(𝑡+1)=(𝑥𝑗𝑡+1,𝑓𝑗𝑡+1) (𝑓𝑖𝑡, 𝑥𝑗𝑡+1-𝑥𝑖𝑡, 𝑓𝑗𝑡+1) MLP
(a)VG4D (b)im-PSTNet
Fig.2. Overallarchitectureofourframework.(a)VG4D(VLMgoes4D).Weuseacross-modalcontrastivelearningobjectivetotrainourproposed4D
encoder:im-PSTNet.TheknowledgeoftheVLMistransferredtothe4Dencoderbyaligningthe4DrepresentationwithlanguageandRGB,respectively.
Duringtesting,anensembleapproachisusedtointegratemultiplescores.(b)Theoverallframeworkofourproposedim-PSTNet.Itconsistsofaspatial
featureextractorandaspatio-temporalfeatureextractor.
a multi-modal prompt learning scheme to balance the super- 4D encoder takes in a point cloud video as the input. The
vised and zero-shot performance. However, unlike the video outputofthe4Dencoderisafeaturevectorthatencapsulates
domain, 3D point cloud processing with VLMs is still in its motiondetails.Tomaintaincongruenceinfeaturedimensions
infancy. PointCLIP [47] directly uses the depth maps of 3D withothermodalities,weintroducedanadditionalprojection
point clouds as the input of CLIP to perform zero-shot clas- layer for the 4D point cloud output. Given a 4D point
sification. ULIP [16] learns a unified representation among cloud xP, we feed the point cloud video xP into the 4D
i i
image,text,andpointcloudthatenhances3Drepresentation encoderE toobtainthefeaturerepresentationinacommon
P
learning.OurmethodisdifferentfromPointCLIPandULIP. embedding space by projecting the encoded feature to a
In particular, we aim to use the fine-grained features learned common dimension represented by:
by VLM to improve 4D point cloud recognition, which can
fP=ψ (E (xP)) (1)
compensate for the shortcomings of missing details in 3D i P P i
point clouds. where fP ∈RCP represent the 4D point cloud embedding
i
after the projection layers ψ . To better learn action infor-
P
III. METHOD mationfromthe4Dpointcloud,weincorporateourcustom-
designed im-PSTNet as the 4D encoder. A comprehensive
In this section, we first give an overview of the proposed
explanationoftheim-PSTNet’sparticularswillbepresented
VG4D framework, followed by details on how to train the
in Sect.III-C.
network effectively. Then, we describe the design of im-
VLM takes in the RGB video and action category texts as
PSTNet, which is an improved 4D encoder proposed for
the input. In video understanding, VLMs learn to synchro-
VG4D.
nize video representations with their corresponding textual
counterparts through the collaborative training of a video
A. Overview of VG4D
and text encoder. Formally, given an RGB video clip xV
i
As illustrated in Fig. 2, our proposed VG4D framework and a text description xT, we feed the video xV into the
i i
consistsof3networks:4DpointcloudencoderE P,videoen- video encoder E
V
and the text x iT into the text encoder
coderE V andtextencoderE T fromVLM.Weuselanguage- E T to obtain the feature representation of each sample in a
RGB-4D point cloud triplets to train the framework. The commonembeddingspacebyprojectingtheencodedfeature
RGB video and 4D point cloud are obtained from a camera to a common dimension represented by:
anddepthsensor,whichcapturesdatafromthesamesample.
fV =ψ (E (xV)), fT =ψ (E (xT)). (2)
In addition, the language component consists of textual V V i T T i
descriptions of all possible action categories. We define a where fV ∈RCV and fT ∈RCT represent the RGB video
set of N language-RGB video-4D point cloud triplets as and text embedding after the projection layers ψ and ψ ,
V T
{xP,xV,xT}N , where xP represents 4D point cloud, xV is respectively.ThedimensionsofC ,andC areidenticalafter
i i i i=1 i i V T
the RGB video, and xT is the text of action category. passing through the projection layer.
iWithin the VG4D framework, we employ X-CLIP [17] C. im-PSTNet
as both the text and video encoders. X-CLIP builds upon In this subsection, we first briefly review the classical
CLIP by incorporating cross-frame attention mechanisms and widely used network in dynamic point cloud modeling.
and video-specific hinting techniques. These enhancements Then, we present how we modernize the classical architec-
enable the extraction of temporal cross-frame interaction in- ture PSTNet [29] into im-PSTNet, the improved version of
formationandthegenerationofinstance-leveldiscriminative PSTNet with effective performance. Finally, we detail the
textual representations, respectively. network structure of im-PSTNet. As depicted in Fig.2, the
im-PSTNet mainly consists of the spatial extractor and im-
B. Cross-Modal Learning
PSTConv units.
To learn more transferable representation from VLM, we Review of PSTNet. PSTNet is a spatial and temporal
introduceacross-modallearningobjectivetojointlyoptimize decoupledfeatureextractorfor4Dpointclouds.PSTNetuses
the correlation alignment across language, RGB video, and point spatio-temporal (PST) convolution to extract informa-
4D point cloud. The overall architecture of our method, tive representations of point cloud sequences. The spatio-
shown in Fig. 2, contains language encoder E T, point cloud temporalfeaturesaredenotedash(pt i),whichareaggregated
encoderE PandvisualencoderE V,whichrespectivelyembed in the following manner:
thethreemodalitiesintotextfeature fT ∈RCT,imagefeature (cid:110) (cid:111)
fV ∈RCV and point cloud feature fP∈RCP, whereC is the h(pt i)= max ζ(ft j′ ,(xt j′ −xt i)) . (7)
pt∈N(pt)
embedding dimension. Through normalization, we constrain j i
the output of each projection network to reside within a where ft′ and xt′ represent the feature and coordinates
j j
unit hypersphere, enabling us to measure feature similar- of points in the spatio-temporal neighbors’ points, and ζ
ity using the inner product. Our cross-modal contrastive represents the MLP layers.
learning mainly jointly optimizes the correlation alignment Our paper focuses on enhancing the 4D point cloud
across languages, images, and point clouds via semantic- recognition task by utilizing VLM pre-training knowledge.
level language-4D alignment and instance-level image-4D To achieve this, we propose the im-PSTNet, an improved
alignment. The core idea of semantic-level language-3D and modernized version of PSTNet, as the 4D backbone of
alignment is to drive the feature of 4D action instances and VG4D.
the corresponding action category text closer. In a training From PSTNet to im-PSTNet. Our exploration mainly fo-
mini-batch of size N, with K action categories, we calculate cuses on training strategy modernization and network archi-
the loss function between the language and 4D point cloud tecture modernization. We first employ data augmentation
as: techniques on point cloud video clips. Contrary to PSTNet’s
L =
1
∑−log
exp(f iT·f iP)
. (3)
methodoftrainingandtestingallpotentialclipsegments,we
(pc,text) N
i∈N
∑j∈Kexp(f jT·f iP) employ a data augmentation strategy for frame sampling,
significantly reducing both training and testing durations.
We further introduce the contrastive alignment between
Specifically, we first divide each point cloud video into T
instance-wise RGB video and 4D instances. The contrastive
segments with equal duration. During the training phase,
aligned objective L across point cloud and image is
(pc,video) a frame is randomly sampled from each segment, while
formulated as:
in the testing phase, a frame is selected from the middle
position, in each segment. Our experiments show that using
1 exp(fV·fP)
L = ∑−log i i . (4) the cosine learning rate decay method can lead to better
(pc,video) N
i∈N
∑j∈Nexp(fV
j
·f iP)
training results than using the step decay method used by
Finally, we minimize the contrastive loss for all modality PSTNet.Asaresult,weadoptthecosinelearningratedecay
pairs with different coefficients α and β, method in our im-PSTNet. In terms of network structure,
we use the radius r of the search neighborhood point to
L cl =αL (pc,video)+βL (pc,text). (5) normalize ∆ x =xt j−xt i, which will make the value of the
relativecoordinateslesssmall,whichisconducivetonetwork
Duringcross-modallearning,WeuseL assupervisionto
cl optimization. In addition, to better aggregate the features of
fine-tune the im-PSTNet model that has been pre-trained to
the spatio-temporal neighbors, we increase the feature ft of
learn4Drepresentations.NotethattheVLMisfrozenatthis i
the center point itself to update the features of each center
stage. Our VG4D also includes two classification heads to
point. The spatio-temporal features are aggregated in the
classifythe4DfeaturesandRGBvideofeaturesextractedby
following manner:
im-PSTNet and Video encoder, respectively. Our final loss
(cid:110) (cid:111)
is as follows: h(pt)= max ζ(ft′ ,ft,(xt′ −xt)/r) . (8)
i j i j i
pt∈N(pt)
j i
L =L +θL +γL . (6)
final cl pc rgb
where
ft′
and
xt′
represent the feature and coordinates of
j j
Inthetestingphase,weensembletheim-PSTNetwiththe points in the spatio-temporal neighbors’ points, r represents
VLM.Specifically,wefusefour4D-text,RGB-text,4D,and the radius of searching for spatio-temporal neighbor points,
RGB scores as the final classification result. and ζ represents the MLP layers.TABLEI
ACCURACIES(%)OFDIFFERENTMETHODSONTHENTURGB+D60ANDNTURGB+D120DATASETS.BESTINBOLD,SECOND-BEST
UNDERLINED.
NTU60 NTU120
Method Venue Modality
Cross-subject Cross-view Cross-subject Cross-setup
Uni-modalrecognitionmethods
3DV-PointNet++ CVPR’20[31] PointCloud 88.8 96.3 82.4 93.5
PSTNet ICLR’21[29] PointCloud 90.5 96.5 87.0 93.8
PSTNet++ TPAMI’21[30] PointCloud 91.4 96.7 88.6 93.8
PST-Transformer TPAMI’22[10] PointCloud 91.0 96.4 87.5 94.0
Kinet CVPR’22[32] PointCloud 92.3 96.4 - -
GeometryMotion TCSVT’21[33] PointCloud 92.7 98.9 90.1 93.6
APSNet TIP’22[48] PointCloud 91.5 98.2 88.3 92.5
Ours - PointCloud 93.9 98.9 90.3 92.0
Multi-modalrecognitionmethods
CAPF CVPR’22[49] RGB+Depth 94.2 97.3 - -
PA-AWCNN ICRA’22[50] RGB+Depth 92.8 95.7 - -
FeatureFusion IROS’19[51] RGB+Skeleton 85.4 91.6 - -
VPN ECCV’20[52] RGB+Skeleton 93.5 96.2 86.3 87.8
STAR-Transformer WACV’23[53] RGB+Pose 92.0 96.5 90.3 92.7
MMNet TPAMI’22[54] RGB+Pose 96.0 98.8 92.9 94.4
PoseC3D CVPR’22[55] RGB+Pose 97.0 99.6 96.4 95.4
Ours - RGB+PointCloud 97.6 99.8 96.8 97.6
Architecture of im-PSTNet. Spatial extractor is designed with NTU RGB+D 60 dataset. We use the cross-subject and
to extract the initial features from the N points in each cross-setup evaluation protocols on the NTU RGB+D 120
frame, which consists of four sub-modules: point sampling, dataset.
grouping, MLP layers, and max-pooling. In the point sam- Implementation Details. For point cloud data preparation,
pling layer, given a spatial subsampling rate S s, the iterative we follow PSTNet to convert depth maps to point cloud
farthest point sampling(FPS) method is used to subsample sequences, in which we sample 2048 points in each frame.
N points to N′ = [N] centroids in each frame. Then the We use the SGD optimizer with cosine learning rate decay
ss
grouping layer searches for a few neighboring points around for optimization. The initial learning rate, the weight decay,
each centroid to construct a local region for the points and the batch size are empirically set as 0.01, 0.1, and 32,
subsampled after the FPS. After applying MLP and MAX respectively. We pre-train 120 epochs on NTU RGB+D 60
pooling,theresultingoutputwillcontainthecoordinates(xt i) and NTU RGB+D 120. The number of neighboring points
and features (pt i) of each point that has undergone down- K and the spatial search radius r at the grouping module
sampling. im-PSTConv is improved based on point spatio- are set as 9 and 0.1, respectively. Following PSTNet, we
temporal (PST) convolution [29], used to extract spatio- set the clip length and frame sampling stride to 23 and 2,
temporal information. The difference between im-PSTConv respectively. For the RGB modality, we set the number of
and spatial extractor is that im-PSTConv will group spatio- input frames to 8, using the same frame sampling method in
temporal points by building point pipes. It searches for the point cloud video. We use the pre-trained X-CLIP-B/16
spatio-temporal neighbors across frames, so this module is model on Kinetics600 [58] to fine-tune for 30 epochs on the
called a cross-grouping module. NTURGB+Ddataset.Incontrastivelearning,wetrainfor30
epochs, the learning rate decays from 0.001 to 0.0001, and
IV. EXPERIMENTS
theothersettingsarethesameasthoseofthepre-trained4D
In this section, we describe the implementation details, encoder.AllourexperimentsareperformedontwoNVIDIA
experiment setup, and experimental results. 12G 3080Ti GPUs.
Dataset. NTU RGB+D [56] is a large-scale benchmark
dataset for action recognition, which contains 56,880 videos
collected from 40 subjects performing 60 different actions A. Comparison with state-of-the-art methods
in 80 camera views. The videos are captured using Kinect
V2tocollectfourdatamodalities:RGBframes,depthmaps, In Table I, we compare our proposed method with other
3D joint information, and IR sequences. Cross-subject and methods on the two datasets. Our im-PSTNet outperforms
cross-view evaluations are adopted. NTU RGB+D 120 [57] other single modal baseline methods under most of the set-
is an extension of NTU60, with 120 action classes and tings on both datasets, which demonstrates the effectiveness
114,480 videos. The action classes include daily actions, of our im-PSTNet for 4D action recognition on large-scale
health-related actions, and mutual actions. This dataset is datasets. Concurrently, our VG4D achieves state-of-the-art
also collected with Kinect V2 and shares the same modality results on multi-modal baseline approaches.TABLEII TABLEIV
CROSS-SUBJECTCLASSIFICATIONACCURACIES(%)OFDIFFERENTLOSS ADDITIVESTUDYOFSEQUENTIALLYAPPLYINGTRAININGSTRATEGIES
ONTHENTURGB+D120DATASET. ANDARCHITECTUREMODERNIZATIONONNTURGB+D120DATASET.
WEUSEGREENANDYELLOWBACKGROUNDCOLORSTODENOTE
Methods Accuracy(%) TRAININGSTRATEGYANDMODELOPTIMIZATIONRESPECTIVELY.
VG4D 96.8
VG4D(w/oclsloss) 96.0 Method Accuracy(%) ∆
VG4D(w/opc-rgbloss) 95.4
VG4D(w/opc-textloss) 95.0 PSTNet 88.6 -
+Randomframesamplingdata 89.0 +0.4
+StepDecay→CosineDecay 89.2 +0.2
TABLEIII +Normalizing∆p (Equation8) 89.9 +0.7
CROSS-SUBJECTCLASSIFICATIONACCURACIES(%)OFDIFFERENT +Featureaggregation(im-PSTNet) 90.3 +0.4
CATEGORICALSCORECOMBINATIONONTHENTURGB+D120
DATASET.CMLSTANDSFORCROSS-MODALLEARNINGINVG4D. TABLEV
CROSS-SUBJECTCLASSIFICATIONACCURACY(%)OFDIFFERENT
CML PC PC-Text RGB-Text RGB Accuracy(%) VISIONLANGUAGEONNTURGB+D120DATASET.
✓ ✓ 96.3
✓ ✓ 95.2 Method Modality Accuracy(%)
✓ ✓ ✓ 96.1 X-CLIP RGB 95.2
✓ ✓ ✓ ✓ 96.7 Vita-CLIP RGB 95.1
✓ ✓ ✓ ✓ 96.5 VG4D(Vita-CLIP) RGB+PointCloud 95.5
✓ ✓ ✓ ✓ ✓ 96.8 VG4D(X-CLIP) RGB+PointCloud 96.8
label staple book reading cross toe touch
B. Ablation Study
Comparison of Different Losses. We report the effect of
using different losses when fine-tuning im-PSTNet in Ta-
bleII.Amongthem,thepc-rgbandpc-textlossrepresentthe
comparativelearninglossofpointcloudandRGBvideoand
text, respectively. The cls loss represents the cross-entropy
result make victory sign writing cross toe touch
loss of im-PSTNet. In particular, after removing the two
contrastivelearninglosses,theaccuracyofactionrecognition
dropped significantly, which proves the effectiveness of the
contrastive learning method we proposed. Comparison of
Different Fusion Methods. To further show the effective-
ness of our method, we compare different combinations of
classification scores in Table III. PC, PC-Text, RGB, and result staple book reading shake fist
RGB-Text represent the FC classification score of the point Fig.3. Actionclassificationcasesforsomedifferentmodalities.
cloud, the comparison score of the point cloud and text, the
FC classification score of RGB, and the comparison score are shown in Fig. 3. Recognition failure in point cloud
of RGB and text. CML representation using cross-modal modalitiesisoftencausedbytheabsenceofdetailedinforma-
learning in VG4D. tion, such as hand and object movements, which are crucial
Comparison of Different Improvements. In Table IV, we fordistinguishingactionsthatonlyinvolvehandmovements.
report the results of our proposed 4D encoder im-PSTNet Conversely, RGB mode recognition fails due to the lack of
when using different modules compared to the original depth information, which is precisely what the point cloud
PSTNet. As can be seen from the table, the im-PSTNet has mode provides.
agreaterimprovementin4Dactionrecognitioncomparedto
V. CONCLUSION
the PSTNet baseline.
Comparison of Different VLMs. We experiment with Inthispaper,weexplorehowVLMknowledgebenefits4D
different VLMs under our framework. We report the results pointcloudunderstanding.WepresentanovelVLMgoes4D
of using X-CLIP and Vita-CLIP pre-trained models and frameworkwithaneffective4Dbackbonenamedim-PSTNet
the effect of integrating our framework in Table V. As to learn better 4D representations. To efficiently transfer
can be seen from the Table, impressive results can also be VLM’simageandtextfeaturestoa4Dnetwork,wepropose
achieved using Vita-CLIP. This shows that the framework anovelcross-modalcontrastivelearningscheme.OurVG4D
we proposed is universal, and we can integrate VLM with approach has achieved state-of-the-art performance on vari-
excellent performance into our framework. ous large-scale action recognition datasets. Additionally, our
proposed im-PSTNet can be utilized as a robust baseline for
C. Further analysis
4D recognition. We hope that this work can inspire action
HardCases.Someclassificationfailurecasesofim-PSTNet recognition research in the future.REFERENCES [27] Z.Fang,X.Li,X.Li,J.M.Buhmann,C.C.Loy,andM.Liu,“Explore
in-contextlearningfor3dpointcloudunderstanding,”NeurIPS,2023.
[1] J.LiangandA.Boularias,“Learningcategory-levelmanipulationtasks
2
frompointcloudswithdynamicgraphcnns,”inICRA,2023. 1
[28] X.Liu,M.Yan,andJ.Bohg,“Meteornet:Deeplearningondynamic
[2] D. Seichter, M. Ko¨hler, B. Lewandowski, T. Wengefeld, and H.-
3dpointcloudsequences,”inICCV,2019. 2
M. Gross, “Efficient rgb-d semantic segmentation for indoor scene
[29] H.Fan,X.Yu,Y.Ding,Y.Yang,andM.Kankanhalli,“Pstnet:Point
analysis,”inICRA,2021. 1
spatio-temporalconvolutiononpointcloudsequences,”inICLR,2021.
[3] C.Huang,O.Mees,A.Zeng,andW.Burgard,“Visuallanguagemaps
2,4,5
forrobotnavigation,”inICRA,2023. 1
[30] H. Fan, X. Yu, Y. Yang, and M. S. Kankanhalli, “Deep hierarchical
[4] B. Chen, F. Xia, B. Ichter, K. Rao, K. Gopalakrishnan, M. S.
representation of point cloud videos via spatio-temporal decomposi-
Ryoo, A. Stone, and D. Kappler, “Open-vocabulary queryable scene
tion,”TPAMI,2022. 2,5
representationsforrealworldplanning,”inICRA,2023. 1
[31] Y.Wang,Y.Xiao,F.Xiong,W.Jiang,Z.Cao,J.T.Zhou,andJ.Yuan,
[5] D.WangandZ.-X.Yang,“Self-supervisedpointcloudunderstanding
“3dv: 3d dynamic voxel for action recognition in depth video,” in
viamasktransformerandcontrastivelearning,”RA-L,2023. 1
CVPR,2020. 2,5
[6] C. R. Qi, L. Yi, H. Su, and L. J. Guibas, “Pointnet++: Deep hierar-
chical feature learning on point sets in a metric space,” in NeurIPS, [32] J. Zhong, K. Zhou, Q. Hu, B. Wang, N. Trigoni, and A. Markham,
2017. 1,2 “Nopain,biggain:Classifydynamicpointcloudsequenceswithstatic
[7] Z.Fang,X.Li,X.Li,J.M.Buhmann,C.C.Loy,andM.Liu,“Explore models by fitting feature-level space-time surfaces,” in CVPR, 2022.
in-contextlearningfor3dpointcloudunderstanding,”NeurIPS,2024. 2,5
1 [33] J.LiuandD.Xu,“Geometrymotion-net:Astrongtwo-streambaseline
[8] Z. Fang, X. Li, X. Li, S. Zhao, and M. Liu, “Modelnet-o: A large- for3dactionrecognition,”TCSVT,2021. 2,5
scalesyntheticdatasetforocclusion-awarepointcloudclassification,” [34] J.Liu,J.Guo,andD.Xu,“Geometrymotion-transformer:Anend-to-
arXivpreprintarXiv:2401.08210,2024. 1,2 endframeworkfor3dactionrecognition,”TMM,2022. 2
[9] A. Osˇep, P. Voigtlaender, M. Weber, J. Luiten, and B. Leibe, “4d [35] X.Wang,Q.Cui,C.Chen,andM.Liu,“Gcnext:Towardstheunity
genericvideoobjectproposals,”inICRA,2020. 1 ofgraphconvolutionsforhumanmotionprediction,”inAAAI,2024.
[10] H. Fan, Y. Yang, and M. S. Kankanhalli, “Point spatio-temporal 2
transformernetworksforpointcloudvideomodeling,”TPAMI,2023. [36] X.Wang,Z.Fang,X.Li,X.Li,C.Chen,andM.Liu,“Skeleton-in-
1,5 context:Unifiedskeletonsequencemodelingwithin-contextlearning,”
[11] Y. Zeng, C. Jiang, J. Mao, J. Han, C. Ye, Q. Huang, D.-Y. Yeung, CVPR,2024. 2
Z. Yang, X. Liang, and H. Xu, “Clip2: Contrastive language-image- [37] X. Wang, W. Zhang, C. Wang, Y. Gao, and M. Liu, “Dynamic
point pretraining from real-world point cloud data,” in CVPR, 2023. densegraphconvolutionalnetworkforskeleton-basedhumanmotion
2 prediction,”TIP,2023. 2
[12] D. Hegde, J. M. J. Valanarasu, and V. M. Patel, “Clip goes 3d: [38] H.Fan,Y.Yang,andM.Kankanhalli,“Point4dtransformernetworks
Leveraging prompt tuning for language grounded 3d recognition,” forspatio-temporalmodelinginpointcloudvideos,”inCVPR,2021.
arXivpreprintarXiv:2303.11313,2023. 2 2
[13] H. Rasheed, M. U. khattak, M. Maaz, S. Khan, and F. S. Khan, [39] H. Wen, Y. Liu, J. Huang, B. Duan, and L. Yi, “Point primitive
“Finetunedclipmodelsareefficientvideolearners,”inCVPR,2023. transformer for long-term 4d point cloud video understanding,” in
2 ECCV,2022. 2
[14] J. Wu, X. Li, S. Xu, H. Yuan, H. Ding, Y. Yang, X. Li, J. Zhang, [40] Y. Wei, H. Liu, T. Xie, Q. Ke, and Y. Guo, “Spatial-temporal
Y.Tong,X.Jiang,B.Ghanem,andD.Tao,“Towardsopenvocabulary transformerfor3dpointcloudsequences,”inWACV,2022. 2
learning:Asurvey,”T-PAMI,2024. 2 [41] X. Chen, W. Liu, X. Liu, Y. Zhang, J. Han, and T. Mei, “MAPLE:
[15] S.Xu,X.Li,S.Wu,W.Zhang,Y.Li,G.Cheng,Y.Tong,K.Chen,and maskedpseudo-labelingautoencoderforsemi-supervisedpointcloud
C.C.Loy,“Dst-det:Simpledynamicself-trainingforopen-vocabulary actionrecognition,”inACMMM,2022. 2
objectdetection,”arXivpreprintarXiv:2310.01393,2023. 2
[42] Z.Shen,X.Sheng,L.Wang,Y.Guo,Q.Liu,andZ.Xi,“Pointcmp:
[16] L.Xue,M.Gao,C.Xing,R.Mart´ın-Mart´ın,J.Wu,C.Xiong,R.Xu,
Contrastivemaskpredictionforself-supervisedlearningonpointcloud
J.C.Niebles,andS.Savarese,“Ulip:Learningaunifiedrepresentation videos,”inCVPR,2023. 2
oflanguage,images,andpointcloudsfor3dunderstanding,”inCVPR,
[43] T.Huang,B.Dong,Y.Yang,X.Huang,R.W.Lau,W.Ouyang,and
2023. 2,3
W. Zuo, “Clip2point: Transfer clip to point cloud classification with
[17] B. Ni, H. Peng, M. Chen, S. Zhang, G. Meng, J. Fu, S. Xiang, and
image-depthpre-training,”inICCV,2023. 2
H. Ling, “Expanding language-image pretrained models for general
[44] T. Yang, Y. Zhu, Y. Xie, A. Zhang, C. Chen, and M. Li, “Aim:
videorecognition,”inECCV,2022. 2,4
Adapting image models for efficient video understanding,” in ICLR,
[18] J. Chen, Y. Zhang, F. Ma, and Z. Tan, “Eb-lg module for 3d point
2023. 2
cloudclassificationandsegmentation,”RA-L,2023. 2
[45] W.Wu,X.Wang,H.Luo,J.Wang,Y.Yang,andW.Ouyang,“Bidirec-
[19] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning
tionalcross-modal knowledge explorationforvideo recognitionwith
onpointsetsfor3dclassificationandsegmentation,”inCVPR,2017.
pre-trainedvision-languagemodels,”inCVPR,2023. 2
2
[46] S. T. Wasim, M. Naseer, S. Khan, F. S. Khan, and M. Shah, “Vita-
[20] G. Qian, Y. Li, H. Peng, J. Mai, H. Hammoud, M. Elhoseiny, and
clip:Videoandtextadaptiveclipviamultimodalprompting,”inCVPR,
B.Ghanem,“Pointnext:Revisitingpointnet++withimprovedtraining
2023. 2
andscalingstrategies,”inNeurIPS,2022. 2
[21] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, and J. M. [47] R.Zhang,Z.Guo,W.Zhang,K.Li,X.Miao,B.Cui,Y.Qiao,P.Gao,
Solomon, “Dynamic graph cnn for learning on point clouds,” TOG, andH.Li,“Pointclip:PointcloudunderstandingbyCLIP,”inCVPR,
2019. 2 2022. 3
[22] H. Zhao, L. Jiang, J. Jia, P. H. S. Torr, and V. Koltun, “Point [48] J.Liu,J.Guo,andD.Xu,“Apsnet:Towardadaptivepointsampling
transformer,”inICCV,2021. 2 forefficient3dactionrecognition,”TIP,2022. 5
[23] L. Chen, H. Wang, H. Kong, W. Yang, and M. Ren, “Ptc-net: [49] B.Zhou,P.Wang,J.Wan,Y.Liang,F.Wang,D.Zhang,Z.Lei,H.Li,
Point-wise transformer with sparse convolution network for place andR.Jin,“Decouplingandrecouplingspatiotemporalrepresentation
recognition,”RA-L,2023. 2 forrgb-d-basedmotionrecognition,”inCVPR,2022. 5
[24] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, and J. Lu, “Point-bert: [50] L. Yao, S. Liu, C. Li, S. Zou, S. Chen, and D. Guan, “Pa-awcnn:
Pre-training3dpointcloudtransformerswithmaskedpointmodeling,” Two-streamparallelattentionadaptiveweightnetworkforrgb-daction
inCVPR,2022. 2 recognition,”inICRA,2022. 5
[25] Y.Pang,W.Wang,F.E.H.Tay,W.Liu,Y.Tian,andL.Yuan,“Masked [51] G. Liu, J. Qian, F. Wen, X. Zhu, R. Ying, and P. Liu, “Action
autoencodersforpointcloudself-supervisedlearning,”inECCV,2022. recognition based on 3d skeleton and rgb frame fusion,” in IROS,
2 2019. 5
[26] X.Yan,H.Zhan,C.Zheng,J.Gao,R.Zhang,S.Cui,andZ.Li,“Let [52] S. Das, S. Sharma, R. Dai, F. Bre´mond, and M. Thonnat, “VPN:
images give you more: Point cloud cross-modal training for shape learningvideo-poseembeddingforactivitiesofdailyliving,”inECCV,
analysis,”inNeurIPS,2022. 2 2020. 5[53] D. Ahn, S. Kim, H. Hong, and B. Ko, “Star-transformer: A spatio-
temporalcrossattentiontransformerforhumanactionrecognition,”in
WACV,2023. 5
[54] B.X.Yu,Y.Liu,X.Zhang,S.-h.Zhong,andK.C.Chan,“Mmnet:
A model-based multimodal network for human action recognition in
rgb-dvideos,”TPAMI,2023. 5
[55] H.Duan,Y.Zhao,K.Chen,D.Lin,andB.Dai,“Revisitingskeleton-
basedactionrecognition,”inCVPR,2022. 5
[56] A. Shahroudy, J. Liu, T. Ng, and G. Wang, “NTU RGB+D: A large
scaledatasetfor3dhumanactivityanalysis,”inCVPR,2016. 5
[57] J. Liu, A. Shahroudy, M. Perez, G. Wang, L.-Y. Duan, and A. C.
Kot,“Nturgb+d120:Alarge-scalebenchmarkfor3dhumanactivity
understanding,”TPAMI,2019. 5
[58] J.Carreira,E.Noland,A.Banki-Horvath,C.Hillier,andA.Zisserman,
“A short note about kinetics-600,” arXiv preprint arXiv:1808.01340,
2018. 5