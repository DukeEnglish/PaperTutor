Embedding Privacy in Computational Social Science and
Artificial Intelligence Research
Keenan Jones1, Fatima Zahrah2, and Jason R.C. Nurse1 *
1 School of Computing & Institute of Cyber Security for Society (iCSS), University of Kent, UK
2 Department of Computer Science, University of Oxford, UK
*j.r.c.nurse@kent.ac.uk
Abstract
Privacy is a human right. It ensures that individuals are free to engage in discussions, participate in
groups, and form relationships online or offline without fear of their data being inappropriately
harvested, analyzed, or otherwise used to harm them. Preserving privacy has emerged as a critical
factor in research, particularly in the computational social science (CSS), artificial intelligence (AI) and
data science domains, given their reliance on individuals’ data for novel insights. The increasing use
of advanced computational models stands to exacerbate privacy concerns because, if inappropriately
used, they can quickly infringe privacy rights and lead to adverse effects for individuals – especially
vulnerable groups – and society. We have already witnessed a host of privacy issues emerge with the
advent of large language models (LLMs), such as ChatGPT, which further demonstrate the
importance of embedding privacy from the start. This article contributes to the field by discussing the
role of privacy and the primary issues that researchers working in CSS, AI, data science and related
domains are likely to face. It then presents several key considerations for researchers to ensure
participant privacy is best preserved in their research design, data collection and use, analysis, and
dissemination of research results.
Keywords: privacy, ethics, computational social science, artificial intelligence, AI, machine
learning, data science, generative AI
1. Introduction
Data and information are the lifeblood of our modern-day, technology-driven world. In traditional
technology systems, data is an artifact that is gathered (e.g., via human or electronic sensors) but
also one that is produced (e.g., for insights and reporting) to supply other technical or socio-
technical systems. As society has advanced, the types of data available have expanded immensely.
This is a consequence of new technologies, such as social media, the internet of things (IoT), and
artificial intelligence (AI), which have fueled an interest in gathering and analyzing this data to better
understand large-scale social phenomena. The field of Computational Social Science (CSS) is
pertinent to these discussions as it involves the development and application of computational
techniques (such as advanced machine learning algorithms) to explore human behavioral data (Lazer
et al., 2020). CSS allows large human-oriented datasets to be analyzed and can produce new insights
on population habits and opinions, online activism, the spread of misinformation, news
consumption, and online hate and radicalism. Another field that has rapidly gained traction and
public interest in the last three years is AI and particularly, Generative AI (GenAI) – i.e., AI algorithms
that can generate new content (e.g., text, audio, visuals, code). The releases of OpenAI’s ChatGPT1
1 https://chat.openai.com/
1and Dall-E 22 in 2022 were significant milestones as they demonstrated how large language models
(LLMs), trained on immense datasets, could be used by anyone for a range of novel tasks – both
constructive and malign. Since then, other GenAI platforms have been launched including
Microsoft’s Copilot3 and Google’s Gemini4.
While there are numerous benefits to the application of computational techniques to individuals’
data, there are also a plethora of ethical and privacy concerns that must be considered. These stem
from the reality that inappropriately applying these techniques can cause immense harm to the
communities they study and to the wider research domain. Ethical practices aim to ensure that
research is conducted to the highest standard; ensuring that the data used has been legitimately
gathered (e.g., using informed consent and obeying platform rules), accurately processed (e.g.,
absent of bias) and responsibly reported on (e.g., acknowledging the ethical implications of any
findings). Privacy, in particular, is a core concern that emerges in any discussion involving human or
social data. In CSS research, this centers around maintaining the privacy of individuals whose data is
used in studies and ensuring that this participation does not lead to harm. There are multiple
examples that demonstrate the relevance of privacy in such research, but some of the most topical
center on the protection of personal data as it is collected, stored and analyzed, and raises questions
around robustly anonymizing datasets (as shown in the well-known Netflix example (Narayanan &
Shmatikov, 2008)). In AI research, the privacy challenge is similar, but also originates from the fact
that the LLMs – which power ChatGPT and other systems – are often trained using significant
amounts of online data including personal information. ChatGPT has already suffered a (temporary)
ban in Italy over privacy fears (BBC, 2023) and Britain’s data protection regulator has recently
launched a consultation into the legality of web scraping data to train generative AI models (ICO,
2024).
This article aims to contribute to the discussion of privacy within CSS research by providing a
foundation for researchers in conducting privacy-aware studies. While our focus is on CSS, we also
consider related advancements in AI and data science considering their prominence and reliance on
individuals’ data. To achieve this, we first reflect on the value of data to CSS, concentrating on the
vital insights possible when personal data (e.g., online posts, opinions, demographics) is analyzed
using computational methods. Next, privacy takes center stage as we reflect on how privacy has
been discussed to date, highlighting its relevance to CSS. We further engage in our own critical
discussion about the privacy challenges that arise in working with data, and ensuring that
aggregated data and any subsequent analyses/releases do not lead to adverse effects. To provide
guidance for researchers, we then conclude by presenting a number of recommendations
addressing how privacy should be planned for and embedded in CSS research.
2. The Value of CSS and AI to Society
In today’s digital age, behavioral data is plentiful, and a plethora of data sources have become
available to model various domains of social interaction. CSS research has utilized such data to
greatly improve our understanding of important phenomena, ranging from social inequality to the
spread of infectious diseases (Lazer et al., 2020; Pereira, da Silva, & Rosa, 2024). AI models and
research have also been applied to personal and behavioral data, and we have witnessed AI being
used to predict cancer years before an official diagnosis (Pesheva, 2023). Cutting-edge research
2 https://openai.com/dall-e-2
3 https://copilot.microsoft.com/
4 https://gemini.google.com/
2across these domains now incorporates the analysis of social network data, harvested textual data,
and fine-grained geospatial data including the traces of movements in space and time. The range of
methodologies in use is equally broad. While descriptive statistics and regression models remain
common in social science publications, research engaging with these new forms of data increasingly
employ automated learning algorithms, including Support Vector Machines (SVM), Random Forests,
and deep learning; natural language processing (NLP) techniques, including statistical methods for
topic, stylographic, and sentiment analysis of text; and dynamic visualization.
When considering the vast array of academic literature, it is clear that some of the most popular big
data sources for CSS research are online social media posts. Social media platforms have
revolutionized the ways humans interact with each other and the world around them. This has
cemented the importance of the Internet as an organizational and mobilization vehicle, which has
been leveraged by a variety of organizations, including hacktivist collectives, such as Anonymous
(Earl & Schussman, 2002). Several studies have made use of CSS approaches to understand how
these movements seek social change and alter relations of power. For instance, Jones et al. (2020)
make use of machine learning classifiers, social network analysis (SNA), and topic modeling to
analyze the network structure of Twitter accounts explicitly affiliated with Anonymous, through
which they prove that certain accounts are more influential than others. Such research has shown
how social media platforms facilitate the activities of online organizations, and how this impacts
offline societal change. Generative AI has also benefited significantly from social media data. This
data captures two-way and group interactions on social platforms and provides the ideal input of
millions of data points that can be learned from.
Various malicious organizations have sought to exploit social media platforms for their own gain,
including disseminating scams (e.g., cryptocurrency and phishing scams) and even radicalizing and
recruiting individuals to criminal and extremist groups. As a result, academic studies within this field
have made use of machine learning algorithms to develop, test, and improve new or existing
detection methods, including SVM (Frenda et al., 2019), Random Forests (Badjatiya et al., 2017), and
deep learning (Jaki et al., 2019). Such approaches have been further extended to incorporate
contextual and psychological dimensions (e.g. personality traits) within detection frameworks (Nouh
et al. 2019; Ebner, Kavanagh & Whitehouse, 2024). Network analysis techniques have also been
applied to identify and disrupt extremist networks, for instance by blocking the most influential
nodes or clusters (Klausen, 2015). Additionally, academic literature has explored how counter-
narratives can be used to diminish the effects of online extremism. For example, Zahrah et al. (2020)
make use of CSS approaches to compare counter-extremism messaging by governments and NGOs
on Twitter with pro-extremist content to assess the nature of these opposing narratives and the
psychological and platform-specific strategies they use. AI also has a key role to play given its
potential use in detecting online crime. Police in Japan have already explored the use of AI to scan
social media for this purpose (Japan Times, 2023).
Other major data sources used within research include those provided by corporations and
governments, which have especially proven to be useful in this field following the emergence of the
COVID-19 pandemic in early 2020. The pandemic significantly impacted societies throughout the
world, leading to increased research efforts into intervention and vaccination. CSS has been integral
to much of the research analyzing COVID-19’s various impacts, and modeling the progression of the
pandemic has become a popular area of study. Rastgoftar and Atkins (2021) proposed a novel
pandemic spread model by integrating historical data of infected cases, deaths, and recoveries, and
obtained a data-driven model for spread dynamics among the human population. Wang et al. (2021)
further analyzed the impact of related multisource urban data, such as local temperature, relative
humidity, air quality, and inflow rate from Hubei province, and proposed an effective short-term
3prediction model for COVID-19 cases. These are just a few topical examples of the key insights that
such research can bring.
3. The Role of Privacy in Research
Through the leveraging of computational methods and AI algorithms researchers have been able to
perform analyses on an unprecedented scale. Central to CSS and AI, however, is not just the usage of
vast amounts of data, but more specifically vast amounts of data capturing human behaviors. This
reliance on huge amounts of personal data, however, brings with it a challenging question: what is
the role of privacy in the collection and analysis of this data?
3.1 Privacy and Information Control
At its core, privacy has typically been formulated as an individual’s ‘right to be left alone’ (DeCew,
2018), and in light of this, attempts have been made to examine how privacy should be applied to
the evolving nature of data in the digital age. Langheinrich et al. (2005) state that privacy is
empowerment, and that individuals should have the power to control and monitor the distribution
of their personal data online. While offering a simple extension to traditional conceptions of privacy
(DeCew, 2018), this approach brings with it further complications. Once data is released onto the
web it is all but impossible to ensure its removal, and this raises questions regarding the extent to
which an individual can truly maintain control over their information online. If a person has allowed
a given web platform, for instance, to publish their posts, can they ever truly regain control over this
information? Currently, it is difficult to provide a concrete answer to this question.
What is clear, however, is that in the digital age a vast degree of responsibility towards the handling
of personal data, including the role of privacy protection, has been ceded to those controlling the
data (Barth & Jong, 2017). This is typically viewed in terms of the large companies who hold the
monopoly over this form of data, but within this bracket also falls the CSS or AI researcher. In
conducting their studies, the researcher places themselves in a position of control over vast swathes
of personal data, and by extension the privacy of the individuals included in these datasets. While it
is beyond the scope of this article to state for certain whether, in this world of technological
innovation, privacy is fundamentally at risk, what we can conclude is that the preservation of privacy
is more so than ever a central responsibility of the ethical CSS researcher.
3.2 The Direct Dangers of Poor Privacy Consideration
Beyond the broader conceptions of privacy as integral to preserving one’s inherent rights, it is also
essential to consider the ways in which poor considerations of privacy can lead to tangible harm. In
turn, there are at least two key threats to user privacy directly relevant to research that warrant
discussion (Katal et al., 2013).
Firstly, there is the threat of direct inference, in which a malicious actor combines publicly available
personal information about a given individual (extracted, say, from their social media account) with
information captured in a given dataset. Through this combination, the malicious actor may then be
able to infer further characteristics or information about the individual that they had intended to
keep secret. The second threat is that of indirect inference via the predictive power contained in
large amounts of data. Through the curation of these vast datasets, it has now become
commonplace to use powerful machine learning algorithms to predict a wide range of personal
attributes and behaviors – as seen in the work dedicated to analyzing extremism and hacktivism on
4social media mentioned in Section 2. By leveraging these models, it is possible to predict private
aspects of an individual from more “innocent” data. We have seen this power in a wide range of
studies, including the prediction of personality traits (Nouh et al. 2019) and user location (Zheng et
al., 2018), leveraging often unassuming data such as social media posts and profile images.
Research that leverages these approaches can potentially intrude on user privacy in harmful ways,
either directly through the creation of these models as part of their research, or indirectly in the
case of malicious actors utilizing the data collected during research to build their own privacy-
busting predictive models (Katal et al., 2013). In recent years we have seen these dangers realized in
a variety of cases, including the infamous Facebook-Cambridge Analytica scandal in which Facebook
data used to psychologically profile users was leveraged to manipulate opinions on the 2016 US
presidential elections and the UK Brexit referendum (Isaak & Hanna, 2018). Beyond this, myriad
other political contests across Europe, Latin America, and Africa have seen private data being
leveraged to profile potential targets for political persuasion, or for broader aims of sowing discord
through the spreading of fake news and misinformation (Patel, 2019).
3.3 The Problem of Aligning Privacy and Mass Data Gathering
From the above two sections, we have highlighted the key risks to privacy posed by the use of vast
amounts of personal data. Before we can begin to consider the ways in which researchers can best
integrate privacy-respecting practices into their research designs, however, it is first necessary to
highlight some of the key issues affecting the alignment of participant privacy with the research
design.
Arguably most central to this alignment is the principle of “informed consent”, an issue which has
formed the backbone of most discussions of ethics and privacy in research involving human
participants and their data. The importance of informed consent is thus emphasized by most
scholarly societies, including the British Society of Sociologists (BSA), the British Psychology Society
(BPS), and the British Society of Criminology (BSC), who highlight in their framework of ethics that
individuals should be able to take part in research “voluntarily, free from any concern…able to give
freely informed consent in all but exceptional circumstances” (BSC, 2015).
Despite the initial clarity of the role of informed consent in the analysis of personal data, it is
immediately and obviously compromised when applied to the large datasets common in CSS, which
are typically collected at scale, and often from online platforms like Twitter, Reddit, and Facebook,
as discussed in Section 2. As noted in the BSA’s ethical guidelines for digital research, to work with
this form of data requires dealing with “new, messy and often confusing definitions of the private
and the public” which require the resolution of “unprecedented tensions between the researcher
and the researched” (BSA, 2017). These tensions, combined with the massive scale typical of online
data gathering, often mean that traditional approaches to gaining informed consent are
fundamentally “impractical” (AoIR, 2019).
To help mitigate this, some platforms – such as Twitter, now X, – include in their terms of service
(ToS) descriptions of how user data may be made available for research purposes (Twitter, 2021),
and attempts have been made to try and justify the existence of informed consent in the analysis of
online data; suggesting that the acceptance of these ToS amount to tacit consent (Fiesler et al.,
2020). However, many studies have demonstrated that users seldom read or understand the
agreements that they are making in these ToS, particularly with respect to a given platform's usage
of their data (Obar & Oeldorf-Hirsch, 2018). It is, therefore, hard to claim that this can truly be
understood as informed consent. Indeed, this has, in part, led to the rise of the so-called Privacy
Paradox, in which an individual’s description of the importance of information privacy does not
5correlate with their behaviors online (Barth & Jong, 2017). Despite most individuals declaring privacy
to be a key concern, studies have found that there is a notable degree of resignation and futility in
regard to their ability to control their personal information (Barth & Jong, 2017).
This inability to provide informed consent necessitates a loss of control over one’s personal
information. As the individual may have little understanding of how their data may be used in a
given CSS study, control over their data is largely handed to the platforms through which the data is
distributed, and to those who seek to use this data. These platforms – and the third parties (of which
CSS researchers are an example) they share this data with – are thus able, to a great degree, to
decide how long they retain this data, who this data may be shared with, and to what use this data
can be put to (DeNardis & Hackl, 2015).
The issues covered in this section thus far lead to a somewhat frustrating conclusion. In general,
current notions of the right to privacy and of how a study can be best constructed to preserve
privacy may be incompatible with the large-scale, detailed datasets central to research. Ultimately,
there are many unanswered questions regarding the alignment of traditional standards of privacy,
particularly in terms of consent and information control, with current trends towards digital data
gathering and mass data analysis within the field. Given this lack of alignment and the importance of
safeguarding participant privacy, the protection of privacy is heavily reliant on researchers and their
respective institutions exacting a high degree of judgment with regards to these difficult issues
(AoIR, 2019). Moreover, this often must be conducted without clear guidance on the range of issues
which need to be considered in what is a fast-paced field of research. In the next section, therefore,
we aim to support such discussions by outlining essential considerations for understanding and
developing privacy-focused approaches to research, highlighting the key steps that can be taken by
researchers to ensure that their work is compatible with each study participant’s reasonable
expectations of privacy, and that their data is handled in a way that protects participants from harm.
4. Conducting Privacy-Aware Research
Given the need to align privacy with CSS and AI research, it is essential that researchers design their
projects with a mind towards participant privacy. In this section, we focus on CSS and examine the
critical stages of a typical CSS project that require clear privacy-protecting considerations: the initial
research design, the collection and use of data (including its storage and dissemination), and the
analysis and model development process. Where appropriate, we also highlight aspects that closely
relate to AI research. Across the section, we provide a discussion of the key considerations that
researchers should make to ensure participant privacy is best preserved at each of these research
stages. We note that while this guidance is primarily to researchers conducting the research, it is
also relevant to journal editors and conference program committees given their role as peer
reviewers and gatekeepers for robust, ethical academic publications.
4.1 Research Design
The first part of conducting privacy-aware CSS and AI research involves understanding the social
context of the proposed research and the impact it could have on society and any participants
involved. The most comprehensive way to do this is by making use of frameworks and guidelines
provided by ethical review boards and legal regulatory bodies. For instance, the EU’s General Data
Protection Regulation (GDPR) requires a Data Privacy Impact Assessment (DPIA) to be carried out
before the start of any project, so as to ensure a privacy-by-design approach. This framework was
utilized by Martin-Ruiz et al. (2018) to assess the potential risks to individual privacy in their study of
using digitalized toys to process children’s health data. In their report, the researchers addressed the
6six protection goals regarding the collection and usage of participants’ personal data, which included
elements such as data availability, integrity, confidentiality, unlinkability, transparency, and
intervenability (Bieker et al., 2016). As exemplified in this study, addressing the privacy concerns
associated with handling digital data requires that both guidance from regulatory bodies and
context-specific areas (e.g., protocols for vulnerable groups such as children) are consulted.
It is also not uncommon for researchers to look to the ToS of the various organizations and online
platforms responsible for providing the data to ascertain whether and how they should interact with
it (BSC, 2015). However, while these considerations are important and can offer some guidance in
this area, they are not inherently sufficient for respecting participant privacy. As discussed by Fiesler
et al. (2020), there is not a guaranteed relationship between what is permitted in a platform’s ToS
and what is best for an individual’s privacy. Looking back to the Privacy Paradox discussed in Section
3, most individuals wish for greater privacy online than they believe is attainable (Barth & Jong,
2017). Given this, we can reason that although a given platform’s ToS may allow for something to be
done with a user's data, this does not guarantee that that individual is meaningfully consenting to
their data being used in such a way. Therefore, although it is important to comply with all relevant
ToS, this alone may be insufficient to guarantee ethical privacy preservation. Beyond this,
researchers must assess the extent to which participant privacy can be maintained within their
studies and consider which methods or practices would be most suited for this. To focus
specifically on AI, there is a growing body of research into ethical AI that should be consulted when
planning studies. The addresses topics of transparency, justice and fairness and responsibility. Some
notable points of reference include Jobin et al. (2019), Stahl and Wright (2018) and Prem (2023).
4.2 Data Collection and Usage
Informed consent, in particular, has emerged as a standard problem in research involving big data,
and is a major area of consideration in the initial collection of participant data. Such research uses a
range of data collection techniques, including automated scraping APIs (as adopted to train many of
today’s LLMs), as well as datasets collected by other researchers or organizations. As personally
identifiable information or sensitive information is often being collected, strong steps are required
to protect the identity of individual participants and, where possible, to obtain their informed
consent to the research being carried out upon them or their data (AoIR, 2019). Gaining this consent
is usually impracticable in the case of big data research, however, so researchers have been forced
to rely upon different processes to ensure participant privacy is appropriately safeguarded. Some
researchers try to obtain first-degree informed consent or retroactive consent from relevant
participants before publishing any findings, whilst others focus on deleting names and other highly
identifiable information from the dataset (Bechmann & Kim, 2020). Ultimately, it is important to
not only use guidance from up-to-date ethical guidelines and regulation regarding participant
consent, but to also reflect on examples from context-specific research to fully understand the
privacy implications of the research for certain communities as well as for individual participants.
As mentioned above, in addition to generating new empirical data for analysis, researchers also
make use of previously collected data from other researchers or organizations. A small amount of
such research has also made use of data that has been accessed and released through illicit means,
including as a result of an unintended disclosure by the data owner, an unauthorized leak by
someone with access to the data, or an exploitation of a vulnerability in a computer system (Thomas
et al., 2017). For instance, several researchers conducted studies using datasets released by
WikiLeaks (a non-profit organization that publishes information leaks provided by anonymous
sources), including modeling and predicting conflicts using the Iraq War Logs datasets (Linke et al.,
2012). These datasets reduce the time and effort required for researchers to collect such data
themselves, but raise profound privacy concerns since the informed consent and privacy of the
7participants involved has already been breached. In such cases, researchers must be able to prove
that their intended research is of high social value, and that the benefits clearly outweigh any
relevant harms (Ienca & Vayena, 2021). It is, therefore, vital to involve independent review
committees to help assess the risks and benefits of the research before data collection begins.
Moreover, when integrating privacy protection into their data handling, researchers must also
consider strategies for data storage. Here, the researcher must consider how they intend to store
their data, and the safeguards they intend to put in place to ensure its protection. At a minimum, it
is crucial that all data be stored behind secured machines to minimize their access to only those
intended, and, where possible, further encryption of the datasets should be used to provide added
protections (AoIR, 2019). In the context of Generative AI systems, one additional point to note is
that they are vulnerable to attacks which can expose personal data including chat messages and
device information. This has been demonstrated multiple times as seen in Newman and Greenberg
(2023) and Toulas (2023). It highlights the importance of managing access as well as engagement
with interactive systems.
Additionally, further considerations must be made if the researchers opt to utilize cloud-based
facilities to store their data or disseminate it to other researchers. This can also be relevant where
data is uploaded for processing, as will be covered later in this section. By using these services,
issues of data ownership come into play, and the researchers risk ceding some control of this data to
these third-parties, whose security measures can often be hard to audit with confidence, thereby
risking participant privacy (Timmermans et al., 2010). Therefore, researchers should give due
consideration to the degree to which cloud-based solutions can provide adequate privacy
protections to their datasets, and may wish to consider alternative forms of data storage where
possible.
Beyond using the collected data for their personal analyses, it is typical that researchers should seek
to disseminate their datasets to the broader academic community. However, with the sharing of
data comes the potential for added risk to the privacy of participants within the dataset. In order to
mitigate this, the first consideration that researchers should make is the methods by which they
shall anonymize their datasets prior to sharing. Where possible, it is good practice to ensure that
any recipients of the dataset are unable to identify the participants whose data they are accessing.
To do this, researchers will typically make efforts to remove identifying features and
pseudonymize any names or usernames present in the dataset. These are important steps to
ensuring that participant privacy is safeguarded.
While previous research has found that many of these anonymization techniques can be bypassed
(Narayanan & Shmatikov, 2008), this does not inherently invalidate the use of anonymization, which
can still offer some protections. However, this does mean that anonymization alone may not be
sufficient to safeguard privacy. Therefore, researchers should make further considerations of the
risks that these adversarial de-anonymization attacks pose to participants in the dataset, and weigh
up whether these risks can be justified when sharing the data. To further protect against this,
researchers can also take additional steps to vet potential data recipients, rather than releasing the
data en masse to the public, which can help further protect participants. Finally, researchers should
ensure that they provide good documentation in any publications accompanying the dataset,
providing clear descriptions of the steps that have been taken to protect their datasets and the
participants within them. In summary, the common adage broadly applies: “research data should
be as open as possible but as closed as necessary” in order to ensure a balance is struck between
availability and privacy protection (Landi et al., 2020).
Beyond this, researchers should also ensure that processes are established for scenarios in which
their datasets are inadvertently leaked. The exact approach taken by researchers is dependent on
the manner of the data being stored and the degree to which prior consent has been obtained, so
8careful consideration is essential to ensuring that every possible step is taken to preserve participant
privacy as much as possible when storing personal data. Therefore, it is vital that researchers
consider any containment protocols that are to be used, the manner in which backups of datasets
are taken and stored, and whether (and how) they intend to notify participants should their data
be leaked (AoIR, 2019).
4.3 Analysis and Dissemination of Results
Beyond data handling, it is also crucial that researchers ensure that any analysis or model creation
conducted is done with due respect to participant privacy. In terms of analysis of participant data
directly, it is thus essential that researchers give considerable thought to the issues posed by
identification (AoIR, 2019). This concerns the potential for participant identities included in any
datasets to be identified based on the results of the analyses conducted. This also covers the ability
of any readers to identify participants through the subsequent publication of these results.
Consideration of this is crucial, as failures here could undo any attempts at anonymization made at
the data gathering stage. As an example, if quotations of a given online post are published alongside
a prediction of that post’s geographic location, these details could quite easily be leveraged to
identify the user behind the post – even if the post itself is sanitized of identifying characteristics. As
such, it is necessary that researchers consider the extent to which individuals can be reidentified
through their analyses and results publication. As far as possible, researchers should ensure that
these forms of participant identification through a study’s results are not possible.
Besides issues of identification, another consideration that researchers must make is the potential
downstream impacts of their work on people’s privacy. This is particularly relevant when developing
powerful models, like the language models GPT-2/3 and BERT, which are trained on large amounts
of user data and then made available for use in a range of tasks (Bender, 2021). Ostensibly, it is
tempting to assume that if a model is trained and released with the source training data being kept
private, participant privacy is therefore maintained. However, studies have now shown that these
models are prone to “hallucinations”, whereby extracts of private information within the model’s
training data can be reproduced or predicted with relative ease (Bender, 2021). Moreover, the
nature of many of the models developed in CSS means one must account for their potential privacy-
busting applications. Models commonly developed in CSS, such as those that can predict political
leaning, personality, and location, could all be leveraged to maliciously attack the privacy of other
individuals not considered in the study (Brundage et al., 2018). Researchers must therefore consider
and evaluate their models’ susceptibility to these downstream risks, and evaluate how their
models can be safely shared in light of this. The reality is that while models may be developed for
good, they can often also be used maliciously.
5. Privacy Protection: A Researcher’s Responsibility
Aligning privacy with the aims of CSS and AI research can be difficult. Privacy considerations are vital
at every step of a research project, and failure at any stage could result in genuine harm being
exacted on participants, who are often unaware that their data is even being used. As researchers, it
is therefore of utmost importance to acknowledge the position of responsibility that we hold, and to
ensure that any studies conducted using large-scale personal data is approached in a way that
respects each individual’s reasonable expectations of privacy and physically shields them from harm.
It is essential that we as a community recognize the importance of these responsibilities and strive
to uphold the highest standards of privacy protection in our research; that we constantly work to
improve our privacy-protection strategies, that we develop all research projects with a mind towards
9privacy, and that each step of our research is conducted using the highest standards of privacy
protection. This involves acknowledging not only the key considerations highlighted in this section,
but the further considerations that present themselves across the innumerable ways in which
research can be conducted.
Whilst we endeavor to provide a clear argument for the importance of privacy, and indicate some
ways in which basic privacy protections can be integrated, it is imperative that researchers take the
initiative in continuously developing their understanding of how privacy can best be integrated into
their work. In essence, it is vital that we, as a community, develop a ‘privacy-aware’ mindset, in
order to ensure that privacy protection is made a priority at every step of our research. In doing so,
we will ensure that our work is conducted to the highest standard, and, most importantly, that no
harm befalls those whose data forms the backbone of our research contributions.
10References
The Association of Internet Researchers (AoIR). (2019). Internet Research: Ethical Guidelines 3.0.
Retrieved from http://aoir.org/ethics/.
Badjatiya, P., Gupta, S., Gupta, M., & Varma, V. (2017). Deep Learning for Hate Speech Detection in
Tweets. In Proceedings of the 26th International Conference on World Wide Web, 759-760. ACM.
Barth, S., Jong, D. T. (2017). The Privacy Paradox – Investigating Discrepancies Between Expressed
Privacy Concerns and Actual Online Behavior – A Systematic Literature Review. Telematics and
Informatics, 34(7), 1038-1058.
BBC. (2023). ChatGPT banned in Italy over privacy concerns. Retrieved from
https://www.bbc.co.uk/news/technology-65139406.
Bechmann A., Kim J. Y. (2019). Big Data. In: Iphofen R. (eds) Handbook of Research Ethics and
Scientific Integrity. Springer.
Bender, Emily M., Gebru, T., McMillan-Major, A., Shmargaret, M. (2021). On the Dangers of
Stochastic Parrots: Can Language Models Be Too Big?. In Proceedings of the 2021 ACM Conference
on Fairness, Accountability, and Transparency, 610-623. ACM.
Bieker, F.; Friedewald, M.; Hansen, M.; Obersteller, H. & Rost, M. (2016). A Process for Data
Protection Impact Assessment under the European General Data Protection Regulation. Proceedings
of the 4th Annual Privacy Forum, 21–37. Springer.
British Society of Criminology (BSC). (2015). Statement of Ethics. Retrieved from
https://www.britsoccrim.org/ethics/.
British Sociological Association (BSA). (2017). Ethics Guidelines and Collated Resources
for Digital Research. Retrieved from https://www.britsoc.co.uk/ethics.
Brundage, M., Avin, S., Clark, J., Toner, H., Eckersley, P., Garfinkel, B., Dafoe, A., Scharre, P., Zeitzoff,
T., Filar, B., Anderson, H., Roff, H., Allen G. C., Steinhardt, J., Flynn, C., hÉigeartaigh, S., Beard, S.,
Belfield, H., Farquhar, S., … Amodei, D. (2018). The Malicious Use of Artificial Intelligence:
Forecasting, Prevention, and Mitigation. arXiv. https://arxiv.org/abs/1802.07228.
DeCew, J. (2018). Privacy. In E. N. Zalta (Ed.). The Stanford Encyclopedia of Philosophy. Retrieved
from https://plato.stanford.edu/entries/privacy/.
DeNardis, L., Hackl, A. M. (2015). Internet governance by Social Media Platforms.
Telecommunications Policy, 39(9), 761-770.
Earl, J., & Schussman, A. (2002). The New Site of Activism: Online Organization, Movement
Entrepreneurs, and the Changing Location of Social Movement Decision Making. Research in Social
Movement, Conflict and Change, 24, 155-187.
Ebner, J., Kavanagh, C., & Whitehouse, H. (2024). Measuring socio-psychological drivers of extreme
violence in online terrorist manifestos: an alternative linguistic risk assessment model. Journal of
Policing, Intelligence and Counter Terrorism, 19(2), 125-143.
11Fiesler, C., Beard, N., Keegan, B. C., (2020). No Robots, Spiders, or Scrapers: Legal and Ethical
Regulation of Data Collection Methods in Social Media Terms of Service. In Proceedings of the 14th
International AAAI Conference on Web and Social Media, 187-196. AAAI.
Frenda, S., Ghanem, B., Montes-y-Gómez, M., & Rosso, P. (2019). Online Hate Speech against
Women: Automatic Identification of Misogyny and Sexism on Twitter. Journal of Intelligent & Fuzzy
Systems, 36(5), 4743–4752.
ICO. (2024). Generative AI first call for evidence: The lawful basis for web scraping to train
generative AI models. Retrieved from https://ico.org.uk/about-the-ico/what-we-do/our-work-on-
artificial-intelligence/generative-ai-first-call-for-evidence/.
Ienca, M., Vayena, E. (2021) Ethical Requirements for Responsible Research With Hacked Data.
Nature Machine Intelligence, 3, 744–748.
Isaak, J., Hanna, M. (2018). User Data Privacy: Facebook, Cambridge Analytica, and Privacy
Protection. Computer, 51(8), 59-58.
Japan Times. (2023). Police to scan social media with AI in hunt for online crime. Retrieved from
https://www.japantimes.co.jp/news/2023/09/28/japan/crime-legal/japan-police-ai-use/.
Jobin, A., Ienca, M., & Vayena, E. (2019). The global landscape of AI ethics guidelines. Nature
machine intelligence, 1(9), 389-399.
Jones, K., Nurse, J. R. C., & Li, S. (2020). Behind the Mask: A Computational Study of Anonymous’
Presence on Twitter. In Proceedings of the 14th International AAAI Conference on Web and Social
Media, 327-338. AAAI.
Katal, A., Wazid, M., Goudar, R. H. (2013). Big Data: Issues, Challenges, Tools and Good Practices. In
Proceedings of the 6tth International Conference on Contemporary Computing, 404-409. IEEE.
Landi, A., Thompson, M., Giannuzzi, V., Bonifazi, F., Labastida, I., Silva Santos, L.O.B., Roos, M.
(2020). The “A” of FAIR – As Open as Possible, as Closed as Necessary. Data Intelligence, 2(1-2), 47-
55.
Langheinrich, M., Coroamă, V., Bohn J., Mattern, F. (2005). Living in a Smart Environment -
Implications for the Coming Ubiquitous Information Society. Telecommunications Review, 15(1),
132-143.
Lazer, D. M., Pentland, A., Watts, D. J., Aral, S., Athey, S., Contractor, N., Freelon, D., Gonzalez-
Bailon, S., King, G., Margetts, H., Nelson, A., Salganik, M. J., Strohmaier, M., Vespignani, A., &
Wagner, C. (2020). Computational social science: Obstacles and opportunities. Science, 369(6507),
1060-1062.
Linke, A. M., Witmer, F. D. W., & O'Loughlin, J. (2012). Space-Time Granger Analysis of the War in
Iraq: A Study of Coalition and Insurgent Action-Reaction. International Interactions, 38(4), 402-425.
Martín-Ruíz, M.L., Fernández-Aller, C., Portillo, E., et al. (2018). Developing a System for Processing
Health Data of Children Using Digitalized Toys: Ethical and Privacy Concerns for the Internet of
Things Paradigm. Science and Engineering Ethics, 24, 1057–1076.
12Narayanan, A., & Shmatikov, V. (2008). Robust De-Anonymization of Large Sparse Datasets. In
Proceedings of the 29th IEEE Symposium on Security and Privacy, 111-125. IEEE.
Newman, L.H. & Greenberg, A., (2023). ChatGPT Spit Out Sensitive Data When Told to Repeat ‘Poem’
Forever. Retrieved from https://www.wired.com/story/chatgpt-poem-forever-security-roundup/.
Nouh, M., Nurse, J. R., & Goldsmith, M. (2019). Understanding the Radical Mind: Identifying Signals
to Detect Extremist Content on Twitter. In Proceedings of the IEEE International Conference on
Intelligence and Security Informatics, 98-103. IEEE
Obar, J. A., Oeldorf-Hirsch, A. (2018). The Biggest Lie on the Internet: Ignoring the Privacy Policies
and Terms of Service Policies of Social Networking Services. Information, Communication & Society,
23(1), 128-147.
Patel, A. (2019). Malicious Use of AI. F-Secure. Retrieved from https://blog.f-secure.com/malicious-
use-of-ai/.
Pereira, C., da Silva, R., & Rosa, C. (2024). How to measure political polarization in text-as-data? A
scoping review of computational social science approaches. Journal of Information Technology &
Politics, 1-14.
Pesheva, E. (2023). AI Predicts Future Pancreatic Cancer. Retrieved from
https://hms.harvard.edu/news/ai-predicts-future-pancreatic-cancer.
Prem, E. (2023). From ethical AI frameworks to tools: a review of approaches. AI and Ethics, 1-18.
Rahman, W. (2020). The Netflix Prize — How Even AI Leaders Can Trip Up. Towards Data Science.
Retrieved from https://towardsdatascience.com/the-netflix-prize-how-even-ai-leaders-can-trip-up-
5c1f38e95c9f.
Rastgoftar, H., & Atkins, E. (2021). A Mass-Conservation Model for Stability Analysis and Finite-Time
Estimation of Spread of COVID-19. IEEE Transactions on Computational Social Systems, 8(4), 930–
937.
Stahl, B. C., & Wright, D. (2018). Ethics and privacy in AI and big data: Implementing responsible
research and innovation. IEEE Security & Privacy, 16(3), 26-33.
Thomas, D., Pastrana Portillo, S. P., Hutchings, A., Clayton, R., & Beresford, A. R. (2017). Ethical issues
in Research Using Datasets of Illicit Origin. In Proceedings of the 2017 Internet Measurement
Conference, 445-462. ACM.
Timmermans, J., Stahl, B.C., Ikonen, V., Bozdag, E. (2010). The Ethics of Cloud Computing: A
Conceptual Review. In Proceedings of the 2010 IEEE Second International Conference on Cloud
Computing Technology and Science, 614-620. IEEE.
Toulas, B. (2023). OpenAI rolls out imperfect fix for ChatGPT data leak flaw. Retrieved from
https://www.bleepingcomputer.com/news/security/openai-rolls-out-imperfect-fix-for-chatgpt-data-
leak-flaw/.
13Towsend, L., Wallace, C. (2016). Social Media Research: A Guide to Ethics. University of Aberdeen.
Retrieved from https://www.bolton.ac.uk/assets/Uploads/Social-media-ethics-study-Aberdeen-
2018.pdf.
Twitter. (2021). Twitter User Agreement. Retrieved from https://twitter.com/en/tos.
Wang, R., Ji, C., Jiang, Z., Wu, Y., Yin, L., & Li, Y. (2021). A Short-Term Prediction Model at the Early
Stage of the COVID-19 Pandemic Based on Multisource Urban Data. IEEE Transactions on
Computational Social Systems, 8(4), 938–945.
Zahrah, F., Nurse, J. R. C., & Goldsmith, M. (2020). #ISIS vs #ActionCountersTerrorism: A
Computational Analysis of Extremist and Counter-extremist Twitter Narratives. In Proceedings of the
2020 IEEE European Symposium on Security and Privacy Workshops, 438-447. IEEE.
Zheng, X., Han, J., Sun, A. (2018). A Survey of Location Prediction on Twitter. IEEE Transactions on
Knowledge and Data Engineering, 30(9), 1652-1671.
14