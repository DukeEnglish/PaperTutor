Related Work and Citation Text Generation: A Survey
XiangciLi JessicaOuyang
DepartmentofComputerScience
UniversityofTexasatDallas
Richardson,TX75080
lixiangci8@gmail.com,
Jessica.Ouyang@UTDallas.edu
Abstract focusedmulti-documentsummarization. Moreover,
sincemostNLPpapershaveanRWSandNLPre-
Toconvincereadersofthenoveltyoftheirre-
searchersarenaturaldomainexpertsforevaluating
search paper, authors must perform a litera-
theseRWS,theRWGtaskisanexcellenttestbed
turereviewandcomposeacoherentstorythat
forexaminingthecapabilityofSOTANLPmodels.
connects and relates prior works to the cur-
rentwork. Thischallengingnatureofliterature RWGalsofillsapracticalneed. Duetotherapid
reviewwritingmakesautomaticrelatedwork paceofresearchpublications,includingpre-prints
generation (RWG) academically and compu- thathavenotyetbeenpeer-reviewed,keepingupto
tationallyinteresting,andalsomakesitanex-
datewiththelatestworkinaresearchareaisvery
cellenttestbedforexaminingthecapabilityof time-consuming. Evenwithdailyfeedtools,like
SOTAnaturallanguageprocessing(NLP)mod-
theSemanticScholarResearchFeed1,researchers
els. SincetheinitialproposaloftheRWGtask,
still have to curate, read, and digest all the new itspopularityhaswaxedandwaned,following
thecapabilitiesofmainstreamNLPapproaches. papersintheirfeed. Thus,thereisaneedforcon-
Inthiswork, wesurveythezooofRWGhis- cise,automaticallygeneratedliteraturereviewsthat
toricalworks,summarizingthekeyapproaches regularlysummarizethepapersinauser’sfeed.
andtaskdefinitionsanddiscussingtheongoing
SinceHoangandKan(2010)initiallyproposed
challengesofRWG.
the task, the popularity of RWG has waxed and
1 Introduction waned, following the capabilities of mainstream
NLP approaches: from rule-based to extractive
Academic research is an exploratory activity to
summarization,thentoabstractivesummarization
solve problemsthat have neverbeen resolvedbe-
on the sentence level, and finally to abstractive
fore. Each academic research paper must sit at
section-level RWG. Currently there is a surge of
thefrontierofthefieldandpresentnoveltiesthat
renewedinterestinRWGduetotherecentsuccess
havenotbeenaddressedbypriorwork;toconvince
oflargelanguagemodels(LLMs). Inthiswork,we
readersofthenoveltyofthecurrentwork,theau-
surveythezooofRWGhistoricalworks.
thorsmustperformaliteraturereviewtocompare
Wefindthat,surprisingly,mostRWGworksare
theirworkwiththepriorwork. Innaturallanguage
notdirectlycomparablebecausetheyvarydrasti-
processing(NLP),ashortliteraturereviewisusu-
callyintaskdefinitionandsimplifyingassumptions
allyconductedunderthe“RelatedWork”section
(Section2),aswellasusingdifferentinputfeatures
(RWS).WritinganRWSisnon-trivial;itisinsuffi-
andrepresentations(Section3). Thereisnostan-
cienttosimplyconcatenategenericsummariesof
dard benchmark dataset for RWG (Section 4), as
priorworks. Instead,composingacoherentstory
mostworksapplycustompreprocessingtoextract
thatconnectseachrelatedworkandthecurrent(cit-
RWSorindividualcitations,reflectingdifferences
ing)work,reflectingtheauthor’sunderstandingof
in their task definitions. Further, many works do
theirfield,ispreferred(LiandOuyang,2024).
not release their models or generated outputs, so
ThechallengingnatureofRWSwritingmakes
it is often impossible for later works to compare
automaticrelatedworkgeneration(RWG)anaca-
againstearlierapproaches(Section5). Finally,we
demically and computationally interesting prob-
discussethicalconcernsrelatedtoRWG,suchas
lem. RWGisacomplextaskthatinvolvesmultiple
NLP subtasks, such as retrieval-augmented gen-
1https://www.semanticscholar.org/faq/what-are-research-
eration,longdocumentunderstanding,andquery- feeds
4202
rpA
71
]LC.sc[
1v88511.4042:viXraOutputUnit CitedPaperInput CitationOrder/Grouping Availability
Sent. Para. Sect. Excerpts FullText Given Predicted Code Data
Extractive
HoangandKan(2010) ✓ ✓ ✓ ✓
HuandWan(2014) ✓ ✓ ✓
Wangetal.(2018) ✓ ✓ ✓ ✓
ChenandZhuge(2019) ✓ ✓ ✓
Wangetal.(2019) ✓ ✓ ✓ ✓
Dengetal.(2021) ✓ ✓ ✓
Abstactive(citation)
AbuRa’edetal.(2020) ✓ ✓ ✓ ✓
Xingetal.(2020) ✓ ✓ ✓
Geetal.(2021) ✓ ✓
Luuetal.(2021) ✓ ✓ ✓
Jungetal.(2022) ✓∗ ✓ ✓ ✓
Lietal.(2022) ✓∗ ✓ ✓ ✓
GuandHahnloser(2023) ✓ ✓ ✓ ✓
Lietal.(2023) ✓∗ ✓† ✓ ✓
Mandaletal.(2024) ✓∗ ✓ ✓ ✓
Abstractive(section)
Chenetal.(2021) ✓ ✓ ✓ ✓
Chenetal.(2022) ✓ ✓ ✓ ✓
Liuetal.(2023) ✓ ✓ ✓ ✓
LiandOuyang(2024) ✓ ✓† ✓ ✓‡
Martin-Boyleetal.(2024) ✓ ✓ ✓∗∗ ✓‡ ✓
Table 1: Comparison of the task definitions of extractive and both single-citation and full-section abstractive
approachestorelatedworkgeneration. *indicatesworksthatallowmulti-sentencecitations. †indicatesworks
thatextractsnippets/featuresfromthecitedpaperfulltext. **indicatesworksthatusehumaneditingtoimprove
predictedcitationgroupings. ‡indicatesworksthatprovidelargelanguagemodelprompts.
plagiarismandnon-factualstatements,andthepo- tences: HoangandKan(2010);Wangetal.(2018);
tential consequences of fully automatic RWG on Chen and Zhuge (2019); Wang et al. (2019) as-
thehumanprocessofscientificthinkingandwrit- sumed the correct ordering as input (either via a
ing(Section6.3). human-constructed topic tree or the ground truth
ordering of the target RWS), while Hu and Wan
2 TaskDefinition (2014); Deng et al. (2021) used topic modeling
andasentencereorderingmodule,respectively,to
The task definition for RWG has varied as the
predictanordering.
SOTA text summarization approach has evolved
over time. Even where the overall approach is 2.2 AbstractiveRelatedWorkGeneration
similar(e.g. extractiveorabstractiveapproaches),
With the advent of neural language models, two
differentassumptionsaremadewithrespecttothe
differentversionsoftheabstractiveRWGtaskhave
availabilityofsysteminputsandtheunitatwhich
beenproposed: generatingsinglecitationtextsver-
anRWSisgenerated(Table1).
susparagraphsorfullRWS.
2.1 ExtractiveRelatedWorkGeneration 2.2.1 CitationTextGeneration
Hoang and Kan (2010) defined RWG as generat- Earlyneurallanguagemodels,suchasthePointer-
ingtheRWSofatargetpapergiventherestofthe Generator (See et al., 2017) and early pretrained
targetpaperandallcitedpapers. Thisfocusonex- Transformers (Vaswani et al., 2017)), were capa-
tractingandconcatenatingsalientsentencesfrom ble of fluent abstractive summarization but had
thecitedpaperstoformanRWSwasusedbymost severeinputlengthrestrictions. Becausescientific
subsequent extractive RWG approaches (Hu and research papers are very long documents, a new
Wan,2014;Wangetal.,2018;Dengetal.,2021). versionoftheRWGtaskarose: generatingindivid-
OnekeyvariantisthatofChenandZhuge(2019); ual citation texts. The system input now needed
Wangetal.(2019),whoextractedsentencesfrom to include only one or a few cited papers, and to
other works that also referenced the cited papers. further shorten the system input, researchers no
Otherwise, the main difference among extractive longerincludedthefulltextsofthetargetandcited
approachesisinhowtheyordertheextractedsen- papers, but used only the target citation contextandthecitedpaperabstract(andoccasionallythe abstractive approaches focus on directly generat-
introductionandconclusionsections). ingthesummary,oftenwithoutexplicitlymodeling
Themaindifferenceamongsinglecitationtext salience. Inthissection,wedonotgiveanexhaus-
generation works is in how a citation is defined. tivedescriptionofallmethodologies,buthighlight
AbuRa’edetal.(2020);Xingetal.(2020);Geetal. some common features and design perspectives
(2021);Luuetal.(2021);GuandHahnloser(2023) fromtheoverallbodyofRWGwork(summarized
restrictcitationtextstobesinglesentences; Jung inTable2). Thedetailsofindividualworkscanbe
etal.(2022)allowanynumberofconsecutivesen- foundinAppendixA.
tences,whileLietal.(2022,2023);Mandaletal.
3.1 RepresentingCitedPapers
(2024)additionallyallowcitationsthatareshorter
thanafullsentence. Almostallworksrestrictcita- Abstracts. In abstractive RWG approaches, and
tionstocontainonlyonecitedpaper;onlyLietal. some extractive approaches, the cited paper title
(2022,2023);Mandaletal.(2024)explicitlyallow and abstract are commonly used as a proxy for
multiplecitedpapers. its full text (AbuRa’ed et al., 2020; Xing et al.,
2020;Geetal.,2021;Chenetal.,2021,2022;Jung
2.2.2 Section-LevelGeneration et al., 2022; Li et al., 2022; Gu and Hahnloser,
Chen et al. (2021, 2022) pioneered section-level 2023;Liuetal.,2023;Mandaletal.,2024;Martin-
RWGbytreatingtheparagraphastheunitofgener- Boyleetal.,2024),occasionallyaugmentedwith
ation;theyrequiredthatatargetparagraphcontain the introduction and/or conclusion (Hu and Wan,
atleasttwocitations,explicitlydistinguishingtheir 2014;ChenandZhuge,2019;Dengetal.,2021).
work from the single citation text generation set- Theabstractisaconcisesummaryofthecentral
ting. WhileChenetal.(2021,2022)usedthegiven ideasofthecitedpaperandcanfitinaneurallan-
paragraphorganizationofthetargetRWS,subse- guagemodel’sinputlengthlimitwherethefulltext
quentworksfocusedonorderingandorganizingci- cannot. Abstracts also play an important role in
tationsintoparagraphsandgeneratingtransitional scientificcommunicationasapreviewofthepaper,
sentences between citations (Liu et al., 2023; Li sotheyareeasytoaccessevenwhentheirfullstext
andOuyang,2024;Martin-Boyleetal.,2024). are blocked by paywalls. Li and Ouyang (2024)
findthatgeneratedRWSconditionedoncitedpaper
Further, the great success of SOTA LLMs in
abstractsarepreferredbyhumanreadersoverthose
multiplenaturallanguageunderstandingandgener-
conditionedonLLM-generatedfacetedsummaries
ationtasks,combinedwiththeirlargecontextwin-
(Mengetal.,2021)ofthecitedpapers.
dows,haverecentlymadeitpossibletogeneratea
Cited Text Spans (CTS). Li et al. (2023) pro-
full RWS in a single pass (Li and Ouyang, 2024;
posedtoconditiononautomaticallypredictedCTS
Martin-Boyle et al., 2024). Thus, the task defini-
ratherthancitedpaperabstracts. CTSreferstothe
tionhasnowreturnedtothefullRWSgeneration
specificspanofthecitedpaperthatagivencitation
originallyproposedbyHoangandKan(2010)and
refersto;todrawaparalleltoclaimverification,the
previouslytackledonlybyextractiveapproaches.
citationcanbethoughtofastheclaim,andtheCTS
3 OverviewofApproaches as its supporting evidence. Thus, Li et al. (2023)
effectively proposed an extract-then-abstract ap-
WhenHoangandKan(2010)proposedtheRWG proachtocitationtextgeneration,arguingthatthe
task,theyidentifiedthreemainsteps: (1)Finding citedpaperabstractmaynotalwayscontainsuffi-
relevantdocuments,(2)Identifyingthesalientas- cient informationto ground the target citation. It
pectsofthesedocumentswithrespecttothecurrent isinterestingtonotethatCTShadpreviouslybeen
work;(3)Generatingatopic-biasedsummary. In used for extractive RWG by Wang et al. (2019),
practice,allexistingworksskipthedocumentre- whoextractedCTSforothercitationsofthecited
trievalstepbyusingthegoldcitedpaperlistinthe paperinworkssimilartothetargetpaper.
targetRWS.Atahighlevel,themethodologiesof CitationGraphs. SinceanRWSdescribesthe
mostextractive,citation-levelandsection-levelab- relationship between the target paper and prior
stractiveRWGapproachesaresimilarwithintheir work,aswellasamongpriorworks,somesection-
respectivecategories: extractiveapproachesfocus levelRWGapproacheshavemodeledthelocalci-
on the salience step and simply concatenate the tationnetworkofthetargetandcitedpapers. Wang
extracted sentences to form the summary, while et al. (2018) used a random walk on a heteroge-CitedPaperRepresentation* TargetPaperContext CitationAnalysisUse
Intro. RWS Concl. CTS Graph Abs. Intro. RWS† Concl. MTL Control Eval.
Extractive
HoangandKan(2010) ✓ ✓ ✓
HuandWan(2014) ✓ ✓ ✓ ✓ ✓
Wangetal.(2018) ✓ ✓ ✓ ✓
ChenandZhuge(2019) ✓ ✓ ✓ ✓ ✓
Wangetal.(2019) ✓ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Dengetal.(2021) ✓
Abstactive(citation)
AbuRa’edetal.(2020)
Xingetal.(2020) ✓
Geetal.(2021) ✓ ✓ ✓
Luuetal.(2021) ✓
Jungetal.(2022) ✓ ✓
Lietal.(2022) ✓
GuandHahnloser(2023) ✓ ✓
Lietal.(2023) ✓ ✓
Mandaletal.(2024) ✓
Abstractive(section)
Chenetal.(2021) ✓
Chenetal.(2022) ✓ ✓
Liuetal.(2023) ✓
LiandOuyang(2024) ✓∗∗ ✓∗∗ ✓∗∗ ✓ ✓ ✓ ✓ ✓ ✓ ✓
Martin-Boyleetal.(2024) ✓ ✓ ✓ ✓
Table2: ComparisonofRWGapproaches. *Allsurveyedworksusedcitedpapertitlesandabstracts,whicharenot
listinthistable. †Thetargetcitationitselfismasked. ✓∗∗indicatesfeaturesextractedfromthelistedsections.
neousbibliographygraphconsistingofpaper,au- ChenandZhuge,2019;Wangetal.,2019)usedthe
thor,venue,andkeywordnodestoprunethesearch contextasaquerytoscorecitedpapersentences. In
spaceofsalientsentencesforextractiveRWG.Ge abstractiveapproaches,conditioningonthecontext
et al. (2021); Chen et al. (2021, 2022) used cus- improvesthecoherenceofthegeneratedtextwith
tomizedneuralnetworkarchitecturesinspiredby the rest of the target paper; Mandal et al. (2024)
GraphAttentionNetworks(Velickovicetal.,2018) foundhumanreaderspreferredcitationsgenerated
toencodethelocalcitationnetworkasanadditional using the entire context, with the target citation
inputforabstractiveRWG,whileLiandOuyang embeddedinsideit,asthegenerationtarget.
(2024)promptedanLLMtogenerateanaturallan- Itisinterestingtonotethatafewworksdidnot
guagedescriptionoftherelationshipbetweenapair useanytargetpapercontextatall(HoangandKan,
ofpapersinthecitationnetwork. 2010; AbuRa’ed et al., 2020; Chen et al., 2021),
butthesewereearlyworksintheirrespectivecat-
3.2 TheImportanceofCitationContext egories (extractive versus abstractive citation- or
section-levelgeneration),andlaterworksallused
Citationcontextreferstothetextprecedingorsur-
targetpapercontext.
rounding the target citation or RWS. In the case
of individual citations, the context is commonly
3.3 ApplyingCitationAnalysis
definedasseveralsentencesbefore,andoptionally
after,thetargetcitation(Xingetal.,2020;Geetal., Citationanalysisisarelatedareaofresearchstudy-
2021; Li et al., 2022, 2023; Mandal et al., 2024); ingthepropertiesofcitationsinscientificwriting.
forsomecitationtextgenerationworksandmost Severalstudieshaveproposedtaxonomiessuchas
section-levelRWGworks, thecontextcanbethe citationfunction(Garfieldetal.,1965;Teufeletal.,
fulltextofthetargetpaper,orafewkeysections, 2006;DongandSchäfer,2011;Jurgensetal.,2018;
most commonly the title, abstract, introduction, Tuarobetal.,2019;Zhaoetal.,2019),citationin-
andconclusion(Luuetal.,2021;Jungetal.,2022; tent(Cohanetal.,2019;Lauscheretal.,2021),and
GuandHahnloser,2023;Chenetal.,2022;Liand citationsentiment(Athar,2011;AtharandTeufel,
Ouyang,2024;Martin-Boyleetal.,2024). 2012;Ravietal.,2018),andsuchlabelshavebeen
Intuitively,thecontextindicateswhichtopicsare usedtoimproveRWGperformance.
salienttothetargetpaper,restrictingtheRWGso- Ge et al. (2021) used citation function predic-
lutionspace. Extractiveworks(HuandWan,2014; tion as an auxiliary training objective. Jung et al.(2022);GuandHahnloser(2023)usedcitationin- written short summary of the main ideas of the
tents to perform controllable citation text genera- targetRWS.Alsoforsection-levelRWG,Martin-
tion. InspiredbytheobservationofLauscheretal. Boyleetal.(2024)introducedahuman-in-the-loop
(2022) that simple citation label sets struggle to componentwheretheusereditedanpredictedcited
representambiguous,real-worldcitations,Liand papergroupingbeforethegenerationstep.
Ouyang(2024)usedLLM-generated,naturallan-
4 Datasets
guagedescriptionsoffunctionofacitedpaperin
other,similarworksthatalsocitedit. DespitethetwentypublishedworksonRWG,there
Otherworkhasstudiedthediscourseproperties isnostandardbenchmarkdatasetforthetask. As
and organization of citations. Jaidka et al. (2010, we discussed in Section 2, most RWG works de-
2011);Khooetal.(2011);Jaidkaetal.(2013b,a) finetheirownversionofthetask;theyalsocreate
classifiedliteraturereviewsintointegrative(sum- theirowndatasets,adaptedtotheirparticulartask
marizingindividualcitedpapers)anddescriptive definition. In this section, we describe the most
(focusingonhigh-levelideasfrommultiplepapers) commonlyusedsourcesofscientificarticles(Table
writing styles. Li et al. (2022) proposed a more 3)andsummarizehowRWGworkshavebuilton
fine-grainedtaxonomyatthecitationlevel,label- thesesources. Thedetailsofeachwork’sdatasets
ingcitationsasdominant(themainfocusoftheir canbefoundinAppendixTable6.
sentence)orreference(tangentialtotherestoftheir
4.1 CommonDatasets
sentence).
LiandOuyang(2024)usedthistaxonomytoan- The ACL Anthology Network (AAN) Corpus
alyzethewritingstyleofLLM-generatedRWSand (Radev et al., 2013) consists of papers published
observedastrongcorrelationbetweenthepropor- bytheAssociationforComputationalLinguistics
tionofreference-typecitationsandhumanprefer- (ACL).Foreachpaper,itannotatesthesetofsen-
encescores,concludingthathumanreadersprefer tencesinanyotherAANpaperthatcitethatpaper.
integrativeRWSsupportedbyreference-typecita- BothintheconstructionofAAN,aswellasinsin-
tions. Similarly,Martin-Boyleetal.(2024)found glecitationtextgenerationworksthatuseit,indi-
thatbothhuman-writtenandhuman-assisted,LLM- vidualcitationtextsareextractedviastringsearch
generatedRWShadsignificantlymorecitedpapers forcitationmarks,suchas“Smithetal. (2024)"or
persentencethanpuremachine-generatedRWS. “[1]"(Xingetal.,2020;Geetal.,2021).
3.4 Human-AssistedGeneration SciSummNet (Yasunaga et al., 2019), used by
AbuRa’ed et al. (2020); Deng et al. (2021), is a
WhileRWGmodelsareoptimizedtoreconstruct
subset 1000 papers from the AAN Corpus with
theoriginalcitationtextsorRWSintheirtraining
human-validatedcitationsentencesandsummaries.
datasets, the ultimate goal of the task is to gener-
ate an RWS that satisfies a user. Human readers Delve (Akujuobi and Zhang, 2017) consists of
are sensitive to errors in cited paper organization papersfromseveralcomputerscienceconferences
(e.g. papers cited in the same paragraph are not spanningmultiplefieldsofresearch. Itincludesau-
sufficiently related to each other) and emphasis tomaticallyextractedpaperabstractsandfulltext,
(e.g. lesssalientpapersaredescribedingreaterde- aswellascitationtextsandlinks.
tailthanmoresalientones);currently,evenSOTA
TheSemanticScholarOpenResearchCorpus
LLMsarenotcapableoforganizingandemphasiz-
(S2ORC) (Loetal.,2020)containsopen-access
ingasetofcitedpaperswithouthumanguidance
papers from multiple disciplines. The papers are
(LiandOuyang,2024;Martin-Boyleetal.,2024).
annotatedwithautomaticallydetectedinlinemen-
Thus, human input has been included in sev-
tionsofcitations,figures,andtables,whichsaves
eral RWG works. To determine the most salient
researcherstheneedtoprocessrawPDFfiles.
aspectsofacitedpaperforsinglecitationtextgen-
eration,Lietal.(2023)proposedtoretrievecited Citation Oriented Related Work Annotation
textspans(CTS)usinguser-providedkeywordsas (CORWA) (Li et al., 2022) is derived from the
queries, while Gu and Hahnloser (2023) directly ACLpartitionofS2ORCandisannotatedspecifi-
usedhuman-writtenkeywordsasanadditionalin- callyforcitationtextgeneration. CORWAlabels
put. Li and Ouyang (2024) extended this idea to citationsandtheirdiscourseroles(dominantorref-
section-levelRWGbyproposingtouseahuman- erence).DataSource Domain
AAN S2ORC Delve Other NLP/AIOnly GeneralCS Non-CS Available?
Extractive
HoangandKan(2010) ✓ ✓ ✓†
HuandWan(2014) ✓ ✓
Wangetal.(2018) ✓ ✓ ✓
ChenandZhuge(2019) ✓ ✓ ✓
Wangetal.(2019) ✓ ✓ ✓
Dengetal.(2021) ✓∗ ✓
Abstactive(citation)
AbuRa’edetal.(2020) ✓∗ ✓ ✓
Xingetal.(2020) ✓ ✓ ✓
Geetal.(2021) ✓ ✓
Luuetal.(2021) ✓ ✓ ✓
Jungetal.(2022) ✓ ✓ ✓ ✓
Lietal.(2022) ✓∗∗ ✓ ✓
GuandHahnloser(2023) ✓ ✓ ✓
Lietal.(2023) ✓∗∗ ✓ ✓
Mandaletal.(2024) ✓∗∗ ✓ ✓
Abstractive(section)
Chenetal.(2021) ✓ ✓ ✓ ✓ ✓
Chenetal.(2022) ✓ ✓ ✓ ✓ ✓
Liuetal.(2023) ✓ ✓ ✓ ✓ ✓
LiandOuyang(2024) ✓ ✓ ✓
Martin-Boyleetal.(2024) ✓ ✓ ✓
Table3: Listofcommondatasetsusedinrelatedworkgeneration. *indicatesworksthatusetheSciSummNet
subsetofAAN.**indicatesworksthatusetheCORWAsubsetofS2ORC.†indicatesworksthatpublishedtheir
datasets,buttherepositoriesarenolongeraccessible.
4.2 Discussion 5 Evaluation
5.1 Baselines
Onecommonchallengewithallexistingdatasetsis
that,foragiventargetpaper,notallofitscitedpa- As Appendix Tables 8 & 9 show, there are a few
persarenecessarilyinthedataset(e.g. becausethey baselineswidelyusedacrossRWGworks. Extrac-
arebehindapaywall). Insinglecitationtextgener- tiveworkscommonlyuseLEAD(Wasson,1998),
ationworks,suchmissingcitedpapersaresimply MEAD(Radevetal.,2004),LexRank(Erkanand
omittedfromtrainingandtesting. Forsection-level Radev,2004),andTexRank(MihalceaandTarau,
RWG,missingcitedpapersareabiggerproblem, 2004),whileabstractiveworksusenaivesequence-
astheirabsencemaydisrupttheflowofthegener- to-sequenceapproaches,withbasemodelssuchas
atedRWS(LiandOuyang,2024). PTGEN(Seeetal.,2017),BertSumAbs(Liuand
Lapata,2019),andLongformerEncoder-Decoder
Itisalsointerestingtonotethatthemajorityof
(Beltagy et al., 2020). These common baselines
RWGworkshaveusedNLPdatasets,andalmost
are relatively easy to replicate because they are
no works use papers from outside the domain of
well-documented,general-purposesummarization
computerscience. ItislikelythatRWGresearchers
approaches. In contrast, most specialized RWG
prefer to use NLP papers because they include a
approaches are not easy to replicate and are thus
separateRWSthatiseasytoextract,whichisnot
rarelyusedasbaselinesforlaterworks;wediscuss
the case in all fields of research; they are within
thisissuefurtherinSection6.1.
theresearchers’owndomainofexpertise,making
system development easier; and they are in the 5.2 Metrics
domain of the researchers’ colleagues, making it
AlmostallRWGworksusethesummarizationmet-
easiertorecruithumanjudgesforevaluation.
ricROUGE(Lin,2004)astheirautomaticevalua-
Finally, with the advent of LLM-based ap- tionmetric;Luuetal.(2021)additionallyusethe
proaches,RWGresearchersmustcontendwiththe translationmetricBLEU(Papinenietal.,2002).
possibilitythatatargetpaperwaspartofthetrain- Most works additionally conduct human eval-
ing data of their model. As a result, LLM-based uations, as is common in natural language gener-
workshaveexplicitlytargetedrecentpapers(Liand ation tasks. While there is no fixed standard for
Ouyang,2024;Martin-Boyleetal.,2024). howtoconductanRWGhumanevaluation,mostworksevaluateatleast15samples,withthreehu- systemthatassumeseachcitationcanonlybeone
manjudgespersample. Judgesaregenerallyasked sentenceandusesthesurroundingsentencesascon-
to rate the fluency or readability, the coherence textwillactuallyusetherestofthesentencesfrom
withrespecttothetargetpaper,andtherelevance thetargetcitationascontext,creatinganinforma-
orinformativenesswithrespecttothecitedpaper tionleakageproblem.
onafive-pointLikertscale. Variation in datasets comes partly from differ-
Therelativelysmallnumberofhuman-evaluated encesinthetaskdefinitionandpartlyfromthefact
samples in RWG works is likely due to the diffi- that, of the commonly used source corpora, only
cultyofrecruitinghumanjudgeswiththeexpertise theCORWApartitionofS2ORC(Lietal.,2022)
tounderstandthegeneratedcitationtextsorRWS, isexplicitlydesignedforRWG;theothers(AAN,
aswellasthehightimecommitmentanddifficulty S2ORC,andDelve)aregeneral-purposescholarly
ofthetask,whichrequiresjudgestoreadmultiple, document and citation analysis datasets. As a re-
highly specialized documents. A more detailed sult,theseothersourcecorporaeitherautomatically
summary of metrics used in RWG works can be extractcitationsbysearchingforsentencescontain-
foundinAppendixTable10. ingcitationmarksordonotlabelcitationsatall;in
thelattercase,RWGresearchersextractcitations
6 ConclusionsandDiscussion themselvesbysearchingforsentencescontaining
citationmarksandimposingassumptionsaboutthe
Having surveyed the field of RWG from the per-
numberofcitedpapersacitationcancontain. Be-
spectivesoftaskdefinition,approach,datasets,and
sidesCORWA,onlytheannotationsofXingetal.
evaluation methods, we conclude by identifying
(2020)providehuman-labeledcitations.
threemainchallengesinmodernRWGandmake
Finally,variationinevaluationstemsfromtheex-
recommendationsforfutureworkinthisarea.
istingproblemingeneralsummarizationresearch
where automated metrics, such as the commonly
6.1 LackofComparability
used ROUGE scores, do not correlate well with
WorkinRWGisfragmentedintermsoftaskdefini- humanjudgments,somanyRWGworksperform
tions,datasetsusedfortrainingandevaluation,and humanevaluation. Whilefluency,coherence,and
howevaluationsareconducted. UnlikemostNLP relevance are commonly used aspects of human
tasks,therearenostandardbenchmarksforRWG. evaluation (Appendix Table 10), many works de-
Table 1 shows that around half of existing works fine custom aspects, such as succinctness (Chen
do not release their models or generated citation et al., 2021; Deng et al., 2021; Liu et al., 2023),
texts/RWS, making itimpossible to reproduceor factualcorrectness(LiandOuyang,2024),andcor-
directlycompareapproaches. rectness of citation intent (Jung et al., 2022; Gu
AswediscussinSection2,RWGworksdonot andHahnloser,2023).
agree on the definition of citation (one or more
citedpapersdiscussedinoneormoresentences,or 6.2 CommonLimitations
justpartofasentence)orrelatedworksection(a
We find several limitations common to existing
concatenationofindividualcitationsorparagraphs
workonRWGforfutureworktoconsider.
versusonecontinuousandcoherentpieceoftext).
Thus,thetargetoutputsofmostRWGsystemsare Citation ordering and organization. Out of
notdirectlycomparabletothoseofothersystems. twenty surveyed RWG works, only four attempt
A deeper problem with the varying definitions topredictthecorrectorderingand/orgroupingof
ofcitationisnotedbyLietal.(2022),whoargue citationsintoparagraphs(HuandWan,2014;Deng
thathumanannotatorscaneasilyfindexamplesof et al., 2021; Liu et al., 2023; Martin-Boyle et al.,
human-writtencitationsthatarelongerorshorter 2024);anadditionaltwopapersacknowledgethe
than a single sentence, or that contain more than citationorderingandgroupingproblembutassume
onecitedpaper,soignoringcitationsthatarelonger ahuman-providedorderingisavailable(Hoangand
thanasinglesentenceordiscussmorethanasingle Kan,2010)oruseachronologicalorderingheuris-
citedpaperisunrealistic. Theyfurtherarguethat tic (Li and Ouyang, 2024). Yet Li and Ouyang
restrictingcitationstobesinglesentencesisprob- (2024) observed that human readers noticed and
lematic when the approach uses citation context; dislikederrorsincitationgrouping,suchaswhen
in the case of a multi-sentence citation, an RWG chronologicallyadjacentcitedpapersaboutdiffer-enttopicswereplacedinthesameparagraph,and RWG researchers that their systems could never
(Martin-Boyleetal.,2024)foundsignificantdiffer- beusedtodirectlywritetheRWSforanewpaper.
encesintheorganizationofgeneratedRWSwith However,extractiveapproachescannothallucinate,
andwithouthuman-assistedcitationgrouping. We so their outputs are less likely to contain factual
suggestfullyautomaticcitationorderingandgroup- errorsaboutthecitedpapers.
ingasanimportantareaforfurtherinvestigation. WithmodernabstractiveRWG,thesituationis
muddier. It is well-known in general summariza-
Transitionsentencesandwritingstyle. Based
tionresearchthatabstractivemodelscanstillcopy
on the terms from general summarization (Kla-
significantchunksoftextdirectlyfromtheirinputs
vans et al., 2001), Hoang and Kan (2010) distin-
(Grusky et al., 2018; Narayan et al., 2018), and
guishedinformativesentences,which“givedetail
factualconsistencyinsummarizationisanactive
on a specific aspect of the problem...definitions,
research area (Cao et al., 2018; Goodrich et al.,
purpose or application of the topic", and indica-
2019; Falke et al., 2019; Kryscinski et al., 2019).
tive sentences, which “make the topic transition
Thus,itispossibleforanabstractiveRWGsystem
explicitandrhetoricallysound". However,modern
to output plagiarized or hallucinated text, which
abstractiveapproacheshavefocusedoninformative
shouldbeofconcerntoanyuserwhowishestouse
sentences: single citation generation approaches
suchasystemtowriteanRWS.
completelyignoreindicativetransitionsentences,
Second,theuseofRWGtowriteanRWSfora
andsection-levelapproachesincludethemonlyin
paperoneintendstosubmitforpublicationraises
thattheyarepartofthetargetparagraphs. Liand
questionsofacademicdishonesty. Isitethicalfora
Ouyang(2024)foundthathumanreadersaskedfor
researchertoputanautomaticallygeneratedRWS
moretransitionsentences,complainingaboutRWS
in a submitted manuscript? Does this mean the
thatsimplyconcatenatedonecitedpapersummary
researcher is claiming to have written that RWS,
afteranother. Further,theiranalysisofRWSwrit-
as they presumably wrote the rest of the paper?
ingstyleandthecitationclustersofMartin-Boyle
Do the answers to these questions change if the
et al. (2024) have shown that generated RWS do
researcherhaseditedtheautomaticallygenerated
notdrawenoughconnectionsamongcitedpapers.
RWS?Aswithmanyconcernsrelatingtotheuse
Thus, the generation of transition sentences and
of powerful modern LLMs, these questions are
multi-papercitationsremainsanopenproblem.
verynew,andthereisasyetnoconsensusamong
Retrieval-augmented related work generation. thescientificcommunityonhowtoanswerthem.
ExistingRWGworksassumethelistofcitedpapers While automatically generated RWS as currently
isavailableasinput,butthisassumptionisunreal- easytorecognize,wenonethelessurgecautionon
istic,asevidencedbytheexistenceof“missingci- thepartofRWGresearchersandusers.
tations"questionsonmanyconferenceandjournal Third, RWG is a challenging task even for hu-
peerreviewforms. LiandOuyang(2024)reported mans;inmanydoctoralprograms,writingaformal
thatseveralhumanjudgesexpressedthedesirefora literature review is part of their candidacy qual-
systemthatwouldnotonlyhelpthemdraftaRWS, ifying exams (Knopf, 2006). Thus, the process
but also alert them to any other relevant papers of writing an RWS may be considered an impor-
theyshouldconsiderciting. Giventherecentsuc- tantprocessforresearcherswheretheymustread
cessandpopularityofretrieval-augmentedgenera- broadlyandthinkdeeplyabouthowtheircontribu-
tion(RAG)approaches(Lewisetal.,2020;Shuster tionsfitintothebiggerpictureoftheirfield. Some
etal.,2021),applyingRAGtoRWGisapromising RWG works have argued that writing an RWS is
directionforfutureRWGresearch. arduousandtime-consuming,andsoRWGshould
saveresearchersfromhavingtodoit,butweargue
6.3 EthicalConcerns
thispositionignoresthevalueofRWSwritingasa
Finally, we discuss three ethical issues related to learningandthinkingexperience. WeurgeRWG
theRWGtask. First,abstractiveRWGworksmust researcherstoconsiderhuman-in-the-loopframe-
beconcernedwiththeproblemsofplagiarismand works,following(GuandHahnloser,2023;Liand
factualerrors. Inextractiveapproaches,thegener- Ouyang,2024)andespecially(Martin-Boyleetal.,
atedRWSisbyitsverynatureplagiarized,sinceits 2024).
sentencesarecopieddirectlyfromthecitedpapers;
it was presumably well-understood by extractiveReferences CailingDongandUlrichSchäfer.2011. Ensemble-style
self-training on citation classification. In Proceed-
AhmedAbuRa’ed,HoracioSaggion,AlexanderShvets,
ingsof5thinternationaljointconferenceonnatural
andAlexBravo.2020. Automaticrelatedworksec-
languageprocessing,pages623–631.
tiongeneration: experimentsinscientificdocument
abstracting. Scientometrics,125(3):3159–3185.
GünesErkanandDragomirRRadev.2004. Lexrank:
Graph-basedlexicalcentralityassalienceintextsum-
UchennaAkujuobiandXiangliangZhang.2017. Delve:
marization. Journalofartificialintelligenceresearch,
adataset-drivenscholarlysearchandanalysissystem.
22:457–479.
ACM SIGKDD Explorations Newsletter, 19(2):36–
46.
Tobias Falke, Leonardo F. R. Ribeiro, Prasetya Ajie
Utama,IdoDagan,andIrynaGurevych.2019. Rank-
AwaisAthar.2011. Sentimentanalysisofcitationsus-
inggeneratedsummariesbycorrectness: Aninterest-
ingsentencestructure-basedfeatures. InProceedings
ingbutchallengingapplicationfornaturallanguage
oftheACL2011studentsession,pages81–87.
inference. InProceedingsofthe57thAnnualMeet-
ingoftheAssociationforComputationalLinguistics,
Awais Athar and Simone Teufel. 2012. Context-
pages 2214–2220, Florence, Italy. Association for
enhancedcitationsentimentdetection. InProceed-
ComputationalLinguistics.
ingsofthe2012ConferenceoftheNorthAmerican
Chapter of the Association for Computational Lin-
EugeneGarfieldetal.1965. Cancitationindexingbe
guistics: HumanLanguageTechnologies,pages597–
automated. In Statistical association methods for
601, Montréal, Canada. Association for Computa-
mechanizeddocumentation,symposiumproceedings,
tionalLinguistics.
volume269,pages189–192.Washington.
Iz Beltagy, Matthew E. Peters, and Arman Cohan.
YubinGe,LyDinh,XiaofengLiu,JinsongSu,Ziyao
2020. Longformer: Thelong-documenttransformer.
Lu, Ante Wang, and Jana Diesner. 2021. BACO:
arXiv:2004.05150.
Abackgroundknowledge-andcontent-basedframe-
workforcitingsentencegeneration. InProceedings
ZiqiangCao,FuruWei,WenjieLi,andSujianLi.2018.
of the 59th Annual Meeting of the Association for
Faithfultotheoriginal: Factawareneuralabstractive
ComputationalLinguisticsandthe11thInternational
summarization. InProceedingsoftheAAAIConfer-
JointConferenceonNaturalLanguageProcessing
enceonArtificialIntelligence,volume32.
(Volume1: LongPapers),pages1466–1478,Online.
AssociationforComputationalLinguistics.
JingqiangChenandHaiZhuge.2019. Automaticgener-
ationofrelatedworkthroughsummarizingcitations.
ConcurrencyandComputation: PracticeandExperi- BenGoodrich,VinayRao,PeterJ.Liu,andMohammad
ence,31(3):e4261. Saleh.2019. Assessingthefactualaccuracyofgener-
atedtext. InProceedingsofthe25thACMSIGKDD
XiuyingChen, HindAlamro, MingzheLi, ShenGao, InternationalConferenceonKnowledgeDiscovery
Rui Yan, Xin Gao, and Xiangliang Zhang. 2022. &DataMining,KDD’19,page166–175,NewYork,
Target-awareabstractiverelatedworkgenerationwith NY,USA.AssociationforComputingMachinery.
contrastivelearning. InProceedingsofthe45thIn-
ternationalACMSIGIRConferenceonResearchand Max Grusky, Mor Naaman, and Yoav Artzi. 2018.
Development in Information Retrieval, pages 373– Newsroom: Adatasetof1.3millionsummarieswith
383. diverseextractivestrategies. InProceedingsofthe
2018ConferenceoftheNorthAmericanChapterof
XiuyingChen,HindAlamro,MingzheLi,ShenGao,Xi- theAssociationforComputationalLinguistics: Hu-
angliangZhang,DongyanZhao,andRuiYan.2021. man Language Technologies, Volume 1 (Long Pa-
Capturing relations between scientific papers: An pers),pages708–719,NewOrleans,Louisiana.As-
abstractivemodelforrelatedworksectiongeneration. sociationforComputationalLinguistics.
In Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the NianlongGuandRichardH.R.Hahnloser.2023. Con-
11thInternationalJointConferenceonNaturalLan- trollablecitationsentencegenerationwithlanguage
guageProcessing(Volume1: LongPapers),pages models.
6068–6077,Online.AssociationforComputational
Linguistics. CongDuyVuHoangandMin-YenKan.2010. Towards
automated related work summarization. In Coling
ArmanCohan,WaleedAmmar,MadeleineVanZuylen, 2010: Posters,pages427–435,Beijing,China.Col-
and Field Cady. 2019. Structural scaffolds for ci- ing2010OrganizingCommittee.
tationintentclassificationinscientificpublications.
arXivpreprintarXiv:1904.01608. Yue Hu and Xiaojun Wan. 2014. Automatic genera-
tionofrelatedworksectionsinscientificpapers: an
Zekun Deng, Zixin Zeng, Weiye Gu, Jiawen Ji, and optimizationapproach. InProceedingsofthe2014
Bolin Hua. 2021. Automatic related work section Conference on Empirical Methods in Natural Lan-
generationbysentenceextractionandreordering. guageProcessing(EMNLP),pages1624–1633.Kokil Jaidka, Christopher Khoo, and Jin-Cheon Na. AnneLauscher,BrandonKo,BaileyKuhl,SophieJohn-
2010. Imitatinghumanliteraturereviewwriting: An son, David Jurgens, Arman Cohan, and Kyle Lo.
approachtomulti-documentsummarization. InIn- 2021. Multicite:Modelingrealisticcitationsrequires
ternationalConferenceonAsianDigitalLibraries, movingbeyondthesingle-sentencesingle-labelset-
pages116–119.Springer. ting. arXivpreprintarXiv:2107.00414.
Kokil Jaidka, Christopher Khoo, and Jin-Cheon Na. PatrickLewis,EthanPerez,AleksandraPiktus,Fabio
2013a. Deconstructinghumanliteraturereviews–a Petroni,VladimirKarpukhin,NamanGoyal,Hein-
framework for multi-document summarization. In richKüttler, MikeLewis, Wen-tauYih, TimRock-
proceedingsofthe14thEuropeanworkshoponnatu- täschel,etal.2020. Retrieval-augmentedgeneration
rallanguagegeneration,pages125–135. forknowledge-intensivenlptasks. AdvancesinNeu-
ralInformationProcessingSystems,33:9459–9474.
KokilJaidka,ChristopherSGKhoo,andJin-CheonNa.
Xiangci Li, Yi-Hui Lee, and Jessica Ouyang. 2023.
2013b. Literaturereviewwriting: howinformation
Citedtextspansforcitationtextgeneration. arXiv
is selected and transformed. In Aslib Proceedings.
preprintarXiv:2309.06365.
EmeraldGroupPublishingLimited.
Xiangci Li, Biswadip Mandal, and Jessica Ouyang.
Kokil Jaidka Jaidka, Christopher Khoo Khoo, and
2022. CORWA: A citation-oriented related work
Jin-Cheon Na Na. 2011. Literature review writ-
annotationdataset. InProceedingsofthe2022Con-
ing: astudyofinformationselectionfromcitedpa-
ference of the North American Chapter of the As-
pers/kokiljaidka,christopherkhooandjin-cheonna.
sociation for Computational Linguistics: Human
LanguageTechnologies,pages5426–5440,Seattle,
Shing-YunJung,Ting-HanLin,Chia-HungLiao,Shyan-
United States. Association for Computational Lin-
Ming Yuan, and Chuen-Tsai Sun. 2022. Intent-
guistics.
controllablecitationtextgeneration. Mathematics,
10(10):1763. XiangciLiandJessicaOuyang.2024. Explainingre-
lationshipsamong researchpapers. arXivpreprint
DavidJurgens,SrijanKumar,RaineHoover,DanMc- arXiv:2402.13426.
Farland, and Dan Jurafsky. 2018. Measuring the
evolutionofascientificfieldthroughcitationframes. Chin-YewLin.2004. Rouge: Apackageforautomatic
TransactionsoftheAssociationforComputational evaluation of summaries. In Text summarization
Linguistics,6:391–406. branchesout,pages74–81.
Jiachang Liu, Qi Zhang, Chongyang Shi, Usman
ChristopherSGKhoo,Jin-CheonNa,andKokilJaidka.
Naseem,ShoujinWang,LiangHu,andIvorTsang.
2011. Analysisofthemacro-leveldiscoursestructure
2023. Causal intervention for abstractive related
ofliteraturereviews. OnlineInformationReview.
work generation. In Findings of the Association
forComputationalLinguistics: EMNLP2023,pages
JudithLKlavans,Min-yenKan,andKathleenMcKe-
2148–2159, Singapore. Association for Computa-
own.2001. Domain-specificinformativeandindica-
tionalLinguistics.
tive summarization for information retrieval. Pro-
ceedingsoftheDocumentUnderstandingWorkshop.
YangLiuandMirellaLapata.2019. Textsummariza-
tion with pretrained encoders. In Proceedings of
JeffreyWKnopf.2006. Doingaliteraturereview. PS:
the2019ConferenceonEmpiricalMethodsinNatu-
PoliticalScience&Politics,39(1):127–132.
ralLanguageProcessingandthe9thInternational
JointConferenceonNaturalLanguageProcessing
WojciechKryscinski,NitishShirishKeskar,BryanMc-
(EMNLP-IJCNLP),pages3730–3740,HongKong,
Cann, Caiming Xiong, and Richard Socher. 2019.
China.AssociationforComputationalLinguistics.
Neuraltextsummarization: Acriticalevaluation. In
Proceedings of the 2019 Conference on Empirical KyleLo,LucyLuWang,MarkNeumann,RodneyKin-
MethodsinNaturalLanguageProcessingandthe9th ney,andDanielWeld.2020. S2ORC:Thesemantic
InternationalJointConferenceonNaturalLanguage scholaropenresearchcorpus. InProceedingsofthe
Processing(EMNLP-IJCNLP),pages540–551,Hong 58thAnnualMeetingoftheAssociationforCompu-
Kong,China.AssociationforComputationalLinguis- tationalLinguistics,pages4969–4983,Online.Asso-
tics. ciationforComputationalLinguistics.
Anne Lauscher, Brandon Ko, Bailey Kuehl, Sophie Kelvin Luu, Xinyi Wu, Rik Koncel-Kedziorski, Kyle
Johnson, Arman Cohan, David Jurgens, and Kyle Lo,IsabelCachola,andNoahA.Smith.2021. Ex-
Lo. 2022. MultiCite: Modeling realistic citations plainingrelationshipsbetweenscientificdocuments.
requiresmovingbeyondthesingle-sentencesingle- In Proceedings of the 59th Annual Meeting of the
labelsetting. InProceedingsofthe2022Conference Association for Computational Linguistics and the
oftheNorthAmericanChapteroftheAssociationfor 11thInternationalJointConferenceonNaturalLan-
ComputationalLinguistics: HumanLanguageTech- guageProcessing(Volume1: LongPapers),pages
nologies, pages 1875–1889, Seattle, United States. 2130–2144,Online.AssociationforComputational
AssociationforComputationalLinguistics. Linguistics.Biswadip Mandal, Xiangci Li, and Jessica Ouyang. reduces hallucination in conversation. In Findings
2024. Contextualizinggeneratedcitationtexts. arXiv of the Association for Computational Linguistics:
preprintarXiv:2402.18054. EMNLP 2021, pages 3784–3803, Punta Cana, Do-
minican Republic. Association for Computational
AnnaMartin-Boyle,AahanTyagi,MartiAHearst,and Linguistics.
DongyeopKang.2024. Shallowsynthesisofknowl-
edge in gpt-generated texts: A case study in au- SimoneTeufel,AdvaithSiddharthan,andDanTidhar.
tomatic related work composition. arXiv preprint 2006. Automaticclassificationofcitationfunction.
arXiv:2402.12255. InProceedingsofthe2006conferenceonempirical
methodsinnaturallanguageprocessing,pages103–
Rui Meng, Khushboo Thaker, Lei Zhang, Yue Dong,
110.
Xingdi Yuan, Tong Wang, and Daqing He. 2021.
Bringing structure into summaries: a faceted sum-
Suppawong Tuarob, Sung Woo Kang, Poom Wet-
marizationdatasetforlongscientificdocuments. In
tayakorn, Chanatip Pornprasit, Tanakitti Sachati,
Proceedingsofthe59thAnnualMeetingoftheAsso-
Saeed-Ul Hassan, and Peter Haddawy. 2019. Au-
ciationforComputationalLinguisticsandthe11th
tomaticclassificationofalgorithmcitationfunctions
InternationalJointConferenceonNaturalLanguage
inscientificliterature. IEEETransactionsonKnowl-
Processing(Volume2: ShortPapers),pages1080–
edgeandDataEngineering,32(10):1881–1896.
1089,Online.AssociationforComputationalLinguis-
tics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
RadaMihalceaandPaulTarau.2004. Textrank: Bring-
Kaiser,andIlliaPolosukhin.2017. Attentionisall
ingorderintotext. InProceedingsofthe2004con-
you need. In Advances in neural information pro-
ference on empirical methods in natural language
cessingsystems,pages5998–6008.
processing,pages404–411.
Shashi Narayan, Shay B. Cohen, and Mirella Lapata. PetarVelickovic,GuillemCucurull,ArantxaCasanova,
2018. Don’tgivemethedetails,justthesummary! Adriana Romero, Pietro Lio’, and Yoshua Ben-
Topic-aware convolutional neural networks for ex- gio. 2018. Graph attention networks. ArXiv,
treme summarization. In Proceedings of the 2018 abs/1710.10903.
Conference on Empirical Methods in Natural Lan-
guageProcessing,Brussels,Belgium. PanchengWang,ShashaLi,HaifangZhou,JintaoTang,
andTingWang.2019. Toc-rwg: Explorethecom-
KishorePapineni,SalimRoukos,ToddWard,andWei- binationoftopicmodelandcitationinformationfor
JingZhu.2002. Bleu: amethodforautomaticevalu- automatic related work generation. IEEE Access,
ationofmachinetranslation. InProceedingsofthe 8:13043–13055.
40thannualmeetingoftheAssociationforComputa-
tionalLinguistics,pages311–318. YongzhenWang,XiaozhongLiu,andZhengGao.2018.
Neural related work summarization with a joint
DragomirRRadev,HongyanJing,MałgorzataStys´,and context-driven attention mechanism. In Proceed-
Daniel Tam. 2004. Centroid-based summarization ingsofthe2018ConferenceonEmpiricalMethods
of multiple documents. Information Processing & inNaturalLanguageProcessing,pages1776–1786,
Management,40(6):919–938. Brussels, Belgium. Association for Computational
Linguistics.
Dragomir R Radev, Pradeep Muthukrishnan, Vahed
Qazvinian, and Amjad Abu-Jbara. 2013. The acl
MarkWasson.1998. Usingleadingtextfornewssum-
anthologynetworkcorpus. LanguageResourcesand
maries: Evaluationresultsandimplicationsforcom-
Evaluation,47(4):919–944.
mercialsummarizationapplications. In36thAnnual
Meeting of the Association for Computational Lin-
KumarRavi,SrirangarajSetlur,VadlamaniRavi,and
guisticsand17thInternationalConferenceonCom-
VenuGovindaraju.2018. Articlecitationsentiment
putationalLinguistics,Volume2,pages1364–1368.
analysis using deep learning. In 2018 IEEE 17th
InternationalConferenceonCognitiveInformatics
XinyuXing,XiaoshengFan,andXiaojunWan.2020.
&CognitiveComputing(ICCI*CC),pages78–85.
Automatic generation of citation texts in scholarly
IEEE.
papers: A pilot study. In Proceedings of the 58th
AbigailSee,PeterJ.Liu,andChristopherD.Manning. AnnualMeetingoftheAssociationforComputational
2017. Gettothepoint: Summarizationwithpointer- Linguistics,pages6181–6190.
generatornetworks. InProceedingsofthe55thAn-
nualMeetingoftheAssociationforComputational MichihiroYasunaga,JungoKasai,RuiZhang,Alexan-
Linguistics (Volume 1: Long Papers), pages 1073– der R Fabbri, Irene Li, Dan Friedman, and
1083,Vancouver,Canada.AssociationforComputa- Dragomir R Radev. 2019. Scisummnet: A large
tionalLinguistics. annotatedcorpusandcontent-impactmodelsforsci-
entificpapersummarizationwithcitationnetworks.
KurtShuster,SpencerPoff,MoyaChen,DouweKiela, InProceedingsoftheAAAIConferenceonArtificial
and Jason Weston. 2021. Retrieval augmentation Intelligence,volume33,pages7386–7393.HeZhao,ZhunchenLuo,ChongFeng,AnqingZheng,
andXiaopengLiu.2019. Acontext-basedframework
formodelingtheroleandfunctionofon-lineresource
citations in scientific literature. In Proceedings of
the2019ConferenceonEmpiricalMethodsinNatu-
ralLanguageProcessingandthe9thInternational
JointConferenceonNaturalLanguageProcessing
(EMNLP-IJCNLP),pages5206–5215.A AppendixPriorWork Inputs
HoangandKan(2010) Topichierarchytreeofthetargetrelatedwork,fullcited
papers
HuandWan(2014) Targetpaper(abstract,introduction),citedpapers(abstract,
introduction,relatedwork,conclusion)
Wangetal.(2018) Full-textsofcitedpapers
ChenandZhuge(2019) Title,abstract,introduction,andconclusionforbothtarget
paperandcitedpapers;papersthatco-citethecitedpapers
Wangetal.(2019) Fullpapersoftargetpaperandcitedpapers,citationsen-
tencesthatco-citingthecitedpapers
Dengetal.(2021) Abstractorconclusionsectionsofthecitedpapers
Table4: Asummaryoftheproblemformulationsofthepriorworksonextractiverelatedworkgeneration. Allof
theirgenerationtargetsareasequenceofextractedsentences.
PriorWork Inputs Target
AbuRa’edetal.(2020) Citedtitle,abstract Citationsentencew/singlereference
Xingetal.(2020) Context sentences, single cited ab- Citationsentencew/singlereference
stract
Geetal.(2021) Citation network, single cited ab- Citation sentence, citation function,
stract,contextsentences salientsentenceincitedabstracts
Luuetal.(2021) Introofthecitingpaper,namedenti- Citationsentencew/singlereference
tiesofthecitedpapers
Lietal.(2022) Context sentences w/o the target Citationspanw/1+citations
span,1+citedabstracts
Jungetal.(2022) Abstract or title of the citing paper, 1+citationsentenceswithsinglecita-
citedabstract,citationintent tion
Lietal.(2023) Context sentences w/o the target Citationspanw/1+citations
span,1+citedabstracts
GuandHahnloser(2023) Title, abstract of the target paper & Citation sentence with presumably
citedpaper;citationtext,citationin- singlecitation
tent,keywords
Mandaletal.(2024) Context sentences w/o the target Contextsentencesw/thetargetspan
span,1+citedabstracts w/1+citations
Chenetal.(2021) Citedabstracts Aparagraphw/2+citations
Chenetal.(2022) Targetabstract,citedabstracts Aparagraphw/2+citations
Liuetal.(2023) Citedabstracts Relatedworkparagraph
LiandOuyang(2024) MainideaoftheRWS,title,abstract, 1+paragraphsofRWS
intro,conclusionofthetargetpaper,
fulltextofcitedpapers
Martin-Boyleetal.(2024) Targetpaperw/oRWS,andabstracts RWS
ofthecitedpapers
Table5:Asummaryoftheproblemformulationsofthepriorworksonneuralnetwork-basedrelatedworkgeneration.
“Context”referstothosesentencesorparagraphsaroundthetargetcitationsentences.PriorWork SourceDomain Size Dataset
HoangandKan(2010) PapersfromNLPandIR,man- 20papers RWSDataa
uallycuratedtopictree
HuandWan(2014) ACLAnthology 1050papers N/A
Wangetal.(2018) ACMdigitallibrary 8080papers Availableb
ChenandZhuge(2019) ACLAnthology&IJCAI 25papers RWS-Citc
Wangetal.(2019) NLPconferences 50papers NudtRwGd
Dengetal.(2021) ScisummNet(ACL) 11954examples N/A
AbuRa’edetal.(2020) ScisummNet(ACL) 940+15574pairs N/A
Xingetal.(2020) ACLAnthologyNetwork 1k+85kexamples Availablee
Geetal.(2021) ACLAnthologyNetwork 1.2k+84kexamples N/A
Luuetal.(2021) S2ORC(CS) 622k citations from 154k Extraction
papers from
S2ORC
f
Lietal.(2022) S2ORC(NLP) Annotated3565dominant CORWAg
spans & 4228 reference
spans from 2927 para-
graphs; 565+362+11465
train/test/distantRWS
Jungetal.(2022) SciCite(CS) 8243/916/1861 Availableh
train/validation/test
samples
Lietal.(2023) CORWA,S2ORC(NLP) 1654/1206/19784 Availablei
train/test/distant dom-
inantcitationspans
GuandHahnloser(2023) arXivcomputersciencepapers 233.6k/1.3k/1.1k Availablej
train/validation/test
samples
Mandaletal.(2024) CORWA 565/362/11465 N/A
train/test/distantRWS
Chenetal.(2021) S2ORC(Multi-domain),Delve 150k,80kexamples Availablek
(CS)
Chenetal.(2022) S2ORC(Multi-domain),Delve 107.7k/5k/5k, N/A
(CS) 208.3k/5k/5k
train/dev/test exam-
ples
Liuetal.(2023) S2ORC(Multi-domain),Delve 126k/5k/5k, 72k/3k/3k N/A
(CS) train/dev/testpairs
LiandOuyang(2024) PDFs from NLP, ML, Speech, 38papers N/A
CV,etc.
Martin-Boyleetal.(2024) 2023ACLbestpapers 10papers N/A
Table6: Asummaryofthedatasetsofthepriorworksonrelatedworkgeneration.
ahttp://wing.comp.nus.edu.sg/downloads/rwsdata
bhttps://github.com/kuadmu/2018EMNLP
chttps://github.com/jingqiangchen/RWS-Cit
dhttps://github.com/NudtRwG/NudtRwG-Dataset
ehttps://github.com/XingXinyu96/citation_generation
fhttps://github.com/Kel-Lu/SciGen/tree/master/data_processing
ghttps://github.com/jacklxc/CORWA
hhttps://github.com/BradLin0819/Automatic-Citation-Text-Generation-with-Citation-Intent-Control
ihttps://github.com/jacklxc/CTS4CitationTextGeneration
jhttps://github.com/nianlonggu/LMCiteGen
khttps://github.com/iriscxy/relatedworkgenerationPriorWork Approaches
HoangandKan(2010) Heuristicapproachtogenerategeneralandspecificcontentseparatelygivena
topictree
HuandWan(2014) PLSA for topic modeling, SVR for sentence importance score, and global
optimizationforsentenceselection
Wangetal.(2018) Custom neural seq2seq model (CNN, LSTM, attention), random walk for
encodingheterogeneousbibliographygraph
ChenandZhuge(2019) Consideringpapersco-citethecitedpapers;Representinggraphforrelationship
modelingofpapers,thenfindingsentencenodesthatcovertheminimumSteiner
treeofthegraph
Wangetal.(2019) Leveragingbothtopicmodelandcitedtextspans
Dengetal.(2021) BERT-basedsentenceextraction&reordering
AbuRa’edetal.(2020) ApplyingPTGenandTransformer
Xingetal.(2020) Manualannotation+automaticannotationofcitationsentences;PTGEN-Cross
basedoncross-attentionmechanism
Geetal.(2021) Citation network as auxiliary input; citation function & salient sentences in
citedpapersasauxiliaryoutput;multi-tasklearning
Luuetal.(2021) SciGPT2;IE-ExtractedTermLists;rankingbasedonentitymatching
Lietal.(2022) LED-basedcitationspangeneration
Jungetal.(2022) BART/T5-basedcitationsentencegenerationwithcitationintents
Lietal.(2023) RAG&LED;ROUGE-basedCTSretrieval
GuandHahnloser(2023) Fine-tunedGPT-Neo&GalacticawithProximalPolicyOptimization
Mandaletal.(2024) Usingcitationcontextalongwithcitationspansasgenerationtarget
Chenetal.(2021) Transformer-basedhierarchicalencoder;relationshipmodelingmodule
Chenetal.(2022) ImprovedoverChenetal.(2021)byencodingtargetpaper’sabstract
Liuetal.(2023) ProposedacustomCausalInterventionModule(CaM)insertedbetweenTrans-
formerblocks
LiandOuyang(2024) GPT-3.5forfeaturegeneration,e.g. facetedsummary,relationship&usageof
citations;GPT-4basedRWG
Martin-Boyleetal.(2024) GPT-4withhuman-in-the-loop
Table7: Asummaryoftheapproachesofthepriorworks.
PriorWork Baselines Automatic HumanEvaluation
HoangandKan(2010) LEAD,MEAD ROUGE recall(1, 2, Correctness, novelty, flu-
S4,SU4) ency,usefulness
HuandWan(2014) MEAD,LexRank ROUGE F1 (1, 2, Correctness, readability,
SU4) usefulness
Wangetal.(2018) Luhn, MMR, LexRank, ROUGEF1(1,2,L) Compliance to target pa-
SumBasic, NltkSum, per, intuitiveness, useful-
PointerNetwork ness
ChenandZhuge(2019) MEAD,LexRank,RoWoS ROUGEF1(1,2) N/A
Wangetal.(2019) LexRank, SumBasic, JS- ROUGErecall&F1 N/A
Gen,TopicSum (1,2,SU4)
Dengetal.(2021) MEAD ROUGE precision, informativeness, fluency,
recall,F1(1,2,L) succinctness
Table8: Asummaryoftheevaluationmethodsoftheextractiverelatedworkgenerationworks.PriorWork Baselines Automatic HumanEvaluation
AbuRa’edetal.(2020) MEAD, TextRank, ROUGE precision, N/A
SUMMA,SEQ3 recall, F1 (1, 2, L,
SU4)
Xingetal.(2020) RandomSen,MaxSimSen, ROUGEF1(1,2,L) Readability, Content, Co-
EXT-Oracle, COPY-CIT, herence,Overall
PTGEN
Geetal.(2021) LexRank,TextRank,EXT- ROUGEF1(1,2,L) Fluency,relevance,coher-
Oracle,PTGEN,PTGEN- ence,overall
Cross
Luuetal.(2021) N/A BLEU,ROUGE-L Correct,Specific
Lietal.(2022) Citation sentence genera- ROUGEF1(1,2,L) Fluency, coherence, rele-
tion vance,overall
Jungetal.(2022) EXT-Oracle,ablations ROUGEF1(1,2,L), Correct, specific, plausi-
SciBERTScore,cita- ble,intent
tionintentaccuracy
Lietal.(2023) Citation span generation BLEU, ROUGE- Fluency, coherence, rele-
basedoncitedabstracts,& F1-L, METEOR, vance,overall
human-annotatedCTS QuestEval,ANLI
GuandHahnloser(2023) BART-base & -large, ROUGE F1 (1, 2, Intentalignment,keyword
GPT-Neo 125M & 1.3B, L), Intent alignment recall,fluency&similarity
Galactica 125M & 1.3B score, keyword re- tothegroundtruth
&6.7B, LLaMA-7B call,fluencyscore
ablations,GPT-3.5-turbo
Mandaletal.(2024) Ablations N/A Fluency, coherence, rele-
vance,overall
Chenetal.(2021) LEAD, TextRank, Bert- ROUGEF1(1,2,L) QA, informativeness, co-
SumEXT, MGSum-ext, herence,succinctness
PTGen+Cov, Trans-
formerABS,BertSumAbs,
MGSum-abs,GS
Chenetal.(2022) LEAD, LexRank, NES, ROUGE F1 (1, 2, L, QA, informativeness, co-
BertSumEXT, MGSum, SU) herence,succinctness
EMS,RRG,BertSumAbs
Liuetal.(2023) TexRank, BertSumEXT, ROUGEF1(1,2,L) QA, informativeness, co-
MGSum-ext & -abs, herence,succinctness
TransformerABS, RRG,
BertSumAbs, GS,
T5-base, BART-base,
Longformer, NG-abs,
TAG
LiandOuyang(2024) Ablations ROUGEF1(1,2,L) Fluency, coherence, rele-
vance(cited,target),factu-
ality, usefulness, writing,
overall,#oferrors
Martin-Boyleetal.(2024) Human & Human-in-the- # of edges, average Qualitativeanalysis
loop nodedegree,density,
clustercoefficient
Table9: Asummaryoftheevaluationmethodsoftheabstractiverelatedworkgenerationworks.Perspective Definition UsedBy
Fluency, Read- Does the summary’s exposition flow well, in HoangandKan(2010);HuandWan
ability termsofsyntaxaswellasdiscourse? (2014);Dengetal.(2021);Xingetal.
(2020); Ge et al. (2021); Chen et al.
(2021,2022);Lietal.(2022,2023);
Gu and Hahnloser (2023); Mandal
etal.(2024);LiandOuyang(2024)
Correctness Isthesummarycontentrelevantto(expressthe HoangandKan(2010);HuandWan
factual relationship with) the hierarchical top- (2014);Luuetal.(2021);Jungetal.
ics/citedpapersgiven? (2022)
Novelty Doesthesummaryintroducenovelinformation HoangandKan(2010)
thatissignificantincomparisonwiththehuman
createdsummary?
Usefulness Is the summary useful in supporting the re- HoangandKan(2010);HuandWan
searchers to quickly grasp the related works (2014); Wang et al. (2018); Li and
givenhierarchicaltopics? Ouyang(2024)
Content, Rele- Whetherthecitationtextisrelevanttothecited Wangetal.(2018);Xingetal.(2020);
vance paper’sabstract Ge et al. (2021); Li et al. (2022,
2023); Gu and Hahnloser (2023);
Mandaletal.(2024);LiandOuyang
(2024)
Coherence Whether the citation text is coherent with the Xing et al. (2020); Ge et al. (2021);
citingpaper’scontext Chen et al. (2021, 2022); Li et al.
(2022,2023);Liuetal.(2023);Man-
dal et al. (2024); Li and Ouyang
(2024)
Informativeness Does the related work convey important facts Dengetal.(2021);Chenetal.(2021,
aboutthetopicquestion? 2022);Liuetal.(2023)
Succinctness Doestherelatedworkavoidrepetition? Dengetal.(2021);Chenetal.(2021);
Liuetal.(2023)
Overall Overallquality Xing et al. (2020); Ge et al. (2021);
Li et al. (2022, 2023); Mandal et al.
(2024);LiandOuyang(2024)
Intuitiveness How intuitive is the related work section for Wangetal.(2018)
readerstograspthekeycontent?
QA Retainthekeyinformation? Chen et al. (2021, 2022); Liu et al.
(2023)
Specific Whether the explanation describes a specific Luuetal.(2021);Jungetal.(2022)
relationshipbetweenthetwoworks
Factuality, # of Doestheoutputcontainfactualerrors? LiandOuyang(2024)
errors
Plausible, writ- Writingstyleofcitationtext/RWS Jung et al. (2022); Li and Ouyang
ing (2024)
Qualitative Descriptivecasestudy LiandOuyang(2024);Martin-Boyle
analysis etal.(2024)
Intent align- Whethertheoutputalignswiththeinputintent. Jungetal.(2022);GuandHahnloser
ment (2023)
Keywordrecall Whether the output contains the input key GuandHahnloser(2023)
words.
Table10: Asummaryoftheperspectivesforhumanevaluation.