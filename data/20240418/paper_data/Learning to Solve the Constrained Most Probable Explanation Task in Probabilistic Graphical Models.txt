LEARNING TO SOLVE THE CONSTRAINED MOST PROBABLE
EXPLANATION TASK IN PROBABILISTIC GRAPHICAL MODELS
APREPRINT
ShivvratArya‚àó TahrimaRahman‚àó
DepartmentofComputerScience DepartmentofComputerScience
TheUniversityofTexasatDallas TheUniversityofTexasatDallas
Richardson,TX75252 Richardson,TX75252
shivvrat.arya@utdallas.edu tahrima.rahman@utdallas.edu
VibhavGogate
DepartmentofComputerScience
TheUniversityofTexasatDallas
Richardson,TX75252
vibhav.gogate@utdallas.edu
ABSTRACT
Weproposeaself-supervisedlearningapproachforsolvingthefollowingconstrainedoptimization
taskinlog-linearmodelsorMarkovnetworks.Letf andg betwolog-linearmodelsdefinedover
the sets X and Y of random variables respectively. Given an assignment x to all variables in X
(evidence)andarealnumberq,theconstrainedmost-probableexplanation(CMPE)taskseeksto
findanassignmentytoallvariablesinYsuchthatf(x,y)ismaximizedandg(x,y)‚â§q.Inour
proposedself-supervisedapproach,givenassignmentsxtoX(data),wetrainadeepneuralnetwork
thatlearnstooutputnear-optimalsolutionstotheCMPEproblemwithoutrequiringaccesstoany
pre-computed solutions. The key idea in our approach is to use first principles and approximate
inferencemethodsforCMPEtoderivenovellossfunctionsthatseektopushinfeasiblesolutions
towardsfeasibleonesandfeasiblesolutionstowardsoptimalones.Weanalyzethepropertiesofour
proposedmethodandexperimentallydemonstrateitsefficacyonseveralbenchmarkproblems.
1 INTRODUCTION
Probabilisticgraphicalmodels(PGMs)suchasBayesianandMarkovnetworks(KollerandFriedman,2009;Darwiche,
2009)compactlyrepresentjointprobabilitydistributionsoverrandomvariablesbyfactorizingthedistributionaccording
toagraphstructurethatencodesconditionalindependenceamongthevariables.Oncelearnedfromdata,thesemodels
canbeusedtoanswervariousqueries,suchascomputingthemarginalprobabilitydistributionoverasubsetofvariables
(MAR)andfindingthemostlikelyassignmenttoallunobservedvariables,whichisreferredtoasthemostprobable
explanation(MPE)task.
Recently,Rouhanietal.(2020)proposedanextensiontotheMPEtaskinPGMsbyintroducingconstraints.More
specifically,giventwoPGMsf andgdefinedoverthesetofrandomvariablesXandarealnumberq,theconstrained
mostprobableexplanation(CMPE)taskseekstofindthemostlikelystateX = xw.r.t.f suchthattheconstraint
g(x)‚â§qissatisfied.EventhoughbothMPEandCMPEareNP-hardingeneral,CMPEisconsiderablymoredifficult
tosolveinpracticethanMPE.Notably,CMPEisNP-hardevenonPGMshavingnoedges,suchaszerotreewidth
orindependentPGMs,whileMPEcanbesolvedinlineartime.Rouhanietal.(2020)andlaterRahmanetal.(2021)
showedthatseveralprobabilisticinferencequeriesarespecialcasesofCMPE.Thisincludesqueriessuchasfindingthe
decisionpreservingmostprobableexplanation(Choietal.,2012),findingthenearestassignment(Rouhanietal.,2018)
androbustestimation(DarwicheandHirth,2023,2020).
*Theseauthorscontributedequallytothiswork.
4202
rpA
71
]GL.sc[
1v60611.4042:viXraLearningtoSolvetheCMPETaskinPGMs APREPRINT
OurinterestintheCMPEtaskismotivatedbyitsextensiveapplicabilitytovariousneuro-symbolicinferencetasks.Many
ofthesetaskscanbeviewedasspecificinstancesofCMPE.Specifically,whenf(x)representsafunctionencodedbya
neuralnetworkandg(x)‚â§qsignifiesparticularsymbolicorweightedconstraintsthattheneuralnetworkmustadhere
to,theneuro-symbolicinferencetaskinvolvesdeterminingthemostlikelypredictionwithrespecttof whileensuring
thattheconstraintg(x)‚â§qissatisfied.AnothernotableapplicationofCMPEinvolvestransferringabstractknowledge
andinferencesfromsimulationstoreal-worldcontexts.Forexample,inrobotics,numeroussimulationscanbeemployed
toinstructtherobotonvariousaspects,suchasobjectinteractions,robot-worldinteractions,andunderlyingphysical
principles,encapsulatingthisabstractknowledgewithintheconstraintg(x)‚â§q.Subsequently,withaneuralnetwork
f trainedonalimitedamountofreal-worlddata,characterizedbyricherfeaturesetsandobjectives,gcanbeusedto
reinforcethepredictionsmadebyf,ensuringthattherobotidentifiesthemostlikelypredictionwithrespecttof while
satisfyingtheconstraintg(x)‚â§q.Thisstrategyenhancesthereliabilityoftherobot‚Äôspredictionsandunderscoresthe
practicalsignificanceofCMPE.
Inthispaper,weexplorenovelmachinelearning(ML)approachesforsolvingtheCMPEtask,drawinginspirationfrom
recentdevelopmentsinlearningtooptimize(Dontietal.,2021;Fiorettoetal.,2020;ParkandVanHentenryck,2022;
ZamzamandBaker,2019).Themainideaintheseworksistotrainadeepneuralnetworkthattakestheparameters,
observations,etc.ofaconstrainedoptimizationproblemasinputandoutputsanear-optimalsolutiontotheoptimization
problem.
Inpractice,apopularapproachforsolvingoptimizationproblemsistousesearch-basedsolverssuchasGurobiand
SCIP.However,adrawbackoftheseoff-the-shelfsolversistheirinabilitytoefficientlysolvelargeproblems,especially
thosewithdenseglobalconstraints,suchastheCMPEproblem.Incontrast,neuralnetworksareefficientbecauseonce
trained,thetimecomplexityofsolvinganoptimizationproblemusingthemscaleslinearlywiththenetwork‚Äôssize.
ThisattractivepropertyhasalsodriventheirapplicationinsolvingprobabilisticinferencetaskssuchasMARandMPE
inference(Gilmeretal.,2017;Kucketal.,2020;Zhangetal.,2020;SatorrasandWelling,2021).However,allofthese
worksrequireaccesstoexactinferencetechniquesinordertotraintheneuralnetwork.Asaresult,theyarefeasible
onlyforsmallgraphicalmodelsonwhichexactinferenceistractable.Recently,Cuietal.(2022)proposedtosolvethe
MPEtaskbytrainingavariationaldistributionthatisparameterizedbyaneuralnetworkinaself-supervisedmanner
(withoutrequiringaccesstoexactinferencemethods).Tothebestofourknowledge,thereisnopriorworkonusing
neuralnetworksforsolvingtheCMPEproblem.
Inthispaper,weproposeanewself-supervisedapproachfortrainingneuralnetworkswhichtakesobservationsor
evidenceasinputandoutputsanearoptimalsolutiontotheCMPEtask.Existingself-supervisedapproaches(Fioretto
etal.,2020;ParkandVanHentenryck,2022)inthelearningtooptimizeliteratureeitherrelaxtheconstrainedobjective
function using Lagrangian relaxation and then use the Langragian dual as a loss function or use the Augmented
Lagrangianmethod.WeshowthatthesemethodscanbeeasilyadaptedtosolvetheCMPEtask.Unfortunately,an
issuewiththemisthatanoptimalsolutiontotheLagrangiandualisnotguaranteedtobeanoptimalsolutiontothe
CMPEtask(becauseofthenon-convexityofCMPE,thereisadualitygap).Toaddressthisissue,weproposeanew
lossfunctionbasedonfirstprinciplesandshowthatanoptimalsolutiontothelossfunctionisalsoanoptimalsolution
totheCMPEtask.Moreover,ournewlossfunctionhasseveraldesirableproperties,whichinclude:(a)duringtraining,
whentheconstraintisviolated,itfocusesondecreasingthestrengthoftheviolation,and(b)whenconstraintsarenot
violated,itfocusesonincreasingthevalueoftheobjectivefunctionassociatedwiththeCMPEtask.
Weconductedacomprehensiveempiricalevaluation,comparingseveralsupervisedandself-supervisedapproachesto
ourproposedmethod.Tothebestofourknowledge,thesearethefirstempiricalresultsonusingmachinelearning,either
supervisedorself-supervised,tosolvetheCMPEtaskinPGMs.Onanumberofbenchmarkmodels,ourexperiments
showthatneuralnetworkstrainedusingourproposedlossfunctionaremoreefficientandaccuratecomparedtomodels
trainedtominimizecompetingsupervisedandself-supervisedlossfunctionsfromtheliterature.
2 NotationandBackground
Wedenoterandomvariablesbyupper-caseletters(e.g.,X,Y,Z,etc.),theircorrespondingassignmentsbylower-case
letters(e.g.,x,y,z,etc.),setsofrandomvariablesbyboldupper-caseletters(e.g.,X,Y,Z,etc.)andassignmentsto
thembyboldlower-caseletters(e.g.,x,y,z,etc.).z denotestheprojectionofthecompleteassignmentzontothe
X
subsetXofZ.Forsimplicityofexposition,weassumethatdiscreteandcontinuousrandomvariablestakevaluesfrom
theset{0,1}and[0,1]respectively.
We use the multilinear polynomial representation (Sherali and Adams, 2009; Sherali and Tuncbilek, 1992; Horst
andTuy,1996)toconciselydescribeourproposedmethodaswellasforspecifyingdiscrete,continuous,andmixed
constrainedoptimizationproblems.LetZ = {Z ,...,Z }beasetofrandomvariables.Let[n] = {1,...,n}and
1 n
i‚àà[n]beanindexoverthevariablesofZ.Let2[n]denotethesetofsubsetsofindicesof[n];thuseachelementof2[n]
2LearningtoSolvetheCMPETaskinPGMs APREPRINT
x y ‚Ñé t
1 1 ! 1
0 0 4 10
ùëã!
h(x, y) = 4 ‚àí 3x ‚àí y + 5xy
ùëå ! 0 1 3 5
1 1 1 1 1 1 1 1 0 1 12
t(x, y) = 10 + 2x ‚àí 5y + 2xy
1 1 1 1 1 1 1 1 1 5 9
y y h t
1 2 2 2
0 0 7 1
h 2(y 1, y 2) = 7 ‚àí 6y 1 + 2y 2 ‚àí y 1y 2 0 1 9 3
t(y, y) = 1 + y + 2y
2 1 2 1 2 1 0 1 2
1 1 2 4
x y h t
2 2 3 3
0 0 7 17
ùëã" ùëå" 0 1 4 13
h(x, y) = 7 + x ‚àí 3ùë¶ ‚àí 4xy
3 2 2 2 " 2 2 1 0 8 10
t(x, y) = 17 ‚àí 7x ‚àí 4y ‚àí xy
3 2 2 2 2 2 2 1 1 1 5
Figure 1: Two Markov networks M and M having the same chain-like structure and defined over the same set
1 2
{X ,X ,Y ,Y }ofvariables.M isdefinedbythesetoflog-potentials{h ,h ,h }andM isdefinedbythesetof
1 2 1 2 1 1 2 3 2
log-potentials{t ,t ,t }.Eachlog-potentialcanbeexpressedasalocalmultilinearpolynomialfunction.Theglobal
1 2 3
multilinearfunctionrepresentingM andM areh(x ,x ,y ,y )=18‚àí3x +x ‚àí7y ‚àíy +5x y ‚àí4x y ‚àíy y
1 2 1 2 1 2 1 2 1 2 1 1 2 2 1 2
and t(x ,x ,y ,y ) = 28+2x ‚àí7x ‚àí4y ‚àí2y +2x y ‚àíx y respectively which are obtained by adding
1 2 1 2 1 2 1 2 1 1 2 2
thelocalfunctionsassociatedwiththerespectivemodelsandthensimplifying,i.e.,h(x ,x ,y ,y )=h (x ,y )+
1 2 1 2 1 1 1
h (y ,y )+h (x ,y ).t(x ,x ,y ,y )isobtainedsimilarly.
2 1 2 3 2 2 1 2 1 2
denotesa(unique)subsetofZ.LetI ‚äÜ2[n] andletw ‚ààRwhereI ‚ààI bearealnumber(weight)associatedwith
I
eachelementI ofI.Then,amultilinearpolynomialisgivenby
(cid:88) (cid:89)
f(z)=f(z ,...,z )= w z (1)
1 n I i
I‚ààI i‚ààI
wherez=(z ,...,z )isanassignmenttoallvariablesinZ.Wewillcallf(z)theweightofz.
1 n
It is known that weighting functions, namely the sum of log of conditional probability tables and log-potentials
associated with Bayesian and Markov networks respectively can be expressed as multilinear polynomials (see for
example(KollerandFriedman,2009)).
Example 1. Figure 1 shows a multilinear representation for a Markov network. The weight of the assignment
(X =0,X =1,Y =0,Y =1)is14and18w.r.t.M andM respectively.
1 2 1 2 1 2
2.1 ConstrainedMostProbableExplanation
Weareinterestedinsolvingthefollowingconstrainedmostprobableexplanation(CMPE)task.LetXandYbetwo
subsetsofZsuchthatZ=X‚à™YandX‚à©Y =‚àÖ.WewillrefertoYasdecisionvariablesandXasevidencevariables.
Givenassignmentsxandy,let(x,y)denotetheircomposition.LethandtdenotetwomultilinearpolynomialsoverZ
obtainedfromtwoMarkovnetworksM andM respectivelythatrepresenttwo(possiblydifferent)jointprobability
1 2
distributionsoverZ.ThengivenarealnumberqandanassignmentxtoallvariablesinX,theCMPEtaskistofindan
assignmenty‚àótoallthevariablesinYsuchthath(x,y‚àó)ismaximized(namelytheprobabilityoftheassignmentw.r.t.
M ismaximized)andt(x,y‚àó)‚â§q(namelytheprobabilityoftheassignmentw.r.t.M isboundedbyaconstant).
1 2
Formally,
maximize h(x,y) s.t. t(x,y)‚â§q (2)
y
For brevity, we will abuse notation and use h (y) and t (y) to denote h(x,y) and t(x,y) respectively. The most
x x
probableexplanation(MPE)taskinprobabilisticgraphicalmodels(KollerandFriedman,2009)isaspecialcaseof
CMPE;MPEisjustCMPEwithouttheconstraintt (y)‚â§q.ThegoalinMPEistofindanassignmenty‚àótoYsuch
x
thattheweighth (y‚àó)oftheassignmentismaximizedgivenevidencex.SimilartoMPE,CMPEisNP-hardingeneral,
x
withthecaveatthatCMPEismuchharderthanMPE.Specifically,CMPEisNP-hardevenonindependentgraphical
models(havingzerotreewidth),whereMPEcanbesolvedinlineartimebyindependentlymaximizingeachunivariate
function(Rouhanietal.,2020).
Example2. GivenX =1,X =1andq =20,theCMPEsolutionoftheexampleprobleminfigure1is(y‚àó,y‚àó)=
1 2 1 2
(0,1)withavalueh(1,1,0,1)=11,whereastheMPEsolutionis(y‚àó,y‚àó)=(0,0)withvalueh(1,1,0,0)=16.
1 2
3LearningtoSolvetheCMPETaskinPGMs APREPRINT
SinceweareinterestedinmachinelearningapproachestosolvetheCMPEtaskandsuchapproachesemployloss
functions,itisconvenienttoexpressCMPEasaminimizationtaskwitha"‚â§0"constraint.Thiscanbeaccomplishedby
negatinghandsubtractingqfromt.Formally,letf (y)=‚àíh (y)andg (y)=t (y)‚àíq.ThenEq.(2)isequivalent
x x x x
tothefollowingminimizationproblem:
minimize f (y) s.t. g (y)‚â§0 (3)
x x
y
Lety‚àóbetheoptimalsolutiontotheproblemgiveninEq.(3)andletp‚àó =f (y‚àó).Also,withoutlossofgenerality,we
x x
assumethatf isstrictlypositive,i.e.,‚àÄy,f (y)>0.
x x
If all variables in Y are binary (or discrete in general), Eq. (3) can be formulated as an (equivalent) integer linear
programming(ILP)problembyintroducingauxiliaryintegervariablesforeachmultilinearterm(e.g.,y = y y ,
1,2 1 2
y = y y ,etc.)andaddingappropriateconstraintstomodeltheequivalencebetweentheauxiliaryvariablesand
2,3 2 3
multilinearterms(seeforexample(KollerandFriedman,2009),Chapter13).Therefore,inpractice,(3)canbesolved
optimallyusingmixedintegerlinearprogramming(MILP)solverssuchasGurobi(GurobiOptimization,2021)and
SCIP(Achterbergetal.,2008;Achterberg,2009).
Unfortunately,duetoapresenceofadenseglobalconstraint,namelyg (y) ‚â§ 0inEq.(3),theMILPsolversoften
x
performpoorly.Instead,inpractice,applicationdesignersoftenuseefficient,specializedalgorithmsthatexploitproblem
structureforlowerboundingp‚àó,andthenusingtheselowerboundsinananytimebranch-and-boundalgorithmtoobtain
x
anupperboundonp‚àó.
x
2.2 SpecializedLowerBoundingAlgorithms
Recently,Rahmanetal.(2021)proposedtwonewapproachesforcomputingupperboundsontheoptimalvalueofthe
maximizationproblemgiveninEq.(2).Thesemethodscanbeeasilyadaptedtoobtainalowerboundonp‚àó;because
x
anupperboundonthemaximizationproblemisalowerboundonthecorrespondingminimizationproblem.Wepresent
theadaptationsofRahmanetal.‚Äôsapproachnext.
The first approach is based on the Lagrangian relaxation method that introduces a Lagrange multiplier ¬µ ‚â• 0 to
transformtheconstrainedminimizationproblemtothefollowingunconstrainedproblem:minimize f (y)+¬µg (y).
y x x
Letd‚àó denotetheoptimalvalueoftheunconstrainedproblem.Then,itiseasytoshowthatd‚àó ‚â§p‚àó.Thelargestupper
¬µ ¬µ x
boundisobtainedbyfindingavalueof¬µthatmaximizesd‚àó.Moreformally,
¬µ
maxd‚àó =maxminf (y)+¬µg (y)‚â§p‚àó (4)
¬µ x x x
¬µ‚â•0 ¬µ‚â•0 y
Rahmanetal.(2021)proposedtosolvetheinnerminimizationproblemusingexacttechniquesfromthegraphicalmodels
literaturesuchasvariable/bucketelimination(Dechter,1999),branchandboundsearchandbest-firstsearch(Marinescu
andDechter,2012,2009;Wuetal.,2020).Whenexactinferenceisnotfeasible,Rahmanetal.(2021)proposedto
solvetheinnerproblemusingapproximateinferencetechniquessuchasmini-bucketelimination,dual-decomposition
andjoin-graphbasedboundingalgorithms(ChoiandDarwiche,2011;DechterandRish,2003;Wainwrightetal.,2005;
GlobersonandJaakkola,2007;Komodakisetal.,2007;Ihleretal.,2012).Theoutermaximizationproblemissolved
usingsub-gradientascent.
ThesecondapproachbyRahmanetal.(2021)usestheLagrangiandecompositionmethodtotransformtheproblem
intoamulti-choiceknapsackproblem(MCKP)andthenutilizesoff-the-shelfMCKPsolvers.Inourexperiments,we
usetheLagrangerelaxationapproachgiveninEq.(4).
IfthesetY containscontinuousvariables,thenitisnotpossibletoreduceittoanequivalentMILP/LP(Horstand
Tuy,1996;SheraliandTuncbilek,1992).However,byleveraginglinearizationmethods(SheraliandTuncbilek,1992;
SheraliandAdams,2009)andsolvingtheresultingproblemusinglinearprogramming(LP)solvers,wecanstillobtain
goodlowerboundsonp‚àó.
x
3 SolvingCMPEusingMethodsfromtheLearningtoOptimizeLiterature
Inthissection,weshowhowtechniquesdevelopedinthelearningtooptimizeliterature(Dontietal.,2021;Fioretto
etal.,2020;ParkandVanHentenryck,2022;ZamzamandBaker,2019)whichseekstodevelopmachinelearning
approachesforsolvingconstrainedoptimizationproblemscanbeleveragedtosolvetheCMPEtask.Themainideaisto
trainadeepneuralnetworkF :X‚ÜíYparameterizedbythesetŒò‚ààRM suchthatattesttimegivenevidencex,
Œò
thenetworkisabletopredictan(near)optimalsolutionyÀÜ totheCMPEproblem.Notethatasfarasweareaware,no
priorworkexistsonsolvingCMPEusingdeepneuralnetworks.
4LearningtoSolvetheCMPETaskinPGMs APREPRINT
3.1 SupervisedMethods
In order to train the parameters of F in a supervised manner, we need to acquire labeled data in the form D =
Œò
{‚ü®x ,y ‚ü©}N where each label y is an optimal solution to the problem given in Eq. (3) given x . In practice, we
i i i=1 i i
cangeneratetheassignments{x }N bysamplingthemfromthegraphicalmodelcorrespondingtof andthelabels
i i=1
{y }N bysolvingtheminimizationproblemgiveninEq.(3)usingoff-the-shelfsolverssuchasGurobiandSCIP.
i i=1
LetyÀÜ =F (x )denotethelabelspredictedbytheneuralnetworkforx .FollowingZamzamandBaker(2019),we
i Œò i i
proposetotrainF usingthefollowingtwolossfunctions
Œò
Mean-SquaredError(MSE): 1 (cid:88) (y ‚àíyÀÜ )2 (5)
N i i
i
1 (cid:88)
Mean-Absolute-Error(MAE): |y ‚àíyÀÜ | (6)
N i i
i
Experimentally(seethesupplementarymaterial),wefoundthatneuralnetworkstrainedusingtheMAEandMSE
lossfunctionsoftenoutputinfeasibleassignments.Toaddressthisissue,followingpriorwork(NellikkathandChatzi-
vasileiadis,2021),weproposetoaddŒª max{0,g (yÀÜ)}tothelossfunctionwhereŒª isapenaltycoefficient.
x x x
Inpriorwork(Fiorettoetal.,2020),itwasobservedthatthequalityofthesolutionsgreatlydependsonthevaluechosen
forŒª .Moreover,itisnotstraightforwardtochooseitoptimallybecauseitvariesforeachx.Tocircumventthisissue,
x
weproposetoupdateŒª viaaLagrangiandualmethod(NocedalandWright,2006).Morespecifically,wepropose
x
tousethefollowingsubgradientmethodtooptimizethevalueofŒª .Whiletraininganeuralnetwork,letŒªk andyÀÜk
x xi i
denotethevaluesofthepenaltyco-efficientandthepredictedassignmentrespectivelyatthek-thepochandforthei-th
exampleinD(ifthei-thexampleispartofthecurrentmini-batch),then,weupdateŒªk+1using
xi
Œªk+1 =Œªk +œÅmax{0,g (yÀÜk)} (7)
xi xi xi i
whereœÅistheLagrangianstepsize.Inourexperiments,weevaluatedboththenaiveandthepenaltybasedsupervised
lossapproaches(forCMPE)andfoundthatthepenaltymethodwithMSElossyieldsthebestresults.Therefore,inour
experiments,weuseitasastrongsupervisedbaseline.
3.2 Self-SupervisedMethods
Supervisedmethodsrequirepre-computedsolutionsfornumerousNP-hard/multilinearprobleminstances,whichare
computationallyexpensivetoderive.Therefore,weproposetotraintheneuralnetworkinaself-supervisedmannerthat
doesnotdependonthepre-computedresults.UtilizingfindingsfromKotaryetal.(2021)andParkandVanHentenryck
(2022),weintroducetwoself-supervisedapproaches:oneisgroundedinthepenaltymethod,andtheotherbuildsupon
theaugmentedLagrangianmethod.
PenaltyMethod(Dontietal.,2021;Kotaryetal.,2021;Fiorettoetal.,2020).Inthepenaltymethod,wesolvethe
constrained minimization problem by iteratively transforming it into a sequence of unconstrained problems. Each
unconstrained problem at iteration k is constructed by adding a term, which consists of a penalty parameter Œªk
x
multipliedbyafunctionmax{0,g (y)}2thatquantifiestheconstraintviolations,totheobjectivefunction.Formally,
x
theoptimizationproblematthek-thstepisgivenby:
Œªk
minf (y)+ x max{0,g (y)}2 (8)
y x 2 x
Here,Œªk isprogressivelyincreasedeitheruntiltheconstraintissatisfiedorapredefinedmaximumŒª isreached.Œªk
x max x
canbeupdatedafterafewepochsusingsimplestrategiessuchasmultiplicationbyafixedfactor(e.g.,2,10,etc.).
Thepenaltymethodcanbeadaptedtolearnaneuralnetworkinaself-supervisedmannerasfollows.Ateachepoch
k,wesampleanassignmentx(ormultiplesamplesforamini-batch)fromthegraphicalmodelcorrespondingtof,
predictyÀÜ usingtheneuralnetworkandthenusethefollowinglossfunctiontoupdateitsparameters:
Œªk
Lpen(yÀÜ)=f (yÀÜ)+ x max{0,g (yÀÜ)}2 (9)
x x 2 x
DeterminingtheoptimalŒªk iscrucial.Inpriorwork,Kotaryetal.(2021)andFiorettoetal.(2020)proposedtoupdate
x
itviaasubgradientmethod,similartotheupdaterulegivenbyEq.(7).Moreformally,wecanupdateŒªk using:
x
Œªk+1 =Œªk +œÅmax(cid:8) 0,g (yÀÜk)(cid:9) (10)
x x x
5LearningtoSolvetheCMPETaskinPGMs APREPRINT
whereœÅistheLagrangianstepsize.
AugmentedLagrangianMethod(ALM).Inthismethod,weaugmenttheobjectiveusedinthepenaltymethodwitha
Lagrangianterm.Moreformally,theoptimizationproblematthek-thstepisgivenby(comparewithEq.(8)):
Œªk
minf (y)+ x max{0,g (y)}2+¬µkg (y) (11)
y x 2 x x x
Here,Œªk maybeprogressivelyincreasedsimilartothepenaltymethodwhile¬µk isupdatedusing
x x
¬µk+1 =max(cid:8) 0,¬µk+Œªkg (yk)(cid:9) (12)
x x x x
Recently,ParkandVanHentenryck(2022)proposedaself-supervisedprimal-duallearningmethodthatleveragestwo
distinctnetworkstoemulatethefunctionalityofALM:thefirst(primal)networktakesasinputxandoutputsywhile
thesecondnetworkfocusesonlearningthedualaspects;specificallyittakesxasinputandoutputs¬µk.Thetraining
x
processusesasequentialapproach,whereonenetworkistrainedwhiletheotherremainsfrozentofurnishtherequisite
valuesforthelosscomputation.
Theprimalnetworkusesthefollowinglossfunction:
Œª
LA,p(yÀÜ|¬µ,Œª)=f (yÀÜ)+ max{0,g (yÀÜ)}2+¬µg (y)
x x 2 x x
Whilethedualnetworkusesthefollowinglossfunction
LA,d(¬µÀÜ|y,Œª,¬µk)=||¬µÀÜ‚àímax(cid:8) 0,¬µk+Œªg (y)(cid:9) ||
x x
where¬µÀÜisthepredictedvalueoftheLagrangianmultiplier.
3.2.1 DrawbacksofthePenaltyandALMMethods
Alimitationofthepenalty-basedself-supervisedmethodisthatitdoesnotguaranteeaglobalminimumunlessspecific
conditions are met. In particular, the optimal solution w.r.t. the loss function (see Eq. (9)) may be far away from
theoptimalsolutiony‚àó oftheproblemgiveninEq.(3),unlessthepenaltyco-efficientŒªk ‚Üí ‚àû.Moreover,when
x
Œªk is large for all x, the gradients will be uninformative. In the case of ALM method (cf. (Nocedal and Wright,
x
2006)), for global minimization, we require that either Œªk ‚Üí ‚àû or ‚àÄx with g (y) > 0, ¬µk should be such that
x x x
min f (y)+¬µkg (y)>p‚àó.Additionally,ALMintroducesadualnetwork,increasingthecomputationalcomplexity
y x x x x
andpotentiallyleadingtonegativeinformationtransferwhenthedualnetwork‚Äôsoutputsareinaccurate.Theseoutputs
aresubsequentlyutilizedinthelosstotraintheprimalnetworkforthefollowingiteration,therebyexertinganegative
effect.Toaddresstheselimitations,next,weintroduceaself-supervisedmethodthatachievesglobalminimization
withouttheneedforadualnetworkorinfinitepenaltycoefficients.
4 ANOVELSELF-SUPERVISEDCMPESOLVER
Anappropriatelydesignedlossfunctionshouldhavethefollowingcharacteristics.Forfeasiblesolutions,namelywhen
g (y)‚â§0,thelossfunctionshouldbeproportionaltof (y).Whileforinfeasibleassignments,itshouldequalinfinity.
x x
Thislossfunctionwillensurethatonceafeasiblesolutionisfound,theneuralnetworkwillonlyexplorethespaceof
feasiblesolutions.Unfortunately,infinitydoesnotprovideanygradientinformation,andtheneuralnetworkwillget
stuckintheinfeasibleregioniftheneuralnetworkgeneratesaninfeasibleassignmentduringtraining.
Analternativeapproachistousegasalossfunctionwhentheconstraintisnotsatisfied(i.e.,g (y)>0)inorderto
x
pushtheinfeasiblesolutionstowardsfeasibleones(LiuandCherian,2023).Unfortunately,thisapproachwilloftenyield
feasiblesolutionsthatlieattheboundaryg (y)=0.Forinstance,foraboundaryassignmenty whereg (y )=0but
x b x b
f (y )>0(ordecreasing),thesub-gradientwillbezero,andtheneuralnetworkwilltreattheboundaryassignmentas
x b
anoptimalone.
Tocircumventthisissue,weproposealossfunctionwhichhasthefollowingtwoproperties:(1)Itisproportionaltog
intheinfeasibleregionwithf actingasacontrolintheboundaryregion(whengiszero);and(2)Itisproportionaltof
inthefeasibleregion.Formally,
(cid:26)
f (yÀÜ) if g (yÀÜ)‚â§0
L (yÀÜ)= x x (13)
x Œ± (f (yÀÜ)+g (yÀÜ)) if g (yÀÜ)>0
x x x x
whereŒ± isafunctionoftheevidencex.OurgoalistofindaboundforŒ± suchthatthefollowingdesirableproperty
x x
issatisfiedandtheboundcanbecomputedinpolynomialtimeforeachxbyleveragingboundingmethodsforCMPE.
6LearningtoSolvetheCMPETaskinPGMs APREPRINT
Property(ConsistentLoss):Thelossforallinfeasibleassignmentsishigherthantheoptimalvaluep‚àó.
x
Tosatisfythisproperty,wehavetoensurethat:
‚àÄyÀÜ s.t. g (yÀÜ)>0, Œ± (f (yÀÜ)+g (yÀÜ))>p‚àó
x x x x x
whichimpliesthatthefollowingconditionholds.
(cid:18) (cid:19)
Œ± min f (yÀÜ)+g (yÀÜ) s.t. g (yÀÜ)>0 >p‚àó
x x x x x
yÀÜ
Letq x‚àó denotetheoptimalvalueofmin yÀÜf x(yÀÜ)+g x(yÀÜ) s.t. g x(yÀÜ)>0.Then,Œ± x > p q‚àó x ‚àó.
x
Proposition 4.1. If L x(yÀÜ) is consistent, i.e., Œ± x > p q‚àó x ‚àó then min yÀÜL x(yÀÜ) = p‚àó x, namely L x(yÀÜ) is an optimal loss
x
function.
Proof. Fromequation(13),wehave
(cid:26)
minL (yÀÜ)=min minf (yÀÜ) s.t. g (yÀÜ)‚â§0,
x x x
yÀÜ yÀÜ
(cid:18) (cid:19)(cid:27)
Œ± min f (yÀÜ)+g (yÀÜ) s.t. g (yÀÜ)>0
x x x x
yÀÜ
=min{p‚àó,Œ± q‚àó} (14)
x x x
BecauseL x(yÀÜ)isconsistent,namely,Œ± x > p q‚àó x ‚àó,wehave
x
min{p‚àó,Œ± q‚àó}=p‚àó (15)
x x x x
Fromequations(14)and(15),theprooffollows.
Weassumethatf (y)andg (y)areboundedfunctions,namelyforanyassignment(x,y),l ‚â§ f (y) ‚â§ u and
x x f x f
l ‚â§g (y)‚â§u where‚àí‚àû<s<‚àûands‚àà{l ,u ,l ,u }.Also,forsimplicity,weassumethatf (y)isastrictly
g x g f f g g x
positivefunction,namelyl >0.
f
Thus,basedontheassumptionsgivenabove,wehave
p‚àó u u
x ‚â§ f and 0<Œ± ‚â§ f
q‚àó l x l
x f f
Theaboveassumptionswillensurethatthegradientsarebounded,becauseŒ± ,f andgarebounded,andbothp‚àó and
x x
q‚àó aregreaterthanzero.
x
Next,weshowhowtocomputeanupperboundonŒ± x usingŒ± x > p q‚àó x ‚àó,thusensuringthatwehaveanoptimalloss
function.Thetermsinthenumerator(p‚àó)anddenominator(q‚àó)requirex
solvingtwoinstancesoftheCMPEtask.Since
x x
solvingCMPEexactlyisimpracticalandmoreover,sinceweareinterestedinself-supervisedmethodswherewedonot
assumeaccesstosuchasolver,weproposetolowerboundq‚àó andupperboundp‚àó.
x x
Foragiveninstancex,alowerboundonq‚àó canbeobtainedusingtheLagrangianrelaxationmethoddescribedin
x
section2.2(seeEq.(4))fordiscretevariablesandtheReformulation-LinearizationmethoddescribedinSheraliand
Tuncbilek(1992)forcontinuousvariables.Ontheotherhand,anyfeasiblesolutioncanserveasanupperboundforp‚àó.
x
AsimpleyetefficientapproachistobeginwithalooseupperboundbyupperboundingtheMPEtask:max f (y),
y x
using fast algorithms such as mini-bucket elimination (Dechter and Rish, 2003) or fast linear programming based
approximations(Ihleretal.,2012;GlobersonandJaakkola,2007)andthenkeeptrackoffeasiblesolutionsduring
batch-stylegradientdescent.
Insummary,weproposedanewlossfunctionwhichusesthequantityŒ± .Whentheneuralnetworkpredictsafeasible
x
yÀÜ,thelossequalsf,whereaswhenitpredictsaninfeasibleyÀÜ,thelossissuchthattheinfeasiblesolutioncanquicklybe
pushedtowardsafeasiblesolution(becauseitusesgradientsfromg).Akeyadvantageofourproposedlossfunctionis
thatŒ± isnottreatedasanoptimizationvariable,andaboundonitcanbepre-computedforeachexamplex.
x
7LearningtoSolvetheCMPETaskinPGMs APREPRINT
Table1:AveragegapandconstraintviolationsovertestsamplesformodelsfromtheUAIcompetition.¬±denotes
standarddeviation.Boldvaluesindicatethemethodswiththehighestperformance.Underlinedvaluesdenotesignificant
violations,particularlythoseexceedingathresholdof0.15.Forthesemethods,thegapvaluesarenotconsideredinour
analysis.
Methods Segment12 Segment14 Segment15 Grids17 Grids18
ILPObj. 463.454 471.205 514.287 2879.469 4160.196
Gap 0.053¬±0.043 0.053¬±0.043 0.053¬±0.041 0.092¬±0.070 0.082¬±0.065
SL
pen Violations 0.238¬±0.426 0.248¬±0.432 0.153¬±0.361 0.054¬±0.226 0.053¬±0.224
Gap 0.051¬±0.042 0.065¬±0.048 0.056¬±0.044 0.089¬±0.055 0.104¬±0.062
SSL
pen Violations 0.149¬±0.357 0.127¬±0.332 0.086¬±0.281 0.004¬±0.059 0.005¬±0.071
Gap 0.063¬±0.050 0.055¬±0.042 0.063¬±0.049 0.102¬±0.059 0.092¬±0.061
PDL
Violations 0.073¬±0.261 0.120¬±0.326 0.016¬±0.126 0.000¬±0.000 0.001¬±0.022
Gap 0.055¬±0.045 0.051¬±0.040 0.068¬±0.051 0.067¬±0.049 0.069¬±0.051
SS-CMPE
Violations 0.093¬±0.291 0.107¬±0.415 0.002¬±0.039 0.001¬±0.032 0.001¬±0.022
4.1 MakingTheLossFunctionSmoothandContinuous
ThelossfunctiondefinedinEq.(13)iscontinuousanddifferentialeverywhereexceptatg (yÀÜ)=0.Thereisajump
x
discontinuityatg (yÀÜ) = 0sincelim f (yÀÜ) Ã∏= lim Œ± (f (yÀÜ)+g (yÀÜ)).Toaddressthisissue,we
x gx(yÀÜ)‚Üí0‚àí x gx(yÀÜ)‚Üí0+ x x x
proposethefollowingcontinuousapproximation
(cid:16) (cid:17)
L(cid:101)x(yÀÜ)= (1‚àíœÉ(Œ≤g x(yÀÜ)))¬∑[f x(yÀÜ)] + (16)
(cid:16) (cid:17)
œÉ(Œ≤g (yÀÜ))¬∑[Œ± (f (yÀÜ)+max{0,g (yÀÜ)})]
x x x x
whereœÉ(.)isthesigmoidfunctionandŒ≤ ‚â•0isahyper-parameterthatcontrolsthesteepnessofthesigmoid.Atahigh
level,theabovecontinuousapproximationusesasigmoidfunctiontoapproximateaHeavisidestepfunction.
5 EXPERIMENTALEVALUATION
0.16
SSL+Penalty
0.10 0.065 0.14 PDL SS-CMPE
0.12
0.09
0.060
0.10
0.08 0.08
SSL+Penalty 0.055 SSL+Penalty
PDL PDL 0.06
0.07 SS-CMPE SS-CMPE
0.04
0.000 0.001 0.002 0.003 0.004 0.005 0.00 0.05 0.10 0.15 0.00 0.02 0.04 0.06
Average Violations Average Violations Average Violations
(a)Grids (b)Segmentation (c)TractableModels
Figure2: OptimalityGap(avg%)andAverageViolationsforSelf-Supervisedmethods
Inthissection,wethoroughlyevaluatetheeffectivenessofourproposedneuralnetworksbasedsolversforCMPE.
Weevaluatethecompetingmethodsonseveraltestproblemsusingthreecriteria:optimalitygap(relativedifference
betweentheoptimalsolutionandtheonefoundbythemethod),constraintviolations(percentageoftimethemethod
outputsaninfeasiblesolution),andtrainingandinferencetimes.
5.1 TheLossFunctions:CompetingMethods
We trained several neural networks to minimize both supervised and self-supervised loss functions. We evaluated
bothMSEandMAEsupervisedlosseswithandwithoutpenaltycoefficients(seesection3).Inthemainpaper,we
showresultsonthebestperformingsupervisedloss,whichisMSEwithpenalty,denotedbySL (resultsforother
pen
supervisedlossfunctionsareprovidedinthesupplement).
8
paG
ytilamitpO
paG
ytilamitpO
paG
ytilamitpOLearningtoSolvetheCMPETaskinPGMs APREPRINT
Table 2: Average gap and constraint violations over test samples from tractable probabilistic models. ¬± denotes
standarddeviation.Boldvaluesindicatethemethodswiththehighestperformance.Underlinedvaluesdenotesignificant
violations,particularlythoseexceedingathresholdof0.15.Forthesemethods,thegapvaluesarenotconsideredinour
analysis.
Methods AD BBC DNA 20NewsGroup WebKB1
ILPObj. 2519.128 871.567 221.119 921.702 824.493
Gap 0.156¬±0.057 0.036¬±0.027 0.143¬±0.113 0.041¬±0.031 0.044¬±0.035
SL
pen Violations 0.135¬±0.341 0.237¬±0.425 0.151¬±0.358 0.084¬±0.277 0.070¬±0.254
Gap 0.159¬±0.055 0.045¬±0.033 0.142¬±0.116 0.045¬±0.036 0.058¬±0.043
SSL
pen Violations 0.008¬±0.089 0.056¬±0.230 0.014¬±0.118 0.005¬±0.071 0.025¬±0.158
Gap 0.154¬±0.055 0.051¬±0.036 0.144¬±0.117 0.046¬±0.035 0.059¬±0.043
PDL
Violations 0.000¬±0.000 0.025¬±0.156 0.006¬±0.077 0.004¬±0.059 0.012¬±0.109
Gap 0.134¬±0.054 0.043¬±0.033 0.138¬±0.112 0.045¬±0.035 0.057¬±0.043
SS-CMPE
Violations 0.006¬±0.077 0.056¬±0.230 0.007¬±0.083 0.005¬±0.071 0.016¬±0.126
Forself-supervisedloss,weexperimentedwiththefollowingthreeapproaches:(1)penalty-basedmethod,(2)ALM,
whichusesaprimal-dualloss(PDL),andtheapproachdescribedinsection4.Wewillrefertothesethreeschemesas
SSL ,PDL,andSS-CMPE,respectively.WeusedtheexperimentalsetupdescribedbyParkandVanHentenryck
pen
(2022)fortuningthehyperparametersofPDLandSSL .FortheSS-CMPEmethod,weemployedagridsearch
pen
approachtodeterminetheoptimalvaluesforŒ≤.TherangeofvaluesconsideredforŒ≤was{0.1,1.0,2.0,5.0,10.0,20.0}.
Notethatallmethodsusedthesameneuralnetworkarchitecture(describedinthesupplement)exceptPDL,whichuses
twoneuralnetworks.Weobtainedtheground-truthforthesupervisedtrainingbysolvingtheoriginalILPproblem
usingSCIP(Achterberg,2009)andGurobi(GurobiOptimization,2021).WereporttheobjectivevaluesoftheILP
solutionsineachtable(seeTables1,2,and3).
5.2 DatasetsandBenchmarks
We evaluate the competing algorithms (SL , SSL , PDL, and SS-CMPE) on a number of log-linear Markov
pen pen
networksrangingfromsimplemodels(lowtreewidth)tohightreewidthmodels.Thesimplemodelscompriseoflearned
tractableprobabilisticcircuits(Choietal.,2020)withoutlatentvariables,specifically,cutsetnetworks(Rahmanetal.,
2014)frombenchmarkdatasetsusedintheliterature.Thecomplex,hightreewidthmodelsaresourcedfrompastUAI
inference competitions (Elidan and Globerson, 2010). Finally, we evaluated all methods on the task of generating
adversarialexamplesforneuralnetworkclassifiers.
5.3 HighTree-WidthMarkovNetworksandTractableProbabilisticCircuits
OurinitialseriesofexperimentsfocusonhightreewidthGridsandImageSegmentationMarkovnetworksfromthe
UAIinferencecompetitions(Gogate,2014,2016).Inthisinvestigation,wegeneratedCMPEproblemsbyutilizing the
modelemployedintheUAIcompetitions,denotedasM .Subsequently,M wascreatedbyadjustingtheparameters
1 2
ofM whileincorporatinganoiseparameterœµdrawnfromanormaldistributionwithmean0andvarianceœÉ2 =0.1.
1
Toselectq,werandomlygenerated100samples,sortedthembasedontheirweight,andthenselectedtheweightof
the10th,30th,60th,80th,and90thsampleasavalueforq.Weassesstheimpactofchangingthevalueofq inthe
supplement.Experimentsinthemainbodyofthepaperuseqequaltotheweightofthe80thrandomsample.Foreach
network,werandomlychose60%ofvariablesasevidence(X)andtheremainingasqueryvariables(Y).Forboththe
UAImodelsandtractableprobabilisticcircuits,wegenerated10KsamplesfromM ,andused9Kfortrainingand1K
1
fortesting.Weused5-foldcrossvalidationforselectingthehyperparameters.
TheresultsfortheUAIdatasetsareshowninTable1.Weseethatforthemajorityofthedatasets,ourmethodproduces
solutionswithsuperiorgapvaluescomparedtotheothermethods.Eveninsituationswhereourmethodsdonotachieve
bettergapvalues,theyexhibitfewerviolations.Thisdemonstratestheeffectivenessandrobustnessofourmethods
in generating solutions that strike a balance between optimizing the objective function and maintaining constraint
adherence.Forcertaindatasets,ourmethodsexhibitsignificantlylowerconstraintviolations,evenupto10timesless
thansupervisedmethods.
Inthenextphaseofourstudy,weemployedMPE(MostProbableExplanation)tractablemodels,whichwerelearned
onfivehigh-dimensionaldatasets(seeLowdandDavis(2010)fordetailsinthedatasets):DNA,NewsGroup(c20ng),
WebKB1(cwebkb),AD,andBBC.TheselearnedmodelsservedasM .WethenappliedGaussiannoiseasdescribed
1
9LearningtoSolvetheCMPETaskinPGMs APREPRINT
ILP MSE MSE MAE MAE SSL PDL SS-CMPE
pen pen pen
Figure 3: Qualitative results on the adversarially generated MNIST digits. Each row represents an original image
followedbyacorrespondingimagegeneratedadversariallyby8differentmethods:ILP,MSE,SL+Penalty,MAE,
MAE+Penalty,SSL ,PDL,andSS-CMPE.
pen
earliertogenerateM basedonM .AsimilartrendcanbeobservedfortractableprobabilisticmodelsinTable2,
2 1
whereourmethodconsistentlyoutperformstheotherself-supervisedmethodsacrossalldatasets.Notonlydoesour
approachexhibitsuperiorperformanceintermsofgapvalues,butitalsodemonstratescomparable constraintviolations.
Whencomparingwiththesupervisedmethod,ourproposedalgorithmexhibitssignificantlyfewerconstraintviolations
whilemaintainingabetterorcomparablegapvalue.Thisemphasizesthestrengthofourmethodineffectivelybalancing
theoptimizationobjectivesandconstraintadherence,therebyofferingimprovedoverallperformancecomparedtoboth
theself-supervisedandsupervisedapproachesinthecontextoftractableprobabilisticmodels.InFigure2,wepresent
theaverageoptimalitygapandaverageviolationsfordifferentdatasetgroups.Itisimportanttonotethatresultscloser
totheoriginindicatebetterperformance.
5.4 AdversarialModificationontheMNISTDataset
Table3:Performancecomparisonofsupervisedandself-supervisedmethods.Thetablepresentstheaverageobjective
value,gap,andconstraintviolationsoverthetestexamples,alongwiththetrainingandinferencetimerequiredforeach
methodforadversarialexamplegeneration.Boldvaluessignifythemethodsthatachievedthebestscores.
Timeinseconds
Methods Obj.Value Gap Violation
Train Inf.
ILP 30.794 0.000 0.000 NA 5.730
SL 63.670 1.069 0.071 57534.4 0.003
pen
SSL 76.316 1.480 0.052 469.540 0.003
pen
PDL 66.400 1.158 0.055 839.025 0.003
SS-CMPE 62.400 1.028 0.021 520.149 0.003
Wealsoevaluatedourapproachonthetaskofadversarialexamplegenerationfordiscriminativeclassifiers,specifically
neuralnetworks.Adversarialexamplesplayacrucialroleinassessingtherobustnessofmodelsandfacilitatingthe
trainingofmoreresilientmodels.ThetaskofAdversarialExampleGenerationinvolvesproducingnewimagesby
makingminimalmodificationstoinputimagesthataremis-classifiedbythemodel. Rahmanetal.(2021)showedthat
thisproblemcanbereducedtoCMPE.Formally,letG beadifferentiable,continuousfunctiondefinedoverasetof
inputsX.GivenanassignmentX =x,weintroducethedecisionvariableD,whichtakesthevaluedwhenG >0and
d¬Øotherwise.Inthecontextofadversarialattacks,givenanimagex,ourobjectiveistogenerateanewimagex‚Ä≤such
thatthedistancebetweenxandx‚Ä≤isminimizedandthedecisionisflipped(namelyG <0).Weusedalog-linearmodel
F torepresentthesumabsolutedistancebetweenthepixels.Thenthetaskofadversarialexamplegenerationcanbe
formulatedasthefollowingCMPEproblem:maximize(cid:80) f(x‚Ä≤|x)s.t.G(x‚Ä≤|x)‚â§0.
f‚ààF
We evaluated the algorithms using the MNIST handwritten digit dataset (LeCun and Cortes, 2010). We trained a
multi-layeredneuralnetworkhaving>95%testaccuracyanduseditasourGfunction.Togenerateadversarialexamples
correspondingtoagiventestexample,wetrainedanautoencoderA:X ‚àí‚ÜíX‚Ä≤usingfourlossfunctionscorresponding
toSL ,SSL ,PDLandSS-CMPE.Weusedthedefaulttrain-testsplit(10Kexamplesfortestingand60Kfor
pen pen
training).
10LearningtoSolvetheCMPETaskinPGMs APREPRINT
Table3showsquantitativeresultscomparingourproposedSS-CMPEmethodwithothercompetingmethods.Wecan
clearlyseethatSS-CMPEissuperiortocompetingself-supervised(SSL andPDL)andsupervisedmethods(SL )
pen pen
intermsofbothconstraintviolationsandoptimalitygap.ThesecondbestmethodintermsofoptimalitygapisSL .
pen
However,itsconstraintviolationsaremuchhigher,anditstrainingtimeissignificantlylargerbecauseitneedsaccessto
labeleddata,whichinturnrequiresusingcomputationallyexpensiveILPsolvers.ThetrainingtimeofSS-CMPEis
muchsmallerthanPDL(becausethelatterusestwonetworks)andisonlyslightlylargerthanSSL .
pen
Figure 3 shows qualitative results on adversarial modification to the MNIST digits {1,2,6,7,8} by all the eight
methods.TheCMPEtaskminimallychangesaninputimagesuchthatthecorrespondingclassisflippedaccordingto
adiscriminativeclassifier.MSEandourproposedmethodSS-CMPEareverycompetitiveandwereabletogenerate
visuallyindistinguishable,high-qualitymodificationswhereastheothermethodsstruggledtodoso.
Summary:OurexperimentsshowthatSS-CMPEconsistentlyoutperformscompetingself-supervisedmethods,PDL
andSSL ,intermsofoptimalitygapandiscomparabletoPDLintermsofconstraintviolations.Thetrainingtimeof
pen
SS-CMPEissmallerthanPDL(byhalfasmuch)andisslightlylargerthanSSL .However,itisconsiderablybetter
pen
thanSSL intermsofconstraintviolations.SS-CMPEalsoemploysfewerhyperparametersascomparedtoPDL.
pen
6 CONCLUSIONANDFUTUREWORK
In this paper, we proposed a new self-supervised learning algorithm for solving the constrained most probable
explanation task which at a high level is the task of optimizing a multilinear polynomial subject to a multilinear
constraint. Our main contribution is a new loss function for self-supervised learning which is derived from first
principles,hasthesamesetofglobaloptimaastheCMPEtask,andoperatesexclusivelyontheprimalvariables.Italso
usesonlyonehyperparameterinthecontinuouscaseandtwohyperparametersinthediscretecase.Experimentally,
weevaluatedournewself-supervisedmethodwithpenalty-basedandLagrangianduality-basedmethodsproposedin
literatureandfoundthatourmethodisoftensuperiorintermsofoptimalitygapandtrainingtime(alsorequiresless
hyperparametertuning)totheLagrangianduality-basedmethodsandalsosuperiorintermsofoptimalitygapandthe
numberofconstraintviolationstothepenalty-basedmethods.
Ourproposedmethodhasseverallimitationsandwewilladdresstheminfuturework.First,itrequiresaboundforŒ± .
x
Thisboundiseasytoobtainforgraphicalmodels/multilinearobjectivesbutmaynotbestraightforwardtoobtainfor
arbitrarynon-convexfunctions.Second,theidealobjectiveintheinfeasibleregionshouldbeproportionaltog (y)but
x
ourmethodusesŒ± (f (y)+g (y)).
x x x
ACKNOWLEDGMENTS
ThisworkwassupportedinpartbytheDARPAPerceptually-EnabledTaskGuidance(PTG)Programundercontract
numberHR00112220005,bytheDARPAAssuredNeuroSymbolicLearningandReasoning(ANSR)undercontract
numberHR001122S0039andbytheNationalScienceFoundationgrantIIS-1652835.
Bibliography
Achterberg,T.(2009). Scip:solvingconstraintintegerprograms. MathematicalProgrammingComputation,1:1‚Äì41.
Achterberg, T., Berthold, T., Koch, T., and Wolter, K. (2008). Constraint integer programming: A new approach
tointegratecpandmip. InIntegrationofAIandORTechniquesinConstraintProgrammingforCombinatorial
OptimizationProblems:5thInternationalConference,CPAIOR2008Paris,France,May20-23,2008Proceedings5,
pages6‚Äì20.Springer.
Choi,A.andDarwiche,A.(2011). Relax,compensateandthenrecover. InOnada,T.,Bekki,D.,andMcCready,E.,
editors,NewFrontiersinArtificialIntelligence,pages167‚Äì180,Berlin,Heidelberg.SpringerBerlinHeidelberg.
Choi,A.,Xue,Y.,andDarwiche,A.(2012). Same-decisionprobability:Aconfidencemeasureforthreshold-based
decisions. InternationalJournalofApproximateReasoning,53(9):1415‚Äì1428.
Choi, Y., Vergari, A., and Van den Broeck, G. (2020). Probabilistic circuits: A unifying framework for tractable
probabilisticmodels. UCLA.URL:http://starai.cs.ucla.edu/papers/ProbCirc20.pdf.
Cui, Z., Wang, H., Gao, T., Talamadupula, K., and Ji, Q. (2022). Variational message passing neural network for
maximum-a-posteriori(map)inference. InUncertaintyinArtificialIntelligence,pages464‚Äì474.PMLR.
Darwiche,A.(2009). ModelingandReasoningwithBayesianNetworks. CambridgeUniversityPress.
Darwiche, A. and Hirth, A. (2020). On the reasons behind decisions. In Twenty Fourth European Conference on
ArtificialIntelligence,volume325ofFrontiersinArtificialIntelligenceandApplications,pages712‚Äì720.IOSPress.
11LearningtoSolvetheCMPETaskinPGMs APREPRINT
Darwiche,A.andHirth,A.(2023). Onthe(complete)reasonsbehinddecisions. JournalofLogic,Languageand
Information,32(1):63‚Äì88.
Dechter,R.(1999). Bucketelimination:Aunifyingframeworkforreasoning. ArtificialIntelligence,113:41‚Äì85.
Dechter,R.andRish,I.(2003). Mini-buckets:Ageneralschemeforboundedinference. JournaloftheACM(JACM),
50(2):107‚Äì153.
Donti,P.L.,Rolnick,D.,andKolter,J.Z.(2021). Dc3:Alearningmethodforoptimizationwithhardconstraints. arXiv
preprintarXiv:2104.12225.
Elidan,G.andGloberson,A.(2010). The2010UAIApproximateInferenceChallenge. Published:Availableonlineat:
http://www.cs.huji.ac.il/project/UAI10/index.php.
Fioretto,F.,Mak,T.W.,andVanHentenryck,P.(2020). Predictingacoptimalpowerflows:Combiningdeeplearning
andlagrangiandualmethods. InProceedingsoftheAAAIconferenceonartificialintelligence,volume34,pages
630‚Äì637.
Gilmer,J.,Schoenholz,S.S.,Riley,P.F.,Vinyals,O.,andDahl,G.E.(2017). Neuralmessagepassingforquantum
chemistry. InInternationalconferenceonmachinelearning,pages1263‚Äì1272.PMLR.
Globerson, A. and Jaakkola, T. (2007). Fixing max-product: Convergent message passing algorithms for map lp-
relaxations. Advancesinneuralinformationprocessingsystems,20:553‚Äì560.
Gogate, V. (2014). Results of the 2014 UAI competition. https://personal.utdallas.edu/~vibhav.
gogate/uai14-competition/index.html.
Gogate, V. (2016). Results of the 2016 UAI competition. https://personal.utdallas.edu/~vibhav.
gogate/uai16-competition/index.html.
GurobiOptimization,L.(2021). Gurobioptimizerreferencemanual.
Horst,R.andTuy,H.(1996). GlobalOptimization:DeterministicApproaches. SpringerBerlinHeidelberg.
Ihler,A.T.,Flerova,N.,Dechter,R.,andOtten,L.(2012). Join-graphbasedcost-shiftingschemes. arXivpreprint
arXiv:1210.4878.
Kingma,D.P.andBa,J.(2014). Adam:AMethodforStochasticOptimization. arXiv.
Koller,D.andFriedman,N.(2009). Probabilisticgraphicalmodels:principlesandtechniques. MITpress.
Komodakis,N.,Paragios,N.,andTziritas,G.(2007). Mrfoptimizationviadualdecomposition:Message-passing
revisited. In2007IEEE11thInternationalConferenceonComputerVision,pages1‚Äì8.IEEE.
Kotary, J., Fioretto, F., and Hentenryck, P. V. (2021). Learning hard optimization problems: A data generation
perspective. arXivpreprintarXiv:Arxiv-2106.02601.
Kuck,J.,Chakraborty,S.,Tang,H.,Luo,R.,Song,J.,Sabharwal,A.,andErmon,S.(2020). Beliefpropagationneural
networks. AdvancesinNeuralInformationProcessingSystems,33:667‚Äì678.
LeCun,Y.andCortes,C.(2010). MNISThandwrittendigitdatabase.
Liu,T.andCherian,A.(2023). Learningaconstrainedoptimizer:Aprimalmethod. InAAAI2023BridgeonConstraint
ProgrammingandMachineLearning.
Lowd,D.andDavis,J.(2010). LearningMarkovNetworkStructurewithDecisionTrees. In2010IEEEInternational
ConferenceonDataMining,pages334‚Äì343.IEEE.
Marinescu,R.andDechter,R.(2009). MemoryintensiveAND/ORsearchforcombinatorialoptimizationingraphical
models. AIJournal,173(16-17):1492‚Äì1524.
Marinescu, R. and Dechter, R. (2012). Best-First AND/OR Search for Most Probable Explanations. CoRR,
abs/1206.5268.
Nellikkath,R.andChatzivasileiadis,S.(2021). Physics-informedneuralnetworksforacoptimalpowerflow. arXiv
preprintarXiv:Arxiv-2110.02672.
Nocedal,J.andWright,S.J.(2006). NumericalOptimization. Springer,NewYork,NY,USA,2eedition.
Park,S.andVanHentenryck,P.(2022). Self-supervisedprimal-duallearningforconstrainedoptimization. arXiv
preprintarXiv:2208.09046.
Rahman,T.,Kothalkar,P.,andGogate,V.(2014). Cutsetnetworks:Asimple,tractable,andscalableapproachfor
improvingtheaccuracyofchow-liutrees. InMachineLearningandKnowledgeDiscoveryinDatabases:European
Conference,ECMLPKDD2014,Nancy,France,September15-19,2014.Proceedings,PartII14,pages630‚Äì645.
Springer.
12LearningtoSolvetheCMPETaskinPGMs APREPRINT
Rahman,T.,Rouhani,S.,andGogate,V.(2021). Novelupperboundsfortheconstrainedmostprobableexplanation
task. AdvancesinNeuralInformationProcessingSystems,34:9613‚Äì9624.
Rouhani,S.,Rahman,T.,andGogate,V.(2018). Algorithmsforthenearestassignmentproblem. InProceedingsofthe
Twenty-SeventhInternationalJointConferenceonArtificialIntelligence,IJCAI2018,July13-19,2018,Stockholm,
Sweden,pages5096‚Äì5102.ijcai.org.
Rouhani,S.,Rahman,T.,andGogate,V.(2020). Anovelapproachforconstrainedoptimizationingraphicalmodels.
AdvancesinNeuralInformationProcessingSystems,33:11949‚Äì11960.
Satorras, V. G. and Welling, M. (2021). Neural enhanced belief propagation on factor graphs. In International
ConferenceonArtificialIntelligenceandStatistics,pages685‚Äì693.PMLR.
Sherali,H.D.andAdams,W.P.(2009). Areformulation-linearizationtechnique(rlt)forsemi-infiniteandconvex
programs under mixed 0-1 and general discrete restrictions. Discrete Applied Mathematics, 157(6):1319‚Äì1333.
ReformulationTechniquesandMathematicalProgramming.
Sherali,H.D.andTuncbilek,C.H.(1992). Aglobaloptimizationalgorithmforpolynomialprogrammingproblems
usingareformulation-linearizationtechnique. JournalofGlobalOptimization,2:101‚Äì112.
Ucla-Starai(2023). Density-Estimation-Datasets. [Online;accessed17.May2023].
VanHaaren,J.andDavis,J.(2012). MarkovNetworkStructureLearning:ARandomizedFeatureGenerationApproach.
AAAI,26(1):1148‚Äì1154.
Wainwright,M.J.,Jaakkola,T.S.,andWillsky,A.S.(2005). Mapestimationviaagreementontrees:message-passing
andlinearprogramming. IEEEtransactionsoninformationtheory,51(11):3697‚Äì3717.
Wu, B., Shen, L., Zhang, T., and Ghanem, B. (2020). Map inference via l2 sphere linear program reformulation.
InternationalJournalofComputerVision,128(7):1913‚Äì1936.
Zamzam,A.andBaker,K.(2019). Learningoptimalsolutionsforextremelyfastacoptimalpowerflow. arXivpreprint
arXiv:Arxiv-1910.01213.
Zhang,Z.,Wu,F.,andLee,W.S.(2020). Factorgraphneuralnetworks. AdvancesinNeuralInformationProcessing
Systems,33:8577‚Äì8587.
13LearningtoSolvetheCMPETaskinPGMs APREPRINT
A DERIVINGUPPERBOUNDSFORp‚àó ANDLOWERBOUNDSFORq‚àó
x x
TocomputethevalueofŒ± ,weutilizethefollowingequation:
x
p‚àó
Œ± > x (17)
x q‚àó
x
Wechoosetosetalowerboundforthedenominatorq‚àóandanupperboundforthenumeratorp‚àó duetothecomputational
x x
challengesoffindingexactsolutionsfortheCMPEtask.
Toestimateanupperboundfortheoptimalvalue(p‚àó)oftheconstrainedoptimizationproblem
x
minf (yÀÜ)s.t.g (yÀÜ)‚â§0, (18)
x x
yÀÜ
webeginbyseekingalooseupperboundthroughsolvingtheunconstrainedtaskmax f (yÀÜ)byutilizingmini-bucket
y x
elimination(DechterandRish,2003).Subsequently,feasiblesolutionsaretrackedduringbatch-stylegradientdescentto
refinetheinitialupperbound(notethattheweightofanyfeasiblesolutionisanupperboundonp‚àó).Foreachiteration,
x
thefeasiblesolutionwiththeoptimalobjectivevalueforeachexampleisstoredandsubsequentlyutilized.
To derive a lower bound for q‚àó, which represents the optimal solution for the following constrained optimization
x
problem,
minf (yÀÜ)+g (yÀÜ) s.t. g (yÀÜ)>0, (19)
x x x
yÀÜ
we can employ the methodologies delineated in Rahman et al. (2021). These techniques provide a mechanism for
eitherupperboundingorlowerboundingtheCMPEtask,contingentonwhetheritisformulatedasamaximizationor
minimizationproblem,respectively.
To establish a lower bound for q‚àó, the constrained optimization task is initially transformed into an unconstrained
x
formulationviaLagrangeRelaxation.Thisresultsinthefollowingoptimizationproblem:
maxminf (yÀÜ)+(1‚àí¬µ)√óg (yÀÜ) (20)
x x
¬µ‚â•0 yÀÜ
Here,¬µdenotestheLagrangianmultiplier.Byaddressingthisdualoptimizationproblem,weenhancetheprecisionof
thelowerboundforq‚àó.Fortheinnerminimizationtask,themini-bucketeliminationmethodisemployed.Theouter
x
maximizationissolvedthroughtheutilizationofsub-gradientdescent.
B EXTENSIONS
AddingaPenaltyforConstraintViolations. Apenaltyoftheformmax{0,g (yÀÜ)}2canbeeasilyaddedtotheloss
x
functionasdescribedbythefollowingequation
(cid:26)
f (yÀÜ) if g (yÀÜ)‚â§0
L (yÀÜ)= x x
x Œ± (f (yÀÜ)+g (yÀÜ))+œÅmax{0,g (yÀÜ)}2 if g (yÀÜ)>0
x x x x x
whereœÅ‚â•0isahyperparameter.
C EXPERIMENTALSETUPANDDETAILS
C.A DatasetandModelDescription
Table ST4 provides a comprehensive overview of the characteristics of each binary dataset, including the number
of variables and functions present in each dataset. These datasets were specifically chosen to provide diverse and
representativeexamplesforevaluatingtheperformanceandscalabilityofouralgorithms.
We used the following two classes of Markov networks from the UAI competitions (Elidan and Globerson, 2010;
Gogate,2014,2016):Isingmodels(Grids)andImageSegmentationnetworks.Specifically,weusedtheGrids_17and
Grids_18networksandSegmentation_12,Segmentation_14andSegmentation_15networks.
14LearningtoSolvetheCMPETaskinPGMs APREPRINT
WelearnedMPEtractablecutsetnetworkswithoutlatentvariablesusingtheschemeofRahmanetal.(2014)onfive
high-dimensionaldatasets:DNA(VanHaarenandDavis,2012;Ucla-Starai,2023),NewsGroup(c20ng)(Lowdand
Davis,2010;Ucla-Starai,2023),WebKB1(cwebkb)(LowdandDavis,2010;Ucla-Starai,2023),AD(VanHaarenand
Davis,2012;Ucla-Starai,2023),andBBC(VanHaarenandDavis,2012;Ucla-Starai,2023).Thesedatasetsarewidely
usedintheprobabilisticcircuitsliterature(LowdandDavis,2010).NotethatCMPEisintractableonthesemodelseven
thoughMPEistractable.
TableST4:DatasetandModelDescriptions
Dataset NumberofVariables NumberofFunctions
TractableProbabilisticCircuits
AD 1556 1556
BBC 1058 1058
20NewsGroup 910 910
WebKB 839 839
DNA 180 180
HighTree-WidthMarkovNetworks
Grids17 400 1160
Grids18 400 1160
Segmentation12 229 851
Segmentation14 226 845
Segmentation15 232 863
C.B DataGeneration
RecallthattheCMPEproblemusestwoMarkovnetworksM andM ,andavalueofq.WeusedtheoriginalMarkov
1 2
networks(chosenfromtheUAIcompetitionsorlearnedfromdata)asM andgeneratedM byaddingavaluev,which
1 2
wasrandomlysampledfromanormaldistributionwithmean0andvariance0.1,toeachentryineachpotentialinM .
1
Weusedthefollowingstrategytogenerateq.Wegenerated100randomsamplesfromM ,sortedthemaccordingto
2
theirweightw.r.t.M ,andthenchosethe10th,30th,60th,and90thsampleasavalueforq.Atahighlevel,aswego
2
fromthe10thsampletothe90thsample,namelyasqincreases,theconstraint(weightw.r.t.M islessthanorequalto
2
q)becomeslessrestrictive.Inotherwords,asweincreaseq,thesetoffeasiblesolutionsincreases(orstaysthesame).
Foreachvalueofq,weuse60%ofthevariablesasevidencevariablesXandtheremainingasY.
ForeachCMPEproblem,wegenerated10000samples,usedthefirst9000samplesfortrainingandtheremaining1000
samplesfortesting.Forthesupervisedmethods,wegeneratedtheoptimalassignmenttoY usinganintegerlinear
programmingsolvercalledSCIP(Achterberg,2009).
Forourproposedscheme,whichwecallSS-CMPE,weusedapproachdescribedinSectionAtofindtheupperbound
ofp‚àó andthelowerboundofq‚àó
x x
NotethatCMPEisamuchhardertaskthanMPE.OurschemecanbeeasilyadaptedtoMPE,allwehavetodoisusef
toyieldasupervisedscheme.
C.C ArchitectureDesignandTrainingProcedure
Inourexperimentalevaluations,weemployedaMulti-LayerPerceptron(MLP)withaRectifiedLinearUnit(ReLU)
activationfunctionforallhiddenlayers.ThefinallayeroftheMLPutilizedasigmoidactivationfunction,asitwas
necessarytoobtainoutputswithintherangeof[0,1]forallourexperiments.Eachfullyconnectedneuralnetworkin
ourstudyconsistedofthreehiddenlayerswithrespectivesizesof[128,256,and512].Wemaintainedthisconsistent
architectureacrossalloursupervised(ZamzamandBaker,2019;NellikkathandChatzivasileiadis,2021)andself-
supervised (Park and Van Hentenryck, 2022; Donti et al., 2021) methods. It is important to highlight that in the
adversarialmodificationexperiments,theneuralnetworkpossessedanequalnumberofinputsandoutputs,specifically
setto28√ó28(sizeofanimageinMNIST).However,intheremainingtwoexperimentsconcerningprobabilistic
graphicalmodels,theinputsizewasthenumberofevidencevariables(|X|),whiletheoutputsizewas|Y|.
For PDL (Park and Van Hentenryck, 2022), the dual network had one hidden layer with 128 nodes. The number
of outputs of the dual network corresponds to the number of constraints in the optimization problem. It is worth
emphasizingthatourmethodisnotconstrainedtotheusageofMulti-LayerPerceptrons(MLPs)exclusively,andwe
15LearningtoSolvetheCMPETaskinPGMs APREPRINT
havetheflexibilitytoexplorevariousneuralnetworkarchitectures.Thisflexibilityallowsustoconsiderandutilize
alternativearchitecturesthatmaybettersuittherequirementsandobjectivesofotheroptimizationtasks.
Regardingthetrainingprocess,allmethodsunderwent300epochsusingtheAdamoptimizer(KingmaandBa,2014)
withalearningrateof1e‚àí3.WeemployedaLearningRateSchedulertodynamicallyadaptthelearningrateastheloss
reachesaplateau.ThetrainingandtestingprocessesforallmodelswereconductedonasingleNVIDIAA40GPU.
C.D Hyper-parameters
Thenumberofinstancesintheminibatchwassetto128foralltheexperiments.Wedecaythelearningrateinall
theexperimentsby0.9whenthelossbecomesaplateau.Giventheempiricalobservationsthatlearningratedecay
oftenleadstoearlyconvergenceinmostcasesanddoesnotyieldbeneficialresultsforthesupervisedbaselines,we
havemadethedecisionnottoapplylearningratedecaytothesemethods.Thischoiceisbasedontheunderstanding
thatthebaselinesperformoptimallywithoutthisparticularformoflearningrateadjustment.Fordetailedinformation
regardingthehyper-parametersutilizedinthebenchmarkingmethods,wereferreaderstothecorrespondingpapers
associatedwitheachmethod.Asstatedinthemaintext,theoptimalhyperparametersweredeterminedusingagrid
searchapproach.FortheSS-CMPEmethod,foreachdataset,thehyperparameterswereselectedfromthefollowing
availableoptions-
‚Ä¢ Œ≤ -{0.1,1.0,2.0,5.0,10.0,20.0}
‚Ä¢ œÅ-{0.01,0.1,1,10,100}
IntheoptimizationproblemofSS-CMPE,theparameterœÅisemployedtopenalizetheviolationofconstraints.The
methodology for this approach, denoted as SS-CMPE , is explained in detail in Section B. The corresponding
pen
experimentsarepresentedintablesST8throughST17.
C.E TheLossFunction:CompetingMethods
Weevaluatedeightdifferentlossfunctions,includingourmethodtotrainadeepneuralnetworktosolveourCMPEtasks.
Thelossfunctionsusedforsupervisedtrainingare1)Mean-Squared-Error(MSE),and2)Mean-Absolute-Error(MAE).
Bothoftheselosseswerethenextendedtoincorporatepenaltytermsassuggestedby(NellikkathandChatzivasileiadis,
2021).WedenotethemasSL+PenaltyandMAE+Penalty.Weevaluatedtheself-supervisedlossproposedby(Donti
etal.,2021)(SSL )andby(ParkandVanHentenryck,2022)(PDL).Finally,weextendourself-supervisedCMPE
pen
lossfunctiontoincorporatethepenaltytermmax{0,g (yÀÜ)}2(seesectionB).WedenoteitasSS-CMPE .
x pen
D EXAMININGTHEINFLUENCEOFq:EVALUATINGTHEPERFORMANCEOF
OURPROPOSEDMETHODFORCHALLENGINGPROBLEMINSTANCES
To determine the value of q, a total of 100 random samples were generated, and their weights were calculated.
Subsequently,thesamplesweresortedbasedontheirweightinascendingorder.Theweightvaluescorresponding
tothe10th,30th,60th,and90thsampleswerethenchosenasthevaluesforq.Foreachvalueofq,wecomparethe
averagegapandviolationsobtainedbyourmethod(SS-CMPEandSS-CMPE )againstsixothersupervisedand
pen
self-supervised methods. Tables ST8 through ST17 show the scores obtained by each of the eight methods along
withtheirstandarddeviationsonthegeneratedtestproblems.Thisstudyinvestigatestheperformanceofeachmethod
in finding near-optimal feasible solutions for difficult problems which is directly controlled by the percentile rank
ofq;problemswithaq valueinthe10thand30thpercentileareconsideredharderproblemstosolveasthesizeof
thefeasibleregionisconsiderablysmallerthanthesizeoftheinfeasibleregion.Asaresultallmethodshavehigher
violationsontheseproblemsthantheproblemswithaqvalueinthe60thand90thpercentile.
TableST5presentsasummaryoftheperformancesofSS-CMPEandothersupervisedmethodsbasedontheiraverage
gapandtheaveragenumberofviolationsonthetestdatafordifferentvaluesofq.Wecomputetheminimumgapand
violationsachievedamongthefoursupervisedmethodsMSE,SL+Penalty,MAE,andMAE+Penaltyandlabelthem
asthebest supervisedmethod.WechoosetheminimumgapandviolationsamongSS-CMPEandSS-CMPE
and label them under the unified term best SS-CMPE. We observe that best SS-CMPE consistently has
pen
significantly lower violations than the best supervised method in all the problem instances, and its gap is often
comparabletothegapachievedbythebestsupervisedmethod,winningwhencomparedtotheaveragegap.
Table ST6 presents a similar summary of the performances of SS-CMPE and other self-supervised methods. We
compute the minimum gap and violations achieved among the 2 self-supervised methods SSL and PDL and
pen
labelthemasthebest SSLmethod.Asbefore,wechoosetheminimumgapandviolationsamongSS-CMPEand
16LearningtoSolvetheCMPETaskinPGMs APREPRINT
SS-CMPE andlabelthemundertheunifiedtermbest SS-CMPE.Althoughself-supervisedmethodshavelarger
pen
gapscomparedtosupervisedmethodsbutlesserviolations,weobservethatSS-CMPEcontinuestoconsistentlyachieve
significantlylowerviolationsthanthebestperformingself-supervisedmethodinalltheprobleminstances,anditsgap
isoftencomparabletothegapachievedbythebestsupervisedmethod,winningwhencomparedtotheaveragegap.
Finally,intableST7,wepresentaquicksummaryoftheperformancesofourbestSS-CMPE methodvsallother
methods.Wechoosetheminimumgapandviolationsachievedamongthe6othersupervisedandself-supervised
methodsandtheminimumgapandviolationsamongSS-CMPEandSS-CMPE .Inallproblemsandqvalues,the
pen
bestSS-CMPEhassignificantlylowerviolationscomparedtoothermethodswhilehavingaverycompetitivegap.
D.A TheFeasible-OnlyOptimalityGaps:ComparingSelf-SupervisedApproaches
FromtheresultspresentedintablesST5throughST17,weobservethatself-supervisedapproachesproducemore
feasiblesolutionscomparedtosupervisedapproaches.Inthissection,wepresenttheresultsofacontrolledstudythat
showshoweachoftheself-supervisedapproachesperformintermsoffindingoptimalsolutionsinthefeasibleregion.
We selected a subset of problems from the test set on which all self-supervised methods, namely, SSL (Donti
pen
etal.,2021),PDL(ParkandVanHentenryck,2022)andourmethodSS-CMPEandSS-CMPE ,obtainedfeasible
pen
solutionsandthiswasdoneforeachpossiblevalueofq.WethencomputedtheirgapsandcomparethemviafigureSF4.
Amongthethreemethodsanalyzed,SS-CMPEandSS-CMPE consistentlyexhibitssuperiorperformanceacross
pen
themajorityofcases.Itsoptimalitygapsaresignificantlysmallercomparedtotheothertwomethods.Thisfinding
suggeststhatSS-CMPEismoreeffectiveinminimizingtheobjectivevalueandachievingsolutionsclosertooptimality
forthegivenexamples.
TableST5: Summary:bestSS-CMPEvsother supervisedmethodsincludingMSE,SL+Penalty,MAE,and
MAE+Penalty.Boldrepresentstheminimumgap,whileunderlinedmeanstheleastviolations
q 10 30 60 90
Models Gap/ best best best best best best best best
/Dataset ViolationsSS-CMPEsupervisedSS-CMPEsupervisedSS-CMPEsupervisedSS-CMPEsupervised
gap 0.057 0.054 0.051 0.052 0.053 0.051 0.050 0.051
Segment-12
violations 0.152 0.511 0.166 0.500 0.084 0.348 0.021 0.131
gap 0.050 0.049 0.047 0.049 0.048 0.048 0.051 0.051
Segment-14
violations 0.134 0.691 0.088 0.616 0.066 0.410 0.046 0.207
gap 0.051 0.051 0.050 0.051 0.052 0.051 0.051 0.052
Segment-15
violations 0.060 0.570 0.060 0.417 0.049 0.248 0.001 0.061
gap 0.072 0.054 0.069 0.057 0.066 0.067 0.063 0.058
Grids-17
violations 0.035 0.304 0.013 0.125 0.002 0.044 0.001 0.002
gap 0.064 0.056 0.067 0.060 0.060 0.065 0.065 0.064
Grids-18
violations 0.017 0.210 0.019 0.087 0.000 0.025 0.000 0.015
gap 0.138 0.135 0.138 0.136 0.137 0.136 0.139 0.137
DNA
violations 0.013 0.434 0.002 0.448 0.001 0.281 0.001 0.089
gap 0.043 0.044 0.045 0.046 0.044 0.046 0.044 0.044
20NewsGr.
violations 0.069 0.455 0.054 0.176 0.007 0.046 0.001 0.001
gap 0.059 0.054 0.058 0.054 0.056 0.054 0.053 0.054
WebKB
violations 0.074 0.471 0.029 0.378 0.001 0.174 0.001 0.018
gap 0.038 0.036 0.043 0.036 0.042 0.037 0.040 0.037
BBC
violations 0.074 0.657 0.067 0.557 0.056 0.384 0.002 0.151
gap 0.129 0.204 0.131 0.201 0.130 0.204 0.131 0.213
Ad
violations 0.017 0.085 0.004 0.041 0.000 0.021 0.000 0.005
gap 0.070 0.074 0.070 0.074 0.069 0.076 0.069 0.076
Average
violations 0.065 0.439 0.050 0.335 0.027 0.198 0.007 0.068
17LearningtoSolvetheCMPETaskinPGMs APREPRINT
TableST6: Summary:bestSS-CMPEvsother self-supervisedmethodsincludingSL ,andPDL.Bold
pen
representstheminimumgap,whileunderlinedmeanstheleastviolations
q 10 30 60 90
Models/ Gap/ best best best best best best best best
Dataset ViolationsSS-CMPE SSL SS-CMPE SSL SS-CMPE SSL SS-CMPE SSL
gap 0.057 0.054 0.051 0.052 0.053 0.051 0.050 0.051
Segment-12
violations 0.152 0.545 0.166 0.622 0.084 0.486 0.021 0.163
gap 0.050 0.058 0.047 0.050 0.048 0.050 0.051 0.053
Segment-14
violations 0.134 0.507 0.088 0.414 0.066 0.394 0.046 0.207
gap 0.051 0.051 0.050 0.053 0.052 0.051 0.051 0.054
Segment-15
violations 0.060 0.676 0.060 0.360 0.049 0.274 0.001 0.086
gap 0.072 0.086 0.069 0.079 0.066 0.093 0.063 0.087
Grids-17
violations 0.035 0.043 0.013 0.003 0.002 0.001 0.001 0.014
gap 0.064 0.105 0.067 0.074 0.060 0.093 0.065 0.118
Grids-18
violations 0.017 0.060 0.019 0.001 0.000 0.003 0.000 0.005
gap 0.138 0.140 0.138 0.141 0.137 0.139 0.139 0.143
DNA
violations 0.013 0.048 0.002 0.062 0.001 0.003 0.001 0.004
gap 0.043 0.043 0.045 0.046 0.044 0.045 0.044 0.046
20NewsGr
violations 0.069 0.278 0.054 0.129 0.007 0.024 0.001 0.001
gap 0.059 0.058 0.058 0.057 0.056 0.057 0.053 0.057
WebKB
violations 0.074 0.149 0.029 0.096 0.001 0.056 0.001 0.013
gap 0.038 0.041 0.043 0.042 0.042 0.038 0.040 0.039
BBC
violations 0.074 0.336 0.067 0.160 0.056 0.149 0.002 0.029
gap 0.129 0.135 0.131 0.140 0.130 0.142 0.131 0.134
Ad
violations 0.017 0.055 0.004 0.006 0.000 0.013 0.000 0.004
gap 0.070 0.077 0.070 0.073 0.069 0.076 0.069 0.078
Average
violations 0.065 0.270 0.050 0.185 0.027 0.140 0.007 0.053
Table ST7: Summary: best SS-CMPE has significantly lower violations compared to other methods on all the
problemsandoverallthechosenqvalues.Ithascomparablegaptotheothermethods.
q 10 30 60 90
Models Gap best best best best
others others others others
/Datasets /ViolationsSS-CMPE SS-CMPE SS-CMPE SS-CMPE
Gap 0.057 0.054 0.051 0.052 0.053 0.051 0.050 0.051
Segment-12
Violations 0.152 0.511 0.166 0.500 0.084 0.348 0.021 0.131
Gap 0.050 0.049 0.047 0.049 0.048 0.048 0.051 0.051
Segment-14
Violations 0.134 0.507 0.088 0.414 0.066 0.394 0.046 0.207
Gap 0.051 0.051 0.050 0.051 0.052 0.051 0.051 0.052
Segment-15
Violations 0.060 0.570 0.060 0.360 0.049 0.248 0.001 0.061
Gap 0.072 0.054 0.069 0.057 0.066 0.067 0.063 0.058
Grids-17
Violations 0.035 0.043 0.013 0.003 0.002 0.001 0.001 0.002
Gap 0.064 0.056 0.067 0.060 0.060 0.065 0.065 0.064
Grids-18
Violations 0.017 0.060 0.019 0.001 0.000 0.003 0.000 0.005
Gap 0.138 0.135 0.138 0.136 0.137 0.136 0.139 0.137
DNA
Violations 0.013 0.048 0.002 0.062 0.001 0.003 0.001 0.004
Gap 0.043 0.043 0.045 0.046 0.044 0.045 0.044 0.044
20NewsGr
Violations 0.069 0.278 0.054 0.129 0.007 0.024 0.001 0.001
Gap 0.059 0.054 0.058 0.054 0.056 0.054 0.053 0.054
WebKB
Violations 0.074 0.149 0.029 0.096 0.001 0.056 0.001 0.013
Gap 0.038 0.036 0.043 0.036 0.042 0.037 0.040 0.037
BBC
Violations 0.074 0.336 0.067 0.160 0.056 0.149 0.002 0.029
Gap 0.129 0.135 0.131 0.140 0.130 0.142 0.131 0.134
Ad
Violations 0.017 0.055 0.004 0.006 0.000 0.013 0.000 0.004
Gap 0.070 0.067 0.070 0.068 0.069 0.070 0.069 0.068
Average
Violations 0.065 0.256 0.050 0.173 0.027 0.124 0.007 0.046
18LearningtoSolvetheCMPETaskinPGMs APREPRINT
FigureSF4:Illustrationoftheoptimalitygapforself-supervisedmethods(onfeasibleexamplesonly)forallapproaches.
Lowerisbetter.
19LearningtoSolvetheCMPETaskinPGMs APREPRINT
TableST8:AveragegapandconstraintviolationsovertestsamplesformodelsappliedtotheSegmentation12Dataset
fordifferentqvalues.Theplotdisplaysthemeanvaluesoftheaveragegapandconstraintviolations,withstandard
deviationsdenotedby¬±
q 10 30 60 90
ILPObj 491.150 476.654 467.913 461.967
Gap 0.064¬±0.051 0.061¬±0.049 0.053¬±0.042 0.052¬±0.040
MAE
Violations 0.569¬±0.495 0.545¬±0.498 0.430¬±0.495 0.131¬±0.337
Gap 0.054¬±0.042 0.053¬±0.042 0.051¬±0.041 0.051¬±0.041
MSE
Violations 0.776¬±0.417 0.580¬±0.494 0.486¬±0.500 0.186¬±0.390
Gap 0.064¬±0.050 0.061¬±0.049 0.059¬±0.046 0.052¬±0.043
MAE+Penalty
Violations 0.511¬±0.500 0.500¬±0.500 0.348¬±0.476 0.140¬±0.347
Gap 0.054¬±0.042 0.052¬±0.041 0.051¬±0.040 0.051¬±0.042
SL+Penalty
Violations 0.651¬±0.477 0.505¬±0.500 0.486¬±0.500 0.186¬±0.390
Gap 0.054¬±0.043 0.052¬±0.040 0.051¬±0.041 0.051¬±0.040
SSL
pen Violations 0.790¬±0.407 0.622¬±0.485 0.486¬±0.500 0.186¬±0.390
Gap 0.063¬±0.050 0.052¬±0.041 0.052¬±0.041 0.051¬±0.041
PDL
Violations 0.545¬±0.498 0.622¬±0.485 0.517¬±0.500 0.163¬±0.369
Gap 0.057¬±0.044 0.051¬±0.040 0.053¬±0.043 0.050¬±0.041
SS-CMPE
Violations 0.503¬±0.293 0.346¬±0.485 0.257¬±0.437 0.104¬±0.305
Gap 0.058¬±0.043 0.052¬±0.040 0.053¬±0.043 0.051¬±0.041
SS-CMPE
pen Violations 0.152¬±0.359 0.166¬±0.372 0.084¬±0.277 0.021¬±0.143
TableST9:AveragegapandconstraintviolationsovertestsamplesformodelsappliedtotheSegmentation14Dataset
fordifferentqvalues.Theplotdisplaysthemeanvaluesoftheaveragegapandconstraintviolations,withstandard
deviationsdenotedby¬±.
q 10 30 60 90
ILPObj 493.647 482.837 476.145 470.485
Gap 0.067¬±0.051 0.062¬±0.048 0.062¬±0.047 0.051¬±0.040
MAE
Violations 0.691¬±0.462 0.623¬±0.485 0.435¬±0.496 0.271¬±0.444
Gap 0.051¬±0.039 0.049¬±0.038 0.048¬±0.037 0.053¬±0.041
MSE
Violations 0.810¬±0.392 0.818¬±0.386 0.606¬±0.489 0.252¬±0.434
Gap 0.065¬±0.051 0.061¬±0.048 0.060¬±0.046 0.054¬±0.043
MAE+Penalty
Violations 0.693¬±0.461 0.616¬±0.487 0.410¬±0.492 0.207¬±0.405
Gap 0.049¬±0.039 0.049¬±0.039 0.049¬±0.037 0.054¬±0.041
SL+Penalty
Violations 0.810¬±0.392 0.803¬±0.397 0.601¬±0.490 0.215¬±0.411
Gap 0.061¬±0.047 0.050¬±0.038 0.050¬±0.039 0.053¬±0.042
SSL
pen Violations 0.590¬±0.492 0.618¬±0.486 0.394¬±0.489 0.207¬±0.405
Gap 0.058¬±0.045 0.068¬±0.051 0.056¬±0.043 0.054¬±0.043
PDL
Violations 0.507¬±0.500 0.414¬±0.493 0.403¬±0.491 0.207¬±0.405
Gap 0.050¬±0.038 0.047¬±0.037 0.048¬±0.037 0.051¬±0.040
SS-CMPE
Violations 0.502¬±0.295 0.444¬±0.401 0.309¬±0.497 0.150¬±0.358
Gap 0.050¬±0.039 0.048¬±0.038 0.050¬±0.037 0.052¬±0.042
SS-CMPE
pen Violations 0.134¬±0.340 0.088¬±0.284 0.066¬±0.248 0.046¬±0.211
20LearningtoSolvetheCMPETaskinPGMs APREPRINT
TableST10:AveragegapandconstraintviolationsovertestsamplesformodelsappliedtotheSegmentation15Dataset
fordifferentqvalues.Theplotdisplaysthemeanvaluesoftheaveragegapandconstraintviolations,withstandard
deviationsdenotedby¬±.
q 10 30 60 90
ILPObj 531.436 520.647 516.797 514.276
Gap 0.053¬±0.043 0.054¬±0.043 0.053¬±0.042 0.052¬±0.041
MAE
Violations 0.570¬±0.495 0.417¬±0.493 0.248¬±0.432 0.061¬±0.239
Gap 0.051¬±0.038 0.052¬±0.041 0.052¬±0.040 0.053¬±0.041
MSE
Violations 0.833¬±0.373 0.715¬±0.452 0.450¬±0.498 0.076¬±0.265
Gap 0.056¬±0.043 0.054¬±0.042 0.053¬±0.041 0.053¬±0.042
MAE+Penalty
Violations 0.616¬±0.486 0.457¬±0.498 0.265¬±0.441 0.075¬±0.263
Gap 0.052¬±0.040 0.051¬±0.040 0.051¬±0.040 0.052¬±0.041
SL+Penalty
Violations 0.833¬±0.373 0.715¬±0.452 0.450¬±0.498 0.076¬±0.265
Gap 0.053¬±0.042 0.059¬±0.047 0.052¬±0.041 0.054¬±0.043
SSL
pen Violations 0.676¬±0.468 0.360¬±0.480 0.274¬±0.446 0.097¬±0.295
Gap 0.051¬±0.040 0.053¬±0.043 0.051¬±0.040 0.054¬±0.043
PDL
Violations 0.698¬±0.459 0.461¬±0.499 0.392¬±0.488 0.086¬±0.280
Gap 0.051¬±0.040 0.050¬±0.041 0.052¬±0.040 0.051¬±0.040
SS-CMPE
Violations 0.366¬±0.474 0.298¬±0.499 0.225¬±0.417 0.059¬±0.236
Gap 0.052¬±0.040 0.052¬±0.042 0.052¬±0.041 0.051¬±0.041
SS-CMPE
pen Violations 0.060¬±0.238 0.060¬±0.238 0.049¬±0.215 0.001¬±0.032
TableST11:AveragegapandconstraintviolationsovertestsamplesformodelsappliedtotheGrids17Datasetfor
different q values. The plot displays the mean values of the average gap and constraint violations, with standard
deviationsdenotedby¬±.
q 10 30 60 90
ILPObj 2892.585191 2884.506703 2877.311872 2878.147272
Gap 0.119¬±0.078 0.133¬±0.088 0.125¬±0.084 0.114¬±0.078
MAE
Violations 0.565¬±0.496 0.376¬±0.485 0.122¬±0.328 0.020¬±0.140
Gap 0.054¬±0.041 0.057¬±0.045 0.067¬±0.054 0.075¬±0.056
MSE
Violations 0.314¬±0.464 0.152¬±0.359 0.044¬±0.205 0.013¬±0.111
Gap 0.129¬±0.081 0.144¬±0.089 0.137¬±0.087 0.127¬±0.085
MAE+Penalty
Violations 0.534¬±0.499 0.376¬±0.485 0.142¬±0.350 0.019¬±0.137
Gap 0.054¬±0.041 0.059¬±0.045 0.069¬±0.054 0.058¬±0.044
SL+Penalty
Violations 0.304¬±0.460 0.125¬±0.331 0.044¬±0.205 0.002¬±0.045
Gap 0.104¬±0.060 0.079¬±0.056 0.093¬±0.059 0.087¬±0.058
SSL
pen Violations 0.149¬±0.357 0.013¬±0.113 0.004¬±0.067 0.026¬±0.159
Gap 0.086¬±0.056 0.087¬±0.058 0.109¬±0.063 0.112¬±0.062
PDL
Violations 0.043¬±0.202 0.003¬±0.055 0.001¬±0.022 0.014¬±0.118
Gap 0.072¬±0.051 0.071¬±0.052 0.066¬±0.048 0.063¬±0.049
SS-CMPE
Violations 0.146¬±0.353 0.032¬±0.176 0.024¬±0.152 0.001¬±0.022
Gap 0.073¬±0.052 0.069¬±0.051 0.073¬±0.052 0.066¬±0.050
SS-CMPE
pen Violations 0.035¬±0.185 0.013¬±0.113 0.002¬±0.039 0.001¬±0.022
21LearningtoSolvetheCMPETaskinPGMs APREPRINT
TableST12:AveragegapandconstraintviolationsovertestsamplesformodelsappliedtotheGrids18Datasetfor
different q values. The plot displays the mean values of the average gap and constraint violations, with standard
deviationsdenotedby¬±.
q 10 30 60 90
ILPObj 4185.600003 4166.310635 4158.737261 4167.11386
Gap 0.138¬±0.087 0.142¬±0.091 0.122¬±0.081 0.102¬±0.073
MAE
Violations 0.606¬±0.489 0.389¬±0.488 0.178¬±0.383 0.019¬±0.138
Gap 0.056¬±0.044 0.060¬±0.047 0.069¬±0.056 0.064¬±0.050
MSE
Violations 0.332¬±0.471 0.194¬±0.395 0.178¬±0.383 0.015¬±0.122
Gap 0.143¬±0.087 0.154¬±0.093 0.145¬±0.092 0.114¬±0.080
MAE+Penalty
Violations 0.551¬±0.498 0.364¬±0.481 0.172¬±0.378 0.021¬±0.145
Gap 0.061¬±0.047 0.064¬±0.049 0.065¬±0.050 0.133¬±0.082
SL+Penalty
Violations 0.210¬±0.407 0.087¬±0.282 0.025¬±0.158 0.025¬±0.158
Gap 0.115¬±0.065 0.074¬±0.055 0.093¬±0.060 0.123¬±0.065
SSL
pen Violations 0.060¬±0.238 0.182¬±0.386 0.013¬±0.115 0.005¬±0.074
Gap 0.105¬±0.063 0.126¬±0.067 0.101¬±0.062 0.118¬±0.064
PDL
Violations 0.097¬±0.295 0.001¬±0.032 0.003¬±0.050 0.005¬±0.071
Gap 0.064¬±0.047 0.072¬±0.053 0.060¬±0.046 0.065¬±0.049
SS-CMPE
Violations 0.200¬±0.400 0.029¬±0.169 0.000¬±0.000 0.001¬±0.032
Gap 0.067¬±0.051 0.067¬±0.051 0.078¬±0.055 0.075¬±0.053
SS-CMPE
pen Violations 0.017¬±0.129 0.019¬±0.137 0.002¬±0.039 0.000¬±0.000
TableST13:AveragegapandconstraintviolationsovertestsamplesformodelsappliedtotheADDatasetfordifferent
q values. The plot displays the mean values of the average gap and constraint violations, with standard deviations
denotedby¬±.
q 10 30 60 90
ILPObj 2535.424 2526.023 2521.957 2519.917
Gap 0.288¬±0.061 0.276¬±0.063 0.283¬±0.061 0.270¬±0.063
MAE
Violations 0.671¬±0.470 0.457¬±0.498 0.257¬±0.437 0.046¬±0.210
Gap 0.204¬±0.061 0.201¬±0.063 0.204¬±0.061 0.213¬±0.059
MSE
Violations 0.336¬±0.472 0.063¬±0.243 0.069¬±0.254 0.013¬±0.111
Gap 0.294¬±0.062 0.290¬±0.066 0.277¬±0.061 0.274¬±0.061
MAE+Penalty
Violations 0.600¬±0.490 0.468¬±0.499 0.271¬±0.444 0.039¬±0.194
Gap 0.216¬±0.061 0.213¬±0.063 0.220¬±0.061 0.229¬±0.060
SL+Penalty
Violations 0.085¬±0.278 0.041¬±0.197 0.021¬±0.142 0.005¬±0.074
Gap 0.135¬±0.055 0.140¬±0.057 0.142¬±0.054 0.134¬±0.055
SSL
pen Violations 0.244¬±0.430 0.143¬±0.350 0.054¬±0.226 0.005¬±0.074
Gap 0.148¬±0.056 0.152¬±0.056 0.146¬±0.055 0.139¬±0.054
PDL
Violations 0.055¬±0.228 0.006¬±0.080 0.013¬±0.113 0.004¬±0.063
Gap 0.135¬±0.055 0.131¬±0.057 0.131¬±0.055 0.131¬±0.054
SS-CMPE
Violations 0.102¬±0.302 0.025¬±0.155 0.005¬±0.071 0.003¬±0.055
Gap 0.129¬±0.054 0.136¬±0.057 0.130¬±0.054 0.133¬±0.054
SS-CMPE
pen Violations 0.017¬±0.127 0.004¬±0.063 0.000¬±0.000 0.000¬±0.000
22LearningtoSolvetheCMPETaskinPGMs APREPRINT
TableST14:AveragegapandconstraintviolationsovertestsamplesformodelsappliedtotheBBCDatasetfordifferent
q values. The plot displays the mean values of the average gap and constraint violations, with standard deviations
denotedby¬±.
q 10 30 60 90
ILPObj 890.289 880.270 875.668 872.118
Gap 0.053¬±0.037 0.046¬±0.034 0.043¬±0.032 0.045¬±0.034
MAE
Violations 0.779¬±0.415 0.624¬±0.485 0.414¬±0.493 0.165¬±0.371
Gap 0.036¬±0.027 0.036¬±0.028 0.037¬±0.028 0.037¬±0.029
MSE
Violations 0.924¬±0.265 0.854¬±0.354 0.578¬±0.494 0.204¬±0.403
Gap 0.047¬±0.036 0.044¬±0.034 0.041¬±0.031 0.040¬±0.031
MAE+Penalty
Violations 0.657¬±0.475 0.557¬±0.497 0.384¬±0.486 0.151¬±0.358
Gap 0.036¬±0.028 0.036¬±0.028 0.037¬±0.030 0.037¬±0.029
SL+Penalty
Violations 0.919¬±0.272 0.854¬±0.354 0.578¬±0.494 0.204¬±0.403
Gap 0.041¬±0.032 0.042¬±0.032 0.038¬±0.030 0.039¬±0.030
SSL
pen Violations 0.516¬±0.500 0.393¬±0.488 0.408¬±0.492 0.130¬±0.336
Gap 0.043¬±0.033 0.051¬±0.036 0.045¬±0.034 0.044¬±0.033
PDL
Violations 0.336¬±0.472 0.160¬±0.366 0.149¬±0.356 0.029¬±0.169
Gap 0.038¬±0.029 0.043¬±0.032 0.042¬±0.033 0.040¬±0.031
SS-CMPE
Violations 0.316¬±0.495 0.239¬±0.427 0.108¬±0.310 0.044¬±0.206
Gap 0.038¬±0.030 0.043¬±0.032 0.044¬±0.033 0.040¬±0.031
SS-CMPE
pen Violations 0.074¬±0.263 0.067¬±0.250 0.056¬±0.229 0.002¬±0.045
TableST15:Averagegapandconstraintviolationsovertestsamplesformodelsappliedtothe20NewsgroupDataset
fordifferentqvalues.Theplotdisplaysthemeanvaluesoftheaveragegapandconstraintviolations,withstandard
deviationsdenotedby¬±.
q 10 30 60 90
ILPObj 928.386 924.439 923.173 921.754
Gap 0.044¬±0.034 0.046¬±0.036 0.047¬±0.035 0.048¬±0.037
MAE
Violations 0.470¬±0.499 0.176¬±0.381 0.049¬±0.215 0.001¬±0.022
Gap 0.050¬±0.038 0.053¬±0.039 0.051¬±0.038 0.051¬±0.037
MSE
Violations 0.639¬±0.480 0.403¬±0.491 0.142¬±0.349 0.008¬±0.089
Gap 0.044¬±0.035 0.047¬±0.036 0.047¬±0.036 0.047¬±0.035
MAE+Penalty
Violations 0.455¬±0.498 0.181¬±0.386 0.046¬±0.210 0.001¬±0.022
Gap 0.046¬±0.036 0.047¬±0.035 0.046¬±0.036 0.044¬±0.034
SL+Penalty
Violations 0.573¬±0.495 0.384¬±0.486 0.161¬±0.367 0.015¬±0.122
Gap 0.045¬±0.036 0.046¬±0.036 0.045¬±0.035 0.046¬±0.035
SSL
pen Violations 0.386¬±0.487 0.139¬±0.346 0.024¬±0.152 0.002¬±0.039
Gap 0.043¬±0.035 0.046¬±0.036 0.046¬±0.036 0.046¬±0.036
PDL
Violations 0.278¬±0.448 0.129¬±0.335 0.028¬±0.165 0.001¬±0.032
Gap 0.043¬±0.034 0.045¬±0.035 0.044¬±0.034 0.044¬±0.034
SS-CMPE
Violations 0.317¬±0.465 0.086¬±0.280 0.019¬±0.137 0.001¬±0.032
Gap 0.044¬±0.033 0.045¬±0.035 0.046¬±0.035 0.045¬±0.034
SS-CMPE
pen Violations 0.069¬±0.254 0.054¬±0.227 0.007¬±0.083 0.001¬±0.022
23LearningtoSolvetheCMPETaskinPGMs APREPRINT
Table ST16: Average gap and constraint violations over test samples for models applied to the Webkb Dataset for
different q values. The plot displays the mean values of the average gap and constraint violations, with standard
deviationsdenotedby¬±.
q 10 30 60 90
ILPObj 828.463 824.361 825.517 823.917
Gap 0.057¬±0.043 0.057¬±0.043 0.058¬±0.043 0.062¬±0.044
MAE
Violations 0.613¬±0.487 0.502¬±0.500 0.249¬±0.433 0.046¬±0.210
Gap 0.065¬±0.046 0.069¬±0.046 0.066¬±0.045 0.065¬±0.045
MSE
Violations 0.695¬±0.461 0.473¬±0.499 0.210¬±0.407 0.018¬±0.133
Gap 0.058¬±0.042 0.058¬±0.042 0.059¬±0.043 0.061¬±0.044
MAE+Penalty
Violations 0.471¬±0.499 0.395¬±0.489 0.211¬±0.408 0.041¬±0.197
Gap 0.054¬±0.042 0.054¬±0.041 0.054¬±0.040 0.054¬±0.040
SL+Penalty
Violations 0.584¬±0.493 0.378¬±0.485 0.174¬±0.380 0.018¬±0.131
Gap 0.058¬±0.043 0.057¬±0.041 0.057¬±0.042 0.057¬±0.042
SSL
pen Violations 0.360¬±0.480 0.217¬±0.413 0.082¬±0.274 0.015¬±0.120
Gap 0.063¬±0.045 0.060¬±0.044 0.058¬±0.042 0.057¬±0.042
PDL
Violations 0.149¬±0.357 0.096¬±0.295 0.056¬±0.229 0.013¬±0.115
Gap 0.059¬±0.043 0.058¬±0.043 0.056¬±0.042 0.056¬±0.042
SS-CMPE
Violations 0.169¬±0.374 0.050¬±0.218 0.026¬±0.159 0.004¬±0.063
Gap 0.062¬±0.045 0.061¬±0.044 0.057¬±0.042 0.053¬±0.040
SS-CMPE
pen Violations 0.074¬±0.263 0.029¬±0.169 0.001¬±0.032 0.001¬±0.022
TableST17:AveragegapandconstraintviolationsovertestsamplesformodelsappliedtotheDNADatasetfordifferent
q values. The plot displays the mean values of the average gap and constraint violations, with standard deviations
denotedby¬±.
q 10 30 60 90
ILPObj 222.848 221.635 221.114 220.625
Gap 0.138¬±0.109 0.142¬±0.109 0.136¬±0.109 0.141¬±0.111
MAE
Violations 0.444¬±0.497 0.448¬±0.497 0.286¬±0.452 0.114¬±0.317
Gap 0.138¬±0.112 0.140¬±0.112 0.139¬±0.111 0.139¬±0.110
MSE
Violations 0.506¬±0.500 0.565¬±0.496 0.322¬±0.467 0.113¬±0.317
Gap 0.140¬±0.111 0.136¬±0.106 0.143¬±0.111 0.137¬±0.113
MAE+Penalty
Violations 0.444¬±0.497 0.448¬±0.497 0.286¬±0.452 0.114¬±0.317
Gap 0.135¬±0.109 0.140¬±0.112 0.141¬±0.111 0.143¬±0.115
SL+Penalty
Violations 0.434¬±0.496 0.494¬±0.500 0.281¬±0.450 0.089¬±0.285
Gap 0.140¬±0.115 0.141¬±0.111 0.146¬±0.116 0.143¬±0.118
SSL
pen Violations 0.048¬±0.214 0.062¬±0.241 0.014¬±0.118 0.004¬±0.067
Gap 0.140¬±0.113 0.141¬±0.113 0.139¬±0.112 0.144¬±0.120
PDL
Violations 0.287¬±0.452 0.129¬±0.335 0.003¬±0.055 0.006¬±0.077
Gap 0.138¬±0.113 0.138¬±0.108 0.137¬±0.106 0.139¬±0.109
SS-CMPE
Violations 0.046¬±0.210 0.017¬±0.129 0.012¬±0.109 0.008¬±0.089
Gap 0.139¬±0.116 0.139¬±0.113 0.140¬±0.112 0.139¬±0.113
SS-CMPE
pen Violations 0.013¬±0.115 0.002¬±0.045 0.001¬±0.022 0.001¬±0.022
24LearningtoSolvetheCMPETaskinPGMs APREPRINT
D.B OptimalityGapAndViolationsinSelf-SupervisedMethodsforDifferentqValues
(a)OptimalityGap(avg%)andAverage (b)Opt.Gap(avg%)andAvg.Violations (c)OptimalityGap(avg%)andAverage
ViolationsforGridsUAInetworks forSegmentationUAInetworks ViolationsforTractableModels
FigureSF5:VisualizationofOptimalityGap(average%)andAverageViolationsforSelf-SupervisedMethodsacross
differentqvalues.Pointsclosertotheoriginindicatebetterperformance.
InthescatterplotsdepictedinFigureSF5,threedistinctevaluationsoftheoptimalitygapagainsttheaverageviolations
for various self-supervised methods across different q values are visualized. Points positioned closer to the origin
indicatebetterperformance,withreducedoptimalitygapsandfewerviolations.InFigureSF5(a),focusedonGrids
UAInetworks,theSS-CMPE methodgenerallyoccupiesapositionneartheorigin,indicatingitscommendable
pen
performanceinthissetting.TheSS-CMPEmethodexhibitsacomparableperformancetotheSS-CMPE method,
pen
withoccasionalhighlevelsofviolationsobservedintwoinstances.
In Figure SF5(b), showcasing the Segmentation UAI networks, the SS-CMPE and SS-CMPE methods again
pen
demonstrate superiority, particularly evident by their prevalence near the origin. Finally, in Figure SF5(c) related
to tractable models, the SS-CMPE method often achieves optimal placement close to the origin, reflecting a
pen
balancedperformance.Theseevaluationsprovidecriticalinsightsintotheeffectivenessandrobustnessoftheproposed
self-supervisedmethodsacrossdifferentproblems.
25