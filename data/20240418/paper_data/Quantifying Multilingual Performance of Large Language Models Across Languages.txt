Quantifying Multilingual Performance of Large Language Models
Across Languages
ZihaoLi1∗,YuchengShi2,ZiruiLiu3,FanYang4,NinghaoLiu2,MengnanDu1
1NewJerseyInstituteofTechnology 2UniversityofGeorgia
3RiceUniversity 4WakeForestUniversity
lizihao9885@gmail.com,mengan.du@njit.edu
Abstract thisimbalancedtokendistributionwillcausebias
towards English (Blasi et al., 2021). As a result,
ThetrainingprocessofLargeLanguageMod-
theexcellentperformanceofLLMisoftenlimited
els (LLMs) requires extensive text corpus.
tosomecommonlanguages,suchasEnglish.
However, these data are often unevenly dis-
tributed in different languages. As a result,
ThisimbalanceddistributionmakestheLLMless
LLMs perform well on common languages,
capableofunderstandinglow-resourcelanguages.
suchasEnglish,German,andFrench,butper-
For example, LLM cannot understand the true
formpoorlyonlow-resourcelanguages. How-
ever, currently there is no work to quantita- meaning of some slang terms with specific cul-
tively measure the performance of LLMs in turalbackgrounds,suchasChineseidioms(Zhang
low-resource languages. To fill this gap, we etal.,2023). Moreover,recentresearchhasshown
proposed the Language Ranker that aims to thatthepre-trainedmodeloftenunderperformsin
benchmark and rank different languages ac-
languagewithinsufficienttrainingdata(Lankford
cordingtotheperformanceofLLMsonthose
etal.,2024). Abovephenomenaillustratetheim-
languages. WeemploytheLLM’sperformance
portance of training data for LLM. However, it
ontheEnglishcorpusasabaselinetocompare
the performances of different languages and isoftennotreleasedbyleadingcompaniesandit
English. Wehavethefollowingthreefindings: does not take the inner representations of LLMs
1.TheperformancerankingsofdifferentLLMs intoaccount. Therefore,itisnecessarytopropose
inalllanguagesareroughlythesame.2. LLMs ametrictomeasuredifferentlanguageproportions
withdifferentsizeshavethesamepartialorder
in the LLM’s pre-training corpus and further im-
ofperformance.3.Thereisastrongcorrelation
plicitlymeasurethelanguageabilityfordifferent
betweenLlaMa2’sperformanceindifferentlan-
languages,especiallylow-sourcelanguages.
guagesandtheproportionofthepre-training
corpus. ThesefindingsillustratethattheLan-
Inthispaper,weproposetoutilizeinternalrepre-
guage Ranker can be used as an indicator to
sentations to quantitatively measure the multilin-
measurethelanguageperformanceofLLMs.
gual abilities of LLMs. Specifically, we employ
therepresentationofLLMsontheEnglishcorpus
1 Introduction
as the baseline. Then, we measure the similarity
LargeLanguageModels(LLMs),suchasChatGPT betweentherepresentationsonthecorpusoflow-
and GPT-4, have demonstrated surprising perfor- resourcelanguagesandthoseofEnglish. Wetake
manceinvariousNLPtasks(Achiametal.,2023; thissimilarityvalueastheperformancescoreofthe
Ouyang et al., 2022; Touvron et al., 2023; Team modelineachlanguage. Inexperiments,wefound
et al., 2024; Jiang et al., 2023; Bai et al., 2023). thattherankingresultsobtainedbyourmethodare
However,themajorityofthetextdatasetsarepre- roughly the same as the ranking results of differ-
sentedinhigh-resourcelanguagessuchasEnglish entlanguageproportionsintheLLM’spre-training
(Xie et al., 2024). According to the statistics, for corpus. It shows that our proposed method can
GPT-3modelapproximately92.65%ofthetraining effectively measure the performance of LLMs in
tokensareEnglishandallotherlanguagessharethe differentlanguages.
remaining7.35%trainingtokens(OpenAI,2023).
Similarly,Englishaccountsfor89.70%ofdatafor 2 AnalysisMethod
pre-trainingLlaMa2(Touvronetal.,2023). Thus,
In this section, we will give an introduction to
∗WorkdoneduringZihaoLi’sremoteinternshipatNJIT. our analysis method. First, we will introduce the
4202
rpA
71
]LC.sc[
1v35511.4042:viXradatasetweusedinourexperiment. Then,wewill wherel ={5,10,15,20,25}isthesubsetofthe
sub
introducehowtoobtainthesimilaritybetweenEn- layersweselected. Finally,weuseSimtoevaluate
glishandotherlanguagesaswellashowtocom- the performance gap between English and Non-
paredifferentLLMs’performances. Englishcorpus.
2.1 ProbingDatasets 2.3 RankCorrelationMeasurement
We use OPUS-100 (Zhang et al., 2020) as our When we get the similarity between each non-
evaluation datasets. OPUS-100 is an English- English representation and the English represen-
centric multilingual corpus that covers 100 lan- tation, we sort them according to the similarity
guages. Each sample consists of text in a non- to get a sorted ranking list of all languages. To
Englishlanguageastheoriginaldata,withitsEn- measure the similarity of the sorted ranking lists
glish translation serving as the target data. For of two LLMs, we use the longest common par-
example, {"German": "Ich wollte dir erst noch tial order sublist to measure. It can be defined
etwaszeigen.","English": "Iwantedtoshowyou as follows: For two sorted lists A and B, find
somethingfirst."}. Afterfiltering,thereare94sub- a sublist C which is a subset of A and B such
sets containing English, including high-resource that for any number of index i ≤ i ≤ ... ≤ i ,
1 2 n
languages such as German, French, and Chinese, Index(C )≤Index(C )≤...≤Index(C )istruefor
i1 i2 in
as well as low-resource languages such as Oriya, bothAandB,andthelongestsublistCthatmakes
Kannada,andKazakh. Eachsubsetcontains2000 it true is called the longest common partial order
samples. sublistofAandB.Weusetheratioofthelength
ofthelongestcommonpartialordersublistoftwo
2.2 SimilarityMeasurement
LLMs to the total length of the ranking list as a
WeemploycosinesimilaritytomeasuretheLLMs’ metrictomeasurethecorrelation.
performancegapbetweenthetargetlanguageand
3 Experiments
English. Specifically, given two sentences X =
{x }n andY = {y }m representingthetextin
i i=1 i i=1 3.1 Open-sourceModels
Englishandthetextinthetargetlanguage. Weuse
Weusefourpopularopen-sourcelargemodelsas
therepresentationobtainedafterLLMmappingof
our analysis baselines: LlaMa2 (Touvron et al.,
the last token x and y as the representation of
n m
2023),Qwen(Baietal.,2023),Mistral-v0.1(Jiang
thetextandcalculatethesimilaritybetweenthem.
et al., 2023), and Gemma (Team et al., 2024). In
Asweknow, LLMconsistsofseverallayersofa
Section 3.2, we concentrate on the 7B version of
Transformerblock(Vaswanietal.,2017). There-
thesemodels. Theperformanceofmodelsofvari-
fore,aftereachlayerofmappingbythetransformer
oussizeswillbediscussedinSection3.3.
block, we can get a representation vector xl and
n
yl ,l = 1...H,whereH representsthenumberof
m 3.2 ComparisonofDifferentModels
thelayerofLLMs. Accordingto(Lietal.,2024),
To visualize the performance of different LLMs
theintermediaterepresentationcanbebrieflysum-
intheselanguages,weselected10representative
marizedbythefollowingequations:
languagestodisplaytheirinferenceresults. They
xl+1 =MLP(xl+MHA(xl)) l=1...H, (1) consistoffivehigh-resourcelanguages,including
German,Spanish,French,Indonesian,andChinese,
whereMHAmeansmulti-headattentionormulti-
and five low-resource languages, including Igbo,
group attention, and MLP means standard multi-
Kazakh,Kannada,Oriya,andTurkmen. Figure1
layer perceptron layer. Next, we take xl and yl
n m showsdetailedresults,wheretheX-axisrepresents
to calculate the similarity. To implement a more
different layers of LLMs, while the Y-axis repre-
robustsimiliaritymeasure,weusetheaveragesim-
sents the similarity between the target language
ilarity obtained by several intermediate layers as
andEnglishforeachlayer. FromFigure1,wecan
thefinalsimilarity. Thisprocesscanbedescribed
derivethefollowingkeyobservations:
asfollows:
(1) High-resource languages have representa-
|l |
Sim =
|l
s1
ub|
(cid:88) is =u 1b Sim i, whereSim
i
= ||xx
i
n|i n |y ||mi
y mi
||, t ri eo sn os urm ceor le ans gim uai gla er
s
t so hoE wng leli ss sh, simwh ile ar re ita ys
.
lo Aw l-
-
(2) thoughtheexactproportionofhigh-resourceandFigure1: PerformanceofdifferentLLMsfortenkindsoflanguage,German,Spanish,French,Indonesianand
Chinesearefivehigh-sourcelanguages;Igbo,Kazakh,Kannada,OriyaandTurkmenarefivelow-sourcelanguages.
ure that for each LLM, the ranking result is used
asthebaseline,andtheremainingthreeLLMsare
roughlysimilartothebaseline.
(3) Fine-tuning on specific languages will im-
proveitsperformance. FromtheresultofQwen,
wecanobservethattheperformanceoftheChinese
improvesasthenumberoflayersincreases. Inthe
lastfewlayers,itsurpassesotherhigh-resourcelan-
guages in the figure. According to the technical
report of Qwen (Bai et al., 2023), Qwen has ad-
ditionalfine-tuningontheChinesecorpus,which
leadstobetterperformanceinChinese.
Figure2: RankcorrelationbetweendifferentLLMs 3.3 ComparisonofLLMsofDifferentSizes
We also conducted analytical experiments on the
samemodelofdifferentsizes. Theresultisshown
low-resourcelanguagesineachLLM’spre-training
in Figure 3. We found that the results of low-
corpus is unknown, high-resource languages are
resource languages fluctuated greatly, so we de-
generallymoreprevalent,andtheresultsinthefig-
finedthelayerdepthasdividingtheinterval[0,1]
ure show that our similarity-based measurement
equallybythenumberoflayersandselectedaspe-
methodcaneffectivelymeasuretheproportionof
cific layer depth interval [0.4,0.6] to display the
eachlanguageintheLLM’spre-trainingcorpus.
resultsoflow-resourcelanguages. Wecanobserve
twophenomena:
(2) Different models display similar results
across languages. Their performance is that the (1)Thereisamodestpositivecorrelationbetween
high-resourcelanguagesimilarityishigherthanthe the size of an LLM and its performance on low-
low-resourcelanguagesimilarity. Figure2further resource languages. As shown in the figure, for
illustratesthisconclusion,wecanseefromthefig- Kannada,Occitan,andWesternFrisian,theperfor-Figure3: TheperformanceofQwen1.5(0.5B,1.8B,4B,and7B)invariouslanguages. German, French, and
Spanish are high-resource languages, Kannada, Occitan, and Western Frisian are low-resource languages. For
low-resourcelanguages,tomaketheresultsclearer,weselectedtheintermediatelayerrepresentation(layersdepth
0.4-0.6)resultsthatchangerelativelysmoothly.
manceofQwen1.5onthreesizesof0.5B,4B,and portionofcorpusdecreases,thesimilaritymetric
7Bgraduallyimprovesasthesizeincreases. alsodecreases. Itdoesnotholdforalllanguages,
becauseitisnotonlytheproportionthataffectsthe
(2)Forhigh-resourcelanguages,thereisastrong
performanceofLLMinacertainlanguagebutalso
negativecorrelationbetweenthesizeoftheLLMs
factorssuchasthegrammaticalsimilaritybetween
andtheperformanceofhigh-resourcelanguages.
thelanguageandEnglish.
In the figure 3, high-resource languages and
Language Proportion Similarity Language Proportion Similarity
low-resourcelanguagesshowcompletelyopposite German 0.17% 0.581 Polish 0.09% 0.534
French 0.16% 0.591 Vietnamese 0.08% 0.529
trends. Thepossiblereasonforthisphenomenon
Swedish 0.15% 0.531 Finnish 0.03% 0.516
isthatasthesizeoftheLLMincreases,thecom- Chinese 0.13% 0.446 Norwegian 0.03% 0.501
plexityofthehigh-resourcetrainingcorpusalsoin-
Table 1: The proportion of different languages in the
creases,leadingtointerferencefromlower-quality LlaMa2 pre-training corpus and the similarity metric
data. Onthecontrary,low-resourcelanguagecor- weproposed. TheEnglishlanguageratiois89.7%.
pusisrelativelyscarce,sothefeaturedistribution
isrelativelyuniform. Insuchcases,thesizeofthe
4 ConclusionsandFutureWork
modelispositivelycorrelatedwiththeperformance
inthelanguage.
Inthiswork,weproposeasimilarity-basedevalu-
ationmethodtomeasuretheLLMs’performance
3.4 RelationshiptoPre-trainingCorpus
in various languages quantitatively. The results
AccordingtothetechnicalreportofLlaMa2(Tou- show that this similarity metric has a clear corre-
vron et al., 2023), we get the proportion of the lationwiththeproportionofeachlanguageinthe
pre-training corpus of some languages. Table 1 pre-trainingcorpus,andcanroughlymeasurethe
showstherelationshipbetweentheproportionof performanceabilityofthemodelineachlanguage.
pre-trainingcorpusforsomelanguagesandthesim- Inthefuture,weplantodesignmoredetailedeval-
ilarity metric. We can observe that from French uation criteria to measure LLM’s capabilities in
to Swedish to Finnish to Norwegian, as the pro- eachlanguage.Limitations model. Advances in Neural Information Processing
Systems,36.
TheproposedLanguageRankerapproachprovides
ChaoqunLiu,WenxuanZhang,YiranZhao,AnhTuan
an initial quantitative way to analyze LLM per-
Luu, and Lidong Bing. 2024. Is translation all you
formance across languages. However, it has sev-
need? astudyonsolvingmultilingualtaskswithlarge
erallimitations. First,thesimilaritymetricbased languagemodels. arXivpreprintarXiv:2403.10258.
on English representations may not fully capture
OpenAI. 2023. Gpt-3 dataset statistics.
the nuances and complexities of each language’s https://github.com/openai/gpt-3/tree/
linguistic properties. Additionally, low-resource master/dataset_statistics.
languagesarelikelytoexhibitmorenoiseandvari-
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
ance in the similarity scores due to the smaller Carroll Wainwright, Pamela Mishkin, Chong Zhang,
datasetsizesusedforpre-trainingtheselanguages Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Traininglanguagemodelstofollowinstructions
in LLMs. Furthermore, the method does not ac-
withhumanfeedback. Advancesinneuralinformation
count for potential biases or skews that could be
processingsystems,35:27730–27744.
presentinthemultilingualevaluationdatasetsthem-
Anton Schäfer, Shauli Ravfogel, Thomas Hofmann,
selves. The existence of such biases can also in-
TiagoPimentel, andImanolSchlag.2024. Language
troducenoiseintheresultingrankingsoflanguage
imbalancecanboostcross-lingualgeneralisation. arXiv
abilitiesfordifferentLLMs. preprintarXiv:2404.07982.
LingfengShen,WeitingTan,SihaoChen,YunmoChen,
Jingyu Zhang, Haoran Xu, Boyuan Zheng, Philipp
References
Koehn,andDanielKhashabi.2024. Thelanguagebar-
rier:Dissectingsafetychallengesofllmsinmultilingual
JoshAchiam,StevenAdler,SandhiniAgarwal,Lama
contexts. arXivpreprintarXiv:2401.13136.
Ahmad,IlgeAkkaya,FlorenciaLeoniAleman,Diogo
Almeida,JankoAltenschmidt,SamAltman,Shyamal Gemma Team, Thomas Mesnard, Cassidy Hardin,
Anadkat, et al. 2023. Gpt-4 technical report. arXiv RobertDadashi,SuryaBhupatiraju,ShreyaPathak,Lau-
preprintarXiv:2303.08774.
rent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juli-
ette Love, et al. 2024. Gemma: Open models based
JinzeBai,ShuaiBai,YunfeiChu,ZeyuCui,KaiDang,
on gemini research and technology. arXiv preprint
Xiaodong Deng, Yang Fan, Wenbin Ge, Yu Han, Fei
arXiv:2403.08295.
Huang, et al. 2023. Qwen technical report. arXiv
preprintarXiv:2309.16609. HugoTouvron,LouisMartin,KevinStone,PeterAlbert,
AmjadAlmahairi,YasmineBabaei,NikolayBashlykov,
DamiánBlasi,AntoniosAnastasopoulos,andGraham
SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal.
Neubig. 2021. Systematic inequalities in language
2023. Llama2: Openfoundationandfine-tunedchat
technologyperformanceacrosstheworld’slanguages.
models. arXivpreprintarXiv:2307.09288.
arXivpreprintarXiv:2110.06733.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
AlexisConneau,KartikayKhandelwal,NamanGoyal,
Uszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
andIlliaPolosukhin.2017. Attentionisallyouneed.
Guzmán,EdouardGrave,MyleOtt,LukeZettlemoyer,
Advancesinneuralinformationprocessingsystems,30.
and Veselin Stoyanov. 2019. Unsupervised cross-
lingualrepresentationlearningatscale. arXivpreprint YangchenXie,XinyuanChen,HongjianZhan,Palaiah-
arXiv:1911.02116. nakoteShivakumara,BingYin,CongLiu,andYueLu.
2024. Weaklysupervisedscenetextgenerationforlow-
AlbertQJiang,AlexandreSablayrolles,ArthurMensch,
resourcelanguages. ExpertSystemswithApplications,
ChrisBamford,DevendraSinghChaplot,Diegodelas
237:121622.
Casas,FlorianBressand,GiannaLengyel,Guillaume
Lample,LucileSaulnier,etal.2023. Mistral7b. arXiv BiaoZhang,PhilipWilliams,IvanTitov,andRicoSen-
preprintarXiv:2310.06825. nrich.2020. Improvingmassivelymultilingualneural
machinetranslationandzero-shottranslation. InPro-
KatikapalliSubramanyamKalyan,AjitRajasekharan,
ceedingsofthe58thAnnualMeetingoftheAssociation
andSivanesanSangeetha.2021. Ammus: Asurveyof
forComputationalLinguistics,pages1628–1639,On-
transformer-basedpretrainedmodelsinnaturallanguage
line.AssociationforComputationalLinguistics.
processing. arXivpreprintarXiv:2108.05542.
XiangZhang,SenyuLi,BradleyHauer,NingShi,and
SeamusLankford,HaithemAfli,andAndyWay.2024.
Grzegorz Kondrak. 2023. Don’t trust chatgpt when
Transformers for low-resource languages: Is f\’eidir
yourquestionisnotinenglish: Astudyofmultilingual
linn! arXivpreprintarXiv:2403.01985.
abilitiesandtypesofllms. InProceedingsofthe2023
Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter ConferenceonEmpiricalMethodsinNaturalLanguage
Pfister,andMartinWattenberg.2024. Inference-time Processing,pages7915–7927.
intervention: ElicitingtruthfulanswersfromalanguageA Appendix Language SimilarityScore Language SimilarityScore
German 0.516 Polish 0.421
A.1 RelatedWork French 0.503 Portuguese 0.482
Swedish 0.435 Vietnamese 0.392
Multilingual Language Model. The imbalance Chinese 0.399 Ukrainian 0.437
distribution of training corpus in different lan- Spanish 0.499 Korean 0.261
Russian 0.494 Catalan 0.445
guagesleadstothebiasofLLMtowardssomecom-
Dutch 0.460 Serbian 0.408
monlanguagessuchasEnglish(Blasietal.,2021). Italian 0.466 Indonesian 0.421
Some approaches employ multilingual language Japanese 0.248 Czech 0.459
modelingtoalleviatethephenomenon(Shenetal.,
Table4: ThesimilarityscoreofMistral7B.
2024; Kalyan et al., 2021; Conneau et al., 2019).
Thesestudiesshowtheimportanceofstrengthen-
Language SimilarityScore Language SimilarityScore
ingthecross-lingualcapabilitiesofthepre-trained German 0.642 Polish 0.596
model. (Schäfer et al., 2024) found that the pres- French 0.634 Portuguese 0.625
Swedish 0.603 Vietnamese 0.584
ence of a primary language in the training pro-
Chinese 0.608 Ukrainian 0.597
cess of LLMs can improve the performance of Spanish 0.638 Korean 0.481
low-resourcelanguagesandleadtoamoreconsis- Russian 0.634 Catalan 0.601
Dutch 0.612 Serbian 0.588
tentrepresentationofLLMsindifferentlanguages.
Italian 0.615 Indonesian 0.597
(Liu et al., 2024) found that for English-centric Japanese 0.457 Czech 0.611
LLMs,althoughtranslationintoEnglishhelpsim-
Table5: ThesimilarityscoreofQwen7B.
provetheperformanceofNLPtasks, itisnotthe
bestchoiceforallsituations.
Figure 4 shows the result of five high-resource
A.2 RankingResultForLLMs
languages. Figure 5 shows the result of five low-
We give the similarity scores of the four LLMs
resource languages. From Figure 4, we observe
usedintheexperimenton18commonlanguages.
thattheperformanceof0.5Bisthebest,while7B
Resultsareshowninfollowingtables.
performs the worst. Figure 5 shows the opposite
result. It can also be found that the performance
Language SimilarityScore Language SimilarityScore
German 0.581 Polish 0.534 varianceinlow-resourcelanguagesismuchgreater
French 0.592 Portuguese 0.598 thantheperformanceinhigh-resourcelanguages.
Swedish 0.531 Vietnamese 0.529
Chinese 0.446 Ukrainian 0.551
Spanish 0.616 Korean 0.199
Russian 0.589 Catalan 0.582
Dutch 0.569 Serbian 0.555
Italian 0.567 Indonesian 0.577
Japanese 0.194 Czech 0.587
Table2: ThesimilarityscoreofLlaMa27B.
Language SimilarityScore Language SimilarityScore
German 0.571 Polish 0.487 Figure4: Resultforfivehigh-resourcelanguages.
French 0.546 Portuguese 0.535
Swedish 0.494 Vietnamese 0.456
Chinese 0.471 Ukrainian 0.484
Spanish 0.537 Korean 0.338
Russian 0.531 Catalan 0.492
Dutch 0.516 Serbian 0.472
Italian 0.522 Indonesian 0.499
Japanese 0.328 Czech 0.512
Table3: ThesimilarityscoreofGemma7B.
A.3 PerformanceComparisonofQwen1.5of
DifferentSizes Figure5: Resultforfivelow-resourcelanguages.
Thefollowingtwofiguresshowtheperformanceof
Qwen1.5indifferentsizesfordifferentlanguages.