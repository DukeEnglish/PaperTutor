SPATIAL CONTEXT-BASED SELF-SUPERVISED LEARNING FOR
HANDWRITTEN TEXT RECOGNITION
APREPRINT
CarlosPenarrubia CarlosGarrido-Munoz JorgeCalvo-Zaragoza JoseJ.Valero-Mas
UniversityofAlicante UniversityofAlicante UniversityofAlicante UniversityofAlicante
carlos.penarrubia@ua.es carlos.garrido@ua.es jcalvo@dlsi.ua.es jjvalero@dlsi.ua.es
April18,2024
ABSTRACT
HandwrittenTextRecognition(HTR)isarelevantproblemincomputervision,andimpliesunique
challengesowingtoitsinherentvariabilityandtherichcontextualizationrequiredforitsinterpretation.
DespitethesuccessofSelf-SupervisedLearning(SSL)incomputervision,itsapplicationtoHTR
hasbeenratherscattered,leavingkeySSLmethodologiesunexplored. Thisworkfocusesonone
ofthem,namelySpatialContext-basedSSL.Weinvestigatehowthisfamilyofapproachescanbe
adaptedandoptimizedforHTRandproposenewworkflowsthatleveragetheuniquefeaturesof
handwrittentext. Ourexperimentsdemonstratethatthemethodsconsideredleadtoadvancementsin
thestate-of-the-artofSSLforHTRinanumberofbenchmarkcases.
Keywords Self-SupervisedLearning·Handwrittentextrecognition·SpatialContext
1 Introduction
Handwrittentextrecognition(HTR)istheresearchareainthefieldofcomputervisionwhoseobjectiveistotranscribe
thetextualcontentofawrittenmanuscriptintoadigitalmachine-readableformat[73]. Thisfieldnotonlyplaysa
keyroleinthecurrentdigitaleraofhandwritingbyelectronicmeans(suchastablets)[11],butisalsoofparamount
relevance for the preservation, indexing and dissemination of historical manuscripts that exist solely in a physical
format[56].
HTRhasdevelopedconsiderablyoverthelastdecadeowingtotheemergenceofDeepLearning[57],whichhasgreatly
increaseditsperformance. However,inordertoattaincompetitiveresults,thesesolutionsusuallyrequirelargevolumes
of manually-labelled data, which is the principal bottleneck of this method. One means by which to alleviate this
problem,Self-SupervisedLearning(SSL),hasrecentlygainedconsiderableattentionfromtheresearchcommunity[61].
SSLemployswhatistermedasapretexttasktoleveragecollectionsofunlabelleddataforthetrainingofneuralmodels
inordertoobtaindescriptiveandintelligiblerepresentations[8],thusreducingtheneedforlargeamountsoflabelled
data[4].
Thepretexttaskscanbeframedindifferentcategoriesaccordingtotheirworkingprinciple[34,61],withthefollowing
being some of the main existing families: (i) image generation strategies [63, 46], which focus on recovering the
originaldistributionofthedatafromdefineddistortionsorcorruptions;(ii)contrastivelearningmethods[60,33],whose
objectiveistolearnrepresentativeanddiscerniblecodificationsofthedata,and(iii)spatialcontextmethods[27,58],
whichfocusoneitherestimatinggeometrictransformationsperformedonthedata[27]—i.e. spatialinformation—or
inferringthedispositionofasetofpatchesinwhichtheimageisdividedasapuzzle[21,58]—i.e.structuralinformation.
Nevertheless, thedevelopmentofSSLincomputervisionhaspredominantlyfocusedonimageclassification[45],
whileotherdomainssuchasHTRhavegarnerednoticeablylessattention[36]. Inthisrespect,whilethereareseveral
researcheffortsthatexploread-hocadaptationsfromstrategiesusedinclassificationtasks[3,81,50]ornewmethods
particularlydevisedforthesecases[49,66],thereisaremarkablescarcityofproposals,thushinderingthedevelopment
oftheHTRfield.
4202
rpA
71
]IA.sc[
1v58511.4042:viXraarXivTemplate APREPRINT
Rotation
Jigsaw puzzle
(a)Classicspatialcontext-basedmethods
Flip
Sorting
(b)HTRspatialcontext-basedmethods
Figure1: (a)ExistingspatialmethodsforSSLparticularlydesignedforimageclassificationcharacteristics. (b)We
propose two new approaches adapted tothe specific features ofHTR: (i) “flip” as ageometric transformation; (ii)
“sorting”asapuzzletransformation.
This work was motivated by the relevance of spatial information in HTR tasks [49], together with the conceptual
simplicityandcompetitiveperformanceofspatialcontext-basedSSLstrategies,andfocusesontheadaptationofa
referencesetofsuchapproachestotheHTRfield. Moreprecisely,weproposetwoalternativesthatenabletheuseof
theseSSLstrategiesinHTRscenarios(seeFig.1): (i)adaptingtheHTRtasktotheoriginal(orclassification-based)
SSLmethod,namelyinputadaptation,and(ii)adaptingtheparticularSSLmethodtothecharacteristicsofHTRtask,
namelymethodadaptation. TheseproposalsarecomprehensivelyevaluatedbyconsideringseveralreferenceHTR
corporaandarecomparedtothereferencestrategiesfollowedinthefield. Theresultsobtainedshowthattheproposed
SSLstrategiesnotonlyprovidebetterrecognitionratesthanexclusivelyrelyingonannotateddatabutalsooutperform
theexistingstate-of-the-artmethodsforSSLpre-trainingofHTR.
Theremainderoftheworkisstructuredasfollows:Section2contextualizesthecurrentworkwithintherelatedliterature
ofthefield,whileSection3introducesthestrategiesdevisedforspatialcontext-basedSSLforHTR;Section4presents
theexperimentalset-upconsidered,andSection5presentsandanalysestheresultsobtained. Finally,Section6provides
theconclusionfoundaftercarryingoutthiswork.
2 Relatedwork
2.1 HandwrittenTextRecognition
WhileHTRinitiallyreliedonHiddenMarkovModels[40,23,68,71],advancesinMachineLearning,andspecificallyin
DeepLearning,favouredtheuseofRecurrentNeuralNetworks(RNNs)owingtotheircapacitytomodelsequentialdata
[28,17,42,31]. Inparticular,theuseofbidirectionalLongShort-TermMemory(LSTMs)networkswithConnectionist
TemporalClassification(CTC)[28]hasbeenthestateoftheartinHTRcompetitionsfordecades[5,70,69,2]. Despite
thisprevalenceofCTC,attention-basedencoder-decoderapproaches[7]haverecentlygainedpopularityowingto
theircompetitiveresults[37,41,1,18]. Foradetailedreviewofthisfield,seetheworkofMichaeletal. [52],which
providesacomprehensivestudyofthevarioussequence-to-sequenceapproachesforHTR.
Ashasoccurredinmanyotherareas,therehasbeenagrowinginterestintheuseofmoreparallelizablearchitectures
suchastheTransformer[72]byadaptingtheworkof[22]totheHTRfield[10,64,67,26,20,65]. HTRhasbenefited
from this adaptation, either in isolation with an encoder-decoder [43, 54, 55, 74] or in combination with the CTC
objective function [75, 19, 9]. The work of Diaz et al. [20] explores universal architectures for text recognition,
concludingthataCNNbackbonewithaTransformerencoder,aCTC-baseddecoder,plusanexplicitlanguagemodel,
2arXivTemplate APREPRINT
isthemosteffectivestrategytodate. Regardlessofthisprogress,however,theneedforlargelabelledcorporaasa
pre-trainingstrategyinTransformer-basedmodelshasbecomenoticeable,[43,20,47,79,48]. ThishasledtheSSL
fieldtoattractmoreinterestintheHTRfieldowingtotheusualscarcityofannotateddataforparticulardomains.
2.2 Self-SupervisedTextRecognition
SSLiscurrentlyapromisingalternativeforthepre-trainingoftextrecognitionmodels,especiallywhenaddressing
scenariosinwhichthereisascarcityoflabelleddata[4]. Inthiscontext,thefirstworkthattacklesthisissueisSeqCLR
[3],whichadaptstheSimCLRcontrastivelearningmethod[13]tothesequentialnatureoftextrecognition. Please
notethat,whilethisworkistestedforbothHTRandSceneTextRecognition(STR),subsequentSSLproposalshave
beenspecificallydevisedinordertoaddressthenuancesofeachscenario. Wenowintroducesomeofthemostrelevant
approachesineachcase.
WithregardtoSTR,PerSEC[44]isacompetitiveexampleofacontrastiveframeworkbasedonextractinglow-level
strokeandhigh-levelsemanticcontextualspacessimultaneouslyviaahierarchicallearningapproach. SimAN[49],on
thecontrary,isframedwithinagenerativestrategybasedonrecoveringtheoriginalformatofanimagecropbymeans
ofanothernon-overlappingcropofthesamedatum. ThisworkhighlightsthecloserelationshipbetweenHTRand
spatialcontextinformation. OthernoticeablegenerativeapproachesrelyonMaskImageModellingforthepre-training
offullTransformerarchitectures,asoccursin[50,66,77].
InthecaseofHTR,thecurrentstateoftheartisdefinedbyChaCo[81],whichtakesadvantageofMomentumContrast
[29,15]topre-trainavisualfeatureextractorbyconsideringaCharacterUnitCroppingmodule,whichensuresthat
twocropsofthesameHTRimagehaveanoverlappingpart. Thesameauthorsalsoshowthatthepre-trainingofonly
thevisualfeatureextractorandnotthesequenceoneimprovesthestateoftheartinsemi-supervisedscenarios,andthis
isalsosupportedbytheworkofLuoetal.[49].
2.3 Spatialcontext-basedmethods
Imagedatainherentlyencodesspatialinformation,whichcanbeexploitedinordertopre-trainrecognitionmodels.
Accordingtothis,SSLapproachesbasedonthisprincipleconstituterathersimpleyeteffectivepretexttasks. However,
despitebeinglargelyexploredinclassificationframeworks,theyhaverarelybeenstudiedintextrecognition. Wenow
presentthetwoprincipalfamiliesofapproachesthatfollowthisprinciple,inadditiontohighlightingthemainworks
relatedtothem.
Intermsofgeometrictransformations,theworkofGidariset. al.[27]introducedtheconceptofpredictingrotation
alterationonimagedataasanSSLstrategy. Itssuccessintermsofbothperformanceandsimplicityledotherworksto
explorethisprincipleingreaterdepthinordertofurtherexploitthispremise[59,80,14,24,76].
Sincetherelativepositionamongpatchesofanimageencompassesrichspatialandcontextualinformation,thereare
severalpretexttasksthatexploitthispremise,namelyjigsawpuzzlestrategies. Someofthemostcommonapproaches
relyonpredictingtherelativepositionsoftwopatchesofanimage[21]orrecognisingtheorderofashuffledsequence
ofpatchesofanimage[58,12,62,53,78].
Spatialcontextisrarelyexploitedinimage-basedtextrecognitiontasks. Nevertheless,thecompetitiveresultsthatare
obtainedbyemployingthesestrategiesforclassificationtasks,togetherwiththestrongrelationshipbetweenspatial
informationandHTR—owingtostrokeandfontstyle—,suggestthat,ifadequatelyadapted,textrecognitionmodels
couldbenefitfrompre-trainingmethodsofthisnature. Notethatourworknotonlyseekstounderstandandunravelthe
existingspatialcontextSSLmethods,butalsopresentsinnovativemethodsthatspecificallyleverageandcapitalizeon
thisspatialcontext,thusprovidingnewperspectivesandadvancementswithinHTR.
3 Methodology
Thissectionintroducesthegeneralrecognitionframeworkconsidered,afterwhichtheproposedspatialcontext-based
SSLmethodsarepresentedanddescribed. Fig. 2depictsagraphicalexampleoftheproposedpretexttasksforHTR.
3.1 Encoder-decoderarchitecture
Inthispaper,astate-of-the-artencoder-decoderpipelineisadoptedfortextrecognition,asshowninFig. 3. Theencoder
isresponsibleforextractingthevisualandsequentialcharacteristicsofanimage,whilethedecoderisresponsiblefor
transformingtherepresentationintotheoutputtext.
3arXivTemplate APREPRINT
ROTATION
CNN Linear º º º º
FLIP
CNN Linear
JIGSAW
CNN MLP
SORTING
CTC /
CNN RNN Transformer Decoder
Figure2: Pre-trainingpipelineforthedifferentpretexttasksproposedinthework. Animageistransformedaccording
tothemethodandthesubsequentlayershandlethetask. NotethatRotation,FlipandtheJigsawpuzzlepredictasingle
classfromthesetspresented,whiletheSortingmethodpredictsasequence.
CNN RNN . . . . CTC /
Transformer Decoder
'MOVE'
Figure3: Encoder-decoderpipeline. TheencoderiscomposedofaCNNthatextractsthevisualfeaturesforagiven
image. After a map-to-sequence operation, an RNN extracts the sequential features in order to provide the final
representation. Thedecoder,whichmayinthiscasebeeitherConnectionistTemporalClassificationoraTransformer
Decoder,transformstherepresentationintotheoutputtext.
Encoder. GivenanimageI ∈Rc×H×W asinput,wherecistheinputchannels,H istheheightandW isthewidth,
thefirstgoesthroughaConvolutionalNeuralNetwork(CNN)cnn:Rc×H×W →Rc′×H′×W′,whichisresponsible
forextractingvisualfeatures,wherec′isthenumberoffeaturemapsofsizeH′×W′. Amap-to-sequenceoperation
m : Rc′×H′×W′ → RW′×(c′·H′) thenreshapesthefeaturesintoasequenceofframes. Finally,aRecurrentNeural
Network(RNN)rnn:RW′×(c′·H′) →RW′×T isresponsibleforextractingsequentialfeatures,whereT istheoutput
size that leads to the final representation S = [s ,s ,...,s ]. The encoder f(·) can, therefore, be expressed as
1 2 W′
f(·)=rnn(m(cnn(·)))
Decoder.GiventherepresentationS,inthisworktwodifferenttextdecodersareused:(i)aCTCdecoderthatseparately
decodeseachframeofS,and(ii)aTransformerDecoder(TD),whichisresponsibleforautoregressivelyproducingthe
textcharactersonthebasisoftherepresentationS andthepreviouslygeneratedoutput.
3.2 SSLpipeline
Thissectionpresentstheproposedadaptationofthemainexistingspatialcontext-basedpretexttaskstoHTRscenarios.
Notethat,foreachpretexttask,wepresenttwopossiblemeansofenablingtheuseofthesetechniquesinHTRscenarios:
(i)adaptingthedatatoaclassification-orientedframework(inputadaptation),and(ii)modifyingtheunderlyingnature
ofthepretexttaskinordertoallowitsusedinsequential-baseddata(methodadaptation).
4arXivTemplate APREPRINT
3.2.1 Geometrictransformations
Asstatedpreviously, oneofthemostrelevantstrategiesintermsofgeometrictransformationsisthepredictionof
rotationswhenappliedtoanimage[27]. Whileatransformationofthisnaturecanbeapplieddirectlyinclassification
scenarios, thetypologyoftheinputinHTRdatamakesitsdirectusecomplicated, since: i)textimagesarerarely
squared,signifyingthatmostrotationsalterthedimensionalityoftheaxes,andii)applyingarotationtosuchelements
drasticallychangesthesemanticmeaningoftheaxes(characteristicsandsequentialorder). We,therefore,nowpresent
thetwopossibleadaptationsofthistechniquetoHTRscenarios:
• Rotation(inputadaptation). GiventhatimagesinHTRrarelydepictthesamewidth,afixed-sizecrop—for
simplicity,weconsiderthatasquarecropofdimensionsH ×H—oftheinputdatamustbeextractedinorder
tobeabletousethispretexttask. Thispre-trainingprocesscanbeformallydescribedasfollows: givenan
imageI,asquaredcropismade,obtainingI ,afterwhicharotationisuniformlychosenfromtheset
square
{0º,90º,180º,270º},retrievingimageI . Finally,theCNNcnn(·)will,togetherwithalinearclassification
r
layerl(·),beresponsibleforsolvingthepretexttask.
• Flip(methodadaptation).Analternativetotheaforementionedcaseconsistsofapplyingafliptransformation
ratherthantherotationtransformation. TheadvantageofthisforHTRisthatitdoesnotmodifythesequential
dimensionofthedata. Moreover,theuniqueasymmetryofwriting,whichlackssymmetrybothverticallyand
horizontally,requiresamodeltohavetheabilitytounderstandlettershapes,carryoutrecognitionanddiscern
whetherafliphasbeenapplied. Formally,givenanimageI,acropismadeinordertogivetheimageI .
crop
Thiselementisthentransformedusingaflippingprocessthatisrandomlyselectedfromtheset{ϕ,Horizontal
(Hor.),Vertical(Vert.),Hor. andVert. (Both)}. Finally,theCNNcnn(·),togetherwithalinearclassification
layerl(·),predictsthetransformationthathasbeenapplied.
3.2.2 Puzzletransformation
Oneofthemostprominentmethodsinthisfamilyistheso-calledjigsawpuzzle,whichconsistsofrearrangingaset
ofjumbledpatchesfromthesameimagebypredictingtheindexofthecorrectpermutation[58]. Sincethisproposal
wasconceivedforclassificationtasksinwhichimagesdepictthesamedimensions,wenowpresentthealternatives
conceivedinordertoenableitsuseinHTRimageswithdifferentwidthdimensions.
• Jigsawpuzzle(inputadaptation). Theoriginalmethodismaintainedbyextractingasquarecropfromthe
HTRimage,andthisislaterprocessedbyemployingthejigsawpuzzlestrategy. Formally,foragivenimage
I,asquarecropofH ×H dimensionsisobtainedfromtheimage,obtainingI ;theimageisinitially
square
reshapedtoobtainI′ ∈ Rc×150×150 andthendividedinto4shuffledpatchesofdimensions75×75;
square
thesepatchesarefurthercroppedintopatchesofsize64×64,i.e.,P =[p ,p ,p ,p ],wherep ∈R1×64×64;
1 2 3 4 i
eachpatchissubsequentlyprocessedbytheCNNfeatureextractor,whoseresultsareconcatenatedandfed
intoamulti-layerperceptronthatestimatesthepermutationperformed.
• Sorting(methodadaptation). Theadaptationofthemethodproposedinthisworkconsistsofrearranging
afixed-sizenon-overlappingnumberofpatches—experimentalparameter—intowhichtheHTRimageis
divided. Formally, givenaninputimageI andanumberofpatchesM—inthiscase, M ∈ {2,3,4}—, a
listofpatchesP = [p ,...,p ],wherep ∈ R1×H×⌊W/M⌋,isobtained. P isthenrandomlyunsortedand
1 M i
rejoinedtoformtheimageI . Finally,theencoderf(·)predictsthecorrectorderofthepatchesusing
shuffled
anauxiliarydecoder.
4 Experimentalsetup
Thissectionpresentsthedatasetsandfiguresofmeritconsideredinordertovalidatetheproposalspresented,along
withtheinformationregardingtheneuralarchitecturesusedandtheirimplementationdetails.
4.1 Datasets
TheproposedmethodsareassessedusingthreereferencepublicdatasetsforHTR:theIAMHandwrittenDatabase[51],
theCVLdatabase(CVL)[39],andGeorgeWashington(G.W.)[25]. Tab. 1providesasummaryofthecharacteristicsof
thecorporaataword-level. Forthepre-training,theSSLmethodsdescribedaboveuseonlysamplesofthetraining
split. WeconsidertheAachensplit1fortheIAMsetandthatshowninFischeretal.[25]forWashington. SinceCVL
1https://www.openslr.org/56/
5arXivTemplate APREPRINT
doesnotcontainanofficialvalidationsplit,weuse10%ofthetrainingdataforvalidation. Notethat,inallcases,we
transformtheinputimagestograyscaleandnormalisethemtoH =64,maintainingtheaspectratio.
Table1: Overviewofthecorporausedinthiswork,depictingtheircharacteristics(numberofsamples,sizeofthe
alphabet,numberofwriters,timeperiod,andlanguage).
Images Alphabet Writers Period Language
IAM [51] 79275 78 657 Modern Eng
G.W.[25] 4894 82 2 Historic Eng
CVL [39] 99904 53 310 Modern Eng,Ger
Theperformanceofthemethodswasanalysedbyemployingtwowidelyusedtextrecognitionmetrics: theCharacter
ErrorRate(CER),definedasthenormalisedLevenshteindistancebetweenground-truthandprediction,andtheWord
Accuracy(WAcc).
4.2 Neuralarchitectures
Asmentionedpreviously,therecognitionframeworkisbasedonanencoder-decoderscheme. Detailsoftheprecise
neuralarchitectureusedarepresentedbelow.
• Encoder: WeconsideredaslightlymodifiedversionofResnet-34[30],inwhichweusedfewerchannelsin
intermediatelayersandreducedtheaggressivenessofthepoolingoperations,obtainingasimilarcapacityin
termsofparametersasin[16,6]. WeoptednottoemploytheResNet-29modelusedintheaforementioned
works,sincethisparticulararchitecturewasspecificallydesignedforSTRtasks—and,moreprecisely,for
addressingimagesofafixedsizeof32×100—inwhichtheaspectratiooftheimagesis,unlikethatwhich
occurswithHTR,notparticularlyrelevant.
• Recurrentfeatureextractor: ThesequentialnatureoftheHTRdatainquestionwasmodelledbyconsidering
twostackedbidirectionalLSTMs[32]withahiddensizeofT =256.
• Transformerdecoder: WefollowedthevanilladecoderimplementationusedinVaswanietal.[72],fixinga
singledecodinglayerwith8attentionheads.
Withregardtothetrainingdetails,amaximumnumberof1000epochswithapatienceof50onthevalidationloss
hasbeenconsideredforboththepre-trainingandthedownstreamstages. Notethat,inthelattercase,thispatience
is increased to 200 epochs when a reduced part is used. The weights of the model are optimized using the Adam
method[38],fixingthelearningratevaluesto3·10−4and10−4forthepre-trainingandfine-tuningphases,respectively.
Onlythetrainingdataisusedfortheself-supervisedpre-training. Dataaugmentationisalsocontemplatedinboth
stages,thedetailsofwhichareprovidedintheSupplementarymaterial.
5 Results
Theresultsofourexperimentsarepresentedbelow,andaredividedintotwomainsections. Inthefirst,weanalyzethe
qualityoftherepresentationslearnedforHTRbythedifferentmethods. Inthesecond,wecomparethesemethodsina
transferlearningscenario,consideringcaseswithbothlimitedandabundantdataforfine-tuning.
Inallcases,wecompareourresultswiththoseobtainedbyChaCo[81],giventhatthismethodhasalreadybeenshown
torepresentthestateoftheartwithrespecttootherproposalsdescribedinSection2.
5.1 Qualityevaluation
TheevaluationofSSLmethodsoftenhingesonthequalityoftherepresentationslearnedforthetask,withouttheaid
ofanyauxiliarytraining. Thisqualityiscritical,asitdirectlyrepresentstheabilityofthemodeltolearnbroadand
generalrepresentations. InordertoassessthisaspectinthecontextofHTR,wefreezeallrepresentationlayersafter
pre-trainingsoastothenfine-tunethemusingallthetrainingdata. Notethatthisfine-tuningprocessisrestrictedtothe
finallayerthatprojectstherepresentationsontothecharacterspace. Byfreezingtherepresentationlayers,weensure
thatthefiguresreportingrecognitionaccuracycanbeattributedtothequalityoftherepresentationsthemselves,rather
thanenhancementsinthearchitectureofthemodelorthefine-tuningprocessasawhole.
6arXivTemplate APREPRINT
Table2providesdetailsofthequalityassessment,averagingacrossallthedatasetsconsidered,2settingourmethods
againsttheChaCobenchmark. FocusingonourtwoprimarySSLblocks—GeometricandPuzzle—itwillbenotedthat
bothhavesomeconfigurationsthatoutperformthoseofChaCo,thusdemonstratingtheirutilityforHTRtasks. With
regardtothebest-performingoptions,theproposedgeometricapproach“Flip”combinedwithTDemergesasthemost
promisingalternative,achievingaCERandaWAccof17.3%and67.8%,respectively,andclearlyoutperformingthe
currentstateoftheartofChaCobyanoticeablemargin(31%ofCERand53%ofWAcc).
Table2: ResultsobtainedintherepresentationqualityevaluationintermsoftheCERandWAccfiguresofmerit(%)
forboththeproposalspresentedandthereferenceChaComethod[81]. Theupanddownarrowsrespectivelydenote
whetherthemetricispositivelyornegativelyvalued. Thefiguresrepresenttheaverageoftheindividualperformances
ofthedatasetscontemplated,withthebestvaluesperdecodingschemeandmetricbeinghighlightedinboldtype.
CTC TD
CER(↓) WAcc(↑) CER(↓) WAcc(↑)
Geometric
Rotation 27.4 45.3 17.8 67.8
Flip 24.4 52.4 17.3 66.9
Puzzle
Jigsaw 37.3 39.9 35.7 50.3
Sorting 54.7 24.9 57.8 27.6
ChaCo[81] 44.5 33.3 31.1 53.9
Oftheresultsreported,itisworthnotingarathercounterintuitivephenomenon: theonlymethodthatleveragesthe
sequentialnatureofthewholewordimage(“Sorting”)leadstonoticeablypoorerrepresentations. Thisoutcomeis
surprising,becauseonewouldexpectthispretexttasktoprovideacomprehensiveviewoftheinput,closelymirroring
whatthenetworkexperiencesduringtheactualHTRtask. Incontrast,partialimages(crops)yieldmoreinformative
representationsforHTR,atleastfromthepointofviewofpurepre-training. Thisreinforcessomeoftheconclusions
drawninliterature,whichsuggestthatfocusingonlocalpatternssuchassinglecharactersorstrokesfavoursthelearning
process[49,81]
Concentratingonmoregeneraltrends,notethattheTDsettingsystematicallymakesbetteruseoftherepresentations
learned through SSL. This trend is evident, as all the TD settings outperform their counterparts that employ CTC.
Therepresentationslearnedwould,therefore,appeartoreflectanintrinsiccompatibilitywiththeTDapproachforthe
decodingofcharactersequences. Furthermore,theorderofmethodsaccordingtotheirperformanceremainsconsistent
regardlessofthedecoderutilized,whetherCTCorTD.Thisconsistencysuggeststhatthequalityoftherepresentations
isinherentlysoundacrossdifferentdecoders,avaluabletraitforthedevelopmentofrobustHTRsystems.
5.2 Transferlearning
Wenowexploreatransferlearningscenarioindepthinordertoevaluatetheadaptabilityandefficiencyofthepre-trained
SSLmethods. Theessenceofthisexperimentistoobservehowwellthesemethodscanleverageacertainamountof
labelleddata—5%,10%,or100%ofthedataset—soastofine-tunetheentirenetworkstartingfromtherepresentations
learned. Thisfine-tuningiscomprehensive,involvingallthelayers,whichcontrastswiththepreviousexperimentin
whichonlythefinallayerwasfine-tuned. Weaimtodiscerntheabilityofthesemethodstoobtainusefulrepresentations
inHTRasadownstreamtaskand,ultimately,toachievethehighestpossiblerecognitionaccuracy.
Table 3 shows the results of this experiment, providing the average WAcc across all the datasets and the average
relativeimprovementswhencomparedtothebaselinewhichis,inthiscontext,definedastheperformancewithoutany
pre-training. Forthesakeofcompactness,weprovideonlythebestdecoder(CTCorTD)foreachmethod.3 These
metricsprovideaholisticviewoftheperformanceofeachmethod,thusallowingustocomparenotonlytheabsolute
accuracyachievedbutalsotheextenttowhichpre-trainingcontributestoperformancegainsoverthebaseline.
In the most data-constrained scenario utilizing merely 5% of the labelled data for fine-tuning, our proposed SSL
methodsnotonlyclearlysurpassthebaseline,asonemightexpect,butalsoexhibitsaslightbutconsistentadvantage
overthestate-of-the-artChaComethod. Theonlyexceptionis“Sorting”,whichperformsonparwiththebaseline.
2ThedetailedresultsforeachdatasetcanbefoundintheSupplementarymaterial.
3ThedetailedresultsforeachdatasetanddecodercanbefoundintheSupplementarymaterial.
7arXivTemplate APREPRINT
Table3:AverageresultsintermsoftheWAcc(%)metricacrossalldatasetsforthetransferlearningscheme,considering
differentpercentagesoflabelleddata. Boththecaseinwhichnopre-trainingstrategyisappliedandthereference
ChaComethod[81]areincludedforthepurposeofcomparison. Valuesshowninboldtypedenotethebestresults
obtainedperpercentageoflabelleddata,whereasunderlinedfigureshighlighttheconfigurationsthatoutperformthe
ChaCoreferencecase. TheaveragerelativeimprovementwhencomparedtotheNopretrainscenarioispresentedin
parenthesesforeachcase.
Percentageoflabeleddata
5% 10% 100%
Baseline
Nopretrain 15.8 24.1 72.9
ChaCo[81] 24.8(↑68.3) 32.9(↑46.1) 77.2(↑6.5)
Proposals
Rotation 24.0(↑63.7) 37.7(↑78.6) 79.5(↑9.4)
Flip 26.0(↑75.3) 39.4(↑90.0) 79.5(↑9.6)
Puzzle 26.5(↑103.0) 36.6(↑71.9) 78.7(↑8.4)
Sorting 15.6(↑5.4) 25.3(↑12.6) 78.3(↑7.9)
Withamodestincreaseto10%forfine-tuning,theSSLmethodscontinuetoshowmarkedimprovementsoverthe
baseline,withevengreaterrelativegains. Atthislevel,theproposedmethods,saveagain“Sorting”,begintodistance
themselvesfurtherfromChaCointermsofimprovement(around5%ofWAcc).
Whenallavailabledataisemployedforfine-tuning,allcompetingapproaches(includingthebaseline)surpass70%of
WAcc. Despitethiscompetitiveness,theSSLmethodologiesproposedinthisworkarestillthosethatperformthebest,
consistentlyoutperformingboththebaselineandChaCo. Thisnewlevelofperformanceisalsoattainedby“Sorting”,
unlikethatwhichoccurredinthepreviousscenarios,althoughstillbelowourotherproposals.
Althoughmanyofourproposalsreflectrobustbehaviourthatimprovesonthestateoftheartinmostcases,the“Flip”
methodispositionedasthebest-performingalternativeingeneralterms. Thismethodnotonlyoutperformsothersin
thissemi-supervisedscenariobutwasalsosuperiorintheprevioussection,whichfocusedsolelyonthequalityofthe
representationslearned. Notably,“Flip”istailoredspecificallytoHTR,thusreinforcingthevalueofdevelopingand
implementingstrategiesthataredirectlyalignedwiththeuniquechallengesandcharacteristicsofthetaskinhand.
5.3 Consistency
Akeyobjectiveofourstudyistoprovidepractitionerswithspecificguidelineswithwhichtotrainmoreconsistent
HTRmodelsthroughtheuseofSSL.Inordertoillustratetheeffectivenessoftheproposedmethods,Fig.4providesa
compellingvisualrepresentationthatshowsthepercentageofinstancesinwhichanyofourmethodsoutperformthe
state-of-the-artChaCoforanydatascenariosandarchitecturalframeworks(TDorCTC).Thesebarsindicatethat,with
theexceptionofthe“Sorting”approach,allourmethodsconsistentlyperformbetterthanthatofChaCointhemajority
ofcases. Themethodologiesintroducedinthisworkcould,therefore,beadoptedasbestpractices,providingagreater
probabilityofdevelopingrobustmodelsforspecificscenariosand/orarchitecturesinthecontextofHTR.The“Flip”
methodwithcroppedpatchesspecificallystandsout,makingitthego-tostrategyforthoseseekingtoachievethemost
robustperformance.
6 Conclusions
Inthispaper,westudyspatialcontext-basedSSLmethodsoriginallydevisedforclassificationtasksandadaptthem
totheHTRfield. Weproposetwopossibleadaptationframeworksforthispurpose: (i)adjustingthedatainhandin
ordertousetheoriginalSSLmethods(inputadaptation),and(ii)adaptingtheseSSLapproachestothesequential
natureofHTR(methodadaptation). TheresultsobtainedwhenconsideringthreebenchmarkHTRdatasets—IAM,
CVLandGeorgeWashington—showthattheseproposalsnotonlyimprovetherecognitionratesofthemodelwhen
notconsideringanypre-training,butalsoconsistentlyoutperformthecurrentstate-of-the-artSSLmethodsforHTR.
Moreover,itshouldbenotedthatoneofourproposalsthatwasspecificallydesignedforHTR(flip)consistentlyobtains
theoverallbestresultsineveryexperimentalscenario. Notealsothatourresultsarealignedwiththoseofprevious
SSLliteratureontextrecognition[49],whicharguesthatthereisacloserelationshipbetweenHTRandspatialcontext
owingtostrokeandfontstyleinformation.
8arXivTemplate APREPRINT
100
80
60
40
20
0
Rotation Flip Jigsaw Sorting
Figure4:Comparisonofoccurrencesoftheproposedmethodsinthesemi-supervisedscenarioinwhichtheyoutperform
thestate-of-the-artChaComethod[81]foranydatacollectionandarchitecturalframework(TDorCTC).Thefigures
areprovidedasaratiowithrespecttothetotalnumberofexperimentalscenariosconsidered. Thedashedlineindicates
thechanceofourmethodstosurpasstheChaCostrategy.
TheobjectiveofthisstudyistoprovidepracticalguidelinesinordertotakeadvantageofSSLtechniquesinthecontext
ofHTR,thuscombatingtheproblemofinsufficientdatainthisfield.
References
[1] Abdallah,A.,Hamada,M.A.,Nurseitov,D.: Attention-basedfullygatedcnn-bgruforrussianhandwrittentext.
JournalofImaging(2020)
[2] Abed,H.E.,Märgner,V.,Blumenstein,M.: InternationalConferenceonFrontiersinHandwritingRecognition
(ICFHR2010)-CompetitionsOverview(2010)
[3] Aberdam,A.,Litman,R.,Tsiper,S.,Anschel,O.,Slossberg,R.,Mazor,S.,Manmatha,R.,Perona,P.: Sequence-
to-sequencecontrastivelearningfortextrecognition.In: ProceedingsoftheIEEE/CVFConferenceonComputer
VisionandPatternRecognition.pp.15302–15312(2021)
[4] Alzubaidi,L.,Bai,J.,Al-Sabaawi,A.,Santamaría,J.,Albahri,A.,Al-dabbagh,B.S.N.,Fadhel,M.A.,Manoufali,
M.,Zhang,J.,Al-Timemy,A.H.,etal.: Asurveyondeeplearningtoolsdealingwithdatascarcity: definitions,
challenges,solutions,tips,andapplications.JournalofBigData10(1), 46(2023)
[5] Aradillas,J.C.,Murillo-Fuentes,J.J.,Olmos,P.M.: Boostingofflinehandwrittentextrecognitioninhistorical
documentswithfewlabeledlines.IEEEAccess(2021)
[6] Baek,J.,Kim,G.,Lee,J.,Park,S.,Han,D.,Yun,S.,Oh,S.J.,Lee,H.: Whatiswrongwithscenetextrecognition
modelcomparisons? datasetandmodelanalysis.In: ProceedingsoftheIEEE/CVFinternationalconferenceon
computervision.pp.4715–4723(2019)
[7] Bahdanau,D.,Cho,K.,Bengio,Y.: Neuralmachinetranslationbyjointlylearningtoalignandtranslate(2015)
[8] Balestriero,R.,Ibrahim,M.,Sobal,V.,Morcos,A.,Shekhar,S.,Goldstein,T.,Bordes,F.,Bardes,A.,Mialon,G.,
Tian,Y.,etal.: Acookbookofself-supervisedlearning.arXivpreprintarXiv:2304.12210(2023)
[9] Barrere,K.,Soullard,Y.,Lemaitre,A.,Coüasnon,B.: Alighttransformer-basedarchitectureforhandwrittentext
recognition(2022)
[10] Barrere,K.,Soullard,Y.,Lemaitre,A.,Coüasnon,B.:Trainingtransformerarchitecturesonfewannotateddata:an
applicationtohistoricalhandwrittentextrecognition.InternationalJournalonDocumentAnalysisandRecognition
(IJDAR)(2024)
[11] Bezerra,B.L.D.,Zanchettin,C.,Toselli,A.H.,Pirlo,G.: Handwriting: recognition,developmentandanalysis.
NovaSciencePublishers,Inc.(2017)
9arXivTemplate APREPRINT
[12] Carlucci,F.M.,D’Innocente,A.,Bucci,S.,Caputo,B.,Tommasi,T.: Domaingeneralizationbysolvingjigsaw
puzzles.In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.2229–
2238(2019)
[13] Chen,T.,Kornblith,S.,Norouzi,M.,Hinton,G.: Asimpleframeworkforcontrastivelearningofvisualrepresen-
tations.In: Internationalconferenceonmachinelearning.pp.1597–1607.PMLR(2020)
[14] Chen, T., Zhai, X., Ritter, M., Lucic, M., Houlsby, N.: Self-supervised gans via auxiliary rotation loss. In:
ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.pp.12154–12163(2019)
[15] Chen,X.,Fan,H.,Girshick,R.,He,K.: Improvedbaselineswithmomentumcontrastivelearning.arXivpreprint
arXiv:2003.04297(2020)
[16] Cheng,Z.,Bai,F.,Xu,Y.,Zheng,G.,Pu,S.,Zhou,S.: Focusingattention: Towardsaccuratetextrecognitionin
naturalimages.In: ProceedingsoftheIEEEinternationalconferenceoncomputervision.pp.5076–5084(2017)
[17] Chung,J.,Gulcehre,C.,Cho,K.,Bengio,Y.: Empiricalevaluationofgatedrecurrentneuralnetworksonsequence
modeling.arXiv: NeuralandEvolutionaryComputing(2014)
[18] Coquenet, D., Chatelain, C., Paquet, T.: End-to-end handwritten paragraph text recognition using a vertical
attentionnetwork(2022)
[19] Coquenet,D.,Chatelain,C.,Paquet,T.: Dan: asegmentation-freedocumentattentionnetworkforhandwritten
documentrecognition(2023)
[20] Diaz,D.H.,Ingle,R.,Qin,S.,Bissacco,A.,Fujii,Y.: Rethinkingtextlinerecognitionmodels.arXiv(2021)
[21] Doersch, C., Gupta, A., Efros, A.A.: Unsupervised visual representation learning by context prediction. In:
ProceedingsoftheIEEEinternationalconferenceoncomputervision.pp.1422–1430(2015)
[22] Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,T.,Dehghani,M.,Minderer,
M.,Heigold,G.,Gelly,S.,Uszkoreit,J.,Houlsby,N.: Animageisworth16x16words: Transformersforimage
recognitionatscale.ArXivabs/2010.11929(2020)
[23] Dreuw,P.,Doetsch,P.,Plahl,C.,Ney,H.:Hierarchicalhybridmlp/hmmorrathermlpfeaturesforadiscriminatively
trainedgaussianhmm:Acomparisonforofflinehandwritingrecognition.201118thIEEEInternationalConference
onImageProcessing(2011)
[24] Feng,Z.,Xu,C.,Tao,D.: Self-supervisedrepresentationlearningbyrotationfeaturedecoupling.In: Proceedings
oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.10364–10374(2019)
[25] Fischer,A.,Keller,A.,Frinken,V.,Bunke,H.: Lexicon-freehandwrittenwordspottingusingcharacterhmms.
Patternrecognitionletters33(7),934–942(2012)
[26] Fujitake,M.: Dtrocr: Decoder-onlytransformerforopticalcharacterrecognition.arXiv.org(2023)
[27] Gidaris,S.,Singh,P.,Komodakis,N.: Unsupervisedrepresentationlearningbypredictingimagerotations.arXiv
preprintarXiv:1803.07728(2018)
[28] Graves, A., Fernández, S., Gomez, F.J., Schmidhuber, J.: Connectionist temporal classification: labelling
unsegmentedsequencedatawithrecurrentneuralnetworks.ICML(2006)
[29] He,K.,Fan,H.,Wu,Y.,Xie,S.,Girshick,R.: Momentumcontrastforunsupervisedvisualrepresentationlearning.
In: ProceedingsoftheIEEE/CVFconferenceoncomputervisionandpatternrecognition.pp.9729–9738(2020)
[30] He,K.,Zhang,X.,Ren,S.,Sun,J.: Deepresiduallearningforimagerecognition.In: ProceedingsoftheIEEE
conferenceoncomputervisionandpatternrecognition.pp.770–778(2016)
[31] Hidasi,B.,Karatzoglou,A.,Baltrunas,L.,Tikk,D.: Recurrentneuralnetworks.HandbookonNeuralInformation
Processing(2020)
[32] Hochreiter,S.,Schmidhuber,J.: Longshort-termmemory.Neuralcomputation9(8),1735–1780(1997)
[33] Jaiswal,A.,Babu,A.R.,Zadeh,M.Z.,Banerjee,D.,Makedon,F.: Asurveyoncontrastiveself-supervisedlearning.
Technologies9(1), 2(2020)
[34] Jing,L.,Tian,Y.: Self-supervisedvisualfeaturelearningwithdeepneuralnetworks: Asurvey.IEEEtransactions
onpatternanalysisandmachineintelligence43(11),4037–4058(2020)
[35] Jung,A.B.,Wada,K.,Crall,J.,Tanaka,S.,Graving,J.,Reinders,C.,Yadav,S.,Banerjee,J.,Vecsei,G.,Kraft,A.,
etal.: imgaug.GitHub: SanFrancisco,CA,USA(2020)
[36] Kang,L.,Rusinol,M.,Fornés,A.,Riba,P.,Villegas,M.: Unsupervisedwriteradaptationforsynthetic-to-real
handwrittenwordrecognition.In: ProceedingsoftheIEEE/CVFWinterConferenceonApplicationsofComputer
Vision.pp.3502–3511(2020)
10arXivTemplate APREPRINT
[37] Kass,D.,Vats,E.: Attentionhtr: Handwrittentextrecognitionbasedonattentionencoder-decodernetworks(2022)
[38] Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: Bengio, Y., LeCun, Y. (eds.) 3rd
International Conference on Learning Representations, ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
ConferenceTrackProceedings(2015)
[39] Kleber, F., Fiel, S., Diem, M., Sablatnig, R.: Cvl-database: An off-line database for writer retrieval, writer
identificationandwordspotting.In: 201312thinternationalconferenceondocumentanalysisandrecognition.pp.
560–564.IEEE(2013)
[40] Knerr,S.,Augustin,E.: Aneuralnetwork-hiddenmarkovmodelhybridforcursivewordrecognition.Proceedings.
FourteenthInternationalConferenceonPatternRecognition(Cat.No.98EX170)(1998)
[41] Kumari,L.,Singh,S.,Rathore,V.V.S.,Sharma,A.,Kumari,L.,Singh,S.,Rathore,V.V.S.,Sharma,A.: Lexicon
andattentionbasedhandwrittentextrecognitionsystem(2022)
[42] Lai,S.,Xu,L.,Liu,K.,Zhao,J.:Recurrentconvolutionalneuralnetworksfortextclassification.AAAIConference
onArtificialIntelligence(2015)
[43] Li,M.,Lv,T.,Chen,J.,Cui,L.,Lu,Y.,Florencio,D.,Zhang,C.,Li,Z.,Wei,F.: Trocr: Transformer-basedoptical
characterrecognitionwithpre-trainedmodels.Proceedingsofthe...AAAIConferenceonArtificialIntelligence
(2023)
[44] Liu,H.,Wang,B.,Bao,Z.,Xue,M.,Kang,S.,Jiang,D.,Liu,Y.,Ren,B.: Perceivingstroke-semanticcontext:
Hierarchicalcontrastivelearningforrobustscenetextrecognition.In: ProceedingsoftheAAAIConferenceon
ArtificialIntelligence.vol.36,pp.1702–1710(2022)
[45] Liu,S.,Mallol-Ragolta,A.,Parada-Cabaleiro,E.,Qian,K.,Jing,X.,Kathan,A.,Hu,B.,Schuller,B.W.: Audio
self-supervisedlearning: Asurvey.Patterns3(12)(2022)
[46] Liu, X., Zhang, F., Hou, Z., Mian, L., Wang, Z., Zhang, J., Tang, J.: Self-supervisedlearning: Generativeor
contrastive.IEEEtransactionsonknowledgeanddataengineering35(1),857–876(2021)
[47] Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolinguistic representations for
vision-and-languagetasks.NeuralInformationProcessingSystems(2019)
[48] Lu,Z.,He,S.,Zhu,X.,Zhang,L.,Song,Y.Z.,Xiang,T.: Simplerisbetter: Few-shotsemanticsegmentationwith
classifierweighttransformer(2021)
[49] Luo,C.,Jin,L.,Chen,J.: Siman: exploringself-supervisedrepresentationlearningofscenetextviasimilarity-
awarenormalization.In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
pp.1039–1048(2022)
[50] Lyu, P., Zhang, C., Liu, S., Qiao, M., Xu, Y., Wu, L., Yao, K., Han, J., Ding, E., Wang, J.: Maskocr: text
recognitionwithmaskedencoder-decoderpretraining.arXivpreprintarXiv:2206.00311(2022)
[51] Marti, U.V., Bunke, H.: The iam-database: an english sentence database for offline handwriting recognition.
InternationalJournalonDocumentAnalysisandRecognition5,39–46(2002)
[52] Michael,J.,Labahn,R.,Grüning,T.,Zöllner,J.: Evaluatingsequence-to-sequencemodelsforhandwrittentext
recognition.IEEEInternationalConferenceonDocumentAnalysisandRecognition(2019)
[53] Misra,I.,Maaten,L.v.d.: Self-supervisedlearningofpretext-invariantrepresentations.In: Proceedingsofthe
IEEE/CVFconferenceoncomputervisionandpatternrecognition.pp.6707–6717(2020)
[54] Momeni,S.,BabaAli,B.: Atransformer-basedapproachforarabicofflinehandwrittentextrecognition.arXiv.org
(2023)
[55] Mostafa, A., Mohamed, O., Ashraf, A., Elbehery, A., Jamal, S., Khoriba, G., Ghoneim, A.: Ocformer: A
transformer-based model for arabic handwritten text recognition. 2021 International Mobile, Intelligent, and
UbiquitousComputingConference(MIUCC)(2021)
[56] Muehlberger,G.,Seaward,L.,Terras,M.,Oliveira,S.A.,Bosch,V.,Bryan,M.,Colutto,S.,Déjean,H.,Diem,M.,
Fiel,S.,etal.: Transformingscholarshipinthearchivesthroughhandwrittentextrecognition: Transkribusasa
casestudy.Journalofdocumentation75(5),954–976(2019)
[57] Nikitha,A.,Geetha,J.,JayaLakshmi,D.: Handwrittentextrecognitionusingdeeplearning.In: 2020International
ConferenceonRecentTrendsonElectronics,Information,Communication&Technology(RTEICT).pp.388–392.
IEEE(2020)
[58] Noroozi,M.,Favaro,P.: Unsupervisedlearningofvisualrepresentationsbysolvingjigsawpuzzles.In: European
conferenceoncomputervision.pp.69–84.Springer(2016)
11arXivTemplate APREPRINT
[59] Novotny,D.,Albanie,S.,Larlus,D.,Vedaldi,A.:Self-supervisedlearningofgeometricallystablefeaturesthrough
probabilisticintrospection.In: ProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition.
pp.3637–3645(2018)
[60] Oord, A.v.d., Li, Y., Vinyals, O.: Representation learning with contrastive predictive coding. arXiv preprint
arXiv:1807.03748(2018)
[61] Ozbulak,U.,Lee,H.J.,Boga,B.,Anzaku,E.T.,Park,H.,VanMessem,A.,DeNeve,W.,Vankerschaver,J.: Know
yourself-supervisedlearning: Asurveyonimage-basedgenerativeanddiscriminativetraining.arXivpreprint
arXiv:2305.13689(2023)
[62] Pang,K.,Yang,Y.,Hospedales,T.M.,Xiang,T.,Song,Y.Z.: Solvingmixed-modaljigsawpuzzleforfine-grained
sketch-basedimageretrieval.In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition.pp.10347–10355(2020)
[63] Pathak,D.,Krahenbuhl,P.,Donahue,J.,Darrell,T.,Efros,A.A.: Contextencoders: Featurelearningbyinpainting.
In: ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.pp.2536–2544(2016)
[64] Paul,S.,Madan,G.,Mishra,A.,Hegde,N.,Kumar,P.,Aggarwal,G.: Weaklysupervisedinformationextraction
frominscrutablehandwrittendocumentimages.arXiv(2023)
[65] Poulos,J.,Valle,R.: Character-basedhandwrittentexttranscriptionwithattentionnetworks.NeuralComputing
andApplications(2021)
[66] Qiao,Z.,Ji,Z.,Yuan,Y.,Bai,J.: Decouplingvisual-semanticfeatureslearningwithdualmaskedautoencoderfor
self-supervisedscenetextrecognition.In: InternationalConferenceonDocumentAnalysisandRecognition.pp.
261–279.Springer(2023)
[67] Sang,D.V.,Cuong,L.T.B.: Improvingcrnnwithefficientnet-likefeatureextractorandmulti-headattentionfor
textrecognition.SoICT2019(2019)
[68] Seiler,R.,Schenkel,M.,Eggimann,F.:Off-linecursivehandwritingrecognitioncomparedwithon-linerecognition.
Proceedingsof13thInternationalConferenceonPatternRecognition(1996)
[69] Sánchez, J.A., Romero, V., Toselli, A., Vidal, E.: Icfhr2014 competition on handwritten text recognition on
transcriptoriumdatasets(htrts).201414thInternationalConferenceonFrontiersinHandwritingRecognition
(2014)
[70] Sánchez, J.A., Romero, V., Toselli, A., Villegas, M., Vidal, E.: Icdar2017 competition on handwritten text
recognitiononthereaddataset.201714thIAPRInternationalConferenceonDocumentAnalysisandRecognition
(ICDAR)(2017)
[71] Tay,Y.H.,Lallican,P.M.,Khalid,M.,Viard-Gaudin,C.,Knerr,S.: Offlinehandwrittenwordrecognitionusinga
hybridneuralnetworkandhiddenmarkovmodel(2001)
[72] Vaswani,A.,Shazeer,N.,Parmar,N.,Uszkoreit,J.,Jones,L.,Gomez,A.N.,Kaiser,L.u.,Polosukhin,I.: Attention
isallyouneed.In: Guyon,I.,Luxburg,U.V.,Bengio,S.,Wallach,H.,Fergus,R.,Vishwanathan,S.,Garnett,R.
(eds.)AdvancesinNeuralInformationProcessingSystems.vol.30.CurranAssociates,Inc.(2017)
[73] Vidal,E.,Toselli,A.H.,Ríos-Vila,A.,Calvo-Zaragoza,J.: End-to-endpage-levelassessmentofhandwrittentext
recognition.PatternRecognition142,109695(2023)
[74] Wick,C.,Zöllner,J.,Grüning,T.: Transformerforhandwrittentextrecognitionusingbidirectionalpost-decoding.
ICDAR(2021)
[75] Wick, C., Zöllner, J., Grüning, T.: Rescoringsequence-to-sequencemodelsfortextlinerecognitionwithctc-
prefixes.arXiv: ComputerVisionandPatternRecognition(2021)
[76] Yamaguchi,S.,Kanai,S.,Shioda,T.,Takeda,S.: Imageenhancedrotationpredictionforself-supervisedlearning.
In: 2021IEEEInternationalConferenceonImageProcessing(ICIP).pp.489–493.IEEE(2021)
[77] Yang,M.,Liao,M.,Lu,P.,Wang,J.,Zhu,S.,Luo,H.,Tian,Q.,Bai,X.: Readingandwriting: Discriminative
andgenerativemodelingforself-supervisedtextrecognition.In: Proceedingsofthe30thACMInternational
ConferenceonMultimedia.pp.4214–4223(2022)
[78] Yang, Z., Yu, H., He, Y., Sun, W., Mao, Z.H., Mian, A.: Fully convolutional network-based self-supervised
learningforsemanticsegmentation.IEEETransactionsonNeuralNetworksandLearningSystems(2022)
[79] Yang, Z., Dai, Z., Yang, Y., Carbonell, J.G., Salakhutdinov, R., Le, Q.V.: Xlnet: Generalized autoregressive
pretrainingforlanguageunderstanding.arXiv: ComputationandLanguage(2019)
12arXivTemplate APREPRINT
[80] Zhang, L., Qi, G.J., Wang, L., Luo, J.: Aet vs. aed: Unsupervised representation learning by auto-encoding
transformationsratherthandata.In: ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
Recognition.pp.2547–2555(2019)
[81] Zhang,X.,Wang,T.,Wang,J.,Jin,L.,Luo,C.,Xue,Y.: Chaco: Charactercontrastivelearningforhandwritten
textrecognition.In: InternationalConferenceonFrontiersinHandwritingRecognition.pp.345–359.Springer
(2022)
13arXivTemplate APREPRINT
A Dataaugmentation
Wedetailthedataaugmentationtechniquesappliedinourstudy,leveragingtheimgaugaugmentationpackage[35]. Our
selectionisinformedbyseminalcontributionsinthefield[3,81],leadingtotheadoptionofthesubsequentalterations:
Listing1: DataaugmentationusedintheSSLstage.
iaa . Sequential ([
iaa .SomeOf((1 , 5), [
iaa . LinearContrast ((0.5 , 1.0)) ,
iaa . GaussianBlur ((0.5 , 1.5)) ,
iaa .Sharpen(alpha=(0.0, 0.5) , lightness =(0.0, 0.5)) ,
iaa . PiecewiseAffine(scale =(0.02, 0.03) , mode=’edge’) ,
iaa . PerspectiveTransform(scale =(0.01, 0.02)) ,
] , random_order=True)]
ExamplesofthetransformationsgeneratedbythepreviouscodeareshowninFig. 5.
Figure5: Examplesofdataaugmentations.
Furthermore,drawingonestablishedpracticesintheliterature,wealsoemployasoftdataaugmentationstrategyduring
thesupervisedstage. Thisincludessubtlecropping,linearcontrastadjustments,andGaussianblur.
B Detailedresults
Owingtotheextensiveexperimentationcarriedoutinthispaper,thissectionoffersacompletecompilationofresultsfor
eachmethodwithrespecttothespecificarchitecturesemployedandthedatasetsusedforpre-trainingandevaluation.
InTab. 4wereportthequalityresultsintermsoftheCERandWAccmetrics.
InTab. 5weprovidethesemi-supervisedresultsintermsofWAccandSingleEditDistance(ED1)metrics. ED1
expressesword-levelaccuracyuptoasingleeditoperation.Notethat,theexclusionofthelattermetricinthemanuscript
wasprimarilyduetoconcernsregardingclarity. Nevertheless,inthissection,weoptedtoincludeittoenhancethe
comprehensivenessoftheresultspresented. Itisworthhighlightingthattheincorporationofthismetricdoesnotdeviate
fromtheconclusionsdrawninthepaper;instead,itservestoreinforceandbolsterthem.
14arXivTemplate APREPRINT
CER WAcc
IAM CVL G.W. IAM CVL G.W.
Geometric
Rotation 27.4 19.7 35.1 53.6 56.8 25.5
Flip 23.8 20.5 28.8 56.1 57.3 43.8
Puzzle
Jigsaw 52.4 28.2 31.3 31.6 46.0 42.2
Sorting 79.7 53.0 31.4 19.4 12.3 43.2
ChaCo 25.2 23.7 84.7 48.3 47.0 4.7
(a)RepresentationqualityunderCTCdecoder.
CER WAcc
IAM CVL G.W. IAM CVL G.W.
Geometric
Rotation 19.0 11.9 22.6 64.6 79.2 59.8
Flip 17.4 15.3 19.3 64.7 75.1 61.1
Puzzle
Jigsaw 49.1 19.6 38.3 35.7 70.5 44.9
Sorting 72.1 58.8 64.0 8.6 24.7 19.8
ChaCo 12.2 11.9 69.2 71.6 80.2 10.0
(b)RepresentationqualityunderTDdecoder.
Table4: Representationquality. Thefrozenpartisonlythevisualencoderpre-trainedintheSSLmethod. Boldvalues
indicatethebestresultsunderthesameconditionsofpre-trainingandtest.
WAcc ED1
IAM CVL G.W. IAM CVL G.W.
5% 10% 100% 5% 10% 100% 5% 10% 100% 5% 10% 100% 5% 10% 100% 5% 10% 100%
Baseline
Nopretrain 26.5 42.2 70.4 7.5 14.2 69.7 13.5 16.0 78.5 42.1 64.9 88.1 17.4 25.4 85.6 26.3 44.6 92.3
ChaCo 30.0 46.3 64.6 13.1 13.2 67.4 15.7 21.2 72.6 47.7 70.2 83.9 34.6 40.7 84.0 28.7 34.9 89.6
Proposals
Rotation 42.6 48.9 67.7 10.0 16.4 70.1 17.0 36.0 82.4 67.7 74.4 88.2 27.2 38.8 85.9 32.9 56.9 94.0
Flip 42.1 50.6 72.1 17.1 35.0 71.1 18.8 32.5 79.1 68.4 75.6 89.4 38.7 61.1 86.8 35.9 54.3 93.2
Jigsaw 42.4 51.4 67.7 5.3 12.8 55.4 12.9 19.5 72.9 66.8 75.2 86.7 18.1 31.2 77.3 26.4 43.9 89.6
Sorting 23.9 41.6 61.1 9.3 14.7 70.0 13.8 8.9 76.6 38.1 63.9 81.1 19.3 26.2 84.7 24.1 33.1 91.4
(a)Semi-supervisedresultsunderCTCdecoder.
WAcc ED1
IAM CVL G.W. IAM CVL G.W.
5% 10% 100% 5% 10% 100% 5% 10% 100% 5% 10% 100% 5% 10% 100% 5% 10% 100%
Baseline
Nopretrain 10.3 50.3 74.8 5.9 14.2 68.7 6.3 2.1 71.3 21.7 63.6 87.6 7.9 25.4 85.6 9.2 2.4 82.8
ChaCo 47.3 53.8 75.1 21.2 38.0 83.4 5.9 7.0 73.2 66.6 74.0 88.2 27.1 44.9 87.9 6.6 8.2 83.1
Proposals
Rotation 47.0 54.5 74.5 21.5 51.8 85.5 3.6 6.9 78.5 65.6 74.8 87.2 28.3 61.4 90.6 7.7 8.7 89.5
Flip 43.8 54.3 74.6 25.2 53.1 86.1 6.0 11.0 78.0 60.2 72.0 88.1 29.7 62.8 90.7 8.9 13.5 90.3
Jigsaw 43.5 51.7 74.3 21.5 38.1 84.8 14.6 20.2 77.1 60.5 69.1 87.1 27.8 44.6 88.5 18.9 26.9 88.7
Sorting 25.2 39.8 75.2 5.4 22.0 83.6 11.9 14.2 76.1 34.5 49.4 87.2 7.0 26.5 87.0 15.4 17.2 86.6
(b)Semi-supervisedresultsunderTDdecoder.
Table5: Semi-supervisedresults. Themodelspre-trainedwiththeSSLmethodsarefine-tunedwith5%,10%and
100%ofthelabelleddata. Boldvaluesindicatethebestresultsunderthesameconditionsofpre-trainingandtest.
15