RANDOMLY PIVOTED PARTIAL CHOLESKY: RANDOM HOW?
STEFANSTEINERBERGER
Abstract. We consider the problem of finding good low rank approxima-
tionsofsymmetric,positive-definiteA∈Rn×n. Chen-Epperly-Tropp-Webber
showed,amongmanyotherthings,thattherandomlypivotedpartialCholesky
algorithmthatchoosesthei−throwwithprobabilityproportionaltothediag-
onalentryAiileadstoauniversalcontractionofthetracenorm(theSchatten
1-norm) in expectation for each step. We show that if one chooses the i−th
row with likelihood proportional to A2 one obtains the same result in the
ii
Frobenius norm (the Schatten 2-norm). Implications for the greedy pivoting
ruleandpivotselectionstrategiesarediscussed.
1. Introduction
1.1. Randomly pivoted Cholesky. We consider the problem of finding a rank
k approximation of a symmetric, positive-definite matrix A ∈ Rn×n in the setting
wherenislargeandevaluatingentriesofthematrixisexpensive. Ausefulmethod
in this setting is the randomly pivoted partial Cholesky decomposition which pro-
vides a rank-k approximation using only (k +1)n entry evaluations and O(k2n)
additional arithmetic operations. Our presentation is inspired by the extensive
theoreticalandnumericalanalysisundertakenbyChen-Epperly-Tropp-Webber[1].
The essence of the method can be described very concisely: given a symmetric,
positive-definite matrix M with rows M 1,...,M
n
∈Rn, we set M(cid:99)(0) =0∈Rn×n,
initializeM(0) =M anditerativelyupdatebothmatriceswitharank-onecorrection
induced by a row M where s ∈{1,2,...,n},
sk k
(M(k))T(M(k))
M(cid:99)(k+1) =M(cid:99)(k)+ sk sk
M(k)
sksk
(M(k))T(M(k))
M(k+1) =M(k)− sk sk .
M(k)
sksk
By construction, for all k ≥0,
M(cid:99)(k)+M(k) =M
and M(cid:99)(k) is a rank k approximation of A with M(k) being the residual. The
algorithm is easy to implement and easy to run. The remaining question is how
to choose the rows s ∈ {1,2,...,n}. Ideally, we would like to choose the rows in
k
such a way that they capture a lot of the ℓ2−energy of the matrix at each step:
however, we are also trying to avoid evaluating entries of the matrix beyond what
is absolutely necessary and will usually only have access to the diagonal entries of
M(k). At this point, different philosophies start to emerge.
(1) Random Pivoting. Pick s ∈{1,2,...,n} at random, e.g. [7].
k
TheauthorwaspartiallysupportedbytheNSF(DMS-212322).
1
4202
rpA
71
]AN.htam[
1v78411.4042:viXra2
(2) Greedy. Choose s so as to select the largest diagonal entry
k
s =arg max M(k)(i,i).
k
1≤i≤n
IfX ∈Rn×n isspd,thentheinequality|X |≤(cid:112) X X ,showsthatsmall
ij ii jj
diagonal elements imply that the entire column is small. The converse is
not necessarily true but in the absence of other information, why not.
(3) Adaptive Random Pivoting. Chen-Epperly-Tropp-Webber [1] propose to
select the s ∈ {1,2,...,n} with a probability proportional to the size of
k
the diagonal entry
M(k)(i,i)
P(s =i)= .
k trM(k)
This prefers removing columns with large diagonals but is more flexible.
(4) Gibbs sampling. As also pointed out by Chen-Epperly-Tropp-Webber [1],
one may want to introduce a parameter β ≥0 and set
P(s =i) to be proportional to M(k)(i,i)β.
k
This contains random pivoting (β =0), adaptive random pivoting (β =1)
and greedy pivoting (β →∞). We will consider β =2.
1.2. The case β =1. A big advantage of adaptive random pivoting, β =1, is the
beautiful arising mathematical structure. If we are given an spd matrix A∈Rn×n
andproduceB ∈Rn×n usingadaptiverandompivoting,oneobtainsinexpectation
A2
EB =A− .
trA
This leads to a rapid decay of large eigenvalues of A which is an excellent property
when trying to obtain good low-rank approximations. The map Φ(A)=EA has a
number of other desirable and useful properties [1, Lemma 5.3]. Taking the trace
of the expectation, we have
(cid:18) tr(A2)(cid:19)
Etr(B)= 1− tr(A)
tr(A)2
which, using the Cauchy-Schwarz inequality,
n (cid:32) n (cid:33)2
(cid:88) 1 (cid:88) 1
tr(A2)= λ (A)2 ≥ λ (A) = tr(A)2
i n i n
i=1 i=1
then implies
(cid:18) (cid:19)
1
Etr(B)≤ 1− tr(A).
n
Thesituationisevenbetterthanthat: theCauchy-Schwarzinequalityisonlysharp
when the eigenvalues are roughly comparable (and in that case, efficient low rank
approximationstartsbeingimpossibleanditdoesnotmatterverymuchwhichrow
one picks). If some eigenvalues are bigger than others, the argument above shows
the presence of a nonlinear feedback loop that leads to much better results. Chen-
Epperly-Tropp-Webber [1] use this in conjunction with a sophisticated argument
across many iterations to prove that adaptive random pivoting can be used to
obtain high quality low-rank approximations within a short number of iterations
when the error is measured in the trace norm.3
2. Results
2.1. Frobenius norm. We saw above that if B ∈Rn×n arises from A∈Rn×n by
removing the i−th row/column with likelihood proportional to A , then
ii
(cid:18) tr(A2)(cid:19) (cid:18) 1(cid:19)
Etr(B)= 1− tr(A)≤ 1− tr(A).
tr(A)2 n
Thisnaturalinequalityimpliesrapiddecayofthetracenorm(Schatten1-norm)un-
derrandomadaptivepivoting. WenowproveananalogousresultfortheFrobenius
norm (Schatten 2-norm) when sampling with likelihood proportional to A2.
ii
Theorem. Suppose A ∈ Rn×n is spd and B arises from A by selecting the i−th
row/column as pivot with likelihood proportional to A2. Then
ii
n (cid:18) (cid:19)
1 (cid:88) 1
E∥B∥2 ≤∥A∥2 − ∥A ∥4 ≤ 1− ∥A∥2
F F (cid:80)n A2 i ℓ2 n F
i=1 ii i=1
The first inequality is sharp when A is a diagonal matrix. In contrast to the
statement for the trace which is an identity, the nonlinearity complicates things
and both inequalities are typically strict. Moreover, as above, we observe that the
inequalitiesgetstrictlystrongerinthepresenceoffluctuationsin∥A ∥ . Theproof
i ℓ2
implies a stronger statement, however, since that is slightly more difficult to parse
we postpone its discussion to the last section.
2.2. GreedyPivoting. Asabyproduct,theproofoftheTheoremhasimplications
for understanding greedy pivoting (β → ∞) and, in particular, when it may be
useful. The story can be concisely summarized in the following two bounds.
Corollary 1. Suppose A ∈ Rn×n is symmetric and positive semi-definite. If B
arises from removing the i−th row/column, then
∥A ∥4
∥B∥2 ≤∥A∥2 − i ℓ2.
F F A2
ii
In particular, since ∥A ∥2 ≥A2, this implies
i ℓ2 ii
∥B∥2 ≤∥A∥2 −A2.
F F ii
Both inequalities are sharp for diagonal matrices. The second inequality suggests
that one should maximize A2 since that leads to guaranteed decay. However, this
ii
is only a good idea if A being large is indicative of ∥A ∥4 /A2 being large or,
ii i ℓ2 ii
phrased differently, if A being large implies that the entire row/column is large.
ii
This leads to an (impractical) deterministic method of purely theoretical interest.
Corollary 2. Suppose A ∈ Rn×n is symmetric and positive semi-definite. If B
arises from removing the i−th row/column, where
∥A ∥2 (cid:18) 1(cid:19)
i=arg max j ℓ2, then ∥B∥2 ≤ 1− ∥A∥2.
1≤j≤n A jj F n F
Ajj̸=0
We emphasize again that this procedure is not practical: the whole purpose of
fast algorithms of this type is that they require relatively few read outs of matrix
entries: computing the ℓ2−norm of all the columns is too costly. However, we
believe it to be a valuable theoretical insight suggesting that, depending on the
matrix structure, greedy pivoting may be safe in practice.4
2.3. Some examples. Itisanaturalquestionwhethertheresultspresentedinthe
previous section have any practical relevance and whether they suggest any type
of pivoting rule. As a starting point, we note that the contraction property in the
Frobenius norm indicates that selecting pivots with likelihood proportional to A2
ii
is likely to produce reasonable results in the Frobenius norm and therefore also in
the trace norm (which is controlled by the Frobenius norm). Nonetheless, it is not
a priori clear how this will manifest in concrete examples.
1. Diagonal Matrices. We discuss three relevant examples. The first example is
very simple: consider the case when A∈Rn×n is a diagonal matrix. The expected
decay (of the trace norm) in each step is given by
(cid:80)n AβA
i=1 ii ii which is monotonically increasing in β.
(cid:80)n Aβ
i=1 ii
Larger values of β lead to better results, greedy pivoting is the best. There is an
interesting question: the example of diagonal matrices shows that if one wants to
samplewithlikelihoodAβ anddesirestohaverapiddecayoftheSchattenp−norm
ii
∥A∥p = (cid:80)n λ (A)p, then a necessary condition is β ≥ p. Is it also sufficient? It
p i=1 i
is for β =1 (by Chen-Epperly-Tropp-Webber [1]) and β =2 (by our Theorem).
2. Random Matrices. We consider 100×100 random spd matrices of the form
A=QTDQ,whereDisdiagonal,D =f(i)andQisarandomorthogonalmatrix.
ii
WemeasurethesizeofM(50) relativetothesizeofM(0) =Ainthreenorms∥·∥ :
X
the operator norm, the Frobenius norm and the trace norm. Due to concentration
effects, the results of the experiment do not seem to depend strongly on the choice
of the random orthogonal matrix Q and are thus reproducible. Greedy pivoting is
uniformly better but the difference is not large. Carrying out more experiments
shows that these types of matrices seem to behave a bit like diagonal matrices
insofar as larger values of β seem to be slightly better.
f(i) ∥·∥ ∥·∥ ∥·∥ f(i) ∥·∥ ∥·∥ ∥·∥
op F 1 op F 1
1+i/100 0.92 0.68 0.49 1+i/100 0.90 0.67 0.48
i 0.82 0.56 0.40 i 0.77 0.53 0.37
i3 0.46 0.27 0.18 i3 0.35 0.22 0.15
i5 0.20 0.11 0.07 i5 0.13 0.07 0.04
Figure1. SizeofM(50)relative Figure2. SizeofM(50)relative
to M(0) =A when β =1. to M(0) =A when β =∞.
3. Spiral Kernel. So far, we have seen two examples where larger values of β lead
to better results. To illustrate a setting where this is not the case, we use a type of
example that was originally used by Chen-Epperly-Tropp-Webber [1] to illustrate
thefailureofgreedypivoting;theexampleisinspiredbythetypeofkernelmatrices
thatmayariseinmachinelearning. Considerthecurveγ(t)=(etcost,etsint)and
sample 500 points unevenly from the interval [0,64] to create two clusters (see
Figure 4 below). The matrix is A
=exp(cid:0)
−∥x −x
∥2/1000(cid:1)
and our goal will be
ij i j
toobtainalow-rankapproximationofA(using,say,50stepsofrandomCholesky).
Thistypeofstructuremaycauseproblems: theentriesonthediagonalare,initially,
A =1 and are in no relation to the size of the associate columns.
ii5
5000
-10000 -5000 5000
-5000
Figure 3. Points on a spiral. Figure 4. The matrix A.
Choosing indices uniformly at random (β = 0) gives the best result. Adaptive
RandomPivoting(β =1)isalmostasgood,themethodsuggestedbyourTheorem
(β = 2) is virtually indistinguishable. The Greedy Method fails in a somewhat
unfair way: initially, all entries on the diagonal are 1. In the implementation of
the author, if the list of indices i with A being maximal contains more than one
ii
element, the largest index is chosen. This means that the greedy method will erase
the last row and column. This has no impact on the rest of the matrix and it
will then, by induction, erase the penultimate row/column. It ends up selecting
and erasing the 50 last rows/columns. A fairer implementation would be to take a
random element among those that are maximal, something that is essentially done
by setting, say, β =20. β =20 is worse than the other methods but not by much.
Method ∥·∥ ∥·∥ ∥·∥
op F 1
Purely Random (β =0) 0.06 0.17 0.56
Adaptive Random (β =1) 0.09 0.20 0.57
Frobenius Random (β =2) 0.10 0.20 0.58
Greedy (β =∞) 1 0.99 0.9
Fixed Greedy (β =20) 0.18 0.28 0.62
Alternating β =0,β =∞ 0.09 0.21 0.59
Table 1. Ratio ∥M(50)∥ /∥M(0)∥ in various norms.
X X
We also note that, frequently, the performance appears to be ordered: when β =1
does better than β =2, then usually β =0 will be better than both. Conversely, if
β =2 is better than β =1, then greedy matching β =∞ is often better still. This
naturally suggests the rule outlined in the subsequent section.
2.4. Alternating Pivot Selection. There are essentially two cases: either the
size of ∥A ∥ is correlatedwith the sizeof A , in thatcase we want to take greedy
i ℓ2 ii
pivoting β =∞ (or at the very least β large), or the size of A is misleading when
ii
it comes to the size of ∥A ∥ : then it is safest to use random sampling β = 0.
i ℓ2
Of course, in practice, we do not know which of these two situations we face and,
trying to avoid looking up more matrix entries, there is no way for us to find out.
This suggests a natural pivoting strategy that hedges its bets.
Alternating Pivoting Rule. Alternate between greedy pivot
selection (β =∞) and a fully randomized pivot selection (β =0).6
We see in Table 1 that it does reasonable well in the example of points on a spiral:
it is comparable to β = 1 and only slightly worse than fully random selection
β =0(which, forthatexample, wasoptimal). Toillustratethestrategyinanother
setting, we return to random matrices A = QTDQ ∈ R100×100 with Q being a
random orthogonal matrix and the diagonal matrix being defined by D = f(i).
ii
This was the setting where greedy pivoting, β = ∞, was very good. We consider
the cases, f(i) = 1/i and f(i) = i2 and try to recover a rank 20 approximation.
Alternating pivoting inherits the good rate from β =∞ while also being protected
against misleading diagonal entries by sampling randomly half the time.
∥·∥ ∥·∥ ∥·∥ ∥·∥ ∥·∥ ∥·∥
op F 1 op F 1
β =0 0.20 0.31 0.49 β =0 0.41 0.31 0.24
β =1 0.19 0.31 0.48 β =1 0.37 0.26 0.21
β =2 0.18 0.30 0.48 β =2 0.34 0.24 0.20
β =∞ 0.11 0.25 0.43 β =∞ 0.23 0.17 0.16
β =0,∞ 0.14 0.27 0.45 β =0,∞ 0.29 0.22 0.20
Figure5. SizeofM(20)relative Figure6. SizeofM(20)relative
to M(0) =A when f(i)=1/i. to M(0) =A when f(i)=i3.
2.5. Related results. This algorithm has a nontrivial history, we refer to [1,
Chapter 3] for a detailed explanation. Random pivoting for QR was considered
by Frieze-Kannan-Vempala [4] with follow-up results due to Deshpande-Vempala
[2]andDeshpande-Rademacher-Vempala-Wang[3]. RandompivotingforCholesky
does not seem to appear in the literature before being mentioned in a 2017 paper
by Musco-Woodruff [5] and a 2020 paper of Poulson [6]. These two references do
not document numerical experiments and also do not investigate theoretical prop-
erties; it appears the first thorough analysis appears to be in a fairly recent paper
by Chen-Epperly-Tropp-Webber [1]. Their paper also illustrates the importance of
algorithms of this type when working with kernel methods in machine learning.
3. Proofs
3.1. Some preparatory computations. We start with an explicit computation
that is independent of the probabilities that are chosen.
Lemma 1. Let A ∈ Rn×n be symmetric with rows/columns a ,...,a . If we set
1 n
B =A−ATA /a with likelihood proportional to p , then
i i ii i
E∥B∥2 =∥A∥2
+(cid:88)n
p
(cid:18) ∥A i∥4 −2(A3) ii(cid:19)
.
F F i A2 A
i=1 ii ii
Proof. We will work with the Frobenius norm in the form
(cid:90) (cid:90) (cid:90)
∥Ax∥2dx= ⟨Ax,Ax⟩dx= (cid:10) x,ATAx(cid:11) dx=c ∥A∥2,
n F
Sn−1 Sn−1 Sn−1
where the last equation uses spherical symmetry to deduce that off-diagonal terms
in
(cid:10) x,ATAx(cid:11)
average out to 0. The constant c depends only on the dimension
n7
and will not be important. We start with a computation that is valid for any fixed
vector v ∈Rn
E∥Bv∥2 =E(cid:13) (cid:13) (cid:13) (cid:13)(cid:18) A− A AT i A i(cid:19) v(cid:13) (cid:13) (cid:13) (cid:13)2 =(cid:88)n p i(cid:13) (cid:13) (cid:13) (cid:13)Av− A AT i A iv(cid:13) (cid:13) (cid:13) (cid:13)2
ii ii
i=1
=∥Av∥2−2(cid:88)n p i(cid:28) Av,A AT i A iv(cid:29) +(cid:88)n p i(cid:13) (cid:13) (cid:13) (cid:13)A AT i A iv(cid:13) (cid:13) (cid:13) (cid:13)2
ii ii
i=1 i=1
=∥Av∥2+(cid:88)n p i(cid:32)(cid:13) (cid:13) (cid:13) (cid:13)A AT i A iv(cid:13) (cid:13) (cid:13) (cid:13)2 −2(cid:28) Av,A AT i A iv(cid:29)(cid:33) .
ii ii
i=1
Integrating v on both sides of the equation over the sphere Sn−1,
c n∥B∥2
F
=c n∥A∥2
F
+(cid:88) i=n 1p i·(cid:32) c n(cid:13) (cid:13) (cid:13) (cid:13)A AT i iA ii(cid:13) (cid:13) (cid:13) (cid:13)2
F
−2(cid:90) Sn−1(cid:28) Av,A AT i iA iiv(cid:29) dv(cid:33) .
A computation shows
(cid:13) (cid:13) (cid:13)AT i A i(cid:13) (cid:13) (cid:13)2 = (cid:13) (cid:13)AT i A i(cid:13) (cid:13)2 F = ∥A i∥4
(cid:13) A (cid:13) A2 A2
ii F ii ii
Using the symmetry of A, we have
(cid:90) (cid:28) ATA (cid:29) (cid:90) (cid:28) ATA (cid:29) (cid:18) ATA (cid:19)
2 Av, i iv dv =2 v,AT i iv dv =2c tr AT i i .
A A n A
Sn−1 ii Sn−1 ii ii
It remains to understand the trace of the matrix. One has
(cid:18) AAT i A i(cid:19) = 1 (cid:88)n A (cid:0) ATA (cid:1) = 1 (cid:88)n A A A
A A jℓ i i ℓj A jℓ iℓ ij
ii jj ii ℓ=1 ii ℓ=1
Summing over j, we get
(cid:88)n (cid:18) AAT
i
A i(cid:19)
=
1 (cid:88)n
A
(cid:88)n
A A =
1 (cid:88)n
A (A2)
a A iℓ ij jℓ A ℓi iℓ
j=1 ii jj ii ℓ=1 j=1 ii ℓ=1
=
1 (cid:88)n
A (A2) =
(A3)
ii.
A ℓi iℓ A
ii ii
ℓ=1
Altogether, we arrive at
∥B∥2 =∥A∥2
+(cid:88)n
p
(cid:18) ∥A i∥4 −2(A3) ii(cid:19)
.
F F i A2 A
i=1 ii ii
□
3.2. An Inequality. The previous Lemma leads to a curious quantity. One is
naturally inclined to believe that this quantity should be negative since, otherwise,
this would be indicative of the possibility of ‘bad’ choices that can increase the
Frobenius norm; this is indeed the case.
Lemma 2. If A∈Rn×n is symmetric, positive semi-definite, then, for 1≤i≤n,
A ·(A3) ≥∥A ∥4 .
ii ii i ℓ28
Proof. WehaveA=QTDQwithQorthogonalandD diagonalandD ≥0. Then
ii
n
(cid:88)
(DQ) = d Q =d Q .
ij ik kj ii ij
k=1
Therefore, we can write an arbitrary entry of A as
n n n
(cid:88) (cid:88) (cid:88)
A = (QT) (DQ) = (QT) d Q = d Q Q
ij ik kj ik kk kj kk ki kj
k=1 k=1 k=1
In particular, the diagonal is positive since
n
(cid:88)
A = d Q2 ≥0.
ii kk ki
k=1
Using the same computation together with Ak =QTDkQ, one sees
n
(cid:88)
(A2) =(QTD2Q) = d2 Q2
ii ii kk ki
k=1
as well as
n
(cid:88)
(A3) =(QTD3Q) = d3 Q2
ii ii kk ki
k=1
Rewriting the norm of a row of A as a diagonal entry of A2
(cid:32) n (cid:33)2 (cid:32) n (cid:33)2
∥A ∥4 = (cid:88) A2 =(cid:0) (A2) (cid:1)2 = (cid:88) d2 Q2 .
i ℓ2 ik ii kk ki
k=1 k=1
The statement then follows from the Cauchy-Schwarz inequality
(cid:32) n (cid:33)2 (cid:32) n (cid:33)2
∥A ∥4 = (cid:88) d2 Q2 = (cid:88) d1/2|Q |·d3/2|Q |
i ℓ2 kk ki kk ki kk ki
k=1 k=1
(cid:32) (cid:33)(cid:32) (cid:33)
(cid:88) (cid:88)
≤ d Q2 d3 Q2 =A ·(A3) .
kk ki kk ki ii ii
k k
□
3.3. Proof of the Theorem.
Proof. The proof of Theorem 1 is now immediate. We choose the i−th row with
probability proportional to A2. Using Lemma 1 and Lemma 2
ii
E∥B∥2 =∥A∥2
+(cid:88)n A2
ii
(cid:18) ∥A i∥4 −2(A3) ii(cid:19)
F F (cid:80)n A2 A2 A
i=1 ℓ=1 ℓℓ ii ii
n
=∥A∥2 + 1 (cid:88)(cid:0) ∥A ∥4−2A ·(A3) (cid:1)
F (cid:80)n A2 i ii ii
ℓ=1 ℓℓ i=1
n
1 (cid:88)
≤∥A∥2 − A ·(A3)
F (cid:80)n A2 ii ii
ℓ=1 ℓℓ i=1
n
1 (cid:88)
≤∥A∥2 − ∥A ∥4 .
F (cid:80)n A2 i ℓ2
ℓ=1 ℓℓ i=19
The second part of the inequality follows from
(cid:32) n (cid:33)2 n
(cid:88) (cid:88)
∥A∥4 = ∥A ∥2 ≤n ∥A ∥4
F i ℓ2 i ℓ2
i=1 i=1
together with ∥A ∥2 ≥A2. □
i ℓ2 ii
Remark. The proof shows the stronger intermediate result
n
1 (cid:88)
E∥B∥2 ≤∥A∥2 − A ·(A3) .
F F (cid:80)n A2 ii ii
ℓ=1 ℓℓ i=1
Note that A3 is also spd and has non-negative entries on the diagonal. Moreover,
n n n n
(cid:88) (cid:88) (cid:88) (cid:88)
A = λ (A) as well as (A3) = λ (A)3.
ii i ii i
i=1 i=1 i=1 i=1
Thisleadsustotheclassicalparadigmalreadyhintedatabove: findingagoodlow-
rankapproximationisaproblemthatisonlyrelevantinthepresenceofeigenvalues
at different scales. If all the eigenvalues are somewhat comparable, then most
rank−k approximations are going to be equally good. However, in the presence of
eigenvaluesatdifferentscales,thequantitytr(A3)isgoingtoundergoquiteabitof
growthwhencomparedwiththeFrobeniusnorm;thisleadstoanonlinearfeedback
loop that leads to a dramatic shrinking of the Frobenius and thus the revealing of
good low rank approximations.
3.4. Proof of Corollary 1.
Proof. Using Lemma 1 with p =1 and p =0 for j ̸=i, we have
i j
(cid:18) ∥A ∥4 (A3) (cid:19)
∥B∥2 =∥A∥2 + i −2 ii .
F F A2 A
ii ii
The Corollary follows from applying Lemma 2. □
3.5. Proof of Corollary 2.
Proof. The result follows at once from the inequality
∥A ∥4 1
max j ℓ2 ≥ ∥A∥2.
1 A≤ jj j≤ ̸=n 0 A2 jj n F
We first observe that, since A is spd, we have
|A |≤|A |1/2·|A |1/2.
ij ii jj10
In particular, if a diagonal entry vanishes, A = 0, then the entire row/column
ii
vanishes as well. Therefore, one can write
 2  2
∥A∥4
=(cid:32) (cid:88)n
∥A ∥2
(cid:33)2
=
(cid:88)n
∥A ∥2  =
(cid:88)n ∥A i∥2
ℓ2A 
F i ℓ2  i ℓ2  A ii
ii
i=1 i=1 i=1
Aii̸=0 Aii̸=0
 
≤
(cid:88)n ∥A i∥4 ℓ2(cid:32) (cid:88)n A2(cid:33)
≤∥A∥2
(cid:88)n ∥A i∥4
ℓ2
 A2  ii F A2
i=1 ii i=1 i=1 ii
Aii̸=0 Aii̸=0
∥A ∥4
≤∥A∥2 ·n· max i ℓ2.
F
1 A≤ iii≤ ̸=n 0
A2
ii
□
References
[1] Y.Chen,E.N.Epperly,J.A.Tropp,andR.J.Webber,RandomlypivotedCholesky: Prac-
ticalapproximationofakernelmatrixwithfewentryevaluations,arXiv:2207.06503.
[2] A.DeshpandeandS.Vempala.Adaptivesamplingandfastlow-rankmatrixapproximation.
InJ.Diaz,K.Jansen,J.D.P.Rolim,andU.Zwick,editors,Approximation,Randomization,
and Combinatorial Optimization. Algorithms and Techniques, Lecture Notes in Computer
Science,pages292–303.Springer,2006
[3] A. Deshpande, L. Rademacher, S. S. Vempala, and G. Wang. Matrix approximation and
projectiveclusteringviavolumesampling.TheoryofComputing,2(12):225–247,2006.
[4] A. Frieze, R. Kannan, and S. Vempala. Fast Monte-Carlo algorithms for finding low-rank
approximations.JournaloftheACM,51(6):1025–1041,2004
[5] C.MuscoandD.P.Woodruff.Sublineartimelow-rankapproximationofpositivesemidefinite
matrices.In2017IEEE58thAnnualSymposiumonFoundationsofComputerScience,pages
672–683,2017
[6] J.Poulson.High-performancesamplingofgenericdeterminantalpointprocesses.Philosoph-
icalTransactionsoftheRoyalSocietyA:Mathematical,PhysicalandEngineeringSciences,
378(2166):20190059,2020.
[7] C. Williams and M. Seeger. Using the Nystr¨om method to speed up kernel machines. In
Proceedingsofthe13thInternationalConferenceonNeuralInformationProcessingSystems,
2000.
Department of Mathematics, University of Washington, Seattle, WA 98195, USA
Email address: steinerb@uw.edu