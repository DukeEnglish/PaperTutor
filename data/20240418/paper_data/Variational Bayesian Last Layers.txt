PublishedasaconferencepaperatICLR2024
VARIATIONAL BAYESIAN LAST LAYERS
JamesHarrison1,JohnWilles2,JasperSnoek1
1GoogleDeepMind,2VectorInstitute
jamesharrison@google.com, john.willes@vectorinstitute.ai,
jsnoek@google.com
ABSTRACT
We introduce a deterministic variational formulation for training Bayesian last
layerneuralnetworks. Thisyieldsasampling-free,single-passmodelandlossthat
effectivelyimprovesuncertaintyestimation. OurvariationalBayesianlastlayer
(VBLL)canbetrainedandevaluatedwithonlyquadraticcomplexityinlastlayer
width,andisthus(nearly)computationallyfreetoaddtostandardarchitectures.We
experimentallyinvestigateVBLLs,andshowthattheyimprovepredictiveaccuracy,
calibration,andoutofdistributiondetectionoverbaselinesacrossbothregression
andclassification. Finally,weinvestigatecombiningVBLLlayerswithvariational
Bayesianfeaturelearning,yieldingalowervariancecollapsedvariationalinference
methodforBayesianneuralnetworks.
1 INTRODUCTION
Well-calibrated uncertainty quantification is essential for reliable decision-making with machine
learningsystems. However,manymethodsforimprovinguncertaintyquantificationindeeplearning
(includingBayesianmethods)haveseenlimitedapplicationduetotheirrelativecomplexityover
standarddeeplearning.Forexample,methodssuchassampling-basedmeanfieldvariationalinference
(Blundelletal.,2015),MarkovchainMonteCarlo(MCMC)methods(Papamarkouetal.,2022;Neal,
1995;Izmailovetal.,2021),andcomparativelysimpleheuristicssuchasBayesiandropout(Gal&
Ghahramani,2016)allhavesubstantiallyhighercomputationalcostthanbaselinenetworks. Single-
passmethods(whereonlyonenetworkevaluationisrequired)oftenrequiresubstantialmodifications
tonetworkarchitectures,regularization,ortrainingandevaluationprocedures,evenforthesimplest
suchmodels(Liuetal.,2022;Wilsonetal.,2016b;Kristiadietal.,2021).
Inthiswork,wetakeasimplicity-firstapproachtoBayesiandeeplearning,anddevelopaconceptu-
allysimpleandcomputationallyinexpensivepartiallyBayesianneuralnetwork. Inparticular,we
investigatevariationallearningofBayesianlastlayer(BLL)neuralnetworks. WhileBLLmodels
consideronlytheuncertaintyovertheoutputlayerofthenetwork,theyhavebeenshowntoperform
comparablytomorecomplexBayesianmodels(Watsonetal.,2021;Harrisonetal.,2018;Fiedler
&Lucia,2023;Kristiadietal.,2020). Ourvariationalformulationreliesonadeterministiclower
bound on the marginal likelihood, which enables highly-efficient mini-batch, sampling-free loss
computation,andisthushighlyscalable.
Contributions. Concretely,thecontributionsofthisworkare:
• WepresentvariationalBayesianlastlayers(VBLLs),anovellastlayerneuralnetworkcomponent
foruncertaintyquantificationwhichcanbestraightforwardlyincludedinstandardarchitectures
andtrainingpipelines(includingfine-tuning),forbothdeterministicandBayesianneuralnetworks.
• Wederiveprincipledandsampling-freeBayesiantrainingobjectivesforVBLLs,andshowthat
withcarefulparameterizationtheycanbecomputedatthesamecostasstandardtraining,and
trainedwithstandardmini-batchtraining.
• WeshowthatVBLLsimprovepredictiveaccuracy,likelihoods,calibration,andoutofdistribution
detectionacrossawidevarietyofproblemsettings. WealsoshowVBLLsstronglyoutperform
baselinemodelsincontextualbandits.
• Wereleaseaneasy-to-usepackageprovidingefficientVBLLimplementationsinPyTorch.
1
4202
rpA
71
]GL.sc[
1v99511.4042:viXraPublishedasaconferencepaperatICLR2024
2 BAYESIAN LAST LAYER NEURAL NETWORKS
WefirstreviewBayesianlastlayermodelswhichmaintainaposteriordistributiononlyforthelast
layerinaneuralnetwork. ThesemodelscorrespondtoBayesian(linearorlogistic)regressionor
BayesianGaussiandiscriminantanalysis(foreachofthethreemodelswepresent,respectively)with
learnedfeatures.WeassumeT totaldatapoints,andwriteinputsasx∈RNx.Forregression,outputs
arey ∈ RNy; forclassification,outputsarey ∈ {1,...,N y},andy denotestheN y-dimensional
one-hotrepresentation. Forallmodelsdiscussedinthissection,wewilluseneuralnetworkfeatures
ϕ : RNx ×Θ → RNϕ. Thesecorrespondtoallpartsofanetworkarchitecturebutthelastlayer,
whereθ ∈Θdenotestheweightsoftheneuralnetwork. Wewilltypicallywriteϕ:=ϕ(x,θ)for
notationalconvenienceandrefertotheseparametersasfeaturesbecausetheydefinethemapfrom
inputstothefeatureembeddingonwhichtheBLLoperates.
2.1 REGRESSION
ThecanonicalBLLmodelfortheregressioncase1is
y =w⊤ϕ(x,θ)+ε (1)
whereεisassumedtobenormallydistributedwithzeromeanandcovarianceΣ,andthesenoiseterms
arei.i.d.acrossrealizations. WespecifyaGaussianprior2p(w)=N(w¯,S),assumedindependent
¯ ¯
ofthenoiseε. PosteriorinferenceintheBLLmodelisanalyticallytractableforafixedsetoffeatures.
The marginal likelihood may be computed either via direct computation or by iterating over the
dataset. FixingadistributionoverwoftheformN(w¯,S),thepredictivedistributionis
p(y |x,η,θ)=N(w¯⊤ϕ,ϕ⊤Sϕ+Σ) (2)
whereηdenotestheparametersofthedistribution,hereη =(w¯,S).
2.2 DISCRIMINATIVECLASSIFICATION
In this subsection we introduce a BLL model that corresponds to standard classification neural
networks,where
p(y |x,W,θ)=softmax(z), z =Wϕ(x,θ)+ε (3)
where z ∈ RNy are the logits. These are also interpreted as unnormalized joint data-label log
likelihoods(Grathwohletal.,2020),where
z =logp(x,y |W,θ)−Z(W,θ) (4)
whereZ(W,θ)isanormalizingconstant,independentofthedata. Thetermε∈RNy isazero-mean
Gaussian noise term with variance Σ. Typically in logistic regression this noise term is ignored,
although it has seen use to model label noise (Collier et al., 2021). We include it to unify the
presentation,andthevariancecanbeassumedzeroasnecessary.
Asintheregressioncase,wespecifyaGaussianpriorforW. Incontrastwiththeregressionsetting,
exactinferenceandcomputationoftheposteriorpredictiveisnotanalyticallytractableinthismodel.
Werefertothismodel—consistingofmultinominalBayesianlogisticregressiononlearnedneural
networkfeatures—asdiscriminativeclassification,aslogisticregressionisaclassicaldiscriminative
learningalgorithm.
2.3 GENERATIVECLASSIFICATION
Thesecondclassificationmodelweconsideristhegenerativeclassificationmodel(Harrisonetal.,
2020;Zhangetal.,2021;Willesetal.,2022),so-calledduetoitssimilaritytoclassicalgenerative
modelssuchasGaussiandiscriminantanalysis. Inthismodel,weassumethatthefeaturesassociated
with each class are normally distributed. Placing a Normal prior on the means of these feature
1Wepresentscalar-outputregressioninthepaperbodyanddeferthemultivariateoutputcasetotheappendix.
2Throughoutthepaper,weuseoverbarstodenotemeanparametersandunderbarstodenotepriorparameters.
2PublishedasaconferencepaperatICLR2024
4 2.0
3 1.5 1.5
2 1.0 1.0
1 0.5 0.5
0 0.0 0.0
1 0.5 0.5
2 1.0 1.0
3 1.5 1.5
4 2.0
7.5 5.0 2.5 0.0 2.5 5.0 7.5 3 2 1 0 1 2 3
Figure 1: Left: A variational BLL (VBLL) regression model with BBB features trained on 50
datapointsgeneratedfromacubicfunctionwithadditiveGaussiannoise. Theplotshowsthe95%
predictivecredibleregionunderthevariationalposteriorforseveralsampledfeatureweights. Right:
Visualizing(re-scaled)p(x|y =1)−p(x|y =0)predictedbyagenerativeVBLLmodelonthe
halfmoondataset,showsgoodsensitivitytoEuclideandistanceandsensibleembeddingdensities.
distributionsanda(conjugate)Dirichletprioronclassprobabilities,wehavepriorsandlikelihoods
(toplineandbottomlinerespectively)oftheform
ρ∼Dir(α) µ ∼N(µ¯ ,S ) (5)
¯ y ¯y ¯y
y |ρ∼Cat(ρ) ϕ|y ∼N(µ ,Σ). (6)
y
I thn eth mis eam no ed me bl, eµ ¯d¯ dy in∈ gfR oN rϕ eaa cn hd
.
TS ¯ hy e∈ suR bsN cϕ ri× pN thϕ ea rr ee inth de exp er sio tr hem se taa tn isa tin cd sc foo rva eari ca hnc ce lao ssv ;e wr eµ ay ls∈ owR rN itϕ e,
µ:={µ ,...,µ }totermsforally. Thetermsρ∈P correspondtoclassprobabilities,where
1 Ny Ny
P
Ny
denotestheprobabilitysimplexembeddedinRNy. Theseclassprobabilitiesareinturnusedin
thecategoricaldistributionovertheclass.
Foradistributionovermodelparameters
Ny
(cid:89)
p(ρ,µ|η)=Dir(α) N(µ¯ ,S ) (7)
k k
k=1
forwhichwewriteη ={α,µ¯,S},wehave
α
p(x|y,η)=N(µ¯ ,Σ+S ), p(y |η)= y (8)
y y (cid:80)Ny
α
k=1 k
viaanalyticalmarginalization. Tocomputethepredictiveoverclasslabels,weapplyBayes’rule,
yielding
p(y |x,η)=softmax (logp(x|y,η)+logp(y |η)). (9)
y
Here,
1
logp(x|y,η)=− ((ϕ−µ¯ )⊤(Σ+S )−1(ϕ−µ¯ )+logdet(Σ+S )+c) (10)
2 y y y y
wherecisaconstant,sharedforallclasses,thatmaybeignoredduetotheshift-invarianceofthe
softmax. Groupingthelogdeterminanttermwiththeclassprioryieldsabiasterm. Insteadofalinear
transformationoftheinputfeaturestoobtainaclasslogit,weinsteadhaveaquadratictransformation.
Thisformulationisastrictgeneralizationofstandardclassifierarchitectures(Harrison,2021),in
whichwehavequadraticdecisionregionsasopposedtolinearones.
2.4 INFERENCEANDTRAININGINBLLMODELS
BLL models have seen growing popularity in recent years, ironically driven in part by a need
forcompatibilitywithincreasinglydeepmodels(Snoeketal.,2015;Azizzadeneshelietal.,2018;
3PublishedasaconferencepaperatICLR2024
Harrison et al., 2018; Weber et al., 2018; Riquelme et al., 2018; Harrison et al., 2020; Ober &
Rasmussen,2019;Kristiadietal.,2020;Thakuretal.,2020;Watsonetal.,2020;2021;Daxberger
et al., 2021a; Willes et al., 2022; Sharma et al., 2022; Schwöbel et al., 2022; Zhang et al., 2021;
Mobergetal.,2019;Fiedler&Lucia,2023). Exactmarginalizationenablescomputationallyefficient
treatment of uncertainty, as well as resulting in lower-variance training objectives compared to
sampling-basedBayesianmodels. AcommonandprincipledobjectivefortrainingBLLmodelsis
the(log)marginallikelihood(Harrisonetal.,2018),viagradientdescenton
T−1logp(Y |X,θ) (11)
where X,Y denote stacked data. We include a factor of T−1 to enable better comparison with
standard,non-Bayesian,trainingpipelines(typicallybasedonaveragelossovermini-batches)and
across dataset sizes. This training objective can be problematic, however: gradient computation
requirescomputingthefullmarginallikelihood,andmini-batchesdonotyieldunbiasedgradient
estimatorsasinstandardtrainingwithanarbitrarylossfunction. Evenmini-batchprocessingofthe
dataset—iteratingbetweenconditioningonmini-batchesandpredictionunderthepartialposterior—
induces long computation graphs that make training at scale impossible. Moreover, due to the
flexibility of neural network features, a full marginal likelihood training objective can result in
substantialover-concentrationoftheapproximateposterior(Thakuretal.,2020;Oberetal.,2021).
3 SAMPLING-FREE VARIATIONAL INFERENCE FOR BLL NETWORKS
Toexploitexactmarginalizationwhileavoidingfullmarginallikelihoodcomputation,wewillturn
tostochasticvariationalinference(Hoffmanetal.,2013). Inparticular,weaimtojointlycompute
anapproximatelastlayerposteriorandoptimizenetworkweightsbymaximizinglowerboundson
marginallikelihood. Assuch,wewillavoiddistributionalassumptionsmadeintheprevioussection.
Wewritethe(uncertain)lastlayerparametersasξandaimtofindanapproximateposteriorq(ξ |η)
parameterizedbyη. Concretely,throughoutthissectionwewilldevelopboundsoftheform
T−1logp(Y |X,θ)≥L(θ,η,Σ)−T−1KL(q(ξ |η)||p(ξ)) (12)
whereLisarchitecturedependentanddevelopedintheremainderofthissection. Thus,practically,
theT−1factorweightsregularizationtermsinourtrainingobjective. Inthissection,weindexdata
witht(viasubscript),includingϕ :=ϕ(x ,θ).
t t
3.1 REGRESSION
Weconsiderthelogmarginallikelihoodlogp(Y | X,θ),withmarginalizedparametersξ = {w},
andhavethefollowinglowerbound.
Theorem1. Letq(ξ |η)=N(w¯,S)denotethevariationalposteriorfortheBLLmodeldefinedin
Section2.1. Then,(12)holdswith
T (cid:18) (cid:19)
1 (cid:88) 1
L(θ,η,Σ)= logN(y |w¯⊤ϕ ,Σ)− ϕ⊤Sϕ Σ−1 . (13)
T t t 2 t t
t=1
TheproofforthisresultandallothersisavailableinAppendixF.Whenq(ξ | η) = p(ξ | Y,X)
anddistributionalassumptionsaresatisfied,thislowerboundistight(thismaybeshownbydirect
substitution).Thiscorrespondencebetweenthevariationalandtrueposteriorforappropriately-chosen
variationalfamiliesiswellknown—seeKnoblauchetal.(2019)forathoroughdiscussion. Wenote
thatasimilarobjectiveforregressionmodelswasdevelopedinWatsonetal.(2021).
3.2 DISCRIMINATIVECLASSIFICATION
Inthediscriminativeclassificationcase,theparametersareξ ={W}. Wewillassumeadiagonal
covariancematrixΣ, andwriteσ2 := Σ . Wewillfixavariationalposterioroftheformq(W |
i ii
η)=(cid:81)Ny
q(w
|η)=(cid:81)Ny
q(w¯ ,S ),wherew denotesthek’throwofW. Thisfactorization
k=1 k k=1 k k k
retainsdensecovariancesforeachclass,butsacrificescross-classcovariances. Whileweonlypresent
thisfactorizedvariationalposterior,asimilartrainingobjectivemaybederivedwithafullydense
variationalposterior. Underthevariationalposterior,wehavethefollowingboundonthemarginal
likelihood.
4PublishedasaconferencepaperatICLR2024
Theorem2. Letq(W
|η)=(cid:81)Ny
N(w¯ ,S )denotethevariationalposteriorforthediscrimina-
k=1 k k
tiveclassificationmodeldefinedinSection2.2. Then,(12)holdswith
T (cid:18) (cid:20) (cid:21)(cid:19)
L(θ,η,Σ)= 1 (cid:88) y⊤W¯ϕ −LSE w¯⊤ϕ + 1 (ϕ⊤S ϕ +σ2) (14)
T t t k k t 2 t k t k
t=1
Here,LSE (·)denotesthelog-sum-expfunction,withthesumoverk. Incontrasttotheregression
k
case,thislowerboundisalowerboundonthestandardELBO(duetotwoapplicationsofJensen’s
inequality) and the bound is not tight. We have reduced variance (which would be induced by
samplinglogitvaluesbeforethesoftmaxinstandardSVI(Ovadiaetal.,2019))forbiasduetothis
lowerbound. OurproofleveragesthesamedoubleapplicationofJensen’sinequalityusedbyBlei
& Lafferty (2007). We note that tighter analytically tractable lower bounds exist for the logistic
regressionmodel(Depraetere&Vandebroek,2017;Knowles&Minka,2011),althoughforsimplicity
oftheresultingalgorithmweusetheabovelowerbound.
3.3 GENERATIVECLASSIFICATION
Inthegenerativeclassificationcase,theparametersareξ = {µ,ρ}. Inthissetting,theDirichlet
posterioroverclassprobabilitiesp(ρ|Y)canbecomputedexactlywithonepassoverthedataby
simplycountingclassoccurrences. Wethereforeonlyconsideravariationalposterioroftheform
q(ξ
|η,Y)=q(µ|η)fortheclassembeddings,whereq(µ|η)=(cid:81)Ny
N(µ¯ ,S ). Thisyields
k=1 k k
thefollowinglowerbound.
Theorem 3. Let q(µ | η) =
(cid:81)Ny
N(µ¯ ,S ) denote the variational posterior over class em-
k=1 k k
beddings for the generative classification model defined in Section 2.3. Let p(ρ | Y) = Dir(α)
denotetheexactDirichletposterioroverclassprobabilities,withαdenotingtheDirichletposterior
concentrationparameters. Then,(12)holdswith
T
1 (cid:88) 1
L(θ,η,Σ)= (logN(ϕ |µ¯ ,Σ)− tr(Σ−1S )+ψ(α )−ψ(α )+logα (15)
T t yt 2 yt yt ∗ ∗
t=1
−LSE [logN(ϕ |µ¯ ,Σ+S )+logα ])
k t k k k
(cid:80)
whereψ(·)isthedigammafunctionandwhereα = α .
∗ k k
Importantly, we note that ψ(α ),ψ(α ),logα all vanish in gradient computation and may be
yt ∗ ∗
ignored. Thetermlogα istheLSEcannotbeignored,however. Thistrainingobjectiveisagaina
k
lowerboundontheELBO,andisnottight. ThefirstDirichletterm(intheupperline)vanishesin
gradientcomputation,butthesecondterminsidethelog-sum-expfunctiondoesnot. Inthecasethat
theposteriorconcentrationparametersareequalforallclasses(asinthecaseofabalanceddataset),
theconcentrationparametercanbepulledoutoftheLSE(·)(duetotheequivarianceoflog-sum-exp
undershifts)andcanbeignored.
3.4 TRAININGVBLLMODELS
WeproposethreemethodstolearnVBLLmodels.
Fulltraining. First,wecanjointlyoptimizethelastlayervariationalposteriortogetherwithMAP
estimationofthefeatures,yieldingcombinedtrainingobjective
θ∗,η∗,Σ∗ =argmax(cid:8) L(θ,η,Σ)+T−1(logp(θ)+logp(Σ)−KL(q(ξ |η)||p(ξ)))(cid:9) . (16)
θ,η,Σ
While one may expect this to result in substantial over-concentration for weak feature priors, in
practiceweobservethatstochasticregularizationduetomini-batchoptimizationpreventsoverconcen-
tration. Throughoutthiswork,wewillplacesimpleisotropiczero-meanGaussianpriorsonfeature
weights (yielding weight decay regularization) and a canonical inverse-Wishart prior on Σ. For
Gaussianpriors(asdevelopedthroughoutthissection)theKLregularizationtermcanbecomputed
inclosedform. Thepriorterms(andtheKLpenalty)introduceasetofnewhyperparametersthat
maybedifficulttoselect. InAppendixC,wediscussthesehyperparametersandtheirinterpretation,
andprovideareformulationofhyperparametersthatincreasesinterpretability.
5PublishedasaconferencepaperatICLR2024
Post-training. Asanalternativetojointlyoptimizingthevariationallastlayerwiththefeatures,atwo
stepprocedurecanbeused. Inthisstep,thefeatureweightsθaretrainedbyanyarbitrarytraining
procedure(e.g.standardneuralnetworktraining)andthelastlayer(andΣ)aretrainedwithfrozen
features. Thetrainingobjectiveisidenticalto(16),althoughθ∗istrainedintheinitialpre-training
stepandη∗,Σ∗aretrainedvia(16).
Feature uncertainty. Lastly, we can combine last layer SVI with variational feature learning
(Blundelletal.,2015),correspondingtoapproximatecollapsedVI(Tehetal.,2006). Thistraining
strategyallowsustoconstructavariationalposterioronthefullmarginallikelihood,via
logp(Y |X)≥E [log(Y |X,ξ,θ,Σ)]−KL(q(ξ,θ,Σ|η)||p(ξ,θ,Σ)). (17)
q(ξ,θ,Σ|η)
Assuming the prior and variational posterior factorize across the features and last layer, we can
partiallycollapsethisexpectation
E [log(Y |X,ξ,θ,Σ)]=E E [log(Y |X,ξ,θ,Σ)]≥TE [L(ξ,η,Σ)]
q(ξ,θ,Σ|η) q(θ,Σ|η) q(ξ|η) q(θ,Σ|η)
(18)
andtheKLpenaltymaybesimilarlydecomposedintoseveraltermsthatcanbecomputedinclosed
form under straightforward distributional assumptions. In the above, we have included Σ in the
variational posterior, although practically we perform MAP estimation of this covariance under
inverse-Wishartpriors. Againinthissetting,pre-trainingandpost-trainingstepsmaybecombined,
butwedonotinvestigatethiscase.
3.5 PREDICTIONWITHVBLLMODELS
ForpredictioninVBLLmodels,wewillpredictunderthevariationalposteriordirectly,approximating
(fortestinput/label(x,y)),
p(y |x,X,Y)≈E [p(y |x,ξ,θ∗,Σ∗)] (19)
q(ξ|η∗)
for the deterministic feature model. This expectation may be computed in closed form (for the
regressionandgenerativeclassificationmodel)duetoconjugacy,andcanbecomputedviainexpensive
lastlayersamplinginthediscriminativeclassificationmodel. Inthevariationalfeaturemodel,
p(y |x,X,Y)≈E E [p(y |x,ξ,θ,Σ∗)] (20)
q(θ|η∗) q(ξ|η∗)
wheretheinnerexpectationmaybecomputedexactlyandtheouterexpectationmaybeapproximated
viasampling. Furtherdetailsoftraining,prediction,andoutofdistributiondetectionwithinallthree
VBLLmodelsisprovidedinAppendixB.
Forbothtrainingandprediction,underrelativelyweakassumptionsoncovariancematrices,com-
putationalcomplexity(fortheclassificationmodels)isatmost3 O(N N2),andcanbereducedto
y ϕ
O(N N )fordiagonalcovariances. Thismatchesthecomplexityofstandardnetworkevaluation;
y ϕ
forreasonablechoicesofcovariancesparsity,theadditionalcomputationalcostofVBLLmodels
overstandardnetworksisnegligible. MoredetailsareprovidedinAppendixC.
4 RELATED WORK AND DISCUSSION
Bayesianmethodscapableofflexiblenonlinearlearninghavebeenatopicofactivestudyforthelast
severaldecades. Historically,earlyinterestinBayesianneuralnetworks(MacKay,1992;Neal,1995)
diminishedasGaussianprocessesrosetoprominence(Rasmussen,2004). Inrecentyears,however,
there has been growing interest in methods capable of learning expressive features, effectively
quantifyinguncertainty,andtrainingefficientlyonlargedatasets. Variationalmethodshaveseen
particular attention in both neural networks (Blundell et al., 2015; Ovadia et al., 2019) and GPs
(Hensman et al., 2013; Titsias, 2009; Liu et al., 2020) due to their flexibility and their ability to
producemini-batchgradientestimationtrainingschemes.
WhileawiderangeofworkhasaimedtoproducemoreperformantapproximateBayesianmethods
(includingmoreexpressivepriorandposteriorrepresentations(Fortuinetal.,2021;Izmailovetal.,
2021;Sunetal.,2019;Wilson&Izmailov,2020)),theyhavestillseenlimitedapplication,often
3ComplexityfortheregressioncaseisO(N2+N2).
y ϕ
6PublishedasaconferencepaperatICLR2024
Table1: ResultsforUCIregressiontasks.
BOSTON CONCRETE ENERGY
NLL(↓) RMSE(↓) NLL(↓) RMSE(↓) NLL(↓) RMSE(↓)
VBLL 2.55±0.06 2.92±0.12 3.22±0.07 5.09±0.13 1.37±0.08 0.87±0.04
GBLL 2.90±0.05 4.19±0.17 3.09±0.03 5.01±0.18 0.69±0.03 0.46±0.02
LDGBLL 2.60±0.04 3.38±0.18 2.97±0.03 4.80±0.18 4.80±0.18 0.50±0.02
MAP 2.60±0.07 3.02±0.17 3.04±0.04 4.75±0.12 1.44±0.09 0.53±0.01
RBFGP 2.41±0.06 2.83±0.16 3.08±0.02 5.62±0.13 0.66±0.04 0.47±0.01
Dropout 2.36±0.04 2.78±0.16 2.90±0.02 4.45±0.11 1.33±0.00 0.53±0.01
Ensemble 2.48±0.09 2.79±0.17 3.04±0.08 4.55±0.12 0.58±0.07 0.41±0.02
SWAG 2.64±0.16 3.08±0.35 3.19±0.05 5.50±0.16 1.23±0.08 0.93±0.09
BBB 2.39±0.04 2.74±0.16 2.97±0.03 4.80±0.13 0.63±0.05 0.43±0.01
VBLLBBB 2.59±0.07 3.13±0.19 3.36±0.22 5.16±0.16 1.35±0.15 0.062±0.03
Table2: FurtherresultsforUCIregressiontasks.
POWER WINE YACHT
NLL(↓) RMSE(↓) NLL(↓) RMSE(↓) NLL(↓) RMSE(↓)
VBLL 2.73±0.01 3.68±0.03 1.02±0.03 0.65±0.01 1.29±0.17 0.86±0.17
GBLL 2.77±0.01 3.85±0.03 1.02±0.01 0.64±0.01 1.67±0.11 1.09±0.09
LDGBLL 2.77±0.01 3.85±0.04 1.02±0.01 0.64±0.01 1.13±0.06 0.75±0.10
MAP 2.77±0.01 3.81±0.04 0.96±0.01 0.63±0.01 5.14±1.62 0.94±0.09
RBFGP 2.76±0.01 3.72±0.04 0.45±0.01 0.56±0.05 0.17±0.03 0.40±0.03
Dropout 2.80±0.01 3.90±0.04 0.93±0.01 0.61±0.01 1.82±0.01 1.21±0.13
Ensemble 2.70±0.01 3.59±0.04 0.95±0.01 0.63±0.01 0.35±0.07 0.83±0.08
SWAG 2.77±0.02 3.85±0.05 0.96±0.03 0.63±0.01 1.11±0.05 1.13±0.20
BBB 2.77±0.01 3.86±0.04 0.95±0.01 0.63±0.01 1.43±0.17 1.10±0.11
VBLLBBB 2.74±0.01 3.73±0.04 0.94±0.03 0.61±0.01 2.96±0.59 0.79±0.05
due to the increased computational expense of these methods (Lakshminarayanan et al., 2017;
Dusenberry et al., 2020). While some approaches to Bayesian neural networks have focused on
improvingthequalityoftheposterioruncertaintythroughe.g. betterpriors(Farquharetal.,2020;
Fortuin,2022)orinferencemethods(Izmailovetal.,2021), otherlinesofworkhavefocusedon
designingcomparativelyinexpensiveapproximateBayesianmethods. Indeed,simplestrategiessuch
asBayesiandropout(Gal&Ghahramani,2016)andstochasticweightaveraging(Maddoxetal.,
2019)haveseenmuchwiderusethanmoreexpressivemethodsduetotheirsimplicity.
OneofthesimplestBayesianmodelsistheBLLmodelthatisthefocusofthispaper,whichenables
single-pass,oftendeterministicuncertaintyprediction. Thismodelhasgainedprominencethrough
thelensofdeepkernellearning(Wilsonetal.,2016b;a;Watsonetal.,2020;Liuetal.,2022)and
withinfew-shotlearning(Harrisonetal.,2018;2020;Harrison,2021;Watsonetal.,2021;Zhang
etal.,2021). Deepkernellearningaimstoaugmentstandardneuralnetworkkernelswithneural
networkinputs. Thisapproachallowscontrolofthebehaviorofuncertainty,particularlyasafunction
ofEuclideandistance(Liuetal.,2022). Whilestochasticvariationalinferencehasbeenappliedto
thesemodels(Wilsonetal.,2016a),efficientanddeterministicmini-batchmethodshavenotbeena
majorfocus. Moreover,classificationinthesemodelstypicallyreliesonsamplinglogitsapplying
softmaxfunctions,whichincreasesvariance(Ovadiaetal.,2019;Kristiadietal.,2020;2021),oron
Laplaceapproximation(Liuetal.,2022).
Withinfew-shotlearning,exactconjugacyoftheBayesianlinearregressionmodel(Harrisonetal.,
2018)andBayesianGDA(Harrisonetal.,2020;Zhangetal.,2021;Snelletal.,2017)hasbeen
exploitedforefficientfew-shotadaptation. Thesemodelshave(inadditiontoVanAmersfoortetal.
(2020)amongothers)shownthestrongperformanceofGDA-based/radialbasisfunctionnetworks,
especiallyonproblemssuchasoutofdistributiondetection,whichwefurtherhighlightinthiswork.
However,trainingthesemodels(aswellastheDKLmethodsdiscussedpreviously)reliesondirect
computationofthemarginallikelihood. IncontrasttopriorworkonDKLandfew-shotlearning,
our approach achieves efficient and deterministic training and prediction through our variational
objectivesandthroughsimilarlyexploitingconjugacy,andthustheaddedcomplexitycomparedto
standardneuralnetworkmodelsisminimal.
7PublishedasaconferencepaperatICLR2024
Table3: ResultsforWideResNet-28-10onCIFAR-10.
Method Accuracy(↑) ECE(↓) NLL(↓) SVHNAUC(↑) CIFAR-100AUC(↑)
DNN 95.8±0.19 0.028±0.028 0.183±0.007 0.946±0.005 0.893±0.001
SNGP 95.7±0.14 0.017±0.003 0.149±0.005 0.960±0.004 0.902±0.003
D-VBLL 96.4±0.12 0.022±0.001 0.160±0.001 0.969±0.004 0.900±0.004
G-VBLL 96.3±0.06 0.021±0.001 0.174±0.002 0.925±0.015 0.804±0.006
DNN+LLLaplace 96.3±0.03 0.010±0.001 0.133±0.003 0.965±0.010 0.898±0.001
DNN+D-VBLL 96.4±0.01 0.024±0.000 0.176±0.000 0.943±0.002 0.895±0.000
DNN+G-VBLL 96.4±0.01 0.035±0.000 0.533±0.003 0.729±0.004 0.661±0.004
G-VBLL+MAP − − − 0.950±0.006 0.893±0.003
Dropout 95.7±0.13 0.013±0.002 0.145±0.004 0.934±0.004 0.903±0.001
Ensemble 96.4±0.09 0.011±0.092 0.124±0.001 0.947±0.002 0.914±0.000
BBB 96.0±0.08 0.033±0.001 0.333±0.014 0.957±0.004 0.844±0.013
D-VBLLBBB 95.9±0.15 0.058±0.019 0.238±0.036 0.832±0.026 0.744±0.010
G-VBLLBBB 95.9±0.16 0.009±0.001 0.229±0.010 0.917±0.005 0.779±0.009
5 EXPERIMENTS
WeinvestigatethethreeVBLLmodels,withbothMAPandvariationalfeaturelearning,inregression
andclassificationtasks. Afulldescriptionofallmetricsusedthroughoutthissectionandbaseline
methodsisavailableintheappendix. ToillustrateVBLLmodels,weshowpredictionsonsimple
datasetsinFigure1. TheleftfigureshowsaregressionVBLLmodelwithvariationalfeaturestrained
onthe function f(x) = cx3, withtraining datashownin red. This figureshowsthe behavioron
so-calledgapdatasets—sonamedbecauseoftheintervalbetweensubsetsofthedata. TheVBLL
modelshowsdesirableincreasinguncertaintybetweentheintervals(Foongetal.,2019). Theright
figureshowsthegenerativeclassificationmodel(G-VBLL)onthehalf-moondataset. Inparticular,
wevisualizethefeaturedensityforeachclass. Importantly,thedensityhashighEuclideandistance
sensitivity,whichhasbeenadvocatedbyLiuetal.(2022)asadesirablefeatureforrobustnessand
outofdistributiondetection.
5.1 REGRESSION
WeinvestigatetheperformanceoftheregressionVBLLmodelsonUCIregressiondatasets(Dua
&Graff,2017),whicharestandardbenchmarksforBayesianneuralnetworkregression(Moberg
etal.,2019;Ober&Rasmussen,2019;Daxbergeretal.,2021b;Watsonetal.,2021;Kristiadietal.,
2021). ResultsareshowninTables1,2. WeincludebaselinemodelsruninWatsonetal.(2021),and
wereplicatetheirexperimentalprocedureandhyperparametersascloselyaspossible(detailsinthe
appendix).
OurexperimentsshowstrongresultsforVBLLmodelsacrossdatasets. Ofparticularinterestisthe
performancerelativetotheGBLLmodel,whichistraineddirectlyontheexactmarginallikelihood
withintheBayesianlastlayermodel. Thereareseveralcontributingfactors: thepriorparameters
were jointly optimized with the feature weights in the GBLL model, whereas prior terms were
fixedinourVBLLmodel,resultinginastrongerregularizationeffect. Moreover,exactBayesian
inferencecanperformpoorlyundermodelmisspecification(Grünwald&VanOmmen,2017),whereas
variationalBayeshascomparativelyfavorablerobustnesspropertiesandasymptotics(Giordanoetal.,
2018;Wang&Blei,2019), althoughtheGaussianprocess(GP)modelgenerallyalsohasstrong
performanceacrossdatasets. Finally,directlytargetingthemarginallikelihood(computedexactly
withinconjugatemodelssuchasBLLmodels)hasbeenshowntoinducesubstantialoverfitting(Ober
etal.,2021;Thakuretal.,2020;Harrison,2021),whichthevariationalapproachmayavoiddueto
worseinferentialefficiency.
5.2 IMAGECLASSIFICATION
ToevaluateperformanceofVBLLmodelsinclassification,wetrainthediscriminative(D-VBLL)and
generative(G-VBLL)classificationmodelsontheCIFAR-10andCIFAR-100imageclassification
task. FollowingLiuetal.(2022),allexperimentsutilizeaWideResNet-28-10backbonearchitecture.
Weinvestigatefulltrainingmethods(withoutapost-trainingstep),indicatedwiththemethodnamein
thetopthirdofTables3,4;post-trainingmethods,indicatedbypre-trainingmethod+post-training
method,inthemiddlethirdoftheTables;andfeatureuncertainty,inthebottomthird.
8PublishedasaconferencepaperatICLR2024
Table4: ResultsforWideResNet-28-10onCIFAR-100.
Method Accuracy(↑) ECE(↓) NLL(↓) SVHNAUC(↑) CIFAR-10AUC(↑)
DNN 80.4±0.29 0.107±0.004 0.941±0.016 0.799±0.020 0.795±0.001
SNGP 80.3±0.23 0.030±0.004 0.761±0.007 0.846±0.019 0.798±0.001
D-VBLL 80.7±0.03 0.040±0.002 0.913±0.011 0.849±0.006 0.791±0.003
G-VBLL 80.4±0.10 0.051±0.003 0.945±0.009 0.767±0.055 0.752±0.015
DNN+LLLaplace 80.4±0.29 0.210±0.018 1.048±0.014 0.834±0.014 0.811±0.002
DNN+D-VBLL 80.7±0.02 0.063±0.000 0.831±0.005 0.843±0.001 0.804±0.001
DNN+G-VBLL 80.6±0.02 0.186±0.003 3.026±0.155 0.638±0.021 0.652±0.025
G-VBLL+MAP − − − 0.793±0.032 0.765±0.008
Dropout 80.2±0.22 0.031±0.002 0.762±0.008 0.800±0.014 0.797±0.002
Ensemble 82.5±0.19 0.041±0.002 0.674±0.004 0.812±0.007 0.814±0.001
BBB 79.6±0.04 0.127±0.002 1.611±0.006 0.809±0.060 0.777±0.008
D-VBLLBBB 77.6±0.17 0.041±0.003 1.169±0.018 0.785±0.022 0.756±0.002
G-VBLLBBB 78.1±0.18 0.046±0.002 1.156±0.008 0.832±0.023 0.742±0.004
Weevaluateoutofdistribution(OOD)detectionperformanceusingtheStreetViewHouseNumbers
(SVHN)(Netzeretal.,2011)asafar-OODdatasetforbothdatasets,andCIFAR-100forCIFAR-10
(andvice-versa)asnear-OODdatasets. In-distributiondatanormalizationisusedinbothcases. The
DNN,BBB,D-VBLLandD-VBLLBBBmodelsusemaximumsoftmaxprobability(Hendrycks&
Gimpel,2016)asanOODmeasure.TheG-VBLLandG-VBLLBBBmodelsuseanormalizedfeature
density. Twomethodsforthisexist: G-VBLLandG-VBLLBBBbothusethelearnedvariational
posteriors to compute feature likelihoods. However, the performance of this is relatively weak,
asthereisnoguaranteethatlearnedfeaturelikelihoodscorrespondeffectivelytotrueembedding
densities. Thus,wealsoinvestigateanapproachinwhichweestimatedistributionsforfixedfeatures
after training. This method estimates noise covariances for each class using the trained features,
similar to the approach used in Liu et al. (2022). We refer to this model as G-VBLL-MAP, as
theapproachcorrespondstoMAPnoisecovarianceestimation. Theseestimatedcovariancesoften
resultinoverly-confidentpredictions,andsowedonotadvocateforlabelpredictionunderthesefit
covariances,anddonotincluderesultsforthem. AppendixB.6discussesOODmethods,andfurther
experimentaldetailsareinAppendixD.
Tables3,4summarizetheCIFAR-10andCIFAR-100results. D-VBLLandG-VBLLreportstrong
accuracy performance and competitive metrics for both ECE and NLL. D-VBLL in particular
demonstratesstrongaccuracyresults,aswellascompetitive(withSNGP)NLLandOODdetection
ability. Despiteitscomparativesimplicity,itoutperformsSNGPonaccuracyandOODonCIFAR-10
andaccuracyonCIFAR-100.ItmatchesSNGPonOODforCIFAR-100,andiscompetitive(although
slightlyworse)onECEandNLL.Overall,D-VBLLmodelsstandoutfortheirstrongperformance
relative to their complexity. They also perform well as post-training models, whereas G-VBLL
performsissubstantiallydegraded.
WhilemodelswithMAPfeatureestimationshowstrongperformanceversusbaselinemodels,the
performanceofvariationalfeaturelearningmodels(BBB)ismoremixed. Inregressiontasks,these
modelsarecompetitive,whileinclassificationtheperformanceisworsethandeterministicmodels.
Inbothsettings,weusedefaultKLtermweighting(oneoverthedatasetsize). Thiscontrastswith
thetempered/coldposterioreffect(Kapooretal.,2022;Wenzeletal.,2020;Izmailovetal.,2021;
Aitchison,2020),inwhichithasbeenobservedthatalternativeweightingsofthelikelihoodandthe
KLmayoutperformthisone. Thisisattributable(inpart)totwofactors: dataaugmentationand
stochasticregularization. Inregressionthereisnodataaugmentationandthemodelistrainedfor
substantiallylongerthandeterministicmodels;inclassificationweusestandardaugmentationand
ourtrainingismorelimited. Thus,itispossiblethatclassificationBBBmodelsareover-regularized.
Weinvestigatethisquestioninmoredetailintheappendix.
5.3 SENTIMENTCLASSIFICATIONWITHLLMFEATURES
WeevaluateVBLLmodelsforlanguagemodellingtasksusingtheIMDBSentimentClassification
Dataset(Maasetal.,2011). TheIMDBdatasetisabinarytextclassificationtaskconsistingof25,000
polarizedmoviereviewsfortrainingandanother25,000fortesting. Apre-trainedOPT-175B(Zhang
etal.,2022)modelisusedfortextfeatureextraction. SequenceembeddingsareobtainedfromOPT
asthelasttokenoutputfromthethefinalnetworklayer. Wetrainboththegenerative(G-VBLL)and
discriminative(D-VBLL)modelsandabaselineMLPonthesequenceembeddingsviasupervised
9PublishedasaconferencepaperatICLR2024
0.30
1.2 MLP
90 G-VBLL 0.25
1.0 D-VBLL
0.20 80
0.8 0.15
70 0.6 0.10
0.4
60 0.05
0.2 0.00
101 102 103 104 101 102 103 104 101 102 103 104
Examples per class
Figure2: AperformancecomparisonofG-VBLL,D-VBLL,andbaselineMLPmodelsontheIMDB
SentimentClassificationDataset. Themodelsutilizetextembeddingsextractedfromapre-trained
OPT-175B model. Results are presented across multiple training dataset scales, and the shaded
regionsrepresent1σerrorbounds.
Table5: Wheelbanditcumulativeregret.
δ=0.5 δ=0.7 δ=0.9 δ=0.95 δ=0.99
VBLL 0.46±0.01 0.89±0.01 2.54±0.02 4.82±0.03 24.44±0.71
NeuralLinear 1.10±0.02 1.77±0.03 4.32±0.11 11.42±0.97 52.64±2.04
NeuralLinear-MR 0.95±0.02 1.60±0.03 4.65±0.18 9.56±0.36 49.63±2.41
LinDiagPost 1.12±0.03 1.80±0.08 5.06±0.14 8.99±0.33 37.77±2.18
Table6: Wheelbanditsimpleregret.
δ=0.5 δ=0.7 δ=0.9 δ=0.95 δ=0.99
VBLL 0.27±0.03 0.69±0.06 2.28±0.14 4.16±0.17 21.05±1.59
NeuralLinear 0.31±0.03 0.68±0.07 2.18±0.13 5.44±0.73 46.42±3.45
NeuralLinear-MR 0.33±0.04 0.79±0.07 2.17±0.14 4.08±0.20 35.89±2.98
LinPost-MR 0.70±0.06 0.99±0.10 3.08±0.22 4.85±0.27 25.42±1.81
learningatmultipletrainingdatasetscales: 10,100,1000and25,000trainingsamples. Evaluation
isperformedusingthecompletetestsetateachtrainingdatasetscale. ResultsareshowninFigure
2. TheVBLLmodelsdemonstratestrongperformanceincomparisontotheMLPbaseline. Wesee
significantlylowerpredictiveNLLandECEatsmallertrainingdatasetsizes. Thesefindingsvalidate
theVBLLmodels’potentialforintegrationwithlarge-scalemodernlanguagemodelsfordiverse
applications,particularlyinsentimentclassificationtasks.
5.4 WHEELBANDIT
ToinvestigatethevalueofVBLLmodelsinanactivelearningsetting,weapplyaVBLLregression
modeltothewheelbanditproblempresentedinRiquelmeetal.(2018). Thisproblemisacontextual
bandit in which the state is sampled randomly in a two dimensional ball, and the learned model
aimstoidentifytherewardfunction. Therearefiveregionsintheballandfiveactions: eachregion
roughlycorrespondstoacorrectactionyieldingahighreward,andincorrectactionchoiceyields
alowreward, althoughaction1alwaysyieldsanintermediaterewardandnohigh-rewardaction
existsforregion1. Theparameterδcontrolsthevolumeofthehigh-rewardregions,withlargerδ
correspondingtosmallerhigh-rewardregions. Wereportbothcumulativeregret—thedifference
in reward compared to an oracle, normalized to the performance of a random agent, aggregated
overthefullproblemduration—andthesimpleregret,whichcapturesonlythelast500timesteps
andthus(roughly)measuresthefinalqualityofthelearnedmodel. WeuseaThompsonsampling
policy(Russoetal.,2018;Thompson,1933),andcomparetothetopmodelsreportedin(Riquelme
etal.,2018). WefindthatourVBLLmodelstronglyoutperformsthetopperformingbaselinesin
cumulativeregret(Table5)andslightlyoutperformstheminsimpleregret(Table6),implyingboth
thecapacityofthemodelmatchesthebestbaselineswhilealsoexploringmoreeffectively.
6 CONCLUSIONS AND FUTURE WORK
Wehavepresentedasimple,nearlycomputationallyfreeBayesianlastlayerarchitecturethatcan
beappliedtoarbitrarynetworkbackbones. ThepracticalrealizationoftheVBLLmodelisasmall
10
ycaruccA
LLN
evitciderP ECEPublishedasaconferencepaperatICLR2024
numberofextraparameters(correspondingtothevariationalposteriorcovariance)andasmallnumber
ofregularizationtermscorrespondingtotermsarisinginthemarginalizedpredictivelikelihood,prior
termsusedinMAPestimation,andKLdivergences. Severalimportantdirectionsforfuturework
exist. First,few-showadaptationthatfurtherexploitsconjugacyofthesemodelsviae.g.recursive
Bayesianleastsquaresispossible. Wehaveonlyleveragedbasicideasfromvariationalinferencein
thiswork;therearemanyhighlypracticalideaswithinvariationalKalmanfilteringwhichmayenable
efficientmodeladaptation,labelnoiserobustness,inferencewithinheavy-tailednoise,orimproved
timeseriesfiltering(Sykacek&Roberts,2002;Sarkka&Nummenmaa,2009;Tingetal.,2007).
ACKNOWLEDGMENTS
WeacknowledgeApoorvaSharma,JaschaSohl-Dickstein,AlexAlemi,andAllanZhouforuseful
conversationsoverthecourseofthiswork. WealsogratefullyacknowledgePaulBrunzema,who
identifiedasubtlebuginourinitialresults.
REFERENCES
LaurenceAitchison. Astatisticaltheoryofcoldposteriorsindeepneuralnetworks. arXivpreprint
arXiv:2008.05912,2020.
Kamyar Azizzadenesheli, Emma Brunskill, and Animashree Anandkumar. Efficient exploration
throughBayesiandeepq-networks. arXiv:1802.04412,2018.
David Blackwell. Conditional expectation and unbiased sequential estimation. The Annals of
MathematicalStatistics,1947.
David M Blei and John D Lafferty. A correlated topic model of science. The annals of applied
statistics,2007.
CharlesBlundell,JulienCornebise,KorayKavukcuoglu,andDaanWierstra. Weightuncertaintyin
neuralnetwork. InInternationalConferenceonMachineLearning(ICML),2015.
GeorgeEPBoxandGeorgeCTiao. Bayesianinferenceinstatisticalanalysis, volume40. John
Wiley&Sons,2011.
MarkCollier, BasilMustafa, EfiKokiopoulou, RodolpheJenatton, andJesseBerent. Correlated
input-dependentlabelnoiseinlarge-scaleimageclassification. InIEEEConferenceonComputer
VisionandPatternRecognition(CVPR),2021.
ErikDaxberger,AgustinusKristiadi,AlexanderImmer,RunaEschenhagen,MatthiasBauer,and
PhilippHennig. Laplaceredux-effortlessBayesiandeeplearning. NeuralInformationProcessing
Systems(NeurIPS),2021a.
ErikDaxberger,EricNalisnick,JamesUAllingham,JavierAntorán,andJoséMiguelHernández-
Lobato.Bayesiandeeplearningviasubnetworkinference.InInternationalConferenceonMachine
Learning(ICML),pp.2510–2521.PMLR,2021b.
NicolasDepraetereandMartinaVandebroek. Acomparisonofvariationalapproximationsforfast
inferenceinmixedlogitmodels. ComputationalStatistics,2017.
DheeruDuaandCaseyGraff. UCImachinelearningrepository,2017. URLhttp://archive.
ics.uci.edu/ml.
MichaelDusenberry,GhassenJerfel,YemingWen,YianMa,JasperSnoek,KatherineHeller,Balaji
Lakshminarayanan, and Dustin Tran. Efficient and scalable bayesian neural nets with rank-1
factors. InInternationalConferenceonMachineLearning(ICML),2020.
SebastianFarquhar,MichaelAOsborne,andYarinGal. Radialbayesianneuralnetworks: Beyond
discrete support in large-scale bayesian deep learning. In Artificial Intelligence and Statistics
(AISTATS),2020.
FelixFiedlerandSergioLucia.Improveduncertaintyquantificationforneuralnetworkswithbayesian
lastlayer. arXivpreprintarXiv:2302.10975,2023.
11PublishedasaconferencepaperatICLR2024
AndrewYKFoong,YingzhenLi,JoséMiguelHernández-Lobato,andRichardETurner.‘in-between’
uncertaintyinbayesianneuralnetworks. arXivpreprintarXiv:1906.11537,2019.
VincentFortuin. Priorsinbayesiandeeplearning: Areview. InternationalStatisticalReview,2022.
VincentFortuin,AdriàGarriga-Alonso,SebastianWOber,FlorianWenzel,GunnarRätsch,RichardE
Turner,MarkvanderWilk,andLaurenceAitchison. Bayesianneuralnetworkpriorsrevisited.
arXiv:2102.06571,2021.
Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model
uncertaintyindeeplearning. InInternationalConferenceonMachineLearning(ICML),2016.
SeymourGeisser.Bayesianestimationinmultivariateanalysis.TheAnnalsofMathematicalStatistics,
1965.
RyanGiordano,TamaraBroderick,andMichaelIJordan. Covariances,robustnessandvariational
bayes. JournalofMachineLearningResearch,2018.
WillGrathwohl,Kuan-ChiehWang,Jörn-HenrikJacobsen,DavidDuvenaud,MohammadNorouzi,
andKevinSwersky. Yourclassifierissecretlyanenergybasedmodelandyoushouldtreatitlike
one. InInternationalConferenceonLearningRepresentations(ICLR),2020.
PeterGrünwaldandThijsVanOmmen. Inconsistencyofbayesianinferenceformisspecifiedlinear
models,andaproposalforrepairingit. BayesianAnalysis,2017.
James Harrison. Uncertainty and efficiency in adaptive robot learning and control. PhD thesis,
StanfordUniversity,2021.
James Harrison, Apoorva Sharma, and Marco Pavone. Meta-learning priors for efficient online
Bayesianregression. WorkshopontheAlgorithmicFoundationsofRobotics(WAFR),2018.
James Harrison, Apoorva Sharma, Chelsea Finn, and Marco Pavone. Continuous meta-learning
withouttasks. NeuralInformationProcessingSystems(NeurIPS),2020.
DanHendrycksandKevinGimpel. Abaselinefordetectingmisclassifiedandout-of-distribution
examplesinneuralnetworks. arXiv:1610.02136,2016.
JamesHensman,NicoloFusi,andNeilDLawrence. Gaussianprocessesforbigdata. Uncertaintyin
ArtificialIntelligence(UAI),2013.
MatthewDHoffman,DavidMBlei,ChongWang,andJohnPaisley. Stochasticvariationalinference.
JournalofMachineLearningResearch,2013.
PavelIzmailov,SharadVikram,MatthewDHoffman,andAndrewGordonGordonWilson. Whatare
bayesianneuralnetworkposteriorsreallylike? InInternationalConferenceonMachineLearning
(ICML),2021.
DavidJacobson. Optimalstochasticlinearsystemswithexponentialperformancecriteriaandtheir
relationtodeterministicdifferentialgames. IEEETransactionsonAutomaticControl,1973.
SanyamKapoor,WesleyJMaddox,PavelIzmailov,andAndrewGWilson.Onuncertainty,tempering,
and data augmentation in bayesian classification. In Neural Information Processing Systems
(NeurIPS),2022.
DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. International
ConferenceonLearningRepresentations(ICLR),2015.
DiederikPKingmaandMaxWelling. Auto-encodingvariationalBayes. InternationalConference
onLearningRepresentations(ICLR),2014.
Jeremias Knoblauch, Jack Jewson, and Theodoros Damoulas. Generalized variational inference:
Threeargumentsforderivingnewposteriors. arXivpreprintarXiv:1904.02063,2019.
DavidKnowlesandTomMinka. Non-conjugatevariationalmessagepassingformultinomialand
binaryregression. InNeuralInformationProcessingSystems(NeurIPS),2011.
12PublishedasaconferencepaperatICLR2024
Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. Being Bayesian, even just a bit, fixes
overconfidenceinrelunetworks. InInternationalConferenceonMachineLearning(ICML),2020.
Agustinus Kristiadi, Matthias Hein, and Philipp Hennig. Learnable uncertainty under laplace
approximations. InUncertaintyinArtificialIntelligence(UAI),2021.
BalajiLakshminarayanan,AlexanderPritzel,andCharlesBlundell. Simpleandscalablepredictive
uncertaintyestimationusingdeepensembles. NeuralInformationProcessingSystems(NeurIPS),
2017.
HaitaoLiu,Yew-SoonOng,XiaoboShen,andJianfeiCai. Whengaussianprocessmeetsbigdata: A
reviewofscalablegps. IEEEtransactionsonneuralnetworksandlearningsystems,2020.
Jeremiah Liu, Shreyas Padhy, Jie Ren, Zi Lin, Yeming Wen, Ghassen Jerfel, Zack Nado, Jasper
Snoek,DustinTran,andBalajiLakshminarayanan. Asimpleapproachtoimprovesingle-model
deepuncertaintyviadistance-awareness. JournalofMachineLearningResearch,2022.
IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. arXiv:1711.05101,2017.
AndrewL.Maas,RaymondE.Daly,PeterT.Pham,DanHuang,AndrewY.Ng,andChristopher
Potts. Learningwordvectorsforsentimentanalysis. InProceedingsofthe49thAnnualMeeting
oftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,pp.142–150,
Portland,Oregon,USA,June2011.AssociationforComputationalLinguistics.
DavidJCMacKay. Apracticalbayesianframeworkforbackpropagationnetworks. NeuralComputa-
tion,1992.
WesleyJMaddox,PavelIzmailov,TimurGaripov,DmitryPVetrov,andAndrewGordonWilson.
Asimplebaselineforbayesianuncertaintyindeeplearning. InNeuralInformationProcessing
Systems(NeurIPS),volume32,2019.
JohnMoberg,LennartSvensson,JulianoPinto,andHenkWymeersch. Bayesianlinearregressionon
deeprepresentations. arXiv:1912.06760,2019.
Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient
estimationinmachinelearning. JournalofMachineLearningResearch,2020.
EricThomasNalisnick. OnpriorsforBayesianneuralnetworks. UniversityofCalifornia,Irvine,
2018.
RadfordMNeal. BayesianLearningforNeuralNetworks. PhDthesis,UniversityofToronto,1995.
YuvalNetzer,TaoWang,AdamCoates,AlessandroBissacco,BoWu,andAndrewY.Ng. Reading
digitsinnaturalimageswithunsupervisedfeaturelearning. InNIPSWorkshoponDeepLearning
and Unsupervised Feature Learning 2011, 2011. URL http://ufldl.stanford.edu/
housenumbers/nips2011_housenumbers.pdf.
SebastianWOberandCarlEdwardRasmussen. Benchmarkingtheneurallinearmodelforregression.
arXivpreprintarXiv:1912.08416,2019.
SebastianWOber,CarlERasmussen,andMarkvanderWilk. Thepromisesandpitfallsofdeep
kernellearning. InUncertaintyinArtificialIntelligence(UAI),2021.
Yaniv Ovadia, Emily Fertig, Jie Ren, Zachary Nado, Sebastian Nowozin, Joshua Dillon, Balaji
Lakshminarayanan, and Jasper Snoek. Can you trust your model’s uncertainty? evaluating
predictiveuncertaintyunderdatasetshift. InNeuralInformationProcessingSystems(NeurIPS),
2019.
TheodorePapamarkou,JacobHinkle,MToddYoung,andDavidWomble. Challengesinmarkov
chainmontecarloforbayesianneuralnetworks. StatisticalScience,2022.
TimPearce,RussellTsuchida,MohamedZaki,AlexandraBrintrup,andAndyNeely. Expressive
priorsinBayesianneuralnetworks: Kernelcombinationsandperiodicfunctions. InUncertaintyin
ArtificialIntelligence(UAI),2020.
13PublishedasaconferencepaperatICLR2024
FabianPedregosa,GaëlVaroquaux,AlexandreGramfort,VincentMichel,BertrandThirion,Olivier
Grisel, Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vincent Dubourg, et al. Scikit-learn:
Machinelearninginpython. theJournalofmachineLearningresearch,2011.
CRadhakrishnaRao. Informationandtheaccuracyattainableintheestimationofstatisticalparame-
ters. InBreakthroughsinstatistics.Springer,1992.
CarlEdwardRasmussen. Gaussianprocessesinmachinelearning. Springer,2004.
Jie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon,
and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. In Neural
InformationProcessingSystems(NeurIPS),2019.
JieRen,StanislavFort,JeremiahLiu,AbhijitGuhaRoy,ShreyasPadhy,andBalajiLakshminarayanan.
Asimplefixtomahalanobisdistanceforimprovingnear-ooddetection. arXiv:2106.09022,2021.
CarlosRiquelme,GeorgeTucker,andJasperSnoek. DeepBayesianbanditsshowdown. InInterna-
tionalConferenceonLearningRepresentations(ICLR),2018.
DanielJRusso,BenjaminVanRoy,AbbasKazerouni,IanOsband,ZhengWen,etal. Atutorialon
Thompsonsampling. FoundationsandTrendsinMachineLearning,2018.
Simo Sarkka and Aapo Nummenmaa. Recursive noise adaptive kalman filtering by variational
bayesianapproximations. IEEETransactionsonAutomaticControl,2009.
PolaSchwöbel,MartinJørgensen,SebastianWOber,andMarkVanDerWilk. Lastlayermarginal
likelihoodforinvariancelearning. InArtificialIntelligenceandStatistics(AISTATS),2022.
Mrinank Sharma, Sebastian Farquhar, Eric Nalisnick, and Tom Rainforth. Do Bayesian neural
networksneedtobefullystochastic? arXivpreprintarXiv:2211.06291,2022.
JakeSnell,KevinSwersky,andRichardZemel. Prototypicalnetworksforfew-shotlearning. Neural
InformationProcessingSystems(NeurIPS),2017.
JasperSnoek, OrenRippel, KevinSwersky, RyanKiros, NadathurSatish, NarayananSundaram,
MostofaPatwary,Prabhat,andRyanAdams. ScalableBayesianoptimizationusingdeepneural
networks. InternationalConferenceonMachineLearning(ICML),2015.
ShengyangSun,GuodongZhang,JiaxinShi,andRogerGrosse. Functionalvariationalbayesian
neuralnetworks. arXiv:1903.05779,2019.
PeterSykacekandStephenJRoberts. Adaptiveclassificationbyvariationalkalmanfiltering. In
NeuralInformationProcessingSystems(NeurIPS),2002.
YeeTeh,DavidNewman,andMaxWelling. Acollapsedvariationalbayesianinferencealgorithmfor
latentdirichletallocation. InNeuralInformationProcessingSystems(NeurIPS),2006.
SujayThakur,CooperLorsung,YanivYacoby,FinaleDoshi-Velez,andWeiweiPan. Uncertainty-
aware(una)basesforBayesianregressionusingmulti-headedauxiliarynetworks. arXivpreprint
arXiv:2006.11695,2020.
WilliamRThompson. Onthelikelihoodthatoneunknownprobabilityexceedsanotherinviewof
theevidenceoftwosamples. Biometrika,1933.
GeorgeCTiaoandArnoldZellner. Onthebayesianestimationofmultivariateregression. Journalof
theRoyalStatisticalSociety: SeriesB(Methodological),1964.
Jo-AnneTing,EvangelosTheodorou,andStefanSchaal. Akalmanfilterforrobustoutlierdetection.
InIEEEInternationalConferenceonIntelligentRobotsandSystems(IROS),2007.
MichalisTitsias. Variationallearningofinducingvariablesinsparsegaussianprocesses. InArtificial
IntelligenceandStatistics(AISTATS),2009.
14PublishedasaconferencepaperatICLR2024
JoostVanAmersfoort,LewisSmith,YeeWhyeTeh,andYarinGal. Uncertaintyestimationusing
asingledeepdeterministicneuralnetwork. InInternationalConferenceonMachineLearning
(ICML),2020.
YixinWangandDavidBlei. Variationalbayesundermodelmisspecification. NeuralInformation
ProcessingSystems(NeurIPS),2019.
JoeWatson,JihaoAndreasLin,PascalKlink,andJanPeters. Neurallinearmodelswithfunctional
gaussianprocesspriors. InAdvancesinApproximateBayesianInference(AABI),2020.
Joe Watson, Jihao Andreas Lin, Pascal Klink, Joni Pajarinen, and Jan Peters. Latent derivative
Bayesianlastlayernetworks. InArtificialIntelligenceandStatistics(AISTATS),2021.
NoahWeber,JanezStarc,ArpitMittal,RoiBlanco,andLluísMàrquez. OptimizingoveraBayesian
lastlayer. InNeurIPSworkshoponBayesianDeepLearning,2018.
FlorianWenzel,KevinRoth,BastiaanSVeeling,JakubS´wia˛tkowski,LinhTran,StephanMandt,
JasperSnoek,TimSalimans,RodolpheJenatton,andSebastianNowozin. Howgoodisthebayes
posterior in deep neural networks really? In International Conference on Machine Learning
(ICML),2020.
John Willes, James Harrison, Ali Harakeh, Chelsea Finn, Marco Pavone, and Steven Waslander.
Bayesianembeddingsforfew-shotopenworldrecognition. IEEETransactionsonPatternAnalysis
&MachineIntelligence,2022.
AndrewGWilsonandPavelIzmailov. Bayesiandeeplearningandaprobabilisticperspectiveof
generalization. InNeuralInformationProcessingSystems(NeurIPS),volume33,2020.
AndrewGWilson,ZhitingHu,RussRSalakhutdinov,andEricPXing. Stochasticvariationaldeep
kernellearning. InNeuralInformationProcessingSystems(NeurIPS),2016a.
AndrewGordonWilson,ZhitingHu,RuslanSalakhutdinov,andEricPXing. Deepkernellearning.
InArtificialIntelligenceandStatistics(AISTATS),2016b.
SergeyZagoruykoandNikosKomodakis. Wideresidualnetworks. arXivpreprintarXiv:1605.07146,
2016.
SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen,ShuohuiChen,Christopher
Dewan,MonaDiab,XianLi,XiVictoriaLin,etal. Opt: Openpre-trainedtransformerlanguage
models. arXivpreprintarXiv:2205.01068,2022.
XuetingZhang,DebinMeng,HenryGouk,andTimothyMHospedales. ShallowBayesianmeta
learningforreal-worldfew-shotrecognition. InInternationalConferenceonComputerVision
(ECCV),2021.
15PublishedasaconferencepaperatICLR2024
A TheMultivariateRegressionModel 17
B AlgorithmicDetails 17
B.1 FeaturePointEstimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B.2 Post-TrainingwithVBLLLayers . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
B.3 CollapsedVariationalInferenceforBayesianNeuralNetworks . . . . . . . . . . . 18
B.4 Training . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
B.5 PredictionandMonitoring . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.6 OutofDistributionDetection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C Parameterization,Complexity,Regularization,andHyperparameters 21
C.1 RegressionComplexity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
C.2 ClassificationComplexity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
C.3 ComplexityofComparableBaselines . . . . . . . . . . . . . . . . . . . . . . . . 23
C.4 Hyperparameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
C.5 UnderstandingPriorRegularizers. . . . . . . . . . . . . . . . . . . . . . . . . . . 23
D ExperimentalDetails 25
D.1 Metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.2 Baselines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.3 ToyExperiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
D.4 Regression. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
D.5 ImageClassification. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
D.6 SentimentClassificationwithLLMFeatures . . . . . . . . . . . . . . . . . . . . . 27
D.7 WheelBandit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
E HyperparameterStudiesandAblations 28
F ProofsandFurtherTheoreticalResults 29
F.1 HelperResults. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
F.2 ProofofTheorem1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
F.3 ProofofTheorem2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
F.4 ProofofTheorem3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
16PublishedasaconferencepaperatICLR2024
A THE MULTIVARIATE REGRESSION MODEL
Inthemultivariateregressioncase,weconsideramodeloftheform
y =Wϕ+ε (21)
and place a matrix normal (Tiao & Zellner, 1964; Geisser, 1965) prior on W, with W ∼
MN(w¯,I,S). For a discussion of the matrix normal distribution, we refer the reader to (Box
¯ ¯
&Tiao,2011).
Giventhematrixnormalpriorandtheabovemodel,theposteriorisalsomatrixnormal. Wethusfixa
matrixnormalvariationalposterior. InAppendixF.2,weobtainanELBOoftheform
T (cid:18) (cid:19)
L(θ,η,Σ)= 1 (cid:88) logN(y |W¯ϕ ,Σ)− 1 ϕ⊤Sϕ tr(Σ−1) . (22)
T t t 2 t t
t=1
forη ={W¯,S},andweusethisasatrainingobjective.
ForaparameterdistributionMN(W¯,I,S),predictioninthismodelisanalyticallytractableandis
p(y |x ,η,θ)=N(W¯ϕ,ϕ⊤Sϕ I+Σ). (23)
t t t t
B ALGORITHMIC DETAILS
InthissectionwepresentconcretedetailsontrainingVBLLmodels. Wefirstdescribetheprocedure
forMAPestimation,lastlayertrainingonfrozenfeatures,andvariationallearningoffeatures,as
describedinthepaperbody. Wethendiscusspriorchoice,describetheresultantregularizationterms,
anddescribepredictionandoutofdistributiondetectionwithinthesemodels.
B.1 FEATUREPOINTESTIMATION
WeproposetotrainourmodelsviajointvariationalinferenceforthelastlayerandMAPestimation
ofnetworkweights(andnoisecovariance),yieldingoptimizationproblem
θ∗,η∗,Σ∗ =argmax(cid:8) L(θ,η,Σ)+T−1(logp(θ)+logp(Σ)−KL(q(ξ |η)||p(ξ)))(cid:9) . (24)
θ,η,Σ
We will write the three terms on the RHS (scaled by 1/T) as R(θ,η,Σ). Reasonable priors for
neuralnetworkweightshavebeendiscussedinseveralpapers(Blundelletal.,2015;Pearceetal.,
2020;Fortuin,2022;Farquharetal.,2020;Watsonetal.,2020;Dusenberryetal.,2020;Nalisnick,
2018). Inthiswork,weusesimpleisotropicGaussianpriorswhichyieldsaweightdecayregularizer.
Whilevariationalinferenceforthenoisecovarianceispossible,wechoose(MAP)pointestimationto
simplifythemodel. Weuseastandardinverse-Wishartprior;ignoringtermsthatvanishingradient
computation,wehavelikelihood
ν+N +1 1
logp(Σ)= logdetΣ−1− tr(MΣ−1) (25)
2 2
whereΣisN ×N,ν > N −1arethedegreesoffreedomandM isthescalematrix. Theterms
ν,M arehyperparametersthatarefixed.
B.2 POST-TRAININGWITHVBLLLAYERS
Inadditiontojointlytrainingthefeaturesandthelastlayer,wecantrainthemindependently. This
ispotentiallydesirableinseveralsituations. Forexample,amodelmayalreadybetrained,andit
isusefultoaugmentthemodelwithuncertaintypost-hoc. Weproposetofirsttrainamodelusing
a standard network architecture and a standard loss function, yielding θ∗ and w¯∗ (or W¯∗ in the
multivariatecase). Giventhesequantities,thelastlayeristrainedvia
η∗,Σ∗ =argmax(cid:8) L(θ∗,η,Σ)+T−1(logp(Σ)−KL(q(ξ |η)||p(ξ)))(cid:9) . (26)
η,Σ
Practically,onecaninitializethemeanofthevariationallastlayer(intheregressionofdiscriminative
classificationcase)withthelastlayerpointestimatew∗fromthefirstphaseoftraining.
17PublishedasaconferencepaperatICLR2024
Algorithm1VariationalBLLTraining: Regression
Require: TrainingdataD = {X,Y}, variationalposteriorinitializationη = (w¯,S), numberof
trainepochsN,minibatchsizeB,optimizationalgorithmopt(·).
1: fori=1toN do
2: SplitdatasetDintominibatchesD j =(X j,Y j), j =1,...,⌊T/B⌋.
3: forj =1to⌊T/B⌋do
4: Lˆ(θ,η,Σ)← 1 (cid:80) (−logp(y |x,w¯)+ 1tr(Σ−1)ϕ(x)⊤Sϕ(x))
B (x,y)∈(Xj,Yj) 2
5: R(θ,η,Σ)← 1(KL(q(ξ |η)||p(ξ |ω))−logp(θ)−logp(Σ))
T
6: θ ←θ−opt(∇ θLˆ(θ,η,Σ)+∇ θR(θ,η,Σ))
7: η ←η−opt(∇ ηLˆ(θ,η,Σ)+∇ ηR(θ,η,Σ))
8: Σ←Σ−opt(∇ ΣLˆ(θ,η,Σ)+∇ ΣR(θ,η,Σ))
9: endfor
10: endfor
B.3 COLLAPSEDVARIATIONALINFERENCEFORBAYESIANNEURALNETWORKS
Stochasticvariationalapproximationstotheposterioroverthenetworkweightshavepreviouslybeen
usedforBayesianlearning(Blundelletal.,2015). Inthissection, wediscussioncomputationof
variationalposteriorq(θ),followingtheSVImethodologyasdiscussedpreviously. Whereasour
approachesdevelopedintheprevioussectionweredeterministic,SVIforallnetworkweightsisnot
possibleviadeterministicmarginalization. Thus,computing
E [logp(y |x,θ)] (27)
q(θ)
istypicallyapproximatedusingMonteCarlomethods. InBlundelletal.(2015),theauthorsturnto
thereparameterizationgradientestimator(Kingma&Welling,2014;Mohamedetal.,2020)toenable
thecomputationofthe(MonteCarloestimatorofthe)gradientwithrespecttotheparametersofthe
variationalposterior. Wecouldtakeasimilarstrategyforbothξandθ,turningtosampling-based
approximation. However,thissamplingschemeyieldsbothnoisygradientestimatesandisexpensive,
aseachsamplecorrespondstoafullnetworkevaluation. Ourapproachwillinsteadmarginalizethe
lastlayerandsample(someof)theotherlayers. ThiscorrespondstoRao-Blackwellization(Rao,
1992;Blackwell,1947)ofthevariationallowerboundestimator,yieldinglowervariancegradient
estimates.
Wewillchooseaposteriorthatfactorizesoverthe(lastlayer)parametersandweights,q(ξ,θ |η)=
q(ξ |η )q(θ |η ). Wealso,inthediscussionbelow,suppressdependenceonΣ;inpractice,wewill
ξ θ
turntopointestimationforthisterm. Notethatfurthermeanfieldfactorizationsforq(θ | η)are
typicallyemployed. Forexample,Blundelletal.(2015)factorizetheposterioroverallweightsinthe
neuralnetwork. Giventhisposteriorapproximation,wehave
logp(Y |X)≥E [logp(Y |X,ξ,θ)]−KL(q(ξ |η )||p(ξ))−KL(q(θ |η )||p(θ))
q(θ|ηθ)q(ξ|ηξ) ξ θ
(28)
undertheassumptionthatthepriorp(ξ,θ)=p(ξ)p(θ)andthus
1 1 1
logp(Y |X)≥E [L(θ,η )]− KL(q(ξ |η )||p(ξ))− KL(q(θ |η )||p(θ)) (29)
T q(θ|ηθ) ξ T ξ T θ
forthelowerboundsLdevelopedinSection3. Thus,algorithmically,wefirstcomputetheinner
expectationandthenapproximatetheouterexpectationwithasampling-basedestimator.
B.4 TRAINING
Wenowpresentourfulltrainingapproachfortheregressionandclassificationsettings. Adetailed
procedurefortrainingtheregressionmodelwithpointfeaturesisshowninAlgorithm1. Generally,
wewillminimizethelowerboundswedevelopedforeachmodel. WenotethatLisasumover
data; following Blundell et al. (2015), we compute an (unbiased) estimator Lˆ for this term with
mini-batches.
18PublishedasaconferencepaperatICLR2024
ThefactorizationoftheELBOoverthedataimpliesamini-batchestimatorforthegradient. Notethat
T
1 (cid:88)
E [logp(y |x ,ξ,θ)]=E E [logp(y |x ,ξ,θ)] (30)
T q(ξ|η) t t t q(ξ|η) t t
t=1
wheretheouterexpectationontheRHSiswithrespecttoauniformdistributionovert=1,...,T.
Notethatthisalsoholdsforlowerboundonthedatalikelihood,inthecaseofclassification. Wecan
constructarandomizedestimatorforthisexpectationbasedonsub-samplingthedata,inourcasein
mini-batches. Foramini-batchofBdatapoints,thisyieldsanestimatorfortheELBOoftheform
B
Lˆ(θ,η,Σ)= 1 (cid:88) E [logp(y |x ,ξ,θ,Σ)]. (31)
B q(ξ|η) t t
t=1
Intheclassificationcase,thismaybeaninequality. Notethatinthelimitofinfinitetrainingdata
(T →∞)theweightontheKLtermgoestozero.
WehavetrainedVBLLmodelswithbothmomentumSGDandAdamW(Loshchilov&Hutter,2017).
Whilebothworkeffectively,theyresultindifferentuncertaintyrepresentationsfarfromthedata.
TheinteractionofVBLLswiththestochasticregularizationassociatedwithdifferentoptimizersis
animportantdirectionoffuturework. Practically,gradientclippingwasnecessarytostabilizelate
training,especiallyintheregressioncase. Asthenoisevarianceconcentrates,gradientmagnitude
ishighlysensitivetosmallperturbationstofeatures,whichcanberapidlydestabilizing;gradient
clippingwasnecessaryandsufficienttopreventthisdestabilization. Beyondthesedetails,training
VBLLmodelsdidnotdifferfromtrainingnormalmodels.
B.5 PREDICTIONANDMONITORING
ForpredictionwithVBLLmodels,wepredictdirectlyusingthevariationalposterior,exploitingthe
conjugatepredictionresultsdescribedinSection2. ForallthreeVBLLmodels,trainingobjective
computationandpredictioncanbereducedfromcubictoquadraticcomplexity(inthelastlayerinput
width)bycarefulparameterizationandcomputation. Theassumptionsrequiredtoachievequadratic
complexity for the first two models are minor. However, for the generative classification model,
diagonalcovariancesmustbeassumed. Wediscusscomplexityinthenextsection.
Trainingyieldslearnednetworkweightsθ∗ (oravariationalposteriorovertheseweights), noise
covarianceΣ∗,andlastlayervariationalposteriorparametersη∗. Tomakepredictions,thereare
twooptions. Inthecaseoftheregressionandgenerativeclassificationmodel,wemaydiscardthe
variationalposteriorandleverageexactconjugacy. Under(Gaussian)distributionalassumptions,
exactposteriorsmaybecomputedwithfixedfeatures. However,exactlastlayerposteriorsmaybe
badlycalibratedduetoviolationofdistributionalassumptions. Instead,wemaymakepredictions
underthevariationalposteriordirectly,undertheassumptionthatq(ξ |η∗)≈p(ξ |X,Y),yielding
p(y |x,X,Y)≈E [p(y |x,ξ,θ∗,Σ∗)] (32)
q(ξ|η∗)
where(x,y)denoteatestpoint. Forthediscriminativeclassificationmodel,onlypredictionunder
thevariationalposteriorispossible,andinthismodel,samplingoranapproximation(e.g. Laplace)
maybeused.
Thegenerativeclassificationcaseprovidespredictedclassprobabilitybiases(thepredictedprobability
ofseeingaparticularclassbeforeobservingalabel)throughtheDirichletposterior. Incaseswherea
systemdesignerbelievesthereislikelytoexistdistributionalshiftbetweenthetrainingdataandthe
evaluationconditions,predictionsmaybedirectlycontrolledbymodifyingthisDirichletposterior.
Forthevariationalfeatureapproach,predictioncanbedonebysamplingfeaturesandcomputing
mixturedistributions,yielding
p(y |x,X,Y)≈E E [p(y |x,ξ,θ,Σ∗)] (33)
q(θ|η∗) q(ξ|η∗)
K
1 (cid:88)
≈ E [p(y |x,ξ,θ ,Σ∗)] (34)
K q(ξ|η∗) k
k=1
19PublishedasaconferencepaperatICLR2024
forθ sampledi.i.d.fromthevariationalposterior. Intheregressioncase,thisaveragingisstraight-
k
forward. Fortheclassificationcases,wecanaveragepre-softmaxorpost-softmax. Forexample,in
thecaseofgenerativeclassification,both
K
1 (cid:88)
p(y |x,X,Y)≈ softmax (logp(y |X,Y)+logE [p(x|y,ξ,θ )]) (35)
K y q(ξ|η∗) k
k=1
and
K
1 (cid:88)
p(y |x,X,Y)≈softmax (logp(y |X,Y)+log E [p(x|y,ξ,θ )]) (36)
y K q(ξ|η∗) k
k=1
arevalidMonteCarloestimatorsforthepredictivedensity,andthesameholdsforthediscriminative
classifier. Inpractice,wetypicallyusetheformer(inwhichwedirectlyaveragethepost-softmax
samples) due to the relative implementation simplicity, although the latter is necessary for some
formsofoutofdistributiondetection. Notethatinthelatterestimator,
1 (cid:88)
log x =LSE (logx )−logK (37)
K k k k
k
forgenericx andlogK vanishesinthesoftmaxandmythereforebeignored,andwheretheuseof
k
log-sum-expimprovesnumericalstability.
B.6 OUTOFDISTRIBUTIONDETECTION
Adesirablefeatureofrobustdeeplearningmodelsistheabilitytodistinguishbetweenindistribution
andoutofdistribution(OOD)data. WeuseseveralmetricsforOODdetectionwithVBLLmodels.
For the discriminative VBLL, we follow Liu et al. (2022) and use the maximum softmax proba-
bility(Hendrycks&Gimpel,2016)foranOODmeasure. Thisiscomputedbysamplingfromthe
distributionoverlogitsandpassingthesesamplesthroughthesoftmax,wheretheyareaveraged.
Forthegenerativeclassificationmodel,wecanusethefeaturedensity
(cid:88)
p(x|X,Y)≈ E [p(x,y |ξ,θ)] (38)
θ,ξ
y
asanOODmeasure. Intheabove,theexpectationarewithrespecttothevariationalposteriors;for
theMAPestimationcase,thiscorrespondstodirectevaluation.
Inpractice,wefoundpost-trainingnoisecovariancecalibrationimprovedOODdetectionperformance
fortheG-VBLLmodel. Moreprecisely,weaimtoreplaceashareddiagonalΣacrossallclasses
withaΣ foreachclass. OurintuitionisthatwhiletheΣthatisusedintrainingisprescriptive—in
y
the sense that it provides a model within which learning occurs—the estimated per-class Σ are
y
descriptive of the accuracy of modelling during training. Indeed, the training objective for the
G-VBLLmodelislabel(marginal)predictivelikelihood,andsothetrainingsignaltomodelclass
featuredensitieshighlyaccuratelyisweak.
Ourcalibrationprocedureisasfollows. First,weassumea(MAP)pointestimateforfeaturemeans
µ . For sufficiently large datasets S rapidly concentrates, so the impact of this assumption is
y y
relativelyminor. Foreachclass,wethencomputetheMAPnoisecovarainceΣ undertheinverse-
y
Wishartprior. Concretely,themeanunderGaussianpriorN(µ¯,Σ)andknownnoisecovarianceΣ
is ¯ ¯
(cid:88)
µ =(Σ−1+T Σ−1)(Σ−1 ϕ +Σ−1µ¯ ) (39)
y ¯y y t ¯y ¯y
1 1 (cid:88)
=( Σ−1Σ+I)( ϕ +ΣΣ−1µ¯ ) (40)
T y¯y T
y
t ¯y ¯y
whereT isthenumberofclassoccurrencesforclassyandwherethesumisoverallinputsinclass
y
y. ForsufficientlylargeT andzeromeanprior,thismeanisapproximatelyequaltotheempirical
average 1 (cid:80) x . Thus,takingµˆ =T−1(cid:80) x ,thenoisecovariancecanbeestimatedas
T t t
Σˆ = 1 (M +(cid:88) (ϕ −µˆ )(ϕ −µˆ )⊤) (41)
y T +ν+N +1 t y t y
y
20PublishedasaconferencepaperatICLR2024
whichcorrespondstotheMAPposteriorwithaknownmean,andwherethesumisagainoverall
inputsinclassy.
WenotethatwhileourstrategyofsequentiallyestimatingtwoMAPestimatesisrelativelyunso-
phisticated,itisstraightforwardandyieldsgoodresults,andisconsistentforlargedatasets(under
straightforwarddistributionalassumptions). Intheabove,N correspondstothedimensionofthe
covariancematrix(asin(25))andν andM correspondstothepriordegreesoffreedomandscale
matrix,respectively.WefoundthatthisMAPcovariancesestimationoutperformedthemaxlikelihood
covarianceestimationasperformedinLiuetal.(2022). Moreover,wenotethatboththeempirical
meanofthefeaturesforeachclassandthecovariancecanberecursivelyestimatedinonepassover
thedata,andsothecomplexityofthisstepisO(T). InspiredbyRenetal.(2019;2021),wesubtract
the log density under the feature prior as a normalization strategy, which also slightly improves
performance.
While this post-training last layer posterior improves OOD performance, it is substantially over-
concentratedforlabelprediction,yieldingtodramaticallyover-confidentpredictions. Itisanopen
questionhowtobestestimatethelastlayerposteriortoachievebotheffectiveandcalibratedlabel
andOODprediction.
C PARAMETERIZATION, COMPLEXITY, REGULARIZATION, AND
HYPERPARAMETERS
Inthissection,wediscusshowtoparameterizeeachofthetermsappearingineachtypeofVBLL.
Ineachmodel,weusea“mixed”parameterization—incontrasttothestandardparameterizationor
naturalparameterization. Moreprecisely,wewillparameterizetheinversenoisecovarianceΣ−1and
thecovarianceofthevariationalposteriorS viaCholeskyfactorizations,anddirectlyparameterize
meansW¯,µ. Inour(limited)comparisonsoftheperformanceofdifferentparameterizations,we
found that our mixed parameterization performed equivalently (if slightly better) to the standard
parameterization,andbothperformedbetterthannaturalparameterization. Interestingly,thisstands
incontrasttostandardpracticeinvariationalGaussianprocesslearning(Hensmanetal.,2013),in
whichauthorsfrequentlyaimtoderivenaturalgradientoptimizationalgorithms.
We will show that for each VBLL model, under a set of reasonable assumptions, complexity is
at worst quadratic in the last layer width and linear in the output dimension. These complexity
resultsenableuseofVBLLmodelsonproblemswithhighinputdimensionalityandhighoutput
dimensionality. Moreover,ourmini-batchgradientestimationtrainingobjectiveresultsin(standard)
linearcomplexityofgradientestimationinbatchsize,enablingtrainingonmuchlargerdatasetsthan
ispossiblewithstandardmarginallikelihoodobjectives.
C.1 REGRESSIONCOMPLEXITY
Ouranalysiswillfocusonthemultivariatecase,forwhichtheunivariateoutputsareaspecialcase.
WedirectlyparameterizethemeanW¯ ∈RNy×Nϕ. ThecovariancesareparameterizedviaCholesky
decompositiontoguaranteepositivesemi-definiteness;inparticularweparameterize
Σ−1 =LL⊤, L=L +diag(exp(l)) (42)
d
S =PP⊤, P =P +diag(exp(p)). (43)
d
WhereP,Larelowertriangularwithpositivediagonals,andthusL ,P arelowertriangularwith
d d
zerodiagonal,andvectorl,pcontroldiagonalelements.
Giventheseparameterizations,weshowthecomplexityofeachoperationrequiredfortrainingisat
mostquadraticinN . Thetrainingobjectivehastwoterms: thelogGaussiandensityandthetrace
ϕ
term. Forthelogdensity,wehave
e⊤Σ−1e=e⊤LL⊤e (44)
for e = y − W¯ϕ . The term L⊤e can be computed in O(N2) time. The second term is
t y
ϕ⊤Sϕ tr(Σ−1),forwhichϕ⊤Sϕ canbecomputedinO(N2)time,andthetraceterm
t t t t ϕ
tr(Σ−1)=tr(LL⊤)=∥L∥2 (45)
F
21PublishedasaconferencepaperatICLR2024
whichcanbecomputedinO(N2)timeviasquaringandsummingtheelementsofL.
y
TheremainingtermsaretheKLpenaltyonthevariationalposterior,andtheinverse-Wishartprioron
thenoisecovariance. FixingapriorMN(w¯,I,S),theKLpenaltyforthemultivariateregression
¯ ¯
caseis(ignoringconstants)
1 detS
KL(q(ξ |η)||p(ξ))= (tr((W¯ −W¯)⊤(W¯ −W¯)S−1)+N tr(S−1S)+N log ¯) (46)
2 ¯ ¯ ¯ y ¯ y detS
Wewillfixanisotropicprior,S =sI fors>0. Thus,thefirsttermis
¯
1
tr((W¯ −W¯)⊤(W¯ −W¯)S−1)= ∥W¯ −W¯∥2 (47)
¯ ¯ ¯ s ¯ F
withcomplexityO(N N ),andthesecondtermis
y ϕ
N
N tr(S−1S)= y tr(S) (48)
y ¯ s
wherethetracecanagainbecomputedasthesquaredFrobeniusnormoftheCholeskyfactorofP,
forcomplexityO(N2). Thelasttermis
ϕ
detS
N log ¯ =N N logs−N logdetS (49)
y detS y ϕ y
wherelogdetS =2logdet(P)whichisequaltothesumofthelogdiagonalelements,whichcanbe
computedinO(N ).
ϕ
Finally, we have the inverse-Wishart noise covariance prior, which has terms logdetΣ−1 and
tr(MΣ−1) for scale matrix M. The log determinant term may be computed as previously, with
complexity O(N ). Choosing scale matrix M = mI, we have tr(MΣ−1) = mtr(Σ−1) which
y
againisO(N2). Summingallofthisup,wehavethetotalcomplexityofVBLLcomputationsas
y
O(N2+N2),whichisequivalenttothecomplexityofstandardmatrixmultiplication;thus,thereis
y ϕ
effectivelyzeroaddedcomputationalexpensefromtheVBLLmodelcomparedtoastandardnetwork.
Thereadermayeasilyverifythatcomplexityofpredictionisnogreaterthanthetrainingcomplexity
intheregressionmodel.
C.2 CLASSIFICATIONCOMPLEXITY
Thecomplexityforthediscriminativeclassificationmodelfollowsfromtheregressionmodel. We
usethesameparameterization,althoughweturntoadiagonalnoisecovarianceΣ. Thecomputation
oftheKLpenaltyisidenticaltotheregressioncase. Theonlydifferenceisthatϕ⊤S ϕ mustbe
t y t
computedforallclassesy, yieldingcomplexityO(N2N ). Thistermdominatesthecomplexity
ϕ y
of this model; however, further factorization of the covariance is straightforward and can reduce
thepracticalcomplexity. Topredictinthesemodels,samplingrealizationsofthelastlayermustbe
donetosamplelogits. ThissamplingisstraightforwardtodousingtheCholeskyfactorizationofthe
covariance,andhasquadraticcomplexity.
Forthegenerativeclassificationmodel,wearelimitedbytheΣ+S terminthelog-sum-exp. As
y
farasweareaware,thereisno(practical)waytocomputethistermwithquadraticcomplexity,or
otherwiseinexpensivelycomputethislogdensity. Thus,inthispaperwerestrictΣandS todiagonal
matrices,whichresultsinlinearcomplexityinN foralloperationsinlosscomputation. Thus,under
ϕ
thisapproximateposterior,thecomplexityofthefulltraininglosscomputationisO(N N ),which
ϕ y
isequivalenttostandardneuralnetworkmodels. Thiscovariancestructureisrelativelyrestrictive,
andimprovementsmayresultsfromsparsecovariancestructures.
Concretely,wecomparetheruntimeofonestepoftrainingacrossabaselineDNN,andbothflavors
ofVBLL.WecomparethesemodelsonCIFAR-10trainingonaNVIDIAT4GPU,withthewide
ResNetencoderusedintherestoftheclassificationexperiments. TheresultsareshowninTable7.
WenotethatourVBLLimplementationsarenotcarefullyoptimized,andsotheseslowdownsarean
upperboundonthepossibleslowdown.
22PublishedasaconferencepaperatICLR2024
Table7: TimeperbatchonCIFAR-10training.
Model Runtime(s) %aboveDNN
DNN 0.321 0%
D-VBLL 0.338 5.2%
G-VBLL 0.364 13.4%
C.3 COMPLEXITYOFCOMPARABLEBASELINES
ThereareasetofbaselinemethodsthataresimilartoVBLLsbutoftenhavedifferentcomplexity.
Asdiscussedthroughoutthepaper,trainingBLLmodelsbyexploitingexactconjugacy(orexactly
computing the marginal likelihood) requires iterating over the full training set, yielding linear
complexityinthesizeofthedataset. Thisalmostalwaysmakesstandardmarginallikelihoodtraining
intractable. MoredirectlycomparableisSNGP(Liuetal.,2022),whichalsoexploitsexactconjugacy
(orapproximationthereofforclassification)butonlycomputesthelastlayercovarianceonceper
epoch. Thisamortizesthecostofiterationoverthefulldataset. Inpractice,theyuseanexponential
movingaverageestimateofthecovariance,whichremovestheneedtoloadthedatamultipletimes
perepoch. However,thiscovariancemuststillbecomputedandinverted,whichhascubiccomplexity
inthelastlayerdimension. LastlayerLaplace(Daxbergeretal.,2021a)methods,similarly,require
apassoverthefulldatasetandmustinvertadensecovariancematrix,yieldingcubiccomplexity.
However,thisisonlydoneasapost-processingstepforatrainedmodel.
C.4 HYPERPARAMETERS
VBLLmodelsintroduceasmallnumberofhyperparametersoverstandardnetworktraining. First,
standard hyperparameters may need to be modified for VBLL models. For example, we found
longertrainingrunsresultedinslightlyimprovedcalibration,butwebelievefurtherinvestigationof
learningrateschedulesisnecessary. ForMAPfeaturesestimation,weusestandardweightdecay
regularizationvalues.
ThemainnovelhyperparametersintroducedbytheVBLLmodelarethoseassociatedwithpriors. In
particular,thelastlayermeanprior(definedbyameanandvariance;intheregressioncase,these
arewrittenw¯,S)mustbechosen. Practically,itiscommontonormalizeoutputstohaveisotropic
¯ ¯
Gaussiandistributionsforregression,andthuswehavefoundw¯ =0andS =I yieldareasonableif
¯ ¯
diffuseprior. Fortheclassificationcase,wefoundthesevaluessimilarlyinducereasonableepistemic
uncertaintyoverthepredictivecategoricaldistribution.
Theothernovelhyperparametersarethoseassociatedwiththenoisecovarianceinverse-Wishartprior,
thedegreesoffreedomν andthescalematrixM. Forallexperiments,wefixthescalematrixasa
scalarmultipleoftheidentitymatrix,M = mI. Inourregressionexperimentswefixthesetobe
(1,1),andfindgoodresultingperformance,butfurtherinvestigationispossible. Intheclassification
case—andinparticularthegenerativeclassificationcase—theseparameterscontrolthedegreeof
concentrationinthefeaturespace,andthusmustbemorecarefullyselected(andoftenco-selected
withtheweightdecaystrength).
C.5 UNDERSTANDINGPRIORREGULARIZERS
Inthissubsectionweinvestigatetheregularizationeffectsoftheprior(andKL)terms, andcom-
pare them to standard regularizers such as weight decay. Note that naive weight decay on these
parameterizationswouldcorrespondtoadditionallosstermsoftheform
λ
(∥L ∥2 +∥l∥2+∥P ∥2 +∥p∥2). (50)
2 d F 2 d F 2
Thelosstermsresultingfromourchosenpriorsintheregressioncase(anddroppingtermswithzero
gradient)are
1
−logp(Σ)= (mtr(Σ−1)−ν˜logdetΣ−1) (51)
2
(cid:18) (cid:19)
1 1 N
KL(q(ξ |η)||p(ξ))= ∥W¯∥2 + y tr(S)−N logdetS) (52)
2 s F s y
23PublishedasaconferencepaperatICLR2024
12 12
10 10
8 8
6 6
4 4
2 2
0 0
2 2
0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0
exp(2pk) exp(2pk)
Figure3: Weightdecay(left)andourKL/Inverse-Wishartregularizers(right)plottedversusexp(p )
k
(whichcorrespondstothediagonalelementofthecovariancematrix). Differentcurvesshowvarying
weightdecaystrengthandvaryingatermin(56),withb=1.
forν˜=ν+N+1;notethat(otherthantheweightdecay-liketermonW¯)bothcovarianceregularizers
areoftheform
atr(M)−blogdet(M). (53)
forconstantsa,bandmatrixM. GivenourCholeskyparameterization,
(cid:88)
tr(Σ−1)=∥P ∥2 + exp(2p ) (54)
d F k
k
(cid:88)
logdetΣ−1 = 2p (55)
k
k
andsimilarlyforS. Thus,theregularizationoftheoff-diagonalcovariancetermsagaincorresponds
simplytoweightdecay,whereasthediagonalelementsofbothcovariancematriceshaveregularizers
oftheform
(cid:88)
(aexp(2p )−2bp ). (56)
k k
k
Notethatthisfunctionisconvex. Thisfunction(insidethesummation)isvisualizedforvaryinga
(comparedtoweightdecay)inFigure3. Ourregularizationtermsprovidesubstantiallymorecontrol
overtheminimizingvalue,andthusmorecontroloverpredictivevariance. However,comparedto
weightdecay,ourregularizersvaryinscalesubstantiallymorewhichmayleadtodifficultiestrading
offregularizationtermswithotherlossterms.
Tocounteractthisrelativelackofinterpretabilityofourhyperparameters,weproposeanalternate
representationofthesevalues. Werewritetheregularizationfunctionas
(cid:88) b
a (exp(2p )−2 p ). (57)
k a k
k
whereacorrespondstoascaleterm,andb/acontrolsthelocationoftheminimum. Wemayspecify
adesiredpredictivevariance,whichcanbemappedtotheminimumoftheregularizationfunction.
Concretely,givensometargetvarianceelementsˆ=exp(2p )(forallk),wechoose
k
b=sˆa (58)
whichassuresthattheminimumof(56)isachievedwhenp = 1logsˆforallk. Sweepsoverthe
k 2
hyperparameters(a,sˆ)arepresentedinFigure4.
Given this transformation between hyperparameters, we can now be concrete in how to specify
thesealternatehyperparametersinVBLLmodels. Theoriginalhyperparametersforthemodel,as
describedearlierinthissection,arethepriorlastlayercovariancescales,thescalematrixforthe
noisecovariancepriorm,thedegreesoffreedomν˜. Additionally,itiscommonisBayesiandeep
learningtoscaledowntheKLpenalty,andwewritethisfactorasλ. Ouralternatehyperparameters
are target (diagonal) values,ˆl > 0 and pˆ > 0, and scale parameters α > 0 and α > 0. The
Σ S
24
noitaziralugeR noitaziralugeRPublishedasaconferencepaperatICLR2024
12 12
10 10
8 8
6 6
4 4
2 2
0 0
2 2
0.0 0.5 1.0 1.5 2.0 2.5 3.0 0.0 0.5 1.0 1.5 2.0 2.5 3.0
exp(2pk) exp(2pk)
Figure4: Sweepingoverourmodifiedhyperparameterrepresentation. Left: sweepingoverdesired
predictivevariancesˆ,witha=100. Right: sweepingoverregularizationscaleawithfixeddesired
predictivevariancesˆ=1. Notethatallfunctionsasymptoteatexp(2p )=0. Inthesefigures,the
k
curveshavebeenverticallyshiftedtoachieveaminimumatzero;thisverticalshiftdoesnotimpact
regularization.
mappingbetweenthesehyperparametersis:
s←pˆ m←α (59)
Σ
pˆTα
λ← S ν˜←ˆlα . (60)
N Σ
y
Ifλ=1asis(perhapsnaively)theoreticallyjustifiedinvariationalinference,thenα iscorrespond-
S
inglyfixed.
D EXPERIMENTAL DETAILS
This section contains details about the experiments in the body of the paper. We note that for
highlightinginthetablesinthepaperbody,ifasingle-passmethod(intheupperhalfofeachtable)
isthebestperforminginametric, thatresultishighlighted. Ifthebestperformingismulti-pass,
wehighlightboththebestmulti-passandsingle-passmethodinthecolumn. Webelievethatthis
is important, as many applications required single-pass methods and thus multi-pass results are
irrelevant.
D.1 METRICS
For regression experiments, we report the predictive negative log likelihood (NLL) of test data,
whichcanbecomputedinclosedformforpointfeatureestimates. Wealsoreporttherootmean
squarederror(RMSE),astandardmetricforregression. Forclassification,inadditiontothenegative
log likelihood, we also report predictive accuracy (based on standard argmax of the predictive
distribution), and expected calibration error (ECE), which measures how the model’s subjective
predictiveuncertaintyagreeswithpredictiveerror. Finally,wealsoinvestigateoutofdistribution
detectionperformance,astandardevaluationschemeforrobustandprobabilisticmachinelearning
(Liuetal.,2022). WecomputetheareaundertheROCcurve(AUC)fornear-OODandfar-OOD
datasets,whichisdiscussedinmoredetaillaterinthissection.
D.2 BASELINES
Wedistinguishbaselinesbetweensingle-passandmulti-passmodels,whichweshowinupperand
lowersegmentsofeachtable,respectively. Single-passmethodsrequireonlyasinglenetworkevalua-
tion,andwecompareVBLLswithMAPfeatureestimationtothesemodels. Multi-passmethods
requireseveralnetworkevaluations,andincludesvariationalmethodslikeBayes-by-backprop(which
werefertoasBBB)(Blundelletal.,2015),ensembles(Lakshminarayananetal.,2017),Bayesian
dropout(Gal&Ghahramani,2016)andstochasticweightaveraging-Gaussian(SWAG)(Maddox
etal.,2019).
25
noitaziralugeR noitaziralugeRPublishedasaconferencepaperatICLR2024
Withinregression,wecomparetomodelswhichexploitexactconjugacy,includingBayesianlast
layermodels(GBLLandLDGBLL(Watsonetal.,2021))andRBFkernelGaussianprocesses. We
notethatthesemethodsrequirecomputingfullmarginallikelihoodandarethusdifficulttoscale
to large training sets. We also compare to MAP learning, in which a full network is trained via
MAPestimation,andaBayesianlastlayerisfittothesefixedfeatures(Snoeketal.,2015). Within
classification,weprimarilycomparetostandardnetworks(DNN),astheseoutputadistributionover
labelsandthuscanbedirectlycomparedtoourmodel. WealsocomparetoSNGP(Liuetal.,2022)
andlastlayerLaplace-basedmethods(Daxbergeretal.,2021a),whicharesimilarlastlayermodels.
SNGPaimstoapproximatedeepkernelGPs(Wilsonetal.,2016b),andLaplacemethodscomputea
lastlayerapproximateposterioraftertraining. WenotethatincontrasttoSNGP(Liuetal.,2022),
wedonotmodifyastandardneuralnetworkbackbone, suchasincludingspectralnormalization,
addingresidualconnections,orusingsinusoidalnonlinearities. BothSNGPandlastlayerLaplace
methodsrequireapassoverthefulldatasettofitthelastlayerdistribution;incontrast,ourmethod
maintainsalastlayerdistributionduringtraining,whichmaybeusefulfore.g.activelearning. We
donotevaluateLaplacemethodsinregressionastheyarenearlyidenticaltotheMAPmodel.
D.3 TOYEXPERIMENTS
Figure1containssimplevisualizationsfortheregressionmodelandthegenerativeVBLLmodel. In
particular,theregressionmodelshowspredictionswithvariationalfeaturelearning(withKLweight
of1.0)onacubicfunctionwithagapinthedata. Thisdatasetconsistedof100pointssampledin
[−4,−2]∪[2,4],withanoisestandarddeviationof0.1. Themodelconsistedofatwohidden-layer
MLPofwidth128,trainedfor1000epochswithabatchsizeof32,withstochasticgradientdescent
withmomentum,withalearningrateof3·10−4,zeroweightdecay,andmomentumbetaparameters
of0.9. Thesevalueswerearbitrarilychosen,althoughthechoiceofSGDMversusAdam(Kingma&
Ba,2015)doesmakeadifferenceonpredictionfarfromthedata. Gradientclippingwithamaximum
magnitudeof2.0wasused. TheDOFandscaleparameterswerebothsetto1.0
Fortheclassificationproblem,weusedthescikit-learn(Pedregosaetal.,2011)implementationof
thehalfmoondataset, with1000datapointsandanoisestandarddeviationof0.2. Wetraineda
G-VBLLmodelwithresidual-structuredMLPofwidth128(eachhiddenlayerisaddedtothelayer
input). ThismodelwastrainedwithSGDMwithlearningrate3·10−2,momentumbeta0.9,and
weightdecay10−4,for100epochsandwithabatchsizeof32. TheDOFparameterwas128,and
thescaleparameterwas1.0.
D.4 REGRESSION
OurUCIexperimentscloselyfollowWatsonetal.(2021),andwecomparedirectlytotheirbaselines.
ForVBLLs,weusedaN(0,I)lastlayermeanpriorandaW−1(1,1)noisecovarianceprior. For
allexperiments,weusethesameMLPusedinWatsonetal.(2021)consistingoftwolayersof50
hiddenunitseach(notcountingthelastlayer). ForalldatasetswematchedWatsonetal.(2021)
andusedabatchsizeof32,otherthanthe POWER datasetforwhichweusedabatchsizeof256
to accelerate training. For all datasets we normalize inputs (using the training set statistics) and
subtractthetrainingsetmeansfortheoutputs. Wedidnotre-scaletheoutputmagnitudes,toretain
comparabilityofNLLs. WenotethattheextenttowhichoutputswerenormalizedinWatsonetal.
(2021)isunclear. However, theymaketheparametersoftheirpriorlearnable, whichcanhavea
similareffecttocenteringtheoutputs, andsowebelieveouroutputcenteringisreasonable. All
resultsshowninthebodyofthepaperareforleakyReLUactivations. Forallexperiments,afixed
learningrateof0.001wasusedwiththeAdamWoptimizer(Loshchilov&Hutter,2017). Adefault
weightdecayof0.01wasusedforallexperiments. Weclippedgradientswithamaxmagnitudeof
1.0.
Foralldeterministicfeatureexperiments, weran20seeds. Foreachseed, wesplitthedatainto
train/val/test sets (0.72/0.18/0.1 of the data respectively). We train on the train set and monitor
performanceonthevalidationsettochooseatotalnumberofepochs. IncontrasttoWatsonetal.
(2021)whocomputevalidationperformanceforeveryepoch,wecomputevalidationperformance
(predictiveNLL)every10epochs(notethatthedatasetsaresmallandtypicallytrainforhundredof
epochs). Afterchoosinganumberofepochs,wetrainonthecombinedtrainingandvalidationset
26PublishedasaconferencepaperatICLR2024
Features BOSTON CONCRETE ENERGY POWER WINE YACHT
MAP 3000 3000 2000 3000 1000 2000
Variational 10000 10000 10000 10000 10000 10000
Table8: MaximumnumberofepochsforeachsetoffeaturesandeachUCIdataset.
andevaluateperformanceonthetestset. WeuseamaxnumberofepochsshowninTable8,which
werelargeenoughtonotbereachedbutoftenlowerthanthoseusedinWatsonetal.(2021).
For our BBB feature models, we ran 10 seeds with a similar procedure to the above. We follow
√
Watson et al. (2021) and use a N(0,4/ n ) for each weight (where n denotes the layer input
in in
width). Validationperformancewasmonitoredevery100epochs,and10weightsampleswereused
tocomputethevalidationpredictivelikelihoodandchooseafulltrainingnumberofepochs.
D.5 IMAGECLASSIFICATION
All classification experiments utilize the Wide ResNet-28-10 (WRN-28-10) backbone network
architecture. HyperparametersaresimilartothoseproposedbyZagoruyko&Komodakis(2016).
UnliketheoriginalimplementationofWRN,wedonotemployNesterovmomentumandwefully
decayaninitiallearningrateof0.1accordingtoaCosineAnnealingscheduleinsteadofastepped
decayschedule. Gradientsareclippedwithamaximummagnitudeof2.0andweimposealastlayer
KLweightof1.0. WeAllclassificationresultsarereportedacross3seedsandusethestandardWRN
data-augmentationsproposedby(Zagoruyko&Komodakis,2016). Forthedeterministicfeature
experiments,wetraineachmodelfor300epochs.
TheBBBbackbone-basedmodelsutilizethesameWRNarchitectureandareprimarilydeterministic.
TheBBBmodelsimplementasinglefinalBayesianlinearlayerwithapriordistributionofN(0,0.01).
EachBBB-basedmodelused10weightsamplesfortestsetevaluation. Thisoperationisrelatively
cheapwhencomparedtoafullystochasticnetworkbecausetheintermediatefeaturesarecachedprior
tothefinalBayesianlinearlayerweightsamplingandcomputation. AllBBBaretrainedfor400
epochsandweimposealastlayerKLweightof1.0andafeatureKLweightof0.5theVBLL-BBB
andDBLL-BBBmodels. TheBBBbaselinemodelutilizedafeatureKLweightof50.
D.6 SENTIMENTCLASSIFICATIONWITHLLMFEATURES
We perform sentiment classification experiments utilizing features extracted from a pre-trained
OPT-175B(Zhangetal.,2022)modelontheIMDBSentimentClassificationdataset(Maasetal.,
2011). WecompareourG-VBLLandD-VBLLmodelswithanMLPbaseline. TheIMDBdataset
isatext-basedbinaryclassificationtaskinwhichinputsarepolarizedmoviereviewsandoutputs
arepositiveandnegativelabels. TextembeddingsareextractedfromtheOPT-175Bmodelforeach
sampleastheoutputofthelastmodellayerforthefinaltokeninthetextsequence. Thisresults
inasequenceembedding,e = R12288,foreachsample. Inallcases,weutilizetwolinearlayers
priortotheclassificationhead. Tounderstandtheimpactoftrainingdatasetsizeonperformance,all
experimentsareperformedatmultipletrainingdatasetscales. TheIMDBdatasetissamplediid. to
constructtrainingdatasetswith10,100,1000samplesalongsidethestandard25,000sampletraining
split. Wetrainmodelsatalldatasetscalesandreportacross3seeds. TheAdamWoptimizerisused
forallmodels. Hyperparameterssuchaslearningrate,weightdecayweretunedacrossboththe10
sampleandfulldatasetscales.
D.7 WHEELBANDIT
WematchtheexperimentalsettingsofRiquelmeetal.(2018). Inparticular,weuseabatchsizeof
512,alearningrateof3e−3,andtrainfor80000stepstotal. Weperform20stepsintheenvironment
perphaseofupdating,andperform100gradientstepswhenupdating. Weuseagradientclipping
normof1.0. Weusethesamenetworkarchitectureasbaselines,anMLPwithwidths(100,100,5),
wherethelastlayerisaVBLL.TheVBLLhyperparametersweresettodefaults: thedegreesof
freedomandthescaleintheWishartprioraresetto1,andthepriorscalewasalsosetto1.
27PublishedasaconferencepaperatICLR2024
0.01 0.01
103 0.1 103 0.1
1.0 1.0
102 10.0 102 10.0
101 101
100 100
10 1 10 1
10 2 10 2
0 250 500 750 1000 1250 1500 1750 2000 0 200 400 600 800 1000
Epoch Epoch
Figure5: SweepingovertheΣlocationparameterforUCIdatasetsEnergy(left)andWine(right).
ThedottedcoloredlinescorrespondtoΣ−1valuesoverthecourseoftraining,andsolidcoloredlines
correspondtotheFrobeniusnormofS. TheblackdottedlinescorrespondtotargetΣ−1values. The
scalehyparparameterwaslargeintheseexperimentstoillustratetheabilitytoeffectivelycontrolnoise
covariance. NotethatforverysmallΣ−1,theimpcatofthepredictivelosslimitsthedegreetowhich
realizednoisecovariancematchesthegoalvalue;thistrade-offiscontrolledbyscaleparameters.
101
101
0.01 100
100 0.1
1.0
10.0
10 1 10 1
0.01
0.1
1.0
10 2 10 2 10.0
0 250 500 750 1000 1250 1500 1750 2000 0 200 400 600 800 1000
Epoch Epoch
Figure6: SweepingovertheS locationparameterforUCIdatasetsEnergy(left)andWine(right).
Again, dotted colored lines correspond to Σ−1 values over the course of training, solid colored
linescorrespondtotheFrobeniusnormofS,andblackdottedlinescorrespondtotargetdiagonalS
values. NotethattheFrobeniusnormofS inallcasesishigherthanthetargetduetotheoff-diagonal
elements,buttherealizedcovariancecanbewellcontrolled.
E HYPERPARAMETER STUDIES AND ABLATIONS
KLweight. WeadditionallyexploretheNLLsensitivityoftheDBLLandDBLL-BBBmodelsto
variousKLweightingconfigurations. InTable9,wesweepacrossordersofmagnitudeforboththe
lastlayerandfeatureKLweightingparameters.
Locationandscalehyperparameters. WeinvestigateourhyperparameterreformulationonUCI
datasetsinFigures5–7. Inparticular,wevaryeachofthelocationandscaleparametersandshow
thatwecaneffectivelycontrolthequantitiesofinterest. Inparticular,Figures5and6showvarying
the location hyperparameter for each covariance matrix Σ,S with a high scale hyperparameter,
enabling fine-grained control over realized values. In practice, this degree of direct control over
realizedmodelvaluesisnotdesirable,andtheseplotsonlyillustratethatsuchadegreeofcontrol
ispossible. InFigure7,wevarythescaleparameterforΣandshowthatiteffectivelycontrolsthe
strengthwithwhichΣisregularized. Withnaivehyperparameterselection,interactionbetweenscale
28
mroN
ecnairavoC
mroN
ecnairavoC
mroN
ecnairavoC
mroN
ecnairavoCPublishedasaconferencepaperatICLR2024
1 1
103 10 103 10
100 100
1000 1000
102 102
101
101
100
100
10 1
10 1
0 250 500 750 1000 1250 1500 1750 2000 0 200 400 600 800 1000
Epoch Epoch
Figure 7: Sweeping over the Σ scale parameter for UCI datasets Energy (left) and Wine (right).
Again,dottedcoloredlinescorrespondtoΣ−1values,solidcoloredlinescorrespondtotheFrobenius
normofS,andtheblackdottedlinecorrespondstoalocationhyperparameterΣ−1valueof1. Note
thatbyvaryingthescalehyperparameter,thestrengthoftheregularizationisvariedwithoutchanging
thetargetvalue,whichisaresultofourhyperparameterreformulation.
FeatureKLWeight
LLKLWeight MAP 50 5 0.5
1.0 0.160 0.266 0.281 0.282
0.1 0.162 0.266 0.286 0.272
0.01 0.168 0.268 0.268 0.280
0.001 0.160 0.267 0.272 0.276
Table9: CIFAR-10NLLforvaryingvaluesofKLweights,forboththelastlayerandthefeature
weightinginvariationalfeaturelearning.
andlocationparameterswouldrequirecarefulplanningtocontrolregularizationscaleindependently
oflocation,whereasourreformulationenablesdirectcontrolofscale.
F PROOFS AND FURTHER THEORETICAL RESULTS
F.1 HELPERRESULTS
Our first result builds on results from the variational Gaussian process literature (Titsias, 2009;
Hensmanetal.,2013).
Lemma 4. Let q(µ) = N(µ¯,S) and p(y | X,µ) = N(Xµ,Σ) with y ∈ RN,µ¯,µ ∈ RM,
X ∈RN×M,andS,Σ∈RM×M. Then
1
E [logp(y |X,µ)]=logp(y |X,µ¯)− tr(Σ−1XSX⊤). (61)
q(µ) 2
Proof. Wehave
1
E [logp(y |Xµ)]=− E [logdet(2πΣ)+(y−Xµ)⊤Σ−1(y−Xµ)] (62)
q(µ) 2 q(µ)
=−1(cid:0)
logdet(2πΣ)+E
[(y−Xµ)⊤Σ−1(y−Xµ)](cid:1)
(63)
2 q(µ)
=−1(cid:0) logdet(2πΣ)+(y−Xµ¯)⊤Σ−1(y−Xµ¯)+tr(Σ−1XSX⊤)(cid:1)
2
(64)
wherethelastlinefollowsfromthefactthaty−Xµ∼N(y−Xµ¯,XSX⊤). Thefirsttwoterms
formthedesiredlogdensity.
29
mroN
ecnairavoC
mroN
ecnairavoCPublishedasaconferencepaperatICLR2024
Basedonthisresult,wecanstateastraightforwardcorollaryforgenerativeclassification.
Corollary1. Letq(µ) = N(µ¯,S)andp(y | µ) = N(µ,Σ)withy,µ¯,µ ∈ RN,S,Σ ∈ RN×N.
Then
1
E [logp(y |µ)]=logp(y |µ¯)− tr(Σ−1S). (65)
q(µ) 2
Proof. ThisresultfollowsfromLemma4bysimplychoosingX =I.
Wecanalsopresentavariantformultivariateclassification.
Corollary2. Letq(W) = MN(W¯,I,S)andp(y | x,W) = N(Wx,Σ)withy ∈ RM,W¯,W ∈
RM×N;x∈RN;S ∈RN×N;andΣ∈RM×M. Then
1
E [logp(y |x,W)]=logp(y |x,W¯)− x⊤Sxtr(Σ−1). (66)
q(W) 2
Proof. OurproofcloselyfollowsthatofLemma4. Expandingthelikelihoodintheexpectation,we
have
1
E [logp(y |x,W)]=logp(y |x,W¯)− E [x⊤(W −W¯)⊤Σ−1(W −W¯)x] (67)
q(W) 2 W
Leveragingthematrixnormalidentity
E W∼MN(W¯,V,U)[W⊤AW]=Utr(A⊤V)+W¯⊤AW¯ (68)
andthefactthatW −W¯ ∼MN(0,I,S),wehave
E[(W −W¯)⊤Σ−1(W −W¯)]=Str(Σ−1) (69)
whichcompletestheproof.
Lemma5. Letp(x|µ)=N(µ,Σ),andletµ∼N(µ¯,S). Then,
E [p(x|µ)]=N(µ¯,Σ+S). (70)
µ
Proof. WebuilduponJacobson(1973)andnote
(cid:115)
1 det(S−1) 1
E [exp(− x⊤Σ−1x)]= exp(− µ¯⊤S−1(S−(Σ−1+S−1)−1)S−1µ¯).
x∼N(µ¯,S) 2 det(S−1+Σ−1) 2
(71)
Note,byWoodbury’sidentity
S−1(S−(Σ−1+S−1)−1)S−1 =(S+Σ)−1 (72)
Letz :=x−µ,thenz ∼N(x−µ¯,S). Wethenhave
1 1
E[p(x|µ)]=E[exp(− ∥x−µ∥2 + logdet(2πΣ−1))] (73)
2 Σ−1 2
1 1
=E[exp(− z⊤Σ−1z)]]exp( logdet(2πΣ−1)) (74)
2 2
Fortheexpectationweapply(71). Wesimplifythedeterminanttermof(71)as
(cid:115)
det(S−1) 1
=exp(− logdet(I+SΣ−1)) (75)
det(S−1+Σ−1) 2
Combining,wehave
1 1
E[exp(− z⊤Σ−1z)]]=exp(− (∥x−µ¯∥2 +logdet(I+SΣ−1)) (76)
2 2 (S+Σ)−1
Wehavetwologdeterminantterms,from(74)andtheabove. Wecancombinethemas
1 1 1 1
logdet(2πΣ−1)− logdet(I+SΣ−1)=− (logdet( Σ)+logdet(I+SΣ−1)) (77)
2 2 2 2π
1 1
=− logdet(( Σ)(I+SΣ−1)) (78)
2 2π
1 1 1
=− logdet( Σ+ S) (79)
2 2π 2π
Combiningalltermscompletestheproof.
30PublishedasaconferencepaperatICLR2024
F.2 PROOFOFTHEOREM1
Theorem1. Letq(ξ |η)=N(w¯,S)denotethevariationalposteriorfortheBLLmodeldefinedin
Section2.1. Then,(12)holdswith
T (cid:18) (cid:19)
1 (cid:88) 1
L(θ,η,Σ)= logN(y |w¯⊤ϕ ,Σ)− ϕ⊤Sϕ Σ−1 . (80)
T t t 2 t t
t=1
Proof. First,
logp(Y |X,θ)=logE [p(Y |X,ξ,θ)] (81)
p(ξ)
p(ξ)
=logE [p(Y |X,ξ,θ) ] (82)
q(ξ|η) q(ξ |η)
≥E [logp(Y |X,ξ,θ)]−KL(q(ξ |η)||p(ξ)) (83)
q(ξ|η)
T
(cid:88)
= E [logp(y |x ,ξ,θ)]−KL(q(ξ |η)||p(ξ)). (84)
q(ξ|η) t t
t=1
NotethatthefirstterminthelastlineisthelogofaNormaldistribution. ApplyingLemma1,we
have
1
E [logp(y |x ,ξ,θ)]=logp(y |x ,ξ,θ)− ϕ⊤Sϕ Σ−1 (85)
q(ξ|η) t t t t 2 t t
whichcompletestheproof.
Wecanalsostatethefollowingcorollaryforthemultivariatecase.
Corollary3. Letq(ξ | η) = MN(W¯,I,S)denotethevariationalposteriorforthemultivariate
BLLmodeldefinedinAppendixA.Then,(12)holdswith
T (cid:18) (cid:19)
L(θ,η,Σ)= 1 (cid:88) logN(y |W¯ϕ ,Σ)− 1 ϕ⊤Sϕ tr(Σ−1) . (86)
T t t 2 t t
t=1
Proof. TheprooffollowstheproofofTheorem1,applyingCorollary2insteadofLemma1.
F.3 PROOFOFTHEOREM2
Theorem2. Letq(W
|η)=(cid:81)Ny
N(w¯ ,S )denotethevariationalposteriorforthediscrimina-
k=1 k k
tiveclassificationmodeldefinedinSection2.2. Then,(12)holdswith
T (cid:18) (cid:20) (cid:21)(cid:19)
L(θ,η,Σ)= 1 (cid:88) y⊤W¯ϕ −LSE w¯⊤ϕ + 1 (ϕ⊤S ϕ +σ2) (87)
T t t k k t 2 t k t k
t=1
Proof. WeconstructanELBOvia
logp(Y |X,θ)=logE [p(Y |X,θ,ξ)] (88)
p(ξ)
≥E [logp(Y |X,θ,ξ)]−KL(q(ξ |η)||p(ξ)) (89)
q(ξ|η)
T
(cid:88)
= E [y⊤logsoftmax (logp(x ,y |θ,ξ))]−KL(q(ξ |η)||p(ξ)) (90)
q(ξ|η) t y t
t=1
Expandingthelog-softmaxterm,wehave
E (cid:2) y⊤logsoftmax (logp(x ,y |θ,ξ))(cid:3) = (91)
q(ξ|η) t y t
E [y⊤logp(x ,y |θ,ξ))]−E [LSE [logp(x ,y |θ,ξ)].
q(ξ|η) t t q(ξ|η) y t
Aspreviously,underthevariationalposteriortheselikelihoodsfactorizeacrossthedata. Thefirst
termmaybedirectlyevaluated,yielding
E [logp(x ,y |θ,ξ))]=E [w⊤]ϕ=w¯⊤ϕ. (92)
q(ξ|η) t q(ξ|η) y y
31PublishedasaconferencepaperatICLR2024
Thesecondterm(containingthelog-sum-exp)cannotbecomputedexactly,andsowewillbound
thistermforboththediscriminativeandgenerativeclassifiers. ViaJensen’sinequality,wehave
(cid:88)
−E [LSE [logp(x ,y |θ,ξ)]≥−log E [exp(logp(x ,y |θ,ξ))] (93)
q(ξ|η) y t q(ξ|η) t
y
Inthecaseofthediscriminativemodel,wefollowBlei&Lafferty(2007)andnotethatforeachrow
k
1
E [exp(w⊤ϕ +ε )]=exp(w¯⊤ϕ + (ϕ⊤S ϕ +σ2)) (94)
wk∼N(w¯k,Sk) k t k k t 2 t k t k
which relies on assumed independence of rows of W (although relaxation of this assumption is
possible). CombiningtheseresultsyieldsalowerboundontheELBO,whichisitselfalowerbound
onthemarginallikelihood.
F.4 PROOFOFTHEOREM3
Theorem 3. Let q(µ | η) =
(cid:81)Ny
N(µ¯ ,S ) denote the variational posterior over class em-
k=1 k k
beddings for the generative classification model defined in Section 2.3. Let p(ρ | Y) = Dir(α)
denotetheexactDirichletposterioroverclassprobabilities,withαdenotingtheDirichletposterior
concentrationparameters. Then,(12)holdswith
T
1 (cid:88) 1
L(θ,η,Σ)= (logN(ϕ |µ¯ ,Σ)− tr(Σ−1S )+ψ(α )−ψ(α )+logα (95)
T t yt 2 yt yt ∗ ∗
t=1
−LSE [logN(ϕ |µ¯ ,Σ+S )+logα ])
k t k k k
(cid:80)
whereψ(·)isthedigammafunctionandwhereα = α .
∗ k k
Proof. Notethat
logp(Y |X,θ)≥E [logp(Y |X,ξ,θ)]−KL(q(ξ |η)||p(ξ)) (96)
q(ξ|η)
where
E [logp(Y |X,ξ,θ)]=E [logp(X |Y,θ,ξ)−logp(X |θ,ξ)]+E [logp(Y |ξ)]
q(ξ|η) q(ξ|η) q(ξ|η)
(97)
Allofthesetermsfactorizeoverthedata,aspreviously. Wefirstnotethatforthelastterm,
(cid:88)
E [logp(y |θ,ρ)]=ψ(α )−ψ( α ) (98)
ρ t yt y
y
whereαcorrespondtoposteriorDirichletconcentrationparametersandψ(·)denotesthedigamma
function. Thefirsttermin(97)istheembeddinglikelihood;wecancomputethisexpectationofthe
loglikelihoodviaCorollary1.
Thesecondtermin(97)islessstraight-forward. Notethat
(cid:88)
E[logp(x |θ,ξ)]=E[log p(x |y,θ,ξ)p(y |θ,ξ)] (99)
t t
y
which can be written as a log-sum-exp of log joint likelihood. We will again apply Jensen’s to
exchangethelogandsum,andnote
(cid:88)
−E[logp(x |θ,ξ)]=−E[log p(x |y,θ,ξ)p(y |θ,ξ)] (100)
t t
y
(cid:88)
≥−logE[ p(x |y,θ,ξ)p(y |θ,ξ)] (101)
t
y
(cid:88)
=−log E [p(x |µ ,θ)]E [p(y |ρ)] (102)
µy t y ρ
y
=−LSE [logE [p(y |ρ)]+logE [p(x |µ ,θ)]] (103)
y ρ µy t y
32PublishedasaconferencepaperatICLR2024
where the second line follows from Jensen’s, and the third line follows from the structure of the
variationalposterior. Wemayapply
(cid:88)
logE [p(y |θ,ρ)]=logα −log α , (104)
ρ t yt y
y
astandardresultfromDirichlet-Categoricalmarginalization. Thesecondtermin(104)(thesum
over concentration parameters) is equivalent for all classes y, and thus can be pulled out of the
log-sum-exp(duetotheequivarianceofthisfunctionundershifts)whereitcancelsthesamethird
termin(97).
Tocomputethesecondexpectationin(103),weapplyLemma5. Combiningalltermscompletesthe
proof.
33