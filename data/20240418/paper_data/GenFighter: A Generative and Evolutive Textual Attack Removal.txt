GenFighter: A Generative and Evolutive Textual
Attack Removal
Md Athikul Islam, Edoardo Serra
Boise State University, Boise, Idaho, USA
{mdathikulislam,edoardoserra}@boisestate.edu
Sushil Jajodia
George Mason University, Fairfax, Virginia, USA
jajodia@gmu.edu
April 18, 2024
Abstract
Adversarial attacks pose significant challenges to deep neural net-
works (DNNs) such as Transformer models in natural language process-
ing (NLP). This paper introduces a novel defense strategy, called Gen-
Fighter, which enhances adversarial robustness by learning and reason-
ing on the training classification distribution. GenFighter identifies po-
tentially malicious instances deviating from the distribution, transforms
them into semantically equivalent instances aligned with the training
data,andemploysensembletechniquesforaunifiedandrobustresponse.
By conducting extensive experiments, we show that GenFighter outper-
forms state-of-the-art defenses in accuracy under attack and attack suc-
cess rate metrics. Additionally, it requires a high number of queries per
attack, making the attack more challenging in real scenarios. The abla-
tionstudyshowsthatourapproachintegratestransferlearning,agenera-
tive/evolutiveprocedure,andanensemblemethod,providinganeffective
defense against NLP adversarial attacks.
Keywords: Adversarial Attacks, Natural Language Processing, Deep Neural
Networks
1 Introduction
The advent of deep neural networks (DNNs) has achieved significant success in
naturallanguageprocessing(NLP)tasks,nevertheless,theyarepronetoadver-
sarial attacks designed to exploit their inherent vulnerabilities (Alzantot et al.,
2018; Jin et al., 2020; Zeng et al., 2021a). These attacks involve manipulating
the input text with the goal of preserving its semantic meaning while deceiving
1
4202
rpA
71
]GL.sc[
1v83511.4042:viXrathe target model into generating incorrect outputs. In the context of NLP clas-
sification models, such attacks alter the input text to a semantically equivalent
version, thereby influencing the model’s classification output to differ from the
original input text’s classification.
Adversarialattacksofteninvolvemanipulationsatthecharacter(Gaoetal.,
2018;Lietal.,2018),word(Ebrahimietal.,2018;Renetal.,2019),orsentence
(Ribeiro et al., 2018; Maheshwary et al., 2021) level. As already pointed out by
(Wang et al., 2023), character-level and sentence-level attacks are less effective
than word-substitution attacks. Therefore, we consider only word-substitution
attacks.
To defend against word-substitution adversarial attacks, an intuitive ap-
proach is to generate adversarial examples through synonym substitutions and
training the target model with perturbed instances (Ren et al., 2019; Jin et al.,
2020; Li et al., 2020). The impracticality of these methods arises from the ex-
ponentialincreaseinthenumberofpotentialperturbations. Furthermore,more
robust adversarial attacks, deviating from these adversarial training strategies,
pose significant challenges to these training-based defense methods (Si et al.,
2021). Some recent works utilize regularization techniques (Ishida et al., 2020;
Liu et al., 2022), or ensemble or randomization methods (Zeng et al., 2021b;
Wang et al., 2023) to improve the model’s robustness. However, their experi-
mentalresultsintermsofaccuracyunderattackandattacksuccessratemetrics
show room for improvement.
In this work, we propose an innovative approach, called GenFighter (see
Figure 1), to extensively enhance adversarial robustness against strong textual
attack strategies. Our method operates under the assumption that a successful
adversarial attack occurs when the resulting instance lies outside the estab-
lished training distribution. While it is possible that attacks aligned with the
training distribution exist, our experiments demonstrate that this assumption
leads to improvement beyond the current state-of-the-art. Consequently, we
designGenFightertolearnthetrainingdatadistributionandidentifyinstances
that deviate from this distribution as potentially malicious. It then searches,
through an evolutive process, for semantically equivalent instances that better
align with the established distribution. Ultimately, GenFighter ensembles the
classifications of all generated semantically equivalent instances to provide a
robust and unified answer.
In summary, our contributions are as follows:
• We design GenFighter, a novel defense strategy that tackles adversarial
attacks in NLP tasks. The distinctive feature of GenFighter lies in its
ability to learn and reason on the training classification distribution. It
transforms an anomalous instance with respect to the training classifica-
tion distribution into several semantically equivalent ones more aligned
with the training distribution and combines their classification results for
a uniquely robust classification.
• Our experiments show that GenFighter often exhibits superior accuracy
2under attack and a lower attack success rate with respect to the state-of-
the-art defensive models for RoBERTa and BERT as victim models, and
the three most popular attacks: PWWS, TextFooler, and BERT-Attack.
Additionally, even the number of queries required to conduct the attack
is among the largest compared to the state-of-the-art, implying a further
degree ofdifficulty in conducting attacks on our defensive model. We also
demonstrate that our approach outperforms the state-of-the-art in terms
of the transferability of attacks, i.e., when attacks are generated against
different victim models.
• The ablation study shows that each sub-component of GenFighter is cru-
cial to achieving its high performance. Our approach represents an effec-
tive synergistic integration of transfer learning, generative/evolutive pro-
cedures, and ensemble modes in the context of a defensive strategy for
NLP adversarial attacks.
2 Related Work
In NLP tasks, modern adversarial attack methods employ word substitution
strategiestogenerateadversarialexamples(GargandRamakrishnan,2020;Ren
et al., 2019; Jin et al., 2020; Li et al., 2020). These attack models identify vul-
nerable words by analyzing the logit output of the victim model and substitute
words using synonyms, similar embedding vectors, or transformer models.
Some defense methods are designed based on insights gained from under-
standing attack methods and others work independent of attack strategies.
Empirical defense methods such as adversarial data augmentation (ADA) (Ren
et al., 2019; Jin et al., 2020) augment training data with adversaries using their
respectiveattackmethodstotrainthevictimmodel. Inaddition,gradient-based
adversarial training is another type of empirical defense method that involves
adding adversarial perturbations to the input’s word embeddings during the
training phase (Zhu et al., 2020; Li et al., 2021). Unfortunately, empirical de-
fense methods are highly vulnerable to robust adversarial attacks (Li et al.,
2020). In contrast, certified defense methods (Huang et al., 2019; Jia et al.,
2019) ensure robustness by establishing a certified space within the range of
synonyms employed by adversaries. An example of a certified defense is the
generation of the convex hull spanned by the word embeddings of a word and
itssynonyms(Zhouetal.,2021). Amajordrawbackofcertifieddefensemethods
is that defenders must know beforehand how an attacker substitutes synonyms.
Randomized masking of words, combined with voting ensemble strategies
operates without requiring knowledge of how adversaries generate synonyms.
Unfortunately, these methods exhibit unsatisfactory results on larger datasets
(Zengetal.,2021b;Lietal.,2021). Additionally,aregularizationtechniquepre-
ventingfurtherreductionofthetraininglossonlyworksondefendingoverfitted
models trained on smaller datasets (Liu et al., 2022).
The robustness of GenFighter stems from its ability to work on the training
3Evolutionary Search Procedure
Paraphraser Module p Anomaly Detection Model m
Possible Word Model Model
Conditional Text 1 (CT lura si tn ei rn ing g) E (v Sa cl ou ra inti go )n f
TIn exp tu t x G (Te M en e .goe x .r dt a u Tet a 5li lv )e Poss Tib exle t W 2ord Px Para Om feters sorteT de (A (x Pr te s x ,NT So o xp r )m- , K ka el y? =Sx) YES Top-K f + ClaR sso ifb liu cs at tion
Possible Word GMM [:K] >= τ Texts K
Text n Sx (Anomaly Scores) Sca on rd
es
Top-K Texts NO f
Figure 1: An overview of GenFighter. The paraphraser (conditional generative
textualmodel)moduleprocessesinputtextsandforwardsinputtextsandtheir
paraphrases P to the anomaly detection model. The anomaly detection model
x
assigns normality scores S and selects top-K normal texts. These selected
x
candidate texts are either looped back into the paraphraser module or directed
to the target model contingent on meeting the threshold τ. Finally, the victim
model performs a weighted mean prediction on the top-K normal texts and
scores.
classification distribution, coupled with its adept use of randomization and en-
semble strategies, allowing it to operate on any datasets independently of the
attacker’s strategies.
3 Methodology
WeproposethemethodGenFighterthatgivenatargetmodelf,themodelto
defend, it generatively and evolutively removes attacks from text in NLP clas-
sification tasks. As described in Figure 1, GenFighter employs a paraphraser
module for text generation along with an anomaly detection module to charac-
terizethedistributionofthetrainingdata. Subsequently,ourmethodintegrates
these modules with an evolutionary search process, that generates paraphrased
(perturbed) semantically equivalent text candidates until they align with the
training classification distribution. Finally, during the inference phase, an en-
semble weighted mean classification is executed on the candidate texts using
the target model f. Algorithm 1 provides all the details of our approach, and
containsthreefunctions: (1)training,(2)evolutionarysearch,and(3)inference.
A detailed description of each function is provided in the following sections.
3.1 Training
Thetrainingphaseincludesgeneratingparaphrases,fine-tuningthetargetmodel,
trainingtheanomalydetectionmodel,anddefiningthresholdselection. Ourde-
fense mechanism first introduces a conditional generative textual model p, such
asaT5transformer(Raffeletal.,2019),withthetaskofgeneratingparaphrases
from input texts. Utilizing this paraphraser module, GenFighter augments the
training data X with their paraphrased instances P to fine-tune the target
4Algorithm 1 Training, evolution and inference procedure of GenFighter
Initialize: number of paraphrases Np, number of best candidates K, evolution
iteration R
1: function training(f,p,m,X)
Input: pre-trainedtargetmodelf,paraphrasermodelp,GMMmodelm,training
instances X
Output: fine-tuned target model f, trained GMM model m, threshold τ
2: Generate paraphrases P ←p(X,Np)
3: X ←X∪P
4: Fine-tune f on X
5: Compute features E for each instance of X from model f
6: Train GMM model m on features E
7: Compute log-likelihood normal scores S ←m(E)
8: Calculate threshold τ as α-percentile of S
9: return (f,m,τ)
10: end function
11: function evolution(f,p,m,τ,x)
Input: defined in inference function
Output: top-K texts and scores
12: for all round=1...R do
13: Generate paraphrases Px ←p(x,Np)
14: Px ←Px∪x
15: Compute features Ex for each instance of Px from model f
16: Normal scores Sx ←m(Ex)
17: PK,SK ←sorted((Px,Sx),key=Sx)[:K]
18: if SK ≥τ then
19: break
20: end if
21: x←PK
22: end for
23: return (PK,SK)
24: end function
25: function inference(f,p,m,τ,x)
Input: fine-tuned target model f, paraphraser model p, trained GMM model m,
threshold τ, input text x
Output: classification label l
26: PK,SK ←evolution(f,p,m,τ,x)
SK−SKmin
27: Weights WK ← (cid:80)KSKma Sx i− −S SK Km min
in
i=1SKmax−SKmin
28: Confidences CK ←f(PK)
29: l←argmax(WK.dot(CK))
30: return l
31: end function
model (Algorithm 1, lines 2-4). To enhance the diversity of paraphrases, it em-
5ploys stochastic sampling, drawing from the model’s probability distribution to
predict the next output token. This approach ensures the generation of varied
paraphrases for the same text across different instances. For each text input,
wedenotehyperparameterthenumberofparaphrasesN . GenFighteralsouses
p
this paraphraser model during the evolutionary search procedure. It trains an
anomaly detection model m specifically the Gaussian Mixture model (GMM)
to learn the distribution of the training data (Algorithm 1, lines 5-6). First,
it obtains feature representations E for training instances by extracting infor-
mation from the final layer of the target model f (which in our experiments
will be a transformer). Then, it determines the optimal number of clusters or
componentsN onthetrainingdatatoinitializeGMMusingtheBayesianInfor-
c
mation Criterion (BIC) (Schwarz, 1978) principle. Following the initialization
of the GMM with N components, it trains the model m (GMM) using the fea-
c
ture representation E. At this stage, the model m is prepared to compute the
log-likelihood scores S from E, indicating the proximity of a given sample to
the training data. Higher scores correspond to samples that align more closely
with the training classification distribution. After training the anomaly detec-
tion model, the algorithm computes a threshold τ to determine if a sample is
aligned with the training classification distribution or is an outlier. The thresh-
old τ is computed as the α-percentile of scores S. Samples with scores below
this threshold are identified as outliers within the training distribution. α is a
hyperparameter of the function training.
3.2 Evolutionary Search
Theevolutionarysearchprocedureperturbsoutlierexamples,bygraduallyalign-
ingthemwiththetrainingclassificationdistributionduringtheinferencephase.
This procedure uses the pre-trained paraphraser module p for text generation
and utilizes the trained anomaly model m for normality scoring.
Let R be the number of iterations and K be the number of best candidate
texts. During each iteration, the paraphraser module p generates semantically
equivalent texts P for input text x (Algorithm 1, line 13). It extends P by
x x
x for top-K candidate texts selection and generates feature representations E
x
from f. Subsequently, it computes the normal likelihood scores S for the P
x x
using m. Finally, it sorts these scores S alongside their corresponding texts
x
P in descending order based on the scores S . Slicing the sorted result by K
x x
produces top-K candidate texts P and their scores S (Algorithm 1, line 17).
K K
The iteration terminates when scores S surpass the threshold τ or when the
K
maximum iteration R is reached.
3.3 Inference
Theinferencefunctionintegratestheevolutionarysearchfunctionandanensemble-
weighted mean classification procedure for robust text classification. During
inference, itinitiallyreceivestheinputtextandinvokestheevolutionarysearch
procedure, gathering the top-K candidate texts P along with their scores S .
K K
6Subsequently, it applies min-max normalization to the S , yielding normalized
K
weights W (Algorithm 1, line 27). To emphasize the relative importance of
K
each score, it normalizes the weights within the range [0, 1] by dividing them
by their sum.
Then, thetargetmodelf generatesconfidencesC forthetop-K candidate
K
texts P . Consequently, it performs a dot product on W and C to derive
K K K
the final confidence of the input text x. Applying the argmax function to the
final confidence produces the final robust classification l.
4 Experiments
Dataset Training Set Test Set # of Words # of Classes
IMDB 25,000 25,000 268 2
AG’s News 120,000 7,600 40 4
SST-2 6,920 1821 19 2
Table 1: Statistics of Datasets.
In this section, we show our experimental evaluation of GenFighter in com-
parisontostate-of-the-artdefensemethodsonthreedifferentdatasetsandagainst
threedifferentadversarialattacks. Morespecificallyinthefollowingsection,we
provide details about our experiments regarding datasets, attack methods, vic-
tim models, and state-of-the-art defense methods considered. Then we provide
the implementation settings and a discussion of the main results. In addition,
wepresentanablationstudyshowingtheimportanceofeachsub-componentof
GenFighter. Moreover,weprovideresultsofrobustnesstoattacktransferability.
4.1 Datasets
Weconductextensiveanalysisonthreebenchmarkclassificationdatasets: IMDB
(Maasetal.,2011),AG’sNews(Zhangetal.,2015),andSST-2(Socheretal.,
2013). IMDB is a document-level movie review dataset for sentiment analysis.
AG’s News is a sentence-level dataset for multi-class news classification, having
four types of classes: World, Sports, Business, and Science. SST-2 is a binary
classification dataset at the sentence level for sentiment analysis. The statistics
of these three datasets including # of examples in training and test sets, # of
classes, and the average # of words are listed in Table 1.
4.2 Attack Methods
Weemploythesamethreestrongattacksusedin(Wangetal.,2023)toevaluate
theefficiencyofGenFighteragainstallbaselineattacks. Suchselectionofattack
strategies is based on their proficiency in preserving semantics and showcasing
superior attack effectiveness.
7PWWS TextFooler BERT-Attack
Datasets DNNs Methods Clean%
Aua% Suc% #Query Aua% Suc% #Query Aua% Suc% #Query
Original 93.82 14.4 84.6 1529 8.6 90.8 554 37.8 59.6 1623
RanMASK(Zengetal.,2021b) 93.37 45.5 51.3 1566 45.3 51.6 859 42.3 54.8 1787
FreeLB++(Lietal.,2021) 94.32 35.7 62.3 1575 19.0 79.9 681 45.2 52.3 1723
RoBERTa
Flooding-X(Liuetal.,2022) 93.90 45.5 51.2 1575 22.0 76.4 735 44.2 52.6 1904
RMLM(Wangetal.,2023) 92.77 37.3 59.5 1569 47.4 48.6 923 22.2 75.9 1209
GenFighter 92.37 64.8 30.2 1606 69.2 25.4 1023 43.0 53.7 1652
IMDB
Original 92.05 22.1 75.9 1544 8.8 90.4 604 15.3 83.3 577
RanMASK(Zengetal.,2021b) 91.95 46.9 44.8 1536 48.1 43.7 916 51.5 39.4 2069
FreeLB++(Lietal.,2021) 91.29 26.4 70.8 1547 22.1 75.6 715 34.0 62.4 1424
BERT
Flooding-X(Liuetal.,2022) 92.35 49.0 47.0 1583 31.9 65.5 809 43.5 52.9 2028
RMLM(Wangetal.,2023) 90.80 28.9 67.8 1568 44.0 50.9 949 23.1 74.3 1240
GenFighter 91.10 58.8 35.5 1588 61.8 32.2 1000 44.0 51.7 1900
Original 94.57 54.9 42.1 260 49.6 47.7 167 58.3 38.5 552
RanMASK(Zengetal.,2021b) 94.75 64.9 31.2 264 64.2 31.9 179 64.0 32.1 585
FreeLB++(Lietal.,2021) 33.37 0.4 98.8 242 6.7 79.5 107 0.5 98.5 101
RoBERTa
Flooding-X(Liuetal.,2022) 94.84 55.7 40.9 262 49.5 47.5 171 58.9 37.5 566
RMLM(Wangetal.,2023) 93.86 67.8 27.3 259 76.3 18.2 189 44.4 52.4 494
GenFighter 94.29 78.3 17.1 268 79.7 15.7 195 65.8 30.4 605
AG’sNews
Original 94.61 57.4 39.1 261 46.7 50.4 168 46.8 50.3 229
RanMASK(Zengetal.,2021b) 94.79 63.4 32.6 263 62.2 34.0 178 64.0 32.1 580
FreeLB++(Lietal.,2021) 95.22 65.0 31.7 264 58.2 38.9 178 61.9 35.0 566
BERT
Flooding-X(Liuetal.,2022) 94.59 64.7 31.3 264 56.1 40.5 175 61.9 34.3 569
RMLM(Wangetal.,2023) 93.79 69.3 24.9 259 77.1 16.6 191 41.8 54.7 558
GenFighter 94.37 74.2 20.7 268 77.5 17.2 194 66.6 28.9 604
Original 94.12 35.2 62.9 115 33.6 64.6 67 25.9 72.7 138
RanMASK(Zengetal.,2021b) 93.30 33.7 63.9 115 41.5 55.6 70 21.5 77.0 125
FreeLB++(Lietal.,2021) 51.84 3.1 93.9 107 4.4 91.3 44 1.5 97.0 47
RoBERTa
Flooding-X(Liuetal.,2022) 95.11 37.3 60.7 115 35.7 62.3 68 19.7 79.2 117
RMLM(Wangetal.,2023) 92.51 45.1 51.2 116 58.7 36.5 80 19.4 78.9 98
GenFighter 93.14 51.2 45.5 117 60.9 35.1 81 27.6 70.6 155
SST-2
Original 91.71 28.1 69.5 113 26.3 71.4 61 14.3 84.5 62
RanMASK(Zengetal.,2021b) 91.32 29.0 68.3 114 35.2 61.3 67 20.3 77.8 122
FreeLB++(Lietal.,2021) 92.59 40.3 56.4 116 37.5 59.5 68 32.0 65.4 148
BERT
Flooding-X(Liuetal.,2022) 91.49 31.0 66.2 114 32.0 65.1 64 20.4 77.7 113
RMLM(Wangetal.,2023) 85.94 36.9 57.6 116 52.1 40.1 80 14.2 83.6 100
GenFighter 91.22 47.4 47.2 117 56.1 37.5 81 25.8 71.3 157
Table 2: Experimental results of GenFighter in comparison of state-of-the-art
methods on three datasets against three word-substitution attacks where all
models are trained on RoBERTa and BERT. The best performance is marked
in bold.
PWWS TextFooler BERT-Attack
Datasets Methods Clean%
Aua% Suc% #Query Aua% Suc% #Query Aua% Suc% #Query
Original 94.57 54.9 42.1 260 49.6 47.7 167 58.3 38.5 552
T5Paraphraser 94.55 48.4 48.4 259 45.4 51.6 168 54.9 41.5 545
AG’sNews
Original+T5Paraphraser 94.55 67.1 29.0 264 59.7 36.8 179 61.8 34.6 594
GenFighter 94.29 78.3 17.1 268 79.7 15.7 195 65.8 30.4 605
Original 94.12 35.2 62.9 115 33.6 64.6 67 25.9 72.7 138
T5Paraphraser 92.86 32.7 64.6 115 31.1 66.3 64 24.9 73.1 134
SST-2
Original+T5Paraphraser 94.67 38.5 59.6 115 35.8 62.5 66 27.2 71.5 144
GenFighter 93.14 51.2 45.5 117 60.9 35.1 81 27.6 70.6 155
Table3: Ablationstudyfindingsevaluatedusingthreeword-substitutionattacks
ontheAG’sNewsandSST-2datasets. Thebestperformanceismarkedinbold.
PWWS(Renetal.,2019)usesWordNet1 toconstructsynonymsassubstitu-
tionwordsandleveragesthechangeinthemodel’sprobabilityandwordsaliency
for the word replacement order. Jin et al. (2020) propose TextFooler attack
which determines the importance score of a word by accumulating probability
changes before and after deleting the word. The attack further uses synonym
replacement through a set of cosine-similar embedding vectors. BERT-Attack
(Li et al., 2020) utilizes the change of logit output of the victim model for find-
ingvulnerablewordsandgeneratessemantic-preservingwordreplacementusing
1https://wordnet.princeton.edu/
8BERT.
4.3 Victim Models and State-of-the-art Defense Methods
We perform experiments on two types of DNNs as victim models: RoBERTa-
base (Liu et al., 2020) and BERT-base-uncased (Devlin et al., 2019).
For the state-of-the-art defense comparison, we compare GenFighter with
onerandomizedsmoothingmodel,onegradient-guidedadversarialtrainingmodel,
one overfitting-based model, and one randomized masked language model.
RanMASK (Zengetal.,2021b)introducesanewrandomizedsmoothingtech-
nique that masks input texts repeatedly and performs prediction with a ”ma-
jority vote”.
FreeLB++ (Lietal.,2021)agradient-basedtrainingmethod,enhancesFreeLB
(Zhu et al., 2020) by expanding the search steps, resulting in a larger l2-norm
search region.
Flooding-X (Liu et al., 2022) restricts any further reduction in the training
lossandimprovesthemodel’sgeneralizationbyintroducingafloodingcriterion.
RMLM (Wangetal.,2023)triestodefendanattackbyrandomlyintroducing
noise to sentences and then correcting the contexts of these corrupted pertur-
bations. At last, it filters out adversarial samples.
4.4 Evaluation Metrics
Wereportfourevaluationmetricstoassesstherobustnessoftheaforementioned
defense methods against different adversarial attacks.
Clean Accuracy (Clean%) isthemodel’sclassificationaccuracyontheorig-
inal full clean test dataset.
Accuracy Under Attack (Aua%) is the prediction accuracy under an ad-
versarial attack. A higher Aua% reflects the better performance of a defender.
Attack Success Rate (Suc%) istheratioof#ofsuccessfullyperturedtexts
to the # of total attempted texts. A lower Suc% suggests a model is more
robust.
Number of Queries (#Query) istheaveragenumberofqueriestheattacker
attempts for each attack. A defending model should produce higher #Query in
order not to be easily compromised.
94.5 Implementation Settings
We leverage OpenAttack (Zeng et al., 2021a) to perform all attacks and com-
pare GenFighter against state-of-the-art defense methods. All defense methods
are reproduced using open-source code and their pre-defined hyperparameters.
We conduct model training on NVIDIA GeForce RTX 4090 and NVIDIA RTX
6000 GPUs. Moreover, we utilize a T5 paraphraser model previously trained
with Google PAWS (Zhang et al., 2019), accessible via the HuggingFace reposi-
tory2. ToevaluatemetricsAua%,Suc%,and#Querywerandomlychoose1,000
samples from the whole test set (similarly to what was done in (Wang et al.,
2023) and others) and we use such set for our approach and the state-of-the-art
methods. While we compute clean accuracy (Clean%) on the entire test set.
4.6 Main Results
We summarize the performance of GenFighter compared to the state-of-the-art
defenses in Table 2. Overall, our proposed approach enhances the robustness
of both the RoBERTa and BERT across all three datasets against three strong
adversarial attack strategies.
Our model outperforms the state-of-the-art defense in 15 out of 18 cases
(83%), as measured by Aua%. Against PWWS and TextFooler attacks, our
model showcases superior resilience, consistently outmatching all state-of-the-
art defense methods in terms of Aua%, Suc%, and #Query. These metrics are
crucial for evaluating the robustness of defense algorithms. Moreover, when
subjected to BERT-Attack, our defense surpasses the state-of-the-art defenses
in 3 out of 6 cases. Across all datasets and attacks, the absolute average im-
provements for Aua%, Suc%, and #Query are +41.6%, +37.0%, and +7.8%,
respectively. Notably,ourmodeloftenconsistentlyachievesthehighest#Query,
making it particularly challenging to be compromised. In addition, our defense
exhibits an extremely competitive Clean% and the difference is marginal. It
is important to highlight that FreeLB++ demonstrates significant performance
degradation, especially apparent with the RoBERTa, on both the AG’s News
and SST-2 datasets. As a result, we’ve chosen to omit comparative evalua-
tions of FreeLB++ on these datasets for the RoBERTa model. When consid-
ering the most recent state-of-the-art defense methods, namely Flooding-X and
RMLM, our model consistently demonstrates superior robustness compared to
these methods.
4.7 Ablation Study
Table 3 illustrates the individual contributions of each component within Gen-
Fighter when countering an adversarial attack. The ”Original” method refers
to the victim DNN model without any defense mechanism. We evaluate the
efficacy of our paraphrase module ”T5 Paraphraser” as it is used as a stan-
dalone victim model with no defense. Additionally, we assess the impact of
2https://huggingface.co/Vamsi/T5_Paraphrase_Paws
10the paraphraser module within the context of GenFighter by ”Original + T5
Paraphraser”. Finally, we examine the overall performance of the GenFighter
method.
We summarize the following findings: (1) The ”T5-Paraphraser” used alone
asavictimmodeldoesnotprovidesufficientdefenseagainstadversarialattacks,
showing a performance decline compared to the ”Original” victim method. (2)
Combining”T5-Paraphraser”withthe”Original”targetmethodenhancesover-
all performance. The absolute average improvements introduced by our ”T5-
Paraphraser” to the ”Original” method are +12.7%, +10.5%, and +4.8% for
Aua%,Suc%and#Queryrespectively. Furthermore,theenhancementachieved
by GenFighter over ”Original + T5 Paraphraser” is attributed to our anomaly
detection model and evolutionary search procedure. In this case, the absolute
average gains are +25.3%, +27.1%, and +4.4% for Aua%, Suc% and #Query
respectively. Thisshowsthatallthesub-componentsoftheGenFightermethod
are well integrated and needed to achieve its performance.
Datasets Attack Methods Original Flooding-X RMLM GenFighter
PWWS 54.9* 61.4 80.7 84.2
AG’s News TextFooler 49.6* 66.8 83.8 87.4
BERT-Attack 58.3* 63.8 70.7 75.9
PWWS 35.2* 40.5 53.6 56.3
SST-2 TextFooler 33.6* 53.7 55.8 61.3
BERT-Attack 25.9* 34.6 48.7 50.4
Table 4: Transferability assessment on AG’s News and SST-2 datasets against
threeword-substitutionattacks,withallmodelstrainedonRoBERTa. Asterisk
(*) denotes adversarial examples generated using the original model, validated
for transferability across other models based on the classification accuracy (%).
The best performance is marked in bold.
4.8 Hyperparameter Analysis
Figure 2 depicts how various hyperparameters, including the number of para-
phrases (N ), α-percentile, number of rounds (R), and number of best candi-
p
dates (K), affect the outcome of GenFighter in terms of Aua%.
Number of Paraphrases GenFighter consistently defends against PWWS
and TextFooler attacks across all values of number of paraphrases N , whereas
p
its effectiveness against BERT-Attack improves with higher values of N . Opt-
p
ing for higher values of N comes with increased computational costs. Hence,
p
choosing a good trade-off is important.
α-percentile Weestablishthresholdτ astheα-percentilevaluebasedontrain-
ing data normality scores. Given the extensive query generation by BERT-
Attack and the challenge of meeting high τ values, aligning selected candidates
with the training distribution becomes difficult. Notably, our model begins to
11PWWS
TextFooler
BertAttack
90 90
80 80
70 70
60 60
4 7 10 13 16 0 20 40 60 80
N α
p
90 90
80 80
70 70
60 60
1 2 3 4 5 1 2 3 4 5
R K
Figure 2: Hyperparameter analysis of GenFighter against three word-
substitution attacks on the AG’s News dataset. We randomly sample 100 test
examples and compute the Aua% for each attack across all hyperparameters.
improve after the 15th and 30th α-percentiles against TextFooler and PWWS,
respectively.
Number of Rounds R stands out as the most critical parameter in our evo-
lutionarysearchprocedure. Witheachiteration,GenFighteraugmentsthecan-
didate population and refines selection within the search process. Remarkably,
Aua%consistentlyimproveswithahighernumberofR. Thisindicatesthatthe
ability of our model to contrast attacks depends on the time allocated for the
evolutivesearchprocess. WemaintainaconstantK throughoutthisevaluation.
Number of Best Candidates To assess the impact of number of best can-
didates K on Aua%, we maintain R=5 while varying K. As K approaches R,
Aua%decreasesacrossallthreeattacks. Optimalperformanceisobservedwith
12
%auA
%auA
%auA
%auAmid values of R for K.
4.9 Transferability
Transferabilityinadversarialattacksinvolvescraftingsuccessfuladversarialex-
amples using one model and exploiting them to deceive other models with min-
imal effort. Table 4 illustrates how GenFighter effectively mitigates the trans-
ferability of adversarial attacks. We evaluate the recent state-of-the-art defense
methods Flooding-X and RMLM for comparison in mitigating transferability.
OurfindingsrevealthatGenFighterconsistentlydefendsagainsttransferability
by achieving the highest classification accuracy (%) across all cases.
5 Conclusion
Inthiswork,wehaveaddressedtheproblemofdefendingagainstword-substitution
adversarial attacks in natural language processing (NLP) tasks, particularly
those targeting Transformer models.
Tocombatsuchattacks,weproposedanoveldefensestrategy,GenFighter,
whichlearnsthedistributionoftrainingdatatoidentifyandmitigatepotentially
maliciousinstances. Throughanevolutionaryprocess,GenFightergeneratesse-
mantically equivalent alternatives aligned with the training data distribution,
and ensembles the classifications outcome of such alternatives to provide a uni-
fied and robust outcome.
OurexperimentalresultsdemonstratetheefficacyofGenFighterinachieving
superioraccuracyunderattackscenariosandreducingthesuccessrateofadver-
sarial attacks compared to state-of-the-art defenses, on different datasets. Last
but not least, our approach forces the attacker models to perform the largest
numberofqueriesduringtheattackamongallthecompetitors,bymakingsuch
attacks infeasible in real scenarios.
136 Limitations
Ourdefensestrategy,GenFighter,showsefficacyinmitigatingarangeofNLP
attacks on RoBERTa and BERT models. However, further research is essential
to address the following limitations:
• GenFighter operates under the assumption that successful attacks pro-
duceinstanceslyingoutsidethedistributionofthetrainingset,wherethe
victimmodellacksexplicittraining. Ourexperimentsconfirmthevalidity
of this assumption, enabling GenFighter to advance the current state
of defensive modeling. However, further studies are needed to identify
attacks more aligned with the training classification distribution of the
victim model. This can potentially lead to new attack models.
• While GenFighter relies on learning the training classification distribu-
tion, we employ a Gaussian Mixture Model to process text embedding
representations. Exploring alternative or more sophisticated approaches
couldenhanceperformance. Experimentingwithdifferenttextembedding
modelsandanomalydetectiontechniquessuchasautoencoderscouldlead
to better ways to learn such distribution.
• Our defense model’s effectiveness relies upon the quality of paraphras-
ing. Investigating additional paraphrasing methodologies could improve
its performance.
• In specialized NLP tasks involving scientific terminology, such as auto-
matic scientific claim verification, the presence of synonymous words can
compromise the statement’s meaning. Consequently, our approach, like
others, necessitates further investigation and refinement in such contexts.
References
Moustafa Alzantot, Yash Sharma, Ahmed Elgohary, Bo-Jhang Ho, Mani Sri-
vastava, and Kai-Wei Chang. 2018. Generating natural language adversarial
examples. In Proceedings of the 2018 Conference on Empirical Methods in
Natural Language Processing, pages 2890–2896, Brussels, Belgium. Associa-
tion for Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019.
BERT: Pre-training of deep bidirectional transformers for language under-
standing. InProceedingsofthe2019ConferenceoftheNorthAmericanChap-
ter of the Association for Computational Linguistics: Human Language Tech-
nologies, Volume 1 (Long and Short Papers), pages 4171–4186, Minneapolis,
Minnesota. Association for Computational Linguistics.
Javid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing Dou. 2018. HotFlip:
White-box adversarial examples for text classification. In Proceedings of the
1456th Annual Meeting of the Association for Computational Linguistics (Vol-
ume 2: Short Papers), pages 31–36, Melbourne, Australia. Association for
Computational Linguistics.
Ji Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun Qi. 2018. Black-box
generation of adversarial text sequences to evade deep learning classifiers. In
2018 IEEE Security and Privacy Workshops (SPW), pages 50–56.
Siddhant Garg and Goutham Ramakrishnan. 2020. BAE: BERT-based adver-
sarial examples for text classification. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language Processing (EMNLP),pages
6174–6181, Online. Association for Computational Linguistics.
Po-SenHuang,RobertStanforth,JohannesWelbl,ChrisDyer,DaniYogatama,
Sven Gowal, Krishnamurthy Dvijotham, and Pushmeet Kohli. 2019. Achiev-
ing verified robustness to symbol substitutions via interval bound propaga-
tion. InProceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Natu-
ral Language Processing (EMNLP-IJCNLP), pages 4083–4093, Hong Kong,
China. Association for Computational Linguistics.
TakashiIshida,IkkoYamane,TomoyaSakai,GangNiu,andMasashiSugiyama.
2020.Doweneedzerotraininglossafterachievingzerotrainingerror? InPro-
ceedingsofthe37thInternationalConferenceonMachineLearning,ICML’20.
JMLR.org.
RobinJia,AditiRaghunathan,KeremG¨oksel,andPercyLiang.2019. Certified
robustnesstoadversarialwordsubstitutions. InProceedings of the 2019 Con-
ference on Empirical Methods in Natural Language Processing and the 9th
International Joint Conference on Natural Language Processing (EMNLP-
IJCNLP), pages 4129–4142, Hong Kong, China. Association for Computa-
tional Linguistics.
Di Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. 2020. Is BERT
really robust? A strong baseline for natural language attack on text classi-
fication and entailment. In Proceedings of the AAAI conference on artificial
intelligence, volume 34, pages 8018–8025.
Jinfeng Li, Shouling Ji, Tianyu Du, Bo Li, and Ting Wang. 2018. Textbugger:
Generating adversarial text against real-world applications. arXiv preprint
arXiv:1812.05271.
Linyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. 2020.
BERT-ATTACK:AdversarialattackagainstBERTusingBERT. InProceed-
ings of the 2020 Conference on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pages 6193–6202, Online. Association for Computational
Linguistics.
15Zongyi Li, Jianhan Xu, Jiehang Zeng, Linyang Li, Xiaoqing Zheng, Qi Zhang,
Kai-WeiChang,andCho-JuiHsieh.2021. Searchingforaneffectivedefender:
Benchmarking defense against adversarial word substitution. In Proceedings
ofthe2021ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,
pages 3137–3147, Online and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
QinLiu,RuiZheng,BaoRong,JingyiLiu,ZhiHuaLiu,ZhanzhanCheng,Liang
Qiao, Tao Gui, Qi Zhang, and Xuanjing Huang. 2022. Flooding-X: Improv-
ingBERT’sresistancetoadversarialattacksvialoss-restrictedfine-tuning. In
Proceedings of the 60th Annual Meeting of the Association for Computational
Linguistics (Volume 1: Long Papers), pages 5634–5644, Dublin, Ireland. As-
sociation for Computational Linguistics.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen,
Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2020.
Ro{bert}a: A robustly optimized {bert} pretraining approach.
AndrewL.Maas,RaymondE.Daly,PeterT.Pham,DanHuang,AndrewY.Ng,
and Christopher Potts. 2011. Learning word vectors for sentiment analysis.
In Proceedings of the 49th Annual Meeting of the Association for Computa-
tional Linguistics: Human Language Technologies, pages 142–150, Portland,
Oregon, USA. Association for Computational Linguistics.
Rishabh Maheshwary, Saket Maheshwary, and Vikram Pudi. 2021. A strong
baseline for query efficient attacks in a black box setting. In Proceedings of
the 2021 Conference on Empirical Methods in Natural Language Processing,
pages 8396–8409, Online and Punta Cana, Dominican Republic. Association
for Computational Linguistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,
Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2019. Exploring
the limits of transfer learning with a unified text-to-text transformer. arXiv
preprint arXiv:1910.10683.
ShuhuaiRen,YiheDeng,KunHe,andWanxiangChe.2019.Generatingnatural
languageadversarialexamplesthroughprobabilityweightedwordsaliency. In
Proceedings of the 57th annual meeting of the association for computational
linguistics, pages 1085–1097.
Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Semantically
equivalent adversarial rules for debugging NLP models. In Proceedings of
the 56th Annual Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pages 856–865, Melbourne, Australia. Association
for Computational Linguistics.
Gideon Schwarz. 1978. Estimating the dimension of a model. The annals of
statistics, pages 461–464.
16Chenglei Si, Zhengyan Zhang, Fanchao Qi, Zhiyuan Liu, Yasheng Wang, Qun
Liu, and Maosong Sun. 2021. Better robustness by more coverage: Adver-
sarial and mixup data augmentation for robust finetuning. In Findings of
the Association for Computational Linguistics: ACL-IJCNLP 2021, pages
1569–1576, Online. Association for Computational Linguistics.
Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Man-
ning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for
semantic compositionality over a sentiment treebank. In Proceedings of the
2013ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages
1631–1642, Seattle, Washington, USA. Association for Computational Lin-
guistics.
Zhaoyang Wang, Zhiyue Liu, Xiaopeng Zheng, Qinliang Su, and Jiahai Wang.
2023. RMLM: A flexible defense framework for proactively mitigating word-
level adversarial attacks. In Proceedings of the 61st Annual Meeting of the
Association for Computational Linguistics (Volume 1: Long Papers), pages
2757–2774, Toronto, Canada. Association for Computational Linguistics.
Guoyang Zeng, Fanchao Qi, Qianrui Zhou, Tingji Zhang, Zixian Ma, Bairu
Hou, Yuan Zang, Zhiyuan Liu, and Maosong Sun. 2021a. OpenAttack: An
open-source textual adversarial attack toolkit. In Proceedings of the 59th
Annual Meeting of the Association for Computational Linguistics and the
11th International Joint Conference on Natural Language Processing: Sys-
tem Demonstrations, pages 363–371, Online. Association for Computational
Linguistics.
Jiehang Zeng, Xiaoqing Zheng, Jianhan Xu, Linyang Li, Liping Yuan, and Xu-
anjing Huang. 2021b. Certified robustness to text adversarial attacks by
randomized [mask].
Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolu-
tional networks for text classification. In Advances in Neural Information
Processing Systems, volume 28. Curran Associates, Inc.
Yuan Zhang, Jason Baldridge, and Luheng He. 2019. PAWS: Paraphrase Ad-
versaries from Word Scrambling. In Proc. of NAACL.
YiZhou, XiaoqingZheng, Cho-JuiHsieh, Kai-WeiChang, andXuanjingHuan.
2021. Defense against synonym substitution-based adversarial attacks via
dirichlet neighborhood ensemble. In Association for Computational Linguis-
tics (ACL).
ChenZhu,YuCheng,ZheGan,SiqiSun,TomGoldstein,andJingjingLiu.2020.
Freelb: Enhancedadversarialtrainingfornaturallanguageunderstanding. In
International Conference on Learning Representations.
17