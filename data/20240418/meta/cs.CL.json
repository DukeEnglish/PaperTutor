[
    {
        "title": "Related Work and Citation Text Generation: A Survey",
        "authors": "Xiangci LiJessica Ouyang",
        "links": "http://arxiv.org/abs/2404.11588v1",
        "entry_id": "http://arxiv.org/abs/2404.11588v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11588v1",
        "summary": "To convince readers of the novelty of their research paper, authors must\nperform a literature review and compose a coherent story that connects and\nrelates prior works to the current work. This challenging nature of literature\nreview writing makes automatic related work generation (RWG) academically and\ncomputationally interesting, and also makes it an excellent test bed for\nexamining the capability of SOTA natural language processing (NLP) models.\nSince the initial proposal of the RWG task, its popularity has waxed and waned,\nfollowing the capabilities of mainstream NLP approaches. In this work, we\nsurvey the zoo of RWG historical works, summarizing the key approaches and task\ndefinitions and discussing the ongoing challenges of RWG.",
        "updated": "2024-04-17 17:37:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11588v1"
    },
    {
        "title": "The Landscape of Emerging AI Agent Architectures for Reasoning, Planning, and Tool Calling: A Survey",
        "authors": "Tula MastermanSandi BesenMason SawtellAlex Chao",
        "links": "http://arxiv.org/abs/2404.11584v1",
        "entry_id": "http://arxiv.org/abs/2404.11584v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11584v1",
        "summary": "This survey paper examines the recent advancements in AI agent\nimplementations, with a focus on their ability to achieve complex goals that\nrequire enhanced reasoning, planning, and tool execution capabilities. The\nprimary objectives of this work are to a) communicate the current capabilities\nand limitations of existing AI agent implementations, b) share insights gained\nfrom our observations of these systems in action, and c) suggest important\nconsiderations for future developments in AI agent design. We achieve this by\nproviding overviews of single-agent and multi-agent architectures, identifying\nkey patterns and divergences in design choices, and evaluating their overall\nimpact on accomplishing a provided goal. Our contribution outlines key themes\nwhen selecting an agentic architecture, the impact of leadership on agent\nsystems, agent communication styles, and key phases for planning, execution,\nand reflection that enable robust AI agent systems.",
        "updated": "2024-04-17 17:32:41 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11584v1"
    },
    {
        "title": "Quantifying Multilingual Performance of Large Language Models Across Languages",
        "authors": "Zihao LiYucheng ShiZirui LiuFan YangNinghao LiuMengnan Du",
        "links": "http://arxiv.org/abs/2404.11553v1",
        "entry_id": "http://arxiv.org/abs/2404.11553v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11553v1",
        "summary": "The training process of Large Language Models (LLMs) requires extensive text\ncorpus. However, these data are often unevenly distributed in different\nlanguages. As a result, LLMs perform well on common languages, such as English,\nGerman, and French, but perform poorly on low-resource languages. However,\ncurrently there is no work to quantitatively measure the performance of LLMs in\nlow-resource languages. To fill this gap, we proposed the Language Ranker that\naims to benchmark and rank different languages according to the performance of\nLLMs on those languages. We employ the LLM's performance on the English corpus\nas a baseline to compare the performances of different languages and English.\nWe have the following three findings: 1. The performance rankings of different\nLLMs in all languages are roughly the same. 2. LLMs with different sizes have\nthe same partial order of performance. 3. There is a strong correlation between\nLlaMa2's performance in different languages and the proportion of the\npre-training corpus. These findings illustrate that the Language Ranker can be\nused as an indicator to measure the language performance of LLMs.",
        "updated": "2024-04-17 16:53:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11553v1"
    },
    {
        "title": "Evaluating Span Extraction in Generative Paradigm: A Reflection on Aspect-Based Sentiment Analysis",
        "authors": "Soyoung YangWon Ik Cho",
        "links": "http://arxiv.org/abs/2404.11539v1",
        "entry_id": "http://arxiv.org/abs/2404.11539v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11539v1",
        "summary": "In the era of rapid evolution of generative language models within the realm\nof natural language processing, there is an imperative call to revisit and\nreformulate evaluation methodologies, especially in the domain of aspect-based\nsentiment analysis (ABSA). This paper addresses the emerging challenges\nintroduced by the generative paradigm, which has moderately blurred traditional\nboundaries between understanding and generation tasks. Building upon prevailing\npractices in the field, we analyze the advantages and shortcomings associated\nwith the prevalent ABSA evaluation paradigms. Through an in-depth examination,\nsupplemented by illustrative examples, we highlight the intricacies involved in\naligning generative outputs with other evaluative metrics, specifically those\nderived from other tasks, including question answering. While we steer clear of\nadvocating for a singular and definitive metric, our contribution lies in\npaving the path for a comprehensive guideline tailored for ABSA evaluations in\nthis generative paradigm. In this position paper, we aim to provide\npractitioners with profound reflections, offering insights and directions that\ncan aid in navigating this evolving landscape, ensuring evaluations that are\nboth accurate and reflective of generative capabilities.",
        "updated": "2024-04-17 16:33:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11539v1"
    },
    {
        "title": "GenFighter: A Generative and Evolutive Textual Attack Removal",
        "authors": "Md Athikul IslamEdoardo SerraSushil Jajodia",
        "links": "http://arxiv.org/abs/2404.11538v1",
        "entry_id": "http://arxiv.org/abs/2404.11538v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11538v1",
        "summary": "Adversarial attacks pose significant challenges to deep neural networks\n(DNNs) such as Transformer models in natural language processing (NLP). This\npaper introduces a novel defense strategy, called GenFighter, which enhances\nadversarial robustness by learning and reasoning on the training classification\ndistribution. GenFighter identifies potentially malicious instances deviating\nfrom the distribution, transforms them into semantically equivalent instances\naligned with the training data, and employs ensemble techniques for a unified\nand robust response. By conducting extensive experiments, we show that\nGenFighter outperforms state-of-the-art defenses in accuracy under attack and\nattack success rate metrics. Additionally, it requires a high number of queries\nper attack, making the attack more challenging in real scenarios. The ablation\nstudy shows that our approach integrates transfer learning, a\ngenerative/evolutive procedure, and an ensemble method, providing an effective\ndefense against NLP adversarial attacks.",
        "updated": "2024-04-17 16:32:13 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11538v1"
    }
]