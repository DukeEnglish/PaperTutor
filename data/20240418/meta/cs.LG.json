[
    {
        "title": "Learning to Solve the Constrained Most Probable Explanation Task in Probabilistic Graphical Models",
        "authors": "Shivvrat AryaTahrima RahmanVibhav Gogate",
        "links": "http://arxiv.org/abs/2404.11606v1",
        "entry_id": "http://arxiv.org/abs/2404.11606v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11606v1",
        "summary": "We propose a self-supervised learning approach for solving the following\nconstrained optimization task in log-linear models or Markov networks. Let $f$\nand $g$ be two log-linear models defined over the sets $\\mathbf{X}$ and\n$\\mathbf{Y}$ of random variables respectively. Given an assignment $\\mathbf{x}$\nto all variables in $\\mathbf{X}$ (evidence) and a real number $q$, the\nconstrained most-probable explanation (CMPE) task seeks to find an assignment\n$\\mathbf{y}$ to all variables in $\\mathbf{Y}$ such that $f(\\mathbf{x},\n\\mathbf{y})$ is maximized and $g(\\mathbf{x}, \\mathbf{y})\\leq q$. In our\nproposed self-supervised approach, given assignments $\\mathbf{x}$ to\n$\\mathbf{X}$ (data), we train a deep neural network that learns to output\nnear-optimal solutions to the CMPE problem without requiring access to any\npre-computed solutions. The key idea in our approach is to use first principles\nand approximate inference methods for CMPE to derive novel loss functions that\nseek to push infeasible solutions towards feasible ones and feasible solutions\ntowards optimal ones. We analyze the properties of our proposed method and\nexperimentally demonstrate its efficacy on several benchmark problems.",
        "updated": "2024-04-17 17:55:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11606v1"
    },
    {
        "title": "Variational Bayesian Last Layers",
        "authors": "James HarrisonJohn WillesJasper Snoek",
        "links": "http://arxiv.org/abs/2404.11599v1",
        "entry_id": "http://arxiv.org/abs/2404.11599v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11599v1",
        "summary": "We introduce a deterministic variational formulation for training Bayesian\nlast layer neural networks. This yields a sampling-free, single-pass model and\nloss that effectively improves uncertainty estimation. Our variational Bayesian\nlast layer (VBLL) can be trained and evaluated with only quadratic complexity\nin last layer width, and is thus (nearly) computationally free to add to\nstandard architectures. We experimentally investigate VBLLs, and show that they\nimprove predictive accuracy, calibration, and out of distribution detection\nover baselines across both regression and classification. Finally, we\ninvestigate combining VBLL layers with variational Bayesian feature learning,\nyielding a lower variance collapsed variational inference method for Bayesian\nneural networks.",
        "updated": "2024-04-17 17:50:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11599v1"
    },
    {
        "title": "Explainable Artificial Intelligence Techniques for Accurate Fault Detection and Diagnosis: A Review",
        "authors": "Ahmed MagedSalah HaridyHerman Shen",
        "links": "http://arxiv.org/abs/2404.11597v1",
        "entry_id": "http://arxiv.org/abs/2404.11597v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11597v1",
        "summary": "As the manufacturing industry advances with sensor integration and\nautomation, the opaque nature of deep learning models in machine learning poses\na significant challenge for fault detection and diagnosis. And despite the\nrelated predictive insights Artificial Intelligence (AI) can deliver, advanced\nmachine learning engines often remain a black box. This paper reviews the\neXplainable AI (XAI) tools and techniques in this context. We explore various\nXAI methodologies, focusing on their role in making AI decision-making\ntransparent, particularly in critical scenarios where humans are involved. We\nalso discuss current limitations and potential future research that aims to\nbalance explainability with model performance while improving trustworthiness\nin the context of AI applications for critical industrial use cases.",
        "updated": "2024-04-17 17:49:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11597v1"
    },
    {
        "title": "Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept Understanding",
        "authors": "Zezhong FanXiaohan LiChenhao FangTopojoy BiswasKaushiki NagJianpeng XuKannan Achan",
        "links": "http://dx.doi.org/10.1145/3589335.3651927",
        "entry_id": "http://arxiv.org/abs/2404.11589v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11589v1",
        "summary": "The rapid evolution of text-to-image diffusion models has opened the door of\ngenerative AI, enabling the translation of textual descriptions into visually\ncompelling images with remarkable quality. However, a persistent challenge\nwithin this domain is the optimization of prompts to effectively convey\nabstract concepts into concrete objects. For example, text encoders can hardly\nexpress \"peace\", while can easily illustrate olive branches and white doves.\nThis paper introduces a novel approach named Prompt Optimizer for Abstract\nConcepts (POAC) specifically designed to enhance the performance of\ntext-to-image diffusion models in interpreting and generating images from\nabstract concepts. We propose a Prompt Language Model (PLM), which is\ninitialized from a pre-trained language model, and then fine-tuned with a\ncurated dataset of abstract concept prompts. The dataset is created with GPT-4\nto extend the abstract concept to a scene and concrete objects. Our framework\nemploys a Reinforcement Learning (RL)-based optimization strategy, focusing on\nthe alignment between the generated images by a stable diffusion model and\noptimized prompts. Through extensive experiments, we demonstrate that our\nproposed POAC significantly improves the accuracy and aesthetic quality of\ngenerated images, particularly in the description of abstract concepts and\nalignment with optimized prompts. We also present a comprehensive analysis of\nour model's performance across diffusion models under different settings,\nshowcasing its versatility and effectiveness in enhancing abstract concept\nrepresentation.",
        "updated": "2024-04-17 17:38:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11589v1"
    },
    {
        "title": "Deep Policy Optimization with Temporal Logic Constraints",
        "authors": "Ameesh ShahCameron VoloshinChenxi YangAbhinav VermaSwarat ChaudhuriSanjit A. Seshia",
        "links": "http://arxiv.org/abs/2404.11578v1",
        "entry_id": "http://arxiv.org/abs/2404.11578v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11578v1",
        "summary": "Temporal logics, such as linear temporal logic (LTL), offer a precise means\nof specifying tasks for (deep) reinforcement learning (RL) agents. In our work,\nwe consider the setting where the task is specified by an LTL objective and\nthere is an additional scalar reward that we need to optimize. Previous works\nfocus either on learning a LTL task-satisfying policy alone or are restricted\nto finite state spaces. We make two contributions: First, we introduce an\nRL-friendly approach to this setting by formulating this problem as a single\noptimization objective. Our formulation guarantees that an optimal policy will\nbe reward-maximal from the set of policies that maximize the likelihood of\nsatisfying the LTL specification. Second, we address a sparsity issue that\noften arises for LTL-guided Deep RL policies by introducing Cycle Experience\nReplay (CyclER), a technique that automatically guides RL agents towards the\nsatisfaction of an LTL specification. Our experiments demonstrate the efficacy\nof CyclER in finding performant deep RL policies in both continuous and\ndiscrete experimental domains.",
        "updated": "2024-04-17 17:24:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11578v1"
    }
]