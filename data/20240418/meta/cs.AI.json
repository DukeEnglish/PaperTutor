[
    {
        "title": "Learning to Solve the Constrained Most Probable Explanation Task in Probabilistic Graphical Models",
        "authors": "Shivvrat AryaTahrima RahmanVibhav Gogate",
        "links": "http://arxiv.org/abs/2404.11606v1",
        "entry_id": "http://arxiv.org/abs/2404.11606v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11606v1",
        "summary": "We propose a self-supervised learning approach for solving the following\nconstrained optimization task in log-linear models or Markov networks. Let $f$\nand $g$ be two log-linear models defined over the sets $\\mathbf{X}$ and\n$\\mathbf{Y}$ of random variables respectively. Given an assignment $\\mathbf{x}$\nto all variables in $\\mathbf{X}$ (evidence) and a real number $q$, the\nconstrained most-probable explanation (CMPE) task seeks to find an assignment\n$\\mathbf{y}$ to all variables in $\\mathbf{Y}$ such that $f(\\mathbf{x},\n\\mathbf{y})$ is maximized and $g(\\mathbf{x}, \\mathbf{y})\\leq q$. In our\nproposed self-supervised approach, given assignments $\\mathbf{x}$ to\n$\\mathbf{X}$ (data), we train a deep neural network that learns to output\nnear-optimal solutions to the CMPE problem without requiring access to any\npre-computed solutions. The key idea in our approach is to use first principles\nand approximate inference methods for CMPE to derive novel loss functions that\nseek to push infeasible solutions towards feasible ones and feasible solutions\ntowards optimal ones. We analyze the properties of our proposed method and\nexperimentally demonstrate its efficacy on several benchmark problems.",
        "updated": "2024-04-17 17:55:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11606v1"
    },
    {
        "title": "VG4D: Vision-Language Model Goes 4D Video Recognition",
        "authors": "Zhichao DengXiangtai LiXia LiYunhai TongShen ZhaoMengyuan Liu",
        "links": "http://arxiv.org/abs/2404.11605v1",
        "entry_id": "http://arxiv.org/abs/2404.11605v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11605v1",
        "summary": "Understanding the real world through point cloud video is a crucial aspect of\nrobotics and autonomous driving systems. However, prevailing methods for 4D\npoint cloud recognition have limitations due to sensor resolution, which leads\nto a lack of detailed information. Recent advances have shown that\nVision-Language Models (VLM) pre-trained on web-scale text-image datasets can\nlearn fine-grained visual concepts that can be transferred to various\ndownstream tasks. However, effectively integrating VLM into the domain of 4D\npoint clouds remains an unresolved problem. In this work, we propose the\nVision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge from\nvisual-text pre-trained models to a 4D point cloud network. Our approach\ninvolves aligning the 4D encoder's representation with a VLM to learn a shared\nvisual and text space from training on large-scale image-text pairs. By\ntransferring the knowledge of the VLM to the 4D encoder and combining the VLM,\nour VG4D achieves improved recognition performance. To enhance the 4D encoder,\nwe modernize the classic dynamic point cloud backbone and propose an improved\nversion of PSTNet, im-PSTNet, which can efficiently model point cloud videos.\nExperiments demonstrate that our method achieves state-of-the-art performance\nfor action recognition on both the NTU RGB+D 60 dataset and the NTU RGB+D 120\ndataset. Code is available at \\url{https://github.com/Shark0-0/VG4D}.",
        "updated": "2024-04-17 17:54:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11605v1"
    },
    {
        "title": "Explainable Artificial Intelligence Techniques for Accurate Fault Detection and Diagnosis: A Review",
        "authors": "Ahmed MagedSalah HaridyHerman Shen",
        "links": "http://arxiv.org/abs/2404.11597v1",
        "entry_id": "http://arxiv.org/abs/2404.11597v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11597v1",
        "summary": "As the manufacturing industry advances with sensor integration and\nautomation, the opaque nature of deep learning models in machine learning poses\na significant challenge for fault detection and diagnosis. And despite the\nrelated predictive insights Artificial Intelligence (AI) can deliver, advanced\nmachine learning engines often remain a black box. This paper reviews the\neXplainable AI (XAI) tools and techniques in this context. We explore various\nXAI methodologies, focusing on their role in making AI decision-making\ntransparent, particularly in critical scenarios where humans are involved. We\nalso discuss current limitations and potential future research that aims to\nbalance explainability with model performance while improving trustworthiness\nin the context of AI applications for critical industrial use cases.",
        "updated": "2024-04-17 17:49:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11597v1"
    },
    {
        "title": "Prompt Optimizer of Text-to-Image Diffusion Models for Abstract Concept Understanding",
        "authors": "Zezhong FanXiaohan LiChenhao FangTopojoy BiswasKaushiki NagJianpeng XuKannan Achan",
        "links": "http://dx.doi.org/10.1145/3589335.3651927",
        "entry_id": "http://arxiv.org/abs/2404.11589v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11589v1",
        "summary": "The rapid evolution of text-to-image diffusion models has opened the door of\ngenerative AI, enabling the translation of textual descriptions into visually\ncompelling images with remarkable quality. However, a persistent challenge\nwithin this domain is the optimization of prompts to effectively convey\nabstract concepts into concrete objects. For example, text encoders can hardly\nexpress \"peace\", while can easily illustrate olive branches and white doves.\nThis paper introduces a novel approach named Prompt Optimizer for Abstract\nConcepts (POAC) specifically designed to enhance the performance of\ntext-to-image diffusion models in interpreting and generating images from\nabstract concepts. We propose a Prompt Language Model (PLM), which is\ninitialized from a pre-trained language model, and then fine-tuned with a\ncurated dataset of abstract concept prompts. The dataset is created with GPT-4\nto extend the abstract concept to a scene and concrete objects. Our framework\nemploys a Reinforcement Learning (RL)-based optimization strategy, focusing on\nthe alignment between the generated images by a stable diffusion model and\noptimized prompts. Through extensive experiments, we demonstrate that our\nproposed POAC significantly improves the accuracy and aesthetic quality of\ngenerated images, particularly in the description of abstract concepts and\nalignment with optimized prompts. We also present a comprehensive analysis of\nour model's performance across diffusion models under different settings,\nshowcasing its versatility and effectiveness in enhancing abstract concept\nrepresentation.",
        "updated": "2024-04-17 17:38:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11589v1"
    },
    {
        "title": "Spatial Context-based Self-Supervised Learning for Handwritten Text Recognition",
        "authors": "Carlos PenarrubiaCarlos Garrido-MunozJose J. Valero-MasJorge Calvo-Zaragoza",
        "links": "http://arxiv.org/abs/2404.11585v1",
        "entry_id": "http://arxiv.org/abs/2404.11585v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11585v1",
        "summary": "Handwritten Text Recognition (HTR) is a relevant problem in computer vision,\nand implies unique challenges owing to its inherent variability and the rich\ncontextualization required for its interpretation. Despite the success of\nSelf-Supervised Learning (SSL) in computer vision, its application to HTR has\nbeen rather scattered, leaving key SSL methodologies unexplored. This work\nfocuses on one of them, namely Spatial Context-based SSL. We investigate how\nthis family of approaches can be adapted and optimized for HTR and propose new\nworkflows that leverage the unique features of handwritten text. Our\nexperiments demonstrate that the methods considered lead to advancements in the\nstate-of-the-art of SSL for HTR in a number of benchmark cases.",
        "updated": "2024-04-17 17:33:32 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11585v1"
    }
]