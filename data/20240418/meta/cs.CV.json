[
    {
        "title": "Factorized Diffusion: Perceptual Illusions by Noise Decomposition",
        "authors": "Daniel GengInbum ParkAndrew Owens",
        "links": "http://arxiv.org/abs/2404.11615v1",
        "entry_id": "http://arxiv.org/abs/2404.11615v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11615v1",
        "summary": "Given a factorization of an image into a sum of linear components, we present\na zero-shot method to control each individual component through diffusion model\nsampling. For example, we can decompose an image into low and high spatial\nfrequencies and condition these components on different text prompts. This\nproduces hybrid images, which change appearance depending on viewing distance.\nBy decomposing an image into three frequency subbands, we can generate hybrid\nimages with three prompts. We also use a decomposition into grayscale and color\ncomponents to produce images whose appearance changes when they are viewed in\ngrayscale, a phenomena that naturally occurs under dim lighting. And we explore\na decomposition by a motion blur kernel, which produces images that change\nappearance under motion blurring. Our method works by denoising with a\ncomposite noise estimate, built from the components of noise estimates\nconditioned on different prompts. We also show that for certain decompositions,\nour method recovers prior approaches to compositional generation and spatial\ncontrol. Finally, we show that we can extend our approach to generate hybrid\nimages from real images. We do this by holding one component fixed and\ngenerating the remaining components, effectively solving an inverse problem.",
        "updated": "2024-04-17 17:59:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11615v1"
    },
    {
        "title": "Dynamic Typography: Bringing Words to Life",
        "authors": "Zichen LiuYihao MengHao OuyangYue YuBolin ZhaoDaniel Cohen-OrHuamin Qu",
        "links": "http://arxiv.org/abs/2404.11614v1",
        "entry_id": "http://arxiv.org/abs/2404.11614v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11614v1",
        "summary": "Text animation serves as an expressive medium, transforming static\ncommunication into dynamic experiences by infusing words with motion to evoke\nemotions, emphasize meanings, and construct compelling narratives. Crafting\nanimations that are semantically aware poses significant challenges, demanding\nexpertise in graphic design and animation. We present an automated text\nanimation scheme, termed \"Dynamic Typography\", which combines two challenging\ntasks. It deforms letters to convey semantic meaning and infuses them with\nvibrant movements based on user prompts. Our technique harnesses vector\ngraphics representations and an end-to-end optimization-based framework. This\nframework employs neural displacement fields to convert letters into base\nshapes and applies per-frame motion, encouraging coherence with the intended\ntextual concept. Shape preservation techniques and perceptual loss\nregularization are employed to maintain legibility and structural integrity\nthroughout the animation process. We demonstrate the generalizability of our\napproach across various text-to-video models and highlight the superiority of\nour end-to-end methodology over baseline methods, which might comprise separate\ntasks. Through quantitative and qualitative evaluations, we demonstrate the\neffectiveness of our framework in generating coherent text animations that\nfaithfully interpret user prompts while maintaining readability. Our code is\navailable at: https://animate-your-word.github.io/demo/.",
        "updated": "2024-04-17 17:59:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11614v1"
    },
    {
        "title": "InFusion: Inpainting 3D Gaussians via Learning Depth Completion from Diffusion Prior",
        "authors": "Zhiheng LiuHao OuyangQiuyu WangKa Leong ChengJie XiaoKai ZhuNan XueYu LiuYujun ShenYang Cao",
        "links": "http://arxiv.org/abs/2404.11613v1",
        "entry_id": "http://arxiv.org/abs/2404.11613v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11613v1",
        "summary": "3D Gaussians have recently emerged as an efficient representation for novel\nview synthesis. This work studies its editability with a particular focus on\nthe inpainting task, which aims to supplement an incomplete set of 3D Gaussians\nwith additional points for visually harmonious rendering. Compared to 2D\ninpainting, the crux of inpainting 3D Gaussians is to figure out the\nrendering-relevant properties of the introduced points, whose optimization\nlargely benefits from their initial 3D positions. To this end, we propose to\nguide the point initialization with an image-conditioned depth completion\nmodel, which learns to directly restore the depth map based on the observed\nimage. Such a design allows our model to fill in depth values at an aligned\nscale with the original depth, and also to harness strong generalizability from\nlargescale diffusion prior. Thanks to the more accurate depth completion, our\napproach, dubbed InFusion, surpasses existing alternatives with sufficiently\nbetter fidelity and efficiency under various complex scenarios. We further\ndemonstrate the effectiveness of InFusion with several practical applications,\nsuch as inpainting with user-specific texture or with novel object insertion.",
        "updated": "2024-04-17 17:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11613v1"
    },
    {
        "title": "VG4D: Vision-Language Model Goes 4D Video Recognition",
        "authors": "Zhichao DengXiangtai LiXia LiYunhai TongShen ZhaoMengyuan Liu",
        "links": "http://arxiv.org/abs/2404.11605v1",
        "entry_id": "http://arxiv.org/abs/2404.11605v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11605v1",
        "summary": "Understanding the real world through point cloud video is a crucial aspect of\nrobotics and autonomous driving systems. However, prevailing methods for 4D\npoint cloud recognition have limitations due to sensor resolution, which leads\nto a lack of detailed information. Recent advances have shown that\nVision-Language Models (VLM) pre-trained on web-scale text-image datasets can\nlearn fine-grained visual concepts that can be transferred to various\ndownstream tasks. However, effectively integrating VLM into the domain of 4D\npoint clouds remains an unresolved problem. In this work, we propose the\nVision-Language Models Goes 4D (VG4D) framework to transfer VLM knowledge from\nvisual-text pre-trained models to a 4D point cloud network. Our approach\ninvolves aligning the 4D encoder's representation with a VLM to learn a shared\nvisual and text space from training on large-scale image-text pairs. By\ntransferring the knowledge of the VLM to the 4D encoder and combining the VLM,\nour VG4D achieves improved recognition performance. To enhance the 4D encoder,\nwe modernize the classic dynamic point cloud backbone and propose an improved\nversion of PSTNet, im-PSTNet, which can efficiently model point cloud videos.\nExperiments demonstrate that our method achieves state-of-the-art performance\nfor action recognition on both the NTU RGB+D 60 dataset and the NTU RGB+D 120\ndataset. Code is available at \\url{https://github.com/Shark0-0/VG4D}.",
        "updated": "2024-04-17 17:54:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11605v1"
    },
    {
        "title": "Variational Bayesian Last Layers",
        "authors": "James HarrisonJohn WillesJasper Snoek",
        "links": "http://arxiv.org/abs/2404.11599v1",
        "entry_id": "http://arxiv.org/abs/2404.11599v1",
        "pdf_url": "http://arxiv.org/pdf/2404.11599v1",
        "summary": "We introduce a deterministic variational formulation for training Bayesian\nlast layer neural networks. This yields a sampling-free, single-pass model and\nloss that effectively improves uncertainty estimation. Our variational Bayesian\nlast layer (VBLL) can be trained and evaluated with only quadratic complexity\nin last layer width, and is thus (nearly) computationally free to add to\nstandard architectures. We experimentally investigate VBLLs, and show that they\nimprove predictive accuracy, calibration, and out of distribution detection\nover baselines across both regression and classification. Finally, we\ninvestigate combining VBLL layers with variational Bayesian feature learning,\nyielding a lower variance collapsed variational inference method for Bayesian\nneural networks.",
        "updated": "2024-04-17 17:50:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2404.11599v1"
    }
]