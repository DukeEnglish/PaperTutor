[
    {
        "title": "Vinoground: Scrutinizing LMMs over Dense Temporal Reasoning with Short Videos",
        "authors": "Jianrui ZhangMu CaiYong Jae Lee",
        "links": "http://arxiv.org/abs/2410.02763v1",
        "entry_id": "http://arxiv.org/abs/2410.02763v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02763v1",
        "summary": "There has been growing sentiment recently that modern large multimodal models\n(LMMs) have addressed most of the key challenges related to short video\ncomprehension. As a result, both academia and industry are gradually shifting\ntheir attention towards the more complex challenges posed by understanding\nlong-form videos. However, is this really the case? Our studies indicate that\nLMMs still lack many fundamental reasoning capabilities even when dealing with\nshort videos. We introduce Vinoground, a temporal counterfactual LMM evaluation\nbenchmark encompassing 1000 short and natural video-caption pairs. We\ndemonstrate that existing LMMs severely struggle to distinguish temporal\ndifferences between different actions and object transformations. For example,\nthe best model GPT-4o only obtains ~50% on our text and video scores, showing a\nlarge gap compared to the human baseline of ~90%. All open-source multimodal\nmodels and CLIP-based models perform much worse, producing mostly random chance\nperformance. Through this work, we shed light onto the fact that temporal\nreasoning in short videos is a problem yet to be fully solved. The dataset and\nevaluation code are available at https://vinoground.github.io.",
        "updated": "2024-10-03 17:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02763v1"
    },
    {
        "title": "Erasing Conceptual Knowledge from Language Models",
        "authors": "Rohit GandikotaSheridan FeuchtSamuel MarksDavid Bau",
        "links": "http://arxiv.org/abs/2410.02760v1",
        "entry_id": "http://arxiv.org/abs/2410.02760v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02760v1",
        "summary": "Concept erasure in language models has traditionally lacked a comprehensive\nevaluation framework, leading to incomplete assessments of effectiveness of\nerasure methods. We propose an evaluation paradigm centered on three critical\ncriteria: innocence (complete knowledge removal), seamlessness (maintaining\nconditional fluent generation), and specificity (preserving unrelated task\nperformance). Our evaluation metrics naturally motivate the development of\nErasure of Language Memory (ELM), a new method designed to address all three\ndimensions. ELM employs targeted low-rank updates to alter output distributions\nfor erased concepts while preserving overall model capabilities including\nfluency when prompted for an erased concept. We demonstrate ELM's efficacy on\nbiosecurity, cybersecurity, and literary domain erasure tasks. Comparative\nanalysis shows that ELM achieves superior performance across our proposed\nmetrics, including near-random scores on erased topic assessments, generation\nfluency, maintained accuracy on unrelated benchmarks, and robustness under\nadversarial attacks. Our code, data, and trained models are available at\nhttps://elm.baulab.info",
        "updated": "2024-10-03 17:59:30 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02760v1"
    },
    {
        "title": "CorPipe at CRAC 2024: Predicting Zero Mentions from Raw Text",
        "authors": "Milan Straka",
        "links": "http://arxiv.org/abs/2410.02756v1",
        "entry_id": "http://arxiv.org/abs/2410.02756v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02756v1",
        "summary": "We present CorPipe 24, the winning entry to the CRAC 2024 Shared Task on\nMultilingual Coreference Resolution. In this third iteration of the shared\ntask, a novel objective is to also predict empty nodes needed for zero\ncoreference mentions (while the empty nodes were given on input in previous\nyears). This way, coreference resolution can be performed on raw text. We\nevaluate two model variants: a~two-stage approach (where the empty nodes are\npredicted first using a pretrained encoder model and then processed together\nwith sentence words by another pretrained model) and a single-stage approach\n(where a single pretrained encoder model generates empty nodes, coreference\nmentions, and coreference links jointly). In both settings, CorPipe surpasses\nother participants by a large margin of 3.9 and 2.8 percent points,\nrespectively. The source code and the trained model are available at\nhttps://github.com/ufal/crac2024-corpipe .",
        "updated": "2024-10-03 17:58:55 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02756v1"
    },
    {
        "title": "SIEVE: General Purpose Data Filtering System Matching GPT-4o Accuracy at 1% the Cost",
        "authors": "Jifan ZhangRobert Nowak",
        "links": "http://arxiv.org/abs/2410.02755v1",
        "entry_id": "http://arxiv.org/abs/2410.02755v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02755v1",
        "summary": "Creating specialized large language models requires vast amounts of clean,\nspecial purpose data for training and fine-tuning. With only a handful of\nexisting large-scale, domain-specific datasets, creation of new datasets is\nrequired in most applications. This requires the development of new\napplication-specific filtering of web-scale data. Filtering with a\nhigh-performance, general-purpose LLM such as GPT-4o can be highly effective,\nbut this is extremely expensive at web-scale. This paper proposes SIEVE, a\nlightweight alternative that matches GPT-4o accuracy at a fraction of the cost.\nSIEVE can perform up to 500 filtering operations for the cost of one GPT-4o\nfiltering call. The key to SIEVE is a seamless integration of GPT-4o and\nlightweight T5 models, using active learning to fine-tune T5 in the background\nwith a small number of calls to GPT-4o. Once trained, it performs as well as\nGPT-4o at a tiny fraction of the cost. We experimentally validate SIEVE on the\nOpenWebText dataset, using five highly customized filter tasks targeting high\nquality and domain-specific content. Our results demonstrate the effectiveness\nand efficiency of our method in curating large, high-quality datasets for\nlanguage model training at a substantially lower cost (1%) than existing\ntechniques. To further validate SIEVE, experiments show that SIEVE and GPT-4o\nachieve similar accuracy, with human evaluators preferring SIEVE's filtering\nresults to those of GPT-4o.",
        "updated": "2024-10-03 17:58:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02755v1"
    },
    {
        "title": "Training Language Models on Synthetic Edit Sequences Improves Code Synthesis",
        "authors": "Ulyana PiterbargLerrel PintoRob Fergus",
        "links": "http://arxiv.org/abs/2410.02749v1",
        "entry_id": "http://arxiv.org/abs/2410.02749v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02749v1",
        "summary": "Software engineers mainly write code by editing existing programs. In\ncontrast, large language models (LLMs) autoregressively synthesize programs in\na single pass. One explanation for this is the scarcity of open-sourced edit\ndata. While high-quality instruction data for code synthesis is already scarce,\nhigh-quality edit data is even scarcer. To fill this gap, we develop a\nsynthetic data generation algorithm called LintSeq. This algorithm refactors\nexisting code into a sequence of code edits by using a linter to procedurally\nsample across the error-free insertions that can be used to sequentially write\nprograms. It outputs edit sequences as text strings consisting of consecutive\nprogram diffs. To test LintSeq, we use it to refactor a dataset of instruction\n+ program pairs into instruction + program-diff-sequence tuples. Then, we\ninstruction finetune a series of smaller LLMs ranging from 2.6B to 14B\nparameters on both the re-factored and original versions of this dataset,\ncomparing zero-shot performance on code synthesis benchmarks. We show that\nduring repeated sampling, edit sequence finetuned models produce more diverse\nprograms than baselines. This results in better inference-time scaling for\nbenchmark coverage as a function of samples, i.e. the fraction of problems\n\"pass@k\" solved by any attempt given \"k\" tries. For example, on HumanEval\npass@50, small LLMs finetuned on synthetic edit sequences are competitive with\nGPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%)\nin absolute score. Finally, we also pretrain our own tiny LMs for code\nunderstanding. We show that finetuning tiny models on synthetic code edits\nresults in state-of-the-art code synthesis for the on-device model class. Our\n150M parameter edit sequence LM matches or outperforms code models with twice\nas many parameters, both with and without repeated sampling, including Codex\nand AlphaCode.",
        "updated": "2024-10-03 17:57:22 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02749v1"
    }
]