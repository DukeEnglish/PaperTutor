[
    {
        "title": "Grounded Answers for Multi-agent Decision-making Problem through Generative World Model",
        "authors": "Zeyang LiuXinrui YangShiguang SunLong QianLipeng WanXingyu ChenXuguang Lan",
        "links": "http://arxiv.org/abs/2410.02664v1",
        "entry_id": "http://arxiv.org/abs/2410.02664v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02664v1",
        "summary": "Recent progress in generative models has stimulated significant innovations\nin many fields, such as image generation and chatbots. Despite their success,\nthese models often produce sketchy and misleading solutions for complex\nmulti-agent decision-making problems because they miss the trial-and-error\nexperience and reasoning as humans. To address this limitation, we explore a\nparadigm that integrates a language-guided simulator into the multi-agent\nreinforcement learning pipeline to enhance the generated answer. The simulator\nis a world model that separately learns dynamics and reward, where the dynamics\nmodel comprises an image tokenizer as well as a causal transformer to generate\ninteraction transitions autoregressively, and the reward model is a\nbidirectional transformer learned by maximizing the likelihood of trajectories\nin the expert demonstrations under language guidance. Given an image of the\ncurrent state and the task description, we use the world model to train the\njoint policy and produce the image sequence as the answer by running the\nconverged policy on the dynamics model. The empirical results demonstrate that\nthis framework can improve the answers for multi-agent decision-making problems\nby showing superior performance on the training and unseen tasks of the\nStarCraft Multi-Agent Challenge benchmark. In particular, it can generate\nconsistent interaction sequences and explainable reward functions at\ninteraction states, opening the path for training generative models of the\nfuture.",
        "updated": "2024-10-03 16:49:59 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02664v1"
    },
    {
        "title": "Agents' Room: Narrative Generation through Multi-step Collaboration",
        "authors": "Fantine HuotReinald Kim AmplayoJennimaria PalomakiAlice Shoshana JakobovitsElizabeth ClarkMirella Lapata",
        "links": "http://arxiv.org/abs/2410.02603v1",
        "entry_id": "http://arxiv.org/abs/2410.02603v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02603v1",
        "summary": "Writing compelling fiction is a multifaceted process combining elements such\nas crafting a plot, developing interesting characters, and using evocative\nlanguage. While large language models (LLMs) show promise for story writing,\nthey currently rely heavily on intricate prompting, which limits their use. We\npropose Agents' Room, a generation framework inspired by narrative theory, that\ndecomposes narrative writing into subtasks tackled by specialized agents. To\nillustrate our method, we introduce Tell Me A Story, a high-quality dataset of\ncomplex writing prompts and human-written stories, and a novel evaluation\nframework designed specifically for assessing long narratives. We show that\nAgents' Room generates stories that are preferred by expert evaluators over\nthose produced by baseline systems by leveraging collaboration and\nspecialization to decompose the complex story writing task into tractable\ncomponents. We provide extensive analysis with automated and human-based\nmetrics of the generated output.",
        "updated": "2024-10-03 15:44:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02603v1"
    },
    {
        "title": "Learning Emergence of Interaction Patterns across Independent RL Agents in Multi-Agent Environments",
        "authors": "Vasanth Reddy BaddamSuat GumussoyAlmuatazbellah BokerHoda Eldardiry",
        "links": "http://arxiv.org/abs/2410.02516v1",
        "entry_id": "http://arxiv.org/abs/2410.02516v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02516v1",
        "summary": "Many real-world problems, such as controlling swarms of drones and urban\ntraffic, naturally lend themselves to modeling as multi-agent reinforcement\nlearning (RL) problems. However, existing multi-agent RL methods often suffer\nfrom scalability challenges, primarily due to the introduction of communication\namong agents. Consequently, a key challenge lies in adapting the success of\ndeep learning in single-agent RL to the multi-agent setting. In response to\nthis challenge, we propose an approach that fundamentally reimagines\nmulti-agent environments. Unlike conventional methods that model each agent\nindividually with separate networks, our approach, the Bottom Up Network (BUN),\nadopts a unique perspective. BUN treats the collective of multi-agents as a\nunified entity while employing a specialized weight initialization strategy\nthat promotes independent learning. Furthermore, we dynamically establish\nconnections among agents using gradient information, enabling coordination when\nnecessary while maintaining these connections as limited and sparse to\neffectively manage the computational budget. Our extensive empirical\nevaluations across a variety of cooperative multi-agent scenarios, including\ntasks such as cooperative navigation and traffic control, consistently\ndemonstrate BUN's superiority over baseline methods with substantially reduced\ncomputational costs.",
        "updated": "2024-10-03 14:25:02 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02516v1"
    },
    {
        "title": "Choices are More Important than Efforts: LLM Enables Efficient Multi-Agent Exploration",
        "authors": "Yun QuBoyuan WangYuhang JiangJianzhun ShaoYixiu MaoCheems WangChang LiuXiangyang Ji",
        "links": "http://arxiv.org/abs/2410.02511v1",
        "entry_id": "http://arxiv.org/abs/2410.02511v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02511v1",
        "summary": "With expansive state-action spaces, efficient multi-agent exploration remains\na longstanding challenge in reinforcement learning. Although pursuing novelty,\ndiversity, or uncertainty attracts increasing attention, redundant efforts\nbrought by exploration without proper guidance choices poses a practical issue\nfor the community. This paper introduces a systematic approach, termed LEMAE,\nchoosing to channel informative task-relevant guidance from a knowledgeable\nLarge Language Model (LLM) for Efficient Multi-Agent Exploration. Specifically,\nwe ground linguistic knowledge from LLM into symbolic key states, that are\ncritical for task fulfillment, in a discriminative manner at low LLM inference\ncosts. To unleash the power of key states, we design Subspace-based Hindsight\nIntrinsic Reward (SHIR) to guide agents toward key states by increasing reward\ndensity. Additionally, we build the Key State Memory Tree (KSMT) to track\ntransitions between key states in a specific task for organized exploration.\nBenefiting from diminishing redundant explorations, LEMAE outperforms existing\nSOTA approaches on the challenging benchmarks (e.g., SMAC and MPE) by a large\nmargin, achieving a 10x acceleration in certain scenarios.",
        "updated": "2024-10-03 14:21:23 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02511v1"
    },
    {
        "title": "SwarmCVT: Centroidal Voronoi Tessellation-Based Path Planning for Very-Large-Scale Robotics",
        "authors": "James GaoJacob LeeYuting ZhouYunze HuChang LiuPingping Zhu",
        "links": "http://arxiv.org/abs/2410.02510v1",
        "entry_id": "http://arxiv.org/abs/2410.02510v1",
        "pdf_url": "http://arxiv.org/pdf/2410.02510v1",
        "summary": "Swarm robotics, or very large-scale robotics (VLSR), has many meaningful\napplications for complicated tasks. However, the complexity of motion control\nand energy costs stack up quickly as the number of robots increases. In\naddressing this problem, our previous studies have formulated various methods\nemploying macroscopic and microscopic approaches. These methods enable\nmicroscopic robots to adhere to a reference Gaussian mixture model (GMM)\ndistribution observed at the macroscopic scale. As a result, optimizing the\nmacroscopic level will result in an optimal overall result. However, all these\nmethods require systematic and global generation of Gaussian components (GCs)\nwithin obstacle-free areas to construct the GMM trajectories. This work\nutilizes centroidal Voronoi tessellation to generate GCs methodically.\nConsequently, it demonstrates performance improvement while also ensuring\nconsistency and reliability.",
        "updated": "2024-10-03 14:17:20 UTC",
        "interpretation": "解释内容未找到",
        "id": "2410.02510v1"
    }
]