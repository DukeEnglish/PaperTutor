Learning Emergence of Interaction Patterns across
Independent RL Agents in Multi-Agent Environments
VasanthReddyBaddam SuatGumussoy
Virginiatech SiemensTechnology
Arlington,VA,UnitedStates Princeton,NJ,UnitedStates
vasanth2608@vt.edu suat.gumussoy@siemens.com
AlmuatazbellahBoker HodaEldardiry
Virginiatech Virginiatech
Arlington,VA,UnitedStates Blacksburg,VA,UnitedStates
boker@vt.edu hdardiry@vt.edu
ABSTRACT applicationswithmultipleagents,interactionsamongagentsarein-
Manyreal-worldproblems,suchascontrollingswarmsofdronesand frequent.Forexample,thinkoftworobotsdeliveringdocumentsin
urbantraffic,naturallylendthemselvestomodelingasmulti-agentre- anoffice.Theydonotneedtocloselymonitoreachotheroften,only
inforcementlearning(RL)problems.However,existingmulti-agent whentheyhavetopassthroughthesamedoor.Similarly,agroupof
RLmethodsoftensufferfromscalabilitychallenges,primarilydue dronesmightbesentonarescuemissionwherecommunicationis
totheintroductionofcommunicationamongagents.Consequently,a challenging.Theycannotalwaysshareinformation,sotheyneedto
keychallengeliesinadaptingthesuccessofdeeplearninginsingle- beinnovativeandcooperateonlywhennecessary.
agentRLtothemulti-agentsetting.Inresponsetothischallenge, Weaskafundamentalquestion:"Wheniscoordinationessential?"
weproposeanapproachthatfundamentallyreimaginesmulti-agent andifneeded,"Howinfrequentcaninteractionsbe?".Ourapproach
environments.Unlikeconventionalmethodsthatmodeleachagent aimstouselocalinteractions,allowingagentstoactindependentlyas
individuallywithseparatenetworks,ourapproach,theBottomUp muchaspossibleandkeepingcommunicationminimal.Thispaper
Network(BUN),adoptsauniqueperspective.BUNtreatsthecollec- focuses on cooperative MARL scenarios in partially observable
tiveofmulti-agentsasaunifiedentitywhileemployingaspecialized environmentswhereagentshavevaryingobservationabilities.
weightinitializationstrategythatpromotesindependentlearning. ThispaperconsidersMARLscenarioswhereinthetaskiscooper-
Furthermore,wedynamicallyestablishconnectionsamongagents ativeandagentsaresituatedinapartiallyobservableenvironment.
usinggradientinformation,enablingcoordinationwhennecessary However,eachisendowedwithdifferentobservationpower.This
whilemaintainingtheseconnectionsaslimitedandsparsetoeffec- paperaimstotacklethisproblembyreimaginingthemulti-agent
tivelymanagethecomputationalbudget.Ourextensiveempirical settingbyredefiningtherepresentationofthemulti-agentsystem,
evaluationsacrossavarietyofcooperativemulti-agentscenarios, treatingitasasingleagentwithauniqueneuralnetworkweight
includingtaskssuchascooperativenavigationandtrafficcontrol, initializationscheme.Thisschemeensuresdecentralizedoperation,
consistentlydemonstrateBUN’ssuperiorityoverbaselinemethods allowingeachagenttoactlocallyandindependentlywithoutinteract-
withsubstantiallyreducedcomputationalcosts. ingwithotheragentsduringexecution.Subsequently,weleverage
gradient information among agents to establish connections, en-
KEYWORDS ablingcoordinationwhennecessary.Importantly,wemaintainthese
connectionsaslimitedandsparse.
SparseTraining,Multi-AgentEnvironments,Communication
Contributions:Inthispaper,weintroduceanapproachinspired
bythebottom-upapproachdescribedfromourpreviouswork[21],
1 INTRODUCTION BottomUpNetwork,denotedasBUN,distinguishedbyaunique
weightinitializationstrategythatissparseanddecentralized.Unlike
Multi-AgentReinforcementLearning(MARL)isanexcitingfield
conventional dense-weight initialization methods, BUN adopts a
withinartificialintelligenceandmachinelearning.Ithasbecome
sparseinitializationapproach,ensuringthatagentsmakeindepen-
increasinglycrucialfortacklingreal-worldproblemswheremulti-
dentlocaldecisionswhileminimizingcomputationalcostsdueto
pleautonomousagentsinteractincomplexenvironments.However,
itssparsity.Thisinitializationfosterslocaldecision-makingamong
MARLcanbecomputationallyexpensiveduetoagentsneedingto
agents,andweightsemergeusinggradientinformationduringtrain-
interactwitheachother.Ontheotherhand,Independentlearning
ing.Thisweightemergencenotonlyfosterscoordinationbutalso
algorithmshavepracticaladvantages.Theyrequirefewercompu-
revealsthetopologyoftheagentsintheenvironment.Throughex-
tationalresourcesandcanscaletolargerenvironments.However,
tensiveempiricalevaluationsincooperativemulti-agentscenarios,
theyfacechallengesinmulti-agentsettingsduetonon-stationarity,
ourapproachshowcasestheadvantagesofsparseanddecentralized
whichcanaffecttraditionalreinforcementlearningguarantees.
initialization,significantlycontributingtomulti-agentreinforcement
Arecentstudyby[11]demonstratedthatindependentlearning
learningbyaddressingnon-stationaritychallengesandpromoting
algorithmscanperformexceptionallywellincooperativescenarios
efficientdecentralizeddecision-makingandcommunicationamong
whenindividualagentshavefullobservability.However,fullob-
agents.
servabilityisoftencomputationallyexpensive.Inmanyreal-world
4202
tcO
3
]AM.sc[
1v61520.0142:viXraInourevaluationofBUN,wetestitintwoapplications:Coop- rewardsacrossvarioustasks.Otherapproaches,suchas[1,4,10],
erativeNavigationandTrafficSignalControlenvironments.Our utilizeadot-productattentionmechanismforinter-agentcommu-
experimentsconsistentlyshowthatBUNoutperformsbaselinemeth- nication, restricting communication to an agent’s neighbours to
ods.Notably,itachievesperformancelevelssimilartomethodswith mitigatecomputationaloverhead.However,learningcommunication
fullcommunicationwhileusingsignificantlyfewercomputational protocolsacrossagentsusingsuchmethodscanbecomputationally
resources.ItisimportanttonotethatBUNcomplementsexisting intensive.Inourwork,weaimtoaddressthischallengebyfocusing
cooperativemulti-agentalgorithms,makingthemmorepracticaland onlearningsharedparametersthatareassparseaspossible.
cost-effectiveforreal-worldapplications. Recently, a notable interest has been in training sparse neural
networkswithinDeepReinforcementLearning(DRL).In[14],they
proposedthePolicyPruningandShrinking(PoPs)method.This
2 RELATEDWORK
approachincorporatesiterativepolicypruningasanintermediary
Multi-agentReinforcementLearning(MARL)hasgarneredsignif- steptoguidethedimensionsofdenseneuralnetworksspecifically
icant attention, with Independent Q-learning (IQL) being one of tailoredforDRLagents.TurningtoexploringtheRiggedLottery
thepioneeringapproaches.IQLtrainsdistinctQ-valuefunctions Ticket(RigL)HypothesisinDRL,theworkin[6]investigatedthis
for each agent, assuming that other agents remain static compo- phenomenon.Incontrast,theworkin[25]madesubstantialstrides
nentswithintheenvironment.However,thismethodfacesanotable bydemonstratingthediscoveryofsparse"winningtickets"through
challengewhentheenvironmentbecomesnon-stationary,leading behaviourcloning(BC),offeringinvaluableinsightsintotheiniti-
toinstability issues, mainlywhenapplied tolarge-scalesystems. ationofsparsenetworks.Intheresearch[6],aspotlightwascast
Recentresearch[11]hasilluminatedscenariosinwhichIQLcan onthechallengesassociatedwithtrainingDRLagentsusingsparse
demonstrate effectiveness within multi-agent systems by relying neuralnetworksfromtheinherenttraininginstabilitywithinsuch
solelyonlocalobservations,eliminatingthenecessityforcoordi- networks.Buildinguponthisfoundation,theworkin[22]delved
nation.Similarly,IA2CandIPPOhaveshownpromiseinspecific deeperintotheissueoftraininginstabilityinsparseDRLagents.
environmentswhereagentcoordinationmaynotbenecessaryde- Theirworknotonlyshedlightonthechallengesbutalsounderscored
spitethenon-stationarityconcern.Itisworthnotingthatincases theinherentlimitationsofachievingstabletraininginthesenetworks.
wherelearnedbehaviourscannotreplacecoordination,directcom- Theirworkresponseintroduceddynamicsparsetraining,theSET
municationbetweenagentsremainsindispensable. algorithm,whichenablesend-to-endtrainingofsparsenetworks
Recentworkshavefocusedontrainingactor-criticalgorithmsto within actor-critic algorithms, ultimately achieving a reasonable
addressthechallengesposedbynon-stationarityandcoordination. sparsitylevel.Furtheradvancesinthefieldweremadein[2,12].
Intheseapproaches,thecriticiscentralizedandutilizesglobalin- Theseresearchworksfocusedontrainingsparseneuralnetworks
formationduringtraining,whileactorsemploylocalinformation fromthegroundup,eliminatingtherelianceonpre-traineddense
duringexecution.MADDPG[16]learnsacentralizedcriticforeach models. In this work [12], they proposed an approach involving
agentbyprovidingallagents’jointstateandactionstothecritic. block-circulantmasksintheearlystagesoftraining,significantly
PoliciesforeachagentaretrainedusingtheDDPGalgorithm[13]. enhancingpruningefficiencyinTD3agents.Whilein[2],theyintro-
COMA[8]alsoemploysacentralizedcriticbutestimatesacoun- ducedauniqueparadigmbyapplyingone-shotpruningalgorithmsin
terfactualadvantagefunctiontoassistwithmulti-agentcreditas- offlinereinforcementlearning(RL)settings.Conductinganexhaus-
signment,isolatingtheimpactofeachagent’sactions.VDN[24] tiveinvestigationintothevariousmethodsemployedinthisdomain,
decomposesacentralizedstate-actionvaluefunctionintoasumof thework[9]conductedacomprehensivecomparativestudy.Their
individualagent-specificfunctions.However,thisdecompositionim- workemphasizedtheeffectivenessofpruningandhighlightedthe
posesastrictprior,whichneedstobethoroughlyjustifiedandlimits substantialimprovementsattainedcomparedtoconventionalstatic
thecomplexityoftheagents’learnedvaluefunctions.Q-Mix[20] sparsetrainingtechniques.However,itisworthnotingthatmostof
buildsuponthisbyremovingtheneedforadditivedecomposition theseworksprimarilyrevolvearoundDRLalgorithmstailoredto
ofthecentralcritic,insteadimposingalessrestrictivemonotonicity single-agentenvironments.Inourpaper,weventurebeyondthese
requirementonagents’state-actionvaluefunctions.Thisapproach boundariesbyapplyingthesemethodologiestotacklethechallenges
allowsforalearnablemixingofindividualfunctionswithoutrestrict- ofsparsityinmulti-agentenvironments.Additionally,weaddressthe
ingthecomplexityofthefunctionsthatcanbelearned.Nevertheless, dynamicdeterminationofnetworktopologyduringsparsetraining,
thesemethodsdonotleveragethestructuralinformationinherent addinganoveldimensiontothisevolvingfieldofresearch.
intheenvironment.Intheseapproaches,eachagent’sobservation
typicallyconsistsofaconcatenationofthestatesofotheragents 3 BACKGROUND
andvariousenvironmentalfeatures.Additionally,usingacentralized
3.1 ReinforcementLearning
criticpreventsthelearnedpolicyfromgeneralizingtoscenarioswith
feweragentsthanencounteredduringtraining.Moreover,granting 3.1.1 Single Agent. RLisasubfieldofmachinelearningthat
accesstotheglobalstateforadensecentralizedcriticincurssignifi- focusesontrainingagentstomakeoptimaldecisionsinsequential
cantcomputationalcosts.Totacklethesechallenges,severalprior andcontinuousenvironments.InRL,anagentgenerallyinteracts
studieshaveexploredthelearningofcommunicationprotocolsthat withanenvironmentoveraseriesofdiscretetimesteps.Theagent
exploittheunderlyingenvironmentalstructure[7,19,23].These perceivesthecurrentstateoftheenvironmentandtakesactionsto
studiestrainmultipleagentstoacquireacommunicationprotocol influencetheenvironment.Theenvironment,inturn,respondsto
andhavedemonstratedthatcommunicatingagentsachieveenhanced theagent’sactionsbytransitioningtoanewstateandprovidingarewardsignal.Thestatespacerepresentsallpossiblesituationsor 3.1.3 DQN. DeepQ-Networks(DQN)[17]areaclassofrein-
configurationsoftheenvironment.Theactionspacerepresentsall forcementlearningalgorithmsthatcombineQ-learningwithdeep
possibleactionsthattheagentcantake.Theagent’staskistolearn neuralnetworks,whicharehighlyeffectiveforfunctionapproxima-
apolicy,whichisamappingfromstatestoactions,thatmaximizes tion.Atitscore,DQNaimstoapproximatetheoptimalaction-value
theexpectedcumulativerewardovertime.Thiscanbeformalized
function,denotedas𝑄𝜋(s,a).Theweightparameters𝜃
aretrained
𝜃
as a Markov Decision Process(MDP). MDP is defined as tuple, usingatemporaldifferencelossfromtransitionssampledfromex-
⟨S,A,𝑃,𝑟,𝛾⟩, where S is the state space, A is the action space, periencereplaybufferD:
𝑃 :S×A→Δ(𝑆)definesthetransitiondynamics,𝑟 :S×A→R (cid:20) (cid:18) (cid:19)(cid:21)
istherewardfunction,and𝛾 ∈ [0,1)isadiscountfactor. L(𝜃)=E (𝑠,𝑎,𝑟,𝑠′)∼D 𝑄 𝜃(𝑠,𝑎)− 𝑟+𝛾 max𝑄 𝜃¯(cid:0)𝑠′,𝑎′(cid:1) (1)
𝑎′∈A
The policy 𝜋 : S → Δ(A) is the strategy that the agent em-
ploys to select actions 𝑎 ∈ Abased on the current state 𝑠 ∈ S. where𝜃¯aretheweightparametersofthetargetnetwork.Thetarget
It defines the agent’s behaviour and can be deterministic or sto- networkisacopyoftheonlinenetworkandisusedtoestimatethe
chastic. The environment rewards the agent 𝑟 signal after each Q-valuesinthetarget.Ithelpsinstabilizingthetrainingprocessby
action.Theagent’sobjectiveistolearnapolicythatleadstoac- providingaconsistenttargetforQ-valueapproximation.
tionsthatmaximizethetotalcumulativerewardovertime.Some-
4 BUN:BOTTOMUPNETWORK
times,costscanbeusedinsteadofrewards,andthegoalbecomes
minimizingthecumulativecost.Apolicy𝜋 formalizesanagent’s Inpracticalcooperativescenarios,agentsareoftendistributed,each
behaviour and the associated value function 𝑉𝜋 : S → R de- equippedwithitslocalobservations,actions,andlocalobjective
fined as: 𝑉𝜋(𝑠) := E 𝑎∼𝜋(𝑥) (cid:2) R(𝑥,𝑎)+𝛾E 𝑥′∼P(𝑥,𝑎)𝑉𝜋 (𝑥′)(cid:3) and rewards.Althoughseparatedwithintheirindividualnetworks,these
state-action value functions 𝑄𝜋 : S × A → R as: 𝑄𝜋(𝑠,𝑎) := agentscancommunicateoverasharedmediumtoworkcollabo-
R(𝑠,𝑎)+𝛾E 𝑥′∼P(𝑠,𝑎)𝑉𝜋 (𝑥′). rativelytowardsachievingglobalobjectives.Inthispaper,wein-
troduceamulti-agentreinforcementlearningproblemformulation,
treatingitasasingle-agentproblemwithmulti-discreteactionsas
3.1.2 Multi-AgentReinforcementLearning. Multi-agentrein-
illustratedinFigure1.Ourmethodologyleveragesastraightforward
forcement Learning (MARL) is an extension of single-agent re-
yeteffectiveDQN(DeepQ-Network)algorithm.However,wecan
inforcementlearningthatdealswithscenarioswheremultipleau-
useanystandardreinforcementlearningalgorithmwiththeprovided
tonomousagentsinteractwithinasharedenvironment.Eachagent
network initialization. BUN starts with a sparse network, and at
seekstolearnapolicythatmaximizesitsexpectedcumulativere-
regularlyspacedintervals,newconnectionsemergeusinggradient
wardovertimewhileconsideringtheactionsandstrategiesofother
information.Afterupdatingtheconnectivity,trainingcontinueswith
agents.Thisfieldiscriticalwhenaddressingproblemsinvolving
theupdatednetworkuntilthenextupdate.Themainpartsofoural-
coordination,competition,andcollaborationamongmultipleagents.
gorithm,NetworkInitialization,MainObjective,WeightEmergence,
WeformalizeMARLusingDEC-POMDP[18],ageneralizationof
UpdateSchedule,andthevariousoptionsconsideredforeach,are
MDPtoallowdistributedcontrolbymultipleagentswhomaybe
explainedbelow.
incapableofobservingtheglobalstate.ADEC-POMDPisdescribed
(1)NeuralNetworkInitialization(𝜃0)AsshowninFig1,We
byatuple⟨S,A,R,𝑃,𝑠,O,𝛾⟩.ThejointstatespaceSencapsulates
haveasinglenetwork.Theinput𝑠forthenetworkisthelistofall
thecollectiveconfigurationofallagentsandtheenvironment.Simi-
larly,ajointactionspaceAincludesallpossiblecombinationsof
agent’sobservations,𝑠 = [𝑜 1,𝑜 2,···𝑜 N].Thegivensetofstates𝑠
outputisthelistofindividualactions,𝑎= [𝑎 1,𝑎 2,···𝑎 N].Thecore
actionsthateachagentcantake.Theinteractionbetweenagents
logicbehindBUNisbasedonthefollowingprinciples:1.BlockDi-
andtheenvironmentunfoldsoverdiscretetimesteps.Eachagent
agonalWeightInitialization(𝜃 𝑖):InBUN,weinitializetheneural
𝑖 ∈ N chooses an action𝑎 𝑖 ∈ A, forming a joint action vector
𝒂= [𝑎 𝑖] ∈A𝑛 andhaspartialobservations𝑜 𝑖 ∈𝑠.Theagents’joint network’sweightsfollowingablockdiagonalpattern.Thisapproach
allocatesspecificnetworkcomponentstohandleparticularagent
observations𝑠 = (o1,...,o𝑁) provideinsightsintothecollective
interactionsortasks.Byincorporatingagent-specificinformation
stateS,butfullknowledgeoftheenvironmentisoftenobscured.
directlyintothearchitecture,eachagentbenefitsfromdedicated
Similarly,theagent’srewardisgivenbythe𝑟(𝑜 𝑖,𝑎 𝑖) ∈R.Eachagent
𝑖 cit ea ske 𝜋s 𝑖a nc ot wion m𝑎 a𝑖 pb oa bs se ed rvo an tii ots no sw ton ap co til oic ny s.𝜋 T𝑖 h(𝑎 e𝑖 jo| i𝑜 n𝑖 t) v. aT luh ee fa ug ne cn tt is o’ np 𝑉ol 𝜋i- net 2w .o Zrk erc oom Inp io tin ae lin zt as, tif oo nste fori rng Os fp f-e Dci ia al giz oa nti ao ln Wan ed igt ha tr sge (t 𝜃e 𝑖d 𝑗l =ea 0rn ):in Ig n.
contrasttotheblockdiagonalweights,BUNinitializesoff-diagonal
capturestheexpectedcumulativerewardachievableunderthejoint
weightstozero.Thisdesignchoicepromotesisolationandinde-
policy𝜋.Itisdefinedsimilarlytobefore,consideringobservationsin-
steadofstates:𝑉𝜋(s) = E a∼𝜋(a|s) (cid:104) (cid:205) 𝑡∞ =0𝛾𝑡(cid:205) 𝑖𝑁 =1𝑟 𝑖(s𝑡,a𝑡)(cid:12) (cid:12) (cid:12)s0=o(cid:105) . p Be ynd seen ttc ine gbe thtw ese een wa eg igen ht ts sw tohe zn ert oh ,ey wd eo en no cot uh ra av ge ed air ge ec nt ti snt te ora ac ct tio an us -.
Thejointstate-actionvaluefunction𝑄𝜋
alsoadaptstopartialob- tonomouslywhentheiractionsdonotsignificantlyimpactordepend
servability:𝑄𝜋(s,a)=(cid:205) 𝑡∞ =0𝛾𝑡E s′,a′∼𝑃(s′,a′|s,a) (cid:2)(cid:205) 𝑖𝑁 =1𝑟 𝑖(s𝑡,a𝑡)(cid:3) .In oneachother,facilitatingefficientlocalizeddecision-making.
summary,multi-agentreinforcementlearningwithpartialobserva- (2)ObjectiveInthecaseofcollaborativemulti-agentreinforce-
tionsintroducesthecomplexityoflimitedinformation,requiring mentlearning,themulti-agentshaveasharedrewardsothatthere
agentstoadapttheirpoliciestomakeeffectivedecisionsbasedon isaglobalobjectivefunctionthatmustbeoptimizedbasedonthe
theirlocalobservations.Adaptingvaluefunctionsandpoliciestothe collaborativeeffortsoftheagentsasgivenin(1).However,inour
observationspaceenablesagentstohandlepartialobservabilityand work,weneedtooptimizetheobjectivefunctionwhileadheringto
learnoptimalstrategiesinchallengingenvironments. apredefinedbudgetofnetworkweights.Theoptimizationproblem(3)UpdateScheduleTheconnectionsemergebasedonapre-
definedscheduledeterminedbythefollowingparameters.𝑏 :The
number of additional connections need to emerge. Δ𝑇 : The fre-
quencyatwhichtheconnectionsneedtoemerge,𝑘 :Thenumber
ofconnectionsthatneedtoemergeateachupdate,𝑇 𝑠𝑡𝑎𝑟𝑡 Theit-
erationatwhichconnectionemergenceshouldstart.Weallowthe
uniformemergenceof𝑘 connectionsateachupdate,sampledata
Δ𝑇 frequencyuntilwereachthebudget𝑏.Thisgradualemergence
fostersstabilityandreliabilitybypreventingabruptandpotentially
unstablechangesinagentinteractions.Inaddition,itmakesiteasier
tointerpretthelearningprocessandcangaininsightsintohowthe
agentsadapttonewconnectionsandmakesenseofthelearning
dynamics.ThepseudoalgorithmforBUNisgiveninAlgorithm1.
DimensionalityWhenusingasinglenetwork,bothobservation
andactionspacecangrowexponentially.Forexample,ifweusea
Figure1:TheBUNapproachinvolvesatwo-step.1.WeightIni- DQNasthebasenetwork,thenumberofoutputnodeswilltypically
tialization:Weightsareinitializedsothat𝑖𝑡ℎ agent’sobservation growexponentially,dependingonthenumberofagents.Onewayto
𝑜 𝑖 isdirectlymappedtoitsaction𝑎 𝑖 withoutanydependenceon dealwiththeexponentialgrowthinthejointactionspaceistouse
theotheragent’sobservation.2.WeightEmergence:Wethen DDPGwithGumbel-Softmaxactionselectioniftheenvironment
growtheweightsacrosstheagentsaccordingtothehighestmag- is discrete to avoid the exploding number of input nodes of the
nitude gradient signal. The Green dotted line represents the observationspace,aswellasanexplodingnumberofoutputnodes
newlyemergedweights/connections. oftheactionspace.Underthisparadigm,theinputandoutputnodes
onlygrowlinearlywiththenumberofagents,astheoutputnodesof
aneuralnetworkinDDPGarethechosenjointaction,asopposedto
aDQN,wheretheoutputnodesmustenumerateallpossiblejoint
canbemathematicallyformulatedasfollows:
actions.
minimize L(𝜃)
(2)
subjectto ∥𝜃∥ℓ =𝑏+∥𝜃0∥ℓ
0 0
Algorithm1BUN
w
t oi
foh tne har ele bn𝑏
uet
di ws
go
eth trke copb nau srd
ta
rg
m
ae
ie
nt t, ter ∥re
s
𝜃p
t
∥r
h
ℓe 0as
t
=e cn
a
𝑏t ns +bt ∥h
e
𝜃e
a
0dl ∥i
d
ℓm 0ei
id
nt
t
to
o
on
𝜃
tht 0h e.e
T
obn
h
ju
e
em
cin
tb
ic
ve
o
er
rp
fo uof
nra
ca
t
td
i io
od
n
ni- 1 2:
:
I
b
Nn
u
ei dt ua
g
rl
e
ai tz l:e
N𝑏
e,N
tS
we ct ow
h
reo kdr
u
Ik
nle
i: ti:t ar
𝑘
la
i,
zi 𝑇n a𝑠i t𝑡n
i𝑎
og
𝑟
n𝑡n
,
𝜃e Δt 𝑇wo ,r 𝑇k 𝑒𝑛𝑄 𝑑𝜃, target network𝑄 𝜃¯,
wen itc ho iu nr ta hg ees spt eh ce ifiem ede brg ue dn gc ee t.o Tf hw ee pi ag rh at ms ein tet rh 𝑏e an le lotw wo sr ck ow nth roil le os vta ey ri tn hg
e
3 4:
:
for𝑡 Sain m𝑇 p𝑡 l𝑜 e𝑡𝑎 a𝑙 bd ao
tch<𝑠,𝑎,𝑟,𝑠′ >fromD
wde hg ir leee 𝑏o =ft 0he cos rp ra er ss pi oty n: da sl ta org the er𝑏 int ie tin ad ls spto arm seak ne ett whe orn ket iw nio tir ak lid ze an tis oe nr,
.
5 6:
:
if𝑡% foΔ r𝑇 ea= c= hl0 ayan erd 𝑙𝑇 d𝑠𝑡 o𝑎𝑟𝑡 <𝑡 <𝑇 𝑒𝑛𝑑 then
Overall,thisoptimizationproblemaimstostrikeabalancebetween 7: 𝑖,𝑗 =argmax(|∇ 𝜃𝑙 L(𝜃)|,𝑘)
minimizing the loss function and controlling the sparsity of the 𝑖𝑗
controller𝐹 withinthedefinedbudgetconstraints. 8: WeightEmergencethroughupdate;𝜃 𝑖𝑙 𝑗
(3) Weight Emergence Solving the optimization problem (2)
9: else
directlyishinderedbythepresenceofthesparsityconstraint.This
10:
𝜃 =𝜃−𝛼∇𝜃𝑖𝑗
constraintintroducesnon-convexityandcombinatorialcomplexity,
primarilybecausetheℓ 0normisusedtoquantifysparsity.Toover- 11:
Updatethetargetnetwork;𝜃¯=𝛽𝜃+(1−𝛽)𝜃¯
comethesechallenges,heuristicmethodsofferaneffectiveapproach,
and in this context, a Greedy Coordinate Descent algorithm [5]
provesparticularlyadvantageous.Thekeyideaistoiterativelyselect
networkparametersthatminimizetheoptimizationobjectivewhile
5 EXPERIMENTS
adheringtothesparsityconstraint.Thesameisexploredin[6,26]to
growtheconnectionsacrosstheneurons.Wefollowthesamegreedy ThissectionthoroughlyevaluatestheBottomUpNetwork(BUN)
principletoallowtheemergenceoftheweights.Duringtraining, frameworkwithintwocooperativeenvironments.Specifically,we
weightsemergeacrosstheagentsbasedonthehighestmagnitude considertheCooperativeNavigationtask,introducedby[16],and
gradientrule,representedas(𝑖,𝑗)=argmax(|∇𝜃𝑖𝑗L(𝜃)|,𝑘).Here, theTrafficSignalControl(TSC)taskasdescribedin[3].Toshow-
𝜃 𝑖𝑗 referstotheweightsacrosstheagentblocks.Giventheinitial casetheversatilityofourapproach,weextendourevaluationto
zeroinitializationofweights,thenewlyemergedweightsinitially encompassvariousadaptationsofCooperativeNavigationandTSC.
donotinfluencethenetwork’soutput.Selectingweightswiththe Detaileddescriptionsoftheseexperimentalenvironmentscanbe
highestgradientsensuresasubstantialreductioninthelossfunction foundinthesubsequentsubsections.Itisimportanttonotethat,for
duringtraining. thisstudy,weemploydiscreteactionsacrossallenvironments.5.1 BenchmarkAlgorithms TrafficSignalControlIntheTrafficSignalControlenvironment,
OurexperimentsaredesignedtocomparetheperformanceofBUN ourobjectiveistoassesstheeffectivenessofBUNinhandlingcom-
againstseveralbenchmarkmethods.Firstly,weevaluateBUNagainst plexanddynamicallychangingtrafficscenarios.Inthissimulated
independentQ-learning(WhichwerefertoastheDecentralized roadnetwork,eachagentisatrafficsignalcontrollerataninter-
method),which servesas thebaselineapproach. In this baseline section.Anagent’sobservationscompriseaone-hotrepresentation
method,eachagentoperatesindependentlywithoutcommunication. ofitscurrenttrafficsignalphase(indicatingdirectionsforredand
Toassesstheefficacyofthesparsenetworkarchitecture,wefur- greenlights)andthenumberofvehiclesoneachincominglaneat
thercompareBUNagainstRigL[6].Additionally,weinvestigate the intersection. At each time step, agents select a phase from a
theimpactofperformanceandtheutilizationofFloatingPointOp- predefinedsetfortheupcominginterval,typicallysetat10seconds.
erationsperSecond(FLOPS)comparedtodensenetworks.This Theoverarchingglobalobjectiveistominimizetheaveragewait-
involvescomparingBUNagainsttwokeyconfigurations:(i)Cen- ingtimeforallvehicleswithintheroadnetwork.Toconductthese
tralized learning (i.e., Dense Model), which represents an ideal experiments,weusetheframework[3]builtonSUMOtrafficsimu-
scenariowhereeachagenthasaccesstotheentireglobalstate,and lator[15].Specifically,weexperimentedwithtwodifferentnetwork
(ii) DGN [10], which utilizes an attention mechanism to enable configurations:a2x2gridnetworkfeaturingfourintersectionsand
communicationbetweenagents. asmallersectionoftheIngolstadtRegionnetworkcomprising7
intersections.Ourtrafficflowsimulationsencompassedavariety
ofscenarios,includingbothpeakandoff-peakperiods,toemulate
dynamictrafficpatternsasgivenin[3].
5.2 Environments
IntheCooperativeNavigationenvironment,wedeployascenario
featuringNagentsandNlandmarks,wheretheoverarchinggoalis
5.3 ImplementationDetails
forallagentstocoverallthedesignatedlandmarkswhileavoiding
collisionsefficiently.Tocomprehensivelyevaluatethecapabilitiesof Inourtrainingsetupacrossallenvironments,ourneuralnetwork
BUN,weemployvariousvariationsofthisenvironment.Whilewe architecturebeginswiththree(ReLU)layers,eachsizedat18times
provideabriefoverview,amoredetaileddescriptioncanbefound the number of agents. For the process of weight emergence, we
inthesupplementarymaterial. graduallyincreasethenetworkweights.Thisgrowthstartsatstep
Simple Spread (SS) In the Simple Spread task, N agents are 10,000(𝑇 𝑠𝑡𝑎𝑟𝑡)andcontinuesuntilstep30,000(𝑇 𝑒𝑛𝑑),withweight
taskedwithreachingNlandmarks.Eachagent’sobservationsen- updatesoccurringatintervalsof1,000steps(Δ𝑇),andeachupdate
compasstheirpositionandtherelativepositionsofthelandmark increasesthenumberofweightsbyafactorof3(𝑘 =3).Weintro-
assignedtothem.Theprimaryobjectiveisforeachagenttostrate- duceadeliberatedelaytoensurethatagentscanlearnfromtheir
gicallypositionthemselvestoreachtheirlandmarks.Notably,this observationsbeforetheweightgrowthbegins.Weightgrowthcom-
environmentservesasabaselinescenariowheretheperformance mencesonlyafter10,000steps,andwehavechosenasuitable(Δ𝑇)
of the Decentralized approach will be comparable to that of the toprovideampleseparationbetweenweightupdates,preventingthe
centralizedlearningalgorithm.Thisisbecauseindividualagents receiptofpotentiallymisleadinggradientsignals.Wealsoempha-
donotnecessitatecommunicationwiththeirpeerstoachievetheir sizetheimportanceofthisstepinouranalysis,whichisprovided
objectives. intheSupplementaryMaterial.Tomaintainfairnessincomparing
SSwithCommunication(SS+C)Inthistask,weretainthesame differentapproaches,weensureanequalnumberofneuronsareem-
objectiveasSimpleSpread,with2agentsand2landmarks.How- ployedacrossallmethodsandadjusthyperparametersaccordingly.
ever,eachagent’sobservationsnowincludetheirpositionandthe Detailedhyperparametersettingsforeachapproachcanbefoundin
relativepositionsoftheotheragent’sdesignatedlandmark.Com- ourSupplementaryMaterial.
municationbetweenagentsbecomesessentialtosuccessfullyreach IncooperativeNavigation,weused3metricstocomparedifferent
theirrespectivelandmarksastheyneedtoshareinformationabout methods:MeanEpisodeReward(R),Theaveragerewardachieved
theirlandmarks.Thisconfigurationhighlightsthesignificanceof bytheteaminanepisode.SuccessRate(S%):Inwhatpercentage
agent-agentcommunicationinachievingthemission. ofepisodesdoestheteamachieveitsobjective?(Higherisbetter)
SS with Cross Communication (SS+CC)In this task, we in- Time(T):Howmanytimestepsdoestheteamrequiretoachieve
troduce3agentsand3landmarks.Eachagent’sobservationsstill itsobjective?(Lowerisbetter).EachepisodeintheCooperative
includetheirpositionandtherelativepositionsoftheirassigned Navigationenvironmentlastsforatotalof25-timesteps.Evaluation
landmark.However,thisenvironmenthasauniquetwist:Agent1 iscarriedoutforeachepisodeinthesameseededenvironmentand
receiveshigherrewardswhenitoccupiesthelandmarkdesignated illustratedinFigure2.Wethentestthetrainedmodelonanewlyset
forAgent3,andAgent2receiveshigherrewardswhenitoccupies seededenvironmentfor25-timestepsandshowtheresultsinTable1.
Agent1landmark.Tooptimizetheirreturns,eachagentmustcom- Inthetrafficenvironment,weused2metrics:AverageWaitingtime
municatestrategicallywiththenecessaryagentstonavigatetoward (Lower is better) and Average Trip time (Lower is better). Each
thelandmarksthatyieldhigherrewards.Thissetupintroducesan episodeisrunfor3600steps.Wethentestthetrainedmodeland
asymmetriccommunicationrequirementamongtheagents,differen- provideperformanceresultsinTable4c.Inbothenvironments,we
tiatingitfromthepreviousscenarios(2).Thistaskwouldshowthe usethemetricfloating-pointoperations(FLOPs)toshowthatour
efficacyofourapproachinestablishingthenecessarycommunica- sparsemethodutilizesfewerarithmeticoperations(Lowerisbetter).
tion.Figure2:LearningcurveduringthetrainingofCooperativeNavigationenvironments.AgentsonSSandSS+CCaretrainedfor
20000time-stepswhileonSS+Caretrainedfor500000time-steps.Theplotsshowthemeanepisoderewardover10randomseeds.
(a)t=0 (b)BUN(t=6) (c)BUN(t=10) (d)RigL(t=10) (e)RigL(t=25)
Figure3:ComparisonbetweenBUN(left)andRigL(right)ontheSimpleSpreadwithCommunication(SS+C)andSimpleSpread
withCrossCommunication(SS+C)environmentsatt=0,6,and10andt=0,10,and25.SmallcirclesindicatelandmarksandBig
circlesindicateAgents.InSS+C,thewhiteagentisassignedawhitelandmark,whiletheblackagentisassignedablacklandmark.In
SS+CC,thewhiteagentispenalizedtwiceasablackagenttoreachtheblacklandmark,whileredandblackagentsareassignedto
theredlandmark.Theblackagentisaggressivetoreachtheredlandmarkasitispenalizedtwiceastheredagenttoreachthered
landmark.Inbothenvironments,theagentstrainedusingBUNtriedtograsptheinformationoftheirtargetlandmarksfromtheir
fellowagentsandreachthetargetlandmarks.Ontheotherhand,theagentstrainedusingRigLstruggletoestablishtheconnection
betweenagents.InSS+C,theblackagentreachestheblacklandmark,establishingthatitonlylearnedthelocalbehaviourbutdidnot
establishtheconnectionbetweenthewhiteagent.Seethevideoforcompletetrajectoriesprovidedinthesupplementarymaterial.
5.4 Results intheutilizationofcomputationalresources,preciselythenumber
(SS)Inourexperiments,wetrainedthemodelsthroughout200,000- of Floating Point Operations (FLOPs), among these approaches.
timesteps.ThelearningcurveforthistrainingperiodintheSimple As detailed in Table 1, DGN utilizes more FLOPs, followed by
SpreadenvironmentispresentedinFigure2(a).Inthecentralized theCentralizedapproach,asbothmethodsemploydensemodels.
approach,whereeachagenthasaccesstotheobservationsofall The increased FLOPs in DGN can be attributed to an attention
otheragents,optimalperformanceisachieved.However,giventhe mechanismintheapproach.Incontrast,DecentralizedandBUN
problem’s simplicity, where each agent can access its landmark employ fewer FLOPs, while RigL utilizes slightly more FLOPs
information,demandinginformationfromfellowagentsbecomes thanBUN.However,despitethesevariationsinFLOPutilization,
unnecessary.Consequently,theDecentralizedapproachconverges allmodelsperformsimilarlyduringtesting,achievingcomparable
tothesamerewardlevelastheCentralizedapproach.Similarly,the
performanceinbothsuccessrate(𝑆%)andTime(T),asdemonstrated
BUN, RigL, and DGN approaches converge to the same reward inTable1.
levelastheCentralizedapproach.Nevertheless,therearedifferencesTable1:InCooperativeNavigationenvironments,weassesstheperformanceofvariousapproachesintermsofSuccessRate(𝑆%)and
TimeSteps(T)foralltrainedagentsastheyaimtoreachtheirdesignatedtargetlandmarkduringtesting.Additionally,weanalyzethe
trainingcost,measuredinFLOPs(FloatingPointOperations),incurredbythesedifferentapproachesduringtheirtrainingphase.Itis
tobenotedthattheBUNapproachhasavaryingnumberofFLOPsaswegrowtheweightsduringthetrainingphase.Thistable
providestheaveragenumberofFLOPsutilizedduringtrainingprogressfortheforwardpass.
COOPERATIVENAVIGATION-SIMPLESPREAD(SS)
SS SS+C SS+CC
MODEL FLOPS S% T FLOPS S% T FLOPS S% T
CENTRALIZED 4.2𝑒3 100 10.25 4.2𝑒3 100 10.36 9.5𝑒3 100 16.12
DECENTRALIZED 2.7𝑒3 100 9.75 2.7𝑒3 0 25 3.2𝑒3 0 25
BUN 2.7𝑒3 100 10.75 2.7𝑒3 100 10.78 3.2𝑒3 100 16.55
DGN 6.1𝑒3 0 25 6.1𝑒3 0 25 9.2𝑒3 0 25
RIGL 2.9𝑒3 100 12.12 2.9𝑒3 0 25 3.4𝑒3 30 23.35
Table2:InTrafficSignalControlenvironments,weassesstheperformanceofvariousapproachesintermsofAverageWaitingTime
(Avg.Wait)andAverageTripTime(Avg.TripTime)forallapproaches.Additionally,weanalyzethetrainingcost,measuredinFLOPs
(FloatingPointOperations),incurredbythesedifferentapproachesduringtheirtrainingphase.Tofacilitatecomparison,wenormalize
thesetrainingcostsconcerningtheFLOPsutilizedbythecentralizedapproach,referredtoastheDenseapproach.
GRID2×2 INGLODASTCORRIDOR
MODEL FLOPS AVG.WAIT AVG.TRIPTIME FLOPS AVG.WAIT AVG.TRIPTIME
CENTRALIZED 2.6𝑒6(1𝑥) 2.41 69.61 7.5𝑒6(1𝑥) 12.53 74.00
DECENTRALIZED 6.6𝑒5(0.25𝑥) 3.96 71.75 1𝑒6(0.14𝑥) 15.87 77.36
BUN 6.6𝑒5(0.25𝑥) 2.25 69.35 1𝑒6(0.14𝑥) 12.42 73.86
RIGL 6.6𝑒5(0.25𝑥) 2.39 69.38 1𝑒6(0.14𝑥) 13.83 76.27
(SS+C)Inthisenvironment,wetrainedthemodelforabout (SS+CC)Inthisenvironment,wetrainedthemodelsthrough-
500ktime-stepsuntilconvergence.Figure2(b)illustratesthelearn- out 200,000 time steps, with each episode consisting of 25 time
ingforthetrainingperiod.Sinceeachagenthastheobservation steps.Liketheabovetwoexperiments,CentralizedandDecentral-
ofotheragent’slandmarkposition,thereneedstobeanecessary izedachieveoptimalandsuboptimalperformance.Meanwhile,BUN
flowofinformationacrossagentstoachievetheoptimalreward.As andRigLconvergetoachievetheperformanceofacentralizedap-
expected,centralizedanddecentralizedapproachesachieveoptimal proach.However,duringtheevaluation,RigLagentsinsomeseeds
andsub-optimalperformances.BUNconvergestoCentralizedper- failed to reach the assigned landmarks. We show an example in
formance,whileRigLfailstoreachtheoptimalreward.Thesame Figure3,whereagentblackoccupiestheblacklandmarkratherthan
canbeobservedduringevaluationasdemonstratedinTable1.The theassignedredlandmarkandagentblackblocksagentwhitefrom
agentstrainedusingcentralizedandBUNhavethe100%success occupyingitsassignedlandmark.Thissituationexplainsthatthe
rateinreachingtheirrespectivelandmarks,whileRigLandDGN blackagentislocallytrainedbuthasnotdevelopedthenecessary
have 0% success rate. We illustrate the simulation in Figure 2 to communicationlinksbetweenagentblackandAgentRed(since
furtherdemonstratethis.Asshowninthefigure,BUNagentslearn AgentRedhasthelocationsofredlandmarks).However,theagents
thelocationoftheirlandmarksandreachthelandmarks,whileonly trainedusingBUNachievethetasksbyreachingtheassignedland-
oneagentlearnstheinformationaboutitslandmark,andtheother marks,andtheredagentstaysneartheredlandmarktominimizeits
agentfailstolearnthelocationofitslandmark.Wehypothesizethat distancepenalty.Similartotheaboveexperiments,BUNutilizesfew
aprimaryreasonforthefailureofRigLinthissettingismainlydue FLOPs,anditisobservedthat,asthenumberofagentsincreases,
tothelackofaconsistentweightemergentinthenetworkofagents, thesparsitylevelisincreasedandwouldbeessentialinthecaseof
asititerativelytriedtopruneandgrowtheweights.BUNagents large-scalesystems.
significantlygainfromtheweightinitialization,wheretheagent’s TrafficSignalControlTovalidateourapproachagainstthemod-
weight gets trained steadily, which aids in the consistent weight elsmentionedearlieronTrafficSignalControl,wetrainedthemod-
growthfromasteadygradientsignalacrosstheagents.Surprisingly, elsfor50,000-timestepsandpresentedourresultsinTable4c.In
DGNdoesnotperformwell.Thisismainlyduetothenetworksize, bothscenarios(Grid2×2andInglodats7),allapproachesachieved
aswechosetofixthesmallernumberofparametersinthenetwork. similarresults,whiletheBUNmodelexhibitedmarginalimprove-
Furthermore,asobservedfromtheexperimentsinSimpleSpread, mentsinbothmetrics,althoughtheseimprovementsdidnotreach
DGNandCentralizedapproachesutilizemoreFLOPs,whileBUN statisticalsignificance.Thecompletetrainingresultsareprovidedin
utilizesfewerFLOPstoachieveoptimalperformance. thesupplementarysection.(a)InitializationofNetwork (b)TrainedweightsusingBUN (c)TrainedweightsusingRigL
Figure4:Inthiscomparison,weexaminethetrainingapproachesofBUNandRigLwithinthecontextoftheSS+CCenvironment.
Thesefiguresshowcasetheevolutionofneuralnetworkweightsinbothmethods.IntheBUNapproach,trainingstartswithlocal
weightinitialization(a),whereagentsoperateindependently.Agentobservationsfollowaspecificsequence,withblackandwhite
agentsprecedingred.Theaimistoestablishconnectionsbetweenagents(highlightedinredboxes)withnoemergenceofweightsacross
agentredandagentwhite(greenboxes).TheweightsinBUNemergewithinafixedbudget(b=30),asdepictedin(b).Conversely,
RigLexhibitsadifferentpatternofweightemergence,asseenin(c).UnlikeBUN,RigLintroducesrandomweightconnections.These
structuralweightemergencepatternsshedlightontheresultspresentedintheaccompanyingtableandtheagenttrajectoriesin
Figure1,highlightingeachapproach’sdistinctcommunicationandcoordinationstrategies.
Ourfindingsalignwiththosepublishedin[3],highlightingthat Table3:Toassesstherobustnessofnetworkstrainedusingboth
independentalgorithmsoutperformcoordinatedcontrolalgorithms the BUN and Centralized (Dense) approaches, we conducted
inrealistictrafficscenarios.Theseexperimentsindicatethatinde- testsinvolvingintroducingGaussiannoisetotheobservations.
pendentlearningmethodsaresufficientfortheseapplicationsand
WesamplednoisefromaGaussiandistribution∼N(0,𝜎),where
underscorethe effectivenessofour approachin twokeyaspects: 𝜎 ∈ [0,0.5],denotedas𝜎.Notably,wepresenttheresultsatfour
1.Weinitiatetheprocesswithanindependentsetting,wherewe specificdatapoints
canachieveresultssimilartowhatindependentlearningalgorithms
typically accomplish. 2. Our approach provides the flexibility to VARIANCE SUCESSRATE(𝑆%)
remainintheindependentsetting.Ifnecessary,wecanfacilitate ENVIRONMENT (𝜎) CENTRALIZED BUN
theemergenceofweightsandthegrowthofconnectionsacrossthe 0 100 100
junctionstoenableessentialcommunication.DetailedresultsinTa- SS 0.1 100 100
ble4cdemonstratethatintheGrid2×2scenario,theCentralized (C) 0.3 0 100
approachconsumesmoreFLOPscomparedtotheDecentralized, 0.5 0 0
BUN,andRiGLmodels,whichutilizeonly25%oftheFLOPswhile 0 100 100
achievingsimilarperformance.Similarly,intheInglodastCorridor, SS 0.1 0 100
(CC) 0.3 0 75
weachievedcomparableperformancetotheCentralizedapproach
0.5 0 0
usingonly14%oftheFLOPs.Asthenumberofjunctionsincreases
withinagivenscenario,thenumberofrequiredFLOPsdecreases
significantly.Thischaracteristicmakesourapproachparticularly
suitableforlarge-scaletrafficnetworkscenarios.Forsuchscenarios,
wecaninitiallyemploytheindependentsetting.Asneeded,wecan SS+CC,comparingthesparsenetwork(BUN)andthecentralized
allowfortheemergenceofsparseconnectionsacrossthejunctions, approach(i.e.,thedensestnetwork).AsdepictedinTable3,itis
providingthenecessarycommunicationandadaptabilitytotraffic evident that agents trained using the BUN model exhibit greater
conditions. robustnesstonoisewhencomparedtotheircentralizedcounterparts,
atrendobservedacrossbothcooperativeenvironments.IntheSS+C
environment, both models deliver strong performance under low
5.5 Robustness
noiseconditions.However,asthenoiselevelincreases,centralized
Inthissection,weaimtoevaluatetherobustnessofsparsenetworks agentscannotcopewithnoise,oftenfailingtoreachtheirdesignated
inthepresenceofnoise.Aspreviouslyexploredin[9]forasingle landmarks. In contrast, the BUN agent maintains a higher level
agent setting, we assess the impact of progressively introducing ofrobustnessandperformsrelativelywelleveninthepresenceof
noiseintotheobservationsandsubsequentlymeasuringitsinfluence elevatednoiselevels.Nevertheless,itisworthnotingthateventhe
on a trained network for a Multi-Agent Setting. Specifically, we BUNagentsdisplayadecreaseinrobustnessatsignificantlyhigh
introduceGaussiannoise,sampledfromadistributionwithmean noise levels. This experiment suggests sparse networks, such as
zeroandvariance𝜎,toeachobservationmadebytheagent.This BUN,exhibitgreaterresiliencetoobservationalnoisethandense
experimentisconductedintwocooperativeenvironments,SS+Cand networks,suchasthecentralizedapproach.6 CONCLUSION
[19] PengPeng,YingWen,YaodongYang,QuanYuan,ZhenkunTang,HaitaoLong,
andJunWang.2017. Multiagentbidirectionally-coordinatednets:Emergence
Inthisstudy,weintroducedBUN,anefficientalgorithmfortraining
ofhuman-levelcoordinationinlearningtoplaystarcraftcombatgames.arXiv
sparseneuralnetworksinmulti-agentenvironments.Notably,BUN preprintarXiv:1703.10069(2017).
outperformsexistingdenseandsparsetrainingalgorithmswithin [20] TabishRashid,MikayelSamvelyan,ChristianSchroederDeWitt,GregoryFar-
quhar,JakobFoerster,andShimonWhiteson.2020.Monotonicvaluefunction
prescribedcomputationalbudgets.Ourapproachdemonstratesits factorisationfordeepmulti-agentreinforcementlearning.TheJournalofMachine
valueacrossthreecriticalscenarios:first,byenhancingtheperfor- LearningResearch21,1(2020),7234–7284.
[21] VasanthReddy,SuatGumussoy,HodaEldardiry,andAlmuatazbellahBoker.2024.
manceofmulti-agentsthroughimprovedcommunication;second,
SearchingforSparseControllerswithaBudget:ABottom-UpApproach.In2024
byoptimizingtheutilizationofavailablecomputationalresources; AmericanControlConference(ACC).IEEE,3674–3679.
andthird,byservingasaninitialstepininterpretingtheunderlying [22] GhadaSokar,ElenaMocanu,DecebalConstantinMocanu,MykolaPechenizkiy,
andPeterStone.2021.Dynamicsparsetrainingfordeepreinforcementlearning.
topology among agents. In essence, BUN offers a versatile solu-
arXivpreprintarXiv:2106.04217(2021).
tion that elevates accuracy while respecting resource constraints, [23] SainbayarSukhbaatar,RobFergus,etal.2016.Learningmultiagentcommunica-
fosterscoordinationamongagents,andunveilsessentialinsights tionwithbackpropagation.Advancesinneuralinformationprocessingsystems
29(2016).
intothestructureofmulti-agentsystems,thusmakingasubstantial [24] PeterSunehag,GuyLever,AudrunasGruslys,WojciechMarianCzarnecki,Vini-
contributiontothefield. ciusZambaldi,MaxJaderberg,MarcLanctot,NicolasSonnerat,JoelZLeibo,
KarlTuyls,etal.2017.Value-decompositionnetworksforcooperativemulti-agent
learning.arXivpreprintarXiv:1706.05296(2017).
REFERENCES [25] MarcAurelVischer,RobertTjarkoLange,andHenningSprekeler.2021. On
lotteryticketsandminimaltaskrepresentationsindeepreinforcementlearning.
[1] AkshatAgarwal,SumitKumar,andKatiaSycara.2019.Learningtransferable arXivpreprintarXiv:2105.01648(2021).
cooperativebehaviorinmulti-agentteams. arXivpreprintarXiv:1906.01202 [26] JaehongYoon,EunhoYang,JeongtaeLee,andSungJuHwang.2017.Lifelong
(2019). learningwithdynamicallyexpandablenetworks.arXivpreprintarXiv:1708.01547
[2] SaminYeasarArnob,RiyasatOhib,SergeyPlis,andDoinaPrecup.2021.Single- (2017).
shotpruningforofflinereinforcementlearning.arXivpreprintarXiv:2112.15579
(2021).
[3] JamesAultandGuniSharon.2021. ReinforcementLearningBenchmarksfor
TrafficSignalControl.InProceedingsoftheThirty-fifthConferenceonNeural
InformationProcessingSystems(NeurIPS2021)DatasetsandBenchmarksTrack.
[4] AbhishekDas,ThéophileGervet,JoshuaRomoff,DhruvBatra,DeviParikh,Mike
Rabbat,andJoellePineau.2019.Tarmac:Targetedmulti-agentcommunication.
InInternationalConferenceonMachineLearning.PMLR,1538–1546.
[5] InderjitDhillon,PradeepRavikumar,andAmbujTewari.2011.Nearestneighbor
basedgreedycoordinatedescent. AdvancesinNeuralInformationProcessing
Systems24(2011).
[6] UtkuEvci,TrevorGale,JacobMenick,PabloSamuelCastro,andErichElsen.
2020.Riggingthelottery:Makingallticketswinners.InInternationalConference
onMachineLearning.PMLR,2943–2952.
[7] JakobFoerster,IoannisAlexandrosAssael,NandoDeFreitas,andShimonWhite-
son.2016.Learningtocommunicatewithdeepmulti-agentreinforcementlearning.
Advancesinneuralinformationprocessingsystems29(2016).
[8] JakobFoerster,GregoryFarquhar,TriantafyllosAfouras,NantasNardelli,andShi-
monWhiteson.2018.Counterfactualmulti-agentpolicygradients.InProceedings
oftheAAAIconferenceonartificialintelligence,Vol.32.
[9] LauraGraesser,UtkuEvci,ErichElsen,andPabloSamuelCastro.2022.Thestate
ofsparsetrainingindeepreinforcementlearning.InInternationalConferenceon
MachineLearning.PMLR,7766–7792.
[10] JiechuanJiang,ChenDun,TiejunHuang,andZongqingLu.2019.GraphCon-
volutionalReinforcementLearning.InInternationalConferenceonLearning
Representations.
[11] KenMingLee,SriramGanapathiSubramanian,andMarkCrowley.2022.Inves-
tigationofindependentreinforcementlearningalgorithmsinmulti-agentenviron-
ments.FrontiersinArtificialIntelligence(2022),211.
[12] NamhoonLee,ThalaiyasingamAjanthan,andPhilipHSTorr.2018. Snip:
Single-shotnetworkpruningbasedonconnectionsensitivity. arXivpreprint
arXiv:1810.02340(2018).
[13] TimothyPLillicrap,JonathanJHunt,AlexanderPritzel,NicolasHeess,Tom
Erez,YuvalTassa,DavidSilver,andDaanWierstra.2015.Continuouscontrol
withdeepreinforcementlearning.arXivpreprintarXiv:1509.02971(2015).
[14] DorLivneandKobiCohen.2020.Pops:Policypruningandshrinkingfordeep
reinforcementlearning.IEEEJournalofSelectedTopicsinSignalProcessing14,
4(2020),789–801.
[15] PabloAlvarezLopez,MichaelBehrisch,LauraBieker-Walz,JakobErdmann,
Yun-PangFlötteröd,RobertHilbrich,LeonhardLücken,JohannesRummel,Peter
Wagner,andEvamarieWießner.2018.Microscopictrafficsimulationusingsumo.
In201821stinternationalconferenceonintelligenttransportationsystems(ITSC).
IEEE,2575–2582.
[16] RyanLowe,YiWu,AvivTamar,JeanHarb,PieterAbbeel,andIgorMordatch.
2017. Multi-AgentActor-CriticforMixedCooperative-CompetitiveEnviron-
ments.NeuralInformationProcessingSystems(NIPS)(2017).
[17] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
Antonoglou,DaanWierstra,andMartinRiedmiller.2013. Playingatariwith
deepreinforcementlearning.arXivpreprintarXiv:1312.5602(2013).
[18] FransAOliehoek,ChristopherAmato,etal.2016. Aconciseintroductionto
decentralizedPOMDPs.Vol.1.Springer.A SUPPLEMENTARYMATERIAL A.3 ExperimentalSettings
A.1 AnalysisonWeightEmergencewithΔ𝑇 AllbenchmarksareperformedonAppleM1,2020(maci64)with16
GBRAM.Inthecontextofcooperativenavigation,agentsoperating
Togainadeepermathematicalunderstanding,weexaminetheopti-
mizationprocess.ConsidertheobjectivefunctionvalueL𝑘+1after in an environment receive rewards based on their actions. These
the𝑘-th iteration. We update𝜃𝑘 in the direction of the negative rewards serve two primary purposes: penalizing agent collisions
𝑖𝑗 witharewardof-1andencouragingagentstominimizethedistance
gradientof𝐽 concerning𝜃 𝑖𝑗 whilekeepingallotherweightsfixed.
betweenthemselvesandtheirassignedlandmarks.Thisrewardstruc-
Let∇𝑖𝑗𝐽 denotethegradientof𝐽 withrespectto𝜃 𝑖𝑗.SinceLisa
turepromotescollisionavoidanceandefficientgoalachievement.
continuouslydifferentiablelossfunction,wecanusethefirst-order
Notably,therewarddistributionisagent-specific,meaningthatdif-
Taylorexpansionaround𝜃𝑘 toapproximateL𝑘+1asfollows:
ferentagentsreceiverewardstailoredtotheiruniqueobjectives.In
L𝑘+1≈L𝑘 +∇L𝑘 ·(𝜃𝑘+1−𝜃𝑘
) (3)
SS+CC,Agent1isincentivizedtooccupyLandmark3withareward
twicethenegativedistancebetweenitscurrentlocationandLand-
Considerthecomponentcorrespondingto∇L𝑘 ·(𝜃𝑘+1−𝜃𝑘)in(3) mark3.Agent1alsoincursa-1penaltyforcollisions,ensuringit
andexpanditasfollows: activelyavoidscollisionswithotheragents.Similarly,Agent2isen-
couragedtooccupyLandmark1throughasimilarrewardstructure,
∇L𝑘 ·(𝜃𝑘+1−𝜃𝑘 ) ≤ ∑︁ ∇𝑖𝑗L𝑘 (cid:16) 𝜃 𝑖𝑘 𝑗+1−𝜃 𝑖𝑘 𝑗(cid:17) receivinga-1penaltyforcollisions.Thisapproachensuresthateach
(𝑖,𝑗)∈S𝑘 agenthasacleargoalandincentivetoachieveitwhiletakingactive
+ ∑︁ ∇𝑖𝑗L𝑘 (cid:16) 𝜃 𝑖𝑘 𝑗+1−𝜃 𝑖𝑘 𝑗(cid:17) m coe na ts rou lre es nvto iroa nv moi ed nc t,o tl hli esi ro en ws aw rdit fh uno ct th ioer na dg ife fn et rs s. .I Hn ea ret ,r ta hf efic res wig an rda sl
(𝑖,𝑗)∈S𝑐𝑘 areassociatedwithsignalcontrolattrafficjunctions.Themeasureof
successistheaveragequeuelengthatthesejunctions.Thegoalisto
Now,substitutetheaboveexpressionbackin(3)
minimizequeuelengths,which,inturn,reducestheaveragewaiting
L𝑘+1−L𝑘 ≤ ∑︁ ∇𝑖𝑗L𝑘 (cid:16) 𝜃 𝑖𝑘 𝑗+1−𝜃 𝑖𝑘 𝑗(cid:17) timeforvehiclesattheseintersections.Thisapproachalignswith
theobjectivesoftrafficmanagementusingreinforcementlearning,
(𝑖,𝑗)∈S𝑘
where agents (in this case, traffic signals) are trained to actively
+ ∑︁ ∇𝑖𝑗L𝑘 (cid:16) 𝜃 𝑖𝑘 𝑗+1−𝜃 𝑖𝑘 𝑗(cid:17) (4) optimizetrafficflowwhileavoidingsituationsthatleadtoexcessive
(𝑖,𝑗)∈S𝑐𝑘 vehiclequeuesandcongestion.
where S is a space of indices of the non-sparse weights, while A.4 AdditionalResults
S𝑐 is a space of the sparse weights. In this form, the expression
Inthissection,weprovidethetrainingcurvesfortrafficsignalcon-
emphasizesthatthechangeinthelossdependsonthecontributions
fromboththenon-sparseweights{𝜃 𝑖𝑗|(𝑖,𝑗) ∈ 𝑆𝑘}andthesparse trol.Inthissection,wepresentthetrainingcurvesfortrafficsignal
control.Figure5demonstratesthatinthe2×2gridscenario,allthe
weights{𝜃 𝑖𝑗|(𝑖,𝑗) ∈ 𝑆 𝑐𝑘},andeachcontributionisproportionalto
approachesconvergeatasimilarrate.However,thecentralizedap-
thecorrespondinggradientmultipliedbythechangeintheelement
proachconvergesfasterintheIngolstadtcorridorregion,asdepicted
value.However,iftheweightsaretrainedproperly,wecanusethe
inFigure6.Comparedtothecentralizedapproach,bothBUNand
factthat∇𝑖𝑗L𝑘 =0∀(𝑖,𝑗) ∈S𝑘 .Thentheexpression(3)reduces
RigLandthedecentralizedapproachexhibitslowerconvergencedue
to
totheirrelianceonlearningtrafficpatternsfromlocalobservations.
L𝑘+1−L𝑘 ≤ ∑︁ ∇𝑖𝑗L𝑘 (cid:16) 𝜃 𝑖𝑘 𝑗+1−𝜃 𝑖𝑘 𝑗(cid:17) (5) Notably,whenconsideringpartialobservations,BUNoutperforms
RigLandtheDecentralizedapproachregardingconvergencespeed.
(𝑖,𝑗)∈S𝑐𝑘
Additionally,weprovideinsightsintothevariousapproachesmem-
Inthiscontext,whilethechangeinthelossdependsonthegradients oryutilizationperformance,asshowninFigure7andSparsitylevel
ofsparseweights,thepivotalinfluenceoftuningbecomesapparent. ofnetworksinTable6.TheFigure7highlightsthatBUNandRigL
Bytrainingthenon-sparseweights,ouralgorithmprioritizessparse utilizealowerpercentageofmemorythanthecentralizedapproach.
weightsefficientlyandoptimizesthevaluesofnon-sparseweights. Thisobservationservesassupportingevidencefortheefficiencyof
For this reason, we ensure a gap in each weight growth update. usingfewerFLOPs(floating-pointoperations)duringtraining.
Thisstrategyensuresthatthechangeinthelossalignscloselywith
theoptimizationobjective,makingourapproachhighlyeffectivein
achievingsuperiorperformancewhilehavingagoodgradientsignal.
A.2 Hyperparametrs
Fortheenvironments,CooperativeNavigationandTrafficSignal
Control,weuseasetofhyperparameterstoachievetheresultsshown
inourpaper.WeprovidethehyperparametersusedforCooperative
NavigationinTable4andTrafficSignalControlinTable5.Itisto
benotedthatthehyperparametersforCentralizedandDecentralized
approachesarethesameasBUN.Table4:HyperparametersofBUN,RigLandDGN.Where𝑁 isthenumberofagents.
HYPERPARAMETER BUN RIGL DGN
DISCOUNT(𝛾) 1024 1024 1024
BUFFERCAPACITY 1𝑒6 1𝑒6 1𝑒6
𝛽
DECAY(𝜖) 0.1 0.1 0.1
OPTIMIZER ADAM ADAM ADAM
LEARNINGRATE 1𝑒-4 1𝑒-4 1𝑒-4
LAYERTYPE MLP MLP MLP
#OFLAYER 3 3 3
#OFUNITS (18*N,18*N,18*N) (18*N,18*N,18*N) (18,18,18)
ACTIVATIONTYPE RELU RELU RELU
WEIGHTINITIALIZATION BUN RANDOMNORMAL RANDOMNORMAL
#OFNEIGHBORS - - ALL
𝑇𝑠𝑡𝑎𝑟𝑡 10K 5K -
𝑇 𝑒𝑛𝑑 30K 0.75*TIME -
Δ𝑇 1000 100 -
Table5:HyperparametersofBUN,RigLandDGN.Where𝑁 isthenumberofagents.
HYPERPARAMETER BUN RIGL DGN
BATCHSIZE(𝛾) 64 64 64
BUFFERCAPACITY 1𝑒6 1𝑒6 1𝑒6
𝛽
DECAY(𝜖) 0.1 0.1 0.1
OPTIMIZER ADAM ADAM ADAM
LEARNINGRATE 1𝑒-4 1𝑒-4 1𝑒-4
LAYERTYPE MLP MLP MLP
#OFLAYER 3 3 3
#OFUNITS (256*N,256*N,256*N) (256*N,256*N,256*N) (256,256,256)
ACTIVATIONTYPE RELU RELU RELU
WEIGHTINITIALIZATION BUN RANDOMNORMAL RANDOMNORMAL
#OFNEIGHBORS - - ALL
𝑇𝑠𝑡𝑎𝑟𝑡 10K 5K -
𝑇 𝑒𝑛𝑑 30K 0.75*TIME -
Δ𝑇 1000 100 -
Figure5:LearningcurveduringthetrainingofGrid2×environment.TheplotsshowtheEpisodewaitingtimeandAverageWaiting
TimeofVehicleinthegridnetwork.Figure6:LearningcurveduringthetrainingofInglodastCorridorenvironment.TheplotsshowtheEpisodewaitingtimeandAverage
WaitingTimeofVehicleinthegridnetwork.
Figure7:LearningcurveduringthetrainingofInglodastCorridorenvironment.TheplotsshowtheEpisodewaitingtimeandAverage
WaitingTimeofVehicleinthegridnetwork.Table6:ThepercentageofsparsityvariesacrossdifferentapproachesinCooperativeNavigationandTrafficSignalControlenvi-
ronments,with(N)representingthenumberofagentsintheenvironment.Itisevidentthatasthenumberofagentsincreases,the
percentageofsparsityalsoincreases.Thistrendhighlightsthescalabilityofsparseapproachesinhandlinglargernumbersofagents.
Notably,thesparsityleveloftheBUNapproachinitiallymatchesthatofthedecentralizedapproachbutgraduallyincreasesperthe
allocatedbudget,eventuallyreachingalevelcomparabletothatofRigL
SPARISTY(%)
SS(N=2) SS+C(N=2) SS+CC(N=3) GRID2×2(N=4) ING.CORRIDOR(N=7)
CENTRALIZED 0 0 0 0 0
DECENTRALIZED 50 50 66.66 75 85.71
BUN 57 57 64.4 75 85.71
DGN 0 0 0 0 0
RIGL 57 57 64.4 75 85.71