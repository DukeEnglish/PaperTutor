Online Learning Guided Quasi-Newton Methods with Global
∗
Non-Asymptotic Convergence
Ruichen Jiang† Aryan Mokhtari
†
Abstract
In this paper, we propose a quasi-Newton method for solving smooth and monotone nonlin-
earequations,includingunconstrainedminimizationandminimaxoptimizationasspecialcases.
Forthestronglymonotonesetting,weestablishtwoglobalconvergencebounds: (i)alinearcon-
vergencerate that matches the rate of the celebratedextragradientmethod, and (ii) an explicit
global superlinear convergencerate that provably surpasses the linear convergence rate after at
most (d) iterations, where d is the problem’s dimension. In addition, for the case where the
O
operator is only monotone, we prove a global convergence rate of (min 1, √d ) in terms of
O {k k1.25 }
the duality gap. This matches the rate of the extragradient method when k = (d2) and is
O
faster when k = Ω(d2). These results are the first global convergence results to demonstrate a
provableadvantageofaquasi-Newtonmethodovertheextragradientmethod,withoutquerying
the Jacobian of the operator. Unlike classical quasi-Newton methods, we achieve this by using
the hybrid proximal extragradient framework and a novel online learning approach for updat-
ing the Jacobian approximation matrices. Specifically, guided by the convergence analysis, we
formulate the Jacobian approximation update as an online convex optimization problem over
non-symmetric matrices, relating the regret of the online problem to the convergence rate of
our method. To facilitate efficient implementation, we further develop a tailored online learn-
ing algorithm based on an approximate separation oracle, which preserves structures such as
symmetry and sparsity in the Jacobian matrices.
∗A preliminary version of part of this work appeared in [JJM23].
†Department of Electrical and Computer Engineering, The University of Texas at Austin, Austin, TX, USA
{rjiang@utexas.edu, mokhtari@austin.utexas.edu}
1
4202
tcO
3
]CO.htam[
1v62620.0142:viXra1 Introduction
In this paper, we consider the problem of solving a system of nonlinear equations:
F(z)= 0, (1)
where F : Rd Rd is a continuously differentiable operator. This class of problems emerges
→
from finite-dimensional approximations to nonlinear differential and integral equations and has
applications in diverse fields of mathematical physics [Mor90; ACXM92; NW06]. Problem (1) is
alsocloselyrelatedtovariationalinequalities(VIs),asitcanberegardedasaVIwithoutconstraints.
Moreover, this formulation captures the optimality conditions of unconstrained minimization and
minimax optimization problems [FP04].
One well-known method for solving the problem in (1) is Newton’s method, noted for its fast
convergence. When the Jacobian F(z) is non-singular, Newton’s method achieves quadratic con-
∇
vergence near the optimal solution [NW06]. However, a major challenge of its implementation is
computing the Jacobian F, especially in high-dimensional settings. Hence, various modifications
∇
to Newton’s method have been considered to improve computational efficiency, and among them
quasi-Newton methods are the most popular. They were first introduced in [Dav59], for minimiza-
tion problems, and in [Bro65], for systems of equations. They aim to emulate Newton’s method
whileapproximating the Jacobian F solely through theoperator information F, and they require
∇
(d2)arithmeticoperations periteration. Forminimization problems,several popularupdaterules
O
includetheDavidon-Fletcher-Powell (DFP)method[Dav59; FP63],theBroyden-Fletcher-Goldfarb-
Shanno (BFGS) method [Bro70; Fle70; Gol70; Sha70], and the symmetric rank-one (SR1) method
[CGT91; KBS93]. For solving systems of equation, Broyden’s methods [Bro65] remain the most
popular choices. In the following paragraphs, we summarize the known theoretical guarantees for
these methods.
Asymptotic superlinear convergence. The classical analysis of quasi-Newton methods aims to
establish their Q-superlinear convergence undersuitable conditions, meaning lim k + kz zk+1 − zz ∗∗ k =
0 where z is the optimal solution of (1). For the special case of minimization pr→ oble∞ msk , wk− henk the
∗
functionislocally smoothandstronglyconvex, itwasestablishedin[BDM73; DM74]thatDFP and
BFGS are locally and Q-superlinearly convergent. To ensure global convergence, it is necessary to
incorporate quasi-Newton updates with a line search or a trust-region method. When the function
is smooth and strongly convex, it was shown in [Pow71; Dix72] that DFP and BFGS with an
exact line search converge globally and Q-superlinearly. Subsequently, it was shown in [Pow76]
that BFGS with an inexact line search retains global and superlinear convergence, and this result
was later extended to the restricted Broyden class except for DFP [BNY87]. Along another line
of research, the SR1 method with trust region techniques was studied in [CGT91; KBS93; BKS96]
and they also proved its global and superlinear convergence. For general nonlinear equations,
it was shown in [BDM73] that when the Jacobian F is non-singular and Lipschitz continuous,
∇
Broyden’s method converges locally and superlinearly. Subsequent work in [MT76] established
the local superlinear convergence of a modified Broyden’s method by Powell [Pow70]. Moreover,
these superlinear convergence results have also been extended to Hilbert spaces [Gri87; KS91].
In addition, several works proposed line search strategies for Broyden’s method [Gri86; LF00] to
ensure global and superlinear convergence. However, these results are all asymptotic and they do
not provide an explicit convergence rate.
Local non-asymptotic superlinear convergence. Recent work hasattempted toestablish non-
2asymptotic guarantees for quasi-Newton methods whenapplied to solving Problem (1) and the spe-
cial case of minimization problems. For smooth and strongly convex minimization problems, this
line of research was initiated by [RN21a], which studied a greedy variant of quasi-Newton methods
and demonstrated a local non-asymptotic superlinear convergence rate of (1 µ )k2/2(dL1)k. Here,
− dL1 µ
d is the problem’s dimension, and L and µ are the smoothness and strong convexity parameters,
1
respectively. Later, two concurrent works [RN21b; JM22] examined local non-asymptotic super-
linear convergence rates of classical quasi-Newton methods. Specifically, the authors in [RN21b]
proved that in a local neighborhood of the optimal solution, if the initial Hessian approximation is
k/2
set as L I, BFGS with unit step size converges at a superlinear rate of d log L1 . Concurrently,
1 k µ
theauthors in [JM22] showed that if theinitial Hessian approximation i(cid:16)s close to(cid:17)theHessian at the
optimal solution or selected as the Hessian at the initial point, BFGS with unit step size achieves
a local superlinear convergence rate of 1 k/2. Further details and follow-up works can be found
k
in [JKRM22; LYZ21b; YLCZ22]. Subsequently, similar results have been established for solving
(cid:0) (cid:1)
general nonlinear equations. When the Jacobian F is Lipschitz continuous and F(z ) is non-
∗
∇ ∇
singular, under the condition that the initial point and the initial Jacobian approximation matrix
are sufficiently close to z and F(z ), respectively, the authors in [LYZ21a] demonstrated that
∗ ∗
∇
Broyden’s method achieves an explicit local non-asymptotic superlinear rate of 1 k/2. Moreover,
k
undersimilar assumptionsandastronger initial condition, thegreedy andrandomvariants of Broy-
(cid:0) (cid:1)
den’s method were proposed in [YLZ21] and shown to achieve a local superlinear rate of (1
1)k2/4.
−d
Building upon this, the block Broyden’s method was later presented in [LCLL23]. However, these
non-asymptotic results are limited to local neighborhoods of the solution, and several of them, espe-
cially thoseforsolvingnonlinearequations, alsorequiretheinitial Hessian/Jacobian approximation
matrix to be sufficiently close to the actual one at z [JM22; LYZ21a; YLZ21; LCLL23]. Thus,
∗
they do not lead to a global convergence guarantee.
Convergence without strong convexity/non-singularity. Notethatalltheresultsaboveonly
apply under the restrictive assumption that the objective is strongly convex in the minimization
setting, or that the operator is non-singular in the nonlinear equation setting. There are a few
works that study quasi-Newton methods when the objective is merely convex or when the operator
is possibly singular. However, to the best of our knowledge, no theoretical result demonstrates the
advantage of quasi-Newton methods in these more general settings. Specifically, for minimizing
smooth convex functions, it has been shown that classical quasi-Newton methods such as BFGS
converge asymptotically [BNY87; Pow72] but no explicit rates have been provided. Several recent
works [ST16; GS18; KZAT23; Sci24] have combined quasi-Newton updates with variable metric
proximal gradient or cubic regularization techniques and proved sublinear rates. However, their
results are no better than the first-order counterparts such as gradient descent or accelerated
gradient descent.
Contributions. In summary, for the strongly monotone case, while quasi-Newton methods have
the potential to achieve a faster rate, the current theory is either asymptotic or applicable only
in a local neighborhood of the solution, with a stringent requirement on the initial Jacobian. Fur-
thermore, for the monotone case, no theoretical advantage of quasi-Newton methods has been
demonstrated in the literature. In this paper, we aim to close these gaps and present a novel quasi-
Newton proximal extragradient (QNPE) method. When the operator is µ-strongly monotone, it
3attains the following global convergence bounds:
k
z
k
z
∗
2 µ −k µ k −
k − k min 1+ , 1+ , (2)
z z 2 ≤  30L 16L M 
k 0 − ∗ k (cid:18) 1(cid:19) 1r ! 
where M = O(kB 0 −∇ LF 2 1(z∗) k2 F + L2 2kz µ0 L− 1z∗ k2 ) = O(d+ L2 2kz µ0 L− 1z∗ k2 ) with L 2 being the Jacobian’s Lips-
chitzconstant. Asweobservefrom(2),whenk = (d), thefirstupperboundin(2)impliesthatthe
O
convergence rate of our method matches that of the classical extragradient (EG) method [Kor76;
Nem04]. This is the best rate we can hope for in this regime, since there are lower bound re-
sults [NY83; ZHZ22] showing that the complexity bound of (L1 log 1) achieved by EG is optimal
O µ ǫ
up to constants when the number of iterations k is (d). Further, based on the second term in the
O
upperbound,itprovablyoutperformsEGoncethenumberofiterationssatisfiesk M = (d)and
≥ O
L2M k/2
attains a superlinear rate of the form 1 . In addition, when the operator is only monotone,
µ2k
our method achieves a global converge(cid:16)nce ra(cid:17)te of
1 √d
min , . (3)
O k k1.25
( )!
In particular, this implies that our method matches the (1) rate by EG, known to be optimal
O k
in the regime where k = (d) [OX21]. Moreover, it converges at a faster rate of
√d
when
O O k1.25
k = Ω(d2). Finally, for both strongly monotone and monotone settings, we fully cha(cid:16)racter(cid:17)ize the
computationalcostintermsofthetotalnumberofoperatorevaluations andmatrix-vector products
(see Theorems 6.8, 6.9, and 6.17).
Ourproposedmethodisdistinctfromclassicalquasi-Newtonmethods,suchasBFGSandBroyden’s
method, in two key aspects. First, our method adopts the hybrid proximal extragradient (HPE)
framework [SS99], resembling aquasi-Newton approximation of theNewton proximal extragradient
method for solving monotone variational inequalities [MS10]. Another notable distinction lies in
the update of the Jacobian approximation matrix. Classical quasi-Newton methods adhere to the
secant condition while maintaining proximity to the previous approximation matrix. Conversely,
our update rule is solely driven by our convergence analysis of the HPE framework. Specifically,
according to our analysis, a better upper bound on the cumulative loss ℓ (B ) implies a faster
k k k
convergence rate for our proposed QNPE method, where B is the Jacobian approximation matrix
k
P
and ℓ : Rd d R is a loss function that in some sense measures the approximation error. As
k × +
→
a result, the update of B boils down to running an online algorithm for solving an online convex
k
optimization problem in the space of matrices.
Finally, weaddressthechallengeofcomputationalefficiencybypresentingatailored onlinelearning
algorithmfortheupdateofB . Notethatmostonlinelearningalgorithms forconstrainedproblems
k
are based on a projection oracle, but in our specific setting, such projections either do not admit
a closed-from solution or require expensive eigendecomposition. In contrast, our online learning
algorithm utilizes an approximate separation oracle that can be efficiently constructed using the
classical Lanczos method. Additionally, when the Jacobian of F possesses certain structures, such
as symmetry or sparsity, our algorithm mirrors the same structure on the Jacobian approximation
matrices with no additional cost. This allows us to fully leverage the structural information of the
Jacobian matrices to reduce the computational cost.
41.1 Overview
In this section, we provide a brief overview of our proposed algorithm. Our QNPE algorithm has
a hierarchical structure of three levels.
• At the highest level, it is based on the HPE framework [SS99; MS10], which can be regarded
as an inexact variant of the proximal point method [Roc76; Mar70]. We fully describe the
HPE framework in Section 3.
• At the intermediate level, the algorithm requires two subroutines in the HPE framework: a
line search subroutine for selecting the step size (Section 3.1) and a Jacobian approximation
update subroutine for selecting the matrix B (Section 4). To implement the line search, we
k
adopt the standard backtracking algorithm with a warm start strategy, where the initial step
size at the k-th iteration is chosen as a multiple of the step size in the previous iteration. In
addition, motivated by our convergence analysis, we formulate our Jacobian approximation
update as an online learning problem (Section 4.1). We first present a general online learning
algorithm based on the approximate separation oracle in Section 4.2 and then instantiate it
to our specific settings in Sections 4.3 and 4.4.
• Furthermore, thelinesearch subroutinerequiresimplementingaLinearSolver oracle, whilethe
Jacobian approximation update subroutine requires an approximate separation oracle. Their
implementations are discussed in Section 5.
We present our complexity analysis in Section 6, which is divided into the strongly monotone
setting (Section 6.1) and the monotone setting (Section 6.2). Finally, the concluding remarks are
presented in Section 7.
2 Preliminaries
In this section, we present our assumptions, discuss three special cases of Problem (1) that are of
interest, and introduce the considered measures of suboptimality for our convergence analyses.
2.1 Assumptions
In this paper, we focus on two specific classes of nonlinear equations: (i) strongly monotone and
(ii) monotone, formally defined in Assumption 1.a and Assumption 1.b.
Assumption 1.a. The operator F isµ-strongly monotone, i.e., F(z) F(z ),z z µ z z 2,
′ ′ ′
h − − i≥ k − k
z,z Rd.
′
∀ ∈
Assumption 1.b. F is monotone, i.e., F(z) F(z ),z z 0, z,z Rd.
′ ′ ′
h − − i≥ ∀ ∈
NotethatunderAssumption1.a,Problem(1)hasauniquesolution,whichwedenotebyz through-
∗
out the paper. Moreover, Assumption 1.b can be regarded as a special case of Assumption 1.a by
setting µ = 0. Therefore, we will sometimes use this observation to present our results in both
settings in a united manner, avoiding unnecessary repetition. In addition, we make the following
two assumptions that the operator F and its Jacobian are Lipschitz.
Assumption 2.2. The operator F is L -Lipschitz, i.e., F(z) F(z ) L z z , z,z Rd.
1 ′ 1 ′ ′
k − k ≤ k − k ∀ ∈
5Assumption 2.3. The Jacobian of F is L -Lipschitz, i.e., F(z) F(z ) L z z ,
2 ′ op 2 ′
k∇ −∇ k ≤ k − k
z,z Rd, where A , sup Az .
∀ ′ ∈ k kop kz k=1k k
Remark 2.1. In our convergence analysis for strongly monotone equations, we can relax As-
sumption 2.3 and F requires to only satisfy a Lipschitz property at the optimal solution z , i.e.,
∗
∇
F(z) F(z ) L z z for any z Rd.
∗ op 2 ∗
k∇ −∇ k ≤ k − k ∈
We remark that Assumptions 1.a, 1.b and 2.2 are common in the convergence analysis of first-
order methods for solving nonlinear equations and monotone variational inequalities. Moreover,
Assumption 2.3 is commonly used in the study of quasi-Newton methods [DM77] and second-order
methods [NP06; MS10]. It provides the necessary regularity condition on F that enables us to
prove a superlinear convergence rate for strongly monotone problems and a faster sublinear rate
for monotone problems.
2.2 Jacobian structures
Assumptions 1.a, 1.b and 2.2 are formulated as properties of the operator F, but they can also be
presented as conditions on the Jacobian F, as shown in the next lemma (see [RY22, Section 2]
∇
for the proof).
Lemma 2.1. Under Assumption 1.a, we have 1( F(z)+ F(z) ) µI for any z Rd, while
2 ∇ ∇ ⊤ (cid:23) ∈
under Assumption 1.b, we have 1( F(z)+ F(z) ) 0 for any z Rd. Further, under Assump-
2 ∇ ∇ ⊤ (cid:23) ∈
tion 2.2, we have F(z) L for any z Rd.
op 1
k∇ k ≤ ∈
Lemma 2.1 provides some properties of F(z), and as we discuss in Section 4, it helps us choose
∇
Jacobian approximation matrices. Moreover, in some cases, F(z) has an additional specific
∇
structure that we want our Jacobian approximation matrix to mimic. This could improve the
approximation accuracy and reduce storage requirements. We give three such examples below.
Minimization. As a special instance of Problem (1), consider the unconstrained minimization
problem
min f(z), (4)
z Rd
∈
wheref:Rd Risa(strongly)convexfunction. Bythefirst-orderoptimalitycondition,theoptimal
→
solution of (4) satisfies the nonlinear equation f(z) = 0. Thus, in this case, F is the gradient
∇
operator, i.e., F(z) = f(z), and its Jacobian is the Hessian matrix of f, i.e., F(z) = 2f(z).
∇ ∇ ∇
In particular, we observe that the Jacobian is a symmetric matrix, which allows us to simplify
the conditions in Lemma 2.1. Specifically, under Assumptions 1.a and 2.2, F belongs to the
∇
set A Sd : µI A LI . Meanwhile, under Assumptions 1.b and 2.2, it belongs to the set
{ ∈ (cid:22) (cid:22) }
A Sd: 0 A LI . As discussed in Section 4.3, in our method, we ensure that the Jacobian
{ ∈ (cid:22) (cid:22) }
approximation matrix stays symmetric and positive semidefinite, which is a key property for the
efficient implementation of the subroutines.
Minimax optimization. Another important instance of Problem (1) is the unconstrained mini-
max optimization problem
min maxf(x,y), (5)
x Rmy Rn
∈ ∈
wheref is(strongly)convexwithrespecttoxand(strongly)concavewithrespecttoy. Toreformu-
late this problem as (1), we define the saddle point operator as F(z) = ( f (x,y), f (x,y))
x y
∇ −∇ ∈
6Rm+n, where z = (x,y) Rm+n. Indeed, finding a solution of F(z) is equivalent to finding a
∈
stationary point of the minimax problem, which is the optimal solution. In this case, the Jacobian
2f (x,y) 2f (x,y)
is F(z) = ∇ xx ∇ xy . As noted in [ALY23], F(z) exhibits J-symmetric
∇ 2f xy(x,y) ⊤ 2f yy(x,y) ∇
(cid:20)−∇ −∇ (cid:21)
structure [MMT03], where it is symmetric on the main diagonal blocks and anti-symmetric on
I 0
the off-diagonal blocks. Specifically, for J = m ×m , matrix M is J-symmetric if JM
0 I
n n
(cid:20) − × (cid:21)
is symmetric, i.e., JM = M J. Similarly, as discussed in Section 4.3, we can ensure that our
⊤
Jacobian approximation matrix in our method respects the J-symmetry property, simplifying the
implementation of some subroutines.
Sparse nonlinear equations. The last special structure that we consider is when the Jacobian
F(z) is sparse. This naturally arises in several applications of scientific computing [Bro71; CS84]
∇
and several works have modified Broyden’s methods to leverage this sparsity structure [Sch70;
Bro71; Mar00]. Specifically, assume that we know the sparsity pattern of the Jacobian matrices,
defined as the set Ω = (i,j) : z Rds.t.[ F(z)] = 0, 1 i d, 1 j d , where
ij
{ ∃ ∈ ∇ 6 ≤ ≤ ≤ ≤ }
[ F(z)] denotes the (i,j)-the entry of the matrix F(z). Moreover, we say a matrix M Rd d
ij ×
∇ ∇ ∈
has the sparsity pattern Ω if [M] = 0 for all (i,j) / Ω and i = j. As we discuss in Section 4.3,
ij
∈ 6
we can incorporate these sparsity constraints on our Jacobian approximation matrix in a seamless
way. This enables us to reduce the storage requirement as well as the computational cost of the
subroutines.
2.3 Suboptimality measures
In this section, we discuss our choice for measuring suboptimality, which is needed to charac-
terize the convergence rate of our proposed method. In the strongly monotone setting (under
Assumption 1.a), recall that there is a unique solution z to Problem (1). Hence, we measure the
∗
suboptimality in terms of distance to z .
∗
In the monotone setting (under Assumption 1.b), a common suboptimality measure is the weak
g aa np df Lu in pc st ci ho in tz[ ,N ites h0 o7 l; dsM tS h1 a0 t], Gd ae pfin (e zd )as 0G fa op rw a( lz l) z= m Ra dx az n′ ∈ dR Gd h aF p(z (′ z), )z =− 0z if′ i a. nS din oc ne lyF ifi zs m soo lvn eo sto tn he
e
w w
≥ ∈
nonlinear equation in (1). However, in some cases, the weak gap function could always be infinite
except at the solutions of (1), rendering it vacuous. To remedy this issue, we use the restricted gap
function
Gap (z; ) = max F(z ),z z , (6)
w D z′ h ′ − ′ i
∈D
where is a given compact set in Rd. It is shown in [Nes07] that: (i) Gap (z; ) 0 for all z .
w
D D ≥ ∈ D
(ii) If z is a solution of (1) and z , then Gap (z ; ) = 0. (ii) Conversely, if Gap (z; ) = 0
∗ ∗ w ∗ w
∈ D D D
and z is in the interior of , then z is a solution of (1). Thus, Gap (z; ) is a valid merit function
D w D
when is sufficiently large.
D
In addition, we can use a more customized measure of suboptimality for the special instances
discussed in Section 2.2. Specifically, for the minimization problem in (4), we can consider the
function value gap:
Gap (z;z )= f(z) f(z ). (7)
f ∗ − ∗
7For the minimax problem in (5), we consider the restricted primal-dual gap:
Gap (z; ) = maxf(x,y ) min f(x,y), (8)
pd X ×Y y′ ′ −x′ ′
∈Y ∈X
where z = (x,y) and and are given compact sets in Rm and Rn, respectively. The following
X Y
classical lemma plays a key role in our analysis, since it provides an upper bound on the gap at the
averaged iterate.
L
z
0e ,m ..m .,a
z
T2 −. 12 ∈. S Ru dp .p Dos ee finA ess thu em ap vti eo rn age1 d.b ith eo rald ts e.
s
aL se zt
¯
Tθ
0
=,...
T
t, =θ
−0T
1−
θ1
tz≥ t.
T0 hw enit :h PT t=−01θ
t
= 1 and let
P
(i) For Problem (1), Gap w(z; D)
≤
max
z
∈D
T t=−01θ
t
hF(z t),z
t
−z i.
(ii) For Problem (4), Gap f(z;z ∗)
≤
T t=−01θ tP hF(z t),z
t
−z
∗
i.
(iii) For Problem (5), Gap pd(z;
X
×YP )
≤
max
z
∈X×Y
T t=−01θ
t
hF(z t),z
t
−z i.
P
In Lemma 2.2, we observe that the gap functions in the three cases can all be upper bounded by a
similar quantity. Thus, to unify these different notions, we define
Gap (z; ), for Problem (1);
w
D
Gap(z; ) = max z′ f(z) f(z ′) , for Problem (4); (9)
D ∈D{ − }
 max (x′,y′) f(x,y ′) f(x ′,y) , for Problem (5).
∈D{ − }

Specifically, note that Gap(z; z ) reduces to Gap (z;z ) for Problem (4) and Gap(z; )
{ ∗ } f ∗ X × Y
reduces to Gap (z; ) for Problem (5). Hence, for ease of exposition, in our main theorem,
pd X ×Y
we report our results in terms of Gap(z; ).
D
3 Quasi-Newton proximal extragradient algorithm
In this section, we present our quasi-Newton proximal extragradient (QNPE) method, which is
based on the hybrid proximal extragradient (HPE) framework [SS99; MS10]. Thus, to lay the
groundwork, we first briefly recap the HPE framework.
HPE framework. The HPE framework is a principled scheme to approximate the proximal point
method with a fast convergence rate for solving variational inequality problems. Its update rule
consists of two steps in each iteration. In HPE, given the current iterate z and the step size η ,
k k
we first take an approximate proximal point step as zˆ z η F(zˆ ). Specifically, we require zˆ
k k k k k
≈ −
to approximately solve the proximal subproblem z z +η F(z)= 0 satisfying the error criterion
k k
−
zˆ z +η F(zˆ ) α zˆ z , (10)
k k k k k k
k − k ≤ k − k
where α (0,1). After computing zˆ , we take an extragradient step and compute
k
∈
z = z η F(zˆ ). (11)
k+1 k k k
−
The iteration complexity of HPE was first analyzed in [MS10] in terms of finding a (ρ,ǫ)-weak
solution of (1). Specifically, z is a (ρ,ǫ)-weak solution if there exists a vector r Rd such that
∈
8Algorithm 1 Quasi-Newton Proximal Extragradient (QNPE) Method (informal)
1: Input: strong monotonicity parameter µ 0, Jacobian feasible set , line search parameters α 1 0
≥ Z ≥
and α >0 such that α +α <1, and initial trial step size σ >0
2 1 2 0
2: Initialization: initial point z 0 Rd and initial Jacobian approximation B 0
∈ ∈Z
3: for iteration k =0,...,N 1 do
−
4: Let η k be the largest possible step size in σ kβi :i 0 such that
{ ≥ }
zˆ z +η (F(z )+B (zˆ z )) α 1+η µ zˆ z ,
k k − k k k k k − k k≤ 1 k k k − k k Line search subroutine;
zˆ
k
z k+η kF(zˆ k) (α 1+α 2) 1+pη kµ zˆ
k
z
k
see Section 3.1
k − k≤ k − k
p
5: Set σ k+1 η k/β
←
6: Update z k+1 ←θ k(z k −η kF(zˆ k))+(1 −θ k)zˆ k, where θ k = 1+21 ηkµ
7: if η k =σ k then # Line search accepted the initial trial step size
8: Set B k+1 B k
←
9: else # Line search bactracked
10: Let z˜ k be the last rejected iterate in the line search Jacobian approximation
11: Set u k F(z˜ k) F(z k), s k z˜ k z k update subroutine; see
12: Define t← he loss fu− nction ℓ k(B)← = ku− k − skBs 2k k2 Section 4
k k
13: Feed ℓ k(B) to an online learning algorithm and obtain B k+1
A
14: end if
15: end for
r ρ and sup z′ Rd F(z ′) r,z z ′ ǫ; see [MS10, Section 3] for more discussions on this
k nok tio≤ n. Under mon∈ otoh nicity o− f F, it− wasi s≤ hown that the averaged iterate z¯
k
= ik =−01zˆ i/ ik =−01η
i
is a (ρ,ǫ)-weak solution with max {ρ,ǫ
}
= O(1/ ik =−01η i).
P P
P
Newton Proximal Extragradient. The HPE method should be regarded as a conceptual algo-
rithmic framework. Indeed, to turn it into an implementable algorithm, we need to specify how
to compute zˆ such that the condition in (10) is satisfied. Assuming access of the operator F
k
and the Jacobian F, [SS99] proposed using a single iteration of Newton’s method to solve the
∇
proximal subproblem. A variant of this method, known as Newton proximal extragradient (NPE)
method, was analyzed by [MS10; MS12] for monotone settings. Specifically, in the first step, the
NPE method chooses a step size η and an iterate zˆ such that
k k
zˆ = z η (I+η F(z )) 1F(z ), (12)
k k k k k − k
− ∇
L
2
α η zˆ z α, (13)
′ k k k
≤ 2 k − k ≤
where 0 < α < α < 1. Note that (12) corresponds to applying one iteration of Newton’s method
′
on the proximal subproblem z z +η F(z)= 0, while (13) imposes that the step size η should
k k k
−
be on the same order as 2 . Together, the conditions in (12) and (13) ensure that the
L2 kzˆ k−z kk
error criterion (10) is satisfied, and thus NPE is an instance of HPE. Using the convergence theory
developed for HPE, [MS10] showed a convergence rate of (1/k1.5) for NPE when F is monotone.
O
Quasi-Newton Proximal Extragradient. In our setting of interest, computing F() is not
∇ ·
feasible, preventing us from deploying NPE. Instead, we introduce our quasi-Newton proximal
extragradient (QNPE) method, which, unlike NPE, does not rely on access to F(). Specifically,
∇ ·
in the initial step of HPE, we simply perform a single quasi-Newton iteration on the proximal
subproblem. Besides the fact that QNPE does not require access to F() and only relies on
∇ ·
9F(), we should emphasize two other key differences from NPE. First, the analysis of NPE in
·
[MS10; MS12] is done for the setting where F is merely monotone. When we introduce our QNPE
method for the strongly monotone setting, we not only replace the F() with its quasi-Newton
∇ ·
approximation but also modify the update rules in both the first and second stages. These changes
aremadeto leverage thestrongmonotonicity of F. Second, theNewton updatein (12) requires the
inverse of a d d matrix, with a complexity of (d3). In comparison, we allow the linear system of
× O
equationsarisingfromthequasi-Newton steptobesolved inexactly uptosomeprescribedaccuracy,
thusreducingthecomputational costandleadingtoanalgorithm withacostperiteration of (d2).
O
Now we formally describe the procedure of our QNPE method, consisting of three stages. In the
first stage, given a Jacobian approximation matrix B and the iterate z , we select a step size η
k k k
and a point zˆ that satisfy the following conditions:
k
zˆ z +η (F(z )+B (zˆ z )) α 1+η µ zˆ z , (14)
k k k k k k k 1 k k k
k − − k ≤ k − k
zˆ
k
z
k
+η kF(zˆ k) (α 1p+α 2) 1+η kµ zˆ
k
z
k
, (15)
k − k ≤ k − k
p
where α [0,1) and α (0,1) are user-specified parameters with α +α < 1. When α = 0, the
1 2 1 2 1
∈ ∈
condition in (14) reduces to zˆ = z η (I+η B ) 1F(z ), which corresponds to one iteration
k k k k k − k
−
of quasi-Newton update on the proximal subproblem. In general, this quasi-Newton step can
be performed inexactly and α determines the accuracy of solving the resulting linear system of
1
equations. Moreover, unlikeNPEthatconstrainsthestepsizeintermsofthedisplacement zˆ z
k k
k − k
in (13), we directly impose the condition in (15) to ensure a small error of solving the proximal
subproblem. Compared with the error criterion in (10) in HPE, we observe that the condition is
relaxed by a factor of √1+η µ. This relaxation, which is inspired by [BTB22], allows us to take
k
a potentially larger step size.
To find a pair of (η ,zˆ ) that satisfies both conditions in (14) and (15), we propose a backtracking
k k
line search scheme. Specifically, given a parameter β (0,1), we iteratively test the step size from
∈
the set σ βi : i 0 , where the initial trial step size σ is chosen as σ = η /β for k 1. We
k k k k 1
{ ≥ } − ≥
elaborate on the implementation of our line search scheme in Section 3.1.
In the second stage, we compute z via
k+1
z = θ (z η F(zˆ ))+(1 θ )zˆ , (16)
k+1 k k k k k k
− −
where θ [0,1] is chosen based on our convergence analysis. Specifically, in the monotone setting
k
∈
(under Assumption 1.b), we choose θ = 1 and (16) reduces to the extragradient step in (11).
k
Moreover, in the strongly monotone setting (under Assumption 1.a), we choose θ = 1 .
k 1+2ηkµ
Finally, in the third stage, we update B , the most important module of our algorithm. Rather
k
than following classical quasi-Newton methods such as Broyden’s method or BFGS, our update
rule of B is purely motivated by our convergence analysis. Specifically, as we explain in Section 4,
k
we need to maintain the Jacobian approximation matrix in a certain feasible set Rd d, to
×
Z ⊂
ensure that B is properly conditioned. Moreover, the convergence rate of our method is related to
k
the cumulative loss ℓ (B ), and a smaller cumulative loss implies a faster convergence rate.
k k k
Here, = k : η < σ ∈Bdenotes the indices where the line search scheme backtracks and the loss
k k
f z˜un ic sti aoB nn aℓ uk{ x(B ilik a) ryis itg eiP rv ae tn} eb rey tuℓ k rn(B edk) by= tk hu ek k− lis nB k ekk 2s sk ek a2 r, cw hh se chre emu ek .= HeF n( cz˜ ek ,) th− isF m(z ok ti) v, as tk es= usz˜ k to− uz sek, toa on ld
s
k
from online learning to minimize the cumulative loss. Specifically, when the line search scheme
accepts the initial trial step size σ (i.e., k / ), the Jacobian approximation matrix remains
k
∈ B
unchanged since it does not contribute to the cumulative loss. Otherwise, when the line search
10Subroutine 1 Backtracking line search
1: Input: iterate z Rd, operator g Rd, Jacobian approximationB, initial trial step size σ >0
∈ ∈
2: Parameters: line search parameters β (0,1), α 1 0 and α 2 >0 such that α 1+α 2 <1
∈ ≥
3: Set η + σ, s + LinearSolver(I+η +B, η +g;α 1√1+η +µ) and zˆ + z+s +
← ← − ←
4: while zˆ + z+η +F(zˆ +) (α 1+α 2)√1+η +µ zˆ + z do
k − k≥ k − k
5: Set z˜ zˆ + and η + βη +
← ←
6: Compute s + LinearSolver(I+η +B, η +g;α 1√1+η +µ) and zˆ + z+s +
← − ←
7: end while
8: if η + =σ then
9: Return η + and zˆ +
10: else
11: Return η +, zˆ + and z˜
12: end if
scheme backtracks, we use a projection-free online learning algorithm to update the matrix B .
k+1
We will present the details of this procedure in Section 4.
3.1 Backtracking line search
Next, we present the backtracking line search scheme for selecting η and the iterate zˆ in the first
k k
stage of QNPE. For brevity, we denote F(z ) by g and omit the iteration subscript k from both
k
z and B . In light of (14) and (15), our goal at the k-th iteration is to identify a pair (η ,zˆ )
k k + +
satisfying
zˆ z+η (g+B(zˆ z)) α 1+η µ zˆ z , (17)
+ + + 1 + +
k − − k ≤ k − k
zˆ
+
z+η +F(zˆ +) (α 1p+α 2) 1+η +µ zˆ
+
z . (18)
k − k ≤ k − k
p
As previously discussed, with η fixed, the first condition in (17) can be met by solving the linear
+
system (I+η B)(zˆ z) = η g to a desired accuracy. To formalize, we let
+ + +
− −
s = LinearSolver(I+η B, η g;α 1+η µ) and zˆ = z+s , (19)
+ + + 1 + + +
−
where the LinearSolver oracle is defined as follows. p
Definition 3.1. The oracle LinearSolver(A,b;ρ) takes a matrix A Rd d, a vector b Rd and
×
∈ ∈
ρ > 0 as input, and returns an approximate solution s satisfying As b ρ s .
+ + +
k − k ≤ k k
By Definition 3.1, the pair (η ,zˆ ) is guaranteed to satisfy (17) when zˆ is computed from (19).
+ + +
Moreover, to implement the LinearSolver(A,b;ρ) oracle, a direct way is to compute the exact
solution s = A 1b, but this has a cost of (d3). To avoid this, we rely on conjugate gradient-
+ −
O
type methods to inexactly solve the linear system, which only requires computing matrix-vector
products. The detailed implementation will be further discussed in Section 5.
With the LinearSolver oracle, we introduce our backtracking line search scheme, detailed in Subrou-
tine 1. We first set η to be the initial trial step size σ and then compute zˆ from (19). If the pair
+ +
(η ,zˆ ) satisfies (18), we accept η and zˆ as the final step size and iterate, respectively. Note
+ + + +
that both conditions in (17) and (18) are indeed satisfied. Otherwise, we multiply the step size
by β (0,1) and repeat the process, until η and zˆ satisfy (18). We show that this procedure
+ +
∈
terminates after finite steps (see Appendix A), and we characterize the overall computational cost
in Theorem 6.8. Moreover, in this case, we also return an auxiliary iterate z˜, which is computed
11from (19) using the step size η /β. In other words, z˜ is the last point rejected by our line search
+
scheme before accepting (η ,zˆ ). This iterate z˜ will be used to establish a lower bound on η ,
+ + k
which is further used to define the loss function ℓ for our online learning algorithm.
k
4 Jacobian approximation update via online learning
Next, we discuss the update for B . Our update rule deviates from classical quasi-Newton
k k 0
{ } ≥
methods and is guided by our convergence analysis. In Section 4.1, we discuss key convergence
results of QNPE for monotone and strongly monotone operators. We quantify how the choice of
Jacobian approximation matrices impacts our method’s convergence rate, turning the update of
these matrices into an online convex optimization problem over a feasible set of matrices. Given
the complexity of the feasible set and the computational intractability of computing its projection,
most common projection-based online learning algorithms are precluded from use. In Section 4.2,
we tackle this issue by proposing a projection-free online learning approach inspired by [Mha22],
laying the groundwork for our subsequent Jacobian approximation update. Later in Sections 4.3
and 4.4, we specialize the online learning algorithm in Section 4.2 to different settings, including
nonlinear monotone equations, minimization, minimax optimization, and monotone equations with
sparse Jacobians.
4.1 From convergence rate to online learning
As the first step, we relate the convergence rate of QNPE to the step size η . Before presenting
k
this result in Proposition 4.2, we first introduce the following key lemma, which plays a crucial role
in the convergence analysis.
Lemma 4.1. Suppose that the operator F in (1) is µ-strongly monotone with µ 0. Let z
k k 0
≥ { } ≥
and zˆ be the iterates generated by Algorithm 1, where α [0,1), α (0,1) and α +α < 1.
k k 0 1 2 1 2
Then{ for} a≥ ny z Rd and k 0, it holds that ∈ ∈
∈ ≥
z z 2 1+2η µ
η F(zˆ ),zˆ z k k − k k z z 2+η µ zˆ z 2
k k k k+1 k k
h − i ≤ 2 − 2 k − k k − k
(20)
1 α α
− 1 − 2 ( zˆ z 2+ zˆ z 2).
k k k k+1
− 2 k − k k − k
Proof. To simplify the notation, let α , α +α (0,1). For any z Rd, we have
1 2
∈ ∈
η F(zˆ ),zˆ z = zˆ z +η F(zˆ ),zˆ z + z zˆ ,zˆ z . (21)
k k k k k k k k k k k
h − i h − − i h − − i
For the first term in (21), we can bound it by
zˆ z +η F(zˆ ),zˆ z zˆ z +η F(zˆ ) zˆ z
k k k k k k k k k k
h − − i ≤ k − kk − k
α 1+η µ zˆ z zˆ z
k k k k
≤ k − kk − k
α α(1+η µ)
p zˆ z 2 + k zˆ z 2. (22)
k k k
≤ 2k − k 2 k − k
Here, thefirstinequality is duetotheCauchy-Schwarz inequality, thesecond inequality comes from
(15), and the last inequality is a result of Young’s inequality. Additionally, to handle the second
12term in (21), we apply the three-point equality, yielding:
1 1 1
z zˆ ,zˆ z = z z 2 zˆ z 2 zˆ z 2. (23)
k k k k k k k
h − − i 2k − k − 2k − k − 2k − k
By combining (21), (22) and (23), we obtain that
z z 2 1 α αη µ
η F(zˆ ),zˆ z k k − k − ( zˆ z 2+ zˆ z 2)+ k zˆ z 2. (24)
k k k k k k k
h − i ≤ 2 − 2 k − k k − k 2 k − k
Furthermore, it follows from the update rule in (16) that η F(zˆ ) = z z +2η µ(zˆ z ).
k k k k+1 k k k+1
− −
Thus, for any z Rd, it also holds that
∈
η F(zˆ ),z z = z z ,z z +2η µ zˆ z ,z z
k k k+1 k k+1 k+1 k k k+1 k+1
h − i h − − i h − − i
z z 2 z z 2 1+2η µ
= k k − k k k − k+1 k k z z 2 (25)
k+1
2 − 2 − 2 k − k
+η µ zˆ z 2 η µ zˆ z 2,
k k k k k+1
k − k − k − k
whereweappliedthethree-pointequality twiceinthelastequality. Hence, byaddingtheinequality
in (24) with z = z to the inequality in (25), we obtain
k+1
η F(zˆ ),zˆ z = η F(zˆ ),z z +η F(zˆ ),zˆ z
k k k k k k+1 k k k k+1
h − i h − i h − i
kz
k
−z k2 ❳ kz❳ k❳ −❳z
k❳+1
❳k❳2 1+2η kµ
z
k+1
z 2
≤ 2 − 2 − 2 k − k
+η kµ zˆ
k
z 2 η kµ zˆ
k
z
k+1
2+❳ kz❳ k❳ −❳z
k❳+1
❳k❳2 (26)
k − k − k − k 2
1 α αη µ
− ( zˆ z 2+ zˆ z 2)+ k zˆ z 2.
k k k k+1 k k+1
− 2 k − k k − k 2 k − k
Moreover,sinceα < 1,wefurtherhave η µ zˆ z 2+αηkµ zˆ z 2 ηkµ zˆ z 2
− k k k − k+1 k 2 k k − k+1 k ≤ − 2 k k − k+1 k ≤
0. Combining this with (26) and rearranging the terms, we arrive at the desired result in (20).
Building on Lemma 4.1, we obtain the following convergence result for Algorithm 1.
Proposition 4.2. Let z and zˆ be the iterates generated by Algorithm 1, where α
k k 0 k k 0 1
{ } ≥ { } ≥ ∈
[0,1), α (0,1) and α +α < 1.
2 1 2
∈
(a) Under Assumption 1.a, we have z z 2 z z 2(1+2η µ) 1 for any k 0.
k+1 ∗ k ∗ k −
k − k ≤ k − k ≥
(b) Under Assumption 1.b, we have z z z z for any k 0. Moreover, define
k+1 ∗ k ∗
the averaged iterate z¯ by z¯
=k PN k=− 0− 1ηkzˆ kk
.
≤ Thk en−
for
ak
ny
compact≥
set Rd, we have
N N PN k=− 01ηk D ⊂
Gap(z¯ N; D)
≤
max 2z P∈D
N
k=−k 0z 10 η− kz k2 .
Proof. Let α = α +α . First, we prove Part (a). Since F(z ) = 0 and F is µ-strongly monotone
1 2 ∗
by Assumption 1.a, it holds that
F(zˆ ),zˆ z = F(zˆ ) F(z ),zˆ z µ zˆ z 2. (27)
k k ∗ k ∗ k ∗ k ∗
h − i h − − i≥ k − k
By setting z = z in (20) and applying (27), it yields
∗
1 1+2η µ 1 α
0 z k z ∗ 2 k z k+1 z ∗ 2 − ( zˆ k z k 2+ zˆ k z k+1 2). (28)
≤ 2k − k − 2 k − k − 2 k − k k − k
13Since α = α + α < 1, we can drop the non-positive last term in (28) and obtain 0 1 z
1 2 ≤ 2k k −
z 2 1+2ηkµ z z 2, which is equivalent to z z 2 z z 2(1+2η µ) 1. This
∗ k − 2 k k+1 − ∗ k k k+1 − ∗ k ≤ k k − ∗ k k −
completes the proof of Part (a).
Next, we prove Part (b). Since F is monotone by Assumption 1.b, we have
F(zˆ ),zˆ z = F(zˆ ) F(z ),zˆ z 0. (29)
k k ∗ k ∗ k ∗
h − i h − − i≥
By setting z = z in (20) with µ = 0 and applying (29), it yields
∗
1 1 1 α
0 z k z ∗ 2 z k+1 z ∗ 2 − ( zˆ k z k 2+ zˆ k z k+1 2). (30)
≤ 2k − k − 2k − k − 2 k − k k − k
Again, since the last term in (30) is non-positive, this immediately implies 1 z z 2 1 z
2k k+1 − ∗ k ≤ 2k k −
z 2, which is equivalent to z z z z . Additionally, by dropping the non-positive
∗ k+1 ∗ k ∗
k k − k ≤ k − k
lasttermin(20) andnotingthatµ = 0, wealso haveη F(zˆ ),zˆ z 1 z z 2 1 z z 2.
k h k k − i≤ 2k k − k −2k k+1 − k
By summing this inequality from k = 0 to k = N −1, we further get kN =−01η
k
hF(zˆ k),zˆ
k
−z
i ≤
1 z z 2 1 z z 2 1 z z 2. Hence, it follows from Lemma 2.2 that Gap(z¯ ; )
2k 0 − k − 2k N − k ≤ 2k 0 − k P N D ≤
max 2z P∈D N k=−k 0z 10 η− kz k2 , where z¯ N is the averaged iterate given by z¯ N = P PN k= N k− =0 −1 01ηk ηkzˆ k.
Proposition 4.2 highlights the role of η in the convergence analysis. In particular, in the strongly
k
monotonesetting,ifη k tendstoinfinityasthenumberofiterationskincreases,thenlim k kz zk+1 − zz ∗∗ k =
→∞ k k− k
0 by Proposition 4.2(a), i.e., QNPE converges superlinearly. Similarly, in the monotone setting, a
larger step size results in a faster convergence rate according to Proposition 4.2(b). That said,
note that the step size η cannot be arbitrarily selected, since it is constrained by the line search
k
conditions in (14) and (15). Intuitively, η depends on how well our Jacobian approximation matrix
k
B captures the local curvature of the operator F. This intuition is made precise in the following
k
lemma, where we present a lower bound on η . Recall that is the set of indices where the line
k
B
search subroutine backtracks, i.e., = k : η < σ .
k k
B { }
Lemma 4.3. Recall that z˜ is the auxiliary iterate returned by our line search scheme in Sub-
k
r Mou ot ri en oe ve1 r,. ifFo 1r (Bk ∈/ +B Bw )e ha 1v µe I,η wk e= haσ vk e, w z˜hile zfor k ∈ 1B +αw
1
e zh ˆave zη k >
.
kF(z˜ k) −α F2 (β zk kz˜ )k −−Bz kk (k z˜ k−z k) k.
2 k ⊤k (cid:23) 2 k k − k k ≤ β(1 α1)k k − k k
−
Proof. If k / , as per the definition, the line search scheme adopts the initial trial step size at
∈ B
the k-th iteration and thus η = σ . On the other hand, if k , we go through the backtracking
k k
∈ B
procedure in Subroutine 1. Let z˜ denote the last rejected point in the line search scheme, which
k
is calculated from (19) using the step size η˜ = η /β. This means that the pair (z˜ ,η˜ ) satisfies
k k k k
(17) but not (18), i.e., z˜ z +η˜ (F(z )+B (z˜ z)) α √1+η˜ µ z˜ z and z˜ z +
k k k k k k 1 k k k k k
k − − k ≤ k − k k −
η˜ F(z˜ ) > (α +α )√1+η˜ µ z˜ z . Moreover, note that η˜ (F(z˜ ) F(z ) B (z˜ z )) =
k k 1 2 k k k k k k k k k
k k − k − − −
(z˜ z +η˜ F(z˜ )) (z˜ z + η˜ (F(z ) + B (z˜ z ))). Thus, it follows from the triangle
k k k k k k k k k k k
− − − −
inequality thatη˜ F(z˜ ) F(z ) B (z˜ z ) > (α +α )√1+η˜ µ z˜ z α √1+η˜ µ z˜
k k k k k k 1 2 k k k 1 k k
k − − − k k − k− k −
1z k +k η˜= µα 2√ 11
,
w+ eη˜ k oµ btk az˜ ik n− thz ek fik r. stT reh si us ltfu ir nth Le er mim mp ali 4e .s 3.th Ta ot pη˜ rk ov>
e
thkF e(z s˜α k e2 ) c−√ oF n1+ d(zη˜ kk r)µ e−k sBz u˜ k k l− t(z˜z ikk n−k z Lk e) km. maSin 4.c 3e
,
k
≥
recall from (17) that zˆ and z˜ are inexact solutions of the linear system of equations:
k k
(I+η B )(z z ) = η F(z ) and (I+η˜ B )(z z )= η˜ F(z ),
k k k k k k k k k k
− − − −
14respectively. Define zˆ = z η (I+η B ) 1F(z ) and z˜ = z η˜ (I+η˜ B ) 1F(z ), i.e., the
k∗ k
−
k k k − k k∗ k
−
k k k − k
exact solutions of the above linear systems. Since (zˆ ,η ) and (z˜ ,η˜ ) satisfy the condition in (17),
k k k k
we have
(I+η B )(zˆ zˆ ) α 1+η µ zˆ z (31)
k
k k k
−
k∗
k ≤
1 k
k
k
−
k
k
and k(I+η˜ kB k)(z˜
k
−z˜ k∗)
k ≤
α 1p1+η˜ kµ kz˜
k
−z
k
k. (32)
p
We divide the proof of the second result in Lemma 4.3 into the following three steps. First, we
show that
(1 α ) zˆ z zˆ z (1+α ) zˆ z , (33)
−
1
k
k
−
k
k ≤ k
k∗
−
k
k ≤
1
k
k
−
k
k
(1 α ) z˜ z z˜ z (1+α ) z˜ z . (34)
−
1
k
k
−
k
k ≤ k
k∗
−
k
k ≤
1
k
k
−
k
k
In the following, we will only prove (33), since the proof of (34) follows similarly. Since we assume
1(B + B ) µI, we have v B v µ v 2 for any v Rd. Therefore, by using Cauchy-
2 k ⊤k (cid:23) 2 ⊤ k ≥ 2k k ∈
Schwarz inequality, we get zˆ zˆ (I +η B )(zˆ zˆ ) (zˆ zˆ ) (I+η B )(zˆ zˆ )
k
k
−
k∗
kk
k k k
−
k∗
k ≥
k
−
k∗ ⊤ k k k
−
k∗
≥
(1+ηkµ) zˆ zˆ 2,whichfurtherimpliesthat (I+η B )(zˆ zˆ ) (1+1η µ) zˆ zˆ . Moreover,
2 k k − k∗ k k k k k − k∗ k ≥ 2 k k k − k∗ k
since √1+η µ 1+ 1η µ, by (31) we also have (I+η B )(zˆ zˆ ) α (1+ 1η µ) zˆ z .
k ≤ 2 k k k k k − k∗ k ≤ 1 2 k k k − k k
Hence, combining these two inequalities, we get zˆ zˆ α zˆ z . It then follows from the
k
k
−
k∗
k≤
1
k
k
−
k
k
triangle inequality that
zˆ z zˆ z + zˆ zˆ (1+α ) zˆ z ,
k
k∗
−
k
k ≤ k
k
−
k
k k
k∗
−
k
k ≤
1
k
k
−
k
k
zˆ z zˆ z zˆ zˆ (1 α ) zˆ z ,
k
k∗
−
k
k ≥ k
k
−
k
k−k
k∗
−
k
k ≥ −
1
k
k
−
k
k
which proves (33). Next, we show that
1
z˜ z zˆ z . (35)
k
k∗
−
k
k ≤ βk
k∗
−
k
k
This follows from [MS10, Lemma 7.8]. For completeness, we present its proof below. Note that by
definition, we have (I+η B )(zˆ z ) = η F(z ) and (I+η˜ B )(z˜ z ) = η˜ F(z ). Hence,
k k k∗
−
k
−
k k k k k∗
−
k
−
k k
we further have
1 1 1 1 1
B (zˆ z˜ )= (z˜ z ) (zˆ z ) = (z˜ zˆ )+ (zˆ z ).
k k∗
−
k∗
η˜
k∗
−
k
− η
k∗
−
k
η˜
k∗
−
k∗
η˜ − η
k∗
−
k
k k k (cid:18) k k(cid:19)
Since (zˆ z˜ ) B (zˆ z˜ ) 0, by taking the inner product with zˆ z˜ on both sides of the
k∗
−
k∗ ⊤ k k∗
−
k∗
≥
k∗
−
k∗
above inequality, we obtain 1 1 (zˆ z ) (zˆ z˜ ) 1 zˆ z˜ 2. Since η = βη˜ , using
η˜k − ηk k∗ − k ⊤ k∗ − k∗ ≥ η˜kk k∗ − k∗ k k k
the Cauchy-Schwarz inequal(cid:16) ity, this f(cid:17) urther leads to zˆ z˜ 2 1 1 (z zˆ ) (zˆ z˜ )
k k∗ − k∗ k ≤ β − k − k∗ ⊤ k∗ − k∗ ≤
1 1 zˆ z zˆ z˜ . Hence, we further have zˆ z˜ (cid:16)1 1 (cid:17)zˆ z , which implies
β − k k∗ − k kk k∗ − k∗ k k k∗ − k∗ k≤ β − k k∗ − k k
(cid:16)that z˜(cid:17) z z˜ zˆ + zˆ z 1 zˆ z . This com(cid:16)pletes(cid:17)the proof of (35). Finally,
k k∗ − k k ≤ k k∗ − k∗ k k k∗ − k k ≤ βk k∗ − k k
by combining (33), (34), and (35), it follows that z˜ z 1 z˜ z 1 zˆ z
k k − k k≤ 1 α1k k∗ − k k≤ (1 α1)βk k∗ − k k≤
1+α1 z˜ z 1+α1 zˆ z . This proves the second resu− lt in Lemma 4.3.−
1 α1k k∗ − k k ≤ (1 α1)βk k − k k
− −
Lemma4.3demonstratesthatthestepsizeη dependsinversely ontherelative approximationerror
k
kF(z˜ k) −F(z k) −B k(z˜ k−z k)
k. Moreover, a smaller approximation error leads to a larger step size, which
z˜ z
in turn ikmk p−liek skfaster convergence. Note that the lower bound in Lemma 4.3 for η is expressed
k
in terms of z˜ , which is not accepted as the actual iterate. Hence, we use the second result in
k
15Lemma 4.3 to relate z˜ z with zˆ z . Building on Proposition 4.2 and Lemma 4.3, we
k k k k
k − k k − k
are ready to quantify the relationship between the convergence rate of QNPE and the choice of the
Jacobian approximation matrix B .
k
To begin with, in the strongly monotone setting, by repeatedly applying Proposition 4.2(a), we
obtain that
k kz zN
0
−− zz ∗∗ kk 22 ≤N k=− 01 (1+2η kµ) −1
≤
(cid:18)1+ kN2 =−µ 01N
1/η
k(cid:19)−N , (36)
Y
where the last inequality is obtained by applying Jensen’s iPnequality to the function log(1 + 1).
t
Similalry, in the monotone setting, it follows from Proposition 4.2(b) that
Gap(z¯ ;z)
kz
0
−z k2 kz
0
−z k2 N −1 1
, (37)
N ≤ 2 kN =−01η k ≤ 2N2 k=0 η k
X
P
where the last inequality is due to Cauchy-Schwarz inequality. Thus, in light of both (36) and
(37), our goal is to establish an upper bound on kN =−011/η
k
and this is achieved in the next lemma.
Lemma 4.4. Let η N 1 be the step sizes in APlgorithm 1 obtained by the line search in Subrou-
{
k }k=−0
tine 1. Then,
N −1 1 1 1 u
k
B ks
k
2
+ N k − k , (38)
η ≤ (1 β)σ (1 β)α β s 2
k 0 2 s k
k=0 − − k k k
X X∈B
where u , F(z˜ ) F(z ), s , z˜ z , and z˜ is the auxiliary iterate in Subroutine 1.
k k k k k k k
− −
Proof. Let u , F(z˜ ) F(z ) and s , z˜ z . In Lemma 4.3, we showed that η = σ if k /
k k k k k k k k
− − ∈ B
and η k > uα2β Bks k sk otherwise. Using the observations above, we can write
k k− k kk
N 1
− 1 1 1 1 1 1 1 1
= + + +β + , (39)
η η η ≤ σ η ≤ σ η η
k k k k k 0 k 1 k
Xk=0 k X∈/ B k X∈B k X∈/ B k X∈B k ∈/ XB,k ≥1 − k X∈B
where we used σ
k
= ηk β−1 for k
≥
1 in the last equality. Since
k ∈/ B,k ≥1
ηk1
−1 ≤
kN =−01 η1 k, rearrang-
ing and simplifying the terms in (39), we arrive at
P P
N 1
− 1 1 1 1
+ . (40)
η ≤ (1 β)σ 1 β η
k 0 k
k=0 − − k
X X∈B
Now using η k > kuα k2 −β Bks kk sk
kk
for k
∈
B, we further have
1 u B s 1 u B s 2 1 u B s 2
k k k k k k k k k
k − k k − k N k − k , (41)
η ≤ α β s ≤ α β |B| s 2 ≤ α β s 2
k k k 2 k k k 2 s k k k k 2 s k k k k
X∈B X∈B X∈B X∈B
where the second inequality is due to the generalized mean inequality. Finally, the inequality in
(38) follows from (40) and (41). This completes the proof.
Nowgiventheaboveresultsweobservetheconnectionbetweenthechoiceof B andourmethod’s
k
{ }
convergencerate. Specifically,bycombiningtheresultsin(36)and(37)withLemma4.4,weobserve
16u B s 2
t isha at so su mr ag lloa al sis poto ssiu bp leda tt oe at ch he ieJ va eco tb hi ean faa stp ep sr tox pi om ssa it bi lo en cm ona vt er ric ge es nc{ eB rk a} tes .uc Mh oth rea ot ver,k
∈ nB
otk ek tk− hs akktk 2 tk hk
e
P
variables u = F(z˜ ) F(z ) and s = z˜ z are determined by Subroutine 1 only after the
k k k k k k
− −
matrix B has been chosen. This implies that the matrix B must be selected first, after which we
k k
u B s 2
can compute the approximation error as k k−
s
k 2kk . Our key insight is that this exactly fits into
k kk
the framework of an online learning problem by regarding the sum in (38) as the cumulative loss
incurred by our choice of B .
k k 0
{ } ≥
Formally, define the loss function ℓ : Rd d R at iteration k as
k ×
→
0, if k / ,
ℓ (B) , ∈ B (42)
k u Bs 2
(k k−
s
2kk , otherwise.
k kk
Thenweconsideranonlinelearningproblemasfollows: (i)Atthebeginningofthek-thiteration,we
commit to a Jacobian approximation matrix B from a given convex feasible set ; (ii) We receive
k
Z
the loss function ℓ (B) defined in (42); (iii) We update our Jacobian approximation matrix to
k
B . Bounding the sum in (38) is equivalent to constraining the cumulative loss in the preceding
k+1
online learning problem. Therefore, we are motivated to utilize an online learning algorithm to
update B .
k k 0
{ } ≥
The choice of feasible set . Before discussing the online learning algorithm, let us address the
Z
choice of the feasible set for selecting B . We provide a high-level overview of why the feasible
k
set structure is crucial inZ constraining kN =−01ℓ k(B k).
Specifically, in the strongly monotone cPase, we decompose the cumulative loss as kN =−01ℓ k(B k) =
kN =−01ℓ k(H)+( kN =−01ℓ k(B k)
−
kN =−01ℓ k(H)), where the first part kN =−01ℓ k(H) is Pthe cumulative
loss incurred by choosing a fixed matrix H , and the second part is known as the regret with
P P P ∈ Z P
respecttoH intheonlinelearningliterature. By usingtools fromonlinelearning, weprove aregret
bound in the form of kN =−01ℓ k(B k)
−
kN =−01ℓ k(H)
≤
Reg N(H; Z), where Reg N(H; Z) denotes a
function depending on N, the matrix H and the feasible set . Thus, by choosing any H ,
we obtain an upper bP ound as kN =−01ℓP k(B k)
≤
kN =−01ℓ k(H) +Z Reg N(H). Intuitively, a nat∈ urZ al
choice is setting H = F(z ), the true Jacobian matrix at the solution z . Thus, we require
∗ ∗
∇ P P
F(z ) . In the monotone setting, we use a similar approach but decompose the cumulative
∗
∇ loss as ∈ kNZ =−01ℓ k(B k) = kN =−01ℓ k(H k) + ( kN =−01ℓ k(B k)
−
kN =−01ℓ k(H k)), where {H
k
}kN =−01 is an
arbitrary sequence of matrices in . In this decomposition, the second part is known as the
P P Z P P
dynamic regret and we can again utilize tools from online learning to prove a regret bound in the
form of kN =−01ℓ k(B k)
−
kN =−01ℓ k(H k)
≤
D-Reg N(H 0,...,H
N
−1; Z). It turns out that the proper
choicefor H istosetH = F(z )fork = 0,...,N 1,andhence,werequirethat F(z )
k k k k
P{ } P ∇ − ∇ ∈ Z
for any k 0.
≥
Additionally, the set constrains our approximation matrices B , ensuring that their operator
k
Z
norm B remains bounded as (L ). This constraint, as demonstrated later in Lemma 6.3,
k op 1
k k O
is essential for achieving a convergence rate similar to EG. Moreover, we require B to satisfy
k
1(B +B ) Ω(µ)I in the strongly monotone setting or 1(B +B ) 0 in the monotone setting.
2 k ⊤k (cid:23) 2 k ⊤k (cid:23)
This condition is pivotal in Lemma 4.3 and ensures our LinearSolver oracle can be implemented
efficiently.
Finally, when F(z ) possesses a certain structure, we can include additional constraints in
∗
∇ Z
to enforce the same structure on B , making the subroutines more efficient and reducing storage
k
requirements. For instance, as discussed in Section 2, if F is derived from a minimization problem
in (4) or a minimax optimization problem in (5), then F(z) is symmetric or J-symmetric (i.e.,
∇
17J F(z) = F(z) J). In addition, if F has a sparse Jacobian with sparsity pattern Ω, then
⊤
∇ ∇
[ F(z)] = 0 for all (i,j) that satisfy (i,j) / Ω and i = j.
ij
∇ ∈ 6
Given these points and the discussions in Section 2.2, we choose the feasible set as follows:
Z
(a) For the minimization problem in (4), we choose
= B Sd : µI B L I, B , (43)
1
Z { ∈ (cid:22) (cid:22) ∈ L}
where Sd is a linear subspace. in the general case, we set = Sd. However, when
L ⊂ L
the Hessians of f exhibit the sparsity pattern Ω, we define as = B Sd : [B] =
ij
L L { ∈
0 for all (i,j) / Ω and i = j .
∈ 6 }
(b) For the minimax problem in (5) and the nonlinear equation in (1), we choose
1
= B Rd d :µI (B+B ) L I, B L ,B , (44)
× ⊤ 1 op 1
Z ∈ (cid:22) 2 (cid:22) k k ≤ ∈ L
(cid:26) (cid:27)
where Rd d is a linear subspace. For general nonlinear equations = Rd d, for minimax
× ×
L ⊂ L
problems = B Rd d : B = JB J , and for sparse nonlinear equations = B Rd d :
× ⊤ ×
L { ∈ } L { ∈
[B] = 0 for all (i,j) / Ω and i = j .
ij
∈ 6 }
Finally, we note that while the feasible set defined in (43) or (44) satisfies all desired properties,
Z
it poses a major computational challenge. Specifically, most online learning algorithms, such as
projected online gradient descent [Zin03], requireperformingEuclidean projection onto the feasible
set ,whichwouldbecomputationally costly duetothestructureof . Toaddressthis, weemploy
Z Z
a projection-free online learning algorithm that is built on the approximate separation oracle in
[Mha22], which is described in the next section.
4.2 Online learning with an approximate separation oracle
To set the stage for our Jacobian approximation update algorithm, we take a detour and con-
sider a general online learning problem over a compact feasible set RD. For T consecu-
C ⊂
tive rounds t = 0,...,T 1, a learner chooses an action x RD and then observes a con-
t
− ∈
vex loss function ℓ : RD R. The goal is to minimize either the static regret defined by
t
→
Reg T(u; C) , T t=−01ℓ t(x t)
−
T t=−01ℓ t(u) with a fixed competitor x
∈
C, or the dynamic regret
defined by D-R Peg T(u 0,...,u TP−1; C) , T t=−01ℓ t(x t)
−
T t=−01ℓ t(u t) with a sequence of competitors
u for any t 0. We assume 0 without loss of generality. Moreover, we assume that the
t
∈ C ≥ ∈ CP P
set is contained in the Euclidean ball B (0) := w RD : w R for some radius R > 0.
R
C { ∈ k k ≤ }
Most online learning algorithms require projection onto the feasible set . However, the projection
C
oracle is computationally intractable in our setting. To address this issue, inspired by [Mha22], we
propose a projection-free algorithm that relies on an approximate separation oracle. To start, we
will define the approximate separation oracle SEP for a compact set . We use span( ) to denote
C C C
the linear span of , i.e., span( ) = k α x : k Z , x , α R . Intuitively, span( ) is
C C i=1 i i ∈ + i ∈ C i ∈ C
the smallest linear subspace that conntains the set . o
P C
Definition 4.1. Let be a compact convex set in RD containing the origin. The oracle SEP (w;δ)
C C
takes w span( ) and δ > 0 and returns γ > 0 and a vector s span( ) with one of these outcomes:
∈ C ∈ C
• Case I: γ 1, which implies that w (1+δ) ;
≤ ∈ C
18Algorithm 2 Projection-Free Online Learning
1: Input: Initial point w 0 span( ), the orthogonal projection matrix P associated with the subspace
∈ C
span( ), step size ρ>0, δ >0, radius R>0
C
2: for t=0,1,...T 1 do
−
3: Query the oracle (γ t,s t) SEP (w t;δ)
4: if γ t 1 then # Case← I: we hC ave w t (1+δ)
≤ ∈ C
w (Option I)
5: Set x t ←( 1w +t t
δ
(Option II) and play the action x t
6: Receive the loss ℓ t(x t) and the subspace gradient g t =P ℓ t(x t)
∇
7: Set g˜ t g t
←
8: else # Case II: we have w t/γ t (1+δ)
∈ C
wt (Option I)
9: Set x t ←( (γ 1t +w δt
)γt
(Option II) and play the action x t
10: Receive the loss ℓ t(x t) and the subspace gradient g t =P ℓ t(x t)
∇
11: Set g˜ t ←g t+max 0, −γ1 thg t,w t
i
s t
12: end if n o
13: Update w t+1 ← maxR( ww tt − ρρ g˜g˜ tt) 2,R # Online projected gradient descent
14: end for {k − k }
• Case II: γ > 1, which implies that w/γ (1+δ) and s,w x γ 1, x .
∈ C h − i≥ − ∀ ∈ C
By Definition 4.1, given an input w span( ), the SEP (w;δ) has two possible outcomes: either
∈ C C
it certifies that w is approximately feasible and lies in (1+δ) (Case I), or it produces a scaled
C
version of w that is in (1+δ) and provides a strict separating hyperplane between w and (Case
C C
II). We note that the requirement of w span( ) is necessary. Otherwise, we have w/γ / (1+δ)
∈ C ∈ C
for any γ R and thus neither of the two cases in Definition 4.1 holds.
∈
Once equipped with the SEP oracle, we are ready to present our projection-free online learning
C
algorithm, detailed in Algorithm 2. We remark that Algorithm 2 has two different options: in
Option I, we slightly relax the feasibility requirement and allow the iterates x to be in a
t t 0
{ }≥
larger set (1+δ) , whereas in Option II, the iterate satisfies x for all t 0. As we shall see
t
C ∈ C ≥
later in Section 6, we will use Option I in the strongly monotone setting and Option II in the
monotone setting.
Remark 4.1. There are several differences between Algorithm 1 in [Mha22] and our presentation
here. First, a standard online learning setup was considered in [Mha22] where the action x must
t
be in the feasible set , while in our setting x can be chosen from a larger set (1+δ) in Option I.
t
C C
Second, their algorithm relied on an oracle that approximates the gauge function γ (w) , inf λ
C { ≥
0 : w λ and its subgradient, which is further explicitly constructed using a membership oracle.
∈ C}
Our oracle in Definition 4.1 is different but related, in the sense that its output γ and s may also
be regarded as an approximation of the gauge function and its subgradient. It is also more general,
since in [Mha22] they assume that is full dimensional, i.e., span( ) = RD. Finally, we focus on
C C
the specific set used in our Jacobian approximation update and offer a more refined regret analysis
along with an efficient construction of the oracle.
To shed light on the design of Algorithm 2, we remark that it can be regarded as a black-box
reduction that transforms the original online learning problem over the feasible set into an
C
auxiliary online learning problem on the larger set B (0). Specifically, the auxiliary online learning
R
problem is defined by surrogate loss functions ℓ˜(w) = g˜ ,w for 0 t T 1, where g˜ is the
t t t
h i ≤ ≤ −
19surrogategradienttobedefinedlater. Insteadofupdatingtheiterates x intheoriginalonline
t t 0
{ }≥
learning problem directly, we will run online projected gradient descent on this auxiliary problem
to obtain the iterates w (note that the projection onto B (0) is easy to compute), and then
t t 0 R
{ }≥
generate the iterates x by calling SEP (w ;δ). More precisely, we initialize w span( ),
t t 0 t 0
{ }≥ C ∈ C
and as we shall prove in Lemma 4.5, we can guarantee that w span( ) for any t 0. Consider
t
∈ C ≥
the iterate w at round t. Since w span( ), SEP (w ;δ) is well-defined and let γ > 0 and
t t t t
s RD be its output. Now we consid∈ er two cC ases depC ending on the value of γ .
t t
∈
(a) InCaseIwhereγ 1, weseteitherx = w inOption I,orx = wt inOption II.Further,
t ≤ t t t 1+δ
denote by P the orthogonal projection matrix associated with the subspace span( ), and we
C
compute the subspace gradient g = P ℓ (x ) span( ). Then the surrogate gradient is
t t t
∇ ∈ C
chosen as g˜ = g .
t t
(b) Otherwise, in Case II where γ > 1, we set either x = wt in Option I, or x = wt
t t γt t (1+δ)γt
in Option II. We further compute the subspace gradient g = P ℓ (x ) and define the
t t t
∇
surrogate gradient by g˜ = g +max 0, 1 g ,w s .
t t { −γth t t i} t
Finally, we update w following the standard online projected gradient descent with step size
t+1
ρ > 0:
R
w = Π w ρ ℓ˜(w ) = (w ρg˜ ). (45)
t+1 BR(0) t − ∇ t t max w
t
ρg˜
t
2,R t − t
{k − k }
(cid:0) (cid:1)
We note that the surrogate loss functions ℓ˜(w) T 1 are constructed explicitly to guarantee that
the immediate regret ℓ˜(w ) ℓ˜(u) = g˜{
,wt
}
ut=−0
serves as an (approximate) upper bound on
t t t t t
− h − i
ℓ (x ) ℓ (u) for any u . As a result, the regret of the original problem can be upper bounded
t t t
− ∈ C
by the regret of the auxiliary problem, which can be further bounded by standard analysis for
online projected gradient descent. This is formalized in the following lemma.
Lemma 4.5. Suppose the loss function ℓ is convex for any t 0 and let x T 1 and w T 1 be
t
≥ {
t }t=−0
{
t }t=−0
the iterates generated by Algorithm 2. Then we have w span( ) for any t 0. Moreover:
t
∈ C ≥
(a) In Option I, we have x (1+δ) for t = 0,1,...,T 1. Also, for any u , it holds that
t
∈ C − ∈ C
w u 2 w u 2 ρ
ℓ (x ) ℓ (u) g˜ ,w u k t − k2 k t+1 − k2 + g˜ 2, (46)
t t − t ≤ h t t − i≤ 2ρ − 2ρ 2k t k2
g˜ g + g ,x s . (47)
t t t t t
k k ≤k k |h i|k k
(b) In Option II, we have x for t = 0,1,...,T 1. Also, for any u , it holds that
t
∈ C − ∈ C
ℓ (x ) ℓ (u) g˜ ,w u δ g ,x (48)
t t t t t t t
− ≤ h − i− h i
w u 2 w u 2 ρ
k t − k2 k t+1 − k2 + g˜ 2 δ g ,x , (49)
≤ 2ρ − 2ρ 2k t k2− h t t i
g˜ g +(1+δ) g ,x s . (50)
t t t t t
k k ≤ k k |h i|k k
Proof. To begin with, we prove by induction that w span( ). By initialization, we have w
t 0
∈ C ∈
span( ). Now suppose w span( ) for some t 0. Since g = P ℓ (x ) span( ) and
t t t t
C ∈ C ≥ ∇ ∈ C
s span( ) by Definition 4.1, we obtain that g˜ span( ) in both Options I and II. Moreover,
t t
∈ C ∈ C
since w is expressed as a linear combination of w and g˜ according to (45), we obtain that
t+1 t t
w span( ). Hence, we conclude by induction that w span( ) for any t 0.
t+1 t
∈ C ∈ C ≥
20Next, we consider the result in Part (a) for Option I. We distinguish two cases depending on the
outcome of SEP(w ;δ).
t
• If γ 1, we have w (1+δ) by Definition 4.1. Thus, we have x = w (1+δ) and
t t t t
≤ ∈ C ∈ C
g˜ = g ,whichimmediatelyimplies(47). Moreover, sinceℓ isconvex, wehaveℓ (x ) ℓ (u)
t t t t t t
− ≤
ℓ (x ),x u . Note that both x and u are in span( ) and recall that P denotes the
t t t t
h∇ − i C
orthogonal projection matrix associated with span( ). Thus, we further have x u =
t
C −
P(x u),whichimpliesthat ℓ (x ),x u = ℓ (x ),P(x u) = P ℓ (x ),x u =
t t t t t t t t t t
− h∇ − i h∇ − i h ∇ − i
g ,x u . Therefore, we have ℓ (x ) ℓ (u) g ,x u = g˜ ,w u , which proves the
t t t t t t t t t
h − i − ≤ h − i h − i
first inequality in (46).
• Otherwise, ifγ > 1, wehave wt (1+δ) byDefinition 4.1 and s ,w x γ 1, x .
t γt ∈ C h t t − i ≥ t − ∀ ∈ C
In this case, Algorithm 2 chooses x = wt (1+δ) and g˜ = g +max 0, 1 g ,w s =
t γt ∈ C t t { −γth t t i} t
g +max 0, g ,x s . To prove the first inequality in (46), first note that we also have
t t t t
{ −h i}
ℓ (x ) ℓ (u) g ,x u using similar arguments as above. Moreover, for any u ,
t t t t t
− ≤h − i ∈ C
g˜ ,w u = g +max 0, g ,x s ,w u
t t t t t t t
h − i h { −h i} − i
= g ,γ x u +max 0, g ,x s ,w u
t t t t t t t
h − i { −h i}h − i
g ,x u +(γ 1) g ,x +(γ 1)max 0, g ,x
t t t t t t t t
≥ h − i − h i − { −h i}
g ,x u ,
t t
≥ h − i
whereweusedw = γ x inthesecondequality and s ,w u γ 1inthefirstinequality.
t t t t t t
h − i ≥ −
Also, by the triangle inequality we obtain
g˜ = g +max 0, g ,x s g + g ,x s ,
t 2 t t t t 2 t 2 t t t 2
k k k { −h i} k ≤ k k |h i|k k
which proves (47).
Finally, from the update rule of w in (45), for any u (0), we have w ρg˜
t+1 R t t
∈ C ⊂ B h − −
w ,w u 0. This further implies that
t+1 t+1
− i ≥
1
g˜ ,w u g˜ ,w w + w w ,w u
t t t t t+1 t t+1 t+1
h − i ≤ h − i ρh − − i
w u 2 w u 2 w w 2
= g˜ ,w w + k t − k2 k t+1 − k2 k t − t+1 k2
t t t+1
h − i 2ρ − 2ρ − 2ρ
1 1 ρ
w u 2 w u 2+ g˜ 2, (51)
≤ 2ρk t − k2− 2ρk t+1 − k2 2k t k2
where we used the Young’s inequality hg˜ t,w t −w t+1
i ≤
kwt − 2w ρt+1 k2 + ρ 2kg˜ t k2 in the last inequality.
This proves the second inequality in (46).
Next, we consider Part (b) for Option II. Similarly, we distinguish two cases depending on the
outcome of SEP(w ;δ).
t
• If γ 1, we have w (1+δ) by Definition 4.1. According to Option II in Algorithm 2,
t t
≤ ∈ C
we have x = wt and g˜ = g , which immediately implies (50). Using similar arguments
t 1+δ ∈C t t
as in Option I, we have ℓ (x ) ℓ (u) g ,x u . Moreover, since w = (1+δ)x , we
t t t t t t t
− ≤ h − i
have g ,w u = g ,(1+δ)x u = g ,x u +δ g ,x . Combining these two, we
t t t t t t t t
h − i h − i h − i h i
further have ℓ (x ) ℓ (u) g ,x u g ,w u δ g ,x . This proves the inequality
t t t t t t t t t
− ≤ h − i≤ h − i− h i
in (48).
21• Otherwise, if γ > 1, By Definition 4.1 we have wt (1 + δ) and s ,w x γ 1
x .
Accot
rding to Option II in Algorithm
γ 2t
,
∈
we have
xC
=
h wt
t
t − i an≥
d
g˜t −
=
∀ ∈ C t γt(1+δ) ∈ C t
g +max 0, 1 g ,w s = g +max 0, (1+δ) g ,x s . Following similar arguments as
t { −γth t t i} t t { − h t t i} t
in Option I, we have ℓ (x ) ℓ (u) g ,x u . Moreover, for any u ,
t t t t t
− ≤ h − i ∈ C
g˜ ,w u = g +max 0, (1+δ) g ,x s ,w u
t t t t t t t
h − i h { − h i} − i
= g ,(1+δ)γ x u +max 0, (1+δ) g ,x s ,w u
t t t t t t t
h − i { − h i}h − i
g ,(1+δ)x u ,
t t
≥ h − i
whereweused s ,w u γ 1inthefirstinequality. Byrearrangingtheabove inequality,
t t t
h − i≥ −
we obtain that ℓ (x ) ℓ (u) g ,x u g˜ ,w u δ g ,x , which proves (48). Also,
t t t t t t t t t
− ≤h − i ≤ h − i− h i
by the triangle inequality we obtain
g˜ = g +max 0, (1+δ) g ,x s g +(1+δ) g ,x s ,
t 2 t t t t 2 t 2 t t t 2
k k k { − h i} k ≤ k k |h i|k k
which proves (50).
Finally, the second inequality in (49) can be shown similarly as in Option I.
Hence, our remaining task is to construct such an approximate separation oracle for the sets in
(43) and (44), which are given as the intersection of one or two convex compact sets and a linear
subspace, each of which is relatively simple. Our key insight is that the approximate separation
oracle is intersection-friendly, i.e., the separation oracle for can be constructed using the
1 2
C ∩C
individual oracles for and , along with projection on the linear subspace span( ). This
1 2 1 2
C C C ∩C
is formalized in the next lemma.
Lemma 4.6. Assume that = , where 0 and = span( ) is a linear
1 2 1 2
C C ∩ C ∩ L ∈ C ∩ C L C
subspace. Suppose we have access to SEP (w;δ), SEP (w;δ) and the orthogonal projection matrix
1 2
C C
P associated with the subspace . Then given inputs w and δ > 0, the oracle SEP (w;δ) can
L L ∈L C
be constructed in the following ways:
(a) let (γ ,s ) = SEP (w;δ) and (γ ,s ) = SEP (w;δ).
1 1 1 2 2 2
C C
(b) Let i = argmax γ and then output (γ,s) = (γ ,P s ).
i 1,2 i i i
∈{ } L
Proof. Without loss of generality, we assume γ γ and thus the output is given by (γ,s) =
1 2
≥
(γ ,P s ). We consider two cases depending on whether γ 1 or not.
1 1 1
L ≤
Inthe firstcase whereγ = γ 1, we have γ γ 1. According to thedefinitionsof SEP (w;δ)
1 ≤ 2 ≤ 1 ≤ C1
and SEP (w;δ), we have w (1+δ) and w (1+δ) . This implies that w (1+δ)( ).
C2 ∈ C1 ∈ C2 ∈ C1 ∩C2
Moreover, since we assume that w , this leads to w (1+δ)( ) = (1+δ) .
1 2
∈ L ∈ C ∩C ∩L C
In the second case where γ = γ > 1, according to the definition of SEP (w;δ) we have w/γ
1 C1 1 ∈
(1+δ) and s ,w x γ 1 for all x . Moreover, by the definition of SEP (w;δ), we
C1 h 1 − i ≥ 1 − ∈ C1 C2
have w/γ (1+δ) . First, we show that w/γ (1+δ) . Since γ γ , we can write w/γ
2 2 2 1 1
∈ C ∈ C ≤
as a convex combination of the origin and w/γ , and thus it follows from the convexity of that
2 2
C
w/γ (1+δ) . Similar to the arguments in the first case, we obtain w/γ = w/γ (1+δ) .
1 2 1
∈ C ∈ C
Next, we will prove that s,w x γ 1. Since , we have x for any x .
1 1
h − i ≥ − C ⊂ C ∈ C ∈ C
Therefore, it implies that s ,w x γ 1= γ 1. Moreover, since w and x , we have
1 1
h − i ≥ − − ∈ L ∈ L
22Subroutine 2 Online Learning Guided Jacobian Approximation Update
1: Input: Initial matrix B 0 , the orthogonal projection matrix P, step size ρ > 0, δ > 0, failure
2:
p Inro itb ia ab liil zi eti :es se{ tq t W}T t= 0−1 ←1, L L1i 1p (s Bc∈ h −itZ z (Lco 1n +st µan )It ),L G1 >
0
←0 a Ln
1
1d Ps ∇tr ℓo 0n (g Bm 0)o an no dto Gn ˜i 0ci ←ty p Ga 0rameter µ ≥0
43 :: fU op rd ta =te 1W ,.1 ..←,Tmax1{√ √ dd do( ,W kW0 − 0 −ρG˜ ρG0 ˜) 0 kF
}
# Projection onto B√d(0)
5: Query (γ t(1),S− ( t1)) ←ExtEvec(Wt+ 2W t⊤ ;δ,q 2t) and (γ t(2),S( t2)) ←MaxSvec(W t;δ,q 2t)
76 :: iL fet
γ
ti= 1ar tg hm enax i ∈{#1,2 C}aγ st( ei), Iand set γ t ←γ t(i) and S t ←PS t(i)
≤
W if µ>0(Option I)
8: Set Bˆ t ←(Wtt
if µ=0(Option II)
and B t ←L 1Bˆ t+(L 1+µ)I
1+δ
9: Set G t
←
L1 1P ∇ℓ t(B t) and G˜ t ←G t
10: else # Case II
Wt
if µ>0(Option I)
11: Set Bˆ t ←( (γ 1t +W δt
)γt
if µ=0(Option II) and B t ←L 1Bˆ t+(L 1+µ)I
12: Set G t
←
L1 1P ∇ℓ t(B t) and G˜ t ←G t+max {0, −γ1 thG t,W t i}S t
13: end if
14: UpdateW t+1
←
max√ √d d( ,W Wt − tρG˜ ρGt ˜)
t F
# Projection onto B√d(0)
15: end for { k − k }
P (w x)= w x. Therefore, we conclude that s,w x = P s ,w x = s ,P (w x) =
1 1
L − − h − i h L − i h L − i
s ,w x γ 1 =γ 1.
1 1
h − i≥ − −
Combining both cases, we observe that the procedure described in Lemma 4.6 indeed satisfies the
definition in Definition 4.1. This completes the proof.
If SEP and SEP can be computed efficiently and projecting onto is easy, Lemma 4.6 shows
C1 C2 L
that constructing SEP from these oracles incurs minimal overhead. This is indeed the case for our
C
setting, as discussed in the following sections.
4.3 Projection-free Jacobian approximation update
Next, wepresentouronlinelearningalgorithm forupdating B . We follow theprojection-free
k k 0
{ } ≥
online learning algorithm outlined in Section 4.2 and apply it to the online learning problem in
Section 4.1. Moreover, recall that denotes the set of indices where the line search subroutine
B
backtracks, and we have ℓ (B) = 0 when k / . Thus, we can simply keep B unchanged for
k k+1
∈ B
these iterations (cf. Line 8 in Algorithm 1). Suppose = k ,k ,...,k , where T N. With
0 1 T 1
B { − } ≤
a slight abuse of notation, in the following, we relabel the indices in as t =0,...,T 1.
B −
The remaining question is how to construct the approximate separation oracle, required by the
projection-free scheme and defined in Definition 4.1, for our specific set . To start, we consider
Z
the nonlinear monotone equation in (1); we will furtherdiscuss the special cases in the next section.
Recall that the feasible set for our online learning problem in Section 4.1 is given in (44) with a
Z
linear subspace . In the following, we assume that the orthogonal projection matrix P associated
L
with the linear subspace is given. Specifically, for a general nonlinear monotone equation, we
L
have = Rd and the matrix P is simply the identity matrix. Moreover, for those special cases
L
discussed in Section 2.2, the linear subspace is specified at the end of Section 4.1 and we will
discuss the corresponding projection matrix in Section 4.4. Since the approximate separation
23oracle in Definition 4.1 requires the feasible set to contain the origin, we translate and rescale B
C
via the transform Bˆ , 1 (B (L +µ)I). We have the following result, which shows that after
L1 − 1
this one-to-one correspondence, Bˆ lies in a set centered at the origin defined below.
C
Lemma 4.7. Recall the set defined in (44) and suppose that the linear subspace satisfies I .
Z L ∈L
Define the set as the following:
C
1
, Bˆ Rd d : I (Bˆ +Bˆ ) I, Bˆ 3, Bˆ . (52)
× ⊤ op
C ∈ − (cid:22) 2 (cid:22) k k ≤ ∈ L
(cid:26) (cid:27)
Let Bˆ , 1 (B (L +µ)I). If B , then Bˆ . Conversely, if Bˆ (1+δ) , then 1(B+B )
L1 − 1 ∈ Z ∈ C ∈ C 2 ⊤ (cid:23)
(µ L δ)I and B 4L +µ+3δL .
1 op 1 1
− k k ≤
Proof. First, we show that B implies Bˆ . By the definition of Bˆ, we have 1(Bˆ +Bˆ ) =
∈ Z ∈ C 2 ⊤
1 (B+B ) L1+µI µ I L1+µI = I. Moreover, we also have 1(Bˆ +Bˆ ) 1 (B+B ) I.
2L1 ⊤ − L1 (cid:23) L1 − L1 − 2 ⊤ (cid:22) 2L1 ⊤ (cid:22)
Combining these two, we obtain that I 1(Bˆ +Bˆ ) I. Additionally, we can bound Bˆ
− (cid:22) 2 ⊤ (cid:22) k kop ≤
1 ( B +(L +µ)) 3since B L andµ L . Finally, notethatBˆ isalinearcombination
L1 k kop 1 ≤ k kop ≤ 1 ≤ 1
of B and I. Since I and B , we also have Bˆ . Hence, we conclude that Bˆ .
∈L ∈ L ∈ L ∈ C
For theotherdirection, assumeBˆ (1+δ) andnotethatB = L Bˆ+(L +µ)I. SinceBˆ (1+δ)
1 1
implies that 1(Bˆ +Bˆ ) (1+δ∈ )I, we haC ve 1(B+B ) L1(Bˆ +Bˆ )+(L +µ)I (µ∈ L δ)C I.
2 ⊤ (cid:23) − 2 ⊤ (cid:23) 2 ⊤ 1 (cid:23) − 1
Finally, Bˆ (1+δ) also implies that Bˆ 3(1+δ), and thus it holds that B L Bˆ +
op op 1 op
∈ C k k ≤ k k ≤ k k
(L +µ) 4L +µ+3δL . This completes the proof.
1 1 1
≤
Hence, we will work with the new set in our online learning algorithm. Moreover, note that
we can write = , where wC e define , Bˆ Rd d : I 1(Bˆ + Bˆ ) I and
C C1 ∩ C2 ∩ L C1 { ∈ × − (cid:22) 2 ⊤ (cid:22) }
, Bˆ Rd d : Bˆ 3 . By Lemma 4.6, it suffices to construct the approximate separation
2 × op
C { ∈ k k ≤ }
oracles for and , respectively.
1 2
C C
Theseparation oracle for is closely related to the problem of computing extreme eigenvalues and
1
C
eigenvectors of a symmetric matrix. Specifically, given an input matrix W Rd d, let λ R
× max
∈ ∈
and v Rd be the largest magnitude eigenvalue and its associated unit eigenvector of the
max
symmetrize∈ d matrix W¯ = 1(W+W ), respectively. We deduce that: (i) If λ 1, then this
2 ⊤ | max | ≤
implies that I 1(W +W ) I, which certifies W . (ii) Otherwise, if λ > 1, then
− (cid:22) 2 ⊤ (cid:22) ∈ C | max |
we set γ = λ and S = sign(λ )v v Sd. We claim that the pair (γ,S) satisfies the
conditionsin|
Dm ea fix
| nition4.1.
Indeedm ,a nx otem ta hx atm⊤ax
γI∈ 1(W+W ) γIandthus W . Moreover,
− (cid:22) 2 ⊤ (cid:22) γ ∈ C
since the matrix S is symmetric, it holds that S,W = S,W¯ = sign(λ )v W¯ v = λ .
h i h i
max m⊤ax max
|
max
|
Similarly, forany Bˆ , wehave S,Bˆ = S, 1(Bˆ+Bˆ ) 1v (Bˆ+Bˆ )v 1. Combining
∈ C h i h 2 ⊤ i ≤ |2 m⊤ax ⊤ max |≤
these two results, we obtain that S,W Bˆ γ 1. Hence, we can build the separation oracle for
h − i≥ −
by computing the extreme eigenvalues and eigenvectors of a given symmetric matrix. However,
1
C
computing the exact values of λ and v can be costly. Therefore, we propose to employ the
max max
randomized Lanczos method [KW92] to compute the extreme eigenvalues and the corresponding
eigenvectors inexactly. This leads to the randomized oracle, ExtEvec, defined below. We defer its
implementation details to Section 5.
Definition 4.2. The oracle ExtEvec(W;δ,q) takes W Rd d, δ > 0, and q (0,1) as input and
×
∈ ∈
returns a scalar γ > 0 and a matrix S Sd. With probability at least 1 q, the returned γ and S
∈ −
satisfy one of the following properties::
24• Case I: γ 1, which implies that (1+δ)I 1(W+W ) (1+δ)I;
≤ − (cid:22) 2 ⊤ (cid:22)
• Case II: γ > 1, which implies that (1 + δ)I 1 (W + W ) (1 + δ)I, S 1 and
− (cid:22) 2γ ⊤ (cid:22) k kF ≤
S,W Bˆ γ 1 for any Bˆ such that I 1(Bˆ +Bˆ ) I.
h − i ≥ − − (cid:22) 2 ⊤ (cid:22)
Note that ExtEvec is an approximate separation oracle for the set in the sense of Definition 4.1
1
C
(with success probability at least 1 q), and it also guarantees that S 1 in Case II.
F
− k k ≤
For the second set , it turns out that it has a close relation to computing the maximum singular
2
C
valueandthecorrespondingsingularvectors oftheinputmatrixW Rd d. Specifically,letσ >
× max
∈
0bethemaximalsingularvalueofWandletv,v Rd betheassociatedleftandrightunitsingular
′
∈
vectors. Since Bˆ = σ , we can similarly deduce that: (i) If σ 3, then this certifies that
op max max
k k ≤
W . (ii) Otherwise, if σ > 3, then we set γ = σmax and S = 1v v Rd d. In this case,
note∈ tC h2 at W 3 and thm uax s W . Moreover, not3 e that S,W3 ′ =⊤ 1∈ v W× v = σmax = γ
k γ kop ≤ γ ∈ C2 h i 3 ⊤ ′ 3
and S,Bˆ = 1v Bˆv 1 Bˆ 1 for any Bˆ , we obtain that S,W Bˆ γ 1. Thus,
h i 3 ⊤ ′ ≤ 3k kop ≤ ∈ C2 h − i ≥ −
this demonstrates that the separation oracle for can be constructed if we can compute inexactly
2
C
the maximal singular value and its associated singular vectors for a given matrix W. Similarly,
this can be efficiently implemented by using a randomized Lanczos algorithm, which underpins the
MaxSvec oracle defined below. Detailed discussions are deferred to Section 5.
Definition 4.3. The oracle MaxSvec(W;δ,q) takes W Rd d, δ > 0, and q (0,1) as input and
×
∈ ∈
returns a scalar γ > 0 and a matrix S Rd d. With probability at least 1 q, the returned γ and
×
∈ −
S satisfy one of the following properties::
• Case I: γ 1, which implies that W 3(1+δ);
op
≤ k k ≤
• Case II: γ > 1, which implies that W/γ 3(1+δ), S 1 and S,W Bˆ γ 1
op F
k k ≤ k k ≤ h − i ≥ −
for any Bˆ Rd d such that Bˆ 3.
× op
∈ k k ≤
Similarly, we remark that MaxSvec is an approximate separation oracle for the set in the sense
2
C
of Definition 4.1 (with success probability at least 1 q), and it also guarantees that S 1 in
F
− k k ≤
Case II.
The oracles in Definitions 4.2 and 4.3 provide the approximate separation oracles for and ,
1 2
C C
respectively. Using these two building blocks, we can construct the approximate separation oracle
for by following the procedure in Lemma 4.6. By instantiating Algorithm 2, we obtain the
C
complete Jacobian approximation update given in Subroutine 2.
4.4 Special cases: minimization, minimax optimization, and sparse nonlinear
equations
Next, we explore the three special cases from Section 2, where F exhibits additional structures.
∇
We then discuss modifications to our online learning algorithm to enforce these structures on the
Jacobian approximation B . Specifically, these additional structures amount to choosing
k k 0
{ } ≥
different linear subspaces in the feasible set defined in (44). Moreover, since they all satisfy the
L
condition that I , by Lemma 4.7, the feasible set in our online learning algorithm is given
∈ L C
by = under the transformation Bˆ , 1 (B (L + µ)I). We have discussed the
C C1 ∩ C2 ∩ L L1 − 1
construction of SEP and SEP in Section 4.3, and according to Lemma 4.6, we only need to
1 2
C C
specify how to project onto the linear subspace .
L
25Subroutine 3 LinearSolver(A,b;ρ)
1: Input: A Rd ×d, b Rd, ρ>0
∈ ∈
v if A is non-symmetric
2: Initialize: s 0 ←0, r 0 ←b, v 0 ←A ⊤r 0, p 0
←(r
00
if A is symmetric
, γ 0 ←v 0⊤p 0, q 0 ←v 0 if A
is symmetric
3: for k =0,... do
4: if r k 2 ρ s k 2 then
k k ≤ k k
5: Return s k
6: end if
7: When A is non-symmetric: When A is symmetric:
8: q k Ap k 8: (q k =Ap k)
←
9: α k ←γ k/ kq k k2 9: α k ←γ k/ kq k k2
10: s k+1 ←s k+α kp k 10: s k+1 ←s k+α kp k
11 1 31 2 :: : γr v kk k ++ + 11 1 ←← ← kr A vk ⊤ k− +r k 1α + kk 21q k CGLS 111 321 ::: γvr kk k ++ + 11 1 ←← ← vr Ak k⊤r +− k 1+ rα 1 kk +q 1k Conjugate Residual
14: β k ←γ k+1/γ k 14: β k ←γ k+1/γ k
15: p k+1 ←v k+1+β kp k 15: p k+1 ←r k+1+β kp k
16: q k+1 =v k+1+β kq k
17: end for
Minimization. Consider the minimization problem in (4). In this case, we have = Sd, the set
L
of symmetric matrices. For any W Rd d, the projection onto can be computed as P (W) =
×
1 W+W . Infact, notethatsinc∈ etheJacobiansaresymmetriL c, thefeasibleset forouL ronline
2 ⊤ Z
learning problem in Section 4.1 can be simplified as in (43). Moreover, the resulting compact set
(cid:0) (cid:1)
in (52) can be simplified as , Bˆ Sd : I Bˆ I . Therefore, the approximate separation
C C ∈ − (cid:22) (cid:22)
oracle for can be directly given bny ExtEvec in Definitioon 4.2 without relying on Lemma 4.6.
C
Minimax optimization. In this case, we have = Bˆ Rd d : JBˆ = Bˆ J , i.e, the set of
× ⊤
L { ∈ }
J-symmetric matrices. For any W Rd d, the projection onto the set of J-symmetric matrices
×
∈
can be easily computed as P (W) = 1(W+JW J).
2 ⊤
L
Sparse nonlinear equations. Inthiscase,wehave = B Rd d :[B] = 0, (i,j) / Ω,i= j .
× ij
L { ∈ ∀ ∈ 6 }
Given aninputmatrixW, theprojection onto correspondsto“zeroingout”theentries ofW that
are not in Ω. Formally, we have P (W) = W˜L , where the matrix W˜ is defined as [W˜ ] = [W]
ij ij
for (i,j) Ω or i= j, and [W˜ ] =L 0 otherwise.
ij
∈
5 Implementation details of the oracles
By now, we have fully described our proposed QNPE method in Algorithm 1, except for the
LinearSolver oracle, which is required in Subroutine 1, and the ExtEvec and MaxSvec oracles, which
are required in Subroutine 2. In this section, we close these gaps and fully discuss the implementa-
tion details of these oracles.
The LinearSolver oracle. We first present an implementation of the LinearSolver oracle in Def-
inition 3.1. At a high level, we propose to run a conjugate gradient-type method to solve the
linear system As = b with the initialization s = 0, and we return the iterate s once it satisfies
0 k
26As b ρ s . ThespecificalgorithmwechoosedependsonwhethertheinputmatrixAissym-
k k
k − k ≤ k k
metric or not. Specifically, when A is non-symmetric, we adopt the CGLS method [HS+52; PS82],
which is analytically equivalent to applying the conjugate gradient method to the normal equation
A As = A b under exact arithmetic, but is more numerically efficient. On the other hand, when
⊤ ⊤
A is symmetric, weusetheconjugate residualmethod[Sti55; Saa03], which isdesigned tominimize
the norm of the residual vector r = b As over the Krylov subspace span b,Ab,...,Ak 1b .
k k −
− { }
For completeness, the full algorithm is shown in Subroutine 3. We also remark that in the conju-
gate residual method, the relation q = Ap holds, but the matrix-vector productis not computed
k k
explicitly. Instead, in Line 16 we compute q directly from v and q , without an additional
k+1 k+1 k
matrix-vector product (noting that v = Ar , q = Ap , and p = r +β p ). Thus,
k+1 k+1 k k k+1 k+1 k k
we observe that LinearSolver requires at most two matrix-vector products per iteration when A is
non-symmetric, and only one when A is symmetric.
The ExtEvec oracle. Next, we discuss implementing the ExtEvec oracle in Definitions 4.2. As we
mentioned in Section 4.3, it is closely related to computing inexactly the extreme eigenvectors and
theextreme eigenvalues of agiven matrix. Thus,we buildthis oracle basedon theclassical Lanczos
method with a random start, where the initial vector is chosen randomly and uniformly from the
unit sphere (see, e.g., [Saa11; YTFUC21].) To begin with, we recall a classical result in [KW92] on
the convergence behavior of the Lanczos method.
Proposition 5.1 ([KW92, Theorem 4.2]). Consider a symmetric matrix W and let λ (W) and
1
λ (W) denote its largest and smallest eigenvalues, respectively. Then after k iterations of the
d
Lanczos method with a random start, we find unit vectors u(1) and u(d) such that
P( Wu(1),u(1) λ (W) ǫ(λ (W) λ (W))) 1.648√de √ǫ(2k 1),
1 1 d − −
h i ≤ − − ≤
P( Wu(d),u(d) λ (W)+ǫ(λ (W) λ (W))) 1.648√de √ǫ(2k 1),
d 1 d − −
h i ≥ − ≤
As a corollary, to ensure that, with probability at least 1 q, Wu(1),u(1) > λ (W) ǫ(λ (W)
1 1
− h i − −
λ (W)) and Wu(d),u(d) < λ (W)+ǫ(λ (W) λ (W)), the number of iterations can be bounded
d n 1 d
h i −
by 1ǫ 1/2log(11d)+ 1 .
⌈4 − q2 2⌉
Now we are ready to describe our procedure detailed in Subroutine 4. Specifically, given the input
matrix W Rd d, we compute the symmetrized matrix W¯ = 1(W+W ) and run the Lanczos
∈ × 2 ⊤
method for N = 1 2(1+ 1)log(11d)+ 1 iterations. This yields approximate unit eigenvectors
⌈4 δ q2 2⌉
u(1) and u(d), whicqh correspond to the largest and the smallest eigenvalues of W¯ , respectively.
Moreover, we define λˆ = W¯ u(1),u(1) and λˆ = W¯ u(d),u(d) as the approximate largest and
1 d
smallest eigenvalues of W¯ ,h respectively.i Then we seh t γ = max i λˆ ,λˆ . To construct the output
1 d
{ }
pair (γ,S) satisfying the conditions in Definition 4.2, we distinguish two cases depending on the
value of γ. In Case I where γ 1, we return γ and set S = 0. Otherwise, in Case II where γ > 1,
≤
we return γ along with the rank-one matrix S given by
u(1)(u(1)) , if λˆ λˆ ;
⊤ 1 d
S = ≥ −
( u(d)(u(d)) ⊤, otherwise.
−
In the following lemma, we prove the correctness of Subroutine 4.
Lemma 5.2. Let γ and S be the output of ExtEvec(W;δ,q) in Subroutine 4. Then with probability
at least 1 q, they satisfy one of the properties given in Definition 4.2.
−
27Subroutine 4 ExtEvec(W;δ,q)
1: Input: W Rd ×d, δ >0, q (0,1)
2: Initialize: ∈ sample v 1 Rd u∈ niformly from the unit sphere, β 1 0, v 0 0
∈ ← ←
3: Set W¯
←
21(W+W ⊤) and set the number of iterations N
←
1
4
2(1+ 1 δ)log(1 q1 2d)+ 1
2⌉
4: for k =1,...,N do l q
5: Set w k W¯ v k β kv k 1
6: Set α k ← w k,v− k and−w k w k α kv k
←h i ← −
7: Set β k+1 w k and v k+1 w k/β k+1
←k k ←
8: end for
Lanczos method
9: Form a tridiagonal matrix T tridiag(β 2:N,α 1:N,β 2:N)
←
10: # Use the tridiagonal structure to compute eigenvectors of T
11: Compute (λˆ 1,z(1)) MaxEvec(T) and (λˆ d,z(d)) MinEvec(T)
← ←
12: Set u(1)
←
N k=1z k(1)v k and u(d)
←
N k=1z k(d)v k
13: Set γ ←ma Px {λˆ 1, −λˆ d } P
14: if γ 1 then
≤
15: Return γ and S=0 # Case I: γ 1, which implies W op 1+δ
≤ k k ≤
16: else if λˆ 1 λˆ d then
≥−
17: Return γ and S=u(1)(u(1)) ⊤ # Case II: γ >1 and S defines a separating hyperplane
18: else
19: Return γ and S= u(d)(u(d)) ⊤ # Case II: γ >1 and S defines a separating hyperplane
−
20: end if
Proof. Note that in Subroutine 4, we run the Lanczos method for 1ǫ 1/2log 11d + 1 iterations,
4 − q2 2
where ǫ = δ . Thus, by Proposition 5.1, with probability at leaslt 1 q we have m
2(1+δ) −
δ
λˆ , W¯ u(1),u(1) λ (W¯ ) (λ (W¯ ) λ (W¯ )), (53)
1 1 1 d
h i≥ − 2(1+δ) −
δ
λˆ , W¯ u(d),u(d) λ (W¯ )+ (λ (W¯ ) λ (W¯ )). (54)
d d 1 d
h i≤ 2(1+δ) −
Combining (53) and (54), we obtain λ1(W¯ 1) − +λ δd(W¯ )
≤
λˆ 1 −λˆ d, which further implies that λ 1(W¯ )
−
λ (W¯ ) (1+δ)(λˆ λˆ ). By plugging the above inequality back into (53) and (54), we further
d 1 d
≤ −
have
δ δ
λ (W¯ ) λˆ + (λ (W¯ ) λ (W¯ )) λˆ + (λˆ λˆ ), (55)
1 1 1 d 1 1 d
≤ 2(1+δ) − ≤ 2 −
δ δ
λ (W¯ ) λˆ (λ (W¯ ) λ (W¯ )) λˆ (λˆ λˆ ). (56)
d d 1 d d 1 d
≥ − 2(1+δ) − ≥ − 2 −
Recall that γ = max λˆ , λˆ . By (55) and (56), we can further bound the eigenvalues of W¯ by
1 d
{ − }
δ δ
λ (W¯ ) γ+ 2γ = (1+δ)γ and λ (W¯ ) γ 2γ = (1+δ)γ. (57)
1 d
≤ 2 · ≥ − − 2 · −
Hence, we can see that (1+δ)γI W¯ = 1(W +W ) (1+δ)γI. Now we distinguish three
− (cid:22) 2 ⊤ (cid:22)
cases.
(a) If γ 1, then we are in Case I and the ExtEvec oracle outputs γ and S = 0. In this case, we
≤
indeed have (1+δ)I 1(W+W ) (1+δ)I.
− (cid:22) 2 ⊤ (cid:22)
28(b) If γ > 1 and λˆ λˆ , then we are in Case II and the ExtEvec oracle returns γ and
1 d
≥ −
S = u(1)(u(1)) . In this case, since max λ (W¯ ), λ (W¯ ) γ(1+δ), we have (1+δ)I
⊤ 1 d
{ − } ≤ − (cid:22)
1 (W + W ) (1 + δ)I. Also, since u(1) is a unit vector, we have S = u(1) 2 = 1.
2γ ⊤ (cid:22) k kF k k
Finally, forany Bˆ suchthat I 1(Bˆ+Bˆ ) I,wehave S,W Bˆ =u Wu u Bˆu =
− (cid:22) 2 ⊤ (cid:22) h − i ⊤1 1 − ⊤1 1
u W¯ u 1u (Bˆ +Bˆ )u λˆ 1 = γ 1. Thus, γ and S satisfy all the properties in
⊤1 1 − 2 ⊤1 ⊤ 1 ≥ 1 − −
Case II.
(c) If γ > 1 and λˆ λˆ , then we are also in Case II and the ExtEvec oracle returns γ and
d 1
− ≥
S = u(d)(u(d)) . The rest follows similarly to the case above.
⊤
−
This completes the proof.
Finally, notethat theLanczos methodcomputes one matrix-vector productin each iteration. Thus,
we conclude that ExtEvec(W;δ,q) requires 1+ 1log( d ) matrix-vector products in total.
O δ q2
(cid:16)q (cid:17)
The MaxSvec oracle. Finally, we discuss the implementation of MaxSvec in Definition 4.3. As we
noted in Section 4, this oracle has a close connection to the problem of computing the maximal
singular value and its associated left and right singular vectors of a given matrix. Our key observa-
tion is the following fact, which converts a singular value problem for a non-symmetric matrix into
an eigenvalue problem for a symmetric matrix.
Lemma 5.3 ([GV13, Section 8.6.1]). Let W Rd d be an arbitrary matrix. Define the augmented
×
∈
matrix W˜ by
0 W
W˜ = S2d. (58)
W ⊤ 0 ∈
(cid:20) (cid:21)
• Let λ (W˜ ) and λ (W˜ ) denote its largest and smallest eigenvalues, respectively. Then λ (W˜ )
1 2d 1
is the maximal singular value of W and λ (W˜ )= λ (W˜ ).
2d 1
−
• Let v˜ R2d denote the eigenvector associated with λ (W˜ ) and partition it as v˜ = (v,v ),
1 ′
∈
where v,v Rd. Then v and v are the left and right singular vectors associated with the
′ ′
∈
maximal singular value of W, respectively.
In light of Lemma 5.3, to construct the MaxSvecoracle, we can similarly apply the Lanczos method
with a random start to compute inexactly the extreme eigenvectors and the extreme eigenvalues
of the augmented matrix in (58). The procedure is detailed in Subroutine 5, and in the following
lemma we prove its correctness.
Lemma 5.4. Let γ and S be the output of MaxSvec(W;δ,q) in Subroutine 5. Then with probability
at least 1 q, they satisfy the properties in Definition 4.3.
−
Proof. Note that in Subroutine 5, we run the Lanczos method for 1ǫ 1/2log 22d + 1 iterations,
4 − q2 2
where ǫ = δ . Thus, by Proposition 5.1, with probability at lleast 1 q we canmfind a unit
2(1+δ) −
vector v˜ R2d such that λ˜ , W˜ v˜,v˜ λ (W˜ ) δ (λ (W˜ ) λ (W˜ )), Moreover, notice that
∈ 1 h i ≥ 1 −2(1+δ) 1 − 2d
λ (W˜ ) = λ (W˜ ) by Lemma 5.3. Hence, the above further implies that λ˜ 1 λ (W˜ ), which
2d − 1 1 ≥ 1+δ 1
is equivalent to λ (W˜ ) (1+δ)λ˜. Moreover, note that λ (W˜ ) = W and γ = λ˜ /3. Hence,
1 1 op 1
≤ k k
the above is further equivalent to W 3(1+δ)γ. Now we distinguish two cases.
op
k k ≤
29Subroutine 5 MaxSvec(W;δ,q)
1: Input: W Rd ×d, δ >0, q (0,1)
∈ ∈
0 W
2: Set W˜ S2d and the number of iterations N 1 2(1+ 1)log22d + 1
← W ⊤ 0 ∈ ← 4 δ q2 2
3: Computea(cid:20) pproximat(cid:21) elargesteigenvalueλ˜ 1 anditseigenvectorv˜l qR2d,byrunningtheLm anczosmethod
on W˜ for N iterations # See Subroutine 4 ∈
4: Partition v˜ as v˜=[v,v ′] where v,v ′ Rd
5: Set γ λ˜ 1/3 ∈
←
6: if γ 1 then
≤
7: Return γ and S=0 # Case I: γ 1, which implies W op 3(1+δ)
≤ k k ≤
8: else
9: Return γ and S= 2 3v ′v ⊤ ∈Rd ×d # Case II: γ >1 and S defines a separating hyperplane
10: end if
(a) If γ 1, then we are in Case I and the MaxSvec oracle outputs γ and S = 0. In this case,
≤
we indeed have W 3(1+δ)γ 3(1+δ).
op
k k ≤ ≤
(b) If γ > 1, then we are in Case II and the MaxSvec oracle returns γ and S = 2v v . In
3 ′ ⊤
this case, since W 3(1 +δ)γ, we have W/γ 3(1+δ). Also, since v˜ is a unit
op op
k k ≤ k k ≤
vector, we have S = 2 v v 1( v 2 + v 2) = 1 v˜ 2 = 1. Finally, it holds that
k kF 3k kk ′ k ≤ 3 k k k ′ k 3k k 3
S,W = 2v Wv = 1v˜ W˜ v˜ = λ˜ 1 = γ. Moreover, for any Bˆ such that Bˆ 3, we
h i 3 ⊤ ′ 3 ⊤ 3 k kop ≤
have S,Bˆ = 2v Bˆv 2 v B v 2 v v v 2 + v 2 = 1. Hence, we obtain
h i 3 ⊤ ′ ≤ 3k kk kop k ′ k ≤ k ′ kk k ≤ k k k ′ k
S,W Bˆ γ 1. This shows that γ and S satisfy all the properties in Case II.
h − i ≥ −
This completes the proof.
Finally, similar to the ExtEvec oracle, we note that MaxSvec(W;δ,q) requires 1+ 1log( d )
O δ q2
matrix-vector products in total. (cid:16)q (cid:17)
6 Convergence rates and complexity analysis of QNPE
So far, we described our proposed QNPE method in Algorithm 1 and discussed how the step
size η is selected by Subroutine 1 and the Jacobian approximation B is updated according to
k k
Subroutine 2. Moreover, we discussed the required LinearSolver, ExtEvec and MaxSvec oracles in
Subroutines 3, 4, and 5, respectively. Next, we will establish the convergence rate and characterize
the computational cost of QNPE for the strongly monotone setting (Section 6.1) and the monotone
setting (Section 6.2).
6.1 Strongly monotone setting
6.1.1 Convergence rate analysis
In this section, we present our main convergence result when F is strongly monotone.
Theorem 6.1. Suppose Assumptions 1.a, 2.2 and 2.3 hold. Let z and zˆ be the iterates
k k
{ } { }
generated by Algorithm 1 using the line search scheme in Subroutine 1, where α ,α (0, 1),
1 2 ∈ 2
30β (0,1), and σ α2β . In addition, the Jacobian approximation matrices are updated in
∈ 0 ≥ 7.5L1
Subroutine 2 with Option I, where ρ = 1 , δ = µ , and q = p for t 1. With
121 t 2L1 t 2.5(t+1)log2(t+1) ≥
probability at least 1 p, the following holds:
−
(a) (Linear convergence) For any k
≥
0, we have kz kzk+ k−1 − zz ∗ k∗ 2k2
≤
1+ 4 1α 52 Lβ 1µ −1 .
(cid:16) (cid:17)
(b) (Superlinear convergence) We have lim k kz zk+1 − zz ∗∗ 2k2 = 0. Furthermore, define the absolute
constant C and the quantity M by →∞ k k− k
(1+α )2
1
C = , (59)
2(1 α )2β2(1 (α +α ))
1 1 2
− −
121 B 2f(z ) 2 15L L2 z z 2
M = k 0 −∇ ∗ kF + C +2+ 1 2k 0 − ∗ k . (60)
L2 2α βµ L2
1 (cid:18) 2 (cid:19) 1
Then for any k 0, we have
≥
k
z z 2 (1 β)α βµ 2 k −
k ∗ 2
k − k 1+ − min k, .
z z 2 ≤ L 15 M
0 ∗ 1 ( r )!
k − k
Theorem 6.1 presents two global convergence rates for QNPE. Specifically, Part (a) shows that
it converges linearly and matches the rate of EG, leading to a complexity bound of (L1 log 1).
O µ ǫ
According to the lower bound results [NY83; ZHZ22], this complexity bound is optimal up to
constants in the regime where the number of iterations k is (d). Additionally, Part (b) shows
O
an explicit superlinear rate of (1+ µ k )k, where M = ( 1 B F(z ) 2 + L2 2 z z 2).
L1 M O L2 1k 0 −∇ ∗ kF L1µk 0 − ∗ k
This rate outperforms the linear rateqin Part (a) when k M. By Lemma 2.1, it holds that
≥
B F(z ) 2 L2d, and thus in the worst case M = (d). Therefore, after at most (d)
k 0 − ∇ ∗ k ≤ 1 O O
iterations, our convergence rate is provably faster than the linear rate by EG. To the best of our
knowledge, our result is the first to show any acceleration of a quasi-Newton method compared
with EG in the regime where k = Ω(d).
Now we move to the proof of Theorem 6.1. First of all, by using a simple union bound, we
can show that the ExtEvec and MaxSvec oracles used in Subroutine 2 are successful in all rounds
with probability at least 1 p. Specifically, since both oracles have a failure probability of qt =
− 2
p in the t-th round, we can use the union bound to upper bound the total failure
5(t+1)log2(t+1)
probability by
T −1 p ∞ 1 p 1 + ∞ 1
q + dt p.
t ≤ 2.5 tlog2t ≤ 2.5 2log22 tlog2t ≤
t=1 t=2 (cid:18) Z2 (cid:19)
X X
Thus, throughout the proof, we assume every call of ExtEvec and MaxSvec is successful.
Proof of Theorem 6.1(a). We first prove the linear convergence rate in (a). To begin with, note
that Subroutine2 with Option I guarantees that Bˆ (1+ µ ) by Lemma4.5. As a corollary of
k ∈ 2L1 C
Lemma 4.7, we have the following result showing the boundedness of the Jacobian approximation
matrix B .
k
Corollary 6.2. Let δ = µ for all t 0. Then for any k 0, we have 1(B +B ) µI and
t 2L1 ≥ ≥ 2 k ⊤k (cid:23) 2
B 6.5L .
k op 1
k k ≤
31Combining this with Lemma 4.3, we obtain the following universal lower bound on the step size η .
k
Lemma 6.3. In Subroutine 1, choose σ α2β . For any k 0, we have η α2β .
0 ≥ 7.5L1 ≥ k ≥ 7.5L1
Proof. Recall that = k : η < σ . We first establish that η α2β for k . To see this,
B { k k } k ≥ 7.5L1 ∈ B
suppose k and recall from Lemma 4.3 that
∈ B
α β z˜ z
2 k k
η > k − k . (61)
k
F(z˜ ) F(z ) B (z˜ z )
k k k k k
k − − − k
Moreover, by using the triangle inequality, we have F(z˜ ) F(z ) B (z˜ z ) F(z˜ )
k k k k k k
k − − − k ≤ k −
F(z ) + B (z˜ z ) . By using Assumption 2.2, we have F(z˜ ) F(z ) L z˜ z .
k k k k k k 1 k k
k k − k k − k ≤ k − k
Moreover, since B 6.5L by Corollary 6.2, we also get B (z˜ z ) 6.5 z˜ z . Thus,
k op 1 k k k k k
k k ≤ k − k ≤ k − k
this implies that F(z˜ ) F(z ) B (z˜ z ) 7.5 z˜ z , which leads to η > α2β from
k k − k − k k − k k ≤ k k − k k k 7.5L1
(61).
Now we can prove that η α2β for all k 0 by induction. To show that this holds for k = 0,
k ≥ 7.5L1 ≥
we distinguish two cases. If 0 / , then we have η = σ > α2β by our choice of σ . Otherwise,
∈ B 0 0 7.5L1 0
if 0 , then it directly follows from our result in the previous paragraph. Moreover, assume
∈ B
that η α2β where l 1. Similarly, we again distinguish two cases: if l / , then we have
l −1 ≥ 7.5L1 ≥ ∈ B
η = σ = ηl−1 > α2 > α2β ; otherwise, if l , it follows from the result above that η α2β .
l l β 7.5L1 7.5L1 ∈ B l ≥ 7.5L1
This completes the induction.
In light of Lemma 6.3, it follows from Proposition 4.2(a) that z z 2 z z 2(1 +
k+1 ∗ k ∗
k − k ≤ k − k
1
2η µ) 1 z z 2 1+ 4α2βµ − . This proves the linear convergence result in (a).
k − ≤ k k − ∗ k 15L1
(cid:16) (cid:17)
Next, we prove the superlinear convergence rate in (b). Our starting point is the observations in
(36) and (38), which demonstrate that it is sufficient to prove an upper bound on the cumulative
loss kN =−01ℓ k(B k). Moreover, sinceℓ k(B k) = 0whenk ∈/
B
by thedefinitionin (42), itis equivalent
to bounding the sum ℓ (B ). Recall that we relabel the indices in by t = 0,...,T 1 with
P k k k B −
T N. ∈B
≤ P
Proof of Theorem 6.1(b). We consider the following three steps.
Step 1: Using regret analysis, we bound the cumulative loss T t=−01ℓ t(B t) incurred by our online
learning algorithm in Subroutine 2. To begin with, we present the following lemma showing a self-
P
bounding property of the loss function ℓ . It is similar to the standard inequality 1 g(x) 2
t 2L1k∇ k ≤
g(x) g for a L -smooth function g, where g denotes the minimum of g. This will be the key to
∗ 1 ∗
−
proving a constant upper bound on the cumulative loss incurred by Subroutine 2.
Lemma 6.4. Recall the loss function ℓ defined in (42). For k , we have
k
∈ B
T
2(u Bs )s
ℓ (B) = k − k k . (62)
∇ k − s 2
k
k k
Moreover, for any B Rd d, it holds that
×
∈
ℓ (B) ℓ (B) = 2 ℓ (B), (63)
k F k k
k∇ k ≤ k∇ k∗
p
where and denote the Frobenius norm and the nuclear norm, respectively.
F
k·k k·k∗
32Proof. Theexpressionin(62) followsfromthedirectcalculation. Thefirstinequality in(63)follows
from the fact that A A for any matrix A Rd d. For the equality, note that
F ×
k k ≤ k k∗ ∈
2 2 u Bs
ℓ (B) = u Bs s = k k − k k = 2 ℓ (B),
k∇ k k∗ s
k
2k k − k kk k k s
k
k
k k k k
p
where we used the fact that the rank-one matrix ab has only one nonzero singular value a b .
⊤
k kk k
Inparticular, byexploiting theself-boundingpropertyofthelossfunctionℓ , weprove a“small-loss
t
bound” [SST10] in the following lemma, where the cumulative loss of the learner is bounded by
that of a fixed action in the competitor set.
Lemma 6.5. In Subroutine 2, choose Option I with ρ = 1 and δ = µ . For any H , we
121 2L1 ∈ Z
have T t=−01ℓ t(B t)
≤
121 kB
0
−H k2
F
+2 T t=−01ℓ t(H).
P P
Proof. Recall that is contained in the linear subspace (see (44)) and P denotes the orthogonal
projection matrix aZ ssociated with . By letting x = Bˆ L , u = Hˆ , 1 (H (L +µ)I), g = G ,
L t t L1 − 1 t t
1 P ℓ (B ), g˜ = G˜ , w = W in Lemma 4.5, we obtain:
L1 ∇ t t t t t t
(i) Bˆ (1+δ) , which means Bˆ 3(1+δ) 4.5 since δ = µ 1.
t ∈ C k t kop ≤ ≤ 2L1 ≤ 2
(ii) It holds that
1 1 ρ
G ,Bˆ Hˆ W Hˆ 2 W Hˆ 2 + G˜ 2, (64)
h t t − i ≤ 2ρk t − kF − 2ρk t+1 − kF 2k t kF
G˜ G + G ,Bˆ S . (65)
t F t F t t t F
k k ≤ k k |h i|k k
First, note that S 1 by Definition 4.2 and G ,Bˆ G Bˆ 4.5 G . Together
t F t t t t op t
k k ≤ |h i| ≤ k k∗k k ≤ k k∗
with (65), we get
5.5 11
G˜ G +4.5 G 5.5 G ℓ (B ) ℓ (B ), (66)
t F t F t t t t t t
k k ≤ k k k k∗ ≤ k k∗ ≤ L 1k∇ k∗ ≤ L
1
p
where we used G = 1 P ℓ (B ) and Lemma 6.4 in the last inequality. Furthermore, since ℓ is
t L1 ∇ t t t
convex and B ,H , we have
t
∈ L
ℓ (B ) ℓ (H) ℓ (B ),B H = ℓ (B ),P(B H) =L2 G ,Bˆ Hˆ , (67)
t t − t ≤h∇ t t t − i h∇ t t t − i 1h t t − i
whereweusedG = 1 P ℓ (B ),Bˆ , 1 (B (L +µ)I),andHˆ , 1 (H (L +µ)I). Therefore,
t L1 ∇ t t t L1 t − 1 L1 − 1
by (64) and (66) we get
L2 L2 ρL2
ℓ (B ) ℓ (H) 1 W Hˆ 2 1 W Hˆ 2 + 1 G˜ 2
t t − t ≤ 2ρk t − kF − 2ρk t+1 − kF 2 k t kF
L2 L2
1 W Hˆ 2 1 W Hˆ 2 +60.5ρℓ (B ).
≤ 2ρk t − kF − 2ρk t+1 − kF t t
Since ρ = 1 , by rearranging and simplifying terms in the above inequality, we obtain ℓ (B )
121 t t ≤
2ℓ (H)+121L2 W Hˆ 2 121L2 W Hˆ 2. By summing the above inequality from t = 0
t 1k t − kF − 1k t+1 − kF
to T 1, we further have
−
T 1 T 1 T 1
− ℓ (B ) 121L2 W Hˆ 2 +2 − ℓ (H)= 121 B H 2 +2 − ℓ (H),
t t ≤ 1k 0 − kF t k 0 − kF t
t=0 t=0 t=0
X X X
33where the last equality is due to W , 1 (B (L +µ)I) and Hˆ , 1 (H (L +µ)I). This
0 L1 0 − 1 L1 − 1
completes the proof.
Note that in Lemma 6.5, we have the freedom to choose any competitor H in the set . To further
obtain an explicit bound, a natural choice would be H , F(z ), which leads to ouZ r next step.
∗ ∗
∇
Step 2: We upper bound the cumulative loss T t=−01ℓ t(H ∗). The proof relies crucially on Assump-
tion 2.3 as well as the linear convergence result we proved in (a). To begin with, we prove the
P
following useful lemma.
Lemma 6.6. If Assumption 1.a holds, then kN =−01 kzˆ
k
−z
k
k2
≤ 1
(α11 +α2)kz
0
−z
∗
k2.
−
P
Proof. Recalltheinequality(28)intheproofofProposition4.2. Since 1 z z 2 1+2ηkµ z
2k k+1 − ∗ k ≤ 2 k k+1 −
z 2, we can further derive that
∗
k
1 α α 1 1
− 1 − 2 ( zˆ k z k 2+ zˆ k z k+1 2) z k z ∗ 2 z k+1 z ∗ 2. (68)
2 k − k k − k ≤ 2k − k − 2k − k
Since α +α < 1, we further have 1 α1 α2 zˆ z 2 1 α1 α2( zˆ z 2 + zˆ z 2)
1 2 − 2−
k
k
−
k
k ≤
− 2−
k
k
−
k
k k
k
−
k+1
k ≤
1 z z 2 1 z z 2. By summing the above inequality from k = 0 to k = N 1, we
2k k − ∗ k − 2k k+1 − ∗ k −
obtain that
N 1
1 −α 1 −α 2 − zˆ
k
z
k
2 1 z
0
z
∗
2 1 z
N
z
∗
2 1 z
0
z
∗
2.
2 k − k ≤ 2k − k − 2k − k ≤ 2k − k
k=0
X
Thus, we arrive at Lemma 6.6 by dividing both sides by 1 α1 α2.
− 2−
With the help of Lemma 6.6, we are ready to upper bound T t=−01ℓ t(H ∗).
Lemma6.7. Recall theconstant C definedin (59). Then T tP =−01ℓ t(H ∗)
≤
C +2+ 21 α5 2L β1
µ
L2 2kz
0 −
z 2. (cid:16) (cid:17)
∗ P
k
Proof. Recall the definition of ℓ in (42). By the fundamental theorem of calculus, we can write
t
u = F(z˜) F(z ) = H¯ (z˜ z ), where H¯ = 1 F(z +λs )dλ. Moreover, using the triangle
t t − t t t − t t 0 ∇ t t
inequality, we have
R
1
H¯ H F(z +λs ) F(z ) dλ
t ∗ op t t ∗ op
k − k ≤ k∇ −∇ k
Z0
1
L z λs +z dλ
2 t t ∗
≤ k − k
Z0
1 1
L ( z z +λ s )dλ = L z z + s ,
2 t ∗ t 2 t ∗ t
≤ k − k k k k − k 2k k
Z0
(cid:16) (cid:17)
where we used Assumption 2.3 in the second inequality. Therefore, we have u H s = (H¯
t ∗ t t
k − k k −
H )s H¯ H s L s z z + 1 s . This further implies that
∗ t k≤ k t − ∗ kop k t k ≤ 2 k t k k t − ∗ k 2k t k
(cid:16) (cid:17)
T −1
ℓ t(H ∗)=
T −1 ku
t −
sH 2∗s
t
k2
≤
L2
2T −1
kz t −z ∗ k+
1
2ks t
k
2
t
Xt=0 Xt=0 k k Xt=0(cid:16) (cid:17)
L2 T −1 T −1
2 s 2+2L2 z z 2. (69)
≤ 2 k t k 2 k t − ∗ k
t=0 t=0
X X
34Note that by our notation, we have T t=−01 ks
t
k2 =
k
ks
k
k2 =
k
kz˜
k
−z
k
k2 and T t=−01 kz
t −
z 2 = z z 2. To bound the first sum ∈B z˜ z∈B2, we use the last result in
∗ k k k k − ∗ k P P k k k −Pk k P
Lemma 4.3 a∈nBd the inequality in Lemma 6.6 to get ∈B
P P
z˜ z 2
(1+α 1)2
zˆ z 2
(1+α 1)2 N −1
zˆ z 2
k k − k k ≤ β2(1 α )2 k k − k k ≤ β2(1 α )2 k k − k k
1 1
k − k − k=0 (70)
X∈B X∈B X
(1+α )2 z z 2
≤ (1 α
)2β1 2(1k 0 (−
α
∗ +k
α ))
= 2C kz 0 −z ∗ k2.
1 1 2
− −
To bound the second sum z z 2, we use the linear convergence result in Part (a) of
k k k − ∗ k
Theorem 6.1: ∈B
P
N 1 N 1 k
z z 2 − z z 2 z z 2 − 1+ 4α 2βµ −
k ∗ k ∗ 0 ∗
k − k ≤ k − k ≤ k − k 15L
k k=0 k=0 (cid:18) 1 (cid:19) (71)
X∈B X X
15L
z z 2 1+ 1 .
0 ∗
≤ k − k 4α βµ
(cid:18) 2 (cid:19)
Lemma 6.7 follows immediately from (69), (70), and (71).
Step 3: Combining Lemmas 6.7 and 6.5, we obtain a constant upper bound on the cumulative
loss as T t=−01ℓ t(B t)
≤
121 kB
0
−H
∗
k2
F
+ C +2+ 21 α5 2L β1
µ
L2 2kz
0
−z
∗
k2 = L2 1M, where we used the
definitiPon of M in (60). Together with th(cid:16)e fact that σ
0
≥(cid:17) 7α .52 Lβ 1, it follows from Lemma 4.4 that
k −1 1 7.5L
1
L 1√kM 2L 1max 7.5,√kM
+ { }.
η ≤ (1 β)α β (1 β)α β ≤ (1 β)α β
i 2 2 2
i=0 − − −
X
This leads to the superlinear convergence result in (b) by Proposition 4.2 and the observation in
(36).
6.1.2 Characterizing the computational cost
Inthefollowingtheorem,wecharacterize thenumberofoperatorevaluationsrequiredbyourQNPE
method.
Theorem 6.8 (Operator evaluaiton complexity). After N iterations, QNPE requires at most 3N+
log (7.5σ0L1) total operator evaluations.
1/β α2
Proof. Let l
k
denote the number of line search steps in iteration k. We first note that η
k
= σ kβlk−1
by our line search subroutine, which implies l = log (σ /η )+1. Thus, the total number of line
k 1/β k k
35search steps after N iterations can be bounded by
N 1 N 1 N 1
− − σ k σ 0 − σ k
l = log +1 = N +log + log
k 1/β η 1/β η 1/β η
k=0 k=0 (cid:18) k (cid:19) 0 k=1 k
X X X
N 1
σ 0 − η k 1
= N +log 1/β η + log 1/β βη− (72)
0 k
k=1
X
N 1
σ 0 − η k 1
= 2N −1+log 1/β η + log 1/β η−
0 k
k=1
X
σ
0
= 2N 1+log , (73)
− 1/β η
N 1
−
where we used the fact that σ = η /β for k 1 in (72). Since we have η α2β by
k k −1 ≥ N −1 ≥ 7.5L1
Lemma 6.3, we further have kN =−01l
k
≤
2N −1+log 1/β(7.5 ασ 20L1) from (73). Note that each line
search step consists of one operator evaluation. Additionally, in each iteration of Algorithm 1, we
P
also need to evaluate F(z ). Thus, we conclude that the total number of gradient evaluations is
k
bounded by 3N +log (7.5σ0L1).
1/β α2
Theorem 6.8 shows that the total number of operator evaluations is upper bounded by 3N +
(log(σ L )). Thus, when N is sufficiently large, the average number of operator evaluations per
0 1
O
iteration can be bounded by a constant close to 3.
In the next theorem, we further characterize the total number of matrix-vector products required
in the subroutines.
Theorem 6.9 (Matrix-vector product complexity). Let N denote the minimum number of itera-
ǫ
tions required by Algorithm 1 to find an ǫ-accurate solution according to Theorem 6.1.
(a) For Problems (1) and (5), QNPE requires
O
N ǫL µ1 log L1 kz0 µ− ǫz∗ k2 matrix-vector products
for LinearSolver. Moreover, itrequires (N (cid:16)L1 log(dN ǫ(cid:16)2 )) matrix-v(cid:17) ec(cid:17) tor products for ExtEvec
O ǫ µ p2
and MaxSvec. q
(b) For Problem (4), QNPE requires
O
N ǫ L µ1 log L1 kz0 µ− ǫz∗ k2 matrix-vector products for
LinearSolver. Moreover, it requires com(cid:16) putq ing (N(cid:16) L1 log(dN(cid:17) ǫ2(cid:17) )) matrix-vector products for
O ǫ µ p2
ExtEvec. q
For the nonlinear equation in (1) and the minimax problem in (5), Theorem 6.9(a) demonstrates
that the total number of matrix-vector products can be boundedby ˜(L1N ), ignoring logarithmic
O µ ǫ
factors. Moreover, for the minimization problem in (4), Theorem 6.9(b) shows that the dependence
on the condition number can be further improved from L1 to L1.
µ µ
q
To prove Theorem 6.9, we first present the following proposition regarding the convergence of
Subroutine 3.
Proposition 6.10. Let s and r be generated by Subroutine 3. Then the following
k k 0 k k 0
{ } ≥ { } ≥
holds:
36(a) If A is non-symmetric, define κ(A) = λ λm ma inx (( AA ⊤⊤ AA )) and we have kr k
k≤
2 κκ (( AA )) +− 11 k kr 0 k. If
A is symmetric, define κ(A) = λ λm ma inx (( AA ))q and we have kr k
k ≤
2 √√κ κ( (A A) )− +1
1
(cid:16)k kr 0 k. (cid:17)
(cid:18) (cid:19)
(b) We have s > s for all k 1.
k k 1
k k k − k ≥
Proof. When A is non-symmetric, Subroutine 3 employs the CGLS method. See [Bjo¨96, Chapter
7.4.2] for the proof of (a) and [Ste83, Theorem 2.1] for the proof of (b). When A is symmetric,
Subroutine 3 employs the conjugate residual method. See [Gre97, Section 3.1] for the proof of Part
(a) and [Fon11, Theorem 2.1.6] for the proof of Part (b).
As a corollary of Proposition 6.10, we can upper boundthe total number of matrix-vector products
when Subroutine 3 returns for given inputs A, b, and ρ.
Lemma 6.11. Given the inputs A Rd d, b Rd, and ρ > 0. When Subroutine 3 returns, the
×
∈ ∈
total number of matrix-vector product evaluations can be bounded by 2κ(A)log
2λmax(A⊤A)
if
ρ√λmin(A⊤A)
(cid:18) (cid:19)
A is non-symmetric, and κ(A)log
2λmax(A)
if A is symmetric.
ρ
p (cid:16) (cid:17)
Proof. We first consider the case where A is non-symmetric. From the update rule of Subroutine3,
A⊤b 2
we can compute that s 1 = kAA⊤bk 2A ⊤b, which implies that
k k
A b 3 A b λ (AA ) b λ (AA ) r
s = k ⊤ k k ⊤ k min ⊤ k k = min ⊤ k 0 k.
k 1 k AA b 2 ≥ λ (AA ) ≥ λ (AA ) λ (AA )
k ⊤ k max ⊤ p max ⊤ p max ⊤
Since ks k
k
is strictly increasing (cf. Proposition 6.10(b)), we have ks k
k ≥
ks 1
k
= √λ λm min a( xA (AA A⊤ ⊤) k )r0 k
for any k
≥
1. Thus, if kr k
k ≤
ρ√λ λm min a( xA (AA A⊤ ⊤) k )r0 k, we obtain that kr k
k≤
ρ√λ λm mi an x( (A AA A⊤ ⊤) )kr0 k
≤
ρ ks k k2.
Moreover, using Proposition 6.10, we obtain that r ρ s if
k 2 k 2
k k ≤ k k
log
2λmax(A⊤A)
2
κ(A) −1 k ρ λ min(A ⊤A)
k
(cid:18)ρ√λmin(A⊤A)
(cid:19).
κ(A)+1 ≤ λ (A A) ⇔ ≥ κ(A)+1
(cid:18) (cid:19) pmax ⊤ log κ(A) 1
−
(cid:16) (cid:17)
Sincelog(x) (x 1)/x forallx > 0, wehave log κ(A)+1 2 1 . Finally, notethatfor
≥ − κ(A) 1 ≥ κ(A)+1 ≥ κ(A)
−
a non-symmetric matrix A, Subroutine 3 requires(cid:16)two ma(cid:17)trix-vector products per iteration. This
completes the proof in the non-symmetric case.
Next, we consider the case where A is symmetric. From the update rule of Subroutine 3, we can
compute that s =
b⊤Abb,
which implies
1 Ab 2
k k2
A1/2b 2 b r
0
s = b k k k k = k k .
1
k k k k· (A1/2b) A(A1/2b) ≥ λ (A) λ (A)
⊤ max max
Since ks k
k
is strictly increasing (cf. Proposition 6.10(b)), we have ks k
k ≥
ks 1
k
= λmk ar x0 (kA) for any
k ≥ 1. Thus, if kr k k ≤ ρ λmkr a0 xk (A2 ), we obtain that kr k k2 ≤ ρ λmkr ax0 k (A2 ) ≤ ρ ks k k2. Combining this with
37Proposition 6.10, we similarly obtain r ρ s if
k 2 k 2
k k ≤ k k
k
κ(A) 1 ρ 2λ (A)
max
2 − k κ(A)log
pκ(A)+1! ≤ λ max(A) ⇐ ≥
(cid:18)
ρ
(cid:19)
p
p
This completes the proof.
Using Lemma 6.11, we are ready to prove Theorem 6.9.
Proof of Theorem 6.9. First, we consider Problems (1) and (5). In this case, the Jacobian approx-
imation matrices B are non-symmetric. Consider the k-th iteration. Note that in each call
k
{ }
of LinearSolver in Subroutine 1, the inputs are given by A = I + η B and ρ = α √1+η µ,
+ k 1 +
with η σ . Moreover, recall from Corollary 6.2 that 1(B +B ) µI and B 6.5L .
+ ≤ k 2 k ⊤k (cid:23) 2 k k kop ≤ 1
Thus, we have λ (A A) = A 1+η B 1+6.5η L . Moreover, we also have
max ⊤ op + k op + 1
k k ≤ k k ≤
1(A+A ) (1+η µ)I, which further implies λ (A A) (1+η µ)2 by Lemma B.1 in Ap-
2 ⊤ (cid:23) p +2 min ⊤ ≥ +2
pendix B. Hence, we can conclude that κ(A) = λmax(A⊤A) 1+6.5η+L1 13L1. Furthermore, by
λmin(A⊤A) ≤ 1+η+µ 2 ≤ µ
Lemma 6.11, the number of matrix-vector prodquct evaluations in each call of LinearSolver can be
bounded by
2λ (A A)
MV 2κ(A)log max ⊤
k
≤ ρ λ min(A ⊤A)!
13L
1
2p (1+6.5η +L 1)2
log
≤ µ α √1+η µ(1+η µ/2)
(cid:18) 1 + + (cid:19)
13L 2 (1+6.5η L )2 13L (1+η µ/2)2
1 + 1 1 +
log + log
≤ µ α (1+η µ/2)2 µ √1+η µ(1+η µ/2)
(cid:18) 1 + (cid:19) (cid:18) + + (cid:19)
26L 2 13L 13L η µ
1 1 1 +
log + log 1+ .
≤ µ α µ 2µ 2
(cid:18)r 1 (cid:19)
(cid:16) (cid:17)
Moreover, since we have η σ = η /β for k 1, we further get
+ k k 1
≤ − ≥
26L 2 13L 13L η µ
MV
k
1 log 1 + 1 log 1+ k −1
≤ µ α µ 2µ 2β
(cid:18)r 1 (cid:19) (cid:18) (cid:19)
26L 2 13L 13L η µ
1 1 1 k 1
log + log 1+ − ,
≤ µ α β µ 2µ 2
(cid:18)r 1 (cid:19)
(cid:16) (cid:17)
where the last inequality is due to the fact that log 1+ ηk−1µ = log β + ηk−1µ + log 1
2β 2 β ≤
log 1+ ηk−1µ + log 1. Let l denote the number of (cid:16)line search(cid:17)steps in iteration k, and then
2 β k (cid:0) (cid:1)
we c (cid:0)an bound (cid:1)the total number of matrix-vector products by N k=ǫ −01l
k
·MV k. Moreover, from the
proof of Theorem 6.8, we know that l = log (σk)+1. For k = 0, we have l log σ0 +
k 1/β ηk P 0 ≤ 1/β η0
1 log σ0L1 +1, and MV 26L1 log 2 13L1 + 13L1 log 1+ σ0µ , where we use(cid:16) d t(cid:17) hat
≤ 1/β α2β 0 ≤ µ α1 µ 2µ 2
η 0 > 7α .52 Lβ
1
(cid:16)by Le(cid:17)mma 6.3. Furthermore, we fi(cid:16)rqst show t(cid:17)hat (cid:0) (cid:1)
Nǫ −2 z
0
z
∗
2
(1+2η µ) k − k . (74)
k
≤ ǫ
k=0
Y
38Toseethis,notethatbyProposition4.2, wehave kz
N
−z
∗
k2
≤
kz
0
−z
∗
k2 kN =−01(1+2η kµ) −1. Then
(74) follows from the fact that N is the minimum number of iterations to achieve z z 2 ǫ.
ǫ N ∗
Thus, the total number of matrix-vector products MV
tol
:= N k=ǫ −11l
k
·MVQ
k
can bek boun− dedk by≤ :
MV
26L
1 log
2 13L
1
Nǫ −1
l +
13L
1
NP ǫ −1
log(1+2η µ) l
tol k k 1 k
≤ µ (cid:18)rα 1β µ
(cid:19) k=1
2µ
k=1
− ·
X X
26L
1
2 13L
1
Nǫ −1
13L
1
Nǫ −1 Nǫ −1
log l + log(1+2η µ) l
k k 1 k
≤ µ (cid:18)rα 1β µ
(cid:19) k=1
2µ
k=1
− ·
k=1
X X X
26L 13√2L z z 2 7.5σ L
1 1 0 ∗ 0 1
log k − k 2N +log ,
≤ µ √α 1βµǫ !·
(cid:18)
ǫ 1/β α
2 (cid:19)
where we used (74) and Theorem 6.8 in the last inequality. Hence, we conclude that LinearSolver
requires
O
N ǫL µ1 log L1 kz0 µ− ǫz∗ k2 matrix-vector products in total.
(cid:16) (cid:16) (cid:17)(cid:17)
Next, we consider Problem (4). Note that in this case, the Jacobian approximation matrices B
k
{ }
are symmetric and positive definite. We follow similar arguments as in the non-symmetric case.
Consider the k-th iteration and in each call of LinearSolver, the inputs are given by A = I+η B
+ k
and ρ = α √1+η µ, with η σ . Therefore, we can bound
λmax(A)
=
1+η+λmax(B k) λmax(B k)
.
1 + + ≤ k λmin(A) 1+η+λmin(B k) ≤ λmin(B k)
Moreover, Corollary 6.2 shows that µI B 6.5L I, and this further implies that κ(A) 13L1.
2 (cid:22) k (cid:22) 1 ≤ µ
Hence, byLemma6.11, thenumberof matrix-vector productevaluations ineach call of LinearSolver
can be bounded by
2λ (A)
MV κ(A)log max
k
≤ ρ
(cid:18) (cid:19)
p
13L 2(1+6.5η L )
1 + 1
log
≤ s µ
(cid:18)
α 1√1+η +µ
(cid:19)
13L 2(1+6.5η L ) 1 13L
1 + 1 1
log + log(1+η µ).
+
≤ s µ
(cid:18)
α 1(1+η +µ)
(cid:19)
2s µ
Moreover, since 1+6.5η+L1 6.5L1 and η σ = ηk−1 for k 1, we further get
1+η+µ ≤ µ + ≤ k β ≥
13L 13L 1 13L η µ
MV
k
1 log 1 + 1 log 1+ k −1
≤ s µ
(cid:18)
α 1µ
(cid:19)
2s µ
(cid:18)
β
(cid:19)
13L 13L 1 13L
1 1 1
log + log(1+η µ).
k 1
≤ s µ (cid:18)α 1√βµ
(cid:19)
2s µ −
Recall that l = log (σk) + 1 denote the number of line search steps in iteration k, and the
k 1/β ηk
total number of matrix-vector products can be bounded by N k=ǫ −01l
k
·MV k. Similar to the non-
symmetric setting, for k = 0, we have l log σ0L1 + 1 and MV 13L1 log 13L1 +
0 ≤ 1/β α2β P 0 ≤ µ α1µ
1
2
13 µL1 log(1+σ 0µ). Furthermore,thetotalnumber(cid:16) ofma(cid:17) trix-vectorproductsq MV
tol
:= (cid:16)N k=ǫ −11(cid:17) l
k ·
q
P
39MV can be bounded by:
k
MV
13L
1 log
13L
1
Nǫ −1
l +
13L
1
Nǫ −1
log(1+2η µ) l
tol k k 1 k
≤s µ (cid:18)α 1√βµ
(cid:19) k=1
s 4µ
k=1
− ·
X X
13L
1
13L
1
Nǫ −1
13L
1
Nǫ −1 Nǫ −1
log l + log(1+2η µ) l
k k 1 k
≤s µ (cid:18)α 1√βµ
(cid:19) k=1
s 4µ
k=1
− ·
k=1
X X X
13L 13L z z 2 7.5σ L
1 1 0 ∗ 0 1
log k − k 2N +log ,
≤s µ
(cid:18)
α 1√βµǫ (cid:19)·
(cid:18)
ǫ 1/β α
2 (cid:19)
where we used (74) and Theorem 6.8 in the last inequality. Hence, we obtain that the LinearSolver
oracle requires
O
N ǫ L µ1 log L1 kz0 µ− ǫz∗ k2 matrix-vector products in total.
(cid:16) q (cid:16) (cid:17)(cid:17)
Finally, we bound the number of matrix-vector products for ExtEvec and MaxSvec in all cases.
Since the parameters are selected as δ = µ and q = p p , it fol-
2L1 t 2.5(t+1)log2(t+1) ≤ 2.5Nǫlog2(Nǫ)
lows from Lemmas 5.2 and 5.4 that the total number of matrix-vector products is bounded by
T 1+δt log d = N L1 log(dN ǫ2 ) . The proof is complete.
O t=1 δt q t2 O ǫ µ p2
(cid:16) q (cid:17) (cid:16) q (cid:17)
P
6.2 Monotone setting
6.2.1 Convergence rate analysis
Next, we present our convergence result when F is only monotone.
Theorem 6.12. Suppose Assumptions 1.b, 2.2 and 2.3 hold. Let z and zˆ be the iterates
k k
{ } { }
generated by Algorithm 1 using the line search scheme in Subroutine 1, where α ,α (0, 1), β
1 2 ∈ 2 ∈
(0,1), and σ α2β. In addition, the Jacobian approximation matrices are updated in Subroutine 2
0 ≥ 5L1
with Option II, where ρ= 1 , δ = 1 , and q = p for t 1. With probability
121 t 2(t+1)1/4 t 2.5(t+1)log2(t+1) ≥
at least 1 p, the following holds:
−
(a) For any k 0, we have z z z z . Moreover, define the averaged iterate z¯ by
k+1 ∗ k ∗ k
z¯
k
= P Pk i=
k
i− =−01 0≥ 1η ηiz iˆi. Then fork any − compk ac≤ t k set D− ⊂k Rd, we have Gap(z¯ k; D)
≤
5L1max 2z α∈ 2D βkkz0 −z k2 .
(b) Moreover, recall the definition of C in (59) and define Q and Q as
1 2
B F(z ) L z z
Q = 9√2k 0 −∇ 0 kF +√2C 2 k 0 − ∗ k, (75)
1
L L
1 1
648√2dL z z
Q = 2 k 0 − ∗ k +6√2. (76)
2
s √1 α 1 α 2L 1
− −
Then for any k 0, we have
≥
L max z z 2 5 Q Q
1 z 0 1 2
Gap(z¯ k; D)
≤ 2(1
∈D βk
)α
β− k
k2
+
k1.5
+
k1.25
.
− 2 (cid:18) (cid:19)
Similar to the strongly monotone setting, Theorem 6.12 demonstrates two global convergence rates
for QNPE. Part (a) shows that QNPE converges at least at a rate of 1 , which matches
O k
(cid:0) (cid:1)
40the rate of EG and is known to be optimal in the regime when the number of iterations k is
(d) [OX21]. Moreover, Part (b) shows that QNPE achieves a rate of 1 + Q1 + Q2 , where
O O k2 k1.5 k1.25
Q 1 =
O
kB 0 −∇F(z0) LkF 1+L2 kz0 −z∗ k and Q 2 =
O
dL2 kz L0 1−z∗ k . Since(cid:16) Lemma 2.1 imp(cid:17) lies that
B (cid:16) F(z ) L √d, in the(cid:17) worst case, Q (cid:18) =q (√d). He(cid:19) nce, the leading term is ( √d ),
k 0 −∇ 0 kF ≤ 1 1 O O k1.25
which outperforms the rate in Part (a) when k = Ω(d2). To the best of our knowledge, this is the
first result demonstrating a theoretical advantage of quasi-Newton methods over EG for solving
monotone nonlinear equations.
The rest of this section is devoted to the proof of Theorem 6.12. Using the same union bound
argument as in Theorem 6.1, we assumethat every call of ExtEvec and MaxSvecis successful, which
holds with probability at least 1 p.
−
Proof of Theorem 6.12(a). To begin with, note that Subroutine 2 with Option II guarantees that
Bˆ by Lemma 4.5. As a corollary of Lemma 4.7, we have 1(B +B ) 0 and B 4L
k ∈ C 2 k ⊤k (cid:23) k k kop ≤ 1
for all k 0. Thus, following the same arguments as in Lemma 6.3, we can show that η
k
α 5L2β
1
by in≥ duction. This leads to kN =−01η
k
≥
α 5L2β 1N, and we obtain from Proposition 4.2(b) tha≥ t
Gap(z¯ N; D)
≤
max 2z P∈D
N
k=−k 0z 10 η− kz k2
≤
5L P1 2k αz 20 β− Nz k2 . This proves the first convergence rate in (a).
Next, weprovethesecondconvergence ratein(b). SimilartotheproofofTheorem6.1, ourstarting
pointistheobservations in(37)and(38),whichimpliesthatitissufficienttoproveanupperbound
on the cumulative loss kN =−01ℓ k(B k). Again, recall that ℓ k(B k) = 0 when k ∈/
B
by the definition
in (42), and we relabel the indices in by t = 0,...,T 1 with T N.
P B − ≤
Proof of Theorem 6.12(b). We consider the following three steps.
Step 1: First, weboundthecumulative loss T t=−01ℓ t(B t)incurredbyouronlinelearningalgorithm
inSubroutine2. However, differentfromtheproofinTheorem6.1,wewillupperboundthedynamic
P
regret instead ofthestaticregret. Thisis because,inthestronglymonotonesetting, theiterates are
guaranteed to converge at least linearly to the optimal solution z (Theorem 6.1(a)). This results
∗
in less variation in the loss functions {ℓ
t
}T t=−01, and in particular we can show that T t=−01ℓ t(H ∗) is
bounded(Lemma 6.7). In contrast, without linear convergence, we need to consider a time-varying
P
sequence H T 1 to control the cumulative loss. Specifically, we present a “small-loss bound”
{
t }t=−0
similar to Lemma 6.5 in the following lemma.
Lemma 6.13. In Subroutine 2, choose Option II with ρ= 1 and δ = 1 . For any sequence
81 t 2(t+1)1/4
of matrices {H
t
}T t=−01 such that H
t ∈
Z, we have T t=−01ℓ t(B t)
≤
4 T t=−01ℓ t(H t)+162 kB
0
−H
0
k2
F
+
648√dL
1
T t=−02 kH
t+1
−H
t kF
+72L2 1√T.
P P
P
Proof. Recall that is contained in the linear subspace (see (44)) and P denotes the orthogonal
Z L
projection matrix associated with . By letting x = Bˆ , u = Hˆ , 1 (H L I), g = G ,
L t t t L1 t − 1 t t
1 P ℓ (B ), g˜ = G˜ , w = W in Lemma 4.5, we obtain:
L1 ∇ t t t t t t
(i) Bˆ , which means Bˆ 3.
t t op
∈C k k ≤
41(ii) It holds that
W Hˆ 2 W Hˆ 2 ρ
G ,Bˆ Hˆ k t − t kF k t+1 − t kF + G˜ 2 δ G ,Bˆ , (77)
h t t − t i≤ 2ρ − 2ρ 2k t kF− t h t t i
G˜ G +(1+δ ) G ,Bˆ S . (78)
t F t F t t t t F
k k ≤ k k |h i|k k
First, note that S 1 by Definition 4.2 and G ,Bˆ G Bˆ 3 G . By using
t F t t t t op t
δ = 1
1k togk eth≤
er with (78), we get
|h i| ≤ k k∗k k ≤ k k∗
t 2(t+1)1/4 ≤ 2
9
G˜ G +3(1+δ ) G 4.5 G ℓ (B ), (79)
t F t F t t t t t
k k ≤ k k k k∗ ≤ k k∗ ≤ L
1
p
where we used G = 1 P ℓ (B ) and Lemma 6.4 in the last inequality. Furthermore, since ℓ is
t L1 ∇ t t t
convex and B ,H , we have ℓ (B ) ℓ (H ) L2 G ,Bˆ Hˆ as shown in (67). Therefore, by
t t ∈ L t t − t t ≤ 1h t t − t i
(77)and(79)wegetℓ (B ) ℓ (H ) L2 1 W Hˆ 2 L2 1 W Hˆ 2 +ρL2 1 G˜ 2 +3L2δ G
t t − t t ≤ 2ρk t − t kF−2ρk t+1 − t kF 2 k t kF 1 t k t k∗ ≤
L2 1 W Hˆ 2 L2 1 W Hˆ 2 +40.5ρℓ (B )+6L δ ℓ (B ). Since ρ = 1 , by rearranging
2ρk t − t kF − 2ρk t+1 − t kF t t 1 t t t 81
and simplifying terms in the above inequality, we obtain
p
ℓ (B ) 2ℓ (H )+81L2 W Hˆ 2 81L2 W Hˆ 2 +12L δ ℓ (B ). (80)
t t ≤ t t 1k t − t kF − 1k t+1 − t kF 1 t t t
p
We observe that the above upper bound on ℓ (B ) is implicit since it appears on both sides of (80).
t t
To derive an explicit upper bound, we apply the following lemma.
Lemma 6.14. If the real number x satisfies x A+B√x, then we have x 2A+B2.
≤ ≤
Proof. By using the assumption, we have √x B 2 A + B2 . Hence, we obtain that x
− 2 ≤ 4 ≤
2
A+ B2 + B 2A+B2. (cid:0) (cid:1)
4 2 ≤
(cid:18)q (cid:19)
By applying Lemma 6.14 to (80) with A = 2ℓ (H )+81L2 W Hˆ 2 81L2 W Hˆ 2 and
t t 1k t − t kF − 1k t+1 − t kF
B = 12L δ , we get
1 t
ℓ (B ) 4ℓ (H )+162L2 W Hˆ 2 W Hˆ 2 +144L2δ2. (81)
t t ≤ t t 1 k t − t kF −k t+1 − t kF 1 t
(cid:16) (cid:17)
Furthermore, note that W Hˆ 2 W Hˆ 2 = ( W Hˆ + W
k t+1 − t+1 kF − k t+1 − t kF k t+1 − t+1 kF k t+1 −
Hˆ )( W Hˆ W Hˆ ) 4√d Hˆ Hˆ = 4√d H H , where
t kF k t+1 − t+1 kF − k t+1 − t kF ≤ k t+1 − t kF L1 k t+1 − t kF
in the last inequality we used that Hˆ ,Hˆ ,W (0) and the triangle inequality. There-
t t+1 t+1 ∈ B√d
fore, we can write T t=−01 kW
t −
Hˆ
t
k2
F −
kW
t+1 −
Hˆ
t
k2
F ≤
kW
0 −
Hˆ
0
k2
F
+ T t=−02 kW
t+1 −
Hˆ
t+1
k2
F
−kW
t+1
−H Pˆ
t
k2
F (cid:0) ≤
kW
0
−Hˆ
0
k2
F
+ 4 L√ 1d T t=−02 kH
t+(cid:1)1
−H
t
kF. By summin Pg the (cid:0)inequality
in (81) from t = 0 to T −(cid:1)1, we obtain that T t=−0P1ℓ t(B t)
≤
4 T t=−01ℓ t(H t)+162L2 1kW
0
−Hˆ
0
k2
F
+
648√dL
1
T t=−02 kH
t+1
−H
t kF
+144L2
1
T t=−0P1δ t2. Finally, note Pthat W
0
−Hˆ
0
= L1 1(B
0
−H 0) and
T t=−01δ t2 P= 41 T
t=1
t11
/2 ≤
21√T. This c Pompletes the proof by substituting these results into the
above inequality.
P P
Note that in Lemma 6.13, we have the freedom to choose any competitor sequence H in the
t t 0
{ }≥
set . To further obtain an explicit bound, we propose to select H = F(z ) for t = 0,...,T 1,
t t
Z ∇ −
which leads to our next step.
42Step 2: For the choice of H = 2F(z ) for all t 0, we upper bound the cumulative loss
t t
T t=−01ℓ t(H t) and the path-length ∇T t=−02 kH
t+1
−H
t
kF≥ . To begin with, we present the following
lemma.
P P
Lemma 6.15. If Assumption 1.b holds, then kN =−01 kzˆ k − z k k2 ≤ 1kz (0 α− 1z +∗ αk 22 ) and kN =−01 kz k+1 −
z k k2
≤
12 kz (0 α− 1+z∗ αk 22 ). P − P
−
Proof. Recall the inequality (30) in the proof of Proposition 4.2 and that α = α +α < 1. By
1 2
rearranging the terms, we can further derive that
1 α 1 1
− ( zˆ k z k 2+ zˆ k z k+1 2) z k z ∗ 2 z k+1 z ∗ 2.
2 k − k k − k ≤ 2k − k − 2k − k
By summing the above inequality from k = 0 to k = N 1, we obtain that
−
N 1
1 α − 1
− ( zˆ k z k 2+ zˆ k z k+1 2) z 0 z ∗ 2. (82)
2 k − k k − k ≤ 2k − k
k=0
X
Since α < 1, by dropping the second non-negative term from the left-hand side of (82), this
immediately leads to the first inequality in Lemma 6.15. In addition, note that z z 2 =
k k+1
k − k
z zˆ +zˆ z 2 2 z zˆ 2+ zˆ z 2 . Thus, from (82), we also have
k k k k+1 k k k k+1
k − − k ≤ k − k k − k
(cid:0) (cid:1)
N −1
z z 2
2N −1
zˆ z 2+ zˆ z 2
2 kz
0
−z
∗
k2
,
k k+1 k k k k+1
k − k ≤ k − k k − k ≤ 1 α
k=0 k=0 −
X X (cid:0) (cid:1)
which proves the second inequality in Lemma 6.15. The proof is now complete.
With the help of Lemma 6.15, we are set to upper bound the cumulative loss T t=−01ℓ t(H t) and the
path-length T t=−02 kH
t+1
−H
t
kF.
P
Lemma 6.1P6. Recall that H
t
= F(z t) for t = 0,...,T 1, and the constant C is defined in
∇ −
(59). Also, let N be the total number of iterations. Then T t=−01ℓ t(H t)
≤
C 2L2 2 kz
0
−
z
∗
k2 and
T t=−02 kH
t+1
−H
t kF ≤
L
2 1
(2 αd 1N +α2)kz
0
−z
∗
k.
P
−
q
P
Proof. By the fundamental theorem of calculus, we have u = F(z˜) F(z ) = H¯ s , where
t t t t t
H¯ = 1 F(z + λs )dλ. Moreover, using the triangle inequality, w− e have H¯ H
t 0 ∇ t t k t − t kop ≤
1 F(z +λs ) F(z ) dλ 1 L λ s dλ = L2 s , where we used Assumption 2.3
0 k∇ R t t −∇ t kop ≤ 0 2 k t k 2 k t k
Rin the second inequality. FurthermorRe, we can bound ℓ t(H t) = kut − sH tt 2st k2 = k(H¯ t − sH
t
t 2)st k2
≤
kH¯ t −H st tk2 o 2pkst k2 = kH¯
t
−
H
t
k2 op. Combining the two inequalities abovk e,k we get T t=−k 01ℓk t(H t)
≤
T t=−0k 1 kk H¯
t −
H
t
k2
op ≤
L 42 2 T t=−01 ks
t
k2. Note that by our notation, T t=−01 ks
t
k2 =P
k
ks
k
k2 =
z˜ z 2. Using the same arguments as in (70) of Lemma 6.7, we have z˜∈B z 2
Pk k k − k k P P k Pk k − k k ≤
2 PC k∈ zB
0
−z
∗
k2. This proves that T t=−01ℓ t(H t)
≤
C 2L2 2 kz
0
−z
∗
k2.
P
∈B
Next, using the fact that A
F
P√d A
op
for any matrix A Rd ×d, we have
k k ≤ k k ∈
T 2 T 2 T 2
− − −
H H √d H H √dL z z ,
t+1 t F t+1 t op 2 t+1 t
k − k ≤ k − k ≤ k − k
t=0 t=0 t=0
X X X
43where we used Assumption 2.3 in the last inequality. Note that by our notation, we denote the in-
ad bic oe vs ein inB eqa us a{ lik t0 y, bk e1 c, o.. m. e, sk T −1
T
t=} −0, 2a kn zd ktz +t 1i −s za ks th ko ,r at nh dan bd yf to hr ez tk rt i. anT gh leus in,t eh qe uasu litm
y
PweT t= h−0 a2 vk ez kt+
z
k1 t− +1z −t k zi kn tkth ≤e
k kt =+ k1 t−1 kz
k+1
−z
k
k. Hen Pce, combining this with the second inequality in Lemma 6.15, we further
Phave T t=−02 kz kt+1−z
ktk ≤
kN =−01 kz
k+1
−z
k k≤
N kN =−01 kz
k+1
−z
k
k2
≤ 1
(α2 1N +α2)kz
0
−z
∗
k,
−
where we used Cauchy-Schwarz inequality in theqlast inequality. q
P P P
Step 3: CombiningLemma6.16 andLemma6.13, weobtainthat T t=−01ℓ t(B t)
≤
2CL2 2kz
0
−z
∗
k2+
162 kB 0 −H 0 k2 F + 648√ √2 1dL1 (αL 12 +kz α0 2− )z∗ k√N + 72L2 1√N. Given the d Pefinitions of Q 1 and Q 2 in (75)
−
and (76), the above upper bound can be further simplified as T t=−01ℓ t(B t)
≤
L2 1Q2
1
+ L2 1Q2 2√N.
Together with the fact that σ = α2β, if follows from Lemma 4.4 that
0 5L1 P
N −1 1 5L
1
N(L2 1Q2 1+L2 1Q2 2√N) L 1(5+Q 1√N +Q 2N3/4)
+ .
η ≤ (1 β)α β q (1 β)α β ≤ (1 β)α β
k 2 2 2
k=0 − − −
X
In light of Proposition 4.2(b) and the observation in (37), we obtain the second rate in Part (b) of
Theorem 6.12.
6.2.2 Charaterizing the computational cost
In this section, we characterize the computation cost of QNPE in the monotone setting. Since
the proof techniques are similar to those used in the strongly monotone setting, we summarize the
computational cost in the following theorem and defer the proof to Appendix C.
Theorem 6.17. Let N denote the minimum number of iterations required by Algorithm 1 to find
ǫ
an ǫ-accurate solution according to Theorem 6.12.
(a) QNPE requires at most 3N +log (5σ0L1) operator evaluations.
ǫ 1/β α2
(b) For Problems (1) and (5), QNPE requires
O
N ǫ+
L1maxz∈D
ǫ
kz0 −z k2
+σ 0L
1
matrix-vector
products for LinearSolver. Moreover, it requires(cid:16) (N1.125log(dN ǫ2 )) matrix-vec(cid:17)tor products for
O ǫ p2
ExtEvec and MaxSvec.
(c) For Problem (4), QNPE requires
O
N ǫ+ N ǫL1maxz∈ 2D ǫkz0 −z k2 +√σ 0L
1
matrix-vector
products for LinearSolver. Moreover,(cid:18) it requq ires (N1.125log(dN ǫ2 )) matrix(cid:19) -vector products
O ǫ p2
for ExtEvec.
As in the case of strongly monotone problems (cf. Theorem 6.8), Theorem 6.17 shows that the
average number of operator evaluations per iteration is bounded by a constant close to 3 for suf-
ficiently large N. Moreover, note that Theorem 6.17 implies that N = min 1,
d2/5
. Hence,
ǫ O {ǫ ǫ4/5}
for the nonlinear equation in (1) and the minimax problem in (5), Theorem(cid:16) 6.17(a) dem(cid:17)onstrates
that the total number of matrix-vector products can be bounded by (1) and ˜ min 1 ,d9/20
O ǫ O {ǫ9/8 ǫ9/10}
for LinearSolver and ExtEvec and MaxSvec, respectively. Moreover, for the minim(cid:16)ization problem(cid:17)
in (4), Theorem 6.9(b) shows that the total number of matrix-vector products can be bounded by
44min 1, d1/5 and ˜ min 1 ,d9/20 for LinearSolver and ExtEvec, respectively. In particular,
O {ǫ ǫ9/10} O {ǫ9/8 ǫ9/10}
the(cid:16)computation(cid:17)al cost is(cid:16)dominated by t(cid:17)he latter in this case.
7 Conclusion
We proposed a novel quasi-Newton proximal extragradient method for solving smooth and mono-
tone nonlinear equations, particularly relevant to unconstrained minimization optimization and
minimax optimization. We also demonstrated how to exploit structures in the Jacobian matrices,
such as symmetry and sparsity, leading to more efficient implementation. Inthe strongly monotone
setting, we established a global linear convergence rate of (1+( µ )) k, comparable to the extra-
30L1 −
gradient method, and an explicit superlinear convergence rate of (1+Ω(√k)) k. Moreover, in the
−
monotone setting, we demonstrated a global convergence rate of min 1, √d , which is also
O {k k1.25}
superior to the convergence rate of (1) by the extragradient metho(cid:16)d. Our results(cid:17)are the first to
O k
show a better global complexity bound for using a quasi-Newton method over first-order methods
such as the extragradient method, without requiring access to the operator’s Jacobian.
45Appendix
A The line search scheme termination
In our backtracking line search scheme, we repeatedly reduce the step size η by a factor of β
+
until we find a pair (η ,zˆ ) that satisfies the condition in (18) (refer also to Lines 4 and 5 in
+ +
Subroutine 1). The following lemma demonstrates that once the step size η falls below a specific
+
threshold, the pair (η ,zˆ ) will meet both conditions in (17) and (18). Consequently, this ensures
+ +
that Subroutine 1 will terminate within a finite number of steps.
Lemma A.1. Suppose Assumption 2.2 holds. If η < α2 and zˆ is computed according to
+ L1+ B op +
k k
(19), then the pair (η ,zˆ ) satisfies the conditions in (17) and (18).
+ +
Proof. Since zˆ follows the update rule in (19), by Definition 3.1, the pair (η ,zˆ ) will always
+ + +
satisfy the condition in (17). Hence, it is sufficient to prove that the condition in (18) also holds.
Recall that g is defined as g = F(z). By Assumption 2.2, the operator F is L -Lipschitz and
1
thus it follow that F(zˆ ) g = F(zˆ ) F(z) L zˆ z . Further, applying the triangle
+ + 1 +
k − k k − k ≤ k − k
inequality yields
F(zˆ ) g B(zˆ z) F(zˆ ) g + B(zˆ z) (L + B ) zˆ z .
+ + + + 1 op +
k − − − k ≤k − k k − k ≤ k k k − k
Hence, if η α2 , we have
+ ≤ L1+ B op
k k
η F(zˆ ) g B(zˆ z) α zˆ z . (83)
+ + + 2 +
k − − − k ≤ k − k
Finally, by using the triangle inequality again, we combine (17) and (83) to establish that
zˆ z+η F(zˆ ) = zˆ z+η (g+B(zˆ z))+η (F(zˆ ) g B(zˆ z))
+ + + + + + + + +
k − k k − − − − − k
zˆ z+η (g+B(zˆ z)) + η (F(zˆ ) g B(zˆ z))
+ + + + + +
≤ k − − k k − − − k
α 1+η µ zˆ z +α zˆ z
1 + + 2 +
≤ k − k k − k
(α 1p+α 2) 1+η +µ zˆ
+
z ,
≤ k − k
p
which confirms that the condition in (18) is satisfied. This completes the proof.
B A supporting lemma for Theorem 6.9
Lemma B.1. Suppose A Rd d satisfies that 1(A+A ) λI. Then we have λ (A A) λ2.
∈ × 2 ⊤ (cid:23) min ⊤ ≥
Proof. Letσ betheminimumsingularvalueofAandletu,v Rd bethecorrespondingleftand
min
∈
right singular vectors, respectively, where u = 1 and v = 1. Therefore, we have Av = σ u
min
k k k k
and A u = σ v. Moreover, we have
⊤ min
1
v Av = σ u v σ ( u 2 + v 2) σ .
⊤ min ⊤ min min
≤ 2 k k k k ≤
Furthermore, we also have v Av = 1v (A + A )v λ v 2 = λ. This proves that σ λ.
⊤ 2 ⊤ ⊤ ≥ k k min ≥
Since A is non-singular, we have λ (A A) = σ2 = λ2. This completes the proof.
min ⊤ min
46C Proof of Theorem 6.17
In this section, we present the proof of Theorem 6.17. To begin with, we prove the result in Part
(a), which follows similarly to Theorem 6.8.
Proof of Theorem 6.17(a). Recall that l denoted the number of line search steps in iteration k.
k
Using the same arguments as in the proof of Theorem 6.8, the total number of line search steps
afterN iterations canbeboundedby kN =−01l
k
= 2N −1+log
1/β
ηNσ0 −1. Sincewehaveη
N −1
≥
α 5L2β
1
as
shown in the proof of Theorem 6.12(a P), we obtain that kN =−01l
k
= 2N −1+log
1/β
5 ασ0 2L β1. Counting
the additional operator evaluation for F(z ) in each iteration, we prove the result in Part (a).
k
P
Inorder to characterize thenumberof matrix-vector products,we presentthe following proposition
regarding the convergence of Subroutine 3 in the monotone setting.
Proposition C.1. Let s be any optimal solution of As = b. Moreover, let s and r
∗ ∗ k k 0 k k 0
be generated by Subroutine 3. If A is non-symmetric, then we have kr
k k2
≤{ √2 k} A kk≥
+op
1ks∗ k2.{ Ot} he≥
r-
wise, if A is symmetric, then kr k k2
≤
kA (k ko +p k 1s )2∗ k2.
Proof. When A is non-symmetric, Subroutine 3 executes the CGLS algorithm on the least-squares
problem As b 2. Moreover, note that CGLS is analytically equivalent to applying the con-
k − k
jugate gradient method on the normal equation A As = A b. Thus, it follows from standard
⊤ ⊤
results on conjugate gradient methods that (see, e.g., [dST+21, Section B.2]): As b 2
k
2λmax(A (⊤ k+A 1)
)k
2s0 −s∗ k2
2. By taking the square roots of both sides and noting that s 0 =
0k
, we
ob− taink th≤
e
first result in Proposition C.1. When A is symmetric, Subroutine3 executes the conjugate residual
method and the second result in Proposition C.1 follows from [Nem95, Chapter 12.4].
As a corollary of Proposition C.1, we upper bound the total number of matrix-vector products in
Subroutine 1 for given inputs A, b, and ρ.
Lemma C.2. Given the inputs A Rd d, b Rd, and ρ > 0. Suppose that 1(A + A ) I.
∈ × ∈ 2 ⊤ (cid:23)
When Subroutine 1 returns, the total number of matrix-vector product evaluations can be bounded
by 2√2( ρρ+1) kA
kop
if A is non-symmetric, and (ρ+1) ρkA kop if A is symmetric.
q
Proof. First of all, we shall prove that Subroutine 1 terminates when r ρ s . To see
k k k2 ≤ ρ+1k ∗ k2
this, note that s s + s s by the triangle inequality. Also, since 1(A+A ) I,
k ∗ k2 ≤ k k k2 k k − ∗ k2 2 ⊤ (cid:23)
we have A(s s ) s s (s s ) A(s s ) s s 2, which implies that r =
k k − ∗ k2 k k − ∗ k2 ≥ k − ∗ ⊤ k − ∗ ≥ k k − ∗ k2 k k k2
A(s s ) s s . Hence, combining the two inequalities leads to s s + r .
k ∗ 2 k ∗ 2 ∗ 2 k 2 k 2
k − k ≥ k − k k k ≤ k k k k
Thus, if r ρ s , we obtain that r ρ ( s + r ), which is equivalent to
k k k2 ≤ ρ+1k ∗ k2 k k k2 ≤ ρ+1 k k k2 k k k2
r ρ s and hence the termination criterion of Subroutine 1 is satisfied.
k 2 k 2
k k ≤ k k
When A is non-symmetric, by Proposition C.1, Subroutine 1 returns if √2 kA kk +op 1ks∗ k2
≤
ρ+ρ 1ks ∗ k2,
√2(ρ+1)
whichisequivalenttok A 1. SinceSubroutine1requirestwomatrix-vectorproducts
≥ ρ k kop −
per iteration for a non-symmetric matrix A, the total number of matrix-vector products can be
2√2(ρ+1)
bounded by A . Similarly, when A is symmetric, it follows from Proposition C.1 that
ρ k kop
Subroutine 1 returns if kA (k ko +p k 1s )2∗ k2
≤
ρ+ρ 1ks ∗ k2, which is equivalent to k
≥
(ρ+1) ρkA kop −1. Since
q
47Subroutine 1 requires one matrix-vector product per iteration for a symmetric matrix A, the total
number of matrix-vector products can be bounded by (ρ+1) kA kop.
ρ
q
Now we are ready to prove the remaining of Theorem 6.17.
Proof of Theorem 6.17(b) and (c). First, we consider Problems (1) and (5), where the Jacobian
approximation matrices B is non-symmetric. Consider the k-th iteration of Algorithm 1 and
k
{ }
let l denote the total number of line search steps in Subroutine 1. For notational convenience,
k
we set η = βσ . Note that at the i-th line search step (i 1), we call the LinearSolver oracle
1 0
with A =− I+η βi 2B and ρ = α . Moreover, recall from≥ the proof of Theorem 6.12(a)that
k 1 − k 1
1(B + B ) 0− and B 4L . Since 1(I + η βi 2B + (I + η βi 2B ) ) I and
2 I+k η β⊤k i 2B(cid:23) 1k +ηk kop β≤ i 2 B1 1+2 4η βk −i1 2L− , byk using Lemk − m1 a C− .2,k we⊤ ca(cid:23) n bound
k 1 − k op k 1 − k op k 1 − 1
k − k ≤ − k k ≤ −
the total number of matrix-vector products at the k-th iteration by
lk
2√2(α +1) 2√2(α +1) 8√2(α +1)
MV 1 (1+4η βi 2L ) 1 l + 1 η L .
k k 1 − 1 k k 1 1
≤ α 1 − ≤ α 1 α 1β(1 β) −
i=1 −
X
Thus, the total number of matrix-vector products is bounded by
Nǫ −1 2√2(α 1+1) Nǫ −1 8√2(α 1+1)L
1
Nǫ −1
MV l + η . (84)
k k k 1
≤ α 1 α 1β(1 β) −
k=0 k=0 − k=0
X X X
By Theorem 6.17(a), we have N k=ǫ −01l
k
≤
3N
ǫ
+ log 1/β(5σ α0 2L1). Further, we can show that
N k=ǫ −02η k
≤
maxz∈D 2k ǫz0 −z k2 by th Pe definition of N ǫ. To see this, by Proposition 4.2, it holds that
G Pap(z¯ N; D) ≤ max 2z P∈D N k=−k 0z 10 η− kz k2 . Moreover, since N ǫ is the minimum number of iterations to achieve
Gap(z¯ ; ) ǫ, we must have Gap(z¯ ; ) > ǫ and this implies the desired result. Hence, we
N
D ≤
Nǫ −1
D
further obtain from (84) that
Nǫ −1 6√2(α 1+1) 2√2(α 1+1) 5σ 0L
1
MV N + log ( )
k ≤ α ǫ α 1/β α
1 1 2
k=0
X
4√2(α +1)L max z z 2 8√2(α +1)L σ
1 1 z 0 1 1 0
+ ∈D k − k + .
α β(1 β)ǫ α (1 β)
1 1
− −
Next, we consider Problem (4), wherethe Jacobian approximation matrices B is symmetric and
k
{ }
positive semi-definite. We follow similar arguments as in the non-symmetric case. Consider the
k-thiteration ofAlgorithm1. Recallthatatthei-thlinesearchstep(i 1),wecalltheLinearSolver
≥
oracle with A = I+η βi 2B and ρ = α . Since 0 B 4L I, by using Lemma C.2, we can
k 1 − k 1 k 1
− (cid:22) (cid:22)
bound the total number of matrix-vector products at the k-th iteration by
lk
(α +1)(1+4η βi 2L )
lk
α +1
lk
α +1
MV
k
1 k −1 − 1 1 + 2 1 η
k
1L 1β2i −1
≤ i=1s α 1 ≤ i=1r α 1 i=1 r α 1 −
X X X p
α +1 2√α +1
1 1
l + η L .
k k 1 1
≤ r α 1 √α 1β(1 −√β) −
p
48Hence, the total number of matrix-vector products is bounded by
Nǫ −1
α 1+1
Nǫ −1
2 (α 1+1)L
1
Nǫ −1
MV l + √η .
k k k 1
k=0 ≤ r α 1 k=0 √ pα 1β(1 −√β) k=0 −
X X X
Again, by Theorem 6.17(a), we have N k=ǫ −01l
k
≤
3N
ǫ
+log 1/β(5σ α0 2L1). Moreover, it follows from
the definition of N ǫ that N k=ǫ −02 √η k ≤P√N ǫ N k=ǫ −02η k
≤
N ǫmaxz∈D 2k ǫz0 −z k2 . Hence, we obtain
q q
P P
Nǫ −1
MV k
≤
α
1
α+1
3N ǫ+log
1/β(5σ α0L
1 ) +
√2
α
β( (α 11+ √1)
β)
N
ǫL 1max
z ∈D
2ǫkz
0
−z k2
Xk=0 r 1 (cid:16) 2 (cid:17) p1 − r
2 (α +1)σ L
1 0 1
+ .
√α (1 √β)
p 1
−
Finally, we bound the number of matrix-vector products for ExtEvec and MaxSvec in all cases.
Since the parameters are selected as δ = 1 and q = p p , it
t 2(t+1)1/4 t 2.5(t+1)log2(t+1) ≤ 2.5Nǫlog2(Nǫ)
follows from Lemmas 5.2 and 5.4 that the total number of matrix-vector products can be bounded
by T 1+δt log d = T (t+1)1/8log(dN ǫ2 ) = N1.125log(dN ǫ2 ) . The proof is
O t=1 δt q t2 O t=1 p2 O ǫ p2
compl(cid:16)ete. q (cid:17) (cid:16) (cid:17) (cid:16) (cid:17)
P P
49References
[ALY23] A. Asl, H. Lu, and J. Yang. “A J-symmetric quasi-Newton method for minimax
problems”. Mathematical Programming (2023), pp. 1–48 (page 7).
[ACXM92] B. M. Averick, R. G. Carter, G.-L. Xue, and J. Mor´e. The MINPACK-2 test problem
collection. Tech. rep. Preprint MCS–P153–0692. Argonne National Laboratory, 1992
(page 2).
[BTB22] M. Barr´e, A. Taylor, and F. Bach. “A note on approximate accelerated forward-
backward methods with absolute and relative errors, and possibly strongly convex
objectives”. OpenJournal of Mathematical Optimization 3(2022),pp.1–15(page10).
[Bjo¨96] ˚A. Bjo¨rck. Numerical methods for least squares problems. SIAM, 1996 (page 37).
[Bro65] C. G. Broyden. “A class of methods for solving nonlinear simultaneous equations”.
Mathematics of Computation 19.92 (1965), pp. 577–593 (page 2).
[Bro71] C. G. Broyden. “The convergence of an algorithm for solving sparse nonlinear sys-
tems”. Mathematics of Computation 25.114 (1971), pp. 285–294 (page 7).
[Bro70] C. G. Broyden. “The convergence of single-rank quasi-Newton methods”. Mathemat-
ics of Computation 24.110 (1970), pp. 365–382 (page 2).
[BDM73] C. G. Broyden, J. E. Dennis Jr, and J. J. Mor´e. “On the local and superlinear
convergence of quasi-Newton methods”. IMA Journal of Applied Mathematics 12.3
(1973), pp. 223–245 (page 2).
[BKS96] R. H. Byrd, H. F. Khalfan, and R. B. Schnabel. “Analysis of a symmetric rank-
one trust region method”. SIAM Journal on Optimization 6.4 (1996), pp. 1025–1039
(page 2).
[BNY87] R. H. Byrd, J. Nocedal, and Y.-X. Yuan. “Global convergence of a class of quasi-
Newton methods on convex problems”. SIAM Journal on Numerical Analysis 24.5
(1987), pp. 1171–1190 (pages 2, 3).
[CS84] H.-S. Chen and M. A. Stadtherr. “On solving large sparse nonlinear equation sys-
tems”. Computers & chemical engineering 8.1 (1984), pp. 1–7 (page 7).
[CGT91] A.R.Conn,N.I.Gould,andP.L.Toint.“Convergenceofquasi-Newtonmatricesgen-
erated by the symmetric rank one update”. Mathematical programming 50.1 (1991),
pp. 177–195 (page 2).
[dST+21] A. d’Aspremont, D. Scieur, A. Taylor, et al. “Acceleration methods”. Foundations
and Trends® in Optimization 5.1-2 (2021), pp. 1–245 (page 47).
[Dav59] W. C. Davidon. Variable metric method for minimization. Techinical Report ANL-
5990. Argonne, IL: Argonne National Laboratory, 1959 (page 2).
[DM74] J. E. Dennis and J. J. Mor´e. “A characterization of superlinear convergence and its
application to quasi-Newton methods”. Mathematics of computation 28.126 (1974),
pp. 549–560 (page 2).
[DM77] J. Dennis J. E. and J. J. Mor´e. “Quasi-Newton Methods, Motivation and Theory”.
SIAM Review 19.1 (1977), pp. 46–89. (Visited on 06/15/2022) (page 6).
[Dix72] L. C. W. Dixon. “Variable metric algorithms: necessary and sufficient conditions for
identical behavior of nonquadratic functions”. Journal of Optimization Theory and
Applications 10.1 (1972), pp. 34–40 (page 2).
50[FP04] F. Facchinei and J.-S. Pang. Finite-Dimensional Variational Inequalities and Com-
plementarity Problems. Springer New York, 2004 (page 2).
[Fle70] R. Fletcher. “A new approach to variable metric algorithms”. The computer journal
13.3 (1970), pp. 317–322 (page 2).
[FP63] R. Fletcher and M. J. Powell. “A rapidly convergent descent method for minimiza-
tion”. The Computer Journal 6.2 (1963), pp. 163–168 (page 2).
[Fon11] D. C.-L. Fong. “Minimum-Residual Methods for Sparse Least-Squares Using Golub-
Kahan Bidiagonalization”. PhD thesis. Stanford University, 2011 (page 37).
[GS18] H. Ghanbari and K. Scheinberg. “Proximal quasi-Newton methods for regularized
convex optimization with linear and accelerated sublinear convergence rates”. Com-
putational Optimization and Applications 69 (2018), pp. 597–627 (page 3).
[Gol70] D. Goldfarb. “A family of variable-metric methods derived by variational means”.
Mathematics of computation 24.109 (1970), pp. 23–26 (page 2).
[GV13] G. H. Golub and C. F. Van Loan. Matrix computations. 4th ed. JHU press, 2013
(page 29).
[Gre97] A. Greenbaum. Iterative methods for solving linear systems. SIAM, 1997 (page 37).
[Gri86] A. Griewank. “The “global” convergence of Broyden-like methods with suitable line
search”. The ANZIAM Journal 28.1 (1986), pp. 75–92 (page 2).
[Gri87] A. Griewank. “The local convergence of Broyden-like methods on Lipschitzian prob-
lems in Hilbert spaces”. SIAM Journal on Numerical Analysis 24.3 (1987), pp. 684–
705 (page 2).
[HS+52] M. R. Hestenes, E. Stiefel, et al. “Methods of conjugate gradients for solving lin-
ear systems”. Journal of research of the National Bureau of Standards 49.6 (1952),
pp. 409–436 (page 27).
[JJM23] R. Jiang, Q. Jin, and A. Mokhtari. “Online Learning Guided Curvature Approxi-
mation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Conver-
gence”.In:Proceedings of ThirtySixth Conference onLearning Theory (COLT).2023,
pp. 1962–1992 (page 1).
[JKRM22] Q. Jin, A. Koppel, K. Rajawat, and A. Mokhtari. “Sharpened Quasi-Newton Meth-
ods: Faster Superlinear Rate and Larger Local Convergence Neighborhood”. In: Pro-
ceedings of the 39th International Conference on Machine Learning. PMLR, 2022,
pp. 10228–10250 (page 3).
[JM22] Q. Jin and A. Mokhtari. “Non-asymptotic superlinear convergence of standard quasi-
Newton methods”. Mathematical Programming (2022) (page 3).
[KZAT23] D. Kamzolov, K.Ziu,A.Agafonov, andM.Tak´aˇc. “Accelerated AdaptiveCubicReg-
ularized Quasi-Newton Methods”. arXiv preprint arXiv:2302.04987 (2023) (page 3).
[KS91] C.T.Kelley andE.W.Sachs. “Anew proofof superlinearconvergence for Broyden’s
method in Hilbert space”. SIAM Journal on Optimization 1.1 (1991), pp. 146–150
(page 2).
[KBS93] H. F. Khalfan, R. H. Byrd, and R. B. Schnabel. “A theoretical and experimental
studyof thesymmetricrank-oneupdate”.SIAM Journal on Optimization 3.1 (1993),
pp. 1–24 (page 2).
51[Kor76] G. Korpelevich. “The extragradient method for findingsaddle points and other prob-
lems”. Ekonomika i Matematicheskie Metody 12 (1976). In Russian; English transla-
tion in Matekon, pp. 747–756 (page 4).
[KW92] J.Kuczyn´skiandH.Wo´zniakowski. “EstimatingtheLargestEigenvaluebythePower
and Lanczos Algorithms with a Random Start”. SIAM Journal on Matrix Analysis
and Applications 13.4 (1992), pp. 1094–1122 (pages 24, 27).
[LF00] D.-H. Li and M. Fukushima. “A derivative-free line search and global convergence
of Broyden-like method for nonlinear equations”. Optimization methods and software
13.3 (2000), pp. 181–201 (page 2).
[LYZ21a] D. Lin, H. Ye, and Z. Zhang. “Explicit superlinear convergence rates of Broyden’s
methods in nonlinear equations”. arXiv preprint arXiv:2109.01974 (2021) (page 3).
[LYZ21b] D. Lin, H. Ye, and Z. Zhang. “Greedy and random quasi-Newton methods with
faster explicit superlinear convergence”. Advances in Neural Information Processing
Systems 34 (2021), pp. 6646–6657 (page 3).
[LCLL23] C. Liu, C. Chen, L. Luo, and J. C. Lui. “Block Broyden’s Methods for Solving Non-
linear Equations”. In: Thirty-seventh Conference on Neural Information Processing
Systems. 2023 (page 3).
[MMT03] D. Mackey, N. Mackey, and F. Tisseur. “Structured tools for structured matrices”.
The Electronic Journal of Linear Algebra 10 (2003), pp. 106–145 (page 7).
[Mar70] B. Martinet. “Br`eve communication. R´egularisation d’in´equations variationnelles par
approximations successives”. ESIAM Mathematical Modelling and Numerical Analy-
sis 4.R3 (1970), pp. 154–158 (page 5).
[Mar00] J. M. Mart´ınez. “Practical quasi-Newton methods for solving nonlinear systems”.
Journalofcomputational andAppliedMathematics 124.1-2(2000),pp.97–121(page7).
[Mha22] Z. Mhammedi. “Efficient Projection-Free OnlineConvex Optimization with Member-
ship Oracle”. In: Proceedings of Thirty Fifth Conference on Learning Theory. PMLR,
2022, pp. 5314–5390 (pages 12, 18, 19).
[MS12] R. D. Monteiro and B. F. Svaiter. “Iteration-complexity of a Newton proximal ex-
tragradient method for monotone variational inequalities and inclusion problems”.
SIAM Journal on Optimization 22.3 (2012), pp. 914–935 (pages 9, 10).
[MS10] R.D.MonteiroandB.F.Svaiter.“Onthecomplexityofthehybridproximalextragra-
dient method for the iterates and the ergodic mean”. SIAM Journal on Optimization
20.6 (2010), pp. 2755–2787 (pages 4–10, 15).
[MT76] J.J.Mor´eandJ.A.Trangenstein.“OntheGlobalConvergenceofBroyden’sMethod”.
Mathematics of Computation 30.135 (1976), pp. 523–540 (page 2).
[Mor90] J.J.Mor´e.“Acollectionofnonlinearmodelproblems”.In:Computational Solution of
NonlinearSystems of Equations. Vol. 26. Lectures in Applied Mathematics. American
Mathematical Society, 1990, pp. 723–762 (page 2).
[Nem95] A.Nemirovski.“Information-basedcomplexityofconvexprogramming”.Lecture notes
834 (1995) (page 47).
[Nem04] A. Nemirovski. “Prox-method with rate of convergence O(1/t) for variational in-
equalities with Lipschitz continuous monotone operators and smooth convex-concave
saddle point problems”. SIAM Journal on Optimization 15.1 (2004), pp. 229–251
(page 4).
52[NY83] A. S. Nemirovsky and D. B. Yudin. Problem complexity and method efficiency in
optimization. John Wiley & Sons, 1983 (pages 4, 31).
[Nes07] Y. Nesterov. “Dual extrapolation and its applications to solving variational inequal-
ities and related problems”. Mathematical Programming 109.2 (2007), pp. 319–344
(page 7).
[NP06] Y.NesterovandB.T.Polyak.“Cubicregularization ofNewtonmethodanditsglobal
performance”. Mathematical programming 108.1 (2006), pp. 177–205 (page 6).
[NW06] J. Nocedal and S. J. Wright. Numerical Optimization. Springer Science+Business
Media, LLC, 2006 (page 2).
[OX21] Y. Ouyang and Y. Xu. “Lower complexity bounds of first-order methods for convex-
concave bilinear saddle-point problems”. Mathematical Programming 185.1 (2021),
pp. 1–35 (pages 4, 41).
[PS82] C. C. Paige and M. A. Saunders. “LSQR: An algorithm for sparse linear equations
and sparseleast squares”. ACM Transactions on Mathematical Software (TOMS) 8.1
(1982), pp. 43–71 (page 27).
[Pow70] M. J. D. Powell. “A hybrid method for nonlinear equations”. In: Numerical Methods
for Non-linear Algebraic Equations. Gordon and Breach, 1970 (page 2).
[Pow71] M.J.D.Powell. “Ontheconvergence ofthevariablemetricalgorithm”. IMA Journal
of Applied Mathematics 7.1 (1971), pp. 21–36 (page 2).
[Pow76] M. J. D. Powell. “Some global convergence properties of a variable metric algorithm
for minimization without exact line searches”. In: Nonlinear Programming. Vol. IX.
SIAM-AMSProceedings.Philadelphia:Society forIndustrialandAppliedMathemat-
ics, 1976 (page 2).
[Pow72] M. Powell. “Some properties of the variable metric algorithm”. Numerical methods
for nonlinear optimization (1972), pp. 1–17 (page 3).
[Roc76] R. T. Rockafellar. “Monotone Operators and the Proximal Point Algorithm”. SIAM
J. Control Optim. 14.5 (1976), pp. 877–898 (page 5).
[RN21a] A. Rodomanov and Y. Nesterov. “Greedy quasi-Newton methods with explicit su-
perlinear convergence”. SIAM Journal on Optimization 31.1 (2021), pp. 785–811
(page 3).
[RN21b] A. Rodomanov and Y. Nesterov. “New Results on SuperlinearConvergence of Classi-
cal Quasi-Newton Methods”.Journal of Optimization Theory and Applications 188.3
(2021), pp. 744–769 (page 3).
[RY22] E. K. Ryu and W. Yin. Large-scale convex optimization: algorithms & analyses via
monotone operators. Cambridge University Press, 2022 (page 6).
[Saa03] Y. Saad. Iterative Methods for Sparse Linear Systems. Second. Society for Industrial
and Applied Mathematics, 2003 (page 27).
[Saa11] Y. Saad. Numerical methods for large eigenvalue problems: revised edition. SIAM,
2011 (page 27).
[ST16] K. Scheinberg and X. Tang. “Practical inexact proximal quasi-Newton method with
global complexity analysis”. Mathematical Programming 160 (2016), pp. 495–529
(page 3).
53[Sch70] L. Schubert. “Modification of a quasi-Newton method for nonlinear equations with
a sparse Jacobian”. Mathematics of Computation 24.109 (1970), pp. 27–30 (page 7).
[Sci24] D. Scieur. “Adaptive Quasi-Newton and Anderson acceleration framework with ex-
plicit global (accelerated) convergence rates”. In: International Conference on Arti-
ficial Intelligence and Statistics. 2024, pp. 883–891 (page 3).
[Sha70] D. F. Shanno. “Conditioning of quasi-Newton methods for function minimization”.
Mathematics of computation 24.111 (1970), pp. 647–656 (page 2).
[SS99] M. V. Solodov and B. F. Svaiter. “A hybrid approximate extragradient–proximal
point algorithm usingthe enlargement of a maximal monotone operator”. Set-Valued
Analysis 7.4 (1999), pp. 323–345 (pages 4, 5, 8, 9).
[SST10] N. Srebro, K. Sridharan, and A. Tewari. “Smoothness, low noise and fast rates”.
Advances in Neural Information Processing Systems 23 (2010) (page 33).
[Ste83] T. Steihaug. “The conjugate gradient method and trust regions in large scale opti-
mization”. SIAM Journal on Numerical Analysis 20.3 (1983), pp. 626–637 (page 37).
[Sti55] E. Stiefel. “Relaxationsmethoden bester strategie zur l¨osung linearer gleichungssys-
teme”. Commentarii Mathematici Helvetici 29.1 (1955), pp. 157–179 (page 27).
[YLCZ22] H. Ye, D. Lin, X. Chang, and Z. Zhang. “Towards explicit superlinear convergence
rate for SR1”. Mathematical Programming (2022), pp. 1–31 (page 3).
[YLZ21] H. Ye, D. Lin, and Z.Zhang.“Greedy and Random Broyden’s Methods with Explicit
SuperlinearConvergenceRatesinNonlinearEquations”.arXivpreprint arXiv:2110.08572
(2021) (page 3).
[YTFUC21] A.Yurtsever,J.A.Tropp,O.Fercoq,M.Udell,andV.Cevher.“Scalablesemidefinite
programming”. SIAM Journal on Mathematics of Data Science 3.1 (2021), pp. 171–
200 (page 27).
[ZHZ22] J. Zhang, M. Hong, and S. Zhang. “On lower iteration complexity bounds for the
convex concave saddle point problems”. Mathematical Programming 194.1-2 (2022),
pp. 901–935 (pages 4, 31).
[Zin03] M. Zinkevich. “Online convex programming and generalized infinitesimal gradient
ascent”. In: Proceedings of the 20th International Conference on Machine Learning.
2003, pp. 928–936 (page 18).
54