ThispaperwasacceptedtoCRAC2024. Pleaseciteitinsteadoncepublished.
CorPipe at CRAC 2024: Predicting Zero Mentions from Raw Text
MilanStraka
CharlesUniversity,FacultyofMathematicsandPhysics
InstituteofFormalandAppliedLinguistics
Malostranskénám. 25,Prague,CzechRepublic
straka@ufal.mff.cuni.cz
Abstract sentence when they can be inferred, for example
by verb morphology, like in the Czech example
WepresentCorPipe24, thewinningentryto “Rˇekl, že neprˇijde”, translated as “(He) said that
theCRAC2024SharedTaskonMultilingual
(he)won’tcome”.
Coreference Resolution. In this third itera-
WepresentCorPipe24,animprovedversionof
tion of the shared task, a novel objective is
to also predict empty nodes needed for zero our system submitted in last years (Straka, 2023;
coreferencementions(whiletheemptynodes StrakaandStraková,2022). Weevaluatetwovari-
were given on input in previous years). This antsofthesystem. Inatwo-stagevariant,theempty
way,coreferenceresolutioncanbeperformed nodesarefirstpredictedbyabaselinesystemutiliz-
onrawtext. Weevaluatetwomodelvariants: ingapretrainedlanguageencodermodel;1then,the
atwo-stageapproach(wheretheemptynodes
predictedemptynodesare,togetherwiththeinput
are predicted first using a pretrained encoder
words,processedbyoriginalCorPipeusinganother
model and then processed together with sen-
pretrainedencoder. Incomparison,asingle-stage
tencewordsbyanotherpretrainedmodel)and
a single-stage approach (where a single pre- variantemploysasinglepretrainedencodermodel,
trainedencodermodelgeneratesemptynodes, whichpredictstheemptynodes,coreferencemen-
coreference mentions, and coreference links tions,andcoreferencelinksjointly.
jointly). In both settings, CorPipe surpasses
Ourcontributionsareasfollows:
otherparticipantsbyalargemarginof3.9and
• We present the winning entry to the CRAC
2.8 percent points, respectively. The source
code and the trained model are available at 2024 Shared Task on Multilingual Corefer-
https://github.com/ufal/crac2024-corpipe. enceResolution,surpassingotherparticipants
byalargemarginof3.9and2.8percentpoints
1 Introduction withatwo-stageandasingle-stagevariant,re-
spectively.
The CRAC 2024 Shared Task on Multilingual
• We compare the two-stage and the single-
Coreference Resolution (Novák et al., 2024) is
stagesettings,showingthatthetwo-stagesys-
a third iteration of a shared task, whose goal is
tem outperforms the single-stage system by
toaccelerateresearchinmultilingualcoreference
circa one percent points, both in the regular
resolution (Žabokrtský et al., 2023, 2022). This
andtheensembledsetting.
year,thesharedtaskfeatures21datasetsin15lan-
• Apart from the CorefUD 1.2, we eval-
guages from the CorefUD 1.2 collection (Popel
uate the CorPipe performance also on
etal.,2024).
OntoNotes(Pradhanetal.,2013),afrequently
Compared to the last year—apart from 4 new
usedEnglishdataset.
datasetsin3languages—anoveltaskistopredict
• The CorPipe 24 source code is available at
theso-calledemptynodes(accordingtotheUniver-
https://github.com/ufal/crac2024-corpipe un-
salDependenciesterminology;Nivreetal.2020).
der an open-source license. The two-stage
The empty nodes can be considered “slots” that
andthesingle-stagemodelsarealsoreleased,
canbepartofcoreferencementionsevenifnotbe-
undertheCCBY-NC-SAlicense.
ingpresentonthesurfacelevelofasentence. The
empty nodes are particularly useful in pro-drop
1Ourimplementationofthebaselinesystemwasavailable
languages (like Slavic and Romance languages),
to all shared task participants in case they do not want to
where pronouns are sometimes dropped from a predicttheemptynodesthemselves.
1
4202
tcO
3
]LC.sc[
1v65720.0142:viXra2 RelatedWork dense, dense,
#deprels units #deprels units
Output
⋯
Traditionally, coreference resolution was solved dense, 2k units, ReLU dense, 2k units, ReLU head 3:
find the
by first predicting the coreference mentions and ⋯
concat concat dependency
subsequentlyperformingcoreferencelinking(clus- relation for
Token representation Token representation ⋯ each empty
tering) of the predicted mentions. However, in of the most probable of the most probable node
recent years, the end-to-end approach (Lee et al., dependency head dependency head candidate
⋯
2017,2018;Joshietal.,2019,2020)hasbecome
⋯
more popular. Indeed, the baseline of the CRAC Self-attention Output
2022, 2023, and 2024 shared tasks (Pražák et al., Q Q K dense, K dense, fh ie na dd t h2 e:
2021)followthisapproach,aswellasthesecond- dense, dense, 768 units 768 units dependency
768 units 768 units
bestsolutionofCRAC2022(PražákandKonopik, dense, dense, ⋯ 2d ke un ns ie t, s, 2d ke un ns ie t, s, ⋯ eah ce ha d e mfo pr ty
2022)andthethird-bestsolutionofCRAC2023. 2k R eu Ln Uits, 2k R eu Ln Uits, ReLU ReLU ⋯ node
candidate
Theend-to-endapproachhasbeenimprovedby ⋯
⋯
Kirstainetal.(2021)nottoexplicitlyconstructthe ⋯
Output
dense, dense, dense, dense,
spanrepresentations,andbyDobrovolskii(2021) 2 units 2 units 2 units 2 units head 1:
is the
toconsideronlythewordlevel,ignoringthespan dense, dense, dense, dense, ⋯ candidate
2k units, 2k units, 2k units, 2k units,
levelaltogetherduringcoreferencelinking. Simul- ReLU ReLU ReLU ReLU an empty
⋯ node
taneously,Wuetal.(2020)formulatedcoreference
⋯
resolution in a question answering setting, reach-
dense, dense, dense, dense,
ingsuperiorresultsattheexpenseofsubstantially 768 units 768 units 768 units 768 units
more model predictions and additional question- dense, dense, dense, dense,
2k units, 2k units, 2k units, 2k units,
answeringdata. ReLU ReLU ReLU ReLU Candidate
empty node
The current state-of-the-art results on concat concat concat concat represen-
tation
OntoNotes (Pradhan et al., 2013), a frequently dense, dense, dense, dense,
768 units 768 units 768 units 768 units
used English coreference resolution dataset, are
dense, dense, dense, dense,
achievedbyautoregressivemodelswithbillionsof 2k units, 2k units, 2k units, 2k units,
ReLU ReLU ReLU ReLU
parameters: Liuetal.(2022)proposeaspecialized
Pretrained
autoregressivesystem,whileBohnetetal.(2023) mT5 large encoder
model
employ a text-to-text paradigm. However, both
Input
these architectures must call the trained model Yekl , že nepZijde
tokens
repeatedlytoprocessasinglesentence. (He) Said that (he)won't come
Figure 1: The system architecture of the empty node
3 Two-stageCorPipe
predictionbaseline. EveryReLUactivationisfollowed
byadropoutlayerlayerwithadropoutrateof50%.
Thetwo-stagevariantofCorPipeiscomposedof
two steps: first, the empty nodes are predicted
using the baseline system available to all shared
empty node (required by the empty node match-
taskparticipants;then,thecoreferenceresolution
ing during evaluation). The baseline predicts the
is performed using CorPipe. This approach is
empty nodes non-autoregressively, generating at
verysimilartothelastyear’seditionoftheCRAC
most two empty nodes for every input word; the
SharedTask,wheretheemptynodeswerealready
input word becomes the dependency head of the
given on input. Therefore, the last year’s version
predictedemptynode.
CorPipe23(Straka,2023)canbeused.
Theoverviewofthearchitectureisdisplayedin
3.1 EmptyNodesBaseline
Figure1. Theinputwordsofasinglesentenceare
Thebaselineforpredictingemptynodesgenerates firsttokenized, passedthroughapretrainedmT5-
for each empty node the minimum amount of in- large encoder (Conneau et al., 2020), and each
formation: thewordorderpositiondefinedbyan inputwordisrepresentedbytheembeddingofits
inputwordthattheemptynodeshouldfollow(the firstsubword. Then,thecandidateforemptynodes
wordorderpositiondeterminesthepositionofthe are generated, two per word. The first candidate
empty node in coreference mentions) and the de- is generated by passing the input word represen-
pendencyheadandthedependencyrelationofthe tations through a 2k-unit dense layer with ReLU
2activation, a dropout layer, and a 768-unit dense Treebank Precison Recall F -score
1
layer. The second candidate is generated by con-
ca 92.32 91.01 91.66
catenating the first candidate representation with
cs_pcedt 78.22 59.84 67.81
theinputwordrepresentationandpassingtheresult
cs_pdt 81.47 71.56 76.19
throughananalogousdense-dropout-densemodule.
cu 81.61 78.76 80.16
Then,threeheadsareattached,eachfirstpassingits
es 92.04 91.92 91.98
inputbyaReLU-activated2k-unitdenselayerand
grc 90.29 86.58 88.39
dropout: (1)aclassificationlayerdecidingwhether
hu_korkor 74.68 60.21 66.67
acandidateactuallygeneratesanemptynode,(2)
hu_szegedkoref 91.93 89.52 90.71
aself-attentionlayerchoosingthewordorderposi-
pl 87.50 91.61 89.51
tion(i.e.,aninputwordtofollow)foreverycandi-
tr 79.05 93.81 85.80
date, and(3)adependencyrelationclassification
layer,whichprocessesthecandidaterepresentation
Table1: Emptynodespredictionbaselineperformance
concatenated with the representation of the word
onthedevelopmentsetsofCorefUD1.2corporacon-
mostlikelyaccordingtotheword-orderprediction
taining empty nodes. An empty node is evaluated as
head. Pleaserefertothereleasedsourcecodefor
correctifithascorrectdependencyhead,dependency
furtherdetails. relation,andwordorder.
Wetrainasinglemultilingualmodelusingthe
AdaFactoroptimizer(ShazeerandStern,2018)for
totheoriginalpaper.
20epochs,eachepochconsistingof5000batches
CorPipeprocessesthedocumentonesentenceat
containing 64 sentences each. The learning rate
atime;toprovideasmuchcontextaspossible,as
firstlinearlyincreasesfromzerotothepeaklearn-
manyprecedingandatmost50followingwordsare
ingrateof1e-5inthefirstepoch,andthendecays
additionallyaddedoninput,tothelimitofthemax-
tozerointherestofthetrainingaccordingtoco-
imumsegmentsize(512or2560). Thewordsare
sineschedule(LoshchilovandHutter,2017). Each
firstpassedthroughapretrainedlanguageencoder
sentence is sampled from the combination of all
model. Then,coreferencementionsarepredicted
corporacontainingemptynodes(seeTable1),pro-
usinganextensionofBIOencodingcapableofrep-
portionally to the square root of the word size of
resentingpossiblyoverlappingsetofspans. Finally,
thecorrespondingcorpus. Themodelistrainedfor
eachpredictedmentionisrepresentedasaconcate-
19hoursusingasingleL40GPUwith48GBRAM.
nationofitsfirstandlastword,andthemostlikely
The source code is released under the MPL
entitylink(possiblytoitself)ofeverymentionis
license at https://github.com/ufal/crac2024_zero_
generatedusingaself-attentionlayer.
nodes_baseline, together with the complete set of
Duringtraining, the maximum segment sizeis
used hyperparameters. Furthermore, the trained
always 512; however, during inference, we con-
model is available under the CC BY-SA-NC li-
sideralsolargersegmentsizeof2560forthemT5
cense at https://www.kaggle.com/models/ufal-mff/
models,whichsupportlargersegmentsizesdueto
crac2024_zero_nodes_baseline/. Finally, the devel-
theirrelativepositionalembeddings.
opment sets and the test sets of the CorefUD 1.2
datasetswithpredictedemptynodesareavailable
3.3 Training
toallparticipantsoftheCRAC2024SharedTask.
Theintrinsicperformanceofthebaselinesystem We train the coreference resolution system
on the development sets of CorefUD 1.2 is pre- analogously to the CorPipe 23 training proce-
sentedinTable1. Apredictedemptynodeiscon- dure (Straka, 2023). Three model variants are
sidered correct if it has correct dependency head, trained,basedoneithermT5-large,mT5-xl(Xue
dependencyrelation,andalsothewordorder. etal.,2021),orInfoXLM-large(Chietal.,2021).
Foreveryvariant,7multilingualmodelsaretrained
3.2 CoreferenceResolution
onacombinationofallcorpora,differingonlyin
With the empty nodes predicted by the baseline, randominitialization. Thesentencesaresampled
we can directly employ the CorPipe 23 from the proportionallytothesquarerootofthewordsize
last year of the shared task (Straka, 2023). The ofthecorrespondingcorpora.
overviewofthearchitectureispresentedinFigure2 Everymodelistrainedfor15epochs,eachepoch
andbrieflydescribed;formoredetails,pleaserefer consisting of 10k batches. The mT5-large and
3Coreference Linking
Charles University is the oldest university in Czech Republic . It is
(mention representation): dense layer, dim (antecedent representation): dense layer, dim
hidden: dense layer + ReLU, dim hidden: dense layer + ReLU, dim
first token rep.last token rep. first token rep.last token rep. first token rep.last token rep. first token rep.last token rep.
Charles University is the oldest university in Czech Republic . It is
Mention Detection
PUSH, PUSH,
PUSH POP 1 0 0 0 0 PUSH POP 1 0 0
POP 1 POP 1
Inference only: dynamic decoding algorithm producing correctly balanced sequence of instructions
tag logits: dense layer, dim number_of_tags
Dense layer + ReLU, dim
Pretrained Masked Language Model
Token representation, dim
mT5 large / mT5 xl / InfoXLM large
Charles University is the oldest university in Czech Republic . It is ...
Figure2: TheCorPipe23modelarchitectureintroducedinStraka(2023).
InfoXLM-large variants use the batch size of 8 4 Single-stageCorPipe
andtrainfor14hoursonasingleA100with40GB
RAM;themT5-xlvariantemploythebatchsizeof While the two-stage variant is full-fledged, al-
12 and train for 17 hours on 4 A100s with 40GB lowing coreference mention to be composed of
RAMeach. ThemT5variantsaretrainedusingthe anycontinualsequenceofinputwordsandempty
AdaFactoroptimizer(ShazeerandStern,2018)and nodes, it requires two large pretrained encoders,
theInfoXLM-largeistrainedusingAdam(Kingma which makes the model about twice as big and
andBa,2015). Thelearningrateisfirstincreased twiceasslow.
from 0 to the peak learning rate in the first 10% Therefore,wealsoproposeasingle-stagevari-
ofthetrainingandthendecayedaccordingtothe ant,withthegoalofusingjustasinglepretrained
cosineschedule(LoshchilovandHutter,2017);we languageencodermodel. Forsimplicity’ssake,we
employthepeaklearningratesof6e-4,5e-4,and restrictthemodelinthefollowingway: ifacoref-
2e-5 for the mT5-large, mT5-xl, and InfoXLM- erencementioncontainsanemptynode,thewhole
largeencoders,respectively. mention must be just this single empty node. In
Foreachmodel,wekeepthecheckpointsafter otherwords,acoreferencementioneitherdoesnot
every epoch, obtaining a pool of 3·7·15 check- contain empty nodes, or it is just a single empty
points. Fromthispool,weselectthreeconfigura- node. Notethatthisrestrictiondoesnotdecrease
tions: (1)asinglecheckpointreachingthehighest thescoreunderthehead-matchmetricbecauseonly
development score on all the corpora, (2) a best- thementionheadisusedduringscorecomputation.
performingcheckpointforeverycorpusaccording Withthedescribedrestriction,wenolongerneed
toitsdevelopmentset, (3)anensembleof5best- todistinguishbetweenemptynodesandzerocoref-
performingcheckpointsforeverycorpus. erencementions;therefore,thesingle-stagemodel
4zero mention zero mention Zero mention that(1)wepassonlytheinputwordsthroughthe
representation representation
represen- pretrainedlanguageencodermodel,(2)weaddthe
empty node empty node empty node empty node tation for
representation representation representation representation given empty lossoftheclassifierpredictingdependencyrelation
⋯ node
orNONEtotheotherlosses(usingsimpleaddition),
⋯ Dependency and(3)weconcatenatethezeromentionrepresenta-
dense, dense, dense,
#deprels units #deprels units #deprels units relation or tionstothesurfacementionrepresentationsbefore
NONE for
dense, dense, dense, ⋯ every empty thecoreferencelinkingstep.
4D units, 4D units, 4D units,
ReLU ReLU ReLU node
⋯ candidate Wecloselyfollowthetrainingprocedureofthe
two-stage model described in Section 3.3. No-
⋯
dense, dense, dense, dense, tably,wealsoconsiderthesamethreepretraineden-
D units D units D units D units
coders,trainthesamenumberofmodelsusingthe
dense, dense, dense, dense,
4D units, 4D units, 4D units, 4D units, sameoptimizersandlearningrates,andselectthe
ReLU ReLU ReLU ReLU Candidate
samethreeconfigurations(singlebest-performing
empty node
concat concat concat concat
represen- checkpoint,per-corpusbestcheckpoint,andaper-
tation
dense, dense, dense, dense, corpus3-modelensemble).2
D units D units D units D units
dense, dense, dense, dense,
4D units, 4D units, 4D units, 4D units, 5 SharedTaskResults
ReLU ReLU ReLU ReLU
Shared
mT5 large/xl / InfoXLM large; embeddings of dim D encoder Inthesharedtask,eachteamwasallowedtosubmit
model atmostthreesystems. Wesubmittedthefollowing
Input
Yekl , že nepZijde configurations:
tokens
(He) Said that (he)won't come • CorPipe-single,thelarge-sizedsingle-stage
modelcheckpointachievingthebestdevelop-
Figure3: ThechangesintheCorPipe23architecture
when empty nodes and zero mentions are generated mentperformanceacrossallcorpora;
jointlywithmentiondetectionandcoreferencelinking. • CorPipe,thebest-performing3-modelsingle-
stageensembleforeverycorpus;
• CorPipe-2stage, the best-performing 5-
predictsonlysuchemptynodesthatarealsozero
modeltwo-stageensembleforeverycorpus.
coreferencementions. Finally,thewordorderofan
Thefirstconfigurationcorrespondstoareal-world
emptynodeisnolongerneededforevaluation;asa
deploymentscenario,whereasinglemodelwould
result,wenolongerpredictthewordorderexplic-
be used for all corpora; the latter configurations
itlyandplacetheemptynodeafteritsdependency
are the highest performing two-stage approach
headinthewordorder.
(CorPipe-2stage,Section3)andthesingle-stage
InFigure3,wevisualizetheproposedchanges approach(CorPipe,Section4).
totheCorPipearchitectureneededtosupportjoint Theofficialresultsofthesharedtask’sprimary
empty nodes/zero mentions prediction. Analo- metric are presented in Table 2. All our submis-
gously to the empty nodes baseline described in sions outperform other participant systems, even
Section3.1,westartbygeneratingtwocandidate if CorPipe-2stage only slightly. Overall, the en-
emptynodesrepresentationsfromeveryinputword sembledsingle-stagevariantoutperformsotherpar-
representation. We then run a classification head ticipantsby2.8percentpoints,andtheensembled
for every candidate, which either predicts NONE two-stagevariantoutperformsotherparticipantsby
whenthecandidateshouldnotgenerateanempty 3.9percentpoints.
node,oritpredictsthedependencyrelationofthe Table 3 shows the results of the submitted sys-
generatedemptynode. Finally,toconstructarepre- temsinfourmetrics. Apartfromtheprimaryhead-
sentationofazerocoreferencemention,werepeat matchmetric,ourthreesubmissionsoutperformall
the empty node representation twice because the othersalsowhenevaluatedusingexactmatchand
emptynodeisboththefirstandthelasttokenofthe withsingletons. Whenconsideringpartialmatch,
mention. The coreference linking then proceeds theCorPipe-singleisoutperformedbyOndfa,as-
as before, just using a concatenation of surface suminglybecausethissubmissionslimitsthepre-
mentionsandzeromentions.
2Weonlymanagedtoevaluatea3-modelensemblebefore
The single-stage model is trained analogously
thesharedtaskdeadline,whileweusea5-modelensemble
to the two-stage model. The only differences are forthetwo-stagevariant.
5cs cs de de en en en hu hu no no
System Avg ca pced pdt cu parc pots gum litb parc es fr grc hbo kork szeg lt book nyno pl ru tr
73.90 82.2 74.8 77.2 61.6 69.5 71.8 75.7 79.6 68.9 82.5 68.2 71.3 72.0 63.2 70.0 75.8 79.8 78.0 78.5 83.2 68.2
CorPipe-2stage
1 2 1 1 1 3 2 1 1 2 1 2 1 1 2 1 1 1 1 1 1 1
72.75 81.0 73.7 75.8 60.7 71.7 71.5 74.6 79.1 69.8 81.0 68.8 68.5 70.9 60.3 68.1 75.8 79.5 77.5 77.0 83.1 59.4
CorPipe
2 3 2 2 2 1 3 2 2 1 3 1 2 2 3 3 2 2 2 2 2 3
70.18 80.4 72.8 74.8 57.1 61.6 67.0 74.4 78.1 58.6 79.8 67.9 66.0 67.2 60.1 67.3 75.2 78.9 76.6 75.2 81.2 53.4
CorPipe-single
3 4 3 4 3 4 4 3 3 3 4 3 3 3 4 4 3 3 3 4 3 4
69.97 82.5 70.8 75.8 55.0 71.4 71.9 70.5 74.2 55.6 81.9 62.7 61.6 61.6 64.9 69.3 72.0 74.5 72.1 76.3 80.5 64.5
Ondfa 4 1 4 3 4 2 1 4 4 4 2 4 4 4 1 2 4 4 4 3 4 2
BASELINE† 53 5.16 68 5.3 64 5.1 63 5.8 24 5.5 47 5.2 55 5.6 63 5.2 63 5.5 33 6.1 69 5.6 53 5.6 28 5.8 24 6.6 35 5.1 54 5.5 62 5.0 65 5.0 63 5.7 66 5.2 65 5.8 44 5.0
33.38 34.8 32.9 30.9 22.5 23.1 45.9 35.5 46.6 32.7 37.8 36.3 25.9 38.0 23.5 33.9 42.7 37.9 35.7 27.2 47.8 9.7
DFKI-CorefGen 6 6 6 6 6 7 7 6 6 7 6 7 6 5 7 6 7 6 6 6 7 6
16.47 0.0 0.0 0.0 6.8 25.4 48.9 0.0 0.0 53.1 0.0 43.7 5.6 0.1 33.4 30.3 44.8 0.0 0.0 0.0 53.9 0.0
Ritwikmishra 7 7 7 7 7 6 6 7 7 5 7 6 7 7 6 7 6 7 7 7 6 7
Table2: OfficialresultsofCRAC2024SharedTaskonthetestset(CoNLLscorein%). Thesystem†isdescribed
inPražáketal.(2021);therestinNováketal.(2024).
ThesectionsC,D,andEofTable4comparethe
Head- Partial- Exact- WithSin-
System
match match match gletons individualcheckpointconfigurationsofthesingle-
73.90 72.19 69.86 75.65 stageandthetwo-stagemodels. Weobservethat
CorPipe-2stage
1 1 1 1 the effect of the two-stage model is 0.9–1.1 per-
72.75 70.30 68.36 74.65
CorPipe centpointincreaseinallcheckpointconfiguration.
2 2 2 2
70.18 68.02 66.07 71.96 Wehypothesizethattwofactorscontributetothe
CorPipe-single
3 4 3 3
betterperformanceofthetwo-stagevariant: first,
69.97 69.82 40.25 70.67
Ondfa 4 3 5 4 the empty node representation is computed by a
53.16 52.48 51.26 46.45 pretrainedencoder,allowingbettercontextualiza-
BASELINE
5 5 4 5
tionoftheemptynoderepresentation. Second,the
33.38 32.36 30.71 38.65
DFKI-CorefGen 6 6 6 6 mentionswithemptynodesarerepresentedinthe
16.47 16.65 14.16 15.42 originalform,i.e.,thementionscancontainanyse-
Ritwikmishra
7 7 7 7
quenceofinputwordsandemptynodes,whilethe
single-stagevariantrepresentzeromentionsalways
Table3: OfficialresultsofCRAC2024SharedTaskon
byasingleemptynode.
thetestsetwithvariousmetricsin%.
Itwouldbeinterestingtoevaluatethetwo-stage
variantusingthegoldemptynodesinsteadofpre-
dictedmentionsjusttotheirheads,whichslightly
dictedemptynodes,toquantifythedecreaseofthe
improvespartialmatchbutseverelydeterioratesthe
scorecausedbyemptynodepredictionerrors. Un-
exactmatch.
fortunately,suchanevaluationisnotsupportedby
thesharedtaskevaluationplatform. Nevertheless,
6 AblationsExperiments
Table4.Fatleastshowsthatsuchadifferencefor
6.1 CurefUD1.2 theprovidedbaselinecoreferencesystem(Pražák
et al., 2021) is 1.4 percent points, as reported by
Table 4 contains quantitative analysis of ablation
thesharedtaskorganizers.
experiments on the CorefUD 1.2 test set. In Ta-
ble 4.A, we compare the three configurations of Finally, meaningful comparison of the shared
thesingle-stagemodelvariant. Selectingthebest- task results between this year and the last year is
performingcheckpointforeverycorpusincreases very difficult to carry out. While many corpora
theoverallscoreby1.4percentpoints,whilemak- havechangedonlymarginallyandtheevaluation
ingthemodelupto21timeslarger. Furtheraddi- metric is the same (so the results are reasonably
tionofensemblingimprovesthescorebyanother comparable),othercorporahavechangedsubstan-
1.2percentpoints. tially(especiallyPolishandTurkish). Evenso,we
The same comparison is available also for the provide numerical comparison of this year’s and
two-stagemodelvariantinTable4.B.Weobservea last year’s best systems in Table 4.G. This year’s
similartrendof1.2percentpointsincreaseforthe resultsareslightlyworsethaninthelastyear,on
bestper-corpuscheckpointandfurther1.4percent averageby0.65percentpoints,butthedifference
pointsincreaseduringensembling. isquitecomparabletotheeffectofpredicted/gold
6cs cs de de en en en hu hu no no
System Avg ca cu es fr grc hbo lt pl ru tr
pced pdt parc pots gum litb parc korkszeg booknyno
A)CORPIPESINGLE-STAGEVARIANTS
Singlemodel 70.18 80.4 72.8 74.8 57.1 61.6 67.0 74.4 78.1 58.6 79.8 67.9 66.0 67.2 60.1 67.3 75.2 78.9 76.6 75.2 81.2 53.4
Per-corpusbest +1.42 –0.4 –0.6 –0.2 +2.5 +7.2 +2.7 –0.4 –0.6 +10.4 –0.0 –0.3 +1.0 +1.5 +2.5 –1.6 +0.9 –0.4 +0.9 –0.2 –0.2 +5.1
Per-corpusensemble +2.62 +0.6 +0.9 +1.0 +3.6 +10.1 +4.5 +0.2 +1.0 +11.2 +1.2 +0.9 +2.5 +3.7 +0.2 +0.8 +0.6 +0.6 +0.9 +1.8 +1.9 +6.0
B)CORPIPETWO-STAGEVARIANTS
Singlemodel 71.32 81.0 74.2 75.9 56.7 64.7 66.4 74.7 78.2 57.9 81.2 67.2 67.6 64.2 61.6 67.9 77.7 77.6 77.3 77.4 81.3 67.0
Per-corpusbest +1.18 +0.1 +0.4 +0.3 +3.7 +4.9 +0.6 –1.2 +0.5 +10.2 +0.7 –0.2 +1.3 +5.6 –0.2 –0.6 –4.2 +2.2 +0.4 +0.5 –0.1 +0.2
Per-corpusensemble +2.58 +1.2 +0.6 +1.3 +4.9 +4.8 +5.4 +1.0 +1.4 +11.1 +1.3 +1.0 +3.7 +7.8 +1.6 +2.1 –1.9 +2.2 +0.7 +1.1 +1.9 +1.2
C)COMPARISONOFSINGLE-MODELVARIANTS
Single-stage 70.18 80.4 72.8 74.8 57.1 61.6 67.0 74.4 78.1 58.6 79.8 67.9 66.0 67.2 60.1 67.3 75.2 78.9 76.6 75.2 81.2 53.4
Two-stage +1.12 +0.6 +1.4 +1.1 –0.4 +3.1 –0.6 +0.3 +0.1 –0.7 +1.5 –0.7 +1.6 –3.0 +1.5 +0.6 +2.5 –1.3 +0.7 +2.2 +0.1 +13.6
D)COMPARISONOFPER-CORPUSBESTVARIANTS
Single-stage 71.59 80.0 72.2 74.6 59.6 68.8 69.7 74.0 77.5 69.0 79.7 67.6 67.0 68.7 62.6 65.7 76.1 78.5 77.5 75.0 81.0 58.5
Two-stage +0.91 +1.1 +2.4 +1.6 +0.8 +0.8 –2.7 –0.5 +1.2 –0.9 +2.2 –0.6 +1.9 +1.1 –1.2 +1.6 –2.6 +1.3 +0.2 +2.9 +0.2 +8.8
E)COMPARISONOFPER-CORPUSENSEMBLEVARIANTS
Single-stage 72.75 81.0 73.7 75.8 60.7 71.7 71.5 74.6 79.1 69.8 81.0 68.8 68.5 70.9 60.3 68.1 75.8 79.5 77.5 77.0 83.1 59.4
Two-stage +1.15 +1.2 +1.1 +1.4 +0.9 –2.2 +0.3 +1.1 +0.5 –0.8 +1.5 –0.6 +2.8 +1.1 +2.9 +1.9 +0.0 +0.2 +0.5 +1.5 +0.1 +8.8
F)COMPARISONOFTHEBASELINESYSTEMWITHGOLDANDPREDICTEDEMPTYNODES
Predictedemptynodes 53.16 68.3 64.1 63.8 24.5 47.2 55.6 63.2 63.5 33.1 69.6 53.6 28.8 24.6 35.1 54.5 62.0 65.0 63.7 66.2 65.8 44.0
Goldemptynodes +1.44 +1.3 +4.8 +2.4 +3.1 0.0 0.0 0.0 0.0 0.0 +1.0 0.0 +3.1 0.0 +6.5 +0.1 0.0 0.0 0.0 +0.8 0.0 +7.2
G)COMPARISONOFTHECORPIPE-2STAGEENSEMBLESYSTEMANDTHECRAC23BESTRESULTS
CorPipe-2stage,ensemble 74.55 82.2 74.8 77.2 — 69.5 71.8 75.7 — 68.9 82.5 68.2 — — 63.2 70.0 75.8 79.8 78.0 78.5 83.2 68.2
CorPipe23,CRAC23 +0.65 +1.0 +4.5 +2.3 — +1.5 +0.0 +0.8 — +2.1 +1.0 +0.4 — — +6.3 +0.8 +0.6 –0.2 +1.0 +1.3 –0.6 –11.7
Table4: AblationsexperimentsontheCorefUD1.2testset(CoNLLscorein%).
#model ∅,ELMO, largePLM xlPLM xxlPLM
Paper Model
calls basePLM ∼350M ∼3B ∼11B
(Leeetal.,2017) e2e 1 67.2∅
(Leeetal.,2018) e2e 1 70.4
ELMO
(Leeetal.,2018) c2f 1 73.0
ELMO
(Joshietal.,2019) c2f 1 73.9 76.9
BERT BERT
(Joshietal.,2020) c2f 1 79.6
SpanBERT
(Kirstainetal.,2021) s2e 1 80.3
Longformer
(Otmazginetal.,2023) s2e/LingMess1 81.4+additionalannotations
Longformer
(Dobrovolskii,2021) WL 1 81.0
RoBERTa
(D’Oosterlincketal.,2023) WL/CAW 1 81.6
RoBERTa
(Liuetal.,2022) ASP O(n) 76.6 79.3 82.3 82.5
T5 T5 T0 FlanT5
(Bohnetetal.,2023) seq2seq O(n) 78.0dev 83.3
mT5 mT5
(Wuetal.,2020) CorefQA O(n) 79.9+QAdata 83.1+QAdata
SpanBERT SpanBERT
Thispaper CorPipe 1 80.7 82.0
T5 FlanT5
Thispaper CorPipe 1 77.2 78.9
mT5 mT5
Table5: ComparisonofCorPipeandothermodelsonOntoNotes,usingpretrainedmodelsofvarioussize.
emptynodesonthebaselinesystem(cf. Table4.F). ing setup, with the two exceptions: we also con-
siderpretrainedEnglish-specificencodersT5(Raf-
6.2 OntoNotes feletal.,2020)andFlan-T5(Chungetal.,2024),
To compare the performance of the CorPipe ar- andweconsiderlargersegmentsizeduringtraining
chitecture to English state-of-the-art models, we (upto1536subwords).
trainalsomodelsontheOntoNotesdataset(Prad- TheresultsarepresentedinTable5. Inthelarge-
han et al., 2013). The dataset does not contain sizedsetting,CorPipeoutperformsallmodelsex-
any empty nodes, so we use the last year’s train- cept models utilizing additional data (Otmazgin
7et al., 2023; Wu et al., 2020) and models utiliz- References
ingtheword-levelapproach(Dobrovolskii,2021;
BerndBohnet,ChrisAlberti,andMichaelCollins.2023.
D’Oosterlinck et al., 2023).3 In the xl-sized set- Coreferenceresolutionthroughaseq2seqtransition-
tings, our model is 0.3 percent points below the based system. Transactions of the Association for
ComputationalLinguistics,11:212–226.
state of the art of Liu et al. (2022); notably, Cor-
PipeoutperformsthestateoftheartsystemBohnet ZewenChi, LiDong, FuruWei, NanYang, Saksham
etal.(2023)andalllarge-sizedmodelsnotusing Singhal,WenhuiWang,XiaSong,Xian-LingMao,
additionaltrainingdata. Unfortunately,wedidnot HeyanHuang,andMingZhou.2021. InfoXLM:An
information-theoretic framework for cross-lingual
havetheresourcestotrainanxxl-sizedmodel.
languagemodelpre-training. InProceedingsofthe
2021ConferenceoftheNorthAmericanChapterof
7 Conclusions
theAssociationforComputationalLinguistics: Hu-
manLanguageTechnologies,pages3576–3588,On-
WepresentedCorPipe24,thewinningentrytothe line.AssociationforComputationalLinguistics.
CRAC 2024 Shared Task on Multilingual Coref-
HyungWonChung,LeHou,ShayneLongpre,Barret
erence Resolution (Novák et al., 2024). Our sys-
Zoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi
temhastwovariants,eitherfirstpredictingempty Wang, Mostafa Dehghani, Siddhartha Brahma, Al-
nodesusingapretrainedlanguageencodermodel bert Webson, Shixiang Shane Gu, Zhuyun Dai,
and then performing coreference resolution em- MiracSuzgun,XinyunChen,AakankshaChowdh-
ery,AlexCastro-Ros,MariePellat,KevinRobinson,
ployinganotherpretrainedmodel,orpredictingthe
DashaValter,SharanNarang,GauravMishra,Adams
empty nodes jointly with mention detection and
Yu, Vincent Zhao, Yanping Huang, Andrew Dai,
coreference linking. Both variants surpass other HongkunYu,SlavPetrov,EdH.Chi,JeffDean,Ja-
participants by a large margin of 3.9 and 2.8 per- cobDevlin,AdamRoberts,DennyZhou,QuocV.Le,
andJasonWei.2024. Scalinginstruction-finetuned
centpoints,respectively. Thesourcecodeandthe
languagemodels. JournalofMachineLearningRe-
trainedmodelareavailableathttps://github.com/
search,25(70):1–53.
ufal/crac2024-corpipe.
AlexisConneau,KartikayKhandelwal,NamanGoyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
Acknowledgements
Guzmán, Edouard Grave, Myle Ott, Luke Zettle-
moyer,andVeselinStoyanov.2020. Unsupervised
ThisworkhasbeensupportedbytheGrantAgency
cross-lingualrepresentationlearningatscale. InPro-
of the Czech Republic, project EXPRO LUSyD
ceedings of the 58th Annual Meeting of the Asso-
(GX20-16819X),andhasbeenusingdataprovided ciationforComputationalLinguistics,pages8440–
by the LINDAT/CLARIAH-CZ Research Infras- 8451, Online. Association for Computational Lin-
guistics.
tructure(https://lindat.cz)oftheMinistryofEd-
ucation, YouthandSportsoftheCzechRepublic VladimirDobrovolskii.2021. Word-levelcoreference
(ProjectNo. LM2023062). resolution. InProceedingsofthe2021Conference
onEmpiricalMethodsinNaturalLanguageProcess-
ing,pages7670–7675,OnlineandPuntaCana,Do-
Limitations
minican Republic. Association for Computational
Linguistics.
Thepresentedsystemhasdemonstrateditsperfor-
manceonlyonalimitedsetof15languages,and KarelD’Oosterlinck,SemereKirosBitew,BrandonPa-
heavilydependsonalargepretrainedmodel,tran- pineau,ChristopherPotts,ThomasDemeester,and
Chris Develder. 2023. CAW-coref: Conjunction-
sitivelyreceivingitslimitationsandbiases.
awareword-levelcoreferenceresolution. InProceed-
Training with the mT5-large pretrained model
ingsofTheSixthWorkshoponComputationalMod-
requiresa40GBGPU,whichweconsiderafford- elsofReference,AnaphoraandCoreference(CRAC
able;however,trainingwiththemT5-xlpretrained 2023),pages8–14,Singapore.AssociationforCom-
putationalLinguistics.
modelneedsnearlyfourtimesasmuchGPUmem-
ory. MandarJoshi,DanqiChen,YinhanLiu,DanielS.Weld,
Luke Zettlemoyer, and Omer Levy. 2020. Span-
BERT:Improvingpre-trainingbyrepresentingand
predictingspans. TransactionsoftheAssociationfor
3Weareofcoursecurioustofindouthowtheword-level ComputationalLinguistics,8:64–77.
approachworksontheCorefUDdataset. Nevertheless,we
hypothesizethatonsomeoftheCorefUDcorporaitmight Mandar Joshi, Omer Levy, Luke Zettlemoyer, and
notworkwell,becausethementionheadsinthesecorporaare Daniel Weld. 2019. BERT for coreference reso-
considerablylessuniquethaninOntoNotes. lution: Baselines and analysis. In Proceedings of
8the2019ConferenceonEmpiricalMethodsinNatu- ShonOtmazgin,ArieCattan,andYoavGoldberg.2023.
ralLanguageProcessingandthe9thInternational LingMess: Linguisticallyinformedmultiexpertscor-
JointConferenceonNaturalLanguageProcessing ersforcoreferenceresolution. InProceedingsofthe
(EMNLP-IJCNLP),pages5803–5808,HongKong, 17thConferenceoftheEuropeanChapteroftheAs-
China.AssociationforComputationalLinguistics. sociationforComputationalLinguistics,pages2752–
2760,Dubrovnik,Croatia.AssociationforComputa-
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A tionalLinguistics.
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
Martin Popel, Michal Novák, Zdeneˇk Žabokrtský,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
DanielZeman,AnnaNedoluzhko,KutayAcar,David
ConferenceTrackProceedings.
Bamman, PeterBourgonje, SilvieCinková, Hanne
YuvalKirstain,OriRam,andOmerLevy.2021. Coref- Eckhoff,Güls¸enCebirog˘luEryig˘it,JanHajicˇ,Chris-
erence resolution without span representations. In tian Hardmeier, Dag Haug, Tollef Jørgensen, An-
Proceedingsofthe59thAnnualMeetingoftheAsso- dre Kåsen, Pauline Krielke, Frédéric Landragin,
ciationforComputationalLinguisticsandthe11th Ekaterina Lapshinova-Koltunski, Petter Mæhlum,
InternationalJointConferenceonNaturalLanguage M. Antònia Martí, Marie Mikulová, Anders Nøk-
Processing(Volume2: ShortPapers),pages14–19, lestad, Maciej Ogrodniczuk, Lilja Øvrelid, Tug˘ba
Online.AssociationforComputationalLinguistics. Pamay Arslan, Marta Recasens, Per Erik Solberg,
ManfredStede,MilanStraka,DanielSwanson,Svet-
KentonLee,LuhengHe,MikeLewis,andLukeZettle- lanaToldova,NoémiVadász,ErikVelldal,Veronika
moyer. 2017. End-to-end neural coreference reso- Vincze,AmirZeldes,andVoldemarasŽitkus.2024.
lution. In Proceedings of the 2017 Conference on Coreferenceinuniversaldependencies1.2(CorefUD
EmpiricalMethodsinNaturalLanguageProcessing, 1.2). LINDAT/CLARIAH-CZdigitallibraryatthe
pages188–197,Copenhagen,Denmark.Association InstituteofFormalandAppliedLinguistics(ÚFAL),
forComputationalLinguistics. FacultyofMathematicsandPhysics,CharlesUniver-
sity.
KentonLee,LuhengHe,andLukeZettlemoyer.2018.
Higher-ordercoreferenceresolutionwithcoarse-to-
SameerPradhan,AlessandroMoschitti,NianwenXue,
fineinference. InProceedingsofthe2018Confer-
HweeTouNg,AndersBjörkelund,OlgaUryupina,
enceoftheNorthAmericanChapteroftheAssoci-
Yuchen Zhang, and Zhi Zhong. 2013. Towards ro-
ation for Computational Linguistics: Human Lan-
bustlinguisticanalysisusingOntoNotes. InProceed-
guageTechnologies,Volume2(ShortPapers),pages
ingsoftheSeventeenthConferenceonComputational
687–692,NewOrleans,Louisiana.Associationfor
NaturalLanguageLearning,pages143–152,Sofia,
ComputationalLinguistics.
Bulgaria.AssociationforComputationalLinguistics.
TianyuLiu, YuchenEleanorJiang, NicholasMonath,
RyanCotterell,andMrinmayaSachan.2022. Autore- Ondˇrej Pražák and Miloslav Konopik. 2022. End-to-
gressivestructuredpredictionwithlanguagemodels. endmultilingualcoreferenceresolutionwithmention
In Findings of the Association for Computational headprediction. InProceedingsoftheCRAC2022
Linguistics: EMNLP 2022, pages 993–1005, Abu SharedTaskonMultilingualCoreferenceResolution,
Dhabi,UnitedArabEmirates.AssociationforCom- pages23–27,Gyeongju,RepublicofKorea.Associa-
putationalLinguistics. tionforComputationalLinguistics.
Ilya Loshchilov and Frank Hutter. 2017. SGDR:
OndˇrejPražák,MiloslavKonopík,andJakubSido.2021.
Stochasticgradientdescentwithwarmrestarts. InIn-
Multilingualcoreferenceresolutionwithharmonized
ternationalConferenceonLearningRepresentations.
annotations. InProceedingsoftheInternationalCon-
ference on Recent Advances in Natural Language
JoakimNivre,Marie-CatherinedeMarneffe,FilipGin-
Processing(RANLP2021),pages1119–1123,Held
ter, Jan Hajicˇ, Christopher D. Manning, Sampo
Online.INCOMALtd.
Pyysalo, Sebastian Schuster, Francis Tyers, and
Daniel Zeman. 2020. Universal Dependencies v2:
Colin Raffel, Noam Shazeer, Adam Roberts, Kather-
Anevergrowingmultilingualtreebankcollection. In
ProceedingsoftheTwelfthLanguageResourcesand ine Lee, Sharan Narang, Michael Matena, Yanqi
EvaluationConference,pages4034–4043,Marseille, Zhou,WeiLi,andPeterJ.Liu.2020. Exploringthe
limitsoftransferlearningwithaunifiedtext-to-text
France.EuropeanLanguageResourcesAssociation.
transformer. JournalofMachineLearningResearch,
MichalNovák,BarboraDohnalová,MiloslavKonopík, 21(140):1–67.
Anna Nedoluzhko, Martin Popel, Ondˇrej Pražák,
JakubSido,MilanStraka,ZdeneˇkŽabokrtský,and Noam Shazeer and Mitchell Stern. 2018. Adafactor:
DanielZeman.2024. FindingsoftheThirdShared Adaptive Learning Rates with Sublinear Memory
Task on Multilingual Coreference Resolution. In Cost. InProceedingsofthe35thInternationalCon-
ProceedingsofTheSeventhWorkshoponComputa- ference on Machine Learning, ICML 2018, Stock-
tionalModelsofReference,AnaphoraandCorefer- holmsmässan,Stockholm,Sweden,July10-15,2018,
ence(CRAC2024),Miami,Florida,USA.Associa- volume80ofProceedingsofMachineLearningRe-
tionforComputationalLinguistics. search,pages4603–4611.PMLR.
9Milan Straka. 2023. ÚFAL CorPipe at CRAC 2023:
Largercontextimprovesmultilingualcoreferenceres-
olution. InProceedingsoftheCRAC2023Shared
TaskonMultilingualCoreferenceResolution,pages
41–51, Singapore. Association for Computational
Linguistics.
MilanStrakaandJanaStraková.2022. ÚFALCorPipe
atCRAC2022:Effectivityofmultilingualmodelsfor
coreferenceresolution. InProceedingsoftheCRAC
2022SharedTaskonMultilingualCoreferenceRes-
olution,pages28–37,Gyeongju,RepublicofKorea.
AssociationforComputationalLinguistics.
WeiWu,FeiWang,AriannaYuan,FeiWu,andJiwei
Li.2020. CorefQA:Coreferenceresolutionasquery-
based span prediction. In Proceedings of the 58th
AnnualMeetingoftheAssociationforComputational
Linguistics,pages6953–6963,Online.Association
forComputationalLinguistics.
LintingXue,NoahConstant,AdamRoberts,MihirKale,
RamiAl-Rfou,AdityaSiddhant,AdityaBarua,and
ColinRaffel.2021. mT5: Amassivelymultilingual
pre-trainedtext-to-texttransformer. InProceedings
ofthe2021ConferenceoftheNorthAmericanChap-
teroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies,pages483–498,On-
line.AssociationforComputationalLinguistics.
Zdeneˇk Žabokrtský, Miloslav Konopik, Anna
Nedoluzhko, Michal Novák, Maciej Ogrodniczuk,
MartinPopel,OndrejPrazak,JakubSido,andDaniel
Zeman.2023. Findingsofthesecondsharedtaskon
multilingualcoreferenceresolution. InProceedings
of the CRAC 2023 Shared Task on Multilingual
Coreference Resolution, pages 1–18, Singapore.
AssociationforComputationalLinguistics.
Zdeneˇk Žabokrtský, Miloslav Konopík, Anna
Nedoluzhko, Michal Novák, Maciej Ogrodniczuk,
Martin Popel, Ondˇrej Pražák, Jakub Sido, Daniel
Zeman, and Yilun Zhu. 2022. Findings of the
shared task on multilingual coreference resolution.
In Proceedings of the CRAC 2022 Shared Task on
Multilingual Coreference Resolution, pages 1–17,
Gyeongju, Republic of Korea. Association for
ComputationalLinguistics.
10