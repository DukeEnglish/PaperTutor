Preprint. Underreview.
LARGE LANGUAGE MODELS AS MARKOV CHAINS
OussamaZekri* AmbroiseOdonnat*
ENSParis-Saclay⋄ HuaweiNoah’sArkLab,Inria†
AbdelhakimBenechehab LinusBleistein NicolasBoulle´
HuaweiNoah’sArkLab,Eurecom InriaParis‡ ImperialCollegeLondon
IevgenRedko*
HuaweiNoah’sArkLab
ABSTRACT
Largelanguagemodels(LLMs)haveproventoberemarkablyefficient,bothacross
awiderangeofnaturallanguageprocessingtasksandwellbeyondthem. However,
acomprehensivetheoreticalanalysisoftheoriginsoftheirimpressiveperformance
remainselusive. Inthispaper,weapproachthischallengingtaskbydrawingan
equivalencebetweengenericautoregressivelanguagemodelswithvocabularyof
sizeT andcontextwindowofsizeK andMarkovchainsdefinedonafinitestate
spaceofsize (TK). Wederiveseveralsurprisingfindingsrelatedtotheexistence
O
ofastationarydistributionofMarkovchainsthatcapturetheinferencepowerof
LLMs,theirspeedofconvergencetoit,andtheinfluenceofthetemperatureon
thelatter. Wethenprovepre-trainingandin-contextgeneralizationboundsand
showhowthedrawnequivalenceallowsustoenrichtheirinterpretation. Finally,
weillustrateourtheoreticalguaranteeswithexperimentsonseveralrecentLLMs
tohighlighthowtheycapturethebehaviorobservedinpractice.
1 INTRODUCTION
Thefieldsofmachinelearningandartificialintelligencehaverecentlyseensignificantprogresswith
theintroductionoflargelanguagemodels(LLMs)(Brownetal.,2020;Touvronetal.,2023a),built
onthetransformerarchitecture(Vaswanietal.,2017). Thesemodels,trainedonvastamountsofdata,
havebeenappliedinmanynaturallanguageprocessingtasks,includingmachinetranslation(Brown
etal.,2020),textgeneration,questionanswering(Robertsetal.,2020),andsentimentanalysis(Zhang
etal.,2023a). Althoughsuccessfulinpractice,theoriginsoftheimpressiveperformanceofLLMs
remainelusive,asthereisnowidelyacceptedagreementinthescientificcommunityonhowthey
achieveremarkablereasoningcapabilitiesthatgofarbeyondtheirtrainingdata(Brownetal.,2020).
Thisworktakesasteptowardsbridgingtheknowledgegapmentionedabovebyprovidinganexplicit
characterizationoftheLLM’sinferencecapabilities. Forthis,weadoptanintuitive,yetoverlooked,
approachthatinterpretsLLMsasMarkovchainsoperatingonafinitestatespaceofsequencesand
tokens(seeFig.1). AkeyinsightisthatdespitetheseeminginfinityofLLMsgeneratingcapacity,
they have a limited vocabulary and context window making all their possible input and output
sequencesenumerable. Weshowthatdespitetheprohibitivelylargesizeofthelatterset,itexhibits
astructurethatmakesitamenabletotheoreticalanalysis. Wefurthergeneralizerecenttheoretical
advancesonthegeneralizationofLLMsandleverageourproposedpointofviewtoprovideamore
insightfulinterpretationofthem.
Markov chains and Large Language Models. While none of the prior works considered the
equivalencebetweenautoregressivemodelsandMarkovchainspresentedinthiswork1,someused
theMarkoviangenerativeprocesstobetterunderstandtheintrinsiccapabilitiesofthetransformer
*Equalcontribution.Mailtooussama.zekri@ens-paris-saclay.fr&ambroiseodonnattechnologie@gmail.com.
⋄WorkdonewhileatHuaweiNoah’sArkLab †Univ.Rennes2,CNRS,IRISA ‡Univ.Paris-Saclay,UEVE
1
4202
tcO
3
]LM.tats[
1v42720.0142:viXraPreprint. Underreview.
Transient class Recurrent class Zero
Figure1: LLMasaMarkovchain. AnautoregressivemodelwithvocabularysizeT andcontext
windowK isequivalenttoaMarkovchainwithasparseandblock-structuredtransitionmatrixof
size(cid:80) Ti (TK). ThelattercapturesallpossibleoutputsofagivenLLMforallpossible
i≤K ∼O
inputsequencesallowedbyitsvocabularyandcontextwindow.
architecture.Makkuvaetal.(2024)assumethattheinputdataisgeneratedbyanunknownhigh-
orderMarkovchaintoanalyzethelearningdynamicsoftheself-attentionmechanisminasingle-
layer single-head transformer. Similarly, Edelman et al. (2024) study in-context learning of a
transformermodeltrainedonsamplesdrawnfromabi-gramMarkovchain.Ildizetal.(2024)establish
an equivalence between context-conditioned Markov chains and the self-attention mechanism in
transformerstoshowthatself-attentionweightscanbelearned,undercertainconditions,byprompting
themodel. Incontrasttothispriorwork,weseektomodelanyautoregressivemodelregardlessofits
exactarchitectureasaMarkovchain.OuranalysisprovidesinsightsintoLLMsbeyondunderstanding
theself-attentionmechanisminsimplifiedtransformers.
In-ContextLearning(ICL). ICListheabilityofLLMstoadapttheirpredictionsduringinference
byleveragingexamplesorpromptsdirectlywithoutupdatingtheirparameters. Xieetal.(2022)
provideatheoreticalguaranteeforICLbyshowingitsequivalencetoimplicitBayesianinference,
whileJeonetal.(2024)studyICLintheBayesiansetupbyadoptingapointofviewthatrelates
ittometa-learningfromnon-independentandidenticallydistributed(i.i.d.) data. Theirguarantees
introduceadesirabledependenceonthelengthoftheinputsequenceandthenumberofsequences
seen,yettheirdatagenerativeprocessassumesthateachsequenceisoutputtedbyatransformermodel
drawnfromapriordistribution. Afollow-upworkbyWiesetal.(2024)generalizesthisanalysis
withinabroaderProbablyApproximatelyCorrect(PAC)frameworkandshedslightonthefew-shot
natureofICL.Lietal.(2023)studiedthegeneralizationerroroftrainedICLtransformersfroma
stabilityviewpointandprovidedgeneralizationguaranteesfortemporallydependentpromptsthatcan
beseenasMarkovchainsofdifferentorders. TheclosestworkonICLtoouranalysisis(Zhangetal.,
2023b)uponwhichweimprovebyrelaxingmanyassumptionsand,mostimportantly,byproviding
strongerinterpretationsandexperimentalobservations.
GeneralizationboundsforLLMs. Derivinggeneralizationboundsforneuralnetworksisinher-
entlydifficultduetothecomplexityoftheoperationsperformedbythemodel. Thiscanrequire
expressingitasacontinuousprocess(Marion,2023). ForLLMs,anavenuetoobtainsuchboundsis
torelyonthePAC-Bayesframework.Lotfietal.(2023;2024)leveragecompressiontechniquesin
combinationwithPAC-Bayestypeboundstoobtaintightgeneralizationboundsbothatthedocument
and token level. These works are connected to the existing literature on compressibility and the
intrinsicdimensionofneuralnetworks(Aghajanyanetal.,2020;Yarasetal.,2024)andtypically
focusonfine-tuningLLMswithLoRA(Huetal.,2022)inspiredadapters. TheworkofZhangetal.
(2023b)considerstheBayesianframeworktoderivegeneralizationboundsforpre-trainingwherethe
dataisassumedtobeaMarkovchain. Ourtheoreticalanalysisofpre-trainingisdonewithoutrelying
1Wenote,however,arecentblogpost(Nardo,2023)discussingasimilarideaonahighlevelwithoutanalyzing
itatthelevelofdetailsofthiswork.
2Preprint. Underreview.
onBayesianmodelingandunderrealisticdatageneratingassumptioncoveringmanycommontypes
ofdatausedtotrainLLMs.
Summaryofourcontributions. Ourmaincontributionsaresummarizedasfollows.
1) WeprovideanexplicitcharacterizationofLLM’sinferencemechanismbyshowingitsequivalence
toafinite-stateMarkovchain.Weanalyzethetransitionmatrixofthelatterandprovetheexistence
anduniquenessofitsstationarydistribution. Wegivearateofconvergencetothisdistribution
thatdependsonthevocabularyandcontextwindowsizes,andthemodel’stemperature.
2) Byleveragingconcentrationinequalitiesfordependentrandomvariables,weobtaingeneralization
bounds for LLMs in both pretraining and in-context inference. Our bounds are proved under
minimalassumptionsonthemodelanddataanddependonthemodel’sdepth,dictionary,and
datasetsizes, aswellastheintrinsicpropertiesofthetemporally-dependentsequencesitwas
trainedon. Wehighlighttheinsightsthatstemfromtheseboundsbyrelatingthemtotheminimax
boundsofMarkovchainlearning.
3) WeexperimentallyshowthatthemostrecentLLMsdatingfrom2023-2024obeythein-context
scalinglawspredictedbyourtheoreticalresults. OnehighlightisthatLLMsarebetterMarkov
chainslearnersthantheminimaxoptimalfrequentistapproach(Wolfer&Kontorovich,2019).
Organizationofthepaper. Section2providesbackgroundmaterialonautoregressivemodelsand
Markovchains. WeformalizeanequivalencebetweenthesetwomodelsinSection3andillustrateit
onatoyexample. InSection4,wederivegeneralizationboundsforLLMstrainedonnon-iiddata
andpromptedonMarkovchains. OurresultsareempiricallyverifiedinSection5.
2 BACKGROUND KNOWLEDGE
WerecallsomeelementaryfactsaboutMarkovchains(Roberts&Rosenthal,2004;Paulin,2015)
andLLMs. MorenotationsandbackgroundmaterialsareavailableinAppendicesAtoC.
Markov chains. Let Ω be a discrete finite set of size Ω. A discrete-time, time-homogeneous
| |
MarkovchainMC(Ω,Q)definedonastatespaceΩ= x |Ω| withtransitionmatrixQ R|Ω|×|Ω|
{ i }i=1 ∈
withentriesQ =Q(x ,x ) [0,1]isasequenceofrandomvariables(X ,X ,...)takingvalues
ij i j 1 2
inΩsuchthatforanyn
Na∈
nd(x ,...,x ) Ωn+1,wehave
1 n+1
∈ ∈
P(X =x X =x ,...,X =x )=P(X =x X =x )=:Q(x ,x ).
n+1 n+1 n n 1 1 n+1 n+1 n n n n+1
| |
A distribution π on Ω is said to be a stationary distribution if Qπ = π. Under mild conditions
on Q, MC(Ω,Q) has a unique stationary distribution to which it converges, i.e., for any x Ω,
lim d (Qn(x, ),π) = 0, where Qn(x, ) denotes the probability of X conditioned∈ on
n→∞ TV n
X =xandthetotalv· ariationbetweentwodistr· ibutionsPandQ,definedon(Ω, ),is
1
F
d (P,Q):= sup P(A) Q(A).
TV
| − |
A∈F
Werecallthatthemixingtimet (ε)ofaMarkovchainistheminimaltimeneededtobeε-close
mix
toitsstationarydistribution(seeDefinitionC.8). Intuitively,aMarkovchainmixesslowlywhen
itremainsclosetotheinitialstateafteragivennumberofstepsanddoesn’texploreitsstatespace.
AMarkovchainthatexhibitsafastmixingtimeonthecontraryquicklyforgetsitsinitialstateand
transitionsmoreeasilytoawidersetofstates.
Autoregressivemodels. Let denoteadictionaryofsizeT usedtoencodeanarbitrarysequence
V
intoasequenceofpredefinedtokensbelongingto .Weassumethatourmodeladmitsamaximumof
V
K tokensasinput,referredtoasthecontextwindowofthemodel. Thedomainoftheautoregressive
modelisthesetofallsequencesconsistingofelementsfrom withuptoK elements. Wedenote
thisby ∗,whichrepresentsarestrictionofKleeneclosureoV f ,i.e., ∗ := v ∗, v K
VK V VK { ∈V | |≤ }
with v thelengthofv. WedefineanautoregressivemodelwithtrainableparametersΘasafunction
fT,K| :| ∗ ∆( ),where∆( )istheprobabilitysimplexover ,thatgivenasequenceoftokens
Θ VK → V V V
voutputsaprobabilitydistributionoverthewholestatespaceindicatingthelikelihoodforeachof
itselementstoappearafterv (seeAppendixBformoredetails). Weconsiderasettingwherethe
3Preprint. Underreview.
learner’sobjectiveistoapproximatetheprobabilitiesofsequencesoveraninputvocabularygivenby
somereferencedistributionP : ( ∗) [0,1]2. Weunderlinethatwedonotspecifytheexact
internalstructureoffT,K,i.e.,L itcP anV rK ely→
onanyneuralnetworkthatusesasoftmaxfinallayer. We
Θ
usetheterms“autoregressivemodel”and“LLMs”interchangeably,andwewritef ,droppingthe
Θ
superscriptsT,K whentheyareofnoimportance.
3 AUTOREGRESSIVE MODELS AS MARKOV CHAINS
WeformallydefineaMarkovchainthatexplicitlycapturesthefullinferencecapacityofagiven
autoregressivemodelf . Webuilduponahigh-levelideathatassociatesatokenizedinputprompt
Θ
withastatev ,fromwhichwetransitiontoanewstatev = [v ,v]byconcatenatingthetokenv
i j i
predicted by an LLM to it. We then provide a theoretical characterization of this Markov chain
highlightingitsintriguingpropertiesandasymptoticbehavior.
3.1 MARKOVCHAINFORMALIZATION
WebeginbydefiningthetransitionmatrixassociatedwithanautoregressivemodelfT,K.
Θ
Proposition3.1. AnyautoregressivemodelfT,K canbeequivalentlyrepresentedbyaMarkov
Θ
chainMC( VK∗,Q f),withasparsetransitionmatrixQ
f
∈R|V K∗|×|V K∗|definedas:
(cid:40)
0, if l 1,..., v ,s.t. (v ) =(v ) ,
v ,v ∗, Q (v ,v )= ∃ ∈{ | i |} i l+1 ̸ j l
∀ i j ∈VK f i j fT,K(v ) , otherwise.
{ Θ i }j
Theproportionofnon-zeroelementsinQ is(T 1)/(TK 1).
f
− −
WediscusstheintuitionbehindthedefinitionofQ pro-
f
videdaboveandillustrateitinFigure2foracaseofT =2 Ini�al Next state
distribu�on distribu�on
andK =3. Forthis,wefirstnotethatgivenaninputse-
quence v of size v < K, a transition to any state v
i i j
| |
has a probability of 0 if v = [v ,v] for some v ,
j i
̸ ∈ V
i.e., if the state we transition to is not a concatenation
oftheinputsequencewithanadditionaltokenfromthe
vocabulary(forinstance,astate 0 cannottransitionto
{ }
1,0 inonestep). Applyingthisreasoningfordifferent Input
{ } prompt
valuesofk <K definesgreenrectangularblocksofsize
Tk Tk+1inthetransitionmatrixportrayedinFigure2.
Whe× nonereachesthebluesquareblockinthetransition Figure2: IllustrationofProposition3.1
matrix,theinputsequencereachesthemaximumcontext withT =2andK =3.
windowlengthv : themodelcannolongerappendtokenstotheinputsequenceandhastodelete
i
the first token from it to proceed. This blue block is of size TK TK: it captures transitions
×
betweenallpossiblesequencesofthemaximumadmissiblelength. Wedefinesimilarlythereference
transitionmatrixQ∗ofthelanguagewheretheprobabilityoftransitions fT,K(v ) arereplaced
byground-truthprobabilitiesP (v v ). InordertouseQ asf ,itisn{ owΘ suffici ie} nj ttodefinean
L j i f Θ
|
inputdistributionδ oftheMarkovchainbasedoninputpromptvasaone-hotencodingvectorof
0
size ∗ with1atthepositionofthestatecorrespondingtov. Then,thetransitiontothenextstate
|VK|
simplywritesasδ =Q δ . Theoutputoff (v)forindividualtokensin wouldthencorrespond
1 f 0 Θ
V
exactlytotheprobabilitiesinδ forstatesthatareconcatenationsofvwithT tokensfrom . This
1
V
processisillustratedinFigure2.
We now characterize this Markov chain and note that, since ∗ is finite, MC( ∗,Q ) admits a
VK VK f
stationary distribution. This stationary distribution is unique given the structure of the transition
matrixQ ,asestablishedinthefollowingresult.
f
2P(V∗)denotesthepowersetofV∗.
K K
4Preprint. Underreview.
Proposition 3.2. Let MC( ∗,Q ) be a Markov chain defined in Proposition 3.1. Then
MC( ∗,Q )isanergodicuV nK ichaif nandhasauniquestationarydistribution.
VK f
Aunichainisachainthathasatmostonerecurrentclassplussomeadditionaltransientstates. From
Proposition3.1,wenoteimmediatelythatgreenblocksinFig.1representtransientclasses,meaning
thatapplyingQ totheinputprompt,representedbyaone-hotencodingofsize ∗ ,willtransition
f |VK|
toastatethatcorrespondstoasequenceoflengthincreasedbyonewithanadditional,mostlikely,
tokenappendedtoit. Thisprocessisrepeatedifthemodeliscalledfurtheron: weappendtokens
untilwereachthecontextwindowlimitK. Atthispoint,wereachtherecurrentclass,representedin
blue,inwhichthechainstaysuntilitreachesitsuniquestationarydistribution. Wenowcharacterize
howmanytimesoneshouldapplyQ totheinputtoreachthestationarydistribution.
f
Proposition3.3. Givenanergodicfinite-stateunichainMC( ∗,Q )ande=(1,1,...,1)⊤,
thenlim Qn =eπwhereπisthestationarydistributionV ofK therf ecurrentclassR ofstates,
n→∞ f
expandedby0’sforeachtransientstateoftheunichain. Moreover,foralln K,
≥
|(Qn f)
i,j
−(eπ)
i,j
|≤(1 −2ε)⌊ Kn⌋−1,
whereε= min (QK) >0.
i,j∈R2{ f i,j }
Thestationarydistributionisthelong-termequilibriumoftheMarkovchaindefinedbytheLLM
andcanbeinterpretedasaproxyofitsunderstandingofnaturallanguageinitstokenspace. Itis
independentoftheinitialstate(i.e.,inputprompt)butrathercapturestheabsolutefrequenciesof
occurrencesofcertaintokensseenduringpre-training. Forawell-performingmodel, itishence
likelytobeheavy-tailed,meaningthatrarestateshaveanon-zeroprobabilityofoccurringdueto
language’sambiguityandcomplexity. Proposition3.3showsthatreachingthestationarydistribution
requiresmoregenerationstepsformodelswithlargercontextwindowK. Additionally,convergence
dependsonε(thatis,thesmallestelementoftheKthpowerofthetransitionmatrix),whichisrelated
totheabilityofthechaintoexplorethestatespaceafterhavingforgottentheinputprompt.
(a) (b) (c) Convergence speed to
the sta�onary distribu�on
Figure3:MarkovchainwithasmallGPT-likemodel. (a)TransitionmatrixQ ofthemodelwhere□
f
denotestheexamplesfromthetrainingset. (b)Stationarydistributionofthetrainedmodelassigning
almostuniformprobabilitiestothestatesseenduringtraining. (c)Convergenceratetothestationary
distributionfortheconsideredtoymodelalongwiththreeLLMs,highlightingthedependenceonK.
They-axisistheupperboundinProposition3.3.
3.2 ILLUSTRATIONONATOYMODEL
We illustrate the results of Section 3 on a toy model trained on a sequence of 0s and 1s. Here,
eachsubsequenttokenis0ifthesumofthreeprevioustokensisevenand0otherwise. Therefore,
T =2andk =3. Wegenerateasequenceof40digits,resultingin37distinctsupervisedexamples,
andtrainasmall“GPT-like”model(Karpathy,2023)onit. Weextractthelogitsfromthemodel
bypromptingitwithallpossiblecombinationsof0sand1soflengthlessthanthreetoobtainthe
transitionmatrixQ R14×14depictedinFig.3(a). Thetransitionmatrix’sstructure(e.g.,presence
f
∈
oftransientandrecurrentclasses)matchestheonepresentedinFig.1.Fig.3(b)displaysthestationary
5Preprint. Underreview.
distributionofthetrainedmodelobtainedbyraisingQ topower105. Wenotethatithasastrong
f
biastowardseentrainingsamplesinaccordancewithourintuitionbehindthestationarydistribution
presentedearlier. Finally,Fig.3(c)illustratestheconvergencerateofthetoymodel,predictedby
Proposition3.3,andcomparesittomodelswithlargerdictionarysizeT andcontextwindowK. In
Fig.3(d),wesetε=10−6andnotethatthisparameterreflectstheabilityoftheLLMtoexplorethe
statespace.
Temperature =0.2 TTeemmppeerraattuurree ==11 Temperature =2
(a) (b) (c) (d)
Temperature
Figure4: Dependenceofεonthetemperatureofthemodel. (a)Forlowtemperatures,εbecomestoo
smalltoachieveconvergencetothestationarydistribution. (b)-(c)Increasingthetemperaturefrom1
to2leadstoa 10fasterconvergence. (d)ε(log-scale)increasefortemperaturevaluesin[0.1,2].
×
Roleofthetemperature. Tobetterillustratetheroleofε,wenowplotthetransitionmatrixofthe
studiedMarkovchainobtainedwhenapplyingdifferenttemperaturescalingtothelogitsreturned
bythetrainedmodel. AsthetemperatureiscommonlylinkedtotheabilityofLLMstotransition
morefreelytoalargesetofstates(Chen&Ding,2023),weexpectthatlowertemperaturesshould
impactnegativelythespeedoftheconvergencetothestationarydistribution. InFig.4(a),weshow
thatforalowtemperature(0.2),theMarkovchainmixesslowlyandisunabletoreachitsstationary
distribution(samelineinthetransitionmatrixasinFig.3(c))evenafter106 steps. Inthecaseof
a more commonly used temperature equal to 1 (Fig. 4(b)), the model requires only 300 steps to
converge. Finally,settingthemodel’stemperatureto2(Fig.4(c))makestheconvergenceextremely
fast,reachingthestationarydistributionafteronly30steps. Theinterplaybetweenεandthemodel’s
temperatureisdisplayedinFig.4(d),increasingthetemperatureleadstoadrasticimprovementin
theconvergencespeed.
4 GENERALIZATION BOUNDS FOR LARGE LANGUAGE MODELS
Theinferenceofanyautoregressivelanguagemodelf canbefullycapturedbyaMarkovchain
Θ
with a finite transition kernel Q defined as above. This, in turn, allows us to see and study the
f
generalization of f as its capacity to infer correctly all the elements of Q that approximate a
Θ f
true reference matrix of transition probabilities Q∗. The hardness of this task lies in achieving
preciseinferencehavingobservedanegligibleamountofQ∗’selementsduringitspre-training. For
GPT-3(Brownetal.,2020),thisrepresents5 1011trainingtokens,whichpalesincomparisonwith
thenumberofnon-zeroelementsinQ ,give× nbyTK+1 109632.
f
≈
Risk definition. We denote by X = (X ,...,X ) the tokens in that f observes (e.g.,
1 N Θ
V
during pre-training or at inference time). The training sequences of tokens can be written as
S = (X ,...,X ) if n K and S = (X ,...,X ) otherwise due to the deletion
n 1 n n n−K+1 n
process(seeDefinitionB.2)≤ . Inparticular,theS areelementsof ∗. Foranyn [N],thetrue
probabilityofnexttokenX givenapastsequn enceS isdefineV dK asP ( S )∈ ∆ andthe
n+1 n L n T
probabilityestimatedbythemodelwritesP ( S ). Weassumetheexistenc· e| ofaco∈ nstantc >0
Θ n 0
suchthatforanyn [N]and(x ,...,x )·| Ωn+1,
1 n+1
∈ ∈
P (X =x X =x ,...,X =x ) c >0. (1)
L n+1 n+1 n n 1 1 0
| ≥
Thisisacommonassumptionusedpreviouslyin(Zhangetal.,2023b;Huetal.,2024;Xieetal.,
2022;Wiesetal.,2024). Toassessthegeneralizationofagivenmodel,werespectivelydefinethe
6Preprint. Underreview.
theoreticalandempiricalrisksforanyΘ as3
∈W
N
1 (cid:88)
(Θ):=E[(cid:98)(Θ)], (cid:98)(Θ):= d TV(P L( S n),P Θ( S n)), (2)
R R R N ·| ·|
n=1
Thegeneralizationproblemconsistsofboundingthedifference (Θ) (cid:98)(Θ).
R −R
Remark 4.1 (Choice of risk). Our risk definition departs from usual generalization bounds in
statisticallearningwhererisksaremostlyderivedfromempiricalriskminimization(Redkoetal.,
2019;Vapnik,1999;Bach,2024;Marion,2023). Aswewanttoassesshowwellthemodelestimates
theprobabilitydistributionofthenexttoken,weratherfollow(Zhangetal.,2023b;Huetal.,2024)
andthelearningandidentitytestingofMarkovchainsliterature(Wolfer&Kontorovich,2023;2019)
andusethetotalvariationdistance.
Transformermodel. Withoutlossofgenerality,f isassumedtobeatransformermodelwithL
Θ
layersandH heads,consistingofalternatingmulti-headattention(MHA)andfeed-forwardblocks
(moredetailsin AppendixB).ThefirstlayerreceivesaninputS(0) =Sembeddedinr-dimensional
space. Toobtainaprobabilitydistributiononthevocabulary ,theoutputS(L) Rr×T ofthefinal
layerisprojectedbacktothevocabularysizebyan“unembedV
dinglayer”W
∈RT×r
andaveraged
U
overthecolumnstoobtainavectorinRT. Asoftmaxlayerisfinallyappliedto∈ obtaintheprobability
distribution of the next token P ( S) := P ( S) = softmax(cid:0) 1 W S(L)1 (cid:1) ∆ , where
Θ ·| Θ ·| nτ U n ∈ T
Θdenotestheparametersoftheentirenetworkandτ isthesoftmaxtemperature(Hinton,2015).
Unless otherwise specified, we assume that the unembedding layer is bounded. The classes of
parametersandneuralnetworksitgeneratesrespectivelywrite = Θs.t. W⊤ B and
W { ∥
U∥2,1
≤
U
}
= f s.t. Θ .
Θ
F { ∈W}
4.1 PRE-TRAININGTHEORETICALANALYSIS
We now significantly extend the scope of our theoretical contributions by assuming that the pre-
trainingdataS =(S ,...,S )isasequenceofdependentrandomvariableswithamildcoupling
1 Ntrain
structure,namelythataMartoncouplingwithmixingmatrixΓexistsforS = (S ,...,S ).4
1 Ntrain
Thisensuresoursettingremainsverybroadasitsubsumesthecaseofindependentvariables,m-
dependentvariables,languagebigrams(Biettietal.,2023),andtheMarkovchainsettingconsidered
instate-of-the-artICLanalysisofautoregressivemodels(Zhangetal.,2023b;Huetal.,2024).
Generalizationbound. Wedenotetherisksby pre(Θ)and (cid:98)pre(Θ)toindicatethatwetake
R R
N =N inEq.(2). Below,westateourmainresult,whoseproofisdeferredtoAppendixD.4,
train
whichprovidesageneralizationboundontheestimationriskofpre-training.
Theorem4.1(Pre-traininggeneralizationbound). ConsideranLLMf . Wedenoteby
Θ
∈F
Γthemixingmatrixofthepre-trainingsequencesoftokens(S ,...,S ). Let0<δ <1,
1 Ntrain
thenwithprobabilityatleast1 δ,
−
(cid:115)
B¯ (cid:18) 2(cid:19)
pre(Θ) (cid:98)pre(Θ)+ log ,
R ≤R √N δ
train
whereB¯ = 2 Γ max log(T)+2B /τ,log(1/c ) 1/2 isaconstantdependingonthepa-
U 0
∥ ∥ { }
rametersoftheproblem.
TheboundinTheorem4.1dependsontheintrinsicstructureofthepre-trainingdatathroughthenorm
ofthemixingmatrix Γ . Ifthepre-trainingdataS isaMarkovchainwithstatespaceΩ,thisnorm
∥ ∥
capturesexactlythemixingtimeofthelatter,makingsequencesthatmixataslowerpaceharder
tolearn. Secondly,andperhapsmostsurprisingly,thisboundbecomesmodel-independentwhen
max log(T)+2B /τ,log(1/c ) is dominated by log(1/c ) term. Hence, if B (T√r),
U 0 0 U
{ } ≈ O
which happens in practice due to the common normalization of the unembedding layer, then the
3Formally, R(Θ) = E S∼P L[d TV(P L(·|S),P Θ(·|S))] = E S∼P L[d TV(Q∗(S,·),Q f(S,·))]. 4∥Γ∥ = 1
forindependentvariablesandmoredetailsonMartoncouplingcanbefoundinAppendixC.3.
7Preprint. Underreview.
model’s hidden dimension r and vocabulary size T should be large enough to ensure log(T)+
2B /τ log(1/c )forsomeunknownreferenceconstantc . Belowthisthreshold,thearchitecture
U 0 0
≥
off isnotexpressiveenoughtohaveanytangibleimpactonitsgeneralization,althoughitmay
Θ
affectthetrainingerror (cid:98)pre(Θ).
R
Depth-dependentvariation. WeextendTheorem4.1tomakeitsdependencyonf morefine-
Θ
grained. Ratherthanassumingthatonlythenormoftheembeddinglayer’smatrixisbounded,we
followthesettingofpriorwork(Zhangetal.,2023b;Furuyaetal.,2024;Marion,2023;Edelman
etal.,2022)andconsidertheparameterspacedefinedasfollows:
W(cid:102)= {Θ
∈W
|∀ℓ ∈[L], ∥W V(ℓ)
∥∞
≤B V, ∥W O(ℓ)
∥∞
≤B O, ∥W 1(ℓ)
∥∞
≤B 1, ∥W 2(ℓ)
∥∞
≤B
2
}.
Thedefinitionof (cid:102)concernsthequery,key,andvaluematricesofalllayersandheads. Similarly
W
toZhangetal.(2023b,Assumption5.1),weassumethateachtokenhasanℓ -normboundedby
1
B .
tok
Wehavethefollowinggeneralizationbound,whoseproofisdeferredtoAppendixD.5.
Corollary4.2(Depth-dependentbound). ConsideranLLMf ˜ := f Θ ˜ . With
Θ Θ
∈F { | ∈W}
thesameassumptionsasinTheorem4.1,wehave
(cid:115)
B¯ (cid:18) 2(cid:19)
pre(Θ) (cid:98)pre(Θ)+ log ,
R ≤R √N δ
train
where B¯ = 2 Γ max log(T)+2(B )L/τ,log(1/c ) 1/2 is a constant depending on the
Θ 0
∥ ∥ { }
parametersoftheproblem,andB =[(1+rmB B )(1+ r3 B B )](B B )1/L.
Θ 1 2 H O V tok U
We note that B¯ exhibits an exponential dependence on the depth of the transformer, which also
amplifies the hidden dimensionality (width) of the embedding layer r. This contrasts with the
dependencyinm,thehiddendimensionalityoftheMLPblock,whichislinear. Allthesefactors
arecommonlyassociatedwithhigherexpressivepoweroftransformerssuggestingthattheyshould
contributetoabetterminimizationof (cid:98)pre(Θ)attheexpenseofrequiringmoretrainingdata. The
numberofheadsH canbeusedasacR ounterbalancetoincreasingthewidthinthecubictermr3,
suggestingthatagoodbalancebetweentheseparametersmayleadtomoredata-efficientmodels.
SamplecomplexityofLLMs. Ourgoalistoshowtheasymptoticdependenceonthenumberof
sequencesthatanLLMrequiressuchthatQ isε-closetothereferencetransitionmatrixQ∗. We
f
thenderiveasamplecomplexitybound. TheproofisdeferredtoAppendixD.6.
Corollary4.3(Samplecomplexity). LetB¯betheparameter-dependentconstantofTheorem4.1
or Corollary 4.2. Let δ [0,1] and let ϵ > 0. If N N∗ := 4B¯2 log(cid:0)2(cid:1) and if we
∈ train ≥ ⌈ ϵ2 δ ⌉
assumeaperfectpre-trainingerrorforf ,thenwehavewithprobabilityatleast1 δ,
Θ
−
E S∼P L∥Q∗(S, ·) −Q f(S, ·) ∥1 ≤ϵ.
ThisresultallowsustocontextualizeLLMs’abilitytolearnMarkovchainswithrespecttotheexisting
literature. Tothebestofourknowledge,theonlyexistingapproachwiththeoreticalguaranteesfor
learningMarkovchainsisthefrequentistmethod: countingthenumberofoccurrencesofdifferent
statestofillinthematrixQ .Wolfer&Kontorovich(2019)showthatthesamplecomplexityofap-
f
proximatingQ∗uptoϵwithsuchapproachrequiresatmost (max ∗ /ϵ2γ ,1/γ π∗ )samples,
whereγ
isa(pseudo)spectralgapoftheMarkovchainandπO∗isthes{ m|V aK lle| steles mentos fit}
sstationary
s
distribution. Theauthorsstatethatthefrequentistapproachisminimaxoptimal(uptologarithmic
√
factors). OurboundhasadependencethatbehavesasB¯2 = (max logT + 2T r,log(1/c ) ).
Given that in practice T > r, it then simplifies to (max TO /ϵ2τ,1{ /ϵ2 ). Noteτ that the LL0 M} s’
O { }
samplecomplexityislinearinthevocabularysizeT,whichisremarkablecomparedtothesample
complexityofthefrequentistapproach,whichscalesas (TK). WeshowinSection5thatthisis
O
confirmedexperimentally: LLM’sabilitytolearnMarkovchainsexceedsthefrequentistapproach
forMarkovchainswithalargestatespace.
8Preprint. Underreview.
4.2 IN-CONTEXTLEARNINGOFMARKOVCHAINS
Althoughinsightful,theanalysispresentedaboveisrelatedtothepre-trainingofLLMs–aprocess
thatishardandextremelycostlytoreproduceinpractice. Similarly,wedonothaveaccesstothe
ground-truthmatrixQ∗toreasonaboutLLM’sabilitytoinferitinpractice. Toprovidetheoretical
resultsthatcanbeconfirmedexperimentally, wenowturnourattentiontoin-contextlearningof
Markovchains: asetupwhereoneprovidesanLLMwithaninputsequenceformedbyaMarkov
chainofsizeN definedoverastatespaceΩofsized5. DifferentfromthesettingofSection4.1,
icl
we now can explicitly use a transition kernel P of this Markov chain for the theoretical analysis
byreplacingP
L
withitinthedefinitionof icl(Θ)and (cid:98)icl(Θ)inEq.(2)(seeAppendixD.7for
R R
detailsontheproblemsetup). Torelatethegeneralizationerrortothepre-trainingerror,wequantify
thediscrepancybetweenanLLMpre-trainedmostlyontextualdata,andahypotheticalLLMwith
parametersin thatispre-trainedonadatasetofMarkovchainswiththesamedatadistribution
mc
W
astheMarkovchainusedasaninputduringin-contextinference. Wedefinethedivergencebetween
twoestimatedtransitionmatricesP ,P as
Θ1 Θ2
N
1 (cid:88)
(Θ ,Θ ):= E [d (P ( S ),P ( S ))]. (3)
K
1 2
N
Sn TV Θ1
·|
n Θ2
·|
n
n=1
The operator is akin to a distance (the separation property is only verified almost surely, see
K
AppendixC.4formoredetails). Thenextresult,whoseproofisdeferredtoAppendixD.7,providesa
generalizationboundonthein-contextlearningphase.
Theorem 4.4 (In-Context Learning generalization bound). Consider an LLM f . We
Θ
∈ F
provide as input of f a d state Markov chain X = (X ,...,X ). The sequence of
Θ
−
1 Nicl
subsequencesofthefirstntermsisdenotedbyS =(S ,...,S ). S isalsoaMarkovchain,
and we denote by t (ε) its mixing time. Let t :=1 inf Nicl t (cid:0)ε(cid:1) (2−ε)2. Let δ > 0.
mix min 0≤ε<1 mix 2 1−ε
Then,withprobabilityatleast1 δ,
−
(cid:115)
(cid:114) (cid:18) (cid:19)
t 2
icl(Θ) inf (cid:98)icl(ϑ)+ (ϑ,Θ) +B¯ min log , (4)
R ≤ϑ∈Wmc{R K } N
icl
δ
whereB¯ =2max log(d)+2B /τ,log(1/p ) 1/2.
U min
{ }
WefirstnotethatinsteadofthenormofthemixingmatrixΓseenbefore,wenowhaveanexplicit
dependencyont ,whichisrelatedtothemixingtimeoftheinputMarkovchain. This,together
min
withtheavailabilityoftheground-truthtransitionmatrix,allowsustouseTheorem4.4toderiveand
verifyexperimentallythescalinglawsofICLforpopularLLMs. Theorem4.4alsosuggeststhatan
LLMpre-trainedondiversedatasequencesdifferentfromMarkovchainsshouldexhibitacertain
degreeofinvariancetocorrectlyinferthetransitionprobabilitiesofthelatter. Thisisreminiscentof
thedomainadaptationbounds(Redkoetal.,2019)thatalsocommonlyinvolveadistributionshift(i.e.,
adistanceoradivergence)termthatvanishesifthemodelisinvarianttoclassesoftransformations
linkingthedistributionoftheinputdatawiththatonwhichitisappliedduringinference. Arecent
successofapplyingLLMstotimeseriesdata(Gruveretal.,2023),forinstance,suggeststhatthis
termisindeedsmallforcertaintypesofdatanotusedduringpre-training.
5 NUMERICAL EXPERIMENTS
Theorem 4.4 provides a practically verifiable result which naturally stems from our analysis in
Section4. WethenevaluatetheabilityofrecentLLMs,namelyMistral 7Bv0.1(Jiangetal.,
2023),Llama2 7B & 13B(Touvronetal.,2023b),andGemma 2B(Teametal.,2024)toinfer
transitionprobabilitiesofMarkovchainsin-context. Weassociateeachstateinthed-stateMarkov
chain with a token from the set 0,...,d 1 , concatenated to obtain a prompt of length N .
icl
{ − }
Bearinginmindthedifferencesinthetokenizationmechanismsofthedifferentmodels,weaddcomas
5ThisisdifferentfromanothervariationofICLwheresupervised(x,y)pairsareprovidedin-context.Rather,the
supervisionisprovidedfromobservingtransitionsbetweenstates(x ,x =f(x ))asdiscussedin(Lietal.,
i i+1 i
2023,Fig.1).
9Preprint. Underreview.
whenevernecessarytoensurethateachstateistokenizedseparately. Moredetailsontheexperimental
setupandadditionalexperimentswithmoreMarkovchainsandwithLlama3.2(Dubeyetal.,2024)
areavailableinAppendixE.1.
100 0.8
O( tmin=43.7
N icl−1/2
) 0.6
t
t
tm
m
mi
i
in
n
n=
=
=3
1
42
5
.6. .26
0.4
Llama2 7B
101 Llama2 13B 0.2
Mistral 7B v0.1
Gemma 2B SmallN icl Scalinglaw
0.0
100 101 102 103 100 101 102 103
Context Length Nicl Context Length Nicl
Figure5: In-contextscalinglaws. Weplottherisk asfunctionsofN ,with95%confidence
icl icl
R
intervals. Left. RisksfordifferentLLMsalongwiththescalinglawofTheorem4.4. Right. Risksin
semi-logscalewithMistral 7B v0.1forrandom3-statetransitionmatricesanddifferentt .
min
Dependence on N . We first analyze the effect of N on the risk calculated for a randomly
icl icl
generated 3-state Markov transition matrix. From the results presented in Fig. 5(left), we note
that Llama-2 models deviate from our (N−1/2) theoretical scaling law, while most recent
models(MistralandGemma)staymuchO closeic rl
toTheorem4.4,similarlytowhatwasobserved
byCabannesetal.(2024). Beingrandomlygenerated,theMarkovchainsprovidedtothemodels
havenotbeenseenduringtraining,andolder(weaker)modelsnaturallystruggletogeneralize.
Dependenceont . Theorem4.4statesthatMarkovchainswithslowmixing(highert )are
min min
slowertolearn. Wenowplotthetrueriskforasinglemodelwithdifferentvaluesoft highlighting
min
in Fig. 5(right) a two-stage regime of ICL. In a first stage, the bound in Eq. (4) is dominated by
(cid:112) t /N forsmallN ,anddependsstronglyont ,whilethescalinglaw (N−1/2)dominates
min icl icl min O icl
asN increasesbeyondN 20.
icl icl
≈
100
100
O(
101
Frequentist
N icl−1/2
)
O(
N ic− l1/2
)
Gemma 2B
101
100 101 102 103 100 101 102
Context Length Nicl Context Length Nicl
Figure6: Impactofthenumberofstates. Weplottherisks asfunctionsofN forGemma
icl icl
2Bandthefrequentistapproach(Wolfer&Kontorovich,2019)R
with95%confidenceintervals. Left.
Theinputsequenceisarandom3-stateMarkovchain. Right. TheinputsequenceisaBrownian
motiondiscretizedasa700-stateMarkovchain,similarlytoLiuetal.(2024).
Dependenceond. WenowverifyTheorem4.4forMarkovchainswithadifferentstatespacesize
(previouslyd=3). Wealsoconsiderabaselinegivenbythefrequentistmethodmentionedbefore.
(cid:112)
Werecallthat,forthelatter,itsdependenceondbehaveslike ( d/N ),whileTheorem4.4gives
(cid:112) O icl
( log(d)/N ). ForMarkovchainswithasmallnumberofstatesd,thereisnocleardifference
icl
O
betweenthefrequentistestimatorandaLLM.However,asdgrowsthefrequentistestimatorstruggles
toestimatethetransitionmatrixduetothe (√d)scalingfactor. Thisisverifiedexperimentallyin
O
Fig.6,wherewevarytheparameterdfrom3(left)to700(right). WeobservethattheLLMfollows
thetheoreticalneuralscalinglaw (N−1/2)andoutperformsthefrequentistmethodford=700,
O icl
whilebeingclosetoitford=3. WeconcludethatouranalysisgivestheoreticalinsightsontheICL
neuralscalinglawobservedempiricallyin(Liuetal.,2024). Theadditionalexperimentsconducted
inAppendixE.5showthatourboundsremainvalidforlargevaluesofd.
10
lci
rorrE
lci
rorrE
lci
rorrE
lci
rorrEPreprint. Underreview.
6 CONCLUSION
Thispaperproposedanexplicitcharacterizationoftheinferencemechanisminlargelanguagemodels
through an equivalent finite-state Markov chain. We provided an insightful theoretical analysis
basedontheestablishedcharacterizationandtheabilityoftheLLMtoinferthetransitionkernel
approximating thetrue transitionprobabilities oflanguage. We adaptedour resultsto in-context
learningwhereexperimentsconfirmourtheoreticalinsights. Inthefuture,wehopethattheproposed
equivalencewillhavefar-reachingimplicationsonourunderstandingofLLMsandallowforamore
fine-grainedunderstandingoftheirexpressiveness.
ACKNOWLEDGMENTS
TheauthorswouldliketothankVasiliiFeofanov,GiuseppePaolo,AlbertThomas,MalikTiomoko,
AladinVirmaux,andAlainDurmusforfruitfuldiscussions. NicolasBoulle´ wassupportedbythe
OfficeofNavalResearch(ONR),undergrantN00014-23-1-2729.
REFERENCES
JoshAchiam,StevenAdler,SandhiniAgarwal,LamaAhmad,IlgeAkkaya,FlorenciaLeoniAleman,
DiogoAlmeida,JankoAltenschmidt,SamAltman,ShyamalAnadkat,etal. Gpt-4technicalreport.
arXivpreprintarXiv:2303.08774,2023.
AlekhAgarwal,ShamKakade,AkshayKrishnamurthy,andWenSun. Flambe: Structuralcomplexity
andrepresentationlearningoflowrankmdps. InAdvancesinNeuralInformationProcessing
Systems,volume33,pp.20095–20107,2020.
Armen Aghajanyan, Luke Zettlemoyer, and Sonal Gupta. Intrinsic dimensionality explains the
effectivenessoflanguagemodelfine-tuning. arXivpreprintarXiv:2012.13255,2020.
Mehdi Ali, Michael Fromm, Klaudia Thellmann, et al. Tokenizer Choice For LLM Training:
NegligibleorCrucial? InFindingsoftheAssociationforComputationalLinguistics: NAACL
2024,pp.3907–3924.AssociationforComputationalLinguistics,2024.
RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,RaduSoricut,Johan
Schalkwyk,AndrewMDai,andAnjaetal.Hauth. Gemini: afamilyofhighlycapablemultimodal
models. arXivpreprintarXiv:2312.11805,2023.
FrancisBach. LearningTheoryfromFirstPrinciples. MITPress,2024.
Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birthof a
Transformer: A Memory Viewpoint. In Advances on Neural Information Processing Systems,
2023.
TomBrown,BenjaminMann,NickRyder,etal. Languagemodelsarefew-shotlearners. InAdvances
inNeuralInformationProcessingSystems,volume33,pp.1877–1901,2020.
VivienCabannes,ElvisDohmatob,andAlbertoBietti. ScalingLawsforAssociativeMemories. In
InternationalConferenceonLearningRepresentations,2024.
H. Chen and N. Ding. Probing the “creativity” of large language models: Can models produce
divergentsemanticassociation? InEMNLP,pp.12881–12888.ACL,2023.
JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:Pre-trainingofdeep
bidirectionaltransformersforlanguageunderstanding. InJillBurstein,ChristyDoran,andThamar
Solorio(eds.),ConferenceoftheNorthAmericanChapteroftheAssociationforComputational
Linguistics: HumanLanguageTechnologies,pp.4171–4186,2019.
AbhimanyuDubey,AbhinavJauhri,AbhinavPandey,AbhishekKadian,AhmadAl-Dahle,Aiesha
Letman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herdofmodels.
arXivpreprintarXiv:2407.21783,2024.
11Preprint. Underreview.
BenjaminLEdelman,SurbhiGoel,ShamKakade,andCyrilZhang. Inductivebiasesandvariable
creation in self-attention mechanisms. In International Conference on Machine Learning, pp.
5793–5831.PMLR,2022.
BenjaminLEdelman,EzraEdelman,SurbhiGoel,EranMalach,andNikolaosTsilivis. Theevolution
ofstatisticalinductionheads:In-contextlearningmarkovchains. arXivpreprintarXiv:2402.11004,
2024.
GeraldBFolland. Realanalysis: moderntechniquesandtheirapplications,volume40. JohnWiley
&Sons,1999.
Takashi Furuya, Maarten V de Hoop, and Gabriel Peyre´. Transformers are universal in-context
learners. arXivpreprintarXiv:2408.01367,2024.
Robert G Gallager. Finite state Markov chains. In Discrete Stochastic Processes, pp. 103–147.
Springer,1996.
SiavashGolkar,MarielPettee,MichaelEickenberg,AlbertoBietti,MilesCranmer,GeraudKrawezik,
FrancoisLanusse,MichaelMcCabe,RubenOhana,LiamParker,etal. xval: Acontinuousnumber
encodingforlargelanguagemodels. arXivpreprintarXiv:2310.02989,2023.
NateGruver,MarcFinzi,ShikaiQiu,andAndrewGWilson. Largelanguagemodelsarezero-shot
timeseriesforecasters. AdvancesinNeuralInformationProcessingSystems,36,2023.
GeoffreyHinton. DistillingtheKnowledgeinaNeuralNetwork. arXivpreprintarXiv:1503.02531,
2015.
EdwardJHu,yelongshen,PhillipWallis,ZeyuanAllen-Zhu,YuanzhiLi,SheanWang,LuWang,and
WeizhuChen. LoRA:Low-rankadaptationoflargelanguagemodels. InInternationalConference
onLearningRepresentations,2022.
XinyangHu,FengzhuoZhang,SiyuChen,andZhuoranYang. UnveilingtheStatisticalFoundations
ofChain-of-ThoughtPromptingMethods. arXivpreprintarXiv:2408.14511,2024.
M Emrullah Ildiz, Yixiao Huang, Yingcong Li, Ankit Singh Rawat, and Samet Oymak. From
Self-AttentiontoMarkovModels: UnveilingtheDynamicsofGenerativeTransformers. arXiv
preprintarXiv:2402.13512,2024.
HongJunJeon,JasonD.Lee,QiLei,andBenjaminVanRoy. AnInformation-TheoreticAnalysis
of In-Context Learning. In International Conference on Machine Learning, volume 235, pp.
21522–21554,2024.
AlbertQJiang,AlexandreSablayrolles,ArthurMensch,ChrisBamford,DevendraSinghChaplot,
DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,LucileSaulnier,etal.
Mistral7B. arXivpreprintarXiv:2310.06825,2023.
DanielJurafskyandJamesH.Martin. SpeechandLanguageProcessing: AnIntroductiontoNatural
LanguageProcessing,ComputationalLinguistics,andSpeechRecognitionwithLanguageModels.
Pearson,3rdedition,2024.
AndrejKarpathy. minGPT:AminimalPyTorchre-implementationoftheGPT(GenerativePretrained
Transformer). https://github.com/karpathy/minGPT,2023. GitHubrepository.
Juno Kim, Tai Nakamaki, and Taiji Suzuki. Transformers are Minimax Optimal Nonparametric
In-ContextLearners. arXivpreprintarXiv:2408.12186,2024.
YingcongLi,MuhammedEmrullahIldiz,DimitrisPapailiopoulos,andSametOymak. Transformers
asalgorithms: Generalizationandstabilityinin-contextlearning. InInternationalConferenceon
MachineLearning,pp.19565–19594.PMLR,2023.
Toni JB Liu, Nicolas Boulle´, Raphae¨l Sarfati, and Christopher J Earls. LLMs learn governing
principles of dynamical systems, revealing an in-context neural scaling law. arXiv preprint
arXiv:2402.00795,2024.
12Preprint. Underreview.
SanaeLotfi,MarcFinzi,YilunKuang,TimGJRudner,MicahGoldblum,andAndrewGordonWilson.
Non-vacuousgeneralizationboundsforlargelanguagemodels. arXivpreprintarXiv:2312.17173,
2023.
Sanae Lotfi, Yilun Kuang, Brandon Amos, Micah Goldblum, Marc Finzi, and Andrew Gordon
Wilson. Unlockingtokensasdatapointsforgeneralizationboundsonlargerlanguagemodels.
arXivpreprintarXiv:2407.18158,2024.
AshokVardhanMakkuva,MarcoBondaschi,AdwayGirish,AlliotNagle,MartinJaggi,HyejiKim,
andMichaelGastpar. AttentionwithMarkov: Aframeworkforprincipledanalysisoftransformers
viamarkovchains. arXivpreprintarXiv:2402.04161,2024.
PierreMarion. Generalizationboundsforneuralordinarydifferentialequationsanddeepresidual
networks. InAdvancesonNeuralInformationProcessingSystems,volume36,2023.
Katalin Marton. Measure concentration for Euclidean distance in the case of dependent random
variables. Ann.Probab.,32(3):2526–2544,2004.
CleoNardo. Remarks1–18onGPT(compressed). https://www.lesswrong.com/posts/
7qSHKYRnqyrumEfbt/,2023. Accessed: 2024-09-27.
Daniel Paulin. Concentration inequalities for Markov chains by Marton couplings and spectral
methods. Electron.J.Probab.,20:79,2015.
IevgenRedko,AmauryHabrard,EmilieMorvant,MarcSebban,andYoune`sBennani. Stateofthe
ArtofStatisticalLearningTheory. InAdvancesinDomainAdaptationTheory,pp.1–19.Elsevier,
2019.
Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the
parametersofalanguagemodel? InEmpiricalMethodsinNaturalLanguageProcessing, pp.
5418–5426.AssociationforComputationalLinguistics,2020.
Gareth O. Roberts and Jeffrey S. Rosenthal. General state space Markov chains and MCMC
algorithms. Probab.Surveys,1:20–71,2004.
RicoSennrich,BarryHaddow,andAlexandraBirch. NeuralMachineTranslationofRareWords
with Subword Units. In Katrin Erk and Noah A. Smith (eds.), Association for Computational
Linguistics,pp.1715–1725,2016.
AadityaKSinghandDJStrouse. Tokenizationcounts: theimpactoftokenizationonarithmeticin
frontierllms. arXivpreprintarXiv:2402.14903,2024.
GemmaTeam,ThomasMesnard,CassidyHardin,RobertDadashi,SuryaBhupatiraju,ShreyaPathak,
LaurentSifre,MorganeRivie`re,MihirSanjayKale,JulietteLove,etal. Gemma: Openmodels
basedongeminiresearchandtechnology. arXivpreprintarXiv:2403.08295,2024.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothe´e
Lacroix, BaptisteRozie`re, NamanGoyal,EricHambro, FaisalAzhar,etal. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023a.
HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,Nikolay
Bashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Openfoundation
andfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023b.
AlexandreB.Tsybakov. IntroductiontoNonparametricEstimation. Springer,1stedition,2008.
V.N.Vapnik. Anoverviewofstatisticallearningtheory. IEEETrans.NeuralNetw.,10(5):988–999,
1999.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,Łukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information
ProcessingSystems,volume30,2017.
NoamWies,YoavLevine,andAmnonShashua. Thelearnabilityofin-contextlearning. InAdvances
inNeuralInformationProcessingSystems,volume36,2024.
13Preprint. Underreview.
GeoffreyWolferandAryehKontorovich.MinimaxlearningofergodicMarkovchains.InAlgorithmic
LearningTheory,pp.904–930.PMLR,2019.
Geoffrey Wolfer and Aryeh Kontorovich. Learning and identity testing of Markov chains. In
HandbookofStatistics,volume49,pp.85–102.Elsevier,2023.
ZhenglongWu,QiQi,ZiruiZhuang,HaifengSun,andJingyuWang. Pre-tokenizationofnumbers
forlargelanguagemodels. InTheSecondTinyPapersTrackatICLR2024,2024.
SangMichaelXie,AditiRaghunathan,PercyLiang,andTengyuMa. AnExplanationofIn-context
LearningasImplicitBayesianInference. InInternationalConferenceonLearningRepresentations,
2022.
CanYaras,PengWang,LauraBalzano,andQingQu. Compressibledynamicsindeepoverparame-
terizedlow-ranklearning&adaptation. arXivpreprintarXiv:2406.04112,2024.
BiaoZhangandRicoSennrich. RootMeanSquareLayerNormalization. InAdvancesinNeural
InformationProcessingSystems,volume32,2019.
WenxuanZhang,YueDeng,BingLiu,SinnoJialinPan,andLidongBing. Sentimentanalysisinthe
eraoflargelanguagemodels: Arealitycheck. arXivpreprintarXiv:2305.15005,2023a.
YufengZhang,FengzhuoZhang,ZhuoranYang,andZhaoranWang. Whatandhowdoesin-context
learninglearn? Bayesianmodelaveraging,parameterization,andgeneralization. arXivpreprint
arXiv:2305.19420,2023b.
14Preprint. Underreview.
Appendix
Roadmap. InAppendixA,wefirstrecallournotations. Weprovideadditionaldetailsonautore-
gressivemodelsandtransformersinAppendixB.ImportantnotionsanddefinitionsrelatedtoMarkov
chainsandMartoncouplingsaregiveninAppendixC.Thedetailedproofsofourtheoreticalresults
aregiveninAppendixD.Finally,weprovideadditionalexperimentsinAppendixE.
TABLE OF CONTENTS
A Notations 16
B BackgroundonAutoregressiveModels 16
B.1 AutoregressiveModels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
B.2 TransformerArchitecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
B.3 AutoregressiveTransformer-basedLLM . . . . . . . . . . . . . . . . . . . . . . . 18
C BackgroundonMarkovChains 18
C.1 BasicNotions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
C.2 ErgodicUnichains . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.3 MartonCouplings . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
C.4 An(Almost)DistancebetweenMarkovChains . . . . . . . . . . . . . . . . . . . 21
D Proofs 22
D.1 ProofofProposition3.1. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
D.2 ProofofProposition3.2. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
D.3 ProofofProposition3.3. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
D.4 ProofofTheorem4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25
D.5 ProofofCorollary4.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
D.6 ProofofCorollary4.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
D.7 ProofofTheorem4.4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
E AdditionalExperiments 44
E.1 ExperimentalSetupandTokenization . . . . . . . . . . . . . . . . . . . . . . . . 44
E.2 ImpactoftheNumberofStatesd . . . . . . . . . . . . . . . . . . . . . . . . . . . 44
E.3 MoreStructuredMarkovChains . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
E.4 RecentModels: ImpactoftheTokenization . . . . . . . . . . . . . . . . . . . . . 49
E.5 DynamicalSystems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
15Preprint. Underreview.
A NOTATIONS
Wedenote 1, ,N as[N]. Werepresentscalarvalueswithregularletters(e.g.,parameterλ),
{ ··· }
vectorswithboldlowercaseletters(e.g.,vectorx),andmatriceswithboldcapitalletters(e.g.,matrix
A). The i-th row of the matrix A is denoted by A , its j-th column is denoted by A and its
i ,j
transposeisdenotedbybyA⊤. TheidentitymatrixofsizenisdenotedbyI Rn×n. Thevector
n
ofsizenwitheachentryequalto1isdenotedby1 . Wedenoteby A th∈ eL matrixnorm
n p,q p,q
∥ ∥
wherethep-normisovercolumnsandtheq-normisoverrows. Wedenoteby A theoperator
∥ ∥
normofAinducedbytheℓ normandby A =max A theoperatornorminducedbythe
2 ∞ ij ij
∥ ∥ | |
ℓ -norm. Similarly,x⊤isthetransposeofthevectorxand x isitsℓ -norm. Thetotalvariation
∞ p p
betweentwoprobabilitydistributionsP,Qisdenotedbyd∥ (∥P,Q).
Theterm“almostsurely”is
TV
denoted by the notation “a.s.” while the term random variable is denoted by the notation “r.v.”.
∆ := p [0,1]n (cid:80)n p =1 istheprobabilitysimplexofRn.
n { ∈ | i=1 i }
B BACKGROUND ON AUTOREGRESSIVE MODELS
We first recall important notions regarding autoregressive models before focusing on the most
widelyusedones,namelythetransformer-basedLLMs. Wedescribethecomponentsofthevanilla
transformer architecture before describing the whole network at the heart of such a model and
formallydefiningtheclassofparametersandneuralnetworksconsideredinourwork.
B.1 AUTOREGRESSIVEMODELS
Inthissection,werecallhowthesequencesoftokensareprocessedbytheautoregressivemodel
notablyregardingthenexttokengenerationandthedeletionprocess.
DefinitionB.1(Generationprocess). Givenaninputs ∗ ofsizep,anautoregressivemodel
∈VK
outputsaprobabilitymassfunctionfT,K(s)overthediscretevocabularyspace. Anexttokenx
Θ
isthensampledfromfT,K(s),toconstructanewsequence(s,x)ofsizep+1.
Θ
Generationcanberepeatedbyconsidering(s,x)asnewinputsequenceanditeratingthisprocess.
SincethesemodelsaredesignedtohandleonlysequencesofsizeatmostK,adeletionprocessis
required.
DefinitionB.2(Deletionprocess). Givenaninputsofsizep > K,anautoregressivemodel
outputs a probability mass function fT,K(s ) where s is a truncation of K tokens of the
Θ K K
sequences. Autoregressivemodelsimplementfronttruncation,whichisdonebysettings as
K
thelastK tokensofs.
AsshowninFig.7,onlythelastK tokensofalonginputsequenceareused. Thisiswhywespeak
ofdeletion,sinceweignorethefirsttokens.
Notethatitispossibletoimplementotherkindsoftruncation,butautoregressivemodelsusuallydo
not(Brownetal.,2020;Touvronetal.,2023a),however,inmodelslikeBERT(Devlinetal.,2019),
whicharenotautoregressive,backtruncationasdescribedinFig.8isalsoanoption.
B.2 TRANSFORMERARCHITECTURE
ThemostpopularautoregressiveLLMsrelyonthetransformerarchitecture(Vaswanietal.,2017)
whichwedescribebelowfollowing(Brownetal.,2020;Edelmanetal.,2022;Zhangetal.,2023b).
Anautoregressivetransformer-basedLLMtakesasinputasequenceoflengthn,withn K andK
≤
isthecontextwindow,tokenswithvaluesinavocabulary ofsizeT. Thetokensareembeddedinto
ar-dimensionalspaceandtheinputcanbewrittenasS
VRr×n.
Weconsideratransformermodel
∈
withLlayersandhheads. Theoutputoftheℓ-thlayerwritesS(ℓ)andisfedasinputofthe(ℓ+1)-th
layer. TheinputofthewholemodelisS(0) =S. Below,wedescribetheoperationsperformedby
themodel,includingtheembeddingsofthetokens.
16Preprint. Underreview.
x x x x
1 2 3 4
x x x x x x x x x x
1 2 3 4 5 6 7 8 9 10
Figure7: Deletionprocess,fronttruncation. AnautoregressivemodelwithcontextwindowK =7
innavyblue,processingsequencesofdifferentlengths. Top. Asequenceoflength4. Bottom. Front
truncationofasequenceoflength10.
x x x x x x x x x x
1 2 3 4 5 6 7 8 9 10
Figure8: Backtruncation. AnautoregressivemodelwithcontextwindowK = 7innavyblue,
processingbacktruncationofasequenceofasequenceoflength10.
• Tokenembeddings. Thetokensareembeddedinar-dimensionalspaceviaanembeddinglayer
W whichresultsinaninputoftheformSr×n;
E
• Positionalembeddings. (Learnable)positionalembeddingsareaddedtoeachtokendepending
onitspositionintheinputsequence. Thisbreaksthepermutation-invarianceofthetransformer
architectureandleads,byabuseofnotation,toanoutputS Rr×n;
∈
• Multi-headattention(MHA).GivenaninputsequenceS Rr×n,query,key,andvaluematrices
W ,W ,W Rr×r (herethevalueandoutputmatrice∈ saremergedforeaseofnotations),the
Q K V
∈
self-attentionmodulecomputes
(cid:16) (cid:17)
(S;W ,W ,W ):=softmax W S(W S)⊤ /√r (W S) Rr×n,
Q K V Q K V
A ∈
with softmax: x Rn exp(x)/(cid:80) exp(x) ∆ . The operation described below corre-
∈ → i i ∈ n
spondstosingle-headself-attention. Inpractice,multi-headattention(MHA)isusedwithH heads
andthequeryandkeymatricesareinR Hr× Hr andthevaluematrixisinR Hr×r (r,H aretaken
suchthat r isaninteger). TheMHAmoduleconcatenatesontherowdimensiontheoutputsof
H
foreachheadandthenprojectsitbacktotheembeddingdimensionr withanoutputmatrix
WA Rr×r. Byabuseofnotation,wealsodenoteby thisoperationwhichresultsinanoutput
O
∈ A
ofdimensionr n,andweincludetheoutputmatrixintheargumentoftheoperator.Theℓ-thlayer
×
ofthetransformerappliesattentionwithlayer-specificweightmatricesandaresidualconnection
thatleadstoanoutput
(cid:16) (cid:17)
Z(ℓ) =S(ℓ−1)+ S(ℓ−1);W(ℓ),W(ℓ),W(ℓ),W(ℓ) .
A Q K V O
Thisisfollowedbyalayernormalization(Zhang&Sennrich,2019)thatprojectseachtokeninto
theℓ -unitball,i.e.,eachcolumnS(ℓ) hasanℓ -normlowerthan1;
2 ·,n 2
• Feed-forward block (FF). Finally, a feed-forward block is applied, consisting of a two-layer
MLPwithhiddendimensionm,layer-specificweightmatricesW(ℓ) Rm×r,W(ℓ) Rr×mand
1 ∈ 2 ∈
ReLUactivationdenotedbyReLU(x)=max 0,x andappliedentry-wise. Theoutputofthis
{ }
layerreads
(cid:16) (cid:17)
Y(ℓ) =W(ℓ)ReLU W(ℓ)Z(ℓ) .
2 1
Itisfollowedbyaresidualconnectiontoproducetheoutput
(cid:16) (cid:17)
S(ℓ) =Z(ℓ)+W(ℓ)ReLU W(ℓ)Z(ℓ) Rr×n,
2 1 ∈
17Preprint. Underreview.
onwhichlayernormalization(Zhang&Sennrich,2019)isappliedensuringthateachcolumnS(ℓ)
·,n
hasanℓ -normlowerthan1.
2
• softmaxoutputlayer. Intheautoregressivesetting,themodeloutputsaprobabilitydistributionon
thevocabulary . Tothatend,theoutputS(L) Rr×nofthefinallayerisprojectedbacktothe
vocabularysizeV byan“unembeddinglayer”W ∈ RT×r andaveragedoverthecolumnstoobtain
U
avectorinRT. Asoftmaxlayerisfinallyapplied∈ ontopofittoobtaintheprobabilitydistribution
ofthenexttokenP ( S). Formally,wehave
Θ
·|
(cid:18) (cid:19)
1
P ( S)=softmax W S(L)1 ∆ ,
Θ U n T
·| nτ ∈
nisthelength(i.e.,numberofcolumns)oftheinputsequenceS(andthusofthelastlayeroutput
S(L)),Θdenotestheparametersofthewholenetworkthatsubsumetheparametersofeachlayer
andeachblockandτ isthesoftmaxtemperature(Hinton,2015).
B.3 AUTOREGRESSIVETRANSFORMER-BASEDLLM
Thearchitecturedescribedaboveisusedinmostofthetransformer-basedautoregressiveLLM(Brown
etal.,2020;Dubeyetal.,2024;Aniletal.,2023;Jiangetal.,2023). Inthetheoreticalanalysisof
Section4,andunlessspecifiedotherwise,weremainfaithfultotheirpracticalimplementationand
onlymakethefollowingmildassumption: weassumethattheunembeddinglayerisbounded. The
classofparametersandtheclassofneuralnetworksitgeneratesrespectivelywrites
= Θ W⊤ B and = f Θ .
W { |∥
U∥2,1
≤
U
} F {
Θ
| ∈W}
It should be noted that this assumption is significantly weaker than what is usually done in the
literature(Zhangetal.,2023b;Edelmanetal.,2022).
C BACKGROUND ON MARKOV CHAINS
WerecallbelowsomeimportantnotionsrelatedtoMarkovchainsbasedon(Roberts&Rosenthal,
2004;Paulin,2015)andthatwillbeusedinourproofs.
C.1 BASICNOTIONS
ConsidertwodistributionprobabilitiesPandQdefinedonameasurablespace(Ω, ).
F
DefinitionC.1. ThetotalvariationbetweenPandQisdefinedas
d (P,Q):= sup P(A) Q(A).
TV
| − |
A∈F
Inthesettingconsideredinthemainpaper,weconsiderMarkovchainswithfinitediscretestatespace
Ω. Inthissection,werefertoΩasageneralPolishspace,whoseelementsarereferredtoasstates.
Informally,adiscrete-time,time-homogeneousMarkovchainwithstatespaceΩisasequenceof
random variables (X ,X ,...) taking values in Ω such that the next observation is independent
1 2
of the past given the present. This property is referred to as the Markov property and is defined
below.
DefinitionC.2. Asequenceofrandomvariables(X ,X ,...)issaidtosatisfytheMarkov
1 2
propertyifforalln 1andany(x ,...,x ) Ωn+1
1 n+1
≥ ∈
P(X =x X =x , ,X =x )=P(X =x X =x ).
n+1 n+1 n n 1 1 n+1 n+1 n n
| ··· |
To a given Markov chain, we associate its transition kernel Q : Ω2 [0,1] which collects the
→
transitionprobabilitiesfromonestatetoanother
n N,(x,y) Ω2, Q(x,y)=P(S =y S =x).
n+1 n
∀ ∈ ∈ |
Inthemaintext,werefertoQasatransitionmatrixastheMarkovchainsweconsiderareoffinite
statespace.
18Preprint. Underreview.
DefinitionC.3. AdistributionπonΩissaidtobeastationarydistributioniftheactionofthe
transitionkernelleavesπunchanged,thatis
(cid:90)
(Qπ)(A):= Q(x,y)dπ(x)=π(A)
y∈A
forallA .
∈F
AnaturalquestioniswhethersuchadistributionexistsforagenericMarkovchain. Beforestatingan
existencetheorem,weintroduceaclassificationofstatesbelow.
Classofstates. Alldefinitionsbellowareborrowedfrom(Gallager,1996)
DefinitionC.4(Accessibilityandcommunication). Astatexisaccessiblefromy(abbreviated
asx y)ifthereexistsn>0suchthatQn(x,y)>0.Twodistinctstatesxandycommunicate
→
(abbreviatedx y)ifxisaccessiblefromyandyisaccessiblefromx.
↔
AccessibilityandcommunicationconceptsdefinehowstatescanreacheachotherwithinaMarkov
chain. Thisleadstoanimportantclassificationofstatesintotransientandrecurrentcategories.
DefinitionC.5(Recurrentandtransientstates). Forfinite-stateMarkovchains,arecurrentstate
isastateithatisaccessiblefromallstatesthatareaccessiblefromi(iisrecurrentifi j
→
impliesthatj i). Atransientstateisastatethatisnotrecurrent.
→
Withthedistinctionbetweenrecurrentandtransientstatesestablished,wecannowgroupstatesinto
classesbasedontheircommunicationproperties.
DefinitionC.6(Classofstates). Aclass ofstatesisanon-emptysetofstatessuchthateach
C
i communicateswitheveryotherstatej andcommunicateswithnoj /
∈C ∈C ∈C
AperiodicityandErgodicity. Aperiodicityensuresthatthesystemdoesnotexhibitcyclicbehavior,
whichisakeyconditionforunderstandingtheasymptoticbehaviorofstates.
DefinitionC.7(Aperiodicity). Theperiodofastatei,denotedd(i),isthegreatestcommon
divisor(gcd)ofthosevaluesofnforwhichQn(i,i)>0.Iftheperiodis1,thestateisaperiodic.
UndersomeconditionsontheMarkovchain(aperiodicityandirreducibility(Roberts&Rosenthal,
2004)), itcanbeproventhatthechainconvergestoitsstationarydistributioni.e. foranyx Ω,
lim d (Qn(x, ),π)=0,whereQn(x, )denotestheprobabilityofS conditionedonS∈ =
n→∞ TV n 1
· ·
x.
WerecallbelowthenotionofmixingtimethatassessesthetimetakenbytheMarkovchaintobe
ε-closetoitsstationarydistribution(seeDefinitionC.8).
DefinitionC.8(Mixingtimefortime-homogeneousMarkovchains(Paulin,2015)). LetX :=
(X ,X ,...)beatime-homogeneousMarkovchainwithastatespaceΩ,atransitionkernelQ,
1 2
andastationarydistributionπ. Itsmixingtimeisdefinedforanyε [0,1]as
∈
t (ε):=min t d(t) ε whered(t):= supd (cid:0) Qt(x, ),π(cid:1) .
mix TV
{ | ≤ } ·
x∈Ω
Wealsointroducethequantity
(cid:16)ε(cid:17) (cid:18) 2 ε(cid:19)2
t := inf t −
min mix
0≤ε<1 2 · 1 ε
−
19Preprint. Underreview.
whichwillbeusefullateron.
RemarkC.1(Well-posednessoft ). Asweonlyconsiderfinitestate-spaceMarkovchainsin
min
ourwork, weknowthatastationarydistributionalwaysexists. However, itsuniquenessandthe
convergencetoitrequireadditionalassumptions(seeAppendixC.2). Inparticular,notallMarkov
chainsadmitafinitet (ε),t forsomeε< 1. Insuchcase,t canbeinfinite. Inourpractical
mix min 2 min
experimentation,thisisneverthecasedespiteconsideringvariousMarkovchains.
C.2 ERGODICUNICHAINS
Wearenowreadytostatethefollowingtheorem,whichformalizestheclassificationofstatesinto
recurrent,transient,andaperiodicclasses.
TheoremC.9(Recurrentandtransientclasses). ForfinitestateMarkovchains,eitherallstates
inaclassaretransientorallarerecurrent. Werefertotheseclassesastransientandrecurrent,
respectively.
ForanyMarkovchain,allstatesinaclasshavethesameperiod. Iftheperiodis1,theclassis
saidtobeaperiodic
Havingcategorizedstatesintorecurrent,transient,andaperiodicclasses,wecannowdefineergodicity.
DefinitionC.10(Ergodicity). Forafinite-stateMarkovchain,anergodicclassofstatesisa
classthatisbothrecurrentandaperiodic. AMarkovchainconsistingentirelyofoneergodic
classiscalledanergodicchain.
Unichains. Wenowintroducetheconceptofunichains.
DefinitionC.11(Unichainsandergodicunichains). Aunichainisafinite-stateMarkovchain
containingasinglerecurrentclassandtransientstates. Anergodicunichainisaunichainfor
whichtherecurrentclassisergodic.
C.3 MARTONCOUPLINGS
WhileweconsiderMarkovchaininputsinSection4.2,weconsiderlessstructuredinputsduringthe
pre-trainingphaseSection4.1.
Morespecifically,wemodelthesequencesoftokensusedduringpre-trainingasgenericdependent
randomvariables. Toderivemeaningfulresults,werelyonthenotionofMartoncouplingsintroduced
byMarton(2004). AMartoncouplingcanbeseenasaweakdependencystructurebetweenrandom
variables. Theassociatednotionofthemixingmatrix,analogoustothemixingtimeofaMarkov
chain,isusedtoassessthestrengthofthedependencebetweenthosevariables.
Thisminimalmodelingchoiceismadetoremainasfaithfulaspossibletothepre-trainingconsidered
in practical applications of LLMs, for which the pre-training data is not public and may contain
arbitrarydatapointssuchasvideo,codesnippets,textandimages(Dubeyetal.,2024;Touvronetal.,
2023a;Aniletal.,2023;Jiangetal.,2023;Achiametal.,2023;Brownetal.,2020).
AsshowninPaulin(2015,Remark2.2.),consideringsequencesofrandomvariableslinkedthrougha
Martoncouplingisaweakerassumptionthanwhatisusuallydoneintheliteratureongeneralization
bounds, which typically relies on independent random variables and Markov chains (Wolfer &
Kontorovich,2019;Zhangetal.,2023b;Huetal.,2024;Marion,2023).
In particular, the results stated in Section 4.1 encompass the case where the pre-training input
sequencesoftokensareindependentrandomvariables(Kimetal.,2024)orMarkovchains(Zhang
etal.,2023b). WealsonotethatMarkovchainscanmodelbigramsusedinnaturallanguage(Jurafsky
&Martin,2024;Biettietal.,2023).
WedonotprovideanexhaustivereviewofMartoncouplings. Wewillsimplyrecallitsdefinitionand
introducetheassociatedmixingmatrix. WerefertheinterestedreadertoMarton(2004)andPaulin
20Preprint. Underreview.
(2015). ConsiderasequenceofdependentrandomvariablesS =(S ,...,S )takingvaluesina
1 N
polishspaceΩ=Ω ... Ω . WewilldenotebyP(S ,...,S )thedistributionofS.
1 N 1 N
× ×
DefinitionC.12(Martoncoupling). WedefineaMartoncouplingforS asasetofcouplings
(cid:16) S(s1,...,si,s′ i),S′(s1,...,si,s′ i)(cid:17)
Ω Ω,
∈ ×
foreveryi [N],everys Ω ,...,s Ω ,s′ Ω ,satisfyingthefollowingconditions.
∈ 1 ∈ 1 i ∈ i i ∈ i
(i) S(s1,...,si,s′ i) =s , ..., S(s1,...,si,s′ i) =s ,
1 1 i i
S′(s1,...,si,s′ i) =s , ..., S′(s1,...,si,s′ i) =s , S′(s1,...,si,s′ i) =s′.
1 1 i−1 i−1 i i
(ii) (cid:16) S(s1,...,si,s′ i) ,...,S(s1,...,si,s′ i)(cid:17)
i+1 N
P(S ,...,S S =s ,...,S =s ),
i+1 N 1 1 i i
∼ |
(cid:16) S′(x1,...,xi,x′ i) ,...,S′(x1,...,xi,x′ i)(cid:17)
i+1 N
P(S ,...,S S =x ,...,S =x ,S =x′).
∼ i+1 N | 1 1 i−1 i−1 i i
(iii) Ifx
i
=x′ i,thenS(x1,...,xi,x′ i) =S′(x1,...,xi,x′ i).
DefinitionC.13(Mixingmatrix(Paulin,2015)). ForaMartoncoupling,wedefinethemixing
matrixΓ RN×N asanupperdiagonalmatrixwith
∈

Γ :=1,
 i,i
1 i<j N, Γ j,i :=0 .
∀ ≤ ≤  Γ :=sup P(cid:104) S(s1,...,si,s′ i) =S′(s1,...,si,s′ i)(cid:105)
i,j s1,...,si,s′ i j ̸ j
Forindependentrandomvariables,onecandefineaMartoncouplingwithamixingmatrixequalto
theidentity(seePaulin,2015,Remark2.2). Inparticular,itmeansthatforindependentvariables,we
havetheoperatornormofthemixingmatrixequalto1,i.e., Γ =1.
∥ ∥
C.4 AN(ALMOST)DISTANCEBETWEENMARKOVCHAINS
InTheoremD.23,Westateelementarypropertiesof inthepropositionbelow.
K
PropositionC.14(Propertiesof ). isanalmost-distancebetweentransitionmatricesinthe
K K
sensethatitsatisfiesthepropertiesbelow:
1. Non-negativity. ForanyΘ ,Θ , (Θ ,Θ ) 0.
1 2 1 2
K ≥
2. Almostsurepositivity. (Θ ,Θ )=0 n [N],P ( S )=P ( S )a.s..
K
1 2
⇐⇒ ∀ ∈
Θ1
·|
n Θ2
·|
n
3. Symmetry. ForanyΘ ,Θ , (Θ ,Θ )= (Θ ,Θ ).
1 2 1 2 1 2
K K
4. Triangularinequality.. ForanyΘ ,Θ ,Θ , (Θ ,Θ ) (Θ ,Θ )+ (Θ ,Θ ).
1 2 3 1 3 1 2 2 3
K ≤K K
ProofofPropositionC.14. Wefirstrecallthefollowingtechnicallemma.
LemmaC.15(Proposition2.16inFolland(1999)). LetY beanon-negativerandomvariable
definedonaprobabilityspaceΩwithprobabilityfunctionP. IfE[Y]=0,thenY =0almost
21Preprint. Underreview.
surely,i.e.,
P( ω Ω Y(ω)=0 )=1
{ ∈ | }
Thenon-negativityandsymmetryof directlycomefromthesymmetryandnon-negativityofthe
K
totalvariationdistance. Thetriangularinequalityfollowsfromthefactthatthetotalvariationisa
distanceandthattheexpectationrespectsinequalities. Forthealmostpositivity,considerΘ ,Θ
1 2
suchthat (Θ ,Θ )=0.Bynon-negativityofallthetermsinthesum,itmeansthatforalln [N],
1 2
K ∈
wehave
E [d (P ( S ),P ( S ))]=0.
Sn TV Θ1
·|
n Θ2
·|
n
As the total variation is a distance, we know that the random variable under the expectation is
non-negative. ApplyingLemmaC.15leadsto
d (P ( S ),P ( S ))=0 almostsurely.
TV Θ1
·|
n Θ2
·|
n
Ontheprobabilityspace,deprivedofthesetwherethedistanceisnon-zero(whichisofnullmeasure),
thetotalvariationisequaltozeroandasadistancebetweenprobabilitydistributions,itmeansthat
onthissubsetoftheprobabilityspace,theprobabilitiesareequal. Again,asthesetonwhichtheyare
notequalisofnullmeasure,wehave
P ( S )=P ( S ) almostsurely.
Θ1
·|
n Θ2
·|
n
Puttingeverythingtogether,wehave
n [N],P ( S )=P ( S ) a.s., (5)
∀ ∈
Θ1
·|
n Θ2
·|
n
whichconcludesthedirectsense. TheconversesenseisprovedbyassumingthatEq.(5)holdsand
usingthedistancepropertiesofthetotalvariation. Thisconcludestheproof.
D PROOFS
D.1 PROOFOFPROPOSITION3.1
WedetailbelowtheproofofProposition3.1.
ProofofProposition3.1. Step 1: Autoregressive models as Markov chains. Given an input
v ∗ ofptokens,anautoregressivemodeloutputsaprobabilitymassfunctionfT,K(v )over
i ∈ VK Θ i
thediscretevocabularyspace. Asthetemperatureispositive,i.e.,τ >0,andastheexponentialis
positive,weknowthatallthetokensinthevocabularywillbegivenapositivemass.
A next sequence v ∗ is then sampled according to fT,K(v ). But the v sequences that fit
j ∈ VK Θ i j
necessarilycontainthev sequence(exceptpossiblythefirstelementofv ,thankstoDefinitionB.2),
i i
i.e. l,(v ) =(v ) . Notealsothesizeofv isp+1whenp<kandkwhenp=k. Allother
j l i l+1 j
∀
sequencesv thatdonotsatisfythisconditionarenotsuitable.
j
In that sense, fT,K can be represented by a Markov chain MC( ∗,Q ) with transition kernel
Q
f
R|V K∗|×|V
K∗Θ
|,asdefinedinProposition3.1.
VK f
∈
Step2: Proportionofnon-zeroelements. WedenotebyR thesetofstatesoflengthK. Theset
ofstatesoflengthstrictlylessthanequalK isdenotedbyT. Wecanconstructatransitionmatrix
PR RTK×TK withthestatesofthisclass,containingtheprobabilitiesofmovingfromonestateof
R to∈ another. PR correspondstotheblueblockinFig.1whilegreenrectangleblockscorrespondto
partofPT andPTR inthefollowingdescriptionofautoregressivemodelsasMarkovchains,
 
PT PTR
Q
f
= . (6)
0 PR
Now,letuscountthenumberofnon-zeroelementsineachofthese4largeblocks.
(cid:104) T (cid:105) (cid:104) T (cid:105)
PT block:Thesizeofthisblockis (TK−1 1) (TK−1 1) . ThereareK 2
T 1 − × T 1 − −
greenblockscontainedinPT. Theblo−cknumberi [K 2−]isofsizeTi Ti+1. Sinceeach
∈ − ×
22Preprint. Underreview.
sentenceofsizeicanbecompletedwithnon-zeroprobability,byanyothertoken,thereareatotal
of(cid:80)Ti T =Ti+1 non-zeroelements. Therearetherefore(cid:80)K−2Ti+1 non-zeroelementsinthe
p=1 i=1
entirePT block.
(cid:104) T (cid:105)
PTR block:Thesizeofthisblockis (TK−1 1) TK. Thegreenblockcontainedin
T 1 − ×
PTR thatcontainsnon-zeroelementsisofs−izeTK−1 TK. SinceeachsentenceofsizeK 1can
becompletedwithnon-zeroprobability,byanyother× token,thereareatotalof(cid:80)TK−1 T−
= TK
p=1
non-zeroelements.
PR block: The size of this block is TK TK. Each sentence v = (v 1,...,v K) of size K is
×
mappedtoanothersentencev′ = (v′,...,v′ )ofsizeK withnon-zeroprobability,ifandonlyif
1 K
v′ =v ,v′ =v ,...,v′ =v . Thefinaltokenv′ canbyanyothertokeninthevocabulary. It
1 2 2 3 k−1 K K
meansthatthereareatotalof(cid:80)TK
T =TK+1non-zeroelements.
p=1
0′sblock:Obviously,therearenonon-zeroelementsinthisblock.
Finally,thereare
K (cid:88)−2 (cid:88)K (cid:18) TK 1(cid:19)
Ti+1+TK +TK+1 = Ti+1 =T2 −
T 1
i=1 i=1 −
non-zeroelements. Thismeansthattheproportionofnon-zeroelementsinthematrixisexactly
(cid:16) (cid:17)
T2 TK−1
T−1 T 1
= − .
(cid:16) (cid:16) (cid:17)(cid:17)2 TK 1
T TK−1 −
T−1
NotethatforlargeT andK wehavethat
T 1 1
− .
TK 1 ∼ TK−1
−
D.2 PROOFOFPROPOSITION3.2
Webeginwithapreliminarylemma.
LemmaD.1(PowersofQ greaterthanK). Foranyinitialstatei,thefollowinghold:
f
• k K, j T,(Qk) =0,
∀ ≥ ∀ ∈ f i,j
• k K, j R,(Qk) >0.
∀ ≥ ∀ ∈ f i,j
Proof. ByconsideringQ asdefinedin(6),wecancomputeitspowers. Foranyk 1,
f
≥
 
Pk B
T k
Qk = ,
f
0 Pk
R
whereB
k
=(cid:80)k m− =1 0P TmPTRP Rk−1−m.
Toprovethefirstitem,wefocusontheblocksontheleftofQ . Sincethelowerleftblockiszero,
f
wehavethat k 1, i R, j T,(Qk) = 0. Intheupperleftblock,theelement(Pk)
designatesthe∀ pro≥ babil∀ ity∈ ofmo∀ vin∈ gfromonf ei t, rj ansientstatei T toanothertransientstatejT Ti,j
∈ ∈
after k iterations. According to Definition B.1, if state i is a sequence of p 1 tokens, state j
≥
isnecessarilyasequenceofmin K,p+K = K elements. Thus,PT isanilpotentmatrixand
k K, i,j,(Pk) =0. This{ provesthat} k K, j T,(Qk) =0.
∀ ≥ ∀ T i,j ∀ ≥ ∀ ∈ f i,j
23Preprint. Underreview.
Wenowmoveontotheseconditem. Fromtheabove, ∀k
≥
K,B
k
= (cid:80)K m=− 01P TmPTRP Rk−1−m.
Notethatthissumisfinite,butthereisstilladependenceonk,inthepowersofthematrixPR. In
thelowerrightblock,theelement(Pk) designatestheprobabilityofmovingfromonerecurrent
R i,j
statei R toanotherrecurrentstatej R afterkiterations. AccordingtothedefinitionofQ in
f
Propos∈ ition3.1andDefinitionB.2, k ∈ K, i,j R2,(Pk) isnonzero. Exploitingthisalsoin
R i,j
B ,weobtaintheresult,i.e. k
K∀
,
≥
j
R∀ ,(Q∈
k) >0.
k ∀ ≥ ∀ ∈ f i,j
WearenowreadytoproveProposition3.2,whichisinspiredby Gallager(1996).
ProofofProposition3.2. ThestatesoflengthstrictlylessthanequalK(elementsofT)aretransient,
becauseofDefinitionB.1.TodiscussthenatureofstatesoflengthK(elementsofR),letusintroduce
aresultregardingthepowersoftheQ matrixasdefinedin(6). ThankstoLemmaD.1,thesetof
f
statesR (i.e. thestatesoflengthK)formaclass. LemmaD.1givesusalsothatR isergodic. In
fact,everystateinR onlycommunicateswithalltheotherstatesinR,whichprovestherecurrence.
Since i,j R2,(QK) >0,wecanmovebetweenanytwostatesinexactlyK steps,regardless
ofthe∀ initia∈ lpositionf . Ti h,j isensuresthatR isaperiodicbecausethetransitionprobabilitiesdonot
dependonaspecificcycle,andstatescanberevisitedatvarioustimesteps,notjustmultiplesof
aparticularnumber. Moresimply,byconsideringatokenx,thestatedefinedasi = xx...xhas
(cid:124) (cid:123)(cid:122) (cid:125)
Ktimes
period1,i.e. (Q ) >0. ThisisaconsequenceofDefinitionB.2andProposition3.1. Thanksto
f i,i
TheoremC.9,itmeansthatthewholeclassR isaperiodic. Finally,thismeansthatMC( ∗,Q )are
VK f
ergodicunichains,inthesenseofDefinitionC.11.
D.3 PROOFOFPROPOSITION3.3
WestartbyintroducingthreetechnicallemmasthatwillbeusefulintheproofofProposition3.3. We
startwiththeChapman–Kolmogorovequation.
LemmaD.2(Chapman-Kolmogorovequation). LetP beamatrixofsized. Then,P satisfies
d
i,j [d]2, n,n′ N2,(Pn+n′ ) =(cid:88) (Pn) (Pn′ ) .
i,j i,k k,j
∀ ∈ ∀ ∈
k=1
Proof. Theresultfollowsfromthefactthat n,n′ N2,Pn+n′ =PnPn′.
∀ ∈
Then,weintroduceasimplebutusefulresultofmonotonicity.
LemmaD.3(Lemma3.3.1. in Gallager(1996)). LetthetransitionmatrixP ofafinitestate
Markovchain. Then,forallstatesj andn 1,wehave
≥
max(Pn+1) max(Pn) , and min(Pn+1) min(Pn) .
i,j i,j i,j i,j
i ≤ i i ≥ i
WenowrefertoaresultonMarkovchainswithpositivetransitionmatrices.
LemmaD.4(Lemma3.3.2. inGallager(1996)). LetthetransitionmatrixP ofafinitestate
Markovchainsatisfy i,j,P >0,andletα=minP >0. Then,forallstatesjandn 1,
i,j i,j
∀ i,j ≥
(cid:16) (cid:17)
max(Pn) min(Pn) (1 2α) max(Pn) min(Pn) ,
i,j i,j i,j i,j
i − i ≤ − i − i
max(Pn) min(Pn) (1 2α)n,
i,j i,j
i − i ≤ −
lim max(Pn) = lim min(Pn) >0.
i,j i,j
n→∞ i n→∞ i
WearenowreadytoproveProposition3.3usingasimilarargumentasinGallager(1996).
24Preprint. Underreview.
ProofofProposition3.3. LetT andR denoterespectivelythesetsoftransientandrecurrentstates.
For any state j, we define π := lim max (Qn) = lim min (Qn) . Then π =
j n→∞ i f i,j n→∞ i f i,j
(π ) isthestationarydistributionforQ .
j j∈Ω f
Step1: Stationarydistributionfortransientstates. LemmaD.1givesusthat i, k K, j
T,(Qk) = 0. Thismeansthat j T,π = 0andhencethelimitisreached∀ at∀ mo≥ stafte∀ rK∈
f i,j ∀ ∈ j
iteration.
Step2:Stationarydistributionforrecurrentstates. LemmaD.1givesus i,j R2,(QK) >
∀ ∈ f i,j
0. Bydefiningε:= min (QK) ,LemmaD.4showsthatforanyintegerℓ 1,
i,j∈R2 f i,j ≥
(cid:18) (cid:19)
max(QℓK) min(QℓK) (1 2ε) max(QℓK) min(QℓK) , (7)
i∈R f i,j −i∈R f i,j ≤ − i∈R f i,j −i∈R f i,j
max(QℓK) min(QℓK) (1 2ε)ℓ, (8)
i∈R f i,j −i∈R f i,j ≤ −
lim max(QℓK) = lim min(QℓK) >0. (9)
ℓ→∞ i∈R f i,j ℓ→∞i∈R f i,j
ThankstoLemmaD.3,max(Qn+1) isnon-decreasinginn,sothelimitontheleftinEq.(9)can
f i,j
i
bereplacedwithalimitinn. Thesameargumentforthelimitontherightgivesthat, j R,
∀ ∈
max(Qn) min(Qn) (1 2ε)⌊n/K⌋,
i∈R f i,j −i∈R f i,j ≤ −
lim max(Qn) = lim min(Qn) >0,
n→∞ i∈R f i,j n→∞i∈R f i,j
wherewehavetakenthefloorfunctiontoalsoconverttheresultof(8). Sinceπ liesbetweenthe
j
minimumandmaximum(Qn) foreachn,wehavethat i,j R2,
f i,j ∀ ∈
|(Qn f)
i,j
−π
j
|≤(1 −2ε)⌊ Kn⌋.
Itmeansthat i,j R2,π =lim (Qn) . Thisalsogivesustheconvergenceratewhenthe
∀ ∈ j n→∞ f i,j
initialstateiisrecurrent. Inthenextstep,weconsiderthegeneralconvergencerate,regardlessofthe
natureoftheinitialstatei.
Step3: Convergencebound. Weproceedtotheremainingcase,i.e. thecasewheretheinitialstate
i T andthefinalstatej R. LemmaD.2saysthat n K,
∈ ∈ ∀ ≥
(cid:88) (cid:88)
(Qn) = (QK) (Qn−K) + (QK) (Qn−K) .
f i,j f i,k f k,j f i,k f k,j
k∈T k∈R
Wethenhavethat i T, n N,
∀ ∈ ∀ ∈
(Qn) π (cid:12) (cid:12) (cid:88) (QK) (cid:2) (Qn−K) π (cid:3) + (cid:88) (QK) (cid:2) (Qn−K) π (cid:3)(cid:12) (cid:12)
| f i,j − j |≤(cid:12) f i,k f k,j − j f i,k f k,j − j (cid:12)
k∈T k∈R
≤ (cid:88) (QK f ) i,k(cid:12) (cid:12)(Qn f−K) k,j −π j(cid:12) (cid:12)+ (cid:88) (QK f ) i,k(cid:12) (cid:12)(Q fn−K) k,j −π j(cid:12) (cid:12)
k∈T k∈R
≤ (cid:88) (QK f ) i,k+ (cid:88) (QK f ) i,k(cid:12) (cid:12)(Qn f−K) k,j −π j(cid:12) (cid:12)
k∈T k∈R
(1
2ε)⌊n− KK⌋,
≤ −
wherethefirstsumvanishesand(cid:80) (QK) 1. Finally, i T, n K,
k∈R f i,k ≤ ∀ ∈ ∀ ≥
|(Qn f)
i,j
−π
j
|≤(1 −2ε)⌊ Kn⌋−1.
CombiningthiswiththeresultofStep2concludestheproof.
D.4 PROOFOFTHEOREM4.1
Inthissection,wedetailtheproofofTheorem4.1. Weprovidebelowanoverviewoftheproofbefore
detailingit.
25Preprint. Underreview.
Overview of the proof. We are going to use McDiarmid’s inequality for dependent random
variablesofPaulin(2015,Theorem2.9). ToadapttheargumentsofPaulin(2015,Theorem2.9)to
oursetting,weboundthetotalvariationbetweenthetrueprobabilityofthenexttokenandtheone
estimatedbytheLLM.Therestofthissectionisorganizedasfollows. FirstinAppendixD.4.1,we
adapttheconcentrationinequalityofPaulin(2015,Theorem2.9). TheninAppendixD.4.2,weshow
howtoboundthetotalvariationbetweenthetrueandtheestimatedprobabilityofthenexttoken. ,in
AppendixD.4.3,werestateTheorem4.1andconcludetheproof.
D.4.1 CONCENTRATIONINEQUALITIESFORDEPENDENTRANDOMVARIABLES
Wefirststateaconcentrationinequalityfordependentrandomvariablesthatwillbeusedtoobtain
ourfinalbound.
Proposition D.5 (McDiarmid’s inequality for dependent random variables). Let S :=
(S ,...,S ) be a sequence of random variables that take values in Ω = Ω ... Ω .
1 N 1 N
× ×
AssumethereexistsaMartoncouplingforS withmixingmatrixΓ. Let Γ betheoperator
normofΓ. Iff: Ω Rissuchthatthereexistsc RN satisfying ∥ ∥
→ ∈
N
(cid:88)
x,y Ω, f(x) f(y) c 1 ,
∀ ∈ − ≤
i {xi̸=yi}
i=1
thenwehaveforanyu 0,
≥
(cid:18) 2u2 (cid:19)
P(f(S) E [f(S)] u) 2exp − .
| − S |≥ ≤ Γ 2 c 2
∥ ∥ ∥ ∥2
Proof. Considerafunctionf verifyingthepropertiesofPropositionD.5. Paulin(2015,Theorem2.9)
ensuresthatforapartitionSˆofS (seePaulin,2015,Definition2.3)thefollowinginequalityholds
(cid:16) (cid:104) (cid:105) (cid:17) (cid:18) 2u2 (cid:19)
u 0, P f(Sˆ) E f(Sˆ) u 2exp − , (10)
∀ ≥ | − |≥ ≤ Γ C(c) 2
∥ · ∥2
where C(c) is a vector of RN whose i-th element isthe sum of the c such that j is an index of
j
theelementsofSˆ . TakingthetrivialpartitionSˆ = S impliesthattheindexoftheelementsinSˆ
i i
arereducedto i . Hencethei-thentryofC(c)isequaltoc andC(c)=c. Bydefinitionofthe
i
{ }
operatornorm(naturallyinducedbytheℓ -norm),wehave
2
Γc Γx
Γ c = ∥ ∥2 c sup∥ ∥2 c Γ c ,
∥ · ∥2 c ·∥ ∥2 ≤ x ·∥ ∥2 ≤∥ ∥·∥ ∥2
2 x̸=0 2
∥ ∥ ∥ ∥
(cid:124) (cid:123)(cid:122) (cid:125)
=∥Γ∥
wherethefirstinequalitycomesfromthefactthatcisnon-zero(otherwisetheonlypossiblef is
thezerofunctionwhichisnotofgreatinterest). Usingthefactthatthefunctionx exp(
2u2
)is
→ − x
increasing,weobtain
(cid:18) 2u2 (cid:19) (cid:18) 2u2 (cid:19)
exp − exp − ,
Γ c 2 ≤ Γ 2 c 2
∥ · ∥2 ∥ ∥ ·∥ ∥2
whichconcludestheproof.
Bylookingatthedefinitionoftherisk (cid:98)pre(Θ),wecanseethatapplyingPropositionD.5tothe
R
function
1 N (cid:88)train
f: (S ,...,S )= d (P ( S ),P ( S )),
1 Ntrain
N
TV L
·|
n Θ
·|
n
train
n=1
wouldleadtothedesiredboundaswealreadyknowS admitsaMartoncouplingwithmixingmatrix
Γ. WeinvestigateinthenextsectionhowtofindtheboundingvectorctoapplyPropositionD.5.
26Preprint. Underreview.
D.4.2 FINDINGTHEBOUNDINGVECTOR
Technicallemmas. Wefirstrecallthefollowingimportantnotionsfrom(Tsybakov,2008). Let
(Ω, )beameasurespaceandconsidertwoprobabilitydistributionsP,Qdefinedon(Ω, ). For
anyF
σ-finitemeasureν on(Ω,
)suchthatP,Qareabsolutelycontinuouswithrespecttoν,F
wecan
definep= dP ,q = dQ whichcF analsobewrittenasP(dω)=q(ω)ν(dω)andQ(dω)=p(ω)ν(dω).
dν dν
Wewilladoptbothnotationsinterchangeably. Itshouldbenotedthattherealwaysexistsatleastone
suchmeasureν asonecantakeν = P+Q. Withthesenotations,thesquaredHellingerdistance
betweenPandQisdefinedas
H(P,Q)2 :=(cid:90) (cid:16)(cid:112) p(ω) (cid:112) q(ω)(cid:17)2 ν(dω)=(cid:90) (cid:16)(cid:112) P(dω) (cid:112) Q(dω)(cid:17)2 .
− −
ω∈Ω ω∈Ω
Thelemmabelowshowsthatthetotalvariationbetweentwoprobabilitydistributionsiscontrolled
fromabovebytheabsolutevalueofthelogarithmoftheirratio.
LemmaD.6. ConsidertwoprobabilitydistributionsP,Qdefinedonameasurespace(Ω, )
F
andaσ-finitemeasureν on(Ω, ). Letp,qbethecorrespondingprobabilitiesdensities,i.e.,
wehaveP(dω)=q(ω)ν(dω)anF dQ(dω)=p(ω)ν(dω),thetotalvariationbetweenPandQ
satisfies
(cid:32) (cid:12) (cid:115) (cid:12) (cid:33)1/2
(cid:90) (cid:12) P(dω)(cid:12)
d (P,Q) 2 (cid:12)log (cid:12)q(ω)dν(dω) .
TV
≤
ω∈Ω(cid:12)
(cid:12)
Q(dω)(cid:12)
(cid:12)
(cid:12) (cid:113) (cid:12)
Ifthereexistsanon-negativeconstantB suchthatforanyz Ω,(cid:12)log P(z)(cid:12) B,thenwe
∈
(cid:12) Q(z)(cid:12)
≤
have
d (P,Q) √2B.
TV
≤
Proof. We have the following relation between the total variation and the Hellinger distance (cf.
Tsybakov,2008,Lem.2.3,Chapt.2,p.86):
 
d (P,Q)2 H(P,Q)2 1 H(P,Q)2/4 H(P,Q)2, (11)
TV ≤ · − ≤
(cid:124) (cid:123)(cid:122) (cid:125)
≥0
wherethelastinequalityusesthepositivityoftheHellingerdistance. Inspiredbythedecomposition
oftheHellingerdistancein(Agarwaletal.,2020,Lem.25),wehave
H(P,Q)2 =(cid:90) (cid:16)(cid:112) P(dω) (cid:112) Q(dω)(cid:17)2 =(cid:90) (cid:16) P(dω)+Q(dω) 2(cid:112) P(dω)(cid:112) Q(dω)(cid:17)
− −
ω∈Ω ω∈Ω
(cid:18) (cid:90)
(cid:112) (cid:112)
(cid:19) (cid:32) (cid:90) (cid:115) P(dω) (cid:33)
=2 1 P(dω) Q(dω) =2 1 Q(dω)
· − · − Q(dω)
ω∈Ω ω∈Ω
(cid:32) (cid:115) (cid:33)
(cid:90) P(dω)
=2 1 q(ω)dν(dω) (bydefinitionofQ(dω))
· − Q(dω)
ω∈Ω
(cid:32) (cid:115) (cid:33)
(cid:90) P(dω)
2log q(ω)dν(dω) (using1 x log(x))
≤− Q(dω) − ≤−
ω∈Ω
ItfollowsusingEq.(11)
d (P,Q)2 H(P,Q)2
TV
≤
(cid:32)(cid:115) (cid:33)
(cid:90) P(dω)
2 log q(ω)dν(dω) (byJensenas logisconvex)
≤ − Q(dω) −
ω∈Ω
(cid:12) (cid:32)(cid:115) (cid:33) (cid:12)
(cid:12)(cid:90) P(dω) (cid:12)
2(cid:12) log q(ω)dν(dω)(cid:12)
≤ (cid:12) (cid:12) ω∈Ω− Q(dω) (cid:12) (cid:12)
27Preprint. Underreview.
(cid:12) (cid:32)(cid:115) (cid:33)(cid:12)
(cid:90) (cid:12) P(dω) (cid:12)
2 (cid:12) log (cid:12)q(ω)dν(dω) (byJensenas isconvex)
≤ ω∈Ω(cid:12) (cid:12)− Q(dω) (cid:12) (cid:12) |·|
(cid:12) (cid:32)(cid:115) (cid:33)(cid:12)
(cid:90) (cid:12) P(dω) (cid:12)
2 (cid:12)log (cid:12)q(ω)dν(dω) (firstpartofLemmaD.6)
≤ ω∈Ω(cid:12) (cid:12) Q(dω) (cid:12) (cid:12)
(cid:124) (cid:123)(cid:122) (cid:125)
≤B
(cid:90)
2B q(ω)dν(dω) 2B. (secondpartofLemmaD.6)
≤ ≤
ω∈Ω
(cid:124) (cid:123)(cid:122) (cid:125)
=1
Thisconcludesbothpartsoftheproof.
The next lemma provides a lower bound on the softmax output if its input is upper-bounded (in
ℓ -norm).
1
LemmaD.7. Letx Rmbesuchthat x c forsomec >0. Then,wehave
1 1 1
∈ ∥ ∥ ≤
1
softmax(u) ,
≥ mexp(2c )
1
wheretheinequalityholdsforeachcomponentofsoftmax(u).
Proof. Usingthefactthat
m
(cid:88)
x = x c ,
1 i 1
∥ ∥ | |≤
i=1
weknowthatforanyi [m],wehave
∈
c x c .
1 i 1
− ≤ ≤
Hence,usingthefactthattheexponentialisincreasing,wehaveforanyi [m]
∈
exp( c ) exp(x ) exp(c ). (12)
1 i 1
− ≤ ≤
Summingandtakingtheinverseleadsto
m m m
(cid:88) (cid:88) (cid:88)
exp( c ) exp(x ) exp(c )
1 j 1
− ≤ ≤
i=1 i=1 i=1 (13)
1 1 1
⇐⇒
(cid:80)m
exp(c ) ≤
(cid:80)m
exp(x ) ≤
(cid:80)m
exp( c
).
j=1 1 j=1 j j=1 − 1
CombiningEq.(12)andEq.(13)yields
exp( c ) exp(x ) exp(c )
1 i 1
(cid:80)m e−
xp(c ) ≤
(cid:80)m
exp(x ) ≤
(cid:80)m
exp( c
).
j=1 1 j=1 j j=1 − 1
Aswedesirealowerbound,weonlyfocusontheleft-handsideofthepreviousinequality.Multiplying
thenumeratoranddenominatorbyexp(c )leadsto
1
exp(x ) 1
∀i ∈[m], softmax(x) i = (cid:80)m expi (x ) ≥ mexp(2c ),
j=1 j 1
whichconcludestheproof.
Upper-boundingthetotalvariation. Wenowproceedwithfindinganupperboundonthetotal
variationbetweenthetrueprobabilityofthenexttokenandtheoneestimatedbytheLLMf . Itwill
Θ
enableustofindtheboundingvectorc. Thenextlemmashowsthattheinputofthesoftmaxlayerof
themodelisbounded.
28Preprint. Underreview.
LemmaD.8. ConsideranLLMf . ForanyinputsequenceS Rr×n,thefollowing
Θ
∈ F ∈
inequalityholds
1 1
W S(L)1 W⊤ ,
∥nτ
U n ∥1
≤ τ∥
U∥2,1
whereτ isthetemperature,W istheunembeddingmatrix(whichisboundedasstatedinthe
U
definitionoftheparametersspace ),andS(L)istheoutputofthelasttransformerlayer.
W
Proof. Werecallthatthelayernormalizationensuresthatateachlayer,thetokensareintheunit
ℓ -ball. Thisis,inparticular,thecasefortheoutputofthelastlayerS(L). Itmeansthatthecolumns
2
ofS(L)verifies
k [n], S(L) 1, (14)
∀ ∈ ∥ ·,k ∥2 ≤
whichimplies
max S(L) 1. (15)
1≤k≤n∥ ·,i ∥2 ≤
RecallingthattheL -normofamatrixA Rn×mcanberewrittenas
p,q
∈

m
(cid:32)
n
(cid:33) pq q1
(cid:88) (cid:88)
∥A ∥p,q := |A ij |p  = ∥( ∥A ·,j ∥p)m j=1∥q, (16)
j=1 i=1
theℓ -normofthelastlayerbeforethesoftmaxlayersatisfies
1
(cid:12) (cid:12) (cid:12) (cid:12)
T (cid:12) r n (cid:12) T (cid:12) r n (cid:12)
1 W US(L)1 n 1 = 1 (cid:88)(cid:12) (cid:12)(cid:88) W ij(cid:88) S jk(cid:12) (cid:12)= 1 (cid:88)(cid:12) (cid:12)(cid:88)(cid:88) W ijS jk(cid:12) (cid:12)
∥nτ ∥ nτ (cid:12) (cid:12) nτ (cid:12) (cid:12)
i=1(cid:12)j=1 k=1 (cid:12) i=1(cid:12)j=1k=1 (cid:12)
T r n
1 (cid:88)(cid:88)(cid:88)
W S (triangularinequality)
ij jk
≤ nτ | |
i=1j=1k=1
T n
≤
n1
τ
(cid:88)(cid:88)(cid:12) (cid:12)W i⊤S ·,k(cid:12) (cid:12)
i=1k=1
T n
1 (cid:88)(cid:88)
W S (Cauchy-Schwartzinequality)
i 2 ·,k 2
≤ nτ ∥ ∥ ∥ ∥
i=1k=1
T n T
1 (cid:88)(cid:88) 1 (cid:88)
W max S n max S W
i 2 ·,k 2 ·,k 2 i 2
≤ nτ ∥ ∥ 1≤k≤n∥ ∥ ≤ nτ 1≤k≤n∥ ∥ ∥ ∥
i=1k=1 i=1
T
1 (cid:88) 1
W W⊤ (byEq.(15)andthedef. ofL inEq.(16))
≤ τ ∥
i ∥2
≤ τ∥
U∥2,1 2,1
i=1
wherewedroppedthesubscriptandsuperscriptonW andS(L)toeasethenotations.Thisconcludes
U
theproof.
Thepreviouslemmacanbeusedtoshowthatthelogarithmoftheratiobetweenthetrueprobabilityof
thenexttokenandtheoneestimatedbytheLLMf isupperboundedasafunctionofthevocabulary
Θ
size T, the temperature, the upper-bound on W and some constant related to the ambiguity of
U
language(seeEq.(1)).
PropositionD.9(Upper-boundonthelogarithm). ConsideranLLMf withvocabulary
Θ
∈F
sizeT. WerecallthatB istheupperboundonthenormofW inthedefinitionofparameter
U U
space ,τ isthesoftmaxtemperatureandc istheconstantrelatedtotheambiguityoflanguage
0
W
29Preprint. Underreview.
(seeEq.(1)). Wehave
n [N],
(cid:12)
(cid:12)
(cid:12)log(cid:18)P
L(X n+1 |S
n)(cid:19)(cid:12)
(cid:12) (cid:12) B¯ =max log(T)+ 2B U
,log(cid:18)
1
(cid:19)
.
∀ ∈ (cid:12) P (X S ) (cid:12)≤ { τ c }
Θ n+1 n 0
|
Proof. The main idea of the proof is to bound the probability ratio and use the fact that log is
non-decreasing. Letn [N]. Themodelf receivesasinputsequencesoftokensS ofsizen K.
Θ n
∈ ≤
Wefirstlower-boundeachtermoftheprobabilityratio. FromEq.(1),wehave
P (X S ) c . (17)
L n+1 n 0
| ≥
WewanttoobtainasimilarinequalityforP (X S ). AstheparametersΘoftheLLMarein
Θ n+1 n
,weknowthat W⊤ B . LemmaD.8ensu| resthat
W ∥
U∥2,1
≤
U
1 1 B
W S(L)1 W⊤ U .
∥nτ
U n ∥1
≤ τ∥
U∥2,1
≤ τ
WecanthenapplyLemmaD.7withc = BU andgiventhat 1 W S(L)1 RT,itleadsto
1 τ nτ U n ∈
(cid:18) (cid:19)
1 1
P ( S )=softmax W S(L)1 ,
Θ n U n
·| nτ ≥ T exp(2B /τ)
U
where the inequality holds for each component of P ( S ). This is in particular the case for
Θ n
P (X S )whichistheentryweareinterestedin,i.· e| .,wehave
Θ n+1 n
|
1
P (X S ) . (18)
Θ n+1 n
| ≥ T exp(2B /τ)
U
Goingbacktotheratioofprobability,considerthesituationwherewehave
P (X S )
L n+1 n
| 1.
P (X S ) ≥
Θ n+1 n
|
Then,usingEq.(18),wehave
P (X S ) 1
L n+1 n
1 | T exp(2B /τ),
≤ P (X S ) ≤ P (X S ) ≤ U
Θ n+1 n Θ n+1 n
| |
whichimplies,asthelogisnon-decreasingmonotonically,
(cid:18)P
(X S
)(cid:19)
2B
0 log L n+1 | n log(T exp(2B /τ))=log(T)+ U . (19)
≤ P (X S ) ≤ U τ
Θ n+1 n
|
Similarly,considerthecasewherewehave
P (X S )
L n+1 n
| 1.
P (X S ) ≤
Θ n+1 n
|
Then,wehave
P (X S )
Θ n+1 n
| 1,
P (X S ) ≥
L n+1 n
|
andsimilarlytoabove,wecanuseEq.(17)toobtain
P (X S ) 1 1
Θ n+1 n
1 | .
≤ P (X S ) ≤ P (X S ) ≤ c
L n+1 n L n+1 n 0
| |
Thisimplies
(cid:18)P
(X S
)(cid:19) (cid:18)
1
(cid:19)
Θ n+1 n
0 log | log ,
≤ P (X S ) ≤ c
L n+1 n 0
|
whichalsorewrites
(cid:18)P
(X S
)(cid:19) (cid:18)
1
(cid:19)
0 log L n+1 | n log . (20)
≤− P (X S ) ≤ c
Θ n+1 n 0
|
Bydefinitionoftheabsolutevalue,combiningEqs.(19)and(20)leadsto
(cid:12)
(cid:12)
(cid:18)P
L(X n+1 S
n)(cid:19)(cid:12)
(cid:12) 2B U
(cid:18)
1
(cid:19)
(cid:12)log | (cid:12) max log(T)+ ,log .
(cid:12) P (X S ) (cid:12)≤ { τ c }
Θ n+1 n 0
|
Thisconcludestheproof.
Wearenowreadytoupper-boundthetotalvariation.
30Preprint. Underreview.
CorollaryD.10(Upper-boundonthetotalvariation). ConsideranLLMf withvocab-
Θ
∈F
ulary size T. We recall that B is the upper bound on the norm of W in the definition of
U U
parameterspace ,τ isthesoftmaxtemperatureandc istheconstantrelatedtotheambiguity
0
W
oflanguage(seeEq.(1)). Forn [N],wehave
∈
(cid:115)
(cid:18) (cid:19)
2B 1
d (P ( S ),P ( S )) 2max log(T)+ U ,log :=c . (21)
TV L n Θ n 2
·| ·| ≤ { τ c }
0
Proof. Using Proposition D.9, we can directly apply Lemma D.6 with B = max log(T) +
(cid:16) (cid:17) {
2BU,log 1 foranyn [N]. Thisleadsto
τ c0 } ∈
(cid:115)
(cid:18) (cid:19)
2B 1
n [N], d (P ( S ),P ( S )) 2max log(T)+ U ,log .
TV L n Θ n
∀ ∈ ·| ·| ≤ { τ c }
0
Thisconcludestheproof.
D.4.3 CONCLUDINGTHEPROOF
Wearenowreadytostateourmainresult.
TheoremD.11(RestatementofTheorem4.1). ConsideranLLMf withvocabularysize
Θ
∈F
T. WedenotebyΓthemixingmatrixofthepretrainingsequencesoftokens(S ,...,S ).
1 Ntrain
Letδ >0. Then,withprobabilityatleast1 δ,
−
(cid:115)
B¯ (cid:18) 2(cid:19)
pre(Θ) (cid:98)pre(Θ)+ log ,
R ≤R √N δ
train
whereB¯ isaconstantdependingontheparametersoftheproblem. Moreprecisely,
(cid:115)
(cid:18) (cid:19)
2B 1
B¯ =2 Γ max log(T)+ U ,log .
∥ ∥ { τ c }
0
ProofofTheorem4.1. Bydefinitionoftherisk,wehave
1 N (cid:88)train 1 N (cid:88)train
(cid:98)pre(Θ)= d TV(P L( S n),P Θ( S n))= g n(S n)
R N ·| ·| N
train (cid:124) (cid:123)(cid:122) (cid:125) train
n=1 n=1
=gn(Sn)
=f(S ,...,S )=f(S).
1 Ntrain
UsingCorollaryD.10,weknowthat
(cid:115)
(cid:18) (cid:19)
2B 1
g (S ) 2max log(T)+ U ,log :=c .
n n 2
| |≤ { τ c }
0
By definition, each sequence of tokens S takes its values in n (again by abuse of notation,
n
n = min n,K )andS takesitsvaluesin 1 ... Ntrain. FV oranytwosequencesζ,Σwith
valuesin{1 .} .. Ntrain,wehave V × ×V
V × ×V
 
1 N (cid:88)train
f(ζ) f(Σ)= d (P ( ζ ),P ( ζ )) d (P ( Σ ),P ( Σ ))
− N  TV L ·| n Θ ·| n − TV L ·| n Θ ·| n 
train (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
n=1
=gn(ζn) =gn(Σn)
1 N (cid:88)train
= (g (ζ ) g (Σ ))
n n n n
N −
train
n=1
31Preprint. Underreview.
1 N (cid:88)train
= (g (ζ ) g (Σ ))1 (removingthezeroterms)
N
n n
−
n n {ζn̸=Σn}
train
n=1
(cid:12) (cid:12)
(cid:12) 1 N (cid:88)train (cid:12)
(cid:12) (g (ζ ) g (Σ ))1 (cid:12)
≤(cid:12) (cid:12)N train n n − n n {ζn̸=Σn}(cid:12) (cid:12)
n=1
≤
N1 N (cid:88)train (cid:12) (cid:12)(g n(ζ n) −g n(Σ n))1 {ζn̸=Σn}(cid:12) (cid:12)
train
n=1
1 N (cid:88)train
g (ζ ) g (Σ ) 1
≤ N |
n n
−
n n
|
{ζn̸=Σn}
train
n=1
 
1 N (cid:88)train
g (ζ ) + g (Σ )1 (CorollaryD.10)
≤ N | n n | | n n | {ζn̸=Σn}
train (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
n=1
≤c2 ≤c2
1 N (cid:88)train
2c 1
=N (cid:88)train(cid:18) 2c
2
(cid:19)
1 ,
≤ N
2 {ζn̸=Σn}
N
{ζn̸=Σn}
train train
n=1 n=1
(cid:114)
(cid:16) (cid:17)
wherec = 2max log(T)+ 2BU,log 1 . Asζ andΣweretakenarbitrary, choosingc
2 { τ c0 } ∈
RNtrain withallentriesequalto 2c2 ensuresthatf verifiestheconditioninPropositionD.5,i.e.,
Ntrain
N (cid:88)train
ζ,Σ, f(ζ) f(Σ) c 1 .
∀ − ≤
n {ζn̸=Σn}
n=1
WeassumedinSection4.1thatthesequencesS wererelatedviaaMartoncouplingwithmixing
n
matrixΓ. Puttingeverythingtogether,wecanapplyPropositionD.5whichleadsto
(cid:18) 2u2 (cid:19)
u 0, P(f(S) E [f(S)] u) 2exp − . (22)
∀ ≥ | − S |≥ ≤ Γ 2 c 2
∥ ∥ ∥ ∥2
Letu 0. Wehavethefollowingeventsordering
≥
(E [f(S)] f(S) u) (E [f(S)] f(S) u) (f(S) E [f(S)] u)
S S S
− ≥ ⊆ − ≥ ∪ − ≥
=(f(S) E [f(S)] u).
S
| − |≥
Hence,asuwastakenarbitraryandusingEq.(22),wehave
(cid:18) 2u2 (cid:19)
u 0, P(E [f(S)] f(S) u) 2exp − .
∀ ≥ S − ≥ ≤ Γ 2 c 2
∥ ∥ ∥ ∥2
Werecallthatbydefinition
(cid:104) (cid:105)
f(S)= (cid:98)pre(Θ)and pre(Θ)=E
S
(cid:98)pre(Θ) .
R R R
Sincethepreviousinequalityholdsforanyu 0,wecanhencechooseusuchthat
≥
(cid:18) 2u2 (cid:19) 2u2 (cid:18) δ(cid:19) 1 (cid:18) 2(cid:19)
δ =2exp − − =log u2 = Γ 2 c 2log
Γ 2 c 2 ⇐⇒ Γ 2 c 2 2 ⇐⇒ 2∥ ∥ ∥ ∥2 δ
∥ ∥ ∥ ∥2 ∥ ∥ ∥ ∥2
(cid:115)
(cid:18) (cid:19)
1 2
u= Γ c log .
2
⇐⇒ √2∥ ∥∥ ∥ δ
Usingthefactthat
c
=(cid:118) (cid:117) (cid:117) (cid:116)N (cid:88)train
c2
=(cid:118) (cid:117) (cid:117) (cid:116)N (cid:88)train(cid:18) 2c
2
(cid:19)2 =(cid:118) (cid:117) (cid:117) (cid:116)N (cid:88)train 4c2
2
=(cid:115) 4c2
2 =
2c
2
∥ ∥2 n N N2 N √N
n=1 n=1 train n=1 train train train
32Preprint. Underreview.
(cid:114)
(cid:16) (cid:17)
andusingthefactthatc = 2max log(T)+ 2BU,log 1 fromCorollaryD.10,weobtain
2 { τ c0 }
(cid:115) (cid:115)
1 2c (cid:18) 2(cid:19) √2c (cid:18) 2(cid:19)
u= 2 Γ log = 2 Γ log
√2√N ∥ ∥ δ √N ∥ ∥ δ
train train
(cid:114)
(cid:16) (cid:17)
2 ∥Γ
∥
max {log(T)+ 2B τU,log c1
0
}(cid:115) (cid:18) 2(cid:19) B¯ (cid:115) (cid:18) 2(cid:19)
= log = log ,
√N δ √N δ
train train
wherewedefine
(cid:115)
(cid:18) (cid:19)
2B 1
B¯ =2 Γ max log(T)+ U ,log .
∥ ∥ { τ c }
0
Puttingeverythingtogether,wehave
(cid:32) B¯ (cid:115) (cid:18) 2(cid:19)(cid:33)
P pre(Θ) (cid:98)pre(Θ) log δ.
R −R ≥ √N δ ≤
train
Takingtheoppositeeventleadstothefollowinginequalitywithprobabilityatleast1 δ
−
(cid:115)
B¯ (cid:18) 2(cid:19)
pre(Θ) (cid:98)pre(Θ)+ log ,
R ≤R √N δ
train
whichconcludestheproof.
D.5 PROOFOFCOROLLARY4.2
Asthelayernormisnotappliedanymore,eachtokenisnolongerintheℓ -unitball,andLemmaD.8
2
doesnotholdanymore. Wewanttoprovideananalogouslemmaforoursetting. Wefirstprovethe
followingtechnicallemmas.
LemmaD.12. TheReLUisanorm-decreasingoperator,i.e.,wehave
A Rn×m, ReLU(A) A ,
1,1 1,1
∀ ∈ ∥ ∥ ≤∥ ∥
wheretheReLUisappliedentry-wise.
Proof. Recalling that ReLU(x) = max 0,x is applied entry-wise, using the fact that
{ }
max 0,x x andconsideringAandA˜ =ReLU(A),wehave
| { }|≤| |
(cid:88) (cid:88) (cid:88)
A˜ = A˜ = max 0,A˜ A A ,
1,1 i,j i,j i,j 1,1
∥ ∥ | | | { }|≤ | |≤∥ ∥
i,j i,j i,j
whichconcludestheproof.
LemmaD.13. TheL -normverifiesthefollowingproperty:
1,1
A Rn×m,B Rm×p, AB n A B .
1,1 ∞ 1,1
∀ ∈ ∈ ∥ ∥ ≤ ∥ ∥ ∥ ∥
Proof. Wehave
p n p n m p n m
(cid:88)(cid:88) (cid:88)(cid:88)(cid:88) (cid:88)(cid:88)(cid:88)
AB = (AB) = A B A B
∥ ∥1,1 | ij| | ik kj |≤ | ik kj |
j=1i=1 j=1i=1 k=1 j=1i=1k=1
p n m p n m
(cid:88)(cid:88)(cid:88) (cid:88)(cid:88)(cid:88)
A B maxA B
ik kj ik kj
≤ | || |≤ ik | | | |
j=1i=1k=1 j=1i=1k=1
33Preprint. Underreview.
p m
(cid:88)(cid:88)
n A B n A B ,
∞ kj ∞ 1,1
≤ ∥ ∥ | |≤ ∥ ∥ ∥ ∥
j=1k=1
whichconcludestheproof.
LemmaD.14. TheL andL -normsverifythefollowingrelation
2,1 ∞,1
A Rn×m, A A .
∞,1 2,1
∀ ∈ ∥ ∥ ≤∥ ∥
Proof. BydefinitionoftheL -norm,wehave
p,q
(cid:88)M (cid:88)M (cid:114)
A = max A = max A2 (asx x2isincreasing)
∥ ∥∞,1 1≤i≤n| ij | 1≤i≤n| ij| →
j=1 j=1
(cid:118)
M (cid:117) n M
(cid:88)(cid:117)(cid:88) (cid:88)
≤ (cid:116) |A2 ij|≤ ∥A ·,j ∥2 ≤∥A ∥2,1,
j=1 i=1 j=1
wherethefirstinequalitycomesfromaddingnon-negativeterms.
WearenowreadytostatethelemmaanalogoustoLemmaD.8.
LemmaD.15. ConsideranLLMf F˜withLlayers. ForanyinputsequenceS Rr×n,the
Θ
∈ ∈
followinginequalityholds
1 c
W S(L)1 3 W⊤ ,
∥nτ
U n ∥1
≤ τ ∥
U∥2,1
where τ is the temperature and c is a constant depending on the parameters upper-bound.
3
Moreprecisely,
(cid:20) (cid:18) r3 (cid:19)(cid:21)L
c = (1+rmB B ) 1+ B B B .
3 1 2 O V tok
· H ·
W istheunembeddingmatrix(whichisboundedasstatedinthedefinitionoftheparameters
U
space ),andS(L)istheoutputofthelasttransformerlayer.
W
ProofofLemmaD.15. Ourmodelf ˜ isgivenasinputasequenceS Rr×n. Withsimilar
Θ
∈ F ∈
computationsthaninLemmaD.8,wehave
(cid:12) (cid:12) (cid:12) (cid:12)
T (cid:12) r n (cid:12) T (cid:12) r n (cid:12)
1 W US(L)1 n 1 = 1 (cid:88)(cid:12) (cid:12)(cid:88) W ij(cid:88) S jk(cid:12) (cid:12)= 1 (cid:88)(cid:12) (cid:12)(cid:88)(cid:88) W ijS jk(cid:12) (cid:12)
nτ∥ ∥ nτ (cid:12) (cid:12) nτ (cid:12) (cid:12)
i=1(cid:12)j=1 k=1 (cid:12) i=1(cid:12)j=1k=1 (cid:12)
T r n
1 (cid:88)(cid:88)(cid:88)
W S (triangularinequality)
ij jk
≤ nτ | |
i=1j=1k=1
T n T n
≤
n1
τ
(cid:88)(cid:88)(cid:12) (cid:12)W i⊤S ·,k(cid:12) (cid:12)
≤
n1
τ
(cid:88)(cid:88) ∥W i ∥∞ ∥S ·,k ∥1 (Ho¨lderinequality)
i=1k=1 i=1k=1
(cid:32) (cid:33) (cid:32) (cid:33)
T n
1 (cid:88) (cid:88) 1
W S W⊤ S(L)
≤ nτ ∥
i ∥∞
· ∥
·,k ∥1
≤ nτ∥
U∥∞,1
∥
∥1,1
i=1 k=1
1
W⊤ S(L) , (LemmaD.14)
≤ nτ∥
U∥2,1
∥
∥1,1
where,again,wedroppedthesubscriptandsuperscriptonW andS(L)toeasethenotations. We
U
obtain
1 1
W S(L)1 W⊤ S(L) . (23)
∥nτ
U n ∥1
≤ nτ∥
U∥2,1
∥
∥1,1
34Preprint. Underreview.
Aswedonotuselayernormalization,wewanttofindanotherwaytoboundS(L). Tothatend,we
willfirstexpressS(ℓ),theoutputofthe(ℓ)-thlayerofthetransformer,asafunctionofS(ℓ−1),the
outputofthe(ℓ 1)-thlayer. Usingthedefinitionofthetransformermodel(seeAppendixB),we
−
have
 (cid:16) (cid:17)
 Z(ℓ) =S(ℓ−1)+
A
(cid:16)S(ℓ−1);W Q( (cid:17)ℓ),W K(ℓ),W V(ℓ),W O(ℓ) ,
Y(ℓ) =W(ℓ)ReLU W(ℓ)Z(ℓ) ,

S(ℓ)
=Z(ℓ)2
+Y(ℓ).
1
Wewillcomputeeachlayer’sL -norm.
1,1
Step 1: MHA. By definition, denoting the number of heads by H, we know that
(cid:16) (cid:17)
S(ℓ−1);W(ℓ),W(ℓ),W(ℓ),W(ℓ) Rr×n multiplies W(ℓ) Rr×r with the concatenation
A Q K V O ∈ ∈
ontherowsoftheH softmaxlayersthateachwrites
(cid:18) (cid:16) (cid:17)⊤ (cid:19)(cid:16) (cid:17)
softmax W(ℓ)S(ℓ) W(ℓ)S(ℓ−1) /√r W(ℓ)S(ℓ−1) R Hr×n,
Q K V ∈
Wekeepthenotationsℓwithoutexplicatingtheindexoftheheadtoeasenotations. Denotingthe
concatenationontherowsbyC(ℓ) Rr×n,wehave
∈
(cid:16) (cid:17)
S(ℓ−1);W(ℓ),W(ℓ),W(ℓ),W(ℓ) = W(ℓ)C(ℓ) r W(ℓ) C(ℓ)
∥A Q K V O ∥1,1 ∥ O ∥1,1 ≤ ·∥ O ∥∞ ∥ ∥1,1
rB C(ℓ) . (definitionof ˜)
O 1,1
≤ ∥ ∥ W
Moreover,bydefinitionofC(ℓ),wehave
r r r r/H H H
(cid:88)(cid:88) (cid:88)(cid:88)(cid:88) (cid:88)
C(ℓ) = C(ℓ) = C(ℓ,h) = C(ℓ,h) , (24)
∥ ∥1,1 | ij | | ij | ∥ ∥1,1
j=1i=1 j=1 i=1h=1 h=1
whereC(ℓ,h) R Hr×nisthesoftmaxmatrixoftheh-thlayer. Werecallthatthesoftmaxmatrixisa
row-stochastic∈ matrixofR Hr×r soithasallvalueslowerthan1. Inthenextcomputations,wedrop
thehindexonthequery,key,andvaluematricestoeasethenotations. UsingLemmaD.13onthe
softmaxmatrixandonthevaluematrixW V(ℓ) ∈R Hr×r,wehave
(cid:18) (cid:16) (cid:17)⊤ (cid:19)(cid:16) (cid:17)
C(ℓ,h) = softmax W(ℓ)S(ℓ) W(ℓ)S(ℓ−1) /√r W(ℓ)S(ℓ−1)
∥ ∥1,1 ∥ Q K V ∥1,1
r (cid:18) (cid:16) (cid:17)⊤ (cid:19) (cid:16) (cid:17)
softmax W(ℓ)S(ℓ) W(ℓ)S(ℓ−1) /√r W(ℓ)S(ℓ−1)
≤ H ·∥ Q K ∥∞ ·∥ V ∥1,1
r (cid:16) (cid:17)
W(ℓ)S(ℓ−1) (thesoftmaxmatrixisrow-stochastic)
≤ H ·∥ V ∥1,1
r r (cid:16) r (cid:17)2
(W(ℓ) S(ℓ−1) B S(ℓ−1) . (definitionof ˜)
≤ H · H∥ V ∥∞ ∥ ∥1,1 ≤ H V ∥ ∥1,1 W
CombiningthepreviousinequalitywithEq.(24)leadsto
r2
C(ℓ) B S(ℓ−1) .
1,1 V 1,1
∥ ∥ ≤ H ∥ ∥
Intheend,themulti-headattentionnormverifies
(cid:16) (cid:17) r3
S(ℓ−1);W(ℓ),W(ℓ),W(ℓ),W(ℓ) B B S(ℓ−1) .
∥A Q K V O ∥1,1 ≤ H O V ∥ ∥1,1
Usingthetriangularinequality,weobtain
(cid:18) r3 (cid:19)
Z(ℓ) 1+ B B S(ℓ−1) . (25)
1,1 O V 1,1
∥ ∥ ≤ H ·∥ ∥
Step2: FF.WerecallthatW Rm×r andW Rr×m. Usingsimilarargumentstotheabove,
1 2
∈ ∈
wehave
(cid:16) (cid:17)
Y(ℓ) = W(ℓ)ReLU W(ℓ)Z(ℓ)
∥ ∥1,1 ∥ 2 1 ∥1,1
35Preprint. Underreview.
(cid:16) (cid:17)
r W(ℓ) ReLU W(ℓ)Z(ℓ) (LemmaD.13)
≤ ·∥ 2 ∥∞ ∥ 1 ∥1,1
r W(ℓ) W(ℓ)Z(ℓ) (LemmaD.12)
≤ ·∥ 2 ∥∞ ∥ 1 ∥1,1
r m W(ℓ) W(ℓ) Z(ℓ) (LemmaD.13)
≤ · ·∥ 2 ∥∞ ∥ 1 ∥∞ ∥ ∥1,1
rmB B Z(ℓ) . (definitionof ˜)
1 2 1,1
≤ ∥ ∥ W
Step3: outputlayer. Again,applyingthetriangularinequalityandusingthepreviousinequalityand
Eq.(25),wehave
S(ℓ) Z(ℓ) + Y(ℓ) (1+rmB B ) Z(ℓ)
1,1 1,1 1,1 1 2 1,1
∥ ∥ ≤∥ ∥ ∥ ∥ ≤ ∥ ∥
(cid:18) r3 (cid:19)
(1+rmB B ) 1+ B B S(ℓ−1) .
1 2 O V 1,1
≤ H ∥ ∥
Iteratingthroughthelayers,recallingthatS(0) =S,wefinallyobtain
(cid:20) (cid:18) r3 (cid:19)(cid:21)L
S(L) (1+rmB B ) 1+ B B S ,
1,1 1 2 O V 1,1
∥ ∥ ≤ H ∥ ∥
whereSistheinputsequence. CombiningthisinequalitywithEq.(23)leadsto
1 (cid:20) (cid:18) r3 (cid:19)(cid:21)L S (cid:18) 1 (cid:19)
W S(L)1 (1+rmB B ) 1+ B B ∥ ∥1,1 W⊤ .
∥nτ
U n ∥1
≤
1 2
H
O V
n τ∥
U∥2,1
Usingthefactthateachtokenhasaℓ -normboundedbyB . Hence,eachcolumnofSistooand
1 tok
wehave
n r n
1 1 (cid:88)(cid:88) 1 (cid:88)
S = S = S B .
1,1 ij ·,j 1 tok
n∥ ∥ n | | n ∥ ∥ ≤
(cid:124) (cid:123)(cid:122) (cid:125)
j=1i=1 j=1
≤Btok
Combiningthelasttwoinequalitiesconcludestheproof.
WecannowrestateCorollary4.2.
CorollaryD.16(RestatementofCorollary4.2). ConsideranLLMf ˜withvocabularysize
Θ
∈F
T composedofLtransformerblocksandH attentionheads. WedenotebyΓthemixingmatrix
ofthepretrainingsequencesoftokens(S ,...,S ). Letδ >0. Then,withprobabilityat
1 Ntrain
least1 δ,
− (cid:115)
B¯ (cid:18) 2(cid:19)
pre(Θ) (cid:98)pre(Θ)+ log ,
R ≤R √N δ
train
whereB¯ isaconstantdependingontheparametersoftheproblem. Moreprecisely,
(cid:115)
(cid:18) (cid:19)
2(B )L 1
B¯ =2 Γ max log(T)+ Θ ,log ,
∥ ∥ { τ c }
0
(cid:104) (cid:16) (cid:17)(cid:105)
withB = (1+rmB B ) 1+ r3 B B (B B )1/L.
Θ 1 2 H O V tok U
ProofofCorollary4.2. WefirstnotethattheonlychangefromLemmaD.8toLemmaD.15isthe
(cid:104) (cid:16) (cid:17)(cid:105)L
multiplicative constant c = (1+rmB B ) 1+ r3 B B B in front of 1 W⊤ . In
3 1 2 H O V tok τ∥ U∥2,1
particular, as we know that ˜ , we also have W⊤ B . Hence, we can apply the
W ⊂ W ∥
U∥2,1
≤
U
proofofTheorem4.1inastraightforwardmannerbychanging BU byc BU. Thisconcludesthe
τ 3 · τ
proof.
36Preprint. Underreview.
D.6 PROOFOFCOROLLARY4.3
WedetailtheproofofCorollary4.3below.
Proof. Wefirstnotethatbydefinitionofthetotalvariationdistance(Wolfer&Kontorovich,2019),
wehave
E S∼P L∥Q∗(S, ·) −Q f(S, ·) ∥1 =E S∼P L[2 ·d TV(Q∗(S, ·),Q f(S, ·))]
=2 ·E S∼P L[d TV(Q∗(S, ·),Q f(S, ·))]
=2 (Θ). (bydefinitionoftheriskEq.(2))
pre
·R
ApplyingTheorem4.1(orsimilarlyCorollary4.2),weknowthat
(cid:115)
B¯ (cid:18) 2(cid:19)
pre(Θ) (cid:98)pre(Θ)+ log ,
R ≤R √N δ
train
whereB¯ isformallydefinedinTheorem4.1(respectivelyCorollary4.2). Assumingaperfectpre-
trainingerroramountstoconsider (cid:98)pre(Θ)=0. WedenotebyN∗theintegersuchthattheerroris
equalto ϵ,i.e., R
2
(cid:115)
B¯ (cid:18) 2(cid:19) ϵ B¯2 (cid:18) 2(cid:19) ϵ2 (cid:18) 2B¯(cid:19)2 (cid:18) 2(cid:19)
log = log = N∗ = log .
√N∗ δ 2 ⇐⇒ N∗ δ 4 ⇐⇒ ϵ δ
Taking the ceiling function ensures that N∗ is an integer. Hence, taking N N∗ =
train
(cid:16) 2B¯(cid:17)2 log(cid:0)2(cid:1)
ensuresthat
≥
⌈ ϵ δ ⌉
(cid:115) (cid:115)
B¯ (cid:18) 2(cid:19) B¯ (cid:18) 2(cid:19) ϵ
log log = .
√N
train
δ ≤ √N∗ δ 2
Puttingeverythingtogether,takingN N∗leadsto
train
≥
ϵ
E S∼P L∥Q∗(S, ·) −Q f(S, ·) ∥1 ≤2 ·Rpre(Θ) ≤2
· 2
=ϵ,
whichconcludestheproof.
D.7 PROOFOFTHEOREM4.4
Inthissection,wedetailtheproofofTheorem4.4. Wefirstrecalltheproblemsetup.
Markov chains inputs. In this section, we give as input of the model a single Markov chain
X =(X ,...,X )withfinite,discretestatespaceΩofsizedwithtransitionprobabilityP. We
1 Nicl
assumetheX arealreadytokenizedandthuswehaveΩ . Wedenotethesequenceoftokens
n
⊂V
theLLMreceivesbyS =(X ,...,X )ifn K andS =(X ,...,X )otherwisedue
n 1 n n n−K+1 n
tothedeletionprocess(seeDefinitionB.2). Inp≤ articular,theS areelementsof ∗. Wenotethat
S =(S ,...,S
)isalsoaMarkovchain(seeAppendixD.7.1n
).
BydefinitionoV fPK
,weknowthat
forany1 n [NNi ]c ,l thenexttokenX followsthedistributionP( S ). Weassumethatthere
icl n+1 n
∈ ·|
existsapositiveconstantp thatlowerboundsallthetransitionprobabilitybetweenstates,i.e.,
min
n [N ], x,y Ω, P(X =y X =x) p > 0. Thisisakintotheambiguityof
icl n+1 n min
∀ ∈ ∀ ∈ | ≥
languageconstantc consideredintheprevioussectionandinZhangetal.(2023b);Huetal.(2024);
0
Xieetal.(2022);Wiesetal.(2024).
Next token probability distribution. An important difference with the setting considered in
Theorem4.1isthathere,wepredictaprobabilitydistributiononthestatespaceΩoftheMarkov
chainandnotonthevocabularyoftheLLM . Tothatend,werestrictthepredictedprobabilitygiven
V
thepasttokensS tothestatespaceΩ. Formally,denotingtheoutputofthelastlayeroff byS(L),
n Θ
thelastlayerbeforethesoftmaxoutputsavectoru = 1 W S(L)1 RT. Wefirstextractthe
nτ U n ∈
entriesofuwhoseindexiaresuchthatthei-thelementofthevocabularyspace isinΩ. Thiscan
beformalizedasfollows. WedenotebyI = (i i ... i ) [T]d thesV ubsetofddistinct
d 1 2 d
≤ ≤ ≤ ∈
37Preprint. Underreview.
elementsof[T]andconsiderthematrixM =e⊤,wheree RT hasvalue1atentryi Iand0
j ij ij ∈ j ∈
elsewhere. ExtractingonlythedentriesofuthatcorrespondstothestatespaceyieldsavectorinRd
thatwritesv = 1 MW S(L)1 RT. SimilarlytoAppendixB,theprobabilitydistributionof
nτ U n ∈
nexttokenX providedbytheLLMf nowwrites
n+1 Θ
(cid:18) (cid:19)
1
P ( S )=softmax MW S(L)1 ∆ .
Θ n U n d
·| nτ ∈
WeaimtoobtainasimilargeneralizationboundthaninTheorem4.1wherethereferenceprobability
distributionistheMarkovchaintransitionprobabilityPinsteadoftheprobabilitydistributionof
languageP . Inparticular,PwillreplaceP inthedefinitionoftherisksinEq.(2). Weprovide
L L
belowanoverviewoftheproofbeforedetailingit.
Overviewoftheproof. WearegoingtouseMcDiarmid’sinequalityforMarkovchainsofPaulin
(2015,Corollary2.11). Toadapttheirargumentstooursetting,weboundthetotalvariationbetween
thetrueprobabilityofthenexttokenandtheoneestimatedbytheLLM.Therestofthissectionis
organizedasfollows. First,inAppendixD.7.1,weshowthatS =(S ,...,S )isaMarkovchain.
1 Nicl
TheninAppendixD.7.2, weadapttheconcentrationinequalityofPaulin(2015, Corollary2.11).
AfterwardsinAppendixD.7.3,weshowhowtoboundthetotalvariationbetweenthetrueandthe
estimatedprobabilityofthenexttoken. FinallyAppendixD.7.4concludestheproof.
D.7.1 CONNECTIONBETWEENTOKENSANDSEQUENCESOFTOKENSMARKOVCHAINS
WefirstshowthatS =(S ,...,S )isalsoaMarkovchain.
1 Nicl
LemmaD.17. Considerasequence(notnecessarilyaMarkovchain)X =(X ,...,X )with
1 N
valuesinΩandletS = (X ,...,X )ifn < K andS = (X ,...,X )otherwise.
n 1 n n n−K+1 n
Then,thesequenceS =(S ,...,S )isaMarkovchainwithstatespaceΩ∗ thatcontainsthe
1 N K
sequenceofelementsinΩoflengthsmallerthanK.
Proof. BydefinitionoftheS ,weknowthattheytakevaluesinΩ∗ . Letx ,...,x Ω. We
n K 1 n+1 ∈
firstassumethatn>K anddenotes =(x ,...,x ). Wehave
i n−K+1 i
P(S =s S =s ,...,S =s )
n+1 n+1 n n n−K+1 n−K+1
|
=P(S =s X =x ,...,X =x )
n+1 n+1 n n n−K+1 n−K+1
|
=P(S =s S =s ). (bydefinitionofS )
n+1 n+1 n n n
|
Similarly,weassumen<K anddenotes =(x ,...,x ). Wehave
i 1 i
P(S =s S =s ,...,S =s )
n+1 n+1 n n 1 1
|
=P(S =s X =x ,...,X =x )
n+1 n+1 n n 1 1
|
=P(S =s S =s ). (bydefinitionofS )
n+1 n+1 n n n
|
Finally,forn=K,wedenotes =(x ,...,x )fori K ands =(x ,...,x ). Wehave
i 1 i K+1 2 K+1
≤
P(S =s S =s ,...,S =s )
K+1 K+1 n n 2 2
|
=P(S =s X =x ,...,X =x )
K+1 K+1 K K 1 1
|
=P(S =s S =s ).
K+1 K+1 K K
|
(bydefinitionofS )
K
ThisestablishestheMarkovpropertyforS.
D.7.2 CONCENTRATIONINEQUALITIESFORMARKOVCHAINS
Wefirststateaconcentrationinequalityfortime-homogeneousMarkovchainsthatwillbeusedto
obtainourfinalbound.
38Preprint. Underreview.
PropositionD.18(McDiarmid’sinequalityfortime-homogeneousMarkovchains). LetS :=
(S ,...,S )beaMarkovchainwithvalueinadiscrete,finitestatespaceΩandmixingtime
1 N
t (ε). Lett :=inf t (cid:0)ε(cid:1) (cid:16) 2−ε(cid:17)2 . Iff: Ω Rissuchthatthereexistsc RN
mix min 0≤ε<1 mix 2 · 1−ε → ∈
satisfying
N
(cid:88)
x,y Ω, f(x) f(y) c 1 ,
∀ ∈ − ≤
i {xi̸=yi}
i=1
thenwehaveforanyu 0,
≥
(cid:18) 2u2 (cid:19)
P(f(S) E [f(S)] u) 2exp − .
| − S |≥ ≤ c 2 t
∥ ∥2· min
Proof. WerecallthatCorollary2.11ofPaulin(2015)ensuresthatforsuchafunctionf,wehave
(cid:18) 2u2 (cid:19)
P(f(S) E[f(S)] u) 2exp − , (26)
| − |≥ ≤ c 2 τ
∥ ∥2· min
whereτ isdefinedas
min
(cid:18) (cid:19)2
2 ε
τ := inf τ(ε) − ,
min
0≤ε<1 1 ε
−
withτ(ε)beingthemixingtimeofaMarkovchainwithoutassumingtimehomogeneity(seePaulin
(2015,Definition1.4)). Asinourcase,weassumethetimehomogeneity,thisinequalityinEq.(26)
hastobeadapted. FollowingRemark1.5ofPaulin(2015),wenoticethat
ε [0,1], τ(2ε) t (ε) τ(ε).
mix
∀ ∈ ≤ ≤
(cid:16) (cid:17)2
Let0 ε<1. Usingthefactthat 2−ε >0,thepreviousinequalityensures
≤ 1−ε
(cid:16)ε(cid:17) (cid:18) 2 ε(cid:19)2 (cid:16)ε(cid:17)(cid:18) 2 ε(cid:19)2
τ(ε) t τ(ε) − t − .
mix mix
≤ 2 ⇐⇒ 1 ε ≤ 2 1 ε
− −
Takingtheinfimumontheleft-handsideleadsto
(cid:18)
2
ε(cid:19)2 (cid:16)ε(cid:17)(cid:18)
2
ε(cid:19)2
τ = inf τ(ε) − t − .
min mix
0≤ε<1 1 ε ≤ 2 1 ε
− −
Aswetookεarbitraryin[0,1),wecantaketheinfimumontheright-handside,whichleadsto
τ t .
min min
≤
(cid:16) (cid:17)
Asthefunctionx exp
−2u2
isdecreasing,wefinallyobtain
→ ∥c∥2 2x
(cid:18) 2u2 (cid:19) (cid:18) 2u2 (cid:19)
exp − exp − . (27)
c 2τ ≤ c 2t
∥ ∥2 min ∥ ∥2 min
CombiningEqs.(26)and(27)concludestheproof.
SimilarlytoTheorem4.1,wewanttoapplyPropositionD.18toafunctionf thatconsistsofsums
of total variation. We investigate in the next section how to find the bounding vector c to apply
PropositionD.18.
D.7.3 FINDINGTHEBOUNDINGVECTOR
WewanttoapplythesameargumentsasintheproofofTheorem4.1tofindtheboundingvectorc.
Theonlydifferenceintermsofsettingisthedefinitionoftheprobabilityofthenexttoken. Indeed,in
ourcase,weapplyanextractionmatrixM Rd×T torecoverthedstatesoftheinputMarkovchain.
∈
Wefirstprovethefollowingtechnicallemma.
39Preprint. Underreview.
Lemma D.19. Let d T and consider a subset of d distinct elements of [T] that writes
I =(i i ... ≤ i ) [T]d. WedenotebyM Rd×T thematrixwithrowsM =e⊤,
d 1 ≤ 2 ≤ ≤ d ∈ ∈ j ij
wheree RT hasvalue1atentryi Iand0elsewhere. Foranyvectoru RT,wehave
ij
∈
j
∈ ∈
Mu u .
1 1
∥ ∥ ≤∥ ∥
Proof. Bydefinitionoftheℓ -norm,wehave
1
d T d T T d
(cid:88)(cid:88) (cid:88)(cid:88) (cid:88) (cid:88)
Mu = M u M u u M .
1 kl l kl l l kl
∥ ∥ | |≤ | |≤ | | | |
k=1 l=1 k=1l=1 l=1 k=1
Moreover,eachcolumnofMcontainsatmostonenon-zeroentry(withvalue1). Otherwise,itmeans
thattwoe areidentical(astheyonlyhaveonenon-zeroentrywithvalue1,havingitatthesame
ij
positionensurestheirequality)whichcontradictsthefactthatthei wheretakendistinct. Hence,for
j
alll,wehave(cid:80)d
M 1,whichconcludestheproof.
k=1| kl |≤
WenowprovealemmaanalogoustoLemmaD.8.
LemmaD.20. LetS Rr×n denotetheentryoftheLLMf andS(L) denotetheoutputof
Θ
∈
thelastlayerbeforethesoftmax. Letd T andconsiderasubsetofddistinctelementsof[T]
thatwritesI =(i i ... i ) ≤ [T]d. WedenotebyM Rd×T thematrixwithrows
d 1 2 d
M = e⊤,wheree ≤ R≤T has≤ value∈ 1atentryi Iand0el∈ sewhere. Then,thefollowing
j ij ij ∈ j ∈
inequalityholds
1 1
MW S(L)1 W⊤ .
nτ∥
U n ∥1
≤ τ∥
U∥2,1
Proof. ApplyingLemmaD.19withthematrixM Rdandthevector 1 W X(L)1 RT leads
∈ nτ U n ∈
to
1 1
MW S(L)1 W X(L)1 .
U n 1 U n 1
nτ∥ ∥ ≤ nτ∥ ∥
ApplyingLemmaD.8concludestheproof.
Thepreviouslemmacanbeusedtoshowthatthelogarithmoftheratiobetweenthetrueprobability
ofthenexttokenandtheoneestimatedbytheLLMf isupperboundedasafunctionofthenumber
Θ
of states of the Markov chain d, the temperature τ, the upper-bound on W and some constant
U
relatedtotheambiguityoflanguage(seeEq.(1)).
PropositionD.21(Upper-boundonthelogarithm). ConsideranLLMf andaninput
Θ
∈ F
MarkovchainX =(X ,...,X )withdstates. WerecallthatB istheupperboundonthe
1 Nicl U
normofW inthedefinitionofparameterspace ,τ isthesoftmaxtemperature,andp is
U min
W
theconstantrelatedtotheminimaltransitionprobabilitybetweenstates. Wehave
n [N], (cid:12) (cid:12) (cid:12)log(cid:18) P(X n+1 |S n) (cid:19)(cid:12) (cid:12) (cid:12) B¯ =max log(d)+ 2B U ,log(cid:18) 1 (cid:19) .
∀ ∈ (cid:12) P (X S ) (cid:12)≤ { τ p }
Θ n+1 n min
|
Proof. The main idea of the proof is to bound the probability ratio and use the non-decreasing
monotonicityofthelog. Letn [N]. Themodelf receivesasinputsequencesoftokensS of
Θ n
∈
sizen K. Wefirstlower-boundeachtermoftheprobabilityratio. Bydefinitionofp ,wehave
min
≤
P(X S )=P(X X ) p >0, (28)
n+1 n n+1 n min
| | ≥
whereweusedtheMarkovpropertyforthefirstequality. Wewanttoobtainasimilarinequality
forP (X S ). AstheparametersΘoftheLLMarein ,weknowthat W⊤ B .
Θ n+1
|
n
W ∥
U∥2,1
≤
U
LemmaD.20ensuresthat
1 1 B
MW S(L)1 W⊤ U .
∥nτ
U T ∥1
≤ τ∥
U∥2,1
≤ τ
40Preprint. Underreview.
WecanthenapplyLemmaD.7withc = BU andgiventhat 1 MW S(L)1 Rd,itleadsto
1 τ Tτ U T ∈
(cid:18) (cid:19)
1 1
P ( S )=softmax MW S(L)1 ,
Θ n U n
·| nτ ≥ dexp(2B /τ)
U
where the inequality holds for each component of P ( S ). This is in particular the case
Θ n
P (X S )whichistheentryweareinterestedin,i.e.· ,| wehave
Θ n+1 n
|
1
P (X S ) . (29)
Θ n+1 n
| ≥ dexp(2B /τ)
U
Goingbacktotheratioofprobability,considerthesituationwherewehave
P(X S )
n+1 n
| 1.
P (X S ) ≥
Θ n+1 n
|
Then,usingEq.(29),wehave
P(X S ) 1
n+1 n
1 | dexp(2B /τ),
≤ P (X S ) ≤ P (X S ) ≤ U
Θ n+1 n Θ n+1 n
| |
whichimplies,asthelogisnon-decreasingmonotonically,
(cid:18) P(X S ) (cid:19) 2B
0 log n+1 | n log(dexp(2B /τ))=log(d)+ U . (30)
≤ P (X S ) ≤ U τ
Θ n+1 n
|
Similarly,considerthecasewherewehave
P(X S )
n+1 n
| 1.
P (X S ) ≤
Θ n+1 n
|
Then,wehave
P (X S )
Θ n+1 n
| 1,
P(X S ) ≥
n+1 n
|
andsimilarlytoabove,wecanuseEq.(28)toobtain
P (X S ) 1 1
Θ n+1 n
1 | .
≤ P(X S ) ≤ P(X S ) ≤ p
n+1 n n+1 n min
| |
Thisimplies
(cid:18)P
(X S
)(cid:19) (cid:18)
1
(cid:19)
Θ n+1 n
0 log | log ,
≤ P(X S ) ≤ p
n+1 n min
|
whichalsorewrites
(cid:18) P(X S ) (cid:19) (cid:18) 1 (cid:19)
0 log n+1 | n log . (31)
≤− P (X S ) ≤ p
Θ n+1 n min
|
Bydefinitionoftheabsolutevalue,combiningEq.(30)andEq.(31)leadsto
(cid:12) (cid:12) (cid:18) P(X n+1 S n) (cid:19)(cid:12) (cid:12) 2B U (cid:18) 1 (cid:19)
(cid:12)log | (cid:12) max log(d)+ ,log .
(cid:12) P (X S ) (cid:12)≤ { τ p }
Θ n+1 n min
|
Thisconcludestheproof.
Wearenowreadytoupper-boundthetotalvariation.
CorollaryD.22(Upper-boundonthetotalvariation). ConsideranLLMf andaninput
Θ
∈F
MarkovchainX =(X ,...,X )withdstates. WerecallthatB istheupperboundonthe
1 Nicl U
normofW inthedefinitionofparameterspace ,τ isthesoftmaxtemperature,andp is
U min
W
theconstantrelatedtotheminimaltransitionprobabilitybetweenstates. Wehave
(cid:115)
(cid:18) (cid:19)
2B 1
n [N], d (P( S ),P ( S )) 2max log(d)+ U ,log :=c .
TV n Θ n 4
∀ ∈ ·| ·| ≤ { τ p }
min
(32)
41Preprint. Underreview.
Proof. Using Proposition D.21, we can directly apply Lemma D.6 with B = max log(d) +
(cid:16) (cid:17) {
2BU,log 1 foranyn [N]. Itleadsto
τ pmin } ∈
(cid:115)
(cid:18) (cid:19)
2B 1
n [N], d (P( S ),P ( S )) 2max log(d)+ U ,log .
TV n Θ n
∀ ∈ ·| ·| ≤ { τ p }
min
Thisconcludestheproof.
D.7.4 CONCLUDINGTHEPROOF
Wearenowreadytostateourmainresult.
TheoremD.23(RestatementofTheorem4.4). ConsideranLLMf . Weprovideasinput
Θ
∈F
off ad stateMarkovchainX =(X ,...,X ). Thesequenceofsubsequencesofthefirst
Θ
−
1 Nicl
ntermsisdenotedbyS =(S ,...,S ). S isalsoaMarkovchain,andwedenotebyt (ε)
1 Nicl mix
itsmixingtime. Lett :=inf t
(cid:0)ε(cid:1) (cid:16) 2−ε(cid:17)2
. Letδ >0. Then,withprobabilityat
min 0≤ε<1 mix 2 · 1−ε
least1 δ,
−
(cid:115)
(cid:114) (cid:18) (cid:19)
t 2
icl(Θ) inf (cid:98)icl(ϑ)+K(ϑ,Θ) +B¯ min log ,
R ≤ϑ∈Wmc{R } N
icl
δ
whereB¯ isaconstantdependingontheparametersoftheproblem. Moreprecisely,
(cid:115)
(cid:18) (cid:19)
2B 1
B¯ =2 max log(d)+ U ,log .
{ τ p }
min
Proof. Letϑ . Wefirstbenefitfromthemetricpropertiesofthetotalvariationtodecompose
mc
∈W
therisk.
1
(cid:88)Nicl
(Θ)= E [d (P( S ),P ( S ))]
Ricl
N
Sn TV
·|
n Θ
·|
n
icl
n=1
1
(cid:88)Nicl
E [d (P( S ),P ( S ))+d (P ( S ),P ( S ))]
≤ N
Sn TV
·|
n ϑ
·|
n TV ϑ
·|
n Θ
·|
n
icl
n=1
1
(cid:88)Nicl
E [d (P( S ),P ( S ))]
≤ N
Sn TV
·|
n ϑ
·|
n
icl
n=1
1
(cid:88)Nicl
+ E [d (P ( S ),P ( S ))]
N
Sn TV ϑ
·|
n Θ
·|
n
icl
n=1
(ϑ)+K(ϑ,Θ). (33)
icl
≤R
Bydefinitionoftherisk,wehave
1 (cid:88)Nicl 1 N (cid:88)train
R(cid:98)icl(ϑ)=
N
d TV(P( ·|S n),P ϑ( ·|S n))=
N
g n(S n)=f(S 1,...,S Nicl)=f(S).
icl (cid:124) (cid:123)(cid:122) (cid:125) icl
n=1 n=1
=gn(Sn)
UsingCorollaryD.22,weknowthat
(cid:115)
(cid:18) (cid:19)
2B 1
g (S ) 2max log(d)+ U ,log :=c .
n n 4
| |≤ { τ p }
min
42Preprint. Underreview.
SimilarlytoTheorem4.1,andusingthefactthatS = (S ,...,S )isaMarkovchain,wecan
1 Nicl
showthatchoosingc RNicl withallentriesequalto 2c4 ensuresthatf verifiestheconditionin
∈ Nicl
PropositionD.5,i.e.,
(cid:88)Nicl
S,Σ, f(S) f(Σ) c 1 .
∀ − ≤
n {Sn̸=Σn}
n=1
Puttingeverythingtogether,wecanapplyPropositionD.18whichleadsto
(cid:18) 2u2 (cid:19)
u 0, P(f(S) E [f(S)] u) 2exp − . (34)
∀ ≥ | − S |≥ ≤ t c 2
min ∥ ∥2
Letu 0. Wehavethefollowingeventsordering
≥
(E [f(S)] f(S) u) (E [f(S)] f(S) u) (f(S) E [f(S)] u)
S S S
− ≥ ⊆ − ≥ ∪ − ≥
=(f(S) E [f(S)] u).
S
| − |≥
Hence,asuwastakenarbitraryandusingEq.(34),wehave
(cid:18) 2u2 (cid:19)
u 0, P(E [f(S)] f(S) u) 2exp − .
∀ ≥ S − ≥ ≤ t c 2
min ∥ ∥2
Werecallthatbydefinition
(cid:104) (cid:105)
f(S)= (cid:98)icl(ϑ)and icl(ϑ)=E
S
(cid:98)icl(ϑ) .
R R R
Moreover,theinequalityontheprobabilityholdsforanyu 0,wecanchooseusuchthat
≥
(cid:18) 2u2 (cid:19) 2u2 (cid:18) δ(cid:19) 1 (cid:18) 2(cid:19)
δ =2exp − − =log u2 = t c 2log
t c 2 ⇐⇒ t c 2 2 ⇐⇒ 2 min ∥ ∥2 δ
min ∥2 min ∥ ∥2
(cid:115)
(cid:18) (cid:19)
1 2
u= √t c log .
min 2
⇐⇒ √2 ∥ ∥ δ
Usingthefactthat
c
=(cid:118) (cid:117) (cid:117) (cid:116)(cid:88)Nicl
c2
=(cid:118) (cid:117) (cid:117) (cid:116)(cid:88)Nicl(cid:18) 2c
4
(cid:19)2 =(cid:118) (cid:117) (cid:117) (cid:116)(cid:88)Nicl 4c2
4
=(cid:115) 4c2
4 =
2c
4 .
∥ ∥2 n N N2 N √N
n=1 n=1 icl n=1 icl icl icl
(cid:114)
(cid:16) (cid:17)
Usingthefactthatc = 2max log(d)+ 2BU,log 1 (CorollaryD.22),weobtain
4 { τ pmin }
(cid:115) (cid:115)
1 2c (cid:18) 2(cid:19) √2c (cid:18) 2(cid:19)
u= 4 √t log = 4 √t log
min min
√2√N δ √N δ
icl icl
(cid:114)
(cid:16) (cid:17)
2√t min max {log(d)+ 2B τU,log pm1
in
}(cid:115) (cid:18) 2(cid:19)
= log
√N δ
train
(cid:115)
(cid:114) (cid:18) (cid:19)
t 2
=B¯ min log ,
N δ
icl
wherewedefine
(cid:115)
(cid:18) (cid:19)
2B 1
B¯ =2 max log(d)+ U ,log .
{ τ p }
min
Puttingeverythingtogether,wehave
(cid:32) (cid:114) (cid:115) (cid:18) (cid:19)(cid:33)
t 2
P icl(ϑ) (cid:98)icl(ϑ) B¯ min log δ.
R −R ≥ N δ ≤
icl
43Preprint. Underreview.
Takingtheoppositeeventleadstothefollowinginequalitywithprobabilityatleast1 δ
−
(cid:115)
(cid:18) (cid:19)
√t 2
icl(ϑ) (cid:98)icl(ϑ)+B¯ min log .
R ≤R √N δ
icl
GoingbacktothedecompositionoftheriskinEq.(33)andrearrangingtheterms,weobtain
(cid:115)
(cid:18) (cid:19)
√t 2
icl(Θ) (cid:98)icl(ϑ)+K(Θ,ϑ)+B¯ min log .
R ≤R √N δ
icl
Astheleft-handsideandtheboundfunctionofB¯ donotdependonϑ,wecanputthembothon
theleftsideoftheinequalityandthentaketheinfimumonϑ. Rearrangingthetermstokeeponly
(cid:98)icl(Θ)ontheleftsideoftheinequalityleadsto
R
(cid:115)
(cid:114) (cid:18) (cid:19)
t 2
icl(Θ) inf (cid:98)icl(ϑ)+K(ϑ,Θ) +B¯ min log ,
R ≤ϑ∈Wmc{R } N
icl
δ
whichconcludestheproof.
E ADDITIONAL EXPERIMENTS
E.1 EXPERIMENTALSETUPANDTOKENIZATION
Experimentalsetup. Toensureafairvalidationofourtheoreticalresults,weconductourexperi-
mentsonsomeofthemostrecentandwidelyusedLLMs: Gemma 2B(Teametal.,2024),Llama2
7B & 13B (Touvron et al., 2023b), Llama3 8B, Llama3.2 1B & 3B (Dubey et al., 2024),
andMistral 7Bv0.1(Jiangetal.,2023).
Tokenization. Asthemodelsweconsiderhavedifferenttokenizations, weneedtodothisstep
withextracareasitisacrucialpartoftheexperimentalprocedure. Indeed,LLMs’abilitytohandle
numericalvalueshasbeenprovedtobedependentonthetokenizationalgorithm(Singh&Strouse,
2024;Alietal.,2024;Gruveretal.,2023). Themostwidelyusedtokenizationalgorithmto-date,
BPE (Sennrich et al., 2016), tends to assign tokens to arbitrary 3-digits numbers based on their
occurrencesinlarge-scalecorpora,andthetokenizer’svocabularysize. Ashighlightedby(Gruver
etal.,2023),thisartifactseverelyhindersLLMs’abilitytopredictnumericalvaluesin-context.Thisis
thecaseforpopularLLMssuchasGPT-3(Brownetal.,2020).Newermodels(LLama3,GPT-3.5,
GPT-4) however, tend to have hard-coded rules on top of BPE, making them able to encode all
3-digitsnumberswiththeirowntoken. AlthoughthisfeaturewouldacceleratetheICLprocedureby
eliminatingtheneedfortheHierarchy-PDFalgorithmin(Liuetal.,2024),theunder-representability
of larger numbers in the training data could be an issue. Other tokenization techniques that are
numericalvalues-focusedhasbeenpresentedintheliterature(Golkaretal.,2023;Wuetal.,2024),
pavingthewayforanotherresearchdirectionthatmaybenefitourmethod.
Rodmap. Intherestofthissection,weextendourexperimentstostudythefollowingsetups:
• InAppendixE.2: impactofthenumberofstatesd;
• InAppendixE.3: extensiontoMarkovchainswithp =0;
min
• InAppendixE.4: impactofthetokenization;
• InAppendixE.5: extensiontodynamicalsystems.
E.2 IMPACTOFTHENUMBEROFSTATESd
Wefurtheranalyzetheeffectofthenumberofstatesdontheriskandconsiderrandomlygenerated
d-statetransitionmatricesinFig.9. Afterafirststageofstagnation,therisktendstotakethecorrect
scaling law coefficient. As in (Liu et al., 2024), we notice that considering randomly generated
transitionmatricesseemstobedifficultforanLLMtolearnwhentherearemorethan9states. We
interpretthisbehaviorasthedistributionshiftterminTheorem4.4. Indeed,thelackofstructurein
44Preprint. Underreview.
thesetransitionmatricescanhinderthecorrectdecayofthisterm. Notealsothattheincreaseind
tendstoimplicitlyincreaset ,whichcouldhaveanimpactontheupperboundon (bothinthe
min icl
R
generalizationtermandinthedistributionshiftterm). WewillnowconsidermorestructuredMarkov
chains,andlookattheirimpactondecay.
100 100
101 101
100 101 102 103 100 101 102 103
Context Length Nicl Context Length Nicl
100 100
101 101
100 101 102 103 100 101 102 103
Context Length Nicl Context Length Nicl
Llama2 7B Llama2 13B Mistral 7B v0.1 Gemma 2B 2N icl1/2
Figure9: Impactofthenumberofstatesd. Weplottherisk asfunctionsofN ,with95%
icl icl
R
confidenceintervals. UpperLeft. 2 statesMarkovtransitionmatrices. UpperRight. 4 states
− −
Markov transition matrices. Lower Left. 6 states Markov transition matrices. Lower Right.
−
8 statesMarkovtransitionmatrices.
−
E.3 MORESTRUCTUREDMARKOVCHAINS
Inthissection,weempiricallyverifyourtheoreticalresultsonmoregeneralMarkovchainsthatdo
notverifyp >0.
min
E.3.1 RANDOMWALKS
RandomwalksareasimpleexampleofmorestructuredMarkovchains. Althoughwestillhavethe
possibilityofdiscretizingthekernelofMarkovchainswithinfinitestatespacesasitisdonein(Liu
etal.,2024),weconsidertwotypesofrandomwalksonfinitestatespaces.
1 0.5 0.5
0 1 2 3
0.5 0.5 1
Figure10: Constrainedrandomwalkwithd=3.
Constrainedrandomwalk. WedefinethetransitionmatrixP ofaconstrainedrandomwalkofd
statesasinEq.(35). WedrawtheprobabilisticgraphinFig.10forthecased=3.

1, ifi=0andj =1,
1,
ifi=d −1andj =d −2,
P = 0.5, if1 i d 2andj =i 1, (35)
ij
0 0. ,5, i of th1 e≤
≤
rwi is≤
≤
e.d− −2andj =i− +1,
45
lci
rorrE
lci
rorrE
lci
rorrE
lci
rorrEPreprint. Underreview.
Fig.11highlightsthescalinglawsofTheorem4.4,aswellasthelog(d)dependency. Asbefore,the
best-performingmodelsgeneralizealmostperfectly.
100 100
101 101
100 101 102 103 100 101 102 103
Context Length Nicl Context Length Nicl
100 100
101 101
100 101 102 103 100 101 102 103
Context Length Nicl Context Length Nicl
d=6 d=5 d=4 d=3 N 1/2
icl
Figure 11: Constrained random walks. We plot the risk as functions of N , with 99%
icl icl
confidence intervals. We consider different size d. UpperR Left. Llama2 7B Upper Right.
Llama2 13BLowerLeft. Mistral 7Bv0.1LowerRight. Gemma 2B
Polygonalrandomwalk. WedefinethetransitionmatrixPofapolygonalrandomwalkofdstates
asinEq.(36). WedrawtheprobabilisticgraphinFig.12forthecased=4.

0.5, ifj =(i+1) mod d(clockwisetransition),

P = 0.5, ifj =(i 1) mod d(counterclockwisetransition), (36)
ij

0,
otherwise−
.
Wedrawthesameconclusionsasaboveforthissecondtypeofrandomwalk,inFig.13.
1
0.5 0.5
0.5 0.5
2 0
0.5 0.5
0.5 0.5
3
Figure12: Polygonalrandomwalkwithd=4.
46
lci
rorrE
lci
rorrE
lci
rorrE
lci
rorrEPreprint. Underreview.
100 100
101 101
100 101 102 103 100 101 102 103
Context Length Nicl Context Length Nicl
100 100
101 101
100 101 102 103 100 101 102 103
Context Length Nicl Context Length Nicl
d=6 d=5 d=4 d=3 N 1/2
icl
Figure13:Polygonalrandomwalks.Weplottherisk asfunctionsofN ,with99%confidence
icl icl
intervals. Weconsiderdifferentsized. UpperLeft.RLlama2 7BUpperRight. Llama2 13B
LowerLeft. Mistral 7Bv0.1LowerRight. Gemma 2B
E.3.2 INNERCLIQUESANDOUTERRIMS
InnerCliquesandOuterRims. WealsowanttotestourmethodontheclassofMarkovchain
putforwardin(Wolfer&Kontorovich,2019)toderivetheirlowerbound. Letη >0andd=3kfor
somek N,anddefinethecollectionofMarkovmatrices = M : τ 0,1 d/3 . Every
η η,τ
∈ H { ∈ { } }
elementofthissetconsistsofaninnercliqueandanouterrim. M istheblockmatrixdefinedas
η,τ
follows,
(cid:18) (cid:19)
C R
M
η,τ
= R⊺η Lτ ,
τ τ
whereC Rd/3×d/3,L R2d/3×2d/3,andR Rd/3×2d/3aregivenby
η τ τ
∈ ∈ ∈
1 (cid:0) (cid:1)
L = diag 7 4τ ε,7+4τ ε,...,7 4τ ε,7+4τ ε ,
τ 8 − 1 1 − d/3 d/3
3 η η ... η 
4 − d/3−1 d/3−1
  η 3 η ... . . .  
C =d/3−1 4 − ,
η   . . . ... ... η  
 d/3−1
η ... η 3 η
d/3−1 d/3−1 4 −
1+4τ ε 1 4τ ε 0 ... ... ... 0 
1 1
−
R τ = 81 

0 . .
.
0 . .
.
1+ . . .4τ 2ε 1 −. . .4τ 2ε 0 . .
.
.. . . .. 0 . .
.
  .
0 ... ... ... 0 1+4τ ε 1 4τ ε
d/3 d/3
−
WeprovideinFig.14aprobabilisticgraphofthecaseM andd=9.
η,0
Fig.15comparesdifferentLLMswiththefrequentistmethod,onthecasedepictedinFig.15with
η = 0.02. Althoughthefrequentistmethodachievesalowerloss,thepowerlawsseemtobethe
samewithLLMs.
47
lci
rorrE
lci
rorrE
lci
rorrE
lci
rorrEPreprint. Underreview.
7 7
8 8
3 4
3 η
4−
1 1
8 8
0
η η
2 2
1 1
8 8
5 η 7
7 1 2 2 7
8 8
3 η 3 η
4− 1 1 4−
8 8
6 8
7 7
8 8
Figure14: ProbabilisticgraphofM whend=9.
η,0
100 100
101 101
100 101 102 103 100 101 102 103
Context Length Nicl Context Length Nicl
100 100
101 101
100 101 102 103 100 101 102 103
Context Length Nicl Context Length Nicl
Llama2 7B Llama2 13B Mistral 7B v0.1 Gemma 2B Frequentist 2N icl1/2
Figure 15: We plot the risk as functions of N , with 95% confidence intervals. Upper
icl icl
Left. Llama2 7BUpperRigR ht. Llama2 13BLowerLeft. Mistral 7Bv0.1LowerRight.
Gemma 2B
48
lci
rorrE
lci
rorrE
lci
rorrE
lci
rorrEPreprint. Underreview.
E.4 RECENTMODELS: IMPACTOFTHETOKENIZATION
AsexplainedinAppendixE.1,modelslikeLlama3tokenize3-digitnumberswithasingletoken.This
savesalotofinferencecomputetime,butnotnecessarilyintermsofperformancewhenconsidering
Markovchainswithafewnumberofstatesd,sincewehavetoseparatethestatesbyacommato
forcetokenizationintoasingledigit(e.g. thetransitions1 0 1willbepromptedas1,0,1(5
tokens)insteadof101(1token). InFig.16,wereproducet→ hesa→ meexperimentasinFig.5(left),but
withLlama3models. Thescalinglawsarequitegood,butmuchlesssothanthoseobtainedwith
Gemma 2BandMistral 7Bv0.1onthesameinputs. Ontheotherhand,withthesemodels,it
canbeextremelyinterestingtoconsiderMarkovchainswithmanystates,aswedidinFig.6(right).In
thenextsection,wewilluseLLama3tolearnotherdynamicsystemspresentedinLiuetal.(2024).
100
O( Llama3 8B
N icl−1/2 L Ll la am ma a3 3. .2
2
1 3B
B
)
101
100 101 102 103
Context Length Nicl
Figure16: In-contextscalinglawsforLLama3herdofmodels. Weplottherisk asfunctions
icl
R
ofN ,with95%confidenceintervals.
icl
E.5 DYNAMICALSYSTEMS
Weconsiderfourofthedynamicsystemshighlightedin(Liuetal.,2024): ageometricBrownian
motion,acorrelatedGaussian,anuncorrelatedGaussianandanuncorrelateduniformprocesses. We
displayinFig.17therisksofLLama3 8Bandthefrequentistmethod,whichonceagainhighlight
theemergingcapacityofin-contextlearning.
100 100
O( O(
Frequentist
N ic− l1/2
)
N ic− l1/2
)
Llama3 8B
101 101
100 101 102 100 101 102
Context Length Nicl Context Length Nicl
100 100
O( O(
N ic− l1/2
)
N ic− l1/2
)
101 101
100 101 102 100 101 102
Context Length Nicl Context Length Nicl
Figure 17: LLama3 8B on dynamical systems. We plot the risks as functions of N for
icl icl
LLama3 8B and the frequentist approach (Wolfer & Kontorovich,R 2019) with 95% confidence
intervals. UpperLeft. GeometricBrownianmotion. UpperRight. CorrelatedGaussian. Lower
Left. UncorrelatedGaussian. LowerRight. UncorrelatedUniform.
49
lci
rorrE
lci
rorrE
lci
rorrE
lci
rorrE
lci
rorrE