Loong: Generating Minute-level Long Videos
with Autoregressive Language Models
YuqingWang1 TianweiXiong1 DaquanZhou2∗ ZhijieLin2
YangZhao2 BingyiKang2 JiashiFeng2 XihuiLiu1∗
1UniversityofHongKong 2ByteDance
A dog swimming
Two raccoons are playing drum kit in NYC Times Square
A white and orange tabby cat is seen happily darting through a dense garden, as if chasing something
Figure 1: One-Minute Videos Generated by Loong Conditioned on Texts. Loong
isanautoregressiveLLM-basedmodelthatcangenerateminute-levellongvideoswith
consistentappearance,largemotiondynamics,andnaturalscenetransitions.
Abstract
It is desirable but challenging to generate content-rich long videos in the scale
ofminutes. Autoregressivelargelanguagemodels(LLMs)haveachievedgreat
successingeneratingcoherentandlongsequencesoftokensinthedomainofnatu-
rallanguageprocessing,whiletheexplorationofautoregressiveLLMsforvideo
generationislimitedtogeneratingshortvideosofseveralseconds. Inthiswork,we
conductadeepanalysisofthechallengesthatpreventautoregressiveLLM-based
video generators from generating long videos. Based on the observations and
analysis,weproposeLoong,anewautoregressiveLLM-basedvideogeneratorthat
cangenerateminute-longvideos. Specifically,wemodelthetexttokensandvideo
tokensasaunifiedsequenceforautoregressiveLLMsandtrainthemodelfrom
scratch. Weproposeprogressiveshort-to-longtrainingwithalossre-weighting
schemetomitigatethelossimbalanceproblemforlongvideotraining. Wefurther
investigateinferencestrategies,includingvideotokenre-encodingandsampling
strategies,todiminisherroraccumulationduringinference. OurproposedLoong
canbetrainedon10-secondvideosandbeextendedtogenerateminute-levellong
videosconditionedontextprompts,asdemonstratedbytheresults. Moresamples
areavailableat: https://epiphqny.github.io/Loong-video.
∗CorrespondingAuthor
Preprint.
4202
tcO
3
]VC.sc[
1v75720.0142:viXra1 Introduction
Overthepastfewyears,videogenerationmodels,includingdiffusion-basedones[1–8]andlanguage
model based approaches [9, 10], have shown impressive results in generating short videos of a
few seconds. To capture more comprehensive content, it is desirable to generate long videos
withconsistentappearance,largermotiondynamics,andnaturalscenetransitions. Despiterecent
works[11–13]togeneratelongvideoswithdiffusion-basedvideogenerators,generatingcontent-rich
longvideosonthescaleofminutesremainslargelyunderexploredandchallenging.
Autoregressivelargelanguagemodels(LLMs)haveshownremarkablesuccessingeneratinglong
andcoherenttextsequences[14–19],demonstratingtheirabilitytocapturelong-rangedependencies
andcomplextemporalpatterns. InspiredbythesuccessofautoregressiveLLMsinothermodalities
and their flexibility in unifying various modalities and tasks, recent works [9, 10] have explored
autoregressivelanguagemodelsforvideogeneration. Thoseapproachesmapvideosintodiscrete
tokensandusetexttokensasconditioningtogeneratethevideotokensbynext-tokenprediction
withdecoder-onlytransformers. State-of-the-artautoregressiveLLM-basedvideogenerator[10]can
generatehigh-quality2-secondshortvideoclipsanditerativelyextendto10-secondcoherentvideos.
DespitedemonstratingtheabilityoflongsequencegenerationinNLPandbeingexploredforshort
videogeneration,thepotentialofLLMstogenerateminute-level,content-rich,anddynamicvideos
remains unexplored. In natural language processing, LLMs can be trained on long sequences
and extended beyond the training length. However, we empirically observe that either training
autoregressiveLLMsonlongvideosequencesorextendingshortvideogeneratorstogeneratelong
videos leads to unsatisfactory performance for minute-level video generation. A question arises:
Whatrestrictsthecapabilityofautoregressivelanguagemodelsforgeneratinglongvideos?
Wehypothesizethatthemainobstaclesarethelargeredundancyandstronginter-framedependency
amongvideo tokens. Thevideo tokensof thecurrent framedependheavilyon thetokens ofthe
previousframes,leadingtotwochallengesforlongvideogeneration: (1)Imbalancedlossduring
training. Whentrainedwiththenext-tokenpredictionobjective,predictingearly-frametokensfrom
text prompts is much more difficult than predicting late-frame tokens based on the ground-truth
tokensofpreviousframes. Theimbalanceddifficultylevelsoftokensleadtoimbalancedlossduring
training. The issue becomes more severe as the video length increases, where the accumulated
loss of many easy tokens largely surpasses the loss of a few difficult tokens and dominates the
gradientdirection. (2)Erroraccumulationduringinference. Whilethemodelpredictsthenext
tokenconditionedonpreviousground-truthtokensduringtraining,ithastopredictthenexttoken
conditionedonpreviouspredictedtokensduringinference. Thistraining-inferencediscrepancyleads
toerroraccumulationduringinference. Becauseofthestronginter-framedependencyamongvideo
tokensandthelargenumberofvideotokens,sucherroraccumulationisnon-negligibleandcauses
visualqualitydegradationforlongvideoinference.
Inthiswork,weproposeLoong,aimingtounleashthepowerofautoregressivelanguagemodelsto
generatelongvideosinthescaleofminutes. OurautoregressiveLLM-basedvideogeneratorconsists
oftwocomponents: avideotokenizerthatcompressesvideosintosequencesofdiscretevideotokens,
andanautoregressiveLLMthatmodelstheunifiedsequenceoftexttokensfollowedbyvideotokens
throughnext-tokenprediction. Tomitigatetheproblemofimbalancedlossforlongvideotraining,
weintroduceaprogressiveshort-to-longtrainingstrategythatgraduallyincreasesthetrainingvideo
length. We further propose loss re-weighting for early frames to prevent the model from being
dominatedbymanyeasytokensinthelateframes. Moreover,weinvestigateinferencestrategies,
includingthevideotokenre-encodingandsamplingstrategy,tofurtherextendthevideolengthby
iterativelygeneratingthenextframesconditionedonpreviouslygeneratedframes. Inordertoenable
trainingandinferencewithlongervideos,weadoptlow-resolutionvideosfortheLLM-basedvideo
generator,andleverageasuper-resolutionandrefinementmoduletofurtherenhancetheresolution
andfine-graineddetailsofthegeneratedlongvideos.
Insummary,weproposeLoong,anovelautoregressiveLLM-basedvideogeneratorthatcangenerate
content-rich,coherent,anddynamiclongvideosinthescaleofminutes. Basedonourobservations
and analysis of the issues that limit the power of LLMs for long video generation, we propose
progressive short-to-long training with a loss weighting scheme to enable model training on 10-
secondvideos. Wefurtherinvestigateinferencestrategiestoextendthe10-secondvideostominute-
levelvideosbyautoregressivegenerationstrategiesdesignedforlongvideoinference. Ourmodel
demonstratesitsabilityingeneratingminute-levellongvideosthroughextensiveexperiments.
22 RelatedWork
Videogeneration.ThemainstreamvideogenerationmethodscanbecategorizedintoGAN-based[20–
22], Diffusion-based [23, 7, 24–26, 3, 6, 27, 28, 13] and language-model-based [29, 10, 30, 31].
Amongthem,Diffusion-basedmethodshaverecentlygainedthemostpopularity. MostDiffusion-
based methods encode videos into latent space [32] for efficient training and utilize progressive
inferencestrategies[25,33,34]togeneratevideoswithhighspatial-temporalresolution. Withanew
scalableDiffusionTransformer[35]architecture,Sora[13]hasfurtherpushedvideogenerationtoa
newstage. Differentfromdiffusion-basedvideogenerationmethods,ourworkaimstoexploreand
unleashthepotentialityoflanguagemodelsforlongvideogeneration,astheirabilityformodeling
longsequenceandscalinguphavebeenprovedinNLP.
Imageandvideogenerationwithlanguagemodels. Languagemodelshaverecentlybeenexplored
forvisualgeneration,withmostworksfocusingontokenizingvisualdataintoaformthatcanbe
processedbythesemodels. QuantizationtechniqueslikeVQ-VAE[36,37]arecommonlyused,and
transformersareemployedtomodeltheresultingtokens. Forimagegeneration,autoregressiveor
maskedtransformersareprevalent[38–44]. Inshortvideogeneration,image-levelorvideo-level
tokenizers are utilized, incorporating spatial-temporal compression and causal structures. Trans-
formersmodelthespatial-temporalrelationships,withvarioustechniquesproposed,suchassparse
attention,spatial-temporalattention,large-scalepre-training,andimprovedtokenization[9,45–48].
VideoPoet[10]standsoutasamultimodalmodelusingbidirectionalattentionforconditioning,while
ourmethodalignsbetterwiththelanguagemodelparadigmbyusingunidirectionalattentionforboth
textandvideo. However,theseshortvideogenerationmodelsfocusonproducing1-5secondclips,
limitingtheirabilitytocapturecomplexeventsandmaintainconsistencyoverlongerdurations.
Long video generation. Previous works have explored long video generation using various ap-
proaches. LongVideoGAN[49],NUWA-XL[50],andGAIA-1[51]utilizedGAN-basedmethods,
diffusion-over-diffusiontechniques,orworldmodelsbutwerelimitedtospecificdomains. More
recently,videodiffusionmodelshavebeenextendedforlongervideogeneration. FreeNoise[52]
andGen-L[11]focusonsamplingnoisevectorsandaggregatingoverlappingshortvideosegments,
respectively,whileStreamingT2V[12]proposesanautoregressiveapproachwithmemoryblocks
forconsistencyandappearancepreservation. Inthelanguagemodeldomain,Phenaki[30]generates
variable-lengthvideosusingamaskedvideotransformer. Despitetheseadvancements,generating
longvideoswithrichmotiondynamics,consistentappearance,andhighvisualqualityintheopen
domainremainsachallenge.
3 Method
WepresentLoong,anautoregressiveLLM-basedmodelforgeneratinglongvideosinthescaleof
minutes. Weintroducetheoverallframework,composedofthevideotokenizerandtheLLM-based
video generator, in Sec. 3.1. We analyze the problem with long video training and propose the
progressiveshort-to-longtrainingwithlossre-weightingscheme,enablingtrainingon10-second
videos,inSec.3.2. Wefurtherinvestigateinferencestrategiestoextendthegeneratedvideolengthto
theminutelevelandpost-processingtechniquestoenhancethespatialresolutionofgeneratedvideos
inSec.3.3.
3.1 OverallFramework
InspiredbypreviousworkinLLM-basedimagegenerationandvideogenerationmodels[38,41,48,
31,10],Loongisdesignedwithtwocomponents: avideotokenizerthatefficientlycompressesthe
videosintodiscretetokens,andadecoder-onlytransformerthatautoregressivelypredictsnextvideo
tokensbasedontexttokens.
VideoTokenizer. Inordertoenablespatial-temporaljointcompressionandjointmodelingofimages
andvideos,weleveragecausal3DCNNarchitectureforthetokenizer,inspiredbyMAGViT2[31].
The encoded spatial-temporal features are quantized into discrete tokens with Clustering Vector
Quantization (CVQ) [53], an improved version of VQGAN [37] designed to enhance codebook
utilization. Toextendthetemporalcoverageofvideoswithinalimitednumberoftokens,wework
with low-resolution videos and leave super-resolution for the post-processing in Sec. 3.3. The
3stage-3
stage-2
Progressive training
stage-1
Text token
Autoregressive Large Language Model
Video token
(first frame)
Video token
(2-17 frame)
Text Tokenizer Video Tokenizer
Video token
(18-65 frame)
text
video frames
Figure2: OverallFrameworkandtheTrainingprocessofLoong. Giventheinputtexttokens,
themodelpredictvideotokensautoregressively. Allthetextandvideoinformationisformulated
intoaunidirectionaldiscretetokensequence,wherethemodelpredictsthenexttokenbasedonthe
previoustokens. VideoTokenizerisutlizedtoconvertvideoframesintodiscretetokens. Weuse
differentcolortorepresentfirstframe,shortclipandlongclipseparately. Wefollowaprogressive
trainingpipelinetotrainonlongvideos. Weomitthespecialtokensforsimplicity.
tokenizercancompressa10-secondvideo(65frames,128×128resolutionforeachframe)intoa
sequenceof17×16×16discretetokenswithavocabularysizeof8192.
Autoregressive LLM-based Video Generation. With the video frames converted into discrete
tokens,wecannowmodelthetextandvideotokensasaunifiedsequenceandformulatetext-to-video
generationasautoregressivelypredictingvideotokensconditionedonthetexttokenswithdecoder-
onlyTransformers. TheprocessisillustratedinFig.2. Forsimplicity,weomitthespecialseparate
tokensinthefollowingformulation. Lett={t ,t ,...,t }representthesequenceoftexttokens,
1 2 N
whereN isthenumberoftexttokens. Similarly,letv={v ,v ,...,v }representthesequenceof
1 2 L
videotokens,whereListhenumberofvideotokens. TheautoregressiveLLMmodelstheunified
tokensequences=[t;v]andistrainedwiththenext-tokenpredictionlossforthevideotokens.
L
(cid:88)
L=− logp(v |v ,t) (1)
i <i
i=1
where v denotes the i-th token in the video sequence v, and v denotes all the video tokens
i <i
precedingv .
i
Discussion.DifferentfromVideoPoet[10],whichencodestextwithapretrainedT5textencoder[54]
andappliesbidirectionalattentionfortheinputconditiontokensandcausalattentionforthevideo
tokens, our approach does not rely on a pretrained text encoder. Instead, we formulate the text
tokensandvideotokensasaunifiedtokensequenceandapplycausalattentiontoalltokens. Our
unifiedautoregressivemodelingoftexttokensandvideotokensprovidesasimplerformulationthatis
consistentwithmodernGPT-styleLLMs[16]. Thisdesignmayleadtopotentialbenefitsinextending
ourmodeltomultimodalLLMsthatunifydifferentmodalitiesanddifferenttasksforunderstanding
andgeneration.
3.2 ProgressiveShort-to-LongTrainingwithLossRe-weighting
Mostvideogenerationmodelsaretrainedonshortvideoclips,typicallynomorethan4seconds,
whichlimitstheirabilitytocapturelong-termdependenciesandcomplexdynamicsinlongervideos.
Toaddressthislimitation,itisessentialtotrainthesemodelsonvideoswithlongerdurations,enabling
themtolearnandgeneratemorecoherentandcontextuallyrichvideocontent.
However,trainingdirectlyonlongvideosleadstosuboptimalperformance,evenwhenthemodelis
trainedforalargenumberofiterations. Weillustratethelosscurveofdifferentframerangeswhen
training on 65-frame videos (with 4,356 tokens, covering 10 seconds) in Fig. 3. We empirically
4observethattokensfromearlyframes(frames1-17)havelargerlossesthanthosefromlaterframes
(tokens from frames 50-65 have the smallest average loss). During training, the model learns
throughnext-tokenprediction,whereitismucheasiertopredicttokensoflaterframesgiventhe
previousground-truthvideoandtexttokens. Incomparison,predictingearly-frametokenswithlittle
visualcuesfrompreviousframesismorechallenging. Theimbalancedlossisasevereproblemfor
long-sequencetrainingbecausetheaccumulatedlossofthemanyeasy-to-predicttokensfromlater
frames(18-65)surpassesthelossofthefewdifficult-to-predicttokensfromearlyframes(1-17)and
dominatesthegradientdirection,leadingtosuboptimalvisualqualityinthegeneratedvideos.
Tomitigatetheaforementionedchallengeofimbal-
anced video token difficulties, we propose a pro-
gressiveshort-to-longtrainingstrategywithlossre-
weighting,demonstratedinthefollowing.
Progressiveshort-to-longtraining.Inordertoallow
themodeltofirstlearnthetext-conditionedappear-
anceandmotionofshortvideos,andthensmoothly
adjusttolonger-rangedependenciesandmorecom-
plexmotionpatternsinlongervideos,wefactorize
trainingintothreestageswhichgraduallyincreases
thetrainingvideolength,asillustratedintheFig.2:
(1) In stage-1, we pretrain the model with text-to-
imagegenerationonalargedatasetofstaticimages,
whichhelpsthemodeltoestablishastrongfounda-
tionformodelingper-frameappearanceandstructure.
(2)Instage-2,wecontinuetotrainthemodeljointly Figure 3: Imbalanced Training Losses
onimagesandshortvideoclipsof17frames,where When Training Directly on Long Videos.
themodellearnstocaptureshort-termtemporalde- The training loss for late frames (18-65) is
pendenciesandmotionpatternswhilepreservingthe smallerthanthatofearlyframes(1-17),and
per-framevisualquality. (3)Instage-3,weincrease thelossforthefirstframeremainsrelatively
thenumberofvideoframesto65,coveringatempo- high,leadingtosuboptimalvisualqualityin
ralrangeof10seconds,andcontinuejointtraining. theearlyframes(despitethemodelbeingpre-
trainedontext-to-image).
Loss re-weighting for early frames. To further
strengthen the supervision of early frames and to
preventthemodelfromforgettingthestage-1andstage-2priors,weproposealossre-weighting
schemeforstage-3. Tobespecific,weapplylargerlossweightsforthetokensofearlyframes,and
theoverallweightedlossisformulatedas
K L
(cid:88) (cid:88)
L =−(1+λ) logp(v |v ,t)− logp(v |v ,t), (2)
weighted i <i i <i
i=1 i=K+1
wherethefirsttermdenotesthelossfortheK tokenscorrespondingtotheearlyframes(thefirst17
frames),andthesecondtermdenotesthelossfortheL−K tokenscorrespondingtothelaterframes
(frames18-65). λisapositivevaluetostrengthenthelossweightofearlyframes.
Withthelossweightingandprogressivetrainingstrategy,ourmodeleffectivelymitigatestheissues
oflongvideotraining. Asthemodelistrainedonatemporalrangeof10seconds,itcangenerate
videosofupto10secondswithimprovedtemporalcoherenceandconsistencywhilemaintainingthe
strongappearanceandmotionpriorslearnedfromtheimageandshortvideoclips.
3.3 InferenceStrategiesforExtendingVideoLengthandResolution
Largelanguagemodelsareproventobelength-generalizable,soweexpecttheLLM-basedvideo
generatortrainedon10-secondvideostobeextendedtogeneratelongervideosautoregressively.
However, generalizing beyond the training video duration is non-trivial and may lead to error
accumulationandqualitydegradation.Forinstance,aone-minutevideocorrespondstoapproximately
26,112videotokensunderourcurrentsettings,whichissignificantlylongerthanmosttextsequences
typically encountered in language modeling tasks. The considerable length and the large inter-
framedependencyamongvideotokensposechallengesforextendingtheLLM-basedgeneratorfor
longvideogeneration. Inthissubsection, weinvestigateinferencestrategiestogenerateminute-
5Predicted Video Tokens v10 v11 v12 v13
Autoregressive Large Language Model
<sot> t1 t2 t3 <eot><soi> v7 v8 v9 Re-encoded Video
Tokens as conditions
Input Text Tokens
Video Tokenizer
Encoder
last n frames
Decoded Video
Frames
Video Tokenizer
Decoder
v1 v2 v3 v7 v8 v9 Predicted Video Tokens
Figure4: InferenceprocessofLoong. Giventheinputtext,themodelfirstpredictsvideotokens
(illustratedbyv1-v9)forthefirst10s. Thetokensfromthelastnframesofthiscliparethendecoded
intovideoframesandre-encodedbythevideotokenizer. Thesere-encodedtokens(v7-v9),along
withthetexttokens,serveasconditionstopredictthevideotokens(v10-v13)forthenextclip. This
iterativeprocessoftokenprediction,partialdecoding,andre-encodingenablesextendingvideos
beyondthetrainingdurationwhilemitigatingqualitydegradation. Thisprocessisrepeateduntilthe
generatedvideoreachesthedesiredlength.
level videos and post-processing methods like video super-resolution and refinement to generate
higher-qualityvideos.
Video token re-encoding. A natural way of extending videos beyond the training duration is
to iteratively generate the tokens of the next video clip, conditioned on the text prompts and the
previouslygeneratedtokensofthecurrentvideoclip,exploitingthebenefitofautoregressivelanguage
models. However,thisstrategyleadstoseverevideoqualitydegradationforvideoframesbeyond
thetrainingrange. Withfurtheranalysis,wefindthatthisissuestemsfromthetokenmisalignment
causedbythecausalvideotokenizer. Tobespecific,thetokensfromthelastnframesinavideoclip
arederivedbasedonthecontextofallpreviousframes,whilethetokensfromthefirstnframesin
anewvideocliparederivedwithoutthecontextofthepreviousvideoclip. Therefore,generating
tokens for the new clip directly conditioned on previous tokens leads to distribution shift in the
inputfeaturesforLLMs. Toaddressthisissue,wedecodetheLLM-generatedvideotokenstothe
pixel-spacevideosandthenre-encodethelastnframeswiththevideotokenizer. There-encoded
videotokensandthetexttokensserveastheconditionstogeneratethetokensofthenextvideoclip.
TheinferenceprocessisillustratedinFig.4.
Samplingstrategy. Decodingvideotokenswithautoregressivelanguagemodelsispronetoerror
accumulationbecauseoftheautoregressivenatureofthemodelandthestronginter-framedependen-
ciesofvideotokens. Errorsinpredictingonetokencanpropagateandinfluencethegenerationof
subsequenttokens,leadingtoadegradationinvideoqualityasthelengthincreases. Tomitigatethis
issue,wedrawinspirationfromtheTop-ksamplingstrategycommonlyusedinNLPtasks. During
thetokensamplingprocess,weonlysamplefromtheTop-kmostprobabletokens,ensuringthatthe
generatedtokensareofhighquality. Byfocusingonthemostlikelytokens,wereducetheinfluence
ofpotentialerrorsonsubsequenttokengeneration, effectivelyalleviatingtheerroraccumulation
problem. Ontheotherhand,wealsoobservethattoosmallvaluesofk(k =1degradestogreedy
decoding) lead to almost static videos with little motion. To balance dynamic motion and error
accumulation,wechoosek =50forourmodel.
Super-resolutionandrefinement. AsintroducedinSec.3.1,ourvideotokenizerandLLM-based
video generator operates on the low-resolution 128×128 videos. This design trades off spatial
resolutionforlongervideosequencesduringtrainingandinference. Weapplyoff-the-shelfsuper-
resolution and refinement models [55–58] on the LLM-genereated low-resolution videos. This
moduleservesasapost-processingtoenhancethespatialresolutionandfine-grainedvisualdetailsof
videos,withoutaffectingthemaincontentandmotionofthegeneratedvideos.
64 Experiments
4.1 ImplementationDetails
ModelArchitecture. OurvideogenerationmodelfollowsthesamearchitectureasLLaMA[18],
withthemodelsizerangingfrom700Mto7Bparameters. Wetrainthemodelsfromscratch,without
usinganytext-pretrainedweights. Thevocabularyconsistsof32,000tokensfortext,8,192tokensfor
video,and10specialtokens,resultinginatotalvocabularysizeof40,202. Forthevideotokenizer,
weattempttoreproducethearchitectureofMAGVIT2[31],whichisacausal3DCNNstructurethat
separatelymodelsthefirstframeofthevideo. Themodelcompressesthespatialdimensions(width
andheight)byafactorof8andthetemporaldimensionbyafactorof4. WeutilizetheClustering
VectorQuantization(CVQ)[53]methodforquantization,asitachievesahighercodebookusage
ratiocomparedtotheoriginalVectorQuantization(VQ)[37,59]approach. Thevideotokenizerhasa
totalof246Mparameters.
Training. Ourmodelsaretrainedon100Mtext-imagepairsfilteredfromthecombinationofthe
CC12M[60]andLAION-2B[61]datasets,aswellastheWebVid-10M[62]videotrainingsetand
5.5M video clips filterd from HDVG [63]. The training process follows the progressive strategy
describedinSec.3.2. Wefirstpre-trainthemodelonthecombinedimagedatasetfor200kiterations,
followedbyjointtrainingonimagesand17-framevideoclipsfromthecombinedvideodatasetfor
another200kiterationswithabatchsizeof512. Wethenjointlytrainon65frames(covering10
seconds)for100kiterationswithabatchsizeof256. Theλissetto1.0fortheweightedlossof
Eq.(2). Ineachstage,weuseAdamWoptimizerwithabaselearningrateof1.0e-4. Thelearning
rateisscheduledusingalinearwarmupforthefirst10,000iterations,followedbyacosineannealing
decayuntilreachingthemaximumiterationcount. Forthetrainingofthetokenizer,wealsousea
progressiveapproachonthesamedataset,increasingthevideolengthfrom1to17to65frameswhile
maintainingaresolutionof128×128,withabatchsizeof64andtrainingfor400kiterations.
Inference. Duringinference,ourmodelfirstgeneratestheinitial65framesbasedonthetextprompt.
Wethenusethelast5predictedframesasconditionsforvideoextension. Theclassifier-freeguidance
ratioissetto7.5.
4.2 AblationStudy
Inthissection,weconductablationstudiestoevaluatetheeffectivenessofourmaindesignchoices.
Unlessotherwisespecified, weusethe3Bmodelwithanoutputspatialresolutionof128×128,
withoutanysuper-resolutionandrefinementmodule. Toreducecomputationalcost,wetrainthe
modelsforhalfthenumberofiterationscomparedtothefullsettingdescribedinSec.4.1. Duetothe
lackofagenerallongvideogenerationbenchmark,webuildacustomonebyselectingthetop-1000
longestclipsfromtheWebVid[62]validationsetandslicingeachto27seconds,thedurationofthe
shortestamongthem. Weemploytwocommonlyusedvideogenerationmetricsonthisbenchmark:
Fréchet Video Distance (FVD)[64] and Video-Text Matching (VTM) score calculated by CLIP
(ViT-L/14)[65]. Weusethetextpromptsetsfrompriorworks[4,6,66,3,13]togeneratevideosfor
visualization.
Model Scaling. Scalability is an important characteristic of LLMs. To study scaling behavior
of our model, we evaluate performance of the models with different sizes. Tab. 1 presents the
quantitative results of our models with 700M, 3B and 7B parameters using the same number of
iterationsonthecustombenchmark. WeobservethatlargermodelsachievebetterFVDandVTM
scores,demonstratingthescalabilityofmodelsizeforourapproach.
ProgressiveTrainingwithLossRe-weighting.Tovalidatethe
Table1: ModelSizeScalabilityof
effectivenessofourproposedtrainingstrategy,wecomparethe
Loong.Theperformanceimproves
modelstrainedwithandwithoutourproposedstrategies. Both
asthemodelsizeincreases.
modelsarepre-trainedonimagesandthentrainedonvideos.
FVD ↓ VTM ↑
Fig.5(toprow)showsthegeneratedframesofmodeltrained I3D c
byasingletrainingstagewithoutourproposedstrategy. Itis 700M 633 21.5
clearthatthevideosgeneratedbythedirectly-trainedmodels 3B 572 22.8
sufferfromsignificantobjectappearancedegradation,losing 7B 432 24.1
muchofthestructureinformation.Incontrast,videosgenerated
7A bear wearing sunglasses and hosting a talk show Two pandas discussing an academic paper
Figure5: EffectivenessoftheProgressiveTrainingwithLossRe-weighting. Wesample4frames
fromthe17earlierframesofthevideogenerationresults,toshowtheperformanceofmodelstrained
withorwithoutourtrainingstrategy. Thetoprowshowsresultsofthemodeltraineddirectlyonlong
video,theappearanceofobjectsdegradeslargely. Thebottomrowshowstheresultsmodeltrained
withourproposedtrainingapproach,theappearancepreserveseffectively.
A golden retriever has a picnic on a beautiful tropical beach
An astronaut cooking with a pan and fire in the kitchen
at sunset
A golden retriever has a picnic on a beautiful tropical beach An astronaut cooking with a pan and fire in the kitchen
at sunset
Figure6: EffectivenessofTokenRe-encodingduringVideoExtension. Foreachsample,theleft
twoimagesshowtheresultsbeforetheextensionprocess,andtherighttwoimagesshowtheresults
afterextension. Withouttokenre-encoding,theextensionfailstogeneratevisuallyconsistentcontent.
Teddy bear walking down 5th Avenue front view beautiful sunset
bythemodeltrainedwiththeproposedapproacheffectively
preservetheappearancedetails.
Video Token Re-encoding. Fig. 6 illustrates the
Teddy bear walking down 5th Avenue front view beautiful sunset
importance of token re-encoding during the video
extensionprocess. Withoutpropertokenre-encoding,
themodelfailstomaintainvisualconsistencywhen
extendingthevideo,resultinginabruptchangesin
appearanceandcontent. Incontrast,byemploying
ourtokenre-encodingtechnique,theextendedframes
seamlesslycontinuethevideowithcoherentvisual
styleandcontent.
Sampling Strategy for Inference. We compare
three sampling strategies when predicting each to-
ken: greedydecoding(k =1),top-ksampling,and
multinomialsamplingfromthewholevocabulary(k
equals video token vocabulary size). As shown in
Fig.7,greedydecodinggeneratesstableresultsbut
Figure 7: Study on Sampling Strategies.
lacksdiversity,whilemultinomialsamplingproduces
Resultsofthreedifferentinferencesampling
moredynamiccontentatthecostofquality. Top-k
strategies. Greedydecodingproducesstable
sampling (k = 50) balances stability and diversity.
results but lacks diversity between frames.
A smaller k value prioritizes stability, resulting in
Multinomial sampling generates more dy-
lessdiversemotion,whilealargerkallowsformore
namicanddiversecontentbutwithlowerqual-
dynamicandvariedcontentattheriskofintroducing
ity. Top-k sampling achieves a balance be-
instability. Intheprocessofvideoextension,select-
tweenstabilityanddiversity. kissetto50in
inganappropriatekvalueiscrucialformaintaining
thisexperiment.
consistencyandmitigatingerroraccumulationover
longersequences.
8
gniniart
ruo
o/w
gniniart
ruo
/w
nekot
o/w
nekot
w
hcaorppa
hcaorppa
gnidocne-er
gnidocne-er
ydeerG
K-poT
elpmaS
nekot
o/w
nekot
w
tnemngila-er
tnemngila-er
ydeerG
K-poT
elpmaS4.3 QuantitativeResults
Table2: Comparisonofzero-shottext-to-short-videogenerationontheMSRVTTbenchmark
Model CogVideo[47] MagicVideo[7] ModelScopeT2V[67] Show-1[28] VideoPoet[10] Loong
CLIPSIM 0.2631 - 0.2930 0.3072 0.3049 0.2903
FVD 1294 998 550 538 213 274
Zero-shotTexttoShortVideoGeneration. Althoughourapproachisnotspecificallydesigned
forshortvideogeneration,wecompareourperformanceontheMSR-VTTdataset[68]usingCLIP
similarity (CLIPSIM) [46] and FVD [64] metrics, evaluated on 16 frames. As shown in Tab. 2,
ourFVDscoreisthesecond-best,onlyslightlybehindVideoPoet[10](pretrained). However,our
CLIPSIMscoreislowercomparedtosomeothermethods,whichcanbeattributedtothefactthatour
approachistrainedfromscratchwithoututilizinganypre-trainedtextweights. Incontrast,methods
withhigherCLIPSIMscores,suchasVideoPoet,leveragepre-trainedlanguagemodelslikeT5[54]
fortextencoding,whilediffusion-basedmethodsoftenemployCLIP[65]textembeddings,which
are already trained on the CLIP dataset. Despite not using pre-trained text models, our method
stillachievescompetitiveperformance, demonstratingitseffectivenessincapturingthesemantic
relationshipbetweentextandvideo.
UserStudyonLongVideoGeneration. Weconductauser
Video Text Matching
studytocompareourmethodwithStreamingT2V[12],astate-
12.5% 4.5% 83%
of-the-artopen-sourcedlongvideogenerationmethodbuilton
Video Consistency
Stable Video Diffusion [26]. We use 50 text prompts from
19% 16% 65%
prior works [4, 6, 66, 3] to generate 1-min videos. In the
0% 20% 40% 60% 80% 100%
study,usersarepresentedwith2videosgeneratedbythetwo Streaming T2V (on SVD) preferred No Preference Loong Preferred
models,conditionedonthesametext. Theyareaskedtochoose
Figure 8: User Study on 1-min
thepreferredvideobasedonvisualtextmatchingandcontent
videos. Comparison with the
consistency. Thevideosarepresentedrandomly,andusersare
StreamingT2VonSVDmodel.Our
notinformedaboutthemodels. Wecollect440responses. As
modelismorepreferredbyhuman
showninFig.8,ourmodeloutperformsStreamingT2Vinboth
raters in terms of both visual text
contentconsistency(winrate0.83vs. 0.125)andvisualtext
matchandcontentconsistency.
matching(winrate0.65vs. 0.19).
4.4 VisualizationResults
Fig.9illustratesthevideoframesgeneratedbyourmodelundervarioustext-to-videogeneration
scenarios.
TexttoShortVideo. Inthetoprowofthefigure,weshowsampleofshortvideogeneration. As
showninthefigure,ourapproachhasthecapabilitytogenerateshortvideoswithrichdetailsand
highfidelitywhilemaintainingstrongalignmentwiththegiventextdescriptions.
TexttoLongVideo. Thesecondrowshowsframessampledfromalongvideogeneratedbyour
model,conditionedonaconcisetextdescription. Thissampledemonstratethatourapproachcan
generatelongvideoscontainingdiversecontentandlargerdynamicchangescomparedtoshortvideo
generation,whilemaintainingsemanticalignmentwiththegiventext.
Dense Text to Long Video. Although not explicitly trained on dense captions, our model can
effectivelyadapttodensetextvideogenerationinazero-shotmanner. Asillustratedinthelastrow
ofFig.9,thegeneratedlongvideodepictsrichcontentthatcorrespondstothedetaileddescriptions,
includingmultiplecharacters,weather,scenery,andbuildinginformation. However,weobservethat
thegeneratedimagesappearslightlyblurry.Weattributethistothelowresolutionofourtransformer’s
output,whichmayresultinblurrinesswhengeneratinghighlydetailedcontent.
InFig.10,wealsopresentavisualizationofthevideosreconstructedbyourtokenizer. Weusevideos
selectedfromtheWebVidvaliationdataset[62](notusedfortraining). Theoriginalvideoframesare
inthetoprowofeachgroup,andthereconstructedvideosofourtokenizerareshowninthebottom
row. Despiteachievingahighcompressionratioofapproximately256(8×8×4),ourtokenizer
effectivelypreservesthefine-graineddetailsoftheoriginalframes,andalsomaintainsnaturaland
coherentmotionalongthetemporaldimension.
9Text to Short Video: A dog listening to music with earphones
Text to Long Video: The story of a robot’s life in a cyberpunk setting
Dense Text to Long Video: Beautiful, snowy Tokyo city is bustling. The camera moves through the bustling city street,
following several people enjoying the beautiful snowy weather and shopping at nearby stalls. Gorgeous sakura petals are
flying through the wind along with snowflakes.
Figure9: GeneratedVideosfromLoongacrossVariousText-to-videoScenarios. Ourmodel
demonstratesdiversityandqualityacrossvarioustext-to-videotasks,includingshortvideo, long
video,anddensetext-to-longvideogeneration. Theresultsexhibitrichdetails,smoothtransitions,
andstrongsemanticalignmentwithinputdescriptions.
Figure 10: Reconstructed Videos by Our Tokenizer. Each group represents a distinct video
sequence,withthetoprowdisplayingtheoriginalframesandthebottomrowpresentingthecorre-
spondingreconstructions.Despiteahighcompressionratioof256,ourtokenizereffectivelypreserves
finedetailsandnatural,coherentmotioninthereconstructedvideos.
105 ConclusionandDiscussions
Inconclusion,weproposeLoong,anautoregressiveLLM-basedvideogenerationmodelthatcan
generateminute-levellongvideoswithconsistentappearance,largemotiondynamics,andnatural
scenetransitions. Wechoosetomodelthetexttokensandvideotokensinaunifiedsequence,and
overcomethechallengesoflongvideotrainingwiththeprogressiveshort-to-longtrainingscheme
andlossre-weighting. Ourexperimentsdemonstratetheeffectivenessofourapproachingenerating
minute-levellongvideos. Wehopeourworkcanmotivateresearchonlongvideogenerationand
multimodalmodelinginthefuture.
Border impact. The model can be deployed to assist visual artists and film producers on video
creation, enhancing their efficiency. It can also be deployed for entertainment purposes. On the
otherhand,itmaybeusedforgeneratingfakecontentanddeliveringmisleadinginformation. The
communityshouldbeawareofthepotentialsocialimpacts. Itisnecessarytodeveloptechniquesto
detectandwatermarkthevideosgeneratedbymachinelearningmodels.
References
[1] JonathanHo,TimSalimans,AlexeyGritsenko,WilliamChan,MohammadNorouzi,andDavidJ
Fleet. Videodiffusionmodels. arXivpreprintarXiv:2204.03458,2022.
[2] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis
Germanidis. Structureandcontent-guidedvideosynthesiswithdiffusionmodels. InProc.IEEE
Int.Conf.Comp.Vis.,pages7346–7356,2023.
[3] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu,
HarryYang,OronAshual,OranGafni,etal. Make-a-video: Text-to-videogenerationwithout
text-videodata. InProc.Int.Conf.Learn.Representations,2022.
[4] OmerBar-Tal,HilaChefer,OmerTov,CharlesHerrmann,RoniPaiss,ShiranZada,ArielEphrat,
JunhwaHur,YuanzhenLi,TomerMichaeli,etal. Lumiere: Aspace-timediffusionmodelfor
videogeneration. arXivpreprintarXiv:2401.12945,2024.
[5] YanZeng,GuoqiangWei,JianiZheng,JiaxinZou,YangWei,YuchenZhang,andHangLi.
Makepixelsdance: High-dynamicvideogeneration. arXiv:2311.10982,2023.
[6] Rohit Girdhar, Mannat Singh, Andrew Brown, Quentin Duval, Samaneh Azadi, Sai Saketh
Rambhatla, Akbar Shah, Xi Yin, Devi Parikh, and Ishan Misra. Emu video: Factorizing
text-to-video generation by explicit image conditioning. arXiv preprint arXiv:2311.10709,
2023.
[7] DaquanZhou,WeiminWang,HanshuYan,WeiweiLv,YizheZhu,andJiashiFeng.Magicvideo:
Efficientvideogenerationwithlatentdiffusionmodels. arXivpreprintarXiv:2211.11018,2022.
[8] WeiminWang,JiaweiLiu,ZhijieLin,JiangqiaoYan,ShuoChen,ChetwinLow,TuyenHoang,
JieWu,JunHaoLiew,HanshuYan,etal. Magicvideo-v2: Multi-stagehigh-aestheticvideo
generation. arXivpreprintarXiv:2401.04468,2024.
[9] WilsonYan,YunzhiZhang,PieterAbbeel,andAravindSrinivas. Videogpt: Videogeneration
usingvq-vaeandtransformers. arXivpreprintarXiv:2104.10157,2021.
[10] DanKondratyuk,LijunYu,XiuyeGu,JoséLezama,JonathanHuang,RachelHornung,Hartwig
Adam,HassanAkbari,YairAlon,VighneshBirodkar,etal. Videopoet: Alargelanguagemodel
forzero-shotvideogeneration. arXivpreprintarXiv:2312.14125,2023.
[11] Fu-Yun Wang, Wenshuo Chen, Guanglu Song, Han-Jia Ye, Yu Liu, and Hongsheng Li.
Gen-l-video: Multi-texttolongvideogenerationviatemporalco-denoising. arXivpreprint
arXiv:2305.18264,2023.
[12] RobertoHenschel,LevonKhachatryan,DaniilHayrapetyan,HaykPoghosyan,VahramTade-
vosyan,ZhangyangWang,ShantNavasardyan,andHumphreyShi. Streamingt2v: Consistent,
dynamic,andextendablelongvideogenerationfromtext. arXivpreprintarXiv:2403.14773,
2024.
[13] OpenAI. Sora: Creatingvideofromtext. https://openai.com/sora,2024.
[14] AlecRadford,KarthikNarasimhan,TimSalimans,IlyaSutskever,etal. Improvinglanguage
understandingbygenerativepre-training. 2018.
[15] AlecRadford,JeffWu,RewonChild,DavidLuan,DarioAmodei,andIlyaSutskever.Language
modelsareunsupervisedmultitasklearners. 2019.
11[16] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,Sastry,etal. Languagemodelsarefew-shotlearners. InH.
Larochelle,M.Ranzato,R.Hadsell,M.F.Balcan,andH.Lin,editors,Proc.AdvancesinNeural
Inf.Process.Syst.,2020.
[17] MachelReid,NikolaySavinov,DenisTeplyashin,DmitryLepikhin,TimothyLillicrap,Jean-
baptisteAlayrac, RaduSoricut, AngelikiLazaridou, OrhanFirat, JulianSchrittwieser, etal.
Gemini1.5: Unlockingmultimodalunderstandingacrossmillionsoftokensofcontext. arXiv
preprintarXiv:2403.05530,2024.
[18] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[19] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[20] IanGoodfellow,JeanPouget-Abadie,MehdiMirza,BingXu,DavidWarde-Farley,SherjilOzair,
AaronCourville,andYoshuaBengio. Generativeadversarialnetworks. InProc.Advancesin
NeuralInf.Process.Syst.,volume27,2014.
[21] CarlVondrick,HamedPirsiavash,andAntonioTorralba.Generatingvideoswithscenedynamics.
InProc.AdvancesinNeuralInf.Process.Syst.,2016.
[22] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing
motionandcontentforvideogeneration. InProc.IEEEConf.Comp.Vis.Patt.Recogn.,2018.
[23] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffusionprobabilisticmodels. InProc.
AdvancesinNeuralInf.Process.Syst.,2020.
[24] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and
DavidJ.Fleet. Videodiffusionmodels. arXivpreprintarXiv:2204.03458,2022.
[25] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,
DiederikPKingma,BenPoole,MohammadNorouzi,DavidJFleet,etal. Imagenvideo: High
definitionvideogenerationwithdiffusionmodels. arXivpreprintarXiv:2210.02303,2022.
[26] AndreasBlattmann,TimDockhorn,SumithKulal,DanielMendelevitch,MaciejKilian,Do-
minikLorenz,YamLevi,ZionEnglish,VikramVoleti,AdamLetts,etal. Stablevideodiffusion:
Scalinglatentvideodiffusionmodelstolargedatasets. arXivpreprintarXiv:2311.15127,2023.
[27] XinLi,WenqingChu,YeWu,WeihangYuan,FanglongLiu,QiZhang,FuLi,HaochengFeng,
ErruiDing,andJingdongWang. Videogen: Areference-guidedlatentdiffusionapproachfor
highdefinitiontext-to-videogeneration. arXivpreprintarXiv:2309.00398,2023.
[28] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu,
DifeiGao, andMikeZhengShou. Show-1: Marryingpixelandlatentdiffusionmodelsfor
text-to-videogeneration. arXivpreprintarXiv:2309.15818,2023.
[29] AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,
ŁukaszKaiser,andIlliaPolosukhin. Attentionisallyouneed. InProc.AdvancesinNeuralInf.
Process.Syst.,2017.
[30] RubenVillegas,MohammadBabaeizadeh,Pieter-JanKindermans,HernanMoraldo,HanZhang,
MohammadTaghiSaffar,SantiagoCastro,JuliusKunze,andDumitruErhan. Phenaki: Variable
length video generation from open domain textual descriptions. In Proc. Int. Conf. Learn.
Representations,2022.
[31] LijunYu,JoséLezama,NiteshBGundavarapu,LucaVersari,KihyukSohn,DavidMinnen,
YongCheng,AgrimGupta,XiuyeGu,AlexanderGHauptmann,etal. Languagemodelbeats
diffusion–tokenizer is key to visual generation. In Proc. Int. Conf. Learn. Representations,
2024.
[32] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-
resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF
conferenceoncomputervisionandpatternrecognition,pages10684–10695,2022.
[33] YingqingHe,TianyuYang,YongZhang,YingShan,andQifengChen. Latentvideodiffusion
modelsforhigh-fidelityvideogenerationwitharbitrarylengths. arXiv:2211.13221,2022.
[34] AndreasBlattmann, RobinRombach, HuanLing, TimDockhorn, SeungWookKim, Sanja
Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent
diffusionmodels. InProc.IEEEConf.Comp.Vis.Patt.Recogn.,pages22563–22575,2023.
12[35] WilliamPeeblesandSainingXie. Scalablediffusionmodelswithtransformers. InProc.IEEE
Int.Conf.Comp.Vis.,pages4195–4205,2023.
[36] AaronvandenOord,OriolVinyals,andkoraykavukcuoglu. Neuraldiscreterepresentation
learning. InProc.AdvancesinNeuralInf.Process.Syst.,2017.
[37] PatrickEsser,RobinRombach,andBjornOmmer. Tamingtransformersforhigh-resolution
imagesynthesis. InProc.IEEEConf.Comp.Vis.Patt.Recogn.,pages12873–12883,2021.
[38] AdityaRamesh,MikhailPavlov,GabrielGoh,ScottGray,ChelseaVoss,AlecRadford,Mark
Chen,andIlyaSutskever. Zero-shottext-to-imagegeneration. InProc.Int.Conf.Mach.Learn.,
pages8821–8831,2021.
[39] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay
Vasudevan, AlexanderKu, YinfeiYang, BurcuKaragolAyan, etal. Scalingautoregressive
modelsforcontent-richtext-to-imagegeneration. arXivpreprintarXiv:2206.10789,2022.
[40] HuiwenChang,HanZhang,JarredBarber,AaronMaschinot,JoseLezama,LuJiang,Ming-
HsuanYang,KevinPatrickMurphy,WilliamT.Freeman,MichaelRubinstein,YuanzhenLi,
andDilipKrishnan. Muse: Text-to-imagegenerationviamaskedgenerativetransformers. In
Proc.Int.Conf.Mach.Learn.,pages4055–4075,2023.
[41] HuiwenChang, HanZhang, LuJiang, CeLiu, andWilliamT.Freeman. Maskgit: Masked
generativeimagetransformer. InProc.IEEEConf.Comp.Vis.Patt.Recogn.,June2022.
[42] Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang,
David Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, et al. Spae: Semantic pyramid
autoencoder for multimodal generation with frozen llms. In Proc. Advances in Neural Inf.
Process.Syst.,2024.
[43] PeizeSun,YiJiang,ShoufaChen,ShilongZhang,BingyuePeng,PingLuo,andZehuanYuan.
Autoregressivemodelbeatsdiffusion: Llamaforscalableimagegeneration. arXivpreprint
arXiv:2406.06525,2024.
[44] KeyuTian,YiJiang,ZehuanYuan,BingyuePeng,andLiweiWang. Visualautoregressivemod-
eling: Scalableimagegenerationvianext-scaleprediction. arXivpreprintarXiv:2404.02905,
2024.
[45] SongweiGe,ThomasHayes,HarryYang,XiYin,GuanPang,DavidJacobs,Jia-BinHuang,and
DeviParikh. Longvideogenerationwithtime-agnosticvqganandtime-sensitivetransformer.
InProc.Eur.Conf.Comp.Vis.,pages102–118,2022.
[46] Chenfei Wu, Lun Huang, Qianxi Zhang, Binyang Li, Lei Ji, Fan Yang, Guillermo Sapiro,
and Nan Duan. Godiva: Generating open-domain videos from natural descriptions.
arXiv:2104.14806,2021.
[47] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-
scale pretraining for text-to-video generation via transformers. In Proc. Int. Conf. Learn.
Representations,2022.
[48] LijunYu,YongCheng,KihyukSohn,JoséLezama,HanZhang,HuiwenChang,AlexanderG
Hauptmann,Ming-HsuanYang,YuanHao,IrfanEssa,etal. Magvit: Maskedgenerativevideo
transformer. InProc.IEEEConf.Comp.Vis.Patt.Recogn.,2023.
[49] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun Wang, Timo Aila, Jaakko Lehtinen,
Ming-YuLiu,AlexeiAEfros,andTeroKarras. Generatinglongvideosofdynamicscenes. In
Proc.AdvancesinNeuralInf.Process.Syst.,2022.
[50] Shengming Yin, Chenfei Wu, Huan Yang, Jianfeng Wang, Xiaodong Wang, Minheng Ni,
ZhengyuanYang,LinjieLi,ShuguangLiu,FanYang,etal. Nuwa-xl: Diffusionoverdiffusion
forextremelylongvideogeneration. arXivpreprintarXiv:2303.12346,2023.
[51] AnthonyHu,LloydRussell,HudsonYeo,ZakMurez,GeorgeFedoseev,AlexKendall,Jamie
Shotton,andGianlucaCorrado. Gaia-1: Agenerativeworldmodelforautonomousdriving.
arXivpreprintarXiv:2309.17080,2023.
[52] HaonanQiu,MenghanXia,YongZhang,YingqingHe,XintaoWang,YingShan,andZiwei
Liu. Freenoise: Tuning-freelongervideodiffusionvianoiserescheduling. InProc.Int.Conf.
Learn.Representations,2024.
[53] ChuanxiaZhengandAndreaVedaldi. Onlineclusteredcodebook. InProc.IEEEInt.Conf.
Comp.Vis.,2023.
[54] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,
YanqiZhou,WeiLi,andPeterJ.Liu. Exploringthelimitsoftransferlearningwithaunified
text-to-texttransformer. JournalofMachineLearningResearch,21(140):1–67,2020.
13[55] RobinRombach,AndreasBlattmann,DominikLorenz,PatrickEsser,andBjörnOmmer. High-
resolutionimagesynthesiswithlatentdiffusionmodels. InProc.IEEEConf.Comp.Vis.Patt.
Recogn.,pages10684–10695,2022.
[56] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh
Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image
diffusionmodelswithoutspecifictuning. InProc.Int.Conf.Learn.Representations,2023.
[57] LvminZhang,AnyiRao,andManeeshAgrawala. Addingconditionalcontroltotext-to-image
diffusionmodels. InProc.IEEEConf.Comp.Vis.Patt.Recogn.,pages3836–3847,2023.
[58] RonMokady,AmirHertz,KfirAberman,YaelPritch,andDanielCohen-Or. Null-textinversion
for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF
ConferenceonComputerVisionandPatternRecognition,pages6038–6047,2023.
[59] AaronVanDenOord,OriolVinyals,etal. Neuraldiscreterepresentationlearning. volume30,
2017.
[60] PiyushSharma,NanDing,SebastianGoodman,andRaduSoricut. Conceptualcaptions: A
cleaned,hypernymed,imagealt-textdatasetforautomaticimagecaptioning. InProceedingsof
the56thAnnualMeetingoftheAssociationforComputationalLinguistics(Volume1: Long
Papers),2018.
[61] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,
MehdiCherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-
5b: An open large-scale dataset for training next generation image-text models. In Proc.
AdvancesinNeuralInf.Process.Syst.,volume35,2022.
[62] MaxBain,ArshaNagrani,GülVarol,andAndrewZisserman. Frozenintime: Ajointvideo
andimageencoderforend-to-endretrieval. InProc.IEEEInt.Conf.Comp.Vis.,2021.
[63] WenjingWang,HuanYang,ZixiTuo,HuiguoHe,JunchenZhu,JianlongFu,andJiayingLiu.
Videofactory: Swapattentioninspatiotemporaldiffusionsfortext-to-videogeneration. arXiv
preprintarXiv:2305.10874,2023.
[64] ThomasUnterthiner,SjoerdVanSteenkiste,KarolKurach,RaphaelMarinier,MarcinMichalski,
andSylvainGelly. Towardsaccurategenerativemodelsofvideo: Anewmetric&challenges.
arXivpreprintarXiv:1812.01717,2018.
[65] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,SandhiniAgarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
modelsfromnaturallanguagesupervision. InProc.Int.Conf.Mach.Learn.,pages8748–8763.
PMLR,2021.
[66] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko,
DiederikPKingma,BenPoole,MohammadNorouzi,DavidJFleet,etal. Imagenvideo: High
definitionvideogenerationwithdiffusionmodels. arXivpreprintarXiv:2210.02303,2022.
[67] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang.
Modelscopetext-to-videotechnicalreport. arXivpreprintarXiv:2308.06571,2023.
[68] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for
bridgingvideoandlanguage. InProc.IEEEConf.Comp.Vis.Patt.Recogn.,pages5288–5296,
2016.
14