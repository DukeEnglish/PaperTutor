[
    {
        "title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM",
        "authors": "Chaoyou FuHaojia LinZuwei LongYunhang ShenMeng ZhaoYifan ZhangXiong WangDi YinLong MaXiawu ZhengRan HeRongrong JiYunsheng WuCaifeng ShanXing Sun",
        "links": "http://arxiv.org/abs/2408.05211v1",
        "entry_id": "http://arxiv.org/abs/2408.05211v1",
        "pdf_url": "http://arxiv.org/pdf/2408.05211v1",
        "summary": "The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. To the best of our knowledge, we are the first to\nexploit non-awakening interaction and audio interrupt in MLLM. VITA is the\nfirst step for the open-source community to explore the seamless integration of\nmultimodal understanding and interaction. While there is still lots of work to\nbe done on VITA to get close to close-source counterparts, we hope that its\nrole as a pioneer can serve as a cornerstone for subsequent research. Project\nPage: https://vita-home.github.io.",
        "updated": "2024-08-09 17:59:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.05211v1"
    },
    {
        "title": "Multi-Garment Customized Model Generation",
        "authors": "Yichen LiuPenghui DuYi Liu Quanwei Zhang",
        "links": "http://arxiv.org/abs/2408.05206v1",
        "entry_id": "http://arxiv.org/abs/2408.05206v1",
        "pdf_url": "http://arxiv.org/pdf/2408.05206v1",
        "summary": "This paper introduces Multi-Garment Customized Model Generation, a unified\nframework based on Latent Diffusion Models (LDMs) aimed at addressing the\nunexplored task of synthesizing images with free combinations of multiple\npieces of clothing. The method focuses on generating customized models wearing\nvarious targeted outfits according to different text prompts. The primary\nchallenge lies in maintaining the natural appearance of the dressed model while\npreserving the complex textures of each piece of clothing, ensuring that the\ninformation from different garments does not interfere with each other. To\ntackle these challenges, we first developed a garment encoder, which is a\ntrainable UNet copy with shared weights, capable of extracting detailed\nfeatures of garments in parallel. Secondly, our framework supports the\nconditional generation of multiple garments through decoupled multi-garment\nfeature fusion, allowing multiple clothing features to be injected into the\nbackbone network, significantly alleviating conflicts between garment\ninformation. Additionally, the proposed garment encoder is a plug-and-play\nmodule that can be combined with other extension modules such as IP-Adapter and\nControlNet, enhancing the diversity and controllability of the generated\nmodels. Extensive experiments demonstrate the superiority of our approach over\nexisting alternatives, opening up new avenues for the task of generating images\nwith multiple-piece clothing combinations",
        "updated": "2024-08-09 17:57:33 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.05206v1"
    },
    {
        "title": "Kalman-Inspired Feature Propagation for Video Face Super-Resolution",
        "authors": "Ruicheng FengChongyi LiChen Change Loy",
        "links": "http://arxiv.org/abs/2408.05205v1",
        "entry_id": "http://arxiv.org/abs/2408.05205v1",
        "pdf_url": "http://arxiv.org/pdf/2408.05205v1",
        "summary": "Despite the promising progress of face image super-resolution, video face\nsuper-resolution remains relatively under-explored. Existing approaches either\nadapt general video super-resolution networks to face datasets or apply\nestablished face image super-resolution models independently on individual\nvideo frames. These paradigms encounter challenges either in reconstructing\nfacial details or maintaining temporal consistency. To address these issues, we\nintroduce a novel framework called Kalman-inspired Feature Propagation (KEEP),\ndesigned to maintain a stable face prior over time. The Kalman filtering\nprinciples offer our method a recurrent ability to use the information from\npreviously restored frames to guide and regulate the restoration process of the\ncurrent frame. Extensive experiments demonstrate the effectiveness of our\nmethod in capturing facial details consistently across video frames. Code and\nvideo demo are available at https://jnjaby.github.io/projects/KEEP.",
        "updated": "2024-08-09 17:57:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.05205v1"
    },
    {
        "title": "Cross-Domain Learning for Video Anomaly Detection with Limited Supervision",
        "authors": "Yashika JainAli DaboueiMin Xu",
        "links": "http://arxiv.org/abs/2408.05191v1",
        "entry_id": "http://arxiv.org/abs/2408.05191v1",
        "pdf_url": "http://arxiv.org/pdf/2408.05191v1",
        "summary": "Video Anomaly Detection (VAD) automates the identification of unusual events,\nsuch as security threats in surveillance videos. In real-world applications,\nVAD models must effectively operate in cross-domain settings, identifying rare\nanomalies and scenarios not well-represented in the training data. However,\nexisting cross-domain VAD methods focus on unsupervised learning, resulting in\nperformance that falls short of real-world expectations. Since acquiring weak\nsupervision, i.e., video-level labels, for the source domain is cost-effective,\nwe conjecture that combining it with external unlabeled data has notable\npotential to enhance cross-domain performance. To this end, we introduce a\nnovel weakly-supervised framework for Cross-Domain Learning (CDL) in VAD that\nincorporates external data during training by estimating its prediction bias\nand adaptively minimizing that using the predicted uncertainty. We demonstrate\nthe effectiveness of the proposed CDL framework through comprehensive\nexperiments conducted in various configurations on two large-scale VAD\ndatasets: UCF-Crime and XD-Violence. Our method significantly surpasses the\nstate-of-the-art works in cross-domain evaluations, achieving an average\nabsolute improvement of 19.6% on UCF-Crime and 12.87% on XD-Violence.",
        "updated": "2024-08-09 17:28:29 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.05191v1"
    },
    {
        "title": "Weak-Annotation of HAR Datasets using Vision Foundation Models",
        "authors": "Marius BockKristof Van LaerhovenMichael Moeller",
        "links": "http://arxiv.org/abs/2408.05169v1",
        "entry_id": "http://arxiv.org/abs/2408.05169v1",
        "pdf_url": "http://arxiv.org/pdf/2408.05169v1",
        "summary": "As wearable-based data annotation remains, to date, a tedious, time-consuming\ntask requiring researchers to dedicate substantial time, benchmark datasets\nwithin the field of Human Activity Recognition in lack richness and size\ncompared to datasets available within related fields. Recently, vision\nfoundation models such as CLIP have gained significant attention, helping the\nvision community advance in finding robust, generalizable feature\nrepresentations. With the majority of researchers within the wearable community\nrelying on vision modalities to overcome the limited expressiveness of wearable\ndata and accurately label their to-be-released benchmark datasets offline, we\npropose a novel, clustering-based annotation pipeline to significantly reduce\nthe amount of data that needs to be annotated by a human annotator. We show\nthat using our approach, the annotation of centroid clips suffices to achieve\naverage labelling accuracies close to 90% across three publicly available HAR\nbenchmark datasets. Using the weakly annotated datasets, we further demonstrate\nthat we can match the accuracy scores of fully-supervised deep learning\nclassifiers across all three benchmark datasets. Code as well as supplementary\nfigures and results are publicly downloadable via\ngithub.com/mariusbock/weak_har.",
        "updated": "2024-08-09 16:46:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.05169v1"
    }
]