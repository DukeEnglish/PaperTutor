[
    {
        "title": "Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions",
        "authors": "Michele MirandaElena Sofia RuzzettiAndrea SantilliFabio Massimo ZanzottoSébastien BratièresEmanuele Rodolà",
        "links": "http://arxiv.org/abs/2408.05212v1",
        "entry_id": "http://arxiv.org/abs/2408.05212v1",
        "pdf_url": "http://arxiv.org/pdf/2408.05212v1",
        "summary": "Large Language Models (LLMs) represent a significant advancement in\nartificial intelligence, finding applications across various domains. However,\ntheir reliance on massive internet-sourced datasets for training brings notable\nprivacy issues, which are exacerbated in critical domains (e.g., healthcare).\nMoreover, certain application-specific scenarios may require fine-tuning these\nmodels on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize\nand inadvertently reveal sensitive information. We explore current threats by\nreviewing privacy attacks on LLMs and propose comprehensive solutions for\nintegrating privacy mechanisms throughout the entire learning pipeline. These\nsolutions range from anonymizing training datasets to implementing differential\nprivacy during training or inference and machine unlearning after training. Our\ncomprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This\nwork aims to guide the development of more secure and trustworthy AI systems by\nproviding a thorough understanding of privacy preservation methods and their\neffectiveness in mitigating risks.",
        "updated": "2024-08-10 05:41:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.05212v1"
    },
    {
        "title": "Cell Morphology-Guided Small Molecule Generation with GFlowNets",
        "authors": "Stephen Zhewen LuZiqing LuEhsan HajiramezanaliTommaso BiancalaniYoshua BengioGabriele ScaliaMichał Koziarski",
        "links": "http://arxiv.org/abs/2408.05196v1",
        "entry_id": "http://arxiv.org/abs/2408.05196v1",
        "pdf_url": "http://arxiv.org/pdf/2408.05196v1",
        "summary": "High-content phenotypic screening, including high-content imaging (HCI), has\ngained popularity in the last few years for its ability to characterize novel\ntherapeutics without prior knowledge of the protein target. When combined with\ndeep learning techniques to predict and represent molecular-phenotype\ninteractions, these advancements hold the potential to significantly accelerate\nand enhance drug discovery applications. This work focuses on the novel task of\nHCI-guided molecular design. Generative models for molecule design could be\nguided by HCI data, for example with a supervised model that links molecules to\nphenotypes of interest as a reward function. However, limited labeled data,\ncombined with the high-dimensional readouts, can make training these methods\nchallenging and impractical. We consider an alternative approach in which we\nleverage an unsupervised multimodal joint embedding to define a latent\nsimilarity as a reward for GFlowNets. The proposed model learns to generate new\nmolecules that could produce phenotypic effects similar to those of the given\nimage target, without relying on pre-annotated phenotypic labels. We\ndemonstrate that the proposed method generates molecules with high\nmorphological and structural similarity to the target, increasing the\nlikelihood of similar biological activity, as confirmed by an independent\noracle model.",
        "updated": "2024-08-09 17:40:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.05196v1"
    },
    {
        "title": "HistoKernel: Whole Slide Image Level Maximum Mean Discrepancy Kernels for Pan-Cancer Predictive Modelling",
        "authors": "Piotr KellerMuhammad DawoodBrinder Singh ChohanFayyaz ul Amir Afsar Minhas",
        "links": "http://arxiv.org/abs/2408.05195v1",
        "entry_id": "http://arxiv.org/abs/2408.05195v1",
        "pdf_url": "http://arxiv.org/pdf/2408.05195v1",
        "summary": "Machine learning in computational pathology (CPath) often aggregates\npatch-level predictions from multi-gigapixel Whole Slide Images (WSIs) to\ngenerate WSI-level prediction scores for crucial tasks such as survival\nprediction and drug effect prediction. However, current methods do not\nexplicitly characterize distributional differences between patch sets within\nWSIs. We introduce HistoKernel, a novel Maximum Mean Discrepancy (MMD) kernel\nthat measures distributional similarity between WSIs for enhanced prediction\nperformance on downstream prediction tasks.\n  Our comprehensive analysis demonstrates HistoKernel's effectiveness across\nvarious machine learning tasks, including retrieval (n = 9,362), drug\nsensitivity regression (n = 551), point mutation classification (n = 3,419),\nand survival analysis (n = 2,291), outperforming existing deep learning\nmethods. Additionally, HistoKernel seamlessly integrates multi-modal data and\noffers a novel perturbation-based method for patch-level explainability. This\nwork pioneers the use of kernel-based methods for WSI-level predictive\nmodeling, opening new avenues for research. Code is available at\nhttps://github.com/pkeller00/HistoKernel.",
        "updated": "2024-08-09 17:40:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.05195v1"
    },
    {
        "title": "ECG-FM: An Open Electrocardiogram Foundation Model",
        "authors": "Kaden McKeenLaura OlivaSameer MasoodAugustin TomaBarry RubinBo Wang",
        "links": "http://arxiv.org/abs/2408.05178v1",
        "entry_id": "http://arxiv.org/abs/2408.05178v1",
        "pdf_url": "http://arxiv.org/pdf/2408.05178v1",
        "summary": "The electrocardiogram (ECG) is a ubiquitous diagnostic test. Conventional\ntask-specific ECG analysis models require large numbers of expensive ECG\nannotations or associated labels to train. Transfer learning techniques have\nbeen shown to improve generalization and reduce reliance on labeled data. We\npresent ECG-FM, an open foundation model for ECG analysis, and conduct a\ncomprehensive study performed on a dataset of 1.66 million ECGs sourced from\nboth publicly available and private institutional sources. ECG-FM adopts a\ntransformer-based architecture and is pretrained on 2.5 million samples using\nECG-specific augmentations and contrastive learning, as well as a continuous\nsignal masking objective. Our transparent evaluation includes a diverse range\nof downstream tasks, where we predict ECG interpretation labels, reduced left\nventricular ejection fraction, and abnormal cardiac troponin. Affirming\nECG-FM's effectiveness as a foundation model, we demonstrate how its command of\ncontextual information results in strong performance, rich pretrained\nembeddings, and reliable interpretability. Due to a lack of open-weight\npractices, we highlight how ECG analysis is lagging behind other medical\nmachine learning subfields in terms of foundation model adoption. Our code is\navailable at https://github.com/bowang-lab/ECG-FM/.",
        "updated": "2024-08-09 17:06:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.05178v1"
    },
    {
        "title": "Beyond Closure Models: Learning Chaotic-Systems via Physics-Informed Neural Operators",
        "authors": "Chuwei WangJulius BernerZongyi LiDi ZhouJiayun WangJane BaeAnima Anandkumar",
        "links": "http://arxiv.org/abs/2408.05177v1",
        "entry_id": "http://arxiv.org/abs/2408.05177v1",
        "pdf_url": "http://arxiv.org/pdf/2408.05177v1",
        "summary": "Accurately predicting the long-term behavior of chaotic systems is crucial\nfor various applications such as climate modeling. However, achieving such\npredictions typically requires iterative computations over a dense\nspatiotemporal grid to account for the unstable nature of chaotic systems,\nwhich is expensive and impractical in many real-world situations. An\nalternative approach to such a full-resolved simulation is using a coarse grid\nand then correcting its errors through a \\textit{closure model}, which\napproximates the overall information from fine scales not captured in the\ncoarse-grid simulation. Recently, ML approaches have been used for closure\nmodeling, but they typically require a large number of training samples from\nexpensive fully-resolved simulations (FRS). In this work, we prove an even more\nfundamental limitation, i.e., the standard approach to learning closure models\nsuffers from a large approximation error for generic problems, no matter how\nlarge the model is, and it stems from the non-uniqueness of the mapping. We\npropose an alternative end-to-end learning approach using a physics-informed\nneural operator (PINO) that overcomes this limitation by not using a closure\nmodel or a coarse-grid solver. We first train the PINO model on data from a\ncoarse-grid solver and then fine-tune it with (a small amount of) FRS and\nphysics-based losses on a fine grid. The discretization-free nature of neural\noperators means that they do not suffer from the restriction of a coarse grid\nthat closure models face, and they can provably approximate the long-term\nstatistics of chaotic systems. In our experiments, our PINO model achieves a\n120x speedup compared to FRS with a relative error $\\sim 5\\%$. In contrast, the\nclosure model coupled with a coarse-grid solver is $58$x slower than PINO while\nhaving a much higher error $\\sim205\\%$ when the closure model is trained on the\nsame FRS dataset.",
        "updated": "2024-08-09 17:05:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.05177v1"
    }
]