[
    {
        "title": "Preserving Privacy in Large Language Models: A Survey on Current Threats and Solutions",
        "authors": "Michele MirandaElena Sofia RuzzettiAndrea SantilliFabio Massimo ZanzottoSébastien BratièresEmanuele Rodolà",
        "links": "http://arxiv.org/abs/2408.05212v1",
        "entry_id": "http://arxiv.org/abs/2408.05212v1",
        "pdf_url": "http://arxiv.org/pdf/2408.05212v1",
        "summary": "Large Language Models (LLMs) represent a significant advancement in\nartificial intelligence, finding applications across various domains. However,\ntheir reliance on massive internet-sourced datasets for training brings notable\nprivacy issues, which are exacerbated in critical domains (e.g., healthcare).\nMoreover, certain application-specific scenarios may require fine-tuning these\nmodels on private data. This survey critically examines the privacy threats\nassociated with LLMs, emphasizing the potential for these models to memorize\nand inadvertently reveal sensitive information. We explore current threats by\nreviewing privacy attacks on LLMs and propose comprehensive solutions for\nintegrating privacy mechanisms throughout the entire learning pipeline. These\nsolutions range from anonymizing training datasets to implementing differential\nprivacy during training or inference and machine unlearning after training. Our\ncomprehensive review of existing literature highlights ongoing challenges,\navailable tools, and future directions for preserving privacy in LLMs. This\nwork aims to guide the development of more secure and trustworthy AI systems by\nproviding a thorough understanding of privacy preservation methods and their\neffectiveness in mitigating risks.",
        "updated": "2024-08-10 05:41:19 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.05212v1"
    },
    {
        "title": "VITA: Towards Open-Source Interactive Omni Multimodal LLM",
        "authors": "Chaoyou FuHaojia LinZuwei LongYunhang ShenMeng ZhaoYifan ZhangXiong WangDi YinLong MaXiawu ZhengRan HeRongrong JiYunsheng WuCaifeng ShanXing Sun",
        "links": "http://arxiv.org/abs/2408.05211v1",
        "entry_id": "http://arxiv.org/abs/2408.05211v1",
        "pdf_url": "http://arxiv.org/pdf/2408.05211v1",
        "summary": "The remarkable multimodal capabilities and interactive experience of GPT-4o\nunderscore their necessity in practical applications, yet open-source models\nrarely excel in both areas. In this paper, we introduce VITA, the first-ever\nopen-source Multimodal Large Language Model (MLLM) adept at simultaneous\nprocessing and analysis of Video, Image, Text, and Audio modalities, and\nmeanwhile has an advanced multimodal interactive experience. Starting from\nMixtral 8x7B as a language foundation, we expand its Chinese vocabulary\nfollowed by bilingual instruction tuning. We further endow the language model\nwith visual and audio capabilities through two-stage multi-task learning of\nmultimodal alignment and instruction tuning. VITA demonstrates robust\nfoundational capabilities of multilingual, vision, and audio understanding, as\nevidenced by its strong performance across a range of both unimodal and\nmultimodal benchmarks. Beyond foundational capabilities, we have made\nconsiderable progress in enhancing the natural multimodal human-computer\ninteraction experience. To the best of our knowledge, we are the first to\nexploit non-awakening interaction and audio interrupt in MLLM. VITA is the\nfirst step for the open-source community to explore the seamless integration of\nmultimodal understanding and interaction. While there is still lots of work to\nbe done on VITA to get close to close-source counterparts, we hope that its\nrole as a pioneer can serve as a cornerstone for subsequent research. Project\nPage: https://vita-home.github.io.",
        "updated": "2024-08-09 17:59:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.05211v1"
    },
    {
        "title": "TaSL: Task Skill Localization and Consolidation for Language Model Continual Learning",
        "authors": "Yujie FengXu ChuYongxin XuZexin LuBo LiuPhilip S. YuXiao-Ming Wu",
        "links": "http://arxiv.org/abs/2408.05200v1",
        "entry_id": "http://arxiv.org/abs/2408.05200v1",
        "pdf_url": "http://arxiv.org/pdf/2408.05200v1",
        "summary": "Language model continual learning (CL) has recently garnered significant\ninterest due to its potential to adapt large language models (LLMs) to dynamic\nreal-world environments without re-training. A key challenge in this field is\ncatastrophic forgetting, where models lose previously acquired knowledge when\nlearning new tasks. Existing methods commonly employ multiple\nparameter-efficient fine-tuning (PEFT) blocks to acquire task-specific\nknowledge for each task, but these approaches lack efficiency and overlook the\npotential for knowledge transfer through task interaction. In this paper, we\npresent a novel CL framework for language models called Task Skill Localization\nand Consolidation (TaSL), which enhances knowledge transfer without relying on\nmemory replay. TaSL first divides the model into `skill units' based on\nparameter dependencies, enabling more granular control. It then employs a novel\ngroup-wise skill localization technique to identify the importance distribution\nof skill units for a new task. By comparing this importance distribution with\nthose from previous tasks, we implement a fine-grained skill consolidation\nstrategy that retains task-specific knowledge, thereby preventing forgetting,\nand updates task-shared knowledge, which facilitates bi-directional knowledge\ntransfer. As a result, TaSL achieves a superior balance between retaining\nprevious knowledge and excelling in new tasks. TaSL also shows strong\ngeneralizability, suitable for general models and customizable for PEFT methods\nlike LoRA. Additionally, it demonstrates notable extensibility, allowing\nintegration with memory replay to further enhance performance. Extensive\nexperiments on two CL benchmarks, with varying model sizes (from 220M to 7B),\ndemonstrate the effectiveness of TaSL and its variants across different\nsettings.",
        "updated": "2024-08-09 17:44:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.05200v1"
    },
    {
        "title": "HistoKernel: Whole Slide Image Level Maximum Mean Discrepancy Kernels for Pan-Cancer Predictive Modelling",
        "authors": "Piotr KellerMuhammad DawoodBrinder Singh ChohanFayyaz ul Amir Afsar Minhas",
        "links": "http://arxiv.org/abs/2408.05195v1",
        "entry_id": "http://arxiv.org/abs/2408.05195v1",
        "pdf_url": "http://arxiv.org/pdf/2408.05195v1",
        "summary": "Machine learning in computational pathology (CPath) often aggregates\npatch-level predictions from multi-gigapixel Whole Slide Images (WSIs) to\ngenerate WSI-level prediction scores for crucial tasks such as survival\nprediction and drug effect prediction. However, current methods do not\nexplicitly characterize distributional differences between patch sets within\nWSIs. We introduce HistoKernel, a novel Maximum Mean Discrepancy (MMD) kernel\nthat measures distributional similarity between WSIs for enhanced prediction\nperformance on downstream prediction tasks.\n  Our comprehensive analysis demonstrates HistoKernel's effectiveness across\nvarious machine learning tasks, including retrieval (n = 9,362), drug\nsensitivity regression (n = 551), point mutation classification (n = 3,419),\nand survival analysis (n = 2,291), outperforming existing deep learning\nmethods. Additionally, HistoKernel seamlessly integrates multi-modal data and\noffers a novel perturbation-based method for patch-level explainability. This\nwork pioneers the use of kernel-based methods for WSI-level predictive\nmodeling, opening new avenues for research. Code is available at\nhttps://github.com/pkeller00/HistoKernel.",
        "updated": "2024-08-09 17:40:08 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.05195v1"
    },
    {
        "title": "Meta-Learning Guided Label Noise Distillation for Robust Signal Modulation Classification",
        "authors": "Xiaoyang HaoZhixi FengTongqing PengShuyuan Yang",
        "links": "http://arxiv.org/abs/2408.05151v1",
        "entry_id": "http://arxiv.org/abs/2408.05151v1",
        "pdf_url": "http://arxiv.org/pdf/2408.05151v1",
        "summary": "Automatic modulation classification (AMC) is an effective way to deal with\nphysical layer threats of the internet of things (IoT). However, there is often\nlabel mislabeling in practice, which significantly impacts the performance and\nrobustness of deep neural networks (DNNs). In this paper, we propose a\nmeta-learning guided label noise distillation method for robust AMC.\nSpecifically, a teacher-student heterogeneous network (TSHN) framework is\nproposed to distill and reuse label noise. Based on the idea that labels are\nrepresentations, the teacher network with trusted meta-learning divides and\nconquers untrusted label samples and then guides the student network to learn\nbetter by reassessing and correcting labels. Furthermore, we propose a\nmulti-view signal (MVS) method to further improve the performance of\nhard-to-classify categories with few-shot trusted label samples. Extensive\nexperimental results show that our methods can significantly improve the\nperformance and robustness of signal AMC in various and complex label noise\nscenarios, which is crucial for securing IoT applications.",
        "updated": "2024-08-09 16:14:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.05151v1"
    }
]