Kalman-Inspired Feature Propagation for
Video Face Super-Resolution
Ruicheng Feng , Chongyi Li , and Chen Change Loy
S-Lab, Nanyang Technological University, Singapore
{ruicheng002, ccloy}@ntu.edu.sg
lichongyi25@gmail.com
Abstract. Despitethepromisingprogressoffaceimagesuper-resolution,
video face super-resolution remains relatively under-explored. Existing
approaches either adapt general video super-resolution networks to face
datasets or apply established face image super-resolution models inde-
pendently on individual video frames. These paradigms encounter chal-
lenges either in reconstructing facial details or maintaining temporal
consistency. To address these issues, we introduce a novel framework
calledKalman-inspiredFeaturePropagation(KEEP),designedtomain-
tain a stable face prior over time. The Kalman filtering principles offer
our method a recurrent ability to use the information from previously
restoredframestoguideandregulatetherestorationprocessofthecur-
rent frame. Extensive experiments demonstrate the effectiveness of our
methodincapturingfacialdetailsconsistentlyacrossvideoframes.Code
andvideodemoareavailableathttps://jnjaby.github.io/projects/
KEEP/.
1 Introduction
ThefieldofFaceSuper-Resolution(FSR),whichfocusesonreconstructinghigh-
resolution (HR) face images from highly degraded versions, has witnessed re-
markable progress. In particular, numerous studies have successfully leveraged
varioustypesofpriorinformation,suchasgeometricfacialpriors[7,8,53],refer-
encepriors[26–28],generativepriors[2,4,45,52],andcodebookpriors[13,46,56].
These approaches have significantly advanced the realism and quality of gener-
ated face images. However, the majority of these studies are confined to still
images, with the extension to Video Face Super-Resolution (VFSR) remaining
relativelyunder-explored.Despitethesubstantialpotentialbenefitsofvideoface
restoration in various practical domains, such as the restoration of old films,
VFSR has yet to receive the same level of attention and development as its
image-based counterpart.
Two main strategies emerge for implementing VFSR. The first approach in-
volvesadaptinggeneralVideoSuper-Resolution(VSR)networks,suchasEDVR
[44], BasicVSR [3], BasicVSR++ [5], and RVRT [30], to large-scale face video
datasets [34,48]. These methods exploit temporal information and propagate
features across video frames. However, they are not specifically tailored for
4202
guA
9
]VC.sc[
1v50250.8042:viXra2 R. Feng et al.
Fig.1:ComparingmainVFSRstrategies.Weshowsevenframeswithaninterval
of6.GenericVSRmodelBasicVSR[3]failstoreconstructfacialcomponentsfaithfully.
Single-imageFSRmodelCodeFormer[56]hallucinatesunnaturalandinconsistentface
details.Ourmethod,incontrast,enablesconsistentrestorationoflow-qualityfacevideo
while preserving temporal coherence across frames.
face restoration and often fall short in reconstructing detailed facial features,
particularly in severely degraded scenarios, as depicted in Fig. 1(second row).
The second method involves applying existing face image SR models to process
each video frame independently. This frame-by-frame approach, while straight-
forward, introduces temporal inconsistencies in the video, as demonstrated in
Fig. 1(third row). This problem arises because FSR is inherently ill-posed and
the existing priors may not suffice to maintain appearance consistency through-
out the video sequence. Specifically, a single degraded image could correspond
to multiple high-resolution interpretations, leading to discrepancies and incon-
sistent structures across independently processed video frames.
In this paper, we wish to devise an effective framework for maintaining a
stable face prior over time for VFSR. We use CodeFormer [56], a representative
model that exploits codebook priors for FSR, to demonstrate how face priors
can be consistently preserved across different time frames. A pivotal aspect of
our approach is the idea that frames previously restored can act as references,
guiding and regulating the restoration process of the current frame. This strat-
egy helps minimize the divergence between consecutive frames. Moreover, this
relianceonpreviouslyrestoredframesnaturallysuggestsarecurrentframework,
enabling the effective use of information from past restorations. This intuition
aligns closely with Kalman filtering principles, or linear quadratic estimation,
which involves using a sequence of time-based measurements, despite their sta-
tistical noise and inaccuracies, to produce more accurate estimates of unknown
tupnI
RSVcisaB
remroFedoC
)sruO(
PEEKKEEP 3
variablesthanwouldbepossiblewithasinglemeasurement.Similarly,inVFSR,
faces observed over time are often noisy and inaccurate, making them suitable
for refinement using Kalman filtering techniques.
Driven by these insights, we formulate a novel method, Kalman-inspired
fEaturE Propagation (KEEP), which recurrently updates the current latent
state in CodeFormer by incorporating information from preceding frames. This
method of temporal propagation within the latent space ensures the stability
of the face prior over time, thereby capturing facial details that consistently
match in appearance. The effectiveness of KEEP is shown in Fig. 1, where it is
evident that our method delivers high-quality restoration with superior consis-
tency, compared to both generic video restoration methods fine-tuned for faces
and approaches that restore frames independently. Please refer to the supple-
mentary video to appreciate the superiority of our approach in terms of tempo-
ral consistency. A key advantage of KEEP is its robustness in handling severe
video-based degradation, outperforming single-image models. In addition, our
model demonstrates enhanced performance on non-frontal faces by providing
more stable estimations of face priors.
Insummary,themaincontributionofthisworkisanovelframeworkformain-
taining a stable and meaningful face prior for VFSR. While we demonstrate its
applicationusingtheCodeFormermethodasacasestudy,theunderlyingprinci-
plesofourframework,inspiredbytheKalmanfilteringapproach,areapplicable
to other approaches. Extensive experimental results on the VFHQ dataset [48]
and real-world data demonstrate the effectiveness of our approach in improving
both the fidelity and coherence of VFSR outputs. Compared to other state-of-
the-art methods, our model achieves superior performance with a large margin
of 0.8 dB in PSNR, while also significantly maintaining temporal coherence.
2 Related Work
Blind Face Restoration. Blind face restoration aims at recovering severely
degraded face images in the wild. Unlike natural images, faces are highly struc-
tured. This property allows researchers to incorporate prior information into
the restoration models, which have demonstrated remarkable progress in capa-
bilities to restore high-quality faces. Most existing FSR methods can be cat-
egorized into four classes: geometric priors, reference priors, generative priors,
and codebook priors. Geometric priors usually include facial elements, such as
face landmarks [8], parsing maps [7], and facial component heatmaps [53]. An-
other major line is reference-based methods that require high-quality exemplar
images. GFRNet [28] and ASFFNet [27] leverage a warped high-quality image
to extract rich details to improve facial detail restoration. DFDNet [26] con-
structs deep dictionaries with facial components from large-scale images to re-
cover fine details. Generative priors from pre-trained GAN, e.g., StyleGAN [20]
andStyleGAN2[21],areemployedthroughiterativelatentoptimizationofGAN
inversion [12,33,37]. Still, they produce face images with low fidelity and are
computationally expensive. To address this, GLEAN [2], GPEN [52], and GFP-4 R. Feng et al.
GAN [45] integrate generative priors into encoder-decoder architectures, which
estimate latent priors in one-forward pass. These methods achieve great trade-
off between quality and fidelity but usually fail when the corruption is severe.
Codebook priors [13,46,56]canberegardedasaspecialcaseofgenerativepriors.
In contrast to continuous generative priors, they squeeze the latent space into a
small finite codebook space and improve the robustness to severe degradation.
However, most existing FSR methods are image-based and thus they cannot
guarantee temporally consistent details for VFSR.
Video Super-Resolution. Most existing video restoration techniques can be
categorized into two paradigms based on their parallelizability: parallel and re-
current methods. Parallel models estimate all frames simultaneously, and the
restoration of each frame does not rely on the update of other frames. These
methods typically involve feature extraction, feature alignment, feature fusion,
and reconstruction. Representative works, FSTRN [25] and VESCPN [1], in-
troduce fast spatio-temporal networks using 3D convolutions to enhance align-
ment, combining motion compensation and super-resolution. EDVR [44] and
TDAN [42] leverage deformable convolutions for aligning adjacent frames. Be-
sides,Transformer-basedmethods[29,32]areproposedtoreconstructallframes
simultaneously by jointly extracting, aligning, and fusing features. In addition,
RVRT [30] and TTVSR [31] integrate optical flow into Transformer and enable
long-range models in videos. Empowered by the great expressive capability of
Transformer, this line of work exhibits remarkable performance improvements
over previous methods. However, they suffer from large model sizes and high
memory consumption. Recurrent methods [3,16,18] do not aggregate informa-
tionsolelyfromadjacentframes.Instead,theymaintainhiddenstatestoconvey
relevantinformationfrompreviousframesandpropagatelatentfeaturessequen-
tially, accumulating information for later restoration. For example, RBPN [14]
treated each frame as a separate source, combined iteratively in a refinement
framework. RSDN [17] divided the input into structure and detail components,
proposing a two-stream structure-detail block to learn textures. BasicVSR [3]
and BasicVSR++ [5] fused bidirectional hidden states from both past and fu-
tureframes,bringsignificantimprovements.Theyaimtofullyutilizeinformation
from the entire sequence, synchronously updating the hidden state through the
weights of the reconstruction network. Due to the recurrent nature of feature
propagation, recurrent methods experience information loss.
VideoConsistency.InadditiontoVSR,previousworksalsoattempttoinflate
image models into video models [43,47,50,51] or improve temporal consistency
with the implicit representation of the given videos [23,24,35]. For example,
Stitch-it-in-Time [43] discovers locally consistent pivots in the latent space to
provide spatially consistent transitions. Tune-A-Video [47] adopts cross-frame
attention and fine-tune it on a single video. All-in-One deflicker [23] learns a
neural atlas for each video to solve long-term inconsistency. These works either
require post-processing or optimization on a video basis. In contrast, we aim
at repurposing image FSR models and enforcing temporal consistency in the
restored face videos.KEEP 5
3 Methodology
In a corrupted face video, local textures and facial details are irrevocably lost.
Therefore, the latent codes, usually estimated by an encoder, are inaccurate to
match the real underlying priors of ground-truth video. Different from image-
based autoencoder, models in the video domain enable us to exploit evidence
accumulated from preceeding frames for better restoration. In this paper, we
repurpose the image-based CodeFormer for video FSR and propose KEEP to
estimate stable face priors in latent space over time given the noisy and inac-
curate estimations, which consequently enable temporally coherent restoration.
An appealing idea to realize KEEP is through reformulating the method in a
Kalman Filter framework.
3.1 Formulation
State Space Model. We consider observations of low-quality (LQ) video se-
quence X = {x
t
}T
t=1
with length of T, where x
t ∈
RH ×W ×3, and underlying
high-quality (HQ) sequences Y = y T . Kalman filter [19] assumes linear dy-
{ t}t=1
namicsystemsthatarecharacterizedbyastatespacemodeldrivenbyGaussian
noise
y =F y +q , (1)
t t t 1 t
−
where F is the transition matrix and q denotes process noise drawn from
t t
Gaussian noise. The observation x is measured by
t
x =Hy +r , (2)
t t t
whereH isthemeasurementmatrixandr representsmeasurementnoise.How-
t
ever, the linear assumption does not hold in some complex real-world scenarios.
Hence, the non-linear Kalman filter can be reformulated as
y =d(y ,q ) (3)
t t 1 t
−
x =h(y )+r , (4)
t t t
where d() and h() are non-linear transition and measurement models. Specif-
· ·
ically, d() can be represented by any explicit motion estimation (e.g., optical
·
flow), which defines how the current frame transits to the next one. h() com-
·
monly models the degradation in video restoration problems. As opposed to the
classicalassumptionsinKalmanfilter,themeasurementfunctionh()isunknown
·
in a blind setting. This is regarded as partially known dynamic models [39].
Inspired by VQGAN [10] and Stable Diffusion [40], we estimate underlying
latent representations Z = z T such that z can correspond to y by a
{ t }t=1 t t
generative model g , given by
θ
y =g (z ). (5)
t θ t
Instead of directly estimating individual pixels, modeling the low-dimensional
latent code is computationally more efficient and focuses on more perceptually
significant variations of the data. The graphical model is depicted in Fig. 2 (a).6 R. Feng et al.
Latent Space 𝑧̂& 𝑦,
𝑧 ! 𝑓 𝑧 " 𝑓 ⋅⋅⋅ 𝑧 # 𝑧 $̂& %! $ 𝑔 ’ $
𝑓
𝑔 𝑔 𝑔 𝒦
𝑦 ! 𝑦 " 𝑦 # 𝑧 $̃ %! KGN 𝑧̂% $ Δ𝑧
ℎ ℎ ℎ 𝑧 $̃ $ + $
𝑧̃
𝑥 𝑥 𝑥 𝑥 𝑒 − $
! " # $ 𝑧̃
$
Pixel Space
(a) (b)
Fig.2: (a) Graphical model of state space. It defines the underlying dynamic
system model, where f describes how the latent states z t transit over time, g is a
generativemodel,andhmodelsthedegradationfromcleanframey ttodegradedframe
x t. (b) Block diagram of Kalman filter model. In each time step, a predictive
state from previous frame zˆ+ (Blue dash box) and new observed state of current
t 1
frame x t (Red dash box) are−fused by Kalman gain t from Kalman Gain Network
(KGN) to produce more accurate estimates. The comK bined state zˆ+ is then used to
t
generate the estimated clean frame yˆ t by g θ. Note that z˜ 1 goes along with z˜ t 1 as an
−
anchor and it is omitted in the diagram for simplicity.
Kalman Filter Model. TheprinciplesofKalmanfiltercanbeformulatedbya
two-stepprocedure,i.e.,stateprediction andstateupdate.Theoverviewdiagram
is illustrated in Fig. 2 (b). In this problem, the observation is a face image x ,
t
and the state is z .
t
In the state prediction step, the model predicts the prior estimation zˆ−t of
thecurrentstatez basedontheposteriorestimationzˆ+ ofthepreviousstate
t t 1
and the dynamic model. Specifically, the prior estimatio− n of latent state z and
t
estimation of observation x are computed as
t
zˆ−t =f(zˆ+
t
1), (6)
−
xˆ−t =h(g θ(zˆ−t )). (7)
The system dynamics f define how the latent state Z evolves over time, and it
incorporates any control inputs that might affect the current state.
In the state update step, a posterior state estimation
zˆ+
is computed based
t
on the prior estimation and new observations x as
t
zˆ+
t
=zˆ−t + Kt∆z t, (8)
where is conceptually referred to as the Kalman gain and ∆z as the inno-
t t
K
vation, i.e., the residual between the prior estimation zˆ−t and approximation of
current state from x , given by
t
∆z
t
=zˆ−t −z˜ t, (9)
where z˜ =e(x ). Note that this formula differs from the original Kalman filter
t t
whichminimizestheinnovationof∆x
t
(i.e.,residualbetweenx
t
andxˆ−t ),sinceKEEP 7
𝑧 $̃ 𝑧̃ !#$ 𝑦’ !#$ Φ !#$→! 𝑣 !& #$
CFA
KGN
Uncertainty Net
𝑥 ! ℇ " 𝑧̃ ! Gain Net 𝑧 !̂# ℇ # 𝒲 𝒟 $
𝒦
!
State Predict
Update state
𝑋 Kalman Filter Network 𝑧 !̂" 𝑣 !& 𝑌 𝑦&
!
Non-temporal Temporal 𝒲 Warping
Fig.3: Overview of the proposed KEEP.Itconsistsoffourmodules:encoder L,
E
decoder Q, Kalman filter network, and CFA. We illustrate the information flow in
D
one timestep.
the assumption of available measurement function h() is no longer valid in our
·
setting. An original measurement system models how observation x is derived
t
fromthelatentstatez ,formallyx =h(z ).InspiredbyKFNet[55],wedirectly
t t t
estimate the state z given new observation x , i.e., mapping x to z˜ with an
t t t t
estimator e.
The remaining problem is to compute the Kalman gain . As discussed in
t
K
KalmanNet [39], covariance estimation is intractable when dealing with high-
dimensional signals. Additionally, the second-order statistical moments are only
used for calculating Kalman gain. Inspired by this, we directly learn the gains
fromthedatadistributionanddonotexplicitlymaintainanestimationofcovari-
ances. Additionally, we follow [51] to include z˜ of the first frame as anchor into
1
KGN for Kalman gain estimation. Then, the final predicted yˆ can be derived
t
by
yˆ =g (zˆ+). (10)
t θ t
3.2 Parameterized Models
As shown in Fig. 2 (b), we will parameterize or define the system dynamics f,
observation estimator e, Kalman Gain Nets (KGN), and generative model g in
θ
this section. The overall framework is shown in Fig. 3.
Generative Model. The generative model g is generally parameterized by a
θ
backboneofCodeFormer[56],whichconsistsofaLQencoder ,aHQencoder
L
E
,acodebooklookupTransformerandquantizationlayerT ,andadecoder .
H Q
E D
For simplicity, denotes the decoder with the codebook lookup Transformer
Q
andquantizatioD nlayerabsorbed.Basically,thepredictedyˆ isgivenby (zˆ+),
t DQ t
and the observed state z˜ is approximated by
t
z˜ =e(x )= (x ). (11)
t t L t
E
⋅⋅⋅
⋅⋅⋅
⋅⋅⋅
⋅⋅⋅8 R. Feng et al.
State Dynamic System. Thesystemdynamicsdefinehowthesystemevolves
over time, and it incorporates any control inputs that might affect the current
state. The prediction for the state z at the current timestep is achieved via
t
state extrapolation. In particular, given the posterior estimation of previous
state
zˆ+
, we define the dynamic model by
t 1
−
zˆ−t =f(zˆ+
t
−1)= EH(ω( DQ(zˆ+
t
−1),Φ
t −1
→t)). (12)
where Φ denotes the flow estimated from LQ frames x to x and ω the
t 1 t t 1 t
spatial
w− ar→
ping modules. Specifically, we first decode the
es−
timated code
zˆ+
t 1
of the previous frame to get the estimation of yˆ = (zˆ+ ) and warp it− to
t −1 DQ t −1
the current frame. Then, it was encoded back in the latent space to obtain the
prediction of the current state zˆ−t .
Kalman Filter System. Given the approximated observed state z˜ and prior
t
estimationzˆ−t fromsystemdynamics,thefiltersystemaimstopromotetemporal
information propagation and maintain stable latent code priors. In particular,
thefilterrecursivelyfusesboththeestimationtoformamoreaccurateposterior
estimate of the current state zˆ+, which is also known as state update.
t
AccordingtoEqn.8,amoreintuitivewaytoexpresstheupdatedstateisalin-
earinterpolationzˆ+
t
= Ktzˆ−t +(1 −Kt)z˜
t
for
Kt
normalizedintherangeof[0,1].
The Kalman Gain measures the estimated accuracy of the predicted states
t
K
compared to the approximated observed states, to update the state and reduce
the uncertainty. As illustrated in Fig. 3, the Kalman gain is approximated via
KalmanGainNetwork(KGN),consistingoftwodistinctparameterizedmodules,
i.e.,uncertaintynetworkandgainnetwork.Theuncertaintyassociatedwiththe
currentpredictionisimplicitlyestimatedbyanuncertaintynetworkconstructed
by spatial-temporal attention [47] and temporal attention layers (or any other
recurrent networks). Then, a gain network calculates Kalman gain for each
t
K
codetoken.Pleaserefertothesupplementaryfilesforthedetailedarchitectures.
3.3 Local Temporal Consistency
Inspired by [51], we adopt cross-frame attention (CFA) layers in the decoder to
further promote local temporal consistency to regularize the information prop-
agation. Specifically, given the latent features from the previous frame v and
t 1
−
current frame v . They are projected onto the embedding space and output the
t
features v i′ by v i′ =Attn(Q,K,V)=softmax(Q √K dT ) ·V, where
Q=W v ,K =W v ,V =W v . (13)
Q t K t 1 V t 1
· · − · −
Intuitively, cross-frame attention modules can be regarded as searching and
matching similar patches from the previous frame and fusing them correspond-
ingly. This module facilitates temporal information propagation in the decoder.
WeadoptCFAmodulesonfeaturesofsmallscale16and32toavoidintroducing
blur to the decoded results.KEEP 9
4 Experiments
Dataset. VFHQ [48] contains over 15,000 high-quality video clips of diverse
interviews and talk shows, where 15,381 clips are used for training and 50 clips
are reserved for testing. Each sequence consists of 100 to 900 frames of reso-
lution 512 512. Following common practice [6,36,45,48], we adopt blind set-
×
tings in all experiments. Specifically, we apply random blur, resize, and noise
asimage-baseddegradations.Moreover,videocompressionisadoptedtocontrol
thevideoqualitybychangingstreamingbitrate.Foracomprehensiveevaluation,
wesynthesizethreesplitsoftheVFHQ-Testdatasetcontainingdifferentlevelsof
degradation, denoted as VFHQ-mild, VFHQ-medium, and VFHQ-heavy. They
follow the same degradation model but differ in the degree of noise, blur, and
compression. In addition to synthetic data, we also collect real corrupted video
for testing. Please refer to the supplementary files for more details.
Alignment. Pre-trained image models for face restoration are trained on the
FFHQ dataset [20], where each image is automatically cropped and aligned.
Hence, employing pre-trained models requires a similar alignment phase on
VFHQdataset[48].However,thediscretestep(i.e.,cropping)issensitivetothe
detected locations of landmarks, which can consequently result in unintentional
temporal inconsistencies. Inspired by the work of Fox et al. [11] and Tzaban et
al. [43], we employ a Gaussian lowpass filter over the landmarks. We find that
this smoothing can significantly attenuate the inconsistencies induced by the
alignment step. See supplementary files for more details.
Implementations. For all stages of training, we initialize all networks with
Kaiming Normal [15] and train them using Adam optimizer [22], and a batch
sizeof4foralltheexperiments.Thelearningrateissetto2 10 −4forstagesIand
×
II, and 1 10 −4 for stage III. The models are trained with 800k, 400k, and 50k
×
iterationsforthreestages,respectively.WeimplementourmodelswithPyTorch
[38] and train them using NVIDIA Tesla V100 GPUs. Hyper-parameters λ ,
1
λ VGG, and λ
GAN
are set to 10 −2, 1, and 1 10 −2. We use GMFlow [49] for
×
optical flow estimation.
Metrics. For quantitative evaluation, we evaluate the fidelity of restoration us-
ing PSNR, SSIM, and LPIPS [54]. In addition, we evaluate the identity preser-
vation scores, termed as IDS, by cosine similarity of the off-the-shelf identity
detection network ArcFace [9]. Besides, we follow Tzaban et al. [43] to measure
the pose consistency using Average Keypoint Distance (AKD), which is quanti-
fied by the average distance of detected landmarks between the generated and
ground-truth video frames. In addition to the above single-frame quality eval-
uation, we also measure the fluctuation of identity/landmarks across frames.
Hence, σ measures the standard deviation of identity similarity over the en-
IDS
tirevideo.Weexpectaconsiderableidentitydriftinthegeneratedvideoswithout
local identity jitter, where σ is supposed to be low. Similarly, we use σ
IDS AKD
to measure the standard deviation of keypoint distances over the video, which
quantifies the temporal consistency of the pose.10 R. Feng et al.
Table 1: Quantitative comparison on the VFHQ-mild. Red and Blue indicate
thebestandthesecondbestresults.Fullresultsonothertestpartitions(mediumand
heavy) are presented in the supplementary material.
Method PSNR ↑SSIM ↑LPIPS
↓
IDS
↑
AKD
↓
σIDS( ×10−2) ↓σAKD↓
GPEN[52] 25.51930.7517 0.2988 0.714211.4691 4.7416 3.5109
GFPGAN[45] 26.29330.7795 0.2482 0.743710.5467 4.5700 3.6482
RestoreFormer[46] 25.57200.7344 0.3195 0.753010.5354 4.7159 3.4122
CodeFormer[56] 24.65970.7454 0.2742 0.627211.4983 6.3726 3.6927
EDVR[44] 26.60510.7858 0.2484 0.719511.6220 4.8048 3.5829
BasicVSR[3] 26.04580.7765 0.2496 0.697311.3679 5.0343 3.6054
BasicVSR++[5] 27.19960.8057 0.1958 0.764111.3136 5.2543 4.6425
KEEP (Ours) 27.99940.8267 0.1619 0.7960 8.8182 3.6866 3.2538
4.1 Comparison with State-of-the-Art Methods
Baselines. We compare our method to two categories of approaches. i) Image-
based Face SR models (CodeFormer [56], GPEN [52], GFPGAN [45], Restore-
Former [46]) are used to generate face videos frame-by-frame. ii) We retrain the
general VSR models (EDVR [44], BasicVSR [3], BasicVSR++ [5]) on VFHQ
dataset [48]. The degradation settings remain unchanged as our experiments
while other training settings follow their original papers.
Quantitative Evaluation. The quantitative results are listed in Table 1. We
observethatourmethodachievesbetterresultsthanexistingmethodsacrossall
the metrics. The results indicate that KEEP can faithfully recover facial details
while preserving the identity. Our method also maintains temporal coherence
across frames, as quantified by σ and σ , which represent the fluctua-
IDS AKD
tion of restored face identities and facial shapes. Though exhibiting structural
distortions and artifacts (See Fig. 4), general VSR models (EDVR, BasicVSR,
BasicVSR++) typically achieve higher performance on fidelity metrics (PNSR,
SSIM, LPIPS) than single-image FSR models. This suggests that image-based
models produce high-quality but relatively low-fidelity results. Inconsistency
could be introduced when latent code estimation is noisy and inaccurate.
Qualitative Evaluation.InFig.4,wecanobservethatthecomparedmethods
failtoreconstructconsistentappearanceswithperceptuallypleasantdetails.For
example, GFPGAN tends to hallucinate facial details (e.g., ears in the second
frame and incomplete glass in the last frame of the left example). CodeFormer
produces unnatural facial shapes (e.g., eyes), and BasicVSR leaves severe arti-
facts on the face images. In the last frame of the right example, both GFPGAN
and CodeFormer generate unpleasant eyes (see yellow arrows). In contrast, our
KEEP exploits temporal information and restores finer and coherent facial de-
tails. We refer readers to supplementary files for more video results.KEEP 11
Fig.4: Qualitative comparison on the VFHQ.OurKEEPproduceshigh-fidelity
face videos with faithful and consistent details. See arrows for details.
Table 2: Ablation study of variant net- Table 3: Ablation study on optical flow
works. estimator.
Models LPIPS IDS AKD Estimator LPIPS IDS AKD
↓ ↑ ↓ ↓ ↑ ↓
w/oCFA 0.1621 0.79708.9029 PWC-Net[41] 0.1623 0.79578.7839
GMFlow[49](Ours) 0.1619 0.79608.8182
w/oKFN 0.1721 0.77739.1952
Fullmodel 0.1619 0.79608.8182
4.2 More Analysis
Effectiveness of KFN. We first investigate the effectiveness of the Kalman
Filter Network (KFN). As shown in Table 2, removing KFN results in worse
performance on LPIPS, IDS, and AKD. The results suggest the design of KFN
is the key to promoting temporal consistency and identity preservation.
Effectiveness of CFA. Table 2 also shows that Cross-Frame Attention (CFA)
can further improve the performance. Though not clearly reflected in the num-
ber, we further qualitatively show the effectiveness of KFN and CFA in the
supplementary video. From the demo video, we can observe that 1) adopting
KFN can ensure consistency in global style and maintain the global appearance
oftherecoveredfaces.2)addingCFAcanfurthercoherentlyrenderlocaltexture
details (e.g., hair), and suppress flicker.
tupnI
NAGPFG
remroFedoC
RSVcisaB
sruO
TG12 R. Feng et al.
Frame GFPGAN CodeFormer RestoreFormer EDVR
BasicVSR BasicVSR++ Ours GT
Frame GFPGAN CodeFormer RestoreFormer EDVR
BasicVSR BasicVSR++ Ours GT
Fig.5: Comparison of temporal flicker.Weselecteachframe’scolumn(redlines)
and show the changes across time. Image-based models (GFPGAN, CodeFormer, and
RestoreFormer) have obvious discontinuity around the eyes and wrinkles, and general
VSR methods leave artifacts behind. In contrast, by maintaining stable facial priors
and aggregating temporal information, our method remarkably suppresses temporal
jitters and promotes coherent local details.
(a) GT (b)𝒟!(𝑧̃") (c) Error map of (b) (d)𝒟!(𝑧̂"#) (e) Error map of (d)
Fig.6: Illustration of predicted state and observed state through Decoder Q. The
D
teeth in red boxes illustrate a slight difference between z˜ t and zˆ−t when decoded in
pixel space, suggesting the potential to supplement and fuse information to obtain a
stable latent code.
Effectiveness of Various Flow Estimator. We compare models with differ-
entflowestimatorsinTable3.Theresultssuggestthattheaccuracyofestimated
flows does not significantly affect the final performance. We conjecture this can
be attributed to two factors: 1) Minor misalignment in pixel space can be rea-
sonablydiminishedasthelatentcodeishighlydownsampledbyafactorof32 ,
×
at which level the latent representations are less sensitive to small spatial dis-
crepancies present in the pixel space. 2) Other modules can compensate for the
inaccuracy caused by flow estimators in a joint training fashion.
Analysis of Flickering. We extract a short vertical segment of pixels from
each frame and stack them horizontally to visualize the jittering issues within
the video. In particular, existing methods demonstrate clear jitter and flicker
acrosstime,whileourmethodshowsbettertemporalconsistency.Fig.5demon-
strates that other image-based models bring obvious jitters around eyes, and
general VSR methods leave behind artifacts, while our method could remark-
ably suppress temporal jitters and promote coherent local details.KEEP 13
IDS=0.4552 IDS=0.7589
IDS=0.6392 IDS=0.7663
CodeFormer Ours GT
Fig.7: Identity similarity across frames. Our method preserves the identity of
inputimagesandexhibitslessfluctuationovertime.CodeFormerresultsoftwoframes
(greenandreddashedline)exemplifyanabruptchangeofidentityintherightfigure,
while our method maintains a stable identity both quantitatively and qualitatively.
Input GFPGAN CodeFormer BasicVSR BasicVSR++ Ours GT
Fig.8: Qualitative comparison on different levels of degradation. Our KEEP
maintains high-fidelity in various degradations.
Analysis of Latent Space.Sincethetruestatez isunavailable,weindirectly
t
analyze the predicted state z˜
t
and observed state zˆ−t by decoding them to the
pixel space through Decoder . As shown in Fig. 6, the areas around teeth
Q
D
exhibitaslightdifference,whilemostremainingpartsinerrormapsshowsimilar
decoded results. This indicates that the predicted and observed states could
supplement each other to obtain a more accurate estimation of z , which is
t
where the power of Kalman filter lies.
Identity Preservation. We show the identity similarity across frames of one
representative video clip in Fig. 7 As illustrated in the left figure, our method
achieves better identity preservation and less identity jitter within the video,
compared to the single-image methods. We also exemplify that the identity of
CodeFormer results can change abruptly within several frames. The identity
score of CodeFormer increases from 0.4552 to 0.6392, and turns down in later
frames(Seeredcurve),demonstratinggreatfluctuationacrosstime.Incontrast,
our method maintains a stable identity both quantitatively and qualitatively.
Various Degradations. Fig. 9a shows that KEEP is consistently better than
the compared methods across different difficulty levels. Fig. 8 demonstrates re-
dlim
muidem
yvaeh
06
emarF
56
emarF14 R. Feng et al.
28
VFHQ-mild VFHQ-medium VFHQ-heavy
27
26
25
24
GPEN GFPGA RN estoreFormer CodeFormer EDVR BasicVSR BasicVSR++ Ours
0.8
VFHQ-mild VFHQ-medium VFHQ-heavy
0.7
0.6
0.5
GPEN GFPGA RN estoreFormer CodeFormer EDVR BasicVSR BasicVSR++ Ours
Input CodeFormer KEEP (ours) GT
(a) PNSRandIDSondifferentpartitions. (b) Comparisonsonnon-frontalfaces.
Fig.9: (a) Our methods achieve consistently better performance on various levels of
degradation.(b)WhileCodeFormerfailstorestoreeyestothesechallengingcases,our
method can still produce plausible facial elements.
sults on different levels of degradation. We draw the following observations. 1)
Even GFPGAN and CodeFormer can restore plausible results on frames with
mild degradation, the performance significantly deteriorates (e.g., eyes) upon
heavier degradation. 2) Our method achieves better results by considering com-
plementary information between adjacent frames and maintaining stable face
priors. In addition, our method is appealing in handling heavy degradation.
Non-Frontal-View Faces.InFig.9b,ourmodelshowsenhancedperformance
on non-frontal faces by providing more stable face priors estimations. While the
single-image model CodeFormer cannot recover the eyes, our KEEP is still able
to show robustness to these challenging cases.
5 Conclusion
We present a novel framework, KEEP, aiming at resolving the challenges as-
sociated with facial detail and temporal consistency in video face restoration.
The proposed method demonstrates a unique capability to maintain a stable
face prior over time, which is achieved by Kalman filtering principles, where our
approach recurrently incorporates information from previously restored frames
to guide and regulate the restoration process of the current frame. Extensive
experiments demonstrate the efficacy of KEEP in consistently capturing facial
details across video frames and keeping the temporal stability of face videos.
RNSP
SDIKEEP 15
Acknowledgment
This study is supported under the RIE2020 Industry Alignment Fund Industry
CollaborationProjects(IAF-ICP)FundingInitiative,aswellascashandin-kind
contribution from the industry partner(s).
References
1. Caballero,J.,Ledig,C.,Aitken,A.,Acosta,A.,Totz,J.,Wang,Z.,Shi,W.:Real-
timevideosuper-resolutionwithspatio-temporalnetworksandmotioncompensa-
tion. In: CVPR (2017)
2. Chan,K.C.,Wang,X.,Xu,X.,Gu,J.,Loy,C.C.:GLEAN:Generativelatentbank
for large-factor image super-resolution. In: CVPR (2021)
3. Chan, K.C., Wang, X., Yu, K., Dong, C., Loy, C.C.: BasicVSR: The search for
essential components in video super-resolution and beyond. In: CVPR (2021)
4. Chan,K.C.,Xu,X.,Wang,X.,Gu,J.,Loy,C.C.:GLEAN:Generativelatentbank
for large-factor image super-resolution and beyond. TPAMI (2022)
5. Chan, K.C., Zhou, S., Xu, X., Loy, C.C.: Basicvsr++: Improving video super-
resolution with enhanced propagation and alignment. In: CVPR (2022)
6. Chan,K.C.,Zhou,S.,Xu,X.,Loy,C.C.:Investigatingtradeoffsinreal-worldvideo
super-resolution. In: CVPR (2022)
7. Chen,C.,Li,X.,Yang,L.,Lin,X.,Zhang,L.,Wong,K.Y.K.:Progressivesemantic-
aware style transformation for blind face restoration. In: CVPR (2021)
8. Chen, Y., Tai, Y., Liu, X., Shen, C., Yang, J.: FSRNet: End-to-end learning face
super-resolution with facial priors. In: CVPR (2018)
9. Deng,J.,Guo,J.,Xue,N.,Zafeiriou,S.:ArcFace:Additiveangularmarginlossfor
deep face recognition. In: CVPR (2019)
10. Esser,P.,Rombach,R.,Ommer,B.:Tamingtransformersforhigh-resolutionimage
synthesis. In: CVPR (2021)
11. Fox,G.,Tewari,A.,Elgharib,M.,Theobalt,C.:StyleVideoGAN:Atemporalgen-
erative model using a pretrained stylegan. In: BMVC (2021)
12. Gu,J.,Shen,Y.,Zhou,B.:Imageprocessingusingmulti-codeganprior.In:CVPR
(2020)
13. Gu,Y.,Wang,X.,Xie,L.,Dong,C.,Li,G.,Shan,Y.,Cheng,M.M.:VQFR:Blind
face restoration with vector-quantized dictionary and parallel decoder. In: ECCV
(2022)
14. Haris, M., Shakhnarovich, G., Ukita, N.: Recurrent back-projection network for
video super-resolution. In: CVPR (2019)
15. He,K.,Zhang,X.,Ren,S.,Sun,J.:Delvingdeepintorectifiers:Surpassinghuman-
level performance on imagenet classification. In: ICCV (2015)
16. Huang, Y., Wang, W., Wang, L.: Bidirectional recurrent convolutional networks
for multi-frame super-resolution. In: NeurIPS (2015)
17. Isobe, T., Jia, X., Gu, S., Li, S., Wang, S., Tian, Q.: Video super-resolution with
recurrent structure-detail network. In: ECCV (2020)
18. Isobe, T., Zhu, F., Wang, S.: Revisiting temporal modeling for video super-
resolution. In: BMVC (2020)
19. Kalman, R.E.: A new approach to linear filtering and prediction problems (1960)
20. Karras,T.,Laine,S.,Aila,T.:Astyle-basedgeneratorarchitectureforgenerative
adversarial networks. In: CVPR (2019)16 R. Feng et al.
21. Karras, T., Laine, S., Aittala, M., Hellsten, J., Lehtinen, J., Aila, T.: Analyzing
and improving the image quality of stylegan. In: CVPR (2020)
22. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. In: ICLR
(2015)
23. Lei, C., Ren, X., Zhang, Z., Chen, Q.: Blind video deflickering by neural filtering
with a flawed atlas. In: CVPR (2023)
24. Lei,C.,Xing,Y.,Chen,Q.:Blindvideotemporalconsistencyviadeepvideoprior.
In: NeurIPS (2020)
25. Li, S., He, F., Du, B., Zhang, L., Xu, Y., Tao, D.: Fast spatio-temporal residual
network for video super-resolution. In: CVPR (2019)
26. Li, X., Chen, C., Zhou, S., Lin, X., Zuo, W., Zhang, L.: Blind face restoration via
deep multi-scale component dictionaries. In: ECCV (2020)
27. Li, X., Li, W., Ren, D., Zhang, H., Wang, M., Zuo, W.: Enhanced blind face
restoration with multi-exemplar images and adaptive spatial feature fusion. In:
CVPR (2020)
28. Li, X., Liu, M., Ye, Y., Zuo, W., Lin, L., Yang, R.: Learning warped guidance for
blind face restoration. In: ECCV (2018)
29. Liang, J., Cao, J., Fan, Y., Zhang, K., Ranjan, R., Li, Y., Timofte, R., Van Gool,
L.: Vrt: A video restoration transformer. IEEE Transactions on Image Processing
(2024)
30. Liang, J., Fan, Y., Xiang, X., Ranjan, R., Ilg, E., Green, S., Cao, J., Zhang, K.,
Timofte, R., Gool, L.V.: Recurrent video restoration transformer with guided de-
formable attention. NeurIPS (2022)
31. Liu,C.,Yang,H.,Fu,J.,Qian,X.:Learningtrajectory-awaretransformerforvideo
super-resolution. In: CVPR (2022)
32. Liu, Z., Ning, J., Cao, Y., Wei, Y., Zhang, Z., Lin, S., Hu, H.: Video swin trans-
former. In: CVPR (2022)
33. Menon, S., Damian, A., Hu, S., Ravi, N., Rudin, C.: Pulse: Self-supervised photo
upsampling via latent space exploration of generative models. In: CVPR (2020)
34. Nagrani, A., Chung, J.S., Zisserman, A.: Voxceleb: A large-scale speaker identifi-
cation dataset. In: Interspeech (2017)
35. Ouyang, H., Wang, Q., Xiao, Y., Bai, Q., Zhang, J., Zheng, K., Zhou, X., Chen,
Q., Shen, Y.: Codef: Content deformation fields for temporally consistent video
processing. In: CVPR (2024)
36. Pan, J., Bai, H., Dong, J., Zhang, J., Tang, J.: Deep blind video super-resolution.
In: ICCV (2021)
37. Pan,X.,Zhan,X.,Dai,B.,Lin,D.,Loy,C.C.,Luo,P.:Exploitingdeepgenerative
priorforversatileimagerestorationandmanipulation.TPAMI44(11),7474–7489
(2021)
38. Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin, Z.,
Desmaison, A., Antiga, L., Lerer, A.: Automatic differentiation in pytorch (2017)
39. Revach, G., Shlezinger, N., Ni, X., Escoriza, A.L., Van Sloun, R.J., Eldar, Y.C.:
Kalmannet: Neural network aided kalman filtering for partially known dynamics.
IEEE Transactions on Signal Processing (2022)
40. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., Ommer, B.: High-resolution
image synthesis with latent diffusion models. In: CVPR (2022)
41. Sun, D., Yang, X., Liu, M.Y., Kautz, J.: Pwc-net: Cnns for optical flow using
pyramid, warping, and cost volume. In: CVPR (2018)
42. Tian, Y., Zhang, Y., Fu, Y., Xu, C.: TDAN: Temporally-deformable alignment
network for video super-resolution. In: CVPR (2020)KEEP 17
43. Tzaban, R., Mokady, R., Gal, R., Bermano, A., Cohen-Or, D.: Stitch it in time:
Gan-based facial editing of real videos. In: SIGGRAPH Asia (2022)
44. Wang,X.,Chan,K.C.,Yu,K.,Dong,C.,ChangeLoy,C.:EDVR:Videorestoration
with enhanced deformable convolutional networks. In: CVPRW (2019)
45. Wang, X., Li, Y., Zhang, H., Shan, Y.: Towards real-world blind face restoration
with generative facial prior. In: CVPR (2021)
46. Wang, Z., Zhang, J., Chen, R., Wang, W., Luo, P.: Restoreformer: High-quality
blind face restoration from undegraded key-value pairs. In: CVPR (2022)
47. Wu, J.Z., Ge, Y., Wang, X., Lei, S.W., Gu, Y., Shi, Y., Hsu, W., Shan, Y., Qie,
X.,Shou,M.Z.:Tune-a-video:One-shottuningofimagediffusionmodelsfortext-
to-video generation. In: ICCV (2023)
48. Xie, L., Wang, X., Zhang, H., Dong, C., Shan, Y.: VFHQ: A high-quality dataset
and benchmark for video face super-resolution. In: CVPR (2022)
49. Xu,H.,Zhang,J.,Cai,J.,Rezatofighi,H.,Tao,D.:Gmflow:Learningopticalflow
via global matching. In: CVPR (2022)
50. Xu, Y., AlBahar, B., Huang, J.B.: Temporally consistent semantic video editing.
In: ECCV (2022)
51. Yang, S., Zhou, Y., Liu, Z., , Loy, C.C.: Rerender a video: Zero-shot text-guided
video-to-video translation. In: SIGGRAPH Asia (2023)
52. Yang, T., Ren, P., Xie, X., Zhang, L.: Gan prior embedded network for blind face
restoration in the wild. In: CVPR (2021)
53. Yu, X., Fernando, B., Ghanem, B., Porikli, F., Hartley, R.: Face super-resolution
guided by facial component heatmaps. In: ECCV (2018)
54. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: CVPR (2018)
55. Zhou, L., Luo, Z., Shen, T., Zhang, J., Zhen, M., Yao, Y., Fang, T., Quan, L.:
KFNet:LearningtemporalcamerarelocalizationusingKalmanfiltering.In:CVPR
(2020)
56. Zhou,S.,Chan,K.C.,Li,C.,Loy,C.C.:Towardsrobustblindfacerestorationwith
codebook lookup transformer. In: NeurIPS (2022)Kalman-Inspired Feature Propagation for
Video Face Super-Resolution
- Supplementary Materials -
Ruicheng Feng , Chongyi Li , and Chen Change Loy
S-Lab, Nanyang Technological University, Singapore
{ruicheng002, ccloy}@ntu.edu.sg
lichongyi25@gmail.com
A Method Details
A.1 Detailed Architecture
Kalman Filter Network. Our Kalman Filter Network, as illustrated in Fig-
ure A1, adopts two distinct parameterized modules in implicitly estimating un-
certainty and Kalman gain. Note that the dynamic model from
zˆ+
to prior
t 1
−
estimation zˆ−t is omitted for simplicity in the illustration. The uncertainty net-
work implicitly estimates the uncertainty of shape h w c, and the Kalman
× ×
gain network calculates the corresponding Kalman gain of shape h w for
t
K × each code token. The Spatial-Temporal Attention (ST-Attn) takes the current
observed latent code z˜ as a query and attends to the combination of the first
t
frame z˜ and previous frame z˜ . Inspired by [13], the spatial-temporal atten-
1 t 1
−
tion also takes the latent code of the first frame z˜ as input, which serves as an
1
anchor prior to all temporal attention.
Uncertainty Net Gain Net
𝑧̃
$ 𝑧 !̃ #$ ⋅⋅⋅⋅⋅⋅ 𝑧 !̃ 𝑊! ⋅⋅⋅
𝒦 ! 𝑊" ⋅⋅⋅
𝑧 !̃ Update state 𝑧 !̂" {𝑧 $̃ ,𝑧 !̃ #$}
𝑊# ⋅⋅⋅
Attn Block Conv Block 𝑧 !̂#
(a) Kalman Filter Network (b) Spatial-Temporal Attention
Fig.A1: Illustration of Kalman Filter Network. (a) We unfold and show one
timestepoftheKalmanfilternetwork.Thenetworkmainlyconsistsoftwoparametriza-
tionmodules,i.e.,uncertaintynetworkandthegainnetwork.Here“ST-Attn” and“T-
Attn” represent spatial-temporal attention and temporal attention, respectively. (b)
The Spatial-Temporal Attention (ST-Attn) takes estimated observed latent code for
current frame z˜ t as a query and attends to the combination of the first frame z˜ 1 and
previous frame z˜ t 1.
−
4202
guA
9
]VC.sc[
1v50250.8042:viXra
nttA-TS NFF nttA-T vnoC vnoC vnoC vnoC diomgiS
xamtfoS2 R. Feng et al.
𝑣%
!
CFA 𝑣 !% $# CFA 𝑣 !% CFA 𝑣 !% "# 𝛽 𝛼 Softmax
CFT CFT CFT
Block Block Block Conv 𝑊! 𝑊" 𝑊!
𝑇( 𝑇( 𝑇(
𝐹 !& $# 𝑧̂ !" $# 𝐹 !& 𝑧̂ !" 𝐹 !& "# 𝑧̂ !" "# 𝐹 !& 𝐹 !’ 𝑣! 𝑣 !% $#
(a) Decoder 𝒟( (b) CFT (c) CFA
Fig.A2: Illustration of the integrated decoder. Controllable Feature Transfor-
mation (CFT) and Cross-Frame Attention (CFA). T Q is a codebook lookup Trans-
former and quantization layer borrowed from CodeFormer [17]. Blocks are the basic
conv blocks in the decoder. CFT is tailored for modulating the features of decoder F d
by the encoder’s features F e. CFA is adopted in the decoder to further promote local
temporal consistency to regularize the information propagation.
Integrated Decoder. Figure A2 depicts how CFT and CFA layers are in-
tegrated into the decoder. Following [17], we leverage the encoder features to
modulatethecorrespondingdecoderfeatures.DenotedF andF astheencoder
e d
and decoder features, respectively, the network learns an affine transformation
defined by α and β.
v =F +(α F +β), (1)
t d d
·
whereα,β = (F ),and ismultipleconvolutionblocks.TheCFTmodulesare
e
C C
adoptedatmultiplescales16,32,64,sinceshallowfeaturesofencoderwouldalso
bringforwardcorruptedinformationtothedecoderandyieldblurryresults.This
designfacilitatesfidelityreservationofeachframeandhenceimprovestemporal
coherence.
Tofurtherenforcetemporalinformationpropagationandreducelocaljitters,
weadoptcross-frameattentionmodules,whichsearchandmatchsimilarfeatures
from the previous frame and attend to them correspondingly. Specifically, given
the latent features from the previous frame v and current frame v . They
t 1 t
−
are projected onto the embedding space and output the features v i′ by v i′ =
Attn(Q,K,V)=softmax(QKT
) V, where
√d ·
Q=W v ,K =W v ,V =W v . (2)
Q t K t 1 V t 1
· · − · −
This module facilitates temporal information propagation in the decoder. We
adopt CFA modules on features of small scale 16 and 32 to avoid introducing
blur to the decoded results.
A.2 Algorithm Pseudocode
As shown in the below Algorithm , we present the pseudocode of our method.
In this algorithm, we only show the process of inference.
⋅⋅⋅ ⋅⋅⋅ ⋅⋅⋅KEEP 3
Algorithm 1: Detailed algorithm of KEEP.
L, H, Q LQ Encoder / HQ Encoder/ Decoder;
E E D ←
φ Kalman Gain network;
←
Φ t 1 t Optical flow from previous frame;
− → ←
ω Spatial warping operation;
←
T length of clips;
←
Initialize z˜ 1, zˆ+ 1, yˆ 1;
for t=2,3, ,T do
···
State Prediction:
zˆ−t ←EH(ω(yˆ
t
−1,Φ
t −1
→t));
State Update:
z˜
t
L(x t);
←E
t
φ(z˜ 1,z˜
t
1,z˜ t);
zK ˆ+
t
← ←(1 −Kt)− zˆ−t + Ktz˜ t;
yˆ (zˆ+))
t ←DQ t
end
A.3 Training Scheme.
Codebook Pre-Training (Stage I). Following CodeFormer [17], we first pre-
train a codebook within a quantized autoencoder. Unlike TAST [5], the learned
codebook is still image-based and does not involve temporal information. Pre-
cisely, given a HQ frame y
t ∈
RH ×W ×3 in pixel space, an encoder in HQ do-
main encodes it into a latent code (y ). Each token of the continuous
code
wEH
ill be mapped to quantized
discrE eH
te
ct
ode
zˆq
from the learnable code-
t
book
C
= {c
k ∈
Rd }N
k=0
via nearest-neighbor matching. The decoder
D
then
reconstructs the high-quality image frame from latent code. Similar to [4,17],
to train the quantized autoencoder, we adopt three image-level reconstruction
losses: pixel loss , perceptual loss [6,16] , and adversarial loss [11] .
1 per adv
L L L
Moreover, since image-level losses are underconstrained when updating the dis-
crete codebook, code-level losses are also used to reduce the distance between
the quantized code and input feature embeddings. The overall objectives in this
stage are defined by
= + + + sg( (y )) zˆq 2+ (y ) sg(zˆq) 2, (3)
LI L1 Lper Ladv || EH t − t||2 ||EH t − t ||2
where sg() denotes stop-gradient operator.
·
Kalman Filter Network(Stage II). In this stage, we train a LQ encoder
, the quantization Transformer T , and the Kalman filter network, while the
L q
E
codebook and decoder are frozen to preserve high-quality restoration from
C D
theVQGAN.Similarto[17],weadoptcross-entropyloss tosupervisetoken
CE
L
prediction,andfeatureloss tominimizethedistancebetweenfeaturesbefore
2
L
and after quantization. The overall objectives are defined by
= + ( (y ),sg(zˆq))). (4)
LII LCE L2 EH t t4 R. Feng et al.
Cross-Frame Attention (Stage III). To train both Cross-Frame Attention
(CFA) modules and Controllable Feature Transformation (CFT), we fix other
modules and use image-level reconstruction loss , and GAN loss ,
1 per adv
L L L
given by
=E[logD(Y)]+E[1 logD(Yˆ)]. (5)
D
L −
The discriminator D is constructed with multiple 3D convolution layers [3],
denoted as temporal PatchGAN, which could further enhance the coherence
of the generated face videos. The adversarial loss for the decoder modules is
formulated as
= E[logD(Yˆ)]. (6)
adv
L −
Additionally, we adopt temporal loss [7] between consecutive output frames,
formulated as
T
= M yˆ yˆ , (7)
Ltemp t −1 →t ·|| t− t −1 →t||1
t=2
X
where M denotes the valid mask computed by forward-backward consis-
t 1 t
− →
tencyassumption[8],andyˆ istheframewarpedfrompreviousframeyˆ
t 1 t t 1
with optical flow estimated b− y→ GT frames y and y . −
t 1 t
−
Hence, the overall training objectives are given by
=λ +λ +λ +λ . (8)
III 1 1 per per adv adv temp temp
L L L L L
Here λ , λ , λ , and λ are the balancing weights and we empirically set
1 per adv temp
λ =0.01, λ =1, λ =0.1, and λ =0.1.
1 per adv temp
A.4 Details of Dataset
Differentfromimage-baseddegradations,videocompressionimplicitlyconsiders
the dependencies across video frames, hence inducing temporal-variant degra-
dations. This is implemented by randomly selecting codecs and constant rate
factor (CRF) during training. The overall degradation model is defined by
x= [(y⊛k σ) +n δ]
codec
, (9)
{ ↓ }↑
wherexandy aredegradedandhigh-qualityvideoclips,respectively.k andn
δ
are Gaussian blur kernel and Gaussian noise specified by σ and δ, respectively.
⊛denotestheconvolutionoperation,and and represent4 downsampleand
↓ ↑ ×
upsample in this paper. Video compression codec is selected from “libx264” and
“h264” andthevideoqualityiscontrolledbyCRF,rangingfrom[25,45].During
training, σ is sampled from [2,10], and noise level δ from [0,10].
Foracomprehensiveevaluation,wesynthesizethreesplitsoftheVFHQ-Test
dataset containing different levels of degradation. As summarized in Table A1,
they follow the same degradation model but differ in the degree of noise, blur,
andcompression.Notethatwemainlyfocusonthevideocompressioncontrolled
by CRF, which is unique for video tasks.KEEP 5
Besides synthetic degradations, we also assess the generalizability of our
methodsonreal-worldfacevideos.Inparticular,wecollect40videosinthewild
from YouTube, covering various degradations and celebrities in different scenes,
e.g., interviews, and talk shows. Given the raw video from online sources, data
processing pipeline proposed by [14] is adopted to extract low-quality real face
videos. For each video clip, we retain a sequence of 100 to 300 frames with-
out scene transitions, which may break the dynamics between frames and hence
deteriorate the temporal propagation.
Table A1: We divide the test data into different levels of difficulty for a more com-
prehensive analysis.
Degradation mild medium heavy
Noise δ [0, 5] [5, 10] [5, 10]
Blur σ [2, 5] [5, 10] [5, 10]
CRF [18, 25] [25, 35] [35, 45]
B More Analysis
B.1 Effectiveness of Alignment
Alignment is the common pre-processing procedure in face-related vision tasks.
This ensures the faces are transformed and centralized in the same canonical
coordinate system. This is realized by detecting landmark keypoints and apply-
ing affine transformation to the original face images, which is sensitive to the
locations of detected facial landmarks. Mild inaccuracy of landmark detections
is tolerable in a single image. However, the noisy detections could consequently
result in unintentional temporal inconsistencies between frames in a video.
To reduce the additional inconsistency, we adopt low-pass Gaussian filter on
thelocationsofeachlandmarksalongthetemporaldimension,whicheliminates
abrupt change (jitters) oriented along time. Denoted k as the k-th detected
Mt
landmarksfromframey ,wheretrepresentsthetimestep.Thefilteredlandmarks
t
are given by
t+r
ˆk = G(n,σ) k, (10)
Mt ·Mn
n=t r
X−
where G(n,σ) = 2π1 σ2e−(n 2− σt 2)2 , and r denotes the radius of the window size. In
ourexperiments,weempiricallysetσ =5andr =20.FigureA3providesavisual
example of landmarks processed by Gaussian filter. The temporal jitters are
largelyalleviatedbythefilter.Wealsodemonstratetheeffectivenessofalignment
in the supplementary video.6 R. Feng et al.
345
Original
340 Gaussian Filter
335
330
325
320
315
310
0 50 100 150 200
Frame Index
Original
350
Gaussian Filter
340
330
320
310
0 50 100 150 200
Frame Index
Fig.A3: A representative example of landmark location processed by Gaussian filter
along time.
B.2 Quantitative Comparison on Various Degradation.
Table A2 provides the full quantitative results of models on different test par-
titions. As can be observed, our proposed method consistently outperforms all
other concurrent approaches on all datasets. In particular, KEEP surpasses Ba-
sicVSR++[2]byalargemarginof0.95dBinPSNR ontestdatasetwithheavy
degradation.Foridentitypreservationandposequalitymetrics(IDS andAKD),
our method achieves top performance and fewer fluctuations. On VFHQ-mild
dataset, KEEP possesses 8.82 average keypoint distances on images of shape
512 512, while the distances of all other methods are over 10.53. This suggests
×
that our method could better preserve identity within the generated video and
introduce far less jitters in the pose of faces. Such improvements are significant
in VFSR.
C Limitations and Future Work
Figure A4 presents a failure case of our method when the input video suffers
heavy degradation. The recovered logo on the hat in different frames shows in-
consistentshapes.Thiscouldstemfromtheinherentlimitationthatthecontents
innon-facialareasareunstructuredandhighlydeviatefromwhatthefacialprior
code encapsulates. A potential solution is to use general well-trained VSR mod-
etanidrooC-X
etanidrooC-YKEEP 7
Table A2: Quantitative comparison on VFHQ dataset with different levels
of degradation. Red and Blue indicate the best and the second best results.
Method PSNR SSIM LPIPS IDS AKD σ IDS( 10−2) σ AKD
↑ ↑ ↓ ↑ ↓ × ↓ ↓
Mild
GPEN [15] 25.5193 0.7517 0.2988 0.714211.4691 4.7416 3.5109
GFPGAN [10] 26.2933 0.7795 0.2482 0.743710.5467 4.5700 3.6482
RestoreFormer [12] 25.5720 0.7344 0.3195 0.753010.5354 4.7159 3.4122
CodeFormer [17] 24.6597 0.7454 0.2742 0.627211.4983 6.3726 3.6927
EDVR [9] 26.6051 0.7858 0.2484 0.719511.6220 4.8048 3.5829
BasicVSR [1] 26.0458 0.7765 0.2496 0.697311.3679 5.0343 3.6054
BasicVSR++ [2] 27.1996 0.8057 0.1958 0.764111.3136 5.2543 4.6425
KEEP (Ours) 27.9994 0.8267 0.1619 0.7960 8.8182 3.6866 3.2538
Medium
GPEN [15] 25.1871 0.7460 0.3063 0.674112.2091 5.3168 3.6872
GFPGAN [10] 26.2826 0.7839 0.2554 0.697011.1332 5.2539 3.7173
RestoreFormer [12] 25.5123 0.7256 0.3346 0.704411.2567 5.3760 3.6448
CodeFormer [17] 24.6238 0.7424 0.2852 0.607711.8149 6.7256 3.7899
EDVR [9] 26.3385 0.7815 0.2625 0.677112.4233 5.2598 3.6660
BasicVSR [1] 25.8332 0.7725 0.2594 0.663812.4503 5.7990 3.8101
BasicVSR++ [2] 26.5465 0.7918 0.2203 0.691913.4386 6.8957 5.6914
KEEP (Ours) 27.4853 0.8171 0.1740 0.7481 9.5937 4.6179 3.3764
Heavy
GPEN [15] 25.0191 0.7437 0.3108 0.654412.4814 5.6768 3.8088
GFPGAN [10] 26.0747 0.7807 0.2613 0.676111.6804 6.8689 3.9346
RestoreFormer [12] 25.3354 0.7216 0.3458 0.671511.7674 5.6277 3.6966
CodeFormer [17] 24.5600 0.7407 0.2916 0.594912.0462 5.9110 3.9079
EDVR [9] 26.1600 0.7792 0.2729 0.652413.0927 5.9243 3.8166
BasicVSR [1] 25.6895 0.7695 0.2686 0.642612.7841 6.1689 3.8356
BasicVSR++ [2] 26.2686 0.7872 0.2289 0.665014.2254 7.2980 6.1919
KEEP (Ours) 27.2165 0.8124 0.1803 0.7282 9.8833 4.8643 3.3217
els to further enhance these regions and backgrounds. We leave this avenue of
research as future work.
D Evaluation on Real-World Videos
Figure A5 shows that our method recovers texture details in each frame. In ad-
dition, the supplementary video delivers high-quality restoration of our method
with superior consistency from highly degraded face videos, demonstrating ex-
traordinary generalization in face videos in the wild.8 R. Feng et al.
Input
Ours
Fig.A4: Limitations. Our method might produce inconsistent results on non-facial
areas when the input video exhibits heavy degradation. For example, the logo on the
hat shows various shapes in different frames.
E Additional Visual Results
Figure A6, A7, A8, and A9 showcase additional visual examples of our methods
and other compared baselines.
References
1. Chan, K.C., Wang, X., Yu, K., Dong, C., Loy, C.C.: BasicVSR: The search for
essential components in video super-resolution and beyond. In: CVPR (2021)
2. Chan, K.C., Zhou, S., Xu, X., Loy, C.C.: Basicvsr++: Improving video super-
resolution with enhanced propagation and alignment. In: CVPR (2022)
3. Chang, Y.L., Liu, Z.Y., Lee, K.Y., Hsu, W.: Free-form video inpainting with 3d
gated convolution and temporal patchgan. In: ICCV (2019)
4. Esser,P.,Rombach,R.,Ommer,B.:Tamingtransformersforhigh-resolutionimage
synthesis. In: CVPR (2021)
5. Ge,S.,Hayes,T.,Yang,H.,Yin,X.,Pang,G.,Jacobs,D.,Huang,J.B.,Parikh,D.:
Longvideogenerationwithtime-agnosticvqganandtime-sensitivetransformer.In:
European Conference on Computer Vision. pp. 102–118. Springer (2022)
6. Johnson,J.,Alahi,A.,Fei-Fei,L.:Perceptuallossesforreal-timestyletransferand
super-resolution. In: ECCV (2016)
7. Lai,W.S.,Huang,J.B.,Wang,O.,Shechtman,E.,Yumer,E.,Yang,M.H.:Learning
blind video temporal consistency. In: ECCV (2018)
8. Sundaram, N., Brox, T., Keutzer, K.: Dense point trajectories by gpu-accelerated
large displacement optical flow. In: ECCV (2010)KEEP 9
9. Wang,X.,Chan,K.C.,Yu,K.,Dong,C.,ChangeLoy,C.:EDVR:Videorestoration
with enhanced deformable convolutional networks. In: CVPRW (2019)
10. Wang, X., Li, Y., Zhang, H., Shan, Y.: Towards real-world blind face restoration
with generative facial prior. In: CVPR (2021)
11. Wang,X.,Yu,K.,Wu,S.,Gu,J.,Liu,Y.,Dong,C.,Qiao,Y.,ChangeLoy,C.:ES-
RGAN: Enhanced super-resolution generative adversarial networks. In: ECCVW
(2018)
12. Wang, Z., Zhang, J., Chen, R., Wang, W., Luo, P.: Restoreformer: High-quality
blind face restoration from undegraded key-value pairs. In: CVPR (2022)
13. Wu, J.Z., Ge, Y., Wang, X., Lei, S.W., Gu, Y., Shi, Y., Hsu, W., Shan, Y., Qie,
X.,Shou,M.Z.:Tune-a-video:One-shottuningofimagediffusionmodelsfortext-
to-video generation. In: ICCV (2023)
14. Xie, L., Wang, X., Zhang, H., Dong, C., Shan, Y.: VFHQ: A high-quality dataset
and benchmark for video face super-resolution. In: CVPR (2022)
15. Yang, T., Ren, P., Xie, X., Zhang, L.: Gan prior embedded network for blind face
restoration in the wild. In: CVPR (2021)
16. Zhang, R., Isola, P., Efros, A.A., Shechtman, E., Wang, O.: The unreasonable
effectiveness of deep features as a perceptual metric. In: CVPR (2018)
17. Zhou,S.,Chan,K.C.,Li,C.,Loy,C.C.:Towardsrobustblindfacerestorationwith
codebook lookup transformer. In: NeurIPS (2022)10 R. Feng et al.
Input
Ours
Input
Ours
Input
Ours
Fig.A5: Qualitative comparison on the real face videos. Our KEEP recovers
high-fidelity face videos with faithful and consistent details.KEEP 11
Fig.A6: Qualitative comparison on the VFHQ. RFormer represents Restore-
Former [12].
tupnI
NAGPFG
remroFR
remroFedoC
RSVcisaB
++RSVcisaB
sruO
TG12 R. Feng et al.
Fig.A7: Qualitative comparison on the VFHQ. RFormer represents Restore-
Former [12].
tupnI
NAGPFG
remroFR
remroFedoC
RSVcisaB
++RSVcisaB
sruO
TGKEEP 13
Fig.A8: Qualitative comparison on the VFHQ. RFormer represents Restore-
Former [12].
tupnI
NAGPFG
remroFR
remroFedoC
RSVcisaB
++RSVcisaB
sruO
TG14 R. Feng et al.
Fig.A9: Qualitative comparison on the VFHQ. RFormer represents Restore-
Former [12].
tupnI
NAGPFG
remroFR
remroFedoC
RSVcisaB
++RSVcisaB
sruO
TG