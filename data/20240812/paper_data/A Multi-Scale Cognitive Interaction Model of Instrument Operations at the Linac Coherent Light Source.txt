A Multi-Scale Cognitive Interaction Model of
Instrument Operations at the Linac Coherent
Light Source
Jonathan Segal1, Wan-Lin Hu2, Paul Fuoss3, Frank E. Ritter4, and
Jeff Shrager5,*
1
SchoolofInformationScience,CornellUniversity. jis62@cornell.com
2EnergySciencesDirectorate,SLACNationalAcceleratorLaboratory.
wanlinhu@stanford.edu
3LinacCoherentLightSourceDirectorate,SLACNationalAcceleratorLaboratory.
fuoss@slac.stanford.edu
4CollegeofIST,PennState. frank.ritter@psu.edu
5ChiefScientist,BlueDotChange,Inc. andAdjunctProfessor,SymbolicSystemsProgram,
StanfordUniversity. jshrager@stanford.edu
*Correspondingauthor
July 2024
Abstract
We describe a novel multi-agent, multi-scale computational cognitive
interaction model of instrument operations at the Linac Coherent Light
Source(LCLS).Aleadingscientificuserfacility,LCLSistheworld’sfirst
hardx-rayfreeelectronlaser,operatedbytheSLACNationalAccelerator
LaboratoryfortheU.S.DepartmentofEnergy. Astheworld’sfirstx-ray
free electron laser, LCLS is in high demand and heavily oversubscribed.
Our overall project employs cognitive engineering methodologies to im-
proveexperimentalefficiencyandscientificproductivitybyrefiningexper-
imental interfaces and workflows, simplifying tasks, reducing errors, and
improving operator safety and stress levels. Our model simulates aspects
of human cognition at multiple cognitive and temporal scales, ranging
from seconds to hours, and among agents playing multiple roles, includ-
inginstrumentoperator,realtimedataanalyst,andexperimentmanager.
Themodelcanpredictimpactsstemmingfromproposedchangestooper-
ationalinterfaces andworkflows. Becausethemodel codeisopensource,
and supplemental videos go into detail on all aspects of the model and
results, this approach could be applied to other experimental apparatus
andprocesses. Exampleresultsdemonstratethemodel’spotentialinguid-
ing modifications to improve operational efficiency and scientific output.
We discuss the implications of our findings for cognitive engineering in
complex experimental settings and outline future directions for research.
1
4202
guA
8
]CH.sc[
1v43740.8042:viXra1 Introduction
LCLS, the Linac Coherent Light Source, located at the SLAC National Accel-
erator Laboratory, is a world-leading x-ray source, conducting hundredsof high
energy x-ray experiments annually. These experiments probe underlying nano-
scale mechanisms in such diverse fields as biology, chemistry, physics, materials
science, and geology.[1]. Beam time at LCLS is a unique and limited resource
that must be efficiently utilized to respond to scientific needs and to maximize
the impact of these ground-breaking experiments. The goal of the present ef-
fort is to apply cognitive engineering methods to optimize LCLS experimental
interfaces and workflows, thereby improving the efficiency of operations and
increasing the rate of production of scientific knowledge at LCLS.
Cognitive Engineering [9] is a methodology tailored to analyze and improve
settingswherehumansinteractwithengineeredsystemsanddevices,commonly
referred to as “human-in-the-loop” settings. The method combines qualita-
tive and quantitative experiments, mathematical analysis, and computer sim-
ulations, with the goal of improving the setting’s overall efficiency by making
common tasks simpler and less error prone, reducing overall errors, improving
human participants’ safety, and reducing their stress.
LCLS, and other scientific user facilities present a unique challenge for cog-
nitive engineering. scientific user facilities host an ever-changing panoply of
experiments, nearly all of which are unique, often actually defining the state-
of-the-art. Each year LCLS hosts approximately 300 teams from around the
world.
Here we briefly describe the LCLS setting and experiment workflow, and a
multi-scale, multi-agent computational model that can make rough predictions
about the impact of proposed changes to LCLS interfaces and workflows. The
details of the model and example results are provided in three supplemental
videos[2].
Modeling of this sort is important because, due to its complexity and cost
and the limited beam time, it is not feasible to run typical human-factors ex-
periments at LCLS, except in very simple cases. A unique feature of our model
is that it integrates simulations at multiple scales, and models aspects of indi-
vidualcognition,teamwork,planning,anddecision-makingbothin-the-moment
andacrossmultipleextendeddata-takingruns. Althoughthemodelonlyrepre-
sents these in rough detail, it is able to make sensible predictions regarding the
impactofawiderangeofproposedchanges. Becauseofthecostlinessandrarity
of beam time, even rough predictions can be useful in increasing the efficiency
of knowledge production at LCLS.
Below we focus on a description of the LCLS setting at SLAC. We then
describe the model at a high level, and explain its core concepts. Details of the
model, including demos of its operation and a deep dive into its code, are left
to thesupplemental videos [2]. The modelitself isopen sourceand available on
GitHub [3]. We conclude with thoughts on future directions and opportunities
for this research approach.
22 The Setting: Machines and Instruments
LCLS is a huge system involving an enormous number of physical and software
components,andmultipleteamsofscientistsandengineers. Thephysicalsystem
consists of a 2 kilometer-long linear accelerator (hereafter the “accelerator” or
“linac”), and a 200 meter long magnetic undulator that, taken together, form
anx-rayfreeelectronlaser(XFEL).Together,thelinacandundulatorareoften
referred to as “the machine”, and produce 120 very short (50 femtosecond)
and extremely bright x-ray pulses every second. This series of x-ray pulses is
referred to as the “x-ray beam” or just the “beam”. After propagating several
hundred meters through various diagnostics and beam shaping optics, these
x-ray pulses arrive at a number of “instruments” whose purpose is to collect
data about what happens when the x-ray beam interacts with materials of
interest, generally called “targets” or “samples”.1 The instruments are located
in “hutches”, which range in size from about a one-car to a four-car garage.2
Due to the large distances, legacy control systems and differing staffing re-
quirements, the Machine and Instruments are operated from separate control
rooms. The control room operating The Machine (the linac+XFEL) is called
theAcceleratorControlRoom(ACR).EachinstrumenthasitsownInstrument
Control Room (ICR, Figure 1), located near its hutch. In the present paper
we are mostly concerned with activities in the local control room of a specific
instrument, called CXI, which is used for x-ray crystallography.[4]
Figure2abstractlydepictsthelayoutofsubsystemsinvolvedinSLAC/LCLS
operations, and their rough interaction. The next section provides more detail
about the systems depicted in Figure 2 (see also supplemental videos [2], [11]).
3 Experiments and Measurements
An“experiment”heremeansthecontiguousperiod—usuallyaround3-5days—during
which users are actively using an LCLS instrument to collect data. An exper-
iment has many sub-steps. The sub-steps that we will be primarily concerned
with are the contiguous periods of time, called “measurements”, during which
the x-ray beam is striking a given target—or at least intended to be doing
so—with the intention of collecting data about that target under given condi-
1Although we use these words essentially synonymously, as do the LCLS users and staff,
the sample, or samples, are the set of materials or items intended to be targeted in a given
experiment,butatagivenmomentthereisonlyonetarget,althoughitisusuallypartofthe
sample. Butmaintainingthisdistinctionisunnecessarilycomplexforthepresentusages,and,
indeed,formostusages.
2Traweek’sclassicethnographyofSLAC[13]providesanexcellent, ifsomewhatoutdated,
depiction of the linac and of the scientists and engineers that built and operate it (esp. pp.
37-39). Thesamebookalsodescribestheequivalentofwhatweareherecalling“instruments”,
whichare,inthatbook,called“detectors”(pp. 48-51and71-72. Thereisatechnicaldifference
betweenwhat we arecallingan “instrument” andwhat, atthe time ofTraweek’sworkwere
called “detectors”, but this distinction is more subtle than is worth going into here. The
XFELandLCLSdidnotexistatthetimeofTraweek’sstudy.
3Figure 1: A typical view of an Instrument Control Room. (Photo by the au-
thors.)
tions.3
Measurements can range in length from just a few seconds (if something is
being briefly tested, or if something goes wrong), to 15 minutes, or sometimes
much longer, as multiple samples are cycled into the path of the x-ray beam.
The specific length of time that measurements take will be of central concern
in our model as time is the most limiting resource at LCLS.
There are usually many dozens — perhaps hundreds — of measurements
madeovertheseveral-dayperiodofanexperimentthatgeneratetheinformation
sought by the scientists running the experiment. There are, of course, many
otherpartsofastart-to-finishexperiment,butweareonlyconcernedatpresent
with the few days that the team is actually at SLAC, and only when they are
actively using the instrument to make measurements. Measurements generally
make up the bulk of the time of an experiment.
Importantly,mostexperimentsutilizeavarietyoftargets,including“dummy”
3Becausethefocusedx-raybeamtypicallymodifiesthesamplealmostimmediately,agiven
measurement typically utilizes a series of putatively identical targets, for example on a tape
orembeddedinastreamofliquid. Wedonotneedtodistinguishthesecases,andsojustuse
the term “target” or “sample” for either of these cases, unless it is necessary to distinguish
them.
4Figure2: TheMachine(linac+XFEL)deliversx-raystoanInstrument,where
the x-rays strike a target producing data which is processed by an analytic
pipeline, as described in the text and supplemental videos. The system is oper-
ated through a close and continuous collaboration between scientists and engi-
neers in the ACR and ICR who play various roles (as described in the text).
targets used to align and calibrate the instrument, “control” targets that have
known properties, and “critical” targets for which the experiment is designed
to gather new data. Because the critical targets are often costly and of lim-
ited availability, the experiment must be planned to maximize the high quality
data—that is, data with low signal-to-noise ratio (SNR)—collected on these
critical targets. Therefore, although an enormous amount of careful planning
goesintoexperimentdesignwellbeforetheexperimentalteamarrivesatSLAC,
unplanned issues always arise, so the team engages in continuous, on-site, real-
time decision making to maximize the collection of high quality data on the
critical targets.
Theprimarydecisionstobemadearehowlongagivenmeasurementshould
continue, and in what order measurements should be taken from different tar-
gets. Because beam time is limited, it seems sensible to start gathering data on
critical targets as soon as possible. However, because unexpected issues often
arise during the experiment’s actual beam time, it may be more sensible to use
dummy or control targets until it is clear that the instrument is working well
(i.e., producing low SNR data), especially if the availability of critical targets
is limited. On the other hand, as the experiment nears the end of its allocated
beam time, the team may wish to get as much data as possible on critical tar-
gets, even if the instrument is not producing the expected level of SNR, and
especiallyifthingsdidnotgoasplanned—forexampleduetounexpectedlinac
down time — one may want to get as much data as possible on critical targets
5in the remaining time, regardless of the quality of the data being non-optimal.
Thissortoftrade-offisveryimportant,and,aswillbeseeninmoredetaillater,
defines the top layer of our model.
4 Experimental Teams
Thescientificproductivityofthismulti-billiondollarsystemdependsonahighly
complex,highperformance,human-machinechoreographyperformedbycombi-
nationsofinternalandexternalscientistsandengineerswho,takentogether,we
call “experimental teams”4. Teams often have a wide diversity of backgrounds
(biologists, chemists, physicists, engineers, etc.) and some diversity of expe-
rience (from undergrads through professional scientists), and usually include
both visiting scientists (i.e., non-SLAC/LCLS staff), and on-site SLAC/LCLS
staff,includingthe“instrumentscientists”whodevelopandusuallyoperatethe
instruments, and the staff operators of the linac and XFEL who work in the
ACR.5 Unless otherwise specified, we use “team” to mean the part of the team
engagedindirectlyusingandoperatingtheCXIinstrumentduringaparticular
experiment, usually operating out of the instrument control room (or remotely
interacting with the team members in the CXI ICR).
LCLS hosts hundreds of experiments each year, often several different ex-
perimentseachweek. Althoughthereisextensiveplanningforeachexperiment,
theteaminvolvedinagivenexperimentreallyonlycomesphysicallytogetherat
LCLSforthefewdaysduringwhichmeasurementsaretaken. Althoughsubsets
of a given team may have long history with one another and with LCLS, the
whole team has usually not previously worked together in the LCLS setting.
Moreover, the team members almost always have different amounts of experi-
encewithLCLS—rangingfromstudentswhoarecomingtoSLACforthefirst
time, to full time LCLS instrument scientists who maintain the instruments
and are usually the ones who carry out direct control manipulations.6 There
areoftenmanyscientistsandengineersintheICRatagivenmoment. Wehave
observed experiments with as few as two, and as many as ten or more people in
anICR.Therearealso,commonly,remoteparticipants,interactingbyphoneor
via online communications such as Zoom, or through “virtual machine” screen
sharing connections.
4TraweekalsodiscussesteamworkintheSLACcontext,likeningscientificteams(basedin
theUnitedStates)tosportsteams(p. 149).[13]
5TheLCLSinstrumentscientistsandACRoperatingteamdonotchangewitheachexper-
iment,althoughexternalscientistsalwaysworkwiththeinstrumentscientistintheICRs,and
oftenwillworkaswellwiththemachinestaffintheACRonparticularlycomplexexperiments.
6Thereareoftenfullyinternalexperimentswhereeverymemberoftheteamisafulltime
LCLS scientist. We are not calling these out as different from typical “user” experiments,
although they do differ in that the teams involved in such internal experiments are more
familiarwiththesettingandwithworkingtogether.
64.1 Experiment Workflow and Virtual Roles
The goal of most experiments is to get data about a particular target’s interac-
tion with the x-ray beam, and the team is generally allocated a few days (say
24-96 hrs) of live beam time to accomplish this. As one can well imagine, there
is a great deal going on during that time. For the moment, as our model is
relatively rough, a general picture of what transpires in a CXI experiment will
suffice (and see supplemental videos [2]).
AlthougheveryLCLSexperimentisdependentuponthex-raybeamcreated
in The Machine, which is operated from the ACR, there comes a point where
the beam is “handed off” from The Machine to The Instrument, in the specific
sensethatthephotonsaremanipulatedbycomponentsofthesystemcontrolled
from the ICR, and no longer from the ACR. Regardless, the success of a given
measurement (and so, of the whole experiment) is dependent upon creation of
x-ray pulses with the desired characteristics, and then using that beam in the
instrument. Therefore, a constant and complex interaction takes place between
the instrument operators in the ICR, and the machine operators in the ACR
in order to get the x-ray beam delivered as desired in terms of energy, pulse
duration, and the many other parameters that describe the x-ray beam, and
which are in constant flux. As the ACR and the ICR are about a 15 minute
walk, or a 3 minute drive, from one another, this interaction takes place mostly
by phone or through screen sharing. In a very real sense, the users at the
instrument end of the system are at the mercy of the skill of the upstream
machine operators’ ability to keep the beam coming with the correct set of
parameters, so that the scientists at the instrument end can get the data they
need in the allotted period. Wasted time, for whatever reason, is data lost, and
giventhecostanddifficultyofgettingbeamtime,losingdatacanmeanmillions
of dollars, and maybe the difference between a student graduating on time or
not at all.7
4.2 Operator, Analyst, and Experiment Manager
Giventhepicturepaintedabove,itshouldbeclearthatthesuccessofanexperi-
mentridesontheteam’sabilitytorapidlydevelopaneffective“teamworkflow”
inordertoconductasmany“good”measurementsaspossible—i.e.,oneswith
high signal-to-noise ratios (SNR) on important targets. We identify three “vir-
tual roles” (depicted in Figure 2) which are always represented, regardless of
the number of separate people involved in an experiment: The Instrument
Operator, the Data Analyst, and the Experiment Manager.
The Instrument Operator actively controls the instrument. They are
usuallyanindividual,andtheyarealmostalwaysseatedatthesetofscreensthat
displaythestatusof,andpermitcontrolofthehundredsofinstrumentsettings,
suchasmotorpositions,valveandshuttersettings,andsoon(Figure3). When
there are multiple operators, there is almost always a primary operator. The
7Also,ofcourse,theyareatthemercyofunanticipatedproblems. Muchoftheskillofthe
operatorsatbothendsofthesystemisinhowtodealwithsuchproblems.
7othersmaybeassistants,observers,apprentices,orstudents. Theoperatorrelies
on several sorts of information in making control decisions: “Direct Control
Data” is information that arises from “Primary Data Processing”, as well as
information coming from the other team members and from the ACR. These
are explained in more detail below.
Figure3: AtypicalcontrolwindowinanLCLSinstrument. Mostofthedepicted
controls are motor controls like those in our model. In addition to using the
left and right buttons, the motor position can be changed by clicking in the
text box containing the number, and typing in a new value. Also, for each
such control, there is almost always a command-line equivalent. Which control
(user interfaces (UIs) like this one, or command lines) is used for a particular
purpose in a particular setting depends on many factors in the setting and the
preferences of the particular operator. (Photo by the authors.)
The Data Analyst analyzes the data flowing from the instrument into the
data pipeline — usually using some combination of SLAC-developed software
8(e.g.,theAMI,Figure4,andexplainedbelow)andtheirownexperiment-specific
software. We call this “Secondary Data Analysis”, and its main goal is to
estimate the data quality, that is the signal-to-noise ratio (SNR) coming from
the instrument. The data analyst’s job is mostly to calculate this estimate and
pass it to the operator and experiment manager.
Figure 4: Examples of many common types of information displays used at
LCLS including the AMI and PSANA analysis toolkit.[5]
The Experiment Manager uses the information from the data analyst
and works with the operator to make decisions about what to do next. The
most important decisions (under normal circumstances) about the experiment
progress and plan are whether to continue or abort the current measurement,
and what sample to measure next. Although each experiment starts out with a
specific experiment plan, dynamic (“opportunistic”) re-planning almost always
takes place based upon many factors. This will constitute a significant aspect
of our model.
Although these three roles are often served by separate people, this is not
necessarily so. The roles are often shared in complex ways across fewer or
more than three people. For example, the instrument operator may serve as
the data analyst, and for in-house experiments the operator might also be the
experiment manager. Moreover, with the exception of the instrument operator,
the people filling the other two roles may sometimes not even be physically
present; They could be involved by phone or screen sharing. Keeping this
flexibility in mind, hereinafter we will assume that each role is occupied by a
single different individual.
94.3 Typical Experiment Workflow
Figure 5 shows the schedule for LCLS in April 2022. The teams using the
beam are color coded by hutch (red is CXI). Experiments with an “X” (as
“LX...”) are internal, that is, it is sponsored by SLAC or LCLS, and the team
is (usually) composed entirely from SLAC/LCLS staff. Grey blocks represent
scheduled downtime, e.g., for maintenance. Note that there are three day shifts
and three night shifts.8 In the present work we are generally focused just on a
single operating shift, but it is worth having a sense of the overall structure of
experiments.
Figure 5: Example month’s schedule, for April 2022, approximately 27 experi-
ments. [6]
Oncetheinstrumentispreparedandatargetisinplacetheinstrumentand
machine operators begin working together to align the x-ray beam to reach the
sample in the desired manner, usually as a stable, focused beam, that data is
being taken correctly by the instrument, and that the various downstream data
pipelines are working as desired. All of this involves hundreds of physical and
computational adjustments. Once alignment is completed the experiment itself
can begin, and what follows is the series of measurements that comprise the
bulk of the experiment.
5 Cognitive Aspects of LCLS Experiment
Operations
LCLS operations are an ideal setting to understand collaborative, real-time,
problem-solving. Thereisanextensiveliteratureinteam-basedproblem-solving
that we will not attempt to review here aside from pointing to some recent
reviews, e.g., [8, 10]. Such problem solving is characterized by complex oppor-
tunisticinterplayofnumerousinter-relatedgoalsandthephysicalandcognitive
activities that the team deploys to reach these goals.
In acting toward reaching their goals, the team takes advantage of what
we will generically call “resources”. The most important category of resources
are actions and their affordances (together, “afforded actions”). A slightly too
simplistic way to think of what an afforded action is, is as an action that can
be accomplished through manipulation of some sort of artifact. The simplest
8At the time of this schedule, there were three day and three night shifts. More recently
theschedulehasbeenreallocatedsothatthereareonlytwodayandtwonightshifts.
10example of this is moving a mirror by virtue of stepping a motor, which is af-
forded by the motor controls (Figure 3).9 But afforded actions are often much
less obvious. For example, particular sorts of displays are artifacts that af-
ford the action of obtaining certain types of information.10 People can also be
thought of, in some cases, providing afforded actions. For example, in the local
instrument-side team, the instrument operator’s expertise affords to the exper-
iment manager actions such as getting data and terminating the measurement,
and the expertise of the linac operators in the ACR affords various actions to
the instrument-side team. Another set of non-obvious actions is afforded by
information recorded in the setting through artifacts such as physical and elec-
tronic records, such as the eLog, notebook, post-its, and so on utilized around
theICR.Thesesortsofartifactsaffordbothinformationgatheringandinforma-
tion storage actions.11 The final set of afforded actions that we shall point out
are those relating to the problem-solving itself, for example, setting a new goal
or declaring a goal to have been reached, rejected, pended, or abandoned. In
these cases the “artifact” is a bit more subtle, and might be called the “plan”,
although the term “plan” gives this too much of a static feel, whereas in reality
the plan is constantly being modified through actions.
Finally, we must point out two important non-physical resources that play
a critical role in this setting, as they do in almost all problem-solving settings:
timeandattention. Theoperatingteammustattendto,andrespondto,dozens
at least, and in some cases hundreds of information resources in order for a
measurementtosucceed. Thereareoftenbetween20and30computerwindows
open across a dozen monitors, and the operator interacts with these partly
“demandantly” (that is, responding to status updates and alarms that require
a response [12]) and partly intentionally (that is, with the intention of making
a change that is not in direct response to, e.g., an alarm). There are many
competing goals at play in the positioning of data and control windows, and
this often changes during the course of the experiment, as different situations
demandattentionto, andcontrolinputsto, differentaspectsoftheexperiment.
The resource of time will play a large role in the present project. However,
althoughattentionisextremelyimportantinthefine-grainedLCLSoperations,
we are not going to pay much attention to it in the present paper.
9There is an important aspect of expertise at play here, which we will not go into much
in the present paper. Instead, we assume that the participants in all roles are highly expert
intheirtasks. Thisisnotthecaseforwhatmightbecalled“situationspecific”expertise—
thatis,expertiseinhowthesystemhappenstoberesponding,say,today. CollinsandEvans
have pointed out that expertise is “fractal” [7], but, again, we will not go more deeply into
thatinterestingissueinthepresentpaper.
10Weusetheterm“informationresources”throughouttoindicatetheresources(otherthan
othermembersoftheoperatingteam)thatacognitiveagentgathersinformationfrom. The
mostcommonsuchresourcesarevisualdisplays,suchasgraphsorgraphicalrepresentationof
instruments on computer screens, but there is a wide variety of components from which one
gathers information, for example, cameras looking at the instrument itself, auditory signals,
paperdocuments,electroniclogs,andeventactilefeedback.
11Post-Its are quite commonly stuck to various parts of the instrument and controls to
facilitatethestorageandrecoveryofimportantinformation.
116 LCLS Experimental Operations from the
Cognitive Point of View
Next,wedescribeasimplifiedtimelineofathree-dayexperiment,anddrilldown
into the measurements, and especially the cognition surrounding measurement-
related decision-making. On the first day of the time allocated to a particular
experiment, the team gets organized, installing any required special equipment
and testing the setup.12 The machine operators (in the ACR) get set up to
deliver x-rays with the required characteristics. Once that is working, the in-
strumentoperators(intheICR)willtesttheinstrumentwithunimportanttest
samples. The team will get their software ready and ensure that it is getting
data,andtheanalystswillconductanalysesonthetestsamples. Letusassume
that all of this takes place on the first day, and that all “real” measurements
take place on the second day. This will be described in greater detail below.
Throughout these measurements, the collected data is being stored and ana-
lyzed at various granularities, also described in more detail below. Finally, on
thethirdandlastdayofoursimplifiedtimelineanyspecializedequipmentadded
totheinstrumentwillberemovedandtheremotepartoftheteamdepartsand
the local staff of instrument scientists begin setting up for the next experiment.
And so it goes.
Inoursimplifiedtimelinewehaveassumedthatallthe“real”measurements
— that is, those that the experimental team will use as actual research data —
are carried out on the second day of three in this example. As described briefly
above, the “real” samples (as opposed to dummy or control samples) — those
from which “real” data is desired — range in importance with respect to the
scientific questions at hand. For simplicity let us assume that there are, in this
example, 8 “critical” samples, and that 15 minutes of data are intended to be
collected on each sample under each of 3 conditions. These details will vary, of
course, from experiment to experiment, but a common case is “pumping” the
samplewithapulsefromalowerenergylaserjustbeforethesampleishitbythe
x-ray beam. This is called the “pump-probe” protocol. The pump — the lower
photon energy (e.g., visible) laser pulse — energizes the molecule, changing
its confirmation, and then the “probe” from the x-ray laser strikes from the
energizedmolecule,destroyingit(becausethex-rayenergyissohigh)butatthe
sametimediffractingandthusprovidingmeasurementsthatcanbeinterpreted
later as what amounts to a picture of its energized configuration.13 In this
sort of experiment, the three desired conditions might be different wavelengths
or intensities of the pump pulse, different timings between the pump and the
probe, etc.
This whole second day — 24 hours, comprising three 8-hour shifts — might
seem like plenty of time to obtain 8x3x15 minutes worth of data, which only
12Sometimesinstallationbeginsbeforetheformallyallocatedtime,ifpartsoftheinstallation
canbedonesafelywhilethejustpreviousexperimentisstillongoinginadifferenthutch.
13Indeed,thisprocesstakesplacesorapidlyatLCLSthatbyvaryingthetime-relationship
betweenthepumpandprobe,onecanactuallymakeanimationsoftheconformationalchanges
thatmoleculesundergoduringtheabsorptionprocess.
12comestoatotalof360minutes,or6hours. However,problemsinevitablyarise.
Beam time at LCLS is a very scarce resource, and, as mentioned above, failure
to obtain enough good data on the most important samples not only wastes
time and money, but could be the difference between a student graduating on
time,oreventhedevelopmentofanimportantdrug. AtLCLS,problemsmight
arise in any of a number of areas: the Machine itself, which is very complex
and dependent on precisely synchronized processes, might be being unstable
leading to instability in the x-ray beam delivery; the instrument may be having
similartroubles(newpartsofitperhapshavingjustbeeninstalledafteracross-
country trip for this experiment)14; the sample may have any of a number of
problems,forexample,deliveryofliquidswithvariouspropertiestendstoresult
in unpredictable surface tension and turbulence effects. One such problem is
central to our model. If enough problems arise, one might run out of time, or
even run out of critical samples.
6.1 The Central Decision: What to Measure When and
for How Long
Inlightofthesecomplexities,thedecisionofwhattomeasurewhen,andforhow
longisthemostcriticaldecisiontobemadebytheexperimentmanager. Many
factors go into this decision — which is, of course, made in consultation with
the rest of the team, and often includes the machine operators in the ACR. To
give a sense of the complexities involved, consider that once the team runs out
oftheirallottedbeamtime(24hoursinthiscase)theymayhavetowaitmonths
to get additional access if they failed to gather all the desired data. Because of
the amount of data involved, and the limited time available to gather it, final
(tertiary) data analysis cannot be accomplished in real time. Therefore, the
team can only roughly tell, through secondary data analysis, whether the data
they are getting is satisfactory. Indeed, this is the specific job of secondary
data analysis. For these reasons one would think that it would make the most
sense to carry out measurements on the more important sample earlier in the
experiment because if problems are leading to lower SNR one can simply take
more data.
However, recall that there may be limited sample material, especially of the
most important samples, and the problems may resolve, improving the SNR.
These considerations vie in the opposite direction, suggesting that if things are
not working well (i.e., for whatever reason, the team is seeing low SNR in sec-
ondary data analysis), but it is still early in the overall experiment period, it
may be better to hold on to the precious critical sample and hope that the
problems resolve. But how long do you wait? Half way? Two thirds? Three
quarters? And if the data is marginal, and getting worse, should you continue
a measurement to wring as much out of the current sample, or simply abort it
and move on to more important samples, given an impending hard stop? And
14This tends to be less of an issue with CXI, x-ray crystallography being a fairly well-
worked-out science, although not so much at the energies at which LCLS operates and the
pump-probeisquitetimingsensitiveandcanbedifficulttogettoworkwell.
13once the ongoing measurement is completed, which of the 8x3x15 minute mea-
surements should be tried next? Perhaps even a previous less-the-satisfactory
measurement should be repeated?
Obviously, many considerations go into these decisions, and making these
decisions as correctly as possible is critical to the success of the experiment.
Becauseofthisimportance,wechoosetofocusonthissetofdecisionsincreating
our cognitive model of LCLS experiment operations.
7 A Multi-Scale Cognitive Interaction Model of
LCLS Instrument Operations
OurgoalinthepresentworkistopredictthepotentialimpactsofUIandwork-
flow changes in the most impactful aspects of LCLS operations. Because of the
difficultyofcarryingoutdirectuserstudies,orofemployinga“flightsimulator”
approach, intheLCLSsetting, wechosetocreateacomputersimulation(here-
after,“themodel”)ofsomeofthemostimportanthuman-in-the-loopaspectsof
experiment operations, and in particular of the phenomena relevant to the core
decision making, as discussed above, of when and whether to continue or abort
a measurement, and what to measure next. The goal is to use this model to
predict the direction and rough shape of changes in experiment efficiency that
might result from changes in instrument procedures and/or control interfaces.
In what follows we discuss the central concepts represented in the model.
Much more detail is included in the three video supplements to this paper[2].
Thefirstvideoisaconceptreview,andprovidesmore-or-lessthesameinforma-
tionasbelow. Thesecondincludesseveraldemonstrationsofthemodelrunning,
and discusses the results depicted in figures 7 through 9. The third video is a
deep dive into the model code, which is available on github.[3].
147.1 Multi-Agent, Multi-Scale
Weexplicitlymodelthethreeprimaryagents,asdescribedabove: TheOperator
is directly in control of the beamline of the instrument. The Data Analyst
observestheAMIanddetermineswhenthedataisgoodenoughtostoptherun
as well as gives updates to the Experiment Manager when there is a problem.
TheExperimentManagerisresponsiblefortheoverallcontroloftheexperiment
andcommunicateswithboththeOperatorandtheDataAnalysttounderstand
the state of the Experiment as well as make changes at a high level to improve
the quality and efficiency of the experiment. These roles implicitly create three
temporal scales, as depicted in Figure 6.
Figure 6: A diagram of the interaction between the different cognitive scales
during the running of the LCLS. This can be understood as Micro is what an
individual in the system does and then zoom out for Meso and again for Macro
where the logic of the entire experimental task exists. This illustrates how the
connection between all of the roles involved influence each other as well as the
overall experiment.
Briefly, the three scales of cognitive analysis, depicted in Figure 6, are:
Phenomena at the Micro-Cognitive scale are related to detailed con-
trols and displays. Here we see both reasoning and decision-making, as
well as skilled performance that we collectively call “peak chasing”. The
primary information inputs to peak chasing involve a cognitive parame-
ter called “Functional Acuity” (FA), and parameters relate to the ability
to respond to changes efficiently, which we collectively call “Functional
Operability” (FO).
15PhenomenaattheMeso-Cognitivescaleinvolvereasoninganddecision-
makingrelatedtothemeasurementbeingmadenow—specifically,whether
to continue or abort it.
Finally, phenomena at the Macro-Cognitive scale involve reasoning
and decision-making related to the course of the experiment, especially
regarding how long to gather data on a given sample, and in what order
samples should be run.
Although we discuss each of these briefly below, they are described in more
detail, and in context, in the supplemental videos[2].
7.2 The Micro-Cognitive Scale: Peak Chasing, Functional
Acuity, and Functional Operability
Realexperimentaldecisionmakingtakesplaceinaverycomplexsetting,where
many extrinsic factors are involved, such as which samples are available, what
the value is of those samples, the stability and performance of all aspects of
the system, how far through the experiment they are, and how much time the
remaininghigh-valuesamplesarelikelytotake,aswellaslarge-scalefactorssuch
as SLAC and LCLS shift-changes and the requirements for relevant members
of the team to take breaks to eat and sleep. Our model, of course, represents a
highly simplified version of these, but it does not attempt to take into account
all aspects of these factors.
In the micro-cognitive domain we focus on a general sort of task that we
call “peak chasing”. Peak chasing is a common type of optimization task akin
to driving down the center of a lane, or, in a more difficult case, balancing
a broom or long pole on one’s hand. Peak chasing in this domain involves
actuatinginstrumentcontrolstotrytooptimizesomeobservablemeasure(more
generally,characteristic)oftherunninginstrumentsetting. Therearehundreds
—perhapsthousands—ofparametersthatcontrolthelinac→xfel→instrument
system, and every one of them has some effect, to greater or lesser degree, on
the quality of the data collected. The “peak” being “chased” is in some cases
literally a peak in histogram or similar graph, and in some cases a single value
onadisplay. Inothercasesthe“peak”ismoresubtle, perhapsbeinga“correct
looking” display. And in still other cases the peak is a statistic being reported
by secondary data processing via the AIM. In our model what is being chased
isliterallythealignmentoftwovalues, whichonecanthinkofasrepresentinga
streamofliquidandthebeamofx-rays. Theliquidundergoesaone-dimensional
random walk, and the instrument operator has to move the beam left or right
to “chase” it. The quality of the data depends upon how close the beam is to
the stream.
Theoperator’sabilitytocarryoutthistaskiscontrolledbytwoparameters:
Functional Acuity (FA) and Functional Operability (FO). FA is the operator’s
(or, more generally, the team’s) ability to recognize the need to take an ac-
tion. FO is their ability to take the appropriate action — more precisely, how
16long it takes to take the appropriate action (and could include terms related
to the probability of taking an incorrect action, although we do not model this
at present). The term “functional” in these concepts emphasizes that these are
not purely cognitive concepts, but are functions of the interaction between in-
dividual cognition, team communications, and technical displays and controls.
Functional Acuity and Functional Operability have a temporal as well as in-
formational component. For example, in addition to a parameter that controls
how far the beam and stream have to be before the operator can notice that an
actionmustbetaken(literallytheFunctionalAcuity),thereisaNoticingDelay
(ND), and then a response delay, and also if the operator has to change which
button is being pressed, there is a cost in terms of time based on the distance
from the current button to the new one, which is incurred only if one has to
switch directions. Again, these are more clearly understandable seeing them in
action in the supplemental videos[2].
Both Functional Acuity and Functional Operability can be thought of as
“UI-related” parameters. However, differences in Functional Acuity arise from
the interaction between cognition and the display characteristics of the instru-
mentUI,whereasdifferencesinFunctionalOperabilityarisefromtheinteraction
betweencognitionandthecontrolcharacteristicsoftheinstrumentUI.Whereas
changes to the displays or controls would affect only one or the other of these,
more general cognitive interventions, such as alertness or amount of experience
with the system will affect both of these parameters.
7.3 The Meso-Cognitive Scale: When to End a
Measurement
One of the most important decisions that the operating team must make dur-
ing an experiment is when to stop taking data on a particular sample, that
is, when to end a measurement. In reality, many considerations go into this
decision, including the stability and controllability of the x-ray beam, how the
data coming from the instruments looks per secondary data analysis combined
with the quality of the data that has been obtained so far, additional planning
considerations, suchastheamountoftimeremainingintherunandperhapsin
the shift, the mental and physical state of the team, how much sample material
remains, and considerations relating to the overall experiment (including ter-
tiary data analysis) such as the importance of the sample, and probably other
considerations. In our model this decision is made based up the current rate of
improvement in SNR, and the beam time remaining.
7.4 The Macro-Cognitive Scale: Sample Choice Planning
and Replanning
The dynamic environment of an LCLS experiment requires a blend of meticu-
lous pre-planning and continuous real-time, opportunistic decision-making and
re-planning. This is markedly different from static, puzzle-like problem-solving
where one has the luxury of time and a limited set of variables. The actual
17measurement process is akin to balancing a pencil on its tip, and is made more
complicated by the nature of the novel physics being studied at LCLS, and,
of course, by a constant barrage of tiny problems, requiring real-time decision-
making and replanning. While some adjustments, like shortening a measure-
mentifoneisobtaininghigh-qualitydata,arerelativelystraightforward,others,
suchasextendingameasurement,orevenbringinginadditionalmaterialsfrom
off-siteifnecessary,canbemorechallenginggiventhestrictlylimitedtimeframe
of LCLS experiments, and are potentially impossible. In our model, the only
information used to make these decisions is the information available regard-
ing how well the measurements have been going so far, how much beamtime
remains, and the number and quality of samples remaining to test.
7.5 Measurement Error and Performance Quality of
Samples
Central to all of the above (and, of course, in the real world LCLS setting) is
thequalityofthedatabeinggathered. DataqualityinsettingssuchasLCLSis
often measured in terms of signal-to-noise ratio (SNR), as variously mentioned
above. SNR is the slope of a scatter plot of the data, divided by its standard
error,andstandarderrorissimplythestandarddeviationofthedatadividedby
thesquarerootofthenumberofdatapoints. Therefore,inourmodelwesimply
usestandarderrorasastand-inforSNR,andthegoalofagivenmeasurementis
to reach a particular target standard error, which we call “Target Error” (TE),
and which is 0.001 by default, for its given sample.
Samplesinourmodelonlydifferfromoneanotherintermsofasingleparam-
eter,called“PerformanceQuality”(PQ).Asample’sPQisintendedtorepresent
knowledge about how easy it is to get data from this sample, based upon ei-
therpriorexperiments, forexampleinasynchrotron, ortheoreticalcalculation.
HigherPQsampleswillproducehigherqualitydata,thatis,theywillreachTE
with fewer samples than samples with lower PQ. Somewhat counter-intuitively,
more important samples have lower PQ. The reason that lower PQ samples are
considered more important is a bit subtle: If it was easy to get data from a
given sample — that is, if they have high PQ — then one would have done so
already, or one would be sure of their theoretical prediction, and thus it would
be less important to get high quality data from samples that have high PQ.
An additional complexity arises from the fact that usually the quantity of low
PQ(moreimportant)samplesonhandisalsooftenlimited—again, ifsamples
wereunlimitedonewouldhavebeenabletogetadequatedataontheseinaless
time-constrained setting, like a ring. One does not want to be wasting rare and
importantlowPQsamples,especiallyearlyintheexperiment,whentheSNRis
low for extrinsic reasons, such as experiment stability and operator experience.
Similarly, one does not want to be wasting the most important samples (low
PQ) stabilizing the instrument. Therefore, one generally will want to run high
PQ (less important) samples during stabilization, until one has high confidence
inthedatacomingfromtheinstrument. Putanotherway,highPQ(lessimpor-
tant) samples are those for which you essentially know what data you are likely
18to get, whereas low PQ (more important) samples are ones that are harder to
get data from in either (or both) of theoretical modeling or ring studies. The
wholereasonforcomingtoLCLSandrunningontheXFELisbecauseit’shard
to get data on the low PQ (more important) samples, otherwise you wouldn’t
be at LCLS to begin with.
7.6 Error Adjustment and Cutoff Time
Ifonelivedinaperfectworldwithunlimitedbeamtimeandunlimitedsamples,
any desired specific Target Error (TE) could always be reached. However, in
the real world of LCLS, both time and samples are limited commodities. As
intimatedjustabove,theteamwantstogenerallyrunlowimportance,highPQ,
samples early on, during instrument stabilization, and more important, high
PQ, samples later, but not so late that they will run out of beam time. In a
morerealisticmodeltheexperimentmanagermightdynamicallymakedecisions
aboutwhichsampletomeasurenext,however,ourpresentmodelisconstrained
to run samples in High-PQ-to-Low-PQ order. Given this constraint, the only
decisionsthattheexperimentmanagercanmakearewhatTEtoshootfor,and
when to cut off a given measurement. The model can be set to be constrained
regarding the total beam time (called “Cutoff-Time”, where a values of “True”
(yes)meansthattheteamwillbekickedoffthemachineafteranallottedtime),
and, similarly, may or may not be allowed to adjust the TE (“Adjust-Error”,
where,again,a“True”(yes)valuesallowstheexperimentmanagertochangethe
TE). The most realistic case, both Cutoff-Time and Adjust-Error will be True
(yes), in which case the team may heuristically decide to shoot for a higher or
lower TE, depending upon how the measurements are going and the amount of
timeremaining,butattheendofthattime,theywillbekickedoffthemachine.
8 Example Results
In this section we discuss some example results obtained from our model that
demonstratevariouswaysthatthemodecouldbeusedtoinvestigatechangesin
theuserinterfacesorworkflowsofLCLSinstrument-sideexperiments. Theseare
discussedinmoredetailinthesecondofthethreeauxiliaryvideos. Ineachcase
theverticalaxisrecordsthenumberofdatapointsrequiredtoreachtheTarget
Error (default is 0.001), over Performance Quality (PQ on the horizontal axis).
In all cases the samples are run from highest to lowest PQ. This is, somewhat
confusingly, from the right to the left on all the graphs. Recall that samples
withhighPQarelessimportant,andarethusassumedtobeusedfirsteitheras
controls, or in order to shake out the instrument. It is actually unrealistic that
themostimportantsamples(lowestPQ)wouldbehelduntiltheveryendofthe
experiment, because it is highly likely that the team will run out of beam time
andnotbeabletogetenoughdatafortheirmostimportantsamples. However,
allowingthemodeltore-plantheexperiment,resultinginreorganizingtheorder
in which samples are run, would be extremely confusing to try to depict and
19explain, sowehaveleftthistoaseparatepublicationfocusedjustonthistopic,
where we can explore the effects of sample re-planning in detail.
Figure 7 depicts two explorations with the model. The left graph in Figure
7scansacrossthreevaluesofFunctionalAcuity(FA),andismadewithAdjust-
Error=FalseandCutoff-time=False,thusallowingtheusertoremainaslongas
necessarytoreachthedefaultTargetError(TE)of0.001. Recallthatrunsstart
at high Performance Quality (PQ) and go to progressively lower PQs — that
is, from lower importance samples to higher, or right to left on these graphs.
Unsurprisingly, it requires less data to reach the TE threshold (0.001) as PQ
increases, and as FA improves (becomes smaller), so that the operator notices
thatthebeamandstreamarenotalignedatsmallerdistancesofmisalignment.
TherightgraphinFigure7holdsFAto0.1(thebestperformingvaluefromthe
left), and compares Noticing Delays (NDs). In this case, Adjust-Error=True
and Cutoff-time=True, which is the most realistic case: The team is going to
get kicked off the machine at a fixed time, but they are allowed to adjust the
ErrorTargettotrytogetsomedataonallsamples. Atfirstglance,inthiscase,
Noticing Delay makes no apparent difference; All the lines are more-or-less the
same. However, as PQ increases, the amount of data gathered appears to be
smalleratbothhighandlowPQ,buthigheratmiddlevaluesofPQ.Thereason
that this happens is that the experiment manager observes that they are going
torunoutoftimewiththedefaultTE,andadjuststheTEupwardsothatsome
dataisobtainedforeventhelast/mostimportant/lowestPQsamples,although
they do run out of time in the end anyway. Again, more flexible re-planning
wouldimprovethedataqualityforthemostimportant(lowestPQ)samples,as
will be explored in a separate paper.
Figure 8 depicts four explorations with the model, scanning Noticing Delay
(ND) at three values, and across all combinations of the conditions Adjust-
Error(ADJ )andCutoffTime(Stop ). Thegoaloftheexperimentalteamisto
achieveagood(low)errorratewithintheallottedtime. TheTargetError(TE)
is the team’s chosen target for a given run. The initial (default) TE is 0.001,
and with Adjust-Error(ADJ )False this never changes. However, with Adjust-
Error(ADJ )TruetheexperimentmanagerwilladjusttheTEbasedonthetime
remainingandtheperformanceperunitPerformanceQuality(PQ)tothispoint.
(This is described in detail in the supplemental videos, including giving the
algorithm and describing the code.) Recall that in all present simulations we
run from higher to lower PQ, that is right to left on these graphs. The first
(rightmost) sample error rate is always 0.001. (Note that the data for this
rightmost point on each graph is, as expected, almost identical in every case.)
When the team is able to adjust the error rate (ADJ True: top pair15) the
second through last (5th) sample’s TE can be adjusted to try to stay within
the given time. When the time is strictly limited Cutoff-time(Stop )True (left
pairinFig. 7)theteamiseventuallykickedoffthemachine. Themostrealistic
condition, is the top left: ADJ True, Stop True, and one can see, by comparing
15The terms “ADJ” and “Stop” are equivalent to the terms used elsewhere in the paper:
“Adjust-error”and“Cutoff-time”,respectively.
20Figure7: Lefthand: VaryingFunctionalAcuity(FAϵ0.1,0.5,1.0)withAdjust-
Error=False and Cutoff-time=False. Lower FA corresponds to noticing finer
misalignments, and requiring fewer samples to reach the Target Error. Runs
start at high Performance Quality (PQ) and go to progressively lower PQs —
that is, from lower importance samples to higher, or right to left. With Adjust-
Error=False and Cutoff-time=False the experiment is allowed to run to reach
TE=0.001 because the team is allowed to continue even after their beam time
technically expires. Right hand: Fixed FA = 0.1, varying Noticing Delay (ND
ϵ 1, 5, 10) with ADJUST-ERROR=True and Cutoff-time=True. Higher ND
corresponds to slower response to misalignments. ND has much less differential
effect at these levels (cf. Fig. 8). With ADJUST-ERROR=True and Cutoff-
time=Truesomedataisgatheredonallsamples,eventhoughtheteamiskicked
offthemachinewhentheirbeamtimeexpires(cf. Fig. 8bottomleft),although
not as much data as if they are allowed to continue, as on the left.
21the amount of data gathered on the last two sample runs between the top left
and bottom left graphs, when the team is allowed to adjust the error target
(top left), they do gather some, although not very much, data on the last two
(leftmost) critical samples, whereas in they cannot adjust error, but are strictly
time-limited(bottomleft),essentiallynodataisgatheronthelasttwo(leftmost)
critical samples (because they are kicked off the machine!) For comparison,
the right pair of graphs are with no time limitation (Stop False), which is an
unrealisticsituation,butgivesusasenseofhowmuchdataneedstobecollected
to reach a given Target Error. The top right case (ADJ False, Stop False) tells
us how long it will take to reach 0.001 error, whereas the bottom right case
(ADJ False, Stop False) gives the times it will take to reach an adjusted TE.
These can be compared with their leftward counterparts. Also, the effect of
ND is more pronounced in the right hand pair of graphs, because they run
significantly longer than those on the left. These results are also discussed in
the second half of the “Demo” (2nd) supplemental video[2].
9 Discussion
Our goal in developing a multi-agent, multi-scale cognitive interaction model of
instrument operations at LCLS was to enable the exploration of the impact of
proposedUIandworkflow/processchangesinthehuman-in-the-loopinstrument
operationsinordertoimprovetheefficiencyofresultsproductionatLCLS.Sci-
entific user facilities, such as LCLS, are cognitively interesting, and cognitively
difficult precisely because they are on the cutting edge of both science and hu-
man performance. There are no perfect simulators; If there were one would not
need the real machine! And although the instrument scientists are experts at
theirinstruments,theyarenotexpertsateachnewexperiment. Moreover,they
are usually working with external users who have widely varying skills, knowl-
edge, and experience, ranging from those who could probably nearly drive the
machine themselves, down to grad students, and sometimes even undergrads.
Eventhoughthecurrentmodelisonlyaroughanalogtothehighlycomplex
LCLS setting, it produces sensible results across a variety of parameter ranges
thatprovideinsightintohowthesevariousfactorsandscalesofcognitioninter-
act and impact the efficiency of experimental operations at LCLS.
ModulatingFunctionalAcuity(FA)andNoticingDelay(ND)highlightshow
improved FA (i.e., faster response to beam mis-alignment) leads to increased
overall experimental efficiency with fewer samples needed for success. In con-
trast, improved ND produces less significant impacts on efficiency, except in
unusual operating regimes of the system, for example when there is no beam
time limit. We also investigated quality and duration trade-offs by adjusting
whether the target SNR (Target Error) is dynamic or fixed, and by imposing
cutofftimes, orallowingrunstocontinueindefinitely. Unsurprisingly, fixedtar-
get error requirements produce higher fidelity given unlimited time. However,
allowingthetargeterrortoadaptyieldsbettertimemanagement, ensuringsuf-
ficient yields from high-performing samples even if lesser-quality measurements
22Figure 8: Three values of Noticing Delay (ϵ 1, 5, 10) across all combinations of
Adjust-Error (ADJ ) and Cutoff Time (Stop ) ϵ True, False.
23Figure 9: The results from the simulation presented in Figure 8 visualize the
differences between the conditions of Adjusted Error and Cutoff Time. Left,
the simulation executed without any time constraints took considerably longer
to yield a result. In contrast, under the time-limited condition, the duration
remained constant, irrespective of whether the goal error was modulated or
not. Right, the goal error threshold was consistently attained when there was
no time restriction, given the unlimited duration available to meet the desired
error rate. However, under time constraints, modulating the error resulted in
a superior overall goal error in comparison to the non-adjusted error condition.
Thisphenomenoncanbeattributedtotheoptimalutilizationoftime, ensuring
that more efficient samples were run while allowing lower goal error for the less
efficient ones, thereby enhancing the overall objective error rate.
become truncated by virtue of limited beam time.
These observations showcase how operational efficiency depends substan-
tially on factors related to the human–machine interface, such as Functional
Acuity,itsoperation,suchasFunctionalOperability,andcognitivefactorssuch
as attentiveness, amount of experience, and fatigue (represented by Noticing
Delayandresponsedelays). EnhancingtheseaspectsoftheLCLSsettingcould
optimize scientific productivity.
Previous computational cognitive models have focused on either individual
cognition (i.e., how an individual reaches his or her goals, generally through
reasoningandaction)oronthecollaborationbetweenindividuals(i.e., howthe
multiple individuals involved in the activity interact to accomplish the goals).
Rarely does a model capture individual cognition at the same time that it cap-
tures the closely related cognitive collaboration. Not only does the present
modelattempttocapturethesecomplexities, butalsocapturesaspectsofplan-
ning and opportunistic re-planning inherent in complex settings like LCLS. Al-
though only a handful of experiments are presented in the present paper, the
modeliscapableofsimulatingamuchwiderscopeofparameters,andrelatively
simpleextensionstotheexistingcodecaneasilybroadenitsscopeevenfurther,
even to the level of modeling specific displays, controls, and workflows, and
showing how these changes echo at the higher, multi-agent collaboration and
experiment planning scales, as is the nature of SLAC/LCLS operations.
24Ourmodelmerelyscratchesthesurfaceofthepotentialofmodel-basedcog-
nitiveengineeringeffortsasappliedtoscientificuserfacilitiessuchasLCLS.The
present model requires further elaboration and validation with respect to de-
tailed measurements of individual and team behavior. It should be extended to
capturemoreinter-andintra-teamcommunications. Perhapsmostimportantly,
models of learning at all levels should be introduced in order to capture both
short-term individual learning, for example the instrument scientists becoming
familiar with each new experiment; social learning, e.g., the team learning to
work together; and longer-term learning, such as staff training.
Asscienceprobesdeeperintothecomplexitiesofnature,large-scale,beyond-
the-state-of-the-arteffortslikethoseatLCLS,andotherscientificuserfacilities,
are rapidly increasing in importance. These facilities are in constant flux at
every scale. Efforts such as ours that attempt to model the system at multiple
scales may be helpful in increasing the efficiency of the science done at these
sites by helping to smooth and optimize the human-in-the-loop workflow that
willundoubtedlybeasignificantcomponentoftheseoperationsofscientificuser
facilities for the foreseeable future.
2510 Acknowledgements
The research described here arises from a project begun in 2019 by Devangi
Vivrekar and Paul Fuoss, with the assistance of Jeff Shrager. The research was
picked up by Teddy Rendahl, and then by Wan-Lin Hu and Jonathan Segal.
ThenumerousLCLSandSLACstaffandvisitingscientiststhatpermittedusto
observethemworking,andsatforinterviewsoverthoseyears,aretoonumerous
to mention, but we are especially grateful to Andy Aquila, Mark Hunter, Meng
Liangwhowelcomedusintotheirexperiments,andtoDanFlathandespecially
Jana Thayer for valuable discussions and support. This project was supported
by the U.S. Department of Energy, Office of Science, Office of Basic Energy
Sciences under Contract No. DE-AC02-76SF00515.
11 Contributions of Authors
Jonathan Segal did most of the model coding, ran the simulations, and created
the graphics. Wan-Lin Hu did most of the data collection and analysis. Frank
Ritterprovideddetailedguidanceonappliedcognitionandhumanfactors. Paul
Fuossconceivedtheproject, obtainedthefunding, dealtwithalladministrative
aspects, and was the primary applied physics informant. Jeff Shrager man-
agedproject,participatedintheobservationsandmodelcoding,andwrotethis
paper. All authors reviewed and commented on the paper.
26References
[1] https://lcls.slac.stanford.edu/overview. Accessed: 2024-08-07.
[2] https://www.youtube.com/playlist?list=PLI13S4Z1cbXggy98pDX-
jqnVnnoekohF2f. Accessed: 2024-07-20.
[3] https://github.com/Jonathannsegal/lclshfe/.Accessed:2024-08-07;
current commit ID: 685700a5b424f99554176133d5c26505cd396911.
[4] https://lcls.slac.stanford.edu/instruments/cxi. Accessed: 2024-
07-20.
[5] From 2021-08-27 Data Systems Update presented at UEC Meeting; Ac-
cessed: 2024-08-07. url: https://www-lcls.slac.stanford.edu/web/
lcls-uec/2021-08-28%20Data%20Systems%20for%20UEC%20Meeting.
pdf.
[6] https://lcls.slac.stanford.edu/sites/default/files/2023-
10/LCLS_Run20_Schedule-v11.pdf. Accessed: 2024-08-07.
[7] Collinsetal.“HowDoesScienceFitintoSociety?TheFractalModel”.In:
Experts and the Will of the People. Palgrave, Macmillin, 2020, pp. 63–88.
[8] Graesser et al. “Advancing the Science of Collaborative Problem Solv-
ing”. In: Psychological Science in the Public Interest 19.2 (2018). DOI:
10.1177/1529100618808244, pp. 59–92.
[9] Wilson et al. “Cognitive Engineering”. In: Wiley Interdisciplinary Re-
views: Cognitive Science 4.1 (2013). doi:10.1002/wcs.1204, pp. 17–31.
[10] S. G. Hutchins and T. Kendall. “The Role of Cognition in Team Collab-
oration During Complex Problem Solving”. In: Informed by Knowledge.
Ed. by K. L. Mosier and U. M. Fischer. New York: Psychology Press,
2010.
[11] F. E. Ritter. Usability method projects for improving studies at the Linac
Coherent Light Source. 2022.
[12] J. Shrager. “Demandance”. In: arXiv (2015). arXiv:1507.01882.
[13] S.Traweek.BeamtimesandLifetimes.Theworldofhigh-energyphysicists.
Cambridge, MA: Harvard U. Press, 1988.
27