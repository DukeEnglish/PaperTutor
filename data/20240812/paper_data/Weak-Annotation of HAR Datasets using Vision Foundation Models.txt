Weak-Annotation of HAR Datasets using
Vision Foundation Models
MariusBock KristofVanLaerhoven MichaelMoeller
UbiquitousComputing& UbiquitousComputing ComputerVision
ComputerVision UniversityofSiegen UniversityofSiegen
UniversityofSiegen Siegen,Germany Siegen,Germany
Siegen,Germany kvl@eti.uni-siegen.de michael.moeller@uni-siegen.de
marius.bock@uni-siegen.de
GMM Distance-based Centroid Clip
Clustering Thresholding Annotation
t
t Label ?
t
t
t
DL Model Lunging Hamstring Training Transferring Stretches Stretches
E.gShallow DeepConvLSTM DL Classifier to IMU Data
Lunges
Push-Ups
Sit-Ups
Figure1:Ourproposedweak-annotationpipeline:VisualembeddingsextractedusingVisionFoundationModelsareclusterd
usingGaussianMixtureModels(GMMs).Decreasingtherequiredlabellingeffort,ahumanannotatorisonlyaskedtoannotate
eachcluster‚Äôscentroidvideoclip.Centroidlabelsarethenpropagatedwithineachcluster.Transferredtothecorresponding
IMU-data,resultingweakly-annotateddatasetscanbeusedtotrainsubsequentclassifiers.
ABSTRACT KEYWORDS
Aswearable-baseddataannotationremains,todate,atedious,time- DataAnnotation;HumanActivityRecognition;Body-wornSensors
consumingtaskrequiringresearcherstodedicatesubstantialtime,
benchmarkdatasetswithinthefieldofHumanActivityRecogni-
1 INTRODUCTION
tioninlackrichnessandsizecomparedtodatasetsavailablewithin
relatedfields.Recently,visionfoundationmodelssuchasCLIPhave Thoughtheautomaticrecognitionofactivitiesthroughwearable
gainedsignificantattention,helpingthevisioncommunityadvance datahasbeenidentifiedasvaluableinformationfornumerousre-
infindingrobust,generalizablefeaturerepresentations.Withthe searchfields[9],currentlyavailablewearableactivityrecognition
majorityofresearcherswithinthewearablecommunityrelying benchmarkdatasetslackrichnessandsizecomparedtodatasets
on vision modalities to overcome the limited expressiveness of availablewithinrelatedfields.Comparedwith,forexample,the
wearabledataandaccuratelylabeltheirto-be-releasedbenchmark newlyreleasedEgo4Ddataset[17],itbecomesapparentthatcur-
datasetsoffline,weproposeanovel,clustering-basedannotation rently used datasets within the inertial-based Human Activity
pipelinetosignificantlyreducetheamountofdatathatneedsto Recognition(HAR)communityaresignificantlysmallerinterms
beannotatedbyahumanannotator.Weshowthatusingourap- ofthenumberofparticipants,lengthofrecordings,andvariety
proach,theannotationofcentroidclipssufficestoachieveaverage ofperformedactivities.Oneofthemaindriversforthisisthat,
labellingaccuraciescloseto90%acrossthreepubliclyavailable eventhoughbody-wornsensingapproachesallowrecordinglarge
HARbenchmarkdatasets.Usingtheweaklyannotateddatasets, amounts of data with only minimal impact on users in various
wefurtherdemonstratethatwecanmatchtheaccuracyscoresof situationsindailylife,wearable-baseddataannotationremains,to
fully-superviseddeeplearningclassifiersacrossallthreebench- date,atedious,time-consumingtaskandrequiresresearchersto
markdatasets.Codeaswellassupplementaryfiguresandresults dedicatesubstantialtimetoitduringdatacollection(takingupto
arepubliclydownloadableviagithub.com/mariusbock/weak_har. 14to20timeslongerthantheactualrecordeddata[30]).
Followingthesuccessinothervision-relatedfields,researchers
withinthevideoactivityrecognitioncommunityhavemadeuseof
CCSCONCEPTS
featureextractionmethodswhichprovidelatentrepresentations
‚Ä¢Human-centeredcomputing‚ÜíUbiquitousandmobilecom- of video clips rather than using raw image data [23, 33]. Such
putingdesignandevaluationmethods. featureextractionmethodsareusuallypretrainedonexistinglarge
4202
guA
9
]CH.sc[
1v96150.8042:viXra
‚Ä¶ ‚Ä¶
Label
PropagationMariusBock,KristofVanLaerhoven,andMichaelMoeller
benchmarkcorpora,thoughoftennotparticularlyrelated,theyare propagation[35],multi-instancelearning[36]orprobabilisticmeth-
capableoftransferringknowledgetotheactivityrecognitiontask ods[16,38].AdaimiandThomaz[2]followedtheworksof[34],
athand.Recently,visionfoundationmodels[28,29]havegained proposinganactivelearningframeworkwhichfocusesonasking
alotofattention.Typicallytrainedonalargeamountofcurated users to label only data which will gain the most classification
anduncuratedbenchmarkdatasets,thesemodelshavehelpedthe performanceboost.WiththeriseinpopularityofDeepLearning,
communityfurtheradvanceinfindingrobust,generalizablevisual deepclusteringalgorithmshavebeenproposedtoclusterlatent
featurerepresentations. representationsinunsupervisedandsemi-supervisedfashionus-
Withthemajorityofresearcherswithinthewearableactivity inge.g.autoencoders[4,5,22,26,42],recurrentnetworks[1,16],
recognitioncommunityrelyingonthevisionmodalitytoovercome self-supervised [31] and contrastive learning [3, 12, 20, 40, 43].
thelackexpressivenessofwearabledataandaccuratelylabeltheir Recently, Xiaet al. [43] and Tonget al. [40] demonstratedhow
to-be-releasedbenchmarkdatasetsoffline(seee.g.[11,18,30]),we vision-foundationmodelssuchasCLIP[29]andI3D[10]canbe
proposeanovelannotationpipelinewhichmakesuseofvisual usedtocreatevisual,complementaryembeddingstoinertialdata
embeddingsextractedusingpretrainedfoundationmodelstosig- suchthatacontrastivelosscanbecalculated.Thisworkmarks
nificantlylimittheamountofdatawhichneedstobeannotatedby oneofthefewinstancesofresearcherstryingtousevisualdata
ahumanannotator.Ourcontributionsarethree-fold: tolimittheamountofannotationsrequiredinwearableactivity
recognition.OurworktiesintotheworksofTongetal.[40]and
(1) Wefindthatvisualembeddingsextractedusingpublicly-
Xiaetal.[43],yetweproposeinsteadtoapplyvisionfoundation
availablevisionfoundationmodelscanbeclusteredactivity-
modelstoperformautomaticlabelpropagationbetweensimilar
wise.
embeddings.
(2) Weshowthatannotatingonlyoneclipperclustersuffices
toachieveaveragelabellingaccuraciesabove60%andclose
to 90% across three publicly available HAR benchmark
3 METHODOLOGY
datasets.
(3) Wedemonstratethatusingtheweaklyannotateddatasets, 3.1 AnnotationPipeline
oneiscapableofmatchingaccuracyscoresoffully-supervised
LatentSpaceClusteringviaVisionFoundationModels. Within
deeplearningclassifiersacrossallthreebenchmarkdatasets.
thefirstphasewedividetheunlabeleddatasetinto(overlapping)
videoclips.Givenaninputvideostreamùëã ofasampleparticipant,
2 RELATEDWORK weapplyaslidingwindowapproachwhichshiftsoverùëã,dividing
Vision Foundation Models. The term foundation models was theinputdataintovideoclips,e.g.offourseconddurationwitha
coinedbyDevlinetal.[14]andreferstomodelswhicharepre- 75%overlapbetweenconsecutivewindows.Thisprocessresultsin
trainedonalargeselectionofdatasets.Theideaofpre-training
ùëã ={x1,x2,...,xùëá}beingdiscretizedintoùë° ={0,1,...,ùëá}timesteps,
modelsonlargebenchmarkdatasetshasbeenprominentwithin whereùëá isthenumberofwindows,i.e.videoclips,foreachpartic-
thevisioncommunityforalongtime.Withinthevideoclassifica- ipant.Inspiredbyclassificationapproachesoriginatingfromthe
tioncommunityresearchersdemonstratedthatpretrainedmethods temporalactionlocalizationcommunity,wemakeuseofpretrained
suchasI3D[10],VideoSwin[24]orSlowFast[15]extractdiscrim- visionfoundationmodelstoextractlatentrepresentationsofeach
inatefeatureembeddingswhichcanbeusetotrainsubsequent clip.Thatis,xùë°
‚ààRùê∏
representsaone-dimensionalfeatureembed-
classifiers.FollowingtheirsuccessinNaturalLanguageProcessing
dingvectorassociatedwiththevideoclipattimestepùë°,whereùê∏
[8,14],researchersappliedmaskedautoencodersonvisualdata isnumberoflatentfeaturestheembeddingvectorconsistsof.In
input.Unlikepreviousmethods,maskedautoencodersarecapable totalweevaluatedthreepopularpretrainedfoundationmodels:a
ofpretrainingthemselvesinaself-supervisedmanner,allowing two-streaminflated3D-ConvNet(I3D)[10]pretrainedontheRGB
theuseoflargerdatasources.TwoofsuchmethodsareCLIP[29] andopticalflowfeaturesextractedfromKinetics-400dataset[19]as
andDINOv2[28].Theformer,publishedbyOpenAI,isavision- wellastwotransformerfoundationmodelsCLIP[29]andDINOv2
languagemodelwhichtriestolearnthealignmentbetweentext [28],whichwerepretrainedonamultitudeofcuratedanduncu-
andimages.Accordingtotheauthors,CLIPispretrainedonalarge rateddatasources.Notethat,unlikeCarreiraandZissermanin[10],
coporaofimage-textpairsscrapedfromtheworldwideweb.Simi- weuseRAFT[39]insteadofTV-L1[37]opticalflowestimation.
larly,therecentlyreleasedDINOv2byMETAAImakesaneffort AstheCLIPandDINOv2modelbotharenotexplicitlytrainedon
ofprovidingafoundationmodelwhichiscapableofextracting opticalflowfeatures,wealsotestcomplementingembeddingsof
generalpurposevisualfeatures,forwhichtheauthorscollected thetwomodelsbyconcatenatingthemwithextractedembeddings
datafromcuratedanduncuratedsources. oftheinflated3D-ConvNettrainedonRAFTopticalflowfeatures
oftheKineticsdataset.Inordertoobtainlatentrepresentations
Weakly-SupervisedWearableHAR. Withtheactivitylabelling wealteredmodelssuchthatintermediatefeaturerepresentations
ofbody-wornsensordatabeingatedioustask,manyresearchers canbeextracted.Table1providesdetailsactivationsatwhichlayer
havelookedatweakly-supervisedlearningtechniquestoreduce wereconsideredtobetheembeddingofwhichpretrainedmethod
therequiredamountofannotationstotrainsubsequentclassifiers. aswellastheirdimension.Tomergetogethertheframe-wisefea-
EarlyworkssuchthatofStikicetal.[34]haveshowntoreduce turesoutputtedoftheCLIPandDINOv2model,weapplyaverage
thelabellingeffortsfortrainingclassicalmachinelearningmod- poolingasdetailledin[25]toobtainasinglelatentrepresentation
elsthroughknowledge-drivenapproachesusinggraph-basedlabel perslidingvideoclip.Weak-AnnotationofHARDatasetsusingVisionFoundationModels
Table1:Networklayerusedforextractingembeddingsofthe 3.2 Weakly-supervisedTraining
differentvisionfoundationmodels[10,28,29].Subsequent
Assuming inertial and video data are synchronised, we further
layersareomittedsuchthatthenetworkoutputslatentrepre-
evaluatehowwelltheresultingannotatedinertialdatawithnon-
sentationsatpointoftheembeddinglayer.NotethattheI3D
uniformlabelnoiseissuitedtobeusedfortraininginertial-based
networkisusedforextractingbothRGBandflowfeatures
deeplearningclassifiers.Asourbenchmarkalgorithmsofchoice
andwerefertothevision-basedpartoftheCLIPmodel.
weusetworecentlypublishedstate-of-the-artmethods,namely
theShallowDeepConvLSTM[6]andTinyHARarchitecture[45].
Model EmbeddingLayer Dimension Weusebotharchitecturesasoriginallyintroducedbytheauthors,
I3D lastaveragepoollayer R1024 specificallyusingthesamesizeandnumberofconvolutionalfilters,
CLIP lastprojectionlayer(vision-CLIP) R768 convolutionallayersandsizeoftherecurrentlayers.Duringtrain-
DINO lastlayerhiddenstate(clf.token) R1024 ingweapplyaslidingwindowofonesecondwithanoverlapof50%,
asitproofedtobeprovideconsistentclassificationperformances
acrossamultitudeofHARdatasets[6].Wetraineachnetworkfor
30epochsusingtheAdamoptimizer(learningrate1ùëí‚àí4andweight
decay1ùëí‚àí6)applyingastep-wiselearningratewithadecayfactor
Havingextractedlatentrepresentationsofeachvideoclipwithin of0.9aftereach10epochs.Tomigitatetheintroducedlabelnoise
thetestedbenchmarkdatasets,weapplyGaussianMixtureModels byourproposedweak-annotationpipeline,wecalculatetheloss
(GMM)toclustertheembeddingsonaper-participantlevel.Though duringtrainingusingtheweightedpartiallyHuberisedgeneralised
GMMsarenotoriginallyintendedtobeusedwithhigh-dimensional cross-entropy(PHGCE)loss[27],whichextendsthedefinitionof
data,theyhaveshowntoprovidegoodresultsclusteringvisualem- thegeneralizedcross-entropyloss[44]withavariantofgradient
beddingsespeciallyinthecontextofactionrecognition[21,41]and clipping.Tocomparethevalidityofourapproach,wecompare
allow,unlikemethodssuchask-nearestneighbors,moreflexibility amongstasetof(weakly-)annotatedtrainingapproaches:
regardingtheshapeofclusters.TrainingoneGMMclusteringalgo-
(1) Fully-supervised:Fully-supervisedresultsusingtheoriginal,
rithmperparticipantandapplyingittosaidthatassignseachvideo
clipxùë° ‚ààRùê∏ aclusterlabelùë• ùëê ‚àà1,...,ùê∂,whereùê∂isthenumberof fully-annotatedbenchmarkdatasets.
(2) Few-Shot-CE:Fully-supervisedtrainingusingonlythean-
GMMcomponents,i.e.clusters,applied.
notatedclipsandaweightedcross-entropyloss.
(3) Random-CE:trainingusinganequalamountofrandom
Weak-LabelingviaCentroidClips. Onceeachvideoclipofastudy
participanthasbeenassignedaclusterlabelùë• ùëê,thesecondphase annotatedclipsasin(2)andaweightedcross-entropyloss.
(4) Weak-CE:Weakly-supervisedtrainingusingtheweakly-
ofourapproachconsistsofahumanannotatoronlyneedingto
annotateddatasetandaweightedcross-entropyloss.
annotateonesampleclippercluster.Assumingthecentroidofa
(5) Weak-PHGCE:Weakly-supervisedtrainingusingtheweakly-
clusterismostrepresentativeofallclipswithinthatcluster,we
canpropagatetheactivitylabelùëé‚àà1,...,ùê¥ofsaidcliptoallother annotateddatasetandaweightedPHGCEloss.
clipseliminatingtheneedofannotatingtheotherclipsviaahuman
annotator,whereùê¥isthenumberofactivitieswithinthedataset.
Table2:Averagelabelingaccuracyandstandarddeviation
AsGMMdonotexplicitlyprovideadefinitionofacentroidofa acrossstudyparticipantsusingdifferenttypesandcombina-
component,wecalculatethecentroidclipofeachclustercompo- tionsofembeddings[10,28,29]extractedfromthreebench-
nentbeingtheclipwhichhasthehighestdensitywithinsaidcluster. markdatasets[7,13,32]applyingaGMM-basedclustering
Thatis,giventhecovariancematrixofeachmixturecomponent using100clusters.Overall,acombinationofCLIPandoptical
Œ£‚ààRùê∏√óùê∏
,assumingeachcomponenthasitsowngeneralcovari- flowembeddingsprovedmostconsistentacrossalldatasets.
ancematrix,andmeanvectorùúá ‚ààRùê∏ ,wecalculatethedensityof
eachpointasthelogarithmoftheprobabilitydensityfunctionof
themultivariatenormaldistributiondefinedbyùúáandŒ£.Having WEAR Wetlab ActionSense
identifiedthecentroidclipwithineachcluster,ourapproachpropa- (1)I3D 82.62(¬±4.65) 66.08(¬±9.53) 53.47(¬±5.95)
gatestheannotationprovidedbythehumanannotatortoallother (2)CLIP 82.47(¬±6.03) 72.70(¬±6.42) 59.85(¬±4.42)
clips,whichwerealsoassignedtothatcluster. (3)DINOv2 79.20(¬±4.04) 69.28(¬±8.12) 60.25(¬±4.04)
Asourapproachforceseachvideocliptobeassignedanactivity (4)RAFT 76.86(¬±4.79) 51.50(¬±6.96) 45.64(¬±5.19)
label,weaugmentourclusteringwithasubsequentdistance-based (1)+(4) 85.17(¬±4.48) 60.91(¬±8.36) 53.00(¬±4.66)
thresholdinginordertoremoveoutlierclipsfromtheautomatic (2)+(4) 83.96(¬±4.99) 66.23(¬±7.86) 57.29(¬±5.64)
labelling.Assumingthatthedistanceofanothercliptothecentroid (3)+(4) 79.13(¬±4.30) 70.18(¬±9.79) 56.55(¬±4.51)
resembles its likelihood of belonging to the same activity class,
weomitclipsfromthedatasetwhichexceedacertaindistance
fromtheirrespectivecentroidclip,withthedistancebeingcalcu-
latedastheùêø2-normbetweentwoembeddingvectors.Eventhough 3.3 Datasets
thisapproachdecreasestheamountofdatawhichcanbeusedto WEAR. TheWEARdatasetoffersbothinertialandegocentric
trainsubsequentclassificationalgorithms,weshowtoincreasethe videodataof18participantsdoingavarietyof18sports-related
overalllabellingaccuracybyasignificantmargin. activities, including different styles of running, stretching, andMariusBock,KristofVanLaerhoven,andMichaelMoeller
(a) WEAR (b) Wetlab (c) ActionSense
100.00%
90.00%
80.00%
70.00%
60.00%
50.00%
40.00%
30.00%
20.00% 10.00%
0.00%
19 30 50 100 150 200 9 30 50 100 150 200 20 30 50 100 150 200
Number of Clusters
Labelling Accuracy % of Data Annotated
Figure2:Box-plotdiagrams showingthedistributionoflabellingaccuraciesacrossstudyparticipantswithincreasingnumber
ofclusters.Thebarplot belowthebox-plotsprovidesdetailsperclustersettingaboutthepercentageofdatacomparedtothe
totalsizeofthethreebenchmarkdatasets[7,13,32]anannotaterwouldneedtoannotate.Onecanseeacleartrendthatwith
anincreaseinclusters,labellingaccuracyincreasesalongwithdeviationacrossstudyparticipantsdecreasing.
strength-basedexercises.Recordingstookplaceatchangingout- thiscliplengthisasuitablelengthtobeinterpretableforahuman
doorlocations.Eachstudyparticipantwasequippedwithahead- annotatorwhilesimultaneouslyavoidingmixingmultipleactivities
mountedcameraandfoursmartwatches,onewornoneachlimbin intooneslidingwindow.Furthermore,duringallexperimentsonly
afixedorientation,whichcaptured3D-accelerometerdata. thelabelofthecentroidclipispropagatedtothatofallothercluster
instances.Ablationexperimentsevaluatingdifferentcliplengths
ActionSense. PublishedbyDelPretoetal.[13],theActionSense andnumberofannotatedclipsperclustertodeterminethelabelto
datasetprovidesamultitudeofsensorscapturingdatawithinan bepropagatedcanbefoundwithinourcoderepository.
indoor,artificialkitchensetup.Amongstthesensors,participants
woreInertialMeasurmentUnits(IMUs)onbothwristsaswellas
smartglasseswhichcapturedtheego-viewofeachparticipant.Dur- 4.1 AnnotationPipeline
ingrecordings,participantsweretaskedtoperformvariouskitchen Table2showstheaveragelabellingaccuracyaveragedacrosspar-
choresincludingchoppingfoods,settingatableand(un)loadinga ticipantsobtainedwhenapplyingourproposedannotationpipeline
dishwasher.Withintheiroriginalpublication,theauthorsprovide usingvarioustypesofextractedvisualembeddings.Onecansee
annotationsof19activitiesof10participants.Notethatthedataset thatincaseoftheWEAR[7]andActionSensedataset[13]labelling
downloadoftheActionSensedatasetprovidesIMUandegocentric accuracycanbeimprovedbycombiningbothRGBandopticalflow
videodataofonly9insteadof10participants. featuresincaseofallembeddings.OverallacombinationofCLIP
andopticalflowfeaturesprovestobemostconsistentacrossour
Wetlab. Takingplaceinawetlablaboratoryenvironment,the
threebenchmarkdatasetsofchoice,makingitthusourembedding
Wetlabdataset[32]comprisesofdataof22studyparticipantswhich
ofchoiceforsubsequentexperiments.Applyingalabellingstrat-
performedtwoDNAextractionexperiments.Forpurposesofthis
egyofonlyannotatingthecentroidclipofeachcluster,Figure2
paperweusedtheannotatedprovidedbytheauthorsofthere-
presentsabox-plotvisualizationofapplyingdifferentnumberof
occurringactivitiesbaseactivities(suchasstirring,cutting,etc.)
clustersduringtheclusteringoftheparticipant-wiseembeddings.
withintheexperimentalprotocol.Duringrecordings,eachpartici-
Onecanseethatbyonlyannotating100clipsperstudyparticipant,
pantworeasmartwatchinafixedorientationonthewristoftheir
ourproposedannotationpipelineiscapableofreachinglabelling
dominant hand, which captured 3D-accelerometer data. Unlike
accuraciesabove85%incaseoftheWEARandcloseto70%incase
theWEARandActionSensedataset,theWetlabdatasetprovides
oftheWetlab[32]andActionSensedataset.
videodataofastaticcamerawhichwasmountedabovethetableat
Furthermore,asevidentbyanoverallshrinkingboxplotwith
whichtheexperimentwasperformed,thuscapturingabirds-eye
increasing number of clusters, our approach is becoming more
perspectiveoftheexperiment‚Äôssurroundings.
stablewiththestandarddeviationacrossstudyparticipantsde-
creasingincaseofallthreedatasets.AsFigure2shows,applying
4 RESULTS aclusteringofùê∂ =ùê¥,i.e.asmanyclustersasthereareactivities
Toensurethatreportedperformancedifferencesarenotthebased inthedataset,resultsintheclusteringnotbeingcapableofdiffer-
onstatisticalvariance,allreportedexperimentsarerepeatedthree entiatingthenormalandcomplexvariationsofactivities,different
times,applyingasetofthreepredefinedrandomseeds.Thisapplies runningstylesandnull-classfromallotheractivities.Ingeneral,
bothfortheannotationpipelineexperimentsaswellasweakly- wewitnessatrendthatbyapplyingalargeramountofclusters
supervisedtrainingresults.Duringallannotation-basedexperiment thanactivitiespresentinthedataset,onegivestheGMMclustering
mentionedinSection4.1weapplyacliplengthoffourseconds enoughdegreesoffreedomtodifferentiateevenactivitieswhich
alongwithathreesecondoverlapbetweenclips.Weassumethat share similarities, yet slightly differ from each other. Lastly, by
%20.2 %91.3 %23.5
%56.01
%79.51
%03.12
%24.1 %47.4 %09.7
%97.51
%96.32
%85.13
%56.2 %89.3 %46.6
%72.31
%19.91
%45.62Weak-AnnotationofHARDatasetsusingVisionFoundationModels
Table3:DeepLearningresultsofapplyingtwoinertial-basedmodels[6,45]onvariousweakly-annotatedversionsofthree
publicdatasets[7,13,32].Trainingusingweakly-annotateddatasetsoutperformedbothfew-shottrainingusingonlythe
annotateddataaswellasanequalamountofrandomannotatedclips.Withanincreaseinnumberofclustersourweakly-
supervisedapproachiscapableofbeingclosetomatchingthepredictiveperformanceoffully-supervisedbaselineshaving
manuallyannotatedonlyafractionoftheactualdataset.ThesuffixT-6(T-4)refertotrainingapplyingathresholdof6(4).
DeepConvLSTM TinyHAR
ùëê=19 ùëê=50 ùëê=100 ùëê=19 ùëê=50 ùëê=100
Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1
Fully-supervised 79.89 78.36 79.89 78.36 79.89 78.36 77.83 71.89 77.83 71.89 77.83 71.89
Few-Shot-CE 37.41 24.76 59.58 46.25 65.61 53.51 37.41 26.55 59.58 46.25 65.61 53.51
Random-CE 45.90 31.13 59.46 46.98 65.91 53.38 23.73 23.73 59.72 46.34 66.27 55.00
Weak-CE 42.55 34.09 64.17 54.59 73.38 63.23 49.45 38.75 66.68 54.05 71.10 59.65
Weak-PHGCE 48.62 35.45 70.34 55.27 76.15 63.43 51.46 39.23 68.53 55.40 73.37 61.68
Weak-CE-T-6 59.70 46.63 73.06 60.60 76.28 66.13 57.22 46.71 68.19 55.56 72.03 60.31
Weak-PHGCE-T-6 59.39 44.97 73.17 58.84 77.55 64.77 58.78 46.27 69.47 56.29 74.05 61.67
Weak-CE-T-4 68.86 57.33 74.72 63.93 77.81 68.22 65.68 55.64 71.31 60.16 73.93 63.42
Weak-PHGCE-T-4 61.35 47.00 74.45 60.61 76.64 64.81 62.90 50.46 72.25 60.84 74.83 63.94
ùëê=9 ùëê=50 ùëê=100 ùëê=9 ùëê=50 ùëê=100
Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1
Fully-supervised 45.27 38.64 45.27 38.64 45.27 38.64 38.75 28.85 38.75 28.85 38.75 28.85
Few-Shot-CE 15.60 11.39 21.89 16.46 22.78 17.38 15.18 11.62 22.43 16.14 25.92 18.27
Random-CE 16.33 8.95 26.48 18.62 26.50 20.05 34.23 24.38 27.74 17.79 29.70 19.37
Weak-CE 16.97 8.53 32.57 25.78 36.51 29.72 23.90 14.70 34.23 24.38 36.30 25.76
Weak-PHGCE 18.62 10.48 27.64 23.17 33.78 27.62 24.06 15.19 33.79 24.35 35.53 25.53
Weak-CE-T-6 23.01 18.12 33.78 27.08 38.41 29.77 26.14 18.63 33.79 24.21 36.20 25.76
Weak-PHGCE-T-6 20.26 13.72 28.77 23.90 32.29 26.71 25.22 18.41 33.34 24.25 34.93 25.17
Weak-CE-T-4 23.54 18.71 32.02 26.37 35.25 28.95 25.15 17.91 32.85 23.84 34.64 25.04
Weak-PHGCE-T-4 21.82 15.81 28.33 23.91 31.20 25.64 23.82 17.29 32.39 23.72 33.66 24.70
ùëê=20 ùëê=50 ùëê=100 ùëê=20 ùëê=50 ùëê=100
Acc F1 Acc F1 Acc F1 Acc F1 Acc F1 Acc F1
Fully-supervised 20.73 14.82 20.73 14.82 20.73 14.82 22.19 18.04 22.19 18.04 22.19 18.04
Few-Shot-CE 6.47 2.58 8.14 4.92 9.96 6.08 6.19 2.32 8.50 4.00 11.15 6.72
Random-CE 4.69 1.63 9.42 5.09 10.02 5.77 5.01 1.70 7.80 4.19 11.44 6.50
Weak-CE 12.32 7.67 13.91 9.94 17.35 12.20 13.74 9.75 15.58 11.55 17.16 13.45
Weak-PHGCE 11.65 6.34 12.34 7.43 11.62 7.02 14.35 9.43 13.88 9.25 14.52 10.09
Weak-CE-T-6 14.67 8.81 15.95 10.07 15.29 10.69 13.35 9.32 15.70 11.22 17.53 13.15
Weak-PHGCE-T-6 9.91 4.92 10.57 5.44 11.78 6.79 12.17 7.52 13.72 8.16 14.53 9.01
Weak-CE-T-4 10.40 7.15 12.31 7.10 12.08 8.80 10.80 8.02 13.65 9.24 15.03 11.16
Weak-PHGCE-T-4 8.34 3.86 8.92 4.35 9.91 5.56 9.24 5.82 11.07 6.09 12.11 7.35
distancethresholdingclustersandexcludinginstanceswhichare clips,butforthecaseofapplying100clustersisclosetomatching
exceedingacertaindistancefromtheirrespectivecentroid,helps accuracyscoresofafully-supervisedtrainingacrossallthreebench-
increasingthelabellingaccuracysignificantlyacrossalldatasets. markdatasets,forbothinertial-basedarchitectures.Comparedto
Whileathresholdof4helpsincreasethelabellingaccuracywell anormalcross-entropyloss,thePHGCElossprovidesmoresta-
above75%andevenupto93%incaseoftheWEARdataset,the bleresultsincaseofhigherlabelnoise,e.g.,whennotapplyinga
thresholdingomitsbetween50%andupto90%ofthedatasets. distance-basedthresholdingand/orapplyingasmallernumberof
clusters.Ingeneral,thedistance-basedthresholdingsignificantly
improvedresultsacrossalldatasets.Althoughthresholdingsignifi-
4.2 Weakly-SupervisedTraining
cantlyreducestheamountoftrainingdata,theresultingdecrease
AsacombinationofCLIPandoptical-flow-basedfeaturesproved inoveralllabellingnoise,especiallyforapproachesthatapplieda
tobemoststableacrossallthreedatasets,wechosetousesaid lowernumberofclusters,improvedclassificationresults.Wepro-
embeddingasbasisforourweakly-supervisedtraining.Table3 videadetailedoverviewoftheinfluenceofthreshodingonlabelling
providesanoverviewacrosstheeightevaluatedtrainingscenarios. accuracyanddatasetsizewithinthepaper‚Äôscoderepository.
Ourproposedweakly-supervisedtrainingisnotonlycapableofout-
performingthefew-shottrainingusingonlytheannotatedcentroid
RAEW
balteW
esneSnoitcAMariusBock,KristofVanLaerhoven,andMichaelMoeller
than vice versa. This caused the activities transfer and pouring,
twoclasseswhichonlyhaveafewinstancesinthegroundtruth
Ground Truth
null dataandwhicharemostfrequentlyannotatedincorrectlyasNULL-
normal class,tonotbepredictedcorrectlyonceacrossallstudyparticipants.
rotating arms
NotethatfortheActionSensedatasetclassificationresultseven
skipping
sidesteps inthefully-supervisedsettingaresignificantlyworsecompared
butt-kicks to that of the other two datasets. As evident by a nevertheless
triceps
largelabellingaccuracyusingourapproach,weassumethatla-
lunging
shoulders belsemanticsofthedatasetaretoovision-centric(e.g.peelinga
hamstrings cucumberorapotato)tobecorrectlyrecognizedusingonlyinter-
lumbar rotation tialdata.Nevertheless,per-classclassificationresultsofthefully-
push-ups
vs.weakly-supervisedtrainingshowasimilarconfusionamongst
push-ups (complex)
sit-ups classes,suggestinglearnedpatternsoftheclassifieraresimilarin
sit-ups (complex) bothtrainingscenarios.
burpees
lunges
5 DISCUSSION&CONCLUSION
lunges (complex)
bench-dips Withinthispaperwepresentedaweak-annotationpipelinefor
null HARdatasetsbasedonVisionFoundationModels.Weshowedthat
normal
visualembeddingsextractedusingVisionFoundationModelscan
rotating arms
skipping beclusteredusingGaussianMixtureModels(GMM).Decreasing
sidesteps therequiredlabellingeffort,usingthesuggestedpipelineahuman
butt-kicks
annotatorisonlyaskedtoannotateeachcluster‚Äôscentroidvideo
triceps lunging clip.Bypropagatingtheprovidedlabelswithineachclusterour
shoulders approachiscapableofachievingaveragelabellingaccuraciesabove
hamstrings 60%andcloseto90%acrossthreepopularHARbenchmarkdatasets.
lumbar rotation
Wefurthershowedthattheresultingweakly-annotatedwearable
push-ups
push-ups (complex) datasetscanbeusedtotrainsubsequentdeeplearningclassifiers
sit-ups withaccuracyscores,incaseofapplyingasufficientlylargenum-
sit-ups (complex)
berofclusters,beingclosetomatchingthatofafully-supervised
burpees
lunges trainingacrossallthreebenchmarkdatasets.
lunges (complex) Ourresultsunderscoreoneoftheimplicationsrecentadvance-
bench-dips mentsinthevisioncommunityinfindinggeneralizablefeature
representationsmighthaveonthefieldofHAR.Withtherapid
progressbeingmadeintheareaoffoundationmodels,follow-ups
ofmodelssuchasCLIPandDINOv2couldfurtherrobustifytheau-
tomaticanalysisofcollectedvideostreamsinwearable-baseddata
collection.Ourclustering-basedpipelinethusnotonlyimproves
Figure3:ConfusionmatricescomparingtheshallowDeep- theefficiencyofdataannotationbutalsocontributestothecreation
ConvLSTMfully-supervisedresultscomparedtothatofthe ofricherandmoreextensiveHARbenchmarkdatasets.
bestperformingweak-labellingapproach.Withexceptionof
theNULL-class,allactivitieswereabletobeclassifiedclose ACKNOWLEDGMENTS
totheperformanceofthefully-supervisedapproach. WegratefullyacknowledgetheDFGProjectWASEDO(DFGLA
275811-1)andtheUniversityofSiegen‚ÄôsOMNIcluster.
Figure3showsaside-by-sidecomparisonoftheconfusionmatri-
REFERENCES
ces of the fully-supervised and best-performing weak-labelling
approach (Weak-CE-T-4) using the WEAR dataset and shallow [1] AlirezaAbedin,FarbodMotlagh,QinfengShi,HamidRezatofighi,andD.Ranas-
inghe.2020. Towardsdeepclusteringofhumanactivitiesfromwearables.
DeepConvLSTM. One can see that, apart from the NULL-class, https://doi.org/10.1145/3410531.3414312
classificationresultsofallactivitiesoftheweakly-supervisedtrain- [2] RebeccaAdaimiandEdisonThomaz.2019. LeveragingActiveLearningand
ConditionalMutualInformationtoMinimizeDataAnnotationinHumanActivity
ingaresimilartothatofthefully-supervised.Withtheintra-class
Recognition.ACMonInteractive,Mobile,WearableandUbiquitousTechnologies
similarityofaNULL-classwithindatasetsbeingquitelow,itisto 3,3(2019),1‚Äì23. https://doi.org/10.1145/3351228
alargerdegreegroupedtogetherwithinstancesofotherclasses [3] AbrarAhmed,HarishHaresamudram,andThomasPloetz.2022.Clusteringof
HumanActivitiesfromWearablesbyAdoptingNearestNeighbors.InACMIn-
makingitharderforaninertial-basedclassifiertolearnunique
ternationalSymposiumonWearableComputers. https://doi.org/10.1145/3544794.
patternsonlyapplicabletothatoftheNULL-class.Lookingatper- 3558477
classresultsoftheWetlabdataset,onecanseethattheintroduced [4] BandarAlmaslukh,JalalAlMuhtadi,andAbdelmonimArtoli.2017.AnEffective
DeepAutoencoderApproachforOnlineSmartphone-BasedHumanActivity
labellingnoisecausedtheclassifiertrainedusingweakly-annotated Recognition.InternationalJournalofComputerScienceandNetworkSecurity17,
dataismorelikelytoconfuseactivitieswithNULL-classrather 4(2017),160‚Äì165. http://paper.ijcsns.org/07_book/201704/20170423.pdf
desivrepus-ylluF
4-T-EC-kaeW
detciderP llun lamron smra
gnitator
gnippiks spetsedis skcik-ttub specirt gnignul sredluohs sgnirtsmah noitator
rabmul
spu-hsup )xelpmoc(
spu-hsup
spu-tis )xelpmoc(
spu-tis
seeprub segnul )xelpmoc(
segnul
spid-hcnebWeak-AnnotationofHARDatasetsusingVisionFoundationModels
[5] BehroozAzadi,MichaelHaslgr√ºbler,BernhardAnzengruber-Tanase,Georgios IEEETransactionsonImageProcessing31(2022). https://doi.org/10.1109/TIP.
Sopidis,andAloisFerscha.2024.RobustFeatureRepresentationUsingMulti- 2022.3195321
TaskLearningforHumanActivityRecognition.Sensors24,2(2024),681. https: [24] ZeLiu,JiaNing,YueCao,YixuanWei,ZhengZhang,StephenLin,andHanHu.
//doi.org/10.3390/s24020681 2022.Videoswintransformer.InIEEE/CVFConferenceonComputerVisionand
[6] MariusBock,AlexanderHoelzemann,MichaelMoeller,andKristofVanLaer- PatternRecognition. https://doi.org/10.1109/cvpr52688.2022.00320
hoven.2021.ImprovingDeepLearningforHARWithShallowLstms.InACMIn- [25] HuaishaoLuo,LeiJi,MingZhong,YangChen,WenLei,NanDuan,andTianrui
ternationalSymposiumonWearableComputers. https://doi.org/10.1145/3460421. Li.2022.CLIP4Clip:AnempiricalstudyofCLIPforendtoendvideoclipretrieval
3480419 andcaptioning.Neurocomputing508(2022),293‚Äì304. https://doi.org/10.1016/j.
[7] MariusBock,HildeKuehne,KristofVanLaerhoven,andMichaelMoeller.2023. neucom.2022.07.028
WEAR:AnOutdoorSportsDatasetforWearableandEgocentricActivityRecog- [26] HaojieMa,ZhijieZhang,WenzhongLi,andSangluLu.2021. Unsupervised
nition.CoRRabs/2304.05088(2023). https://arxiv.org/abs/2304.05088 HumanActivityRepresentationLearningwithMulti-taskDeepClustering.ACM
[8] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan, onInteractive,Mobile,WearableandUbiquitousTechnologies5,1(2021),1‚Äì25.
PrafullaDhariwal,ArvindNeelakantan,PranavShyam,GirishSastry,Amanda https://doi.org/10.1145/3448074
Askell,SandhiniAgarwal,ArielHerbert-Voss,GretchenKrueger,TomHenighan, [27] AdityaKrishnaMenon,AnkitSinghRawat,SashankJ.Reddi,andSanjivKumar.
RewonChild,AdityaRamesh,DanielZiegler,JeffreyWu,ClemensWinter,Chris 2020.Cangradientclippingmitigatelabelnoise?.InInternationalConferenceon
Hesse,MarkChen,EricSigler,MateuszLitwin,ScottGray,BenjaminChess,Jack LearningRepresentations. https://openreview.net/forum?id=rklB76EKPr
Clark,ChristopherBerner,SamMcCandlish,AlecRadford,IlyaSutskever,and [28] MaximeOquab,Timoth√©eDarcet,Th√©oMoutakanni,HuyVo,MarcSzafraniec,
DarioAmodei.2020. Languagemodelsarefew-shotlearners.InAdvancesin VasilKhalidov,PierreFernandez,DanielHaziza,FranciscoMassa,Alaaeldin
NeuralInformationProcessingSystems. https://proceedings.neurips.cc/paper_ El-Nouby,MahmoudAssran,NicolasBallas,WojciechGaluba,RussellHowes,
files/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf Po-YaoHuang,Shang-WenLi,IshanMisra,MichaelRabbat,VasuSharma,Gabriel
[9] AndreasBulling,UlfBlanke,andBerntSchiele.2014. ATutorialonHuman Synnaeve,HuXu,Herv√©Jegou,JulienMairal,PatrickLabatut,ArmandJoulin,
ActivityRecognitionUsingBody-WornInertialSensors.Comput.Surveys46,3 andPiotrBojanowski.2024.DINOv2:LearningRobustVisualFeatureswithout
(2014),1‚Äì33. https://doi.org/10.1145/2499621 Supervision.CoRRabs/2304.07193(2024). https://doi.org/10.48550/arXiv.2304.
[10] JoaoCarreiraandAndrewZisserman.2017.QuoVadis,ActionRecognition?A 07193
NewModelandtheKineticsDataset.InIEEEConferenceonComputerVisionand [29] AlecRadford,JongWookKim,ChrisHallacy,AdityaRamesh,GabrielGoh,
PatternRecognition. https://doi.org/10.1109/cvpr.2017.502 SandhiniAgarwal,GirishSastry,AmandaAskell,PamelaMishkin,JackClark,
[11] ShingChan,HangYuan,CatherineTong,AidanAcquah,AbramSchonfeldt, GretchenKrueger,andIlyaSutskever.2021.LearningTransferableVisualModels
JonathanGershuny,andAidenDoherty.2024. CAPTURE-24:Alargedataset FromNaturalLanguageSupervision.CoRRabs/2103.00020(2021). https://doi.
ofwrist-wornactivitytrackerdatacollectedinthewildforhumanactivity org/10.48550/arXiv.2103.00020
recognition.CoRRabs/2402.19229(2024). https://doi.org/10.48550/arXiv.2402. [30] DanielRoggen,AlbertoCalatroni,MircoRossi,ThomasHolleczek,KilianF√∂rster,
19229 GerhardTr√∂ster,PaulLukowicz,DavidBannach,GeraldPirkl,AloisFerscha,
[12] ShohrehDeldari,HaoXue,AaqibSaeed,DanielV.Smith,andFloraD.Salim. JakobDoppler,ClemensHolzmann,MarcKurz,GeraldHoll,RicardoChavar-
2022. COCOA:CrossModalityContrastiveLearningforSensorData. ACM riaga,HesamSagha,HamidrezaBayati,MarcoCreatura,andJos√©delR.Mill√†n.
onInteractive,Mobile,WearableandUbiquitousTechnologies6,3(2022),1‚Äì28. 2010.CollectingComplexActivityDatasetsinHighlyRichNetworkedSensor
https://doi.org/10.1145/3550316 Environments.InIEEESeventhInternationalConferenceonNetworkedSensing
[13] JosephDelPreto,ChaoLiu,YiyueLuo,MichaelFoshey,YunzhuLi,Antonio Systems. https://doi.org/10.1109/INSS.2010.5573462
Torralba,WojciechMatusik,andDanielaRus.2022.ActionSense:AMultimodal [31] AaqibSaeed,TanirOzcelebi,andJohanLukkien.2019.Multi-taskSelf-Supervised
DatasetandRecordingFrameworkforHumanActivitiesUsingWearableSensors LearningforHumanActivityDetection.ACMonInteractive,Mobile,Wearable
inaKitchenEnvironment.InNeuralInformationProcessingSystemsTrackon andUbiquitousTechnologies3,2(2019),1‚Äì30. https://doi.org/10.1145/3328932
DatasetsandBenchmarks. https://action-sense.csail.mit.edu [32] PhilippM.Scholl,MatthiasWille,andKristofVanLaerhoven.2015.Wearables
[14] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaN.Toutanova.2019. intheWetLab:ALaboratorySystemforCapturingandGuidingExperiments.
BERT:Pre-trainingofDeepBidirectionalTransformersforLanguageUnder- InACMInternationalJointConferenceonPervasiveandUbiquitousComputing.
standing.InConfernceoftheNorthAmericanChapteroftheAssociationfor https://doi.org/10.1145/2750858.2807547
ComputationalLinguistics. https://arxiv.org/abs/1810.04805 [33] Dingfeng Shi, Yujie Zhong, Qiong Cao, Lin Ma, Jia Li, and Dacheng Tao.
[15] ChristophFeichtenhofer,HaoqiFan,JitendraMalik,andKaimingHe.2019.Slow- 2023. TriDet:TemporalActionDetectionWithRelativeBoundaryModeling.
FastNetworksforVideoRecognition.InInternationalConferenceonComputer InIEEE/CVFConferenceonComputerVisionandPatternRecognition. https:
Vision. https://doi.org/10.1109/ICCV.2019.00630 //doi.org/10.1109/cvpr52729.2023.01808
[16] Marjan Ghazvininejad, Hamid R. Rabiee, Nima Pourdamghani, and Parisa [34] MajaStikic,DianeLarlus,SandraEbert,andBerntSchiele.2011.WeaklySuper-
Khanipour.2011. HMMbasedsemi-supervisedlearningforactivityrecogni- visedRecognitionofDailyLifeActivitieswithWearableSensors.IEEETrans-
tion.InACMInternationalWorkshoponSituationActivity&GoalAwareness. actionsonPatternAnalysisandMachineIntelligence33,12(2011),2521‚Äì2537.
https://doi.org/10.1145/2030045.2030065 https://doi.org/10.1109/tpami.2011.36
[17] KristenGrauman,AndrewWestbury,EugeneByrne,ZacharyChavis,Antonino [35] MajaStikic,DianeLarlus,andBerntSchiele.2009. Multi-graphBasedSemi-
Furnari,RohitGirdhar,JacksonHamburger,HaoJiang,MiaoLiu,andXingyu supervisedLearningforActivityRecognition.InIEEEInternationalSymposium
Liu.2022. Ego4D:AroundtheWorldin3,000HoursofEgocentricVideo.In onWearableComputers. https://doi.org/10.1109/ISWC.2009.24
IEEE/CVFConferenceonComputerVisionandPatternRecognition. https://doi. [36] MajaStikicandBerntSchiele.2009.ActivityRecognitionfromSparselyLabeled
org/10.1109/CVPR52688.2022.01842 DataUsingMulti-InstanceLearning.InSpringerInternationalSymposiumon
[18] AlexanderHoelzemann,JuliaL.Romero,MariusBock,KristofVanLaerhoven, Location-andContext-Awareness. https://doi.org/10.1007/978-3-642-01721-6_10
andQinLv.2023.Hang-TimeHAR:ABenchmarkDatasetforBasketballActivity [37] JavierS√°nchezP√©rez,EnricMeinhardt-Llopis,andGabrieleFacciolo.2013.TV-L1
RecognitionUsingWrist-WornInertialSensors. MDPISensors23,13(2023). OpticalFlowEstimation. ImageProcessingOnLine3(2013),137‚Äì150. https:
https://doi.org/10.3390/s23135879 //doi.org/10.5201/ipol.2013.26
[19] WillKay,Jo√£oCarreira,KarenSimonyan,BrianZhang,ChloeHillier,Sudheendra [38] JafarTanha,MaartenVanSomeren,andHamidehAfsarmanesh.2017. Semi-
Vijayanarasimhan,FabioViola,TimGreen,TrevorBack,PaulNatsev,Mustafa supervisedself-trainingfordecisiontreeclassifiers. InternationalJournalof
Suleyman,andAndrewZisserman.2017. TheKineticsHumanActionVideo MachineLearningandCybernetics8,1(2017),355‚Äì370. https://doi.org/10.1007/
Dataset.CoRRabs/1705.06950(2017). http://arxiv.org/abs/1705.06950 s13042-015-0328-7
[20] BulatKhaertdinov,EsamGhaleb,andStylianosAsteriadis.2021. Contrastive [39] ZacharyTeedandJiaDeng.2020.RAFT:RecurrentAll-PairsFieldTransforms
Self-supervisedLearningforSensor-basedHumanActivityRecognition.InIEEE forOpticalFlow.InEuropeanConferenceonComputerVision. https://doi.org/10.
InternationalJointConferenceonBiometrics. https://doi.org/10.1109/IJCB52358. 1007/978-3-030-58536-5_24
2021.9484410 [40] CatherineTong,JinchenGe,andNicholasD.Lane.2021.Zero-ShotLearningfor
[21] AnnaKukleva,HildeKuehne,FadimeSener,andJurgenGall.2019.Unsupervised IMU-BasedActivityRecognitionUsingVideoEmbeddings.ACMonInteractive,
LearningofActionClassesWithContinuousTemporalEmbedding.In2019 Mobile,WearableandUbiquitousTechnologies5,4(2021),1‚Äì23. https://doi.org/
IEEE/CVFConferenceonComputerVisionandPatternRecognition(CVPR).IEEE, 10.1145/3494995
LongBeach,CA,USA,12058‚Äì12066. https://doi.org/10.1109/CVPR.2019.01234 [41] RosauraG.VidalMata,WalterJ.Scheirer,AnnaKukleva,DavidCox,andHilde
[22] YongmouLi,DianxiShi,BoDing,andDongboLiu.2014.UnsupervisedFeature Kuehne.2021.JointVisual-TemporalEmbeddingforUnsupervisedLearningof
LearningforHumanActivityRecognitionUsingSmartphoneSensors.InSpringer ActionsinUntrimmedSequences.InIEEEWinterConferenceonApplicationsof
SecondInternationalConferenceonMiningIntelligenceandKnowledgeExploration. ComputerVision. https://doi.org/10.1109/wacv48630.2021.00128
https://doi.org/10.1007/978-3-319-13817-6_11 [42] AiguoWang,GuilinChen,CuijuanShang,MiaofeiZhang,andLiLiu.2016.
[23] XiaolongLiu,QimengWang,YaoHu,XuTang,ShiweiZhang,SongBai,and HumanActivityRecognitioninaSmartHomeEnvironmentwithStackedDe-
XiangBai.2022. End-To-EndTemporalActionDetectionWithTransformer. noisingAutoencoders.InWeb-AgeInformationManagement,ShaoxuSongandMariusBock,KristofVanLaerhoven,andMichaelMoeller
YongxinTong(Eds.).Vol.9998.SpringerInternationalPublishing,Cham,29‚Äì [44] ZhiluZhangandMertR.Sabuncu.2018. GeneralizedCrossEntropyLossfor
40. https://doi.org/10.1007/978-3-319-47121-1_3SeriesTitle:LectureNotesin TrainingDeepNeuralNetworkswithNoisyLabels.InAdvancesinNeuralIn-
ComputerScience. formationProcessingSystems. https://proceedings.neurips.cc/paper_files/paper/
[43] KangXia,WenzhongLi,ShiweiGan,andSangluLu.2023.TS2ACT:Few-Shot 2018/file/f2925f97bc13ad2852a7a551802feea0-Paper.pdf
HumanActivitySensingwithCross-ModalCo-Learning.ACMonInteractive, [45] YexuZhou,HaibinZhao,YiranHuang,TillRiedel,MichaelHefenbrock,and
Mobile,WearableandUbiquitousTechnologies7,4(2023),1‚Äì22. https://doi.org/ MichaelBeigl.2022.TinyHAR:ALightweightDeepLearningModelDesigned
10.1145/3631445 forHumanActivityRecognition.InACMInternationalSymposiumonWearable
Computers. https://doi.org/10.1145/3544794.3558467