Multi-Garment Customized Model Generation
YichenLiu,1 PenghuiDu,2 YiLiu2 QuanweiZhang3
1UniversityofChineseAcademyofSciences
2BeihangUniversity
3ZheJiangUniversity
Abstract Traditionalsubject-drivenimagegenerationmethodsfall
short for these tasks, as they emphasize holistic informa-
This paper introduces Multi-Garment Customized Model
tion of the conditions, such as appearance, structure, and
Generation, a unified framework based on Latent Diffusion
style, but fail to preserve clothing details. So far, research
Models (LDMs) aimed at addressing the unexplored task
focused on generating models based on given clothing and
of synthesizing images with free combinations of multiple
text prompts (like OMS (Chen et al. 2024) and StableGar-
pieces of clothing. The method focuses on generating cus-
tomizedmodelswearingvarioustargetedoutfitsaccordingto ment) has been limited to single garments, unable to meet
different text prompts. The primary challenge lies in main- thecommercialneedformodelsshowcasingmultiplecloth-
taining the natural appearance of the dressed model while ingitems.
preservingthecomplextexturesofeachpieceofclothing,en- To address this issue, we propose Multi-Garment Cus-
suringthattheinformationfromdifferentgarmentsdoesnot tomizedModelGeneration,anetworkarchitecturebasedon
interferewitheachother.Totacklethesechallenges,wefirst LDM that customizes models based on multiple reference
developedagarmentencoder,whichisatrainableUNetcopy
garments and text prompts. Inspired by reference mecha-
withsharedweights,capableofextractingdetailedfeaturesof
nisms in image-to-video tasks, we developed a semantic
garments in parallel. Secondly, our framework supports the
clothingencoderbasedontheReferenceUNetarchitecture,
conditionalgenerationofmultiplegarmentsthroughdecou-
pled multi-garment feature fusion, allowing multiple cloth- a trainable UNet replica with shared weights that can ex-
ingfeaturestobeinjectedintothebackbonenetwork,signifi- tract detailed features of clothing in parallel. Unlike previ-
cantlyalleviatingconflictsbetweengarmentinformation.Ad- ous attention concatenation operations, we adopt an atten-
ditionally,theproposedgarmentencoderisaplug-and-play tiondecouplingapproachtoinjectclothingfeaturesintothe
modulethatcanbecombinedwithotherextensionmodules imagegenerationprocess,preservingtheappearancedetails
suchasIP-AdapterandControlNet,enhancingthediversity andspatialinformationofeachgarment.Additionalseman-
andcontrollabilityofthegeneratedmodels.Extensiveexperi- tic conditions represent the categories of the clothing, en-
mentsdemonstratethesuperiorityofourapproachoverexist-
suring different types of garments are not confused during
ingalternatives,openingupnewavenuesforthetaskofgen-
the generation process. Furthermore, our attention reorder-
eratingimageswithmultiple-piececlothingcombinations.
ingoperationensuressmoothtransitionsatthejunctionsof
differentclothingitems,avoidingabruptseams.Ourseman-
tic clothing encoder is a plug-in module that can be com-
1Introduction
binedwithvariousbasemodelsorotherextensionmodules
In recent years, the field of image generation has seen (likeControlNetandIP-Adapter(Yeetal.2023)),specifying
transformativeadvancements,withLatentDiffusion Model the model’s pose, facial features, and style while maintain-
(LDM)-based methods achieving remarkable success in ingthedetailsofmultiplegarments.
text-to-image generation tasks. Many research efforts have Withthesemethods,ourmodeldemonstrateshighquality
attempted to incorporate control conditions beyond text, and controllability in multi-garment model image genera-
such as pose, densepose, sketches, and facial features, for tion.Ourcontributionsaresummarizedasfollows:
image generation. However, to date, few studies have fo-
cusedonimagegenerationconditionedonspecificclothing. • Weconstructedanovelframework,Multi-GarmentCus-
Thisisahighlypromisingarea,particularlywithsignificant tomized Model Generation, supporting text-prompted,
commercial value in e-commerce. Such tasks present two multi-garmentcontrollableimagegeneration.
key challenges: on the one hand, adjusting the generated • We proposed a semantic clothing encoder that injects
person’s pose, background, and facial features through text multiple garment features into the denoising process
promptsoradditionalcontrolconditions;ontheotherhand, through decoupled additive attention and applies atten-
maintainingthetextureandstyleofthetargetclothing. tion reordering to balance the features of different gar-
ments.
Copyright©2024,AssociationfortheAdvancementofArtificial
Intelligence(www.aaai.org).Allrightsreserved. • Our model’s performance, benchmarked against com-
4202
guA
9
]VC.sc[
1v60250.8042:viXraSelf Attention
(a)
K V person upper lower
Q person upper lower
(b)
K V person upper lower
Q person
(c)
K V person upper
Q person
K V person lower
Figure1:Theframeworkofourmethod.
petitors, shows state-of-the-art results, highlighting the when customizing images with multiple garments. Gener-
superiorityofourapproach. ally, finetuning-free methods encode reference images into
embeddings or image prompts without requiring additional
2RELATEDWORK finetuning. Thus, finetuning-free methods offer great flex-
ibility and have more potential for practical applications.
2.1ControllableImageGeneration ELITEproposesglobalandlocalmappingschemesbutsuf-
fers from limited fidelity. Instantbooth employs an adapter
In recent years, with the successful application of Latent
structuretrainedondomain-specificdatatoachievesubject-
DiffusionModels(LDMs)intext-to-imagegenerationtasks,
driven generation without finetuning. IP-Adapter designs a
controllable image generation has gained significant atten-
decoupled cross-attention mechanism that encodes images
tion. Many works have attempted to introduce spatial in-
intopromptsseparatefromthecross-attentionlayersoftext
formation into LDMs to enhance the controllability of the
features, achieving comparable performance to dedicated
models. ControlNet is one of the most effective methods
finetunedmodelsinasimplemanner.VersatileDiffusionex-
forcontrollingdiffusionmodels,integratingatrainablecopy
tendsexistingsingle-streamdiffusionpipelinesintoamulti-
of the UNet encoder block with zero initialization into the
taskmultimodalnetworktoachievecross-modalversatility.
originalUNet.T2I-Adapterproposesacompactnetworkde-
Magic Clothing uses a Reference UNet to extract features
signthatachievesthesamefunctionalityasControlNetbut
and implements feature fusion in the self-attention layer,
with a lighter model. Uni-ControlNet introduces a unified
but this method is limited to single garments. In this work,
frameworkthatallowsforflexiblecombinationsofdifferent
wefocusonmulti-garment-drivenportraitsynthesis,which
conditionalcontrolswithinasinglemodel,furtherreducing
requires the model to have the capability to retain details
trainingcosts.However,thespatialstructureinformationin-
across multiple garments, create according to diverse text
troduced by these methods only serves as auxiliary in the
prompts, and be applicable to out-of-domain data without
generationofmodelimagesanddoesnotmeettherequire-
finetuning.
mentsforproducingcustomizedimages.
3METHOD
2.2Subject-drivenHumanImageSynthesis
3.1Preliminary
Subject-drivenhumanimagesynthesisaimstogeneratetar-
get subject’s images using text prompts from given refer- LatentDiffusionModels(LDMs)havebeensuccessfullyap-
enceimages,withthekeychallengebeingtomaintainhigh- plied to text-to-image generation tasks. To add spatial con-
fidelity visualfeatures whilegenerating newimages in dif- ditioningcontroltopre-trainedLDMs,ControlNetintegrates
ferent contexts. This can be categorized into two main cat- trainable encoder blocks into the original UNet architec-
egories: test-time finetuning methods and finetuning-free ture. Meanwhile, T2I-Adapterproposes a compact network
methods. Test-time finetuning methods can produce satis- design that offers the same functionality as ControlNet but
factoryresultsbutrequiremoretimetoadapttoeachaspect with lower computational complexity. To further reduce
Garment
extractor
Denoising
UNet
Garment
extractortraining costs, Uni-ControlNetintroduces a unified frame- garments,makingitthebest-performingmethodamongthe
work that can handle different conditional controls flexibly three.
andincombinationwithinasinglemodel.
On the other hand, LDMs have also played a significant
4EXPERIMENTS
roleinimageediting.InstructPix2Pixre-trainstheUNetof
LDMs by adding extra input channels to the first convolu- Dataset
tional layer and fine-tuning it on a large dataset of image
editing examples, enabling the model to follow editing in- All experiments were conducted on DressCode, a multi-
structions. MasaCtrl transforms self-attention in diffusion garmentdatasetthatwascreatedbysplittingadditionalgar-
models into mutual self-attention, achieving consistent im- ments from a single-garment dataset, forming triplet data
age generation and complex non-rigid image editing with- pairs.
out additional training costs. InfEdit achieves coherent and
faithfulsemanticchanges,bothrigidandnon-rigid,through ImplementationDetails
denoisingdiffusionconsistencymodelsandattentioncontrol
mechanisms. In our experiments, we initialized the weights of our gar-
Inthispaper,weleveragethepowerfulcapabilitiesofpre- ment extractor by inheriting the pre-trained weights of the
trainedLDMsintext-to-imagegenerationandintroducean UNetcomponentfromStableDiffusionv1.5.Wefine-tuned
additional clothing extractor to achieve clothing-driven im- only these weights while keeping the weights of the other
agesynthesis. modules unchanged. Our model underwent pre-training on
single-garment data and subsequent fine-tuning on multi-
3.2GarmentEncoder garmentdatafromtheDressCodedataset,whichhasanim-
We propose a semantic-aware garment encoder that iden- ageresolutionof768×576.Correspondingcaptionsforthe
tifies the category of the extracted features based on pre- imageswereobtainedfromtheBLIP(Li,Li,andHoi2024)
specified garment categories as prompts, enabling effective model.WeutilizedanAdamWoptimizerwithafixedlearn-
matchingofinputgarmentstobodyparts. ingrateof5e-5.Themodelwastrainedfor180epochswith
We also utilize classifier-free guidance for multiple gar- a batch size of 6 on a single NVIDIA V100 GPU. During
ments, which helps the diffusion model achieve a balance inference,weemployedtheUniPCsamplerfor20sampling
betweensamplequalityanddiversitythroughjointtraining stepstogenerateimages.
ofconditionalandunconditionalmodels.
Ourmodelisdesignedasaplug-and-playmodulethatcan Resultcomparison
be combined with IP-Adapter and ControlNet, and it sup-
portsLoRAfine-tuningforbaseadaptercompatibility. It can be observed that both the naive settings and the
Our training strategy involves pre-training on a single- Attention Concatenate methods fail to adequately preserve
garmentsubsetoftheDressCodedataset,followedbytrain- the detailed information of multiple clothing items. In
ingonamulti-garmentdataset. contrast, the Attention Addition method yields the best
results. Furthermore, our approach, when combined with
3.3AttentionfusionBlock textprompts,enablesthegenerationofcustomizedavatars.
3.3.1NaiveSettings
We simply concatenate the garment features with the noise
CONCLUSION
features before performing self-attention operations. This
process causes the information from different garments to
In conclusion, this paper presents Multi-Garment Cus-
influenceeachother,failingtofullyexploitthegarmentin-
tomizedModelGeneration,anovelunifiedframeworkbased
formation.
onLatentDiffusionModels(LDMs)designedtosynthesize
images with free combinations of multiple pieces of cloth-
3.3.2AttentionConcatenate
ing. By addressing the challenges of maintaining natural
Weonlyconcatenatethegarmentfeatureswiththenoisefea- appearances and complex textures while preventing infor-
turesforthekeys(K)andvalues(V),thencomputetheop- mationinterference, ourmethodintroduces atrainable gar-
eration using the noise query (Q) with the concatenated K ment encoder and decoupled multi-garment feature fusion.
andV.Duringthisprocess,themultiplegarmentsstillinflu- These innovationsenable theparallel extractionof detailed
ence each other to some extent, such as after the Q and K garment features and the conditional generation of multi-
operations. ple garments, significantly reducing conflicts between gar-
mentinformation.Thegarmentencoderisaflexibleandex-
3.3.3AttentionAddition
tendable module that can integrate with other components
We do not concatenate the garment features with the noise likeIP-AdapterandControlNet,enhancingthediversityand
features;instead,weseparatelycomputetheoperationsbe- controllability of generated images. Extensive experiments
tweenthenoisefeaturesandthekeys(K)andvalues(V)of validatethesuperiorityofourapproachoverexistingmeth-
each garment, and finally sum them up. This process com- ods,pavingthewayforadvancedapplicationsinthefieldof
pletely decouples the attention computations for different multi-garmentimagesynthesis.References
Chen,W.;Gu,T.;Xu,Y.;andChen,C.2024. MagicCloth-
ing: Controllable Garment-Driven Image Synthesis. arXiv
preprintarXiv:2404.09512.
Li,D.;Li,J.;andHoi,S.2024. Blip-diffusion:Pre-trained
subjectrepresentationforcontrollabletext-to-imagegenera-
tionandediting. AdvancesinNeuralInformationProcess-
ingSystems,36.
Ye,H.;Zhang,J.;Liu,S.;Han,X.;andYang,W.2023. IP-
Adapter: Text Compatible Image Prompt Adapter for Text-
to-ImageDiffusionModels.