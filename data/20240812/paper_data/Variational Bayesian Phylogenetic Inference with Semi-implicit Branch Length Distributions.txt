Variational Bayesian Phylogenetic Inference with Semi-implicit
Branch Length Distributions
Tianyu Xie1, Frederick A. Matsen IV2, Marc A. Suchard3, Cheng Zhang4,∗
Abstract
Reconstructing the evolutionary history relating a collection of molecular sequences is the main
subject of modern Bayesian phylogenetic inference. However, the commonly used Markov chain
Monte Carlo methods can be inefficient due to the complicated space of phylogenetic trees, es-
pecially when the number of sequences is large. An alternative approach is variational Bayesian
phylogenetic inference (VBPI) which transforms the inference problem into an optimization prob-
lem. Whileeffective,thedefaultdiagonallognormalapproximationforthebranchlengthsofthetree
used in VBPI is often insufficient to capture the complexity of the exact posterior. In this work, we
propose a more flexible family of branch length variational posteriors based on semi-implicit hierar-
chicaldistributionsusinggraphneuralnetworks. Weshowthatthissemi-implicitconstructionemits
straightforward permutation equivariant distributions, and therefore can handle the non-Euclidean
branchlengthspaceacrossdifferenttreetopologieswithease. Todealwiththeintractablemarginal
probability of semi-implicit variational distributions, we develop several alternative lower bounds
for stochastic optimization. We demonstrate the effectiveness of our proposed method over baseline
methods on benchmark data examples, in terms of both marginal likelihood estimation and branch
length posterior approximation.
Keywords: Bayesianphylogenetics,variationalinference,semi-implicitdistributions,lowerbounds.
1 Introduction
Bayesian phylogenetic inference is a fundamental statistical framework in molecular evolution and sys-
tematicsthataimstoreconstructtheevolutionaryhistoriesamongtaxaorotherbiologicalentities,with
a wide range of applications including genomic epidemiology (du Plessis et al., 2021) and conservation
genetics (DeSalle & Amato, 2004). Given observed biological sequences (e.g., DNA, RNA, protein) and
a model of molecular evolution, Bayesian phylogenetic inference seeks to estimate the posterior distri-
bution of phylogenetic trees. The exact computation of this posterior is intractable as it would require
integrating out all possible tree topologies and branch lengths. Thus, practitioners use approximation
methods. A typical approach is Markov chain Monte Carlo (MCMC) (Yang & Rannala, 1997; Mau
et al., 1999; Larget & Simon, 1999; Ronquist et al., 2012) that relies on efficient proposal mechanisms to
explore the tree space. As the tree space, however, contains both continuous and discrete components
1SchoolofMathematicalSciences,PekingUniversity,Beijing,100871,China. Email: tianyuxie@pku.edu.cn
2Computational Biology Program, Fred Hutchinson Cancer Research Center, Department of Genome Sciences and
DepartmentofStatistics,UniversityofWashington,Seattle,WA98195,USA.Email: matsen@fredhutch.org
3DepartmentofBiostatistics,DepartmentofBiomathematics,andDepartmentofHumanGenetics,UniversityofCali-
fornia,LosAngeles,CA90095,USA.Email: msuchard@ucla.edu
4SchoolofMathematicalSciencesandCenterforStatisticalScience,PekingUniversity,Beijing,100871,China. Email:
chengzhang@math.pku.edu.cn
∗Correspondingauthor
1
4202
guA
9
]LM.tats[
1v85050.8042:viXra(e.g., the branch lengths and the tree topologies), phylogenetic posteriors are often complex multimodal
distributions. Further,treeproposalsusedinMCMCareoftenlimitedtolocalmodificationsthatleadto
low exploration efficiency, and this makes Bayesian phylogenetic inference a challenging task for MCMC
algorithms (Lakner et al., 2008; Höhna & Drummond, 2012; Whidden & Matsen IV, 2015; Dinh et al.,
2017; Hassler et al., 2023).
An alternative approximate Bayesian inference method is variational inference (VI) (Jordan et al.,
1999; Blei et al., 2016). Unlike MCMC, VI seeks the closest member from a family of candidate distri-
butions(i.e., thevariationalfamily)totheposteriordistributionbyminimizingsomestatisticaldistance
criterion, usually the Kullback-Leibler (KL) divergence. By converting the inference problem into an
optimization problem, VI tends to be faster and easier to scale up to large data (Blei et al., 2016).
Unlike MCMC methods that are asymptotically unbiased, variational approximations are often biased,
especially when the variational family of distributions is insufficiently flexible. The success of VI, there-
fore, relies on the construction of expressive variational families and efficient optimization procedures.
While classical mean-field VI requires conditionally conjugate models and often suffers from limited
approximation power, much progress has been made in recent years to allow for more generic model-
agnostic optimization methods (Ranganath et al., 2014) and more flexible variational families that have
tractable densities (Rezende & Mohamed, 2015; Dinh et al., 2016; Kingma et al., 2016; Papamakarios
et al., 2021). Moreover, variational families can be further expanded by allowing implicit models which
haveintractabledensitiesbutareeasytosamplefrom(Huszár,2017). Theseimplicitmodelsareusually
constructedbyeitherpushingforwardasimplebasedistributionthroughaparameterizedmap,i.e.,deep
neural networks (Tran et al., 2017; Mescheder et al., 2017; Shi et al., 2018; Song et al., 2019) or using
a semi-implicit hierarchical architecture (Yin & Zhou, 2018; Titsias & Ruiz, 2019; Sobolev & Vetrov,
2019).
Until recently, VI has received limited attention in the field of phylogenetics. For a fixed tree topol-
ogy, VI-based approaches have been developed to approximate the posterior of continuous parameters
via coordinate ascent (Dang & Kishino, 2019) and to estimate marginal likelihoods for model compar-
ison (Fourment & Darling, 2019). However, when taking the tree topology as also random, the design
of variational methods can be highly nontrivial, partially due to the absence of an appropriate family of
distributions on phylogenetic trees. Zhang & Matsen IV (2019) took the first step in this direction by
developingageneralframeworkforvariationalBayesianphylogeneticsinference(VBPI),wheretheyused
a product of a tree topology model and a branch length model to provide variational approximations.
They originally chose the tree topology model to be a subsplit Bayesian network (SBN), a powerful
probabilistic graphical model specifically designed for distributions over tree topologies. Although effec-
tive, SBNs require a pre-selected sample of candidate tree topologies that confines their support to a
subspaceofallpossibletreetopologies. Manyotherapproacheshavebeenintroducedrecently(Koptagel
et al., 2022; Xie & Zhang, 2023; Mimori & Hamada, 2023; Zhou et al., 2023) that remove this constraint
and hence may provide more flexible distributions over the entire tree topology space. The conditional
branchlengthmodelisoftenasimplediagonallognormaldistributionthatisamortizedovertreetopolo-
gies via either hand-engineered heuristic features (Zhang & Matsen IV, 2019) or learnable topological
features (Zhang, 2023). Although there were follow-up works for improved branch length models, e.g.,
VBPIwithnormalizingflows(Zhang,2020),therequirementofpermutationequivarianttransformations
and explicit density adds to the difficulty of architecture design and may also limit the approximation
accuracy, especially for complicated real data branch length posteriors.
In this work, we introduce a semi-implicit hierarchical construction for the branch length model
in VBPI, with an emphasis on unrooted models. We show that distributions under this construction
2can be easily made permutation invariant; therefore, they are naturally suitable for modeling branch
lengths across different tree topologies. To address the intractable density of semi-implicit variational
distributions, we adapt ideas from semi-implicit variational inference (SIVI) (Yin & Zhou, 2018) and
importance weighted hierarchical variational inference (IWHVI) (Sobolev & Vetrov, 2019) to design
alternative surrogate objectives for optimization. Our synthetic and real-world experiments show that
VBPI with semi-implicit branch length distributions (VBPI-SIBranch) outperforms baseline methods in
both marginal likelihood estimation and branch length posterior approximation.
The rest of the paper is organized as follows. In Section 2, we introduce the essential ingredients of
SIVI methods, phylogenetic models, and the variational Bayesian phylogenetic inference framework. In
Section 3, we present our semi-implicit branch length variational distributions, describe two surrogate
objective functions for optimization, and prove their statistical properties. In Section 4, we conduct
experiments to compare VBPI-SIBranch to baseline methods in terms of both marginal likelihood esti-
mation and branch length approximation. We conclude with a discussion in Section 5.
2 Background
2.1 Semi-implicit Variational Inference
GivenobserveddataDandrandomvariablesxthatcharacterizethegenerationofD,VIreformulatesthe
BayesianinferenceofaposteriordistributionP(x|D)∝P(x,D)asanoptimizationproblembyminimiz-
ing the distance between P(x|D) and a parametrized variational distribution Q (x) which is commonly
θ
assumed to have tractable density (Jordan et al., 1999; Blei et al., 2016). The most commonly used dis-
tance is the reversed KL divergence defined as D (Q (x)∥P(x|D))=E [logQ (x)−logP(x|D)].
KL θ Qθ(x) θ
AstheposteriordistributionP(x|D)isoftenonlyknownuptoaconstantP(D),inpracticewemaximize
the evidence lower bound (ELBO) instead, defined as
(cid:18) P(x,D)(cid:19)
L(θ)=E log =logP(D)−D (Q (x)∥P(x|D))≤logP(D). (1)
Qθ(x) Q (x) KL θ
θ
Another popular objective function for VI is the multi-sample lower bound (Burda et al., 2015; Mnih &
Rezende, 2016)
1 XK P(xk,D)!
LK(θ)=E log ≤logP(D), (2)
Qθ(x1:K) K Q (xk)
θ
k=1
where one averages over multiple samples with Q (x1:K) = QK Q (xk), and we will use K for the
θ k=1 θ
number of particles in the rest of the paper.
Beyond the explicit assumptions of Q (x), semi-implicit variational inference (SIVI) (Yin & Zhou,
θ
2018) assumes a more flexible variational family defined hierarchically as
ˆ
Q (x)= Q (x|z)Q (z)dz, (3)
θ θ θ
where z is a latent variable, Q (x|z) is required to be explicit and Q (z) can be implicit. Compared
θ θ
to standard VI, the above semi-implicit hierarchical construction allows a much richer family that can
capture complicated correlation between parameters (Yin & Zhou, 2018). However, the ELBO L(θ)
used in standard VI is no longer suitable for SIVI as Q (x) is intractable. The variational family (3) is
θ
3instead fitted by maximizing the semi-implicit lower bound (SILB; Yin & Zhou (2018, equation 9))
!
P(x,D)
E log , (4)
Qθ(x,z0)Qθ(z1:J) 1 PJ Q (x|zj)
J+1 j=0 θ
whereQ (x,z0)=Q (x|z0)Q (z0), Q (z1:J)=QJ Q (zj), andJ isthenumberofextrasamplesfor
θ θ θ θ j=1 θ
an importance-sampling-based estimator of Q (x). Here, we put z0 together with x to emphasize that
θ
x depends on z0. Noticing that samples from Q (z) might not be informative for estimating Q (x),
θ θ
Sobolev & Vetrov (2019) use an auxiliary reverse model R (z|x) as the importance distribution, and
α
maximize the following importance weighted lower bound (IWLB; Sobolev & Vetrov (2019, equation 4))
 
P(x,D)
E Qθ(x,z0)Rα(z1:J|x)log 
1 PJ
Qθ(x,zj), (5)
J+1 j=0 Rα(zj|x)
where R (z1:J|x)=QJ R (zj|x). Here, Q (z) and R (z|x) need to be explicit.
α j=1 α θ α
2.2 Phylogenetic Trees
Given N observed taxa, an important goal in phylogenetic inference is to estimate their evolutionary
history, which is often described as a phylogenetic tree that includes a tree topology τ and a vector of
non-negative branch lengths q for the edges on τ.
The tree topology τ is a bifurcating tree graph with a node set V(τ) and an edge set E(τ). There
are two types of nodes in V(τ): nodes with degree one are called leaf nodes that represent the existing
(observed) taxa; nodes with degree two or three are called internal nodes that represent the ancient
(unobserved)taxa. Forarootedtreetopology,thereexistsauniquenodewithdegreetwocalledtheroot
node (or the root for simplicity), and the edges in E(τ) are directed away from the root node. For an
unrooted tree topology, all the nodes in V(τ) have one or three degrees, and all the edges in E(τ) are
undirected. Furthermore,anunrootedtreetopologycanbeconvertedtoarootedone(andviceversa)by
placingarootnodeonanedge(removingtherootnodeandconnectingitstwoneighbors). Asmentioned
above, the focus of our work is on unrooted phylogenetic trees (rather than rooted phylogenetic time
trees), however, our algorithm can be easily adapted to rooted phylogenetic trees. In this article, we use
“tree topology” for an unrooted tree topology unless otherwise specified.
For each edge e = (u,v) ∈ E(τ), there is a non-negative scalar q (or equivalently, q ) called the
uv e
branch length. The branch length q quantifies the amount of evolution along edge e = (u,v), i.e., the
uv
expected number of character substitutions between the two neighboring nodes u and v. The vector
q =[q ] contains all the branch lengths associated with tree topology τ.
e e∈E(τ)
2.3 Bayesian Phylogenetic Inference
Theleafnodesofaphylogenetictreecorrespondtotheobservedtaxa,whosealignedmolecularsequences
is represented as a matrix Y = {Y ,Y ,...,Y } ∈ ΩN×S. Here, Ω is the alphabet set of characters
1 2 N
(e.g., nucleotides: A, C, G, T) that comprise the sequences, and S is the character sequence length.
For each 1 ≤ s ≤ S, Y denotes the observed characters of all taxa at a single aligned position, also
s
called a site, that are homologous, meaning that they all arose from a common character somewhere
on the phylogenetic tree through a process of replication and substitution along its edges. The goal of
phylogenetic inference is then to reconstruct (τ,q) based on the observed sequence data Y.
4Given a rooted tree topology τ and branch lengths q, the generative process of the observed data
Y can be described as follows. Starting from the root node, the evolution along the edges of the tree
is governed by a substitution model, often a continuous-time Markov chain (CTMC) that governs the
transition probabilities among the characters from a parent node to its child node (Jukes et al., 1969;
Tavaréetal.,1986). LetQbethetransitionratematrix. Thetransitionprobabilityalonganedge(u,v)
at site s is P (q ) = (exp(q Q)) , where as is the character assignment of node u at site s.
as uas v uv uv as u,as v u
Assuming that each site evolves independently and identically, the phylogenetic likelihood of observing
Y is obtained by summing out all the possible states of internal nodes as
S S
Y YX Y
P(Y|τ,q)= P(Y |τ,q)= η(as) P (q ), (6)
s r as uas v uv
s=1 s=1 as (u,v)∈E(τ)
where r represents the root node, as ranges over all extensions of Y to the internal nodes, and η is
s
a prior distribution on the root states. The phylogenetic likelihood (6) can be efficiently evaluated by
Felsenstein’s pruning algorithm (Felsenstein, 2004).
For an unrooted tree topology, one can also obtain a valid phylogenetic likelihood from equation (6)
by placing a root node r on an arbitrary edge at any position. In fact, equation (6) does not depend on
thelocationoftherootnodeaslongastheCTMCistime-reversibleandoneassumesthattherootprior
is the stationary distribution of Q (Felsenstein, 1981). This is also a common choice of η in practice.
Given a prior distribution P(τ,q) over the space of phylogenetic trees, the joint posterior density
takes the following form
P(Y|τ,q)P(τ,q)
P(τ,q|Y)= ∝P(Y|τ,q)P(τ,q). (7)
P(Y)
A common choice of the prior consists of a uniform distribution over tree topologies and independent
exponential distributions over branch lengths (Ronquist et al., 2012).
2.4 Variational Bayesian Phylogenetic Inference
Toestimatethe phylogeneticposteriorinVI, VBPIpositsa parameterized variationalfamily Q (τ,q)
ϕ,ψ
that is a product of a tree topology model Q (τ) and a branch length model Q (q|τ). The variational
ϕ ψ
approximation is then obtained by maximizing the multi-sample lower bound (MLB)
1 XK P(Y|τk,qk)P(τk,qk)!
LK(ϕ,ψ)=E log , (8)
Qϕ,ψ(τ1:K,q1:K) K Q (τk)Q (qk|τk)
ϕ ψ
k=1
whereQ (τ1:K,q1:K)≡QK Q (τk,qk). Theoptimizationofequation(8)isdonethroughstochas-
ϕ,ψ k=1 ϕ,ψ
ticgradientascent(SGA),wherethestochasticgradientsfortreetopologyparametersandbranchlength
parameters are obtained by the VIMCO/RWS estimator (Mnih & Rezende, 2016; Bornschein & Bengio,
2015)andthereparameterizationtrick(Kingma&Welling,2014)respectively. ComparedtotheELBO,
the MLB (8) enables efficient variance-reduced gradient estimators and encourages exploration over the
vast and multimodal tree space. However, as a large K may also reduce the signal-to-noise ratio and
deteriorate the training of variational parameters (Rainforth et al., 2019), a moderate K is suggested in
practice (Zhang & Matsen IV, 2024).
ThetreetopologymodelQ (τ)canbeparametrizedbySBNs(Zhang&MatsenIV,2018)asfollows.
ψ
A non-empty subset of the leaf nodes is called a clade with a total order ≻ (e.g., lexicographical order)
5on all clades. An ordered clade pair (W,Z) satisfying W ∩Z = ∅ and W ≻ Z is called a subsplit. An
SBN is then defined as a Bayesian network whose nodes take subsplit values or singleton clade values
that describe the local topological structures of tree topologies. For a rooted tree topology, one can find
its corresponding node assignment of SBNs by starting from the root node, iterating towards the leaf
nodes, and gathering all the visited parent-child subplit pairs. The SBN-based probability of a rooted
tree topology τ then takes the form
Y
p (T =τ)=p(S =s ) p(S =s |S =s ), (9)
sbn 1 1 i i πi πi
i>1
where S denotes the subsplit- or singleton-clade-valued random varaibles at node i (node 1 is the root
i
node), π istheindexsetoftheparentsofnodeiand{s } isthecorrespondingnodeassignment. For
i i i≥1
unrooted tree topologies, we can also define their SBN-based probabilities by viewing them as rooted
tree topologies with unobserved roots and summing out the root positions. For VBPI, the conditional
probabilitiesinSBNsareoftenparameterizedbasedonasubsplitsupportestimatedfromfastbootstrap
or MCMC tree samples (Minh et al., 2013; Zhang & Matsen IV, 2024). See more details of SBNs in
Appendix A.
As the branch lengths are non-negative, the branch length model Q (q|τ) is often taken to be a
ψ
diagonal lognormal distribution
Y
Q (q|τ)= pLognormal(q |µ(e,τ),σ(e,τ)), (10)
ψ e
e∈E(τ)
whereµ(e,τ)andσ(e,τ)arethemeanandstandarddeviationparametersofthelognormaldistribution,
and are amortized over the tree topologies via shared local structures (Zhang & Matsen IV, 2019) or
learnablenodefeatures(Zhang,2023). However,thesimplediagonallognormalvariationalapproximation
(10) maybe too simple to capture the complicated posterior distributions of branch lengths due to the
hierarchical structure of tree topologies. Although Zhang (2020) proposed to parameterize Q (q|τ)
ψ
with normalizing flows (VBPI-NF), the requirement of invariant and explicit distribution confines the
flexibility of these branch length models.
3 Methodology
In this section, we present a more flexible family of branch length distributions for VBPI, featuring a
hierarchical semi-implicit structure, which we call VBPI-SIBranch. We begin by outlining the construc-
tion of semi-implicit branch length distributions with learnable topological features via powerful graph
neural networks (GNNs) (Kipf & Welling, 2017; Gilmer et al., 2017). These distributions exhibit natu-
ral permutation equivariance, making them well-suited for branch length approximation across various
tree topologies. Note that the branch lengths are defined upon the edges and thus do not naturally
map across different tree topologies. We then develop efficient surrogate objective functions, provide
theoretical guarantees, and illustrate their application in the training process.
6GNNs
MLPμ
MLPσ
e
Q ψ(z|τ) i.i.d. samples e 21
=N(0,I) e
3
e
4
e leaf node internal node edge
5
topological node embedding edge feature hidden variable branch length distribution
Figure 1: An overview of VBPI-SIBranch for a five-leaf phylogenetic tree. We begin with topological
nodeembeddings(Zhang,2023)(upperleft)andapplyGNNstoobtaintheedgefeatures. Thesefeatures,
joined together with the i.i.d. hidden variables, are finally fed into the MLPµ and MLPσ to form the
parameters of branch length distributions.
3.1 Semi-implicit Branch Length Distributions
To improve the expressiveness of branch length models, we introduce the following semi-implicit hierar-
chical construction for branch length distributions
q ∼Q (q|τ,z), z ∼Q (z|τ), (11)
ψ ψ
where z is a hidden variable with prior distribution Q (z|τ) (i.e., the mixing distribution) conditioned
ψ
on the tree topology τ, and Q (q|τ,z) is the conditional branch length distribution. Both Q (z|τ)
ψ ψ
andQ (q|τ,z)areassumedtobereparameterizable, whileQ (z|τ)isgenerallyimplicitandQ (q|τ,z)
ψ ψ ψ
is required to be explicit. Integrating out the hidden variable z, we have the marginal variational
distribution of branch lengths
ˆ
Q (q|τ)= Q (q|τ,z)Q (z|τ)dz. (12)
ψ ψ ψ
Thisaugmentedhiddenvariableintroducesadditionalflexibilitytothemodelingofbranchlengths. Note
thatequation(12)degeneratestotheexplicitbranchlengthdistributioninvanillaVBPIwhenthemixing
distribution z ∼Q (z|τ) collapses to a Dirac measure.
ψ
For a given tree topology τ, the distribution of its associated branch lengths q should not depend
on the edge orderings on E(τ). This naturally requires the branch length model to be permutation
invariant (Definition 1). In what follows, we show that the semi-implicit hierarchical construction (11)
allows permutation invariant construction of the marginal branch length distributions (Proposition 1).
Definition 1 (Permutation Invariance). For a tree topology τ, let π :E(τ)→E(τ) be a specific permu-
tation function on the edges of τ and q =[q ] . The branch length distribution Q (q|τ) is said
π π(e) e∈E(τ) ψ
to be permutation invariant, if for any permutation function π, we have Q (q |τ)=Q (q|τ).
ψ π ψ
Proposition 1. Suppose z =[z ] and z =[z ] . If Q (q|τ,z) and Q (z|τ) in (11) are
e e∈E(τ) π π(e) e∈E(τ) ψ ψ
permutation invariant, i.e., Q (q |τ,z )=Q (q|τ,z), Q (z |τ)=Q (z|τ), then the marginal branch
ψ π π ψ ψ π ψ
length distribution Q (q|τ) is also permutation invariant.
ψ
Proof. Let L be the permutation matrix corresponding to π with |det(L )| = 1. By the permutation
π π
invariance ofQ (q|τ,z), weknow thatQ (q |τ,z )=Q (q|τ,z). This, together withthepermutation
ψ ψ π π ψ
7invariance of Q (z |τ), yields
ψ π
ˆ ˆ
Q (q |τ)= Q (q |τ,z )Q (z |τ)dz = Q (q|τ,z)Q (z|τ)|det(L )|dz =Q (q|τ),
ψ π ψ π π ψ π π ψ ψ π ψ
which implies that Q (q|τ) is a permutation invariant distribution.
ψ
3.2 Graph Neural Networks for Semi-implicit Branch Length Distributions
BoththeinvariantconditionalbranchlengthdistributionQ (q|τ,z)andthemixingdistributionQ (z|τ)
ψ ψ
can be parametrized by GNNs. We will first introduce the topological node embeddings and then give a
concrete example for constructing semi-implicit branch length distributions with GNNs.
Topological Node Embeddings Zhang (2023) introduces topological node embedding for phyloge-
netic trees that allows integration of deep learning methods for structural representation learning of
phylogenetic trees for downstream tasks (Xie & Zhang, 2023). For a tree topology τ, the set of topo-
logical node embeddings is defined as f(τ) = {f ∈ RN;u ∈ V(τ)} which assigns an embedding vector
u
for each node. To obtain the topological node embeddings, we first assign one-hot embedding vectors to
the leaf nodes and then compute the embedding vectors for internal nodes by minimizing the Dirichlet
energy
X
ℓ(f,τ):= ||f −f ||2 (13)
u v
(u,v)∈E(τ)
thatcanbeanalyticallysolvedbyalinear-timetwo-passalgorithm(Zhang,2023). Thefollowingtheorem
reveals the representation power of topological node embeddings.
Theorem 1 (Identifiability; Zhang (2023)). Let Vo(τ) be the set of internal nodes of τ and fo(τ) =
{f ;u ∈ Vo(τ)} denote the topological node embeddings of Vo(τ). For two tree topologies τ and τ ,
u 1 2
τ =τ if and only if fo(τ )=fo(τ ).
1 2 1 2
Learnable Node Features To form learnable node features (initialized as the topological node fea-
tures {f(0);u ∈ V(τ)}) that encode the topological information of τ, we utilize GNNs with message
u
passing steps where the node features are updated by aggregating the information from their neigh-
borhoods in a convolutional manner (Gilmer et al., 2017). Concretely, the l-th message passing step is
implemented as
(cid:16)n o(cid:17)
m(l+1) = AGG(l) f(l);v ∈N(u) ,
u v
(cid:16) (cid:17)
f(l+1) = UPDATE(l) f(l),m(l+1) ,
u u u
where AGG(l) and UPDATE(l) are the aggregation function and update function in the l-th step
parametrized by neural networks. After L message passing steps, {f(L);u ∈ V(τ)} are fed into a
u
multi-layer perceptron (MLP), i.e.,
(cid:16) (cid:17)
h =MLP f(L) ,
u u
which outputs the learnable node features h(τ)={h ;u∈V(τ)}.
u
Semi-implicit Construction Let g be a permutation invariant function (e.g., sum). We first trans-
form the learnable node features into edge features {h ;e ∈ E(τ)} with h = g({h ,h }) where u and
e e u v
v are the two neighboring nodes of e. Let z be the corresponding hidden variable for edge e, and the
e
z = [z ] follows the mixing distribution Q (z|τ). These features are then concatenated to form
e e∈E(τ) ψ
8Algorithm 1: VBPI-SIBranch with MSILB
Input: Observed sequences Y ∈ΩN×S; initialized parameters ϕ,ψ.
while not converged do
τ1,...,τK ← independent samples from the current tree topology approximating
distribution Q (τ);
ϕ
for k =1,...,K do
zk,0,...,zk,J ← independent samples form the current mixing distribution Q (z|τk);
ψ
qk ← a sample from the current conditional branch length distribution Q (q|τk,zk,0);
ψ
Calculate the conditional probabilities Q (qk|τk,zk,j) for 0≤j ≤J;
ψ
end
gˆ← the estimate of the gradient ∇ LK,J(ϕ,ψ);
ϕ,ψ
ϕ,ψ ← Updated parameters using gradient estimate gˆ.
end
the mixing edge features {h¯ = h ∥z ;e ∈ E(τ)}, where ∥ means vector concatenation along the edge
e e e
featureaxis. TheconditionalbranchlengthdistributionQ (q|τ,z)inequation(11)takestheform(i.e.,
ψ
a diagonal lognormal distribution)
Y
Q (q|τ,z)= pLognormal(q |µ(e,τ,z),σ(e,τ,z)),
ψ e
e∈E(τ)
where the mean and standard deviation parameters are parametrized using MLPs as follows:
µ(e,τ,z)=MLPµ(cid:0) h¯ (cid:1)
,
σ(e,τ,z)=MLPσ(cid:0) h¯ (cid:1)
,
e e
and ψ are all the learnable parameters in this conditional branch length distribution construction. Al-
though the mixing distribution Q (z|τ) can also be parameterized using learnable node features of τ,
ψ
here we use the simple standard Gaussian distribution for Q (z|τ) which ignores the dependency on τ
ψ
for simplicity.
3.3 Multi-sample Semi-implicit Lower Bound for VBPI-SIBranch
Due to the semi-implicit construction of branch length distributions, the MLB LK(ϕ,ψ) in equation
(8) is no longer tractable. However, we can use a multi-sample extension of the SILB in Yin & Zhou
(2018) for training. Letting Q (τ) be the variational distribution over tree topologies, the multi-sample
ϕ
semi-implicit lower bound (MSILB) is defined as
1 XK P(Y|τk,qk)P(τk,qk) !
LK,J(ϕ,ψ)=E QK k=1Qϕ,ψ(τk,qk,zk,0)E QK k=1Qψ(zk,1:J|τk)log
K k=1Q ϕ(τk) J+1 1PJ j=0Q ψ(qk|τk,zk,j)
,
(14)
where Q (τ,q,z) = Q (q|τ,z)Q (z|τ)Q (τ) and Q (zk,1:J|τk) = QJ Q (zk,j|τk). In fact, the
ϕ,ψ ψ ψ ϕ ψ j=1 ψ
above MSILB is a lower bound of the MLB LK(ϕ,ψ), as proved in Theorem 2.
Theorem 2. The MSILB LK,J(ϕ,ψ) in equation (14) is a lower bound of LK(ϕ,ψ) in equation (8),
and is an increasing function of J, i.e., LK,J(ϕ,ψ) ≤ LK,J+1(ϕ,ψ) ≤ LK(ϕ,ψ), ∀J. Moreover, it is
asymptotically unbiased, i.e., lim LK,J(ϕ,ψ)=LK(ϕ,ψ).
J→∞
Thegradientofthesurrogatefunction(14)w.r.t.ϕandψ canbeestimatedbytheVIMCOestimator
and a reparameterization trick respectively. Therefore, the MSILB in equation (14) can be maximized
9the same way as in Zhang & Matsen IV (2019). We summarize the VBPI-SIBranch approach with
MSILB in Algorithm 1.
3.4 Multi-sample Importance Weighted Lower Bound for VBPI-SIBranch
TheMSILBLK,J(ϕ,ψ)forVBPI-SIBranchreliesonsampleszk,1:J fromthemixingdistributionQ (z|τk)
ψ
to estimate the marginal densities of the branch length sample qk, for 1 ≤ k ≤ K. However, these un-
informed samples may miss the high posterior domain of Q (z|τk,qk) and become less efficient in
ϕ,ψ
high-dimensional settings, e.g., conditional branch length distributions Q (qk|τk,zk,j) for 1 ≤ j ≤ J
ψ
can be close to zero. Similarly to Sobolev & Vetrov (2019), one may employ an auxiliary reverse model
R (z|τ,q) as an importance distribution that can adapt to the high posterior domain automatically.
ξ
More precisely, we consider the following multi-sample importance weighted lower bound (MIWLB)
LK w,J(ϕ,ψ,ξ)=E
QK
k=1Qϕ,ψ(τk,qk,zk,0)E
QK
k=1Rξ(zk,1:J|τk,qk)
  (15)
1 XK P(Y|τk,qk)P(τk,qk)
log  ,
K Q (τk) 1 PJ Qψ(qk|τk,zk,j)Qψ(zk,j|τk)
k=1 ϕ J+1 j=0 Rξ(zk,j|τk,qk)
where Q (τ,q,z) = Q (q|τ,z)Q (z|τ)Q (τ), R (zk,1:J|τk,qk) = QJ R (zk,j|τk,qk), and “w” is
ϕ,ψ ψ ψ ϕ ξ j=1 ξ
the abbreviation of “weighted”. Note that the MIWLB LK,J(ϕ,ψ,ξ) becomes MSILB LK,J(ϕ,ψ) if we
w
takethereversemodelR (z|τ,q)=Q (z|τ). Moreover,LK,J(ϕ,ψ,ξ)isalsoalowerboundoftheMLB
ξ ψ w
LK(ϕ,ψ) in equation (8), as proved in Theorem 3.
Theorem3. TheMIWLBLK,J(ϕ,ψ,ξ)inequation(15)isalowerboundoftheMLBLK(ϕ,ψ)inequa-
w
tion (8), and is an increasing function of J, i.e., LK,J(ϕ,ψ,ξ) ≤ LK,J+1(ϕ,ψ,ξ) ≤ LK(ϕ,ψ), ∀J, for
w w
arbitrary choices of ξ. Moreover, it is asymptotically unbiased, i.e., lim LK,J(ϕ,ψ,ξ)=LK(ϕ,ψ).
J→∞ w
There are many choices for the reverse model R (z|τ,q), e.g., normal distributions and normalizing
ξ
flows. For simplicity, here we use a diagonal normal distribution
Y
R (z|τ,q)= pNormal(z |µ (e,τ,q),σ (e,τ,q)),
ξ e R R
e∈E(τ)
whereµ (e,τ,q)andσ (e,τ,q)arethemeanandstandarddeviationthatareparameterizedwithMLPs
R R
using the edge features
µ (e,τ,q)=MLPµ(h ∥q ), σ (e,τ,q)=MLPσ(h ∥q ),
R R e e R R e e
where ∥ means vector concatenation. This way, the gradient of MIWLB LK,J(ϕ,ψ,ξ) w.r.t. ξ can be
w
estimated by the reparameterization trick. We summarize the VBPI-SIBranch approach with MIWLB
in Algorithm 2.
4 Experiments
In this section, we test the effectiveness of VBPI-SIBranch on two common tasks for Bayesian phyloge-
netic inference: marginal likelihood estimation and posterior approximation. Our code is available at
https://github.com/tyuxie/VBPI-SIBranch.
10Algorithm 2: VBPI-SIBranch with MIWLB
Input: Observed sequences Y ∈ΩN×S; initialized parameters ϕ,ψ,ξ.
while not converged do
τ1,...,τK ← independent samples from the current tree topology approximating
distribution Q (τ);
ϕ
for k =1,...,K do
zk,0 ← a sample form the current mixing distribution Q (z|τk);
ψ
qk ← a sample from the current conditional branch length distribution Q (q|τk,zk,0);
ψ
zk,1,...,zk,J ← independent samples from the reverse distribution R (z|τk,qk);
ξ
Calculate Q (qk|τk,zk,j),Q (zk,j|τk) and R (zk,j|τk,qk) for 0≤j ≤J;
ψ ψ ξ
end
gˆ← the estimate of the gradient ∇ LK,J(ϕ,ψ,ξ);
ϕ,ψ,ξ w
ϕ,ψ,ξ ← Updated parameters using gradient estimate gˆ.
end
4.1 Experimental Setup
Targets TheexperimentsareperformedoneightbenchmarkdatasetswhichwewillcallDS1-8. These
data sets consist of nucleotide sequences from 27 to 64 eukaryote species with 378 to 2520 sites and are
commonly used to benchmark the Bayesian phylogenetic inference task in previous works (Zhang &
Matsen IV, 2019; Zhang, 2023; Mimori & Hamada, 2023; Xie & Zhang, 2023; Zhou et al., 2023). We
assume a uniform prior on the tree topologies, an i.i.d. exponential prior Exp(10) on branch lengths,
and the simple Jukes & Cantor (JC) substitution model (Jukes et al., 1969).
Variational Family We use the same tree topology variational distribution Q (τ), i.e., the simplest
ϕ
SBNs, for all branch length variational distributions. The conditional probability supports for SBNs
are gathered from 10 replicates of 10000 maximum likelihood bootstrap trees using UFBoot (Minh
etal.,2013),followingZhang&MatsenIV(2019). Forthebranchlengths,wecompareoursemi-implicit
variationalapproximationtotwobaselines: VBPI(Zhang,2023)andVBPI-NF(Zhang,2020). Toobtain
the learnable topological node features, both VBPI-SIBranch and VBPI use the same architecture for
GNNs, which contain L=2 rounds of message passing steps with the aggregation function and update
function following the edge convolution operator (Wang et al., 2018). On all data sets, we set the
dimension of learnable topological node features to 100 and the dimension of hidden variables to 50. All
the activation functions in MLPs are exponential linear units (ELUs) (Clevert et al., 2015). For VBPI-
NF, we use the best RealNVP (Dinh et al., 2016) model with 10 layers to model the branch lengths,
following Zhang (2020).
Optimization We set the number of particles K = 10 for all the MLB, MSILB, and MIWLB. For
both MSILB and MIWLB, we set the number of extra samples to be J =50. To accommodate the mul-
timodality of phylogenetic posterior, we target the annealed phylogenetic posterior at the i-th iteration:
P(Y,τ,q;λ i) = P(Y|τ,q)λiP(τ,q), where the annealing schedule λ
i
= min(1,0.001+i/100000) goes
from 0.001 to 1 after 100000 iterations. The gradient estimates for the tree topology parameters are
obtainedbytheVIMCOestimator(Mnih&Rezende,2016),andthoseforthebranchlengthparameters
and reverse model parameters are obtained by the reparameterization trick (Kingma & Welling, 2014).
AllthesemodelsareimplementedinPyTorch(Paszkeetal.,2019)andtrainedwiththeAdamoptimizer
(Kingma&Ba,2015). Thelearningrateis0.001forthetreetopologymodel,0.001forthebranchlength
model in VBPI and VBPI-SIBranch, and 0.0001 for the branch length model in VBPI-NF. All results
116 VBPI
VBPI-NF
7200 7200 VBPI-SIBranch(MSILB)
− − 5 VBPI-SIBranch(MIWLB)
7400 7400 4
− − -7108.5
-7110.0 3
-7108.75
7600 -7111.0 7600
− −
105 105 2
−7800 VBPI VBPI-SIBranch(MSILB) −7800 VBPI VBPI-SIBranch(MSILB) 1
VBPI-NF VBPI-SIBranch(MIWLB) VBPI-NF VBPI-SIBranch(MIWLB)
0
104 105 104 105 DS1 DS2 DS3 DS4 DS5 DS6 DS7 DS8
Iterations Iterations Dataset
Figure 2: Visualization of the training processes of different methods for VBPI. Left: evidence lower
bound (ELBO, estimated using J = 1000 extra samples) as a function of iterations on DS1. Middle:
10-sample lower bound (LB-10, estimated using J = 1000 extra samples) as a function of iterations on
DS1. Right: Time cost per 10 training iterations of different methods on a single core of Intel Xeon
Platinum 9242 processor. The results are averaged over 100 runs with the standard deviation as the
error bar.
are collected after 400000 parameter updates.
4.2 Marginal Likelihood Estimation
We first investigate the performances of different methods for estimating the marginal likelihood and its
lower bounds. Figure 2 depicts the training processes and the time costs for VBPI on DS1. We see that
the ELBO and the 10-sample lower bound (LB-10) as functions of iterations for VBPI-SIBranch align
with those for VBPI and VBPI-NF. Moreover, VBPI-SIBranch with MIWLB finally achieves the best
lower bounds compared to the other three methods. In the right plot of Figure 2, we find that VBPI-
SIBranchrequirescomparabletimeintrainingalthoughmultipleextrasamples(J =50)areneeded,due
totheefficientvectorizedimplementation. Table1showstheELBO,LB-10,andmarginallikelihood(ML)
estimatesofdifferentmethodsonDS1-8. ItisworthnotingthatthecomparisonbetweenVBPI-SIBranch
(MSILB)andVBPI-SIBranch(MIWLB)mightbeunfairsincetheyusedifferentimportancedistributions
for evaluation. Therefore, we train a reverse model for the variational approximation in VBPI-SIBranch
(MSILB) and calculate the lower bound estimates using MIWLB. Results in this setting are reported
in VBPI-SIBranch (MSILB∗). We see that VBPI-SIBranch consistently outperforms the VBPI baseline
in terms of lower bounds and marginal likelihood estimates, indicating the effectiveness of semi-implicit
branch length distributions. Moreover, the superior performance of VBPI-SIBranch (MIWLB) over
VBPI-SIBranch(MSILB)andVBPI-SIBranch(MSILB∗)suggeststhatemployingalearnableimportance
distribution can be beneficial for the training of VBPI-SIBranch.
4.3 Posterior Approximation
Inference Gaps on Individual Trees Tobetterunderstandtheeffectofsemi-implicitbranchlength
distributionsfortheoverallimprovementonvariationalapproximationaccuracy,wefurtherevaluatethe
performance of different methods on individual trees in the 95% credible set of DS1. For a fixed tree
topology τ, we define the ELBO L(Q |τ) of a variational approximation Q (q|τ) and the best ELBO
ψ ψ
that can be achieved by the corresponding variational family Q as
(cid:18) P(Y|τ,q)P(q)(cid:19)
L(Q |τ)=E log , L(Q |τ)= max L(Q |τ).
ψ Qψ(q|τ) Q ψ(q|τ) ψ∗ Qψ∈Q ψ
12
OBLE 01-BL
)sdnoces(emitUPCTable 1: Evidence lower bound (ELBO), 10-sample lower bound (LB-10), and marginal likelihood (ML)
estimates of different methods across 8 benchmark data sets. The MSILB∗ refers to the MIWLB es-
timates of the variational approximation in VBPI-SIBranch (MSILB). The ML estimates are obtained
via importance sampling using 1000 samples. For ELBO, LB-10, and ML, the results are averaged over
100, 100, and 1000 independent runs respectively with standard deviation in the brackets. Results of
stepping-stone (SS) are from Zhang & Matsen IV (2019).
Dataset DS1 DS2 DS3 DS4 DS5 DS6 DS7 DS8
#Taxa 27 29 36 41 50 50 59 64
#Sites 1949 2520 1812 1137 378 1133 1824 1008
VBPI-SIBranch(MSILB) -7110.00(0.30) -26368.66(0.09) -33736.07(0.07) -13331.60(0.32) -8217.31(0.20) -6728.25(0.44) -37334.41(0.34) -8654.55(0.32)
VBPI-SIBranch(MSILB∗) -7109.99(0.28) -26368.66(0.09) -33736.06(0.07) -13331.59(0.29) -8217.29(0.21) -6728.21(0.44) -37334.39(0.34) -8654.49(0.33)
VBPI-SIBranch(MIWLB) -7109.34(0.13) -26368.56(0.09) -33735.93(0.06) -13330.81(0.08) -8215.95(0.09) -6725.05(0.07) -37333.22(0.09) -8651.49(0.09)
VBPI -7110.26(0.10) -26368.84(0.09) -33736.25(0.08) -13331.80(0.10) -8217.80(0.12) -6728.57(0.16) -37334.84(0.14) -8655.01(0.14)
VBPI-NF -7109.83(0.10) -26368.44(0.19) -33735.73(0.10) -13331.36(0.09) -8217.59(0.10) -6728.04(0.14) -37333.85(0.09) -8654.10(0.12)
VBPI-SIBranch(MSILB) -7108.53(0.02) -26367.82(0.02) -33735.22(0.02) -13330.12(0.02) -8215.03(0.03) -6724.81(0.03) -37332.30(0.03) -8651.26(0.04)
VBPI-SIBranch(MSILB∗) -7108.53(0.02) -26367.82(0.01) -33735.23(0.02) -13330.11(0.02) -8215.01(0.03) -6724.77(0.03) -37332.29(0.03) -8651.22(0.04)
VBPI-SIBranch(MIWLB) -7108.46(0.01) -26367.80(0.01) -33735.20(0.01) -13330.02(0.01) -8214.70(0.02) -6724.26(0.01) -37332.11(0.02) -8650.49(0.02)
VBPI -7108.69(0.02) -26367.87(0.02) -33735.26(0.02) -13330.29(0.02) -8215.42(0.04) -6725.33(0.04) -37332.58(0.03) -8651.78(0.04)
VBPI-NF -7108.58(0.02) -26367.75(0.01) -33735.15(0.01) -13330.15(0.02) -8215.30(0.03) -6725.18(0.04) -37332.29(0.03) -8651.43(0.04)
VBPI-SIBranch(MSILB) -7108.39(0.07) -26367.71(0.06) -33735.09(0.07) -13329.91(0.10) -8214.48(0.27) -6724.21(0.25) -37331.91(0.16) -8650.44(0.35)
VBPI-SIBranch(MSILB∗) -7108.39(0.06) -26367.71(0.05) -33735.09(0.07) -13329.91(0.09) -8214.47(0.28) -6724.20(0.23) -37331.91(0.15) -8650.43(0.33)
VBPI-SIBranch(MIWLB) -7108.39(0.04) -26367.71(0.05) -33735.09(0.07) -13329.91(0.06) -8214.43(0.19) -6724.16(0.06) -37331.90(0.09) -8650.33(0.11)
VBPI -7108.41(0.15) -26367.71(0.08) -33735.09(0.09) -13329.94(0.20) -8214.62(0.40) -6724.37(0.43) -37331.97(0.28) -8650.64(0.50)
VBPI-NF -7108.39(0.17) -26367.71(0.03) -33735.09(0.05) -13329.92(0.15) -8214.59(0.45) -6724.33(0.42) -37331.93(0.18) -8650.55(0.39)
SS -7108.42(0.18) -26367.57(0.48) -33735.44(0.50) -13330.06(0.54) -8214.51(0.28) -6724.07(0.86) -37332.76(2.42) -8649.88(1.75)
Table2: Inferencegapsontreetopologiesinthe95%crediblesetofDS1. TheAvg. columnreferstothe
average gaps over all tree topologies in the credible set. Results of VBPI-NF are from Zhang (2020).
VBPI VBPI-NF VBPI-SIBranch (MISLB) VBPI-SIBranch (MIWLB)
Gap
Avg. Tree 36 Avg. Tree 36 Avg. Tree 36 Avg. Tree 36
Approximation 1.22 1.29 0.40 0.43 0.64 0.68 0.34 0.36
Amortization 0.51 0.91 0.93 1.83 0.80 1.19 0.22 0.91
Inference 1.73 2.20 1.33 2.26 1.44 1.87 0.56 1.27
IfQ (q|τ)issemi-implicitasinequation(12),onemayimitateMSILBandMIWLBtoestimateL(Q |τ)
ψ ψ
and L(Q |τ), i.e.
ψ∗
!
P(Y|τ,q)P(q)
L(Q |τ)≈LJ(Q |τ)=E log , L(Q |τ)≈ max LJ(Q |τ),
ψ ψ Qψ(q,z0|τ)Qψ(z1:J|τ) J+1 1PJ j=0Q ψ(q|τ,zj) ψ∗ Qψ∈Q ψ
in the MSILB setting, or
 
L(Q ψ|τ) ≈ LJ w(Q ψ,R ξ|τ)=E Qψ(q,z0|τ)Rξ(z1:J|τ,q)log 
1
PJP(Y Q| ψτ, (q q|) τP ,z( jq )) Qψ(zj|τ),
J+1 j=0 Rξ(zi,j|τi,qi)
L(Q |τ) ≈ max LJ(Q ,R |τ),
ψ∗ Qψ∈Q,Rξ∈R w ψ ξ
intheMIWLBsetting. TocomputethebestELBOL(Q |τ),wetakeJ =50fortrainingandJ =1000
ψ∗
for evaluation in practice. For a fixed tree topology τ, the inference gap of each variational family is
defined as the difference between the marginal log-likelihood logP(Y|τ) and the ELBO L(Q |τ), which
ψ
can be decomposed as
logP(Y|τ)−L(Q |τ)=(cid:2)logP(Y|τ)−L(Q∗|τ)(cid:3)+(cid:2) L(Q∗|τ)−L(Q |τ)(cid:3) ,
ψ ψ ψ ψ
i.e., the sum of approximation and amortization gaps (Cremer et al., 2018; Zhang, 2020).
Figure3showsthedecompositionoftheinferencegapofdifferentvariationalfamiliesonDS1. Inthe
13
OBLE
01-BL
LMVBPI VBPI-SIBranch(MSILB) VBPI-SIBranch(MIWLB)
7036 7036 7036
− − −
logP(Y τ) logP(Y τ) logP(Y τ)
−7037 L(Qψ∗|τ| ) −7037 L(Qψ∗|τ| ) −7037 L(Qψ∗|τ| )
7038
L(Qψ|τ)
7038
L(Qψ|τ)
7038
L(Qψ|τ)
− − −
7039 7039 7039
− − −
7040 7040 7040
− − −
7041 7041 7041
− − −
7042 7042 7042
− − −
7043 7043 7043
− − −
−7044 0 10 20 30 36 40 −7044 0 10 20 30 36 40 −7044 0 10 20 30 36 40
TreeID TreeID TreeID
Figure 3: Inference gaps on tree topologies in the 95% credible set of DS1. The L(Q |τ) refers to the
ψ
ELBOofthevariationalapproximation,andtheL(Q |τ)referstothebestELBOthatcanbeachieved
ψ∗
bythecorrespondingvariationalfamily. Alllowerboundswerecomputedbyaveragingover10000Monte
Carlo samples. The ground truth marginal log-likelihood logP(Y|τ) is estimated using the generalized
stepping-stone (GSS) algorithm (Fan et al., 2010).
left plot of Figure 3, the large approximation gap indicates that the diagonal lognormal distribution in
VBPI is too restricted to fit the true branch length distribution. In contrast, the semi-implicit branch
length distribution in VBPI-SIBranch performs much better, as indicated by the considerably smaller
approximationgapsinthemiddleandrightplots. Moreover, comparedtoVBPI-SIBranchwithMSILB,
VBPI-SIBranch with MIWLB significantly reduces the approximation gap and generalizes better to the
tree topology space by employing a learnable importance distribution, as evidenced by the reduction of
the amortization gap.
Branch Length Approximation To examine the approximation accuracy of the learned branch
length model Q (q|τ) to the ground truth P(q|τ,Y) more directly, we compare their empirical density
ψ
functions estimated from branch length samples. This also excludes the effects from the importance
distributioninthelowerboundcomparison. Thetotalvariation(TV)distancebetweentwodistributions
with probability density function P (x) and P (x) (x∈Rd) is defined as
1 2
ˆ
1
D (P ∥P )= |P (x)−P (x)|dx.
TV 1 2 2 1 2
Rd
The left plot of Figure 4 shows the TV distance between the learned branch length variational distribu-
tion Q (q|τ) and the ground truth P(q|τ,Y). We find that VBPI-SIBranch indeed provides a better
ψ
approximation to the ground truth branch lengths than VBPI. Also, the variational approximation on
tree 36 still has a relatively large error, which coincides with the observation in Figure 3. In fact, this
relativelylargeapproximationerrorofVBPI-SIBranch(MIWLB)ontree36isidentifiedtobetheresult
of the poor fitting on branch 35 (the middle right plot in Figure 5), and VBPI-SIBranch reaches better
or comparable approximations on other branches.
EvaluationofImportanceWeighting Inthepreviousdiscussions,theimportanceweightingscheme
aswellasthelearnableimportancedistributionemployedintheMIWLBprovedtobebeneficialtotheop-
timizationofVBPI-SIBranch. Wenowinspecttheeffectofimportantweightingmorespecifically. InMI-
´
WLB,whenusingR (z|τ,q)asanimportancedistributiontoestimateQ (q|τ)= Q (q|τ,z)Q (z|τ)dz,
ξ ψ ψ ψ
14
OBLE OBLE OBLE1.2
180
3.0
1.0 160
2.5
0.8 140
2.0
120
1.5 0.6
100
1.0 0.4
0 10 20 30 40 0 10 20 30 40 0 10 20 30 40
TreeID TreeID TreeID
VBPI VBPI-SIBranch(MSILB) VBPI-SIBranch(MIWLB)
Figure 4: Branch length approximation accuracy of different methods for VBPI on DS1. Left/Middle:
The TV distance and KL divergence between the branch length variational distribution and the ground
truth on individual tree topologies. Right: the effective sample size of the importance sampling es-
timation of Q (q|τ) in VBPI-SIBranch. To simplify computation, the TV distance and KL diver-
ψ
gence are defined as P D (Q (q |τ)∥P(q |τ,Y)) and P D (Q (q |τ)∥P(q |τ,Y)), re-
e∈E(τ) TV ψ e e e∈E(τ) KL ψ e e
spectively, where one million samples are drawn from each distribution. The ground truth samples are
gathered from a long MrBayes run with 4 chains for one billion iterations and sampled every 100 itera-
tions.
the effective sample size (ESS) is defined as
1
Qψ(q|τ,zj)Qψ(zj|τ)
ESS=E E , w = Rξ(zj|τ,q) .
Qψ(q|τ) Rξ(z1:J|τ,q)PJ w2 j PJ Qψ(q|τ,zi)Qψ(zi|τ)
j=1 j i=1 Rξ(zi|τ,q)
ESS as a criterion is also suitable for MSILB by letting R (z|τ,q) = Q (z|τ). From the right plot in
ξ ψ
Figure 4, we see that the ESS of MIWLB consistently outperforms that of MSILB, implying that the
reverse model R (z|τ,q) in MIWLB indeed provides a better importance distribution.
ξ
4.4 Ablation Studies
Finally, we explore the effect of different numbers of extra samples J on the performance of VBPI-
SIBranch (Figure 6). We see that the ELBO estimates of VBPI-SIBranch (MSILB) get significantly
better as the number of extra samples increases, while those of VBPI-SIBranch (MIWLB) exhibit ran-
domness across different numbers of extra samples. This implies that more extra samples are beneficial
to the training of VBPI-SIBranch and MIWLB is less sensitive to the choice of the number of extra
samples.
5 Conclusion
This work presented VBPI-SIBranch, which incorporated a semi-implicit branch length model in the
variational family of phylogenetic trees for VBPI. We gave a concrete example of semi-implicit branch
length distribution construction with graph neural networks. Two surrogates of the multi-sample lower
bound, i.e., multi-sample semi-implicit lower bound (MSILB) and multi-sample importance weighted
lowerbound(MIWLB),astrainingobjectiveswerederivedandtheirstatisticalpropertieswerediscussed.
Experiments on benchmark data sets demonstrated that VBPI-SIBranch achieves comparable or better
resultsregardingmarginallikelihoodestimationandbranchlengthapproximation. Thisworkalsoshowed
thegreatpotentialofthevariationalinferenceforphylogeneticinference,alignedwithsomelatestefforts
15
ecnatsiDnoitairaVlatoT
ecnegreviDLK
eziSelpmaSevitceffEBranch1 Branch3 Branch15 Branch35 Branch39
300 100 100 300 300
80 80 250 250
200 60 60 200 200
150 150
100 40 40 100 100
20 20 50 50
0 0 0 0 0
0.000 0.005 0.010 0.015 0.01 0.02 0.03 0.04 0.05 0.01 0.02 0.03 0.04 0.000 0.005 0.010 0.015 0.000 0.005 0.010 0.015
100 100 400 300
300 80 80 300
200
200 60 60
200
100 24 00 24 00 100 100
0 0 0 0 0
0.000 0.005 0.010 0.015 0.01 0.02 0.03 0.04 0.05 0.01 0.02 0.03 0.04 0.000 0.005 0.010 0.015 0.000 0.005 0.010 0.015
BranchLength BranchLength BranchLength BranchLength BranchLength
MCMC VBPI VBPI-SIBranch(MSILB) VBPI-SIBranch(MIWLB)
Figure5: Selectedmarginalbranchlengthvariationaldistributionsobtainedbydifferentmethodsontree
36 of DS1. For each method, we estimated the probability density function with one million samples.
VBPI-SIBranch(MSILB) VBPI-SIBranch(MIWLB)
0.1 0.1
0.0 0.0
0.1 0.1
− −
0.2 0.2
− −
Lˆ J=20−Lˆ
J=50
Lˆ J=20−Lˆ
J=50
−0.3 Lˆ J=100−Lˆ
J=50
−0.3 Lˆ J=100−Lˆ
J=50
DS1 DS2 DS3 DS4 DS5 DS6 DS7 DS8 DS1 DS2 DS3 DS4 DS5 DS6 DS7 DS8
Figure6: AblationstudyaboutthenumberofextrasamplesJ inVBPI-SIBranch. Foreachmethod, we
train the models using K =10 and different J =20,50,100, and estimate the ELBOs of the variational
approximations for different training objectives (Lˆ ,Lˆ ,Lˆ ) with K =1 and J =1000.
J=20 J=50 J=100
in this domain (Zhang, 2023; Xie & Zhang, 2023; Kviman et al., 2023), and demonstrated the power of
deep learning methods (Zhang, 2023) for representing phylogenetic trees. Designing more flexible and
scalable variational families for tree topologies and branch lengths based on powerful tree embeddings
can be an important future direction in the field of variational phylogenetic inference.
Limitations Throughout this paper, the mixing distribution Q (z|τ) is set to a standard Gaussian
ψ
distribution which ignores the dependency on τ. Designing Q (z|τ) with the information of τ, e.g.,
ψ
learnable node features, would be an interesting future direction.
Acknowledgments
The research of Cheng Zhang was supported in part by National Natural Science Foundation of China
(grant no. 12201014 and grant no. 12292983), the National Engineering Laboratory for Big Data
Analysis and Applications, the Key Laboratory of Mathematics and Its Applications (LMAM), and the
FundamentalResearchFundsfortheCentralUniversities,PekingUniversity. Thisresearchwaspartially
supported through US National Institutes of Health grant R01 AI162611.
16
)tnioJ(ytisneDdetamitsE
)laudividnI(ytisneDdetamitsE
setamitseOBLEfoecnereffiD setamitseOBLEfoecnereffiDReferences
David M. Blei, Alp Kucukelbir, and Jon D. McAuliffe. Variational inference: A review for statisticians.
Journal of the American Statistical Association, 112:859 – 877, 2016. 2, 3
Jörg Bornschein and Yoshua Bengio. Reweighted wake-sleep. In Proceedings of the International Con-
ference on Learning Representations, 2015. 5
YuriBurda,RogerGrosse,andRuslanSalakhutdinov.Importanceweightedautoencoders.arXivpreprint
arXiv:1509.00519, 2015. 3
Djork-ArnéClevert,ThomasUnterthiner,andSeppHochreiter. Fastandaccuratedeepnetworklearning
by exponential linear units (ELUs). arXiv: Learning, 2015. 11
Chris Cremer, Xuechen Li, and David Kristjanson Duvenaud. Inference suboptimality in variational
autoencoders. In International Conference on Machine Learning, 2018. 13
T. Dang and H. Kishino. Stochastic variational inference for Bayesian phylogenetics: a case of CAT
model. Molecular biology and evolution, 36(4):825–833, 2019. 2
RobDeSalleandGeorgeAmato. Theexpansionofconservationgenetics. Nat.Rev.Genet.,5(9):702–712,
September 2004. ISSN 1471-0056. doi: 10.1038/nrg1425. 1
Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real nvp. In Interna-
tional Conference on Learning Representations, 2016. 2, 11
VuDinh,ArmanBilge,ChengZhang,andFrederickAMatsenIV. ProbabilisticpathHamiltonianMonte
Carlo. In Proceedings of the 34th International Conference on Machine Learning, pp. 1009–1018, July
2017. 2
LouisduPlessis,JohnTMcCrone,AlexanderEZarebski,VerityHill,ChristopherRuis,BernardoGutier-
rez, Jayna Raghwani, Jordan Ashworth, Rachel Colquhoun, Thomas R Connor, Nuno R Faria, Ben
Jackson, Nicholas J Loman, Áine O’Toole, Samuel M Nicholls, Kris V Parag, Emily Scher, Tetyana I
Vasylyeva, Erik M Volz, Alexander Watts, Isaac I Bogoch, Kamran Khan, COVID-19 Genomics UK
(COG-UK) Consortium†, David M Aanensen, Moritz U G Kraemer, Andrew Rambaut, and Oliver G
Pybus. EstablishmentandlineagedynamicsoftheSARS-CoV-2epidemicintheUK. Science,January
2021. ISSN 0036-8075, 1095-9203. doi: 10.1126/science.abf2946. 1
Yu Fan, Rui Wu, Ming-Hui Chen, Lynn Kuo, and Paul O. Lewis. Choosing among partition models in
Bayesian phylogenetics. Molecular Biology and Evolution, 28:523–532, 2010. 14
J. Felsenstein. Evolutionary trees from DNA sequences: A maximum likelihood approach. Journal of
Molecular Evolution, 17:268–276, 1981. 5
Joseph Felsenstein. Inferring Phylogenies. Sinauer associates, 2 edition, 2004. 5
M. Fourment and A. E. Darling. Evaluating probabilistic programming and fast variational Bayesian
inference in phylogenetics. PeerJ., 7:e8272, 2019. 2
Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural
message passing for quantum chemistry. ArXiv, abs/1704.01212, 2017. 6, 8
17Gabriel W Hassler, Andrew F Magee, Zhenyu Zhang, Guy Baele, Philippe Lemey, Xiang Ji, Mathieu
Fourment, and Marc A Suchard. Data integration in Bayesian phylogenetics. Annual Review of
Statistics and Its Application, 10:353–377, 2023. 2
Sebastian Höhna and Alexei J. Drummond. Guided tree topology proposals for Bayesian phylogenetic
inference. Syst. Biol., 61(1):1–11, 2012. 2
Ferenc Huszár. Variational inference using implicit distributions. arXiv preprint arXiv: 1702.08235,
2017. 2
Michael I Jordan, Zoubin Ghahramani, Tommi S Jaakkola, and Lawrence K Saul. An introduction to
variational methods for graphical models. Machine learning, 37:183–233, 1999. 2, 3
Thomas H Jukes, Charles R Cantor, et al. Evolution of protein molecules. Mammalian protein
metabolism, 3:21–132, 1969. 5, 11
D. P. Kingma and J. Ba. Adam: A method for stochastic optimization. In ICLR, 2015. 11
Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. In International Conference on
Learning Representations, 2014. 5, 11
Durk P Kingma, Tim Salimans, Rafal Jozefowicz, Xi Chen, Ilya Sutskever, and Max Welling. Improved
variational inference with inverse autoregressive flow. Advances in neural information processing sys-
tems, 29, 2016. 2
Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In
International Conference on Learning Representations, 2017. 6
Hazal Koptagel, Oskar Kviman, Harald Melin, Negar Safinianaini, and Jens Lagergren. VaiPhy: a
variational inference based algorithm for phylogeny. In Advances in Neural Information Processing
Systems, 2022. 2
Oskar Kviman, Ricky Molén, and Jens Lagergren. Improved variational Bayesian phylogenetic inference
using mixtures. arXiv preprint arXiv:2310.00941, 2023. 16
C. Lakner, P. van der Mark, J. P. Huelsenbeck, B. Larget, and F. Ronquist. Efficiency of Markov chain
Monte Carlo tree proposals in Bayesian phylogenetics. Syst. Biol., 57:86–103, 2008. 2
Bret R. Larget and D. L. Simon. Markov chain Monte Carlo algorithms for the Bayesian analysis of
phylogenetic trees. Molecular Biology and Evolution, 16:750–750, 1999. 1
B. Mau, M. Newton, and B. Larget. Bayesian phylogenetic inference via Markov chain Monte Carlo
methods. Biometrics, 55:1–12, 1999. 1
L. M. Mescheder, S. Nowozin, and A. Geiger. Adversarial variational bayes: Unifying variational au-
toencoders and generative adversarial networks. In Proceedings of the 34th International Conference
on Machine Learning, 2017. 2
Takahiro Mimori and Michiaki Hamada. GeoPhy: Differentiable phylogenetic inference via geometric
gradients of tree topologies. In Thirty-seventh Conference on Neural Information Processing Systems,
2023. 2, 11
18BuiQuangMinh, MinhAnhNguyen, andArndtvonHaeseler. Ultrafastapproximationforphylogenetic
bootstrap. Molecular Biology and Evolution, 30:1188 – 1195, 2013. 6, 11, 21
Andriy Mnih and Danilo Jimenez Rezende. Variational inference for monte carlo objectives. In Interna-
tional Conference on Machine Learning, 2016. 3, 5, 11
George Papamakarios, Eric Nalisnick, Danilo Jimenez Rezende, Shakir Mohamed, and Balaji Laksh-
minarayanan. Normalizing flows for probabilistic modeling and inference. The Journal of Machine
Learning Research, 22(1):2617–2680, 2021. 2
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Köpf, Edward
Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning
library. In Neural Information Processing Systems, 2019. 11
Tom Rainforth, Adam R. Kosioreck, Tuan Anh Le, Chris J. Maddison, Maximilian Igl, Frank Wood,
and Yee Whye Teh. Tighter variational bounds are not necessarily better. In Proceedings of the 36th
International Conference on Machine Learning, 2019. 5
Rajesh Ranganath, Sean Gerrish, and David Blei. Black box variational inference. In Artificial intelli-
gence and statistics, pp. 814–822. PMLR, 2014. 2
Danilo Rezende and Shakir Mohamed. Variational inference with normalizing flows. In International
conference on machine learning, pp. 1530–1538. PMLR, 2015. 2
Fredrik Ronquist, Maxim Teslenko, Paul Van Der Mark, Daniel L Ayres, Aaron Darling, Sebastian
Höhna, Bret Larget, Liang Liu, Marc A Suchard, and John P Huelsenbeck. MrBayes 3.2: Efficient
Bayesian phylogenetic inference and model choice across a large model space. Systematic Biology, 61
(3):539–542, 2012. 1, 5
J.Shi,S.Sun,andJ.Zhu. Kernelimplicitvariationalinference. InInternational Conference on Learning
Representations, 2018. 2
Artem Sobolev and Dmitry P. Vetrov. Importance weighted hierarchical variational inference. In Neural
Information Processing Systems, 2019. 2, 3, 4, 10
Y.Song,S.Garg,J.Shi,andS.Ermon. Slicedscorematching: Ascalableapproachtodensityandscore
estimation. In Proceedings of the Thirty-Fifth Conference on Uncertainty in Artificial Intelligence,
2019. 2
SimonTavaréetal. Someprobabilisticandstatisticalproblemsintheanalysisofdnasequences. Lectures
on Mathematics in the Life Sciences, 1986. 5
Michalis K Titsias and Francisco Ruiz. Unbiased implicit variational inference. In International Confer-
ence on Artificial Intelligence and Statistics. PMLR, 2019. 2
D. Tran, R. Ranganath, and D. M. Blei. Hierarchical implicit models and likelihood-free variational
inference. In Advances in Neural Information Processing Systems, 2017. 2
Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon.
Dynamic graph CNN for learning on point clouds. ACM Transactions on Graphics (TOG), 38:1 – 12,
2018. 11
19Chris Whidden and Frederick A Matsen IV. Quantifying MCMC exploration of phylogenetic tree space.
Syst. Biol., 64(3):472–491, 2015. 2
Tianyu Xie and Cheng Zhang. ARTree: A deep autoregressive model for phylogenetic inference. In
Proceedings of the Thirty-seventh Conference on Neural Information Processing Systems, 2023. 2, 8,
11, 16
ZihengYangandBruceRannala. BayesianphylogeneticinferenceusingDNAsequences: aMarkovchain
Monte Carlo method. Molecular Biology and Evolution, 14(7):717–724, 1997. 1
Mingzhang Yin and Mingyuan Zhou. Semi-implicit variational inference. In International Conference
on Machine Learning, pp. 5646–5655, 2018. 2, 3, 4, 9
Cheng Zhang. Improved variational Bayesian phylogenetic inference with normalizing flows. In Neural
Information Processing Systems, 2020. 2, 6, 11, 13, 21
Cheng Zhang. Learnable topological features for phylogenetic inference via graph neural networks. In
International Conference on Learning Representations, 2023. 2, 6, 7, 8, 11, 16
ChengZhangandFrederickAMatsenIV.GeneralizingtreeprobabilityestimationviaBayesiannetworks.
In Neural Information Processing Systems, 2018. 5, 21
Cheng Zhang and Frederick A Matsen IV. Variational Bayesian phylogenetic inference. In International
Conference on Learning Representations, 2019. 2, 6, 10, 11, 13
Cheng Zhang and Frederick A Matsen IV. A variational approach to Bayesian phylogenetic inference.
Journal of Machine Learning Research, 25(145):1–56, 2024. URL http://jmlr.org/papers/v25/
22-0348.html. 5, 6
Mingyang Zhou, Zichao Yan, Elliot Layne, Nikolay Malkin, Dinghuai Zhang, Moksh Jain, Mathieu
Blanchette, and Yoshua Bengio. Phylogfn: Phylogenetic inference with generative flow networks.
ArXiv, abs/2310.08774, 2023. 2, 11
20··· A A
A
S2
S S4
5
·
·
··
·
··
·
·
DB C A DBC B DC
D
DB C assign
S2
S S4
5
S1
···
S1
S3
S6
···
A
A
A
S3
S6
S7 ···
B
C
CA DB B
C
B
C
assign
S7
D
··· D D
Figure 7: Subsplit Bayesian networks and a simple example for a leaf set of 4 taxa (denoted by A,B,C,D respectively).
Left: General subsplit Bayesian networks. The solid full and complete binary tree network is B∗. The dashed arrows
X
representtheadditionaldependenceformoreexpressiveness. Middle Left: Examplesof(rooted)phylogenetictreesthat
arehypothesizedtomodeltheevolutionaryhistoryofthetaxa. Middle Right: Thecorrespondingsubsplitassignments
forthetrees. Foreaseofillustration,subsplit(Y,Z)isrepresentedas Y inthegraph. Right: TheSBNforthisexample,
Z
whichisB∗ inthiscase. ThisfigureisfromZhang&MatsenIV(2018).
X
A Details of Subsplit Bayesian Networks
One recent and expressive graphical model that provides a flexible family of distributions over tree topologies is
the subsplit Bayesian network, as proposed by Zhang & Matsen IV (2018). Let X be the set of N labeled leaf
nodes. A non-empty set C of X is referred to as a clade and the set of all clades of X, denoted by C(X), is a
totally ordered set with a partial order ≻ (e.g., lexicographical order) defined on it. An ordered pair of clades
(W,Z) is called a subsplit of a clade C if it is a bipartition of C, i.e., W ≻Z, W ∩Z =∅, and W ∪Z =C.
Definition 2 (Subsplit Bayesian Network). A subsplit Bayesian network (SBN) B on a leaf node set X of
X
size N is defined as a Bayesian network whose nodes take on subsplit or singleton clade values of X and has the
following properties: (a) The root node of B takes on subsplits of the entire labeled leaf node set X; (b) B
X X
contains a full and complete binary tree network B∗ as a subnetwork; (c) The depth of B is N −1, with the
X X
root counted as depth 1.
DuetothebinarystructureofB∗,thenodesinSBNscanbeindexedbydenotingtherootnodewithS and
X 1
twochildrenofS withS andS recursivelywhereS isaninternalnode(seetheleftplotinFigure7). For
i 2i 2i+1 i
any rooted tree topology, by assigning the corresponding subsplits or singleton clades values {S =s } to its
i i i≥1
nodes, one can uniquely map it into an SBN node assignment (see the middle and right plots in Figure 7).
As Bayesian networks, the SBN-based probability of a rooted tree topology τ takes the following form
Y
p (T =τ)=p(S =s ) p(S =s |S =s ), (16)
sbn 1 1 i i πi πi
i>1
whereπ istheindexsetoftheparentsofnodei. Forunrootedtreetopologies,wecanalsodefinetheirSBN-based
i
probabilities by viewing them as rooted tree topologies with unobserved roots and integrating the positions of
the root node as follows:
X
p (Tu =τ)= p (τe) (17)
sbn sbn
e∈E(τ)
where τe is the resulting rooted tree topology when the rooting position is on edge e.
In practice, SBNs are parameterized according to the conditional probability sharing principle where the
conditional probability for parent-child subsplit pairs are shared across the SBN network, regardless of their
locations. Thesetofallconditionalprobabilitiesarecalledconditionalprobabilitytables(CPTs). Parameterizing
SBNs, therefore, often requires finding an appropriate support of CPTs. For tree topology density estimation,
this can be done using the sample of tree topologies that is given as the data set. For variational Bayesian
phylogeneticinference,asnosampleoftreetopologiesisavailable,oneoftenresortstofastbootstraporMCMC
methods (Minh et al., 2013; Zhang, 2020). Let S denote the root subsplits and S denotes the child-parent
r ch|pa
21subsplit pairs in the support. The parameters of SBNs are then p={p ;s ∈S }∪{p ;s|t∈S } where
s1 1 r s|t ch|pa
p =p(S =s ), p =p(S =s|S =t), ∀i>1. (18)
s1 1 1 s|t i πi
B Theoretical Results
B.1 Proof of Theorem 2
The asymptotically unbiasedness is a direct result of the strong law of large numbers. To prove LK,J(ϕ,ψ) ≤
LK,J+1(ϕ,ψ)≤LK(ϕ,ψ),∀J, we have three steps as follows.
Step1 Asthefirststep,wewillgivealternativeexpressionsforLK,J(ϕ,ψ)andLK(ϕ,ψ). LetQJ(q|τk,zk,0:J)=
ψ
1 PJ Q (q|τk,zk,j). By symmetry, we have
J+1 j=0 ψ
LK,J(ϕ,ψ)
!
1 XJ 1 XK P(Y|τk,qk)P(τk,qk)
= E E log
J+1
j=0
⟨(τk,qk,zk,j)∼Qϕ,ψ(τ,q,z)⟩K k=1 ⟨zk,(0:J)\j∼Qψ(z|τk)⟩K k=1 K
k=1
Q ϕ(τk) J+1 1PJ j=0Q ψ(qk|τk,zk,j)
!
1 XK P(Y|τk,qk)P(τk,qk)
=E ⟨τk∼Qϕ(τ)⟩K k=1E ⟨zk,0:J∼Qψ(z|τk)⟩K k=1E (cid:10) qk∼QJ ψ(q|τk,zk,0:J)(cid:11)K k=1log K
k=1
Q ϕ(τk)QJ ψ(qk|τk,zk,0:J) .
where (0:J)\j ={0,...,j−1}∪{j+1,...,J}. Using the fact that
E QJ(q|τk,zk,0:J)=Q (q|τk), k=1,...,K,
zk,0:J∼Qψ(z|τk) ψ ψ
we can rewrite LK(ϕ,ψ) as
!
1 XK P(Y|τk,qk)P(τk,qk)
LK(ϕ,ψ)=E ⟨τk∼Qϕ(τ)⟩K k=1E ⟨zk,0:J∼Qψ(z|τk)⟩K k=1E (cid:10) qk∼QJ ψ(q|τk,zk,0:J)(cid:11)K k=1log K
k=1
Q ϕ(τk)Q ψ(qk|τk) .
In this way, LK,J(ϕ,ψ) and LK(ϕ,ψ) share the same reference distribution for expectation.
Step 2 Let QJ (τ1:K,q1:K,z1:K,0:J) = QK QJ(qk|τk,zk,0:J)Q (zk,0:J|τk)Q (τk). We will show that the
ϕ,ψ k=1 ψ ψ ϕ
following two functions are both probability density functions:
 PK P(Y|τk,qk)P(τk,qk)
 f ϕJ ,ψ(τ1:K,q1:K,z1:K,0:J) = Pk= K1Qϕ P( (τ Yk |) τQ kJ ψ ,q(q kk )P|τ (k τ, kz ,k q, k0 ):J) QJ ϕ,ψ(τ1:K,q1:K,z1:K,0:J);
k=1 Qϕ(τk)Qψ(qk|τk)
PK P(Y|τk,qk)P(τk,qk)
 hJ ϕ,ψ(τ1:K,q1:K,z1:K,0:J+1) = PKk=1Qϕ P( (τ Yk |) τQ kJ ψ ,q(q kk )P|τ (k τ, kz ,k q, k0 ):J) QJ ϕ+ ,ψ1(τ1:K,q1:K,z1:K,0:J+1).
k=1Qϕ(τk)QJ ψ+1(qk|τk,zk,0:J+1)
To prove fJ is a probability density function, we first integrate out z1:K,0:J, i.e.
ϕ,ψ
ˆ
fJ (τ1:K,q1:K,z1:K,0:J)dz1:K,0:J
ϕ,ψ
ˆ " #
K K
1 X Y Y
= · P(Y|τk,qk)P(τk,qk) Q (τl)QJ(ql|τl,zl,0:J) Q (zl,0:J|τl)dz1:K,0:J
PK P(Y|τk,qk)P(τk,qk) ϕ ψ ψ
k=1 Qϕ(τk)Qψ(qk|τk) k=1 l̸=k l=1
PK P(Y|τk,qk)P(τk,qk)Q Q (τl)Q (ql|τl)
= k=1 l̸=k ϕ ψ .
PK P(Y|τk,qk)P(τk,qk)
k=1 Qϕ(τk)Qψ(qk|τk)
22Noting that
XK Y XK P(Y|τk,qk)P(τk,qk) YK
P(Y|τk,qk)P(τk,qk) Q (τl)Q (ql|τl)= · Q (τl)Q (ql|τl),
ϕ ψ Q (τk)Q (qk|τk) ϕ ψ
ϕ ψ
k=1 l̸=k k=1 l=1
we therefore have
ˆ
K
Y
fJ (τ1:K,q1:K,z1:K,0:J)dz1:K,0:J = Q (τl)Q (ql|τl),
ϕ,ψ ϕ ψ
l=1
which is clearly a density function of τ1:K and q1:K.
To prove hJ is a probability density function, it suffices to show
ϕ,ψ
PK P(Y|τk,qk)P(τk,qk)
E ⟨τk∼Qϕ(τ)⟩K k=1E ⟨zk,0:J+1∼Qψ(z|τk)⟩K k=1E (cid:10) qk∼QJ ψ(q|τk,zk,0:J+1)(cid:11)K k=1PK k=k 1= Q1 ϕQ (ϕ τP k( (τ )Y Qk |)
J
ψτQ +kJ ψ 1,q (( k qq k)k P || ττ ( kk τ ,, k zz , kk q ,, k 00 ) :: JJ +)
1)
=1.
Let {I : I ⊂ {0,...,J +1},|I | = J +1,k = 1,...,K} be uniformly distributed subsets with distinct indices
k k k
from {0,...,J+1}. Let QJ ψ(q|τk,zk,Ik)= J+1 1P j∈IkQ ψ(q|τk,zk,j). By symmetry, we have
PK P(Y|τk,qk)P(τk,qk)
E ⟨zk,0:J+1∼Qψ(z|τk)⟩K k=1E (cid:10) qk∼QJ ψ(q|τk,zk,0:J+1)(cid:11)K k=1PK k=k 1= Q1 ϕQ (ϕ τP k( (τ )Y Qk |)
J
ψτQ +kJ ψ 1,q (( k qq k)k P || ττ ( kk τ ,, k zz , kk q ,, k 00 ) :: JJ +)
1)
PK P(Y|τk,qk)P(τk,qk)
=E ⟨zk,0:J+1∼Qψ(z|τk)⟩K k=1E I1:KE (cid:10) qk∼QJ ψ(q|τk,zk,Ik)(cid:11)K k=1PK k=k 1= Q1 ϕQ (ϕ τP k( (τ )Y Qk |)
J
ψτQ +kJ ψ 1,q (( k qq k)k P || ττ ( kk τ ,, k zz , kk q ,, k 00 ) :: JJ +)
1)
ˆ
PK P(Y|τk,qk)P(τk,qk)Q QJ(ql|τl,zl,Il)

=E
⟨zk,0:J+1∼Qψ(z|τk)⟩K
k=1E
I1:K
k=1
PK
Qϕ(τk P) (Y|τk,qk)Pl̸= (k τk,qψ
k)
dq1:K

k=1 Qϕ(τk)QJ ψ+1(qk|τk,zk,0:J+1)
ˆ
PK P(Y|τk,qk)P(τk,qk)Q QJ+1(ql|τl,zl,0:J+1)

=E
⟨zk,0:J+1∼Qψ(z|τk)⟩K k=1
k=1 PKQϕ(τk) P(Y|τk,ql̸= k)k P(τψ
k,qk)
dq1:K

k=1 Qϕ(τk)QJ ψ+1(qk|τk,zk,0:J+1)
ˆ
K
Y
=E QJ+1(ql|τl,zl,0:J+1)dq1:K.
⟨zk,0:J+1∼Qψ(z|τk)⟩K
k=1
ψ
l=1
=1.
Here, we use the fact that
E QJ(ql|τl,zl,Il)=QJ+1(ql|τl,zl,0:J+1), ∀l=1,...,K.
Il ψ ψ
Step 3 Now, we are ready to prove that LK,J(ϕ,ψ) ≤ LK,J+1(ϕ,ψ) ≤ LK(ϕ,ψ),∀J. The gap between LK
and LK,J can be expressed as
LK(ϕ,ψ)−LK,J(ϕ,ψ)
 
PK P(Y|τk,qk)P(τk,qk)
=E ⟨τk∼Qϕ(τ)⟩K k=1E ⟨zk,0:J∼Qψ(z|τk)⟩K k=1E (cid:10) qk∼QJ ψ(q|τk,zk,0:J)(cid:11)K k=1log PK k=k 1= Q1 ϕP (Q ( τYϕ k|( )ττ Qkk
J
ψ,) qQ (k qψ ) kP |( τq ( kτk ,k| zτ , kqk ,k) 0) :J).
(cid:18) QJ (τ1:K,q1:K,z1:K,0:J)(cid:19)
=E ⟨τk∼Qϕ(τ)⟩K k=1E ⟨zk,0:J∼Qψ(z|τk)⟩K k=1E (cid:10) qk∼QJ ψ(q|τk,zk,0:J)(cid:11)K k=1log f ϕJϕ ,, ψψ (τ1:K,q1:K,z1:K,0:J)
=KL(cid:0) QJ (τ1:K,q1:K,z1:K,0:J)∥fJ (τ1:K,q1:K,z1:K,0:J)(cid:1)
ϕ,ψ ϕ,ψ
23This proves that LK,J(ϕ,ψ)≤LK(ϕ,ψ). Using a similar argument,
LK,J+1(ϕ,ψ)−LK,J(ϕ,ψ)
 
PK P(Y|τk,qk)P(τk,qk)
=E ⟨τk∼Qϕ(τ)⟩K k=1E ⟨zk,0:J+1∼Qψ(z|τk)⟩K k=1E (cid:10) qk∼QJ ψ(q|τk,zk,0:J+1)(cid:11)K k=1log  Pk K k= =1 1Q Qϕ ϕ( Pτ (( τk Y k)Q | )τ QJ ψ k
J
ψ+ ,q1 (k q(q ) kPk |τ| (τ kτk ,k z, ,z kqk ,k 0, ) :0 J:J ))  .
(cid:18) QJ+1(τ1:K,q1:K,z1:K,0:J+1)(cid:19)
=E ⟨τk∼Qϕ(τ)⟩K k=1E ⟨zk,0:J+1∼Qψ(z|τk)⟩K k=1E (cid:10) qk∼QJ ψ(q|τk,zk,0:J+1)(cid:11)K k=1log hJ ϕϕ ,, ψψ (τ1:K,q1:K,z1:K,0:J+1) .
=KL(cid:0) QJ+1(τ1:K,q1:K,z1:K,0:J+1)∥hJ (τ1:K,q1:K,z1:K,0:J+1)(cid:1) .
ϕ,ψ ϕ,ψ
This proves that LK,J(ϕ,ψ)≤LK,J+1(ϕ,ψ).
B.2 Proof of Theorem 3
WewillproveTheorem3followingasimilarthreestepsprocedureasintheAppendixB.1. Notethatasymptot-
ically unbiasedness of LK,J(ϕ,ψ,ξ) is still a direct result of the strong law of large numbers.
w
Step 1 We first derive alternative expressions for LK,J(ϕ,ψ,ξ) and LK(ϕ,ψ). Let HJ (qk,zk,0:J|τk) =
w ψ,ξ
1 PJ Qψ(zk,j|τk)Qψ(qk|τk,zk,j) and
J+1 j=0 Rξ(zk,j|τk,qk)
K
Y
QJ (τ1:K,q1:K,z1:K,0:J)= HJ (qk,zk,0:J|τk)R (zk,0:J|τk,qk)Q (τk).
ϕ,ψ,ξ ψ,ξ ξ ϕ
k=1
Note that QJ (τ1:K,q1:K,z1:K,0:J) is indeed a valid proability density function. By symmetry,
ϕ,ψ,ξ
LK,J(ϕ,ψ,ξ)
w
!
1 XJ 1 XK P(Y|τk,qk)P(τk,qk)
= E E log
J+1
j=0
⟨(τk,qk,zk,j)∼Qϕ,ψ(τ,q,z)⟩K
k=1
⟨zk,(0:J)\j∼Qψ(z|τk,qk)⟩K
k=1
K
k=1
Q ϕ(τk)H ψJ ,ξ(qk,zk,0:J|τk)
!
1 XK P(Y|τk,qk)P(τk,qk)
=E log .
(τ1:K,q1:K,z1:K,0:J)∼QJ ϕ,ψ,ξ(τ1:K,q1:K,z1:K,0:J) K
k=1
Q ϕ(τk)H ψJ ,ξ(qk,zk,0:J|τk)
Using the fact that ˆ
QJ (τ1:K,q1:K,z1:K,0:J)dz1:K,0:J =Q (q1:K,τ1:K)
ϕ,ψ,ξ ψ
we can rewrite LK(ϕ,ψ) as
!
1 XK P(Y|τk,qk)P(τk,qk)
LK(ϕ,ψ)=E log .
(τ1:K,q1:K,z1:K,0:J)∼QJ ϕ,ψ,ξ(τ1:K,q1:K,z1:K,0:J) K Q ϕ(τk)Q ψ(qk|τk)
k=1
Therefore, the LK,J(ϕ,ψ,ξ) and LK(ϕ,ψ) share the same reference distribution in expectation, as in Appendix
w
B.1.
Step 2 Next, we will show the following two functions are both probability density functions:
 PK P(Y|τk,qk)P(τk,qk)
 f ϕJ ,ψ,ξ(τ1:K,q1:K,z1:K,0:J) = k P=1 KQϕ P(τ (k Y)H |τψJ k, ,ξ qk(q )k P, (z τk k, ,0 q:J k| )τk) QJ ϕ,ψ,ξ(τ1:K,q1:K,z1:K,0:J);
k=1 Qϕ(τk)Qψ(qk|τk)
PK P(Y|τk,qk)P(τk,qk)
 hJ ϕ,ψ,ξ(τ1:K,q1:K,z1:K,0:J+1) = PKk=1Qϕ P(τ (k Y)H |τψJ k, ,ξ qk(q )k P, (z τk k, ,0 q:J k| )τk) QJ ϕ+ ,ψ1 ,ξ(τ1:K,q1:K,z1:K,0:J+1)
k=1Qϕ(τk)H ψJ+ ,ξ1(qk,zk,0:J+1|τk)
24Integrating out z1:K,0:J in fJ (τ1:K,q1:K,z1:K,0:J) yields
ϕ,ψ,ξ
ˆ
fJ (τ1:K,q1:K,z1:K,0:J)dz1:K,0:J
ϕ,ψ,ξ
ˆ " #
K K
1 X Y Y
= · P(Y|τk,qk)P(τk,qk) HJ (ql,zl,0:J|τl)Q (τl) R (zl,0:J|τl,ql)dz1:K,0:J
PK P(Y|τk,qk)P(τk,qk) ψ,ξ ϕ ξ
k=1 Qϕ(τk)Qψ(qk|τk) k=1 l̸=k l=1
PK P(Y|τk,qk)P(τk,qk)Q Q (τl)Q (ql|τl)
= k=1 l̸=k ϕ ψ
PK P(Y|τk,qk)P(τk,qk)
k=1 Qϕ(τk)Qψ(qk|τk)
PK P(Y|τk,qk)P(τk,qk) ·QK Q (τl)Q (ql|τl)
=
k=1 Qϕ(τk)Qψ(qk|τk) l=1 ϕ ψ
PK P(Y|τk,qk)P(τk,qk)
k=1 Qϕ(τk)Qψ(qk|τk)
K
Y
= Q (τl)Q (ql|τl)
ϕ ψ
l=1
which just the joint variational distribution of (τ1:K,q1:K). Therefore, fJ (τ1:K,q1:K,z1:K,0:J) is a valid
ϕ,ψ,ξ
probability density function.
To prove hJ (τ1:K,q1:K,z1:K,0:J+1) is a valid probability density function, it suffices to show
ϕ,ψ,ξ
PK P(Y|τk,qk)P(τk,qk)
E
k=1 Qϕ(τk)H ψJ ,ξ(qk,zk,0:J|τk)
=1.
QJ ϕ+ ,ψ1 ,ξ(τ1:K,q1:K,z1:K,0:J+1)PK P(Y|τk,qk)P(τk,qk)
k=1 Qϕ(τk)H ψJ+ ,ξ1(qk,zk,0:J+1|τk)
Let {I : I ⊂ {0,...,J +1},|I | = J +1,k = 1,...,K} be uniformly distributed subsets with distinct indices
k k k
from {0,...,J+1}. Let HJ (qk,zk,Ik|τk)= 1 P Qψ(zk,j|τk)Qψ(qk|τk,zk,j) and
ψ,ξ J+1 j∈Ik Rξ(zk,j|τk,qk)
K
Y
QJ (τ1:K,q1:K,z1:K,I1:K)= HJ (qk,zk,Ik|τk)R (zk,Ik|τk,qk)Q (τk).
ϕ,ψ,ξ ψ,ξ ξ ϕ
k=1
By symmetry, we have
K
Y
E QJ (τ1:K,q1:K,z1:K,I1:K) R (zk,−Ik|τk,qk)=QJ+1 (τ1:K,q1:K,z1:K,0:J+1)
I1:K ϕ,ψ,ξ ξ ϕ,ψ,ξ
k=1
25where −I =(0:J+1)\I , and thus
k k
PK P(Y|τk,qk)P(τk,qk)
E
k=1 Qϕ(τk)H ψJ ,ξ(qk,zk,0:J|τk)
QJ ϕ+ ,ψ1 ,ξ(τ1:K,q1:K,z1:K,0:J+1)PK P(Y|τk,qk)P(τk,qk)
k=1 Qϕ(τk)H ψJ+ ,ξ1(qk,zk,0:J+1|τk)
=E E
PK
k=1
QϕP (τ(Y k)| Hτk ψJ, ,q ξk () qP k( ,zτ kk ,, Iq kk |) τk)QK k=1R ξ(zk,−Ik|τk,qk)
I1:K QJ ϕ,ψ,ξ(τ1:K,q1:K,z1:K,I1:K) PK P(Y|τk,qk)P(τk,qk)
k=1 Qϕ(τk)H ψJ+ ,ξ1(qk,zk,0:J+1|τk)
=E E
ˆ ˆ PK
k=1
P(Y|τk Q,q ϕk (τ)P k)(τk,qk)R ξ(zk,0:J+1|τk,qk)Q l̸=kH ψJ ,ξ(ql,zl,Il|τl)R ξ(zl,0:J+1|τl,ql)
dz1:K,0:J+1 dq1:K
Qϕ(τ1:K) I1:K PK P(Y|τk,qk)P(τk,qk)
k=1 Qϕ(τk)H ψJ+ ,ξ1(qk,zk,0:J+1|τk)
ˆ ˆ PK P(Y|τk,qk)P(τk,qk)R (zk,0:J+1|τk,qk)Q HJ+1(ql,zl,0:J+1|τl)R (zl,0:J+1|τl,ql)
=E k=1 Qϕ(τk) ξ l̸=k ψ,ξ ξ dz1:K,0:J+1 dq1:K
Qϕ(τ1:K) PK P(Y|τk,qk)P(τk,qk)
k=1 Qϕ(τk)H ψJ+ ,ξ1(qk,zk,0:J+1|τk)
ˆ ˆ PK P(Y|τk,qk)P(τk,qk) QK HJ+1(ql,zl,0:J+1|τl)R (zl,0:J+1|τl,ql)
=E
k=1 Qϕ(τk)H ψJ+ ,ξ1(qk,zk,0:J+1|τk) l=1 ψ,ξ ξ
dz1:K,0:J+1 dq1:K
Qϕ(τ1:K) PK P(Y|τk,qk)P(τk,qk)
k=1 Qϕ(τk)H ψJ+ ,ξ1(qk,zk,0:J+1|τk)
ˆ ˆ
K
Y
=E HJ+1(ql,zl,0:J+1|τl)R (zl,0:J+1|τl,ql)dz1:K,0:J+1 dq1:K
Qϕ(τ1:K) ψ,ξ ξ
l=1
=1.
Therefore, hJ (τ1:K,q1:K,z1:K,0:J+1) is a valid probability density function.
ϕ,ψ,ξ
Step 3 Now,wearereadytoprovethatLK,J(ϕ,ψ,ξ)≤LK,J+1(ϕ,ψ,ξ)≤LK(ϕ,ψ), ∀J. Thegapbetween
w w
LK,J(ϕ,ψ,ξ) and LK(ϕ,ψ) is
w
LK(ϕ,ψ)−LK,J(ϕ,ψ,ξ)
w
 
PK P(Y|τk,qk)P(τk,qk)
=E
(τ1:K,q1:K,z1:K,0:J)∼QJ
ϕ,ψ,ξ(τ1:K,q1:K,z1:K,0:J)log
PK
k=1 PQ (Yϕ |( ττ kk ,) qQ kψ )P(q (τk k|τ ,qk k)
)
.
k=1 Qϕ(τk)H ψJ ,ξ(qk,zk,0:J|τk)
(cid:18) QJ (τ1:K,q1:K,z1:K,0:J)(cid:19)
=E log ϕ,ψ,ξ
(τ1:K,q1:K,z1:K,0:J)∼QJ ϕ,ψ,ξ(τ1:K,q1:K,z1:K,0:J) f ϕJ ,ψ,ξ(τ1:K,q1:K,z1:K,0:J)
=KL(cid:0) QJ (τ1:K,q1:K,z1:K,0:J)∥fJ (τ1:K,q1:K,z1:K,0:J)(cid:1) .
ϕ,ψ,ξ ϕ,ψ,ξ
This proves LK,J(ϕ,ψ,ξ)≤LK(ϕ,ψ). The gap between LK,J(ϕ,ψ,ξ) and LK,J+1(ϕ,ψ,ξ) is
w w w
LK,J+1(ϕ,ψ,ξ)−LK,J(ϕ,ψ,ξ)
w w
 
PK P(Y|τk,qk)P(τk,qk)
=E log
k=1 Qϕ(τk)H ψJ+ ,ξ1(qk,zk,0:J+1|τk)
.
(τ1:K,q1:K,z1:K,0:J+1)∼QJ ϕ+ ,ψ1 ,ξ(τ1:K,q1:K,z1:K,0:J+1)  PK P(Y|τk,qk)P(τk,qk) 
k=1 Qϕ(τk)H ψJ ,ξ(qk,zk,0:J|τk)
(cid:18) QJ+1 (τ1:K,q1:K,z1:K,0:J+1)(cid:19)
=E log ϕ,ψ,ξ .
(τ1:K,q1:K,z1:K,0:J+1)∼QJ+1 (τ1:K,q1:K,z1:K,0:J+1) hJ (τ1:K,q1:K,z1:K,0:J+1)
ϕ,ψ,ξ ϕ,ψ,ξ
=KL(cid:0) QJ+1 (τ1:K,q1:K,z1:K,0:J+1)∥hJ (τ1:K,q1:K,z1:K,0:J+1)(cid:1) .
ϕ,ψ,ξ ϕ,ψ,ξ
This proves LK,J(ϕ,ψ,ξ)≤LK,J+1(ϕ,ψ,ξ).
w w
26