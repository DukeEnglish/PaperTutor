Beyond Closure Models: Learning Chaotic-Systems
via Physics-Informed Neural Operators
ChuweiWang1,JuliusBerner1,ZongyiLi1,DiZhou2,JiayunWang1,
JaneBae2,AnimaAnandkumar1∗
1DepartmentofComputingandMathematicalSciences,Caltech
2GraduateAerospaceLaboratories,Caltech
{chuweiw, jberner, zongyili, dizhou, peterw, jbae, anima}@caletch.edu
Abstract
Accurately predicting the long-term behavior of chaotic systems is crucial for
variousapplicationssuchasclimatemodeling.However,achievingsuchpredictions
typicallyrequiresiterativecomputationsoveradensespatiotemporalgridtoaccount
for the unstable nature of chaotic systems, which is expensive and impractical
in many real-world situations. An alternative approach to such a full-resolved
simulationisusingacoarsegridandthencorrectingitserrorsthroughaclosure
model,whichapproximatestheoverallinformationfromfinescalesnotcaptured
inthecoarse-gridsimulation. Recently,MLapproacheshavebeenusedforclosure
modeling, but they typically require a large number of training samples from
expensivefully-resolvedsimulations(FRS).Inthiswork,weproveanevenmore
fundamental limitation, i.e., the standard approach to learning closure models
suffersfromalargeapproximationerrorforgenericproblems,nomatterhowlarge
themodelis,anditstemsfromthenon-uniquenessofthemapping. Weproposean
alternativeend-to-endlearningapproachusingaphysics-informedneuraloperator
(PINO)thatovercomesthislimitationbynotusingaclosuremodeloracoarse-grid
solver. WefirsttrainthePINOmodelondatafromacoarse-gridsolverandthen
fine-tuneitwith(asmallamountof)FRSandphysics-basedlossesonafinegrid.
Thediscretization-freenatureofneuraloperatorsmeansthattheydonotsuffer
fromtherestrictionofacoarsegridthatclosuremodelsface,andtheycanprovably
approximatethelong-termstatisticsofchaoticsystems. Inourexperiments,our
PINOmodelachievesa120xspeedupcomparedtoFRSwitharelativeerror∼5%.
Incontrast,theclosuremodelcoupledwithacoarse-gridsolveris58xslowerthan
PINOwhilehavingamuchhighererror∼205%whentheclosuremodelistrained
onthesameFRSdataset.
1 Introduction
Predictinglong-termbehaviorisanimportanttaskinmanyphysicalsystems,e.g.,climatemodeling,
aircraft design, and plasma evolution in nuclear fusion [1–5]. This can be framed as estimating
statisticsofasysteminitsdynamicalequilibrium. Toreducecostlyorevenimpracticalphysical
experiments,simulationsarewidelyadoptedforestimatingsuchlong-termstatistics. However,one
majorchallengefornumericalsimulationsisthatmanyphysicalsystemsarechaoticandhaveextreme
sensitivitytoperturbations[6–9]. Smallerrorsaccumulateovertime,leadingtolargedivergencesin
trajectoriesinchaoticsystems.
Toaccountfortheunstablenatureofchaoticsystems,high-fidelitysimulationshavetobecarried
outonextremelyfinespatiotemporalgridstomakediscretizationerrorssufficientlysmallsothat
∗Correspondenceto:AnimaAnandkumar<anima@caltech.edu>.
Preprint.Underreview.
4202
guA
9
]GL.sc[
1v77150.8042:viXratheoverallerroralongthetrajectorydoesnotgrowrapidly. Thismakesfully-resolvedsimulations
(FRS),e.g. directnumericalsimulations(DNS)inturbulence,prohibitivelyexpensiveintermsof
bothcomputationtimeandmemory. Asanexample,theFRSsimulationofasmallregioninthe
atmospheretakesseveralmonthsandpetabytesofmemory[10,11].
Given the computation cost of FRS in chaotic dynamics and the fact that the ultimate goal is to
evaluatethelong-termstatisticsinsteadoftrackinganyindividualtrajectory,manyworkshavebeen
exploringwaystogiveagoodestimateofsuchstatisticswithsimulationsonlyconductedoncoarse
spatialgrids,e.g.,large-eddysimulation(LES)[12,13]forturbulentflows. Tocorrecttheerrors
introducedbyLESorothercoarse-gridsolversincalculatinglong-termstatisticsofchaoticsystems,
apopularapproachistouseaclosuremodelinconjunctionwiththesolver[14–16]. Thisapproachis
alsoknownascoarse-grainedmodeling[17]orrenormalizationgroupsinsomedisciplines[18].
There are multiple approaches to designing closure models. The traditional framework is based
on physical intuition, which requires substantial domain expertise or derived by mathematical
simplification for which strong modeling assumptions are needed. Hence, such approaches are
typicallyinaccurateforreal-worldsystems[19–21].
To improve the expressivity of closure models and reduce modeling errors, this past decade has
witnessedextensivedevelopmentofmachine-learningmethodsforclosuremodeling[22–26]. While
suchlearnedclosuremodelsrepresentanimprovementovertraditionalhand-craftedapproaches,they
stillsufferfromsomefundamentallimitationsinestimatinglong-termstatisticsofchaoticsystems.
Thelearnedclosuremodelisconstrainedtobeonthesamecoarsespatialgridasthenumericalsolver
thatiterativelyevolveswith relativelysmall timesteps. Infact, someclosuremodelingmethods
further require the coarse-grid simulation to start from a downsampled version of a high-fidelity
simulationclosetoitsdynamicalequilibriumtoobtainaccurateresults. Moreover,theselearning-
basedclosuremodels[27–30]oftenrelyonalargeamountofhigh-fidelitytrainingdatagenerated
fromexpensiveFRS,whichmayevenbeimpossibletogenerateformanyproblemsofinterest.
OurApproach: Inthiswork,weprovideanewtheoreticalunderstandingofestimatinglong-term
statisticsofachaoticsystem. WeformallyproveinTheorem3.1thatforgenericproblems,previous
learningmethodsbasedonclosuremodelsarefundamentallyill-posedsincetheyareconstrained
tobeonthesamecoarsegridasthesolver,andtheycannotaccuratelyapproximatetheunderlying
chaoticsystem. Specifically,weprovethatthemappingthatclosuremodelsattempttolearnina
reducedspace(coarsegrid)isnon-unique,i.e.,therearemultiplepotentialoutputsforagiveninput
(fig.1,left). Hence,thestandardapproachtolearningclosuremodelsundersuchnon-uniqueness
results in the average of possible outputs, and that cannot accurately approximate the long-term
statisticsofachaoticsystem,nomatterhowlargeorexpressivetheclosuremodelis.
Figure1: Left: Manypoints(e.g.,u andu )oftheground-truthattractorΩ(i.e.,equilibriumstate,
1 2
blue) map to the same filtered (e.g., downsampled) value u (gray), making it impossible for the
closuremodeltoidentifythecorrectdynamics(F(Au )andF(Au ))inthefilteredspaceF(H).
1 2
Byminimizingthelossfunction,themodellearnstopredictanaverageofthesemultiplechoices
(green arrow), which leads the simulation to wrongly diverge from the filtered attractor. Right:
Totalvariationdistancefromground-truthinvariantmeasureversuscomputationcost. ‘FRS’(blue
line): gold-standardfully-resolvedsimulations. ‘CGS’:coarse-gridsimulationwithoutclosuremodel.
‘Smag.’: Smagorinskymodel.‘Single’: learning-basedsingle-statemodel. Ourmethodisthefastest
andclosesttogroundtruth(‘FRS’)amongallcoarse-gridmethods(‘CGS’,‘Smag.’,‘Single’).
2Table 1: Comparison between different approaches for predicting long-term statistics of Navier-
Stokesequations. TheReynoldsnumberReislargeinmostapplications. Thetoptwoareclassical
approaches,andtherestaremachinelearningapproaches. Trainingdataiscountedinthenumber
ofsnapshotsandtrajectories. Thecomplexitytakesintoaccountbothspatialgridsandtemporal
grids. Ourapproachisevencheaperthancoarse-gridsimulationsbecauseourscanevolvewithO(1)
timestepinsteadofsmalltime-gridsfollowingtheCFLcondition,asisthecaseforothermethods
utilizingacoarsesolver. δtisthetime-gridsizeforlatentSDEin[34].
Optimal High-res. trainingdata
Method (cid:12) Complexity
statistics FRSSnapshots(cid:12)Trajs.
Fully-resolvedSimulation,e.g.,DNS[35,36] ✓ - Re3.52
Coarse-gridSimulation,e.g.,LES[35,36] ✗ - Re2.48
Single-statemodel[30] ✗ 24000(cid:12) (cid:12)8 Re2.48
History-awaremodel[37] ✗ 250000(cid:12) (cid:12)50 Re2.48
LatentNeuralSDE[34] ✗ 179200(cid:12) (cid:12)28 1Re1.86
δt
OnlineLearning[38] ✗ - Re3.52
Physics-InformedOperatorLearning(Ours) ✓ 110(cid:12) (cid:12)1 Re1.86
Wefurtherproposeanalternativeend-to-endMLframeworktomitigatethisissueofnon-uniqueness
with closure models. We remove the constraint that the learned model is on a coarse grid and
insteademployagrid-freeapproachtolearning. Itisbasedonneuraloperators[31,32],whichlearn
mappingsbetweenfunctionspaces,asopposedtostandardneuralnetworksthatarelimitedtoafixed
grid. Inaneuraloperator,inputsofdifferentresolutionsareviewedasdifferentdiscretizationsofthe
samefunctiononacontinuousdomain,thusensuringconsistencybetweencoarseandfinegrids.
Wetrainaneuraloperatormodelfirst,ondataobtainedfromcoarse-gridsolvers. Sincesuchsolvers
arerelativelycheaptorun,wecanobtainsufficientdatatotraintheneuraloperatortoaccurately
emulatethecoarse-gridsolver. However,thisisnotsufficient,sinceourgoalistoobtaintheaccuracy
ofFRS.Todothis,insteadofemployingaseparateclosuremodel,wefine-tunetheneuraloperator,
alreadytrainedonacoarse-gridsolver,using(asmallamountof)FRSdataandphysics-basedlosses
definedonafinegrid. Sinceneuraloperatorscanoperateonanygrid,wecanemploythesamemodel
totrainondatafrombothcoarseandfine-gridsolvers. Theadditionofphysics-basedlossesonafine
gridfurtherreducestheFRSdatarequirementandimprovesgeneralization,inlinewithwhathas
beenseeninpriorworksonphysics-informedlearning[33].
Theclassofneuraloperatorshasbeenpreviouslyestablishedasuniversalapproximationsinfunction
spaces [32, 39], meaning they can accurately approximate any continuous operator. We further
strengthenthisresulthereandproveinTheorem3.1thataneuraloperatorthatapproximatesthe
underlyingground-truthsolutionoperatorcanprovidesufficientlyaccurateestimatesofthelong-term
statisticsofachaoticsystem,andthereisnocatastrophicbuild-upoferrorsoverlongrollouts. We
derivealltheabovetheoreticalresultsthroughthelensofmeasureflowinfunctionspaces,introducing
anoveltheoreticalframework,viz.,functionalLiouvilleflow.
Wetesttheperformanceofourapproachinseveralinstancesfromfluiddynamics. OurPINOmodel
achieves120xspeedupwitharelativeerror∼5%comparedtoFRS.Incontrast,theclosuremodel
coupledwithacoarse-gridsolveris58xslowerthanPINOwhilehavingamuchhighererror∼205%.
Moredetailsaboutthespeed-accuracyperformanceofdifferentapproachesareshowninfigure1.
Theclosuremodelhasasignificantlyhighererrorunderoursetupcomparedtopriorworks[30,29,40].
ThisisbecauseweassumethatonlyafewFRSsamplesareavailablefortraining,bothforPINOand
theclosuremodel,viz.,just110-timestepsfromasingleFRStrajectory. Incontrast,priorworkson
closuremodelsassumethousandsoftimestepsoverhundredsofFRStrajectories. Inparticular,we
showthattomitigatethenon-uniqueissueofclosuremodelsmentionedabove,previousmethodshave
toresorttotheclosenessbetweenthelimitdistributionintheoriginaldynamicsandtheempirical
measureoftrainingdata.Moreover,priorworksassumestartingthesimulationclosetothedynamical
equilibrium of the chaotic system, whose estimation requires long rollouts of FRS, which is not
realistic. Instead,werandomlyinitializebothPINOandclosuremodelswithoutanypriorknowledge,
andmeasuretheirperformance.
3Anillustrativecomparisonbetweenourmethodandrepresentativeexistingmethodscanbefoundin
Table1. Ourcontributionsaresummarizedasfollows.
• WeproposeanovelframeworkbasedonfunctionalLiouvilleflow, totheoreticallyanalyzethe
problemofestimatinglong-termstatisticsofchaoticsystemswithcoarse-gridsimulations.
• We formally prove that restricting the learning object in the reduced space, as existing closure
modelsdo,suffersfromthenon-uniquenessofthelearningtarget.
• Weleveragephysics-informedneuraloperatorasanalternativeapproachthatcombineslearningon
datafrombothcoarseandfine-gridsolvers,andphysics-basedlosses. Weprovideboththeoretical
andempiricalevidenceofitssuperiority.
2 BackgroundandExistingMethods
We formally introduce the problem setting of evaluating long-term statistics as well as existing
numerical and machine learning methods. We will show the potential shortcomings of previous
learningmethodsandstatesomeofourtheoreticalresults. SeeAppendixAformorebackgrounds.
2.1 ProblemBackground
Consideranevolutionpartialdifferentialequation(PDE)thatgovernsa(nonlinear)dynamicalsystem
inthefunctionspace,
(cid:26)
∂ u(x,t)=Au(x,t)
t (1)
u(x,0)=u (x), u ∈H,
0 0
whereu istheinitialvalueandHisafunctionspacecontainingfunctionsofinterests,e.g.,fluid
0
field,temperaturedistribution,etc;seeAppendixA.4forfurtherassumptions. Thisequationnaturally
inducesasemigroup{S(t)} definedasthemappingfromtheinitialstatetothestateattimet,
t≥0
S(t):u →u(·,t). Werefertotheset{S(t)u } astrajectoryfromu .
0 0 t≥0 0
Attractor, invariant measure, long-term statistics: The (global) attractor of the dynamics is
definedasthemaximalinvariantsetof{S(t)} towardswhichalltrajectoriesconvergeovertime.
t
Formanyrelevantsystems,theexistenceofacompactattractoriseitherrigorouslyproved[41,42]or
demonstratedbyextensiveexperiments[43–45]. Theinvariantmeasureisthetimeaverageofany
trajectory,independentofinitialvalueaslongasthesystemisergodic,
1 (cid:90) T
µ∗ := lim δ dt, u∈H, a.e. (2)
T→∞T
t=0
S(t)u
whereδistheDiracmeasure. µ∗ isameasureoffunctionsandissupportedontheattractor. Intu-
itively,theinvariantmeasurecapturesthesystem’slong-termbehaviorwhenitreachesadynamical
equilibrium. Thelong-termstatisticsareexpectationsoffunctionalsontheinvariantmeasure. The
moststraightforwardapproachtoestimatestatisticsistorunanaccuratesimulationoftrajectory
andcomputefollowingthedefinition. Inpractice,wefirstfixasufficientlylargeT andthenchoose
spatiotemporalgridsizeaccordinglysothattheoverallerrorofthesimulationwithin[0,T]remains
small. Wewillrefertothisapproachashigh-fidelitysimulationsorFRS.
Chaoticsystems,characterizedbypositiveLyapunovexponents[46],areknownfortheirextreme
sensitivitytoperturbationsandcatastrophicaccumulationofsmallerrorsovertime. Toaccountfor
theunstablenatureofchaoticsystems,high-fidelitysimulationshavetobecarriedoutonverydense
spatiotemporalgridstomakediscretizationerrorssmallenoughsothattheoverallerroralongthe
trajectorydoesnotgrowrapidly. ThismakestheFRSapproachprohibitivelyexpensive.
2.2 Coarse-gridSimulationandClosureModeling
GiventhecomputationcostofFRSandthefactthattheultimategoalistoevaluatethestatistics
insteadofanyindividualtrajectory,manyworkshavebeenexploringwaystogivegoodestimations
of statistics with simulations only conducted on coarse grids. We will refer to this approach as
coarse-gridsimulation(CGS).Itservesasthecorefocusofthiswork. Letusformalizeitasfollows.
DenotebyDthesetofgridpointsusedinFRSandD′thatinCGS,with|D′|≪|D|. Simulatingon
D′couldbeviewedasevolvingafilteredfunctionudefinedasu=Fu,whereF isalinearfiltering
4operator. Forinstance,F isaspatialconvolutionforcaseslikedown-samplinginthefinitedifference
methodandFourier-modetruncationinthespectralmethod.
Theoretically, the evolution of u is governed by ∂ u = FAu = Au+(FA−AF)u, where F
t
andAarenotcommutingduetothenonlinearityofA. However,thecommutator(FA−AF)uis
intractableifrestrictedtoD′sinceuisunderresolved. Toaccountfortheeffectofsmallscalesnot
capturedbyD′,inmanyCGSmethodsanadjustingtermclos(u;θ)(θdenotesthemodelparameters),
knownasclosuremodel,isaddedtotheequationasatractablesurrogatemodelof(FA−AF)u.
TheCGStrajectoryisderivedbysimulating
(cid:26) ∂ v(x,t)=Av(x,t)+clos(v;θ), x∈D′
t (3)
v(x,0)=u (x), u ∈F(H),
0 0
andthestatisticsareestimatedasthetimeaverageofthecorrespondingfunctionalswithv(·,t)input.
Weusethenotationvinsteadofuheretounderscorethedifferencebetweencoarse-gridtrajectories
andfilteredfully-resolvedtrajectories,astheyfollowdifferentdynamicsingeneral.
Classical Closure Models: Closure modeling is a classical and relevant topic in computational
methods for science and engineering, with rich literature available [19, 20]. Despite their wide
applicationinnumerousscenarios,thedesignofclosuremodelsismoreofanartthanscience. Many
existingmethodsaregroundedinphysicalintuitionorderivedbymathematicalsimplificationsthat
incorporatestrongassumptions,whichoftendonotholdupundergeneralconditions. Additionally,
selectingparametersintheseclosuremodelstypicallyrequiressubstantialdomainexpertise. Nev-
ertheless,forseveralpracticalapplications,suchsimplemodelingassumptionsarenotsufficientto
capturethecomplexoptimalclosure[21].
In recent years, there has been a growing interest in leveraging machine learning tools to design
closuremodels(see[24]forsurvey). Wesummarizethemainstreammethodologiesasfollows,along
withtheinformalversionofourtheoreticalresults,whichrevealstheirpotentialshortcomings.
Learningsingle-stateclosuremodel: Broadlyspeaking,neuralnetwork-representedclosuremodels
clos(v;θ)areproposedwithvariousansatzandneuralnetworkarchitectures,andtheyaretrainedby
minimizinganapriorilossfunctionaimingatfittingthecommutator[24],
1 (cid:88)
J (θ;D)= ∥clos(u ;θ)−(FA−AF)u ∥2, (4)
ap |D| i i
i∈D
wherethetrainingdatau comefromsnapshotsofFRStrajectories,i.e.,S(t)u forparticulart.
i 0
Thismethodologyappearsunconvincingwhenwescrutinizetheinputandoutputofthemodel. Due
tothedimensionreductionnatureoffilterF,therearemultipleFRSsnapshotsu ∈Hthatendup
i
inthesamestateinthereducedspaceF(H)(orfunctionsrestrictedonD′,equivalently). However,
thefilteringoftheoriginalvectorfieldAu(describingthemovingdirectionforthenexttimestep),
differsattheseu . TogetherwiththeAvtermin(3),theclosuremodelplaystheroleofassigninga
i
uniquemovingdirectioninthefilteredspaceF(H)foreachstateu. Byminimizingthetrainingloss
(4),themodeltypicallylearnstopredictthereducedvectorfieldatuastheaverage(insomesense)
ofalltheseF(Au ),whichmightnotmakesense. AnillustrationisshowninFig. (1). Toclarify,
i
wepointoutthedifferencebetweenourproblemandmanyproblemsininverseproblemsandlinear
regressionwheremakinganaveragingpredictionislogical. Modelpredictionsneedtoadheretothe
manifoldstructurecharacterizedbyphysicaltrajectories.
Therearealsoextensionsofthelearningframeworkabove. Someworksproposedtoaddaposterior
lossintothetrainingobject[47,48],
1 (cid:88)
J (θ;D)=J (θ)+ ∥v (·,∆t;θ)−F(S(∆t)u )∥2, (5)
post ap |D| i i
i∈D
where v comes from evolving (3) with Fu initialization for a time period ∆t. Clearly, this
i i
modificationstillsuffersfromtheissueresultingfromtheunderdeterminancydiscussedabove,notto
mentionitsheavyloadofbackpropagationthroughthenumericalsolverduringtraining.
Learninghistory-awareclosuremodel: Asshownabove,themainissueoflearningasingle-state
closuremodel(i.e.,theoutputoftheclosuremodelonlydependsonu)isthatthe‘mapping’fromuto
uandcorrespondingly(FA−AF)uisnotawell-definedmapping,wheremultipleoutputsarerelated
5toonesingleinput. Tohandlethisissue,someworks[40,37]proposetotakeaccountofhistory
informationinthereducedspace,namelyaclosuremodelwhoseinputis{u(x ,t−s)}
i xi∈D′,0<s≤t0
atthemomentt,wheret isamodelparameter.
0
Stochastic formulation of closure model: Another direction to handle the ill-posed issue is to
replacethedeterministicclosuremodelwithastochasticone[34,49]. Thislineofworkisinspiredby
[50],whichshowsthattheoptimalchoiceofclosuremodelhastheformofaconditionalexpectation.
However,weprovethatnoneoftheseapproachesresolvethenon-uniquenessissue. Weinformally
summarizeourresultsasfollows.
Theorem2.1. (i)Ingeneral,themappingofclosuremodelsu→(FA−AF)uisnotwell-defined.
Consequently,theapproximationerrorhasalowerboundindependentofthemodelcomplexity.
(ii)Foranyuandfiniteτ,thereexistinfiniteu′ ∈HsuchthatFS(t)u′ =FS(t)uforallt∈[0,τ).
(iii)Onecannotobtainthebestapproximationofµ∗amongdistributionssupportedinthereduced
spaceifthereisrandomnessintheevolutionofdynamics.
TheproofcanbefoundinAppendixC.Foroursecondclaim,since{FS(t)u} containsthe
x∈D′,t<τ
entireinformationthatcouldbeusedtopredicttheclosuretermattimeτ,andu′(·,t)willdeviate
dramaticallyfromu(·,t)inachaoticsystem,theunder-determinancyissueremainsunsolvedwith
history-awaremodels. Weremarkherethattherehavebeensometheoreticalresultsstatinghistorical
informationinthereducedspacesufficestorecovertheunderlyingtruetrajectory[51],buttheyare
allderivedinODEsystemsandhighlyrelyonthefinite-dimensionalityofODEsystems. Forour
thirdclaim,asanimplication,theparametersrelatedtorandomnessinthestochasticclosuremodel
willbeclosetozeroaftertraining. Thus,introducingrandomnessinthemodelmightberedundant.
Besidesthethreemainstreamlearning-basedclosuremodelingwelooselyclassifiedabove,thereare
otherapproachesthatresorttoaninteractiveuseoffine-gridsimulators[52,53]andleveragingonline
learningalgorithms[54,38,23,47]. However,callingandauto-differentiatingalongfully-resolved
simulationsmakethetrainingofthisapproachprohibitivelyexpensive.
2.3 PerspectivethroughLiouvilleFlowinFunctionSpace
Wehavedemonstratedthatexistinglearningmethodstargetanon-uniquemapping,resultinginan
averageofallpossibleoutputs,whichcanbeundesirable. Despitethis,thesemethodsstillmanageto
achievecompetitiveperformance. Inthissection,weshowthattheirempiricalresultheavilyrelieson
theavailabilityofalargeamountofFRStrainingdata. Thisdependencyisasignificantlimitation,as
FRSdataaretypicallyscarce. IfasufficientamountofFRSdatawerealreadyavailablefortraining,
wecoulddirectlycomputethestatisticsusingthedata,eliminatingtheneedfortrainingaclosure
modelorrunningcoarse-gridsimulations.
Ouranalysisinvestigatestheevolutionofthedistribution(ormeasure)todeterminewhetheritcon-
vergestoµ∗. Intermsoffinite-dimensionaldynamicalsystems(ODEs),theevolutionofdistribution
is governed by the Liouville equation. This observation motivates us to generalize the Liouville
equationintofunctionspaceandconductourstudytherein. Rigorousdefinitionsofrelatednotions
anddetailedproofsforallclaimsmadeinthissectioncanbefoundinAppendixB.
(cid:80)
Functional Liouville Flow: If we expand functions onto an orthonormal basis, u = z ψ , a
i i i
PDEsystem(ofu)canbeviewedasaninfinite-dimensionalODE(ofz). Inthisway,weyieldthe
functional version of the Liouville equation describing how the probability density of u evolves.
Underthisframework,weonlyneedtocheckthestationaryLiouvilleequationtoobtainthelimit
invariantdistributionofadynamicalsystemandcompareitwithµ∗.
Inthecoarse-gridsetting,wesimilarlyderivetheevolutionofthedensityofuandyieldtheoptimal
dynamicsof v ∈F(H)(differentfromCGSineq.(3)),visexactlythesameasuhere),
∂ v =E [FAu|Fu=v], (6)
t u∼µt
whereµ isthedistributionofu∈Hfollowingtheoriginaldynamicsattimetandthisexpectation
t
isconditionedonthesamplingsofusatisfyingFu=v. Herewearriveatthesameresultin[50].
Unfortunately,µ dependsontandtheinitialdistributionofu∈H,whichisunderdeterminedand
t
willsufferfromnon-uniqueissuesifrestrictedtoacoarse-gridsystem,similartowhatisdiscussed
in the previous section. In practice, one can only fix one particular µˆ, a distribution in H, and
assign the dynamics in reduced space as ∂ v = E [FAu|Fu = v]. Checking the resulting
t u∼µˆ
6Liouvilleequation, weshowthatµ∗ isthechoiceforµˆ toguaranteeconvergencetowardsF ρ∗,
#
theoptimalapproximationofµ∗inF(H). BacktothelearningmethodslistedinSection2.2,due
totheL2 variationalcharacterizationofconditionalexpectation,theunderlyingchoiceofµˆisthe
empiricalmeasureofthosetrainingdatacomingfromFRS,ideallyµ∗. Consequently,onehastouse
numerousFRStrainingdataduetotheslowconvergenceofempiricalmeasuresforhigh-dimensional
distribution.
Asisthecase,theselearningmethodsoftenrelyonalargeamountoffine-griddatacomingfromone
longFRStrajectoryormultipleFRStrajectorieswhichareexpensive. Furthermore,mostmethods
stillrelyonacoarse-gridsolverthatiterativelyevolveswithrelativelysmalltimesteps,andsome
methodsrequirethatthecoarsesimulationstartsfromadownsampledversionofhigh-fidelitydata
closetotheattractor. Theseaspectshinderthefurtherapplicationofthesemethods.
3 Methodology: Physics-InformedOperatorLearning
Fromprevioussections,weseethatrestrictingthelearningobjectinthefilteredspaceandexplicitly
learningtheclosuremodelwouldalwayssufferfromthenon-uniquenessofthistarget. Inlightof
that,weproposetoextendthelearningtaskintofunctionspaceHanddirectlydealwiththesolution
operatorS(t)ofPDEgoverningthedynamics,whichisawell-definedmapping.
OperatorLearning: Thegoalofoperatorlearning[31,32,55]istoapproximatemappingsbetween
function spaces rather than vector spaces. One of the representatives is Fourier Neural Operator
(FNO)[31],whosearchitecturecanbedescribedas:
G :=Q◦(W +K )◦···◦σ(W +K )◦P, (7)
FNO L L 1 1
where P and Q are pointwise lifting and projection operators. The intermediate layers consist
of an activation function σ, pointwise operators W and integral kernel operators K : u →
ℓ ℓ
F−1(R ·F(u)),whereR areweightedmatricesandF denotesFouriertransform.
ℓ ℓ
WithFNO,welearnthemappingu→{S(t)u} ,wherehisamodelparameter. Ithastwomain
t∈[0,h]
advantages: (1)Resolution-Invariance: Themodelsupportsinputfromdifferentresolutions(grid
sizes),andinputsareallviewedasdiscretizationofanunderlyingfunction. Consequently,whenwe
feedacoarse-gridinitialstatetothewell-trainedmodelandrollouttogenerateaCGStrajectory,
there exists an FRS trajectory such that the CGS trajectory we obtain is its filtering. This CGS
trajectorymatchestheoptimalcoarse-griddynamicsdiscussedinSection2.3.(2)FasterConvergence:
The burning time T is the moment when a trajectory approaches the attractor close enough.
burn
Forpreviousmethods,afterthelearning-basedclosuremodelsaretrained,theyaremergedintoa
coarse-gridsolverandevolveiterativelywithrelativelysmalltimesteps. Inoperatorlearningwhere
hisusuallyofO(1)magnitude,thesimulationarrivesatT morequickly.
burn
ItremainstoovercomethelackofFRStrainingdatainrealisticsituations. NotethatthePDE(1)
containsalltheinformationofthedynamicalsystem. Weadoptphysics-informedmethodologies[56]
toremovetherelianceondata. Tobespecific,theoperatormodelG istrainedbyminimizingthe
θ
physics-informedlossfunction:
1 (cid:88)
J (θ;D)= ∥(∂ −A)G u (x)∥ , (8)
pde |D| t θ 0i L2(Ω×[0,h])
i∈D
wheretheinitialvaluesu inthelossfunctioncouldbeanyfine-gridfunctionsanddonothaveto
0i
comefromFRStrajectories. Ωisthespatialdomainofthesefunctions.
PracticalAlgorithm: Inpractice,theoptimizationofphysics-informedlossishard[57]andmight
encountersomeabnormalfunctionswithsmalllossbutlargeerrors[58]. Tofacethesechallenges,we
pre-trainthemodelviasupervisedlearningwithadatalossfunctiontoachieveagoodinitialization
ofthemodelparametersforJ optimization:
pde
1 (cid:88)
J (θ;D)= ∥G u −S([0,h])u ∥. (9)
data |D| θ i i
i∈D
ToenhancethelimitedFRStrainingdataavailable,wepre-trainwithJ usingplentyofCGSdata
data
firstandthenaddFRSdataintothelossfunction. Afterthat,wegraduallydecreasetheweightof
CGSdatalossinthelossfunctionsinceCGSdataispotentiallyincorrect. Afterwarmingupwith
7dataloss,wefurthertrainourmodelwithphysics-informedloss. Theformalizedalgorithmandits
implementationdetailscanbefoundinAppendixF.
ProvableConvergencetoLong-termStatistics: TheuniversalapproximationcapabilityofFNO
hasbeenproved[32,39]. Somemightdoubtthatsincesmallerrorswillrapidlyescalateovertime
inchaoticsystems, wehavetotrainanFNOthatperfectlyfitsthegroundtruth, whichwouldbe
unrealistic. However, we have the following result. Intuitively, we show that there exists a true
trajectory (from a different initial value) that is consistently close to the simulation we get with
approximateFNO.Sincetheinvariantmeasureisindependentoftheinitialcondition,weobtaina
goodapproximationofthe(filtered)invariantmeasure.
N
Theorem3.1. Foranyh > 0,denoteµˆ := lim 1 (cid:80) δ ,anyv (x)withx ∈ D′. For
h,θ N→∞N
n=1
G θnv0(x) 0
any ϵ > 0, there exists δ > 0 s.t. as long as ∥(G u)(·,h)−S(h)u∥ < δ,∀u ∈ H, we have
θ H
W (µˆ ,F µ∗)<ϵ,whereW isageneralizationofWassersteindistanceinfunctionspace.
H h,θ # H
TheproofcanbefoundinAppendixD.DetailsaboutW canbefoundinAppendixA.2. These
H
results show that even if the trained operator jumps a large step h in time and has errors as is in
practice,wecanstillobtainagoodestimationofstatisticsbyrollingitoutandcomputingthetime
average. Inpractice,a10%∼20%relativeerrorofsingle-steppredictionsuffices.
4 Experiments
Weverifyouralgorithmwithtwoequationsfromfluiddynamics,1DKuramoto-Sivashinsky(KS)
and2DNavier-Stokes(NS).WeuseoneNVIDIA4090GPUforallexperiments.
For our method, we adopt FNO as the model architecture and predict the mapping from u to
0
{S(t)u } , where h is of O(1) magnitude. We compare our estimation of long-term statis-
0 t∈[0,h]
ticswithgold-standardgroundtruthfromfullyresolvedsimulations(FRS),andseveralbaselines.
(1)CGS:coarse-gridsimulationwithoutanyclosuremodel. (2)Classicalclosuremodel: Smagorin-
sky model [14] is the most classical and popular closure model applied in computational fluid
dynamics. WecomparewithSmagorinskymodelforNSanditscounterparteddy-viscositymodel
for KS[59]. We have selected the best-performing parameter in these models. (3) Single-state
model: itsmethodologyisshowcasedinSection2.2. Toleveragetheup-to-datemachinelearning
toolkits, we replace the convolution neural network (CNN) model in original papers [30] with a
transformer-basedmodel. Theimplementationdetails,hyperparameters,anddatagenerationcanbe
foundinAppendixGandAppendixE.
Kuramoto–SivashinskyEquationWeconsidertheone-dimensionalKSequationforu(x,t),
∂ u+u∂ u+∂ u+ν∂ u=0, (x,t)∈[0,6π]×R , (10)
t x xx xxxx +
withperiodicboundaryconditions. Thepositiveviscositycoefficientν reflectsthetraceabilityofthis
equation. Thesmallerν is,themorechaoticthesystemis. Westudythecaseforν =0.01.
(a)EnergySpectrum (b)HistogramofVorticity (c)Dissipation
Figure2: ResultsforNavier-Stokes. (a)Energyspectrum,(b)histogramofvorticity,(c)histogram
ofdissipation.Inthelabel,‘FRS’(blueline)referstogold-standardfully-resolvedsimulations.‘CGS’:
coarse-gridsimulationwithoutclosuremodel. ‘Smag.’: Smagorinskymodel. ‘Single’: single-state
learning-basedmodel. Ourmethodisclosesttogroundtruth(‘FRS’)amongallcoarse-gridmethods
(‘CGS’,‘Smag.’,‘Single’).
8Table2: ExperimentResultsforNavier-StokesEquation. Left: Errorsondifferentstatistics,i.e.,
averagetotalvariation(‘Avg. TV’),energyspectrum(‘Energy’),TVerrorforvorticitydistribution
(‘Vorticity’),andvelocityvariance(‘Variance’). Percentagesrefertoaveragerelativeerrors. Other
numbersrefertoTVdistances(ranging[0,1])betweengroundtruthandprediction. Right: Compari-
sonoftheinferencetime(seconds)ofonetrajectoryfort∈[0,100]. Bestresultsaremarkedbold.
Method Avg. TV Energy Vorticity Variance
FRS 39.70
CGS(Noclosure) 0.4914 178.4651% 0.1512 253.4234% CGS(Noclosure) 4.50
Smagorinsky[14] 0.2423 52.9511% 0.0483 20.1740% Smagorinsky 4.81
Single-state[30] 0.5137 205.3709% 0.1648 298.2027% Single-state 18.57
OurMethod 0.0726 5.3276% 0.0091 2.8666% Ours 0.32
Forourmodel,wechooseh=0.1. ThetotalamountofFRStrainingdatais105snapshotscoming
from3trajectories. Whenwecompletethetraining,theL2relativeerrorofourmodelonthetestset
is12%. Tomakeafaircomparison,otherlearning-basedmethodsarerestrictedtothesameamount
oftrainingdata. ThissettingwillbethesameforNS.
Navier-StokesEquationWeconsidertwo-dimensionalKolmogorovflow(aformoftheNavier-
Stokesequations)foraviscousincompressiblefluid(fluidfield)u(x,y,t)∈R2,
1
∂ u=−(u·∇)u−∇p+ ∆u+(sin(4y),0)T, ∇·u=0, (x,y,t)∈[0,2π]2×R , (11)
t Re +
withperiodicboundaryconditions. Thefunctionpisaknownpressure. ThepositivecoefficientReis
Reynoldsnumber. ThelargerReis,themorechaoticthesystemis. WeconsiderthecaseRe=100.
Forourmodel,wechooseh=1. ThetotalamountofFRStrainingdatais110snapshotscoming
from1trajectory. Whenwecompletethetraining,theL2relativeerroronthetestsetis19%.
Evaluation: InspiredbySection2.3whereweanalyzethroughtheviewpointofprobabilitydistri-
butions,weproposetocomparethepredictedinvariantmeasureandgroundtruthdirectly,inthat
wecangetagoodestimationofanystatisticsaslongasweestimatethedistributionwell. Tobe
specific,wecomputethetotalvariation(TV)distancebetweenmarginaldistributionsofeveryz
i
component(Section 2.3). To give a more convincing comparison, we also check useful statistics
likeenergyspectrum,auto-correlation,variance,velocityandvorticitydensity,kineticenergyand
dissipationrate. Duetothespacelimit,acomprehensivecomparisonoferrorandvisualizationof
thesestatistics,alongwiththeirdefinitions,arepresentedinAppendixG.Wealsoreferthereadersto
theappendixfordetailsonhowwecomputethestatisticsandresultsfortheKSequation.
ForNSequation,weaverageover400trajectoriesfromt∈[1800,3000]tocomputethestatistics.
The error of some statistics (compared with FRS) and the running time of a single trajectory for
t∈[0,100]areshowninTable2. Visualizationforthepredictionofthesethreestatisticsisshown
inFigure2. Acost-error(intermsofaveragetotalvariationdistancefromground-truthinvariant
measureforamongallz )summaryispresentedinfig.1right.
i
From the result, we see that even though using a very limited number of FRS training data, our
methodmanagestoestimatelong-termstatisticsaccuratelyandefficiently,muchbetterthanallthe
baselines. Wealsoseethatpreviouslearning-basedmethodsperformquitebadlywhenrestrictedtoa
realisticusageofFRSdata,muchworsethanreportedinoriginalpaperswheretheyusethousandsof
dataorhundredsoftrajectoriesfortraining.
Anablationstudytodemonstratetheeffectofdata-losspretrainingiscarriedoutinappendixG.3.
5 Conclusions
Inthiswork,westudytheproblemofestimatinglong-termstatisticsinchaoticsystemswithonly
coarse-grid simulations. We propose a new theoretical framework, functional Liouville flow, to
analyze this problem. We rigorously demonstrate the inherent shortcomings of existing learning
methods. Also inspired by our theoretical result, we leverage physics-informed neural operators
togiveanefficientandprovablyaccurateestimationoflong-termstatisticswithverylimitedfine-
resolutiondatausageduringtraining. Asevaluatedintheexperiments,ourmethodhasthepotential
9toaddressthechallengingtasksregardingchaoticsystemsarisinginvariousphysicalsciences. The
implicationofthisworkisnotrestrictedtothespecifictaskofestimatinglong-termstatistics. This
workexhibitsthebenefitofgoingbeyondthefinitegridsystemandunderstandingproblemsthrougha
functionspaceviewpoint.FunctionalLiouvilleflowwouldbeusefulininvestigatingimagegeneration
tasks,byviewingimagesasfunctionsrepresentedonpixels.
Acknowledgements
A.AnandkumarissupportedinpartbyBrenendowedchair,ONR(MURIgrantN00014-18-12624),
andbytheAI2050seniorfellowprogramatSchmidtSciences. J.Berneracknowledgessupportfrom
the Wally Baerand JeriWeissPostdoctoral Fellowship. C. Wanghopes tothank Andrew Stuart,
RicardoBaptista,andArushiGuptaforhelpfuldiscussions.
References
[1] JAS Lima, R Silva Jr, and Janilo Santos. Plasma oscillations and nonextensive statistics.
PhysicalReviewE,61(3):3260,2000.
[2] GregoryFlato, JochemMarotzke, BabatundeAbiodun, PascaleBraconnot, SinChanChou,
WilliamCollins,PeterCox,FatimaDriouech,SeitaEmori,VeronikaEyring,etal. Evaluationof
climatemodels. InClimatechange2013: thephysicalsciencebasis.ContributionofWorking
GroupItotheFifthAssessmentReportoftheIntergovernmentalPanelonClimateChange,
pages741–866.CambridgeUniversityPress,2014.
[3] Stephen H Schneider and Robert E Dickinson. Climate modeling. Reviews of Geophysics,
12(3):447–493,1974.
[4] Jeffrey P Slotnick, Abdollah Khodadoust, Juan Alonso, David Darmofal, William Gropp,
Elizabeth Lurie, and Dimitri J Mavriplis. CFD vision 2030 study: a path to revolutionary
computationalaerosciences. TechnicalReportCR-2014–21817,NASA,2014.
[5] David M Wootton and David N Ku. Fluid mechanics of vascular systems, diseases, and
thrombosis. AnnualReviewofBiomedicalEngineering,1(1):299–329,1999.
[6] JulioMOttino. Mixing,chaoticadvection,andturbulence. AnnualReviewofFluidMechanics,
22(1):207–254,1990.
[7] Gerald Jay Sussman and Jack Wisdom. Chaotic evolution of the solar system. Science,
257(5066):56–62,1992.
[8] Henri Korn and Philippe Faure. Is there chaos in the brain? ii. experimental evidence and
relatedmodels. ComptesRendusBiologies,326(9):787–840,2003.
[9] Steven H Strogatz. Nonlinear dynamics and chaos: with applications to physics, biology,
chemistry,andengineering. CRCpress,2018.
[10] KiranRavikumar,DavidAppelhans,andPKYeung. Gpuaccelerationofextremescalepseudo-
spectralsimulationsofturbulenceusingasynchronism. InProceedingsoftheInternational
ConferenceforHighPerformanceComputing,Networking,StorageandAnalysis,pages1–22,
2019.
[11] Tapio Schneider, João Teixeira, Christopher S Bretherton, Florent Brient, Kyle G Pressel,
Christoph Schär, and A Pier Siebesma. Climate goals and computing the future of clouds.
NatureClimateChange,7(1):3–5,2017.
[12] JavierJimenezandRobertDMoser. Large-eddysimulations: whereareweandwhatcanwe
expect? AIAAjournal,38(4):605–612,2000.
[13] StephenBPope. Tenquestionsconcerningthelarge-eddysimulationofturbulentflows. New
JournalofPhysics,6(1):35,2004.
[14] JosephSmagorinsky. Generalcirculationexperimentswiththeprimitiveequations: I.thebasic
experiment. MonthlyWeatherReview,91(3):99–164,1963.
10[15] YupengZhangandKaushikBhattacharya. Iteratedlearningandmultiscalemodelingofhistory-
dependentarchitecturedmetamaterials. arXivpreprintarXiv:2402.12674,2024.
[16] BurigedeLiu,EricOcegueda,MargaretTrautner,AndrewMStuart,andKaushikBhattacharya.
Learning macroscopic internal variables and history dependence from microscopic models.
JournaloftheMechanicsandPhysicsofSolids,178:105329,2023.
[17] ValentinaTozzini. Coarse-grainedmodelsforproteins. CurrentOpinioninStructuralBiology,
15(2):144–150,2005.
[18] Kenneth G Wilson. Renormalization group and strong interactions. Physical Review D,
3(8):1818,1971.
[19] CharlesMeneveauandJosephKatz. Scale-invarianceandturbulencemodelsforlarge-eddy
simulation. AnnualReviewofFluidMechanics,32(1):1–32,2000.
[20] RobertDMoser,SigfriedWHaering,andGopalRYalla. Statisticalpropertiesofsubgrid-scale
turbulencemodels. AnnualReviewofFluidMechanics,53:255–286,2021.
[21] Di Zhou and H Jane Bae. Sensitivity analysis of wall-modeled large-eddy simulation for
separatedturbulentflow. JournalofComputationalPhysics,506:112948,2024.
[22] KarthikDuraisamy,GianlucaIaccarino,andHengXiao. Turbulencemodelingintheageof
data. AnnualReviewofFluidMechanics,51:357–377,2019.
[23] KarthikDuraisamy. Perspectivesonmachinelearning-augmentedreynolds-averagedandlarge
eddysimulationmodelsofturbulence. PhysicalReviewFluids,6(5):050504,2021.
[24] Benjamin Sanderse, Panos Stinis, Romit Maulik, and Shady E Ahmed. Scientific machine
learningforclosuremodelsinmultiscaleproblems: areview. arXivpreprintarXiv:2403.02913,
2024.
[25] SuryanarayanaMaddu,ScottWeady,andMichaelJShelley. Learningfast,accurate,andstable
closuresofakinetictheoryofanactivefluid. JournalofComputationalPhysics,504:112869,
2024.
[26] VarunShankar,VedantPuri,RameshBalakrishnan,RomitMaulik,andVenkatasubramanian
Viswanathan.Differentiablephysics-enabledclosuremodelingforburgers’turbulence.Machine
Learning: ScienceandTechnology,4(1):015017,2023.
[27] Masataka Gamahara and Yuji Hattori. Searching for turbulence models by artificial neural
network. PhysicalReviewFluids,2(5):054604,2017.
[28] AndreaBeck,DavidFlad,andClaus-DieterMunz. Deepneuralnetworksfordata-drivenLES
closuremodels. JournalofComputationalPhysics,398:108910,2019.
[29] Romit Maulik, Omer San, Adil Rasheed, and Prakash Vedula. Subgrid modelling for two-
dimensional turbulence using neural networks. Journal of Fluid Mechanics, 858:122–144,
2019.
[30] YifeiGuan,AsheshChattopadhyay,AdamSubel,andPedramHassanzadeh. Stableaposteriori
LESof2Dturbulenceusingconvolutionalneuralnetworks: Backscatteringanalysisandgen-
eralizationtohigherreviatransferlearning. JournalofComputationalPhysics,458:111090,
2022.
[31] ZongyiLi,NikolaKovachki,KamyarAzizzadenesheli,BurigedeLiu,KaushikBhattacharya,
AndrewStuart,andAnimaAnandkumar. Fourierneuraloperatorforparametricpartialdifferen-
tialequations. arXivpreprintarXiv:2010.08895,2020.
[32] NikolaKovachki,ZongyiLi,BurigedeLiu,KamyarAzizzadenesheli,KaushikBhattacharya,
AndrewStuart,andAnimaAnandkumar. Neuraloperator: Learningmapsbetweenfunction
spaceswithapplicationstoPDEs. JournalofMachineLearningResearch,24(89):1–97,2023.
11[33] ZongyiLi,HongkaiZheng,NikolaKovachki,DavidJin,HaoxuanChen,BurigedeLiu,Kamyar
Azizzadenesheli,andAnimaAnandkumar.Physics-informedneuraloperatorforlearningpartial
differentialequations. ACM/JMSJournalofDataScience,2021.
[34] AnudhyanBoral,ZhongYiWan,LeonardoZepeda-Núñez,JamesLottes,QingWang,Yi-fan
Chen, JohnRobertsAnderson, andFeiSha. Neural ideallargeeddy simulation: Modeling
turbulencewithneuralstochasticdifferentialequations. arXivpreprintarXiv:2306.01174,2023.
[35] WilliamCReynolds. Thepotentialandlimitationsofdirectandlargeeddysimulations. In
WhitherTurbulence? TurbulenceattheCrossroads:ProceedingsofaWorkshopHeldatCornell
University,Ithaca,NY,March22–24,1989,pages313–343.Springer,2005.
[36] HaecheonChoiandParvizMoin.Grid-pointrequirementsforlargeeddysimulation:Chapman’s
estimatesrevisited. PhysicsofFluids,24(1),2012.
[37] Qian Wang, Nicolò Ripamonti, and Jan S Hesthaven. Recurrent neural network closure of
parametricpod-galerkinreduced-ordermodelsbasedonthemori-zwanzigformalism. Journal
ofComputationalPhysics,410:109402,2020.
[38] Justin Sirignano and Jonathan F MacArt. Dynamic deep learning LES closures: Online
optimizationwithembeddedDNS. arXivpreprintarXiv:2303.02338,2023.
[39] SamuelLanthaler,ZongyiLi,andAndrewMStuart. Thenonlocalneuraloperator: Universal
approximation. arXivpreprintarXiv:2304.13221,2023.
[40] ChaoMa,JianchunWang,etal. Modelreductionwithmemoryandthemachinelearningof
dynamicalsystems. arXivpreprintarXiv:1808.04258,2018.
[41] RogerTemam. Infinite-dimensionaldynamicalsystemsinmechanicsandphysics,volume68.
SpringerScience&BusinessMedia,2012.
[42] JohnMilnor. Ontheconceptofattractor. CommunicationsinMathematicalPhysics,99:177–
195,1985.
[43] SergeiPKuznetsov. Dynamicalchaosanduniformlyhyperbolicattractors: frommathematics
tophysics. Physics-Uspekhi,54(2):119,2011.
[44] SimonaDinicola,FabrizioD’Anselmi,AlessiaPasqualato,SaraProietti,ElisabettaLisi,Alessan-
draCucina,andMarianoBizzarri. Asystemsbiologyapproachtocancer: fractals,attractors,
andnonlineardynamics. Omics: ajournalofintegrativebiology,15(3):93–104,2011.
[45] Sui Huang, Ingemar Ernberg, and Stuart Kauffman. Cancer attractors: a systems view of
tumorsfromagenenetworkdynamicsanddevelopmentalperspective. InSeminarsincell&
developmentalbiology,volume20,pages869–876.Elsevier,2009.
[46] AlfredoMedioandMarjiLines. Nonlineardynamics: Aprimer. CambridgeUniversityPress,
2001.
[47] Justin Sirignano, Jonathan F MacArt, and Jonathan B Freund. Dpm: A deep learning pde
augmentationmethodwithapplicationtolarge-eddysimulation. JournalofComputational
Physics,423:109811,2020.
[48] Björn List, Li-Wei Chen, and Nils Thuerey. Learned turbulence modelling with differen-
tiablefluidsolvers: physics-basedlossfunctionsandoptimisationhorizons. JournalofFluid
Mechanics,949:A25,2022.
[49] FeiLu,KevinKLin,andAlexandreJChorin. Data-basedstochasticmodelreductionforthe
kuramoto–sivashinskyequation. PhysicaD:NonlinearPhenomena,340:46–57,2017.
[50] JacobALangfordandRobertDMoser. OptimalLESformulationsforisotropicturbulence.
JournalofFluidMechanics,398:321–346,1999.
[51] Matthew Levine and Andrew Stuart. A framework for machine learning of model error in
dynamicalsystems. CommunicationsoftheAmericanMathematicalSociety,2(07):283–344,
2022.
12[52] BenediktBarthelSorensen,AlexisCharalampopoulos,ShixuanZhang,BryceHarrop,Ruby
Leung,andThemistoklisSapsis. Anon-intrusivemachinelearningframeworkfordebiasing
long-timecoarseresolutionclimatesimulationsandquantifyingrareeventsstatistics. arXiv
preprintarXiv:2402.18484,2024.
[53] VivekOommen,KhemrajShukla,SaakethDesai,RemiDingreville,andGeorgeEmKarniadakis.
Rethinkingmaterialssimulations: Blendingdirectnumericalsimulationswithneuraloperators.
arXivpreprintarXiv:2312.05410,2023.
[54] HugoFrezat,GuillaumeBalarac,JulienLeSommer,andRonanFablet. Gradient-freeonline
learningofsubgrid-scaledynamicswithneuralemulators. arXivpreprintarXiv:2310.19385,
2023.
[55] LuLu,PengzhanJin,GuofeiPang,ZhongqiangZhang,andGeorgeEmKarniadakis. Learning
nonlinearoperatorsviadeeponetbasedontheuniversalapproximationtheoremofoperators.
NatureMachineIntelligence,3(3):218–229,2021.
[56] GeorgeEmKarniadakis,IoannisGKevrekidis,LuLu,ParisPerdikaris,SifanWang,andLiu
Yang. Physics-informedmachinelearning. NatureReviewsPhysics,3(6):422–440,2021.
[57] PratikRathore,WeimuLei,ZacharyFrangella,LuLu,andMadeleineUdell. Challengesin
trainingpinns: Alosslandscapeperspective. arXivpreprintarXiv:2402.01868,2024.
[58] ChuweiWang,ShandaLi,DiHe,andLiweiWang. Isl2physicsinformedlossalwayssuitable
fortrainingphysicsinformedneuralnetwork? AdvancesinNeuralInformationProcessing
Systems,35:8278–8290,2022.
[59] PritpalMatharuandBartoszProtas. Optimalclosuresinasimplemodelforturbulentflows.
SIAMJournalonScientificComputing,42(1):B250–B272,2020.
[60] CédricVillanietal. Optimaltransport: oldandnew,volume338. Springer,2009.
[61] FilippoSantambrogio. Optimaltransportforappliedmathematicians. Birkäuser,NY,55(58-
63):94,2015.
[62] Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and
processes. SpringerScience&BusinessMedia,2013.
[63] NicholasVakhania,VazhaTarieladze,andSChobanyan. ProbabilitydistributionsonBanach
spaces,volume14. SpringerScience&BusinessMedia,2012.
[64] Michael Brin and Garrett Stuck. Introduction to dynamical systems. Cambridge university
press,2002.
[65] LanWen. Differentiabledynamicalsystems,volume173. AmericanMathematicalSoc.,2016.
[66] IsaacPCornfeld,SergejVFomin,andYakovGrigorevichSinai. Ergodictheory,volume245.
SpringerScience&BusinessMedia,2012.
[67] RogerTemam. Navier-Stokesequations:theoryandnumericalanalysis,volume343. American
MathematicalSoc.,2001.
[68] Qiqi Wang, Rui Hu, and Patrick Blonigan. Least squares shadowing sensitivity analysis of
chaoticlimitcycleoscillations. JournalofComputationalPhysics,267:210–224,2014.
[69] SergeyPKuznetsov. Hyperbolicchaos. Springer,2012.
[70] StephenSmale. AninfinitedimensionalversionofSard’stheorem. InTheCollectedPapersof
StephenSmale: Volume2,pages529–534.WorldScientific,2000.
[71] Aly-KhanKassamandLloydNTrefethen. Fourth-ordertime-steppingforstiffPDEs. SIAM
JournalonScientificComputing,26(4):1214–1233,2005.
[72] GaryJChandlerandRichRKerswell. Invariantrecurrentsolutionsembeddedinaturbulent
two-dimensionalkolmogorovflow. JournalofFluidMechanics,722:554–595,2013.
13[73] AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,
ThomasUnterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,etal.
Animageisworth16×16words: Transformersforimagerecognitionatscale. arXivpreprint
arXiv:2010.11929,2020.
[74] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017.
[75] J-PEckmannandDavidRuelle. Ergodictheoryofchaosandstrangeattractors. Reviewsof
ModernPhysics,57(3):617,1985.
14Appendix
In this appendix, we will first provide detailed proofs of our theoretical results (A-D), and then
presentimplementationdetailsandmoreexperimentresults(E-G).Thestructureoftheappendixis
asfollows.
• AppendixAprovidesalistofnotations,alongwithanintroductionofimportantbackground
conceptions,preliminaryresults,andbasicassumptionsinthispaper.
• AppendixBfirstformallyintroducesfunctionalLiouvilleflow,andthenpresentsadetailed
versionofSection2.3.
• AppendixCprovidestheproofofthethreeclaimsinTheorem2.1.
• AppendixDprovidestheproofofTheorem3.1.
• AppendixEcontainsinformationaboutthedatasetintheexperimentsandavisualizationof
theNavier-Stokesdataset.
• AppendixFprovidestheimplementationdetailsforourmethodandbaselinemethods.
• AppendixGfirstformallyintroducesthestatisticsweconsider,followedbythefullexperi-
mentresults(tableandplots)andablationstudies.
A Notations,AuxiliaryResults,andBasicAssumptions
Inthissection,wefirstsummarizethenotationsinthispaper,thenreviewanddefinesomeofthe
importantconcepts,andfinallystatethebasicassumptionsinthiswork. Wewouldencouragethe
readerstoalwayscheckthissectionwhentheyhaveanyconfusionregardingtheproof.
A.1 Notations
Table3: ListofNotations
Notation Description
µ Distributions.
F Push-forwardofamappingF. Ify = F(x)andthedistributionofxisµ ,thenthe
# x
distributionofyisF µ .
# x
F# Pull-back of a mapping F. If y = F(x) and the distribution of y is µ , then the
y
distributionofxisF#µ .
y
H (Original)FunctionSpace,seeeq.(1).
A Theoperatorofthedynamics,seeeq.(1).
S(t) Semigroupinducedbyeq.(1).
u FunctionsinH.
F Filter. F :H→H. Theimagespaceisfinite-rankanddenotedasF(H).
D Thesetofgridsinfully-resolvedsimulations. Thenumberofgridsis|D|.
D′ Thesetofgridsincoarse-gridsimulations. Thenumberofgridsis|D′|.
P(Ω) ThesetofallprobabilitydistributionssupportedonasetΩ.
W TheWassersteindistanceformeasuresinH,with∥·∥ beingthecostfunction.
H H
15Notation Description
⟨·,·⟩ ThepairoflinearfunctionalsandelementsinaBanachspaceX. Specifically, for
x ∈ X, f ∈ X∗, itsdualspace, ⟨f,x⟩ := f(x). ThankstoRieszRepresentation
Theorem,wewillalsousethisnotationforinnerproductsinHilbertspace.
⊗ For a Hilbert space H, u ∈ H, v ∈ H, u ⊗ v is defined as the linear operator
w →⟨v,w⟩u, w ∈H.
⊕ u⊕v :=(u,v).
C Continuousfunctionspace,equippedwithL∞norm.
0
C∞ Smoothandcompactly-supportedfunctions.
c
dG(u,v) TheGateauxderivativeofoperatorG atuinthedirectionofv.
ℵ Aleph-zero,countablyinfinite.
0
[n] {1,2,...n}
D Thesetoftime-grids.
T
M (u ) The set of functions that are indistinguishable from u merely based on values on
DT 0 0
spatiotemporalgridD′×D ,seeAssumptionC.5.
T
I , I (Spatial)Grid-measurementoperators,seeDefinitionC.1.
x
F , F Spatiotemporalgrid-measurementoperators,seeTheoremC.2.
DT
w.r.t. withregardto
wlog withoutlossofgenerality
A.2 OptimalTransportinFunctionSpace
GivenaBanachspaceX andtwodistributionsµ ,µ ∈P(X),wewanttomeasurethecloseness
1 2
ofthesetwodistributions.
Recallthatinfinite-dimensionalX,the(Mongeformulationof)c−Wassersteindistanceisdefined
as
(cid:90)
W (µ ,µ ):=inf c(x,Tx)µ (dx), s.t.T µ =µ , (12)
c 1 2 1 # 1 2
T X
whereT isameasurablemappingfromX toX,andc=c(x,y)isnon-negativebi-variatefunction
knownascostfunction.
WecouldnaturallygeneralizethisconceptintomeasuresinarbitraryBanachspaceX anddefine
theWassersteindistancecorrespondingly. Inparticular,weusethemetricinX ascostfunctionand
define
(cid:90)
WX(µ 1,µ 2):=inf ∥Tx−x∥µ 1(dx), s.t.T #µ
1
=µ 2. (13)
T X
Formorebackgroundsandrigorousdefinitionsofconceptsappearedabove,[60,61]arestandard
referencesforoptimaltransport,and[62,63]aregoodreferencesformeasuretheory(andprobability)
infunctionspace.
A.3 DynamicalSystems
Wewouldliketoreferthereaderstoclassicaltextbooksondynamicalsystems[64,65]andergodic
theorems[66]fordetailedproofsoflemmasstatedinthissubsection.
16A.3.1 DiscreteTimeSystem
LetX beacompactmetricspaceandf :X →X beacontinuousmap. Thegenericformofdiscrete-
timedynamicalsystemiswrittenasx =f(x ), n∈Norn∈Z(iff isahomeomorphism).
n+1 n
DefinitionA.1. Λ⫋X isaninvariantsetoff iff(Λ)=Λ.
Inthefollowingdiscussion,wefurtherassumeXtobeaC∞Riemannianmanifoldwithoutboundary.
DefinitionA.2. AninvariantsetΛ⫋X off ishyperbolicifforeachx∈Λ,thetangentspaceT X
x
splitsintoadirectsum
T X =Es(x)⊕Eu(x), (14)
x
invariantinthesensethat
Tf(Es(x))=Es(f(x)), Tf(Eu(x))=Eu(f(x)), (15)
suchthat,forsomeconstantC ≥1andλ∈(0,1),thefollowinguniformestimateshold:
|Tfn(v)|≤Cλn|v|, ∀x∈Λ, v ∈Es(x), n≥0, (16)
1
|Tfn(v)|≥ λ−n|v|, ∀x∈Λ, v ∈Eu(x), n≥0. (17)
C
LemmaA.3. (ShadowingLemma)LetΛ ⊂ X beahyperbolicsetoff. Foranyϵ > 0, thereis
η 0,η
1
>0suchthatforany{x n} n∈Nsatisfying(i)d(x n,Λ)<η 0;(ii)|x n+1−f(x n)|<η 1forall
n,thereexistsy ∈X suchthat|x −f(n)y|<ϵ, ∀n.
n
A.3.2 ContinuousTimeSystem
Recallthatthedynamicalsystemweconsideris
(cid:26)
∂ u(x,t)=Au(x,t)
t (18)
u(x,0)=u (x), u ∈H,
0 0
whereu istheinitialvalueandH isafunctionspacecontainingfunctionsofinterests. Wewill
0
occasionallyrefertoAuasvectorfieldgoverningthedynamics. Thisdynamicsinducedasemigroup
{S(t)} .
t≥0
Definition A.4. Given a measure µ ∈ P(H), the system is mixing if for any measurable set
A, B ⊂H, lim µ(A∩S(t)(B))=µ(A)µ(B).
t→∞
DefinitionA.5. Ameasureµ∈P(H)issaidtobeaninvariantmeasureofthissystemifS(t) µ=µ
#
forallt>0.
LemmaA.6. Ifasystemismixing,thenitisergodic.
Forergodicsystems,thereisaninvariantmeasureindependentoftheinitialcondition,definedas
1 (cid:90) T
µ∗ := lim δ dt, u∈H, a.e. (19)
T→∞T
t=0
S(t)u
whereδistheDiracmeasure.
DefinitionA.7. Thesemigroupissaidtobeuniformlycompactfortlarge,ifforeveryboundedset
B ⊂Hthereexistst whichmaydependonBsuchthat∪ S(t)BisrelativelycompactinH.
0 t≥t0
LemmaA.8. IfS(t)isuniformlycompact,thenthereisacompactattractorinthissystem.
A.4 Assumptions
Without loss of generality, we carry out our discussion in the regime where H is a separable
Hilbertspacetomaketheproofmorereadableandconcise. Wealsomakethefollowingtechnical
assumptions.
AssumptionA.9. HcanbecompactlyembeddedintoC .
0
AssumptionA.10. Thesystemeq.(18)ismixing. Thesemigroupisuniformlycompact.
AssumptionA.11. Theattractorandinvariantmeasureareunique.
17AssumptionA.12. Theattractorishyperbolicw.r.tS(t)foranyt>0.
Weremarkthattheseassumptionsareeitherprovedorsupportedbyexperimentalevidenceinmany
realscenarios[67,68,43,69].
Forbrevity,wewillignorethedifferencebetweenfully-resolvedsimulation(FRS)andtheexact
solutiontoeq.(18)inthefollowingdiscussions.
B FormalIntroductionofFunctionalLiouvilleFlow
Inthissection,wefirstformallyintroducefunctionalLiouvilleflowinAppendixB.1. Basedonthis
theoreticalframework,wewillreformulatethetaskofestimatinglong-termstatisticsofdynamical
systemswithcoarse-gridsimulationsinAppendixB.2. Finally,wewillprovideinAppendixB.3a
detailedversionofthediscussioninSection2.3.
B.1 FrameworkofFunctionalLiouvilleFlowforstudyingInvariantMeasure
Functionsasvectors: AsacorollaryofHahn-Banachtheorem,wecouldalwaysconstructasetof
orthonormalbasis{ψ } ofHsuchthatthefilteredspaceF(H)=span{ψ ,...ψ },whereweusually
i i 1 n
haven=|D′|. Foranyfunctionu∈H,thereexistsauniquedecompositionu(x)=(cid:80)∞ z ψ (x)
i=1 i i
withz =⟨u,ψ ⟩. Thiscanonicallyinducesanisometricisomorphism:
i i
T : H→ℓ2, u(cid:55)→z=(z ,z ,...). (20)
1 2
Bythismeans,wecanrewritetheoriginalPDE(18)intoanODEinℓ2,denotedby
dz
=f(z), wheref(z)∈ℓ2, f(z) =⟨ψ ,A◦T−1z⟩, (21)
dt i i
where⟨,⟩istheinnerproductinH.
ExampleB.1. ForKuramoto–SivashinskyEquation
∂ u+u∂ u+∂ u+∂ u=0, (x,t)∈[0,2π]×R , (22)
t x xx xxxx +
ifwechoose{ψ k}astheFourierbasis{eikx} k∈Z,thenz
k
isthecoefficientofk-thFouriermodeand
theODEforzis(component-wise),
dz k =(−k4+k2)z − ik (cid:88) z z . (23)
dt k 2 j l
j+l=k
Onecouldfurthermakez realnumbersbychoosingsin, cosbasis.
k
FunctionalLiouvilleflow: RecallthatinODEsystem dx =f(x), x∈Rd,iftheinitialstatex
dt 0
followsthedistributionµ whoseprobabilitydensityisρ (x),thentheprobabilitydensityofx(t),
0 0
denotedbyρ(x,t),satisfiestheLiouvilleequation,
∂ ρ=−∇·(fρ). (24)
t
Nowwewanttogeneralizethisresultintofunctionspace. Weneedtoaddresstheissuethatthere
isingeneralnoprobabilitydensityfunctionformeasuresinfunctionspace. Wewillshowthatit
isreasonabletocarryonourstudybyfixingasufficientlylargeN andinvestigatingthetruncated
systemofthefirstN basis,andviewingthedensitiesasthe(weak-)limitwhenN →∞.
PropositionB.2. ForanyµsupportedonaboundedsetB ⊂Handanyϵ>0,thereexsitst and
0
N s.t. foranyu ∼µandanyt>t ,ifwewriteS(t)u
as(cid:80)∞
z ψ
,then∥(cid:80)
z ψ ∥<ϵ.
0 0 0 i=0 i i i>N i i
(cid:80)
Proof. DefineQ := ψ ⊗ψ . Thenthestatementisequivalentto∥Q S(t)u ∥<ϵ,forall
m i>m i i N 0
u∈B,t>t .
0
DuetoAssumptionA.10,thereexistst suchthat∪ S(t)Bisrelativelycompact. Thisimplies
0 t>t0
thatthereexistsfinite(denotedbyN )points{u }satisfyingthatforanyu ∈B, t>t ,thereexists
1 i 0 0
i ≤ N s.t. ∥S(t)u −u ∥ < ϵ. WedefineM := min {j|∥Q u ∥ < ϵ}. WehaveM < ∞, ∀i.
1 0 i 3 i j j i 5 i
ChoosingN asmaxM completestheproof.
i
i≤N1
18RemarkB.3. Wecanalwaysrestrictourdiscussionwithindistributionssupportedonboundedset
whosecomplementoccurswithaprobabilitysmallerthanmachineprecision.
Back to the dynamics eq. (18) or the equivalent ODE eq. (21), we first make a generalization.
Sincetheinvariantmeasureisindependentofinitialcondition,weknowthatforanydistribution
µ ∈P(H)(insteadofonlydeltadistributionsatu )fromwhichwesamplerandominitialconditions
0 0
u ∼ µ and evolve these functions, the long-term average lim 1 (cid:82)T (cid:0) S(t) (cid:1) µ dt will still
0 0 T→∞T t=0 # 0
convergetoµ∗. Wewillcarryoutourdiscussioninthisgeneralizedsettingwheretheinitialcondition
(cid:0) (cid:1)
issampledfromadistributionµ infunctionspace. Wewilldenoted S(t) µ ,thedistributionat
0 # 0
timet,asµ ,anddenotedtheirdensityfunctionsforcorrespondingzasρ(·,t)(i.e.,µ =T#ρ(·,t)).
t t
Forbrevity,wewillviewu∈Handz∈ℓ2asthesameandnotmentionT#orT forµ andρ(·,t).
# t
Ifweusethecomponent-formoff,f = (f ,f ,....),witheachf amappingfromℓ2 → R,with
1 2 i
exactlythesameargumenttoderiveeq.(24),wehave
∞
(cid:88)
∂ ρ(z,t)=− ∂ (f (z)ρ(z,t)):=−∇ ·(fρ), ρ(z,0)=ρ (z). (25)
t zi i z 0
i
WewillrefertothisasfunctionalLiouvilleflow,i.e.,theLiouvilleequationinfunctionspace,and
denotetheR.H.S.operatorρ→−∇ ·(fρ)asLρ.
z
ReinterpretationofInvariantMeasure: WithfunctionalLiouvilleflow,weobtainanewcharac-
terizationofinvariantmeasureµ∗(whosedistributionisdenotedasρ∗).
PropositionB.4. ρ∗isthesolutiontostationaryLiouvilleequationLρ=0.
Proof. Denotep(z,t):=
1(cid:82)t
ρ(z,s)ds,thefinite-timeaveragedistribution.
t s=0
Notethatforanyz,
(cid:90) t
ρ(z,t)=ρ(z,0)+ ∂ ρ(z,s)ds (26)
t
s=0
(cid:90) t
= Lρ(z,s)ds+ρ(z,0) (27)
0
(cid:90) t
=L ρ(z,s)ds+ρ(z,0)=L(tp(z,t))+ρ(z,0). (28)
s=0
(cid:82)t
Also,wehaveρ(z,t)=∂ ( ρ(z,s)ds)=∂ (tp(z,t)),weconcludethat
t s=0 t
∂ (tp(z,t))=tLp(z,t)+ρ(z,0). (29)
t
Fromthis,weyield
1
∂ p(z,t)=Lp(z,t)+ (ρ(z,0)−p(z,t)). (30)
t t
Bydefinitionweknowp(z,t) → ρ∗ ast → ∞,thus∂ p → 0. Theterm 1(ρ(z,0)−p(z,t))will
t t
alsotendtozeroast→∞(recallthattheyareprobabilitydensityandthusareuniformlybounded
inL1). Therefore,thelimitdensityρ∗satisfiesLρ∗ =0.
B.2 ReformulationofEstimatingLong-termStatistics
Wereformulatetheproblemofestimatinglong-termstatisticswithcoarse-gridsimulationwiththe
help of functional Liouville flow. Recall that D′ is the set of coarse grid points, and coarse-grid
simulation(CGS)isequivalenttoevolvingfunctionsinF(H),whereF isthefilter(seeSection2.2).
B.2.1 Notations
Westartbydefiningseveralnotations.
19n
(cid:80)
DefinetheorthonormalprojectionontoF(H)asP = ψ ⊗ψ . Weremarkherethatinmany
i i
i=1
situations,wehaveP =F.
Letusdecompositezanduintotheresolvedpartandunresolvedpart,
z=v⊕w, v:=(c ,c ,...c ), w:=(c ,c ,...); (31)
1 2 n n+1 n+2
u(x)=v(x)+w(x), v(x):=T−1v=Pu, w(x):=T−1w=(I−P)u. (32)
Inparticular,w ∈F(H)⊥istheunresolvedpartincoarse-gridsimulations. Withthisdecomposition,
werewriteanydensityρ(z)asajointdistributionρ(v,w)anddefinemarginaldistributionforvas
ρ (v),andtheconditionaldistributionofwgivenvasρ(w|v). Withalittleabuseofnotation,we
1
willoccasionallyrefertotheprobabilitydensityasitsdistribution,andviceversa.
We will also divide the vector field f into resolved part f and unresolved part f , which are
r u
(f ,f ,...,f )and(f ,f ,...)respectively.
1 2 n n+1 n+2
B.2.2 ReformualtionofCoarse-gridSimulation
We first show that the optimal approximation of µ∗ (or ρ∗) in the reduced space is its marginal
distribution,ifweconstructdensitieswithorthonormalbasis,asisineq.(20).
PropositionB.5. ρ∗ = argmin W (µ,µ∗).
1 H
µ∈P(F(H))
Proof. FromtheconstructionofP andthedefinitionofW ,foranymeasurablemapping
H
T:H→F(H),
(cid:90) (cid:90)
∥Tu−u∥µ∗(du)≥ ∥Pu−u∥µ∗(du). (33)
H H
Thus,foranyµ∈P(F(H)),
(cid:90)
W (µ,µ∗)≥ ∥Pu−u∥µ∗(du)=W (P µ∗,µ∗). (34)
H H #
H
Notethatρ∗ =P µ∗,thiscompletestheproof.
1 #
Thisresultmotivatesustochecktheevolutionofρ (v,t),whichshouldachievetheoptimalapproxi-
1
mationρ∗.
1
Notethatbydefinition,foranydistributionρ∈P(H),ρ (v)=(cid:82) ρ(v,w)dw. Combinethiswith
1
eq.(25),weyield
(cid:90)
∂ ρ (v,t)= ∂ ρ(v,w,t)dw (35)
t 1 t
(cid:90) (cid:90)
=− ∇ ·(f (v,w)ρ(v,w,t))dw− ∇ ·(f (v,w)ρ(v,w,t))dw (36)
v r w u
(cid:18) ρ (v,t)(cid:90) (cid:19)
=−∇ · 1 f (v,w)ρ(v,w,t)dw −0 (37)
v ρ (v,t) r
1
(cid:18) (cid:90) ρ(v,w,t) (cid:19)
=−∇ v· ρ 1(v,t) f r(v,w)(cid:82) ρ(v,w′,t)dw′dw (38)
=−∇ ·(cid:0) ρ (v,t)E [f (v,w)|v](cid:1) . (39)
v 1 w∼ρ(w|v;t) r
whereweusethedivergencetheoremforthesecondterminthesecondline.
ThecorrespondingODEdynamicsforthisLiouvilleequationeq.(39)is
dv
=E [f (v,w)|v]. (40)
dt w∼ρ(w|v;t) r
IfwetransformitbackintoHspace,itbecomes(informally)
∂ v =E [FAu|Fu=v], (41)
t u∼µt
as is presented in Section 2.3 in main text. This describes (one of) the optimal dynamics in the
reducedspace.
20B.2.3 TheEffectofClosureModeling
ApartfromtheoriginalmotivationofclosuremodelingtoapproximatethecommutatorFA−AF,
wealternativelyinterpretitasassigningavectorfieldA inthereducedspaceF(H)andaccordingly
θ
thecoarse-griddynamicsis
∂ v =A v, (42)
t θ
hereA playstheroleofAv+clos(v;θ)ineq.(3).
θ
WewillrefertobothA andclos(·;θ)asthetargetofclosuremodelingforbrevity.
θ
AsanapplicationofPropositionB.4,weonlyneedtocheckthesolutiontothestationaryLiouville
equation related to this dynamics to decide whether or not the resulting limit distribution is the
optimaloneρ∗.
1
B.3 DetailesforDiscussioninSection2.3
The dynamics of the filtered trajectory in Equation (41) (we will refer to the equivalent version
eq.(40)forconvenience),whichisalsoderivedin[50],hasinspiredmanyworksforthedesignof
closuremodels. Unfortunately,wewanttopointoutthatitisimpracticaltoutilizethisresultfor
closuremodeldesign.
Thedecisionregardingsubsequentmotionatthestate(v,t)havetomadeonlybasedoninformation
fromthereducedspace,whichcontainsmerelyvitselfandthedistributionofv.Foranygivenv,only
onepredictioncanbemadeforthenexttimestep. Similartothenon-uniquenessissuehighlightedin
Section2,however,sinceρ(w|v;t)dependsontandρ (theinitialdistributionofu∈H),typically
0
therearemultipledistinctρ(v,w,t)withexactlythesamevandmarginaldistributioninF(H).
Inpractice,ifonehopestofollowtheformofconditionalexpectationasineq.(40),hecanonly
fix one particular q(v,w), a distribution in H, and assign the vector field in reduced space as
E w∼q(w|v)[f r(v,w)(cid:12) (cid:12)v].
Now,wecheckthelimitdistributionwewillobtainwiththisdynamics. FromPropositionB.4,we
knowthatlimitdistributionρˆ (v)isthesolutionto(usuallyinweaksense)
1
(cid:18) (cid:19)
∇ v· E w∼q(w|v)[f r(v,w)(cid:12) (cid:12)v]ρ 1(v) =0. (43)
PropositionB.6. ρ∗isthesolutiontoeq.(43)ifq =ρ∗.
1
Proof. Bydefinition,ρ∗(v,w)satisfies
∇ ·(f (v,w)ρ∗(v,w,t))+∇ ·(f (v,w)ρ∗(v,w,t))=0. (44)
v r w u
Integraloverwandusedivergencetheorem,weyield
(cid:90)
0= ∇
·(cid:0)
f
(v,w)ρ∗(v,w)(cid:1)
dw+0 (45)
v r
(cid:90)
=∇ · f (v,w)ρ∗(v)ρ∗(w|v)dw (46)
v r 1
(cid:18) (cid:19)
=∇ v· E w∼ρ∗(w|v)[f r(v,w)(cid:12) (cid:12)v]ρ 1(v) (47)
Thisgivestheproof.
Thus, we show that ρ∗ is the correct choice for q to guarantee convergence towards ρ∗ in F(H).
1
Back to the learning methods discussed in Section 2.2, if we follow the new interpretation in
AppendixB.2.3,thelossfunctionis
J (θ)=E ∥A Fu−FAu∥2 (48)
ap u∼pdata θ
=E |f (v;θ)−f (v,w)|2, (49)
(v,w)∼pdata(v,w) r r
wherewetransformtheoriginalobjectivefunctionintoℓ2spaceofzinthesecondline,andf (·;θ)
r
isthecounterpartofA inℓ2,p istheempiricalmeasureoftrainingdatafromfully-resolved
θ data
simulations(FRS).
21DuetotheL2variationalcharacterizationofconditionalexpectation,theunderlyingchoiceofq(v,w)
inthoseexistinglearningmethodsisp . Consequently,onehastousenumerousFRStraining
data
dataduetotheslowconvergenceofempiricalmeasureofthehigh-dimensionaldistributionρ∗.
C ProofofTheorem2.1
ForthefirstclaiminTheorem2.1,ithasalreadybeenshowninthemaintextthatthemappingof
closuremodel u → (FA−AF)uisnotwell-defined. Wewillmake thisclaimmoreprecisein
AppendixC.1,andthengivetheproofforthesecondclaiminAppendixC.2andtheproofforthe
thirdclaiminAppendixC.3.
C.1 ProofofTheorem2.1(i)
Bytransformingtheoriginaldynamicsintothespaceofℓ2,itiseasiertoseewhythemappingof
closuremodelisnotwell-defined. SinceAFu=Au,weonlyneedtoshowthatu→FAuisnot
welldefined. Thecounterpartofthismappinginℓ2spaceisv→f (v,w). Ifitwereawell-defined
r
mapping,therewouldbeamappingf˜(v)suchthatf (v,w)≡f˜(v)forallw. Inotherwords,the
r r r
reducedsystemisindependentoftheunresolvedpart. Thispropertyrarelyholdsinmostdynamical
systems,exceptforafewtrivialcasesliketheheatequation.
Nextweshowthattheapproximationerrorhasapositivelowerbound.
Wecouldalwaysconstructu ,u ∈Hsuchthatu =u andFAu ̸=FAu . Therefore,forany
1 2 1 2 1 2
modelclos(u;θ)theapproximationerror
sup∥clos(u;θ)−(FA−AF)u∥ (50)
H
u∈H
≥ sup ∥clos(u;θ)−(FA−AF)u∥ (51)
H
u∈{u1,u2}
1(cid:0) (cid:1)
≥ ||(FA−AF)u −clos(u;θ)|| +||clos(u;θ)−(FA−AF)u || (52)
2 1 H 2 H
1
≥ ||(FA−AF)u −(FA−AF)u || (53)
2 1 2 H
1
= ∥F(Au −Au )∥ (54)
2 1 2 H
hasalowerboundindependentofthemodel,whereweapplythefactthatFu =Fu =uinthe
1 2
lastline.
C.2 ProofofTheorem2.1(ii)
Notations: RecallthatHisthefunctionspace,andD′isthesetofcoarsegridpoints,
D′ = {x ,x ,...,x }. Thefilteredvalueoftwofunctionsbeingthesameisequivalenttothefact
1 2 n
thatthesetwofunctionshavethesamevaluesonthegridpointsinD′.
DefinitionC.1. Definegrid-measurementoperator(atx )
0
I : H→R: u(cid:55)→u(x ) (55)
x0 0
Forbrevity,wewilluseI forI . WefurtherdefineI (abbreviatedasI ifthereisnoambiguity),
j xj D′
I : H→Rn, u(cid:55)→(u(x ),u(x ),...u(x ))T. (56)
D′ 1 2 n
Beforewedelveintothedetailsoftheproof,wewouldliketoremindthereadersofheatequationas
anconcreteandeasy-to-checkexamplewhereourresultholds.
Ouroriginaltheoremisstatedforacontinuoustimeinterval. Wefirstproveitsfiniteversion.
TheoremC.2. GivenD thesetoftimegrids,with|D | = N andD = {t ,t ,...t },forany
T T T 1 2 N
r ∈N,anyfunctionu ∈H∩C ,thereexistsanr-dimensionalmanifoldM ⊂H∩C suchthat
0 0 r 0
IS(t)u′ =IS(t)u , ∀t∈D , ∀u′ ∈M . (57)
0 T r
22Proof. For m ∈ N, given m linearly independent functions {ϕ } ⊂ H, we can construct
i 1≤i≤m
anaffinemanifoldA:=u +span{ϕ ,...ϕ }anddefinethefollowingmapping(spatiotemporal
0 1 m
grid-measurementoperator):
F=F :A→RnN (58)
DT
N
(cid:77)
v (cid:55)→ IS(t )v. (59)
j
j=1
NotethatwehaveacanonicalcoordinatesystemforA:
A ↔Rm (60)
m
(cid:88)
v =u + c ϕ ↔(c ,...c ), (61)
0 i i 1 m
i=1
thus,Fisamappingbetweenfinite-dimensionalmanifolds,andwecancomputeitsJacobian
J(v) ∈ Rm×Nn, whose elements consist of the grid-measurement of Gateaux derivatives,
I dS(t )(v,ϕ ), j ∈[n], k ∈[N], l∈[m].
j k l
ByageneralizationofSard’stheoremforBanachmanifold[70],weknowthatforanyu ,anyr,there
0
existsm = Nn+r linearlyindependentfunctions{ϕ }m suchthattheJacobianiseverywhere
i i=1
full-rank (i.e., rank= Nn) in the affine manifold A. By pre-image theorem, since F−1{Fu } is
0
non-empty(foratleastFu isinthisset),itisanm−Nn=rdimensionalmanifold. Thisgivesthe
0
proof.
CorollaryC.3. GivenD thesetoftimegrids,with|D |=N andD ={t ,t ,...t },forany
T T T 1 2 N
r ∈N,anyfunctionu ∈H∩C ,thereexistsinfiniteu′ ∈HsuchthatIS(t)u′ =IS(t)u forall
0 0 0
t∈D .
T
Proof. Foranyr ≥1,thereareinfinitepointsinthemanifoldweyieldinthetheoremabove.
RemarkC.4. Werequireu∈C onlytoexcludetheartificialcaseofmodifyingthefunctionvalue
0
onazero-measureset.
Fromtheresultabove,weseethatforanyu andfinitetime-gridsetD ,F−1{Fu }isaninfinite-
0 T 0
dimensionalmanifold. Tocompleteourproof,wemaketwotechnicalassumptions. Onecancheck
theseassumptionsforspecificdynamicalsystemstoderivethefinalresult. Wealsoremarkthatthey
arenottheweakestsetofassumptionstoguaranteethefinalresult,weadoptthemhereprimarilyto
keeptheproofconcise.
Foratime-gridsetD ,andafunctionu ,wedenoteasM (u )thesetofallfunctionsu′ that
T 0 DT 0
IS(t)u′ =IS(t)u , ∀t∈D .
0 T
AssumptionC.5. Foreveryu andfiniteD ,theinfinite-dimensionalmanifoldM (u )isun-
0 T DT 0
bounded.
Assumption C.6. Given any finite τ, and an arbitrary bounded set Ω ⊂ H, we could assign a
sequence of linearly independent functions {ϕ }∞ ⊂ H for each u ∈ H such that for any
i;u i=1
functions u, v, any subset B ⊂ N, and any finite subset D of [0,τ], there exists a continuous
T
mapping
(cid:92)(cid:8) (cid:9) (cid:92)(cid:8) (cid:9)
G: M (u) u+span{ϕ |i∈B} →M (v) v+span{ϕ |i∈B} (62)
DT i;u DT i;v
dependingonlyonu, v, B, D ,suchthat
T
sup ∥Gw−w∥≤C ∥u−v∥, (63)
Ω
w∈Ω∩MDT(u)
wheretheconstantC onlydependsonΩandτ,andnotonu, v, B, D .
Ω T
Beforemovingon,wefirstreviewaclassicalresult.
LemmaC.7. ThereexistsabijectionbetweenNandN2.
23Proof. Thisisastandardresultinsettheory,anditsproofcanbefoundinmanytextbooks.
We give an example on how to construct such a bijection (see Figure 3). We write 1,2,3,4,...
zigzaggingly to fill the N2 plane. In this way, we construct a mapping ι : N2 → N, with ι(i,j)
definedasthevaluewrittenat(i,j)-positionintheN2plane. Clearly,ιisabijection.
Withι,wecanpartitionNintoℵ 0(countablyinfinite)disjointsubsequences,{ι(i,j)} j∈Nforeachi.
1 2 6 7 ···
3 5 8 14 ···
4 9 13 ···
10 12 ···
. . . .
. . . .
. . . .
Figure3: IllustrationofabijectionbetweenNandN2usingazigzagnumberingscheme.
WearefinallyreadyfortheproofofTheorem2.1(ii).
TheoremC.8. Foranyu∈Handfiniteτ,thereexistinfiniteu′ ∈HsuchthatFS(t)u′ =FS(t)u
forallt∈[0,τ).
Proof. LetKbeadenseHilbertsubspaceofHthatcancompactlyembeddedintoH,withnorm
∥·∥ . Forinstance,ifH=Hk,theSobolevspaceW2,k,thenwecanchooseKasHk+1.
K
Step1: Wefirstdealwiththecasewhenu∈K.
Defineasequenceoftime-gridsetD as{ i τ|0≤i<2j}. SimilartotheargumentinTheoremC.2,
j 2j
wecanconstructasequence{ϕ }∞ ⊂Ksatisfyingthefollowingproperties.
i i=1
(i) Theyarelinearlyindependent.
(ii) Foranyj ∈N,m>2j,B ⊂Nwith|B|=m,thespatiotemporalgrid-measurementFfor
D′×D hasfull-rankJacobianeverywhereintheaffinemanifoldu+span{ϕ : k ∈B}.
j k
BasedontheN−N2bijectionι,wedefinethefollowingsubspaces:
E :=span{ϕ : i∈N}. (64)
k ι(k,i)
Thereareℵ suchsubspacesintotal,wewillnextfindapointu′ineachE suchthat
0 k
IS(t)u′ =IS(t)uforallt∈[0,τ).
WLOG,wewillonlyshowhowtoconstructu′inE .
1
Wedenote
M =E ∩M (u). (65)
j 1 Dj
BytheconstructionofD wehaveM ⊃M ⊃M ⊃....
j 1 2 3
Wefirstfixthreeconstants0<B <B , B >0andandconstructasequenceof{u }⊂E such
0 1 2 i 1
that
(i) u ∈M .
i i
(ii) B <∥u −u∥ <B .
0 i H 1
(iii) ∥u ∥ <B .
i K 2
24ThisconstructionisachievableduetoAssumptionC.5andthefactthatu∈K.
SinceKcanbecompactlyembeddedintoH,therethereexistsasubsequence{u }of{u }thatis
ij i
convergentinH. Wedenoteitslimitasu . From(ii),wehave∥u ∥ <∞andu ̸=u.
∞ ∞ H ∞
DuetoAssumptionA.9,wehaveu →u inC ,whichimpliesthatforanyD ,F u =F u.
ij ∞ 0 j Dj ∞ Dj
Because of the continuity of the mapping t (cid:55)→ I S(t)v for any x ∈ D′, v ∈ H, we know that
x
IS(t)u′ =IS(t)uforallt∈[0,τ).
Toconclude,foreachE ,thereexistsu′ ∈E thatisnotdistinguishablefromumerelybasedon
k k
functionvaluesrestrictedtothegridD′×D . Recallthatforanyj ̸=k,byconstructionwehave
T
E ∩E ={u},thustheseu′indifferentE aremutuallydifferent. Thiscompletestheproofforthe
j k k
caseu∈K.
Step2Nowwegivetheproofforgeneralu∈H.
SinceKisdenseinH,thereexistsasequence{un}⊂Ksuchthat∥un−u∥ < 1 . Wekeepusing
H 2n
theconstantB ,B ,B anddefineΩ:={v ∈H|∥v−u∥ <B +1}.FollowingAssumptionC.6,
0 1 2 H 1
weobtaintheconstantC andlinearlyindependentset{ϕ }∞ foreachn. Followingthefirst
Ω i;un i=1
partofthisproof,wedefine
En :=span{ϕ |i∈N} Mn :=En∩M (un), (66)
k ι(k,i);un j,k k Dj
andagainweonlyneedtoconsiderthecasewhenk =1,andthusabbreviateMn asMn.
j,k j
(cid:24) (cid:25)
Wewillrestrictourdiscussionwithinn>n := max{log 6(CΩ+1),log 3(C +1)} +1.
0 2 B0 2 Ω
Weinductivelyconstructasequence(indexedbyn)ofsequence{un} ⊂Enasfollows:
j j 1
(I)Forn=n ,weconstruct{un} thesameasthefirstpartoftheproof.
0 j j
(i) un ∈Mn.
j j
(ii) B <∥un−un∥ <B .
0 j H 1
(iii) ∥un∥ <B .
j K 2
(II)Nowsupposewehaveconstructed{un} ,weapplyAssumptionC.6forun, un+1,
j j
B ={ι(1,i)|i∈N}andD andobtainacontinuousmappingG. Wechooseun+1asGun.
j j j
Next,wegivesomeestimationsforun+1.
j
First,notethatwehave
3
∥un−un+1∥ ≤∥un−u∥ +∥u−un+1∥ < , (67)
H H H 2n+1
andthus∥un−un+1∥ ≤C 3 byconstruction. Basedonthis,wehave
j j H Ω2n+1
∥un+1−un+1∥ ≥∥un−un∥ −∥un−un+1∥ −∥un−un+1∥ (68)
j H j H j j H H
3(C +1)
≥∥un−un∥ − Ω . (69)
j H 2n+1
Byinduction,wehave
∥un+1−un+1∥ ≥∥un0 −un0∥ − (cid:88) 3(C Ω+1) > B 0. (70)
j H j H 2n+1 2
n>n0
Wealsohave
∥un+1−un+1∥ ≤∥un−un∥ +∥un−un+1∥ +∥un−un+1∥ (71)
j H j H j j H H
3(C +1)
≤∥un−un∥ + Ω . (72)
j H 2n+1
25Byinduction,wehave
∥un+1−un+1∥ ≤∥un0 −un0∥ +
(cid:88) 3(C Ω+1)
<B +
1
. (73)
j H j H 2n+1 1 2
n>n0
Similartowhatisdoneinthefirstpartoftheproof,wecanchoosevn0 asoneofthelimitpointsof
{un0} . Inductively,wecanconstructasequenceofvnsuchthat
j j
(i) vnisoneofthelimitpointsof{un}
j j
(ii) ∥vn−vn−1∥ H ≤ 3C 2Ω n+1.
Thus,{vn} isaCauchysequenceinHandwedenoteitslimitasv. BecauseofAssumptionA.9,
n
wehavethatIS(t)v =IS(t)u, ∀t∈[0,τ). Itisalsoclearthat
v ∈M
(u)∩(cid:8)
u+span{ϕ
|i∈N}(cid:9)
, (74)
[0,τ) ι(1,i);u
and
B
∥v−u∥ =∥ lim (vn−un)∥ ≥liminf∥un−vn∥ ≥ 0. (75)
H n→∞ H n→∞ H 2
Withexactlythesameargumentasinthefirststep,weconstructinfinitemutually-differentfunctions
thatarenotdistinguishablefromuonD′×[0,τ). Thiscompletestheproof.
C.3 ProofofTheorem2.1(iii)
TheoremC.9. Onecannotobtaintheρ∗ifthereisrandomnessintheevolutionofdynamics.
1
Proof. Ourproofwillbecarriedoutforamoregeneralsetting.
Considertwodynamics
dv
=b (v) (76)
dt 1
dv=b (v)dt+σ(v)dW. (77)
2
Thefirstonecorrespondstothedeterministicmotionasisintheoriginaldynamicalsystem(trans-
formed into ℓ2). Either ρ∗ it ρ∗ is the limit distribution of certain deterministic dynamics. The
1
secondonecorrespondstothedynamicsofthestochasticclosuremodel. HeredW isaddimensional
Brownianmotionwheredisthelatentdimensionofthemodel.
FromAppendixB,weknowthatthelimitdistributionofeq.(76)isthesolutiontostationaryLiouville
equation,
∇·(b ρ)=0. (78)
1
Asforeq.(77),similartohowwehandledeterministicsystemsinAppendixBandhowwederive
theFokker-Planckequationinfinite-dimensionalsystems,wecangeneralizeFokker-Planckequation
into function space and yield that the limit distribution of eq. (77) is the solution to stationary
Fokker-Planckequation,
1
∇·(b ρ)− ∇2 :(σσTρ)=0. (79)
2 2
Next,wearguebycontradictiontoshowthatρ∗orρ∗willnotsatisfyeq.(79).
1
Supposeρ∗isthesolutiontobotheq.(78)andeq.(79). Wethenhavethatforanyk ∈R,ρ∗isthe
1 1
solutionto
1
∇·(kb +b ρ)− ∇2 :(σσTρ)=0. (80)
1 2 2
Wecanexpandthisequationandwriteitinthefollowingform
A(v):∇2ρ+B (v)·∇ρ+C (v)ρ=0, (81)
k k
inparticular,A=−1σσT andC (v)hastheformC(v)+k∇·b .
2 k 1
26Recallthatρ∗issupportedonthecompactattractor(denotedasΩ)oftheoriginaldynamicalsystem.
1
Nowletusconsiderthemaximumpointofρ∗inΩ. Ifthereexistslocalmaximumpointv ∈Ω◦(its
1 0
interior),thentheHessianofρ∗atv issemi-negative-definiteand∇ρ∗(v )=0. Thuswehave
1 0 1 0
C (v )ρ∗(v )=−A(v):∇2ρ∗(v )≥0. (82)
k 0 1 0 1 0
WecouldalwayschooseksuchthattheL.H.S.hasanegativevalue. Contradiction!
Thissuggeststhatthereisnolocalmaximaofρ∗inΩ◦. Thus,themaximaofρ∗isontheboundary.
(cid:12) 1 1
However,ρ∗(cid:12) =0. Thissuggestsρ∗ =0. Contradiction!
1 ∂Ω 1
Thiscompletestheproof.
D ProofofTheorem3.1
Asapreliminaryresult,weshowthefollowingpropertiesofthedynamicalsystemsunderconsidera-
tion.
N
LemmaD.1. Foranyh>0,anyinitialconditionu ∈H, lim 1 (cid:80) δ =µ∗.
0 N→∞N
n=1
S(nh)u0
Proof. DenoteG:=S(h). FromAssumptionA.10,weknowthatforanymeasurablesetA, B ⊂H,
lim µ(A ∩ S(t)(B)) = µ(A)µ(B). In particular lim µ(A ∩ S(nh)(B)) = µ(A)µ(B). This
t→∞ n→∞
suggeststhatthesystemdefinedby
u =Gu (83)
n+1 n
ismixing.
FromLemmaA.6,thissystemisergodic,thushavinganinvariantmeasurew.r.tG.SinceG µ∗ =µ∗,
#
duetotheuniquenessofinvariantmeasure,wederivetheproof.
Inthefollowingproof,wewillusethenotationG := S(h),whichisthelearningtarget(ground-
truthoperator),andGˆ fortheapproximateoperatorweobtainaftertraining. Bythedesignofthe
neuraloperator, theinputofGˆ canbevectorsinRd foranydimensionalityd, servingasvarious
discretizationsofaparticularfunctionfromH. Hereweviolatetheconceptsalittlebitbydenoting
Gˆ as a mapping from H to H (approximating S(h)) instead of H to H ×[0,h](approximating
u → {S(t)u} )asisinouralgorithminmain. Inpractice,weonlyusethelastelementofthe
t≤h
outputsequence,correspondingtothepredictionforS(h)u,forestimatingstatistics. Tobespecific,
toestimatelong-termstatisticsincoarse-gridsystemswithlearnedoperatorGˆ,weuseasinputa
functioninreducedspacev(x) ∈ F(H), x ∈ D′ (equivalenttoanR|D′| vectorconsistingofthe
functionvaluesonthegrids),andautoregressivelycomputeGˆ(n)v, n∈N. Theinvariantmeasureis
estimatedby
N
1 (cid:88)
µˆ := lim δ . (84)
D′
N→∞N
Gˆ(n)v
n=1
Wefirstremindreadersofthefollowingfact.
FactD.2. Foranyfunctionu ∈H,let⃗u:=(u (x ),...u (x ))T, n=|D′|,bethediscretization
0 0 1 0 n
ofu inthecoarse-gridsystem. Thereexistsu∈H(possiblydifferentfromu )suchthat
0 0
⃗u=I u′, Gˆ(⃗u)=I Gˆu. (85)
D′ D′
Now,weproveourmainresult.
TheoremD.3. Foranyh>0andanyϵ>0,thereexistsδ >0suchthat,aslongas
∥Gˆu−Gu∥ <δ, ∀u∈H,wewillhaveW (µˆ ,ρ∗)<ϵ.
H H D′ 1
27Proof. WewillfirstdealwithdynamicsintheoriginalspaceH. Wehavetwodynamics,theexact
oneandtheapproximateone,
un+1 =Gun, u0 =u ∈H; (86)
0
uˆn+1 =Gˆuˆn, uˆ0 =u ∈H. (87)
0
Bothdynamicswillconvergetoanattractor,ΩandΩˆ,respectively.
WithLemmaA.3,weknowthatthereexistsη 0,η 1 >0suchthatforany{u n} n∈N ⊂Hsatisfying
(i)d(u ,Ω)<η ;(ii)∥u −G(u )∥<η foralln,thereexistsu˜∈Hsuchthat
n 0 n+1 n 1
∥u −G(n)u˜∥<ϵ, ∀n∈N. (88)
n
FromTherorem1.2inChapter1of[67],weknowthatthereexistsη >0suchthat
2
η
∥Gu−Gˆu∥<η , ∀u∈H⇒ dist(Ω,Ωˆ)< 0. (89)
2 5
Nowwechooseδasmin{η ,η }anddefinetheapproximateoperatorGˆ aswellasdynamicsand
1 2
attractoraccordingly.
We next choose n ∈ N such that sup dist(uˆn,Ωˆ) < η0. This implies that
0 n≥n0 5
sup dist(uˆn,Ω)< 3η0. WeapplyLemmaA.3toobtainafunctionu˜∈Hsuchthat
n≥n0 5
∥un−G(n−n0)u˜∥<ϵ, ∀n≥n . (90)
0
ForanyN ∈N,wedefine
N
1 (cid:88)
µ := δ (91)
N N G(n)u˜
n=0
1
n (cid:88)0+N
µˆ := δ . (92)
N N uˆn
n=n0
ByconstructingthetransportmappingT :un (cid:55)→G(n−n0)u˜, n
0
≤n≤n 0+N,wehavethat
W (µˆ ,µ )<ϵ. (93)
H N N
Notethatµˆ →µˆ (estimatedinvariantmeasurewithfine-gridsimulations)asN →∞,wederive
N D
W (µˆ ,µ∗)≤ϵ. (94)
H D
RecallthatP istheorthonormalprojectiontowardsF(H)andthat∥Pu−Pu′∥≤∥u−u′∥forany
u,u′ ∈H. InlightofFactD.2,wederiveW (µˆ ,ρ∗)≤ϵ.
H D′ 1
E ExperimentSetupandDataGeneration
E.1 Kuramoto–SivashinskyEquation
Weconsiderthefollowingone-dimensionalKSequationforu(x,t),
∂ u+u∂ u+∂ u+ν∂ u=0, (x,t)∈[0,L]×R , (95)
t x xx xxxx +
withperiodicboundaryconditions. Thepositiveviscositycoefficientν reflectsthetraceabilityofthis
equation. Thesmallerν is,themorechaoticthesystemis. Westudythecaseforν =0.01, L=6π.
FRSisconductedwithexponentialtimedifference4-orderRunge-Kutta(ETDRK4)[71]with1024
uniformspatialgridand10−4timegrid. TheCGSisconductedwiththesamealgorithmexceptwith
128uniformspatialgridsand10−3timegrid. Wechooseh=0.1forourmodel.
Dataset Thetrainingdatasetfortheneuraloperatorconsistsoftwoparts,theCGSdataandFRS
data. TheCGSdatasetcontains6000snapshotsfrom100CGStrajectories. Snapshotsarecollected
from time t = 20+k, k = 1,2....60. The data appears as input-label pairs (v(·,t),v(·,t+h)),
whereh=0.1forKS.TheFRSdatasetcontains105snapshotsfrom3FRStrajectories. Snapshots
arecollectedfromt=20+2k,k =1,2,...35. Thedataappearsasinput-labelpairs
(u(·,t),{u(·,t+ kh)|k =1,2,3,4}).
4
AsforinputfunctionsofPDEloss,theycomefromaddingGaussianrandomnoisetoFRSdata.
28EstimatingStatistics Forallmethodsinthisexperiment,statisticsarecomputedbyaveragingover
t∈[20,150]and400trajectorieswithrandominitializations.
E.2 Navier-StokesEquation
Weconsidertwo-dimensionalKolmogorovflow(aformoftheNavier-Stokesequations)foraviscous
incompressiblefluid(fluidfield)u(x,y,t)∈R2,
1
∂ u=−(u·∇)u−∇p+ ∆u+(sin(4y),0)T, ∇·u=0, (x,y,t)∈[0,L]2×R , (96)
t Re +
withperiodicboundaryconditions. Intheexperiment,wedealwiththevorticityformofthisequation.
1
∂ w =−u·∇w+ ∆w+∇×(sin(4y),0)T, (97)
t Re
wherew =∇×u. ThepositivecoefficientReistheReynoldsnumber. ThelargerReis,themore
chaoticthesystemis. WeconsiderthecaseRe=100, L=2π.
FRSisconductedwithpseudo-spectralsplit-step[72]with128∗128uniformspatialgridandself-
adaptivetimegrid. TheCGSisconductedwiththesamealgorithmexceptwith16∗16uniform
spatialgrids. Forourmodel,wechooseh=1.
Dataset Thetrainingdatasetfortheneuraloperatorconsistsoftwoparts,theCGSdataandFRSdata.
TheCGSdatasetcontains8000snapshotsfrom80CGStrajectories.Snapshotsarecollectedfromtime
t=80+4k,k =1,2....100.Thedataappearsasinput-labelpairs(u(·,t),{u(·,t+ k h)|k ∈[16]}).,
16
whereh=0.1forKS.TheFRSdatasetcontains110snapshotsfrom1FRStrajectories. Snapshots
arecollectedfromt=50+3k,k =1,2,...110. Thedataappearsasinput-labelpairs
(u(·,t),{u(·,t+ k h)|k ∈[16]}).
16
AsforinputfunctionsofPDEloss,theycomefromaddingGaussianrandomnoisetoFRSdata.
EstimatingStatistics Forallmethodsinthisexperiment,statisticsarecomputedbyaveragingover
t∈[1800,3000]and400trajectorieswithrandominitializations.
F ImplementationDetails
F.1 Physics-InformedOperatorLearning
Algorithm1Multi-stagePhysics-InformedOperatorLearning
Input: NeuraloperatorG ;trainingdatasetD (CGS),D (FRS),D (randomlysampled).
θ c f p
Hyper-parameters: TrainingiterationsN (i=1,2,3). Weightscombiningtwolossλ (t)
i i
(i=1,2),whichdecayastincreases. Parametersregardingoptimizer.
1: fort=1,··· ,N 1do
2: MinimizeJ(θ;D c)
3: fort=1,··· ,N 2do
4: Minimizeλ 1(t)J data(θ;D c)+J data(θ;D f)
5: fort=1,··· ,N 3do
6: Minimizeλ 2(t)J data(θ;D f)+J pde(θ;D p)
7: returnG θ
Followingthenotationsinthemain,weformallysummarizeouralgorithmasinAlgorithm1.
Forinputinitialvalueu (functionrestrictedonthegrid,whichisa1DtensorforKSand2Dtensor
0
forNS),werepeatu T timestomakeita2Dtensoror3Dtensor(u ,u ,...u ),respectively,where
0 0 0 0
T isahyperparameter. Fortheimplementations,theneuraloperatorwilllearntopredictthemapping
T (cid:18) (cid:19)
(cid:77) j
(u ,u ,...u )→ S h u , (98)
0 0 0 T 0
j=1
whichisadiscretizationof{S(t)u } .
0 t∈[0,h]
29KSEquation FollowingthearchitectureintheoriginalFNOpaper[31],ourmodelisa4-layer
FNOwith32hiddenchannelsand64projectionchannels. Wechooseh=0.1,T =64. Thedata
losswillonlybecomputedforthetimegridwherethereislabelinformation.
WefirsttrainthemodelwithCGSdata,weuseADAMforoptimization,withlearningrate5e-2,
schedulergamma0.7andschedulerstepsize100. Wetrainwithbatchsize32for1000epochs.
ThenwetrainthemodelwithCGSdataandFRSdata. λ (0)=1andhalvesevery100epochs. We
1
trainwithbatchsize32for250epochs.
Finally,wetrainthemodelwithPDEloss. Wetrainwithbatchsize8for1487epochs. Eachbatch
contains4functionsforcomputingthedatalossand4functionsforcomputingthePDEloss. λ (t)
2
decreasesby1.7forevery500epochs.
Whenwefinishtraining,theL2relativeerrorontheFRStestsetis∼12%.
NSEquation Ourmodelisa4-layerFNO,with32hiddenchannelsand64projectionchannels.
Wechooseh=1,T =32. Thedatalosswillonlybecomputedfortimegridwherethereislabel
information.
WefirsttrainthemodelwithCGSdata,weuseADAMforoptimization,withlearningrate4e-3,
schedulergamma0.6andschedulerstepsize50. Wetrainwithbatchsize32for60epochs.
ThenwetrainthemodelwithCGSdataandFRSdata. λ (t)=1 andhalvesevery100epochs.
1 t≤20
Wetrainwithbatchsize8for53epochs.
Finally,wetrainthemodelwithPDEloss. Wetrainwithbatchsize16for1530epochs. Eachbatch
contains8functionsforcomputingdatalossand8functionsforcomputingPDEloss. λ (t)decreases
2
by1.8forevery60epochs.
Whenwefinishtraining,theL2relativeerrorontheFRStestsetis∼19%. Thetrainingtakes∼40
minutestocomplete.
F.2 BaselineMethod: SingleStateClosureModel
The network follows the Vision Transformer [73] architecture. For KS equation, the input was
partitionedinto1×4patches,with2transformerlayersof6heads. Thehiddendimensionis96and
theMLPdimensionis128. ForNSequation,theinputwaspartitionedinto4×4patches,with2
transformerlayersof6heads. Thehiddendimensionis96andtheMLPdimensionis128. Forboth
experiments,weuseAdamWoptimizer[74]withlearningrate1e−4andweightdecay1e−4.
G MoreExperimentResultsandVisualizations
G.1 Statistics
Weformallyintroducethestatisticsweconsider.
TotalVariationforInvariantMeasures Asismentionedinthemaintext,weproposetodirectly
comparetheestimatedinvariantmeasureresultingfromthetimeaverageofsimulationsandthatof
groundtruth.
Recall that we have expanded u ∈ H onto orthonormal basis u =
(cid:80)∞
z ψ . In particular, for
i=1 i i
v ∈F(H),v ∈span{ψ :i≤|D′|}. Wecomputethetotalvariation(TV)distanceforthe(marginal)
i
distributionofeachv ,whereTVdistanceoftwodistributions(probabilitydensities)ν, µisdefined
i
as
1(cid:90)
d (µ,ν)= |µ(x)−ν(x)|dx. (99)
TV 2
For experiments we consider in this work, a natural choice of ψ is the Fourier basis functions,
i
{ei2k Lπx} k∈Zfor1DKSand{ei2 Lπ(kx+jy)} k,j∈Z2 for2DNS.
Withalittleabuseofdefinition,thecorrespondingz arecomplexnumbers. [75]showsthatthelimit
i
distributionofArgz isuniformdistributionon[0,2π]. Thus,itsufficestocheckthedistributionof
i
modelength|z |.
i
30OtherStatistics Inthefollowing,weuseuˆ todenotethek-thFouriermodeofu. Whenuisa
k
multi-variatefunction,kisatuple.
• EnergySpectrum.
O (u;k)=|uˆ |2(1D),O (u;k )=(cid:80) |uˆ |2(general).
e k e 0 |k|1=k0 k
Thek -thenergyspectrumisO (k):=E O (u;k ).
0 e u∼µ∗ e 0
• SpatialCorrelation.
O (u;h)=(cid:82) u(x)u(x+h)dx. ThehspatialcorrelationisO (h):=E O (u;h).
s s u∼µ∗ s
• AutoCorrelationCoefficient.
NotethatO isafunctionofh.
s
Thek-thAutoCorrelationCoefficientisO (k):=|(Oˆ ) |2.
a s k
• ThedistributionofVorticity(w(x)forNS)andVelocity(u(x)forKS).
• Thevarianceofthefunctionvalue.
(cid:90) (cid:90)
• DissipationRate: 1 −u(x)2dx,where −referstoaveragedintegral
Re
(cid:90) (cid:82) f(x)dx
− Ωf(x)dx:= Ω (cid:82) . (100)
dx
Ω
Inpractice,weusuallycheckthedistributionofthisquantity.
(cid:90)
• KineticEnergy: −(u−u¯)2dxwhereu¯(x):= lim 1 (cid:82)T u(x,t)dt. Inpractice,weusually
T→∞T 0
checkthedistributionofthisquantity.
Table4: ErroronDifferentStatistics: KSequationHeader: Fromlefttoright: Averagerelative
error on energy spectrum, max relative error on energy spectrum, average relative error on auto-
correlationcoefficient,maxerroronauto-correlationcoefficient,totalvariationdistancefrom(ground
truth)velocitydistribution,averagecomponent-wiseTVdistance(error),andmaxcomponent-wise
TVdistance(error).
Method Avg. Eng. MaxEng. Avg. Cor. MaxCor. Velocity Avg. TV MaxTV
CGS(Noclosure) 12.5169% 77.8223% 13.1275% 80.5793% 0.0282 0.0398 0.2097
Eddy-Viscosity[59] 7.6400% 48.3684% 8.7583% 56.5878% 0.0276 0.0282 0.1462
Single-state[30] 12.5323% 78.6410% 13.1052% 81.2461% 0.0280 0.0410 0.2111
OurMethod 7.4776% 20.4176% 7.8706% 22.7046% 0.0284 0.0272 0.0849
Table5: ErroronDifferentStatistics: NSequationHeader: Fromlefttoright: Averagerelative
erroronenergyspectrum,maxrelativeerroronenergyspectrum,totalvariationdistancefrom(ground
truth)vorticitydistribution,averagecomponent-wiseTVdistance(error),maxcomponent-wiseTV
distance(error),andvarianceofvorticity.
Method Avg. Eng. MaxEng. Vorticity Avg. TV MaxTV Variance
CGS(Noclosure) 178.4651% 404.9923% 0.1512 0.4914 0.8367 253.4234%
Smagorinsky[14] 52.9511% 120.0723% 0.0483 0.2423 0.9195 20.1740%
Single-state[30] 205.3709% 487.3957% 0.1648 0.5137 0.8490 298.2027%
OurMethod 5.3276% 8.9188% 0.0091 0.0726 0.2572 2.8666%
G.2 ExperimentResults
TheerrorofallstatisticsweconsideredforKSequationislistedinTable4andplottedinFigure4.
TheerrorofallstatisticsweconsideredforNSequationislistedinTable5andplottedinFigure7.
ThevisualizationofTVerrorforeach(marginal)distributionisshowninFigure5anditslogscale
visualizationisshowninFigure6.
31(a)EnergySpectrum (b)SpatialCorrelation
(c)AutoCorrelationCoefficient
Figure4: ExperimentResultsforKSEquation’FRS’(blueline)referstofully-resolvedsimulation,
andservesasgroundtruth. ’CGS’:coarse-gridsimulation(noclosuremodel). ’Eddy-Visc.’: classical
eddy-viscositymodel. ’Single’: learning-basedsingle-stateclosuremodel. Ourmethod(purple)is
closesttogroundtruthamongallcoarse-gridmethods.
G.3 AblationStudy
WecarryoutanablationstudyforKSequationtoverifytheeffectofpertainingwithdatalossand
CGSdataloss. ThetrainingdatasetisdescribedinAppendixE.TheresultsareasinFigure8.
WeconcludethatpretrainingwithdatalossisbeneficialtotheoptimizationofthePDElossfunction
and that pretraining with CGS data can improve the generalization property of the model. Even
though CGS data is potentially incorrect, it is utilizable for training because they contain some
informationoftheunderlyingPDE.
32(a)Coarse-gridSimmulation(noclosuremodel) (b)CGSwithSmagorinskymodel
(c)Learning-basedsingle-statemodel (d)Ourmethod
Figure 5: TV error for NS Equation The (k,j)-element represents the TV error regarding the
distributionofthemodelengthof(k,j)Fourierbasisei2 Lπ(kx+jy).
33(a)Coarse-gridSimulation(noclosuremodel)) (b)CGSwithSmagorinskymodel
(c)Learning-basedsingle-statemodel (d)Ourmethod
Figure6: log-scaleTVerrorforNSEquationThe(k,j)-elementrepresentsthelogarithmofTV
errorregardingthedistributionofthemodelengthof(k,j)Fourierbasisei2 Lπ(kx+jy).
34(a)EnergySpectrum (b)DissipationDistribution
(c)VorticityDistribution (d)KineticEnergyDistribution
(e)Distributionofcomponentfor(2,2)Fourierbasis (f)Distributionofcomponentfor(5,6)Fourierbasis
Figure7: ExperimentResultsforNSEquation’FRS’(blueline)referstofully-resolvedsimulation,
andservesasgroundtruth. ’CGS’:coarse-gridsimulation(noclosuremodel). ’Smag.’: classical
Smagorinskymodel. ’Single’: learning-basedsingle-stateclosuremodel. Ourmethod(purple)is
closesttogroundtruthamongallcoarse-gridmethods.
35(a) (b)
(c)
Figure8: ExperimentResultsduringtrainingforKSEquation(a)L2relativeerrorontraining
setforPDE-lossminimization. Theoptimizationachievesasmallerlosswhenthemodelhasbeen
pre-trainedwithdataloss(purplecurve).(b)L2relativeerroronthetestsetforPDE-lossminimization.
Themodelachievesasmallererrorwhenithasbeenpre-trainedwithdataloss(purplecurve). (c)L2
relativeerroronthetestsetforFRSdata-lossminimization. Themodelhasbettergeneralizationifit
hasbeenpre-trainedwithCGSdataloss(greencurve).
36