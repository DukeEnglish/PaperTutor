IEEE Internet of Things Journal (2024) 1
Meta-Learning Guided Label Noise Distillation
for Robust Signal Modulation Classification
Xiaoyang Hao, Zhixi Feng, Member IEEE, Tongqing Peng, and Shuyuan Yang*, Senior Member IEEE
Abstract—Automatic modulation classification (AMC) is features of signals. Convolution neural networks (CNNs) [8],
an effective way to deal with physical layer threats of the [9], recurrent neural networks (RNNs) [10], [11], transformer
internet of things (IoT). However, there is often label networks (TNs) [12], generative adversarial networks (GANs)
mislabeling in practice, which significantly impacts the [13], [14], and their hybrid networks [15] achieve superior
performance and robustness of deep neural networks
performance compared to traditional methods. However, the
(DNNs). In this paper, we propose a meta-learning guided
robustness of traditional methods and deep neural networks
label noise distillation method for robust AMC. Specifically,
(DNNs) has been a key factor limiting their applications.
a teacher-student heterogeneous network (TSHN)
There are some studies to improve the robustness of AMC.
framework is proposed to distill and reuse label noise.
Based on the idea that labels are representations, the In [16], an AMC method based on multi-task learning (MTL)
teacher network with trusted meta-learning divides and was proposed, and its generalization ability was derived from
conquers untrusted label samples and then guides the multi-task feature learning with knowledge sharing in different
student network to learn better by reassessing and noisy scenes. In [17], a joint framework consisting of
correcting labels. Furthermore, we propose a multi-view two-channel spectral fusion, signal enhancement, and signal
signal (MVS) method to further improve the performance of
classification was proposed, which significantly improved the
hard-to-classify categories with few-shot trusted label
performance and robustness of signal recognition corrupted by
samples. Extensive experimental results show that our
channel noise by integrating a multi-level attention mechanism
methods can significantly improve the performance and
into the framework. In [18], the communication accumulation
robustness of signal AMC in various and complex label
noise scenarios, which is crucial for securing IoT features were used to solve the severe AMC performance
applications. degradation caused by the multipath fading channel. Yun. L et
al. explored the impact of adversarial examples on the
Index Terms—Automatic modulation classification (AMC), robustness of DNN-based modulation recognition [19].
label noise, meta-learning (ML), few-shot trusted label Furthermore, to improve the robustness of training and test data
samples, multi-view signal (MVS). with distributional bias, K. Bu et al. proposed to perform the
asymmetric mapping between datasets by adversarial training
I. INTRODUCTION to improve domain generalization ability [20]. The above
W ITH the rapid development of 5G technology and the methods consider the signal sample noise caused by a complex
Internet of Things (IoT), the electromagnetic electromagnetic environment. However, their performance also
environment is becoming more and more complex. The relies on an important assumption: a large amount of labeled
inherent mobile nature of most IoT devices increases their risk data and that these labels are correct.
of exposure to untrusted electromagnetic environments. These Like most tasks in computer vision [21], [22], the annotation
devices are vulnerable to physical active attacks, such as of signals is expensive, time-consuming, and even error-prone.
deceptive jamming, pilot jamming, and sybil attacks [1], [2]. Solving the performance and robustness deterioration problem
Automatic modulation classification (AMC) can be used to caused by signal labeling errors is the primary motivation of
verify whether a signal is authorized or interfering. Therefore, our study. We conclude that the signal labeling errors mainly
AMC is an effective way to detect and identify physical layer come from the following aspects. 1) Difficult-to-describe
threats, which is essential to ensure the security of signals cause label noise. For example, for scarce signals such
communications and the reliability of IoT systems. In addition, as IoT attack against signals, non-cooperative communication
AMC is of great significance for military and civilian signals, and new institutional signals, their description
applications such as dynamic spectrum access, jammer information and representative samples are often insufficient. 2)
identification, and intruder detection [3]-[5]. Indistinguishable signals cause label noise. For low
Traditional AMC methods mainly include maximum signal-to-noise ratio signals, fine-grained signals (such as
likelihood methods [6] based on decision theory and pattern QAM16 and QAM64, or emitter signals with fingerprint
recognition methods [7] based on expert priors. These methods differences), signals of various complex channels, and signals
usually rely on expert features extracted from specific signal of different working states, it is difficult for even experts to
types. However, the expert features usually have problems such judge the category. 3) Individual differences cause label noise.
as unclear application scope, insufficient generalization, Differences in the professional level, subjective judgment, and
complex calculation, and difficult selection of parameters and working status of each worker will cause label noise. In
types, which are difficult to meet IoT devices' robustness and addition, constant signals are usually segmented into multiple
real-time usage requirements. In recent years, AMC methods samples, and signal mislabeling can result in batch labeling
based on data-driven and deep learning have achieved great errors for multiple signal samples. To sum up, it is difficult to
success, which can automatically extract high-level semantic guarantee the quality of signal labeling in practice. Therefore,IEEE Internet of Things Journal (2024) 1
AMC is usually a weakly supervised learning (WSL) with untrusted labeled samples with higher confidence to
inaccurate labels rather than strongly supervised learning. On participate in the learning of the student network.
the other hand, AMC is sensitive to label quality, and fitting to Untrusted labels with less confidence are distilled by an
noisy samples will greatly reduce the generalization attention mask for further reuse.
performance of the model or even bring disaster. For example, 3) A multi-view signal (MVS) method is proposed to
when noisy label samples are introduced as adversarial attack further improve the performance of hard-to-classify
samples, there is a high probability of misjudging friends and categories with few trusted label samples and a large
foes. Also, label noise may increase the number of training number of untrusted label noise samples.
features and model complexity. Therefore, weakly supervised 4) For symmetric, asymmetric, and mixed label noise in
AMC with label noise is crucial for IoT security, electronic practice, extensive experimental results show that our
countermeasures, and other applications. methods can significantly improve the performance and
To the best of our knowledge, signal label noise learning robustness of signal AMC.
(SLNL) as a common inaccurate supervision has not been The rest of this paper is organized as follows. The problem
systematically studied. Depending on whether the labeling definition is presented in Section II. The proposed TSHN and
scenario involves experts, we roughly classify the existing label MVS are introduced in Section III. The dataset construction
noise learning methods in computer vision into two categories: method, experiments, and discussions are provided in Section
with trusted sample (WTS) reference and without trusted IV. Lastly, the conclusions and future work are discussed in
sample (OTS) reference. The main drawback of the WTS Section V.
methods [23],[24] is that when the model falls into
optimization bias by incorrectly correcting for noisy labels, this II. PROBLEM STATEMENT
error can gradually accumulate and may lead to task failure.
Denote the modulated signal as the signal can be written
The main drawback of the OTS methods [25] is that they
require a small amount of trusted labeled data as a guide. As we as:
know from our practical experience, in most signal labeling (1)
scenarios, one or several experts lead many non-professional where n(t) is additive white Gaussian noise (AWGN).
workers to complete the labeling task for several months or
C(t) represents a response function of the complex radio
even longer, including completing the labeling task by
themselves, outsourcing to a qualified team, or even channel . * represents the time-domain convolution. The goal
crowdsourcing. Therefore, we can usually obtain a small of AMC with label noise is to robustly identify the modulation
amount of trusted labeled samples by experts and a large type of from . Without loss of generality, assume that
amount of untrusted labeled samples by non-professional the signal dataset with noisy labels
workers. However, there are two key issues to be addressed for is , and the unknown noise
SLNL: 1) How to re-estimate the labels of untrusted samples. 2)
distribution is , the goal of SLNL is to find the best mapping
How to extract, guide, and utilize a large number of untrusted
function . The label noise model can be formulated
labels with a small number of trusted labels.
To solve the above problems, we propose a teacher-student as follows:
heterogeneous network (TSHN) framework. The teacher  
network is a meta-learning method for processing few-shot (2)
trusted labeled data, while the student network is a DNN for 
processing a large amount of untrusted labeled data. We
re-evaluate the labels of untrusted samples with the idea that
labels are representations. By comparing with the trusted data,
we select the untrusted labeled samples with high confidence as
purified data through the attention mask. Therefore, as the loss
is continuously optimized, the purified data participates in the
training of the student network in a self-training manner. In
particular, for hard-to-classify categories with few trusted
samples, we propose a multi-view signal (MVS) method to
improve the guidance ability of the teacher network.
In summary, the main contributions of our work can be
summarized as follows: (a) SLN (b) ALN (flip one)
1) We propose TSHN for SLNL. Meta-learning based Fig.1. Examples of SLN and ALN (flip one).
teacher network for trusted few-shot learning (FSL), SLNL is mainly divided into symmetric label noise (SLN)
while a DNN-based student network learns a large and asymmetric label noise (ALN). In practice, it is possible
number of untrusted labeled samples with re-evaluated that SLNL is a mixture of SLN and ALN. The generation
labels. To the best of our knowledge, this is the first process of SLN is completely random, and it can be understood
work to study SLNL for robust AMC. that the real label y is flipped to other label i with the same
n
2) We re-evaluate labels by the idea of signal labels as probability . ALN refers to the fact that real labels are
feature representations. Through the joint loss
xn,i
flipped to other labels i with a different probability . For
optimization, the teacher network gradually guides xn,i
D  {
s ( t )
( x ,1
f :
y
1
X
r
) , ..., (
P
Y
y n
(
x
r ( t )  C
t )
, y ) } n
n
 i, i  [ k
y , p r o
n
( t ) *
( X ,
] , i 
b (1 
Y
s
y
( t ) 
)
n
, p rn
)xn
n
o
(
b
t )
i
r
yn
( t )
x ,in

xnIEEE Internet of Things Journal (2024) 1
ALN, a certain class is more likely to be mislabeled as a for multiple meta-tasks, the teacher network can quickly
specific label with a higher probability, i y , j y , transfer it to a new task with few-shot labeled samples. For the
n n
trusted support set and the trusted query set, we compute the
  .
cross-entropy loss between the predicted labels and the
We measure the performance of the classifier through the ground-truth labels.
loss function L(f(x),y) , and deal with label noise by 1 UV N
L  yclog F z  (5)
minimizing empirical riskR. The empirical riskRon signal t UV i  i
i1 c1
datasetDis defined as:
where represents the predicted label
 (3)
obtained through the fully connected output. and
represents the number of trusted support set samples and
The result of empirical risk minimization is expressed as:
trusted query set samples sampled by the current episode from (4)
the trusted set, respectively.
In practice, empirical risk minimization is insufficient to deal
with diverse label noise. Therefore, it is necessary to combine
the data itself, loss function, model structure, and training
method for collaborative optimization.
III. PROPOSED METHODS
In this section, we formally present TSHN and MVS for
SLNL.
A. Teacher-Student based Heterogeneous Network
(TSHN) framework
Based on the idea that labels are representations, we
calculate the soft labels of signal samples as their feature
Fig.2. Teacher-Student Heterogeneous Network (TSHN) framework.
representations. The soft label is usually a vector with a sum of
Different from the original prototype network, we start to
1, which can describe how similar the signal sample is to each
calculate the prototype after the number of iterations, which
category. Therefore, we can re-evaluate the confidence of
helps get a good initial prototype. In addition, we update the
untrusted labeled samples by computing their soft labels.
prototype vector every five episodes with excessive changes in
Specifically, for each untrusted sample, we compute its
prototype vectors. Suppose there is an embedding space where
similarity to the class prototype of the trusted samples in the
each sample is clustered around its own class prototype. A
feature space, thereby obtaining a soft label. The samples with
sample is mapped to the feature space using a feature
higher confidence are selected as purification-labeled samples,
extraction network (CNN2 [8]) to obtain . Therefore,
and the samples with lower confidence are distilled for further  
learning. During this process, few-shot trusted samples act as a one can obtain the prototype of the c-th class by calculating
guide to drive the meta-learning network to realize the
the mean of all supporting samples of that class in the
evolution of features and prototypes. On the other hand, a large
embedding space. The prototype of the support samples of
number of untrusted samples with high confidence are selected the class is denoted as
as purified samples to participate in the training of the student
1
network. It is worth noting that the purified samples learn in a P  F (x),c1,...,N (6)
c K 
self-training manner. That is, as the training progress, the yc
purified samples with greater confidence gradually participate Denote the new prototype of class c calculated every I
b
in the correction of model parameters. The framework of
iterations asPnew, then the prototype is updated as follows
TSHN is shown in Fig.2. c
1) The Learning Process of Teacher Network .P Pnew(1)P (7)
c c c
We improve the prototype network [26] as the teacher
where represents the prototype update step size, for the
network, which is a meta-learning method for few-shot
relative stability of the prototype evolution process
learning (FSL). During the meta-training process, the teacher
((0,1)and  in this paper). Then the probability that
network is trained on some meta-training tasks{ }G sampled
i i1
the sample xbelonging to the class c is
from a trusted set. The goal of meta-training is to learn a
exp(dist(F (x),P))
reliable model m based on the supporting samples in a p(xc)  c (8)
trusted support set ( J is the number of types and is the Nexp(dist(F (x),P ))
c'  c'
number of labeled samples in each class, and it is defined as a
where dist(,)can be some metrics such as Euclidean distance,
J-way I-shot meta-task.), which can classify the
cosine similarity, or a learnable metric. In this paper, we adopt
JH unlabeled samples (H is set to 15.) in the trusted query
the cosine similarity. p(xc)represents the probability that the
set into J classes. By learning and extracting meta-knowledge
untrusted labeled sample xbelongs to class c. Therefore, the
xn ,i

xn ,j
R
D
( f
f
)
*


1
n
a

r g
n L (i1
m in Rf
J  I
f
D
(
(
x
f
)i
)
, y
i
)
I
V
F
x
c
 z
i
  p


0
y
i
.3
| x
i

P
c
F
K
F ( x )
UIEEE Internet of Things Journal (2024) 1
soft label of x can be expressed as it is difficult to adapt to this situation due to the bounded
(9) gradient; 3) The loss of wrong label position is ignored, that is,
the relationship between categories is not considered.
The label confidence c is expressed as
Label smoothing is a regularization technique that perturbs
cc1p(x)Ty (10) the target variable to make the model less certain about its
where  represents the confidence update step size, which is predictions. That is, it limits the maximum probability of the
softmax function so that the maximum probability is not much
used for the relative stability of label confidence estimation
larger than the other labels (overconfidence). Therefore, for the
results ( and in this paper)
purified dataset , we design a cross-entropy loss with label
2) The Learning Process of Student Network
At the same time, we use CNN2 [8] as the student network, smoothing to combat the small amount of label noise still
which shares the weights with the teacher network. For the present.
untrusted query set, we compare it with the trusted support set
(1)  ce (15)
for feature similarity to obtain soft labels and confidence scores. p ce N
Its loss function is expressed as:
where is the standard cross-entropy loss, is a small
W N
L yc log F z  (11) positive number ( and in this paper), and is
ut i i  i
i1 c1
the number of classes. can be understood as: loss is a
where represents the number of samples sampled from the
untrusted query set in the current episode.  represents the penalty for "predicted distribution and real distribution" and
"predicted distribution and prior distribution (uniform
mask operation, when the confidence of the sample is greater
distribution)".
than ((0,1)and in this paper),  , otherwise
 . (16)

 (12)
For dataset D which still has more noisy labels, we use a u
forward correction loss [27].
In summary, the loss on the training set is expressed as
 py |x ,y ek logc C p yj |x  (17)
(13) corr i i i jk i i
j1
Through the guidance of the teacher network to the student where is a one-hot encoding form, representing the
network, the purified samples (untrusted samples with high standard vector. C ccis a noise transformation
confidence) gradually participate in the student network  
matrix that satisfies C  p y j|yi . In this paper, is
parameter learning, while the rest of the untrusted labeled ij
samples are distilled out as noise samples. That is, the entire estimated by the Gold Loss Correction (GLC) method [28]
dataset is divided into three parts: a small number of trusted using a trusted dataset .The loss function for the remaining
samples set , a purified sample set D (untrusted samples noisy label data is
p
Du
with higher confidence), and a noise sample set (untrusted L   F x ,y  (18)
samples with lower confidence). Although the TSHN distilled untrusted corr  i i
i1
most of the untrusted label samples, we made further In summary, the loss function in the training process of the
corrections with . This process is similar to a teacher entire neural network is as follows
teaching a student to complete most of the problems with a
(19)
small number of examples, and the problems that are difficult
for the student to complete need to be reviewed and confirmed
again. Based on the above insights, we use a
B. A Multi-View Signal (MVS) method
divide-and-conquer strategy for further learning onD ,D , and
t p Assuming that the time length of the signal sample is , it is
D . Specifically, for the trusted datasetD , we directly use the randomly divided into N segments .
u t
cross-entropy loss. Shuffle randomly and splicing the segments
L
Dt
 F x ,y  (14)
together in a new order, so that each shuffle can get a new view
trusted ce  i i sample of the same type. For the waveform variation triggered
i1 at the splice, it can be interpreted as a phase interference.
However, for purified data that may still have a small amount
of label noise, directly using the cross-entropy loss will bring
the following problems: 1) One-hot labels will make the
network overconfident and may overfit to a small amount of
label noise; 2) The one-hot label encourages the gap between
the category and other categories to be as large as possible, and
i
 0
W
 (
D
0 ,1
t
)
p
D
(
t
x )
W



L
[ p
0 .6
0 .5
' 
cls
1( x ) , p (
W
 ii1
L tU
 V
2x ) ,
c
i
L
ut
W '
, p ( Nx
i
i
) ]
c
i
D u
k th
e
ce
k
s ( t1 ) ~
D
s (
p
t
N

L
)
( 0 ,1 )
p
L purified
D
t
L
trusted
D

D pi
1
 L
t
0 .5
 Fp
N  N
purified
D p
s ( t )


 x , yi
i
L
untrusted
D
u
[ s ( t ) , s1

( t
2
) , ...,
C
ts
( t
N
ij
)N ]IEEE Internet of Things Journal (2024) 1
Fig.3. The process of MVS. are WB-FM, AM-SSB, and AM-DSB. The digital modulation
modes are BPSK, QPSK, 8PSK, 16QAM, 64QAM, BFSK,
CPFSK, and PAM4. The dataset contains a total of 220,000
samples. Each SNR contains 1000 samples for each modulation.
Each sample includes in-Phase and quadrature (IQ) channels,
and the data dimension is . The signal is modulated at a
rate of roughly eight samples per symbol with a normalized
average transmit power of 0dB. The dataset is generated in a
harsh simulated propagation environment, including additive
white Gaussian noise (AWGN), selective fading (Rician and
Rayleigh), sample rate offset (SRO) and center frequency
offset (CFO), etc.
Fig.4.The example of MVS (N=4).
RadioML2016.04C [8] The modulation modes are the same
as the RadioML2016.10A dataset. This is a variable-SNR
IV. EXPERIMENTS AND DISCUSSION
dataset with moderate LO drift, light fading, and numerous
A. Datasets different labeled SNR increments for use in measuring
We perform extensive experiments on two publicly available performance across different signal and noise power scenarios.
datasets with different channel environments and sample sizes. Furthermore, RadioML2016.04C contains fewer total samples
RadioML2016.10A [8] Radio2016.10A dataset contains (162,060) than RadioML2016.10A, which is used to validate
three analog modulations and eight digital modulations (from the performance of SLNL with fewer label.
−20 dB to +18 dB, 2 dB apart SNR) that are widely used in IoT
and wireless communications. The analog modulation modes
TABLE I
THE ACC (%) OF DIFFERENT METHODS WITH DIFFERENT LABEL NOISE RATES.
Noise Ratio (RadioML2016.10A, SLN)
Methods
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
MAE[29] 38.23 38.19 37.01 36.26 34.69 31.38 27.63 15.67 7.88 8.56 7.19
GCE[23] 40.71 40.15 39.91 38.70 38.60 37.54 35.07 24.42 12.43 9.10 6.23
GLC[29] 48.32 47.82 47.74 46.91 46.57 45.13 45.08 44.16 43.61 41.22 39.79
CNN2[8] 49.64 48.95 48.47 47.69 46.25 44.71 39.30 26.44 14.63 9.09 9.09
TSHN(Ours) 50.90 50.21 49.82 49.01 49.00 48.73 48.21 47.88 47.25 46.33 45.93
TSHN(↑) 1.26↑ 1.26↑ 1.35↑ 1.32↑ 2.75↑ 4.02↑ 8.91↑ 21.44↑ 32.62↑ 37.24↑ 36.84↑
B. Comparison methods and implementation details C. Experiment 1: Investigation on the AMC with SLN
We compare our method with MAE [29], GCE [23], GLC Based on the proposed method and the comparison methods,
[29] and CNN2 [8], where MAE and GCE are the typical we first performed a comparative analysis of AMC with SLN.
representatives of OTS methods and GLC is the typical Based on the RadioML2016.10A dataset, the table I shows the
representative of WTS methods. In addition, CNN2 with average accuracy of the different methods with different label
cross-entropy loss and two-layer convolution is a classical noise rates, where TSHN( ↑ ) represents the accuracy
baseline approach for AMC. Both the comparison methods and improvement score compared to CNN2 at the corresponding
our proposed TSHN use CNN2 as the feature extraction noise rate. We can conclude that our proposed TSHN
network, which facilitates fair comparison of all methods and outperforms the baseline method CNN2 in all noise rate cases,
references by later researchers. The hyper-parameters and the performance and robustness improvement becomes
associated with the feature extraction network (CNN2) and the more and more obvious as the noise rate increases. The
remaining hyper-parameters of the comparison methods use the maximum accuracy improvement reaches 37.24% when the
values from the original paper. For our proposed TSHN, the noise rate is 0.9. The comparison of the curves in Fig.5(a) and
teacher network uses the meta-task setting of "11way5shot". Fig.5(d) more visually represents the variation of CNN2 and
All algorithms are implemented with the PyTorch framework, TSHN methods with noise rate (label noise) and signal-to-noise
and all experiments are conducted on NVIDIA RTX 3090 24G. ratio (sample noise). The performance of the CNN2 method
For the experimental setup, the training set, validation set, starts to seriously degrade until complete failure when the noise
and test set are divided by 6:2:2. Then, the training set is further rate is greater than 0.5, while the TSHN method always
divided into two parts, the trusted set (default trusted fraction is maintains good performance and robustness. Moreover, for
1%) and the untrusted set. For untrusted sets, experiments are lower noise rates, CNN2 still exhibits a small amount of
performed on SLN, ALN, or a mixture of them. We use the performance degradation. In addition, WTS methods (GLC and
noise rate to denote the proportion of noise labels to the TSHN) significantly outperform CNN2 at label noise rates
untrusted labeled samples. In this paper, the noise rate takes 0 greater than 0.3, while OTS methods (GCE and MAE) show
to 1, and the interval is 0.1. We repeat each experiment five some failure or even inferiority to CNN2. The poor
times and average the accuracy to ensure the reliability of the performance of OTS methods is perhaps due to its optimization
results. More details will be introduced in the experimental bias affected by label noise, and the network is under-fitting.
part. Fig.6 shows the classification confusion matrix of CNN2 and
2  1 2 8IEEE Internet of Things Journal (2024) 1
TSHN. It can be seen that TSHN greatly improves the by sample similarity and sample noise. As shown in Fig.5 (b)
classification performance due to the correction of label noise, and Fig.5 (e), our proposed method achieves similar results on
while the remaining misclassification cases are mainly caused RadioML2016.04C.
(a) CNN2 (RadioML2016.10A, SLN) (b) CNN2 (RadioML2016.04C, SLN) (c) CNN2 (RadioML2016.10A, mixed label noise)
(d) TSHN (RadioML2016.10A, SLN) (e) TSHN (RadioML2016.04C, SLN) (f) TSHN (RadioML2016.10A, mixed label noise)
Fig.5. Accuracy for different noise rates and SNRs.
(a) CNN2 (RadioML2016.10A, SLN, NR=0.5) (b) CNN2 (RadioML2016.04A, SLN, NR=0.8) (c) CNN2 (RadioML2016.04C, mixed label noise=0.6)
(d) TSHN (RadioML2016.10A, SLN, NR=0.5) (e) TSHN (RadioML2016.04A, SLN, NR=0.8) (f) TSHN (RadioML2016.04C, mixed label noise=0.6)
Fig.6. Classification confusion matrix for CNN2 and TSHN (RadioML2016.10A uses non-negative SNR samples and RadioML2016.04C uses full SNR samples)First Author et al.: Title 8
RadioML2016.04C (Samples of all signal-to-noise ratios),
D. Experiment 2: Investigation on the Few-Shot Trusted
respectively. Among them, when the label noise rate is 0.8, and
AMC with SLN
above, CNN2 completely fails, while TSHN(↑) reaches
Although TSHN has shown good performance and 28%~49%
robustness under 1% trusted fraction condition (about 1200
trusted labeled samples per class). However, only very few F. Experiment 4: Investigation on the Few-Shot trusted
trusted labeled samples might be available under extreme AMC with ALN (Flip One) and Difficulty Classes
conditions, such as the scarcity type at the tail end of the long In practice, indistinguishable categories tend to have a small
tail distribution. In addition, scarcity types tend to have a high number of trusted labeled samples and high label noise rates
labeling noise rate. This is due to the fact that when there are because it is difficult to determine the label. Therefore, in this
very few reference samples, markers are more likely to section, we set up asymmetric label noise (QAM16 and
mislabel samples of similar types as the scarce type. Therefore, QAM64, QPSK, and 8PSK) for hard-to-classify pairs and
we further investigate the performance of TSHN under the classify them with very few trusted labeled samples. Fig.7
conditions of very few trusted samples and a higher label noise shows the comparison results of CNN2, TSHN, and TSHN+
rate. The results in Table Ⅱ show that the robustness MVS. It can be concluded that TSHN improves the accuracy by
improvement of TSHN becomes more and more significant as 6.75%~9.03% compared with CNN2 under the experimental
the number of trusted labeled samples gradually increase, and settings in this section. Moreover, our proposed MVS further
there is a substantial improvement at positive signal-to-noise improves the performance when used in TSHN, that is, the
ratios (with more label noise and fewer sample noise). accuracy is improved by 13.55%~15.4% compared to CNN2.
Moreover, when the label noise rate is 0.8 and 1.0, TSHN with Among them, we perform MVC 20 times, and the multi-view
only 5 trusted labeled samples per class could achieve far signal samples are assigned the same labels as the original
better performance than CNN2. When there are 60 trusted samples. According to the settings in this section, we can
labeled samples per class, TSHN outperforms CNN2 quickly obtain pseudo-labels of untrusted samples according to
comprehensively. In conclusion, TSHN is still effective under the learning of few-shot trusted AMC (TSHN+MVS) so as to
the condition of few trusted labeled samples and more label assist and improve the labeling efficiency and accuracy of
noise, which can meet the robust AMC requirements of professionals.
extreme IoT environments to a certain extent.
E. Experiment 3: Investigation on the AMC with Mixed
Label Noise
In practice, it is often a mixture of symmetric and
asymmetric label noise due to factors such as the inherent
indistinguishable properties of the sample, sample noise, or
subjective staff factors. For this reason, we further explore the
performance of TSHN. Specifically, we set ALN(flip one) for
the indistinguishable categories in CNN2 [8] (QAM16 and
QAM64, QPSK and 8PSK), while the rest of the categories are
set to SLN. We call it mixed-label noise. It can be concluded
from Fig.5(c) and Fig.5(f) that when the fraction ratio of the
trusted labeled samples is 1% and the untrusted samples are
mixed label noise, compared with CNN2, TSHN always
maintains stronger robustness. TSHN(↑) in Table Ⅲ shows the
accuracy improvement of TSHN over CNN2 on Fig.7. Accuracy of the Few-Shot trusted AMC with ALN (flip One) and
difficulty classes (RadioML2016.10A, non-negative SNRs).
RadioML2016.10A (Samples of all signal-to-noise ratios) and
TABLE Ⅱ
THE ACC (%) OF TSHN WITH DIFFERENT LABEL NOISE RATES AND VERY FEW TRUSTED LABELED SAMPLES.
Noise Trusted SNRs (RADIOML2016.10A, SLN)
Ratio Samples -18 dB -12 dB -6 dB 0 dB 6 dB 12 dB 18 dB
5 10.23 10.49 32.19 36.44 46.38 47.83 47.98
0.6 10 10.40 10.41 31.28 45.23 56.12 60.56 60.29
60 9.98 12.56 34.74 53.26 68.89 70.43 70.56
120 10.00 10.95 33.50 55.59 71.18 73.64 73.78
5 10.42 10.35 26.66 31.49 38.56 40.23 40.99
0.8 10 9.22 10.80 26.70 32.23 44.69 45.23 45.28
60 10.01 11.28 30.24 42.14 62.25 63.20 63.21
120 10.00 12.00 31.18 49.00 61.55 66.45 65.01
5 9.63 10.25 13.56 24.78 26.23 25.62 26.24
1.0 10 10.47 9.63 22.46 27.27 31.29 33.71 31.26
60 10.76 11.23 25.21 31.11 40.25 42.26 41.24
120 10.00 11.09 29.77 36.42 43.09 44.59 44.18First Author et al.: Title 8
TABLE Ⅲ
THE ACC (%) OF DIFFERENT METHODS WITH DIFFERENT LABEL NOISE RATES.
Noise Ratio (RADIOML 2016.10A, mixed label noise)
Methods 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
CNN2[8] 50.38 49.36 47.99 47.45 46.47 40.18 29.72 28.19 20.05 8.10 8.08
Ours 50.50 50.37 49.72 49.33 48.97 49.00 48.56 47.60 47.92 47.36 46.67
TSHN(↑) 0.12↑ 1.01↑ 1.73↑ 1.88↑ 2.50↑ 8.82↑ 18.84↑ 19.41↑ 27.87↑ 39.26↑ 38.59↑
Noise Ratio (RADIOML 2016.04C, mixed label noise)
Methods 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
CNN2[8] 55.26 54.80 53.46 53.28 51.55 50.35 44.81 40.45 9.08 2.63 2.89
TSHN(ours) 54.50 54.31 53.96 54.15 53.62 53.77 53.48 52.89 52.32 51.74 51.70
TSHN(↑) 0.76↓ 0.49↓ 0.50↑ 0.87↑ 2.07↑ 3.42↑ 8.67↑ 12.44↑ 43.24↑ 49.11↑ 48.81↑
Classification Scheme Based on LSTM With Random Erasing and
Attention Mechanism," in IEEE Access, vol. 8, pp. 154290-154300,
V. CONCLUSION
2020.
In this paper, we propose a robust AMC against label noise [12] J. Cai, F. Gan, X. Cao and W. Liu, “Signal Modulation Classification
for the first time, which is derived from our practical Based on the Transformer Network,” IEEE Transactions on Cognitive
Communications and Networking, vol. 18, no.9, pp. 1-9, 2022.
application. To achieve this goal, we propose a meta-learning
[13] K. Bu, Y. He, X. Jing and J. Han, “Adversarial Transfer Learning for
guided label noise distillation method called TSHN. TSHN Deep Learning Based Automatic Modulation Classification”, IEEE
combines the idea of the signal label as a representation to Signal Processing Letters, vol. 27, pp. 880-884, 2020.
realize the divide and conquer of label noise samples. [14] Q. Wang, P. Du, X. Liu, J. Yang and G. Wang, “Adversarial
unsupervised domain adaptation for cross scenario waveform
Furthermore, our proposed MVS further improves the label
recognition”, Signal Processing, 2020.
noise for hard-to-classify. Extensive experiments show that [15] N. E. West and T. O’Shea, ‘‘Deep architectures for modulation
TSHN has a strong tolerance to all label noise rates. Moreover, recognition,’’ in Proc. IEEE Int. Symp. Dyn. Spectr. Access Netw.
TSHN shows better performance in the cases of symmetric, (DySPAN), pp. 1–6, Mar. 2017.
[16] Y. Wang et al., ‘‘Multi-Task Learning for Generalized Automatic
asymmetric, and mixed label noise. Even extreme cases
Modulation Classification Under Non-Gaussian Noise With Varying
perform well, including few-shot trusted labeled sample guide SNR Conditions,’’ IEEE Trans. Wirel. Commun., vol. 20, no. 6, pp.
and hard-to-classify label noise. Therefore, our proposed 3587–3596,Mar. 2021.
methods are of great significance for ensuring applications such [17] S. Lin, Y. Zeng and Y. Gong, ‘‘Modulation Recognition Using Signal
Enhancement and Multi-Stage Attention Mechanism,’’ IEEE Trans.
as IoT security and electronic countermeasures in complex and
Wirel. Commun., vol. 20, no. 6, pp. 1-15,Mar. 2022.
interference electromagnetic environments. In the future, we [18] H.C. Wu, et al., ‘‘Novel Automatic Modulation Classification Using
will explore robust AMC methods with better performance to Cumulant Features for Communications via Multipath Channels,’’ IEEE
deal with the dual problems of label noise and sample noise. Trans. Wirel. Commun., vol. 7, no. 8, pp. 3098–3105,Mar. 2008.
[19] Y. Lin, et al., ‘‘Threats of Adversarial Attacks in DNN-Based
Modulation Recognition,’’ IEEE INFOCOM., July. 2020.
REFERENCES
[20] K. Bu, Y. He, X. Jing and J. Han, “Adversarial Transfer Learning for
[1] N. Wang et al., “Physical-layer security of 5G wireless networks for IoT: Deep Learning Based Automatic Modulation Classification”, IEEE
Challenges and opportunities,” IEEE Internet Things J., vol. 6, no.5, pp. Signal Processing Letters, vol. 27, pp. 880-884, 2020.
8169-8181, 2019. [21] C. Zhang, S. Bengio, M .Hardt, B. Recht, O. Vinyals, “Understanding
[2] M. Serror et al., “Challenges and Opportunities in Securing the deep learning (still) requires rethinking generalization”,
Industrial Internet of Things,” IEEE Trans. Ind. Informat., vol. 17, no. 5, Communications of the ACM, vol64, pp 107–115, 2021.
pp.2985-2996, 2021. [22] Y. Liu, N. Xu, Y. Zhang, X. Geng, “Label Distribution for Learning with
[3] M. Liu et al., “Data-Driven Deep Learning for Signal Classification in Noisy Labels”, International Joint Conference on Artificial Intelligence
Industrial Cognitive Radio Networks”, IEEE Trans. Ind. Informat., vol. (IJCAI), pp. 2568-2574, 2020.
17, no. 5, pp.3412-3421, 2021. [23] Z. Zhang, M. Sabuncu, “Generalized Cross Entropy Loss for Training
[4] S. Yang et al., “One-Dimensional Deep Attention Convolution Network Deep Neural Networks with Noisy Labels”, Conference on Neural
(ODACN) for Signals Classification”, IEEE Access, vol. 8, pp. Information Processing Systems (NeurIPS 2018), pp 1-14, 2018.
2804–2812, 2020. [24] J. Li, R. Socher, S. Hoi, “DivideMix: Learning with Noisy Labels as
[5] Q. Peng et al., “A Support Vector Machine Classification-Based Signal Semi-supervised Learning”, International Conference on Learning
Detection Method in Ultrahigh-Frequency Radio Frequency Representations (ICLR), pp. 1-14, 2020.
Identification Systems”, IEEE Trans. Ind. Informat., vol. 17, no. 7, [25] D. Hendrycks et al. “Using Trusted Data to Train Deep Networks on
pp.4646-4656, 2021. Labels Corrupted by Severe Noise”, Advances in Neural Information
[6] S. Majhi et al., ‘‘Hierarchical hypothesis and feature-based blind Processing Systems (NeurIPS 2018), pp1-17, 2018.
modulation classification for linearly modulated signals,’’ IEEE Trans. [26] J. Snell, K. Swersky, and R. Zemel, “Prototypical Networks For Few
Veh. Technol., vol. 66, no. 12, pp. 11057–11069, Dec. 2017. Shot Learning,” in Proc. Adv. Neural Inform. Process. Syst., 2017.
[7] A. Swami and B. M. Sadler, ‘‘Hierarchical digital modulation [27] G.Patrini et al., “Making deep neural networks robust to label noise: A
classification using cumulants,’’ IEEE Trans. Commun., vol. 48, no. 3, loss correction approach.”, IEEE Conference on Computer Vision and
pp. 416–429,Mar. 2000. Pattern Recognition (CVPR), pp.1944–1952, 2017.
[8] T. J. O’Shea, T. Roy and T. C. Clancy, “Convolutional radio modulation
[28] B. Han et al., “Co-teaching: Robust training of deep neural networks with
recognition networks,” in Proc. Int. Conf. Eng. Appl. Neural Netw., pp.
extremely noisy labels.”, Conference and Workshop on Neural
213–226, 2016.
Information Processing Systems (NIPS), pp.8527–8537, 2018.
[9] T. J. O’Shea, T. Roy, and T. C. Clancy, “Over-the-air deep learning
[29] A. Ghosh et al., “Co-teaching: Robust loss functions under label noise for
based radio signal classification,” IEEE J. Sel. Topics Signal Process.,
deep neural networks.”,(AAAI), pp.1919–1925, 2017.
vol. 12,no. 1, pp. 168–179, Feb. 2018.
[10] S. Rajendran, W. Meert, D. Giustiniano, V. Lenders and S. Pollin, ‘‘Deep
learning models for wireless signal classification with distributed
low-cost spectrum sensors,’’ IEEE Trans. Cognit. Commun. Netw., vol.
4, no. 3, pp. 433–445, Sep. 2018.
[11] Y. Chen, W. Shao, J. Liu, L. Yu and Z. Qian, "Automatic Modulation