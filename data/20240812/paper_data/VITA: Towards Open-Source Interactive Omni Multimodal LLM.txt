VITA: Towards Open-Source Interactive
Omni Multimodal LLM
ChaoyouFu♠,HaojiaLin3,ZuweiLong1,YunhangShen1,MengZhao1
YifanZhang,XiongWang1,DiYin1,LongMa1,XiawuZheng3
RanHe4,RongrongJi3,YunshengWu1,CaifengShan2,XingSun1,†
1TencentYoutuLab,2NJU,3XMU,4CASIA
♠ProjectLeader †CorrespondingAuthor
https://vita-home.github.io
Abstract
The remarkable multimodal capabilities and interactive experience of GPT-4o
underscoretheirnecessityinpracticalapplications,yetopen-sourcemodelsrarely
excelinbothareas. Inthispaper,weintroduceVITA,thefirst-everopen-source
MultimodalLargeLanguageModel(MLLM)adeptatsimultaneousprocessing
andanalysisofVideo, Image, Text, andAudiomodalities, andmeanwhilehas
anadvancedmultimodalinteractiveexperience. StartingfromMixtral8×7Bas
alanguagefoundation,weexpanditsChinesevocabularyfollowedbybilingual
instructiontuning. Wefurtherendowthelanguagemodelwithvisualandaudio
capabilitiesthroughtwo-stagemulti-tasklearningofmultimodalalignmentand
instructiontuning. VITAdemonstratesrobustfoundationalcapabilitiesofmulti-
lingual,vision,andaudiounderstanding,asevidencedbyitsstrongperformance
acrossarangeofbothunimodalandmultimodalbenchmarks. Beyondfoundational
capabilities,wehavemadeconsiderableprogressinenhancingthenaturalmulti-
modalhuman-computerinteractionexperience. Tothebestofourknowledge,
wearethefirsttoexploitnon-awakeninginteractionandaudiointerruptinMLLM.
Wedesignadditionalstatetokens,andcorrespondingtrainingdataandstrategiesto
perceivevariousinteractionscenarios. ThedeploymentofVITAemploysaduplex
scheme,whereonemodelisresponsibleforgeneratingresponsestouserqueries,
and the other continuously tracks environmental inputs, selectively outputting
newresponseswithupdatedinteractions. ThisallowsVITAtofeatureimpressive
human-machineinteractionfunctionalitiessuchasnon-awakeninginteractionand
audiointerruptinteraction. VITAisthefirststepfortheopen-sourcecommunity
toexploretheseamlessintegrationofmultimodalunderstandingandinteraction.
WhilethereisstilllotsofworktobedoneonVITAtogetclosetoclose-source
counterparts, we hope that its role as a pioneer can serve as a cornerstone for
subsequentresearch.
1 Introduction
Large Language Models (LLMs) have undergone significant evolution [39, 21, 9, 48, 12] and
rencently,wehavewitnessedtheflourishingofMultimodalLargeLanguageModels(MLLMs)[52,
29, 31, 30, 40], which exhibit surprising multimodal capabilities. Particularly, GPT-4o [40] has
significantly advanced the field of MLLMs, distinguishing itself with two key attributes: (1) a
unifiedframeworkthatprocessestext,vision,andaudiosignalsinanend-to-endmanner,and(2)
thecapabilitytoenablenaturalmultimodalhuman-computerinteraction. Thesebreakthroughshave
establishedanewstandardinthediscipline. However,thereisaconspicuousabsenceofopen-source
Email:{bradyfu24,winfred.sun}@gmail.com
4202
guA
9
]VC.sc[
1v11250.8042:viXraLimitation I Limitation II
Awakening is Necessary UninterruptibleandAudioSupportOnly
I amajuniorvoiceassistant, Wait un�l agent
ThecapitalofFranceisParis.
…, what can I do for you? finishes replying
Hi!Siri~ Who are you? What is the capital of France?
or
Wake-up Word Button 1stQuestion 2edQuestion
VITA
Mul�modal Input Support NoWake-upWordNeeded Interrup�on-Friendly
Interruptandreplyto
T wh ee ll -v ci ad pe to u ry eo du mto oo mk ei ns tb te ha au tt sif hu ol w… cai st e's s a Taking better videos involves the latest ques�on Y wo hu ic'r he iw s e aa cr li an sg s ia c b al na dck c ato sp u aa ln cd o mbl bu ie n j ae ta ion ns, .
severalkeyelements.Here
your photographic skills effectively. Hereareafewsuggestions:…
Please rate the How to take Forget it, it’s too difficult. Take a look
video I took. better videos? at my current clothes and give me
Background someoutfitsuggestions.
1st Question Noise 2ed Question 3rd Question
Figure1: InteractionofVITA.Traditionalaudiointeractionrequiresapre-definedwake-upword,
e.g., saying “Hi! Siri∼” every time you ask a question, or a button to control the input question
audio(Limitation1). Thehuman-computerinteractionisalwaysblockedwhenthemodelgenerates
output,astheprevioussystemcanonlyrespondtoinputqueriessequentially(Limitation2). By
contrast,ontheonehand,unlikepreviousmethodswhereaudio,text,andvisionarealwaysseparated,
VITAsupportsthesemodalitiesend-to-end. Ontheotherhand,VITAmakestwocontributionsto
multimodalinteraction. Non-awakeningInteraction: VITAautomaticallyfiltersbackgroundnoise
likenon-queryhumanvoices, therebyeliminatingtheneedforthewake-upwordandthebutton.
AudioInterruptInteraction: Iftheuserinterruptswithanotherquestion,thegenerationprocessis
paused,andthemodelimmediatelyrespondstothelatestquery.
modelsthatechothesehighlights. Thepressingneedforopen-sourcecommunitytofurtherpromote
developmentinthisfieldcannotbeoverstated.
ThispaperintroducesVITA,asapioneerthathasinitiallyachievedthebothtwoattributes,byintegrat-
ingarchitecturalinnovationswithadvancedtraininganddevelopmentstrategies. Thefunctionality
andarchitectureofVITAaredepictedinFig.1andFig.2,respectively. TheimplementationofVITA
mainlycomprisesthreesteps:
Bilingual Instruction Tuning of LLM. The official Mixtral 8×7B [22] always lacks proficient
Chineselanguageexpressionandcomprehension. Totacklethis,weexpandthevocabularyofthe
basemodelandcontinuedwithfurtherinstructiontuningusingthecollectedhigh-qualitybilingual
textcorpus. ThismakestheLLMproficientinbothChineseandEnglish.
Multimodal Alignment and Instruction Tuning. To align the text feature space with video,
image,andaudio,wecollectmassivehigh-qualitymultimodaldatatoalignindividualencodersand
connectors, which process different modalities respectively, to the LLM. Multimodal instruction
tuningdataaremeticulouslyconstructed. WhilegivingVITAapowerfulmultimodalfoundational
capability,weteachittorecognizethetypeofinputqueriesend-to-endbyintroducingastatetoken.
Thismakesitpossibletointeractwithoutaudioawakeningwhileinference.
DevelopmentwithDuplexPipeline. Intermsofmodeldeployment,weintroduceaduplexscheme.
AsshowninFig.2,twoVITAmodelsaredeployedsimultaneously: oneisresponsibleforgenerating
responsestothecurrentaudioquery,andtheothercontinuouslymonitorsforthenewone. Ifany,
thecurrentgenerationisinterrupted,andthemodeloutputstheresponsetothenewquery. Inorder
toimprovetheefficiencyoftheinteraction,wehavecarriedoutmassiveengineeringoptimizations,
suchasadaptingmultimodalvLLM[26].
Thecontributionsofthispaperareasfollows:
• Wedevelopanopen-sourcehigh-performancemultimodalbasemodelthatsimultaneously
supports video, image, text, and audio inputs in both English and Chinese. The model
acceptseitherpuretext/audioinputsorvideo/imagecombinedwithtext/audioinputs. We
designacomprehensivetrainingprocess,whichincludesenhancingtheChinesecapabilities
oftheLLM,constructingmultimodaltrainingdata,andamulti-stagetrainingpipeline.
2Aggregate Historical Context
State Token
Interrupt Generation
<1> Query Audio Text-to-Speech Interrup�on
<2> NoisyAudio Toolkit
<3> Query Text <3> <2> <1> <1> <2>
Aggrega�on
VITA (Generation) VITA (Monitoring)
Modality Encoders Audio Encoder
Audio Image Video Text Environmental Audio
Figure2: ArchitectureofVITA.VITAiscapableofprocessinginputsintheformofpuretext/audio,
aswellasvideo/imagecombinedwithtext/audio. Besides,wesetdifferentstatetokensfordifferent
queryinputs. <1>correspondstotheeffectivequeryaudio,suchas“whatisthebiggestanimalin
theworld?”,forwhichweexpectaresponsefromthemodel. <2>correspondstothenoisyaudio,
suchassomeoneintheenvironmentcallsmetoeat,forwhichweexpectthemodelnottoreply. <3>
correspondstothequerytext,i.e.,thequestiongivenbytheuserintextform. Duringthetraining
phase, wetrytoteachthemodeltoautomaticallydistinguishdifferentinputqueries. Duringthe
deploymentphase,with<2>wecanimplementnon-awakeninginteraction. Basedonthis,wefurther
introduceaduplexschemefortheaudiointerruptinteraction. Twomodelsarerunningatthesame
time, where the generation model is responsible for handling user queries. When the generation
modelstartsworking,theothermodelmonitorstheenvironment. Iftheuserinterruptswithanother
effectiveaudioquery,themonitoringmodelaggregatesthehistoricalcontexttorespondtothelatest
query,whilethegenerationmodelispausedandtunetomonitor,i.e.,thetwomodelsswapidentities.
• Asapioneer,wemakepreliminaryexplorationsinthefieldofnaturalmultimodalhuman-
computerinteraction. Byintroducingastatetoken,themodelcanautomaticallyidentify
thetypeoftheinputaudiotoachievenon-awakeninginteraction. Meanwhile,theduplex
schememakesitpossibletorealizeaudiointerruptinteraction.
• Wefullyopen-sourceourmodel,trainingcode,andinferencedeploymentframework,aiming
atpromotingtheadvancementsoftheresearchcommunity. Asacutting-edgeresearch,we
willcontinuetocontributetothemultimodalfoundationmodelsandinteractions.
2 RelatedWork
LeveragingadvancedLLMssuchasGPTs[39,2],LLaMA[47,48],Alpaca[43],Vicuna[9],and
Mistral [21], MLLMs exhibit enhanced multimodal capabilities, particularly through end-to-end
trainingtechniques.Recentopen-sourceMLLMs,suchasOtter[27],mPLUG-Owl[51],LLaVA[31],
Qwen-VL [1], Cambrian-1 [46], Mini-Gemini [29], MiniCPM-V 2.5 [19], DeepSeek-VL [33],
SliME[56],andBunny[17],havemadeprogressinsolvingmultimodalfundamentalproblems,such
asvision-languagealignmentandinstructionfollowing.
Among them, some representative open-source models like InternLM-XComposer-2.5 [55] and
InternVL-2[8]havebeenrapidlyadvancing,demonstratingstrongmultimodalunderstandingcapabil-
itiesandcloselyrivalingproprietarymodelsinvariousmultimodalbenchmarks. However,compared
toproprietarymodelssuchasGPT-4o[40]andGemini-Pro1.5[44],whichsupportmorethantwo
modalitieslikeaudio,image,andtext,mostopen-sourcemodelsfocusonimage-textmodalities[53].
Furthermore,open-sourcemodelsrarelyconcentrateonuserinteractioncapabilities,leavingthisarea
relativelyunexplored.Incomparison,theproposedVITAnotonlyexhibitsimpressiveperformancein
perceivingdataacrossfourmodalities,i.e.,video,image,text,andaudio,butalsomakespreliminary
stridesinenhancinguserinteractioncapabilities. Throughthecomprehensiveopen-sourcingofVITA,
wehopetoacceleratedevelopmentsinthisfield.
3State Token
StageI Stage II <1> QueryAudio Stage III
InstrucL tiL onM Tuning Visual (Left)/Audio (Right) ModalityAlignment <2> Noisy Audio Multimodal Instruction Tuning
… … <3> Query Text <1> <2> <3>
VITA VITA VITA
VITA
MLP MLP MLP
Visual Audio
ModalityEncoders
Encoder Encoder
Bilingual
Pure Text Corpus Image Video Text Audio Audio Image Video Text
Figure 3: Training pipeline of VITA. The first stage LLM Instruction Tuning enhances the
languagemodelMixtral8×7Bbyexpandingitsvocabularysizeandfine-tuningitwithahigh-quality
bilingualtextcorpus,therebyachievingproficiencyinbothChineseandEnglish. Thesecondstage
MultimodalAlignmentconnectsindividualencoderswiththeLLMtoprocessvariousmodalities.
By amassing a substantial collection of high-caliber multimodal data, we synchronize the text
featurespacewiththatofvideo,image,andaudio. ThelaststageMultimodalInstructionTuning
allowsthemodeltofollowtextoraudioinstructionstounderstandtheimageorvideo. Aspecially
designedstatetokenisusedtodistinguishthetypeofinputquery,facilitatingsubsequentmultimodal
human-computerinteraction.
3 VITA
AsdepictedinFig.3,theoveralltrainingpipelineofVITAconsistsofthreestages: LLMinstruction
tuning,multimodalalignment,andmultimodalinstructiontuning. ThedevelopmentofVITAisalso
animportantpart.
3.1 LLMInstructionTuning
Mixtral8x7B1[22]isarepresentativeLLMwithanarchitectureofsparsemixtureofexperts(SMoE).
Itsperformanceisamongthetop-tieropen-sourceLLMs,makingitanidealstartingpointofourwork.
Nonetheless,weobservethattheofficialMixtralmodelexhibitslimitedproficiencyinunderstanding
Chinese. To infuse bilingual (Chinese and English) comprehension capabilities, we broaden the
vocabularyofthebasemodelwithChinese,increasingthevocabularysizefrom32,000to51,747.
Thisextensioncanalsoreducethenumberoftokensunderthesametext,thusimprovinginference
efficiency. Withtheextendedvocabularyinplace,weuse5millionsyntheticbilingualcorpusfor
pure-textinstructiontuning.
3.2 MultimodalAlignment
Inthisstage, weaimtobridgetherepresentationgapbetweentextandothermodalities, thereby
layingthegroundworkformultimodalunderstanding.
3.2.1 VisualModality
VisualEncoder.WeemployInternViT-300M-448pxasthevisualencoder2,whichacceptsa448×448
imageasinput,generating256tokensafterusingavisualconnectorthatisasimpletwo-layerMLP.
Forhigh-resolutionimageinput,weimplementthedynamicpatchingstrategy[7]tocapturelocal
details. Videos are treated as special cases of images. If the video length is less than 4 seconds,
weuniformlysample4frames. Ifthevideolengthisbetween4and16seconds, wesampleone
framepersecond. Forvideoslongerthan16seconds,weuniformlysample16frames. Toprevent
1https://huggingface.co/mistralai/Mixtral-8x7B-v0.1
2https://huggingface.co/OpenGVLab/InternViT-300M-448px
4Table1: Trainingdataofmultimodalinstructiontuning. Theimagesofthesyntheticdatacomefrom
open-sourcedatasetsWukong[15],LAION[41],andCC12M[3].
#Concatenated #Total #Audio #Text
DataScenario QAType DatasetName Language
Entries(K) Entries(K) Questions(K) Questions(K)
ShareGPT4V 50.7 99.5 49.7 49.7 Eng
Allava-Caption 362.2 708.1 354.6 353.6 Eng
Description
ShareGTP4o-Image 39.2 57.3 28.6 28.7 Eng
SyntheticData 304.6 594.5 297.0 297.5 CN
General Image LLaVA-150K 57.9 99.8 109.0 109.1 CN
LLaVA-Mixture-sample 308.5 308.5 1103.0 920.9 Eng
QA Lvis-Instruct 110.4 110.4 562.1 466.0 Eng
ScienceQA 12.7 12.7 0.0 12.7 Eng
SyntheticData 14.1 21.8 106.0 105.3 CN
Anyword-3M 47.5 770.3 384.5 385.8 CN
Description ICDAR2019-LSVT 233.2 233.2 680.0 583.1 CN
OCR & Diagram & ICDAR2017-RCTW 6.6 7.7 3.7 4.0 CN
QA Open-Chart 32.3 41.5 229.2 229.0 CN
SyntheticData 97.3 156.0 418.8 345.2 CN
Description ShareGemini 777.7 777.7 104.1 673.7 CN&Eng
General Video
QA SyntheticData 160.5 160.5 280.6 179.5 CN&Eng
Pure Text QA SyntheticData 134.5 800.8 398.5 1113.9 CN&Eng
Total 2749.9 4960.3 5109.4 5857.7 CN&Eng
theintroductionofanexcessivenumberofvisualtokens,wedonotperformdynamicpatchingon
individualframesofthevideo.
Visual Alignment. We only train the visual
connectorduringthevisualalignmentphase.Ta-
Q:What is the capital of France? A:The capital of France is Paris. Q:What
ble1summarizestheusedtrainingdata,except is the smallest unit of life? A:The smallest unit of life is the cell. Q:What is
thelargestplanetinoursolarsystem?A:Thelargestplanetinoursolar
thepuretextpart. Inaddition,inthisphasewe PureText systemisJupiter.
do not use audio questions. (1) For the gen- Q:What scenes are shown in the picture? A: It
features a striking sunset or sunrise scene with a large, glowing sun low in
eral image description task, we introduce the thesky,surroundedbyamixofclouds. Q:Is
Image&Text thereadoginthepicture?A:Yes.
GPT-4VgeneratedpartfromShareGPT4V[6]
Q:Whatscenesare
toensuredataquality. WealsointroduceAllava- shown in the video? A: This video depicts a scene taking place in the
savanna. A lioness is on her way with her two children. One of them is
Caption[4]andShareGTP4o-Image3,andsup- Video&Text walking on her own, and the other is held in the mouth of the lion.
plement these with Chinese image descrip-
Figure4: DataConcatenation. Differenttextdata
tions generated by existing MLLMs. (2) For
isdirectlyconcatenatedto6Ktokens. Imagesare
the general image Question-Answering (QA)
firstdividedintolocalpatchesandthendifferent
task,weinitiallygatherthreedatasets: LLaVA-
image-textpairsareconcatenated. Videodatais
Mixture-sample [31], Lvis-Instruct [50], and
directly sampled frame by frame as input, with-
ScienceQA[34]. WethenuseexistingMLLMs
out the need for concatenation. In this way, we
to generate an additional 21.8K Chinese QA
couldunifythelengthofdataindifferenttraining
data. Besides, we remove the caption subset
batches,thusimprovingtrainingefficiency.
from LLaVA-150K [31] and translate the rest
intoChinese. (3)FortheOCR&Diagramtasks,
weincludeAnyword-3M[49],ICDAR2019-LSVT4,ICDAR2017-RCTW5,Open-Chart(acollection
ofChartQA[36],DVQA[23],InfoVQA[37],Pew[25],andOpenCQA[24]),aswellassomesyn-
theticdatageneratedbyexistingMLLMsfromotheropen-sourcedatawithtext. ForAnyword-3M,
weselectdatawheretheanswer’scorrespondingtextlengthisbetween20and50,withthequestion
askingtoidentifythetextintheimage.
ForICDAR2019-LSVT,ICDAR2017-RCTW,andOpen-Chart,wegeneratedetaileddescriptionsand
QApairsusingexistingMLLMs. (4)Forgeneralvideodescriptions,weusetheShareGemini[42]
dataset. (5) For general video QA, we re-label open-source data from Video-ChatGPT [35] and
VideoChat2[28]usingexistingMLLMs. Wesample800kentriesfrom5millionpuretextdatato
maintaintheLLM’stextcomprehensionabilities. Itisusedduringmultimodalinstructiontuning
insteadofvisualalignment,becausetheLLMparametersofthelatterarefrozen.
DataConcatenation. Forpuretextdataandimagedata,weaimtoconcatenatethecontextlengthto
6KtokensasillustratedinFig.4(theamountofconcatenateddataisindicatedintheConcatenated
3https://sharegpt4o.github.io/
4http://icdar2019.org/
5https://iapr.org/icdar2017
5Table2: Systempromptsfor imageinput, videoinput,and puretextinput.
System prompt for image data
YouareanAIrobotandyournameisVITA.
Youareamultimodallargelanguagemodeldevelopedbytheopen-sourcecommunity.Youraimistobehelpful,honest,andharmless.
Yousupporttheabilitytocommunicatefluentlyandansweruserquestionsinmultiplelanguagesoftheuser’schoice.
Iftheusercorrectsthewrongansweryougenerated,youwillapologizeanddiscussthecorrectanswerwiththeuser.
Youmustanswerthequestionstrictlyaccordingtothecontentoftheimagegivenbytheuser,anditisstrictlyforbiddentoanswerthequestion
withoutthecontentoftheimage.Pleasenotethatyouareseeingtheimage,notthevideo.
System prompt for video data
YouareanAIrobotandyournameisVITA.
Youareamultimodallargelanguagemodeldevelopedbytheopen-sourcecommunity.Youaimtobehelpful,honest,andharmless.
Yousupporttheabilitytocommunicatefluentlyandansweruserquestionsinmultiplelanguagesoftheuser’schoice.
Iftheusercorrectsthewrongansweryougenerated,youwillapologizeanddiscussthecorrectanswerwiththeuser.
Youmustanswerthequestionstrictlyaccordingtothecontentofthevideogivenbytheuser,anditisstrictlyforbiddentoanswerthequestion
withoutthecontentofthevideo.Pleasenotethatyouareseeingthevideo,nottheimage.
System prompt for text data
YouareanAIrobotandyournameisVITA.
Youareamultimodallargelanguagemodeldevelopedbytheopen-sourcecommunity.Youraimistobehelpful,honest,andharmless.
Yousupporttheabilitytocommunicatefluentlyandansweruserquestionsinmultiplelanguagesoftheuser’schoice.
Iftheusercorrectsthewrongansweryougenerated,youwillapologizeanddiscussthecorrectanswerwiththeuser.
Entries column of Table 1). Video data, on the other hand, is not subjected to concatenation.
Concatenatingdifferentdataofferstwobenefits: (1)Itsupportslongercontextlengths, allowing
for the expansion from single to multiple image-question interactions, resulting in more flexible
input forms and extended contexts. (2) It enhances computational efficiency, as video frames
typically contain a high number of visual tokens. By concatenating image-question pairs, we
maintainabalancednumberoftokensinthetrainingbatch,thusimprovingcomputationalefficiency.
Furthermore,wefindthatthemodeltrainedwithconcatenateddataperformcomparablytothose
trainedwiththeoriginaldata.
3.2.2 AudioModality
AudioEncoder. TheinputaudioisinitiallyprocessedthroughaMelFilterBankblock. Thisblock
breaksdowntheaudiosignalintoindividualfrequencybandsonthemelfrequencyscale,mimicking
thenonlinearhumanperceptionofsound. Subsequently,weutilize4×CNNdownsamplinglayers
followedbya24layersoftransformer,totaling341Mparameters,toprocesstheinputfeatures. We
employasimpletwo-layerMLPastheaudio-textmodalityconnector. Intheend,each2secondsof
audioinputisencodedinto25tokens.
AudioAlignment. Foroneofthealignmenttasks,wehaveoptedforAutomaticSpeechRecognition
(ASR). Our dataset includes Wenetspeech [54], which encompasses over 10,000 hours of multi-
domainspeechrecognitiondata,withaprimaryfocusonChinesetasks. Similarly,Gigaspeech[5]
alsocontains10,000hoursofhigh-qualityaudiodata,withthemajorityofthedatagearedtowards
Englishspeechrecognitiontasks. Theothertaskisaudiocaptioning,whichreliesontheAudioSet
SLsubsetofWavcaps[38]. Thisdatasetfeatures400Kaudioclipsalongwiththeircorresponding
audiocaptions. Duringalignment,boththeaudioencoderandconnectoraretrained.
3.3 MultimodalInstructionTuning
Duringthisstage,weperforminstructiontuningonthemodeltoenhanceitsinstructionfollowing
capability,whethertextoraudio.
3.3.1 TrainingData
Data Construction. The data source in the instruction tuning phase are same as the alignment
phaseinTable1,andwemakethefollowingimprovements: (1)thequestionsarerandomly(about
half)replacedwiththeiraudioversions,usingTTStechniquesuchasGPT-SoVITS6,toenhancethe
model’sunderstandingofaudioqueriesanditsinstructionfollowingcapabilities. Thenumberof
audioquestionsandtextquestionscanbefoundinTable1. (2)Differentsystempromptsaresetto
avoidconflictsbetweendifferenttypesofdata,aslistedinTable2. Forinstance,somequestions
canbeansweredbasedonvisualinformationorbasedonthemodel’sownknowledge,leadingto
conflicts. Additionally,sincetheimagedatahavebeenpatchedthataresimilartomultipleframesof
6https://github.com/RVC-Boss/GPT-SoVITS
6videodata,whichmayconfusethemodel. Thesystempromptexplicitlydistinguishesdifferentdata
types,makingitmoreintuitivetounderstand.
Noisy Audio Construction. During human-computer interaction, not all audio inputs require a
response,whicharecollectivelyreferredtoasnoisyaudio.Asystemwithgoodinteractivecapabilities
shouldbeabletoactivelyidentifythetypeofaudio[11]andselectivelyexecutesubsequentoutputs.
Tothisend,weneedtoconstructvariousnoisyaudiosamplesforthemodeltorecognize. Specifically,
werandomlysample474KsentencesfromanswersofexistingmultimodalandunimodalQAdata.
These negative sample texts, focusing on non-query-related content that does not require a user
response,havealengthdistributionconsistentwiththepositivequestionlengthdistribution. Then,
weusetheTTStooltoconvertthesesentencesintoaudio. Theconstructionofnoisyaudiosamples
enablesthemodeltorecognizeaudioinputsthatdonotrequirearesponse,whichisbeneficialfor
implementingNon-awakeningInteraction. Thespecifictrainingstrategywillbeelaboratedinthe
followingsection.
3.3.2 TrainingProcess
InaccordancewiththeQApairsconstructedintheabovesection,themodelneedstodistinguish
threetypesofqueries:
-QueryAudio: Thequestionisinitiatedbyaudio.
-NoisyAudio: Theinputisaudio,butitdoesnotcontainaquestion.
-QueryText: Thequestionisinitiatedbytext.
Based on these query types, we have designed three state tokens <1>, <2>, and <3>. During the
trainingphase,weinsertcorrespondingstatetokensatthebeginningoftheanswers,allowingthe
modeltoflexiblyhandledifferentinteractivebehaviors. Specifically:
-Statetoken<1>denotesthatthequestioninputisthequeryaudio. Inthiscase,theoutputofthe
modelneedstobepresentedtotheuser,eitherastextorspeechconvertedbyTTStools.
-Statetoken<2>indicatesthatthequestioninputisthenoisyaudio. Themodelshouldoutputan
EOStokenasaterminator. However,weobservethatabruptlyterminatingtheoutputduringtraining
cansignificantlydegradeperformance. Consequently,wesendthetextcorrespondingtothenoisy
audiotoaLLManduseitsoutputtextasthetrainingtarget. Duringinference,<2>servesasanother
specialEOStoken.
-Statetoken<3>signifiesthequestionofpuretext,whichisusedtodistinguishbetweentheabove
twoqueriesinthetrainingset.
Duringtraining,bothvisualandaudioencodersarefrozen,andtheconnectorsaretrainedinconjunc-
tionwithMixtral8×7B.
3.4 DevelopmentwithDuplexPipeline
In this section, we primarily discuss how we implement two interaction functionalities, namely
non-awakeninginteractionandaudiointerruptinteraction.
3.4.1 Non-awakeningInteraction
Non-awakeninginteractionimpliesthatthemodelcanbeactivatedandrespondtouseraudioquestions
intheenvironmentwithouttheneedforawake-upwordorbutton. Thedeploymentprocessmust
meetthefollowingrequirements:
-Real-timeTrackingofEnvironmentalSounds. Thisinvolvesdeterminingwhethertheaudiocontent
constituteshumanspeech.
-Filteringoutnoisyaudio. Themodelshouldonlyrespondtoeffectivehumanqueryaudio.
Forthefirstrequirement,existingVoiceActivityDetection(VAD)canprovideassistance. Itisalso
knownasspeechactivitydetectionorspeechdetection,identifyingthepresenceofhumanspeech.
VITAemploysSileroVAD[45],whichistrainedonhugecorporathatincludeover6,000languages
andperformswellwithvariousbackgroundnoise. Forthesecondrequirement,weleveragethestate
7Table3: ComparisonofofficialMixtral8x7BInstructandourtrainedMixtral8x7B.“CN”/“ENG”
denotethatthebenchmarkcontainsChinese/Englishdata.
C-EVAL AGIEVAL MMLU GSM8K
Method
CN CN&ENG ENG ENG
Mixtral-8x7BInstruct 53.30 41.72 70.35 63.99
Mixtral-8x7BOurs 56.68 46.17 70.98 75.66
Table4: EvaluationonASRtasks. “CN”/“ENG”referstoChinese/Englishspeech. Themetricof
wenetspeech/librispeechisCER(CharacterErrorRate)/WER(WordErrorRate).
Wenetspeech(CN) Librispeech(ENG)
Method
Test_Net Test_Meeting Dev_clean Dev_other est_clean est_other
VITA 12.15 16.53 7.57 16.57 8.14 18.41
token<2>describedinSec.3.3.2. Thisallowsthemodeltoautomaticallydistinguishwhetherthe
inputaudioisaneffectivequery. Iftheinputisofanon-querytype,themodeldirectlyterminatesthe
inference,therebyonlyrespondingtoquery-typeinputs.
3.4.2 AudioInterruptInteraction
Audiointerruptinteractionenablesuserstointerruptthemodel’sgenerationatanytimewithnew
questions. Toaccomplishthis,thedeploymentenvironmentmustfulfillthefollowingrequirements:
-Real-timeTrackingandFilteringofExternalQueries. Whilegeneratingresponses,thesystemmust
simultaneouslytrackandfilterexternalqueriesinrealtime.
- Answering New Questions. When a new question emerges, the system must cease its current
generation,consolidatethehistoricalcontext,andrespondtothepresentquery.
Toachievethis,weproposetheduplexdeploymentframework. AsillustratedinFig.1,twoVITA
modelsaredeployedconcurrently. Underatypicalcondition,theGenerationmodelanswersuser
queries. Simultaneously,theMonitoringmodeldetectsenvironmentalsoundsduringthegeneration
process. Itdisregardsnon-queryusersounds,i.e.,noisyaudio,butceasestheGenerationmodel’s
progress when it identifies query audio. The Monitoring model subsequently consolidates the
historicalcontextandrespondstothelatestuserquery. Atthispoint,theidentitiesoftheGeneration
modelandtheMonitoringmodelaretransformed.
4 Evaluation
LanguagePerformance. Tovalidatetheefficacyofourtrainingprocessforlanguagemodel,we
evaluateourtrainedmodel“Mixtral8x7BOurs”againsttheofficialversion“Mixtral8x7BInstruct”,
onfourdatasets: C-EVAL[20],AGIEVAL[58],MMLU[18],andGSM8K[10]. Thesedatasets
encompassavarietyofscenariosincludinggeneralmultiple-choicequestions,multidisciplinaryQA,
aswellasmathematicalandlogicalreasoningtasks,coveringbothChineseandEnglishcontexts.
TheresultspresentedinTable3demonstratethatourtrainingsignificantlyenhancesthelanguage
model’scapabilitiesonChineseevaluationsets(C-EVALandAGIEVAL),whilemaintainingoriginal
performancelevelsontheEnglishrelatedbenchmark(MMLU)andshowingnotableimprovementin
themathematicalreasoningtask(GSM8K).
AudioPerformance. Tovalidatetherobustnessofthespeechrepresentationslearnedbyourmodel,
wetestitontheWenetspeech7andLibrispeech8datasets. TheWenetspeechfeaturestwoevaluation
splits: test_netandtest_meeting. Theformerhasdatasourcesthataremorecloselyalignedwiththe
trainingdata,makingiteasier,whilethelatterpresentsagreaterchallenge. Asaheld-outdatasetfor
ourmodel,Librispeechassessesthemodel’sgeneralizationabilityonunseendatasets. Ithasfour
evaluationsplits: thosestartingwith“dev”arevalidationsets,andthosestartingwith“test”aretest
7https://github.com/wenet-e2e/WenetSpeech
8https://www.openslr.org/12
8100
Model
GPT-4o LLaVA-Next (Yi-34B) Video-CCAM (Phi-3-14B)
81.3
80 Gemini 1.5 Pro LLaVA-Next-Video (Qwen1.5-32B) VITA (Mixtral 8x7B) 77.2
2310.3 736.0 754.0 63.0
60 2110.6 2097.0 603.0 55.0 57.4 57.6 2006.5 574.0
45.6
40 39.7
34.8
20
NaN NaN NaN NaN NaN NaN NaN
0
MME OCRBench HallusionBench Video-MME
Benchmarks
Figure5: Evaluationonimageandvideounderstanding. VITAexhibitscomparableperformanceto
thecurrentleadingopen-sourcemodels,butstillfellshortofadvancedclosed-sourcecounterparts.
sets. “Clean”referstolesschallengingsets,while“other”indicatesmorechallengingones. Wecan
seethatVITAhasachievedconsiderableresultsontheASRbenchmarks.
MultimodalPerformance. Toassessmultimodalcapabilities,weevaluateVITAonfourrepresenta-
tivebenchmarks,includingMME[13],OCRBench[32],HallusionBench[16],andVideo-MME[14].
AsdepictedinFig.5,intermsofimageunderstanding,VITAoutperformsimagespecializedopen-
sourcemodelLLaVA-Next[30]andisclosetoclosed-sourcemodelGemini1.5Pro[44]. Invideo
understanding,VITAsurpassesvideospecializedopen-sourcemodelVideo-CCAM9.Althoughthere
isagapbetweenVITAandthevideo-specializedLLaVA-Next-Video[57],thisisacceptablegiven
thatVITAsupportsabroaderrangeofmodalitiesandprioritizesinteraction. However,itisworth
notingthatasubstantialgapstillexistsbetweencurrentopen-sourcemodelsandproprietarymodels
intermsofvideounderstandingcapabilities.
5 ConclusionandFutureWork
Inthispaper,wehaveintroducedVITA,astrongopen-sourceMLLMthatintegratesvideo,image,
text,andaudiounderstandingintoaunifiedframework,withadvancedinteractiveexperience. Apart
fromrobustmultimodalfundationalcapabilities,VITApioneersnovelmultimodalinteractionsforthe
open-sourcecommunity,throughnon-awakeninginteractionandaudiointerruptinteraction.However,
thecurrentversionstillhasthefollowinglimitations:
-EnhancementofFoundationalCapabilities. WhileVITAdemonstratescompetitiveperformancein
unimodalandmultimodaltasksrelativetoleadingopen-sourcemodels,thereremainsanotablegap
comparedtoproprietarycounterparts.
-RefinementofNoisyAudioConstruction. Usingnon-queryresponsesofexistingdataasnoisyaudio
samplesissimpleyeteffective. However,thereareinstanceswhereVITAmisclassifiesnoisyaudio
asqueryaudio,highlightingtheneedforamorenuancedconstructionapproach.
-Buildingend-to-endTTSinconjunctionwithLLM.WecurrentlyuseanadditionalTTStoolto
convertLLMgeneratedtextintospeech,whichisquitetime-consuming. IfTTScanbecombined
withLLMtoachieveend-to-endspeechoutput,itmaygreatlyboostthereal-timeinteraction.
References
[1] JinzeBai,ShuaiBai,ShushengYang,ShijieWang,SinanTan,PengWang,JunyangLin,Chang
Zhou, and Jingren Zhou. Qwen-vl: A frontier large vision-language model with versatile
abilities. arXivpreprintarXiv:2308.12966,2023.
[2] TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,
ArvindNeelakantan,PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsare
few-shotlearners. NeurIPS,2020.
9https://github.com/QQ-MM/Video-CCAM
9
erocS
dezilamroN[3] SoravitChangpinyo,PiyushSharma,NanDing,andRaduSoricut. Conceptual12m: Pushing
web-scaleimage-textpre-trainingtorecognizelong-tailvisualconcepts. InCVPR,2021.
[4] GuimingHardyChen,ShunianChen,RuifeiZhang,JunyingChen,XiangboWu,ZhiyiZhang,
Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-
synthesizeddataforalitevision-languagemodel,2024.
[5] GuoguoChen,ShuzhouChai,GuanboWang,JiayuDu,Wei-QiangZhang,ChaoWeng,Dan
Su,DanielPovey,JanTrmal,JunboZhang,etal. Gigaspeech: Anevolving,multi-domainasr
corpuswith10,000hoursoftranscribedaudio. arXivpreprintarXiv:2106.06909,2021.
[6] LinChen,JisongLi,XiaoyiDong,PanZhang,ConghuiHe,JiaqiWang,FengZhao,andDahua
Lin. Sharegpt4v: Improvinglargemulti-modalmodelswithbettercaptions. arXivpreprint
arXiv:2311.12793,2023.
[7] ZheChen,WeiyunWang,HaoTian,ShenglongYe,ZhangweiGao,ErfeiCui,WenwenTong,
Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to
commercialmultimodalmodelswithopen-sourcesuites. arXivpreprintarXiv:2404.16821,
2024.
[8] ZheChen,JiannanWu,WenhaiWang,WeijieSu,GuoChen,SenXing,MuyanZhong,Qinglong
Zhang,XizhouZhu,LeweiLu,BinLi,PingLuo,TongLu,YuQiao,andJifengDai. Internvl:
Scalingupvisionfoundationmodelsandaligningforgenericvisual-linguistictasks. arXiv
preprintarXiv:2312.14238,2023.
[9] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,LianminZheng,
SiyuanZhuang,YonghaoZhuang,JosephEGonzalez,etal. Vicuna: Anopen-sourcechatbot
impressinggpt-4with90%*chatgptquality. Seehttps://vicuna.lmsys.org(accessed14April
2023),2023.
[10] KarlCobbe,VineetKosaraju,MohammadBavarian,MarkChen,HeewooJun,LukaszKaiser,
MatthiasPlappert,JerryTworek,JacobHilton,ReiichiroNakano,etal. Trainingverifiersto
solvemathwordproblems. arXivpreprintarXiv:2110.14168,2021.
[11] PranayDighe,YiSu,ShangshangZheng,YunshuLiu,VineetGarg,XiaochuanNiu,andAhmed
Tewfik. Leveraginglargelanguagemodelsforexploitingasruncertainty. InICASSP.IEEE,
2024.
[12] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle,
AieshaLetman,AkhilMathur,AlanSchelten,AmyYang,AngelaFan,etal. Thellama3herd
ofmodels. arXivpreprintarXiv:2407.21783,2024.
[13] ChaoyouFu,PeixianChen,YunhangShen,YuleiQin,MengdanZhang,XuLin,JinruiYang,
Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: A comprehensive
evaluationbenchmarkformultimodallargelanguagemodels. arXivpreprintarXiv:2306.13394,
2023.
[14] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang,
ChenyuZhou,YunhangShen,MengdanZhang,etal. Video-mme:Thefirst-evercomprehensive
evaluationbenchmarkofmulti-modalllmsinvideoanalysis. arXivpreprintarXiv:2405.21075,
2024.
[15] Jiaxi Gu, Xiaojun Meng, Guansong Lu, Lu Hou, Niu Minzhe, Xiaodan Liang, Lewei Yao,
Runhui Huang, Wei Zhang, Xin Jiang, et al. Wukong: A 100 million large-scale chinese
cross-modalpre-trainingbenchmark. NeurIPS,2022.
[16] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang,
LichangChen,FurongHuang,YaserYacoob,etal. Hallusionbench: anadvanceddiagnostic
suiteforentangledlanguagehallucinationandvisualillusioninlargevision-languagemodels.
InCVPR,2024.
[17] MuyangHe,YexinLiu,BoyaWu,JianhaoYuan,YuezeWang,TiejunHuang,andBoZhao.
Efficientmultimodallearningfromdata-centricperspective. arXivpreprintarXiv:2402.11530,
2024.
10[18] DanHendrycks,CollinBurns,StevenBasart,AndyZou,MantasMazeika,DawnSong,and
Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint
arXiv:2009.03300,2020.
[19] ShengdingHu,YugeTu,XuHan,ChaoqunHe,GanquCui,XiangLong,ZhiZheng,Yewei
Fang,YuxiangHuang,WeilinZhao,etal. Minicpm: Unveilingthepotentialofsmalllanguage
modelswithscalabletrainingstrategies. arXivpreprintarXiv:2404.06395,2024.
[20] YuzhenHuang,YuzhuoBai,ZhihaoZhu,JunleiZhang,JinghanZhang,TangjunSu,Junteng
Liu,ChuanchengLv,YikaiZhang,YaoFu,etal. C-eval: Amulti-levelmulti-disciplinechinese
evaluationsuiteforfoundationmodels. NeurIPS,2024.
[21] Albert Q Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh
Chaplot,DiegodelasCasas,FlorianBressand,GiannaLengyel,GuillaumeLample,Lucile
Saulnier,etal. Mistral7b. arXivpreprintarXiv:2310.06825,2023.
[22] AlbertQJiang,AlexandreSablayrolles,AntoineRoux,ArthurMensch,BlancheSavary,Chris
Bamford,DevendraSinghChaplot,DiegodelasCasas,EmmaBouHanna,FlorianBressand,
etal. Mixtralofexperts. arXivpreprintarXiv:2401.04088,2024.
[23] KushalKafle,BrianPrice,ScottCohen,andChristopherKanan. Dvqa: Understandingdata
visualizationsviaquestionanswering. InCVPR,2018.
[24] ShankarKantharaj, XuanLongDo, RixieTiffanyKoLeong, JiaQingTan, EnamulHoque,
and Shafiq Joty. Opencqa: Open-ended question answering with charts. arXiv preprint
arXiv:2210.06628,2022.
[25] ShankarKantharaj,RixieTiffanyLeong,XiangLin,AhmedMasry,MeghThakkar,Enamul
Hoque,andShafiqJoty. Chart-to-text: Alarge-scalebenchmarkforchartsummarization. In
SmarandaMuresan,PreslavNakov,andAlineVillavicencio,editors,ACL,2022.
[26] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu,
Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large
languagemodelservingwithpagedattention. InACMSIGOPS,2023.
[27] BoLi,YuanhanZhang,LiangyuChen,JinghaoWang,JingkangYang,andZiweiLiu. Otter: A
multi-modalmodelwithin-contextinstructiontuning. arXivpreprintarXiv:2305.03726,2023.
[28] KunchangLi,YinanHe,YiWang,YizhuoLi,WenhaiWang,PingLuo,YaliWang,LiminWang,
andYuQiao. Videochat: Chat-centricvideounderstanding. arXivpreprintarXiv:2305.06355,
2023.
[29] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu,
Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality vision
languagemodels. arXivpreprintarXiv:2403.18814,2024.
[30] HaotianLiu,ChunyuanLi,YuhengLi,BoLi,YuanhanZhang,ShengShen,andYongJaeLee.
Llava-next: Improvedreasoning,ocr,andworldknowledge,January2024.
[31] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. Visualinstructiontuning. arXiv
preprintarXiv:2304.08485,2023.
[32] YuliangLiu,ZhangLi,BiaoYang,ChunyuanLi,XuchengYin,Cheng-linLiu,LianwenJin,
and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint
arXiv:2305.07895,2023.
[33] HaoyuLu,WenLiu,BoZhang,BingxuanWang,KaiDong,BoLiu,JingxiangSun,Tongzheng
Ren, Zhuoshu Li, Yaofeng Sun, et al. Deepseek-vl: towards real-world vision-language
understanding. arXivpreprintarXiv:2403.05525,2024.
[34] PanLu,SwaroopMishra,TanglinXia,LiangQiu,Kai-WeiChang,Song-ChunZhu,Oyvind
Tafjord,PeterClark,andAshwinKalyan. Learntoexplain: Multimodalreasoningviathought
chainsforsciencequestionanswering. NeurIPS,2022.
11[35] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFahadShahbazKhan. Video-chatgpt:
Towardsdetailedvideounderstandingvialargevisionandlanguagemodels. InACL,2024.
[36] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: A
benchmarkforquestionansweringaboutchartswithvisualandlogicalreasoning.arXivpreprint
arXiv:2203.10244,2022.
[37] MineshMathew,VirajBagal,RubènTito,DimosthenisKaratzas,ErnestValveny,andCVJawa-
har. Infographicvqa. InWACV,2022.
[38] XinhaoMei,ChutongMeng, HaoheLiu, QiuqiangKong, TomKo, ChengqiZhao, MarkD
Plumbley,YuexianZou,andWenwuWang. Wavcaps: Achatgpt-assistedweakly-labelledaudio
captioningdatasetforaudio-languagemultimodalresearch. TASLP,2024.
[39] OpenAI. Gpt-4technicalreport. 2023.
[40] OpenAI. Hellogpt-4o. 2023.
[41] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman,
MehdiCherti,TheoCoombes,AarushKatta,ClaytonMullis,MitchellWortsman,etal. Laion-
5b: An open large-scale dataset for training next generation image-text models. NeurIPS,
2022.
[42] Share. Sharegemini: Scalingupvideocaptiondataformultimodallargelanguagemodels,June
2024. https://github.com/Share14/ShareGemini.
[43] RohanTaori,IshaanGulrajani,TianyiZhang,YannDubois,XuechenLi,CarlosGuestrin,Percy
Liang,andTatsunoriBHashimoto. Stanfordalpaca: Aninstruction-followingllamamodel,
2023.
[44] GeminiTeam,RohanAnil,SebastianBorgeaud,YonghuiWu,Jean-BaptisteAlayrac,JiahuiYu,
RaduSoricut,JohanSchalkwyk,AndrewMDai,AnjaHauth,etal. Gemini: afamilyofhighly
capablemultimodalmodels. arXivpreprintarXiv:2312.11805,2023.
[45] Silero Team. Silero vad: pre-trained enterprise-grade voice activity detector (vad), number
detectorandlanguageclassifier. https://github.com/snakers4/silero-vad,2021.
[46] ShengbangTong,EllisBrown,PenghaoWu,SanghyunWoo,ManojMiddepogu,SaiCharitha
Akula,JihanYang,ShushengYang,AdithyaIyer,XichenPan,etal. Cambrian-1: Afullyopen,
vision-centricexplorationofmultimodalllms. arXivpreprintarXiv:2406.16860,2024.
[47] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timo-
théeLacroix,BaptisteRozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Open
andefficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
[48] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,YasmineBabaei,
NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhosale,etal. Llama2: Open
foundationandfine-tunedchatmodels. arXivpreprintarXiv:2307.09288,2023.
[49] Yuxiang Tuo, Wangmeng Xiang, Jun-Yan He, Yifeng Geng, and Xuansong Xie. Anytext:
Multilingualvisualtextgenerationandediting. arXivpreprintarXiv:2311.03054,2023.
[50] JunkeWang,LingchenMeng,ZejiaWeng,BoHe,ZuxuanWu,andYu-GangJiang. Toseeisto
believe: Promptinggpt-4vforbettervisualinstructiontuning. arXivpreprintarXiv:2311.07574,
2023.
[51] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yiyang Zhou, Junyang Wang,
Anwen Hu, Pengcheng Shi, Yaya Shi, et al. mplug-owl: Modularization empowers large
languagemodelswithmultimodality. arXivpreprintarXiv:2304.14178,2023.
[52] ShukangYin,ChaoyouFu,SiruiZhao,KeLi,XingSun,TongXu,andEnhongChen. Asurvey
onmultimodallargelanguagemodels. arXivpreprintarXiv:2306.13549,2023.
12[53] JunZhan,JunqiDai,JiashengYe,YunhuaZhou,DongZhang,ZhigengLiu,XinZhang,Ruibin
Yuan,GeZhang,LinyangLi,etal. Anygpt: Unifiedmultimodalllmwithdiscretesequence
modeling. arXivpreprintarXiv:2402.12226,2024.
[54] BinbinZhang,HangLv,PengchengGuo,QijieShao,ChaoYang,LeiXie,XinXu,HuiBu,
XiaoyuChen,ChenchenZeng,etal. Wenetspeech: A10000+hoursmulti-domainmandarin
corpusforspeechrecognition. InICASSP,2022.
[55] PanZhang, XiaoyiDongBinWang, YuhangCao, ChaoXu, LinkeOuyang, ZhiyuanZhao,
ShuangruiDing,SongyangZhang,HaodongDuan,HangYan,etal. Internlm-xcomposer: A
vision-languagelargemodelforadvancedtext-imagecomprehensionandcomposition. arXiv
preprintarXiv:2309.15112,2023.
[56] Yi-FanZhang,QingsongWen,ChaoyouFu,XueWang,ZhangZhang,LiangWang,andRong
Jin. Beyondllava-hd: Divingintohigh-resolutionlargemultimodalmodels. arXivpreprint
arXiv:2406.08487,2024.
[57] YuanhanZhang,BoLi,haotianLiu,YongjaeLee,LiangkeGui,DiFu,JiashiFeng,ZiweiLiu,
andChunyuanLi. Llava-next: Astrongzero-shotvideounderstandingmodel,April2024.
[58] WanjunZhong,RuixiangCui,YiduoGuo,YaoboLiang,ShuaiLu,YanlinWang,AminSaied,
WeizhuChen,andNanDuan. Agieval: Ahuman-centricbenchmarkforevaluatingfoundation
models. arXivpreprintarXiv:2304.06364,2023.
13