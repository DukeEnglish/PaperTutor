Active Learning for Level Set Estimation Using Randomized Straddle
Algorithms
Yu Inatsu1,∗ Shion Takeno2,3 Kentaro Kutsukake4,5 Ichiro Takeuchi2,3
1 Department of Computer Science, Nagoya Institute of Technology
2 Department of Mechanical Systems Engineering, Graduate School of Engineering, Nagoya
University
3 RIKEN Center for Advanced Intelligence Project
4 Institute of Materials and Systems for Sustainability, Nagoya University
5 Department of Materials Process Engineering, Graduate School of Engineering, Nagoya University
∗ E-mail: inatsu.yu@nitech.ac.jp
ABSTRACT
Level set estimation (LSE), the problem of identifying the set of input points where a function
takes value above (or below) a given threshold, is important in practical applications. When the
function is expensive-to-evaluate and black-box, the straddle algorithm, which is a representative
heuristic for LSE based on Gaussian process models, and its extensions having theoretical guar-
antees have been developed. However, many of existing methods include a confidence parameter
β1/2 that must be specified by the user, and methods that choose β1/2 heuristically do not provide
t t
theoretical guarantees. In contrast, theoretically guaranteed values of β1/2 need to be increased
t
depending on the number of iterations and candidate points, and are conservative and not good
forpracticalperformance. Inthisstudy, weproposeanovelmethod, therandomized straddle algo-
rithm, in which β in the straddle algorithm is replaced by a random sample from the chi-squared
t
distribution with two degrees of freedom. The confidence parameter in the proposed method has
the advantages of not needing adjustment, not depending on the number of iterations and can-
didate points, and not being conservative. Furthermore, we show that the proposed method has
theoreticalguaranteesthatdependonthesamplecomplexityandthenumberofiterations. Finally,
we confirm the usefulness of the proposed method through numerical experiments using synthetic
and real data.
1. Introduction
In various practical applications, including engineering, level set estimation (LSE), the estimation
of the region where the value of a function is above (or below) a given threshold θ, is important.
A specific example of LSE is the estimation of defective regions of materials in quality control. For
example, for silicon ingots, a material used in solar cells, the carrier lifetime value, which is a measure
of the quality of the ingot, is observed at each point on the ingot surface before shipping, and the
regions that can be used as solar cells and those that cannot are identified. Since many of functions
handled in practical applications, such as the carrier lifetime in the silicon ingot example, are black-
box functions that have high evaluation costs, it is desirable to identify the desired region without
performing an exhaustive search of black-box functions.
Bayesian optimization (BO) (Shahriari et al., 2015) is a powerful tool for optimizing black-box
functionsthathavehighevaluationcosts. BOpredictsblack-boxfunctionsusingsurrogatemodelsand
adaptively observes the black-box function values based on an evaluation function called acquisition
functions (AFs). Many studies have been conducted on BO, with particular focus on developing new
AFs. Among them, BO based on the AF called Gaussian process upper confidence bound (GP-UCB)
(Srinivas et al., 2010) has a theoretical guarantee for the optimal solution, and is a powerful method
that is flexible and can be extended to various problem settings. GP-UCB-based methods in various
settings have been proposed, such as the LSE algorithm (Gotovos et al., 2013), multi-fidelity BO
(Kandasamy et al., 2016, 2017), multi-objective BO (Zuluaga et al., 2016; Inatsu et al., 2024), high-
dimensional BO (Kandasamy et al., 2015; Rolland et al., 2018), parallel BO (Contal et al., 2013),
cascade BO (Kusakawa et al., 2022), and robust BO (Kirschner et al., 2020). These GP-UCB-based
methods, like the original GP-UCB-based BO, provide some theoretical guarantee for optimality in
each problem setting.
1
4202
guA
6
]LM.tats[
1v44130.8042:viXraHowever, GP-UCB and its related methods have a problem in that the user must specify a con-
1/2
fidence parameter β to adjust the trade-off between exploration and exploitation, where t is the
t
number of iterations in BO. As a theoretical value for GP-UCB, Srinivas et al. (2010) proposes that
1/2
β should increase with the iteration t, but this value is conservative, and it has been pointed out
t
by Takeno et al. (2023) that the practical performance is not good as a result. On the other hand,
Takeno et al. (2023) recently proposed the IRGP-UCB, an AF that randomizes the β of GP-UCB
t
by replacing it with a random sample from a two-parameter exponential distribution. IRGP-UCB
does not require parameter tuning, and its realized values from the exponential distribution are not as
conservative as the theoretical values of GP-UCB, resulting in good practical performance. Further-
more, it has been shown that IRGP-UCB gives a tighter bound for the Bayesian regret bound, which
is one of the optimality measures in BO, than existing methods. On the other hand, it is not clear
whether IRGP-UCB can be extended to various methods including LSE. In this study, we propose a
new method for LSE based on the randomization used in IRGP-UCB.
1.1. Related Work
GPs(RasmussenandWilliams,2005)areoftenusedassurrogatemodelsinBO,andmethodsusing
GP for LSE have also been proposed. A representative heuristic using GP is the straddle heuristic
by Bryan et al. (2005). The straddle is a method that considers the trade-off between the absolute
value of the difference between the predicted mean and the threshold value by the GP model and the
uncertainty of the prediction, but no theoretical analysis has been performed. An extension of the
straddle to the case where the black-box function is a composite function has also been proposed by
Bryan and Schneider (2008), but it is a heuristic method that has not been theoretically analyzed.
As a GP-UCB-based method using GP, Gotovos et al. (2013) proposed the LSE algorithm. The
1/2
LSE algorithm uses the same confidence parameter β as GP-UCB, and is based on the degree of
t
violation from the threshold of the confidence interval determined by the GP prediction model. It has
beenshownthattheLSEalgorithmreturnsanϵ-accuratesolutionforthetruesetwithhighprobability.
Bogunovic et al. (2016) proposed the truncated variance reduction (TRUVAR), a method that can
handle both BO and LSE. TRUVAR also takes into account the situation where the observation cost
differs for each observation point, and is a method that maximizes the reduction in uncertainty on
the uncertain set in one observation point per unit cost. In addition, Shekhar and Javidi (2019)
proposed a chaining-based method, which handles the case where the input space is continuous. As
an expected improvement-based method, Zanette et al. (2019) proposed the maximum improvement
for level-set estimation (MILE) method. MILE is an algorithm that observes from the input point
that has the largest expected number of points estimated as the super-level set in one-step ahead by
data observation.
LSE methods under different settings for black-box functions are also proposed. For example,
Letham et al. (2022) proposed a method for the case where the observation of the black-box function
is binary. In the robust BO framework, where inputs of black-box functions have uncertainty, LSE
methods for several robust measures have been proposed. Iwazaki et al. (2020) proposed LSE for
probability threshold robustness measures, and Inatsu et al. (2021) proposed LSE for distributionally
robust probability threshold robustness measures, both of which are AFs based on MILE. In addition,
Hozumi et al. (2023) proposed the straddle-based method in the framework of transfer learning, where
a large amount of data for similar functions is available in addition to the black-box function that
is primarily to be classified, and Inatsu et al. (2020) proposed the MILE-based method in the LSE
problem for black-box functions under settings where the uncertainty of the input changes depending
on the cost. Mason et al.(2022) dealt with the LSE problemin the setting that the black-box function
is an element of a reproducing kernel Hilbert space.
The straddle, LSE algorithm, TRUVAR, chaining-based algorithm, and MILE, which have been
proposed under the same settings considered in this study, have the following problems. The straddle
1/2
is not an AF proposed based on GP-UCB, but it includes the confidence parameter β , which is
t
essentially the same as GP-UCB. However, the value of this parameter is determined heuristically,
and as a result, it is a method without theoretical guarantee. The LSE algorithm and TRUVAR have
been theoretically analyzed, but the theoretical value of the confidence parameter must be increased
according to the iteration t, as in GP-UCB, making them conservative. The chaining-based algorithm
2Histogram of b t12 Iteration vs b t12 Number of candidate points vs b t12
Proposed Proposed
LSE LSE
0 1 2 3 4 5 00 220000 440000 660000 880000 11000000 11ee++0000 11ee++0022 11ee++0044 11ee++0066 11ee++0088 11ee++1100
b t12 Iteration Number of candidate points
1/2
Figure 1: Comparison of the confidence parameter β in the randomized straddle and LSE al-
t
1/2
gorithms. The figure on the left shows the histogram of β when β is sampled 1,000,000 times
t t
from χ2, where χ2 is the chi-squared distribution with two degrees of freedom. The red line in the
2 2 √
center and right figures shows
E[(cid:112)
χ2] = 2π/2 ≈ 1.25, the shaded area shows the 95% confidence
2
interval of (cid:112) χ2, and the black line shows the theoretical value of β1/2 in the LSE algorithm given by
2 t
β1/2 = (cid:112) 2log(|X|π2t2/(6δ)), where δ = 0.05. The figure in the center shows the behavior of β1/2 with
t t
an increase in the number of iterations t when the number of candidate points |X| is fixed at 1000,
1/2
and the figure on the right shows the behavior of β with an increase in the number of candidate
t
points |X| when the number of iterations t is fixed at 100.
can handle continuous spaces by discretization, but it has many adjustment parameters, and the
theoretical recommended values depend on the model parameters, including kernel parameters of the
surrogate model, and are unknown except for some special settings. MILE is a method for cases
where the number of candidate points is finite, and does not support continuous settings like the
chaining-based algorithm.
1.2. Contribution
In this study, we propose a novel straddle AF called the randomized straddle, which introduces the
confidence parameter randomization technique used in IRGP-UCB and solves the problems described
in Section 1.1. Figure 1 shows a comparison of the confidence parameters in the proposed AF and
those in the LSE algorithm. The contributions of this study are as follows:
• Inthisstudy, weproposetherandomizedstraddleAF,whichreplacesβ inthestraddleheuristic
t
with a random sample from the chi-squared distribution with two degrees of freedom. We
emphasize that the confidence parameter in the randomized straddle does not need to increase
1/2
withtheiterationt,unliketheLSEalgorithm. Inaddition,β intheLSEalgorithmdependson
t
1/2 1/2
thenumberofcandidatepoints|X|,andβ alsoincreaseswiththeincreaseof|X|,whileβ in
t t
the randomized straddle does not depend on |X|, and can be applied even when X is an infinite
1/2
set. Furthermore, the expected value of the realized value of β in the randomized straddle is
√ t
2π/2 ≈ 1.25, which is less conservative than the theoretical value in the LSE algorithm.
• We show that the randomized straddle guarantees that the expected loss for misclassification
in LSE converges to 0. Specifically, for the misclassification loss r = 1 (cid:80) l (x), the ran-
t |X| x∈X t
domized straddle guarantees E[r ] = O((cid:112) γ /t), where l (x) is 0 if the input point x is correctly
t t t
classified, and |f(x)−θ| if it is misclassified, and γ is the maximum information gain which is
t
a commonly used sample complexity measure.
• In addition, through numerical experiments using synthetic and real data, we confirm that the
proposed method has performance equal to or better than existing methods.
3
ycneuqerF
000001
00006
00002
0
retemarap
ecnedifnoC
0011
88
66
44
22
00
retemarap
ecnedifnoC
0011
88
66
44
22
002. Preliminary
Let f : X → R be an expensive-to-evaluate black-box function, where X ⊂ Rd is a finite set, or
an infinite compact set with positive Lebesgue measure Vol(X). Also let θ ∈ R be a known threshold
given by the user. The purpose of this study is to efficiently identify subsets H∗ and L∗ of X defined
as
H∗ = {x ∈ X | f(x) ≥ θ}, L∗ = {x ∈ X | f(x) < θ}.
For each iteration t ≥ 1, we can query x ∈ X, and f(x ) is observed with noise as y = f(x )+ε ,
t t t t t
where ε follows the normal distribution with mean 0 and variance σ2 . In this study, we assume
t noise
that f is a sample path from a GP GP(0,k), where GP(0,k) is the zero mean GP with a kernel
function k(·,·). Moreover, we assume that k(·,·) is a positive-definite kernel satisfying k(x,x) ≤ 1 for
all x ∈ X, and f,ε ,...,ε are mutually independent.
1 t
Gaussian Process Model We use a GP surrogate model GP(0,k) for the black-box function.
Given a dataset D = {(x ,y }t , where t ≥ 1 is the number of iterations, the posterior distribution
t j j j=1
of f is again a GP. Then, its posterior mean µ (x) and posterior variance σ2(x) can be calculated as
t t
follows:
µ (x) = k (x)⊤(K +σ2 I )−1y ,
t t t noise t t
(2.1)
σ2(x) = k(x,x)−k (x)⊤(K +σ2 I )−1k (x),
t t t noise t t
where k (x) is the t-dimensional vector whose i-th element is k(x,x ), y = (y ,...,y )⊤, K is the
t i t 1 t t
t×t matrix whose (j,k)-th element is k(x ,x ), I is the t×t identity matrix, with a superscript
j k t
⊤ indicating the transpose of vectors or matrices. In addition, we define D = ∅, µ (x) = 0 and
0 0
σ2(x) = k(x,x).
0
3. Proposed Method
In this section, we propose a method for estimating H∗ and L∗ based on the GP posterior and an
AF for determining the next point to be evaluated.
3.1. Level Set Estimation
First, we propose the method to estimate H∗ and L∗. While the existing study (Gotovos et al.,
2013) proposes an estimation method using the lower and upper bounds of a credible interval of f(x),
we propose the following estimation method using the posterior mean instead of using the credible
interval.
Definition 3.1 (Level Set Estimation). For each t ≥ 1, we estimate H∗ and L∗ as follows:
H = {x ∈ X | µ (x) ≥ θ}, L = {x ∈ X | µ (x) < θ}. (3.1)
t t−1 t t−1
Note that by definition 3.1, any x ∈ X belongs to either H or L , and H ∪L = X. Therefore, the
t t t t
unknown set, which is the set of points that do not belong to either set, as in existing study (Gotovos
et al., 2013), is not defined in this study.
3.2. Acquisition Function
In this section, we propose an AF for determining the next point to be evaluated. For each t ≥ 1
and x ∈ X, we define the upper bound ucb (x) and lower bound lcb (x) in the credible interval
t−1 t−1
of f(x) as
1/2 1/2
ucb (x) = µ (x)+β σ (x), lcb (x) = µ (x)−β σ (x),
t−1 t−1 t t−1 t−1 t−1 t t−1
1/2
where β ≥ 0 is a user-specified confidence parameter. Here, the straddle heuristic STR (x)
t t−1
proposed by Bryan et al. (2005) is defined as follows:
1/2
STR (x) = β σ (x)−|µ (x)−θ|.
t−1 t t−1 t−1
4Algorithm 1 Active Learning for Level Set Estimation Using Randomized Straddle Algorithms
Input: GP prior GP(0,k), threshold θ ∈ R
for t = 1,2,...,T do
Compute µ (x) and σ2 (x) for each x ∈ X by (2.1)
t−1 t−1
Estimate H and L by (3.1)
t t
Generate β from the chi-squared distribution with two degrees of freedom
t
Compute ucb (x), lcb (x) and a (x)
t−1 t−1 t−1
Select the next evaluation point x by x = argmax a (x)
t t x∈X t−1
Observe y = f(x )+ε at the point x
t t t t
Update GP by adding the observed data
end for
Output: Return H and L as the estimated sets
T T
Thus, by using ucb (x) and lcb (x), STR (x) can be rewritten as
t−1 t−1 t−1
STR (x) = min{ucb (x)−θ,θ−lcb (x)}.
t−1 t−1 t−1
In this study, we consider sampling β of the straddle heuristic from a probability distribution. In
t
the framework of black-box function maximization, Takeno et al. (2023) proposes to use a sample
from a two-parameter exponential distribution as the trade-off parameter of the original GP-UCB.
The two-parameter exponential distribution considered by Takeno et al. (2023) can be expressed as
2log(|X|/2)+s , where s follows the chi-squared distribution with two degrees of freedom. Therefore,
t t
we use a similar argument and consider β of the straddle heuristic as a sample from the chi-squared
t
distribution with two degrees of freedom, and propose the following randomized straddle AF.
Definition 3.2 (Randomized Straddle). For each t ≥ 1, let β be a sample from the chi-squared
t
distribution with two degrees of freedom, where β ,...,β ,ε ,...,ε ,f are mutually independent.
1 t 1 t
Then, the randomized straddle a (x) is defined as follows:
t−1
a (x) = max{min{ucb (x)−θ,θ−lcb (x)},0}. (3.2)
t−1 t−1 t−1
Hence, using a (x), the next point to be evaluated is selected by x = argmax a (x). Here,
t−1 t x∈X t−1
Takeno et al. (2023) adds a constant 2log(|X|/2) that depends on the number of elements of X to the
sample from the chi-squared distribution with two degrees of freedom, whereas the random sample
proposed in this study does not need to add such a constant. Therefore, the confidence parameter in
the randomized straddle does not depend on the number of iterations t and the number of candidate
points. The only difference between the straddle heuristic STR (x) and the randomized straddle
t−1
1/2
(3.2) is that β is randomized, and (3.2) performs the max operation with 0. We show in Section 4
t
that this modification leads to theoretical guarantees. Finally, we give the pseudocode of the proposed
algorithm in Algorithm 1.
4. Theoretical Analysis
In this section, we give theoretical guarantees for the proposed model. First, we define the loss
l (x) for each x ∈ X and t ≥ 1 as
t
 0 if x ∈ H∗,x ∈ H ,
 t

 0 if x ∈ L∗,x ∈ L ,
l (x) = t .
t f(x)−θ if x ∈ H∗,x ∈ L ,
 t

 θ−f(x) if x ∈ L∗,x ∈ H
t
Then, the loss r(H ,L ) for the estimated sets H and L is defined as follows1:
t t t t
(cid:40) 1 (cid:80) l (x) if X is finite
r(H t,L t) = |X|
1
x (cid:82)∈X
l
t
(x)dx if X is infinite
Vol(X) X t
≡ r .
t
1The discussion of the case where the loss is defined based on the maximum valuer(H ,L )=max l (x) is given
t t x∈X t
in Appendix A.
5We also define the cumulative loss as R =
(cid:80)t
r . Let γ be a maximum information gain, where γ
t i=1 i t t
is one of indicators for measuring the sample complexity. The maximum information gain γ is often
t
used in theoretical analysis of BO using GP (Srinivas et al., 2010; Gotovos et al., 2013), and γ is given
t
by
1
γ = max logdet(I +σ−2 K˜ ),
t 2 x˜1,...,x˜t t noise t
where K˜ is the t×t matrix whose (j,k)-th element is k(x˜ ,x˜ ). Then, the following theorem holds.
t j k
Theorem 4.1. Assume that f follows GP(0,k), where k(·,·) is a positive-definite kernel satisfying
k(x,x) ≤ 1 for any x ∈ X. For each t ≥ 1, let β be a sample from the chi-squared distribution with
t
two degrees of freedom, where β ,...,β ,ε ,....ε ,f are mutually independent. Then, the following
1 t 1 t
inequality holds:
(cid:112)
E[R ] ≤ C tγ ,
t 1 t
where C = 4/log(1+σ−2 ), and the expectation is taken with all randomness including f, ε and
1 noise t
β .
t
From Theorem 4.1, the following theorem holds.
Theorem 4.2. Under the assumptions of Theorem 4.1, the following inequality holds:
(cid:114)
C γ
E[r ] ≤ 1 t ,
t
t
where C is given in Theorem 4.1.
1
Here, by the definition of the loss l (x), l (x) represents how far f(x) is from the threshold when
t t
x is misclassified, and r represents the average values of l (x) across all candidate points. Under mild
t t
assumptions, it is known that γ is sublinear (Srinivas et al., 2010). Therefore, by Theorem 4.1, it
t
is guaranteed that R is also sublinear in the expected value sense. Furthermore, by Theorem 4.2, it
t
is guaranteed that r converges to 0 in the expected value sense. Finally, regarding the theoretical
t
analysis comparison with GP-based methods such as the LSE algorithm and TRUVAR, it is difficult
to simply compare the proposed method with these methods. The reason is that, firstly, the proposed
methodandthesemethodsusedifferentmethodstoestimateH∗ andL∗,andsecondly,themethodsfor
measuring the goodness of the estimated sets are different. However, recall that the proposed method
1/2
has theoretical guarantees, and the confidence parameter β does not depend on the number of
t
iterations t and the input space X, and it can be applied whether X is finite or infinite. In addition,
√
since
E[β1/2
] = 2π/2 ≈ 1.25, realized values of
β1/2
have the property of not being conservative,
t t
and to the best of our knowledge, there is no existing method that satisfies all of these. Moreover, we
confirm in Section 5 that the practical performance of the proposed method is equal to or better than
existing methods.
5. Numerical Experiments
We confirm the practical performance of the proposed method using synthetic functions and real-
world data.
5.1. Synthetic Data Experiments when X is Finite
In this section, the input space X was defined as a set of grid points that uniformly cut the region
[l ,u ]×[l ,u ] into 50×50. In all experiments, we used the following Gaussian kernel:
1 1 2 2
(cid:18) ∥x−x′∥2(cid:19)
k(x,x′) = σ2exp − 2 .
f L
As black-box functions, we considered the following three synthetic functions:
Case 1 The black-box function f(x ,x ) is a sample path from GP(0,k), where k(·,·) is given by
1 2
k(x,x′) = exp(−∥x−x′∥2/2).
2
6Table 1: Experimental parameters for each setting in Section 5.1
Black-box function l u l u σ2 L σ2 θ
1 1 2 2 f noise
GP sample path −5 5 −5 5 1 2 10−6 0.5
Sinusoidal function 0 1 0 2 exp(2) 2exp(−3) exp(−2) 1
Himmelblau’s function -5 5 -5 5 exp(8) 2 exp(4) 0
Case 2 The black-box function f(x ,x ) is the following sinusoidal function:
1 2
f(x ,x ) = sin(10x )+cos(4x )−cos(3x x ).
1 2 1 2 1 2
Case 3 The black-box function f(x ,x ) is the following shifted negative Himmelblau function:
1 2
f(x ,x ) = −(x2+x −11)2−(x +x2−7)2+100.
1 2 1 2 1 2
In addition, we used the normal distribution with mean 0 and variance σ2 for the observation noise.
noise
Thethresholdθ andtheparametersusedforeachsettingaresummarizedinTable1. Here,thesettings
for the sinusoidal and Himmelblau functions are the same as those used in Zanette et al. (2019). The
performance was evaluated using the loss r and Fscore, where Fscore is the F-score calculated by
t
|H ∩H∗| |H ∩H∗| 2×Pre×Rec
t t
Pre = ,Rec = ,Fscore = .
|H | |H∗| Pre+Rec
t
Then, we compared the following six AFs:
(Random) Select x by using random sampling.
t
(US) Perform uncertainty sampling, that is, x = argmax σ2 (x).
t x∈X t−1
(Straddle) PerformthestraddleheuristicproposedbyBryanetal.(2005),thatis,x = argmax STR (x).
t x∈X t−1
(LSE) Perform the LSE algorithm using the LSE AF a (x) proposed by Gotovos et al. (2013), that
t−1
is, x = argmax a (x).
t x∈X t−1
(MILE) PerformtheMILEalgorithmproposedbyZanetteetal.(2019),thatis,x = argmax a (x),
t x∈X t−1
where, a (x) is the same as the robust MILE, another AF proposed by Zanette et al. (2019),
t−1
with the tuning parameters ϵ and γ set to 0 and −∞, respectively.
(Proposed) Select x by using (3.2), that is, x = argmax a (x).
t t x∈X t−1
In all experiments, the classification rules were the same for all six methods, and only the AF was
1/2 1/2
changed. We used β = 3 as the confidence parameter required for MILE and Straddle, and β =
t t
(cid:112)
2log(2500×π2t2/(6×0.05)) for LSE. Under this setup, one initial point was taken at random and
the algorithm was run until the number of iterations reached 300. This simulation repeated 100 times
and the average r and Fscore at each iteration were calculated, where in Case 1, f was generated for
t
each simulation from GP(0,k).
From Fig. 2, it can be confirmed that the proposed method has performance equal to or better
than the comparison methods in all three cases in terms of both the loss r and Fscore.
t
5.2. Synthetic Data Experiments when X is Infinite
In this section, we used the region [−5,5]5 ⊂ R5 as X and the same kernel as in Section 5.1. As
black-box functions, we used he following three synthetic functions:
Case 1 The black-box function f(x ,x ,x ,x ,x ) is the following shifted negative sphere function:
1 2 3 4 5
(cid:32) 5 (cid:33)
(cid:88)
f(x ,x ,x ,x ,x ) = 41.65518− x2 .
1 2 3 4 5 d
d=1
7GP sample path Sinusoidal Himmelblau
Random Random Random
US US US
Straddle Straddle Straddle
LSE LSE LSE
MILE MILE MILE
Proposed Proposed Proposed
000000 555555000000 111111000000000000 111111555555000000 222222000000000000 222222555555000000 333333000000000000 000000 555555000000 111111000000000000 111111555555000000 222222000000000000 222222555555000000 333333000000000000 000000 555555000000 111111000000000000 111111555555000000 222222000000000000 222222555555000000 333333000000000000
Iteration Iteration Iteration
GP sample path Sinusoidal Himmelblau
Random Random Random
US US US
Straddle Straddle Straddle
LSE LSE LSE
MILE MILE MILE
Proposed Proposed Proposed
000000 555555000000 111111000000000000 111111555555000000 222222000000000000 222222555555000000 333333000000000000 000000 555555000000 111111000000000000 111111555555000000 222222000000000000 222222555555000000 333333000000000000 000000 555555000000 111111000000000000 111111555555000000 222222000000000000 222222555555000000 333333000000000000
Iteration Iteration Iteration
Figure 2: Averages of the loss r and Fscore for each acquisition function (AF) over 100 simulations
t
for each setting when the input space is finite. The top row shows r , the bottom row shows Fscore,
t
and each error bar length represents the six times the standard error.
Case 2 Theblack-boxfunctionf(x ,x ,x ,x ,x )isthefollowingshiftednegativeRosenbrockfunction:
1 2 3 4 5
(cid:34) 4 (cid:35)
f(x ,x ,x ,x ,x ) = 53458.91− (cid:88)(cid:8) 100(x −x2)2+(1−x )2(cid:9) .
1 2 3 4 5 d+1 d d
d=1
Case 3 Theblack-boxfunctionf(x ,x ,x ,x ,x )isthefollowingshiftednegativeStyblinski-Tangfunc-
1 2 3 4 5
tion:
(cid:80)5 (x4 −16x2 +5x )
f(x ,x ,x ,x ,x ) = −20.8875− d=1 d d d .
1 2 3 4 5
2
In addition, we used the normal distribution with mean 0 and variance σ2 for the observation
noise
noise. The threshold θ and the parameters used for each setting are summarized in Table 2. The
performance was evaluated using the loss r and Fscore. For each simulation, 100,000 points were
t
randomly selected from [−5,5]5, and these were used as the input point set X˜ for the calculation
of r and Fscore. The values of r and Fscore in X˜ were calculated as approximations of the true
t t
values. As AFs, we compared five methods used in Section 5.1, except for MILE, which does not
1/2
handle continuous settings. We used β = 3 as the confidence parameter required for Straddle, and
t
β1/2 = (cid:112) 2log(1015×π2t2/(6×0.05)) for LSE. Here, the original LSE algorithm uses the intersection
t
of ucb (x) and lcb (x) in the previous iterations given below to calculate the AF:
t−1 t−1
u˜cb (x) = min ucb (x),lc˜b (x) = max lcb (x).
t−1 i−1 t−1 i−1
1≤i≤t 1≤i≤t
Ontheotherhand,intheinfinitesetsetting,wedidnotperformthisoperation,andinsteadcalculated
the AF using u˜cb (x) = ucb (x) and lc˜b (x) = lcb (x). Under this setup, one initial point
t−1 t−1 t−1 t−1
was taken at random and the algorithm was run until the number of iterations reached 500. This
simulation repeated 100 times and the average r and Fscore at each iteration were calculated.
t
8
ssoL
erocsF
000000222222......000000
555555111111......000000
000000111111......000000
555555000000......000000
000000000000......000000
000000......111111
888888......000000
666666......000000
444444......000000
222222......000000
000000......000000
ssoL
erocsF
555555111111......000000
000000111111......000000
555555000000......000000
000000000000......000000
000000......111111
888888......000000
666666......000000
444444......000000
222222......000000
000000......000000
ssoL
erocsF
000000444444
000000333333
000000222222
000000111111
000000
000000......111111
888888......000000
666666......000000
444444......000000
222222......000000
000000......000000Table 2: Experimental parameters for each setting in Section 5.2
Black-box function σ2 L σ2 θ
f noise
Sphere 900 40 10−6 9.6
Rosenbrock 300002 40 10−6 14800
Styblinski-Tang 752 40 10−6 12.3
Sphere Rosenbrock Styblinski−Tang
Random Random Random
US US US
Straddle Straddle Straddle
LSE LSE LSE
Proposed Proposed Proposed
00000 111110000000000 222220000000000 333330000000000 444440000000000 555550000000000 00000 111110000000000 222220000000000 333330000000000 444440000000000 555550000000000 00000 111110000000000 222220000000000 333330000000000 444440000000000 555550000000000
Iteration Iteration Iteration
Sphere Rosenbrock Styblinski−Tang
Random Random Random
US US US
Straddle Straddle Straddle
LSE LSE LSE
Proposed Proposed Proposed
00000 111110000000000 222220000000000 333330000000000 444440000000000 555550000000000 00000 111110000000000 222220000000000 333330000000000 444440000000000 555550000000000 00000 111110000000000 222220000000000 333330000000000 444440000000000 555550000000000
Iteration Iteration Iteration
Figure 3: Averages of the loss r and Fscore for each AF over 100 simulations for each setting when
t
the input space is infinite. The top row shows r , the bottom row shows Fscore, and each error bar
t
length represents the six times the standard error.
From Fig 3, it can be confirmed that the proposed method has performance equal to or better
than the comparison methods in terms of both the loss r and Fscore in the sphere function setting.
t
In the case of the Rosenbrock function setting, it can be confirmed that the proposed method has
performance equivalent to or better than the comparison method in terms of r . On the other hand, in
t
termsofFscore, Random has thebestperformance uptoabout250iterations, butitcanbeconfirmed
that the proposed method has performance equivalent to or better than the comparison method at
the end of the iterations. In the Styblinski-Tang function setting, Random performs best in terms of
r and Fscore up to about 300 iterations, but the proposed method performs at least as well as the
t
comparison methods at the end of the iterations.
5.3. Real-world Data Experiments
In this section, we conducted experiments using the carrier lifetime value, which is a measure of
the quality performance of silicon ingots used as materials for solar cells (Kutsukake et al., 2015). The
data we used include the two-dimensional coordinates x = (x ,x ) ∈ R2 of the sample surface and the
1 2
carrier lifetime values f˜(x) ∈ [0.091587,7.4613] at each coordinate, where x ∈ {2a+6 | 1 ≤ a ≤ 89},
1
x ∈ {2a+6 | 1 ≤ a ≤ 74} and |X| = 89×74 = 6586. In quality evaluation using carrier lifetime
2
values, it is important to identify defective regions called red zones, that is, regions where the value
of f˜(x) is below a certain value. In this experiment, the threshold was set to 3, and the problem of
9
ssoL
erocsF
00000.....33333
55555.....22222
00000.....22222
55555.....11111
00000.....11111
55555.....00000
00000.....00000
00000.....11111
88888.....00000
66666.....00000
44444.....00000
22222.....00000
00000.....00000
ssoL
erocsF
00000000000000088888
00000000000000066666
00000000000000044444
00000000000000022222
00000
00000.....11111
88888.....00000
66666.....00000
44444.....00000
22222.....00000
00000.....00000
ssoL
erocsF
5555522222
0000022222
5555511111
0000011111
55555
00000
00000.....11111
88888.....00000
66666.....00000
44444.....00000
22222.....00000
00000.....00000Defective area identification Defective area identification
Random
US
Straddle
LSE
MILE
Proposed
Random
US
Straddle
LSE
MILE
Proposed
000000 555555000000 111111000000000000 111111555555000000 222222000000000000 000000 555555000000 111111000000000000 111111555555000000 222222000000000000
Iteration Iteration
Figure 4: Averages of the loss r and Fscore for each AF over 100 simulations for the carrier lifetime
t
data. The left figure shows r , the right figure shows Fscore, and each error bar length represents the
t
six times the standard error.
identifying regions where f˜(x) is 3 or less was considered. We considered f(x) = −f˜(x)+3 as the
black-boxfunctionandperformedexperimentswithθ = 0. Inaddition, theexperimentwasconducted
assuming that there was no noise in the observations. On the other hand, to stabilize the posterior
distribution calculation, σ2 = 10−6 was used in the calculation. We used the following Mat´ern 3/2
noise
kernel:
(cid:32) √ (cid:33) (cid:32) √ (cid:33)
3∥x−x′∥ 3∥x−x′∥
k(x,x′) = 4 1+ 2 exp − 2 .
25 25
The performance was evaluated using the loss r and Fscore. As AFs, we compared six methods
t
1/2
used in Section 5.1. We used β = 3 as the confidence parameter required for MILE and Straddle,
t
and β1/2 = (cid:112) 2log(6586×π2t2/(6×0.05)) for LSE. Under this setup, one initial point was taken at
t
random and the algorithm was run until the number of iterations reached 200. Since the observation
noisewassetto0,theexperimentwasconductedunderthesettingthatapointthathadbeenobserved
once would not be observed thereafter. This simulation repeated 100 times and the average r and
t
Fscore at each iteration were calculated.
From Fig. 4, it can be confirmed that the proposed method has performance equal to or better
than the comparison methods in terms of both the loss r and Fscore.
t
6. Conclusion
In this study, we proposed the novel method, the randomized straddle algorithm, which is an
extension of the straddle algorithm for LSE problems for black-box functions. The proposed method
replaces the value of β in the straddle algorithm with a random sample from the chi-squared dis-
t
tribution with two degrees of freedom, and LSE is performed based on the GP posterior mean. By
(cid:112)
these modifications, we showed that the expected value of the loss in the estimated sets is O( γ /t).
t
Compared to existing methods, the proposed method has the following three advantages. First, most
theoretical analyses of existing methods include conservative confidence parameters that depend on
thenumberofcandidatepointsanditerations,whilesuchtermsdonotappearintheproposedmethod.
Second, existing methods are not applicable to the case of a continuous search space, or they require
discretization, and in addition, parameters required for discretization are generally unknown. On
the other hand, the proposed method does not require any change of the algorithm even when the
search space is continuous, and gives same theoretical guarantees as when the search space is finite.
Thirdly, while confidence parameters in existing methods are too conservative, the expectation of the
√
confidence parameter in the proposed method is equal to 2π/2 ≈ 1.25, which is not too conservative.
Furthermore, through numerical experiments, we confirmed that the performance of the proposed
method is equal to or better than that of existing methods. This means that the performance of the
proposed method is comparable to that of heuristic methods, and furthermore, the proposed method
is more useful than existing methods in that it also has theoretical guarantees. On the other hand, it
10
ssoL
888888......000000
666666......000000
444444......000000
222222......000000
000000......000000
erocsF
000000......111111
888888......000000
666666......000000
444444......000000
222222......000000
000000......000000is important to propose a method that has both theoretical guarantees and practical applicability for
other evaluation measures of classification performance, such as Fscore. These issues are left as future
work.
Acknowledgement
ThisworkwaspartiallysupportedbyJSPSKAKENHI(JP20H00601,JP23K16943,JP23K19967,JP24K20847),
JST ACT-X (JPMJAX23CD), JST CREST (JPMJCR21D3, JPMJCR22N2), JST Moonshot R&D
(JPMJMS2033-05),JSTAIPAccelerationResearch(JPMJCR21U2),NEDO(JPNP18002,JPNP20006)
and RIKEN Center for Advanced Intelligence Project.
References
Bogunovic, I., Scarlett, J., Krause, A., and Cevher, V. (2016). Truncated variance reduction: A
unified approach to bayesian optimization and level-set estimation. Advances in neural information
processing systems, 29.
Bryan, B., Nichol, R. C., Genovese, C. R., Schneider, J., Miller, C. J., and Wasserman, L. (2005).
Active learning for identifying function threshold boundaries. Advances in neural information pro-
cessing systems, 18.
Bryan, B. and Schneider, J. (2008). Actively learning level-sets of composite functions. In Proceedings
of the 25th international conference on Machine learning, pages 80–87.
Contal, E., Buffoni, D., Robicquet, A., and Vayatis, N. (2013). Parallel gaussian process optimization
with upper confidence bound and pure exploration. In Joint European Conference on Machine
Learning and Knowledge Discovery in Databases, pages 225–240. Springer.
Gotovos, A., Casati, N., Hitz, G., and Krause, A. (2013). Active learning for level set estimation.
In Proceedings of the Twenty-Third international joint conference on Artificial Intelligence, pages
1344–1350.
Hozumi, S., Kutsukake, K., Matsui, K., Kusakawa, S., Ujihara, T., and Takeuchi, I. (2023). Adap-
tive defective area identification in material surface using active transfer learning-based level set
estimation. arXiv preprint arXiv:2304.01404.
Inatsu, Y., Iwazaki, S., and Takeuchi, I. (2021). Active learning for distributionally robust level-set
estimation. In International Conference on Machine Learning, pages 4574–4584. PMLR.
Inatsu, Y., Karasuyama, M., Inoue, K., and Takeuchi, I. (2020). Active Learning for Level Set
Estimation Under Input Uncertainty and Its Extensions. Neural Computation, 32(12):2486–2531.
Inatsu, Y., Takeno, S., Hanada, H., Iwata, K., and Takeuchi, I. (2024). Bounding box-based multi-
objective Bayesian optimization of risk measures under input uncertainty. In Dasgupta, S., Mandt,
S., and Li, Y., editors, Proceedings of The 27th International Conference on Artificial Intelligence
and Statistics, volume 238 of Proceedings of Machine Learning Research, pages 4564–4572. PMLR.
Iwazaki, S., Inatsu, Y., and Takeuchi, I. (2020). Bayesian experimental design for finding reliable level
set under input uncertainty. IEEE Access, 8:203982–203993.
Kandasamy, K., Dasarathy, G., Oliva, J. B., Schneider, J., and P´oczos, B. (2016). Gaussian pro-
cess bandit optimisation with multi-fidelity evaluations. Advances in neural information processing
systems, 29.
Kandasamy, K., Dasarathy, G., Schneider, J., and P´oczos, B. (2017). Multi-fidelity bayesian opti-
misation with continuous approximations. In International conference on machine learning, pages
1799–1808. PMLR.
Kandasamy,K.,Schneider,J.,andP´oczos,B.(2015).Highdimensionalbayesianoptimisationandban-
dits via additive models. In International conference on machine learning, pages 295–304. PMLR.
11Kirschner, J., Bogunovic, I., Jegelka, S., and Krause, A. (2020). Distributionally robust bayesian opti-
mization. In Chiappa, S. and Calandra, R., editors, Proceedings of the Twenty Third International
Conference on Artificial Intelligence and Statistics, volume 108 of Proceedings of Machine Learning
Research, pages 2174–2184. PMLR.
Kusakawa, S., Takeno, S., Inatsu, Y., Kutsukake, K., Iwazaki, S., Nakano, T., Ujihara, T., Kara-
suyama, M., and Takeuchi, I. (2022). Bayesian optimization for cascade-type multistage processes.
Neural Computation, 34(12):2408–2431.
Kutsukake, K., Deura, M., Ohno, Y., and Yonenaga, I. (2015). Characterization of silicon in-
gots: Mono-like versus high-performance multicrystalline. Japanese Journal of Applied Physics,
54(8S1):08KD10.
Letham, B., Guan, P., Tymms, C., Bakshy, E., and Shvartsman, M. (2022). Look-ahead acquisition
functions for bernoulli level set estimation. In International Conference on Artificial Intelligence
and Statistics, pages 8493–8513. PMLR.
Mason, B., Jain, L., Mukherjee, S., Camilleri, R., Jamieson, K., and Nowak, R. (2022). Nearly
optimal algorithms for level set estimation. In Camps-Valls, G., Ruiz, F. J. R., and Valera, I.,
editors, Proceedings of The 25th International Conference on Artificial Intelligence and Statistics,
volume 151 of Proceedings of Machine Learning Research, pages 7625–7658. PMLR.
Rasmussen, C. E. and Williams, C. K. I. (2005). Gaussian Processes for Machine Learning (Adaptive
Computation and Machine Learning). The MIT Press.
Rolland,P.,Scarlett,J.,Bogunovic,I.,andCevher,V.(2018). High-dimensionalbayesianoptimization
via additive models with overlapping groups. In International conference on artificial intelligence
and statistics, pages 298–307. PMLR.
Shahriari, B., Swersky, K., Wang, Z., Adams, R. P., and De Freitas, N. (2015). Taking the human out
of the loop: A review of bayesian optimization. Proceedings of the IEEE, 104(1):148–175.
Shekhar, S. and Javidi, T. (2019). Multiscale gaussian process level set estimation. In The 22nd
International Conference on Artificial Intelligence and Statistics, pages 3283–3291. PMLR.
Srinivas, N., Krause, A., Kakade, S., andSeeger, M.(2010). Gaussianprocessoptimizationintheban-
dit setting: No regret and experimental design. In Proceedings of the 27th International Conference
on Machine Learning, pages 1015–1022.
Takeno, S., Inatsu, Y., and Karasuyama, M. (2023). Randomized Gaussian process upper confidence
bound with tighter Bayesian regret bounds. In Krause, A., Brunskill, E., Cho, K., Engelhardt, B.,
Sabato, S., and Scarlett, J., editors, Proceedings of the 40th International Conference on Machine
Learning, volume 202 of Proceedings of Machine Learning Research, pages 33490–33515. PMLR.
Zanette, A., Zhang, J., and Kochenderfer, M. J. (2019). Robust super-level set estimation using
gaussian processes. In Machine Learning and Knowledge Discovery in Databases: European Con-
ference, ECML PKDD 2018, Dublin, Ireland, September 10–14, 2018, Proceedings, Part II 18,pages
276–291. Springer.
Zuluaga, M., Krause, A., et al. (2016). e-pal: An active learning approach to the multi-objective
optimization problem. Journal of Machine Learning Research, 17(104):1–32.
12Algorithm 2 Randomized Straddle Algorithms for Max-value Loss in the Finite Setting
Input: GP prior GP(0,k), threshold θ ∈ R
for t = 1,2,...,T do
Compute µ (x) and σ2 (x) for each x ∈ X by (2.1)
t−1 t−1
Estimate H and L by (3.1)
t t
Generate ξ from the chi-squared distribution with two degrees of freedom
t
Compute β = ξ +2log(|X|), ucb (x), lcb (x) and a˜ (x)
t t t−1 t−1 t−1
Select the next evaluation point x by x = argmax a˜ (x)
t t x∈X t−1
Observe y = f(x )+ε at the point x
t t t t
Update GP by adding the observed data
end for
Output: Return H and L as the estimated sets, where Tˆ is given by (A.2)
Tˆ Tˆ
Appendix
A. Extension to Max-value Loss
In this section, we consider the following max-value loss defined based on the maximum value of
l (x):
t
r(H ,L ) = maxl (x) ≡ r˜.
t t t t
x∈X
When X is finite, we need to modify the definition of the AF and the estimated sets returned at the
end of the algorithm. On the other hand, if X is an infinite set, the definitions of H and L should
t t
be modified in addition to the above. Therefore, we discuss the finite and infinite cases separately.
A.1. Proposed Method for Max-value Loss when X is Finite
When X is finite, we propose the following AF with a modified distribution that β follows.
t
Definition A.1 (Randomized Straddle for Max-value Loss). For each t ≥ 1, let ξ be a random
t
sample from the chi-squared distribution with two degrees of freedom, where ξ ,...,ξ ,ε ,...,ε ,f
1 t 1 t
are mutually independent. Define β = ξ +2log(|X|). Then, the randomized straddle AF for the
t t
max-value loss, a˜ (x), is defined as follows:
t−1
a˜ (x) = max{min{ucb (x)−θ,θ−lcb (x)},0}. (A.1)
t−1 t−1 t−1
By using a˜ (x), the next point to be evaluated is selected by x = argmax a˜ (x). In
t−1 t x∈X t−1
addition,wechangeestimationsetsreturnedattheendofiterationsT inthealgorithmtothefollowing
instead of H and L :
T T
Definition A.2. For each t, define
tˆ= argminE [r˜], (A.2)
t i
1≤i≤t
where E [·] represents the conditional expectation given D . Then, at the end of iterations T, we
t t−1
define H andL to be the estimated sets.
Tˆ Tˆ
Finally, we give the pseudocode of the proposed algorithm in Algorithm 2.
A.1.1. Theoretical Analysis for Max-value Loss when X is Finite
For the max-value loss, the following theorem holds under Algorithm 2.
Theorem A.1. Let f be a sample path from GP(0,k), where k(·,·) is a positive-definite kernel sat-
isfying k(x,x) ≤ 1 for any x ∈ X. For each t ≥ 1, let ξ be a random sample from the chi-squared
t
distribution with two degrees of freedom, where ξ ,...,ξ ,ε ,....ε ,f are mutually independent. De-
1 t 1 t
fine β = ξ +2log(|X|). Then, the following holds for R˜ = (cid:80)t r˜:
t t t i=1 i
(cid:113)
E[R˜ ] ≤ C˜ tγ ,
t 1 t
13where C˜ = (4+4log(|X|))/log(1+σ−2 ) and the expectation is taken with all randomness including
1 noise
f,ε and β .
t t
From Theorem A.1, the following theorem holds.
Theorem A.2. Under the assumptions of Theorem A.1, the following inequality holds:
(cid:115)
C˜ γ
E[r ] ≤ 1 t ,
tˆ
t
where tˆand C˜ are given in (A.2) and Theorem A.1, respectively.
1
Comparing Theorem 4.1 and A.1, when considering the max-value loss, β needs to be 2log(|X|)
t
larger than in the case of r , and the constant that appears in the upper bound of the expected value
t
of the cumulative loss has the relationship C˜ = (1+log(|X|))C . Note that while the upper bound in
1 1
the case of r does not depend on X, for the max-value loss it depends on the logarithm of the number
t
of elements in X. Also, when comparing Theorem 4.2 and A.2, it is not necessary to consider tˆin r ,
t
whereas it is necessary to consider tˆin the max-value loss. For the max-value loss, it is difficult to
analytically derive E [r˜], and therefore it is also difficult to precisely calculate tˆ. Nevertheless, since
t i
theposteriordistributionoff givenD isagainaGP,wecangenerateM samplepathsfromtheGP
t−1
posterior distribution and calculate the realization r˜(j) of r˜ from each sample path f(j), and therefore
i i
calculate the estimate tˇof tˆas
M
tˇ= argmin 1 (cid:88) r˜(j) .
M i
1≤i≤t
j=1
A.2. Proposed Method for Max-value Loss when X is Infinite
In this section, we assume that the input space X ⊂ Rd is a compact set and satisfies X ⊂ [0,r]d,
where r > 0. Furthermore, we assume the following additional assumption for f:
Assumption A.1. Let f be differentiable with probability 1. Assume that there exist positive
constants a,b such that
P(cid:18) sup(cid:12)
(cid:12)
(cid:12)
∂f
(cid:12)
(cid:12)
(cid:12) >
L(cid:19)
≤
aexp(cid:32) −(cid:18) L(cid:19)2(cid:33)
, j ∈ [d],
(cid:12)∂x (cid:12) b
x∈X j
where x is the j-th element of x and [d] ≡ {1,...,d}.
j
Next, we provide a LSE method based on the discretization of the input space.
A.2.1. Level Set Estimation for Max-value Loss when X is Infinite
For each t ≥ 1, let X be a finite subset of X. Also, for any x ∈ X, let [x] be the element of X that
t t t
has the shortest L distance from x2. Then, we define H and L as
1 t t
H = {x ∈ X | µ ([x] ) ≥ θ}, L = {x ∈ X | µ ([x] ) < θ}. (A.3)
t t−1 t t t−1 t
A.2.2. Acquisition Function for Max-value Loss when X is Infinite
We define a randomized straddle AF based on X :
t
Definition A.3. Foreacht ≥ 1, letξ bearandomsamplefromthechi-squareddistributionwithtwo
t
degreesoffreedom, whereξ ,...,ξ ,ε ,...,ξ ,f aremutuallyindependent. Defineβ = 2log(|X |)+ξ .
1 t 1 t t t t
Then, the randomized straddle AF for the max-value loss when X is infinite, aˇ (x), is defined as
t−1
follows:
aˇ (x) = max{min{ucb (x)−θ,θ−lcb (x)},0}.
t−1 t−1 t−1
The next point to be evaluated is selected by x = argmax aˇ (x). Finally, we give the
t x∈X t−1
pseudocode of the proposed algorithm in Algorithm 3.
2Iftherearemultiplex∈X withtheshortestL distance,determinetheonethatisunique. Forexample,weadopt
t 1
theonewiththesmallestfirstcomponent. Ifitstillcannotbeuniquelydetermined,wechoosetheonewiththesmallest
second component. By repeating this operation up to the d-th component, we can uniquely determine.
14Algorithm 3 Randomized Straddle Algorithms for Max-value Loss in the Infinite Setting
Input: GP prior GP(0,k), threshold θ ∈ R, discretized sets X ,...,X
1 T
for t = 1,2,...,T do
Compute µ (x) and σ2 (x) for each x ∈ X by (2.1)
t−1 t−1
Estimate H and L by (A.3)
t t
Generate ξ from the chi-squared distribution with two degrees of freedom
t
Compute β = ξ +2log(|X |), ucb (x), lcb (x) and a˜ (x)
t t t t−1 t−1 t−1
Select the next evaluation point x by x = argmax aˇ (x)
t t x∈X t−1
Observe y = f(x )+ε at the point x
t t t t
Update GP by adding the observed data
end for
Output: Return H and L as the estimated sets, where Tˆ = argmin E [r˜]
Tˆ Tˆ 1≤i≤T T i
A.2.3. Theoretical Analysis for Max-value Loss when X is Infinite
Under Algorithm 3, the following theorem holds.
Theorem A.3. Let X ⊂ [0,r]d be a compact set with r > 0. Assume that f is a sample path from
GP(0,k), where k(·,·) is a positive-definite kernel satisfying k(x,x) ≤ 1 for any x ∈ X. Also assume
(cid:112) √
that Assumption A.1 holds. Moreover, for each t ≥ 1, let τ = ⌈bdrt2( log(ad)+ π/2)⌉, and let X
t t
be a finite subset of X satisfying |X | = τd and
t t
dr
∥x−[x] ∥ ≤ , x ∈ X.
t 1
τ
t
Suppose that ξ is a random sample from the chi-squared distribution with two degrees of free-
t
(cid:112)
dom, where ξ ,...,ξ ,ε ,...,ε ,f are mutually independent. Define β = 2dlog(⌈bdrt2( log(ad)+
√ 1 t 1 t t
π/2)⌉)+ξ . Then, the following holds for R˜ = (cid:80)t r˜:
t t i=1 i
E[R˜ ] ≤
π2
+(cid:112) C tγ (2+s ),
t 1 t t
6
where Cˇ = 2/log(1+σ−2 ) and s = 2dlog(⌈bdrt2((cid:112) log(ad)+√ π/2)⌉), and the expectation is taken
1 noise t
with all randomness including f,ε and β .
t t
From Theorem A.3, the following holds.
Theorem A.4. Under the assumptions of Theorem A.3, define
tˆ= argminE [r˜].
t i
1≤i≤t
Then, the following holds:
(cid:115)
π2 Cˇ γ (2+s )
E[r˜] ≤ + 1 t t ,
tˆ
6t t
where Cˇ and s are given in Theorem A.3.
1 t
B. Proofs
B.1. Proof of Theorem 4.1
Proof. Let δ ∈ (0,1). For any t ≥ 1, D and x ∈ X, from the proof of Lemma 5.1 in Srinivas et al.
t−1
(2010), the following holds with probability at least 1−δ:
1/2 1/2
lcb (x) ≡ µ (x)−β σ (x) ≤ f(x) ≤ µ (x)+β σ (x) ≡ ucb (x), (B.1)
t−1,δ t−1 δ t−1 t−1 δ t−1 t−1,δ
where β = 2log(1/δ). Here, we consider the case where x ∈ H . If x ∈ H∗, we have l (x) = 0. On
δ t t
the other hand, if x ∈ L∗, noting that lcb (x) ≤ f(x) by (B.1) we get
t−1,δ
l (x) = θ−f(x) ≤ θ−lcb (x).
t t−1,δ
15Moreover, the inequality µ (x) ≥ θ holds because x ∈ H . Hence, from the definition of lcb (x)
t−1 t t−1,δ
and ucb (x), we obtain
t−1,δ
θ−lcb (x) ≤ ucb (x)−θ.
t−1,δ t−1,δ
Therefore, we get
l (x) ≤ θ−lcb (x) = min{ucb (x)−θ,θ−lcb (x)}
t t−1,δ t−1,δ t−1,δ
≤ max{min{ucb (x)−θ,θ−lcb (x)},0} ≡ a (x).
t−1,δ t−1,δ t−1,δ
Similarly,weconsiderthecasewherex ∈ L . Ifx ∈ L∗,weobtainl (x) = 0. Thus,sincea (x) ≥ 0,
t t t−1,δ
we get l (x) ≤ a (x). On the other hand, if x ∈ H∗, noting that f(x) ≤ ucb (x) by (B.1), we
t t−1,δ t−1,δ
obtain
l (x) = f(x)−θ ≤ ucb (x)−θ.
t t−1,δ
Here, the inequality µ (x) < θ holds because x ∈ L . Hence, from the definition of lcb (x) and
t−1 t t−1,δ
ucb (x), we obtain
t−1,δ
ucb (x)−θ ≤ θ−lcb (x).
t−1,δ t−1,δ
Thus, the following inequality holds:
l (x) ≤ ucb (x)−θ = min{ucb (x)−θ,θ−lcb (x)} ≤ a (x).
t t−1,δ t−1,δ t−1,δ t−1,δ
Therefore, for all cases, the inequality l (x) ≤ a (x) holds. This implies that the following inequal-
t t−1,δ
ity holds with probability at least 1−δ:
l (x) ≤ a (x) ≤ maxa (x˜). (B.2)
t t−1,δ t−1,δ
x˜∈X
Next, we consider the conditional distribution of l (x) given D . Note that this distribution does
t t−1
not depend on β . Let F (·) be a distribution function of l (x) given D . Then, from (B.2) we
δ t−1 t t−1
have
(cid:18) (cid:19)
F maxa (x˜) ≥ 1−δ.
t−1 t−1,δ
x˜∈X
Hence, by taking the generalized inverse function of F (·) for both sides, the following inequality
t−1
holds:
F−1(1−δ) ≤ maxa (x˜).
t−1 t−1,δ
x˜∈X
Here,ifδ followstheuniformdistributionontheinterval(0,1),then1−δ followsthesamedistribution.
Inthiscase, thedistributionofF−1(1−δ)isequaltothedistributionofl (x)givenD . Thisimplies
t−1 t t−1
that
(cid:20) (cid:21)
E [l (x)] ≤ E maxa (x) ,
t t δ t−1,δ
x∈X
where E [·] means the expectation with respect to δ. Furthermore, since 2log(1/δ) and β follow the
δ t
chi-squared distribution with two degrees of freedom, the following holds:
E [l (x)] ≤ E [a (x )].
t t βt t−1 t
Thus, if X is finite, from the definition of r we obtain
t
(cid:34) (cid:35)
1 (cid:88) 1 (cid:88) 1 (cid:88)
E [r ] = E l (x) = E [l (x)] ≤ E [a (x )] = E [a (x )].
t t t
|X|
t
|X|
t t
|X|
βt t−1 t βt t−1 t
x∈X x∈X x∈X
Similarly, if X is infinite, from the definition of r and non-negativity of l (x), using Fubini’s theorem
t t
we get
(cid:20) (cid:90) (cid:21) (cid:90) (cid:90)
1 1 1
E [r ] = E l (x)dx = E [l (x)]dx ≤ E [a (x )]dx = E [a (x )].
t t t
Vol(X)
t
Vol(X)
t t
Vol(X)
βt t−1 t βt t−1 t
X X X
Therefore, the inequality E [r ] ≤ E [a (x )] holds for both cases. Moreover, from the definition of
t t βt t−1 t
a (x), the following inequality holds:
t−1
1/2
a (x ) ≤ β σ (x )
t−1 t t t−1 t
16Hence, we get the following inequality:
(cid:34) t (cid:35) (cid:34) t (cid:35)
E[R ] = E (cid:88) r ≤ E (cid:88) β1/2 σ (x )
t i i i−1 i
i=1 i=1
 
(cid:32) t (cid:33)1/2(cid:32) t (cid:33)1/2
−C −a −u −ch −y −-S −ch −w −a −rz −i −n −eq −u −al −it →y ≤ E  (cid:88) β i (cid:88) σ i2 −1(x i) 
i=1 i=1
(cid:118) (cid:118)
(cid:117) (cid:34) t (cid:35)(cid:117) (cid:34) t (cid:35)
−H −¨o −ld −e −r’s −−in −eq −u −a −lit →y ≤ (cid:117) (cid:116)E (cid:88) β (cid:117) (cid:116)E (cid:88) σ2 (x )
i i−1 i
i=1 i=1
(cid:118)
√ (cid:117) (cid:34) t (cid:35)
−E −[ −βi−]= −→2 = 2t(cid:117) (cid:116)E (cid:88) σ2 (x )
i−1 i
i=1
(cid:118)
√ (cid:117) (cid:117) (cid:34) 2 (cid:35)
≤ 2t(cid:116)E γ
log(1+σ−2 ) t
noise
(cid:112)
= C tγ ,
1 t
where the last inequality is derived by the proof of Lemma 5.4 in Srinivas et al. (2010).
B.2. Proof of Theorem 4.2
We first give three lemmas to prove Theorem 4.2. Theorem 4.2 is proved by Lemma B.1 and B.3.
Lemma B.1. Under the assumptions of Theorem 4.1, let
tˆ= argminE [r ].
t i
1≤i≤t
Then, the following inequality holds:
(cid:114)
C γ
E[r ] ≤ 1 t .
tˆ
t
Proof. From the definition of tˆ, the inequality E [r ] ≤ (cid:80)t i=1E t[ri] holds. Therefore, we obtain
t tˆ t
(cid:80)t E[r ] E(cid:2)(cid:80)t r (cid:3) E[R ]
E[r ] ≤ i=1 i = i=1 i = t .
tˆ
t t t
By combining this and Theorem 4.1, we get the desired result.
Lemma B.2. For any t ≥ 1, i ≤ t and x ∈ X, the expectation E [l (x)] can be calculated as follows:
t i
(cid:26)
σ (x)[ϕ(−α)+α{1−Φ(−α)}] if x ∈ L
E [l (x)] = t−1 i ,
t i σ (x)[ϕ(α)−α{1−Φ(α)}] if x ∈ H
t−1 i
where α =
µt−1(x)−θ
, and ϕ(z) and Φ(z) are the density and distribution function of the standard
σt−1(x)
normal distribution, respectively.
Proof. From the definition of l (x), if x ∈ L , l (x) can be expressed as l (x) = (f(x)−θ)1l[f(x) ≥ θ],
i i i i
where 1l[·] is the indicator function which takes 1 if the condition · holds, otherwise 0. Furthermore,
the conditional distribution of f(x) given D is the normal distribution with mean µ (x) and
t−1 t−1
17variance σ2 (x). Thus, from the definition of E [·], the following holds:
t−1 t
(cid:90) ∞ 1 (cid:18) (y−µ (x))2(cid:19)
E [l (x)] = (y−θ) exp − t−1 dy
t i θ (cid:113) 2πσ2 (x) 2σ t2 −1(x)
t−1
(cid:90) ∞ (cid:18) y−µ (x) µ (x)−θ(cid:19) 1 (cid:18) (y−µ (x))2(cid:19)
t−1 t−1 t−1
= σ (x) + exp − dy
θ t−1 σ t−1(x) σ t−1(x) (cid:113) 2πσ2 (x) 2σ t2 −1(x)
t−1
(cid:90) ∞ 1 (cid:18) z2(cid:19)
= σ (x)(z+α) √ exp − dz
t−1
2π 2
−α
(cid:90) ∞
= σ (x) (z+α)ϕ(z)dz = σ (x){[−ϕ(z)]∞ +α(1−Φ(−α))}
t−1 t−1 −α
−α
= σ (x)[ϕ(−α)+α{1−Φ(−α)}].
t−1
Similarly, if x ∈ H , l (x) can be expressed as l (x) = (θ−f(x))1l[f(x) < θ]. Then, we obtain
i i i
(cid:90) θ 1 (cid:18) (y−µ (x))2(cid:19)
E [l (x)] = (θ−y) exp − t−1 dy
t i −∞ (cid:113) 2πσ2 (x) 2σ t2 −1(x)
t−1
(cid:90) θ (cid:18) θ−µ (x) µ (x)−y(cid:19) 1 (cid:18) (y−µ (x))2(cid:19)
t−1 t−1 t−1
= σ (x) + exp − dy
−∞ t−1 σ t−1(x) σ t−1(x) (cid:113) 2πσ2 (x) 2σ t2 −1(x)
t−1
(cid:90) α 1 (cid:18) z2(cid:19)
= σ (x)(z−α) √ exp − (−1)dz
t−1
2π 2
∞
(cid:90) ∞
= σ (x) (z−α)ϕ(z)dz = σ (x){[−ϕ(z)]∞−α(1−Φ(α))}
t−1 t−1 α
α
= σ (x)[ϕ(α)−α{1−Φ(α)}].
t−1
Lemma B.3. Under the assumptions of Theorem 4.1 the equality tˆ= t holds.
Proof. Let x ∈ X. If x ∈ H , the inequality µ (x) ≥ θ holds. This implies that α ≥ 0. Hence,
t t−1
from Lemma B.2 we obtain
E [l (x)] = σ (x)[ϕ(α)−α{1−Φ(α)}].
t t t−1
Thus, since α ≥ 0, the following inequality holds:
σ (x)[ϕ(α)−α{1−Φ(α)}] ≤ σ (x)[ϕ(−α)+α{1−Φ(−α)}].
t−1 t−1
Therefore, from the definition of E [l (x)], we get
t i
E [l (x)] = σ (x)[ϕ(α)−α{1−Φ(α)}] ≤ E [l (x)].
t t t−1 t i
Similarly, if x ∈ L , using the same argument we have
t
E [l (x)] = σ (x)[ϕ(−α)+α{1−Φ(−α)}] ≤ E [l (x)].
t t t−1 t i
Here, if X is finite, from the definition of r we obtain
i
(cid:34) (cid:35)
1 (cid:88) 1 (cid:88) 1 (cid:88)
E [r ] = E l (x) = E [l (x)] ≤ E [l (x)] = E [r ].
t t t t t t t i t i
|X| |X| |X|
x∈X x∈X x∈X
Similarly, if X is infinite, by using the same argument and Fubini’s theorem, we get E [r ] ≤ E [r ].
t t t i
Therefore, for all cases the inequality E [r ] ≤ E [r ] holds. This implies that tˆ= t.
t t t i
From Lemma B.1 and B.3, we get Theorem 4.2.
18B.3. Proof of Theorem A.1
Proof. Let δ ∈ (0,1). For any t ≥ 1 and D , from the proof of Lemma 5.1 in Srinivas et al. (2010),
t−1
with probability at least 1−δ, the following holds for any x ∈ X:
1/2 1/2
lcb (x) ≡ µ (x)−β σ (x) ≤ f(x) ≤ µ (x)+β σ (x) ≡ ucb (x),
t−1,δ t−1 δ t−1 t−1 δ t−1 t−1,δ
where β = 2log(|X|/δ). Here, by using the same argument as in the proof of Theorem 4.1, the
δ
inequality l (x) ≤ a˜ (x) holds. Hence, the following holds with probability at least 1−δ:
t t−1,δ
r˜ = maxl (x) ≤ maxa˜ (x). (B.3)
t t t−1,δ
x∈X x∈X
Next, we consider the conditional distribution of r˜ given D . Note that this distribution does
t t−1
notdependonβ . LetF (·)beadistributionfunctionofr˜ givenD . Then, from(B.3), weobtain
δ t−1 t t−1
(cid:18) (cid:19)
F maxa˜ (x) ≥ 1−δ.
t−1 t−1,δ
x∈X
Therefore, by taking the generalized inverse function for both sides, we get
F−1(1−δ) ≤ maxa˜ (x).
t−1 t−1,δ
x∈X
Here, if δ follows the uniform distribution on the interval (0,1), 1−δ follows the same distribution.
Furthermore, since the distribution of F−1(1−δ) is equal to the conditional distribution of r˜ given
t−1 t
D , we have
t−1
(cid:20) (cid:21)
E [r˜] ≤ E maxa˜ (x) .
t t δ t−1,δ
x∈X
Moreover, noting that 2log(|X|/δ) and β follow the same distribution, we obtain
t
E [r˜] ≤ E [a˜ (x )].
t t βt t−1 t
In addition, from the definition of a˜ (x), the following inequality holds:
t−1
1/2
a˜ (x ) ≤ β σ (x ).
t−1 t t t−1 t
Therefore, since E[β ] = 2+2log(|X|)), the following inequality holds:
t
(cid:34) t (cid:35) (cid:34) t (cid:35)
E[R˜ ] = E (cid:88) r˜ ≤ E (cid:88) β1/2 σ (x )
t i i i−1 i
i=1 i=1
 
(cid:32) t (cid:33)1/2(cid:32) t (cid:33)1/2
(cid:88) (cid:88)
≤ E  β i σ i2 −1(x i) 
i=1 i=1
(cid:118) (cid:118)
(cid:117) (cid:34) t (cid:35)(cid:117) (cid:34) t (cid:35)
(cid:117) (cid:88) (cid:117) (cid:88)
≤ (cid:116)E β (cid:116)E σ2 (x )
i i−1 i
i=1 i=1
(cid:118)
(cid:117) (cid:34) t (cid:35)
(cid:112) (cid:117) (cid:88)
≤ t(2+2log(|X|))(cid:116)E σ2 (x )
i−1 i
i=1
(cid:118)
(cid:117) (cid:34) (cid:35)
(cid:112) (cid:117) 2
≤ t(2+2log(|X|))(cid:116)E γ
log(1+σ−2 ) t
noise
(cid:113)
= C˜ tγ .
1 t
B.4. Proof of Theorem A.2
Proof. Theorem A.2 is proved by using the same argument as in the proof of Lemma B.1.
19B.5. Proof of Theorem A.3
Proof. Let x ∈ X. If x ∈ H∗∩H or x ∈ L∗∩L , the equality l (x) = 0 holds. Hence, the following
t t t
inequality holds:
l (x) ≤ l ([x] ) ≤ l ([x] )+|f(x)−f([x] )|.
t t t t t t
We consider the case where x ∈ H∗ and x ∈ L , that is, l (x) = f(x)−θ. Here, since x ∈ L , the
t t t
inequality µ ([x] ) < θ holds. This implies that [x] ∈ L . If [x] ∈ H∗, noting that l ([x] ) =
t−1 t t t t t t
f([x] )−θ we get
t
l (x) = f(x)−θ = f(x)−f([x] )+f([x] )−θ ≤ f([x] )−θ+|f(x)−f([x] )| = l ([x] )+|f(x)−f([x] )|.
t t t t t t t t
Similarly, if [x] ∈ L∗, noting that f([x] ) < θ and 0 ≤ l ([x] ) we obtain
t t t t
l (x) = f(x)−θ = f([x] )−θ+f(x)−f([x] ) ≤ 0+f(x)−f([x] ) ≤ l ([x] )+|f(x)−f([x] )|.
t t t t t t t
Next, we consider the case where x ∈ L∗ and x ∈ H , that is, l (x) = θ − f(x). Here, since
t t
x ∈ H , the inequality µ ([x] ) ≥ θholds. This implies that [x] ∈ H . If [x] ∈ L∗, noting that
t t−1 t t t t
l ([x] ) = θ−f([x] ), we have
t t t
l (x) = θ−f(x) = θ−f([x] )+f([x] )−f(x) ≤ l ([x] )+|f(x)−f([x] )|
t t t t t t
Similarly, if [x] ∈ H∗, noting that f([x] ) ≥ θ and 0 ≤ l ([x] ), we get
t t t t
l (x) = θ−f(x) = θ−f([x] )+f([x] )−f(x) ≤ 0+f([x] )−f(x) ≤ l ([x] )+|f(x)−f([x] )|.
t t t t t t t
Therefore, for all cases the following inequality holds:
l (x) ≤ l ([x] )+|f(x)−f([x] )|.
t t t t
(cid:12) (cid:12)
Here, let L = sup sup (cid:12) ∂f (cid:12). Then, the following holds:
max j∈[d] x∈X (cid:12)∂xj(cid:12)
dr
|f(x)−f([x] )| ≤ L ∥x−[x] ∥ ≤ L .
t max t 1 max
τ
t
Thus, noting that
dr
l (x) ≤ l ([x] )+L
t t t max
τ
t
we obtain
dr dr dr
r˜ = maxl (x) ≤ L +maxl ([x] ) ≡ L +maxl (x˜) ≡ L +rˇ.
t t max t t max t max t
x∈X τ t x∈X τ t x˜∈Xt τ t
In addition, from Lemma H.1 in Takeno et al. (2023), the following inequality holds:
(cid:112) √
E[L ] ≤ b( log(ad)+ π/2).
max
Hence, we get
(cid:20) (cid:21) (cid:112) √ (cid:112) √
dr b( log(ad)+ π/2) b( log(ad)+ π/2)
E L max ≤ dr = (cid:112) √ dr
τ t τ t ⌈bdrt2( log(ad)+ π/2)⌉
(cid:112) √
b( log(ad)+ π/2) 1
≤ bdrt2((cid:112) log(ad)+√ π/2)dr = t2.
Therefore, the following inequality holds:
(cid:34) (cid:88)t (cid:35) (cid:88)t 1 (cid:34) (cid:88)t (cid:35) π2 (cid:34) (cid:88)t (cid:35)
E[R˜ ] = E r˜ ≤ +E rˇ ≤ +E rˇ .
t i i2 i 6 i
i=1 i=1 i=1 i=1
Here, rˇ is the maximum value of the loss l (x˜) restricted on X , and since X is a finite set, by
i i i i
replacing X with X in the proof of Theorem A.1 and performing the same proof, we obtain E [rˇ] ≤
i i i
20E [max aˇ (x˜)]. Furthermore, since the next point to be evaluated is selected from X, the
δ x˜∈Xt i−1
following inequality holds:
E [rˇ] ≤ E [maxaˇ (x˜)] ≤ E [maxaˇ (x)].
i i δ i−1 δ i−1
x˜∈Xt x∈X
Therefore, we have
(cid:34) t (cid:35) (cid:34) t (cid:35)
E (cid:88) rˇ ≤ E (cid:88) β1/2 σ (x )
i i i−1 i
i=1 i=1
 
(cid:32) t (cid:33)1/2(cid:32) t (cid:33)1/2
(cid:88) (cid:88)
≤ E  β i σ i2 −1(x i) 
i=1 i=1
(cid:118) (cid:118)
(cid:117) (cid:34) t (cid:35)(cid:117) (cid:34) t (cid:35)
(cid:117) (cid:88) (cid:117) (cid:88)
≤ (cid:116)E β (cid:116)E σ2 (x )
i i−1 i
i=1 i=1
(cid:118)
(cid:117) (cid:34) t (cid:35)
(cid:112) (cid:117) (cid:88)
≤ tE[β ](cid:116)E σ2 (x )
t i−1 i
i=1
≤ (cid:113) t(2+2dlog(⌈bdrt2((cid:112) log(ad)+√ π/2)⌉))(cid:113) E(cid:2) Cˇ γ (cid:3)
1 t
(cid:113)
= Cˇ tγ (2+s ).
1 t t
B.6. Proof of Theorem A.4
Proof. Theorem A.4 is proved by using the same argument as in the proof of Lemma B.1.
21