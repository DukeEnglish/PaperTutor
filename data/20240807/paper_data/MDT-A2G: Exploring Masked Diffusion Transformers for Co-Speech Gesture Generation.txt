MDT-A2G: Exploring Masked Diffusion Transformers for
Co-Speech Gesture Generation
XiaofengMao‚àó ZhengkaiJiang‚àó QilinWang JiangningZhang
xfmao23@m.fudan.edu.cn zkjiang@ust.hk ChencanFu JiafuWu
FudanUniversity TencentYoutuLab qlwang22@m.fudan.edu.cn vtzhang@tencent.com
Shanghai,China Shanghai,China chencan.fu@zju.edu.cn jiafwu@tencent.com
FudanUniversity TencentYoutuLab
ZhejiangUniversity Shanghai,China
China
YabiaoWang ChengjieWang WeiLi MingminChi‚Ä†
caseywang@tencent.com jasoncjwang@tencent.com liwei.yxgh@vivo.com mmchi@fudan.edu.cn
ZhejiangUniversity TencentYoutuLab VivoCommunication FudanUniversity
TencentYoutuLab Shanghai,China TechnologyCo.Ltd Shanghai,China
China Shanghai,China
Abstract diffusiontransformersandaninferencespeedthatis5.7√óthanthe
RecentadvancementsinthefieldofDiffusionTransformershave standarddiffusionmodel.OurcodeisavailableatMDT-A2G.
substantiallyimprovedthegenerationofhigh-quality2Dimages,
3Dvideos,and3Dshapes.However,theeffectivenessoftheTrans- CCSConcepts
formerarchitectureinthedomainofco-speechgesturegeneration ‚Ä¢Human-centeredcomputing‚ÜíHumancomputerinterac-
remainsrelativelyunexplored,aspriormethodologieshavepre- tion(HCI);‚Ä¢Computingmethodologies‚ÜíMotionprocessing.
dominantlyemployedtheConvolutionalNeuralNetwork(CNNs)
orsimpleafewtransformerlayers.Inanattempttobridgethis Keywords
researchgap,weintroduceanovelMaskedDiffusionTransformer
GestureGeneration,MotionProcessing,Data-DrivenAnimation,
forco-speechgesturegeneration,referredtoasMDT-A2G,which
MaskedDiffusionTransformer
directlyimplementsthedenoisingprocessongesturesequences.To
enhancethecontextualreasoningcapabilityoftemporallyaligned ACMReferenceFormat:
XiaofengMao,ZhengkaiJiang,QilinWang,ChencanFu,JiangningZhang,Ji-
speech-drivengestures,weincorporateanovelMaskedDiffusion
afuWu,YabiaoWang,ChengjieWang,WeiLi,andMingminChi.2024.MDT-
Transformer.Thismodelemploysamaskmodelingschemespecif-
A2G:ExploringMaskedDiffusionTransformersforCo-SpeechGesture
icallydesignedtostrengthentemporalrelationlearningamong
Generation.InProceedingsofthe32ndACMInternationalConferenceonMul-
sequencegestures,therebyexpeditingthelearningprocessand
timedia(MM‚Äô24),October28‚ÄìNovember1,2024,Melbourne,VIC,Australia.
leadingtocoherentandrealisticmotions.Apartfromaudio,Our ACM,NewYork,NY,USA,9pages.https://doi.org/10.1145/3664647.3680684
MDT-A2Gmodelalsointegratesmulti-modalinformation,encom-
passingtext,emotion,andidentity.Furthermore,weproposean
1 Introduction
efficientinferencestrategythatdiminishesthedenoisingcomputa-
Theobjectiveofco-speechgesturegenerationistosynthesizehu-
tionbyleveragingpreviouslycalculatedresults,therebyachieving
mangesturesfromaudioandothermodalities.Itssignificanceis
aspeedupwithnegligibleperformancedegradation.Experimental
escalatinginthedevelopmentofvirtualavatarsandinteractive
resultsdemonstratethatMDT-A2Gexcelsingesturegeneration,
technologies.Ineverydaylife,thereisasubstantialdemandfor
boastingalearningspeedthatisover6√ófasterthantraditional
theefficientproductionofhigh-qualityanddiversehumangesture
animations.Consequently,theproductionofhigh-qualityanddi-
‚àóBothauthorscontributedequallytothisresearch. versegesturesatanaffordablecostplaysapivotalroleinpractical
‚Ä†Correspondingauthor(ThisworkwassupportedbyNaturalScienceFoundationof
applications.
Chinaundercontract62171139).
WiththerapiddevelopmentofgenerativemodelssuchasVari-
ationalAutoencoders(VAEs),GenerativeAdversarialNetworks
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor (GANs),andthemorerecentDiffusionModels(DMs),numerous
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation works[2,3,6,14,16,21‚Äì24,31,34‚Äì38,40,41]haveproposedutiliz-
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe ingthesepowerfulfoundationalmodelsforthetaskofco-speech
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or
gesturegeneration.Althoughtheseworkshaveachievedgoodgen-
republish,topostonserversortoredistributetolists,requirespriorspecificpermission
and/orafee.Requestpermissionsfrompermissions@acm.org. erativequality,wefindthefollowinglimitationsinpracticalappli-
MM‚Äô24,October28-November1,2024,Melbourne,Australia. cations:1)Poordiversity.ForVAE-basedmethods[15,19,38],the
¬©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. goalistolearnameaningfullatentspacewheresimilarinputsare
ACMISBN979-8-4007-0686-8/24/10
https://doi.org/10.1145/3664647.3680684 mappedclosetogether.However,ifthemodelreliestoomuchonthe
4202
guA
6
]VC.sc[
1v21330.8042:viXraMM‚Äô24,October28-November1,2024,Melbourne,Australia. XiaofengMaoetal.
fullbody,outperformingexistingco-speechgenerationmethods.
Themaincontributionscanbesummarizedasfollows:
‚Ä¢ WeproposeMDT-A2G,amaskeddiffusiontransformerframe-
workdesignedforaudio-to-gesturetask,enhancingsemantic
understandingofdiversemodalitiesandhasteningtraining
convergencethatis6√ófaster.
‚Ä¢ Weintroduceanovelsamplingaccelerationtechniquethat
significantlyreducestheinferencetime,achievingaspeedup
~6√ó
~10√ó of5.7√ófastercomparedtothestandarddiffusionmodel.
‚Ä¢ Extensiveexperimentsdemonstratethatourproposedap-
proachachievesstate-of-the-artperformancebothqualita-
tivelyandquantitatively.
2 RelatedWork
Figure1:ComparisonbetweenDSG+[37]andourMDT-A2G- 2.1 Co-speechGestureGeneration
Bwithrespecttotrainingsteps/timesonasingleA100GPU. Thefieldofco-speechgesturegeneration,whichconcentrateson
ComparedtoDSG+,MDT-A2G-Bexhibitsafastertraining theproductionofgesturesthatsynchronizewithspeechaudio,has
convergencespeedandsuperiorperformance,demonstrating witnessedtheadventofnumerouslearning-basedmethodologies.
theeffectivenessofproposedmethod. Amongthese,thehierarchicalapproachproposedbyLiuetal.[24]is
significant,asitconsidersbothspeechsemanticsandthestructure
decoder‚Äôscapacitytoreconstructthedatafromthelatentvariables, ofhumangestures.Yoonetal.[39]havealsomadesubstantialcon-
itmightignorethelatentvariables,leadingtoaphenomenonknown tributionsbyutilizingarecurrentneuralnetworkandmulti-modal
asposteriorcollapse.Thisresultsinalackofdiversityinthegener- contexttodeviseatranslation-basedapproach.Liuetal.further
atedsamples.ForGAN-basedmethods[11,23],traininginstability enrichedthefieldbyintroducingtheBEATdatasetandtheCas-
andmodecollapseissuesarecommon,leadingtothegeneratorpro- cadedMotionNetwork(CaMN)[23].Othernotablecontributions
ducingalimitedvarietyofhumangestures,orevenidenticalones. includeEMAGE[22],amethodthatintegratesMaskedGesture
2)Inefficiency.DM-basedmethods[2,3,6,16,31,34,36,37,40] ReconstructionandAudio-ConditionedGestureGeneration,and
requiremultipleiterationsduringthegenerationprocess.Eachit- TalkSHOW[38]byYietal.,whichemploysanautoencoderand
erationinvolvesaforwardpassthroughthemodel,resultingin aVQ-VAEforthegenerationoffaceandbodymotions.Inrecent
typicallylonginferencetime.Besides,currentDMsappliedinA2G years,diffusionmodels,celebratedfortheircapacitytomodelin-
domainoftenstruggletoeffectivelylearnthesemanticrelationships tricatedatadistributionsandperformmany-to-manymappings,
betweenfeaturesofdifferentmodalities,leadingtoslowtraining havebeenincreasinglyharnessedforgesturesynthesis.Anumber
convergence. ofstudies[1,14,31,35,36,40,41]havefocusedonthegeneration
Toaddressabovechallenges,weproposeMDT-A2G,aMasked ofco-speechgesturesusingdiffusionmodels,introducingvarious
DiffusionTransformerframeworkdesignedforAudio-to-Gesture strategiestoaugmentmotiongenerationandstylecontrol.Despite
task.Sepecifically,weleveragethecontextualreasoningcapability theseadvancements,existingworkscontinuetostrugglewiththe
ofmaskmodelingandcombineitwithDMstoensurethequality challengeofharmonizingdiversity,authenticity,andspeedinthe
anddiversityofco-speechgesturesynthesis.Duringtraining,the generationofgesturesfromspeech.
maskedmodelingdenoisingnetworktakesmulti-modalconditions
andpartiallymaskednoisyfeaturesasinputandoutputsoriginal 2.2 DiffusionTransformers
gestures.AsshowninFigure1,benefitedbythemaskedmodeling‚Äôs DiffusionTransformershavemadesignificantadvancementsin
strongsemanticlearningability,ourMDT-A2Gcangeneratehuman thegenerationofhigh-qualityimages[4,29],videos[25],and3D
gesturesthatarebothrealisticanddiverse.Theconvergenceofthe models[27].DiT[29]isamongthepioneeringworkstoexplore
learningprocessisalsoaccelerated.Tothebestofourknowledge, transformer-basedarchitecturesinthefieldofimagegeneration.
thisisthefirstattemptofmaskmodelingDMsinA2Gtask.Wealso Itintroducesinnovativedesignstotrainlatentdiffusionmodels
proposetheincorporationofasimple-yet-effectivemulti-modal byreplacingthecommonlyusedU-Netwithatransformerback-
featurefusionmoduleintoexistingarchitecture.Thismoduleis bone.PixArt-ùõº[4]proposestheincorporationofcross-attention
carefullydesignedforfusingvariousmodalities,simplifyingthe modulesintotheDiffusionTransformer(DiT)toinjecttextcon-
complexfeatureprocessingandfusionmechanism.Toimprovethe ditions,therebyachievingphotorealistictext-to-imagesynthesis.
efficiencyofinference,weintroduceanefficientinferencestrategy DiT-3D[27]explorestheuseofplaindiffusiontransformersfor
duringsamplingwithoutcompromisingthequalityanddiversity 3Dgeneration,whichdirectlyoperatethedenoisingprocesson
ofthegeneratedgestures. voxelizedpointcloudsinsteadoflatentspaces.TheMaskedDiffu-
Extensivequalitativeandquantitativeexperimentsdemonstrate sionTransformer(MDT)[9]introducesamaskedlatentmodeling
thatourproposedMDT-A2Gcanproducehigh-quality,diverse scheme,enhancingtheabilitytolearncontextualrelationsamong
andtemporally-coherenthumangesturesinahighefficiency. semanticparts.LiketheMaskedAutoencoder(MAE)[12],MDT
Weachievestate-of-the-artperformanceregardlessofupperand alsoadoptsanasymmetrictransformertopredictmaskedtokensMDT-A2G MM‚Äô24,October28-November1,2024,Melbourne,Australia.
fromunmaskedoneswhilesimultaneouslyundergoingthediffu- Conversely,thereversedenosingphaseisdesignedtorevertthe
siongenerationprocess.Asaresult,itachievessuperiorimage noise-infuseddatabacktoitsoriginalstructuredform,effectively
synthesisperformancewithastate-of-the-art(SOTA)FrechetIn- reconstructingthejointdistributionùëù ùúÉ(ùë• 0:ùëá).Thisphaseisdefined
ceptionDistance(FID)scoreontheImageNetdatasetandfaster by:
learningspeed.Inthispaper,weaimtoexploresuchamasked ùëá
(cid:214)
schemeinthedomainofco-speechgesturegeneration.OurMDT- ùëù ùúÉ(ùíô0:ùëá)=ùëù ùúÉ(ùíôùëá) ùëù ùúÉ(ùíôùë°‚àí1|ùíôùë°),
(2)
A2Gdirectlyoperatesthedenoisingprocessonthegestureand ùë°=1
incorporatesmulti-modalinformation,includingspeech,emotion, ùëù ùúÉ(ùíôùë°‚àí1|ùíôùë°)=N(ùíôùë°‚àí1;ùúá ùúÉ(ùíôùë°,ùë°),Œ£ ùúÉ(ùíôùë°,ùë°)).
andidentity,toalignwiththegesture.
Here,themodelassumesaconstanttime-dependentvariance
Œ£ ùúÉ(ùíôùë°,ùë°)=ùõΩ ùë°I.AgenerativemodelGùúÉ isthenformulatedtoesti-
2.3 MaskModeling matethemeanoftheGaussiandistribution.Forscenariosrequiring
ThemaskedschemefirstdemonstrateditseffectivenessinNatu- conditional generation, the conditional variable c is seamlessly
ralLanguageProcessing(NLP),withBERT-basedmodels[8,17] incorporatedintothemodel‚Äôsarchitecture.
enhancingtheperformanceofwordembeddingsthroughmasked Wefollow[37,44,49]topredictthesignalitselfinsteadofpre-
languagemodelingandtransformerarchitecture.Subsequently,the dictingùúñ ùúÉ(ùë• ùë°,ùë°)[18].Thenetworkùê∑learnsparametersùúÉ basedon
MaskedAutoencoder(MAE)[12]extendedthemaskingstrategy theinputnoiseùë• ùë°,noisingstepùë° andconditionsùëêtoreconstruct
intothefieldofcomputervisionanddevelopedanasymmetric theoriginalsignalùë•0as:
encoder-decoder architecture with a high mask ratio. This con-
ceptofmaskedrepresentationlearninghasbeenemployedinother
ùë•ÀÜ 0=ùê∑(ùë• ùë°,ùë°,ùëê) (3)
modalitiesaswell,suchasvideo[32],audio[13],andtimeseries[7].
4 Methodology
Mostrelatedtoourwork,EMAGE[22]proposesatemporaltrans-
formertolearnrobustmotionrepresentationforgesturegeneration. 4.1 ProblemFormulation
Incontrast,weresorttomaskingstrategytostrengthentemporal Thetaskofco-speechgesturegenerationcanbeformulatedasa
relationlearningsequencegesture,resultingfastertrainingspeed sequence-to-sequencelearningproblem.Givenaninputsequenceof
andmorerealisticgesturegeneration. speechfeaturesandothermodalconditions,thegoalistogenerate
acorrespondingsequenceofhumangestures.Themodelneedsto
3 Preliminaries considerthetemporalalignmentbetweenthespeechandgesture
sequences,aswellastheincorporationofmulti-modalinformation
3.1 GestureFormat
(e.g.,emotion,speakerIDandtexttranscript)togeneratemore
WeemploytheBEATdataset[23],recognizedforbeingthemost
realisticanddiversegestures.
comprehensivemotioncapturedatasetintermsofdurationandthe
diversityofmodalitiesitcovers.TheBEATdatasetstoresmotion
4.2 OverallFramework
capture data in the BVH file format, articulating motion repre-
WeproposeanovelframeworknamedMDT-A2Gforgenerating
sentationthroughEulerangles.FollowingDSG+[37],wefavor
humangesturesfrommulti-modalconditions.Theoveralltraining
using rotation matrices over Euler angles to represent joint ro-
andsamplingpipelineisillustratedinFigure2.Specifically,gestures
tations.Weleverage75jointsincluding27bodyjoints,48hand
aregeneratedbasedonthenoisygestureùë• ,audioùë• ,textùë• ,ID
jointsforwhole-bodygesturegenerationandchooseasubsetof ùë° ùëé ùë°ùë•ùë°
ùë• ,emotionùë• ,andtimestepsùë°.Wefirstobtaintheinputnoisy
14upper-bodyjoints,alongwiththe48handjoints,forgenerating ùë† ùëí
gestureùë• andspecificmulti-modalcondition.Then,wefusethe
upper-bodygestures. ùë°
multi-modalfeaturetogetùë• asfollows:
ùëìùë¢ùë†ùëí
3.2 DiffusionModel ùë°ÀÜ=ùëÄùêøùëÉ(ùê∏ ùë°(ùë°))),
Weconsiderstartingthedenoisingprocesswithpurenoiseand ùë•ÀÜ ùë° =Concat[ùë• ùë°,ùê∏ ùë†(ùë• ùë†)+ùë°ÀÜ,ùê∏ ùëí(ùë• ùëí)+ùë°ÀÜ,ùê∏ ùë°ùë•ùë°(ùë• ùë°ùë•ùë°),ùê∏ ùëé(ùë• ùëé)], (4)
generatinggesturesusingadiffusionmodel.Weintroducethen
denoisingdiffusionprobabilisticmodels(DDPM).DDPMdefinesa
ùë•
ùëìùë¢ùë†ùëí
=ùê¥ùë°ùë°ùëíùëõùë°ùëñùëúùëõ(ùë•ÀÜ ùë°).
ùëá-stepforwardprocessandaùëá-stepreverseprocess.DDPMturns whereùê∏(‚Ä¢)representstheembeddinglayer.ùê¥ùë°ùë°ùëíùëõùë°ùëñùëúùëõ(‚Ä¢)utilizes
thepresentstateùë•0intothepreviousstateùë• ùë° bygraduallyadding cross-local attention used in DSG+ [37]. We mask a portion of
randomnoisethroughtheforwardprocess.Thediffusionprocess ùë• toobtainùë• ,whichisthenfedintothetransformerencoder
ùëìùë¢ùë†ùëí ùë¢
formulasisasfollows: tomapittoalatentrepresentation.Wethenconcatenatethelatent
representationwithmultiplesetsoflearnabletokensùë• andthen
ùëû(ùë• ùë°|ùë•0)=N(ùë• ùë°,‚àöÔ∏Å ùõº ùë°ùë•0,(1‚àíùõº ùë°)I), feedthemintoaself-attention(SA)blockforfeatureintùëô egration.
(1)
ùë• ùë° =‚àöÔ∏Å ùõº ùë°ùë•0+‚àöÔ∏Å1‚àíùõº ùë°ùúñ,ùúñ ‚àºN(0,I). ùë•ÀÜ ùë¢ =(1‚àíùëÄùê¥ùëÜùêæ)‚àóùë• ùë¢+ùëÄùê¥ùëÜùêæ‚àóùëÜùê¥(Concat[ùë• ùëô,ùê∏ùëõùëêùëúùëëùëíùëü(ùë• ùë¢)])
(5)
whereùë• isthenoisedimageattime-stepùë°,ùõº isthepredefined
ùë° ùë° Finally,wepassùë•ÀÜ throughthetransformerdecodertoobtainthe
ùë¢
scalefactor,andN representstheGaussiandistribution.
finalreconstructedfeatures:
It means that the original stateùë•0 is transformed intoùë•
ùë°
by
graduallyaddingGaussiannoise. ùë•ÀÜ 0=ùê∑ùëíùëêùëúùëëùëíùëü(ùë•ÀÜ ùë¢) (6)MM‚Äô24,October28-November1,2024,Melbourne,Australia. XiaofengMaoetal.
Composite Multi-modal Feature Extractor Masked Diffusion Transformers
Timesteps ùë° ep no cs oit dio inn g MLP Side-
Emotion üòÅ Linear ùë•‡∑ú interpolater
ùëí
MLP
One-hot Encoder Decoder
Speaker ID Embedding Linear ùë•‡∑ú ùë†
Text Transcript üìù Te Ex xt t rF ae ca tt ou rre ùë•‡∑ú ùë°ùë•ùë° ùë• ùëìùë¢ùë†ùëí Masked Transformer Transformer ùë•‡∑ú 0
Block x ùëÅ1 Block x ùëÅ2
ùë•‡∑ú
Audio üéß Audio Feature ùëé Predicted
Extractor gesture
Unmasked
Noisy Input
gesture Linear Input
ùë•ùë°~ùëÅ(ùëÇ,ùêº)
Scaling-aware Accelerated Sampling Process
Multi-modal Multi-modal
condition condition
Timesteps Denoising ùë•‡∑ú 0 D 0 i f f u Ts -1e ùë•‡∑ú ùëá‚àí1 Sc Dal e Nin n g So t- is ea i pw ngar e ùë•‡∑ú 0 0D i f Tfu -1s -e N ùë•‡∑ú ùëá‚Ä¶‚àí1‚àíùëÅ ùë•‡∑ú 0 Timesteps Denoising ùë•‡∑ú 0
ùë• ùëá ùë• ùëá
Eq. 8
Figure2:OverviewofMDT-A2G.Itprimarilyconsistsofthreecomponents:(1)CompositeMulti-modalFeatureExtractor,(2)
MaskedDiffusionTransformers,and(3)Scaling-awareAcceleratedSamplingProcess.Forthemulti-modalfeatureextractor,
weproposeaninnovativefeaturefusionstrategythatintegratestimeembeddingswithemotionandIDfeatures.Thesewillbe
furtherconcatenatedwithtext,audio,andgesturefeatures,resultinginacomprehensivefeaturerepresentation.Additionally,
wehavedesignedaMaskedDiffusionTransformerstructuretoexpeditetheconvergenceofthedenoisingnetwork,thereby
leadingtomorecoherentmotions.Finally,weintroduceascaling-awareacceleratedsamplingprocessbyutilizingdiffused
resultsfromprevioustimesteps,resultinginafastersamplingprocess.
4.3 CompositeMulti-modalFeatureExtraction normaldistributionN(0,I)duringtrainingandsamplingstages.
Previousmethods,suchasDSG+,handlefeaturesfromdifferent Then,ùë• ùë° isdimensionallyreducedthroughalinearlayer.During
modalitiesinacomplexmanner,repeatedlyconcatenatinginfor- thetrainingprocess,ùë• ùëí andùë• ùë† israndomlymaskedaccordingtoa
mationfromdifferentconditions.Webelievethatthisapproach Bernoullidistribution.Thetemporalembeddingùë• ùë° isthenadded
hindersthelearningoffeaturesfromdifferentmodalities.There- totheIDandemotionfeaturesafterpassingthroughanMLPlayer,
fore,wehavesimplifiedthefeaturefusionprocesscomparedto resultinginthefusedfeature.
formermethods.Ourfeaturefusionstrategyissimple-yet-effective, Multi-modalFeatureFusion.Theprocessoffeaturefusionin-
requiringonlyasingleconcatenationprocess. volves the concatenation of the aforementioned conditions and
Obtainingspecificconditions.Thegenerationofhumangestures noisy gestures along the feature dimension. This concatenated
isdependentonasetofspecificconditions,whichareextracted featureisthensubjectedtoacross-localattentionmodule[37],
as follows. For audioùë• , the raw input is resampled at 16 kHz, whichservestofusethedifferentmodalitieseffectively,allowing
ùëé
andasuiteoffeaturesincludingMelFrequencyCepstralCoeffi- themodeltounderstandthecomplexrelationshipsbetweenspeech,
cients(MFCCs),Melspectrogramfeatures,rhythmicfeatures,and text,style,emotion,andgestures.Thisisparticularlyimportant
onsetpointsarecomputed.Thesefeaturesarethenintegratedwith inco-speechgesturegeneration,asgesturesareoftencloselyre-
featuresextractedbythepre-trainedWavLM[5]modeltoform latedtothecontentandemotionofthespeech.Furthermore,the
thefinalaudiofeaturerepresentationùë•ÀÜ .Fortextùë• ,wordem- inclusionofemotionasanadditionalconditionallowsthemodel
ùëé ùë°ùë•ùë°
beddingsareobtainedusingthepre-trainedFastText[26]model. togenerategesturesthatarenotonlycontextuallyappropriatebut
Alinearlayeristhenusedtogeneratethefinaltextfeatureùë•ÀÜ . alsoemotionallycongruentwiththespeech.
ùë°ùë•ùë°
ForIDùë• ,theIDinformationisencodedasaone-hotvectorand
ùë†
transformedintoanIDembeddingùë•ÀÜ ùë† viaalinearlayer.Similarly, 4.4 MaskedDiffusionTransformers
theemotionalstateùë• isencodedasaone-hotvectorandtrans-
ùëí PreviousdiffusionmodelmethodsonA2Gtasksdonotpayatten-
formedintoanemotionfeatureùë•ÀÜ .Duringtraining,thenoisestep
ùëí tiontotheimportanceofmaskedmodeling,whichresultinslow
ùë° israndomlyselectedfromauniformdistributionrangingfrom
learningofthesemanticcorrelationsamongdifferentmodalfea-
1toùëá.Afterpositionencoding,ùë° isprocessedbyaMLPtoyield
turesandslowtrainingconvergence.Inspiredbythesuccessof
thetimefeatureùë°ÀÜ.Thenoisygestureùë• issampledfromastandard
ùë° maskingschemeonvideo[32],image[12],weutilizethisstrategy
etanetacnoc
noitnettAMDT-A2G MM‚Äô24,October28-November1,2024,Melbourne,Australia.
ongesturegenerationtask,leadingtomorecoherentmotiongener- ThisisfurthersupportedbythefindingsinFigure1,whichdemon-
ations.Wemasktheinputfeaturesandthenfeedthemintomultiple stratethatMDT-A2G-Bachievesfasterlearningprogressinterms
TransformerBlocks,whicharedividedintoencoders,decoders,and oftotaltraininghourscomparedtotheDSG+.
side-interpolators.Weadoptanasymmetricdesignthatallowsthe
encodertoonlytakeinunmaskedfeatures,reducingcomputational 4.6 Scaling-awareAcceleratedSamplingProcess
cost.
Previousdiffusionmodels[31,37]intheAudio-to-Gesture(A2G)
MaskingOperation.Givenafeaturematrixùë•
ùëìùë¢ùë†ùëí
‚ààRùëÅ√óùëë,where
domainhavenotfullyexploredthepossibilitiesofskipsampling
ùëësignifiesthenumberofchannelsandùëÅ denotesthenumberof
techniquessimilartoDDIM[30],whichcansignificantlyexpedite
tokens,werandomlyconcealtokensatarateùúå.Thisresultsin
thesamplingprocess.However,itsdirectapplicationisproblem-
a set of unmasked tokens, denoted asùë• ùë¢ ‚àà Rùëë√óùëÅÀÜ , where ùëÅÀÜ = aticasitexacerbatesthediffusionmodel‚Äôsexposurebias[28],the
(1‚àíùúå)ùëÅ.Totrackwhichtokensaremasked,wecreateabinary discrepancybetweenthesampledùë•ÀÜ andthetraining‚Äôsùë• .Assume
ùë° ùë°
maskùëÄùê¥ùëÜùêæ ‚àà RùëÅ withonesassignedtothemaskedpositions. thatùë•ÀÜùë° representstheoriginalgesturepredictedbythediffusion
0
Theunmaskedtokensùë• ùë¢ aresubsequentlyprocessed.Usingonly modelatstepùë°,tomitigatethisissue,weproposethefollowing
theunmaskedtokensùë• ùë¢ helpstoreducecomputationaloverhead. method:
Encoder.Fortheencoder,weonlymapthevisibleunmaskedfea-
turestolatentrepresentations,whichallowsustousefewercom- Theorem 4.1. Givenùë•ÀÜ 0ùë° and the sampledùë•ÀÜ ùë° at time t, we can
putations.Differentmaskingratesaffecttheexperimentalresults, computeùë•ÀÜ 0ùë°‚àí1 fortimet-1withoutrelyingonneuralnetworks.The
andwewilldiscussthemindetailintheexperimentalsection. subsequentformulaisemployedtodiminishtheexposurebias:
S p oi rrd io ge c ie- ni s an s let d fe er atp h to uro rla eut g de h ir m. tI h en nes sp s ii oir d ne e sd - .inb Ay t se[ r i9 p ll] o u, l st a th t roe ar tl , ea w dte h in nit ch Fre igrp e ur se rts eoe r 2n e ,t sa tht iti eo tn o sS iditi es s
-
ùë•ÀÜ 0ùë°‚àí1 =(ùë•ÀÜ ùë° ‚àí‚àöÔ∏Å1‚àíùõº ùë° ‚àó (ùë•ÀÜ ùë° ‚àí‚àö ùõº ùë†ùë° ùëêùë•ÀÜ ùëé0ùë° ùëô) ùëí/‚àö 1‚àíùõº ùë° )/‚àöÔ∏Å ùõº ùë°. (8)
interpolator,acompactnetwork,leveragestheencoder‚Äôsoutput Here,ùë†ùëêùëéùëôùëíisahyperparametergreaterthan1butcloseto1.
to estimate the masked tokens during training, working solely
TheproofisdetailedinAppendixA.Ourapproachincorporates
withtheunmaskedtokensùë• .Duringinference,alltokensùë• are
ùë¢ ùë¢
a1:Nacceleratedsamplingmethod,wherethecurrentstepisfully
processed,creatingadisparityinthenumberoftokens.Tomain-
processedandthenextNstepsareexpeditedthroughskipsampling.
tainconsistency,theside-interpolatorfillsmaskedpositionswitha
Itiscrucialtonotethatthisstrategyisdistinctfromdistillation-it
sharedlearnabletoken,generatinganinterpolatedembeddingùë• ,
ùêº
maintainsthenetwork‚Äôsperformanceduringfullsamplingwithout
computedasùë•
ùêº
=ùëÜùê¥(Concat[ùë• ùëô,ùê∏ùëõùëêùëúùëëùëíùëü(ùë• ùë¢)]).Amaskedshort-
necessitatingextratraining,anddoesnotintroduceadditionalcom-
cutconnectioncombinesùë• andtheinterpolatedembeddingùë•
ùë¢ ùêº
putationaloverhead,thusofferingastraightforwardandpractical
intoùë•ÀÜ
ùë¢
=(1‚àíùëÄùê¥ùëÜùêæ)¬∑ùë• ùë¢+ùëÄùê¥ùëÜùêæ¬∑ùë• ùêº.Withintheside-interpolator,
solution.
theunmaskedfeaturesareconcatenatedwithmultiplesetsoflearn-
ablelatentmasktokensùë• ,whichassistinrestoringtheoriginal
ùëô
featuresize,beforebeingfedintoaTransformerblockforfeature 4.7 Modelstructure
interaction. We have set different number of blocks for our MDT-A2G and
Decoder.Thedecoderreceivestheoutputfromtheside-interpolator, designedvariantswithvaryinghyperparameters(e.g.,numberof
which comprises the encoded latent representation along with layersandheads).WedenotethesevariantsofourmodelasMDT-
thelatentmasktokens.Ultimately,theseside-interpolatoroutputs A2G-XS(extrasmall),MDT-A2G-S(small),MDT-A2G-B(base),and
arepassedtothedecoderforreconstruction.Thesedecodersare MDT-A2G-L (large). Among them, the whole parameter size of
lessdeepcomparedtotheencoders.Inthetrainingstage,theen- MDT-A2G-BiscomparabletothatofDSG+.Thespecificnetwork
coder,decoder,andside-interpolatorworkintandemtoforecastthe configurationsaredetailedinAppendixE.
maskedfeaturesbasedontheunmaskedfeatures.Duringinference,
theside-interpolatorisnolongerutilized. 5 Experiments
5.1 Settings
4.5 TrainingObjective
Datasets.WeutilizetheBEAT[23]dataset,whichincludes120Hz
Totrainournetworks,weemploytheHuberlossasthegesture
motioncapturedataandaudiosfrom30speakers,featuring10-
lossLùëî:
minuteconversationsand1-minuteself-talks.Inadditiontomotion
andaudio,BEATincludesextrainformationsuchastext,identity,
Lùëî =ùê∏ ùë•0‚ààùëû(ùë•0|ùëê),ùë°‚àº[1,ùëá][HuberLoss(ùë•0‚àíùë•ÀÜ))]. (7) e sem leo ct tio 1-n hoan urn ao uta dt ii oo pn es r, sa pn ed akfa ec ri aa nl dex spp lr ie ts ts hi eo dn as t. aF so el tl 7o 0w %in fog r[ t3 ra3 i] n, iw nge
,
FollowingtheDDPMdenoisingprocess,wepredictthegesture 10%forvalidationand20%fortesting.
ùë•ÀÜ
0
ateachtimestepùë°,asdepictedinFigure2.Duringthetrain- Metrics.WeuseFrechetGestureDistance(FGD)[39]toevaluate
ingprocess,weprovideboththecompletefeatureùë• andthe thequalityofthegeneratedgestures,whichusesapretrainedges-
ùëìùë¢ùë†ùëí
maskedtokensùë• tothediffusionmodel.Thisapproachensures turefeatureextractorandcalculatestheFrechetdistancebetween
ùë¢
thatthemodeldoesnotoverlyfocusonreconstructingthemasked thedistributionofthefeaturesofrealandgeneratedgestures.Fol-
regionwhileneglectingthediffusiontraining.Theadditionalcosts lowing[23],weuseSemantic-RelevantGestureRecall(SRGR)[23]
associatedwithusingthemaskedlatentembeddingareminimal. andBeatAlignmentScore(BeatAlign)[20]toevaluatethediversityMM‚Äô24,October28-November1,2024,Melbourne,Australia. XiaofengMaoetal.
Table1:QuantitativecomparisonwithSOTAs.‚Üí:representsacloserproximitytothegroundtruth,indicatingbetterperfor-
mance.
Method FGD‚Üì Diversity‚Üí SRGR‚Üë BeatAlign‚Üí
GroundTruth - 395.20 - 0.894
CaMN[23] 79.24 295.84 0.216 0.828
UpperBody MDM[31] 94.91 305.73 0.209 0.831
DSG+[37] 87.91 421.74 0.212 0.842
MDT-A2G-B(Ours) 77.79 348.05 0.214 0.876
GroundTruth - 395.20 - 0.893
CaMN[23] 57.46 304.49 0.241 0.821
WholeBody MDM[31] 92.37 337.42 0.231 0.812
DSG+[37] 83.91 432.16 0.236 0.840
MDT-A2G-B(Ours) 46.42 381.33 0.240 0.871
N
M
a
C
gnitareleccA
oN
+ G S D 02:1
:R
A500 e0 la.1
c=
s
sru
O
52:1
:R
A500
e0 la.1
c=
s
T
G
Figure4:Comparisonwithdifferentaccelerationratio.ARis
Accelerationratio.AlluseMDT-A2G-B
Figure3:Qualitativecomparisonofwholemotiongenera-
methodsinthesamesetting.Specifically,weretrainCaMNtouse
tion.Refertothesupplementaryvideoforamoreintuitive
audio,text,emotionandspeakeridentityasconditionsandDSG+to
comparison.
useseedgestures,audioandspeakeridentityasconditionalinputs.
ForMDM,weretrainittotakeaudiofeaturesasinput.Addition
andsynchronyofthegeneratedgesture.SRGRenhancesthetradi- towholebodygesturegeneration,wealsoconductexperimentsto
tionalProbabilityofCorrectKeypoint(PCK)[19]byintroducing comparetheresultsofupperbodygesturegeneration.
semanticrelevancescoringasweights.BeatAlignisameasurement
betweenaudioandgesturebeatsthroughChamferDistance[20], 5.2 QuantitativeComparison
whichreflectsthesimilaritybetweenaudioandgesturebeats.We Table1presentsthequantitativeresults.Ourapproachsurpasses
alsouseDiversityScore[18]tomeasurethediversityofthegen- othermethodsingeneratingbothupperbodyandfull-bodymo-
eratedgestures.Thediversityscoreismeasuredoftheaverage tions.Whilemaskedmodelinggreatlyimprovestherealismofthe
featuredistance[18].Weusethepretrainedautoencoderusedfor generatedgestures,itdoesnotnecessarilyenhancetheDiversity
FGD. Score.It‚ÄôsimportanttonotethatahigherDiversityScoreisnot
ImplementationDetails.Themotiondataisdownsampledto alwaysinherentlybeneficial.Randomnoisemayachieveahigh
30Hzfromoriginal120Hzandweuse300framesfortraining. DiversityScore,butitisnotthetargetweaimfor.Additionally,
Following[2],weuserotationmatrixinsteadoftheoriginalEuler theinherentrandomnessinthesamplingofdiffusionmodelscan
angleforrotationrepresentation.Wetrainourmodelfor120ksteps negativelyaffecttheSRGRmetric,whichfocusesontheaccuracy
withabatchsizeof150anduseAdamWoptimizeratalearning ofgeneratedgestures.However,creatingrealisticmotionsdoes
rateof3e-5.AlltheexperimentsareconductedonasingleNVIDIA notalwaysrequirestrictconformitytoground-truthmovements.
A100GPU. Nonetheless,ourmethodeffectivelyreducestheadverseimpacts
ComparisonMethods.Wecompareourproposedmethodwith ontheSRGRmetric.Furthermore,wehavediscoveredthatem-
state-of-the-artgesturegenerationmethods,includingDSG+[37], ployingmaskedmodelingcanenhancetheBeatalignmentscore
CaMN[23]andMDM[31].Forafaircomparison,weretainallthese betweenaudioandgestures,therebyimprovingtheBeatalignmentMDT-A2G MM‚Äô24,October28-November1,2024,Melbourne,Australia.
Table2:Quantitativecomparisonwithdifferentacceleratingconfigurations.
Method Accelerationratio ùë†ùëêùëéùëôùëí AverageTime(s)‚Üì FGD‚Üí Diversity‚Üí SRGR‚Üë BeatAlign‚Üí
MDT-A2G-B NoAccelerating 1 8.711¬±1.495 46.42 381.33 0.240 0.871
MDT-A2G-B 1:20 1 1.984¬±0.214 59.93 334.88 0.240 0.872
MDT-A2G-B 1:20 1.0005 1.984¬±0.215 57.61 340.11 0.240 0.872
MDT-A2G-B 1:25 1 1.567¬±0.196 64.17 326.28 0.240 0.873
MDT-A2G-B 1:25 1.0005 1.587¬±0.204 61.81 331.12 0.240 0.873
Table3:Ablationstudyontheproposedcomponents. Table5:Ablationstudyondifferentwridermaskratios.
CMFE Masked FGDScore‚Üì Diversity‚Üí SRGR‚Üë BeatAlign‚Üí Method MaskRatio FGD‚Üì Diversity‚Üí
√ó √ó 83.91 432.16 0.236 0.840 MDT-A2G-B 0.1 49.54 376.77
‚úì √ó 54.62 387.68 0.238 0.863 MDT-A2G-B 0.2 48.38 382.46
√ó ‚úì 45.68 365.8 0.240 0.869 MDT-A2G-B 0.3 47.74 381.14
‚úì ‚úì 46.42 381.33 0.240 0.871
MDT-A2G-B 0.4 46.42 381.33
MDT-A2G-B 0.5 48.47 376.88
Table4:Ablationstudyonmodelscale. MDT-A2G-B 0.6 50.46 372.06
MDT-A2G-B 0.7 49.38 366.84
MDT-A2G-B 0.8 52.28 373.49
Name FGD‚Üì Diversity‚Üí SRGR‚Üë BeatAlign‚Üí
MDT-A2G-TS 95.85 335.71 0.241 0.872
MDT-A2G-S 64.39 392.84 0.240 0.870 Table6:Ablationstudyonfeatureprocess.
MDT-A2G-B 46.42 381.33 0.240 0.871
MDT-A2G-L 50.27 390.27 0.238 0.872 Structure Name FGD‚Üì Diversity‚Üí
MDT-A2G-B baseline 45.68 365.8
MDT-A2G-B AdaLN-Zero 82.57 319.23
score.Thisapproachcompelsthedenoisingnetworktolearnthe
correlatedfeaturesbetweenmulti-modalconditionsandgestures. MDT-A2G-B Ours 46.42 381.33
5.3 QualitativeResults
generation of diverse actions but also maintains a high degree
Inthevisualrepresentationofourexperimentaloutcomesillus-
ofrealismandaccuracy.Theactionsgeneratedbyourmodelare
tratedinFigure3,weclearlydelineatethedistinctionsbetween
bothvariedandrealistic,effectivelycapturingthesubtletiesand
the competing methodologies and our proposed approach. The
complexitiesofhumanmotionwithoutthegenerationofabnormal
CaMNmodelemploysastraightforwardmethodologythatmerely
gestures. More detailed qualitative comparison is shown in the
concatenatesmultimodalinputs.Whilethisresultsintechnically
appendix.
correctoutputs,theresultantactionsarenotablymonotonousand
lackdynamicvariation,ashighlightedwithintheblueboxofthe
figure.Thismethod,whilereliable,doesnotallowforthegenera- 5.4 AblationStudies
tionofnuancedorcomplexmotionpatterns,leadingtoavisually Effectivenessofdiffrentmodelscale.Theimpactofdifferent
staticpresentation.Ontheotherhand,DSG+leveragesanadvanced scalemodelsontheoutcomeshasbeenextensivelyvalidatedin
diffusionmodelcoupledwithself-attentionmechanisms,aimedat thefieldofimagegeneration.However,previousmethodshavenot
enhancingthediversityofgeneratedactions.Thismodeliscapa- exploredtheimpactofmodelscalabilityontheaudio-to-gesture
bleofproducingawidearrayofactions,therebyenrichingthe task. As we utilize Masked Diffusion Transformer as backbone,
dynamicqualityoftheoutputs.However,thiscomplexitycomes whichpossessesexcellentscalabilityproperties,wecomparethe
withitsownsetofchallenges,asthereisatendencytogenerate effectsofdifferentnetworksizesongesturegenerationhere.Table4
anatomicallyincorrectgesturesorunrealisticmotions,asindicated showsthequantitativeresults.
bytheexamplesintheredbox.WhiletheambitionofDSG+to Effectivenessofsamplingacceleration.Table2illustratesthe
achievehighvariabilityiscommendable,itsometimessacrifices impactofvariousaccelerationratios.Notably,atanacceleration
accuracyandrealisminitsgeneratedactions. ratioof1:20and1:25.Wedonotobservesignificantincreasesin
Incontrast,ourproposedmethodologysynthesizesthestrengths FGDScoreanddecreasesinDiversityScore.Meanwhile,theSRGR
ofboththeCaMNandDSG+approacheswhilemitigatingtheir remain relatively stable. Furthermore, we find that introducing
weaknesses.Byintegratingarefinedfeatureextractionmechanism theScaling-awareAcceleratedSamplingProcess(ùë†ùëêùëéùëôùëí >1)yield
witharobustdiffusionprocess,ourmodelnotonlyensuresthe betterresults.QualitativecomparisonisshowninFigure4.MM‚Äô24,October28-November1,2024,Melbourne,Australia. XiaofengMaoetal.
Table7:AblationstudywithMDT-A2G-B.
(a)EffectofDecoderDepth. (b)Effectofside-interpolater. (c)Effectofmaskedshortcut.
DecoderDepth FGD‚Üì Side-interpolater FGD‚Üì Maskedshortcut FGD‚Üì
2 46.42 √ó 54.08 √ó 49.77
4 46.69 ‚úì 46.42 ‚úì 46.42
(d)EffectofFull/UnmaskedFeatures. (e)Effectofwiderratio. (f)Effectofblocksinside-interpolator.
Imputtype FGD‚Üì Widerratio FGD‚Üì Number FGD‚Üì
Full+Unmasked 46.42 1 46.42
Full 55.08 ‚úì 46.42 2 46.85
Unmasked 96.63 √ó 47.75 3 55.28
Effectivenessofmulti-modalfusionandmaskmodeling.We side-interpolator.Increasingside-interpolatorscanhomogenizese-
conductaquantitativecomparisonwithandwithoutmulti-modal manticinformationbetweenlearnablemasksandunmaskedtokens,
fusionmoduleandmaskmodeling.Table3indicatesthatmask potentiallyreducingeffectivereconstructionofmaskedtokens,im-
modeling reduce the Diversity score, this may be attributed to pairingfeaturedifferentiation,anddegradingperformance.
theenhancedconsistencyamongdifferentgestures.However,the Effectivenessofmaskedshortcut.Table7cshowsourinvesti-
realismofthegeneratedgesturesissignificantlyimproved. gationintohowmaskedshortcutsimpactnetworkperformance.
EffectivenessofDifferentFeatureProcessing.Toevaluatethe Incorporatingmaskedshortcutsensuresthatthelearnablemask
impactofourproposedfeatureprocessingtechnique,weestablish parametersintheside-interpolatordonotinterferewithunmasked
abaselinebyexcludingthefeatureprocessingcomponentfromour tokens,facilitatingmoreeffectiveinformationflow.Thisdesignpre-
MDT-A2G.Wesubsequentlyincorporatedifferentfeatureprocess- ventsthedilutionofimportantfeaturesduringencoding,enhancing
ingmethodsforcomparison:AdaLN-Zeroisthemethodutilized themodel‚Äôsoverallintegrityandeffectiveness.
byDiT[29].Table6demonstratesthatourproposedmulti-modal EffectivenessofFull/UnmaskedFeatures.AsshowninTable7d,
fusionmoduleshowsthebestperformance.Notably,theuseof weanalyzedthescenariowhereonlyunmaskedfeaturesarefedinto
AdaLN-Zero can not enhance results, potentially due to the in- theMDT,ratherthanbothunmaskedfeaturesandallfeatures.This
troductionofexcessiveconditionalinformation.Detailednetwork practicecaninadvertentlycausethenetworktofocusonpredicting
architecturediagramsforthesemethodsareincludedintheappen- themaskedfeatures,potentiallyharmingperformance.Balancing
dix. themaskingprocessiscrucialtooptimizethenetwork‚Äôsabilityto
Effectivenessofdifferentwidermaskratio.Weusedifferent learnfrombothmaskedandunmaskedfeatures.
widermaskratios[10],where"wider"referstomaskingthefea-
6 Conclusion
tureswithintherange[ùúå,ùúå+0.2].Table5providesacomparisonof
WeintroducethenovelMDT-A2G,amaskeddiffusiontransformer
differentwidermaskratios.Notably,MDT-A2G-Bachievesoptimal
forco-speechgesturegeneration.Benefitedbytheuniquemask
performancewithamaskratioof40%.Ahighermaskratiomight
modelingscheme,MDT-A2Genhancesstrongertemporalandse-
limitthenetworktoreconstructingmaskedfeatures,reducingdiver-
manticrelationlearningamonggestures,therebyacceleratingthe
sity.Ourexperimentsshowthatusingvariedmaskratios,instead
learningprocess.Theintegrationofmulti-modalinformationhas
ofafixedone,improvesmodelrobustnessandadaptabilityacross
proveneffectiveingeneratingmorerealisticgestures.Besides,our
differentscenarios.AsshowninTable7e,removingawiderrange
efficient inference strategy has reduced denoising computation,
ofmaskratiosledtoanincreaseinFGDto47.75.Implementing
variablemaskratioshelpsthenetworklearntoreconstructand
resultingina5.7√óspeedupwithminimalperformancedegradation.
ExperimentalresultsconfirmMDT-A2G‚Äôssuperiorityingesture
generatefeaturesundervariedlevelsofinformationavailability,
generation,withsignificantlyfastertrainingandinferencespeeds
therebyimprovingitsabilitytohandlereal-worlddata.
thantraditionalcounterparts.Thisresearchhasbridgedasignif-
Effectivenessofdecoderdepth.AsshowninTable7a,optimal
icantgapinco-speechgesturegeneration,openingavenuesfor
resultsareobtainedwhenthenumberoftransformerblocksofde-
futurestudies.
coderissetto4.Thissuggeststhatusingadeeperdecoderenhances
theabilitytoassimilateandprocesscorrelatedinformationacross
differentfeatures,therebyimprovingoverallmodeleffectiveness. 7 LIMITATIONS
EffectivenessofSide-Interpolator.Table7billustratestheim- DiTshowcasesstrongscalabilityinimagegeneration,andweaimto
pactoftheside-interpolatoronFGDScore.Side-Interpolatorhelps exploresimilarscalabilityintheA2Gdomain.However,wehaven‚Äôt
maintain uniformity in feature processing, which is critical for observedthebenefitsofMDT-A2G‚Äôsscalabilityyet.Onepossible
theaccuratereconstructionoftheinputdata.Additionally,asde- reasonisinsufficienttrainingdata;asnetworkcomplexitygrows,
pictedinTable7f,weinvestigatetheeffectofdifferentnumber soshouldthedataset.Thesearehypothesesthatrequirefurther
ofside-interpolators.Optimalresultsareobtainedwithasingle validation.MDT-A2G MM‚Äô24,October28-November1,2024,Melbourne,Australia.
References HolisticCo-SpeechGestureGenerationviaMaskedAudioGestureModeling.
[1] SimonAlexanderson,RajmundNagy,JonasBeskow,andGustavEjeHenter.2023. arXivpreprintarXiv:2401.00374(2023).
Listen,denoise,action!audio-drivenmotionsynthesiswithdiffusionmodels. [23] HaiyangLiu,ZihaoZhu,NaoyaIwamoto,YichenPeng,ZhengqingLi,YouZhou,
ACMTransactionsonGraphics(TOG)42,4(2023),1‚Äì20. ElifBozkurt,andBoZheng.2022.Beat:Alarge-scalesemanticandemotional
[2] TenglongAo,ZeyiZhang,andLibinLiu.2023.Gesturediffuclip:Gesturediffusion multi-modaldatasetforconversationalgesturessynthesis.InEuropeanconference
modelwithcliplatents.ACMTransactionsonGraphics(TOG)42,4(2023),1‚Äì18.
[24]
o Xn iac nom Lip uu ,Qte ir av ni ys iio Wn. uS ,p Hr ain ng ge Zr, h6 o1 u2 ,‚Äì Y6 in30 g.
haoXu,RuiQian,XinyiLin,XiaoweiZhou,
[3] JunmingChen,YunfeiLiu,JiananWang,AilingZeng,YuLi,andQifengChen.
WayneWu,BoDai,andBoleiZhou.2022.LearningHierarchicalCross-Modal
2024.Diffsheg:Adiffusion-basedapproachforreal-timespeech-drivenholistic
[4]
3 Jud ne sx op nr ges Cs hio en n,a Jn ind cg he es nt gur Ye ug ,e Cn he or na gti jo ian n. Gar eX ,i Lv ep wr ee ip Yri an ot ,a Er nX zi ev: X24 ie0 ,1 Y.0 u4 e74 W7 u( ,2 Z02 h4 o) n.
g-
A Coss no fec ri ea nti co en of no Cr oC mo p-S up tee rec Vh isG ioe ns atu nr de PG ae ttn ee rnra Rti eo cn o. gI nn itP ior no .ce 1e 0d 4i 6n 2g ‚Äìs 1o 0f 4t 7h 2e .IEEE/CVF
[25] XinMa,YaohuiWang,GengyunJia,XinyuanChen,ZiweiLiu,Yuan-FangLi,
daoWang,JamesKwok,PingLuo,HuchuanLu,etal.2023. PixArt-ùõº:Fast
CunjianChen,andYuQiao.2024.Latte:Latentdiffusiontransformerforvideo
TrainingofDiffusionTransformerforPhotorealisticText-to-ImageSynthesis.
arXivpreprintarXiv:2310.00426(2023).
[26]
g Te on me ara st Mio in k. oa lorX v,iv Edp ore up ar rin dt Ga rr aX vi ev ,:2 P4 i0 o1 t. r03 B0 o4 j8 an(2 o0 w24 sk).
i,ChristianPuhrsch,andAr-
[5] SanyuanChen,ChengyiWang,ZhengyangChen,YuWu,ShujieLiu,Zhuo
mandJoulin.2018.AdvancesinPre-TrainingDistributedWordRepresentations.
Chen,JinyuLi,NaoyukiKanda,TakuyaYoshioka,XiongXiao,JianWu,Long
Zhou,ShuoRen,YanminQian,YaoQian,JianWu,MichaelZeng,andFuruWei.
InProceedingsoftheInternationalConferenceonLanguageResourcesandEvalua-
2021.WavLM:Large-ScaleSelf-SupervisedPre-trainingforFullStackSpeech
tion(LREC2018).
[27] ShentongMo,EnzeXie,RuihangChu,LanqingHong,MatthiasNiessner,and
Processing.(2021).arXiv:2110.13900[cs.CL]
ZhenguoLi.2024.Dit-3d:Exploringplaindiffusiontransformersfor3dshape
[6] KiranChhatre,RadekDanƒõƒçek,NikosAthanasiou,GiorgioBecherini,Christo-
pherPeters,MichaelJBlack,andTimoBolkart.2023. EmotionalSpeech-
generation.AdvancesinNeuralInformationProcessingSystems36(2024).
[28] MangNing,MingxiaoLi,JianlinSu,AlbertAliSalah,andItirOnalErtugrul.2023.
d arr Xiv ie vn :233 1D 2.B 04o 4d 6y 6A (2n 0i 2m 3)a .tionviaDisentangledLatentDiffusion. arXivpreprint (E 2l 0u 2c 3id ).atingtheexposurebiasindiffusionmodels.arXivpreprintarXiv:2308.15321
[7] JonathanCrabb√©andMihaelaVanDerSchaar.2021. Explainingtimeseries
[29] WilliamPeeblesandSainingXie.2023.Scalablediffusionmodelswithtransform-
p Pr Med Li Rct ,i 2o 1n 6s 6w ‚Äì2it 1h 7d 7y .namicmasks.InInternationalConferenceonMachineLearning.
ers.InProceedingsoftheIEEE/CVFInternationalConferenceonComputerVision.
4195‚Äì4205.
[8] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova.2018.Bert:
[30] JiamingSong,ChenlinMeng,andStefanoErmon.2020. Denoisingdiffusion
P prr ee p-t rr ia ni tn ain rXg io vf :1d 8e 1e 0p .0b 4i 8d 0ir 5e (c 2ti 0o 1n 8a ).ltransformersforlanguageunderstanding.arXiv
[31]
i Gm up ylic Tit evm eo t,de Sl is g. aa lrX Ri av ap br ,ep Br ri in at nar GX oiv rd:2 o0 n1 ,0.0 Y2 o5 n0 i2 S(2 h0 a2 fi0 r) ,.
Daniel Cohen-or, and
[9] ShanghuaGao,PanZhou,Ming-MingCheng,andShuichengYan.2023.Masked
d Iniff teu rs ni ao tn iot nra an ls Cf oo nrm fee rer nis cea os ntro Cn og mi pm ua teg re Vsy isn ioth ne .s 2i 3z 1er 6. 4I ‚Äìn 2P 3r 1o 7c 3ee .dingsoftheIEEE/CVF
[32]
A
I Zn
hm
te
ai
r
nt nH
a Tt
oa
i
noim
n ga
,lB
YC
ie br
o
im
n nf
ga en
r
Seo on. nc2
e
g0 ,o2 Jn2 u. eLeH Wau
r
anm
nin
ga ,gn
aR
nM
e
dpo
r
Lt ei io
s
men
n
intD
a
Wi tff
io
au
n
ns
s
gi .o .n 20M 22o .d Vel i. dI en omTh ae e:E Mle av se kn eth
d
[10] ShanghuaGao,PanZhou,Ming-MingCheng,andShuichengYan.2023.MDTv2:
autoencodersaredata-efficientlearnersforself-supervisedvideopre-training.
M arXas ik v:e 2d 30D 3.i 1ff 4u 3s 8i 9on (2T 02r 3a )n .sformerisaStrongImageSynthesizer. arXivpreprint
[33]
A Sed nva Wnc ae ns gi ,n Jn iae nu gra nl inin gfo Zr hm aa nt gio ,n Wp er io jc iaes nsi Cng aos ,y Xst ie am os bi3 n5 H(2 u0 ,2 M2), o1 r0 a0 n7 L8‚Äì i,1 X00 ia9 o3 z.
hong
[11] ShiryGinosar,AmirBar,GefenKohavi,CarolineChan,AndrewOwens,and
Ji,XinTan,MengtianLi,ZhifengXie,ChengjieWang,etal.2024.MMoFusion:
JitendraMalik.2019. Learningindividualstylesofconversationalgesture.In
P 34ro 9c 7e ‚Äìe 3d 5in 0g 6s .oftheIEEE/CVFConferenceonComputerVisionandPatternRecognition.
M aru Xl it vi- :2m 4o 03d .a 0l 2C 90o 5-S (p 2e 0e 2c 4h ).MotionGenerationwithDiffusionModel.arXivpreprint
[34] SichengYang,ZilinWang,ZhiyongWu,MingleiLi,ZhensongZhang,Qiaochu
[12] KaimingHe,XinleiChen,SainingXie,YanghaoLi,PiotrDoll√°r,andRossGirshick.
Huang,LeiHao,SongcenXu,XiaofeiWu,ChangpengYang,etal.2023.Unifiedges-
[13]
2 I PE0 oE2 -E Y2 / a. C oM V HFas uck o ae n nd f ge ,a re Hu nt uco ee Xn o unco ,cd Jo ue m nr ps cu ha t er ee nr gs vc i Lsa i il o ,a nb Al a le en xv d eis p ii ao Btn t ae el r ve na sr kr en ic ,e o Mr gs n. iciI t hn io aP n er l.o 1 Ac 6e u0e l0d i0 ,in ‚Äì Wg 16s o0 jo c0f i9 et . ch he t thu ere 3: 1A stu An Cifi Med Ing te ers ntu ar te ios ny an lt Ch oe nsi fs erm eno cd ee ol nfo Mr um ltu il mti ep dle ias .k 1e 0l 3e 3to ‚Äìn 1s 0. 4I 4n .Proceedingsof
[35] SichengYang,ZhiyongWu,MingleiLi,ZhensongZhang,LeiHao,Weihong
Galuba,FlorianMetze,andChristophFeichtenhofer.2022.Maskedautoencoders
Bao,MingCheng,andLongXiao.2023. DiffuseStyleGesture:stylizedaudio-
t 2h 8a 7t 20li .sten.AdvancesinNeuralInformationProcessingSystems35(2022),28708‚Äì
drivenco-speechgesturegenerationwithdiffusionmodels.InProceedingsofthe
[14] JihoonKim,JiseobKim,andSungjoonChoi.2023.Flame:Free-formlanguage-
Thirty-SecondInternationalJointConferenceonArtificialIntelligence.5860‚Äì5868.
[36] SichengYang,ZunnanXu,HaiweiXue,YongkangCheng,ShaoliHuang,Ming-
basedmotionsynthesis&editing.InProceedingsoftheAAAIConferenceon
mingGong,andZhiyongWu.2024.Freetalker:ControllableSpeechandText-
ArtificialIntelligence,Vol.37.8255‚Äì8263.
DrivenGestureGenerationBasedonDiffusionModelsforEnhancedSpeaker
[15] DiederikPKingmaandMaxWelling.2013. Auto-encodingvariationalbayes.
[16]
a Zr hX ifiv enp gre Kp ori nn gt ,a WrX ei iv P:1 i3 n1 g2 ,.6 J1 ia1 j4 iH(2 u01 a3 n) g.
,KexinZhao,andBryanCatanzaro.2020.
N Spa et eu cr hal an ne dss S. igIn naIC lPA rS oS ceP ss2 i0 n2 g4 (- IC2 A02 S4 SPIE ).EEInternationalConferenceonAcoustics,
[37] SichengYang,HaiweiXue,ZhensongZhang,MingleiLi,ZhiyongWu,Xiaofei
DiffWave:AVersatileDiffusionModelforAudioSynthesis.InInternational
Wu,SongcenXu,andZonghongDai.2023.TheDiffuseStyleGesture+entryto
[17]
C Zo hn ef ne zr hen oc ne gon LaL ne ,ar Mni in ng gdR aep Cre hs een nt ,a Sti eo bn as s.
tianGoodman,KevinGimpel,Piyush
theGENEAChallenge2023.InProceedingsofthe25thInternationalConference
Sharma,andRaduSoricut.2019.Albert:Alitebertforself-supervisedlearning
onMultimodalInteraction.
[38] HongweiYi,HualinLiang,YifeiLiu,QiongCao,YandongWen,TimoBolkart,
oflanguagerepresentations.arXivpreprintarXiv:1909.11942(2019).
DachengTao,andMichaelJBlack.2023.Generatingholistic3dhumanmotion
[18] Hsin-YingLee,XiaodongYang,Ming-YuLiu,Ting-ChunWang,Yu-DingLu,
M infi on rg m-H ats iu oa nn pY roa cn eg ss, ia nn gd syJa stn emKa su 3t 2z. (22 00 11 99 ). .Dancingtomusic.Advancesinneural
[39]
f
P
Yr oao
t
um
t ner
gs
n
wp Re oe
e
oc
c
Yh
og
o.
n
oIn
i nt
,iP
o
Br no o.c k4e Ce 6d
9
hi
‚Äì
an
4
,g
8
Js o0o o.f -Hth ae enIE gE LE e/C e,V MF iC nso unf Je ar ne gn ,c Je aeo yn eC oo nm Lp eeu ,te Jar eV hi os nio gn Ka in md
,
[19] JingLi,DiKang,WenjiePei,XuefeiZhe,YingZhang,ZhenyuHe,andLinchao
andGeehyukLee.2020.Speechgesturegenerationfromthetrimodalcontext
Bao.2021.Audio2gestures:Generatingdiversegesturesfromspeechaudiowith
conditionalvariationalautoencoders.InProceedingsoftheIEEE/CVFInternational
(o 2f 0t 2e 0x )t ,, 1a ‚Äìu 1d 6i .o,andspeakeridentity.ACMTransactionsonGraphics(TOG)39,6
ConferenceonComputerVision.11293‚Äì11302.
[40] MingyuanZhang,ZhongangCai,LiangPan,FangzhouHong,XinyingGuo,Lei
[20] RuilongLi,ShanYang,DavidARoss,andAngjooKanazawa.2021.Aichoreog-
Yang,andZiweiLiu.2024.Motiondiffuse:Text-drivenhumanmotiongenera-
[21]
r
I
HEa aEp ih
E
ye
/
aCr n:
V
gM
F
Lu
I in
usi
t
,c
e
Nrc
n
ao
a
on
t
yd
io
ai nt Ii ao wln aCe md
on
o3 tfd
oer
,d
e
Za
n
in
c
hec ae
o
ong Ze
C
hn
o
ue mr ,a Zpti huo etn
e nr
gw
V
qi
i
it
s
nh
i
goa
n
Li .s it
1
,+
3
Y+
4
o.
0
uI 1n Z‚ÄìP h1r
3
oo
4
uc
1
,e
2
Ee .d lii fn Bgs ozo kf uth rte
,
t Inio ten llw igi et nh cedi (ff 2u 02si 4o )n
.
model.IEEETransactionsonPatternAnalysisandMachine
[41] LingtingZhu,XianLiu,XuanyuLiu,RuiQian,ZiweiLiu,andLequanYu.2023.
andBoZheng.2022.DisCo:DisentangledImplicitContentandRhythmLearn-
i In ntg erf no ar tD ioi nv ae lrs Ce onC fo e- rS enp ce eec oh nG Me us lt tu imre es dS iay .nthesis.InProceedingsofthe30thACM T
1ce
0a
e
5m
d
4ii 4nn ‚Äìgg
1s
0d
o
5i fff 5tu
3h
.s eio IEn Em E/o Cd Ve Fls Cfo or nfa eu red nio ce-d or niv Cen omco p- us tp ee re Vc ih siog nes atu nr de Pg ae tn tee rr nat Rio ecn o. gI nn itP ir oo n-
.
[22] HaiyangLiu,ZihaoZhu,GiorgioBecherini,YichenPeng,MingyangSu,YouZhou,
NaoyaIwamoto,BoZheng,andMichaelJBlack.2023.EMAGE:TowardsUnified