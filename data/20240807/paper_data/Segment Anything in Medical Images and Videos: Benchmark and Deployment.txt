Segment Anything in Medical Images and Videos:
Benchmark and Deployment
Jun Ma1,2,5, Sumin Kim1∗, Feifei Li1∗, Mohammed Baharoon2, Reza
Asakereh1, Hongwei Lyu5, and Bo Wang1,2,3,4,5
1 Peter Munk Cardiac Centre, University Health Network, Toronto, Canada
2 Vector Institute for Artificial Intelligence, Toronto, Canada
3 Department of Computer Science, University of Toronto, Toronto, Canada
4 Department of Laboratory Medicine and Pathobiology, University of Toronto,
Toronto, Canada
5 AI Hub, University Health Network, Toronto, Canada
∗Equal contributions
Abstract. Recentadvancesinsegmentationfoundationmodelshaveen-
abledaccurateandefficientsegmentationacrossawiderangeofnatural
images and videos, but their utility to medical data remains unclear. In
this work, we first present a comprehensive benchmarking of the Seg-
mentAnythingModel2(SAM2)across11medicalimagemodalitiesand
videos and point out its strengths and weaknesses by comparing it to
SAM1 andMedSAM. Then, we develop a transfer learning pipeline and
demonstrate SAM2 can be quickly adapted to medical domain by fine-
tuning.Furthermore,weimplementSAM2asa3DslicerpluginandGra-
dioAPIforefficient3Dimageandvideosegmentation.Thecodehasbeen
made publicly available at https://github.com/bowang-lab/MedSAM.
Keywords: Segmentation·FoundationModel·MedicalImage·Video
· Multi-modality.
1 Introduction
Segmentation is one of the important fundamental tasks in medical image anal-
ysis, which is the prerequisite for many downstream tasks, such as early cancer
detection [7] and disease diagnosis [49]. During the past decade, there has been
a clear methodology trend from specialist segmentation models [13,43] to gen-
eralist segmentation models or segmentation foundation models [25][57]. Spe-
cialist models are usually developed for specific anatomical structures [32,34],
diseases [30,18,5] or medical imaging modalities [41,4,20], which can only seg-
mentthedatafromthesamedomainasthetrainingset.Incontrast,foundation
models are trained on large-scale and diverse datasets, which have strong capa-
bilities to segment a wide range of objects, image types and scenarios, and even
zero-shot generalization ability on images from unseen domains [6,50].
Segment Anything Model (SAM) [25] is the first foundation model in image
segmentation,whichwastrainedon256GPUswith1.1billionmaskannotations
4202
guA
6
]VI.ssee[
1v22330.8042:viXra2 J. Ma, S. Kim, F. Li, M. Baharoon, R. Ashakereh, H. Lyu, and B. Wang
from 11 million images. Although SAM supports commonly used interaction
prompts(e.g.,points,boundingboxes,andmasks)forflexiblesegmentationand
adoptionacrossabroadspectrumofimagesanddownstreamtasks[40],itfailed
to segment medical images [36][19] because the training data (SAM-1B) only
contained natural images. Medical images have a large domain gap to natural
images, which have unique challenges, such as low resolution and limited image
quality [46]. Nevertheless, SAM can be easily extended to the medical domain
by transfer learning [29].
MedSAM[29]hasdemonstratedthatfine-tuningSAMonlarge-scalemedical
imagescanachievesuperiorperformancefor2Dmedicalimagesegmentation,but
its ability in segmenting 3D medical images and videos remains limited because
it is inefficient to produce prompts for each image via an image-by-image seg-
mentationpipelineinrealpractice.Manyrecentstudieshaveexploredextending
SAM to volumetric image segmentation, such as adding 3D adapters [17,8] to
theimageencoder,directlyaugmentingimageencoder[48,12]to3D,Mixtureof
Experts(MoE)withgatingnetworktoselecttask-specificfinetunedmodels[47],
and text-guided 3D segmentation [54].
Fig.1.Datasetsandevaluationprotocol.WeevaluateSAM2onvarious2D&3Dmed-
ical images and videos. The 2D images and the bounding box prompts are directly
passed to SAM2 to generate segmentation results. The 3D images and video are ini-
tialized with a bounding box prompt on the middle slice and the first frame to get
the2Dmasks,respectively.Then,themodelpropagatesthe2Dmasktotheremaining
slices/frames.SAM2 In Medical Images and Videos 3
Recently, SAM2 [10] has further augmented SAM to promptable video seg-
mentation without sacrificing the image segmentation ability (Methods). The
model was trained on an unprecedented dataset with 50.9K videos and out-
performed existing work in established video object segmentation benchmarks.
Notably, it also achieved strong zero-shot performance in many other video and
image segmentation benchmarks across various distributions. However, it is not
clear how SAM2 performs on medical images, especially 3D medical images and
videos, since only one medical dataset (instrument segmentation in endoscopy
videos [3]) was evaluated in SAM2. Moreover, the official interface only sup-
ports evaluating short videos with limited data format, which cannot be used
for medical professionals to test their own medical data with SAM2.
In this work, we address the above issues by presenting a comprehensive
evaluation of SAM2 across ten medical modalities, including various 2D & 3D
images and videos. We also compare SAM2 with SAM16 and MedSAM to ob-
tain a holistic understanding of their advantages and disadvantages. Compared
toconcurrentworks[10,56,44],wedevelopatransferlearningpipelinetoquickly
adaptSAM2intomedicalimagesegmentationbyfine-tuning,andfurtherincor-
porate SAM2 into the 3D Slicer plugin [14], allowing users to easily use SAM2
to annotate various 3D medical data (e.g., CT, MR, and PET) that are not
supportedintheofficialSAM2interface.WealsoimplementGradio[1]interface
to support efficient medical video segmentation.
2 Results
2.1 Datasets and evaluation protocol
The benchmark dataset covers 11 commonly used medical image modalities, in-
cludingcomputedtomography(CT),magneticresonanceimaging(MRI),positron
emission tomography (PET), ultrasound (US), encoscopy, fundus, dermoscopy,
mammography,lightmicroscope,andopticalcoherencetomography(OCT)(Meth-
ods).SinceSAM2isageneralmodelforimageandvideosegmentation,weeval-
uated its ability on 2D images, 3D images, and videos. The 2D datasets contain
all the modalities while the 3D datasets include CT, MR, and PET scans. The
video datasets are made of US and endoscopy videos.
In addition to the recent SAM2 [39], we also compare it with its predecessor
SAM [25] and the general medical image segmentation model (MedSAM [29]),
allowingacomprehensiveunderstandingoftheirperformances.SAMandSAM2
have three and four different model sizes, respectively, and all of them are eval-
uated in the experiments. Similar to MedSAM, we still advocate the bounding
box prompt because it is not only efficient without trial and error but also has
fewer ambiguities compared to the point prompt. For 2D images, we pass both
imagesandboundingboxpromptstothemodeltogetthecorrespondingmasks.
6 Werefertothefirstsegmentationanythingmodel[25]asSAM1,aimingtodifferen-
tiate it from the recent SAM2 [39].4 J. Ma, S. Kim, F. Li, M. Baharoon, R. Ashakereh, H. Lyu, and B. Wang
For 3D images, we initialize the bounding box prompts in the middle slice fol-
lowedbycallingthemodelstogeneratethe2Dmasks.SinceSAMandMedSAM
only support 2D image segmentation, the remaining parts are segmented slice-
by-slicefrommiddletobottomandtop.Thesegmentationmasksonthecurrent
sliceareusedtosimulateboundingboxpromptsofthenextsliceinasequential
way. SAM2 supports video segmentation and 3D medical images can be cast as
videos where each image is one frame [55]. Thus, the middle slice segmentation
maskispropagatedtotheremainingimagesbythevideosegmentationfunction
inSAM2.Weoptforthemiddlesliceasthestartingframein3Dimagesegmen-
tation because it usually has the largest object size among all the slices in the
conventional axial view. For video segmentation, we initialize the bounding box
prompt on the first slice to get the initial mask followed by passing the mask
to the video segmentation model. Only SAM2 is evaluated for video segmenta-
tionbecausetheobjectlocationsvarygreatlyacrossdifferentframesandsimply
applying SAM or MedSAM in a frame-by-frame way does not work.
2.2 Evaluation results on 2D image segmentation
The segmentation results of 2D images across 11 modalities are presented in
Fig. 2 and Table 1. SAM2 obtained higher DSC scores than SAM1 in MR,
dermoscopy, and light microscopy images but lower scores in PET, and OCT
images. Their performance is comparable in CT, X-Ray, ultrasound, endoscopy,
fundus, and mammography images. In contrast, MedSAM consistently outper-
formedbothSAM1andSAM2innineofthe11modalities,exceptthePETand
light microscopy images because its training set did not contain these types of
images.
Table 1.Quantitativesegmentationresults(averageDSCscores)ofeightSAMmodel
variantson2Dimagesacross11modalities.SAM1-Base:93.7M;SAM1-Large:312.3M;
SAM1-Huge: 641.1M; SAM2-Tiny: 38.9M; SAM2-Small: 46.0M; SAM2-Base: 80.8M;
SAM2-Large: 224.4M; MedSAM: 93.7M.
Modality SAM1-BSAM1-LSAM1-HSAM2-TSAM2-SSAM2-BSAM2-LMedSAM
CT 0.8825 0.9001 0.9028 0.9058 0.9090 0.9242 0.9167 0.9572
MR 0.8620 0.8673 0.8602 0.8897 0.8867 0.8858 0.8863 0.9507
PET 0.8198 0.8151 0.8042 0.7874 0.7791 0.7877 0.7727 0.8160
Ultrasound 0.7749 0.7789 0.7703 0.7792 0.7614 0.7540 0.7873 0.9398
X-Ray 0.8095 0.8181 0.8065 0.7835 0.7851 0.7939 0.7822 0.8573
Dermoscopy 0.8683 0.8731 0.8706 0.8927 0.8706 0.8875 0.9068 0.9197
Endoscopy 0.9170 0.9328 0.9395 0.9370 0.9346 0.9338 0.9315 0.9673
Fundus 0.9119 0.9522 0.9495 0.9338 0.9475 0.9483 0.9465 0.9498
Mammography 0.6931 0.7485 0.7512 0.7495 0.7304 0.7197 0.7601 0.8320
OCT 0.7482 0.7148 0.7160 0.6414 0.6374 0.6498 0.6273 0.8166
LightMicroscope 0.8332 0.8246 0.8178 0.8431 0.8388 0.8188 0.8322 0.6873SAM2 In Medical Images and Videos 5
Fig.2. Dot and box plot of the DSC scores for 2D image segmentation on 11 modal-
ities. The plot shows descriptive statistics with the median value represented by the
horizontalsolidlinewithinthebox,thelowerandupperquartilesdelineatingthebor-
ders of the box and the vertical black lines indicating 1.5×IQR.
2.3 Evaluation results on 3D images
One of the major improvements of SAM2 is the video segmentation capability.
Fig.3andTable2showsthequantitativeresultsofSAM1,SAM2,andMedSAM
in 3D CT, MRI, and PET images. By formulating the 3D images as videos
and propagating the mask from the middle slice to the remaining slices, SAM2-
BasemodelachievedremarkableimprovementsoverSAM1andMedSAMonCT
and MR images that processed the 3D data slice-by-slice. However, all SAM1
modelsoutperformedSAM2inPETimagesbecausethemodeltendstogenerate
over-segmentaion errors in the middle slice and the error will propagate to the
remaining slices as shown in Fig. 3b.
2.4 Effects of mask initialization on 3D segmentation performance
Wefurtheranalyzedhowthedifferent2Dmaskinitializationsofthemiddleslice
affect the segmentation performance during the mask propagation in SAM2.
In particular, we initialized the middle slice 2D mask with MedSAM-generated
segmentationandtheground-truthmask,respectively.Table3showsthequanti-
tativesegmentationinthethree3Dmodalities.Comparedtothedefaultsettings
that used SAM2 to segment the middle slice, using MedSAM-generated mask
improvedthefinal3Dsegmentationacrossallthreemodalitieswithupto17.5%
and33.3%inDSCandNSDscores,respectively.Furthermore,usingtheground
truth of the middle slice as the initialization can bring larger accuracy gains
across the modalities and model sizes.6 J. Ma, S. Kim, F. Li, M. Baharoon, R. Ashakereh, H. Lyu, and B. Wang
Table 2. Quantitative segmentation results on 3D images.
CT MR PET
Models
DSC NSD DSC NSD DSC NSD
SAM1-B 0.6557 0.6492 0.6433 0.6575 0.76080.6393
SAM1-L 0.6982 0.6990 0.6486 0.6639 0.7411 0.6137
SAM1-H 0.6862 0.6848 0.6359 0.6496 0.7280 0.5877
SAM2-T 0.7168 0.6729 0.6919 0.6813 0.6305 0.3937
SAM2-S 0.7134 0.6679 0.6868 0.6733 0.5932 0.3444
SAM2-B 0.7199 0.6812 0.6758 0.6579 0.6331 0.3971
SAM2-L 0.7178 0.6795 0.6820 0.6673 0.6009 0.3400
MedSAM0.73760.74340.69980.7172 0.6600 0.5788
Table 3. Quantitative segmentation results on 3D images with three different initial-
izations for the middle slice. Default: Using the corresponding SAM2 to generate the
mask. MedSAM: Using MedSAM to generate the mask. GT: Using the ground-truth
mask of the middle slice.
Models CT MR PET
(Initialization) DSC NSD DSC NSD DSC NSD
SAM2-T (Default) 0.7168 0.6729 0.6919 0.6813 0.6305 0.3937
SAM2-T (MedSAM) 0.7790 0.7840 0.7600 0.7779 0.7661 0.6826
SAM2-T (GT) 0.8388 0.8386 0.81580.83310.82570.7730
SAM2-S (Default) 0.7134 0.6679 0.6868 0.6733 0.5932 0.3444
SAM2-S (MedSAM) 0.7830 0.7883 0.7537 0.7685 0.7682 0.6778
SAM2-S (GT) 0.8431 0.8443 0.8104 0.8245 0.8152 0.7570
SAM2-B (Default) 0.7199 0.6812 0.6758 0.6579 0.6331 0.3971
SAM2-B (MedSAM) 0.7875 0.7966 0.7487 0.7630 0.7656 0.6672
SAM2-B (GT) 0.8419 0.8453 0.8027 0.8039 0.8159 0.7551
SAM2-L (Default) 0.7178 0.6795 0.6820 0.6673 0.6009 0.3400
SAM2-L (MedSAM) 0.7881 0.7991 0.7552 0.7754 0.7594 0.6642
SAM2-L (GT) 0.85190.8634 0.8107 0.8263 0.8081 0.7511SAM2 In Medical Images and Videos 7
Fig.3. a, Dot and box plot of the DSC scores for 3D image segmentation. The plot
shows descriptive statistics with the median value represented by the horizontal solid
linewithinthebox,thelowerandupperquartilesdelineatingthebordersoftheboxand
the vertical black lines indicating 1.5×IQR. b, Visualized MR and PET segmentation
examples of the best-performing SAM2. The bounding box prompt is initialized on
the middle slicer and the generated mask is propagated to the top and bottom slices,
respectively.
2.5 Evaluation results on video segmentation
Next, we directly applied SAM2 for video segmentation where the first frame
was initialized with box prompts. Table 4 shows the heart and polyp segmen-
tation results of SAM2 in echocardiography [26] and endoscopy [37,21] videos,
respectively.SAM2-TachievedthebestperformancewithaDSCscoreof0.8537
forultrasoundvideoswhileSAM-BobtainedthehighestDSCscoreof0.8397for
endoscopy videos. We show some failure cases in Fig. 4 of the best-performing
model and it can be found that the model failed to segment the first frame or
generateover-segmentationerrorsduringthemaskpropagationwhentheobject
boundary is not clear or the images have low contrast.
Table 4. Quantitative segmentation results on ultrasound and endoscopy video
datasets.
SAM2-T SAM2-S SAM2-B SAM2-L
Modality
DSC NSD DSC NSD DSC NSD DSC NSD
Ultrasound0.85370.88240.74570.7602 0.8365 0.8654 0.83580.8659
Endoscopy 0.8263 0.8385 0.79540.80460.83970.85060.81210.82308 J. Ma, S. Kim, F. Li, M. Baharoon, R. Ashakereh, H. Lyu, and B. Wang
Fig.4. Visualized examples of the best video segmentation model. The model either
failedtosegmentthefirstframeorgeneratedover-segmentationerrorsduringthemask
propagation when the object boundary is not clear or the images have low contrast.
2.6 Transfer learning: a case study on adapting SAM2 to the
medical domain
Although SAM2 brings multiple improvements on top of SAM1, there is still
a need to enhance its segmentation capability in medical image segmentation.
Transfer learning has been demonstrated as an effective way to adapt SAM to
new domain [29,23]. However, SAM2 team has no plan to release the training
code. Thus, we developed a transfer learning pipeline to fine-tune SAM2 in
medical images.
WeusedSAM2-Tmodelasanexampleandconductedexperimentsonthe3D
abdominal organ segmentation dataset [33] (Methods). The model was trained
with2DCTslicesbuttheinferencewasconductedin3Dwithinitializationonly
onthemiddleslice,followedbyaslice-wisepropagationwiththetrainedmodels.
Table 5 shows the comparison between SAM2-T and the fine-tuned model. It
can be found that the fine-tuned model achieved remarkable improvements of
3.5−45.62% and 10.1−61.4% for DSC score and NSD scores, respectively.
2.7 Deployment: 3D Slicer Plugin and Gradio API for effective
medical data annotation
Numerous medical image segmentation models have been published and new
models are continuously increasing every month, but only few of them areSAM2 In Medical Images and Videos 9
Table 5. Quantitative segmentation results on transfer learning with SAM-T for ab-
dominal 3D organ segmentation in CT scans.
SAM2 SAM2 Fine-tuning Improvements
Organ
DSC NSD DSC NSD DSC NSD
Liver 0.58020.36050.9681 0.9127 38.79%55.22%
Right Kidney 0.90590.80980.9410 0.9108 3.51% 10.10%
Spleen 0.80400.65050.9601 0.9584 15.61%30.79%
Pancreas 0.16820.11380.4901 0.4732 32.19%35.95%
Aorta 0.18350.13380.6397 0.6512 45.62%51.73%
Inferior Vena Cava 0.14380.11320.3468 0.3961 20.31%28.30%
Right Adrenal Gland0.36490.41260.6509 0.8009 28.60%38.84%
Left Adrenal Gland 0.31950.36070.7095 0.8118 39.00%45.11%
Gallbladder 0.66560.54120.8058 0.8481 14.02%30.70%
Esophagus 0.33420.26140.7951 0.8718 46.09%61.04%
Stomach 0.43330.25550.8452 0.7742 41.20%51.88%
Left Kidney 0.86940.77000.9589 0.9247 8.95% 15.47%
adopted by medical professionals. One of the main barriers is the lack of a
user-friendly interface for medical professionals to access the model (without
any coding) and provide feedback. SAM2 has provided an online interface to
testthemodelbutonlyshortvideosareallowedtobeuploaded,anditalsodoes
not support most of the 3D medical data format.
In order to make SAM2 accessible to a wider audience, especially medical
professionals, we developed two interfaces for general 3D medical image and
video segmentation based on 3D Slicer [14] and Gradio [1], respectively. Fig. 5
showsanoverviewofthetwointerfaces.3DSlicersupportsmostofthecommon
medicaldataformatsandhasbuilt-intoolsforuserinteractions,suchasdrawing
boundingboxes,addingpoints,andrevisingmasks.OurSAM23DSlicerplugin
(Fig.5a)wasdesignedforgeneral3Dmedicalimagesegmentationwithbounding
box prompts. Users first specify the top and bottom slices of the segmentation
target and draw a bounding box prompt in the middle slice. Then, SAM2 au-
tomatically generates the 2D mask on the middle slice and propagates it to the
remaining slices. The Gradio API (Fig. 5a) allows users to upload videos with-
out time limitations. The initial frame can be any of the video frames and users
can draw bounding boxes, point prompts, or their combinations to segment the
target object. Then, SAM2 automaticallytracksand segments the objectacross
alltheframes.WealsomaketheAPIpubliclyavailablesothatuserscandeploy
it on their computing resources.
3 Discussion
Model Variants: SAM1 versus SAM2 When SAM2 came out, a straight-
forward question emerged: "Does SAM2 outperform SAM1 in medical image
segmentation?". The answer could be "Yes" for 2D MRI, dermoscopy, and light
microscopyimages.For3DCTandMRIscans,SAM2-Balsoachievedthehigh-10 J. Ma, S. Kim, F. Li, M. Baharoon, R. Ashakereh, H. Lyu, and B. Wang
Fig.5.SAM2deploymentformedicalimageandvideoannotation.a,Slicerpluginfor
3D medical image segmentation. b, Gradio API for video segmentation.SAM2 In Medical Images and Videos 11
est scores, which outperformed all SAM1 models by a large margin. However,
the answer could also be "No" for 2D OCT and PET images where SAM1 had
better performance. One can attribute the improvements of SAM2 to the fact
that it had advanced network architecture and was trained on both large-scale
image and video datasets. The performance drop on some modalities could be
due to the smaller model size compared to SAM1. Nevertheless, considering the
sophisticatedmodeldevelopmentprocess,understandingthereasonsforthisper-
formancegaprequiresaccesstothemodeltrainingcodeandconductingrigorous
ablation studies.
Scaling Laws: Model Size versus Performance Large models usually have
a greater capacity for better performance, but we found that segmentation per-
formance varies greatly across different model sizes on various medical image
segmentation tasks. For example, the largest SAM2 model, SAM2-L, did not
obtain the best performance in 8 of the 11 2D modalities and all the 3D modal-
ities and videos. Notably, for 3D PET, light microscope images, and ultrasound
videos,thesmallestSAM-Tmodelachievedthebestperformance.Thisindicates
that model size alone is not a determinant of success in medical image segmen-
tation tasks. Other factors, such as the specific characteristics of the training
datasetandtrainingprotocols,likelyplaycrucialrolesinachievingoptimalper-
formance.
General-purpose Model (SAM2) versus Adapted Model in Medical
Images (MedSAM) ThevideosegmentationcapabilityinSAM2significantly
enhancesperformancefor3Dmedicalimagesegmentation,butitremainsinferior
toMedSAMformost2Dmedicalimagemodalities.Thetransferlearningexper-
imentsdemonstratethatSAM2canbenefitfromtransferlearningtoimproveits
ability to segment medical images. Nevertheless, it should be noted that direct
fine-tuning can compromise its original versatile segmentation capabilities, as
evidenced by the 2D image segmentation results. Therefore, it is important to
considerabalancebetweenleveragingtransferlearningforimprovedtask-specific
performance and maintaining the general segmentation abilities of SAM2.
LimitationandFutureWork Thebenchmarkstudyhascoveredthecommon
medical image modalities, but it can be further enhanced by including more 3D
modalities,suchas3DultrasoundandOCTimages.Thereareseveralopenques-
tions worth exploring further. First, the video segmentation capability in SAM2
has broad applications in the medical domain, but the current model often fails
to segment targets without clear boundaries. This issue can be addressed by
transfer learning on medical datasets. Although fine-tuning video segmentation
model is much more complicated, our 2D image transfer learning pipeline offers
a good foundation for further development. Second, SAM2 only supports point,
box, and mask prompts. In contrast, text prompts offer greater flexibility for
complex structure segmentation [53,54,52]. Implementing natural language pro-
cessingcapabilitieswithinSAM2couldbridgethegapbetweencomplexmedical12 J. Ma, S. Kim, F. Li, M. Baharoon, R. Ashakereh, H. Lyu, and B. Wang
terminology and model understanding, facilitating a more intuitive and efficient
user experience. Another critical direction is to make SAM2 more lightweight.
SAM2-T is much smaller than SAM1-B but still requires large GPU RAM for
longvideosegmentation.Itisnecessarytofurtherreducethemodelsizeandim-
provetheinferenceefficiencywithoutcompromisingperformancefordeployment
in a broader range of clinical settings.
Inconclusion,wehaveconductedacomprehensiveevaluationofSAM2across
various medical image segmentation modalities. The performance varies signifi-
cantly on different model sizes and segmentation tasks, indicating new or larger
models are always better in terms of segmentation accuracy. Moreover, we de-
velop a fine-tuning pipeline and demonstrate its effectiveness in improving the
performancefor3Dmedicalimagesegmentation.Wealsoprovide3DSlicerplu-
gins and Gradio API to facilitate the deployment of SAM2 for medical image
and video segmentation. Although such engineering implementations are rarely
presented in deep learing-based medical image segmentation papers, we believe
they are important for the adoption in clinical practice. We have made all the
code publicly available for further development.
Methods
Data sources and preprocessing All the evaluated images and videos were
collectedfrompublicdatasets[9,32,33,38,16,15,45,31,37,21,26,22,51,24,2],which
were used in the validation set in the CVPR 2024 Medical Image Segmentation
onLaptopChallenge7.NoneofthemwasusedintheMedSAMtrainingset.The
CT images were preprocessed with intensity cutoff based on the typical window
level and window width. MR and PET images were clipped to the 0.5 and 99.5
percentiles of the non-zero region intensities. Then the intensity values were
normalizedto[0,255]viamin-maxnormalization.Fortheremainingmodalities,
the intensities are not changed. Finally, all the images are converted to npz
format for batch inference.
Key differences between SAM1 and SAM2 in methodology SAM2 [39]
isanaturalextensionofSAM1[25]forvideosegmentationwithaunifiedframe-
work for image and video segmentation tasks. Technically, there are two main
methodologyimprovements.First,theVisionTransformer(ViT)[11]isreplaced
withHiera[42],whichcanextractmulti-scalefeaturestoproducehigh-resolution
segmentationdetails.Fortheinteractivesegmentationofvideos,thememoryat-
tention module is used to condition the predictions of the current frame on the
informationuptothecurrenttimepoint.Thetransformerarchitecturetakesthe
image and prompt features from the current frame as well as the memories of
features and predictions from previous frames. Endowing the memory attention
module with the memory bank, SAM2 effectively extends the SAM’s prompt
encoder, image encoder, and mask decoder model architecture to enable robust
andconsistentsegmentationacrossvideoframesbyleveragingtemporalcontext.
7 https://www.codabench.org/competitions/1847/SAM2 In Medical Images and Videos 13
Fine-tunig protocol We fine-tuned the pre-trained SAM2-Tiny model on the
MICCAI FLARE22 abdomen CT training dataset [33] with a data splitting ra-
tio of 80% and 20% for training and validation. During fine-tuning, the prompt
encoder was frozen because it is domain agnostic, while the image encoder and
mask decoder were updated, allowing the model to adapt to the specific char-
acteristics of the CT scans. The input images were resized to [1024, 1024] and
normalized using z-score normalization. For such CT dataset, the preprocessed
CTscansliceimageswereduplicatedtocreate3-channelimagesbeforeinputting
themintothemodel.Theboundingboxpromptswereobtainedfromtheground
truth masks at the scale of [1024, 1024], and random perturbation of bounding
box coordinates was applied, with a maximum coordinate shift of 5 pixels to
improve the model’s robustness. The ground truth masks were resized to [256,
256] to match the output dimension of the mask decoder. We fine-tuned the
model using an AdamW optimizer [27] with a learning rate of 6e-5 and a batch
size of 16. The fine-tuning was conducted for 1000 epochs, with manual early
stopping when the training loss plateaued. We used an unweighted sum of Dice
loss and cross-entropy loss as the loss function because this compound loss has
been proven to be robust in various segmentation tasks [28].
Evaluaiton metrics We follow the suggestions in Metrics Reloaded [35] and
employDiceSimilarityCoefficient(DSC)andNormalizedSurfaceDistance(NSD)
to evaluate the region and boundary overlap ratio, respectively.
Acknowledgements The authors of this paper highly appreciate Meta AI for
making SAM2 publicly available to the community. We thank all the dataset
owners for making the invaluable data publicly available. We also thank 3D
Slicer and Gradio team for providing the user-friendly platforms.
References
1. Abid, A., Abdalla, A., Abid, A., Khan, D., Alfozan, A., Zou, J.: Gradio: Hassle-
freesharingandtestingofmlmodelsinthewild.arXivpreprintarXiv:1906.02569
(2019) 3, 9
2. Ahmed,Z.,Panhwar,S.Q.,Baqai,A.,Umrani,F.A.,Ahmed,M.,Khan,A.:Deep
learningbasedautomateddetectionofintraretinalcystoidfluid.InternationalJour-
nal of Imaging Systems and Technology 32(3), 902–917 (2021) 12
3. Allan,M.,Kondo,S.,Bodenstedt,S.,Leger,S.,Kadkhodamohammadi,R.,Luengo,
I.,Fuentes,F.,Flouty,E.,Mohammed,A.,Pedersen,M.,etal.:2018roboticscene
segmentation challenge. arXiv preprint arXiv:2001.11190 (2020) 3
4. Bakas,S.,Reyes,M.,Jakab,A.,Bauer,S.,Rempfler,M.,Crimi,A.,etal.:Identify-
ingthebestmachinelearningalgorithmsforbraintumorsegmentation,progression
assessment, and overall survival prediction in the brats challenge. arXiv preprint
arXiv:1811.02629 (2018) 1
5. Bilic,P.,Christ,P.,Li,H.B.,Vorontsov,E.,Ben-Cohen,A.,Kaissis,G.,Szeskin,A.,
Jacobs, C., Mamani, G.E.H., Chartrand, G., et al.: The liver tumor segmentation
benchmark (lits). Medical Image Analysis 84, 102680 (2023) 114 J. Ma, S. Kim, F. Li, M. Baharoon, R. Ashakereh, H. Lyu, and B. Wang
6. Bommasani, R., Hudson, D.A., Adeli, E., Altman, R., Arora, S., von Arx, S.,
Bernstein, M.S., Bohg, J., Bosselut, A., Brunskill, E., et al.: On the opportunities
and risks of foundation models. arXiv preprint arXiv:2108.07258 (2021) 1
7. Cao,K.,Xia,Y.,Yao,J.,Han,X.,Lambert,L.,Zhang,T.,Tang,W.,Jin,G.,Jiang,
H.,Fang,X.,etal.:Large-scalepancreaticcancerdetectionvianon-contrastctand
deep learning. Nature Medicine 29(12), 3033–3043 (2023) 1
8. Chen, C., Miao, J., Wu, D., Yan, Z., Kim, S., Hu, J., Zhong, A., Liu, Z., Sun,
L.,Li,X.,etal.:Ma-sam:Modality-agnosticsamadaptationfor3dmedicalimage
segmentation. arXiv preprint arXiv:2309.08842 (2023) 2
9. Clark, K., Vendt, B., Smith, K., Freymann, J., Kirby, J., Koppel, P., Moore, S.,
Phillips,S.,Maffitt,D.,Pringle,M.,etal.:Thecancerimagingarchive(tcia):main-
taining and operating a public information repository. Journal of Digital Imaging
26(6), 1045–1057 (2013) 12
10. Dong,H.,Gu,H.,Chen,Y.,Yang,J.,Mazurowski,M.A.:Segmentanythingmodel
2: an application to 2d and 3d medical images. arXiv:2408.00756 (2024) 3
11. Dosovitskiy,A.,Beyer,L.,Kolesnikov,A.,Weissenborn,D.,Zhai,X.,Unterthiner,
T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.: An image is worth
16x16 words: Transformers for image recognition at scale. In: International Con-
ference on Learning Representations (2020) 12
12. Du,Y.,Bai,F.,Huang,T.,Zhao,B.:Segvol:Universalandinteractivevolumetric
medical image segmentation. arXiv preprint arXiv:2311.13385 (2023) 2
13. Falk,T.,Mai,D.,Bensch,R.,Çiçek,Ö.,Abdulkadir,A.,Marrakchi,Y.,Böhm,A.,
Deubner, J., Jäckel, Z., Seiwald, K., et al.: U-net: deep learning for cell counting,
detection, and morphometry. Nature Methods 16(1), 67–70 (2019) 1
14. Fedorov,A.,Beichel,R.,Kalpathy-Cramer,J.,Finet,J.,Fillion-Robin,J.C.,Pujol,
S., Bauer, C., Jennings, D., Fennessy, F., Sonka, M., et al.: 3d slicer as an image
computing platform for the quantitative imaging network. Magnetic Resonance
Imaging 30(9), 1323–1341 (2012) 3, 9
15. Gatidis, S., Früh, M., Fabritius, M., Gu, S., Nikolaou, K., La Fougère, C., Ye, J.,
He, J., Peng, Y., Bi, L., et al.: The autopet challenge: towards fully automated
lesion segmentation in oncologic pet/ct imaging (2023) 12
16. Gatidis, S., Hepp, T., Früh, M., La Fougère, C., Nikolaou, K., Pfannenberg, C.,
Schölkopf,B.,Küstner,T.,Cyran,C.,Rubin,D.:Awhole-bodyfdg-pet/ctdataset
with manually annotated tumor lesions. Scientific Data 9(1) (2022) 12
17. Gong, S., Zhong, Y., Ma, W., Li, J., Wang, Z., Zhang, J., Heng, P.A., Dou, Q.:
3dsam-adapter: Holistic adaptation of sam from 2d to 3d for promptable medical
image segmentation. arXiv preprint arXiv:2306.13465 (2023) 2
18. Heller,N.,Isensee,F.,Maier-Hein,K.H.,Hou,X.,Xie,C.,Li,F.,etal.:Thestateof
theartinkidneyandkidneytumorsegmentationincontrast-enhancedctimaging:
Results of the kits19 challenge. Medical Image Analysis 67, 101821 (2021) 1
19. Huang, Y., Yang, X., Liu, L., Zhou, H., Chang, A., Zhou, X., Chen, R., Yu, J.,
Chen, J., Chen, C., Liu, S., Chi, H., Hu, X., Yue, K., Li, L., Grau, V., Fan, D.P.,
Dong, F., Ni, D.: Segment anything model for medical images? Medical Image
Analysis 92, 103061 (2024) 2
20. Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J., Maier-Hein, K.H.: nnu-net: a
self-configuring method for deep learning-based biomedical image segmentation.
Nature Methods 18(2), 203–211 (2021) 1
21. Ji,G.P.,Xiao,G.,Chou,Y.C.,Fan,D.P.,Zhao,K.,Chen,G.,VanGool,L.:Video
polyp segmentation: A deep learning perspective. Machine Intelligence Research
19(6), 531–549 (2022) 7, 12SAM2 In Medical Images and Videos 15
22. Jin,K.,Huang,X.,Zhou,J.,Li,Y.,Yan,Y.,Sun,Y.,Zhang,Q.,Wang,Y.,Ye,J.:
Fives:Afundusimagedatasetforartificialintelligencebasedvesselsegmentation.
Scientific data 9(1), 475 (2022) 12
23. Ke, L., Ye, M., Danelljan, M., liu, Y., Tai, Y.W., Tang, C.K., Yu, F.: Segment
anything in high quality. In: Neural Information Processing Systems (2023) 8
24. Khaled, R., Helal, M., Alfarghaly, O., Mokhtar, O., Elkorany, A., El Kassas, H.,
Fahmy, A.: Categorized contrast enhanced mammography dataset for diagnostic
and artificial intelligence research. Scientific Data 9(1), 122 (2022) 12
25. Kirillov, A., Mintun, E., Ravi, N., Mao, H., Rolland, C., Gustafson, L., Xiao, T.,
Whitehead,S.,Berg,A.C.,Lo,W.Y.,etal.:Segmentanything.In:Proceedingsof
the IEEE International Conference on Computer Vision. pp. 4015–4026 (2023) 1,
3, 12
26. Leclerc, S., Smistad, E., Pedrosa, J., Østvik, A., Cervenansky, F., Espinosa, F.,
Espeland,T.,Berg,E.A.R.,Jodoin,P.M.,Grenier,T.,etal.:Deeplearningforseg-
mentation using an open large-scale dataset in 2d echocardiography. IEEE Trans-
actions on Medical Imaging 38(9), 2198–2210 (2019) 7, 12
27. Loshchilov, I., Hutter, F.: Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101 (2017) 13
28. Ma, J., Chen, J., Ng, M., Huang, R., Li, Y., Li, C., Yang, X., Martel, A.: Loss
odysseyinmedicalimagesegmentation.MedicalImageAnalysis71,102035(2021)
13
29. Ma, J., He, Y., Li, F., Han, L., You, C., Wang, B.: Segment anything in medical
images. Nature Communications 15, 654 (2024) 2, 3, 8
30. Ma,J.,Wang,Y.,An,X.,Ge,C.,Yu,Z.,Chen,J.,Zhu,Q.,Dong,G.,He,J.,He,Z.,
Cao,T.,Zhu,Y.,Nie,Z.,Yang,X.:Towardsdata-efficientlearning:Abenchmark
forcovid-19ctlungandinfectionsegmentation.MedicalPhysics48(3),1197–1210
(2021) 1
31. Ma, J., Xie, R., Ayyadhury, S., Ge, C., Gupta, A., Gupta, R., Gu, S., Zhang, Y.,
Lee,G.,Kim,J.,Lou,W.,Li,H.,Upschulte,E.,Dickscheid,T.,deAlmeida,J.G.,
Wang, Y., Han, L., Yang, X., Labagnara, M., Gligorovski, V., Scheder, M., Rahi,
S.J., Kempster, C., Pollitt, A., Espinosa, L., Mignot, T., Middeke, J.M., Eckardt,
J.N., Li, W., Li, Z., Cai, X., Bai, B., Greenwald, N.F., Valen, D.V., Weisbart, E.,
Cimini, B.A.,Cheung, T.,Brück,O., Bader,G.D., Wang, B.:The multi-modality
cell segmentation challenge: Towards universal solutions. Nature Methods (2024)
12
32. Ma, J., Zhang, Y., Gu, S., An, X., Wang, Z., Ge, C., Wang, C., Zhang, F., Wang,
Y.,Xu,Y.,Gou,S.,Thaler,F.,Payer,C.,Štern,D.,Henderson,E.G.,McSweeney,
D.M., Green, A., Jackson, P., McIntosh, L., Nguyen, Q.C., Qayyum, A., Conze,
P.H., Huang, Z., Zhou, Z., Fan, D.P., Xiong, H., Dong, G., Zhu, Q., He, J., Yang,
X.:Fastandlow-gpu-memoryabdomenctorgansegmentation:Theflarechallenge.
Medical Image Analysis 82, 102616 (2022) 1, 12
33. Ma,J.,Zhang,Y.,Gu,S.,Ge,C.,Ma,S.,Young,A.,Zhu,C.,Meng,K.,Yang,X.,
Huang,Z.,Zhang,F.,Liu,W.,Pan,Y.,Huang,S.,Wang,J.,Sun,M.,Xu,W.,Jia,
D.,Choi,J.W.,Alves,N.,deWilde,B.,Koehler,G.,Wu,Y.,Wiesenfarth,M.,Zhu,
Q., Dong, G., He, J., the FLARE Challenge Consortium, Wang, B.: Unleashing
thestrengthsofunlabeleddatainpan-cancerabdominalorganquantification:the
flare22 challenge. arXiv preprint arXiv:2308.05862 (2023) 8, 12, 13
34. Ma,J.,Zhang,Y.,Gu,S.,Zhu,C.,Ge,C.,Zhang,Y.,An,X.,Wang,C.,Wang,Q.,
Liu,X.,Cao,S.,Zhang,Q.,Liu,S.,Wang,Y.,Li,Y.,He,J.,Yang,X.:Abdomenct-
1k: Is abdominal organ segmentation a solved problem? IEEE Transactions on
Pattern Analysis and Machine Intelligence 44(10), 6695–6714 (2022) 116 J. Ma, S. Kim, F. Li, M. Baharoon, R. Ashakereh, H. Lyu, and B. Wang
35. Maier-Hein,L.,Reinke,A.,Godau,P.,Tizabi,M.D.,Buettner,F.,Christodoulou,
E., Glocker, B., Isensee, F., Kleesiek, J., Kozubek, M., Reyes, M., Riegler, M.A.,
Wiesenfarth, M., Kavur, A.E., Sudre, C.H., Baumgartner, M., Eisenmann, M.,
Heckmann-Nötzel, D., Rädsch, T., Acion, L., Antonelli, M., Arbel, T., Bakas,
S., Benis, A., Blaschko, M.B., Cardoso, M.J., Cheplygina, V., Cimini, B.A.,
Collins, G.S., Farahani, K., Ferrer, L., Galdran, A., van Ginneken, B., Haase, R.,
Hashimoto, D.A., Hoffman, M.M., Huisman, M., Jannin, P., Kahn, C.E., Kain-
mueller, D., Kainz, B., Karargyris, A., Karthikesalingam, A., Kofler, F., Kopp-
Schneider, A., Kreshuk, A., Kurc, T., Landman, B.A., Litjens, G., Madani, A.,
Maier-Hein, K., Martel, A.L., Mattson, P., Meijering, E., Menze, B., Moons,
K.G.M., Müller, H., Nichyporuk, B., Nickel, F., Petersen, J., Rajpoot, N., Rieke,
N.,Saez-Rodriguez,J.,Sánchez,C.I.,Shetty,S.,vanSmeden,M.,Summers,R.M.,
Taha, A.A., Tiulpin, A., Tsaftaris, S.A., Van Calster, B., Varoquaux, G., Jäger,
P.F.: Metrics reloaded: recommendations for image analysis validation. Nature
Methods 21(2), 195–212 (2024) 13
36. Mazurowski, M.A., Dong, H., Gu, H., Yang, J., Konz, N., Zhang, Y.: Segment
anythingmodelformedicalimageanalysis:Anexperimentalstudy.MedicalImage
Analysis 89, 102918 (2023) 2
37. Misawa, M., Kudo, S.e., Mori, Y., Hotta, K., Ohtsuka, K., Matsuda, T., Saito,
S., Kudo, T., Baba, T., Ishida, F., Itoh, H., Oda, M., Mori, K.: Development of
acomputer-aideddetectionsystemforcolonoscopyandapubliclyaccessiblelarge
colonoscopy video database (with video). Gastrointestinal Endoscopy 93(4), 960–
967.e3 (2021) 7, 12
38. Quinton,F.,Popoff,R.,Presles,B.,Leclerc,S.,Meriaudeau,F.,Nodari,G.,Lopez,
O., Pellegrinelli, J., Chevallier, O., Ginhac, D., Vrigneaud, J.M., Alberini, J.L.: A
tumour and liver automatic segmentation (atlas) dataset on contrast-enhanced
magnetic resonance imaging for hepatocellular carcinoma. Data 8(5), 79 (2023)
12
39. Ravi, N., Gabeur, V., Hu, Y.T., Hu, R., Ryali, C., Ma, T., Khedr, H., Rädle,
R., Rolland, C., Gustafson, L., Mintun, E., Pan, J., Alwala, K.V., Carion, N.,
Wu,C.Y.,Girshick,R.,Dollár,P.,Feichtenhofer,C.:Sam2:Segmentanythingin
images and videos. arXiv:2408.00714 (2024) 3, 12
40. Ren,T.,Liu,S.,Zeng,A.,Lin,J.,Li,K.,Cao,H.,Chen,J.,Huang,X.,Chen,Y.,
Yan, F., et al.: Grounded sam: Assembling open-world models for diverse visual
tasks. arXiv preprint arXiv:2401.14159 (2024) 2
41. Ronneberger,O.,Fischer,P.,Brox,T.:U-net:Convolutionalnetworksforbiomedi-
calimagesegmentation.In:InternationalConferenceonMedicalImageComputing
and Computer-Assisted Intervention. pp. 234–241 (2015) 1
42. Ryali, C., Hu, Y.T., Bolya, D., Wei, C., Fan, H., Huang, P.Y., Aggarwal, V.,
Chowdhury, A., Poursaeed, O., Hoffman, J., et al.: Hiera: A hierarchical vision
transformer without the bells-and-whistles. In: International Conference on Ma-
chine Learning. pp. 29441–29454 (2023) 12
43. Shelhamer, E., Long, J., Darrell, T.: Fully convolutional networks for semantic
segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence
39(4), 640–651 (2017) 1
44. Shen, C., Li, W., Shi, Y., Wang, X.: Interactive 3d medical image segmentation
with sam 2. arXiv:2408.02635 (2024) 3
45. Simpson, A.L., Antonelli, M., Bakas, S., Bilello, M., Farahani, K., Van Ginneken,
B.,Kopp-Schneider,A.,Landman,B.A.,Litjens,G.,Menze,B.,etal.:Alargean-SAM2 In Medical Images and Videos 17
notatedmedicalimagedatasetforthedevelopmentandevaluationofsegmentation
algorithms. arXiv preprint arXiv:1902.09063 (2019) 12
46. Tajbakhsh, N., Jeyaseelan, L., Li, Q., Chiang, J.N., Wu, Z., Ding, X.: Embracing
imperfectdatasets:Areviewofdeeplearningsolutionsformedicalimagesegmen-
tation. Medical Image Analysis p. 101693 (2020) 2
47. Wang, G., Ye, J., Cheng, J., Li, T., Chen, Z., Cai, J., He, J., Zhuang, B.: Sam-
med3d-moe:Towardsanon-forgettingsegmentanythingmodelviamixtureofex-
perts for 3d medical image segmentation. arXiv preprint arXiv:2407.04938 (2024)
2
48. Wang, H., Guo, S., Ye, J., Deng, Z., Cheng, J., Li, T., Chen, J., Su, Y., Huang,
Z., Shen, Y., Fu, B., Zhang, S., He, J., Qiao, Y.: Sam-med3d (2023) 2
49. Wang, Y.R., Yang, K., Wen, Y., Wang, P., Hu, Y., Lai, Y., Wang, Y., Zhao, K.,
Tang,S.,Zhang,A.,etal.:Screeninganddiagnosisofcardiovasculardiseaseusing
artificialintelligence-enabledcardiacmagneticresonanceimaging.NatureMedicine
pp. 1–10 (2024) 1
50. Zhang, S., Metaxas, D.: On the challenges and perspectives of foundation models
for medical image analysis. Medical Image Analysis p. 102996 (2023) 1
51. Zhang,Y.,Ye,F.,Chen,L.,Xu,F.,Chen,X.,Wu,H.,Cao,M.,Li,Y.,Wang,Y.,
Huang, X.: Children’s dental panoramic radiographs dataset for caries segmenta-
tion and dental disease detection. Scientific Data 10(1), 380 (2023) 12
52. Zhang,Y.,Cheng,T.,Hu,R.,Liu,H.,Ran,L.,Chen,X.,Liu,W.,Wang,X.,etal.:
Evf-sam:Earlyvision-languagefusionfortext-promptedsegmentanythingmodel.
arXiv preprint arXiv:2406.20076 (2024) 11
53. Zhao,T.,Gu,Y.,Yang,J.,Usuyama,N.,Lee,H.H.,Naumann,T.,Gao,J.,Crab-
tree, A., Abel, J., Moung-Wen, C., Piening, B., Bifulco, C., Wei, M., Poon, H.,
Wang, S.: Biomedparse: a biomedical foundation model for image parsing of ev-
erything everywhere all at once. arXiv:2405.12971 (2024) 11
54. Zhao,Z.,Zhang,Y.,Wu,C.,Zhang,X.,Zhang,Y.,Wang,Y.,Xie,W.:Onemodel
to rule them all: Towards universal segmentation for medical images with text
prompts. arXiv preprint arXiv:2312.17183 (2024) 2, 11
55. Zhou,T.,Li,L.,Bredell,G.,Li,J.,Unkelbach,J.,Konukoglu,E.:Volumetricmem-
ory network for interactive medical image segmentation. Medical Image Analysis
83, 102599 (2023) 4
56. Zhu, J., Qi, Y., Wu, J.: Medical sam 2: Segment medical images as video via
segment anything model 2. arXiv:2408.00874 (2024) 3
57. Zou, X., Yang, J., Zhang, H., Li, F., Li, L., Wang, J., Wang, L., Gao, J., Lee,
Y.J.:Segmenteverythingeverywhereallatonce.In:NeuralInformationProcessing
Systems. vol. 36, pp. 19769–19782 (2023) 1