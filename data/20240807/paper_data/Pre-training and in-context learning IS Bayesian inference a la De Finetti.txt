Pre-training and in-context learning IS Bayesian
inference a la De Finetti
Naimeng Ye Hanming Yang Andrew Siah Hongseok Namkoong
Columbia University
{ny2336, hy2781, andrew.siah, namkoong}@gsb.columbia.edu
Abstract
Accurately gauging uncertainty on the underlying environment is a longstanding goal of
intelligent systems. We characterize which latent concepts pre-trained sequence models are
naturally able to reason with. We go back to De Finetti’s predictive view of Bayesian reasoning:
instead of modeling latent parameters through priors and likelihoods like topic models do, De
Finetti has long advocated for modeling exchangeable (permutation invariant) sequences of
observables. According to this view, pre-training autoregressive models formulates informed
beliefs based on prior observations (“empirical Bayes”), and forward generation is a simulated
instantiation of an environment (“posterior inference”). This connection allows extending in-
contextlearning(ICL)beyondpredictivesettings,highlightingsequencemodels’abilitytoperform
explicitstatisticalinference. Inparticular,weshowthesequencepredictionlossoverexchangeable
documents controls performance on downstream tasks where uncertainty quantification is key.
Empirically, we propose and demonstrate several approaches for encoding exchangeability in
sequence model architectures: data augmentation, regularization, and causal masking.
1 Introduction
Intelligent systems must be able to utilize the information gathered so far to gauge uncertainty on the
underlying environment they are interacting with. For example, given a sequence of questions and
solutions on trigonometry, an intelligent model should be able to maintain an internal understanding
of its level of uncertainty on core trigonometry concepts. Following standard terminology in Bayesian
statistics, we refer to uncertainty on the underlying concept as epistemic, as it is fully reducible
as the agent observes a large set of trigonometry questions and answers. Systems that can reason
through epistemic uncertainty based on natural language feedback has been a longstanding challenge.
On the other hand, autoregressive models pre-trained on massive web data exhibit striking
predictive capabilities when conditioned on even a small number of demonstrations [9]. “In-context
learning” (ICL) has thus emerged as a powerful learning paradigm where autoregressive generation
provides a versatile pattern recognition model without explicit training, even on complex tasks like
mathematical reasoning [46, 17]. Several recent works study predictive uncertainty in ICL, i.e., model
confidence in predictions. They provide interpretations of autoregressive probabilities as Bayesian
posterior predictive distributions [36, 37, 30] and study their calibratedness [32, 41, 35].
In this work, we go beyond predictive uncertainty in a single inference step and focus on a
dimension of ICL that has received little attention.
Which latent concepts (if any) can pre-trained sequence models reason about its own level
of epistemic uncertainty?
Our main observation is that autoregressive generation of an entire sequence can be viewed as a
simulatedinstantiationofanenvironment. Byconnectingautoregressivesequencemodelstoposterior
1
4202
guA
6
]LM.tats[
1v70330.8042:viXrainference on an underlying latent parameter, we articulate when the pre-training loss (perplexity)
controls the validity of uncertainty quantification over latent parameters (e.g., trignometry concepts).
To describe our main conceptual contribution, we begin by briefly reviewing competing views
of how to operationalize Bayesian reasoning. Instead of the traditional Bayesian modeler who
posits a prior and likelihood on latent variables that are fundamentally unobservable (e.g., the
notion of a latent trigonometry concept is ill-defined), De Finetti’s celebrated work [20] focuses on
modelingobservables. DeFinetti’sviewhighlightsakeyinvarianceunderwhichautoregressivemodels
quantify epistemic uncertainty over latent parameters. We consider a sequence of exchangeable inputs
(e.g., math exam questions and solutions) so that their joint distribution remains invariant under
permutations. De Finetti-Hewitt-Savage [20, 15, 28] showed that for a exchangeable sequence, there
is a latent random variable (“concept”) that governs the data generation process: conditional on the
random variable, the inputs are i.i.d.. Traditional Bayesian models use this result as a justification
for latent factor models: probabilistic topic models posit a prior over latent topics, conditioned on
which documents are generated from a likelihood [7].
In lieu of modeling latent variables that are never observed, autoregressive models consider
a sequence prediction problem over observables. If a sequence model satisfies exchangeability—
invariant under permutations—it defines a proper Bayesian inference machine through autoregressive
generation of sequences (Section 3). In a nutshell:
Pre-trained sequence models are able to reason about uncertainty over latent parameters
that govern a permutation invariant (exchangeable) set of documents.
We show the sequence prediction loss (perplexity) over exchangeable documents measures the quality
of uncertainty quantification over latent concepts that govern them. Thus, standard pre-training
methods are in fact directly optimizing them through auto-differentiation and GPU parallelization.
De Finetti’s perspective highlights the inferential capabilities of ICL, going beyond the predictive
paradigms studied in prior works [24, 2]. Viewing ICL as a Bayesian statistician, we expand
previously proposed downstream ICL tasks to include those that require comprehending uncertainty
(Sections 4 and 5). First, we consider length generalization where we wish to achieve robust predictive
performance over sequences longer than that seen during pre-training. Second, we consider statistical
inference wherewewishtodevelopvalidconfidenceintervalsonparametersgoverningdatageneration.
We show that the sequence prediction loss directly controls performance on these downstream tasks.
Our theory explains permutation invariance in autoregressive models allows robust ICL performance
on long sequences and protection against distribution shift from pre-training to ICL.
Self-attention is permutation invariant over tokens without positional embeddings [43], but
transformer-based sequence models with positional embeddings are not naturally permutation
invariant over sets of tokens (documents). Empirically, we explore several different methods for
instilling permutation invariance in a transformer, including data augmentation (green), loss-based
regularizers (orange), and causal masking schemes (orange). We analyze their impact on the above
two tasks where comprehending epistemic uncertainty is critical, and find the direct modification to
the causal masks is the most effective.
The main contribution of this work is conceptual rather than practical. Following prior work
on ICL [24], we use contrived yet principled examples to articulate our insight. Beyond these
simple examples, we hope our perspective can open up new applications of in-context learning with
uncertainty quantification as a central focus. For example, imagine a social media platform (e.g.,
Twitter) that decides the activity boundaries of a new user (e.g., number of direct messages per
day) depending on its belief of whether the new user is a spammer or a foreign actor spreading
2(a) (b)
Figure 1: (a) Given an observed sequence (“prompt”), in-context learning provides inferential
capabilities by autoregressively generating observations and computing a statistic over them. The
panel on the left plots trajectories of forward generated outcomes; the panel on the right plots the
histogram of the empirical mean of a trajectory. Under permutation invariance (exchangeability),
this histogram is a valid approximation of the posterior distribution over the population mean.
(b) Autoregressive models provide approximate posterior draws via forward sampling. We plot the
KL divergence between the posterior of a latent parameter and the oracle. Our experiments show
that enforcing exchangeability via causal masking (Figure 4) provides large gains in ICL inferential
capabilities with 41x less parameters.
misinformation. Our results show that by autoregressively predicting the next social media post of a
new user based on their history so far, the sequence model can provide proper Bayesian inference on
the user’s maliciousness. Notably, a key feature of our framework is that the model can develop
sharper beliefs as it observes more posts from the user, which in turn can lead to granular controls
on their activity on the platform.
While the connection between ICL and Bayesian reasoning is folklore at this point (e.g., [48, 30]),
we substantiate it by exactly characterizing ICL as explicit Bayesian inference. Our observation
is tautological, and aside from exchangeability, it does not rely on particular architectures nor
elaborate data generation models considered in prior work [48]. We emphasize we are not the
first to make this observation, which is due to De Finetti [20, 15, 10, 16] and decidedly classical.
Our main contribution is of an expository nature, crystallizing and contextualizing this insight
in the modern context of pre-training and ICL. We connect several disparate works on Bayesian
statistics [20, 39, 19, 14, 13, 3, 23, 22, 27, 5, 6, 21], meta learning [36, 37], and Bayesian deep
learning [25], and demonstrate ICL can go beyond predictions and quantify epistemic uncertainty
through forward generation. Our work contributes to the nascent literature that builds formal
models of ICL using hidden Markov models [48], statistical learning with stability conditions [34], or
gradient descent-based algorithms [12, 2, 44, 1]. Compared to the concurrent work [? ] that use
similar connections to study meta-bandit problems, we focus on expanding the scope of ICL to tasks
that require uncertainty quantification and provide an in-depth study of inductive biases that can
instill exchageability in sequence models.
32 Preliminaries
In the next two sections, we formally define pre-training and in-context learning (ICL) as Bayesian
inference through posterior predictives, a concept already recognized in the burgeoning literature on
ICL [48, 36, 37, 49]. We extend this understanding by demonstrating that under exchangeability,
autoregressively generating from a pre-trained model is equivalent to sampling a latent parameter
from the posterior distribution. We connect a line of work in Bayesian statistics focusing on sequence
modeling [20, 3, 23, 22, 27, 5, 6, 21] with modern computational tools. Our primary insight extends
the scope of ICL to tasks that require quantification of epistemic uncertainty.
2.1 Generative modeling
Consider a sequence of observables
Yi = {Yi,··· ,Yi} for i = 1,...,n,
1:T 1 T
where Yi is a set of tokens (documents)—basic units of observables—that take on continuous or
t
discrete values. Define one-step predictive probabilities over these sets of tokens
One-step autoregressive probability: p (cid:98)t(y) := p (cid:98)t(y | Y 1:t) := P (cid:98)(Y
t+1
= y | Y 1:t). (1)
The autoregressive probabilities can also be viewed as posterior predictives of future data given past
observations. We use the two terms interchangeably in the rest of the paper.
Generative modeling fits an autoregressive model (e.g., decoder transformer) to optimize the
joint log likelihood/marginal likelihood of the observed sequences by autoregressively breaking it
down into the above mentioned one-step probabilities
(cid:40) n n T−1 (cid:41)
Pre-training: maximize 1 (cid:88) logp(cid:0) Yi (cid:1) = 1 (cid:88)(cid:88) logp (cid:0) Yi (cid:1) . (2)
p(·) n (cid:98) 1:T n (cid:98)t t+1
(cid:98) i=1 i=1 t=0
The objective (2) models a subset of the usual prediction loss used to train LLMs, since our units
of analysis y models sets of tokens. As an example, the observable sequeces Y could be a list of
1:T
images, each represented by a set of tokens, and we model each image as a single unit of observation
(instead of the pixels).
A generative model p can be used to tackle a range of different tasks by conditioning on any
(cid:98)
sequence Y (typically a prompt) at inference time (“in-context learning”). Although our subsequent
1:s
results and algorithms can consider covariates/features for each example, we ignore them to simplify
exposition, and only mention the generalization as a comment in theorem statements.
2.2 Traditional Bayesian modeling
We will contrast two approaches to Bayesian modeling: the traditional Bayesian view that models
latent parameters, and De Finetti’s view that directly models sequence of observables. We briefly
review the former in this subsection.
A traditional Bayesian statistician posits a latent parameter θ, a distribution over the parameter,
π (“prior”), and how θ governs data generation, P(Y = · | θ) = P (Y ) (“likelihood”). This
θ 1:t θ 1:t
subsumes the observables are conditionally i.i.d., where the likelihood factorizes as
(cid:90) ∞
(cid:89)
P(Y = y ) = P(Y = y | θ)π(dθ) (3)
1:∞ 1:∞ t t
t=1
4for some latent θ ∼ π(·) drawn from some prior.
We use the term epistemic uncertainty to refer to the modeler’s reducible uncertainty and the
term aleatoric uncertainty to refer to the inherent randomness in the data. Then, given observed
data Y , the posterior distribution π(θ = · | Y ) measures the epistemic uncertainty on the latent
1:s 1:s
parameter. Usually, this posterior is gradually reduced to the usual dirac measure (on the true latent)
as more data is gathered. The likelihoods represent the aleatoric uncertainty, since the randomness
is intrinsic to the data-generating process. To summarize, in traditional Bayesian modeling
Prior: π(θ)
Likelihood: P(Y = · | θ) = P (Y ) aleatoric uncertainty
1:t θ 1:t
Posterior: π(θ = · | Y ) epistemic uncertainty
1:s
(cid:90)
Posterior predictive: P(Y = · | Y ) = P(Y = · | θ)π(θ = · | Y )dθ
s+1:T 1:s s+1:T 1:s
The main challenge with this modeling paradigm is the need to specify a model over latent factors
and argue for its validity despite its fundamentally unobservable nature. While its philosophical
standing is subject to debate, a practical model validation metric is to check whether the posited
model on latent parameters explain observed data well [26]: for a posited model p, the marginal
(cid:98)
likelihood measures whether p explains observed sequences
(cid:98)
n (cid:90)
1 (cid:88)
R (p) := log p(Y | θ)π(θ)dθ.
n (cid:98) (cid:98) 1:T
n
i=1
Tuning hyperparameters of the prior p(θ) based on this measure is often called empirical Bayes.
(cid:98)
Bayesian deep learning methods subscribe to this view and propose model designs that aim to
capture this latent structure [8, 38, 45, 31].
2.3 De Finetti’s theorem and the role of exchangeability
De Finetti’s characterization of an exchangeable sequence Y (i.e., its distribution is permutation
1:∞
invariant) provides an elegant assumption over the observables itself for when the conditional i.i.d.
assumption (3) holds.
Definition 1. A sequence Y is infinitely exchangeable if for any finite permutation σ, we have
1:∞
P(Y ,Y ,Y ,...) = P(Y ,Y ,Y ,...).
1 2 3 σ(1) σ(2) σ(3)
Under this definition, there is a latent parameter θ such that
(cid:90) ∞
(cid:89)
P(Y = y ) = P(Y = y | θ)π(dθ). (4)
1:∞ 1:∞ t t
t=1
A key structural property that this perspective highlights is that the one-step probabilities (1)
must correspond to posterior predictions consistent with a single prior. Prior works study minimal
conditions that one-step probabilities should satisfy to guarantee a notion of predictive coherence,
ensuring they (roughly) follow Bayes’ rule according to some prior. For example, Berti et al. [4]
discusses a notion called conditionally identically distributed (c.i.d.), which extends the familiar
concept of exchangeability: Y is c.i.d. if
1:∞
P(Y = y | Y ) = P(Y = y | Y ) =: p (y | Y ) =: p (y) for all y ∈ R. (5)
t+2 1:t t+1 1:t t 1:t t
5Evidently, if a sequence is exchangeable, it satisfies condition (5). As we will demonstrate, exchange-
ability of the generated sequence (equivalently, permutation invariance of the autoregressive sequence
model) provides valid statistical inference and ensures robust performance on downstream tasks that
require uncertainty quantification (Sections 4 and 5).
3 Bayesian modeling a la De Finetti
As stated before, we want to move away from modeling a
fictitious latent parameter θ that has no physical meaning,
since it is never observed. Instead, we want to use the sequence
Y , which is observable in principle and has a direct physical
1:∞
interpretation: the “future” sequence Y is simply yet to
s+1:∞
be observed.
De Finetti’s theorem shows that the two seemingly different
modeling viewpoints are in fact equivalent. Its characterization
of exchangeable sequences [20, 15] goes beyond the represen-
tation from Equation (4): De Finetti [15], Hewitt and Savage
[28] show that the latent parameter θ in Equation (4) is in fact
entirely a function of the infinite sequence of observables Y .
1:∞
From here, we are able to conclude the following:
Autoregressive probabilities (1) are sufficient primitives for defining a Bayesian model.
As an example, consider the posterior predictive mean
θ¯ = E[θ | Y ].
T 1:T
By Doob’s martingale convergence theorem, θ¯ converges almost surely to a random variable
T
θ¯ = θ ∼ π(·) that is a function of the infinite sequence Y . Note that mathematically, the
∞ 1:∞
posterior predictives are random probability measures defined over realizations of Y . Given a
1:∞
random infinite dataset y , the limiting point estimate θ¯ (y )—the posterior mean computed on
1:∞ ∞ ∞
the entire dataset—is in fact distributed according to the true prior π(·).
This equivalence highlights the following fact:
Epistemic uncertainty in θ is equivalent to predictive uncertainty in Y .
1:∞
3.1 Empirical Bayes and sequence modeling
De Finetti’s work [10] focuses on modeling the relationships between observable quantities (1).
Notably, we can now validate the modeler’s claims by masking part of the observed data from the
modeler! As we explain below, this allows using validation losses on hidden data—the empirical
foundation of ML—to measure the quality of the model’s ability to comprehend uncertainty.
To operationalize De Finetti’s philosophy, we take the posterior predictive probabilities (1) as our
modeling primitive to approximate the probability of seeing the observed dataset
T−1
(cid:89)
Marginal likelihood: P(Y = y ) ≈ p(y ) = p (y )
1:T 1:T (cid:98) 1:T (cid:98)t t+1
t=0
Instead of priors and likelihoods, Bayes a la De Finetti specifies one-step probabilities (1) on
observables. A long line of work in Bayesian statistics advocates for this approach to Bayesian
6modeling [39, 19, 14, 13, 3, 23, 22, 27, 5, 6, 21]. They propose simple parameterizations for one-step
probabilities (e.g., copulas [27, 21]) and identify conditions under which one-step posterior predictive
distributions implicitly characterize the prior and likelihood over the latent factor θ.
We note a specific connection between empirical Bayes and autoregressive models, which allows
us to perform statistical inference using sequence models (e.g., transformers, state space models)
that are multiple orders of magnitude more expressive than previously considered. Since it is difficult
to specify one-step probabilities over long sequences, we model them using modern sequence models
(e.g., transformers) following previous works. We adopt the empirical Bayes philosophy: when our
one-step probabilities accurately model the data-generating distribution, masked observations will
have high marginal likelihood p(Y | Y ). Note that this is precisely the original pre-training
(cid:98) s+1:T 1:s
problem (2)! We conclude that pre-training is equivalent to performing empirical Bayes by directly
optimizing posterior predictive densities.
By modeling these one-step probabilities collectively through a sequence model, we leverage a
key factor in the empirical success of language modeling: training a differentiable loss on a flexibly
parameterized model that can be optimized using abundant data. As long as there is a wealth of
previously observed sequences Yi ,i = 1,...,n, we can train any sequence model on perplexity to
1:T
learn the collection of posterior predictives.
3.2 Explicit Bayesian inference through autoregressive forward sampling
Given a pre-trained sequence model, we can perform ICL by conditioning on any test time observable
sequence and autoregressively predicting the next set of tokens. The exchangeability condition is a
starting point for studying previously proposed formalizations of ICL [24]. As an example, consider
a question answering task, where we expect question-answer pairs to be exchangeable. Naturally, we
do not expect exchangeability at the token level, so our mathematical insights on Bayesian inference
only applies over units of exchangeability (e.g., documents).
Unlike prior works that interpret ICL as implicit Bayesian inference by leveraging structures
like hidden Markov models [48], we analyze in-context learning (ICL) as explicitly modeling the
latent variable through autoregressive sampling. As long as we have exchangeability, the one-step
probabilities converge to a limit, which we interpret as a “latent parameter θ” fully determined by
an infinite sequence of observations. Berti et al. [4] shows that if condition (5) holds, then {p }∞
t t=1
forms a martingale
(cid:90)
E[p (y) | Y ] = P(Y = y | Y )dp(Y | Y ) = p (y) for any y
t 1:t−1 t+1 1:t t t−1 t−1
and the martingale convergence theorem yields
∃ random distribution p (· | Y ) s.t. ∀y p (y | Y ) → p (y | Y ) almost surely. (6)
∞ 1:∞ t 1:t ∞ 1:∞
Weinterprettherandomlimitp asa“latentparameter” entirelydeterminedbyinfiniteobservations.
∞
Therefore, the ICL paradigm—conditioning on a sequence of observables and autoregressively
generating the future—is equivalent to modeling the latent parameter θ := p . This is the key
∞
insight of this work: ICL is equivalent to Bayesian inference on θ a la De Finetti.
Moreover, under condition (5), the pre-training objective (2) (perplexity) is the correct perfor-
mance measure capturing Bayesian inferential capabilities.
Assumption A. The true data-generating distribution satisfies condition (5) and its pre-trained
counterpart satisfies the analogue P (cid:98)(Y(cid:98)t+2 = · | y 1:t) = p (cid:98)t(· | y 1:t) for all t ∈ N and y 1:∞.
7For the filtration F generated by the sequence Y , define the martingale difference sequence
t 1:t
(cid:90)
D := logp (Y | Y )− p (y)logp (y)dy.
t (cid:98)t t+1 1:t t (cid:98)t
Assume E[D2] < ∞, and
t
(cid:118)
(cid:88)∞ P(cid:18) |D t| > logt
logt
(cid:12) (cid:12)
(cid:12)
F t−1(cid:19) < ∞ and (cid:117) (cid:117) (cid:116)(cid:88)t E(cid:104) D j2 | F j−1(cid:105) ·tloglogt → 0 almost surely. (7)
t=1 j=1
Theorem 1. Let Assumption A and the regularity condition (7) hold. If p (y)logp (y) is point-wise
t (cid:98)t
bounded by some integrable function,
T (cid:90)
1 (cid:88)
logp (Y ) → p (y)logp (y)dy =: H(p) a.s..
(cid:98)t−1 t ∞ (cid:98)∞ (cid:98)
T
t=1
See Appendix A for the proof.
The true distribution is clearly the “best model”: from Jensen’s inequality, for any p
(cid:98)
(cid:34) T (cid:35) (cid:34) T (cid:35) (cid:90) (cid:90)
1 (cid:88) 1 (cid:88)
E logp (Y ) ≤ E logp (Y ) and p (y)logp (y)dy ≤ p (y)logp (y)dy.
(cid:98)t−1 t t−1 t ∞ (cid:98)∞ ∞ ∞
T T
t=1 t=1
In contextual tasks (e.g., question answering) where X is the random variable representing the
context/covariates (question) and Y (answer) is generated by p (Y|X), we have an analogous result
∞
(cid:34) T (cid:35) (cid:90)
1 (cid:88)
E logp (Y | X ,Y ) → E p (y | X)logp (y)dy =: E H(p | X).
X1:T∼PX T
t=1
(cid:98)t−1 t 1:t 1:t−1
X∼PX
∞ (cid:98)∞ X∼PX (cid:98)∞
We provide analytical examples for a toy attention model in Appendix C.
4 Length generalization
We highlight overlooked applications of ICL that require uncertainty quantification and Bayesian
reasoning. First, wedemonstratethatBayesianinferenceenableslength generalization (2)insequence
predictions, providing robustness to longer sequence prediction contexts than those encountered
during training. Sequences in the pre-training data will be naturally limited in their length—there
is a bound on the number of documents Y observed from the same exchangeable cluster. We are
t
interested in the model’s ability to achieve robust predictive performance over sequences longer than
those seen during pre-training. This ability requires the model to correctly propagate epistemic
uncertainty, meaning it must learn to extract information from the longer context sequence and
place more confidence in its own predictions (Figure 2). The environment generates observable
sequences of length T, and we use our model, denoted as p, to predict the probability of observing
(cid:98)
this sequence, i.e. p(y ). Recently, Hollmann et al. [29] empirically observed that transformers
(cid:98) 1:T
pre-trained on synthetic data can achieve competitive predictive performance on real data, even over
sequences longer than that seen during pre-training.
In this work, we theoretically analyze the performance gap between the ground truth environment
Q and the model, expressed as E [logq(Y )−logp(Y )], and demonstrate that the model’s
Y1:T∼Q 1:T (cid:98) 1:T
8pˆ(yˆ 1) pˆ(yˆ 2) pˆ(yˆ Tpt) pˆ(yˆ T−1) pˆ(yˆ T) pˆ(yˆ sb +1) pˆ(yˆ sb +2) pˆ(yˆ Tb)
∼ ∼
Pre-trained Sequence Model Pre-trained Sequence Model
... ... ... ...
y y y y y y y yb yb
1 2 Tpt T−1 1 2 s s+1 s+2
x x x x x x x x xb xb xb
1 2 Tpt T−1 T 1 2 s s+1 s+2 T
pre-trained length longer sequence context autoregressively
generate B times
Figure 2: Length generalization. Figure 3: Statistical inference.
performanceonlongersequencesischaracterizedbythelimitingperplexityH(p)definedinTheorem1.
(cid:98)
Assuming that our sequence model p is infinitely exchangeable, De Finetti’s theorem shows that
(cid:98)
there is a prior π and a likelihood p corresponding to this model. In this section, we will require a
(cid:98)∞
stronger condition, that data generated from p is mixture of i.i.d. over a finite-dimensional latent
(cid:98)
parameter θ ∈ Rd. Concretely, we let θ (cid:55)→ p (y | θ) =: p (y),θ ∈ Θ := supp(π) be the likelihood
(cid:98)∞ (cid:98)θ
function mappings implicitly defined by the sequence model p. At test time, the environment
(cid:98)
generates i.i.d. sequences Y ∼ Q, where the likelihood under Q does not necessarily lie in the
1:T
likelihood class {p : θ ∈ Θ} that the model posits.
(cid:98)θ
4.1 ICL generalizes robustly if exchangeable
By using an analogous mathematical machinery used to prove the Bernstein-von Mises theorem [42,
33], we can characterize the predictive performance of the model under very long sequences. In
particular, the limiting performance gap achieves the best in-class performance among the set of
possible likelihoods Θ. We assume the KL divergence of the model p relative to q is finite and
(cid:98)
θ⋆ ∈ Θ is the KL projection of the i.i.d. data-generating distribution Q(Y = ·) to the likelihood
space {p : θ ∈ Θ}
(cid:98)θ
(cid:16) (cid:17)
θ⋆ = argminD
kl
Q(Y = ·)||P(cid:98)θ(Y = ·) .
θ∈Θ
Define the standardized score function
T
1 (cid:88)
S (θ) := √ ℓ˙ (Y ) where ℓ˙ (y) := ∇ logp (y).
T θ t θ θ (cid:98)θ
T
t=1
In the following result, assume the density p (y) is twice continuously differentiable at θ⋆ for
(cid:98)θ
(cid:16) (cid:17)
almost every y, and let θ (cid:55)→ D
kl
Q(Y = ·)||P(cid:98)θ(Y = ·) have a positive definite Hessian V
θ⋆
at θ = θ⋆.
Moreover, assume there exists a δ such that
E Y∼Q(cid:34)(cid:12) (cid:12) (cid:12) ∂ logp (cid:98)θ(Y)(cid:12) (cid:12) (cid:12)2(cid:35) < ∞ and E Y∼Q(cid:34) sup (cid:12) (cid:12) (cid:12) ∂2 logp (cid:98)θ(Y)(cid:12) (cid:12) (cid:12)2(cid:35) < ∞.
(cid:12)∂θ (cid:12) (cid:12)∂θ ∂θ (cid:12)
j ∥θ−θ⋆∥<δ j k
9Theorem 2. Let the prior density π be continuous and positive in a neighborhood of θ⋆, and let
E [ℓ˙ ℓ˙T ] be invertible. For Y i ∼id Q, we have
Q θ⋆ θ⋆ t
inf D (Q(Y = ·)||p (Y = ·))
kl (cid:98)θ
θ∈Θ
(cid:18)
1
≥ limsup E [logq(Y )−logp(Y )]
T→∞
T Yti ∼id Q 1:T (cid:98) 1:T
(cid:19)
d T 1 1 1 1
− log + E [S (θ⋆)TV−1S (θ⋆)]− log − logdet(V ) .
2T 2π 2T Yti ∼id Q T θ⋆ T T π(θ⋆) 2T θ⋆
See Appendix B for the proof.
Combining with Theorem 1, we have shown that an exchangeable model’s performance is optimal
in the induced model class Θ up to O(logT)
T
(cid:18) (cid:18) (cid:19)(cid:19)
1 1
D (q||p ) ≤ inf D (Q(Y = ·)||p (Y = ·))+O logT +log .
kl (cid:98)∞ θ∈Θ kl (cid:98)θ T π(θ⋆)
For contextual tasks where X is the covariates and Y is the observable, we have the analogous result:
for any realization x,
(cid:18) (cid:18) (cid:19)(cid:19)
1 1
D (q(· | X)||p (· | x)) ≤ inf D (Q(Y = · | x)||p (Y = · | x))+O logT +log .
kl (cid:98)∞ θ∈Θ kl (cid:98)θ T π(θ⋆ | x)
(8)
This demonstrates that optimizing perplexity is the appropriate objective function to ensure
the quality of Bayesian inference for the exchangeable model. Our result partially explains the
striking robustness of ICL against distribution shift from the pre-training distribution. So long as
the implicit and misspecified prior π puts some weight on the best in-class approximation θ⋆ to
the data-generating ICL environment Q(Y = ·), the sequence model becomes a robust predictor,
incurring only logT regret as it sees more contexts. These bounds formalize and highlight two key
components of pre-training that practitioners track: the diversity of pre-training data (i.e., assigning
weights to θ⋆) and perplexity.
4.2 Case study: a one-layer transformer
To solidify our insights from Theorem 2, we provide an explicit example where the sequence model is
based on a simple one-layer transformer. In the contextual case with X ∈ Rd, we consider a model
t
p (Y | X ) that is parameterized by an self-attention layer given by Q,K,V ∈ Rd×T matrices,
(cid:98)t t+1 t+1
followed by an extra layer on top that takes the output of the Q,K,V matrices to a R2T dimensional
vector, representing the mean and variances of the posterior predictive distributions. In particular,
we take the model class p ∈ P to be the family of normal distributions.
(cid:98)
We organize the input matrix Z which is fed into a self-attention layer parameterized by
t+1
Q,K,V to give the output Z(cid:98)t+1 ∈ Rd×t+1
(cid:34) (cid:35)
(cid:20) (cid:21)
Z
t+1
:= X
Y
11 . .. ..
.
X
Y
tt X 0t+ ,1 → self-attention (Q,K,V) → Z(cid:98)t+1 := X Y(cid:98)(cid:98) 11 . .. ..
.
X Y(cid:98)(cid:98) tt X Y(cid:98)(cid:98) tt ++ 11 (9)
where X(cid:98)t is the prediction at time t and Y(cid:98)t is the target at time t. This output matrix then goes
through the final linear layer to get the final prediction vector
[µ ,σ ,...,µ ,σ ] =⇒ p (y | x) = N(µ (x),σ2 (x))(y) ∀t = 1,...,T −1.
(cid:98)1 (cid:98)1 (cid:98)t+1 (cid:98)t+1 (cid:98)t (cid:98)t+1 (cid:98)t+1
10Throughout, we let X⃗ ∈ Rt×d be the set of contexts stacked together (design matrix) and let Y⃗ ∈ Rt
t t
be the vector of target outcomes
X⃗ := (cid:2) X ··· X (cid:3)⊤ , Y⃗ := (cid:2) Y ··· Y (cid:3)⊤ .
t 1 t t 1 t
Following previous works [44, 47], we assume for simplicity that the final layer and the self-
attention parameters Q,K,V matrices are such that the output vector always has the variances
matching the closed form oracle. In other words, we only study the expressiveness of a one-layer
transformer on mean estimation. Elementary calculations—which we give in Appendix C.1—show
that autoregressive mean predictions are equivalent to performing one-step gradient descent on the
squared loss, based on the observed data so far.
Lemma 1 (Reparameterization). The autoregressive mean predictions of the transformer described
above is given by one-step gradient descent for linear regression
1
µ (x) = Y⃗⊤X⃗ Γ⊤x (10)
(cid:98)t+1 t+1 t t
where Γ ∈ Rd×d is a reparameterization of the trainable part of the transformer parameters.
Data-generating process To substantiate our abstract results in the previous subsection, we
consider a tractable data-generation distribution. We take the usual Bayesian linear regression
problem with latent parameters w ∼ π
w ∼ N(0,τ2I), X i ∼id N(0,H), Y = w⊤X +ε where ε i ∼id N(0,σ2) for a known σ2. (11)
t t t t t
We assume H is full rank throughout. The pretraining data is generated from the distribution (11),
where we assume the pre-training sequence has length T .
pt
At inference time, the modeler observes data generated i.i.d. from a particular distribution Q.
Denoting by w the coefficients that give test data, we abuse notation to write Y = w⊤X +ε . In
q t q t t
this case, the oracle autoregressive variances (which we assume the modeler knows) are given by
1 1
σ2 = xTA−1 x+σ2 where A := X⃗T X⃗ + I. (12)
(cid:98)t t t−1 t−1 σ2 t−1 t−1 τ2
Exact characterization of length generalization Any fixed weight Γ for the one-layer trans-
former defines an infinitely exchangeable model (since self-attention is permutation invariant). By De
Finetti’s theorem, an infinitely exchangeable sequence model implicitly defines a likelihood and prior.
In the case where the oracle posterior variance (12) is known, we can go beyond the approximation
in Theorem 2 and exactly characterize the excess risk of our sequence model. The exact calculations
are given in Appendix C.2.
Proposition 3. Consider the test-time data generating distribution Q described above and let
X ∼ P be independent of everything else. Then, we have
X
1
E [logq(Y | X )−logp(Y | X )]
Q 1:T 1:T (cid:98) 1:T 1:T
T
=
1 (cid:88)T
E
(cid:34) −X⊤A t−1X +log(cid:18)
1+
X⊤A t−1X(cid:19)
+
1 (cid:18)
w⊤X −
1
Y⃗⊤ X⃗
Γ⊤X(cid:19)2(cid:35)
2T Q σ2+X⊤A X σ2 σ2+X⊤A X q t t−1 t−1
t−1 t−1
t=1
1
→ w⊤(I −HΓ⊤)H(I −HΓ⊤)⊤w as T → ∞.
2σ2 q q
In particular, if Γ = H−1 the one-layer transformer perfectly generalizes to long sequences.
11Pre-training a one-layer transformer Finally, we explicitly characterize the solution to the
population version of the pre-training problem (2) as the number of observations n generated from
the distribution (11) goes to infinity. Assuming the variance is known as above, the (marginal)
log likelihood is equivalent to the averaged mean squared error across all timesteps. Denoting the
pre-training sequences length as T , the population level pre-training objective is given by
pt
1
T (cid:88)pt−1 (cid:18)
1
(cid:19)2
R (Γ) = E Y⃗⊤X⃗ Γ⊤x −Y . (13)
Tpt T t+1 t t t+1 t+1
pt
t=0
Elementary calculations (e.g., Wu et al. [47, Appendix C]) show the population-level pretraining
objective (13) can be simplified
 
1
T (cid:88)pt−1
1
T (cid:88)pt−1
R Tpt(Γ) =
T
H⊤(Γ−Γ(cid:101)t)H(cid:101)t(Γ−Γ(cid:101)t)⊤+H⊤ τ2I −
T
Γ(cid:101)tH(cid:101)tΓ(cid:101)⊤
t
+σ2.
pt pt
t=0 t=0
where recalling H := E[X X⊤], we used the shorthand
t t
(cid:18) 1 (cid:19)(cid:18) 1 (cid:19)⊤ (cid:18) tr(H)+σ2/τ2 t+1 (cid:19)
H(cid:101)t := E tX⃗ t⊤Y⃗
t
tX⃗ t⊤Y⃗
t
= τ2H
t
I +
t
H ,
(cid:18) σ2 (cid:19)−1
Γ(cid:101)t := t tr(H)+ τ2I +(t+1)H is the least quares solution at timestep t.
The pre-training problem can be interpreted as a multi-task learning problem where we learn a single
Γ for all timesteps. Solving the quadratic objective explicitly, we arrive at
 −1
Tpt−1 Tpt−1
(cid:88) (cid:88)
Γ⋆ Tpt :=  H(cid:101)t · H(cid:101)tΓ(cid:101)t. (14)
t=0 t=0
Plugging Γ⋆ into Proposition 3, we have characterized the approximation error incurred by the
Tpt
one-layer transformer model in the Bayesian linear regression setting.
5 In-context learning as a Bayesian statistician
We now explicitly highlight that ICL can be utilized for
Given (y ),p ,T ≫ s
statisticalinference(3),extendingitsutilitybeyondtypical 1:s (cid:98)1:∞
for b → 1 to B do
predictive applications. We study how we can use forward
generation to construct confidence intervals for a latent for t → s+1 to T do
parameterthatgovernsdatageneration. Goingbacktothe Y(cid:98) tb i ∼id p (cid:98)t−1(· | y 1:t−1)
case without contexts for simplicity, for some measurable
end for
function g : R → R, let the estimand be its mean under τb := (cid:82) g(y)dFb(y)
p T T
∞
(cid:90) end for
τ⋆ = g(y)p (y)dy.
∞ Output τ1,...,τB i ∼id Π (.|y )
T T T 1:s
For a finite predictive observable size T, we can write the
parameter estimate as Algorithm 1: Autoregressive boot-
straps. Fb is the empirical distri-
T (cid:90) T
τ(y 1:T) = 1 (cid:88) g(y t) = g(y)dF T(y) bution of (y 1:s,Y(cid:98) sb +1:T).
T
t=1
12where F denotes the empirical distribution. Note that this approach straightforwardly generalizes
T
to loss minimization settings argmin (cid:82) ℓ(τ,y)dp (y); for instance, in the Bayesian linear regression
τ ∞
setting, the parameter of interest is often the linear coefficient.
Letting y be the observed data, our goal is to generate a confidence/credible interval around
1:s
τ⋆. To this end, we can draw insights from the extensive body of work in Bayesian statistics. For
instance, Efron and Tibshirani [18] proposed the Bayesian bootstrap, and Fong et al. [21] introduced
a clear forward sampling algorithm for the Bayesian bootstrap within De Finetti’s framework. We
instantiate their approach using autoregressive sequence models in Algorithm 1 and Figure 2. The
following classical result shows autoregressive forward sampling allows sampling from the true
posterior.
Theorem 4 (Berti et al. [4, Lemma 2.1, Theorem 2.2]). Let Assumption A hold, and let Y(cid:98)s+1:T be
autoregressively generated conditional on Y = y . When E[|g(y)|] < ∞, we have
1:s 1:s
T T (cid:90)
1 (cid:88) 1(Y(cid:98)t ≤ y) a →.s. P(cid:98)∞(y | y 1:s), and 1 (cid:88) g(Y(cid:98)t) a →.s. g(y)p (cid:98)∞(y | y 1:s)dy.
T −s T −s
t=s+1 t=s+1
We can again show the limiting sequence loss H(p) offers pathwise control over τ −τ⋆, where
(cid:98) (cid:98)T
τ
(cid:98)T
= (cid:82) g(y)dF(cid:98)T(y) is the parameter estimate from the autoregressive generation from the model p (cid:98).
lim τ (cid:98)T −τ⋆ ≲ ∥g∥ ∞(cid:112) D kl(p ∞||p (cid:98)∞) ∝ H(p (cid:98))1 2
T→∞
Thus, the pre-training problem (2) is the “right” objective to guarantee the quality of Bayesian
inference. Under exchangeability, autoregressive generation gives a natural Bayesian inference
procedure based on the bootstrap: calculating a α-confidence interval from Algorithm 1 by finding
the corresponding quantile q , we have p (τ⋆ ≤ q ) ∼ α.
(cid:98)α,n,B ∞ (cid:98)α,s,B
If we only care about the squared loss (as opposed to log likelihoods), the limiting sequence loss
H(p) in Theorem 1 also governs the sequential prediction performance over long horizons. That is,
(cid:98)
evaluating on squared loss instead of the perplexity. Given a “prompt” consisting of the sequence of
tokens Y
1:s
at inference time, we generate Y(cid:98)s+1:T to predict unseen observations Y s+1:T. We evaluate
ourselves on the T-horizon squared loss R(cid:98)T := T1 (cid:80)T t=1(Y(cid:98)t−Y t)2. The limiting perplexity H(p (cid:98)) again
controls length generalization capabilities of the fitted model
(cid:16) (cid:17)
lim R(cid:98)T ≤ Var Y(cid:98)∞ +Var(Y ∞)+D kl(p ∞||p (cid:98)∞) ∝ (1+H(p (cid:98))). (15)
T→∞
6 Inductive biases for exchangeability
Our theory highlights exchangeability/permutation invariance as a key property of the autoregressive
model that enables Bayesian inference. In this section, we investigate various inductive biases that
can be applied to promote permutation invariance in a transformer, and study their effectiveness in
enhancing performance on the two aforementioned tasks in Sections 4 and 5 where understanding
epistemic uncertainty is crucial.
Following prior works on ICL [24], we study sequence generating processes where the oracle Bayes
model that knows the true prior can be explicitly computed. This allows us to directly compare the
autoregressive loss of the fitted sequence model to that of the optimal Bayesian model. In particular,
we return to the Bayesian linear regression problem (11). Clearly, the sequence prediction loss
is maximized by the Bayesian linear regression oracle that knows the true prior w ∼ N(0,τ2I). By
13the Bayes rule, this data-generating process is equivalent to marginalizing over latent w ∼ π(·), and
simply generating data from the oracle posterior predictives Y ∼ p(· | Y ,X ) iteratively over
t 1:t−1 1:t
i.i.d. covariates X i ∼id P .
t X
6.1 Promoting Permutation Invariance
Without any constraints on the autoregressive probabilities, we cannot make reliable predictions
beyond the pre-training context length. In other words, we lack guarantees on the quality of p for
(cid:98)t
any t > T. This underscores the necessity of restricting the model class to a collection of models that
enforce some coherency condition. We investigate various inductive biases that can be applied to
promote permutation invariance in a transformer model. Specifically, we evaluate data augmentation,
loss-based regularization, and causal masking strategies as compared various positional embeddings
on GPT2 in order to improve the coherency in the autoregressive probabilities.
Data augmentation A straightforward way to promote permutation invariance is data augmen-
tation where we permute the order of the input data during training, so that the autoregressive
model learns to provide identical outputs regardless of the order of the input sequence. We perform
permutation-based data augmentation to train GPT2 with and without positional embeddings.
Regularization As an alternative, we propose a novel cid regularization method that forces
themodeltorespecttheexchangeabilitystructureofthedata. Inadditiontotheusualautoregressive
loss term in the training objective, we add a KL-regularizer that encourages the model to predict the
next token in the same way as predicting the token after the next token, inspired by condition (5).
Denoting the one-step autoregressive probability by pi := p(Yi | Xi ,Yi ,Xi ), we add the
(cid:98)t (cid:98) t+1 1:t 1:t t+1
following term to the usual training objective
T−1 T−2 (cid:32) (cid:33)
(cid:88) (cid:88)
logpi + λ · D pi || p (Yi | Xi ,Yi ,Xi ) ,
(cid:98)t kl (cid:98)t (cid:98)t+1 t+1 1:t 1:t t+1
(cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
t=0 t=0
autoregressiveloss one-step prediction two-step prediction
(if Yi ,Xi observed in two steps)
t+1 t+1
(16)
wherethetwo-steppredictorp marginalizesovernext-steppredictions. Weestimatetheregularizer
(cid:98)t+1
using Monte Carlo samples as detailed in Appendix D.
Causal masking Finally, we propose the Exchangeable Transformer, an autoregressive
transformer model without positional embeddings, an attention masks where target tokens attend to
past context, with (x,y) concatenated into each token (Figure 4). For points being predicted, the x
are concatenated with zeroes, i.e., (x,0). This architecture is inspired by prior works [36, 37], but
differ in a small yet important way in how future predictions attend to prior true labels and the
point being predicted (see Appendix E for a detailed comparison). Each token in our exchangeable
transformer consists of concatenated feature label pairs to allow for the removal of positional
embeddings. An attention mechanism and model architecture was also designed to respect the
autoregressive conditioning. See Appendix E for ablations on our architecture.
14(a) Attention mask (b) Architecture
Figure 4: The attention mask and architecture of an exchangeable transformer. Each token consists
of a feature-label pair. Auxiliary tokens are target covariates padded with zeros. Both auxiliary and
target tokens attend to themselves and target tokens of smaller index. Our model is then evaluated
on autoregressive loss based on the predictions made on the auxiliary tokens.
(a) Bayesian linear regression (b) Exchangeable Transformer trained on different lengths
Figure 5: Optimality gap in autoregressive loss over sequences longer than that seen during pre-
training, D (p(· | X ,Y )||pˆ(· | X ,Y )), approximated over 100 trajectories. Both plots
kl 1:T+1 1:T 1:T+1 1:T
are in log scale. Exchangeable Transformer significantly outperforms all other approaches. Compared
to 9M parameters in GPT2, Exchangeable Transformer only has 0.2M parameters, demonstrating
the importance of exchangeability in length generalization. Training on longer lengths improves
posterior predictive accuracy during length generalization to a certain point.
15Figure 6: Approximate posterior draws from autoregressive bootstrap (Algorithm 1). Orange is
oracle, Blue is the model’s forward samples across 100 trajectories and T = 200 forward sampling
steps. The mean of the posterior is yellow, and the mean of the forward sample trajectories is red.
Dim 1 Dim 2 Dim 4 Dim 8
Length 2 10.653 7.959 7.188 12.606
Length 8 0.083 1.087 3.641 6.187
Length 32 0.025 0.064 0.167 0.499
Length 128 0.023 0.048 0.080 0.187
Table 1: Posterior Estimation Optimality Gap (KL Divergence) for Bayesian Linear Regression with
Exchangeable Transformer. Training on different length and dimensions.
6.2 Performance Evaluation
Length generalization Recalling Section 4, we evaluate the model’s ability to generalize to longer
sequences than those seen during pre-training (T > T ). In Figure 5, we plot the optimality gap in
pt
autoregressive loss, which is equal to the KL divergence between the posterior predictive under the
oraclemodelthatknowsthetrueprior(“DGP”)versusthefittedautoregressivesequencemodels. Even
when the pre-training sequence length is extremely short (T = 8), the Exchangeable Transformer
pt
generalizes well at inference time. On the other hand, the original GPT-2 model performs poorly
on longer sequences: removing positional embeddings improves performance, meaning incorrect
positional embeddings can hinder the model’s learning process, more so than having no inductive
bias at all.
Statistical inference For Bayesian linear regression where we consider statistical inference on the
latentparameterθ. ConnectingtoournotationinSection5,wehaveθ = τ⋆ = argmin E[(θ′TX−Y)2]
θ′
and Algorithm 1 computes the ordinary least squares estimator θ(cid:98)b over autoregressively generated
trajectories. InFigure6,wecomparethe(approximate)posteriordrawsofθ(cid:98)basedonourtransformers
with the oracle posterior. Recall a more precise visualization in Figure 1b where we compare the
KL divergence between the oracle posterior and the autoregressively sampled approximation. We
observe a similar trend as in Figure 5 where we see that the Exchangeable Transformer performs
other inductive biases by orders of magnitude, and that simply removing positional embeddings
provides a strong baseline for statistical inference. Motivated by the strong performance of the
Exchangeable Transformer, we perform ablation studies over different covariate dimensions and
pre-training sequence lengths in Table 1. Our results highlight that pre-training recipes for higher
dimensional covariates is an important direction for future work.
167 Discussion
Our work focuses on what type of uncertainty quantification sequence models automatically provide
as a result of pre-training. Our main insight is that autoregressive generative modeling captures
epistemic uncertainty over latent parameters that generate exchangeable sets of documents. Despite
the classical nature of this simple insight due to De Finetti, it appears to be not widely known
in the burgeoning literature on ICL. We hope the explicit connections we make spur subsequent
methodological innovations that expand the scope of uncertainty quantification possible by LLMs.
Ourexperimentsareconfinedtosyntheticsettings. Scalingupinductivebiasesonexchangeability
will likely require substantive engineering innovations. Our heavy reliance on De Finetti’s theorem
for infinitely exchangeable sequences allows us to automatically decompose the model into priors
and likelihoods. In practice, our model can only achieve finite exchangeability, and we have not
accounted for the approximation error this introduces in our theoretical justifications. Addressing
these errors and extending our approach to real-world datasets are important directions for future
research.
Acknowledgements We thank Daniel Russo for insightful discussions. This work was partially
supported by the CBS Digital Future Initiative.
17References
[1] E. Akyürek, D. Schuurmans, J. Andreas, T. Ma, and D. Zhou. What learning algorithm is in-
context learning? investigations with linear models. In Proceedings of the Eleventh International
Conference on Learning Representations, 2023.
[2] Y. Bai, F. Chen, H. Wang, C. Xiong, and S. Mei. Transformers as statisticians: Provable
in-context learning with in-context algorithm selection. In Advances in Neural Information
Processing Systems 36, 2023.
[3] P. Berti, E. Regazzini, and P. Rigo. Well calibrated, coherent forecasting systems. Theory of
Probability & Its Applications, 42(1):82–102, 1998.
[4] P. Berti, L. Pratelli, and P. Rigo. Limit theorems for a class of identically distributed random
variables. Annals of Probability, 32(3):2029 – 2052, 2004.
[5] P. Berti, E. Dreassi, L. Pratelli, and P. Rigo. A class of models for bayesian predictive inference.
Bernoulli, 27(1):702 – 726, 2021.
[6] P. Berti, E. Dreassi, F. Leisen, L. Pratelli, and P. Rigo. Bayesian predictive inference without a
prior. Statistica Sinica, 34(1), 2022.
[7] D. Blei, A. Ng, and M. Jordan. Latent dirichlet allocation. Journal of Machine Learning
Research, 3:993–1022, 2003.
[8] C. Blundell, J. Cornebise, K. Kavukcuoglu, and D. Wierstra. Weight uncertainty in neural
network. In Proceedings of the 32nd International Conference on Machine Learning, pages
1613–1622. PMLR, 2015.
[9] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
P. Shyam, G. Sastry, and A. Askell. Language models are few-shot learners. In Advances in
Neural Information Processing Systems 33, 2020.
[10] D.M.CifarelliandE.Regazzini. Definetti’scontributiontoprobabilityandstatistics. Statistical
Science, 11(4):253–282, 1996.
[11] B. S. Clarke and A. R. Barron. Information-theoretic asymptotics of bayes methods. IEEE
Transactions on Information Theory, 36(3):453–471, 1990.
[12] D. Dai, Y. Sun, L. Dong, Y. Hao, S. Ma, Z. Sui, and F. Wei. Why can GPT learn in-
context? language models secretly perform gradient descent as meta-optimizers. In Findings of
the Association for Computational Linguistics: ACL 2023, pages 4005–4019. Association for
Computational Linguistics, 2023.
[13] A. Dawid and V. Vovk. Prequential probability: principles and properties. Bernoulli, 3:1–38,
1997.
[14] A. P. Dawid. Prequential analysis, stochastic complexity and Bayesian inference. 1992.
[15] B. De Finetti. La prévision: ses lois logiques, ses sources subjectives. In Annales de l’institut
Henri Poincaré, number 1, pages 1–68, 1937.
18[16] B. De Finetti. Theory of probability: A critical introductory treatment, volume 6. John Wiley &
Sons, 2017.
[17] Q. Dong, L. Li, D. Dai, C. Zheng, Z. Wu, B. Chang, X. Sun, J. Xu, and Z. Sui. A survey on
in-context learning. arXiv:2301.00234 [cs.CL], 2022.
[18] B. Efron and R. J. Tibshirani. An Introduction to the Bootstrap. Chapman & Hall, 1993.
[19] W. A. Ericson. Subjective bayesian models in sampling finite populations. Journal of the Royal
Statistical Society Series B: Statistical Methodology, 31(2):195–224, 1969.
[20] B. d. Finetti. Classi di numeri aleatori equivalenti. la legge dei grandi numeri nel caso dei
numeri aleatori equivalenti. sulla legge di distribuzione dei valori in una successione di numeri
aleatori equivalenti. R. Accad. Naz. Lincei, Rf S 6a, 18:107–110, 1933.
[21] E. Fong, C. Holmes, and S. G. Walker. Martingale posterior distributions. Journal of the Royal
Statistical Society, Series B, 2023.
[22] S. Fortini and S. Petrone. Predictive distribution (de f inetti’s view). Wiley StatsRef: Statistics
Reference Online, pages 1–9, 2014.
[23] S. Fortini, L. Ladelli, and E. Regazzini. Exchangeability, predictive distributions and parametric
models. Sankhy¯a: The Indian Journal of Statistics, Series A, pages 86–109, 2000.
[24] S. Garg, D. Tsipras, P. S. Liang, and G. Valiant. What can transformers learn in-context? a
case study of simple function classes. Advances in Neural Information Processing Systems, 35:
30583–30598, 2022.
[25] M. Garnelo, D. Rosenbaum, C. Maddison, T. Ramalho, D. Saxton, M. Shanahan, Y. W.
Teh, D. Rezende, and S. A. Eslami. Conditional neural processes. In Proceedings of the 35th
International Conference on Machine Learning, pages 1704–1713. PMLR, 2018.
[26] A. Gelman, J. Carlin, H. Stern, D. Dunson, A. Vehtari, and D. Rubin. Bayesian Data Analysis.
Chapman & Hall, third edition, 2013.
[27] P. R. Hahn, R. Martin, and S. G. Walker. On recursive bayesian predictive distributions.
Journal of the American Statistical Association, 113(523):1085–1093, 2018.
[28] E. Hewitt and L. J. Savage. Symmetric measures on cartesian products. Transactions of the
American Mathematical Society, 80(2):470–501, 1955.
[29] N. Hollmann, S. Müller, K. Eggensperger, and F. Hutter. Tabpfn: A transformer that solves
small tabular classification problems in a second. In Proceedings of the Eleventh International
Conference on Learning Representations, 2023.
[30] H. J. Jeon, J. D. Lee, Q. Lei, and B. Van Roy. An information-theoretic analysis of in-context
learning. arXiv:2401.15530 [cs.LG], 2024.
[31] L. V. Jospin, H. Laga, F. Boussaid, W. Buntine, and M. Bennamoun. Hands-on bayesian neural
networks—a tutorial for deep learning users. IEEE Computational Intelligence Magazine, 17(2):
29–48, 2022.
19[32] S. Kadavath, T. Conerly, A. Askell, T. Henighan, D. Drain, E. Perez, N. Schiefer, Z. Hatfield-
Dodds, N. DasSarma, E. Tran-Johnson, et al. Language models (mostly) know what they know.
arXiv:2207.05221 [cs.CL], 2022.
[33] B. Kleijn and A. Van der Vaart. The bernstein-von-mises theorem under misspecification.
Electronic Journal of Statistics, 6:354–381, 2012.
[34] Y. Li, M. E. Ildiz, D. Papailiopoulos, and S. Oymak. Transformers as algorithms: Generalization
and stability in in-context learning. In Proceedings of the 40th International Conference on
Machine Learning, pages 19565–19594. PMLR, 2023.
[35] Q. Lyu, K. Shridhar, C. Malaviya, L. Zhang, Y. Elazar, N. Tandon, M. Apidianaki,
M. Sachan, and C. Callison-Burch. Calibrating large language models with sample consistency.
arXiv:2402.13904 [cs.CL], 2024.
[36] S.Müller, N.Hollmann, S.P.Arango, J.Grabocka, andF.Hutter. Transformerscandobayesian
inference. In Proceedings of the Tenth International Conference on Learning Representations,
2022.
[37] T. Nguyen and A. Grover. Transformer neural processes: Uncertainty-aware meta learning via
sequence modeling. In Proceedings of the 39th International Conference on Machine Learning,
2022.
[38] I. Osband, J. Aslanides, and A. Cassirer. Randomized prior functions for deep reinforcement
learning. In Advances in Neural Information Processing Systems 31, volume 31, 2018.
[39] H. V. Roberts. Probabilistic prediction. Journal of the American Statistical Association, 60
(309):50–62, 1965.
[40] H. Teicher. Strong laws for martingale differences and independent random variables. Journal
of Theoretical Probability, 11:979–995, 1998.
[41] K. Tian, E. Mitchell, A. Zhou, A. Sharma, R. Rafailov, H. Yao, C. Finn, and C. D. Manning.
Just ask for calibration: Strategies for eliciting calibrated confidence scores from language
models fine-tuned with human feedback. arXiv:2305.14975 [cs.CL], 2023.
[42] A. W. van der Vaart. Asymptotic Statistics. Cambridge Series in Statistical and Probabilistic
Mathematics. Cambridge University Press, 1998.
[43] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and
I. Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems
30, 2017.
[44] J. Von Oswald, E. Niklasson, E. Randazzo, J. Sacramento, A. Mordvintsev, A. Zhmoginov, and
M. Vladymyrov. Transformers learn in-context by gradient descent. In International Conference
on Machine Learning, pages 35151–35174, 2023.
[45] H. Wang and D.-Y. Yeung. A survey on bayesian deep learning. ACM Computing Surveys, 53
(5):1–37, 2020.
[46] J. Wei, X. Wang, D. Schuurmans, M. Bosma, b. ichter, F. Xia, E. Chi, Q. V. Le, and D. Zhou.
20Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural
Information Processing Systems 35, volume 35, pages 24824–24837, 2022.
[47] J. Wu, D. Zou, Z. Chen, V. Braverman, Q. Gu, and P. Bartlett. How many pretraining tasks are
needed for in-context learning of linear regression? In Proceedings of the Twelveth International
Conference on Learning Representations, 2024.
[48] S. M. Xie, A. Raghunathan, P. Liang, and T. Ma. An explanation of in-context learning as
implicit bayesian inference. arXiv:2111.02080 [cs.CL], 2021.
[49] Y. Zhang, F. Zhang, Z. Yang, and Z. Wang. What and how does in-context learning learn?
bayesian model averaging, parameterization, and generalization. arXiv:2305.19420 [stat.ML],
2023.
21A Proof of Theorem 1
We begin by noting that under the cid condition (5), the random measures p , p are martingales for
t (cid:98)t
∀y, under the filtration generated by Y
1:t
and Y(cid:98)1:t respectively. This is because
E[p (y) | Y ] = E[1{Y = y} | Y ] = p (y), (17)
t 1:t−1 t+1 1:t−1 t−1
where the first equality is due to the tower law and the second equality follows from the c.i.d.
condition. We can also apply this to the p sequence. The martingale convergence theorem gives
(cid:98)t
p (y) → p (y), p (y) → p (y) ∀y,a.s.
t ∞ (cid:98)t (cid:98)∞
for some p (y) and p (y). Since the limit of measurable functions is measurable, we can show that
∞ (cid:98)∞
the limiting quantities are valid random measures over y. By dominated convergence, we have
(cid:90) (cid:90)
p (y)logp (y)dy → p (y)logp (y)dy a.s..
t (cid:98)t ∞ (cid:98)∞
To show the desired result, we use the decomposition
T (cid:90)
1 (cid:88)
logp (Y )− p (y)logp (y)dy
(cid:98)t t+1 ∞ (cid:98)∞
T
t=1
T (cid:18) (cid:90) (cid:19) T (cid:18)(cid:90) (cid:90) (cid:19)
1 (cid:88) 1 (cid:88)
= logp (Y )− p (y)logp (y)dy + p (y)logp (y)dy− p (y)logp (y)dy .
(cid:98)t t+1 t (cid:98)t t (cid:98)t ∞ (cid:98)∞
T T
t=1 t=1
The second average converges to zero since individual elements converges to zero; it now suffices to
show that the first average converges to zero. Evidently, the sequence
(cid:90)
D = logp (Y | Y )− p (y)logp (y)dy = logp (Y | Y )−E[logp (Y ) | Y ]
t (cid:98)t t+1 1:t t (cid:98)t (cid:98)t t+1 1:t (cid:98)t t+1 1:t
is a martingale difference sequence adapted to the filtration F generated by Y . We use the
t 1:t
following basic lemma due to Teicher [40, Corollary 2].
Lemma 2 (SLLN for martingale differences). Let {M } be an L2-martingale, and let {D } be the
t t
corresponding martingale difference sequence. If E[D2] < ∞ and condition (7) holds, then M /t → 0
t t
almost surely.
By Lemma 2, we have 1 (cid:80)T D → 0 almost surely.
T t=1 t
B Proof of Theorem 2
Since Y are i.i.d. under Q and p , we have the following decomposition of the log likelihood ratio
1:T (cid:98)θ
(cid:20) (cid:21)
1
E [logq(Y )−logp(Y )]
Y1:T∼Q
T
1:T (cid:98) 1:T
(cid:20) (cid:21)
1
= E [logq(Y )−logp (Y )+logp (Y )−logp(Y )]
Y1:T∼Q
T
1:T (cid:98)θ⋆ 1:T (cid:98)θ⋆ 1:T (cid:98) 1:T
(cid:20) (cid:21)
(cid:16) (cid:17) 1
= D
kl
Q||P(cid:98)θ⋆ +E
Y1:T∼Q
T[logp (cid:98)θ⋆(Y 1:T)−logp (cid:98)(Y 1:T)] .
22Our proof for the convergence of E [logp (Y )−logp(Y )] is inspired by that of Clarke
Y1:T∼Q (cid:98)θ⋆ 1:T (cid:98) 1:T
and Barron [11], but we make rexquisite modifications to properly bound error terms.
We prove the convergence for any instantiation y of the random variables Y . Following a
1:T 1:T
similar notation as Clarke and Barron [11], define
B(θ(cid:98),δ) = {θ : ∥θ−θ(cid:98)∥
V θ⋆
≤ δ} a neighborhood of θ(cid:98) where ∥θ∥2
V θ⋆
:= θV θ⋆θT
and the modulus of continuity of the log prior
(cid:12) (cid:12)
ρ(δ,θ⋆) = sup (cid:12) (cid:12)log π(θ) (cid:12) (cid:12).
(cid:12) π(θ⋆)(cid:12)
θ∈B(θ⋆,δ)
For fixed K > d, we will consider the radius δ = (cid:112) K/T. The standardized score funciton is again
T
defined by
T
1 (cid:88)
S (θ) := √ ℓ˙ (Y ) where ℓ˙ (y) := ∇ logp (y).
T θ t θ θ (cid:98)θ
T
t=1
Define the moving estimator θ(cid:98)
θ(cid:98)= θ⋆+ √1 V θ− ⋆1S T1(cid:8) S TTV θ− ⋆1S
T
≤ K(cid:9) (18)
T
where for notational simplicity, we omit the dependence on θ⋆ in the S , i.e., S := S (θ⋆). Note
T T T
(cid:13) (cid:13)
that by definition, (cid:13) (cid:13)θ(cid:98)−θ⋆(cid:13)
(cid:13)
≤ δ T.
V θ⋆
Controlling the log likelihood difference by restricting to the neighborhood B(θ(cid:98),δ T)
In the following lemma, we bound the difference between the log likelihoods by focusing on a
neighborhood of θ(cid:98)where the posterior is concentrated. We defer its proof to Section B.1.
Lemma 3. Denote the truncated normal centered at θ(cid:98)as
(cid:18) (cid:19)
1 T (cid:110) (cid:111)
ϕ T(θ) =
c
exp − 2∥θ−θ(cid:98)∥2
V θ⋆
1 θ ∈ B(θ(cid:98),δ T) (19)
T
where c
T
:=
(cid:82) B(θ(cid:98),δT)e−(T/2)∥θ−θ(cid:98)∥2
Vθ⋆dθ. Then, we have the bound
1 1
logp (y )−logp(y ) ≤ log +ρ(2δ ,θ⋆)+ STV−1S
(cid:98)θ⋆ 1:T (cid:98) 1:T c T 2 T θ⋆ T
T
(cid:90) (cid:18) p (y ) T √ (cid:19)
+ log (cid:98)θ⋆ 1:T − ∥θ−θ⋆∥2 − T(θ⋆−θ)TS ϕ (θ)dθ
p (y ) 2 V θ⋆ T T
B(θ(cid:98),δT) (cid:98)θ 1:T
+ 3 STV−1S 1(cid:8) STV−1S > K(cid:9) +log 1 . (20)
2 T θ⋆ T T θ⋆ T π(θ⋆)
We proceed by bounding each term in the inequality (20).
Bounding the leading term log 1 We can upper bound c using the normalizing constant of
the normal distribution
cT T
c ≤
(2π)d/2T−d/2detV−1/2
.
T θ⋆
23The lower bound follows from Chebyshev’s inequality. Consider θ as a Gaussian random variable
centered around θ(cid:98)with covariance (TV θ⋆)−1. Then by Chebyshev’s inequality,
(cid:90)
e−(T/2)∥θ−θ(cid:98)∥2 Vθ⋆dθ = ((2π)d/2T−d/2detV θ− ⋆1/2 )·P(θ ∈ B(θ(cid:98),δ T))
B(θ(cid:98),δT)
= ((2π)d/2T−d/2detV θ− ⋆1/2 )(1−P(∥θ−θ(cid:98)∥
V θ⋆
≥ δ T))
≥
((2π)d/2T−d/2detV−1/2
)·(1−d/K)
θ⋆
Therefore, we have that
(1−d/K)(2π)d/2T−d/2detV−1/2
≤ c ≤
(2π)d/2T−d/2detV−1/2
,
θ⋆ T θ⋆
or equivalently,
1 d T 1 K
0 ≤ log − log − logdetV ≤ log . (21)
θ⋆
c 2 2π 2 K −d
T
Controlling the pdf ϕ (θ) (19) To bound the integral term in the inequality (20), note that for
T
any θ ∈ B(θ(cid:98),δ T), we have that
(cid:18)(cid:13) (cid:13) (cid:13) (cid:13) (cid:19)2 K
∥θ−θ⋆∥2 ≤ (cid:13)θ−θ(cid:98)(cid:13) +(cid:13)θ(cid:98)−θ⋆(cid:13) ≤ ∥θ−θ(cid:98)∥2 +4δ2 = ∥θ−θ(cid:98)∥2 +4 .
V θ⋆ (cid:13) (cid:13) V θ⋆ (cid:13) (cid:13) V θ⋆ V θ⋆ T V θ⋆ T
We can then bound the density ϕ T(θ) which is centered around θ(cid:98)with the above density centered
around θ⋆ by
(cid:18) (cid:19)
T (cid:110) (cid:111)
ϕ T(ϕ) = 1/c
T
·exp − 2∥θ−θ(cid:98)∥2
V θ⋆
1 θ ∈ B(θ(cid:98),δ T)
(cid:18) (cid:19)
T
≤ 1/c ·e2K ·exp − ∥θ−θ⋆∥2 1{θ ∈ B(θ⋆,2δ)}
T 2 V θ⋆
≤ (1−d/K)−1e2Kϕ⋆(θ)
T
where the density ϕ⋆ = N(θ⋆,(TV )−1), and the last inequality follows from the lower bound of c .
T θ⋆ T
Hence, the integral term in the inequality (20) can be bounded by
1−e2 dK
/K
(cid:90) ϕ⋆ T(θ)(cid:12) (cid:12) (cid:12) (cid:12)log p (cid:98) pθ⋆ (( yy 1:T )) − T 2∥θ−θ⋆∥2
V θ⋆
−√ T(θ⋆−θ)TS T(cid:12) (cid:12) (cid:12) (cid:12)dθ. (22)
B(θ⋆,2δT) (cid:98)θ 1:T
Controlling the integrand in the bound (20) via Taylor expansion Noting that y are
1:T
i.i.d. under p , we can also break down the integrand
(cid:98)θ
(cid:12) (cid:12) (cid:12) (cid:12)log p (cid:98) p (cid:98)θ θ⋆ (( yy 11 :: TT )) −√ T(θ⋆−θ)TS T − T 2 ∥θ−θ⋆∥2 V θ⋆(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)
=
(cid:12) (cid:12)(cid:88)T
log
p (cid:98)θ⋆(y t) −√
T(θ⋆−θ)TS −
T
∥θ−θ⋆∥2
(cid:12)
(cid:12)
(cid:12)
(cid:12)
p (cid:98)θ(y t) T 2 V θ⋆(cid:12)
(cid:12)
t=1
(cid:12) (cid:12)
T
≤
(cid:12) (cid:12)(cid:88)
log
p (cid:98)θ⋆(y t)
−(θ⋆−θ)Tℓ˙ (y )−
1
(θ⋆−θ)T∇2logp (y
)(θ⋆−θ)(cid:12)
(cid:12)
(cid:12) p (y ) θ⋆ t 2 (cid:98)θ⋆ t (cid:12)
(cid:12) (cid:98)θ t (cid:12)
t=1
(cid:12) (cid:12)
T
+ 1 (cid:12) (cid:12)(θ⋆−θ)T (cid:88)(cid:0) ∇2logp (y )−V (cid:1) (θ⋆−θ)(cid:12) (cid:12).
2 (cid:12) (cid:98)θ⋆ t θ⋆ (cid:12)
(cid:12) (cid:12)
t=1
24The first term in the preceding bound can be directly controlled via a second order Taylor expansion
of the log likelihood
(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12)log p (cid:98) pθ⋆ (( yy t )) −(θ⋆−θ)Tℓ˙ θ⋆(y t)− 1 2(θ⋆−θ)T∇2logp (cid:98)θ⋆(y t)(θ⋆−θ)(cid:12) (cid:12)
(cid:12)
= o(∥θ−θ⋆∥2
V
θ⋆).
(cid:98)θ t
To bound the second term, note that there exists a universal constant c > 0 such that
(cid:12) (cid:32) T (cid:33) (cid:12) (cid:13) T (cid:13)
(cid:12) 1 (cid:88) (cid:12) (cid:13)1 (cid:88) (cid:13)
(cid:12)(θ⋆−θ)T ∇2logp (y )−V (θ⋆−θ)(cid:12) ≤ ∥θ⋆−θ∥2·(cid:13) ∇2logp (y )−V (cid:13)
(cid:12) T (cid:98)θ⋆ t θ⋆ (cid:12) 2 (cid:13)T (cid:98)θ⋆ t θ⋆(cid:13)
(cid:12) (cid:12) (cid:13) (cid:13)
t=1 t=1 2
(cid:13) (cid:13)
T
(cid:13)1 (cid:88) (cid:13)
≤ c·d·∥θ⋆−θ∥2 ·(cid:13) ∇2logp (y )−V (cid:13)
V θ⋆ (cid:13)T (cid:98)θ⋆ t θ⋆(cid:13)
(cid:13) (cid:13)
t=1 2
(cid:13) (cid:13)
T
(cid:13)1 (cid:88) (cid:13)
≤ c·d·δ2 (cid:13) ∇2logp (y )−V (cid:13) .
T (cid:13)T (cid:98)θ⋆ t θ⋆(cid:13)
(cid:13) (cid:13)
t=1 2
Collecting these approximations and plugging them into our previous bound (22), we arrive at
(cid:90) (cid:18) p (y ) T √ (cid:19)
log (cid:98)θ⋆ 1:T − ∥θ−θ⋆∥2 − T(θ⋆−θ)TS ϕ (θ)dθ (23)
p (y ) 2 V θ⋆ T T
B(θ(cid:98),δT) (cid:98)θ 1:T
e2K (cid:32) cdK (cid:13) (cid:13)1 (cid:88)T (cid:13) (cid:13) (cid:33)
≤ d·o(1)+ ·(cid:13) ∇2logp (y )−V (cid:13) . (24)
1−d/K 2 (cid:13)T (cid:98)θ⋆ t θ⋆(cid:13)
(cid:13) (cid:13)
t=1 2
Setting K large to control the final approximation error Returning to the inequality (20)
in Lemma 3, consider the approximation error
(cid:18) (cid:19)
d T 1 1 1
R := logp (y )−logp(y )− log +log + logdet(V )− STV−1S .
T (cid:98)θ⋆ 1:T (cid:98) 1:T 2 2π π(θ⋆) 2 θ⋆ 2 T θ⋆ T
So far, we have shown
(cid:32) (cid:114) K (cid:33) Ke2K (cid:32) (cid:13) (cid:13)1 (cid:88)T (cid:13) (cid:13) (cid:33)
R ≤ ρ 2 ,θ⋆ + d·o(1)+dK(cid:13) ∇2logp (y )−V (cid:13)
T T K −d (cid:13)T (cid:98)θ⋆ t θ⋆(cid:13)
(cid:13) (cid:13)
t=1 2
3 (cid:110) (cid:111)
+ S⊤V−1S 1 S⊤V−1S > K .
2 T θ⋆ T T θ⋆ T
Fixanarbitrarilysmallϵ > 0. SinceS⊤V−1S = O(1)bythestronglawoflargenumbers,wecanpick
T θ⋆ T
K largeenoughsuchthatthereexistsT suchthatforallT ≥ T , 3S⊤V−1S 1(cid:8) S⊤V−1S > K(cid:9) ≤ ϵ.
0 0 2 T θ⋆ T T θ⋆ T
For such choice of K, uniform positivity and continuity of π(·) guarantees that
(cid:32) (cid:114) (cid:33)
K
ρ 2 ,θ⋆ → 0 as T → ∞.
T
This gives the desired result.
B.1 Proof of Lemma 3
We begin by rewriting the log likelihood ratio as
p (y )π(θ⋆) (cid:90) p (y )π(θ)
log
(cid:98)θ⋆ 1:T
=−log
(cid:98)θ 1:T
dθ
p(y ) p (y )π(θ⋆)
(cid:98) 1:T (cid:98)θ⋆ 1:T
25(cid:90)
p (y )π(θ)
≤−log
(cid:98)θ 1:T
dθ
p (y )π(θ⋆)
B(θ(cid:98),δT) (cid:98)θ⋆ 1:T
=−log(cid:90) c
T
·p (cid:98)θ(y 1:T)exp(T/2·∥θ−θ(cid:98)∥2
V
θ⋆)π(θ)
ϕ (θ)dθ.
p (y )π(θ⋆) T
B(θ(cid:98),δT) (cid:98)θ⋆ 1:T
We use Jensen’s inequality to put the log inside the integral
log p (cid:98)θ⋆(y 1:T)π(θ⋆) ≤−log(cid:90) c T ·p (cid:98)θ(y 1:T)exp(T/2·∥θ−θ(cid:98)∥2 V θ⋆)π(θ) ϕ (θ)dθ (25)
p(y ) p (y )π(θ⋆) T
(cid:98) 1:T B(θ(cid:98),δT) (cid:98)θ⋆ 1:T
(cid:12) (cid:12) (cid:90) (cid:18) (cid:19)
≤−logc T + sup (cid:12) (cid:12) (cid:12)log ππ (( θθ ⋆) )(cid:12) (cid:12) (cid:12)+ log p (cid:98) pθ⋆ (( yy 1:T )) − T 2∥θ−θ(cid:98)∥2 V θ⋆ ϕ T(θ)dθ
θ∈B(θ(cid:98),δT) B(θ(cid:98),δT) (cid:98)θ 1:T
(cid:90) (cid:18) (cid:19)
p (y ) T
≤−logc
T
+ρ(2δ T,θ⋆)+ log (cid:98) pθ⋆ (y1:T
)
− 2∥θ−θ(cid:98)∥2
V θ⋆
ϕ T(θ)dθ
B(θ(cid:98),δT) (cid:98)θ 1:T
wherethelastinequalityfollowsformtriangleinequality∥θ−θ⋆∥
V θ⋆
≤ ∥θ−θ(cid:98)∥
V
θ⋆+∥θ(cid:98)−θ⋆∥
V θ⋆
≤ 2δ T.
Using the definition of θ(cid:98)(18), we can rewrite ∥θ−θ(cid:98)∥2
V θ⋆
T 2∥θ(cid:98)−θ∥2 V θ⋆ =T 2 (cid:13) (cid:13) (cid:13) (cid:13)θ−θ⋆− √1 TV θ− ⋆1S T1(cid:8) S TTV θ− ⋆1S T ≤ K(cid:9)(cid:13) (cid:13) (cid:13) (cid:13)2
V θ⋆
T √ 1
= ∥θ−θ⋆∥2 + T(θ⋆−θ)TS + STV−1S
2 V θ⋆ T 2 T θ⋆ T
(cid:18)√
1
(cid:19)
− T(θ⋆−θ)TS + STV−1S 1
T 2 T θ⋆ T S TTV θ− ⋆1ST>K
by simply expanding the norm term.
Then by Cauchy-Schwarz, under the event S TTV θ− ⋆1S
T
> K and θ ∈ B(θ(cid:98),δ T)
√ √ √
T|(θ−θ(cid:98))TS T| ≤ √T∥θ−θ(cid:98)∥
V θ⋆
·∥S T∥
V θ− ⋆1
≤ Tδ T(S TTV θ− ⋆1S T)1/2
= K(STV−1S )1/2 ≤ STV−1S
T θ⋆ T T θ⋆ T
Putting this together, we have the inequality
T T √ 1 3
2∥θ−θ(cid:98)∥2
V θ⋆
≥ 2∥θ−θ⋆∥2
V θ⋆
+ T(θ⋆−θ)TS
T
+ 2S TTV θ− ⋆1S
T
− 2S TTV θ− ⋆1S T1(S TTV θ− ⋆1S
T
> K)
Substituting this inequality into the previous bound on the likelihood ratio (25), conclude
p (y )π(θ⋆)
log(cid:98)θ⋆ 1:T
p(y )
(cid:98) 1:T
1
≤−logc +ρ(2δ ,θ⋆)+ STV−1S
T T 2 T θ⋆ T
(cid:90) (cid:18) p (y ) T √ (cid:19)
+ log (cid:98)θ⋆ 1:T − ∥θ−θ⋆∥2 − T(θ⋆−θ)TS ϕ (θ)dθ
p (y ) 2 V θ⋆ T T
B(θ(cid:98),δT) (cid:98)θ 1:T
3
+ STV−1S 1(STV−1S > K).
2 T θ⋆ T T θ⋆ T
26C One-layer Transformer for Bayesian Linear Regression
C.1 Proof of Lemma 1
We replicate the proof of Wu et al. [47] for completeness. Recalling the input matrix Z (9), we drop
t
the subscript to ease notation. By definition, the output of the self-attention layer is given by
(cid:20) (cid:21)
1
y = Z+ (VZ)(QZ)⊤(KZ)
(cid:98)t
n
d+1,t
(cid:18) (cid:19)
1
= e⊤ Z+ VZZ⊤Q⊤KZ e
d+1 n t
1
= 0+ e⊤ VZZ⊤Q⊤KZe
t−1 d+1 t
(cid:32)(cid:34) (cid:35)(cid:33)
1 X⃗⊤ X⃗ +x xt X⃗⊤ Y⃗ (cid:20) x (cid:21)
= (e⊤ V) t−1 t−1 t t t−1 t−1 Q⊤K t
t−1 d+1 Y⃗⊤ X⃗ Y⃗⊤ Y⃗ 0
t−1 t−1 t−1 t−1
The key assumption is that the bottom left 1×d blocks of the V and QTK matrices are fixed 0,
that is
(cid:20) (cid:21) (cid:20) (cid:21)
∗ ∗ W ∗
V = and QTK =
0 v 0 ∗
where v is a scalar and W is a d×d matrix. This implies that the output of the transformer is
1
y = Y⃗⊤ X⃗ Wv⊤x .
(cid:98)t t t−1 t−1 t
Letting Γ⊤ := Wv⊤, we have the desired reparameterization.
C.2 Proof of Proposition 3
We start with the usual chain rule for KL divergences
1
E [logq(Y | X )−logp(Y | X )]
Q 1:T 1:T (cid:98) 1:T 1:T
T
T (cid:20) (cid:21)
= 1 (cid:88) E E log q(Y t | X 1:t,Y 1:t−1) | X ,Y
Q Q 1:t 1:t−1
T p(Y | X ,Y )
(cid:98) t 1:t 1:t−1
t=1
T
1 (cid:88) (cid:104) (cid:16) (cid:17)(cid:105)
= E D N(w⊤X ,σ2)||N(µ (X ),σ2)
T Q kl q t (cid:98)t t (cid:98)t
t=1
=
1 (cid:88)T
E
(cid:34) (cid:18) σ2 −1(cid:19)
+log
σ (cid:98)t2
+
1 (cid:18)
w⊤X −
1
Y⃗⊤ X⃗ Γ⊤X
(cid:19)2(cid:35)
.
2T Q σ2 σ2 σ2 q t t t−1 t−1 t
t=1
(cid:98)t (cid:98)t
Inthefinalequality, weusedtheformulaD (cid:0) N(µ ,v2)||N(µ ,v2)(cid:1) = log v2+ 1 (v2−v2)+ 1 (µ −
kl 1 1 2 2 v1 2v 22 1 2 2v 22 1
µ )2. Recall the autoregressive mean (10) and the definition (12) of the autoregressive (posterior)
2
variance σ = σ2+XTA−1 X . Since X is independent of A ,Y⃗ ,X⃗ , conclude
(cid:98)t t t−1 t t t−1 t−1 t−1
1
E [logq(Y | X )−logp(Y | X )]
Q 1:T 1:T (cid:98) 1:T 1:T
T
27=
1 (cid:88)T
E
(cid:34) −X⊤A t−1X +log(cid:18)
1+
X⊤A t−1X(cid:19)
+
1 (cid:18)
w⊤X −
1
Y⃗⊤ X⃗
Γ⊤X(cid:19)2(cid:35)
.
2T Q σ2+X⊤A X σ2 σ2+X⊤A X q t t−1 t−1
t−1 t−1
t=1
To show the limit, note that since A is positive semidefinite, the integrand in the preceding
t−1
display (which is clearly positive) is bounded above by
X⊤A X 1 (cid:18) 1 (cid:19)2
0+ t−1 + w⊤X − Y⃗⊤ X⃗ Γ⊤X ,
σ2 σ2 q t t−1 t−1
which is evidently integrable. Applying dominated convergence and noting that X⊤A X a →.s. 0 and
t−1
1Y⃗⊤ X⃗ a →.s. E [YX⊤] = E [(w⊤X +ε)X⊤] = w⊤H, we have shown a stronger result that each
t t−1 t−1 Q Q q q
summand converges to
(cid:20) (cid:21)
(cid:16) (cid:17)2
σ−2E w⊤X −w⊤HΓ⊤X = σ−2w⊤(I −HΓ⊤)H(I −HΓ⊤)⊤w .
q q q q
D Regularization
Recall we want to "enforce" the exchangeability/cid condition (5) on our fitted model by adding
a penalty term to the loss function, as a form of regularization. Specifically, we could consider
regularizing with the KL divergence between the one- vs. two-step forward prediction (16). In
the contextual setting with realizations xi as covariates and yi as observables, the regularized
1:T 1:T
training objective for the i-th sequence is
T−1
(cid:88)
logp (Yi = yi |Xi = xi ,Yi = yi ,Xi = xi )
(cid:98)t t+1 t+1 1:t 1:t 1:t 1:t t+1 t+1
(cid:124) (cid:123)(cid:122) (cid:125)
t=0
autoregressivelossterms
 
 
 
T−2
(cid:88)  
+λ· D  p (yi |xi ,yi ,xi ) || p(Yi = yi | Xi = xi ,Yi = yi ,Xi = xi ) 
kl (cid:98)t t+1 1:t 1:t t+1 (cid:98) t+2 t+1 1:t 1:t 1:t 1:t t+2 t+1 
 (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) 
t=0  one-step forward pred two-step forward pred 
 
(same as autoreg loss terms) (if yi ,xi had been observed two steps forward)
t+1 t+1
where the λ is a hyperparameter for the KL term.
Since the term p(Yi = yi | Xi = xi ,Yi = yi ,Xi = xi ) is not directly obtainable
(cid:98) t+2 t+1 1:t 1:t 1:t 1:t t+2 t+1
from an autoregressive sequence model, we estimate it in the following derivation.
Assuming that this term is a normal distribution, the KL divergence between two normal
(cid:16) (cid:17)
distributions is D (cid:0) N(µ ,σ2)||N(µ ,σ2)(cid:1) = 1 σ−1σ −1+σ−1(µ −µ )2+ln σ2 . First, we take
kl 1 1 2 2 2 2 1 2 2 1 σ1
a Monte Carlo estimate of the two-step conditional mean. Again, for realizations xi as covariates
1:T
and yi as observables, using the tower property
1:T
(cid:90) (cid:90)
p(Y = y | x ,y ,X = x) = p (y | x ,y ,x ,Y = ζ,X = x)
(cid:98) t+2 1:t 1:t t+2 (cid:98)t+2 1:t 1:t t+1 t+1 t+2
·p (ζ | x ,y ,X = x )·p (x )dζdx ,
(cid:98)t+1 1:t 1:t t+1 t+1 X t+1 t+1
Based on the above equatoin, rewrite the two-step conditional mean as
(cid:90) (cid:90) (cid:18)(cid:90) (cid:19)
E (cid:98)[Y
t+2
| x 1:t,y 1:t,X
t+2
= x] = ϕ(cid:0) µ t+2(x t+1,ζ,x),σ t2 +2(x t+1,ζ,x)(cid:1) (y)·ydy
28·ϕ(cid:0) µ (x ),σ2 (x )(cid:1) (ζ)·ϕ(x )dx dζ
t+1 t+1 t+1 t+1 t+1 t+1
(cid:90) (cid:90)
= µ (x ,ζ,x)·ϕ(cid:0) µ (x ),σ2 (x )(cid:1) (ζ)
t+2 t+1 t+1 t+1 t+1 t+1
·ϕ(x )dζdx
t+1 t+1
(cid:90) (cid:90)
= µ (x ,ζ,x)·p (ζ | X ,Y ,X = x )
t+2 t+1 (cid:98)t+1 1:t 1:t t+1 t+1
·P (x )dζdx
Xt+1 t+1 t+1
= E E [µ (X ,ζ,x)]
t+2 t+1
Xt+1 ζ∼p (cid:98)t+1(|Xt+1)
where ϕ denotes the density of the normal distribution, and the equality follows from interchangebale
intergrals. From the above equation, we can draw X samples from a fixed covariate distribution
t+1
P , and then draw ζ samples given this x from the distribution predicted by model.
X t+1
Similarly, we can approximate the two-step conditional variance
(cid:90) (cid:90) (cid:18)(cid:90) (cid:19)
E (cid:98)(cid:2) Y t2
+2
| X 1:t,Y 1:t,X
t+2
= x(cid:3) = ϕ(cid:0) µ t+2(x t+1,ζ,x),σ t2 +2(x t+1,ζ,x)(cid:1) (y)·y2dy
·ϕ(cid:0) µ (x ),σ2 (x )(cid:1) (ζ)·ϕ(x )dx dζ
t+1 t+1 t+1 t+1 t+1 t+1
(cid:90) (cid:90)
= (cid:0) µ2 (x ,ζ,x)+σ2 (x ,ζ,x)(cid:1)
t+2 t+1 t+2 t+1
·ϕ(cid:0) µ (x ),σ2 (x )(cid:1) (ζ)·ϕ(x )dζdx
t+1 t+1 t+1 t+1 t+1 t+1
= E E [µ2 (X ,ζ,x)+σ2 (X ,ζ,x)]
t+2 t+1 t+2 t+1
Xt+1 ζ∼p (cid:98)t+1(|Xt+1)
E Experiments Details
Model Architectures
GPT2
• Model dimension: 1
• Number of embeddings layers: 4
• Feed forward dimension: 128
• Number of attention heads: 4
• Number of transformer layers: 12
• Batch size: 32
• Number of training steps: 30000
• Learning rate: 1e−4 with Cosine annealing scheduler
For the transformers without positional embedding, we modified the Huggingface Transformer’s
GPT2 architecture to remove positional embeddings by setting them to [0,0,0,...,0]. Additionally,
we experimented with different positional embeddings [sin,0101], but found the results inconclusive
and not critical to our theory. The GPT2 architecture comprises 9 million parameters, whereas the
Exchangeable Transformer has 220 thousand parameters. We also experimented across different
number of layers [ 6, 12 ] and heads for GPT2.
29Exchangeable Transformer
• Model dimension: 1
• Number of embeddings layers: 4
• Feed forward dimension: 128
• Number of attention heads: 4
• Number of transformer layers: 12
• Batch size: 32
• Number of training steps: 30000
• Learning rate: 1e−4 with Cosine annealing scheduler
TNP, PFN, and Exchangeable Transformer
We construct each token through concatenating (x ,y ) to preserve the pair wise structure of feature-
i i
label pairs. As in TNP, we also make use of auxiliary tokens consisting of (x ,0). Not only does
i
doing so remove the y label from the points that the model will learn to predict, but having such
tokens - when combined with the attention scheme illustrated in Fig. 5 - preserves the autoregressive
structure of our tasks. Deviating from Nguyen et al’s work, we remove the context points which were
allowed to attend to each other in TNP. We do so as our aim is to investigate autoregressive loss,
which begins at predicting the 1st label while conditioning on no previous context. Removing the
initial context also allows for a full examination of our model’s ability to encapsulate a Bayesianprior,
as the prior is more pronounced than the likelihood when a Bayesianstatistician makes predictions
based on a few or no context. We further extend our contribution by augmenting TNP’s attention
mechanism. We adjust the attention mask such that each auxiliary/padded token attends to itself.
We introduce this improvement as this adaptation enables the model to access query value pairs
associated with the current token’s index, whereas previously the model was only allowed to query
information at previous indices. We observe that this improvement in informational access offers
performance improvements, especially in short/zero context predictions. We posit that allowing for
self attention helps the model embody knowledge for predicting each point, rather than forming
predictions solely based on context.
Training Compute
Training was conducted on 8x A100 GPUs. The CID-Regularizer, with its Monte Carlo Sampling
for KL Divergence computation, necessitated parallel computation. All code was implemented
in PyTorch, with data generated from deterministic and random seeds to average results across
trajectories.
Parameter Inference
For each batch, we draw one batch of X from a standard normal distribution for Bayesian Linear
Regression and a uniform distribution [−2,2] for Gaussian Process to ensure stability. This X is
then passed into the BLR or Gaussian Process function, with each batch of sequential data drawn
from the same function/coefficient.
30CID Regularizer
MonteCarloSamplingwasusedtocomputetheKLDivergenceforCIDRegularizer. Weexperimented
with λ values [0.001,0.1,1,10,100] and Monte Carlo samples [5,10,50,100], selecting λ = 0.1 and
M = 50 based on the low validation loss.
Permuted Data
We permute the data by first drawing a sequence of data, and then permuting it across the sequence.
We tested data permutations of [16,32,64] and chose 32, as it showed no significant difference and
matched our batch size.
31