Compress and Compare: Interactively Evaluating Efficiency and
Behavior Across ML Model Compression Experiments
AngieBoggust*† ,VenkateshSivaraman*† ,YannickAssogba ,DonghaoRen ,
DominikMoritz ,andFredHohman
D Performance Comparison
Evaluate behaviors and layers
B Model Scatterplot
A Model Map Compare top-level metrics
Visual overview of

compression experiments
C Selection Details
Automatically identify
model differences
amount
Fig.1:COMPRESSANDCOMPAREhelpsMLpractitionersanalyzeandcomparecompressionexperiments.The(A)ModelMapdisplays
anoverviewofthecompressedmodelsandtheexperimentaloperationsusedtocreatethem.Userscancomparemodels’top-level
metricsinthe(B)ModelScatterplotandtheiroperationaldifferencesinthe(C)SelectionDetailsview.Tocomparemodels’behaviors
andinternallayercharacteristics,userscanvisitthe(D)PerformanceComparisonviewsshowninFig.5.
Abstract—Todeploymachinelearningmodelson-device,practitionersusecompressionalgorithmstoshrinkandspeedupmodels
whilemaintainingtheirhigh-qualityoutput.Acriticalaspectofcompressioninpracticeismodelcomparison,includingtrackingmany
compressionexperiments, identifyingsubtlechangesinmodelbehavior, andnegotiatingcomplexaccuracy-efficiencytrade-offs.
However,existingcompressiontoolspoorlysupportcomparison,leadingtotediousand,sometimes,incompleteanalysesspread
acrossdisjointtools.Tosupportreal-worldcomparativeworkflows,wedevelopaninteractivevisualsystemcalledCOMPRESSAND
COMPARE.Withinasingleinterface,COMPRESSANDCOMPAREsurfacespromisingcompressionstrategiesbyvisualizingprovenance
relationshipsbetweencompressedmodelsandrevealscompression-inducedbehaviorchangesbycomparingmodels’predictions,
weights,andactivations.WedemonstratehowCOMPRESSANDCOMPAREsupportscommoncompressionanalysistasksthroughtwo
casestudies,debuggingfailedcompressionongenerativelanguagemodelsandidentifyingcompressionartifactsinimageclassification
models. WefurtherevaluateCOMPRESSANDCOMPAREinauserstudywitheightcompressionexperts,illustratingitspotentialto
providestructuretocompressionworkflows,helppractitionersbuildintuitionaboutcompression,andencouragethoroughanalysisof
compression’seffectonmodelbehavior.Throughtheseevaluations,weidentifycompression-specificchallengesthatfuturevisual
analyticstoolsshouldconsiderandCOMPRESSANDCOMPAREvisualizationsthatmaygeneralizetobroadermodelcomparisontasks.
IndexTerms—Efficientmachinelearning,modelcompression,visualanalytics,modelcomparison
1 INTRODUCTION
Machinelearning(ML)modelshavedramaticallyincreasedinscale
• *Authorscontributedequally. overthepastseveralyears,withpublishedmodelsrisingfrom1billion
• †WorkdoneatApple. parametersin2018toover100billionparametersasof2024[17,59].
• AngieBoggustiswiththeMassachusettsInstituteofTechnology.E-mail: Thistrendhasproducedmodelswithexcitingemergentcapabilities
aboggust@csail.mit.edu thathaveenablednewuserexperiences,likereal-timetranslation[44]
• VenkateshSivaramaniswithCarnegieMellonUniversity.E-mail: and code generation [13]. However, this scale also incurs greater
venkats@cmu.edu technical,financial,andenvironmentalcoststointegratethesemodels
• YannickAssogba,DonghaoRen,DominikMoritz,andFredHohmanare intoeverydayuse[4]. Asaresult,modelcompressionhasemerged
withApple.E-mail:{yassogba,donghao,domoritz,fredhohman}@apple.com asanessentialfamilyoftechniquestomakelargemodelsviablefor
Manuscriptreceivedxxxxx.201x;acceptedxxxxx.201x.DateofPublication practicalusecases,particularlyindomainswheremodelsmustrunon
xxxxx.201x;dateofcurrentversionxxxxx.201x.Forinformationon end-userdevicestolowerlatencyoraccessprivateuserdata[24].
obtainingreprintsofthisarticle,pleasesende-mailto:reprints@ieee.org.
DigitalObjectIdentifier:xx.xxxx/TVCG.201x.xxxxxxx
4202
guA
6
]CH.sc[
1v47230.8042:viXraMLpractitionersapplycompressionwiththeintenttomaintainthe individualparameter,(2)pruningremovesparameterswhileoptionally
accuracyofalargemodelwhilereducingthespacerequiredtostoreit adjustingtheotherstocompensate,and(3)factorizationanddistillation
andthetimerequiredtoperforminference.However,whichcompres- findadifferentsetofparametersthatmimicthebehavioroftheoriginal
siontechniqueorcombinationoftechniqueswillachievethisbalance model[8].Themoststraightforwardinstantiationsofthesetechniques
remainstask-andmodel-specific[24].TheMLliteraturehasproposed arequantization(convertinghigh-precisionformatslike32-bitfloatsto
variouscompressiontechniquesfordifferentmodelarchitecturesand lower-precisionformatslike8-bitintegers)andunstructuredmagnitude
userpriorities, suchaslowspaceconsumption, lowlatency, orfast pruning(zero-ingoutweightswiththesmallestabsolutevalues).These
executiononoptimizedhardware[11,12,16,66].Nevertheless,iden- foundationaltechniquesformthestartingpointformanyreal-world
tifyingtherightcompressionstrategycanrequireanywherefroma compressionstrategiesbecausetheyperformwellinpractice,areeasy
fewtoseveraldozenexperiments[24],takingtimethatisoftennot tounderstand,andarestraightforwardtocompute[24,37].Additional
accountedforinaccuracy-focusedmodeldevelopmenttimelines.Itcan routinesoftenemployedtotunetheresultingcompressedmodelin-
alsobechallengingtocommunicateexperimentalresultswithinand cludefine-tuning(trainingthemodelonadatasubset)andcalibration
acrossteams,particularlythosewithvaryingMLexpertise.Evenwhen (adjustingmodelparameterstocompensateforcompression).
theseeffortsaresuccessful,compressioncanaltermodelbehaviorin Incaseswhereoff-the-shelftechniquesareinsufficient,task-specific
subtleandunexpectedways,creatingnewerrorsorbiasedoutputs[27] techniques have been developed to achieve better efficiency trade-
thatarehardtocapturewithasinglemetric. offs [14,15,22,31,65,70]. These methods vary by which aspects
Althoughcompressionisincreasinglyusedinresearchandindus- ofmodelefficiencytheytarget,howcomputationallyexpensiveapply-
try domains, there has been little work using visualization to make ingthecompressionis,andwhetherornottheydependonadditional
compressiontechniquesmoreinterpretableandcomprehensible.Initial trainingorcalibrationdata.Forexample,somemethodsutilizerandom
workoncompressionvisualizationhasfocusedonspecificcompres- datasamplestodecidewhichparameterscanmosteasilybepruned
sion techniques, like neural network pruning [32,53] or profiling a orrestoreintermediateactivations[2,12,39], whileothersaredata-
singlemodel’spowerandperformancecharacteristicsonspecifichard- agnostic[20,49]andcanoptionallybefollowedbyaretrainingstep.
ware [25]. While these methods begin to demonstrate the value of Individualweightscanbemodifiedindependently[12,49],orcompres-
visualtoolsforcompressiontasks,MLpractitionersoftenneedtotake sioncanbeperformedinastructuredmanneratthelevelofneurons
abroaderapproachtoexperimentation.Astheymixandmatchmulti- orlayers[55].Thesealgorithmicchoicesgiverisetoalargespaceof
pletechniquesindifferentorderings,thenumberofexperimentsand possiblecompressionstrategies,eachofwhichhasdifferentoverall
modelstheyproducequicklyexpands,creatingvisualizationchallenges performancecharacteristicsintermsofspaceconsumption,inference
that are not well-supported by available tools for either interactive time,andaccuracypreservation.HelpingMLpractitionersnavigatethis
compressionormodelcomparison[19,40,41]. spaceisakeydesignopportunityaddressedinourwork(seeSec.3).
Inthiswork,weexplorehowtoaddressmodelcompressionchal-
lenges using interactive visualization. We first identify four model 2.2 PitfallsinEvaluatingCompressedModels
compressionchallengesbysynthesizingpriorqualitativefindingson While compression techniques are designed to improve model effi-
howMLpractitionersusecompression[24]withinsightsfromtheML ciencywhilepreservingaccuracy,theyhavebeenknowntosubstan-
literature. Inresponsetothesechallenges,weintroduceCOMPRESS tiallyaltermodelbehaviorevenwhilemaintainingsimilartop-level
ANDCOMPARE,aninteractivevisualizationsystemforcomparingthe metrics. For example, Hooker et al. [27] find that pruning image
performanceandbehaviorofasuiteofcompressedmodels.Throughan classification models has negligible effects on overall accuracy but
overviewvisualizationcalledtheModelMap,oursystemhelpsusers disproportionatelyimpactstheaccuracyofraresubgroups.Similarly,
tracktheircompressionexperimentsandhowtheyrelatetooneanother. Liebenweinetal.[34]findthatpruningimagemodelsoftenresultsin
Userscanselectsubsetsofmodelstoautomaticallyvisualizediffer- poorgeneralizationondistribution-shiftedinputs.
encesintheiraccuracy,efficiency,andprovenance.Todeeplyinspect Sincethesebehaviorchangesdonotalwaysimpacttop-levelmetrics,
asmallersetofmodels,thesystemprovidesdetailedcomparisonsof theycanbehardtoidentifyinadvancewithoutconductingbespoke
instance-levelbehaviorsandinternalactivations.Throughcasestudies analysesdedicatedtofindingthem. Forexample, apriorinterview
anduserstudies,wedemonstratehowCOMPRESSANDCOMPAREcan studywithMLpractitioners[24]describedasituationwhereanobject
leadtoinsightsthroughoutmodelcompressionandhowitexpands detectionmodelproducedjitteredoutputsafterquantization, aphe-
thedesignspaceofinteractiveMLdevelopmenttoolstoaccountfor nomenonthatwasnotuncovereduntilthemodelwastestedon-device
challengesmadesalientbycompression.Wecontribute: inademosetting. Whiletoolsformodelcomparisonareapplicable
• FouridentifiedcompressionchallengesMLpractitionersface tothistask,compressionposesadditionalanalyticalchallenges,such
whendevelopingandselectingmodelcompressionstrategies. ascomparingmorethantwomodelsatonceandunderstandinghow
modelsarederivedfromoneanother.Ourworkaimstoaddressthese
• COMPRESSANDCOMPARE,aninteractivevisualizationsystem challengesbyprovidingpractitionerswithvisualtoolstoinspectthe
enablingcomparativeanalysisovermanycompressedmodels. behaviorofseveralcompressedmodelsduringdevelopment.
• Casestudiesontwocommoncompressiontasksdemonstrating
2.3 VisualizationforModelUnderstandingandComparison
howCOMPRESSANDCOMPAREcanhelpdebugfailedcompres-
sionexperimentsandidentifycompression-inducedbias. Visualizationhasbeenessentialinbuildinggeneralizableknowledge
about ML architectures [23,60,67] and helping practitioners make
• Auserstudywitheightcompressionpractitionersillustrating
senseofspecificmodels[1,28,50,56,62,63]. Manytoolsusecom-
how COMPRESS AND COMPARE helpusersbuildintuitionby
parison to make insights about model behavior and internals more
providingstructuretotheircompressionworkflows.
meaningful,e.g.,byjointlyvisualizingembeddingspacestodistinguish
meaningfuldataclustersfromspuriousones[5,54]. Othercompara-
2 BACKGROUNDANDRELATEDWORK
tiveapproachesextendanalysissubtaskstomultiplemodels,including
2.1 TechniquesforModelCompression
comparingmodelerrors[41],instance-leveloutputs[19,64],orinternal
Modelcompressionencompassesvarioustechniquesthatreducethe representations[40].Toolshavebeendevelopedtohelppractitioners
storagespace,memory,power,ortimerequiredtorunanMLmodel select from a wide array of models using comparisons of top-level
whilepreservingitsoriginalbehaviorasmuchaspossible[7,8,10,38, metrics[52,68].Inourwork,weusecomparativevisualizationtohelp
57]. CompressionisincreasinglyessentialforrunningMLmodels, usersidentifypromisingstrategiesandfilteroutunviableexperiments.
bothinresource-constrainedsettingssuchasmobiledevicesandfor GeneratingandevaluatingcompressedMLmodelsisamorenascent
extremelylargemodels(e.g.,generativelanguagemodels). areainVIS4ML—mostpriorworkforthistaskislimitedtoeither
Most compression techniques fall into one of three classes: (1) specificcompressiontechniquesorprofilingefficiencymetricswithout
quantizationandpalettizationreducethespacerequiredtostoreeach considering behavior. For example, CNNPruner [32] uses a TaylorFig.2:HoveringoveraModelMapmodeldisplaysatooltipcontaining
themodels’top-levelmetrics,includinglatency,size,sparsity,accuracy,
andcompressionoperation. Here,theselectedmodelhasbeen50%
pruned,improvingitslatencyandsizebutreducingitsaccuracy.
Fig.3:TheFilterviewallowsuserstodefinehardbudgetsusingmetrics,
likeaccuracy,modelsize,orlatency. Theusercanaddseveralfilters,
expansioncriteriontoprunefiltersinconvolutionalnetworks,while seethedistributionofvaluesfortheselectedmetrics,andbrushonthe
ViNNPruner[53]supportsawidervarietyofarchitecturesbutuses histogramstodisablemodelswithmetricvaluesoutsideoftherange.
aninteractivepruningschemethatisdifficulttoscaletolargemodels.
Meanwhile,Talaria[25]helpsusersestimatetheeffectsofcompression
ontheefficiencyofanymodel,butitrequiresaccuracytobeevaluated whileadeviceischargingwouldbegrantedmorememoryandtime
separately.Unlikethesepriorworks,oursystemaimstosupportmore sinceitwouldbeunlikelytodisrupttheuser. Similardecisionstake
general,iterativecompressionworkflowsthatmayinvolvedozensof placearoundaccuracy;amodelthaterrsmoreonsensitiveorcritical
modelswithdifferentsetsoftechniquesapplied. subgroupswouldnotbedeployed,whileonethatmakesreasonableor
Overall, these prior systems have focused on either model com- recoverableerrorscouldbeviable[24].Asaresult,practitionersneed
pression or model comparison alone without taking into account toolstoquicklyassessmodelvariantsandmaketrade-offsbetween
howthetwotasksareoftenintertwinedduringamodeldevelopment theirmetricstohelpthemsatisfythesemultifacetedconstraints.
pipeline[24].Evaluatingthepotentialofinteractivetoolstohelpatthe
intersectionofthesetwochallengesistheprimaryfocusofourwork. C3. Top-levelmetricscanobscureimportantdifferencesbetweencom-
pressedmodels. Althoughcompressioncanachievecomparableper-
3 DESIGNCHALLENGESFORCOMPRESSION formancetotheoriginalmodel,itcanalterthemodel’sbehaviorby
introducingnewerrorsorbiases.Insomecases,thesedifferencesare
To identify key compression challenges and motivate the design of
randomorimperceptible,butinothers,theycanbeproblematic,such
interactivetoolsforcompression,wesynthesizedinsightsfromMLand
as reducing the quality of model predictions [24], changing model
HCIliterature.Recently,Hohmanetal.[24]exploredMLpractitioners’
explanations[34],orincreasingbias[27]. Unfortunately,behavioral
needsandperspectivesonmodelcompressionviaaninterviewstudy
changesdonotalwayscorrespondtoachangeinevaluationmetrics,so
with30compressionexperts. Thisworkdescribeshowcompression
humansmustbeinvolvedintheevaluationprocesstocatchdangerous
expertsexperimentwithdifferentcompressiontechniquestosatisfyeffi-
behaviors.Whilevariousvisualizationtechniquescomparethebehav-
ciencyandaccuracyconstraintswithinmultidisciplinaryteams.While
iorsofpairsofmodels[5,40,54],thereremainopportunitiestohelp
theirfocuswasprovidingabroadoverviewofcompressionexperts’
practitionerscomparemanycompressedmodels’behavioralchanges.
tacitknowledge,wedistillspecificfindingsfromtheirstudythatare
relevanttothedesignofcompressiontoolsandsupplementthemwith
C4. Compressioncanhaveunintended,hard-to-debugeffectsonmodel
recentinsightsfromMLliteraturetosynthesizekeychallengesforour
internals.Practitionersoftenhaveasetofheuristicstheyexpectcom-
systemtoaddress.
pressionalgorithmstofollow,suchasnotcompressingearlylayersand
C1. Identifyingtheoptimalcompressionstrategyistime-consuming compressinglayersproportionaltotheirnumberofparameters.How-
andtask-specific. Aone-size-fits-allstrategyformodelcompression ever,whencompressionalgorithmsdonotfollowtheseexpectations,it
doesnotexist.Evencompressionexpertsdonotknowhowtoachieve canbedifficultforpractitionerstodeterminewhy[24].Forexample,
thebestbalanceofefficiencyandaccuracyaprioriandoftenexper- anetwork’soutputsmaychangesignificantlybecausealayerceased
iment with multiplecompression algorithms for each new task and toproducemeaningfuloutputorbecauseitsactivationshadadifferent
model[24,25].However,existingcompressiontoolsfocusonexplor- distributionthatcauseddownstreamlayers’outputstochangeaswell.
ingtheresultsofasinglecompressionexperiment[43,52,61]instead Toensurecompressiononlyimpactsthedesiredportionsofthemodel,
ofassessingthedesignspaceofallpossibleexperiments.Asaresult, practitionersoftengolayer-by-layertofinderrorsandbottlenecks.Par-
compressionexpertsinourevaluationstudy(Sec.6)hadcomplexand ticularly for deep networks with hundreds of layers and billions of
time-consumingcomparisonworkflows,suchasflippingbetweenthe intermediateoutputs, parameter-wisemodelcomparisonbecomesa
sametoolloadedwithdifferentmodelsandmaintaininglargespread- challengingtaskandanopportunityforvisualizationtooling.
sheetsofmodelresults.Compressiontoolsshouldsupportpractitioners
4 DESIGNOFCOMPRESSANDCOMPARE
in comparing compressed models based on their performance, effi-
ciency,behavior,andprovenancetoidentifypromisingstrategies. WedevelopedaninteractiveinterfacecalledCOMPRESSANDCOM-
PAREtoaddressthefourcompressiondesignchallenges(Sec.3).The
C2. Compressionrequireshumantrade-offsacrossmultiplemetrics. toolconsistsoftwomainviews:theCompressionOverviewsupports
Practitioners often discuss model compression as an effort to meet high-levelcomparisonandmodelselectionfromlarge-scalecompres-
resourcetargetsonmemory,time,andaccuracy[24].Thesebudgetsare sionexperiments(C1andC2), andthePerformanceComparison
oftennegotiated,set,andadjustedbasedonhowmodelswouldaffect viewenablesfine-grainedinspectionofmodelbehaviorsandinternals
theuserexperience. Forexample,amodelthatcouldrunovernight forasmallnumberofcandidatemodels(C3andC4).Train Magnitude Prune Gradient Prune Train Train Magnitude Prune Gradient Prune Train Train Magnitude Prune Gradient Prune Train
A Compare Successive Operations B Compare Multiple Experiments C Refine Ambiguous Comparisons
Select descendants of a model Select multiple model subtrees Select multiple arbitrary subtrees
Fig.4:TheSelectionDetailsviewautomaticallygeneratesmeaningfulcomparisonsbetweenselectedModelMapmodels,suchascomparing
successiveoperationsbyselectingthedescendantsofamodel(A)orcomparingmultiplecompressionalgorithmsbyselectingmultiplesubtrees(B).
Ifadirectcomparisondoesnotexist,theinterfacepromptstheusertorefinethecomparisontotheattributestheyareinterestedinvisualizing(C).
4.1 CompressionOverview absenceofanoperation,andtypeofoperationapplied.Thealgorithm
attemptstofitmultiplealternativevariabletypesateachstageofthe
InCOMPRESSANDCOMPARE,experimentsarerepresentedasasetof
treetraversalandchoosestheassignmentthatresultsintheshortestand
trees,whereeachnodeisamodelandedgeisanoperationperformed
simplestsetofvariables(e.g.,avariableforanoperation’sparametersis
onaparentmodeltoproduceachildmodel. Thisstructuralchoice
consideredsimplerthanavariableforoperationtype).Thisensuresthat
helps address one of the main visualization challenges of tracking
similaroperationsaremappedtoeachother.Forexample,themodels
compressionexperiments(C1):simultaneouslydepictingthevariation
resultingfromPrune→QuantizeandPrune→Calibrate→Quantize
inmetricsacrossmodelsalongwithdependenciesinhowthemodels
canbeexplainedbyCalibrate=trueorfalse.
weregenerated.TheModelMap(Fig.1A)addressesthisusinganode-
Ifthemodelscanberepresentedusingtwovariablesorfewer,we
linktreediagram,wherenodesarepositionedusingacustomalgorithm
generateabarchartbymappingthex-axisandcolorencodingstothe
that vertically aligns nodes based on either the operations used to
twovariables. Ifmorevariablesarerequired,thealgorithmattempts
producethemortheirstepinthecompressionexperiment.Modelsare
toiterativelysimplifythevariablesetbyidentifyingconditionalde-
renderedascircleswhosecolorandsizeencodeperformanceproperties,
pendenciesandcumulativerelationshipsbetweenpairsofvariables.
commonlyaccuracyandmodelsize.Toemphasizethesequentialnature
AsshowninFig.4AandB,thissimplificationallowsustovisualize
of compression experimentation, the color and width of the edges
severalsuccessiveoperationsasasinglex-axisencodingorcombine
smoothlyinterpolatebetweentheparentandchildnodes. Hovering
multiplevariablesthatarerelatedtothesameoperation.Whenthelist
over a model displays a tooltip with the model’s top-level metrics
ofvariablesneededtodescribetheselectioncannotbesimplifiedto
(Fig.2),suchaslatency,size,sparsity,accuracy,andtheoperationthat
twoencodings,aRefineComparisonviewallowstheusertogenerate
createdthisnewmodelfromitsparent.
barchartsforsubsetsoftheselectionthatcanbecompared(Fig.4C).
WhiletheModelMap’slayoutprioritizesunderstandingmodelde-
pendencies,theModelScatterplot(Fig.1B)visualizesmodelmetrics
4.2 PerformanceComparison
alongthespatialaxes, helpingdirecttheusertoviablemodelsand
findnaturalmodelgroupings. Forexample,theclassicParetocurve WhiletheCompressionOverviewvisualizestrendsacrosslargesets
oftenusedbycompressionexperts[24]caneasilyberecreatedbyset- ofmodels,thePerformanceComparisonviewenablesadeepercom-
tingmodelsizeorlatencyonthex-axisandaccuracyonthey-axis. parisonofasmallergroupofmodels’BehaviorsandLayers.Weuse
NodecolorandsizeareconsistentbetweentheModelScatterplotand acombinationofjuxtapositionandexplicitencodings[18]toenable
ModelMap,andthetwovisualizationsareconnectedviabrushing comparisonsofmultiplemodelsatatime.Comparisonisperformed
andlinking[3]. TheFilterviewalsodepictsmodelmetricsthrough withrespecttoabasemodel,whichisuser-customizablebutdefaults
customizablehistogramsthatcanbebrushedtofiltertheModelMap totheselectedmodelclosesttothetree’srootnode.
andModelScatterplot.Whentheuserspecifiesafilter,modelsthat TheBehaviorstab(Fig.5,left)presentstheresultsofevaluating
donotmeetthefiltercriteriabecomesemitransparentandunselectable. eachselectedmodelonavalidationdataset.Eachmodelisacolumnin
Thisallowsuserstonarrowdownthesetofviablemodelsbyexpressing atable,whererowsrepresenteitherclass-levelcomparisonsorindivid-
theirproject’sspaceandperformancebudgets(C2). ualinstances.Whenconfiguringtheirdata,userscandefinecomparison
Whenoneormoremodelsareselected,theusercanviewinforma- metricsthatoperateonthemodeloutputs,enablingcomparisonslike
tionaboutthemodels’metricsintheSelectionDetailsview(Fig.1C). differencesintop-1predictionsortheKLdivergenceofthesoftmax
While displaying metrics for multiple models in a table would be probabilities.Thesecomparisonmetricsaresummarizedinthetable
straightforward,itwouldobscurethedependencystructurebetweenthe headersandaredepictedassparklinebarchartsineachrow.Notably,
models,makingithardertoreasonabouttheeffectsofdifferentcom- theinterfacesupportsbothabsoluteper-modelvaluesandrelativeval-
pressionoperations.Therefore,wedevelopatechniquetoautomatically uescomparedtothebasemodel.Userscansortandfilterbyabsolute
createagroupedbarchartfromasubsetofthemodeldependencytree. orrelativemetricsforanymodel,allowingthemtoquicklyidentify
Ouralgorithmtraversesthetreerecursivelytoidentifyaminimum- classesorinstancesimpactedthemostbycompression(C3).
cost set of “variables” that compactly explain the selected models’ Thefinalandlowest-levelcomponentoftheinterfaceistheLayers
differences.Variablesincludeoperationparametervalues,presenceor tab(Fig.5,right),whichexposestheinternalsoftheselectedmodels.Fig.5:ThePerformanceComparisonviewprovidesanin-depthcomparisonoftwoormoremodels.TheBehaviorstab(left)displaysdifferences
betweenmodels’predictions,distributionsofcomparisonmetrics,andabreakdownoftheselectedcomparisonmetricattheclassorinstancelevel.
Meanwhile,theLayerstab(right)comparesthesparsity,weights,andactivationsacrosslayersinthemodelsusingafiletreestructure.
LiketheBehaviorstab,thisviewcomprisesatablewhereeachcolumn outputs.However,itcanbechallengingforuserstodeterminewhich
containsinformationaboutamodelrelativetothebase;however,here, componentsofamodelarecausingitsperformancetodegradeafter
eachrowrepresentsamoduleinthenestedhierarchyofmodulesthat compression(Sec.6.2.2).WedemonstratehowCOMPRESSANDCOM-
makesupeachnetwork. Withinthisstructure, theusercanchoose PAREcanhelppractitionersidentifyandresolvebreakages(C4)inthe
fromvisualizingtheproportionofzeroweightsineachmodule,the contextofagenerativelanguagemodelforquestionanswering.
distributionoftheweightvalues,andthedistributionoftheactivations We use an off-the-shelf T5-Large model [46] that achieves an
(intermediateoutputs)onarandomdatasample. Weightvaluesand F1scoreof90.5%ontheStanfordQuestionAnsweringdataset[47]
activationsaredepictedasstackedhistogramssothattheheightofthe (Fig.6A).Themodel’soriginalperformanceiscompetitivewithhu-
barsformstheoverallvaluedistributionwhilethecolorindicatesthe mans’,butsincethemodelislarge(775millionparameters),we’dlike
degreeofchangerelativetothebase.Thishighlightsparametersand tocompressittoimproveitsspeedandspaceutilization. Following
modelsthathavechangedmorethanothers,whichcanrevealbugssuch commoncompressionworkflowsfromourparticipants(Sec.6.2)and
asover-prunedlayersoroutlieractivations(C4). theliterature(Sec.2),weapplymagnitudepruningacrossallofthe
model’sparameters. However,thiscausessteepperformancedrops
4.3 SetupandImplementationDetails evenatlowlevelsofcompression(e.g.,4%F1afterpruningonly10%
WedesignedCOMPRESS AND COMPAREforahighlycustomizable ofparameters).Lookingatthetopchangesinpredictedanswersinthe
user workflow. To begin visualizing models, users write a simple Behaviorsview(Fig.6B),weseethatmagnitudepruninghasbroken
PythonscriptthatinvokestheCOMPRESS AND COMPAREbackend themodel’sgeneration.The10%prunedmodelrepeatswordsfromthe
serverandprovidesinformationaboutthemodels.Usersspecifymod- contextparagraph(e.g.,“Super Bowl LII LII LII ...”),andthe
elsasaJSONobjectthatdetailstheoperationsusedtoproduceeach 30%prunedmodel’soutputismeaningless(“a a ...”).
modelandtheirperformanceacrossasetofuser-definedmetrics.Users COMPRESSANDCOMPAREcanhelpusunderstandwhymagnitude
can easily integrate specification creation into their existing model pruninghasnegativelyaffectedthemodel. Therearemanypossible
training procedures by updating the JSON file each time they train reasonsthiscompressionstrategycouldhavefailed—themodelmay
andevaluateanewmodelorevaluateagainstanewmetric.Toaccess havelowcompressibility,essentialweightsmayhavebeeninadvertently
informationaboutthemodel’sbehaviorsandlayers,userswritePython pruned,ormagnitudepruningmaynotbewell-suitedtothistask.Since
callbackstoretrieveinstance-leveloutputsandlayeractivations,bothof itischallengingtodeterminethecauseusingperformancealone,we
whichcanbeeitherpre-computedorevaluatedinreal-time.Thisflexi- use the Layers view to inspect parts of the model that have been
bilityallowsCOMPRESSANDCOMPAREtosupportanyPython-based pruned. Sortingthemodels’layersbyhowmuchtheirweightshave
MLtoolchainandaccommodateverylargemodels.Additionally,the changed, weseethatthemostchangedlayersareallnormalization
COMPRESSANDCOMPAREPythonpackageprovideshelperfunctions layers(Fig.6C).Normalizationlayersensureaconsistentactivation
to accelerate setup with common frameworks such as PyTorch and distributionthroughoutthemodel,soover-pruningthemcanleadto
HuggingFace.Themodelserversfortheusecasesinthispaperrequire unexpected behavior. However, since the model has relatively few
around200linesofcode,mostlyconsistingofboilerplatecodethat normalizationweights, wewouldnotnecessarilyexpectmagnitude
wouldalreadyhavebeenwritteninthecourseofexperimentation. pruningtohaveprunedthemsoaggressively. Totestifpruningthe
The COMPRESS AND COMPARE frontend, implementedinSvel- normalizationlayerscausedtheperformancedrop,wedesignafollow-
teKit1, is static and can be hosted publicly. Visualizations are de- up experiment that restores the normalization layers in the pruned
velopedusingD3.js2 andLayerCake3. Codeisavailableat: https: modelstomatchtheoriginalmodel,effectivelyunpruningthem.This
//github.com/apple/ml-compress-and-compare. leadstoafullrecoveryinF1forthe10%and30%prunedmodels,
indicatingthatpruningthenormalizationlayerswasasubstantialissue
5 CASESTUDIESOFCOMMONCOMPRESSIONTASKS inouroriginalcompressionexperiment.
We illustrate how COMPRESS AND COMPARE supports real-world WecanalsouseCOMPRESSANDCOMPAREtounderstandifthe
compressionworkflowsviatwocasestudies. repaired models can be pruned any further. To do so, we browse
modelactivationsintheLayersviewfortheoriginalmodel,the30%
5.1 RepairingModelsBrokenByCompression prunedmodelwithrestorednormalizationlayers(thefixedmodel),and
the50%prunedmodelwithrestorednormalizationlayers(abroken
Apreviouslyaccuratemodelcan“break”whencompressionisapplied
model).Weobservethattheoutputsoftheself-attentionmodulehave
tooheavilyorbroadly,resultinginlowperformanceandnonsensical
changedsignificantlyduringpruning,evenintheworkingmodel,which
1https://kit.svelte.dev maysignifythatthemodelisrobusttochangesinthesemodules.By
2https://d3js.org pruning additional parameters from the attention modules, we can
3https://layercake.graphics reducethemodelsizebyroughly30%whileachieving83%F1score.A Generative Question-Answering B Initial Evaluation D Repaired Sparse Models
Base model T5-Large (770M parameters) achieves good performance Even low levels of global magnitude pruning Layer-specific pruning excluding normalization
on SQuAD question answering tasks (F1 = 90.5). breaks the model’s generative capabilities. layers better preserves model behavior.
...
C Analyzing Weight Changes
Sorting the pruned model’s modules by how much their
weights have changed reveals excessive change to the
normalization layers.
Fig.6:COMPRESSANDCOMPAREhelpsdebugcompressionexperiments.Onagenerativequestion-answeringtask(A),theBehaviorsviewreveals
thatglobalmagnitudepruningseverelydeterioratesgenerationquality(B),whereaslayer-specificpruningmatchestheoriginalmodel’sbehavior(C).
Further, reviewing these models’ predictions relative to the base in Table1: WeevaluateCOMPRESS AND COMPAREwith8compression
practitioners. Whileeachparticipantusescompressionintheirwork,
theBehaviorsview(Fig.6D)confirmsthattheirgenerativeabilities
theyhavediverseroles,MLapplications,andcompressionworkflows.
are preserved. Here, COMPRESS AND COMPARE helped us debug
our compressed model’s low performance and design compression
ID Role MLApplication CompressionWorkflow
modificationsthatresultinefficient,high-performingmodels.
P1 MLEngineer EfficientMLalgorithms Compressesexistingmodels
5.2 DiscoveringCompressionArtifacts P2 MLManager EfficientMLtooling Compressesexistingmodels
P3 ResearchScientist EfficientMLtooling Compressesexistingmodels
Compressionartifacts—changesinmodelbehaviorcausedbycompres- P4 MLManager 3Dcomputervision Developsefficientmodels
sion—cansubtlyaffectmodelquality,decreaseedgecaseperformance, P5 MLManager 3Dcomputervision Developsefficientmodels
P6 ResearchScientist EfficientMLalgorithms Developsefficientmodels
andincreasebiaswithoutreducingoverallaccuracy,makingthemdiffi-
P7 ResearchScientist Multi-modalML Developsefficientmodels
culttodetectyetcrucialtoaddress.TodemonstrateCOMPRESSAND P8 SoftwareEngineer EfficientMLtooling Buildscompressiontools
COMPARE’sabilitytoidentifycompressionartifacts(C3),weapply
ittostudyknowncompression-inducedbiasesinfaceclassification
models[27,36].FollowingHookeretal.[27],wetrainaResNet18[21] meritadditionalinspectionorhuman-in-the-loopannotation,similar
onCelebA[36]topredictwhethereachimagehastheattributeblond.
toCompressionIdentifiedExemplars[26,27]. Webeginbyloading
Then,weiterativelytrainandcompresssevenResNet18[21]models thecompressedanduncompressedmodelsintotheBehaviorstaband
onthesamebinaryclassificationtaskusingglobalmagnitudeprun- sortingallimagesbythechangeinerrorratebetweentheuncompressed
ing[70]at10%,30%,50%,70%,90%,95%,and99%finalsparsity and99%compressedmodels(Fig.7C).Manyimagesareclassified
(seeSupplementaryMaterialSec.S3).Eachresultingmodelachieves differentlybetweentheoriginalmodelandthecompressedmodels,in-
similaraccuracyonthetestset,rangingfrom87.4%to94.4%.
dicatingthattheseimagesaremostsensitivetocompression.Looking
Toidentifypotentialsourcesofbias,weuseCOMPRESSANDCOM- closelyattheseimages,weseetheyareoftenchallengingimages(e.g.,
PAREtounderstandhowthemodels’smalldropsinperformanceare peoplewithsilverhairorhairoccludedbyheadwear),confirmingprior
distributedovertheimages.IntheCelebAdataset[36],maleandnot
analysis[26,27]thatimagessensitivetocompressionareoftenchal-
youngareunderrepresentedattributes.Ifcompressionisintroducing
lengingtoclassify.Inthisway,COMPRESSANDCOMPAREsupports
biasbyforgettingrareclasses,itwillhaveadisparateimpactonthese dataauditingtohelpusersuncoverinstancessensitivetocompression
images.Toinspectthis,wecomparetherelativeaccuracyofthepruned thatmaywarrantfurtherdatacleaningandqualitycontrol.
modelstotheuncompressedmodelintheBehaviorstab(Fig.7B).This
viewimmediatelysurfacesthatcompressionhasdisproportionatelyim- 6 USERSTUDYWITHMLPRACTITIONERS
pactedtheperformanceofrareclasses.Whilethe99%-prunedmodel
makes64.9%moreerrorsfornot maleand72.3%moreforyoung,in ToevaluateCOMPRESSANDCOMPARE,weconductedauserstudy
therareclasses,itmakes145.5%moreerrorsformaleand96.5%for witheightMLpractitionerswhoworkonmodelcompression.Partici-
not young.Thisisconcerningsinceperformanceonunderrepresented pantsincludedresearchscientistsdevelopingnewcompressiontech-
niques,MLengineerscompressingmodelsfordeployment,andsoft-
anddifficultinstancesisaprimaryreasonthatMLpractitionersstart
wareengineersbuildingcompressiontools(Table1).Thegoalofthe
withalargemodel[33,34].Thismaysignaltheneedforfurthermodel
development,compressionexperiments,andbiasmitigationbeforewe
studywastounderstandhowparticipantswoulduseCOMPRESSAND
arecomfortableusingthesemodels.Previously,wewouldhaveneeded
COMPAREtomakesenseofcompressionexperiments,buildintuition
aboutcompressionstrategies,andexplorecompressedmodelbehavior.
tocomputemodelperformanceoneachattributeandinterpretatable
ofrelativepercentages(suchasinHookeretal.[27]);however,using
6.1 StudyMethods
thevisualaffordancesofCOMPRESSANDCOMPARE,weareableto
quicklyidentifybiaswithoutadditionalcomputation. Eachstudysessionwasconductedviavideochatandlasted45minutes–
WhileCOMPRESSANDCOMPAREsupportsbiasidentificationusing 1hour.Webeganbydiscussingtheparticipant’scurrentcompression
traditionalerrormetrics,itcanalsoauditbiasinsettingswherewedo workflowsandintroducingthemtoCOMPRESSANDCOMPARE.Then,
nothaveaccesstodatawithsensitiveattributelabels.Inthissetting,we weaskedparticipantstoimaginetheywerepartofateamcompressing
useCOMPRESSANDCOMPAREtoidentifyasubsetofimagesthatmay anexistingimageclassificationmodelandtothinkaloudastheyusedA Blond Classification B Identifying Amplified Bias C Uncovering Images Sensitive to Compression
Magnitude pruning preserves All levels of pruning disproportionately increase the error rate Images whose predictions change across many compressed models are challenging
accuracy even at 99% sparsity. for underrepresented groups (male and non young). images that may warrant additional data auditing.
magnitudepruning90 magnitudepruning95 magnitudepruning99
Fig.7: COMPRESS AND COMPARE canhelpidentifycompressioninducedbiasandperformdataauditing. TheBehaviorsviewrevealsthat
compressingimageclassificationmodels(A)disproportionatelyimpactsrareclasses(B)byforgettinghard-to-classifyimages(C).
COMPRESSANDCOMPAREtocompletetwotasks: toaparticularaspectofcompressionanalysis(e.g.,hardwareperfor-
mance[25]).Asaresult,participantsfounditchallengingtoperform
T1. First, we showed participants a ResNet50 image classification
comprehensiveanalysisandlamentedthatexistingtoolsdidnotsup-
model[9,21,45]and19ofitscompressedvariants, including8-bit
portmanyoftheirmostcriticaltasks,likebudgeting(P2),layer-wise
quantization,globalgradientandmagnitudepruningatvarioussparsity
analysis(P1),andcompression-specificcomparison(P2,P3,P4).
levels,andpost-pruningfine-tuningandcalibration.Weaskedpartici-
Unlikeparticipants’existingworkflows,withCOMPRESSANDCOM-
pantstoidentifysuccessfulexperiments,explainwhysomeexperiments
PARE,criticalcompressiontasks,likemetricanalysisandbudgeting
failed,andsuggestfutureexperiments.
(C2),experimentcomparison(C1),andlayer-wisebehavioralinspec-
tion(C3andC4)areallinoneplace:
T2. Next,weupdatedtheinterfacetodisplayall52models,includ-
ingthe20modelsfromT1,aMobileNetV2model[51]withglobal “[In]alotofmytypicalworkflows[...]youhavetohave10
magnitudepruning,iterativelyprunedandfine-tunedmodels,andmod- tabsinparallelinyourbrowserandswitchbetweenthem.
elswithcombinedcompressiontechniques.Weaskedparticipantsto Ifindthat[COMPRESSANDCOMPARE]isreallybringing
evaluatethelatency,size,accuracy,andbehaviorofthesemodelsand allthedifferentaspectsintoasingleview.”—P2
selectthemodel(s)theyfeltconfidentdeploying.
Forinstance,anapproachusedby4/8participantswastoidentifycan-
Toanalyzetheresultsofthestudy,twoauthorsreviewedthevideo- didatemodelsthatfittheirperformancebudgetviatheFilter,honeinto
recordedsessions.Theyperformedopencodingtoidentifyparticipants’ theoneortwobest-performingmodelsusingtheModelScatterplot
insightsduringthetasksandtheirbroaderperspectivesoncompres- metrics,andsearchforpatternsinthebestmodels’compressionrecipes
sionandCOMPRESSANDCOMPARE.Usingtheresulting357codes, usingtheModelMap.Throughthisprocess,participantsquicklyiden-
theyconductedaniterativeaffinitydiagrammingandthematicanalysis tified “deployable” models that were small enough to fit on device
processtoidentifyprogressivelyhigher-levelthemes. Thisprocess while still maintaining task performance. P1, P5, P6, P7, and P8’s
generated17finalthemesthatweusetostructureSec.6.2. CompressionOverviewanalysisquicklyconvergedfrom52modelsto
asingle8-bitquantizedResNet50modelthatreducedlatencyandsize
6.2 StudyResults whilenearlymaintainingtheuncompressedmodel’saccuracy.Having
acomprehensiveoverviewofcompressedmodelvariantsgavepartici-
Overall, participantsreportedthat COMPRESS AND COMPARE pro- pantsconfidencetopitchthismodeltotheirteammatesanduseitto
videdastructuredcompressionworkflow,enablingthemtoperform
designnewexperimentsthatcouldresultinevengreaterefficiency.
analysesthatwouldhavebeenchallengingandtime-consumingwith
Theunifiedcompressioninterfacealsosparkeddiscussionabouthow
existingtools,buildintuitionaboutcompressiontechniquesthatinspire
COMPRESSANDCOMPAREcouldintegrateintocollaborativecompres-
new compression experiments, and identify subtle but problematic
sionsettings.Participantsregularlycollaboratewithteammembersto
modelbehaviorsintroducedduringcompression.
completetheircompressiontasks,suchassendingcompressedmodels
toQAspecialistsfortargetedevaluation(P4,P5),negotiatingresource
6.2.1 UnifyingDisjointCompressionWorkflows
budgetswithproductmanagers(P6,P8),andmentoringmodeldevel-
Ouruserstudyparticipantsreportedneedingmultipletoolstoexecute opersoncompressionmethods(P2). However,itcanbechallenging
theircurrentcompressiontasks,leadingtotediousback-and-forthanal- tocollaborateonacompressedmodelacrossvarioustoolsandwith
ysis across tools that made it challenging to understand the overall collaboratorswithvaryingskillsets.Inparticipants’currentworkflows,
impactoftheircompressionexperiments.Whileeachparticipantwas experimentalresultsaredistributedacrosstools,soparticipantswere
knowledgeableaboutcompression(average3.5onaself-reported1–5 excitedtouseCOMPRESSANDCOMPAREasacentralizedcommuni-
scaleofcompressionexperience)andusedcompressionregularly(5/8 cationtool.P6andP8wereinterestedinpresentingtheirexperiments
usecompressionineveryproject),therewasnotastandardsetoftools tobudgetmanagersusingCOMPRESSANDCOMPAREtoadvocatefor
usedbyallparticipants.Thisinconsistencywasoftenduetoalackof budgetincreasesbyinteractivelydemonstratinghowbest-casemodel
compression-specifictoolingthatsupportedthebreadthofparticipants’ performance improves as the budget relaxes. Participants also ex-
tasks, includingtrainingandcompressingmodels(P1–P8), evaluat- pressedinterestinusingCOMPRESSANDCOMPAREtocollaboratively
ingtheirperformanceandbehavior(P4,P6,P7),andensuringthey compressmodels,suchasbyflaggingpotentialcompression-induced
meetspecifiedefficiencybudgets(P1–P8). Asaresult,participants issuesintheBehaviorsviewforreviewbytheirQAteammates(P4,P5)
oftencreatedcustomtools,suchasJupyterNotebookchartsforper- andsettingupexperimentsintheModelMapthatdemonstratecom-
formanceanalyses(P1,P6,P8)andspreadsheetsformodeltracking pressionpitfallstolessexperiencedengineers(P2).Whereasexisting
(P3,P5),ortheyrepurposedexistinggeneralmodelanalysistoolsto compressionworkflowstendedtobecomeadhocwhenexperimenting
understandtheperformanceofacompressedmodel(P1–P4). When withmanydifferentalgorithms, techniques, andpipelinestructures,
compression-specifictoolsexisted,participantsfoundtheminvaluable participantswereexcitedforCOMPRESSANDCOMPAREtoprovide
totheiranalysis(P3,P4).However,thesetoolswereoftenstillspecific structuretothecollaborativesearchforanefficientandaccuratemodel.6.2.2 BuildingIntuitionaboutModelCompression orspuriouscorrelations.Theabilitytosortbyrelativechangeincor-
rectnesshelpedthemidentifyclassesthatexperiencedthemosterrors
Beyond simply selecting a desired compressed model, COMPRESS
andinspectindividualinstancesthatweremisclassified. Thisproce-
AND COMPARE’s visual and interactive components helped partici-
durerevealedthatmanycompressedmodels’mistakeswereacceptable,
pantsbuildintuitionforhowcompressionalgorithmsimpactmodel
suchasmistakesonmulti-objectimages(e.g.,coffee potinastove
performanceandgeneratehypothesesaboutwaystoimprovefuture
image)orrelatedclasses(e.g.,Great Danemisclassifiedasanother
experiments. ParticipantsfoundthecombinationoftheModelMap
dogbreed).However,italsohelpedthemuncoversubtlepatternsand
andtheSelectionDetailsviewtobeanintuitivewaytounderstand
identifypotentialcompression-inducedbiases.Forinstance,P5sorted
andreasonaboutthespaceofcompressionexperiments.Viewingthe
theBehaviorsviewbydecreasingchangeincorrectnessandobserved
columnsintheModelMaphelpedusersunderstandthesetofcom-
thatstoveimageshadlost22%accuracy,whereasoverallthemodel
pressionalgorithmsthathadbeenapplied(P3,P4,P7)andidentify
onlyexperiencedafewpercentdecrease. Inspectingthestoveim-
patternsinhowthebest-performingmodelsweregenerated(P2,P7).
ageswhoseclassificationshadchangedrevealedaspuriouscorrelation
Todiginto aparticularpattern, participantswouldoften runvisual
betweenstovesandmicrowaves. P5worriedthatthecompressed
“experiments”byselectingagroupofmodelswithinaregionofthe
modelcouldberelyingonthepresenceofonetoclassifytheother.
ModelMap(e.g.,allthemagnitudeprunedmodels)andcomparing
Byviewingmodelbehavioroveranentiredataset,participants,like
theirmetricsintheSelectionDetailsview(P2,P4,P6,P8). These
P5,wereabletoidentifyconcerningpatternsinthemodel’sbehavior,
in-depthexplorationsinfluencedparticipants’intuitionsabouthowcom-
hypothesizereasonsfortheproblem(e.g.,adisproportionateamount
pressiontechniquesaffectedtheirmodelsmorebroadly.Forinstance,
oftrainingimagescontainbothobjects),anddevelopaplantoaddress
bycomparingtheaccuracyandefficiencyoftwoquantizedmodels,
them(e.g.,flaggingtheseexamplesforQAteamreview).
P4identifiedthatquantizationpreservedperformancemuchbetterfor
While participants primarily analyze correctness in their current
alargeResNet50(25.6Mparameters)thanforasmallerMobileNet
workflows,havingaccesstoadditionalplug-and-playmetricsintheBe-
V2(3.5Mparameters).WhileP4regularlyusesquantization,thisdis-
haviorsviewspurrednewanalysisprocesses.DuringP1’sbehavioral
crepancy in performance caused them to reflect that the success of
analysis,theydiscoveredthatmagnitudepruningresultedinalarger
quantization“isdefinitelydependentonthebasemodel;ifyouwant
KLdivergenceinoutputprobabilitiesthanquantization. Whilethey
somethingtoworkforquantization,youhavetostartattherightplace.”
donotuseKLdivergenceintheirstandardanalysispipeline,having
Findingthebestcompressiontechniqueischallenging(C1),sobuilding
intuitionforthetypesofmodelsthatbenefitfromquantizationcanhelp accesstoitin COMPRESS AND COMPARE helpedthemdistinguish
betweenotherwisesimilarcompressedmodelsandselecttheonethat
P4designmoreeffectivecompressionrecipesmovingforward,suchas
bestreflectedtheoriginalmodel’soutputs.
thosethatonlyapplyquantizationtolargemodels.
Buildingontheirhigh-levelunderstandingoftheexperimentalspace, “Idon’tlookatKLdivergenceveryfrequently,butKLdi-
participantsusedthePerformanceComparisonviewstodevelopa vergenceiszerofor[thequantized]modelandnon-zero
deeperintuitionaboutcompression’simpactonmodels’internalrep- for [the pruned] model. There was only a 6% accuracy
resentations. Using the Layers view, participants debugged subtle regression[fortheprunedmodel],soit’ssurprising. I’m
problemsincompressionexperimentsthatledtopoormodelperfor- impressedbytheKLdivergence[ofthequantizedmodel]
mance. Forexample,P8recognizedthataparticularmodel’s“batch beingextremelylow.It’stheincumbentsolution.”–P1
normshadbeenabsolutelyflattened”byquantization.Batchnormaliza-
tionlayerscanhaveasubstantialimpactondownstreamperformance Similarly, P2 and P3 uncovered that magnitude pruning resulted in
becausetheysettheoutputvaluerangesateachlayer,sothisfinding highermodelconfidencethanquantization.Withtheknowledgethat
ledP8tosuggest“freezingallthebatchnorms”duringquantizationas thesedifferencesinmodeloutputsexisted,participantswereableto
awaytomaintainmodelperformance.Bybuildingintuitionthrough generatehypothesesfortheirexistence(e.g.,confidenceincreasesmay
theirLayersexploration,participantsideatedarangeofsubsequent bearesultofoverfittingwithfewerparameters(P2))andstrategiesto
experiments. Thesenextstepsincludedcombiningcurrentcompres- accountforthem(e.g.,settingadifferentpredictionthresholdbasedon
siontechniquesinnewways(e.g.,combiningmagnitudeandgradient thecompressionalgorithm(P3)).Overall,COMPRESSANDCOMPARE
pruning)andexpandingthespaceofoperations,suchasbytailoring extendedparticipants’compressionworkflowstointegratebehavioral
compressiontospecificlayersofthenetwork. Forinstance,intheir analyseswiththeirstandardmetric-basedbudgetingprocedures. As
Layersanalysis,P5andP7noticedearliernetworklayershadfewer aresult,participantsseamlesslyswitchedbetweenthetwo,iteratively
parametersyetwereprunedatthesamerateaslaterlayers.Theyhy- selectingacandidatecompressedmodelandinterrogatingitsbehavior
pothesized that, since later layers have more parameters, they have toidentifybiasesandhypothesizenewcompressionexperiments.
moreredundancyandcouldwithstandgreatercompressionrates,so
theydesignedanexperimentthatprunedlayersasafunctionoftheir 7 DISCUSSION
numberofparameters.WithCOMPRESSANDCOMPARE,participants WepresentCOMPRESSANDCOMPARE,aninteractivevisualization
deepened their understanding of how model parameters respond to systemfortrackingandcomparingcompressionexperiments.Basedon
compressiontechniques(C4)andusedtheirinsightstogeneratenew challengesexperiencedbyreal-worldusers,wedesignCOMPRESSAND
experimentsthatcouldleadtomoreefficientandaccuratemodels. COMPAREtosupportcriticalandunsupportedcompressionanalysis
tasks,includingmanaginginterconnectedcompressionexperiments,in-
6.2.3 EncouragingComprehensiveCompressionAnalysis terrogatingtheimpactofcompressiononmodelbehaviors,andideating
Byinteractivelyintegratingbehavioralanalysiswithtraditionalmetric- promisingfuturecompressionexperiments.Throughcasestudieson
based compression analysis, COMPRESS AND COMPARE extended generativelanguageandimageclassificationmodels,wedemonstrate
participants’ existing behavioral analysis workflows and motivated howoursystemhelpsusersrepairissueswithcompressionandidentify
them to consider the broader impacts of compression. Evaluating compression-inducedbias.Moreover,ouruserstudywithcompression
compressedmodelbehavioronheldoutdatawasakeyaspectofsome expertsillustrateshowCOMPRESSANDCOMPAREshiftsusers’com-
participants’workflows(P5–P8)becauseithelpedthemidentifysubtle pressionworkflowsfromdisjointanalysisacrosstoolstowardasingle
butimportantbehavioralchanges(C3): analysisplatformthatfacilitatescollaborativedecision-makingabout
modelselectionandexploration.Here,wediscusstheimplicationsof
“When you compress a model you care about its quality ourresultsforfutureMLdevelopmentandevaluationtools,aswellas
onrareclasses.Thebiggestriskwhenyoucompressyour thecurrentlimitationsofourworkandpossiblesolutions.
modelisallofasuddenitbecomes[problematic].”—P8
7.1 DesigningCompression-AwareMLWorkflows
ParticipantsusedtheBehaviorsviewtoanalyzemodelbehavioracross ThroughoutthedesignandanalysisofCOMPRESS AND COMPARE,
anentiredatasettoensurethatcompressionhadnotinducedbiases weencounteredcompression-specificchallenges.WhilecompressionanalysiscouldbeconsideredaspecialcaseofgeneralMLevaluation, Additionally, featuresdemonstratedin COMPRESS AND COMPARE
ouruserstudyparticipantsstruggledtoextendtraditionalmodelevalua- maybeworthintegratingintogeneral-purposeMLtools,helpingthose
tionworkflowstotheircompression-specifictasks.Ourworksuggests tools cover a broader range of existing workflows and encouraging
waysfuturetoolscanimprovetheoverallMLdevelopmentprocessby practitionerstofocusonefficiencyearlierinthedevelopmentprocess.
integratingcompressionconsiderations:
7.2 LimitationsandFutureWork
Bridgingdata-andmodel-centricevaluations. MostexistingML
developmenttoolseitherfocusondata-centricevaluationsofmodelac- WedesignedCOMPRESSANDCOMPAREtosupportpractitionerscur-
curacyandbehavior[5,6,50,54]orarchitecture-specificevaluationsof rentcompressionworkflows;however,asmodelcompressionbecomes
modelinternallayercharacteristics[25,42].Incontrast,wefoundthat amoreestablishedandstandardizeddiscipline,itispossiblethatmany
linkingdata-centric(i.e.,Behaviors)andarchitecture-specificvisualiza- oftheiterativeworkflowsweobservedinthisstudywillbesuperseded
tions(i.e.,Layers)helpedusersinterpretthefunctionalcharacteristics byautomatedapproaches. However,interactivevisualizationhaspo-
ofmodelcomponents,similartosystemslikeDeepCompare[40]and tentialbenefitsforcompressionworkevenifautomatedapproaches
ActiVis[28].Forinstance,inourcasestudiesanduserstudies,partici- eventually become standard. First, even though effective AutoML
pantsusedtheconnectionbetweentheLayersandBehaviorsviewsto strategiesexist,thesetechniquesoftenstillrequiredatascientiststo
identifythatcompressingbatchnormalizationlayersdirectlyworsened investconsiderabletimetodistillmodelbehaviorintoasingleobjective
thequalityofthemodel’soutputs.Byidentifyingafunctionalrelation- function[29].Second,COMPRESSANDCOMPAREseeksnotonlyto
shipbetweenthemodel’sarchitectureanditsbehavior,userswerethen helppractitionersproducethebestcompressedmodel,butalsotohelp
abletoideatenew,better-performingexperiments(e.g.,removingcom- thembuildintuitionaboutcompressiontechniquesandhowtheyaffect
pressiononnormalizationlayers).Bydevelopingjointvisualizations modelcharacteristics.JustaspriorworkvisualizingMLmodelinter-
ofevaluationdataandmodelarchitectures,compressiontoolscanhelp nalsandbehaviors[30,48,69]hasallowedpeopletounderstandhow
usersconnectaspectsofamodel’sdesigntochangesinitsbehavior. thesemodelswork,abettercollectiveunderstandingofcompression
canmakethesetechniquesmoreaccessibleandinspirenewapproaches.
Negotiatingtrade-offsbetweenmodelqualitymetrics.Compression
Ouruserstudyandpriorformativeresearchprimarilyinterviewed
practitionersoftentradeoffbetweenmodelsize,latency,andaccuracy.
compressionexpertsandMLengineerswhowerealreadywell-versed
WhileexistingMLpipelineshaveaddressedsimilarissuesbetween
in model compression. While this allowed us to get the most rele-
accuracyandfairness[58]andaccuracyacrosstasks[35],interactive
vant feedback on how to design compression-specific systems, our
modelselectiontoolshavenotexplicitlyexploredhelpingpractitioners
systemandtakeawaysdonotconsidertheneedsofnoviceusers.Itis
makethesetrade-offs, e.g., byidentifyingParetofrontiers. Further,
ouruserstudyparticipantsindicatedthatefficiencyandaccuracybud-
likelyCOMPRESSANDCOMPAREcouldbeextendedtosupportML
practitionerslessaccustomedtocompression,suchasbyprovidingsug-
getsareoftencollaborativeandmalleabletargets,asopposedtohard
gestionsonwhattechniquestotryorincorporatingagraphicalinterface
quantitativethresholds.Byvisualizingexperimentalresultsandmetric
toruncompressionexperiments. Suchadaptationswouldempower
trade-offs,compressiontoolscanhelppractitionerscommunicatetheir
non-expertstoapplycompression,buttheycouldalsosupportcompres-
constraintsandadvocateforbudgetchangeswhenneeded.
sionexpertsinrunningon-the-flyexperimentsbasedontheirinsights
Trackingmodelprovenanceduringiterativedevelopment. Unlike
fromCOMPRESSANDCOMPARE,suchasremovingcompressionfrom
model architecture and hyperparameter search, where experiments
aspecificlayerormakingaslightchangetothesparsityvalue.
areoftensimpleCartesianproductsofseveralvariables,compression
Further, several participants suggested extensions to COMPRESS
experimentationismorereadilymodeledasatreeof“recipes”where
AND COMPAREthatcouldhelpitbettermatchthespecificneedsof
nodesrepresentmodelsandedgesrepresentoperations.Practitioners
theirteams,includingsupportingverylargedatasetsandmodelsaswell
oftenstartwithasinglemodelorafewrelatedmodelsthatareknownto
asdisplayingcustomefficiencymetrics. Althoughthetoolcurrently
performwellandapplyvaryingcompressionrecipestothem,creating
supportsrunningmodelcomputationsremotelyorinadvance,itdoes
the branching structure depicted in the Model Map. This process
require the models being compared to be loaded simultaneously in
resultsinmanyinterconnectedmodels,anditcanbechallengingto
memory so that each layer can be compared one-at-a-time. More
keep track of the operations that created a particular model and its
efficient comparison techniques that do not require jointly loading
relationshiptoothermodels.Userstudyparticipantsfoundvisualizing
severalmodelsmayhelpCOMPRESSANDCOMPAREscalemoreeasily.
modelsinthistreestructurehelpedthembuildintuitionforaspectsof
Finally, a key requirement of COMPRESS AND COMPARE is its
experimentsthatworkedwellandhowfutureexperimentsmaybehave.
integrationbetweenthevisualizationsystem’sandauser’smodelde-
Futurecompressiontoolsmayconsiderasimilartreevisualizationor
velopmentcode. However, likemanyotherprototypetoolsforML
newwaystocommunicatemodelprovenancetopractitioners.
development,thecapabilitiesthatmakeCOMPRESSANDCOMPAREa
Comparingcomplexdifferencesacrossmanymodels.Existingtools versatiletoolcanalsonecessitateconsiderableset-upwork,particularly
(includingthoseforcompression)tendtofocusonprofilingandevalu- forcustommodelarchitectures.Userstudyparticipantsnotedthatthe
atingasinglemodel[1,6,25,53].Incontrast,ourstudyunderscoresthe potentialdifficultyofimportingmodelsintothetoolcouldbeacritical
valueofdirectlysupportingcomparison[18,54].Byvisuallyjuxtapos- hurdletoacceptingitintotheirworkflow. Futureworkisneededto
ingmetrics,predictions,andinternalsfromseveralmodels,COMPRESS designcode-levelinterfacesthatlinkCOMPRESSANDCOMPAREinto
ANDCOMPAREreducesthecognitiveloadrequiredtodeterminewhich thetoolspractitionersalreadyusetodevelopmodels.
differencesaremostactionable. Oursystemenablesworkflowsthat
rapidly transition between comparative relationships, ranging from 8 CONCLUSION
black-boxcomparisonsoftop-levelmetricstocomparisonsofindivid-
MakingMLmodelssmaller,faster,andmoreenergy-efficientcanen-
uallayeractivations.Futuretoolscanprioritizecomparisonandlook
able exciting new user experiences and broaden access to existing
to practitioners’ existing comparison strategies to understand when
ones.Ourworkformspartofanascentbodyofliteratureonfacilitat-
linkingmultiplecomparativerelationshipsleadstoproductiveinsights.
ingtheprocessofcompressingmodels,whichcanhelpmakethese
Thesecompression-specificchallengesprovideabasisforthedesign usecasespracticalasmodelsbecomeincreasinglylargeandpowerful.
offuturecompression-vistools,buttheycouldalsosuggestwaysto ThroughthedesignandevaluationofCOMPRESSANDCOMPARE,we
improvegeneralMLanalysisworkflows.Forinstance,ouruserstudy aimedtounderstandandaddresschallengesinMLmodeldevelopment
participantsfoundtheModelMaptreestructuretobehighlyintuitive, madesalientbycompression,includingtheneedforextensiveiteration,
andsomesuggestedapplyingittootherstagesofmodeldevelopment human-centeredtrade-offsbetweenuserexperiencemetrics,andsubtle
(P4, P18, P15), such as creating a timeline-based Model Map that changesinmodelbehavior. Futurecompression-focuseddatavisual-
organizedmodelresultsbasedonwhenauserraneachexperiment. izationresearchcancontinuetomakecreatingefficientMLmodels
Moreover,extendingthelayer-wiseactivationcomparisonintheLay- easierforawiderrangeofdevelopersandteams,helpingthemmake
ers view could support complex hyperparameter search workflows. theexperiencestheyenvisionareality.ACKNOWLEDGMENTS [17] C.Giattino,E.Mathieu,V.Samborska,J.Broden,andM.Roser.Artificial
intelligence. OurWorldinData, 2023. https://ourworldindata.
WethankourcolleaguesatApplefortheirguidanceonthisresearch org/artificial-intelligence.1
andourcasestudyparticipantsforgenerouslysharingtheirtimeand [18] M.Gleicher.ConsiderationsforVisualizingComparison.IEEETransac-
knowledgewithus.WeespeciallythankGuillaumeSeguinandXiaoyi tionsonVisualizationandComputerGraphics,24(1):413–423,2018.doi:
Zhangforlendingtheirexpertiseoncompressionandencouragingus 10.1109/TVCG.2017.27441994,9
topursuethislineofresearch. [19] M.Gleicher,A.Barve,X.Yu,andF.Heimerl. Boxer:InteractiveCom-
parisonofClassifierResults.ComputerGraphicsForum,39(3):181–193,
2020.doi:10.1111/CGF.139722
[20] S.Han, H.Mao, andW.J.Dally. DeepCompression: Compressing
REFERENCES
DeepNeuralNetworkswithPruning,TrainedQuantizationandHuffman
[1] A.Bäuerle,Á.A.Cabrera,F.Hohman,M.Maher,D.Koski,X.Suau, Coding. InProceedingsoftheInternationalConferenceonLearning
T.Barik,andD.Moritz. Symphony: ComposingInteractiveInterfaces Representations(ICLR),2016.doi:10.48550/arXiv.1510.001492
forMachineLearning.InProceedingsoftheCHIConferenceonHuman [21] K.He,X.Zhang,S.Ren,andJ.Sun.DeepResidualLearningforImage
FactorsinComputingSystems,pp.210:1–210:14.ACM,NewYork,2022. Recognition. In Proceedings of the Conference on Computer Vision
doi:10.1145/3491102.35021022,9 andPatternRecognition(CVPR),pp.770–778.IEEEComputerSociety,
[2] C. Baykal, L. Liebenwein, I. Gilitschenski, D. Feldman, and D. Rus. Piscataway,NJ,2016.doi:10.1109/CVPR.2016.906,7
SiPPingneuralnetworks:Sensitivity-informedprovablepruningofneural [22] Y.He,G.Kang,X.Dong,Y.Fu,andY.Yang. SoftFilterPruningfor
networks.arXiv,2019.doi:10.48550/arXiv.1910.054222 AcceleratingDeepConvolutionalNeuralNetworks.InProceedingsofthe
[3] R.A.BeckerandW.S.Cleveland.BrushingScatterplots.Technometrics, InternationalJointConferenceonArtificialIntelligence,pp.2234–2240.
29(2):127–142,1987.doi:10.2307/12697684 IJCAI,2018.doi:10.24963/IJCAI.2018/3092
[4] E.M.Bender,T.Gebru,A.McMillan-Major,andS.Shmitchell.Onthe [23] F.Hohman,M.Kahng,R.Pienta,andD.H.Chau. VisualAnalyticsin
DangersofStochasticParrots: CanLanguageModelsBeTooBig? In DeepLearning: AnInterrogativeSurveyfortheNextFrontiers. IEEE
ConferenceonFairness,Accountability,andTransparency(FAccT),pp. TransactionsonVisualizationandComputerGraphics,25(8):2674–2693,
610–623.ACM,2021.doi:10.1145/3442188.34459221 2018.doi:10.1109/TVCG.2018.28433692
[5] A.Boggust,B.Carter,andA.Satyanarayan. EmbeddingComparator: [24] F.Hohman,M.B.Kery,D.Ren,andD.Moritz.ModelCompressionin
VisualizingDifferencesinGlobalStructureandLocalNeighborhoods Practice:LessonsLearnedfromPractitionersCreatingOn-deviceMachine
viaSmallMultiples.InProceedingsoftheInternationalConferenceon LearningExperiences.InProceedingsoftheCHIConferenceonHuman
IntelligentUserInterfaces(IUI),pp.746–766.ACM,NewYork,2022. FactorsinComputingSystems,pp.645:1–645:18.ACM,NewYork,NY,
doi:10.1145/3490099.35111222,3,9 2024.doi:10.1145/3613904.36421091,2,3,4
[6] Á.A.Cabrera,E.Fu,D.Bertucci,K.Holstein,A.Talwalkar,J.I.Hong, [25] F.Hohman,C.Wang,J.Lee,J.Görtler,D.Moritz,J.P.Bigham,Z.Ren,
andA.Perer.Zeno:AnInteractiveFrameworkforBehavioralEvaluation C.Foret,Q.Shan,andX.Zhang. Talaria:InteractivelyOptimizingMa-
ofMachineLearning.InProceedingsoftheCHIConferenceonHuman chineLearningModelsforEficientInference.InProceedingsoftheCHI
FactorsinComputingSystems,pp.419:1–419:14.ACM,NewYork,2023. ConferenceonHumanFactorsinComputingSystems,pp.648:1–648:19.
doi:10.1145/3544548.35812689 ACM,NewYork,NY,2024.doi:10.1145/3613904.36426282,3,7,9
[7] Y.Cheng,D.Wang,P.Zhou,andT.Zhang. ModelCompressionand [26] S.Hooker,A.Courville,G.Clark,Y.Dauphin,andA.Frome. What
AccelerationforDeepNeuralNetworks: ThePrinciples,Progress,and DoCompressedDeepNeuralNetworksForget? arXiv,2019.doi: 10.
Challenges.IEEESignalProcessingMagazine,35(1):126–136,2018.doi: 48550/arXiv.1911.052486
10.1109/MSP.2017.27656952 [27] S.Hooker,N.Moorosi,G.Clark,S.Bengio,andE.Denton.Characterising
[8] T.Choudhary,V.Mishra,A.Goswami,andJ.Sarangapani.Acomprehen- BiasinCompressedModels. arXiv,2020.doi: 10.48550/arXiv.2010.
sivesurveyonmodelcompressionandacceleration.ArtificialIntelligence 030582,3,6
Review,53:5113–5155,2020.doi:10.1007/s10462-020-09816-72 [28] M.Kahng,P.Y.Andrews,A.Kalro,andD.H.Chau. ActiVis: Visual
[9] J.Deng,W.Dong,R.Socher,L.-J.Li,K.Li,andL.Fei-Fei. ImageNet: ExplorationofIndustry-ScaleDeepNeuralNetworkModels.IEEETrans-
Alarge-scalehierarchicalimagedatabase.InProceedingsoftheConfer- actionsonVisualizationandComputerGraphics,24(1):88–97,2018.doi:
enceonComputerVisionandPatternRecognition(CVPR),pp.248–255. 10.1109/TVCG.2017.27447182,9
CVF/IEEE,2009.doi:10.1109/CVPR.2009.52068487 [29] S.K.Karmaker(“Santu”),M.M.Hassan,M.J.Smith,L.Xu,C.Zhai,
[10] L.Deng,G.Li,S.Han,L.Shi,andY.Xie. ModelCompressionand andK.Veeramachaneni.AutoMLtoDateandBeyond:Challengesand
HardwareAccelerationforNeuralNetworks:AComprehensiveSurvey. Opportunities.ACMComputingSurveys,54(8):175:1–175:36,2022.doi:
ProceedingsoftheIEEE,108(4):485–532,2020.doi: 10.1109/JPROC. 10.1145/34709189
2020.29764752 [30] A.Karpathy,J.Johnson,andL.Fei-Fei.VisualizingandUnderstanding
[11] J.FrankleandM.Carbin.TheLotteryTicketHypothesis:FindingSparse, RecurrentNetworks.arXiv,2015.doi:10.48550/arXiv.1506.020789
TrainableNeuralNetworks.InProceedingsoftheInternationalConfer- [31] N.Lee,T.Ajanthan,andP.H.S.Torr.SNIP:single-shotnetworkpruning
enceonLearningRepresentations(ICLR),2019.doi: 10.48550/arXiv. based on connection sensitivity. In Proceedings of the International
1803.036352 ConferenceonLearningRepresentations(ICLR),2019.doi:10.48550/
[12] E.FrantarandD.Alistarh.SparseGPT:MassiveLanguageModelsCan arXiv.1810.023402
beAccuratelyPrunedinOne-Shot. InProceedingsoftheInternational [32] G.Li,J.Wang,H.Shen,K.Chen,G.Shan,andZ.Lu. CNNPruner:
ConferenceonMachineLearning(ICML),vol.202,pp.10323–10337. Pruningconvolutionalneuralnetworkswithvisualanalytics.IEEETrans-
PMLR,2023.doi:10.48550/arXiv.2301.007742 actionsonVisualizationandComputerGraphics,27(2):1364–1373,2021.
[13] N. Friedman. Introducing github copilot: your ai pair pro- doi:10.1109/TVCG.2020.30304612
grammer. https://github.blog/news-insights/product-news/ [33] Z.Li,E.Wallace,S.Shen,K.Lin,K.Keutzer,D.Klein,andJ.Gonza-
introducing-github-copilot-ai-pair-programmer/, February lez. TrainLarge,ThenCompress:RethinkingModelSizeforEfficient
2022.Accessed:2024-08-01.1 TrainingandInferenceofTransformers. InProceedingsoftheInterna-
[14] T.Gale,E.Elsen,andS.Hooker. TheStateofSparsityinDeepNeural tionalConferenceonMachineLearning(ICML),vol.119,pp.5958–5968.
Networks.arXiv,2019.doi:10.48550/arXiv.1902.095742 PMLR,2020.doi:10.48550/arXiv.2002.117946
[15] N.Gamboa,K.Kudrolli,A.Dhoot,andA.Pedram. Campfire: Com- [34] L.Liebenwein,C.Baykal,B.Carter,D.Gifford,andD.Rus. Lostin
pressible,Regularization-Free,StructuredSparseTrainingforHardware Pruning:TheEffectsofPruningNeuralNetworksbeyondTestAccuracy.
Accelerators.arXiv,2020.doi:10.48550/arXiv.2001.032532 ProceedingsofMachineLearningandSystems,3:93–138,2021.doi:0.
[16] A.Gholami,S.Kim,Z.Dong,Z.Yao,M.W.Mahoney,andK.Keutzer. 48550/arXiv.2103.030142,3,6
ASurveyofQuantizationMethodsforEfficientNeuralNetworkInfer- [35] X.Lin,H.Zhen,Z.Li,Q.-F.Zhang,andS.Kwong. ParetoMulti-Task
ence. InG.K.Thiruvathukal, Y.Lu, J.Kim, Y.Chen, andB.Chen, Learning.AdvancesinNeuralInformationProcessingSystems(NeurIPS),
eds.,Low-PowerComputerVision:ImprovingtheEfficiencyofArtificial 32:12037–12047,2019.doi:10.48550/arXiv.1912.128549
Intelligence,pp.291–326.CRCPress,BocaRaton,FL,2022.doi: 10. [36] Z.Liu,P.Luo,X.Wang,andX.Tang.DeepLearningFaceAttributesin
1201/9781003162810-132 theWild. InProceedingsoftheInternationalConferenceonComputerVision(ICCV),pp.3730–3738.IEEEComputerSociety,Washington,D.C., Spaces. InProceedingsoftheInternationalConferenceonIntelligent
2015.doi:10.1109/ICCV.2015.4256 UserInterfaces(IUI),pp.418–432.ACM,NewYork, 2022.doi: 10.
[37] Z.Liu,M.Sun,T.Zhou,G.Huang,andT.Darrell.RethinkingtheValue 1145/3490099.35111372,3,9
ofNetworkPruning.InProceedingsoftheInternationalConferenceon [55] X.Suau,L.Zappella,andN.Apostoloff.FilterDistillationforNetwork
LearningRepresentations(ICLR),2019.doi:10.48550/arXiv.1810.05270 Compression.InProceedingsoftheWinterConferenceonApplicationsof
2 ComputerVision(WACV),pp.3129–3138.IEEE,Piscataway,NJ,2020.
[38] G. Menghani. Efficient Deep Learning: A Survey on Making Deep doi:10.1109/WACV45572.2020.90935462
LearningModelsSmaller,Faster,andBetter.ACMComputingSurveys, [56] H.Suresh,D.Shanmugam,T.Chen,A.G.Bryan,A.D’Amour,J.Guttag,
55(12):259:1–259:37,2023.doi:10.1145/35789382 andA.Satyanarayan. Kaleidoscope: Semantically-grounded,context-
[39] P.Molchanov,A.Mallya,S.Tyree,I.Frosio,andJ.Kautz. Importance specificMLmodelevaluation. InProceedingsoftheCHIConference
EstimationforNeuralNetworkPruning.InProceedingsoftheConference onHumanFactorsinComputingSystems,pp.775:1–775:13.ACM,New
onComputerVisionandPatternRecognition(CVPR),pp.11264–11272. York,2023.doi:10.1145/3544548.35814822
CVF/IEEE,2019.doi:10.1109/CVPR.2019.011522 [57] M.Treviso,J.-U.Lee,T.Ji,B.vanAken,Q.Cao,M.R.Ciosici,M.Hassid,
[40] S.Murugesan,S.Malik,F.Du,E.Koh,andT.M.Lai. DeepCompare: K.Heafield,S.Hooker,C.Raffel,P.H.Martins,A.F.T.Martins,J.Z.
visualandinteractivecomparisonofdeeplearningmodelperformance. Forde,P.Milder,E.Simpson,N.Slonim,J.Dodge,E.Strubell,N.Bal-
IEEEComputerGraphicsandApplications,39(5):47–59,2019.doi:10. asubramanian,L.Derczynski,I.Gurevych,andR.Schwartz. Efficient
1109/MCG.2019.29190332,3,9 MethodsforNaturalLanguageProcessing:ASurvey.Transactionsofthe
[41] S. Narkar, Y. Zhang, Q. V. Liao, D. Wang, and J. D. Weisz. Model AssociationforComputationalLinguistics,11:826–860,2023.doi: 10.
LineUpper:Supportinginteractivemodelcomparisonatmultiplelevels 1162/tacl_a_005772
forautoml.InProceedingsoftheInternationalConferenceonIntelligent [58] A.Valdivia,J.Sánchez-Monedero,andJ.Casillas. Howfaircanwego
UserInterfaces(IUI),pp.170–174.ACM,NewYork,NY,2021.doi:10. inmachinelearning?Assessingtheboundariesofaccuracyandfairness.
1145/3397481.34506582 InternationalJournalofIntelligentSystems,36(4):1619–1643,2021.doi:
[42] C.Olah,A.Mordvintsev,andL.Schubert.FeatureVisualization.Distill, 10.1002/INT.223549
2017. https://distill.pub/2017/feature-visualization.doi: [59] P.Villalobos,J.Sevilla,T.Besiroglu,L.Heim,A.Ho,andM.Hobbhahn.
10.23915/distill.000079 MachineLearningModelSizesandtheParameterGap.arXiv,2022.doi:
[43] J.P.Ono,S.Castelo,R.Lopez,E.Bertini,J.Freire,andC.Silva.Pipeline- 10.48550/arXiv.2207.028521
Profiler:AVisualAnalyticsToolfortheExplorationofAutoMLPipelines. [60] J.Wang,S.Liu,andW.Zhang.VisualAnalyticsforMachineLearning:
IEEETransactionsonVisualizationandComputerGraphics,27(2):390– ADataPerspectiveSurvey. IEEETransactionsonVisualizationand
400,2021.doi:10.1109/TVCG.2020.30303613 ComputerGraphics,2024.Toappear.doi:10.1109/TVCG.2024.3357065
[44] OpenAI. Hello gpt-4o. https://openai.com/index/ 2
hello-gpt-4o/,May2024.Accessed:2024-08-01.1 [61] D.K.I.Weidele,J.D.Weisz,E.Oduor,M.Muller,J.Andres,A.Gray,
[45] A.Paszke,S.Gross,F.Massa,A.Lerer,J.Bradbury,G.Chanan,T.Killeen, andD.Wang.AutoAIViz:OpeningtheBlackboxofAutomatedArtificial
Z.Lin,N.Gimelshein,L.Antiga,etal. PyTorch: AnImperativeStyle, IntelligencewithConditionalParallelCoordinates.InProceedingsofthe
High-PerformanceDeepLearningLibrary. AdvancesinNeuralInfor- InternationalConferenceonIntelligentUserInterfaces(IUI),p.308–312.
mation Processing Systems (NeurIPS), 32:8024–8035, 2019. doi: 10. ACM,NewYork,NY,2020.doi:10.1145/3377325.33775383
48550/arXiv.1912.017037 [62] M.M.Welsh,D.Koski,M.Sarabia,N.Sivakumar,I.Arawjo,A.Joshi,
[46] C.Raffel,N.Shazeer,A.Roberts,K.Lee,S.Narang,M.Matena,Y.Zhou, M.Doumbouya, X.Suau, L.Zappella, andN.Apostoloff. Dataand
W.Li,andP.J.Liu. ExploringtheLimitsofTransferLearningwitha NetworkIntrospectionKit,2023.2
UnifiedText-to-TextTransformer.JournalofMachineLearningResearch, [63] T.Wu,M.T.Ribeiro,J.Heer,andD.S.Weld. Errudite: Scalable,Re-
21:140:1–140:67,2020.doi:10.5555/3455716.34558565 producible,andTestableErrorAnalysis. InProceedingsoftheAnnual
[47] P.Rajpurkar,J.Zhang,K.Lopyrev,andP.Liang. SQuAD:100,000+ MeetingoftheAssociationforComputationalLinguistics(ACL),pp.747–
questionsformachinecomprehensionoftext.InProceedingsoftheCon- 763.ACL,Stroudsburg,PA,2019.doi:10.18653/V1/P19-10732
ferenceonEmpiricalMethodsinNaturalLanguageProcessing(EMNLP), [64] X.Xuan,X.Zhang,O.-H.Kwon,andK.-L.Ma. VAC-CNN:AVisual
pp.2383–2392.ACL,Stroudsburg,PA,2016.doi:10.18653/V1/D16-1264 AnalyticsSystemforComparativeStudiesofDeepConvolutionalNeural
5 Networks.IEEETransactionsonVisualizationandComputerGraphics,
[48] E.Reif,A.Yuan,M.Wattenberg,F.B.Viegas,A.Coenen,A.Pearce,and 28(6):2326–2337,2022.doi:10.1109/TVCG.2022.31653472
B.Kim.VisualizingandMeasuringtheGeometryofBERT.Advancesin [65] R.Yu,A.Li,C.Chen,J.Lai,V.I.Morariu,X.Han,M.Gao,C.Lin,
NeuralInformationProcessingSystems(NeurIPS),32:8592–8600,2019. andL.S.Davis. NISP:PruningNetworksUsingNeuronImportance
doi:10.48550/arXiv.1906.027159 ScorePropagation.InProceedingsoftheConferenceonComputerVision
[49] A.Renda,J.Frankle,andM.Carbin. ComparingRewindingandFine- andPatternRecognition(CVPR),pp.9194–9203.CVF/IEEEComputer
tuninginNeuralNetworkPruning. InProceedingsoftheInternational Society,2018.doi:10.1109/CVPR.2018.009582
ConferenceonLearningRepresentations(ICLR),2020.doi:10.48550/ [66] S.Yu,T.Chen,J.Shen,H.Yuan,J.Tan,S.Yang,J.Liu,andZ.Wang.
arXiv.2003.023892 UnifiedVisualTransformerCompression. InProceedingsoftheInter-
[50] S.Robertson,Z.J.Wang,D.Moritz,M.B.Kery,andF.Hohman.Angler: nationalConferenceonLearningRepresentations(ICLR),2022.doi:10.
HelpingMachineTranslationPractitionersPrioritizeModelImprovements. 48550/arXiv.2203.082432
InProceedingsoftheCHIConferenceonHumanFactorsinComputing [67] J.Yuan,C.Chen,W.Yang,M.Liu,J.Xia,andS.Liu.Asurveyofvisual
Systems,pp.832:1–832:20.ACM,NewYork,2023.doi:10.1145/3544548 analyticstechniquesformachinelearning.ComputationalVisualMedia,
.35807902,9 7(1):3–36,2021.doi:10.1007/S41095-020-0191-72
[51] M.Sandler,A.Howard,M.Zhu,A.Zhmoginov,andL.-C.Chen. Mo- [68] M.Zaharia,A.Chen,A.Davidson,A.Ghodsi,S.A.Hong,A.Konwinski,
bileNetV2:InvertedResidualsandLinearBottlenecks.InProceedingsof S.Murching,T.Nykodym,P.Ogilvie,M.Parkhe,etal. Accelerating
theConferenceonComputerVisionandPatternRecognition(CVPR),pp. theMachineLearningLifecyclewithMLflow.IEEEDataEngineering
4510–4520.CVF/IEEE,2018.doi:10.1109/CVPR.2018.004747 Bulletin,41(4):39–45,2018.doi:10.1145/3399579.33998672
[52] S.Schelter,J.-H.Böse,J.Kirschnick,T.Klein,andS.Seufert.Automat- [69] M.D.ZeilerandR.Fergus.VisualizingandUnderstandingConvolutional
icallyTrackingMetadataandProvenanceofMachineLearningExperi- Networks. InProceedingsoftheEuropeanConferenceonComputer
ments. InWorkshoponMLSystemsatAdvancesinNeuralInformation Vision(ECCV),vol.8689,pp.818–833.Springer,NewYork,NY,2014.
ProcessingSystems(NIPS),2017.2,3 doi:10.1007/978-3-319-10590-1_539
[53] U.Schlegel,S.Schiegg,andD.A.Keim.ViNNPruner:VisualInteractive [70] M.ZhuandS.Gupta.ToPrune,orNottoPrune:ExploringtheEfficacy
PruningforDeepLearning.InMachineLearningMethodsinVisualisation ofPruningforModelCompression. arXiv,2017.doi: 10.48550/arXiv.
forBigData(MLVis@EuroVis),pp.13–17.EurographicsAssociation, 1710.018782,6
Eindhoven,TheNetherlands,2022.doi:10.2312/MLVIS.202210702,3,
9
[54] V.Sivaraman, Y.Wu, andA.Perer. Emblaze: IlluminatingMachine
LearningRepresentationsthroughInteractiveComparisonofEmbedding