Understanding How Blind Users Handle Object Recognition
Errors: Strategies and Challenges
JonggiHong HernisaKacorri
DepartmentofComputerScience CollegeofInformation,UMIACS
StevensInstituteofTechnology UniversityofMaryland,CollegePark
Hoboken,NJ,USA CollegePark,MD,USA
jhong8@stevens.edu hernisa@umd.edu
ABSTRACT 1 INTRODUCTION
Objectrecognitiontechnologiesholdthepotentialtosupportblind Thefieldofcomputervisionhasmadesignificantstrides,achieving
andlow-visionpeopleinnavigatingtheworldaroundthem.How- considerablebenchmarkingratesinobjectrecognitiontasks.Yet,de-
ever,thegapbetweenbenchmarkperformancesandpracticalus- spitetheseadvancements,real-worldapplicationsoftenencounter
abilityremainsasignificantchallenge.Thispaperpresentsastudy substantialdiscrepanciesbetweenexpectedandobservedperfor-
aimedatunderstandingblindusers’interactionwithobjectrecog- mance[21,62].Factorssuchascomplextasks,resourcelimitations
nitionsystemsforidentifyingandavoidingerrors.Leveraginga (e.g.,mobiledeviceprocessing),andinputsthatdeviatefromthe
pre-existingobjectrecognitionsystem,URCam,fine-tunedforour trainingdata(e.g.,classifyingimageswithpersonalitemsorclut-
experiment,weconductedauserstudyinvolving12blindandlow- teredbackgroundscollectedbyauser)posepersistentchallenges,
visionparticipants.Throughin-depthinterviewsandhands-oner- leadingtohigher-than-anticipatederrorratesinpracticalscenar-
roridentificationtasks,wegainedinsightsintousers’experiences, ios[9].Moreover,thevulnerabilityofobjectrecognitionsystems
challenges,andstrategiesforidentifyingerrorsincamera-based toadversarialattacksfurthercompoundsthesechallenges[31,47].
assistivetechnologiesandobjectrecognitionsystems.Duringinter- Whileimageclassifiersholdpotentialforsupportingtheblindcom-
views,manyparticipantspreferredindependenterrorreview,while munityindaytodaytasks,theyarehinderedbytheirinabilityto
expressingapprehensiontowardmisrecognitions.Intheerroriden- effectivelyconveyrecognitionerrors,especiallywhentactileor
tificationtask,participantsvariedviewpoints,backgrounds,and olfactoryverificationisimpractical(e.g.,distantobjectsorscenes).
objectsizesintheirimagestoavoidandovercomeerrors.Evenafter Thus,despiteadvancements,thegapbetweenbenchmarkperfor-
repeatingthetask,participantsidentifiedonlyhalfoftheerrors, manceandreal-worldusabilityremainsacriticalconcernforassis-
andtheproportionoferrorsidentifieddidnotsignificantlydiffer tiveobjectrecognitionsystems.
fromtheirfirstattempts.Basedontheseinsights,weofferimpli- Inthiswork,weexplorethechallengesthatblindusersface
cationsfordesigningaccessibleinterfacestailoredtotheneedsof whenhandlingobjectrecognitionerrorsandthestrategiestheyuse
blindandlow-visionusersinidentifyingobjectrecognitionerrors. toovercomethem.Specifically,weconductauserstudywith12
blindandlow-visionparticipants,usingatwo-prongedapproach:a
CCSCONCEPTS semi-structuredremoteinterviewandahands-onerroridentifica-
•Human-centeredcomputing→Humancomputerinterac- tiontaskinparticipants’homes.Intheinterview,weaimtoanswer
tion(HCI);Empiricalstudiesininteractiondesign;Ubiqui- thefollowingresearchquestion:“Whataretheexperiencesofblind
tousandmobiledevices. andlow-visionuserswitherrorhandlingincamera-basedassistive
technologies?” Participantsdescribehowoftentheyverifyrecogni-
KEYWORDS tionresults,thefrequencywithwhichtheyencountererrors,the
importancetheyplaceontheseerrors,andthechallengestheyface
objectrecognitionerrors,camera-basedassistivetechnology,blind,
inidentifyingthem.Tobettercontextualizetheirresponses,we
visualimpairment
discusstheirconfidenceinphotocomposition,thefrequencyof
ACMReferenceFormat: use,andthepurposesforeachoftheircamera-basedassistivetech-
JonggiHongandHernisaKacorri.2024.UnderstandingHowBlindUsers nologies.Theinterviewisthenfollowedbytheexperimentwith
HandleObjectRecognitionErrors:StrategiesandChallenges.InThe26th
anerroridentificationtask,whereweaimtoanswerthefollowing
InternationalACMSIGACCESSConferenceonComputersandAccessibility
researchquestions:“Howdoblindandlow-visionusersidentifyand
(ASSETS’24),October27–30,2024,St.John’s,NL,Canada.ACM,NewYork,
respondtoobjectrecognitionerrors,andwhataretherelationships
NY,USA,15pages.https://doi.org/10.1145/3663548.3675635
betweenrecognitionerrortypes,decision-makingtime,confidence
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor levels,andtaskrepetition?”ParticipantsinteracttwicewithURCam,
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed anobjectrecognitioniOSappthatwedevelopedforthisexperi-
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
ment.Wefine-tunedtheunderlyingmodeltorecognize15object
onthefirstpage.Copyrightsforcomponentsofthisworkownedbyothersthanthe
author(s)mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,or stimulirelevanttoourstudy.Theappprovidesobjectlabelsora
republish,topostonserversortoredistributetolists,requirespriorspecificpermission ’Don’tknow’responsewhentherecognitionconfidenceislow.To
and/orafee.Requestpermissionsfrompermissions@acm.org.
bettercontextualizetheresults,wereporttheaccuracyofURCam
ASSETS’24,October27–30,2024,St.John’s,NL,Canada
©2024Copyrightheldbytheowner/author(s).PublicationrightslicensedtoACM. duringthetaskandmanuallycodethestrategiesparticipantsuse
ACMISBN979-8-4007-0677-6/24/10...$15.00 forcapturingphotoswhenURCamrespondswith’Don’tknow.’
https://doi.org/10.1145/3663548.3675635
4202
guA
6
]CH.sc[
1v30330.8042:viXraASSETS’24,October27–30,2024,St.John’s,NL,Canada JonggiHongandHernisaKacorri
Findingsfromourstudyprovideinsightsonblindusers’interac- Table1:CharacteristicsofrelatedstudiesonerrorsinAI-
tionwitherror-proneobjectrecognitiontechnologies.Interviews infusedassistivetechnologyjuxtaposedwithours.
indicatethatmanyparticipantspreferredtoindependentlyreview
photoqualityandidentifyerrorsincamera-basedassistivetech- [59] [75] [18] [74] [37] [10] Ours
nologies. They often triangulated information using contextual
Blind&lowvision 6,100 7 15 22,13 12 20 12
cues,theirremainingvision,multipletrials,orotherAIapps,rather
Sighted 235 12
thanseekingsightedassistance.Althoughthefrequencyofencoun-
terederrorsvariedamongparticipants,mostexpressedconcern Photo • • • •
aboutmisrecognitions.However,somedidnotreporteddifficulty Speech •
in identifying these errors. During the error identification task, Other • • •
weobservedthatparticipantscouldidentify,onaverage,onlyhalf
Interview • • • • •
of the errors, with most of these being false positives. Notably, Survey • •
participantsstrategicallyadjustedviewpoints,backgrounds,and Focusgroup •
objectsizetoavoidthe“Don’tknow”predictions,oftenrotating Crowdsourcing •
objects or the camera to reveal different angles. We found that Labstudy • • •
participantstendedtomakedecisionsmorequicklywhentheyfelt
Imagecaptioning • •
moreconfidentabouttheaccuracyofthepredictions.Comparing Speechrecognition •
participants’firstandsecondattemptsatthesametask,wedidnot Objectrecognition •
observeasignificantdifferenceintheproportionoferrorsidenti- Navigation •
fied.However,therewasanotabledecreaseintimespenttomakea Obfuscation •
decisionduringthesecondattempt.Additionally,participants’cer- Controllingacar •
taintyregardingrecognitioncorrectnessdecreasedinsubsequent
attempts,attributedmainlytoinconsistentrecognitionoutcomes
amongsimilarobjects.
2.1 InteractionswithErrorsinAITechnologyin
Thecontributionsofthisworkarethefollowing:(1)Providing
theContextoftheBlindCommunity
insightsintoblindusers’experiencesinassessingthequalityof
photosandhandlingerrorsincamera-basedassistivetechnology.(2) Previousresearchhasconsistentlydemonstratedthesignificant
Characterizingthechallengesencounteredbyblindpeopleinusing impactoferrorsontheexperiencesofblindusers.Table1illustrates
objectrecognitiontechnologies,particularlyinerroridentification previousresearchexamplesconcerningtheramificationsoferrors
anduser’sconfidence.(3)Suggestingpracticalimplicationsforthe inAI-infusedassistivetechnology.Forinstance,safetyconcerns
designofobjectrecognitionsystems,withafocusonerror-handling regardingmalfunctionsinautopilotsystemsofself-drivingvehi-
mechanisms,basedonempiricalfindings. clesposeaprimaryapprehensionforblindindividualswhoare
encouragedtousesuchvehiclesautonomously[18,19].Similar
concernsariseinsystemswhereerrorrisksarelesscriticalthan
2 RELATEDWORK
thoseinself-drivingvehiclesbutstillconsequential.Forinstance,
Objectrecognition,encompassingbothobjectdetectionandclassi- studieshaveshownthatminorerrorsinnavigationsystemscan
fication[12,73],hasbeenthesubjectofactiveresearchfordecades, leadtofrustrationanddisorientation,evenwhenthedestination
representing fundamental and inherently challenging problems isjustafewmetersaway(e.g.,[74]).Priorstudiesalsohighlighted
withincomputervision.Objectdetectionspecificallyseekstoas- the need for blind users to distinguish and handle errors when
certainthepreciselocationanddimensionsofobjectswithinan understandingimageswithAI-basedimagedescriptions[30,51].
image,oftenrepresentedthroughboundingboxes[91,92].Onthe Thesefindingsunderscoretheimportanceofuser-errorinteraction
otherhand,imageclassificationaimstodeterminewhethercertain interfacesthatprovidecontextualinformationandpredictionsfrom
objects,belongingtopredefinedclasses,arepresentwithinanim- machinelearningmodelstohelpblindusersaccuratelyassesserror
ageornot[48,57].Bothobjectdetectionandimageclassification causesandseverity.
findapplicationinadiversearrayoffields,includingaccessibility. Similarly,errorssignificantlyimpactblindusers’experiences
Justwithinthecontextoftechnologiesforblindandlowvision with object recognition systems, as blind individuals often rely
people,thefocusofthispaper,thereareamyriadofpublications. solelyonsystemoutputsduetothechallengeofverifyingthem[64,
InarecentreviewbyGamageetal.,thebreakdownhighlightsthe 75].Consequently,understandingtheimplicationsoferrorsinAI-
variousassistivetaskswherethistechnologyisbeingutilized,cov- infusedassistivetechnologyiscritical.Researchhashighlighted
eringawiderangeofcontextsfromhandlingobjectanddevices, instances where such errors have led to adverse outcomes. For
orientationandmobility,communicationandinformation,personal example, blind users tend to overtrust automatically generated
careandprotection,culturalandsportsactivities,topersonalmedi- captionsonsocialmediaimages,evenwhenthecaptionsareincor-
caltreatment[26].Giventheinherentlyerror-pronenatureofthis rectandnonsensical[59].Whilesomeerrorsinblindnavigation
technology,understandinganddesigningforuserinteractionswith systemsaremanageableinfamiliarenvironments,theybecome
predictionerrorsiscritical.Belowwesynthesizepriorliterature problematicwhentheycanleadtoembarrassingsituationswithby-
thatdiscussthisinthecontextofassistivetechnologiesforthe standers[1,55].Moreover,errorsinimagerecognitionsystems
blindandmorebroadly. used for controlling household objects can pose safety threats.
elpoeP
tupnI
sdohteM
ksaTUnderstandingHowBlindUsersHandleObjectRecognitionErrors:StrategiesandChallenges ASSETS’24,October27–30,2024,St.John’s,NL,Canada
Consequently, robust safety mechanisms are essential for such models[16,25,27,28,82].However,despiteadvancements,predict-
tools.Giventhesignificanceoferrorhandlinginobjectrecogni- ingspeechrecognitionerrorsremainsanongoingresearcharea,
tionsystemsforblindusers,thisworkdelvesintoanddelineates withcurrentmethodsachievingmoderateprecisionandrecallrates.
thechallengestheyfaceinidentifyingandrecoveringfromobject Beyondgestureandspeech,researchersarealsoendeavoring
recognitionerrorswiththenumberofparticipants,methods,and tomitigateerrorsinotherAI-infusedapplications,includingro-
taskcontextualizedwithinthisliterature. botics[56]andautonomousvehicles[85].Inthesedomains,where
safetyandreliabilityareparamount,error-handlingmechanisms
playacriticalroleinensuringsmoothoperationandusertrust[7,
71].Strategiessuchasfaulttolerance,redundancy,andfail-safe
mechanismsarebeingexploredtominimizetheimpactoferrors
2.2 InteractionswithErrorsinAI-infused
andsafeguardagainstcatastrophicfailures[13,60,86].Moreover,
TechnologyinaBroaderContext
advancementsinsimulationandtestingmethodologiesenablere-
Whileerrorsareeasytotellinsomeapplicationswhereuserscan searcherstosystematicallyevaluaterobustnessanduserexperience
understandtheoutcomefromthesystemandgroundtrutheasily witherrorsinreal-worlddeploymentscenarios[2,3,69].Overall,
(e.g.,navigatingfamiliarrouteswithaway-findingsystem),the thequestforerror-resilientAI-infusedsystemsrepresentsamul-
outcomefromthesystemmaynotbeclearlyperceivedduetothe tifacetedandinterdisciplinaryendeavor,requiringcollaboration
characteristicsofthetask,apoorlydesignedinterface,thecom- acrossdomainstoachievethevisionofintelligent,trustworthy
plexityoftheinformation,orpoorconcentrationcausedbyahigh technology.
workload[45,46].Forexample,thegroundtruthmaynotbeavail-
ableimmediatelywhentheoutcomeisprovidedbythesystem(e.g.,
3 METHODS
medicaldiagnosis,weatherprediction).Thegroundtruthmaynot
bestraightforwardtotheuserifthesystemhandlesdatainanunfa- To gain insight into blind people’s challenges and strategies in
miliarworkdomain[76].Therefore,manyresearchershaveworked handlingerrorsinAI-infusedapplicationsforobjectrecognition,
ondevelopinguserinterfacesforAI-infusedsystemsaimedateffec- wecarryoutacomprehensivetwo-phaseuserstudy.Thestudyfirst
tivelymanagingerrorsandaidingusersinnavigatingdiscrepancies encompassesasemi-structuredinterviewthatcapturesparticipants’
betweensystemoutputsanddesiredoutcomes.Noteworthyefforts experiencewithcamera-basedassistivetools.Theinterviewisthen
includestrategiestotemperuserexpectationsregardingAIsystem followedbyanobjectrecognitiontask,whereparticipantsareasked
performance[44,58],alongsidethepresentationofuser-friendly toidentifyerrorswheninteractingwithamobileapplicationintheir
interfacestailoredtoaddresserrorsarisingfromdiverseAI-infused homes.Weadoptthistwo-prongedapproachfromapriorstudy
applications. byHongetal.[37]lookingatchallengesandstrategiesadoptedby
Gesture recognition technology has found utility in control- blindpeoplewhenreviewingautomaticspeechrecognitionerrors.
linganarrayofdevices,fromvisualdisplays[50]androbots[63] OurstudywasapprovedbytheInstitutionalReviewBoardatour
towearabledevices[66,83],andeveninvirtualrealityinterac- anonymizedinstitution(IRBnumberanonymized).Participantswere
tions[22,33,38].Despiteconsiderableadvancementsingesture compensatedata15$/hourrateforatotalof$26.21onaverage
recognitionaccuracyandusability,inputrecognitionerrorspersist, ($23−29,𝑆𝐷 =1.73).
significantlydetractingfromuserexperiences[49].Researchen-
deavorshavethusdelvedintocomprehendingtheramificationsof
3.1 Participants
theseerrors,uncoveringthatusertoleranceisfrequentlyshaped
morebythecontextofinteractionthansolelybysystemperfor- Werecruited12blindparticipants(6women,6men,0nonbinary)
mance.Remarkably,usersmaytoleraterecognitionerrorratesof fromcampusemaillistsandlocalorganizations.AsshowninTa-
upto40%beforeoptingforalternativeinteractionmodesoverges- ble2,theiragerangedfrom32to70(𝑀 =54.3,𝑆𝐷 =15.2).Three
tures[43].Moreover,endeavorstoalleviatethedetrimentalimpacts participantsreportedbeingtotallyblind,fivehavingsomelight
ofgesturerecognitionerrorshaveencompassedvariousstrategies, perception,andfourbeinglegallyblind.P1andP2reported“anau-
suchasreal-timeerrordetectionandadaptivemodeladjustments ditoryprocessingdisorder”anddifficultyhearing“veryhighsounds”,
basedondiscerningwhethertheerroneousinputsstemfromuser respectively.Yet,allparticipantsindicatedthattheyfacednoprob-
mistakesorrecognitionerrors[77]. lemsinusingascreenreader.Allmentionedusingsmartphones
Similarly,inthecontextofspeechrecognitionsystems,while severaltimesaday.Allparticipantswereright-handedexceptfor
significant progress has been made in minimizing errors under one,whowasleft-handed(P4).Whenaskedtoreporttheirlevelsof
controlledenvironments,practicalchallengessuchasspeakervari- familiaritywithmachinelearning,twoparticipantsreportedbeing
abilityandambientnoisepersist[29,40,65].Theseerrorsmanifest somewhatfamiliar,eightbeingslightlyfamiliar,andtwobeingnot
invariousforms,includingfailuretodetectspeech,misrecognition, familiaratall.Weuseda4-pointscaleforthisquestion,where
orincorrecthandlingofrecognizedspeech[37,68].Studieshave notfamiliaratallindicatedthatparticipantshaveneverheardof
revealedthatusersoverlookmorethanhalfofspeechrecognition machinelearning,slightlyfamiliarthattheyhaveheardofitbut
errorsintheabsenceofvisualcues[37].Toaddressthis,researchers don’tknowwhatitdoes,somewhatfamiliarthattheyhaveabroad
haveexploredtechniquesforautomatederrordetectioninspeech understandingofwhatitisandwhatitdoes,andextremelyfamil-
recognitionoutputs,rangingfromvisuallyhighlightingpotentially iarthattheyhaveextensiveknowledgeonmachinelearning.All
erroneouswordstoemployingneuralnetwork-basedpredictive questionsareavailableinAppendixA.ASSETS’24,October27–30,2024,St.John’s,NL,Canada JonggiHongandHernisaKacorri
ID Age Gender Levelofvision Onset FamiliaritywithML*
P1 39 Female Lightperception Birth Notfamiliaratall
P2 67 Male Legallyblind 55 Slightlyfamiliar
P3 62 Female Totallyblind Birth Somewhatfamiliar
P4 32 Male Legallyblind 20 Slightlyfamiliar
P5 66 Male Lightperception 46 Slightlyfamiliar
P6 61 Male Lightperception 41 Somewhatfamiliar
P7 70 Male Legallyblind Birth Slightlyfamiliar
P8 50 Female Legallyblind 45 Slightlyfamiliar
P9 69 Female Totallyblind 55 Notfamiliaratall Figure1:ObjectstimuliinourstudyfromKacorrietal.[42]:
P10 66 Female Lightperception Birth Slightlyfamiliar bakingsoda,caramelcoffee,Cheetos,chewybars,chicken
P11 33 Female Lightperception Birth Slightlyfamiliar
broth, coca-cola, diced tomatoes, diet coke, dill, Fritos,
P12 36 Male Totallyblind Birth Slightlyfamiliar
Lacroixapricot,Lacroixmango,Lays,oregano,roastcoffee.
*ML:Machinelearning
Table2:Participants’demographicsandbackground.
3.2 Procedure andmoreimportantlypartiallyincludedorout-of-frameobjectsof
interest[14,23,54].Thus,wedonotconductthissessioninourlab,
Thestudyisconductedovertwodaysthatmaybeupto7daysapart.
butmovethestudytothehomesofblindparticipants.AsinLeeet
Onthefirstday,participantsengageinasemi-structuredinterview
al.[52],allstudymaterialsaredeliveredathome,andinstructions
and answer questions related to demographics, and technology
areconveyedviaZoom.Eachparticipantreceivedalaptopwhere
experience.Onthesecondday,theycompletearecognitiontask
theZoomcallissetupforremotecommunication.Furthermore,
withanobjectrecognitionapplicationengineeredbyourteamthat
participantsareprovidedwithVuzixBladesmartglasses,featur-
aimstoserveasatestbed.TheappiscalledURCam.Duringthis
inganintegratedcameraandinitiatedwiththeZoomcall.The
session,participantsinteractwithURCamandasetofgivenobject
smart-glassescanbothenablereal-timeaccesstoparticipants’first-
stimuli.Theyattempttoidentifyanyrecognitionerrorsthatthe
personperspectivesandallowforrecordingsofobservationsfor
appmighthavemadeandexpresstheirconfidence.
subsequentdataanalysis.Atthebeginningofthetask,theexperi-
3.2.1 Semi-structuredinterview. Theinterviewlasted51minutes menterpresentsalistof15objectsforreference(Figure1).During
on average (18−90𝑚,𝑆𝐷 = 21.37). It was completed remotely eachtrial,participantsrandomlyselectanobject,captureitsim-
overZoomandrecordedforlateranalysis.Beyonddemographics, age,andobtainalabelfromtheobjectrecognitionapp,whichis
participantsrespondedtoquestionsabout: communicatedviasynthesizedspeech.Uponhearing“Don’tknow”
• frequencyofusingamobiledevice,takingphotos,reviewing fromtheapp,indicatingthatitfailedtorecognizeanyobjectin
photos,andchangingsettingsofthecamera; thephoto,participantsproceedtocaptureadditionalphotosuntil
• purposeoftakingphotos,subjectsincluded,applicationsand theappprovidesalabelforanobject.Subsequently,participants
devicesused,andconfidenceonphotocomposition; indicatewhethertherecognitionwasaccurateandexpresstheir
• frequencyofuseofacamera-basedassistiveapplication,its confidencelevelintheirjudgmentofcorrectnessfortherecogni-
usefulness,anddevice; tion.Aftercompletingtheinitial15trialswithallobjects(Attempt
• frequencyofverifyingtherecognitionresultsofacamera- 1),participantsrepeattheprocesswiththeobjectsinarandomized
basedassistiveapplication,encounteringerrors,importance order(Attempt2),totaling30trials.Participantsareencouraged
oferrors,anddifficultyofidentifyingtheerrors; tothinkaloudthroughoutthetask.Upontaskcompletion,partic-
• strategyoftakingphotoswithanassistiveapplication,de- ipantsprovidefeedbackonthedifficultylevelandthestrategies
greeofunderstandinghowthatapplicationworks. theyemployforidentifyingerrors.
AsshowninAppendixA,questionsassessingfrequencyarecat-
egorizedintotwogroups.Thefirstincludesthoseanswerablewith 3.3 ObjectStimuli
anabsolute7-pointscale,adoptedfromRosenetal.[72](ranging
Fortheerroridentificationtask,weutilizeafixedsetof15objects
from‘never’to‘severaltimesaday’).Forexample,‘Howoftendo
acrossallparticipants(Figure1).Weadoptsimilarstimulitothose
youtakephotosorrecordavideo?’Thesecondgroupincludesthose
previouslyemployedinastudyexaminingtheinteractionofblind
suitedtoarelative6-pointscale(from‘never’to‘always’)[20]e.g.,
userswithateachableobjectrecognizerbyKacorrietal.[42].We
‘HowoftendoyouencountermisrecognitionswhenyouuseSeeing
adopttheirmethodology,whichinvolvedtheselectionofobjects
AI?’
toencompassavarietyofshapes,sizes,materials,andvisualsimi-
3.2.2 Erroridentificationtask. Givenasetofobjectstimuliandan larities.Whilesomeproducts,suchasbakingsoda,chickenbroth,
iPhone8devicewithanobjectrecognitionapp,participantsare dicedtomatoes,anddietcoke,featuredlogosorimagesontheircon-
askedtotrytoidentifytheobjectsusingtheapplication.When tainersthatdifferedslightlyfromthoseusedinthepriorstudydue
deployed in real-world environments, object recognition errors todesignupdates,thefundamentalaspectsaffectingparticipants’
aretypicallyconfoundedbyblurredimages,viewpointswithlow tactileperception,suchasshape,material,andweight,remained
discriminativecharacteristics,clutteredbackgrounds,lowsaliency, consistentacrossallobjects.UnderstandingHowBlindUsersHandleObjectRecognitionErrors:StrategiesandChallenges ASSETS’24,October27–30,2024,St.John’s,NL,Canada
3.4 URCam:AnObjectRecognitionApp
Fortheerroridentificationtask,webuildanobjectrecognition
app,calledURCam,thatservesasatestbed;thesoftwareusedasa
basisforexperimentation.URCamisfine-tunedusingtheimages
ofobjectsinFigure1.Thebasemodeloftheobjectrecognizeris
InceptionV3[81],originallytrainedontheImageNetdataset[24].
Thedatasetforfine-tuningcomprisesphotoscapturedbynineblind
participantsinapreviousstudybyLeeetal.[53],wheretheytrained
ateachableobjectrecognizer.Theirdatasetincludes225imagesfor
eachobject,totaling3375images.Althoughotherexistingdatasets
(e.g.,[14])provideimagescollectedbyblindandlow-visionpeople,
theydidnotincludefine-grainedlabelsforthespecificobjectsin
ourstudy.Therefore,weoptedforthedatasetcollectedwithblind
participantsthatincludedthoseobjects.Fine-tuninginvolves500
iterationsofgradientdescentwithalearningrateof0.01.During
theidentificationtask,ourstudyparticipantsinteractwithURCam
onanAppleiPhone8.AsshowninFigure2,uponpressingthe
FScanitembutton,theapptransmitstheimagetoaservervia
HTTP,wherethefine-tunedobjectrecognitionmodelgenerates
predictionsregardingtheimage’slabel,subsequentlyrelayingit
backtotheparticipant’sdeviceviavoiceandvisualdisplay.
Todifferentiatebetweenobjectswithinourtrainingsetandthose
the app hasn’t encountered previously, we employ a technique
Figure2:AseriesofscreenshotsfromURCamthatwasde-
that assesses the model’s discriminative capacity by measuring
ployedinthestudy,whereparticipantsP1andP2experi-
theentropyofitsconfidencescores[88].Specifically,weestablish
encedcorrect,incorrect,anduncertainpredictionscommu-
athresholdforboththeentropyvalueandtheconfidencescore
nicatedviaa“Don’tknow”message.
todetermineinstanceswherethemodelshouldrefrainfrompro-
vidingapredictedlabelandinsteadoutput“Don’tknow”.Ifthe
entropyvalueexceeds2.0ortheconfidencescorefallsbelow0.4,
theapplicationsynthesizesthephrase“Don’tknow”ratherthan objectrecognitionresultsrecordedbytheapptoassesstheaccuracy
presentingapredictedlabel.Withthisprecautionarymeasure,the ofobjectrecognitionduringthetask.Wecategorizethetrialsbased
modelstrivestoabstainfromdeliveringpotentiallymisleadingor onhowwellparticipantsidentifyanyobjectrecognitionerrorsby
inaccuratepredictionswhenitlackssufficientconfidenceinits analyzingtheirresponsescapturedinthevideorecordings.During
discriminatoryabilities. thisanalysis,ifparticipantscannottellwhethertherecognition
wascorrectorincorrect,whichhappenedforatotalof7trials,
3.5 DataAnalysis weinterpretthisasthemperceivingthattherecognitioncanbe
incorrectbutbeingveryuncertainaboutit.Specifically,wegroup
Theresponsesfromthesemi-structuredinterviewandtasksare
thetrialsinto:
capturedviaZoom.Wetranscribetheseresponsestoenableacom-
prehensiveanalysisoftheparticipants’experienceandfeedback. Truepositive: Theobjectrecognitioniscorrectandthepar-
Wealsoexplorehowparticipantshandleapplicationuncertainty ticipantperceiveitascorrect.
(i.e.,“Don’tknow”)anddealwithpotentialmisrecognitionsduring Falsepositive: Theobjectrecognitionisincorrect,butthepar-
theerroridentificationtask. ticipantperceiveitascorrect.
Truenegative: Theobjectrecognitionisincorrectandthepar-
3.5.1 Semi-Structured Interview. We use a thematic coding ap-
ticipantperceiveitasincorrect.
proachtofindthemajorthemesintheparticipants’responses[17].
Falsenegative: Theobjectrecognitioniscorrect,butthepar-
Toreducethesubjectivity,tworesearcherscooperatetocodethe
ticipantperceiveitasincorrect.
responses.Oneoftheresearcherstranscribestheresponses.With
Weexaminethecorrelationbetweenparticipants’confidence
thetranscribeddata,thetworesearcherscodetheresponsesin-
levels,andtrialcompletiontime,alongthese4groups.Trialcom-
dependentlyandcreateinitialcodebooks.Theycomparethetwo
pletiontimeismanuallymeasuredthroughvideoanalysis,which
codebooks and code data to resolve the disagreements through
involvesrecordingtheelapsedtimefromwhentheappprovidedthe
consensus.Afterresolvingthedisagreements(atotalof35outof
recognitionresulttowhentheparticipantreportsitscorrectnessto
373answers),theyestablishasharedcodebookandcodethedata.
theexperimenter.Additionally,weinvestigateanyadjustmentsin
Inthefinalcodebook,theresponsesof17openquestionsinthe
participants’strategiesforcapturingphotoswhentheyreceivea
semi-structuredinterviewincludeatotalof153codes.
“Don’tknow”responsefromtheURCam.Wecategorizetheadjust-
3.5.2 ErrorIdentificationTask. Wemanuallyannotatetheimages mentsbylookingatvariationinbackground,viewpoint,illumination,
capturedbyparticipantsandcomparetheseannotationswiththe andobjectsize;acodingschemeadoptedbyHongetal.[36].ASSETS’24,October27–30,2024,St.John’s,NL,Canada JonggiHongandHernisaKacorri
4 INSIGHTSFROMTHEINTERVIEW
Thecentralthemesexploredduringtheinterviewencompassblind
people’sexperienceswithphotographyorvideorecordingandtheir
interactionwithcamera-basedassistiveapplications.Ourdiscus-
siondelveintovariousaspects,suchashowblindpeopleassessthe
qualityoftheirphotographs,themotivationsbehindtheirphotog-
raphy,andthemethodstheyemploytodiscerninaccuracieswithin
camera-basedassistiveapps.
4.1 CapturingandReviewingPhotos Figure3:Participants’experienceintakingphotos.
Bydelvingintoparticipants’experienceswithcapturingphotos
orvideos,ourgoalistouncoverthedegreeofintegrationofthese
aslegallyblind(𝑁 =4)predominantlyreliedontheirownvisual
technologiesintothedailyroutinesofblindpeople.Furthermore,
assessment.Theyutilizedautomaticallygeneratedimagedescrip-
throughanexplorationofthetechniquesparticipantsemployedto
tionsfromassistivetoolslikeSeeingAIandiOS’sbuilt-inimage
manipulatecamerasettings,weaimtouncoverinsightsintotheir
captioningfunction(𝑁 = 5).Forinstance,P12wouldjudgethe
approachforcapturingphotosthatwouldallowthemtoachieve
qualityoftheirphotobasedontextrecognitionresults,stating,
theirgoalbeitsharingthemwithothersorcompletingvisualtasks.
“What’srelevantaretheOCRresultsIgetfromit.Especiallyifthere
Wefindthatallparticipantsconsistentlyengageinphotography
isagarbledsectionthatdoesn’tfallintoanormalOCRerrorpattern,
activities,eachcapturingphotosatleastonceamonth,asdepicted
thenIknowthephoto’snotgood.”Apossiblereasonforindependent
inFigure3.Thisalignswithfindingsfromapreviousstudyindi-
reviewingbehaviorcouldbeconcernsaboutprivacyissueswhen
catingthatBLVpeopleactivelyusecamerasfordailytasks[39].
sharingtheirphotoswithsightedpeople[8,80,87,90].Few(3out
Oneoftheprimaryreasonsforusingacamerawastoshareimages
of12)participantssoughtassistancefromsightedindividualsin
orvideosviasocialmediaorvideocallsasshowninpriorstud-
theirvicinityandonlyone(P1)utilizedremoteassistancethrough
ies[39,78].Themajority(8outof12)reporttakingphotosorvideos
appslikeAira[6]andBeMyEyes[15].
morefrequentlythanseveraltimesaweek.Onereasonforusinga
Toprovidecontextforunderstandingthemotivationsbehind
camerawastosharephotosorengageinvideocalls.Forinstance,
participants’photography,weaskedquestionsaboutthesubjects
P4explained,“Videocalls,sharephotos,ItakevideosofbandsasI
they captured in their photos. Participants cited documents for
playsongs.I’vegotaYouTubechannelwithseveralhundredvideos
textrecognition(𝑁 = 10),people(𝑁 = 9),objects(𝑁 = 8),food
ofshowsI’vegoneto.” Additionally,usingassistivetechnologywas
(𝑁 = 6),landscapes(𝑁 = 5),andmiscellaneousitemssuchasa
citedasanotherreason,asdescribedbyP9:“SometimesI’llcheckto
sceneandabill(𝑁 = 4).Similarly,themostprevalentpurposes
seewhatSeeingAIwillsay.Justcurioustoknow,whattheappwill
fortakingphotosorrecordingvideoswerefortextrecognition
sayabout.I’veusedglasseswithanappAira.[...]ifyou’reinlike
(𝑁 =10),videocalls(𝑁 =8),andobjectrecognition(𝑁 =5).These
Walgreens,youcanconnectwithAiraandtheywilltellyouwhat’s
responsesdivergesomewhatfromthefindingsofapreviousstudy
ontheshelf.” Duringtheirphotographicendeavors,participants
conductedbyJayantetal.[39]in2011,whichsuggestedthatblind
tendtomaintainconsistentcamerasettingsandenvironmental
individualsprimarilytookphotostocapturefriendsorfamilyfor
conditions.Themajority(8outof12)ofparticipantsreportednever
leisure,whiletheirmostsought-aftercamerafunctionwastext
alteringtheircamerasettings.Amongtheparticipantswhodid
recognition.Thisresultindicatestheincreasingprevalence
makeadjustments(4outof12),modificationsprimarilyaimedto
ofcomputervision-basedassistiveapplicationsamongblind
optimizelightingconditions.Specifically,threeparticipantssought
andlow-visionpeople.However,manyparticipantsstillfound
outlocationswithamplenaturallight,whileoneexperimentedwith
imageframingchallenging(𝑁 =9),adifficultyhighlightedinprior
flashsettings.Forinstance,P7soughttoevadeshadows,stating,
studies[4,39,53].Forinstance,P1andP5expressedconcernssuch
“I’llstrategicallyrepositionthemtoensureoptimallightingwithout
as,“MakingsuretheinformationI’mtryingtocaptureisintheframe
anexcessofshadowsorothervisualdistractions.” Additionally,one
ofthecamera,”and“Idon’tknowhowfarawayfromtheobjecttohold
participant(P8)exploredvaryingcameraanglestoenhancetheir
thephone”,respectively.Participantsalsoidentifiedotherdifficulties
photographicoutcomes.P8expressedapreferenceforhomepho-
suchasmaintainingfocusontheobject(𝑁 = 2),stabilizingthe
tographyduetothefavorablelightingconditionsandexploring
camera(𝑁 =2),adjustinglightingconditions(𝑁 =2),andorienting
cameraangles,explaining,“whenI’mhome,Ifeelitgivesmethe
objectscorrectly(𝑁 =2).
maximumamountoflightandIgetthebestpictures.[...]Imight
moveitaroundacoupleoftimessothatit’lldescribeitinthemost
4.2 HandlingImageRecognitionErrors
detailedway.”
Wealsoposedquestionsregardinghowoftentheyreviewtheir Togaininsightsintotheexperiencesandpreferencesregarding
photos,asthispracticemayinfluencethequalityoftheirimages. camera-basedassistiveapplications,weconductedacomprehen-
Ingeneral,participantsdidnotfrequentlyreviewtheirphotos.The siveinquiryintotheappstheyregularlyutilize.Participantsre-
majority(8outof12)reportedcheckingtheirphotosseveraltimes ported using a total of eight camera-based assistive apps, with
amonthorless.Mostparticipantsreviewedtheirphotosin- inquiriesaimedatelucidatingtheirexperienceswitheach.Across
dependentlywithoutsightedhelp.Participantswhoidentified 20participant-apppairs,thepredominantchoicewasSeeingAI,UnderstandingHowBlindUsersHandleObjectRecognitionErrors:StrategiesandChallenges ASSETS’24,October27–30,2024,St.John’s,NL,Canada
Figure5:Participants’responsesaboutfrequencyofencoun-
terederrorsandverificationoftheoutputfromtheapps.
responsealignswithfindingsfrompriorstudiesindicatingthat
Figure4:Camera-basedassistiveappstheparticipantshave
blinduserstendtotrustcomputer-visionsystems[59].Somepartic-
usedregularly.
ipantsrefrainedfromvalidatingoutputsbecausetheyfounderrors
easytodetect(𝑁 =6).Particularlywithtextrecognitionapps,they
couldidentifyerrorsiftheoutputsdidnotmakesense.Forinstance,
asdepictedinFigure4.Additionally,participantsemployedother P11,whoneververifiedoutputsfromSeeingAIandVoiceDream
appsofferingtextandobjectrecognitionfunctionalities,includ- Scanner,stated“Ifittellsmeacertainthing,I’llknowthatitactually
ingGoogleLookout,KNFBReader,SuperLidar,Supersense,and meantcertainnumbers.Theerrorsthataresometimesmade,they
VoiceDreamScanner.AiraandBeMyEyeswerealsoutilizedfor kindofhavepatternsifyouknowwhatitis.” Thisresponsealigns
obtainingremotesightedassistance.Participantsvariedinthefre- withthefindingsofastudybyGuerreiroetal.[32],whichsuggests
quencyofappusage,withsomeemployingthemseveraltimesa thaterrorsareoftenacceptablewhenusersunderstandtheimper-
day(𝑁 =5),severaltimesaweek(𝑁 =7),severaltimesamonth fectionsofthetechnology.Whenrecognizingobjects,participants
(𝑁 =5),oronceamonth(𝑁 =3).Whenaskedaboutthefrequency comparedappoutputswiththeirexpectationsbasedonobjecttex-
ofencounteringmisrecognitions,responsesvaried,rangingfrom tures,shapes,andweights.Forinstance,P6,whonevervalidated
veryfrequently(𝑁 =2)andoccasionally(𝑁 =5)torarely(𝑁 =6), outputsfromSeeingAI,mentioned“[...]Icouldsaysometimesit
veryrarely(𝑁 =1),andnever(𝑁 =6),asshowninFigure5.How- doesgetthecannedsoupnamewrong,butIguessIdon’tconsiderit
ever,it’snoteworthythatparticipantsmightnothaveperceivedall wrongenoughtocallitwrong.” Someparticipantsverifiedoutputs
errors.Thus,thereportedfrequencyoferrorscouldbelowerthan occasionally(𝑁 = 5),rarely(𝑁 = 3),orveryrarely(𝑁 = 1).The
theactualfrequency. mostcommonreasonforverifyingresultswasuncertaintywith
We also inquired about participants’ strategies for capturing asingleoutput,promptingtheneedformultipletrialstomakea
“good” photos when using each camera-based assistive app. To decision(𝑁 =8).Forexample,P3explained,“ifI’mconsistentlynot
capturequalityphotos,participantsemployedstrategiessuchasad- gettingaresultwithSeeingAI,thenI’llseeifKNFBReaderwillgive
justingthedistanceandorientationofthecamera(𝑁 =9and𝑁 =7, meresults.”
respectively)andcenteringobjectsinthecameraframe(𝑁 = 7). Whilethefrequencyofencounteringerrorsvariedamong
Thisreflectstheperceivedchallengeofimageframingmentioned theparticipants,themajorityexpressedconcernaboutthe
earlier.Additionally,participantsutilizedcomputer-generatedfeed- misrecognitions.Wedelvedintotheimpactoferrorsincamera-
backforblindphotography(𝑁 =8),suchastheaudiotonesystem basedassistivetechnologyonusers’experienceswithit.Inmost
describedbyP12,auserofVoiceDreamScanner,whostated,“It cases,participantseitheragreed(𝑁 =13)orstronglyagreed(𝑁 =3)
hasthissystemwherethelouderandsteadiertheaudiotoneis,the thattheycaredaboutthemisrecognitionsfromtheapps,asdepicted
betteryouare.There’sacertaintone.You’vegottheperfectpicture inFigure6.Sometimes,however,theydidnotprioritizeerrorcor-
andyousnapit.” P1highlightedcomparablefeedbackfromSeeing rectionbecausetheycouldunderstandtheoutputsevenwithsome
AIfortakingaphotoofaperson,stating,“Ilistentotheprompts. errors.Forinstance,errorsintextrecognitiondidnotsignificantly
It’lltellmeifthefaceisatthebottomleftortopright.Orfaceisat alterthemeaningofthetexts,ortheappswerenotutilizedfor
center.WhenIhearthat.That’swhenIpushthebutton.” sensitiveorcriticaltasks.P8(SeeingAI)expressedthissentiment,
We delved deeper into how participants addressed potential stating,“It’snotthemostimportantthing,becauseI’mnotusingit
recognitionerrorstheymayhaveexperiencedwiththeseapps.We forsomethingcritical.” Whenaskedifthereweresituationswhere
queriedparticipantsaboutthefrequencywithwhichtheyvalidated theycaredmoreabouterrors,participantsoftencitedtextrecogni-
predictionsfromtheapps(Figure5).Inthemajorityofcases,par- tionscenariosinvolvingimportantcontentsuchasbills,currency,
ticipantsreportedneververifyingoutputswhileusingtheapps expirationdates,orothercrucialnumbers(𝑁 =11).Forinstance,
(𝑁 =9).Manyofthemexpressedtrustintheapp’soutputswithout P1,aBeMyEyesuser,explained,“iftheydon’tseetheexpiration
validation(𝑁 =7),exemplifiedbystatementssuchas“ifitsaysit’s dateproperlyonsomethingandit’sexpired,youknow,Icouldget
a$5bill,Ibelieveit” (P2,SeeingAI),“Iassumeit’scorrectwhenit sick.” Othercriticalsituationsincludedreadingdirectionsfortasks
readsittome” (P6,SeeingAI),and“(Irarelyverifytherecognition (𝑁 = 5)andreviewingimportantdocuments(𝑁 = 5).P9(Voice
results)becauseit’sprettyaccurate.” (P12,GoogleLookout).This DreamScanner)providedexamplesofsuchdocuments,stating,ASSETS’24,October27–30,2024,St.John’s,NL,Canada JonggiHongandHernisaKacorri
Figure6:Participants’responsesabouthandlingerrors.
‘‘probablywhenit’ssomethingthatisconnectedtolegaldocuments,
financialstatements,legalfinancialstatements.” Responsesregard-
ingthedifficultyofidentifyingmisrecognitionsvaried.Inhalf
ofallcases(𝑁 =10),participantsdisagreedorstronglydis-
agreedthatidentifyingerrorswaschallengingwhenthey
couldeasilydetectthemusingcontextualcuessuchassur-
roundingtextorobjecttextures.Forexample,P1(BeMyEyes)
remarked,“ifthey’rewrong,Iknowthey’rewrong.Soit’snotreally
achallengetoidentifythatit’samisrecognitionforme.” P12(Seeing
AI)similarlycommented,“Icancatchtheerrorsastheycomeup
Figure7:Likertchart(top)andmosaicplot(bottom)ofcer-
becauseoften,it’snotwrongenoughformetonotbeabletofigureout
whatitsays.”Conversely,inothercases,participants(𝑁 =9)found taintylevelsandtrialcategories.Thesizeoftherectangles
inthemosaicplotcorrespondstothenumberoftrials.
errorslessdistinguishableandchallengingtoidentify.P8(Seeing
AI), acknowledging the possibility of missing errors, expressed,
“Ifit’swrong,Iwouldn’tknow.[...]Idon’tevenknowwhetherit’s
Table3:Participants’strategiestoovercome“Don’tknow”.
wrongortrue.” Additionally,P9recountedinstanceswheresighted
individualsdetectederrorsfromVoiceDreamScannerthatshehad
Code Strategy Cases
missed,stating,“TherehavebeenoccasionswhenIdidn’tdetectany-
Objectsize Adjustthecameradistanceforbetterframing 11
thingandasightedpersonmayhaveindicatedtherewassomething
Rotatetheobjecttoshowdifferentsides 119
thatIjustdidnotget.” Whenaskedhowtheyidentifiederrors,the
Background Movetheobjecttoanotherplace 23
majority(𝑁 = 10)ofparticipantsreliedoncontextualcues.For
Hidethebackgroundobjectswithapaper 1
example,P1,usingSeeingAIfortextrecognition,mentioned,“If
Movethecameratodisplayothersidesoftheobject 29
theinformationreadingisn’tveryclear,ifIcantellthatit’sonly Changethewayofholdingtheobject 17
Viewpoint
readingapartofsomethingthenIhavetoreadjustit.” Similarly,P6, Rotatetheobjecttochangeperspective 5
identifyingobjectswithSeeingAI,explained,“ifIgetasoup,andit’s Rotatethecamera(portraitandlandscape) 1
notpronouncingthetypeofsoup,thattypeofthing.” Thisbehavior Nochange 10
contrastswiththestrategiesofblindusersinhandlingerrorsin
navigationsystems,wherethemajoritysoughtsightedassistance
when they encountered errors [32]. In other cases, participants
Participantsencountered7.33incorrectrecognitionsonaverage
soughtclarificationfromsightedindividuals(𝑁 = 5)orverified (𝑆𝐷 =2.99),experiencingmorefalsepositives(𝑀 =3.67,𝑆𝐷 =2.46)
appoutputsthroughmultipletrials(𝑁 =5). than false negatives (𝑀 = 0.83, 𝑆𝐷 = 1.03). When looking at
whether participants could distinguish between correct and in-
5 ERRORIDENTIFICATIONRESULTS
correctrecognitions,wefindthatonaverage,participantssuccess-
Weconductedacomprehensiveevaluationofparticipants’experi- fullyidentified21.83(𝑆𝐷 =2.82)correct(truepositives)and3.17
encewithidentifyingerrorswithinthecontextofobjectrecognition. (𝑆𝐷 =2.44)incorrect(truenegatives)recognitionresults.However,
Ouranalysiscenteredondiscerningpatternsinparticipants’error- participantsidentifiederrorsataproportionof0.49onaverage
handlingbehaviorthroughoutthetask.Additionally,weexamined (𝑆𝐷 =0.32),indicatingthattheycoulddetectlessthanhalfof
theinfluenceofrepeatedobjectrecognitioneffortsonerrorhan- theerrors.Thisoutcomeisconsistentwiththeover-relianceon
dlingbycomparingthetwoattempts.Furthermore,participants’ imagerecognitionresultsobservedinapreviousstudybyMacLeod
feedback provided valuable insights into their attitudes toward etal.[59].Thelowrateoferroridentificationcanbeattributedto
errorsencounteredinobjectrecognition. thechallengeofdistinguishingobjectswithinthesamecategory
thatsharesimilarshapes,textures,andweights(e.g.,coca-colaand
5.1 IdentifyingObjectRecognitionErrors
dietcoke)whenlimitedvisualinformationisavailable.
Acrossthe30trials(15inthefirstand15inthesecondattempt),the Whenlookingatparticipants’strategiesforrecoveryfrom“Don’t
averageaccuracyofobjectrecognitionstoodat0.76(𝑆𝐷 =0.10). know” predictions,wefindthattheyclusteraroundvaryingobjectUnderstandingHowBlindUsersHandleObjectRecognitionErrors:StrategiesandChallenges ASSETS’24,October27–30,2024,St.John’s,NL,Canada
size,background,andviewpoint(showninTable3).Thisisexcit- thattheproportionoferrorsidentifiedbytheparticipants
ingasnoneoftheparticipantsreportedhavingmachinelearning wasnotsignificantly1differentacrossthetwowithitbeingat
expertise.Yet,thesepatternsunderscoreparticipants’awareness 0.51onaverage(𝑆𝐷 =0.40)forthefirstand0.46(𝑆𝐷 =0.36)forthe
ofthepotentialimpactofobject’ssize,viewpointandback- secondattempt.Regardingthelevelofcertaintyinthecorrectness
groundontheperformanceoftheobjectrecognitionmodel, oftherecognitions,inthesecondattempt,participantswerecer-
drawingfromparallelstohowhumansrecognizeobjectsindepen- tainorverycertainforasmallerproportionoftrialsacrossall
dentofsize,viewpoint,location,andillumination[67].Onaverage, fourcategoriescomparedtothefirstattempt,asshowninFigure8.
theobjectrecognitionappprovideda“Don’tknow” responsein8.2 Oneofthereasonsforthisdifferencewasinconsistentrecognition
trials(𝑆𝐷 =4.17)outof30trials,totaling216cases;a“Don’tknow” resultswiththesameobjectacrossthefirstandsecondattempts,
responsewouldoftenbefollowedbysubsequenta“Don’tknow” supportedbyP9’sresponse:“thesecondtimearound,theygaveme
responseswithanaverageof2.01(𝑆𝐷 =0.89)occurrences.Asde- differentinformation.SothenIbecameuncertainabouttrustingwhat
tailedinTable3,whenparticipantsencountered“Don’tknow,” the itwastellingme.”
mostprevalent(116cases)approachtocircumventitwasrotating Furthermore,inthesecondattempt,thetrialcompletiontime
theobjecttodisplayitsotherside,therebyvaryingtheviewpoint significantly2decreasedto4.22seconds(𝑆𝐷 =2.33),comparedto
intheimage,astrategyalsoprevalentamongsightednon-experts thefirst,wherewerecordedalongerdurationof6.75seconds(𝑆𝐷 =
inpriorwork[36].Thesecondmostcommonapproach(29cases) 3.15).Thisdiscrepancysuggestsameaningfulvariationbetweenthe
alsoinvolvedadjustingtheviewpoint,withparticipantsmovingthe attempts.Possibleexplanationsforthisobserveddifferencecould
camerainsteadoftheobject.Additionally,participantsoccasionally beattributedtoparticipants’increasedfamiliaritywiththetask
(23cases)alteredthebackgroundoftheimagebyrelocatingthe procedureinthesecondattempt,aswellasquickerdecision-making
objecttodifferentpositions. basedonpriorexperiencewiththetaskinthefirstattempt.
Weexamineparticipantscertaintyaroundtheerroridentifica-
tiontaskbylookingattheirresponsesforeachtrialwherethey 5.3 SubjectiveFeedback
indicatetheirconfidenceintheirjudgmentofthemodelprediction. Whileparticipantsmissedaroundhalfoftheerrors,theygener-
Overall,participantsexpressedvaryinglevelsofcertainty,report- allyperceivedidentifyingerrorsasnotchallenging,confirming
ingbeing“verycertain,”“certain,”“uncertain,”and“veryuncertain” thefindingfromapriorstudythatBLVusershavemixedfeelings
across17.67(𝑆𝐷 =5.71),7.83(𝑆𝐷 =5.77),2.25(𝑆𝐷 =2.83),and1.75
withbothconfidenceandconcernsregardingidentifyingerrorsin
(𝑆𝐷 =2.01)trials,respectively.AsshowninFigure7,wefindthat
privateobjectdetection[89].Whenaskedaboutthedifficulty,the
participantsreportedbeingeithercertainorverycertainin90%of majoritydisagreed(𝑁 =5),withsomestronglydisagreeing(𝑁 =3).
truepositivetrials;trialswheretheobjectrecognitioniscorrect Forinstance,P8,whohaslowvision,wasabletodiscerncorrect
andtheparticipantperceiveitassuch.Thisseemspromising.Yet, andincorrectpredictionsbasedontheirvisionandthetextures
theyalsoreportedbeingeithercertainorverycertainin84%of oftheobject.Otherparticipantsidentifiederrorsbycomparing
falsepositivetrials;trialswheretheobjectrecognitionisincorrect
but theparticipant perceiveit as such. Incontrast, participants
1Wedidnotobserveastatisticallysignificantdifferenceintheresultsofrepeatedmea-
suresAnalysisofVariance(ANOVA)withAlignedRankTransform(ART)regarding
reportedbeingeithercertainorverycertainin76%oftruenegative thenumberoferrorsandtheproportionoferrors(𝑝>.05).
trials,and40%offalsenegativetrials.Thistrendunderscoresa 2TheresultsoftherepeatedmeasuresANOVAwithARTexhibitedastatistically
tendencyforheightenedcertaintywhenparticipantsper-
significantdifference(𝐹 1,9=9.67,𝑝=.013,𝜂2=0.52).
ceivedrecognitionoutcomesascorrect.Overall,thesefindings
indicateaprevalentinclinationamongparticipantstoplacetrust
inthepredictionsfromtheobjectrecognizer.
Throughanalysisoftrialcompletiontime,weobservethatpar-
ticipantstendtomakequickerdecisionsregardingthecor-
rectnessofpredictionswhentheywereverycertain(𝑀 =
3.91𝑠,𝑆𝐷 = 2.70)comparedtowhentheywerejustcertain(𝑀 =
8.48𝑠,𝑆𝐷 = 5.28),uncertain(𝑀 = 7.87𝑠,𝑆𝐷 = 2.71),orveryun-
certain(𝑀 =7.69𝑠,𝑆𝐷 =8.30).Asmallcorrelationwasobserved
betweenthelevelofcertaintyandthetrialcompletiontime,as
indicatedbythePearsonCorrelationCoefficient(𝑟 =0.27).
5.2 IdentifyingErrorsaSecondTime
Inareal-worldscenario,participantstendtointeractwitharecog-
nitionapplicationandsimilarobjectsoveralongperiodandoften
learntoanticipatefailures.InSection5.1,wepresentaggregated
observationsfrombothattempts.Tounderstandevenatasmall
scaletheeffectofrepeateduseoftheobjectrecognitionapplication
Figure8:Percentagesofthecertaintylevelsacrossthecate-
onhandlingincorrectrecognitions,inthissectionwecompared
goriesofthetrialsinthefirstandsecondattempts.
thetwoattemptsintheerroridentificationtask.Overall,wefindASSETS’24,October27–30,2024,St.John’s,NL,Canada JonggiHongandHernisaKacorri
predictionsacrossmultipletrials.Forexample,P10explained,“I personalizedobjectrecognitionsystems[35]andreal-timefeed-
didn’trecognizeamistakeuntilthesecondsimilarobjectappeared. backforblinduserstocapturehigh-qualityphotos[4,61],there
SolikethetwocansoftheLacroixapricotandLacroixmango,one remainsaneedforfurtherinvestigation.Specifically,itisessen-
ofthemwasincorrectbecauseitwastellingmeapricotbothtimes.” tialtoevaluatetheeffectivenessofthesedescriptorsinidentifying
Errorsweresometimesevidenttoparticipantsbecausepredicted errorsandprovidingactionableinsightstousers.
andtrueobjectshaddistincttextures,shapes,orweights,asnoted Incorporatethecontextandrecognitionsystemtypein
byP12:“[...]forexample,thedicedtomatoesversusthechickenbroth, designingintuitiveuserinterfaces.Inourinterviews,partici-
chickenbrothismoreliquid.Itwaseasytoidentifythatitwaswrong.” pantsdelineateddiverseapproachestopinpointingerrorsinboth
Ontheotherhand,threeparticipantsstronglyagreedthatidenti- objectandtextrecognition.Whenutilizingtextrecognition,they
fyingerrorswaschallenging.Amongthem,twomentionedthat reliedheavilyoncontextualcues,aserrorsoftenmanifestedasde-
therecognitionresultswereinconsistentwithanobject,making viationsfromthesurroundingtext’slogicalflow.Incontrast,with
itdifficulttodeterminetheircorrectness.P9said“Twothingsthat object recognition, participants leveraged intrinsic object prop-
seemsimilar,butthefirsttimetheysaidtheywerethesame,andthen ertiessuchasweightandtexturetogaugerecognitionaccuracy.
thenexttimeputtingthemback,theysaidsomethingdifferentonone Additionally,certainapplicationswithvisionlanguagemodelslike
ofthem.SonowI’mnotsure.SoIstronglyagree,itwasdifficultfor BeMyAI[5]furnishdetailedimagedescriptions,enrichinguser
metotellusitwasinerror.” Anotherparticipantmentionedthatit experience. However, this potentially introduces more complex
waschallengingtorememberallobjectsexplainedatthebeginning challengesinerrordetectionduetothelongerandmoredescrip-
ofthestudy,whichcomplicatedthedecision-makingregardingthe tive texts [11], compared to the simple object labels in URCam.
correctnessofrecognitionresults. Whileourstudyprimarilydelvedintoobjectrecognition,partici-
pantfeedbackunderscoresthepivotalroleofrecognitionsystem
typeandcontextualunderstandingincraftinguserinterfacesfor
6 DISCUSSION
errordetectionincamera-basedassistivetechnologies.
Ouruserstudy,exploratoryinnature,showsbothpromisingresults Consequently,ourfindingsoffervaluableinsightsfordesigning
andfutureresearchdirectionsforsupportingblindusers’interac- intuitiveinterfacestailoredtoobjectrecognitionerroridentifica-
tionswitherror-proneAI-infusedtechnologies.Inthissectionwe tionwithimages.Forinstance,ourfindingssuggestthatproviding
discusslessonslearnedandlimitationsthatmayaffectthegeneral- descriptiveinformationaboutdifferentfacetsofanobjectinan
izabilityofourfindings. imagecouldmitigateerroroccurrences,asevidencedbypartici-
pantsfrequentlyresortingtorotatingobjectstoavoid"Don’tknow"
responsefromtheURCamappduringtheerroridentificationtasks.
6.1 Implications
Ontheotherhand,real-timecamera-basedassistivetechnologies
Enableuserstoleveragetheirexpertiseinreviewingerrors maypresentuniquechallenges.Weexpectthatblinduserswould
independently.Thefindingsfromtheinterviewhaveshedlight employdifferentstrategiesforavoidingandvalidatingerrorssince
onaninterestingtrend:mostparticipantsexpressedapreference theycanobservetheeffectsoftheircameraframingimmediately.
forevaluatingthequalityoftheirphotographswithouttheassis- Thisimmediatefeedbackloopcouldencourageadaptivebehaviors,
tanceofsightedindividualsorremotesightedaidservices,suchas suchasrepositioningthecameraoralteringtheangleofcapture
BeMyEyesorAira,whenusingcamera-basedassistivetechnolo- toensurebetterrecognitionaccuracy.Futureresearchshouldex-
gies.Thispreferenceseemstostemfromafundamentalaspectof ploretheseadaptivestrategiesindepth,examininghowreal-time
theutilizationofAI-basedsystems–namely,thedesiretocarry feedbackinfluencesuserinteractionpatternsanderrormitigation
outvisualtasksindependentlywhensightedassistanceisunavail- techniques.
able.Itfurthershowsthepreferenceofblindandlow-visionusers Enableuserstounderstandtheperformanceoftheobject
toutilizetheirexpertiseinassistivetechnology,suchasintegrat- recognizer.Whileparticipantsmissedapproximatelyhalfofthe
ingrecognitionresultsfrommultipleAIappstoidentifyerrors. errors,theircollectiveperceptionoferroridentificationasnon-
Thispersonalizedapproachtousingassistivetechnologywashigh- challengingwasnotable.Whenprobedaboutthedifficultylevel,
lightedinapreviousstudy[34].Thisobservationunderscoresa themajorityofparticipantsdisagreed,withsomeexpressingstrong
crucialneedwithintheblindcommunity:theabilityforindividu- disagreement.Thisobservationunderscoresthenuanceddifficulty
alstoautonomouslyassessthequalityoftheirphotos,takinginto inherentincomprehendingboththeoverallperformancemetrics
accountfactorssuchasframing,backgroundclutter,andblurriness. oftheobjectrecognizer(i.e.,errorrate)andpinpointingindividual
Addressingthischallengewilllikelyrequireinnovativeapproaches, errors,achallengecompoundedforblindandlow-visionusers.This
particularlyintherealmofcomputervision.Developingtechniques corroboratesfindingsfrompriorstudiesthatshowedtheblindand
thatcanaccuratelyquantifythequalityfactorsofphotographswith- low-visionusers’tendencytoexhibitanovertrustontheoutputAI-
outrelyingonvisualcuesaccessibleonlytosightedindividuals basedassistivetechnologiessuchasimagerecognitionsystems[59]
holdsgreatpromiseinthisregard.Suchtechniquescouldpoten- andautomaticspeechrecognition[37].
tiallyleverageadvancedalgorithmsandmachinelearningmodels WhilestudieswithinthedomainofExplainableAIhavedemon-
toanalyzevariousaspectsofaphotograph,fromcompositionto stratedthepotentialefficacyofelucidatingthecertaintyandratio-
sharpness,andprovidemeaningfulfeedbacktoblindandlow-vision nalebehindmachinelearningmodeloutputsinenhancingperfor-
users.Whilesomeinitialstrideshavebeenmadeinthisarea,such manceunderstandingandusability[79,84],manyofthesestudies
asimagedescriptorsforblinduserstoassessphotosfortraining rely on visual information such as heatmap [41] and plots [70]UnderstandingHowBlindUsersHandleObjectRecognitionErrors:StrategiesandChallenges ASSETS’24,October27–30,2024,St.John’s,NL,Canada
inaccessibletoblindusersorhavenotbeenassessedwithblindin- photo-takingforthepurposeofutilizingcamera-basedassistive
dividuals.Consequently,tofacilitateerroridentificationeffectively, systems,ratherthansolelyforcapturingmemoriesorsharingwith
forthcomingresearchendeavorsmustprioritizethedevelopment others.Additionally,participantsrevealedtheirinclinationtowards
ofmethodologiesthatenableblindandlow-visionuserstoassess independentlyreviewingphotoqualityandidentifyingerrors,de-
theperformanceofobjectrecognitionsystems. spiteacknowledgingthechallengingnatureofthesetasksforap-
proximatelyhalfoftheparticipants.Furthermore,ourempirical
6.2 Limitations investigationthrougherroridentificationtasksprovidedvaluable
insightsintothechallengesassociatedwithidentifyingobjectrecog-
Variabilitybetweenconfinedstudyconditionsandreal-world
nitionerrors.Theresultsindicatedthatparticipantssuccessfully
experience.Anotablelimitationofourstudyliesinthepotential
identifiedonlyaround50%oftheerrors,predominantlyemploying
disparitybetweenerroridentificationunderconfinedconditions
viewpoint,background,andobjectsizealterationswithinimages
andreal-worldusagescenarios.Intheuserstudysetting,partici-
to mitigate errors. Additionally, we observed that the certainty
pantswereconfinedtoaspecificstudysetupintheirhomeenvi-
regardingrecognitioncorrectnesscouldbeadverselyaffectedbyin-
ronmentwithlimitedvariablessuchaslighting,background,and
consistentrecognitionoutcomesinsubsequentinteractions.These
framing.Typically,theywouldplacethestudymaterialsonatable
findingssignificantlycontributetoourunderstandingandquan-
andsitnearby.Inaway,theywererestrictedintheirabilitytomove
tificationofthechallengesinidentifyingobjectrecognitionerrors
aroundfreelytofindoptimalpositionsforcapturingphotos,which
withinassistivetechnologies.
couldinfluencethequalityoftheimageandsubsequentlyimpacter-
roridentification.Furthermore,asomewhat‘staged’indoorsetting
maynotfullyreplicatethediverseconditionsencounteredinreal- ACKNOWLEDGMENTS
worldscenarios,suchasvaryinglightingconditions,backgrounds,
WethankKyungjunLee,EbrimaJarjue,andErnestEssuahMensah,
andthepresenceofoutdoorelements.Participants’leveloffamil-
whowerestudentsattheUniversityofMarylandatthetimeofthe
iarityandexperiencewiththeapplicationmayalsodifferbetweena
datacollectionandcontributedtotheremotestudyprotocol.Jonggi
one-offandreal-worldusagecontexts.Whileparticipantsreceived
HonginitiatedthisworkattheUniversityofMaryland,College
guidanceandinstructionsduringtheuserstudy,theirexperience
Park.ThismaterialisbaseduponworksupportedbytheNational
inusingtheapplicationinreal-worldsettingsmayvary,potentially
ScienceFoundationunderGrantNo.1816380.HernisaKacorriwas
affectingtheirproficiencyinerroridentification.Last,thenumber
additionallysupportedbytheNationalInstituteonDisability,Inde-
andtypesofobjectsencounteredinreal-worldscenariosmaydiffer
pendentLiving,andRehabilitationResearch(NIDILRR),ACL,HHS
fromthestimuliinthestudy.Real-worldscenariosofteninvolvea
underGrantNo.90REGE0008and90REGE0024.
widervarietyofobjectsandcontexts,presentinguniquechallenges
forerroridentification.
Single-sessionlimitationandpotentiallongitudinalvari-
REFERENCES
ability.Aninherentlimitationofourstudyisthattheerroridenti-
[1] AliAbdolrahmani,WilliamEasley,MicheleWilliams,StacyBranham,andAmy
ficationtaskwasconductedwithinasinglesessionatparticipants’
Hurst.2017. Embracingerrors:Examininghowcontextofuseimpactsblind
homes.Whilethisapproachallowedustogathervaluabledata individuals’acceptanceofnavigationaiderrors.InProceedingsofthe2017CHI
inanaturalisticsetting,itmaynotfullycapturetheevolutionof ConferenceonHumanFactorsinComputingSystems.4158–4169.
[2] AfsoonAfzal,DeborahSKatz,ClaireLeGoues,andChristopherSTimperley.
participants’erroridentificationabilitiesovertime.Indeed,ourob-
2020.Astudyonthechallengesofusingroboticssimulatorsfortesting.arXiv
servationsrevealeddifferencesbetweenparticipants’performance preprintarXiv:2004.07368(2020).
inthefirstandsecondattemptsoftheerroridentificationtask.This [3] AfsoonAfzal,DeborahSKatz,ClaireLeGoues,andChristopherSTimperley.
2021.Simulationforroboticstestautomation:Developerperspectives.In2021
discrepancysuggeststhatparticipants’understandingoftheobject 14thIEEEconferenceonsoftwaretesting,verificationandvalidation(ICST).IEEE,
recognizer’sperformance,thecharacteristicsofobjects,andoptimal 263–274.
[4] DraganAhmetovic,DaisukeSato,UranOh,TatsuyaIshihara,KrisKitani,and
photo-takingtechniquesmayhaveimprovedwithrepeatedexpo-
ChiekoAsakawa.2020.Recog:Supportingblindpeopleinrecognizingpersonal
sureandexperience.Consequently,alongitudinalstudyspanning objects.InProceedingsofthe2020CHIConferenceonHumanFactorsinComputing
multiplesessionscouldprovidedeeperinsightsintohowpartici- Systems.1–12.
[5] BeMyAI.2024. Introducing:BeMyAI. https://www.bemyeyes.com/blog/
pants’erroridentificationabilitiesevolveovertime.
introducing-be-my-ai
Therefore, while our study provides valuable initial insights [6] Aira.2024.YourLife,YourSchedule,RightNow. https://aira.io
intoerroridentificationinasingle-sessioncontext,futureresearch [7] IghoyotaBenAjenaghughrure,SoniaClaudiadaCostaSousa,andDavidLamas.
2020. RiskandTrustinartificialintelligencetechnologies:Acasestudyof
employinglongitudinalmethodologiescouldofferamorecompre- AutonomousVehicles.In202013thInternationalConferenceonHumanSystem
hensiveunderstandingofthedevelopmentandrefinementoferror Interaction(HSI).IEEE,118–123.
[8] TaslimaAkter,BryanDosono,TousifAhmed,ApuKapadia,andBryanSemaan.
identificationexperienceandexpertisethatblindusersbuildwhile
2020."IamuncomfortablesharingwhatIcan’tsee":PrivacyConcernsofthe
interactingwiththeircamera-basedassistivetechnologies. VisuallyImpairedwithCameraBasedAssistiveApplications.In29thUSENIX
SecuritySymposium(USENIXSecurity20).1929–1948.
[9] MichaelAAlcorn,QiLi,ZhitaoGong,ChengfeiWang,LongMai,Wei-ShinnKu,
7 CONCLUSION andAnhNguyen.2019.Strike(with)apose:Neuralnetworksareeasilyfooled
bystrangeposesoffamiliarobjects.InProceedingsoftheIEEE/CVFConferenceon
Weexploredtheexperiencesofblindandlow-visionpeoplere- ComputerVisionandPatternRecognition.4845–4854.
gardingphoto-taking,usageofcamera-basedassistivesystems,and [10] RahafAlharbi,RobinNBrewer,andSaritaSchoenebeck.2022.Understanding
emergingobfuscationtechnologiesinvisualdescriptionservicesforblindand
erroridentificationwithinthesesystems.Throughsemi-structured
lowvisionpeople. ProceedingsoftheACMonHuman-ComputerInteraction6,
interviews,weuncoveredthatparticipantspredominantlyutilize CSCW2(2022),1–33.ASSETS’24,October27–30,2024,St.John’s,NL,Canada JonggiHongandHernisaKacorri
[11] AkhterAlAmin,SaadHassan,MattHuenerfauth,andCeciliaOvesdotterAlm. AdvancedComputationalIntelligenceTechniquesforVirtualRealityinHealthcare
2023. ModelingWordImportanceinConversationalTranscripts:Towardim- (2020),85–105.
provedlivecaptioningforDeafandhardofhearingviewers.InProceedingsof [34] JaylinHerskovitz,AndiXu,RahafAlharbi,andAnhongGuo.2023. Hacking,
the20thInternationalWebforAllConference.79–83. switching,combining:understandingandsupportingDIYassistivetechnology
[12] AlexanderAndreopoulosandJohnKTsotsos.2013.50yearsofobjectrecognition: designbyblindpeople.InProceedingsofthe2023CHIConferenceonHuman
Directionsforward.Computervisionandimageunderstanding117,8(2013),827– FactorsinComputingSystems.1–17.
891. [35] JonggiHong,JainaGandhi,ErnestEssuahMensah,FarnazZamiriZeraati,Ebrima
[13] JyotikaAthavale,AndreaBaldovin,RalfGraefe,MichaelPaulitsch,andRafael Jarjue,KyungjunLee,andHernisaKacorri.2022.BlindUsersAccessingTheir
Rosales.2020. AIandreliabilitytrendsinsafety-criticalautonomoussystems TrainingImagesinTeachableObjectRecognizers.InProceedingsofthe24th
ongroundandair.In202050thAnnualIEEE/IFIPInternationalConferenceon InternationalACMSIGACCESSConferenceonComputersandAccessibility(Athens,
DependableSystemsandNetworksWorkshops(DSN-W).IEEE,74–77. Greece)(ASSETS’22).AssociationforComputingMachinery,NewYork,NY,USA,
[14] RezaAkbarianBafghiandDannaGurari.2023.Anewdatasetbasedonimages Article14,18pages. https://doi.org/10.1145/3517428.3544824
takenbyblindpeoplefortestingtherobustnessofimageclassificationmodels [36] JonggiHong,KyungjunLee,JuneXu,andHernisaKacorri.2020.Crowdsourcing
trainedforimagenetcategories.InProceedingsoftheIEEE/CVFConferenceon thePerceptionofMachineTeaching.InProceedingsofthe2020CHIConference
ComputerVisionandPatternRecognition.16261–16270. onHumanFactorsinComputingSystems.1–14.
[15] BeMyEyes.2024.Lendyoueyestotheblind. http://www.bemyeyes.org/ [37] JonggiHong,ChristineVaing,HernisaKacorri,andLeahFindlater.2020. Re-
[16] LarwanBerke,ChristopherCaulfield,andMattHuenerfauth.2017. Deafand viewing Speech Input with Audio: Differences between Blind and Sighted
hard-of-hearingperspectivesonimperfectautomaticspeechrecognitionfor Users. ACMTrans.Access.Comput.13,1,Article2(April2020),28pages.
captioningone-on-onemeetings.InProceedingsofthe19thInternationalACM https://doi.org/10.1145/3382039
SIGACCESSConferenceonComputersandAccessibility.155–164. [38] Yi-JhengHuang,Kang-YiLiu,Suiang-ShyanLee,andI-ChengYeh.2021.Evalua-
[17] VirginiaBraunandVictoriaClarke.2006.Usingthematicanalysisinpsychology. tionofahybridofhandgestureandcontrollerinputsinvirtualreality.Interna-
Qualitativeresearchinpsychology3,2(2006),77–101. tionalJournalofHuman–ComputerInteraction37,2(2021),169–180.
[18] RobinNBrewerandVaishnavKameswaran.2018. Understandingthepower [39] ChandrikaJayant,HanjieJi,SamuelWhite,andJeffreyPBigham.2011. Sup-
ofcontrolinautonomousvehiclesforpeoplewithvisionimpairment.InPro- portingblindphotography.InTheproceedingsofthe13thinternationalACM
ceedingsofthe20thInternationalACMSIGACCESSConferenceonComputersand SIGACCESSconferenceonComputersandaccessibility.203–210.
Accessibility.185–197. [40] HuiJiang.2005.Confidencemeasuresforspeechrecognition:Asurvey.Speech
[19] JulianBrinkley,BriannaPosadas,JuliaWoodward,andJuanEGilbert.2017. communication45,4(2005),455–470.
Opinionsandpreferencesofblindandlowvisionconsumersregardingself- [41] WeinaJin,XiaoxiaoLi,MostafaFatehi,andGhassanHamarneh.2023.Guidelines
drivingvehicles:Resultsoffocusgroupdiscussions.InProceedingsofthe19th andevaluationofclinicalexplainableAIinmedicalimageanalysis. Medical
InternationalACMSIGACCESSConferenceonComputersandAccessibility.290– ImageAnalysis84(2023),102684.
299. [42] HernisaKacorri,KrisM.Kitani,JeffreyP.Bigham,andChiekoAsakawa.2017.
[20] SorrelBrown.2010.Likertscaleexamplesforsurveys.ANRProgramevaluation, PeoplewithVisualImpairmentTrainingPersonalObjectRecognizers:Feasibility
IowaStateUniversity,USA(2010). andChallenges.InProceedingsofthe2017CHIConferenceonHumanFactorsin
[21] YangTristaCao,KyleSeelman,KyungjunLee,andHalDauméIII.2022.What’s ComputingSystems(Denver,Colorado,USA)(CHI’17).AssociationforComputing
DifferentbetweenVisualQuestionAnsweringforMachine"Understanding" Machinery,NewYork,NY,USA,5839–5849. https://doi.org/10.1145/3025453.
VersusforAccessibility?arXivpreprintarXiv:2210.14966(2022). 3025899
[22] TaizhouChen,LantianXu,XianshanXu,andKeningZhu.2021. Gestonhmd: [43] MariaKaramandMCSchraefel.2006.Investigatingusertoleranceforerrorsin
Enablinggesture-basedinteractiononlow-costvrhead-mounteddisplay.IEEE vision-enabledgesture-basedinteractions.InProceedingsoftheworkingconference
TransactionsonVisualizationandComputerGraphics27,5(2021),2597–2607. onAdvancedvisualinterfaces.225–232.
[23] Tai-YinChiu,YinanZhao,andDannaGurari.2020. Assessingimagequality [44] RafalKocielnik,SaleemaAmershi,andPaulN.Bennett.2019.WillYouAccept
issuesforreal-worldproblems.InproceedingsoftheIEEE/CVFconferenceon anImperfectAI?ExploringDesignsforAdjustingEnd-UserExpectationsofAI
computervisionandpatternrecognition.3646–3656. Systems.InProceedingsofthe2019CHIConferenceonHumanFactorsinComputing
[24] J.Deng,W.Dong,R.Socher,L.Li,KaiLi,andLiFei-Fei.2009. ImageNet:A Systems(Glasgow,ScotlandUk)(CHI’19).AssociationforComputingMachinery,
large-scalehierarchicalimagedatabase.In2009IEEEConferenceonComputer NewYork,NY,USA,1–14. https://doi.org/10.1145/3290605.3300641
VisionandPatternRecognition.248–255. [45] TomKontogiannis.1999. Userstrategiesinrecoveringfromerrorsinman–
[25] RahhalErrattahi,AsmaaElHannani,andHassanOuahmane.2018.Automatic machinesystems.SafetyScience32,1(1999),49–68.
speechrecognitionerrorsdetectionandcorrection:Areview.ProcediaComputer [46] TomKontogiannisandStathisMalakis.2009.Aproactiveapproachtohuman
Science128(2018),32–37. errordetectionandidentificationinaviationandairtrafficcontrol.SafetyScience
[26] BhanukaGamage,Thanh-ToanDo,NicholasSeowChiangPrice,ArthurLow- 47,5(2009),693–706. https://doi.org/10.1016/j.ssci.2008.09.007
ery,andKimMarriott.2023. WhatdoBlindandLow-VisionPeopleReally [47] AlexeyKurakin,IanGoodfellow,SamyBengio,etal.2016.Adversarialexamples
Want from Assistive Smart Devices? Comparison of the Literature with a inthephysicalworld.
FocusStudy.InProceedingsofthe25thInternationalACMSIGACCESSCon- [48] AlinaKuznetsova,HassanRom,NeilAlldrin,JasperUijlings,IvanKrasin,Jordi
ferenceonComputersandAccessibility (<conf-loc>,<city>NewYork</city>, Pont-Tuset,ShahabKamali,StefanPopov,MatteoMalloci,AlexanderKolesnikov,
<state>NY</state>,<country>USA</country>,</conf-loc>)(ASSETS’23).As- etal.2020. Theopenimagesdatasetv4:Unifiedimageclassification,object
sociationforComputingMachinery,NewYork,NY,USA,Article30,21pages. detection,andvisualrelationshipdetectionatscale. Internationaljournalof
https://doi.org/10.1145/3597638.3608955 computervision128,7(2020),1956–1981.
[27] SaharGhannay,NathalieCamelin,andYannickEsteve.2015.WhichASRerrors [49] BenLafreniere,TanyaR.Jonker,StephanieSantosa,MarkParent,MichaelGlueck,
arehardtodetect.InErrorsbyHumansandMachinesinMultimedia,Multimodal ToviGrossman,HrvojeBenko,andDanielWigdor.2021. Falsepositivesvs.
andMultilingualDataProcessing(ERRARE2015)Workshop,Sinaia,Romania. falsenegatives:Theeffectsofrecoverytimeandcognitivecostsoninputerror
11–13. preference.InThe34thAnnualACMSymposiumonUserInterfaceSoftwareand
[28] SaharGhannay,YannickEsteve,andNathalieCamelin.2015.Wordembeddings Technology.54–68.
combinationandneuralnetworksforrobustnessinasrerrordetection.In2015 [50] ChanhwiLee,JaehanKim,SeoungbaeCho,JinwoongKim,JisangYoo,and
23rdEuropeanSignalProcessingConference(EUSIPCO).IEEE,1671–1675. SoonchulKwon.2020.Developmentofreal-timehandgesturerecognitionfor
[29] SharonGoldwater,DanJurafsky,andChristopherDManning.2010. Which tabletopholographicdisplayinteractionusingazurekinect.Sensors20,16(2020),
wordsarehardtorecognize?Prosodic,lexical,anddisfluencyfactorsthatincrease 4566.
speechrecognitionerrorrates.SpeechCommunication52,3(2010),181–200. [51] JaewookLee,JaylinHerskovitz,Yi-HaoPeng,andAnhongGuo.2022.ImageEx-
[30] RicardoEGonzalezPenuela,JazminCollins,CynthiaBennett,andShiriAzenkot. plorer:Multi-layeredtouchexplorationtoencourageskepticismtowardsimper-
2024.InvestigatingUseCasesofAI-PoweredSceneDescriptionApplicationsfor fectAI-generatedimagecaptions.InProceedingsofthe2022CHIConferenceon
BlindandLowVisionPeople.InProceedingsoftheCHIConferenceonHuman HumanFactorsinComputingSystems.1–15.
FactorsinComputingSystems.1–21. [52] KyungjunLee,JonggiHong,EbrimaJarjue,ErnestEssuahMensah,andHernisa
[31] IanJGoodfellow,JonathonShlens,andChristianSzegedy.2014.Explainingand Kacorri.2022. Fromthelabtopeople’shome:lessonsfromaccessingblind
harnessingadversarialexamples.arXivpreprintarXiv:1412.6572(2014). participants’interactionsviasmartglassesinremotestudies.InProceedingsof
[32] JoãoGuerreiro,EshedOhn-Bar,DraganAhmetovic,KrisKitani,andChieko the19thinternationalwebforallconference.1–11.
Asakawa.2018. Howcontextanduserbehavioraffectindoornavigationas- [53] KyungjunLee,JonggiHong,SimonePimento,EbrimaJarjue,andHernisaKa-
sistanceforblindpeople.InProceedingsofthe15thInternationalWebforAll corri.2019. Revisitingblindphotographyinthecontextofteachableobject
Conference.1–4. recognizers.InThe21stInternationalACMSIGACCESSConferenceonComputers
[33] SarthakGupta,SiddhantBagga,andDeepakKumarSharma.2020.Handgesture andAccessibility.83–95.
recognitionforhumancomputerinteractionanditsapplicationsinvirtualreality.UnderstandingHowBlindUsersHandleObjectRecognitionErrors:StrategiesandChallenges ASSETS’24,October27–30,2024,St.John’s,NL,Canada
[54] KyungjunLeeandHernisaKacorri.2019.Handsholdingcluesforobjectrecogni- [78] WoosukSeoandHyungguJung.2021.Understandingthecommunityofblind
tioninteachablemachines.InProceedingsofthe2019CHIConferenceonHuman orvisuallyimpairedvloggersonYouTube.UniversalAccessintheInformation
FactorsinComputingSystems.1–12. Society20(2021),31–44.
[55] Kyungjun Lee, Daisuke Sato, Saki Asakawa, Hernisa Kacorri, and Chieko [79] DongheeShin.2021.Theeffectsofexplainabilityandcausabilityonperception,
Asakawa.2020. Pedestriandetectionwithwearablecamerasfortheblind:A trust,andacceptance:ImplicationsforexplainableAI.InternationalJournalof
two-wayperspective.InProceedingsofthe2020CHIConferenceonHumanFactors Human-ComputerStudies146(2021),102551.
inComputingSystems.1–12. [80] AbigaleStangl,EmmaSadjo,PardisEmami-Naeini,YangWang,DannaGurari,
[56] DewenLiu,ChangfeiLi,JieqiongZhang,andWeidongHuang.2023.Robotservice andLeahFindlater.2023.“Dumpit,Destroyit,SendittoDataHeaven”:Blind
failureandrecovery:Literaturereviewandfuturedirections.InternationalJournal People’sExpectationsforVisualPrivacyinVisualAssistanceTechnologies.In
ofAdvancedRoboticSystems20,4(2023),17298806231191606. Proceedingsofthe20thInternationalWebforAllConference.134–147.
[57] LiLiu,WanliOuyang,XiaogangWang,PaulFieguth,JieChen,XinwangLiu,and [81] C.Szegedy,V.Vanhoucke,S.Ioffe,J.Shlens,andZ.Wojna.2016.Rethinkingthe
MattiPietikäinen.2020. Deeplearningforgenericobjectdetection:Asurvey. InceptionArchitectureforComputerVision.In2016IEEEConferenceonComputer
Internationaljournalofcomputervision128,2(2020),261–318. VisionandPatternRecognition(CVPR).2818–2826. https://doi.org/10.1109/CVPR.
[58] OlgaLukashova-Sanz,MartinDechant,andSiegfriedWahl.2023.TheInfluence 2016.308
ofDisclosingtheAIPotentialErrortotheUserontheEfficiencyofUser–AI [82] Yik-CheungTam,YunLei,JingZheng,andWenWang.2014.ASRerrordetection
Collaboration.AppliedSciences13,6(2023),3572. usingrecurrentneuralnetworklanguagemodelandcomplementaryASR.In2014
[59] HaleyMacLeod,CynthiaLBennett,MeredithRingelMorris,andEdwardCutrell. IEEEInternationalConferenceonAcoustics,SpeechandSignalProcessing(ICASSP).
2017.Understandingblindpeople’sexperienceswithcomputer-generatedcap- IEEE,2312–2316.
tionsofsocialmediaimages.InProceedingsofthe2017CHIConferenceonHuman [83] PuchuanTan,XiHan,YangZou,XuechengQu,JiangtaoXue,TongLi,Yiqian
FactorsinComputingSystems.5988–5999. Wang,RuizengLuo,XiCui,YuanXi,etal.2022.Self-poweredgesturerecognition
[60] CarlMacrae.2022. Learningfromthefailureofautonomousandintelligent wristbandenabledbymachinelearningforfullkeyboardandmulticommand
systems:Accidents,safety,andsociotechnicalsourcesofrisk.Riskanalysis42,9 input.AdvancedMaterials34,21(2022),2200793.
(2022),1999–2025. [84] EricSVorm.2018. Assessingdemandfortransparencyinintelligentsystems
[61] ManiratnamMandal,DeeptiGhadiyaram,DannaGurari,andAlanCBovik.2023. usingmachinelearning.In2018InnovationsinIntelligentSystemsandApplications
HelpingVisuallyImpairedPeopleTakeBetterQualityPictures.IEEETransactions (INISTA).IEEE,1–7.
onImageProcessing(2023). [85] JunhongWang,YunLi,ZhaoyuZhou,ChengshunWang,YijieHou,LiZhang,
[62] DanielaMassiceti,CamillaLongden,AgnieszkaSlowik,SamuelWills,Martin XiangyangXue,MichaelKamp,XiaolongLukeZhang,andSimingChen.2022.
Grayson,andCecilyMorrison.2023.ExplainingCLIP’sperformancedisparities When,whereandhowdoesitfail?aspatial-temporalvisualanalyticsapproach
ondatafromblind/lowvisionusers.arXivpreprintarXiv:2311.17315(2023). forinterpretableobjectdetectioninautonomousdriving.IEEETransactionson
[63] MMeghana,ChUshaKumari,JSthuthiPriya,PMrinal,KAbhinavVenkatSai, VisualizationandComputerGraphics29,12(2022),5033–5049.
SPrashanthReddy,KVikranth,TSantoshKumar,andAsisaKumarPanigrahy. [86] ChenyunWu,RabiaSehab,AhmadAkrad,andCristinaMorel.2022. Fault
2020. Handgesturerecognitionandvoicecontrolledrobot. MaterialsToday: diagnosismethodsandFaulttolerantcontrolstrategiesfortheelectricvehicle
Proceedings33(2020),4121–4123. powertrains.,4840pages.
[64] MeredithRingelMorris.2020.AIandAccessibility.Commun.ACM63,6(2020), [87] JingyiXie,RuiYu,HeZhang,SooyeonLee,SyedMasumBillah,andJohnM
35–37. Carroll.2024.BubbleCam:EngagingPrivacyinRemoteSightedAssistance.In
[65] ChelseaMyers,AnushayFurqan,JessicaNebolsky,KarinaCaro,andJichenZhu. ProceedingsoftheCHIConferenceonHumanFactorsinComputingSystems.1–16.
2018. Patternsforhowusersovercomeobstaclesinvoiceuserinterfaces.In [88] GuangxiaoZhang,ZhuolinJiang,andLarrySDavis.2012.Onlinesemi-supervised
Proceedingsofthe2018CHIconferenceonhumanfactorsincomputingsystems. discriminativedictionarylearningforsparserepresentation.InAsianconference
1–7. oncomputervision.Springer,259–273.
[66] NooruddinNooruddin,RahoolDembani,andNizamuddinMaitlo.2020.HGR: [89] LotusZhang,AbigaleStangl,TanusreeSharma,Yu-YunTseng,InanXu,Danna
Hand-gesture-recognitionbasedtextinputmethodforAR/VRwearabledevices. Gurari,YangWang,andLeahFindlater.2024.DesigningAccessibleObfuscation
In2020IEEEinternationalconferenceonsystems,man,andcybernetics(SMC). SupportforBlindIndividuals’VisualPrivacyManagement.InProceedingsofthe
IEEE,744–751. CHIConferenceonHumanFactorsinComputingSystems.1–19.
[67] ThomasJPalmeriandIsabelGauthier.2004.Visualobjectunderstanding.Nature [90] ZhuohaoJerryZhang,SmirityKaushik,JooYoungSeo,HaolinYuan,SauvikDas,
ReviewsNeuroscience5,4(2004),291. https://doi.org/10.1038/nrn1364 LeahFindlater,DannaGurari,AbigaleStangl,andYangWang.2023.{ImageAlly}:
[68] CathyPearl.2016. Designingvoiceuserinterfaces:Principlesofconversational A{Human-AI}HybridApproachtoSupportBlindPeopleinDetectingand
experiences."O’ReillyMedia,Inc.". RedactingPrivateImageContent.InNineteenthSymposiumonUsablePrivacy
[69] JaumeRPerello-March,ChristopherGBurns,RogerWoodman,MarkTElliott, andSecurity(SOUPS2023).417–436.
andStewartABirrell.2021. Driverstatemonitoring:Manipulatingreliability [91] Zhong-QiuZhao,PengZheng,Shou-taoXu,andXindongWu.2019. Object
expectationsinsimulatedautomateddrivingscenarios. IEEEtransactionson detectionwithdeeplearning:Areview.IEEEtransactionsonneuralnetworksand
intelligenttransportationsystems23,6(2021),5187–5197. learningsystems30,11(2019),3212–3232.
[70] BiswajeetPradhan,AbhirupDikshit,SaroLee,andHyesuKim.2023.Anexplain- [92] ZhengxiaZou,KeyanChen,ZhenweiShi,YuhongGuo,andJiepingYe.2023.
ableAI(XAI)modelforlandslidesusceptibilitymodeling.AppliedSoftComputing Objectdetectionin20years:Asurvey.Proc.IEEE111,3(2023),257–276.
142(2023),110324.
[71] KasparRaats,VaikeFors,andSarahPink.2020.Trustingautonomousvehicles:An
A INTERVIEWQUESTIONS
interdisciplinaryapproach.TransportationResearchInterdisciplinaryPerspectives
7(2020),100201. Inthissection,you’llfindthequestionsposedtotheparticipants
[72] LarryDRosen,KellyWhaling,LMarkCarrier,NancyACheever,andJeffrey
Rokkum.2013.Themediaandtechnologyusageandattitudesscale:Anempirical duringtheuserstudy.Ifaquestionhasmultiple-choiceoptions,
investigation.Computersinhumanbehavior29,6(2013),2501–2511. they’relistedinsquarebracketsafterthequestion.
[73] OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,SanjeevSatheesh,Sean
Ma,ZhihengHuang,AndrejKarpathy,AdityaKhosla,MichaelBernstein,etal.
A.1 DemographicInformation
2015.Imagenetlargescalevisualrecognitionchallenge.InternationalJournalof
ComputerVision115,3(2015),211–252.
• Whatisyourage?
[74] ManaswiSaha,AlexanderJFiannaca,MelanieKneisel,EdwardCutrell,and
MeredithRingelMorris.2019.Closingthegap:Designingforthelast-few-meters • Whatisyourgenderorgenderidentity?[woman,man,non-
wayfindingproblemforpeoplewithvisualimpairments.InThe21stinternational binary]
acmsigaccessconferenceoncomputersandaccessibility.222–235.
• Whatisyouroccupation?
[75] ElliotSalisbury,EceKamar,andMeredithMorris.2017.Towardscalablesocial
alttext:Conversationalcrowdsourcingasatoolforrefiningvision-to-language • Whatisyourdominanthand?[left,right]
technologyfortheblind.InProceedingsoftheAAAIConferenceonHumanCom- • Whatphonedoyouuse?Doyouusethescreenreader(e.g.,
putationandCrowdsourcing,Vol.5.
[76] AbigailJSellen.1994. Detectionofeverydayerrors. AppliedPsychology43,4 VoiceOver)?
(1994),475–498.
[77] NaveenSendhilnathan,TingZhang,BenLafreniere,ToviGrossman,andTanyaR A.1.1 VisualImpairments.
Jonker.2022. Detectinginputrecognitionerrorsandusererrorsusinggaze • Doyouhavevisualimpairments?[yes,no]
dynamicsinvirtualreality.InProceedingsofthe35thAnnualACMSymposiumon
UserInterfaceSoftwareandTechnology.1–19. • Describeyourcurrentlevelofvision.ASSETS’24,October27–30,2024,St.John’s,NL,Canada JonggiHongandHernisaKacorri
• Forhowmanyyearshaveyouhadthislevelofvisionability? • Whatchallengesdoyoufacewhentakingphotosorbroadly
manipulatingacamera?
A.1.2 HearingImpairments.
• Doyouhavehearingimpairments?[yes,no] A.4 ExperiencewithImage-BasedAssistive
• Describeyourcurrentlevelofhearingability.
Tools
• For how many years have you had this level of hearing
(Thefollowingquestionsareaskedforeachapplicationfromthe
ability?
question above “With what applications or tasks do you use a
A.1.3 MotorImpairments. camera?”)
• Doyouhavemotorimpairments?[yes,no] • Howoftendoyouusetheapp/tool?[never,onceamonth,
• Describeyourcurrentlevelofmotorability. severaltimesamonth,onceaweek,severaltimesaweek,
• Forhowmanyyearshaveyouhadthislevelofmotorability? onceaday,severaltimesaday]
• Howoftendoyouusetheapp/toolwhenyoudon’thave
A.2 TechnologyExperience accesstosightedhelp?[never,onceamonth,severaltimes
• Howoftendoyouuseamobiledevice?[never,onceamonth, a month, once a week, several times a week, once a day,
severaltimesamonth,onceaweek,severaltimesaweek, severaltimesaday]
onceaday,severaltimesaday] – Canyouprovidesomeexamplesofwhenthisoccurs?
• Howwouldyouclassifyyourleveloffamiliaritywithma- • Howoftenwouldyounoticethattheapp/toolwaswrong
chinelearning?[ afterthefact?[never,onceamonth,severaltimesamonth,
– notfamiliaratall(haveneverheardofmachinelearning) onceaweek,severaltimesaweek,onceaday,severaltimes
– slightlyfamiliar(haveheardofitbutdon’tknowwhatit aday]
does) • Howoftendoyouencountermisrecognitionswhenyouuse
– somewhatfamiliar(Ihaveabroadunderstandingofwhat theapp/tool?[never,veryrarely,rarely,occasionally,very
itisandwhatitdoes) frequently,always]
– extremelyfamiliar(Ihaveextensiveknowledgeofmachine • Howoftendoyouverifytherecognitionresultswhenyou
learning) usetheapp/tool?[never,veryrarely,rarely,occasionally,
] veryfrequently,always]
– Why?
A.3 Photo-takingExperience • Ifindtheapp/tooltobeuseful.[stronglydisagree,disagree,
neitheragreenordisagree,agree,stronglyagree]
• Howoftendoyoutakephotosorrecordavideo?[never,
• Icareaboutthemisrecognitionsoftheapp/tool?
onceamonth,severaltimesamonth,onceaweek,several
– Why?
timesaweek,onceaday,severaltimesaday]
• Aretheresomesituationsinwhichyoucareaboutthemis-
• Howoftendoyouchangethesettingofthecameraorsome-
recognitionsmorethanothers?
thingintheenvironment?Forexample,sittingatthesame
• It is challenging to detect the misrecognitions. [strongly
table,lightcondition,orusingflash.[never,onceamonth,
disagree,disagree,neitheragreenordisagree,agree,strongly
severaltimesamonth,onceaweek,severaltimesaweek,
agree]
onceaday,severaltimesaday]
• Onwhatdevicesdoyouusetheapp/tool?
– (ifnot“never”)Pleasedescribeforwhattasksandwhy.
• Forwhattasksdoyouusetheapp/tool?
• Howoftendoyoucheckifaphotoisgoodaftertakingit?
• Whatmechanismsdoyouusetodetectthemisrecognitions
[never,onceamonth,severaltimesamonth,onceaweek,
ifany?
severaltimesaweek,onceaday,severaltimesaday]
• Whatkindsofobjectsdoyoutypicallytrytorecognizewith
– Doyouhaveastrategyforcheckingaphoto?
theapp/tool?
• Whichofthefollowingdoyoucapturewithacamera?(select
• Whatisyourstrategyfortakinggoodphotoswhenusing
allthatapply)[document,people,landscapes,food,objects,
theapp/tool?
others]
• Doyouhaveasenseofhowtheapp/toolworksandhowit
– (foreach,)Howoftendoyoucaptureitwithacamera?
isabletorecognizetheobject?
• Onwhatdevicesdoyouinteractwithacameralikeasmart-
• Howdidyoulearntousetheapp/toolwhenyoufirstin-
phone,computer,smartglasses,orotherdevices?
stalledit?
• Withwhatapplicationsortasksdoyouuseacamera?For
• Doyoulikeanyfunctionsorspecificinteractionswiththe
example,postingonsocialmedia,videocalls,assistivetech-
app/tool?
nologies,etc.
• Doyoudislikeanyfunctionsorspecificinteractionswith
– Whydo(ordon’t)youuseacamerawiththeseapplications
theapp/tool?
ortasks?
• Whenyoutakeaphoto,howoftendoyoufeelconfidentthat • Areyouawareofanymobileapplicationsthatallowyouto
itwasgood?[never,veryrarely,rarely,occasionally,very personalizethembygivingphotosofobjectsorpeoplethat
frequently,always] youcareabout?UnderstandingHowBlindUsersHandleObjectRecognitionErrors:StrategiesandChallenges ASSETS’24,October27–30,2024,St.John’s,NL,Canada
– (Ifyes)Listthem.Whichofthemhaveyouusedbefore? • Itwasdifficulttoidentifyerrorsmadebytheobjectrecog-
Canyoutellmeabitmoreaboutyourexperience? nizer.[stronglydisagree,disagree,neitheragreenordisagree,
agree,stronglyagree]
A.5 Post-TaskQuestions – Why?
Weaskedthefollowingquestionstotheparticipantsaftertheerror • Howdidyouknowwhenanobjectwasincorrectlyrecog-
identificationtask. nized?