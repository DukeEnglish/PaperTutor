ClassiFIM: An Unsupervised Method To Detect Phase
Transitions
VictorKasatkin EvgenyMozgunov NicholasEzzell
USCViterbi USCViterbiISI USC
postdoc@worldofml.com
UtkarshMishra ItayHen DanielLidar
UniversityofDelhi USC USC
Abstract
EstimationoftheFisherInformationMetric(FIM-estimation)isanimportanttask
thatarisesinunsupervisedlearningofphasetransitions,aproblemproposedby
physicists. This work completes the definition of the task by defining rigorous
evaluationmetricsdistMSE,distMSE ,anddistREandintroducesClassiFIM,a
PS
novelmachinelearningmethoddesignedtosolvetheFIM-estimationtask.Unlike
existingmethodsforunsupervisedlearningofphasetransitions,ClassiFIMdirectly
estimates a well-defined quantity (the FIM), allowing it to be rigorously com-
paredtoanypresentandfutureothermethodsthatestimatethesame.ClassiFIM
transforms a dataset for the FIM-estimation task into a dataset for an auxiliary
binaryclassificationtaskandinvolvesselectingandtrainingamodelforthelatter.
WeprovethattheoutputofClassiFIMapproachestheexactFIMinthelimitof
infinitedatasetsizeandundercertainregularityconditions.WeimplementClas-
siFIMonmultipledatasets,includingdatasetsdescribingclassicalandquantum
phasetransitions,andfindthatitachievesagoodgroundtruthapproximationwith
modestcomputationalresources.Furthermore,weindependentlyimplementtwo
alternativestate-of-the-artmethodsforunsupervisedestimationofphasetransition
locationsonthesamedatasetsandfindthatClassiFIMpredictssuchlocationsat
leastaswellastheseothermethods.Toemphasizethegeneralityofourmethod,
we also propose and generate the MNIST-CNN dataset, which consists of the
outputofCNNstrainedonMNISTfordifferenthyperparameterchoices.Using
ClassiFIMonthisdatasetsuggeststhereisaphasetransitioninthedistribution
ofimage-predictionpairsforCNNstrainedonMNIST,demonstratingthebroad
scopeofFIM-estimationbeyondphysics.
1 Introduction
ThetaskofestimatingtheFisherInformationMetric(FIM)anditsrelationshipwithunsupervised
detectionofbothclassicalandquantumphasetransitionsisofgreatinteresttophysicistsseeking
to discover new phases of matter. A companion paper Kasatkin et al. (2024) introduces this task
anddescribesthegenerationandanalysisofcorrespondingphysicsdatasets.FIMestimationwitha
1-dimensionalparameterspaceisrelatedtothechangepointdetectiontask(defined,e.g.,inTruong
etal.(2020))whendatapointscanbeinterpretedassamplesfromaprobabilitydistribution.Wenote
thattheFIMwasalsousedinmachinelearning,albeitinadifferentcontext(Kasatkinetal.,2024,
App.G):see,e.g.,Karakidaetal.(2019);Martens(2020);Kirkpatricketal.(2017);Amari(1998).
Here,weintroduceClassiFIM,amethodforsolvingtheFIM-estimationtask,evaluateitonanumber
ofdatasets,andcompareittoexistingstate-of-the-artmethodsvanNieuwenburgetal.(2016);Huang
Preprint.Underreview.
4202
guA
6
]GL.sc[
1v32330.8042:viXraetal.(2022)(eventhoughtheyaredesignedtosolvedifferenttasks,alsorelatedtounsupervised
learningofphasetransitions)onthesamedatasets.
ThisworkandthecompanionpaperKasatkinetal.(2024)arecomplementary,writtenfordifferent
audiences,andcanbereadindependently.Thisworkisaimedprimarilyatcomputerscientists,while
thecompanionpaperisaimedprimarilyatphysicists.
2 BackgroundonFisherinformationmetric
In order to formally define our task, we must first define the Fisher information metric (FIM),
which is a formal way to measure the square of the rate of change of a probability distribution.
Considerastatisticalmanifold(SM),i.e.acollectionofdistributionsP (•),dependingonavectorof
λ
continuousparametersλ=(λ ,··· ,λ ),satisfyingcertainregularityconditionsSchervish(1995).
1 N
Thescorefunctionfortheµ-thparameterλ isthederivativeofthelog-likelihoodandisgivenby
µ
s (x;λ)=∂ lnP (x).TheFisherInformationMetric(FIM)isthevarianceofthescorefunction
oµ r,equivalentλ lyµ (sincλ ebynormalizationE[s (X;λ)]=(cid:82) dxP (x)s (x;λ)=0),theexpectation
µ λ µ
valueoftheproductofpairsofscorefunctions:
g (λ)=E[s (X;λ)s (X;λ)]. (1)
µν µ ν
Chencov’stheoremimpliesthatg (λ)is(uptorescaling)theuniqueRiemannianmetriconthe
µν
space of probability distributions satisfying a monotonicity condition and invariant under taking
sufficientstatisticsChencov(2000);Amari(2016).Thatis,theFIMisthenaturalwaytomeasure(the
squareof)thespeedwithwhichtheprobabilitydistributionchangeswithrespecttotheparameters
λ.Infact,theFIM(uptoascalingfactor)canbewrittenasthefirstnon-trivialtermintheTaylor
seriesexpansionintheparameterdifferencesδλofmultipledivergencemeasuresD(P ||P ),
λ λ+δλ
includingtheKullback-LeiblerdivergenceandtheJensen-Shannondivergence.
TheFIMisa(non-negativedefinite)RiemannianmetricontheparameterspaceM,i.e.,itcanbe
naturallyusedtomeasurelengthsofcurvesintheparameterspace.
The above coordinate description can be generalized to λ belonging to a smooth manifold M,
resulting in a symmetric 2-form g, which is the FIM, and the ClassiFIM method would still be
applicable.Intheremainderofthiswork,however,we,forsimplicity,focusonthecaseM=[0,1]N
forN =1,2andthecasewherethedatasetcontainssampleswithλ ∈M′ ⊂M,whereM′isa
i
regularsquaregridinM.
MoredetailsontheFIMcanbefoundin(Kasatkinetal.,2024,Sec.II)andinAppendixA.
3 TheFIM-estimationtask
TheFIM-estimationtaskwasproposedin(Kasatkinetal.,2024,SecIII.A).Hereweremindthe
readerofthetaskandproposetheevaluationmetrics,thuscompletingtheformaldefinitionofthe
task.
Throughoutthiswork,theinputdatasetisassociatedwithanSM(M,P)andhastheformD =
train
{(λ ,x )}|Dtrain|,whereeachx isasamplefromtheprobabilitydistributionP (•).Inadditiontothe
i i i=1 i λi
dataset,someinformationaboutthenatureandthedataformatofthesamplesx mightbegiven(see
i
AppendixBforanexample).Thetaskistoprovideanestimategˆ (λ)oftheFIMforallλ∈M.
µν
WhenthegroundtruthFIMisavailable,onecanmeasurethequalityofsuchanestimateusingthe
metricsdistMSE,distMSE ,anddistREintroducedbelow.Weprovidemoredetails,including
PS
the precise definitions of each of the metrics in Appendix C. The key property of the evaluation
metricsisthattheyestimatehowwelltheestimatedFIMreproducesthedistancesbetweenpoints
intheparameterspace.Inordertocomputeeachofthemetrics,oneneedstocomputethelengths
ofstraightlineintervalsbetweenall|M′|(|M′|−1)/2pairsofpointsinM′accordingtoboththe
groundtruthFIMg andtheestimatedFIMgˆ.Then,distREis(uptominortechnicaldetails)the
probabilitythattheestimatedFIMrankstwosuchlengthsincorrectly.distMSEisthemeansquared
errorbetweenthelengthsaccordingtotheestimatedandthegroundtruthFIMs.distMSE isa
PS
variantofdistMSEthatisinvariantunderrescalingoftheestimatedFIM:itisequaltotheminimum
ofdistMSEofascaledpredictiontakenoverallpositive(constant)scalingfactors.
2Whenevaluatingtheperformanceofamethod,onemightusedistMSEwhenwell-calibrated(cor-
rectlyscaled)predictionsareneededanddistMSE ordistREwhenthescaleofthepredictions
PS
islessimportant.distREisalwaysbetween0and1(100%)andisindependentofthescaleofthe
predictionsorthegroundtruth.
4 TheClassiFIMmethod
WeproposeClassiFIMasanunsupervisedMLmethodtosolveFIM-estimationtasks.
4.1 IdeabehindClassiFIM
ConsiderasimplecasewheretheprobabilitydistributionP isparameterizedbyasingleparameter
λ
λandwewishtoestimatetheFIMatλ = 0.Theideaistopickasmallεandrandomlyselecta
sign±(withprobability0.5each),thensamplexfromP andhandbothεandxtoatrained
λ=±ε
binaryclassificationmodel(i.e.,abinaryclassifier(BC))tryingtopredictwhatthesignwas.Ifthe
distributionsP areidentical,theFIMislikelytobesmallandawell-trainedmodeltrainedto
λ=±ε
minimizethecross-entropyshouldreturnlogoddscloseto0.Ontheotherhand,ifthesedistributions
areverydifferent,themodelshouldbeconfidentofwhatthesignwasandthelogoddsshouldbe
largebyabsolutevalue.AsthecarefulderivationinTheorem1belowwillshow(inamoregeneral
setting),themeanofsquareofthelogoddsdividedby4ε2 indeedconvergestotheFIMatλ = 0.
Whenimplementingthisidea,oneneedstotakecareofafewtechnicalaspects:(1)asε→0,the
denominatorbecomessmall,hencesomethingneedstobedonetoavoidamplifyingtheerrors;(2)if
xcomesfromthesamedatasetastheoneusedtotrainthemodel,themodelcanbebiasedtowards
largeabsolutevaluesofthelogoddsonthosesamples,whichcanleadtobiasedestimatesoftheFIM;
(3)λcanbemulti-dimensionalandwewouldlikeestimatesoftheFIMatallpointsofM.
Based on the above idea, we propose ClassiFIM as an unsupervised ML method to solve FIM-
estimationtasks.Itconsistsofthreestepsdescribedbelow:transform,train,andestimate.
4.2 Step1:Transformthedataset
ThefirststepistotransformtheinputdatasetD ={(λ ,x )}|Dtrain|,whichhasnolabels,intoa
train i i i=1
datasetD = {(λ ,δλ,x,y)}withlabelsy suitablefortrainingaBC.Ineveryrow,thefirst
train,BC 0
twocomponents(λ ,δλ)representapairofdistinctpointsλ =λ ±δλ/2intheparameterspace
0 ± 0
M,y =±isasignpickedrandomlywithprobability0.5each(thelabelforbinaryclassification),
andxisarandomsamplefromthedatasetD withλ=λ .Thisdistributionisasubstituteforthe
train y
(unknown)distributionP .
λ=λy
Thiscanbeachieved,e.g.,byAlgorithm1,oravectorizedalgorithmdescribedinAppendixD.3.In
practice,thetrainingdataD canbegeneratedon-the-flyduringtraining.
train,BC
Algorithm1:GeneratingD fromD
train,BC train
Input:datasetD,evenpositiveintegerN
BC
Output:D
BC
1: D ←∅
BC
2: while|D |<N do
BC BC
3: Pick(λ +,x +),(λ −,x −)∼D{twoi.i.d.samples}
4: ifλ + =λ −then
5: continue{i.e.,gotoline2}
6: endif
7: λ 0 ←(λ ++λ −)/2
8: δλ=λ +−λ −
9: D
BC
←D BC∪{(λ 0,δλ,x −,−),(λ 0,δλ,x +,+)}{thelastcomponentisthelabel}
10: endwhile
11: returnD
BC
34.3 Step2:PickandtrainamodelforD
train,BC
WepickandtrainaBCmodell=M(λ ,δλ,x;θ)satisfyingthefollowingconditions:
0
(1) theoutputprobabilityisgivenbyPˆ(Y =+1|λ ,δλ,x)=(1+e−δλ·l)−1;
0
(2) thelearnedfunctionl=M(λ ,δλ,x;θ)extrapolateswellasδλ→0;
0
(cid:0) (cid:1)
(3) theestimatedvaluesoflogoddsδλ·lareaccurateestimatesofln P (x)/P (x) .
λ+ λ−
4.4 Step3:EstimatetheFIM
AftertrainingthisBC,onecanestimatetheFIM,gˆ,usingthefollowingformula:
gˆ (λ)=mean (l l ), (2)
µν (λ,x)∈Dtrain xµ xν
wherel =M(λ,0,x).Here,(λ,x)∈D meansthatthemeanistakenwithrespecttoallpairs
x train
(λ′,x)∈D satisfyingλ′ =λ.
train
4.5 Discussion
Severalcommentsaboutconditions(1)–(3)fromstep2andformulaEq.(2)areinorder.Condition
(1)requiresthatlogoddsarecomputedasδλ·l,whichallowsustoavoiddivisionbyasmallnumber
inEq.(2).Condition(2)isimportantbecauseD onlycontainsrowswithδλ̸=0,butEq.(2)
train,BC
evaluatesM atδλ=0.Whilecondition(3)mayseemtobeautomaticallysatisfiedbytrainingwith
cross-entropylossasanobjective,anoverparameterizedunregularizedmodelmaymemorizethe
trainingdataand,e.g.,findoutthatgivenafixedfiniteD ,itisoptimaltooutputlargelogodds
train
evenwhenP (x)=P (x)(e.g.,when(λ ,x)∈D but(λ ,x)∈/ D ).
λ+ λ− + train − train
Asdemonstratedbyouropen-sourceimplementationKasatkin(2024)andnumericalexperiments
inSection6,conditions(2)and(3)canbeachievedinpracticebytrainingaBCwithasmallL
2
regularization,i.e.,trainingintheclassicalregimeasopposedtothemodernregime.Inparticular,we
donotusenormalizationlayerssincetheychangetheroleofweightdecayinthepreviouslayers.
ThechallengewithusingClassiFIMwithBCtrainedinthemodernregimeisthetendencyofthe
overparameterized models to achieve near-zero cross-entropy loss on the training set (which is
possibleifallsamplesxaredistinct;see,e.g.,Zhangetal.(2021);Ishidaetal.(2020)),whichleads
toinflatedabsolutevaluesofthelogoddsandbiasedestimatesgˆ(i.e.,typically,gˆ>g).
OurimplementationisdescribedinSection6.3andAppendixD.2.
4.6 Theoreticaljustification
Here,weformallyjustifywhyClassiFIMworks,i.e.,weformalizeandprovetheideathatiftheBC
modelisperfect,thenEq.(2)providesanunbiasedestimateoftheFIM,whichconvergestothetrue
FIMasthedatasetsizegoestoinfinity.
Theorem1. Considerastatisticalmanifold-likepair(M,P)s.t.M⊂RmandthespaceΩofall
possiblesamplesxisfinite.Letλ beapointintheinteriorofM,andM∗beamodel,satisfying
0
thefollowingconditions:
1. M∗ is optimal at λ , i.e., for all δλ s.t. λ = λ ± δλ/2 ∈ M, and all x ∈ Ω,
0 ± 0
M∗(λ ,δλ,x)satisfies
0
(cid:16) 1+e−δλ·M∗(λ0,δλ,x)(cid:17)−1(cid:0)
P (x)+P
(x)(cid:1)
=P (x). (3)
λ+ λ− λ+
2. Thefunctionδλ(cid:55)→M∗(λ ,δλ,x)iscontinuousatδλ=0.
0
3. ∀x∈Ωthefunctionλ(cid:55)→P (x)isdifferentiableatλ .
λ 0
(cid:16) (cid:17)
4. ∀x∈ΩwithP (x)=0wehaveP (x)=o ∥δλ∥2 .
λ0 λ0+δλ
Then
g (λ )=E (l∗ l∗ ), where l∗ =M∗(λ ,0,x). (4)
µν 0 x∼Pλ0 xµ xν x 0
4Figure1:IllustrationofthedifferencesbetweentheoutputsofthreeunsupervisedMLmethods:W
(vanNieuwenburgetal.,2016),SPCA(Huangetal.,2022),andClassiFIM,whenappliedtothe
samedatasetforIsNNN400SM.Panels(a)–(c)show1DphasediagramsgeneratedbyW,SPCA,
andClassiFIM,respectively,forλ =48/64[thesliceindicatedbytheblacklineinpanels(d)–(g)].
1
TheverticaldottedlinesindicatethemaximaofthegroundtruthFIMshowninpanels(c)and(g).
Panels(d)–(f)show2DphasediagramsgeneratedbyW,SPCA,andClassiFIM,respectively.Panel
(g)showsthegroundtruthFIM.Notethatthescalesonthediagrams(a)–(c),aswellascolorschemes
onthediagrams(d)–(g),aredifferent,reflectingthedifferentmeaningsoftheoutputsofthemethods:
theWmethodproducestheaccuracyof“mislabelled”samples,whichisexpectedtoachieveapeak
withavaluecloseto1.0atthelocationsofphasetransitions;SPCAproducesthecomponentsofa
kernelprincipalcomponentsanalysis(PCA)whichareexpectedtochangerapidlyatthelocationsof
phasetransitions;ClassiFIMproducesanestimateoftheFisherInformationMetric,reflectingthe
rateofchangeoftheunderlyingprobabilitydistribution.FormoredetailsseeAppendixE(Figs.4
and5).
HereweassumethatthedefinitionoftheFIMisextendedtocaseswhereprobabilitiesofindividual
eventsmightbeequaltozeroasexplainedinAppendixA.TheproofisgiveninAppendixD.1.
5 Comparisonwithpriorwork
Phasetransitionsareoftencharacterizedbyabruptchangesintheprobabilitydistributionofthestates
ofthephysicalsystemastheparametersλchangeslightly.SincetheFIMrepresentsthesquarerate
ofchangeoftheunderlyingprobabilitydistribution,peaksoftheFIMestimatedusingtheClassiFIM
methodcanbeusedasaproxyforthelocationsofphasetransitions.Here,wedescribetwoalternative
methodsforunsupervisedlearningofphasetransitions,whichcanbeappliedtothesamedatasetsas
ClassiFIM,and,thus,canbecomparedtoit.1TheoutputsofthesetwoapproachesandClassiFIM,
whenappliedtooneofthetenIsNNN400datasetsareillustratedinFig.1.IsNNN400isbasedona
classicalantiferromagneticIsingmodelwithnearestandnext-nearestneighbor(diagonal)interactions,
involving400spinsona20×20grid.
Thefirstapproach(vanNieuwenburgetal.,2016),referredtoasWbelow,computesa“W”-like
shapewiththemiddlepeakatthelocationofthephasetransition.Therearethreekeychallengesin
comparingthismethodtoClassiFIM.First,vanNieuwenburgetal.(2016)suggestsusingtheinput
datasetcontainingso-called“entanglementspectra”foreachλ.Asnotedin(Kasatkinetal.,2024,
App.K),thisdataishardtogenerateforlargersystemsizes,eithernumericallyorexperimentally;
moreover, given this data, one can already predict a phase boundary without the use of ML by
directly computing the FIM, which defeats the purpose of applying ML. Second, the method is
directlyapplicableonlytoone-dimensionalphasediagrams(i.e.,N = 1,whereN isthenumber
ofcomponentsofλ).Third,themethodlacksawell-definedgroundtruthoraquantitativesuccess
metric(therearenoquantitativemetricsproposedinvanNieuwenburgetal.(2016)tojudgethe
qualityofWplot;instead,onecaninspectitvisuallyorcomparethepredictedlocationofphase
transitionwiththegroundtruth).
Thesecondmethod(Huangetal.,2022),referredtoasSPCAbelow,istoapplykernelprincipal
componentanalysis(PCA)toclassicalshadowswhichcanbeefficientlyobtainedonaquantum
computercapableofpreparingthegroundstateofthesystem.(Kasatkinetal.,2024,App.J)has
shownhowtoapplythismethodtothesamedatasetD asClassiFIMinsteadofclassicalshadows.
train
The output of SPCA can be visualized by using the first 3 PCA components as color channels,
resultingindifferentcoloredregionsthatcanbeinterpretedasphases.
1TheauthorshaverecentlybecomeawareofthreeadditionalmethodsthatsolvetheFIM-estimationtaskand,
therefore,canbedirectlycomparedtoClassiFIM:I andI fromArnoldetal.(2023),andFINEintroducedin
2 3
Duyetal.(2022).Weplantoincludethecomparisonwiththesemethodsinafutureversionofthiswork.
5Since W and SPCA do not solve the FIM-estimation task, we cannot use the metrics distMSE,
distMSE , and distRE to compare them to ClassiFIM. Instead, to overcome the above-listed
PS
challenges, we use the PeakRMSE metric introduced in (Kasatkin et al., 2024, App. L), which
measures the accuracy of the predicted phase boundaries in the 1D slices of the phase diagrams.
Thereisonlyoneslicefor1DSMsbut128slicesfor2DSMsdescribedbya64×64grid:64vertical
and64horizontal.FollowingthemethodologyofPeakRMSE,weneedtomodifyallmethodsto
takethemaximalnumberofguessesn foreachslicesandoutputn guessesforapositionofa
s s
phase-transition-likefeatureonthatslice.PeakRMSEisthengivenby
(cid:113)
PeakRMSE= mean min(x −y )2, (5)
s,j s,k s,j
k
wherethelist{x }ns+1 containsn guessesandtwoboundarypointsofthesliceand{y }n′ s
s,k k=0 s s,j j=1
arethelocationsofthepeaksofthegroundtruthFIMonslices,n′ ≤ n .Theguessesshouldbe
s s
madewiththegoalofminimizingPeakRMSE.Wecallsuchmodifiedmethodsmod-W,mod-SPCA,
andmod-ClassiFIM.
Toensureafaircomparison,weattempttoadheretothefollowingprinciplesforallmethods:(i)use
thesameinput(D anddescriptions)formakingthepredictions;(ii)avoidanycomparisonswith
train
groundtruthuntiltheneuralnetwork(NN)architectures,hyperparameters,andtrainingprocedures
arefinalized;(iii)imposethesamecomputationaltimelimit;(iv)includepost-processingtoimprove
the PeakRMSE. We document in Appendix E cases where these rules were broken. There are
naturalmodificationsforallthreemethodsforthePeakRMSEmetric.Inmod-W,weperformthe
training and extraction of the accuracy plot separately for each slice and optimize the training
timebytrainingall63networkscorrespondingtoasinglesliceinparallel.Theaccuracyplotsare
post-processedusingslope-capping.Inbothmod-Wandmod-ClassiFIM,thehighest-prominence
peaks(scipy.signal.find_peaks)aresubmittedastheguesses.Post-processingformod-
ClassiFIMandmod-SPCAinvolvesGaussiansmoothing.Toextractpeaksinmod-SPCA,weuse
k-meansclusteringonthefirsttwoPCAcomponentsaugmentedtoencouragespatialclustering.For
moredetails,seeAppendixE.WereportthecomparisonresultsinthePeakRMSEcolumnofTable3
inSection6.5below.
6 Numericalexperiments
6.1 Datasetsandcomputelimits
ThedatasetsusedinournumericalexperimentsaresummarizedinTable1.Thefirstsixrowsare
from(Kasatkinetal.,2024,TableI)andcorrespondtothesixphysicalSMs(60datasets)introduced
there.ThelastrowcorrespondstotheMLdatasetMNIST-CNNintroducedinSection6.2.Eachof
the61datasetsDhas140samplesforeachλ∈M′andissplitinto90%D and10%D .The
train test
sixphysicalSMshaveagroundtruthFIM,whileMNIST-CNNdoesnot.Whenmultipledatasets
areavailableforthesameSM,theyareeffectivelydifferentonlybecauseofthedifferentrandom
seedsusedtogeneratethesamples.Thevaluesinthe“Time”columnarethesuggestedtimelimitsin
minuteswhenexecutingonasystemwithasingleNVIDIAGeForceGTX10606GBGPU(tobe
scaleddownappropriatelywhentrainingonmoremodernhardware).Weaddaproposedvaluefor
MNIST-CNNandusethesetoensureafaircomparisonbetweenthemethods(ClassiFIM,W,SPCA).
Table1: Summaryofstatisticalmanifoldsanddatasetsusedinthiswork.
Name #D #features M |M′| Time systemdescription
Ising400 10 400 (0,1] 1000 10 2Dclassical,Gibbsstate
IsNNN400 10 400 (0,1]2 4096 20 2Dclassical,Gibbsstate
FIL24 10 24 [0,1)2 4096 10 2DSpin-1/2,groundstate
Hubbard12 10 24 [0,1)2 4096 10 2DFermionic,groundstate
Kitaev20 10 20 (−4,4) 20000 10 1DFermionic,groundstate
XXZ300 10 300 [0,1]2 4096 125 1DSpin-1/2,groundstate
MNIST-CNN 1 795 [0,1)2 4096 60 outputsofneuralnetworks
6λ
Concat FC ReLU FC ReLU
δλ δλδλT
Concat FC ReLU FC ReLU FC l
x Conv ReLU Conv ReLU AvgPool
Figure2: TheneuralnetworkarchitectureweusedfortheHubbard12andFIL24datasets.Here
“Conv”layersaregraphconvolutionallayersforthe12-sitelatticewithtwodifferenttypesofedges.
6.2 MNIST-CNNdataset
We construct the MNIST-CNN dataset as a toy example to show how ClassiFIM can be used to
study the space of different regimes that occur in the training of a neural network. Many works
(seeBaldassietal.(2022)andreferencestherein)interpretedregimessuchassuccessandfailureof
learning,aswellashighlyoverparametrizedbehavior,asdistinctphasesofmatterofthedynamical
systemthatistheneuralnetwork,withthestategivenbyitsweightsand/orthetableofpredictions.
We note that while this pursuit did not directly influence any performance improvement, it is of
academicinterestnonetheless.HerewesuggestrepurposingMNISTforsuchinvestigations,while
deferringthemajorityoftherequiredanalysistofuturework.TheMNIST-CNNSMisbasedona
CNNmodelOikarinen(2021)fortheMNISTdataset(LeCunetal.,1998).Withspecificchoicesof
themaximumlearningratemax_lr=10−2usedinOneCycleLR(SmithandTopin,2019)andthe
parameterβ = 0.7usedintheAdamoptimizer,thenetworkachieves99%accuracyonMNIST
1
injustoneepoch(≈1second),butperformancecandegradedramaticallyawayfromtheoptimal
choice.
The parameter space M of the MNIST-CNN dataset is a set of pairs (λ ,λ ) ∈ [0,1)2, where
0 1
λ = (log (max_lr)+3)/2,λ = −log (1−β )/4.ThegridM′ consistsofpointsλ,where
0 10 1 10 1
λ = k /64 for k = 0,...,63, j = 0,1. For each λ, a CNN is trained with the corresponding
j j j
parameters.Ineachrow(λ,x)ofthedatasetD ,thesamplex=(i,pˆ,y)contains(i)arandomly
train
chosenimageifromMNIST,(ii)theoutputclassprobabilitiespˆforimageireturnedbytheCNN
trainedwiththeparametersλ,(iii)thegroundtruthMNISTlabelyassignedtoi.
Eachofthe4096trainedCNNshastwosourcesofrandomness:theweightsarepseudorandomly
initialized,andthetrainingdatasetisapseudorandompermutationoftheMNISTtrainingdataset.
Giventhisrandomness,althoughwetrainedonlyonemodelforeachλ,weconceptualizethedataset
asrepresentingaSMwhereeachsamplexisobtainedfromarandomlyinitializedCNNevaluated
onarandomlychosenimage.Thedataset’ssampleshavethecorrectprobabilitiesP (x),butthe
λ
samplesxarenotindependent(makingthemindependentwouldrequiretraining140×4096instead
of4096CNNs).Unlikethephysicsdatasets,wedonotprovidethegroundtruthFIMbecause,inthe
MNIST-CNNcase,itisunclearhowtogenerateit.
6.3 Neuralnetworkdesignandimplementation
To implement ClassiFIM on the seven SMs, following the prescription given in Section 4.3, we
implementsixneuralnetwork(NN)architecturesreflectingtheformatoftheinputdata:oneforeach
SMwiththeexceptionofFIL24,whereweusethesameNNandtrainingpipelineasforHubbard12.
Forexample,theHubbard12NNarchitectureisdepictedinFig.2.Neuralnetworksforotherdatasets
mainlydifferinthetypeofconvolutionallayersused,numberoflayers,thenumberofchannels.
The type of convolutional layer used is chosen based on the structure of the data: e.g., 1D CNN
for1Ddata,2DCNNfor2Ddata,withpaddingmatchingtheboundaryconditionsofthesystem.
Other hyperparameters of NN architecture together with the number of epochs used during the
trainingareadjustedtomatchthetimelimitgiveninTable1.EachNNistrainedtominimizethe
cross-entropyerrorfortheauxiliarybinaryclassificationtask,andincludesasmallL regularization
2
onthenetworkweights.Thecodewiththeexactneuralnetworkarchitecture,trainingprocedure,and
learnedhyperparametersusedforeachSMisprovidedinKasatkin(2024).TheseNNarchitectures
andhyperparameterchoicesarebasedonearlyexperimentsonlower-qualitydatasetsnotdescribed
inthiswork.WeusetheAdamoptimizerKingmaandBa(2014)andaOneCycleLRlearningrate
schedulerSmithandTopin(2019).
7Table 2: Comparison of metrics for two ClassiFIM models trained on the XXZ300 datasets for
differentnumbersofepochs.Thevaluesinthetablearethemeanvaluesofthemetricsforthetwo
models,andthep-valuesareobtainedfromapairedt-test.IncolumnsCF200andCF580andin
Table3valuesinbracketsindicatethe(rounded)standarddeviation(acrossthe10datasets)inunits
corresponding to the last place of the value (omitted when (0)): e.g., 4.5(4) indicates a standard
deviationof≈0.4,and26.06indicatesastandarddeviationlessthan0.005.
Metric CF200 CF580 p-value
PeakRMSE 0.0080(7) 0.0078(5) 0.536
distMSE 0.78(17) 0.39(7) 2.2·10−4
distMSE 0.57(9) 0.36(4) 4.6·10−5
PS
distRE(%) 1.42(11) 1.27(7) 3.2·10−3
Table3:PerformanceofClassiFIM(CF)onphysicsdatasets.CE=cross-entropy.distREvalues
arein%.Thebestmethod(s)accordingtothePeakRMSEcolumnarehighlightedinbold(multiple
valuesarehighlightedwhenthedifferenceisnotstatisticallysignificant).MethodsinthePeakRMSE
partrefertotheirmodifiedversions(mod-ClassiFIM,mod-SPCA,andmod-W).
SM Method TestCE distMSE distMSE distRE Method PeakRMSE
PS
IsNNN CF 0.0755(2) 11(1) 4.6(9) 4.5(4) CF 0.017(4)
400 const 0.6931 — 55.6(2) 26.06 SPCA 0.016(1)
W 0.035(7)
Hubbard CF 0.250(3) 4.0(6) 1.5(1) 7.7(6) CF 0.035(7)
12 const 0.6931 — 6.2032 23.64 SPCA 0.039(1)
best 0.2380(9) 0.0000 0.0000 0.000 W 0.076(2)
FIL24 CF 0.2422(8) 0.47(3) 0.28(1) 4.4(1) CF 0.028(8)
const 0.6931 — 4.2078 25.60 SPCA 0.017(1)
best 0.2410(8) 0.0000 0.0000 0.000 W 0.026(4)
XXZ300 CF 0.0945(1) 0.39(7) 0.36(4) 1.27(7) CF 0.0078(5)
const 0.6931 — 41.307 24.55 SPCA 0.012(2)
W 0.011(3)
Kitaev20 CF 0.2086(7) 5.4(1) 0.71(2) 3.2(1)
const 0.6931 — 1.6873(6) 7.21
Ising400 CF 0.1745(4) 0.69(10) 0.55(6) 1.52(7)
const 0.6931 — 3.58(6) 10.26
6.4 Comparisonofmetrics
HereweevaluatehowwellthemetricsdistMSE,distMSE ,distRE,andPeakRMSEcandistin-
PS
guishbetweentheperformanceoftwosolutionstotheFIM-estimationtask.Inordertodothis,we
trainedClassiFIMontheXXZ300datasetsusingallavailabletime,whichwenameCF580since
wecouldtrainitfor580epochs,andcompareittoamodeltrainedfor200epochs,whichwename
CF200. Such a comparison is repeated for 10 datasets, allowing us to perform a paired t-test to
determineifthedifferenceintheperformanceofthetwomodelsisstatisticallysignificant.Asone
canseeinTable2,eachofthe3metricsdistMSE,distMSE ,anddistREisabletodeterminethat
PS
CF580isbetterthanCF200,butPeakRMSEcannot.Intuitively,thisisbecausePeakRMSEisbased
onlyonafewpropertiesoftheprediction(locationsof126predictedpeaks),whiletheothermetrics
takeintoaccountall3×642predictedvaluesgˆ (λ).Bydefinition,thethreemetricsareinvariant
µν
underlineartransformationsoftheparameterspace,whilePeakRMSEisnot.
6.5 ClassiFIMperformanceonphysicsdatasets
TheresultsofournumericalexperimentsonthephysicsdatasetsaresummarizedinTable3.This
tablecomprisessixsections,eachcorrespondingtooneSM,forwhichwehave10datasets.The
8Figure3: ClassiFIMoutput(left)comparedwithpropertiesoftheoutputsofthemodelstrainedon
MNIST.Fromlefttoright,theseare1−accuracy,1−Epˆ ,wherepˆ isthepredictedprobability
y y
ofthecorrectclass,andthemeanentropyofthepredictedclassprobabilities.Forthesethreeplots,
theaveragesarecomputedoverallimagesinthetrainingset,andover14trainedmodelsforeachλ.
Theaxescorrespondtomax_lr = 10−3+2λ0 andβ
1
= 1−10−4λ1.Thewhitecircleindicatesthe
≈99%accuracymodelfromOikarinen(2021).
“TestCE”columncomparesthehold-out(D )cross-entropyfortheauxiliarybinaryclassification
test
taskoftheClassiFIMmodelwehavetrainedwitha“const”model(returning0logodds)and,where
available,thebestpossiblemodelreturningthegroundtruthlogoddsforthatSM.ThedistMSE,
distMSE ,anddistREcolumnsshowthemetricsfortheFIM-estimationtask.Heretherowlabeled
PS
“const”correspondstopredictingtheconstantFIMcorrespondingtotheEuclideanmetriconthe
parameterspace,withanunknownscalingfactor(distMSEmetricisnotscale-invariantand,thus,is
undefinedforthe“const”method)and“best”correspondstopredictingthegroundtruthFIM.For
moredetailson“const”and“best”rowsseeAppendixF.WefindthattheClassiFIMmodelismore
preciseontheeasierFIL24SMeventhoughwereusedthesamearchitecture,hyperparameters,and
trainingprocedureaswereusedontheHubbard12SM.ApartfromthemetricsinTable3,thequality
oftheagreementbetweentheClassiFIMpredictionandthegroundtruthononeofthetenIsNNN400
datasetscanbeseenbycomparingpanels(f)and(g)ofFig.1orthetwocurvesinpanel(c).
ThelasttwocolumnsofTable3comparetheperformanceofClassiFIMwithotherstate-of-the-art
methods(allmodifiedasexplainedinSection5).Wenotethatallmethods,includingClassiFIM,
used,onaverage,126examplesperλ,whichisafixedamount(independentofthelatticesizeof
theunderlyingphysicalsystem),andissignificantlylessthanthenumberofprobabilitiesneeded
to compute the FIM directly from the definition (e.g., 2300 for XXZ300). In summary, from the
perspectiveofthismetric,mod-ClassiFIMoutperformsmod-Wandiscompetitivewithmod-SPCA.
6.6 UseofClassiFIMonMNIST-CNNdataset
WepresenttheresultsofusingClassiFIMonMNIST-CNNinFig.3.TheFIMphasediagramdetects
rapidchangesinthebehaviorofthemodelinthetopandbottomrightregions,whichcoincidewith
thelossofaccuracy.AregionwithmanysharppeaksintheFIMmayalsobeobservedinso-called
glassysystems(see,e.g.,Figs.3and4ofTakahashiandHukushima(2019)),forwhichitmaybe
possibletoobtainthegroundtruthandverifytheperformanceofClassiFIM.Fig.3alsosuggestsa
phasetransitionintheupper-leftcorner:astripedarkerthanthesurroundingareaindicatesapeak
inthepredictedFIM.Thisboundaryisnotcapturedbyaccuracyalone:thereisnosignofarapid
change in the accuracy plot (second panel from the left) at that location. However, one can find
otherquantitiesthatarecapableofcapturingthistransition:itcanbeseeninthemeanprobability
ofthecorrectclass,Epˆ (thirdpanel),and,moreclearly,inthemeanentropyofthepredictedclass
y
probabilities(lastpanel).SincewedidnotstudyClassiFIMintheglassyphase,wedonothavehigh
confidenceinhowtointerpretthepredictedhighvaluesshownintheClassiFIMoutputinFig.3,
andcannotexcludeapotentialmis-prediction.WeleavestudyingtheFIMandClassiFIMforphase
diagramshavingatransitiontoaglassyphaseasachallengingfuturedirection.Wenotethatwhile
therightthreepanelsofFig.3useaveragesoverthetrainsplitofMNIST,theplotsforthetestsplit
arequalitativelysimilar.
6.7 Scalability
Fordatasetsofsimilarsubjectivecomplexity(Ising400andIsNNN400are“easier”thanQPTdatasets
inthenextfourrows),thetimeallocated(asprescribedbycolumn“Time”inTable1)isroughly
9proportionaltothesizeofinputdata,i.e.,|D |nwherenisthenumberoffeaturesineachsample
train
x .Withthatscaling,ClassiFIMachievesbetterperformanceonlargerdatasetsofsimilarnature,
i
bothintermsofabsolutevaluesofdistRE(whichisinvariantnotonlytothescaleofthepredicted
FIM,butalsotothescaleofgroundtruth),andintermsofacomparisonwithothermethodsusing
PeakRMSE.Thissuggeststhatlinearscalingofcomputationalresourceswithinputsizesufficesfor
thetypesofdatasetsconsideredhere.
7 Limitationsandfuturework
WhilewehaveshownthatClassiFIMcansolvetheFIM-estimationtask,weexpectthatthereare
FIM-estimationtaskswhereClassiFIMoranyothermethodwouldfail.Moreover,alloftheneural
networksweimplementedinthisworkeffectivelyusedlocalfeatures,whichisincompatiblewith
solvingsomeotherwisesolvableFIM-estimationtasks.Nevertheless,ClassiFIMdoesnotrequireany
particularchoiceofNNarchitecture,andfutureworkcouldexploreotherchoicesandapplicationsof
ClassiFIMtootherproblems.SeeAppendixGforanextendeddiscussionofthesetopics.
References
S.-I.Amari. Naturalgradientworksefficientlyinlearning. Neuralcomputation,10(2):251–276,
1998. doi:https://doi.org/10.1162/089976698300017746.
S.-i.Amari. Informationgeometryanditsapplications,volume194. Springer,Japan,2016. doi:
10.1007/978-4-431-55978-8. URL https://link.springer.com/book/10.1007/
978-4-431-55978-8.
J.Arnold,N.Lörch,F.Holtorf,andF.Schäfer. Machinelearningphasetransitions:Connectionsto
thefisherinformation. arXivpreprintarXiv:2311.10710,2023.
C.Baldassi,C.Lauditi,E.M.Malatesta,R.Pacelli,G.Perugini,andR.Zecchina. Learningthrough
atypical phase transitions in overparameterized neural networks. Physical Review E, 106(1):
014116,2022.
N.N.Chencov. Statisticaldecisionrulesandoptimalinference. Number53.AmericanMathematical
Soc.(originallypublishedinRussian,Nauka,1972),2000. URLhttps://bookstore.ams.
org/mmono-53.
T.T.Duy,L.V.Nguyen,V.-D.Nguyen,N.L.Trung,andK.Abed-Meraim. Fisherinformationneural
estimation. In202230thEuropeanSignalProcessingConference(EUSIPCO),pages2111–2115.
IEEE,2022.
H.-Y.Huang,R.Kueng,G.Torlai,V.V.Albert,andJ.Preskill.Provablyefficientmachinelearningfor
quantummany-bodyproblems.Science,377(6613):eabk3333,2022.doi:10.1126/science.abk3333.
URLhttps://www.science.org/doi/abs/10.1126/science.abk3333.
T. Ishida, I. Yamane, T. Sakai, G. Niu, and M. Sugiyama. Do we need zero training loss after
achievingzerotrainingerror? InProceedingsofthe37thInternationalConferenceonMachine
Learning,ICML’20.JMLR.org,2020. URLhttps://dl.acm.org/doi/abs/10.5555/
3524938.3525366.
R.Karakida,S.Akaho,andS.-i.Amari. Universalstatisticsoffisherinformationindeepneural
networks:Meanfieldapproach. InThe22ndInternationalConferenceonArtificialIntelligence
andStatistics,pages1032–1041.PMLR,2019. URLhttp://proceedings.mlr.press/
v89/karakida19a/karakida19a.pdf.
V.Kasatkin. Classifim,2024. URLhttps://github.com/USCqserver/classifim.
V. Kasatkin, E. Mozgunov, N. Ezzell, and D. Lidar. Detecting Quantum and Classical Phase
TransitionsviaUnsupervisedMachineLearningoftheFisherInformationMetric,2024.
D.P.KingmaandJ.Ba.Adam:Amethodforstochasticoptimization.arXivpreprintarXiv:1412.6980,
2014. URLhttps://arxiv.org/abs/1412.6980.
10J.Kirkpatrick,R.Pascanu,N.Rabinowitz,J.Veness,G.Desjardins,A.A.Rusu,K.Milan,J.Quan,
T.Ramalho,A.Grabska-Barwinska,etal. Overcomingcatastrophicforgettinginneuralnetworks.
Proceedingsofthenationalacademyofsciences,114(13):3521–3526,2017. doi:https://doi.org/
10.1073/pnas.1611835114.
Y. LeCun, C. Cortes, and C. J. Burges. The mnist database of handwritten digits, 1998. URL
http://yann.lecun.com/exdb/mnist/.
J. Martens. New insights and perspectives on the natural gradient method. Journal of Machine
LearningResearch,21(146):1–76,2020. URLhttp://jmlr.org/papers/v21/17-678.
html.
T. Oikarinen. Training a nn to 99% accuracy on mnist in 0.76 seconds:
https://github.com/tuomaso/train_mnist_fast,2021. URLhttps://github.com/tuomaso/
train_mnist_fast.
M.J.Schervish. TheoryofStatistics. Springerseriesinstatistics.SpringerNewYorkNewYork,NY,
NewYork,NY,1995. ISBN9781461242505;1461242509. doi:10.1007/978-1-4612-4250-5.
L.N.SmithandN.Topin. Super-convergence:Veryfasttrainingofneuralnetworksusinglargelearn-
ingrates. InArtificialintelligenceandmachinelearningformulti-domainoperationsapplications,
volume11006,pages369–386.SPIE,2019. doi:https://doi.org/10.1117/12.2520589.
J. Takahashi and K. Hukushima. Phase transitions in quantum annealing of an np-hard problem
detectedbyfidelitysusceptibility. JournalofStatisticalMechanics:TheoryandExperiment,2019
(4):043102,2019.
C.Truong,L.Oudre,andN.Vayatis. Selectivereviewofofflinechangepointdetectionmethods.
SignalProcessing,167:107299,2020. doi:https://doi.org/10.1016/j.sigpro.2019.107299.
E. P. L. van Nieuwenburg, Y.-H. Liu, and S. D. Huber. Learning phase transitions by confusion.
NaturePhysics,13:435–439,2016. doi:10.1038/nphys4037. URLhttps://www.nature.
com/articles/nphys4037.
C. Zhang, S. Bengio, M. Hardt, B. Recht, and O. Vinyals. Understanding deep learning (still)
requires rethinking generalization. Communications of the ACM, 64(3):107–115, 2021. doi:
https://doi.org/10.1145/3446776.
11TechnicalAppendix
Here, we provide additional details in support of the main text. In Appendix A, we provide an
additional discussion of the Fisher information metric. In Appendix B we give an example of a
textualdescriptionaccompanyingD.InAppendixCweprovidecompletedefinitionsofdistMSE,
distMSE ,anddistRE.InAppendixDweprovideadditionaldetailsaboutClassiFIM:weprove
PS
Theorem 1 (theoretical justification of ClassiFIM), describe the design choices we made in our
implementation of ClassiFIM, and discuss an alternative algorithm for generation of D . In
train,BC
AppendixEwedescribeourimplementationofmod-W,mod-SPCA,andmod-ClassiFIManddiscuss
placeswerewedeviatedfromrulesweintroducedinSection5.InAppendixFwedefinethe“const”
and“best”rowsfromTable3anddiscussthenaivemethodforFIMestimation.Finally,inAppendixG
wediscussthelimitationsandfutureworkpreviouslyoutlinedinSection7.
OtherdetailsareprovidedinthecompanionpaperKasatkinetal.(2024).Forexample,thedefinitions
ofthephysics-inspiredstatisticalmanifoldsusedinourworkaredescribedin(Kasatkinetal.,2024,
Sec.IV.B.);thenumericaldetailsthatwentintogeneratingthegroundtruthFIMandsamplesforour
datasetsaredescribedin(Kasatkinetal.,2024,App.H).ThecodecanbefoundatKasatkin(2024a)
andthedataatKasatkin(2024b).
A BackgroundonFisherInformationMetric
In Section 2, Eq. (1) we introduced the Fisher Information Metric (FIM). In this section, for the
convenienceofthereader,wereproducetherigorousdefinitionoftheFisherInformationMetric
(FIM)andmotivationbehindit,asintroducedin(Kasatkinetal.,2024,AppB).Thisdefinitionis
usedinTheorem1.
Giventheformula
s (x;λ)=∂λ logP (x) (6)
µ µ λ
forthescorefunction,onemightthinkthatfourconditionsarenecessaryfortheFIMg (λ)tobe
µν
defined:(i)thespaceofsamplesshouldformadisjointunionofoneormoremanifolds;(ii)foreach
λtheprobabilitydistributionshouldbeabsolutelycontinuousoneachofthosemanifolds,sothat
the density function P (•) is well-defined; (iii) P (x) ̸= 0 for all x; (iv) P (x) is differentiable
λ λ λ
withrespecttoλforallx.However,amoregeneraltreatmentoftheFIMispossible,whichdoes
notrequireanyoftheseconditions.Infact,therearemultipleinequivalentchoicesforsuchamore
generaltreatment(theybecomeequivalentwhentheaboveconditionsaresatisfied),andweselectone
ofthembasedonourmainapplication.ThesechoicesarebasedonthefactthattheFIMcoincides
(under certain regularity conditions, up to a constant factor) with the first non-trivial term in the
Taylorexpansionofmultiplemeasuresofdistanceorsimilaritybetweenprobabilitydistributions,
includingtheKullback-Leiblerdivergence,Jensen-Shannondivergence,andBhattacharyyadistance
(Amari,2016,section3.5FisherInformation:TheUniqueInvariantMetric).Specifically,wepickthe
lastofthesemeasures,theBhattacharyyadistance,whichisalsoknownastheclassicalfidelityF in
c
thephysicsliteratureonquantumphasetransitions(butsomesourcesdefineF tobethesquareof
c
r.h.s.ofEq.(7)instead,seee.g.,(Wilde,2013,Def9.2.4onpage271)).
AssumethatwehaveamanifoldMofparametersλandameasurablespace(Ω,F),whereΩis
thesetofallpossiblesamples,andF isasigma-algebraonthatspace.Furthermore,assumethat
foreveryλ∈Mwehaveaprobabilitymeasureσ .Theclassicalfidelitybetweentwoprobability
λ
measuresσandσ′isdefinedas
(cid:90) √
F (σ,σ′)= dσdσ′. (7)
c
Ω
WenotethatF isalwayswell-defined,wheretheintegralonther.h.s.isdefinedusingtheRadon-
c
Nikodymderivativewithrespecttoanymeasurewithrespecttowhichbothσandσ′areabsolutely
continuous(e.g.,σ+σ′).WhenF (σ ,σ )hastheform
c λ0 λ0+δλ
F (σ ,σ )=1+
1(cid:88)
g (λ )δλ δλ +o(∥δλ∥2 ) (8)
c λ0 λ0+δλ
8
µν 0 µ ν
µ,ν
for some g (λ ), we say that the FIM is defined. In that case, we can always make g (λ )
µν 0 µν 0
symmetric with respect to the interchange of indices µ and ν by replacing it with (g (λ ) +
µν 0
g (λ ))/2.Then,suchasymmetricg (λ )istheFIM.
νµ 0 µν 0
12B DescriptionofHubbard12andFIL24datasets
AsmentionedinSection3,thedatasetsareaccompaniedbyanadditionaltextualdescription.Here
weprovidesuchadescriptionfortheHubbard12andFIL24datasets(thedescriptionforthese20
datasetsisidentical):
The twelve-site grid consists of 12 sites indexed by j ∈ {0,1,...,11}. There
are two types of edges: NN and NNN. Each site j is connected via NN edges
with 4 sites: (j ±1)mod12 and (j ±3)mod12, and via NNN edges with 4
sites:(j±2)mod12and(j±4)mod12.Eachsamplex isanintegerin[0,224)
i
representinga 24-bitbitstring (with 2 bitsper latticesite). Thebits of x corre-
spondingtositej
are(cid:4)
x
/2j(cid:5) mod2and(cid:4)
x
/212+j(cid:5) mod2.Thesetwobi
itsmay
i i
havedifferentmeanings.
AllNNedgesareidenticaltoeachother.AllNNNedgesareidenticaltoeachother.
Alllatticesitesareidenticaltoeachother.Inotherwords,anygraphautomorphism
ofthetwelve-sitegridwhichpreservesedgelabels“NN”and“NNN”(e.g.j (cid:55)→
(j+k)mod12orj (cid:55)→11−j)preservestheprobabilitydistributionoverbitstrings
x .
i
TextualdescriptionsforalldatasetsareavailableinKasatkin(2024b).Whileinthisworkwemanually
constructedNNarchitecturescorrespondingtosuchdescriptions,wehopethatfutureworkcould
automatethisprocess,e.g.,byusinganLLMtogenerateanNNarchitecturefromatextualdescription
ofthedataset.
C Metricsandbenchmarking
C.1 Performancemetrics
Letg(λ)denotetheground-truthFIMandgˆ(λ)denoteanestimate.Aseeminglyreasonableperfor-
mancemetricisgivenby
(cid:90)
distNaive(gˆ,g)= ∥g(λ)−gˆ(λ)∥2 dλ, (9)
F
M
where∥•∥ istheFrobeniusnorm(recallthatg(λ)isarank-2tensor).However,wearenotaware
F
ofanaturalgeometricinterpretationofdistNaiveandsuchametricisnotinvariantwithrespectto
parameterizationchangeinM.Anotherdisadvantageofthismetricisthatitmayassignalargeerror
toablurredestimatorgˆ whenthegroundtruthFIMhassharppeaks,totheextentofmakingitworse
1
thananestimatorgˆ missinghalfofthepeaks(phasetransitions)butotherwisematchingtheground
2
truth.Thisisinconsistentwiththegoalofqualitativelyreproducingaphasediagram.Toovercome
someoftheseissues,weproposethreemetrics:distMSE,distMSE ,anddistREwhichfavorgˆ
PS 1
overgˆ .
2
GivenaRiemannianmetric,anaturalquantitytocomputeisthedistancebetweentwopointsalongthe
shortestpathinthatmetric.However,thisapproachiscomputationallyexpensive,andthegeodesic
connectingtwopointsmaytranscendtheregionofinterestintheparameterspace.Asapractical
alternative,weinsteadconsiderthe“straightlinedistance”dist betweentwopointsλandλ′:
SL
(cid:90) 1 (cid:113)
dist (g;λ,λ′)= dt g(λ+t(λ′−λ);λ′−λ), (10)
SL
0
(cid:80)
whichweusetodefineourthreemetrics.Hereg(λ;v)= g(λ) v v isthesquarelengthofthe
ij ij i j
vectorv inthemetricg atthepointλ.Inournumericalexperiments,boththegroundtruthg and
thepredictiongˆarerepresentedbytheirvaluesonagridM′,inwhichcaseweassumethatthese
valuesrepresentapiecewiseconstantfunction.Hence,Eq.(10)isanintegralofapiecewiseconstant
functionandcanbewrittenasasum.Fora1Dparameterspace,dist forN pairs(λ,λ′)can
SL SL
becomputedintimeΘ(N +|M′|)usingdynamicprogramming.Forgridsinmulti-dimensional
SL
parameterspaces(withafixednumberofdimensions),thecostisO(N L)whereListhelargest
SL
griddimension.
13Thefirststepinthecomputationofall3metricsisthecomputationofdist (•;λ,λ′)foralargeset
SL
ofpairs(λ,λ′)ofdifferentpoints(e.g.,allpairsofpointsinthegridM′).ThefirstmetricistheMSE
fordist givenby,
SL
distMSE(gˆ,g)=mean λ̸=λ′∈M′(cid:0) [dist SL(gˆ;λ,λ′)−dist SL(g;λ,λ′)]2(cid:1) . (11)
Ascale-invariantversiondistMSE isdefinedbyoptimizingthescalingconstant:
PS
distMSE (gˆ,g)=min(distMSE(cgˆ,g)) (12)
PS
c>0
Finally, the third metric is the distance ranking error distRE representing the probability that
dist (gˆ;•,•) ranks two pairs of points differently from dist (g;•,•). The exact definition of
SL SL
distREisgivenbelow.Outofthethreemetrics,distMSEpenalizesforanymismatchbetweenthe
predictionandthegroundtruth,distMSE allowsfora(multiplicative)biaswhichmayappeardue
PS
toalackofaccuratecalibrationinotherwiseaccuratepredictions,anddistREisthemostlenient(it
couldbe0eveniferrorsotherthanoverallmultiplicativeerrorsoccurwhichdonotaffectdistance
rankings).
C.2 Definitionofthedistancerankingerror
Whenfollowingtheabove-givenrecipefordefiningdistRE,specialcareshouldbetakeninthecase
ofatie.Here,weprovidethecompletedefinition.Thedistancerankingerrorofgˆ(comparedtothe
groundtruthg)isdefinedastheaveragevaluereturnedfromthefollowingprocedure:
• Pickrandomλ ,λ ,λ ,λ ∈M′s.t.
0 1 2 3
– λ ̸=λ andλ ̸=λ ,and
0 1 2 3
– theset(i.e.,unorderedpair){λ ,λ }isdifferentfrom{λ ,λ }.
0 1 2 3
• Let aˆ,a ∈ {−1,0,1} be the results of a comparison of dist (gˆ;λ ,λ ) with
SL 0 1
dist (gˆ;λ ,λ )andofdist (g;λ ,λ )withdist (g;λ ,λ ),respectively:−1,0,1cor-
SL 2 3 SL 0 1 SL 2 3
respondto<,=,>,respectively.
• Ifa=0,return0.5.Otherwise,return|aˆ−a|/2(e.g.,0.5ifaˆ=0).
Notethatweintroducedaspecialcasefora=0toavoidthesituationinwhichamethodcandecrease
itserrorbyroundingthevaluesofgˆ(toincreasetheprobabilityofties).Thisisequivalenttoresolving
thetiesatrandom(withaprobabilityof0.5foreachoutcome).
ThemetricdistREthusdefinedissymmetricwithrespecttogˆandg,andisalwaysin[0,1].Usinga
merge-sortbasedinversioncountingalgorithm,distREcanbecomputedinΘ(N log|N |)time,
SL SL
whereN isthenumberofpairsforwhichdist wascomputedfor(e.g.,N =|M′|(M′−1)/2
SL SL SL
ifallvaluesofdist areused),whichistypicallylessexpensivethancomputingthosedist values
SL SL
inthefirstplace.
D ClassiFIMmethoddetails
D.1 Theoreticaljustification
ProofofTheorem1. First,notethatther.h.s.ofEq.(4)[g (λ ) = E (l∗ l∗ ),wherel∗ =
µν 0 x∼Pλ0 xµ xν x
M∗(λ ,0,x)]iswell-definedandissymmetricwithrespecttointerchangingµandν.Letusdefine
0
itsvaluealongsomevectorvasgˆ(λ ;v):
0
(cid:32) (cid:33)
(cid:88)
gˆ(λ ;v)=E l∗ l∗ v v . (13)
0 x∼Pλ0 xµ xν µ ν
µν
BasedonthedefinitionweusegivenbyEqs.(7)and(8),itissufficienttoshowthat,asϵ→0,
(cid:32) (cid:33)
8 (cid:88)(cid:112)
1− P (x)P (x) =gˆ(λ ;v)+o(1). (14)
ϵ2 λ0 λ0+vϵ 0
x
14Indeed, if we show this for an arbitrary fixed v, then we can write δλ from Eq. (8) as ϵv with
ϵ=∥δλ∥andv =δλ/ϵ.Then,itwouldremaintoshowthattheremaindertermo(ϵ2)inEq.(8)is
uniforminvwith∥v∥=1,whichisnecessarilythecasebecausetheunitsphereiscompactandall
termsarecontinuousinv.
First,letustrytoreplacethel.h.s.ofEq.(14),f (ϵ),withitssymmetricversionf(ϵ) = f (ϵ),
1/2 0
where
(cid:32) (cid:33)
8 (cid:88)(cid:113)
f (ϵ)= 1− P (x)P (x) . (15)
t ϵ2 λ0+vϵ(t−1/2) λ0+vϵ(t+1/2)
x
Letp (x)=P (x).Then,accordingtocondition3,wecanwrite
s λ0+vs
(cid:12)
∂ (cid:12)
p s(x)=p 0(x)+p˙ 0(x)s+δp s(x), where p˙ 0(x)= (cid:12) p s(x), δp s(x)=o(s). (16)
∂s(cid:12)
s=0
Accordingtocondition4,foranyxwithp (x) = 0,wehavep (x) = o(s2),i.e.,p˙ (x) = 0and
0 s 0
δp
(x)=o(s2).Since(cid:80)
p
(x)=1,wehave(cid:80)
p˙
(x)=0and(cid:80)
δp (x)=0.Byexpanding
s x s x 0 x s
Eq.(15)too(1)inϵusingp (x),p˙ (x),andδp (x),andapplyingtheirpropertieslistedabove,we
0 0 s
seethatsuchanexpansiondoesnotdependont.Therefore,itremainstoshowthat
f(ϵ)=gˆ(λ ;v)+o(1). (17)
0
Sinceweareonlyinterestedinthelimitϵ→0,wecanassumethatp (x)>0forallxwithp (x)>0
s 0
for|s| ≤ |ϵ|.Letλ = λ +vϵ/2,λ = λ −vϵ/2,p (x) = p (x),p (x) = p (x),and
+ 0 − 0 + 1/2 − −1/2
q(x)=(p (x)+p (x))/2.Thenwecanwrite
+ −
8 (cid:16) (cid:112) (cid:17)
f(ϵ)= 1−E p (x)p (x)/q2(x) +o(1), (18)
ϵ2 x∼q + −
whereo(1)absorbedtermswithq(x)=0.FromEq.(3)itfollowsthatforxwithq(x)>0wehave
p (x)p (x)/q2(x)=(cosh(ϵv·M∗(λ ,vϵ,x)/2))−2. (19)
+ − 0
SubstitutingEq.(19)intoEq.(18)weget
f(ϵ)= 8 E (cid:0) 1−cosh(ϵv·M∗(λ ,vϵ,x)/2)−1(cid:1) +o(1)
ϵ2 x∼q 0
=E (v·M∗(λ ,0,x))2+o(1)=gˆ(λ ;v)+o(1). (20)
x∼p0 0 0
D.2 ClassiFIMBCimplementationchoices
WesummarizethedesignchoiceswemadeinourownimplementationofaClassiFIMBC:
(a) Weuseaneuralnetworkasthetrainablefunctionl =M(λ ,δλ,x;θ)andminimizethe
0
cross-entropyerror.
(b) WeaddasmallL regularizationtothecross-entropycosttoavoidoverfitting.
2
(c) Wechooseaneuralnetworkarchitecturesuitableforourproblems(e.g.,foraperiodiclattice,
we use a CNN to respect translation symmetry) among other common sense, problem-
specificchoices.
(d) Wedonotusebatchnormalization(BN)orothernormalizationlayers.
Asidefromoutputtingavectorlasin(a),thechoicesin(a)–(c)arerelativelystandardformodern
state-of-the-artBCs.Ontheotherhand,mostmodernBCsuseBN,so(d)isnon-standard.Weremark
that(d)isknowntohelptraintonear-zerolossforclassificationaccuracy.Inourwork,BNmakesit
moredifficulttoregularizethemodelusingL regularization,whichwefindismoresuitableforour
2
usecasewhereaccurateestimatesoflog-odds(insteadofaccurateclassifications)areneeded.
15D.3 Fine-tuningofalgorithm1
In the actual implementation, we notice that D fits into the GPU memory for the datasets we
train
usedinournumericalexperiments.TooptimizeClassiFIMperformance,weuseaslightlydifferent
algorithm than Algorithm 1 to generate D . We first load D (not D ) into the GPU
train,BC train train,BC
memory. Then, before each epoch, we generate a version of the dataset D on the GPU as
train,BC
describedinAlgorithm2.Inourearlyexperiments,weobservedthatthisoptimizationimprovesboth
computationalperformance(insamplespersecondwetrainon)andgeneralization(becauseforeach
epochweuseadifferentD ,althoughitisgeneratedfromthesameD )whencomparedto
train,BC train
generatingasingleD withsizeN ≃10|D |.
train,BC BC train
WeapplythesameAlgorithm2toD totestthebinaryclassifieronthehold-outset(whichisused
test
forreportingthetestcross-entropy).
Algorithm2:GeneratingD fromD onGPUbeforeeachepoch
train,BC train
Input:datasetD
Output:D
BC
1: D +,D − ←tworandompermutationsofD
2: DiscardrowsifromD ±withλ +,i =λ −,i
3: D ←∅
BC
4: {Executethefollowinginparallelforeachi:}
5: fori∈range(len(D +))do
6: (λ +,x +)←row#ifromD +
7: (λ −,x −)←row#ifromD −
8: λ 0 ←(λ ++λ −)/2
9: δλ=λ +−λ −
10: D
BC
←D BC∪{(λ 0,δλ,x −,0),(λ 0,δλ,x +,1)}{thelastcomponentisthelabel}
11: endfor
12: returnD
BC
E Modificationsforcomparisonwithpriorwork
InSection5wedescribeourmethodologyforcomparingClassiFIMwithWandSPCAusingthe
PeakRMSEmetric.Inparticular,itrequiresustore-implementtheWandSPCAmethodsandto
post-process the outputs of all three methods so that the methods return the guesses {x }ns
s,k k=1
foreachslicesoftheparameterspace.Wedenotetheresultingimplementationsasmod-W,mod-
SPCA,andmod-ClassiFIM.Herethe“mod-”prefixindicatesourmodificationsincludingefficiency
improvementsandpost-processingtoobtaintheguesses,“W”standsfortheexpectedshapeofthe
accuracyplotinConfusionSchemeofvanNieuwenburgetal.(2016),and“SPCA”standsforthe
ShadowPrincipalComponentAnalysismethodofHuangetal.(2022).
WefirstdescribethemethodsandmodificationsindetailinAppendicesE.1toE.3andthendetail
in Appendix E.4 the instances where the rules from Section 5 were broken, explaining why and
discussingtheanticipatedimpact.
Somemodificationswedescribeimprovethecomputationalefficiencyofthemethod.Wenotethat
theseareimportanttoachieveabetterPeakRMSEbecausebettercomputationalefficiencyallowsus
tousealargernetworksizeandnumberofepochsinmod-Wandavoidsubsamplinginmod-SPCA
withinthecomputationalbudget,bothofwhichimprovethequalityoftheresultingpredictions.
E.1 Detailsofmod-W
TheConfusionSchememethodinRef.vanNieuwenburgetal.(2016)producesa“W”-likeshapewith
themiddlepeakpointingatthepredictedlocationofphasetransition(ora“V”-likeshapeindicating
nophasetransitionisfound).Themethod,asdescribed,onlyappliestosystemsparameterizedby
asingleparameterλ.Therefore,toapplyittodatasetswithtwoparameters,weapplyitseparately
to each horizontal and vertical slice of that dataset. Consider, for example, a 64×64 parameter
grid.Thenthereare128suchslices.Foreachhorizontalslice,λ isfixedandthemethodconsiders
1
63possiblevaluesofλ (possiblephasetransitionlocations,midpointsoftheoriginalgrid).For
0c
16Figure4:Illustrationofmod-W.Panel(a):AccuracyplotgeneratedbytheWmethodforhorizontal
slices,assembledintoa2Dphasediagram(identicaltoFig.1(d)).Accuracyvaluesbelow75%are
truncatedto75%andrepresentedindarkblue.Panel(b):Theplotafterpost-processing,showing
theremovalofspuriouslow-accuracypoints.Groundtruthandpredictedpeaksareoverlaidonthe
2Ddiagram.Eachhorizontalslicecontainsn′ blackcirclesrepresentingthe“inner”groundtruth
s
peaks,whicharethetargetsforpredictionbymethodslikemod-W.Additionaln −n′ lightsquares
s s
ineachslicerepresentgroundtruthpeakswhichareeithertooshallowortooclosetotheborder,
andtheirpredictionsarenotnecessaryinthePeakRMSEmetricgivenbyEq.(5),buttheircount
n −n′ representstheadditionalnumberofguesseseachmethodlikemod-Wisallowedtomake
s s
inthatslice.Then greencrossesineachsliceindicatetheseguessesmadebymod-W.Foreach
s
blackcircleineachslice,thereisacorrespondingcontributiontothePeakRMSEmetricequalto
thedistancetothenearestgreencrossinthatslice(ortheborderofthesliceifitiscloser).Panel
(c):AccuracyplotgeneratedbymethodWforverticalsliceswithinthesamecomputationalbudget.
Inmod-W,wethenpost-processitandpredictthepeaks(notshown,theprocessisthesameasfor
horizontalslices).Panel(d):Asinglehorizontalsliceatλ =18/64(markedonpanels(a)and(b)
1
withahorizontalline)withWplotbeforeandafterpost-processing,i.e.,accuracyasafunctionofλ .
0
Verticaldottedlinesmarkthegroundtruthpeaks.Forthisdatasetandslice,thepredictedpeaks(i.e.,
localmaximaofthemod-Wplot)alignperfectlywiththegroundtruth.Onpanels(a)–(c)theaxesare
λ (horizontal)andλ (vertical)rangingfrom0to1.
0 1
eachsuchlocation,aseparateneuralnetworkistrainedtopredict,givenasamplex,whetherxwas
sampledfromP(•|λ)withλ <λ orλ >λ .
0 0c 0 0c
Oneisthenexpectedtoplotclassificationaccuracyasafunctionofλ ,wherethepeaksindicatethe
0c
possiblelocationsofphasetransitions.Asoriginallydescribed,themethodwouldrequiretraining
128·63differentneuralnetworksforthe2Ddatasetsweuse.Thismeansthat,e.g.,fortheHubbard12
dataset, about 0.074 seconds out of a ten-minute budget is available for training a single neural
network,whichisproblematicduetotheoverheadsinvolved.Therefore,weoptimizedthetraining
procedure by combining all neural networks for a given slice into a single larger network: it is
mathematically equivalent to L−1 separate networks with identical architectures (but different
weights),butcomputationallymoreefficientwhentrainingusingPyTorch.HereListhenumberof
gridpointsintheslice:L=64forall2Ddatasetsweuse.Duringtraining,L−1separatenetworks
receivethesamesamplesx butdifferentlabelscorrespondingtodifferentvaluesofλ .Wetooka
i 0c
neuralnetworkarchitecturesimilartotheoneforClassiFIM,removingthepartsunnecessaryforthis
simplertask.Furthermore,wehadtoreducethenetworksizeandthenumberofepochssignificantly
tofitwithinthesamecomputationalbudget.WeusedhalfofD fortrainingandhalfforevaluation.
train
Thisiscontrarytoothermethodsthatusethewholedatasetfortrainingbutisnecessarytofollow
therulesfromSection5.Indeed,intheWmethod,predictedphasetransitionlocationsareextracted
fromtheplotofvalidationaccuracyasafunctionofλ ,whichrequiresavalidationsetseparate
0c
fromthetrainingset,buttherulesrequirethatonlyD isusedformakingsuchpredictions,thus
train
requiringustosplitD .Theresultingaccuracyplots(stitchedtogetherintoone2Dphasediagram,
train
withpurpleindicatingperfectaccuracyandblueindicating75%accuracy)aredisplayedinFig.4(a).
Inordertopost-processthepredictedaccuracyplots,observethatifNN istrainedtopredictwhether
j
λ <λ forj =1,2andtheresultingaccuraciesarea ,thenonewouldexpect
0 0cj j
a ≥a −|λ −λ | (21)
2 1 0c1 0c2
(assumingtheuniformdistributionofthevaluesofλ inthevalidationset).Yet,inpractice,this
0
constraintissometimesviolatedbecauseofimperfecttraining(ascanbeseenfromthebluepoints
inFig.1(d)ofthemaintext).ThismotivatesthefirsttwostepsofthepostprocessingweuseforW:
(1)padthelistofaccuraciesby1.0onbothsides;(2)resettheaccuracyvaluestothemaximumof
17ther.h.s.ofEq.(21).Thefinalpost-processingstepistofurtherreducethespuriouspointswithlow
accuracy:ifthearithmeticmeanofaccuraciesoftwoneighboringpointsisgreaterthantheaccuracy
atthecurrentpoint,weincreasethecurrentestimatedaccuracytothatvalue.
Finally, per the methodology of PeakRMSE (Kasatkin et al., 2024, Appendix L), for each slice,
we are given a number of peaks (i.e., the maximal number of guesses) n and need to re-
s
turn their predicted locations. We do this by extracting the highest prominence peaks using
scipy.signal.find_peaks.Whenthenumberofpeaksthusextractedislessthann ,weadd
s
additionalguessestosplitthelargestgapsasevenlyaspossible:e.g.,ifwearemissingonepeak,we
addaguessatthemidpointofthelargestgap.ThisprocessisillustratedinFig.1.
E.2 Detailsofmod-SPCA
Inallphysicsdatasets,bitstringsx ∈D areusedassamples.I.e.,itisnaturaltouseindividual
i train
bitsofx asfeatures.Thisisincontrastto,e.g.,MNIST-CNN,wherethefeaturesofx arenaturally
i i
interpretedasrealnumbersandnotindividualbits.(Kasatkinetal.,2024,AppJ)describedhowto
applySPCAtosuchbitstringdatasets.Specifically,weneedtoapplykernelPCAtothekernelgiven
by
K(λ,λ′)=exp(cid:32) τ (cid:88)Tλ (cid:88)T λ′ exp(cid:32) γ (cid:88)n I(cid:16)
x(t)
=x(t′)(cid:17)(cid:33)(cid:33)
. (22)
T λT λ′
t=1t′=1
n
i=1
λ,i λ′,i
Hereλ,λ′ aretwopointsintheparameterspace,T isthenumberofpairs(λ,x) ∈ D witha
λ train
givenλ,{x(t)}Tλ arethecorrespondingsamples,x(t) isthei-thbitofx(t) andI istheindicator
λ t=1 λ,i λ
function.Thekernelisparameterizedbytwohyperparameters:τ,γ >0.
Wepickτ = 1andγ = 1asinHuangetal.(2022).Wenotethat(Kasatkinetal.,2024,AppJ.3)
suggeststhatanalternativeinterpretationoftheoriginalSPCAmethodwhenappliedtobitstring
datasetsistopickτ =e−1andγ =1inEq.(22).However,wedidnottestthesebecausewearenot
awareofanymethodtotesttheperformanceofdifferentchoicesofhyperparametersinSPCAwithout
acomparisontothegroundtruth,andsuchacomparisonwouldviolatetherulessetinSection5.
ThisisincontrasttoWandClassiFIM,wherewecansplitavalidationsetoutofD andcompute
train
thetrainingaccuracyandthetraininglossrespectivelytoevaluatethechoiceofhyperparameters.
Ourfirstmodificationisefficiency.ThemostcomputationallyexpensivepartofSPCAistheevaluation
ofthekernelmatrixusingEq.(22),whichrequiresO(M2T2n)operations,whereM =|M′|isthe
numberofgridpointsintheparameterspace,T ≃135isthemaximalnumberofsamplesforasingle
gridpoint,andnisthenumberofbitsinasample.Anaiveimplementationwouldhavefivenested
loops.However,theinnerloopinEq.(22)canbecomputedefficientlyusingthebitwiseexclusiveor
(⊕,XOR)andthePOPCNTinstructionavailableonmodernCPUs.Thelattercountsthenumberof
bitssetto1ina64-bitinteger,typicallywithathroughputofoneinstructionpercycle(andlatencyof
∼3cycles),whichismuchfasterthanaloop.Thisassumesthatthebitstringsarestoredasintegers.
Weuse64-bitintegersinourimplementationandnotethatforlargernwecouldalsotakeadvantage
ofoneofSIMDinstructionsetsbutthiswasnotdoneinourimplementation.Forn≤64wehave
(cid:88)n I(cid:16) x(t) =x(t′)(cid:17) =n−POPCNT(x(t)⊕x(t′) ). (23)
λ,i λ′,i λ λ′
i=1
Forlargern,weused⌈n/64⌉suchPOPCNToperations.
TheresultoftheSPCAisillustratedinFig.1(e)ofthemaintext,wherethered,green,andbluecolor
channelscorrespondtothetopthreePCAcomponentsscaledto[0,1].Toextractthepeaklocations
fromahorizontalslice,wefirstapplysmoothingtothetoptwoPCAcomponents(alongλ ),which
0
is identical to the ClassiFIM post-processing explained below. Then, we create a list of features
byappendingamonotonicallyincreasing(withλ )featuretothelistofPCAcomponentsandrun
0
sklearn.cluster.KMeans,andadjusttheclusterssothattheyarecontiguousinλ .Cluster
0
boundariesarethenthereturnedpeaklocations.Whiletechnicallythetotaltimeoftenexceedsthe
timelimit,thisisbecausewedidnotattempttofurtherparallelizetheshadowmatrixcomputationor
portittoGPU.Forexample,runningthecomputationfortenHubbard12datasetsinparallelona
singlei7-6850KCPUresultsinatotalcomputationtimeofabout30minutes,i.e.,threeminutesper
dataset(lessthanthe10minutesbudget).
18Figure 5: Illustration of mod-ClassiFIM. Panel (a): The ClassiFIM prediction from Fig. 1(f).
Darker colors denote higher values of Tr(gˆ(λ)). To compute the intensity I of color channels
c
c ∈ {red,green,blue}, we selected three unit vectors v , each separated by 2π/3, and com-
c
(cid:112)
puted I = 1−max(1, gˆ(λ;v )/C) where C = 140 serves as a normalization constant and
c c
(cid:80)
gˆ(λ;v) = gˆ (λ)v v is the squared length of vector v as per the metric gˆ(λ). This color
µν µν µ ν
schemewasalsousedinpanels(f)and(g)ofFig.1.Panel(b):Semidiskillustratingthecolorscheme.
−→
ForeachpointAinthesemidisk,wedefineavectorv =OA(anexampleofsuchavectorisshown).
ThenthecolorofsuchapointArepresentsgdefinedusingg =C2v v .Forinstance,thecolorof
µν µ ν
pointOiswhitebecauseitcorrespondstov =0and,hence,g =0.Suchasemicircleonlyshows
thecolorscorrespondingtorank-1tensorsg:forexample,thecolorofg =C2I wouldbeblack(not
shown).Panel(c):gˆ ,themetriccomponenttobepostprocessedinthehorizontalslices.Panel
00
(d):Post-processedgˆ .Groundtruthandpredictedpeaksareoverlaidonthe2Ddiagramsimilarly
00
topanel(b)ofFig.4.Panel(e):Asinglehorizontalsliceatλ = 18/64(indicatedinpanels(b)
1
and(c)usingahorizontalline)showingtherawandpost-processedgˆ valuesalongsidetheground
00
truth.Verticaldottedlinesdenotethepositionsofgroundtruthpeaks.Nearbylocalmaximaofthe
post-processedgˆ arethepredictedpeaklocations.
00
E.3 Detailsofmod-ClassiFIM
TopredictpeaksforeachhorizontalslicewithClassiFIM,wetakethepredictedgˆ andapplythe
00
smoothingalongtheλ directionbyconvolvinggˆ withaGaussiankernel.Weusetwoversions
0 00
ofthekernelshiftedbyhalfofthelatticespacingtodoubletheresolutionoftheresultingfunction.
Similarly to mod-W, we extract the highest value peaks using scipy.signal.find_peaks.
ThisisillustratedinFig.5.Verticalslicesareprocessedanalogously.
E.4 PotentialdatasnoopinginevaluatingPeakRMSEmetric
GiventhelimitednumberofdatasetsandlowstatisticalpowerofthePeakRMSEmetric(asdemon-
stratedinSection6.4),itisimportanttoavoidintroducingbiasintotheresultofthecomparisonof
themod-W,mod-SPCA,andmod-ClassiFIMmethods.ThisiswhySection5introducedasetoffour
rulesonemightfollowtomakethecomparisonfair.
Strict adherence to these rules is nontrivial, and below we point out when a rule was broken, to
whatextent,andwhy.Thewholeprocessinvolvedtrialanderror,violatingrule(ii):thePeakRMSE
metricwasoriginallydesignedonlyforFIL24and,morespecifically,onlyforhorizontalslices.Inits
originalformulation,itsufferedfromissueslistedin(Kasatkinetal.,2024,App.L.1)resultinginthe
valueofthemetricbeingdominatedbyasingleslicewhereamethod’smistakecouldbeexplained
byasmallmisjudgmentinthelocationorheightofthepeak.Toavoidtheseissues,thedefinition
ofPeakRMSEwasadjustedinmultipleiterations,eachofwhichoccurredonlyafterweobserved
theresults.Initsoriginalform,italsohadaversionthatonlyconsideredsliceswithasinglephase
transition.ThiswasbecauseRef.vanNieuwenburgetal.(2016)onlyconsideredacaseofasingle
phasetransition.However,weobservedthattheperformanceofWwasnotsignificantlyaffectedby
whethertheslicehadoneormorephasetransitionsanddecidedtouseallavailableslices,which,in
theory,shouldimprovethestatisticalpowerofthemetric.
Thespecificmodificationsmadetoeachmethodweremadeonlyafterobservingtheoutputsofthe
methodsonourdatasets.Forexample,weobservedthattheactualaccuracycurvereturnedbytheW
methodoftendidnotreachvaluescloseto1atthelocationsofphasetransitions,motivatingustouse
thehighestprominencepeaks.However,suchhighestprominencewasaffectedbyafewpointswith
exceptionallylowaccuracy,whichmotivatedtheslope-cappingpost-processingstep.Theoriginal
19clusteringforSPCAoperatedonthetoptenPCAcomponents,butweobservedthatthisresulted
inworseclustersthanwhatonecanguessbyeyebylookingatthe2Dphasediagramwiththetop
threePCAcomponents,usedasthreecolorchannels,motivatingustoreducethenumberofPCA
componentstotwo.
ThedesignoftheoriginalNNarchitecturesinvolvedsometrialanderror.Mostofsuchtrialswere
doneonlowerqualitydatasetsnotusedinthefinalcomparison.Yet,somechoiceswerestillmade
whentrainingonthecurrentdatasets,butwithoutlookingatthegroundtruth.Insuchcases,thechoice
wasmadebasedonthetrainingloss(forClassiFIM)oraccuracy(forW)onthetrainingorvalidation
set(whichwassplitoutofD ).However,suchdecisionsweremadeonceandnotseparatelyfor
train
eachseed.Additionally,thetimespentonsuchtuningwasnotincludedinthecomputationalbudget.
F Comparisonmethods
InTable3,weshowtheperformanceofClassiFIMonphysicsdatasets.Therows“const”and“best”
are added as a comparison. In ClassiFIM, a binary classification model is used to provide the gˆ
estimates.Ontheotherhand,forthecomparisonrows,thebinaryclassificationpredictionsarenot
relatedtothepredictionsofg.Below,weprovidethedefinitionsofthesecomparisonrows.
F.1 “Best”
Forbinaryclassification,“best”producesthebestpossibleprobabilityestimatesequaltotheground
truthaposterioriprobabilitiesP(y =1|x,λ ,δλ)=P (x)/(P (x)+P (x)).Thestandard
0 λ+ λ+ λ−
deviationisnon-zerobecausedifferentrunshaddifferenttestsets.
The“bestpossible”classicalfidelitysusceptibilitypredictionsareequaltothegroundtruthg (λ)
µν
(contrarytoClassiFIM,theyarenotgeneratedforthegridpointsinM′butfortheshiftedgridM′′).
F.2 “Const”
For binary classification, “const” generates the best possible probability independent of x. This
probabilityisequalto0.5sincexhasthesamechanceofbeingsampledfromP asfromP .
λ+ λ−
The “const” classical fidelity susceptibility predictions are gˆ(λ) = α(dλ2 +dλ2) where α is a
0 1
positiveconstant.NotethatdistMSE anddistREarescale-invariant,hencetheirvaluesarenot
PS
affected by the choice of α. One could pick the value of α to minimize distMSE, in which case
distMSEwouldbeequaltodistMSE .
PS
Notefurtherthat“best”isnotreallya“method”onecouldapplytosolvetheFIM-estimationtask,
asinthesettingofthattaskonewouldnotknowthegroundtruthgortheprobabilitiesusedforthe
“best”row.Onewouldnotknowtheperfectcalibrationconstantαusedinthe“const”methodfor
thedistMSEmetriceither.Ontheotherhand,achievinga“const”distREanddistMSE scoreis
PS
easywithintherulesofthetaskbecausethesetwometricsarescale-invariant;henceonecanpick
anypositiveα.
F.3 NaiveapproachtoestimatingtheFIM
Consider,forexample,thatsamplesxarebitstringsoflengthn(asisthecaseinallphysicsdatasets
weuseinthiswork),andletS ={0,1}nbethesetofallpossiblesamples.Supposewehavejustone
parameterλandthedatasetcontainsN samplesforλ=λ −δ/2andN samplesforλ=λ +δ/2.
0 0
Thenwecouldtrytoestimateg(λ )as:
0
(cid:32) (cid:112) (cid:33)
gˆ(λ )= 8 (cid:16) 1−Fˆ (cid:17) = 8 1−(cid:88) N x,−N x,+ . (24)
0 δ2 c δ2 N
x∈S
HereN arethenumbersofsamplesequaltoxforλ=λ ±δ/2.Thatis,onewoulduseN /N
x,± 0 x,±
asestimatesoftheprobabilitiesofxatλ ±δ/2.
0
Toseeaproblemwiththisestimate,consideranexamplewhereallprobabilitydistributionsP are
λ
identical,i.e.,g(λ )=0andF (λ −δ/2,λ +δ/2)=1.Sincethenumberofpossiblebitstringsis
0 c 0 0
exponentialinthelengthofthebitstrings(e.g.,2400forIsNNN400),onemightexpectthattypically
20E(N )=E(N )=p N ≪1forallx,wherep =P (x)=P (x).Inthiscase,however,
x,− x,+ x x λ+ λ−
wehave
(cid:32)(cid:112) (cid:33)
E N x,−N x,+ = 1 (cid:16) E(cid:16)(cid:112) N (cid:17)(cid:17)2 =p2N(1+O(p N)). (25)
N N x,− x x
i.e.,ifε=max p N ≪1,then
x x
E(Fˆ )<ε+O(ε2). (26)
c
Thus,wemightexpectsuchanestimategˆ(λ )tobecloseto8/δ2andfarfromthetruevalue0.For
0
thisreason,wedidnotusethisnaiveapproachinourwork.
G Limitationsandfuturework
Section7outlinedthelimitationsofthisworkandsuggesteddirectionsforfuturework.Herewe
discussthesetopicsinmoredetail.
G.1 TheFIM-estimationtaskisnotalwayssolvable
Here we provide an example of an FIM-estimation task which, one might expect, would not be
solvablebyClassiFIMoranyotherefficientmethod.
Consideracryptographichashfunctionf andaSMwheretheonlyparameterisλ ∈ [−1,1]and
thecorrespondingprobabilitydistributionsP areoverbitstringsy =f(x),wherexisdistributed
λ
uniformlyamongbitstringswithx =v(λ)forsomefixedindexlandsomefixedfunctionvwith
l
values in {0,1} (that is, one bit of x is set to a fixed value dependent on λ, while all other bits
areuniformlyrandom).ThesamplesareprovidedaspartofD ,andthefulldescriptionofthe
train
generationprocedureincludingthedefinitionoffunctionf butexcludingthechoiceofvandlare
givenaspartofthetextualdescription.
IfanalgorithmcanestimatetheFIMefficiently,itcoulddistinguishbetweentheconstantfunction
v andafunctionexperiencingajumpatλ = 0.Thatis,thealgorithmwouldefficientlypredicta
propertyofthepre-imagesofthehashfunctionf,which,onemightexpect,isnotpossible(more
precisely,thisisnotpossibleundertherandomoraclemodelofcryptographichashfunctions).
G.2 Useoflocalfeatures
AtypicalarchitectureusedforClassiFIMNNswithinthisworkisillustratedinFig.2.Samplesx are
i
firstprocessedbyaCNN.TheoutputoftheCNNistheresultof(possiblymultiple)localconvolutions
andnon-linearities,whichcanresultinoutputvaluesdependingonlyonpartsofx correspondingto
i
asmallneighborhoodofthelatticesiteassociatedwiththeoutputvalue.Consequently,thesefeatures
areaveragedoveralllatticesitesandfedintoafullyconnectedlayer.Inotherwords,theoutputof
theNNcanbedescribedas
  
 1 (cid:88) 
g f α,j(x i) , (27)
N
 
j
α
whereN isthenumberofsites,j indexesthesites,andf (x )isthelocalfeature#αatsitej.
α,j i
ThisprecludestheNNfromcomputingfeaturesliketheparityofbitsinx alonga(longnon-local)
i
loopinthelattice,sincef (x )arelocalandusedbyapermutation-invariant(withrespecttoj)
α,j i
expressionEq.(27).
Iftwodistributionsareonlygloballydistinguishable,asinthecaseofsometopologicalquantum
phasetransitions(TQPTs,see,e.g.,Hammaetal.(2008);Wen(2017)),itispossiblethatourmethod
with such NN architectures would fail to estimate the underlying FIM, though, as we show in
(Kasatkinetal.,2024,Fig.6,10),ClassiFIMproducesaccurateFIMphasediagramsfortheXXZ
andKitaevmodels,bothofwhichareknowntoexhibitTQPTsKitaev(2003);Elbenetal.(2020).
Neverthless, this is not a limitation of ClassiFIM itself because ClassiFIM does not require any
particularuseofNNarchitecture,aswediscussbelow.
21G.3 OtherNNarchitectures
TheClassiFIMmethodisexplainedinSection4(steps1–3).Noneofthesestepsrequireanyparticular
NNarchitecture.Rather,someBCmodelisneededasexplainedinstep2.Withinthiswork,wechose
touseasimplearchitectureasshowninFig.2assuchBCmodel.Futureworkmayfindthatsome
other NN architecture would achieve better performance if used within ClassiFIM. For example,
onecouldtryapositionalencodingfortheλinput,possiblywithanalternativewayofencoding
theset{λ ,λ }:e.g.,insteadorinadditiontosupplyingδλδλT,onemaysupplythissetviaa
− +
(two-element) attention layer. One may also try non-neural network models as binary classifiers
withinClassiFIM.
WedidnotuseanynormalizationlayersinourNNarchitecturesbecausewedidnotobtaingood
performanceusingtheminourearlyexperiments(onsyntheticdatasetsnotreportedinthiswork).
Weconjecturethat,inthecontextofclassificationtasks,suchlayersaresignificantlymoreeffective
inthemodernregimewhenoneaimstomaximizethehold-outaccuracy,thanforproblemswhere
groundtruthprobabilitiesarefarfrom0or1andthoseprobabilitiesaredesired(e.g.,whenthegoal
istominimizethehold-outcross-entropyerror).Apossiblecauseforthiscouldbethatnormalization
layerschangetheroleofweightdecayinthepreviouslayers,makingtheregularizationharder.Future
workcouldinvestigatethishypothesis,provideatheoreticalexplanationforthisobservation,and
possiblyusesuchanexplanationtodesignbetterNNarchitecturesforthelattertypeofproblems.
G.4 Othertasks
FutureworkmayinvestigatewhetherClassiFIMcanachievegoodperformanceonotherproblems
(possiblywheretheformaljustificationinTheorem1doesnotapply).
InSection6.6,weconsideredthedetectionofqualitativechangesintheresultsofCNNtrainingas
hyperparametersarevaried.AnotherpotentiallymoreinterestingapplicationofClassiFIMarises
whenaBoltzmannmachineisusedinsteadofaCNN:onemaylookforaqualitativechangeinthe
distributionofstatesofalayeroftheBoltzmannmachineassomeparametersarevaried.Sucha
problemwouldbeanaturalapplicationoftheClassiFIMbecausesuchdistributionsarenaturally
interpretedasastatisticalmanifold.
OnecouldalsotrytouseClassiFIMforchange-pointdetectionandcompareitwithothermethods
(e.g.,AminikhanghahiandCook(2017);Truongetal.(2020)).
References
S.-i.Amari. Informationgeometryanditsapplications,volume194. Springer,Japan,2016. doi:
10.1007/978-4-431-55978-8. URL https://link.springer.com/book/10.1007/
978-4-431-55978-8.
S.AminikhanghahiandD.J.Cook. Asurveyofmethodsfortimeserieschangepointdetection.
Knowledgeandinformationsystems,51(2):339–367,2017. doi:10.1007/s10115-016-0987-z.
A.Elben,J.Yu,G.Zhu,M.Hafezi,F.Pollmann,P.Zoller,andB.Vermersch. Many-bodytopological
invariantsfromrandomizedmeasurementsinsyntheticquantummatter. ScienceAdvances,6(15):
eaaz3666,2020. doi:10.1126/sciadv.aaz3666.
A.Hamma,W.Zhang,S.Haas,andD.Lidar. Entanglement,fidelity,andtopologicalentropyina
quantumphasetransitiontotopologicalorder. PhysicalReviewB,77(15):155111,2008. doi:10.
1103/PhysRevB.77.155111. URLhttps://link.aps.org/doi/10.1103/PhysRevB.
77.155111.
H.-Y.Huang,R.Kueng,G.Torlai,V.V.Albert,andJ.Preskill.Provablyefficientmachinelearningfor
quantummany-bodyproblems.Science,377(6613):eabk3333,2022.doi:10.1126/science.abk3333.
URLhttps://www.science.org/doi/abs/10.1126/science.abk3333.
V.Kasatkin. Classifim,2024a. URLhttps://github.com/USCqserver/classifim.
V.Kasatkin.Publicdatasets,2024b.URLhttps://huggingface.co/datasets/fiktor/
FIM-Estimation.
22V. Kasatkin, E. Mozgunov, N. Ezzell, and D. Lidar. Detecting Quantum and Classical Phase
TransitionsviaUnsupervisedMachineLearningoftheFisherInformationMetric,2024.
A. Y. Kitaev. Fault-tolerant quantum computation by anyons. Annals of Physics, 303(1):
2–30, 2003. doi: http://dx.doi.org/10.1016/S0003-4916(02)00018-0. URL http://www.
sciencedirect.com/science/article/pii/S0003491602000180.
C.Truong,L.Oudre,andN.Vayatis. Selectivereviewofofflinechangepointdetectionmethods.
SignalProcessing,167:107299,2020. doi:https://doi.org/10.1016/j.sigpro.2019.107299.
E. P. L. van Nieuwenburg, Y.-H. Liu, and S. D. Huber. Learning phase transitions by confusion.
NaturePhysics,13:435–439,2016. doi:10.1038/nphys4037. URLhttps://www.nature.
com/articles/nphys4037.
X.-G.Wen. Colloquium:Zooofquantum-topologicalphasesofmatter. ReviewsofModernPhysics,
89(4):041004–,122017. doi:10.1103/RevModPhys.89.041004. URLhttps://link.aps.
org/doi/10.1103/RevModPhys.89.041004.
M.M.Wilde. Quantuminformationtheory. Cambridgeuniversitypress,2013. doi:https://doi.org/
10.1017/9781316809976.
23