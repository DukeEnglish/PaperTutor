EUROGRAPHICS2025/A.BousseauandA.Dai COMPUTERGRAPHICSforum
(GuestEditors) Volume44(2025),Number2
TextIM: Part-aware Interactive Motion Synthesis from Text
SiyuanFan1,2,3 andBoDu†1,2,3 andXiantaoCai‡1,2,3 andBoPeng1,2,3 andLonglingSun1,2,3
1NationalEngineeringResearchCenterforMultimediaSoftware,SchoolofComputerScience,WuhanUniversity,China
2InstituteofArtificialIntelligence,WuhanUniversity,China
3HubeiKeyLaboratoryofMultimediaandNetworkCommunicationEngineering,WuhanUniversity,China
Abstract
Inthiswork,weproposeTextIM,anovelframeworkforsynthesizingTEXT-drivenhumanInteractiveMotions,withafocuson
theprecisealignmentofpart-levelsemantics.Existingmethodsoftenoverlookthecriticalrolesofinteractivebodypartsand
failtoadequatelycaptureandalignpart-levelsemantics,resultingininaccuraciesandevenerroneousmovementoutcomes.To
addresstheseissues,TextIMutilizesadecoupledconditionaldiffusionframeworktoenhancethedetailedalignmentbetween
interactivemovementsandcorrespondingsemanticintentsfromtextualdescriptions.Ourapproachleverageslargelanguage
models,functioningasahumanbrain,toidentifyinteractinghumanbodypartsandtocomprehendinteractionsemanticsto
generatecomplicatedandsubtleinteractivemotion.Guidedbytherefinedmovementsoftheinteractingparts,TextIMfurther
extendsthesemovementsintoacoherentwhole-bodymotion.Wedesignaspatialcoherencemoduletocomplementtheentire
bodymovementswhilemaintainingconsistencyandharmonyacrossbodypartsusingapartgraphconvolutionalnetwork.For
trainingandevaluation,wecarefullyselectedandre-labeledinteractivemotionsfromHUMANML3Dtodevelopaspecialized
dataset.ExperimentalresultsdemonstratethatTextIMproducessemanticallyaccuratehumaninteractivemotions,significantly
enhancingtherealismandapplicabilityofsynthesizedinteractivemotionsindiversescenarios,evenincludinginteractionswith
deformableanddynamicallychangingobjects.
CCSConcepts
•Computingmethodologies → Animation;Proceduralanimation;
1. Introduction tions. These human interactive motions require sophisticated co-
ordinationofspecificbodyparts,posingsignificantchallengesin
The field of computer animation has increasingly embraced the
matchingtextsemanticswithdetailedinteractionmovementsofin-
challengeoftext-to-motionsynthesis,wherefaithfulandplausible
dividualparts.Ourgoalistoenhancepart-levelalignmentbetween
human movements are expected to be generated from descriptive
interactivemovementsandtheinteractionsemanticsinthetextde-
natural language. This task not only bridges a gap between nat-
scription,concentratingoninteractivebodyparts.
urallanguageunderstandingand3Dhumanmotionrepresentation
butalsoenhancesapplicationsinvirtualreality,gaming,anddigital Thiscapabilityiscrucialinapplicationssuchasvirtualreality,
contentcreation. advancedsimulations,andinteractivetrainingsystems,whereusers
interact with virtual environments or avatars that are not physi-
While significant advancements have been made in this field,
cally present. Human interactive motions involve complex move-
most existing text-to-motion synthesis methods mainly focus on
ments that require precise control of interactive body parts and
generatinghumanmotionsthatareisolatedfromthesurrounding
comprehensivecoordinationamongallbodyparts.Whileinterac-
environment.However, thedynamicnature ofmosthuman activ-
tionmovementsconstituteonlyaminorpartoftheoverallmotion
itiesintherealworldinvolvesinteractionswithobjects,environ-
sequenceintermsofdurationandspatialextent,theyremaincru-
ments, or other people. Meanwhile, no existing work has suffi-
cialforthesemanticallyaccuracyofthegenerationcondition.The
ciently addressed the challenge of generating motions that accu-
main challenge lies in accurately aligning the interactive seman-
ratelyalignwiththesemanticsofinteraction.Recognizingthisgap,
ticsdescribedintextwiththesubtleyetessentialinteractivemove-
ourresearchrefinesthetext-to-motiontasktospecificallyaddress
ments.
thegenerationofhumaninteractivemotionsfromtextualdescrip-
Currenttext-to-motionmethodsoftenfailtopreciselyalignde-
tailed movements and their respective segments of description in
† correspondingauthor the text. They commonly treat the human body as a uniform en-
‡ correspondingauthor tity,overlookingtheuniquecontributionsofindividualbodyparts
©2025Eurographics-TheEuropeanAssociation
forComputerGraphicsandJohnWiley&SonsLtd.
4202
guA
6
]VC.sc[
1v20330.8042:viXra2of9 SiyuanFan&BoDu&XiantaoCai&BoPeng&LonglingSun/TextIM:Part-awareInteractiveMotionSynthesisfromText
Figure1: TextIMgenerateshumaninteractivemotionswithpart-levelsemanticaccuracyfromtextualdescriptions.
indynamicinteractions.Thisoversightresultsinadependencyon parts along with the generated interactive parts motion, ensuring
datathatcanmisinterpretdirectivessuchas’apersonwipestheta- spatial consistency and coherence. This part-aware synthesis ap-
blewiththeirlefthand,’wherethemodelmighterroneouslygener- proachnotonlyalignsgeneratedmotionsmorecloselywiththese-
atearight-handedactionduetothepredominanceofright-handed manticsofthetextbutalsoenhancesthefidelityandrealismofthe
data. This misalignment derives from a traditional focus on gen- resulting sequences, ensuring that even minor interactive actions
eralcorrectnessratherthanadetailedanalysisofmotionandtext, arecapturedwiththenecessarydetailandprecision.
leading to a loss of detailed alignment between the text and the
motion.Consequently,themodelonlygeneratesexpectedmotion Totrainandevaluateourmethod,weselectandrelabelasmall-
sequenceswhengiventhetextthatcloselyresemblesorreplicates scalehumaninteractivemotiondatasetbasedonHUMANML3D,
the form and order of previously learned semantics. In scenarios consistingof1.5Ksequencesofhumanmotionsundercategories
wheresub-actionswithinthedataset’ssentencesappearinisolation ofdailyactivitiesandexercises.Tofulfillthemotioncategory,one-
orarerecombined,themodelfailstoaccuratelyinfertheintended thirdofthemotionsequencesarenon-interactivemotionsinvolving
actions. basichumanactions.Experimentalresultsdemonstratethatourap-
proachproduceshumaninteractivemotionswithpart-levelseman-
Insightofthis,weproposeTextIM,apart-awarehumaninterac- ticconsistencythatalignswiththetextualdescription.Wequanti-
tivemotionsynthesismodelthatprioritizesthesemanticaccuracy tativelyevaluateoursynthesizedsequencesonestablishedmetrics
ofbodypartsinvolvedininteractions.Specifically,TextIMlever- in the field of motion synthesis and interactive motion synthesis.
ageslargelanguagemodelsintheinteraction-awaremoduletoex- Further,weconductavisualperceptualstudyforhumanevaluation
tractandcomprehensivekeyinteractioninstructionsandemploysa comparedtotherelatedbaselinemethod.
decouplingapproachtogeneratetheinteractiveandnon-interactive
partstoensuretheaccuracyoftheinteraction.Thedecoupledap- Tosummarize,ourcontributionsareasfollows:
proachisdesignedtoparseandunderstandinteractionsatagranu-
larlevel,focusingonthespecificbodypartsmostrelevanttoeach
• WepresentTextIM,anoveldiffusion-basedapproachtogenerate
describedaction.Thisapproachalsoprovidesamoreflexiblegen-
semanticallyaccuratehumaninteractivemotion.
erationcapability,allowingforthesynthesisofmixedmotionin-
• TextIMleverageslargelanguagemodelstocomprehensiveinter-
volvingbothinteractiveandnon-interactivemovements,enabling
activemotiondescriptionandgeneratetheinteractivemotionin
interactionsinvolvingmultipleparts.Whileeffectiveatenforcing
adecoupledmannerthatenhancethedetailedalignmentbetween
interaction instruction, the decoupling approach sometimes leads
thetextandthemotion.
to coordination and spatial continuity issues among the various
• TextIMintroducesagraphstructuretothedecouplingapproach
body parts. To tackle these challenges, drawing inspiration from
tomaintaintheconsistencyandcoherenceofthegeneratedhu-
the action recognition framework ST-GCN, we introduce a spa-
man interactive motion sequence, providing enhanced control
tialcoherencemodulethatutilizesgraphstructuretoextractspatial
overtheentiremotionontopofpart-awaremovements.
features.Thesefeaturesco-guidethegenerationofnon-interactive
©2025Eurographics-TheEuropeanAssociation
forComputerGraphicsandJohnWiley&SonsLtd.SiyuanFan&BoDu&XiantaoCai&BoPeng&LonglingSun/TextIM:Part-awareInteractiveMotionSynthesisfromText 3of9
2. RelatedWork theformofactionclass-objectpairs[GDG∗23,LWLD24]arein-
volvedtothegenerationguidance.Recentapproachesexpandob-
2.1. Text-to-MotionGeneration
jectinformationsuchasgeometry[LCM∗23,PXW∗23],trajectory
Text-to-motion generation can be broadly divided into two cate- [LWL23] and contact [DD23,MCZT24] to advance HOI. How-
gories: action class guided motion synthesis and text instruction ever,comprehensivedatasetforHOIthatinvolveswhole-bodyhu-
guidedmotionsynthesis. manmotionsequencesalongwithcorrespondinghumaninforma-
tion and object information is still vacant. Existing HOI datasets
Early studies focus on generating human motion from lim-
[BXP∗22,LWL23]lacksfree-formlanguagedescriptionofhuman
ited action classes in text form. A substantial amount of works
intention.
on action2motion datasets has established to solve this task, in-
cludingUESTC[JXY∗18],NTU-RGB+D[LSP∗19],HumanAct12 Althoughtext-conditionedinteractivemotionsynthesisandHOI
[GZW∗20], and BABEL [PCA∗21], which have been instrumen- tasks operate within similar interactive scenarios, our research
tal in advancing this domain. Generative Adversarial Network pointsoutcertainchallengesinthetraditionalHOIapproach.HOI
(GAN)-basedapproaches[YZL∗20,DNL∗22]enhancethegener- studiesprimarilyfocusonthephysicalrealismofinteractionsbe-
ationability ofGANarchitecture withGraphConvolutional Net- tween humans and objects, emphasizing detailed object informa-
work (GCN) to generate accurate motion sequences. Variational tion such as geometries and contacts. Consequently, HOI meth-
Autoencoder (VAE)-based methods [GZW∗20,PBV21,LZLR22, odsmayfinditchallengingtohandleinteractionswithsoftorde-
LBWR22,LML23] leverage an encoder-decoder structure to ef- formableobjects,whoseshapesandcontactschangeduringthein-
fectivelygeneratemotionsequence,reflectingtheevolutioninthe teractionprocess.Incontrast,TextIMconcentratesonthesemantic
field’stechnicalapproach. consistencybetweenhumanmotionsandtextualdescriptionsfrom
ahuman-centricperspective,enablingmoreflexiblehandlingofa
However, the limited predefined action classes is constrained
widervarietyofinteractions.Furthermore,TextIMcanbeextended
to illuminate human action willing, leading to the exploration of
toimprovesemanticguidancewithintheHOIdomain,therebyin-
text instruction guided motion synthesis. KIT [PMA16] and HU-
MANML3D [GZZ∗22] are the two main dataset used in text in- creasingtheaccuracyandapplicabilityofinteraction-drivenmotion
synthesisacrossdiversescenarios.
structionguidedmotionsynthesis,consistingofpairedmotionse-
quences and corresponding text annotation. VAE-based methods
[GZZ∗22,PBV22,GZWC22,APBV22,ZZC∗23,LCL∗23]tendsto
3. Method
learn the mapping relation between textual descriptions and hu-
manmotioninajointembeddingspace.Additionally,integrating 3.1. Overview
thepretrainedCLIPmodel[RKH∗21]intothetext-to-motiongen-
Our proposed TextIM architecture is shown in Figure 2. Given a
eration process [TGH∗22,HZP∗22] introduces prior cross-modal conditionaltextualdescriptionc,ourgoalistogeneratehumanin-
knowledge, thereby enhancing semantic comprehension capabili- teractivemotionsequencesxthataresemanticallyaccurateatpart-
ties.Morerecentapproaches[TRG∗22,ZCP∗24,STKB23,CJL∗23,
level.Totacklethemisalignmentbetweenpart-leveltextsandmo-
YSI∗23,JCL∗24]adoptdiffusionmodelasthebackbonenetwork,
tions, TextIM employs a decoupled generation pipeline based on
which offers robust frameworks to accept multiple conditioning adiffusionframeworktodealwithhumaninteractivemotions.By
typeswithinthesamemodel. decomposingbothtextsandmotionsintofine-grainedrepresenta-
Despite these advances, the demands of generating interactive tions,TextIMleveragesatwo-modulesystemtofulfillthegener-
humanmotionsbasedondetailedtextualinstructionshavenotbeen ationofthefinalmotionsequence.Theinteraction-awaremodule
fullyexplored.Interactivemotionsynthesispresentsuniquechal- isdesignedtoencodetheinteractivedescriptionsaccurately,allow-
lenges,particularlyincapturingthesubtletiesofcontext-dependent ingfortheprecisegenerationoffine-grainedinteractivemotions.
interactionsthatgobeyondgeneralmotions.Thecomplexityofac- However, naively segmenting the motion into distinct body parts
curatelysynthesizinginteractivemotionrequiresadeepersemantic resultsinunnaturaloutcomesinthegeneratedmotionsequences.
understandingandamoregranularapproachtomotiongeneration. Consequently,weintroducethespatialcoherencemoduletoensure
coordination within the different body parts, adjusting the poten-
tialinconsistenciescausedbyindependentpart-wisemotiongener-
2.2. InteractionMotionGeneration ation.
Interaction motion generation mainly includes human-scene in-
Inthefollowing,wefirstpresentthemotionrepresentationand
teraction (HSI) task and human-object interaction (HOI) task.
thegeneraldiffusiongenerationmodelinSection3.1.Wethengive
Sincerecenthuman-sceneinteractionstudies[HCV∗21,WXX∗21,
the composition of the interaction-aware module in Section 3.2.
WRL∗22,MPKPM23] mostly involving navigation and obstacle
Last, we elucidate the details of the spatial coherence module in
avoidancegoal,wefocusonmorerelatedHOItask.
Section3.3.
EarlyHOIsynthesisworks[TCBT22,ZWZ∗22,KRG∗23]start
with reducing the distance between human and static objects
3.2. HumanMotionDiffusionModel
while maintaining a correct contacting pose. Subsequent studies
[WWZ∗22]extendhumaninteractiontodynamicobjects,yetthey Webeginourintroductionwithabriefoverviewofthefundamen-
remained limited to small objects and only considering interac- tals of the human motion generation task, motion representation,
tionsinvolvinghands.Meanwhile,humanintentionrepresentedin andtheconditionaldiffusionmodel.
©2025Eurographics-TheEuropeanAssociation
forComputerGraphicsandJohnWiley&SonsLtd.4of9 SiyuanFan&BoDu&XiantaoCai&BoPeng&LonglingSun/TextIM:Part-awareInteractiveMotionSynthesisfromText
Figure2: TextIMoverview.TextIMsynthesizeshumaninteractivemotionsinadecoupledmannertoalignpart-levelmotionswithtextual
semantics. Given a textual instruction, we employ LLM to extract interaction instructions and body parts to generate the corresponding
interactionmotion.Subsequently,weuseGCNtolearnspatialfeaturesbasedoninteractionmotiontoguidethegenerationofthefinalresult
alongwiththeinteractioninformation.
Humaninteractivemotionx∈RT×Dconsistsofasequencesof 3.3. Interaction-awareModule
human poses p, where T represents the number of frames in the
When textual instruction is given, fully and accurately compre-
motion sequence and D represents the dimensions characterizing
(cid:104) (cid:105) hendingthesemanticinformationiskeytogeneratingplausiblere-
thehumanpose.Eachhumanposep= ra,rl,rh,p,v,r,f includes sults.Inthissection,wewillintroduceourinteraction-awaremod-
theangularvelocityra,thelinearvelocitiesrl,theheightrhofroot uletofulfillsuchanobjective.
jointintheSMPLhumanskeletonstructureandtherelativeposi- Interaction semantic extraction. In human interactions, the
tionsp,therelativevelocitiesv,therelativerotationsrofeachjoint specificinteractivemotiontypicallyhasthenatureofbeingsmall-
comparedtotherootjointandthebinaryfeature f offootcontacts scaleanddetailedcomparedtotheentiremovement,yettheypos-
withtheground. sessthegreatestsignificance.Forinstance,intheactionofwalk-
ingthroughthecorridortopickupafile,theactofpickingoccurs
Diffusionmodelshowsextraordinaryabilityinmotiongenera-
overabrieftimeandwithinalimitedspatialscale,whileitcarries
tion tasks. In this paper, we leverage conditional diffusion model
significantlymoreimportancethanthemoreextendedandbroader
inaclassifier-freemannertogenerateinteractivemotionfromtext
movement of walking. Therefore, using text instructions directly
description.Intheforwardprocess,diffusionmodelgraduallyadds
astheconditionforadiffusionmodelcanleadtotheoversightof
noisetotherawmotiondatax usingaMarkovchain.
0 crucialinteractioninformation.Toaddressthis,weemployalarge
√
(cid:0) (cid:1) languagemodeltoidentifyandextractthecrucialcontentsofinter-
q(xt |x t−1)=N xt; αtx t−1,(1−αt)I (1)
actionsemanticsatthebodypartlevelfromtheoriginalinstruction.
wherext representsthemotionsequenceatthetthnoisestep,αt = Toleveragethesemanticunderstandingcapabilitiesoflargelan-
1−βt andβt representsavarianceschedule. guagemodels,weuseOpenAI’sGPT-3.5[CTJ∗21]toextractin-
teractioninstructionandcorrespondingbodypartsfromthegiven
Inthereverseprocess,conditionaldiffusionmodellearnstogen-
textual instruction. To be more specific, we structure the request
eratecleandatafrompureGaussiannoisext withconditionc.
prompt from four key aspects. (i) First, we clarify the objective
byspecifyingtheexpectedoutcome,withthepromptdirectlystat-
p θ(x t−1|xt,c)=N(x t−1;µ θ(xt,t,c),(1−αt)I) (2)
ing: "Identify all possible body parts involved in interacting with
anobjectfromthegivensentenceandextracttheexactphrasethat
where µ represents the learned mean. Instead of predicting the
θ
noiseϵt atthetthnoisestep,wefollowTelvetetal.[TRG∗22]and describestheiraction."(ii)Next,wegiveaprecisedefinitionofin-
teraction,asitsmeaningvariesunderdifferentcircumstances."An
predictthecleanmotionxˆ 0= f θ(xt,t,c),someanµ θisformulated
’interaction’involvesanypurposefulphysicalengagementwithan
as
object,suchasholding,touching,lifting,carrying,moving,manip-
µ θ(xt,t,c)= √1
αt
(cid:18) xt−√ 1β −t
α¯t
(cid:0) xt−√ α¯txˆ 0(cid:1)(cid:19) (3) u anla st win eg r, co hr ou is ci en sg ti oti an va on idy rw ea sy p. o" n( si eii s)M thao tre ao reve or, vw ere lypr fio nv ei -d ge rao ip nt eio dn oa rl
excessively coarse-grained. "If no body parts are interacting with
whereα¯t=∏t i=0αi.Themodelparametersθareoptimizedbymin- anobject,respondwith’none’.Choosebodypartsfrom:leftarm,
imizing∥xˆ −x ∥2. rightarm,leftleg,rightleg,torso,pelvis."(iv)Last,few-shotex-
0 0 2
©2025Eurographics-TheEuropeanAssociation
forComputerGraphicsandJohnWiley&SonsLtd.SiyuanFan&BoDu&XiantaoCai&BoPeng&LonglingSun/TextIM:Part-awareInteractiveMotionSynthesisfromText 5of9
amplesofquestion-answerpairsaregivenforguidance,alongwith generatedparts.Inthissection,weintroducethespatialcoherence
theactualquestion."Hereareexamplesofhowtoformatyourre- moduletoaccomplishtheentiregenerationprocesswhilemaintain-
sponses:examples.Provideyouranswerforthefollowingsentence: ingconsistencyandharmonythroughoutthefull-bodymovement.
[QUESTION]."
Spatialfeatureextraction.Similartotheinteractivebodyparts,
Based on the aforementioned prompt, the responses from theentirebodymotionisalsogeneratedwithinaconditionaldif-
GPT3.5 are usually appropriate as expected. However, to ensure fusionmodel.However,thegenerationoftheremainingmotionis
allresponsesmeetthestandard,formatcheckingprocessandcon- influencedbytheinteractivemotiongeneratedbytheinteraction-
tentamendmentprocessaredesignedtoavoidunnecessarycosts. aware module and must maintain spatial coherence with it. Con-
Inrarecases,responsesmayincludeunexpectedinteractionbody sequently,weextractspatialfeaturesusingaPartGraphConvolu-
partsorincorrectextractionofinteractioninstructions.Theinter- tional Network (Part-GCN) during the training process to ensure
activebodypartsintheresponseareprimarilyverifiedagainstthe spatialcoherenceandconsistencythroughoutthemotionsequence.
expectedbodypartchoices.Anyunexpectedbodypartsaretrans-
The graph G = (V,E) is constructed based on the motion
formedintocorrespondingoptionalpartsbasedonapredetermined
sequence x , encapsulating both temporal and spatial dynam-
0
mapping.ThepredeterminedmappingbasedonSMPLskeletalhi-
ics. The node set V = {v ti|t=1,...,T,i=1,...,N} contains
erarchy maps common body parts to the six optional parts. For
nodes for N joints across T frames, which collectively repre-
example, "hip, knee, ankle, foot" are mapped to "foot" and "col-
sent the human motion sequence. The feature map F(v ti) of
larbone, shoulder, elbow, wrist" are mapped to "arm" and so on.
each node consists of the joint’s rotation and position informa-
Subsequently,thetextformatoftheresponseischeckedtoenable (cid:8) (cid:9)
tioninthemotiondata.TheedgesetE= (v ti,v tj)|(i,j)∈C ∪
informationextractionforthefollowingpost-processes.Eachques- (cid:8) (cid:9)
(v ti,v t+1,i)|t=1,...,T,i=1,...,N , where C represents the
tionisgiventhreeattemptstoconformtotheexpectedanswer.If
anatomical connections between human joints. This set includes
theanswerconsistentlyfailstopasstheverification,itissettothe
edgesrepresentingbothintra-framejointconnectionsbasedonreal
default’none’type.
human anatomy and inter-frame temporal connections that illus-
interactivemotionsegmentation.Duringthetrainingandgen- tratethetrajectoryofjoints.
erating processes, interacting body parts and their corresponding FeatureaggregationwithinthePart-GCNutilizedforspatialfea-
interactioninstructionsareextractedfromtheinitialtextdescrip- tureextractionisformalizedasfollows:
tion.Byfocusingonpart-leveldecompositionofthetext,TextIM (cid:32) (cid:33)
k
enablestheprecisegenerationofinteraction-specificmotions,ad- F(l+1)=σ ∑A F(l)W(l) (4)
k k
dressing the challenge of aligning detailed text descriptions with k=1
the corresponding motion at a granular level. The decomposition
where F(l+1) represents the output feature matrix of layer l, σ is
approachensuresthateachbodypart’smovementisbothcontex-
thenon-linearactivationfunctionReLU,A istheadjacencyma-
tuallyrelevantandsemanticallyconsistent,significantlyenhancing k
(l)
thefidelityandrealismofthegeneratedinteractivemotions. trices,andW isthelearnableweightmatricescorrespondingto
k
each adjacency matrix at each layer l. Unlike traditional Graph
Since interactive parts in a whole-body motion are subtle and
ConvolutionalNetworks,Part-GCNleveragemulti-layeradjacency
minor,wedecouplethemfromtherestpartstobetterlearninter-
matricesthatrepresentpracticalconnectionrelationships,enabling
actionsemantics.Theextractedbodypartsareconvertedintoabi-
featureaggregationtailoredtothepartitionofhumanbodyparts.
narymaskm∈R263 accordingtothecompoundedmotionrepre-
Furthermore,learnableweightmatricesareinvolvedtoassessthe
sentationinHUMANML3D[GZZ∗22].Thebinarymaskmisthen
varyingimportanceofdifferentpartsandthescaleofconvolutional
appliedtotheoriginalwhole-bodymotionx∈RT×Dforthecorre-
layers.
spondinginteractionbodypartsx inter∈RT×B,whereBrepresents
the corresponding interaction parts dimension. This approach not The joint nodes are segmented into k subsets based on the hu-
onlyencouragesthemodeltofocusonfine-grainedinteractivemo- man skeletal structure and body part partitions, allowing for the
tionfeaturesduringthefirstgenerationstagebutalsosupportsthe aggregationofjointfeatureintra-partandinter-part.Asillustrated
inclusionofarbitraryinteractionpartswithinasinglemotion. inFigure3,thehumanbodyisnaturallydividedintofiveparts:left
arm,rightarm,torso,leftleg,andrightleg.Nodesfromdifferent
Thebinarypartmaskisprocessedthroughalearnablelinearpro-
body parts which directly connects to each other form the subset
jection, which is then utilized as guidance for generation. Mean-
forinter-partinformationupdating.
while, the interaction instructions are encoded through the pre-
Human motion sequences are processed through 2D convolu-
trained CLIP model as text instruction to serve as supplementary
tional layers to compress temporal dimension and through part
guidance alongside the masks. Together, the original textual in-
graph convolution layers to extract spatial features. After feature
struction,thetransformedbodypartmask,andtheencodedinterac-
aggression and information updating, joint nodes connectivity is
tioninstructionsguidethegenerationofinteractivemotionswithin
assessedbycalculatingcosinesimilarityamongnodefeaturevec-
theinteraction-awaremodule.
tors S=F(l) ·(F(l) )T, where F(l) = F(l) . This spatial
norm norm norm (cid:113)
∑(F(l))2
joint
3.4. SpatialCoherenceModule similarity feature is subsequently utilized to guide the generation
ofwhole-bodymotions,ensuringspatialcoherence.
Oncetheinteractivepartsofaninteractivemotionareconfirmed,
weproceedtocomplementtheentirebodymovementbasedonthe Interaction guided generation. To integrate the interaction
©2025Eurographics-TheEuropeanAssociation
forComputerGraphicsandJohnWiley&SonsLtd.6of9 SiyuanFan&BoDu&XiantaoCai&BoPeng&LonglingSun/TextIM:Part-awareInteractiveMotionSynthesisfromText
Table1:QuantitativeevaluationonrelabelledinteractiveHUMANML3Ddataset.
RPrecision
Method MultimodalDist↓ MPJPE↓ MPVPE↓ HandJPE↓ FootJPE↓
(top3)↑
MDM 0.203±0.012 6.818±0.075 1.296±0.019 7.367±0.081 2.450±0.029 1.294±0.021
TextIM 0.220±0.012 6.476±0.058 1.144±0.011 5.768±0.036 2.017±0.017 1.147±0.015
wordsegmentationprocedure.Tobroadenmotionrangeofmotions
andtoincludefundamentalactionsabsentfromtheavailableinter-
activemotionsequences,suchas"run"and"turnaround",weran-
domlyselectedanadditional500motionsandtheiroriginaltextan-
notationfromtheunselectedportionsofdailyactivitiesandsports
categorytoenrichourdataset.Allmotionsrepresenthumanmove-
mentacross22bodyjointsandhaveanaveragedurationof7sec-
onds,resultinginatotalmotionlengthofapproximately2hours.In
theend,ourdatasetcomprises14923Dhumaninteractivemotion
sequencesalongwithtextualdescriptionstotrainandevaluateour
Figure3: Multi-layeradjacencymatricesenablesintra-partand
method.
inter-part feature aggregation. The black loops show intra-part
connectionsofeachbodypartsandtheredloopshowstheinter-
partconnection. 4.2. 4.2EvaluationMetrics
We employ established metrics from both text-to-motion genera-
tiontaskandhuman-objectinteractiongenerationtasktoevaluate
body parts into the final result, we overwrite the corresponding
oursynthesizedresults.Specifically,weutilizeevaluationmetrics
interaction body parts onto the generated result in the spatial co-
from the text-to-motion generation task to evaluate the condition
herencemoduleaftereachdenoisingstep.Themodifiedgeneration
matchingquality,whilemetricsfromthehuman-objectinteraction
resultiscomposedoftheinteractivemotionandthenewlygener-
generationtaskareappliedtoevaluatethequalityofhumaninter-
atedmotionbasedonthebodypartmaskm,definedas:
activemotionresults.
xˆ′ =m⊙x +(1−m)⊙xˆ (5)
0 inter 0 First,weadaptedgeneralevaluationmetricsfromtext-to-motion
Wealsomodifytheoptimizationobjectiveto(cid:13) (cid:13)xˆ 0′−x 0(cid:13) (cid:13)2
2
todirect g ine tn ee rara ct ti io vn emta osk tis on[G syZ nZ t∗ h2 es2 i] s.to Gib ve et nte tr ha al tig hn umw ai nth inth tee rao cb tij ve ect miv oes tioo nf
theoptimizingfocustowardstheremainingpartsofthemotion.
synthesisprimarilyfocusesoninteractionmovements,wehavein-
Furthermore,tokeepconsistentwiththeinteraction-awaremod- creased the semantic significance of interaction by replacing the
ule,wemaintainsimilarconditiontypestoguidethemotiongen- standardtextembeddingusedinR-PrecisionandMM-distmetrics
erationinthespatialcoherencemodule.Thebodymaskm′ isre- withinteractiontextembedding.Furthermore,weomittheFIDand
versedfromtheoriginalmaskm′=1−m,representingtheoppo-
Multimodalitymetrics,astheycalculatetheentiremotionembed-
siteactivebodyparts.Thenon-interactioninstructions,whichre- dinganddonotreflectthecrucialyetminimalfeatureofinteractive
fertotheremainingpartsoftheoriginaltextthatarenotcovered motion.Instead,weleavemotionqualityevaluationtometricsin
bytheextractedinteractioninstructions,areusedasanadditional thehuman-objectinteractiongenerationtask.
conditiontoenhancethesemanticguidanceformotiongeneration.
Together,theoriginaltextualinstruction,thetransformedbodypart Asformetricsusedinhuman-objectinteractiongenerationtask
mask,theencodednon-interactioninstructions,andthespatialfea- [LWL23],weselectthemetricsthatbestfitourcurrenttask.These
tureguidethegenerationofthefinalresultmotionwithinthespatial includemeanper-jointpositionerror(MPJPE)andmeanper-joint
coherencemodule. velocity error (MPVPE). Additionally, we choose hand joint po-
sitionerror(HandJPE)andfootjointpositionerror(FootJPE),as
thesebodypartsaremostinvolvedininteractivemotions.
4. Experiments
4.1. 4.1Dataset
4.3. 4.3Results
Sincenodatasetexistsisdesignedfortext-driveninteractivemo-
Baselines.Ourworkfocusesongeneratinghumaninteractivemo-
tionsynthesis,weselected992interactivemotionsequencesfrom
tion from textual descriptions. Given that existing text-to-motion
HUMANML3Ddataset[GZZ∗22]underthecategoriesofdailyac-
methodshavedifferentfocusesandobjectives,weselectthepromi-
tivitiesandsports,basedontheoriginaltextdescriptionsandthe nentMDMmethod[TRG∗22]torepresentthesemethodsforour
animations of the motion. Subsequently, we manually annotated
comparativeanalysis.
eachmotionsequencewiththreetextinstructionsthatdescribethe
humanmotioninvariousways,rangingfromdetailedtosketchy. Quantitativeresults.Table1reportsthequantitativeresultson
These annotated captions are then processed through a standard therelabeledinteractionHUMANML3Ddataset.Comparedtothe
©2025Eurographics-TheEuropeanAssociation
forComputerGraphicsandJohnWiley&SonsLtd.SiyuanFan&BoDu&XiantaoCai&BoPeng&LonglingSun/TextIM:Part-awareInteractiveMotionSynthesisfromText 7of9
Figure 4: Semantic accuracy. TextIM accurately generates human motions based on detailed, part-level textual descriptions, ensuring
semanticprecisionininteractivemotiongeneration.Theexamplesillustratehowspecifyinganinteractivebodypartforthesameactioncan
leadtodifferentmotionoutcomes.
Figure5: Motioncombination.TextIMcombinesexistingmotionsintonewonesacrossbothtemporalandspatialdimensions,enhancing
thedataefficiencyofinteractivemotiongeneration.Theexamplesillustratehowmotionscanbecombinedbothtemporallyandspatiallyto
producevariedoutcomes.
baselinemethod,TextIMhasabetterperformanceonallmetrics, leg"showcasingitsrobustabilitytodistinguishandapplypart-level
indicatingitseffectivenessinsynthesishumaninteractivemotions. interactionsasdirectedbytheinputdescription.
Qualitativeresults.ToillustratetheperformanceofTextIM,we Figure 5 illustrates the capability of TextIM to automatically
presentqualitativeevaluationresultsthatdemonstrateitscapacity combineexistingmotionsfromthedatasetintonew,contextually
forfine-grainedsemanticaccuracy.AsshowninFigure4,thebase- relevantmotionsacrossspatialortemporaldimensions.Forexam-
linemethodMDMfailstofollowthetextualinstructionsspecifying ple,themotionlabeled"throwingaballandwaving"successfully
bodyparts,generatingsimilarresultsfortwodistinctinputs.Tex- integratesthesetwodistinctactionsbysequentiallyconnectingthe
tIMaccuratelyfollowsthesemanticnuancesofdescriptionsuchas "waving"movementsimmediatelyfollowingthe"throwing"action.
"cleanthetablewiththelefthand"and"kickaballwiththeright Meanwhile, the motion "cleaning the table while looking at the
©2025Eurographics-TheEuropeanAssociation
forComputerGraphicsandJohnWiley&SonsLtd.8of9 SiyuanFan&BoDu&XiantaoCai&BoPeng&LonglingSun/TextIM:Part-awareInteractiveMotionSynthesisfromText
watch" demonstrates simultaneous integration by combining "ta- interactioninstructionsandmotiongeneration.Thenaspatialco-
blecleaning"with"watchlooking,"executingbothactionsconcur- herencemoduleisemployedtoextractspatialfeaturesofinterac-
rently.Thedecompositionandrecombinationofinteractionseman- tive motion to maintain spatial coherence. We selected and rela-
ticsallowTextIMtoeffectivelymimicthenaturalformationofhu- belled interactive motions in HUMANML3D dataset for evalua-
man actions, showcasing its advanced capacity for understanding tion.TextIMachievesbetterquantitativeresultsontheinteraction
complexmotiondynamics. datasetcomparedtoprevioustext-to-motionwork,specificallyex-
cellinginpart-levelsemanticaccuracyandeffectivemotioncom-
binationabilityincomplexinteractivemotionsynthesis.Moreover,
4.4. 4.4Application our results can be directly applied to all kinds of objects, includ-
ingsoftobjectsanddeformableobjectsusingphysicalsimulation
WepresentapracticalapplicationofTextIM,enablinghumaninter-
environmentscomparedtohuman-objectinteractionapproaches.
actionsynthesiswithsoftobjectsanddeformableobjects.Objects
donothavetheirownmotionintentionsandmustbemanipulated Limitations and future work. Although TextIM introduces a
in accordance with human movement. In our approach, the mo- novel approach for generating interactive motion from a human-
tionsof objectsarenot directlygenerated butareinstead derived centricperspective,integratingobjectinformationmoredeeplyinto
throughthesimulatedinteractioneffectsalongsidethehumanmo- the generation process, especially for interactions with rigid ob-
tions.WeutilizethephysicalsimulationcapabilitiesofUnrealEn- jects, remains an area for future enhancement. Meanwhile, the
ginetofulfillourgoal.Byattachingaphysicallysimulatedobject vacancy of interactive motion datasets that include precise finger
tothegeneratedhumaninteractivemotionwithinsimulationsoft- movementsconstrainedthefidelityofinteractivemotionsynthesis.
ware,wecanaccuratelyreproducecomplexinteractiondynamics.
As illustrated in Figure 6, we show the interactions like "waving
References
a handkerchief" and "splashing water" which are challenging to
achievewithcurrentmethods.TheseexampleshighlightTextIM’s [APBV22] ATHANASIOU N., PETROVICH M., BLACK M. J., VAROL
unique ability to integrate human motion synthesis with physical G.: Teach:Temporalactioncompositionfor3dhumans. In2022In-
ternationalConferenceon3DVision(3DV)(2022),IEEE,pp.414–423.
objectinteractions,providingarobustdemonstrationofitspracti-
3
calapplicationsinscenarioswherehuman-centricinteractionsare
expected.
[BXP∗22] BHATNAGAR B. L., XIE X., PETROV I. A., SMINCHIS-
ESCU C., THEOBALT C., PONS-MOLL G.: Behave: Dataset and
methodfortrackinghumanobjectinteractions. InProceedingsofthe
IEEE/CVF Conference on Computer Vision and Pattern Recognition
(2022),pp.15935–15946.3
[CJL∗23] CHENX.,JIANGB.,LIUW.,HUANGZ.,FUB.,CHENT.,
YUG.: Executingyourcommandsviamotiondiffusioninlatentspace.
InProceedingsoftheIEEE/CVFConferenceonComputerVisionand
PatternRecognition(2023),pp.18000–18010.3
[CTJ∗21] CHEN M., TWOREK J., JUN H., YUAN Q., PINTO H. P.
D. O., KAPLAN J., EDWARDS H., BURDA Y., JOSEPH N., BROCK-
MAN G., ET AL.: Evaluatinglargelanguagemodelstrainedoncode.
arXivpreprintarXiv:2107.03374(2021).4
[DD23] DILLERC.,DAIA.: Cg-hoi:Contact-guided3dhuman-object
interactiongeneration.arXivpreprintarXiv:2311.16097(2023).3
[DNL∗22] DEGARDINB.,NEVESJ.,LOPESV.,BRITOJ.,YAGHOUBI
E.,PROENÇAH.: Generativeadversarialgraphconvolutionalnetworks
for human action synthesis. In Proceedings of the IEEE/CVF Winter
ConferenceonApplicationsofComputerVision(2022),pp.1150–1159.
3
[GDG∗23] GHOSH A., DABRAL R., GOLYANIK V., THEOBALT C.,
SLUSALLEK P.: Imos: Intent-driven full-body motion synthesis for
human-objectinteractions.InComputerGraphicsForum(2023),vol.42,
WileyOnlineLibrary,pp.1–12.3
Figure6: Human-objectinteractionresultsof"wavingahandker-
chief"and"closingthenewspaper". [GZW∗20] GUOC.,ZUOX.,WANGS.,ZOUS.,SUNQ.,DENGA.,
GONG M., CHENG L.: Action2motion:Conditionedgenerationof3d
humanmotions. InProceedingsofthe28thACMInternationalConfer-
enceonMultimedia(2020),pp.2021–2029.3
[GZWC22] GUOC.,ZUOX.,WANGS.,CHENGL.: Tm2t:Stochastic
andtokenizedmodelingforthereciprocalgenerationof3dhumanmo-
5. Conclusion tionsandtexts. InEuropeanConferenceonComputerVision(2022),
Springer,pp.580–597.3
In summary, we proposed TextIM for synthesizing human inter-
activemotionguidedbytextualdescriptionswithanemphasison
[GZZ∗22] GUOC.,ZOUS.,ZUOX.,WANGS.,JIW.,LIX.,CHENG
L.:Generatingdiverseandnatural3dhumanmotionsfromtext.InPro-
part-awareinteractionalignment.Specifically,weemployadecou- ceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
pled diffusion framework to prioritize precise alignment between Recognition(2022),pp.5152–5161.3,5,6
©2025Eurographics-TheEuropeanAssociation
forComputerGraphicsandJohnWiley&SonsLtd.SiyuanFan&BoDu&XiantaoCai&BoPeng&LonglingSun/TextIM:Part-awareInteractiveMotionSynthesisfromText 9of9
[HCV∗21] HASSANM.,CEYLAND.,VILLEGASR.,SAITOJ.,YANG N.,QUIROS-RAMIREZA.,BLACKM.J.:Babel:Bodies,actionandbe-
J.,ZHOUY.,BLACKM.J.: Stochasticscene-awaremotionprediction. haviorwithenglishlabels.InProceedingsoftheIEEE/CVFConference
InProceedingsoftheIEEE/CVFInternationalConferenceonComputer onComputerVisionandPatternRecognition(2021),pp.722–731.3
Vision(2021),pp.11374–11384.3
[PMA16] PLAPPERTM.,MANDERYC.,ASFOURT.: Thekitmotion-
[HZP∗22] HONG F., ZHANG M., PAN L., CAI Z., YANG L., LIU Z.: languagedataset.Bigdata4,4(2016),236–252.3
Avatarclip:Zero-shottext-drivengenerationandanimationof3davatars. [PXW∗23] PENGX.,XIEY.,WUZ.,JAMPANIV.,SUND.,JIANGH.:
arXivpreprintarXiv:2205.08535(2022).3
Hoi-diff: Text-driven synthesis of 3d human-object interactions using
[JCL∗24] JIANGB.,CHENX.,LIUW.,YUJ.,YUG.,CHENT.: Mo- diffusionmodels.arXivpreprintarXiv:2312.06553(2023).3
tiongpt:Humanmotionasaforeignlanguage. AdvancesinNeuralIn- [RKH∗21] RADFORDA.,KIMJ.W.,HALLACYC.,RAMESHA.,GOH
formationProcessingSystems36(2024).3 G.,AGARWALS.,SASTRYG.,ASKELLA.,MISHKINP.,CLARKJ.,
[JXY∗18] JIY.,XUF.,YANGY.,SHENF.,SHENH.T.,ZHENGW.- ET AL.: Learning transferable visual models from natural language
S.: Alarge-scalergb-ddatabaseforarbitrary-viewhumanactionrecog- supervision. InInternationalconferenceonmachinelearning(2021),
nition. In Proceedings of the 26th ACM international Conference on PMLR,pp.8748–8763.3
Multimedia(2018),pp.1510–1518.3 [STKB23] SHAFIR Y., TEVET G., KAPON R., BERMANO A. H.:
[KRG∗23] KULKARNIN.,REMPED.,GENOVAK.,KUNDUA.,JOHN- Human motion diffusion as a generative prior. arXiv preprint
SONJ.,FOUHEYD.,GUIBASL.:Nifty:Neuralobjectinteractionfields arXiv:2303.01418(2023).3
forguidedhumanmotionsynthesis. arXivpreprintarXiv:2307.07511 [TCBT22] TAHERIO.,CHOUTASV.,BLACKM.J.,TZIONASD.:Goal:
(2023).3 Generating 4d whole-body motion for hand-object grasping. In Pro-
ceedingsoftheIEEE/CVFConferenceonComputerVisionandPattern
[LBWR22] LUCAS T., BARADEL F., WEINZAEPFEL P., ROGEZ G.:
Posegpt:Quantization-based3dhumanmotiongenerationandforecast-
Recognition(2022),pp.13263–13273.3
ing. In European Conference on Computer Vision (2022), Springer, [TGH∗22] TEVET G., GORDON B., HERTZ A., BERMANO A. H.,
pp.417–435.3 COHEN-ORD.:Motionclip:Exposinghumanmotiongenerationtoclip
[LCL∗23] LINJ.,CHANGJ.,LIUL.,LIG.,LINL.,TIANQ.,CHEN space. InEuropeanConferenceonComputerVision(2022),Springer,
pp.358–374.3
C.-W.: Beingcomesfromnot-being:Open-vocabularytext-to-motion
generationwithwordlesstraining.InProceedingsoftheIEEE/CVFcon- [TRG∗22] TEVETG.,RAABS.,GORDONB.,SHAFIRY.,COHEN-OR
ferenceoncomputervisionandpatternrecognition(2023),pp.23222– D.,BERMANOA.H.: Humanmotiondiffusionmodel. arXivpreprint
23231.3 arXiv:2209.14916(2022).3,4,6
[LCM∗23] LI J., CLEGG A., MOTTAGHI R., WU J., PUIG X., LIU [WRL∗22] WANGJ.,RONGY.,LIUJ.,YANS.,LIND.,DAIB.: To-
C.K.: Controllablehuman-objectinteractionsynthesis. arXivpreprint wardsdiverseandnaturalscene-aware3dhumanmotionsynthesis. In
arXiv:2312.03913(2023).3 ProceedingsoftheIEEE/CVFConferenceonComputerVisionandPat-
ternRecognition(2022),pp.20460–20469.3
[LML23] LEET.,MOONG.,LEEK.M.:Multiact:Long-term3dhuman
motiongenerationfrommultipleactionlabels. InProceedingsofthe [WWZ∗22] WU Y., WANG J., ZHANG Y., ZHANG S., HILLIGES O.,
AAAI Conference on Artificial Intelligence (2023), vol. 37, pp. 1231– YU F., TANG S.: Saga:Stochasticwhole-bodygraspingwithcontact.
1239.3 InEuropeanConferenceonComputerVision(2022),Springer,pp.257–
274.3
[LSP∗19] LIUJ.,SHAHROUDYA.,PEREZM.,WANGG.,DUANL.-Y.,
KOTA.C.:Nturgb+d120:Alarge-scalebenchmarkfor3dhumanac- [WXX∗21] WANGJ.,XUH.,XUJ.,LIUS.,WANGX.: Synthesizing
tivityunderstanding.IEEEtransactionsonpatternanalysisandmachine long-term3dhumanmotionandinteractionin3dscenes.InProceedings
intelligence42,10(2019),2684–2701.3 oftheIEEE/CVFConferenceonComputerVisionandPatternRecogni-
tion(2021),pp.9401–9411.3
[LWL23] LIJ.,WUJ.,LIUC.K.:Objectmotionguidedhumanmotion
synthesis.ACMTransactionsonGraphics(TOG)42,6(2023),1–11.3, [YSI∗23] YUANY.,SONGJ.,IQBALU.,VAHDATA.,KAUTZJ.:Phys-
6 diff: Physics-guided human motion diffusion model. In Proceedings
oftheIEEE/CVFInternationalConferenceonComputerVision(2023),
[LWLD24] LIQ.,WANGJ.,LOYC.C.,DAIB.:Task-orientedhuman- pp.16010–16021.3
objectinteractionsgenerationwithimplicitneuralrepresentations. In
Proceedings of the IEEE/CVF Winter Conference on Applications of [YZL∗20] YUP.,ZHAOY.,LIC.,YUANJ.,CHENC.:Structure-aware
ComputerVision(2024),pp.3035–3044.3 human-actiongeneration. InComputerVision–ECCV2020:16thEuro-
peanConference,Glasgow,UK,August23–28,2020,Proceedings,Part
[LZLR22] LU Q., ZHANG Y., LU M., ROYCHOWDHURY V.: Action- XXX16(2020),Springer,pp.18–34.3
conditionedon-demandmotiongeneration. InProceedingsofthe30th
ACMInternationalConferenceonMultimedia(2022),pp.2249–2257.3 [ZCP∗24] ZHANGM.,CAIZ.,PANL.,HONGF.,GUOX.,YANGL.,
LIUZ.: Motiondiffuse:Text-drivenhumanmotiongenerationwithdif-
[MCZT24] MA S., CAO Q., ZHANG J., TAO D.: Contact-aware fusionmodel. IEEETransactionsonPatternAnalysisandMachineIn-
human motion generation from textual descriptions. arXiv preprint telligence(2024).3
arXiv:2403.15709(2024).3
[ZWZ∗22] ZHAO K., WANG S., ZHANG Y., BEELER T., TANG S.:
[MPKPM23] MIRA.,PUIGX.,KANAZAWAA.,PONS-MOLLG.:Gen- Compositionalhuman-sceneinteractionsynthesiswithsemanticcontrol.
erating continual human motion in diverse 3d scenes. arXiv preprint InEuropeanConferenceonComputerVision(2022),Springer,pp.311–
arXiv:2304.02061(2023).3 327.3
[PBV21] PETROVICHM.,BLACKM.J.,VAROLG.:Action-conditioned [ZZC∗23] ZHANG J., ZHANG Y., CUN X., HUANG S., ZHANG Y.,
3d human motion synthesis with transformer vae. In Proceedings of ZHAO H., LU H., SHEN X.: T2m-gpt: Generating human motion
the IEEE/CVF International Conference on Computer Vision (2021), fromtextualdescriptionswithdiscreterepresentations. arXivpreprint
pp.10985–10995.3 arXiv:2301.06052(2023).3
[PBV22] PETROVICHM.,BLACKM.J.,VAROLG.:Temos:Generating
diversehumanmotionsfromtextualdescriptions. InEuropeanConfer-
enceonComputerVision(2022),Springer,pp.480–497.3
[PCA∗21] PUNNAKKALA.R.,CHANDRASEKARANA.,ATHANASIOU
©2025Eurographics-TheEuropeanAssociation
forComputerGraphicsandJohnWiley&SonsLtd.