Training LLMs to Recognize Hedges in Spontaneous Narratives
AmieJ.Paige∗Ψ ,AdilSoubki∗(cid:1)y,JohnMurzaku∗(cid:1)y,OwenRambow(cid:220)y,SusanE.BrennanΨ
(cid:1)DepartmentofComputerScience,(cid:220)DepartmentofLinguistics,Ψ
DepartmentofPsychology
yInstituteforAdvancedComputationalScience,StonyBrookUniversity
∗
Theseauthorscontributedequallytothisstudy.
amie.paige@stonybrook.edu,{asoubki,jmurzaku}@cs.stonybrook.edu
Abstract quantities of text without ever learning to inter-
act. Transformer-basedchatprogramscangenerate
Hedges allow speakers to mark utterances paragraphs-worthoftextremarkablywellwithout
as provisional, whether to signal non-
modelingthecoordinationbetweenagents–butis
prototypicalityor“fuzziness”,toindicatealack
thisconversation?
ofcommitmenttoanutterance,toattributere-
Whether a sequence of prompts and responses
sponsibilityforastatementtosomeoneelse,to
inviteinputfromapartner,ortosoftencritical exchanged in a dialogue between an LLM agent
feedback in the service of face-management and a human counts as truly (rather than superfi-
needs. Herewefocusonhedgesinanexper- cially) “conversational” depends on how conver-
imentally parameterized corpus of 63 Road- sation is conceptualized. Conversation is often
runner cartoon narratives spontaneously pro-
presumedtobethepassingbackandforthofmes-
duced from memory by 21 speakers for co-
sages (a “message model”); but that does not ex-
presentaddressees,transcribedtotext(Galati
plainphenomenacommontospontaneousconver-
and Brennan, 2010). We created a gold stan-
sationsuchasincrementalturns,clarifications,and
dard of hedges annotated by human coders
(theRoadrunner-Hedgecorpus)andcompared repair. Here we conceptualize conversation as a
threeLLM-basedapproachesforhedgedetec- collaborativeprocessofgroundingmeanings(seek-
tion: fine-tuning BERT, and zero and few- ingandprovidingevidence)duringwhichtwoor
shot prompting with GPT-4o and LLaMA-3.
more partners signal, coordinate, and align their
Thebest-performingapproachwasafine-tuned
beliefs or cognitive states (Brennan, 2005; Clark
BERT model, followed by few-shot GPT-4o.
andWilkes-Gibbs,1986). Thisleadstoabroader
Afteranerroranalysisonthetopperforming
research agenda that we hope will push genera-
approaches,weusedanLLM-in-the-Loopap-
proachtoimprovethegoldstandardcoding,as tive AI to model phenomena such as a partner’s
wellastohighlightcasesinwhichhedgesare knowledge or theory of mind, mutual beliefs or
ambiguous in linguistically interesting ways commonground,aswellaswhentotakeinitiative
thatwillguidefutureresearch. Thisisthefirst inadialogue.
stepinourresearchprogramtotrainLLMsto
Themaincontributionsofthisworkinclude:
interpretandgeneratecollateralsignalsappro-
(i)Aftergroundingtheprojectinpsycholinguistic
priatelyandmeaningfullyinconversation.
theory(Section2)andrelatedwork(Section3),we
1 Introduction presenttheRoadrunner-HedgeCorpus(Section4),
acorpusofspontaneousface-to-facenarrativesan-
ThevirtuosityofLLMssuchasChatGPThasled notatedforhedging.1
sometotheimpressionthatAIalreadyconverses
(ii)Wedescribeasetofexperimentsonthiscorpus
(orwillsoonbeabletoconverse)aspeopledo. But
usingzero-shot,few-shot,andfine-tuningmethods
aslanguageusers,LLMsandhumansarequitedif-
onmodernLLMs(Section5).
ferent. Theunderlyingfoundationsforlearningby
(iii)Weperformadetailederroranalysispinpoint-
these distinct kinds of language users share little
ing where LLMs fail in detecting hedges (Sec-
in common: Humans learn as infants to interact
tion 6). With this analysis, we take an LLM-in-
withotherswellbeforetheylearntheirfirstwords,
the-Loopapproachtocorrectinggoldannotations,
andoncewordlearningbegins,theycanpickupa
reducingerrorsinourtopperformingsystems.
newwordinoneorjustafewexposures,whereas
LLMs are pre-trained on humanly unfathomable 1https://github.com/cogstates/hedging
4202
guA
6
]LC.sc[
1v91330.8042:viXraWeconcludewithadiscussionandimplications cies,longerlatencies,morerisingintonation,and
ofourresultsinSection7,limitationsandthefuture moreexpressionsofdoubtwhentheyreportedhav-
ofourworkinSection8, andafinalsummaryof ingalowfeelingofknowingaboutananswer. This
oursalientcontributionsinSection9. metacognitiveinformationwasconfirmedtobeac-
curate when compared to the ground truth in the
2 TheoreticalFoundationsfrom formoftheiranswertothesame(multiple-choice)
Psycholinguistics questionlater(SmithandClark,1993). Notonly
arehedgesinformativeascollateralsignalsabout
In conversation, people communicate not only
whataspeakerknows,buttheyareaccuratelyinter-
about the purpose or topic at hand, but they also
pretedassuchbylisteners(BrennanandWilliams,
communicatemeta-informationaboutwhatthey’re
1995).
saying within the context of interaction, or col-
Thathedgesfunctionasinteractionalsignalsin
lateral signals (Clark, 1996). Along with pro-
extended dialogue is evident from studies of ref-
viding evidence for grounding in conversation,
erential communication. Typically in such stud-
about whether a prior turn has been understood
ies, two partners who can’t see each other con-
asintended(ClarkandBrennan,1991),collateral
verse in order to arrange and rearrange duplicate
signals can also provide information about the
setsofobjectsinmatchingorders,withtheobjects
speaker’srelationshipwiththecontentoftheirmes-
needingtobedistinguishedfromsimilarobjectsor
sage—howconfidenttheyareinwhattheyaresay-
consistingofTangrams(abstractgeometricshapes
ing,whetheritisdifficulttorecallorexpress,and
unassociatedwithanyconventionalorlexicalized
whethertheywouldwelcomeinputfromtheirpart-
labels). Hedges are common in initial referring
ner. Inthisproject,wefocusonaparticularkind
expressions, where they tend to appear in wordy,
ofcollateralsignalusedforcoordination,hedges.
disfluent,andoftententativedescriptions,andthen
theydropoutinrepeatedreferringexpressionsonce
2.1 WhySpeakersHedge
partnershavereachedasharedconceptualization
Therehavebeenseveralproposalsforwhyspeakers forthatobject(markedbyentrainment,orre-using
hedge. Hedgeshavebeenclaimedtocharacterize thesameshortenedreferringexpression)(Brennan
powerless“feminine”language(Lakoff,1973)or andClark,1996;GalatiandBrennan,2021),asin
toserveapolitenessfunctionbyminimizingthreat this sequence of repeated references to the same
toapartner’s“face”(BrownandLevinson,1987); objectovermultiplerounds(adaptedfromBrennan
see also (Fraser, 2010). Hedges have also been andClark,1996,p. 1488):
thoughttoconveyacertain“fuzziness”ofcategory
Round1: “acar,sortofsilverypurplecolored”
membership when a speaker means to describe a
Round2: “purplishcargoingtotheleft”
non-prototypicalmemberofacategory(e.g.,apen-
. . .
guinbelongingtothebirdcategory;Lakoff,1975).
Round5: “thepurplecar”
Princeetal.(1982)suggestedthathedgesplaytwo
functions: First,tomakepropositionalcontentless Inanotherstudythatrequiredtriadsofstrangers
exact(approximators,e.g. “sortof”)andsecond,to toreachconsensuswhilerecallingtheeventsfroma
changetherelationshipaspeakerhastothecontent movieclipthattheyhadwatchedearlier,thespeak-
oftheirmessage(shieldhedges). Shieldhedgesare ersoftenhedgedtheircontributionstotheconversa-
furtherdividedintoplausibilityshieldsthatsignal tion,presumablytomarkalackofcertaintyabout
alackofcommitmenttothecontentofamessage anutteranceandanopennesstobeingcorrectedby
(“Ithinkhisfeetwereblue,”Princeetal.,1982,p. theirpartners(BrennanandOhaeri,1999). Forex-
5),andattributionshieldsthatassignresponsibility ample,fromatriadthatcommunicatedbyspeaking
for a message to a source other than the speaker face-to-face:
orwriterthemself(“Accordingtoherestimates...” Yeah, they were sitting around the fire-
Princeetal.,1982,p. 13). placeinthenight... sortoflikeabedtime
Severalexperimentalstudieshavedemonstrated storykindofthing
howhedgescanconveyspeakers’commitmentto Peoplewhodidthesametaskbytextingratherthan
whattheyaresaying. Forexample,inaquestion- speakingusedfewerwords,butstillhedged:
answering task, people trying to recall the an-
Weallagreeitwasawreathythingyon
swerstotriviaquestionsproducedmoredisfluen-
hisneck???2.2 HowListenersReacttoHedges hedge)vs. “Ithinkhe’llwin?”(ahedge). Previous
workfoundmanycasesoftokensthatcanserveas
Hedges convey meaningful information that can
hedgesaswellasnon-hedges,withsystematictests
affectlisteners’subsequentbehavior;ahandfulof
forcoderstouseinannotatingthemforgoldstan-
psychologicalstudieshavemeasuredtheimpacts
dards (Prokofieva and Hirschberg, 2014; Ulinski
of hedges on listeners. For example, children ex-
andHirschberg,2019;Ulinskietal.,2018).
posed to new words from a speaker who hedged
The coding of hedges is complicated by the
learned fewer novel words compared to children
fact that in spoken dialogue, they often co-occur
exposedtoaspeakerwhodidnothedge(Sabbagh
withspeechdisfluencies. Insomecontexts,itmay
andBaldwin,2001). Listenersratedutterancesas
be difficult to distinguish these two kinds of sig-
moreuncertainwhentheyincludedshieldhedges
nals (Prokofieva and Hirschberg, 2014), particu-
(e.g., “I think it was a mug”), and these ratings
larly since listeners can use disfluencies in much
wererelatedtospeakers’ratingsoftheirownuncer-
the same way they can use hedges to draw con-
tainty in identifying an image (Pogue and Tanen-
clusionsaboutthespeaker’smentalstate(Arnold
haus,2018). Moreover,addresseesinareferential
etal.,2003,2007)
communication task expended more effort while
grounding(theyproducedmorelow-confidencere- Astrongmotivationforcomputationalworkon
sponsessuchasclarificationquestions)todemon- hedging comes from work on computer-assisted
strateunderstandingwhenthespeaker’sdescription learning by Cassell and colleagues, specifically
hadcontainedahedge(Dahan,2023). tutoring dialogues (Abulimiti et al., 2023a,b;
Raphalen et al., 2022). Most similar to our work
Hedgesalsoinfluencewhichdetailsareretoldto
is Raphalen et al. (2022), where the authors pro-
anotherperson;inonestudy,hedgeddetailswere
poseamodelthatcombinesrule-basedclassifiers
less likely to be repeated to another addressee as
and machine learning models with interpretable
comparedtounhedgeddetails(LiuandFoxTree,
featuressuchasunigramandbigramcounts,part-
2012), although in the same study, hedged infor-
of-speech tags, and LIWC categories to identify
mationpresentedinastorywasmorelikelytobe
and classify hedge clauses. Our work differs in
rememberedbylisteners;thiswasthoughttostem
two major ways: first, our work operates on the
fromdeeperengagementwithhedgedinformation
tokenlevelratherthanontheclauselevel. Token
when it was first presented (Liu and Fox Tree,
levelclassificationmakespossibleatrulyend-to-
2012). Andintutoringdialogues,wherefaceman-
endapproach(classifyingallhedgeandnon-hedge
agement can be particularly important, students
tokensinutterances). Second,weincludeexperi-
were more successful at solving problems when
mentswithmodernLLMsandofferadetailederror
theirpeertutorsusedhedges(Madaioetal.,2017).
analysisintotheirmistakes;stemmingfromthiser-
3 RelatedComputationalWork roranalysis,weuseanLLM-in-the-Loopapproach
(Daietal.,2023)tocorrectinggoldstandardhedge
3.1 Hedging
codings.
Severalresearchprogramshaveexaminedhedges
and the criteria for coding them, with computa- 3.2 Belief
tionalgoalsthatincludeautomatichedgedetection.
Hedgingandthenotionofbelief(howcommitted
Hedging is domain-specific, meaning that their
thespeakeristothetruthofanevent)areclosely
forms and frequencies vary across corpora; they
related;hedgesareoftenusedbyspeakerstoindi-
are also context-specific, as they cannot be iden-
catealackofbelieforcommitmenttowardswhat
tified accurately simply by searching for strings
they say. Ulinski et al. (2018) improved belief
(ProkofievaandHirschberg,2014). Hedgesaredis-
classification using a hedge detector, yielding an
tributeddifferentlywithindifferentcorpora(ibid).
improvementforthenon-committedandreported
Hedgesareoftenambiguousanddifficulttocode
belieflabels.
intheabsenceofdialoguecontext. In“Ithinkit’s
a little odd,” I think is often a hedge, but might Corpora Severalcorporahavebeencreatedthat
not be when proffered in response to a question annotatetheauthor’sdegreeofbelief(Diabetal.,
(“So what do you think?”). Hedges in spoken ut- 2009; Prabhakaran et al., 2010; Lee et al., 2015;
terancesmaybedisambiguatedbystressandother Stanovsky et al., 2017; Rudinger et al., 2018;
intonationalcues,asin“I thinkhe’llwin!”(nota PouranBenVeysehetal.,2019;JianganddeMarn-HedgeType Example(s)
Like(notusedasasimile,verb,orcomparison) "andthenhelikewentoverby..."
Youknow(nottocommunicateanother’sknowledgeorasadiscoursemarker) "andyouknowashe’sfallingdown"
Just(notusedtomean"only") "hejustjoltsaway"
Approximators/Rounders "kindof","about"
Proxies(foradetailthespeakercannotorchoosesnottorecall) "thing,""whatever,""orsomething,""andeverything"
Morphemesuffixestocontentwords "circley,""springy"
Expressionsofdoubtattachedtoclaims;self-speech "Idon’tknow,""maybe,""Iguess,""what’sitcalled?"
Tagquestionsandtrymarkers "he’sstandingthere,right?"
Table1: Codingschemeusedtomarkhedgesincorpus.
effe, 2021). There are two corpora that further andBrennan,2010)andgestures(GalatiandBren-
annotatenestedbeliefsofthesourcesmentionedin nan,2014)weredrivenbybothspeakers’andad-
thetext: FactBank(SauríandPustejovsky,2009) dressees’knowledgestates–thatis,shortenedupon
and the Modal Dependency corpus (Yao et al., retellingthestorytothesameaddressee,butlength-
2021). eneduponretellingtoanewaddressee.
MachineLearningApproaches Modernneural Gold Standard Coding. The original corpus
methodsforbeliefdetectionincludeLSTMswith transcribed the spontaneous narratives in detail,
multi-task or single-task approaches (Rudinger includingspeakingturnsanddisfluencies(forde-
et al., 2018), using BERT representations along- tails, see Galati and Brennan, 2010), segmented
sideagraphconvolutionalneuralnetwork(Pouran intolinesbyinstallmentsthatcorrespondedtonar-
BenVeysehetal.,2019),orfine-tuningBERTwith rative elementss in the cartoons. We annotated
aspanself-attentionmechanismJianganddeMarn- hedges on the original Roadrunner corpus to cre-
effe(2021). Recentstate-of-the-artworkfindsthat ate the gold standard for hedge training and de-
fine-tuning RoBERTa (Murzaku et al., 2022) or tection(theRoadrunner-Hedgecorpus;seehttps:
fine-tuningFlan-T5(Murzakuetal.,2023)yields //github.com/cogstates/hedgingfortheanno-
thebestperformanceonmostcorpora. Forthelabel tationcodebook).
Underspecified (or, corresponding to no commit- TheRoadrunner-Hedgecorpusisdistributedas
mentand/orahedge),thesemodernmethodsyield acsvfile. Itisstructuredasatotalof5,508lines,
f-measures in the low to high 80s. We also have overa quarter of which (N=1424) includeone or
prior work exploring multi-modal approaches to more hedges. The first author annotated hedges
beliefdetection(Murzakuetal.,2024). inthecorpusasinTable1. Althoughdisfluencies
suchasfillers(uh,um)andre-startscanfunctionas
4 TheRoadrunner-HedgeCorpus hedges,wemadeaprincipleddecisiontonotcode
themassuch;hedgesinourcorpusarepresumed
For training and testing, we obtained a corpus
to be shaped by the speaker’s intention, whereas
(Galati and Brennan, 2010) of spontaneous nar-
disfluenciesarenotnecessarilyunderaspeaker’s
rativesproducedfrommemoryby20speakerswho
controlasacommunicativesignal,butmayreflect
hadwatchedaRoadrunnercartoon. Eachspeaker
difficultiesinspeaking(Grice,1957;Clark,1994).
narrated the story face-to-face to an audience, a
Overall word counts for hedges and non-hedges
total of three times: first to a naïve addressee, a
are 1,728 and 38,018 words respectively. Most
second time to the same addressee, and a third
hedgesareoneword,butafewcasescontainmany
timetoanewnaïveaddressee(withthelattertwo
words. Foreachlineinthecsvfile(corresponding
episodes counterbalanced for order). The origi-
toanarrativeelement),hedgesarelisted(separated
nalexperimentwasdesignedtodetectdifferences
bycommas)inanadjacentcell. Eachlinehasan
in collateral signals (intelligibility vs. attenua-
averageof0.33hedges.
tion of speech and gestures) stemming from the
speaker’svs. theaddressee’sknowledgestates–that Inter-RaterReliability. Tocomputeinter-rater
is,whetherthestorywasnewforthespeaker(told reliability, a trained research assistant coded 7
for the first time) vs. old (retold), compared to randomly-selectedtranscriptswithnooverlapping
the addressee’s knowledge state (new vs. heard speakers (10% of the corpus). We calculated Co-
for the second time). Findings included that the hen’s Kappa from each word marked as a hedge
attenuation of both referring expressions (Galati withineachtranscript. Therewashighagreementbetweencoders,withκ = 0.985. I representstheinsideofahedgespan,andOrep-
resents another token, all separated by “/”. For
Corpus Analysis. The Roadrunner-Hedge cor-
example, given the utterance It is like warm, we
pus,likethetutoringdialoguesusedbyAbulimiti
prompted the model to generate It/O is/O like/B
et al. (2023b); Raphalen et al. (2022), has fewer
warm/O.
cases with hedges than without, but with more
We provide our exact prompts with their cor-
hedges per segment overall (25.85% of lines vs.
responding instructions in Appendix A. For our
14.26%ofturnsrespectively).
GPT-4oexperiments,weusedthedefaultOpenAI
Overthethreeversionsofthecartoonstorypro-
APIhyperparametersandatemperatureof1.0.
ducedbyeachspeaker,hedgesweremostfrequent
inthefirsttellingwhenthestorywasnewtoboth 5.3 Fine-tuning
speakerandaddresseeandleastfrequentwhentold
We performed all fine-tuning experiments using
to the same addressee a second time, consistent
BERT(Devlinetal.,2019),specificallybert-base-
withtheoriginalfindingsfromGalatiandBrennan
uncased. We also performed experiments with
thatcollateralsignalsareaffectedbytheknowledge
thelargevariantsofthemodel(bert-large),newer
statesofbothspeakerandaddressee.
encoder-only models like RoBERTa (Liu et al.,
2019) and DeBERTa-v3 (He et al., 2021), and
5 Experiments
encoder-decodermodelslikeFlan-T5(Chungetal.,
5.1 ExperimentalSetup 2022), but got either worse or closely similar re-
In this section, we present our hedge classifica- sults.
tionexperimentsontheRoadrunner-Hedgecorpus,
Task Description All experiments followed a
conducted by fine-tuning BERT and performing
standardBIOtokenlabellingapproachtoclassify
zero-shotandfew-shotexperimentswithstate-of-
hedgetokens(B),tokensinsideofhedgespans(I),
the-artLLMs. Forallexperiments,weperformed
andallothertokens(O).Inotherwords,givenanin-
five-fold cross validation using a fixed seed (42),
pututteranceofntokens,therespectiveBIOlabels
splittingthecorpusintoa80/20train/testsplit. For
were output for each of the n tokens. Following
our fine-tuning experiments, we did not perform
thesameexampleasdescribedinourzero-shotand
any hyperparameter tuning, and therefore do not
few-shotexperimentsinSection5.2,wefine-tuned
haveavalidationset.
BERT to classify the tokens as It/O is/O like/B
Weperformedallzero-shot,few-shot,andfine-
warm/O.
tuningexperimentsonthefold’srespectivetestsets
andreporttheaverageandstandarddeviationover Hyperparameters Wefollowedastandardfine-
allfivefoldstestsetsforF1,precision,andrecall. tuningapproach,fine-tuningforafixed5epochs.
Wesetthebatchsizeto16andlearningrateto2e-5.
5.2 ZeroShotandFewShot
Weperformedfive-foldcrossvalidationandteston
For the zero-shot and few-shot experiments, we eachfoldsrespectivetestset. Wedidnotperform
usedGPT-4o(OpenAI,2024)andLLaMA-3-8B- anyhyperparametertuning.
Instruct(AI@Meta,2024),asthesetwoLLMshave
5.4 Results
achievedstate-of-the-artresultsinmanyzero-shot
orfew-shotbenchmarktasks. TheperformanceofthemodelsisshowninTable
Weconductedtwoclassesofzero-shotandfew- 2,whichreportsaverageprecision(P),recall(R),
shot experiments: count/list generation and BIO andF1overthefive-folds. Forourzero-shot,few-
tag generation. Both prompts began with an in- shot,andfine-tuningexperiments,thesemetricsare
structiondetailingthespecifictask,andarandom calculatedoneachfold’stestsetandthenaveraged.
example. In our few-shot experiments, we pro- Despiteitsmuchsmallerparametercount,BERT
videdthreefixedhand-craftedexamples. Forour fine-tuned for BIO tagging outperforms even the
count/listgeneration,wepromptedthemodelsto best scoring prompting approaches by nearly 20
listtheintegernumberofhedgespresentintheut- pointsinF-measure. Thisisconsistentwithagen-
teranceandthengeneratedalistoftheexacthedge eraltrendintheliteratureofmoreparametereffi-
words. ForourBIOtaggeneration,wegenerated cientfine-tuningapproachesoutperforminglarger
thetokensandtheirrespectivetags,wherelabelB zero-shotandfew-shotmethods(Liuetal.,2022),
representsthebeginningofahedgetokenorspan, thoughthegaphereislargerthanonemightexpect.Model Training Prompt Precision(P) Recall(R) F1Score(F1)
BERT Finetuned - 0.883±0.015 0.934±0.012 0.908±0.010
GPT-4o Few-Shot List 0.613±0.027 0.848±0.018 0.712±0.021
LLaMA-3 Few-Shot List 0.518±0.035 0.799±0.022 0.628±0.031
GPT-4o Few-Shot BIO 0.514±0.024 0.766±0.036 0.616±0.030
GPT-4o Zero-Shot List 0.430±0.014 0.711±0.004 0.536±0.012
GPT-4o Zero-Shot BIO 0.436±0.026 0.618±0.033 0.510±0.028
LLaMA-3 Few-Shot BIO 0.298±0.018 0.625±0.016 0.404±0.019
LLaMA-3 Zero-Shot BIO 0.167±0.014 0.428±0.019 0.240±0.017
LLaMA-3 Zero-Shot List 0.274±0.023 0.146±0.010 0.190±0.011
Table2: Averageperformancemetricsoverthefivefoldswithstandarddeviationsfordifferentmodels,training
methods,andprompttypes,orderedbyF1score.
In comparisons of the zero-shot and few-shot OfthehundrederrorssampledfromtheBERT
prompting methods, the few-shot models, unsur- model,approximatelythesamenumberoferrors
prisingly, performed better. The few-shot exper- were false negatives (26) as false positives (29).
iments averaged an F1 of 0.59, 22 points higher Of the hundred errors sampled from the GPT-4o
thanthezero-shotmodelsaverageof0.37. FSL model, 66 were false positives and 25 were
Ofthetwooutputformatspromptedfor,listing false negatives (reflecting the low precision and
andBIO,thelistingapproachperformedbetter. On higherrecallforthisapproach; seeTable3and4
average,modelsinstructedtooutputalisthadan for full error descriptions for BERT and GPT-4o
F1of0.52comparedto0.44forthoseinstructedto FSLmodels).
performBIOtagging. Althoughthecorpusannotationdoesnotinclude
AmongthetwoLLMsprompted,GPT-4oalways thetypeofhedge(onlythepresenceorabsenceof
performedbest. Acrossallmodelsandapproaches, hedgetokens),ourerroranalysislookedathedge
includingfine-tunedBERT,precisiontendedtobe typesinordertoteaseapartmodelbehaviors. We
lowerthanrecall,withameanof0.46forprecision observedsystematicdifferencesbetweenmodelsin
compared to 0.65 for recall. In other words, the theirtypesofmismatcheswiththegoldstandard.
modelsover-predictedthepresenceofhedges. False Positives. First, the GPT-4o FSL model
inaccuratelyclassifieddisfluencies(e.g.,“uh”)as
6 ErrorAnalysis hedges in 37 of the 66 false positives reviewed,
whereas BERT did not. Second, BERT showed
Whilethefine-tunedBERTmodelperformedfairly quiteadifferentpatternofmismatchesthanGPT-
well,acertainnumberofcasesdidnotalignwith 4owhenclassifying“like”,returningfalsepositives
the gold labels in the data. We performed error thatalwaysturnedouttobecomparatives(e.g.,“it’s
analysistounderstandwhethertherewereanysys- likeanopenelevator”). Theseweconsideredtobe
tematicdeviationsfromthecorpusannotation. trueerrorsintheirtextform, althoughsomemay
Weconductedanerroranalysisonthetoptwo beambiguitiesthatcouldberesolvedprosodically.
performing models, the fine-tuned BERT model FalseNegatives. Tokensdenotingapproximator
and the GPT-4o Few-shot List (FSL) model (F1 hedges(e.g. “that’sbasicallyit”)werefrequently
= 0.91 and 0.71, respectively). Starting with the misclassifiedasfalsenegativesbyBERT(9of26
first fold, we selected the first hundred errors to falsenegativesreviewed),butneverbytheGPT-4o
categorize. These errors are broadly divided into FSLmodel.
instanceswherethemodelsfailedtodetectahedge Inaddition, Otheremergedasacategorytype
(false negatives) and instances where models re- for situations that could not clearly be described
turnedcasesthatwerenotannotatedhedges(false as false positives, false negatives, or gold errors.
positives). Theremainingerrorsfellintotwoother In the BERT model, these cases were typically
categories: a gold error category, wherein errors segmentationerrors(i.e.,aninnertokenmislabeled
inthe(human)annotationwerediscovered,andan asabeginningtoken).
“other”category. Notably,thelargestclassoferrorsfortheBERTGoldErrors FalseNegative FalsePositive Other
Like 13 Approximator 9 Like 13 I shouldbeB 4
Proxy 12 Proxy 8 Just 8 OshouldbeI 3
Just 7 Self-talk 4 Falseproxy 4 BshouldbeI 2
Approximator 1 Like 3 Youknow 2 Other 2
Other 1 Just 1 Misc. word 2
Morpheme 1
Total 34 26 29 11
Table3: ExpandederroranalysisontheBERTfine-tunedmodel,byhedgetype.
GoldErrors FalseNegative FalsePositive Other
Approximator 4 Just 12 Disfluencytag 37 Other 1
Just 1 Proxy 8 Misc. word 15
Like 1 Like 3 Like 7
Proxy 1 Morpheme 1 Approximator 3
Self-talk 1 Self-talk 1 Intensifiers 3
Youknow 1
Total 8 25 66 1
Table4: ExpandederroranalysisontheGPT-4oFSLmodel,byhedgetype.
model was the Gold Error category (34 of 100). learned,butitcannotbelearnedinthemannerthat
ThiswasnotthecasefortheGPT-4omodel(only9 LLMsaretaught,namelybysimplyingestinglarge
golderrors). TheBERTfine-tunedmodelrevealed amountsofvarieddata. Weinterpretthistomean
mistakesmadebythehumanannotatorsforhedges that if we want to make LLMs able to converse
denoted by “like”, “just”, and proxy hedges (e.g. withhumansashumansdo,weneedtounderstand
“andstuff”). Uponcloserinspection,someofthese whatcapabilitiesLLMsneedandhowtoprovide
caseswereambiguous. Forexample,“hejusthits themwiththeabilitytodoso.
theground”couldbetakentomeanthattheonly Theprevalenceofgolderrorsdiscoveredbythe
action performed was hitting the ground (where BERTmodelraisestwointerestingpointsfordis-
“just” means only) or “just” might function to re- cussion. First,someofthesediscrepanciesidenti-
duce the speakers’ certainty (as in Madaio et al., fiedbytheBERTmodelwereclearlyerrorsmade
2017). Again, the text format of the storytelling bythehumancoders;thiswastrueinparticularfor
corpusleavessomeinterpretationsambiguousthat proxies,whichBERTcodedforhedgesmorecon-
couldbeclarifiedwithsignalssuchastimingand sistentlythandidhumancoders. Thiserroranalysis
prosodicstress. allowed us to iteratively improve the human cod-
The number of Gold Errors identified by the ingbeforethefinalanalysis,essentiallydeploying
BERTmodelallowedustomodifytheoriginalgold an LLM-in-the-Loop approach. Second, the dis-
annotationwithmissedcasesandtore-evaluatethe crepanciesbetweenBERTandgoldcodingonthe
performance of our models more accurately – a tokens just and like highlight that these types of
sortofLLM-in-the-Loopapproach(seeTable5). hedgeshavehighpotentialforambiguity–perhaps
the very sort of ambiguity that could be resolved
7 Discussion
byprosody.
Theresultsshowthatevenenormous,recentlyre-
8 LimitationsandFutureWork
leased LLMs cannot reliably recognize hedges.
Thereisno“emergent”abilityinLLMstounder- Thisworkrepresentsthefirststepinourresearch
standfullhumanlinguisticbehavior. Ontheother programthataimstotrainLLMstousecollateral
hand,whenweexplicitlytrainasmall,ratherold signalsinsupportofhuman-LLMdialogue. Once
LLM (BERT) to perform our task by fine-tuning hedgescanberecognizedbyanLLM,itremainsto
it, it performs quite well. What this shows is beshownthattheycanbemeaningfullyinterpreted
that detecting hedges is a capability that can be andgenerated. RelevantworkbyCassellandcol-Model OriginalGoldF1 LLM-in-the-LoopGoldF1 ErrorReduction(%)
BERT 0.908±0.010 0.925±0.019 18.5%
GPT-4oFew-Shot 0.712±0.021 0.721±0.020 3.1%
GPT-4oZero-Shot 0.510±0.028 0.551±0.011 8.4%
Table5: F1scoreswithstandarddeviationsontheoriginalcorpus,F1scoreswithstandarddeviationsobtainedon
thecorpuscorrectedafterLLM-in-the-Loop,andthechangeinaverageperformanceforourtopperformingmodels.
leagues has shown that it is possible to generate Gerrig (1990); they count as hedges in that the
hedgesintutoringdialogues,butnotalwaysposi- speakermarkswhatfollowsasnotverbatim.
tionedwheretheyaremostprobableoruseful(Ab-
Training with audio input. Our results for de-
ulimitietal.,2023a). Infuturework,weplanexper-
tectinghedgesinthistranscribedspokencorpusare
imentsusingtop-performingmodelssuchasBERT
surprisinglystrong,especiallygiventhattheLLMs
andGPT-4oinhigh-andlow-probabilitysituations
we used were pre-trained primarily on originally
that systematically vary the certainty associated
writtentext. Butitiswell-knownthatfeaturessuch
withprompted-forinformation(wherehedgescan
aspausingandintonationarerelatedtospeakers’
bemostuseful). Itisalreadyclearfromourpilottri-
levels of commitment to and confidence in their
alsusingChatGPT3.5thatLLMshedgesomewhat
utterances. We plan to incorporate audio into fu-
superficially(hedgingwherehumanswouldn’tand
turehedgingstudiesandwillexploremulti-modal
failingtohedgewherehumanswould).
neuralarchitecturesfusingbothspeechandlexical
Domains of Dialogue. Here we have used features as we did in (Murzaku et al., 2024) for
human-generateddialoguefromasingledomain, beliefrecognition.
retelling stories from Roadrunner cartoons; the
trainingdataaretexttranscriptsofspeech. Because Reliability. Itiscriticaltokeepinmindthathu-
theinitiativewasunbalancedinthiscollaborative man and LLMs are very different sorts of agents.
task,mostofthespeakingineachtriadwasdoneby Psychometric tests show that individual humans
thethepartnerwhoviewedandretoldthecartoon are likely to respond consistently when tested re-
storiesinseriestothetwoco-presentaddressees. peatedly,whereasanLLMisnot(Shuetal.,2024).
A more balanced domain in which partners LLMshavenosenseof“self”andarelikelytore-
continuouslymonitoreachother’sunderstanding sponddifferentlywhenre-promptedwiththesame
to do a physical task–such as matching pictures prompt. To the extent that a hedge signals that a
of difficult-to-describe objects–could yield more speakerdoesnotwishtobeheldentirelyaccount-
hedges, distributed differently. We plan to con- able for what they’re saying, hedging on the part
ductsimilarteststoreplicatethecurrentresultson ofanLLMmayactuallybedesirableasawayto
suchreferentialcommunicationcorporacollected encourageuserstonotassumetheycanholditac-
previouslyinourlab. countable. Ontheotherhand,itmaybedesirable
Itisinterestingthatdespitethefactthatthereis foranLLMtobeabletosignalitsconfidence–the
notasingleinstanceofdialogueinRoadrunnercar- reliabilityorquality(orlackthereof)ofinformation
toons(apartfromRoadrunner’ssmug, trademark it’s presenting – through the presence or absence
“meepmeep”uponescapingfromCoyote),speak- of hedges. Finally, it remains to be seen whether
erswhoretellthestoryinadramaticandhumorous LLMscanlearnaboutinteractionthroughexposure
way do a great deal of what looks like quoting tocollateralsignalsinmeaningfulcontexts.
Coyote’sandRoadrunner’sreactions:
9 Conclusion
sothenhe’ssayinghe’slikegoneallsad
andstuffyouknow? Ourprojectisgroundedinpsycholinguistictheory
andaimstocapturetheory-of-mindaspectsofhedg-
and he’s like whatever she’s gonna be
ingamongdiscourseparticipants. Wepresentthe
deadright?
Roadrunner-Hedgecorpus,withhedgesannotated
Suchusesoflikeinthiscorpusmatchthequotation- fromnaturallyoccurringdialoguesbyspeakersde-
as-demonstrations forms described by Clark and scribingRoadrunnercartoons. Weusethecorpustotrainandperformexperimentsondetectinghedges using end-to-end neural models? Preprint,
using BERT, GPT-4o, and LLaMA-3. We find arXiv:2306.14696.
that fine-tuning BERT significantly outperforms
AI@Meta.2024. Llama3modelcard.
state-of-the-art LLMs in few-shot and zero-shot
settings. With our systems outputs, we perform JenniferArnold, MariaFagnano, andMichaelTanen-
haus. 2003. Disuencies signal theee, um, new in-
anerroranalysisanduseanLLM-in-the-Loopap-
formation. Journal of Psycholinguistic Research,
proach to correct gold standard annotations. Our
32:25–36.
LLM-in-the-loopapproachprovidedfurthererror
reductionsonallmodels. Jennifer Arnold, Carla Hudson Kam, and Michael
Tanenhaus. 2007. If you say thee uh you are de-
scribingsomethinghard: Theon-lineattributionof
EthicalConsiderations
disfluencyduringreferencecomprehension. Journal
ofexperimentalpsychology.Learning,memory,and
TheRoadrunner-Hedgecorpuswascollectedwith
cognition,33:914–30.
Institutional Review Board approval from under-
graduatestudentswhogaveinformedconsentprior SusanE.Brennan.2005. Howconversationisshapedby
toparticipatingintheexperiments. visualandspokenevidence. InJohnTrueswelland
MichaelTanenhaus,editors,Approachestostudying
world-situatedlanguageuse: Bridgingthelanguage-
Acknowledgments
as-productandlanguage-as-actiontraditions,pages
95–129.MITPress.
This material is based upon work supported in
part by the National Science Foundation (NSF)
SusanE.BrennanandHerbertH.Clark.1996. Concep-
under No. 2125295 (NRT-HDR: Detecting and tualpactsandlexicalchoiceinconversation. Journal
Addressing Bias in Data, Humans, and Institu- ofExperimentalPsychology:Learning,Memory,and
Cognition,22(6):1482–1493.
tions)aswellasbyfundingfromtheDefenseAd-
vancedResearchProjectsAgency(DARPA)under Susan E. Brennan and J. O. Ohaeri. 1999. Why do
theCCUprogram(No. HR001120C0037,PRNo. electronicconversationsseemlesspolite? thecosts
andbenefitsofhedging. InACMSIGSOFTSoftware
HR0011154158,No. HR001122C0034). Anyopin-
EngineeringNotes.
ions,findingsandconclusionsorrecommendations
expressedinthismaterialarethoseoftheauthor(s) Susan E Brennan and Maurice Williams. 1995. The
anddonotnecessarilyreflecttheviewsoftheNSF feeling of another’s knowing: Prosody and filled
pausesascuestolistenersaboutthemetacognitive
orDARPA.
statesofspeakers. JournalofMemoryandLanguage,
WethankboththeInstituteforAdvancedCom-
34(3):383–398.
putationalScienceandtheInstituteforAI-Driven
Discovery and Innovation at Stony Brook for ac- P.BrownandS.C.Levinson.1987. Politeness: Some
universalsinlanguageusage. CambridgeUniversity
cess to the computing resources needed for this
Press.
work. These resources were made possible by
NSF grant No. 1531492 (SeaWulf HPC cluster Hyung Won Chung, Le Hou, Shayne Longpre, Bar-
ret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
maintainedbyResearchComputingandCyberin-
Wang,MostafaDehghani,SiddharthaBrahma,etal.
frastructure)andNSFgrantNo. 1919752(Major
2022. H.chi,jeffdean,jacobdevlin,adamroberts,
ResearchInfrastructureprogram),respectively. denny zhou, quoc v. le, and jason wei. 2022. scal-
We would also like to thank our reviewers for ing instruction-finetuned language models. arXiv
preprintarXiv:2210.11416.
theirhelpfulcomments,aswellasKaylaHuntfor
assistancewithreliabilitycoding.
HerbertH.Clark.1994. Managingproblemsinspeak-
ing. SpeechCommunication,15:243–250.
References Herbert H. Clark. 1996. Using Language. “Using”
LinguisticBooks.CambridgeUniversityPress.
Alafate Abulimiti, Chloé Clavel, and Justine Cassell.
2023a. When to generate hedges in peer-tutoring HerbertH.ClarkandSusanE.Brennan.1991. Ground-
interactions. InProceedingsofthe24thAnnualMeet- ingincommunication. InL.B.Resnick,J.Levine,
ingoftheSpecialInterestGrouponDiscourseand andS.D.Teasley,editors,PerspectivesonSocially
Dialogue,pages572–583,Prague,Czechia.Associa- SharedCognition,pages127–149.AmericanPsycho-
tionforComputationalLinguistics. logicalAssociation.
Alafate Abulimiti, Chloé Clavel, and Justine Cassell. HerbertH.ClarkandRichardJ.Gerrig.1990. Quota-
2023b. How about kind of generating hedges tionsasdemonstrations. Language,66:764–805.HerbertH.ClarkandDeannaWilkes-Gibbs.1986. Re- NanjiangJiangandMarie-CatherinedeMarneffe.2021.
ferringasacollaborativeprocess. Cognition,22(1):1– Hethinksheknowsbetterthanthedoctors: BERT
39. for event factuality fails on pragmatics. Transac-
tionsoftheAssociationforComputationalLinguis-
DelphineDahan.2023. Collaborationunderuncertainty tics,9:1081–1097.
inunscriptedconversations:Theroleofhedges. Jour-
nalofExperimentalPsychology: Learning,Memory, George Lakoff. 1975. Hedges: A study in meaning
andCognition,49:320–335. citeriaandthelogicoffuzzyconcepts. Journalof
PhilosophicalLogic,pages458–508.
Shih-ChiehDai,AipingXiong,andLun-WeiKu.2023.
LLM-in-the-loop: Leveraginglargelanguagemodel Robin Lakoff. 1973. Language and woman’s place.
forthematicanalysis. InFindingsoftheAssociation LanguageinSociety,2(1):45–79.
forComputationalLinguistics: EMNLP2023,pages
9993–10001,Singapore.AssociationforComputa- KentonLee,YoavArtzi,YejinChoi,andLukeZettle-
tionalLinguistics. moyer.2015. Eventdetectionandfactualityassess-
mentwithnon-expertsupervision. InProceedingsof
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and the2015ConferenceonEmpiricalMethodsinNatu-
KristinaToutanova.2018. Bert: Pre-trainingofdeep ralLanguageProcessing,pages1643–1648,Lisbon,
bidirectionaltransformersforlanguageunderstand- Portugal.AssociationforComputationalLinguistics.
ing. arXivpreprintarXiv:1810.04805.
HaokunLiu,DerekTam,MohammedMuqeeth,JayMo-
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and hta,TenghaoHuang,MohitBansal,andColinARaf-
Kristina Toutanova. 2019. BERT: Pre-training of fel.2022. Few-shotparameter-efficientfine-tuning
deepbidirectionaltransformersforlanguageunder- is better and cheaper than in-context learning. Ad-
standing. InProceedingsofthe2019Conferenceof vances in Neural Information Processing Systems,
theNorthAmericanChapteroftheAssociationfor 35:1950–1965.
ComputationalLinguistics: HumanLanguageTech-
nologies,Volume1(LongandShortPapers),pages Kris Liu and Jean Fox Tree. 2012. Hedges enhance
4171–4186,Minneapolis,Minnesota.Associationfor memorybutinhibitretelling. Psychonomicbulletin
ComputationalLinguistics. &review,19:892–8.
MonaDiab,LoriLevin,TerukoMitamura,OwenRam- YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
bow, Vinodkumar Prabhakaran, and Weiwei Guo. dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
2009. Committedbeliefannotationandtagging. In Luke Zettlemoyer, and Veselin Stoyanov. 2019.
ProceedingsoftheThirdLinguisticAnnotationWork- Roberta: A robustly optimized bert pretraining ap-
shop(LAWIII),pages68–73,Suntec,Singapore.As- proach. arXivpreprintarXiv:1907.11692.
sociationforComputationalLinguistics.
MichaelA.Madaio,JustineCassell,andAmyE.Ogan.
BruceFraser.2010. Pragmaticcompetence: Thecase 2017. “ithinkyoujustgotmixedup”: confidentpeer
ofhedging. NewApproachestoHedging,9:15–34. tutorshedgetosupportpartners’faceneeds. Interna-
tionalJournalofComputer-SupportedCollaborative
AlexiaGalatiandSusanE.Brennan.2010. Attenuat- Learning,12:401–421.
inginformationinspokencommunication: Forthe
speaker, orfortheaddressee? JournalofMemory John Murzaku, Tyler Osborne, Amittai Aviram, and
andLanguage,62:35–51. Owen Rambow. 2023. Towards generative event
factualityprediction. InFindingsoftheAssociation
AlexiaGalatiandSusanE.Brennan.2014. Speakers forComputationalLinguistics:ACL2023,pages701–
adapt gestures to addressees’ knowledge: implica- 715,Toronto,Canada.AssociationforComputational
tions for models of co-speech gesture. Language, Linguistics.
CognitionandNeuroscience,29:435–451.
JohnMurzaku,AdilSoubki,andOwenRambow.2024.
Alexia Galati and Susan E. Brennan. 2021. What is Multimodalbeliefprediction. InProceedingsofthe
retainedaboutcommonground? Distincteffectsof AnnualConferenceoftheInternationalSpeechCom-
linguisticandvisualco-presence. Cognition,215. municationAssociation,INTERSPEECH2024.Inter-
nationalSpeechCommunicationAssociation.
H Paul Grice. 1957. Meaning. The philosophical re-
view,66(3):377–388. John Murzaku, Peter Zeng, Magdalena Markowska,
andOwenRambow.2022. Re-examiningFactBank:
PengchengHe,JianfengGao,andWeizhuChen.2021. Predicting the author’s presentation of factuality.
Debertav3:Improvingdebertausingelectra-stylepre- In Proceedings of the 29th International Confer-
trainingwithgradient-disentangledembeddingshar- enceonComputationalLinguistics,pages786–796,
ing. arXivpreprintarXiv:2111.09543. Gyeongju,RepublicofKorea.InternationalCommit-
teeonComputationalLinguistics.
SeppHochreiterandJürgenSchmidhuber.1997. Long
short-termmemory. Neuralcomputation,9(8):1735– OpenAI. 2024. Gpt-4o. https://openai.com/
1780. index/hello-gpt-4o/.Amanda Pogue and Michael K. Tanenhaus. 2018. Gabriel Stanovsky, Judith Eckle-Kohler, Yevgeniy
Learningfromuncertainty: exploringandmanipulat- Puzikov,IdoDagan,andIrynaGurevych.2017. Inte-
ingtheroleofuncertaintyonexpressionproduction gratingdeeplinguisticfeaturesinfactualitypredic-
andinterpretation. InProceedingsofthe40thAnnual tionoverunifieddatasets. InProceedingsofthe55th
ConferenceoftheCognitiveScienceSociety. AnnualMeetingoftheAssociationforComputational
Linguistics(Volume2:ShortPapers),pages352–357,
AmirPouranBenVeyseh,ThienHuuNguyen,andDe-
Vancouver,Canada.AssociationforComputational
jing Dou. 2019. Graph based neural networks for
Linguistics.
eventfactualitypredictionusingsyntacticandseman-
tic structures. In Proceedings of the 57th Annual MorganUlinski,SethBenjamin,andJuliaHirschberg.
Meeting of the Association for Computational Lin- 2018. Usinghedgedetectiontoimprovecommitted
guistics,pages4393–4399,Florence,Italy.Associa- belieftagging. InProceedingsoftheWorkshopon
tionforComputationalLinguistics. ComputationalSemanticsbeyondEventsandRoles,
pages1–5,NewOrleans,Louisiana.Associationfor
VinodkumarPrabhakaran,OwenRambow,andMona
ComputationalLinguistics.
Diab. 2010. Automatic committed belief tagging.
InColing2010: Posters,pages1014–1022,Beijing, Morgan Ulinski and Julia Hirschberg. 2019. Crowd-
China.Coling2010OrganizingCommittee. sourcedhedgetermdisambiguation. InLAW@ACL.
E.F.Prince,J.Frader,andC.Bosk.1982. Onhedging JiaruiYao,HaolingQiu,JinZhao,BonanMin,andNi-
inphysician-physiciandiscourse. InRobertDiPrieto, anwenXue.2021. Factualityassessmentasmodal
editor,LinguisticsandtheProfessions,pages83–97. dependencyparsing. InProceedingsofthe59thAn-
Albex. nualMeetingoftheAssociationforComputational
Linguisticsandthe11thInternationalJointConfer-
AnnaProkofievaandJuliaHirschberg.2014. Hedging
ence on Natural Language Processing (Volume 1:
andspeakercommitment. In5thInternationalWork-
LongPapers),pages1540–1550,Online.Association
shoponEmotion,SocialSignals,Sentiment&Linked
forComputationalLinguistics.
OpenData.
A PromptingDetails
YannRaphalen,ChloéClavel,andJustineCassell.2022.
“You might think about slightly revising the title”:
TheexactprompttemplatesusedfortheBIOand
Identifyinghedgesinpeer-tutoringinteractions. In
listingexperimentsareshownbelow.
Proceedings of the60th Annual Meeting of the As-
sociationforComputationalLinguistics(Volume1:
Given an utterance, perform BIO tagging to↩
LongPapers),pages2160–2174,Dublin,Ireland.As- classify hedges in the sentence. ``↩
sociationforComputationalLinguistics. BIO" tagging is a method used in ↩
named entity recognition where each ↩
RachelRudinger,AaronStevenWhite,andBenjamin token (word) in the sentence is ↩
VanDurme.2018. Neuralmodelsoffactuality. In tagged as follows:
Proceedings of the 2018 Conference of the North
AmericanChapteroftheAssociationforComputa- B (Beginning): The token is the beginning ↩
tionalLinguistics: HumanLanguageTechnologies, of a hedge.
Volume 1 (Long Papers), pages 731–744, New Or- I (Inside): The token is inside, but not ↩
leans,Louisiana.AssociationforComputationalLin- the first token of a hedge.
O (Outside): The token is not part of a ↩
guistics.
hedge.
Mark Sabbagh and Dare Baldwin. 2001. Learning Please assign one of these tags to each ↩
token in the given utterance, ↩
words from knowledgeable versus ignorant speak-
representing whether each word is ↩
ers: Linksbetweenpreschoolers’theoryofmindand
part of a hedge phrase or not. Format↩
semanticdevelopment. ChildDevelopment,72:1054
your response by listing each token ↩
–1070.
followed by its corresponding BIO tag↩
.
Roser Saurí and James Pustejovsky. 2009. Factbank:
acorpusannotatedwitheventfactuality. Language
Example:
resourcesandevaluation,43:227–268.
If the utterance is ``I think maybe you ↩
Bangzhao Shu, Lechen Zhang, Minje Choi, Lavinia
could try an approach like that" then↩
Dunagan, Lajanugen Logeswaran, Moontae Lee, ``I think" and ``maybe" are ↩
Dallas Card, and David Jurgens. 2024. You don’t identified as hedges so your output ↩
need a personality test to know these models are should look like this:
unreliable: Assessing the reliability of large lan-
guagemodelsonpsychometricinstruments. Preprint, Utterance:
arXiv:2311.09718. I think maybe you could try an approach ↩
like that
Vicki L. Smith and Herbert H. Clark. 1993. On the
courseofansweringquestions. JournalofMemory Tags:
andLanguage,32(1):25–38.I/B think/I maybe/B you/O could/O try/O an↩ F1 Theharmonicmeanofprecisionandrecall.
/O approach/O like/O that/O
precision⋅recall
Now given the following input, please ↩ F1 = 2⋅
precision+recall
classify the hedges in the sentence.
Utterance: ItisalsocalledF-measureorF-score. Loosely
{utterance} speaking,themetricisabalanceofhowoften
themodeliscorrectwhenitpredictsaparticu-
larclass(precision),andhowoftenthemodel
Given a conversation, answer a question. ↩
Be as precise and succinct as ↩ predictsthatclasswhenitwouldbecorrectto
possible. If asked for a number, ↩
doso(recall). 5
provide a numeric value.
Format the output as follows: LLM LargeLanguageModelsarelarge(typically
Number of Hedges: Integer number of ↩
byparametercount)modelswhichtakeintext
linguistic hedges (e.g. 0)
List of Hedges: List of hedges found (e.g.↩ and produce a distribution over their vocab-
[``first hedge", ``second hedge", ↩ ulary which can be used to predict the next
etc...])
token. 1
Conversation:
{utterance} <stop sign emoji> LSTM Long Short-Term Memory networks
(Hochreiter and Schmidhuber, 1997) are a
Question:
typeofrecurrentneuralnetworkdesignedto
At the line that ends with <stop sign ↩
emoji>, how many linguistic hedges ↩ capturelong-rangedependencies. 4
are there? List all the linguistic ↩
hedges using quotations. Do not add ↩
any additional information. narrativeelement ObservableeventsintheRoad-
runnercartoonthatandwerelikelytobemen-
tionedinnarrations(seeGalatiandBrennan,
B Glossary
2010). Segmentation by narrative elements
allowedforcomparisonsacrossspeakersfor
Due to the interdisciplinary nature of this work,
elementsrealizedineachnarration. 4
weprovidebelowbriefdefinitionsfortermswhich
maybeunfamiliar. Thenumbersrefertothepages
precision Thenumberofcorrectpredictions(true
inthispaperinwhichthetermfirstappears.
positives) for a class divided by the number
oftimesthemodelpredictedthatclass(true
BERT BERT(Devlinetal.,2018)standsforBidi-
positives+falsepositives). 5,12
rectionalEncoderRepresentationsfromTrans-
formers. BERTisatransformer-basedmodel
whichproducescontextualrepresentationsof recall Thenumberofcorrectpredictions(truepos-
textbyconditioningonboththeleftandright itives) for a class divided by the number of
surroundingwords. 4 sampleswhichbelongtothatclass(trueposi-
tives+falsenegatives). 5,12
BIO BIO,shortforBeginning,Inside,Outside,is
aformatforlabelingchunksoftokens. Tokens temperature Ahyperparameterthatmodifiesthe
areassignedBiftheybeginasequencewhich next token distribution of language models.
should be labeled (e.g., a named entity), I if Largertemperaturevaluesincreasethelikeli-
theybelongtoapreviouslybegunsequence, hoodoflowerprobabilitytokens. 5
andOotherwise. 5
token Thesmallestunitoftext,oftenwordsorsub-
words,whichareusedastheinputforvarious
Cohen’sKappa Measure of agreement between
NLPmodels. 3
tworatersthatanitemfallswithinasubjective
category;highervaluesdenotehigheragree-
ment. 4
epoch Asinglepassthroughthetrainingdata. 5