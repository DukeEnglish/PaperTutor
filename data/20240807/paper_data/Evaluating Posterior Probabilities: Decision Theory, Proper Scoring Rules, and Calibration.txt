Evaluating Posterior Probabilities:
Decision Theory, Proper Scoring Rules, and Calibration
Luciana Ferrer
lferrer@dc.uba.ar
Instituto de Ciencias de la Computaci´on
Universidad de Buenos Aires - CONICET, Argentina
Daniel Ramos
daniel.ramos@uam.es
AUDIAS Lab. - Audio, Data Intelligence and Speech
Escuela Polit´ecnica Superior, Universidad Aut´onoma de Madrid, Spain
Abstract
Most machine learning classifiers are designed to output posterior probabilities for the
classes given the input sample. These probabilities may be used to make the categorical
decision on the class of the sample; provided as input to a downstream system; or pro-
vided to a human for interpretation. Evaluating the quality of the posteriors generated by
these system is an essential problem which was addressed decades ago with the invention
of proper scoring rules (PSRs). Unfortunately, much of the recent machine learning litera-
tureusescalibrationmetrics—mostcommonly,theexpectedcalibrationerror(ECE)—asa
proxy to assess posterior performance. The problem with this approach is that calibration
metrics reflect only one aspect of the quality of the posteriors, ignoring the discrimination
performance. For this reason, we argue that calibration metrics should play no role in
the assessment of posterior quality. Expected PSRs should instead be used for this job,
preferablynormalizedforeaseofinterpretation. Inthiswork,wefirstgiveabriefreviewof
PSRs from a practical perspective, motivating their definition using Bayes decision theory.
We discuss why expected PSRs provide a principled measure of the quality of a system’s
posteriors and why calibration metrics are not the right tool for this job. We argue that
calibrationmetrics,whilenotusefulforperformanceassessment,maybeusedasdiagnostic
toolsduringsystemdevelopment. Withthispurposeinmind,wediscussasimpleandprac-
tical calibration metric, called calibration loss, derived from a decomposition of expected
PSRs. We compare this metric with the ECE and with the expected score divergence cali-
brationmetricfromthePSRliteratureandargue,usingtheoreticalandempiricalevidence,
that calibration loss is superior to these two metrics.
1 Introduction
High-stakes machine learning applications, like those used to make health, military or legal
decisions, often require systems that can provide a measure of uncertainty of the prediction
given the input sample (Tomsett et al., 2020; Quinonero-Candela et al., 2005). In clas-
sification tasks, the uncertainty is a property of the posterior probability for the classes
given the input sample. We use the term probabilistic classifier to refer to a classification
system that outputs posterior probabilities. The posterior probabilities produced by a good
©2024L.FerrerandD.Ramos.
License: CC-BY4.0,seehttps://creativecommons.org/licenses/by/4.0/.
4202
guA
5
]LM.tats[
1v14820.8042:viXraFerrer et al.
probabilistic classifier can be used to make optimal decisions for a given cost function using
Bayes decision theory (DeGroot, 1970; Bernardo and Smith, 1994; Jaynes, 2003), and they
can be readily interpreted by an end user or passed on to a downstream system. Evaluating
the quality of the posteriors produced by a classification system is not a trivial task since,
unlike for the evaluation of categorical decisions for which class labels are used as ground
truth, there are no ground-truth posteriors against which to compare the system-generated
posteriors. Except in simulations, the true posterior distribution is not available to us. All
we ever have are models trained on data. Every model we develop for a given problem
provides us with a new probabilistic classifier that does inference of the class of an input
sample in the form of a posterior distribution. The focus of this work is how assess the
goodness of such models.
Much work was done on the evaluation of posterior probabilities over the last decades.
The term proper scoring rule (PSR) was first introduced by Winkler and Murphy (1968)
who motivated their work by the need to assess the quality of weather forecasts. PSRs
were later further studied in a large number of works (see, for example, Dawid et al., 2016;
Gneiting and Raftery, 2007; Br¨ocker, 2009, among many others). Brier score and the neg-
ative logarithmic loss (NLL), proposed in the 1950s, are special cases of PSRs (Brier, 1950;
Good, 1952). PSRs provide us with a principled way to measure the quality of posterior
probabilities. The expectation of a PSR with respect to a given reference probability distri-
bution over the classes is minimized when the distribution under evaluation coincides with
this reference distribution. Hence, a low PSR expectation indicates that the distribution
under evaluation is close to the reference distribution with respect to which the expectation
is taken. Note that we need to refer to a reference distribution since, as mentioned above,
the true distribution is never available in practice.
AperhapsmoreinsightfulwaytounderstandPSRsistoseethattheycanbeconstructed
as the cost that results from making minimum-expected-cost Bayes decisions using the pos-
teriors under evaluation. This cost directly reflects the quality of the posterior probability
that was used to make the Bayes decisions. Since a PSR measures the quality of the class
posterior distribution for one specific input sample, to obtain a metric to assess the quality
of a probabilistic classifier’s output we take the expectation of the PSR (EPSR) over the
data (Bru¨mmer, 2010; Filho et al., 2023). For example, the cross-entropy, widely used as
objective function to train classification systems, is the expectation of the PSR given by
the negative logarithmic loss.
In contrastto the largebodyof literatureon PSRs, many of the recent machine learning
works concerned with the evaluation of posteriors do not use PSRs for the task, resorting
instead to calibration metrics (for example, Guo et al., 2017; Widmann et al., 2019; Gruber
andBuettner,2022;Nixonetal.,2019;Popordanoskaetal.,2022;Vaicenaviciusetal.,2019;
Van Hoorde et al., 2015; Huang et al., 2020; Mu¨ller et al., 2019). A classification system is
said to be well-calibrated if its output, q, coincides with a reference posterior distribution
for the classes given q, for every possible input sample, x (Br¨ocker, 2009; Guo et al., 2017;
Widmann et al., 2019). Note that, as before, we need to refer to a reference posterior
since the true posterior is not available in practice. The reference posterior needed to check
calibration is yet another model of the posterior, one that we trust to be good.
Note that the calibration definition does not refer to posteriors for the classes given the
inputsample. Instead,itreferstoposteriorsfortheclassesgiventhesystem’soutput. Hence,
2Evaluating Posterior Probabilities
goodcalibrationdoesnotimplythatthesystem’sposteriorsaredoingagoodjobofinferring
the classes from the inputs, which is what matters in practice. In fact, a calibrated system
can be useless. For example, a naive system that always outputs the prior probabilities
of the classes is perfectly calibrated but does not provide any information about the input
samples. As we will see, the overall quality of the system’s posteriors, as measured by
EPSRs, can be decomposed in two terms: a calibration and a discrimination or refinement
component (Filho et al., 2023; Br¨ocker, 2009). The discrimination component quantifies
the amount of information present in the system’s output about the class of the samples
while the calibration component quantifies the similarity between the system’s output and
a reference distribution for the class given that output. Neither component by its own fully
describes the goodness of the posteriors produced by the system.
As a consequence of the fact that calibration metrics only reflect one aspect of the
posteriors’ performance, they do not inform how useful the classifier will be in practice.
To address this problem, papers that use calibration metrics resort to a separate metric,
usually accuracy, error rate, or area under the ROC curve, to complement the calibration
metric (Guo et al., 2017; Van Hoorde et al., 2015; Minderer et al., 2021; Mukhoti et al.,
2020). Thisisaproblematicpracticesinceitdoesnotallowforadirectcomparisonbetween
two systems. If a system’s calibration error is lower but the error rate is higher than for
anothersystem, whichofthemisbetter? Whichoneshouldwechoosefordeploymentifour
main goal is to produce good posteriors for interpretation and decision-making? Also, this
practice does not solve the problem of assessing whether the performance of the posteriors
is good enough for a certain task. These questions are answered by EPSRs which provide a
comprehensive measure of the value provided by the system’s posteriors. Using calibration
metrics, on the other hand, leads to unnecessary conflict.
Papers that report calibration metrics motivate its use by arguing that good calibration
is an essential characteristic for a probabilistic system to be interpretable, safe, or reliable
(Guo et al., 2017; Widmann et al., 2019; Mukhoti et al., 2020; Kumar et al., 2019; Minderer
etal.,2021;Nixonetal.,2019),usingstatementslike: “anetworkshouldprovideacalibrated
confidence” (Guo et al., 2017), “miscalibration [...] makes [...] predictions hard to rely on”
(Mukhoti et al., 2020), and “model calibration is essential for the safe application of neural
networks” (Minderer et al., 2021). The implicit or explicit implication in those statements
is that a very discriminative system with relatively high calibration loss should not be used
in high-stakes scenarios. We challenge this view and propose that calibration is neither
necessary, nor sufficient for posterior probabilities to be useful to the end user. A system
may be somewhat miscalibrated but still be useful, as long as the posteriors that it outputs
result in sufficiently good Bayes decisions. In particular, a miscalibrated system may be
muchmoreusefulthantheperfectly-calibratednaivesystemmentionedabove. Ifasystem’s
EPSR is lower than the EPSR of an alternative system, we can conclude that its posteriors
are better, regardless of the calibration error of either system. Hence, when evaluating the
utility of posteriors we should not be concerned with whether they are calibrated or not.
Assessing the quality of probabilistic classifiers is exactly the purpose of EPSRs. When the
goal is to assess the value of an individual classifier or compare classifiers with each other
to select the best one, there is no need to resort to the concept of calibration.
In this paper, we argue that the only purpose of calibration metrics should be to diag-
nose whether a system is well-calibrated in order to fix it if that is not the case, much like
3Ferrer et al.
learning curves over validation and training data are used to diagnose overfitting. While
regularization, early stopping, or smaller models may be explored when overfitting is de-
tected, various approaches can be explored when miscalibration is detected. In particular,
a miscalibrated system can be very easily improved by adding a post-hoc calibration stage
(Filho et al., 2023): a transformation of the system output designed to reduce the calibra-
tion error. Such a stage, if successful, would result in a new system with a lower EPSR,
which is our final goal. Beyond the use as a diagnostic tool during development, calibra-
tion metrics should not play any role in system evaluation since they do not provide any
particular insight about the value of a system for the end user. If changing the system is
not an option, assessing and reporting calibration performance has no practical role.
For the calibration analysis needed during system development, we propose to use the
calibration loss metric. Calibration loss is obtained from a decomposition of an EPSR
into calibration and discrimination terms, and directly reflects the improvement we would
obtain if a post-hoc calibration stage was added to the system. A particular version of this
metric was introduced in the literature years ago for the binary task of speaker verification
(Bru¨mmer and du Preez, 2006) and later adopted in forensic science (Ramos and Gonzalez-
Rodriguez, 2013; Ramos et al., 2017, 2020). We compare it, theoretically and empirically,
with the widely-used expected calibration error (ECE) metric (Naeini et al., 2015a; Guo
et al., 2017), and argue that ECE has no theoretical or practical advantage over calibration
loss having, on the other hand, various disadvantages.
The rest of this paper gives an introduction to Bayes decision theory and PSRs, high-
lighting their tight relationship. Then, it describes the problem of calibration, introduces
the calibration loss metric, and compares it with the ECE and the expected score diver-
gence, a classic calibration metric from the statistics literature, providing novel insights and
discussing the advantages of the calibration loss over those classic alternatives. Finally, it
presents empirical results on synthetic and real datasets to demonstrate and further discuss
thetheoreticalideasintheprevioussections. Asubstantialpartofthecontentinthispaper
revisits well-established concepts, some dating back decades. Yet, we believe a discussion of
theseideasisstillneededinthemachinelearningcommunity, giventhecurrentwide-spread
use of calibration metrics as a proxy to assess the quality of a system’s posteriors.
In summary, the goals of this work are to argue: 1) that calibration metrics should only
be used during system development, for example, for the purpose of deciding whether a
post-hoc calibration stage is needed in the system, 2) that for those cases, calibration loss,
a simple and principled calibration metric, is preferable to the ECE and to the expected
score divergence, 3) that the goodness of a probabilistic system, as will be perceived by an
end user, a downstream system, or a decision stage, should be assessed using EPSRs, not
calibration metrics.
2 From Bayes decision theory to calibration
The goal of this section is to review known concepts from the statistical learning literature
and discuss them in the light of current trends in the machine learning literature, where the
quality of posterior probabilities is most often assessed using the ECE or one of its variants.
First, we discuss the reference distribution, a necessary construct when dealing with real
data where the underlying data-generating distribution is not known. Then, we describe
4Evaluating Posterior Probabilities
how to make optimal (Bayes) decisions for a given cost function selected for the application
of interest. We then define PSRs as the cost of Bayes decisions motivating their use as
metrics to evaluate the quality of posteriors. We also explain how they can be normalized
to obtain interpretable metrics. Further, we explain how EPSRs can be understood as inte-
gralsoverafamilyofBayesrisks, providingfurtherintuitiononwhyandhowEPSRsreflect
the quality of posteriors. Finally, we show how to decompose an EPSR into calibration and
discrimination components in two ways, using the traditional divergence-entropy decompo-
sition, andusingthecalibrationlossdecomposition, andcomparetheresultingmetricswith
the widely used ECE metric.
While most of the content in this section is based on decades-old concepts from the
statistical literature, our treatment is different from that in most works in that the need
for a reference distribution is made explicit in every step of the way. As we will see, this
uncovers some practical issues that are usually not considered in the literature.
2.1 The reference distribution
Instatisticsandmachinelearningliterature, itiscommonpracticetoexplicitlyorimplicitly
rely on ‘the true distribution of the data’. We find this an ill-defined concept, which is not
useful in practice and which, in particular, poses formidable obstacles to understanding
calibration. Even if one could theoretically consider that a given dataset was created by
sampling from true underlying probability distribution (an infinite-size dataset), such a
distribution will never be known to us—except in simulations.
In machine learning, though, both at training time and test time, it is very useful to
work with probability distributions for1 the data. By ‘the data’ we refer to some future, or
otherwiseunseendata,andby‘distributionsforthedata’werefertoprobabilisticpredictions
of the unseen data. For example, if you are training a classifier by minimizing the cross-
entropy over a supervised training dataset, you are effectively minimizing a prediction of
the cross-entropy on future, unseen data. If you are testing your classifier with error-rate or
cross-entropy, you are also effectively predicting how it will perform on future data. We can
obtain the probabilistic predictions of the data, which we will call reference distributions,
by selecting modeling assumptions and then fitting the model to a given dataset. Different
assumptions will lead to different reference distributions.
The reference distributions can be very simple. Perhaps the simplest reference distribu-
tion is the empirical one, obtained by uniformly sampling with replacement from a dataset.
Expectations with respect to the empirical distribution are given by averages over the sam-
ples in the dataset. The empirical distribution is the most common reference distribution
used in machine learning both for training and testing models, where cross-entropy or error
rates are computed as averages over the training or test samples. As we will see, though,
it turns out that empirical distributions do not provide useful references against which to
judge calibration—if we make use of the classical definition of calibration. It is therefore
necessary for our exploration and understanding of calibration to consider more general
reference distributions than the empirical ones. In the theoretical sections below we will
1. We follow the advice by Jaynes (2003) of using the preposition for, rather than of to refer to the
relationshipsbetweendistributionsanddata. Distributionsthatwechoosetousetomodelsomeunseen
dataarenotanintrinsicpropertyoftheunseendata,rathertheyareinducedbyassumptionsandgiven
data.
5Ferrer et al.
simply leave the reference undefined, assuming enough regularity conditions are imposed
in the model to result in a good predictor of future data. We will discuss a simple way to
build such reference distributions in Section 3.4.
2.2 Bayes decision theory
Assume that we have selected a cost function for our classification problem of interest,
C(h,d), where h = H ,...,H is the true class of the sample, d is the decision
1 K
∈ H { } ∈ D
made by the system, and C : R. Decisions can be categorical, in which case we
H×D →
take = D ,...,D . The set of decisions and the set of classes do not need to be the
1 M
D { }
same. The decisions are, in general, the actions that will be taken based on the system’s
output (Duda et al., 2001). For example, could include a “reject” or “abstain” option
D
(see, for example, Bishop, 2006, section 1.5). Decisions can also be soft, in the form of
a distribution over classes, in which case = SK, the simplex where K-class categorical
D
distributions live.
Given the cost function, our goal will be to make optimal decisions in the sense that
they minimize the expectation of this cost (Peterson, 2009), E [C(h,d(x))], where
h∼Pr(h|x)
we have explicitly added the dependency of d on the input sample, x. The expectation is
taken with respect to P (h x), a reference distribution for the class label given the input.
r
|
This expectation is minimized by taking2 (Bishop, 2006; Hastie et al., 2001):
K
(cid:88)
d(x) = d (x) := argmin C(H ,d)P (H x) (1)
B i r i
|
d
i=1
The decision d is called the Bayes decision. If decisions are made in this way for every x,
B
then the expected cost with respect to the joint distribution P (h,x) is also minimized.
r
2.3 Proper scoring rules
Properscoringrules(PSRs)areafamilyoffunctionsspecificallydesignedtoassessthequal-
ity of posterior probabilities (Gneiting and Raftery, 2007; Bru¨mmer, 2010). The principle
behind PSRs is that the quality of posteriors is given by the quality of the Bayes decisions
made with them: better decisions imply better posteriors. Formally, given a posterior for a
sample x provided by a classifier, which we denote3 as q(x) = P (. x), and a cost function
c
|
C(h,d), we can construct a PSR, C∗(h,q), as follows (Dawid and Musio, 2014; Bru¨mmer,
2010):
C∗(h,q) = C(h,d (q)). (2)
B
That is, C∗ is the cost of the Bayes decision made with q. Note that here we have expressed
d as a function of q rather than x since, as shown in Equation (1), Bayes decisions only
B
depend on the posterior which, here, is given by q.
2. Theremaysometimesbemorethanoneminimizingdecision. Insuchcasesweassumeargminappliesa
tie-breaker to return a single minimizing decision. The details of such tie breakers is unimportant here.
3. WeuseboldqforvectorsinSK representingdiscretedistributions,anditalicwithasubindexqi torefer
to the ith component of the q vector.
6Evaluating Posterior Probabilities
The C∗ constructed in this way satisfies the defining property of PSRs which is that
their expected value with respect to any distribution p over the classes is minimized if q
coincides with p (Dawid and Musio, 2014; Bru¨mmer, 2010). That is,
p argminE [C∗(h,q)]. (3)
h∼p
∈ q
When the minimum is unique, the PSR is called strict.
A PSR measures the quality of the posterior vector q for a single sample. To obtain a
metric that can be used for evaluation, we can compute the expectation of the PSR with
respect to the joint reference distribution:
EPSR = E [C∗(h,q(x))]. (4)
Pr(h,x)
Now, since C∗ is a function of x only through q, we can invoke the law of the unconscious
statistician (DeGroot and Schervish, 2014, page 213) and compute the expectation with
respect to q instead:
EPSR = E [C∗(h,q)]. (5)
Pr(h,q)
This equation shows that we do not need a reference distribution for (h,x) to compute the
EPSR. Instead, we only need a reference for (h,q).
Inpractice,tocomputethismetric,wetakethereferencedistributiontobetheempirical
distribution in the test dataset so that the EPSR is given by
N
1 (cid:88)
EPSR = C∗(h ,q ). (6)
t t
N
t=1
where q and h are the classifier’s output and the label for sample t in a test dataset
t t
containing N samples.
Different EPSRs can be obtained by using different cost functions C. Below we will
discuss the three most common EPSRs.
2.3.1 Bayes risk
When the decision d is categorical (d D ,...,D ), the cost function can be expressed
1 M
∈ { }
as a matrix of costs c for each combination of true class H and decision D . The expected
ij i j
cost with respect to the empirical distribution in a test dataset is given by the average cost
over the samples:
K M K M
(cid:88)(cid:88) N ij (cid:88)(cid:88)
EC = c = c P R (7)
ij ij i ij
N
i=1 j=1 i=1 j=1
where P = N /N is the empirical prior probability of class H , N is the number of samples
i i i i
of class H , and R = N /N is the fraction of samples from class H for which the system
i ij ij i i
madedecisionD . TheECcanbenormalizedtoeaseinterpretationbydividingitbytheEC
j
(cid:80)K
ofasystem that outputs alwaystheleast-costly decision, whichisgivenbymin c P .
d i=1 id i
A special case of this metric is obtained for the 0-1 cost matrix where c = 1 when i = j
ij
̸
7Ferrer et al.
(thedecisionisincorrect), andc = 0wheni = j (thedecisioniscorrect). Theaveragecost
ij
in this case reduces to the standard error rate, which is equal to one minus the accuracy.
The EC can be computed regardless of how decisions are made. If decisions are made
using Bayes decision theory, though, the resulting EC is an EPSR commonly called Bayes
risk(Dudaetal.,2001). Givenacostmatrix,Equation(1)correspondstoaspecificpartition
of the simplex SK into decision regions. For binary classification with a square cost matrix
(M = 2), the regions can be determined by a threshold on either of the two posteriors.
For the 0-1 cost, the Bayes decision is the class with the maximum posterior (Hastie et al.,
2001), or argmax decision. Hence, the error rate of argmax decisions, a widely-used metric
in the machine learning classification literature, is one instance of the Bayes risk.
When evaluating Bayes risk, we are, as for any EPSR, measuring the quality of the
posteriors used to make the Bayes decisions. Yet, we are restricting our assessment of the
posteriors to one specific operating point defined by the cost matrix. As a result of this,
the Bayes risk is not a strict PSR: Two classifiers that produce posterior distributions for
which the Bayes decisions for the selected cost matrix are the same, will lead to the same
Bayes risk. The Bayes risk for other cost matrices, though, will not necessarily be the same
for both classifiers, since their posteriors are different.
2.3.2 Cross-entropy
When we want to evaluate the quality of posteriors in general rather than for a single
operating point, we need to resort to strict PSRs. One such PSR can be obtained by
setting C to be the negative logarithmic loss (NLL), C(h,d) = log(d ), where d SK
h
− ∈
and d is the element of d corresponding to class h (if h = H , then d is the ith element of
h i h
vector d). It can be shown that, for this cost function, the Bayes decision (Equation 1) is
(cid:80) (cid:80) (cid:80)
d (q) = q, since, by Gibbs inequality, C(H ,d) q = log(d ) q log(q ) q .
B i i i − i i i ≥ − i i i
That is, the (soft) decision that minimizes the expected NLL is q itself. Hence, the PSR
resulting from this cost function (Equation 2) is also the NLL. It is easy to show that the
NLL is a strict PSR (Dawid and Musio, 2014).
The expectation of the NLL over the data is the cross-entropy, which is widely used
as objective function for training. Taking the expectation with respect to the empirical
distribution, we get
N
1 (cid:88)
CE = log(q (x )), (8)
−N
ht t
t=1
where q (x ) is the system’s output for sample x for class h , the true class of sample t.
ht t t t
This expression can be rewritten to make its dependence on the class priors explicit (see
Appendix A), which allows us to manipulate these priors independently of those in the test
data. This is useful when the priors in our test data do not reflect those we expect to see
when the system is deployed, in which case we can use the target priors instead of the ones
in our data when computing the CE.
The absolute CE values are not easily interpretable. This issue, though, can be solved
by normalizing its value with the CE of the best naive system. A naive system is one that
does not have access to the input data. The best naive system for any EPSR is the one that
outputs the prior distribution in the test data (Bru¨mmer, 2010, Section 2.4.1). The CE
8Evaluating Posterior Probabilities
of a system that always outputs the prior distribution is the entropy of such distribution,
(cid:80)K
P log(P ). Dividing the CE by this value we obtain the normalized CE, or NCE for
− i=1 i i
short. The NCE values can be readily interpreted: values above 1.0 mean that the system
is worse than the best naive system and, hence, one should either 1) throw it away and
replace it with a system that outputs the prior distribution, or 2) fix it by doing calibration
(as we will see in Section 2.4.2). A well-calibrated system will never have a normalized CE
larger than one.
2.3.3 Brier score
If we take C(h,d) = 1 (cid:80)K (d I(h = H ))2, with d SK, we get another PSR called
K i=1 i − i ∈
Brier loss. Here, I(h = H ) is the indicator function which is 1 if the argument is true and
i
0 otherwise. As for the NLL, it can be shown that the Bayes decision for this loss is the
posterior used to make the decision. Hence, as for the NLL, the PSR coincides with the
cost function. Averaging this cost over the data we get the Brier score
N K
1 (cid:88) 1 (cid:88)
BS = (q (x ) I(h = H ))2. (9)
i t t i
N K −
t=1 i=1
As for the CE, this expression can be manipulated to show its dependency with the priors
explicitlywhichallowsustoturnthepriorsintoparametersofthemetric(seeAppendixA).
Further, as for CE, it can be shown that BS is a strict PSR (Dawid and Musio, 2014).
Unlike the CE, though, which can be infinite if the posterior for the right class is 0.0 for
any sample in the dataset, BS is bounded, being more forgiving of extremely incorrect
posteriors. Finally, we can compute a normalized version of BS, NBS, by dividing its value
(cid:80)K
by the BS of the best naive system, which is given by P (1 P )/K.
i=1 i − i
2.3.4 EPSRs as integrals over Bayes risks
As explained, for example in (Gneiting and Raftery, 2007) and in (Bru¨mmer, 2010), we can
construct a PSR, C∗ , by taking the following integral:
W
(cid:90)
C∗ (h,q) = W(a)C∗(h,q)da (10)
W a
SK
wherehistheclassofthesample, q SK isaposteriordistribution, andW(a)isafunction
∈
of a SK that satisfies W(a) 0 a. The C∗ function inside that integral is the PSR
a
∈ ≥ ∀
obtained as explained in Section 2.3.1 for the following cost function:
1 I(i = j)
C (i,j) = − , (11)
a
(N 1) a
i
−
where C (i,j) is the cost for deciding D when the true class of the sample is H . For this
a j i
construction, we take the set of decisions to be the same as the set of classes. This matrix
has zeroes in the diagonal and a cost proportional to 1/a everywhere else. Taking the
i
expectation with respect to the data on both sides of Equation (10), we get that
(cid:90)
EPSR = W(a) Risk(a)dr where Risk(a) = E[C∗(h,q)]. (12)
a
SK
9Ferrer et al.
The function W(a) determines how much each Bayes risk inside the integral influences
the final value. Taking W(a) to be uniform in the simplex, we obtain the CE. For the
binary case, taking W(a) to be a Beta distribution with both parameters equal to 2, so that
W(a) = a (1 a )/B(2,2), where B is the beta function, we obtain the BS. The proofs
1 1
−
for these statements can be found in (Bru¨mmer, 2010, section 7.4). Section 3.1 shows an
illustration of this way of constructing the CE and BS metrics.
2.4 Calibration
Following the literature (for example, Br¨ocker, 2009; Guo et al., 2017; Vaicenavicius et al.,
2019; Nixon et al., 2019; Gruber and Buettner, 2022), a classifier, P , that for input x,
c
outputs the class posterior, q = [q ,...,q ], where q = P (H x), has perfect calibration
1 K i c i
|
with respect to a reference distribution P , if:
r
q = P (H q), i,x (13)
i r i
| ∀
In the cited literature, the role of P is implicitly deferred to the ill-defined concept of
r
the ‘true distribution’. With the reference made explicit, we note that a classifier may
be calibrated with respect to some reference distribution, but miscalibrated with respect
to another. In this section we carefully analyze this definition of perfect calibration and
also compare it to the optimal classifier.
Given a reference distribution for h and x, P (h,x), the optimal classifier is the one
r
that outputs the class posterior, p = [p ,...,p ] = P ( x) with components:
1 K r
· |
P (H ,x)
r i
p = P (H x) = (14)
i r i | (cid:80)K P (H ,x)
k=1 r k
Note that this distribution is not the reference for perfect calibration of the classifier: the
righthandsideinEquation(13)isP (. q),notP (. x). Inotherwords,perfectcalibration
r r
| |
does not ensure that q is the optimal classifier—it is a weaker condition that can hold for
q = p, that is, for a suboptimal classifier.
̸
Before moving on to analyzing the calibration of q, we need to understand the implicit
perfect calibration of the optimal classifier, p. Since p is a function of x, it is also a random
variable for which the joint, marginal and conditional distributions can be derived from
P (h,x). The perfect calibration of p is defined as p = P (H p), and this equality always
r i r i
|
holds if p = P (H x). This can be seen as follows:4
i r i
|
(cid:90)
P (H ,p) = P (H ,x˜)dx˜
r i r i
x˜:Pr(·|x˜)=p
(cid:90)
= P (H x˜)P (x˜)dx˜
r i r
x˜:Pr(·|x˜)=p | (15)
(cid:90)
= p P (x˜)dx˜
i r
x˜:Pr(·|x˜)=p
= p P (p)
i r
4. Ameasure-theoretic(terse,perhapslessaccessible)derivationisgivenintheappendixofBr¨ocker(2009).
Our derivation is more similar to a derivation of likelihood-ratio calibration, in the appendix of Slooten
and Meester (2012).
10Evaluating Posterior Probabilities
where we used that P(H x˜) is constant with value p = P (H x), for the values of x˜
i i r i
| |
over which we are integrating. Rearranging, we find:
P (H ,p)
r i
P (H p) = = p = P (H x) (16)
r i i r i
| P (p) |
r
This result can be summarized succinctly as:
P ( p) = p = P ( x) (17)
r r
· | · |
This is intuitive: We have already inferred h from x in the form of p, so that if we want to
infer h directly from p, without any extra information, the result remains the same.
We have shown that p, the optimal classifier for a reference P (h,x), is perfectly cali-
r
brated with respect to the posterior distribution consistent with that reference. Now, recall
thatourclassifieristhemodelP ,thatoutputstheposteriorq = P ( x),withcomponents
c c
· |
q = P (H x). In general, our classifier will be different from the optimal classifier:
i c i
|
q = P ( x) = p = P ( x). (18)
c r
· | ̸ · |
Here and elsewhere, by ‘=’, we mean not equal in general. In contrast to P (h,x) which
r
̸
provides the full joint distribution, all we need to practically implement a classifier is the
conditional, P (h x). We can however allow the thought experiment to extend this model
c
|
to P (h,x) = P (x)P (h x). Then Equation (15) shows that the classifier is also perfectly
c c c
|
calibrated—if we use P itself as reference:
c
P ( q) = q = P ( x) (19)
c c
· | · |
What we want to do however, is to judge the calibration of q with respect to P instead.
r
Again, sinceqisafunctionofx, themodelP providesalsotheposteriorP (. q), whichwe
r r
|
will call s, against which q may be judged.5 We have now defined a total of three different
posteriors, q = s = p: ourclassifier, theclassposteriorgivenourclassifier’soutput, andthe
̸ ̸
optimal classifier, the latter two derived from the reference distribution. Their definitions
and relationships may be summarized as:
q = P ( x) = P ( q) = s = P ( q) = P ( p) = p = P ( x) (20)
c c r r r
· | · | ̸ · | ̸ · | · |
The first ‘=’ is simply because P = P . The second ‘=’ follows from the data processing
c r
̸ ̸ ̸
inequality6 if we assume the function from x to q is non-invertible. As noted above, the
literature defines perfect calibration as the special case when q = P ( q), for every x, that
r
· |
is, when the first ‘=’ in Equation (20) is replaced with equality. Even then, we still have the
̸
second ‘=’, which highlights the fact that a perfectly calibrated classifier is not necessarily
̸
optimal.
(cid:82)
5. Pr(h|q)∝Pr(h,q)= x˜:Pc(·|x˜)=qPr(h,x˜)dx˜.
6. With Pr as reference, the data processing inequality states that I(h;x) = I(h;p) ≥ I(h;q), where I
denotes mutual information (Cover and Thomas, 2006). While p contains all of the information about
hthatispresentinx,qgenerallycontainsless. Thenon-invertiblefunctionx(cid:55)→pisalsosubjecttothe
data processing inequality, but in this special case, equality is given by Equation (15).
11Ferrer et al.
TheclassifierP isoptimal withrespect tothereference, ifP ( x) = q = p = P ( x).
c c r
· | · |
But this is a much stronger requirement than merely having perfect calibration, as defined
by Equation (13). While q = p for all x implies perfect calibration by Equation (17), the
converse is not true. An extreme counterexample is the naive classifier, say P = P , that
c 0
completely ignores its input, while having perfect calibration:
q = P (H x) = P (H ) = P (H q) (21)
i 0 i r i r i
| |
where the first two equalities define P : it outputs the class prior as given by the reference
0
P , irrespective of the value of x. The last equality, which shows that P has perfect
r 0
calibration, follows because q is constant. The naive classifier is just an example of a
perfectly-calibrated but useless classifier that highlights the problem with using calibration
metrics to assess performance of probabilistic prediction. Calibration metrics do not reflect
the quality of the posteriors. Instead, EPSRs are the right tool for that job.
2.4.1 Calibration and Bayes decisions
Most machine learning classifiers make their final decisions based only on the classifier
output, q. This means that the x in Equation (1) is given by q, since those are the input
features available for decision making. Replacing x with q in Equation (1), we get that the
Bayes decisions based on q are given by
K
(cid:88)
d (q) = argmin C(H ,d)P (H q) (22)
B i r i
|
d
i=1
Thosedecisionsminimizetheexpectedcostwithrespecttothatsamereferencedistribution,
P .
r
According to the definition of calibration given by Equation (13), a system is perfectly
calibrated with respect to P if P (H q) = q . This means that, for a perfectly calibrated
r r i i
|
system, we can simply plug in q in place of the posterior in the equation above to get
i
optimal decisions. In other words, Bayes decisions made with posteriors provided by a
perfectly calibrated system are the best possible decisions, as long as both calibration and
optimality of the decisions are judged with respect to the same distribution, P . No other
r
strategyformakingdecisionsbased on the system’s output wouldresultinabetterexpected
cost. On the other hand, if we had access to the system’s input features, there could very
well exist a better decision strategy. A calibrated system does not guarantee that the Bayes
decisions are the best that can be made with our system’s input, x. Rather, it guarantees
that they are the best that can be made with our system’s output, q. If the system is poor,
the decisions will be poor, regardless of how well calibrated it may be.
2.4.2 Calibration transformations
The posterior P (H q) in Equation (13), which we have called s (Equation 20), can be
r i i
|
interpretedasatransformationoftheclassifier’sposteriorq = P ( x),intoaposteriorthat
c
· |
isperfectlycalibratedwithrespecttoP . Toseethis, wecanusetheproofinEquation(15),
r
substitutingp sandx q,tofindthatP (H s) = s . Thatis,sisperfectlycalibrated.
r i i
→ → |
For this reason, s = P (. q) is called a calibration transformation. This transformation
r
|
needs to be learned from data.
12Evaluating Posterior Probabilities
For binary classification tasks, the calibration transformation can be constructed by
collecting pairs q,h for several samples from the task. Noting that q = [q ,1 q ], we can
1 1
−
quantize q into, say, 10 bins and compute the frequency of each class h on each bin to
1
obtain P (h q). As we will see, this is the basis for the computation of the ECE metric.
r
|
Quantizing and counting, though, is often problematic, as we will discuss later in this work,
and is not appropriate for multi-class problems. Fortunately, a large variety of calibration
transformations that do not rely on quantization have been proposed in the literature over
the last decades. A review of these approaches can be found, for example, in the work by
Filho et al. (2023).
One of the most standard calibration approaches consists of applying an affine transfor-
mation to the logarithm of the posterior vector, training the parameters of this transforma-
tiontominimizethecross-entropy. Sincethecross-entropyisastrictEPSR,minimizingthis
lossguidestheparametersoftheaffinetransformationtowardvaluesthatproducegoodpos-
teriors. Instances of this approach are linear logistic regression, also known as Platt scaling
for binary classification (Platt, 2000), an extension of Platt scaling for the multi-class case
(Bru¨mmer and van Leeuwen, 2006), and temperature scaling which is a single-parameter
versionofthelattermethod(Guoetal.,2017). Inourexperiments,wewillusethedirection-
preserving (DP) transformation proposed by Bru¨mmer and van Leeuwen (2006) where the
transformed posterior s = R(q) is given by
s = softmax(αlog(q)+β) (23)
where q, s, and β are vectors of dimension K, the number of classes, and α > 0 is a scalar.
When taking β = 0, this transformation reduces to temperature scaling7.
For the binary classification case, it is possible to obtain a monotonic transformation of
one of the two posterior probabilities output by the system that leads to the lowest EPSR
value on the test data itself. This transformation can be obtained with the pool-adjacent-
violator (PAV) algorithm (Ayer et al., 1955). The EPSR resulting after this transformation
can be seen as the minimum EPSR possible on that test dataset that does not change the
ranking of the posteriors. Interestingly, the transformation is simultaneously optimal for all
EPSRsregardlessofwhichoneisusedtoobtainit(AhujaandOrlin,1998;Bru¨mmer,2010).
This minimum is likely not achievable on any other dataset, though, since the transform is
non-parametric and overfits easily.
To train any calibration transform, a dataset of q values obtained by running the classi-
fier on a set of samples needs to be created. For supervised approaches like those mentioned
above, the class, h, for each sample is also required. It is important to note that these sam-
ples should not be extracted from the dataset used to train the classifier we are aiming to
calibrate. The posteriors that the classifiers produce on their training samples are, for most
modern models, already well-calibrated. Yet, unless no degree of overfitting occurred dur-
ing training, the distribution of those posteriors is not representative of the distribution on
unseen data, which is where we wish the calibration transform to perform well. Hence, the
data used to train the classifier should not be used to train the calibration model. Ideally,
a fraction of the available training data should be held-out when training the classifier to
7. Temperature scaling, as defined by Guo et al. (2017), takes the pre-softmax activations of a neural
networkasinput,insteadofthelogposteriorasinEquation(23). Yet,inbothcasesthefinalcalibrated
output is transformed by the softmax function, making both expressions equivalent.
13Ferrer et al.
be used as training data for the calibration model. It is particularly important that this
data is representative of the one we expect to see when the system is deployed since cali-
bration performance appears to be more sensitive to domain mismatch than discrimination
performance (Ferrer et al., 2021).
2.5 Calibration metrics
Equation (13) gives the definition of perfect calibration with respect to a reference distri-
bution. It does not provide a calibration metric, that is, a way to measure a degree of
(imperfect) calibration. In this section we describe three calibration metrics: the expected
divergence, commonly studied in theoretical papers; the expected calibration error, com-
monly used in empirical machine learning papers; and the calibration loss, our proposed
metric, which we believe has various advantages and no disadvantages over both of those
well-established metrics.
2.5.1 Expected score divergence
Much of the literature on calibration solves the problem of measuring the degree of im-
perfect calibration by defining a divergence between the left-hand side and the right-hand
side in Equation (13), where the left-hand side is given by the output of the system under
evaluation, q, and the right-hand side is given by the reference posterior, s. A permissive
definition of a divergence is a non-negative function d(s,q) that is zero if s = q. A strict
divergence is zero only at s = q and positive, or infinite otherwise. Then, a class of metrics
that evaluate the goodness of the calibration of a probabilistic classifier P , relative to a
c
reference model P , can be defined as the expectation of the divergence with respect to the
r
reference distribution (Br¨ocker, 2009):
D(P ,P ) = E [d(R(q);q)]. (24)
r c Pr(q)
where we defined R(q) = s, making explicit the fact that s is a function of q.
Interestingly, it can be shown that EPSRs (Equation 5) can be decomposed into an
expected divergence and a generalized entropy term (DeGroot and Fienberg, 1983; Br¨ocker,
2009; Bru¨mmer, 2010; Gneiting and Raftery, 2007; Dawid, 2007):
E [C∗(h,q)] = E [d(R(q);q)]+E [C∗(h,s)] (25)
Pr(h,q) Pr(q) Pr(h,s)
where
d(s,q) = E [C∗(h,q) C∗(h,s)]. (26)
h∼s
−
which satisfies the conditions listed above. That is, it is zero if s = q, and non-negative
otherwise. This last property derives directly from the defining property of PSRs, Equa-
tion (3). When the PSR is strict, the divergence is zero if and only if s = q. A divergence
that is induced by a PSR as in Equation (26) is called a score divergence (Ovcharov, 2015).
The score divergence corresponding to the Brier loss is the squared Euclidean distance,
(cid:80) (s q )2, and the one corresponding to the logarithmic loss is the Kullback-Leibler
i i − i (cid:80)
divergence, s log(s /q ).
i i i i
14Evaluating Posterior Probabilities
Theright-mostterminEquation(25)istheEPSRofswhich, forthelogarithmicloss, is
theentropyofthisdistribution. Thistermmeasuresthediscriminationorrefinementofq: it
is the EPSR that is obtained after solving (as best as we could) any miscalibration problem
by adding the calibration transform s to the system. Hence, Equation (25) expresses the
overall performance of the system, given by the EPSR on the left-hand side, as a sum of a
calibration and a discrimination term. Note, though, that all terms in the decomposition
depend on P , a model that needs to be learned from data. Different models will lead to
r
different values for all three terms. While this may be unsettling, it is an issue inherent
to the fact that the true distribution is not known in practice and, hence, it applies to all
metrics designed to assess the performance of posterior probabilities.
The decomposition in Equation (25) is theoretically appealing but it is problematic in
practice. The two EPSRs in this equation are given by expectations taken with respect
to a reference distribution consistent with s. In particular, P (h,q) should be given by
r
P (q)P (h q) with the second factor given by s, and in order for the equality to hold.
r r
|
Hence, the left-hand side, which corresponds to the overall performance of the classifier,
depends on the output of a calibration model, s. If this model is poor, the assessment of
the system performance may be suboptimal (examples of this problem are given in Section
3.4).
In contrast, the common practice in the literature is to take the expectations of PSRs
with respect to the empirical distribution, that is, as the average PSR over the data. Unfor-
tunately, doing this in Equation (25) leads to a meaningless decomposition. The posterior s
derived from the empirical distribution consists of a one-hot vector with a one at the index
corresponding to the sample’s true class for the q values obtained on the dataset and un-
defined values for any other value of q. This s will, for most systems, have an entropy term
of 0 (unless two samples from different classes have exactly the same q value) suggesting
that the system has perfect discrimination. Yet, such s would certainly not result in zero
entropy on any other dataset.
In order for the decomposition in Equation (25) to be meaningful, s needs to be a good
predictive model. That is, a model that generalizes well to unseen data. The empirical
distributiondoesnotsatisfythisconditionand,hence,cannotbeusedforthedecomposition.
Perhaps for this reason, the divergence/entropy decomposition of EPSRs is not commonly
used in empirical papers. As we will see in Section 2.5.3, our proposed calibration loss
metric solves this problem.
2.5.2 ECE: Expected calibration error
The most common calibration metric in current machine learning literature is the expected
calibration error (ECE) (for example, Guo et al., 2017; Liang et al., 2023; Mu¨ller et al.,
2019; Ovadia et al., 2019), which, as we will see, is a variant of the expected divergence
described above. This metric was proposed by Naeini et al. (2015a) as a metric to assess
the calibration quality of binary classification systems. To compute the ECE, the posteriors
for class H for all the samples in the test dataset are binned into M bins. The ECE is
2
then computed as (Guo et al., 2017):
M
(cid:88) B m
ECE = | | class2(B ) avep2(B ) (27)
m m
N | − |
m=1
15Ferrer et al.
whereB istheset ofsamplesforwhichtheposteriorforclassH isinthemthbin, B is
m 2 m
| |
the number of samples in that bin, class2(B ) is the fraction of samples of class H within
m 2
that bin, and avep2(B ) is the average posterior for class H for the samples in that bin.
m 2
TheexpressionabovecanberewrittentolooklikethedivergenceinEquation(24), with
the expectation taken with respect to the empirical distribution, by replacing B with a
m
| |
sum over all samples in that bin:
N
1 (cid:88)
ECE = d(s(x ),qˆ(x )) (28)
t t
N
t=1
where qˆ is such that qˆ (x ) = avep2(B ), and s is such that s (x ) = class2(B ), where
2 t mt 2 t mt
B is the bin corresponding to q (x ), the output of the system for class H . The s
mt 2 t 2
defined this way corresponds to a very common calibration method called histogram bin-
ning (Zadrozny and Elkan, 2001; Naeini et al., 2015b). To obtain the ECE defined by
Equation (27), the distance d should be defined as d(s,q) = s q . Unfortunately, the
2 2
| − |
absolute distance is not a score divergence, that is, it is not induced by any PSR. To see
this, we can use the fact that all score divergences are Bregman divergences and Bregman
divergences satisfy the following property (Ovcharov, 2015):
E[s] = argminE[d(s;q )] (29)
0
q0
where q is a fixed vector. That is, the divergence between s and a fixed posterior vector
0
is minimized when that vector is equal to the expectation of s. It is easy to find coun-
terexamples (see Appendix B) where this property is not satisfied for the absolute distance
and, hence, we can conclude that the absolute distance is not induced by any PSR. As a
consequence, the standard form of the ECE cannot be decomposed as in Equation (25).
The histogram binning calibration transformation used to compute the ECE is, in all
works we found that used this metric, always trained on the test data itself. If this trans-
formation overfits the data, the ECE will be overestimated. Alternative versions for the
ECE have been proposed where the bins are adapted to contain equal number of samples,
showing that the resulting ECE is less prone to overfitting (Nixon et al., 2019). Yet, this
only mitigates the problem, without fully solving it. A direct solution to the problem is to
treat the calibration transform like any other stage of the system, estimating its parameters
on held-out data or through cross-validation on the test data. This, as far as we know, has
never been done in the literature where ECE is computed. Further, histogram binning is
not necessarily a good calibration transform for every problem (we will show examples of
this in the experimental section). If the calibration transform used to compute the cali-
brated posteriors is not a good match for the problem, then the calibration error will be
underestimated since the calibrated posteriors will be poorer than they could be. Again,
the ECE could be computed using a different calibration transformation, but this it not
what is done in the literature where the ECE definition is always tied to using histogram
binning as transform.
Finally, perhaps the biggest weakness of the ECE appears when it is used for multi-class
problems. Inthatcase,inordertobeabletocontinueusinghistogrambinningascalibration
transform, the multi-class problem is mapped to a new binary problem where only the
16Evaluating Posterior Probabilities
quality of the confidences is evaluated, ignoring all other values in the posterior vector
(Nixon et al., 2019). The confidence is the posterior corresponding to the class selected by
the system which, in the literature that uses ECE is taken to be the one with the maximum
posterior. Thiseffectivelymapsthemulti-classproblemintoabinaryclassificationproblem:
deciding whether the system was correct in its decision by using its confidence as the
posterior for correctness. Given this new problem, one can now use the definition for the
ECE, where class2 is the fraction of samples correctly classified by the system and avep2 is
theaverageconfidence. Wecallthemulti-classversionofECE,ECEmc. Asaconsequenceof
the fact that ECEmc only evaluates confidences rather than the full posterior, it may fail to
diagnose calibration problems on any of the other components of the posterior distribution
(seeSection3.3). Whenevergoodposteriorsarerequired,thefullvectorofposteriorsshould
be evaluated and not just its maximum value (Popordanoska et al., 2022). Notably, in some
works, including (Guo et al., 2017), the multi-class definition of ECE is unnecesarily used
for binary problems.
While the expression for the ECE in Equation (27) is the one used in most machine
learning empirical papers, a more general form for the ECE is sometimes used in theoretical
papers where the quantization of the system’s output is not done, and the divergence is a
general function that satisfies d(s,q) = 0 when s = q and positive otherwise (Widmann
et al., 2019). Setting d to be the square euclidean distance, the ECE directly corresponds
to the expected divergence for the Brier loss, when s is obtained by histogram binning. In
fact, the transform could also be changed to a general transform in which case the ECE
simply coincides with the expected divergence in Equation (24). As we discussed above,
though,theexpecteddivergencecorrespondstoadecompositionoftherawEPSRwherethe
expectation is taken with respect to a distribution based on a calibration model, making it
a rather impractical decomposition. The calibration loss proposed in the next section aims
to solve this problem.
2.5.3 Calibration loss
As discussed earlier in this work, in our view, the main goal of calibration metrics should
be to diagnose calibration problems during system development. We then propose the
followingprocedureforcomputingacalibrationmetricforthatpurpose: 1)addacalibration
transformattheoutputofthesystem, and2)assessthelevelofimprovementobtainedfrom
this additional stage. The level of improvement, measured as the difference in EPSR before
and after adding the calibration stage, indicates the degree of miscalibration of the system.
We call this metric calibration loss. If the calibration loss is large relative to the original
EPSR value, then it means the system was poorly calibrated and would benefit from the
addition of the proposed calibration stage. If the improvement is relatively small, then it
meansthatthesystemisalreadywellcalibrated. Alternatively,asmallrelativeimprovement
may be due to the calibration transform being poor. This is the same problem discussed
several times above. In order to measure calibration, a reference distribution needs to be
created first. The quality of the calibration metric will strongly depend on the quality of
this reference distribution. No calibration metric is immune to this issue.
Note that the process to obtain the calibration loss is, essentially, what is done at every
step during system development: a new approach is tried (in this case, the addition of a
17Ferrer et al.
calibrationstage)anditsbenefitisassessedbymeasuringtherelativeimprovementobtained
compared to the baseline (in this case, the system without the calibration stage). As for
every other development process, the assessment of the gains will be more reliable if the
data where the performance is computed is not used for training or tuning purposes. When
computing calibration loss, this means that the test data should not be used to train the
calibration transform. This approach which, as far as we know, was never proposed in the
calibration literature, ensures that the estimation of the calibration error will not be biased
by overfitting effects.
Formally, we define the calibration loss (CalLoss for short) as:
CalLoss = EPSR EPSR (30)
raw cal
−
where EPSR and EPSR are the two EPSRs in Equation (25) with the expectation
raw cal
taken with respect to the empirical distribution, that is, computed as averages over the
data, as in Equation (6):
N
1 (cid:88)
EPSR = C∗(h ,q ) (31)
raw t t
N
t=1
N
1 (cid:88)
EPSR = C∗(h ,s ) (32)
cal t t
N
t=1
These two EPSRs measure the overall performance of the posteriors with (EPSR ) and
cal
without (EPSR ) the addition of the calibration stage, s. As we discussed with regards
raw
to Equation (25), this latter EPSR can be seen to measure the inherent discrimination
performance of the system since it is the EPSR that remains after the miscalibration has
been fixed. Again, this holds as long as the calibration transform is doing a reasonable job
at reducing the miscalibration.
For ease of interpretation, in our experiments we report relative CalLoss, RCL for short:
RCL = 100 (EPSR EPSR )/EPSR . (33)
raw cal raw
−
The RCL reflects the relative improvement in EPSR that we would be able to achieve by
adding a post-hoc calibration stage given by s to our system.
CalLossdiffersfromthedivergencedefinedbyEquation(24)inthewaytheexpectations
of the PSRs are computed on both sides. Instead of computing them with respect to a
distribution consistent with s they are computed with respect to the empirical distribution.
As a consequence, the calibration term is no longer an expected score divergence. Instead,
it is simply the difference between the two empirical EPSRs. As we will show in the
experimental section, as long as s is a good calibration model, the EPSRs computed as in
Equation (24) or as in Equation (30) are very similar, resulting in the same value for the
calibration metric, which is the difference between the two EPSRs. On the other hand,
when s is not a good calibration model, the left-hand side in Equation (25) can be a very
poor estimator of the classifier’s performance, resulting, as a consequence, in a meaningless
decomposition. In contrast, EPSR computed as in Equation (31) does not depend on
raw
a calibration model, resulting in a more reliable measure of the overall performance of the
18Evaluating Posterior Probabilities
classifier. Evenifthecalibrationmodelispoor, thecalibrationlossvalueisstillmeaningful:
it indicates the improvement in EPSR that can be achieved by using that same calibration
model as part of the system.
A specific instance of the calibration loss metric was proposed almost two decades ago
for the binary classification problem of speaker verification by Bru¨mmer and du Preez
(2006). This metric which, as far as we know, has since only been used for the speaker
verification and forensic applications (Ramos et al., 2017, 2020), is a special case of our
proposed approach above for binary classification where the calibration transform is given
by the best isotonic transformation obtained on the test data itself. In this work, we
consider a general form of the calibration loss where the transform can be adapted to the
problem of interest and, preferably, trained on held-out data rather than on the test data
itself. This generalization allows us to use the metric for multi-class problems where the
isotonictransformationisnotapplicableandtoavoidthebiasproducedbythetrain-on-test
approach.
3 Experiments
In this section we show an analysis of the metrics discussed in the previous section using
both synthetic and real datasets. Synthetic datasets allow us to have the ground-truth
distribution, which in turn allows us to obtain perfectly-calibrated posteriors where we
would expect any good calibration metric to give a value of zero. As we will see, this does
notalwayshappen, allowingustorevisitsomeofthetheoreticalissuesdiscussedinprevious
sections.
For synthetic data, the procedure for generating calibrated and miscalibrated posteriors
isdescribedinAppendixC.Briefly, N samplesaregeneratedbysamplingtheinputfeatures
from a multivariate Gaussian distribution for each class. The number of samples generated
for each class is determined by an imbalanced prior distribution with a prior for class H
1
of 0.8 and equal prior for other classes, unless otherwise noted. The per-class multivariate
GaussiandistributionsaredeterminedsuchthatthemeansareequidistantatanL2distance
of 1. The covariance is diagonal with same variance for all dimensions and shared across
all classes and it controls the performance of the resulting posteriors. We set the variance
to 0.15 unless otherwise indicated. Perfectly calibrated posteriors are obtained using the
known feature distribution for each class. This is the optimal classifier for this data since it
is derived from the generating distribution. Then, threemiscalibrated versions of thesepos-
teriors are created, one by artificially manipulating the prior distribution to be mismatched
to that in the data (mcp), another one by scaling the logarithm of the posteriors and renor-
malizing the result to produce overconfident or underconfident posteriors depending on the
scale (mcs), and a final one combining both causes of miscalibration (mcps).
InSection3.1through3.4weshowanddiscussresultsfordatasetswith2and10classes,
including first an illustration of the property described in Section 2.3.4 and then focusing
on the comparison of various EPSRs and calibration metrics. Finally, in Section 3.5, results
on real datasets are presented and discussed, showing the effect of post-hoc calibration on
the different metrics on posteriors produced by actual systems.
Inpractice,itisimportanttoassesstherobustnessoftheperformanceestimateobtained
on our test dataset, specially when the number of samples is small. Appendix E includes
19Ferrer et al.
an example on how to obtain confidence intervals using bootstrapping. We do not include
confidence intervals in the results in this section to keep both the code that produces them
and the plots simple.
3.1 Illustration of EPSR construction as integrals over Bayes risks
In this section we illustrate the construction of CE and BS as integrals over Bayes risks (see
Section 2.3.4) using synthetic posteriors for a 2-class dataset. We generate posteriors for
four different classifiers as described above and further detailed in Appendix C:
• cal: Perfectly calibrated system obtained with a per-class variance of 0.15.
• mcs-u: Miscalibrated system obtained by scaling the log posteriors from the cal
system by 0.48 to simulate an underconfident system, and then converting the result
back into posteriors by doing softmax.
• mcs-o: Same as mcs-u but scaling the log posteriors by 2.0, to simulate an overcon-
fident system.
• cal-h: Same the cal system but with a variance of 0.19 to obtain a harder dataset.
In all cases, the prior for class H is set to 0.6 and the total number of samples is set to
1
100,000.
The left plot in Figure 1 shows the Bayes risk for those four systems for cost matrices
given by
(cid:20) (cid:21)
0 1/a
1
C = (34)
a
1/(1 a ) 0
1
−
while varying the value of a between 0 and 1. The CE for each system is simply the area
1
under the corresponding curve. Systems cal, mcs-u, and mcs-o have the same Bayes risk
for a = 0.5, which corresponds to the total error rate multiplied by 2. The corresponding
1
Bayes decisions are given by the argmax rule. Since mcs-u and mcs-o are constructed by
scaling the log posteriors of the cal system, which does not affect the argmax decisions, the
total error rate is the same for all three systems. Yet, since the Bayes risk at other values of
a differsacrosssystems,theCEalsodiffers,beinghigherforthetwomiscalibratedsystems.
1
Finally, we can see that the calibrated posteriors for the harder dataset, cal-h, have a worse
total error rate than the other three systems, but better Bayes risk than the mcs posteriors
at extreme values of a . Overall, integrating the curves for mcs-u, mcs-o and cal-h leads to
1
(cid:80)
the same NCE values (recall that NCE = CE/( P logP ), with P = 0.6 and P = 0.4
i i 1 2
−
in these synthetic datasets).
The right plot in Figure 1 shows the Bayes risk multiplied by W(s) = a (1 a )/β(2,2)
1 1
−
so that the area under these curves corresponds to the BS. We can see that the BS de-
emphasizes the performance of the system at extreme values of a . Note that the Bayes
1
decisions corresponding to the cost matrix defined by Equation (34) are given by thresh-
olding the posterior for class H at a threshold of a (this can be derived by replacing
1 1
C(H ,D ) = 1/a and C(H ,D ) = 1/(1 a ) in Equation (1)). Hence, the operating
1 2 1 2 1 1
−
points that are deemphasized by BS are those corresponding to extreme threshold values.
20Evaluating Posterior Probabilities
0.7 0.7
cal,NCE=0.34 cal,NBS=0.29
0.6 0.6
mcs-u,NCE=0.42 mcs-u,NBS=0.34
0.5 mcs-o,NCE=0.42 0.5 mcs-o,NBS=0.31
cal-h,NCE=0.42 cal-h,NBS=0.36 0.4 0.4
0.3 0.3
0.2 0.2
0.1 0.1
0.0 0.0
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
a a
1 1
Figure 1: Weighted Bayes risk curves for four different systems. Left: the weight is
1.0 so that the integral under these curves is the CE. Right: the weight is
a (1 a )/β(2,2) so that the integral is the BS. The normalized CE, NCE,
1 1
−
and normalized BS, NBS, are shown in the legends.
Poor posteriors in those regions will be penalized much less by BS than by CE. A similar
conclusions can be derived from comparing the log-loss expression corresponding to the CE
and the squared loss expression corresponding to BS. While the squared loss is bounded by
one, which happens when the posterior for the true class is zero, the log-loss gives infinite
penalization to those errors. This difference between CE and the BS results in a difference
in ranking of the four systems according to each metric. While systems mcs-u, mcs-o, and
cal-h are identical in terms of CE, they are not so in terms of BS due to the different
weighting given to each operating point.
As is clear from this example, different EPSRs may result in different development
decisions. Hence, it is important to choose an EPSR that correctly reflects the needs of the
application. Expression (10) provides an intuitive way of creating new PSRs, allowing us
to select a weight function W appropriate for the task. By default, though, if no specific
needs are identified, the uniform weighting function, which results in the CE metric, is a
good general choice. The BS may not be a desirable choice for high-stakes application due
to the effect observed in Figure 1 where the behavior of the risk for extremely imbalanced
cost matrices is deemphasized. This means that if a system produces poor posteriors in the
extremes(veryhighorverylowvalues),theBSwillnotcorrectlydiagnosetheproblem. The
CE, on the other hand, will severely penalize such systems, making it a more appropriate
choice for high-stakes applications where having good extreme posteriors is important.
TheplotsinFigure1alsohelpusillustrateourargumentabouttherolethatcalibration
should (or, rather, should not) play in the evaluation of posteriors. While system mcs-o is
miscalibrated, it is better than system cal-h which is perfectly calibrated in terms of BS
(and the same in terms of CE). If we had decided that BS is the EPSR of choice for our
problem, there would be no reason to select the cal-h system over the mcs-o system, despite
the fact that the latter system is miscalibrated. If we had the chance to add a post-hoc
calibrationstagetooursystem, wecouldcomputeacalibrationmetrictoseethatthemcs-o
posteriors are miscalibrated which would indicate that a calibration stage is needed. Yet,
after adding that stage, the performance of the system should again be assessed in terms
of its new EPSR.
21
ksiR a
)2,2(β/)
a
1(
a
ksiR
1 −
1
∗aFerrer et al.
2.0 100
cal mcp mcs mcps
1.5 75
50
1.0
25
0.5
0
0.0
NRisk-01 NRisk-01r NRisk-im NCE NBS RCL-BS-DxvRCL-BS-Hxv RCL-BS-Dtt RCL-BS-Htt ECEmc ECE
Figure 2: Various metrics for a binary classification task for four different systems, one
perfectly calibrated (cal) and three miscalibrated ones (mcs, mcp, mcps). Left:
normalized overall performance metrics. The dashed line indicates the perfor-
mance of a naive system. Right: calibration metrics including the binary ECE,
the multiclass ECE (ECEmc), and the RCL based on BS for two calibration
approaches, DP (D), and histogram binning (H), trained either through cross-
validation on the test data (xv) or training on the full test dataset (tt).
3.2 Results on a Binary classification problem
The left plot in Figure 2 shows five different metrics for the 2-class synthetic dataset, using
theproceduredescribedinAppendixCtoproduce400samples. Wechosetousearelatively
small dataset to show the effect that overfitting of the calibration transform may have in
someofthemetrics. Themetricsincludenormalizedcross-entropy(NCE),normalizedBrier
score (NBS), and three normalized Bayes risks, with cost matrices given by 1) the 0-1 cost
matrix (NRisk-01), 2) the 0-1 cost matrix with an additional column corresponding to a
reject decision with cost of 0.1 for both classes (NRisk-01r), and 3) a square imbalanced
cost matrix with c = 1 and c = 10 (NRisk-imb). The accuracy for each system can be
12 21
computed as 1 0.2 NRisk-01, since NRisk-01 = Risk-01/0.2, where Risk-01 is total error
−
rate.
The results show that NRisk-01 is unaffected by the miscalibration in mcs, a scaling
in the logarithm of the posteriors which results in overconfident posteriors. This happens
because the argmax decisions, which are the Bayes decisions for this cost function, are not
affected by the scaling. NRisk-01r and NRisk-im, on the other hand, are affected by this
type of miscalibration, with NRisk-im being less affected than NRisk-01 by the mismatch in
priors in mcp. As expected, we see that the risks have different behavior depending on the
cost matrix, highlighting the importance of correctly selecting this matrix for the problem
of interest when categorical decisions are required.
The three risks in the figure are EPSRs since they are computed using Bayes decisions.
Yet, they are not strict PSRs. They only assess the performance of the posteriors for one
specific operating point given by the cost matrix, which corresponds to a set of decision
regions determined by Equation (1). The decisions regions are, in turn, used to make
categorical decisions which are then used to compute the risk. Hence, two systems that
results on the same categorical decisions for a given cost matrix would have the same risk
value, while one may have better posteriors than the other for other cost matrices.
If we want to assess the goodness of the posteriors in general, across the full simplex, we
need to use the expectation of strict PSRs, like the CE or BS. These two metrics assess the
22Evaluating Posterior Probabilities
goodness of the posteriors without going through the step of making categorical decisions.
As discussed in Section 3.1, despite both metrics being strict PSRs, they may result in
different conclusions about the quality of the posteriors. This can be observed in the left
plotinFigure2wherethenormalizedCEandBSmetricsshowaratherdifferentassessment
of the quality of the mcs and mcps systems. This happens because the overconfident
posteriors from the mcs and mcps systems are pushed to the extremes, where the BS pays
little attention.
The right plot in Figure 2 shows various calibration metrics: binary ECE, multiclass
ECE,andvariousRCLmetrics. TheRCLmetricsarecomputedusinghistogrambinningor
DPcalibrationtrainedwithcross-validationoronthetestdatatoobtainEPSR (Equation
cal
32). We can see that the histogram binning approach is problematic for the cal and mcs
datasets. When using cross-validation to train it (as for RCL-BS-Hxv), the transform
overfits the training data for each fold resulting in bad calibration when applying that
transform on the test data for that fold. Hence, the posteriors are actually worse than the
original ones, resulting in a negative calibration loss, something that should never happen
for a well-designed well-trained calibration transform. When training on the test data (as
for RCL-BS-Htt), the transform overfits the data and the metric diagnoses a non-existing
calibration problem on the cal posteriors. These results show that, for this particular
problem, histogram binning is a poor calibration approach. In contrast, the RCL values
obtained with the DP transform are quite robust for both training approaches, since the
number of parameters in this transform is small and it is much less prone to overfitting.
Both of these RCL metrics correctly diagnose the calibration problem in the mc posteriors,
and the correct calibration in the cal posteriors. The value of these metrics tell us the
relative improvement in BS that we would get from adding an DP calibration stage to
the system and, hence, directly quantify how badly calibrated the original posteriors are
without this stage.
Note that, given the way the posteriors are generated in our synthetic datasets, the DP
transformation is, by design, able to perfectly reverse the miscalibration present in those
posteriors. In other datasets, the DP transformation may not be necessarily optimal since
misscalibration can potentially have non-linear effects in the posteriors. Other calibration
transformations available in the literature could be explored in those cases. In our experi-
ence, though, DP calibration has given excellent results across a variety of tasks. We will
see further evidence of this in Section 3.5. As discussed in the next section, this is not the
case for temperature scaling, which fails in cases where the prior distribution in the training
and the test data are different, a common scenario in many applications.
Turning to the ECE metrics, we can see that ECEmc gives a different estimate of the
calibrationerrorthantheoriginalbinaryECE,whichisexpectedsinceECEmcisevaluating
theperformanceofadifferentbinaryproblem,asexplainedinSection2.5.2. Wecanalsosee
thattheECEshowsasimilartrendastheRCL-BS-Httsincethesetwometricsusethesame
calibratedposteriorsobtainedwithhistogrambinningtrainedonthetestdata,differingonly
inthewaythedistancebetweentherawposteriorsandthecalibratedposteriorsiscomputed
(seeSection2.5.2). Hence,theECEsuffersfromthesameproblemasRCL-BS-Httdiscussed
above, mistakenlydiagnosingasmallcalibrationprobleminthecalposteriors. Finally, note
that the value of the ECE does not have a clear interpretation, while, as discussed above,
23Ferrer et al.
3 100
cal mcp mcs mcps
75
2
50
1 25
0
0
NRisk-01 NRisk-01r NRisk-im NCE NBS RCL-CE-DxvRCL-CE-TxvRCL-BS-DxvRCL-BS-Txv ECEmc
Figure 3: Overall and calibration metrics as in Figure 2 but for a 10-class classfication task.
theRCLvaluesdirectlyindicatethepercentageoftheEPSRoftheoriginalposteriorswhich
can be reduced by doing calibration, providing a more actionable result.
3.3 Results on a Multi-class classification problem
In this section we show results for a multi-class classification task instead of a binary task
as above, including additional versions of the calibration loss, using CE as the EPSR as
well as BS, and using temperature scaling as well as DP calibration to obtain the calibrated
posteriors needed to compute that metric. The dataset was created following the process
described in Appendix C with K = 10 classes, feature variance of 0.08, and a total number
of samples N = 2000.
TheleftplotinFigure3showstheresultsforvariousoverallmetrics. Thethreerisksare
a direct generalization of the ones for the 2-class case: 1) the 0-1 cost matrix (NRisk-01),
2) the 0-1 cost matrix with an additional column corresponding to a reject decision with
costs equal to 0.1 for all classes (NRisk-01r), and 3) a square cost matrix with imbalanced
costs, c = 1 for all i = j, except when i = 10 in which case c = 10 (NRisk-imb). The
ij ij
̸
conclusionfromtheseresultsissimilartothatforthebinarycase: eachriskgivesadifferent
ranking of systems, and the NCE is more severely degraded by the scaling miscalibration
in mcs and mcps than the NBS.
The right plot in this figure shows the relative CalLoss (RCL) based on BS and CE
for two calibration methods, DP calibration and temperature scaling trained with cross-
validation,andthemulticlassECE.Theseresultsshowalargedifferencebetweensomeofthe
calibration metrics. Those that use temperature scaling for calibration fail to diagnose the
severity of the calibration problem in mcp. This is because this method cannot compensate
for calibration problems due to mismatched priors and, hence, it also cannot be used to
diagnose them. As any other calibration metric, CalLoss fails if the calibration transform
is not a good match for the problem. Yet, the CalLoss framework allows us to explore a
variety of transforms. If one such transform leads to a relatively large CalLoss, and that
transform was not trained on the test data itself, then we can conclude that the system is
miscalibrated. Importantly, the same transform that was used to diagnose the problem can
be used as post-hoc calibration stage and reduce the EPSR by the CalLoss amount.
TurningtoECEmc,wecanseethatitgivesadifferentassessementoftherelativeseverity
of the miscalibration of the different datasets compared to the RCL metrics. This is partly
because it uses a different way to measure the distance between the raw and calibrated
scores—one that is not induced by any PSR—and partly because it only evaluates the
24Evaluating Posterior Probabilities
quality of the maximum posterior for each sample rather than the full vector. When the
miscalibration occurs on classes other than the one with maximum posterior, the ECEmc
metric cannot properly diagnose the problem. This issue with the multi-class ECE has
also been discussed by Nixon et al. (2019). The CalLoss metric does not suffer from this
problem as it assesses the quality of the full posterior vector. In addition, note that the
scale of the ECEmc is not interpretable. While, for example, the RCL-CE-Dxv values
for miscalibrated systems indicate that over 70% of the CE is due to miscalibration, no
equivalent interpretation is possible with the ECEmc.
3.4 Effect of the reference distribution in the EPSR value
As explained in Section 2.5, EPSRs values may differ depending on the reference distribu-
tion used to take the expectation. In this section, we compare cross-entropy values (the
expectation of the NLL) given by:
E [C∗(h,q)] = E [ log(q )] (35)
Pr(h,q) Pr(h,q)
−
h
for different reference distributions: the empirical distribution in the test data used in all
prior results in this section (Equation 8) and various semi-empirical distributions where
P (q) is the empirical distribution for q, but P (h q) is given by a classifier. Since
r r
|
we are working with synthetic data, we can derive P (h q) from the true generating
r
|
distribution. This posterior, of course, would not be available on real data. Hence, we
comparewithdifferentP (h q)givenbycalibratingtheposteriorsusinghistogrambinning,
r
|
DPcalibration,andtemperaturescalingcalibrationtrainedwithcross-validationordirectly
on the test data.
Table 1 shows the relative difference between the semi-empirical cross-entropy and the
empirical cross-entropy for the same four datasets used in Figure 2 with varying number
of samples. We can see that, in most cases, the relative difference between the empirical
and the semi-empirical cross-entropy is very small, specially for the larger datasets where
the empirical cross-entropy is less noisy. Larger differences occur when the calibration
transformation does not fully fix the calibration problem, which happens with temperature
scaling for the mcs and mcps posteriors and for histogram binning in most cases, even for
the larger datasets.
These examples illustrate the practical problem involved in using the expected score
divergence as a calibration metric. If the calibration model used to obtain s(q) = P (. q)
r
|
in Equation (25) does not do a good job at calibrating the posteriors, the EPSR in the left-
hand side may be a poor estimate of the system performance. This will, in turn, result in
a meaningless decomposition with the two terms adding up to a value that does not reflect
the actual performance of the system. On the other hand, CalLoss is meaningful regardless
of the quality of the calibration model. The EPSR and EPSR used to compute it
raw cal
reflect the performance of the system before and after calibration with that same—perhaps
suboptimal—calibration model. Hence, CalLoss always measures the level of improvement
inEPSRthatcanbeobtainedfromusingtheselectedcalibrationtransform. Forthisreason,
we believe that, in practice, CalLoss is a better calibration metric than the expected score
divergence.
25Ferrer et al.
System cal mcs mcp mcps
#Samples 200 2000 20000 200 2000 20000 200 2000 20000 200 2000 20000
true -3 -5 -2 -7 -10 -5 4 -2 0 4 -2 0
Dtt 0 0 0 0 0 0 0 0 0 0 0 0
Dxv 1 0 0 1 0 0 1 0 0 0 0 0
Ttt 1 -1 0 1 -1 0 29 28 28 29 28 28
Txv 1 -1 0 2 -1 0 29 28 28 29 28 28
Htt -6 -8 -9 -153 -143 -132 -19 -21 -17 -62 -84 -82
Hxv -2 -8 -9 -171 -144 -132 -15 -21 -17 -62 -83 -82
Table 1: Relativedifferencebetweenthesemi-empiricalcross-entropy(CE )andtheempir-
se
ical cross-entropy (CE ) computed as (CE CE )/CE 100 and then rounded
e e se e
− ∗
to the nearest integer. Results are shown for synthetic posteriors as in Figure 2,
for different number of samples. The semi-empirical cross-entropy is computed
with respect to posteriors given by the true posterior (true), and by calibrated
versions of the posteriors using DP calibration (D), temperature scaling (T), and
histogram binning (H) trained with cross-validation (xv) or trained on the test
data (tt).
3.5 Results on real datasets
Inthissection,weshowresultsonrealdatasetsforanumberofdifferenttaskscorresponding
to speech, image, and natural language processing tasks. The datasets and systems used
to generate the posteriors studied in this section are described in Appendix D. For each
system, we show results obtained on the raw posteriors as they come out of the system, and
on calibrated posteriors using DP calibration trained with cross-validation on the test data.
For the binary classification tasks, we also show results for the best calibrated posteriors
obtained with the PAV algorithm run on the full test set. Table 2 shows three normalized
risk metrics, and NCE, RCL, ECE, and ECEmc for each of those systems.
Focusing on the calibration metrics on the right, we can see that both ECE metrics
often fail to diagnose calibration problems. For example, according to ECE, the SITW raw
posteriorsarecalibrated, whileweknowtheyarenotsincetheirNCEimprovessignificantly
after calibration. Similarly, while for FVCAUS the miscalibration is to blame for almost
100% of the NCE value, ECE is below 10 and ECEmc is close to 0: both ECE metrics
strikingly fail to diagnose the severity of the miscalibration of this system.
Conversely, ECEmc may suggest a miscalibration problem exists where RCL does not.
For example, for the CIFAR100 RepVgg-a2 raw posteriors, the ECEmc of 5.6 indicates
a small calibration error while the RCL of 0.8 suggest the system is very well-calibrated.
First, it is important to note that the ECEmc is not a relative measure. It is not possible
to assess the impact that an ECE value of 5.6 would have on the performance of a system.
In fact, the impact will be different depending on the overall performance of the system.
Unfortunately, this metric cannot be turned into a relative value like the RCL because
there is no corresponding NCE to use as reference. In contrast, an RCL value of 0.8 means
that only 0.8% of the NCE value is due to miscalibration. Further, note that ECEmc only
assesses the performance of the maximum posterior for each sample while RCL assesses the
performance of the full vector of posteriors. The ECEmc would tend to overestimate the
26
sroiretsopEvaluating Posterior Probabilities
NRisk Calibration
Dataset System proc C01 Cab Cimb NCE RCL ECEmc ECE
raw 0.996 0.812 1.000 1.073 62.4 34.4 34.8
SST2 GPT2-4sh
cal 0.226 0.585 0.711 0.404 -0.6 1.6 2.2
calp 0.223 0.555 0.660 0.385 -0.4 0.0 0.0
raw 0.828 0.818 1.000 0.917 46.0 20.0 27.3
SST2 GPT2-0sh
cal 0.310 0.655 0.685 0.495 -0.6 1.4 1.5
calp 0.298 0.638 0.661 0.478 -0.4 0.0 0.0
raw 0.324 0.225 0.191 0.189 16.7 0.2 0.2
SITW XvPLDA
cal 0.306 0.190 0.153 0.158 -0.1 0.0 0.0
calp 0.304 0.188 0.152 0.155 -0.0 0.0 0.0
raw 3.915 1.992 1.049 1.966 99.6 1.8 8.6
FVCAUS XvPLDA
cal 0.012 0.008 0.005 0.008 -1.0 0.0 0.0
calp 0.012 0.007 0.004 0.006 -0.9 0.0 0.0
raw 0.420 0.236 0.158 0.214 7.1 0.2 0.2
CIFAR-1vsO Resnet-20
cal 0.430 0.199 0.130 0.199 -2.3 0.1 0.2
calp 0.400 0.175 0.132 0.169 -1.1 0.0 0.0
raw 0.700 0.442 0.371 0.393 8.2 0.3 0.4
CIFAR-2vsO Resnet-20
cal 0.560 0.440 0.367 0.361 -0.4 0.3 0.3
calp 0.560 0.415 0.363 0.329 -0.5 0.0 0.0
raw 0.504 1.056 0.607 0.635 3.1 6.3 -
IEMOCAP W2V2
cal 0.494 0.984 0.606 0.615 -0.1 2.7 -
raw 0.780 1.009 0.936 0.814 34.2 18.4 -
AGNEWS GPT2-0sh
cal 0.378 1.014 0.762 0.536 -0.1 3.7 -
raw 0.082 0.406 0.121 0.122 17.2 3.9 -
CIFAR10 Resnet-20
cal 0.083 0.311 0.112 0.101 -0.8 0.8 -
raw 0.068 0.486 0.100 0.153 32.4 5.0 -
CIFAR10 Vgg19
cal 0.069 0.322 0.097 0.103 -0.6 1.7 -
raw 0.053 0.309 0.076 0.092 19.9 3.2 -
CIFAR10 RepVgg-a2
cal 0.052 0.239 0.067 0.074 -1.2 0.9 -
raw 0.315 0.918 0.331 0.266 8.6 10.3 -
CIFAR100 Resnet-20
cal 0.317 0.784 0.330 0.243 -0.8 1.6 -
raw 0.264 1.583 0.288 0.392 36.9 19.7 -
CIFAR100 Vgg19
cal 0.266 0.804 0.285 0.247 -0.9 4.6 -
raw 0.227 0.728 0.245 0.200 0.8 5.6 -
CIFAR100 RepVgg-a2
cal 0.228 0.706 0.246 0.198 -1.7 5.0 -
Table 2: Various metrics on raw and calibrated posteriors for different speech, image and
naturallanguageprocessingdatasets. Calibratedposteriors(cal)areobtainedwith
DPcalibrationusingacross-validationprocedure. Forthebinarytasks, calibrated
posteriors obtained with the PAV algorithm (calp) are also considered. We report
three normalized risk values (NRisk) for the same cost matrices as in Figure 2,
normalized empirical cross-entropy (NCE), relative calibration loss for CE using
DP calibration with cross-validation (RCL-CE-Dxv in Figures 2 and 3, here RCL
for short), binary ECE, and multi-class ECE (ECEmc).
27Ferrer et al.
impact of a small miscalibration in the maximum posterior since it disregards the other
posteriors (99 of them in this 100-class task) which may be very well calibrated. Finally,
it is possible that the low RCL is due to the DP calibration transformation not being
enough to fix the misscalibration problem, resulting in an underestimation of the RCL. In
practice, a developer could explore several calibration alternatives before deciding on one
approach. The best calibration approach is the one that results in the lowest EPSR value
(and, hence, the largest RCL), as long as the calibration model was not trained on the test
data. Exploration of various calibration techniques is out of the scope of this paper.
Another important observation from this table is the fact that, sometimes, a miscal-
ibrated system is better in terms of the quality of its posteriors than one that is well
calibrated. See, for example, the results for the CIFAR10 posteriors. The Resnet-20 Dxv-
calibrated posteriors have perfect calibration and an NCE of 0.101. The RepVgg-a2 raw
posteriors, on the other hand, are miscalibrated with an RCL of 19.9%, but have an NCE
of 0.092. This means that the posterior probabilities generated by the latter system are
better. Further evidence of this can be found in the NRisk values, which are consistently
better for the RepVgg-a2 raw posteriors than for the Resnet-20 calibrates ones. This exam-
ple illustrates why assessing calibration is not helpful to determine the quality of posterior
probabilities and should never be used to compare systems with each other.
Results for CIFAR10 also illustrate the fact that two systems with the same NCE are
not necessarily equally good for every operating point, as determined by a specific risk
function. For example, the Resnet-20 and Vgg19 Dxv-calibrated posteriors have almost
identical NCE (0.101 vs 0.103), but show different trends for the three NRisk metrics.
When the application of interest has a specific well-defined risk function, then that metric
should be used to make development decisions rather than NCE, which integrates over all
operating points instead of focusing on the one relevant for the task.
Comparing the results for Dxv-calibrated (cal) and PAV-calibrated posteriors (calp) we
can see that they are similar in most cases. This implies that the Dxv-calibration procedure
is doing the best possible job at calibrating the posteriors for those datasets. The larger
difference between the NCE for the cal and calp posteriors for the CIFAR datasets may be
due to a number of reasons: 1) that the DP transformation is not sufficiently expressive for
thesedatasets, 2)thatthetransformstrainedwithcross-validationarenotgeneralizingwell
tothetestsetsduetothesmallnumberofsamplesfromtheminorityclassinthesedatasets,
or 3) that the isotonic PAV transform overfitted the test data resulting in an unrealistically
low NCE value. The only way to determine whether the NCE for calp posteriors is too low
ortheoneforcalposteriorsistoolargeistokeepworkingonthecalibrationtransformation
in search of one that, when trained on held-out data or through cross-validation, reduces
the NCE further than Dxv calibration.
4 Discussion and conclusions
In this work, we focus on the problem of evaluating the quality of probabilistic classifiers.
We review the classical concept of proper scoring rules (PSRs) which are scoring functions
designed to assess the performance of probability distributions. The expectation of a PSR
(EPSR) reflects the quality of the posterior probabilities provided by a classifier. When
comparing two systems, a lower normalized EPSR indicates that the system’s posteriors
28Evaluating Posterior Probabilities
are better for making decisions and, hence, also better for interpretation by an end user.
A normalized EPSR larger than 1.0 indicates that making decisions with those posteriors
would be worse than making decisions simply based on the class priors.
Despite the existence of this elegant and principled tool to assess the performance of
probabilistic classifiers, the current trend in much of the machine learning literature is
to use calibration metrics for this purpose, under the claim that good calibration is an
essential requirement for posteriors to be interpretable, safe, and reliable. In this work,
we contend this view and argue that good calibration is neither necessary nor sufficient for
a probabilistic classifier’s posterior to be of value to the end user. A system can have a
relatively high calibration error and still be useful to the user, as long as its EPSR is low.
On the other hand, a system might be perfectly calibrated but provide very little value to
the user if its EPSR is too high. Hence, we see no reason to report calibration metrics as a
way to assess the value of a probabilistic classifier’s outputs.
It is important to note, though, that there is also usually no reason to tolerate a mis-
calibrated system. Any miscalibrated system can be fixed by adding a post-hoc calibration
stage at the end of the pipeline, transforming the posteriors output by the system into bet-
ter posteriors. We believe the only role of calibration metrics should be to aid the developer
in deciding whether such post-hoc calibration stage is needed for a given system. After that
decisionhasbeenmade, though, weargue, calibrationmetricsshouldplaynofurtherrolein
evaluation. In particular, calibration metrics should not be used to compare systems with
each other or to decide whether a system is safe for use in high-stakes applications. EPSRs
should instead be used for these purposes.
If the role of calibration metrics is simply to reflect the performance gain that can
be achieved by fixing the potential miscalibration of the system, then, we argue, they
should be designed to do exactly that. A direct way to measure the impact of a classifier’s
miscalibration is to add a post-hoc calibration stage to the system and assess the gain in
EPSR provided by this addition. The difference between the EPSR of the original classifier
andtheEPSRafteraddingpost-hoccalibration—calledcalibrationloss—isadirectmeasure
of the miscalibration of the original system. If the calibration loss relative to the EPSR
of the original system is large, then a calibration stage should be added to the system.
Importantly, the calibration stage should be designed and trained as any other system
stage, without using the test data to avoid overestimating the impact that a calibrator
would have in the system’s performance.
We compared calibration loss with the ECE, the most widely used calibration metric
in the machine learning literature, and show both theoretically and empirically that the
ECE has a number of important disadvantages. First, it lacks interpretability since it is not
related to a PSR and cannot be turned into a relative measure of the gains that would be
achieved with post-hoc calibration. In addition, the multi-class version of ECE measures
only the performance of the largest posterior probability generated by the system for each
sample rather than of the full posterior, failing to directly address the question of interest.
In contrast, calibration loss is directly interpretable, evaluating the calibration of the full
posterior distribution as the gain that can be obtained from adding a post-hoc calibrator
to the system. Other problems with the ECE include the fact that it relies on histogram
binning, an often poor choice as calibration transformation, and that this transformation
is trained on the test data itself, leading to potential overestimation of the miscalibration
29Ferrer et al.
due to overfitting of the transform. While calibration loss could also be computed with
this choice of calibration transform, we recommend against it, favoring instead the use of a
direction-preservingcalibrationtrainedwithcross-validationonthetestdataoronheld-out
data.
Finally, we also compared calibration loss with a calibration metric obtained from a
classic decomposition of the expected PSR of the classifier. We show that, in practice,
this decomposition is highly affected by the quality of the calibration transform used to
compute it, loosing its meaning when the transform does not adequately fix the classifier’s
miscalibration. Calibration loss, on the other hand, retains its interpretation as the gain
that can be obtained after post-hoc calibration with the selected transform, even if the
calibration transform is suboptimal.
The paper is accompanied by an open-source repository which provides code for the
computation of the various metrics in this paper. All plots and tables in this paper can be
replicated using the repository. We hope that this work along with the provided code will
facilitate the wider adoption of these metrics that offer a principled, elegant, and general
solution to the evaluation and diagnosis of probabilistic classifiers.
Acknowledgments
ThismaterialisbaseduponworksupportedbytheEuropeanUnion’sHorizon2020research
and innovation program under grant No 101007666/ESPERANTO/H2020-MSCA-RISE-
2020.
WethankNikoBru¨mmerformanyenlighteningdiscussionsaboutcalibrationandPSRs,
for his valuable feedback on this paper, and for contributing the proof in Section 2.4.
References
R. K. Ahuja and J. B. Orlin. Solving the convex ordered set problem with applications to
isotone regression. Technical Report SWP no.3988, Massachusetts Institute of Technol-
ogy, Sloan School of Management, February 1998.
M. Ayer, H. D. Brunk, G. M. Ewing, W. T. Reid, and E. Silverman. An empirical dis-
tribution function for sampling with incomplete information. Annals of Mathematical
Statistics, pages 641—647, 1955.
J. M. Bernardo and A. F. M. Smith. Bayesian Theory. John Wiley & Sons, 1994.
C. M. Bishop. Pattern Recognition and Machine Learning. Springer, 2006.
G.Brier. Verificationofforecastsexpressedintermsofprobability. MonthlyWeatherreview,
78, 1950.
J. Br¨ocker. Reliability, sufficiency, and the decomposition of proper scores. Quarterly
Journal of the Royal Meteorological Society, 135:1512 – 1519, 07 2009.
N. Bru¨mmer. Measuring, Refining and Calibrating Speaker and Language Information Ex-
tracted from Speech. PhD thesis, Stellenbosch University, 2010.
30Evaluating Posterior Probabilities
N. Bru¨mmer and J. du Preez. Application independent evaluation of speaker detection.
Computer Speech and Language, 20, Apr. 2006.
N. Bru¨mmer and D. A. van Leeuwen. On calibration of language recognition scores. In
Proc. Odyssey-06, Puerto Rico, USA, June 2006.
T. Cover and J. Thomas. Elements of Information Theory. Wiley, 2006.
A. P. Dawid. The geometry of proper scoring rules. Annals of the Institute of Statistical
Mathematics, 59:77–93, 2007.
A. P. Dawid and M. Musio. Theory and applications of proper scoring rules. METRON,
72(2):169–183, Apr 2014. ISSN 2281-695X.
A. P. Dawid, M. Musio, and L. Ventura. Minimum scoring rule inference. Scandinavian
Journal of Statistics, 43(1):123–138, 2016.
M. DeGroot and M. Schervish. Probability and Statistics. Pearson Education Limited, 4th
edition, 2014.
M. H. DeGroot. Optimal Statistical Decisions. McGraw-Hill, 1970.
M. H. DeGroot and S. E. Fienberg. The comparison and evaluation of forecasters. Journal
of the Royal Statistical Society. Series D (The Statistician), 32(1/2):12–22, 1983.
R. Duda, P. Hart, and D. Stork. Pattern Classification. Wiley, 2001.
L. Ferrer, M. McLaren, and N. Bru¨mmer. A speaker verification backend with robust
performance across conditions. Computer Speech and Language, 71:101258, 2021.
T. S. Filho, H. Song, M. Perello-Nieto, R. Santos-Rodriguez, M. Kull, and P. Flach. Clas-
sifier calibration: How to assess and improve predicted class probabilities: a survey.
Machine Learning, 112, 2023.
T. Gneiting and A. E. Raftery. Strictly proper scoring rules, prediction, and estimation.
Journal of the American Statistical Association, 2007.
I. Good. Rational decisions. Journal of the Royal Statistical Society, 14, 1952.
S. G. Gruber and F. Buettner. Better uncertainty calibration via proper scores for classifi-
cation and beyond. In Proc. of NeurIPS, New Orleans, December 2022.
A. Gulli. The anatomy of a news search engine. In Special Interest Tracks and Posters of
the 14th International Conference on World Wide Web, pages 880–881, New York, NY,
USA, 2005. Association for Computing Machinery.
C.Guo,G.Pleiss,Y.Sun,andK.Q.Weinberger. Oncalibrationofmodernneuralnetworks.
In Proc. of the 34th International Conference on Machine Learning, Sydney, Australia,
2017.
T. Hastie, R. Tibshirani, and J. Friedman. The Elements of Statistical Learning. Springer-
Verlag, 2001.
31Ferrer et al.
Y. Huang, W. Li, F. Macheret, R. A. Gabriel, and L. Ohno-Machado. A tutorial on
calibration measurements and calibration models for clinical prediction models. Journal
of the American Medical Informatics Association, 27(4):621–633, 02 2020.
E. T. Jaynes. Probability theory: The logic of science. Cambridge university press, 2003.
M. Keller, S. Bengio, and S. Wong. Benchmarking non-parametric statistical tests. In Proc.
of Neural Information Processing Systems, volume 18, Vancouver, December 2005.
A. Kumar, P. S. Liang, and T. Ma. Verified uncertainty calibration. In in Proc. of
NeurIPS, 2019. URL https://proceedings.neurips.cc/paper_files/paper/2019/
file/f8c0c968632845cd133308b1a494967f-Paper.pdf.
P. Liang, R. Bommasani, T. Lee, D. Tsipras, D. Soylu, M. Yasunaga, Y. Zhang,
D. Narayanan, Y. Wu, A. Kumar, et al. Holistic evaluation of language models. Trans-
actions on Machine Learning Research, 2023.
M. McLaren, L. Ferrer, D. Castan, and A. Lawson. The speakers in the wild (SITW)
speaker recognition database. In Proc. Interspeech, San Francisco, Sept. 2016.
M. Minderer, J. Djolonga, R. Romijnders, F. Hubis, X. Zhai, N. Houlsby, D. Tran, and
M. Lucic. Revisiting the calibration of modern neural networks. In Proc. of NeurIPS,
volume 34, pages 15682–15694, 2021.
G. Morrison, C. Zhang, and E. Enzinger et. al. Forensic database of voice recordings of
500+ australian english speakers. http://databases.forensic-voice-comparison.
net, 2015.
G. S. Morrison, P. Rose, and C. Zhang. Protocol for the collection of databases of record-
ings for forensic-voice-comparison research and practice. Australian Journal of Forensic
Sciences, 44(2):155–167, jun 2012.
J. Mukhoti, V. Kulharia, A. Sanyal, S. Golodetz, P. Torr, and P. Dokania. Calibrating deep
neural networks using focal loss. In Proc. of NeurIPS, volume 33, pages 15288–15299,
2020.
R. Mu¨ller, S. Kornblith, and G. E. Hinton. When does label smoothing help? In Proc. of
NeurIPS, volume 32, 2019.
M. P. Naeini, G. Cooper, and M. Hauskrecht. Obtaining well calibrated probabilities using
Bayesian binning. In Twenty-Ninth AAAI Conference on Artificial Intelligence, 2015a.
M. P. Naeini, G. F. Cooper, and M. Hauskrecht. Binary classifier calibration: Non-
parametric approach. In Proc. of SIAM Int Conf Data Min, 2015b.
J. Nixon, M. W. Dusenberry, L. Zhang, G. Jerfel, and D. Tran. Measuring calibration in
deep learning. In CVPR Workshops, 2019.
Y. Ovadia, E. Fertig, J. Ren, Z. Nado, D. Sculley, S. Nowozin, J. V. Dillon, B. Lakshmi-
narayanan, and J. Snoek. Can you trust your model’s uncertainty? Evaluating predictive
uncertainty under dataset shift. In in Proc. of NeurIPS, Vancouver, June 2019.
32Evaluating Posterior Probabilities
E. Ovcharov. Proper scoring rules and Bregman divergences. Bernoulli, 03 2015.
M. Peterson. An Introduction to Decision Theory. Cambridge Introductions to Philosophy.
Cambridge University Press, 2009.
J. C. Platt. Probabilistic outputs for support vector machines and comparisons to regular-
ized likelihood methods. Adv. Large Margin Classifiers, 2000.
N. Poh and S. Bengio. Estimating the confidence interval of expected performance curve in
biometric authentication using joint bootstrap. In Proc. ICASSP, Honolulu, Apr. 2007.
T.Popordanoska,R.Sayer,andM.B.Blaschko.AconsistentanddifferentiableLpcanonical
calibration error estimator. In Proc. of NeurIPS, New Orleans, 12 2022.
J.Quinonero-Candela,C.E.Rasmussen,F.Sinz,O.Bousquet,andB.Sch¨olkopf.Evaluating
predictive uncertainty challenge. In Machine Learning Challenges Workshop, pages 1–27.
Springer, 2005.
D.RamosandJ.Gonzalez-Rodriguez. Reliablesupport: Measuringcalibrationoflikelihood
ratios. Forensic Science International, 230(1–3):156–169, July 2013.
D. Ramos, R. P. Krish, J. Fierrez, and D. Meuwly. From biometric scores to forensic
likelihood ratios. Handbook of biometrics for forensic science, pages 305–327, 2017.
D. Ramos, D. Meuwly, R. Haraksim, and C. E. Berger. Validation of forensic automatic
likelihood ratio methods. In Handbook of forensic statistics, pages 143–162. Chapman
and Hall/CRC, 2020.
S.Raschka. Modelevaluation, modelselection, andalgorithmselectioninmachinelearning.
arXiv preprint arXiv:1811.12808, 2018.
K. Slooten and R. Meester. Forensic identification: Database likelihood ratios and familial
DNA searching, 2012.
R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts. Recursive
deep models for semantic compositionality over a sentiment treebank. In Proceedings
of the 2013 Conference on Empirical Methods in Natural Language Processing, pages
1631–1642, Seattle, Washington, USA, Oct. 2013.
R. J. Tibshirani and B. Efron. An introduction to the bootstrap. Monographs on statistics
and applied probability, 57(1), 1993.
R. Tomsett, A. Preece, D. Braines, F. Cerutti, S. Chakraborty, M. Srivastava, G. Pearson,
and L. Kaplan. Rapid trust calibration through interpretable and uncertainty-aware AI.
Patterns, 1(4), 2020.
J.Vaicenavicius,D.Widmann,C.Andersson,F.Lindsten,J.Roll,andT.Sch¨on. Evaluating
model calibration in classification. In The 22nd International Conference on Artificial
Intelligence and Statistics, pages 3459–3467. PMLR, 2019.
33Ferrer et al.
K. Van Hoorde, S. Van Huffel, D. Timmerman, T. Bourne, and B. Van Calster. A spline-
based tool to assess and visualize the calibration of multiclass risk predictions. Journal
of biomedical informatics, 54:283–293, 2015.
D. Widmann, F. Lindsten, and D. Zachariah. Calibration tests in multi-class classification:
A unifying framework. In Proc. of NeurIPS, Vancouver, December 2019.
R. L. Winkler and A. H. Murphy. Good probability assessors. Journal of Applied Meteo-
rology, 1968.
B. Zadrozny and C. Elkan. Obtaining calibrated probability estimates from decision trees
and naive Bayesian classifiers. In Proc. of ICML, May 2001.
X. Zhang, J. Zhao, and Y. LeCun. Character-level convolutional networks for text classifi-
cation. Advances in neural information processing systems, 28, 2015.
Appendix A. Cross-entropy and Brier score parameterized by the priors
The cross-entropy (CE) and the Brier scores (BS) can be expressed as a function of the
priors, allowing us to turn them into parameters of the metric. For the CE, the expression
is given by
N
CE =
(cid:88) P ht
log(q (x )) (36)
− N
ht t
t=1
ht
where N is the number of samples of class h . Setting the prior for each class h, P , to
ht t h
N /N werecoverthestandardCEexpression(Equation8). Theadvantageoftheexpression
h
above is that it allows us to manipulate P independently of the test dataset. This is useful
h
when the class frequencies present in the test data do not reflect the prior distribution that
is expected during deployment. In that case, the P ’s can be set to those we expect to see
h
when the system is used. The resulting CE will then better reflect the values we would
measure on that target data if it was available for evaluation.
For the BS, the expression parameterized by the priors is given by:
N K
BS = (cid:88) P ht 1 (cid:88) (q (x ) I(h = H ))2 (37)
i t t i
N K −
t=1
ht
i=1
Appendix B. The absolute distance loss is not a score divergence
Here we provide a counterexample to show that the absolute distance loss does not sat-
isfy the mean-as-minimizer property of Bregman divergences. To show this, we generate
posteriors s as described in Appendix C for a 2-class problem with P = 0.6. Then, we
1
take the expectation with respect to the empirical distribution of their divergence to a fixed
posterior q . We do this for a range of q and plot the resulting expected divergence as a
0 0
function of the first component of q . Further, we plot with a star the first component of
0
34Evaluating Posterior Probabilities
L1post2 L1 L2 KL
1.2
0.6 1.2 1.0 2.5
2.0
0.5 1.0 0.8
1.5
0.6
0.4 0.8 1.0
0.4
0.3 0.6 0.5
0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0 0.0 0.2 0.4 0.6 0.8 1.0
q01 q01 q01 q01
Figure 4: Expected divergence between synthetic posteriors for a binary task and q , a
0
fixed vector of posteriors. The figure shows the expected divergence as a function
of the first component of q , which we call q , for different divergences (blue
0 01
curves) and the mean value of the first component of the posterior (star). The
star has to coincide with the minimum of the curve for valid score divergences
proving by contradiction that the L1 loss is not a score divergence.
the mean s. For valid score divergences, the mean should coincide with the minimum of
the curve.
Figure 4 shows these curves for four different divergences: 1) the L1 loss only over
the posterior for H , which is, approximately (given that the ECE actually quantizes the
2
posteriors before computing the distance), what the ECE computes, 2) the L1 loss between
the full posterior vectors, 3) the L2 loss, which corresponds to the Brier score PSR, and 4)
the Kullback-Leibler loss, which corresponds to the negative logarithmic loss PSR. We can
see that the first two losses do not satisfy the mean-as-minimizer property and, hence, are
not score divergences.
Appendix C. Synthetic dataset for experiments
Here we describe the procedure used to create synthetic datasets for the experiments in this
paper. Given a number of classes K, a total number of samples N, a prior for class H
1
of P , and a variance σ, which are taken as a parameters of the simulation, we proceed as
1
follows:
1. Set the class priors to P for class H , and P = (1 P )/(K 1) for classes 2 through
1 1 i 1
− −
K. Unless otherwise indicated, P is set to 0.8.
1
2. Determine the number of samples for each class, N as the closest integer to P N.
i i
3. GenerateN samplesusingamultivariateGaussiandistribution (µ ,σI), withmean
i i
N
µ givenbyaone-hotvectorwiththeoneattheithdimensionanddiagonalcovariance
i
matrix with equal variance in all dimensions given by σ which, unless is otherwise
indicated, is set to 0.15. We take these samples to be the x , the input features for
t
each sample.
4. Compute the likelihoods for each class for each generated sample according to the
class distributions used to draw these samples, that is, P(x H ) (µ ,σI).
i i
| ∼ N
35Ferrer et al.
5. Finally, we assume two possible prior distributions: 1) the same prior distribution
given by the P s according to which the data was generated in step 3, and 2) a
i
mismatched distribution Pˆ, where Pˆ = 0.1/(K 1) for i = K and Pˆ = 0.9.
i K
− ̸
6. Using those two prior distributions, compute two sets of posteriors which we call cal
and mcp (for mis-calibrated due to a mismatch in posteriors) which correspond to
using the data priors and the mismatched priors, respectively, to obtain the posteriors
according to:
P(x H ) P(H ) P(x H ) P(H )
i i i i
P(H i x) = | = (cid:80) | . (38)
| P(x) P(x H ) P(H )
j | j j
where the P(x H ) are the likelihoods computed in step 4 and P(H ) are the corre-
i i
|
sponding matched or mismatched priors, P and Pˆ, respectively.
i i
Notethatwiththeprocedureabove,thecalposteriorsareperfectlycalibratedforthetest
data. The mcp posteriors, though, are not calibrated because, even though the likelihoods
usedtocomputeitareobtainedfromthegeneratingdistribution, thepriorsaremismatched
to the ones used for testing. Further, we create misscalibrated versions of the posteriors
which we call mcs and mcps by scaling the cal and mcp posteriors, respectively, in the
log domain and then converting them back to posteriors by computing the softmax. The
scale is set to 5.0 unless otherwise indicated, simulating an overfitted system that produces
overconfident posteriors.
Appendix D. Real datasets for experiments
For the experiments with real datasets in Section 3.5, we use a variety of datasets from
speech, image, and natural language processing tasks.
SST2 (Socher et al., 2013) is a natural language processing dataset where the task is to
decide whether a certain text has positive or negative sentiment. AGNEWS (Gulli, 2005;
Zhang et al., 2015) is another natural language processing dataset where the task is to
classify news into 4 different classes. The posteriors for these datasets were produced
with the GPT-2 model using the code provided in https://github.com/LautaroEst/
efficient-reestimation using zero-shot prompts.
SITW(McLarenetal.,2016)andFVCAUS(Morrisonetal.,2015,2012)aretwospeaker
verification datasets where the task is to decide whether two audio samples belong to the
same speaker or not. To obtain posteriors for these two datasets, we ran an X-vector PLDA
system using the code provided in [link hidden to preserve anonymity].
IEMOCAPisaspeechprocessingdatasetwherethetaskistoclassifyeachspeechsample
intoasetofemotions: angry,happy,sad,andneutral. Theposteriorsweredownloadedfrom
https://github.com/habla-liaa/ser-with-w2v2/tree/master/experiments/w2v2PT-fusion.
Finally, CIFAR10 and CIFAR100 are two image processing datasets where the task is to
classify the object in an image into one of 10 or 100 classes, respectively. The posteriors for
these datasets were obtained using the code available in https://github.com/chenyaofo/
image-classification-codebase. We run three models for each of the two datasets:
resnet20, vgg19, and repvgg a2. These models have approximately 0.27 million, 20 million,
and 27 million parameters, respectively.
36Evaluating Posterior Probabilities
Dataset #Classes #Samples Priors
SST2 2 1821 0.50 0.50
SITW 2 721788 0.99 0.01
FVCAUS 2 114072 0.98 0.02
CIFAR-1vsO 2 10000 0.99 0.01
CIFAR-2vsO 2 10000 0.99 0.01
IEMOCAP 4 5473 0.20 0.29 0.31 0.20
AGNEWS 4 7600 0.25 0.25 0.25 0.25
CIFAR10 10 10000 0.10 for all classes
CIFAR100 100 10000 0.01 for all classes
Table 3: Number of classes, number of samples, and class priors for each dataset included
in our experiments.
From the CIFAR100 Resnet20 posteriors, we also created binary classification tasks
for the detection of one specific class versus all others, using the posterior provided by
the system for that class and 1 minus that posterior for the “other” class. We call these
posteriors CIFAR-XvsO, where X identifies the target class.
Table 3 shows the priors and total number of samples for all the datasets used in Sec-
tion 3.5.
Appendix E. Confidence Intervals
Themetricsinthemaintextandinpriorsectionsofthisappendixarecomputedasaverages
over the whole test set. For small datasets, this estimate might be quite unreliable, not
necessarily reflecting the performance we would observe in practice. Hence, having an
estimate of the range of values that our metric of interest could take on a new dataset is
essential, specially for small datasets. A standard method for obtaining such ranges is the
bootstrapping approach (Tibshirani and Efron, 1993; Keller et al., 2005; Poh and Bengio,
2007). Given a test set of size N, the procedure for obtaining confidence intervals is as
follows:
• Obtain B bootstrap sets of size N by sampling the original test set with replacement.
• Compute the metric of interest in each of the B bootstrap sets.
• Compute the confidence interval with confidence level γ by calculating the γ/2 and
100 γ/2 percentiles from the list of B metric values computed above.
−
The confidence intervals obtained in this way assume that the system is fixed and exactly
what will be deployed. Only the variability due to the test data is reflected in these in-
tervals (Raschka, 2018). Importantly, when computing calibration metrics, the calibration
transform, if computed using cross-validation or train-on-test rather than using a separate
calibration dataset (in which case the calibration transform can be considered part of the
system and also frozen), should be retrained for each bootstrap set. This allows the con-
fidence interval to reflect the variability in metric values due to changes in the transform
37Ferrer et al.
2.0 100
cal mcp mcs mcps
1.5
50
1.0
0.5 0
0.0
NBS RCL-BS-Axv RCL-BS-Hxv RCL-BS-Att RCL-BS-Htt
Figure 5: Metrics with confidence intervals obtained with bootstrapping. Left: the normal-
ized BS for the same posteriors as in Figure 2. Right: the RCL for BS using the
same calibration methods as in Figure 2.
training data. Importantly, when doing cross-validation, the folds should be defined by
original sample to avoid having the same sample across more than one fold.
To illustrate, Figure 5 shows the confidence intervals obtained from 100 bootstrap sam-
ples for the same posteriors as in Figure 2 for NBS and BS-based RCL. The code to create
this plot can be found in the accompanying repository.
38