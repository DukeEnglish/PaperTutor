Optimizing Cox Models with Stochastic Gradient Descent:
Theoretical Foundations and Practical Guidances
Lang Zeng1, Weijing Tang2, Zhao Ren3, and Ying Ding1,∗
1Department of Biostatistics, University of Pittsburgh, Pittsburgh, PA, U.S.A.
2Department of Statistics and Data Science, Carnegie Mellon University, Pittsburgh, PA, U.S.A.
3Department of Statistics, University of Pittsburgh, Pittsburgh, PA, U.S.A.
*email: yingding@pitt.edu
Summary: Optimizing Cox regression and its neural network variants poses substantial computational challenges
in large-scale studies. Stochastic gradient descent (SGD), known for its scalability in model optimization, has
recently been adapted to optimize Cox models. Unlike its conventional application, which typically targets a sum
of independent individual loss, SGD for Cox models updates parameters based on the partial likelihood of a subset
of data. Despite its empirical success, the theoretical foundation for optimizing Cox partial likelihood with SGD
is largely underexplored. In this work, we demonstrate that the SGD estimator targets an objective function that
is batch-size-dependent. We establish that the SGD estimator for the Cox neural network (Cox-NN) is consistent
and achieves the optimal minimax convergence rate up to a polylogarithmic factor. For Cox regression, we further
√
prove the n-consistency and asymptotic normality of the SGD estimator, with variance depending on the batch
size. Furthermore, we quantify the impact of batch size on Cox-NN training and its effect on the SGD estimator’s
asymptoticefficiencyinCoxregression.Thesefindingsarevalidatedbyextensivenumericalexperimentsandprovide
guidance for selecting batch sizes in SGD applications. Finally, we demonstrate the effectiveness of SGD in a real-
world application where GD is unfeasible due to the large scale of data.
Key words: Cox model; linear scaling rule; minimax rate of convergence; neural network; stochastic gradient
descent.
4202
guA
5
]LM.tats[
1v93820.8042:viXraSGD for Cox Models 1
1. Introduction
Coxproportional-hazardregression(Cox1972)isoneofthemostcommonlyusedapproaches
in survival analysis, where the primary outcome of interest is the time to a certain event.
It assumes that the covariates have a linear effect on the log-hazard function. With the de-
velopment of deep learning, Cox proportional-hazard-based neural networks (Cox-NN) have
been proposed to model the potential nonlinear relationship between covariates and survival
outcomes and improve survival prediction accuracy (Faraggi and Simon 1995; Katzman et al.
2018; Ching et al. 2018; Zhong et al. 2022). Despite the success of Cox models in survival
analysis, they face a significant optimization challenge when applied to large-scale survival
data. In particular, the Cox models are typically trained by maximizing the partial likelihood
(Cox1975)throughthegradientdescent(GD)algorithm,whichrequirestheentiredatasetto
compute the gradient. This can be slow and computationally extensive for large datasets. For
example, as shown in Figure 1, performing GD in Cox-NN training requires more than 500
GB of memory, which makes GD impractical due to hardware memory limitations. Besides
Cox-NN,thelarge-scaledataalsochallengestheoptimizationofCoxregression.Tarkhanand
Simon (2024) reported that the GD algorithm for Cox regression is vulnerable to round-off
errors when dealing with large sample sizes. Consequently, GD optimization constrains Cox
models’ scalability in large-scale data applications.
[Figure 1 about here.]
Stochastic gradient descent algorithm (SGD) is a scalable solution for optimization in
applications with large-scale data and has been widely used for NN training (Amari 1993;
Hinton et al. 2012; Bottou 2012). It avoids the calculation and memory burdens by using
a randomly selected subset of the dataset, which is called a mini-batch, to calculate the
gradient and update parameters in each recursion. In NN optimization, SGD also accelerates
the convergence process, as the inherent noise from the stochastic nature of mini-batches2
helps escape from the local minimizer (Kleinberg et al. 2018). Unfortunately, SGD cannot
be directly used to optimize the partial likelihood of all samples because calculating the
individual partial likelihood requires access to all data that survived longer (at risk). This
makes it unclear how to optimize the Cox model through SGD.
Several attempts have been made to enable parameter updates of the Cox model using the
mini-batchdata.Kvammeetal.(2019)followedtheideaofnestedcase-controlCoxregression
from Goldstein and Langholz (1992), proposing an approximation of the gradient of partial
likelihood from the case-control pairs instead of using all at-risk samples. This strategy
worked well in practice, making it possible to fit the Cox model with SGD. Sun et al. (2020)
fitted a Cox-NN to establish a prediction for eye disease progression through SGD where
the recursion was based on the partial likelihood of a subset of data. Recently, Tarkhan and
Simon (2024) studied the SGD for Cox regression in online learning and demonstrated that
the maximizer of the expected partial likelihood of any subset is the true parameter under
the Cox regression model. This observation and the practical successes imply that the Cox
models can be optimized through the SGD, but lack the theoretical guarantee.
Motivated by the knowledge gap, in this work, we establish theoretical foundations for
employing SGD in Cox models, where the gradient is calculated only on a subset of data and
is, therefore, scalable for large-scale applications. Specifically, our contributions come from
the following three folds:
• Firstly, the SGD estimator of the Cox models does not optimize the partial likelihood of
all samples (Cox 1975) but optimizes a different function. Therefore, it is unclear whether
the SGD estimator is valid. We establish the asymptotic properties of the SGD estimators
for Cox models. For Cox-NN, we show that the SGD estimator is consistent with the
unknown true function and able to circumvent the curse of dimensionality. It achieves the
minimax optimal convergence rate up to a polylogarithmic factor. For Cox regression, weSGD for Cox Models 3
√
prove the SGD estimator is n-consistent, asymptotically normal. These results justify
the application of SGD algorithm in the Cox model.
• Secondly, the target function optimized by SGD for the Cox model is batch-size dependent.
We present the properties of this function and demonstrate how its curvature changes
with the batch size. We further investigate the impact of batch size in Cox-NN training
and provide both theoretical and numerical evidence that (the ratio of the learning rate
to the batch size) is a key determinant of SGD dynamics in Cox-NN training. Keeping
the ratio constant makes the Cox-NN training process almost unchanged (linear scaling
rule). Additionally, for Cox regression, we demonstrate the improvement in statistical
efficiency when batch size doubles. This phenomenon is not seen in SGD optimizations
such as minimizing the mean squared error where the estimators’ statistical efficiency is
independent of the SGD batch size.
• Lastly, we study the convergence of the SGD algorithm in terms of iteration for Cox
regression. We note that the target loss function optimized by SGD is not strongly convex
and an additional projection step is needed in the SGD recursion (projected SGD). We
perform the non-asymptotic analysis of the projected SGD algorithm and demonstrate the
algorithm converges to the global minimizer with large enough iterations.
The rest of the paper is organized as follows. We introduce the Cox models and SGD with
necessary notations in Section 2. We establish the theoretical properties of SGD estimators
for Cox-NN and investigate the impact of batch size in Cox-NN training in Section 3. The
theoretical properties of SGD estimators for Cox regression as well as the convergence of
SGD algorithm over iterations are studied in Section 4. Section 5 presents simulation studies
and a real-world data example.4
2. Background and Problem Setup
LetD(n) = {D }n denotenobservedindependentandidenticaldistributed(i.i.d.)samples.
i i=1
SurvivalanalysisistounderstandtheeffectofcovariateX ∈ Rp onthetime-to-eventvariable
T∗ (e.g., time-to-death). Let C∗ denote the true censoring time. Due to the right censoring,
the observed time-to-event data is the triplet set D = (X ,T ,∆ ), where T = min(T∗,C∗)
i i i i i i i
is the observed time and ∆ = I(T∗ ⩽ C∗) is the event indicator.
i i i
2.1 Cox Model and Deep Neural Network
Cox model assumes that the hazard function takes the form
λ(t|X) = λ (t)exp{f (X)}, (2.1)
0 0
where λ (t) is an unspecified baseline hazard function. The covariate X influences the hazard
0
through the unknown relative risk function f (X). When f (X) = XTθ , the model (2.1) is
0 0 0
the Cox regression model where the effect of X is linear. When the form f (X) is unspecified
0
and is approximated through a neural network f with parameter θ, we refer to it as Cox-NN.
θ
We briefly review the NN structure:
Let K be a positive integer and p = (p ,...,p ,p ) be a positive integer sequence.
0 K K+1
A (K + 1)-layer NN with layer-width p, i.e. number of neurons, is a composite function
f : Rp0 → RpK+1 recursively defined as
f(x) = W f (x)+v
K K K
f (x) = σ(W f (x)+v ) for k = 2,...,K (2.2)
k k−1 k−1 k−1
f (x) = σ(W x+v ),
1 0 0
where the matrices W ∈ Rp k+1 × Rp k and vectors v ∈ Rp k+1 (for k = 0,...,K) are the
k k
parameters of the NN. The pre-specified activation function σ is a nonlinear transformation
that operates componentwise on a vector, that is, σ((x ,...,x )T) = (σ(x ),...,σ(x ))T,
1 p 1 p
k k
which thus gives f : Rp k−1 → Rp k for k = 1,...,K. Among the activation functions that
kSGD for Cox Models 5
have been widely considered in deep learning, we focus on the most popular one named
Rectified Linear Units (ReLU) in Nair and Hinton (2010) throughout this paper:
σ(x) = max{x,0},
which is also considered in Schmidt-Hieber (2020) and Zhong et al. (2022). For NN in (2.2),
K denotes the depth of the network. The vector p lists the width of each layer with the first
element p as the dimension of the input variable, p ,...,p are the dimensions of the K
0 1 K
hidden layers, and p is the dimension of the output layer. The matrix entries (W ) are
K+1 k i,j
the weight linking the jth neuron in layer k to the ith neuron in layer k+1, and the vector
entries (v ) represent a shift term associated with the ith neuron in layer k+1. In Cox-NN,
k i
the output dimension p = 1 as the output of the f (X) is a real value.
K+1 θ
ThestandardoptimizationoftheCoxmodelisthroughminimizingthenegativelog-partial
likelihood function L(n) (θ) defined as
Cox
n
1 (cid:88) exp{f (X )}
L(n) (θ) := − ∆ log θ i (2.3)
Cox n i (cid:80)n I(T ⩾ T )exp{f (X )}
i=1 j=1 j i θ j
where sample i who has experienced the event (∆ = 1) is compared against all samples that
i
have not yet experienced the event up to time T (i.e., “at risk” set). The minimizer of (2.3)
i
is typically found using gradient-descent-based algorithms where the gradient ∇ L(n) (θ) is
θ Cox
calculated using n samples in each iteration (Therneau et al. 2015; Katzman et al. 2018;
Zhong et al. 2022). The statistical properties of the minimizer have been well-studied by
Cox (1972); Andersen and Gill (1982) for Cox regression and by Zhong et al. (2022) for
Cox-NN.
Note that all the above studies optimize the Cox model through GD. However, when
optimizing the Cox model through the SGD algorithm using a subset of samples, it does not
minimize (2.3), which will be further explained in the next section.6
2.2 Stochastic Gradient Descent for Cox Model
WestartthissectionwithabriefreviewofthegeneralSGDalgorithm.IntheSGDiterations,
the gradient is estimated on a subset of data D(s) ⊂ D(n). The subset D(s) is called a mini-
batch with batch size s (1 ⩽ s ⩽ n). Throughout this paper, we consider the batch size s
is fixed and does not depend on the sample size n. Let L(n)(D(n);θ) be an empirical loss
function of n samples. To find the minimizer of L(n)(D(n);θ), at the (t+1)th iteration step
of SGD, the parameter is updated through
θˆ = θˆ −γ ∇ L(s)(D (s);θˆ ), (2.4)
t+1 t t θ t t
whereγ isapre-scheduledlearningrateforthetthiteration.∇ L(s)(D (s);θˆ )isthegradient
t θ t t
ˆ
calculated from the mini-batch data D (s) at tth iterations. To ensure θ converges to
t t
argminL(n)(θ) over the iterations, SGD requires that
θ
E [∇ L(s)(D (s);θˆ )|D(n)] = ∇ L(n)(D(n);θˆ ) (2.5)
Dt(s) θ t t θ t
for all t, where the expectation is over the randomness of generating D (s) from D(n). The
t
equality (2.5) indicates that the gradient based on D (s) should be an unbiased estimator
t
of ∇ L(n) at any given θ. Throughout the paper, we consider D (s) is randomly sampled
θ t
from D(n) at each iteration, therefore the distribution does not depend on t. To simplify
the notation, for any θ, we omit the input data and drop the subscript t and let L(s)(θ) :=
L(s)(D(s);θ), L(n)(θ) := L(n)(D(n);θ), E[∇ L(s)(θ)|D(n)] := E [∇ L(s)(D(s);θ)|D(n)] if
θ D(s) θ
no confusion arises.
The requirement (2.5) is satisfied in the SGD optimizations for the mean squared error,
where the gradient function is the average of individual gradient, that is ∇ L(n)(θ) =
θ
1 (cid:80)n ∇ L (θ). Let ∇ L(s)(θ) := 1 (cid:80) ∇ L (θ) and D(s) is generated by randomly
n i=1 θ i θ s i:Di∈D(s) θ iSGD for Cox Models 7
sampling s subjects without replacement from D(n), we have
 (cid:12)   
(cid:12)
E[∇ θL(s)(θ)|D(n)] = E 1
s
(cid:88) ∇ θL i(θ)(cid:12) (cid:12) (cid:12)D(n) = (cid:0)1 n(cid:1) (cid:88) 1
s
(cid:88) ∇ θL i(θ)
 
i:Di∈D(s) (cid:12) s D(s)⊂D(n) i:Di∈D(s)
(cid:0)n−1(cid:1)
(cid:88) 1 (cid:88)
= s−1 ∇ L (θ) = ∇ L (θ) = ∇ L(n)(θ).
s(cid:0)n(cid:1) θ i
n
θ i θ
s i:Di∈D(n) i:Di∈D(n)
(2.6)
For Cox models, the SGD implementation (Kvamme et al. 2019; Tarkhan and Simon 2024)
requires at least two samples to calculate the gradient, which restricts s ⩾ 2. The parameters
are updated through
θˆ = θˆ −γ ∇ L(s) (θˆ ), (2.7)
t+1 t t θ Cox t
where the gradient from D(s) is calculated by
 
 1 (cid:88) exp{f (X )} 
∇ L(s) (θ) := ∇ − ∆ log θ i , (2.8)
θ Cox θ s i (cid:80) I(T ⩾ T )exp{f (X )}
 i:Di∈D(s) j:Dj∈D(s) j i θ j 
and the at-risk set is constructed on D(s). Note that (2.8) can not be written as the average
ofsi.i.d.gradients.Followtherequirement(2.5),andsupposeD(s)isgeneratedbyrandomly
sampling s subjects without replacement from D(n), the SGD recursion (2.7) is to solve the
root of
1 (cid:88) (cid:110) (cid:111)
E[∇ L(s) (θ)|D(n)] = ∇ L(s) (θ) . (2.9)
θ Cox (cid:0)n(cid:1) θ Cox
s D(s)⊂D(n)
Notethattherightsideof(2.9)isdifferentfrom∇ L(n) (θ)inCoxmodel,whichindicatesthe
θ Cox
target function solved by SGD differs from the one solved by GD. Therefore, the theoretical
results for the GD estimator can not be applied to the SGD estimator. This motivates us to
(cid:104) (cid:105)
investigate the statistical properties of the root of E ∇ L(s) (θ)|D(n) , or equivalently the
θ Cox
(cid:104) (cid:105)
minimizer of E L(s) (θ)|D(n) .
Cox
Wemakethefollowingstandardassumptionsforthetime-to-eventdatawithright-censoring:8
(A1) The true failure time T∗ and true right-censoring time C∗ are independent given the
i i
covariates X .
i
(A2) The study ends at time τ and there exists a small constant δ > 0 such that P(∆ = 1|X) ⩾
τ
δ and P(T∗ ⩾ τ|X) ⩾ δ almost surely with respect to the probability measure of X.
τ τ
(A3) X takes value in a bounded subset of Rp with probability function bounded away from
zero. Without loss of generality, we assume that the domain of X is taken to be [0,1]p.
Assumptions (A1)-(A3) are common regularity assumptions in survival analysis.
3. SGD Estimator for Cox Neural Network
Zhong et al. (2022) studied the asymptotic properties of the NN estimator which minimizes
(2.3) by GD. They showed that the NN estimator achieves the minimax optimal convergence
rate up to a polylogarithmic factor. Our work is inspired by the theoretical developments in
Zhong et al. (2022) but has major differences. First, Zhong et al. (2022) shows the at-risk
term (cid:80)n j=1I(Tj⩾t)exp{f θ(Xj)} converges to a fixed function E[I(Y > t)exp{f (X)}]. However,
n θ
in SGD with a fixed s, the at-risk term in L(s) (θ) always consists of s samples. Thus both
Cox
the empirical loss E[L(s) (θ)|D(n)] and the population loss lim E[L(s) (θ)|D(n)] are different
Cox Cox
n→∞
from Zhong et al. (2022) and depend on s. Second, when D(s) is generated by randomly
sampling s subjects without replacement from D(n) in each iteration, E[L(s) (θ)|D(n)] is an
Cox
average over
(cid:0)n(cid:1)
combinations of the mini-batch, where different batches may share the same
s
samples. Hence the correlation between the batches needs to be handled when deriving the
asymptotic properties of the minimizer of E[L(s) (θ)|D(n)].
CoxSGD for Cox Models 9
3.1 Consistency and Convergence Rate of SGD Estimator
Following Schmidt-Hieber (2020) and Zhong et al. (2022), we consider a class of NN with
sparsity constraints defined as
F(K,ς,p,D) = {f : f is a DNN with (K+1) layers and width vector p such that
max{∥W ∥ ,∥v ∥ } ⩽ 1} , for all k = 0,...,K,
k ∞ k ∞ (3.1)
K
(cid:88)
∥W ∥ +∥v ∥ ⩽ ς,∥f∥ ⩽ D},
k 0 k 0 ∞
k=1
where ∥W ∥ and ∥v ∥ denotes the sup-norm of matrix or vector, ∥.∥ is the number of
k ∞ k ∞ 0
nonzero entries of matrix or vector, and ∥f∥ is the sup-norm of the function f. D > 0 and
∞
thesparsityς isapositiveinteger.Thesparsityassumptionisemployedduetothewidespread
application of techniques such as weight pruning (Srinivas et al. 2017) or dropout (Srivastava
et al. 2014). These methods effectively reduce the total number of nonzero parameters,
preventing neural networks from overfitting, which results in sparsely connected NN. We
approximate f in (2.1) using a NN f ∈ F(K,ς,p,∞) parameterized by θ. More precisely,
0 θ
(cid:104) (cid:105)
f is estimated by minimizing E L(s) (θ)|D(n) through SGD procedure (2.7) and D(s) is
0 Cox
generated by randomly sampling s subjects without replacement from D(n). To simplify the
notation, we omit θ and denote the estimator by
1 (cid:88)
f˜(s) = arg min L(s) (f), (3.2)
n (cid:0)n(cid:1) Cox
f∈F(K,ς,p,∞)
s D(s)⊂D(n)
(cid:104) (cid:105)
whereL(s) (f) := 1 (cid:80) ∆ −f(X )+log(cid:80) I(T ⩾ T )exp{f(X )} .Weuse
Cox s j:Dj∈D(s) j j k:D k∈D(s) k j k
the superscript (s) to indicate its dependency on the batch size s.
Some restrictions on the unknown function f are needed to study the asymptotic prop-
0
erties of
f˜(s).
We assume f belongs to a composite smoothness function class, which is
n 0
broad and also adopted by Schmidt-Hieber (2020) and Zhong et al. (2022). Specifically, let
Hα(D,M) be a H¨ølder class of smooth functions with parameters α,M > 0 and domain
r10
D ∈ Rr defined by
 
 (cid:88) (cid:88) |∂βh(x)−∂βh(y)| 
Hα(D,M) = h : D → R : ∥∂βh∥ + sup ⩽ M ,
r

β:|β|<α
∞
β:|β|=⌊α⌋x,y∈D,x̸=y
∥x−y∥α ∞−⌊α⌋

where⌊α⌋isthelargestintegerstrictlysmallerthanα,∂β := ∂β1...∂βr withβ = (β ,...,β ),
1 r
and |β| =
(cid:80)r
β .
k=1 k
Let q ∈ N, α⃗ = (α ,...,α ) ∈ Rq+1 and d = (d ,...,d ) ∈ Nq+2, d˜ = (d˜ ,...,d˜ ) ∈ Nq+1
0 q + 0 q+1 + 0 q +
with d˜ ⩽ d ,j = 0,...q, where R is the set of all positive real numbers. The composite
j j +
smoothness function class is
H(q,α⃗,d,d˜,M) := {h = h ◦···◦h : h = (h ,...,h )T and
q 0 i i1 idi+1
(3.3)
h ∈ Hαi([a ,b ]d˜ i,M), for some |a |,|b | ⩽ M},
ij d˜
i
i i i i
where d˜ is the intrinsic dimension of the function. For example, if
f(x) = f (f (f (x ,x ),f (x ,x )),f (f (x ,x ),f (x ,x ))) x ∈ [0,1]8,
21 11 01 1 2 02 3 4 12 03 5 6 04 7 8
and f are twice continuously differentiable, then smoothness α = (2,2,2), dimensions d =
ij
(8,4,2,1) and d˜ = (2,2,2). We assume that
(N1) The unknown function f is an element of H = {f ∈ H(q,α⃗,d,d˜,M) : E[f(X)] = 0}.
0 0
The mean zero constraint in (N1) is for the identifiability of f since the presence of two
0
unknown functions in (2.1).
We denote α˜ = α (cid:81)q (α ∧1) and γ = max n−α˜i/(2α˜i+d˜ i) with notation a∧b :=
i i k=i+1 k n i=0,...,q
min{a,b}. Furthermore, we denote a ≲ b as a ⩽ cb for some c > 0 and any n. a ≍ b
n n n n n n
means a ≲ b and b ≲ a . We assume the following structure of the NN:
n n n n
(N2) K = O(logn), ς = O(nγ2logn) and nγ ≲ min(p ) ⩽ max(p ) ≲ n.
n n k k=1,...,K k k=1,...,K
A large neural network leads to a smaller approximation error and a larger estimation error.
Assumption (N2) determines the structure of the NN family in (3.1) and has been adoptedSGD for Cox Models 11
in Zhong et al. (2022) as a trade-off between the approximation error and the estimation
error.
We first present a lemma which is critical to study the asymptotic properties of
f˜(s):
n
Lemma 1: Let L(s)(f) := E[L(s) (f)]. Under the Cox model, with assumption (A1)-(A3),
0 Cox
(N1) and for any integer s ⩾ 2 and constant c > 0, we have
L(s)(f)−L(s)(f ) ≍ d2(f,f )
0 0 0 0
for all f ∈ {f : d(f,f ) ⩽ c,E[f(X)] = 0}, where d(f,f ) = [E{f(X)−f (X)}2]1/2.
0 0 0
Remark 1: Suppose f˜(s) = arg min L(s)(f) and by definition L(s)(f˜(s)) ⩽
0 0
f:d(f,f0)⩽c,E[f(X)]=0
L(s)(f ). On the other hand, we have L(s)(f˜(s)) ⩾ L(s)(f ) from Lemma 1. Hence, L(s)(f˜(s))−
0 0 0 0 0 0
L(s)(f ) = 0 and it implies f˜(s) = f . That is for any integer s ⩾ 2, the minimizer of L(s)(f)
0 0 0 0
on a neighborhood of f is f and does not depend on s. It is a generalization of the result
0 0
in Tarkhan and Simon (2024) where they consider a parametric function f with the truth
θ
f = f and demonstrate that argminL(s)(f ) = θ for any integer s ⩾ 2.
0 θ0 0 θ 0
θ
Then we obtain the result regarding the asymptotic properties of the SGD estimator for
Cox-NN:
Theorem 1: Under the Cox model, with assumptions (A1)-(A3), (N1), (N2) and for
any integer s ⩾ 2, there exists an estimator f˜(s) in (3.2) satisfying E{f˜(s)(X)} = 0, such
n n
that
∥f˜(s) −f ∥ = O (γ log2n).
n 0 L2([0,1]p) P n
Remark 2: For any integer s ⩾ 2, the convergence rate of f˜(s) is the same as the result
n
in Zhong et al. (2022) and Schmidt-Hieber (2020). We notice that the convergence rate is
dominated by the error of approximating f ∈ H by NN estimator f ∈ F, which is at the
0 0
order O (γ log2n).
p n12
Remark 3: Theorem 1 demonstrates that the convergence rate is determined by the
smoothness and the intrinsic dimension d˜ of the function f , rather than the dimension d.
0
Therefore, the estimator
f˜(s)
can circumvent the curse of dimensionality (Bauer and Kohler
n
2019). It has a fast convergence rate when the intrinsic dimension is relatively low.
Remark 4: Theminimaxlowerboundforestimatingf canbederivedfollowingtheproof
0
of Theorem 3.2 in Zhong et al. (2022) as their Lemma 4 and derivations still hold without
the parametric linear term. Specifically, let Ω = {λ (t) : (cid:82)∞ λ (s)ds < ∞ and λ (t) ⩾
0 0 0 0 0
0 for t ⩾ 0}. Under the Cox model with assumptions (A1)-(A3) and (N1), there exists a
constant 0 < c < ∞, such that inf sup E{fˆ (X)−f (X)}theTh2 ⩾ cγ2, where
fˆ (λ0,f0)∈Ω0×H0 0 n
ˆ
the infimum is taken over all possible estimators f based on the observed data. Therefore,
the NN-estimator in Theorem 1 is rate optimal since it attains the minimax lower bound up
to a polylogarithm factor.
Remark 5: Note that the smallest batch size that can be chosen is s = 2 and L(2) (f) is
Cox
nonzero only when one subject experiences the event while another remains at risk at that
time. Kvamme et al. (2019) proposes first sampling d subjects experiencing the event and
then sampling one subject from the corresponding at-risk set for each event in each SGD
iteration,thusformingdcase-controlpairs.Theyobservedthatutilizingtheaveragegradient
of L(2) (f) over the d pairs in SGD updates results in an effective estimator for f . This can
Cox 0
be attributed to the fact that E[L(2) (f)] is a valid loss function for f , as demonstrated in
Cox 0
Lemma 1 and Theorem 1.
3.2 Impact of Mini-batch Size on Cox-NN Optimization
Section 3.1 presents the statistical properties of the global minimizer
f˜(s).
Whether
f˜(s)
can
n n
be reached by SGD is affacted by many factors since the NN training is a highly non-convex
optimization problem (see the visualization by Li et al. (2018)). When applying SGD in NNSGD for Cox Models 13
training, the learning rate γ and batch size and greatly influence the behavior of SGD. It has
been noticed that the ratio of the learning rate to the batch size γ/s is a key determinant
of the SGD dynamics (Goyal et al. 2017; Jastrzebski et al. 2017, 2018; Xie et al. 2020). A
lower value of γ/s leads to a smaller size of the step. Goyal et al. (2017) demonstrates a
phenomenon that keeping γ/s constant makes the SGD training process almost unchanged
on a broad range of minibatch sizes, which is known as the linear scaling rule. This rule
guides the tuning of hyper-parameters in NN such that we can fix either γ or s and only
tune the other parameter to optimize the behavior of SGD.
Whether the linear scaling rule can be applied to Cox-NN training is questionable since
the population loss E[L(s) ] in Cox-NN depends on the batch size. Previous theoretical works
Cox
focusedonthepopulationlossE[L(θ)]whoseconvexity∇2E[L(θ)]| isinvarianttodifferent
θ θ=θ0
batch sizes and assumed that the Hessian equals the covariance of gradient at the truth,
that is ∇2E[L(θ)]| = V[∇ L(θ)]| (Jastrzebski et al. 2017, 2018; Xie et al. 2020). For
θ θ=θ0 θ θ=θ0
example, the population loss for MSE satisfies both assumptions and hence the linear scaling
rule holds in this case. However, these key assumptions are violated as shown in the following
theorem for the population loss E[L(s) ] under the Cox model.
Cox
Theorem 2: Under the Cox model with f = f parameterized by θ , with assumption
0 θ0 0
(A1)-(A3) and suppose ∇ f , ∇2f exist and f ,∇ f , ∇2f are bounded for all X on a
θ θ θ θ θ θ θ θ θ
neighborhood of θ , then for any integer s ⩾ 2 we have
0
∇2E[L(s) (θ)]| = sV[∇ L(s) (θ)]| , (3.4)
θ Cox θ=θ0 θ Cox θ=θ0
and
∇2E[L(2s)(θ)]| ⪰ ∇2E[L(s) (θ)]| , (3.5)
θ Cox θ=θ0 θ Cox θ=θ0
where ∇2 is second-order derivative operator with respect to θ and A ⪰ B denotes A−B is
θ
semi-positive definite.14
Remark 6: Inequality (3.5) shows that the convexity of E[L(s) (θ)] increases when s
Cox
doubles. On the other hand, for the different choice of s, the global minimizer of E[L(s) (θ)]
Cox
is θ which does not depend on s, as introduced in Remark 1.
0
With Theorem 2, we demonstrate the linear scaling rule in Cox-NN training still holds,
especially when the batch size is large. Following the derivation in Section 2.2 of Jastrzebski
et al. (2017), γTr(H ) is the key determinant of the dynamics of SGD in Cox-NN, where
s s
H = ∇2E[L(s) (θ)]| and Tr(·) is the trace. When s doubles, the difference of Hessian
s θ Cox θ=θ0
trace Tr(H ) − Tr(H ) is at most O(1/s) and negligible for large s (see Supplementary
2s s
Materials for more discussion). Therefore, the linear scaling rule still holds for Cox-NN when
batch size is large. In Section 5, we numerically demonstrate that the increase of Tr(H ) is
s
small for s > 32 and the linear scaling rule holds in Cox-NN training for large batch size.
4. SGD Estimator for Cox Regression
In this section, we consider the Cox regression model with f(X) = θTX where the effect of
X is linear and estimate θ through the SGD procedure (2.7) with
0
∇ L(s) (θ) := −1 (cid:88) ∆ (cid:34) X − (cid:80) j:Dj∈D(s)I(T j ⩾ T i)exp{θTX j}X j(cid:35) . (4.1)
θ Cox s i i (cid:80) I(T ⩾ T )exp{θTX }
i:Di∈D(s) j:Dj∈D(s) j i j
We adopt the following assumptions for the regression setting:
(R1) The true relative risk f (X) = θTX with parameter θ ∈ Rp := {θ ∈ Rp : ∥θ∥ ⩽ M}.
0 0 0 M
(R2) There exist constants 0 < c < c < ∞ such that the subdensities p(x,t,∆ = 1) of
1 2
(X,T,∆ = 1) satisfies c < p(x,t,∆ = 1) < c for all (x,t) ∈ [0,1]p ×[0,τ].
1 2
(R3) For some k ⩾ 2 and δ ∈ {0,1}, the kth partial derivative of the subdensity p(x,t,∆ = δ)
of (X,T,∆ = δ) with respect to (x,t) ∈ [0,1]p ×[0,τ] exists and is bounded.
The subdensity p(x,t,∆ = δ) is defined as
∂2P(T ⩽ t,X ⩽ x,∆ = δ)
p(x,t,∆ = δ) = .
∂t∂xSGD for Cox Models 15
Assumption (R2) ensures the information bound for θ exists. Assumption (R3) is used to
establish the asymptotic normality of the SGD estimator. Moreover, assumption (R1) and
(R2) are necessary to demonstrate the strong convexity of E[L(s) (θ)], i.e., ∇2E[L(s) (θ)] > 0
Cox θ Cox
for all θ ∈ Rp .
M
Depending on the nature of the dataset, we study the SGD estimator for Cox regression
undertheofflinescenario(unstreamingdata)andtheonlinescenario(streamingdata).Inthe
first scenario, the dataset D(n) of n i.i.d. samples has been collected. In the second scenario,
theobservationsarriveinacontinualstreamofstrata,andthemodeliscontinuouslyupdated
as new data arrives without storing the entire dataset.
4.1 SGD for Offline Cox Regression
To study the statistical properties of θ˜(s) := argminE[L(s) (θ)|D(n)], we consider two
n Cox
θ
sampling strategy of D(s) which determines the form of E[L(s) (θ)|D(n)]. The first strategy
Cox
has been considered in Theorem 1 where D(s) is generated by randomly sampling s subjects
from D(n) without replacement. We refer to it as stochastic batch (SB) with the estimator
1 (cid:88)
θ˜SB(s) = arg min L(s) (θ). (4.2)
n θ∈Rp (cid:0)n(cid:1) Cox
M s D(s)⊂D(n)
Another strategy is to randomly split the whole samples into m = n/s non-overlapping
batches. Once the mini-batches are established, they are fixed and then repeatedly used
throughout the rest of the algorithm (Qi et al. 2023). We refer to it as fixed batch (FB) with
the estimator
1 (cid:88)
θ˜FB(s) = argmin L(s) (θ), (4.3)
n θ m Cox
D(s)∈D(n|s)
where we use D(n|s) to denote that D(n) has been partitioned into m = n/s fixed disjoint
batches. The element of D(n|s) is a mini-batch containing s i.i.d. samples and D(s) is
generated by randomly picking one element from D(n|s). We consider the second sampling16
strategy because it is a popular choice in the SGD application and the impact of batch size
on the asymptotic variance of
θ˜FB(s)
can be well-established.
n
We present the asymptotic properties of
θ˜SB(s)
and
θ˜FB(s)
in the following theorem:
n n
Theorem 3: Under the Cox model, with assumptions (A1)-(A3), (R1)-(R3) and for
any integer s ⩾ 2, we have
√
n(θ˜SB(s) −θ ) →d N(0,s2H−1Σ (H−1)T), (4.4)
n 0 s (s|1) s
√
n(θ˜FB(s) −θ ) →d N(0,sH−1Σ (H−1)T), (4.5)
n 0 s s s
when n → ∞, where H = E[∇2L(s) (θ)]| , Σ = V[∇ L(s) (θ)]| , and
s θ Cox θ=θ0 s θ Cox θ=θ0
(cid:110) (cid:111)(cid:12)
Σ = V ∇ L(s) (D ,D ,...,D |θ),∇ L(s) (D ,D˜ ,...,D˜ |θ) (cid:12) ,
(s|1) θ Cox i1 i2 is θ Cox i1 i2 is (cid:12)
θ=θ0
which is the covariance of ∇ L(s) on two mini-batches D(s) sharing the same sample D
θ Cox i1
˜
but different rest s−1 samples (denoted by D).
Remark 7: Theorem 3 states that, for any choice of batch size s, both θ˜SB(s) and θ˜FB(s)
n n
√
converge to θ at n speed. Additionally, they are asymptotically normal with a sandwich-
0
type variance while the middle part of the asymptotic variance differs. By the theory of
the U-statistics (Hoeffding 1992), we have s2Σ ⪯ sΣ and the equality holds only if
(s|1) s
∇ L(s)(θ)canbewrittenastheaverageofsi.i.d.gradients,suchasthemeansquarederrorin
θ
(2.6). Because this condition does not hold for ∇ L(s) (θ) presented in (2.8), it implies that
θ Cox
s2H−1Σ (H−1)T ≺ sH−1Σ (H−1)T and therefore θ˜SB(s) is asymptotically more efficient
s (s|1) s s s s n
than
θ˜FB(s).
This is not typically observed in SGD optimizations. Take (2.6) for instance,
n
one can verify that s2H−1Σ (H−1)T = sH−1Σ (H−1)T and θ˜FB(s) is therefore as efficient
s (s|1) s s s s n
as
θ˜SB(s)
for mean squared error.
n
Remark 8: The convergence rate established in Theorem 1 still holds if we consider the
FBstrategyforNNtrainingandreplacetheempiricallossin(3.2)by 1 (cid:80) L(s) (f).
m D(s)∈D(n|s) CoxSGD for Cox Models 17
The proof is even simpler because there are no correlations between the batches and the
empirical loss is the average of m i.i.d. tuples. Hence, the convergence rate is O (γ log2m),
p m
which is equivalent to O (γ log2n) as m = n/s with s being a fixed constant.
p n
The impact of batch size on the asymptotic variances can be explicitly described by the
following Corollary from Theorem 2:
Corollary 1: Under the Cox model, suppose the assumptions in Theorem 3 holds, then
for any integer s ⩾ 2, we have
H = E[∇2L(s) (θ)]| = sV[∇ L(s) (θ)]| = sΣ , (4.6)
s θ Cox θ=θ0 θ Cox θ=θ0 s
and
H = E[∇2L(2s)(θ)]| ⪰ E[∇2L(s) (θ)]| = H . (4.7)
2s θ Cox θ=θ0 θ Cox θ=θ0 s
Remark 9: Equality(4.6)indicatesthattheasymptoticvarianceofθ˜FB(s) issH−1Σ (H−1)T =
n s s s
H−1. Then by (4.7), we have H−1 ⪯ H−1, i.e., the asymptotic efficiency of θ˜FB(s) improves
s 2s s n
when the s doubles. The asymptotic variance of
θ˜SB(s)
can not be further simplified to
n
directly evaluate the impact of batch size. Nevertheless, the decrease of its upper bound
sH−1Σ (H−1)T implies the efficiency improvement when batch size doubles. Moreover, such
s s s
an impact of batch size on the asymptotic variance is not typically observed in SGD opti-
mizations. Take the minimization of mean square error in (2.6) for instance, one can verify
that sH−1Σ (H−1)T degenerates to H−1Σ (H−1)T which is not depending on s.
s s s 1 1 1
Remark 10: To the best of our awareness, the equivalence of E[∇2L(s) (θ)]| and
θ Cox θ=θ0
sV[∇ L(s) (θ)]| has not been studied. Theorem 3 shows that these two quantities de-
θ Cox θ=θ0
termine the asymptotic variance of SGD estimators in Cox regression. Moreover, one can
verify that
E[∇2L(s) (θ)]| → I(θ ) and sV[∇ L(s) (θ)]| → I(θ )
θ Cox θ=θ0 0 θ Cox θ=θ0 018
when s → ∞, where I(θ ) is the information matrix of θ . This demonstrates that H−1
0 0 s
decreases towards its lower bound I(θ )−1 as s continues doubling, suggesting that the SGD
0
estimator is less efficient than the GD estimator, whose asymptotic variance is I(θ )−1.
0
Moreover, when batch size is large and H−1 is approaching I(θ )−1, the efficiency gain from
s 0
doubling the batch size would diminish, which has been empirically reported in Goldstein
and Langholz (1992); Tarkhan and Simon (2024).
4.2 SGD for Online Cox Regression
In contrast to the offline Cox regression where the target is to minimize the empirical loss
E[L(s) (θ)|D(n)], the online Cox regression minimizes the population loss E[L(s) (θ)] by
Cox Cox
directly sampling the mini-batch D(s) from the population. For online learning, it is of
ˆ
interest to study the convergence of θ to θ as well as its asymptotic behavior when the
t 0
ˆ
iteration step t → ∞, where θ is the estimator at tth recursion in (2.4). Note that this is
t
an optimization problem of whether an algorithm can reach the global minimizer over the
iterations. It differs from our previous investigation of an estimator’s asymptotic properties
with large sample size, where the estimator is the global minimizer of empirical loss.
Strong convexity of the target function is critical to establish the convergence of SGD
algorithm to the global minimizer (Ruppert 1988; Polyak and Juditsky 1992; Toulis and
Airoldi 2017). It requires the function to grow as fast as a quadratic function. Specifically,
function h(x) is strongly convex if and only if there exists a constant c > 0 and ∇2h(x) ⪰ cI
x
for all x where I is an identity matrix. Unfortunately, the population loss of online Cox
regression
 
1 (cid:88) exp(θTX )
E[L( Cs o) x(θ)] = E − s ∆ ilog (cid:80) I(T ⩾ Ti )exp(θTX )
i:Di∈D(s) j:Dj∈D(s) j i j
is not strongly convex over Rp. It can be easily verified when p = 1 and E[L(s) (θ)] = O(θ) =
CoxSGD for Cox Models 19
o(θ2)whenθ → ∞.Nonetheless,wenoticethatE[L(s) (θ)]isstronglyconvexonanycompact
Cox
set covering the true parameter θ :
0
Lemma 2: Suppose the integer s ⩾ 2 and θ ∈ Rp . Then under Cox model, with
0 B
assumptions (A1)-(A3), (R1), (R2), there exist a constant µ > 0 such that for any θ ∈ Rp
B
νTE[∇2L(s) (θ)]ν ⩾ µ > 0 ∀ν ∈ {ν : ∥ν∥ = 1}.
θ Cox 2
The radius B can be arbitrarily large so that Rp covers the true parameter θ to guarantee
B 0
E[L(s) (θ)] is strongly convex on Rp := {θ ∈ Rp : ∥θ∥ ⩽ B}. Therefore, a modification of
Cox B
(2.7) could be applied to restrict the domain of θ and establish the convergence of SGD
algorithm for Cox regression, which is called projected SGD. That is,
θˆ = Π [θˆ −γ ∇ L(s) (θˆ )], (4.8)
t+1 Rp t t θ Cox t
B
where Π is the orthogonal projection operator on the ball Rp := {θ ∈ Rp : ∥θ∥ ⩽ B}
Rp
B
B
(Moulines and Bach 2011). The projection step ensures the iterations are restricted to the
area where E[L(s) (θ)] is strongly convex. Then we have the following non-asymptotic result
Cox
for the projected SGD algorithm by confirming the conditions of Theorem 2 in Moulines and
Bach (2011):
Theorem 4: Consider the SGD procedure (4.8) with learning rate γ = C where the
t tα
constant C > 0 and α ∈ [0,1]. Under Cox model, with assumptions (A1)-(A3), (R1), (R2),
assume ∥θ ∥ ⩽ B, then for any integer s ⩾ 2, we have
0


 {δ2 +D2C2φ (t)}exp(−µCt1−α)+ 2D2C2 if α ∈ [0,1);
E∥θˆ −θ ∥2 ⩽  0 1−2α 2 µtα (4.9)
t 0

 δ2t−µC +2D2C2t−µCφ (t) if α = 1,
 0 µC−1
where δ is the distance between the initial point of SGD and θ , D = max∥∇ L(s) (θ)∥, and
0 0 θ∈Rp θ Cox
B20
µ is a constant in Lemma 2. The function φ (t) : R+ \{0} → R is given by
β


 tβ−1 if β ̸= 0,

β
φ (t) =
β

 logt if β = 0.

Remark 11: This result demonstrates the convergence of projected SGD to the global
minimizer of E[L(s) (θ)] with large enough iterations. When α ∈ (0,1), the convergence is at
Cox
rate O(t−α). When α = 1, we have t−µCφ (t) ⩽ 1 1 if µC > 1, t−µCφ (t) = logt
µC−1 (µC−1)t µC−1 t
if µC = 1, and t−µCφ (t) ⩽ 1 1 if µC < 1. Therefore, when α = 1, the choice
µC−1 (1−µC)tµC
of C is critical. If C is too small, the convergence is at an arbitrarily small rate O( 1 ).
tµC
While a large C leads to the optimal convergence rate O(t−1), the term 2D2C2 would be
large and result in an increasing bound when t is small so that the orthogonal projection
should be repeatedly operated in the early phase of the training, which leads to additional
calculation cost. The optimal convergence rate can be achieved regardless of C by averaging.
Lacoste-Julien et al. (2012) showed that, for projected SGD recursion (4.8) and if the target
loss is strongly convex on Rp , then ∥θˇ −θ ∥2 = O (1) with learning rate γ = C for any
B t 0 2 p t t t+1
C > 0, where θˇ := 2 (cid:80)t (i+1)θˆ .
t (t+1)(t+2) i=0 i
Remark 12: The convergence of SGD for offline Cox regression can be similarly estab-
lished. If D(s) is sampled from the D(n) using either method considered in Section 4.1,
and if argminE[L(s) (θ)|D(n)] is finite, then θˆ will converge to argminE[L(s) (θ)|D(n)]
Cox t Cox
θ θ
over the iterations (2.7). This justifies our previous investigation of the statistical properties
of θ˜ = argminE[L(s) (θ)|D(n)], as it can be reached by SGD algorithm. The sufficient
n Cox
θ
condition on a given D(n) for the finiteness of argminE[L(s) (θ)|D(n)] can be derived
Cox
θ
following Chen et al. (2009). Please refer to the Supplementary Materials for more details.SGD for Cox Models 21
5. Numerical Implementation and Results
5.1 Impact of Batch Size on the Convexity
We performed numerical experiments to evaluate the relation betweenE[∇2L(s) (θ)]| and
θ Cox θ=θ0
E[∇2L(2s)(θ)]| in Cox-regression with θ ∈ R1. We set θ = 1 and X follows a uniform
θ Cox θ=θ0 0
distribution [0,10]. For each s, we estimated Eˆ [∇ L(s) (θ)] on a neighborhood of θ from
θ Cox 0
the average of 20,000 realizations of ∇ L(s) (θ), where each realization consists of s i.i.d.
θ Cox
time-to-event data (X ,T ,∆ ) from a Cox model with f (X ) = θ X and an independent
i i i 0 i 0 i
censoring distribution.
Figure (2a) presents E[∇ L(s) (θ)] at a neighborhood of θ = 1 with different batch size
θ Cox 0
s. It verifies E[∇2L(s) (θ)]| ⩽ E[∇2L(2s)(θ)]| as we observe the slope increment of
θ Cox θ=θ0 θ Cox θ=θ0
E[∇ L(s) (θ)] at the truth θ = 1 when batch size doubles. The increment is negligible for
θ Cox 0
a large s as discussed in Remark 10. Moreover, θ = 1 is always the root of E[∇ L(s) (θ)]
0 θ Cox
regardless the choice of s.
[Figure 2 about here.]
5.2 Linear Scaling Rule for Cox Neural Network
We conducted a numerical experiment to evaluate whether keeping (learning rate/ batch
size) constant makes the SGD training process unchanged in Cox-NN. The true event time
T∗ is generated from a distribution with true hazard function λ(t|X ) = λ (t)exp(f (X ))
i i 0 0 i
where λ (t) = 1 and
0
√
f (X) = x2x3 +log(x +1)+ x x +1+exp(x /2)−8.6, x ∼ U(0,1),
0 1 2 3 4 5 5 i
where−8.6istocenterE[f (X)]towardszero.Theobserveddatais(X ,min(T∗,C∗),I(T∗ <
0 i i i i
C∗)) where C∗ is generated from an independent exponential distribution. We adjust the
i i
parameter of the exponential distribution to make the censoring rate 30%. Cox-NN is fitted
on the survival data and the full negative log-partial likelihood L(Ntest) is calculated on the
Cox22
test data to make a fair comparison when choosing different batch sizes. Identical to the
spirit of Goyal et al. (2017), our goal is to match the test errors across minibatch sizes by
only adjusting the learning rate, even better results for any particular minibatch size could
be obtained by optimizing hyper-parameters for that case. Figure (2) shows that the linear
scalingrulestillholdsinCox-NNwhiletheslightdifferencesbetweenthetrainingtrajectories
are due to the convexity change of the loss function with different choice of batch size.
5.3 Impact of Batch-size in Cox Regression
We carried out the simulations to empirically assess the impact of batch size in Cox regres-
sion. The survival data generating mechanism is identical to Section 5.2 except for the true
event time T∗ is from λ(t|X ) = λ (t)exp(XTθ ) where θ = 1 , and X i. ∼i.d. Uniform(0,1)
i i 0 i 0 0 10×1 ip
for p ∈ {1,2,...,10}. We performed projected SGD (4.8) with B = 106 and initial point
θˆ(0) = 0 to estimate θ based on N = 2,048 samples. The SGD batch size is set as 2k
10×1 0
wherek = 2,...,9.Thetotalnumberofepochs(trainingthemodelwithallthetrainingdata
for one cycle) is fixed as E = 200 and the learning rate is set as γ = 4 which decreases
E E+1
after each epoch. Besides SGD-SB and SGD-FB, we also fit a stratified Cox model (CoxPH-
strata) by treating the fixed batches from FB strategy as strata. Note that CoxPH-strata
directly solves Eq. (4.3) and gives
θ˜FB(s)
from GD algorithm. The convergence of SGD to
n
argminE[L(s) (θ)|D(n)] can be evaluated by comparing the estimators from SGD-FB and
Cox
θ
CoxPH-strata.CoxPHisfittedunderthestandardfunctionusingallsamples.Thesimulation
is repeated for 200 runs.
[Figure 3 about here.]
We note that θˆ(t) stays inside of Rp throughout the iterations. Hence the projection step is
B
redundant in the simulation and (4.8) degenerates to (2.7). Simulation results are presented
in Figure 3. The convergence of SGD is validated by confirming the loss curve’s stability over
epochs as well as the small difference between the SGD estimator
θˆFB(s)
and the estimator
tSGD for Cox Models 23
from CoxPH-strata
θˆstrata.
Actually
log(∥θˆFB(s)
−
θˆstrata∥2)
< −6 for all s throughout the
2
simulations. As expected, SGD-SB is more efficient than SGD-FB. For both SGD-SB and
SGD-FB, there is efficiency loss when batch size is small. The efficiency loss is negligible
when the batch size is larger than 128 in our simulation.
5.4 Real Data Analysis
We applied the Cox-NN on Age-Related Eye Disease Study (AREDS) data and built a
prediction model for time-to-progress to the late stage of AMD (late-AMD) using the fundus
images. The analysis data set includes 7,865 eyes of 4,335 subjects with their follow-up time
andthediseasestatusattheendoftheirfollow-up.Ourpredictorsincludethreedemographic
variables (age at enrollment, educational level, and smoking status) and the fundus image.
The size of the raw fundus image is 3 × 2300 × 3400. We cropped out the middle part of
fundus image and resized it to 3 × 224 × 224. In this application, the Cox-NN employs
ResNet50 structure (He et al. 2016) to take fundus image as input. Cox-NN is optimized
through SGD on a training set (7,087 samples) using Nvidia L40s GPU with 48 GB memory
and then evaluated in a separate test set (778 samples). In Figure 1, we list the memory
required to perform SGD and epoch time with different choices of batch size in SGD. The
size of 7,087 fundus images in training data is 3.97 GB. Besides loading the batch data,
additional memories are needed to store the model parameters and the intermediate values
for the gradient computation. Therefore, less memory is required to perform SGD with a
smaller batch size. The GD is equivalent to set batch size as 7,087 and can not be performed
in this application. In addition to the lower memory requirement, the time to process the
training data for one cycle (one epoch) is shorter for the SGD with a smaller batch size.
From the trajectories of C-index over the training epochs in Figure 4, it justifies our
discussion in Section 3.2 that the ratio of learning rate to batch size γ/s determines the
dynamics of SGD in Cox-NN training. The SGD training history would be similar when γ/s24
is the same and doubling the batch size is equivalent to half the learning rate. For a fixed
learning rate, SGD with a smaller batch size converges faster than with a larger batch size.
[Figure 4 about here.]
6. Discussion
This paper investigates the statistical properties of SGD estimator for Cox models. The
consistency as well as the optimal convergence rate of the SGD estimators are established.
These results justify the application of the SGD algorithm in the Cox model for large-scale
time-to-event data. Moreover, SGD optimizes a function that depends on batch size and
differs from the all-sample partial likelihood. We further investigate the properties of the
batch-size-dependent function and present the impact of batch size in Cox-NN and Cox
regression.
To improve the interpretability of Cox-NN, Zhong et al. (2022) proposed the partially
linear Cox model with the nonlinear component of the model implemented using NN and
demonstrated the statistical properties of the GD estimator. We expect the SGD estimator
for the partially linear Cox model to have the same properties shown in this work: the NN
estimator achieves the minimax optimal rate of convergence while the corresponding finite-
√
dimensional estimator for the covariate effect is n-consistent and asymptotically normal
with variance depending on the batch size.
The partial likelihood in the Cox model essentially models the rank of event times through
aPlackett-Luce(PL)model(Plackett1975;Luce1959)withmissingoutcomedata.PLmodel
has been widely used in deep learning for various tasks, such as learning-to-rank (LTR) (Cao
et al. 2007) and contrastive learning (Chen et al. 2020). These applications can be seen as
analogous to applying Cox-NN to time-to-event data without right-censoring. Therefore, our
results and discussions could be extended to these tasks as well.SGD for Cox Models 25
Besides the standard SGD considered in this work, the variants of SGD such as Adam
(Kingma and Ba 2014) have been widely used in NN training. We expect those variants
can also effectively optimize the Cox-NN. Recently, it has been noticed that the sharpness
(convexity) of the loss function around the minima is closely related to the generalization
ability of the NN, and the sharpness-aware optimization algorithm has been proposed (An-
driushchenkoandFlammarion2022).Itwouldbeinterestingtoevaluatetherelationbetween
the sharpness at the minima found by SGD and generalization ability in Cox-NN as we
observe the sharpness of the Cox-NN depends on the batch size choice.
References
Amari, S.-i. (1993). Backpropagation and stochastic gradient descent method. Neurocom-
puting 5, 185–196.
Andersen, P. K. and Gill, R. D. (1982). Cox’s regression model for counting processes: a
large sample study. The annals of statistics pages 1100–1120.
Andriushchenko, M. and Flammarion, N. (2022). Towards understanding sharpness-aware
minimization. In International Conference on Machine Learning, pages 639–668. PMLR.
Bauer,B.andKohler,M.(2019). Ondeeplearningasaremedyforthecurseofdimensionality
in nonparametric regression. The annals of statistics .
Bottou, L. (2012). Stochastic gradient descent tricks. In Neural Networks: Tricks of the
Trade: Second Edition, pages 421–436. Springer.
Cao, Z., Qin, T., Liu, T.-Y., Tsai, M.-F., and Li, H. (2007). Learning to rank: from pairwise
approach to listwise approach. In Proceedings of the 24th international conference on
Machine learning, pages 129–136.
Chen, M.-H., Ibrahim, J. G., and Shao, Q.-M. (2009). Maximum likelihood inference for
the cox regression model with applications to missing covariates. Journal of multivariate
analysis 100, 2018–2030.26
Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. (2020). A simple framework for
contrastive learning of visual representations. In International conference on machine
learning, pages 1597–1607. PMLR.
Ching,T.,Zhu,X.,andGarmire,L.X.(2018). Cox-nnet:anartificialneuralnetworkmethod
for prognosis prediction of high-throughput omics data. PLoS computational biology 14,
e1006076.
Cox,D.R. (1972). Regressionmodelsand life-tables. Journal of the Royal Statistical Society:
Series B (Methodological) 34, 187–202.
Cox, D. R. (1975). Partial likelihood. Biometrika 62, 269–276.
Faraggi, D. and Simon, R. (1995). A neural network model for survival data. Statistics in
medicine 14, 73–82.
Goldstein, L. and Langholz, B. (1992). Asymptotic theory for nested case-control sampling
in the cox regression model. The Annals of Statistics pages 1903–1928.
Goyal, P., Doll´ar, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., Tulloch, A.,
Jia, Y., and He, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1 hour.
arXiv preprint arXiv:1706.02677 .
He, K., Zhang, X., Ren, S., and Sun, J. (2016). Deep residual learning for image recognition.
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages
770–778.
Hinton, G., Srivastava, N., and Swersky, K. (2012). Neural networks for machine learning
lecture 6a overview of mini-batch gradient descent. Cited on 14, 2.
Hoeffding, W. (1992). A class of statistics with asymptotically normal distribution. Break-
throughs in Statistics: Foundations and Basic Theory pages 308–334.
Jastrzebski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y., and Storkey, A.
(2017). Three factors influencing minima in sgd. arXiv preprint arXiv:1711.04623 .SGD for Cox Models 27
Jastrzebski, S., Kenton, Z., Arpit, D., Ballas, N., Fischer, A., Bengio, Y., and Storkey, A.
(2018). Width of minima reached by stochastic gradient descent is influenced by learning
rate to batch size ratio. Artificial Neural Networks and Machine Learning–ICANN 2018:
27th International Conference on Artificial Neural Networks, Rhodes, Greece, October
4-7, 2018, Proceedings, Part III 27 pages 392–402.
Katzman, J. L., Shaham, U., Cloninger, A., Bates, J., Jiang, T., and Kluger, Y. (2018).
Deepsurv: personalized treatment recommender system using a cox proportional hazards
deep neural network. BMC medical research methodology 18, 1–12.
Kingma, D. P. and Ba, J. (2014). Adam: A method for stochastic optimization. arXiv
preprint arXiv:1412.6980 .
Kleinberg, B., Li, Y., and Yuan, Y. (2018). An alternative view: When does sgd escape local
minima? In International conference on machine learning, pages 2698–2707. PMLR.
Kvamme, H., Borgan, Ø., and Scheel, I. (2019). Time-to-event prediction with neural
networks and cox regression. Journal of Machine Learning Research 20, 1–30.
Lacoste-Julien, S., Schmidt, M., and Bach, F. (2012). A simpler approach to obtaining an
O(1/t) convergence rate for the projected stochastic subgradient method. arXiv preprint
arXiv:1212.2002 .
Li,H.,Xu,Z.,Taylor,G.,Studer,C.,andGoldstein,T.(2018). Visualizingthelosslandscape
of neural nets. Advances in neural information processing systems 31,.
Luce, R. D. (1959). Individual choice behavior, volume 4. Wiley New York.
Moulines, E. and Bach, F. (2011). Non-asymptotic analysis of stochastic approximation
algorithms for machine learning. Advances in neural information processing systems
24,.
Nair, V. and Hinton, G. E. (2010). Rectified linear units improve restricted boltzmann
machines. In Proceedings of the 27th international conference on machine learning28
(ICML-10), pages 807–814.
Plackett, R. L. (1975). The analysis of permutations. Journal of the Royal Statistical Society
Series C: Applied Statistics 24, 193–202.
Polyak, B. T. and Juditsky, A. B. (1992). Acceleration of stochastic approximation by
averaging. SIAM journal on control and optimization 30, 838–855.
Qi, H., Wang, F., and Wang, H. (2023). Statistical analysis of fixed mini-batch gradient
descent estimator. Journal of Computational and Graphical Statistics pages 1–24.
Ruppert, D. (1988). Efficient estimations from a slowly convergent robbins-monro process.
Technical report, Cornell University Operations Research and Industrial Engineering.
Schmidt-Hieber, J. (2020). Nonparametric regression using deep neural networks with relu
activation function. The Annals of Statistics .
Srinivas, S., Subramanya, A., and Venkatesh Babu, R. (2017). Training sparse neural
networks. In Proceedings of the IEEE conference on computer vision and pattern
recognition workshops, pages 138–145.
Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., and Salakhutdinov, R. (2014).
Dropout: a simple way to prevent neural networks from overfitting. The journal of
machine learning research 15, 1929–1958.
Sun, T., Wei, Y., Chen, W., and Ding, Y. (2020). Genome-wide association study-based
deep learning for survival prediction. Statistics in medicine 39, 4605–4620.
Tarkhan, A. and Simon, N. (2024). An online framework for survival analysis: reframing cox
proportional hazards model for large data sets and neural networks. Biostatistics 25,
134–153.
Therneau, T. et al. (2015). A package for survival analysis in s. R package version 2, 2014.
Toulis, P. and Airoldi, E. M. (2017). Asymptotic and finite-sample properties of estimators
based on stochastic gradients. The Annals of Statistics .SGD for Cox Models 29
Xie, Z., Sato, I., and Sugiyama, M. (2020). A diffusion theory for deep learning dynamics:
Stochasticgradientdescentexponentiallyfavorsflatminima. InInternational Conference
on Learning Representations.
Zhong, Q., Mueller, J., and Wang, J.-L. (2022). Deep learning for the partially linear cox
model. The Annals of Statistics 50, 1348–1375.30
Figure 1: A Cox-NN model is fitted to predict the time-to-eye disease based on the fundus
image and demographics. The model is optimized through SGD on data from a cohort of
7,865 subjects. Each fundus image has dimensions of 3 × 224 × 224 pixels. Cox-NN uses
ResNet50 to take fundus images (3×224×224) as input. The model is trained with 7,087
training data for 30 cycles (30 epochs) and is evaluated on the rest 778 test data after
each epoch. When performing SGD with a fixed learning rate and increasing batch sizes,
both the required memory and epoch time increase, while the convergence speed decreases.
This analysis is conducted on a GPU with 48 GB of memory and GD is impractical due to
hardware memory limitations.SGD for Cox Models 31
(a) Population Gradient with θ =1.00
0
(b) Cox-NN training with linear scaling rule
Figure 2: (a) Estimated Eˆ [∇ L(s) (θ)] at a neighborhood of θ = 1 with different batch
θ Cox 0
size s. Each estimation is based on 20,000 realizations of the mini-batch data consisting of s
i.i.d. samples generated from a Cox model with f (X) = θ X. (b) The negative log-partial
0 0
likelihood L(N)(θ) over the training epochs when optimizing a Cox-NN. It is calculated on
Cox
a separate test data (N = 2,048). The learning rate γ is 0.1/16 when the batch size is 32.
γ is doubled when doubling the batch size and it reaches 0.1 when the batch size is 512
(= 32×16). All the other hyper-parameters are kept the same.32
Figure 3: Boxplots of log(∥θˆ − θ ∥2) over B = 200 runs with sample size N = 2,048.
0 2
SGD with two different batch sampling strategy are considered, wither with fixed batch
sampling strategy (FB) or with stochastic batch sampling strategy (SB). CoxPH-strata is
a stratified Cox model treating the fixed batches from SGD-FB as strata and serves as the
global minimizer for SGD-FB. Throughout the simulations,
log(∥θˆFB(s)
−
θˆstrata∥2)
< −6,
t 2
demonstrating the convergence of SGD-FB algorithm.SGD for Cox Models 33
Figure 4: The C-index of Cox-NN over training epochs in a real data application. The
Cox-NN is optimized by SGD with different choices of batch size and learning rate. All the
other hyper-parameters are fixed. C-index is calculated on a separate test data with 778
samples after each training epoch. Cox-NN is trained through SGD with fundus image and
demographic as predictors to predict time-to-eye disease.