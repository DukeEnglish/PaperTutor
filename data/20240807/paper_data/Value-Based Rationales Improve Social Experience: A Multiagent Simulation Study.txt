Value-Based Rationales Improve Social Experience:
A Multiagent Simulation Study
Sz-TingTzenga,NiravAjmerib andMunindarP.Singhc
aUmeåUniversity,Umeå,Sweden
bUniversityofBristol,Bristol,UK
cNorthCarolinaStateUniversity,Raleigh,NC,USA
ORCID(Sz-TingTzeng): https://orcid.org/0000-0001-9304-6566,ORCID(NiravAjmeri):
https://orcid.org/0000-0003-3627-097X,ORCID(MunindarP.Singh): https://orcid.org/0000-0003-3599-3893
Abstract. WeproposeExanna,aframeworktorealizeagentsthat oftheirstakeholders[34,35,36],includingprovidingandrecogniz-
incorporatevaluesindecisionmaking.AnExanna agentconsiders ingfelicitousrationalesforone’sbehaviors[18].Deliberatingover
thevaluesofitselfandotherswhenprovidingrationalesforitsac- others’valuescanenhancepersuasivenessandfosteracceptance.
tionsandevaluatingtherationalesprovidedbyothers.Viamultiagent Insteadofsharingallavailableandrelatedinformationinaratio-
simulation,wedemonstratethatconsideringvaluesindecisionmak- nale,itisbeneficialforanagenttoshareonlytheinformationthat
ingandproducingrationales,especiallyfornorm-deviatingactions, aligns well with self and others’ values. Sharing such information
leads to (1) higher conflict resolution, (2) better social experience, preservestheprivacyoftherationaleproviderandensuresthatun-
(3)higherprivacy,and(4)higherflexibility. necessaryinformationdoesnotinundatetheobserverofarationale.
Contribution and Findings Accordingly, this paper extends be-
1 Introduction yondexistingresearchbyconsideringvaluesindecisionmakingand
rationalegenerationandevaluation.OurExanna frameworkgener-
A social norm states a shared standard of acceptable behavior in a
atesrationalesthatincorporatevaluesandincludeonlytheinforma-
society[31]andprovidesabasisforlegitimateexpectationsregard-
tionneededtojustifyadecision.
ingthebehaviorofothersinthesociety.Eachagentplaystworoles:
WeevaluateExanna viaamultiagentsimulationbasedonapan-
actorandobserver.Whileexercisingautonomy,anactorcandeviate
demicscenario.Weconsidersocietiesofagentswithdifferentkinds
fromthenorms[26].Suchdeviationsmayresultinsocialconflicts
ofrationales:Share-All,Share-Rules,andsharevalue-alignedrules.
andtriggerpositiveornegativesanctionsfromobservers.Anaccept-
With Exanna, we find that agents who consider value importance
ablerationale[33]canjustifyadeviationfromasocialnorm.
whengivingrationalesexhibitenhancedconflictresolutioncapabili-
Example1 Sharing a rationale. Alice wears a mask to the office ties.Additionally,rationalesalignedwithvalues,albeitwithlessin-
andnoticesthatBellaisnotwearingamask.Bellajustifiesherdeci- formationprovided,contributetomorefavorablesocialexperience.
sionbystatingthat,first,theofficehasnomaskmandateasthesur- Novelty Although prior research supports constructing explana-
roundingenvironmentissafe.Second,shehateswearingamaskbe-
tionsormakingdecisionsbasedonvalues,thisisthefirststudyto
causewearingonegiveshereczema.AliceagreeswithBella’sview.
investigatehowvaluesguideproducingandusingrationalesfornorm
violationtosupportnormemergenceandimprovesocialexperience.
Arationaleprovidestheinformationtojustifyadecision[15].In
practice,rationalesincludeadditionalinformationthatothersmaybe Organization Section2discussesrelevantrelatedworks.Section3
unabletoobserve,suchastheactor’sbeliefsandpreferences.Craft- detailstheExanna framework.Section4describesasimulatedpan-
ingarationaleremainsanongoingchallenge.Rationalesmaybever- demic scenario for evaluation. Section 5 demonstrates the results.
bose,leadingtoinformationoverload.Additionally,theymighten- Section6concludeswithlistingpotentialfuturedirections.
compassprivateinformationthatonemaybehesitanttodisclose,a
concernparticularlyprevalentinhealthcaresettings.
2 RelatedWork
Example2 Adaptingarationale.BellaandAliceshareaconcern
Researchonagentsinteractingbasedontheirrationalesandmodel-
for health. Despite Bella’s aversion to wearing a mask because of
ingvaluesisrelevanttoourapproach.
eczema,giventhesafeenvironment,shefeelsitunnecessarytodis-
closeherskincondition.Bellarationalizesherbehaviorofnotwear- Agents and Rationales Hind et al. [14] leverage existing super-
ingamaskbystatingthatthesurroundingenvironmentissecureand vised machine-learning techniques to generate rationales together
thatamaskisunnecessary.AlicefindsBella’srationaleacceptable. with decisions without values involved and without exposing the
inner details of the model. Whereas Hind et al. generate rationales
Values are motivational bases of one’s behavior [23]. Reasoning basedontheexistingtrainingset,Exannageneratesrationalesbased
aboutvaluesisanessentialcapabilitytoalignagentswiththevalues oncontextandvalues.
4202
guA
4
]AM.sc[
1v71120.8042:viXraGeorgaraetal.[11]showhowtobuildrationalesonwhyspecific adviceonpossibleactionstoimproveperformance.Whereasprivacy
teamsareformed.Specifically,Georgaraetal.buildrationalesbased isarightmotivatedbyvalues,Exanna encompassesbroaderaspects
on contrastive explanations and by exploring what-if scenarios. A ofcorevaluesthatserveasguidingprinciplesfordecisionsandra-
causalattributionexplainswhyabehavioroccurs.Weprovidecausal tionales.Additionally,rationalesfocusonexplainingdecisionswhile
attributionoftheselectedaction,preciselythepremise,asrationale theseworksprioritizemaintainingprivacyamongmultipleusersvia
andwithholdprivateinformationbasedonvalues. argumentation.Aycıetal.[4]explaintheprivacydecisions(sharing
Wangetal.[32]formulaterationaleswiththesimplestsubsetof content) using labels (private or public) that are assigned to topics
featuresthatissufficientascausalattributionforprobabilisticsolid predicted by machine learning. While their values (privacy) come
guaranteesonmodelbehaviorunderobserveddatadistribution.Con- from post-interpretation from humans, Exanna incorporates values
treras et al. [7] propose a mirror model and assume a high under- inrationaleconstruction.
standabilityfromperformingsimilartoanobserver’smentalsimu- Table 1 summarizes the above comparisons, emphasizing values
lation. They apply deep Q-network and saliency maps in rationale andrationales.Whereasexplanationsimplyclarityandsatisfaction
generation, highlighting related input features as rationales. These to the recipient, we emphasize justifying a decision based on un-
worksrevealmodelfeaturesbutnotconsidervalues. derlyingvalueswithoutmodelingtherecipient’scomprehensionor
Ajmerietal.[2]proposePoros,aframeworkthatsharesfullcon- acceptanceoftherationale.
text as a rationale. Therefore, agents can adopt the perspectives of
othersandmakecorrespondingdecisions.However,Ajmerietal.do
3 Method
notconsidervalues.InExanna,anactorselectivelysharesinforma-
tionbasedonitsvaluesandthoseoftheobserver.
We now describe the schematics and decision making in Exanna
Agents,Norms,andValues Tzengetal.[29]definesocialcom- alongwithitsrationalecomponents.
munication (sanction, message, and hint), which besides actual re-
ward or punishment, indicates normative information and potential
3.1 SchematicsofanExannaAgent
outcomes.Unlikesignalingotherswithnormativeinformation,ratio-
nalescanenableinformationsharingandconflictresolution.Tzeng
Belief: Anagent’sviewoftheworld,formedbasedonobservations.
et al. [28] incorporate social value orientation (SVO) in decision
making. Whereas values define what is important to agents, SVO
btindicatesthebeliefattimet.Abeliefiscapturedasasetofpairs
ofattributesandbindings.
describes the importance an agent places on its gain in relation to
Context: The factors that characterize the situation of an agent.
others.Exannacoversabroaderrangeofmotivationsandbehaviors.
Contextisrepresentedasasetofattribute-bindingpairs.Anex-
Cranefieldetal.[8]representagents’planstoachievegoalsasa
ampleofcontextisasfollows.
goal-plan tree and expand the Belief-Desire-Intention language by
annotatingactionswiththeeffectsregardingvalues.Lera-Lerietal. {Risk=None, Preference= Wear,
[16]considerethicalprinciples(e.g.,maximumutilityandmaximum
InteractWith=Colleagu¬e,
OtherAgentType=Health,
fairness) for aggregating value systems, not just one value. Ajmeri
RiskFromAnother=High,
et al. [3] aggregate users’ value preferences to make ethically ap-
OtherAgentPreference=Wear,
propriate decisions. Besides making decisions based on aggregate
Location=Office}
valueimportance,Exanna agentsgeneraterationalesfortheirdeci-
sionswithnecessaryinformation. Acontextcomprisespublic(e.g.,anagent’slocation)andprivate
Agrawaletal.’s[1]agentlearnsnormsasrulesofoptimalbehav- (e.g.,beliefs,preferences,andvalues)factors.Contextualfactors
iors,butconsidersnovalues.Exannaadaptivelyshareslearnedrules maybeassociatedwithvalues(e.g.,RiskrelatestoHealth).
asrationalesthatalignwithindividuals’values. Goal: Asetofstatesthatanagentwantstoachieve.Theoutcomeof
MoscaandSuch’s[20]agentsupportsvaluesinmultiusersettings agoalafterperformingtheselectedactionsisbinary:achievedor
byconsideringthepreferencesandvaluesofusers.Theagentsjustify not.
solutionsthroughcontrastiveexplanationsandpositiveanswers.Ex- Action: Ameanstochangethestateinpursuitofone’sgoalsand
annapresentsfactorsinalignmentwithvaluesratherthanpresenting maximizeassociatedpayoffs.
allthatconveycausalattribution. Preference: Asubjectiveinclinationforanactionoverthealterna-
Whereas other works construct explanations with values [33] or tives.
make decisions based on values [8], our focus is on building suit- Decisionrule: A mapping between a premise (set of attribute-
ablerationalesandinvestigatinghowvalue-alignedrationalesshape binding pairs) and a consequent (an action to be taken). An ex-
decision-makingandinfluencesocialexperience,especiallyprivacy. ampleruleis
Previousresearchconsidersonlythecausallinksbetweenthebehav-
{Risk=None, InteractWith=Colleague} =>
iorsandeachcontextualfactorbutnottheimportanceofthefactors.
Wear
Isitessentialtoincludeeverysinglerelatedfactor?Numerousfac- ¬
torsmaycomeintoplayinreal-worldsituations,yetpeopletypically Norm: Theexpectedbehaviororthebehaviorofthemajorityina
donotprovideorneedtoprovideexhaustiveexplanationstoaccount group. When a majority applies the same decision rule, the rule
foreveryoneofthesefactors. becomesanorm.InExanna,anormusesthesameif-thenrepre-
OgunniyeandKökciyan[21]proposeanontologytorepresentthe sentationasadecisionrule.
privacydomain.Theyintroduceargumentation-basedmultipartydi- Sanction: Aresponsetonormviolationorsatisfaction.Asanction
aloguestoreasonaboutcontextualnorms,andresolveprivacycon- canbeapositiveornegativereactionfromoneagenttoanother.
flicts. Di Scala and Yolum [10] propose an argumentation-based Payoff: Thebenefitsanagentreceivesinagivenstateaftertaking
agenttoachieveagreementonprivacyamongmultipleusers.Their anaction.Payoffsinvolveintrinsicbenefits(e.g.,preferences)and
agentprovidesfeedbacktotheuser,suchasasummaryordetailed extrinsicbenefits(e.g.,sanctionsimposedbyothers).Table1:Summaryofcomparisonswithrelatedworkwithrespecttotheapplicationofvaluesindecisionmakingandinthegenerationand
evaluationofrationales.Withholdmeansthatnotallfactorsarepresentedintherationale.
Valuesappliedin
Rationale Withhold Rationalerepresentation
Decision Rationale
Cranefieldetal.[8] ✗ ✗ ✓ ✗ Norationalesprovided
Ajmerietal.[3] ✗ ✗ ✓ ✗ Norationalesprovided
Lera-Lerietal.[16] ✗ ✗ ✓ ✗ Norationalesprovided
Tzengetal.[28] ✗ ✗ ✓ ✗ Norationalesprovided
Agrawaletal.[1] ✓ ✗ ✗ ✗ Normascausalattributionbutnoinformationhiding
Georgaraetal.[11] ✓ ✗ ✗ ✗ Originalallocationandanothersolutionwithconstraints
Contrerasetal.[7] ✓ ✗ ✗ ✗ HighlightedinputfeaturesindeepQ-networkbutnoinformationhiding
Wangetal.[32] ✓ ✗ ✗ ✗ Predictionandaminimumsubsetofinputsbutnoinformationhiding
Hindetal.[14] ✓ ✗ ✗ ✗ Textspredictedviasupervisedlearning,alongwiththepredictedaction
Ajmerietal.[2] ✓ ✗ ✗ ✗ Fullcontext
MoscaandSuch[20] ✓ ✗ ✓ ✓ Suggestedactionbasedoninputsfromallusersandpossibleoutcomeofthe
user’spreferenceascausalattribution,butnoinformationhiding
Winikoffetal.[33] ✓ ✗ ✗ ✓ Englishmappingoftraversednodesfromgoal-treerelevanttotheexplanation
Exanna ✓ ✓ ✓ ✓ Behaviorrules(withinformationhiding)andalignmentwithvalues
Values: General motivations of agents. Specifically, values define totheobserverwhowitnessesitsbehavior.Uponreceivingaratio-
what agents believe to be important, while goals are the desired nalefromtheactor,theobserverevaluatestherationalebymaking
states.Whereasgoalsaretime-boundedanddynamictocontext, ananalogousdecision.Withaweightedsumofpayoffs,weincorpo-
values are long-lasting and stable and may transcend contexts ratevaluesindecisionmakingwhereasubstantialvaluecastsamore
[17].Asubsetofvaluesisapplicablewithinacontext[17],and significanteffectonthefinaldecision.
eachagentassignsanimportanceratingtoeachvalue.
Valueimportance: the importance of values in one context [23].
WestoreeachvalueimportanceVcontext inatuplewherenum- Form Select & Generate
Receive
bersaddupto1.vi denotestheweightofonevalueinonevalue Observe beliefs & perform & share sanction
importance(vi ∈Vcontext)where0 ≤vi ≤1and(cid:80)n i=1vi =1. goals action rationale
We treat each Vcontext as an attribute and store the corre-
⟨ ⟩
spondingvalueimportanceasitsbinding.Forinstance,anagent
with value importances V = Vpandemic = vhealth = 0.6, Observe Evaluate Generate
{ { rationale sanction
vprivacy = 0.4 ;Vnormal = vhealth = 0.4,vprivacy = 0.6
} { }}
indicatesthattheagentvalueshealthoverprivacyduringapan-
demicbuttheoppositeinanormalcontext.
Figure1:InteractionsbetweenExannaagents.
3.2 PayoffCalculationwithValues Algorithm1givesthepseudo-codeofanagent’sdecision-making
loop.Anagentformsbeliefsbtabouttheworldbasedonitsobserva-
Whereas preferences define the tendency of an individual to make tions(Line4).Anagent’spayoffisaweightedsumofpayoffscorre-
a subjective selection among alternatives, values define the impor- spondingtoitsvalues,goalachievement,andfactorsinfluencingthe
tant things to an individual. Although both values and preferences decision-makingprocess,suchassocialcircleorsocialenvironment.
arecontext-specific,valuesmaytranscendcontexts[17].Eachagent The Q function in Line 7 and reward in Line 8 refer to the payoff
stores values in a tuple where each value maintains a correspond- calculationinSection3.2,whichincorporatevalueimportancesand
ingMindividual.Sinceagentsdonotmakedecisionswithsinglevalues feedbackfromothers.InLine7,theagentselectstheactionthatgives
butwithtradeoffsamongmultiplerelatedvalues,weaggregatevalue thebestpayoffforbt.Iftheagentinteractswithanotheragent,for
importanceswhenconstructingapayoff[3,22].Below,f istheag- itsactiontheagentcreatesrationalesbasedonbt (includingvalues
gregatedpayoffwithallcorrespondingvaluesafterselectingstrategy ofbothparties)andtheselectedaction(Line11withAlgorithm2)
RxwhentheotherplayerselectsstrategyCyfromMindividual. andsendsthoserationales.Otheragentswhoobservetheactionand
receivetherationalesupdatetheirbeliefs,evaluatetherationales(Al-
values gorithm3)withtheircontext,andgivesanctionsaccordingly.Algo-
(cid:88)
f = vi ri,RxCy (1) rithm4definesafunctiontoretrievevalueimportancefrombeliefs
×
i basedoncontext.Exanna appliestoscenariosthatcanbemodeled
We model interactions with payoffs f from the aggregation of asapartiallyobservableMarkovdecisionprocessandwithclearly
Mindividual. definedlinksbetweenvaluesandcontextualfactors.
3.3 InteractionandDecisionMaking 3.4 RationaleGeneration
InteractionsinExannaarebetweenanactor(rationaleprovider)and Rationale generation in Exanna follows a rule learning process—
anobserver(rationaleobserver),asshowninFigure1.Anactorse- aprocessofevolvingrulesfromdatasetsorinteractions.Thebasic
lectsanactionbasedonitsgoalandbeliefsandprovidesarationale formofaruleisifpremisethenconsequent,wheretheconsequent
rotcA
revresbO3.4.2 GeneratingValue-BasedRationale
Algorithm1Decision-makingforanExannaagent
1: Initializeagent(includingvalueimportancesV andothermental Not all factors of a rule generated by the rule learning process are
states) suitableforinclusioninarationale.InExample2,sharingpersonal
2: Initializerule-valuefunctionQ preferencesisunnecessarywhenbothagentsvaluehealth.Aftergen-
3: fort 1,...,T do eratingthebaserule,wepost-processitsfactorsusingthevaluesof
∈{ }
4: Formbeliefsbtbasedonperceivedstate the actor and observers. Thus, the agent who prefers the value of
5: IdentifyactionspaceA health adjusts its rationale for the colleague who also cares about
6: Vactor,Vobserver=GetValueImportance(bt) healthtoahealth-relatedcausalattributionifitexists.Forinstance,
7: Withaprobabilityϵselectarandomactionaactor A nomaskisrequiredbecausethereisnoriskofinfectionwheninter-
∈
Otherwiseselectaactor=argmaxaQ(bt,a,Vactor) actingwithacolleague.
8: Executeactionaactorandobserverewardrt Algorithm2detailstheprocessofconstructingrationale.Anagent
9: ifanyobserveragentobserverexistthen identifiesitsrulesassociatedwithbeliefsbt(Line3)andtheselected
10: /*Generaterationalesbasedonselectedactionandbeliefs action (Line 4) and then aggregates all rules related to a rationale
*/ (Line5).Tominimizeinformationexposure,anagentrevealsprivate
11: Ratactor=GenRationale(bt,aactor) informationonlyifitisassociatedwithitsvaluesorthoseofothers
12: SendRatactortoobserver involvedintheinteraction(Line8–9).Forinstance,ifanagentwho
13: Observeagentobserver’sactionaobserver caresaboutfreedominteractswithonewhocaresaboutfreedom,it
14: if Receive rationales Ratobserver from agent observer willexcludetheinfectionriskfromtheenvironmentinitsrationales.
then Foreachrationale,anagentestimatesprivacybasedontheleastpro-
15: UpdatebeliefsbtbasedonRatobserver portionofprivatefactorsincludedintherationale(Line13).
16: endif
17: /*Generatesanctionsbasedonbeliefsandrationales*/ Algorithm2Rationalegeneration
18: sanctionactor=EvalRationale(Ratactorifany,aactor,bt) Input:beliefsbt,Actiona
19: endif Output:RationaleRat
20: /*Agentslearnfromrewardandsanction*/ Function:GenRationale
21: learn(bt,aactor,rt+sanctionactor,bt+1) 1: IdentifyprivatefactorsP bt
22: endfor 2: /*Generateassociatedrule⊆ swithbeliefsbt*/
3: Getmatchsetmswithbt
holds whenever the premise is true. We adapt XCS [5], which ap-
4: Generateactionsetfrommswitha
pliesageneticalgorithmandreinforcementlearningtoevolveasetof
5: AggregaterulesRatassociatedwithactionset
rulesorstrategiesbasedonpayoffsorrewardsproducedbythepro-
6: Vactor,Vobserver=GetValueImportance(bt)
posedactions.Unlikeothermachinelearningtechniques,XCSgen- 7: forfactorinRat do
eratesasetofrulesdescribingitsdecision.XCSprocessenablesflex- 8: iffactor P andfactornotrelatedtoVactor andVobserver
ibilityfortheimplementationofnormsandsupportsinterpretability then ∈
byproducinglogicalrules.AnexampleruleofExample2is
9: removefactorfromRat
{Risk=None, InteractWith=Colleague} => Wear 10: endif
¬
11: endfor
The premise of a learned rule is a conjunction of attribute-binding
12: /*Computeprivacybasedonproportionofprivatefactors*/
pairs,e.g.,{Risk=None,InteractWith=colleague}.Itsconsequentis 13: privacy=1 #privatefactorsrevealedinRat
anactiontobetakenwhenthepremiseholds—intheaboveexample, − #privatefactorsinRat
Wear. Each rule associates (1) a fitness, i.e., its suitability, (2) a
¬
numerosity, i.e., the number of its instances in the rule set, (3) the
expectedrewardiftheruleapplies,and(4)predictionerror.
3.5 RationaleEvaluation
Onreceivingarationalefromtheactor,theobserverfirstupdatesits
3.4.1 XCSforRationaleGenerationBriefly
beliefsbasedontherationale.Specifically,theobserverupdatesthe
The key features of XCS are Rule Discovery, Rule Subsumption, beliefsofunobservableinformationfromtheactor’scontext.Inthe
andActionSelection.Rulediscoverythroughthecrossoverandmu- rationalegenerationmaskexample,theobserverupdatesitsbeliefs
tationprocessesinvolvesintroducingrandomnesstotheantecedent oftheinfectionriskto“None”.Whenevaluatingarationale,theob-
byaddingorremovingfactors,therebygeneratingrulesthataremore servermakesananalogousdecisionbasedontheupdatedbeliefs.If
generalormorespecific.Giventworules,ifthemoregeneraloneex- theobserver’scomputedactionmatchestheactor’sobservedaction
hibitslowerpredictiveerrorwithinthegivencontext,thealgorithm inthatcontext,theobserveracceptstheactor’srationale.
retainsitanddiscardsthemorespecificone.Whenselectinganac- Algorithm 3 defines how agents evaluate rationales. Initially, in
tion,thealgorithmselectstheonewiththebest-aggregatedfitness. Line2,theobserverupdatesitsbeliefsbt basedontheprivatecon-
DetailsonXCSareinAppendixA. text or beliefs included in the actor’s rationale. Subsequently, the
Anexampleofarationalefornotwearingamaskis{Risk=None, agentmakesadecisionanalogoustotheactor’scontextbasedonthe
Preference= Wear, InteractWith=colleague}. This rule means the updated beliefs. Specifically, with the provided rationale, an agent
¬
mask is not needed when there is no infection risk and the actor checksifanyapplicablerulesalignwithitsrulesetsinLine3.The
prefersnottowearamaskwhileinteractingwithacolleague.Each agentidentifiesassociatedrulesfrombtandaddsthemtoapplicable
agentkeepstherulesitdiscoversandevolvesthoseinarulesetfor rules in Line 4. In Line 7, the agent calculates the fitness for each
decisionmaking. available action for each applicable rule and keeps the best actionforeachrule.Theagentacceptsthisrationaleifanyselectedaction theymovetohome,office,andparty,i.e.,anagentismorelikelyto
matchestheobservedaction. visittheirownhomeratherthansomeoneelse’shome.
Eachagentformsitsgoalbasedonitsvalueimportances.Specifi-
Algorithm3Evaluatingarationale cally,eachvalueinonecontexthasapayoffmatrix(Table5and6);
Input:RationalesRat,Observedactionaactor,Beliefsbt theweightedsumofthepayoffdeterminesthegoal(desiredstates).
Output:Decisiond Anagentselectinganactionthatdoesnotalignwithitsgoaliscon-
Function:EvalRationale sidereddeviatingfromitsgoal.
1: Initializeapplicablerulesars Inthesimulatedenvironment,whenanagentencountersanother
2: UpdatebtwithprivateinformationinRat agent at the same place, it chooses an action based on its goal—
3: AddtriggeredmatchsetfromRattoapplicablerulesars whethertowearamask.Inaddition,theagentjustifiesitsbehavior
4: Addtriggeredmatchsetfrombttoapplicablerulesars based on its beliefs in that context. For instance, the agent gives a
5: forruleinapplicablerulesarsdo rationale—{Risk=None,InteractWith=Colleague}—whilenotwear-
6: foractinpossibleactionsdo ingamask.Thebeliefsofanagentincludepublicandprivatefac-
7: calculatefitnessfact tors.Eachagentreceivesapayoffaccordingtotheinteractionplace
8: endfor foractionselection,asinTable2.Wearingamaskatahospitaldur-
9: Keeptheactwithbestfitness ingapandemicisdesirable.Placeandvalueimportancesdetermine
10: endfor thepayoffanagentgivestoitself.Anagentalsogivessanctionsas
11: ifactcontainsaactorthen feedbacktoothersbasedontheiractions.
12: Decisiond=accept; Thesanctionsarebasedonthesocialcircle.Table3liststhesanc-
13: else tionsassociatedwithsocialcircles.Weruneachsimulation10times,
14: Decisiond=reject; witheachrunlasts30,000steps.Weconsiderthevaluesoffreedom
15: endif and health. In this setting, freedom refers to agents claiming their
freewill andadheringtotheirpreferences.
Algorithm4Getvalueimportance
Table2:Actor’spayoffbasedontheplace.Numbersreflectgeneral
Input:Beliefsbt
expectationsofplaces.
Output:ValueimportanceVactor,Vobserver
Places Wear ¬Wear
Function:GetValueImportance
Home −0.25 0.25
1: context=get_context(bt) Office 0.25 −0.25
2: /*Retrieveactorandobserver’svalueimportancesfrombtbased Party −0.25 0.25
oncontext*/ Park −0.50 0.50
3: Vactor,Vobserver =get_value_importance(bt,context) Hospital 0.50 −0.50
4: ifnoobserverexistthen
5: Vobserver =null
6: endif
Table3:Feedbackfromanobserverbasedonsocialcircle.
Social Observermove
Circle
Reject Accept
4 Simulation
Family −1.00 1.00
Real-world factors may be associated with underlying values. For Friend −0.75 0.75
instance,healthstatecorrespondstohealthconcerns.Weassumethe Coworker −0.50 0.50
Stranger −0.25 0.25
importanceoffactorsaccordingtotheirassociatedvalues.
WeevaluateExannaviaapandemicscenariobasedonExamples1
and2andsimulatedusingMASON[19].Hereagentsmovetovar-
iousplaces,interactwithotheragents,decidetowearornotweara
Table4:Payoffscorrespondingtoapreferenceforwearingamask.
mask,andprovideajustificationfortheiractions.
Agent2
Wear ¬Wear
4.1 Scenario
Wear 1.00 1.00
¬Wear −1.00 −1.00
Theenvironmentrepresentsamultiagentsocietywithseveralplaces
and social circles. Our environment involves a finite population of
200 agents with different social circles. The environment includes
one park, one hospital, five homes, five offices, and five parties.
Table 5: Payoffs corresponding to a preference for not wearing a
Agentsmovearoundandinteractwithinthesefiveplaces.Eachagent
mask.
isnativetoonehome,oneoffice,andoneparty.Agentsinthesame
Agent2
home,office,orpartysharethesamefamily,colleague,orfriendso-
Wear ¬Wear
cialcircle.Eachsocialcirclehas40agents.Timeisrepresentedin
steps.Eachagentmovestooneplaceateachstepandhasaprobabil- Wear −1.00 −1.00
ity(50%)ofinteractingwithoneagentatthesameplace.Agentsare ¬Wear 1.00 1.00
morelikely(75%)tomovetoplacestheyareassociatedwithwhen
1tnegA
1tnegA5 Results
Table6:Payoffsforthevalueofhealth.Thenumbersreflecthowsafe
Table 8 summarizes our statistical analyses. Exanna offers higher
anagentfeels.
socialexperience,conflictresolution,andflexibility,indicatingthat
Infectionrisk
Exannaagentslearntoactforthegreatersocietalgood.Theseresults
Norisk Highrisk
followourintuitionthatvalue-alignedrationalesaresuperior.How-
Wear 0.00 1.00
ever,ifanagentpreferstokeepcertaininformationprivate,deviation
¬Wear 0.00 −1.00
fromgoalsisexpected.ResultsfortheShare-AllandShare-Rulesso-
cietiesindicatethatincreasedinformationinarationalemaynotbe
4.2 ContextualProperties
helpful.
Whereas agents have limited observations on the environment, the
context includes the place (home, office, party, park, and hospital) Table8:Results:Comparingmean(X¯)andstandarddeviation(σ)of
whereinteractionsoccur,therelationship(family,friend,colleague,
socialexperience,resolution,privacy,andflexibilityinvarioussoci-
andstranger)withtheobserver,thesubjectivebeliefofinfectionrisk
eties.pisp-valuefromt-test.
of the environment, the personal preference on mask-wearing, and
Share-All Share-Rules Exanna
the types of observers. Due to the partial observations, agents act
basedontheirbeliefsandupdatethebeliefswithgivenrationales. X¯ 0.58 0.58 0.60
σ 0.02 0.01 0.02
p <0.001 <0.001 –
4.3 TypesofSocieties ∆ 1.80 3.11 –
X¯ 0.59 0.62 0.70
Wedefinetypesofsocietiesbasedontherationaletypes.Allsocieties σ 0.06 0.02 0.05
include50%ofagentsvaluehealthand50%ofagentsvaluefreedom. p <0.001 <0.001 –
ThevalueimportancesofagentsarepresentedinTable7.Allagents ∆ 1.80 3.11 –
optimizetheirbehaviorbasedontheweightedsumofpayoffsfrom X¯ 0.00 1.34e−5 0.25
themselvesandothers. σ 0.00 2.11e−5 2.90e−3
p <0.001 <0.001 –
Baseline1:Share-AllSociety Agents share all information as ra- ∆ ∞ 11896.52 –
tionalesandarecapableofassumingtheviewpointsofothersto X¯ 0.10 0.09 0.12
facilitatedecisionmaking. σ 0.01 0.01 0.03
Baseline2:Share-RulesSociety Agentssharetheirdecisionrules p 0.08 <0.01 –
∆ 1.59 2.89 –
asrationales.
Exanna:ShareValue-AlignedRulesSociety Agents share their
decisionrulesalongwithselectedinformationthatalignswithval- H Resolution Figure 2 compares conflict resolution in various soci-
uesofagentspresentasrationales. eties.Exanna offersbetterconflictresolution(p<0.001;∆>0.8,
indicatingalargeeffect)thanothersocieties.Thus,werejectthenull
hypothesis corresponding to H . We observe that, in scenar-
Resolution
ioswhereotheragentsdonotaccepttheprovidedrationales,Exanna
Table 7: Value importances of agents in a pandemic setting. Each agents more flexibly deviate from their own goals to resolve con-
societyhashalfFreedom-lovingandhalfHealth-freakagents. flicts.Ourresultsdemonstratethedynamicsofagentbehaviorsand
Agents:Values Freedom Health thestrategyofrationales.
Freedom-loving 1.00 0.00
Health-freak 0.00 1.00 65
60
4.4 Evaluation
WerunsimulationswiththeShare-All,Share-Rules,andExannaso-
55
cieties.Weevaluatehypothesesonresolution,socialexperience,pri-
Share-all Share-rules Exanna
vacy,andflexibilityusingthefollowingmetrics.
Society
M [0,100] Percentageofrationalesaccepted.
MR Soe cso ial lution ∈ ∈[–3,3] A ceg ivg ere sg fa ot re id tsp ba ey ho af vf iot rh .at an agent re- F cii eg tu iere s.2 T: hC eo Em xp aa nr nin ag at gh ee ntre ss oo cl iu et ti yon ha( sM bR ee ts to elu rti ro en) soin lutv ia or nio (u Gs laa sg se ’n ∆tso >-
M [0,1] Proportion of private information re- 0.8;p<0.001)thanthebaselinesocieties.
Privacy ∈
tainedduringaninteraction.
M Flexibility ∈[0,1] Extent of deviation from an agent’s H SocialExperience ForH SocialExperience,wemeasuretheoverallpayoffs
owngoal. of agents in a society. An agent’s payoff includes personal payoff
We conduct the independent t-test across pairs of societies. We from its action and feedback from its interaction. Figure 3 com-
measureeffectsizewithGlass’s[12]∆sincethesocietieshavedif- paresthesocialexperienceforShare-All,Share-Rules,andExanna
ferent standard deviations [13]. We adopt Cohen’s [6] descriptors agentsocieties.WefindthatExanna yieldsbettersocialexperience
tointerpreteffectsize:<0.2(negligible),[0.2,0.5)(small),[0.5,0.8) (p < 0.001; ∆ > 0.8, indicating a large effect)) than other soci-
(medium),and 0.8(large). eties.Specifically,Exannaagentsreceivebetterfeedbackfromother
≥
noitcA
%noituloseRtciflnoC
noituloseRM
laicoSM
ycavirPM
ytilibixelFMagentswhoreceivetheirrationales.Thus,werejectthenullhypoth- 6 Conclusions,Limitations,andDirections
esiscorrespondingtoH .Oncloseranalysis,weobserve
SocialExperience
Responsible autonomy requires that agents represent, reason with,
thatExanna agentsreceivemorenegativesanctionsthanothersoci-
and communicate values in their rationales. We demonstrate via a
etiesinitiallybutsoonlearntodeviatefromtheirgoals.
multiagentstudyhowwecouldcreateagentswhoincorporatevalues
indecisionmakingandinrationalegenerationandevaluation.Value-
0.6 alignedrationalesofferbettersocialexperienceandhigherconflict
resolution.Whereasvalue-alignedrationaleswithholdpartialinfor-
0.4
mation,agentslearntodeviatefromtheirgoalstoprotecttheirpri-
Share-all
0.2 Share-rules vacy.Specifically,agentswhoreceiverejectionsfromothersbecome
moreflexibletoimprovecooperation.
Exanna
0
AssumptionsandLimitations Wemakesimplifyingassumptions.
0 5 10 15 20 25 30
First,agentscanidentifyotheragents’types,whichindicatetheirval-
Timein1,000steps
ues.Welimitoursimulationtotwovalues(healthandfreedom)to
Figure3:Comparingthesocialexperience(M )invarioussoci-
Social demonstrate how value importances shape behaviors. A real-world
eties.Exanna agentsocietyhasbetterexperience(Glass’∆ >0.8;
scenariomayincludemoreintertwinedvalues.Investigatingtheim-
p<0.001)thanotherbaselines.
pact of these values on decision making and exploring approaches
toelicitvalueimportancesnecessitatesfurtherresearch.Inaddition,
H Privacy Exannaagentsbetterretaintheirprivacy(p<0.001;∆> ournumericalrepresentationcapturestheimportanceofeachvalue,
0.8,indicatingalargeeffect)comparedtoShare-AllorShare-Rules butwedonottargetthecomplexcontextualfeaturesinthiswork.We
agents.Thus,werejectthenullhypothesiscorrespondingtoH Privacy. focusonprovidinginsightsandmethodologiestounderstandandas-
AlthoughboththeShare-RulesandExannasocietiessharelearned sesscomplexsituationsandgenerateinformedrationales.Wecreate
rulesasrationales,eachExanna agentalignsrationalestoitsvalues andmodelsimplifiedabstractionsofintricatebehaviors,enablingthe
andthoseoftheobservers,andlimitstheprivateinformationshared analysisofcomplexsituations.
tovaluesthatagentsappraise.Ourresultsshowthatarationalestat- Bymodelingthecontextasattribute-bindingpairs,ourapproach
ingcausalattributionwithminimumprivateinformationwhilealign- isadaptabletovarioussettings.However,ourapproachcouldsuffer
ingwithindividuals’valuesissufficienttoexplainbehaviors. fromstatespaceexplosionlikeotherrule-basedlearningapproaches.
H Flexibility Figure4comparesM FlexibilityforShare-All,Share-Rules, Despiteconsideringsingleactionsforsimplicity,wefocusonstudy-
andExanna agentsocieties.WefindthatExanna offershigherflex- ingtheeffectsofvalue-basedexplanations.
ibility(p< 0.01;∆ > 0.8)thantheShare-Rulessociety.Although Ourworkcontributestodevelopingvalue-aligned,trustworthyAI
themeanflexibilityintheShare-AllsocietyislowerthanintheEx- byshowinghowvalue-drivenrationaleshelpresolveconflictsaris-
anna society, this difference is not significant (p > 0.05). Our re- ing from norm deviation and thus affect the emergence of norms.
sultsdemonstratethatanagentcanachieveabettersocialexperience Ourworkcomplementsapproachesfocusedonelicitingvaluesfrom
withoutstickingtoonlyitsgoal. stakeholders[17,18]andfiguringoutwhatareadequatereasonsfor
normdeviation[26]andforachievingtrustworthyAI[25].
1 Weassumetheimportanceofcontextualfactorsaccordingtotheir
Share-all
associated values. For instance, someone prioritizing health may
Share-rules
presentfactorscorrelatingtohealthbenefitsordrawbacks.Although
Exanna
0.5 our scenario is simple, the rationales are dynamically constructed
basedonrulelearning.
FutureDirections First,incorporatingthecostofprivacylossis
0
crucial. For instance, sharing sensitive information and sharing in-
0 5 10 15 20 25 30
terests with chatbots impose different costs. Thus, modeling such
Timein1,000steps
costsisessentialtocapturehowagentsdecide.Second,empowering
Figure4:Comparingflexibility(M )invariousagentsocieties.
Flexibility agentstomakeinformedchoicesregardingdisclosure.Forinstance,
TheExanna societyshowshigherflexibilitythanbaselinesocieties
whenanagentactingonbehalfofastakeholderengageswithagents
(Glass’∆>0.8;p<0.05forShare-Rulesbutp>0.05forShare-
otherthanthehealthcareproviderandthestakeholder,restrictingthe
All).
sharing of sensitive information may be desirable. Third, integrat-
ingrationalesintothedecision-makingprocess,notjustusingthem
Emerged Norm A norm emerges when the proportion of agents assupplementaryinformation[29].Havingrationalesaspartofthe
adheringtoaparticularbehaviorsurpassesathreshold.Weconsider decisions may increase the flexibility of an agent. Fourth, build an
90% as the threshold [9]. We observe that Exanna promotes more ontology to associate information with values, which we model as
generalnormsthantheShare-AllandShare-RulesSocieties.Forin- factors.Anontologyhelpstomodelvariedfactorsorconceptsand
stance,thefollowingnormsemergedonlyinExanna.1
their intertwined relationships. While Exanna enables value-driven
{preference = Wear, InteractWith = rationalesandfocusesonthedecisionsofasingleagent,onefuture
Colleague,¬location=OFFICE} => Wear directionistopromotevaluesandnormsinamultiagentsystem[24].
Reproducibility The codebase for our simulation is publicly
{OberverAgentType = FREEDOM, InteractWith =
available [27]. The appendices provide additional details, includ-
Colleague, location=HOSPITAL} => Wear
inghyperparametersforreproducibility,thecompletesetofemerged
norms,andsupplementaryevaluations.
1AdditionalemergednormsarelistedinAppendixC.
ecneirepxElaicoS
ytilibixelFAcknowledgements MultiAgentSystems(AAMAS),pages799–808,London,2021.IFAA-
MAS.doi:10.5555/3463952.3464048.
[18] E. Liscio, R. Lera-Leri, F. Bistaffa, R. I. Dobbe, C. M. Jonker,
Wethanktheanonymousreviewersfortheirhelpfulcomments.ST
M.Lopez-Sanchez,J.A.Rodriguez-Aguilar,andP.K.Murukannaiah.
andMPSthanktheNSF(grantIIS-2116751)forpartialsupportfor Value inference in sociotechnical systems. Proc. 22nd International
thisresearch.NAacknowledgespartialsupportfromtheUKRIEP- ConferenceonAutonomousAgentsandMultiagentSystems(AAMAS),
pages1774–1780,London,2023.IFAAMAS. doi:10.5555/3545946.
SRCgrantEP/Y028392/1:AIforCollectiveIntelligence(AI4CI).
3598838.
[19] S.Luke,C.Cioffi-Revilla,L.Panait,K.Sullivan,andG.Balan. MA-
SON:Amultiagentsimulationenvironment. Simulation:Transactions
References oftheSocietyforModelingandSimulationInternational,81(7):517–
527,July2005.
[1] R.Agrawal,N.Ajmeri,andM.P.Singh. Sociallyintelligentgenetic [20] F.MoscaandJ.M.Such.ELVIRA:Anexplainableagentforvalueand
agentsfortheemergenceofexplicitnorms. Proc.31stInternational utility-drivenmultiuserprivacy.Proc.20thInternationalConferenceon
Joint Conference on Artificial Intelligence (IJCAI), pages 10–14, Vi- AutonomousAgentsandMultiagentSystems(AAMAS),pages916–924,
enna,July2022.IJCAI.doi:10.24963/ijcai.2022/2. London,May2021.IFAAMAS.doi:10.5555/3463952.3464061.
[2] N.Ajmeri,H.Guo,P.K.Murukannaiah,andM.P.Singh.Robustnorm [21] G.OgunniyeandN.Kökciyan.Contextualintegrityforargumentation-
emergencebyrevealingandreasoningaboutcontext:Sociallyintelli- basedprivacyreasoning. Proc.22ndInternationalConferenceonAu-
gentagentsforenhancingprivacy. Proc.27thInternationalJointCon- tonomousAgentsandMultiagentSystems(AAMAS),pages2253–2261,
ferenceonArtificialIntelligence(IJCAI),pages28–34,Stockholm,July London,2023.IFAAMAS.doi:10.5555/3545946.3598903.
2018.IJCAI.doi:10.24963/ijcai.2018/4. [22] N.OsmanandM.d’Inverno.Acomputationalframeworkofhumanval-
[3] N. Ajmeri, H. Guo, P. K. Murukannaiah, and M. P. Singh. Elessar: ues.Proc.23rdConferenceonAutonomousAgentsandMultiAgentSys-
Ethicsinnorm-awareagents. Proc.19thInternationalConferenceon tems(AAMAS),pages1531–1539,Auckland,May2024.IFAAMAS.
AutonomousAgentsandMultiagentSystems,(AAMAS),pages16–24, doi:10.5555/3635637.3663013.
Auckland,May2020.IFAAMAS.doi:10.5555/3398761.3398769. [23] S.H.Schwartz. AnoverviewoftheSchwartztheoryofbasicvalues.
[4] G.Aycı,A.Özgür,M.S¸ensoy,andP.Yolum. Canweexplainprivacy? OnlineReadingsinPsychologyandCulture,2(1),2012. doi:10.9707/
IEEEInternetComputing,27(4):75–80,2023.doi:10.1109/MIC.2023. 2307-0919.1116.
3270768. [24] M.Serramia,M.Rodriguez-Soto,M.Lopez-Sanchez,J.A.Rodriguez-
[5] M. V. Butz and S. W. Wilson. An algorithmic description of XCS. Aguilar,F.Bistaffa,P.Boddington,M.Wooldridge,andC.Ansotegui.
Proc.3rdInternationalWorkshoponLearningClassifierSystems,vol- Encodingethicstocomputevalue-alignednorms.MindsandMachines,
ume1996ofLNCS,pages253–272,Paris,France,2000.Springer.doi: pages1–30,2023.doi:10.1007/s11023-023-09649-7.
10.1007/3-540-44640-0_15. [25] A.M.SinghandM.P.Singh. Wasabi:Aconceptualmodelfortrust-
[6] J. Cohen. Statistical Power Analysis for the Behavioral Sciences. worthyartificialintelligence.IEEEComputer,56(2):20–28,Feb.2023.
Lawrence Erlbaum Associates, Hillsdale, New Jersey, 2nd edition, doi:10.1109/MC.2022.3212022.
1988.doi:10.4324/9780203771587. [26] A.M.SinghandM.P.Singh. Normdeviationinmultiagentsystems:
[7] V.Contreras,M.Schumacher,andD.Calvaresi.Integrationoflocaland Afoundationforresponsibleautonomy.Proc.32ndInternationalJoint
globalfeaturesexplanationwithglobalrulesextractionandgeneration ConferenceonArtificialIntelligence(IJCAI),pages289–297,Macau,
tools.InExplainableandTransparentAIandMulti-AgentSystems(EX- Aug.2023.IJCAI.doi:10.24963/ijcai.2023/33.
TRAAMAS),volume13283ofLNCS,pages19–37,VirtualConference, [27] S.-T.Tzeng. Codefor"Value-BasedRationalesImproveSocialExpe-
2022.Springer.doi:10.1007/978-3-031-15565-9_2. rience:AMultiagentSimulationStudy",2024.URLhttps://doi.org/10.
[8] S.Cranefield,M.Winikoff,V.Dignum,andF.Dignum. Nopizzafor 5281/zenodo.12801893.
you: Value-based plan selection in BDI agents. Proc. 26th Interna- [28] S.-T.Tzeng,N.Ajmeri,andM.P.Singh. Fleur:Socialvaluesorienta-
tionalJointConferenceonArtificialIntelligence(IJCAI),pages178– tion for robust norm emergence. Proc. 15th International Workshop
184,Melbourne,2017.IJCAI.doi:10.24963/ijcai.2017/26. on Coordination, Organizations, Institutions, Norms, and Ethics for
[9] J. Delgado. Emergence of social conventions in complex net- GovernanceofMulti-AgentSystems(COINE),volume13549ofLec-
works. ArtificialIntelligence,141(1–2):171–185,2002. doi:10.1016/ tureNotesinComputerScience,pages185–200,Auckland,May2022.
S0004-3702(02)00262-X. Springer.doi:10.1007/978-3-031-20845-4_12.
[10] D.DiScalaandP.Yolum. Paccart:Reinforcingtrustinmultiuserpri- [29] S.-T.Tzeng,N.Ajmeri,andM.P.Singh. Normenforcementwitha
vacyagreementsystems. Proc.22ndInternationalConferenceonAu- softtouch:Fasteremergence,happieragents. Proc.23rdInternational
tonomousAgentsandMultiagentSystems(AAMAS),pages2787–2789, ConferenceonAutonomousAgentsandMultiAgentSystems(AAMAS),
London,2023.IFAAMAS.doi:10.5555/3545946.3599078. pages1837–1846,2024.doi:10.5555/3635637.3663046.
[11] A. Georgara, J. A. Rodriguez Aguilar, and C. Sierra. Building con- [30] R.J.UrbanowiczandW.N.Browne. IntroductiontoLearningClas-
trastive explanations for multi-agent team formation. Proc. 21st In- sifierSystems. SpringerBriefsinIntelligentSystems.Springer,New
ternational Conference on Autonomous Agents and Multiagent Sys- York,2017.doi:10.1007/978-3-662-55007-6.
tems (AAMAS), pages 516–524, Auckland, 2022. IFAAMAS. doi: [31] G.H.VonWright. NormandAction:ALogicalEnquiry. International
10.5555/3535850.3535909. LibraryofPhilosophyandScientificMethod.HumanitiesPress,New
[12] G. V. Glass. Primary, secondary, and meta-analysis of re- York,1963.
search. Educational Researcher, 5(10):3–8, 1976. doi: 10.3102/ [32] E.Wang,P.Khosravi,andG.V.d.Broeck. Probabilisticsufficientex-
0013189X005010003. planations. Proc.30thInternationalJointConferenceonArtificialIn-
[13] R.J.GrissomandJ.J.Kim. EffectSizesforResearch:Univariateand telligence(IJCAI),pages3082–3088,Montreal,Aug.2021.IJCAI.doi:
Multivariate Applications. Routledge, Abingdon-on-Thames, 2012. 10.24963/ijcai.2021/424.
doi:10.4324/9780203803233. [33] M.Winikoff,G.Sidorenko,V.Dignum,andF.Dignum. Whybadcof-
[14] M.Hind,D.Wei,M.Campbell,N.C.Codella,A.Dhurandhar,A.Mo- fee?ExplainingBDIagentbehaviourwithvaluings. ArtificialIntelli-
jsilovic´,K.NatesanRamamurthy,andK.R.Varshney. TED:Teach- gence,300:103554,2021.doi:10.1016/j.artint.2021.103554.
ingAItoexplainitsdecisions. Proc.2019AAAI/ACMConferenceon [34] J. Woodgate and N. Ajmeri. Macro ethics for governing equitable
AI,Ethics,andSociety,pages123–129,Honolulu,2019.ACM. doi: sociotechnical systems. Proc. 21st International Conference on Au-
10.1145/3306618.3314273. tonomousAgentsandMultiagentSystems(AAMAS),pages1824–1828,
[15] P.Langley. Explainable,normative,andjustifiedagency. Proc.33rd Online,May2022.IFAAMAS. doi:10.5555/3535850.3536118. Blue
AAAIConferenceonArtificialIntelligence(AAAI),pages9775–9779, SkyIdeasTrack.
Honolulu,2019.AAAIPress.doi:10.1609/aaai.v33i01.33019775. [35] J.WoodgateandN.Ajmeri.MacroethicsprinciplesforresponsibleAI
[16] R. Lera-Leri, F. Bistaffa, M. Serramia, M. Lopez-Sanchez, and systems:Taxonomyanddirections. ACMComputingSurveys,56(11):
J.Rodriguez-Aguilar.Towardspluralisticvaluealignment:Aggregating 1–37,July2024.doi:10.1145/3672394.
valuesystemsthroughℓp-regression. Proc.21stInternationalConfer- [36] V.Yazdanpanah,E.H.Gerding,S.Stein,M.Dastani,C.M.Jonker,and
enceonAutonomousAgentsandMultiagentSystems(AAMAS),pages T.J.Norman. Reasoningaboutresponsibilityinautonomoussystems:
780–788,Auckland,2022.IFAAMAS.doi:10.5555/3535850.3535938. Challenges and opportunities. AI & Society: Journal of Knowledge,
[17] E.Liscio,M.vanderMeer,L.C.Siebert,C.M.Jonker,N.Mouter,and Culture and Communication, 38(4):1453–1464, Aug. 2023. doi: 10.
P.K.Murukannaiah.Axies:Identifyingandevaluatingcontext-specific 1007/s00146-022-01607-8.
values.Proc.20thInternationalConferenceonAutonomousAgentsandA Appendix:ProceduresofXCS Subsumption: A process that replaces offspring rules with more
•
generalparentrulesifitexists.Otherwise,savetheoffspringrules.
TheoverallprocessofXCSincludesthefollowingsub-processes.
Specifically,amoregeneralruleyieldsaminorpredictionerror.
Matching: A process that matches the current context and all Forinstance,ifrule{Risk=Low} Wearhaslessprediction
• rules/classifierstogenerateamatchset.Forinstance,inourrun- errorthanrule{Risk=Low,Relati⇒ ons¬ hip=Friend} Wear,
⇒ ¬
ning example, the match set for Bella may include (1) {Risk = theformerrulewouldreplacethelaterruleandincreasesthenu-
Low} Wear[fitness=0.3],(2){Risk=Low} Wear[fitness merosity.
= 0.7],⇒ (3) {OtherAgentType = Health} We⇒ ar [¬ fitness = 0.8], Deletion:Eachactionsethasthesamemaximumnumberofrules.
and(4){OtherAgentType=Health} W⇒ ear[fitness=0.2].The • XCSremovesthelow-fitnessrules.
⇒¬
fitnessisbasedontheaccuracyofeachrule’srewardprediction.
Covering: A process that guarantees diversity via adding a ran- B Appendix:HyperparametersforReproducibility
•
dom classifier whose conditions match the current context. For
instance,adding{Risk=Low,Relationship=Friend} Wear Table9liststhehyperparameterswesetforoursimulations.Weopt
⇒ ¬
totheruleset. forthedefaultconfigurationsasintheliterature[30]sinceourem-
Action selection: XCS selects actions with pure exploration or phasis is on studying the impact of values in rationales and not in
•
pure exploitation with ϵ greedy. If not in exploration mode, this fine-tuning the learning process. Learning rate refers to reinforce-
processreturnstheactionwiththehighestfitness-weightedaggre- ment learning and determines the proportion an agent learns from
gationofreward. recent experiences. Adjusting the don’t care probability alters the
probabilityofincludingtheperceivedfactorsinthegeneratedrule.
(cid:88)rule Increasing the accuracy threshold reduces the number of rules to
fitnessa = fitnessi ×numerosity i×predicted_reward
i
(2)
maintain.Fitnessexponentandfitnessfalloffdeterminetheaccuracy
i oftherules.Geneticalgorithmthreshold controlstheprobabilityof
wherea AandAistheactionspace.Rulesrepresentallrules ruleexploration.Mutationprobability andcrossoverprobability de-
∈
appliedtothecontextandforactiona.Withtheaboveexample termine how often the mutation and crossover happen. Experience
andformula,theagentwouldchoosenottowearamaskdueto thresholds fordeletion andsubsumption guaranteesaspecificnum-
fitness¬Wear>fitness wear. ber of times a rule has to be applied before being deleted or sub-
Formationofactionset:Itincludesallclassifiersthatproposethe sumed.
•
chosenactionbasedonthematchset.Forinstance,{Risk=Low} Thecodebaseforoursimulationispubliclyavailable[27].
Wear,{OtherAgentType=Health} Wear,and{Risk=
⇒ ¬ ⇒ ¬
Low,Relationship=Friend} Wear.
Updatingclassifierparameter⇒ s[3¬ 0]:Anagentupdatestherulepa- Table9:Hyperparametersforoursettings.
•
rameters(e.g.,accuracyandfitness)basedonthereceivedpayoff. Parameter Value
Thefollowingequationupdatesthepredictedreward,wherepis
Populationsize 200.00
thepredictedreward,β isthelearningrate,andristhereceived Learningrate 0.10
reward. Don’tcareprobability 0.30
p p+β(r p) (3) Accuracythreshold 0.01
← − Fitnessexponent 5.00
Thepredictionerrorεisupdatedwiththefollowingequation. Geneticalgorithmthreshold 25.00
Mutationprobability 0.40
ε ε+β(r p ε) (4) Crossoverprobability 0.80
← | − |− Experiencethresholdfordeletion 20.00
The fitness of a rule is based on its accuracy, which is in- Experiencethresholdforsubsumption 20.00
Fitnessfalloff 0.10
verselyproportionaltothepredictionerror.Weupdatetheaccu-
racykappawiththefollowingformula.
(cid:40)
κ=
1 ifε<ε0
(5) C Appendix:DetailedResults
α( ε )−ν otherwise,
ε0
Table10summarizesthestatisticalanalysisofoursimulations,in-
whereαisthescalingfactorthatraisesanon-accurateruletobe cludingadditionalresultsforactorpayoffsandobserverpayoffsand
closetoanaccuraterule.ε0isthethresholdofpredictionerrorbe- flexibility across different agent types. Actor payoff and observer
low which the prediction error of a rule is assumed to be zero. payoffcomprisesocialexperiences.
ν defines how accuracy is related to prediction error and aims Theseresultsdemonstratethatfirst,withthesamecontext,agents
tohelpdifferentiatesimilarclassifiers.Forfitnesscalculation,we withdifferentvalueimportancescanstillevolvetodifferentbehav-
nextcalculatetherelativeaccuracyκ′ofeachrule.
iors. Second, more adaptive norms can emerge among agents with
κ differentvalues.
κ′ = (cid:80) (6)
cl∈[A]κcl H SocialExperience Social experience includes actor payoff and ob-
server payoff. Figures 5 and 6 plot the payoffs of the actors who
where[A]representsthecorrespondingactionset.Finally,thefit-
select actions, explain their behaviors, and receive feedback from
nessupdateofaruleisasfollows.
observersintheShare-All,Share-Rules,andExannaagentsocieties.
F F +β(κ′ F) (7) Figures 7 and 8 compare the payoff from the observer who reacts
← − to the actor’s behavior in the Share-All, Share-Rules, and Exanna
whereFisthefitnessofarule. agent societies. The freedom-loving agents within Exanna societyceivedbyhealth-freakagents.
Table10:Results:Comparingmean(X¯)andstandarddeviation(σ)
socialexperience,resolution,privacy,andflexibilityinvarioussoci-
0.6
etiesandagenttypes.pisp-valuefromt-test.M hastwosub-
Social
classes,actorpayoffandobserverpayoff. 0.4
Share-All Share-Rules Exanna Share-all
0.2
Share-rules
X¯ 0.58 0.58 0.60
Exanna
σ 0.02 0.01 0.02 0
p <0.001 <0.001 – 0 5 10 15 20 25 30
∆ 1.80 3.11 – Timein1,000steps
X¯ 0.59 0.62 0.70 Figure5:Comparingtheactorpayoffforhealth-freakagentsinvari-
σ 0.06 0.02 0.05 ousagentsocieties.Actorsareagentswhoactandreceivefeedback
p <0.001 <0.001 – from others. Health-freak agents in each societyhave similar actor
∆ 1.80 3.11 – payoffs.
X¯ 0.63 0.63 0.63
σ 0.03 0.03 0.03
p <0.001 <0.001 – 1
∆ 10.40 9.62 –
0.5
Share-all
X¯ 0.96 0.96 0.94 Share-rules
σ 0.03 0.03 0.03 Exanna
0
p <0.001 <0.001 –
∆ 0.60 0.64 – 0 5 10 15 20 25 30
Timein1,000steps
Figure6: Comparing the actor payoff for freedom-loving agents in
X¯ −0.05 −0.07 0.02 variousagentsocieties.Actorsareagentswhoactandreceivefeed-
σ 0.09 0.09 0.09 backfromothers.Thefreedom-lovingagentsinExannasocietyhave
p <0.001 <0.001 – loweractorpayoffs(Glass’∆ >0.5;p < 0.001)thanthebaseline
∆ 2.71 2.39 –
societies.
X¯ −0.38 −0.36 −0.28
σ 0.09 0.09 0.10
p <0.001 <0.001 –
0
∆ 1.10 0.81 –
Share-all
Share-rules
0.1
X¯ 0.00 1.34e−5 0.25 − Exanna
σ 0.00 2.11e−5 2.90e−3 0 5 10 15 20 25 30
p <0.001 <0.001 – Timein1,000steps
∆ 11896.52 –
∞ Figure7: Comparing the observer payoff for health-freak agents in
X¯ 0.10 0.09 0.12 varioussocieties.Observersgivefeedbackbasedonobservedbehav-
σ 0.01 0.01 0.03 iorsandreceivedrationales.Thehealth-freakagentsinExannasoci-
p 0.08 <0.01 – etyhavebetterobserverpayoffs(Glass’∆ >0.8;p < 0.001)than
∆ 1.59 2.89 – thebaselinesocieties.
X¯ 0.14 0.12 0.13
σ 0.06 0.07 0.06
p <0.001 <0.001 – 0 Share-all
∆ 0.85 0.44 – Share-rules
Exanna
0.2
−
X¯ 0.06 0.06 0.09
σ 0.03 0.03 0.03 0.4
p <0.001 <0.001 – −
0 5 10 15 20 25 30
∆ 0.90 0.87 –
Timein1,000steps
Figure8:Comparingtheobserverpayoffforfreedom-lovingagents
invarioussocieties.Observersgivefeedbackbasedonobservedbe-
encountermoreadversefeedbackthanothersocietiesinitially.How- haviors and received rationales. The freedom-loving agents in Ex-
ever,someofthemquicklyadaptandbegintodivertfromtheirorig- anna society have better observer payoffs (Glass’ ∆ > 0.8; p <
inal goals. As a result of the behavioral change made by freedom- 0.001)thanthebaselinesocieties.
loving agents, there has been an enhancement in the feedback re-
noituloseRM
laicoSM
ffoyaP
rotcA
ffoyaP
rotcA
-yaP
revresbO
-yaP
revresbO
ycavirPM
ytilibixelFM
rofytilibixelF
rofytilibixelF
rof
rof
rofffo
rofffo
kaerf-htlaeh
gnivol-modeerf
kaerf-htlaeh
gnivol-modeerf
kaerf-htlaeh
gnivol-modeerf
sffoyaProtcA
sffoyaProtcA
sffoyaPrevresbO
sffoyaPrevresbOH Flexibility Wecompareagents’flexibilityofgoalsasthemetricof
65
evaluatingH .Figures9and10compareM forhealth-
Flexibility Flexibility
freakandfreedom-lovingagentsintheShare-All,Share-Rules,and
Exanna agent societies. Referring to Figure 8, the freedom-loving
60
agents compromise on goals, thereby enhancing flexibility and en-
richingsocialexperience.
55
1
Share-all Share-all Share-rules Exanna
Share-rules Society
Exanna
Figure11:Comparingtheresolution(M )invariousagentso-
0.5 Resolution
cietieswithvalueimportancesasinTable12.TheExanna agentso-
cietyhasbetterresolutionthanthebaselinesocieties.
0
0 5 10 15 20 25 30 indicating a large effect) than other societies. However, the mixed
valuesleadtoaworsesocialexperience.Withmixedvalues,anac-
Timein1,000steps
tionnowincludesdifferentconcerns,whichsometimesmaycontra-
Figure9:Comparingtheflexibilityforhealth-freakagentsinvarious
dicteachother.Forexample,anagentmayappreciatefreedomand
agent societies. The health-freak agents in Exanna society exhibit
prefernottowearamask,butinthemeantime,healthconcernsde-
higher flexibility (Glass’ ∆ > 0.8; p < 0.001) compared to the
creaseitspayoffonthatdecision.
baselinesocieties.
0.6
1
Share-all
0.4
Share-rules
Share-all
Exanna 0.2
0.5 Share-rules
Exanna
0
0 5 10 15 20 25 30
0
Timein1,000steps
0 5 10 15 20 25 30
Timein1,000steps Figure12:Comparingthesocialexperience(M Social)invarioussoci-
etieswithvalueimportancesasinTable12.Exannaagentsocietyhas
Figure10:Comparingtheflexibilityforfreedom-lovingagentsinvar-
betterexperience(Glass’∆>0.8;p<0.001)thanotherbaselines.
ious agent societies. The freedom-loving agents in Exanna society
hashigherflexibility(Glass’∆>0.8;p<0.001)thanthebaseline
societies. H Privacy Exannaagentsbetterretaintheirprivacy(p<0.001;∆>
0.8,indicatingalargeeffect)comparedtoShare-AllorShare-Rules
agents.
CompleteSetofEmergedNorms H Flexibility Figure 13 compares M Flexibility for Share-All, Share-
Rules,andExanna agentsocieties.Themixedvaluesleadtohigher
Table11liststhenormsthatemergeinthesimulations.Anemerged
flexibility in agent societies than in the main simulations. Specifi-
normisaruleadoptedbymorethan90%ofagentsinonesociety.
cally,themixedvaluesnarrowthenumericalgapbetweeneachac-
tion,decreasingthethresholdforagentstochangetheirminds.
D Appendix:AdditionalExperimentswithVarying
ValueImportances 1 Share-all
0.8 Share-rules
Agentsmaynotalwaysplaceextremeimportancetoonevalueover
Exanna
other.Forinstance,anagentcanappreciatefreedom(0.3)butcares 0.6
moreabouthealth(0.7).Toinvestigatefurther,weconductadditional 0.4
experiments with a different set of value importances for freedom-
0.2
lovingandhealth-freakagents.Table12liststhevalueimportances
the freedom-loving and health-freak agents place in the additional 0 5 10 15 20 25 30
scenario.Weruneachsimulationfivetimes,withothersettingsiden- Timein1,000steps
ticaltotheoriginalsimulations. Figure13:Comparingflexibility(M Flexibility)invariousagentsocieties
Table13showsthedetailedresultsforagentsocietieswithvalue with value importances as in Table 12. The Exanna society shows
importancesasinTable12. higherflexibilitythanbaselinesocieties(Glass’∆ >0.8;p < 0.05
forShare-Rulessocietybutp>0.05forShare-Allsociety).
H Resolution Figure11comparesconflictresolutioninvarioussoci-
eties.Exannaoffersbetterconflictresolutionthanothersocieties.
H SocialExperience Figure12comparesthesocialexperienceforShare-
All, Share-Rules, and Exanna agent societies. A social experience
includespersonalpayofffromitsactionandfeedbackfromitsinter-
action.Exanna yieldsbettersocialexperience(p< 0.01;∆ > 0.8,
ytilibixelF
ytilibixelF
%noituloseRtciflnoC
ecneirepxElaicoS
ytilibixelFTable11:Emergednormsinagentsocieties.Commonmeansthenormsemergeineachagentsociety.
Norm
Society
Premise Consequence
Risk = NONE;
preference = ¬WEAR;
Common OberverAgentType = HEALTH; WEAR
InteractWith = COLLEAGUE;
location = OFFICE
Risk = NONE;
preference = ¬WEAR;
OberverAgentType = HEALTH; WEAR
InteractWith = COLLEAGUE;
location = HOSPITAL
Risk = RISK;
preference = ¬WEAR;
OberverAgentType = HEALTH; WEAR
InteractWith = COLLEAGUE;
location = OFFICE
Risk = RISK;
preference = ¬WEAR;
OberverAgentType = HEALTH; WEAR
InteractWith = COLLEAGUE;
location = HOSPITAL
Risk = NONE;
OberverAgentType = HEALTH;
Share-All WEAR
InteractWith = COLLEAGUE;
location = OFFICE
preference = ¬WEAR;
OberverAgentType = HEALTH;
Share-Rules WEAR
InteractWith = COLLEAGUE;
location = OFFICE
preference = ¬WEAR;
Exanna InteractWith = COLLEAGUE; WEAR
location = OFFICE
preference = ¬WEAR;
InteractWith = COLLEAGUE; WEAR
location = HOSPITAL
preference = ¬WEAR;
OberverAgentType = HEALTH;
WEAR
InteractWith = COLLEAGUE;
location = OFFICE
preference = ¬WEAR;
OberverAgentType = HEALTH;
WEAR
InteractWith = COLLEAGUE;
location = HOSPITAL
OberverAgentType = HEALTH;
InteractWith = COLLEAGUE; WEAR
location = OFFICE
OberverAgentType = HEALTH;
InteractWith = COLLEAGUE; WEAR
location = HOSPITAL
OberverAgentType = FREEDOM;
InteractWith = COLLEAGUE; WEAR
location = HOSPITAL
Risk = RISK;
OberverAgentType = HEALTH;
WEAR
InteractWith = COLLEAGUE;
location = OFFICE
Risk = NONE;
OberverAgentType = HEALTH;
WEAR
InteractWith = COLLEAGUE;
location = OFFICETable12:Valueimportancesofagentsintheadditionalscenario.Each
societyhashalfFreedom-lovingandhalfHealth-freakagents.
Agents:Values Freedom Health
Freedom-loving 0.7 0.3
Health-freak 0.3 0.7
Table13:Results:Comparingmean(X¯)andstandarddeviation(σ)
ofsocialexperience,resolution,privacy,andflexibilityinvariousso-
cietiesintheadditionalscenario.pisp-valuefromt-test.
Share-All Share-Rules Exanna
X¯ 60.79 59.50 63.23
σ 2.43 2.20 1.70
p 0.1 <0.05 –
∆ 1.00 1.69 –
X¯ 0.53 0.52 0.65
σ 0.06 0.05 0.03
p <0.01 <0.01 –
∆ 2.24 2.57 –
X¯ 0.00 5.78e−5 0.25
σ 0.00 1.04e−4 2.60e−3
p <0.001 <0.001 –
∆ ∞ 2414.62 –
X¯ 0.22 0.20 0.25
σ 0.03 0.02 0.02
p <0.05 <0.01 –
∆ 1.29 2.02 –
noituloseRM
laicoSM
ycavirPM
ytilibixelFM