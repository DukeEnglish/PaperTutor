Predictive Performance Test based on the
Exhaustive Nested Cross-Validation for
High-dimensional data
Iris Ivy Gauran1, Hernando Ombao1, Zhaoxia Yu2
1 Statistics Program, King Abdullah University of Science and Technology (KAUST)
2 Department of Statistics, University of California, Irvine
Abstract
Itiscrucialtoassessthepredictiveperformanceofamodelinordertoestablishits
practicality and relevance in real-world scenarios, particularly for high-dimensional
data analysis. Among data splitting or resampling methods, cross-validation (CV) is
extensively used for several tasks such as estimating the prediction error, tuning the
regularization parameter, and selecting the most suitable predictive model among
competing alternatives. The K-fold cross-validation is a popular CV method but
its limitation is that the risk estimates are highly dependent on the partitioning of
the data (for training and testing). Here, the issues regarding the reproducibility
of the K-fold CV estimator is demonstrated in hypothesis testing wherein different
partitions lead to notably disparate conclusions. This study presents an alternative
novel predictive performance test and valid confidence intervals based on exhaus-
tive nested cross-validation for determining the difference in prediction error between
two model-fitting algorithms. A naive implementation of the exhaustive nested cross
validation is computationally costly. Here, we address concerns regarding computa-
tional complexity by devising a computationally tractable closed-form expression for
the proposed cross-validation estimator using ridge regularization. Our study also
investigates strategies aimed at enhancing statistical power within high-dimensional
scenarios while controlling the Type I error rate. To illustrate the practical util-
ity of our method, we apply it to an RNA sequencing study and demonstrate its
effectiveness in the context of biological data analysis.
Keywords: Exhaustive Cross-validation, Nested Cross-validation, Predictive Performance Test,
Ridge Regression
1 Introduction
Inhigh-throughput‘omicsstudies,includinggenomics,metabolomics,transcriptomics,proteomics,
and epigenomics, scientists typically generate massive datasets. These data can contain informa-
tion on millions of single nucleotide polymorphisms (SNPs) in the case of genomics studies, or
differentially methylated probes in epigenomics researches, among other types of data. Analo-
gously, in neuroimaging studies, data contains hundred of thousands of voxels in a functional
magnetic resonance image (fMRI) and dozens to hundreds of electroencephalogram (EEG) chan-
nels. The large-scale nature of these data allows researchers to comprehensively investigate bi-
1
4202
guA
6
]EM.tats[
1v83130.8042:viXraological systems, enabling the identification of underlying patterns, associations, and potential
biomarkers related to various complex physiological or pathological conditions. Statistically, this
highlights that both explanatory and response variables can be high-dimensional, presenting both
computational and analytical challenges.
Drivenbythisproblemof understanding whetherthechanges indisease-relatedbiological metrics
can be predicted by information from genes or possible biomarkers, the primary objective of this
study is to investigate how models fitted using the training data generalize to unseen data points
in the testing data. Among data splitting or resampling methods, cross-validation (CV) is an
algorithmic technique extensively used to evaluate the predictive performance of a learning model
(Geisser, 1975; Stone, 1974, 1977, 1978). In Stone (1974), the cross-validation criterion is most
popularly used for the selection and assessment of statistical prediction. One of the most popular
CV methods is K-fold cross-validation because it is conceptually simple to implement. However,
Lund (2013) mentioned that cross-validated LASSO has shown marked instability and varying
results when the test-train partition involve test sets with at least one sampling unit. In light
of this, we investigate the reproducibility of K-fold CV in high-dimensional testing and highlight
that the stability of the regularization depends on the properties of the data set.
Recently, cross-validation has been widely recognized as a reliable method for estimating the test
error of a prediction rule or calculating the generalization error (Anguita et al., 2012; Bayle et al.,
2020; Markatou et al., 2005; Nadeau and Bengio, 1999; Weiss and Kulikowski, 1991). However,
the common assumption that cross-validation estimates the prediction error for the model fitted
using the training data does not hold for the linear model fit by ordinary least squares (Bates
et al., 2023). Instead, they mentioned that cross-validation estimates the average prediction error
of models fit on other unseen training sets drawn from the same population. In addition, Bengio
and Grandvalet (2003) stated that a universal unbiased estimator of the standard error of the K-
foldcross-validationpointestimatedoesnotexist. Thisledtoadditionalassumptionsorproposed
modifications in the CV procedure to estimate the standard error (Dietterich, 1998; Markatou
et al., 2005; Nadeau and Bengio, 1999).
Moreover, point estimates of prediction accuracy based on cross-validation can be highly variable,
especially for small sample sizes. Because of this, Jiang et al. (2008) emphasized the necessity
of providing confidence intervals of prediction accuracy. This prompted various researchers to
study the asymptotic behavior of cross-validation (Austern and Zhou, 2020; Bayle et al., 2020;
Dudoit and van der Laan, 2005) and propose confidence intervals for generalization errors (for
examples, see Bates et al. (2023); Bayle et al. (2020); Jiang et al. (2008); Nadeau and Bengio
(1999); Rajanala et al. (2022)).
There is a diverse landscape of scientific disciplines that employ CV methods to understand the
prediction accuracy of a given method on future test points. Yazici et al. (2023) developed a
miRcorrNet tool which performs integrative analysis of microRNA (miRNA) and gene expres-
sion profiles to identify significant miRNA groups and their associated target genes by tracking
group scoring, ranking, and other necessary information through the cross-validation iterations.
Several approaches involving the prediction of protein-protein interactions (Chen et al., 2019; Li
et al., 2021; Wang et al., 2019, 2020) implement cross-validation to evaluate the performance of
the proposed learning-based approach. Similarly, CV methods are used in studies that explore
further into cognitive functions and associated impairments (for example, see Palmqvist et al.
(2019); White et al. (2020)). In particular, Chen et al. (2023) recently proposed a novel diag-
nostic strategy for Autism Spectrum Disorder (ASD) using spatial-temporal self-attention-based
deepneuralnetworks(DNN)wheretheyimplementedten-foldcross-validationintheirsystematic
experimentstoshowimprovedperformanceoverexistingmethodsinASDclassification. Likewise,
clinicians implement CV-based confidence intervals of test errors and other repeated sample split-
tingestimatorsforpredictivecancerprognosisandmortalitypredictiontoimprovereproducibility
and avoid spurious results (Michiels et al., 2005). However, Jiang et al. (2008) emphasized that
2these confidence intervals often do not have guarantees on correctness and could potentially yield
severely misleading findings.
In ‘omics studies, limited sample sizes can arise due to numerous factors including cost, sample
availability, or experimental limitations. With fewer samples, prediction tasks can be particularly
challenging due to the reduced amount of data available for training predictive models. In Zhao
et al. (2021), they challenge the conventional statistical wisdom regarding “double-peeking” in
data analysis. Contrary to the belief that using standard statistical inference tools after peeking
at the data twice (once to fit a LASSO model and again to fit a least squares model on the
LASSO-selected set) is invalid, they theoretically demonstrate that, under specific assumptions,
the two-step approach can provide asymptotically valid inference. This finding aligns with a
similar suggestion made for studies with limited sample sizes by Tsamardinos (2022). The study
proposed practical alternatives in predictive modeling for low-sample datasets, proposing training
the final model on all available data, estimating its performance using proxy models on subsets
of the data, and correcting performance estimates for multiple model iterations.
Despite the challenges posed by restricted sample sizes in ‘omics studies, predictive modeling
remains a valuable tool for gaining insights into biological processes and help identify potential
biomarkers or therapeutic targets. Motivated by this, the overarching goal of this study is to
develop a novel high-dimensional predictive performance test utilizing exhaustive nested cross-
validation. Nested Cross-validation (NCV) has demonstrated excellent predictive performance
in comprehensive evaluations, particularly in datasets with limited samples (Aliferis et al., 2006;
Iizuka et al., 2003; Salzberg, 1997; Tsamardinos et al., 2015). Moreover, we demonstrate that
the non-exhaustive test based on K-fold CV can lead to contradicting conclusions depending on
the test-train partition, thereby highlighting the need for an exhaustive procedure that provides
a reproducible test decision.
Rationale for Exhaustive Cross-validation
There are several categories with which to classify cross-validation procedures (Arlot and Celisse,
2010). First, CV methods can be distinguished based on whether they involve either one or
multiple test-train splits. Hold-out cross-validation is the simplest CV method because the data
is partitioned to only one disjoint training and testing data. This implies that model refitting is
notnecessaryandthecomputationalburdenislowestrelativetootherCVmethods. Ontheother
hand, most CV methods involve repeated partitioning of the data, with each fold being used as
the test set. The primary advantage of testing the model on multiple subsets of the data is that
it provides a more reliable estimate of its ability to generalize to new data. More importantly,
it aids in overcoming overfitting, where a model is too closely tailored to the training data and
performs poorly on test observations.
Furthermore, multiple test-train CV methods can be further categorized according to whether or
not they are exhaustive. As the name suggests, exhaustive CV methods fit and test all possible
ways to split a given sample into a training and testing set. The main method under this category
is the leave-N -out cross-validation where the total number of folds used as the testing data is
0
L = N!/(N !(N−N )!). Leave-one-outcross-validation(LOOCV)isaspecialcaseofthismethod
0 0
where L = N. On the contrary, the classic example of a non-exhaustive CV method is the K-fold
cross-validation. The data is split into into K different folds where the model-fitting algorithm is
trained on K −1 folds and tested on the remaining fold. This process is repeated K times, with
each fold serving as the test set once. Again, LOOCV is a special case of K-fold cross-validation,
where K = N. Another widely used non-exhaustive CV method is stratified cross-validation. In
this method, the data is divided into folds in a specific way in order to ensure that the proportion
of samples in each class is the same in each fold.
Finally, CV methods can be classified as either nested or non-nested. Nested cross-validation
3involves a sequential procedure with typically two main loops, an inner and outer loop. The
inner loop is often used to train the model-fitting method to identify an optimal hyperparameter.
Meanwhile, the outer loop is used for estimating the generalization error of the underlying model
after plugging in the estimate of hyperparameter obtained using the inner loop. Thus, nested CV
methods encapsulate both model selection and model assessment in the inner and outer loops,
respectively (Parvandeh et al., 2020; Varma and Simon, 2006).
To motivate this study, the problem concerning lack of reproducibility of the high-dimensional
test results based on the K-fold cross-validation estimator is presented. The liver toxicity data by
Bushel et al. (2007) is used as a motivating example. This high-dimensional data contains gene
expression measurements of 3116 genes for 64 rats exposed to subtoxic (150 mg/kg) and toxic
(1500, 2000 or 2500 mg/kg) doses of acetaminophen in a controlled experiment. We employ this
data to rationalize the use of exhaustive cross-validation because this data has been used recently
to better understand overfitting (Gauran et al., 2022; Kobak et al., 2020).
In K-fold cross-validation, the randomization of the N indices of all sampling units is performed
onlyonce, whichthenleadstotheassignmentofeachsamplingunittoexactlyonefold. Toensure
reproducibility, theseedusedfortherandomizationprocedureshouldbetakenintoconsideration.
Suppose 500 different seeds indexed by 8891 + s,s ∈ {1,2,...,500} are used to produce 500
differentconfigurationsofsamplingunitassignmentsintoK-folds. Foreachseed, theteststatistic
based on K-fold cross-validation estimator is computed. The resulting test statistics are then
comparedtothecriticalvaluerepresentedbythebluedashedlineinFigure1. Thevisualizationof
this procedure together with the rigorous calculation of the test statistic is presented in Appendix
D.
For the clinical measurement creatinine as response variable, some partitions of the entire data
yield significant test results for both K = 5 and K = 10. In contrast, for the response variable
Alkaline Phosphatase (ALP), some partitions of the data resulted to the non-rejection of the
null hypothesis using both K = 5 and K = 10. These results highlight the weakness of the
current approaches based on the K-fold cross validation estimator because the the overall test
results (of either rejection or non-rejection of the null hypothesis) are not consistent across the
seeds for data partitioning. While in real data analysis there is only one test statistic computed,
the visualization provided in Figure 1 shows problems with reproducibility and the chance of a
flipping the decision across various data partitions. To address this major problem, we derive
a closed-form expression for the exhaustive cross-validation estimator and propose novel a test
statistic based on this estimator. This simultaneously addresses the limitations of K-fold CV in
terms of instability and reproducibility. Moreover, this ensures the computational feasibility of
the exhaustive CV.
The contributions of our work are the following. First, we provide a valuable framework for pre-
dictive performance testing in high-dimensional data analysis. By leveraging exhaustive nested
cross-validation, the proposed method offers enhanced accuracy in evaluating the performance of
predictive models. In turn, our approach addresses key challenges inherent in high-dimensional
datasets, providing researchers and practitioners with a valuable tool for model selection and val-
idation. Second, we address the scalability problem due to the high computational cost required
by test statistics constructed based on the generated samples from the null distribution which is
evident in permutation tests. Instead, we characterize the null distribution of the test statistic
and use this as a benchmark to compare the predictive performance of the model-fitting method
with the hypothesized features. Third, we address the problem of reproducibility of K-fold cross-
validation by utilizing an exhaustive cross-validation method which learns and tests all possible
ways to divide the original sample into a training, validation, and testing set. Fourth, we de-
rive a computationally feasible closed-form expression for the exhaustive nested cross-validation
estimator. This addresses the computational cost of the naive approach which refits the model-
fitting procedure multiple times. To the best of our knowledge, this is the first study utilizing
4cross-validation as a test statistic for high-dimensional testing. Our proposed method is straight-
forward to apply, and applicable to many settings with continuous response. More importantly,
our proposed method can provide valid confidence intervals for the difference in prediction error
among the models with and without the features being tested.
4
0
−10 0
−20
−4
−30
0 100 200 300 400 500 0 100 200 300 400 500
s s
(A) Creatinine (mg/dL), K = 5 (B) Creatinine (mg/dL), K = 10
30
10
20
10 5
0
0
0 100 200 300 400 500 0 100 200 300 400 500
s s
(C) ALP (IU/L), K = 5 (D) ALP (IU/L), K = 10
Figure 1: Illustration of problems regarding reproducibility of the K-fold Cross-Validation
test using the liver toxicity data with 500 test statistics depending on the seed indexed by
8891+s,s ∈ {1,2,...,500}. The blue dashed line corresponds to the critical value, above
which we conclude that the null hypothesis should be rejected. Thus, at α = 0.05, the red
dots correspond to test statistics that are significant while all other test statistics in gray
are not significant.
The remainder of this paper is organized as follows. Section 2 provides a review of the current
landscape of research involving prediction error, ridge predictive performance, and ridge cross-
validation. We present several main results in Section 3 and develop the proposed method in 4.
Numerical studies that investigate the performance of the proposed method, and the application
of the proposed method to an RNA sequencing data set are presented in Section 5. Novel find-
ings were highlighted in this section including the significant improvement in the prediction of
5
scitsitats
tset
scitsitats
tset
scitsitats
tset
scitsitats
tsetproteins that are highly implicated biomarkers in the progression of Alzheimer’s disease using the
hypothesized features involved in epigenetic processes. Finally, conclusions and open problems
are discussed in Section 6.
2 Review of Relevant Literature
Inthissection,wediscussthebackgroundoftheproblemandthenecessarynotationstodetermine
the predictive performance of methods or model-fitting procedures based on cross-validation.
Existing work involving prediction error and ridge predictive performance are also discussed.
2.1 Mathematical Preliminaries
Consider the supervised learning setting where the data can be represented as D = {D }N ,
n n=1
where D = (X ,Y ) ∈ RP ×R, n ∈ [N] = {1,2,...,N}. For the n-th subject, denote Y to be
n n n n
the response variable and X⊤ = (X ,X ,··· ,X ),P⋆ = P −1 is the vector of measurements
n n0 n1 nP⋆
includingthefeatures{X }P⋆ andX = 1,foralln ∈ [N]. Invariousapplications,thesampling
np p=1 n0
units{(X ,Y )}N canbecharacterizedasdrawsfromsomejointdistribution,(X ,Y ) ∼ Q(Y |
n n n=1 n n n
f(X ,β))P(X ),n ∈ [N].
n n
Data splitting procedures involve partitioning the data D into G subsets, G ≥ 2. The subsets are
often described as folds, each with N sampling units, g ∈ G = {0,1,...,(G−1)} and |G| = G. In
g
abasictwo-foldsplit(G = 2), typicallyreferredtoastest-trainpartition, denoteG = {0,1}where
N sampling units are used for testing the model performance while the remaining N = N −N
0 1 0
samplingunitsareemployedforgeneratingthepredictivefunctionf(cid:98),whichdependsonparameter
estimates derived from the training data. The primary goal of this study is to address the
standard prediction problem by investigating whether a model-fitting method can accurately
predict the value of a target variable on newly observed data point (X ,Y ), independent of
new new
D = {(X ,Y )}N , from the distribution Q(Y | f(X ,β))P(X ). However, instead of utilizing
n n n=1 n n n
allavailabledataformodelfitting, thespecificobjectiveofthisstudyisondatasplittingmethods
to better understand how models fitted using the training data generalize to the observations in
the testing data.
While cross-validation is considered to be a resampling method, it is clear that the partitioning
of the testing and training data is not unique. In particular, there are L = N!/(N !N !) possible
0 1
combinations or subsets of distinct indices that can be used as testing data. Suppose T =
0ℓ
{ℓ(h)}N0 is the ℓth collection of ordered indices corresponding to sampling units assigned in the
h=1
testing data, where ℓ(1) < ℓ(2) < ··· < ℓ(N ), ℓ(h) ∈ [N],h ∈ [N ], and ℓ ∈ [L]. Correspondingly,
0 0
letT = [N]\T = {ℓ(h)}N betheℓ-thsetoforderedindicesamongsamplingunitsassigned
1ℓ 0ℓ h=N0+1
to the training data, with ℓ(N +1) < ℓ(N +2) < ··· < ℓ(N), ℓ(h) ∈ [N],h ∈ {N +1,...,N},
0 0 0
and ℓ ∈ [L].
Itfollowsdirectlythattheℓ-thpartitionofindicesisarearrangementoftheoriginalsetofindices.
This can be stated as {1,2,...,N} = {ℓ(1),...,ℓ(N ),ℓ(N +1),...,ℓ(N)} which is equivalent
0 0
to T ∪T , T ∩T = ϕ, where |T | = N and |T | = N −N for all ℓ = 1,2,...,L. It also
0ℓ 1ℓ 0ℓ 1ℓ 0ℓ 0 1ℓ 0
follows that for every n ∈ [N], the n-th original observation D = (X ,Y ) can be re-expressed
n n n
as the h-th sampling unit in the ℓ-th test-train partition, denoted by D = (X ,Y ). To
ℓ(h) ℓ(h) ℓ(h)
illustrate this, suppose N = 5, N = 2 and T = {ℓ(1),ℓ(2)}, ℓ(1) < ℓ(2), ℓ = 1,2,...,10. The
0 0ℓ
ten possible combinations of indices for the test-train data splits can be shown as follows:
6ℓ = 1 ℓ = 2 ℓ = 3 ℓ = 4 ℓ = 5
T {1, 2} {1, 3} {1, 4} {1, 5} {2, 3}
0ℓ
T {3, 4, 5} {2, 4, 5} {2, 3, 5} {2, 3, 4} {1, 4, 5}
1ℓ
D {D ,D } {D ,D } {D ,D } {D ,D } {D ,D }
T 1 2 1 3 1 4 1 5 2 3
0ℓ
ℓ = 6 ℓ = 7 ℓ = 8 ℓ = 9 ℓ = 10
T {2, 4} {2, 5} {3, 4} {3, 5} {4, 5}
0ℓ
T {1, 3, 5} {1, 3, 4} {1, 2, 5} {1, 2, 4} {1, 2, 3}
1ℓ
D {D ,D } {D ,D } {D ,D } {D ,D } {D ,D }
T 2 4 2 5 3 4 3 5 4 5
0ℓ
From the fifth test-train partition in the illustration above, the original data {D }5 can be
n n=1
re-expressed as {D ,D ,D ,D ,D } = D ∪D where D = {D = D ,D = D }
2 3 1 4 5 T05 T15 T05 5(1) 2 5(2) 3
and D = {D = D ,D = D ,D = D }. In general, D = {D }N can be rewritten
T15 5(3) 1 5(4) 4 5(5) 5 n n=1
using the ℓth partition of test-train data as
D ∪D = {D }N0 ∪{D }N = {(X ,Y )}N0 ∪{(X ,Y )}N .
T 0ℓ T 1ℓ ℓ(h) h=1 ℓ(h) h=N0+1 ℓ(h) ℓ(h) h=1 ℓ(h) ℓ(h) h=N0+1
Given data set D, suppose T = {T }L where T ∩T ̸= ϕ for some ℓ ̸= ℓ⋆. Without loss
0 0ℓ ℓ=1 0ℓ 0ℓ⋆
of generality, as illustrated above, T ∩T ̸= ϕ but T ∩T = ϕ and T ∪T = [N]. This
01 02 01 11 01 11
also implies that for a given iteration or partition, each observation is used exactly once, either
for generating the predictive rule (i.e., in training) or assessing the model performance (i.e., in
testing). Alternatively, the matrix form of the data for the ℓ−th partition can be viewed as
   
X⊤ X⊤ 
Y
 
Y

ℓ(1) ℓ(N0+1) ℓ(1) ℓ(N0+1)
 X⊤  X⊤   Y  Y 
X T 0ℓ =    ℓ . . .(2)   , X T 1ℓ =    ℓ(N . . .0+2)  , y T 0ℓ =  

ℓ . . .(2)  

and y T 1ℓ =  

ℓ(N . . .0+2)   .
   
X⊤ X⊤ Y Y
ℓ(N0) ℓ(N) ℓ(N0) ℓ(N)
where X is the N ×P matrix of covariates and y as the N ×1 observed response vector
T g T g
gℓ gℓ
for the ℓ-th partition, g ∈ {0,1}.
Meanwhile,D = (X ,y )canbeformulatedbasedonthefeaturematrixXandtheresponse
T T T
gℓ gℓ gℓ
vectory asX = [E ]⊤Xandy = [E ]⊤y whereE istheN×N matrixwhosecolumns
T T T T T g
gℓ gℓ gℓ gℓ gℓ
correspond to the N ×1 elementary vector
E = (e ,e ,...,e ), ℓ(h) ∈ T ,h ∈ {1,2,...,N } (1)
T 0ℓ ℓ(1) ℓ(2) ℓ(N0) 0ℓ 0
E = (e ,e ,...,e ), ℓ(h) ∈ T ,h ∈ {N +1,...,N}.
T 1ℓ ℓ(N0+1) ℓ(N0+2) ℓ(N) 1ℓ 0
The elementary vector e is the column vector containing (N −1) 0’s except for a value of 1 in
ℓ(h)
the ℓ(h)-th position.
2.2 Predictive Performance of Models
Consider a class of models y = f(X,β)+ε parameterized by β ∈ Θ where y ∈ RN is the vector
of observed responses, X⋆ ∈ RN×P⋆ is the matrix of features being tested, f is a general unknown
function of the design matrix X = (1 , X⋆), and ε is the random noise with zero mean, variance
N
σ2, and independent of X. When P = 1 and X = 1 , this model reduces to the intercept-only
ε N
model denoted by y = f(1 ,β )+ε. It follows directly that E(y|X) = f(X,β) is the conditional
N 0
expectation of y for a fixed design matrix X.
Conventionalhigh-dimensionalstatisticalmodelsoftenimposesomeformofregularization. Given
the nature of the data where P is substantially larger than N, regularization is crucial because
7ill-posedness is recognized for both parametric and non-parametric methods (Wainwright, 2014).
Historically, (Tikhonov, 1943) introduced regularization to solve integral equations numerically.
Several estimation methods including maximum likelihood estimation, general M-estimation, and
regularized loss minimization are aimed towards estimating the unknown quantity of interest
β (in the parameter space Θ ⊆ RP) by solving an optimization problem where a penalty or
regularization term is added to the data fitting objective function. More rigorously,
(cid:40) N (cid:41)
(cid:88)
β(cid:98)(λ) = arg min L(Y n,f(X n,b))+λR(b) (2)
b∈RP
n=1
where L : R2 → R is the convex loss function, R : RP → R+ is a convex regularizer, and λ ∈ Λ is
the tuning parameter that quantifies the amount of regularization. In this study, Λ ⊆ R following
the work of Gauran et al. (2022); Kobak et al. (2020); Patil et al. (2021).
Furthermore, the penalized regression literature generally adopts the perspective of maximum
likelihood theory where maximizing the likelihood is equivalent to minimizing the loss function
in (2) where L(Y ,f(X ,b)) is set to logQ(Y | f(X ,b)), when Q is known. In the context of
n n n n
linear regression with Gaussian error terms, the loss function is set to the negative log-likelihood
and has the form L(y,Xβ) = ∥y−Xβ∥2. When N > P and X has full column rank, the unique
2
solution minimizing the quadratic loss function is β(cid:98) = (X⊤X)−1X⊤y. In the case of N < P
OLS
and X⊤X is singular, the estimate of β(cid:98) is not unique.
OLS
Consequently, when performing model assessment, the goal is to examine the generalization per-
formance of a single, fixed model-fitting method M. For a given model, let M be a model-fitting
method whose input is a set of N data points and output is a rule f(cid:98)(·) that predicts the response
y from the set of features X. As an illustration, suppose the true model is y = Xβ +ε, ε ∼
N(0,σ2I ), N > P and rank(X) = P. Define M as the classical linear regression procedure
ε N
whose input is the entire data D, to obtain β(cid:98) and f(cid:98). Employing the quadratic loss function,
OLS
M and L is expressed as follows:
M : RN×P ×RN → RN M({D n}N n=1) = f(cid:98)(X,β(cid:98) OLS) = Xβ(cid:98)
OLS
(3)
L : RN ×RN → R+∪{0} L(y, Xβ(cid:98) ) = ∥y−Xβ(cid:98) ∥2 = r⊤r (4)
OLS OLS 2
where L(y,y) = 0. Based on (3) and (4), the output of the model-fitting method M is an input
of the loss function to obtain a positive scalar that measures the predictive performance of M.
It is important to note that the length of the output vector f(cid:98)should match the length of the
response vector input in L. From (4), the expected value of the average squared residuals is
E (r⊤r) (cid:18) N −P(cid:19)
y|X = σ2 . (5)
N ε N
The quantity above is also referred to as the training error because the entire data is used to
obtain f(cid:98). In the intercept-only model, denoted by y = 1 Nβ 0+ε where P = 1, (5) is equal to the
expectation of the maximum likelihood estimator (MLE) of σ2. For data splitting procedures, the
ε
input of the model-fitting algorithm M is the set of data points from the ℓth partition {X }N0
ℓ(h) h=1
and {(X ,Y )}N . Following the formulation in (2), suppose the estimated value of the
ℓ(h) ℓ(h) h=N0+1
parameter based on the training data is denoted by β(cid:98) . The output of M is a rule f(cid:98)(·) that
T
1ℓ
predicts the value of {Y ℓ(h)}N h=0 1. In many applications, Y(cid:98)ℓ(h) = f(cid:98)(X ℓ(h),β(cid:98)
T
1ℓ) is referred to as the
fitted value used to predict the value of Y using the corresponding covariate X , given that
ℓ(h) ℓ(h)
(X ,Y ) belongs in the ℓth partition of testing data, h ∈ [N ]. To evaluate the predictive
ℓ(h) ℓ(h) 0
performance of the model-fitting algorithm M, define the following functions:
M(X ,X ,y ) : RN0×P ×RN1×P ×RN1 → RN0 (6)
T T T
0ℓ 1ℓ 1ℓ
L(y
T
0ℓ,y
(cid:98)T 0ℓ
= f(cid:98)(X
T
0ℓ,β(cid:98)
T
1ℓ)) : RN0 ×RN0 → R+∪{0}. (7)
8Thus, to address the classical prediction problem, it is crucial to understand how M generalizes
to testing data points using the estimated function f(cid:98). Some prediction error terminologies and
formulations available in previous works are displayed in Table 1.
Let E and E are the expected value with respect to X and y | X , g ∈ {0,1},
X gℓ y gℓ|X gℓ T gℓ T gℓ T gℓ
respectively. Also, let E is the expected value with respect to D . Using (6) and (7), the
D 1ℓ T 1ℓ
prediction error of a fixed f(cid:98), following the definition provided by Borra and Di Ciaccio (2010), is
defined as
L
1 (cid:88) (cid:110) (cid:104) (cid:105)(cid:111)
Err(f(cid:98)) =
LN
E
X 0ℓ
E
y 0ℓ|X 0ℓ
L(y
T
0ℓ,f(cid:98)(X
T
0ℓ,β(cid:98)
T
1ℓ)) | f(cid:98),X
T 0ℓ
. (8)
0
ℓ=1
This is also referred to as the generalization error, out-of-sample error, or extra-sample error
(Bates et al., 2023; Efron and Tibshirani, 1997; Nadeau and Bengio, 1999). It is crucial to note
that Err(f(cid:98)) in (8) is a random quantity that depends on the training data. Hence, after averaging
the prediction error across possible training sets, we obtain the expected prediction error or
average generalization error as
L L
1 (cid:88) (cid:104) (cid:105) 1 (cid:88) (cid:110) (cid:104) (cid:105)(cid:111)
Err =
LN
E
D 1ℓ
Err(f(cid:98)) =
LN
E
X 1ℓ
E
y 1ℓ|X 1ℓ
Err(f(cid:98)) (9)
0 0
ℓ=1 ℓ=1
The expectation (9) enables a general evaluation of the given model-fitting method generating f(cid:98),
independently of a particular sample. Another expression considered by Efron (1986, 2004); Shen
and Ye (2002); Ye (1998) for model selection is the in-sample error of f(cid:98). This is a more restrictive
definition of the prediction error where the covariates are fixed at their observed sample values
X = x while Y are random variables, where {(X ,Y )},h ∈ [N ] belongs in the
ℓ(h) ℓ(h) ℓ(h) ℓ(h) ℓ(h) 0
testing data. The formulation is given by
Err( Xin)
=
LN1
(cid:88)L (cid:88)N0
Err( xi ℓn ()
h)
=
LN1
(cid:88)L (cid:88)N0
E(cid:104)
L(Y ℓ(h),f(cid:98)(x ℓ(h),β(cid:98)
T
1ℓ)) |
f(cid:98),X(cid:105)
. (10)
0 0
ℓ=1h=1 ℓ=1h=1
The expected in-sample prediction error of (10) at the sample point X = x is
ℓ(h) ℓ(h)
(cid:104) (cid:105) (cid:110) (cid:104) (cid:105)(cid:111)
Err( xi ℓn ()
h)
= E
y 1ℓ
Err( xi ℓn ()
h)
= E
y 1ℓ
E
Y ℓ(h)|X ℓ(h)
L(Y ℓ(h),f(cid:98)(x ℓ(h),β(cid:98)
T
1ℓ)) | f(cid:98),X (11)
while the expected in-sample prediction error is
Err(in) =
1
(cid:88)L (cid:88)N0
Err(in) =
1
(cid:88)L (cid:88)N0
E
(cid:104)
Err(in)
(cid:105)
. (12)
X LN x ℓ(h) LN y 1ℓ x ℓ(h)
0 0
ℓ=1h=1 ℓ=1h=1
As shown above, the expression of the expectedin-sample prediction error is computed by averag-
ing with respect to the sampling units in the training data. By using the quadratic loss function
(in)
in (12), we decompose Err into
X
Err(in) = σ2+[Prediction Bias2 | X]+[Prediction Variance | X]. (13)
X ε
where σ2 is also called the irreducible error James and Hastie (1997). If f is a linear function
ε
with N > P parameters and f(cid:98)(x n) = x⊤ n(X⊤X)−1X⊤y = x⊤ nβ(cid:98) OLS, Hastie et al. (2009) showed
that the expected in-sample error in (13) can be rewritten as
N N
1 (cid:88) Err(in) = σ2+ 1 (cid:88) (µ −E(µ ))2+ P σ2. (14)
N xn ε N n (cid:98)n N ε
n=1 n=1
910
erutaretiL
ni
elbaliava
snoitalumroF
dna
seigolonimreT
rorrE
noitciderP
emoS
:1
elbaT
secnerefeR
rorrE
noitciderP
rof
alumroF
ygolonimreT
(cid:105)
(cid:104)
)9991(
oigneB
dna
uaedaN
)y,X(
|
))β(cid:98),
wenX((cid:98)f,
wenY(L
E
=:
µ
n
rorrE
noitazilareneG
(cid:105)
(cid:104)
rorrE
elpmas-artxE
)7991(
inarihsbiT
dna
norfE
D
|
))β(cid:98),
wenX((cid:98)f,
wenY(L
E
=:
)Q,D(rrE
etaR
rorrE
eurT
)1202(
.la
te
litaP
(cid:105)
(cid:104)
)3202(
.la
te
setaB
)y,X(
|
))β(cid:98),
wenX((cid:98)f,
wenY(L
wenDE
=:
YXrrE
rorrE
elpmas-fo-tuO
(cid:111)(cid:105)
(cid:104)
(cid:110)
)0102(
oiccaiC
iD
dna
arroB
wenX,(cid:98)f
|
))β(cid:98),
wenX((cid:98)f,
wenY(L
wenX|wenYE
wenXE
=
)(cid:98)f(rrE
(cid:98)f
fo
rorrE
noitciderP
(cid:105)(cid:17)
(cid:16)
(cid:104)
)7991(
inarihsbiT
dna
norfE
D
|
))β(cid:98),
wenX((cid:98)f,
wenY(L
E
DE
=
])Q,D(rrE[
DE
=
)Q(µ
rorrE
eurT
detcepxE
(cid:111)(cid:105)
(cid:104)
(cid:110)
(cid:105)
(cid:104)
)0102(
oiccaiC
iD
dna
arroB
)(cid:98)f(rrE
X|yE
XE
=
)(cid:98)f(rrE
DE
=
rrE
rorrE
noitciderP
detcepxE
)4002
,6891(
norfE
(cid:105)
(cid:104)
noitciderP
elpmas-nI
)8991(
eY
;)2002(
eY
dna
nehS
X,(cid:98)f
|
))β(cid:98),
wenX((cid:98)f,
wenY(L
wenX|wenYE
=
)(cid:98)f()ni(rrE
(cid:98)f
fo
rorrEThe formulations in (13) and (14) demonstrate the classic trade-off between the bias and the
variance in order to minimize the prediction error. In particular, Borra and Di Ciaccio (2010)
mentioned that if the signal-to-noise ratio is high in (14), overfitting will lead to an expected
in-sample prediction error which is at most twice the error variance. In contrast, underfitting
may result in a much higher prediction error. While these results hold for the squared error loss
and some model assumptions, a major drawback in investigating the prediction error is that it
is non-trivial to develop a general method to decompose the error into any function of the bias
and variance (James and Hastie, 1997). In this study, we adopt the average in-sample prediction
error where the design matrix is assumed to be fixed X. The calculation of this quantity usually
involves two stages where the inner expectation is taken with respect to y | X while the
T T
0ℓ 0ℓ
outer expectation is computed with respect to y | X , for all ℓ ∈ [L].
T T
1ℓ 1ℓ
2.3 Ridge Regression Models
Regularization in (2) is critical in addressing both computational and statistical issues. From
the computational perspective, regularization can lead to algorithmic speed-ups by aiding in the
stability of the optimization (Wainwright, 2014). From the statistical perspective, regularization
is imposed to incorporate certain desirable properties, such as smoothness, in the estimates of
the model parameters (Takane, 2008), and avoid overfitting so that newly observed cases are
uniquely predictable (Bickel et al., 2006). Both the tuning parameter λ and the regularizer
R(b) also control the complexity of the model thereby enabling guarantees on the errors of the
estimators (Wainwright, 2014). Choosing an appropriate regularizer in (2) can lead to a better
bias-variance trade-off and special properties such as low-rank structure Wang et al. (2018).
Anoverviewofvariousformsofregularizationdrivenbynumerousunderlyingstatisticalproblems
is presented by Bickel et al. (2006) and Wainwright (2014). Among the class of well-known reg-
ularizers, information criteria methods such as Akaike information criterion (AIC) and Bayesian
information criterion (BIC) belong to the L norm, where ∥b∥ is the number of non-zero ele-
0 0
ments in b. Meanwhile, the L norm is often considered a convex relaxation of the L norm,
1 0
and it achieves both sparsity and computational efficiency (Tibshirani, 1996). Tikhonov regular-
ization incorporates the L norm which aims to address ill-conditioned or close to ill-conditioned
2
problems (Hoerl, 1962; Hoerl et al., 1975; Hoerl and Kennard, 1970; Tikhonov, 1943).
In many high-throughput ‘omics data, the ridge type of regularization is implemented because
the signals are considered to be dense rather than sparse. As highlighted in Zhao et al. (2023),
genome-wide association studies (GWAS) epitomize this high-dimensional dense (sparsity free)
setting, but similar applications arise in neuroimaging, and social science, among others. In a
modelwithsparsesignals,mostparametersarezeroexceptforafeworstrongsignals,thatis,non-
zero parameters with relatively large magnitude. In contrast, a model with dense signals consists
of many weak or small in magnitude, non-zero parameters. Therefore, ridge regularization takes
into consideration the prior knowledge that the model parameters are not far away from zero,
and in consequence, their estimates should be shrunk towards zero (Hoerl and Kennard, 1970).
Suppose β ∈ Θ is the unknown parameter of interest, given a closed convex set Θ ⊆ RP.
For a linear function f, the full model y = f(X,β)+ε includes an intercept and the features
being tested X⋆. Allowing for the extended range of regularization parameters λ ∈ R, the ridge
regression estimate β(cid:98) R(λ) ∈ RP using the entire data {(X n,Y n)}N
n=1
is given by
(cid:26) (cid:27)
β(cid:98) R(λ) = arg min ∥y−Xb∥2 2+λ∥b∥2
2
= (X⊤X+λI P)+X⊤y (15)
b∈RP
whereA+ denotestheMoore-PenrosegeneralizedinverseorpseudoinverseofamatrixA. Several
highlightsofmathematicallyattractivepropertiesofridgeregressionincludetwicedifferentiability
of the loss function L(y, Xb) = ∥y−Xb∥2 and the regularizer R(b) = ∥b∥2 = b⊤b, closed-form
2 2
11for the maximum penalized likelihood estimator, and a straightforward minimum as shown in
(15).
Likewise, ridge regression is a linear smoother which satisfies Xβ(cid:98) (λ) = H(λ)y where H(λ) =
R
X(X⊤X + λI )+X⊤. When λ = 0 and X⊤X is non-singular, the expression in (15) has the
P
minimum L
2
norm among all least squares solution and reduces to β(cid:98) OLS. Sen and Srivastava
(2012) stated that the formulation in (15) is equivalent to solving the ordinary least squares
problem using the augmented design matrix X (λ) and response vector y
A A
(cid:18) (cid:19) (cid:18) (cid:19)
X y
X (λ) = √ , y = (16)
A λI A 0
P
which yields β(cid:98) A(λ) = [X⊤ A(λ)X A(λ)]−1X⊤ A(λ)y
A
= β(cid:98) R(λ).
Consider the Singular Value Decomposition (SVD) of the matrix X = USV⊤, where U and V
are orthogonal, and S is a diagonal matrix with the (non-negative) singular values. Analogous to
(5), the training error for ridge regression models is
E (r⊤r) (cid:18) N −tr(H(λ))(cid:19) β⊤X⊤(I −H(λ))Xβ
y|X = σ2 + N (17)
N ε N N
 
= σ ε2 1−
N1 (cid:88)R d2d +2
p λ+
β⊤V NBV⊤β
(18)
p=1 p
(cid:32) (cid:33)
λd2 λd2 λd2
where R = rank(X) and B = diag 1 ,··· , p ,··· , P .
λ+d2 λ+d2 λ+d2
1 p P
It is clear from (5) and (17) that the ridge regression methodology yields a class of biased estima-
tors. However, it is possible that its MSE is lower than that of an unbiased estimator. Note that
when λ = 0 and rank(X) = P, the training error in (18) reduces to the training error in (5). On
the other hand, as λ → ∞, (18) reduces to σ2 +(β⊤VSS⊤V⊤β)/N. In Pluta et al. (2021), the
ε
ridgeregressionscorestatisticispresentedasacompromisebetweenthefixedeffectsandvariance
components tests. When λ → 0, the resulting test is close to the fixed effects score statistic with
equivalence at λ = 0. Meanwhile, large values of λ results to a test close to the random effects
score statistic, converging to identical tests as λ → ∞.
Given that ridge regression allows for mathematically rigorous studies, it has been used to ex-
plore and gain initial understanding about various combinations of scenarios, such as asymptotic
and non-asymptotic settings, random coefficients or fixed coefficients, varying ratio of number
of features to number of subjects, among others, the predictive performance of ridge regression
has been studied extensively (see Bartlett et al. (2020); Cule and De Iorio (2013); Dobriban and
Wager (2018); Hastie et al. (2022); Hsu et al. (2012); Karoui (2013); Patil et al. (2021); Richards
et al. (2021); Wu and Xu (2020) for examples). Leveraging these studies, we are interested in
characterizing the LN OCV estimator using ridge regression as the model-fitting algorithm for
0
generating predictive rules f(cid:98)under the full model. The proposed method in the next section is
aimed towards understanding the performance of ridge-regularized linear prediction method rela-
tive to the model-fitting algorithm M under the intercept-only model, that is, when the features
0
being tested are excluded as input to generate the predictive rule f(cid:98).
3 Main Results on Exhaustive Nested Cross-Validation
In this section, we discuss the proposed method that is geared towards identifying the reference
distribution with which to compare other learning-based methods. Suppose that the outcome y
12is continuous and X = (1 ,X⋆). If the goal is to predict y using X, then the null model can be
N
characterized as fitting the model without the features being tested X⋆. For g ∈ {0,1}, define
(g)
Err as
X
L L
Err( X0)
=
L1 (cid:88) E(cid:104)
L(y
T
0ℓ,f(cid:98)0(y
T
1ℓ))(cid:105)
and
Err( X1)
=
L1 (cid:88) E(cid:104)
L(y
T
0ℓ,f(cid:98)1(X,y
T
1ℓ)) |
X(cid:105)
ℓ=1 ℓ=1
as the baseline and full model expected in-sample prediction errors, respectively. To evaluate
the incremental value of X⋆ for the prediction of y, we propose a test for the null hypothesis
(0) (1)
H : Err −Err ≤ 0. In the linear model literature, this is equivalent to testing the null that
0 X X
β = 0 for the fixed-effects setting or σ2 = 0 for the random-effects case.
b
Consider two state-of-the-art learning methods widely used in practice, denoted by M (D) and
1
M (D) for a given data set D = (X,y). Instead of comparing M (D) and M (D) directly with
2 1 2
one another, we end up with examining M (D) and M (y), g = 1,2. This indicates that gains
g 0
in predictive performance of the candidate models are established by comparing them both to the
same null model. The critical advantage of this proposed procedure is that if the performance
of M or M is not significantly different from M , then it is futile to compare them with one
1 2 0
another. Given the proliferation of learning models with the artificial intelligence revolution, it
is crucial to establish a baseline with which to compare all existing and soon-to-be proposed
model-fitting algorithms.
3.1 Leave-N -out Cross-validation Estimator
0
In this study, we will use the Leave-N -out Cross-Validation (LN OCV) because it yields re-
0 0
producible results and is straightforward to apply. More importantly, we will circumvent the
existing drawbacks of the LN OCV estimator by developing computationally feasible closed-form
0
expressions and investigating the asymptotic behavior of the LN OCV estimator. Two simpli-
0
fied expressions of the LN OCV estimator are derived corresponding to (i) the intercept-only
0
model and (ii) the full model with the features being tested X⋆. To characterize these LN OCV
0
estimators, let M and M be the model-fitting method for the intercept-only and full model,
0 1
respectively. First, we present the general results for the LN OCV estimator using ridge regres-
0
sion for generating f(cid:98)1. The procedure involving M
0
for solving the predictive rule f(cid:98)0 is presented
subsequently.
For a fixed λ, there are two main steps to compute the LN OCV(M ) estimator.
0 1
1. Training: The model-fitting algorithm M uses the ℓth training data denoted by D =
1 T
1ℓ
{(X ,y )}N ≡ (X ,y ) and the features corresponding to the hth sampling
ℓ(h) ℓ(h) h=N0+1 T 1ℓ T 1ℓ
unitintheℓthtestingdata,denotedby{X ℓ(h)}N h=0 1,toobtainf(cid:98)1(X ℓ(h),D
T
1ℓ) = X⊤ ℓ(h)β(cid:98)
T
1ℓ(λ)
for all h ∈ [N 0]. When N
0
= 0, it is clear that β(cid:98)
T
(λ) is equivalent to β(cid:98)
R
in (15). When
1ℓ
N > 0, N −N sampling units in the ℓth training data are employed to compute
0 0
(cid:110) (cid:111)
β(cid:98)
T
1ℓ(λ) = arg min ∥y
T 1ℓ
−X
T
1ℓb∥2 2+λ∥b∥2
2
= (X⊤
T
1ℓX
T 1ℓ
+λI P)+X⊤
T
1ℓy
T
1ℓ. (19)
b∈RP
2. Testing: The evaluation of the predictive performance of f(cid:98)1(X ℓ(h),D
T
1ℓ) is employed for
all {Y }N0 in the ℓth partition of the testing data.
ℓ(h) h=1
This procedure is repeated until the model is trained and evaluated on all possible partitions of
13the data, ℓ ∈ [L]. Formally, the LN OCV estimator using M is defined as follows
0 1

   
LN1
0
(cid:88)L (cid:88)N0
(Y ℓ(h)−X⊤ ℓ(h)β(cid:98) T 1ℓ(λ))2, N 0 > 0
LN OCV(M ) = ℓ=1h=1 (20)
0 1 N
 1 (cid:88)
 
 N
(Y n−X⊤ nβ(cid:98) R(λ))2, N
0
= 0
n=1
This implies that the LN OCV estimator in (20) reduces to the mean squared error (MSE) under
0
the setting where the full data is used to obtain the fitted value corresponding to the observed
responses.
Computing β(cid:98) across all possible training data requires L runs of an optimization procedure
T
1ℓ
which is prohibitively costly. To address this issue, we derive an alternative expression that
enables us to obtain the estimate β(cid:98) (λ) in a single shot using the entire data. Therefore, the
R
following main result provides us with considerable savings in terms of computational time.
Lemma 1: Let {(X ,Y )}N be independent and identically distributed draws from some joint
n n n=1
distribution Q(Y | f(X ,β))P(X ). Suppose the full model is y = f(X,β)+ε parameterized
n n n
by β ∈ RP where y is the vector of observed responses, f is a linear function of the design matrix
X ∈ RN×P, E(ε) = 0, V(ε) = σ2I , and ε ⊥⊥ X. For a fixed λ ∈ Λ ⊆ R and N > 0, the
ε N 0
estimator of LN OCV(M ) := LN OCV(1)(λ) can be simplified as
0 1 0
1
(cid:88)L (cid:88)N0
(cid:16) (cid:17)2
LN 0OCV(1)(λ) =
LN
Y ℓ(h)−X⊤ ℓ(h)β(cid:98)
T
1ℓ(λ)
0
ℓ=1h=1
L
1 (cid:88)
= r⊤ (λ)[I −H (λ)]−2r (λ) (21)
LN T 0ℓ N0 T 0ℓ T 0ℓ
0
ℓ=1
1 (cid:88)L y⊤[W
T
(λ)]y
= 0ℓ (22)
L N
0
ℓ=1
wherer (λ)astheN ×1vectoroforiginalresidualscorrespondingtothesubjectswhoseindices
T 0
0ℓ
belong in T , H (λ) is the N ×N symmetric matrix with diagonal elements corresponding
0ℓ T 0ℓ 0 0
to the diagonal elements in H(λ) = X(X⊤X+λI )+X⊤ whose indices belong to T while the
P 0ℓ
off-diagonal elements correspond to the N (N −1)/2 off-diagonal elements in H(λ) whose indices
0 0
are pairwise configurations of T , and W (λ) is an N ×N matrix of weights which is also a
0ℓ T 0ℓ
function of I −H(λ). The proof of Lemma 1 is provided in Appendix A.
N
Remark 1: The computation of LN OCV(1)(λ) in (21) can be translated to a procedure that
0
primarilyrequiresthe(i)N×N hatmatrixH(λ) = X(X⊤X+λI )+X⊤ givenafixedλ ∈ Λ ⊆ R,
P
and the (ii) N ×N matrix E in (1) given T , the collection of indices in the ℓth partition of
0 T 0ℓ 0ℓ
the testing data. The procedure can be described as follows:
1. Using H(λ), obtain the N ×1 vector of residuals r = y−Xβ(cid:98) R(λ) = (I
N
−H(λ))y.
2. UsingE ,obtainI −H (λ) = [E ]⊤[I −H(λ)][E ],[I −H (λ)]−1 andr (λ) =
T
0ℓ
N0 T
0ℓ
T
0ℓ
N T
0ℓ
N0 T
0ℓ
T
0ℓ
[E ]⊤r.
T
0ℓ
3. Repeat Step (2) for all collections of indices in the testing set T ,ℓ ∈ [L].
0ℓ
For the ridge-regularized formulation (15) and (19) with uncentered design matrix X, the estima-
tor of the intercept is
(cid:40)
β(cid:98)0(λ) =
Y −Xβ(cid:98) R(λ), N
0
= 0,
(23)
Y
T 1ℓ
−X
T
1ℓβ(cid:98)
T
1ℓ(λ), N
0
> 0
14where Y = (1⊤ y )/N while X and X are the 1×P vector of column averages of X and
T 1ℓ N1 T 1ℓ 1 T 1ℓ
X , respectively.
T
1ℓ
On the other hand, suppose the null model is described as the intercept-only model, that is, the
model without the features being tested X⋆. In contrast to β(cid:98)0(λ) in (23), the estimator of the
intercept in this model is β(cid:98)0 = Y
T
which which does not depend on the ridge regularization
1ℓ
parameter λ. Consider the ℓ-th test-train partition of the data D. The model-fitting method M
0
and the loss function L in the formulation of the LN OCV(M ) estimator can be characterized
0 0
as
M
0
: RN1 → RN0 M 0({Y ℓ(h)}N h=N0+1) = f(cid:98)(Y
T
1ℓ,1 N0) = Y
T
1ℓ1
N0
(24)
L : RN0 ×RN0 → R+∪{0} L(y , Y 1 ) = ∥y −Y 1 ∥2. (25)
T 0ℓ T 1ℓ N0 T 0ℓ T 1ℓ N0 2
Thus, the formulation of the LN OCV(M ) := LN OCV(0) estimator becomes
0 0 0

   
LN1
0
(cid:88)L (cid:88)N0
(Y ℓ(h)−Y T 1ℓ)2, N 0 > 0
LN OCV(0) = ℓ=1h=1 (26)
0 N
 1 (cid:88)
  (Y −Y)2, N = 0.
 N n 0
n=1
Corollary 1: Let Y ,Y ,...,Y be independent and identically distributed draws from some
1 2 N
distribution P. Suppose the null model corresponds to the intercept-only model which is given by
y = β 1 +ε where ε is the error term with zero mean and variance σ2,n ∈ [N]. The estimator
0 N n ε
of LN OCV(0) can be simplified as
0

   
LN1
0
(cid:88)L (cid:88)N0
(Y ℓ(h)−Y T 1ℓ)2 =
(cid:18)
1+ N
−1
N
0(cid:19)
S Y2, N 0 > 0
LN OCV(0) = ℓ=1h=1 (27)
0 N (cid:18) (cid:19)
 1 (cid:88) N −1
  (Y −Y)2 = S2, N = 0
 N n N Y 0
n=1
where S2 is the sample variance of the entire response vector y. The proof of Corollary 1 is
Y
available in Appendix B.
According to (27), the evaluated loss function among sampling units on all possible testing sets
can be re-expressed simply as a scaled sample variance, which is a function of the full data. In
addition, itisclearthatthequantityin(27)isabiasedestimatorofσ2. However, LN OCV(0) can
ε 0
be approximated by the sample variance S2 of the observed response vector when the number
Y
of sampling units in the training data is sufficiently large, i.e., LN OCV(0) − S2 converges in
0 Y
probability to 0 as N → ∞, while N /N → 0.
0
Corollary 2: Consider the ℓ-th test-train split of the data D. The vector of residuals can be
computed as r
T
0ℓ(λ) = y
T
0ℓ−X
T
0ℓβ(cid:98)
T
1ℓ
with β(cid:98)
T
1ℓ
as the ridge regression estimator fitted using the
trainingdatain(19). WhenN > 0,theexpectedin-samplepredictionerroroftheLN OCV(M )
0 0 1
in (22) can be computed as
L
Err(1) = 1 (cid:88) E(y⊤[W (λ)]y) | X ] (28)
X LN T 0ℓ
0
ℓ=1
σ2 (cid:88)L 1 (cid:88)L
= ε tr{[I −H (λ)]−1}+ β⊤X⊤[W (λ)]Xβ
LN
N0 T
0ℓ LN
T
0ℓ
0 0
ℓ=1 ℓ=1
where W (λ) = [I −H(λ)]⊤[E ][I −H (λ)]−2[E ]⊤[I −H(λ)]. When β = 0 for all
T
0ℓ
N T
0ℓ
N0 T
0ℓ
T
0ℓ
N j
j ∈ {1,2,...,P⋆} and H ≡ 11 1⊤, the in-sample prediction error in (28) reduces to
N N N
(cid:18) (cid:19)
1
Err(1) = σ2 1+ . (29)
X ε N −N
0
15Remark 2: The expected in-sample prediction error of the LN OCV(M ) in (27) is
0 0
(cid:18) (cid:19)
1
Err( X0) =    (cid:18)1 N+ −N 1(cid:19)−N 0 σ ε2, N 0 > 0
   N σ ε2, N 0 = 0.
(0) (1)
It is clear that the expression of Err when N > 0 is equivalent to Err in (29) when β = 0
X 0 X j
for all j ∈ {1,2,...,P⋆} and H ≡ 11 1⊤. As mentioned previously, the training error in (5) is
N N N
equal to the expected value of the MLE of σ2 under the intercept only model with P = 1. More
ε
importantly, the training error in (5) is equal to the expected in-sample prediction error of the
LN OCV(0) estimator in (26) when P = 1 and N = 0.
0 0
Furthermore, the asymptotic behavior of the LN OCV(M ) estimator is also investigated in this
0 1
study,providedN /N → 0. TostudythefeaturesanddistinguishthecomponentsoftheLN OCV
0 0
estimator using the model-fitting algorithm M , the squared terms can be expanded by adding
1
and subtracting f(X ℓ(h)) = E[f(cid:98)(X ℓ(h),β(cid:98)
T
) | X]. Thus, LN 0OCV(M 1) can re-expressed as
1ℓ
LN OCV(M ) = CV (M )+CV (M )+2CV (M ) (30)
0 1 1 1 2 1 3 1
where
1
(cid:88)L (cid:88)N0
CV (M ) = (Y −f(X ))2
1 1 LN ℓ(h) ℓ(h)
0
ℓ=1h=1
1
(cid:88)L (cid:88)N0
CV 2(M 1) =
LN
(f(X ℓ(h))−f(cid:98)(X ℓ(h),β(cid:98)
T
1ℓ))2
0
ℓ=1h=1
1
(cid:88)L (cid:88)N0
CV 3(M 1) =
LN
(Y ℓ(h)−f(X ℓ(h)))(f(X ℓ(h))−f(cid:98)(X ℓ(h),β(cid:98)
T
1ℓ)).
0
ℓ=1h=1
From (30), CV (M ) is the oracle estimate of the excess error of the fitted predictive rule and
2 1
CV (M ) is the cross-term (Rosset and Tibshirani, 2019; Wager, 2020). It is clear that CV (M )
3 1 2 1
and CV 3(M 1) depend on the training data through f(cid:98)(X ℓ(h),β(cid:98)
T
). Meanwhile, CV 1(M 1) can
1ℓ
be re-expressed back to the original form of the observations because it is a function solely
of the test observations {(X ,Y )}N0 for all possible partitions. Counting techniques can
ℓ(h) ℓ(h) h=1
be implemented to determine how many times the test observation (X ,Y ) ≡ (X ,Y ) is
ℓ(h) ℓ(h) n n
encountered among all possible testing sets. This yields
(cid:18) (cid:19) N N
1 N −1 (cid:88) 1 (cid:88)
CV (M ) = (Y −f(X ))2 = (Y −f(X ))2 (31)
1 1 n n n n
LN N −N N
0 0
n=1 n=1
Hence, CV (M ) can be restated as CV because it is no longer a function of the model-fitting
1 1 1
algorithm M 1. CV
1
can be viewed as the training error of the optimally fitted predictor f(cid:98)(·). At
this point, to derive the distribution of LN OCV(M ) as N → ∞, the asymptotic properties of
0 1
(30) can be investigated.
Corollary 3: Considerasequenceofindependentandidenticallydistributedsamples(X ,Y ) ∈
n n
RP ×R which satisfy the following assumptions:
A.1 E(Y4) < ∞ and V(Y | X ) ≤ Σ < ∞
n n n
16A.2 There are constants 0 < c ≤ c < ∞ and 0.25 < δ < 0.5 such that the excess risk of
− +
f(cid:98)(X ,β(cid:98) ) evaluated on a testing data point scales as
ℓ(h) T
1ℓ
(cid:34) (cid:20) (cid:21)1 (cid:35)
(cid:16) (cid:17)2 2
Nl →im ∞P N 1δE f(cid:98)(X ℓ(h),β(cid:98)
T
1ℓ)−f(X ℓ(h)) | {X ℓ(h),Y ℓ(h)}N
h=N0+1
≤ c
−
= 0,
(cid:34) (cid:20) (cid:21)1 (cid:35)
(cid:16) (cid:17)2 2
Nl →im ∞P N 1δE f(cid:98)(X ℓ(h),β(cid:98)
T
1ℓ)−f(X ℓ(h)) | {X ℓ(h),Y ℓ(h)}N
h=N0+1
≤ c
+
= 1
for a given an algorithm M , where X is the set of features from a randomly selected
1 ℓ(h)
observation in the testing set, drawn independently from the collection of observations
belonging to the training set {X ,Y }N .
ℓ(h) ℓ(h) h=N0+1
It follows that
√
N(LN OCV(M )−Err(1)) →d N(0,σ2 ), as N → ∞ (32)
0 1 CV1
σ2
where E(CV ) = Err(1) and V(CV ) = CV1. The proof of Corollary 3 is detailed in Appendix C.
1 1 N
Remark 3: Let Y ,Y ,...,Y be independent and identically distributed draws from some
1 2 N
distribution P where the fourth central moment µ = E[(Y − E(Y ))4] < ∞. When N is
√ 4 n n
sufficiently large, N(LN OCV(M )−Err(0)) is approximately normally distributed with mean
0 0
zero and variance
1
(cid:20) (cid:18)
N
−3(cid:19) (cid:21)(cid:18)
1
(cid:19)2 (cid:18)
1
(cid:19)
µ − σ4 1+ where Err(0) = σ2 1+ . (33)
N 4 N −1 ε N −N ε N −N
0 0
As N −N → ∞ with N /N → 0, Err(0) → σ2. These results are consistent with Corollary 3
0 0 ε
whereLN OCV(M )isasymptoticallyequivalenttothetrainingseterroroftheoptimalpredictor
0 0
without the features being tested, that is,
 
L N
1 (cid:88) (cid:88)
E 
L(N −N )
(Y ℓ(h)−Y T 1ℓ)2  → σ ε2, as N → ∞ with N 0/N → 0.
0
ℓ=1h=N0+1
Consequently, the first-order behavior of LN OCV(M ) does not depend on the method M
0 1 1
being evaluated because the dominant source of noise CV does not depend on M . This also
1 1
means that CV cancels out when comparing two methods M and M where
1 1 0
LN OCV(M )−LN OCV(M ) = (CV (M )−CV (M ))+2(CV (M )−CV (M )).
0 0 0 1 2 0 2 1 3 0 3 1
Furthermore, another goal in this study is to use cross-validation for comparing two different
predictive rules in the context of hypothesis testing. Suppose M and M are two model-fitting
0 1
methods for generating predictive rules that satisfy the assumptions in Corollary 3 with constants
(δ,c ,c ) and (δ⋆,c⋆,c⋆), respectively. If δ > δ⋆ or that δ = δ⋆ and c < c⋆, then the results
− + − + + −
presented in Wager (2020) can be applied to get
LN 0OCV(M 0)−LN 0OCV(M 1) p
→ 1
CV (M )−CV (M )
2 0 2 1
and lim P[LN OCV(M ) > LN OCV(M )] = 1.
0 0 0 1
N→∞
All in all, these results are consistent with Wager (2020) where it can be concluded that given
two methods M and M for generating predictive rules that satisfy Assumptions A.1 and A.2,
0 1
model selection via cross-validation can accurately identify the better model. This suggests that
cross-validation allows for asymptotically perfect model selection.
17Corollary 4: When N = 1 and L = N, a special case of LN OCV(1)(λ) in (21) is the
0 0
Leave-one-out Cross-Validation (LOOCV) estimator is given by
LOOCV(1)(λ) =
1 (cid:88)N
[1−[H(λ)] ]−2r2(λ) =
1 (cid:88)N (cid:32) Y n−X⊤ nβ(cid:98) R(λ)(cid:33)2
. (34)
N nn n N 1−[H(λ)]
nn
n=1 n=1
[I −H (λ)]−2 simplifiestoascalar(1−[H(λ)] )−2 where[H(λ)] isthenthdiagonalelement
N0 T
0ℓ
nn nn
of the hat matrix, and r
T
0ℓ(λ) reduces to scalar residual r n(λ) = Y
n
− X⊤ nβ(cid:98) R(λ). Likewise,
(34) can be interpreted as a weighted mean of the squared residuals where the weights {(1 −
[H(λ)] )−2}N are a function of the features X and the regularization parameter λ ∈ Λ ⊆ R.
nn n=1
In a standard prediction setting, the weights are greater than 1 except when rank(X) = N and
λ = 0.
Remark 4: A special case of the formulation of LN OCV(0) in (26) when N = 1 and L = N
0 0
reduces to the Leave-one-out Cross-Validation (LOOCV) estimator given by
N (cid:18) (cid:19)
LOOCV(0) = 1 (cid:88)(cid:0) Y −Y (cid:1)2 = N S2, N > 1. (35)
N n −n N −1 Y
n=1
where Y is the sample mean of the observed response vector excluding the nth entry Y .
−n n
The weights of the LOOCV estimator and MLE of σ2 are reciprocals of each other. The weight
ε
multiplied by S2 using the MLE is less than one while the weight multiplied by S2 using LOOCV
Y Y
is more than 1. This is because the entire data is used for the MLE while LOOCV leaves one
data point for each of the N stages of testing. This result also highlights the difference between
estimation and prediction.
Remark 5: Suppose β ∈ RP and λ ∈ Λ ⊆ [0,∞) is the ridge regularization parameter.
Let X ∈ R(N−1)×P be the design matrix excluding the nth row X⊤ and y ∈ RN−1 is the
−n n −n
response vector excluding the nth entry Y . When X is full rank, Allen’s Predicted Residual Sum
n
of Squares (PRESS) statistic (Allen, 1974) is given by
N
(cid:88)(cid:16) (cid:17)2
PRESS(D) = Y n−Y(cid:98)−n (36)
n=1
where Y(cid:98)−n = (X⊤X−X nX⊤
n
+D)+(X⊤y −X nY n) and D = diag(d 1,...,d P),d
j
≥ 0,j ∈ [P].
The LOOCV(M ) estimator based on the formulation in Lemma 1 is
1
N
1 (cid:88)(cid:16) (cid:17)2
LOOCV(1)(λ) =
N
Y n−X⊤ nβ(cid:98) −n(λ) (37)
n=1
where β(cid:98) −n(λ) = (X⊤ −nX −n+λI P)+X⊤ −ny −n. The LOOCV estimator in (34) is equivalent to (36)
if instead of the sum, the average among N data points is calculated and D = λI , λ ≥ 0. Also,
P
(34) is equivalent to the computationally efficient Sherman-Morrison-Woodbury formula provided
by Hastie et al. (2009).
When λ = 0 and rank(X) = N, both numerator and denominator in (34) are zero. Hastie et al.
(2022) defined the LOOCV estimates based on the limits λ → 0. Additionally, there has been
an extensive body of literature that aims to develop computationally efficient approximations to
the leave-one-out cross-validation error for ridge regularized estimation problems and its variants
(for examples, see Allen (1974); Cawley and Talbot (2008); Golub et al. (1979); Kearns and Ron
(1997); Meijer and Goeman (2013); Rad and Maleki (2020); Vehtari et al. (2016); Wang et al.
(2018)). Patil et al. (2021) has shown that optimal ridge regularization for predictive accuracy
can be achieved through the minimization of generalized or leave-one-out cross-validation. GCV
18and LOOCV, asymptotically almost surely, ensures the optimal level of regularization for λ ∈ R.
Therefore, LOOCV is used to determine the optimal ridge regularization parameter, discussed
extensively in the next section.
3.2 Nested Leave-N -out Cross-validation Estimator
0
It is critical to emphasize thatthe Leave-N -out Cross-validation estimator described in Lemma 1
0
isafunctionofthetuningparameterλ,whichhastobedeterminedusingthedata. Intheanalyses
ofhigh-dimensionalrealdatawithridgeregularization,theoptimalvalueofλistypicallyobtained
using cross-validation as well. This highlights the need for nested cross-validation wherein the
inner loop is used for identifying the optimal ridge regularization parameter while the outer loop
is used for estimating the prediction error. The N sampling units allocated in either the outer or
inner data are further subdivided into testing and training data.
(out) (out) (out)
Define the ℓ-th outer testing set as T , |T | = N together with the ℓth outer training
0ℓ 0ℓ 0
(out) (out) (out) (out)
set T = [N]\T , with N = N −N sampling units. The outer testing and training
1ℓ 0ℓ 1 0
(out) (out) (out) (out)
data satisfiesT ∪T = [N], T ∩T = ϕ. Also, theℓth innertesting andtraining set
0ℓ 1ℓ 0ℓ 1ℓ
(in) (in) (in) (in) (in) (out) (out) (in)
can be defined as T , |T | = N and T = [N]\(T ∪T ) where T ∪T = T ,
0ℓ 0ℓ 0 1ℓ 0ℓ 0ℓ 0ℓ 0ℓ 0ℓ
(out) (in) (in) (in)
T ∩T = ϕ and T ∩T = ϕ. Following these notations, it is clear that the smallest
0ℓ 0ℓ 0ℓ 1ℓ
(out) (in)
testing set size N to implement nested cross-validation is two, wherein N = N = 1 and
0 0 0
(out) (in)
N = N +N = 2. As an illustration, consider another special case where N = 2 with
0 0 0 0
{(X ,Y ),(X ,Y )} as the testing data for the ℓth partition, ℓ ∈ [L]. There are two ways
ℓ(1) ℓ(1) ℓ(2) ℓ(2)
to proceed:
Method 1: Using N = 2, the indices of the testing set can be expressed as T = {ℓ(1),ℓ(2)}.
0 0ℓ
This leads to the straightforward calculation of the L2OCV estimator.
(out) (in) (out) (in)
Method 2: Using N = N +N where N = N = 1, the indices of the testing set can
0 0 0 0 0
(out) (in) (out) (in)
be expressed as T = {ℓ(1) = n}, T = {ℓ(2) = m}, and T = T ∪T =
0ℓ 0ℓ 0ℓ 0ℓ 0ℓ
{ℓ(1),ℓ(2)} = {m,n}. ThisformulationleadstothecalculationoftheNestedLOOCV
estimator.
To describe the similarities and differences of these two methods, consider the setting where
N = 4, N = 2 and L = 6. For Method 1, the terms necessary to calculate L2OCV(1)(λ) estima-
0
tor can be expressed as a 2×L matrix of q2 (λ) = (Y −f(cid:98)(X ))2 = (Y −X⊤ β(cid:98) (λ))2
ℓ(h) ℓ(h) ℓ(h) ℓ(h) ℓ(h) T 1ℓ
presented in the table below.
ℓ=1 ℓ=2 ℓ=3 ℓ=4 ℓ=5 ℓ=6
T {1,2} {1,3} {1,4} {2,3} {2,4} {3,4}
0ℓ
T {3,4} {2,4} {2,3} {1,4} {1,3} {1,2}
1ℓ
q ℓ2
(1)
(Y1−f(cid:98)(X1))2 (Y1−f(cid:98)(X1))2 (Y1−f(cid:98)(X1))2 (Y2−f(cid:98)(X2))2 (Y2−f(cid:98)(X2))2 (Y3−f(cid:98)(X3))2
q ℓ2
(2)
(Y2−f(cid:98)(X2))2 (Y3−f(cid:98)(X3))2 (Y4−f(cid:98)(X4))2 (Y3−f(cid:98)(X3))2 (Y4−f(cid:98)(X4))2 (Y4−f(cid:98)(X4))2
T
ℓ
T1 T2 T3 T4 T5 T6
The L2OCV(1)(λ) estimator is simply the average of the 2L terms in the matrix. It can also
be viewed as the average of average testing error within a partition. First, compute T as the
ℓ
average of q2 and q2 . Second, compute the average {T }6 . As a consequence of Lemma
ℓ(1) ℓ(2) ℓ ℓ=1
1, the computationally feasible closed form of the L2OCV(1)(λ) estimator is presented in the
following corollary.
Corollary 5: Another special case of LN OCV(1)(λ) in (21) is the Leave-two-out Cross-
0
Validation (L2OCV) estimator where N = 2 and L = N(N −1)/2. For a fixed λ ∈ Λ ⊆ R, this
0
19can be formulated as
L2OCV(1)(λ) =
1 (cid:88)L (cid:34) (1−h ℓ(22)(λ))r ℓ(1)(λ)+h ℓ(12)(λ)r ℓ(2)(λ) (λ)(cid:35)2
(38)
N(N −1) (1−h (λ))(1−h (λ))−h2
ℓ=1 ℓ(11) ℓ(22) ℓ(12)
1 (cid:88)L (cid:34) h ℓ(12)(λ)r ℓ(1)(λ)+(1−h ℓ(11)(λ))r ℓ(2)(λ)(cid:35)2
+
N(N −1) (1−h (λ))(1−h (λ))−h2 (λ)
ℓ=1 ℓ(11) ℓ(22) ℓ(12)
where [H(λ)] = h (λ) is the (ℓ(i),ℓ(j))th element of the hat matrix H(λ).
ℓ(i)ℓ(j) ℓ(ij)
Correspondingly, the terms in the 2×L matrix presented above are reconfigured to provide the
visualization of the nested procedure described in Method 2, shown as follows:
n=1 n=2 n=3 n=4
m=1 × (Y1−X⊤ 1β(cid:98){3,4}(λ))2 (Y1−X⊤ 1β(cid:98){2,4}(λ))2 (Y1−X⊤ 1β(cid:98){2,3}(λ))2 T( 1in)
m=2 (Y2−X⊤ 2β(cid:98){3,4}(λ))2 × (Y2−X⊤ 2β(cid:98){1,4}(λ))2 (Y2−X⊤ 2β(cid:98){1,3}(λ))2 T( 2in)
m=3 (Y3−X⊤ 3β(cid:98){2,4}(λ))2 (Y3−X⊤ 3β(cid:98){1,4}(λ))2 × (Y3−X⊤ 3β(cid:98){1,2}(λ))2 T( 3in)
m=4 (Y4−X⊤ 4β(cid:98){2,3}(λ))2 (Y4−X⊤ 4β(cid:98){1,3}(λ))2 (Y4−X⊤ 4β(cid:98){1,2}(λ))2 × T( 4in)
(out) (out) (out) (out)
T T T T
1 2 3 4
From the illustrations above, it is clear that the terms necessary to obtain L2OCV(M ) and
1
NLOOCV(M ) are the same. However, the nested procedure incorporates an N×N matrix with
1
zero diagonal elements instead of the 2×L matrix.
Remark 6: The Leave-two-out Cross-Validation (L2OCV) estimator can be re-expressed as the
Nested Leave-one-out Cross-validation (NLOOCV) estimator as follows
N
1 (cid:88) (cid:88) (cid:104) (cid:105)
NLOOCV(1)(λ) = ω(1)(λ)r2(λ)+ω(2)(λ)r (λ)r (λ) (39)
N(N −1) mn n mn m n
n=1m̸=n
(1) (2)
where the weights ω (λ) and ω (λ) can be computed as
mn mn
[1−[H(λ)] ]2+[H(λ)]2
ω(1)(λ) = mm mn
mn {[1−[H(λ)] ][1−[H(λ)] ]−[H(λ)]2 }2
mm nn mn
[H(λ)] [2−[H(λ)] −[H(λ)] ]
ω(2)(λ) = mn mm nn .
mn {[1−[H(λ)] ][1−[H(λ)] ]−[H(λ)]2 }2
mm nn mn
Mathematically and visually, we have shown that (38) and (39) require 2L = N(N −1) similar
(in)
terms. ItistrivialtoshowthatwithN = 1, theL(N = 2)OCVestimatorisequivalenttothe
0 0
(in) (in) (out)
Nested L(N −N = 1)OCV estimator where N −N = N . Without loss of generality,
0 0 0
(out)
this implies that LN OCV estimator is equivalent to the Nested LN OCV estimator. The
0 0
(out)
optimal ridge regularization parameter associated with T corresponds to the value of λ ∈ Λ
0ℓ
that minimizes the Inner LN(in) OCV(1)(λ) estimator. Using the result from Lemma 1 in (21),
0
compute q (λ) = [I − H (λ)]−1r (λ). From this N × 1 vector, select the N(in) × 1
T 0ℓ N0 T 0ℓ T 0ℓ 0 0
subvector including observations whose indices belong in T(in) , denoted by q⋆ (λ). Finally, the
0ℓ T(in)
0ℓ
ridge regularization parameter can be computed as
L(in)
1 (cid:88)
λ(cid:98) = arg min [q⋆ (λ)]⊤[q⋆ (λ)]. (40)
T 0( ℓout) λ∈Λ L(in)N 0(in)
ℓ=1
T 0( ℓin) T 0( ℓin)
(out) (in) (out)
Asanillustration, considerthesimplestsettingwhereN = N = 1anddefineT = {n},
0 0 0ℓ
T(in) = {m}, and T = {m,n},m ̸= n,m,n ∈ [N]. The Inner LN(in) OCV(1)(λ) estimator can be
0ℓ 0ℓ 0
20calculated as
N−1
Inner LOOCV(1)(λ) = N1
−1
(cid:88) T( ℓin) (λ) = N1
−1
(cid:88) (cid:16) Y m−X⊤ mβ(cid:98) [N]\{m,n}(λ)(cid:17)2 . (41)
ℓ=1 m̸=n
Hence, the optimal ridge penalty parameter λ(cid:98)n is chosen to be the minimizer of the Inner
LOOCV(1)(λ) for all candidate values of λ in the collection Λ. This procedure is repeated for
all possible outer testing sets T 0( ℓout) = {n},n ∈ [N] to obtain {λ(cid:98)n}N n=1. The general pseudocode
for the Inner Cross-validation procedure to obtain the optimal ridge regularization parameter is
shown below.
1: procedure Inner CV Estimator:(X,y,Λ,T ,T(out) )
0ℓ 0ℓ
(in) (out) (in) (in) (out)
2: Specify T = T \T where T = [N]\(T ∪T ) = T .
0ℓ 0ℓ 0ℓ 1ℓ 0ℓ 0ℓ 1ℓ
(cid:18)N −N(out)(cid:19)
3: Specify L(in) = 0 where |T(out) | = N(out) and N(in) = N −N(out) .
(in) 0ℓ 0 0 0 0
N
0
4: for λ ∈ Λ do
5: for ℓ ∈ {1,2...,L(in)} do
(in)
6: Using T
1ℓ
≡ T 1ℓ, solve β(cid:98)
T
1ℓ(λ)).
7: Using T(in) , solve T(in) (λ) = 1 (cid:88) (cid:16) Y −X⊤ β(cid:98) (λ)(cid:17)2 .
0ℓ ℓ N(in) ℓ(h) ℓ(h) T 1ℓ
0 ℓ(h)∈T(in)
0ℓ
8: end for
L(in)
9: Solve Inner LN(in) OCV(1)(λ) = 1 (cid:88) T(in) (λ).
0 L(in) ℓ
ℓ=1
10: end for
11: Solve λ(cid:98) := arg min Inner LN(in) OCV(1)(λ).
T(out) 0
0ℓ λ∈Λ
12: end procedure
Moreover,thesquaredtermsbeingsummedupin(41),afterplugging-inλ(cid:98)n,correspondtotheN−
1termsinL2OCVestimatorforafixedoutertestingset. Thus,summingupInnerLOOCV(1)(λ(cid:98)n)
estimator for all possible outer testing sets {n},n ∈ [N] yield the Outer LOOCV(1) estimator.
(out) (out)
The Outer LN OCV estimator is also referred to as the Nested Leave-N -out Cross-
0 0
validationestimatorusingridgeregression. Consequently, itisclearfromtheInnerandOuterCV
procedures presented above that the main results from Lemma 1 can yield substantial computa-
(out) (in)
tionalsavingsbyprovidingatractableforminsolvingT andT . Thepipelineincorporating
ℓ ℓ
Lemma 1 to the calculations in the the Inner and Outer CV procedures is summarized in Figure
2.
SimilartothecalculationoftheLN
OCV(1)(λ)estimator,thecomputationofNestedLN(out)
OCV(M )
0 0 1
requires the N ×N hat matrix H(λ) to obtain the N ×1 vector of residuals r = y−Xβ(cid:98) (λ) =
R
(I −H(λ))y, for a fixed λ ∈ Λ. However, instead of using just E , the procedure employs the
N T
0ℓ
(out) (out)
N ×N matrix E given the outer testing set of indices T . Together with H(λ) and
0 T(out) 0ℓ
0ℓ
r(λ), E and E can be used to compute the N ×N matrix of weights W and W ,
T 0ℓ T(out) T 0ℓ T(out)
0ℓ 0ℓ
respectively. The remaining step is to implement the formulation of (22) in Lemma 1.
21(out)
ThegeneralpseudocodeforcalculatingtheOuterCross-validationestimatorwhenbothN ,N >
0 0
0 is as follows:
1: procedure Outer CV Estimator:(X,y,Λ,N ,N(out) )
0 0
(cid:18) N(cid:19) (cid:18)
N
(cid:19) (cid:18)N −N(out)(cid:19)
2: Specify L = , L(out) = , and L(in) = 0 .
N 0 N 0(out) N 0−N 0(out)
3: Specify all testing sets {T = {ℓ(1),...,ℓ(N(out) ),ℓ(N(out) +1),...,ℓ(N )}}L .
0ℓ 0 0 0 ℓ=1
4: Using T , specify all outer testing sets {T(out) = {ℓ(1),ℓ(2),...,ℓ(N(out) )}L(out).
0ℓ 0ℓ 0 ℓ=1
5: UsingT ,specifyallinnertestingsets{T(in) = {ℓ(N(out) +1),ℓ(N(out) +2),...,ℓ(N )}L(in).
0ℓ 0ℓ 0 0 0 ℓ=1
6: for ℓ ∈ {1,...,L(out)} do
(in) (out)
7: Using T and T , solve λ(cid:98) using the Inner CV procedure.
0ℓ 0ℓ T(out)
0ℓ
(out)
8: Using T , solve β(cid:98) (λ(cid:98) ).
1ℓ T(out) T(out)
1ℓ 0ℓ
9: Using T(out) , solve T(out) (λ(cid:98) ) = 1 (cid:88) (cid:16) Y −X⊤ β(cid:98) (λ(cid:98) )(cid:17)2 .
0ℓ ℓ T(out) (out) ℓ(h) ℓ(h) T(out) T(out)
0ℓ N 1ℓ 0ℓ
0 ℓ(h)∈T(out)
0ℓ
10: end for
L(out)
(out) 1 (cid:88) (out)
11: Solve the Outer LN OCV := T (λ(cid:98) ).
0 L(out) ℓ T(out)
0ℓ
ℓ=1
12: end procedure
Figure 2: Pipeline for calculating the Nested Leave-N(out)-out Cross-validation estimator
0
using ridge regression as the model-fitting method.
224 Exhaustive Nested Cross-validation Tests for High-
dimensional data
Forhigh-dimensionaltesting, weproposetousetheexpectedin-samplepredictionerrordifference
as the measure to compare the predictive performance with or without the hypothesized features.
(0) (1)
Therefore, we are interested in testing the null hypothesis H : Err −Err ≤ 0. An alternative
0 X X
expression for the null hypothesis is to view it in terms of the percent change in prediction error,
denoted by ∆,
(0) (1)
Err −Err
∆ = X X ×100%, provided Err(0) > 0, (42)
(0) X
Err
X
where high values of ∆(cid:98) provide evidence against the null. Using Corollary 2, it was shown
that Err(1) is equivalent to Err(0) when {β = 0}P⋆ and X = 1 . This is consistent with
X X j j=1 N
the interpretation that the information carried by the hypothesized features do not yield any
significant additional improvement in the predictive performance, if we fail to reject the null
hypothesis. Incontrast,therejectionofH suggeststhatthesetoffeaturesX⋆producessignificant
0
improvement to the prediction of y. The procedure of the proposed high-dimensional test can be
described as follows:
1: procedure Nested LN(out)OCV Test:(X,y,Λ,α,N ,N(out) )
0 0 0
(cid:32) (cid:33)
1
2: Solve the average, LN(out) OCV(0) := 1+ S2.
0 (out) Y
N −N
0
L
3: Solve the average, Nested LN 0(out) OCV(1) := L1 (cid:88) [q T(out)(λ(cid:98)ℓ)]⊤[q T(out)(λ(cid:98)ℓ)].
0ℓ 0ℓ
ℓ=1
4: Solve the variance associated with the LN(out) OCV(0) estimator.
0
5: Solve the variance associated with the Nested LN(out) OCV(1) estimator.
√ 0
N[LN(out) OCV(0)−NLN(out) OCV(1)]
6: Solve the test statistic T := 0 0
N(out)CV S
0 N(out)CV
0
7: Solve the P-value := P (T > T = t ).
t df=N−1 N(out)CV obs
0
8: Reject the H if P-value ≤ α.
0
9: end procedure
ItisimportanttonotethatwearecomparingLN(out) OCV(0) andNLN(out) OCV(1) becauseunder
0 0
the null where we employ the intercept-only model, there is no need for regularization.
4.1 Test based on Nested Leave-one-out Cross-validation Esti-
mator
To compute the test statistic based on the Nested LOOCV estimator, obtain two N ×1 vectors
T(g) = (T(g) ,T(g) ,...,T(g) )⊤, g = 0,1 where
1 2 N
(cid:18)
N
(cid:19)2
T(0) = (Y −Y )2 = (Y −Y)2 (43)
n n −n N −1 n
(cid:32) (cid:33)2
T n(1) = (Y n−X⊤ nβ(cid:98) −n(λ(cid:98)n))2 =
Y n−X⊤ nβ(cid:98)(λ(cid:98)n)
. (44)
1−[H(λ(cid:98)n)]
nn
23and the optimal regularization parameter is derived to be
1 (cid:88)
(cid:20)
{1−[H(λ)] nn}r m(λ)+[H(λ)] mnr n(λ)
(cid:21)2
λ(cid:98)n = arg min
N −1 {1−[H(λ)] }{1−[H(λ)] }−[H(λ)]2
(45)
λ∈Λ mm nn mn
m∈[N]\{n}
(1)
after implementing Lemma 1. Plugging-in (45) to T , the average of the paired differences
n
becomes
T =
(cid:18) N (cid:19)
S2 −
1 (cid:88)N (cid:32) Y n−X⊤ nβ(cid:98)(λ(cid:98)n)(cid:33)2
= LOOCV(0)−NLOOCV(1) (46)
1CV N −1 Y N
n=1
1−[H(λ(cid:98)n)]
nn
Meanwhile, V(T ) = V(LOOCV(0))+V(NLOOCV(1))−2Cov(LOOCV(0),NLOOCV(1)) is es-
1CV
(cid:92)
timated by V(T ) = S2 = S +S −2S where
1CV 1CV 1CV,00 1CV,11 1CV,01
1
(cid:18)
N
(cid:19)4 (cid:88)N (cid:18)
N
(cid:19)3
S = (Y −Y)4− S4 (47)
1CV,00 N −1 N −1 n N −1 Y
n=1
S 1CV,11 =
1 (cid:88)N (cid:32) Y n−X⊤ nβ(cid:98)(λ(cid:98)n)(cid:33)4 −(cid:18) N (cid:19)

1 (cid:88)N (cid:32) Y n−X⊤ nβ(cid:98)(λ(cid:98)n)(cid:33)2 2
N −1
n=1
1−[H(λ(cid:98)n)]
nn
N −1 N
n=1
1−[H(λ(cid:98)n)]
nn
S =
1 (cid:88)N T(0)T(1)−(cid:18) N (cid:19)2 S Y2 (cid:88)N (cid:32) Y n−X⊤ nβ(cid:98)(λ(cid:98)n)(cid:33)2
.
1CV,01 N −1
n=1
n n N −1 N
n=1
1−[H(λ(cid:98)n)]
nn
Finally, the test statistic is calculated as
√
N[LOOCV(0)−NLOOCV(1)]
T = . (48)
1CV
S
1CV
The procedure described above is visualized in Figure 3. Suppose the data set includes N = 4
observations. First, one observation is allocated to the outer loop testing set while the remaining
N − 1 inner loop observations are considered to be the outer loop training data. Second, the
observations in the inner loop are split once again by assigning one observation to the inner loop
testing set while the remaining N −2 observations are for fitting the model. The training and
testing data in the inner loop is used to select the optimal ridge regularization parameter. After
model selection is performed, the optimal ridge regularization parameter is used to fit the model
on the N −1 observations in the outer loop training data. Then, model performance using the
testing set in the outer loop is evaluated. This procedure is repeated until all N observations are
used as testing set in the outer loop.
4.2 Test based on Nested Leave-two-out Cross-validation Esti-
mator
(out)
Similar to Section 4.1, the algorithm described in Nested LN OCV Test is developed. The
0
test statistic based on the Nested L2OCV estimator is
√
N[L2OCV(0)−NL2OCV(1)]
T = . (49)
2CV
S
2CV
To compute the test statistic, obtain two N(N −1)×1 vectors T(g) = (T(g) ,T(g) ,...,T(g) )⊤,
1 2 N
g = 0,1 where T(g) = (T(g) ,...,T(g) ,T(g) ,T(g) )⊤ is an (N −1)×1 vector.
n 1n n−1,n n+1,n Nn
24T(0) = (Y −Y )2, T(1) = (Y −X⊤β(cid:98) (λ(cid:98) ))2 T(0) = (Y −Y )2, T(1) = (Y −X⊤β(cid:98) (λ(cid:98) ))2
1 1 −1 1 1 1 −1 1 2 2 −2 2 2 2 −2 2
T(0) = (Y −Y )2, T(1) = (Y −X⊤β(cid:98) (λ(cid:98) ))2 T(0) = (Y −Y )2, T(1) = (Y −X⊤β(cid:98) (λ(cid:98) ))2
3 3 −3 3 3 3 −3 3 4 4 −4 4 4 4 −4 4
Figure 3: Illustration of the procedure to obtain T(g),g = 0,1 in (43) and (44) for ℓ =
ℓ
1,2,3,4 = N using the test based on Nested LOOCV. The vector of paired differences
(T ,T ,T ,T ) with T = T(0) −T(1) will be used to compute the test statistic.
1 2 3 4 ℓ ℓ ℓ
25For any m ̸= n,m,n ∈ [N], let T m(0 n) = (Y m−Y [N]\{m,n})2 and T m(1 n) = (Y m−X⊤ mβ(cid:98) [N]\{m,n}(λ(cid:98)mn))2.
Using Lemma 1 and Corollary 1, this can be simplified as
(cid:20)(cid:18) (cid:19) (cid:21)2
1 Y −Y
T(0) = 1+ (Y −Y)+ n (50)
mn N −N m N −N
0 0
(cid:34) (cid:35)2
T(1) =
{1−h nn(λ(cid:98)mn)}r m(λ(cid:98)mn)+[h mn(λ(cid:98)mn)r n(λ(cid:98)mn)
mn
{1−h mm(λ(cid:98)mn)}{1−h nn(λ(cid:98)mn)}−h2 mn(λ(cid:98)mn)
where h mn(λ(cid:98)mn) = [H(λ(cid:98)mn)]
mn
is the (m,n)th element of the hat matrix H(λ(cid:98)mn). The N(N−1)
(0) (1)
paired differences T = T −T can be computed using (50). The average of these N(N−1)
mn mn mn
terms becomes T = L2OCV(0)−NL2OCV(1).
2CV
However, intermediate steps are necessary to identify the appropriate value of λ to plug into
NL2OCV(1), denoted by λ(cid:98)mn. To determine this optimal regularization parameter λ(cid:98)mn, we also
use N(in) = 1 where L(in) = N −2. Define T = {m,n}∪{v} where T(in) = {v},v ∈ [N]\{m,n}.
0 0ℓ 0ℓ
To visualize of the calculation of λ(cid:98)mn, consider the table below where the rows are set to the
value of m and the columns are set to the value of n when N = 4. The N −2 = 2 values inside
the cells correspond to the terms needed to be averaged to compute λ(cid:98)mn for each pair of outer
test sampling units {m,n}.
n = 2 n = 3 n = 4
m = 1 (Y 3−X⊤ 3β(cid:98) {4})2 (Y 2−X⊤ 2β(cid:98) {4})2 (Y 2−X⊤ 2β(cid:98) {3})2
(Y 4−X⊤ 4β(cid:98) {3})2 (Y 4−X⊤ 4β(cid:98) {2})2 (Y 3−X⊤ 3β(cid:98) {2})2
m = 2 × (Y 1−X⊤ 1β(cid:98) {4})2 (Y 1−X⊤ 1β(cid:98) {3})2
× (Y 4−X⊤ 4β(cid:98) {1})2 (Y 3−X⊤ 3β(cid:98) {1})2
m = 3 × × (Y 1−X⊤ 1β(cid:98) {2})2
× × (Y 2−X⊤ 2β(cid:98) {1})2
Using the Inner CV procedure, the optimal value of λ corresponds to
1 (cid:88) (cid:16) (cid:17)2
λ(cid:98)mn = arg min
N −2
Y
v
−X⊤ vβ(cid:98) [N]\{m,n,v}(λ) . (51)
λ∈Λ
v∈[N]\{m,n}
Again, V(T ) = V(L2OCV(0))+V(NL2OCV(1))−2Cov(L2OCV(0),NL2OCV(1)) can be esti-
2CV
(cid:92)
mated by V(T ) = S2 = S + S + S + S − 2Cov(T(0),T(1)). To
2CV 2CV 2CV,01 2CV,02 2CV,11 2CV,12
compute S and S , define the weights associated with (Y −Y)b as
2CV,01 2CV,02 n
(cid:18)
1
(cid:19)b
ζb = (52)
0 N −2
ζb = (1+aζ )b, a = 1,2,3, b = 1,2,3,4. (53)
a 0
Using (52) and (53),
N (cid:20) (cid:18) (cid:19) (cid:21)
S = (cid:0) ζ ζ4−ζ2ζ ζ2(cid:1)(cid:88) (Y −Y)4+ζ4 2ζ 1+ 3ζ 0 −ζ S4
2CV,01 0 1 0 1 3 n 1 0 ζ 1 Y
2
n=1
ζ3ζ2 (cid:88)N
S = 0 2 (Y −Y)4−ζ2ζ ζ S4.
2CV,02 ζ n 0 1 2 Y
1
n=1
The remaining terms are computed directly from the data. We consider several configurations of
the test statistics described in (48) and (49). This will enable us to comprehensively investigate
the performance of the procedures. Ultimately, we aim to provide a guide identifying the best
method to implement. Therefore, we examine the performance of the proposed methods using
simulation studies presented in the next section.
265 Data Analysis and Numerical Experiments
In this section, results from the simulation studies are presented to gain insights regarding the
performance of the proposed procedures in terms of the Type I error and empirical power. These
simulationresultswillprovideaguideonwhichtesttouseandwhichinsightfulfindingstoreport,
when applied to the RNA sequencing data.
5.1 Numerical Studies
Let P be the number of columns in X, P = P⋆+1, where P⋆ is the number hypothesized features
being tested, while N is the number of observations. The comparison is based on the following
simulation settings:
(i) number of observations N, N ∈ {50,75,100,125,150}
(ii) ratio γ = P/N where P ∈ {N +1,2N,3N,4N,5N}
(iii) strength of the signal ξ and,
(iv) design matrix X.
The design matrix X is generated from multivariate normal distribution N(0 ,Σ ) where the
P X
covariance structure is either heteroskedastic or compound symmetric. For the heteroskedastic
covariance structure, Σ = G where the jth diagonal entry is g = log(j +1), j = 1,2,...,P.
X P j
On the other hand, the compound symmetric structure Σ is characterized by ρ = 0.025. If the
X X
covariance structure is compound symmetric, the values of the signals considered are ξ = 0.05A,
A = 0,1,2,...,9. Meanwhile, if the covariance structure is heteroskedatic, the ten values of the
signals considered are ξ = 0.1A, A = 0,1,2,...,9. The response vector y is generated using
y = Xβ + ε where β = ξ1 and ε ∼ N(0,σ2),n ∈ [N] with σ2 = 0.5. The null setting
P n ε ε
corresponds to the case wherein the signal ξ is equal to zero.
Given the popularity of LOOCV among existing literature, the test based on Nested LOOCV
discussed in Section 4.1 is used as a benchmark procedure. In particular, Bayle et al. (2020) pro-
posed a test for algorithm improvement and a confidence interval using K-fold cross-validation,
of which LOOCV was considered as a special case. However, to ensure that all comparisons are
meaningful, all tests considered are based on exhaustive nested CV methods. Hence, the bench-
mark Nested LOOCV method resembles the proposed method of Bates et al. (2023). Specifically,
(out)
there are three test statistics being compared based on Nested LN OCV estimator:
0
√
N[LOOCV(0)−NLOOCV(1)]
1. T = in (48),
1CV
S
1CV
√
N[L2OCV(0)−NL2OCV(1)]
2. T = in (49), and
2CV
S
2CV
√
N[(LOOCV(0)−NLOOCV(1))+(L2OCV(0)−NL2OCV(1))]
3. T = .
Hybrid
S
Hybrid
ThethirdteststatisticisahybridbetweentheT andT tobalanceoutthetendencyofT
1CV 2CV 1CV
to yield liberal results whereas T tends towards conservative results. Given that all three test
2CV
statisticscaptureapairedtestsetting, thentheirobservedvaluesarecomparedwiththe(1−α)th
quantile of the t-distribution with degrees of freedom equal to N −1. Additionally, a fourth test
corresponds to the Wilcoxon Signed Rank Test using the two N×1 vector T(g),g = 0,1 based on
the Nested LOOCV estimator described in Section 4.1. It follows directly from these four tests
27that four lower bounds can be computed. If the lower bound is zero or negative, then we fail to
reject the null hypothesis. Conversely, a positive lower bound leads to the rejection of the null
hypothesis. The total number of rejections can be computed as
B
1 (cid:88) (b)
Rej = I{P ≤ α} (54)
B CV
b=1
(b)
where P represents the p-value of the test for the bth generated data. In this study, the level of
CV
significance α is set to 0.05 and B = 1000. When ξ = 0 and the null hypothesis is true, the Type
I error rate is computed empirically using (54). However, when ξ > 0 and the null hypothesis
does not hold, (54) corresponds to the empirical power of the test. The four tests being compared
are referred to as: (1) Nested LOOCV (t), (2) Nested LOOCV (W), (3) Nested L2OCV, and (4)
Hybrid CV method. As mentioned previously, tests (1) and (2) are both based on the Nested
LOOCV estimator but differ on whether a parametric paired t-test is implemented compared to
a non-parametric Wilcoxon Signed Rank Test for paired samples.
Figure 4 presents the empirical Type I error of the four tests based on the Nested LN OCV
0
estimator. Thenumericalcomparisonisdoneforatotalof50settingsusingtwotypesofcovariance
structuresand25combinationsofN andγ = N/P. BasedonFigure4,theTypeIerrorresultsfor
the two covariance structures being compared are almost identical. For the test based on Nested
L2OCV, we observe an increasing empirical Type I error rate when γ increases for a fixed N,
regardless of the covariance structure. For all 50 combinations of settings being considered, the
Type I error is controlled using the test based on Nested L2OCV estimator. However, we observe
some settings with inflated Type I error results when N = 50 and N = 75 for the parametric and
non-parametric Nested LOOCV methods as well as the Hybrid CV method. In particular, we
note a decreasing Type I error from γ = P/N = 1 to γ = 3 while it increases from γ = P/N = 3
to γ = 5. In general, the empirical Type I error rate decreases as N increases, for all tests being
considered.
Meanwhile, the empirical power of the tests based on (1) Nested LOOCV (t), (2) Nested LOOCV
(W), (3) Nested L2OCV, and (4) Hybrid CV when the covariance structure is compound sym-
metric or heteroskedastic are displayed in Figures 5 and 6, respectively. For all the tests, the
empirical power approaches one faster as the sample size increases. This finding is intuitive be-
cause increased sample size is typically associated to increased statistical power. Similarly, Figure
5 shows that for all the tests, the empirical power approaches one faster as γ increases. In Figure
6, the opposite is true where the empirical power approaches one faster for lower values of γ. This
finding suggests that the covariance structure together with the ratio of P to N play a role in the
empirical power of the Nested CV-based tests.
5.2 Analysis of RNA Sequencing Data
Many neurodegenerative dementias exhibit a multifactorial origin, arising from both environmen-
tal and genomic influences. Among these potential factors, a history of traumatic brain injury
(TBI) is a proposed life event risk factor, which involves head trauma resulting in either focal or
diffuse injury (Blennow et al., 2012; Dams-O’Connor et al., 2016). The effects of TBI can mani-
fest acutely, but it is the chronic form that exhibits a stronger association with cognitive decline
(Kenney et al., 2018; Torjesen, 2018). Individuals with a history of TBI often present with neu-
rodegenerativelesions, collectivelyreferredtoasTBI-relatedneurodegeneration(TReND)(Smith
et al., 2021, 2013; Wilson et al., 2017). However, despite documented alterations in clinical phe-
notype, brain activity, and histopathological features observed in TBI and TReND cases, our
understanding of the molecular mechanisms underpinning long-term neuropathological changes
remains limited.
28Compound Symmetric Heteroskedastic
5 0.091 0.08 0.055 0.051 0.047 0.096 0.08 0.052 0.036 0.05
4 0.066 0.053 0.035 0.027 0.031 0.077 0.054 0.032 0.021 0.032
3 0.049 0.029 0.016 0.008 0.01 0.045 0.038 0.019 0.013 0.008
2 0.009 0.003 0.006 0.004 0.002 0.008 0.008 0.008 0.002 0.002
~1 0.009 0.002 0.001 0.001 0 0.007 0.004 0 0.002 0
5 0.029 0.02 0.016 0.011 0.011 0.032 0.025 0.018 0.009 0.011
4 0.022 0.012 0.01 0.003 0.006 0.022 0.016 0.011 0.005 0.006
3 0.018 0.005 0.004 0.002 0.001 0.016 0.015 0.005 0.001 0.001
2 0 0 0.004 0 0 0 0.002 0.003 0 0
~1 0.002 0 0 0 0 0.003 0.001 0 0 0
5 0.096 0.092 0.068 0.069 0.059 0.099 0.091 0.065 0.052 0.059
4 0.081 0.066 0.045 0.031 0.039 0.084 0.063 0.04 0.023 0.043
3 0.073 0.044 0.018 0.012 0.018 0.073 0.043 0.023 0.016 0.018
2 0.092 0.051 0.009 0.008 0.005 0.092 0.033 0.011 0.004 0.004
~1 0.109 0.085 0.04 0.008 0.003 0.104 0.086 0.035 0.01 0.002
5 0.093 0.086 0.058 0.063 0.056 0.094 0.084 0.059 0.048 0.057
4 0.084 0.063 0.04 0.035 0.042 0.084 0.063 0.046 0.027 0.046
3 0.077 0.046 0.025 0.031 0.026 0.079 0.049 0.03 0.025 0.024
2 0.108 0.091 0.039 0.031 0.013 0.104 0.056 0.037 0.026 0.015
~1 0.113 0.122 0.069 0.044 0.027 0.108 0.097 0.066 0.039 0.028
50 75 100 125 150 50 75 100 125 150
N
Figure 4: Empirical Type I error of the tests based on (1) Nested LOOCV (t), (2) Nested
LOOCV (W), (3) Nested L2OCV, and (4) Hybrid CV.
29
g
Hybrid
CV
Nested
L2OCV
Nested
LOOCV
(t)
Nested
LOOCV
(W)Hybrid CV Nested L2OCV Nested LOOCV (t) Nested LOOCV (W)
1.00
0.75
0.50
0.25
0.00
1.00
0.75
0.50
0.25
0.00
ratio
1.00
~1
0.75
2
0.50
3
0.25
4
0.00
5
1.00
0.75
0.50
0.25
0.00
1.00
0.75
0.50
0.25
0.00
0.1 0.2 0.3 0.4 0.1 0.2 0.3 0.4 0.1 0.2 0.3 0.4 0.1 0.2 0.3 0.4
x
Figure5: EmpiricalPowerofthetestsbasedon(1)NestedLOOCV(t), (2)NestedLOOCV
(W), (3) Nested L2OCV, and (4) Hybrid CV when the covariance structure is compound
symmetric.
30
N
=
50
N
=
75
N
=
100
N
=
125
N
=
150Hybrid Nested L2OCV Nested LOOCV (t) Nested LOOCV (W)
1.0
0.8
0.6
0.4
1.0
0.8
0.6
0.4
ratio
1.0
~1
0.8
2
0.6 3
4
0.4
5
1.0
0.8
0.6
0.4
1.0
0.8
0.6
0.4
0.25 0.50 0.75 0.25 0.50 0.75 0.25 0.50 0.75 0.25 0.50 0.75
x
Figure6: EmpiricalPowerofthetestsbasedon(1)NestedLOOCV(t), (2)NestedLOOCV
(W),(3)NestedL2OCV,and(4)HybridCVwhenthecovariancestructureisheteroskedas-
tic.
31
N
=
50
N
=
75
N
=
100
N
=
125
N
=
150Given these challenges in understanding the role of epigenetic processes, we analyze the data ob-
tained from the Aging, Dementia, and Traumatic Brain Injury Study using the proposed method.
Collected by the Allen Institute for Brain Science (https://aging.brain-map.org), mRNA ex-
pressionpatterns, Luminexproteinquantifications, andimmunohistochemistrypathologymetrics
amongN = 44subjectswithahistoryofTBIwereincludedintheanalysis. Specifically, thetran-
scriptomewasanalyzedbyRNAsequencingintissueisolatedfromfourbrainregions, namely: (1)
white matter underlying the parietal neocortex (FWM), (2) hippocampus (HIP), (3) neocortex
fromtheposteriorsuperiortemporalgyrus(TCx), and(4)theinferiorparietallobule(PCx). The
mRNA expression profiles involved in epigenetic processes among subjects who experienced TBI
areconsideredtobefeaturesorpredictors. Ontheotherhand, protein-levelalterationsintauand
phosphorylated tau variants, Aβ species, α-Synuclein, inflammatory mediators, neurotrophic fac-
tors, and other immunohistochemistry pathology metrics are utilized as response variables. This
translates to a high-dimensional supervised learning problem because the sample size is small
relative to the total number of genes involved in epigenetic processes available per subject, i.e.,
N ≪ P.
Among the extensively studied epigenetic processes are DNA and RNA methylation. DNA and
RNA methylation processes are regulated by three distinct families of effector proteins, each
with a specific role in regulating these epigenetic modifications (PerezGrovas-Saltijeral et al.,
2023). These families are commonly known as writers, readers, and erasers. The interplay among
the three protein families can potentially influence the regulation of transcription and translation
activity. Inturn, theseprocessesmayyieldcriticalimplicationsforgeneexpressionwithinvarious
brain regions and specific cellular environments.
A total of P⋆ = 139 DNA and RNA methylation regulators are filtered following the list provided
by PerezGrovas-Saltijeral et al. (2023). Given that there are N = 44 subjects and P = 140, then
it follows that the γ = P/N ≈ 3. Consequently, for the setting where N = 50 and γ = 3, Figure 4
showsthatbothtestsbasedonNestedLOOCVestimatorhaveaninflatedTypeIerror. However,
the tests based on Nested L2OCV estimator and Hybrid CV yields a test with a controlled Type
I error at level α = 0.05. Based on the simulation results that Nested LOOCV test can lead
to inflated empirical Type I error rate, it is not surprising there are are more rejections using
Nested LOOCV test compared to the other two tests. However, the proposed Hybrid CV method
provides meaningful findings while controlling the proportion of false positives.
Among four brain regions (HIP, PCx, FWM and TCx), the set of DNA and RNA methylation
effectors provide significant improvement in the prediction of the protein measurements listed in
Table 2. Results presented include the test statistic and corresponding p-value, lower bound,
estimate of the % change in prediction error in (42), denoted by ∆(cid:98) and the average λ(cid:98). The results
highlighted in blue are significant for parametric Nested LOOCV, Nested L2OCV, and Hybrid
CV tests. However, the results in red are only significant for parametric Nested LOOCV and
Hybrid CV tests.
In the hippocampal region, results from the Nested LOOCV, Nested L2OCV, and Hybrid CV
tests revealed that the DNA and RNA methylation effectors provide significant improvement in
the prediction of the Aβ 42/40 ratio. Meanwhile, in the inferior parietal lobule (PCx) and white
matter underlying the parietal neocortex (FWM) regions, the hypothesized features consisting
of DNA and RNA methylation regulators provide significant improvement in the prediction of
abnormallyphosphorylatedtauatthreonine181(p-Tau )usingNestedLOOCVandHybridCV
181
testsbutnottheNestedL2OCVtest. Incontrast,allthreetestsyieldedasignificantimprovement
in the prediction of p-Tau using the features related to epigenetic processes in the neocortex
181
from the posterior superior temporal gyrus (TCx) region. Accordingly, Table 2 also shows that
in the FWM and TCx regions, the prediction of the ratio of p-Tau to full length tau (t-Tau)
181
is significantly improved using the features being tested, using both Nested LOOCV and Hybrid
CV tests. Additionally, in the FWM region, the prediction of Interleukin-6 (pg/mg) marker is
32Region Test Protein Test Stat Pvalue Bound ∆(cid:98) Average λ(cid:98)
HIP NLOOCV Aβ 42/40 ratio 2.5304 0.00795 0.09936 29.04863 25.5102
IHC α-Synuclein 2.34875 0.01175 0.01075 3.69419 2488.98423
IHC Tau-2 3.40822 0.00077 0.00983 1.89491 2500
IHC AT8 2.69168 0.00512 0.01242 3.23439 2500
IHC Aβ 3.38638 0.00083 0.01562 3.03134 2500
p-Tau 3.65887 0.00040 0.01935 3.49555 2500
181
p-Tau /t-Tau 3.80172 0.00027 0.02018 3.53211 2500
181
Aβ40 2.06351 0.02317 0.00671 3.59303 2500
α-Synuclein 3.13046 0.00182 0.02014 4.25554 2500
MCP-1 1.86188 0.03564 0.00464 4.91346 2500
NL2OCV Aβ 42/40 ratio 1.85324 0.03603 0.02546 27.81392 25.5102
Hybrid Aβ 42/40 ratio 3.05936 0.00209 0.26201 28.43104 25.5102
PCx NLOOCV p-Tau 1.49062 0.07205 0.03412 25.52405 76.5306
181
Interleukin-6 1.69239 0.04927 0.00136 29.84369 25.5102
IHC α-Synuclein 2.07989 0.02184 0.00447 2.2824 2500
VEGF 2.76955 0.00427 0.01251 3.11346 2500
Aβ 42/40 ratio 1.74847 0.04413 0.00119 3.19823 2424.1071
RANTES 3.18163 0.00144 0.01903 3.94328 2500
α-Synuclein 3.35752 0.00092 0.02074 4.05918 2500
Interferon-γ 2.47755 0.00883 0.01227 3.73977 2348.8520
Interleukin-7 2.92719 0.00284 0.01508 3.46402 2500
Hybrid p-Tau 1.85845 0.03533 0.05544 28.92604 76.5306
181
FWM NLOOCV p-Tau 2.12519 0.02016 0.05400 25.50625 174.5435
181
Interleukin-6 2.00623 0.02609 0.05299 32.43694 24.16756
p-Tau /t-Tau 2.06917 0.02278 0.06338 33.41875 89.28571
181
Aβ40 2.07732 0.02238 0.00903 4.67862 2443.609
Interferon-γ 3.01184 0.00233 0.01475 3.26469 2500
Interleukin-7 2.44289 0.00973 0.01144 3.60101 2500
Aβ42 2.73782 0.00473 0.01431 3.62989 2500
Hybrid p-Tau 1.73546 0.04549 0.01550 27.06826 174.5435
181
Interleukin-6 2.31636 0.01309 0.19219 34.42891 24.1676
p-Tau /t-Tau 2.1279 0.02004 0.15277 35.88944 89.2857
181
TCx NLOOCV p-Tau 3.14023 0.00161 0.21409 45.03895 26.7857
181
p-Tau /t-Tau 2.75177 0.00461 0.14936 37.60344 27.5786
181
IHC α-Synuclein 2.04386 0.02330 0.00551 3.01487 2500
IHC Aβ 2.36499 0.01125 0.00560 1.8897 2500
IHC IBA1 2.30018 0.01306 0.00817 2.95978 2500
IHC GFAP 2.79772 0.00394 0.01566 3.83731 2500
TNFα 4.31445 0.00005 0.02536 4.05709 2500
Interleukin-6 1.70303 0.04816 0.00214 18.53766 85.2414
Interleukin-4 3.53531 0.00052 0.01832 3.41232 2500
RANTES 3.09919 0.00177 0.01675 3.57728 2500
Aβ42 2.51343 0.00810 0.01380 4.08012 2445.1531
NL2OCV p-Tau 1.95466 0.02891 0.06609 46.65463 26.7857
181
Hybrid p-Tau 3.32176 0.00098 0.46359 45.84705 26.7857
181
p-Tau /t-Tau 2.52299 0.00810 0.26615 39.12159 27.5786
181
Table 2: Protein-level alterations and Immunohistochemistry (IHC) pathology metrics
wherein the set of DNA and RNA methylation effectors provide significant improvement
in the prediction of the measurements among four brain regions.
33also significantly improved using the hypothesized epigenetics-related features, using both Nested
LOOCV and Hybrid CV tests.
ExistingliteratureshowsthatthepathophysiologyofAlzheimer’sdisease(AD)islinkedtoseveral
key factors, including the abnormal, progressive buildup of amyloid beta (Aβ) plaques and the
formationofneurofibrillarytangles(Qiuetal.,2015). Additionally, BraakandBraak(1991)men-
tionedthatinthecentralnervoussystemofindividualswithAlzheimer’sdisease, thefundamental
neuropathological characteristics revolve around the presence of extracellular senile plaques, pri-
marily comprised of Aβ peptides, and intracellular neurofibrillary tangles, which consist of a
hyperphosphorylated form of Tau (p-Tau) molecules. Hence, the group of Aβ peptides and p-Tau
molecules are considered to be the best-validated AD biomarkers (Lewczuk et al., 2021). On the
other hand, inflammatory mediators such as cytokines have a key role in AD linked-immunology
and pathogenesis (Rani et al., 2023). Amyloid-beta (Aβ) and amyloid precursor protein (APP)
induce cytokine and chemokine release from microglia, astrocytes, and neurons that induce the
release of various cytokines such as Interleukin-6 (IL-6) (Park et al., 2020). Moreover, Kummer
et al. (2021) asserted that the pleiotropic cytokine IL-6 is gaining recognition for its dual nature,
having both beneficial and destructive potentials. Its effects can diverge, either promoting neuron
survival following injury or instigating neurodegeneration and cell death in neurodegenerative or
neuropathic disorders.
Overall, results from the proposed Hybrid CV test revealed that DNA and RNA regulators as-
sociated belonging in three distinct families (i.e., writers, readers, erasers) can provide significant
reduction in the expected in-sample prediction error of the Aβ 42/40 ratio, p-Tau , p-Tau /t-
181 181
Tauratio, andIL-6marker, amongsubjectswhoexperiencedtraumaticbraininjury. Ourmethod
produced results that support the current findings because these proteins are highly implicated
biomarkers in the progression of AD. In addition, our method also produced novel findings. To
the best of our knowledge, this has not been reported in the literature. These novel findings are
insightfulbecausetheitprovidesevidenceregardingtheinvolvementofepigeneticprocessesinthe
pathophysiology of AD, particularly on subjects who already experienced TBI. These certainly
warrant further investigation because targeting these underlying epigenetic processes opens up
novel therapeutic avenues for treating neurodegenerative brain conditions.
6 Conclusions and Future Work
In this study, we primarily develop a reproducible and scalable framework for predictive perfor-
mance testing in high-dimensional data analysis. Our approach is based on an exhaustive nested
cross-validation estimator and the proposed test yields superior empirical power while maintain-
ing the Type I error. The rationale for utilizing an exhaustive method is to address the instability
and reproducibility concerns associated with K-fold cross-validation by exploring all possible par-
titions of the original sample into training and testing sets. Moreover, nested cross-validation is
utilized for both the estimation of the optimal regularization parameter and the prediction error.
Hence, the proposed high-dimensional test based on exhaustive nested cross-validation captures
both model selection and assessment and provides a reproducible test decision.
The significant contributions in this study are summarized as follows. Firstly, we tackle a limita-
tion often observed in existing black box tests. Instead of solely concentrating on the importance
of individual variables or single feature inferences, we assess the collective behavior of hypoth-
esized features. Secondly, we address the significant computational demands in generating the
reference distribution of the test statistic under the null hypothesis, which is prominent in permu-
tation tests. Under the null, the test statistic is characterized without the features being tested
and this is used as a benchmark to compare the predictive performance of ridge regression with
the hypothesized features. Thirdly, we address the computational burden of repeatedly refitting
the model. We accomplish this by deriving a computationally feasible closed-form expression for
34the cross-validation estimator. To the best of our knowledge, our study marks the pioneering
implementation of cross-validation as a test statistic for high-dimensional testing. Our proposed
method stands out because of its ease of application among various settings with continuous re-
sponse variables. Furthermore, valid confidence intervals can be generated for the difference in
prediction error between models with and without the features of interest.
Consequently,thisworkcanbeextendedtothegeneralizedlinearmodelsettingwheretheresponse
variablesarecontinuousbuthavenon-normaldensities(e.g.,exponential,gamma)ordiscrete(e.g.,
binomial, Poisson). Another potential research work is to examine the algorithmic stability of the
exhaustive nested cross-validation estimator and investigate the implications to high-dimensional
testing. Lastly, selective inference after implementing cross-validation is left as future work.
A Proof of Lemma 1:
Let {(X ,Y )}N be independent and identically distributed draws from some joint distribution
n n n=1
Q(Y | f(X ,β))P(X ). Suppose the full model is y = f(X,β)+ε parameterized by β ∈ RP
n n n
wherey isthevectorofobservedresponses, f isalinearfunctionofthedesignmatrixX ∈ RN×P,
E(ε) = 0, V(ε) = σ2I , and ε ⊥⊥ X. Let H(λ) = X(X⊤X+λI )+X⊤ be the N ×N projection
ε P P
matrix which describes the influence each response value has on each fitted value (Hoaglin and
Welsch, 1978). Also, let r = (I −H(λ))y be the N ×1 vector of residuals.
N
Suppose N > 0 and define T = {ℓ(h)}N0 as the ℓth collection of ordered indices corresponding
0 0ℓ h=1
to sampling units assigned in the testing data, where ℓ(1) < ℓ(2) < ··· < ℓ(N ), ℓ(h) ∈ [N],h ∈
0
[N ], and ℓ ∈ [L]. Similarly, let T = [N]\T = {ℓ(h)}N be the ℓth set of ordered indices
0 1ℓ 0ℓ h=N0+1
among sampling units assigned to the training data, with ℓ(N +1) < ··· < ℓ(N), ℓ(h) ∈ [N],h ∈
0
{N +1,...,N}, and ℓ ∈ [L].
0
Using the N ×N matrix E in (1), the following expressions can be defined:
0 T
0ℓ
H (λ) = [E ]⊤H(λ)[E ] (55)
T T T
0ℓ 0ℓ 0ℓ
R (λ) = [I −H(λ)]⊤[E ] (56)
T N T
0ℓ 0ℓ
I −H (λ) = [E ]⊤[I −H(λ)][E ] = [R ]⊤[E ] (57)
N0 T
0ℓ
T
0ℓ
N T
0ℓ
T
0ℓ
T
0ℓ
where (57) can be viewed as
 
1−[H(λ)] −[H(λ)] ··· −[H(λ)]
ℓ(1)ℓ(1) ℓ(1)ℓ(2) ℓ(1)ℓ(N0)
 −[H(λ)] 1−[H(λ)] ··· −[H(λ)] 
I N0 −H T 0ℓ(λ) =  

. .
.
ℓ(1)ℓ(2)
. .
.
ℓ(2)ℓ(2)
... . .
.ℓ(2)ℓ(N0)
  .
−[H(λ)] −[H(λ)] ··· 1−[H(λ)]
ℓ(1)ℓ(N0) ℓ(2)ℓ(N0) ℓ(N0)ℓ(N0)
From (55),H (λ) is the N ×N symmetric matrix with diagonal elements corresponding to the
T 0 0
0ℓ
diagonal elements in H(λ) whose indices belong to T while the off-diagonal elements correspond
0ℓ
to the N (N −1)/2 off-diagonal elements in H(λ) whose indices are pairwise configurations of
0 0
T . Using (56), define
0ℓ
r (λ) = [E (λ)]⊤r = [E ]⊤[I −H(λ)]y = [R (λ)]⊤y
T T T N T
0ℓ 0ℓ 0ℓ 0ℓ
as the N ×1 vector of residuals corresponding to the subjects whose indices belong in T .
0 0ℓ
Furthermore, define q (λ) as the N ×1 vector of weighted residuals where each element of the
T 0
0ℓ
vector can be viewed as

q (λ) =
 Y ℓ(h)−X⊤ ℓ(h)β(cid:98)
T
1ℓ(λ) =
(cid:80)N0
ω ℓ(h)(λ)r ℓ(h)(λ), N
0
> 0
(58)
ℓ(h) h=1
 Y ℓ(h)−X⊤ ℓ(h)β(cid:98) R(λ) = r ℓ(h)(λ), N
0
= 0
35given a fixed λ ∈ Λ, ℓ(h) ∈ [N]. From (58), it can be seen that instead of refitting the model
to obtain β(cid:98) (λ) from the ℓth training data {D }N , each term q (λ) can be computed
T 1ℓ ℓ(h) h=N0+1 ℓ(h)
as a linear combination of some specified weights {ω (λ)}N0 and the residuals {r (λ)}N0
ℓ(h) h=1 ℓ(h) h=1
computed using the entire data set {D }N . In the classical regression setting where all the
ℓ(h) h=1
available data is used to fit the model and N = 0, the weights in (58) are set to 1. This means
0
that q (λ) reduces to the residual r (λ),ℓ(h) ∈ [N],ℓ ∈ [L].
ℓ(h) ℓ(h)
Alternatively, q (λ) can be expressed as
T
0ℓ
q (λ) = y −X (X⊤ X +λI )+X⊤ y
T 0ℓ T 0ℓ T 0ℓ T 1ℓ T 1ℓ P T 1ℓ T 1ℓ
= [E ]⊤y−[E ]⊤X(X⊤[E ][E ]⊤X+λI )+X⊤[E ][E ]⊤y
T T T T P T T
0ℓ 0ℓ 1ℓ 1ℓ 1ℓ 1ℓ
(cid:104) (cid:105)
= [E ]⊤ I −X(X⊤[E ][E ]⊤X+λI )+X⊤[E ][E ]⊤ y. (59)
T N T T P T T
0ℓ 1ℓ 1ℓ 1ℓ 1ℓ
(cid:124) (cid:123)(cid:122) (cid:125)
IN−MT1ℓ(λ)
Suppose I −H (λ) is invertible. The weighted residual q (λ) can be re-expressed as
N0 T
0ℓ
T
0ℓ
q (λ) = [E ]⊤[I −M (λ)]y
T T N T
0ℓ 0ℓ 1ℓ
= [I −H (λ)]−1[I −H (λ)][E ]⊤[I −M (λ)]y
N0 T
0ℓ
N0 T
0ℓ
T
0ℓ
N T
1ℓ
= [I −H (λ)]−1[U (λ)]⊤y (60)
N0 T
0ℓ
T
0ℓ
where
[M (λ)]⊤ = [E ][E ]⊤X(X⊤[E ][E ]⊤X+λI )+X⊤
T T T T T P
1ℓ 1ℓ 1ℓ 1ℓ 1ℓ
and
U (λ) = [I −M (λ)]⊤[E ][I −H (λ)]
T
0ℓ
N T
1ℓ
T
0ℓ
N0 T
0ℓ
= [I −M (λ)]⊤[E ][E ]⊤[I −H(λ)][E ].
N T T T N T
1ℓ 0ℓ 0ℓ 0ℓ
To simplify U (λ), some preliminaries involve simplifying the block matrix representation of
T
0ℓ
I −[M (λ)]⊤ and [E ][E ]⊤[I −H(λ)][E ]. These can be written as
N T T T N T
1ℓ 0ℓ 0ℓ 0ℓ
(cid:32) (cid:33)
I 0
I
N
−[M
T
(λ)]⊤ =
N0
N0×N1
1ℓ −X (X⊤ X +λI )+X⊤ I −X (X⊤ X +λI )+X⊤
T 1ℓ T 1ℓ T 1ℓ P T 0ℓ N1 T 1ℓ T 1ℓ T 1ℓ P T 1ℓ
(cid:18) [E ]⊤[I −H(λ)][E ] −[E ]⊤[H(λ)][E ] (cid:19)
T N T T T
I −H(λ) = 0ℓ 0ℓ 1ℓ 0ℓ
N −E⊤ H(λ)E [E ]⊤[I −H(λ)][E ]
T 1ℓ T 0ℓ T 1ℓ N T 1ℓ
and
(cid:32) (cid:33)
[I −H(λ)]⊤[E ] = (cid:18) [E T 0ℓ]⊤[I N −H(λ)][E T 0ℓ] −[E T 1ℓ]⊤[H(λ)][E T 0ℓ] (cid:19) I N0
N T 0ℓ −E⊤ H(λ)E [E ]⊤[I −H(λ)][E ] 0
T 1ℓ T 0ℓ T 1ℓ N T 1ℓ N1×N0
(cid:18) [E ]⊤[I −H(λ)][E ](cid:19)
T N T
= 0ℓ 0ℓ . (61)
−E⊤ H(λ)E
T 1ℓ T 0ℓ
36It follows that [E ][E ]⊤[I −H(λ)][E ] can be expanded as
T T N T
0ℓ 0ℓ 0ℓ
=
(cid:18) I N0(cid:19) (cid:0)
I
0(cid:1)(cid:18) [E T 0ℓ]⊤[I N −H(λ)][E T 0ℓ] −[E T 1ℓ]⊤[H(λ)][E T 0ℓ] (cid:19)(cid:18) I N0(cid:19)
0 N0 −E⊤ H(λ)E [E ]⊤[I −H(λ)][E ] 0
T 1ℓ T 0ℓ T 1ℓ N T 1ℓ
(cid:18) I 0(cid:19)(cid:18) [E ]⊤[I −H(λ)][E ] −[E ]⊤[H(λ)][E ] (cid:19)(cid:18) I (cid:19)
= N0 T 0ℓ N T 0ℓ T 1ℓ T 0ℓ N0
0 0 −E⊤ H(λ)E [E ]⊤[I −H(λ)][E ] 0
T 1ℓ T 0ℓ T 1ℓ N T 1ℓ
(cid:18) [E ]⊤[I −H(λ)][E ] −[E ]⊤[H(λ)][E ](cid:19)(cid:18) I (cid:19)
= T 0ℓ N T 0ℓ T 1ℓ T 0ℓ N0
0 0 0
(cid:32) (cid:33)
I −H
=
N0 T
0ℓ .
0
N1×N0
Using the above mentioned results, U (λ) can be stated as
T
0ℓ
(cid:32) (cid:33)(cid:32) (cid:33)
I 0 I −H
U
T
(λ) =
N0
N0×N1
N0
0
T
0ℓ
0ℓ −X (X⊤ X +λI )+X⊤ I −H
N×N0 T 1ℓ T 1ℓ T 1ℓ P T 0ℓ N1 T 1ℓ N1×N0
(cid:18) (cid:19)
I −H
=
N0 T
0ℓ . (62)
−X (X⊤ X +λI )+X⊤ [I −H ]
T 1ℓ T 1ℓ T 1ℓ P T 0ℓ N0 T 0ℓ
Now, X⊤X+λI = X⊤ X +X⊤ X +λI and
P T 0ℓ T 0ℓ T 1ℓ T 1ℓ P
I = (X⊤ X +X⊤ X +λI )(X⊤X+λI )+
P T 0ℓ T 0ℓ T 1ℓ T 1ℓ P P
= (X⊤ X )(X⊤X+λI )++(X⊤ X +λI )(X⊤X+λI )+
T 0ℓ T 0ℓ P T 1ℓ T 1ℓ P P
which leads to
I −(X⊤ X )(X⊤X+λI )+ = (X⊤ X +λI )(X⊤X+λI )+. (63)
P T 0ℓ T 0ℓ P T 1ℓ T 1ℓ P P
Expanding X (X⊤ X +λI )+X⊤ [I −H ] in (62) and using (63) leads to
T 1ℓ T 1ℓ T 1ℓ P T 0ℓ N0 T 0ℓ
X (X⊤ X +λI )+X⊤ −X (X⊤ X +λI )+X⊤ X (X⊤X+λI )+X⊤
T 1ℓ T 1ℓ T 1ℓ P T 0ℓ T 1ℓ T 1ℓ T 1ℓ P T 0ℓ T 0ℓ P T 0ℓ
= X (X⊤ X +λI )+[I −X⊤ X (X⊤X+λI )+]X⊤
T 1ℓ T 1ℓ T 1ℓ P P T 0ℓ T 0ℓ P T 0ℓ
= X (X⊤ X +λI )+(X⊤ X +λI )(X⊤X+λI )+X⊤
T 1ℓ T 1ℓ T 1ℓ P T 1ℓ T 1ℓ P P T 0ℓ
= X (X⊤X+λI )+X⊤
T 1ℓ P T 0ℓ
= E⊤ X(X⊤X+λI )+X⊤E
T 1ℓ P T 0ℓ
= E⊤ H(λ)E .
T 1ℓ T 0ℓ
Therefore,
(cid:18) [E ]⊤[I −H(λ)][E ](cid:19)
U (λ) = T 0ℓ N T 0ℓ = [I −H(λ)]⊤[E ] (64)
T 0ℓ −E⊤ H(λ)E N T 0ℓ
N×N0 T 1ℓ T 0ℓ
usingtheresultin(61). TheℓthterminLN OCV(1)(λ)inLemma1canbewrittenasq⊤ (λ)q (λ).
0 T 0ℓ T 0ℓ
Finally, it follows from (59), (60), and (64) that
q⊤ (λ)q (λ) = y⊤[I −M (λ)]⊤[E ][E ]⊤[I −M (λ)]y
T 0ℓ T 0ℓ N T 1ℓ T 0ℓ T 0ℓ N T 1ℓ
= y⊤[U (λ)][I −H (λ)]−2[U (λ)]⊤y
T
0ℓ
N0 T
0ℓ
T
0ℓ
= y⊤[I −H(λ)]⊤[E ][I −H (λ)]−2[E ]⊤[I −H(λ)]y
N T
0ℓ
N0 T
0ℓ
T
0ℓ
N
(cid:104) (cid:105)−2
= y⊤R R⊤ (λ)E R⊤ y
T 0ℓ T 0ℓ T 0ℓ T 0ℓ
= y⊤W (λ)y
T
0ℓ
37where W (λ) = [R (λ)][R⊤ (λ)E ]−2[R (λ)]⊤ is an N ×N matrix of weights which is a
T 0ℓ T 0ℓ T 0ℓ T 0ℓ T 0ℓ
function of the design matrix X and the regularization parameter λ ∈ Λ. Equivalently, this can
be re-expressed as
q⊤ (λ)q (λ) = r⊤[E ][I −H (λ)]−2[E ]⊤r
T 0ℓ T 0ℓ T 0ℓ N0 T 0ℓ T 0ℓ
= r⊤ [I −H (λ)]−2r . ■
T 0ℓ N0 T 0ℓ T 0ℓ
B Proof of Corollary 1:
Let Y ,Y ,...,Y be independent and identically distributed draws from some distribution P.
1 2 N
Suppose the null model corresponds to the intercept only model which is given by y = β 1 +ε
0 N
where ε is the unobserved errors with zero mean and variance σ2,n ∈ [N].
n ε
Define Y(cid:101)n = Y
n
− Y,n ∈ [N] or the alternative expression Y(cid:101)ℓ(h) = Y
ℓ(h)
− Y,ℓ(h) ∈ [N]. The
sample mean Y of y , the vector of N = N −N ,N > 0, observed responses among the
T T 1 0 0
1ℓ 1ℓ
sampling units assigned in the ℓth training data, {Y }N is given by
ℓ(h) h=N0+1
1
(cid:88)N
1
(cid:32) (cid:88)N (cid:88)N0 (cid:33)
Y = Y = Y − Y (65)
T 1ℓ N −N ℓ(h) N −N ℓ(h) ℓ(h)
0 0
h=N0+1 h=1 h=1
1
(cid:88)N0
1
(cid:88)N0 (cid:18)
N
0
(cid:19)
= Y −
N −N
Y(cid:101)ℓ(h) = Y −
N N −N
Y(cid:101)ℓ(h). (66)
0 0 0
h=1 h=1
The quantity in (65) can be viewed in (66) as the sample mean for the entire response vector
Y minus the weighted mean of the centered response measurements among the sampling units
assigned to the testing data. The weight used in (66) is the ratio between the number of sampling
units in the testing data and the number of sampling units in the training data.
For every sampling unit in the ℓth partition of the testing data, ℓ(h) ∈ [N], the squared differ-
ence between the observed response and training data sample mean, denoted by (Y −Y )2,
ℓ(h) T 1ℓ
simplifies to
Y(cid:101) ℓ2 (h)+
(N
−1
N )2
(cid:32) (cid:88)N0 Y(cid:101)ℓ(h)(cid:33)2
+
N2Y(cid:101)
−ℓ( Nh)
(cid:88)N0
Y(cid:101)ℓ(h)
0 0
h=1 h=1
and the average across all sampling units in the testing data can be viewed as
1
(cid:88)N0
1
(cid:88)N0
1
(cid:18)
N
(cid:19)(cid:32) (cid:88)N0 (cid:33)2
N
(Y ℓ(h)−Y
T
1ℓ)2 =
N
Y(cid:101) ℓ2 (h)+
N (N −N ) N −N
+1 Y(cid:101)ℓ(h) . (67)
0 0 0 0 0
h=1 h=1 h=1
The estimator of Leave-N -out Cross-validation (LN OCV) using Y is the average of (67)
0 0 T
1ℓ
across L partitions of the testing data given by
1
(cid:88)L (cid:88)N0
1
(cid:88)L (cid:34)
1
(cid:88)N0 (cid:35)
LN OCV(0) := (Y −Y )2 = (Y −Y )2
0 LN ℓ(h) T 1ℓ L N ℓ(h) T 1ℓ
0 0
ℓ=1h=1 ℓ=1 h=1
1
(cid:88)L

(cid:88)N0
1
(cid:18)
N
(cid:19)(cid:32) (cid:88)N0
(cid:33)2
=
LN
 Y(cid:101) ℓ2 (h)+
N −N N −N
+1 Y(cid:101)ℓ(h) 
0 0 0
ℓ=1 h=1 h=1
38This can be expressed as
1
(cid:18) N(cid:19)−1 (cid:88)L (cid:88)N0
1
(cid:18)
N
−1(cid:19)−1(cid:18)
1 1
(cid:19) (cid:88)L (cid:32) (cid:88)N0 (cid:33)2
LN 0OCV(0) =
N N
Y(cid:101) ℓ2 (h)+
N N N −N
+
N
Y(cid:101)ℓ(h)
0 0 0 0 0
ℓ=1h=1 ℓ=1 h=1
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
(I) (II)
= C (I)+C (II)
1 2
ItisclearthatthereareLN termsineachofthedoublesumsin(I)and(II). Uponinvestigation
0
of the term (I) closely, the quantity (Y −Y)2 will appear in several partitions of the testing data
n
(cid:18) (cid:19)
N N
among L possible partitions. Specifically, the quantity (Y −Y)2,n ∈ [N] appears 0 =
n
N N
0
(cid:18) (cid:19) (cid:18) (cid:19)
N −1 N −1
= times in the sum (I).
N −N N −1
0 0
For example, suppose N = 5, N = 2 and T = {ℓ(1),ℓ(2)}, ℓ(1) < ℓ(2), ℓ = 1,2,...,10. The ten
0 0ℓ
possible combinations of indices for the test-train data splits can be shown as follows: From the
illustration above, there are LN = 10(2) = 20 terms in (I) and (II). In particular, the quantity
0
(Y −Y)2 appears N −1 = 4 times in the sum (I) for any n ∈ [N]. The quantity in (I) simplifies
n
to
(cid:88)L (cid:88)N0 (cid:18)
N −1
(cid:19) (cid:88)N
(I) = Y(cid:101) ℓ2
(h)
=
N −N
(Y n−Y)2.
0
ℓ=1h=1 n=1
Following the same argument, it follows that
(cid:88)L (cid:32) (cid:88)N0 (cid:33)2 (cid:18)
N
−2(cid:19) (cid:88)N
(II) = Y(cid:101)ℓ(h) =
N −1
(Y n−Y)2.
0
ℓ=1 h=1 n=1
This means that the quantity in C (I) can be simplified as
1
1
(cid:18) N(cid:19)−1 (cid:88)L (cid:88)N0
1
(cid:18) N(cid:19)−1(cid:18)
N −1
(cid:19) (cid:88)N
N N
Y(cid:101) ℓ2
(h)
=
N N N −N
(Y n−Y)2
0 0 0 0 0
ℓ=1h=1 n=1
(cid:20) (cid:21) N
=
N 0!(N −N 0)! (N −1)! (cid:88)
(Y −Y)2
n
N!N (N −N )!(N −1)!
0 0 0
n=1
N
1 (cid:88)
= (Y −Y)2 (68)
n
N
n=1
whereas C (II) can be expressed as
2
(cid:18) (cid:19) (cid:18) (cid:19)
1 1 1 N −2
N
0
(cid:18)N N− −N
10
(cid:19)+
N
(cid:88)L (cid:32) (cid:88)N0 Y(cid:101)ℓ(h)(cid:33)2
=
(cid:18)N
N0
−− 11
(cid:19)
N1
(cid:18)
N
−1
N
+
N1
(cid:19) (cid:88)N
(Y n−Y)2
0 0
ℓ=1 h=1 n=1
N N
0 0
(cid:18) (cid:19) N
1 1 1 (cid:88)
= + (Y −Y)2 (69)
n
N −1 N −N N
0
n=1
39Finally, putting (68) and (69) together completes the proof where
N (cid:18) (cid:19) N
1 (cid:88) 1 1 1 (cid:88)
LN OCV(0) = (Y −Y)2+ + (Y −Y)2
0 n n
N N −1 N −N N
0
n=1 n=1
(cid:20) (cid:18) (cid:19)(cid:21) N
1 1 1 (cid:88)
= + (Y −Y)2
n
N −1 N −1 N −N
0
n=1
(cid:18) (cid:19) N
=
1 N −N 0+1 (cid:88)
(Y −Y)2
n
N −1 N −N
0
n=1
(cid:18) (cid:19)
1 N −N +1
= 0 (N −1)S2
N −1 N −N Y
0
(cid:18) (cid:19)
1
= 1+ S2 ■ (70)
N −N Y
0
C Asymptotic Results for Leave-N -out Cross-Validation
0
Following assumptions described in Wager (2020),
A1: Suppose (X ,Y ) ∈ RP ×R is a sequence of independent and identically distributed samples
n n
with E(Y4) < ∞ and V(Y | X ) ≤ Σ < ∞.
n n n
A2: Given an algorithm M , there are constants 0 < c ≤ c < ∞ and 0.25 < δ < 0.5 such that
1 − +
the excess risk of f(cid:98)(X ,β(cid:98) ) evaluated on a testing data point scales as
ℓ(h) T
1ℓ
(cid:34) (cid:20) (cid:21)1 (cid:35)
(cid:16) (cid:17)2 2
Nl →im ∞P N 1δE f(cid:98)(X ℓ(h),β(cid:98)
T
1ℓ)−f(X ℓ(h)) | {X ℓ(h),Y ℓ(h)}N
h=N0+1
≤ c
−
= 0,
(cid:34) (cid:20) (cid:21)1 (cid:35)
(cid:16) (cid:17)2 2
Nl →im ∞P N 1δE f(cid:98)(X ℓ(h),β(cid:98)
T
1ℓ)−f(X ℓ(h)) | {X ℓ(h),Y ℓ(h)}N
h=N0+1
≤ c
+
= 1
where X is the set of features from a randomly selected observation in the testing
ℓ(h)
set, drawn independently from the collection of observations belonging to the training set
{X ,Y }N .
ℓ(h) ℓ(h) h=N0+1
Define Err(1) = E[(Y −f(X ))2] and σ2 = V[(Y −f(X ))2]. A direct consequence of A.1 is
n n CV1 n n
N
1 (cid:88)
E(CV ) = E[(Y −f(X ))2] = Err(1),
1 n n
N
n=1
V(CV ) =
1 (cid:88)N
V[(Y −f(X ))2] =
σ C2
V1.
1 N2 n n N
n=1
Using the decomposition in (30) and the simplification of CV in (31), the simplification leads to
1
√ (cid:16) (cid:17)
N CV −Err(1) →d N(0, σ2 ), as N → ∞. (71)
1 CV1
Moreover, given the scaling in A.2, it follows from Markov’s Inequality that
CV 2(M 1) p
→ 0. (72)
N2δ
40Now, suppose Y
ℓ(h)
−f(X ℓ(h)) and Y
ℓ h⋆
−f(X
ℓ
h⋆) are pairwise uncorrelated given f(cid:98)(·) for any
observations h, h⋆ belonging in the same testing set T . By construction, it can be shown that
0ℓ
for all h ∈ [N 0], E[Y ℓ(h)−f(X ℓ(h)) | X ℓ(h),f(cid:98)(X ℓ(h),β(cid:98)
T
)] = 0. Define
1ℓ
(cid:88)N0
CV
3ℓ
= (Y ℓ(h)−f(X ℓ(h)))(f(X ℓ(h))−f(cid:98)(X ℓ(h),β(cid:98)
T
))
1ℓ
h=1
(cid:104) (cid:105)
The statements above together with assumptions A.1 and A.2 yield E CV 3ℓ|f(cid:98)(·) = 0 for all
ℓ = 1,2,...,L and
L
1 (cid:88) (cid:104) (cid:105)
E[CV 3(M 1) | f(cid:98)(·)] = E CV 3ℓ|f(cid:98)(·) = 0.
LN
0
ℓ=1
Another consequence of the Cauchy-Schwarz inequality, and Assumptions (1) and (2) is that
(cid:104)(cid:16) (cid:12) (cid:105)
(cid:88)N0
(cid:104) (cid:12) (cid:105)2
E CV 3ℓ)2(cid:12) (cid:12)f(cid:98)(·) ≤ N
0
E (Y ℓ(h)−f(X ℓ(h)))(f(X ℓ(h))−f(cid:98)(X ℓ(h),β(cid:98)
T
))(cid:12) (cid:12)f(cid:98)(·)
1ℓ
h=1
= N
0(cid:88)N0
E(cid:2) (Y ℓ(h)−f(X
ℓ(h)))(cid:3)2E(cid:104)
(f(X ℓ(h))−f(cid:98)(X ℓ(h),β(cid:98)
T
))(cid:12)
(cid:12)
(cid:12)f(cid:98)(·)(cid:105)2
1ℓ
h=1
c2
≤ N2Σ +
0 N2δ
1
which yields
L
E[CV2 3(M 1) | f(cid:98)(·)] = LN1 (cid:88) E(cid:104) CV2 3k(cid:12) (cid:12)f(cid:98)(·)(cid:105) ≤ NN 20 δc2 +Σ
0 ℓ=1 1
and
(cid:20) c2Σ(cid:21) E[CV (M )] (cid:18) N (cid:19)2δ
P CV2(M ) ≥ + ≤ 3 1 ≤ N . (73)
3 1 N2δ c2Σ 0 N −N
+ 0
Therefore, there exists N such that for all N ≥ N ,
0 0
CV (M )
3 1 ≤ c⋆
N−0.5−δ
and CV (M ) = O
(cid:0) N−(0.5+δ)(cid:1)
. From these results, it can be verified further that CV (M )
3 1 p 3 1
decays faster than the CV (M ) for 0.25 < δ < 0.5.
2 1
Finally,ifCV convergesindistributiontoanormalrandomvariableandCV (M )andCV (M )
1 2 1 3 1
convergesinprobabilityto0,thenbySlutsky’sTheorem,LN OCV(M )convergesindistribution
0 1
toanormalrandomvariable,asN → ∞. Formally,putting(71),(72)and(73)together,itfollows
that
√
N(LN OCV(M )−Err(1)) →d N(0,σ2 ), as N → ∞. (74)
0 1 CV1
On the other hand, the moments of LN OCV(M ) are
0 0
(cid:18) (cid:19) (cid:18) (cid:19)
1 N −N +1
E(LN OCV(M )) = σ2 1+ = σ2 0 = Err(0) (75)
0 0 ε N −N ε N −N
0 0
1
(cid:20) (cid:18)
N
−3(cid:19) (cid:21)(cid:18)
1
(cid:19)2
V(LN OCV(M )) = µ − σ4 1+ (76)
0 0 N 4 N −1 ε N −N
0
where µ = E[(Y −E(Y ))4] < ∞ is the fourth central moment. If Y , n ∈ [N] are identically and
4 n n n
independently normally distributed with mean 0, variance σ2 and E(Y4) < ∞, then the variance
ε n
in (76) reduces to
2σ4 (cid:18) 1 (cid:19)2
V(LN OCV(M )) = ε 1+
0 0
N −1 N −N
0
41Using the normality assumption of Y , it is clear that LN OCV(M ) follows the Gamma distri-
n 0 0
bution with shape and rate parameters as follows:
(cid:20) (cid:21)
N −1 1 (N −1)(N −N ) shape(N −N )
0 0
shape = , and rate = = .
2 2σ2 N −N +1 σ2(N −N +1)
ε 0 ε 0
When N is sufficiently large
(cid:32) (cid:33)
√ 2σ4 (cid:18) N −N +1(cid:19)2
N(LN OCV(M )−Err(0)) →d N 0, σ2 = ε 0 . ■
0 0 CV0 N −1 N −N
0
D Description of the test based on K-fold Cross-Validation
Thedetailsofthehigh-dimensionaltestbasedontheK-foldcross-validationestimatorispresented
in this section. Using the liver toxicity data by Bushel et al. (2007), the goal is to determine
whether the 3116 gene expression measurements in X provide substantial improvement in the
prediction of the clinical measurement y. In K-fold cross-validation, the randomization of the
k = 1 k = 2 k = 3 k = 4 k = 5 Testing Set Indices
52,53,54,55
56,57,58,59
60,61,62,63
64
39,40,41,42
43,44,45,46
47,48,49,50
51
26,27,28,29
30,31,32,33
34,35,36,37
38
13,14,15,16
17,18,19,20
21,22,23,24
25
1,2,3,4
5,6,7,8
9,10,11,12
Figure 7: Illustration of the five-fold cross-validation: After randomizing the N = 64 in-
dices, the the testing set indices are determined using K = 5 folds. The measurements from
the rats allocated into four folds (blue) are used as the training data while the remaining
measurements from the rats in the kth fold (red) is used as the testing data.
N indices of all sampling units is performed once, which then leads to the assignment of each
sampling unit to exactly one fold. To illustrate, suppose K = 5 and the N = 64 rats are
42labeled from 1 to 64. Consider the randomized set of indices as the decreasing sorted indices
{64,63,62,...,3,2,1}. This means that the 1st fold contains the 52nd until the 64th rats, the
2nd fold contains the 39th to the 51st rats, and so on until the last fold which contains the 1st to
the 12th rats. The visualization of the procedure is presented in Figure 7. The full linear model
y = Xβ+ε is compared to the intercept-only model y = β 1 +ε, both with the usual model
0 N
assumptions on ε.
The training data, represented by folds in blue, is used to obtain the predictive rules f(cid:98)0 and
f(cid:98)1 generated by the model-fitting algorithms M
0
and M
1
corresponding to the intercept-only
and full model, respectively. The predictive rule f(cid:98)0 reduces to the sample mean of the response
measurements in the training data, denoted by Y −k. Likewise, the predictive rule f(cid:98)1 is a function
of the ridge regression estimator computed without the measurements from the sampling units
assigned to the k-th fold, denoted by β(cid:98) (λ(cid:98)) where λ(cid:98) is obtained using the cv.glmnet function
−k
in the R package glmnet for a specified K (Friedman et al., 2010). For every sampling unit in the
kth testing data, the following quantities are calculated:
N
C(0) =
1 (cid:88)k
(Y −Y )2 (77)
k N k(h) −k
k
h=1
N
C(1) =
1 (cid:88)k
(Y −X⊤ β(cid:98) (λ(cid:98)))2 (78)
k N k(h) k(h) −k
k
h=1
This procedure is repeated until all folds are used as testing data, k ∈ [K]. Using (77) and (78),
(0) (0)
the paired differences C = C −C are also computed as well as the average paired differences
k k k
K K
1 (cid:88) 1 (cid:88)(cid:16) (0) (1)(cid:17)
C = C = C −C .
K k K k k
k=1 k=1
The test statistic using the sth random seed is obtained using the paired differences of the K-fold
cross-validation estimator. More rigorously, it is computed as follows
C
T = . (79)
KCV,s (cid:118)
(cid:117) K
(cid:117) 1 (cid:88)(cid:0) (cid:1)2
(cid:116) C −C
k
K −1
k=1
For any of the random seeds, the null hypothesis is rejected if the test statistic in (79) is greater
than the critical value t (α). Using both K = 5 and K = 10, the results for the response
df=K−1
variables Creatinine and Alkaline Phosphatase (ALP) are presented in Section 1 to emphasize the
issues regarding the reproducibility of the test based on the K-fold cross-validation.
References
Aliferis, C. F., Statnikov, A., and Tsamardinos, I. (2006). Challenges in the analysis of mass-
throughput data: a technical commentary from the statistical machine learning perspective.
Cancer Informatics, 2:117693510600200004.
Allen, D. M. (1974). The relationship between variable selection and data agumentation and a
method for prediction. Technometrics, 16(1):125–127.
Anguita, D., Ghio, A., Oneto, L., and Ridella, S. (2012). In-sample and out-of-sample model
selection and error estimation for support vector machines. IEEE Transactions on Neural
Networks and Learning Systems, 23(9):1390–1406.
43Arlot, S. and Celisse, A. (2010). A survey of cross-validation procedures for model selection.
Statistics Surveys, 4(none):40 – 79.
Austern, M. and Zhou, W. (2020). Asymptotics of cross-validation. arXiv preprint
arXiv:2001.11111.
Bartlett, P. L., Long, P. M., Lugosi, G., and Tsigler, A. (2020). Benign overfitting in linear
regression. Proceedings of the National Academy of Sciences, 117(48):30063–30070.
Bates, S., Hastie, T., and Tibshirani, R. (2023). Cross-validation: what does it estimate and how
well does it do it? Journal of the American Statistical Association, pages 1–12.
Bayle, P., Bayle, A., Janson, L., and Mackey, L. (2020). Cross-validation confidence intervals for
test error. Advances in Neural Information Processing Systems, 33:16339–16350.
Bengio, Y. and Grandvalet, Y. (2003). No unbiased estimator of the variance of k-fold cross-
validation. Advances in Neural Information Processing Systems, 16.
Bickel, P. J., Li, B., Tsybakov, A. B., van de Geer, S. A., Yu, B., Vald´es, T., Rivero, C., Fan, J.,
and van der Vaart, A. (2006). Regularization in statistics. Test, 15:271–344.
Blennow, K., Hardy, J., and Zetterberg, H. (2012). The neuropathology and neurobiology of
traumatic brain injury. Neuron, 76(5):886–899.
Borra, S. and Di Ciaccio, A. (2010). Measuring the prediction error. a comparison of cross-
validation, bootstrap and covariance penalty methods. Computational statistics & data
analysis, 54(12):2976–2989.
Braak, H. and Braak, E. (1991). Neuropathological stageing of alzheimer-related changes. Acta
neuropathologica, 82(4):239–259.
Bushel,P.R.,Wolfinger,R.D.,andGibson,G.(2007). Simultaneousclusteringofgeneexpression
data with clinical chemistry and pathological evaluations reveals phenotypic prototypes. BMC
Systems Biology, 1(1):1–20.
Cawley, G. C. and Talbot, N. L. (2008). Efficient approximate leave-one-out cross-validation for
kernel logistic regression. Machine Learning, 71(2-3):243–264.
Chen, C., Zhang, Q., Ma, Q., and Yu, B. (2019). Lightgbm-ppi: Predicting protein-protein
interactions through lightgbm with multi-information fusion. Chemometrics and Intelligent
Laboratory Systems, 191:54–64.
Chen, L., Wang, S., Wei, Z., Zhang, Y., Luo, M., and Liang, Y. (2023). A self-attention based
dnn model to classify dynamic functional connectivity for autism spectrum disorder diagnosis.
In 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages
1856–1859. IEEE.
Cule, E. and De Iorio, M. (2013). Ridge regression in prediction problems: automatic choice of
the ridge parameter. Genetic Epidemiology, 37(7):704–714.
Dams-O’Connor, K., Gibbons, L. E., Landau, A., Larson, E. B., and Crane, P. K. (2016). Health
problems precede traumatic brain injury in older adults. Journal of the American Geriatrics
Society, 64(4):844–848.
Dietterich, T. G. (1998). Approximate statistical tests for comparing supervised classification
learning algorithms. Neural computation, 10(7):1895–1923.
44Dobriban, E.andWager, S.(2018). High-dimensionalasymptoticsofprediction: Ridgeregression
and classification. The Annals of Statistics, 46(1):247–279.
Dudoit, S. and van der Laan, M. J. (2005). Asymptotics of cross-validated risk estimation in
estimator selection and performance assessment. Statistical methodology, 2(2):131–154.
Efron, B. (1986). How biased is the apparent error rate of a prediction rule? Journal of the
American statistical Association, 81(394):461–470.
Efron, B. (2004). The estimation of prediction error: covariance penalties and cross-validation.
Journal of the American Statistical Association, 99(467):619–632.
Efron, B. and Tibshirani, R. (1997). Improvements on cross-validation: the 632+ bootstrap
method. Journal of the American Statistical Association, 92(438):548–560.
Friedman, J., Hastie, T., and Tibshirani, R. (2010). Regularization paths for generalized linear
models via coordinate descent. Journal of statistical software, 33(1):1.
Gauran, I. I., Xue, G., Chen, C., Ombao, H., and Yu, Z. (2022). Ridge penalization in high-
dimensionaltestingwithapplicationstoimaginggenetics. FrontiersinNeuroscience,16:836100.
Geisser,S.(1975). Thepredictivesamplereusemethodwithapplications. JournaloftheAmerican
statistical Association, 70(350):320–328.
Golub, G. H., Heath, M., and Wahba, G. (1979). Generalized cross-validation as a method for
choosing a good ridge parameter. Technometrics, 21(2):215–223.
Hastie, T., Montanari, A., Rosset, S., and Tibshirani, R. J. (2022). Surprises in high-dimensional
ridgeless least squares interpolation. Annals of statistics, 50(2):949.
Hastie, T., Tibshirani, R., and Friedman, J. (2009). The elements of statistical learning. Springer
Series in Statistics, Second edition, page 33.
Hoaglin, D. C. and Welsch, R. E. (1978). The hat matrix in regression and anova. The American
Statistician, 32(1):17–22.
Hoerl, A. E. (1962). Applications of ridge analysis to regression problems. Chem. Eng. Progress.,
58:54–59.
Hoerl, A. E., Kannard, R. W., and Baldwin, K. F. (1975). Ridge regression: some simulations.
Communications in Statistics-Theory and Methods, 4(2):105–123.
Hoerl, A. E. and Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthogonal
problems. Technometrics, 12(1):55–67.
Hsu, D., Kakade, S. M., and Zhang, T. (2012). Random design analysis of ridge regression. In
Conference on Learning Theory, pages 9–1. JMLR Workshop and Conference Proceedings.
Iizuka, N., Oka, M., Yamada-Okabe, H., Nishida, M., Maeda, Y., Mori, N., Takao, T., Tamesa,
T., Tangoku, A., Tabuchi, H., et al. (2003). Oligonucleotide microarray for prediction of
early intrahepatic recurrence of hepatocellular carcinoma after curative resection. The lancet,
361(9361):923–929.
James,G.andHastie,T.(1997). Generalizationsofthebias/variancedecompositionforprediction
error. Dept. Statistics, Stanford Univ., Stanford, CA, Tech. Rep.
45Jiang, W., Varma, S., and Simon, R. (2008). Calculating confidence intervals for prediction error
in microarray classification using resampling. Statistical applications in genetics and molecular
biology, 7(1).
Karoui, N. E. (2013). Asymptotic behavior of unregularized and ridge-regularized high-
dimensional robust regression estimators: rigorous results. arXiv preprint arXiv:1311.2445.
Kearns, M. and Ron, D. (1997). Algorithmic stability and sanity-check bounds for leave-one-out
cross-validation. In Proceedings of the tenth annual conference on Computational learning
theory, pages 152–162.
Kenney, K., Iacono, D., Edlow, B. L., Katz, D. I., Diaz-Arrastia, R., Dams-O’Connor, K.,
Daneshvar, D. H., Stevens, A., Moreau, A. L., Tirrell, L. S., et al. (2018). Dementia after
moderate-severe traumatic brain injury: coexistence of multiple proteinopathies. Journal of
Neuropathology & Experimental Neurology, 77(1):50–63.
Kobak, D., Lomond, J., and Sanchez, B. (2020). The optimal ridge penalty for real-world high-
dimensional data can be zero or negative due to the implicit ridge regularization. Journal of
Machine Learning Research, 21:169–1.
Kummer, K. K., Zeidler, M., Kalpachidou, T., and Kress, M. (2021). Role of il-6 in the regulation
of neuronal development, survival and function. Cytokine, 144:155582.
Lewczuk, P., Wiltfang, J., Kornhuber, J., and Verhasselt, A. (2021). Distributions of aβ42 and
aβ42/40 in the cerebrospinal fluid in view of the probability theory. Diagnostics, 11(12):2372.
Li, Y., Wang, Z., Li, L.-P., You, Z.-H., Huang, W.-Z., Zhan, X.-K., and Wang, Y.-B. (2021).
Robust and accurate prediction of protein–protein interactions by exploiting evolutionary in-
formation. Scientific Reports, 11(1):16910.
Lund, K. V. (2013). The instability of cross-validated lasso. Master’s thesis, University of Oslo.
Markatou, M., Tian, H., Biswas, S., and Hripcsak, G. M. (2005). Analysis of variance of cross-
validationestimatorsofthegeneralizationerror. JournalofMachineLearningResearch,6:1127–
1168.
Meijer, R. J. and Goeman, J. J. (2013). Efficient approximate k-fold and leave-one-out cross-
validation for ridge regression. Biometrical Journal, 55(2):141–155.
Michiels, S., Koscielny, S., and Hill, C. (2005). Prediction of cancer outcome with microarrays: a
multiple random validation strategy. The Lancet, 365(9458):488–492.
Nadeau, C. and Bengio, Y. (1999). Inference for the generalization error. Advances in neural
information processing systems, 12.
Palmqvist, S., Insel, P. S., Zetterberg, H., Blennow, K., Brix, B., Stomrud, E., Mattsson, N.,
Hansson, O., Initiative, A. D. N., et al. (2019). Accurate risk estimation of β-amyloid posi-
tivity to identify prodromal alzheimer’s disease: cross-validation study of practical algorithms.
Alzheimer’s & Dementia, 15(2):194–204.
Park, J.-C., Han, S.-H., and Mook-Jung, I. (2020). Peripheral inflammatory biomarkers in
alzheimer’s disease: a brief review. BMB reports, 53(1):10.
Parvandeh, S., Yeh, H.-W., Paulus, M. P., and McKinney, B. A. (2020). Consensus features
nested cross-validation. Bioinformatics, 36(10):3093–3098.
46Patil, P., Wei, Y., Rinaldo, A., and Tibshirani, R. (2021). Uniform consistency of cross-validation
estimators for high-dimensional ridge regression. In International Conference on Artificial
Intelligence and Statistics, pages 3178–3186. PMLR.
PerezGrovas-Saltijeral, A., Rajkumar, A. P., and Knight, H. M. (2023). Differential expression of
m5c rna methyltransferase genes nsun6 and nsun7 in alzheimer’s disease and traumatic brain
injury. Molecular Neurobiology, 60(4):2223–2235.
Pluta, D., Shen, T., Xue, G., Chen, C., Ombao, H., and Yu, Z. (2021). Ridge-penalized adaptive
mantel test and its application in imaging genetics. Statistics in Medicine, 40(24):5313–5332.
Qiu, T., Liu, Q., Chen, Y.-X., Zhao, Y.-F., and Li, Y.-M. (2015). Aβ42 and aβ40: similarities
and differences. Journal of Peptide Science, 21(7):522–529.
Rad, K. R. and Maleki, A. (2020). A scalable estimate of the out-of-sample prediction error via
approximate leave-one-out cross-validation. Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 82(4).
Rajanala, S., Bates, S., Hastie, T., and Tibshirani, R. (2022). Confidence intervals for the gener-
alisation error of random forests. arXiv preprint arXiv:2201.11210.
Rani, V., Verma, R., Kumar, K., and Chawla, R. (2023). Role of pro-inflammatory cytokines
in alzheimer’s disease and neuroprotective effects of pegylated self-assembled nanoscaffolds.
Current Research in Pharmacology and Drug Discovery, 4:100149.
Richards, D., Mourtada, J., and Rosasco, L. (2021). Asymptotics of ridge (less) regression under
general source condition. In International Conference on Artificial Intelligence and Statistics,
pages 3889–3897. PMLR.
Rosset, S. and Tibshirani, R. J. (2019). From fixed-x to random-x regression: Bias-variance
decompositions, covariance penalties, and prediction error estimation. Journal of the American
Statistical Association.
Salzberg, S. L. (1997). On comparing classifiers: Pitfalls to avoid and a recommended approach.
Data mining and knowledge discovery, 1:317–328.
Sen, A. and Srivastava, M. (2012). Regression analysis: theory, methods, and applications.
Springer Science & Business Media.
Shen, X. and Ye, J. (2002). Adaptive model selection. Journal of the American Statistical
Association, 97(457):210–221.
Smith,D.H.,Doll´e,J.-P.,Ameen-Ali,K.E.,Bretzin,A.,Cortes,E.,Crary,J.F.,Dams-O’Connor,
K., Diaz-Arrastia, R., Edlow, B. L., Folkerth, R., et al. (2021). Collaborative neuropathology
network characterizing outcomes of tbi (connect-tbi). Acta neuropathologica communications,
9(1):1–13.
Smith, D. H., Johnson, V. E., and Stewart, W. (2013). Chronic neuropathologies of single and
repetitive tbi: substrates of dementia? Nature Reviews Neurology, 9(4):211–221.
Stone, M. (1974). Cross-validatory choice and assessment of statistical predictions. Journal of
the Royal Statistical Society: Series B (Methodological), 36(2):111–133.
Stone, M. (1977). Asymptotics for and against cross-validation. Biometrika, pages 29–35.
Stone, M. (1978). Cross-validation: A review. Statistics: A Journal of Theoretical and Applied
Statistics, 9(1):127–139.
47Takane, Y. (2008). More on regularization and (generalized) ridge operators. New Trends in
Psychometrics (University Academic Press, Tokyo, 2008), pages 443–452.
Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. Journal of the Royal
Statistical Society Series B: Statistical Methodology, 58(1):267–288.
Tikhonov,A.N.(1943). Onthestabilityofinverseproblems. Dokl.Akad.NaukSSSR,39:195–198.
Torjesen, I. (2018). Traumatic brain injury raises dementia risk, large study suggests.
Tsamardinos, I. (2022). Don’t lose samples to estimation. Patterns, 3(12).
Tsamardinos, I., Rakhshani, A., and Lagani, V. (2015). Performance-estimation proper-
ties of cross-validation-based protocols with simultaneous hyper-parameter optimization.
International Journal on Artificial Intelligence Tools, 24(05):1540023.
Varma, S. and Simon, R. (2006). Bias in error estimation when using cross-validation for model
selection. BMC bioinformatics, 7(1):1–8.
Vehtari, A., Mononen, T., Tolvanen, V., Sivula, T., and Winther, O. (2016). Bayesian leave-
one-out cross-validation approximations for Gaussian latent variable models. The Journal of
Machine Learning Research, 17(1):3581–3618.
Wager, S. (2020). Cross-validation, risk estimation, and model selection: Comment on a paper
by Rosset and Tibshirani. Journal of the American Statistical Association, 115(529):157–160.
Wainwright, M. J. (2014). Structured regularizers for high-dimensional problems: Statistical and
computational issues. Annual Review of Statistics and Its Application, 1:233–253.
Wang, L., Wang, H.-F., Liu, S.-R., Yan, X., and Song, K.-J. (2019). Predicting protein-protein
interactions from matrix-based protein sequence using convolution neural network and feature-
selective rotation forest. Scientific reports, 9(1):9848.
Wang, M., Cang, Z., and Wei, G.-W. (2020). A topology-based network tree for the prediction
of protein–protein binding affinity changes following mutation. Nature Machine Intelligence,
2(2):116–123.
Wang, S., Zhou, W., Lu, H., Maleki, A., and Mirrokni, V. (2018). Approximate leave-one-out for
fast parameter tuning in high dimensions. In International Conference on Machine Learning,
pages 5228–5237. PMLR.
Weiss, S. M. and Kulikowski, C. A. (1991). Computer systems that learn: classification and
prediction methods from statistics, neural nets, machine learning, and expert systems. Morgan
Kaufmann Publishers Inc.
White, D. J., Korinek, D., Bernstein, M. T., Ovsiew, G. P., Resch, Z. J., and Soble, J. R. (2020).
Cross-validation of non-memory-based embedded performance validity tests for detecting in-
valid performance among patients with and without neurocognitive impairment. Journal of
Clinical and Experimental Neuropsychology, 42(5):459–472.
Wilson, L., Stewart, W., Dams-O’Connor, K., Diaz-Arrastia, R., Horton, L., Menon, D. K., and
Polinder, S. (2017). The chronic and evolving neurological consequences of traumatic brain
injury. The Lancet Neurology, 16(10):813–825.
Wu, D. and Xu, J. (2020). On the optimal weighted ℓ regularization in overparameterized linear
2
regression. arXiv preprint arXiv:2006.05800.
48Yazici, M. U., Yousef, M., Marron, J., and Bakir-Gungor, B. (2023). mircorrnetpro: Unraveling
algorithmic insights through cross-validation in multi-omics integration for comprehensive data
analysis. In 2023 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),
pages 3234–3240. IEEE.
Ye,J.(1998). Onmeasuringandcorrectingtheeffectsofdataminingandmodelselection. Journal
of the American Statistical Association, 93(441):120–131.
Zhao, B., Zou, F., and Zhu, H. (2023). Cross-trait prediction accuracy of summary statistics in
genome-wide association studies. Biometrics, 79(2):841–853.
Zhao,S.,Witten,D.,andShojaie,A.(2021). Indefenseoftheindefensible: Averynaiveapproach
to high-dimensional inference. Statistical Science, 36(4):562–577.
49