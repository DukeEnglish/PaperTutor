KaPO: Knowledge-aware Preference Optimization for Controllable
Knowledge Selection in Retrieval-Augmented Language Models
RuizheZhang*,YongxinXu*,
YuzhenXiao,RunchuanZhu,XinkeJiang,
XuChu†,JunfengZhao†‡,YashaWang†
KeyLaboratoryofHighConfidenceSoftwareTechnologies(PekingUniversity)
MinistryofEducation;SchoolofComputerScience,
PekingUniversity,Peking,China
Abstract maygenerateseeminglycoherentbutactuallyunreliablein-
formation,aphenomenonknownas”hallucination”(Jietal.
By integrating external knowledge, Retrieval-Augmented 2023a; Cao et al. 2020; Ji et al. 2023b), due to outdated
Generation (RAG) has become an effective strategy for
knowledge and a lack of long-tail knowledge (He, Zhang,
mitigating the hallucination problems that large language
andRoth2022;Kandpaletal.2023;Jiangetal.2024).
models (LLMs) encounter when dealing with knowledge-
To mitigate the hallucination problem in LLMs, the
intensivetasks.However,intheprocessofintegratingexter-
nal non-parametric supporting evidence with internal para- Retrieval-AugmentedGeneration(RAG)paradigmhasbeen
metricknowledge,inevitableknowledgeconflictsmayarise, widely adopted (Izacard et al. 2022; Asai et al. 2023b,a).
leadingtoconfusioninthemodel’sresponses.Toenhancethe A common retrieve-and-read framework retrieves rele-
knowledgeselectionofLLMsinvariouscontexts,somere- vant information from external knowledge bases, such as
searchhasfocusedonrefiningtheirbehaviorpatternsthrough Wikipedia, and integrates it with the reader model (i.e.,
instruction-tuning.Nonetheless,duetotheabsenceofexplicit LLMs)toenhancetheaccuracyoftheresponses(Gaoetal.
negative signals and comparative objectives, models fine-
2024; Yu et al. 2023a; Lewis et al. 2021). This method
tunedinthismannermaystillexhibitundesirablebehaviors
leverages rich information from reliable knowledge bases
in the intricate and realistic retrieval scenarios. To this end,
tocompensateforthelimitationsintheinternalknowledge we propose a Knowledge-aware Preference Optimization,
of LLMs. However, because the knowledge acquired dur-
dubbed KaPO, aimed at achieving controllable knowledge
selection in real retrieval scenarios. Concretely, we explore ing LLM pretraining is static, the latest and correct knowl-
andsimulateerrortypesacrossdiversecontextcombinations edge from external sources can conflict with the paramet-
and learn how to avoid these negative signals through pref- ric knowledge within the model, leading to knowledge
erence optimization methods. Simultaneously, by adjusting conflicts (Xu et al. 2024; Jin et al. 2024). Research indi-
the balance between response length and the proportion of cates that the behavior of LLMs in the face of knowledge
preference data representing different behavior patterns, we conflicts is uncertain, influenced by model parameters and
enhance the adherence capabilities and noise robustness of
context (Longpre et al. 2022; Xie et al. 2024). In many
LLMsinabalancedmanner.Experimentalresultsshowthat
cases,evenwhenRAGretrievescorrectinformation,LLMs
KaPOoutperformspreviousmethodsforhandlingknowledge
may not necessarily adhere to it when addressing the con-
conflictsbyover37%,whilealsoexhibitingrobustgeneral-
flict(Wu,Wu,andZou2024).Therefore,enhancingtheca-
izationacrossvariousout-of-distributiondatasets.
pabilityofLLMstohandleknowledgeconflictsisacrucial
stepinimprovingtheperformanceofRAG.
I.Introduction In response to the aforementioned issue, a mainstream
approachistoconstructspecificinstruction-tuningdatasets
Large Language Models (LLMs) (Taylor et al. 2022; Zhao
to optimize the knowledge prioritization of LLMs in con-
et al. 2023b) have been widely applied in various fields,
textswithvaryingdegreesofrelevance(Lietal.2022;Xue
such as natural language processing, question-answering
etal.2023).However,duetothelackofexplicitcontrastive
systems,andtextgeneration,givingrisetonumerousAIap-
target training, this prioritization has not been deeply in-
plications(Kaplanetal.2020;Vuetal.2024).Thesemod-
tegrated into the model’s self-awareness, resulting in the
els exhibit outstanding performance in many tasks, primar-
probabilitymassstillbeingconcentratedonundesirablere-
ily due to their large-scale parameters and extensive pre-
sponses (Tajwar et al. 2024). In real-world scenarios, the
trainingdata(Ziegleretal.2020;Wangetal.2023d).How-
context of RAG is complex, and even with relevant con-
ever,becauseofthestaticnatureofthetrainingdata,LLMs
texts,themodelmaystillselectirrelevantinformationfrom
*Theseauthorscontributedequally. othercontenttorespond (Shietal.2023a;Jiangetal.2024).
†Correspondingauthors. Therefore,ourinsightsstemfromtheerrortypesobservedin
‡Junfeng Zhao is also at the Big Data Technology Research real-world scenarios involving RAG. We propose introduc-
Center,NanhuLaboratory,314002,Jiaxing. ingnegativesignalsandoptimizingknowledgeselectionin
4202
guA
6
]LC.sc[
1v79230.8042:viXradifferent contexts through preference learning. In the pro- (ResponsetoChallengeC1).
cess of learning from human feedback, Direct Preference
• Secondly, we propose a rewriting strategy to address
Optimization(DPO)reparameterizestherewardfunctionto
length imbalance and a data ratio balancing strategy to
directlyfithumanpreferencedata.Duetoitssimplicityand
address behavior pattern imbalance, using DPO to opti-
stability, it has been widely adopted and has achieved re-
mizeLLMs’adherencecapabilityandnoiserobustness.
markable results in aligning with human values and inten-
Thesestrategiesnotonlyeliminatelengthbiasesandim-
tions. Inspired by this, in this work, we consider utilizing
balancesinbehaviorpatterndistributionbutalsoenhance
the negative gradient terms in the DPO contrastive objec-
theexhaustivenessofthemodel’sresponses.Thisprevents
tives to push down the likelihood of undesirable responses
the degradation of conversational abilities that can occur
invariouscontexts,furtherenhancingtheadherencecapa-
when training on datasets with shorter answers, such as
bility and noise robustness of LLMs. Although seemingly
inreadingcomprehensiontasks(ResponsetoChallenge
straightforward, implementing this intuition will encounter
C2).
thefollowingchallenges:
• We validated the training effectiveness of our method on
C1. How to more accurately simulate real-world RAG
multiplemodelsanddatasetsandtesteditsgeneralization
scenarios and introduce more comprehensive and fine-
abilityinout-of-distribution(OOD)scenarios.Theresults
grainednegativesignals?Attheinputlevel,duetofactors
indicate that our method not only improves the perfor-
such as data storage and retriever performance, the context
manceofmodelsontestsetsbutalsoenhancestheiradapt-
withintheretrievalrecallwindowinactualretrievalenviron-
ability and robustness when confronted with unknown
mentsishighlycomplex(Yaoetal.2024;Gaoetal.2023).
data.
Attheoutputlevel,comprehensivenegativesignalsarecru-
cial for shifting probability mass from undesired responses
II.RelatedWork
to desired responses (Tajwar et al. 2024). Therefore, simu-
latingcommonerrorscenariosbasedoncomplexinputcon- Knowledge Conflicts. Numerous studies have explored
text combinations to construct reasonable chosen-rejected the behavior of LLMs in knowledge conflict scenarios,
answerpairsisasignificantchallenge. providing valuable insights for our work. Longpre et al.
C2.Howtoavoidpreferenceimbalanceissuesinprefer- (2022) discovered that large Pre-trained Language Mod-
enceoptimization?Whenconstructingpreferencedatasets, els often prefer to ignore contextual information in favor
the length imbalance of chosen-rejected response pairs can of the parametric knowledge when facing knowledge con-
leadtolengthutilizationissues,wherethefine-tunedmodel flicts. Recently, with the emergence of LLM such as Chat-
tendstogeneratelongersentencesratherthanlearningpref- GPT (OpenAI 2022, 2023) and PaLM (Chowdhery et al.
erences for specific behavior patterns (Park et al. 2024a; 2022),researchersre-examinedthisissue. Wu,Wu,andZou
Meng, Xia, and Chen 2024). Additionally, the imbalance (2024) highlighted that this tendency to disregard context
in preference data representing different behavior patterns is strongly influenced by the model’s prior token probabil-
(e.g., directly answering ”I don’t know” by ignoring the ity and the deviation degree of the conflicting knowledge.
context versus finding answers from the context) can sig- Specifically, conflict knowledge that significantly deviates
nificantly impact the behavior patterns of the fine-tuned from the model’s existing knowledge is less likely to be
model (Liao et al. 2024). In the process of preference op- adopted, and knowledge with higher prior probabilities is
timization,avoidingbehaviorpatternimbalancescausedby more challenging to update. Kassner and Schu¨tze (2019)
data discrepancies is crucial for simultaneously enhancing demonstratedthatLLMsaresusceptibletobeingmisledby
themodel’sadherencecapabilityandnoiserobustness. task-irrelevant context. Furthermore, Tan et al. (2024) in-
By jointly considering the above issues, we propose dicated that the model’s contextual preferences are closely
KaPO, a Knowledge-aware Preference Optimization strat- related to the semantic completeness of the context and its
egy, which constructs comprehensive and balanced prefer- relevancetothequestion.
ence relations to optimize LLMs’ knowledge selection in Several studies aim to improve the adherence of LLMs
different contexts. Specifically, our main contributions are to context when faced with knowledge conflicts. For in-
summarizedasfollows: stance, Knowledge Aware Fine-Tuning (KAFT) (Li et al.
2022) attempts to enhance models’ ability to utilize exter-
• In the process of constructing the preference dataset, we nal knowledge by constructing challenging counterfactual
firstsimulatedreal-worldRAGscenariosattheinputlevel. knowledge from standard training datasets, as well as in-
We perform refined noise classification based on the rel- corporatingirrelevantknowledgetoimprovemodels’noise
evance between the knowledge context and the question resistance. However, as previously mentioned, the applica-
topic, and explore combination methods with evidence bility of this approach in real-world RAG scenarios is lim-
to form conflicting context and irrelevant context. At ited.Additionally,decoding-basedmethods(Jinetal.2024;
the output level, we simulate two common error types in Chen, Zhang, and Choi 2022), such as Context-Aware De-
different context relevance scenarios: Contextual Igno- coding(CAD)(Shietal.2023b),adjusttheoutputprobabil-
ranceandContextualOverinclusion,anddeveloptrain- ities of LLMs during token generation in a manner similar
ing strategies to avoid these errors. This is a general tocontrastivedecoding,conditionedontherelevantcontext.
paradigm for constructing knowledge conflict datasets, However,thisapproachmayimpactthesemanticcoherence
which can be generalized to various model architectures oflongresponses.Moreover,prompt-basedmethodsemploysophisticateddesignedpromptstoensurethatLLMsadhere It’s clearly that α and a are independent. Knowledge
i
to the provided context (Si et al. 2023; Zhou et al. 2023). conflict appears when α ∈/ S, and at this time response y
Nevertheless,relatedresearchindicatesthatmerelymodify- of Θ(q|τ) can be uncertain. To simplify the discussion, we
ingpromptsdoesnotsignificantlyalterLLM’sinternalprior limitN toamaximumof1,whichmeanscontextτ contains
tokenprobabilities(Wu,Wu,andZou2024),potentiallylim- atmostonedocumentDr fromwhichtheanswercanbede-
1
itingtheeffectivenessofthisapproach. rived.Ourpurposeistomakesurey =a when|S|=1and
1
y =αwhen|S|=0.Inotherword,LLMΘshoulduseap-
Retrieval-Augmented Generation. RAG incorporates
propriateexternalknowledgewhenthereexistsadocument
theexternalknowledgeretrievalcomponentviaprompten-
whichcontainsthenecessaryknowledgeregardlessofcon-
gineeringtoachievemorefactualconsistency,enhancingthe
flicting with parameter knowledge, while use its parameter
reliability and interpretability of LLMs’ responses (Lewis
knowledgewhenretrieveddocumentsareallirrelevant.
et al. 2021). Some studies have made improvements to
the retrieve-and-read framework by generating intermedi-
ate contexts using the parameter knowledge acquired dur-
ing the pretraining phase, thereby enhancing the quality of B.ContradictoryKnowledge
thefinalresponse.Theseintermediatecontextsmayinclude
commonsenseknowledge(Liuetal.2022),domain-specific
knowledge (Luo et al. 2023), and chain-of-thought(COT) ConstructingknowledgethatconflictswithLLM’sparame-
reasoning process (Wei et al. 2023; Kojima et al. 2023; Li terknowledgeiscrucialtocondition|S|=1.Forquestionq
etal.2023).Furthermore, Zhaoetal.(2023a), Wangetal. inRAGscenarios,thisconflictisreflectedinconflictingan-
(2023a)and Yuetal.(2023b)haveutilizedretrievedknowl- swers a i which are inconsistent with LLM’s parameter an-
edge to edit the intermediate contexts of the COT process, swerα.Itisimportanttonotethattheseconflictinganswers
therebyupdatingconflictingknowledge.However,thesein- a i do not necessarily have to be correct, nor is the LLM’s
termediatecontextsgeneratedbyLLMsmaycontainhallu- parameteranswerαalwaysincorrect.Inourapproach,both
cinations or other inaccurate information, potentially mis- answerscanbeincorrecttothequestionaslongastheycon-
leadingtheretrievalorreadermodels.Additionally,thefre- flict with each other. The key to knowledge conflict lies in
quentinteractionswithLLMsresultininefficienciesinreal- theconflictitself,regardlessofcorrectness.Thisaddressesa
worldapplications(Jiangetal.2024). commonmisconceptioninpreviouswork,whereresearchers
oftenensuredthatoneanswerwascorrectandtheotherin-
Knowledge editing. Knowledge editing is a classic correct(Tanetal.2024;Wu,Wu,andZou2024),whichnot
method for updating model knowledge, focusing on iden- only increased the difficulty of data filtering but also over-
tifying how models store factual knowledge and designing lookedsomeknowledgeconflictscenarios.
effective strategies to update parametric knowledge stored
Specifically, we first extract world knowledge acquired
in pre-trained models (Cao, Aziz, and Titov 2021; Onoe
during the pretraining phase of the large model, marked as
etal.2023;Mengetal.2023). Jangetal.(2022)proposed
parameter answer α. We encourage LLM to abstain from
acontinuallearningframeworkaimedatupdatingoutdated
answeringwhenuncertainandnormalizeallinstancesofre-
knowledge while preserving stable knowledge that is unaf-
fusal.Additionally,werefinetheresponseformatsforother
fected by temporal changes. However, these strategies may
parametric knowledge. The revised results are presented in
unintentionally affect unrelated parameters or cause incon-
Table1.
sistencieswithinthemodel’sinternalknowledge(Pinterand
Elhadad 2023; Xu et al. 2024; Wang et al. 2023b). More- For a given question q and LLM’s parameter answer α,
over, in the constantly evolving context of RAG scenarios, there are two potential sources of conflicting answers. The
thesequentialtrainingmethodforinjectingnewknowledge firstsourceistherealisticanswera tothequestion.Thesec-
r
provesimpractical. ondsourceisafabricatedanswera generatedusingGPT-4
c
that deviates from the realistic answer a . The latter is of-
r
III.Methodology tenreferredtoasacounterfactualanswer,whichwerequire
A.TaskDefinition to be as plausible as possible. Thus, for a question q and
LLM’sparameteranswerα,wecanobtainatleastonecon-
GivenanLLMΘandaninputnaturallanguagequestionq,
flictinganswer,ensuringthatthisanswerisnotexcessively
weaskΘtogeneratearesponseα=Θ(q),whichrepresents
far-fetched.Thefew-shotpromptsarestructuredasfollows:
theparametricknowledgeforq.Besides,atypicalretrieve-
and-readframeworkcanbeexpressedasy =Θ(q|τ),where
y istheoutputofΘbasedonτ.Contextτ isapermutation Prompt3.1:ExtractParameterAnswer
ofDr,j =1,2,...,K,whichrepresentsasetofdocuments
j
retrieved based on q. Assume S = {a i},i = 1,2,...,N This is a question about {Title}. Please answer
constitutes the set of correct answer, each of which is de- the question {Question q}. Please provide a direct
rived from the retrieved document Dr. Note that K is not answer without analysis. If you are unsure or do
i
necessarily equals with N, because some retrieved docu- not know the answer, please respond with ‘I don’t
ments may not contain any answer for q which are known know’.
asnoises.Prompt3.2:GenerateCounterfactualAnswer inthesecapabilitiesmanifestastwodistincterrortypes:one
inwhichtheLLMincorrectlyusesirrelevantcontextualin-
Please generate speciously plausible but incorrect formationtoconstructanswers,termedContextualOverin-
answer to the question. Provide only the false an- clusion;andanotherwheretheLLMdisregardsthecontext
swers;donotreiteratethequeries. entirely and relies exclusively on its parameter knowledge,
Question: What is the capital of France? Answer: termedContextualIgnorance.Theseerrorscanoccurwith
Paris.Fakeanswer:Lyon. both types of contexts as illustrated in Table 1. To address
Question: What is the highest mountain in the theseissues,wehavemeticulouslydesignedadatasetcom-
world? Answer: Mount Everest. Fake answer: prisingpositiveandnegativesamplepairstospecificallytar-
Lhotse. getandmitigatetheseerrors.
...7moreexamples...
Question:WhoisthefounderofMicrosoft?Answer:
BillGates.Fakeanswer:SteveJobs.
Question:{Questionq}Answer:{RealisticAnswer
a }Fakeanswer:
r Errors in Adherence Capability. The ideal behavior of
the LLM demonstrating adherence capability, as shown by
positive samples, is to answer using the conflicting knowl-
C.ContextFormulation
edgepresentinthecontext.Whencontextualoverinclusion
In this section, we illustrated how to formulate context τ
happens with conflicting context, LLM often utilizes inap-
basedondifferentkindsofknowledgeconflict.
propriate information from the context due to insufficient
To align with the RAG scenario, we utilized the
noiserobustnessorhallucinationissues.Forinstance,inthe
SQuAD2.0dataset(Rajpurkar,Jia,andLiang2018),aread-
examplepresentedinTable1,LLMchoosesnoisyinforma-
ing comprehension dataset encompassing multiple general
tionmarkedinred.Toaddressthiserror,weconstructedneg-
domains, with a substantial corpus of documents and as-
ativesamplesbyusingapromptmechanismtoguideGPT-4
sociated QA tasks. Notably, unlike corpora such as Trivi-
to generate incorrect answers from conflicting contexts. To
aQA(Joshietal.2017),whicharecollectedfromWikipedia,
ensurethequalityofthegenerateddata,weadheredtostrin-
SQuAD2.0isannotatedbyhumanstodeterminewhethera
gent validation criteria:(1) The generated answers must be
document can derived to an answer for a specific question.
derivedfromthecontext,ensuringthattheerrorisunequivo-
Previousresearchhashighlightedthattreatingarelevantyet
callyattributabletocontextualoverinclusion;(2)Thegener-
non-informative document as a reference external knowl-
atedanswersshouldbeasplausibleaspossibleanddistinctly
edge source can impair LLM’s adherence capabilities (Li
differentfromtheconflictinganswers,therebyensuringthe
et al. 2022). Following the chunk-size commonly used in
highqualityofthedata.
RAGtasks,wesetthelengthofcontextτ toK =4.
For scenarios with |S| = 1, we initially select pertinent
Whencontextualignoranceoccursinconflictingcontext,
documentsfromSQuAD2.0basedontheconflictingknowl-
LLMmayeitherfailtorecognizetheutilityofthecontextor,
edge. For question q and realistic answer a , we directly
r
select the corresponding document Dr from the original evenuponrecognizingit,mayopttodisregardtheconflict-
ing answer in favor of relying on its parameter knowledge.
dataset.Forquestionqandcounterfactualanswera ,were-
c
placealloccurrencesofa witha inDr.Subsequently,we Forinstance,intheexampleshowninTable1,LLManswers
r c
the question without utilizing supplemental knowledge. To
selectonerelevantdocumentonthesametopicandtworel-
simulatethiserror,weconstructednegativesamplesbyex-
evantdocumentsondifferenttopicsbasedonsemanticsim-
tractingLLM’sresponsetothequeryintheabsenceofany
ilarity.Weensurethattheselatterthreedocumentsareinca-
contextualsupport.
pableofansweringthequestionq.Thesefourdocumentsare
thenshuffledtoconstitutetheconflictingcontextτ.
Forscenarioswith|S| = 0,wedistinguishbetweenhard
and easy irrelevant documents. Hard documents, derived
fromhumanannotations,consistoftwodocumentsthatare
onrelatedtopicsbutcannotanswerthequestion.Easydoc- Errors in Noise Robustness. It is evident that positive
umentsarerandomlyselected,consistingoftwodocuments sample for noise robustness is to use LLM’s parameter
onunrelatedtopics.Thesefourdocumentsarethenshuffled knowledge to respond when encountering irrelevant con-
toconstitutetheirrelevantcontextτ. texts. When contextual overinclusion occurs in irrelevant
contexts, LLM may fail to recognize the context as irrele-
D.ErrorTypeAnalyse
vantormaybeinfluencedbyhallucinations,leadingittouse
As previously mentioned, we expect LLMs to utilize con- informationfromthecontextinsteadofdisregardingit.Sim-
textual knowledge when encountering conflicting context, ilar to contextual overinclusion in conflicting contexts, we
while relying on parameter knowledge when faced with ir- constructedcorrespondingnegativesamplesbyusingGPT-
relevant context. These two modes of handling context re- 4toextractincorrectanswersfromirrelevantcontexts.The
flect the model’s adherence capability and noise robust- promptsusedtogeneratecontextualoverinclusionandcon-
ness, respectively. In practical RAG scenarios, deficiencies textualignorancearestructuredasfollows:SampleQuestion:WhoistheDemocraticpresidentialcandidateinthe2024USpresidentialelection?
ParameterAnswer:TheDemocraticcandidateisJoeBiden.
ConflictingContext IrrelevantContext
WithPresidentJoeBidendroppingoutofthe
raceonJuly21,VicePresidentKamalaHarris ...adecisionnotwithoutprecedentinpolitical
becamethepresumedDemocraticnominee. history.Backin1968,PresidentLyndonB.
Thesearchforhervicepresidentialrunning JohnsonoftheDemocraticPartyalsooptedout
ContextExample
mateiscloselywatched,withtopcontenders amidstintensepoliticalchallenges.Following
includingSecretaryofTransportationPete Johnson’swithdrawal,HubertHumphreytook
Buttigieg,ArizonaSenatorMarkKelly,Illinois overastheDemocraticnomineeforpresident.
GovernorJ.B.Pritzker...
Ideal
TheDemocraticcandidateisKamalaHarris. TheDemocraticcandidateisJoeBiden.
GoldOutput Answer
Supplementalknowledgedoesnotprovidean
answertothisquestion,butbasedonmyown
knowledgeandunderstanding,theanswerto
thisquestionisthattheDemocraticcandidateis
Basedonsupplementalknowledgeandmyown JoeBiden.
Revised
understanding,theanswertothisquestionis Ifparameterknowledgeis‘Idon’tknow’:
Result
thattheDemocraticcandidateisKamalaHarris. SorryIdon’tknowtheanswertothisquestion
anditcannotbeansweredwiththe
supplementalknowledgeprovided.Please
provideamoredetailedquestionorupdatethe
supplementalinformation.
Error
Contextual TheDemocraticcandidateisMarkKelly. TheDemocraticcandidateisHubertHumphrey.
Answer
Overinclusion
Basedonsupplementalknowledgeandmyown
Basedonsupplementalknowledgeandmyown
Revised understanding,theanswertothisquestionis
understanding,theanswertothisquestionis
Result thatTheDemocraticcandidateisHubert
thatTheDemocraticcandidateisMarkKelly.
Humphrey.
Error
Contextual TheDemocraticcandidateisJoeBiden. /
Answer
Ignorance
Supplementalknowledgedoesnotprovidean
answertothisquestion,butbasedonmyown
Revised
knowledgeandunderstanding,theanswerto /
Result
thisquestionisthattheDemocraticcandidateis
JoeBiden.
Table1:AnexampleofhowtheKaPOdatasetisformulated.LLM’sparameterknowledgearemarkedinorange,whilecon-
flictingknowledgeincontextaremarkedingreen,andinappropriateinformationispresentedinred.
Prompt3.3:GenerateContextualOverinclusion E.TrainingMethod
Our training consists of two phases. First, we perform in-
Pleaseselectawordfromtheprovidedcontextasan
struction tuning using the conflicting knowledge and con-
alternativeanswertothisquestion.
textsconstructedinSectionBandSectionCtoenhancethe
Question:{Questionq}
LLM’s adherence capability and noise robustness in RAG
Correctanswer:{RealisticAnswera }
r task scenarios. Next, we utilize the preference dataset con-
Context:{Contextτ}
structed in Section D for DPO training to further improve
theLLM’sabilitytoavoidthetwotypesoferrors,whileen-
Pleasefollowtheserequirements:
suringthatitsfinalresponsesalignwithuserpreferences.
1. The answer must not be the same as the correct
answer. Instruction Tuning. Instruction tuning is a multi-task
2.Thealternativeanswerdoesnotneedtobecorrect, learningframeworkthatenablestheuseofhuman-readable
butitmustappearinthecontext. instructionstoguidetheoutputofLLMs.Givenasourcetext
3.Thealternativeanswermustbeinaformthatcan and task-specific instructions, the model is trained to gen-
answerthequestionandshouldbeasreasonableas erate a sequence of tokens representing the desired output
possible. structureanditscorrespondinglabels.Reviewingourdefini-
tionofadherencecapabilityandnoiserobustness,wewouldliketogetafinetunedmodelΘ fromoriginalLLMΘthat of contextual knowledge”, while the preference pairs re-
ft
satisfiesthefollowingcriteria: lated to error contextual ignorance in conflicting context
|S|=1: Θ (q|τ)=a ,whereDr →a guide the LLM to “utilize contextual knowledge without
ft 1 1 1 (1) rejecting it”, we realized that the ratio of these contrast-
|S|=0: Θ (q|τ)=α,whereΘ(q)=α
ft ingpreferencepairscouldsignificantlyinfluencetraining
Notethatalthoughthepresenceoftheanswerinτ wasdis- efficacy. During KaPO training, we ensured that the pro-
tinguished during dataset construction, the LLM does not portionR ofthesetwotypesofdatawasmaintained
error
possess this prior knowledge. The model must indepen- at approximately 1:1. Furthermore, we validated the im-
dentlydeterminethecontexttypeandformulatearesponse portanceofthisratioR insubsequentexperiments.
error
during the RAG task. The instruction prompts we used are
outlinedbelow: IV.Experiments
Inthissection,weconductaseriesofexperimentsonthree
Prompt3.3:InstructionTuning
datasetstoanswerthefollowingresearchquestions:
[Instruction] As a knowledge-based QA expert, • RQ1 (Section B): Does KaPO outperform other ap-
you will provide professional responses based on proaches for resolving knowledge conflict across various
user’s question, utilizing any supplemental knowl- basemodelsanddatasets?
edge provided to enhance the quality of your re- • RQ2 (Section C): What impact does each training phase
sponse.Ifthesupplementalinformationisirrelevant anddifferentknowledgetypeshaveontheoverallperfor-
tothequestion,relyonyourownexpertisetoformu- mance?
late an answer. If you are unsure about the answer,
• RQ3 (Section D): Does KaPO training conducted in
pleaserespondwith‘Idon’tknow’.
general domains remain effective in out-of-distribution
[SupplementalKnowledge] {Contextτ}
(OOD)scenarios?
[User’sQuestion] {Questionq}
[Answer] • RQ4 (Section E): How sensitive is KaPO to hyper-
parametersdataratioR andcontextlengthwindows
error
K?
Direct Preference Optimization. As previously dis-
cussed, LLMs may exhibit errors contextual overinclusion
and contextual ignorance in real-world RAG scenarios. A.ExperimentalSetup
To further enhance adherence capability and noise robust- Datasets WeconstructedtheKaPOtrainingdatasetbased
ness,weproposeaKnowledge-awarePreferenceOptimiza- on SQuAD 2.0 (Rajpurkar, Jia, and Liang 2018). The test
tion(KaPO) training strategy. This strategy employs three datasetscomprisethefollowingthreetypes:(1)SQuAD2.0-
typesofpreferencesbetweenpositiveandnegativesamples Eval, a validation set partitioned using the same construc-
intwodifferentcontextualsettingstoconductDPOtraining tion method. (2) Open-source counterfactual datasets:
ontheLLM.Detailsofpreferencepairsconstructioncanbe RGB (Chen et al. 2023) and KNOT (Liu et al. 2024) are
foundinSectionD.Usingthisapproach,wetraintheLLM two general-domain QA datasets containing counterfactual
toavoidtheseerrorsandimproveitsabilitytoutilizediffer- knowledgeandcontexts.Weaugmentedthesedatasetswith
entcontexts. irrelevant contexts for testing purposes. Notably, RGB is a
DuringtheselectionofdataratiosforDPO,wealsoiden- Chinesedataset.(3)Domain-specificdataset:CMB(Wang
tifiedtwopreferenceimbalancesthatimpacttrainingeffec- et al. 2023c) is a multi-task QA dataset in the medical do-
tiveness. main, encompassing 269,359 questions across four clini-
• Length Imbalance. Some studies suggest that reward calmedicinespecialtiesofphysicians,nurses,medicaltech-
hacking observed in RLHF can also negatively impact nicians, and pharmacists. Due to quantity constraints, we
DPO training (Gao, Schulman, and Hilton 2022; Park Given the extensive size of the CMB dataset, we randomly
et al. 2024b). We observed that in our previously con- sample4,000questionsfortesting.
structed dataset, for the same preference pair, the posi-
BaseModel WeselectedthefollowingtwotypesofLLM
tivesamplewasoftenthebetter-formattedandlongerre-
as the base model and explored the gains brought by
sponse, whilethe negative samplewas a shorter conflict-
KaPO: Baichuan7B-chat (Yang et al. 2023) and Llama2-
inganswer.DuetothetendencyofLLMstobeinfluenced
13B-chat(Touvronetal.2023).
by length bias during DPO (Singhal et al. 2024), they
might prefer generating longer responses, which overall Compared Methods In order to explore the advantages
manifestsasagreatertendencytorefuseansweringrather of the KaPO, we compare the KaPO results against four
thanprovidingaconflictinganswer.Tomitigatethisissue, other models: (1) Base Model (Base) answers user ques-
we standardized the format for all positive and negative tions based on supplementary external knowledge, which
samples, aligning their lengths to ensure that the average can be considered a fundamental retrieve-and-read frame-
lengthlen approximatelyequalslen . workunderRAG(Lewisetal.2021).WeuseBaichuan7B-
win loss
• Error Type Imbalance. Given that the preference pairs chat and Llama2-13B-chatas base models. (2) Prompt-
associated with error contextual overinclusion in irrele- based Method (Prompt) employs meticulously designed
vantcontextexhibitatendencytowards“rejectingtheuse prompts to enhance the model’s capability to adhere toTable2:Performancecomparison(inpercent)onSquad2.0-Eval,RGBandKNOT.Redshadingindicatesthebest-performing
model,while bluesignifiesthesecond-bestintheablationstudy,andgreensignifiesthesecond-bestinbaselines.
LLMTurbo LLM Baichuan2-7B-Chat Llama2-13B-Chat
Dataset Squad2.0-Eval RGB KNOT Squad2.0-Eval RGB KNOT
Method
Metric R R R R R R R R R R R R
Ad Ro Ad Ro Ad Ro Ad Ro Ad Ro Ad Ro
Base 43.51 9.80 65.00 24.00 26.42 7.65 52.71 11.95 69.00 25.00 49.66 21.67
Baselines
KAFT 58.83 21.43 75.00 27.00 54.45 17.93 65.73 34.34 73.50 29.50 62.21 24.47
Ours KaPO 80.64 38.77 93.50 37.00 69.95 39.73 76.11 44.64 83.50 37.50 77.03 38.28
*PerformanceGain↑
KaPO(w/oDPO) 71.09 36.50 89.50 31.00 64.45 36.50 75.96 42.87 80.00 35.00 70.28 36.21
Ablation
KaPO(w/oSFT) 69.39 37.50 92.50 35.00 66.92 38.76 74.73 42.86 81.00 34.00 69.39 36.74
external knowledge (Zhou et al. 2023). (3) Finetuning: DPOcomparativeobjectivestoreducethelikelihoodofun-
KAFT (Li et al. 2022) employs instruction fine-tuning to desiredresponses.Furthermore,byaligningdatalengthsand
improve the LLM’s adherence to contexts of varying rele- balancingdataratios,weeffectivelymitigatepreferenceim-
vance. (4) Decode-Based Method: CD2 uses a contrastive balancesinherentinDPO.Experimentalevaluationsacross
decoding-likemethodtoadjusttheprobabilitiesofoutputto- diverse datasets and two base models substantiate the effi-
kens.(5)Chain-of-Thought:ChainofThought(COT)(Wei cacy and generalization capability of KaPO. In the future,
etal.2023)isacommonmethodtoenhancetheperformance wewillexplorehowthecompositionandproportionofdif-
ofLLMsindownstreamtasks.COT-VE(Zhaoetal.2023a) ferenttypesofcontextsaffecttheabilityofLLMstoutilize
extendsCOTbyguidingLLMtoidentifyconflictingknowl- externalknowledge.
edgeandmodifyitsresponsesaccordingly.
References
Metrics We designed statistical metrics to evaluate the
twocapabilitiesofLLMs.Foradherencecapability,weuti- Asai,A.;Min,S.;Zhong,Z.;andChen,D.2023a.Retrieval-
lized the conflicting contexts from the test set as supple- basedlanguagemodelsandapplications. InProceedingsof
mentaryknowledge,measuringtheproportionR ofLLM the 61st Annual Meeting of the Association for Computa-
Ad
responses that align with the conflicting knowledge within tionalLinguistics(Volume6:TutorialAbstracts),41–46.
these contexts. For the RGB and KNOT datasets, the con- Asai,A.;Wu,Z.;Wang,Y.;Sil,A.;andHajishirzi,H.2023b.
flicting knowledge exclusively consists of counterfactual Self-RAG: Learning to Retrieve, Generate, and Critique
knowledge. For noise robustness, we employed the irrele- throughSelf-Reflection. arXivpreprintarXiv:2310.11511.
vantcontextsfromthetestsetassupplementaryknowledge,
Cao,M.;Dong,Y.;Wu,J.;andCheung,J.C.K.2020. Fac-
examining the proportion R of LLM responses that cor-
Ro tual Error Correction for Abstractive Summarization Mod-
respondwiththemodel’sparameterknowledge.
els. In Proceedings of the 2020 Conference on Empirical
MethodsinNaturalLanguageProcessing(EMNLP),6251–
B.PerformanceComparison
6258.Online:AssociationforComputationalLinguistics.
ToanswerRQ1,weconductexperimentsandreportresults
Cao, N. D.; Aziz, W.; and Titov, I. 2021. Editing Factual
ofthetwometricsonSquad2.0-Eval,RGBandkNOTwith
KnowledgeinLanguageModels. arXiv:2104.08164.
twoLLMturbos,asillustratedinTable2.Fromthereported
results,wecanfindthefollowingobservations: Chen, H.-T.; Zhang, M. J. Q.; and Choi, E. 2022. Rich
Comparison of KaPO and other methods. Firstly, it Knowledge Sources Bring Complex Knowledge Conflicts:
is evident that our mothed, KaPO, outperforms the base- Recalibrating Models to Reflect Conflicting Evidence.
line methods across all metrics. For instance, the R arXiv:2210.13701.
Ad
and R Ro scores see an improvement of approximately Chen, J.; Lin, H.; Han, X.; and Sun, L. 2023. Benchmark-
37.07%-85.33% and 80.91%-295.61% for the Squad2.0- ing Large Language Models in Retrieval-Augmented Gen-
EvaldatasetwithBaichuan2-7B-Chat.Moreover,compared eration. arXiv:2309.01431.
to KAFT, KaPO uses more complicated contexts and com-
Chowdhery,A.;Narang,S.;Devlin,J.;Bosma,M.;Mishra,
prehensivenegativesignalstoenhanceLLM’sadherenceca-
G.; Roberts, A.; Barham, P.; Chung, H. W.; Sutton,
pabilityandnoiserobustness.
C.; Gehrmann, S.; Schuh, P.; Shi, K.; Tsvyashchenko,
S.; Maynez, J.; Rao, A.; Barnes, P.; Tay, Y.; Shazeer,
V.Conclusion
N.; Prabhakaran, V.; Reif, E.; Du, N.; Hutchinson, B.;
In this paper, we propose KaPO, a Knowledge-aware Pope, R.; Bradbury, J.; Austin, J.; Isard, M.; Gur-Ari, G.;
PreferenceOptimizationstrategytoenhanceLLM’sadher- Yin, P.; Duke, T.; Levskaya, A.; Ghemawat, S.; Dev, S.;
encecapabilityandnoiserobustnesstoexternalknowledge. Michalewski, H.; Garcia, X.; Misra, V.; Robinson, K.; Fe-
Specifically, we abstract and simulate two common error dus,L.;Zhou,D.;Ippolito,D.;Luan,D.;Lim,H.;Zoph,B.;
types in scenarios with varying contextual relevance: Con- Spiridonov,A.;Sepassi,R.;Dohan,D.;Agrawal,S.;Omer-
textual Ignorance and Contextual Overinclusion. Based on nick, M.; Dai, A. M.; Pillai, T. S.; Pellat, M.; Lewkowycz,
instruction-tuning,weutilizenegativegradienttermsinthe A.; Moreira, E.; Child, R.; Polozov, O.; Lee, K.; Zhou, Z.;Wang, X.; Saeta, B.; Diaz, M.; Firat, O.; Catasta, M.; Wei, Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa,
J.; Meier-Hellstern, K.; Eck, D.; Dean, J.; Petrov, S.; and Y.2023. LargeLanguageModelsareZero-ShotReasoners.
Fiedel, N. 2022. PaLM: Scaling Language Modeling with arXiv:2205.11916.
Pathways. arXiv:2204.02311. Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin,
Gao,L.;Schulman,J.;andHilton,J.2022.ScalingLawsfor V.; Goyal, N.; Ku¨ttler, H.; Lewis, M.; tau Yih, W.;
RewardModelOveroptimization. arXiv:2210.10760. Rockta¨schel, T.; Riedel, S.; and Kiela, D. 2021. Retrieval-
Gao, Y.; Xiong, Y.; Gao, X.; Jia, K.; Pan, J.; Bi, Y.; Dai, Augmented Generation for Knowledge-Intensive NLP
Y.; Sun, J.; and Wang, H. 2023. Retrieval-augmented gen- Tasks. arXiv:2005.11401.
erationforlargelanguagemodels:Asurvey. arXivpreprint Li, D.; Rawat, A. S.; Zaheer, M.; Wang, X.; Lukasik, M.;
arXiv:2312.10997. Veit,A.;Yu,F.;andKumar,S.2022. LargeLanguageMod-
Gao, Y.; Xiong, Y.; Gao, X.; Jia, K.; Pan, J.; Bi, Y.; Dai, elswithControllableWorkingMemory. arXiv:2211.05110.
Y.; Sun, J.; Wang, M.; and Wang, H. 2024. Retrieval- Li, X.; Zhao, R.; Chia, Y. K.; Ding, B.; Joty, S.; Poria, S.;
AugmentedGenerationforLargeLanguageModels:ASur- andBing,L.2023. Chain-of-Knowledge:GroundingLarge
vey. arXiv:2312.10997. Language Models via Dynamic Knowledge Adapting over
He, H.; Zhang, H.; and Roth, D. 2022. Rethinking HeterogeneousSources. arXiv:2305.13269.
with Retrieval: Faithful Large Language Model Inference. Liao,K.;Li,S.;Zhao,M.;Liu,L.;Xue,M.;Hu,Z.;Han,H.;
arXiv:2301.00303. andYin,C.2024. EnhancingReinforcementLearningwith
Izacard, G.; Lewis, P.; Lomeli, M.; Hosseini, L.; Petroni, Label-Sensitive Reward for Natural Language Understand-
F.; Schick, T.; Dwivedi-Yu, J.; Joulin, A.; Riedel, S.; and ing. arXivpreprintarXiv:2405.19763.
Grave, E. 2022. Atlas: Few-shot Learning with Retrieval Liu, J.; Liu, A.; Lu, X.; Welleck, S.; West, P.; Bras,
AugmentedLanguageModels. arXiv:2208.03299. R. L.; Choi, Y.; and Hajishirzi, H. 2022. Gener-
Jang, J.; Ye, S.; Yang, S.; Shin, J.; Han, J.; Kim, G.; Choi, ated Knowledge Prompting for Commonsense Reasoning.
S. J.; and Seo, M. 2022. Towards Continual Knowledge arXiv:2110.08387.
LearningofLanguageModels. arXiv:2110.03215. Liu, Y.; Yao, Z.; Lv, X.; Fan, Y.; Cao, S.; Yu, J.; Hou, L.;
Ji,Z.;Lee,N.;Frieske,R.;Yu,T.;Su,D.;Xu,Y.;Ishii,E.; and Li, J. 2024. Untangle the KNOT: Interweaving Con-
Bang,Y.J.;Madotto,A.;andFung,P.2023a. Surveyofhal- flictingKnowledgeandReasoningSkillsinLargeLanguage
lucinationinnaturallanguagegeneration. ACMComputing Models. arXiv:2404.03577.
Surveys,55(12):1–38. Longpre, S.; Perisetla, K.; Chen, A.; Ramesh, N.; DuBois,
Ji,Z.;Lee,N.;Frieske,R.;Yu,T.;Su,D.;Xu,Y.;Ishii,E.; C.;andSingh,S.2022. Entity-BasedKnowledgeConflicts
Bang,Y.J.;Madotto,A.;andFung,P.2023b.SurveyofHal- inQuestionAnswering. arXiv:2109.05052.
lucinationinNaturalLanguageGeneration. ACMComput. Luo,Z.;Xu,C.;Zhao,P.;Geng,X.;Tao,C.;Ma,J.;Lin,Q.;
Surv.,55(12).
and Jiang, D. 2023. Augmented Large Language Models
Jiang, X.; Zhang, R.; Xu, Y.; Qiu, R.; Fang, Y.; Wang, Z.; withParametricKnowledgeGuiding. arXiv:2305.04757.
Tang, J.; Ding, H.; Chu, X.; Zhao, J.; and Wang, Y. 2024.
Meng, K.; Bau, D.; Andonian, A.; and Belinkov, Y.
HyKGE:AHypothesisKnowledgeGraphEnhancedFrame-
2023. Locating and Editing Factual Associations in GPT.
workforAccurateandReliableMedicalLLMsResponses.
arXiv:2202.05262.
arXiv:2312.15883.
Meng, Y.; Xia, M.; and Chen, D. 2024. Simpo: Simple
Jin, Z.; Cao, P.; Chen, Y.; Liu, K.; Jiang, X.; Xu, J.; Li,
preferenceoptimizationwithareference-freereward. arXiv
Q.; and Zhao, J. 2024. Tug-of-War Between Knowledge:
preprintarXiv:2405.14734.
ExploringandResolvingKnowledgeConflictsinRetrieval-
Onoe, Y.; Zhang, M. J. Q.; Padmanabhan, S.; Durrett, G.;
AugmentedLanguageModels. arXiv:2402.14409.
andChoi,E.2023. CanLMsLearnNewEntitiesfromDe-
Joshi,M.;Choi,E.;Weld,D.S.;andZettlemoyer,L.2017.
scriptions?ChallengesinPropagatingInjectedKnowledge.
TriviaQA: A Large Scale Distantly Supervised Challenge
arXiv:2305.01651.
DatasetforReadingComprehension. arXiv:1705.03551.
OpenAI. 2022. Introducing ChatGPT. https://openai.com/
Kandpal,N.;Deng,H.;Roberts,A.;Wallace,E.;andRaffel,
blog/chatgpt.
C.2023. Largelanguagemodelsstruggletolearnlong-tail
OpenAI. 2023. GPT-4 Technical Report. ArXiv,
knowledge. InInternationalConferenceonMachineLearn-
abs/2303.08774.
ing,15696–15707.PMLR.
Park,R.;Rafailov,R.;Ermon,S.;andFinn,C.2024a. Dis-
Kaplan, J.; McCandlish, S.; Henighan, T.; Brown, T. B.;
entanglinglengthfromqualityindirectpreferenceoptimiza-
Chess, B.; Child, R.; Gray, S.; Radford, A.; Wu, J.; and
tion. arXivpreprintarXiv:2403.19159.
Amodei,D.2020. ScalingLawsforNeuralLanguageMod-
els. arXiv:2001.08361. Park,R.;Rafailov,R.;Ermon,S.;andFinn,C.2024b. Dis-
Kassner,N.;andSchu¨tze,H.2019. NegatedandMisprimed entangling Length from Quality in Direct Preference Opti-
Probes for Pretrained Language Models: Birds Can Talk, mization. arXiv:2403.19159.
ButCannotFly. CornellUniversity-arXiv,CornellUniver- Pinter,Y.;andElhadad,M.2023. EmptyingtheOceanwith
sity-arXiv. aSpoon:ShouldWeEditModels? arXiv:2310.11958.Rajpurkar, P.; Jia, R.; and Liang, P. 2018. Know What CMB: A Comprehensive Medical Benchmark in Chinese.
You Don’t Know: Unanswerable Questions for SQuAD. arXiv:2308.08833.
arXiv:1806.03822. Wang, Y.; Kordi, Y.; Mishra, S.; Liu, A.; Smith, N. A.;
Shi, F.; Chen, X.; Misra, K.; Scales, N.; Dohan, D.; Chi, Khashabi, D.; and Hajishirzi, H. 2023d. Self-Instruct:
E.; Scha¨rli, N.; and Zhou, D. 2023a. Large Language Aligning Language Models with Self-Generated Instruc-
Models Can Be Easily Distracted by Irrelevant Context. tions. arXiv:2212.10560.
arXiv:2302.00093. Wei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; Ichter, B.;
Shi,W.;Han,X.;Lewis,M.;Tsvetkov,Y.;Zettlemoyer,L.; Xia, F.; Chi, E.; Le, Q.; and Zhou, D. 2023. Chain-of-
andtauYih,S.W.2023b. TrustingYourEvidence:Halluci- Thought Prompting Elicits Reasoning in Large Language
nateLesswithContext-awareDecoding.arXiv:2305.14739. Models. arXiv:2201.11903.
Si,C.;Gan,Z.;Yang,Z.;Wang,S.;Wang,J.;Boyd-Graber, Wu,K.;Wu,E.;andZou,J.2024. ClashEval:Quantifying
J.; and Wang, L. 2023. Prompting GPT-3 To Be Reliable. thetug-of-warbetweenanLLM’sinternalpriorandexternal
arXiv:2210.09150. evidence. arXiv:2404.10198.
Singhal,P.;Goyal,T.;Xu,J.;andDurrett,G.2024. ALong Xie, J.; Zhang, K.; Chen, J.; Lou, R.; and Su, Y. 2024.
Way to Go: Investigating Length Correlations in RLHF. Adaptive Chameleon or Stubborn Sloth: Revealing the Be-
arXiv:2310.03716. haviorofLargeLanguageModelsinKnowledgeConflicts.
Tajwar, F.; Singh, A.; Sharma, A.; Rafailov, R.; Schneider, arXiv:2305.13300.
J.;Xie,T.;Ermon,S.;Finn,C.;andKumar,A.2024. Pref- Xu,R.;Qi,Z.;Guo,Z.;Wang,C.;Wang,H.;Zhang,Y.;and
erence fine-tuning of llms should leverage suboptimal, on- Xu, W. 2024. Knowledge Conflicts for LLMs: A Survey.
policydata. arXivpreprintarXiv:2404.14367. arXiv:2403.08319.
Tan,H.;Sun,F.;Yang,W.;Wang,Y.;Cao,Q.;andCheng,X. Xue,B.;Wang,W.;Wang,H.;Mi,F.;Wang,R.;Wang,Y.;
2024.BlindedbyGeneratedContexts:HowLanguageMod- Shang, L.; Jiang, X.; Liu, Q.; and Wong, K.-F. 2023. Im-
elsMergeGeneratedandRetrievedContextsWhenKnowl- provingFactualConsistencyforKnowledge-GroundedDia-
edgeConflicts? arXiv:2401.11911. logueSystemsviaKnowledgeEnhancementandAlignment.
Taylor,R.;Kardas,M.;Cucurull,G.;Scialom,T.;Hartshorn, arXiv:2310.08372.
A.; Saravia, E.; Poulton, A.; Kerkez, V.; and Stojnic, R. Yang,A.;Xiao,B.;Wang,B.;Zhang,B.;Bian,C.;Yin,C.;
2022. Galactica: A Large Language Model for Science. Lv,C.;Pan,D.;Wang,D.;Yan,D.;Yang,F.;Deng,F.;Wang,
arXiv:2211.09085. F.; Liu, F.; Ai, G.; Dong, G.; Zhao, H.; Xu, H.; Sun, H.;
Zhang, H.; Liu, H.; Ji, J.; Xie, J.; Dai, J.; Fang, K.; Su, L.;
Touvron,H.;Martin,L.;Stone,K.;Albert,P.;Almahairi,A.;
Song, L.; Liu, L.; Ru, L.; Ma, L.; Wang, M.; Liu, M.; Lin,
Babaei,Y.;Bashlykov,N.;Batra,S.;Bhargava,P.;Bhosale,
M.;Nie,N.;Guo,P.;Sun,R.;Zhang,T.;Li,T.;Li,T.;Cheng,
S.; Bikel, D.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucu-
W.;Chen,W.;Zeng,X.;Wang,X.;Chen,X.;Men,X.;Yu,
rull,G.;Esiobu,D.;Fernandes,J.;Fu,J.;Fu,W.;Fuller,B.;
X.; Pan, X.; Shen, Y.; Wang, Y.; Li, Y.; Jiang, Y.; Gao, Y.;
Gao, C.; Goswami, V.; Goyal, N.; Hartshorn, A.; Hosseini,
Zhang, Y.; Zhou, Z.; and Wu, Z. 2023. Baichuan 2: Open
S.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V.; Khabsa, M.;
Large-scaleLanguageModels. arXiv:2309.10305.
Kloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.;
Lavril,T.;Lee,J.;Liskovich,D.;Lu,Y.;Mao,Y.;Martinet, Yao,Z.;Qi,W.;Pan,L.;Cao,S.;Hu,L.;Liu,W.;Hou,L.;
X.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y.; Poul- andLi,J.2024.SeaKR:Self-awareKnowledgeRetrievalfor
ton,A.;Reizenstein,J.;Rungta,R.;Saladi,K.;Schelten,A.; Adaptive Retrieval Augmented Generation. arXiv preprint
Silva,R.;Smith,E.M.;Subramanian,R.;Tan,X.E.;Tang, arXiv:2406.19215.
B.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.; Yu, W.; Iter, D.; Wang, S.; Xu, Y.; Ju, M.; Sanyal, S.; Zhu,
Zarov,I.;Zhang,Y.;Fan,A.;Kambadur,M.;Narang,S.;Ro- C.; Zeng, M.; and Jiang, M. 2023a. Generate rather than
driguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023. Retrieve:LargeLanguageModelsareStrongContextGen-
Llama 2: Open Foundation and Fine-Tuned Chat Models. erators. arXiv:2209.10063.
arXiv:2307.09288. Yu, W.; Zhang, H.; Pan, X.; Ma, K.; Wang, H.; and Yu, D.
Vu, M. D.; Wang, H.; Li, Z.; Chen, J.; Zhao, S.; Xing, Z.; 2023b. Chain-of-Note:EnhancingRobustnessinRetrieval-
andChen,C.2024.GPTVoiceTasker:LLM-PoweredVirtual AugmentedLanguageModels. arXiv:2311.09210.
AssistantforSmartphone. arXiv:2401.14268. Zhao, R.; Li, X.; Joty, S.; Qin, C.; and Bing, L.
Wang,K.;Duan,F.;Wang,S.;Li,P.;Xian,Y.;Yin,C.;Rong, 2023a. Verify-and-Edit:AKnowledge-EnhancedChain-of-
W.;andXiong,Z.2023a. Knowledge-DrivenCoT:Explor- ThoughtFramework. arXiv:2305.03268.
ing Faithful Reasoning in LLMs for Knowledge-intensive Zhao,W.X.;Zhou,K.;Li,J.;Tang,T.;Wang,X.;Hou,Y.;
QuestionAnswering. arXiv:2308.13259. Min, Y.; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y.; Yang, C.;
Wang,L.;Zhang,X.;Li,Q.;Zhang,M.;Su,H.;Zhu,J.;and Chen,Y.;Chen,Z.;Jiang,J.;Ren,R.;Li,Y.;Tang,X.;Liu,
Zhong,Y.2023b. Incorporatingneuro-inspiredadaptability Z.; Liu, P.; Nie, J.-Y.; and Wen, J.-R. 2023b. A Survey of
for continual learning in artificial intelligence. Nature Ma- LargeLanguageModels. arXiv:2303.18223.
chineIntelligence,5(12):1356–1368. Zhou, W.; Zhang, S.; Poon, H.; and Chen, M. 2023.
Wang,X.;Chen,G.H.;Song,D.;Zhang,Z.;Chen,Z.;Xiao, Context-faithful Prompting for Large Language Models.
Q.; Jiang, F.; Li, J.; Wan, X.; Wang, B.; and Li, H. 2023c. arXiv:2303.11315.Ziegler, D. M.; Stiennon, N.; Wu, J.; Brown, T. B.; Rad-
ford, A.; Amodei, D.; Christiano, P.; and Irving, G. 2020.
Fine-Tuning Language Models from Human Preferences.
arXiv:1909.08593.