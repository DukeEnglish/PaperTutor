2024-8-7
Scaling LLM Test-Time Compute Optimally can
be More Effective than Scaling Model Parameters
‚ô¶,1 2 ‚ô£,2 ‚ô£,2
CharlieSnell ,JaehoonLee ,KelvinXu andAviralKumar
‚ô£Equaladvising,1UCBerkeley,2GoogleDeepMind,‚ô¶WorkdoneduringaninternshipatGoogleDeepMind
Enabling LLMs to improve their outputs by using more test-time computation is a critical step towards
buildinggenerallyself-improvingagentsthatcanoperateonopen-endednaturallanguage. Inthispaper,
westudythescalingofinference-timecomputationinLLMs,withafocusonansweringthequestion: ifan
LLMisallowedtouseafixedbutnon-trivialamountofinference-timecompute,howmuchcanitimproveits
performanceonachallengingprompt? Answeringthisquestionhasimplicationsnotonlyontheachievable
performanceofLLMs,butalsoonthefutureofLLMpretrainingandhowoneshouldtradeoffinference-time
and pre-training compute. Despite its importance, little research attempted to understand the scaling
behaviorsofvarioustest-timeinferencemethods. Moreover,currentworklargelyprovidesnegativeresults
for a number of these strategies. In this work, we analyze two primary mechanisms to scale test-time
computation: (1) searching against dense, process-based verifier reward models; and (2) updating the
model‚Äôsdistributionoveraresponseadaptively,giventhepromptattesttime. Wefindthatinbothcases,the
effectivenessofdifferentapproachestoscalingtest-timecomputecriticallyvariesdependingonthedifficulty
of the prompt. This observation motivates applying a ‚Äúcompute-optimal‚Äù scaling strategy, which acts to
mosteffectivelyallocatetest-timecomputeadaptivelyperprompt. Usingthiscompute-optimalstrategy,we
canimprovetheefficiencyoftest-timecomputescalingbymorethan4√ócomparedtoabest-of-Nbaseline.
Additionally,inaFLOPs-matchedevaluation,wefindthatonproblemswhereasmallerbasemodelattains
somewhatnon-trivialsuccessrates,test-timecomputecanbeusedtooutperforma14√ólargermodel.
1. Introduction
Humans tend to think for longer on difficult problems to reliably improve their decisions [9, 17, 18].
Can we instill a similar capability into today‚Äôs large language models (LLMs)? More specifically, given
a challenging input query, can we enable language models to most effectively make use of additional
computation at test time so as to improve the accuracy of their response? In theory, by applying additional
computation at test time, an LLM should be able to do better than what it was trained to do. In addition,
such a capability at test-time also has the potential to unlock new avenues in agentic and reasoning
tasks [28, 34, 47]. For instance, if pre-trained model size can be traded off for additional computation
during inference, this would enable LLM deployment in use-cases where smaller on-device models could
be used in place of datacenter scale LLMs. Automating the generation of improved model outputs by
using additional inference-time computation also provides a path towards a general self-improvement
algorithm that can function with reduced human supervision.
Prior work studying inference-time computation provides mixed results. On the one hand, some works
show that current LLMs can use test-time computation to improve their outputs [4, 8, 23, 30, 48], on
the other hand, other work shows that the effectiveness of these methods on more complex tasks such
as math reasoning remains highly limited [15, 37, 43], even though reasoning problems often require
drawing inferences about existing knowledge as opposed to new knowledge. These sorts of conflicting
findingsmotivatetheneedforasystematicanalysisofdifferentapproachesforscalingtest-timecompute.
Correspondingauthor(s):csnell22@berkeley.edu
4202
guA
6
]GL.sc[
1v41330.8042:viXraIteratively Revising Answers at Test-time
Comparing Test-time and Pretraining Compute
Compute Optimal Revisions in a FLOPs Matched Evauation
45 Majority 30 +27.8%
Best-of-N Weighted +21.6%
Compute Optimal 20 +16.7%
40 Parallel +11.8% 10
35 +3.5% +5.4%
0
30 10 -11.9%
25 20 -24.3%
30 Easy Questions
20 Medium Questions
Hard Questions -37.2%
40
21 23 25 27 <<1 ~=1 >>1
Generation Budget Ratio of Inference Tokens to Pretraining Tokens
Test-time Search Against a PRM Verifier
Comparing Test-time and Pretraining Compute
Compute Optimal Search in a FLOPs Matched Evauation
45
Majority 20 +19.1%
ORM Best-of-N Weighted
40 PRM Best-of-N Weighted 10
35 PRM Compute Optimal 0 0.0% +2.2% +2.0%
-5.6%
30 10
20
25
30 -30.6%
20 -35.6% -35.3%
40
15 E Ma es dy iu Q mu Qes ut eio sn tis ons
50 Hard Questions -52.9%
10
21 23 25 27 29 <<1 ~=1 >>1
Generation Budget Ratio of Inference Tokens to Pretraining Tokens
Figure 1 ‚à£ Summaryofourmainresults.Left:Compute-optimalscalingforiterativeself-refinement(i.e.,revisions)andsearch.On
theleft,wecomparethecompute-optimalscalingpolicyforourPaLM2-S*revisionmodelagainstbaselinesintherevisionsetting(top)andthe
PRMsearchsetting(bottom).Weseethatintherevisionscase,thegapbetweenstandardbest-of-N(e.g.‚Äúparallel‚Äù)andcompute-optimal
scalinggraduallywidens,enablingcompute-optimalscalingtooutperformbest-of-Nwith4√ólesstest-timecompute.Similarly,inthePRM
searchsetting,weobservesignificantearlyimprovementsoverbest-of-Nfromcompute-optimalscaling,nearlyoutperformingbest-of-Nwith4√ó
lesscomputeatpoints.SeeSections5and6fordetails.Right:Comparingtest-timecomputeandmodelparameterscaling.Wecompare
theperformanceofcompute-optimaltest-timescalingwithPaLM2-S*againsttheperformanceofa‚àº14√ólargerpretrainedmodelwithout
additionaltest-timecompute(e.g.greedysampling).Weconsiderthesettingwhereweexpectùëãtokensofpretrainingforbothmodelsandùëå
tokensofinference.Bytrainingalargermodel,weeffectivelymultiplytheFLOPsrequirementforbothoftheseterms.Ifweweretoapply
additionaltest-timecomputewiththesmallermodel,soastomatchthislargermodel‚ÄôsFLOPsrequirement,howwoulditcompareinterms
ofaccuracy?Weseethatfortherevisions(top)whenùëå <<ùëã,test-timecomputeisoftenpreferabletoadditionalpretraining.However,as
theinferencetopretrainingtokenratioincreases,test-timecomputeremainspreferableoneasyquestions. Whereasonharderquestions,
pretrainingispreferableinthesesettings.WealsoseeasimilartrendwithPRMsearch(bottom).SeeSection7formoredetails.
We are interested in understanding the benefits of scaling up test-time compute. Arguably the simplest
and most well-studied approach for scaling test-time computation is best-of-N sampling: sampling N
outputs in ‚Äúparallel‚Äù from a base LLM and selecting the one that scores the highest per a learned verifier
or a reward model [7, 22]. However, this approach is not the only way to use test-time compute to
improve LLMs. By modifying either the proposal distribution from which responses are obtained (for
instance,byaskingthebasemodeltoreviseitsoriginalresponses‚Äúsequentially‚Äù[28])orbyalteringhow
the verifier is used (e.g. by training a process-based dense verifier [22, 45] and searching against this
verifier), the ability scale test-time compute could be greatly improved, as we show in the paper.
To understand the benefits of scaling up test-time computation, we carry out experiments on the
challenging MATH [13] benchmark using PaLM-2 [3] models specifically fine-tuned1 to either revise
1Capability-specificfinetuningisnecessarytoinducerevisionandverificationcapabilitiesintothebasemodelonMATH
2
)%(
ycaruccA
HTAM
)%(
ycaruccA
HTAM
ycaruccA
ni tnemevorpmI
evitaleR
ycaruccA
ni
tnemevorpmI
evitaleR
)%(
etupmoC
emit-tseT
morF
)%(
etupmoC
emit-tseT
morFincorrect answers [28] (e.g. improving the proposal distribution; Section 6) or verify the correctness of
individual steps in an answer using a process-based reward model (PRM) [22, 45] (Section 5). With
both approaches, we find that the efficacy of a particular test-time compute strategy depends critically
on both the nature of the specific problem at hand and the base LLM used. For example, on easier
problems, for which the base LLM can already readily produce reasonable responses, allowing the model
toiterativelyrefineitsinitialanswerbypredictingasequenceofNrevisions(i.e.,modifyingtheproposal
distribution),maybeamoreeffectiveuseoftest-timecomputethansamplingNindependentresponsesin
parallel. Ontheotherhand,withmoredifficultproblemsthatmayrequiresearchingovermanydifferent
high-level approaches to solving the problem, re-sampling new responses independently in parallel or
deployingtree-searchagainstaprocess-basedrewardmodelislikelyamoreeffectivewaytousetest-time
computation. This finding illustrates the need to deploy an adaptive ‚Äúcompute-optimal‚Äú strategy for
scaling test-time compute, wherein the specific approach for utilizing test-time compute is selected
depending on the prompt, so as to make the best use of additional computation. We also show that a
notion of question difficulty (Section 4) from the perspective of the base LLM can be used to predict the
efficacy of test-time computation, enabling us to practically instantiate this ‚Äòcompute-optimal‚Äô strategy
givenaprompt. Byappropriatelyallocatingtest-timecomputeinthisway,weareabletogreatlyimprove
test-time compute scaling, surpassing the performance of a best-of-N baseline while only using about 4x
less computation with both revisions and search (Sections 5 and 6).
Using our improved test-time compute scaling strategy, we then aim to understand to what extent
test-time computation can effectively substitute for additional pretraining. We conduct a FLOPs-matched
comparison between a smaller model with additional test-time compute and pretraining a 14x larger
model. We find that on easy and intermediate questions, and even hard questions (depending on the
specific conditions on the pretraining and inference workload), additional test-time compute is often
preferable to scaling pretraining. This finding suggests that rather than focusing purely on scaling
pretraining, in some settings it is be more effective to pretrain smaller models with less compute,
and then apply test-time compute to improve model outputs. That said, with the most challenging
questions, we observe very little benefits from scaling up test-time compute. Instead, we find that
on these questions, it is more effective to make progress by applying additional pretraining compute,
demonstrating that current approaches to scaling test-time compute may not be 1-to-1 exchangeable
with scaling pretraining. Overall, this suggests that even with a fairly na√Øve methodology, scaling up
test-time computation can already serve to be more preferable to scaling up pretraining, with only more
improvements to be attained as test-time strategies mature. Longer term, this hints at a future where
fewer FLOPs are spent during pretraining and more FLOPs are spent at inference.
2. A Unified Perspective on Test-Time Computation: Proposer and Verifier
Wefirstunifyapproachesforusingtest-timecomputationandthenanalyzesomerepresentativemethods.
First,weviewtheuseofadditionaltest-timecomputethroughthelensofmodifyingthemodel‚Äôspredicted
distribution adaptively at test-time, conditioned on a given prompt. Ideally, test-time compute should
modifythe distributionso as to generate better outputs than na√Øvely samplingfrom theLLM itself would.
In general, there are two knobs to induce modifications to an LLM‚Äôs distribution: (1) at the input level:
sincethesecapabilitiesareabsenteveninstrongproprietaryLLMs[15,33].However,weexpectthatfutureLLMswillbemore
effectiveatverificationandrevisionduetobothincreasedscaleandtheinclusionofadditionaldatatargetedspecificallytowards
thesecapabilities[5,24,36].Thereforeinordertomakeprogresstowardsunderstandingscalingoftest-timecomputation,
wemustusemodelsfinetunedforthesecapabilities.Thatsaid,weexpectfuturemodelstobepretrainedforsuchcapabilities
directly,thereforeavoidingtheneedforcapability-specificfinetuning.
3by augmenting the given prompt with an additional set of tokens that the LLM conditions on to obtain
themodifieddistribution,or(2)at the output level: bysamplingmultiplecandidatesfromthestandard
LM and performing surgery on these candidates. In other words, we could either modify the proposal
distribution induced by the LLM itself such that it is an improvement over na√Øvely conditioning on the
promptorwecouldusesomepost-hocverifiersorscorerstoperformoutputmodifications. Thisprocess
isreminiscentofMarkovchainMonteCarlo(MCMC)[2]samplingfromacomplextargetdistributionbut
by combining a simple proposal distribution and a score function. Modifying the proposal distribution
directly by altering input tokens and using a verifier form two independent axes of our study.
Modifyingtheproposaldistribution. Onewaytoimprovetheproposaldistributionistodirectlyoptimize
themodelforagivenreasoningtaskviaRL-inspiredfinetuningmethodssuchasSTaRorReSTEM [35,50].
Notethatthesetechniquesdonotutilizeanyadditionalinputtokensbutspecificallyfinetunethemodelto
induce an improved proposal distribution. Instead, techniques such as self-critique [4, 8, 23, 30] enable
the model itself to improve its own proposal distribution at test time by instructing it to critique and
revise its own outputs in an iterative fashion. Since prompting off-the-shelf models is not effective at
enablingeffectiverevisionsattesttime,wespecificallyfinetunemodelstoiterativelyrevisetheiranswers
in complex reasoning-based settings. To do so, we utilize the approach of finetuning on on-policy data
with Best-of-N guided improvements to the model response [28].
Optimizing the verifier. Inourabstractionoftheproposaldistributionandverifier,theverifierisusedto
aggregate or select the best answer from the proposal distribution. The most canonical way to use such
a verifier is by applying best-of-N sampling, wherein we sample N complete solutions and then select
the best one according to a verifier [7]. However, this approach can be further improved by training
a process-based verifier [22], or a process reward model (PRM), which produces a prediction of the
correctnessofeachintermediatestepinansolution,ratherthanjustthefinalanswer. Wecanthenutilize
these per-step predictions to perform tree search over the space of solutions, enabling a potentially more
efficient and effective way to search against a verifier, compared to na√Øve best-of-N [6, 10, 48].
3. How to Scale Test-Time Computation Optimally
Given the unification of various methods, we would now like to understand how to most effectively utilize
test-time computation to improve LM performance on a given prompt. Concretely we wish to answer:
Problem setup
We are given a prompt and a test-time compute budget within which to solve the problem. Under
the abstraction above, there are different ways to utilize test-time computation. Each of these
methods may be more or less effective depending on the specific problem given. How can we
determine the most effective way to utilize test-time compute for a given prompt? And how well
would this do against simply utilizing a much bigger pretrained model?
When either refining the proposal distribution or searching against a verifier, there are several different
hyper-parametersthatcanbeadjustedtodeterminehowatest-timecomputebudgetshouldbeallocated.
For example, when using a model finetuned for revisions as the proposal distribution and an ORM as the
verifier, we could either spend the full test-time compute budget on generating N independent samples
in parallel from the model and then apply best-of-N, or we could sample N revisions in sequence using a
revisionmodelandthenselectthebestanswerinthesequencewithanORM,orstrikeabalancebetween
these extremes. Intuitively, we might expect ‚Äúeasier‚Äù problems to benefit more from revisions, since the
4model‚Äôs initial samples are more likely to be on the right track but may just need further refinement.
On the other hand, challenging problems may require more exploration of different high-level problem
solving strategies, so sampling many times independently in parallel may be preferable in this setting.
In the case of verifiers, we also have the option to choose between different search algorithms (e.g.
beam-search, lookahead-search, best-of-N), each of which may exhibit different properties depending on
the quality of the verifier and proposal distribution at hand. More sophisticated search procedures might
be more useful in harder problems compared to a much simpler best-of-N or majority baseline.
3.1. Test-Time Compute-Optimal Scaling Strategy
In general, we would therefore like to select the optimal allocation of our test-time compute budget
for a given problem. To this end, for any given approach of utilizing test-time compute (e.g., revisions
and search against a verifier in this paper, various other methods elsewhere), we define the ‚Äútest-time
compute-optimal scaling strategy‚Äù as the strategy that chooses hyperparameters corresponding to
a given test-time strategy for maximal performance benefits on a given prompt at test time. Formally,
define Target (ùúÉ,ùëÅ,ùëû) as the distribution over natural language output tokens induced by the model for a
given prompt ùëû, using test-time compute hyper-parameters ùúÉ, and a compute budget of ùëÅ. We would
like to select the hyper-parameters ùúÉ which maximize the accuracy of the target distribution for a given
problem. We express this formally as:
‚àó
ùúÉ ùëû,ùëé‚àó (ùëû)(ùëÅ) = argmax ùúÉ(E ùë¶‚àºTarget(ùúÉ,ùëÅ,ùëû)[1 ùë¶=ùë¶‚àó (ùëû)]), (1)
where ùë¶‚àó (ùëû) denotes the ground-truth correct response for ùëû, and ùúÉ ùëû‚àó
,ùë¶‚àó
(ùëû)(ùëÅ) represents the test-time
compute-optimal scaling strategy for the problem ùëû with compute budget ùëÅ.
3.2. Estimating Question Difficulty for Compute-Optimal Scaling
In order to effectively analyze the test-time scaling properties of the different mechanisms discussed in
Section 2 (e.g. the proposal distribution and the verifier), we will prescribe an approximation to this
optimal strategy ùúÉ ùëû‚àó
,ùë¶‚àó
(ùëû)(ùëÅ) as a function of a statistic of a given prompt. This statistic estimates a notion
of difficulty for a given prompt. The compute-optimal strategy is defined as a function of the difficulty of
this prompt. Despite being only an approximate solution to the problem shown in Equation 1, we find
thatitcanstillinducesubstantialimprovementsinperformanceoverabaselinestrategyofallocatingthis
inference-time compute in an ad-hoc or uniformly-sampled manner.
Ourestimateofthequestiondifficultyassignsagivenquestiontooneoffivedifficultylevels. Wecanthen
use this discrete difficulty categorization to estimate ùúÉ ùëû‚àó
,ùë¶‚àó
(ùëû)(ùëÅ) on a validation set for a given test-time
compute budget. We then apply these compute-optimal strategies on the test-set. Concretely, we select
thebestperformingtest-timecomputestrategyforeachdifficultybinindependently. Inthisway,question
difficulty acts as a sufficient statistic of a question when designing the compute-optimal strategy.
Defining difficulty of a problem. Following the approach of Lightman et al. [22], we define question
difficulty as a function of a given base LLM. Specifically, we bin the model‚Äôs pass@1 rate ‚Äì estimated
from2048samples‚Äìoneachquestioninthetestsetintofivequantiles,eachcorrespondingtoincreasing
difficultylevels. Wefoundthisnotionofmodel-specificdifficultybinstobemorepredictiveoftheefficacy
of using test-time compute in contrast to the hand-labeled difficulty bins in the MATH dataset.
That said, we do note that assessing a question‚Äôs difficulty as described above assumes oracle access to a
ground-truth correctness checking function, which is of course not available upon deployment where we
5areonlygivenaccesstotestpromptsthatwedon‚Äôtknowtheanswerto. Inordertobefeasibleinpractice,
a compute-optimal scaling strategy conditioned on difficulty needs to first assess difficulty and then
utilizetherightscalingstrategytosolvethisproblem. Therefore,weapproximatetheproblem‚Äôsdifficulty
via a model-predicted notion of difficulty, which performs the same binning procedure over the the
averaged final answer score from a learned verifier (and not groundtruth answer correctness checks) on
the same set of 2048 samples per problem. We refer to this setting as model-predicted difficulty and
the setting which relies on the ground-truth correctness as oracle difficulty.
While model-predicted difficulty removes the need for need knowing the ground truth label, estimating
difficulty in this way still incurs additional computation cost during inference. That said, this one-time
inference cost can be subsumed within the cost for actually running an inference-time strategy (e.g.,
when using a verifier, one could use the same inference computation for also running search). More
generally,thisisakintoexploration-exploitationtradeoffinreinforcementlearning: inactualdeployment
conditions, we must balance the compute spent in assessing difficulty vs applying the most compute-
optimal approach. This is a crucial avenue for future work (see Section 8) and our experiments do not
account for this cost largely for simplicity, since our goal is to present some of the first results of what is
in fact possible by effectively allocating test-time compute.
So as to avoid confounders with using the same test set for computing difficulty bins and for selecting
the compute-optimal strategy, we use two-fold cross validation on each difficulty bin in the test set. We
selectthebest-performingstrategy accordingtoperformanceonone fold andthenmeasureperformance
using that strategy on the other fold and vice versa, averaging the results of the two test folds.
4. Experimental Setup
We first outline our experimental setup for conducting this analysis with multiple verifier design choices
and proposal distributions, followed by the analysis results in the subsequent sections.
Datasets. We expect test-time compute to be most helpful when models already have all the basic
‚Äúknowledge‚Äùneededtoansweraquestion,andinsteadtheprimarychallengeisaboutdrawing(complex)
inferences from this knowledge. To this end, we focus on the MATH [13] benchmark, which consists of
high-schoolcompetitionlevelmathproblemswitharangeofdifficultylevels. Forallexperiments,weuse
the dataset split consisting of 12k train and 500 test questions, used in Lightman et al. [22].
Models. We conduct our analysis using the PaLM 2-S* [3] (Codey) base model. We believe this model
is representative of the capabilities of many contemporary LLMs, and therefore think that our findings
likelytransfertosimilarmodels. Mostimportantly,thismodelattainsanon-trivialperformanceonMATH
and yet has not saturated, so we expect this model to provide a good test-bed for us.
5. Scaling Test-Time Compute via Verifiers
In this section we analyze how test-time compute can be scaled by optimizing a verifier, as effectively as
possible. Tothisend,westudydifferentapproachesforperformingtest-timesearchwithprocessverifiers
(PRMs) and analyze the test-time compute scaling properties of these different approaches.
5.1. Training Verifiers Amenable to Search
PRM training. Originally PRM training [22, 42] used human crowd-worker labels. While Lightman
etal.[22]releasedtheirPRMtrainingdata(i.e., thePRM800kdataset),wefoundthisdatatobelargely
6ineffective for us. We found that it was easy to exploit a PRM trained on this dataset via even na√Øve
strategies such as best-of-N sampling. We hypothesize that this is likely a result of the distribution shift
between the GPT-4 generated samples in their dataset and our PaLM 2 models. Rather than proceeding
with the expensive process of collecting crowd-worker PRM labels for our PaLM 2 models, we instead
apply the approach of Wang et al. [45] to supervise PRMs without human labels, using estimates of
per-step correctness obtained from running Monte Carlo rollouts from each step in the solution. Our
PRM‚Äôs per-step predictions therefore correspond to value estimates of reward-to-go for the base model‚Äôs
sampling policy, similar to recent work [31, 45]. We also compared to an ORM baseline (Appendix F)
but found that our PRM consistently outperforms the ORM. Hence, all of the search experiments in this
section use a PRM model. Additional details on PRM training are shown in Appendix D.
Answer aggregation. At test time, process-based verifiers can be used to score each individual step in a
set of solutions sampled from the base model. In order to select the best-of-N answers with the PRM, we
need a function that can aggregate across all the per-step scores for each answer to determine the best
candidate for the correct answer. To do this, we first aggregate each individual answer‚Äôs per-step scores
to obtain a final score for the full answer (step-wise aggregation). We then aggregate across answers to
determine the best answer (inter-answer aggregation). Concretely, we handle step-wise and inter-answer
aggregation as follows:
‚Ä¢ Step-wise aggregation. Rather than aggregating the per-step scores by taking the product or
minimum [22, 45], we instead use the PRM‚Äôs prediction at the last step as the full-answer score.
We found this to perform the best out of all aggregation methods we studied (see Appendix E).
‚Ä¢ Inter-answeraggregation. WefollowLietal.[21]andapply‚Äúbest-of-Nweighted‚Äùselectionrather
than standard best-of-N. Best-of-N weighted selection marginalizes the verifier‚Äôs correctness scores
across all solutions with the same final answer, selecting final answer with the greatest total sum.
5.2. Search Methods Against a PRM
We optimize the PRM at test time via search methods. We study three search approaches that sample
outputs from a few-shot prompted base LLM (see Appendix G). An illustration is shown in Figure 2.
Best-of-N weighted. We sample N answers independently from the base LLM and then select the best
answer according to the PRM‚Äôs final answer judgement.
Beam search. BeamsearchoptimizesthePRMbysearchingoveritsper-steppredictions. Ourimplemen-
tationissimilartoBFS-V[10,48]. Concretely,weconsiderafixednumberofbeams ùëÅ andabeamwidth
ùëÄ. We then run the following steps:
1. sample ùëÅ initial predictions for the first step in the solution
2. scorethegeneratedstepsaccordingtothePRM‚Äôspredictedstep-wisereward-to-goestimate(which
also corresponds to the total reward from the prefix since the reward is sparse in this setting)
3. filter for only the top ùëÅ highest scoring steps
ùëÄ
4. now from each candidate, sample ùëÄ proposals from the next step, resulting in a total of ùëÅ/ùëÄ √óùëÄ
candidate prefixes again. Then repeat steps 2-4 again.
We run this algorithm until the end of a solution or the maximum number of rounds of beam expansion
are attained (40 in our case). We conclude the search with N final answer candidates, to which we apply
best-of-N weighted selection described above to make our final answer prediction.
7Best-of-N Beam Search Lookahead Search
Beam search, but at each step
Question G s ve ee l rn e ife c ier tia rnt ge tN h efu bll e s so t l ou nti eo n ws i, t h the Question S a Pte R el Me ac ct h t h se te t po p u- sN in s ga tm hp e les Question r t r th ho oe el ll l o oP cu u uRt t r M rk to e- s v n rt a tee l spup tres ee psain et nta h td e tv h a e en n vc d ae o l, u fu ets h fi en o g r
Propagate
PRM value
R k-o sl tl eo pu st b sta ec pk to
Continue Search from
the top-N options
Select the best final answer using the verifier Select the best final answer using the verifier
Key:
= Apply Verifier = Full Solution = Intermediate solution step = Selected by verifier = Rejected by verifier
Figure 2 ‚à£ ComparingdifferentPRMsearchmethods. Left: Best-of-NsamplesNfullanswersandthenselectsthebest
answeraccordingtothePRMfinalscore. Center: BeamsearchsamplesNcandidatesateachstep, andselectsthetopM
accordingtothePRMtocontinuethesearchfrom.Right:lookahead-searchextendseachstepinbeam-searchtoutilizeak-step
lookaheadwhileassessingwhichstepstoretainandcontinuethesearchfrom.Thuslookahead-searchneedsmorecompute.
Lookahead search. Lookahead search modifies how beam search evaluates individual steps. It uses
lookahead rollouts to improve the accuracy of the PRM‚Äôs value estimation in each step of the search
process. Specifically,ateachstepinthebeamsearch,ratherthanusingthePRMscoreatthecurrentstep
to select the top candidates, lookahead search performs a simulation, rolling out up to ùëò steps further
while stopping early if the end of solution is reached. To minimize variance in the simulation rollout,
we perform rollouts using temperature 0. The PRM‚Äôs prediction at the end of this rollout is then used
to score the current step in the beam search. That is, in other words, we can view beam search as a
special case of lookahead search with ùëò = 0. Given an accurate PRM, increasing ùëò should improve the
accuracy of the per-step value estimates at the cost of additional compute. Also note that this version of
lookahead search is a special case of MCTS [38], wherein the stochastic elements of MCTS, designed
to facilitate exploration, are removed since the PRM is already trained and is frozen. These stochastic
elements are largely useful for learning the value function (which we‚Äôve already learned with our PRM),
but less useful at test-time when we want to exploit rather than explore. Therefore, lookahead search is
largely representative of how MCTS-style methods would be applied at test-time.
5.3. Analysis Results: Test-Time Scaling for Search with Verifiers
We now present our results comparing various search algorithms and identify a prompt difficulty depen-
dent compute-optimal scaling strategy for search methods.
Comparing search algorithms. We first conduct a sweep over various search settings. In addition to the
standard best-of-N approach, we sweep over the two main parameters that distinguish different tree-
search methods: beam-width ùëÄ and number of lookahead steps ùëò. While we are not able to extensively
sweep every single configuration, we sweep over the following settings with a maximum budget of 256:
‚àö
1) Beam search with the beam width set to ùëÅ, where ùëÅ is the generation budget.
2) Beam search with a fixed beam width of 4.
8
‚Ä¶Comparing PRM Search Methods Comparing Beam Search and Best-of-N by Difficulty Level
40
Beam Search
Best-of-N Weighted
80
35 Majority
30 60
25
40
Best-of-N Weighted 20
Majority
Beam; M := sqrt(N)
Beam; M := 4 20
15 1 Step Lookahead; M := sqrt(N)
3 Step Lookahead; M := sqrt(N)
3 Step Lookahead; M := 4
10 0
21 23 25 27 29 1 2 3 4 5
Generation Budget Test Questions Binned by Increasing Difficulty Level
Figure 3 ‚à£ Left:ComparingdifferentmethodsforconductingsearchagainstPRMverifiers.Weseethatatlowgeneration
budgets,beamsearchperformsbest,butaswescalethebudgetfurthertheimprovementsdiminish,fallingbelowthebest-of-N
baseline.Lookahead-searchgenerallyunderperformsothermethodsatthesamegenerationbudget.Right:Comparingbeam
searchandbest-of-Nbinnedbydifficultylevel.Thefourbarsineachdifficultybincorrespondtoincreasingtest-timecompute
budgets(4,16,64,and256generations).Ontheeasierproblems(bins1and2),beamsearchshowssignsofover-optimization
withhigherbudgets,whereasbest-of-Ndoesnot. Onthemediumdifficultyproblems(bins3and4),weseebeamsearch
demonstratingconsistentimprovementsoverbest-of-N.
3) Lookahead search with ùëò = 3 applied to both beam-search settings 1) and 2).
4) Lookahead search with ùëò = 1 applied to beam-search setting 1).
To compare search methods as a function of generation budget fairly, we build a protocol for estimating
the cost of each method. We consider a generation to be a sampled answer from the base LLM. For beam
search and best-of-N the generation budget corresponds to the number of beams and ùëÅ respectively.
Lookaheadsearch,however,utilizesadditionalcompute: ateachstepofthesearch,wesampleùëòadditional
steps ahead. Therefore, we define the cost of lookahead-search to be ùëÅ √ó (ùëò+1 ) samples.
Results. As shown in Figure 3 (left), with smaller generation budgets, beam search significantly out-
performs best-of-N. However, as the budget is scaled up, these improvements greatly diminish, with
beam search often underperforming the best-of-N baseline. We also see that, lookahead-search generally
underperforms other methods at the same generation budget, likely due to the additional computation
inducted by simulating the lookahead rollouts. The diminishing returns from search are likely due to
exploitation of the PRM‚Äôs predictions. For example, we see some instances (such as in Figure 29), where
search causes the model to generate low-information repetitive steps at the end of a solution. In other
cases, we find that over-optimizing search can result in overly short solutions consisting of just 1-2 steps.
This explains why the most powerful search method (i.e., lookahead search) underperforms the most.
We include several of these examples found by search in Appendix M.
Whichproblemsdoessearchimprove? Tounderstandhowtocompute-optimallyscalesearchmethods,
wenowconductadifficultybinanalysis. Specifically,wecomparebeam-search(ùëÄ = 4)againstbest-of-N.
In Figure 3 (right) we see that while in aggregate, beam search and best-of-N perform similarly with a
high generation budget, evaluating their efficacy over difficulty bins reveals very different trends. On the
easy questions (levels 1 and 2), the stronger optimizer of the two approaches, beam search, degrades
performance as the generation budget increases, suggesting signs of exploitation of the PRM signal. In
contrast, on the harder questions (levels 3 and 4), beam search consistently outperforms best-of-N. On
the most difficult questions (level 5), no method makes much meaningful progress.
9
)%(
ycaruccA
tseT
HTAM
)%(
ycaruccA
tseT
HTAMThese findings match intuition: we might expect that on the easy questions, the verifier will make mostly
correctassessmentsofcorrectness. Therefore,byapplyingfurtheroptimizationviabeamsearch,weonly
further amplify any spurious features learned by the verifier, causing performance degredation. On the
moredifficultquestions,thebasemodelismuchlesslikelytosamplethecorrectanswerinthefirstplace,
so search can serve to help guide the model towards producing the correct answer more often.
Compute-optimal search. Given the above re- Compute Optimal Search
sults, it is clear that question difficulty can be a 40
usefulstatistictopredicttheoptimalsearchstrat-
35
egy to use at a given compute budget. Addition-
ally, the best choice of search strategy can vary
30
drastically as a function of this difficulty statis-
tic. We therefore visualize the ‚Äúcompute-optimal‚Äù 25
scalingtrend,asrepresentedbythebestperform- 20
ing search strategy at each difficulty level in Fig- Majority
ORM Best-of-N Weighted
ure 4. We see that in the low generation budget 15 PRM Best-of-N Weighted
PRM Compute Optimal Oracle
regime, using both the oracle and predicted diffi- PRM Compute Optimal Predicted
10
culty, compute-optimal scaling can nearly out-
21 23 25 27 29
Generation Budget
perform best-of-N using up to4xless test-time Figure 4
‚à£ Comparingcompute-optimaltest-timecompute
compute (e.g. 16 verses 64 generations). While allocationagainstbaselineswithPRMsearch.Byscalingtest
in the higher budget regime, some of these ben- timecomputeperthenotionofquestiondifficulty,wefindthat
efits diminish with the use of predicted difficulty, wecannearlyoutperformPRMbest-of-Nusingupto4√óless
test-timecompute(e.g.16verses64generations).‚ÄúCompute-
with oracle bins we still see continued improve-
optimal oracle‚Äù refers to usingoracledifficulty binsderived
ments from optimally scaling test-time compute.
fromthegroundtruthcorrectnessinformation,and‚Äúcompute-
This result demonstrates the performance gains optimal predicted‚Äù refers to using the PRM‚Äôs predictions to
that could be obtained by adaptively allocating generatedifficultybins.Observethatthecurveswitheithertype
ofdifficultybinslargelyoverlapwitheachother.
test-time compute during search.
Takeaways for compute-optimal scaling of verifiers
Wefindthattheefficacyofanygivenverifiersearchmethoddependscriticallyonboththecompute
budget and the question at hand. Specifically, beam-search is more effective on harder questions
andatlowercomputebudgets,whereasbest-of-Nismoreeffectiveoneasierquestionsandathigher
budgets. Moreover,byselectingthebestsearchsettingforagivenquestiondifficultyandtest-time
compute budget, we can nearly outperform best-of-N using up to 4x less test-time compute.
6. Refining the Proposal Distribution
So far, we studied the test-time compute scaling properties of search against verifiers. Now we turn
to studying the scaling properties of modifying the proposal distribution (Section 2). Concretely, we
enable the model to revise their own answers iteratively, allowing the model to dynamically improve it‚Äôs
own distribution at test time. Simply prompting existing LLMs to correct their own mistakes tends to be
largely ineffective for obtaining performance improvements on reasoning problems [15]. Therefore, we
build on the recipe prescribed by Qu et al. [28], incorporate modifications for our setting, and finetune
language models to iteratively revise their own answers. We first describe how we train and use models
that refine their own proposal distribution by sequentially conditioning on their own previous attempts
at the question. We then analyze the inference-time scaling properties of revision models.
10
)%(
ycaruccA
tseT
HTAMParallel Sampling
U Ins fein rg
en
R ce ev Tis imio en Model + Verifier at K e y :
= Apply Verifier = Selected by verifier = Rejected by verifier
A: So 7/4 yap/dap ‚Ä¶ Parallel Best-of-N Sequential Revisions
Verifier selects
Q ya: p I sf ,4 a d na dp 5s = 7 LM proposes answers the best answer
yaps = 3 baps, LM A: We have 4 dap‚Ä¶ independently, in Verifier
h eqo uw a m l 4a 2n by ad pa sp ?s parallel selects Question
the best
answer
A: If 7/4 yaps/dap ...
‚Ä¶
Combining Sequential / Parallel
Question
Sequential Revisions
Verifier
LM proposes a sequence of revisions, each selects the
conditioned on previous revisions Verifier selects the best best answer
Question answer within each chain across chains
Q: If 4 daps = 7
yaps, and 5
yaps = 3 baps, LM A: We ‚Ä¶ A: So ‚Ä¶ A: If 7/4 ...
how many daps
equal 42 baps?
Figure 5 ‚à£ Parallel sampling (e.g., Best-of-N) verses sequential revisions. Left: ParallelsamplinggeneratesNanswers
independentlyinparallel,whereassequentialrevisionsgenerateseachoneinsequenceconditionedonpreviousattempts.Right:
Inboththesequentialandparallelcases,wecanusetheverifiertodeterminethebest-of-Nanswers(e.g.byapplyingbest-of-N
weighted).Wecanalsoallocatesomeofourbudgettoparallelandsometosequential,effectivelyenablingacombinationofthe
twosamplingstrategies.Inthiscase,weusetheverifiertofirstselectthebestanswerwithineachsequentialchainandthen
selectthebestansweraccrosschains.
6.1. Setup: Training and Using Revision Models
Our procedure for finetuning revision models is similar to [28], though we introduce some crucial
differences. For finetuning, we need trajectories consisting of a sequence of incorrect answers followed
by a correct answer, that we can then run SFT on. Ideally, we want the correct answer to be correlated
with the incorrect answers provided in context, so as to effectively teach the model to implicitly identify
mistakes in examples provided in-context, followed by correcting those mistakes by making edits as
opposed to ignoring the in-context examples altogether, and trying again from scratch.
Generating revision data. The on-policy approach of Qu et al. [28] for obtaining several multi-turn
rollouts was shown to be effective, but it was not entirely feasible in our infrastructure due to compute
costs associated with running multi-turn rollouts. Therefore, we sampled 64 responses in parallel at
a higher temperature and post-hoc constructed multi-turn rollouts from these independent samples.
Specifically, following the recipe of [1], we pair up each correct answer with a sequence of incorrect
answers from this set as context to construct multi-turn finetuning data. We include up to four incorrect
answersincontext,wherethespecificnumberofsolutionsincontextissampledrandomlyfromauniform
distributionovercategories0to4. Weuseacharactereditdistancemetrictoprioritizeselectingincorrect
answers which are correlated with the final correct answer (see Appendix H). Note that token edit
distance is not a perfect measure of correlation, but we found this heuristic to be sufficient to correlate
incorrect in-context answers with correct target answers to facilitate training a meaningful revision
model, as opposed to randomly pairing incorrect and correct responses with uncorrelated responses.
Using revisions at inference-time. Given a finetuned revision model, we can then sample a sequence of
revisions from the model at test-time. While our revision model is only trained with up to four previous
answersin-context,wecansamplelongerchainsbytruncatingthecontexttothemostrecentfourrevised
responses. InFigure6(left),weseethataswesamplelongerchainsfromtherevisionmodel,themodel‚Äôs
pass@1ateachstepgraduallyimproves,demonstratingthatweareabletoeffectivelyteachthemodelto
learn from mistakes made by previous answers in context.
11Revision Model Pass@1 At Each Step Revision Model Parallel Verses Sequential
26
Sequential Best-of-N Weighted
25 40 Parallel Best-of-N Weighted
Sequential Majority
24 Parallel Majority
35
23
22
30
21
20 25
19
18 20
17
0 10 20 30 40 50 60 20 21 22 23 24 25 26
Number of Generations Number of Generations
Figure 6 ‚à£ Left:Ourrevisionmodel‚Äôspass@1ateachrevisionstep.Pass@1graduallyimprovesaftereachrevisionstep,
evenimprovingbeyondthe4revisionstepsthatitwastrainedfor.Weestimatepass@1ateachstepbybyaveragingoverthe
performanceof4revisiontrajectoriesoflength64foreachquestioninthetest-set.Right:Sequentialvsparallelsampling
fromtherevisionmodel.ComparingperformancewhengeneratingNinitialanswersinparallelfromourrevisionmodel,verses
generatingNrevisionssequentially,withthemodel.Whenusingboththeverifierandmajorityvotingtoselecttheanswer,we
seethatgeneratinganswerssequentiallywiththerevisionmodelnarrowlyoutperformsgeneratingtheminparallel.
That said, there is a distribution shift at inference time: the model was trained on only sequences with
incorrect answers in context, but at test-time the model may sample correct answers that are included
in the context. In this case, it may incidentally turn the correct answer into an incorrect answer in the
next revision step. We find that indeed, similar to Qu et al. [28], around 38% of correct answers get
converted back to incorrect ones with our revision model using a na√Øve approach. Therefore, we employ
a mechanism based on sequential majority voting or verifier-based selection to select the most correct
answer from the sequence of revisions made by the model (see Figure 5) to produce the best answer.
Comparisons. To test the efficacy of modifying the proposal distribution via revisions, we setup an even
comparison between the performance of sampling N revisions in sequence and sampling N attempts at
a question in parallel. We see in Figure 6 (right), that with both the verifier-based and majority-based
selection mechanisms sampling solutions in sequence outperforms sampling them in parallel.
6.2. Analysis Results: Test-Time Scaling with Revisions
Wesawpreviouslythatproposinganswerssequentiallyoutperformsproposingtheminparallel. However,
we might expect sequential and parallel sampling to have different properties. Sampling answers in
parallel may act as more of a global search process, that could in principle, provide coverage over many
totallydifferentapproachesforsolvingaproblem,forinstance,differentcandidatesmightutilizedifferent
high-level approaches altogether. Sequential sampling, on the other hand, may work more as a local
refinement process, revising responses that are already somewhat on the right track. Due to these
complementary benefits, we should strike a balance between these two extremes by allocating some of
‚àö ‚àö
our inference-time budget to parallel sampling (e.g. ùëÅ) and the rest to sequential revisions (e.g. ùëÅ).
We will now show the existence of a compute-optimal ratio between sequential and parallel sampling,
and understand their relative pros and cons based on difficulty of a given prompt.
Trading off sequential and parallel test-time compute. To understand how to optimally allocate
sequentialandparallelcompute,weperformasweepoveranumberofdifferentratios. Wesee,inFigure7
(left), that indeed, at agiven generationbudget, there exists an ideal sequential to parallel ratio, that
12
)%(
ycaruccA
tseT
HTAM
)%(
ycaruccA
tseT
HTAMVarying Sequential/Parallel with Verifier Revisions@128, Varying the Sequential to Parallel Ratio
45
102
40 102 80
101
35
60
30 100
101 40
25
101
20
20
102
15
27 25 23 21 21 23 25 27
100 0
1 2 3 4 5
Test Questions Binned by Increasing Difficulty Level
Sequential/Parallel Ratio
Figure 7 ‚à£ Left:Varyingtheratioofthegenerationbudgetallocatedsequentialrevisionstoversesparallelsamples.Each
linerepresentsafixedgenerationbudgetastheratioischanged.Weusetheverifierforanswerselection.Weseethatwhile
increasedsequentialrevisionstendstooutperformmoreparallelcompute,athighergenerationbudgetsthereisanidealratio
thatstrikesabalancebetweenthetwoextremes.Right:Varyingthesequentialtoparallelratioforagenerationbudgetof
128acrossdifficultybins.Usingverifier-basedselection,weseethattheeasierquestionsattainthebestperformancewithfull
sequentialcompute.Ontheharderquestions,thereisanidealratioofsequentialtoparalleltest-timecompute.
achieves the maximum accuracy. We also see in Figure 7 (right) that the ideal ratio of sequential to
parallel varies depending on a given question‚Äôs difficulty. In particular, easy questions benefit more from
sequential revisions, whereas on difficult questions it is optimal to strike a balance between sequential
andparallelcomputation. Thisfindingsupportsthehypothesisthatsequentialrevisions(i.e.,varyingthe
proposal distribution) and parallel sampling (i.e., search with verifiers) are two complementary axes for
scaling up test-time compute, which may be more effective on a per-prompt basis. We include examples
of our model‚Äôs generations in Appendix L. Additional results are shown in Appendix B.
Compute-optimal revisions. Given that the effi- Compute Optimal Revisions
cacy of sequential and parallel sampling depends 45
Majority
onquestiondifficulty,wecanselecttheidealratio Best-of-N Weighted
Compute Optimal Oracle
ofsequentialtoparallelcomputeperdifficultybin. 40 Compute Optimal Predicted
Parallel
In Figure 8, we plot results using this compute-
35
optimalscalingstrategywhenemployingbothour
oracle and predicted notions of difficulty. In both
30
cases, we‚Äôre able to substantially improve test-
time compute scaling by improving the proposal 25
distribution via revisions. In particular, we see
20
that at higher generation budgets, parallel sam-
plingseemstoplateau,whereascompute-optimal 21 23 25 27
scaling demonstrates continued improvements. Generation Budget
Figure 8
For both oracle and predicted difficulty bins, we ‚à£ Comparingcompute-optimaltest-timecompute
allocationagainsttheparallelcomputebaselinewithourre-
see that compute-optimal scaling can outper- visionmodel.Byoptimallyscalingtest-timecomputeaccording
form best-of-N using up to 4x less test-time toquestiondifficulty,wefindthatwecanoutperformbest-of-N
compute (e.g. 64 samples verses 256). Overall, usingupto4xlesstest-timecompute(e.g. 64samplesverses
these results demonstrate the potential for im- 256). ‚ÄúCompute-optimal oracle‚Äù refers to using the oracle
difficultybinsderivedfromthegroundtruthcorrectnessinfor-
provedtest-timecomputescalingbyadjustingthe
mation,and‚Äúcomputeoptimalpredicted‚Äùreferstousingthe
proposal distribution on a per-prompt basis.
PRM‚Äôspredictionstoproducemodel-predicteddifficultybins.
13
)%(
ycaruccA
tseT
HTAM
snoitareneG
fo
rebmuN
)%(
ycaruccA
tseT
HTAM
)%(
ycaruccA
tseT
HTAM
oitaR
lellaraP
ot
laitneuqeSTakeaways for compute-optimal scaling by refining the proposal distribution with revisions
We find that there exists a tradeoff between sequential (e.g. revisions) and parallel (e.g. standard
best-of-N) test-time computation, and the ideal ratio of sequential to parallel test-time compute
depends critially on both the compute budget and the specific question at hand. Specifically,
easier questions benefit from purely sequential test-time compute, whereas harder questions often
perform best with some ideal ratio of sequential to parallel compute. Moreover, by optimally
selecting the best setting for a given question difficulty and test-time compute budget, we can
outperform the parallel best-of-N baseline using up to 4x less test-time compute.
7. Putting it Together: Exchanging Pretraining and Test-Time Compute
So far, we saw that utilizing additional test-time computation can enable us to represent more complex
distributions than the one predicted by the base LLM itself, thereby improving performance. We now
posit that this increased flexibility of representing distributions means that we can expect additional
test-time compute to make up for the lack of a higher-capacity model or training for more FLOPs during
pre-training. In this section, we study to what extent this is possible. We pose the following question:
Question: Exchanging pretraining and test-time compute
Suppose a model was pre-trained with ùëã FLOPs. Assume that we plan to runùëå FLOPs of inference
with this model. If we want to improve performance by increasing the total FLOPs budget by a
factor of ùëÄ (i.e., ùëÄ(ùëã +ùëå) total FLOPs across both pretraining and inference), should we spend
our FLOPs on increased pretraining compute or on additional test-time compute?
IncreasingpretrainingFLOPSintroducestheadditionaldesigndecisionofwhethertoallocatecomputeto
trainingwithmoredataormoreparameters[14]. Wefocusonthesettinginwhichmodelparametersare
scaled up and training data amount is fixed, matching the approach taken with the open-source LLaMA
series of models [41]. We choose this setting as it is representative of a canonical approach to scaling
pretraining compute and leave the analysis of compute-optimal scaling of pretraining compute [29]
where the data and parameters are both scaled equally to future work.
DefininganexchangeratebetweenFLOPs. Wenowdescribehowwedefinetheexchangeratebetween
pretraining and inference FLOPs. To determine pretraining FLOPs, use use the common approximation
ùëã = 6ùëÅùê∑ [14], and for inference FLOPs, we useùëå = 2ùëÅùê∑ [29]. Here ùëÅ represents model
pretrain inference
parameters, ùê∑ is the number of tokens used for pretraining, and ùê∑ the total number of
pretrain inference
tokensgeneratedatinferencetime. Withtheseapproximations,wecanseethat,ifwemultiplythemodel
parameters by a factor of ùëÄ, then both the pretraining and inference FLOPs (due to the cost of greedy
decoding with the larger model), increase by a factor of ùëÄ (giving ùëÄ(ùëã +ùëå) total FLOPs).
To match the FLOPs from scaling up model paramters using test-time compute with the smaller model
we can multiply the smaller model‚Äôs inference compute by a factor of ùëÄ +3 (
ùê∑pretrain
)(ùëÄ ‚àí1 ). Notably,
ùê∑inference
the amount of inference compute we can utilize to match the FLOPs for the larger model depends on
the ratio ùê∑pretrain . We refer to the inverse of this ratio as ùëÖ (e.g. ùê∑inference). Depending on the specific
ùê∑inference ùê∑pretrain
production setting or use-case, we should expect very different values of ùëÖ. In particular, in many large
scale production settings, we may expect significantly more inference tokens than pretraining tokens, in
which case we would have ùëÖ >> 1. On the other hand, in many contemporary self-improvement setups,
that would use test-time compute to improve the model, we would likely generate significantly fewer
14Comparing Test-time and Pretraining Compute
Revisions PRM Search
100 1
80
80
2
60
60
3
40
40
4
20 20
5
0 1 2 3 4 5 6 7 8 0 1 2 3 4 5 6 7 8
2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2
Proportional to Inference FLOPs Proportional to Inference FLOPs
Pretraining Compute Test-time Compute R >> 1 R ~= 1 R << 1
Figure 9 ‚à£ Tradeoffbetweenpretrainingandtest-timecomputeinaFLOPs-matchedevaluation.Eachlinerepresentsthe
performanceofscalingtest-timecomputewithourcompute-optimalpolicyineachoracledifficultybin.Weplottheresultsfor
revisionsontheleftandsearchontheright.Thestarsrepresentthegreedypass@1performanceofabasemodelpretrained
with‚àº 14timesmoreparameters. Weplottest-timecomputebudgetonthex-axis, andplacethestarsatthreedifferent
locationsalongthex-axis,eachcorrespondingtotheFLOPsequivalentpointofcomparisonbetweenscalingparametersand
scalingtest-timecomputeforthreedifferentinferencecomputeloads(e.g. ùëÖ = ùê∑inference). Ifthestarisbelowtheline,this
ùê∑pretrain
impliesthatitismoreeffectivetousetest-timecomputethantoscalemodelparameters,andifthestarisabovethelinethis
impliesthatscalingparametersismoreeffective.Weseethatontheeasyquestionsorinsettingswithalowerinferenceload
(e.g. ùëÖ<<1),test-timecomputecangenerallyoutperformscalingmodelparameters.However,ontheharderquestionsorin
settingswithahigherinferenceload(e.g. ùëÖ>>1),pretrainingisamoreeffectivewaytoimproveperformance.
inference tokens than pretraining tokens, giving ùëÖ << 1. Therefore, since the scale of test-time compute
wecanapplyisdependentonthisratio,weexpectdifferingconclusionsdependingonthespecificsetting.
In Figure 9, we use this approach to exchanging test-time and pretraining compute to compare our
compute-optimalscalingagainstscalingupmodelparametersbyafactorof‚àº 14. Weconductcomparisons
for 3 different values of R: 0.16 (ùëÖ << 1), 0.79 (ùëÖ ‚àº 1), and 22 (ùëÖ >> 1), with each ratio corresponding
to an inference budget. Observe that if we only expect to see very difficult questions (e.g. difficulty
bins 4/5) or have a larger ùê∑ (corresponding to a larger ùëÖ value), then it is often more effective to
inference
allocate our budget towards pretraining (e.g. the star is above the line). If instead, we expect mostly
easy or intermediate difficulty questions (e.g. bins 1/2/3 and sometimes 4) or have lower inference
requirements (as is the case in self-improvement pipelines), then utilizing test-time compute is better.
Takeaways for exchanging pretraining and test-time compute
Test-time and pretraining compute are not 1-to-1 ‚Äúexchangeable‚Äù. On easy and medium questions,
which are within a model‚Äôs capabilities, or in settings with small inference requirement, test-time
compute can easily cover up for additional pretraining. However, on challenging questions which
are outside a given base model‚Äôs capabilities or under higher inference requirement, pretraining is
likely more effective for improving performance.
8. Discussion and Future Work
In this work, we conducted a thorough analysis of the efficacy of different techniques that aim to either
improvesearchagainstaverifierortorefineanLLM‚Äôsproposaldistribution,forscalingtest-timecompute
15
)%(
ycaruccA
leveL
ytluciffiD
HTAM
)%(
ycaruccA
leveL
ytluciffiD
HTAM
leveL
ytluciffiDfor math reasoning. In general, we found that the efficacy of a given approach heavily correlates with
the difficulty of the problem from the perspective of the base LLM‚Äôs capabilities. This motivated us to
introduce the notion of ‚Äúcompute-optimal‚Äù scaling of test-time computation, which prescribes a adaptive,
prompt-dependentstrategytoimproveperformanceunderagiventest-timecomputebudget. Byapplying
such a compute-optimal scaling strategy, we find that can improve the efficiency of test-time compute
scaling by a factor of 2‚àí4√ó. When comparing benefits obtained from additional test-time compute
against benefits from additional pre-training compute in a FLOPs-matched setting, we show for the first
time that using test-time computation with seemingly simple methods (i.e., revisions and search) can
already scale well on certain types of prompts, providing gains over spending those FLOPs in pretraining.
That said, there are also limitations associated with our study that future work can aim to address.
Further improving test-time compute scaling. In this work we focused on improving the test-time
compute scaling of two primary mechanisms: the verifier and the proposal distribution (via revisions).
While we combined verifiers with revisions in Section 6, we did not experiment with PRM tree-search
techniques in combination with revisions. Neither did we study other techniques such as critique and
revise [23]. Future work should investigate how test-time compute scaling can be further improved by
combining a variety of these approaches. Additionally, we found that across the board these schemes
provided small gains on hard problems; future work should work to develop new ways of using test-time
compute which can circumvent this limitation.
Assessing question difficulty quickly. Weusedanotionquestiondifficultyasasimplesufficientstatistic
for approximating the compute-optimal test-time scaling strategy. While this scheme was effective,
estimating our notion of difficulty requires applying a non-trivial amount of test-time compute itself.
Future work should consider alternative ways of more efficiently estimating question difficulty (e.g., by
pretraining or finetuning models to directly predict difficulty of a question) or dynamically switching
between assessing difficulty and attempting to solve a question.
Interleaving test-time and training-time compute. We focused purely on test-time compute scaling
in this work and the degree to which test-time compute can be traded off for additional pretraining.
However, in the future, we envision that the outputs of applying additional test-time compute can be
distilledbackintothebaseLLM,enablinganiterativeself-improvementloopthatoperatesonopen-ended
natural language. To this end, future work should extend our findings and study how the outputs of
applying test-time compute can be used to improve the base LLM itself.
Acknowledgements
We thank Yi Su, Rishabh Agarwal, Yinlam Chow, Aleksandra Faust, Vincent Zhuang, George Tucker,
Hao Liu, Jiayi Pan, Ethan Dyer, Behnam Neyshabur, Xavier Garcia, Yamini Bansal, Lampros Lamprou,
Yuxiao Qu, and Amrith Setlur for their feedback on an earlier version of the paper and discussions. We
attribute and thank Rishabh Agarwal, Vincent Zhuang, Yi Su, and Avi Singh for ideas and discussions,
and experiments that concretely demonstrated the promise of pairwise sample generation for training
revision models, and edit distance based sampling in [1]. We thank Slav Petrov for leadership support.
References
[1] Training revision models with synthetic data. Coming soon, 2024.
16[2] C. Andrieu, N. De Freitas, A. Doucet, and M. I. Jordan. An introduction to mcmc for machine
learning. 2003.
[3] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa, P. Bailey,
Z. Chen, E. Chu, J. H. Clark, L. E. Shafey, Y. Huang, K. Meier-Hellstern, G. Mishra, E. Moreira,
M.Omernick,K.Robinson,S.Ruder,Y.Tay,K.Xiao,Y.Xu,Y.Zhang,G.H.Abrego,J.Ahn,J.Austin,
P. Barham, J. Botha, J. Bradbury, S. Brahma, K. Brooks, M. Catasta, Y. Cheng, C. Cherry, C. A.
Choquette-Choo, A. Chowdhery, C. Crepy, S. Dave, M. Dehghani, S. Dev, J. Devlin, M. D√≠az, N. Du,
E.Dyer,V.Feinberg,F.Feng,V.Fienber,M.Freitag,X.Garcia,S.Gehrmann,L.Gonzalez,G.Gur-Ari,
S. Hand, H. Hashemi, L. Hou, J. Howland, A. Hu, J. Hui, J. Hurwitz, M. Isard, A. Ittycheriah,
M. Jagielski, W. Jia, K. Kenealy, M. Krikun, S. Kudugunta, C. Lan, K. Lee, B. Lee, E. Li, M. Li,
W. Li, Y. Li, J. Li, H. Lim, H. Lin, Z. Liu, F. Liu, M. Maggioni, A. Mahendru, J. Maynez, V. Misra,
M. Moussalem, Z. Nado, J. Nham, E. Ni, A. Nystrom, A. Parrish, M. Pellat, M. Polacek, A. Polozov,
R. Pope, S. Qiao, E. Reif, B. Richter, P. Riley, A. C. Ros, A. Roy, B. Saeta, R. Samuel, R. Shelby,
A.Slone,D.Smilkov,D.R.So,D.Sohn,S.Tokumine,D.Valter,V.Vasudevan,K.Vodrahalli,X.Wang,
P.Wang,Z.Wang,T.Wang,J.Wieting,Y.Wu,K.Xu,Y.Xu,L.Xue,P.Yin,J.Yu,Q.Zhang,S.Zheng,
C. Zheng, W. Zhou, D. Zhou, S. Petrov, and Y. Wu. Palm 2 technical report, 2023.
[4] Y. Bai, S. Kadavath, S. Kundu, A. Askell, J. Kernion, A. Jones, A. Chen, A. Goldie, A. Mirhoseini,
C. McKinnon, C. Chen, C. Olsson, C. Olah, D. Hernandez, D. Drain, D. Ganguli, D. Li, E. Tran-
Johnson, E. Perez, J. Kerr, J. Mueller, J. Ladish, J. Landau, K. Ndousse, K. Lukosuite, L. Lovitt,
M. Sellitto, N. Elhage, N. Schiefer, N. Mercado, N. DasSarma, R. Lasenby, R. Larson, S. Ringer,
S. Johnston, S. Kravec, S. E. Showk, S. Fort, T. Lanham, T. Telleen-Lawton, T. Conerly, T. Henighan,
T.Hume,S.R.Bowman,Z.Hatfield-Dodds,B.Mann,D.Amodei,N.Joseph,S.McCandlish,T.Brown,
and J. Kaplan. Constitutional ai: Harmlessness from ai feedback, 2022.
[5] C.Blakeney,M.Paul,B.W.Larsen,S.Owen,andJ.Frankle. Doesyourdatasparkjoy? performance
gains from domain upsampling at the end of training, 2024. URL https://arxiv.org/abs/
2406.03476.
[6] G. Chen, M. Liao, C. Li, and K. Fan. Alphamath almost zero: process supervision without process,
2024.
[7] K. Cobbe, V. Kosaraju, M. Bavarian, M. Chen, H. Jun, L. Kaiser, M. Plappert, J. Tworek, J. Hilton,
R. Nakano, C. Hesse, and J. Schulman. Training verifiers to solve math word problems, 2021.
[8] Y. Du, S. Li, A. Torralba, J. B. Tenenbaum, and I. Mordatch. Improving factuality and reasoning in
language models through multiagent debate, 2023.
[9] J. S. B. T. Evans. Heuristic and analytic processes in reasoning. British Journal of Psychology, 75(4):
451‚Äì468, 1984.
[10] X.Feng,Z.Wan,M.Wen,S.M.McAleer,Y.Wen,W.Zhang,andJ.Wang. Alphazero-liketree-search
can guide large language model decoding and training, 2024.
[11] L. Gao, A. Madaan, S. Zhou, U. Alon, P. Liu, Y. Yang, J. Callan, and G. Neubig. Pal: Program-aided
language models, 2023. URL https://arxiv.org/abs/2211.10435.
[12] S.Goyal,Z.Ji,A.S.Rawat,A.K.Menon,S.Kumar,andV.Nagarajan. Thinkbeforeyouspeak: Train-
ing language models with pause tokens, 2024. URL https://arxiv.org/abs/2310.02226.
17[13] D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt.
Measuring mathematical problem solving with the math dataset, 2021.
[14] J. Hoffmann, S. Borgeaud, A. Mensch, E. Buchatskaya, T. Cai, E. Rutherford, D. de Las Casas,
L. A. Hendricks, J. Welbl, A. Clark, T. Hennigan, E. Noland, K. Millican, G. van den Driessche,
B. Damoc, A. Guy, S. Osindero, K. Simonyan, E. Elsen, J. W. Rae, O. Vinyals, and L. Sifre. Training
compute-optimal large language models, 2022.
[15] J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou. Large language models
cannot self-correct reasoning yet, 2023.
[16] A.L.Jones. Scalingscalinglawswithboardgames,2021. URLhttps://arxiv.org/abs/2104.
03113.
[17] D. Kahneman. Maps of bounded rationality: Psychology for behavioral economics. The American
Economic Review, 93(5):1449‚Äì1475, 2003.
[18] D. Kahneman. Thinking, fast and slow. Farrar, Straus and Giroux, New York, first paperback edition
edition, 2013.
[19] L.KocsisandC.Szepesv‚Äôari. Banditbasedmonte-carloplanning. InEuropeanconferenceonmachine
learning, pages 282‚Äì293. Springer, 2006.
[20] A. Lewkowycz, A. Andreassen, D. Dohan, E. Dyer, H. Michalewski, V. Ramasesh, A. Slone, C. Anil,
I. Schlag, T. Gutman-Solo, Y. Wu, B. Neyshabur, G. Gur-Ari, and V. Misra. Solving quantitative
reasoning problems with language models, 2022.
[21] Y. Li, Z. Lin, S. Zhang, Q. Fu, B. Chen, J.-G. Lou, and W. Chen. Making large language models
better reasoners with step-aware verifier, 2023.
[22] H.Lightman,V.Kosaraju,Y.Burda,H.Edwards,B.Baker,T.Lee,J.Leike,J.Schulman,I.Sutskever,
and K. Cobbe. Let‚Äôs verify step by step, 2023.
[23] A.Madaan,N.Tandon,P.Gupta,S.Hallinan,L.Gao,S.Wiegreffe,U.Alon,N.Dziri,S.Prabhumoye,
Y. Yang, S. Gupta, B. P. Majumder, K. Hermann, S. Welleck, A. Yazdanbakhsh, and P. Clark. Self-
refine: Iterative refinement with self-feedback, 2023.
[24] N. McAleese, R. Pokorny, J. F. Cer√≥n Uribe, E. Nitishinskaya, M. Trƒôbacz, and J. Leike. Llm critics
help catch llm bugs. OpenAI, 2024.
[25] OpenAI. Gpt-4 technical report, 2024.
[26] Y. Qin, S. Liang, Y. Ye, K. Zhu, L. Yan, Y. Lu, Y. Lin, X. Cong, X. Tang, B. Qian, S. Zhao, L. Hong,
R. Tian, R. Xie, J. Zhou, M. Gerstein, D. Li, Z. Liu, and M. Sun. Toolllm: Facilitating large language
models to master 16000+ real-world apis, 2023. URL https://arxiv.org/abs/2307.16789.
[27] C. Qu, S. Dai, X. Wei, H. Cai, S. Wang, D. Yin, J. Xu, and J.-R. Wen. Tool learning with large
language models: A survey, 2024. URL https://arxiv.org/abs/2405.17935.
[28] Y. Qu, T. Zhang, N. Garg, and A. Kumar. Recursive introspection: Teaching foundation models how
to self-improve. 2024.
18[29] N. Sardana and J. Frankle. Beyond chinchilla-optimal: Accounting for inference in language model
scaling laws, 2023.
[30] W. Saunders, C. Yeh, J. Wu, S. Bills, L. Ouyang, J. Ward, and J. Leike. Self-critiquing models for
assisting human evaluators, 2022.
[31] A. Setlur, S. Garg, X. Geng, N. Garg, V. Smith, and A. Kumar. Rl on incorrect synthetic data scales
the efficiency of llm math reasoning by eight-fold. arXiv preprint arXiv:2406.14532, 2024.
[32] Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. K. Li, Y. Wu, and D. Guo.
Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024.
[33] A. Sharma, S. Keh, E. Mitchell, C. Finn, K. Arora, and T. Kollar. A critical evaluation of ai feedback
for aligning large language models, 2024. URL https://arxiv.org/abs/2402.12366.
[34] N. Shinn, F. Cassano, E. Berman, A. Gopinath, K. Narasimhan, and S. Yao. Reflexion: Language
agents with verbal reinforcement learning, 2023.
[35] A.Singh,J.D.Co-Reyes,R.Agarwal,A.Anand,P.Patil,X.Garcia,P.J.Liu,J.Harrison,J.Lee,K.Xu,
A. Parisi, A. Kumar, A. Alemi, A. Rizkowsky, A. Nova, B. Adlam, B. Bohnet, G. Elsayed, H. Sedghi,
I.Mordatch,I.Simpson,I.Gur,J.Snoek,J.Pennington,J.Hron,K.Kenealy,K.Swersky,K.Mahajan,
L. Culp, L. Xiao, M. L. Bileschi, N. Constant, R. Novak, R. Liu, T. Warkentin, Y. Qian, Y. Bansal,
E. Dyer, B. Neyshabur, J. Sohl-Dickstein, and N. Fiedel. Beyond human data: Scaling self-training
for problem-solving with language models, 2024.
[36] C. Snell, E. Wallace, D. Klein, and S. Levine. Predicting emergent capabilities by finetuning.
Conference on Language Modeling 2024, 2024.
[37] K.Stechly,M.Marquez,andS.Kambhampati. Gpt-4doesn‚Äôtknowit‚Äôswrong: Ananalysisofiterative
prompting for reasoning problems, 2023.
[38] R. S. Sutton and A. G. Barto. Reinforcement learning: An introduction. Second edition, 2018.
[39] G. Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context,
2024.
[40] Y. Tian, B. Peng, L. Song, L. Jin, D. Yu, H. Mi, and D. Yu. Toward self-improvement of llms via
imagination, searching, and criticizing, 2024.
[41] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,N.Bashlykov,S.Batra,P.Bhargava,
S. Bhosale, D. Bikel, L. Blecher, C. C. Ferrer, M. Chen, G. Cucurull, D. Esiobu, J. Fernandes, J. Fu,
W. Fu, B. Fuller, C. Gao, V. Goswami, N. Goyal, A. Hartshorn, S. Hosseini, R. Hou, H. Inan,
M. Kardas, V. Kerkez, M. Khabsa, I. Kloumann, A. Korenev, P. S. Koura, M.-A. Lachaux, T. Lavril,
J.Lee,D.Liskovich,Y.Lu,Y.Mao,X.Martinet,T.Mihaylov,P.Mishra,I.Molybog,Y.Nie,A.Poulton,
J. Reizenstein, R. Rungta, K. Saladi, A. Schelten, R. Silva, E. M. Smith, R. Subramanian, X. E. Tan,
B. Tang, R. Taylor, A. Williams, J. X. Kuan, P. Xu, Z. Yan, I. Zarov, Y. Zhang, A. Fan, M. Kambadur,
S. Narang, A. Rodriguez, R. Stojnic, S. Edunov, and T. Scialom. Llama 2: Open foundation and
fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288.
[42] J.Uesato, N. Kushman, R.Kumar, F.Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I. Higgins.
Solving math word problems with process- and outcome-based feedback, 2022.
19[43] K. Valmeekam, M. Marquez, and S. Kambhampati. Can large language models really improve by
self-critiquing their own plans?, 2023.
[44] P. Villalobos and D. Atkinson. Trading off compute in training and inference, 2023. URL https:
//epochai.org/blog/trading-off-compute-in-training-and-inference. Accessed:
2024-07-03.
[45] P. Wang, L. Li, Z. Shao, R. X. Xu, D. Dai, Y. Li, D. Chen, Y. Wu, and Z. Sui. Math-shepherd: Verify
and reinforce llms step-by-step without human annotations, 2023.
[46] R.Wang,E.Zelikman,G.Poesia,Y.Pu,N.Haber,andN.D.Goodman. Hypothesissearch: Inductive
reasoning with language models, 2024. URL https://arxiv.org/abs/2309.05660.
[47] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi, Q. Le, and D. Zhou. Chain-of-
thought prompting elicits reasoning in large language models, 2023.
[48] S. Yao, D. Yu, J. Zhao, I. Shafran, T. L. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts:
Deliberate problem solving with large language models, 2023.
[49] Z. Yuan, H. Yuan, C. Li, G. Dong, K. Lu, C. Tan, C. Zhou, and J. Zhou. Scaling relationship on
learning mathematical reasoning with large language models, 2023.
[50] E. Zelikman, Y. Wu, J. Mu, and N. D. Goodman. Star: Bootstrapping reasoning with reasoning,
2022.
[51] E. Zelikman, G. Harik, Y. Shao, V. Jayasiri, N. Haber, and N. D. Goodman. Quiet-star: Language
models can teach themselves to think before speaking, 2024. URL https://arxiv.org/abs/
2403.09629.
20Appendices
A. Related Work
Languagemodelreasoning. Languagemodelperformanceonchallengingmathematicalreasoningtasks
has rapidly improved in recent years [20, 22, 25, 32, 39]. These improvements can be attributed to four
primaryfactors: 1)runningcontinuedpretrainingonlargecorporaofmathfocuseddata[20,22,32,39];
2)improvingtheLLMproposaldistributionbyeitherapplyingtargetedoptimizationonspecificreasoning
tasks by finetuning with RL [32, 35, 49, 50] enabling models to critique and revise their answers
iteratively[4,8,23,30];3)enablingLLMstobenefitfromadditionaltest-timecomputationbyfinetuning
verifiers [6, 7, 10, 22, 40, 42, 45, 48]. Our work builds on these second and third lines of research by
analyzingtheextenttowhichtest-timecomputescalingcanbeimprovedby1)refininganLLM‚Äôsproposal
distribution and 2) conducting search against verifiers.
Analyzing test-time compute scaling. The tradeoff between train-time and test-time compute using
Monte-CarlotreesearchappliedtotheboardgameHexwaspreviouslystudiedbyJones[16]. Weinstead
focus our analysis on full-scale language model math reasoning problems. A survey work by Villalobos
and Atkinson [44] analyzed the tradeoff between training and inference across a number of domains.
However, much of their language-model analysis focused on test-time compute scaling in settings where
theground-truthanswerisknown. Incontrast,ouranalysisfocusesonthesettingwhentheground-truth
answer is not known. Additionally, a number of works in the RL literature have proposed methods, such
as MCTS [19], which aim to navigate the tradeoff between test-time and training-time compute so as
to enable a form of iterative self-play. The findings in our work can be used to help develop similar
algorithms that can operate on open-ended natural language.
AugmentingLLMswithtest-timecompute. Beyondverifiersandrevisions,anumberofadditionalworks
haveproposedalternativemethodsforenablingLMstousetest-timecomputeforreasoning. Namely,Wang
etal.[46]conductsahierarchicalhypothesissearchtoenableinductivereasoningcapabilities. Anumber
of related works have proposed augmenting language models with tools at test-time, which can greatly
improve their performance on downstream tasks [11, 26, 27]. Finally, several works have proposed
methods for learning thought tokens in an unsupervised manner [12, 51], enabling models to more
effectivelyutilizetheadditionaltest-timecomputethatcomeswithsamplinglongersequences. Whilewe
focusouranalysisontwoprimarymechanismsbywhichtest-timecomputecanbescaledinthiswork(e.g.
verifiers and revisions), many of the methods by which we conduct our analysis (e.g. compute optimal
scalingaccordingtoquestiondifficulty)could,inprinciple,alsobeappliedtoanyoftheseothermethods
of scaling test-time compute, and we believe that this is an interesting direction for future research.
B. Additional Revision Results
We plot additional results for majority selection using out PaLM 2-S* revision model in Figure 10. With
majority selection, we see largely similar trends to those found in Figure 7 for verifier selection.
C. Unsupervised Difficulty Bins
We compute difficulty bins without oracle ground-truth correctness information by averaging the PRM
final-answerscoreover2048samplesoneachquestion,soastoobtainavalueestimatecorrespondingto
21Varying Sequential/Parallel with Majority Revisions Majority@128, Varying the Sequential to Parallel Ratio
40 102
102
80
35 101
60
30 100
101 40
25 101
20
20 102
27 25 23 21 21 23 25 27
100 0
1 2 3 4 5
Sequential/Parallel Ratio Test Questions Binned by Increasing Difficulty Level
Figure 10 Varying the ratio of generation budget allocated to sequential verses parallel samples, using
‚à£
majoritytoselecttheanswer,ratherthantheverifier. Left: Eachlinerepresentsafixedgenerationbudget
as the ratio is changed. We see that similar to the verifier case, in the majority case, there exists an ideal
ratio of sequential to parallel test-time compute at a given budget. Right: Analyzing performance across
difficulty bins, we see that the easier questions are mostly invariant the ratio of sequential to parallel,
whereas on the harder questions there is an ideal ratio of sequential to parallel test-time compute.
the question. We then bin the value for each question in the test-set into five quintiles (using the same
procedure as the oracle difficulty bins). We refer to this as ‚Äúpredicted difficulty‚Äù rather than ‚Äúoracle
difficulty‚Äù. Technically this procedure is extremely costly because it requires generating many samples.
While we do not account for this cost in our analysis, in a practical production setting, this cost would
be problematic. A more efficient approach would be to finetune a model to predict correctness directly,
given the question. We do not explore this in our work, but leave such exploration of cheaper methods of
estimating difficulty to future work.
In Figure 12 we plot PRM-search results using our difficulty bins, and in Figure 11 we plot the corre-
sponding revision results. We see that in both settings these predicted bins demonstrate similar trends to
the oracle bins.
D. PRM Training Details
We finetune our PRM as a binary classifier, where the model predicts a value between 0 and 1 at each
step in the solution. We train the model with soft values obtained from the monte-carlo rollouts, using
a binary cross entropy loss function (e.g. ‚àí (ùë¶ùëôùëúùëî(ÀÜùë¶)+ (1‚àí ùë¶)ùëôùëúùëî(1‚àí ÀÜùë¶)) where ùë¶ corresponds to the
soft ground-truth value and ÀÜùë¶ the model‚Äôs predicted value). We finetune the model base model using the
AdamW optimizer, with lr 3e-5, batch size 128, dropout 0.05, and Adam betas (0.9,0.95 ). We conduct
early stopping, selecting the checkpoint with the lowest validation loss on a random held-out validation
set, consisting of 10% of the questions in the original PRM800k training split.
WefinetunethePRMon16samplesperquestionfromthecorrespondingfew-shotpromptedbasemodel.
At each step, we use 16 monte-carlo rollouts, using the same base model and prompt, to estimate the
step-level value. We filter out all samples which fail to output a valid, parsable final answer from the
training data, as we found these to hurt PRM performance in initial experiments.
22
)%(
ycaruccA
tseT
HTAM
snoitareneG
fo
rebmuN
)%(
ycaruccA
tseT
HTAM
oitaR
lellaraP
ot
laitneuqeSRevisions Best-of-128 Weighted, Varying the Sequential to Parallel Ratio Revisions Majority@128, Varying the Sequential to Parallel Ratio
80 102 80 102
70 70
60 101 60 101
50 50
100 100
40 40
30 30
101 101
20 20
10 102 10 102
0 0
1 2 3 4 5 1 2 3 4 5
Test Questions Binned with Unsupervised Difficulty Bins Test Questions Binned with Unsupervised Difficulty Bins
Figure 11 Using our PaLM 2-S* PRM to compute difficulty bins without ground truth correctness
‚à£
informationforrevisions. Ontheleftweplotverifierselectionandontherightweplotmajorityselectionl
WeseelargelysimilarperformancetrendswiththesebinsaswedowiththegroundtruthonesinFigures7
and 10.
Comparing Beam Search and Best-of-N with Unsupervised Difficulty Bins
80 Beam Search
Best-of-N Weighted
Majority
70
60
50
40
30
20
10
0
1 2 3 4 5
Test Questions Binned with Unsupervised Difficulty Bins
Figure 12 Using our PaLM 2-S* PRM to compute difficulty bins without ground truth correctness
‚à£
informationforPRMsearch. Weseelargelysimilarperformancetrendswiththesebinsaswedowiththe
ground truth ones in Figure 3.
23
)%(
ycaruccA
tseT
HTAM
)%(
ycaruccA
tseT
HTAM
oitaR
lellaraP
ot laitneuqeS
)%(
ycaruccA
tseT
HTAM
oitaR
lellaraP
ot laitneuqeSComparing PRM Aggregation Strategies
40 PRM min
PRM prod
PRM last
35 Base-LM Majority
ORM
30
25
20
15
10
0 1 2 3 4 5 6 7 8
2 2 2 2 2 2 2 2 2
Number of Samples
Figure 13 We compare different methods of aggregating per-step PRM scores to produce a final score
‚à£
for the full solution: ‚Äúmin‚Äù refers to taking the minimum score accross all steps, ‚Äúprod‚Äù takes the product
of all step correctness probabilities, and ‚Äúlast‚Äù just uses the last step score. We see that last performs the
best across all aggregation strategies.
When generating the samples, the base model is prompted to output answers in newline separated a
step-by-step format, as done in Lightman et al. [22]. We then separate each of the answers into steps
using a simple newline splitting procedure. We include details about our prompt in Appendix G.
E. Comparing PRM Aggregation Strategies
We compare different methods of aggregating per-step PRM scores to produce a final score for the full
solution. Specifically we compare: 1) taking the minimum score accross all steps as done in Lightman
et al. [22] (e.g. ‚Äúmin‚Äù); 2) taking the product of all step correctness probabilities (e.g. ‚Äúprod‚Äù); and 3)
taking just the last step prediction (e.g. ‚Äúlast‚Äù). We see in Figure 13 that taking the last step outperforms
the other two approaches. Prior works [22, 45] found min to be the best aggregator. We believe that the
discrepancy is due to the fact that our verifier was trained with soft MC return labels, which surface very
differently from binary correctness labels, and therefore other aggregation strategies may not have the
same effect.
Interestingly, when using the last step aggregation, we are effectively using the PRM like an ORM.
However, we see that the PRM outperforms the ORM, suggesting that in our case the per-step PRM
training may be largely useful as a form of representation learning, rather than purely as a tool at
inference time. Future work should further explore this line of reasoning.
F. Comparing PRM and ORM
We trained a PRM and ORM model using the PaLM 2-S* base LM. We see in Figure 14, that the PRM
outperforms the ORM, and the gap between the gap between the PRM and ORM grows with the number
24
)%(
ycaruccA
tseT
HTAMORM Verses PRM
40 PRM best-of-N weighted
Base-LM Majority
ORM best-of-N weighted
35
30
25
20
15
10
1 3 5 7 9 11
2 2 2 2 2 2
Number of Samples
Figure 14 We compare PRM and ORM models finetuned from PaLM 2-S* in a best-of-N evaluation. We
‚à£
use the PaLM 2-S* base LM to sample outputs, using a few-shot prompt. We see that the PRM greatly
outperforms the ORM at a larg number of samples.
of samples used. We use the last step prediction from the PRM to score the answers as described in
Appendix E.
G. Prompting Details
In order to enable the base model to output answers in a step-by-step format to which a PRM can be
applied, we use a 4-shot prompt consisting of randomly selected correct answer examples from the
PRM800k data released by Lightman et al. [22]. Specifically we use answers from the phase 1 training
split. These answers correspond to GPT-4 generated correct answer examples, which include the correct
step-by-step format. In initial experiments, we found that this prompting procedure produces similar
results to the prompt used in Lewkowycz et al. [20]. We use this prompt for generating training data for
the PRM and the revision model. We also use this prompt when conducting search against the PRM on
the test-set. To grade the final answer predicted by this prompt, we use the grading function released
by Lightman et al. [22].
H. Revision Model Finetuning Details
For fine-tuning the revision model, we follow the procedure outlined in Section 6.1. We first sample 64
outputs per question. We then filter out all answers which end in an invalid solution. For each correct
answer, we then sample a number uniformly between 0 and 4 indicating how many incorrect answers to
include in context for training. The correct answer is used as the last answer in the trajectory (which we
train the model to produce) and the incorrect answers are included in context. If the sampled number
is greater than 0, we then find the closest incorrect answer according to a character-level edit distance
metric to include as the last incorrect answer in the trajectory. The goal here is to select an incorrect
25
)%(
ycaruccA
tseT
HTAManswer which is somewhat correlated with the correct answer, to improve learning. The remaining
incorrect answers, we sample randomly from the set of available answers. In the case where there are
fewerthan4incorrectanswerssampled,wetruncatetheuniformdistribution‚Äôsmaxtomatchthenumber
of incorrect samples. We use this procedure to generate trajectories for all questions in the training data.
Wethenfinetunethebaselanguagemodelonthecorrectanswersolutionsinthesegeneratedtrajectories.
We use the AdamW optimizer with lr 1e-5, batch size 128, dropout 0.0, and Adam betas (0.9,0.95 ).
Wefindthatgenerallyevaluatinglossonanevaluationsetconsistingoftrajectoriesgeneratedasdescribed
above, does not provide a good signal for early stopping. Rather, we find that checkpoints much after
the evaluation loss begins increasing are much more capable of revisions. This is likely because after
finetuningtherevisionmodel,theevaluationsetrepresentsoff-policydata,whichwillnaturallybeout-of-
distributioncomparedtothetrajectiresthatthemodelitselfwouldgenerateon-policy. Wethereforeselect
our revision model checkpoint slightly after the point where we observe overfitting on the validation set.
I. Revision Model Selection Criteria
As described in Section 6.1, in order to effective use our revision model we need to deploy a criteria for
selecting the best answer both within a revision trajectory and between multiple parallel trajectories. We
use two approaches: 1) ORM verifier; and 2) majority voting.
For the ORM verifier, we train an ORM on the revision model‚Äôs outputs according to the procedure in
Appendix J. At inference, time we then use this verifier to select the best answer. Since we have two axes
acrosswhichtoaggregate(withineachrevisiontrajectoriesandbetweenmultipletrajectories),wedeploy
ahierarchicalstrategy,firstselectingthebestanswerwithineachrevisiontrajectoryandthenaggregating
these selected answers across trajectories. To select the best answer within each trajectory, we perform
best-of-Nweightedaggregationandthenchoosethehighestscoringsolutionwiththemaximumbest-of-N
weighted answer. Then, to select the final answer across all revision chains, we perform another round
of best-of-N weighted selection using the best answer from each revision chain. The answer after this
second round of best-of-N weighted represents our final answer prediction.
Formajorityvotingwefoundhierarchicalaggregationtocreateproblemswhenthelengthofthetrajectory
or the number of trajectories was too small. The problem being that without enough samples, majority
voting is unable to effectively select the best option. Therefore, for majority voting, we simply take all
answers, across all trajectories, at once and take their majority as the final-answer. We found this to
produce much smoother scaling behavior than the hierarchical approach.
J. Revision Model Verifier Training
We found that the PRM we finetuned on the PaLM 2-S* base model outputs was not as effective when
applied to the PaLM 2-S* revision model‚Äôs outputs (see Figure 15(a)), likelydueto distribution shift with
the revision model. We therefore, trained a separate ORM verifier to use with our PaLM 2-S* revision
model. We could have trained a PRM as well, but opted for an ORM due to the high cost of generating
per-step PRM labels.
We modified the standard ORM slightly for the revision setting, by finetuning the ORM with previous
revision in context, such that the verifier has access to the same context as the revision model, allowing
26Revision Model Verifier Verses Base-LM PRM Revision Model Verifier With Verse Without History
45
Sequential + Revision ORM Sequential + Verifier With History
Sequential + Base LM PRM 40 Sequential + Verifier Without History
40 Parallel Parallel
35
35
30 30
25 25
20
20
15
20 21 22 23 24 25 26 20 21 22 23 24 25 26
Number of Generations Number of Generations
Figure 15 Left: we compare the ORM we trained on the revision model‚Äôs outputs against the PRM we
‚à£
trained on the PaLM 2-S* base model‚Äôs outputs. We see that when applied to outputs from the revision
model, the ORM adapted to the revision model outperforms the PRM, likely due to distribution shift
with the revision model. Right: we ablate the effect of including previous revisions in the revision model
verifier‚Äôs context. We see that including revisions in-context helps the verifier slightly, but both settings
still outperform the parallel baseline.
the verifier see the revision model‚Äôs previous answer attempts when scoring the current answer. All other
experiment details are identical to those used for training the PRM.
Empirically, we find that including the revision history in context improves performance slightly (see
Figure 15(b)). Additionally, even without the revisions in context, we see that sequential revisions still
slightly outperforms parallel, demonstrating improvements from sequential sampling are not just due to
the verifier‚Äôs context.
EM
K. ReST Revision Model Experiments
We experimented with further optimizing our PaLM 2-S* revision model by training the model with a
simplified RL algorithm: ReSTEM [35]. Specifically, we generated 64 revision trajectories of maximum
length 5 for each question on the MATH training set. We stopped the revision model at the first correct
answerineachtrajectory. Usingthisgenerateddata,wethenfinetunedthebaseLMonthecorrectanswer
data. To help the model learn the task, we explicitly balanced the distribution of trajectory lengths.
In Figure 16, we plot the performance of this new revision model as we vary the sequential to parallel
ratio. We see that additional sequential revisions substantially hurts performance with this new model.
We hypothesize that this degradation is due to the fact that the online data obtained from running
ReSTEM exacerbates spurious correlations in revision data, causing the optimized model to fail to learn
the revision task. We believe that using a more offline data collection strategy, as done in Qu et al. [28],
may be more effective, and leave further exploration to future work.
27
)%(
ycaruccA
tseT
HTAM
)%(
ycaruccA
tseT
HTAMVarying Sequential/Parallel
40
38
2
10
35
32
30
1
10
28
25
22
20 100
5 3 1 1 3 5
2 2 2 2 2 2
Sequential/Parallel Ratio
Figure 16 Performance of our ReSTEM optimized revision model as the sequential to parallel ratio
‚à£
is varied. We use majority voting to select the answer. We see that this optimized revision model
demonstrates substantial performance degradations with additional sequential revisions.
L. Revision Model Example Outputs
In Figures 17, 18, 19, 20, 21, 22, and 23, we include select examples of our revision model‚Äôs outputs.
M. PRM Beam Search Example Outputs
In Figures 24, 25, 26, 27, 28, and 29, we include select examples of PRM beam search. We include the
PRM score, between 0 and 1, for each step in the examples.
28
)%(
ycaruccA
tseT
HTAM
snoitareneG
fo
rebmuNFigure 17 Revision model example 1. The model calculates the sum at the end incorrectly on the first
‚à£
two attempts, but on the third attempt it succeeds and gets the answer correct.
29Figure 18 Revision model example 2. On the first attempt the model takes the incorrect approach, on
‚à£
the second attempt it gets closer but then makes a mistake towards the end. On the final attempt it gets
to the correct answer.
30Figure19 Revisionmodelexample3. Onthefirstattemptthemodelmakesamistakewiththeformatting
‚à£
of the final answer; it corrects this on the second attempt.
31Figure 20 Revision model example 4. On the first few attempts the model fails the base 10 to base 8
‚à£
conversion. On the final attempt it makes the correct calculation.
32Figure 21 Revision model example 5. On the first two attempts the model makes an error when
‚à£
converting euclidean to polar coordinates. On the final attempt it does not make these mistakes.
33Figure 22 Revision model example 6. On the first two attempts the model makes a mistake when
‚à£
summing the proper divisors of 284. On the third attempt, it evaluates this sum correctly.
34Figure23 Revisionmodelexample7. Onthefirstattemptthemodelevaluates 1 +2incorrectly. Onthe
‚à£
3
second attempt it corrects this error.
35Figure 24 PRM beam search example 1.
‚à£
Figure 25 PRM beam search example 2.
‚à£
Figure 26 PRM beam search example 3.
‚à£
Figure 27 PRM beam search example 4.
‚à£
36Figure 28 PRM beam search example 5.
‚à£
Figure 29 PRM beam search example 6.
‚à£
37