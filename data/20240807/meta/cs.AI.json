[
    {
        "title": "LLaVA-OneVision: Easy Visual Task Transfer",
        "authors": "Bo LiYuanhan ZhangDong GuoRenrui ZhangFeng LiHao ZhangKaichen ZhangYanwei LiZiwei LiuChunyuan Li",
        "links": "http://arxiv.org/abs/2408.03326v1",
        "entry_id": "http://arxiv.org/abs/2408.03326v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03326v1",
        "summary": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.",
        "updated": "2024-08-06 17:59:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03326v1"
    },
    {
        "title": "Training LLMs to Recognize Hedges in Spontaneous Narratives",
        "authors": "Amie J. PaigeAdil SoubkiJohn MurzakuOwen RambowSusan E. Brennan",
        "links": "http://arxiv.org/abs/2408.03319v1",
        "entry_id": "http://arxiv.org/abs/2408.03319v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03319v1",
        "summary": "Hedges allow speakers to mark utterances as provisional, whether to signal\nnon-prototypicality or \"fuzziness\", to indicate a lack of commitment to an\nutterance, to attribute responsibility for a statement to someone else, to\ninvite input from a partner, or to soften critical feedback in the service of\nface-management needs. Here we focus on hedges in an experimentally\nparameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced\nfrom memory by 21 speakers for co-present addressees, transcribed to text\n(Galati and Brennan, 2010). We created a gold standard of hedges annotated by\nhuman coders (the Roadrunner-Hedge corpus) and compared three LLM-based\napproaches for hedge detection: fine-tuning BERT, and zero and few-shot\nprompting with GPT-4o and LLaMA-3. The best-performing approach was a\nfine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on\nthe top performing approaches, we used an LLM-in-the-Loop approach to improve\nthe gold standard coding, as well as to highlight cases in which hedges are\nambiguous in linguistically interesting ways that will guide future research.\nThis is the first step in our research program to train LLMs to interpret and\ngenerate collateral signals appropriately and meaningfully in conversation.",
        "updated": "2024-08-06 17:51:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03319v1"
    },
    {
        "title": "Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks",
        "authors": "Rafael SterzingerChristian StippelRobert Sablatnig",
        "links": "http://arxiv.org/abs/2408.03304v1",
        "entry_id": "http://arxiv.org/abs/2408.03304v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03304v1",
        "summary": "Etruscan mirrors constitute a significant category in Etruscan art,\ncharacterized by elaborate figurative illustrations featured on their backside.\nA laborious and costly aspect of their analysis and documentation is the task\nof manually tracing these illustrations. In previous work, a methodology has\nbeen proposed to automate this process, involving photometric-stereo scanning\nin combination with deep neural networks. While achieving quantitative\nperformance akin to an expert annotator, some results still lack qualitative\nprecision and, thus, require annotators for inspection and potential\ncorrection, maintaining resource intensity. In response, we propose a deep\nneural network trained to interactively refine existing annotations based on\nhuman guidance. Our human-in-the-loop approach streamlines annotation,\nachieving equal quality with up to 75% less manual input required. Moreover,\nduring the refinement process, the relative improvement of our methodology over\npure manual labeling reaches peak values of up to 26%, attaining drastically\nbetter quality quicker. By being tailored to the complex task of segmenting\nintricate lines, specifically distinguishing it from previous methods, our\napproach offers drastic improvements in efficacy, transferable to a broad\nspectrum of applications beyond Etruscan mirrors.",
        "updated": "2024-08-06 17:11:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03304v1"
    },
    {
        "title": "Understanding How Blind Users Handle Object Recognition Errors: Strategies and Challenges",
        "authors": "Jonggi HongHernisa Kacorri",
        "links": "http://dx.doi.org/10.1145/3663548.3675635",
        "entry_id": "http://arxiv.org/abs/2408.03303v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03303v1",
        "summary": "Object recognition technologies hold the potential to support blind and\nlow-vision people in navigating the world around them. However, the gap between\nbenchmark performances and practical usability remains a significant challenge.\nThis paper presents a study aimed at understanding blind users' interaction\nwith object recognition systems for identifying and avoiding errors. Leveraging\na pre-existing object recognition system, URCam, fine-tuned for our experiment,\nwe conducted a user study involving 12 blind and low-vision participants.\nThrough in-depth interviews and hands-on error identification tasks, we gained\ninsights into users' experiences, challenges, and strategies for identifying\nerrors in camera-based assistive technologies and object recognition systems.\nDuring interviews, many participants preferred independent error review, while\nexpressing apprehension toward misrecognitions. In the error identification\ntask, participants varied viewpoints, backgrounds, and object sizes in their\nimages to avoid and overcome errors. Even after repeating the task,\nparticipants identified only half of the errors, and the proportion of errors\nidentified did not significantly differ from their first attempts. Based on\nthese insights, we offer implications for designing accessible interfaces\ntailored to the needs of blind and low-vision users in identifying object\nrecognition errors.",
        "updated": "2024-08-06 17:09:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03303v1"
    },
    {
        "title": "KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models",
        "authors": "Ruizhe ZhangYongxin XuYuzhen XiaoRunchuan ZhuXinke JiangXu ChuJunfeng ZhaoYasha Wang",
        "links": "http://arxiv.org/abs/2408.03297v1",
        "entry_id": "http://arxiv.org/abs/2408.03297v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03297v1",
        "summary": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has\nbecome an effective strategy for mitigating the hallucination problems that\nlarge language models (LLMs) encounter when dealing with knowledge-intensive\ntasks. However, in the process of integrating external non-parametric\nsupporting evidence with internal parametric knowledge, inevitable knowledge\nconflicts may arise, leading to confusion in the model's responses. To enhance\nthe knowledge selection of LLMs in various contexts, some research has focused\non refining their behavior patterns through instruction-tuning. Nonetheless,\ndue to the absence of explicit negative signals and comparative objectives,\nmodels fine-tuned in this manner may still exhibit undesirable behaviors in the\nintricate and realistic retrieval scenarios. To this end, we propose a\nKnowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving\ncontrollable knowledge selection in real retrieval scenarios. Concretely, we\nexplore and simulate error types across diverse context combinations and learn\nhow to avoid these negative signals through preference optimization methods.\nSimultaneously, by adjusting the balance between response length and the\nproportion of preference data representing different behavior patterns, we\nenhance the adherence capabilities and noise robustness of LLMs in a balanced\nmanner. Experimental results show that KaPO outperforms previous methods for\nhandling knowledge conflicts by over 37%, while also exhibiting robust\ngeneralization across various out-of-distribution datasets.",
        "updated": "2024-08-06 16:55:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03297v1"
    }
]