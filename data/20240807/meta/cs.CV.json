[
    {
        "title": "LLaVA-OneVision: Easy Visual Task Transfer",
        "authors": "Bo LiYuanhan ZhangDong GuoRenrui ZhangFeng LiHao ZhangKaichen ZhangYanwei LiZiwei LiuChunyuan Li",
        "links": "http://arxiv.org/abs/2408.03326v1",
        "entry_id": "http://arxiv.org/abs/2408.03326v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03326v1",
        "summary": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.",
        "updated": "2024-08-06 17:59:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03326v1"
    },
    {
        "title": "Segment Anything in Medical Images and Videos: Benchmark and Deployment",
        "authors": "Jun MaSumin KimFeifei LiMohammed BaharoonReza AsakerehHongwei LyuBo Wang",
        "links": "http://arxiv.org/abs/2408.03322v1",
        "entry_id": "http://arxiv.org/abs/2408.03322v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03322v1",
        "summary": "Recent advances in segmentation foundation models have enabled accurate and\nefficient segmentation across a wide range of natural images and videos, but\ntheir utility to medical data remains unclear. In this work, we first present a\ncomprehensive benchmarking of the Segment Anything Model 2 (SAM2) across 11\nmedical image modalities and videos and point out its strengths and weaknesses\nby comparing it to SAM1 and MedSAM. Then, we develop a transfer learning\npipeline and demonstrate SAM2 can be quickly adapted to medical domain by\nfine-tuning. Furthermore, we implement SAM2 as a 3D slicer plugin and Gradio\nAPI for efficient 3D image and video segmentation. The code has been made\npublicly available at \\url{https://github.com/bowang-lab/MedSAM}.",
        "updated": "2024-08-06 17:58:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03322v1"
    },
    {
        "title": "MDT-A2G: Exploring Masked Diffusion Transformers for Co-Speech Gesture Generation",
        "authors": "Xiaofeng MaoZhengkai JiangQilin WangChencan FuJiangning ZhangJiafu WuYabiao WangChengjie WangWei LiMingmin Chi",
        "links": "http://dx.doi.org/10.1145/3664647.3680684",
        "entry_id": "http://arxiv.org/abs/2408.03312v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03312v1",
        "summary": "Recent advancements in the field of Diffusion Transformers have substantially\nimproved the generation of high-quality 2D images, 3D videos, and 3D shapes.\nHowever, the effectiveness of the Transformer architecture in the domain of\nco-speech gesture generation remains relatively unexplored, as prior\nmethodologies have predominantly employed the Convolutional Neural Network\n(CNNs) or simple a few transformer layers. In an attempt to bridge this\nresearch gap, we introduce a novel Masked Diffusion Transformer for co-speech\ngesture generation, referred to as MDT-A2G, which directly implements the\ndenoising process on gesture sequences. To enhance the contextual reasoning\ncapability of temporally aligned speech-driven gestures, we incorporate a novel\nMasked Diffusion Transformer. This model employs a mask modeling scheme\nspecifically designed to strengthen temporal relation learning among sequence\ngestures, thereby expediting the learning process and leading to coherent and\nrealistic motions. Apart from audio, Our MDT-A2G model also integrates\nmulti-modal information, encompassing text, emotion, and identity. Furthermore,\nwe propose an efficient inference strategy that diminishes the denoising\ncomputation by leveraging previously calculated results, thereby achieving a\nspeedup with negligible performance degradation. Experimental results\ndemonstrate that MDT-A2G excels in gesture generation, boasting a learning\nspeed that is over 6$\\times$ faster than traditional diffusion transformers and\nan inference speed that is 5.7$\\times$ than the standard diffusion model.",
        "updated": "2024-08-06 17:29:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03312v1"
    },
    {
        "title": "Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks",
        "authors": "Rafael SterzingerChristian StippelRobert Sablatnig",
        "links": "http://arxiv.org/abs/2408.03304v1",
        "entry_id": "http://arxiv.org/abs/2408.03304v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03304v1",
        "summary": "Etruscan mirrors constitute a significant category in Etruscan art,\ncharacterized by elaborate figurative illustrations featured on their backside.\nA laborious and costly aspect of their analysis and documentation is the task\nof manually tracing these illustrations. In previous work, a methodology has\nbeen proposed to automate this process, involving photometric-stereo scanning\nin combination with deep neural networks. While achieving quantitative\nperformance akin to an expert annotator, some results still lack qualitative\nprecision and, thus, require annotators for inspection and potential\ncorrection, maintaining resource intensity. In response, we propose a deep\nneural network trained to interactively refine existing annotations based on\nhuman guidance. Our human-in-the-loop approach streamlines annotation,\nachieving equal quality with up to 75% less manual input required. Moreover,\nduring the refinement process, the relative improvement of our methodology over\npure manual labeling reaches peak values of up to 26%, attaining drastically\nbetter quality quicker. By being tailored to the complex task of segmenting\nintricate lines, specifically distinguishing it from previous methods, our\napproach offers drastic improvements in efficacy, transferable to a broad\nspectrum of applications beyond Etruscan mirrors.",
        "updated": "2024-08-06 17:11:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03304v1"
    },
    {
        "title": "TextIM: Part-aware Interactive Motion Synthesis from Text",
        "authors": "Siyuan FanBo DuXiantao CaiBo PengLongling Sun",
        "links": "http://arxiv.org/abs/2408.03302v1",
        "entry_id": "http://arxiv.org/abs/2408.03302v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03302v1",
        "summary": "In this work, we propose TextIM, a novel framework for synthesizing\nTEXT-driven human Interactive Motions, with a focus on the precise alignment of\npart-level semantics. Existing methods often overlook the critical roles of\ninteractive body parts and fail to adequately capture and align part-level\nsemantics, resulting in inaccuracies and even erroneous movement outcomes. To\naddress these issues, TextIM utilizes a decoupled conditional diffusion\nframework to enhance the detailed alignment between interactive movements and\ncorresponding semantic intents from textual descriptions. Our approach\nleverages large language models, functioning as a human brain, to identify\ninteracting human body parts and to comprehend interaction semantics to\ngenerate complicated and subtle interactive motion. Guided by the refined\nmovements of the interacting parts, TextIM further extends these movements into\na coherent whole-body motion. We design a spatial coherence module to\ncomplement the entire body movements while maintaining consistency and harmony\nacross body parts using a part graph convolutional network. For training and\nevaluation, we carefully selected and re-labeled interactive motions from\nHUMANML3D to develop a specialized dataset. Experimental results demonstrate\nthat TextIM produces semantically accurate human interactive motions,\nsignificantly enhancing the realism and applicability of synthesized\ninteractive motions in diverse scenarios, even including interactions with\ndeformable and dynamically changing objects.",
        "updated": "2024-08-06 17:08:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03302v1"
    }
]