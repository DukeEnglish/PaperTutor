[
    {
        "title": "Fusing Forces: Deep-Human-Guided Refinement of Segmentation Masks",
        "authors": "Rafael SterzingerChristian StippelRobert Sablatnig",
        "links": "http://arxiv.org/abs/2408.03304v1",
        "entry_id": "http://arxiv.org/abs/2408.03304v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03304v1",
        "summary": "Etruscan mirrors constitute a significant category in Etruscan art,\ncharacterized by elaborate figurative illustrations featured on their backside.\nA laborious and costly aspect of their analysis and documentation is the task\nof manually tracing these illustrations. In previous work, a methodology has\nbeen proposed to automate this process, involving photometric-stereo scanning\nin combination with deep neural networks. While achieving quantitative\nperformance akin to an expert annotator, some results still lack qualitative\nprecision and, thus, require annotators for inspection and potential\ncorrection, maintaining resource intensity. In response, we propose a deep\nneural network trained to interactively refine existing annotations based on\nhuman guidance. Our human-in-the-loop approach streamlines annotation,\nachieving equal quality with up to 75% less manual input required. Moreover,\nduring the refinement process, the relative improvement of our methodology over\npure manual labeling reaches peak values of up to 26%, attaining drastically\nbetter quality quicker. By being tailored to the complex task of segmenting\nintricate lines, specifically distinguishing it from previous methods, our\napproach offers drastic improvements in efficacy, transferable to a broad\nspectrum of applications beyond Etruscan mirrors.",
        "updated": "2024-08-06 17:11:40 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03304v1"
    },
    {
        "title": "Understanding How Blind Users Handle Object Recognition Errors: Strategies and Challenges",
        "authors": "Jonggi HongHernisa Kacorri",
        "links": "http://dx.doi.org/10.1145/3663548.3675635",
        "entry_id": "http://arxiv.org/abs/2408.03303v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03303v1",
        "summary": "Object recognition technologies hold the potential to support blind and\nlow-vision people in navigating the world around them. However, the gap between\nbenchmark performances and practical usability remains a significant challenge.\nThis paper presents a study aimed at understanding blind users' interaction\nwith object recognition systems for identifying and avoiding errors. Leveraging\na pre-existing object recognition system, URCam, fine-tuned for our experiment,\nwe conducted a user study involving 12 blind and low-vision participants.\nThrough in-depth interviews and hands-on error identification tasks, we gained\ninsights into users' experiences, challenges, and strategies for identifying\nerrors in camera-based assistive technologies and object recognition systems.\nDuring interviews, many participants preferred independent error review, while\nexpressing apprehension toward misrecognitions. In the error identification\ntask, participants varied viewpoints, backgrounds, and object sizes in their\nimages to avoid and overcome errors. Even after repeating the task,\nparticipants identified only half of the errors, and the proportion of errors\nidentified did not significantly differ from their first attempts. Based on\nthese insights, we offer implications for designing accessible interfaces\ntailored to the needs of blind and low-vision users in identifying object\nrecognition errors.",
        "updated": "2024-08-06 17:09:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03303v1"
    },
    {
        "title": "JetUnit: Rendering Diverse Force Feedback in Virtual Reality Using Water Jets",
        "authors": "Zining ZhangJiasheng LiZeyu YanJun NishidaHuaishu Peng",
        "links": "http://dx.doi.org/10.1145/3654777.3676440",
        "entry_id": "http://arxiv.org/abs/2408.03285v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03285v1",
        "summary": "We propose JetUnit, a water-based VR haptic system designed to produce force\nfeedback with a wide spectrum of intensities and frequencies through water\njets. The key challenge in designing this system lies in optimizing parameters\nto enable the haptic device to generate force feedback that closely replicates\nthe most intense force produced by direct water jets while ensuring the user\nremains dry. In this paper, we present the key design parameters of the JetUnit\nwearable device determined through a set of quantitative experiments and a\nperception study. We further conducted a user study to assess the impact of\nintegrating our haptic solutions into virtual reality experiences. The results\nrevealed that, by adhering to the design principles of JetUnit, the water-based\nhaptic system is capable of delivering diverse force feedback sensations,\nsignificantly enhancing the immersive experience in virtual reality.",
        "updated": "2024-08-06 16:33:35 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03285v1"
    },
    {
        "title": "Compress and Compare: Interactively Evaluating Efficiency and Behavior Across ML Model Compression Experiments",
        "authors": "Angie BoggustVenkatesh SivaramanYannick AssogbaDonghao RenDominik MoritzFred Hohman",
        "links": "http://arxiv.org/abs/2408.03274v1",
        "entry_id": "http://arxiv.org/abs/2408.03274v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03274v1",
        "summary": "To deploy machine learning models on-device, practitioners use compression\nalgorithms to shrink and speed up models while maintaining their high-quality\noutput. A critical aspect of compression in practice is model comparison,\nincluding tracking many compression experiments, identifying subtle changes in\nmodel behavior, and negotiating complex accuracy-efficiency trade-offs.\nHowever, existing compression tools poorly support comparison, leading to\ntedious and, sometimes, incomplete analyses spread across disjoint tools. To\nsupport real-world comparative workflows, we develop an interactive visual\nsystem called Compress and Compare. Within a single interface, Compress and\nCompare surfaces promising compression strategies by visualizing provenance\nrelationships between compressed models and reveals compression-induced\nbehavior changes by comparing models' predictions, weights, and activations. We\ndemonstrate how Compress and Compare supports common compression analysis tasks\nthrough two case studies, debugging failed compression on generative language\nmodels and identifying compression artifacts in image classification models. We\nfurther evaluate Compress and Compare in a user study with eight compression\nexperts, illustrating its potential to provide structure to compression\nworkflows, help practitioners build intuition about compression, and encourage\nthorough analysis of compression's effect on model behavior. Through these\nevaluations, we identify compression-specific challenges that future visual\nanalytics tools should consider and Compress and Compare visualizations that\nmay generalize to broader model comparison tasks.",
        "updated": "2024-08-06 16:17:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03274v1"
    },
    {
        "title": "Connections Beyond Data: Exploring Homophily With Visualizations",
        "authors": "Poorna Talkad SukumarMaurizio PorfiriOded Nov",
        "links": "http://arxiv.org/abs/2408.03269v1",
        "entry_id": "http://arxiv.org/abs/2408.03269v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03269v1",
        "summary": "Homophily refers to the tendency of individuals to associate with others who\nare similar to them in characteristics, such as, race, ethnicity, age, gender,\nor interests. In this paper, we investigate if individuals exhibit racial\nhomophily when viewing visualizations, using mass shooting data in the United\nStates as the example topic. We conducted a crowdsourced experiment (N=450)\nwhere each participant was shown a visualization displaying the counts of mass\nshooting victims, highlighting the counts for one of three racial groups\n(White, Black, or Hispanic). Participants were assigned to view visualizations\nhighlighting their own race or a different race to assess the influence of\nracial concordance on changes in affect (emotion) and attitude towards gun\ncontrol. While we did not find evidence of homophily, the results showed a\nsignificant negative shift in affect across all visualization conditions.\nNotably, political ideology significantly impacted changes in affect, with more\nliberal views correlating with a more negative affect change. Our findings\nunderscore the complexity of reactions to mass shooting visualizations and\nsuggest that future research should consider various methodological\nimprovements to better assess homophily effects.",
        "updated": "2024-08-06 15:59:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03269v1"
    }
]