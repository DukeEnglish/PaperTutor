[
    {
        "title": "Pre-training and in-context learning IS Bayesian inference a la De Finetti",
        "authors": "Naimeng YeHanming YangAndrew SiahHongseok Namkoong",
        "links": "http://arxiv.org/abs/2408.03307v1",
        "entry_id": "http://arxiv.org/abs/2408.03307v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03307v1",
        "summary": "Accurately gauging uncertainty on the underlying environment is a\nlongstanding goal of intelligent systems. We characterize which latent concepts\npre-trained sequence models are naturally able to reason with. We go back to De\nFinetti's predictive view of Bayesian reasoning: instead of modeling latent\nparameters through priors and likelihoods like topic models do, De Finetti has\nlong advocated for modeling exchangeable (permutation invariant) sequences of\nobservables. According to this view, pre-training autoregressive models\nformulates informed beliefs based on prior observations (\"empirical Bayes\"),\nand forward generation is a simulated instantiation of an environment\n(\"posterior inference\"). This connection allows extending in-context learning\n(ICL) beyond predictive settings, highlighting sequence models' ability to\nperform explicit statistical inference. In particular, we show the sequence\nprediction loss over exchangeable documents controls performance on downstream\ntasks where uncertainty quantification is key. Empirically, we propose and\ndemonstrate several approaches for encoding exchangeability in sequence model\narchitectures: data augmentation, regularization, and causal masking.",
        "updated": "2024-08-06 17:16:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03307v1"
    },
    {
        "title": "Active Learning for Level Set Estimation Using Randomized Straddle Algorithms",
        "authors": "Yu InatsuShion TakenoKentaro KutsukakeIchiro Takeuchi",
        "links": "http://arxiv.org/abs/2408.03144v1",
        "entry_id": "http://arxiv.org/abs/2408.03144v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03144v1",
        "summary": "Level set estimation (LSE), the problem of identifying the set of input\npoints where a function takes value above (or below) a given threshold, is\nimportant in practical applications. When the function is expensive-to-evaluate\nand black-box, the \\textit{straddle} algorithm, which is a representative\nheuristic for LSE based on Gaussian process models, and its extensions having\ntheoretical guarantees have been developed. However, many of existing methods\ninclude a confidence parameter $\\beta^{1/2}_t$ that must be specified by the\nuser, and methods that choose $\\beta^{1/2}_t$ heuristically do not provide\ntheoretical guarantees. In contrast, theoretically guaranteed values of\n$\\beta^{1/2}_t$ need to be increased depending on the number of iterations and\ncandidate points, and are conservative and not good for practical performance.\nIn this study, we propose a novel method, the \\textit{randomized straddle}\nalgorithm, in which $\\beta_t$ in the straddle algorithm is replaced by a random\nsample from the chi-squared distribution with two degrees of freedom. The\nconfidence parameter in the proposed method has the advantages of not needing\nadjustment, not depending on the number of iterations and candidate points, and\nnot being conservative. Furthermore, we show that the proposed method has\ntheoretical guarantees that depend on the sample complexity and the number of\niterations. Finally, we confirm the usefulness of the proposed method through\nnumerical experiments using synthetic and real data.",
        "updated": "2024-08-06 12:39:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03144v1"
    },
    {
        "title": "Predictive Performance Test based on the Exhaustive Nested Cross-Validation for High-dimensional data",
        "authors": "Iris Ivy GauranHernando OmbaoZhaoxia Yu",
        "links": "http://arxiv.org/abs/2408.03138v1",
        "entry_id": "http://arxiv.org/abs/2408.03138v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03138v1",
        "summary": "It is crucial to assess the predictive performance of a model in order to\nestablish its practicality and relevance in real-world scenarios, particularly\nfor high-dimensional data analysis. Among data splitting or resampling methods,\ncross-validation (CV) is extensively used for several tasks such as estimating\nthe prediction error, tuning the regularization parameter, and selecting the\nmost suitable predictive model among competing alternatives. The K-fold\ncross-validation is a popular CV method but its limitation is that the risk\nestimates are highly dependent on the partitioning of the data (for training\nand testing). Here, the issues regarding the reproducibility of the K-fold CV\nestimator is demonstrated in hypothesis testing wherein different partitions\nlead to notably disparate conclusions. This study presents an alternative novel\npredictive performance test and valid confidence intervals based on exhaustive\nnested cross-validation for determining the difference in prediction error\nbetween two model-fitting algorithms. A naive implementation of the exhaustive\nnested cross-validation is computationally costly. Here, we address concerns\nregarding computational complexity by devising a computationally tractable\nclosed-form expression for the proposed cross-validation estimator using ridge\nregularization. Our study also investigates strategies aimed at enhancing\nstatistical power within high-dimensional scenarios while controlling the Type\nI error rate. To illustrate the practical utility of our method, we apply it to\nan RNA sequencing study and demonstrate its effectiveness in the context of\nbiological data analysis.",
        "updated": "2024-08-06 12:28:16 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03138v1"
    },
    {
        "title": "Evaluating Posterior Probabilities: Decision Theory, Proper Scoring Rules, and Calibration",
        "authors": "Luciana FerrerDaniel Ramos",
        "links": "http://arxiv.org/abs/2408.02841v1",
        "entry_id": "http://arxiv.org/abs/2408.02841v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02841v1",
        "summary": "Most machine learning classifiers are designed to output posterior\nprobabilities for the classes given the input sample. These probabilities may\nbe used to make the categorical decision on the class of the sample; provided\nas input to a downstream system; or provided to a human for interpretation.\nEvaluating the quality of the posteriors generated by these system is an\nessential problem which was addressed decades ago with the invention of proper\nscoring rules (PSRs). Unfortunately, much of the recent machine learning\nliterature uses calibration metrics -- most commonly, the expected calibration\nerror (ECE) -- as a proxy to assess posterior performance. The problem with\nthis approach is that calibration metrics reflect only one aspect of the\nquality of the posteriors, ignoring the discrimination performance. For this\nreason, we argue that calibration metrics should play no role in the assessment\nof posterior quality. Expected PSRs should instead be used for this job,\npreferably normalized for ease of interpretation. In this work, we first give a\nbrief review of PSRs from a practical perspective, motivating their definition\nusing Bayes decision theory. We discuss why expected PSRs provide a principled\nmeasure of the quality of a system's posteriors and why calibration metrics are\nnot the right tool for this job. We argue that calibration metrics, while not\nuseful for performance assessment, may be used as diagnostic tools during\nsystem development. With this purpose in mind, we discuss a simple and\npractical calibration metric, called calibration loss, derived from a\ndecomposition of expected PSRs. We compare this metric with the ECE and with\nthe expected score divergence calibration metric from the PSR literature and\nargue, using theoretical and empirical evidence, that calibration loss is\nsuperior to these two metrics.",
        "updated": "2024-08-05 21:35:51 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02841v1"
    },
    {
        "title": "Optimizing Cox Models with Stochastic Gradient Descent: Theoretical Foundations and Practical Guidances",
        "authors": "Lang ZengWeijing TangZhao RenYing Ding",
        "links": "http://arxiv.org/abs/2408.02839v1",
        "entry_id": "http://arxiv.org/abs/2408.02839v1",
        "pdf_url": "http://arxiv.org/pdf/2408.02839v1",
        "summary": "Optimizing Cox regression and its neural network variants poses substantial\ncomputational challenges in large-scale studies. Stochastic gradient descent\n(SGD), known for its scalability in model optimization, has recently been\nadapted to optimize Cox models. Unlike its conventional application, which\ntypically targets a sum of independent individual loss, SGD for Cox models\nupdates parameters based on the partial likelihood of a subset of data. Despite\nits empirical success, the theoretical foundation for optimizing Cox partial\nlikelihood with SGD is largely underexplored. In this work, we demonstrate that\nthe SGD estimator targets an objective function that is batch-size-dependent.\nWe establish that the SGD estimator for the Cox neural network (Cox-NN) is\nconsistent and achieves the optimal minimax convergence rate up to a\npolylogarithmic factor. For Cox regression, we further prove the\n$\\sqrt{n}$-consistency and asymptotic normality of the SGD estimator, with\nvariance depending on the batch size. Furthermore, we quantify the impact of\nbatch size on Cox-NN training and its effect on the SGD estimator's asymptotic\nefficiency in Cox regression. These findings are validated by extensive\nnumerical experiments and provide guidance for selecting batch sizes in SGD\napplications. Finally, we demonstrate the effectiveness of SGD in a real-world\napplication where GD is unfeasible due to the large scale of data.",
        "updated": "2024-08-05 21:25:10 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.02839v1"
    }
]