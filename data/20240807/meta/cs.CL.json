[
    {
        "title": "LLaVA-OneVision: Easy Visual Task Transfer",
        "authors": "Bo LiYuanhan ZhangDong GuoRenrui ZhangFeng LiHao ZhangKaichen ZhangYanwei LiZiwei LiuChunyuan Li",
        "links": "http://arxiv.org/abs/2408.03326v1",
        "entry_id": "http://arxiv.org/abs/2408.03326v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03326v1",
        "summary": "We present LLaVA-OneVision, a family of open large multimodal models (LMMs)\ndeveloped by consolidating our insights into data, models, and visual\nrepresentations in the LLaVA-NeXT blog series. Our experimental results\ndemonstrate that LLaVA-OneVision is the first single model that can\nsimultaneously push the performance boundaries of open LMMs in three important\ncomputer vision scenarios: single-image, multi-image, and video scenarios.\nImportantly, the design of LLaVA-OneVision allows strong transfer learning\nacross different modalities/scenarios, yielding new emerging capabilities. In\nparticular, strong video understanding and cross-scenario capabilities are\ndemonstrated through task transfer from images to videos.",
        "updated": "2024-08-06 17:59:44 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03326v1"
    },
    {
        "title": "CoverBench: A Challenging Benchmark for Complex Claim Verification",
        "authors": "Alon JacoviMoran AmbarEyal Ben-DavidUri ShahamAmir FederMor GevaDror MarcusAvi Caciularu",
        "links": "http://arxiv.org/abs/2408.03325v1",
        "entry_id": "http://arxiv.org/abs/2408.03325v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03325v1",
        "summary": "There is a growing line of research on verifying the correctness of language\nmodels' outputs. At the same time, LMs are being used to tackle complex queries\nthat require reasoning. We introduce CoverBench, a challenging benchmark\nfocused on verifying LM outputs in complex reasoning settings. Datasets that\ncan be used for this purpose are often designed for other complex reasoning\ntasks (e.g., QA) targeting specific use-cases (e.g., financial tables),\nrequiring transformations, negative sampling and selection of hard examples to\ncollect such a benchmark. CoverBench provides a diversified evaluation for\ncomplex claim verification in a variety of domains, types of reasoning,\nrelatively long inputs, and a variety of standardizations, such as multiple\nrepresentations for tables where available, and a consistent schema. We\nmanually vet the data for quality to ensure low levels of label noise. Finally,\nwe report a variety of competitive baseline results to show CoverBench is\nchallenging and has very significant headroom. The data is available at\nhttps://huggingface.co/datasets/google/coverbench .",
        "updated": "2024-08-06 17:58:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03325v1"
    },
    {
        "title": "Training LLMs to Recognize Hedges in Spontaneous Narratives",
        "authors": "Amie J. PaigeAdil SoubkiJohn MurzakuOwen RambowSusan E. Brennan",
        "links": "http://arxiv.org/abs/2408.03319v1",
        "entry_id": "http://arxiv.org/abs/2408.03319v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03319v1",
        "summary": "Hedges allow speakers to mark utterances as provisional, whether to signal\nnon-prototypicality or \"fuzziness\", to indicate a lack of commitment to an\nutterance, to attribute responsibility for a statement to someone else, to\ninvite input from a partner, or to soften critical feedback in the service of\nface-management needs. Here we focus on hedges in an experimentally\nparameterized corpus of 63 Roadrunner cartoon narratives spontaneously produced\nfrom memory by 21 speakers for co-present addressees, transcribed to text\n(Galati and Brennan, 2010). We created a gold standard of hedges annotated by\nhuman coders (the Roadrunner-Hedge corpus) and compared three LLM-based\napproaches for hedge detection: fine-tuning BERT, and zero and few-shot\nprompting with GPT-4o and LLaMA-3. The best-performing approach was a\nfine-tuned BERT model, followed by few-shot GPT-4o. After an error analysis on\nthe top performing approaches, we used an LLM-in-the-Loop approach to improve\nthe gold standard coding, as well as to highlight cases in which hedges are\nambiguous in linguistically interesting ways that will guide future research.\nThis is the first step in our research program to train LLMs to interpret and\ngenerate collateral signals appropriately and meaningfully in conversation.",
        "updated": "2024-08-06 17:51:42 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03319v1"
    },
    {
        "title": "Scaling LLM Test-Time Compute Optimally can be More Effective than Scaling Model Parameters",
        "authors": "Charlie SnellJaehoon LeeKelvin XuAviral Kumar",
        "links": "http://arxiv.org/abs/2408.03314v1",
        "entry_id": "http://arxiv.org/abs/2408.03314v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03314v1",
        "summary": "Enabling LLMs to improve their outputs by using more test-time computation is\na critical step towards building generally self-improving agents that can\noperate on open-ended natural language. In this paper, we study the scaling of\ninference-time computation in LLMs, with a focus on answering the question: if\nan LLM is allowed to use a fixed but non-trivial amount of inference-time\ncompute, how much can it improve its performance on a challenging prompt?\nAnswering this question has implications not only on the achievable performance\nof LLMs, but also on the future of LLM pretraining and how one should tradeoff\ninference-time and pre-training compute. Despite its importance, little\nresearch attempted to understand the scaling behaviors of various test-time\ninference methods. Moreover, current work largely provides negative results for\na number of these strategies. In this work, we analyze two primary mechanisms\nto scale test-time computation: (1) searching against dense, process-based\nverifier reward models; and (2) updating the model's distribution over a\nresponse adaptively, given the prompt at test time. We find that in both cases,\nthe effectiveness of different approaches to scaling test-time compute\ncritically varies depending on the difficulty of the prompt. This observation\nmotivates applying a \"compute-optimal\" scaling strategy, which acts to most\neffectively allocate test-time compute adaptively per prompt. Using this\ncompute-optimal strategy, we can improve the efficiency of test-time compute\nscaling by more than 4x compared to a best-of-N baseline. Additionally, in a\nFLOPs-matched evaluation, we find that on problems where a smaller base model\nattains somewhat non-trivial success rates, test-time compute can be used to\noutperform a 14x larger model.",
        "updated": "2024-08-06 17:35:05 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03314v1"
    },
    {
        "title": "KaPO: Knowledge-aware Preference Optimization for Controllable Knowledge Selection in Retrieval-Augmented Language Models",
        "authors": "Ruizhe ZhangYongxin XuYuzhen XiaoRunchuan ZhuXinke JiangXu ChuJunfeng ZhaoYasha Wang",
        "links": "http://arxiv.org/abs/2408.03297v1",
        "entry_id": "http://arxiv.org/abs/2408.03297v1",
        "pdf_url": "http://arxiv.org/pdf/2408.03297v1",
        "summary": "By integrating external knowledge, Retrieval-Augmented Generation (RAG) has\nbecome an effective strategy for mitigating the hallucination problems that\nlarge language models (LLMs) encounter when dealing with knowledge-intensive\ntasks. However, in the process of integrating external non-parametric\nsupporting evidence with internal parametric knowledge, inevitable knowledge\nconflicts may arise, leading to confusion in the model's responses. To enhance\nthe knowledge selection of LLMs in various contexts, some research has focused\non refining their behavior patterns through instruction-tuning. Nonetheless,\ndue to the absence of explicit negative signals and comparative objectives,\nmodels fine-tuned in this manner may still exhibit undesirable behaviors in the\nintricate and realistic retrieval scenarios. To this end, we propose a\nKnowledge-aware Preference Optimization, dubbed KaPO, aimed at achieving\ncontrollable knowledge selection in real retrieval scenarios. Concretely, we\nexplore and simulate error types across diverse context combinations and learn\nhow to avoid these negative signals through preference optimization methods.\nSimultaneously, by adjusting the balance between response length and the\nproportion of preference data representing different behavior patterns, we\nenhance the adherence capabilities and noise robustness of LLMs in a balanced\nmanner. Experimental results show that KaPO outperforms previous methods for\nhandling knowledge conflicts by over 37%, while also exhibiting robust\ngeneralization across various out-of-distribution datasets.",
        "updated": "2024-08-06 16:55:54 UTC",
        "interpretation": "解释内容未找到",
        "id": "2408.03297v1"
    }
]