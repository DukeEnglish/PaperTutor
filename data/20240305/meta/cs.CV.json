[
    {
        "title": "DistriFusion: Distributed Parallel Inference for High-Resolution Diffusion Models",
        "authors": "Muyang LiTianle CaiJiaxin CaoQinsheng ZhangHan CaiJunjie BaiYangqing JiaMing-Yu LiuKai LiSong Han",
        "links": "http://arxiv.org/abs/2402.19481v1",
        "entry_id": "http://arxiv.org/abs/2402.19481v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19481v1",
        "summary": "Diffusion models have achieved great success in synthesizing high-quality\nimages. However, generating high-resolution images with diffusion models is\nstill challenging due to the enormous computational costs, resulting in a\nprohibitive latency for interactive applications. In this paper, we propose\nDistriFusion to tackle this problem by leveraging parallelism across multiple\nGPUs. Our method splits the model input into multiple patches and assigns each\npatch to a GPU. However, na\\\"{\\i}vely implementing such an algorithm breaks the\ninteraction between patches and loses fidelity, while incorporating such an\ninteraction will incur tremendous communication overhead. To overcome this\ndilemma, we observe the high similarity between the input from adjacent\ndiffusion steps and propose displaced patch parallelism, which takes advantage\nof the sequential nature of the diffusion process by reusing the pre-computed\nfeature maps from the previous timestep to provide context for the current\nstep. Therefore, our method supports asynchronous communication, which can be\npipelined by computation. Extensive experiments show that our method can be\napplied to recent Stable Diffusion XL with no quality degradation and achieve\nup to a 6.1$\\times$ speedup on eight NVIDIA A100s compared to one. Our code is\npublicly available at https://github.com/mit-han-lab/distrifuser.",
        "updated": "2024-02-29 18:59:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19481v1"
    },
    {
        "title": "Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers",
        "authors": "Tsai-Shien ChenAliaksandr SiarohinWilli MenapaceEkaterina DeynekaHsiang-wei ChaoByung Eun JeonYuwei FangHsin-Ying LeeJian RenMing-Hsuan YangSergey Tulyakov",
        "links": "http://arxiv.org/abs/2402.19479v1",
        "entry_id": "http://arxiv.org/abs/2402.19479v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19479v1",
        "summary": "The quality of the data and annotation upper-bounds the quality of a\ndownstream model. While there exist large text corpora and image-text pairs,\nhigh-quality video-text data is much harder to collect. First of all, manual\nlabeling is more time-consuming, as it requires an annotator to watch an entire\nvideo. Second, videos have a temporal dimension, consisting of several scenes\nstacked together, and showing multiple actions. Accordingly, to establish a\nvideo dataset with high-quality captions, we propose an automatic approach\nleveraging multimodal inputs, such as textual video description, subtitles, and\nindividual video frames. Specifically, we curate 3.8M high-resolution videos\nfrom the publicly available HD-VILA-100M dataset. We then split them into\nsemantically consistent video clips, and apply multiple cross-modality teacher\nmodels to obtain captions for each video. Next, we finetune a retrieval model\non a small subset where the best caption of each video is manually selected and\nthen employ the model in the whole dataset to select the best caption as the\nannotation. In this way, we get 70M videos paired with high-quality text\ncaptions. We dub the dataset as Panda-70M. We show the value of the proposed\ndataset on three downstream tasks: video captioning, video and text retrieval,\nand text-driven video generation. The models trained on the proposed data score\nsubstantially better on the majority of metrics across all the tasks.",
        "updated": "2024-02-29 18:59:50 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19479v1"
    },
    {
        "title": "Learning a Generalized Physical Face Model From Data",
        "authors": "Lingchen YangGaspard ZossPrashanth ChandranMarkus GrossBarbara SolenthalerEftychios SifakisDerek Bradley",
        "links": "http://arxiv.org/abs/2402.19477v1",
        "entry_id": "http://arxiv.org/abs/2402.19477v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19477v1",
        "summary": "Physically-based simulation is a powerful approach for 3D facial animation as\nthe resulting deformations are governed by physical constraints, allowing to\neasily resolve self-collisions, respond to external forces and perform\nrealistic anatomy edits. Today's methods are data-driven, where the actuations\nfor finite elements are inferred from captured skin geometry. Unfortunately,\nthese approaches have not been widely adopted due to the complexity of\ninitializing the material space and learning the deformation model for each\ncharacter separately, which often requires a skilled artist followed by lengthy\nnetwork training. In this work, we aim to make physics-based facial animation\nmore accessible by proposing a generalized physical face model that we learn\nfrom a large 3D face dataset in a simulation-free manner. Once trained, our\nmodel can be quickly fit to any unseen identity and produce a ready-to-animate\nphysical face model automatically. Fitting is as easy as providing a single 3D\nface scan, or even a single face image. After fitting, we offer intuitive\nanimation controls, as well as the ability to retarget animations across\ncharacters. All the while, the resulting animations allow for physical effects\nlike collision avoidance, gravity, paralysis, bone reshaping and more.",
        "updated": "2024-02-29 18:59:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19477v1"
    },
    {
        "title": "The All-Seeing Project V2: Towards General Relation Comprehension of the Open World",
        "authors": "Weiyun WangYiming RenHaowen LuoTiantong LiChenxiang YanZhe ChenWenhai WangQingyun LiLewei LuXizhou ZhuYu QiaoJifeng Dai",
        "links": "http://arxiv.org/abs/2402.19474v1",
        "entry_id": "http://arxiv.org/abs/2402.19474v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19474v1",
        "summary": "We present the All-Seeing Project V2: a new model and dataset designed for\nunderstanding object relations in images. Specifically, we propose the\nAll-Seeing Model V2 (ASMv2) that integrates the formulation of text generation,\nobject localization, and relation comprehension into a relation conversation\n(ReC) task. Leveraging this unified task, our model excels not only in\nperceiving and recognizing all objects within the image but also in grasping\nthe intricate relation graph between them, diminishing the relation\nhallucination often encountered by Multi-modal Large Language Models (MLLMs).\nTo facilitate training and evaluation of MLLMs in relation understanding, we\ncreated the first high-quality ReC dataset ({AS-V2) which is aligned with the\nformat of standard instruction tuning data. In addition, we design a new\nbenchmark, termed Circular-based Relation Probing Evaluation (CRPE) for\ncomprehensively evaluating the relation comprehension capabilities of MLLMs.\nNotably, our ASMv2 achieves an overall accuracy of 52.04 on this relation-aware\nbenchmark, surpassing the 43.14 of LLaVA-1.5 by a large margin. We hope that\nour work can inspire more future research and contribute to the evolution\ntowards artificial general intelligence. Our project is released at\nhttps://github.com/OpenGVLab/all-seeing.",
        "updated": "2024-02-29 18:59:17 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19474v1"
    },
    {
        "title": "Retrieval-Augmented Generation for AI-Generated Content: A Survey",
        "authors": "Penghao ZhaoHailin ZhangQinhan YuZhengren WangYunteng GengFangcheng FuLing YangWentao ZhangBin Cui",
        "links": "http://arxiv.org/abs/2402.19473v1",
        "entry_id": "http://arxiv.org/abs/2402.19473v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19473v1",
        "summary": "The development of Artificial Intelligence Generated Content (AIGC) has been\nfacilitated by advancements in model algorithms, scalable foundation model\narchitectures, and the availability of ample high-quality datasets. While AIGC\nhas achieved remarkable performance, it still faces challenges, such as the\ndifficulty of maintaining up-to-date and long-tail knowledge, the risk of data\nleakage, and the high costs associated with training and inference.\nRetrieval-Augmented Generation (RAG) has recently emerged as a paradigm to\naddress such challenges. In particular, RAG introduces the information\nretrieval process, which enhances AIGC results by retrieving relevant objects\nfrom available data stores, leading to greater accuracy and robustness. In this\npaper, we comprehensively review existing efforts that integrate RAG technique\ninto AIGC scenarios. We first classify RAG foundations according to how the\nretriever augments the generator. We distill the fundamental abstractions of\nthe augmentation methodologies for various retrievers and generators. This\nunified perspective encompasses all RAG scenarios, illuminating advancements\nand pivotal technologies that help with potential future progress. We also\nsummarize additional enhancements methods for RAG, facilitating effective\nengineering and implementation of RAG systems. Then from another view, we\nsurvey on practical applications of RAG across different modalities and tasks,\noffering valuable references for researchers and practitioners. Furthermore, we\nintroduce the benchmarks for RAG, discuss the limitations of current RAG\nsystems, and suggest potential directions for future research. Project:\nhttps://github.com/hymie122/RAG-Survey",
        "updated": "2024-02-29 18:59:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19473v1"
    }
]