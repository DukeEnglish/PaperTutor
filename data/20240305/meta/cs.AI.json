[
    {
        "title": "The Counterfeit Conundrum: Can Code Language Models Grasp the Nuances of Their Incorrect Generations?",
        "authors": "Alex GuWen-Ding LiNaman JainTheo X. OlaussonCeline LeeKoushik SenArmando Solar-Lezama",
        "links": "http://arxiv.org/abs/2402.19475v1",
        "entry_id": "http://arxiv.org/abs/2402.19475v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19475v1",
        "summary": "While language models are increasingly more proficient at code generation,\nthey still frequently generate incorrect programs. Many of these programs are\nobviously wrong, but others are more subtle and pass weaker correctness checks\nsuch as being able to compile. In this work, we focus on these counterfeit\nsamples: programs sampled from a language model that 1) have a high enough\nlog-probability to be generated at a moderate temperature and 2) pass weak\ncorrectness checks. Overall, we discover that most models have a very shallow\nunderstanding of counterfeits through three clear failure modes. First, models\nmistakenly classify them as correct. Second, models are worse at reasoning\nabout the execution behaviour of counterfeits and often predict their execution\nresults as if they were correct. Third, when asking models to fix counterfeits,\nthe likelihood of a model successfully repairing a counterfeit is often even\nlower than that of sampling a correct program from scratch. Counterfeits also\nhave very unexpected properties: first, counterfeit programs for problems that\nare easier for a model to solve are not necessarily easier to detect and only\nslightly easier to execute and repair. Second, counterfeits from a given model\nare just as confusing to the model itself as they are to other models. Finally,\nboth strong and weak models are able to generate counterfeit samples that\nequally challenge all models. In light of our findings, we recommend that care\nand caution be taken when relying on models to understand their own samples,\nespecially when no external feedback is incorporated.",
        "updated": "2024-02-29 18:59:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19475v1"
    },
    {
        "title": "Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling",
        "authors": "Gabriel GrandValerio PepeJacob AndreasJoshua B. Tenenbaum",
        "links": "http://arxiv.org/abs/2402.19471v1",
        "entry_id": "http://arxiv.org/abs/2402.19471v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19471v1",
        "summary": "Questions combine our mastery of language with our remarkable facility for\nreasoning about uncertainty. How do people navigate vast hypothesis spaces to\npose informative questions given limited cognitive resources? We study these\ntradeoffs in a classic grounded question-asking task based on the board game\nBattleship. Our language-informed program sampling (LIPS) model uses large\nlanguage models (LLMs) to generate natural language questions, translate them\ninto symbolic programs, and evaluate their expected information gain. We find\nthat with a surprisingly modest resource budget, this simple Monte Carlo\noptimization strategy yields informative questions that mirror human\nperformance across varied Battleship board scenarios. In contrast, LLM-only\nbaselines struggle to ground questions in the board state; notably, GPT-4V\nprovides no improvement over non-visual baselines. Our results illustrate how\nBayesian models of question-asking can leverage the statistics of language to\ncapture human priors, while highlighting some shortcomings of pure LLMs as\ngrounded reasoners.",
        "updated": "2024-02-29 18:58:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19471v1"
    },
    {
        "title": "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning",
        "authors": "Kate SandersNathaniel WeirBenjamin Van Durme",
        "links": "http://arxiv.org/abs/2402.19467v2",
        "entry_id": "http://arxiv.org/abs/2402.19467v2",
        "pdf_url": "http://arxiv.org/pdf/2402.19467v2",
        "summary": "It is challenging to perform question-answering over complex, multimodal\ncontent such as television clips. This is in part because current\nvideo-language models rely on single-modality reasoning, have lowered\nperformance on long inputs, and lack interpetability. We propose TV-TREES, the\nfirst multimodal entailment tree generator. TV-TREES serves as an approach to\nvideo understanding that promotes interpretable joint-modality reasoning by\nproducing trees of entailment relationships between simple premises directly\nentailed by the videos and higher-level conclusions. We then introduce the task\nof multimodal entailment tree generation to evaluate the reasoning quality of\nsuch methods. Our method's experimental results on the challenging TVQA dataset\ndemonstrate intepretable, state-of-the-art zero-shot performance on full video\nclips, illustrating a best of both worlds contrast to black-box methods.",
        "updated": "2024-03-01 03:06:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19467v2"
    },
    {
        "title": "Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models",
        "authors": "Chen QianJie ZhangWei YaoDongrui LiuZhenfei YinYu QiaoYong LiuJing Shao",
        "links": "http://arxiv.org/abs/2402.19465v1",
        "entry_id": "http://arxiv.org/abs/2402.19465v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19465v1",
        "summary": "Ensuring the trustworthiness of large language models (LLMs) is crucial. Most\nstudies concentrate on fully pre-trained LLMs to better understand and improve\nLLMs' trustworthiness. In this paper, to reveal the untapped potential of\npre-training, we pioneer the exploration of LLMs' trustworthiness during this\nperiod, focusing on five key dimensions: reliability, privacy, toxicity,\nfairness, and robustness. To begin with, we apply linear probing to LLMs. The\nhigh probing accuracy suggests that \\textit{LLMs in early pre-training can\nalready distinguish concepts in each trustworthiness dimension}. Therefore, to\nfurther uncover the hidden possibilities of pre-training, we extract steering\nvectors from a LLM's pre-training checkpoints to enhance the LLM's\ntrustworthiness. Finally, inspired by~\\citet{choi2023understanding} that mutual\ninformation estimation is bounded by linear probing accuracy, we also probe\nLLMs with mutual information to investigate the dynamics of trustworthiness\nduring pre-training. We are the first to observe a similar two-phase\nphenomenon: fitting and compression~\\citep{shwartz2017opening}. This research\nprovides an initial exploration of trustworthiness modeling during LLM\npre-training, seeking to unveil new insights and spur further developments in\nthe field. We will make our code publicly accessible at\n\\url{https://github.com/ChnQ/TracingLLM}.",
        "updated": "2024-02-29 18:55:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19465v1"
    },
    {
        "title": "Curiosity-driven Red-teaming for Large Language Models",
        "authors": "Zhang-Wei HongIdan ShenfeldTsun-Hsuan WangYung-Sung ChuangAldo ParejaJames GlassAkash SrivastavaPulkit Agrawal",
        "links": "http://arxiv.org/abs/2402.19464v1",
        "entry_id": "http://arxiv.org/abs/2402.19464v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19464v1",
        "summary": "Large language models (LLMs) hold great potential for many natural language\napplications but risk generating incorrect or toxic content. To probe when an\nLLM generates unwanted content, the current paradigm is to recruit a\n\\textit{red team} of human testers to design input prompts (i.e., test cases)\nthat elicit undesirable responses from LLMs. However, relying solely on human\ntesters is expensive and time-consuming. Recent works automate red teaming by\ntraining a separate red team LLM with reinforcement learning (RL) to generate\ntest cases that maximize the chance of eliciting undesirable responses from the\ntarget LLM. However, current RL methods are only able to generate a small\nnumber of effective test cases resulting in a low coverage of the span of\nprompts that elicit undesirable responses from the target LLM. To overcome this\nlimitation, we draw a connection between the problem of increasing the coverage\nof generated test cases and the well-studied approach of curiosity-driven\nexploration that optimizes for novelty. Our method of curiosity-driven red\nteaming (CRT) achieves greater coverage of test cases while mantaining or\nincreasing their effectiveness compared to existing methods. Our method, CRT\nsuccessfully provokes toxic responses from LLaMA2 model that has been heavily\nfine-tuned using human preferences to avoid toxic outputs. Code is available at\n\\url{https://github.com/Improbable-AI/curiosity_redteam}",
        "updated": "2024-02-29 18:55:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19464v1"
    }
]