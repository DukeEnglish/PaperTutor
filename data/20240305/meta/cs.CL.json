[
    {
        "title": "Loose LIPS Sink Ships: Asking Questions in Battleship with Language-Informed Program Sampling",
        "authors": "Gabriel GrandValerio PepeJacob AndreasJoshua B. Tenenbaum",
        "links": "http://arxiv.org/abs/2402.19471v1",
        "entry_id": "http://arxiv.org/abs/2402.19471v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19471v1",
        "summary": "Questions combine our mastery of language with our remarkable facility for\nreasoning about uncertainty. How do people navigate vast hypothesis spaces to\npose informative questions given limited cognitive resources? We study these\ntradeoffs in a classic grounded question-asking task based on the board game\nBattleship. Our language-informed program sampling (LIPS) model uses large\nlanguage models (LLMs) to generate natural language questions, translate them\ninto symbolic programs, and evaluate their expected information gain. We find\nthat with a surprisingly modest resource budget, this simple Monte Carlo\noptimization strategy yields informative questions that mirror human\nperformance across varied Battleship board scenarios. In contrast, LLM-only\nbaselines struggle to ground questions in the board state; notably, GPT-4V\nprovides no improvement over non-visual baselines. Our results illustrate how\nBayesian models of question-asking can leverage the statistics of language to\ncapture human priors, while highlighting some shortcomings of pure LLMs as\ngrounded reasoners.",
        "updated": "2024-02-29 18:58:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19471v1"
    },
    {
        "title": "TV-TREES: Multimodal Entailment Trees for Neuro-Symbolic Video Reasoning",
        "authors": "Kate SandersNathaniel WeirBenjamin Van Durme",
        "links": "http://arxiv.org/abs/2402.19467v2",
        "entry_id": "http://arxiv.org/abs/2402.19467v2",
        "pdf_url": "http://arxiv.org/pdf/2402.19467v2",
        "summary": "It is challenging to perform question-answering over complex, multimodal\ncontent such as television clips. This is in part because current\nvideo-language models rely on single-modality reasoning, have lowered\nperformance on long inputs, and lack interpetability. We propose TV-TREES, the\nfirst multimodal entailment tree generator. TV-TREES serves as an approach to\nvideo understanding that promotes interpretable joint-modality reasoning by\nproducing trees of entailment relationships between simple premises directly\nentailed by the videos and higher-level conclusions. We then introduce the task\nof multimodal entailment tree generation to evaluate the reasoning quality of\nsuch methods. Our method's experimental results on the challenging TVQA dataset\ndemonstrate intepretable, state-of-the-art zero-shot performance on full video\nclips, illustrating a best of both worlds contrast to black-box methods.",
        "updated": "2024-03-01 03:06:37 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19467v2"
    },
    {
        "title": "Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training Period of Large Language Models",
        "authors": "Chen QianJie ZhangWei YaoDongrui LiuZhenfei YinYu QiaoYong LiuJing Shao",
        "links": "http://arxiv.org/abs/2402.19465v1",
        "entry_id": "http://arxiv.org/abs/2402.19465v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19465v1",
        "summary": "Ensuring the trustworthiness of large language models (LLMs) is crucial. Most\nstudies concentrate on fully pre-trained LLMs to better understand and improve\nLLMs' trustworthiness. In this paper, to reveal the untapped potential of\npre-training, we pioneer the exploration of LLMs' trustworthiness during this\nperiod, focusing on five key dimensions: reliability, privacy, toxicity,\nfairness, and robustness. To begin with, we apply linear probing to LLMs. The\nhigh probing accuracy suggests that \\textit{LLMs in early pre-training can\nalready distinguish concepts in each trustworthiness dimension}. Therefore, to\nfurther uncover the hidden possibilities of pre-training, we extract steering\nvectors from a LLM's pre-training checkpoints to enhance the LLM's\ntrustworthiness. Finally, inspired by~\\citet{choi2023understanding} that mutual\ninformation estimation is bounded by linear probing accuracy, we also probe\nLLMs with mutual information to investigate the dynamics of trustworthiness\nduring pre-training. We are the first to observe a similar two-phase\nphenomenon: fitting and compression~\\citep{shwartz2017opening}. This research\nprovides an initial exploration of trustworthiness modeling during LLM\npre-training, seeking to unveil new insights and spur further developments in\nthe field. We will make our code publicly accessible at\n\\url{https://github.com/ChnQ/TracingLLM}.",
        "updated": "2024-02-29 18:55:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19465v1"
    },
    {
        "title": "Curiosity-driven Red-teaming for Large Language Models",
        "authors": "Zhang-Wei HongIdan ShenfeldTsun-Hsuan WangYung-Sung ChuangAldo ParejaJames GlassAkash SrivastavaPulkit Agrawal",
        "links": "http://arxiv.org/abs/2402.19464v1",
        "entry_id": "http://arxiv.org/abs/2402.19464v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19464v1",
        "summary": "Large language models (LLMs) hold great potential for many natural language\napplications but risk generating incorrect or toxic content. To probe when an\nLLM generates unwanted content, the current paradigm is to recruit a\n\\textit{red team} of human testers to design input prompts (i.e., test cases)\nthat elicit undesirable responses from LLMs. However, relying solely on human\ntesters is expensive and time-consuming. Recent works automate red teaming by\ntraining a separate red team LLM with reinforcement learning (RL) to generate\ntest cases that maximize the chance of eliciting undesirable responses from the\ntarget LLM. However, current RL methods are only able to generate a small\nnumber of effective test cases resulting in a low coverage of the span of\nprompts that elicit undesirable responses from the target LLM. To overcome this\nlimitation, we draw a connection between the problem of increasing the coverage\nof generated test cases and the well-studied approach of curiosity-driven\nexploration that optimizes for novelty. Our method of curiosity-driven red\nteaming (CRT) achieves greater coverage of test cases while mantaining or\nincreasing their effectiveness compared to existing methods. Our method, CRT\nsuccessfully provokes toxic responses from LLaMA2 model that has been heavily\nfine-tuned using human preferences to avoid toxic outputs. Code is available at\n\\url{https://github.com/Improbable-AI/curiosity_redteam}",
        "updated": "2024-02-29 18:55:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19464v1"
    },
    {
        "title": "Accelerating materials discovery for polymer solar cells: Data-driven insights enabled by natural language processing",
        "authors": "Pranav ShettyAishat AdeboyeSonakshi GuptaChao ZhangRampi Ramprasad",
        "links": "http://arxiv.org/abs/2402.19462v1",
        "entry_id": "http://arxiv.org/abs/2402.19462v1",
        "pdf_url": "http://arxiv.org/pdf/2402.19462v1",
        "summary": "We present a natural language processing pipeline that was used to extract\npolymer solar cell property data from the literature and simulate various\nactive learning strategies. While data-driven methods have been well\nestablished to discover novel materials faster than Edisonian trial-and-error\napproaches, their benefits have not been quantified. Our approach demonstrates\na potential reduction in discovery time by approximately 75 %, equivalent to a\n15 year acceleration in material innovation. Our pipeline enables us to extract\ndata from more than 3300 papers which is ~5 times larger than similar data sets\nreported by others. We also trained machine learning models to predict the\npower conversion efficiency and used our model to identify promising\ndonor-acceptor combinations that are as yet unreported. We thus demonstrate a\nworkflow that goes from published literature to extracted material property\ndata which in turn is used to obtain data-driven insights. Our insights include\nactive learning strategies that can simultaneously optimize the material system\nand train strong predictive models of material properties. This work provides a\nvaluable framework for research in material science.",
        "updated": "2024-02-29 18:54:46 UTC",
        "interpretation": "解释内容未找到",
        "id": "2402.19462v1"
    }
]