Imagine, Initialize, and Explore: An Effective Exploration Method in Multi-Agent
Reinforcement Learning
ZeyangLiu,LipengWan,XinruiYang,ZhuoranChen,XingyuChen,XuguangLan*
NationalKeyLaboratoryofHuman-MachineHybridAugmentedIntelligence
NationalEngineeringResearchCenterforVisualInformationandApplication
InstituteofArtificialIntelligenceandRobotics,Xi’anJiaotongUniversity,Xi’an,China,710049
{zeyang.liu,wanlipeng,xinrui.yang,zhuoran.chen}@stu.xjtu.edu.cn,xingyuchen1990@gmail.com,xglan@mail.xjtu.edu.cn
Abstract paradigm,valuedecompositionmethods(Rashidetal.2018;
Son et al. 2019; Rashid et al. 2020) factorize the joint Q-
Effectiveexplorationiscrucialtodiscoveringoptimalstrate-
value as a function of individual utility functions, ensuring
giesformulti-agentreinforcementlearning(MARL)incom-
consistencybetweenthecentralizedpolicyandtheindivid-
plexcoordinationtasks.Existingmethodsmainlyutilizein-
ualpolicies.Consequently,theyhaveachievedstate-of-the-
trinsicrewardstoenablecommittedexplorationoruserole-
artperformanceinchallengingtasks,suchasStarCraftunit
based learning for decomposing joint action spaces instead
ofdirectlyconductingacollectivesearchintheentireaction- micromanagement(Samvelyanetal.2019).
observation space. However, they often face challenges ob- Despite their success, the simple ϵ-greedy exploration
taining specific joint action sequences to reach success- strategy used in these methods has been found ineffective
ful states in long-horizon tasks. To address this limitation, insolvingcoordinationtaskswithcomplexstateandreward
we propose Imagine, Initialize, and Explore (IIE), a novel
transitions(Mahajanetal.2019;Wangetal.2020c;Zheng
method that offers a promising solution for efficient multi-
et al. 2021). To address this limitation, MAVEN (Mahajan
agentexplorationincomplexscenarios.IIEemploysatrans-
et al. 2019) adopts a latent space for hierarchical control,
formermodeltoimaginehowtheagentsreachacriticalstate
allowing agents to condition their behavior on shared la-
that can influence each other’s transition functions. Then,
we initialize the environment at this state using a simulator tentvariablesandenablingcommittedexploration.EITIand
beforetheexplorationphase.Weformulatetheimagination EDTI(Wangetal.2020c)quantifyandcharacterizethein-
as a sequence modeling problem, where the states, obser- fluenceofoneagent’sbehavioronothersusingmutualinfor-
vations,prompts,actions,andrewardsarepredictedautore- mationandthedifferenceofexpectedreturns,respectively.
gressively.Thepromptconsistsoftimestep-to-go,return-to- ByoptimizingEITIorEDTIobjectivesasintrinsicrewards,
go, influence value, and one-shot demonstration, specifying agents are encouraged to coordinate their exploration and
the desired state and trajectory as well as guiding the ac-
learnpoliciesthatoptimizeteamperformance.EMC(Zheng
tion generation. By initializing agents at the critical states,
etal.2021),ontheotherhand,usespredictionerrorsofindi-
IIE significantly increases the likelihood of discovering po-
vidualQ-valuestocapturethenoveltyofstatesandtheinflu-
tentially important under-explored regions. Despite its sim-
ence from other agents for coordinated exploration. Unfor-
plicity, empirical results demonstrate that our method out-
performs multi-agent exploration baselines on the StarCraft tunately, these methods suffer from the exponential growth
Multi-AgentChallenge(SMAC)andSMACv2environments. of the action-observation space with the number of agents
Particularly,IIEshowsimprovedperformanceinthesparse- andbecomeinefficientinlong-horizontasks.
reward SMAC tasks and produces more effective curricula It is possible to decompose the complex task rather than
overtheinitializedstatesthanothergenerativemethods,such
directly conducting collective searches across the entire
asCVAE-GANanddiffusionmodels.
action-observation space. To this end, RODE (Wang et al.
2020b) decomposes joint action spaces into restricted role
Introduction action spaces by clustering actions based on their effects
on the environment and other agents. Low-level role-based
Recent progress in cooperative multi-agent reinforcement
policies explore and learn within these restricted action-
learning (MARL) has shown attractive prospects for real-
observation spaces, reducing execution and training com-
worldapplications,suchasautonomousdriving(Zhouetal.
plexity.However,RODEstillstruggleswithlong-termtasks
2021) and active voltage control on power distribution net-
asitisstillunlikelytoobtainalongsequenceofspecificac-
works (Wang et al. 2021). To utilize global information
tionstoachievesuccessfulstates.TheGo-Explorefamilyof
during training and maintain scalability in execution, cen-
algorithms(Ecoffetetal.2019;Guoetal.2020;Ecoffetetal.
tralized training with decentralized execution (CTDE) has
2021) decomposes the exploration into two phases: return-
become a widely adopted paradigm in MARL. Under this
ingtothepreviousstateandthenstartingtoexplore.These
methodsstorethehigh-scoringtrajectoriesinanarchiveand
*Correspondingauthor.
Copyright©2024,AssociationfortheAdvancementofArtificial returntosampledstatesfromthisarchivebyrunningagoal-
Intelligence(www.aaai.org).Allrightsreserved. conditionedpolicy.However,inthemulti-agentfield,agents
4202
raM
1
]GL.sc[
2v87971.2042:viXraEnvironment Simulator
Reset Reset 𝑠𝓣 Initialize
Explorefrom𝒔𝟎 𝒔𝟎 Imagined trajectory Explorefrom 𝒔𝓣
𝑥0:𝑇
𝒔𝟏′
Prom𝑠 p0
t
𝒔𝟐′
𝑥 𝒯′ :𝑇′
Decentralized execution Generator … 𝑥0:𝒯−1
𝜏𝑖 MLP GRU MLP 𝑄𝑖 𝓟(𝒔𝟎′)
Few-shot 𝒔𝓣′ 𝑥0:𝑇′
Demonstrations
Buffer
Centralized training 𝓟𝒔𝟎′ ⋃𝒅𝒆𝒎𝒐 Imagine
𝑄 … 𝑄1
𝑛
miM xio nn go nt eo tn wic
o rk
𝑄𝑡𝑜𝑡
… 𝒐𝟎 𝒫(𝑠0)
𝒖0Imagi 𝑟n 0ation
𝑠
1mod …el
𝒐𝒯 𝒫(𝑠𝒯) 𝒖𝒯 𝑟𝒯
EC xo pn lc oa rate tin oa nt e
p hase
Training phase
Prompt-based Causal Transformer
Buffer
𝑥0:𝑡𝑘𝑘
𝒅𝒆𝒎𝒐 𝑠0 𝒐𝟎 𝒫(𝑠0) 𝒖0 𝑟0
…
𝑠𝒯 𝒐𝒯 𝒫(𝑠𝒯) 𝒖𝒯
Token embeddings
Figure 1: An overview of Imagine, Initialize, and Explore. In the pretraining phase, individual agents collect data from the
initial state s provided by the environment simulator. The interaction sequence is divided into several trajectory segments
0
usinginfluencevalues,whichserveasthetrainingdatasetfortheimaginationmodelandfew-shotdemonstrations.Givens ,
0
thepromptgeneratoristrainedtoproducecriticalstates,andtheimaginationmodellearnstopredicthowtoreachsuchcritical
states from s . After pretraining, the imagination model generates a trajectory from the initial state s to a critical state s
0 0 T
conditioned on P(s ) sampled from the prompt generator, and the most related trajectory from the few-shot demonstration
0
dataset.Theagentsareinitializedats bytheenvironmentsimulatorandtheninteractwiththeenvironmentusingtheϵ-greedy
T
strategy.Weconcatenatetheimaginedandtheexploredtrajectorytotrainthejointpolicyinthecentralizedtrainingphase.
should be encouraged to teleport to the states where inter- explorationphase,theagentsaregivenanewinitialstateand
actions happen and may lead to critical under-explored re- few-shotdemonstrationstoconstructthepromptandgener-
gions. These approaches only consider the sparse and de- atetheimaginedtrajectory.Theagentsoperatinginpartially
ceptive rewards in single-agent settings, making them im- observable environments can benefit from this imagination
practicalforMARLwithcomplexrewardandtransitionde- byutilizingittoinitializetherecurrentneuralnetworkstate.
pendenciesamongcooperativeagents. Then,theagentsareinitializedtothelaststateoftheimag-
Teleportingagentstointeractionstatesthatinfluenceeach ined trajectory by the simulator and interact with the envi-
other’stransitionfunctioncansignificantlyincreasethelike- ronment to explore. IIE gradually provides high-influence
lihoodofdiscoveringpotentiallyimportantyetrarelyvisited startingpointsthatcanbeviewedasauto-curriculathathelp
statesandreducetheexplorationspace.However,thereof- agents collect crucial under-explored regions. To make the
tenexistmultiplefeasiblebutinefficienttrajectoriestoreach bestuseoftheimagination,wealsostitchitwiththeinter-
suchinteractionstatesduetotheabundanceofagents’tac- actionsequencefromexplorationforpolicytraining.
ticsandthecompositionalnatureoftheirfunctionalities.In The main contributions of this paper are threefold: First,
lightofthis,weproposeanovelMARLexplorationmethod it introduces Imagine, Initialize, and Explore, which lever-
named Imagine, Initialize, and Explore (IIE). It leverages agestheGPTarchitecturetoimaginehowtheagentsreach
the GPT architecture (Radford et al. 2018) to imagine tra- critical states before exploration. This method bridges se-
jectoriesfromtheinitialstatetointeractionstates,actingas quence modeling and transformers with MARL instead of
apowerful“memorizationengine”thatcangeneratediverse usingGPTasareplacementforreinforcementlearningalgo-
agentbehaviors.Weusetargettimestep-to-go,return-to-go, rithms(Chenetal.2021).Second,empiricalresultsdemon-
influence value, and a one-shot demonstration as prompts strate significant performance improvements of IIE on par-
to specify the “path” of the imagined trajectory. The in- tially observable MARL benchmarks, including StarCraft
fluence value is an advantage function that compares an Multi-Agent Challenge (SMAC) with dense and sparse re-
agent’s current action Q-value to a counterfactual baseline wardsettingsaswellasSMACv2.Andthird,guidedbythe
that marginalizes this agent. Specifically, we obtain sev- target timestep-to-go, return-to-go, influence value, and a
eraltrajectorysegmentsbydividingtheexplorationepisode, one-shotdemonstration,theimaginationmodelcanproduce
where each segment starts at the initial state from the en- more effective curricula and outperforms behavior cloning,
vironment and ends at an interaction state with the high- CVAE-GAN,andclassifier-guideddiffusion.
est influence value. Then, the GPT model learns to predict
states,observations,prompts,actions,andrewardsinanau- Background
toregressivemanneronthesesegments. Decentralized Partially Observable Markov Decision
Fig.1presentsanoverviewofIIEarchitecture.Beforethe Process. A fully cooperative multi-agent task in the par-tially observable setting can be formulated as a Decentral- where t represents the timestep, n is the number of agents,
ized Partially Observable Markov Decision Process (Dec- andP(s )isthepromptforactiongenerationatthestates ,
t t
POMDP)(OliehoekandAmato2016),consistingofatuple withthedefinitionprovidedinthenextsubsection.
G = ⟨A,S,Ω,O,U,P,r,γ⟩, where a ∈ A ≡ {1,...,n} Weobtainthetokenembeddingsforstates,observations,
is a set of agents, S is a set of states, and Ω is a set of prompts, actions, and rewards through a linear layer fol-
joint observations. At each time step, each agent obtains lowedbylayernormalization.Moreover,anembeddingfor
its observation o ∈ Ω based on the observation function eachtimestepislearnedandaddedtoeachtoken.Thetrans-
O(s,a) : S ×A → Ω, and an action-observation history formermodelprocessesthetokensandperformsautoregres-
τ ∈ T ≡ (Ω × U)∗. Each agent a chooses an action sivemodelingtopredictfuturetokens.
a
u a ∈U byastochasticpolicyπ a(u a|τ a):T ×U →[0,1], The imagination model is trained to focus on interaction
whichformsajointactionu∈U.Itresultsinajointreward stateswheretheagentscaninfluenceeachother’stransition
r(s,u)andatransittothenextstates′ ∼P(·|s,u).Thefor- function by prioritizing trajectories with a high-influence
malobjectivefunctionistofindthejointpolicyπthatmaxi- last state in the sampled batch. Specifically, we split inter-
mizesajointaction-valuefunctionQπ(s t,u t)=r(s t,u t)+ actionsequencesfromthesampledbatchintoK segments,
γE s′[Vπ(s′)], where Vπ(s) = E[(cid:80)∞ t=0γtr t|s 0 =s,π], startingfromtheinitialstatebythesimulatorandendingat
andγ ∈[0,1)isadiscountedfactor. thestateswithtop-Khigh-influencelevels.Toensuregener-
alizationacrosstheentireexploredregions,wealsoenforce
CentralizedTrainingwithDecentralizedExecution. In
all state-action pairs of the sampled batch to be assigned a
this paradigm, agents’ policies are trained with access to
minimum probability of λ, where λ is a hyperparameter,
global information in a centralized way and executed only N
andN isthenumberofstate-actionpairsinthebatch.The
based on local histories in a decentralized way (Kraemer
influence I is defined as an advantage function that com-
andBanerjee2016).Oneofthemostsignificantchallenges
parestheQ-valueforthecurrentactionua toacounterfac-
istoguaranteetheconsistencybetweentheindividualpoli-
tualbaseline,marginalizingoutua∗atagivenstates:
cies and the centralized policy, which is also known as
Individual-GlobalMax(Sonetal.2019): I(s)=max(cid:8) Q (s,τ,u)−E Q (cid:0) s,τ,(ua∗,u−a)(cid:1)(cid:9) ,
j ua∗ j
 argmax Q1(s ,u1) a∈A
 u1 t  (4)
argmaxQ j(s t,u)= ... (1) whereQ (s,τ,u)=f (Q1(τ1,u1;θ),...,Qn(τn,un;θ);ϕ)
u argmax Qn(s ,un) j s
un t representsthejointQ-valuefunction,−adenotesallagents
To address this problem, QMIX (Rashid et al. 2018) ap- A except agent a, f s denotes the monotonic mixing
plies a state-dependent monotonic mixing network f to network whose non-negative weights are generated by
s
combineper-agentQ-valuefunctionswiththejointQ-value hyper-networksthattakethestateasinput.
function Q (s,u). The restricted space of all Q (s,u) that Theimaginationmodelparameterizedbyψistrainedby:
j j
QMIXcanberepresentedas:
T (cid:20)
Qm :={Q j|Q j =f s(Q1(s,u1),...,Qn(s,un))}, (2) L m =(cid:88) logqψ(s t|x <st)+logqψ(r t|x <rt)
whereQa(s,ua)∈R, ∂fs ⩾0,∀a∈A. t=1 (5)
∂Qa n (cid:21)
+(cid:88)(cid:0) logqψ(ua|x )+logqψ(oa|s )(cid:1) ,
Method t <ua t t t
a=1
This section presents a novel multi-agent exploration
method,Imagine,Initialize,andExplore,consistingofthree whereT istheepisodelength.Sincetheobservationisonly
key components: (1) the imagination model, which utilizes relatedtothecurrentstateandthevisionrangeoftheagents,
a transformer model to generate the trajectory represent- wefilteroutthehistoricalmemoriesinx < o ta anduses t
inghowagentsreachatargetstateautoregressively,(2)the astheinputoftheobservationmodelqψ(oa t|s t).
prompt generator, which specifies the target state and tra-
jectory for imagination, and (3) the environment simulator, PromptForImagination
whichcanteleporttheagentstoastateinstantly.
Theprimaryobjectiveofourimaginationmodelistoiden-
tify critical states and imagine how to reach them from the
ImaginationModel
initialpoint.However,multiplefeasibletrajectoriesexistto
Ithasbeenfoundthatagentsoperatinginapartiallyobserv-
reach these states due to the diverse tactics and composi-
able environment can benefit from the action-observation
tional nature of agents’ functionalities. Despite their feasi-
history(HausknechtandStone2015;Karkus,Hsu,andLee
bility, many of these trajectories are highly inefficient for
2017;Rashidetal.2018),e.g.,amodelthathasarecallca-
imagination.Forexample,agentsmaywanderaroundtheir
pabilitysuchasgatedrecurrentunit(GRU).Therefore,itis
initialpointsbeforeengagingtheenemy,decreasingsuccess
necessary to imagine the trajectory from the initial state to
rates within a limited episode length. Therefore, a prompt,
the selected interaction state. We formulate the problem of
servingasaconditionforactiongeneration,isnecessaryto
trajectorygenerationasasequencemodelingtask,wherethe
specifythetargettrajectory.
sequenceshavethefollowingform:
Given the starting state s , we propose a prompt gener-
i
x={...,s ,o1,...,on,P(s ),u1,...,un,r ,st+1,...}, (3) ator Pξ(s ) to predict the sequence {s ,I ,T ,R }, where
t t t t t t t i i i i iI istheinfluencevalue,T isthetimestep-to-gorepresent- The individual Q-value functions Qa(oa,τa,ua) are op-
i i
ing how quickly we can achieve the interaction state, and timizedjointlytominimizethefollowingloss:
R
=(cid:80)i+T
r isthereturn-to-go.
i t=i t T−1
Asthetrainingdatasetsfortheimaginationmodelcontain min(cid:88) [Q (s ,τ ,u ;θ,ϕ)−y(s ,τ ,u )]2, (7)
amixtureoftrajectorysegments,directlysamplingfromthe θ,ϕ j t t t t t t
t=0
promptgeneratorisunlikelytoproducethecriticalstatecon-
sistently.Instead,weaimtocontrolthepromptgeneratorto wherey(s ,τ ,u )=r +γmax Q′(s ,τ ,u)isthe
t t t t u j t+1 t+1
producefrequentlyvisitedbuthigh-influencestatesandthe target value function, and Q′ is the target network whose
j
trajectoryleadingtothem.Tothisend,wesamplethetarget parametersareperiodicallycopiedfromQ .
j
influencevalueaccordingtothelog-probability:
RelatedWork
I =logp(I|s )+κ(I−I )/(I −I ), (6)
i i low high low
Multi-agent Exploration. Individual exploration suffers
whereκisahyperparameterthatcontrolsthepreferenceon
from the inconsistency between local and global informa-
high-influence states, I and I are the lower bound
low high tion as well as the non-stationary problem in multi-agent
andupperboundofI ,respectively.Thetargettimestep-to-
i settings. To address these limitations, Jaques et al. (2019)
goandreturn-to-goareobtainedinthesameway.
introduceintrinsicrewardsbasedon“socialinfluence”toin-
Inaddition,wealsoprovideaone-shotdemonstrationfor centivizeagentsinselectingactionsthatcaninfluenceother
theimaginationtoavoiditbeingtoofarawayfromthetar- agents. Similarly, Wang et al. (2020c) leverage mutual in-
gettrajectory,alsoknownasthe“hallucination”problemin formation to capture the interdependencies of rewards and
the large language model (McKenna et al. 2023; Manakul, transitions, promoting exploration efficiency and facilitat-
Liusie,andGales2023).Specifically,westorethetrajectory ing the policies training. EMC (Zheng et al. 2021) lever-
segments in a few-shot demonstration dataset and use the agespredictionerrorsfromindividualQ-valuesasintrinsic
prompt to characterize the segments. Before imagination, rewards and uses episodic memory to improve coordinated
wesearchforthetrajectorywhosedescriptionhasthehigh- exploration.MAVEN(Mahajanetal.2019)employsahier-
estsimilaritywithcurrentpromptP(s i)andthenprependit archical policy for committed and temporally extended ex-
intotheoriginalinputx.Thisprocessonlyaffectstheinfer- ploration, learning multiple state-action value functions for
ence procedure of the model – training remains unaffected each agent through a shared latent variable. RODE (Wang
and can rely on standard next-token prediction frameworks et al. 2020b) introduces a role selector to enable informed
andinfrastructure. decisions and learn role-based policies in a smaller action
space based on the actions’ effects. However, learning in
InitializeandExplore long-horizoncoordinationtasksremainschallengingdueto
Beforeexploration,webeginbysamplingapromptP(s )= theexponentialstate-actionspaces.
0
{I,T,R} for imagination given the initial state s from
0 Go-Explore. Go-Explore (Ecoffet et al. 2019) is one of
theenvironmentsimulator.Then,theagentsimaginehowto
the most famous exploration approaches, particularly well-
reachtheinteractionstates inanautoregressivemannerby
T suitedforhard-explorationdomainswithsparseordeceptive
conditioning on the prompt and the most related trajectory
rewards. In the Go-Explore family of algorithms (Ecoffet
from the few-shot demonstration dataset. After rolling out
et al. 2021; Guo et al. 2020), an agent returns to a promis-
eachimaginationstepandobtainingthenextstate,wehave
ingstatewithoutexplorationbyrunningagoal-conditioned
the next prompt through decrementing the target timestep-
policy or restoring the simulator state and then explores
to-go T and return-to-go R by 1 and the predicted reward
from this state. However, these methods are primarily de-
r from the imagination model, respectively. We maintain
t signedforsingle-agentscenarios,neglectingtheinterdepen-
the influence value I as a constant and repeat this process
dencies between agent policies and the strategies of others
untilthetargettimestep-to-goiszero.Theimaginedtrajec-
inMARL.Moreover,thesemethodsrequirerestoringprevi-
tory x = {s ,o ,u ,r ,s ,...,u ,r } is then used to
0:T 0 0 0 0 1 T T ouslyvisitedsimulatorstatesortrainingagoal-conditioned
initializeGRUstateoftheagentnetwork.
policytogenerateactionsforreturning.Thisleadstosignif-
Next, we define a probability α of initializing the agents
icantcomputationalcosts,especiallyincoordinationscenar-
at state s to emphasize the importance of the target task.
0 iosinvolvingcomplexobservationsandtransitions.
With the probability 1−α, we initialize the agents at the
last state s of the imagined trajectory using the simula- TransformerModel. Severalworkshaveexploredthein-
T
tor. For StarCraft II, we add a distribution over team unit tegrationoftransformermodelsintoreinforcementlearning
types, start positions, and health points in the reset func- (RL) settings. We classify them into two major categories
tion of the environment simulator. We anneal α from 1.0 depending on the usage pattern. The first category focuses
to 0.5 over a fixed number of steps after the pretraining onrepresentingcomponentsinRLalgorithms,suchaspoli-
phase of the imagination model. Finally, the agents inter- ciesandvaluefunctions(Parisottoetal.2020;Parisottoand
actwiththe environmenttocollecttheonlinedata x = Salakhutdinov2021).ThesemethodsrelyonstandardRLal-
T:T
{s ,o ,u ,r ,s ,...,u ,r }.Westitchtheimagined gorithmistoupdatepolicy,wherethetransformeronlypro-
T T T T T+1 T T
(cid:76)
trajectoryandtheonlinedataX =x x astrain- videsthelargerepresentationcapacityandimprovesfeature
0:T−1 T:T
ingdatasetsforpolicytraining. extraction.Conversely,thesecondcategoryaimstoreplace , , , , ( (  & & : :   4 4 0 0 , , ; ;  4 4 3 3 / / ( ( ; ;  ( ( 0 0 & &  0 0 $ $ 9 9 ( ( 1 1  5 5 2 2 ' ' ( (  4 4 0 0 , , ; ;  0 0 $ $ 3 3 3 3 2 2
  D   0 0 0    E    K B Y V B  ]   F    F B Y V B   ] J   G    V  ] B Y V B  V  ]
               
           
           
           
           
       
      0   0     0   0     0   0   0   0   0       0   0     0   0     0   0   0   0   0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
  H    P B Y V B  P   I    V B Y V B  ]   J     P B Y V B   P   K   F R U U L G R U
               
           
           
           
           
       
      0   0     0   0       0   0     0   0       0   0     0   0     0   0   0   0   0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
  L   S U R W R V V B  Y    M   W H U U D Q B  Y    N   ] H U J B  Y    O   S U R W R V V B   Y  
               
           
           
           
           
       
      0   0     0   0       0   0     0   0       0   0     0   0       0   0     0   0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
Figure2:Performancecomparisonsonthedense-rewardSMACandSMACv2benchmarks.
the RL pipeline with sequence modeling. They autoregres- Eachtaskneedstotrainforabout12to20hours,depending
sively generate states, actions, and rewards by condition- on the number of agents and the episode length limit. We
ingonthedesiredreturn-to-goduringinference(Chenetal. evaluate32episodeswithdecentralizedgreedyactionselec-
2021; Lee et al. 2022; Reed et al. 2022). However, there is tionevery10k timestepsforeachalgorithm.Allfiguresare
ariskofunintendedbehaviorswhenofflinedatasetscontain plotted using mean and standard deviation with confidence
destructivebiases.Moreover,itremainsanopenquestionof internal95%.Weconductfiveindependentrunswithdiffer-
howtoextendthesesupervisedlearningparadigmstoonline entrandomseedsforeachlearningcurve.
settingsandsatisfythescalabilityinpractice.
We utilize a transformer model to imagine the trajectory PerformanceComparison
toreachtheinteractionstateguidedbyaprompt,offeringin- In our evaluation, we compare the performance of CW-
creased representational capacity and stability compared to QMIX (Rashid et al. 2020), QPLEX (Wang et al. 2020a),
existing Go-Explore methods. In contrast to current multi- MAVEN (Mahajan et al. 2019), EMC (Zheng et al. 2021),
agentexplorationmethods,weinitializeagentsatthestates RODE (Wang et al. 2020b), QMIX (Rashid et al. 2018),
withhighinfluencevalues.Thisformofcurriculumlearning MAPPO (Yu et al. 2022), and IIE on the StarCraft Multi-
significantlyreducestheexplorationspaceandenhancesco- Agent Challenge (SMAC) (Samvelyan et al. 2019) and
ordinationexploration.Weaimtobridgesequencemodeling SMACv2 (Ellisetal.2022)benchmarks.
andtransformerswithMARLratherthanreplacingconven- SMACisapartiallyobservableMARLbenchmarkknown
tionalRLalgorithms. foritsrichenvironmentsandhighcontrolcomplexity.Itre-
quireslearningpoliciesinalargeobservation-actionspace,
Results
where agents take various actions, such as “move” in car-
Inthissection,weconductempiricalexperimentstoanswer dinal directions, “stop”, and selecting an enemy to attack.
the following questions: (1) Is Imagine, Initialize, and Ex- The maximum number of actions and the episode length
plore(IIE)betterthantheexistingMARLexplorationmeth- varyacrossdifferentscenarios,rangingfrom7to70actions
ods in complex cooperative scenarios or sparse reward set- and60to400timesteps,respectively.Incontrast,SMACv2
tings? (2) Can IIE generate a reasonable curriculum of the presents additional challenges of stochasticity and general-
last states over timesteps and outperform other returning ization. It includes procedurally generated scenarios with
methods?Wealsoinvestigatethecontributionofeachcom- startpositions,unittypes,attackrange,andsightrange.The
ponentintheproposedprompttotheimaginationmodel. agents must generalize their learned policies to previously
We conduct experiments on NVIDIA RTX 3090 GPUs. unseensettingsduringtesting.
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7 , , , , ( (  / / , , , , 5 5  0 0 $ $ 6 6 ( ( 5 5  ( ( 0 0 & &  0 0 $ $ 9 9 ( ( 1 1  5 5 2 2 ' ' ( (  4 4 0 0 , , ; ;  0 0 $ $ 3 3 3 3 2 2
  D    P   E    P   F    P B Y V B  ]   G    V  ]
               
           
           
           
           
       
      0   0     0   0       0   0     0   0       0   0     0   0       0   0     0   0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
Figure3:Performancecomparisonsonthesparse-rewardSMACbenchmark.
 , , , , ( (  & & * *   G G L L I I I I X X V V L L R R Q Q  & & 9 9 $ $ ( (   * * $ $ 1 1  * * & &   S S R R O O L L F F \ \  % % & &  4 4 0 0 , , ; ;
  D   0 0 0    E    F B Y V B   ] J   F    K B Y V B  ]   G   F R U U L G R U
               
           
           
           
           
       
      0   0     0   0       0   0     0   0     0   0   0   0   0     0   0   0   0   0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
  H   * H Q H U D W H G B D O O \ B K H D O W K   I   * H Q H U D W H G B H Q H P \ B K H D O W K   J   ' L V W D Q F H   K   7 K H   '  W  6 1 (  H P E H G G L Q J V
             7 D U J H W  W U D M H F W R U \
 , P D J L Q H G  W U D M H F W R U \
          
          
          
          
     
      0   0     0   0       0   0     0   0       0   0     0   0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
Figure4:(a-d)PerformancecomparisonswithdifferentreturningmethodsontheSMACbenchmark.(e-g)Themeanhealthof
alliesandenemies,aswellastherelativedistancebetweentwogroupsatthelaststateintheMMM2scenario.(h)The2Dt-SNE
embeddingsofthetrajectoryreturnedfromIIEintheMMM2scenarioafterpretraining.
Fig.2showsthatIIEconsiderablyoutperformsthestate- TheimaginationmodelinIIEcanimproverobustnessin
of-the-art MARL methods in both SMAC and SMACv2 thesesettingsbecauseitmakesminimalassumptionsonthe
mapswiththedenserewardsetting.Thisresultdemonstrates density of the reward. As shown in Fig. 3, sparse and de-
thatIIEsignificantlyenhanceslearningspeedandcoordina- layedrewardsminimallyaffectIIE.Wehypothesizethatthe
tionperformanceincomplextasks.Inspecificscenarioslike transformerarchitectureintheimaginationmodelcanbeef-
3s5z vs 3s6zandcorridor,EMCshowsfasterlearn- fectivecriticsandenablemoreaccuratevalueprediction.In
inginthebeginning,whichcanbeattributedtothefactthat contrast,QMIXandMAPPOfailtosolvethesetaskssince
IIE requires more time to pre-train the imagination model theyheavilyrelyondenselypopulatedrewardsforcalculat-
inmorecomplextasks.However,asthetrainingprogresses, ingtemporaldifferencetargetsorgeneralizedadvantagees-
IIEexcelsinprovidingamoretargetedandefficientexplo- timates. LIIR, MASER, RODE, and MAVEN exhibit slow
rationofcomplexcoordinationscenarios,leadingtothebest learning and instability, particularly on the heterogeneous
finalperformanceacrossallscenarios. map 2s3z, suggesting the difficulty of learning such in-
trinsic rewards, subgoals, roles, or noise-based hierarchical
Sparse-rewardBenchmark policiesinhard-explorationscenarios.
We investigate the performance of IIE, EMC, MAVEN,
DifferentReturningMethods
RODE,QMIX,andMAPPO,inadditiontotwomulti-agent
explorationmethodsdesignedforsparserewards,including In this section, we seek insight into whether the imagina-
LIIR (Du et al. 2019) and MASER (Jeon et al. 2022), on tion model in IIE can be thought of as performing efficient
theSMACbenchmarkwiththesparse-rewardsetting.Inthis curriculumlearningandisbetterthanotherreturningmeth-
setting,globalrewardsaresparselygivenonlywhenoneor ods.Toinvestigatethis,wecompareitwithbehaviorcloning
allenemiesaredefeated,withnoadditionalrewardforstate (BC),agoal-conditionedpolicymethod(GC-policy)(Ecof-
informationsuchasenemyandallyhealth.Thisproblemis fet et al. 2021), and generative models, including CVAE-
difficultforcreditassignmentsbecausecreditmustbeprop- GAN(Baoetal.2017)andclassifier-guideddiffusion(CG-
agatedfromtheendoftheepisode. diffusion)(Ajayetal.2023).Beforeexploration,weinitial-
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   K W O D H +  H Y L W D O H 5
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
   K W O D H +  H Y L W D O H 5
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7
 H F Q D W V L '  H Y L W D O H 5
   H W D 5  Q L :  W V H 7
   H W D 5  Q L :  W V H 7 , , , , ( (  5 5 7 7 * *  , , 9 9  , , , , ( (   Z Z R R   , , 9 9  , , , , ( (   Z Z R R   7 7  , , , , ( (   Z Z R R   5 5 7 7 * *  4 4 0 0 , , ; ;
  D   0 0 0    E    F B Y V B   ] J   F    K B Y V B  ]   G   F R U U L G R U
               
           
           
           
           
       
      0   0     0   0       0   0     0   0     0   0   0   0   0     0   0   0   0   0
 7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V  7 L P H V W H S V
Figure5:IIEwithdifferentpromptsontheSMACbenchmark.
ize the agents to the last state of the imagination trajectory DifferentPromptsforImagination
produced by BC, CVAE-GAN, and CG-diffusion using the
In this section, we conduct ablation studies to analyze the
environmentsimulatororrunagoal-conditionedpolicyfrom
contributions of each component in the prompts, including
GC-policytoreturnagentstothetargetstate.
thetimesteps,theaccumulatedreturn-to-go(RTG),thecon-
We show the performance comparison and visualization stant influence value (IV), and the most related trajectory
resultsinFig.4.IIEprovidesagentswithinteractionpoints (T). We integrate the desired timesteps into the following
for exploration with the shortest relative distance between prompts:(1)RTG,(2)IV,(3)RTGwiththeinfluencevalue,
agents and enemies and decreasing but comparable health denotedasIIE-wo-T,(4)RTGwiththemostrelatedtrajec-
levels as training progresses. This contributes to early and tory,denotedasIIE-wo-IV,(5)IVwiththemostrelatedtra-
more frequent multi-agent interactions against the enemies jectory,denotedasIIE-wo-RTG.WecomparethemwithIIE
quickly, reducing the complexity of the multi-agent explo- andQMIXontheSMACbenchmark.
ration space. As a result, IIE outperforms BC, GC-policy,
Fig. 5 shows that RTG and IIE-wo-IV achieve poor per-
CVAE-GAN, and CG-diffusion across all tasks, indicating formanceinMMM2,6h vs 8z,andcorridorduetotheir
that the prompt-based imagination can be more effective
ignorance of the importance of interactions in multi-agent
and has better generalization than simply performing imi-
exploration and limited positive samples for value decom-
tationlearningorothergenerativemodels.Wealsoillustrate
position. IV and IIE-wo-T perform worse than IIE-wo-IV
the data distribution of imagined trajectories in a 2D space
and IIE with a considerable gap, as they do not exploit
withdimensionalreductionviaT-SNE.Theresultsshowthat
one-shot demonstration to guide action generation, lead-
theimaginedtrajectorywillbeclosetothetargettrajectory,
ing to a potential risk that the imagined trajectories may
which verifies that IIE can capture and learn the dynamics
deviate significantly from the target trajectory, especially
ofthemulti-agentenvironmentbasedontheprompt.
in long-horizon tasks with complex transition functions.
CG-diffusion and GC-policy have plateaued in perfor- IIE outperforms IIE-wo-IV across MMM2, 6h vs 8z, and
mance,showingasimilartrendofemergentcomplexitywith 2c vs 64zgmapsbecauseRTGcanfurtherspecifyatra-
IIEbuttakingfarlongertolearnthejointpolicy.Ontheone jectory,improvingtheefficiencyandrobustnessoftheimag-
hand,thecontinuousdiffusionmodelshavebeenextremely ination learning. However, IIE-wo-IV performs better than
successfulinvisionandaudiodomains,buttheydonotper- IIE in corridor, implying that the imagination model
form well in text because of the inherently discrete nature may have some degree of generalization without RTG and
oftext(Lietal.2022).Theimaginationofthetrajectoryis providemorediversesamplesforpolicytraining.
morerelatedtotextthanimagegenerationbecauseithaslow
redundancy,andthetransitionbetweentwoconnectedstates Conclusion
is essential. On theother hand, the applicability of the cur-
rentgoal-conditionedmethodswithcomplexobservationsis WeproposedImagine,Initialize,andExplore,whichenables
limited,astheirflexibilityisoftenconstrainedtotaskspaces ustobreakdownadifficultmulti-agentexplorationproblem
usinglow-dimensionalparametersandrequireswell-defined into a curriculum of subtasks created by initializing agents
task similarity. CVAE-GAN does not show positive results at the interaction state - the last state in the imagined tra-
and keeps generating similar health levels. We hypothesize jectory. We empirically evaluated our algorithm and found
that the mode collapse problem is a bottleneck for CVAE- itoutperformscurrentmulti-agentexplorationmethodsand
GAN. The generator can find data that can easily fool the generative models on various benchmarks. We also show
discriminator becauseof the unbalancedtraining data from thattheprompt-basedimaginationmodelperformsefficient
theexploration.BCshowstheworstresultsbecauseitimi- conditional sequence generation and has one-shot general-
tatesallpastinteractionsequencesandlacksthegeneraliza- ization. We hope this work will inspire more investigation
tion ability to avoid sub-optimal solutions. From Fig. 4e-g, of sequence-prediction models’ applications in multi-agent
wecanseethatBCprioritizesstateswithshortdistancesbut reinforcement learning (MARL) rather than using them to
does not provide reasonable health levels for enemies - the replace conventional MARL. In future work, we consider
meanhealthlevelofenemiesfarexceedsthatofallies,mak- learning continuous prompts to specify the imagination, as
ingitdifficultorevenimpossibletoachieveanysuccess. opposedtothesimpleinfluencevalueusedinthispaper.
   H W D 5  Q L :  W V H 7    H W D 5  Q L :  W V H 7    H W D 5  Q L :  W V H 7    H W D 5  Q L :  W V H 7Acknowledgments Janner, M.; Du, Y.; Tenenbaum, J. B.; and Levine, S.
This work was supported in part by National Key R&D 2022. Planningwithdiffusionforflexiblebehaviorsynthe-
ProgramofChinaundergrantNo.2021ZD0112700,NSFC sis. arXivpreprintarXiv:2205.09991.
under grant No. 62125305, No. 62088102, No. 61973246, Jaques,N.;Lazaridou,A.;Hughes,E.;Gulcehre,C.;Ortega,
No. 62203348, and No. U23A20339, the Fundamental Re- P.;Strouse,D.;Leibo,J.Z.;andDeFreitas,N.2019. Social
search Funds for the Central Universities under Grant influence as intrinsic motivation for multi-agent deep rein-
xtr072022001. forcementlearning. InInternationalconferenceonmachine
learning,3040–3049.PMLR.
Jeon, J.; Kim, W.; Jung, W.; and Sung, Y. 2022. MASER:
References
Multi-Agent Reinforcement Learning with Subgoals Gen-
Ajay, A.; Du, Y.; Gupta, A.; Tenenbaum, J. B.; Jaakkola, erated from Experience Replay Buffer. In Chaudhuri, K.;
T.S.;andAgrawal,P.2023.IsConditionalGenerativeMod- Jegelka, S.; Song, L.; Szepesvari, C.; Niu, G.; and Sabato,
eling all you need for Decision Making? In The Eleventh S., eds., Proceedings of the 39th International Conference
InternationalConferenceonLearningRepresentations. on Machine Learning, volume 162 of Proceedings of Ma-
Ao,S.;Zhou,T.;Jiang,J.;Long,G.;Song,X.;andZhang,C. chineLearningResearch,10041–10052.PMLR.
2022. EAT-C: Environment-Adversarial sub-Task Curricu-
Karkus,P.;Hsu,D.;andLee,W.S.2017. Qmdp-net:Deep
lumforEfficientReinforcementLearning. InInternational
learningforplanningunderpartialobservability. Advances
ConferenceonMachineLearning,822–843.PMLR.
inneuralinformationprocessingsystems,30.
Bao,J.;Chen,D.;Wen,F.;Li,H.;andHua,G.2017.CVAE-
Kraemer,L.;andBanerjee,B.2016. Multi-agentreinforce-
GAN: fine-grained image generation through asymmetric
mentlearningasarehearsalfordecentralizedplanning.Neu-
training. In Proceedings of the IEEE international confer-
rocomputing,190:82–94.
enceoncomputervision,2745–2754.
Kurach,K.;Raichuk,A.;Stanczyk,P.;Zajac,M.;Bachem,
Chen, L.; Lu, K.; Rajeswaran, A.; Lee, K.; Grover, A.;
O.;Espeholt,L.;Riquelme,C.;Vincent,D.;Michalski,M.;
Laskin,M.;Abbeel,P.;Srinivas,A.;andMordatch,I.2021.
Bousquet,O.;andGelly,S.2020. Googleresearchfootball:
Decisiontransformer:Reinforcementlearningviasequence
A novel reinforcement learning environment. In Proceed-
modeling. Advances in neural information processing sys-
ings of the AAAI conference on artificial intelligence, vol-
tems,34:15084–15097.
ume34.04.
Du,Y.;Han,L.;Fang,M.;Liu,J.;Dai,T.;andTao,D.2019.
Lee, K.-H.; Nachum, O.; Yang, M. S.; Lee, L.; Free-
Liir:Learningindividualintrinsicrewardinmulti-agentre-
man, D.; Guadarrama, S.; Fischer, I.; Xu, W.; Jang, E.;
inforcementlearning. AdvancesinNeuralInformationPro-
Michalewski, H.; et al. 2022. Multi-game decision trans-
cessingSystems,32.
formers. Advances in Neural Information Processing Sys-
Ecoffet, A.; Huizinga, J.; Lehman, J.; Stanley, K. O.; and
tems,35:27921–27936.
Clune, J. 2019. Go-explore: a new approach for hard-
Li, X.; Thickstun, J.; Gulrajani, I.; Liang, P. S.; and
explorationproblems. arXivpreprintarXiv:1901.10995.
Hashimoto,T.B.2022. Diffusion-lmimprovescontrollable
Ecoffet, A.; Huizinga, J.; Lehman, J.; Stanley, K. O.; and
textgeneration. AdvancesinNeuralInformationProcessing
Clune, J. 2021. First return, then explore. Nature,
Systems,35:4328–4343.
590(7847):580–586.
Lowe,R.;Wu,Y.I.;Tamar,A.;Harb,J.;PieterAbbeel,O.;
Ellis,B.;Moalla,S.;Samvelyan,M.;Sun,M.;Mahajan,A.;
and Mordatch, I. 2017. Multi-agent actor-critic for mixed
Foerster, J. N.; and Whiteson, S. 2022. SMACv2: An Im-
cooperative-competitive environments. Advances in neural
provedBenchmarkforCooperativeMulti-AgentReinforce-
informationprocessingsystems,30.
mentLearning. arXivpreprintarXiv:2212.07489.
Mahajan, A.; Rashid, T.; Samvelyan, M.; and Whiteson, S.
Fang,K.;Zhu,Y.;Savarese,S.;andFei-Fei,L.2021. Adap-
2019.Maven:Multi-agentvariationalexploration.Advances
tiveProceduralTaskGenerationforHard-ExplorationProb-
lems. InInternationalConferenceonLearningRepresenta-
inNeuralInformationProcessingSystems,32.
tions. Manakul, P.; Liusie, A.; and Gales, M. J. 2023. Self-
Florensa,C.;Held,D.;Geng,X.;andAbbeel,P.2018.Auto- checkgpt: Zero-resource black-box hallucination detection
maticgoalgenerationforreinforcementlearningagents. In for generative large language models. arXiv preprint
International conference on machine learning, 1515–1528. arXiv:2303.08896.
PMLR. Matheron,G.;Perrin,N.;andSigaud,O.2020. PBCS:Effi-
Guo, Y.; Choi, J.; Moczulski, M.; Feng, S.; Bengio, S.; cient exploration and exploitation using a synergy between
Norouzi, M.; and Lee, H. 2020. Memory based trajectory- reinforcement learning and motion planning. In Artificial
conditioned policies for learning from sparse rewards. Ad- NeuralNetworksandMachineLearning–ICANN2020:29th
vances in Neural Information Processing Systems, 33: International Conference on Artificial Neural Networks,
4333–4345. Bratislava,Slovakia,September15–18,2020,Proceedings,
Hausknecht, M.; and Stone, P. 2015. Deep recurrent q-
PartII,295–307.Springer.
learning for partially observable mdps. In 2015 aaai fall McKenna, N.; Li, T.; Cheng, L.; Hosseini, M. J.; Johnson,
symposiumseries. M.; and Steedman, M. 2023. Sources of Hallucination byLargeLanguageModelsonInferenceTasks. arXivpreprint Wang, T.; Gupta, T.; Mahajan, A.; Peng, B.; Whiteson, S.;
arXiv:2305.14552. and Zhang, C. 2020b. Rode: Learning roles to decompose
Oliehoek,F.A.;andAmato,C.2016.Aconciseintroduction multi-agenttasks. arXivpreprintarXiv:2010.01523.
todecentralizedPOMDPs. Springer. Wang,T.;Wang,J.;Wu,Y.;andZhang,C.2020c.Influence-
Parisotto, E.; and Salakhutdinov, R. 2021. Efficient Trans- Based Multi-Agent Exploration. In International Confer-
formers in Reinforcement Learning using Actor-Learner
enceonLearningRepresentations.
Distillation. InInternationalConferenceonLearningRep- Yu,C.;Velu,A.;Vinitsky,E.;Gao,J.;Wang,Y.;Bayen,A.;
resentations. and Wu, Y. 2022. The surprising effectiveness of ppo in
cooperative multi-agent games. Advances in Neural Infor-
Parisotto, E.; Song, F.; Rae, J.; Pascanu, R.; Gulcehre, C.;
mationProcessingSystems,35:24611–24624.
Jayakumar, S.; Jaderberg, M.; Kaufman, R. L.; Clark, A.;
Noury, S.; et al. 2020. Stabilizing transformers for rein- Zheng,L.;Chen,J.;Wang,J.;He,J.;Hu,Y.;Chen,Y.;Fan,
forcementlearning. InInternationalconferenceonmachine C.;Gao,Y.;andZhang,C.2021.EpisodicMulti-agentRein-
learning,7487–7498.PMLR. forcementLearningwithCuriosity-drivenExploration. Ad-
vancesinNeuralInformationProcessingSystems,34.
Radford, A.; Narasimhan, K.; Salimans, T.; Sutskever, I.;
et al. 2018. Improving language understanding by gener- Zhou, M.; Luo, J.; Villella, J.; Yang, Y.; Rusu, D.; Miao,
ativepre-training. InOpenAI.OpenAI. J.; Zhang, W.; Alban, M.; FADAKAR, I.; Chen, Z.; et al.
2021.SMARTS:AnOpen-SourceScalableMulti-AgentRL
Rashid, T.; Farquhar, G.; Peng, B.; and Whiteson, S.
TrainingSchoolforAutonomousDriving. InConferenceon
2020. Weighted qmix: Expanding monotonic value func-
RobotLearning,264–285.PMLR.
tionfactorisationfordeepmulti-agentreinforcementlearn-
ing.Advancesinneuralinformationprocessingsystems,33:
10199–10210.
Rashid,T.;Samvelyan,M.;Schroeder,C.;Farquhar,G.;Fo-
erster, J.; and Whiteson, S. 2018. Qmix: Monotonic value
function factorisation for deep multi-agent reinforcement
learning. In International Conference on Machine Learn-
ing,4295–4304.PMLR.
Reed, S.; Zolna, K.; Parisotto, E.; Colmenarejo, S. G.;
Novikov, A.; Barth-Maron, G.; Gimenez, M.; Sulsky, Y.;
Kay,J.;Springenberg,J.T.;etal.2022. Ageneralistagent.
arXivpreprintarXiv:2205.06175.
Samvelyan,M.;Khan,A.;Dennis,M.D.;Jiang,M.;Parker-
Holder, J.; Foerster, J. N.; Raileanu, R.; and Rockta¨schel,
T.2023. MAESTRO:Open-EndedEnvironmentDesignfor
Multi-Agent Reinforcement Learning. In The Eleventh In-
ternationalConferenceonLearningRepresentations.
Samvelyan, M.; Rashid, T.; De Witt, C. S.; Farquhar, G.;
Nardelli, N.; Rudner, T. G.; Hung, C.-M.; Torr, P. H.; Fo-
erster,J.;andWhiteson,S.2019. Thestarcraftmulti-agent
challenge. arXivpreprintarXiv:1902.04043.
Silver,D.;Hubert,T.;Schrittwieser,J.;Antonoglou,I.;Lai,
M.;Guez,A.;Lanctot,M.;Sifre,L.;Kumaran,D.;Graepel,
T.;etal.2018. Ageneralreinforcementlearningalgorithm
thatmasterschess,shogi,andGothroughself-play.Science,
362(6419):1140–1144.
Son, K.; Kim, D.; Kang, W. J.; Hostallero, D. E.; and Yi,
Y. 2019. Qtran: Learning to factorize with transformation
for cooperative multi-agent reinforcement learning. In In-
ternational Conference on Machine Learning, 5887–5896.
PMLR.
Wang, J.; Ren, Z.; Liu, T.; Yu, Y.; and Zhang, C. 2020a.
QPLEX: Duplex Dueling Multi-Agent Q-Learning. In In-
ternationalConferenceonLearningRepresentations.
Wang,J.;Xu,W.;Gu,Y.;Song,W.;andGreen,T.C.2021.
Multi-agent reinforcement learning for active voltage con-
trol on power distribution networks. Advances in Neural
InformationProcessingSystems,34:3271–3284.Appendix SMACv2isanewversionofSMACthatusesprocedural
content generation to improve stochasticity, including ran-
Environments
domteamcompositions,randomstartpositions,andincreas-
StarCraftIIisareal-timestrategygamefeaturingthreedif- ingdiversityamongunittypesbyusingthetrueunitattack
ferentraces,Protoss,Terran,andZerg,withdifferentprop- andsightranges.Eachracehasaspecialunitthatshouldnot
ertiesandassociatedstrategies.Theobjectiveistobuildan begeneratedtoooften:thecolossusinProtoss,themedivac
army powerful enough to destroy the enemy’s base. When unit in Terran, and the baneling unit in Zerg. All of these
battlingtwoarmies,playersmustensurearmyunitsareact- special units are spawned with a probability of 10%. The
ingoptimally. otherunitsusedspawnwithaprobabilityof45%.Random
StarCraft Multi-Agent Challenge (SMAC) is a partially startpositionsinSMACv2comeintwodifferentflavors.In
observablereinforcementlearningbenchmarkbuiltinStar- reflectscenarios,thealliedunitsarespawneduniformlyran-
Craft II. An individual agent with parameter sharing con- domlyononesideofthescenario.Theenemypositionsare
trolseachalliedunit,andahand-codedbuilt-inStarCraftII the allied positions reflected in the vertical midpoint of the
AI controls enemy units. The difficulty of the game AI is scenario.Thisissimilartohowunitsspawnedintheorigi-
set to the “very difficult” level. On the SMAC benchmark, nalSMACbenchmarkbutwithoutclusteringthemtogether.
agentscanaccesstheirlocalobservationswithinthefieldof Insurroundscenarios,alliedunitsarespawnedatthecenter
viewateachtimestep.Thefeaturevectorcontainsattributes and surrounded by enemies stationed along the four diago-
of both allied and enemy units: distance, relative nals. There are two changes to the observation space from
x,relative y,health,shield,andunit type.In SMAC.First,eachagentobservestheirfield-of-viewdirec-
addition, agents can observe the last actions of allied units tion.Secondly,eachagentobservestheirpositioninthemap
and the terrain features surrounding them. The global state as x- and y-coordinates. This is normalized by dividing by
vector includes the coordinates of all agents relative to the themapwidthandheight,respectively.Theonlychangeto
centerofthemapandotherfeaturespresentinthelocalob- thestatefromSMACwastoaddthefield-of-viewdirection
servationofagents.ThestatestorestheenergyofMedivacs, of each agent to the state. In addition, the fixed attack and
thecooldownoftherestofthealliedunits,andthelastac- sightrangearereplacedbythevaluesfromSC2.Theattack
tionsofallagents.Notethattheglobalstateinformationis rangeformeleeunitsisimposedto2becauseusingtheac-
onlyavailabletoagentsduringcentralizedtraining.Allfea- tualattackrangesmakesattackingtoodifficult.
turesinstateandlocalobservationsarenormalizedbytheir The reward setting for the dense and sparse cases on the
maximumvalues. SMACbenchmarkisshowninTab.1.
After receiving the observations, each agent is al- We use SC2.4.6.2.69232 (the same version for the
lowed to take action from a discrete set which consists evaluationofVDN,QMIX,QPLEX,QTRAN,andRODE)
of move[direction], attack[enemy id], stop instead of SC2.4.10 (the version for the evaluation of
and no-op. Move direction includes north, south, east, MAPPO) and SC2.4.1.4 (the version for the evaluation
and west. Note that the dead agents can only take ofLIIR).Performanceisnotcomparableacrossversions.
no-op action while live agents cannot. For health
units, Medivacs use heal[agent id] actions instead ExperimentalSetup
of attack[enemy id]. Depending on different sce- We adopt the same architectures for QMIX1, QPLEX1,
narios, the maximum number of actions varies between CW-QMIX1,RODE2,MAVEN3,EMC4 astheirofficialim-
7 and 70. Note that agents can only perform the
plementations (Samvelyan et al. 2019; Wang et al. 2020a;
attack[enemy id]actionwhentheenemyiswithinits
Rashidetal.2020;Wangetal.2020b;Mahajanetal.2019;
shooting range. At each time step, agents take joint action
Zhengetal.2021).
andreceiveapositiveglobalrewardbasedonthetotaldam-
Each agent independently learns a policy with fully
age dealt to the enemy units. In addition, they can receive
sharedparametersbetweenallpolicies.Weused RMSProp
an extra reward of 10 points after killing each enemy unit with alearning rateof 5×10−4 and γ = 0.99, buffersize
and200pointsafterkillingallenemyunits.Therewardsare
5000,mini-batchsize32forallalgorithms.Thedimension
scaledtoaround20,sothemaximumcumulativerewardis
ofeachagent’sGRUhiddenstateissetto64.Forourexper-
achievableineachscenario.
iments, we employ an ϵ-greedy exploration scheme for the
jointpolicy,whereϵdecreasesfrom1to0.05over1million
Attribution Densereward Sparsereward timesteps in 6h vs 8z, 3s5z vs 3s6z and corridor,
andover50thousandtimestepsinothermaps.
Win +200 +200
Thepretrainingphasestartsatthebeginningofthetrain-
Oneenemydies +10 +10
ing and ends at 50k and 100k for 2M-step and 5M-step
Oneallydies 0 0
maps,respectively.Duringthepretraining,wesettheprob-
Enemy’shealth +healthdifference 0
abilityαofinitializingtheagentsatstates as1.Afterpre-
Enemy’sshield +shielddifference 0 0
Ally’shealth 0 0
1https://github.com/oxwhirl/wqmix
Enemy’sshield 0 0
2https://github.com/TonghanWang/RODE
3https://github.com/AnujMahajanOxf/MAVEN
Table1:Rewardsetting. 4https://github.com/kikojay/EMCScenario EM Scenario EM Hyperparameter Value Hyperparameter Value
t t
MMM2 180 6h vs 8z 150 criticlr 5e-4 actorlr 5e-4
2c vs 64zg 400 3s5z vs 3s6z 170 ppoepoch 5 ppo-clip 0.2
5m vs 6m 70 10m vs 11m 150 optimizer Adam batchsize 3200
corridor 400 3s vs 5z 250 optimeps 1e-5 hiddenlayer 1
protoss 5v5 200 terran 5v5 200 gain 0.01 trainingthreads 32
protoss 10v11 200 zerg 5v5 200 rolloutthreads 8 γ 0.99
3m 60 8m 120 hiddenlayerdim 64 activation ReLU
2m vs 1z 150 2s3z 120
Table4:Hyper-parametersinMAPPO.
Table2:Themaximaltimestepsineachmap.
Hyperparameter Value Hyperparameter Value andthenpredictascore.ThelossfunctionforCVAE-GAN
isthesameastheobjectivein(Baoetal.2017).
numberoflayers 6 maxtimesteps 400
For GC-policy, we treat any trajectory as a successful
attentionheads 8 weightdecay 0.1
trail for reaching its final state conditioned on the prompt.
embeddingdims 64 maxRTG 20
Therefore, we train the individual policy for each agent i
gradnormclip 1.0 learningrate 6×10−4
bymaximizingthelikelihoodoftheactionsforareachgoal
Adambetas (0.9,0.95) trainingepochs 5 J(π ) = E [log (a|s,P(s))].Thispolicyisparametrized
i D πi
using a neural network that takes as the input state and the
Table 3: Hyper-parameters in the transformer-based imagi-
goal (prompt), then returns probabilities for a discretized
nationmodelandbehaviorcloning.
gridofactionsoftheactionspace.Theneuralnetworkcon-
catenates the state and goal together. It passes the concate-
nated input into a feedforward network with two hidden
training,theprobabilityαannealedfrom1.0to0.5over50k layers of size 256 and 128, respectively, outputting logits
and100kfor2M-stepand5M-stepmaps,respectively. for each discretized action. The loss is optimized using the
We build our imagination model implementation based Adam optimizer with learning rate α = 5×10−4, with a
on Decision Transformer5 (Chen et al. 2021). The full list batchsizeof256transitions,takingonegradientstepforev-
of hyperparameters can be found in Tab. 3. We use the erystepintheenvironment.InGC-policy,theagentsreach
maximum timesteps EM in the environment as the context theselectedstatefromanarchiveusingthegoal-conditioned
t
length, which is shown in Tab. 2. The imagination models policyratherthantheenvironmentsimulator.Thearchiveis
weretrainedusingtheAdamWoptimizer. a first-in-first-out buffer that stores states with the top-128
The prompt generator uses a feedforward network with influencevalue.
two hidden layers of size {64,32} to encode the state and The implementation of MAPPO is consistent with their
output{E tM,20,10}dimensionalvectorsforeachmodality, official repositories2 (Yu et al. 2022). As shown in Tab. 4,
i.e., the target timestep, return-to-go, and influence value, all hyper-parameters are left unchanged at the origin best-
respectively.Thesevectorsthenturnintoclassprobabilities performingstatus.
usingasoftmaxfunction.
For the baselines in the sparse-reward setting, we adopt
ForCW-QMIX,theweightfornegativesamplesissetto the same architecture for LIIR3 (Du et al. 2019) and
α=0.5forallscenarios. MASER4 (Jeon et al. 2022) as their official implementa-
For CG-diffusion, the implementation is based on Dif-
tions.
fuser1 (Janneretal.2022).WeuseN =100diffusionsteps
WeconductexperimentsonanNVIDIARTX3090GPU.
forallscenarios.
Each task needs to train for about 12 to 20 hours, depend-
For CVAE-GAN, the generator G(s′|z,c) first encodes
ingonthenumberofagentsandepisodelengthlimitofeach
the 64-dimensional input noise z using a 64-dimensional
map.Weevaluate32episodeswithdecentralizedgreedyac-
fully-connected layer, where c = (s,P(s)) is the condi-
tionselectionevery10ktimestepsforeachalgorithm.
tion, Then, it produces the reconstructed next state by an-
Allfiguresintheexperimentsareplottedusingmeanand
otherfully-connectedlayer.Weapplyasigmoidfunctionat
standard deviation with confidence internal 95%. We con-
the output layer and scale the output by the range of each
duct five independent runs with different random seeds for
modality defined by the task space. Batch normalization is
eachlearningcurve.
used in all layers. The classifier C(P(s)|s) uses the same
architectureasthepromptgeneratorinIIEandistrainedin
thesameway.InthediscriminatorD(G(s′|z,c)),afeedfor- PseudocodeforPromptSampling
ward network with two hidden layers of size {128,64} is
used to encode the state or the output from the generator,
2https://github.com/zoeyuchao/mappo
5https://github.com/kzl/decision-transformer 3https://github.com/yalidu/liir
1https://github.com/jannerm/diffuser 4https://github.com/Jiwonjeon9603/MASERAlgorithm1:PseudocodeforPromptSampling compute the current health by hc = [h −(1−β)H ]/H
i i i i
Given an environment E, the state s . κ = 10. The return anduseitasthefeatureofhealthvaluesforthestateandthe
0
upperboundR high =20,thereturnlowerboundR low =0, observations. Moreover, we kill the agent i if hc i ≤ 0 after
thetimestepupperboundT = EM,thetimesteplower takingthejointactionateachtimestep.
max t
bound T = 0, the influence upper bound I = 10,
min max Imagine,Initialize,ExplorePseudocode
theinfluencelowerboundI =0
min
1: Compute 11 logits (I = 0,...,10) for the categorical Algorithm2:Imagine,Initialize,ExplorePseudocode
influencevaluedistributionp(I|s 0) Input: initial agent network parameters θ, initial mixing
#Increaselogitsproportionallytoinfluencevaluemag- network parameters ϕ, initial imagination model ψ, initial
nitudestopreferhigh-influencestateinthetrajectory prompt generator ξ, replay buffer B, few-shot demonstra-
2: Definealog-probabilitylogP(I∗|s 0) = logp(I|s 0)+ tionsD
κ(I−I low)/(I high−I low) Hyperparameters:samplingratioα,segmentsnumberK
#Sampleainfluencevalue
3: I 0 ∼P(T∗|s 0) 1: whilenotconvergeddodo
4: ComputeE tM +1logits(T = 0,...,E tM)forthecate- 2: Resettheenvironmenttotheinitialstates 0
goricaltimestepdistributionp(T|s 0,I 0) 3: Samplez ∼U(0,1)
#Increaselogitsproportionallytotimestepmagnitudes 4: ifz >αthen
5:
t Do ep fir ne efert ahet li om ge -s pt re op bl aa bte ilr iti ynth le ogtra Pje (Tcto ∗r |sy
0,I 0) =
5: S ina gm tp ole Aa lgp or ro itm hmpt 1P(s i;ξ) = {I 0,T 0,R 0}accord-
logp(T|s ,I )+κ(T −T )/(T −T )
0 0 low high low 6: Prependtheone-shotdemonstrationwhosediscrip-
#Sampleatimestep
tion has the highest similarity with the sampled
6: T 0 ∼P(T∗|s 0,I 0) promptbeforetheinputoftheimaginationmodel
7: Compute 21 logits (R = 0,...,20) for the categorical #Imagine
returndistributionp(R|s 0,T 0)
7: Generatetheimaginedtrajectorythroughtheimag-
#Increaselogitsproportionallytoreturnmagnitudesto
inedmodel
preferhigh-returnstateinthetrajectory #Initialize
8: Define a log-probability logP(R∗|s 0,I 0,T 0) = 8: Resettheenvironmenttothelaststateoftheimag-
logp(R|s ,I ,T )+κ(R−R )/(R −R )
0 0 0 low high low inedtrajectory
#Sampleareturn
9: InitializeGRUstateoftheagentnetwork
9: R 0 ∼P(R∗|s 0,I 0,T 0) #Explore
10: returnI 0,T 0,R 0 10: Explore until the environment is terminated, store
thestitchedtrajectoryintoB
11: else
EnvironmentSimulator 12: Explore until the environment is terminated, store
theinteractionsequenceintoB
Wedesignaconvenientinterfaceintheresetfunction,serv-
13: Obtain K trajectory segments and store them into
ingastheenvironmentsimulatorinImagine,Initialize,and
D
Explore.Supposeallunitsareinitializedatthestartingpoint
14: Updatethepromptgeneratorξusingsegments
s , and we have already obtained the imagined trajectory
0 15: Updatetheimaginationmodelψusingsegments
x = {s ,o ,u ,r ,s ,...,u ,r }. The last state s is
0:T 0 0 0 0 1 T T T 16: endif
formulated as a distribution class D over team unit types,
17: SampleabatchofsequencesfromB
startpositions,andhealthlevels.
18: Update the agent network θ and the mixing network
First, we kill all units through the DebugKillUnit com-
ϕ
mand from the s2clientprotocol.debug pb2 package. Then,
19: endwhile
wecreatenewunitsbasedonthedistributionsDthroughthe
DebugCreateUnitcommand,wherewecansettheunit type,
theowner(alliesorenemies),andinitializedpositionsusing
MoreRelatedWork
thePoint2Dcommand.Notethatwehavetorecovertheco-
ordinatesofallunitsintherealaxisbecausetheyhavebeen Auto-curriculaLearning. Curriculumlearningisatech-
processedastherelativevaluetothecenterofthemapinthe niquethatleverageseasierdatasetsortaskstofacilitatetrain-
stateandtheobservations. ing.Inreinforcementlearning(RL),thisapproachinvolves
Since we cannot directly initialize the health level of a selectingtasksfromapredefinedsetorparameterizedspace
unit through the DebugCreateUnit command, we scale the of goals and scenes to expedite performance improvement
health value in the raw observation from the controller and on the target task (Florensa et al. 2018; Matheron, Perrin,
kill agents when their health is below the health level. For and Sigaud 2020; Fang et al. 2021; Ao et al. 2022). In the
example, consider the maximal health of the agent i is H , multi-agent field, recent works have explored the adaptive
i
theremaininghealthvalueatthelaststates isβH .Sup- selectionofco-playersincompetitivegames,whereplaying
T i
posetheexacthealthvalueintherawobservationish .We against increasingly stronger opponents is crucial to avoid
iexploitation by other agents (Silver et al. 2018; Samvelyan
etal.2023).Thesemethodsusearegret-basedcurriculumto
attainaNash-Equilibriumpolicyagainsteveryrationalagent
in every environment. It is important to note that these ap-
proaches primarily concentrate on competitive tasks rather
thancooperativeones.Additionally,theapplicabilityofthe
currentauto-curriculalearningmethodstoMARLtaskswith
complex observations is limited, as their flexibility is often
constrained to task spaces using low-dimensional parame-
tersandrequireswell-definedtasksimilarity.
Limitations
Themainlimitationofthispaperisthatitisconfinedtosce-
narioswithinthegameStarCraftII.Thisisanenvironment
that, while complex, cannot represent the dynamics of all
multi-agent tasks. Evaluation of MARL algorithms, there-
fore,shouldnotbelimitedtoonebenchmarkbutshouldtar-
getavarietywitharangeoftasks.
The environment simulator in this paper is a small dis-
tribution class that can change how units are generated. It
can be easily generalized to many multi-agent benchmarks
and applications. For example, we can add a configuration
oftheinitialpositionoftheagentsintothereset worldfunc-
tion on the particle world benchmark (Lowe et al. 2017).
WecanalsoutilizetheAddPlayerfunctiontobuildaplayer
bydefiningitscoordinatesandroleontheGoogleResearch
Footballbenchmark(Kurachetal.2020).Weleavetheim-
plementationofthisfunctionalityasfuturework.