Learning a Generalized Physical Face Model From Data
LINGCHENYANG,ETHZurich,Switzerland
GASPARDZOSS,DisneyResearch|Studios,Switzerland
PRASHANTHCHANDRAN,DisneyResearch|Studios,Switzerland
MARKUSGROSS,ETHZurich,SwitzerlandandDisneyResearch|Studios,Switzerland
BARBARASOLENTHALER,ETHZurich,Switzerland
EFTYCHIOSSIFAKIS,UniversityofWisconsinMadison,USA
DEREKBRADLEY,DisneyResearch|Studios,Switzerland
actuations bones skin
Generalized
Physical
Face Model
Input Image Fitting Fitted Physical Face Model Simulated Facial Animation
Fig.1. Wepresentadeepgeneralizedphysicalfacemodelthatcanbefittoasinglefaceimageor3Dscan.Themodelproducesanidentity-specificmaterial
spacewithbones,skinandsofttissue,togetherwithper-expressionjawtransformationsandelementactuationsforfacialsimulation.Oncefittoanunseen
identity,themodelcanbeanimatedtocreatephysics-basedfacialanimation.Applicationslikeretargeting,interpolation,anatomyediting,andphysical
effectssuchascollisionavoidanceandmuscleparalysisareshowninSec.6.
Physically-basedsimulationisapowerfulapproachfor3Dfacialanimationas areoneoftheleadingcontributorstowidespreadadoptionofCG
theresultingdeformationsaregovernedbyphysicalconstraints,allowingto contentinfilms,videogamesandvirtualenvironments.Thereverse
easilyresolveself-collisions,respondtoexternalforcesandperformrealistic isalsotrue,inthatunrealistic3Dcharacterscanleadtoawidespread
anatomyedits.Today’smethodsaredata-driven,wheretheactuationsfor rejection of the CG content, which is what makes high-quality
finiteelementsareinferredfromcapturedskingeometry.Unfortunately,
facialanimationsocritical.Mostoften,facesareparameterizedand
theseapproacheshavenotbeenwidelyadoptedduetothecomplexity
controlledbylinearblendshaperigs,whicharepopularduetotheir
ofinitializingthematerialspaceandlearningthedeformationmodelfor
simplicityandeaseofuse.Unfortunately,blendshaperigssuffer
eachcharacterseparately,whichoftenrequiresaskilledartistfollowedby
fromwell-knowndrawbackslikeprovidingonlylinearmotionand
lengthynetworktraining.Inthiswork,weaimtomakephysics-basedfacial
animationmoreaccessiblebyproposingageneralizedphysicalfacemodel exhibitingsurfaceinter-penetrations(particularlyaroundthelips).
thatwelearnfromalarge3Dfacedataset.Oncetrained,ourmodelcanbe Analternative,andmorephysicallyaccurateapproachistouse
quicklyfittoanyunseenidentityandproduceaready-to-animatephysical physicalsimulationforfacialanimation,whereanatomicalmuscle
facemodelautomatically.Fittingisaseasyasprovidingasingle3Dface actuationsdrivethesofttissuedeformation.Asabenefit,withina
scan,orevenasinglefaceimage.Afterfitting,weofferintuitiveanimation simulatoritiseasytoaddconstraintstoidentifysurfacecontacts
controls,aswellastheabilitytoretargetanimationsacrosscharacters.All andpreventcollisions,whileproducingmorenaturalnonlinearface
thewhile,theresultinganimationsallowforphysicaleffectslikecollision
motion.Originally,physics-basedmethodswerepopularonlyin
avoidance,gravity,paralysis,bonereshapingandmore.
academicsettingsduetotheircomplexityofadoptioninproduction,
AdditionalKeyWordsandPhrases:DifferentiablePhysics,DeepLearning, howeverrecentlymoreanatomicalandphysics-inspiredapproaches
Physically-BasedFacialAnimation,DigitalHumans havegainedpopularityinthefilmindustry[Choietal.2022]1.
Therearegenerallytwoapproachestophysics-basedfacialani-
1 INTRODUCTION
mation.Inthefirst-principles-basedmethod,researchersmodelthe
Fordecades,3Dfacialanimationhascontinuedtobeachallenging muscleactuationmechanismaccordingtotheauthenticcomplex
andwell-studiedproblemincomputergraphics.Thequestformore musclestructuresintheface,eitherobtainedthroughCT/MRI[Sifakis
realisticfacialmotioncontinuesbecausebelievable3Dcharacters etal.2005]oranoff-the-shelfmuscletemplate[Baoetal.2018;Ichim
etal.2017].Althoughphysiologicallysound,thisstrategyislabori-
Authors’ addresses: Lingchen Yang, ETH Zurich, Switzerland, lingchen.yang@
ous,anditsrealismhighlydependsontheaccuracyoftheanatomical
inf.ethz.ch; Gaspard Zoss, DisneyResearch|Studios, Switzerland, gaspard.zoss@
disneyresearch.com; Prashanth Chandran, DisneyResearch|Studios, Switzerland, model.Assuch,asecondapproachemergedbasedonshapetar-
prashanth.chandran@disneyresearch.com;MarkusGross,ETHZurich,Switzerlandand geting[Kláretal.2020],wherefine-grainedandstructure-agnostic
DisneyResearch|Studios,Switzerland,gross@disneyresearch.com;BarbaraSolenthaler,
ETHZurich,Switzerland,solenthaler@inf.ethz.ch;EftychiosSifakis,UniversityofWis-
consinMadison,USA,sifakis@cs.wisc.edu;DerekBradley,DisneyResearch|Studios, 1https://www.fxguide.com/fxfeatured/exclusive-joe-letteri-discusses-weta-fxs-new-
Switzerland,derek.bradley@disneyresearch.com. facial-pipeline-on-avatar-2/
4202
beF
92
]VC.sc[
1v77491.2042:viXra2 • L.Yang,G.Zoss,P.Chandran,M.Gross,B.Solenthaler,E.Sifakis,D.Bradley
actuationisinferredfromthecapturedskingeometryandusedto abilitytoparalyzepartsoftheface,editanatomicalbonestructures,
targettheundeformedsofttissue[Srinivasanetal.2021;Yangetal. andobeygravitationalorotherexternalforces.
2022,2023].Sinceonlytheskingeometryandskeletonarerequired Insummary,weproposeaphysicalfacemodelthatisasgener-
tolearnthemodel,thismethodlargelyeasesthemodelinglabor alizableandcontrollableascurrentlinearblendshapemodels,but
whileassuminggreatflexibility,highfidelity,andimprovedrealism. withtheaddedbenefitofmorephysically-accuratefacialdeforma-
Whiletheshapetargetingapproachisattractive,itiscurrently tions.Webelieveourworkwillhelptodemocratizephysics-based
onlytractableforafewherocharactersinaproduction.Thislim- facialanimation,makingitassimpleasfittingananimation-ready
itation stems from two aspects. First, although the modeling ef- simulationmodeltoasinglescanorimage.
fort is less than a first-principles approach, there is still a dedi-
catedsetupprocedurerequiredforeachcharacter.Thisinvolves
2 RELATEDWORK
scanning and tracking a dataset of facial geometry from the ac-
tor in various expressions, building a simulation-ready material Facialmodelsusedinanimationrangefromsimplegloballinear
spacetomodelthesofttissueandbonesatrest,andthentrain- shapemodelstocomplexlocalmodelsthatincorporatetheunder-
ingadedicatedneuralnetworktopredictthemuscleactuations lyingfacialanatomythroughanatomicalconstraintsorphysical
foreachexpression-dependentdeformation.Secondly,thereisa simulation.Wewillfocusourdiscussiononanatomicalfacemodels
time-consuming,memory-intensive,andidentity-specificdifferen- andthegenerationof3Dfacemodelsfrommonocularinput.
tiablesimulationinthetrainingloop.Itwouldbeinfeasibletotrain
manycharacter-specificnetworkswithinareasonableamountof
2.1 Physics-BasedFacialAnimation
timeandresources,usingthecurrently-availablearchitecturesfor
soft-tissue-basedphysicalfaceanimation. Physics-based facial animation typically employs two main ap-
Inthisworkweaimtoalleviatetheseissuesandmakephysics- proaches. The first, known as the first-principles-based method,
basedfacialanimationmorefreelyaccessible.Tothisend,wepro- requiresuserstointricatelymodelthemuscleactuationmechanism
pose a new, generalized physical face model that can be easily basedonthecomplexstructuresoffacialmuscles.Thesestructures
adaptedtoanynewcharacterwithoutmanualsetupcosts.Weac- arederivedfromsourcessuchasCT/MRIscans[Sifakisetal.2005]
complishthisbylearningasingleimplicitneuralmodelforactuation orpre-existingmuscletemplates[Baoetal.2018;Ichimetal.2017].
mechanismstrainedonalargedatasetofhundredsofidentitiesper- Whilethisapproachensuresphysiologicalaccuracy,thegeneration
formingavarietyofexpressions.Naturally,theabove-mentioned ofsuchmodelsislabor-intensiveandtherealismoftheresulting
limitationswouldposeaconsiderableproblemforthisapproach animation heavily relies on the precision of the anatomical and
aseachidentitywouldrequireapersonalizedmaterialspaceand biomechanicalmodels.
trainingonsuchalargedatasetwithsimulationintheloopwould Inresponsetothechallengesposedbythefirst-principles-based
beimpossible.Weovercomethesehurdleswithourtwomaincon- method,asecondapproachhasemerged,knownasshapetargeting
tributions.First,weproposeadesignthatallowsforsimulation-free [Kláretal.2020].Inthismethodology,everyelementwithinthe
training,whichisfastandmemoryefficient,allowingustotrainon meshisconsideredactiveandsubjecttoactuationforthepurposeof
alargedatasetoffacesandlearnthegeneralizabilityonewouldneed inducingforcesthatdrivethemotionofthefaceorbody.Torepro-
forsuchanapplication.Second,weproposeanarchitecturethat duceidentity-specificfacialexpressions,anactuationmechanism
automaticallypredictsanidentity-conditionedmaterialspaceby isoptimizedbasedonfacecapturedata.IntheworkofSrinivasan
warpingasinglecanonicalmaterialspace.Despitenothavingsimu- etal.[2021],theintegrationofneuralnetworkswithacomprehen-
lationinthetrainingloopnorhand-craftedper-identitymaterial sivemusclemodelfacilitatedthelearningofthemuscleactuation
spaces,ournetworkstillproducesoutputsthatarecompatiblewith mechanism.Subsequentadvancementsinthefieldtransitionedto-
aphysicssimulator,suchthattheresultingfacescanbeanimated wardsanimplicitneuralrepresentation,renderingtheactuation
usingphysics-basedfacialanimation. mechanism more compact and independent of resolution [Yang
Ourmodelisconditionedontwolatentcodes,oneforidentityand etal.2022].Buildinguponthisfoundation,Yangetal.[2023]fur-
oneforexpression,andtheoutputisanidentity-specificmaterial therextendedthemethodologybytrainingtheneuralnetworkon
space(i.e.,skull,jaw,skinandsofttissueinbetween)coupledwith captureddatafromasmallnumberofindividualssimultaneously.
identity-andexpression-specificactuationsandbonekinematics Thismulti-identitytrainingapproachenablesthemodeltolearn
thatcanallbereadilyprovidedtoanoff-the-shelfphysicssimulator. diverseexpressionstylesbasedondistinctactuationpatternsand
Asaresult,ourpre-trainednetworkcanbeusedtogeneratean allowsapplicationssuchasstyleretargeting.
animation-readyphysics-basedsimulationmodelforanynewchar- Notably,theseactuation-basedapproachesstreamlinethemod-
acter,simplybymodifyingtheidentitylatentcode.Wedemonstrate elingprocessbyonlynecessitatingskingeometryandbonedata
thatourmodelcanbefittoasingle3Dneutralscanofanactor,or formodeltraining.Nevertheless,theirheavyrelianceonidentity-
evensimplytoasinglefaceimage.Oncefitted,themodelallows specificdata,hand-craftedmaterialspacesandarchitecturesthat
animationthroughthecontrolsofacommon3Dmorphableface onlyallowtrainingononeorfewidentitiesatatimerepresent
model,whicharemappedtoourlatentspace.Furthermore,our significantlimitations,particularlyhinderingtheirgeneralizability
methodsupportsanimationretargetingbyswappingidentitycodes. andwidespreadapplicationinproductionsettings.Weaddressthis
Inallcases,theresultinganimationbenefitsfromphysicaleffects byintroducingageneralizedphysicalfacemodelthatcanbeeasily
likethedetectionofsurfacecontactsandcollisionavoidance,the adaptedtoanynewcharacter.LearningaGeneralizedPhysicalFaceModelFromData • 3
TherecentworkofWagneretal.[2023]istheclosestinspirit etal.2020a],withthemaindifferencethatoursallowstosimulate
toourwork,asweshareacommonmotivationalthoughverydif- physicaleffects.Acommonapplicationofmost3DMMsistheiruse
ferentsolutions.Theyproposeanextensiontolinearblendshape inmonocularfacereconstruction,e.g.,fittingthemodeltoimages.As
modelsthataimstomimicphysics-basedfacialanimationwithin such,wepresentahigh-levelsummaryofexistingtechniqueswhere
alinearframework.Physicalsimulationisonlyusedtogenerate amorphablemodelisusedtorecoveraperson’sfacialgeometryin
trainingdata,notatruntime,andthustheyaretiedtotheprescribed 3Deitherbydirectlyoptimizingthefaceshapebasedonanobserved
simulationeffectspresentinthedataset.Forexample,theycannot imageorthroughaninference-drivenapproachthattrainsneural
handlelipcollisions,whichisaprimaryreasontoemploysimula- networkstopredicttheparametersofamorphablemodel.
tionforfacialanimation.Ontheotherhand,weproposeanonlinear Determiningtheoptimal3DMMshape,expression,andposepa-
physics-basedfacemodeltrainedonrealcapturedata,whichcan rametersforagivenRGBimageisachievedthrougheitheranalysis-
befittoanynovelidentityandprovideatruephysicalmodelthat by-synthesisoptimization[Geceretal.2019,2021]ordeepneural
employssimulationatruntimetoallowanydesiredphysicaleffect, networkregression[Fengetal.2021;Zhangetal.2023;Zielonka
includinglipcollisionavoidance. etal.2022].Recently,therehavealsobeenseveralapproachesthat
relyonadditionalperceptionbasedlosstermstoimprovethevisual
2.2 Anatomically-ConstrainedFaceModels qualityofthesemorphablemodelfits[Daneceketal.2022;Filntisis
etal.2022;Ottoetal.2023].
Anatomicalconstraintsonthefacialsurfaceareoftenusedtoplau-
Comprehensivesurveyson3DMMsandtheirapplicationinmonoc-
siblyrestricttherangeoftheskindeformations.Theanatomically-
ularfacecaptureareprovidedbyEggeretal.[2020],andMoraleset
constrainedlocaldeformationmodel,introducedinthecontextof
al.[2020].Asanapplicationofourgeneralizedphysicalfacemodel
monocularfacialperformancecapturebyWuetal.[2016],initially
wealsoshowtheabilitytofitthemodeltounseenidentitiesinthe
establishedaconnectionbetweentheskinsurfaceandanatomical
formofasinglefaceimage.However,wedonotproposeacompet-
bones. This was achieved by modeling the thickness of the soft
ingmethodforaccurate3Dgeometryreconstructionbutrathera
tissuebetweenabonepointandtheskinsurface,togetherwithskin
convenientapproachtoobtainananimation-readyphysicalmodel,
slidingcoupledwithbulging.Thisanatomically-constrainedface
usingasimilarfittingapproachasinthefieldofmonocularface
modelwasalsobeneficialinfacemodelingapplications,helping
capture.
untraineduserstoquicklycreatebelievabledigitalcharacters[Gru-
beretal.2020].Inthedomainoffacialperformanceretargeting, 3 PRELIMINARIES:ACTUATEDFACESIMULATION
Chandranetal.[2022]employedthesamemodeltoconfineare-
Incontinuummechanics,motionischaracterizedbyaninvertible
targetedshapewithintherealmofanatomicallyplausibleshapes
map𝜙 : X ∈ Ω0 → x ∈ Ωfromtheundeformedmaterialspace
specifictoatargetactor.Animplicitvariantoftheanatomicalface
Ω0 tothedeformedspace Ω.Thedeformationgradient,F(X) =
modelwaspresentedbyChandranandZoss[2023],whichfacili-
tatesthelearningofacontinuousanatomicalstructurethatdensely
∇ X𝜙(X),encodesthelocaltransformationsincludingrotationand
stretch.Thequasi-staticstateof𝜙 intheabsenceofexternalforce
constrainstheskinsurface.Themodelcandisentangledeforma-
isgovernedbythepoint-wiseequilibrium:
tionarisingfromrigidbonemotionandnon-rigiddeformations
𝜕Ψ
createdbymuscleactivations.Qiuetal.[2022]learnedananatomi- div·P=div· (F)=0, (1)
calfacialshapemodelfrommedicalimagingdata,andpresenteda 𝜕F
morphablemodelthatisabletogeneratefacesthatjointlymodel where P is the first Piola–Kirchhoff stress tensor that measures
theskull,facialsurfaceandappearance.Choietal.[2022]replaced theinternalforce.Forhyperelasticmaterial,Pisassociatedwith
themuscle-basedparameterizationusedearlier[Sifakisetal.2005; a specific energy density function Ψ that describes the material
Srinivasanetal.2021]byacollectionofmusclefibercurves,whose behavior.Intuitively,Eqn.(1)meansthenetforcewithinthematerial
contractionandrelaxationprovideafine-grainedparameterization iszeroeverywhere.
ofhumanfacialexpression.Theapproachstrikesabalancebetween Inthecontextofactuatedfacesimulation,thematerialspaceΩ0
therequirementsforanatomically-basedandartist-friendlymod- isdefinedastheundeformedsofttissuespaceconfinedbetweenthe
els,butcomesattheexpenseofreducedphysicalaccuracy,asthe restbones𝜕Ω0 andskin𝜕Ω0 .𝜕Ω0 consistsoftheskull
bones skin bones
simulationissolelyemployedinapre-processingsteptoacquirean 𝜕Ω0 andthejaw𝜕Ω0 ,whichwillconstrainanddragthesoft
skull jaw
approximatedeformationmodelofmusclefibers. tissueduringarticulation.ThedeformedspaceΩisthesofttissue
Similarly, our approach is designed for ease of usability, as it spaceofthetargetexpression.ForΨ,theshapetargetingmodel[Klár
sharesthegeneralityandcontrollabilitycharacteristicsfoundin etal.2020]isemployed:
existinglinearblendshapemodels,yetitofferstheadditionaladvan-
tageofachievingmorephysicallyaccuratefacialdeformation. Ψ(F,A)= min ||F−RA||2 𝐹 =||F−R∗A||2 𝐹, (2)
R∈SO(3)
whereAisasymmetricactuationtensormimickingthelocalmuscle
2.3 Morphable3DFaceModels
actuation.R∗isthepolardecompositionofFA,makingΨrotationally-
Asourgeneralizedphysicalfacemodelisdrivenbyidentityand invariant. Based on embedded simulation, Ω0 is uniformly dis-
expressionandproducesadeformed3Dfaceastheoutput,itisakin cretizedintoasimulationmeshM simusingregularelementswith
totraditional3Dmorphablefacemodelsincurrentliterature[Blanz nodal vertices u0, where the discretized skin, skull and jaw are
andVetter1999;Chaietal.2022;Daietal.2019;Lietal.2017;Yang linearly embedded with barycentric weights W𝑠u0, W𝑚u0, and4 • L.Yang,G.Zoss,P.Chandran,M.Gross,B.Solenthaler,E.Sifakis,D.Bradley
W𝑛u0respectively.WiththeFiniteElementMethod(FEM)applied 4.1 Simulation-freeLearning
to Eqn. (1), the simulation then reduces to an energy minimiza- Given a material space Ω0, where the skin 𝜕Ω0 , skull 𝜕Ω0
tionproblemw.r.t.thedeformedverticesusuchthattheboundary andjaw𝜕Ω jaw areexplicitlydefined,wewantts oki in nferaconts ik nu ull -
conditionsfromthearticulatedbonearesatisfied,asfollows: ousactuationfieldA(·) andajawtransformation{Rjaw,tjaw}to
matchagivenexpressioninasimulation-freemanner.Formally,the
arg umin∑︁
𝑒
𝑉 2𝑒 ||F𝑒(u,u0)−R𝑒∗A𝑒||2 𝐹 (3) objectiveforthei an rv ge mrs inedesi ∫gnisas ||f 𝜙ol (l Xow )−s:
𝜙ˆ(X)||2𝑑X (5)
(cid:20) (cid:21) (cid:20) (cid:21) (cid:20) (cid:21) 2
s.t. W𝑚 u= W𝑚 u0+ 0 , (4) 𝜙(·),A(·),Rjaw,tjaw 𝜕Ω s0 kin
W𝑛 RjawW𝑛 tjaw s.t. div·(∇ X𝜙(X)−R∗(X)A(X))=0, ∀X∈Ω0A∈SO(6) (6)
𝜙(X)=X, ∀X∈𝜕Ω0 (7)
w thh ee rr ie g𝑉 id𝑒 ti rs at nh se fv oo rmlu am tie of nor foe rac thh eel je am we .nt𝑒while{Rjaw,tjaw}denotes 𝜙(X)=RjawX+tjaw,sku ∀ll
X∈𝜕Ω j0 awRjaw ∈SO(3) (8)
Insummary,thesimulationentailsthreecoreelements: 𝜙 ∈Φ (9)
bio
• Identitymaterialspace(Ω0)togetM sim withembedded where𝜙ˆdenotesthegroundtruthdeformationthatisonlydefined
skin,skull,andjaw. on𝜕Ω0 ,i.e.,through3Dscanning.Eqn.(6)comesfromthepoint-
skin
• FacialactuationdefinedbythetensorfieldAoverM sim. wiseequilibriumofEqn.(1)withPinstantiatedfromEqn.(2).Eqn.(7)
• Jawkinematicsviamandibletransformation{Rjaw,tjaw}. and Eqn. (8) guarantee that the skull is fixed and jaw is rigidly
articulated.Eqn.(9)constrainsthemapping𝜙suchthatitresembles
Thelattertwocomponentsfunctionasmuscleactuationmecha- bio-mechanicallyplausiblesofttissuedeformation,whichwerefer
nismsandareutilizedastheinputphysicalconstraints.Thissimu- toasthespaceΦ (andwillelaborateonlaterinthediscussionof
bio
lationprocesscanbemadeefficientlydifferentiablewiththeadjoint theSoftLoss).Themotivationofoursolutionliesinthefactthat
method,allowinginversedesignofAand{Rjaw,tjaw}fromatarget givenanyinvertiblemappingfunction𝜙,Eqn.(6)canbesatisfied
expressionoftheidentity.However,theexpensivecomputation bysettingtheactuationtensorfieldA(·)as:
fromthesimulationandtherequirementofthepre-definedma-
terialspaceΩ0preventthismethodfromlendingitselftoalarge A(X)=R𝜙(X)⊤∇ X𝜙(X), (10)
facedatasetofhundredsofidentities,wheretheidentity-specific whereR𝜙(X)isthepolardecompositionof∇ X𝜙(X)atX.Thisrep-
materialspace(underlyinganatomy)istypicallymissing.
resentationgivesthezerostresstensorPandhencethezerodiver-
genceeverywhere.Basedonthisobservation,theinversedesigncan
4 METHOD besimplifiedtofinding𝜙∗thatcloselyapproximates𝜙ˆon𝜕Ω while
skin
atthesametimesatisfyingEqn.(7),Eqn.(8)andEqn.(9)asmuchas
Ourgoalistobuildageneralizedphysicalfacemodelfromalarge
3Dfacedatasetwhereonlyskingeometryisgiven.Thisfacemodel
possible.Then,wecancomputeA(·)and{Rjaw,tjaw}from𝜙∗auto-
shouldbegenerativeandanimatable,suchthatitcanbefittoava-
matically.Specifically,A(·)isobtainedwithEqn.(10).{Rjaw,tjaw}is
rietyofinputdata(e.g.,3Dscansorfaceimages)andthenanimated obtainedwithProcrustesalignmentbetween𝜙∗(𝜕Ω j0 aw)and𝜕Ω j0 aw.
withintuitivecontrols,providingaconvenientmechanismtoobtain Thesephysicalconstraintswillbecompatiblewiththesimulation
aphysicalfacemodelforanimationpurposes. afterdiscretizationwithFEM.
To achieve this goal, we propose a new network architecture
Toachievethisgoal,weparameterizethemappingfunction𝜙
forimplicitactuationmechanisms(illustratedinFig.2).Atahigh withanimplicitneutralnetworkN𝑒.Then,weintroduceseveral
level,ourmodelisdrivenbytwolatentvariables𝛽 and𝛾,which novellossfunctionstoconstrainit.
representtheidentityandexpression,respectively.Theoutputisthe
ReconstructionLossL .Thefirstlossisthereconstruction
identity-specificmaterialspaceΩ0withdiscretizedsimulationmesh
lossdefinedon𝜕Ω0 :
skin
M simatrest,coupledwiththejawtransformation{Rjaw,tjaw}and skin
actuationtensorfieldAthat,whensimulated,deformsthefaceto ∑︁𝑁𝑣
1
performthegivenexpression. L skin(𝜕Ω s0 kin)=
𝑁
𝑣||N𝑒(X𝑖)−xˆ𝑖||2 2, (11)
AsmentionedinSec.3,actuatedfacesimulationtraditionallycan- 𝑖=1
notbetrainedonlargedatasets.Ourmethodispossiblethanksto whereX𝑖representsthe𝑖-thsampledpointfrom𝜕Ω0 ,andxˆ𝑖indi-
twonovelcontributions.Thefirstoneisdesignedforefficienttrain- skin
catesthecorrespondinggroundtruthposition.Thecorrespondence
ing,whereweparameterizeandlearnthephysicalconstraintsina
istypicallyapproximatedthroughsurfaceregistration.
simulation-freemanner(Sec.4.1).Second,weintroduceamaterial-
spacegenerativenetworkthatproducestheidentity-specificmate- RigidityLossL rigid.Thesecondlossistoenforcetherigidityof
rialspacesautomatically,withnomanualmodeling(Sec.4.2).We thebone,inspiredbyEqn.(8):
w (o Sri eal cl t .efi 4r o .s 3nt )d .he os wcri tb he et yh ae rs eet cw omoc bo inn etr dib tu ot pio rn os vii dn eis to hl eat cio on m, pa ln ed teth pe in pee ll ia nb e- L rigid(𝜕Ω b0)= R∈SOm (3in ),t∈R3∑︁𝑁 𝑖𝑏 𝑁1 𝑏||N𝑒(X𝑖)−(RX𝑖 +t)||2 2, (12)LearningaGeneralizedPhysicalFaceModelFromData • 5
continuous discretized
canonical identity expression ground truth
simulator
Fig.2. Overviewofourmodel.Drivenbyidentity𝛽andexpression𝛾latentcodes,N𝐶 learnstodeformacanonicalmaterialspaceΩ𝐶 tobeidentity-specific,
Ω0,𝛽.ThenN𝑒learnstodeformthematerialspacetomatchagivenexpression,Ω𝛾,𝛽.Thelatentcodesareparameterizedbyacommon3DMM(𝛽ˆ,𝛾ˆ).The
networkistrainedwithphysically-inspiredconstraintssothat,afterdiscretization,asimulatorwillproducephysics-basedfacialanimation.
wherewesample𝑁 𝑏 pointsintotalforeachregion𝜕Ω0.Weapply themethodmuchfasterandeasilyscalablesinceallthelossfunc-
b
thislossseparatelytotheskull𝜕Ω0 andthejaw𝜕Ω0 .Therefore, tionsarepoint-wise.Insummary,thisnetworkgeneratesphysically-
skull jaw
wehaveL =L (𝜕Ω0 )+L (𝜕Ω0 ). constraineddeformationsthatarestrategicallyconvertedintophys-
rigid rigid skull rigid jaw icalconstraintsusedinthesimulation.
FixationLossL .Specificallyto𝜕Ω0 ,weintroducethethird
fix skull 4.2 MaterialSpaceMorphing
loss to enforce the fixation of the skull area, corresponding to
Eqn.(7): Wenowdescribeoursecondmaincontributiontoaddressthefact
thateveryidentityinthedatasetneedsacustommaterialspace.Our
𝑁𝑓 approachistoinferthematerialspaceofanidentityautomatically
L fix(𝜕Ω s0 kull)=∑︁ 𝑁1 ||N𝑒(X𝑖)−X𝑖||2 2, (13) bylearningtomorphasinglecanonicalmaterialspacetoanynew
𝑖 𝑓 person.
GivenacanonicalmaterialspaceΩ𝑐
,wherethecanonicalskin
wherewesample𝑁 𝑓 pointsintotalontheskullarea𝜕Ω0 .
𝜕Ω𝑐 skinandbones𝜕Ω𝑐
bonesareexplicitlydefined,wewanttomorph
skull itintothematerialspaceΩ0ofanidentityusinganotherimplicitnet-
SoftLossL soft.Thefourthlossistolearnbio-mechanicallyplau- workN𝑐.Weproposethreelossfunctionstoconstrainthesolution
sibledeformationbasedontheYoung’sModulus(𝐸)andPoisson’s
duringtraining.
Ratio(𝜈),inspiredbyEqn.(9).Thislossconsistsoftwoterms,an
IdentityLossL .Thefirstlossistoprovidesupervisiononthe
elasticoneandavolume-preservingone: id
skinarea,similartoEqn.(11).
L soft(Ω0)=∑︁𝑁 𝑖𝑠 𝑁1
𝑠
R∈m SOin (3)𝜇||∇ XN e(X𝑖)−R||2 2+
(14)
L id(𝜕Ω𝑐 skin)=∑︁ 𝑖𝑁 =𝑣
1
𝑁1 𝑣||N𝑐(X𝑐 𝑖)−Xˆ 𝑖||2 2, (15)
detm (Din )=1𝜆||∇ XN e(X𝑖)−D||2 2, whereX𝑐
𝑖
representsthe𝑖-thsampledpointfrom𝜕Ω𝑐 skin,andXˆ
𝑖
indicatesthecorrespondinggroundtruthpositionontheneutral
identitymeshinthedataset,attainablefrom3Dscanning.
wherewesample𝑁
𝑠
pointsintotalintheentirematerialspaceΩ0.
𝜇and𝜆aretheLaméparameters,describingthematerialbehavior. BoneShapeLossL bone.Sinceweonlyhavedirectsupervision
Thesetwoparametersareparameterizedby𝐸and𝜈as𝜆=𝐸𝜈/(1+ ontheskinarea,weproposetoconstraintheboneshapesusingan
𝜈)(1−2𝜈)and𝜇=𝐸/2(1+𝜈)respectively.Thislossisessentialas off-the-shelfparametricbonegenerator[Qiuetal.2022],whichcan
itnotonlyconstrainsthedeformationbutalsoimplicitlybuildsthe predictplausibleskullandjawshapesgivenaneutralfacemesh.
connectionbetweentheboneandtheskin.Whentheoutputskin Formally,thelossisasfollows:
issupervisedtowardsthegroundtruth,thejawisalsoplacedina
c oo un T rs h et er na s di en -tfe ood -u ep r no l dosi s tt s rio atn ie n, r imh ne gsn (c fL ue nsi kn
ci
tnf ie , or L nri rn digg
eid
fit ,h nLe edfija xw i, nLk Ssi eon cfe
t
.)m 4wa .3t ii l .c l Ts b. he eu ss ie gd ni in
f-
L bone(𝜕Ω𝑐 bones)=∑︁ 𝑖𝑁 =𝑏
1
𝑁1 𝑏||N𝑐(X𝑐 𝑖)−Xˆ 𝑖||2 2, (16)
icantbenefitofthisformulationisthatwemovethetraditional wherewesample𝑁 𝑏 pointson𝜕Ω𝑐 intotal,andhereXˆ 𝑖 isthe
bones
FEM-discretizedsimulationoutofthelearningloop,whichmakes pseudo-groundtruthbonepositiononthebonesurfacesgenerated6 • L.Yang,G.Zoss,P.Chandran,M.Gross,B.Solenthaler,E.Sifakis,D.Bradley
bytheparametricmodelgiventhegroundtruthidentityneutral
shape.
L =𝜆 L +𝜆 L +𝜆 L +𝜆 L
test skin skin rigid rigid fix fix soft soft
ElasticRegularizationL ereg.Finally,sinceweonlysuperviseon +𝜆 L +𝜆 L +𝜆 L . (19)
bone bone ereg ereg lreg lreg
thesurfaceareas,weincorporateanelasticregularizationtosmooth
thevolumetricmorphing: Here,L andL canvarydependingontheformoftheinput
skin bone
data.Forexample,whenfittingtoanunseen3Dscanwecanuse
L ereg(Ω𝑐 )=∑︁𝑁 𝑖𝑠 𝑁1
𝑠
R∈m SOin (3)||∇ X𝑐N𝑐(X𝑐 𝑖)−R||2 2, (17) t ah ne ds Ea qm ne .(s 1c 6a ))n a-t so d- um re ins gh tl ro as is na inn gd .Hpr oe wdi ec vt ee rd ,ob uo rn me olo ds es l( isi.e fl., exE iq bn le.( a1 n1 d)
allowsfittingtootherdata,suchas2Dfaciallandmarkscomputed
wherewesample𝑁 𝑠 pointsonΩ𝑐 intotal.Thislossalsomakesthe fromanimage,wherewecanformulateL skinasa2Dprojectionloss
trainingrobusttoanyincorrectestimationfromtheboneprediction andL asaself-supervisedboneloss,aswewilldescribelater
bone
inEqn.(16).Thesethreelossterms(L id,L bone,L ereg)willbeadded inSec.6.1.Finally,asmentionedearlierwecandirectlymanipulate
toourend-to-endtrainingapproach,describednext. thephysicalmodelfromtheartist-friendly3DMMparameters,or
swaptheidentitylatentcodeforanimationretargeting.Wecan
4.3 GeneralizedPhysicalFaceModel
alsointerpolatebetweenidentitycodesfornovelfacegeneration.
Wecannowdescribeourcompletepipeline,illustratedinFig.2.The Inallcases,givena𝛽and𝛾 codeattesttime,wecangeneratethe
twocontributionsdescribedinSec.4.1andSec.4.2makeitpossible materialspaceviaN𝑐 andevaluatethephysicalconstraintswith
tolearnageneralizedphysicalfacemodelfromalargedatasetof N𝑒 andperformphysicalsimulation.Themainsuperiorityofour
3Dfacialscans(skinonly).Tosummarize,givenanidentitylatent physicalfacemodeloverothertraditionaldeepfacemodelsisthat
code𝛽,ourgenerativeimplicitnetworkN𝑐 learnstodeformpoints duringsimulation,wecanaddadditionalphysicaleffectssuchas
fromacanonicalmaterialspaceΩ𝑐 toanidentity-specificmaterial collisionhandling,externalforces,etc.Alloftheseapplicationswill
spaceΩ0,𝛽
.Thengivenboththeidentitycodeandanexpression
bedemonstratedinSec.6.
latentcode𝛾,ourimplicitdeformationnetworkN𝑒 generatesthe
identity-andexpression-specificdeformationsΩ𝛾,𝛽
,fromwhich
5 IMPLEMENTATION
thephysicalconstraintscanbeobtainedforsimulation. Inordertotrainournetworkthereareanumberofimplementation
Inordertoregularizetheidentityandexpressionlatentspaces, detailstoconsider.
encouragedisentanglement,andallowforartist-drivenanimation
TrainingData.Weusethe3DfacedatasetpresentedbyChan-
aftertraining,weparameterizethelatentcodesusinganoff-the-
dranetal.[2020],whichconsistsof336identitiesundervarious
shelfmorphable3Dfacemodel[Lietal.2017].Specifically,𝛽 =
expressionstotaling13000facescansintopologicalcorrespondence,
P id(𝛽ˆ) and𝛾 = Pexp(𝛾ˆ),whereP
id
andPexp aresmallMLPswith
withrigidheadmotionremoved.Everyidentityhasone“neutral”
threelayerseachthatarelearnedwiththerestofthenetwork,and
expressionthatisusedinEqn.(15).Asthedatasetcontainsonlyskin
𝛽ˆand𝛾ˆaretheidentityandexpressionparametersofthe3DMM.To
geometry,wepre-fitthebonesusingaparametricskeletalmodel
obtaintheinputfortraining,wepre-computethe3DMMparameters
calledSCULPTOR[Qiuetal.2022],whichprovidestheconstraints
correspondingtoeachfaceinourdatasetusingleast-squaresfitting.
fortheboneshapelossinEqn.(16).Theskinandthebonesurfaces
CompleteLossFunction.Ourfullsetofoptimizationvariables discretize𝜕Ω∗ and𝜕Ω∗ ,respectively.AsmentionedinSec.4.3
skin bones
includethenetworkweightsofN𝑐,N𝑒,P idandPexp.Weregularize ofthemaintext,wefitFLAMEmodelparameters(𝛽ˆ,𝛾ˆ)toalldata
thelatentcodeswith𝑙-2regularizationL lreg = ||𝛽||2 2+||𝛾||2 2.For inapre-processingstep.
smoothness,wealsoapplyLipschitzregularizationL liptoP idand CanonicalSpace.ThecanonicalmaterialspaceΩ𝑐
isdefinedusing
Pexp,asinYangetal.[2023].Puttingitalltogether,thecomplete
themeanboneandskinsurfacesofSCULPTOR.Fortheboneswe
objectivefunctionfortrainingourfacemodelis:
usethetopologydirectlyfromSCULPTOR,butforthefacialskinwe
fitthetopologyfromour3DfacedatasettotheSCULPTORmean
L train=𝜆 skinL skin+𝜆 rigidL rigid+𝜆 fixL fix+𝜆 softL soft faceinordertoobtainvertexconsistencywithourdataset,forthe
+𝜆 idL id+𝜆 boneL bone+𝜆 eregL ereg (18) constraintsinEqn.(11)andEqn.(15).
+𝜆 lregL lreg+𝜆 lipL lip, TrainingDetails.Allthelossesdefinedon𝜕Ω∗ 𝜕Ω∗ ,𝜕Ω∗ ,
skin bones skull
and𝜕Ω∗ areevaluateddirectlyonthemeshvertices,whilelosses
where𝜆 ∗arebalancingweights.Ourmodelistrainedend-to-endin definedja iw
nΩ∗arecomputedthroughuniformsamplingofthesoft
asimulation-freemannerwiththedirectsupervisiononlyfromthe
tissuespace.Wemaskoutthenecessaryfaceandboneregionsfor
skinscanswhiletheanatomicalfeaturessuchastheboneshapes,
regression.Specifically,weonlyconsiderthefrontalfaceandassign
jawkinematics,andthefacialactuation,areinferredautomatically.
10timeslesssupervisionweighttothelow-confidenceregions(con-
Test-timeOptimizationandApplications.Oncetrained,one fidencepervertexisavailableinthe3Ddataset).Duringtestingand
applicationofourmodelistofitittounseenidentities.Toaccom- evaluation,weusethesamemasksforconsistency.Forsimulation,
plishthisweoptimizethelatentcodes𝛽and𝛾 usingthefollowing thediscretizationresolutionisapproximately2mmifnotexplicitly
objective: mentioned.WeusethesamesolverasYangetal.[2022].LearningaGeneralizedPhysicalFaceModelFromData • 7
Table1. Timingofdifferentcomponents.
Fittingtoa3DScan.Ithasbecomecommonpracticetoperformat
leastasmallamountof3Dfacescanningfortheprimaryactorsin
Training Testing
high-endproductions.Givenjustasinglescan,ourmodelcanbefit
Total PerIter Optim. PhyCons. Sim.
toprovidephysics-basedanimation.Whenfittingtoa3Dscan,L
skin
23.13h 0.50s 6.98s 0.05s 2.58s andL aredefinedthesamewayasduringtraining(Eqn.(11)
bone
andEqn.(16)).SeveralfittingresultsareshowninFig.3,including
theinputscan,thepredictedactuations(forthepredictedsimulation
NetworkandHyper-Parameters.ForbothN𝑒 andN𝑐,weuse
meshes),theestimatedboneshapesandmandibletransformation,
aconditionalSIRENnetworkinspiredbyYangetal.[2022]forits
andthefinalsimulatedfacialskinsurface.Wehighlighttheverylow
sounddifferentialproperties.Thedimensionsforourlatentcodes
errorbetweenthesimulatedresultandtheinputscan.Notethatthe
𝛽 and𝛾 are set to 128 while the corresponding 3DMM codes 𝛽ˆ inputscandoesnotneedtobeinarestconfigurationsincewefit
and𝛾ˆ usedforparameterizationaresetto100dimensionseach. boththeidentityandtheexpressionparameters.Inthisapplication,
Thebalancingweightsfortrainingaresetasfollows:𝜆 skin = 20, fitting takes less than 15 seconds on a desktop machine with a
𝜆 rigid=20,𝜆 fix=20,𝜆 soft=0.1,𝜆 id=1,𝜆 bone=0.1,𝜆 lreg=1𝑒−4,
commodityGPU(NvidiaGTX1080Ti),yieldinganincrediblyfast
𝜆
ereg
=0.1,and𝜆
lip
=2𝑒−6.Thenumberofsamplingpointsare
methodtoobtainanidentity-specificphysicalfacemodel.
𝑁
𝑣
=45𝑘,𝑁
𝑏
=5𝑘,𝑁
𝑓
=10𝑘and𝑁
𝑠
=10𝑘.Thevaluesfordensity,
Young’sModulus(𝐸)andPoissonratio(𝜈)aresetto0.9g/ml,5kPa FittingtoaFaceImage.Whena3Dscanisnotavailable,our
and0.47,respectively,accordingtorelatedliterature[XuandYang modelisflexibleandcanbefiteventoasinglefaceimage.When
2015]. fittingtoanimagetheskinlossisdefinedintermsof2Dfacial
landmarks,andourmethodpredictsnotonly𝛽and𝛾 butalsothe
TestSet.Fortesting,wepreparetwodatasets.Thefirstdatasetisa cameraprojectionmatrixC.L isthendefinedas:
skin
staticdatasetthatcontains28unseenidentitiestotaling529scans,
w i dd ah e tni ac t sh i eti tw e csil ol w nb i se t ih su tds inie vd gert oo s fee 5ex xa upm nri sen ese s ei nt oh n pe s e.m rT fo ohd ree msl’ e as c ng o ce n en de sd era a ql t ui az esa net t cio i esn sat oo d fyu 5nn a ss eme ee i nn c L skin(𝜕Ω s0 kin)=∑︁ 𝑖𝑁 =𝑙
1
𝑁1 𝑙||C·N𝑒(X𝑖)−xˆ𝑖||2 2, (20)
identities,witheachsequencelastingaround10s.Thisdynamic
testsetwillbeusedtoexaminethemodel’sgeneralizationtoun- whereX𝑖 representsthe𝑖-thlandmarkoutofthe 𝑁 𝑙 landmarks
sampledfrom𝜕Ω0 ,andxˆ𝑖 indicatesthecorrespondingground
seenexpressionblendweightvectors.Thisisvaluableforanimation skin
truth2Dlandmarkpositioninscreenspace.Toobtaintheground
retargetingpurposes.
truthlandmarksweemployarecenthigh-qualitydenselandmark
Timing.ThedetailedtimingresultsarepresentedinTable(1).Dur- detector [Chandran et al. 2023] and predict approximately 8000
ingthetrainingphase,theaveragedurationperiterationisapprox- landmarksdistributedontheface(pleaserefertothesupplemental
imately0.5seconds.Inthetestingphase,foreachframe,thelatent videoforanillustration).Inthisscenario,thereisnogroundtruth
spaceoptimization(Optim.)requiresabout7seconds,whereasthe neutralscantoobtainapredictedboneshapefortheboneshape
generationoftheinputphysicalconstraints(PhyCons.)takesap- lossL inEqn.(16).However,wecanreformulateL asa
bone bone
proximately0.05seconds,andthesimulationprocess(Sim.)takes self-supervisedbonelossusingtheestimatedskintopredictthe
around2.6seconds.Thesetimingexperimentswereconductedona bones:
systemequippedwithasingleRTXA6000GPUanda16-coreCPU.
W6 enR oE wSU deL mTS
onstrateresultsandapplicationsofourgeneralized
L bone(𝜕Ω𝑐 bones)=∑︁ 𝑖𝑁 =𝑏
1
𝑁1 𝑏||N𝑐(X𝑐 𝑖)−P(N𝑐(𝜕Ω𝑐 skin))(X𝑐 𝑖)||2 2, (21)
physicalfacemodel,startingwithfittingtounseendata(Sec.6.1),
showcasingthebenefitofhavingaphysicalmodelbyillustrating where P represents the SCULPTOR predictor model, evaluated
physicaleffects(Sec.6.2),facialanimationretargeting(Sec.6.3), ontheregressedneutralfacialskinN𝑐(𝜕Ω𝑐 ).Fittingresultsare
skin
andidentitygeneration/blendingthroughlatentspaceinterpolation showninFig.1andFig.4.Wesystematicallyevaluateavarietyof
(Sec.6.4).Pleaserefertothesupplementalvideoforamorevivid imageswithdifferingresolutionsandlightingconditions.While
visualization. not as accurate as fitting to a ground truth 3D scan, the image
fittingresultsshowcaseourabilitytopreservecriticalfacialfeatures
6.1 FittingtoUnseenData whileprovidingthemostflexibleandeasy-to-employversionofour
model.
Theprimarybenefitofourfacemodelisitsabilitytofittounseen
Forallfittingresults,eithertoscanortoimage,oncetheparame-
identitiesautomatically,alleviatingtheexpensiveburdenofcreating
tersarefitthenthemodelcanbeanimatedasillustratedinFig.1
anidentity-specificmaterialspacebyhandandtraininganidentity-
andthesupplementalvideo.
specificactuationnetwork.Fittingproceedsbyoptimizingforthe
identityandexpressionlatentcodes(𝛽,𝛾)usingEqn.(19).Here,the
6.2 PhysicalEffects
skinlossL andbonelossL areadaptedbasedonthetype
skin bone
ofdatabeingfit.Weillustratefittingtotwodifferentmodalities: Ourmodelallowsthesimulationofphysicaleffects,whichisoneof
fittingtoasingle3Dfacescanandfittingtoasinglefaceimage. thekeybenefitsofusingphysically-basedanimation.8 • L.Yang,G.Zoss,P.Chandran,M.Gross,B.Solenthaler,E.Sifakis,D.Bradley
0 7mm
Input Actuations Bones Skin Error Input Actuations Bones Skin
Fig.3. Modelfittingtoa3Dscan.Fiveexamplesareshownfordifferent Fig.4. Modelfittingtoasinglefaceimage.Fivedifferentidentitiesand
identitiesanddifferentexpressions.Afterfitting,thepredictedactuations expressionsareshown.Afterfitting,thepredictedactuationsandbones
andbonesallowtosimulatethefinalfacialskinmesh,whichmatchesthe allowtosimulatethefinalfacialskinmesh.Theresultisananimatable
inputwithaverylowerror. physicalmodelfromaverylightweightinput.
CollisionHandling.Fig.5illustratesourmodel’scapacitytoac-
Gravity.Ourmodelcansimulatetheeffectsofgravity.Fig.8shows
curatelydetectandresolvecollisions,includinglip-lipandbone-lip
thiseffectwherewerotatetheheadtodifferentorientationsand
penetrations,acommonchallengeinfacialanimation.
thesofttissueisnaturallypulledinthecorrespondingdirection.
Paralysis.Byadjustingmuscleactuationparameters,ourmodel
canreplicatesomedegreesoffacialparalysisasillustratedinFig.6. 6.3 Retargeting
Thisexampledemonstratesthemodel’ssensitivityandprecisionin
Ourmodelcanbeusedforphysics-basedanimationretargeting,
depictingsubtlephysiologicalchanges.
where we transfer facial animations between identities without
BoneReshaping.Ourphysicalmodelcansimulatevariouscran- interpenetration(seeFig.9).Thisisaccomplishedbychangingthe
iofacialeffectslikeosteotomy,illustratedinFig.7wherethejaw identitylatentcodetoatargetsubjectwhilekeepingtheexpression
bonehasbeenscaleddown,showingthecomparativeanalysisbe- codeofthesourcesubject.Thisapplicationconfirmsourmodel’s
tweenpre-andpost-treatmentstatesandhighlightingourmodel’s abilitytomaintainrealismandphysicalintegritywhileproducing
effectivenessinrepresentingandadaptingtoskeletaldeformations. identity-specificfacialexpressionsthatmatchasourceinput.LearningaGeneralizedPhysicalFaceModelFromData • 9
Animation Samples
Sources Target Identities
Fig.5. Collisionhandling.Ourphysicalmodelisabletoaccuratelydetect
andresolveinterpenetratinggeometries. Fig.9. Ourphysicalmodelisabletotransferfacialanimationsbetween
differentidentitieswhilebeinginterpenetration-free.
Facial Actuation Animation Samples
Fig.6. Paralysis.Ourphysicalmodelcansimulatesomedegreesoffacial
paralysis,demonstratingthemodel’ssensitivityandprecisionindepicting Fig.10. Ourphysicalmodelisabletosmoothlyinterpolatebetweendiffer-
subtlephysiologicalchanges. entidentitieswhilepreservingphysicalplausibility,suchascollision-free
properties.
Fig.10.Foreachnovelidentity,themodelcanbeevaluatedwith
differentexpressioncodestoobtainphysics-basedfacialanimation.
7 EVALUATION
We evaluate our model from multiple perspectives numerically,
includingreconstructionaccuracy,jawrigidity,skullfixationand
anatomicalfidelity.Forreconstructionaccuracy,weadoptfourtypes
Bones Animation Samples ofmetrics,including3Dvertex-to-vertexerror(V2V),3Dscan-to-
mesherror(S2M),F-Score,andnormalerror.Finally,weevaluate
Fig.7. Bonereshaping.Ourphysicalmodelcansimulatevariouscraniofa-
anatomicalfidelityintermsofthebonefidelityandthebone-skin
cialeffectslikeosteotomy,wherethejawhasbeenscaleddownandthe
penetration.Thespecificdetailsofeachmetricareelaboratedas
actuationsarereplayedontheeditedanatomy.
follows:
• Vertex-to-vertex Error (V2V) measures the average of
theEuclideandistancesbetweentheground-truthandthe
reconstructionvertices.
• Scan-to-meshError(S2M)measurestheaverageofthe
Euclideandistancesbetweentheground-truthverticesand
thereconstructedmeshsurface.
Fig.8. Gravity.Weshowtheeffectofgravityonastabilizedfacewith • F-Scoreevaluatesthereconstructionqualityfromthepoint
differentheadorientations(inset). cloudaspect.Itharmonizestherecallandtheprecisionby
computingtheirharmonicmean.AhighF-Scoreisindica-
tiveofareconstructionthatisbothaccurateandcomplete.
6.4 LatentSpaceInterpolation
Wesample32kpointsintotalanduse1mmastheerror
Anotherapplicationofourgeneralizedmodelisthatwecansample threshold.
newidentitiesfromthelatentspace.Wedemonstratethisbyinter- • NormalErrormeasurestheaverageofthecosinedistances
polatingbetweentwodifferentidentitiesfromourtrainingsetin betweentheground-truthandthereconstructionnormals.
noisilloC
o/w
noisilloC
/w
sisylaraP
o/w
sisylaraP
/w
gnipahseR
o/w
gnipahseR
/w
gnildnaH
gnildnaH10 • L.Yang,G.Zoss,P.Chandran,M.Gross,B.Solenthaler,E.Sifakis,D.Bradley
Table2. Numericalevaluationofdifferentsimulationresolutions.
Res.(mm) V2V↓ S2M↓ F-Score↑ Normal↓
6.8 1.4679 0.6615 0.7346 0.0178
4.5 1.1270 0.5466 0.8033 0.0157
3.0 0.9469 0.4892 0.8427 0.0146
2.0(Ours) 0.8975 0.4721 0.8546 0.0143 7mm
1.3 0.8848 0.4666 0.8583 0.0142
0
0 (Disp) 0.8642 0.4532 0.8672 0.0141
V2V Metric F-Score Metric
0 mm (disp) 1.3 mm 2.0 mm 3.0 mm 4.5 mm 6.8 mm
Fig.12. Qualitativecomparisonofdifferentsimulationresolutions.
Fig.11. CumulativecurvesofV2VandF-Scoremetricsfromdifferentsimu-
lationresolutions.
• JawRigidityquantifieshowrigidlythenetworkmovesthe
jaw,basedonV2Vmetric(seeEqn.(12)).
• SkullFixationquantifieshowwellthenetworkfixesthe
skull,basedonV2Vmetric(seeEqn.(13)).
• BoneFidelityquantifieshowwellthenetworkpreserves
theSCULPTORbonespace,basedonV2Vmetric(seeEqn.(16)).
• PenetrationPairscountsthepenetrationbetweenthebone
andtheskinmeshes.Weusetheedge-trianglepair.
ModelAccuracy.Wefirstevaluateourmodelaccuracyintermsof
fittingaccuracy.Specifically,wefitourmodeltothestatictestset 7mm
byoptimizingourlatentcodesscanbyscan,usingL test(Eqn.(19)). 0
Ours Ours w/o Ours w/o Ours Ours w/o Ours w/o
Wethenrunthediscretizationandthesimulationtogettheresults
forcalculatingthemetrics.Table(2)reportstheaccuracyofour
Fig.13. Qualitativecomparisonofdifferentmethods.
modelwithdifferentdiscretizationresolutions.Thediscretization
resolutionlargelyimpactsthesimulationaccuracy.Thehigherthe
resolutionis,themoreaccuratethesimulatedresultsare.Weinclude
animationretargeting.Wealsoreportthejawrigidityandskullfixa-
arowforevaluatingthepuredisplacementoutputofthenetwork,
tioninbothdatasets,provingthattheconstraintsarewellenforced
labelled“0(Disp)”,whichhasthelowesterrorasthisisthetarget
(seeTable(3)).
ofthetrainingobjective.Withadiscretizationresolutionofaround
2mmwecanachievecomparableaccuracywithourdisplacement Ablation. There are two main integral loss terms in our learn-
outputs,substantiatingthatoursimulation-freelearningframework ingframework,therigiditylossL (Eqn.(12))andthesoftloss
rigid
iseffectiveininferringplausiblephysicalconstraintsusedinsimula- L (Eqn.(14)).InTable(3)andFig.13,weshowtheablationstud-
soft
tion.Fig.11plotsthecumulativecurvesonV2VandF-Scoremetrics. iesofthereconstructionsdenotedas"Oursw/oL "and"Ours
rigid
Fig.12furtherdemonstratesthatoursimulationresultscanachieve w/o L " to assess the importance of each loss term. For both
soft
highreconstructionquality. thequantitativeandqualitativemeasurements,removingoneloss
Tofurtherevaluateourmodelintermsofanimationquality,we termresultsinasevereperformancedecrease,whichconfirmsboth
testourmodelonthedynamicset,wherewedirectlyusetheunseen losstermscontributetohigherreconstructionaccuracyandbetter
expressionblendweightvectorstoanimatetheseenidentitieswith- physicalplausibility.Specifically,L helpstoenforcetherigid
rigid
outanyoptimization.Table(3)showsthatthemodelgeneralizes movementofthebone,givingbetterjawrigidityandskullfixation
welltotheunseenexpressionblendweights,pavingthewayfor metrics.Thisleadstobetteraccuracysincethelearnedactuation
noitalumiS
rorrE
snoitautcA
noitalumiS
snoitautcA
senoB
rorrELearningaGeneralizedPhysicalFaceModelFromData • 11
Table3. Numericalevaluationofdifferentcomponentsonstaticanddynamictestsets.Weturnonthelatentspaceoptimizationforstaticsetwhileturnoffit
fordynamicset.
Methods V2V↓ S2M↓ F-Score↑ Normal↓ JawRig.↓ SkullFix.↓
Oursw/oL 0.8942 0.4702 0.8560 0.0143 0.1378 0.0724
bone
Oursw/oL 1.2891 0.6758 0.7552 0.0158 2.2540 0.1145
rigid
Oursw/oL 0.9282 0.5077 0.8567 0.0158 0.0460 0.0647
soft
Oursw/oL 0.9178 0.4804 0.8486 0.0144 0.1338 0.0764
lip
Ours 0.8975 0.4721 0.8546 0.0143 0.1349 0.0785
Oursw/oL 0.7237 0.3182 0.9396 0.0070 0.0883 0.0557
bone
Oursw/oL 1.0632 0.4918 0.8407 0.0077 2.1084 0.0866 rigid
Oursw/oL 0.7815 0.3723 0.9059 0.0086 0.0296 0.0400
soft
Oursw/oL 0.7751 0.3399 0.9270 0.0071 0.0915 0.0612
lip
Ours 0.7248 0.3235 0.9354 0.0070 0.0915 0.0638
0 5mm
Error Animation Samples Ours Ours w/o SCULPTOR
Fig.14. QualitativecomparisonwithFLAME.Ontheleft,weshowthe Fig.15. Qualitativecomparisonofdifferentmethodsintermsofanatomy.
fittingerrormaps.Ontheright,weshowtheanimationsampleswiththe
mouthcut-awayonthesideofeachframe.
Table(4)indicatethatourmethodattainsaccuracyonparwith
mechanismsaremorecompatiblewiththerigidjawkinematicsthat
FLAMEacrossbothdatasets.Asignificantadvantageofourmodel,
arestrictlyenforcedduringsimulation.L givesbirthtotwomer-
soft however,liesinitsabilitytosupportadditionalphysicaleffects,
its.First,ithelpsthemodellearnplausiblesofttissuedeformation
suchascollisionhandling—areaswheretraditionalmethodsoften
(seeActuationsinFig.13).Second,itbuildstheconnectionbetween
falter.Fig.14providesaqualitativecomparisontohighlightthis
theboneandtheskin,thereforebeingabletograduallydragthejaw
capability.
tothephysicallyplausibleplacesduringtraining.Therefore,"Ours
Inaddition,weconductacomparisonofourmodelwithSCULP-
w/oL "failstoinferthejawkinematicsandalwaysproduces
soft TOR,specificallyfocusingontheissueofbone-skinpenetration.
fixedjawposition(seeBonesinFig.13).Thisiswhyitsjawrigidity
OurexperimentsrevealthatfittingSCULPTORtoaneutralscanfre-
metricsandskullfixationarebetterthanourfullmodel(Table(3)).
quentlyresultsinbone-skinpenetrationproblems.However,thanks
WealsoexaminetheeffectivenessofourboneshapelossL
bytraininganothermodelwithoutit,denotedas"Oursw/oL
bone
".
toourelasticregularization(L ereg),ourmodeleffectivelymitigates
bone theseissues.Atthesametime,itmaintainscomparableboneshape
Table(5)andFig.15showthatL canbetterconstraintheoverall
bone (seeFig.15andTable(5)).
boneshape.Finally,ourLipschitzregularizationL leadstobetter
lip Finally,ourmethodisstillapplicableinYangetal’sscenariowhen
reconstructionaccuracywithoutspoilingothermetrics(see"Ours
thematerialspaceisprovided[Yangetal.2023].Toshowthis,we
w/oL "vs."Ours"inTable(3)andTable(5)).
lip integrateouruniquelossfunctions—L ,L ,andL —into
skin rigid fix
Comparison. As the first generalized physical face model, we Yangetal.’smodelarchitecture,whilestilladheringtotheirestab-
benchmarkourmethodagainstthecloselyrelatedFLAMEmodel, lishedregularizationandtrainingprotocols.Table(6)showsthe
focusingonfittingaccuracy.RecognizingthatFLAMEoptimizes evaluationontheirtestset.Ourmethodnotonlyachievescompa-
usingscan-to-meshdistance,weadoptthesamemetric,replacing rableaccuracy,butitalsoofferssignificantlyimprovedscalability.
thevertex-to-vertexenergyinourfittingschemewithscan-to-mesh Forinstance,whereasYangetal.requiredsixGPUsandnearly2
distance,toensureanequitablecomparison.Beyondourstatictest daystotrainmodelsforsixidentities,ourmethodologycanbeef-
set,wehavecompiledanadditionaltestsetofcomparablesizefrom ficientlyimplementedonasingleGPU,enablingtrainingforover
theFACESCAPEdataset[Yangetal.2020b].Resultspresentedin 300identitiesinjustoneday.
EMALF
sruO
citatS
cimanyD
1
esaC
2
esaC12 • L.Yang,G.Zoss,P.Chandran,M.Gross,B.Solenthaler,E.Sifakis,D.Bradley
Table4. Scan-to-meshdistanceofdifferentmodelsondifferentdatasets. REFERENCES
MichaelBao,MatthewCong,StéphaneGrabli,andRonaldFedkiw.2018.High-Quality
Datasets Ours* FLAME-100 FLAME-300
FaceCaptureUsingAnatomicalMuscles.ProceedingsoftheIEEE/CVFConference
OurDataset 0.3487 0.4858 0.4318 onComputerVisionandPatternRecognition(CVPR)(122018). http://arxiv.org/abs/
1812.02836
FaceScape 0.4713 0.6355 0.5596 VolkerBlanzandThomasVetter.1999. Amorphablemodelforthesynthesisof3D
faces.InSiggraph,Vol.99.187–194.
ZenghaoChai,HaoxianZhang,JingRen,DiKang,ZhengzhuoXu,XuefeiZhe,Chun
Yuan,andLinchaoBao.2022.REALY:RethinkingtheEvaluationof3DFaceRecon-
Table5. Anatomicalevaluationofdifferentmodelsonneutralscans.
struction.InProceedingsoftheEuropeanConferenceonComputerVision(ECCV).
PrashanthChandran,DerekBradley,MarkusGross,andThaboBeeler.2020.Semantic
Methods PenetrationPairs↓ BoneFidelity↓ DeepFaceModels.In2020InternationalConferenceon3DVision(3DV).IEEE,345–354.
https://doi.org/10.1109/3DV50981.2020.00044
Oursw/oL bone 0 1.9979 PrashanthChandran,LoïcCiccone,MarkusGross,andDerekBradley.2022. Local
Oursw/oL 0 1.3124 Anatomically-ConstrainedFacialPerformanceRetargeting.ACMTrans.Graph.41,
lip 4,Article168(jul2022).
Ours 0 1.3164 PrashanthChandranandGaspardZoss.2023.AnatomicallyConstrainedImplicitFace
Models. arXiv:2312.07538[cs.GR]
Sculptor 2628 0 PrashanthChandran,GaspardZoss,PauloGotardo,andDerekBradley.2023.Continu-
ousLandmarkDetectionWith3DQueries.InProceedingsoftheIEEE/CVFConference
onComputerVisionandPatternRecognition.16858–16867.
ByungkukChoi,HaekwangEom,BenjaminMouscadet,StephenCullingford,KurtMa,
Table6. QuantitativeComparisonwith[Yangetal.2023].
StefanieGassel,SuziKim,AndrewMoffat,MillicentMaier,MarcoRevelant,Joe
Letteri,andKaranSingh.2022.Animatomy:AnAnimator-Centric,Anatomically
Metric Yangetal Ours InspiredSystemfor3DFacialModeling,AnimationandTransfer.InSIGGRAPHAsia
2022ConferencePapers.AssociationforComputingMachinery,Article16,9pages.
V2V 0.3849 0.3813
HangDai,NickPears,WilliamSmith,andChristianDuncan.2019.StatisticalModeling
ofCraniofacialShapeandTexture.InternationalJournalofComputerVision(2019).
RadekDanecek,MichaelJ.Black,andTimoBolkart.2022.EMOCA:EmotionDriven
MonocularFaceCaptureandAnimation.InConferenceonComputerVisionand
8 CONCLUSION PatternRecognition(CVPR).20311–20322.
BernhardEgger,WilliamA.P.Smith,AyushTewari,StefanieWuhrer,MichaelZollhoe-
Wepresentanewmodelforphysics-basedfacialanimationthatis fer,ThaboBeeler,FlorianBernard,TimoBolkart,AdamKortylewski,SamiRomdhani,
ChristianTheobalt,VolkerBlanz,andThomasVetter.2020. 3DMorphableFace
trainedonrealdataofhundredsofidentitiesperformingvarious
Models—Past,Present,andFuture.ACMTransactionsonGraphics39,5(102020),
expressions,andassuchisextremelygeneralizableandcanbefit 1–38. https://doi.org/10.1145/3395208
tonewunseenidentitiesatruntime.Asaresult,weproposeavery YaoFeng,HaiwenFeng,MichaelJ.Black,andTimoBolkart.2021.LearninganAni-
matableDetailed3DFaceModelfromIn-the-WildImages.ACMTransactionson
convenientwaytogenerateactor-specificphysicalfaceanimation
Graphics(ToG),Proc.SIGGRAPH40,4(Aug.2021),88:1–88:13.
withoutanymanualmodelsetup.Thisispossibleduetoourtwo PanagiotisP.Filntisis,GeorgeRetsinas,FoivosParaperas-Papantoniou,AthanasiosKat-
maincontributions:anapproachforsimulation-freelearningwhere samanis,AnastasiosRoussos,andPetrosMaragos.2022. VisualSpeech-Aware
Perceptual3DFacialExpressionReconstructionfromVideos. arXivpreprint
neuralnetworksaretrainedtoproducedeformationsthatarecom- arXiv:2207.11094(2022).
patiblewithphysicalsimulationbutwithoutrequiringsimulation BarisGecer,StylianosPloumpis,IreneKotsia,andStefanosZafeiriou.2019.GANFIT:
GenerativeAdversarialNetworkFittingforHighFidelity3DFaceReconstruction.
inthetrainingloop,andamaterialspacemorphingmethodthatcan
InTheIEEEConferenceonComputerVisionandPatternRecognition(CVPR).
predictactor-specificskin,bonesandsoft-tissuevolumesautomati- BarisGecer,StylianosPloumpis,IreneKotsia,andStefanosPZafeiriou.2021. Fast-
cally.Thesecontributionsarethekeytobeingabletotrainonsuch GANFIT:GenerativeAdversarialNetworkforHighFidelity3DFaceReconstruction.
IEEETransactionsonPatternAnalysisandMachineIntelligence(2021).
alargedatasetefficiently,providingthegeneralizabilityneededfor
AurelGruber,MarcoFratarcangeli,GaspardZoss,RomanCattaneo,ThaboBeeler,
fittingtonewidentities. MarkusGross,andDerekBradley.2020. InteractiveSculptingofDigitalFaces
Intermsoflimitations,wenotethatwedonotattempttoaccu- UsinganAnatomicalModelingParadigm.ComputerGraphicsForum(2020),93–102.
https://doi.org/10.1111/cgf.14071
ratelymodeltheinsideofthemouth,inpartbecausethetraining Alexandru-EugenIchim,PetrKadleček,LadislavKavan,andMarkPauly.2017.Phace:
datasetdoesnotaccuratelytrackthisregion.Assuch,weignore Physics-basedFaceModelingandAnimation.ACMTransactionsonGraphics36,4
(72017),1–14. https://doi.org/10.1145/3072959.3073664
theteethregionontheanatomymodel.Also,whileweendeavor
GergelyKlár,AndrewMoffat,KenMuseth,andEftychiosSifakis.2020.ShapeTargeting:
tocreateconstraintsthatproducephysically-accurateanimations, AVersatileActiveElasticityConstitutiveModel.InACMSIGGRAPH2020Talks
thereisnoguaranteethatthelearnedactuationsarebiologically (VirtualEvent,USA)(SIGGRAPH’20).AssociationforComputingMachinery,New
York,NY,USA,Article59,2pages. https://doi.org/10.1145/3388767.3407379
accurate,especiallyforunseenidentitiesthatarefarfromtheones
TianyeLi,TimoBolkart,Michael.J.Black,HaoLi,andJavierRomero.2017.Learninga
seenattrainingtime. modeloffacialshapeandexpressionfrom4Dscans.ACMTransactionsonGraphics,
Nevertheless,wedemonstratethesuccessofourtrainedmodel (Proc.SIGGRAPHAsia)36,6(2017).
AraceliMorales,GemmaPiella,andFedericoM.Sukno.2020. Surveyon3Dface
withveryplausibleapplicationsoffittingtonewfacescans,fittingto reconstructionfromuncalibratedimages.Comput.Sci.Rev.40(2020),100400.
faceimages,animatingwithphysicaleffects,animationretargeting ChristopherOtto,PrashanthChandran,GaspardZoss,MarkusGross,PauloGotardo,
andDerekBradley.2023.APerceptualShapeLossforMonocular3DFaceRecon-
andidentityinterpolation.Weadditionallyprovideadetailedevalu-
struction. arXiv:2310.19580[cs.CV]
ationofourmethod.Wehopethatourworkwillhelptodemocratize ZesongQiu,YuweiLi,DongmingHe,QixuanZhang,LongwenZhang,YinghaoZhang,
physics-basedfacialanimationformanyapplications. JingyaWang,LanXu,XudongWang,YuyaoZhang,etal.2022.SCULPTOR:Skeleton-
consistentfacecreationusingalearnedparametricgenerator.ACMTransactionson
Graphics(TOG)41,6(2022),1–17.
ACKNOWLEDGMENTS
EftychiosSifakis,IgorNeverov,andRonaldFedkiw.2005.Automaticdeterminationof
facialmuscleactivationsfromsparsemotioncapturemarkerdata.ACMTransactions
TheworkissupportedbytheSwissNationalScienceFoundation onGraphics24,3(72005),417–425. https://doi.org/10.1145/1073204.1073208
underGrantNo.:200021_197136.LearningaGeneralizedPhysicalFaceModelFromData • 13
SangeethaGramaSrinivasan,QisiWang,JuniorRojas,GergelyKlár,LadislavKavan, VisionandPatternRecognition(CVPR).
andEftychiosSifakis.2021.Learningactivequasistaticphysics-basedmodelsfrom HaotianYang,HaoZhu,YanruWang,MingkaiHuang,QiuShen,RuigangYang,and
data.ACMTransactionsonGraphics40,4(82021),1–14. https://doi.org/10.1145/ XunCao.2020b.Facescape:alarge-scalehighquality3dfacedatasetanddetailed
3450626.3459883 riggable3dfaceprediction.InProceedingsoftheieee/cvfconferenceoncomputer
NicolasWagner,MarioBotsch,andUlrichSchwanecke.2023. SoftDECA:Compu- visionandpatternrecognition.601–610.
tationallyEfficientPhysics-BasedFacialAnimations.InProceedingsofthe16th LingchenYang,ByungsooKim,GaspardZoss,BaranGözcü,MarkusGross,andBarbara
ACMSIGGRAPHConferenceonMotion,InteractionandGames(MIG’23).Asso- Solenthaler.2022.Implicitneuralrepresentationforphysics-drivenactuatedsoft
ciation for Computing Machinery, New York, NY, USA, Article 11, 11 pages. bodies.ACMTransactionsonGraphics(TOG)41,4(2022),1–10.
https://doi.org/10.1145/3623264.3624439 LingchenYang,GaspardZoss,PrashanthChandran,PauloGotardo,MarkusGross,
ChengleiWu,DerekBradley,MarkusGross,andThaboBeeler.2016.Ananatomically- BarbaraSolenthaler,EftychiosSifakis,andDerekBradley.2023.AnImplicitPhysical
constrainedlocaldeformationmodelformonocularfacecapture.ACMTransactions FaceModelDrivenbyExpressionandStyle.InSIGGRAPHAsia2023Conference
onGraphics35,4(72016),1–12. https://doi.org/10.1145/2897824.2925882 Papers.Article106,12pages.
MingXuandJamesYang.2015.Humanfacialsofttissuethicknessandmechanicalprop- TiankeZhang,XuangengChu,YunfeiLiu,LijianLin,ZhendongYang,ZhengzhuoXu,
erties:aliteraturereview.InInternationalDesignEngineeringTechnicalConferences ChengkunCao,FeiYu,ChangyinZhou,ChunYuan,andYuLi.2023.Accurate3D
andComputersandInformationinEngineeringConference,Vol.57045.American FaceReconstructionwithFacialComponentTokens.InProceedingsoftheIEEE/CVF
SocietyofMechanicalEngineers,V01AT02A045. InternationalConferenceonComputerVision(ICCV).9033–9042.
HaotianYang,HaoZhu,YanruWang,MingkaiHuang,QiuShen,RuigangYang,and WojciechZielonka,TimoBolkart,andJustusThies.2022. TowardsMetricalRecon-
XunCao.2020a.FaceScape:aLarge-scaleHighQuality3DFaceDatasetandDetailed structionofHumanFaces.InEuropeanConferenceonComputerVision.
Riggable3DFacePrediction.InProceedingsoftheIEEEConferenceonComputer