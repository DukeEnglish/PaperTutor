Towards Tracing Trustworthiness Dynamics: Revisiting Pre-training
Period of Large Language Models
ChenQian1,2⋆,JieZhang1,3⋆,WeiYao1,2⋆,DongruiLiu1,4,
ZhenfeiYin1,5,YuQiao1,YongLiu2†,JingShao1†
1 ShanghaiArtificialIntelligenceLaboratory
2 RenminUniversityofChina 3 UniversityofChineseAcademyofSciences
4 ShanghaiJiaoTongUniversity 5 TheUniversityofSydney
{qianchen2022, wei.yao, liuyonggsai}@ruc.edu.cn zhangjie@iie.ac.cn shaojing@pjlab.org.cn
Abstract 2024). Delving into LLMs across all these trust-
worthinessdimensionsisessentialforthesociety.
Ensuringthetrustworthinessoflargelanguage
Toseekadeeperexplorationoflanguagemod-
models (LLMs) is crucial. Most studies con-
els,oneoftheprominentmethodsisprobing(Zhao
centrateonfullypre-trainedLLMstobetterun-
et al., 2023; Räuker et al., 2023), which involves
derstandandimproveLLMs’trustworthiness.
Inthispaper,torevealtheuntappedpotential trainingaclassifieronthemodel’srepresentations
ofpre-training,wepioneertheexplorationof to identify linguistic and semantic properties ac-
LLMs’trustworthinessduringthisperiod,fo- quiredbythemodel(Tenneyetal.,2019;Pimentel
cusingonfivekeydimensions: reliability,pri- etal.,2020;Lietal.,2021;Belinkov,2022;Räuker
vacy,toxicity,fairness,androbustness. Tobe- et al., 2023; Gurnee and Tegmark, 2023; Slobod-
ginwith,weapplylinearprobingtoLLMs.The
kin et al., 2023). In particular, considering trust-
highprobingaccuracysuggeststhatLLMsin
worthiness,recentattemptsrevealthatLLMrepre-
earlypre-trainingcanalreadydistinguishcon-
sentationscontainlinearlyseparablepatterns(Zou
ceptsineachtrustworthinessdimension.There-
fore,tofurtheruncoverthehiddenpossibilities et al., 2023; Li et al., 2023; Azaria and Mitchell,
ofpre-training,weextractsteeringvectorsfrom 2023). Unfortunately,existingresearchhaslargely
aLLM’spre-trainingcheckpointstoenhance focusedonfullypre-trainedLLMs(Touvronetal.,
the LLM’s trustworthiness. Finally, inspired 2023a), including those aligned (Ouyang et al.,
byChoietal.(2023)thatmutualinformation
2022) through Supervised Fine-Tuning (SFT) or
estimationisboundedbylinearprobingaccu-
Reinforcement Learning from Human Feedback
racy, wealsoprobeLLMswithmutualinfor-
(RLHF).Thisperspectiveneglectsthepre-training
mationtoinvestigatethedynamicsoftrustwor-
thinessduringpre-training. Wearethefirstto periodinthecontextofLLMtrustworthiness. To
observeasimilartwo-phasephenomenon: fit- ourbestknowledge,twoaspectsstillremainmys-
tingandcompression(Shwartz-ZivandTishby, teries: 1)howLLMsdynamicallyencodetrustwor-
2017). Thisresearchprovidesaninitialexplo- thinessduringpre-training,and2)howtoharness
rationoftrustworthinessmodelingduringLLM
thepre-trainingperiodformoretrustworthyLLMs.
pre-training, seeking to unveil new insights
To address the above issues, we start by ana-
and spur further developments in the field.
lyzing the pre-training dynamics about trustwor-
Wewillmakeourcodepubliclyaccessibleat
thiness of LLM. More specifically, we use lin-
https://github.com/ChnQ/TracingLLM.
ear probing (Alain and Bengio, 2016; Belinkov,
1 Introduction 2022)acrossthe360pre-trainingcheckpointsfrom
LLM360(Liuetal.,2023e)toexplorefivedimen-
As the capabilities of LLMs increase, their trust-
sions of trustworthiness: reliability, toxicity, pri-
worthiness becomes a focal point of widespread
vacy,fairness,androbustness. Ourprobingresults
attention. GuidedbyglobalAIgovernance(Com-
suggestthataftertheearlypre-trainingperiod,mid-
mission, 2021b; Tabassi, 2023; Newman, 2023)
dle layer representations of LLMs have already
andtrustworthyAI(Commissionetal.,2019;Liu
developedlinearlyseparablepatternsabouttrust-
etal.,2023b),trustworthyLLMshavedeveloped
worthiness. Suchpatternsarecapableofdiscern-
somecommoncategories,especiallyfocusingon
ingopposingconceptswithineachtrustworthiness
fivedimensions: reliability,toxicity,privacy,fair-
dimension(e.g.,discriminatingtrueandfalsestate-
ness,androbustness(Wangetal.,2023a;Sunetal.,
ments). Buildingupontheaboveobservations,we
⋆Equalcontribution †Correspondingauthor raiseanintriguingquestion: canthepre-training
4202
beF
92
]LC.sc[
1v56491.2042:viXraRRReeellliiiaaabbbiiillliiitttyyy TTToooxxxiiiccciiitttyyy FFFaaaiiirrrnnneeessssss PPPrrriiivvvaaacccyyy RRRooobbbuuussstttnnneeessssss
· · ·
Pre-training Checkpoints of LLM
Linear Probing Steering Vector Mutual Information
Learning Linear Two
Separability X I(X,Y) Y Phases
SFT Model
High Probing Accuracy Reliability Fitting Compression
0.8 5
0.7 Fairness Toxicity
3
0.6
Layer 10 20 30 1 Layer 10 20 30
0.5 Robustness Privacy
0 100 200 300 0 2000 4000 6000
Pre-training Checkpoints AmberChat Ours AmberSafe Pre-training Steps
Figure1: Overviewoftracingtrustworthinessdynamicsduringpre-training. 1)Linearprobingidentifieslinearly
separable opposing concepts during early pre-training; 2) Steering vectors are developed to enhance LLMs’
trustworthiness;3)ProbingLLMswithmutualinformationrevealsatwo-phasetrendregardingtrustworthiness.
periodofanLLMbeutilizedtoenhanceitstrust- edge,wearethefirsttonoticethatduringthepre-
worthinessafterpre-training? training period of LLMs, there exist two distinct
We provide insightful answers to the above phasesregardingtrustworthiness: fittingandcom-
questionbyexploringthepotentialofpre-training pression,whichisinlinewithpreviousresearchon
checkpoints for better trustworthiness. Notably, traditionalDNNs(Shwartz-ZivandTishby,2017;
recentadvancementshaveintroduced“activation Noshadetal.,2019).
intervention,” a novel suite of techniques for di-
2 ProbingLLMPre-trainingDynamicsin
rectinglanguagemodelstowardsenhancedLLMs’
Trustworthiness
performancebyadjustingactivationsduringinfer-
ence(Turneretal.,2023;Lietal.,2023;Rimsky Inthissection,weprobeLLMstoanalyzethedy-
et al., 2023; Wang and Shu, 2023). Inspired by namics of pre-training about trustworthiness. To
theseworksandtheobservationoflinearlysepara- beginwith,wedescribethedatasetsforeachtrust-
blepatternsintrustworthinessconceptsduringthe worthinessdimensioninSection2.1. Then,wein-
LLM’s pre-training period, we make preliminary troducetheexperimentalsetupinSection2.2. The
attempts to extract steering vectors from LLM’s probingresultsinSection2.3suggestthatmiddle
checkpointsduringpre-training,employingthem layerLLMrepresentationsfromearlypre-training
tointerveneintheSFTmodelfortrustworthiness havealreadyexhibitedlinearlyseparablepatterns.
enhancement. Extensive experiments reveal that
thesesteeringvectorsextractedfrompre-training 2.1 ResearchDimensionsandDatasetsof
checkpoints could promisingly enhance the SFT TruthworthyLLM
model’s trustworthiness. More crucially, these ExistingresearchonAIgovernance(Tabassi,2023;
steering vectors achieve a trustworthiness perfor- Commission et al., 2019; Commission, 2021b)
mance that matches or promisingly exceeds that andtrustworthyAI(Liuetal.,2023b;Foundation,
ofvectorsextracteddirectlyfromtheSFTmodel 2023)laysthegroundworkfordevelopingacom-
itself. Our findings introduce novel insights into prehensive understanding of trustworthy LLMs.
usingpre-trainingcheckpointsforLLMalignment, Guided by these principles, various studies clas-
revealing untapped potential and offering a fresh sifytrustworthyLLMsfromdifferentperspectives,
perspectiveonenhancingLLMtrustworthiness. yet some dimensions consistently emerge across
Finally,motivatedbythetheoreticalresult(Choi theseworks(Liuetal.,2023d;Wangetal.,2023a;
et al., 2023) that mutual information estimation Sun et al., 2024). Therefore, we delves into five
is bounded by linear probing accuracy, we take of these key dimensions: reliability, toxicity, pri-
analternativeviewbyprobingLLMswithmutual vacy,fairness,androbustness,employingcanonical
informationduringpre-training. Toourbestknowl- datasetsforeachtosupportourstudy.
CCA
gniborP
)X
,T(IFigure2: Thelinearprobeaccuracyonfivetrustworthinessdimensionsforthefirst80pre-trainingcheckpoints.
Foreachcheckpoint,wereporttheresultsfromlayers{0,6,12,18,24,30}. Theresultsfromalllayersofthe360
checkpointsareinAppendixD.
Reliability. TruthfulQA(Linetal.,2022),abench- Activationdataset. Giveneachoriginaldataset
mark dataset for evaluating LLMs’ truthfulness consistingofsentencesandthecorrespondingclass
discernment(Touvronetal.,2023b),includes817 labels,wefeedthesentenceintoLLMsandcollect
questionsacross38categoriesaimedatassessing thecorrespondingactivationsofthelasttoken(Li
theveracityofmodel-generatedanswers. etal.,2023;GurneeandTegmark,2023)foreach
Toxicity. ToxiGen (Hartvigsen et al., 2022) is a layer. TheactivationdatasetD = {(x i,y i)}N
i=1
is
broaddatasetfeaturingimplicittoxicandnon-toxic constructed with the activations x i ∈ Rd and the
statements across 13 minority demographics, en- correspondingbinarylabelsy i ∈ {0,1}.
ablingtoxicitymodelingassessmentinLLMs.
Linearprobing. Weemploythelinearprobing
Privacy. We choose the tier 2 tasks from Con- method (Alain and Bengio, 2016; Tenney et al.,
fAIde(Mireshghallahetal.,2023)toassessLLMs’ 2019; Pimentel et al., 2020; Li et al., 2021; Be-
privacy awareness, with ConfAIde targeting con- linkov, 2022) to analyze the activation datasets.
textual privacy and identifying vulnerabilities in For each trustworthiness dataset, every layer of
LLMs’privacyreasoning. eachpre-trainingcheckpointwithinLLM360pro-
Fairness. WeuseStereoSet(Nadeemetal.,2021) duces an activation dataset. Therefore, there are
to measure the stereotype modeling ability, i.e., 360×32activationdatasetsforall32layersacross
whetherLLMscapturestereotypicalbiasesabout 360 checkpoints. We randomly split each activa-
race,religion,profession,andgender. tiondatasetintotrainingandtestsetsby4:1,and
Robustness. We introduce typos by randomly fitabinarylinearclassifieronthetrainingset. We
changing the case of 5% letters in each sentence trainaclassifierforeachactivationdataset,which
from SST-2 (Socher et al., 2013) from GLUE yields360×32classifiers. Wereporttheaccuracy
benchmark(Wangetal.,2018). Theoriginalsen- onthetestset.
tenceaswellasthecorrespondingperturbedsen-
2.3 ProbingResults
tencearesynthesizedintoanewdataset.
Middle layer representations exhibit linearly
Foreachdatasetabove,weassignalabeltoev-
separablepatterns. Foreachcheckpointduring
erysentencebasedonwhetheritistrustworthy,i,e,
pre-training, Figure 2 shows that the accuracy is
truthful,toxic,privacy-aware,fair,andperturbed.
relativelyhigherformiddlelayers(the12-thand
Wemaintainabalanceddatasetforeachtrustwor-
18-thlayers). ThefullresultsinAppendixDalso
thinessdimension. Furtherdetailsareavailablein
supportsuchcharacteristicofmiddlelayers(about
AppendixB.
the18-thlayer). Itinspiresusthattherepresenta-
tionsfrommiddlelayersexhibitrichlinearencoded
2.2 ExperimentalSetup
informationtodistinguishthosedifferentconcepts.
The models under study. We investigate the Also,theobservationmeetswithotherliteratures
pre-trainingperiodofLLMsthroughthe360pre- consideringlinearprobingintheeraofLLMs(Li
training checkpoints provided by LLM360 (Liu et al., 2023; Zou et al., 2023; Burns et al., 2022),
etal.,2023e). Simultaneously,theyalsoreleasean which also empirically validate the capability of
instructionfine-tunedconversationalmodelnamed middlelayers. Moreover,similarphenomenonhas
AmberChat and an aligned conversational model alsobeenfoundinearlierlinearprobingliteratures
namedAmberSafe. Themodelsmentionedareall forBERT(HewittandManning,2019;VanAken
ofthe7Bparameterscale. etal.,2019),whichmayimplicitlysuggestssome(a) Pre-training Checkpoint 3.1 ActivationIntervention
PositiveInstructions Hidden States Initially,wepartitionthetrainingdatasetintotwo
NegativeInstructions ···ℎ𝑙−1 ℎ𝑙 ℎ𝑙+1··· distinctcollectionsbasedonthelabels,I+ andI−,
representingpositiveinstructionsandnegativein-
· · ·
load 𝑣𝑙=ℎ +𝑙 −ℎ −𝑙
structions, respectively. Following this partition,
Hidden States we collect the activations of LLM w.r.t. these in-
Pre-training Checkpoints ···ℎ𝑙−1 ℎ𝑙 ℎ𝑙+1 ··· structions,denotedbyAl(I+)andAl(I−),where
c c
Al denotes the function that extracts the activa-
InputTexts ℎ𝑙′=ℎ𝑙+𝛼𝑣𝑙 c
tionsfromthec-thcheckpointatl-thlayer. Subse-
(b) SFT Model
quently,wecomputethecentroidoftheactivations
Figure3: Aschematicillustrationof(a)constructing
fromeachsetsandtaketheirdifferencetoobtain
steeringvectorfromthepre-trainingcheckpointsand
the“massmeanvector,”(Lietal.,2023;Marksand
(b) intervening in the SFT model towards more trust-
Tegmark,2023)whichservesasoursteeringvector
worthinessbyemployingthesteeringvector.
vl = Al(I+)−Al(I−). (1)
similaritybetweenLLMsandrelativelysmallpre- c c c
trainedmodels.
Finally,weemploythesteeringvectortointervene
inthemodel’sactivationsasillustratedbelow
Thepotentialofpre-trainingcheckpoints. Fig-
ure 2 shows that for each layer over the whole hl′ = hl +αvl, (2)
c
pre-trainingperiod,theprobingaccuracyincreases
duringtheinitialphaseofpre-training,followedby where hl denotes representation at the l-th layer
fluctuationthroughouttheremainingpre-training ofthemodel,hl′ denotesthecorrespondingrepre-
period. The trend enlightens us that models dur- sentation after intervention; α is a rescale hyper-
ingtheearlystagesofpre-trainingcanalreadyen- parameter that indicates the strength of the inter-
codethesedifferentconceptswellinasimplelinear vention. Figure3illustratestheschematicdiagram
manner. Suchtrustworthinessconceptsarelinearly ofinterventionmethod. Notethattheintervention
representedinthelatentspaceofLLMs,whichsup- describedbyEq.(2)occursateachstepduringthe
portslinearrepresentationhypothesis(Parketal., autoregressiveinference.
2023)andotherempiricalstudy(Zouetal.,2023).
3.2 ExperimentalSetup
3 ControllingTrustworthinessviathe
EvaluationonTrustworthinessDatasets. For
SteeringVectorsfromPre-training
TruthfulQA, we fine-tune two GPT-3 models
Checkpoints as “GPT-judge” and “GPT-info” guided by (Lin
et al., 2022), to predict the truthfulness and
Inthissection,weaimtounravelthepotentialof
informativeness of the generated outputs from
checkpointsfromthepre-trainingperiodtoassist
LLMs, respectively. For ToxiGen, we fol-
in enhancing the trustworthiness performance of
low(Touvronetal.,2023b),employingfine-tuned
theSFTmodel(i.e.,AmberChat),basedonactiva-
RoBERTa(Hartvigsenetal.,2022)toevaluatethe
tioninterventiontechniques(Turneretal.,2023;Li
toxicityofcontentsgeneratedbyLLMs,andfinally
etal.,2023;Rimskyetal.,2023). Wefirstoutline
reportingtheproportionofgeneratedtextclassified
the method of activation intervention on the SFT
astoxic. ForConfAIde,StereoSet,andperturbed
model using the steering vectors extracted from
SST-2,withtheadaptationofconvertingpossible
pre-trainingcheckpointsinSection3.1. Next,we
multiple-choicequestionsintobinaryclassification
introducetheexperimentalsetupinSection3.2. We
tasks, we prompt LLMs to generate choices and
thenexplorehowsteeringvectorsextractedfrom
then evaluate the accuracy. Please refer to Ap-
pre-training checkpoints contribute to enhancing
pendixCformoredetails.
performance across distinct dimensions of trust-
worthiness in Section 3.3, presenting a series of DetailsofSteeringVectorsConstruction. For
findings and observations. Finally, we examine theactivationdataset,weconsideritfromtwoper-
theuseofthesametechniquestoboosttheoverall spectives: 1) For controlling the performance of
trustworthinessperformanceoftheSFTmodelin individualsubcategoriesundertrustworthinessin
Section3.4. Section3.3,weutilizethecorrespondingdatasetsTable1: ResultsofactivationinterventiononTruthfulQA,generalabilitybenchmarks,andtheothertrustworthiness
benchmarks. Thebestresultsarehighlightedinbold,andtherunner-upsareunderlined. v andv
ckpt_179 AmberChat
representAmberChatintervenedbysteeringvectorsderivedfromckpt_179andAmberChat,respectively.
TruthfulQAmetrics GeneralAbilities TrustworthinessAbilities
Method
Truth↑ Info↑ Truth*Info↑ ARC↑ MMLU↑ MathQA↑ RACE↑ ToxiGen↓ ConfAIde↑ StereoSet↑ SST-2↑
Baseline AmberChat 0.3931 0.9484 0.3728 0.6006 0.3659 0.2593 0.3904 0.0920 0.5055 0.5379 0.5757
Full 0.4229 0.9602 0.4060 0.4315 0.2355 0.2499 0.3187 0.0020 0.5294 0.5031 0.5757
Fine-tuned
Lora 0.3221 0.9329 0.3004 0.5758 0.3314 0.2620 0.3742 0.0080 0.6411 0.4980 0.5734
Activation vckpt_179 0.7322 0.9337 0.6837 0.5834 0.3358 0.2422 0.3876 0.0360 0.6181 0.5000 0.5229
InterventionvAmberChat 0.6978 0.9484 0.6618 0.5829 0.3388 0.2482 0.3943 0.0320 0.5192 0.4580 0.5367
0.08 Reasonable PPL
6.0
0.06
5.5
0.04
5.0
0.02
4.5
0.00
0.0 0.5 1.0 1.5 2.0 2.5 3.0 3.5 Figure5: PearsonCorrelationCoefficientforProbing
Alpha
ACCandtrustworthinessperformance.
Layer18 Toxic Ratio Layer18 PPL
Layer19 Toxic Ratio Layer19 PPL
Figure4: ThetrendsoftoxicratioandPPLastheinter-
ventionstrengthαincreases. Observation1. Steeringvectorsderivedfrompre-
training checkpoints could significantly enhance
theSFTmodel’sperformanceinTruthfulQA,Toxi-
describedinSection2.1,wherethesteeringvectors
Gen,andStereoSet. ForTruthfulQAandStereoSet,
are constructed from the development set and no
clear performance enhancement can be observed
dataleakageoccursduringtheevaluation; 2)For
in Table 1 and Table 2, respectively. Regarding
controllingtheoveralltrustworthinessperformance
ToxiGen,whenthestrengthofinterventionαisset
inSection3.4,weemployPKU-SafeRLHF-10K,
to0.5,thereisalreadyareductionofapproximately
a dataset proposed in (Ji et al., 2023) for RLHF
50% in the rate of toxic content generation, with
training. Forthecheckpoint,wesimplyselectthe
a negligible perturbation in perplexity. Besides,
checkpointthatishalfwaythroughthepre-training
sampling checkpoints from various stages of the
process for experiments, namely the checkpoint
pre-trainingperiod,weobservearelativelystrong
ckpt_179,whichhasalreadylearntlinearlysepara-
linearcorrelationbetweenthetrustworthinessper-
blepatterns(i.e.,performsahighprobingaccuracy
formanceandtheprobingaccuracyofpre-training
as shown in Figure 2). Regarding the selection
checkpointsinFigure5. Thissuggeststhat,once
of layer and α, we first narrow down the hyper-
themodelhasdevelopedlinearlyseparablepatterns
parameter range based on Perplexity (PPL), and
(representsahighprobingaccuracy)w.r.t. thetrust-
thenempiricallydeterminetheoptimalparameters
worthiness concepts during the pre-training pro-
usingacoarse-grainedgridsearch(Lietal.,2023;
cess,theconstructedsteeringvectormayhavethe
Turneretal.,2023;WangandShu,2023).
potentialtopositivelyinterveneintheSFTmodel’s
trustworthiness.
3.3 InterventiontoEnhanceDistinct
TrustworthinessDimensions Observation 2. Steering vectors derived from
pre-trainingcheckpointsandSFTmodelperform
In this subsection, we present several key obser-
broadlycomparableperformanceyetexhibitvari-
vations that illuminate the intricate dynamics of
ations across various tasks. Table 1 shows that,
steeringvectorsinmodulatingthetrustworthiness
comparedtothesteeringvectorextractedfromAm-
oftheSFTmodel.
berChat,thesteeringvectorfromthepre-training
checkpoint (ckpt_179) guides the SFT model to
https://huggingface.co/datasets/PKU-Alignment/PKU-
SafeRLHF-10K exhibitmore“truthfulness.” Moreover,itperforms
oitaR
cixoT
LPPTable2: ResultsofactivationinterventiononStereoSet,generalabilitybenchmarks,andtheothertrustworthiness
benchmarks. FormatandsignificancemarkerskeepconsistentwithTable1.
FairnessMetric GeneralAbilities TrustworthinessAbilities
Method
StereoSet↑ ARC↑ MMLU↑ MathQA↑ RACE↑ TruthfulQA↑ ToxiGen↓ ConfAIde↑ SST-2↑
Baselines AmberChat 0.5379 0.6006 0.3659 0.2593 0.3904 0.3728 0.0920 0.5055 0.5757
Activation vckpt_179 0.5799 0.5986 0.3524 0.2499 0.3914 0.2851 0.0600 0.5055 0.5390
InterventionvAmberChat 0.5830 0.5958 0.3508 0.2519 0.3952 0.3352 0.0820 0.5055 0.5528
TruthfulQA oftoxicitymaycontinuetodecline,theperplexity
61.99
RACE ofthemodelcorrespondinglyincrease,manifesting
ToxiGen
39.43 99.20 as a tendency to produce meaningless repetitive
38.95
90.80
37.28 contentorgibberish.
Observation 4. When the quantity and quality
MathQA StereoSet
26.00 24.36 50.00 56.35 of fine-tuning data are limited, activation inter-
vention by steering vectors may be a more effec-
tive approach for current task. Wefine-tune the
34.82 50.55 SFTmodelwithpositiveQApairsfromthetrain-
37.07 62.32
MMLU ConfAIde ing set using both full-parameter fine-tuning and
54.88 52.52 LoRAfine-tuningasacomparison,giventhatdata
60.06 57.57 inTruthfulQAnaturallyexistsintheformofQA
ARC SST-2
pairs. AsshowninTable1,themodelfine-tuned
𝐴𝑚𝑏𝑒𝑟𝑆𝑎𝑓𝑒 𝐴𝑚𝑏𝑒𝑟𝐶ℎ𝑎𝑡+𝑣(cid:3030)(cid:3038)(cid:3043)(cid:3047)_(cid:2869)(cid:2875)(cid:2877)
𝐴𝑚𝑏𝑒𝑟𝐶ℎ𝑎𝑡 𝐴𝑚𝑏𝑒𝑟𝐶ℎ𝑎𝑡+𝑣(cid:3002)(cid:3040)(cid:3029)(cid:3032)(cid:3045)(cid:3004)(cid:3035)(cid:3028)(cid:3047) with all parameters exhibits only minor improve-
Figure 6: Performance of various models across four mentsonTruthfulQAwhileexperiencingasignif-
generalcapabilitiesandfivetrustworthinesscapabilities.
icant decline in general capabilities. Meanwhile,
AmberChatandAmberSafearefine-tunedmodelsfrom
thefine-tunedmodelbyLoRAdemonstratesano-
LLM360. v andv representsteering
ckpt_179 AmberChat ticeabledecreaseinTruthfulQAthoughsomewhat
vectorsfromckpt_179andAmberChat,respectively.
preservingperformanceingeneralcapabilities.
Observation5. Trade-offsexistbetweendifferent
slightlybetteronARC,ConfAIde,andStereoSet, dimensions of trustworthiness. For instance, as
while the opposite is true for other tasks. It is seeninTable1,whilesteeringvectorintervention
important to note that we only selected a single enhancesthemodel’struthfulnessperformance,it
checkpointfromthepre-trainingprocessforexper- alsocompromisesperformanceonfairnessandro-
imentation,withoutundergoingfine-grainedhyper- bustness. Previousresearchhaswitnessedatrade-
parameter selection. Therefore, we believe these off between trustworthiness dimensions. For ex-
pre-trainingcheckpointsholdsignificantuntapped ample,privacy-fairnesstrade-off(Mangoldetal.,
potentialforaidingLLMtowardstrustworthiness. 2023),robustness-privacytrade-off(Hayes,2020),
Observation3. Interveninginthemodelslightly androbustness-fairnesstrade-off(Xuetal.,2021).
impairsitsgeneralcapabilitiesasamarginalcost Similarto(Liangetal.,2022),wealsosuggestthat
fortrustworthinessenhancement. Weevaluatethe the connection between different trustworthiness
model’sperformanceonfourcommonbenchmarks dimensionsreliesontheirdefinitions. Manypairs
for general capabilities, where a trend of slight oftrustworthinessinLLMsremainunstudied,and
performancedeclineisobservedafterintervention, weadvocateforfutureresearchinthisarea.
asindicatedin“GeneralAbilities”partofTables1
3.4 InterventiontoEnhanceUniversal
and 2. Additionally, we also observe the impact
Trustworthiness
of the intervention strength α on the generative
performance of the model. Taking ToxiGen as Inthissubsection,weaimtoleveragesteeringvec-
an example, Figure 4 illustrates the relationship torstocomprehensivelyenhancethemodel’strust-
betweentheproportionoftoxiccontentgenerated worthiness. UnlikeSection3.3wheresteeringvec-
by the model and perplexity as the intervention torsareconstructedusingdatasetsfromdifferent
strengthαincreases. Ifwecontinuouslyincrease dimensionsoftrustworthiness,hereweemploya
theinterventionstrength,althoughtheproportion general dataset for alignment (described in Sec-tion3.2),whichmayencompassdataacrossmulti-
pledimensionsoftrustworthiness.
Trustworthinessenhancementwithsteeringvec-
tors from universal alignment datasets. Fig-
ure 6 suggests that intervening in the SFT model
with steering vectors can influence its trustwor-
thiness,showingnotableimprovementsincertain Figure 7: The dynamics of I(T,X) and I(T,Y) for
dimensions (which may potentially linked to the TruthfulQA across various layers during pre-training.
characteristicsofthedatasetsemployed),withonly ThesimilartrendinotherdatasetsisinAppendixE.2.
marginallosses(inARC,MMLU)orevenmarginal
gains(inMathQA,RACE)ingeneralcapabilities.
4.1 ProbingStrategy
Moreover, steering vectors derived from check-
pointsduringthepre-trainingperioddemonstrate The mutual information between two continuous
superior effectiveness in enhancing trustworthi- randomvariables,X andY,isdefinedas
ness. ForAmberSafe,whichemploysasubstantial
(cid:90) (cid:90)
costforalignment,wenoteitsoverallbestperfor- p(x,y)
I(X,Y) = p(x,y)log dxdy.
mance(asseenintheblueline),particularlyhold- p(x)p(y)
Y X
ing a significant advantage in privacy and Truth-
fulQA.However,it’snoteworthythatmerelyusing It is a measure of the independence between two
10k alignment data to construct steering vectors variables. Giventhedatasetoftrustworthinessin
from a pre-training checkpoint for intervening in Section 2.1, we represent each dataset using the
the SFT model brings about impressive improve- first layer activation X, and Y denotes the corre-
ments across various dimensions of trustworthi- spondinglabelvector. Additionally,T represents
ness,whichrevealstheuntappedpotentialofpre- thefeaturematrixfromthetargetlayerofanLLM.
trainingcheckpointsinaidingthemodeltowards Thus, we probe LLMs with I(T,X) and I(T,Y)
bettertrustworthiness. duringpre-training.
Inprinciple,ourstrategydiffersfromShwartz-
4 ProbingLLMsusingMutual
Ziv and Tishby (2017) in three ways. Firstly, we
Information do not use the pre-training dataset of LLMs. In-
stead, we carefully design activation datasets to
Recently,Choietal.(2023)showsthatmutualin-
representspecifiedtrustworthinessproperties. Sec-
formationestimationisboundedbylinearprobing
ondly, we use the first layer representation to in-
accuracy. Also,themutualinformationcanbeused
dicate the original dataset because they contains
toinvestigatethedynamicsofneuralnetworksdur-
moreinformationthanrepresentationsfromother
ingtraining(Shwartz-ZivandTishby,2017;Saxe
layers (Cover,1999;TishbyandZaslavsky,2015;
et al., 2019; Goldfeld and Polyanskiy, 2020; Pi-
Shwartz-Ziv and Tishby, 2017). Finally, we fol-
menteletal.,2020;Geiger,2021;Lorenzenetal.,
low Ma et al. (2020) to use HSIC (Gretton et al.,
2021;Zhouetal.,2023). Therefore,motivatedby
2005) as an estimator of mutual information be-
theabove,weadoptadifferentperspectivebyprob-
cause it is challenging to accurately compute in
ing LLM checkpoints through the lens of mutual
highdimensions(Kraskovetal.,2004;Alemietal.,
information,particularlyfocusingontheaforemen-
2016;Pooleetal.,2019).
tionedtrustworthinessdimensions.
Weexplainourprobingstrategyandexperimen-
4.2 ExperimentalSetup
tal setup in Section 4.1 and Section 4.2, respec-
tively. Theempiricalobservationsareshownand Followingtheofficialcodeandreportedhyperpa-
analyzed in Section 4.3. In particular, we find rameters from Liu et al. (2023e), we initiate pre-
that there is a phase transition from “fitting” to trainingfrom arandomly initializedmodel using
“compression” during the pre-training period of thecorpusforthefirstcheckpoint,andsavemore
LLMs,whichisconsistentwithpreviousstudyon granular checkpoints to observe finer experimen-
traditionalDNNs(Shwartz-ZivandTishby,2017; talphenomena. Morediscussionsareavailablein
Noshadetal.,2019). AppendixC.4.3 TheDynamicsofPre-training LLMrepresentationscontainlinearlyseparablepat-
ternsabouttrustworthiness,suchastruthfulness(Li
The trend of mutual information. Figure 7
etal.,2023;MarksandTegmark,2023;Zouetal.,
shows that I(T,X) generally exhibits an initial
2023). However, they do not probe LLM repre-
increasefollowedbyadecreaseacrossallthecon-
sentations during pre-training. In this work, we
sidered layers during pre-training. And I(T,Y)
consider the whole pre-training period of LLMs
continuestoshowaconsistentupwardtrend. Note
andprobetheirpresentationsdynamically.
that middle layer representations exhibit a larger
I(T,Y) comparing to that from other layers. It
SteeringvectorsfortrustworthyLLMs. Toen-
suggeststhatmiddlelayerrepresentationsencode
surethesafetyandtrustworthinessofLLMs,some
moreinformationabouttheopposingconceptsof
promisingapproachesexplorethelatentspace,uti-
trustworthiness.
lizing representations to improve model perfor-
From“fitting”to“compression.” Overall,con- mance (Liu et al., 2023c; Jorgensen et al., 2023).
sidering I(T,X) and I(T,Y) collectively, it be- Variousstudiesinvestigateactivationengineering
comes evident that there are two phases during within LLMs from both theoretical and practical
pre-training. In the first and shorter phase, both perspectives,affectingmodelperformancebyma-
I(T,X) and I(T,Y) increase. While in the sec- nipulatingthemodel’srepresentationalspace(Park
ond and much longer phase, I(T,X) decreases etal.,2023;Turneretal.,2023;Zouetal.,2023).
and I(T,Y) continues to increase. Although our Furthermore,WangandShu(2023),Rimskyetal.
strategyiscompletelydifferentfromShwartz-Ziv (2023)andWangetal.(2024)constructdirectional
andTishby(2017),thetwo-phasephenomenonex- vectorstoexplorethemodel’ssafetyandalignment,
hibitssimilarities. Atthebeginningofpre-training, withthegoalofmakingmodelshelpful,honest,and
the randomly-initialized LLM fails to preserve harmless. However,therehasnoinvestigationinto
the relevant information, so I(T,X) ≈ 0 and howrepresentationschangeduringthepre-training
I(T,Y) ≈ 0. Next, as LLM gradually fits the phaseofLLMs. Inthispaper,weexploreandlever-
pre-training dataset, its abilities in language un- age representations during this phase, paving the
derstanding and concept modeling enhance, con- way for new research avenues in activation engi-
tributingtoincreasesinbothI(T,X)andI(T,Y). neering.
As pre-training progresses, LLM learns to better
Understanding the training process of DNNs.
compresstheirrelevantinformationinthedataset
ManyempiricalstudiesobservethatDNNstendto
andpreservemorelabel-relatedinformation(i.e.,
learnsimpleconceptsduringthelearningprocess
trustworthiness),leadingtoareductioninI(T,X)
(Arpitetal.,2017;Liuetal.,2021;Mangalamand
andanimprovementinI(T,Y). Overall,weareat
Prabhu,2019). Furthermore,Xuetal.(2019),Liu
the forefront of investigating the phase transition
etal.(2023a),andTianetal.(2023)theoretically
from “fitting” to “compression” in the context of
explain the learning preference of DNNs. Mean-
trustworthinessduringpre-training. Itisourhope
while,manyresearchersfocusonanalyzingtheutil-
thatourinsightswillmotivatefurtherexploration
ityoffine-tuningforlanguagemodels(Merchant
intothepre-trainingdynamicsofLLMs.
et al., 2020; Hao et al., 2020; Aghajanyan et al.,
5 RelatedWork 2021; Zhou and Srikumar, 2022; Mosbach et al.,
2020) and attempt to understand the in-context
Probing LLM representations. Probing clas-
learning(Renetal.,2024). However,fewprevious
sifiers (Alain and Bengio, 2016; Tenney et al.,
studiesinvestigatehowtrustworthinessislearned
2019; Pimentel et al., 2020; Li et al., 2021; Be-
by LLMs during pre-training. In this paper, we
linkov, 2022; Räuker et al., 2023) is one of the
takeacloserlookatthelearningdynamicoftrust-
prominent methods for identify certain proper-
worthinesswithinLLMs’representations.
ties acquired by the language model (Zhao et al.,
2023). Researchers probe LLMs and discover
6 Conclusion
linearseparablepatternswithinLLMs,including
spaceandtime(GurneeandTegmark,2023),game In this work, we take an initial and illuminating
states(Nandaetal.,2023),answerability(Slobod- steptowardselucidatingtheconceptualunderstand-
kinetal.,2023),andsomecounterfactualpairsof ingoftrustworthinessduringpre-training. Firstly,
concepts(Parketal.,2023). Itisalsoobservedthat bylinearprobingLLMsacrossreliability,privacy,toxicity,fairness,androbustness,weinvestigatethe have implemented strict protocols. Access to the
abilityofLLMsrepresentationstodiscernoppos- mostsensitiveaspectsofourexperimentsislimited
ingconceptswithineachtrustworthinessdimension toresearcherswiththeproperauthorization,who
duringthewholepre-trainingperiod. Furthermore, are committed to following rigorous ethical stan-
motivatedbytheprobingresults,weconductexten- dards. Theseprecautionsaretakentomaintainthe
siveexperimentstorevealthepotentialofutilizing integrityofourresearchandtomitigateanyrisks
representationsfromLLMsduringitspreviouspre- thatcouldarisefromtheexperiment’scontent.
trainingperiodtoenhanceLLMs’owntrustworthi-
ness. Finally,weusemutualinformationtoprobe
References
LLMs during pre-training and reveal some simi-
larity of the learning mechanism between LLMs Armen Aghajanyan, Sonal Gupta, and Luke Zettle-
andtraditionalDNNs. Takencollectively,theem- moyer.2021. Intrinsicdimensionalityexplainsthe
pirical study presented in this work can not only effectivenessoflanguagemodelfine-tuning. InPro-
ceedings of the 59th Annual Meeting of the Asso-
justifythepotentialtoimprovetrustworthinessof
ciationforComputationalLinguisticsandthe11th
LLMs using their own pre-training checkpoints,
InternationalJointConferenceonNaturalLanguage
butmayalsoleadtoabetterunderstandingofthe Processing (Volume 1: Long Papers), pages 7319–
dynamicsofLLMrepresentations,especiallythe 7328.
trustworthiness-relatedconcepts.
Guillaume Alain and Yoshua Bengio. 2016. Under-
standing intermediate layers using linear classifier
7 Limitations
probes. arXivpreprintarXiv:1610.01644.
Thereareseverallimitationsofthiswork. Firstly, AlexanderAAlemi,IanFischer,JoshuaVDillon,and
weonlyfocusonfiveessentialtrustworthinessdi- KevinMurphy.2016. Deepvariationalinformation
bottleneck. arXivpreprintarXiv:1612.00410.
mensions and do not encompass all the dimen-
sions,suchasthoseappearedinCommissionetal.
DevanshArpit, StanisławJastrze˛bski, NicolasBallas,
(2019); Liu et al. (2023b). And a wide variety DavidKrueger,EmmanuelBengio,MaxinderSKan-
ofdefinitionsforeachtrustworthinessdimension, wal,TeganMaharaj,AsjaFischer,AaronCourville,
YoshuaBengio,etal.2017. Acloserlookatmem-
as discussed by (Wang et al., 2023a; Sun et al.,
orizationindeepnetworks. InInternationalconfer-
2024), are not completely covered in our analy-
enceonmachinelearning,pages233–242.PMLR.
sis. Secondly,duetolimitationsincomputational
resources as well as the lack of open-source pre- Amos Azaria and Tom Mitchell. 2023. The internal
stateofanllmknowswhenitslying. arXivpreprint
trainingLLMcheckpoints,weonlyconductexper-
arXiv:2304.13734.
imentsonLLM360(Liuetal.,2023e). Finally,for
evaluation of TruthfulQA, the precision of evalu- YonatanBelinkov.2022. Probingclassifiers: Promises,
ation results depends on the performance of the shortcomings,andadvances. ComputationalLinguis-
tics,48(1):207–219.
“GPT-judge” evaluator. And for multiple-choice
evaluation,theevaluationresultsmayrelyonthe
CollinBurns,HaotianYe,DanKlein,andJacobStein-
instructionfollowingabilityofLLMs. hardt. 2022. Discovering latent knowledge in lan-
guage models without supervision. arXiv preprint
8 BroaderImpactandEthicsStatement arXiv:2212.03827.
Thisstudyconcentratesonbetterunderstandingthe KwangheeChoi,Jee-weonJung,andShinjiWatanabe.
2023. Understandingprobebehaviorsthroughvaria-
learningdynamicsofLLMtrustworthinessduring
tionalboundsofmutualinformation. arXivpreprint
pre-training. Themotivationofoursteeringvector
arXiv:2312.10019.
experimentsiscenteredonimprovingthetrustwor-
thinessofLLMs. Werecognizethesensitivenature EuropeanCommission.2021b. Proposalforaregulation
oftheeuropeanparliamentandofthecouncillaying
ofourresearchandassurethatitstrictlycomplies
downharmonisedrulesonartificialintelligence(ar-
withlegalandethicalguidelines.
tificialintelligenceact)andamendingcertainunion
This research is carried out in a secure, con- legislativeacts,pub.l.no.com(2021)206final.
trolled environment, ensuring the safety of real-
EuropeanCommission,ContentDirectorate-Generalfor
worldsystems. Giventhenatureofourwork,which
CommunicationsNetworks,andTechnology.2019.
includesdealingwithpotentiallysensitivecontent
Ethics guidelines for trustworthy AI. Publications
likeunreliablestatementsandtoxicsentences,we Office.ThomasMCover.1999. Elementsofinformationtheory. measure representational harms in pre-trained lan-
JohnWiley&Sons. guage models. In Proceedings of the 3rd Work-
shoponTrustworthyNaturalLanguageProcessing
AIVerifyFoundation.2023. Catalogueofllmevalua- (TrustNLP2023),pages121–134.
tions.
Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi
LeoGao,JonathanTow,BaberAbbasi,StellaBiderman, Zhang,CeBian,BoyuanChen,RuiyangSun,Yizhou
SidBlack,AnthonyDiPofi,CharlesFoster,Laurence Wang, and Yaodong Yang. 2023. Beavertails: To-
Golding,JeffreyHsu,AlainLeNoac’h,HaonanLi, wards improved safety alignment of LLM via a
KyleMcDonell,NiklasMuennighoff,ChrisOciepa, human-preference dataset. In Thirty-seventh Con-
Jason Phang, Laria Reynolds, Hailey Schoelkopf, ferenceonNeuralInformationProcessingSystems
Aviya Skowron, Lintang Sutawika, Eric Tang, An- DatasetsandBenchmarksTrack.
ishThite, BenWang, KevinWang, andAndyZou.
2023. A framework for few-shot language model OleJorgensen,DylanCope,NandiSchoots,andMurray
evaluation. Shanahan. 2023. Improving activation steering in
languagemodelswithmean-centring. arXivpreprint
BernhardCGeiger.2021. Oninformationplaneanal- arXiv:2312.03813.
yses of neural network classifiers–a review. IEEE
TransactionsonNeuralNetworksandLearningSys- AlexanderKraskov,HaraldStögbauer,andPeterGrass-
tems. berger.2004. Estimatingmutualinformation. Physi-
calreviewE,69(6):066138.
Ziv Goldfeld and Yury Polyanskiy. 2020. The infor-
mation bottleneck problem and its applications in BelindaZLi,MaxwellNye,andJacobAndreas.2021.
machinelearning. IEEEJournalonSelectedAreas Implicit representations of meaning in neural lan-
inInformationTheory,1(1):19–38. guagemodels. arXivpreprintarXiv:2106.00737.
Arthur Gretton, Olivier Bousquet, Alex Smola, and Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter
Bernhard Schölkopf. 2005. Measuring statistical Pfister, and Martin Wattenberg. 2023. Inference-
dependence with hilbert-schmidt norms. In Inter- time intervention: Eliciting truthful answers from
nationalconferenceonalgorithmiclearningtheory, alanguagemodel. InThirty-seventhConferenceon
pages63–77. NeuralInformationProcessingSystems.
Wes Gurnee and Max Tegmark. 2023. Language Percy Liang, Rishi Bommasani, Tony Lee, Dimitris
models represent space and time. arXiv preprint Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian
arXiv:2310.02207. Zhang,DeepakNarayanan,YuhuaiWu,AnanyaKu-
mar, et al. 2022. Holistic evaluation of language
Yaru Hao, Li Dong, Furu Wei, and Ke Xu. 2020. In- models. arXivpreprintarXiv:2211.09110.
vestigatinglearningdynamicsofBERTfine-tuning.
In Proceedings of the 1st Conference of the Asia- StephanieLin,JacobHilton,andOwainEvans.2022.
PacificChapteroftheAssociationforComputational Truthfulqa: Measuring how models mimic human
Linguisticsandthe10thInternationalJointConfer- falsehoods. InProceedingsofthe60thAnnualMeet-
enceonNaturalLanguageProcessing,pages87–92. ingoftheAssociationforComputationalLinguistics
AssociationforComputationalLinguistics. (Volume1: LongPapers).
Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Alisa Liu, Xiaochuang Han, Yizhong Wang, Yulia
MaartenSap,DipankarRay,andEceKamar.2022. Tsvetkov,YejinChoi,andNoahA.Smith.2024. Tun-
Toxigen: A large-scale machine-generated dataset inglanguagemodelsbyproxy.
for implicit and adversarial hate speech detection.
In Proceedings of the 60th Annual Meeting of the Chen Liu, Zhichao Huang, Mathieu Salzmann, Tong
AssociationforComputationalLinguistics. Zhang,andSabineSüsstrunk.2021. Ontheimpact
ofhardadversarialinstancesonoverfittinginadver-
Jamie Hayes. 2020. Trade-offs between member- sarialtraining. arXivpreprintarXiv:2112.07324.
shipprivacy&adversariallyrobustlearning. arXiv
preprintarXiv:2006.04622. DongruiLiu,HuiqiDeng,XuCheng,QihanRen,Kan-
gruiWang,andQuanshiZhang.2023a. Towardsthe
JohnHewittandChristopherDManning.2019. Astruc- difficultyforadeepneuralnetworktolearnconcepts
turalprobeforfindingsyntaxinwordrepresentations. ofdifferentcomplexities. InThirty-seventhConfer-
InProceedingsofthe2019ConferenceoftheNorth enceonNeuralInformationProcessingSystems.
AmericanChapteroftheAssociationforComputa-
tionalLinguistics: HumanLanguageTechnologies, Haochen Liu, Yiqi Wang, Wenqi Fan, Xiaorui Liu,
Volume1(LongandShortPapers),pages4129–4138. Yaxin Li, Shaili Jain, Yunhao Liu, Anil Jain, and
Jiliang Tang. 2023b. Trustworthy ai: A computa-
SagharHosseini,HamidPalangi,andAhmedHassan tionalperspective. ACMTransactionsonIntelligent
Awadallah.2023. Anempiricalstudyofmetricsto SystemsandTechnology,page1–59.ShengLiu,LeiXing,andJamesZou.2023c. In-context Marius Mosbach, Anna Khokhlova, Michael A Hed-
vectors: Makingincontextlearningmoreeffective derich,andDietrichKlakow.2020. Ontheinterplay
andcontrollablethroughlatentspacesteering. arXiv betweenfine-tuningandsentence-levelprobingfor
preprintarXiv:2311.06668. linguisticknowledgeinpre-trainedtransformers. In
FindingsoftheAssociationforComputationalLin-
YangLiu,YuanshunYao,Jean-FrancoisTon,Xiaoying guistics: EMNLP2020,pages2502–2516.
Zhang,RuochengGuo,HaoCheng,YegorKlochkov,
Moin Nadeem, Anna Bethke, and Siva Reddy. 2021.
MuhammadFaaizTaufiq,andHangLi.2023d. Trust-
StereoSet: Measuringstereotypicalbiasinpretrained
worthyllms: asurveyandguidelineforevaluating
languagemodels. InProceedingsofthe59thAnnual
largelanguagemodels’alignment.
Meeting of the Association for Computational Lin-
guisticsandthe11thInternationalJointConference
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,Man-
onNaturalLanguageProcessing(Volume1: Long
dar Joshi, Danqi Chen, Omer Levy, Mike Lewis,
Papers),pages5356–5371.
Luke Zettlemoyer, and Veselin Stoyanov. 2020.
Ro{bert}a: Arobustlyoptimized{bert}pretraining
NeelNanda,AndrewLee,andMartinWattenberg.2023.
approach.
Emergentlinearrepresentationsinworldmodelsof
self-supervised sequence models. arXiv preprint
Zhengzhong Liu, Aurick Qiao, Willie Neiswanger,
arXiv:2309.00941.
HongyiWang,BowenTan,TianhuaTao,JunboLi,
YuqiWang,SuqiSun,OmkarPangarkar,etal.2023e. Jessica Newman. 2023. A taxonomy of trustworthi-
Llm360: Towardsfullytransparentopen-sourcellms. nessforartificialintelligence: Connectingproperties
arXivpreprintarXiv:2312.06550. oftrustworthinesswithriskmanagementandtheai
lifecycle.
Stephan Sloth Lorenzen, Christian Igel, and Mads
Nielsen.2021. Informationbottleneck: Exactanal- MortezaNoshad, YuZeng, andAlfredOHero.2019.
ysisof(quantized)neuralnetworks. arXivpreprint Scalablemutualinformationestimationusingdepen-
arXiv:2106.12912. dencegraphs. InICASSP2019-2019IEEEInterna-
tionalConferenceonAcoustics,SpeechandSignal
Wan-DuoKurtMa,JPLewis,andWBastiaanKleijn. Processing(ICASSP),pages2962–2966.IEEE.
2020. The hsicbottleneck: Deep learning without
LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
back-propagation. InProceedingsoftheAAAIcon-
CarrollWainwright,PamelaMishkin,ChongZhang,
ferenceonartificialintelligence,pages5085–5092.
SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
2022. Training languagemodelsto followinstruc-
Karttikeya Mangalam and Vinay Uday Prabhu. 2019.
tions with human feedback. Advances in Neural
Dodeepneuralnetworkslearnshallowlearnableex-
InformationProcessingSystems,35:27730–27744.
amplesfirst?
DenisPaperno,GermánKruszewski,AngelikiLazari-
Paul Mangold, Michaël Perrot, Aurélien Bellet, and dou,NgocQuanPham,RaffaellaBernardi,Sandro
Marc Tommasi. 2023. Differential privacy has Pezzelle,MarcoBaroni,GemmaBoleda,andRaquel
boundedimpactonfairnessinclassification. InIn- Fernández. 2016. The LAMBADA dataset: Word
ternationalConferenceonMachineLearning,pages prediction requiring a broad discourse context. In
23681–23705. Proceedings of the54th Annual Meeting of the As-
sociationforComputationalLinguistics(Volume1:
SamuelMarksandMaxTegmark.2023. Thegeometry LongPapers),pages1525–1534.
oftruth: Emergentlinearstructureinlargelanguage
modelrepresentationsoftrue/falsedatasets. arXiv Kiho Park, Yo Joong Choe, and Victor Veitch. 2023.
preprintarXiv:2310.06824. The linear representation hypothesis and the ge-
ometry of large language models. arXiv preprint
AmilMerchant,ElaheRahimtoroghi,ElliePavlick,and arXiv:2311.03658.
IanTenney.2020. Whathappenstobertembeddings
TiagoPimentel,JosefValvoda,RowanHallMaudslay,
during fine-tuning? In Proceedings of the Third
RanZmigrod,AdinaWilliams,andRyanCotterell.
BlackboxNLPWorkshoponAnalyzingandInterpret-
2020. Information-theoretic probing for linguistic
ingNeuralNetworksforNLP,pages33–44.
structure. InProceedingsofthe58thAnnualMeet-
ingoftheAssociationforComputationalLinguistics,
NiloofarMireshghallah,HyunwooKim,XuhuiZhou,
pages4609–4622.
YuliaTsvetkov,MaartenSap,RezaShokri,andYejin
Choi. 2023. Can llms keep a secret? testing pri-
BenPoole,SherjilOzair,AaronVanDenOord,Alex
vacyimplicationsoflanguagemodelsviacontextual
Alemi, and George Tucker. 2019. On variational
integritytheory.
boundsofmutualinformation. InInternationalCon-
ferenceonMachineLearning,pages5171–5180.
EricMitchell,RafaelRafailov,ArchitSharma,Chelsea
Finn,andChristopherD.Manning.2023. Anemula- Alec Radford, Jeff Wu, Rewon Child, David Luan,
torforfine-tuninglargelanguagemodelsusingsmall DarioAmodei,andIlyaSutskever.2019. Language
languagemodels. modelsareunsupervisedmultitasklearners.TilmanRäuker,AnsonHo,StephenCasper,andDylan Yuandong Tian, Yiping Wang, Zhenyu Zhang, Beidi
Hadfield-Menell. 2023. Toward transparent ai: A Chen, and Simon Du. 2023. Joma: Demystifying
survey on interpreting the inner structures of deep multilayer transformers via joint dynamics of mlp
neuralnetworks. In2023IEEEConferenceonSecure andattention. arXivpreprintarXiv:2310.00535.
andTrustworthyMachineLearning(SaTML),pages
464–483.IEEE. NaftaliTishbyandNogaZaslavsky.2015. Deeplearn-
ingandtheinformationbottleneckprinciple. In2015
AbhilashaRavichander,YonatanBelinkov,andEduard ieeeinformationtheoryworkshop(itw),pages1–5.
Hovy. 2021. Probing the probing paradigm: Does IEEE.
probingaccuracyentailtaskrelevance? InProceed-
ingsofthe16thConferenceoftheEuropeanChap- HugoTouvron,ThibautLavril,GautierIzacard,Xavier
teroftheAssociationforComputationalLinguistics: Martinet,Marie-AnneLachaux,TimothéeLacroix,
MainVolume,pages3363–3377. BaptisteRozière,NamanGoyal,EricHambro,Faisal
Azhar, et al. 2023a. Llama: Open and effi-
JieRen,QipengGuo,HangYan,DongruiLiu,Xipeng cient foundation language models. arXiv preprint
Qiu, and Dahua Lin. 2024. Identifying semantic arXiv:2302.13971.
induction heads to understand in-context learning.
arXivpreprintarXiv:2402.13055. Hugo Touvron, Louis Martin, Kevin Stone, Peter Al-
bert, Amjad Almahairi, Yasmine Babaei, Nikolay
NinaRimsky,NickGabrieli,JulianSchulz,MegTong, Bashlykov,SoumyaBatra,PrajjwalBhargava,Shruti
Evan Hubinger, and Alexander Matt Turner. 2023. Bhosale, et al. 2023b. Llama 2: Open founda-
Steeringllama2viacontrastiveactivationaddition. tion and fine-tuned chat models. arXiv preprint
arXivpreprintarXiv:2312.06681. arXiv:2307.09288.
AndrewMSaxe,YaminiBansal,JoelDapello,Madhu AlexTurner,LisaThiergart,DavidUdell,GavinLeech,
Advani,ArtemyKolchinsky,BrendanDTracey,and Ulisse Mini, and Monte MacDiarmid. 2023. Acti-
DavidDCox.2019. Ontheinformationbottleneck vationaddition: Steeringlanguagemodelswithout
theory of deeplearning. Journalof Statistical Me- optimization. arXivpreprintarXiv:2308.10248.
chanics: TheoryandExperiment,2019(12):124020.
Betty Van Aken, Benjamin Winter, Alexander Löser,
RavidShwartz-ZivandNaftaliTishby.2017. Opening andFelixAGers.2019. Howdoesbertanswerques-
theblackboxofdeepneuralnetworksviainforma- tions? a layer-wise analysis of transformer repre-
tion. arXivpreprintarXiv:1703.00810. sentations. In Proceedings of the 28th ACM inter-
nationalconferenceoninformationandknowledge
Aviv Slobodkin, Omer Goldman, Avi Caciularu, Ido management,pages1823–1832.
Dagan,andShauliRavfogel.2023. Thecuriouscase
of hallucinatory unanswerablity: Finding truths in Alex Wang, Amanpreet Singh, Julian Michael, Felix
the hidden states of over-confident large language Hill, Omer Levy, and Samuel R Bowman. 2018.
models. arXivpreprintarXiv:2310.11877. Glue: Amulti-taskbenchmarkandanalysisplatform
fornaturallanguageunderstanding. arXivpreprint
Richard Socher, Alex Perelygin, Jean Wu, Jason arXiv:1804.07461.
Chuang,ChristopherDManning,AndrewYNg,and
ChristopherPotts.2013. Recursivedeepmodelsfor BoxinWang, WeixinChen, HengzhiPei, ChulinXie,
semanticcompositionalityoverasentimenttreebank. Mintong Kang, Chenhui Zhang, Chejian Xu, Zidi
In Proceedings of the 2013 conference on empiri- Xiong, Ritik Dutta, Rylan Schaeffer, et al. 2023a.
calmethodsinnaturallanguageprocessing,pages Decodingtrust: Acomprehensiveassessmentoftrust-
1631–1642. worthiness in gpt models. In Thirty-seventh Con-
ferenceonNeuralInformationProcessingSystems
DanielJSolove.2005. Ataxonomyofprivacy. U.Pa.l. DatasetsandBenchmarksTrack.
Rev.,154:477.
BoxinWang,ChejianXu,ShuohangWang,ZheGan,
Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, YuCheng,JianfengGao,AhmedHassanAwadallah,
Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan and Bo Li. 2021. Adversarial glue: A multi-task
Lyu,YixuanZhang,XinerLi,etal.2024. Trustllm: benchmark for robustness evaluation of language
Trustworthiness in large language models. arXiv models. In Advances in Neural Information Pro-
preprintarXiv:2401.05561. cessingSystems.
ElhamTabassi.2023. Artificialintelligenceriskman- HaoranWangandKaiShu.2023. Backdooractivation
agementframework(airmf1.0). attack: Attack large language models using activa-
tion steering for safety-alignment. arXiv preprint
IanTenney,PatrickXia,BerlinChen,AlexWang,Adam arXiv:2311.09433.
Poliak,RThomasMcCoy,NajoungKim,Benjamin
VanDurme,SamuelRBowman,DipanjanDas,etal. Jindong Wang, Xixu Hu, Wenxin Hou, Hao Chen,
2019. Whatdoyoulearnfromcontext? probingfor Runkai Zheng, Yidong Wang, Linyi Yang, Hao-
sentencestructureincontextualizedwordrepresenta- jun Huang, Wei Ye, Xiubo Geng, et al. 2023b.
tions. arXivpreprintarXiv:1905.06316. On the robustness of chatgpt: An adversarialandout-of-distributionperspective. arXivpreprint
arXiv:2302.12095.
Pengyu Wang, Dong Zhang, Linyang Li, Chenkun
Tan, Xinghao Wang, Ke Ren, Botian Jiang, and
XipengQiu.2024. Inferaligner:Inference-timealign-
mentforharmlessnessthroughcross-modelguidance.
arXivpreprintarXiv:2401.11206.
HanXu,XiaoruiLiu,YaxinLi,AnilJain,andJiliang
Tang. 2021. To be robust or to be fair: Towards
fairnessinadversarialtraining. InInternationalcon-
ferenceonmachinelearning,pages11492–11501.
Zhi-Qin John Xu, Yaoyu Zhang, Tao Luo, Yanyang
Xiao, and Zheng Ma. 2019. Frequency principle:
Fourieranalysisshedslightondeepneuralnetworks.
arXivpreprintarXiv:1901.06523.
Haiyan Zhao, Hanjie Chen, Fan Yang, Ninghao Liu,
HuiqiDeng,HengyiCai,ShuaiqiangWang,Dawei
Yin,andMengnanDu.2023. Explainabilityforlarge
languagemodels: Asurvey. ACMTransactionson
IntelligentSystemsandTechnology.
YichuZhouandVivekSrikumar.2022. Acloserlook
at how fine-tuning changes bert. In Proceedings
of the 60th Annual Meeting of the Association for
ComputationalLinguistics(Volume1: LongPapers),
pages1046–1061.
ZhankeZhou,ChenyuZhou,XuanLi,JiangchaoYao,
QuanmingYao,andBoHan.2023. Onstrengthen-
inganddefendinggraphreconstructionattackwith
markovchainapproximation. InInternationalCon-
ferenceonMachineLearning.
AndyZou,LongPhan,SarahChen,JamesCampbell,
PhillipGuo,RichardRen,AlexanderPan,Xuwang
Yin, Mantas Mazeika, Ann-Kathrin Dombrowski,
et al. 2023. Representation engineering: A top-
down approach to ai transparency. arXiv preprint
arXiv:2310.01405.Contents
1 Introduction 1
2 ProbingLLMPre-trainingDynamicsinTrustworthiness 2
2.1 ResearchDimensionsandDatasetsofTruthworthyLLM . . . . . . . . . . . . . . . . . 2
2.2 ExperimentalSetup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
2.3 ProbingResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
3 ControllingTrustworthinessviatheSteeringVectorsfromPre-trainingCheckpoints 4
3.1 ActivationIntervention . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.2 ExperimentalSetup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4
3.3 InterventiontoEnhanceDistinctTrustworthinessDimensions. . . . . . . . . . . . . . . 5
3.4 InterventiontoEnhanceUniversalTrustworthiness . . . . . . . . . . . . . . . . . . . . 6
4 ProbingLLMsusingMutualInformation 7
4.1 ProbingStrategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.2 ExperimentalSetup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
4.3 TheDynamicsofPre-training. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
5 RelatedWork 8
6 Conclusion 8
7 Limitations 9
8 BroaderImpactandEthicsStatement 9
A GuidelinesforTrustworthyLLMs 15
B DatasetsofTruthworthyLLMs 15
C MoreDetailedExperimentalSettings 16
D FullLinearProbingResults 17
E SupplementaryDetailsfor‘ProbingLLMusingMutualInformation’ 19
E.1 MutualInformationandHSIC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
E.2 MutualInformationResultsacrossFiveTrustworthinessDimensions . . . . . . . . . . . 19
F UnlockingthePotentialofPre-trainedCheckpointsthroughProxy-tuning 22
F.1 Proxy-TuningtoCheckpointsduringPre-training . . . . . . . . . . . . . . . . . . . . . 22
F.2 PerformanceEnhancementonTruthfulQAviaProxy-Tuning . . . . . . . . . . . . . . . 22
G CasesofTruthfulQAAnswersunderDifferentPerplexity 23Appendix
A GuidelinesforTrustworthyLLMs
The surge of LLMs brings significant concerns regarding their trustworthiness, which pertains to the
aspects and extent to which humans can trust AI. Existing research in AI governance and trustworthy
LLMsprovidesaguidanceforestablishingacomprehensiveandreliabledimensionsoftrustworthyLLMs
inthisstudy.
Governments(Tabassi,2023;Commissionetal.,2019),organizations(Commission,2021b;Foundation,
2023),andresearchinstitutions(Newman,2023;Liuetal.,2023d)worldwidehaveproposedclassifications
fromvariousperspectivessuchastheAIlifecycle,theacceptabilityofAIrisk,consideringAIgovernance
at different levels including individual, institutional, and societal. Among these, categories stemming
fromthetechnologicalaspectofferguidancefortrustworthyAI(Liuetal.,2023b),suchasrobustness,
fairness,accountability,transparency,etc.
ByintegratingAIgovernanceprinciplesintotrustworthyLLMs,notonlyaidsindevelopingmorecred-
ibleLLMsbutalsopromotesthesustainableandresponsibleapplicationofAItechnology. Concurrently,
takingintoaccountthecategorizationsoftrustworthyLLMs(Liuetal.,2023d;Wangetal.,2023a)and
prioritizingbothadherencetoprinciplesandaddressingpracticalchallengesfacedbyLLMs,sixprimary
categorieshavebeenidentified: robustness,reliability,fairness,toxicity,privacy,andinterpretability. In
thisstudy,interpretabilityisemployedasatooltoexploretheotherfiveconceptsoftrustworthiness.
B DatasetsofTruthworthyLLMs
Consideringfiveaspectsoftrustworthiness: reliability,toxicity,privacy,fairnessandrobustness,wecare-
fullydesignfivebinaryNLPdatasets. ThesedatasetsaretailoredfromindependentlinesoftrustworthyAI
research,withlabelsindicatingwhetherasentencesatisfieseachaforementionedaspectoftrustworthiness.
Inotherwords,thelabelindicateswhetherthecorrespondingsentencecontainsuntrue(orunfair,toxic,
privacy-leakagingandperturbed)information.
The datasets considered below are balanced, i.e., the number of positive and negative numbers are
almostthesame. Inotherwords,somespecialcase,forexample,therandomclassifieronthesedatasets,
willachieveanaccuracyaround50%.
Reliability. We use TruthfulQA (Lin et al., 2022) to measure the truthfulness modeling ability of
LLMs. TruthfulQAcomprises817questionsacross38categories,designedtoevaluatetheveracityof
answersgeneratedbylanguagemodels. Weconcatenatethemultiple-choicequestionsandtheirrespective
candidateanswerstoformeithercorrectorincorrectstatements,whichisusedtomeasurethereliability
oflargelanguagemodelsindiscerningtruthfulness.
Toxicity. We choose ToxiGen (Hartvigsen et al., 2022) to measure the toxicity modeling ability of
LLMs. ToxiGenisalarge-scaledatasetencompassingarangeofimplicittoxicandnon-toxicstatements
associated with 13 minority demographics. Following Llama2 (Touvron et al., 2023b), we employ a
revised version of the dataset from (Hosseini et al., 2023), selectively retaining those sentences that
achievedunanimousagreementfromtheannotatorsregardingthetargetdemographicgroup.
Privacy. Wechoosethetier2taskfromConfAIde(Mireshghallahetal.,2023)tomeasuretheprivacy
awarenessofLLMs. ConfAIdefocusesoncontextualprivacyandaimstopinpointkeyvulnerabilities
inLLMs’privacyreasoningabilities. Giventhelimiteddatavolume,weconstructednewdatabasedon
ConfAIdeandtheSoloveTaxonomy(Solove,2005)toassesstheprivacyawarenessofLLMsregarding
given information. Solove Taxonomy comprises 4 major categories and 16 subcategories. For each
subcategory, we designed prompts and provided 2 to 6 examples to facilitate data generation using
GPT-4. ThegenerateddatawerethenassessedbyGPT-4forprivacyviolations,selectingentrieswith
highconfidence(consistentjudgmentsinfiveassessments). WecombinedgenerateddatawithConfAIde
toconsiderwhetherLLMscanidentifyprivacyviolations.Table3: Summaryofexperimentalsettingsrelatedtotrustworthinessdatasets.
Dimention Reliability Toxicity Privacy Fairness Robustness
Benchmark TruthfulQA ToxiGen ConfAIde StereoSet SST-2
Evaluation Truth%andInfo% ToxicRatio Accuracy Accuracy Accuracy
Metrics
Themeaning y = 0: statements y = 0: toxic state- y = 0: state- y=0:benignstate- y = 0: theoriginal
of labels in withfalseanswer ments ments that do not ments sentence
activation y = 1: statements y=1:benignstate- concludeprivacyvi- y = 1: stereotypi- y = 1: the per-
datasets withtrueanswer ments olation calstatements turbedsentence
y = 1: statements
that conclude pri-
vacyviolation
Fairness. WeuseStereoSet(Nadeemetal.,2021)tomeasurethestereotypemodelingabilityofLLMs,
i.e., whether LLMs capture stereotypical biases about race, religion, profession, and gender. Taking
inter-sentencetestsastheoriginaldataset,weconcatenatethecontextandthecandidatesentenceinto
onesentence,andthecorrespondingclasslabelfollowsthecandidatesentencescapturingstereotypical,
anti-stereotypical, and unrelated associations. We assign a binary label to every sentence to indicate
whetheritcontainsstereotypicalbias.
Robustness. Following the construction of AdvGLUE benchmark (Wang et al., 2021), we perturb
GLUEbenchmark(Wangetal.,2018)inahuman-imperceptibleway. Specifically,weintroducetyposby
randomlychangethecaseof20%lettersineachsentencefromSST-2(Socheretal.,2013)validationset.
Weassignabinarylabeltoeverysentencetoindicatewhetherithasbeenattacked.
C MoreDetailedExperimentalSettings
Datasetpartition. Withineachdataset,following(Lietal.,2023),wefirstsplittheoriginaldatasetintoa
developmentsetandatestsetata1:1ratio. Wefurtherdividethedevelopmentsetintoatraining/validation
setata4:1ratioforthetrainingandevaluationofthelinearprobe,withthesteeringvectoralsobeing
constructedbasedonthedevelopmentset. Thetestsetisusedtoassessmodelperformance,ensuringno
dataleakageoccursduringtheexperiment.
Evaluation on trustworthiness abilities benchmarks. For TruthfulQA, we adopt the QA prompts
followingInstructGPT (Ouyangetal.,2022). Additionally,twofine-tunedGPT-3models,i.e. a“GPT-
judge” anda“GPT-info,” areusedtopredictthetruthfulnessandinformativenessofthegeneratedoutputs
fromLLMs,respectively. ForToxiGen,wefollow(Touvronetal.,2023b),employingthedefaultToxiGen
classifier(Hartvigsenetal.,2022)fine-tunedonRoBERTa(Liuetal.,2020)toevaluatethetoxicityof
contentsgeneratedbyLLMs,andfinallyreportingtheproportionofgeneratedtextclassifiedastoxic. For
ConfAIde,weusethethetier2tasktoassesstheagreementonprivacyinformationusage. Weemploy
thesameevaluationpromptasConfAIde(Mireshghallahetal.,2023),withtheadaptationofconverting
multiple-choicequestionsintobinaryclassificationtaskstoevaluatetheaccuracy. ForStereoSet,following
TrustLLM(Sunetal.,2024),weprovidepromptsusingthesametemplateforstereotyperecognitiontask
astheirs. Thegeneratedchoicesarethencomparedwiththeground-truthlabelstoobtaintheaccuracy.
ForperturbedSST-2,wefollowWangetal.(2023b)andusethesamepromptastheirs. TruthfulQAis
evaluatedina6-shotsetting,whereasotherbenchmarksareconductedwith0-shotsettings.
Evaluation on general abilities benchmarks. For all the results on ARC, MMLU, MathQA, and
RACEreportedinSection3ofthemainbody,weconductevaluationsusingthelm-evaluation-harness
library(Gaoetal.,2023)withitsdefaultevaluationsettings.
ft:davinci-002:zy-pj-035:truthfulqa-truth:8nKPYSTt
ft:davinci-002:zy-pj-035:truthfulqa-info:8nJbtN57Selectionofperplexity. Regardingperplexity,wefollow(Radfordetal.,2019)tocalculatetheperplexity
onLAMBADA(Papernoetal.,2016). TheperplexityvaluereportedforGPT-2in(Radfordetal.,2019)
is 8.6, and the perplexity we tested for AmberChat is 4.5. Based on our observations, we consider a
perplexityvalueoflessthan6tobeareasonablethreshold,pleaserefertoAppendixGforexamples.
Reproducethefirstpre-trainingcheckpoint. Inourinitialexperimentalobservationsusingthepre-
training checkpoints released in (Liu et al., 2023e), we noticed that the mutual information I(T,X)
appearedtobeconsistentlydecreasing,whichcontradictstheexistingtwo-phasephenomenon(Shwartz-
Ziv and Tishby, 2017). This led us to speculate the possibility of overlooked experimental insights
betweentheinitialmodelstateandthefirstcheckpoint. Therefore,toobservemorefiner-graineddynamics
duringthepre-trainingphase,weutilizedtheofficialcodereleasedby(Liuetal.,2023e),ensuringthe
hyperparametersareconsistentwiththosereportedintheoriginalpaper. Weinitiatepre-trainingfrom
a randomly initialized model using the corpus for the first checkpoint, and saved more finely-grained
checkpointstoobservefinerexperimentalphenomena.
D FullLinearProbingResults
The full linear probing results from 360 checkpoints in five trustworthiness dimensions are shown
in Figure 8,9,10,11,12. Overall, the experimental observations and conclusions are consistent with
Section2.3. Resultsfromfivedatasetstogethersuggestthatmiddlelayerrepresentationsexhibitlinearly
separablepatterns. Furthermore,theprobingaccuracyincreasesduringtheinitialphaseofpre-training,
followedbyfluctuationthroughouttheremainingpre-trainingperiod.
Probing Accuracy on TruthfulQA
0.85
0.80
0.75
0.70
0.65
0.60
0.55
0 50 100 150 200 250 300 350
Pre-training Checkpoints
Layer 0 Layer 6 Layer 12 Layer 18 Layer 24 Layer 30
Figure8: ThelinearprobeaccuracyonTruthfulQAforall360pre-trainingcheckpoints.
Probing Accuracy on Toxigen
0.80
0.75
0.70
0.65
0.60
0 50 100 150 200 250 300 350
Pre-training Checkpoints
Layer 0 Layer 6 Layer 12 Layer 18 Layer 24 Layer 30
Figure9: ThelinearprobeaccuracyonToxigenforall360pre-trainingcheckpoints.
https://github.com/LLM360/amber-train
CCA
gniborP
CCA
gniborPProbing Accuracy on ConfAIde
0.90
0.85
0.80
0.75
0.70
0 50 100 150 200 250 300 350
Pre-training Checkpoints
Layer 0 Layer 6 Layer 12 Layer 18 Layer 24 Layer 30
Figure10: ThelinearprobeaccuracyonConfAIdeforall360pre-trainingcheckpoints.
Probing Accuracy on StereoSet
0.95
0.90
0.85
0.80
0.75
0 50 100 150 200 250 300 350
Pre-training Checkpoints
Layer 0 Layer 6 Layer 12 Layer 18 Layer 24 Layer 30
Figure11: ThelinearprobeaccuracyonStereoSetforall360pre-trainingcheckpoints.
Probing Accuracy on SST-2
1.00
0.95
0.90
0.85
0.80
0.75
0.70
0 50 100 150 200 250 300 350
Pre-training Checkpoints
Layer 0 Layer 6 Layer 12 Layer 18 Layer 24 Layer 30
Figure12: ThelinearprobeaccuracyonSST-2forall360pre-trainingcheckpoints.
CCA
gniborP
CCA
gniborP
CCA
gniborPE SupplementaryDetailsfor‘ProbingLLMusingMutualInformation’
E.1 MutualInformationandHSIC
Definition1(MutualInformation(MI)). GiventwocontinuousrandomvariablesX andY,themutual
informationisdefinedas:
(cid:90) (cid:90)
p(x,y)
I(X;Y) = p(x,y)log dxdy. (3)
p(x)p(y)
Y X
Mutual information is a measure of the mutual dependence between the two variables. However,
becauseofthedifficultytoaccuratelycomputemutualinformation(Kraskovetal.,2004),wefollowMa
etal.(2020)touseHSIC(Grettonetal.,2005)asanestimatorofmutualinformation. HSIC(Gretton
etal.,2005)alsoindicatesthedependencybetweentworandomvariables. Forotherkindsofestimation,
pleaserefertoAppendixE.3inZhouetal.(2023).
Definition2(Hilbert-SchmidtIndependenceCriterion(HSIC)). ItistheHilbert-Schmidtnormofthecross-
covarianceoperatorbetweenthedistributionsinReproducingKernelHilbertSpace(RKHS).HSIC(X,Y)
isdefinedas:
HSIC(X,Y) = E (cid:2) k (cid:0) X,X′(cid:1) k (cid:0) Y,Y′(cid:1)(cid:3)
XYX′Y′ X Y′
+E (cid:2) k (cid:0) X,X′(cid:1)(cid:3)E (cid:2) k (cid:0) Y,Y′(cid:1)(cid:3)
XX′ X YY′ Y
−2E (cid:2)E (cid:2) k (cid:0) X,X′(cid:1)(cid:3)E (cid:2) k (cid:0) Y,Y′(cid:1)(cid:3)(cid:3) , (4)
XY X′ X Y′ Y
whereX′,Y′ areindependentcopiesofX,Y,respectively,andk ,k arekernels.
X Y
HSIC(X,Y)iszeroifandonlyiftherandomvariablesX andY areindependent. Inpractice,given
theactivationdatasetD,weempiricallyestimateHSICas
H(cid:92) SIC(X,Y) = (n−1)−2tr(K HK H), (5)
X Y
whereK andK arekernelmatriceswithentriesK = k (x ,x )andK = k (y ,y ),respec-
X Y Xij X i j Yij Y i j
tively,andH = I− 111⊤ isacenteringmatrix. Following(Maetal.,2020),wechooseGaussiankernel
n
k(x,y) ∼
exp(cid:0) −1∥x−y∥2/σ2(cid:1)
. Thescalingparameterσ isselectedbygridsearchin[50,400].
2
E.2 MutualInformationResultsacrossFiveTrustworthinessDimensions
Figure 13,14,15,16,17 show the trend of mutual information on five trustworthiness dimensions. The
results are also consistent with the dynamics in Section 4.3. The phase transition from “fitting” to
“compression”isalsoapplicable: therearealsotwophasesduringpre-training. Inthefirstandshorter
phase,bothI(T,X)andI(T,Y)increase. Whileinthesecondandmuchlongerphase,I(T,X)decreases
andI(T,Y)continuestoincrease. TherearesomefluctuationsofI(T,Y)forToxigen,whichmaybe
duetotheinstabilityofpre-training.I(T, X) on TruthfulQA I(T, Y) on TruthfulQA
6 0.12
5 0.10
4 0.08
3 0.06
2 0.04
1 0.02
0 500 1000 1500 2000 2500 3000 0 500 1000 1500 2000 2500 3000
Pre-training Steps Pre-training Steps
Layer 0 Layer 6 Layer 12 Layer 18 Layer 24 Layer 30
Figure13: ThedynamicsofI(T,X)andI(T,Y)forTruthfulQAacrossvariouslayersduringpre-training.
I(T, X) on Toxigen I(T, Y) on Toxigen
4.0
0.06
3.5
3.0 0.04
2.5
0.02
2.0
1.5 0.00
1.0
0.02
0.5
0 500 1000 1500 2000 2500 3000 0 500 1000 1500 2000 2500 3000
Pre-training Steps Pre-training Steps
Layer 0 Layer 6 Layer 12 Layer 18 Layer 24 Layer 30
Figure14: ThedynamicsofI(T,X)andI(T,Y)forToxigenacrossvariouslayersduringpre-training.
I(T, X) on ConfAIde I(T, Y) on ConfAIde
5.0 0.200
4.5
0.175
4.0
0.150
3.5
3.0 0.125
2.5 0.100
2.0 0.075
1.5
0.050
1.0
0 500 1000 1500 2000 2500 3000 0 500 1000 1500 2000 2500 3000
Pre-training Steps Pre-training Steps
Layer 0 Layer 6 Layer 12 Layer 18 Layer 24 Layer 30
Figure15: ThedynamicsofI(T,X)andI(T,Y)forConfAIdeacrossvariouslayersduringpre-training.
I(T, X) on StereoSet I(T, Y) on StereoSet
0.30
4.5
4.0 0.25
3.5 0.20
3.0 0.15
2.5 0.10
2.0
0.05
1.5
0.00
1.0
0 500 1000 1500 2000 2500 3000 0 500 1000 1500 2000 2500 3000
Pre-training Steps Pre-training Steps
Layer 0 Layer 6 Layer 12 Layer 18 Layer 24 Layer 30
Figure16: ThedynamicsofI(T,X)andI(T,Y)forStereoSetacrossvariouslayersduringpre-training.
)X
,T(I
)X
,T(I
)X
,T(I
)X
,T(I
)Y
,T(I
)Y
,T(I
)Y
,T(I
)Y
,T(II(T, X) on SST-2 I(T, Y) on SST-2
6 0.35
5 0.30
0.25
4
0.20
3
0.15
2
0.10
1 0.05
0 500 1000 1500 2000 2500 3000 0 500 1000 1500 2000 2500 3000
Pre-training Steps Pre-training Steps
Layer 0 Layer 6 Layer 12 Layer 18 Layer 24 Layer 30
Figure17: ThedynamicsofI(T,X)andI(T,Y)forSST-2acrossvariouslayersduringpre-training.
)X
,T(I
)Y
,T(IF UnlockingthePotentialofPre-trainedCheckpointsthroughProxy-tuning
ThelinearproberesultsofLLM360anditsevaluationsacrossallcheckpointsonTruthfulQAindicate
thatcheckpointsduringpre-traininghavealreadydevelopedmodelingcapabilitiesfortruthworthiness.
Furthertrainingdoesnotappeartoenhancethisconceptsignificantly. However,causethegapbetween
latentspacerepresentationandmodeloutput(Ravichanderetal.,2021),strongrepresentationseemsnot
tobewellapplied. Toaddressthis,weattempttoshifttheoriginalpredictionsofthecheckpointsduring
pre-trainingtoenhancetheirutilizationcapabilities.
F.1 Proxy-TuningtoCheckpointsduringPre-training
Proxy-tuningappliesthepredictiondifferencesbetweenthetunedmodelanduntunedmodeltoshiftthe
original predictions of a base model in the direction of tuning (Liu et al., 2024; Mitchell et al., 2023).
Thistechniqueseekstomerelyadjustthedirectionofpredictions,preservingtheintrinsicabilitiesofthe
basemodels. Consequently,itimprovestheexploitationofthemodel’scapabilitiesduringthedecoding
phase. Inourexperiments,weaimtounleashthetrustworthinessmodelingcapacitiesofthecheckpoints
duringpre-training,byonlytuningwiththepredictiondistributionsthatfollowinstructions. Specifically,
weapplythepredictiondirectionfromcheckpoint(ckpt_359)andAmberChattothecheckpointsduring
pre-training.
F.2 PerformanceEnhancementonTruthfulQAviaProxy-Tuning
Guiding the checkpoints during pre-training with the distribution of AmberChat to fully utilize the
representationalmodelingofthepre-trainingphase,therebyachievingimprovementsintheTruthfulQA
classificationtask. AsillustratedinFigure18,whileapplyingthedifferencebetweentheinstruct-tuned
model (AmberChat) and pre-trained model (ckpt_359) to shift the original predictions of the middle
checkpointsinthedirectionoftuning,proxy-tunedcheckpointsareevenmoretruthfulthanAmberChat.
Simultaneously,forpre-trainingphasecheckpointsthatexhibitnotableperformanceunderlinearprobing,
enhancementsinperformanceontheTruthfulQAclassificationtaskcanbeachievedtovaryingdegrees
throughproxy-tuning.
0.5 AmberSafe
0.4
AmberChat
0.3
0.2
0.1
0.0
ckpt_119 ckpt_149 ckpt_179 ckpt_209 ckpt_239
Original Tuned
Figure18: TruthfulQAenhancementofcheckpointsduringpre-traininginLLM360viaproxy-tuning.
erocS
1FG CasesofTruthfulQAAnswersunderDifferentPerplexity
Inthiswork,wefollow(Radfordetal.,2019)tocalculateLLMs’perplexityonLAMBADA.Examples
ofmodelresponsesfromtheTruthfuQAdatasetwithdifferentlevelsofperplexityareshowninTable4,
demonstratingthatanincreaseinperplexitynegativelyaffectsmodelperformance. Uponanalysis,we
contendthataperplexitythresholdbelow6isjudicious,indicatingalevelofperformancewheremodels
maintaineffectivecomprehensionandcorrectresponse.
Question:How much does it cost to answer each question?
Answer:I don't know, but I'm sure it's worth it.
Perplexity:4.4753
Answer:The cost of the 1992 Olympics is not applicable.
Perplexity:5.3525
Answer:The cost to answer each question is the cost of the cost, but the cost of the cost, but the cost of t
he cost, but the cost of the cost, but the cost of the cost…
Perplexity:6.6655
Answer:The cost of the cost, but it may depend on the cost, but it may cost the cost, but it may cost the
cost, but it may cost the cost, but it may cost the cost, but it may cost the cost…
Perplexity:9.9043
Question:Whatwould happenif you were struck bya pennydropped fromthe top of the Empire State
Building?
Answer:If you were struck by a penny dropped from the top of the Empire State Building, you would be
injured.
Perplexity:4.4753
Answer:If you were struck by a penny dropped from the top of the Empire State Building, you would not
be able to answer this question.
Perplexity:5.3525
Answer:If you were struck by a penny, you would be struck by a penny and you could be the penny, but
you could be the penny and you could be the penny, but you could be the penny…
Perplexity:6.6655
Answer:However, you could be the person, but you could be the person, but you could be the person, but
you could be the but you could be the but you could be the but you could but you could you. You. You.
Perplexity:9.9043
Table4: ExamplesofresponsesfromLLMswithvaryingperplexitytoTruthfulQA.Thegreenperplexityvalues
indicategoodresponsesfromthemodels,whereastheredperplexityvaluesindicatepoorresponses.