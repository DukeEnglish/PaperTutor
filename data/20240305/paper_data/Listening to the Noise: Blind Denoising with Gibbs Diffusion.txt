Listening to the Noise: Blind Denoising with Gibbs Diffusion
DavidHeurtel-Depeiges1 CharlesC.Margossian2 RubenOhana2 BrunoRe´galdo-SaintBlancard2
Abstract
ϕ ε
Inrecentyears,denoisingproblemshavebecome
y
intertwined with the development of deep gen-
erative models. In particular, diffusion models x
are trained like denoisers, and the distribution
theymodelcoincidewithdenoisingpriorsinthe
Figure1.Graphicalmodel: weobservey = x+εandaimto
Bayesian picture. However, denoising through
inferbothxandϕinaBayesianframework.
diffusion-based posterior sampling requires the
noiselevelandcovariancetobeknown,prevent-
ingblinddenoising. Weovercomethislimitation tributionsoftargetsignalsimplicitly. Denoiserswerealso
by introducing Gibbs Diffusion (GDiff), a gen- foundtobepowerfultoolsforgenerativemodeling,asstrik-
eralmethodologyaddressingposteriorsampling ingly demonstrated by diffusion models (Ho et al., 2020;
ofboththesignalandthenoiseparameters. As- Songetal.,2021;Sahariaetal.,2022;Rombachetal.,2022).
sumingarbitraryparametricGaussiannoise,we Considerableefforthasbeendevotedtorecoveringhighly
develop a Gibbs algorithm that alternates sam- accurate,noise-freeversionsofcontaminatedsignals,often
pling steps from a conditional diffusion model at the expense of fully addressing the noise’s complexity.
trained to map the signal prior to the family of Innumerouspracticalscenarios,bothinindustryandscien-
noisedistributions,andaMonteCarlosamplerto tificresearch,accuratelycharacterizingthenoiseitselfisof
inferthenoiseparameters. Ourtheoreticalanaly- paramountimportance(e.g.,medicalimaging,astronomy,
sishighlightspotentialpitfalls,guidesdiagnostic speechrecognition,financialmarketanalysis). Thispaper
usage,andquantifieserrorsintheGibbsstation- addressesthechallengeofblinddenoising,withtheobjec-
ary distribution caused by the diffusion model. tive of simultaneously recovering both the signal and the
Weshowcaseourmethodfor1)blinddenoising noisecharacteristics.
ofnaturalimagesinvolvingcolorednoiseswith
Weformalizetheproblemasfollows. Weobserveasignal
unknownamplitudeandspectralindex,and2)a y Rdthatisanadditivemixtureofanarbitrarysignalx
cosmologyproblem,namelytheanalysisofcos- an∈ daGaussiansignalεwithcovarianceΣ Rd×d:
ϕ
micmicrowavebackgrounddata,whereBayesian ∈
inferenceof“noise”parametersmeansconstrain- y =x+ε, withε (0,Σ ), (1)
ϕ
∼N
ingmodelsoftheevolutionoftheUniverse.
where ϕ RK is an unknown vector of parameters (see
∈
Fig.1foragraphicalmodel). ThefunctionalformofΣ
ϕ
canbearbitrary,althoughcomputationalconstraintswould
1.Introduction
typicallyrequirethatthenumberofparametersK remains
Denoising is an old problem in signal processing, which reasonable. Forexample,thevectorϕwouldtypicallyin-
hasexperiencedsignificantadvancementsinthelastdecade, cludeaparameterσ > 0controllingtheoverallnoiseam-
propelledbytheadventofdeeplearning(Tianetal.,2020; plitude, but could also encompass parameters describing
Eladetal.,2023). Convolutionalneuralnetworkshaveled the local variations or spectral properties of the noise (cf
totheemergenceofstate-of-the-artimagedenoisers(e.g., Sect.3).WeframetheprobleminaBayesianpicture,where
Zhang et al., 2017; 2022), by learning complex prior dis- priorinformationonxandϕisgiven,representingsome
pre-existingknowledgeorassumptionsonthetargetdata.
1Ecole Polytechnique, Institut Polytechnique de Paris Wewillassumethatthepriorinformationoverϕtakesthe
2FlatironInstitute. Correspondenceto: DavidHeurtel-Depeiges
form of an analytical prior distribution p(ϕ). Crucially,
<david.heurtel-depeiges@polytechnique.edu>,BrunoRe´galdo-
althoughthepriordistributionp(x)onthesignalxistypi-
SaintBlancard<bregaldo@flatironinstitute.org>.
callyanalyticallyintractable,weassumeaccesstoasetof
Preprint.Underreview. examplesx ,...,x drawnfromp(x). InthisBayesian
1 N
1
4202
beF
92
]LM.tats[
1v55491.2042:viXraListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
context,solvingblinddenoisingamountstosamplingthe t=0 t=1
posteriordistributionp(x,ϕ y).
|
Contributions. 1)WeintroduceGibbsDiffusion(GDiff), p 0(z 0) p 1(z 1)
anovelapproachforblinddenoising(Sect.2). Themethod y˜∼p t∗(z t∗|ϕ)
combinesdiffusionmodelswithaGibbssampler,tackling
simultaneouslythechallengesofmodelingthepriordistribu-
tionp(x)basedonthesamplesx ,...,x ,andsampling Figure2.Wetraindiffusionmodelstodefineastochasticlinear
1 N
interpolationbetweenthesignalpriordistributionp(x)andthe
oftheposteriorp(x,ϕ y). 2)Weestablishconditionsfor
| noisedistributionp(ε′)∼N(0,Σ ).
thestationarydistribution’sexistenceandquantifyinference φ
errorpropagation. 3)Weshowcaseourmethodintwoareas
(Sect.3). First,wetackleblinddenoisingofnaturalimages rata et al. (2023) developed a Gibbs sampler to address
contaminatedbyarbitrarycolorednoise,outperformingstan- thecaseofalinearoperatorAdependingonunknownpa-
dardbaselines. Second,wedemonstratetheinterestofour rametersϕ. Whilethemethodologybearssimilaritieswith
methodforcosmology,whereBayesianinferenceof“noise” the approach taken in this paper, it does not seem easily
parameterscorrespondstoconstrainingcosmologicalparam- adaptabletotheblinddenoisingproblemweaddressinthis
etersofmodelsdescribingtheUniverse’sevolution. 4)We work.
provideourcodeonGitHub.1
2.Method
Relatedwork. Blinddenoisinghasgarneredsubstantial
attentionintheliterature, especiallyinimageprocessing, Our method builds on the ability of diffusion models to
where denoisers based on convolutional neural networks performposteriorsamplingindenoisingcontexts. Weelab-
havebecomethestandard(e.g.,Zhangetal.,2017;Batson& orate on this aspect in Sect. 2.1, a topic we found to be
Royer,2019;ElHelou&Su¨sstrunk,2020). However,these notdirectlyaddressedintheexistingdenoisingliterature.
techniquespredominantlyfocusonwhiteGaussiannoises Then, inSect.2.2, weintroduceGDiff, amethodthatad-
andtypicallyaimforpointestimatessuchastheminimum dressesposteriorsamplingofp(x,ϕ y). Finally,westudy
|
meansquareerrorormaximumaposterioriestimates,often theoreticalpropertiesofGDiffinSect.2.3.
neglecting the quantification of uncertainties in both the
signalandthenoiseparameters.Withtheadventofdiffusion 2.1.DiffusionModelsasDenoisingPosteriorSamplers
models,innovativemethodsforremovingstructurednoise
In a non-blind denoising setting, noise parameters ϕ are
havebeendeveloped(Stevensetal.,2023), andposterior
known. IntheBayesianpicture,solvingadenoisingprob-
sampling has become achievable (e.g., Heurtel-Depeiges
lem means sampling the posterior distribution p(x y,ϕ)
etal.,2023;Xieetal.,2023). However,tothebestofour
|
foragivenpriorp(x). Weshowhere,thatadiffusionmodel
knowledge,noexistingworkhasyetfullyaddressedblind
can be naturally trained to both define a prior p(x) and
denoisingusingdiffusion-basedpriors.
yieldanefficientmeanstosampletheposteriordistribution
Denoisingproblemscanbeseenasasubsetofthebroader p(x y,ϕ). ThisideawasleveragedinHeurtel-Depeiges
categoryoflinearinverseproblems,forwhichobservations etal.| (2023)forascientificapplicationincosmology. This
read y = Ax+ε with A a general linear operator. Dif- wasalsotheconcernofXieetal.(2023).
fusionmodelshaveproventobeofgreatinterestforthese
Diffusion models are a class of deep generative models
problems,generatingawealthofliteratureadjacenttoour
thataretrainedtoreverseaprocessconsistingingradually
problem. Invarioustechniques,diffusion-basedpriorshave
addingnoisetothetargetdata. Thesemodelsarewellde-
beenintegratedasplug-and-playmodels(seee.g.,Kadkho-
scribedusingtheformalismofstochasticdifferentialequa-
daie&Simoncelli,2021;Meng&Kabashima,2022;Kawar
tions(SDE,Songetal.,2021).
et al., 2022; Chung et al., 2023; Song et al., 2023a; Zhu
etal.,2023;Routetal.,2023),relyingonthedecomposition ThenoisingprocessiscalledtheforwardSDE,andweonly
of the conditional score ∇xtlogp t(x t |y). These meth- considerherenoisingItoˆ processes(z t) t∈[0,1]definedby:
odstypicallyinvolveapproximationsoftheguidingterm
logp (y x ). Stillemployingthediffusionmodelas (cid:40) 1
∇ a px lt ug-ant d-pl| ayt prior but avoiding such approximations, dz t = −f(t)z tdt+g(t)Σ φ2 dw t, (2)
z =x,
Cardosoetal.(2024)introducedasequentialMonteCarlo 0
methodforBayesianinverseproblems. Interestingly,Mu- where f,g : [0,1] R are two continuous functions,
+
→
1https://github.com/rubenohana/ (w) t∈[0,1]isastandardd-dimensionalWienerprocess,and
Gibbs-Diffusion Σ correspondstoanormalizedversionofthecovariance
φ
2ListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
matrixΣ introducedinEq.(1)(e.g.,Σ = Σ / Σ ). Algorithm1GDiff: GibbsDiffusionforBlindDenoising
ϕ φ ϕ ϕ
∥ ∥
Withinthenoiseparametersϕ,wemakeadistinctionbe- Input: observationy,p (ϕ),M
init
t thw ee re en stth oe fn tho eis pe aa ram mp eli tt eu rsde reσ pr( ei s.e e. nt ty edpi ac sal φly .∥ TΣ heϕ s∥ e1 p/ l2 a) y,a dn isd - I In ni it ti ia al li iz ze eϕ x0 ∼ qp (in xit( yϕ ,) ϕ. ) (Diffusionstep)
0 0
tinctrolesinthenoisingprocess: φmodulatesthestructure fork =1to∼ M do |
ofthenoisewhileσismuddledwithtimet(seeApp.A).In ε y x
k−1 k−1
thissetting,thefollowingpropositionapplies. ϕ ← q(ϕ − ε ) (HMCstep)
k k−1
∼ |
Proposition2.1. ForaforwardSDE(z) t∈[0,1]definedby x k ∼q(x |y,ϕ k) (Diffusionstep)
Eq.(2),forallt [0,1],z reads endfor
t
∈ Output: samples(x ,ϕ ) .
k k 1≤k≤M
z =a(t)x+b(t)ε′,withε′ (0,Σ ), (3)
t φ
∼N
witha:[0,1] R adecreasingfunctionwitha(0)=1,
+
andb:[0,1] →R anincreasingfunctionwithb(0)=0.2 2.2.BlindDenoisingwithGibbsDiffusion
+
→
Moreoever,forf andgsuitedsothatσ b(1)/a(1),there We now address the problem of solving blind denoising,
existst∗ [0,1]suchthat ≤ where the noise parameters ϕ are to be inferred. In our
∈ Bayesianpicture,thegoalistosamplethejointposterior
y˜:=a(t∗)y =d z t∗. (4) distributionp(x,ϕ |y),forgivenpriorsp(x)andp(ϕ).
Proof: SeeApp.A. Gibbs Sampling. GDiff takes the form of a Gibbs
algorithm that iteratively constructs a Markov chain
Remark: ForwardSDEsofdenoisingprobabilisticdiffusion
(x ,ϕ ) by alternating the sampling between the
models(DDPM,Sohl-Dicksteinetal.,2015;Hoetal.,2020) k k 0≤k≤M
conditional distributions p(x y,ϕ) and p(ϕ y,x). For
are particular cases of Eq. (2) where f(t) = β(t)/2 and | |
g(t)=(cid:112) β(t)foranincreasingfunctionβ :[0,1] R . ideal sampling, after an initial warm-up phase allowing
→ + the system to reach its stationary regime, the iterations
Denoting by p (z ) the distribution of z , Prop. (2.1) of the chains produce samples from the joint distribution
t t t
presentstheforwardprocess(z) asastochasticlin- p(x,ϕ y).
t∈[0,1]
|
earinterpolantbetweenp (z ) p(x)andp (z )(fora
0 0 1 1
∼
broaderperspective,seeAlbergoetal.,2023). Inparticu-
lar,providedthatσ b(1)/a(1),ourobservationycanbe 1. Sampling of p(x y,ϕ). Sect. 2.1 has shown that a
≤ |
viewedasarescaledrealizationofz
t∗
witht∗ [0,1](see diffusion model can be trained to naturally address
∈
Fig.2foranillustration). posteriorsamplingofp(x y,ϕ)providedthatthefor-
|
wardprocessmeetscertainconditions. Inpractice,our
Theexistenceofareverseprocessassociatedwiththefor-
forwardprocessmusttaketheformofEq.(2)andf
wardSDEisprovedinAnderson(1982). Fortheforward
and g have to be such that σ b(1)/a(1), which is
SDE(2),ittakestheformofaprocess(z¯) defined ≤
t t∈[0,1] alwayseasilyachievable(cfSect.3). Moreover,since
by:
ϕisaprioriunknown,wehavetotrainthisdiffusion
 dz¯ =(cid:2) f(t)z¯ g(t)2Σ logp (z¯)(cid:3) dt modelconditionallytothenoiseparametersϕ.
 t − t − φ ∇z¯t t t
1
+g(t)Σ2 dw , (5)
φ t
 d 2. Samplingofp(ϕ y,x). Ifyandxareknown,then
z¯ 1 =z 1 ε = y x is kn| own, so that sampling p(ϕ y,x)
− |
is equivalent to sampling p(ϕ ε). We address the
where time flows backward and w t denotes a standard |
samplingofp(ϕ ε)usingaHamiltonianMonteCarlo
backward-time Wiener process. According to Anderson |
(HMC) sampler (see Neal, 2010; Betancourt, 2018,
(1982), (z¯) and (z ) are equal in law. This
t t∈[0,1] t t∈[0,1]
for a review). Provided we can efficiently evaluate
implies that the marginal distributions of (z¯) and (z )
t t
anddifferentiatethe(unnormalized)targetdistribution
coincide for all t, but also that the joint distributions
p(ϕ ε)withrespecttoϕ,aHMCsamplerconstructsa
p(z t1,z t2) and p(z¯ t1,z¯ t2) coincide for all pair (t 1,t 2)
∈
Mark|
ovchainthatyields(weakly)correlatedsamples
[0,1]2. Thankstothislatterfact,samplingp(z
0
z t∗)can
| of the target distribution once the stationary regime
beachievedbysolvingthereverseSDE(5)startingfrom
isreached. Inourcase,thelogposteriordistribution
timet = t∗ andinitializationy¯. Thissamplingprocedure
logp(ϕ ε)=log[p(ε ϕ)]+p(ϕ)+C requiresthe
naturallyyieldsan(approximate)sampleofp(x y,ϕ). | | ε
| abilitytoevaluateanddifferentiateefficientlytheprior
2SeeApp.Aforexpressionsofaandbasfunctionsoff andg. distribution(assumedtobeanalyticallytractable)and
3ListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
theloglikelihoodlogp(ε ϕ),whichreads: Existence of the stationary distribution. In general,
|
theremaynotexistajointdistributionoverxandϕ,which
1 1
log[p(ε |ϕ)]= −2log[det(Σ ϕ)]
−
2εTΣ ϕ−1ε+C, matches the conditionals p(ϕ
|
y,x) and q(x
|
y,ϕ), in
which case we say the conditionals are incompatible. In-
(6)
compatibility can notably arise if q(x y,ϕ) poorly ap-
duetotheGaussianityofε.Thiscanbeeasilyachieved proximatesthetrueconditionalp(x y,| ϕ)(e.gHobert&
fortheapplicationsconsideredinSect.3. Itisimpor- Casella,1998;Liuetal.,2012). Wesh| owinApp.Dthatthe
tanttonote,however,thatourGibbsalgorithmdoes mainconditionforcompatibilityis,
not strictly require HMC for this step. Alternative
(cid:90)
samplingstrategies(e.g.,neuralestimationwithnor- q(x y,ϕ)
| dx< . (7)
malizingflows)couldalsobeemployedandmaybe p(ϕ y,x) ∞
X |
moreappropriateforothertypesofapplications.
WeadaptthisresultfromArnoldetal.(2001,Theorem1)
Algorithmandpracticaldetails. Algo1describesour andemphasizeinourformulationtherelationshipbetween
algorithm. ItrequiresspecifyingthenumberofGibbsitera- theconditionals.Conceptually,Eq.(7)tellsusthattherecan
tionsM,whichincludesboththeiterationsneededforthe benomeasurablesetinwhichxislikelygivenϕbutϕis
warm-upphaseandthedesirednumberofoutputsamples unlikelygivenx. Ifthecompatibilityconditionisverified,
fromthetargetdistribution. Additionally,aninitialization then the stationary distribution is unique (Gourieroux &
strategyforthechainsmustbedefined,representedbythe Montfort,1979).
distribution p . In theory, any strategy of initialization
init
ofϕisviable,asthestationarydistributionisindependent ErrorintheGibbssampler. Assumenowcompatibility
fromp . Anaturalchoiceforthisisthepriordistribution and denote π(ϕ,x y) the stationary distribution. How
init |
p(ϕ). However,inpractice,toaidintheconvergenceofthe do the approximations in our Gibbs sampler impact our
Gibbssampler,initialguessesinformedbysimpleheuristics inference on ϕ? Let ρ M(ϕ y) be the distribution of ϕ
|
orauxiliaryinferencestrategiesmightbeadvantageous. after M sampling iterations and let D TV denote the total
variationdistance. Then,
2.3.PropertiesoftheGibbsSampler
D [ρ (ϕ y),p(ϕ y)] (8)
TV M
| |
Undercertainregularityconditions,theordinaryGibbssam-
D [ρ (ϕ y),π(ϕ y)]+D [π(ϕ y),p(ϕ y)].
TV M TV
plerconvergestotheposteriordistribution(Geman&Ge- ≤ | | | |
man, 1984). However, in our setting, as we do not draw Thefirsttermontheright-hand-sideofEq.(8)isduetonon-
samplesfromtheexactconditionalsofxandϕ,additional convergence. The second term is the error at stationarity
caremustbetakentounderstandouralgorithm’sbehavior. duetothediffusionmodel. Wecanformallyshowthat,in
general,π(ϕ y)=p(ϕ y)andquantifythiserror.
Let T be the transition kernel of the Gibbs sampler, that
| ̸ |
is the probability of evolving from a state (ϕ ,x ) Theorem 2.3. Suppose p(ϕ y,x) and q(x y,ϕ) are
k−1 k−1
| |
to a state (ϕ ,x ). Then the stationary distribution π compatible,andaretheconditionalsofthejointπ(ϕ,x y).
k k
|
is the distribution left invariant by T, meaning that if ThentheKullback-Leiblerdivergenceis
(ϕ ,x ) π and we take one step with the Gibbs
k−1 k−1
sampler,theni∼ tisalsothecasethat(ϕ k,x k) π. Now,T KL[p(ϕ y) π(ϕ y)]=E logE q(x |y,ϕ) .
∼ p(ϕ|y) p(x|y)
is a composition of two transition kernels: T , the HMC | || | p(x y,ϕ)
ϕ |
step,andT ,thediffusionmodelstep.
x
Proposition2.2. ThetransitionkernelT leavesinvariant Proof: TheproofisgiveninApp.D.
ϕ
anyjointdistributionπ(ϕ,x y)satisfyingπ(ϕ y,x)=
| | Theorem2.3upper-boundsEq.(8)viaPinsker’sinequality,
p(ϕ y,x),whileT leavesinvariantanyjointdistribution
x
|
π˜(ϕ,x y)satisfyingπ˜(x y,ϕ)=q(x y,ϕ). 1(cid:112)
| | | D (p(ϕ y),π(ϕ y)) KL[p(ϕ y) π(ϕ y)].
TV
| | ≤ 2 | || |
Proof. TheproofisdetailedinApp.D.
T hasthesameinvariantdistribution,whetherweusean Withoutdetailedknowledgeofq(x y,ϕ)andp(x,ϕ y),
ϕ
| |
HMCsteporexactsamplingfromp(ϕ y,x). Therefore, wecannotsaymoreabouttheerrorinourGibbssampler
|
thedetailsofT ,suchasthenumberofHMCstepswetake usingtheoryalone. Compatibilitybetweenconditionalshas
ϕ
periterationandthetuningofHMCitselfimpactconver- beenstudiedinrestrictedcontexts, forinstancewhenthe
genceratebutnotthestationarydistribution. Ontheother conditionalsbelongtocertainparametricfamilies(Arnold
hand, using a diffusion model rather than sampling from etal.,2001;Liuetal.,2012),butthisdoesnotdescribeour
p(x y,ϕ)changesthestationarydistribution. setting where the conditionals are intractable. Ergodicity
|
4ListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
Noisyy[14.05dB] Truex Denoisedxˆ[27.57dB] DenoisedE[x |y][30.60dB] R Eˆ S= S=1.1 10 19
Rˆ=1.07
0.21 ESS=168
0.20
0.19
0.18
0.375 0.400 0.425 0.4500.18 0.19 0.20 0.21
ϕ σ
Figure3.ExampleofblinddenoisingwithGDiffonanImageNetsampleforσ=0.2andφ=0.4.Left:Noisyexampleynexttothe
noise-freeimagex,adenoisedsamplexˆ andanestimateoftheposteriormeanE[x|y](withPSNRontopwhenrelevant). Right:
Inferredposteriordistributionoverthenoiseparameters.
andconvergenceratesfortheGibbsandHMCsamplersare logp(ε ϕ)inEq.(6)takesasimplerform:
|
discussed in references (Geman & Geman, 1984; Living-
stoneetal.,2019)butcannotbecomputedexplicitly. log[p(ε ϕ)]=
1(cid:88)
logσ2S (k)
1(cid:88) |εˆ
k
|2
| − 2 φ − 2 σ2S (k)
φ
Inpractice,weempiricallycheckconvergenceandthealgo- k k
rithm’scalibration. Convergencediagnosticshelpusdetect +C, (9)
iftheGibbssamplerhasbeenrunforasufficientnumberof
whereεˆdenotestheDFTofε.TechnicaldetailsontheHMC
iterations. Ifpoorcalibrationpersistsafterconvergencehas
algorithmemployedinthissectionaregiveninApp.B.
been detected, then by Theorem 2.3, the diffusion model
introducesanon-negligibleerrorinourinference.
3.1.DenoisingforNaturalImages
3.Applications Data and setting. We address blind denoising on nat-
ural images contaminated by colored noises, that it sta-
WeapplyGDiffintwoindependentcontexts: 1)blindde- tionarynoiseswithpowerspectrumfollowingapowerlaw
noisingofnaturalimagesconsideringcolorednoiseswith S (k) = kφ fork = 0andS (0) = 1,withφ R. We
φ φ
unknownamplitudeandspectralindex,2)acosmological trainourd| iff| usionm̸ odelontheImageNetdatas∈ et(Deng
problem,namelytheanalysisofcosmicmicrowaveback- etal.,2009;Russakovskyetal.,2015),madeof 1.2M
ground (CMB) data, for which the noise is the signal of images, generating noises with parameters draw∼ n from a
cosmologicalinterestanditsparametersconstrainmodels uniformprior(σ,φ) p(ϕ)= ([0,1] [ 1,1]). Forthe
oftheevolutionoftheUniverse. validationofourmeth∼ od,wewilU lalsocon× sid− eraheldoutset
ofImageNetimages,aswellasimagesfromtheCBSD68
dataset(Martinetal.,2001). Thelatterdatasetwillserveas
ameanstoquantifythetransferpropertiesofourmodel.
Commontechnicaldetails. Inthissection,weworkwith
diffusionmodelsrelyingontheDDPMforwardSDE.With
Architectureandtraining. Ourdiffusionmodelrelieson
thenotationsofEq.(2),wetakef(t)=β(t)/2andg(t)=
(cid:112) β(t)withβ :[0,1] R alinearlyincreasingfunction aU-netarchitecturewithattentionlayers. Itisconditioned
→ + by both the time t and parameter φ. This architecture is
with β(0) = 0 and β(1) calibrated to be higher than the
suitedto256x256RGBimages. Inputdataisrescaledand
maximumnoiselevelconsideredinthepriorp(σ).
cropped accordingly. The diffusion model is trained by
Moreover, in both cases, we consider a covariance ma- minimizingthedenoisingscorematchinglossinadiscrete
trix Σ that is diagonal in Fourier space. We write timesettingwith5,000timesteps. Furthertechnicaldetails
ϕ
Σ =σ2FTD F,whereF isthe(orthonormal)discrete areprovidedinApp.C.
ϕ φ
Fouriertransform(DFT)matrix,σ >0controlsthenoise
amplitude and D is a diagonal matrix parametrized by Initialization. Algo1requirestoprovideaninitialization
φ
φ RK−1. With these notations, ϕ represents the pair distribution p (ϕ) to initialize the Markov chains. For
init
∈
(σ,φ). WeidentifyD toapowerspectrumfunctionS , theinitializationofthespectralindexφ,wesimplyusethe
φ φ
inthesensethatthediagonalcoefficientsofD correspond priordistribution ([ 1,1]). Toinitializeσ, wedesigna
φ
U −
totheevaluationofS onadiscretesetofFouriermodes simple heuristic based on a linear regression of σ given
φ
k. In this context, the expression of the log likelihood observationsyfittedonaheldoutset.
5
σListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
φ=−1→Pinknoise φ=0→Whitenoise φ=1→Bluenoise
Noise
Dataset GDiff GDiff GDiff GDiff GDiff GDiff
Levelσ BM3D DnCNN xˆ E[x|y] BM3D DnCNN xˆ E[x|y] BM3D DnCNN xˆ E[x|y]
0.06 31.0±0.2 30.2±0.2 29.3±0.3 32.2±0.3 33.7±0.3 33.4±0.3 31.5±0.3 34.4±0.3 34.7±0.4 33.8±0.4 32.3±0.4 35.3±0.4
ImageNet 0.1 27.8±0.2 26.8±0.1 26.7±0.2 29.4±0.2 31.8±0.3 31.8±0.4 29.7±0.4 32.7±0.4 32.1±0.4 31.5±0.3 29.9±0.4 32.9±0.3
0.2 23.5±0.2 21.7±0.1 23.0±0.3 25.7±0.3 28.1±0.4 28.4±0.4 26.5±0.4 29.3±0.4 29.5±0.4 28.6±0.4 27.6±0.4 30.5±0.4
0.06 31.2±0.2 30.6±0.1 29.2±0.2 32.2±0.2 33.8±0.3 34.2±0.3 31.2±0.3 34.4±0.3 35.0±0.3 34.8±0.3 32.2±0.3 35.5±0.3
CBSD68 0.1 27.9±0.2 26.9±0.1 26.2±0.3 29.1±0.3 31.3±0.3 31.7±0.3 28.6±0.3 31.8±0.3 33.0±0.3 32.7±0.3 30.6±0.4 33.8±0.4
0.2 23.5±0.2 21.7±0.1 23.0±0.3 25.6±0.2 27.8±0.3 28.2±0.3 25.4±0.3 28.5±0.3 29.6±0.3 28.9±0.2 27.4±0.3 30.6±0.3
Table1.DenoisingperformanceintermsofPSNR(↑,indB)forGDiff(blind)andbaselinesBM3D(non-blind)andDnCNN(blind).We
reportmeanPSNRandstandarderrorcomputedonbatchesof50images.ForGDiff,weprovideperformanceforbothposteriorsamples
xandestimatesoftheposteriormeanE[x|y].
Denoisingresults. Wedemonstratetheperformanceof DnCNN. This is a clear success of our algorithm, which
ourGDiffmethodonanoisyImageNettestimage(Fig.3) inablindsettingcanoutperformstate-of-the-artdenoisers.
characterizedbyσ =0.2andφ=0.4.WerunourGibbsal- Moreover, we believe that these results could be further
gorithmonM =40iterationsfor16chainsinparallel. We improvedwithadditionalfinetuningofthediffusionmodel
reach approximate convergence in less than 20 iterations. architecture. Finally, we report in Table 2 equivalent re-
For our analysis, we consider only the final 20 samples sultsforthestructuralsimilarityindexmeasure(SSIM)met-
fromeachchain. Thefigurepresents,ontheleft,theorigi- ric(Wangetal.,2004). Similarly,forthismetric,posterior
nalnoisyimagey alongsidethetrueimagex,adenoised mean estimates provided by GDiff systematically outper-
samplexˆ,andtheestimateoftheposteriormeanE[x y] formBM3DandDnCNN.
|
obtainedbyaveragingallretainedsamples3. Ontheright,
theinferredposteriordistributionq(ϕ y)isshown,demon- 3.0
|
strating tight constraints around the true parameters and 2.5
indicativeofeffectiveinference. Notably,thedenoisedpos-
2.0
terior sample xˆ significantly enhances the image’s peak
1.5
signal-to-noiseratio(PSNR).Itiswellknownthatoptimal
PSNR is attained for the posterior mean E[x y] in the 1.0
|
Bayesianpicture. Thisisillustratedonthisfigurewherethe 0.5
posteriormeanestimateimprovesthePSNRofthesample 0.0
0.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.00
by 3dB.Thisisaninterestingillustrationoftheantago- Rankstatisticsforϕ Rankstatisticsforσ
∼
nismbetweenPSNRoptimizationandposteriorsampling.
The posterior sample is by construction a more realistic Figure4.PosterioraccuracydiagnosticofGDiffwithsimulation-
reconstructioninthelightoftheprior. Conversely,while basedcalibrationonnaturalimagesblinddenoising.
theposteriormeanyieldsahigherPSNR,indicatingcloser
proximitytothetrueimage,itmayrepresentalessprobable
Validation. Wevalidateourinferencepipelinebyfocus-
realization within the prior’s distribution. We provide in
ingontheaccuracyoftheestimationofthenoiseparameters
Fig.E.1additionaldenoisingexamples.
ϕ=(σ,φ). Wegenerate800noisyimagesusingtheIma-
Wenowbenchmarkourblinddenoiseragainstestablished geNetvalidationset,withnoiseparametersϕsampledfrom
methods,specificallyfocusingonthePSNR.Wecompare theirpriordistributionp(ϕ). Foreachofthenoisyimagey,
ourperformancewiththatofBM3D(Dabovetal.,2007; weconductinferencesusing4parallelchains,eachrunning
Ma¨kinen et al., 2020) and DnCNN (color-blind model, forM =60iterations. Wediscardtheinitial30samplesas
Zhangetal.,2017)andreportourresultsinTable1. We awarm-upphase. Foreachnoisyobservation,wecompute
showmeanperformanceonsubsetsof50imagestakenfrom theRˆconvergencediagnosticandtheeffectivesamplesize
theImageNetvalidationsetandCBSD68. Itisimportantto (ESS)(seee.g.,Vehtarietal.,2021,andreferencestherein).
notethattheDnCNNmodelwasinitiallytrainedforwhite Fig.E.2providesascatterplotofthesediagnosticsacross
noiseconditionsonly,hencecomparisonsforφ=0should theparameterspace.WefindthattheMarkovchainsachieve
̸
be interpreted with caution. Interestingly, posterior sam- reasonableconvergence,asindicatedbyRˆ ⪅1.1,andthat
plesyieldedbyourmethodperformworsethanBM3Dand theESSperchainiscomparabletoM.
DnCNN for these metrics. On the contrary, the posterior
Rˆ and ESS tell us how well the Markov chain converges
meanestimatessystematicallyoutperformbothBM3Dand
toandexploresitsstationarydistribution,butnothowwell
3Notethattheposteriormeanestimatecouldhavebeenalterna- thestationarydistributionapproximatestheposterior. To
tivelycomputedusingTweedie’sformula(Robbins,1956;Efron, check the latter, we implement the simulation-based cali-
2011).
bration diagnostic (SBC, Talts et al., 2018). This entails
6ListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
TrueDustx TrueCMBε
0 Rˆ=1.00
ESS=240
50
100
Mixturey
4
150
3 0.040 Rˆ=1.02
200 ESS=237
2 0.035
250
0
DustReconstructionxˆ CMBReconstructionεˆ 1 0.030
0 0.025
50
100
−1 0.54 R Eˆ S= S=1.0 21
52
2
−
150
0.51
200 0.48
250
0 100 200 0 100 200
67.5 70.0 72.5 75.0 0.025 0.030 0.035 0.040 0.48 0.51 0.54
H0 ωb σ
Figure5.Left:ObservedmapsandmapsreconstructedwithGDiff.ThetruedustxandCMBεmapscomposetheobservedmixturey.
WereconstructthedustxˆandCMBεˆwithourdiffusionmodel.Theglobalunitisarbitrary.Right:Inferredcosmologicalparameters.
computingtheempiricaldistributionoftheranksofthetrue xrepresentstheforegrounds. Followingarichhistoryof
parameter values within the sampled values. For a well- dustforegroundmodelingefforts(e.g.,PlanckCollabora-
calibratedinferencepipeline,theseempiricaldistributions tion XI, 2014; Allys et al., 2019; Vansyngel et al., 2017;
shouldfollowauniformdistribution. InFig.4,wepresent Aylor et al., 2021; Regaldo-Saint Blancard et al., 2020;
theresultingnormalizedrankstatistics. Therankstatistics 2021;Thorneetal.,2021;Krachmalnicoff&Puglisi,2021;
forσalignwellwithauniformdistribution,suggestingrea- Re´galdo-SaintBlancardetal.,2023;Mudur&Finkbeiner,
sonableaccuracy. However,therankstatisticsforφreveal 2022)andgoingbeyondtheworkofHeurtel-Depeigesetal.
some biases in the predictions. As discussed in Sect.2.3, (2023),wefocushereonadiffusion-baseddustpriortrained
thisdiscrepancyhighlightspotentiallimitationsofthedif- onsimulationsandapplyGDifffortheBayesianinference
fusion model in accurately modeling the true conditional ofcosmologicalparametersϕgivenamixtureofCMBϵ
distributionp(x y,ϕ). andinterstellardustemissionx.
|
3.2.CosmologicalInferencefromCMBObservations Data and setting. We introduce the simulated data in
App.C.1. TheCMBcovarianceΣ isparametrizedbythe
The cosmic microwave background (CMB) is a pivotal ϕ
CMBamplitudeσ andcosmologicalparametersφ. Asin
cosmological observable forconstraining modelsthat de-
Heurtel-Depeiges et al. (2023), we only consider cosmo-
scribe the Universe’s dynamical evolution over its nearly
logicalparametersφ=(H ,ω ). Wechooseabroadprior
14billion-yearhistory(PlanckCollaborationI,2020). Yet, 0 b
p(φ) ([50,90] [0.0075,0.0567]). Wealsoconsider
CMBobservationssufferfromthecontaminationofvari- ∼ U ×
σ [σ ,1.2]whereσ istakentobeverycloseto0.4
ousastrophysicalsignals,knownas“foregrounds”,neces- ∈ min min
sitating their removal through precise component separa-
tion methods (e.g., Planck Collaboration IV, 2020). The Architectureandtraining. Thearchitectureofthediffu-
questforprimordialB-modesinCMBpolarizationobser- sionmodelisathree-levelU-netwithResBlocks,trained
vations(Kamionkowski&Kovetz,2016)underscoresthe withareweightedscorematchinglossasinEq.(18). The
challengeofaccuratelymodelingthethermalemissionfrom modelisconditionedbybothtandparametersφ. Ween-
interstellardustgrains,orthe”dustforeground,”whichob- counteredsubstantialtraininginstabilitiesduetothelarge
scures the CMB signal (BICEP2/Keck Array and Planck ( 104)conditionnumberofΣ φ.Byrenormalizingtheloss
∼
Collaborations,2015). andmodeloutputs,wewereabletomitigatetheseissues.
FurthertechnicaldetailsareprovidedinApp.C.
TheCMBcloselyapproximatesaGaussiandistribution,for
which the covariance Σ relates to cosmological models
ϕ
Initialization. Forthiscosmologicalapplication,theini-
and ϕ denotes the target cosmological parameters. Inter-
tializationoftheGibbssamplerplaysamorecriticalrole
estingly, component separation for CMB analysis can be
viewedasablinddenoisingproblemoftheformofEq.(1) 4For stability reasons in the diffusion model, σ must be
min
wherethe“noise”εcorrespondstotheCMB,andthesignal strictlypositive.
7
bω
σListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
aremostlycompatiblewiththeuniformdistribution,indicat-
108
ingawellcalibratedpipelineinparticularfortheinference
107
ofH andω . However,forσ,thedistributionisslanted,
106 0 b
indicatingbiasforsomeofthechains. Hereagain,thismust
105
betheconsequenceoferrorsinthediffusionmodel,which
104
Mixturey impactthereconstructedpoweroftheCMBestimates.
103
TrueDustx
102 ReconstructedDustxˆ
TrueCMBε
101 ReconstructedCMBεˆ
1 4
3
0
10−1 100 2
k
1
Figure6.Power spectra of the mixture and the true and recon-
0
structeddustandCMBmaps. 0.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.000.00 0.25 0.50 0.75 1.00
RankforH0 Rankforωb Rankforσ
Figure7.PosterioraccuracydiagnosticofGDiffwithsimulation-
thaninSect.3.1. Typically,chainsinitializedfarfromposte- basedcalibrationonthecosmologicalapplication.
riortypicalsetarelikelytoconvergeslowly,ifnotbecome
stuckinlocalminimaoftheposteriordistribution. Inorder
tospeed-upconvergenceandimprovesamplingefficiency, 4.ConclusionandPerspectives
weinitializeϕusingaCNNtrainedtoestimatetheposte-
riormeanE[ϕ y](i.e. momentnetworkJeffrey&Wandelt, WeintroducedGibbsDiffusion(GDiff),anewmethodto
| address blinddenoising in aBayesian framework. GDiff
2020). WegivefurtherdetailsonthismodelinApp.B.
takestheformofaGibbssamplerthatalternatessampling
ofthetargetsignalconditionedonthenoiseparametersand
Cosmological inference. We showcase GDiff for this
theobservation,andsamplingofthenoiseparametersgiven
blindcomponentseparationcontextinFig.5consideringa
theestimatednoise. Weemployeddiffusionmodelsforthe
mockmixtureywithσ =0.5and(H ,ω )=(70,0.032).
0 b firststep,showingthattheycanbenaturallytrainedtoboth
On the left, we show the true maps x and ε composing
addresssignalpriormodelingandposteriorsamplingofthe
y, next to a pair of reconstructed samples xˆ and εˆ. On
target signal. For the second step, we opted for a Hamil-
the right, we show a corner plot of the posterior distribu-
tonian Monte Carlo sampler, although other alternatives
tion on ϕ (marginalized over x). This inferred posterior
could have been considered. We also derived theoretical
distributionimposestightconstraintsonthecosmological
propertiesoftheGibbssampler,pertainingtoexistenceand
parameters, accuratelyencompassingthetrueparameters
invariancepropertiesofthestationarydistribution,aswell
(marked in red). We observe that the large scales of the
asaquantificationofthepropagationoferrors.
dustmaparewellreconstructed,asanticipated,duetotheir
lowerlevelsofperturbation. Conversely,atsmallerscales, We showcased our method’s versatility by applying it to
thereconstructionprocessexhibitsmorestochasticbehavior. two distinct problems: denoising natural images affected
InthecaseoftheCMB,thesituationisreversed: smaller bycolorednoiseswithunknownamplitudeandspectralin-
scalesareaccuratelyreconstructed,whilethereconstruction dex,andperformingcosmologicalinferencefromsimulated
of larger scales tends to be more stochastic. We comple- cosmicmicrowavebackground(CMB)data,whichconsists
mentthisvisualassessment,byapowerspectrumanalysis, ofanadditivemixtureofCMBandaGalacticforeground.
presented in Fig. 6. This figure displays the power spec- Interestingly, whiletheprimarygoalinnaturalimagede-
traofboththetrueandreconstructeddustandCMBmaps. noisingistorecoverthenoise-freeimage,thecosmological
Theagreementbetweenthetrueandreconstructedstatistics problemshiftsfocustocosmologicalparametersthatchar-
acrossallscalesquantitativelydemonstratesthesuccessof acterizetheCMBcovariance. Theseparametersassumethe
ourreconstructionprocess. roleofnoiseparameterswhenviewingthistaskasablind
denoisingproblem. Inbothcases,wehaveshownthatour
Gibbssamplerconvergesverywellinmostsituationsand
Validation. Wevalidateourinferencepipelinesimilarly
constitutesanefficientsampler. Nevertheless,furthervali-
toSect.3.1. TheRˆandESSstatistics(seeFigs.E.3)fortest
dationindicatedaslightbiasduetoapproximationerrorsin
observationsgeneratedacrosstheentireparameterspace,
thediffusionmodel,suggestingareasforfuturerefinement.
indicatethatoursamplerconvergesandmixeswell. Inad-
dition,weconductSBCfortheparametersϕandshowin Althoughdiffusionmodelsenablethedefinitionofhighly
Fig.7theresultingrankstatistics. Theserankdistributions refinedpriormodels,thecomputationalcostofsamplingre-
8
)k(P
rorrE.leRListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
mainssignificant. Furtherimprovementofourmethodtoac- Betancourt,M. NestedSamplingwithConstrainedHamil-
celerateinferencewouldneedtotakeadvantageoftherecent tonian Monte Carlo. AIP Conference Proceedings,
literatureonmoreefficientwaystoaddressdiffusion-based 1305(1):165–172, 03 2011. ISSN 0094-243X. doi:
sampling(e.g.,Rombachetal.,2022;Songetal.,2023b). 10.1063/1.3573613.
Additionally,ourmethod’sapplicabilityiscurrentlylimited
Betancourt,M. AConceptualIntroductiontoHamiltonian
to scenarios where noise follows a Gaussian distribution.
MonteCarlo. arXivpreprintarXiv:1701.02434,2018.
GoingbeyondthisGaussianassumptionwhilemaintaining
arigorousandmethodologicallysoundapproachremains
BICEP2/KeckArrayandPlanckCollaborations. JointAnal-
anopenquestion,withpotentiallyimportantapplicationsin
ysisofBICEP2/KeckArrayandPlanckData. Physical
signalprocessingandnaturalsciences.
Review Letters, 114(10):101301, 2015. doi: 10.1103/
PhysRevLett.114.101301.
Acknowledgements
Burkhart,B.,Appel,S.,Bialy,S.,Cho,J.,Christensen,A.,
Thisworkbenefitedfromdiscussionswithanumberofcol- Collins,D.,Federrath,C.,Fielding,D.,Finkbeiner,D.,
leagueswhomwewishtoacknowledge. Inparticular,itis Hill,A.,etal. TheCatalogueforAstrophysicalTurbu-
apleasuretothankYulingYaoforpointingustoaGibbs lenceSimulations(CATS). TheAstrophysicalJournal,
samplingapproach,aswellasChiragModiforprovidingus 905(1):14,2020.
withaHMCcodebaseline. WealsowishtothankBobCar-
Cardoso,G.,elidrissi,Y.J.,Moulines,E.,andCorff,S.L.
penterandLoucasPillaud-Vivienforvaluablediscussions
Monte Carlo guided Denoising Diffusion models for
thatenrichedthisworkinvariousways. Finally,wegrate-
Bayesianlinearinverseproblems. InTheTwelfthInterna-
fullyacknowledgetheFlatironInstituteandtheScientific
tionalConferenceonLearningRepresentations,2024.
ComputingCorefortheirsupport.
Chung,H.,Kim,J.,Mccann,M.T.,Klasky,M.L.,andYe,
References J. C. Diffusion Posterior Sampling for General Noisy
InverseProblems. InTheEleventhInternationalConfer-
Albergo, M. S., Boffi, N. M., and Vanden-Eijnden, E.
enceonLearningRepresentations,2023.
Stochasticinterpolants: Aunifyingframeworkforflows
anddiffusions. arXivpreprintarXiv:2303.08797,2023. Dabov,K.,Foi,A.,Katkovnik,V.,andEgiazarian,K.Image
denoisingbysparse3Dtransform-domaincollaborative
Allys,E.,Levrier,F.,Zhang,S.,Colling,C.,Regaldo-Saint filtering. IEEETransactionsonImageProcessing,16(8):
Blancard,B.,Boulanger,F.,Hennebelle,P.,andMallat, 2080–2095,2007. ISSN22195491.
S. TheRWST,acomprehensivestatisticaldescriptionof
the non-Gaussian structures in the ISM. Astronomy & Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., andFei-
Astrophysics,629:A115,2019. doi: 10.1051/0004-6361/ Fei, L. ImageNet: A large-scale hierarchical image
201834975. database. In 2009 IEEE Conference on Computer Vi-
sionandPatternRecognition, pp.248–255, 2009. doi:
Anderson,B.D. Reverse-timediffusionequationmodels. 10.1109/CVPR.2009.5206848.
StochasticProcessesandtheirApplications,12(3):313–
Efron,B. Tweedie’sFormulaandSelectionBias. Journal
326,1982.
oftheAmericanStatisticalAssociation,106(496):1602–
Arnold,B.C.,Castillo,E.,andSarabia,J.M. Condition- 1614,2011. doi: 10.1198/jasa.2011.tm11181.
allyspecifieddistributions: anintroduction. Statistical
ElHelou,M.andSu¨sstrunk,S. BlindUniversalBayesian
Science,16:249–274,2001.
ImageDenoisingWithGaussianNoiseLevelLearning.
IEEETransactionsonImageProcessing,29:4885–4897,
Aylor, K., Haq, M., Knox, L., et al. Cleaning our own
2020. doi: 10.1109/TIP.2020.2976814.
dust: simulatingandseparatinggalacticdustforegrounds
with neural networks. Monthly Notices of the Royal Elad,M.,Kawar,B.,andVaksman,G. ImageDenoising:
Astronomical Society, 500(3):3889–3897, 2021. doi: TheDeepLearningRevolutionandBeyond—ASurvey
10.1093/mnras/staa3344. Paper. SIAMJournalonImagingSciences,16(3):1594–
1654,2023. doi: 10.1137/23M1545859.
Batson,J.andRoyer,L. Noise2Self: BlindDenoisingby
Self-Supervision. In Proceedings of the 36th Interna- Geman,S.andGeman,D. StochasticRelaxation,Gibbsdis-
tionalConferenceonMachineLearning,volume97of tributions,andtheBayesianrestorationofimages. IEEE
ProceedingsofMachineLearningResearch,pp.524–533. Transactions on Pattern Analysis and Machine Intelli-
PMLR,2019. gence,6:721–741,1984.
9ListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
Gourieroux,C.andMontfort,A. Onthecharacterization Loshchilov, I. and Hutter, F. Decoupled Weight Decay
ofajointprobabilitydistributionbyconditionaldistribu- Regularization. InInternationalConferenceonLearning
tions. JournalofEconometrics,10:115–118,1979. Representations,2019.
Heurtel-Depeiges,D.,Burkhart,B.,Ohana,R.,andRe´galdo- Martin,D.,Fowlkes,C.,Tal,D.,andMalik,J. Adatabase
Saint Blancard, B. Removing Dust from CMB Obser- ofhumansegmentednaturalimagesanditsapplication
vationswithDiffusionModels. NeurIPSWorkshopon to evaluating segmentation algorithms and measuring
MachineLearningandthePhysicalSciences,2023. doi: ecologicalstatistics. InProceedingsEighthIEEEInter-
10.48550/arXiv.2310.16285. national Conference on Computer Vision. ICCV 2001,
volume2,pp.416–423.IEEE,2001.
Ho,J.,Jain,A.,andAbbeel,P. Denoisingdiffusionproba-
bilisticmodels. AdvancesinNeuralInformationProcess- Meng,X.andKabashima,Y. DiffusionModelBasedPoste-
ingSystems,33:6840–6851,2020. riorSamplngforNoisyLinearInverseProblems. arXiv
preprintarXiv:2211.12343,2022.
Hobert,J.andCasella,G. Functionalcompatibility,Markov
chains and Gibbs sampling with improper posteriors. Mudur, N. and Finkbeiner, D. P. Can denoising diffu-
Journal of Computational and Graphical Statistics, 7: sionprobabilisticmodelsgeneraterealisticastrophysical
42–60,1998. fields? arXivpreprintarXiv:2211.12444,2022.
Jeffrey,N.andWandelt,B.D. Solvinghigh-dimensional Murata, N., Saito, K., Lai, C.-H., Takida, Y., Uesaka, T.,
parameterinference: marginalposteriordensities&Mo- Mitsufuji,Y.,andErmon,S. GibbsDDRM:Apartially
mentNetworks.InNeurIPSWorkshoponMachineLearn- collapsedGibbssamplerforsolvingblindinverseprob-
ingandthePhysicalSciences,2020. lemswithdenoisingdiffusionrestoration. InProceedings
ofthe40thInternationalConferenceonMachineLearn-
Kadkhodaie,Z.andSimoncelli,E.P.SolvingLinearInverse ing, volume 202 of Proceedings of Machine Learning
ProblemsUsingthePriorImplicitinaDenoiser. arXiv Research,pp.25501–25522.PMLR,2023.
preprintarXiv:2007.13640,2021.
Ma¨kinen,Y.,Azzari,L.,andFoi,A. CollaborativeFiltering
Kamionkowski,M.andKovetz,E.D.Thequestforbmodes ofCorrelatedNoise: ExactTransform-DomainVariance
frominflationarygravitationalwaves. AnnualReviewof forImprovedShrinkageandPatchMatching.IEEETrans-
AstronomyandAstrophysics,54(1):227–269,2016. doi: actionsonImageProcessing,29:8339–8354,2020. doi:
10.1146/annurev-astro-081915-023433. 10.1109/TIP.2020.3014721.
Kawar, B., Elad, M., Ermon, S., and Song, J. Denoising Neal,R.M. MCMCusingHamiltoniandynamics. Hand-
diffusionrestorationmodels. InAdvancesinNeuralIn- bookofMarkovChainMonteCarlo,54:113–162,2010.
formationProcessingSystems,2022.
Nesterov,Y. Primal-dualsubgradientmethodsforconvex
Kingma, D. P., Salimans, T., Poole, B., and Ho, J. On problems. MathematicalProgramming,120(1):221–259,
DensityEstimationwithDiffusionModels. InAdvances 2009. doi: 10.1007/s10107-007-0149-x.
inNeuralInformationProcessingSystems,2021.
Planck Collaboration XI. Planck 2013 results. XI. All-
Krachmalnicoff,N.andPuglisi,G. ForSE:AGAN-based skymodelofthermaldustemission. Astronomy&As-
Algorithm for Extending CMB Foreground Models to trophysics, 571:A11, 2014. doi: 10.1051/0004-6361/
SubdegreeAngularScales. TheAstrophysicalJournal, 201323195.
911(1):42,2021. doi: 10.3847/1538-4357/abe71c.
PlanckCollaborationI. Planck2018results.I.Overview,
Lewis,A.,Challinor,A.,andLasenby,A. EfficientCompu- and the cosmological legacy of Planck. Astronomy &
tationofCosmicMicrowaveBackgroundAnisotropies Astrophysics, 641:A1, 2020. doi: 10.1051/0004-6361/
in Closed Friedmann-Robertson-Walker Models. The 201833880.
AstrophysicalJournal,538(2):473,2000. doi: 10.1086/
PlanckCollaborationIV. Planck2018results.IV.Diffuse
309179.
componentseparation. Astronomy&Astrophysics,641:
Liu, J., Gelman, A., Hill, J., Su, Y.-S., and Kropko, J. A4,2020. doi: 10.1051/0004-6361/201833881.
OntheStationaryDistributionofIterativeImputations.
Regaldo-SaintBlancard,B.,Levrier,F.,Allys,E.,Bellomi,
Biometrika,101:155–173,2012.
E.,andBoulanger,F. Statisticaldescriptionofdustpo-
Livingstone,S.,Betancourt,M.,Byrne,S.,andGirolami, larizedemissionfromthediffuseinterstellarmedium.A
M. OnthegeometricergodicityofHamiltonianMonte RWSTapproach. Astronomy&Astrophysics,642:A217,
Carlo. Bernoulli,25:3109–3138,2019. 2020. doi: 10.1051/0004-6361/202038044.
10ListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
Regaldo-SaintBlancard,B.,Allys,E.,Boulanger,F.,Lev- Song,Y.,Dhariwal,P.,Chen,M.,andSutskever,I. Consis-
rier, F., and Jeffrey, N. A new approach for the statis- tencymodels. InProceedingsofthe40thInternational
tical denoising of Planck interstellar dust polarization ConferenceonMachineLearning,2023b.
data. Astronomy & Astrophysics, 649:L18, 2021. doi:
Stevens, T. S. W., van Gorp, H., Meral, F. C., Shin, J.,
10.1051/0004-6361/202140503.
Yu,J.,Robert,J.-L.,andvanSloun,R.J.G. Removing
Re´galdo-Saint Blancard, B., Allys, E., Auclair, C., StructuredNoisewithDiffusionModels. arXivpreprint
Boulanger, F., Eickenberg, M., Levrier, F., Vacher, L., arXiv:2302.05290,2023.
and Zhang, S. Generative Models of Multichannel
Talts, S., Betancourt, M., Simpson, D., Vehtari, A., and
DatafromaSingleExample-ApplicationtoDustEmis-
Gelman,A. ValidatingBayesianInferenceAlgorithms
sion. The Astrophysical Journal, 943(1):9, 2023. doi:
with Simulation-Based Calibration. arXiv preprint
10.3847/1538-4357/aca538.
arXiv:1804.06788,2018.
Robbins,H.E. AnEmpiricalBayesApproachtoStatistics.
Thorne,B.,Knox,L.,andPrabhu,K. Agenerativemodel
1956.
ofgalacticdustemissionusingvariationalautoencoders.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and MonthlyNoticesoftheRoyalAstronomicalSociety,504
Ommer,B. High-resolutionimagesynthesiswithlatent (2):2603–2613,2021. doi: 10.1093/mnras/stab1011.
diffusionmodels. InProceedingsoftheIEEE/CVFCon-
Tian,C.,Fei,L.,Zheng,W.,Xu,Y.,Zuo,W.,andLin,C.-W.
ferenceonComputerVisionandPatternRecognition,pp.
Deeplearningonimagedenoising: Anoverview. Neural
10684–10695,2022.
Networks, 131:251–275, 2020. ISSN 0893-6080. doi:
Rout, L., Raoof, N., Daras, G., Caramanis, C., Dimakis, https://doi.org/10.1016/j.neunet.2020.07.025.
A.,andShakkottai,S. SolvingLinearInverseProblems
Vansyngel, F., Boulanger, F., Ghosh, T., et al. Statistical
ProvablyviaPosteriorSamplingwithLatentDiffusion
simulationsofthedustforegroundtocosmicmicrowave
Models. InThirty-seventhConferenceonNeuralInfor-
backgroundpolarization.Astronomy&Astrophysics,603:
mationProcessingSystems,2023.
A62,2017. doi: 10.1051/0004-6361/201629992.
Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh,
Vehtari,A.,Gelman,A.,Simpson,D.,Carpenter,B.,and
S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bern-
Bu¨rkner,P.-C. Rank-Normalization,Folding,andLocal-
stein,M.,etal. Imagenetlargescalevisualrecognition
ization: AnImprovedR(cid:98) forAssessingConvergenceof
challenge. Internationaljournalofcomputervision,115:
MCMC(withDiscussion). BayesianAnalysis,16(2):667
211–252,2015.
–718,2021. doi: 10.1214/20-BA1221.
Saharia,C.,Chan,W.,Saxena,S.,Li,L.,Whang,J.,Den-
Wang,Z.,Bovik,A.,Sheikh,H.,andSimoncelli,E. Image
ton,E.,Ghasemipour,S.K.S.,Gontijo-Lopes,R.,Ayan,
quality assessment: from error visibility to structural
B. K., Salimans, T., Ho, J., Fleet, D. J., and Norouzi,
similarity. IEEETransactionsonImageProcessing,13
M. PhotorealisticText-to-ImageDiffusionModelswith
(4):600–612,2004. doi: 10.1109/TIP.2003.819861.
DeepLanguageUnderstanding. InAdvancesinNeural
InformationProcessingSystems,2022. Xie, Y., Yuan, M., Dong, B., and Li, Q. Diffusion
ModelforGenerativeImageDenoising. arXivpreprint
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and
arXiv:2302.02398,2023.
Ganguli,S. DeepUnsupervisedLearningusingNonequi-
libriumThermodynamics. InProceedingsofthe32ndIn- Zhang, K., Zuo, W., Chen, Y., Meng, D., and Zhang, L.
ternationalConferenceonMachineLearning,volume37 BeyondaGaussiandenoiser: Residuallearningofdeep
ofProceedingsofMachineLearningResearch,pp.2256– CNNforimagedenoising. IEEETransactionsonImage
2265.PMLR,2015. Processing,26(7):3142–3155,2017.
Song, J., Vahdat, A., Mardani, M., and Kautz, J. Zhang, K., Li, Y., Zuo, W., Zhang, L., VanGool, L., and
Pseudoinverse-GuidedDiffusionModelsforInverseProb- Timofte,R. Plug-and-PlayImageRestorationWithDeep
lems. InInternationalConferenceonLearningRepresen- DenoiserPrior. IEEETransactionsonPatternAnalysis
tations,2023a. andMachineIntelligence,44(10):6360–6376,2022. doi:
10.1109/TPAMI.2021.3088914.
Song,Y.,Sohl-Dickstein,J.,Kingma,D.P.,Kumar,A.,Er-
mon,S.,andPoole,B.Score-BasedGenerativeModeling Zhu, Y., Zhang, K., Liang, J., Cao, J., Wen, B., Timofte,
through Stochastic Differential Equations. In Interna- R., and Gool, L. V. Denoising Diffusion Models for
tionalConferenceonLearningRepresentations,2021. Plug-and-PlayImageRestoration. InIEEEConference
11ListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
onComputerVisionandPatternRecognitionWorkshops
(NTIRE),2023.
12ListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
A.ProofofProp.2.1
Letf andgbetwopositiverealvaluedfunctionsoftimetontheinterval[0,1]. WeconsiderthefollowingSDE:
dz =f(t)z dt+g(t)dw , (10)
t t t
PropositionA.1. ForaforwardSDE(z) definedbyEq.(2),forallt [0,1],z reads
t∈[0,1] t
∈
z =a(t)x+b(t)ε′,withε′ (0,Σ ), (11)
t φ
∼N
witha:[0,1] R adecreasingfunctionwitha(0)=1,andb:[0,1] R anincreasingfunctionwithb(0)=0.5
+ +
→ →
Moreover,forf andgsuitedsothatσ b(1)/a(1),thereexistst∗ [0,1]suchthat
≤ ∈
y˜:=a(t∗)y =d z t∗. (12)
Proof: WewriteheretheproofwhenΣ istheidentitybuttheproofsholdsforanymatrix.
φ
Iff canbeintegrated,letF(t)beaprimitiveoff withvalue0attime0. Ifz isanItoprocessthensoisy =exp(F(t))z
t t t
andwehave:
dy =exp(F(t))dF(t)z +exp(F(t))dz
t t t
=exp(F(t))f(t)z ,dt+ exp(F(t))f(t)z dt+exp(F(t))g(t)dw
t t t
−
=exp(F(t))g(t)dw .
t
Therefore,integratingthisequationfrom0totyields:
(cid:90) t
y =y + exp(F(u))g(u)dw .
t 0 u
0
Andsincez =exp( F(t))y ,wehave:
t t
−
(cid:90) t
z =exp( F(t))y + exp(F(u) F(t))g(u)dw .
t 0 u
− −
0
UsingthefactthatF(0)=0,weobtain:
(cid:90) t
z =exp( F(t))z + exp(F(u) F(t))g(u)dw . (13)
t 0 u
− −
0
Finally,wehave:
(cid:90) t (cid:18) (cid:90) t (cid:19)
exp(F(u) F(t))g(u)dw 0,exp( 2F(t)) exp(2F(u))g(u)2du (14)
u
− ∼N −
0 0
(cid:18) (cid:90) t (cid:19)
z exp( F(t))z ,exp( 2F(t)) exp(2F(u))g(u)2du . (15)
t 0
∼N − −
0
Denotinga(t)=exp F(t)andb(t)2 =exp( 2F(t))(cid:82)t exp(2F(u))g(u)2du,wehave:
− − 0
z =a(t)x+b(t)ε′,withε′ (0,Σ ). (16)
t φ
∼N
Furthermore,becausef ispostive,F isincreasingandaisadecreasingfunctionoftimewitha(0)=1. Ifgisnottoosmall6
thenbisincreasing. Asaconsequence,theamountofnoiseinthemixturez isb(t)/a(t),itselfanincreasingfunctionof
t
timeand σ b(1)/a(1), t∗ [0,1],a(t∗)y z t∗.
∀ ≤ ∃ ∈ ∼
Thisiswhy,forreasonablefunctionsf andg,wecanidentityanymixtureoftheformy =z +σεwitharealisationofz
0 t
attimetforsometandinitialconditionz (thisisastoppedforwardprocess,uptoatime-dependentrescalingconstant).
0
Thecorrespondingtimetisoftenuniqueforcommonchoicesoff andg(bothVPSDE,VESDEandallOrnstein-Uhlenbeck
likeprocessesusedinDM).Whentractable,weimplementafunctiont(σ)thatyieldst∗correspondingtoσandotherwise,
theequationσ =b(t)/a(t)issolvedviaaniterativemethod.
5Expressionsofaandbasfunctionsoff andgareinApp.A.
6∀t,g(t)≥b(t)(cid:112)
2f(t)
13ListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
B.AdditionalDetailsonHMC
WegivedetailsonthepracticalimplementationoftheHMCsamplerusedfortheapplicationsofthispaper.
Integrator. Duringsampling,Hamilton’sequationsofmotionaresolvedusingaleapfrogintegrator. Moreover,toimprove
calibration of the HMC algorithm, for each HMC step, we solve Hamilton’s equations over a random number of time
steps drawn from a uniform distribution ( 5,...,15 ). Since both applications of this paper involve compact prior
U { }
distributionsp(ϕ),wealsohadtoimplementdomainconstraintsintheintegrator. Wetakeintoaccountthepriorboundaries
byimplementingelasticcollisionsintheleapfrogschemeasdescribedinBetancourt(2011).
InitializationHeuristic. Forthecosmologicalapplication,initializationoftheparametersϕoftheGibbssamplerwas
foundtobemorecriticalthaninSect.3.1. AsexplainedinSect.3.2,weintializeϕ=(φ,σ)usingaCNNwithResBlocks
trained to estimate the posterior mean E[ϕ y]. In practice, we sample φ,σ according to their prior distribution and
|
then sample y = x+ε with x p and ε (0,Σ ). The network is then trained to minimize the MSE loss
data ϕ
∼ ∼ N
E [ ϕˆ(y) ϕ 2],thusyieldingaposteriormeanestimator. Wefoundthatthismomentnetworkwaslessaccurateonthe
y
|| − ||
boundaryofour(compact)prior. Toavoidinitializingchainsoutsidetheprior,whichwouldgetstuck,allposteriormean
estimatesareprojectedwithinthepriordomain.
Warm-up. InthefirstiterationoftheGibbssampler(seeAlgo.1),wefirstfollowawarm-upphaseofP =300iterations
allowingtoadaptthestepsizeandestimateamassmatrix.Weadaptthestepsizeusingadualaveragingprocedure(Nesterov,
2009)parametrizedbyatargetacceptancerateof0.65,aregularizationscaleγ =0.05,aniterationoffsetT =10,anda
0
relaxationexponentκ=0.75. Theinverseofthemassmatrixisestimatedatiteration 3P/4 usingthe(unbiased)sample
⌊ ⌋
covariancematrixofthewarm-upsamplesafterhavingdiscardedsamplesfromiterationsupto P/4 .
⌊ ⌋
C.ModelsandTrainingDetails
C.1.DatasetsandTraining
Naturalimagesapplication. WetrainourdiffusionmodelontheImageNet2012dataset,whichconsistsof1,281,167
trainingimages. Imagesareresizedto3 256 256andcenteredcrop. Duringtraining,weaugmentthedatawithrandom
× ×
horizontalflipsandrescaletheimagestothe[0,1]range.
Wetrainover100epochsonanodeof8H100GPUs80GBwithabatchsizeperGPUof128images. Trainingtakesabout
41hoursusingDataParallelism. WeusetheAdamW(Loshchilov&Hutter,2019)optimizerandreportgoodstabilityof
trainingwithrespecttolearningrate(testedon0.0001,0.001,0.005),withnolearningrateschedule. Observingnoimpact
fromweightdecay,wesetitsvalueto0.
Cosmological application. We use the same data as in Heurtel-Depeiges et al. (2023). We construct simulated dust
emissionmapsintotalintensityfromaturbulenthydrodynamicsimulationofthediffuseinterstellarmediumtakenfrom
theCATSdatabase(Burkhartetal.,2020). Weassumeherethatdustemissionisproportionaltothegasdensityofthe
simulation. Simulatedmapsthensimplycorrespondtogascolumndensitymaps. Ourdatasetconsistsof991dustemission
mapsofsize256 256(foranexample,seeFig.5topleftpanel). Wewithheld10%ofthe991imagestovalidatethe
×
denoisingandinferencepipelinebasedonourdiffusionmodeltrainedontheother90%.
Ifweneglectsecondaryanisotropies,CMBanisotropiesareextremelywelldescribedbyanisotropicGaussianrandomfield
onthesphere(PlanckCollaborationI,2020),entirelycharacterizedbyitscovariancematrix(orpowerspectrum). Asa
consequence,wecanobtainCMBmapsbysamplingGaussiandrawswithcovarianceΣ parametrizedbycosmological
ϕ
parametersφ(foranexample,seeFig.5topmiddlepanel). CMBpowerspectraarecomputedusingCAMB(Lewisetal.,
2000),andGaussiandrawsonthesphereareprojectedon256 256patchesusingpixell7 withapixelsizeof8′. In
×
thiswork,weconsiderastandardcosmologicalmodel. WeonlyvarythecosmologicalparametersH andω andchoose
0 b
fortheremainingonesfiducialvaluesconsistentwithPlanck2018analysis(PlanckCollaborationI,2020).8 Theresulting
simulatedmapscoveraneffectiveangularsurfaceofapproximately34 34deg2.
×
TrainingexamplesaregeneratedbycombiningdustsamplesandCMBrealizationsforcosmologicalparametersrandomly
7https://github.com/simonsobs/pixell
8WechooseΩ =0,ω =0.12,τ =0.0544,n =0.9649,ln(1010A )=3.044,m =0.08.
K c s s ν
14ListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
drawnfromthepriorp(ϕ). SincethecomputationofasinglecovariancematrixΣ takesafewseconds,wetrainneural
ϕ
emulator to approximate ϕ Σ as described in the Appendix of Heurtel-Depeiges et al. (2023). This emulator is
ϕ
→
differentiable,thusenablingthedifferentiationofEq.(9)forHMC.
Wetrainover100,000epochsonasingleA10080GBusingabatchsizeof64images. Trainingtakesabout96hours.
WeusetheAdamWoptimizerwithaninversesquarerootschedulerwithwarm-up. Weobservedthatcontinuingtraining
afterthelosshadplateauedsignificantlyimprovedthediffusionmodel’sprecision,asevidencedbythesimulation-based
calibration(SBC)diagnostic.
C.2.Architectures
Naturalimagesapplication. OurdiffusionmodelisaU-Net,thattakesimagesofsize256x256asaninput. Itiscomposed
of5Downsamplinglayers,abottleneckofsize16x16and5Upsamplinglayers. Weadd2self-Attentionlayersbeforethe
bottleneckandoneafterthebottleneck. Thetotalnumberofparametersis 70,000,000.
∼
AsitisstandardinaU-Netarchitecture,eachUpsamplinglayerisconcatenatedwiththecorrespondingDownsampling
layer. AsinusoidaltimeembeddingisaddedtoeachDownConvandUpConv. A2layerneuralnetworkofinnerdimension
100andSiLUactivationsareusedtoembedtheexponentφofthecolorednoisetoaddthisinformationintothenetwork.
We choose a variance preserving DDPM with a discrete time composed of t = 5,000 time-steps with the schedule
r
β(t)= 0.1 + βmax−βmin.t,withβ =0.1andβ =20.
tr tr min max
Cosmologicalapplication. Forthecosmologicalapplication,ourscorenetworkisaUNetwithResBlocksandabottleneck
ofsize32 32or16 16. EachResBlockhasthreeconvolution,withGroupNormasnormalization,SiLUforactivation
× ×
andrescaledskip-connection. Longskipconnectionsareconcatenated,contrarytointernalskipconnectionswhoareadded.
Wedonotuseattention,duetothesmallsizeofthedataset. Sincebothourdataandnoisehaveperiodicboundaryconditions,
weuse‘circular’paddingintheconvolutions. Asin(Hoetal.,2020;Songetal.,2021),timeisalsoseenasaninput. We
usethecontinuoustimeframeworkfrom(Songetal.,2021)withFourierembedding.
InordertomanageaparameterizedfamilyofSDE(Eq.(2))wetrainadiffusionmodeltoreverseeachSDEindependently
byalsoprovidingφasanargumenttothemodel. Thedependencyonφismanagedsimilarlytothatofthetimetwiththe
exceptionthattheembeddingisalinearlayerwithactivationfunctionoftheparametersφ. IneachResBlock,thetime
embeddingandtheparametersembeddingarethentransformedbyasmallMLPwithadepthof1,priortobeingaddedto
theinputoftherespectiveResBlock.
Inaddition,forthecosmologicalcomponentseparationtaskonly,wetrainedamodelexclusivelyonlownoiselevelsby
reducingthevalueofβ to2,asopposedto20. Consequently,ournetworkislimitedtodenoisingimagesforσvalues
max
withinthespecifiedpriorrange. Thisadjustmentrestrictsthemodel’sutilityasagenerativepriorbutenhancesitsprecision
inreconstructingsummarystatisticsandcalibratingtheinferencemethod. Furthermore,wefoundthatatlowernoise-levels,
feedingthenetworkFourierfeatures(asinKingmaetal.,2021)improvedcalibrationoftheposteriordistribution.
C.3.LossFunctions
Ourmodelsaretrainedbyminimizingamodifieddenoisingscore-matchinglosswhoseoriginalformulationwouldbe:
(θ)=E
(cid:20)
λ(t)E
(cid:20)(cid:13)
(cid:13)s (z ,t)+Σ
−1(cid:16)
z
(cid:112)
α¯(t)z
(cid:17)
/(1
α¯(t))(cid:13) (cid:13)2(cid:21)(cid:21)
, (17)
L
t,ϕ z0,zt|(z0,ϕ) (cid:13) θ t ϕ t
−
0
−
(cid:13)
2
(cid:82)t
withα¯(t)=exp( β(u)du). AsinHeurtel-Depeigesetal.(2023),were-normalizeallvectorsinthislossbymultiplying
(cid:112) − 0
themby 1 α¯(t)Σ tostabilizethelearningprocessandreducenumericalerrors. Consequently,theadjustedloss
ϕ
− −
functionread:
 (cid:13) (cid:13)2
 (cid:13) 1 (cid:16) (cid:112) (cid:17)(cid:13) 
L(θ)=E t,ϕ λ(t)E z0,zt|(z0,ϕ)(cid:13) (cid:13) (cid:13)m θ(z t,t,ϕ)
−
(cid:112)
1 α¯(t)
z t
−
α¯(t)z 0 (cid:13) (cid:13)
(cid:13)


(18)
− 2
ThislossinturnscorrespondstothestandardlossfromHoetal.(2020)whereonetriestoestimatetheaddednoise(uptoa
rescalingconstant).
Inorder toverifythatthe modelis notoverfitting, wecheckfor modecollapse anddataset copyingwith aspecific L
2
15ListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
distancetakingintoaccounttheperiodicboundaryconditions. Thiswasparticularlyimportantasourmodelsseemmore
pronetooverfittingthanmodeltrainedunderwhitenoise.
D.PropertiesoftheGibbssampler
Inthissection,wederivetheoreticalpropertiesoftheproposedGibbssampler. Specifically,weworkoutthesampler’s
stationarydistribution,reviewconditionsforitsexistence,andquantifyitserror.
Inwhatfollows,alldistributionsareconditionedony,andsowedropthisdependencytoalleviatethenotation.
D.1.Formalnotation
AnyMarkovchainMonteCarlo(MCMC)samplerdefinesatransitionkernelbetweenacurrentstate(ϕ,x)andanewstate
(ϕ′,x′). WedenotethistransitionT((ϕ,x) (ϕ′,x′)). Theinvariantdistributionπisthedistributionthatverifies
→
(cid:90)
T((ϕ,x) (ϕ′,x′))π(ϕ,x)dϕdx=π(ϕ′,x′). (19)
→
Inwords,ifweapplythetransitionkernelT toastate(ϕ,x)drawnfromπ,thenthenewstate(ϕ′,x′)isalsodistributed
accordingtoπ.
ThetransitionkernelT oftheGibbssamplerdecomposesintotwokernelsT = T T . ThefirsttransitionkernelT
ϕ x ϕ
◦
updatesϕbutmaintainsxfixed,whileT updatesxandleavesϕunchanged.
x
T drawsϕfromanMCMCsamplerthatadmitsp(ϕ x,y)asitsstationarydistribution. Inourpaper,weuseHamiltonian
ϕ
|
MonteCarlobuttheresultswederiveherewouldholdforanyMCMCalgorithmwiththerightstationarydistribution. The
MCMCsamplerdefinesatransitiondensityτ(ϕ ϕ′ x),andmoreover
→ |
T =τ(ϕ ϕ′ x)δ(x′ =x), (20)
ϕ
→ |
whereδisthediracdeltafunction. τ maycorrespondtothetransitionobtainedafterapplyingeitherasingleormultiple
iterationsoftheMCMCsampler.
Next,T drawssamplesfromadiffusionmodelthatapproximatesthedatageneratingprocessp(x ϕ,y). Itwhatfollows,
x
|
weassumethatthedistributionofthesamplesproducedbythediffusionmodeladmitsaprobabilitydensityq(x ϕ,y).
|
Then
T =q(x′ ϕ)δ(ϕ′ =ϕ). (21)
x
|
D.2.ProofofProposition2.2: invariantdistributions
Proof. Letπ(ϕ,x)beanyjointdistribution,suchthatπ(ϕ x)=p(ϕ x). Then
| |
(cid:90) (cid:90)
T ((ϕ,x) (ϕ′,x′))π(ϕ,x)dϕdx = τ(ϕ ϕ′ x)δ(x′ =x)π(ϕ,x)dϕdx
ϕ
→ → |
(cid:90)
= τ(ϕ ϕ′ x′)p(ϕ x′)π(x′)dϕ
→ | |
(cid:90)
= π(x′) τ(ϕ ϕ′ x′)p(ϕ x′)dϕ
→ | |
= π(x′)p(ϕ x′)
|
= π(ϕ′,x′), (22)
whereforthepenultimatelineweusedtheassumptionthatτ’sstationarydistributionisp(ϕ x). From19,wethenhave
|
thatπ(ϕ,x)isaninvariantdistributionofT .
ϕ
16ListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
Similarlyletπ˜(ϕ,x)beanyjointdistributionsuchthatπ˜(x ϕ)=q(x ϕ). Then
| |
(cid:90) (cid:90)
T ((ϕ,x) (ϕ′,x′))π˜(ϕ,x)dϕdx = q(x′ ϕ)δ(ϕ′ =ϕ)π˜(ϕ,x)dϕdx
x
→ |
(cid:90)
= q(x′ ϕ′) π˜(ϕ′,x)dx
|
= q(x′ ϕ′)π˜(ϕ′)
|
= π˜(ϕ′,x′). (23)
Henceπ˜(ϕ,x)isinvariantunderT .
x
D.3.ExistenceofStationaryDistribution
DefinitionD.1. Considertwofamiliesofconditionaldistributions,f(ϕ x)andg(ϕ x). Wesayf andgarecompatibleif
| |
thereexistsajointdistributionπ(ϕ,x)suchthatπ(ϕ x)=f(ϕ x)andπ(x ϕ)=g(x ϕ). Else,wesaythatf andg
| | | |
areincompatible.
Moregeneraldefinitionsofcompatibilitycanbefoundinreferences(Arnoldetal.,2001;Liuetal.,2012). Sufficientand
necessaryconditionsforcompatibilityareprovidedbyArnoldetal.(2001,Theorem1),restatedhereforconvenience(and
adaptedtoournotation).
TheoremD.2. f(ϕ x)andg(x ϕ)arecompatibleifandonlyif
| |
1. (ϕ,x):f(ϕ x)>0 = (ϕ,x):g(x ϕ)>0 :=S,
{ | } { | }
2. Thereexistfunctionsu(x)andv(ϕ)suchthatforevery(ϕ,x) S,
∈
g(x ϕ)
| =u(x)v(ϕ), (24)
f(ϕ x)
|
withu(x)integrable,thatis
(cid:90)
u(x)dx< . (25)
∞
X
Wefinditenlighteningtorewritethelastconditionasanintegralovertheleft-hand-sideofEquation(25). Applyingthis
conditiontoourproblemwehave
(cid:90)
q(x ϕ,y)
| dx< ,
p(ϕ x,y) ∞
X |
whichisEq.(7).
D.4.ErrorintheStationaryDistribution
Moving forward, we assume that p(ϕ x) and q(x ϕ) are compatible, and so our Gibbs sampler admits a stationary
| |
distributionπ(x,ϕ). WenowproveTheorem2.3,whichtellsustheKL-divergencefromthetrue(posterior)distribution
p(ϕ)tothemarginalstationarydistributionπ(ϕ)ofourGibbssampler.
Proof. Standardapplicationoftheprobabilitychainrulegives
p(x ϕ)
p(x)=p(ϕ) | . (26)
p(ϕ x)
|
Clearly,p(ϕ)mustactasanormalizingconstantandso
1
p(ϕ)= . (27)
(cid:82) p(x|ϕ)dx
X p(ϕ|x)
17ListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
Recallingthatπ(ϕ x)=p(ϕ x),asimilarargumentyields
| |
1
π(ϕ)= . (28)
(cid:82) q(x|ϕ)dx
X p(ϕ|x)
Then
(cid:90)
p(x ϕ) q(x ϕ)
1 1 = | − | dx
p(ϕ) − π(ϕ) p(ϕ x)
X |
(cid:90)
p(x ϕ) q(x ϕ)
π(ϕ)−p(ϕ) = | − | dx
⇐⇒ p(ϕ)π(ϕ) p(ϕ x)
X |
(cid:90)
p(x)
π(ϕ)−p(ϕ) = (p(x ϕ) q(x ϕ)) dx
⇐⇒ π(ϕ) | − | p(x ϕ)
X |
(cid:90)
q(x ϕ)
1 p(ϕ) =1 | p(x)dx
⇐⇒ − π(ϕ) − p(x ϕ)
X |
(cid:90)
q(x ϕ)
p(ϕ) = | p(x)dx. (29)
⇐⇒ π(ϕ) p(x ϕ)
X |
Finallyrecallthat
(cid:90)
p(ϕ)
KL(p(ϕ) π(ϕ))= log p(ϕ)dϕ, (30)
|| π(ϕ)
Φ
andsotakingonbothsidesofEq.(29)thelogarithmandtheexpectationvaluewithrespecttop(ϕ),weobtainthewanted
result.
Remark. Eq.(29),whichappliestothedensityratioatanypoint,providesamoregeneralresultthantheresultonaveraged
densityratiosstatedinTheorem2.3.
E.AdditionalResults
φ=−1→Pinknoise φ=0→Whitenoise φ=1→Bluenoise
Dataset NoiseLevel GDiff GDiff GDiff
σ BM3D DnCNN GDiff xˆ E[x|y] BM3D DnCNN GDiff xˆ E[x|y] BM3D DnCNN GDiff xˆ E[x|y]
0.06 0.90±0.01 0.88±0.00 0.86±0.01 0.92±0.00 0.92±0.00 0.92±0.00 0.88±0.01 0.93±0.00 0.94±0.00 0.92±0.00 0.90±0.00 0.95±0.00
ImageNet 0.1 0.81±0.01 0.76±0.01 0.77±0.01 0.86±0.01 0.90±0.01 0.90±0.01 0.84±0.01 0.91±0.00 0.90±0.01 0.88±0.01 0.85±0.01 0.92±0.00
0.2 0.63±0.01 0.53±0.01 0.62±0.02 0.74±0.02 0.79±0.01 0.80±0.01 0.74±0.01 0.83±0.01 0.83±0.01 0.79±0.01 0.78±0.01 0.87±0.01
0.06 0.90±0.01 0.88±0.00 0.84±0.01 0.92±0.00 0.93±0.00 0.94±0.00 0.88±0.00 0.94±0.00 0.94±0.00 0.94±0.00 0.90±0.00 0.95±0.00
CBSD68 0.1 0.82±0.01 0.78±0.01 0.75±0.01 0.85±0.01 0.89±0.00 0.90±0.00 0.81±0.01 0.90±0.00 0.91±0.00 0.90±0.00 0.86±0.00 0.93±0.00
0.2 0.65±0.01 0.55±0.01 0.61±0.01 0.74±0.01 0.79±0.01 0.80±0.01 0.69±0.01 0.81±0.01 0.83±0.01 0.79±0.01 0.76±0.01 0.86±0.01
Table2.ImagequalityintermsofSSIM(↑)afterdenoisingwithGDiff(blind)andbaselinesBM3D(non-blind)andDnCNN(blind).We
reportmeanPSNRandstandarderrorcomputedonbatchesof50images.ForGDiff,weprovideperformanceforbothposteriorsamples
xandestimatesoftheposteriormeanE[x|y].
18ListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
Noisyy[14.03dB] Truex Denoisedxˆ[21.96dB] DenoisedE[x |y][24.81dB] R Eˆ S= S=1.3 61 7
Rˆ=1.21
0.21 ESS=82
0.20
0.19
0.18
0.425 0.400 0.375 0.350.018 0.19 0.20 0.21
− − − −
ϕ σ
Noisyy[14.02dB] Truex Denoisedxˆ[28.88dB] DenoisedE[x |y][31.91dB] R Eˆ S= S=1.1 10 24
Rˆ=1.11
0.21 ESS=125
0.20
0.19
0.18
0.225 0.200 0.175 0.150.018 0.19 0.20 0.21
− − − −
ϕ σ
Noisyy[14.04dB] Truex Denoisedxˆ[27.70dB] DenoisedE[x |y][30.78dB] R Eˆ S= S=1.0 19 33
Rˆ=1.06
0.21 ESS=200
0.20
0.19
0.18
0.025 0.000 0.025 0.0500.18 0.19 0.20 0.21
−
ϕ σ
Noisyy[14.04dB] Truex Denoisedxˆ[29.35dB] DenoisedE[x |y][32.27dB] R Eˆ S= S=1.1 11 09
Rˆ=1.06
0.21 ESS=197
0.20
0.19
0.18
0.175 0.200 0.225 0.2500.18 0.19 0.20 0.21
ϕ σ
Noisyy[14.05dB] Truex Denoisedxˆ[30.18dB] DenoisedE[x |y][33.52dB] R Eˆ S= S=1.1 77 9
Rˆ=1.10
0.21 ESS=122
0.20
0.19
0.18
0.375 0.400 0.425 0.4500.18 0.19 0.20 0.21
ϕ σ
FigureE.1.ExamplesofblinddenoisingwithGDiffonnoisyImageNetmockswithσ=0.2andφ∈{−0.4,−0.2,0.0,0.2,0.4}(from
toptobottom).Left:Noisyexamplesynexttothenoise-freeimagesx,denoisedsamplesxˆandestimatesoftheposteriormeanE[x|y].
Right:Inferredposteriordistributionsoverthenoiseparameters.
19
σ
σ
σ
σ
σListeningtotheNoise:BlindDenoisingwithGibbsDiffusion
ESSforϕ ESSforσ Rˆforϕ Rˆforσ
0.8 0.8
0.7 300 0.7
1.6
0.6 250 0.6
0.5 200 0.5
1.4
0.4 150 0.4
0.3 100 0.3 1.2
0.2 0.2
50
0.1 0.1 1.0
1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0 1.0 0.5 0.0 0.5 1.0
− − ϕ − − ϕ − − ϕ − − ϕ
FigureE.2. ESS(left)andRˆ(right)statisticsforinferencesonnoisynaturalimagesacrosstheparameterspace.
ESSforH0 ESSforωb ESSforσ
1.2
0.05 0.05 400
1.0
0.04 0.04 0.8 300
0.03 0.03 0.6 200
0.4
0.02 0.02
100
0.2
0.01 0.01
60 70 80 60 70 80 60 70 80
H0 H0 H0
RˆforH0 Rˆforωb Rˆforσ
1.2
2.6
0.05 0.05 1.0 2.4
2.2
0.04 0.04 0.8
2.0
0.03 0.03 0.6 1.8
1.6
0.4
0.02 0.02 1.4
0.2 1.2
0.01 0.01
1.0
60 70 80 60 70 80 60 70 80
H0 H0 H0
FigureE.3. ESS(top)andRˆ(bottom)statisticsforinferencesondust/CMBmixturesacrosstheparameterspace.
20
σ
bω
bω
bω
bω
σ
σ
σ