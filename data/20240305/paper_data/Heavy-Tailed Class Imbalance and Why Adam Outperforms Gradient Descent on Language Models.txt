Heavy-Tailed Class Imbalance
and Why Adam Outperforms Gradient Descent on Language Models
FrederikKunstner1 RobinYadav1 AlanMilligan1 MarkSchmidt12 AlbertoBietti3
Abstract difficult to develop optimizers that consistently improve
overAdam(see,e.g.,Schmidtetal.,2021,Table2).Thisis
Adamhasbeenshowntooutperformgradientde-
perhapsnotsurprisingaswelackacleardefinitionofwhat
scentinoptimizinglargelanguagetransformers
specificproblemwithSGDissolvedbyAdam,havelittle
empirically,andbyalargermarginthanonother
theoretical guidance, and trying new optimizers on large
tasks,butitisunclearwhythishappens.Weshow
scalemodelscomeswithlargecosts.Improvingourunder-
thattheheavy-tailedclassimbalancefoundinlan-
standingofthisperformancegapcouldenableustodevelop
guagemodelingtasksleadstodifficultiesinthe
methodsthatfurtherimproveonitandreducetrainingcosts.
optimizationdynamics.Whentrainingwithgra-
dientdescent,thelossassociatedwithinfrequent GiventhatAdamoutperformsSGDbyalargermarginon
wordsdecreasesslowerthanthelossassociated languagetransformersasopposedtoothertasks,manypa-
withfrequentones.Asmostsamplescomefrom pershavelookedintoalternativemetricstojustifyitsperfor-
relativelyinfrequentwords,theaveragelossde- manceinthissetting.Liuetal.(2020)showevidenceofvan-
creasesslowlywithgradientdescent.Ontheother ishinggradientsintransformerarchitecturesandattribute
hand,Adamandsign-basedmethodsdonotsuf- the benefit of Adam to more uniform parameter updates
fer from this problem and improve predictions despite different gradient magnitudes. Pan and Li (2023)
on all classes. To establish that this behavior is showthatthesharpnessinthedirectionusedbyAdamis
indeedcausedbyclassimbalance,weshowempir- smallerthanforSGD,allowingformoreprogressperiter-
icallythatitpersistthroughdifferentarchitectures ation.Jiangetal.(2022)showthatarobustvariantofthe
anddatatypes,onlanguagetransformers,vision conditionnumberissmalleroverthepathtakenbyAdam.
CNNs,andlinearmodels.Wefurtherstudythis But despite these measurable improvements, it has been
phenomenononalinearclassificationwithcross- difficulttopin-pointapropertyoftheproblemthatleadsto
entropyloss,showingthatheavy-tailedclassim- theperformancegapbetweenGDandAdam.
balanceleadstoill-conditioning,andthatthenor-
From a theoretical perspective, a line of work has devel-
malizationusedbyAdamcancounteractit.
opedalternativeassumptionstobettercapturetheobserved
behavioronlanguagetransformers,attributingthebenefit
ofAdamtoeithernoiseorcurvature.Zhangetal.(2020b)
1.Introduction showedthatthenoiseinthestochasticgradientshasheavier
tailsinlanguagetransformersthaninvisiontasks,andargue
The recent success of large language models such as
thatthebenefitofAdamliesinanimprovedresilienceto
GPT-3(Brownetal.,2020)anditssuccessorshasreliedon
heavy-tailednoise.However,noisedoesnotappeartobe
verycostlytrainingproceduresatunprecedentedscale.A
the root cause of the performance gap, as Kunstner et al.
keyingredientintheselargetrainingrunsistheuseofthe
(2023)provideevidencethatthegapdoesnotdecreasein
Adamoptimizer(KingmaandBa,2015),whichempirically
deterministictraining.Oncurvature,Zhangetal.(2020a)
outperformsstochasticgradientdescent(SGD)ontheselan-
observethatthemagnitudeofthegradientandHessianare
guagemodelingproblems.Despitethislargeperformance
correlatedthroughouttraining.Tocapturethisrelationship,
gap,wehaveapoorunderstandingofbothwhyAdamworks
theyproposearelaxationofthestandardassumptionthat
betterandwhySGDperformspoorly.
theHessianisboundedbyaconstant,toallowittogrow
Despitemuchworkonalgorithmicdevelopment,ithasbeen withthegradientnorm.Thisassumptionjustifiesclipping-
likemethods,andwasgeneralizedelement-wisetojustify
1Department of Computer Science, University of British
Adamandsign-basedmethodsbyCrawshawetal.(2022).
Columbia 2Canada CIFAR AI Chair (Amii) 3Flatiron Institute.
However,weonlyhavealimitedunderstandingastowhy
Correspondenceto:FrederikKunstner<kunstner@cs.ubc.ca>.
thiswouldoccurindeepnetworks.Forexample,Orvieto
1
4202
beF
92
]GL.sc[
1v94491.2042:viXraHeavy-tailedclassimbalance
(a) # samples/class (b) Overall loss (c) SGD (+m) (d) Adam (+m)
106
10 10 10
103
5 5 5
100
100 102 104 0 5000 10000 15000 0 5000 10000 15000 0 5000 10000 15000
Class index (sorted) Step Step Step
SSGGDD ((wwiitthh mmoommeennttuumm)) 1100%% ssaammpplleess,, lleeaasstt ffrreeqq.. ccllaasssseess
AAddaamm ((wwiitthh mmoommeennttuumm)) 1100%% ssaammpplleess,, mmoosstt ffrreeqq.. ccllaasssseess
Figure1.Gradientdescentdoesnotmakeprogressonlow-frequencyclasses,whileAdamdoes.TrainingaGPT2-Smalltransformer
onWikiText-103withSGDandAdam.(a)Distributionoftheclassesandsubsetsofthedatasortedbyclassfrequency,eachcorresponding
to≈10%ofthesamples.(b)Overalltrainingloss.(c,d)TraininglossforeachsubsetusingSGDandAdam.SGDmakeslittletono
progressonlow-frequencyclasseswhileAdammakesprogressonallsubsets.(b)istheaverageof(c,d)fortherespectiveoptimizer.
etal.(2022)showthatthevanishinggradientproblemisac- Weestablishexperimentallythatheavy-tailedclassim-
companiedbyvanishingHessians,providinganexplanation balanceleadstoanincreasedperformancegapbetween
for why gradients and Hessian would become correlated GDandAdam,beyondlanguagetransformers.
withdepth.Butthismechanismdoesnotexplainwhythis
WeshowinSection2thatthebehaviorobservedinFigure1
correlationwouldappearmoreonlanguagetransformersas
canbereproducedwithsmallertransformers,deterministic
opposedto,forexample,visionmodels.
training algorithms, vision models, and even linear mod-
elsonsyntheticdata.Whileimbalanceindeeplearningis
1.1.Contributions
mostoftenstudiedinthecontextofrepresentationlearning
We isolate a feature of language tasks that explain why orforgeneralizationperformance(e.g., Zhuetal.,2014;
Adamoutperforms(S)GDbyalargermarginthanonother Huangetal.,2016),thediscussionofitsconsequencesfor
problems:heavy-tailedclassimbalance. optimizationhavebeenlimited.Ourresultshighlightthat
heavy-tailedclassimbalance,mostsalientintherecentde-
Languagedataisimbalanced,inthesensethatsomewords
velopmentoflargelanguagemodels,islikelyasignificant
aremuchmorefrequentthanothers,typicallyfollowinga
factorinthelargeperformancegapbetweenGDandAdam.
power-law.AcommonmodelingassumptionistheZipfdis-
tribution,inwhichthekthmostfrequentwordhasfrequency Weshowthat,onalinearmodel,thescaleofthegradient
∝1/k(Piantadosi,2014).Forlanguagemodeling,whichis andHessianreflecttherelativeclassfrequencies.This
framedasnext-tokenclassification,thispropertyisreflected explainsthepoorperformanceofGDbyshowingthatheavy-
inthetokensandleadstoheavy-tailedclassimbalance. tailedclassimbalanceleadstoill-conditioning,andGDhas
vastlydifferentconvergencespeedacrossclasses.
Our main empirical finding is that SGD makes slow
progressonlow-frequencyclasses,asshowninFigure1. BecausethegradientandtheHessianarerelatedthrough
Thelossofsamplesfromlow-frequencyclassesgoesdown classfrequencies,thebenefitofAdammaybeattributedto
slowerthanthoseofhigh-frequencyclasseswhentrained preconditioning.Indeed,thisobservationgivescredenceto
withSGD,whileAdamislessaffectedbytherelativeclass the idea that using the gradient magnitude as a precondi-
frequenciesandclassesarelearnedatamoreuniformspeed. tionercanapproximatesecond-orderinformation,bypro-
vidingasimplelinearmodelexhibitingthisbehavior.
Whiletheobservationthatlow-frequencyclassesconverge
slowerwithSGDlikelyappliestoanyproblemwithclass
2.Experimentalresultsandablationstudies
imbalance,thiswouldonlyleadtoslowtrainingonsome
classes.Thekeypropertyofheavy-tailedclassimbalance
TheresultsinFigure1indicateacorrelationbetweenrel-
thatleadstoslowoveralltrainingisthatamajorityofthe
ativeclassfrequenciesandoptimizationperformancethat
samplescomefromclasseswithalowrelativefrequency,
impacts optimization algorithms differently. The goal of
andaccountforanon-negligiblefractionofthetotalloss.1
this section is to establish that class imbalance is a root
1Forexample,onabinaryclassificationproblemwithanimbal- causefortheperformancegapbetweenSGDandAdam,as
anceratioof100:1,GDmightconvergenceslowlyandgeneralize opposedtothisbeingduetoconfoundingwithotherproper-
poorlyonthelow-frequencyclass,buttheimpactontheaverage ties,includingstochasticityintheoptimizationprocedure,
losswouldbesmallasitonlyaccountsfor1%ofthetotalloss. otherpropertiesofthedata,orarchitecturaldifferences.We
donotclaimthatclassimbalanceistheonlyreasonAdam
2
selpmas
#
ssoL
niarTHeavy-tailedclassimbalance
(a) CNN on MNIST (b) Overall loss (c) GD (+m) (d) Adam (+m)
10 10 10 10
5 5 5 5
0 0 0 0
0 100 200 0 100 200 300 0 100 200 300 0 100 200 300
Epochs Epoch Epoch Epoch
GGDD ((wwiitthh mmoommeennttuumm)) 5500%% ssaammpplleess,, lleeaasstt ffrreeqq.. ccllaasssseess
AAddaamm ((wwiitthh mmoommeennttuumm)) 5500%% ssaammpplleess,, mmoosstt ffrreeqq.. ccllaasssseess
Figure2.ClassimbalanceimpactstheoptimizationdynamicsonCNNsonimagedata.(a)PerformanceontheMNISTdataset.(b)
PerformanceonamodifiedMNISTwithtwogroupsofclasses.Thefirstgroupconsistsofthe10originalclasseswith≈5ksampleseach,
whilethesecondconsistsof≈10kaddedclasseswith5exampleseach.(c,d)PerformanceofGDandAdamonthetwogroups.
outperformsSGD,butthat,underclassimbalance,Adam wefirstconfirmthatasimilarqualitativebehaviorappears
consistentlyoutperformsSGD. with smaller models and datasets. We observe that with
eitherstochasticandfullbatchoptimization,GDisslowerat
Focusontrainingperformance.Ourmainfocusisonwhat
minimizingthelossfromlow-frequencyclassesthanAdam.
makesoptimizationdifficult,andexperimentsfocusonprop-
ResultsonsmallertransformersaredeferredtoAppendixB.
ertiesofthedynamicsofthetrainingloss.Ourobservations
We use deterministic updates (marked as GD instead of
need not generalize to the validation loss, especially for
SGD)inthenextexperiments,showninFigures2to6.
models/datasetspairsthatoverfit,thoughwenotethattrain-
ingdynamicsontheempiricalandpopulationlossareoften
2.2.Otherarchitectureanddatatypes
similar, particularly early in training (see, e.g., Nakkiran
etal.,2021;Ghoshetal.,2022). Ourhypothesisisthatthelargerperformancegapbetween
GDandAdamobservedonlanguagetransformers,isdue
Experimentaldetails.Foreachsetting,weuseaconstant
toheavy-tailedclassimbalance,asopposedtootherprop-
step-sizetunedbygridsearchforbesttrainingperformance.
erties of the problem, such as properties of the architec-
ModelsanddatasetsaredescribedinAppendixA.
ture,languagedata,orinteractionsbetweenthoseattributes.
Splittingthelossperfrequency.Tolookattheeffectofim- Toshowthatheavy-tailedclassimbalanceleadstoalarge
balanceontraining,wesplitthedataintogroupscontaining performance gap across architectures and data types, we
asimilarfractionofthedataandplotthelossforeachgroup reproducetheobservedoptimizationdynamicswithacon-
separately.E.g.,for10groups,thefirstgroupcorresponds volutionalneuralnetworks(CNN)ontheMNISTdataset,
to≈10%ofthesamplescorrespondingtothemostfrequent augmentedtofeatureheavy-tailedclassimbalance.
classes.Theoveralllossistheaverageofthelossesacross
Inadditiontotheoriginal10classeswith≈5ksamples/class,
groups,butitletsustracktheevolutionofthelossonlow-
we add ≈10k new classes with 5 samples/class. The new
vs.high-frequencyclassesseparately.Thegroupsareonly
imagesarecopiesofexistingoneswitha“barcode”,abinary
usedforvisualization,thetrainingprocedureisunchanged.
codeinacorneroftheimage(seeAppendixA).Thenew
labelscombinethedigitandbarcode.Thisgivestwogroups
2.1.Stochasticity
of classes with a relative frequency difference of 1000×,
A natural hypothesis to explain the impact of imbalance whilehalfoftheexamplesarefromlow-frequencyclasses,
ontrainingisthatduetothesubsamplingusedtocompute
WetrainaCNNontheoriginalMNISTandthisimbalanced
gradients,low-frequencyclassesaresampledlessoftenand
MNIST,showninFigure2.GDandAdamcanbothdrive
thus learned more slowly. While such an effect may be
thetraininglossdownonMNIST.Butontheimbalanced
present,webeginbyshowingthattrainingwithmini-batch
variant,GDmakesalmostnoprogressonhalfofthedata
stochastic gradients or full-batch deterministic gradients
correspondingtothelow-frequencyclasses,andtheoverall
displaysimilargapsintrainingperformancebetweenGD
lossstalls.Adammakesprogressonbothgroups.
andAdam.Thisallowsustofocusondeterministicupdates,
removingstochasticityasapotentialconfounder.
2.3.Linearmodels
However, training a large transformer on a large dataset
Tohighlightthatheavy-tailedclassimbalancealonecanlead
asinthesettingofFigure1withfullbatchupdatesisnot
totheobserveddifficulties,wereproducethisbehaviorona
computationallyfeasible.Tomakethiscomparisonfeasible,
linearmodel.Wecreateaclassificationproblemwherethe
3
ssoL
niarT
ssoL
niarTHeavy-tailedclassimbalance
(a) # samples/class (b) Overall loss (c) GD (+m) (d) Adam (+m)
104
10 10 10
102
5 5 5
100
0 0 0
100 102 104 0 50 100 150 0 50 100 150 0 50 100 150
Class index (sorted) Epoch Epoch Epoch
GGDD ((wwiitthh mmoommeennttuumm)) 99%% ssaammpplleess,, lleeaasstt ffrreeqq.. ccllaasssseess
AAddaamm ((wwiitthh mmoommeennttuumm)) 99%% ssaammpplleess,, mmoosstt ffrreeqq.. ccllaasssseess
Figure3.TheclassimbalancedynamicsofFigure1arereproduciblewithlinearmodels.Softmaxregressiononsyntheticdata.The
inputsaredrawnfromauniformdistributonon[0,1]d.Thetargetclassesareheavy-tailed(a)andindependentoftheinputs,butthemodel
canstillfitthedataasitisoverparameterized.(b,c,d)OveralltraininglossanderformanceofGDandAdamoneachsubset.
relativefrequencyoftheclassesapproximatesapower-law, layersatinitialization,inFigure4.Weobservethatnormal-
as shown in Figure 3. The inputs are drawn from a high izationalonedoesnotimproveGD.Signdescentbehaves
dimensional uniform distribution on [0,1], independently similarlytoAdamandisabletofitlow-frequencyclasses.
ofthelabel.Whilethereisnorelationshiptolearn,alinear Momentumimprovesperformanceoverallbuthaslessim-
modelcanstillseparatethedataifthedimensionislarge pactonthedifferenceacrossclassfrequenciesthanchanging
enough.AsonthetransformerofFigure1,fittingasoftmax theupdatedirection.Weobservesimilarbehavioronother
regression(withabiasterm)withGDleadstolessprogress modelsanddatasets,showninAppendixD.
on low-frequency classes, while the progress of Adam is
moreuniformacrossclassesshowninFigure3. 2.5.Discussion
Thisexampleillustratesthataproblemthatmightlookin- Themechanismweidentifyhereisrepeatableoverdifferent
nocuousatfirstishardtooptimizewithGDduetoheavy- architecturesanddatatypes,andislikelytobeadifferen-
tailedclassimbalance,whiletheperformanceofAdamis tiatingfeatureofthetrainingdifficultiesencounteredwith
notnegativelyimpacted.Weobservesimilarresultswhen languagetransformersasopposedtootherproblems.How-
training only the last layer of a transformer, freezing the ever,classimbalanceislikelynottheonlytrainingdifficulty.
otherlayersatinitialization,showninFigure4.
Otherdifficultiesassociatedwithtextdatamightoccurin
However,imbalancealoneisnotsufficienttomakeGDslow languagetransformers.Weonlylookattheeffectofthenext
onlow-frequencyclasses.Itispossibletogenerateapatho- tokentobepredicted.Theinputsoflanguagemodels,i.e.,
logicaldatasetwithheavy-tailedclassimbalancewhereGD thesequenceoftokensindexingintotheembeddinglayer,
fitsallclassesfast,bymakingtheinputs(closeto)orthog- arealsoheavy-tailed.Thisimbalancemightleadtosimilar
onal.Inthiscase,eachsampleislearnedindependentlyof difficulties,whereembeddingweightsforraretokenscould
theothers,andthereisnodifferenceacrossclasses.Wegive beupdatedmoreslowlywithGD,givinganotherpotential
examplesandmoreinformationonthebehaviorofGDon causeforaperformancegap.Fullsentences(Williamsetal.,
thelinearmodelinAppendixE. 2015)andlatentrulesormechanismsrequiredtounderstand
agivenparagraph(Michaudetal.,2023)mayalsodisplay
2.4.Interactionsbetweenoptimizerandimbalance heavytails,andifthesearecapturedbyintermediatelayers
ofatransformer(e.g.,Mengetal.,2022;Wangetal.,2022;
Weexperimentwithsimpleroptimizerstoidentifywhich
Biettietal.,2023),Adammaybebeneficialthereaswell.
componentofAdammakesitlesssensitivetoclassimbal-
ance.Forexample,variantsofnormalizedGDcanperform Difficulties associated with depth and signal propaga-
betteronseparablelogisticregression(Nacsonetal.,2019) tion(Nocietal.,2022;Heetal.,2023),vanishinggradients,
bychangingthemagnitudeoftheupdate,whilethebenefits andhigherorderderivatives(Liuetal.,2020;Orvietoetal.,
ofAdamhavebeenattributedtosimilaritiestosigndescent 2022) likely also play a significant role. Even the simpli-
(e.g. Balles and Hennig, 2018), by changing the update fiedlineartransformermodelsofAhnetal.(2023)exhibit
direction.FollowingKunstneretal.(2023),weincludenor- manyofthetrainingdifficultiesobservedintheliterature.
malizedGDandsigndescent,anduseeachoptimizerwith Heavy-tailedclassimbalancedoesnotplayaroleinthose
andwithoutmomentum(withfixedβ =0.9orβ =0.9). experiments as they solve a regression problem. This in-
1
dicatesthatotherpropertiesoftransformersmighthavea
Wepresentresultsfortrainingthelastlayerofaone-hidden-
largeinfluenceonoptimizationperformance.
layer transformer, freezing the embedding and attention
4
selpmas
#
ssoL
niarTHeavy-tailedclassimbalance
Overall loss GD ( m) Adam ( m) NormGD ( m) Sign ( m)
10 10 10 10 10
0 0 0 0 0
0 500 0 500 0 500 0 500 0 500
GD (+m) Adam (+m) NormGD (+m) Sign (+m)
10 10 10 10 10
0 0 0 0 0
0 500 0 500 0 500 0 500 0 500
Epoch Epoch Epoch Epoch Epoch
GGDD (( mm)) AAddaamm (( mm)) NNoorrmmGGDD (( mm)) SSiiggnn (( mm))
GGDD ((++mm)) AAddaamm ((++mm)) NNoorrmmGGDD ((++mm)) SSiiggnn ((++mm))
Figure4.Signdescent,asAdam,fitslow-frequencyclasses.Trainingthelastlayerofasimplifiedone-layertransformerwithGD,
Adam,normalizedGD,andsigndescent,withandwithoutmomentum(+m,bottom/−m,top).Signdescentrecoverssimilardynamicsto
Adamwhilemomentumornormalizingthemagnitudehassmallereffects.
3.Aninvestigationonlinearmodels Althoughsoftmaxclassificationisnotquadratic,asimilar
behaviorappearsresponsiblefortheslowconvergenceof
Theoptimizationdifficultiesassociatedwithheavy-tailed
GD. In particular, we provide evidence that for a linear
class imbalance already appear on softmax classification
model,theHessianisclosetoblock-diagonallydominant,
withalinearmodel,asshowninSection2.3.Althoughitis
withblockscorrespondingtothecclasses.Throughouttrain-
asmoothandconvexproblem,wedonothaveagoodun-
ing,themagnitudeofthediagonalblocksbecomecorrelated
derstandingastowhyGDisslowonlow-frequencyclasses
withtherelativefrequencyofthecorrespondingclass.This
and Adam does not suffer from this problem. Previous
leadstoc“almostindependent”problemswithverydiffer-
work on optimization under class imbalance focused on
entscales,anddifferentconvergencespeedacrossclasses.
fewclasses(Anandetal.,1993;Francazietal.,2023).They
arguethatGDisslowbecausethegradientisprimarilyin-
3.1.Theproblem
fluencedbythemajorityclassandassuchdoesnotdecrease
thelossasmuchontheminorityclasses.Butunderheavy- We establish notation to formalize this idea. We have n
tailedimbalancewithmanyclasses,themajorityofsamples samples(x ,y )n acrosscclasses,vectorsx ∈Rdwith
i i i=1 i
belong to the tail of low-frequency classes, so it remains labelsy ∈[c],weightsW∈Rc×d,andusetheloss
i
unclearwhyGDisslowatoptimizingtheaverageloss.
ℓ(W,x,y)=−log(σ(Wx) ), with σ(z) = ezk .
y k (cid:80) ezj
Inthissection,welookatthegradientandHessianthrough- j
outthetrainingtrajectorytoidentifywhyGDisslowerthan TheoveralllossisL(W)= n1 (cid:80)n i=1ℓ(W,x i,y i).Wesplit
Adam(orsigndescent).Wefocusonprovidinginsightsand theparametersbyrow,w 1,...,w c ∈Rdandwritep(x)=
high-levelintuitionbasedonempiricalevidence;ourmain σ(Wx).Thegradientwithrespecttow k is
take-awayisthatclassimbalanceleadstoill-conditioning.
∇ ℓ(W,x,y)=(1 −p(x) )x. (2)
wk [y=k] k
Ill-conditioningandgradientdescent.Considerthetoy
Theblocks(ofsize[d×d])oftheHessianare
modelofoptimizingaquadraticL(w)= 1w⊤Hwwhere
2
H = diag(λ 1,...,λ d), with GD with a step-size α ≤ ∇ w2 kℓ(W,x,y)=p(x) k(1−p(x) k)xx⊤, (3)
1/λ max(H).Thelossatiteratew tdecomposesas ∇ wk∇
w
k′ℓ(W,x,y)=−p(x) kp(x) k′xx⊤, (4)
L(w t)=(cid:80)d i=1(1−αλ i)tei 0, (1) for k ̸= k′. The magnitude of the off-diagonal blocks is
whereei = 1λ w [i]2istheithcomponentoftheerrorat smallerthatthediagonalblocks,andtheHessianiscloseto
0 2 i 0 block-diagonallydominantinthesensethat
initialization.Onthisproblem,wewouldseethefollowing;
Tr(∇2 L(W))=−(cid:80) Tr(∇ ∇ L(W)).
– Errorsassociatedwithlargeeigenvaluesconvergefaster wk k′̸=k wk w k′
thanthoseassociatedwithloweigenvalues. Wewillthusignoretheoff-diagonalblocksandstudythe
– Convergenceontheoveralllosswillbeslowifthemajor- scale of the diagonal blocks throughout training, on the
ityoftheerrorcomesfromlow-eigenvaluecomponents. samelinearmodelusedinSection2.3,Figure3.
5
ssoL
niarT
ssoL
niarTHeavy-tailedclassimbalance
Figure5.Evolutionof thegradientnorm andHessiantrace throughiterationsin thepathof GD. For eachweightvectorw
k
predictingthelogitsofclasskweshowthegradientnorm,thetraceoftheHessianandtheircorrelationonthelinearmodelofFigure3
(timestepscorrespondtotheepochs).Thegradientisinitiallylargestfortheweightsofthemajorityclass,anddecaysashigh-frequency
classesgetsfit.TheHessianblocksareinitiallyuniform,suggestingthateventhoughthegradientisprimarilyinfluencedbyhigh-frequency
classes,itisagooddescentdirection.Butasthemodelimproves,therelativeclassfrequenciesbecomesvisibleinthediagonalofthe
Hessian,suggestingthatlargerstep-sizesfortheweightsassociatedwithlow-frequencyclasseswouldbebeneficialafterafewsteps.
3.2.DynamicsofthegradientsandHessians Initialization.WhenW =0,themodeloutputsuniform
0
predictions of σ(W 0x)
k
= 1/c and Equations (2) and (3)
InthefirsttworowsofFigure5,weshowthatthemagnitude
canbesimplifiedtohighlighttheimpactofclassfrequency;
ofthegradients(norm)andHessians(trace)foreachweight
vectorw 1,...,w coverthepathtakenbyGDinFigure3.At ∇ wkL(W 0)=π kx¯k− 1 cx¯ (5)
initialization,thegradientsarehigherforweightsofhigh- ∇2 L(W )= 1(cid:0) 1− 1(cid:1) H¯. (6)
frequencyclasses,reflectingtheclassdistribution,andthe wk 0 c c
magnitude of the gradients of high-frequency classes go Forthegradients,ifthemagnitudeofthedataisuniform
downasthepredictionsimprove.Meanwhile,thetracesof across classes (∥x¯k∥≈∥x¯k′∥), the main difference across
theHessianblocksareuniformatinitialization,butstartto classesistherelativefrequencyπ k.Forfrequentclasses,π k
reflecttheclassfrequencypatternduringtraining. dominatestheuniform1/cterm,andthemagnitudeofthe
gradientsreflectstheclassfrequencies.Instead,theHessian
The higher Hessian trace for weights of high-frequency
onlydependsontheuniformpredictionsofthemodel,sothe
classes in Figure 5 coincides with oscillations in the loss
blocksareindependentoftheclass,asobservedinFigure5.
ofhigh-frequencyclassesinFigure3.Thisisasignofill-
conditioning.Weshouldtakelargerstepsindirectionsof Duringtraining.Toconsidertheevolutionofthegradient
lowcurvaturetomovetheweightsoflow-frequencyclasses, andHessianduringtraining,andshowthatimprovedpre-
butcannotdosowithoutcausinglargeroscillationsonhigh- dictiononagivenclassleadstoacorrelationbetweenthe
frequency classes. The third row of Figure 5 shows the scaleofitsgradientandHessian,weconsiderthefollowing
correlationbetweenthegradientandtheHessian,indicating simpledefinitionfora“good”predictiononagivenclass.
thatdirectionswithlargegradientsanddirectionswithhigh Definition 1 (good prediction). The model makes good
curvaturealignduringtraining. predictionsonclassk ifitpredictsk withnon-negligible
probabilityponsamplesfromthatclass(p(x ) =pforx
Toillustratetheaboveobservationsanalytically,wewrite i k i
fromclassy =k),andpredictskwithnear-randomchance
n forthenumberofsamplesofclassk,π =n /nforthe i
rek lativefrequencies,anddefinethefirstandk seconk
dmoments
onothersamples(p(x i)
k
=O(1/c)forx iwherey
i
̸=k).
oftheinputsacrossclassesandtheentiredatasetas
Underthisassumption,thegradientandHessianonclassk
x¯k = n1
k
(cid:80)n i=1:yi=kx i, x¯ = n1 (cid:80)n i=1x i, aredominatedbytherelativeclassfrequencyπ k asc→∞
H¯k = n1
k
(cid:80)n i=1:yi=kx ix⊤ i, H¯ = n1 (cid:80)n i=1x ix⊤ i. ∇ wkL(W)=Θ((1−p)π kx¯k),
∇2 L(W)=Θ(p(1−p)π H¯k),
wk k
6Heavy-tailedclassimbalance
Figure6.Evolution of the gradient norm and Hessian trace through iterations in the path of Adam. See caption of Figure 5.
TheprimarydifferencewithFigure5isthatlow-frequencyclasseschangefasterwithAdam.TheHessianblockcorrespondingto
low-frequencyclassesdecreasemore,leadingtoabettercorrelationbetweengradientsandHessianacrossblocks.
seeAppendixFforfullderivations.Thismodelconfirms Linearsoftmaxclassificationonheavy-tailedclassimbal-
that,afterwereachnon-negligiblepredictionperformance anceprovidesasimplemodelwherethisrelationshipholds.
onaclass,thegradientstillproportionaltoπ x¯k,butthe We observe that the correlation between gradients and
k
Hessianalsoisoforderπ .Ifsimilarprogressismadeon HessiansseenwhenrunningGD(Figure5)appearsmore
k
multipleclasses,theimbalanceoftheπ naturallyleadsto quicklyalongthetrajectorytakenbyAdam,showninFig-
k
ill-conditioning,whichmakesitdifficultforGDtomake ure 6. This coincides with Adam learning low-frequency
progress,whileAdammaybenefitfromthecorrelation. classes faster than GD, while the gradient and Hessian
blocksofhigh-frequencyclassesbehavesimilarlyalongthe
3.3.Adam,Signdescentand“adaptingtocurvature” trajectoryofbothAdamandSGD(compareFigures5and6).
Adaminitiallyincreasesthelossonlow-frequencyclasses
Acommonhigh-levelintuitionforthesuccessofAdamis
(seeFigure3),whichcoincideswithuniformHessianblocks,
that it takes smaller steps in directions of high-curvature.
butthenconvergesfasteronthelow-frequencyclassesthan
Indeed, the Adam and AdaGrad papers (Kingma and Ba,
GDwhenthemagnitudeofthegradientandHessianbecome
2015;Duchietal.,2011)justifythebenefitsofAdamand
correlated.Sinceπ issmallontheseclasses,GDtakesvery
k
AdaGradasapproximatingsecond-ordermethods.Butthis
smallsteps,whileAdam’snormalizationcancanceloutπ .
k
interpretationdoesnotholdgenerally,evenonlinearmodels
(Kunstneretal.,2019).ThepreconditionerusedbyAdam Ouranalysisindicatesthat,onlinearmodels,themagnitude
doesnotleadtosmallerstepsindirectionsofhighcurvature, ofthegradientandtheHessianacrossblocksareprimarily
butsmallerstepsindirectionsoflargegradients.Theupdate influencedbyrelativeclassfrequency.Thisobservationpro-
isclosertosign-basedmethods(Ballesetal.,2020),aswas videsajustificationforthepreconditioninginterpretation
theideabehindRMSProp(TielemanandHinton,2012). of Adam on a model where this relation could be further
analyzed, and provides an alternative perspective on the
Despitelackingatheoreticaljustification,observationson
sparsityproblemusedtomotivateAdaGrad(Duchietal.,
deepnetworkshaveshownthattherelativemagnitudeof
2011), on a smooth problem rather than the non-smooth
theentriesofthegradientcanbeausefulproxyforthoseof
convexadversarialsetting.However,thisrelationshipdoes
theHessian(e.g.,SinghandAlistarh,2020).Theobserva-
notholdgloballyandonlyappearsduringoptimization.
tionthatthemagnitudeofthegradientandHessianappear
correlated across time motivated the relaxed smoothness Beyond linear models, a similar behavior might occur in
assumptionofZhangetal.(2020a),whiletheircorrelatation intermediatelayersofneuralnetworks.Ifasetofweights
acrossdimensionsmotivatedthecoordinate-wisevariantof capturesaspecificfeatureofthedata,themagnitudeoftheir
Crawshawetal.(2022).However,theseempiricalobserva- gradientandHessiancouldbeinfluencedbythenumberof
tionshavesofaronlybeenmadeondeepmodels,andwe sampleswiththisfeature,possiblyexplainingoscillations
donotyethaveagoodunderstandingofthisphenomenon. observedatafeature-levelbyRosenfeldandRisteski(2023).
7Heavy-tailedclassimbalance
Overall loss SGD ( m) SGD (+m) Adam ( m) Adam (+m)
10 10 10 10 10
0 0 0 0 0
0 50 100 0 50 100 0 50 100 0 50 100 0 50 100
Epoch Epoch Epoch Epoch Epoch
SSGGDD ((nnoo mmoommeennttuumm)) AAddaamm ((nnoo mmoommeennttuumm)) 1100%% ssaammpplleess,, lleeaasstt ffrreeqq.. ccllaasssseess
SSGGDD ((wwiitthh mmoommeennttuumm)) AAddaamm ((wwiitthh mmoommeennttuumm)) 1100%% ssaammpplleess,, mmoosstt ffrreeqq.. ccllaasssseess
Figure7.ReweightingthelossimprovestheperformanceofSGDonlow-frequencyclasses.Theplotsshowtheunweightedloss,
whileSGDandAdamoptimizeareweightedloss.WeobservealargeimprovementintheperformanceofSGDoverdirectlyoptimizing
theunweightedloss.SGDnowmakesprogressonallclasses,andthelargegapbetweenSGDandAdamdisappears.
4.Discussion Long-tailed learning. The long-tailed learning literature
(e.g.,Zhuetal.,2014;Liuetal.,2019)focusesonsettings
The mechanism we identify, that heavy-tailed imbalance
withmanyclasseswithfewexamples.Buttheprimarycon-
leadstooptimizationdifficultieswith(S)GD,isreproducible
cerns are often generalizing from few examples, transfer
overdifferentarchitecturesanddatatypes.Duetothepreva-
learning (Wang et al., 2017), or achieving balanced error
lenceofclassimbalanceinlanguagetasks,thismechanism
rates.Classimbalanceinamulticlasssettingissufficient
islikelytobeadifferentiatingfeatureofthetrainingdiffi-
to exhibit these problems, for example on a “long-tailed”
cultiesencounteredwithlanguagetransformers.
variantofCIFAR10(e.g.,Cuietal.,2019).However,the
The correlation between gradient and Hessian blocks ob- problemwehighlightisoneoftrainingperformance,and
servedthroughouttrainingprovidesajustificationforthe imbalancewith10classesonlyhasminimalimpactonthe
claimsthatAdamcan“adapttocurvature”,onalinearmodel overalltrainingloss.Tohavealargelevelofclassimbalance
wherethisbehaviorcanbefurtheranalyzed.Althoughthe (eg,a1000×gapinrelativefrequency)andalargenumber
problemisalreadysmooth,asithasaboundedHessian,the ofsamplesfromminorityclasses,theproblemneedsmore
relaxedsmoothnessassumptionofZhangetal.(2020a)(or classesandheavytails.Ourresultsfurtheridentifythatthe
relaxationthereof,e.g.Crawshawetal.,2022)couldbetter choiceoftrainingalgorithmcanbeaconfounder,assolu-
describethecurvaturealongthepathtakenbytheoptimizer. tionsfoundby(S)GDmightgeneralizepoorlybecauseless
data,butalsobecauseminorityclassestrainmoreslowly.
Alternativefixes.Givenourhypothesisthatalargepartof
thegapbetween(S)GDandAdamonlanguagetransformers Generalization. Our findings suggest that using Adam
isduetoclassimbalance,weexpectalternativeinterventions or other approaches targeting the class imbalance, such
targetingclassimbalancetohavealargeeffectontraining as reweighting the loss, can lead to faster optimization
performance. In Figure 7, we show the result of training on low-frequency classes. At least early in training, this
a 2-hidden layer transformer on the PTB dataset, where improvementmaytranslatetogeneralizationgains.How-
√
samples from class k are reweighted by 1/ π , and plot ever, training longer may lead to different behaviors. For
k
the(unweighted)loss.Weseealargeimprovementinthe instance,reweightingmaynolongerplayaroleininterpo-
performanceofSGD,andprogressonallclasses.Running latingregimes(ByrdandLipton,2019;Soudryetal.,2018),
SGDonthereweightedlossleadstofasterprogressonthe and overfitting with over-parameterized models can lead
unweightedlossthandirectlyoptimizingontheunweighted to undesirable memorization behavior on long-tail exam-
loss,closingtheperformancegapwithAdam. ples(Sagawaetal.,2020;Hernandezetal.,2022),which
couldbeduetosignificantdifferencesbetweenempirical
Tokenizers.Specificallyforlanguagemodeling,thechoice
and population distributions in the tail. The (nearly) one-
oftokenizationprocedurecanhavealargeimpactondown-
passtrainingregimecommonlyusedinlargelanguagemod-
streamperformance,whichhasbeenattributedtotheheavy-
els,aswellasothersuitableregularizationapproachesmay
tailednatureoftokendistributionsandlackofsampleson
helpmitigatetheseissues.Ontheotherhand,ifsomeform
raretokens(GowdaandMay,2020).Zouharetal.(2023)
ofmemorizationonlow-frequencyclassesisnecessaryin
alsoarguethatuniformtokenizersmaybemoreefficient.
long-tailedsettings(Feldman,2020),failingtooptimizethe
Thisappearstobetruenotonlyforgeneralizationbutalso
loss from low-frequency classes is likely to lead to poor
foroptimization,Then,inadditiontosampleefficiency,our
performance.Yetapreciseunderstandingofthebenefitsof
resultsindicatethatmoreefficienttokenizerscouldleadto
Adamforgeneralizationinsuchregimesremainsanimpor-
easieroptimizationproblems.
tantopenquestiontobeaddressedinfuturework.
8
ssoL
niarTHeavy-tailedclassimbalance
Acknowledgements tiveNumberofSamples”.In:ConferenceonComputer
VisionandPatternRecognition(CVPR),pp.9268–9277.
WethankGregd’Eon,AaronMishkin,andVictorSanches
Portella for useful discussions and comments on the John C. Duchi, Elad Hazan, and Yoram Singer (2011).
manuscript. This research was supported by the Canada “AdaptiveSubgradientMethodsforOnlineLearningand
CIFARAIChairProgram,theNaturalSciencesandEngi- StochasticOptimization”.In:JournalofMachineLearn-
neeringResearchCouncilofCanada(NSERC)throughthe ingResearch(JMLR)12,pp.2121–2159.
DiscoveryGrantsRGPIN-2022-03669,andwasenabledby
VitalyFeldman(2020).“Doeslearningrequirememoriza-
thesupportprovidedbytheBCDRIGroupandtheDigital
tion?ashorttaleaboutalongtail”.In:Proceedingsof
ResearchAllianceofCanada(alliancecan.ca).
the52ndAnnualACMSIGACTSymposiumonTheory
ofComputing,pp.954–959.
References
Emanuele Francazi, Marco Baity-Jesi, and Aure´lien Luc-
KwangjunAhn,XiangCheng,MinhakSong,ChulheeYun,
chi (2023). “A Theoretical Analysis of the Learning
AliJadbabaie,andSuvritSra(2023).“Linearattention
Dynamics under Class Imbalance”. In: International
is(maybe)allyouneed(tounderstandtransformeropti-
Conference on Machine Learning (ICML). Vol. 202,
mization)”.In:arXivpreprintarXiv:2310.01082.
pp.10285–10322.
RangachariAnand,KishanG.Mehrotra,ChilukuriK.Mo-
Philip Gage (1994). “A new algorithm for data compres-
han,andSanjayRanka(1993).“Animprovedalgorithm
sion”.In:CUsersJournal12.2,pp.23–38.
forneuralnetworkclassificationofimbalancedtraining
sets”.In:IEEETransactionsonNeuralNetworks4.6, NikhilGhosh,SongMei,andBinYu(2022).“TheThree
pp.962–969. StagesofLearningDynamicsinHigh-dimensionalKer-
nelMethods”.In:InternationalConferenceonLearning
Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton
Representations(ICLR).
(2016).“LayerNormalization”.In:NeuralInformation
ProcessingSystems(NeurIPS),DeepLearningSympo- Thamme Gowda and Jonathan May (2020). “Finding the
sium. OptimalVocabularySizeforNeuralMachineTransla-
tion”.In:FindingsoftheAssociationforComputational
LukasBallesandPhilippHennig(2018).“DissectingAdam:
Linguistics(EMNLP),pp.3955–3964.
TheSign,MagnitudeandVarianceofStochasticGradi-
ents”.In:InternationalConferenceonMachineLearn- Bobby He, James Martens, Guodong Zhang, Aleksandar
ing(ICML).Vol.80,pp.413–422. Botev,AndrewBrock,SamuelL.Smith,andYeeWhye
Teh (2023). “Deep Transformers without Shortcuts:
Lukas Balles, Fabian Pedregosa, and Nicolas Le Roux
Modifying Self-attention for Faithful Signal Propaga-
(2020). The Geometry of Sign Gradient Descent.
tion”.In:InternationalConferenceonLearningRepre-
arXiv/2002.08056.
sentations(ICLR).
AlbertoBietti,VivienCabannes,DianeBouchacourt,Herve
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun
Jegou,andLeonBottou(2023).“BirthofaTransformer:
(2016). “Deep Residual Learning for Image Recogni-
AMemoryViewpoint”.In:NeuralInformationProcess-
tion”.In:ConferenceonComputerVisionandPattern
ingSystems(NeurIPS).
Recognition(CVPR),pp.770–778.
TomB.Brownetal.(2020).“LanguageModelsareFew-
DannyHernandez,TomBrown,TomConerly,NovaDas-
ShotLearners”.In:NeuralInformationProcessingSys-
Sarma, Dawn Drain, Sheer El-Showk, Nelson El-
tems(NeurIPS).
hage, Zac Hatfield-Dodds, Tom Henighan, Tristan
Jonathon Byrd and Zachary Lipton (2019). “What is the Hume, et al. (2022). “Scaling laws and interpretabil-
effect of importance weighting in deep learning?” In: ityoflearningfromrepeateddata”.In:arXivpreprint
Internationalconferenceonmachinelearning(ICML). arXiv:2205.10487.
MichaelCrawshaw,MingruiLiu,FrancescoOrabona,Wei Chen Huang, Yining Li, Chen Change Loy, and Xiaoou
Zhang, and Zhenxun Zhuang (2022). “Robustness to Tang (2016). “Learning Deep Representation for Im-
UnboundedSmoothnessofGeneralizedSignSGD”.In: balancedClassification”.In:ConferenceonComputer
NeuralInformationProcessingSystems(NeurIPS). VisionandPatternRecognition(CVPR),pp.5375–5384.
YinCui,MenglinJia,Tsung-YiLin,YangSong,andSergeJ. Kaiqi Jiang, Dhruv Malik, and Yuanzhi Li (2022). “How
Belongie(2019).“Class-BalancedLossBasedonEffec- DoesAdaptiveOptimizationImpactLocalNeuralNet-
workGeometry?”In:arXivpreprintarXiv:2211.02254.
9Heavy-tailedclassimbalance
Diederik P. Kingma and Jimmy Ba (2015). “Adam: A scent on Separable Data”. In: International Confer-
MethodforStochasticOptimization”.In:International enceonArtificialIntelligenceandStatistics(AISTATS).
ConferenceonLearningRepresentations(ICLR). Vol.89,pp.3420–3428.
Taku Kudo (2018). “Subword Regularization: Improving PreetumNakkiran,BehnamNeyshabur,andHanieSedghi
NeuralNetworkTranslationModelswithMultipleSub- (2021).“Thedeepbootstrapframework:Goodonline
wordCandidates”.In:AnnualMeetingoftheAssocia- learnersaregoodofflinegeneralizers”.In:International
tionforComputationalLinguistics,pp.66–75. ConferenceonLearningRepresentations(ICLR).
FrederikKunstner,JacquesChen,JonathanWilderLaving- LorenzoNoci,SotirisAnagnostidis,LucaBiggio,Antonio
ton,andMarkSchmidt(2023).“Noiseisnotthemain Orvieto,SidakPalSingh,andAure´lienLucchi(2022).
factorbehindthegapbetweenSGDandAdamontrans- “SignalPropagationinTransformers:TheoreticalPer-
formers,butsigndescentmightbe”.In:International spectivesandthe RoleofRankCollapse”. In: Neural
ConferenceonLearningRepresentations(ICLR). InformationProcessingSystems(NeurIPS).
FrederikKunstner,PhilippHennig,andLukasBalles(2019). AntonioOrvieto,JonasKohler,DarioPavllo,ThomasHof-
“LimitationsoftheempiricalFisherapproximationfor mann,andAure´lienLucchi(2022).“VanishingCurva-
naturalgradientdescent”.In:NeuralInformationPro- tureinRandomlyInitializedDeepReLUNetworks”.In:
cessingSystems(NeurIPS),pp.4158–4169. InternationalConferenceonArtificialIntelligenceand
Statistics(AISTATS).Vol.151,pp.7942–7975.
Yann LeCun, Le´on Bottou, Yoshua Bengio, and Patrick
Haffner(1998).“Gradient-BasedLearningAppliedto YanPanandYuanzhiLi(2023).TowardUnderstandingWhy
DocumentRecognition”.In:ProceedingsoftheIEEE. Adam Converges Faster Than SGD for Transformers.
Vol.86.11,pp.2278–2324. NeurIPS2022WorkshoponOptimizationforMachine
Learning.arXiv/2306.00204.
Liyuan Liu, Xiaodong Liu, Jianfeng Gao, Weizhu Chen,
andJiaweiHan(2020).“UnderstandingtheDifficulty AdamPaszkeetal.(2019).“PyTorch:AnImperativeStyle,
ofTrainingTransformers”.In:ConferenceonEmpirical High-PerformanceDeepLearningLibrary”.In:Neural
Methods in Natural Language Processing, pp. 5747– InformationProcessingSystems(NeurIPS),pp.8024–
5763. 8035.
ZiweiLiu,ZhongqiMiao,XiaohangZhan,JiayunWang, Steven T. Piantadosi (2014). “Zipf’s word frequency law
Boqing Gong, and Stella X. Yu (2019). “Large-Scale innaturallanguage:Acriticalreviewandfuturedirec-
Long-TailedRecognitioninanOpenWorld”.In:Con- tions”.In:Psychonomicbulletin&review21,pp.1112–
ference on Computer Vision and Pattern Recognition 1130.
(CVPR),pp.2537–2546.
AlecRadford,JeffWu,RewonChild,DavidLuan,Dario
Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Amodei,andIlyaSutskever(2019).LanguageModels
Marcinkiewicz (1993). “Building a Large Annotated areUnsupervisedMultitaskLearners.Tech.Report.
CorpusofEnglish:ThePennTreebank”.In:Computa-
Elan Rosenfeld and Andrej Risteski (2023). “Outliers
tionalLinguistics19.2,pp.313–330.
with Opposing Signals Have an Outsized Effect on
KevinMeng,DavidBau,AlexAndonian,andYonatanBe- Neural Network Optimization”. In: arXiv preprint
linkov(2022).“Locatingandeditingfactualassociations arXiv/2311.04163.
in GPT”. In: Neural Information Processing Systems
Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, and
(NeurIPS).
PercyLiang(2020).“Aninvestigationofwhyoverpa-
Stephen Merity, Caiming Xiong, James Bradbury, and rameterization exacerbates spurious correlations”. In:
RichardSocher(2017).“PointerSentinelMixtureMod- InternationalConferenceonMachineLearning(ICML).
els”.In:InternationalConferenceonLearningRepre-
RobinM.Schmidt,FrankSchneider,andPhilippHennig
sentations(ICLR).
(2021).“DescendingthroughaCrowdedValley-Bench-
EricJ.Michaud,ZimingLiu,UzayGirit,andMaxTegmark markingDeepLearningOptimizers”.In:International
(2023).“Thequantizationmodelofneuralscaling”.In: Conference on Machine Learning (ICML). Vol. 139,
NeuralInformationProcessingSystems(NeurIPS). pp.9367–9376.
MorShpigelNacson,JasonD.Lee,SuriyaGunasekar,Pe- RicoSennrich,BarryHaddow,andAlexandraBirch(2016).
droHenriquePamplonaSavarese,NathanSrebro,and “NeuralMachineTranslationofRareWordswithSub-
DanielSoudry(2019).“ConvergenceofGradientDe-
10Heavy-tailedclassimbalance
wordUnits”.In:AnnualMeetingoftheAssociationfor SuvritSra(2020b).“WhyareAdaptiveMethodsGood
ComputationalLinguistics. forAttentionModels?”In:NeuralInformationProcess-
ingSystems(NeurIPS),pp.15383–15393.
Sidak Pal Singh and Dan Alistarh (2020). “WoodFisher:
EfficientSecond-OrderApproximationforNeuralNet- Xiangxin Zhu, Dragomir Anguelov, and Deva Ramanan
workCompression”.In:NeuralInformationProcessing (2014). “Capturing Long-Tail Distributions of Object
Systems(NeurIPS). Subcategories”.In:ConferenceonComputerVisionand
PatternRecognition(CVPR),pp.915–922.
DanielSoudry,EladHoffer,MorShpigelNacson,Suriya
Gunasekar, and Nathan Srebro (2018). “The implicit Vile´m Zouhar, Clara Meister, Juan Luis Gastaldi, Li Du,
biasofgradientdescentonseparabledata”.In:Journal Mrinmaya Sachan, and Ryan Cotterell (2023). “Tok-
ofMachineLearningResearch(JMLR)19.1,pp.2822– enizationandtheNoiselessChannel”.In:AnnualMeet-
2878. ing of the Association for Computational Linguistics
(Volume1:LongPapers),ACL,pp.5184–5207.
NitishSrivastava,GeoffreyE.Hinton,AlexKrizhevsky,Ilya
Sutskever,andRuslanSalakhutdinov(2014).“Dropout:
a simple way to prevent neural networks from over-
fitting”. In: Journal of Machine Learning Research
(JMLR)15.1,pp.1929–1958.
TijmenTielemanandGeoffreyHinton(2012).RMSPROP:
Dividethegradientbyarunningaverageofitsrecent
magnitude.Lecturenotes
http://www.cs.toronto.edu/˜tijmen/
csc321/slides/lecture_slides_lec6.
pdf.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszko-
reit,LlionJones,AidanN.Gomez,LukaszKaiser,and
Illia Polosukhin (2017). “Attention is All you Need”.
In:NeuralInformationProcessingSystems(NeurIPS),
pp.5998–6008.
KevinWang,AlexandreVariengien,ArthurConmy,Buck
Shlegeris,andJacobSteinhardt(2022).“Interpretability
inthewild:acircuitforindirectobjectidentificationin
GPT-2small”.In:arXivpreprintarXiv:2211.00593.
Yu-XiongWang,DevaRamanan,andMartialHebert(2017).
“Learning to Model the Tail”. In: Neural Information
ProcessingSystems(NeurIPS),pp.7029–7039.
JakeRylandWilliams,PaulR.Lessard,SumaDesu,Eric
M.Clark,JamesP.Bagrow,ChristopherM.Danforth,
andPeterSheridanDodds(2015).“Zipf’slawholdsfor
phrases,notwords”.In:Scientificreports5.1,p.12209.
Jingfeng Wu, Vladimir Braverman, and Jason D. Lee
(2023). “Implicit Bias of Gradient Descent for Lo-
gistic Regression at the Edge of Stability”. In: arXiv
arXiv:2305.11788.
JingzhaoZhang,TianxingHe,SuvritSra,andAliJadbabaie
(2020a).“WhyGradientClippingAcceleratesTraining:
ATheoreticalJustificationforAdaptivity”.In:Interna-
tionalConferenceonLearningRepresentations(ICLR).
JingzhaoZhang,SaiPraneethKarimireddy,AndreasVeit,
SeungyeonKim,SashankJ.Reddi,SanjivKumar,and
11Heavy-tailedclassimbalance
Appendix
A.Experimentaldetails
Thenextfewsectionsdocumentthedatasets,models,softwareandexperimentalsetupusedinourexperiments.
A.1.Datasets
• TheWikiText-103dataset(Merityetal.,2017)isusedinFigure1foralanguagemodelingtask.Weusesequencesof
1024tokens,usetheBPEtokenizer(Sennrichetal.,2016),leadingtoavocabularyofsize50608.
• TheWikiText-2dataset(Merityetal.,2017)isusedinAppendixA.5toillustratethatothercombinationsofdatasets
andtokenizersleadtoheavy-taileddistributions.
• ThePTBdataset(Marcusetal.,1993)isusedforthelanguagemodelingtaskinFigures4and7andAppendicesBandD.
Weusesequencesof35tokensbuiltfromaword-basedtokenizer(basic englishprovidedbytorchtext),for
avocabularyofsize9920.Fordeterministicruns,weusethevalidationsetasareducedtrainingset,labeledTinyPTB.
• TheMNISTdataset(LeCunetal.,1998)isusedinourexperimentonCNNsinFigure2andAppendixC.
A.2.Models
• The2-layertransformerusedinFigure7andAppendixBisatransformerVaswanietal.(2017),basedonthePyTorch
implementationofTransformerEncoderLayer(Paszkeetal.,2019).
Embedding→2×[Attention→Linear→ReLU→Linear]→Classifier.
ThemodelincludesLayerNorm,dropout,andskipconnections(Heetal.,2016;Baetal.,2016;Srivastavaetal.,2014).
Theembeddingdimensionandwidthofthelinearlayersis1000andtheattentionmodulesuse4heads.
• ThesimplifiedtransformerusedinFigure4andAppendixBdoesnotuseencoderblocks,andonlyusesattention:
Embedding→Attention→Classifier.
We remove LayerNorm, dropout, and the non-linearity induced by the [Linear → ReLU → Linear] part of the
transformermodule.InFigure4,wefreezetheembeddingandattentionlayersarefrozenatinitializationandonlythe
lastclassificationlayeristrained.Themodelisthenequivalenttoalinearmodelusingafixedfeaturetransformation.
• The GPT2-Small model (Radford et al., 2019) is used in Figure 1. The block includes LayerNorm and residual
connections, uses dropout on the embedding and dense layers. We use sinusoidal positional encoding as in the
transformerarchitecture(Vaswanietal.,2017).Theembeddingdimensionis768,thewidthoftheintermediatelinear
layersis3072,andweuse12encoderblockswith12selfattentionheadseach.
• TheconvolutionalnetworkusedinFigure2andAppendixCisa2-layerconvolution
Conv→Relu→MaxPool→Conv→Relu→MaxPool→Linear
• ThelinearmodelusedinFigures3,5and6andAppendixDusesabiasvectorandthecrossentropyloss.
A.3.Customdatasets
• TheRandomHeavy-TailedLabelsdatasetisusedinourexperimentwithlinearmodelsinFigure3andAppendicesD
andE.Thenumberofsamplesperclassandthenumberofclassesarepickedtoapproximateapower-lawdistribution.
Wecreatem“groups”ofclasses,whereeachclasswithinagrouphasthesamerelativefrequency;
1classwith2msamples, 2classeswith2m−1samples, ..., 2m−1classeswith2samples.
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
group1 group2 groupm
The inputs are drawn from a uniform distribution on [0,1], independently of the class label. The inputs are in
d=(m+1)2mdimensions,thenumberofsamplesisn=m2mandthenumberofclassesisc=2m+1−1.Weuse
twovariantsofthedatasets;alargeoneinFigure3,AppendixD(m=11,n=22528,d=24576,c=4095)anda
smalloneinAppendixE(m=8,n=2048,d=2304,c=511).
12Heavy-tailedclassimbalance
• TheBarcodedMNISTdatasetusedinourexperimentsonCNNsinFigure2andAppendixCisamodifiedvariantof
MNIST.Westartwith50kexamplesfromtheoriginalMNISTdatasetacross10classes,andcreate51200(5×10×210)
newimages.Thenewexamplesarecopiesofexistingimagewithanadded“barcode”,a10-bitnumberencodedina
corneroftheimage,asintheexamplesbelow.Theclasslabelisacombinationoftheoriginalclassandthisbarcode.
TheBarcoded-onlydatasetcontains10×210classeswith5sampleseach,andisusedinsanitychecksinAppendixC.
Toobtainanimbalanceddataset,wecombinethebarcodedimageswiththeoriginalsamplesfromtheMNISTdataset
toget101200examplesspreadacross10250(10×210+10)classesclasses;10240with5examplesperclassand10
classeswith≈5kexamplesperclass,labelledMNIST+Barcoded
A.4.Trainingprocedures
Ourprimaryfocusisontheperformanceoftheoptimizersonthetrainingerror,usingassimpleatrainingprocedureas
possible.Weuseaconstantstep-sizethroughouttraining,setbygridsearch.Westartwithasparsegridofpowersof10
[10−6,10−2,...,101]andincreasethedensitytohalf-powersaroundthebeststep-size.Thestep-sizeisselectedtominimize
themaximumoverseedsofthetraininglossattheendoftraining.Forsomesettings,thisselectionstillproducesrunsthat
areunstable;thetraininglossisthesmallestattheendbutoscillatesalotduringtraining,reachinglossvaluesthatareworse
thanatinitialization.Forthoseruns,weusethenextsmallerstep-size,whichhassimilarperformancebutismorestable.
withgradientaccumulation(computingthegradientthroughmultiplepasses).
Weusegradientaccumulation(computingthegradientthroughmultiplepasses)toachievethefollowingbatchsizes;
- ThelargetransformerexperimentinFigure1usesmini-batchesof512sequencesof1024tokens.
- ThestochasticexperimentswithasmallertransformerinAppendixBusesmini-batchesof512sequencesof35tokens.
- Otherexperimentsusetheentiredatasettocomputeupdates
OurexperimentsranonaclusterusingamixofA100,P100,V100,andH100GPUs.ThelargescaleexperimentinFigure1
took3daysonaH100,whileallotherexperimentsranin2–8hours.Thetotalamountofcomputeusedforthisprojectis
≈3GPU-years,includingpreliminaryexperiments.
A.5.Classdistributionforcommondatasetsandtokenizers
Figure 8 provides additional examples of the heavy-tailed distribution of tokens using the basic english tokenizer in
torchtext(Paszkeetal.,2019),Byte-PairEncoding(BPE,Sennrichetal.,2016;Gage,1994)andUnigram(Kudo,2018)
onthePTBandWikiText-2datasets.Therelationshipbetweentherelativefrequencyrankkandandtherelativefrequency
π isroughlyπ ≈1/kuntilthemostraretokens,whichstillaccountforalargenumberofclassesduetothelog-scale.
k k
Word Unigram BPE
103
100
103
100
100 101 102 103 104 100 101 102 103 104 100 101 102 103 104
Token rank Token rank Token rank
Figure8.Differenttokenizersanddatasetsleadtoheavy-tailedtokendistributions.Comparisonofwordandsubwordtokenization
(BPE,Unigram)onthePTBandWikiText2datasets.
13
BTP
2txeTikiW
selpmas#
selpmas#Heavy-tailedclassimbalance
B.Smallermodelsandstochasticity
Figure 1 shows training loss of Adam and SGD on a large language model (GPT2-Small) and dataset (WikiText103),
makingitcomputationallyinfeasibletooptimizeusingfullbatchmethods.Tocheckwhethertheobservedbehaviorisdueto
stochasticity,wetrainsmallermodelsonsmallerdatasetswithlargerbatchsizes.
B.1.Largerbatchesonasmallermodel
Figure9showsthedynamicsofSGDandAdamonasmallermodelanddataset(2-layertransformeronPTB)usingalarger
relativebatchsize(≈2%ofthedataset,vs≈0.5%forFigure1)with50fullpassesoverthedataset.Thisallowstheleast
frequenttokenstobepresentinthetrainingdatamorefrequentlyduringtraining.Adamstillmakessignificantlymore
progressonlowfrequencyclassescomparedtoSGD.
Overall loss SGD (+m) Adam (+m)
# samples/class
10 10 10
104
5 5 5
102
100 0 0 0
100 102 104 0 20 40 0 20 40 0 20 40
Class index (sorted) Epoch Epoch Epoch
SSGGDD ((wwiitthh mmoommeennttuumm)) 1100%% ssaammpplleess,, lleeaasstt ffrreeqq.. ccllaasssseess
AAddaamm ((wwiitthh mmoommeennttuumm)) 1100%% ssaammpplleess,, mmoosstt ffrreeqq.. ccllaasssseess
Figure9.AdammakesprogressonlowfrequencyclasseswhileSGDdoesnotwithseveralpassesoverthedataset.Traininga
2-layertransformeronPTBwithAdamandSGD.(a)Distributionoftheclassesandsubsetsofthedatasortedbyclassfrequency,each
correspondingto≈10%ofthesamples.(b)Overalltrainingloss.(c,d)TraininglossforeachsubsetforSGDandAdam.SGDmakes
littletonoprogressonlow-frequencyclasses,whileAdammakesprogressonallsubsets.(b)istheaverageof(c,d).
B.2.Deterministicupdatesonanevensmallermodel
Figure10showsthetrainingperformancesimplifiedtransformer(seeAppendixA)trainedononlythevalidationsetofPTB
(TinyPTB).Bothoptimizersseealltokensateverystep.Adammakessignificantlymoreprogressonlowfrequencyclasses
thanSGD,asinthestochasticsettings.Stochasticitydoesnotappeartomakeastrongdifferenceinthebehavioroffitting
low-frequencyclasses.Inthemainpaper,Figures3to6allusedeterministicupdates.
Overall loss GD (+m) Adam (+m)
10 10 10
5 5 5
0 0 0
0 100 200 0 100 200 0 100 200
Epoch Epoch Epoch
GGDD ((wwiitthh mmoommeennttuumm)) 1100%% ssaammpplleess,, lleeaasstt ffrreeqq.. ccllaasssseess
AAddaamm ((wwiitthh mmoommeennttuumm)) 1100%% ssaammpplleess,, mmoosstt ffrreeqq.. ccllaasssseess
Figure10.AdammakesprogressonlowfrequencyclasseswhileSGDdoesnotinthedeterministicsetting.Trainingasimplified
transformeronPTB.(a)Overalltrainingloss.(b,c)TraininglossforeachsubsetforSGDandAdam.SGDmakeslittletonoprogresson
low-frequencyclasses,whileAdammakesprogressonallsubsets.
14
selpmas
#
ssoL
niarT
ssoL
niarTHeavy-tailedclassimbalance
C.Heavy-tailedimbalanceonvisiondatasets
ThetrainingdifficultiesshowninFigure2,couldbeduetootherproblemswiththedataratherthanduetoaclassimbalance.
Forexample,itcouldbethatthebarcodedimagesarehardertofitwithGDinthefirstplace,evenwithoutclassimbalance.
Tocheckforthisconfounder,werunAdamandGDtotrainthesamenetworkontheBarcoded-onlyvariantofMNIST.The
resultsareshowninFigure11.WhileAdamoutperformsGD,bothalgorithmscansolvetheproblemandreachnegligible
errorwithin200steps.ThisisincontrasttoFigure2,whichshowstheoptimizationprocessoverthefulljointdataset
(MNIST+BarcodedMNIST),whereGDfailstomakeprogressontheoverallloss.
BarcodedMNIST
10
5
Figure11.GDoptimizesonbalancedbarcodeddata.TrainingaCNNononly
0
0 100 200 300 thebarcodedportionofthedata,whichhasbalancedclasses.WhileAdamis
Epochs slightlyfaster,bothoptimizersreachnegligibleerrorwithin200steps.
D.Additionaloptimizers
WerepeattheevaluationofnormalizedgradientdescentandsigndescentinFigure4onotherarchitecturesanddatasets.We
testeachadditionaloptimizerwithandwithoutmomentumon
- Figure12:AlinearmodelandRandomHeavy-TailedLabelsdataset,asinFigure3.
- Figure13:Aone-layertransformerandtheTinyPTBdataset,asinFigure10.
- Figure14:ACNNandtheMNIST+Barcodeddatasets,asinFigure2.
LikeAdam,signdescentfitslow-frequencyclasseswithoutissue,whileGDandNormGDstruggle.NormGDcanleadto
improvementsoverGD,butthebenefitissmallerthanchangingtheupdatedirection.
Overall loss GD ( m) Adam ( m) NormGD ( m) Sign ( m)
10 10 10 10 10
0 0 0 0 0
0 100 200 0 100 200 0 100 200 0 100 200 0 100 200
GD (+m) Adam (+m) NormGD (+m) Sign (+m)
10 10 10 10 10
0 0 0 0 0
0 100 200 0 100 200 0 100 200 0 100 200 0 100 200
Epoch Epoch Epoch Epoch Epoch
GGDD (( mm)) AAddaamm (( mm)) NNoorrmmGGDD (( mm)) SSiiggnn (( mm))
GGDD ((++mm)) AAddaamm ((++mm)) NNoorrmmGGDD ((++mm)) SSiiggnn ((++mm))
Figure12.AdamandSigndescentoutperformGDandNormGDonoptimizingalinearmodelwithlowfrequencyclasses.Training
alinearmodelonGaussianHeavyTailedLabels.(a)OveralltrainingLoss.(b,c,d,e)TraininglossforGD,Adam,NormGD,andSign
descent,withandwithoutmomentum(+m,bottom/−m,top).
15
ssoL
niarT
ssoL
niarT
ssoL
niarTHeavy-tailedclassimbalance
Overall loss GD ( m) Adam ( m) NormGD ( m) Sign ( m)
10 10 10 10 10
0 0 0 0 0
0 100 200 0 100 200 0 100 200 0 100 200 0 100 200
GD (+m) Adam (+m) NormGD (+m) Sign (+m)
10 10 10 10 10
0 0 0 0 0
0 100 200 0 100 200 0 100 200 0 100 200 0 100 200
Epoch Epoch Epoch Epoch Epoch
GGDD (( mm)) AAddaamm (( mm)) NNoorrmmGGDD (( mm)) SSiiggnn (( mm))
GGDD ((++mm)) AAddaamm ((++mm)) NNoorrmmGGDD ((++mm)) SSiiggnn ((++mm))
Figure13.AdamandSigndescentoutperformGDandNormGDonoptimizingaone-layertransformeronlanguagedata.Training
aone-layertransformeronthePTBdataset.(b,c,d,e)TraininglossforGD,Adam,NormGD,andSigndescent,withandwithout
momentum(+m,bottom/−m,top).
Overall loss GD ( m) Adam ( m) Sign ( m) NormGD ( m)
10 10 10 10 10
0 0 0 0 0
0 200 400 0 200 400 0 200 400 0 200 400 0 200 400
GD (+m) Adam (+m) Sign (+m) NormGD (+m)
10 10 10 10 10
0 0 0 0 0
0 200 400 0 200 400 0 200 400 0 200 400 0 200 400
Epoch Epoch Epoch Epoch Epoch
GGDD (( mm)) AAddaamm (( mm)) NNoorrmmGGDD (( mm)) SSiiggnn (( mm))
GGDD ((++mm)) AAddaamm ((++mm)) NNoorrmmGGDD ((++mm)) SSiiggnn ((++mm))
Figure14.AdamandSigndescentoutperformGDandNormGDonoptimizingaCNNontheBarcoded+MNISTdataset.Training
atwo-layerconvolutionalneuralnetworkonthebarcodedMNISTdataset.(b,c,d,e)TraininglossforGD,Adam,NormGD,andSign
descent,withandwithoutmomentum(+m,bottom/−m,top).
E.Additionaldetailsonlinearmodelswithclassimbalance
This section gives more details about the behavior of GD on the linear model in the presence of class imbalance, and
highlightsomeofitslimitationsinmodelingthedynamicsoftrainingtransformers.Weuseasmallerdataset(thesmaller
versionoftheRandomHeavy-TailedLabelsdescribedinAppendixA.1)tomakeitpossibletorunmoreiterations.
E.1.Anearlyiterationproblem
Theobservedbehavior thatGDisslowerthanAdam atfittingthelow-frequencyclasses, mightmakeit seemthatGD
doesnotfitthelow-frequencyclassesatall.Ofcourse,whenrunforlonger,GDconvergesandfitallclasses,asshow
ininFigure15.Thishighlightthatthedifferencebetweenthealgorithmsisprimarilyadifferenceatthestartoftraining.
However,this“start”canbequitelongonlargeproblems,asinthetransformerofFigure1,theaveragelosson10%ofthe
datacorrespondingtotheleastfrequentclassesisstillhigherthanatinitializationafter15ksteps.
16
ssoL
niarT
ssoL
niarT
ssoL
niarT
ssoL
niarTHeavy-tailedclassimbalance
Short (100 steps) Medium (1k steps) Long (10k steps) Figure15.TrainingwithGDeventually
drives the loss down for all classes.
6 6 6
Training loss over time, with the same
4 4 4 step-size,fordifferenttimehorizons(100,
1k,10ksteps).GDeventuallydrivesthe
2 2 2
lossdownforallclasses,butthelossfor
0 0 0 theleast-frequentclassesonlygoesbelow
0 25 50 75 100 0 250 500 750 1000 0 2500 5000 7500 10000 thelossatinitializationafter1ksteps.
Epoch Epoch Epoch
E.2.Forlinearmodels,largestep-sizescanworkbetteriftrainedforlonger
Onbinarylogisticregressionproblems,GDeventuallyconvergeswithanychoiceofstep-size(Wuetal.,2023).However,in
ourexperiments,thestep-sizeisselectedbygridsearchtominimizethetraininglossafterafixednumberofiterations,and
thetimehorizonwepickfortheRandomHeavy-TailedLabelsdatasetappearstooshorttomakelargestep-sizesstable.
Instead,largestep-sizesleadtohighlosses.OnthesmallervariantoftheRandomHeavy-TailedLabelsdataset,weindeed
observethat,iftrainedforlongenough,largestep-sizesleadtobetterperformance,showninFigure16.However,those
step-sizesleadtolossvaluesthatareordersofmagnitudehigherthanatinitializationatthestartoftherun,andthisregime
isunlikelytoberepresentativeoftrainingdeepmodels.
Small step-size ( ) Large step-size ( )
Figure16.Largerstep-sizescanleadtobetterperfor-
manceattheend,givenenoughiterations,butare
104 104 unstableatthestartoftraining.Linearsoftmaxregres-
siononthesmallervariantoftheRandomHeavy-Tailed
102 102
Labels dataset (see Appendix A). A small step-sizes
100 100 yieldsconsistentdecrease,whileaverylargestep-size
leadstoalossthatisordersofmagnitudehigherthanat
102 102
initializationforthefirst≈300iterations,buteventually
100 101 102 103 100 101 102 103
convergestoabettervaluewhenrunforlongenough.
Epochs Epochs
E.3.Impactofinputdistribution
ImbalancealoneisnotsufficienttoinduceslowperformanceofGDonlow-frequencyclasses.Itispossibletogenerate
adatasetwithheavy-tailedclassimbalancewhereGDfitsallclassesfast,bymakingtheinputsx (closeto)orthogonal,
i
⟨x ,x ⟩≈0fori̸=j.Thismakesthelossforeachsampleindependentofeachother,andwenolongerseeadifference
i j
of performance across classes for GD. In Figure 3, we draw the inputs from a high-dimensional uniform distribution
on[0,1]d toavoidthiscase.UsingthesmallerRandomHeavy-TailedLabelsdataset,weshowthebehaviorofGDand
AdaminFigure17whentheinputdataisdrawnfromN(1,1)(left)andN(0,1)(right).Thezero-meandata,whichisbe
approximatelyorthogonalasd>n,doesnotexhibitaslowprogressonlow-frequencyclasses.
The behavior of GD on non-zero mean data appears to be a better representation of the behavior of GD on language
transformers, as we observe a performance separation per class frequency on GD when tuning only the last layer of a
languagetransformerinFigure4.Althoughtheembeddingareinitializedtobezero-meanGaussiannoise,theembedding
representationofthetokensintransformerarecorrelated,andthiscorrelationincreaseswithdepth(Nocietal.,2022,e.g.).
Overall loss GD ( m) Adam ( m) Overall loss GD ( m) Adam ( m)
5.0 5.0 5.0 5.0 5.0 5.0
2.5 2.5 2.5 2.5 2.5 2.5
0.0 0.0 0.0 0.0 0.0 0.0
0 10 20 0 10 20 0 10 20 0 10 20 0 10 20 0 10 20
Epoch Epoch Epoch Epoch Epoch Epoch
Figure17.Thedistributionoftheinputscanhavealargeimpactontheperformanceofoptimizers.Linearsoftmaxregressiononthe
RandomHeavy-TailedLabelsdataset,butwithinputssampledfromN(1,1)(left)andN(0,1)(right).
17
ssoL
niarT
ssoL
niarT
ssoL
niarT
ssoL
niarT
ssoL
niarT
ssoL
niarT
ssoL
niarTHeavy-tailedclassimbalance
Theimpactoforthogonaldata.Thelossforourlinearmodelisanaverageofnfunctions,f(w)= 1 (cid:80)n f (w),andif
n i=1 i
thedataareorthogonal,thegradientsofeachfunctionsalsoare.Thismakesthepredictiononeachsampleindependentof
theothersamplesthroughouttheiterationsofgradientdescent,andthereisnolongerany“interference”acrossclasses.
We argue in Section 3.2 that the trace should scale proportionally to π . However, the optimization performance and
k
whichstep-sizeisstabledoesnotdirectlydependonthetracebutontheeigenvalues.Ourresultsdonotguaranteethatthe
eigenvaluesscalewithπ ,asitdependsonthestructureoftheouterproducts(cid:80) x x⊤.Weconsiderthefollowingtoy
k i i i
examples,whichhighlightthatthetraceandtheeigenvaluescandiffersignificantlyifthesamplesareorthogonal.
Supposewe’reatapointwherewehavegoodpredictionsonclassk,asinDefinition1(Section3.2),andwithenough
classessuchthatthefollowingapproximationholds,
∇2 L(W)≈p(1−p)π H¯k.
wk k
Forsimplicity,weassumethattheinputsallhavenorm∥x ∥2 =d,whichholdsapproximatelyforGaussiansamplesin
i
highdimension.RememberthatH¯k isoftheform 1 (cid:80) x x⊤,
nk i:yi=k i i
Correlatedcase.Ifallthevectorswithinoneclassarecolinear,then 1 (cid:80) x x⊤haseigenvalues0withmultiplicity
nk i:yi=k i i
d−1anddwithmultiplicity1,leadingtoatraceandmaximumeigenvalueof
Tr(∇2 L(W))=p(1−p)π d, λ (∇2 L(W))=p(1−p)π d.
wk k max wk k
Boththetraceandmaximumeigenvaluescalewithπ ,(botharep(1−p)π d),andweexpecttoseeaclassseparation.
k k
Orthogonalcase.Supposeallthevectorsareorthogonal,⟨x ,x ⟩=0fori̸=j.Then 1 (cid:80) x x⊤hastheeigenvalues
i j nk i:yi=k i i
0withmultiplicityd−n andd/n withmultiplicityn ,leadningtoatraceandmaximumeigenvalueof
k k k
1
Tr(∇2 L(W))=p(1−p)π d, λ (∇2 L(W))=p(1−p) d.
wk k max wk n
Whilethetracescaleswithπ ,theeigenvaluesdonot,andwedonotexpectaclassseparationastheeigenvaluesareonthe
k
samescaleforallclasses.
Thoseexamplesareunlikelytobedirectdescriptionsofrealdatasets,asrealdataisunlikelytobeateitherofthecorrelated
ororthogonalextremes,buthighlighttheeffectofthecorrelationbetweensamplesontheeigenvaluesandrealdatashould
fallsomewherebetweenthosetwo.
18Heavy-tailedclassimbalance
F.Derivations
WebeginbyrecallingthegradientandHessianforeachblockassociatedwiththeweightsw ,...,w (Equations(2)and(3))
1 c
∇ ℓ(W,x,y)=(1 −p(x) )x, ∇2 ℓ(W,x,y)=p(x) (1−p(x) )xx⊤. (7)
wk [y=k] k wk k k
andthedefinitionsofthefirstandsecondmomentsofthedata,perclassandoverall.
x¯k = 1 (cid:80)n x , x¯ = 1 (cid:80)n x ,
nk i=1:yi=k i n i=1 i
H¯k = 1 (cid:80)n x x⊤, H¯ = 1 (cid:80)n x x⊤.
nk i=1:yi=k i i n i=1 i i
Ourgoalistoshowthat,ifthemodelmakesgoodpredictions(Definition1)onclassk,thegradientandHessiansatisfy
∇ L(W)=Θ((1−p)π x¯k), ∇2 L(W)=Θ(p(1−p)π H¯k),
wk k wk k
WebeginbysplittingtheaveragesinthegradientandHessianbywhetherthesamplesisfromclassk;
n
1 (cid:88)
∇ L(W)= (1 −p(x ) )x ,
wk n [yi=k] i k i
i=1
c n
1 (cid:88) (cid:88)
= (1 −p(x ) )x , (Splitbyclass)
n [yi=k] i k i
j=1i=1:yi=j
c n
(cid:88) 1 (cid:88)
= π (1 −p(x ) )x , (Introduceclassfrequenciesπ =n /n)
jn [yi=k] i k i j j
j
j=1 i=1:yi=j
n c n
1 (cid:88) (cid:88) 1 (cid:88)
=π (1−p(x ) )x + π (−p(x ) )x . (Separateclassk)
kn i k i jn i k i
k j
i=1:yi=k j=1,j̸=k i=1:yi=j
n
1 (cid:88)
∇2 L(W)= p(x ) (1−p(x ) )x x⊤,
wk n i k i k i i
i=1
c
1 (cid:88) (cid:88)
= p(x ) (1−p(x ) )x x⊤, (Splitbyclass)
n i k i k i i
j=1i=1:yi=j
c
(cid:88) 1 (cid:88)
= π p(x ) (1−p(x ) )x x⊤, (Introducefrequenciesπ =n /n)
jn i k i k i i j j
j
j=1 i:yi=j
1 (cid:88) (cid:88) 1 (cid:88)
=π p(x ) (1−p(x ) )x x⊤+ π p(x ) (1−p(x ) )x x⊤. (Separateclassk)
kn i k i k i i jn i k i k i i
k j
i=1:yi=k j̸=k i:yi=j
Foreachexpression,underourassumption,thefirsttermsimplifiesasp(x ) =pforallx suchthaty =k.
i k i i
c n
(cid:88) 1 (cid:88)
∇ L(W)=(1−p)π x¯k+ π (−p(x ) )x ,
wk k jn i k i
j
j=1,j̸=k i=1:yi=j
∇2 L(W)=p(1−p)π
H¯k+(cid:88)
π
1 (cid:88)
p(x ) (1−p(x ) )x x⊤.
wk k jn i k i k i i
j
j̸=k i:yi=j
While the assumption that the prediction for incorrect labels is of order O(1/c) yields makes the second term in both
expressionssmall,andvanishasc→∞.
19