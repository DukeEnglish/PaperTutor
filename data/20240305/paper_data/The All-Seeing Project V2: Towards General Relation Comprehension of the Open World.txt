The All-Seeing Project V2: Towards General
Relation Comprehension of the Open World
Weiyun Wang2,1,Yiming Ren3,1,Haowen Luo3,Tiantong Li3,1,Chenxiang Yan3,
Zhe Chen5,1, Wenhai Wang4,1, Qingyun Li6,1, Lewei Lu7, Xizhou Zhu3,1,7,
Yu Qiao1, and Jifeng Dai†3,1
1OpenGVLab, Shanghai AI Laboratory 2Fudan University
3Tsinghua University 4The Chinese University of Hong Kong
5Nanjing University 6Harbin Institute of Technology 7SenseTime Research
Abstract. We present the All-Seeing Project V2: a new model and
dataset designed for understanding object relations in images. Specifi-
cally, we propose the All-Seeing Model V2 (ASMv2) that integrates the
formulationoftextgeneration,objectlocalization,andrelationcompre-
hension into a relation conversation (ReC) task. Leveraging this unified
task, our model excels not only in perceiving and recognizing all ob-
jects within the image but also in grasping the intricate relation graph
between them, diminishing the relation hallucination often encountered
byMulti-modalLargeLanguageModels(MLLMs).Tofacilitatetraining
andevaluationofMLLMsinrelationunderstanding,wecreatedthefirst
high-quality ReC dataset (AS-V2) which is aligned with the format of
standard instruction tuning data. In addition, we design a new bench-
mark, termed Circular-based Relation Probing Evaluation (CRPE) for
comprehensively evaluating the relation comprehension capabilities of
MLLMs. Notably, our ASMv2 achieves an overall accuracy of 52.04 on
this relation-aware benchmark, surpassing the 43.14 of LLaVA-1.5 by a
large margin. We hope that our work can inspire more future research
andcontributetotheevolutiontowardsartificialgeneralintelligence.Our
project is released at https://github.com/OpenGVLab/all-seeing.
Keywords: Multimodal Large Language Model · Pointer instructions
1 Introduction
Thestudyofartificialgeneralintelligence(AGI)systemsthatcanmatchhuman
intelligence and excel in any task across domains represents the ultimate goal
in the field of artificial intelligence. Benefiting from the advancements of Large
LanguageModels(LLMs),Multi-modalLargeLanguageModels(MLLMs)have
demonstratedimpressive capabilitiesinavarietyofVision-Languagetasks,sug-
gesting new avenues for achieving AGI. However, as shown in Fig. 1a, most
popular MLLMs [11,49,50] are limited to understanding images as a whole.
†Corresponding to Jifeng Dai <daijifeng@tsinghua.edu.cn>.
4202
beF
92
]VC.sc[
1v47491.2042:viXra2 W. Wang et al.
ImageI-mTaegxIemt -DTageaxtea-ts TDeetasxtta sDeatstasets GrounGdreodu GDndraeotadus nDedtasetda sDetastasets The ATSh-eV A2T SDh-eaV tAa2s SDe-taV:ta2s Deta:taset:
e.g., Le.Ag.I,O eLN.gA.,I OLNAION e.g., Ge.Rg.i,T eG.gR.i,T GRiT RelatiRonel aCtoiRonenvl eaCrtiosoanntvi oeCnrossantvioenrssations
LargeM L Lau arl gnt Lei gM- aum Lrau ago gl nedt eMi g a - LMumlu aal oo ngti dd ge- ea u m Mll ao god eda eMl lodel LG ar ro gu en LG Ld are arod gnu egLGMn uLd ar au reo aggldu nt eei n gM- MLd ume au ao od gnld dt e giMa e- uMml lauo goltd edi- a eMml lo od da el l The ATllh-eS eATelihln-egS eAMelilon-dSge eMle vion2dge Ml vo2del v2
OutpuOtutpOututput OutpuOtutpuOtutput OutpuOtutpuOtutput
w/ Grwou/ nGdrwion/ug Gndrionugnding w/ Rewla/t iRonewla/t iRonelation
PromPptromIPmprtoamgeIpmtagIemage PrompPtromIPmprtaogmeIpmtagIemage PrompPtromImPptraogmeImptageImage BesideBesideBeside
(a) Multi-modal Large Lan- (b) Grounded MLLMs can (c) Our ASMv2 can compre-
guage Models (MLLMs) can link the objects mentioned in hendandgroundtherelations
processbothtextandimages, thesentencetotheregionsin between the objects in the
buttheycanonlycapturethe the image while struggling to image while maintaining the
holistic visual information of efficiently understand the re- capabilities of MLLMs and
thewholeimage. lationsbetweenobjects. GroundedMLLMs.
Fig.1: Overview and comparison of our All-Seeing Model v2 with other
MLLMs.Toenhancerelationcomprehensionabilitywhilemaintaininggrounding,re-
ferring, and other general capabilities, we propose (1) a novel task, termed Relation
Conversation (ReC), which unifies the formulation of text generation, object localiza-
tion, and relation comprehension; (2) a high-quality dataset AS-V2, which consists of
over127KsamplesforReC;(3)theAll-SeeingModelv2(ASMv2),whichiscapableof
comprehending and grounding the relations between the objects in the image.
As an effective method to improve interaction efficiency, the capabilities
of grounding and referring (i.e., adopting bounding boxes in responses) have
attracted increasing attention and have been widely integrated into current
Grounded MLLMs [2,8,62,80,81]. Such capabilities empower models to provide
visual responses (e.g., bounding boxes), supporting more vision-language tasks
suchasregioncaptioning[31,57],referringexpressioncomprehension[30,57],and
referring question answering [91]. However, as shown in Fig. 1b, existing models
primarily focus on recognizing certain objects within images, overlooking the
perception of relations between these objects. Due to the lack of appropriate
modelingmethodsandsuitabletrainingdataforrelationknowledge,thesemod-
els struggle to comprehend the inter-object relations within images accurately.
Consequently, these models are prone to hallucinations when dealing with rela-
tion questions or overly relying on language priors for judgment.
To enhance relation comprehension ability while maintaining grounding, re-
ferring,andothergeneralcapabilities,weintroduceanoveltask,termedRelation
Conversation(ReC).TheformulationofReCunifiesthemodelingoftextgener-
ation, object localization, and relation comprehension. Specifically, as depicted
in Fig. 1c, ReC requires the model to generate the text response while linking
allmentionedobjectsaswellasthesubjectsandobjectsofeachpredicateinthe
responsetothecorrespondingregionsintheimagesimultaneously.Suchexplicit
requirement for predicate grounding challenges the model to comprehend rela-
tions between objects within the image. Notably, models trained on ReC can be
naturally adapted to the Scene Graph Generation task. The grounded objects
serve as the nodes in the scene graph while the grounded predicates serve as
the edges. Compared with the traditional scene graph generation, ReC enables
the model to generate the scene graph in an open-ended manner, demonstrat-The All-Seeing Project V2 3
Conversation Visualization Scene Graph
Provide a scene graph caption of the given image.
chair man
In the image, a young girl is holding and eating Sitting on
a slice of pizza. She is sitting on a chair and Beside
appears to be smiling. Beside her, there is a
man who is also sitting on the same chair. Holding
girl Eating pizza
Did the man grab the pizza? Grab
Answer the question with scene graph.
man pizza
No, the girl is the one holding the pizza. Holding
girl pizza
Please provide a scene graph description for this
region man
A man is sitting next to a girl who is holding a
Sitting
next to
slice of pizza.
girl Holding pizza
Fig.2: Examples of relation conversation responses from ASMv2.
ing the potential to generalize to previously unseen predicate labels, while also
maintaining the general ability of MLLMs.
Fromthedataaspect,weconstructtheAll-SeeingDatasetV2(AS-V2)com-
prising127Khigh-qualityrelationconversationsamples,whichisbuiltuponthe
existingcaption[10],location[45],andrelation[84]annotations.CombiningAS-
V2 with other image-level and region-level multimodal corpora for training, we
proposetheAll-SeeingModelv2(ASMv2).Benefitingfromthetailoredtaskfor-
mat and data, our model can deal with three types of relation tasks, including
(1) Relation Conversation, which requires the model to link all mentioned ob-
jects and predicates to the corresponding regions in the image; (2) Open-ended
Scene Graph Generation, which requires the model to generate a scene graph
based on the given image in an open-ended manner; (3) Predicate Classifica-
tion,whichrequiresthemodeltogenerateascenegraphgiventheground-truth
object labels and localization. An example of ASMv2 is shown in Fig. 2.
To evaluate the relation comprehension ability of existing MLLMs, we con-
structabenchmarkcalledCircular-basedRelationProbingEvaluation(CRPE),
which is the first benchmark that covers all elements of the relation triplets
(subject, predicate, object), providing a systematic platform for the eval-
uation of relation comprehension ability. CRPE is formulated as single-choice
questions and consists of four splits: Existence, Subject, Predicate, and Object.
The Existence split evaluates the object recognition ability while the remaining
splits are designed to evaluate the relation comprehension capability. Addition-
ally, to evaluate the dependency on language priors, we include abnormal data
in CRPE, which depict relations that are rare but reasonable in the real world.
Our main contributions are as follows:
gnittiS
no4 W. Wang et al.
(1) We introduce the All-Seeing Project V2, which endows MLLMs with the
ability not only to perceive all objects within the image but also to recognize
the relations between these objects, leading to superior relation comprehension
capability and the potential to generate scene graphs in an open-ended manner.
(2) We propose a novel task, termed Relation Conversation, and the corre-
sponding formulation method, unifying the modeling of captioning, grounding,
and relation tasks flexibly. Based on the task and formulation, we constructed
the AS-V2 dataset. Combining AS-V2 with other general multimodal corpora
for training, we propose the All-Seeing Model v2 (ASMv2), which demonstrates
powerful performance across various tasks, including Open-ended Scene Graph
Generationandothergeneralimage-levelandregion-levelvision-languagetasks.
(3) To evaluate the relation comprehension ability of existing MLLMs, we
construct the CRPE benchmark. Notably, our ASMv2 achieves an overall accu-
racy of 52.04 on CRPE, surpassing the 43.14 of LLaVA-1.5 by a large margin.
WealsoevaluateASMv2onvariousimage-levelandregion-levelvision-language
tasks. Specifically, our model achieves an overall score of 74.4 on MMBench [51]
and 1621.0 on MME [18], surpassing LLaVA-1.5 [49] by 5.5 points and 90.0
points separately. Besides, the average accuracy of ASMv2 on grounding bench-
marks [30,57] is 87.42, outperforming Qwen-VL [2] by 1.69 points.
2 Related Work
2.1 Vision-Language Models
Significant advancements have been made in the field of visual recognition and
understanding in recent years. Models based on the image-text matching frame-
work [11,17,28,65] achieve powerful zero-shot performance on various down-
stream tasks, thereby initiating the era of open-world semantic recognition and
understanding. Subsequent works [37,87] further integrate this framework with
language modeling tasks to support more generative tasks. The recent progress
ofLargeLanguageModels[5,60,77]leadstotheemergencyofmanyLLM-based
multimodal models [11,35,38,50,76,81,89,94,105], which aim to integrate the
powerful understanding and reasoning ability of LLMs with multimodal percep-
tion and comprehension. Despite their powerful performance, these works are
only capable of capturing the holistic visual information of the whole image.
Some recent methods [8,32,52,61,62,66,80,95,98] begin to focus on location-
awareunderstanding.However,duetothelackofappropriatemodelingmethods
and training data for relation comprehension, these methods struggle to com-
prehendtheinter-objectrelationswithinimagesaccurately.Toenhancerelation
comprehension ability while maintaining other general capabilities of MLLMs,
we introduce a novel task, termed Relation Conversation, which unifies the for-
mulation of text generation, object localization, and relation comprehension.
2.2 Scene Graph Generation
Scene Graph Generation (SGG) [54] is a crucial task in scene understanding
and has attracted substantial interest across the research community. This areaThe All-Seeing Project V2 5
has witnessed the proposal of diverse model architectures, including message-
passing-based frameworks [14,21,23,41,42,92], attention-based networks [64,
102], tree-structured networks [25,96], and DETR-based networks [13,39,70].
Whilemostexistingmethodsonlyutilizeimagesasinput,recentworksbeginto
incorporate language information or knowledge graphs to facilitate SGG [16,26,
43,54,97,104],althoughthescopeoflanguageutilizationremainslimitedtobasic
objectorrelationconcepts.Comparedtopriorspecializedmodels,ourmodelisa
powerfulgeneralmodelwithstrongvision-languageunderstandingandreasoning
ability and can generate the scene graph in an open-ended manner, exhibiting
the potential to generalize to previously unseen predicate labels.
2.3 Benchmarks for Relation Comprehension
Evaluating the comprehension of relations between objects is a crucial aspect
of advancing MLLMs. Benchmarks like Visual Genome [31] and COCO [10,45]
provide foundational datasets for object detection and image captioning. These
datasetsprimarilyfocusonindividualobjectrecognitionandgeneraldescriptive
capabilities. They include annotations for object relations but are not explic-
itly designed to probe the depth of relation comprehension in a structured and
focused manner. Some synthetic datasets [1,29,74], are introduced to probe
thespatialreasoningcapabilitiesofvision-languagemodels.Thesedatasetsoffer
controlled environments for model evaluation but inherently limit the problem’s
scope due to their bounded expressivity. The Visual Spatial Reasoning (VSR)
dataset[47]asksthemodeltoclassifywhetherthecaptioncorrectlydescribesthe
relation of two objects presented in the image. This approach primarily focuses
on binary classification tasks instead of the understanding of relations within
the scene. In this work, we introduce the CRPE benchmark, which consists of
different splits and each split is designed to probe one of the elements in the re-
lation triplet (subject, predicate, object). Therefore, we can evaluate the
relation comprehension ability of existing MLLMs more systematically.
3 Data Construction
3.1 The All-Seeing Dataset v2
Our objective is to establish a dataset to unlock the Relation Conversation ca-
pability for Multi-modal Large Language Models (MLLMs), which requires the
modeltopredictnotonlytheboundingboxesofeachobjectbutalsothoseofthe
subjects and objects for each predicate mentioned in the sentence. In this sec-
tion, we elaborate on the method for constructing the training dataset for ReC,
termed All-Seeing Dataset v2 (AS-V2). Specifically, we utilize GPT-4V [61] to
construct AS-V2 based on COCO images [6] and their annotations [10,45,84].
ThekeyideaistoqueryGPT-4Vtogenerateresponseswhilelinkingtheobjects
and predicates mentioned in the generated response to specific regions within
the image, referring to the given location annotations and relation annotations.
The formulation of Relation Conversation is presented in Sec. 4.1.6 W. Wang et al.
In the image, a tall red bus is driving on a road through a busy
intersection in a metropolitan area. The bus is surrounded by various
cars, with one car beside the bus and driving on the same road. There
are multiple people visible in the scene, with one person in the bus
and others on the surrounding pavement and road. The backdrop
features buildings and trees, with a clear sky above. The scene
captures the essence of city life with its bustling traffic and urban
Describe the image in detail. architecture.
person
In
car bus
Beside
Driving on Driving on
road
(bus, driving on, road) (car, beside, bus) (car, driving on, road) (person, in, bus)
Fig.3: Data example in the AS-V2 dataset. In the relation conversation, all
mentionedobjectsarelinkedtotheircorrespondingregionswithintheimagewhilethe
predicates are linked to the regions corresponding to their subjects and objects.
The prompt for GPT-4V comprises six components: (1) Task description,
which explains the formulation of relation conversation. (2) Image to be an-
notated. (3) Caption annotations of this image, intended to enhance GPT-
4V’s understanding of the scene. (4) Location annotations, which are the
bounding boxes of the objects in the scene and guide GPT-4V in annotating
theobjectsinthecaption.(5)Relation annotations,whicharepresentedasa
list of (subject, predicate, object) triplets and help GPT-4V to annotate
the predicate in the caption. (6) Seed examples, which are manually anno-
tated to assist GPT-4V in comprehending the task description and formatting
the output. Although the caption annotations are not necessary for GPT-4V to
produce the desired relation conversation data, incorporating these details into
the prompt significantly reduces the hallucinations in the generated data. An
example of the prompt is presented in Appendix B.
The generated data comprise three types, including: (1) Detailed descrip-
tion, which requires the model to generate a comprehensive description for an
image. (2) Region captioning, which requires the model to generate a com-
prehensivedescriptionforacertainregionwithintheimage.(3)Conversation,
whichrequiresthemodeltorespondtotheuserqueryinthemulti-turnconversa-
tion.Thequestiontypesincludetherelationsbetweenobjects,theobjecttypes,
counting the objects, object actions, object locations, and relative positions be-
tween objects. Each type of data is generated using different task descriptions
and human-annotated seed examples. These tasks require the model to under-
stand pointer instructions (e.g., utilizing bounding boxes as prompts) and link
theobjectsandpredicatesmentionedinthegeneratedresponsetotheimagere-
gions. An example is shown in Fig. 3, with additional examples in Appendix B.
Inthisway,wecollected127Krelationconversationsamplesintotal,includ-
ing 42K in detailed descriptions, 63K in region captioning, and 22K in conver-
sations (90K turns in total), respectively. The conversation samples also include
negative instructions to enhance model robustness. These instances contain in-
correct relations, and the model should be able to recognize their incorrectness.
tupnI
noitazilausiV
tuptuOThe All-Seeing Project V2 7
Question(Exist.): Which of the following Question(Subj.): What is in front of the dining table?
objects exists in the image? Choices:
Choices: A. The light is in front of the dining table.
A. Grass. B. The cabinet is in front of the dining table.
B. Floor. C. The window is in front of the dining table.
C. Parking meter. D. The person is in front of the dining table.
D. Dining table.
Qustion(Pred.): What is the relation between the clock
Question(Obj.): What is the person on? and the pavement?
Choices: Choices:
A. The person is on the platform. A. The clock is beside the pavement.
B. The person is on the sand. B. The clock is in front of the pavement.
C. The person is on the surfboard. C. The clock is over the pavement.
D. The person is on the building. D. The clock is in the pavement.
Question: What is the relation between the Question: What is the relation between the
bird and the bench? person and the umbrella?
Choices: Choices:
A. The bird is beside the bench. A. The person is holding the umbrella.
B. The bird is flying over the bench. B. The person is attached to the umbrella.
C. The bird is attached to the bench. C. The person is standing on the umbrella.
D. The bird is sitting on the bench. D. The person is touching the umbrella.
LLaVA-1.5: D. The bird is sitting on the bench. LLaVA-1.5: B. The person is attached to the umbrella.
ASMv2: A. The bird is beside the bench. ASMv2: C. The person is standing on the umbrella.
Fig.4:DataexamplesintheCRPE.Thebenchmarkisformulatedassingle-choice
questions and consists of four splits: Existence, Subject, Predicate, and Object.
The Existence split is designed to evaluate the object recognition ability while the
remainingsplitsfocusontheevaluationofrelationcomprehension.Aqualitativecom-
parison in abnormal data between LLaVA-1.5 and ASMv2 is shown at the bottom.
3.2 Circular-based Relation Probing Evaluation
In this section, we introduce CRPE, a benchmark designed to quantitatively
evaluate the object recognition and relation comprehension capabilities of mod-
els. The evaluation is formulated as single-choice questions. For a robust evalu-
ation, we adopt CircularEval [51] as our evaluation strategy. Under this setting,
a question is considered as correctly answered only when the model consistently
predictsthecorrectanswerineachoftheN iterations,withN correspondingto
the number of choices. In each iteration, a circular shift is applied to both the
choices and the answer to form a new query for the model.
As shown in Fig. 4, each sample in our benchmark consists of an image and
a single-choice question with one correct answer and three wrong answers. The
location annotations [45] and the triplets (subject, predicate, object) in
the relation annotations [84] are utilized to generate the evaluation data. We
construct four evaluation splits, including (1) the Existence split: the question
ofthissplitis“Which of the following objects exists in the images?”.Thecorrect
answer is sampled from the semantic tags that exist in the image while the
incorrect answer is sampled from those not exist in the image. (2) the Subject
split:wegeneratethequestionbasedonthetemplate“What is <predicate> the
<object>?” and consider the subject in the triplet as the correct answer. The
negative subjects are sampled from other semantic tags that exist in the image.
(3) the Predicate split: we generate the question based on the template “What
istherelationbetween<subject>and<object>?” andconsiderthepredicatein
thetripletasthecorrectanswer.Thenegativepredicatesarerandomlysampled.
Only the predicates satisfying P(p|s) > 0 and P(p|o) > 0 can be sampled,
wherep,s,orefertopredicates,subjects,andobjects,separately.(4)theObject
EPRC
lamronbA8 W. Wang et al.
split: we generate the question based on the template “What is the <subject>
<predicate>?” andconsidertheobjectinthetripletasthecorrectanswer.The
negative objects are sampled from other semantic tags that exist in the image.
To avoid reference ambiguity, we ensure that the semantic tags of the subject
and object in each triplet are distinct in the image. We also manually verify the
generated samples and filter those with ambiguous questions or choices.
Additionally, to evaluate the dependency on language priors, we further in-
clude abnormal data in the Predicate split, which depict relations that are rare
butreasonableintherealworld.Specifically,wefirstselectrelationtripletswith
minimal P(p|s,o) and then employ DALLE-3 [3] to generate corresponding im-
ages for these triplets. Considering that the generated images might not match
the specified triplets exactly, we perform a manual filtering process for these
triplet-imagepairstoensuredataquality.Afterthat,wegeneratetheevaluation
datausingthemethodmentionedabove.Fig.4showsabnormalexamplesatthe
bottomandmoreexamplesoftheseabnormaldataarepresentedinAppendixD.
4 The All-Seeing Model v2
ASMv2 is a powerful Multi-modal Large Language Model (MLLM), which in-
tegrates the Relation Conversation (ReC) ability while maintaining powerful
general capabilities. Specifically, it follows the model architecture of LLaVA-
1.5 [49], comprising a vision encoder, a vision-language connector, and a lan-
guage model. This model can deal with three types of relation tasks, including
(1)RelationConversation,whichrequiresthemodeltolinkallobjectsandpred-
icates mentioned in the response to the corresponding regions in the image. (2)
Open-ended Scene Graph Generation, which requires the model to generate a
scene graph based on the given image in an open-ended manner; (3) Predicate
Classification, which requires the model to generate a scene graph given the
ground-truth object labels and localization; In addition, our model is also capa-
ble of multi-modality dialogue tasks such as Image Captioning, Visual Question
Answering, and Multi-turn conversation. Since the ReC task requires the model
tolinktheobjectsandpredicatestothecorrespondingregionsintheimage,our
ASMv2 is also endowed with grounding and referring capabilities and exhibits
state-of-the-art performance on region-level tasks, including Referring Expres-
sion Comprehension, Region Captioning, and Referring Question Answering.
4.1 Relation Conversation
In this section, we elaborate on the formulation of ReC. Our objective is to
propose a task that can enhance the relation comprehension ability while main-
taining grounding, referring, and other general capabilities.
As depicted in Fig. 5, we represent the sentence in the relation conversa-
tion as a text sequence. Specifically, our relation conversation marks the object
and predicate in the sentence using <ref></ref> and <pred></pred>, respec-
tively. Each marked object is followed by a bounding box, indicating its lo-
calization. Similarly, each predicate is followed by two bounding boxes, whichThe All-Seeing Project V2 9
(bus, driving on, road)
A tall red <ref>bus</ref><box>[[290, 202, 835, 851]]</box> is <pred>driving on</pred><box>[[290, 202, 835, 851]]</box>
<box>[[0, 604, 984, 999]]</box> a <ref>road</ref><box>[[0, 604, 984, 999]]</box>.
Fig.5:DataformulationforRelationConversation.Eachobjectismarkedwith
<ref></ref> and followed by a bounding box indicating its location while each pred-
icate is marked with <pred></pred> and followed by two bounding boxes referring to
its subjects and objects.
specificallyrefertothesubjectsandobjectsofthepredicate.Allboundingboxes
are normalized to integer values within the range [0, 1000) and formatted as:
<box>[[x ,y ,x ,y ]]</box>. Please refer to Appendix A for more details.
1 1 2 2
Notably, the response in the relation conversation can be easily parsed into
a scene graph. In a typical scene graph, each node denotes an object in the
scene grounded by a bounding box with a semantic label, and each directed
edge denotes the relation between a pair of objects with a predicate label. By
utilizing the prediction of bounding boxes for each object (serving as semantic
tags for nodes) and those for subjects and objects related to each predicate
(serving as nodes, edges, and predicate labels), the generated ReC sentence can
be naturally converted into a scene graph. Nodes without semantic tags will be
labeledasUnknown.ToconverttheresponseshowninFig.5intoascenegraph,
wefirstparsetheobjectsmarkedby“<ref></ref>” andassignthemarkedtext
as the semantic tag of the following bounding box. Here, we assign “bus” and
“road” as the semantic tag of the bounding box highlighted in red and green,
separately. Then we extract the predicate label marked by “<pred></pred>”
(i.e., “driving on”) and the box coordinates of the subjects and objects of it
(i.e., bounding boxes highlighted with bold underline). After that, we utilize
these box coordinates as keys to match their semantic tags. In this example,
the bounding box for the subject of “driving on” is matched with the box
highlightedinred,thereforethesubjectof“driving on” is“bus”.Similarly,the
object of it is “road”. Hence, we obtain the parsed triplet (bus, driving on,
road). Note that the bounding boxes of the subject and object are also part of
the triplet while we omit them for simplicity.
Compared with the traditional Scene Graph Generation task, our ReC task
exhibits three advantages: (1) More flexible. Models trained on our proposed
Relation Conversation task can be naturally adapted to the Scene Graph Gen-
eration task in an open-ended manner. (2) Open-World. Benefiting from the
open-ended generation manner, models trained on ReC have the potential to
generalize to previously unseen predicate labels in the Scene Graph Genera-
tion task. (3) More general. ReC requires models to generate a text response
and link all mentioned objects to their corresponding regions within the image,
therebymaintainingthegrounding,referring,andgeneralcapabilitiesofMLLMs
and broadening the applicability of these models in real-world scenarios.10 W. Wang et al.
4.2 Model Training
The training process of ASMv2 is divided into two stages, with each stage com-
prising a pre-training phase and an instruction-tuning phase. The first stage
is designed to enable the model to effectively understand visual information at
the image level. The pre-training phrase utilizes 595K samples from CC3M [69]
filtered by LLaVA [50] while the instruction-tuning phrase utilizes a blend of
665K samples from LLaVA-Mix [49]. We update the data format of the region-
level data [30,31,57] in LLaVA-Mix to the format introduced in Sec. 4.1. The
second stage trains the model with a mixture of image-level data and region-
leveldata,whichenablesthemodeltocomprehendthevisualinformationatthe
regionlevel,facilitatingeffectivegroundingofobjectsandpredicateswithinsen-
tences.Thepre-trainingphraseemploys5MsamplesfromCC12M[69]filteredby
BLIP[36],10MfilteredsamplesfromAS-1B[80],and15Mfilteredsamplesfrom
GRiT [62]. The instruction-tuning phase employs 4M samples collected from a
variety of sources, including image-level datasets [4,9,12,29,47–50,56,63,99],
region-leveldatasets[30,31,57,80,91,100]andourproposedAS-V2dataset.The
summary of these datasets is presented in Tab. 10.
5 Experiments
In this section, we first compare our ASMv2 with leading Multi-modal Large
Language Models (MLLMs) on representative vision-language benchmarks in
Sec.5.1.Inadditiontotheseimage-levelbenchmarks,wealsoevaluateASMv2on
threerepresentativeregion-leveltasksinSec.5.2.Afterthat,ASMv2isevaluated
ontheOpen-endedSceneGraphGenerationtask[84]inSec.5.3.Theresultsand
analyses of our proposed CRPE are presented in Sec. 5.4. Note that we utilize a
consistent checkpoint for all evaluations.
5.1 Results on General Benchmarks
To evaluate the general ability of ASMv2, we perform a comprehensive com-
parison with leading MLLMs in Tab. 1. Benefiting from the stronger relation
comprehensionability,ASMv2exhibitsSoTAperformanceonthesebenchmarks.
Results of Visual Question Answering. OngeneralVQAbenchmarks,such
as VQAv2 [20] and GQA [24], our ASMv2 demonstrates superior overall perfor-
mance compared to LLaVA-1.5 [49] and VILA [44]. On the VQAv2 dataset, our
ASMv2 outperforms the LLaVA-1.5-13B by 1.0 points. Besides, our model also
achieves competitive performance with baselines on text-oriented VQA bench-
marks, including VizWiz-VQA [22] and TextVQA [72].
ResultsofMulti-modalbenchmarks. Inrecentcomprehensivebenchmarks,
which consist of a wide range of sub-tasks covering various fine-grained capa-
bilities, our model significantly outperforms the current SoTA MLLMs, such asThe All-Seeing Project V2 11
Table1:Resultson12generalvisual-languagebenchmarks.Benchmarknames
are abbreviated due to space limits. VQA-v2 [20]; GQA [24]; VisWiz [22]; SQAI:
ScienceQA-IMG [56]; VQAT: TextVQA [72]; POPE [40]; MME [18]; MMB: MM-
Bench [51]; MMBCN: MMBench-Chinese [51]; SEED: SEED-Bench [33]; LLaVAW:
LLaVA-Bench (In-the-Wild) [50]; MM-Vet [90]. ∗The training images of the datasets
are observed during training. The best performances are marked bold.
Model VQAv2GQA VisWizSQAI VQATPOPE MME MMBMMBCNSEEDLLaVAWMM-Vet
BLIP-2[35] 41.0 41.0 19.6 61.0 42.5 85.3 1293.8 - - 46.4 38.1 22.4
InstructBLIP-7B[15] - 49.2 34.5 60.5 50.1 - - 36.0 23.7 53.4 60.9 26.2
InstructBLIP-13B[15]- 49.5 33.4 63.1 50.7 78.9 1212.8 - - - 58.2 25.6
Shikra[8] 77.4* - - - - - - 58.8 - - - -
IDEFICS-9B[27] 50.9 38.4 35.5 - 25.9 - - 48.2 25.2 - - -
IDEFICS-80B[27] 60.0 45.2 36.0 - 30.9 - - 54.5 38.1 - - -
Qwen-VL[2] 78.8* 59.3* 35.2 67.1 63.8 - - 38.2 7.4 56.3 - -
Qwen-VL-Chat[2] 78.2* 57.5* 38.9 68.2 61.5 - 1487.5 60.6 56.7 58.2 - -
LLaVA-1.5-7B[49] 78.5* 62.0* 50.0 66.8 58.2 85.9 1510.7 64.3 58.3 58.6 63.4 30.5
LLaVA-1.5-13B[49] 80.0* 63.3* 53.6 71.6 61.3 85.9 1531.3 67.7 63.6 61.6 70.7 35.4
VILA-7B[44] 79.9* 62.3* 57.8 68.2 64.4 85.5 1533.0 68.9 61.7 61.1 69.7 34.9
VILA-13B[44] 80.8* 63.3* 60.6 73.7 66.6 84.2 1570.1 70.3 64.3 62.8 73.0 38.8
ASMv2(ours) 81.0* 63.9* 58.1 87.1* 60.2 86.3 1621.0 74.4 64.3 66.3 78.9 41.3
Table 2: Accuracy scores on the Referring Expression Comprehension task.
RefCOCO RefCOCO+ RefCOCOg
Model Avg.
Val Test-ATest-B Val Test-ATest-B Val Test
OFA-L [79] 79.96 83.67 76.39 68.29 76.00 61.75 67.57 67.50 72.64
VisionLLM-H [81] - 86.70 - - - - - - -
Shikra-7B [8] 87.01 90.61 80.24 81.60 87.36 72.12 82.27 82.19 82.93
Shikra-13B [8] 87.83 91.11 81.81 82.89 87.79 74.41 84.64 83.16 84.21
Qwen-VL-7B [2] 88.55 92.27 84.51 82.82 88.59 76.79 85.96 86.32 85.73
MiniGPT-V2-7B [7] 88.06 91.29 84.30 79.58 85.52 73.32 84.19 84.31 83.82
Ferret-7B [85] 87.49 91.35 82.45 80.78 87.38 73.14 83.93 84.76 83.91
Ferret-13B [85] 89.48 92.41 84.36 82.81 88.14 75.17 85.83 86.34 85.57
ASMv2 (ours) 90.56 94.24 86.24 84.81 90.83 76.89 87.5288.26 87.42
LLaVA-1.5[49]andVILA[44].Specifically,ourmodelachievesanoverallscoreof
74.4onMMBenchand1621.0onMME,surpassingVILAby4.1pointsand50.9
points separately. Besides, ASMv2 also exhibits state-of-the-art performance on
SEED [33], LLaVA-Bench [50], and MM-Vet [90], outperforming baselines by a
large margin. These results demonstrate the general ability of our model.
5.2 Results on Region-level Benchmarks
To evaluate the region comprehension and grounding capability, we evaluate
ASMv2 on three representative region-level tasks, including (1) Referring Ex-
pression Comprehension [30,57], which requires the model to localize the tar-
get object conditioned on the given description. (2) Region Captioning [31,57],
which requires the model to generate a caption for a certain object in the image
conditioned on the given region. (3) Referring Question Answering [91], which
contains region referring in both questions and answers.12 W. Wang et al.
Table 3: Results on the Region Captioning task.Wemarkthebestperformance
bold and the second-best underlined.
VG [31] RefCOCOg [57]
Model
METEORCIDErMETEORCIDEr
GRiT [82] 17.1 142.0 15.2 71.6
SLR [88] - - 15.4 59.2
SLR+Rerank [88] - - 15.9 66.2
Kosmos-2 [62] - - 14.1 62.3
GPT4RoI-7B [98] 17.4 145.2 - -
GPT4RoI-13B [98] 17.6 146.8 - -
ASM-FT [80] 18.3 148.7 21.8 107.8
ASMv2 (ours) 17.9 153.5 21.7 114.7
Table 4: Results on Visual Commonsense Reasoning. Q, A, and R denote the
Question, Answer, and Rationale. X→Y means that the model needs to select the
correct option for Y conditioned on X. ∗The single-task fine-tuning setting.
Validation Acc. (%)
Method
Q→AQA→RQ→AR
ViLBERT [55] 72.4 74.5 54.0
Unicoder-VL [34] 72.6 74.5 54.5
VLBERT [73] 75.5 77.9 58.9
ERNIE-ViL-L [86] 78.5 83.4 65.8
VILLA [19] 78.5 82.6 65.2
*GPT4RoI-7B [98] 87.4 89.6 78.6
ASMv2 (ours) 87.8 88.8 78.4
*ASMv2 (ours) 88.4 89.9 79.4
Results of Referring Expression Comprehension. Our ASMv2 achieves
state-of-the-art performance on the representative REC benchmarks [30,57]. As
shown in Tab. 2, our ASMv2 significantly outperforms current state-of-the-art
MLLMs,includingQwen-VL[2]andFerret[85].Specifically,ourmodelsurpasses
Ferret by an average of 1.85 points across all benchmarks.
ResultsofRegionCaptioning. Ourmodeldemonstratesstate-of-the-artper-
formanceontherepresentativeregioncaptioningbenchmarks,includingVG[31]
and RefCOCOg [57]. As shown in Tab. 3, our model achieves a CIDEr score of
114.7 on RefCOCOg, which surpasses the current state-of-the-art model (i.e.
ASM-FT) by 6.9 points. On the VG dataset, our model also exhibits competi-
tive results compared to the current state-of-the-art model.
ResultsofReferringQuestionAnswering. WeevaluatetheReferringQues-
tionAnswering(RQA)abilityofASMv2ontheVisualCommonsenseReasoning
(VCR) dataset [91], which evaluates the commonsense reasoning abilities in the
form of single-choice questions. The questions and candidate choices in VCR
contain region referring. The results are presented in Tab. 4. Although trainedThe All-Seeing Project V2 13
in a multi-task setting, ASMv2 exhibits competitive performance compared to
thecurrentstate-of-the-artmodel(i.e.,GPT4RoI[98]),whichisfinetunedonthe
VCRdatasetinasingletasksetting.Inaddition,afterthesingletaskfinetuning,
our model outperforms GPT4RoI by 0.8 points.
5.3 Results on Open-ended Scene Graph Generation
In this section, we evaluate the Relation Conversation capability of our model
through the Open-ended Scene Graph Generation task on the Panoptic Scene
Graph(PSG)dataset[84],whichisawidely-usedbenchmarkforthescenegraph
generation.SeeAppendixC.1fortheresultsonthePredicateClassificationtask.
Baselines. Despite the powerful performance, most previous methods [78,84,
103,104] are constrained by pre-defined label sets and struggle to capture a
diverse range of visual concepts from natural language in an open-ended man-
ner. On the other hand, TextPSG [101] explores a methodology for generating
scene graphs in an open-ended manner, which first generates the region propos-
als and then asks BLIP [36] to predict the semantic tags and predicate labels
for these regions auto-regressively. Here, we consider traditional close-set scene
graph generation models and TextPSG as our baseline in OpenSGG.
Metrics. Following the common practice [84,101], we report the triplet Recall
and mean Recall for every predicate category (mRecall) in the OpenSGG task.
Concretely, a scene graph consists of a set of triplets (subject, predicate,
object). A triplet is considered to be correct if the phrase labels are all correct
and the location of the subject and object should match the ground truth with
IoUgreaterthan0.5respectively.Wealsoreport#Tuplestodenotetheaverage
number of predicted tuples for each generated scene graph.
Results. As shown in Tab. 5, our Table 5: Recall scores on PSG. Gray
ASMv2 demonstrates state-of-the-art denotesthatthemodelgeneratesthescene
performance in the OpenSGG task. graphs in a close-ended manner.
Specifically, our ASMv2 significantly Model #TuplesRecallmRecall
outperforms TextPSG by 8.7 points
IMP [83] 20.0 16.5 6.5
in recall while generating a signifi- MOTIFS [93] 20.0 20.0 9.1
cantly fewer average number of tuples VCTree [75] 20.0 20.6 9.7
compared to it (9.2 vs. 100.0). Note GPSNet [46] 20.0 17.8 7.0
PSGFormer [84] 20.0 18.6 16.7
that having more tuples generally im-
pliesanadvantageincomputingrecall. TextPSG [101] 50.0 4.8 -
TextPSG [84] 100.0 5.5 -
When compared to traditional scene
ASMv2 (ours) 9.2 14.2 10.3
graphgenerationmodels,whichgener-
atescenegraphsinaclose-endedman-
ner, ASMv2 also exhibits competitive performance. Despite generating fewer
tuples, our model maintains a competitive recall of 14.2 and a mean recall of14 W. Wang et al.
10.3. Another factor negatively impacting the performance is that our ASMv2
generates scene graphs in an open-ended manner while recall is calculated in an
exact-match manner. Therefore, the triplets (people, standing on, grass)
and (person, standing on, grass) are considered mismatched even though
theyrepresentthesamesemantics.AmoreappropriatemetricfortheOpenSGG
task will be left for future work.
5.4 Results on CRPE
In this section, we evaluate the Table 6: Accuracy scores on CRPE.
relation comprehension ability of
Model Exist.Subj.Pred. Obj. Overall
our ASMv2 and current leading
Qwen-VL[2] 76.34 19.47 26.99 35.36 27.27
MLLMs[2,49]usingourproposed LLaVA-1.5[49] 84.90 37.86 43.68 47.89 43.14
ASMv2(ours) 87.24 50.55 48.46 57.10 52.04
CRPE benchmark. This bench-
mark consists of four splits: Existence, Subject, Predicate, and Object. The Ex-
istencesplitevaluatesthemodels’objectrecognitionabilitywhiletheremaining
splitsaredesignedtoevaluatethemodels’relationcomprehensionability.Inad-
dition to reporting the performance of each split in the benchmark individually,
we also report the average score of the latter three splits as the overall score for
relation comprehension ability.
As shown in Tab. 6, the performance of existing MLLMs on the Existence
questionsissignificantlyhigherthanontheSubject,Predicate,andObjectques-
tions.Thissuggeststhatthesemodelshaveamorerobustcapabilitytorecognize
objectswithinanimagethantocomprehendtherelationsbetweenthem.Specif-
ically, our ASMv2 shows a remarkable improvement in understanding object re-
lations compared to the other models. For example, ASMv2 achieves an overall
accuracy of 52.04, which is significantly higher than the 43.14 of LLaVA-1.5
andthe27.27ofQwen-VL.Theseresultsdemonstratethatourcancomprehend
the relations between the objects within the image better, benefiting from the
training of relation conversation data.
6 Conclusion
In this paper, we propose a novel task, termed Relation Conversation (ReC), to
challenge the model to understand the relations between the objects within the
image.WeconstructtheAll-SeeingDatasetV2(AS-V2),whichisahigh-quality
ReC dataset to unlock the ReC ability of Multi-modal Large Language Models
(MLLMs) and the CRPE to quantitatively evaluate the relation comprehension
ability. Leveraging AS-V2 and other general multimodal corpora for training,
weintroducetheAll-SeeingModelv2(ASMv2),whichexhibitsstrongerrelation
comprehension ability compared to existing leading MLLMs and achieves state-
of-the-art performance on the Open-ended Scene Graph Generation task and
various general image-level and region-level tasks. We hope that our work can
inspire more future research and contribute to the evolution towards artificial
general intelligence, equipping artificial intelligence systems with an “all-seeing
eye” to achieve a deeper understanding of the world.The All-Seeing Project V2 15
References
1. Andreas, J., Rohrbach, M., Darrell, T., Klein, D.: Neural module networks. In:
ProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition.
pp. 39–48 (2016) 5
2. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., Zhou,
J.:Qwen-vl:Afrontierlargevision-languagemodelwithversatileabilities.arXiv
preprint arXiv:2308.12966 (2023) 2, 4, 11, 12, 14
3. Betker, J., Goh, G., Jing, L., Brooks, T., Wang, J., Li, L., Ouyang, L., Zhuang,
J., Lee, J., Guo, Y., et al.: Improving image generation with better captions.
Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf (2023) 8
4. Biten, A.F., Tito, R., Mafla, A., Gomez, L., Rusinol, M., Valveny, E., Jawahar,
C., Karatzas, D.: Scene text visual question answering. In: Proceedings of the
IEEE/CVF international conference on computer vision. pp. 4291–4301 (2019)
10, 28
5. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J.D., Dhariwal, P., Nee-
lakantan, A., Shyam, P., Sastry, G., Askell, A., et al.: Language models are few-
shot learners. NeurIPS (2020) 4, 27
6. Caesar,H.,Uijlings,J.,Ferrari,V.:Coco-stuff:Thingandstuffclassesincontext.
In: CVPR (2018) 5
7. Chen,J.,Zhu,D.,Shen,X.,Li,X.,Liu,Z.,Zhang,P.,Krishnamoorthi,R.,Chan-
dra, V., Xiong, Y., Elhoseiny, M.: Minigpt-v2: large language model as a unified
interfaceforvision-languagemulti-tasklearning.arXivpreprintarXiv:2310.09478
(2023) 11
8. Chen, K., Zhang, Z., Zeng, W., Zhang, R., Zhu, F., Zhao, R.: Shikra: Unleash-
ingmultimodalllm’sreferentialdialoguemagic.arXivpreprintarXiv:2306.15195
(2023) 2, 4, 11
9. Chen, L., Li, J., Dong, X., Zhang, P., He, C., Wang, J., Zhao, F., Lin, D.:
Sharegpt4v: Improving large multi-modal models with better captions. arXiv
preprint arXiv:2311.12793 (2023) 10, 28
10. Chen,X.,Fang,H.,Lin,T.Y.,Vedantam,R.,Gupta,S.,Dollár,P.,Zitnick,C.L.:
Microsoft coco captions: Data collection and evaluation server. arXiv preprint
arXiv:1504.00325 (2015) 3, 5
11. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Zhong, M., Zhang, Q.,
Zhu, X., Lu, L., Li, B., Luo, P., Lu, T., Qiao, Y., Dai, J.: Internvl: Scaling up
vision foundation models and aligning for generic visual-linguistic tasks. arXiv
preprint arXiv:2312.14238 (2023) 1, 4
12. Clark,C.,Gardner,M.:Simpleandeffectivemulti-paragraphreadingcomprehen-
sion. arXiv preprint arXiv:1710.10723 (2017) 10, 28
13. Cong,Y.,Yang,M.Y.,Rosenhahn,B.:Reltr:Relationtransformerforscenegraph
generation. IEEE Transactions on Pattern Analysis and Machine Intelligence
(2023) 5
14. Dai, B., Zhang, Y., Lin, D.: Detecting visual relationships with deep relational
networks. In: CVPR (2017) 5
15. Dai, W., Li, J., Li, D., Huat, A., Zhao, J., Wang, W., Li, B., Fung, P., Hoi, S.:
Instructblip: Towards general-purpose vision-language models with instruction
tuning. arXiv preprint arXiv:2305.06500 (2023) 11
16. Dupty, M.H., Zhang, Z., Lee, W.S.: Visual relationship detection with low rank
non-negative tensor decomposition. In: AAAI (2020) 516 W. Wang et al.
17. Fang, Y., Wang, W., Xie, B., Sun, Q., Wu, L., Wang, X., Huang, T., Wang, X.,
Cao, Y.: Eva: Exploring the limits of masked visual representation learning at
scale. arXiv preprint arXiv:2211.07636 (2022) 4
18. Fu,C.,Chen,P.,Shen,Y.,Qin,Y.,Zhang,M.,Lin,X.,Qiu,Z.,Lin,W.,Yang,J.,
Zheng, X., et al.: Mme: A comprehensive evaluation benchmark for multimodal
large language models. arXiv preprint arXiv:2306.13394 (2023) 4, 11
19. Gan, Z., Chen, Y.C., Li, L., Zhu, C., Cheng, Y., Liu, J.: Large-scale adversarial
training for vision-and-language representation learning. NeurIPS (2020) 12
20. Goyal,Y.,Khot,T.,Summers-Stay,D.,Batra,D.,Parikh,D.:Makingthevinvqa
matter: Elevating the role of image understanding in visual question answering.
In: CVPR (2017) 10, 11, 28
21. Gu, J., Zhao, H., Lin, Z., Li, S., Cai, J., Ling, M.: Scene graph generation with
external knowledge and image reconstruction. In: CVPR (2019) 5
22. Gurari,D.,Li,Q.,Stangl,A.J.,Guo,A.,Lin,C.,Grauman,K.,Luo,J.,Bigham,
J.P.: Vizwiz grand challenge: Answering visual questions from blind people. In:
CVPR (2018) 10, 11
23. Hu,Y.,Chen,S.,Chen,X.,Zhang,Y.,Gu,X.:Neuralmessagepassingforvisual
relationship detection. In: ICMLW (2019) 5
24. Hudson,D.A.,Manning,C.D.:Gqa:Anewdatasetforreal-worldvisualreasoning
and compositional question answering. In: CVPR (2019) 10, 11, 28
25. Hung,Z.S.,Mallya,A.,Lazebnik,S.:Contextualtranslationembeddingforvisual
relationshipdetectionandscenegraphgeneration.IEEEtransactionsonpattern
analysis and machine intelligence pp. 3820–3832 (2020) 5
26. Hwang, S.J., Ravi, S.N., Tao, Z., Kim, H.J., Collins, M.D., Singh, V.: Tensorize,
factorize and regularize: Robust visual relationship learning. In: CVPR (2018) 5
27. IDEFICS: Introducing idefics: An open reproduction of state-of-the-art visual
language model. https://huggingface.co/blog/idefics (2023) 11
28. Jia,C.,Yang,Y.,Xia,Y.,Chen,Y.T.,Parekh,Z.,Pham,H.,Le,Q.,Sung,Y.H.,
Li, Z., Duerig, T.: Scaling up visual and vision-language representation learning
with noisy text supervision. In: Int. Conf. Mach. Learn. (2021) 4
29. Johnson, J., Hariharan, B., Van Der Maaten, L., Fei-Fei, L., Lawrence Zitnick,
C., Girshick, R.: Clevr: A diagnostic dataset for compositional language and ele-
mentary visual reasoning. In: CVPR (2017) 5, 10, 28
30. Kazemzadeh, S., Ordonez, V., Matten, M., Berg, T.: Referitgame: Referring to
objects in photographs of natural scenes. In: EMNLP (2014) 2, 4, 10, 11, 12, 28
31. Krishna, R., Zhu, Y., Groth, O., Johnson, J., Hata, K., Kravitz, J., Chen, S.,
Kalantidis, Y., Li, L.J., Shamma, D.A., et al.: Visual genome: Connecting lan-
guageandvisionusingcrowdsourceddenseimageannotations.Internationaljour-
nal of computer vision (2017) 2, 5, 10, 11, 12, 28
32. Lai, X., Tian, Z., Chen, Y., Li, Y., Yuan, Y., Liu, S., Jia, J.: Lisa: Reasoning
segmentation via large language model. arXiv preprint arXiv:2308.00692 (2023)
4
33. Li,B.,Wang,R.,Wang,G.,Ge,Y.,Ge,Y.,Shan,Y.:Seed-bench:Benchmarking
multimodalllmswithgenerativecomprehension.arXivpreprintarXiv:2307.16125
(2023) 11
34. Li,G.,Duan,N.,Fang,Y.,Gong,M.,Jiang,D.:Unicoder-vl:Auniversalencoder
for vision and language by cross-modal pre-training. In: AAAI (2020) 12
35. Li, J., Li, D., Savarese, S., Hoi, S.: Blip-2: Bootstrapping language-image pre-
training with frozen image encoders and large language models. arXiv preprint
arXiv:2301.12597 (2023) 4, 11The All-Seeing Project V2 17
36. Li,J.,Li,D.,Xiong,C.,Hoi,S.:Blip:Bootstrappinglanguage-imagepre-training
for unified vision-language understanding and generation. In: ICML (2022) 10,
13
37. Li, J., Selvaraju, R., Gotmare, A., Joty, S., Xiong, C., Hoi, S.C.H.: Align before
fuse: Vision and language representation learning with momentum distillation.
NeurIPS (2021) 4
38. Li,K.,He,Y.,Wang,Y.,Li,Y.,Wang,W.,Luo,P.,Wang,Y.,Wang,L.,Qiao,Y.:
Videochat: Chat-centric video understanding. arXiv preprint arXiv:2305.06355
(2023) 4
39. Li, R., Zhang, S., He, X.: Sgtr: End-to-end scene graph generation with trans-
former. In: CVPR (2022) 5
40. Li, Y., Du, Y., Zhou, K., Wang, J., Zhao, W.X., Wen, J.R.: Evaluating object
hallucination in large vision-language models. EMNLP (2023) 11
41. Li,Y.,Ouyang,W.,Wang,X.,Tang,X.:Vip-cnn:Visualphraseguidedconvolu-
tional neural network. In: CVPR (2017) 5
42. Li,Y.,Ouyang,W.,Zhou,B.,Wang,K.,Wang,X.:Scenegraphgenerationfrom
objects, phrases and region captions. In: ICCV (2017) 5
43. Liao, W., Rosenhahn, B., Shuai, L., Ying Yang, M.: Natural language guided
visual relationship detection. In: CVPRW (2019) 5
44. Lin, J., Yin, H., Ping, W., Lu, Y., Molchanov, P., Tao, A., Mao, H., Kautz, J.,
Shoeybi, M., Han, S.: Vila: On pre-training for visual language models. arXiv
preprint arXiv:2312.07533 (2023) 10, 11
45. Lin,T.Y.,Maire,M.,Belongie,S.,Hays,J.,Perona,P.,Ramanan,D.,Dollár,P.,
Zitnick, C.L.: Microsoft coco: Common objects in context. In: ECCV (2014) 3,
5, 7
46. Lin,X.,Ding,C.,Zeng,J.,Tao,D.:Gps-net:Graphpropertysensingnetworkfor
scene graph generation. In: CVPR. pp. 3746–3753 (2020) 13, 27
47. Liu, F., Emerson, G., Collier, N.: Visual spatial reasoning. Transactions of the
Association for Computational Linguistics (2023) 5, 10
48. Liu, F., Lin, K., Li, L., Wang, J., Yacoob, Y., Wang, L.: Aligning large multi-
modal model with robust instruction tuning. arXiv preprint arXiv:2306.14565
(2023) 10, 28
49. Liu,H.,Li,C.,Li,Y.,Lee,Y.J.:Improvedbaselineswithvisualinstructiontuning.
arXiv preprint arXiv:2310.03744 (2023) 1, 4, 8, 10, 11, 14
50. Liu, H., Li, C., Wu, Q., Lee, Y.J.: Visual instruction tuning. arXiv preprint
arXiv:2304.08485 (2023) 1, 4, 10, 11, 28
51. Liu, Y., Duan, H., Zhang, Y., Li, B., Zhang, S., Zhao, W., Yuan, Y., Wang, J.,
He,C.,Liu,Z.,etal.:Mmbench:Isyourmulti-modalmodelanall-aroundplayer?
arXiv preprint arXiv:2307.06281 (2023) 4, 7, 11
52. Liu, Z., He, Y., Wang, W., Wang, W., Wang, Y., Chen, S., Zhang, Q., Lai, Z.,
Yang, Y., Li, Q., Yu, J., et al.: Interngpt: Solving vision-centric tasks by inter-
acting with chatgpt beyond language. arXiv preprint arXiv:2305.05662 (2023)
4
53. Loshchilov,I.,Hutter,F.:Decoupledweightdecayregularization.In:ICLR(2019)
28
54. Lu,C.,Krishna,R.,Bernstein,M.,Fei-Fei,L.:Visualrelationshipdetectionwith
language priors. In: ECCV (2016) 4, 5
55. Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic visiolin-
guistic representations for vision-and-language tasks. NeurIPS (2019) 1218 W. Wang et al.
56. Lu, P., Mishra, S., Xia, T., Qiu, L., Chang, K.W., Zhu, S.C., Tafjord, O., Clark,
P., Kalyan, A.: Learn to explain: Multimodal reasoning via thought chains for
science question answering. NeurIPS (2022) 10, 11, 28
57. Mao, J., Huang, J., Toshev, A., Camburu, O., Yuille, A.L., Murphy, K.: Gener-
ation and comprehension of unambiguous object descriptions. In: CVPR (2016)
2, 4, 10, 11, 12, 28
58. Marino, K., Rastegari, M., Farhadi, A., Mottaghi, R.: Ok-vqa: A visual question
answering benchmark requiring external knowledge. In: CVPR (2019) 28
59. Mishra, A., Shekhar, S., Singh, A.K., Chakraborty, A.: Ocr-vqa: Visual question
answering by reading text in images. In: ICDAR. pp. 947–952. IEEE (2019) 28
60. OpenAI: Gpt-4 technical report. arXiv preprint arXiv:2303.08774 (2023) 4
61. OpenAI: Gpt-4v(ision) system card (2023) 4, 5
62. Peng, Z., Wang, W., Dong, L., Hao, Y., Huang, S., Ma, S., Wei, F.: Kosmos-
2: Grounding multimodal large language models to the world. arXiv preprint
arXiv:2306.14824 (2023) 2, 4, 10, 12
63. Plummer, B.A., Wang, L., Cervantes, C.M., Caicedo, J.C., Hockenmaier, J.,
Lazebnik, S.: Flickr30k entities: Collecting region-to-phrase correspondences for
richer image-to-sentence models. In: ICCV (2015) 10
64. Qi, M., Li, W., Yang, Z., Wang, Y., Luo, J.: Attentive relational networks for
mapping images to scene graphs. In: CVPR (2019) 5
65. Radford, A., Kim, J.W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry,
G., Askell, A., Mishkin, P., Clark, J., et al.: Learning transferable visual models
from natural language supervision. In: ICML (2021) 4
66. Rasheed, H., Maaz, M., Shaji, S., Shaker, A., Khan, S., Cholakkal, H., Anwer,
R.M., Xing, E., Yang, M.H., Khan, F.S.: Glamm: Pixel grounding large multi-
modal model. arXiv preprint arXiv:2311.03356 (2023) 4
67. Schwenk, D., Khandelwal, A., Clark, C., Marino, K., Mottaghi, R.: A-okvqa: A
benchmarkforvisualquestionansweringusingworldknowledge.In:ECCV(2022)
28
68. ShareGPT: https://sharegpt.com/ (2023) 28
69. Sharma,P.,Ding,N.,Goodman,S.,Soricut,R.:Conceptualcaptions:Acleaned,
hypernymed, image alt-text dataset for automatic image captioning. In: ACL
(2018) 10
70. Shit, S., Koner, R., Wittmann, B., Paetzold, J., Ezhov, I., Li, H., Pan, J., Shar-
ifzadeh,S.,Kaissis,G.,Tresp,V.,etal.:Relationformer:Aunifiedframeworkfor
image-to-graph generation. In: ECCV (2022) 5
71. Sidorov, O., Hu, R., Rohrbach, M., Singh, A.: Textcaps: a dataset for image
captioningwithreadingcomprehension.In:ECCV.pp.742–758.Springer(2020)
28
72. Singh, A., Natarajan, V., Shah, M., Jiang, Y., Chen, X., Batra, D., Parikh, D.,
Rohrbach, M.: Towards vqa models that can read. In: CVPR (2019) 10, 11
73. Su,W.,Zhu,X.,Cao,Y.,Li,B.,Lu,L.,Wei,F.,Dai,J.:Vl-bert:Pre-trainingof
generic visual-linguistic representations. arXiv preprint arXiv:1908.08530 (2019)
12
74. Suhr, A., Lewis, M., Yeh, J., Artzi, Y.: A corpus of natural language for visual
reasoning. In: Ann. Meeting of the Assoc. for Comput. Linguistics (2017) 5
75. Tang, K., Zhang, H., Wu, B., Luo, W., Liu, W.: Learning to compose dynamic
tree structures for visual contexts. In: CVPR. pp. 6619–6628 (2019) 13, 27
76. Tian, C., Zhu, X., Xiong, Y., Wang, W., Chen, Z., Wang, W., Chen, Y., Lu,
L., Lu, T., Zhou, J., et al.: Mm-interleaved: Interleaved image-text generativeThe All-Seeing Project V2 19
modeling via multi-modal feature synchronizer. arXiv preprint arXiv:2401.10208
(2024) 4
77. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T.,
Rozière, B., Goyal, N., Hambro, E., Azhar, F., et al.: Llama: Open and efficient
foundation language models. arXiv preprint arXiv:2302.13971 (2023) 4
78. Wang,J.,Wen,Z.,Li,X.,Guo,Z.,Yang,J.,Liu,Z.:Pairthenrelation:Pair-net
for panoptic scene graph generation. arXiv preprint arXiv:2307.08699 (2023) 13
79. Wang, P., Yang, A., Men, R., Lin, J., Bai, S., Li, Z., Ma, J., Zhou, C., Zhou,
J.,Yang,H.:Ofa:Unifyingarchitectures,tasks,andmodalitiesthroughasimple
sequence-to-sequence learning framework. In: ICML (2022) 11
80. Wang,W.,Shi,M.,Li,Q.,Wang,W.,Huang,Z.,Xing,L.,Chen,Z.,Li,H.,Zhu,
X., Cao, Z., et al.: The all-seeing project: Towards panoptic visual recognition
and understanding of the open world. arXiv preprint arXiv:2308.01907 (2023) 2,
4, 10, 12, 28
81. Wang,W.,Chen,Z.,Chen,X.,Wu,J.,Zhu,X.,Zeng,G.,Luo,P.,Lu,T.,Zhou,
J.,Qiao,Y.,etal.:Visionllm:Largelanguagemodelisalsoanopen-endeddecoder
for vision-centric tasks. arXiv preprint arXiv:2305.11175 (2023) 2, 4, 11
82. Wu, J., Wang, J., Yang, Z., Gan, Z., Liu, Z., Yuan, J., Wang, L.: Grit: A
generative region-to-text transformer for object understanding. arXiv preprint
arXiv:2212.00280 (2022) 12
83. Xu, D., Zhu, Y., Choy, C.B., Fei-Fei, L.: Scene graph generation by iterative
message passing. In: CVPR (2017) 13, 27
84. Yang,J.,Ang,Y.Z.,Guo,Z.,Zhou,K.,Zhang,W.,Liu,Z.:Panopticscenegraph
generation. In: ECCV (2022) 3, 5, 7, 10, 13, 27
85. You,H.,Zhang, H.,Gan,Z.,Du, X.,Zhang,B.,Wang, Z.,Cao,L.,Chang, S.F.,
Yang, Y.: Ferret: Refer and ground anything anywhere at any granularity. arXiv
preprint arXiv:2310.07704 (2023) 11, 12
86. Yu,F.,Tang,J.,Yin,W.,Sun,Y.,Tian,H.,Wu,H.,Wang,H.:Ernie-vil:Knowl-
edge enhanced vision-language representations through scene graphs. In: AAAI
(2021) 12
87. Yu, J., Wang, Z., Vasudevan, V., Yeung, L., Seyedhosseini, M., Wu, Y.:
Coca: Contrastive captioners are image-text foundation models. arXiv preprint
arXiv:2205.01917 (2022) 4
88. Yu,L.,Tan,H.,Bansal,M.,Berg,T.L.:Ajointspeaker-listener-reinforcermodel
for referring expressions. In: CVPR (2017) 12
89. Yu, Q., Sun, Q., Zhang, X., Cui, Y., Zhang, F., Wang, X., Liu, J.: Capsfusion:
Rethinking image-text data at scale. arXiv preprint arXiv:2310.20550 (2023) 4
90. Yu,W.,Yang,Z.,Li,L.,Wang,J.,Lin,K.,Liu,Z.,Wang,X.,Wang,L.:Mm-vet:
Evaluating large multimodal models for integrated capabilities. arXiv preprint
arXiv:2308.02490 (2023) 11
91. Zellers,R.,Bisk,Y.,Farhadi,A.,Choi,Y.:Fromrecognitiontocognition:Visual
commonsense reasoning. In: CVPR (2019) 2, 10, 11, 12, 28
92. Zellers,R.,Yatskar,M.,Thomson,S.,Choi,Y.:Neuralmotifs:Scenegraphpars-
ing with global context. In: CVPR (2018) 5
93. Zellers,R.,Yatskar,M.,Thomson,S.,Choi,Y.:Neuralmotifs:Scenegraphpars-
ing with global context. In: CVPR (2018) 13, 27
94. Zhai,X.,Wang,X.,Mustafa,B.,Steiner,A.,Keysers,D.,Kolesnikov,A.,Beyer,
L.: Lit: Zero-shot transfer with locked-image text tuning. In: Proceedings of the
IEEE/CVFConferenceonComputerVisionandPatternRecognition.pp.18123–
18133 (2022) 420 W. Wang et al.
95. Zhang,A.,Zhao,L.,Xie,C.W.,Zheng,Y.,Ji,W.,Chua,T.S.:Next-chat:Anlmm
for chat, detection and segmentation. arXiv preprint arXiv:2311.04498 (2023) 4
96. Zhang, H., Kyaw, Z., Chang, S.F., Chua, T.S.: Visual translation embedding
network for visual relation detection. In: CVPR (2017) 5
97. Zhang, J., Kalantidis, Y., Rohrbach, M., Paluri, M., Elgammal, A., Elhoseiny,
M.: Large-scale visual relationship understanding. In: AAAI (2019) 5
98. Zhang, S., Sun, P., Chen, S., Xiao, M., Shao, W., Zhang, W., Chen, K., Luo,
P.: Gpt4roi: Instruction tuning large language model on region-of-interest. arXiv
preprint arXiv:2307.03601 (2023) 4, 12, 13
99. Zhang,Y.,Zhang,R.,Gu,J.,Zhou,Y.,Lipka,N.,Yang,D.,Sun,T.:Llavar:En-
hancedvisualinstructiontuningfortext-richimageunderstanding.arXivpreprint
arXiv:2306.17107 (2023) 10, 28
100. Zhao, B., Wu, B., Huang, T.: Svit: Scaling up visual instruction tuning. arXiv
preprint arXiv:2307.04087 (2023) 10, 28
101. Zhao, C., Shen, Y., Chen, Z., Ding, M., Gan, C.: Textpsg: Panoptic scene graph
generation from textual descriptions. In: ICCV (2023) 13
102. Zheng, S., Chen, S., Jin, Q.: Visual relation detection with multi-level attention.
In: ACM MM (2019) 5
103. Zhong,Y.,Shi,J.,Yang,J.,Xu,C.,Li,Y.:Learningtogeneratescenegraphfrom
natural language supervision. In: ICCV (2021) 13
104. Zhou,Z.,Shi,M.,Caesar,H.:Vlprompt:Vision-languagepromptingforpanoptic
scene graph generation. arXiv preprint arXiv:2311.16492 (2023) 5, 13
105. Zhu, D., Chen, J., Shen, X., Li, X., Elhoseiny, M.: Minigpt-4: Enhancing vision-
language understanding with advanced large language models. arXiv preprint
arXiv:2304.10592 (2023) 4
106. Zhu, Y., Groth, O., Bernstein, M., Fei-Fei, L.: Visual7w: Grounded question an-
swering in images. In: Proceedings of the IEEE conference on computer vision
and pattern recognition. pp. 4995–5004 (2016) 28The All-Seeing Project V2 21
(people, standing on, grass) (people, standing on, grass)
Two <ref>people</ref><box>[[101, 252, 430, 963], [539, 246, 826, 984]]</box> <pred>standing on</pred><box>[[101, 252, 430, 963],
[539, 246, 826, 984]]</box><box>[[0, 444, 999, 999]]</box> the <ref>grass</ref><box>[[0, 444, 999, 999]]</box>.
Fig.6: A complex example for the formulation of Relation Conversation.
A Relation Conversation
In this section, we introduce more details about the formulation of Relation
Conversation. As depicted in Fig. 6, when a certain text span is associated with
multiple regions, these bounding boxes are formatted as:
<box>[[x1,y1,x1,y1], ..., [xn,yn,xn,yn]]</box>,
1 1 2 2 1 1 2 2
where [xi,yi,xi,yi] denotes the i-th bounding box linked to the object or
1 1 2 2
predicate. For a specific predicate, the subject and object must be linked to an
equal number of bounding boxes. Otherwise, one of them must be linked to just
oneboundingboxandthuscanbebroadcasttomatchthecountofanotherone.
AnexampleisshowninFig.6.AsdiscussedinSec.4.1,toparsethisexample
intoascenegraph,wefirstassignthesemantictag“people” totheboundingbox
highlightedinredandblue.Similarly,weassignthesemantictag“grass” tothe
boundingboxhighlightedingreen.Wethenextractthepredicatelabelenclosed
in“<pred></pred>” (i.e.,“standing on”)andtheboxcoordinatesofitssubjects
andobjects(i.e.,boundingboxeshighlightedwithbold underline).Afterthat,
weutilizetheseboxcoordinatesaskeystomatchtheirrespectivesemantictags.
Considering that two subjects are linked to the predicate while only one object
is linked, we broadcast the object to match the number of subjects. Given N
subjects and N objects, we pack them into N tuples in order, where each tuple
consists of one subject and one object. In this example, we obtain two tuples,
resultingintwoparsedtriplet(people, standing on, grass),eachconnected
to a people with different bounding boxes.
B The All-Seeing Dataset v2
More data examples of AS-V2 are shown in Figs. 7 to 9. Besides, prompts used
to generate detailed description data are shown in Tabs. 7 and 822 W. Wang et al.
In the image, a group of zebras are gathered in a grassy field. One
zebra appears to be kissing another, while a third zebra stands beside
them. All the zebras are standing on the grass. There is also a bird
standing on the grass. A tree is visible in the background, adding to
the natural setting of the scene.
Describe the image in detail.
zebra zebra
Kissing
Standing on
grass
Standing on
Standing on
(zebra, standing on, grass)
(zebra, beside, zebra) (zebra, kissing, zebra) (zebra, standing on, grass) (bird, standing on, grass) zebra bird zebra
(a)
In the image, a young man is jumping over another person who is
crouching on a skateboard. The skateboarder in the air is performing a
trick over his friend, who is also on a skateboard that is on the road.
The scene is set against a backdrop of lush trees with the sky over
them. There is also a strip of pavement and a patch of grass visible in
the image. The young man in the air is in front of the trees, creating a
dynamic and action-packed scene.
Describe the image in detail.
sky tree
Over
Jumping over
In
front
of
person young man
On
(young man, in front of, trees) (skateboard, on, road) (young man, jumping, person) (sky, over, trees) road skateboard
(b)
In the image, multiple people are standing on a grass-covered field.
Some of them are engaged in kite flying, as one person is looking at
another, while another person is looking at and pulling a kite. The
park is filled with lush grass and bordered by dirt. Trees tree line the
background, and a section of a building is also visible. The
environment looks peaceful, with the activity centered around the kite
flying on this overcast day.
Describe the image in detail.
kite person
Looking Pat ulling Looking
at
Standing on
person person
Standing on
(person, standing on, grass)
(person, looking at, person) (person, pulling, kite) (person, looking at, kite) (person, standing on, grass) grass
(c)
Fig.7: Data Examples of Detailed Description task in AS-V2.
tupnI
noitazilausiV
tupnI
noitazilausiV
tupnI
noitazilausiV
tuptuO
tuptuO
tuptuO
BesideThe All-Seeing Project V2 23
One man holding an umbrella talking to another man while both are
standing on a large set of stairs
Provide a short description
for this region.
stairs
Standing on Standing on
man man
Talking to
Holding
(man, holding, umbrella) (man, talking to, man) (man, standing on, stairs) umbrella
(man, standing on, stairs)
(a)
A woman holding one bear cub while sitting on the pavement and
petting another bear cub.
Provide a short description
for this region.
bear cub
Holding
pavement woman
Sitting on
Petting
bear cub
(woman, holding, bear cub) (woman, sitting on, pavement) (woman, petting, bear cub)
(b)
A cat lying on the table underneath an elevated laptop and looking
at a keyboard.
Provide a short description
for this region.
keyboard
Looking at
laptop Underneath cat
Laying on
table
(cat, laying on, table) (cat, underneath, laptop) (cat, looking at, keyboard)
(c)
Fig.8: Data Examples of Region Captioning task in AS-V2.
tupnI
noitazilausiV
tupnI
noitazilausiV
tupnI
noitazilausiV
tuptuO
tuptuO
tuptuO24 W. Wang et al.
There are three visible tennis rackets in the image. The second, third,
and fourth men from the left are each holding one racket.
How many tennis rackets
are visible in the image, and
who is holding them?
man man man
Holding Holding Holding
tennis tennis tennis
(man, holding, tennis racket) (man, holding, tennis racket) (man, holding, tennis racket) racket racket racket
(a)
In the image, one person is holding an umbrella, while the other
person appears to be dressed in a Pikachu costume. They are both
standing on the grass and are positioned beside each other.
What are the two people
doing in the image?
grass
Standing on Standing on
person person
Beside
Holding
umbrella
(person, holding, umbrella) (person, standing on, grass) (person, standing on, grass) (person, beside, person)
(b)
There are four double decker buses visible in the image. They are all
driving on the road, seemingly in a line, as part of the traffic flow on
a busy street.
How many buses are visible
in the image, and what are
they doing?
double decker
bus
double decker Driving
bus on
Driving Driving
onroad douo bn
le decker
Drivingbus
(double decker bus, (double decker bus, (double decker bus, (double decker bus, double decker on
driving on, road) driving on, road) driving on, road) driving on, road) bus
(c)
Fig.9: Data Examples of Conversation task in AS-V2. Due the space limita-
tions, we exhibit only one turn for each conversation.
tupnI
noitazilausiV
tupnI
noitazilausiV
tupnI
noitazilausiV
tuptuO
tuptuO
tuptuOThe All-Seeing Project V2 25
Table 7: For each query, the system info explains the task description and the in-
context-learning examples are presented in the form of multi-turn conversation. For
each turn, the input query[‘context’] consists of (1) the image to be annotated,
(2) the captions annotations of this image, (3) the location annotations, as well
as(4)therelation annotations.Theoutputquery[‘context’]comprisestheman-
ually annotated scene graph conversation data. In this example, we provide the task
description for the Detailed Description data in the relation conversation.
messages = [{"role":"system", "content": f"""You are an AI visual
assistant that can analyze a single image. You receive one image and five
sentences, each describing this image you are observing. In addition, specific
object locations within the image are given, along with detailed coordinates.
Thesecoordinatesareintheformofboundingboxes,representedas[x1, y1,
x2, y2] with int numbers ranging from 0 to 999. These values correspond to
the top left x, top left y, bottom right x, and bottom right y. Note that these
coordinates are normalized. Besides, the scene graph of this image is also
provided as a list of tuples. Each tuple is represented as (subject, bounding
box of the subject, object, bounding box of the object, predicate).
Using the provided caption, bounding box, and scene graph information,
describe the scene in a detailed manner. If there are errors in the caption,
please ignore them and do not point them out in your description.
Instead of directly mentioning the bounding box coordinates, utilize this
data to explain the scene using natural language with its bounding box
in the format like "<ref>object</ref><box>[[x1, y1, x2, y2]]</box>".
When mentioning the predicate between two objects, you should men-
tion it in the format like "<pred>predicate</pred><box>[[x1, y1, x2,
y2]]</box><box>[[x3, y3, x4, y4]]</box>", where "<box>[[x1, y1, x2,
y2]]</box>" denotes the bounding box coordinates of the subject and
"<box>[[x3, y3, x4, y4]]</box>" denotes the bounding box coordinates of
the object. Include details like object counts, position of the objects, relative
position between the objects.
When using the information from the caption, coordinates, or scene graph,
directly explain the scene, and do not mention that the information source is
the caption or the bounding box or the scene graph. You should mention all
tuples and predicates included in the scene graph in the generated caption.
Make sure that the box following the <pred>predicate</pred> has already
been mentioned after a <ref>object</ref>."""}
]
for sample in fewshot_samples:
messages.append({"role":"user","content":sample[‘context’]})
messages.append({"role":"assistant","content":sample[‘response’]})
messages.append({"role":"user","content":‘\n’.join(query)})26 W. Wang et al.
Table8:Oneexampletoillustratetheinstruction-followingdata.Thetopblockshows
the contexts such as captions, locations, relations and images used to prompt GPT,
and the bottom block shows the three types of responses. Note that the visual image
is also used to prompt GPT.
Context type 1: Captions
A pretty young lady holding a dark colored umbrella.
A girl in a Pikachu suit standing beside a girl holding an umbrella.
A woman is wearing a raincoat and another woman is holding an umbrella
over her head.
two girls stand in a yard, one holds an umbrella and the other is dressed as
pikachu.
Two women who are standing in the grass.
Context type 2: Locations
person: [101, 252, 430, 963]
person: [539, 246, 826, 984]
grass: [0, 444, 999, 999]
···
Context type 3: Relations
(person, [101, 252, 430, 963], grass, [0, 444, 999, 999], standing on)
(person, [101, 252, 430, 963], person, [539, 246, 826, 984], beside)
(person, [539, 246, 826, 984], grass, [0, 444, 999, 999], standing on)
···
Response type 1: Detailed Description
In the image, two <ref>people</ref><box>[[101, 252, 430, 963], [539,
246, 826, 984]]</box> are <pred>standing on</pred><box>[[101, 252,
430, 963], [539, 246, 826, 984]]</box><box>[[0, 444, 999, 999]]</box> a
<ref>grass</ref><box>[[0, 444, 999, 999]]</box>. ···
Response type 2: Region Captioning
person: [539, 246, 826, 984]
A <ref>girl</ref><box>[[101, 252, 430, 963]]</box> in a Pikachu suit
standing<pred>beside</pred><box>[[101,252,430,963]]</box><box>-
[[539,246,826,984]]</box>a<ref>girl</ref><box>[[539,246,826,984]]-
</box>.
···
Response type 3: Conversation
Question:
Whatarethetwo<ref>people</ref><box>[[101,252,430,963],[539,246,
826, 984]]</box> doing in the image?
—
Answer:
In the image, one <ref>person</ref><box>[[539, 246, 826, 984]]</box>
is <pred>holding</pred><box>[[539, 246, 826, 984]]</box><box>[[326,
117, 976, 435]]</box> an <ref>umbrella</ref><box>[[326, 117, 976,
435]]</box>, ···
···The All-Seeing Project V2 27
Table 9: Recall scores on Predicate Classification task.
Predicate Classification
Method
R@20mR@20R@50mR@50R@100mR@100
IMP [83] 30.5 9.0 35.9 10.5 38.3 11.3
MOTIFS [93] 45.1 19.9 50.5 21.5 52.5 22.2
VCTree [75] 45.9 21.4 51.2 23.1 53.1 23.8
GPSNet [46] 38.8 17.1 46.6 20.2 50.0 21.3
ASMv2 (ours) 17.6 21.4 25.9 34.4 32.6 44.5
(a) (b)
Fig.10: Word Clouds for evaluation data in PSG.Fig.10avisualizesthedistri-
butionofground-truthpredicateswhileFig.10bvisualizesthosepredictedbyASMv2.
C The All-Seeing Model v2
C.1 Predicate Classification
In this section, we evaluate the relation comprehension capability of our model
throughthePredicateClassificationtask(PredCls)onthePanopticSceneGraph
(PSG)dataset[84].ComparedtotheOpen-endedSceneGraphGenerationtask,
PredCls aims to generate a scene graph given the ground-truth object labels
and localization, focusing on the relation prediction performance without the
interference of the detection performance.
Evaluation Setup. Assuming that the number of ground-truth objects is N,
wequerythemodelforN×(N −1)times,consideringeachoftheground-truth
objects as the subject or object. For each query, we ask the model “What is the
relation between the <subject> and the <object>? Answer the question using a
single word or phrase.” andemployavocabularyrankingmethod[5]togenerate
the scores for each predicate label. Following prior works [75,84], we report the
Recall and mean Recall (mRecall) here.
Results. As shown in Tab. 9, our ASMv2 demonstrates competitive perfor-
mance on the Predicate Classification task within the PSG dataset. Specifically,28 W. Wang et al.
Table 10: Details of the instruction-tuning data for ASMv2 in stage 2. We
collect a wide range of high-quality data, totaling approximately 4 million samples.
Task #Samples Dataset
Captioning 124K TextCaps [71], ShareGPT4V [9]
VQAv2 [20], GQA [24], OKVQA [58], A-OKVQA [67],
VQA 314K
ScienceQA [56], CLEVR [29], Visual7W [106]
OCR 157K ST-VQA [4], LLaVAR [99], OCR-VQA [59], DocVQA [12]
Grounding 643K RefCOCO/+/g [30,57]
RegionVQA 2.3M RefCOCOg [57], VG [31], VCR [91], AS-Core [80]
Conversation 500K LLaVA-Instruct [50], SVIT [100], LRV [48], AS-V2 (ours)
Text 40K ShareGPT [68]
our ASMv2 achieves superior performance in mRecall but is inferior in Recall.
For instance, our ASMv2 significantly outperforms VCTree by 11.3 points in
mR@50 and 20.7 points in mR@100, while it falls behind in terms of Recall.
These results stem from the PSG dataset’s inherently imbalanced distribution
of predicate labels, where broad predicates such as “on” and “over” are more
frequent. As depicted in Fig. 10, our ASMv2 is less likely to predict these com-
mon but general predicates. Instead, it tends to predict more specific and less
frequent predicates, like “standing on” and “parked on”, resulting in superior
mRecall while inferior Recall. These results underline our model’s deeper and
more detailed comprehension of visual relations.
C.2 Implementation Details
Training Stage 1. Theglobalbatchsizeissetto256inthepre-trainingphase
and 128 in the instruction-tuning phase. We employ the AdamW optimizer [53]
withtheβ of0.9,theβ of0.999,andtheweightdecayof0.Thelearningrateis
1 2
initializedas1×10−3 forthepre-trainingphaseand2×10−5 fortheinstruction-
tuning phase. Both phases include a linear warmup that lasts until the first 3%
of training steps. The warmup is followed by a cosine decay strategy with a
minimum learning rate of 0. We only train the vision-language connector in the
pre-training phase while both the vision-language connector and the language
model are trainable in the instruction-tuning phase. We train the model for 1
epoch in both phases. The image resolution of ASMv2 is set to 336 × 336.
Training Stage 2. The global batch size is set to 512 and the learning rate is
initialized as 2×10−3 in both the pre-training phase and the instruction-tuning
phase. The language model and vision-language connector are trainable in both
phases while the vision encoder is always frozen. We train the model for 5000
stepsinthepre-trainingphaseand1epochintheinstruction-tuningphase.The
other settings remain the same as the instruction-tuning phase of Stage 1.The All-Seeing Project V2 29
D The Circular-based Relationship Probing Evaluation
Inthissection,wepresentmoreexamplesofabnormaldatainCRPEinFig.11.
Question: What is the relation between the Question: What is the relation between the
person and the surfboard? person and the cell phone?
Choices: Choices:
A. The person is falling off the surfboard. A. The person is walking on the cell phone.
B. The person is standing on the surfboard. B. The person is holding the cell phone.
C. The person is beside the surfboard. C. The person is sitting on the cell phone.
D. The person is wearing the surfboard. D. The person is standing on the cell phone.
Question: What is the relation between the Question: What is the relation between the
banana and the truck? person and the horse?
Choices: Choices:
A. The banana is sitting on the truck. A. The person is looking at the horse.
B. The banana is on the truck. B. The person is sitting on the horse.
C. The banana is attached to the truck. C. The person is holding the horse.
D. The banana is driving the truck. D. The person is attached to the horse.
Question: What is the relation between the cat Question: What is the relation between the
and the person? person and the backpack?
Choices: Choices:
A. The cat is sitting on the person. A. The person is pulling the backpack.
B. The cat is standing on the person. B. The person is beside the backpack.
C. The cat is beside the person. C. The person is walking on the backpack.
D. The cat is in front of the person. D. The person is wearing the backpack.
Fig.11: Data examples of abnormal data in the CRPE.