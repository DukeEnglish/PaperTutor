Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers
Tsai-ShienChen1,2,∗ AliaksandrSiarohin1 WilliMenapace1,3,∗ EkaterinaDeyneka1
Hsiang-weiChao1 ByungEunJeon1 YuweiFang1 Hsin-YingLee1 JianRen1
Ming-HsuanYang2 SergeyTulyakov1
1SnapInc. 2UniversityofCalifornia,Merced 3UniversityofTrento
https://snap-research.github.io/Panda-70M
HDVILA-100M Panda-70M (Ours)
“Yeah, now everybody thought that we couldn't replace cat; yeah, “It is a close-up shot of a brown and white englishbulldog with
because you're such animal lovers.” wrinkles on its face, sitting on a person's lap.”
“It's good that we have aquariumsthey bring this wonderful “It is a red and purple betta fish swimming in a tank with gravel
experienceinto our homes for special moments” and plants.”
“We're gonna cook this all together stirring it constantly for just a “A person is adding chicken broth to a pot of quinoa on a stove.”
minute until it smells nice and fragrant.”
“He thought he was gonna get showsterrible communication on the “A basketball player is dribbling the ball and shooting it into the
teams part.” hoop.”
Figure 1. Comparison of Panda-70M to the existing large-scale video-language datasets. We introduce Panda-70M, a large-scale
videodatasetwithcaptionsthatareannotatedbymultiplecross-modalityvision-languagemodels.Comparedtotextannotationsinexisting
dataset[80],captionsinPanda-70Mmorepreciselydescribethemainobjectandactioninvideos(highlightedingreen). Besides,videos
inPanda-70Maresemanticallycoherent,high-resolution,andfreefromwatermarks.MoresamplescanbefoundinAppendixE.
Abstract liclyavailableHD-VILA-100Mdataset. Wethensplitthem
Thequalityofthedataandannotationupper-boundsthe intosemanticallyconsistentvideoclips,andapplymultiple
qualityofadownstreammodel. Whilethereexistlargetext cross-modality teacher models to obtain captions for each
corpora and image-text pairs, high-quality video-text data video. Next,wefinetunearetrievalmodelonasmallsubset
is much harder to collect. First of all, manual labeling is where the best caption of each video is manually selected
moretime-consuming,asitrequiresanannotatortowatch and then employ the model in the whole dataset to select
anentirevideo. Second,videoshaveatemporaldimension, thebestcaptionastheannotation. Inthisway,weget70M
consisting of several scenes stacked together, and showing videos paired with high-quality text captions. We dub the
multiple actions. Accordingly, to establish a video dataset datasetasPanda-70M.Weshowthevalueoftheproposed
with high-quality captions, we propose an automatic ap- datasetonthreedownstreamtasks: videocaptioning,video
proachleveragingmultimodalinputs,suchastextualvideo and text retrieval, and text-driven video generation. The
description,subtitles,andindividualvideoframes. Specifi- modelstrainedontheproposeddatascoresubstantiallybet-
cally,wecurate3.8Mhigh-resolutionvideosfromthepub- teronthemajorityofmetricsacrossallthetasks.
∗ThisworkwasdonewhileinterningatSnap.
1
4202
beF
92
]VC.sc[
1v97491.2042:viXraTable1. ComparisonofPanda-70Mandothervideo-languagedatasets. Wesplitthedatasetsintotwogroups: thegroupatthetopis
annotatedbyASR,andthegroupatthebottomislabeledwithcaptions.
Dataset Year Text Domain #Videos Avg/Totalvideolen Avgtextlen Resolution
HowTo100M[52] 2019 ASR Open 136M 3.6s 134.5Khr 4.0words 240p
ACAV[32] 2021 ASR Open 100M 10.0s 277.7Khr - -
YT-Temporal-180M[87] 2021 ASR Open 180M - - - -
HD-VILA-100M[80] 2022 ASR Open 103M 13.4s 371.5Khr 32.5words 720p
MSVD[13] 2011 Manualcaption Open 1970 9.7s 5.3h 8.7words -
LSMDC[58] 2015 Manualcaption Movie 118K 4.8s 158h 7.0words 1080p
MSR-VTT[79] 2016 Manualcaption Open 10K 15.0s 40h 9.3words 240p
DiDeMo[3] 2017 Manualcaption Flickr 27K 6.9s 87h 8.0words -
ActivityNet[11] 2017 Manualcaption Action 100K 36.0s 849h 13.5words -
YouCook2[93] 2018 Manualcaption Cooking 14K 19.6s 176h 8.8words -
VATEX[73] 2019 Manualcaption Open 41K ∼10s ∼115h 15.2words -
Panda-70M(Ours) 2024 Automaticcaption Open 70.8M 8.5s 166.8Khr 13.2words 720p
1.Introduction description,subtitlesofthevideo,individualstaticframes,
We enter an era where the size of computing and data are and the video itself. The value of this data cannot be
indispensable for large-scale multimodal learning. Most fullymaximized whenonlypartiallyused. In comparison,
breakthroughs are achieved by large-scale computing in- weproposetoutilizedifferentcombinationsofmultimodal
frastructure, large-scale models, and large-scale data. Due data as inputs to various cross-modality captioning mod-
to these integral components, we have powerful text-to- els. Tosubstantiatethisidea,weconductanumericalanal-
image [4, 57, 59, 61, 83] and image-to-text models [2, 36, ysisbasedonahumanevaluation(thedetailsareprovided
43,53]. Scalingthemodelsizeorthecomputeischalleng- inAppendixB.3). Ifweusemultiplecross-modalitymod-
ing and expensive; however, it requires a finite amount of els to caption some video samples and evaluate the results
engineering time. Scaling the data is relatively more chal- byshowingthemtohumans,weseethatthereisnosingle
lenging,asittakestimeforahumantoanalyzeeachsample. modelabletogenerategoodcaptionsformorethan31%of
Especially, compared to image-text pairs [10, 12, 62], videos. However,ifwejointlycollectallthecaptionsfrom
video-textpairsareevenhardertoobtain. First,annotating different models, we observe that 84.7% of videos can be
videos is more time-consuming, as an annotator needs to annotatedwithatleastonegoodcaption.
watchtheentirevideobeforelabeling.Second,videosoften To establish the dataset with this mindset, we begin by
containmultiplescenesstitchedtogetherandconsistoftem- using3.8Mhigh-resolutionlongvideoscollectedfromHD-
porally varying content. Finally, meta-information, such VILA-100M [80] and process them through the following
as subtitles, video description, and voice-over, is often too threesteps. First,wedesignasemantics-awarevideosplit-
broad or not correctly aligned in time or cannot precisely ting algorithm to cut long videos into semantically consis-
describeavideo.Forexample,several100M-scaledatasets, tentclipswhilestrikingthebalancebetweensemanticsco-
such as HD-VILA-100M [80] and HowTo100M [52], are herenceandthedurationofthevideoclips. Second,weuse
annotated by automatic speech recognition (ASR). How- arangeofcross-modalityteachermodels,includingimage
ever, as shown in Figure 1, the subtitles usually fail to in- captioning models [37] and image/video visual-question-
clude the main content and action presented in the video. answering (VQA) models [38, 88, 94] with additional text
Thislimitsthevalueofsuchdatasetsformultimodaltrain- inputs, such as video description and subtitles, to predict
ing. Wesummarizethedatasetsavailabletothecommunity several candidate captions for a clip. Lastly, we collect a
inTable1. Somearelow-resolution,someareannotatedby 100Kvideosubset,wherehumanannotatorsactasanoracle
ASR, some contain data from a limited domain, some are toselectthebestcaptionforeachvideo. Weusethisdataset
small-scale,andsomeoffershortcaptions. tofinetuneafine-grainedvideo-to-textretrievalmodel[39]
In this work, we present a large-scale dataset contain- whichisthenappliedtothewholedatasettoselectthemost
ing 70M video clips with caption annotations. It includes precisecaptionastheannotation. Runningmultipleteacher
high-resolutionvideosfromanopendomainwithrichcap- models is computationally expensive and time-consuming.
tions averaging 13.2 words per caption. While manually To pursue efficient video captioning at scale in the future,
annotating 70M videos is prohibitively expensive, we opt we train a student model to distill the knowledge from the
for automatic annotation. Our key insight is that a video teachers. Thestudentmodeladoptsatwo-brancharchitec-
typically comes with information from several modalities turewhichcantakebothvisualandtextualinputstobenefit
thatcanassistautomaticcaptioning. Thisincludesthetitle, thecaptioningfrommultimodalinformation.
2Figure2. Videocaptioning pipeline. Givena longvideo, wefirst splitit intoseveralsemantically coherentclips. Subsequently, we
utilizeanumberofteachermodelswithdifferentmultimodalinputstogeneratemultiplecaptionsforavideoclip. Lastly,wefinetunea
fine-grainedretrievalmodeltoselectthecaptionthatbestdescribesthevideoclipastheannotation.
Extensiveexperimentsdemonstratethatpretrainingwith trieval[17,39,49].Weutilizeseveralvision-languagemod-
the proposed Panda-70M1 can benefit several downstream els for the annotation of Panda-70M. BLIP-2 [37] intro-
tasks, including video captioning, video and text retrieval, ducesanefficientvision-languagepretrainingthatcanfacil-
and text-to-video generation. We also show that training a itateimagecaptioning. WeuseBLIP-2asoneoftheteach-
studentmodelinaknowledgedistillationmannerfacilitates ersandinputarandomlysampledvideoframeforcaption-
learningastrongstudentmodelwhichcanoutperformany ing. MiniGPT-4[94]isanimageVQAmodelthatlearnsa
teachermodelbymorethan7.7%preferenceratioasinTa- projectionlayertoalignalargelanguagemodel(LLM)and
ble 3, where the performance can be further enhanced by avisualencoder.Inadditiontoavideoframe,wealsoinput
additionaltextinputs,likevideodescriptionandsubtitles. apromptwithextratextinformation,suchasvideodescrip-
tionandsubtitles,andaskthemodeltosummarizeallmul-
2.RelatedWork
timodalinputs. Forthevideomodality,Video-LLaMA[88]
Vision-LanguageDatasets. Trainingwithmillionsoreven
andVideoChat[38]arebothvideoVQAmodelsandlearn
billions of image-text pairs [10, 31, 55, 62, 86] has been
toextractLLM-compatiblevisualembeddings.Weuseboth
shown to be effective in learning powerful image founda-
models and ask them to caption a video with prompt in-
tionmodels[2,6,21,25,27,82]. Withthiswork,ourgoal
put. Besides, Unmasked Teacher [39] is a video founda-
is to build a large video-language dataset containing rich
tion model which can facilitate video understanding. We
captions. We compare related datasets in Table 1. Several
finetuneittoimplementfine-grainedretrievalanduseitto
precedentvideo-languagedatasets[3,11,13,58,73,79,93]
selectthemoreprecisecaptionastheannotation.
containdatatacklingvarioustasks,suchasactionrecogni-
Video Annotation through Multi-modal Models. With
tion, video understanding, VQA, and retrieval. However,
the aforementioned development on vision-language mod-
manually annotating data is costly and limits the scale of
els, some concurrent works [7, 72, 75] also leverage these
such datasets (typically they contain less than 120K sam-
models for video captioning. VideoFactory [72] employs
ples). Toalleviatethelackofdata,theworksof[52,80,87]
BLIP-2[37]tocaptionvideoclips. However,asreportedin
propose to automatically annotate data with subtitles, gen-
Appendix B.3, the performance of a single BLIP-2 model
eratedbyASR.Whilethisapproachsignificantlyincreases
issuboptimal. Moresimilartoourcaptioningpipeline,In-
the dataset scale reaching 100M of samples, the subtitles,
ternVid [75] and Stable Video Diffusion [7] also use mul-
unfortunately,donotpreciselydescribethemainvideocon-
tiplecaptioningmodelswhicharefollowedbyanLLMfor
tent,asshowninFigure1. Incomparison,inthiswork,we
summarization.Inpractice,wefoundtheLLMwouldprop-
proposeanautomaticcaptioningpipelinewiththeinputsof
agateerrorsfromnoisyoutputsofvision-languagemodels.
multimodal data that enables us to scale up the dataset of
high-qualityvideo-captionpairstoa70Mscale. 3.Methodology
Vision-LanguageModelslearnthecorrelationbetweenvi-
TobuildPanda-70M,weutilize3.8Mhigh-resolutionlong
sual data (images or videos) and linguistic signals (words
videoscollectedfromHD-VILA-100M[80]. Wethensplit
or sentences) and can be applied to several downstream
theminto70.8Msemanticallycoherentclipsasdescribedin
applications, including text-driven image or video gener-
Section3.1.Section3.2showshowmultiplecross-modality
ation [4, 8, 29, 57, 59, 61, 65, 83, 92], captioning [2,
teachermodelsareusedtogenerateasetofcandidatecap-
36, 37, 43, 63, 81], VQA [14, 38, 53, 88, 94] and re-
tionannotations. Next,wefinetuneafine-grainedretrieval
1WecallourdatasetPanda,drawingananalogytoPandaPo,wholearns modeltoselectthemostaccuratecaptionasdetailedinSec-
frommultiplemartialartsteachers. tion3.3.Finally,inSection3.4,wedescribeourapproachto
3trainingastudentcaptioningmodelusingPanda-70M.The Table2. Comparisonofsplittingalgorithms. Wesplit1Klong
high-levelviewofourapproachisshowninFigure2. videosbythreealgorithmsandtestthesemanticsconsistencyof
theoutputclipsbytheproposedMaxRunningLPIPS.Oursplit-
3.1.Semantics-awareVideoSplitting
ting strikes a better balance for the trade-off between semantics
Adesiredvideosampleinavideo-captioningdatasetshould consistencyandcliplength.
have two somewhat contradictory characteristics. On the
Method MaxrunningLPIPS↓ AvgVideoLen
one hand, the video should be semantically consistent, so
Sub.Align[52,80] 0.408 11.8s
thevideosamplescanbetterbenefitthedownstreamtasks,
PySceneDetect[1] 0.247 4.1s
such as action recognition, and the caption can also more
accurately express its semantics content without ambigu- OurSplitting 0.256 7.9s
ity. On the other hand, the video cannot be too short or
fragmentary to contain meaningful motion content, which
isbeneficialtotasks,likevideogeneration. Pretrained UMT
Toachievebothgoals,wedesignatwo-stagesemantics- Finetuned UMT
Annotators
aware splitting algorithm to cut a long video into semanti-
cally coherent clips. In the first stage, we split the video
based on shot boundary detection [1], as the semantics of-
ten change when a new scene starts. In the second stage,
westitchadjacentclipsiftheyareincorrectlyseparatedby
thefirststage,ensuringthevideosdonotendupbeingtoo
short. To do so, we use ImageBind [25] to extract em-
beddings of video frames and merge the adjacent clips if
Selective rate
the frame embeddings from two clips are similar. We also Figure 3. Distributions of the selective rate of teacher mod-
implementadditionalprocedurestohandle: 1)longvideos els. Weplotthedistributionsoftheselectiverateofeightteachers
withoutanycut-scenes,2)videosusingcomplextransitions, on1,805testingvideos. Theresultsarebasedontheselectionof
such as fade-in and fade-out effects, which are not usually thepretrained(red)orfinetuned(green)UnmaskedTeacher[39]
detected as cut-scenes, and 3) removal of redundant clips andhumanannotators(blue).
toincreasethediversityofthedataset. Moredetailsofthe Westartwithalargepoolincluding31captioningmod-
splitting algorithm are in Appendix A. Notably, while our els. TheintroductionofthemodelpoolisinAppendixB.1.
dataset focuses on fine-grained video-text pairs with con- Since running the inference of all models on 70M video
sistent semantics, users can still acquire long videos with clipsiscomputationallyexpensive,weconstructashortlist
multiplecut-scenesbyconcatenatingconsecutiveclipsand ofeightwell-performingmodelsbasedonauserstudy. The
captions,astheseclipsaresplitfromthesamelongvideo. listisshowninthey-axisofFigure3. Moredetailsofthis
To quantitatively verify the semantic consistency of a processareinAppendixB.3. Briefly,themodelsarecom-
videoclip,weintroduceMaxRunningLPIPS,whichhigh- posedoffivebasemodelswithdifferentpretrainingweights
lightsthemostsignificantperceptualchangewithinavideo andinputinformation. ThefivebasemodelsincludeVideo-
clip. Formally,givenann-secondvideoclip,wesubsample LLaMA [88] (video VQA), VideoChat [38] (video VQA),
thevideoframeseachsecondanddenotethekeyframesas VideoChatText[38](naturallanguagemodelwhichtextu-
{f ,...,f }. TheMaxRunningLPIPSisformulatedas: alizes the video content), BLIP-2 [37] (image captioning),
1 n
and MiniGPT-4 [94] (image VQA). To implement video
max({LPIPS(f i,f i+1)|i∈[1,n−1]}). (1) captioningbycross-modalityteachermodels,weformulate
distinctcaptioningprocessestailoredtoeachmodality. For
where LPIPS(·,·) is the perceptual similarity [89] of two
example, for the VQA models, in addition to visual data,
images.AsinTable2,oursplittingachievesabetterseman-
wealsoinputapromptwithadditionaltextinformationand
ticsconsistencythanthesplittingbasedonthealignmentof
askthemodelstosummarizeallmultimodalinputsintoone
subtitlessentences[52,80],whilemaintaininglongervideo
sentence. Detailsonthecaptioningprocessofeachteacher
lengththanthevanillashotboundarydetection[1].
modelaredescribedinAppendixB.2.
3.2.CaptioningwithCross-ModalityTeachers We hypothesize that teacher models using different
VideosinHD-VILA-100M[80]containrichmultimodalin- modalitydataperformwellondifferentkindsofvideos.For
formation beneficial for captioning. Specifically, besides example, video models can perform better on videos with
thevideoitself,therearealsousefultexts(e.g.,videotitle, complexdynamicsduetotheadditionalmodulestohandle
description,andsubtitles)andimages(e.g.,individualvideo temporalinformation.Ontheotherhand,imagemodelscan
frames). Driven by this insight, we propose to use several accuratelycaptionthevideoswithrareanduncommonob-
captioningmodelswiththeinputsofdifferentmodalities. jects, since they were trained using large-scale datasets of
4image-text pairs [62]. Finally, for videos that are visually Video Clip Input
hardtounderstand,VQAmodelshaveleverageastheycan
employadditionaltextualclues.
Thishypothesiscanbesupportedbyanumericalevalu-
ation. Specifically,weconductauserstudywherethepar-
ticipantsareaskedtoselectthebestcaptionfromeightcan- Text Input (Optional)
didates. Weplottheselectiverateofeachteachermodelin [Title]Old VS New -1966 Ford Mustang GT & 2018 Ford Mustang | Just a Quick Look
[Subtitles] Today, we're gonna take a quick look at the 1966 Ford Mustang with a 289…
Figure3(bluebars). Theresultsshowthatthebestcaptions
[Description] Lets check out this beautiful 1966 Ford Mustang GT 289 in the showroom!
are generated by different teacher models. Moreover, the
highest selective rate of an individual teacher model (i.e., Tokenizer
Video Encoder
& Embedding
BLIP-2 with opt6.7b [90]) is only 17.85%. This fact ex-
presses the limited captioning capability of a single model K/V K/V
onawidevarietyofvideos. Q Video Q-Former Q
Text Q-Former
& Linear Projection
3.3.Fine-grainedVideo-to-TextRetrieval
Learned
Givenmultiplecandidatecaptionsforavideo,weseekthe Queries
onethatbestalignswiththevideocontent.Anintuitiveidea
is to use the available generic video-to-text retrieval mod-
Text Representation Video Representation
els [25, 39] to pick such a caption. Unfortunately, we find
thattheyusuallyfailtopicktheoptimalresult. Onereason Large Language Model
is that generic models are trained using contrastive learn-
ingobjectives[15,82]andlearntodistinguishonesample "A red mustang in a showroom with american flags on the wall."
from other completely unrelated samples2. In contrast, in Figure4.Architectureofstudentcaptioningmodel.
our case, all candidate captions are highly relevant to the
selectedcaptions(bluebars).WerunthefinetunedUMTon
video sample and require the model to discern subtle dis-
thewholedatasettoselectthebestcaptionastheannotation
tinctionswithineachcaptionforoptimalperformance.
aselaboratedinAppendixC.3.
To tailor the retrieval model to our “fine-grained” re-
trieval scenario, we collect a subset of 100K videos, for 3.4.MultimodalStudentCaptioningModel
which human annotators select the caption containing the
Whiletheaforementionedcaptioningpipelinecangenerate
most correct and detailed information about the main con-
promisingcaptions,theheavycomputationaldemandshin-
tentofthevideo. WethenfinetuneUnmaskedTeacher[39]
der its capability to expand the dataset to an even larger
(UMT) on this dataset. We implement hard negative min-
scale. Indeed, one needs to run 8+1 different models to
ing [16, 35] for contrastive loss, where the seven captions
annotateasinglevideoclip. Todealwiththisproblem,we
notselectedbyannotatorscomposethehardnegativesam-
learn a student captioning model on Panda-70M to distill
plesandareassignedalargertrainingweight. Wedescribe
theknowledgefrommultipleteachermodels.
thedetailsofthedatasetcollectionandfinetuningofUMT
As shown in Figure 4, the student model includes vi-
inAppendixC.1andC.2respectively.
sual and text branches, leveraging multimodal inputs. For
We quantitatively evaluate the retrieval performance of
the vision branch, we use the same architecture as Video-
UMTs with and without finetuning on the validation set.
LLaMA[88]toextractLLM-compatiblevideorepresenta-
TheexperimentsindicatethatafinetunedUMTcanachieve
tion. Forthetextbranch, astraightforwarddesignistodi-
35.90% R@1 accuracy which significantly outperforms a
rectly input text embedding into the LLM. However, this
pretrainedUMTwhichhas21.82%R@1.Notably,wecon-
willleadtotwoproblems: first,thetextpromptwithvideo
ducted a human agreement evaluation by asking two other
descriptionandsubtitlescanbetoolong,dominatingthede-
personstore-performtheannotationandcomparingthere-
cisionoftheLLMandburdeningheavycomputation; sec-
sults with the original annotations. The average human
ond, the information from the description and subtitles is
agreementscoreisonly44.9%R@1showingthatthetask
often noisy and not necessary to align with the content of
is subjective when more than one caption is equally good.
thevideo. Totacklethis,weaddatextQ-formertoextract
Alternatively, if we consider the captions selected by any
the text representation with fixed length and better bridge
of the three persons as good captions (i.e., a video might
the video and text representations. The Q-former has the
havemultiplegoodcaptions),UMTachieves78.9%R@1.
samearchitectureastheQueryTransformerinBLIP-2[37].
Besides,inFigure3,weshowthatafinetunedUMT(green
Duringtraining,weblockthegradientpropagationfromthe
bars)canselectthecaptionsdistributedsimilarlytohuman-
textbranchtothevisionbranchandtrainthevisualencoder
2Negativesamplesforcontrastivelearning[15]areusuallyrandomlysam- onlybasedonthevideoinput. Moredetailsaboutthearchi-
pledfromthewithin-batchdataandshownoassociationtotheanchor. tectureandtrainingofthestudentmodelareinAppendixD.
5
tneidargTable3. Zero-shotvideocaptioning(%). WecompareVideo-LLaMA[88]withofficialweight(pretrainedon2.5Mvideosand595K
images)andourPanda-2Mpretrainingweight. Wealsotestourstudentmodel(withvisionbranchonly)trainedonthecompletePanda-
70Mdataset.WereportBLEU-4(B-4)[54],ROUGE-L(R)[40],METEOR(M)[5],CIDEr(C)[69],andBERTScore(BERT)[91]ontwo
benchmarksMSR-VTT[79]andMSVD[13].
MSR-VTT MSVD
Method PretrainingData
B4↑ R↑ M↑ C↑ BERT↑ B4↑ R↑ M↑ C↑ BERT↑
Video-LLaMA[88] 2.5Mvid+595Kimg 5.8 30.0 15.9 14.3 84.5 12.7 43.0 23.6 38.5 87.3
Video-LLaMA[88] Panda-2M(Ours) 23.5 48.6 26.7 29.1 87.2 31.2 59.9 34.7 47.0 89.8
Student(Ours) Panda-70M(Ours) 25.4 50.1 27.7 31.5 87.9 32.8 61.2 35.3 49.2 90.2
Table4. Comparisonoftheteacher(s)andstudentcaptioning
models(%). Weconductauserstudytocomparesingleteacher,
allteacher,andtwostudentmodels(withandwithouttext).
Model PreferenceRatio↑
Video-LLaMA[88](pretrain) 9.4
Video-LLaMA[88](finetune) 7.0
VideoChat[38] 7.7
VideoChatText[38] 3.3
BLIP-2[37](opt2.7b) 10.7
BLIP-2[37](opt6.7b) 9.0
BLIP-2[37](flant5xl) 9.9
MiniGPT-4[94] 3.1
Student(videoinput)(Ours) 18.4
Student(video+textinputs)(Ours) 21.4
Figure5.Qualitativecomparisonofvideocaptioning. Wevisu-
AllTeachers(Ours) 23.3 alizeasamplefromthetestingsetofPanda-70Mandshowitsan-
notation(bottommost). Wealsoshowthecaptionspredictedfrom
three models, including Video-LLaMA [88] with official weight
4.Experiments andthestudentmodelswithvideo-onlyorvideoandtextinputs.
WevisualizethesamplesofPanda-70MinAppendixE.To
textual information for a fair comparison. For the student
quantitativelyevaluatetheeffectivenessofPanda-70M,we
model,inadditiontothevideo,wealsorandomlyinputthe
test its pretraining performance on three downstream ap-
metadataandsubtitlesintothemodelduringtraining.
plications: video captioning in Section 4.1, video and text
retrievalinSection4.2,andvideogenerationinSection4.3. Downstream datasets and evaluation metrics. We test
Thetrainingdetailsofthedownstreammodelsadheretothe zero-shot video captioning on two benchmarks: MSR-
officialcodebasesunlessexplicitlyspecified. VTT[79]andMSVD[13].MSR-VTTcontains10Kvideos
with20manuallyannotatedcaptionsforeachvideo;were-
4.1.VideoCaptioning
port the results on the 2,990 testing split. MSVD consists
Experiment setup. To evaluate the performance of video of1,970videoswithatotalof80Kdescriptions;wereport
captioning, we use Video-LLaMA [88] with the vision thenumbersonthe670testingvideos. Notethatwedonot
branch only as the base model. We compare two pretrain- useanytrainingorvalidationvideosfromthedownstream
ingweights: theofficialweight,whichisjointlytrainedon datasets. To quantitatively evaluate the quality of output
2.5Mvideo-textpairsand595Kimage-textpairs[43], and captions,wefollowthecommonprotocols[41,48,78]and
the weight trained on our Panda-2M from scratch. Panda- report BLEU-4 [54], ROGUE-L [40], METEOR [5], and
2MisarandomlysampledsubsetofPanda-70Mandshares CIDEr[69]. Allthemetricsarecomputedusingthepyco-
thesameamountoftrainingsamplesastheofficialweight. coevalcap[42]package. WealsocomputeBERTScore[91]
We also train our student model with both video and text to evaluate the contextual similarity for each token in the
branchesoncompletePanda-70Mforbettercaptioningper- groundtruthandthepredictedcaptions. Theresultsarere-
formance. Forallmodels,weusethesamebackbone,using portedinTable3.Forafaircomparison,wedonotinputany
Vicuna-7B[18]asthelarge-languagemodel,ViT[22]and additionaltextinformationtothestudentmodelduringthe
Q-Former[37]asthevideoencoder, andthelinearprojec- inferenceonthedownstreamdatasets. InFigure5,wealso
tionlayerfromMiniGPT-4[94].ForPanda-2Mpretraining, showcaseavideosamplefromthetestingsetofPanda-70M
weonlyusethevideoandcaptiondatawithoutusingother andthepredictedcaptionsforthequalitativecomparison.
6Table5. Videoandtextretrieval(%). WecomparetheUnmaskedTeacher[39]withtheofficialcheckpoint(pretrainedon2.5Mvideos
and3Mimages)andourPanda-5Mpretraining. Weevaluatetheirperformanceonzero-shotandfinetunetext-to-video(T2V)andvideo-
to-text(V2T)retrieval.WereportR@1,R@5,andR@10accuracyonthreebenchmarks:MSR-VTT[79],DiDeMo[3],andMSVD[13].
MSR-VTT DiDeMo MSVD
Method PretrainingData
R@1↑ R@5↑ R@10↑ R@1↑ R@5↑ R@10↑ R@1↑ R@5↑ R@10↑
Zero-shotT2V/V2TRetrieval
AlignPrompt[34] 2.5Mvid+3Mimg 24.1/ - 44.7/ - 55.4/ - 23.8/ - 47.3/ - 57.9/ - - / - - / - - / -
BridgeFormer[24] 2.5Mvid+3Mimg 26.0/ - 46.4/ - 56.4/ - 25.6/ - 50.6/ - 61.6/ - 43.6/ - 74.9/ - 84.9/ -
UMT[39] 2.5Mvid+3Mimg 30.2/33.3 51.3/58.1 61.6/66.7 33.6/32.1 58.1/57.3 65.5/66.7 66.3/44.4 85.5/73.3 89.3/82.4
UMT[39] Panda-5M(Ours) 37.2/36.3 58.1/61.0 69.5/69.7 34.2/33.4 58.4/57.9 66.5/65.8 71.2/37.2 88.4/65.1 92.7/75.6
FinetuneT2V/V2TRetrieval
CLIP4Clip[49] 400Mimg 44.5/40.6 71.4/69.5 81.6/79.5 43.4/42.5 70.2/70.6 80.6/80.2 46.2/62.0 76.1/87.3 84.6/92.6
X-CLIP[50] 400Mimg 49.3/48.9 75.8/76.8 84.8/84.5 50.4/66.8 80.6/90.4 - / - 47.8/47.8 79.3/76.8 - / -
InternVideo[74] 146Mvid+100Mimg 55.2/57.9 - / - - / - 57.9/59.1 - / - - / - 58.4/76.3 - / - - / -
UMT[39] 2.5Mvid+3Mimg 53.3/51.4 76.6/76.3 83.9/82.8 59.7/59.5 84.9/84.5 90.8/90.7 53.7/77.2 80.5/91.6 86.8/94.8
UMT[39] Panda-5M(Ours) 58.4/58.5 80.9/81.0 86.9/87.0 60.6/58.9 86.0/84.6 92.4/90.4 57.5/81.3 83.6/93.7 89.5/96.6
AsinTable3,Video-LLaMAwithPanda-2Mpretraining 4.2.VideoandTextRetrieval
weight achieves significantly superior performance com-
Experimentsetup. WeuseUnmaskedTeacher[39]asthe
pared to the official weight. Numerically, our pretraining
base model to evaluate the performance on video and text
weight yields 17.7% and 18.5% improvement respectively
retrieval. Thestandardprotocols[17,24,34,39,78]jointly
onMSR-VTTandMSVDintermsofB-4. Besides,inFig-
use 3M images from CC3M [64] and 2.5M videos as the
ure5,wecanfindthatthecaptionfromtheoriginalVideo-
pretraining datasets. Thus, we randomly sample a Panda-
LLaMA contains irrelevant and generic information, such
5Msubset,whichsharesthesamenumberoftrainingsam-
as date and location. In comparison, our prediction better
ples as the standard pretraining dataset for a fair compari-
alignswiththevideocontent.
son.Forbothdatasets,weusethesamebackbonecomposed
Canthestudentperformbetterthanitsteacher? InSec-
of ViT-L/16 [21] and BERTlarge [20]. We use the official
tion 3.4, we learn a student model in a knowledge distil-
weights for the standard datasets pretraining and train the
lation manner. To evaluate the performance of the student
modelfromscratchforourPanda-5M.
model,weconductauserstudywhereparticipantsareasked
Downstream datasets and evaluation metric. We test
toselectthebestcaptionfromtencandidatesforeachvideo.
both zero-shot and finetune retrieval on three benchmarks:
Ten captions are predicted from eight teacher models and
MSR-VTT[79],DiDeMo[3],andMSVD[13]. ForMSR-
twostudentmodels(withandwithouttextinputs). Wecol-
VTT, we follow the common protocol [34, 85] to evaluate
lecttheresultsfromfiveparticipantstoreducethepersonal
on 1K testing split, which is not the same as the testing
subjectivebias. Eachparticipantsawthesame200videos,
videos for captioning in Section 4.1. For DiDeMo [3], it
whichwererandomlysampledfromthetestingsetandhad
contains 10K Flickr videos with a total of 40K dense cap-
notbeenseenduringthetrainingofthestudentmodeland
tions. Asinthepreviousstandard[24,33,44],weevaluate
UMT.Wereportthepreferenceratioofeachmodelandthe
R@1 accuracy of the finetuned UMT (i.e., all teachers) in paragraph-to-video retrieval by concatenating all sentence
descriptionsofonevideointoasinglequery. Wereportthe
Table4.Wecanobservethatthestudentmodeloutperforms
resultsonthe1Ktestingset. ForMSVD[13],wereportthe
any individual teacher model and achieves a comparable
results on the 670 testing videos. We employ the standard
performancewithallteachermodels.
metricandreportR@1,R@5,andR@10accuracyonboth
Canmultimodalinputsleveragevideocaptioning? Our
text-to-videoandvideo-to-textretrievalinTable5.
student model supports both video and text inputs. In Ta-
ble4,weshowthatthestudentmodelwithbothvideoand WecanobservethatpretrainingwithourPanda-5Mout-
textinputsoutperformsthemodelwithvideoinputonlyby performstheofficialweightinboth zero-shotandfinetune
3.0% preference ratio. Qualitatively, we show the predic- retrieval settings. Especially, our pretraining yields 7.0%,
tions with and without text inputs in Figure 5. While the 0.6%, and 4.9% lifts in terms of R@1 of zero-shot text-
predictionwithpurevideoinputcanincludepartialcontent to-video retrieval on MSR-VTT [79], DiDeMo [3], and
of the video, like “cactus”, the model with both video and MSVD [13] respectively. Besides, pretraining UMT [39]
text inputs can more comprehensively include keywords with our Panda-5M also outperforms the existing state-of-
suchas“succulents”and“differentspecies”fromthevideo the-artmethods[49,50,74]whicharepretrainedwithmuch
title,description,andsubtitles. morevision-textdatapairs(i.e.,>100M).
7Table6. Zero-shottext-to-videogeneration. Wecomparethe
“Cut tusuncub walking in the snow, blurry, looking at viewer,
zero-shot text-to-video generation of AnimateDiff [26] with the depth of field, blurry background, full body, solo, cute …”
official weight (pretrained on 2.5 M videos) and our Panda-2M
pretraining.WereportFVD[68]onUCF101[66]andCLIPsimi-
larity(CLIPSim)[76]onMSR-VTT[79]. Weonlycomparewith
themodelstrainedwithlessthan10Mvideos.
UCF101 MSR-VTT
Method (#)P-TVideos
FVD↓ CLIPSim↑
CogVideo[30] 5M 701.6 -
MagicVideo[92] 10M 699.0 -
LVDM[28] 18K 641.8 0.2751
ModelScope[70] 10M 639.9 0.3000 Figure6. Qualitativeresultsoftext-to-videogeneration. We
VideoLDM[8] 10M 550.6 - visualize the videos generated by the AnimateDiff [26] with of-
ficialweight(top)andourPanda-2Mpretraining(bottom). Note
AnimateDiff[26] 2.5M 499.3 0.2869
thatthetestpromptandthevideosampleoftheoriginalAnimate-
AnimateDiff[26] Panda2M(Ours) 421.9 0.2880
Diff(top)aredirectlyfromtheprojectwebsiteofAnimateDiff.
4.3.Text-to-VideoGeneration tively,ourpretrainingweightscangeneratethevideowitha
Experiment setup. To evaluate the effectiveness of text- moremeaningfulmotionandphotorealisticappearanceand
to-video generation, we use AnimateDiff [26] as the base donotincludeawatermark.
model and compare two weights: the officially released
5.ConclusionandLimitations
weight, which is trained on 2.5M text-video pairs, and the
This paper introduces Panda-70M, a large-scale video
weight trained on our Panda-2M, a 2.5M subset of Panda-
datasetwithcaptionannotations.Thedatasetincludeshigh-
70M. We follow the official codebase and use Stable Dif-
resolution and semantically coherent video samples. To
fusion v1.5 [59] (SD) as the base text-to-image (T2I) gen-
caption70Mvideos,weproposeanautomaticpipelinethat
erator. Duringtraining, wefixT2Imodulesandonlytrain
can leverage multimodal information, such as video de-
themotionmodelingmodules. Foreachtrainingvideo,we
scription, subtitles, andindividualstaticvideoframes. We
sample 16 frames with a stride of 4, and then resize and
demonstratethatpretrainingwithPanda-70Mcanfacilitate
center-cropto256×256pxresolution.
three downstream tasks: video captioning, video and text
Downstream datasets and evaluation metrics. To eval-
retrieval,andtext-to-videogeneration.
uate the models, we follow the evaluation protocols [8,
Despiteshowingimpressiveresults,theproposeddataset
23, 65, 72] for zero-shot evaluation on UCF101 [66] and
is still bound by a few limitations. First, we collect the
MSR-VTT[79]. Specifically,wegenerate16-framevideos
videosfromHD-VILA-100M[80],wheremostofthesam-
in 256 × 256px resolution. For UCF101 [66], we pro-
ples are vocal-intensive videos. Hence, the major cate-
duceatextpromptforeachclass[23]andgenerate10,000
goriesofourdatasetarenews,televisionshows,documen-
videos which share the same class distribution as the orig-
tary films, egocentric videos, and instructional and narra-
inal dataset [8, 72]. We compute Fre´chet Video Dis-
tivevideos. Asourannotationpipelinedoesnotrequirethe
tance (FVD) [68] on the I3D embeddings [84]. For MSR-
presence of video subtitles, we list the collection of more
VTT [79], we generate a video sample for each of the
unvocalvideosasanimportantextensionofthiswork.
59,800 test prompts [23, 65] and compute CLIP similarity
Second, we focus on a fine-grained dataset where the
(CLIPSim)[76].WereportthenumbersinTable6.Wealso
videosamplesaresemanticallyconsistentsothecaptioncan
showthegeneratedvideosamplesinFigure6. Tovisualize
accuratelyexpressitssemanticscontentwithoutambiguity.
theresults,wefollowtheofficialcodebaseandreplaceSD
T2IwithpersonalizedDreamboothweight[60], TUSUN3. Nevertheless, it would limit the content diversity within a
singlevideoandalsoreduceaveragevideoduration,which
NotethatthetestpromptandthevideosamplefromtheAn-
mightbehurtfultothedownstreamtasks,suchaslongvideo
imtateDiffwiththeofficialweight(toprowinFigure6)are
generation[9]anddensevideocaptioning[71,81]. Future
directlyfromtheprojectpageofAnimateDiff.
effortsinbuildingdatasetswithlongvideosanddensecap-
Panda-2M pretraining consistently shows superior per-
tionscanbenefitthesedownstreamapplications.
formance on both metrics compared to the official weight.
As highlighted, our pretraining yields 77.4 lower FVD on Risk mitigation. Prior to the release of the dataset, we
UCF101andoutperformsstate-of-the-artmodelspretrained used the internal automatic pipeline to filter out the video
samples with harmful or violent language and texts that
onadatasetwithina10MscaleintermsofFVD.Qualita-
include drugs or hateful speech. We also use the NLTK
3https://civitai.com/models/33194/pallass-catmanul-lora framework to replace all people’s names with “person”.
8
lanigirO
htiw
gniniarterP
ffiDetaminA
)sruO(
M2-adnaP[16] Tsai-ShienChen,Wei-ChihHung,Hung-YuTseng,Shao-Yi
References
Chien, and Ming-Hsuan Yang. Incremental false negative
detectionforcontrastivelearning. InICLR,2022. 5
[1] Pyscenedetect. https : / / github . com /
Breakthrough/PySceneDetect. 4,1 [17] Feng Cheng, Xizi Wang, Jie Lei, David Crandall, Mohit
[2] Jean-BaptisteAlayrac, JeffDonahue, PaulineLuc, Antoine Bansal,andGedasBertasius. Vindlu: Arecipeforeffective
Miech,IainBarr,YanaHasson,KarelLenc,ArthurMensch, video-and-languagepretraining. InCVPR,2023. 3,7
KatherineMillican, MalcolmReynolds, etal. Flamingo: a [18] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhang-
visuallanguagemodelforfew-shotlearning.NeurIPS,2022. haoWu,HaoZhang,LianminZheng,SiyuanZhuang,Yong-
2,3 hao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P.
[3] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Xing. Vicuna: An open-source chatbot impressing gpt-4
Sivic, Trevor Darrell, and Bryan Russell. Localizing mo- with90%*chatgptquality,2023. 6,3
mentsinvideowithnaturallanguage. InICCV,2017. 2,3, [19] HyungWonChung,LeHou,ShayneLongpre,BarretZoph,
7 YiTay,WilliamFedus,YunxuanLi,XuezhiWang,Mostafa
[4] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Dehghani, Siddhartha Brahma, et al. Scaling instruction-
Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, finetunedlanguagemodels. arXivpreprint,2022. 3
SamuliLaine,BryanCatanzaro,etal. ediff-i:Text-to-image [20] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
diffusionmodelswithanensembleofexpertdenoisers.arXiv Toutanova. Bert: Pre-training of deep bidirectional trans-
preprint,2022. 2,3 formers for language understanding. arXiv preprint, 2018.
[5] SatanjeevBanerjeeandAlonLavie. Meteor: Anautomatic 7,5
metricformtevaluationwithimprovedcorrelationwithhu- [21] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
manjudgments. InACLworkshop,2005. 6 Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
[6] HangboBao,LiDong,SonghaoPiao,andFuruWei. BEit: MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl-
BERTpre-trainingofimagetransformers. InICLR,2022. 3 vainGelly,JakobUszkoreit,andNeilHoulsby. Animageis
[7] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel worth16x16words: Transformersforimagerecognitionat
Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, scale. InICLR,2021. 3,7,5
ZionEnglish,VikramVoleti,AdamLetts,etal.Stablevideo
[22] YuxinFang,WenWang,BinhuiXie,QuanSun,LedellWu,
diffusion: Scaling latent video diffusion models to large
Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue
datasets. arXivpreprintarXiv:2311.15127,2023. 3
Cao. Eva:Exploringthelimitsofmaskedvisualrepresenta-
[8] AndreasBlattmann,RobinRombach,HuanLing,TimDock- tionlearningatscale.InProceedingsoftheIEEE/CVFCon-
horn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. ferenceonComputerVisionandPatternRecognition,pages
Alignyourlatents: High-resolutionvideosynthesiswithla- 19358–19369,2023. 6
tentdiffusionmodels. InCVPR,2023. 3,8
[23] SongweiGe,SeungjunNah,GuilinLiu,TylerPoon,Andrew
[9] Tim Brooks, Janne Hellsten, Miika Aittala, Ting-Chun
Tao,BryanCatanzaro,DavidJacobs,Jia-BinHuang,Ming-
Wang, Timo Aila, Jaakko Lehtinen, Ming-Yu Liu, Alexei
YuLiu,andYogeshBalaji. Preserveyourowncorrelation:
Efros,andTeroKarras. Generatinglongvideosofdynamic
Anoisepriorforvideodiffusionmodels. InICCV,2023. 8
scenes. NeurIPS,2022. 8
[24] YuyingGe,YixiaoGe,XihuiLiu,DianLi,YingShan,Xi-
[10] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun
aohuQie,andPingLuo. Bridgingvideo-textretrievalwith
Lee, Woonhyuk Baek, and Saehoon Kim. Coyo-700m:
multiplechoicequestions. InCVPR,2022. 7
Image-text pair dataset. https://github.com/
[25] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat
kakaobrain/coyo-dataset,2022. 2,3
Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan
[11] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem,
Misra. Imagebind: Oneembeddingspacetobindthemall.
and Juan Carlos Niebles. Activitynet: A large-scale video
InCVPR,2023. 3,4,5,1
benchmark for human activity understanding. In CVPR,
[26] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu
2015. 2,3
Qiao, DahuaLin, andBoDai. Animatediff: Animateyour
[12] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu
personalizedtext-to-imagediffusionmodelswithoutspecific
Soricut. Conceptual 12M: Pushing web-scale image-text
tuning. arXivpreprint,2023. 8
pre-trainingtorecognizelong-tailvisualconcepts.InCVPR,
2021. 2 [27] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr
[13] DavidChenandWilliamBDolan.Collectinghighlyparallel Dolla´r,andRossGirshick.Maskedautoencodersarescalable
dataforparaphraseevaluation. InACL,2011. 2,3,6,7 visionlearners. InCVPR,2022. 3
[14] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechu [28] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and
Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, QifengChen.Latentvideodiffusionmodelsforhigh-fidelity
Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. longvideogeneration. arXivpreprint,2023. 8
Minigpt-v2: largelanguagemodelasaunifiedinterfacefor [29] Jonathan Ho, WilliamChan, ChitwanSaharia, JayWhang,
vision-languagemulti-tasklearning. arXivpreprint,2023. 3 Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben
[15] TingChen,SimonKornblith,MohammadNorouzi,andGe- Poole, Mohammad Norouzi, David J Fleet, et al. Imagen
offreyHinton. Asimpleframeworkforcontrastivelearning video:Highdefinitionvideogenerationwithdiffusionmod-
ofvisualrepresentations. InICML,2020. 5 els. arXivpreprint,2022. 3
9[30] WenyiHong, MingDing, WendiZheng, XinghanLiu, and [46] IlyaLoshchilovandFrankHutter. Decoupledweightdecay
JieTang.Cogvideo:Large-scalepretrainingfortext-to-video regularization. arXivpreprint,2017. 5,7
generationviatransformers. arXivpreprint,2022. 8 [47] IlyaLoshchilovandFrankHutter. SGDR:Stochasticgradi-
[31] ChaoJia,YinfeiYang,YeXia,Yi-TingChen,ZaranaParekh, entdescentwithwarmrestarts. InICLR,2017. 7
HieuPham, QuocLe, Yun-HsuanSung, ZhenLi, andTom [48] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan
Duerig.Scalingupvisualandvision-languagerepresentation Duan,TianruiLi,JasonLi,TaroonBharti,andMingZhou.
learningwithnoisytextsupervision. InICML,2021. 3 Univl: Aunifiedvideoandlanguagepre-trainingmodelfor
[32] Sangho Lee, Jiwan Chung, Youngjae Yu, Gunhee Kim, multimodal understanding and generation. arXiv preprint,
Thomas Breuel, Gal Chechik, and Yale Song. Acav100m: 2020. 6
Automatic curation of large-scale datasets for audio-visual [49] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei,
videorepresentationlearning. InICCV,2021. 2 NanDuan,andTianruiLi. Clip4clip:Anempiricalstudyof
[33] JieLei, LinjieLi, LuoweiZhou, ZheGan, TamaraLBerg, clipforendtoendvideoclipretrievalandcaptioning. Neu-
MohitBansal,andJingjingLiu. Lessismore: Clipbertfor rocomputing,2022. 3,7
video-and-languagelearningviasparsesampling. InCVPR,
[50] YiweiMa,GuohaiXu,XiaoshuaiSun,MingYan,JiZhang,
2021. 7
and Rongrong Ji. X-clip: End-to-end multi-grained con-
[34] DongxuLi, JunnanLi, HongdongLi, JuanCarlosNiebles, trastivelearningforvideo-textretrieval. InACMMM,2022.
andStevenCHHoi. Alignandprompt:Video-and-language 7
pre-trainingwithentityprompts. InCVPR,2022. 7
[51] MuhammadMaaz,HanoonaRasheed,SalmanKhan,andFa-
[35] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare,
hadShahbazKhan. Video-chatgpt: Towardsdetailedvideo
Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi.
understandingvialargevisionandlanguagemodels. arXiv
Alignbeforefuse:Visionandlanguagerepresentationlearn-
preprint,2023. 2,3
ingwithmomentumdistillation. NeurIPS,2021. 5
[52] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
[36] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi.
Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
Blip: Bootstrappinglanguage-imagepre-trainingforunified
Howto100m: Learningatext-videoembeddingbywatching
vision-language understanding and generation. In ICML,
hundredmillionnarratedvideoclips. InICCV,2019. 2,3,4
2022. 2,3
[53] OpenAI. Gpt-4technicalreport. arXivpreprint,2023. 2,3
[37] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi.
[54] KishorePapineni,SalimRoukos,ToddWard,andWei-Jing
Blip-2: Bootstrapping language-image pre-training with
Zhu. Bleu: a method for automatic evaluation of machine
frozen image encoders and large language models. arXiv
translation. InACL,2002. 6
preprint,2023. 2,3,4,5,6
[55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
[38] KunchangLi,YinanHe,YiWang,YizhuoLi,WenhaiWang,
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
PingLuo,YaliWang,LiminWang,andYuQiao.Videochat:
AmandaAskell,PamelaMishkin,JackClark,etal. Learn-
Chat-centricvideounderstanding. arXivpreprint,2023. 2,
ingtransferablevisualmodelsfromnaturallanguagesuper-
3,4,6
vision. InICML,2021. 3
[39] Kunchang Li, Yali Wang, Yizhuo Li, Yi Wang, Yinan He,
[56] ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,
Limin Wang, and Yu Qiao. Unmasked teacher: Towards
SharanNarang, MichaelMatena, Yanqi Zhou, WeiLi, and
training-efficientvideofoundationmodels. ICCV,2023. 2,
Peter J Liu. Exploring the limits of transfer learning with
3,4,5,7
a unified text-to-text transformer. The Journal of Machine
[40] Chin-Yew Lin and Franz Josef Och. Automatic evaluation
LearningResearch,2020. 3
ofmachinetranslationqualityusinglongestcommonsubse-
quenceandskip-bigramstatistics. InACL,2004. 6 [57] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray,
[41] KevinLin,LinjieLi,Chung-ChingLin,FaisalAhmed,Zhe ChelseaVoss,AlecRadford,MarkChen,andIlyaSutskever.
Gan,ZichengLiu,YumaoLu,andLijuanWang. Swinbert: Zero-shottext-to-imagegeneration. InICML,2021. 2,3
End-to-endtransformerswithsparseattentionforvideocap- [58] AnnaRohrbach,MarcusRohrbach,NiketTandon,andBernt
tioning. InCVPR,2022. 6 Schiele. Adatasetformoviedescription. InCVPR,2015. 2,
[42] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays, 3
PietroPerona,DevaRamanan,PiotrDolla´r,andCLawrence [59] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Zitnick. Microsoft coco: Common objects in context. In PatrickEsser,andBjo¨rnOmmer.High-resolutionimagesyn-
ECCV,2014. 6 thesiswithlatentdiffusionmodels. InCVPR,2022. 2,3,8
[43] HaotianLiu,ChunyuanLi,QingyangWu,andYongJaeLee. [60] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch,
Visualinstructiontuning. arXivpreprint,2023. 2,3,6 MichaelRubinstein,andKfirAberman. Dreambooth: Fine
[44] YangLiu,SamuelAlbanie,ArshaNagrani,andAndrewZis- tuning text-to-image diffusion models for subject-driven
serman. Usewhatyouhave: Videoretrievalusingrepresen- generation. InCVPR,2023. 8
tationsfromcollaborativeexperts. arXivpreprint,2019. 7 [61] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
[45] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,Zheng Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour,
Zhang, Stephen Lin, and Baining Guo. Swin transformer: RaphaelGontijoLopes,BurcuKaragolAyan,TimSalimans,
Hierarchical vision transformer using shifted windows. In etal.Photorealistictext-to-imagediffusionmodelswithdeep
ICCV,2021. 3 languageunderstanding. NeurIPS,2022. 2,3
10[62] Christoph Schuhmann, Romain Beaumont, Richard Vencu, [76] ChenfeiWu,LunHuang,QianxiZhang,BinyangLi,LeiJi,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo FanYang,GuillermoSapiro,andNanDuan. Godiva: Gen-
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts- eratingopen-domainvideosfromnaturaldescriptions.arXiv
man,etal.Laion-5b:Anopenlarge-scaledatasetfortraining preprint,2021. 8
nextgenerationimage-textmodels. NeurIPS,2022. 2,3,5 [77] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan,
[63] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and ZichengLiu,JunsongYuan,andLijuanWang. Grit: Agen-
CordeliaSchmid.End-to-endgenerativepretrainingformul- erative region-to-text transformer for object understanding.
timodalvideocaptioning. InCVPR,2022. 3 arXivpreprint,2022. 3
[78] Haiyang Xu, Qinghao Ye, Ming Yan, Yaya Shi, Jiabo Ye,
[64] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu
Yuanhong Xu, Chenliang Li, Bin Bi, Qi Qian, Wei Wang,
Soricut. Conceptualcaptions: Acleaned,hypernymed,im-
et al. mplug-2: A modularized multi-modal foundation
agealt-textdatasetforautomaticimagecaptioning. InACL,
model across text, image and video. arXiv preprint, 2023.
2018. 7
6,7
[65] UrielSinger,AdamPolyak,ThomasHayes,XiYin,JieAn,
[79] JunXu,TaoMei,TingYao,andYongRui. Msr-vtt:Alarge
Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
videodescriptiondatasetforbridgingvideoandlanguage.In
OranGafni,DeviParikh,SonalGupta,andYanivTaigman.
CVPR,2016. 2,3,6,7,8
Make-a-video: Text-to-video generation without text-video
[80] HongweiXue,TiankaiHang,YanhongZeng,YuchongSun,
data. InICLR,2023. 3,8
Bei Liu, Huan Yang, Jianlong Fu, and Baining Guo. Ad-
[66] KhurramSoomro,AmirRoshanZamir,andMubarakShah.
vancinghigh-resolutionvideo-languagerepresentationwith
Ucf101:Adatasetof101humanactionsclassesfromvideos
large-scalevideotranscriptions.InCVPR,2022.1,2,3,4,8
inthewild. arXivpreprintarXiv:1212.0402,2012. 8
[81] Antoine Yang, Arsha Nagrani, Paul Hongsuck Seo, An-
[67] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier toineMiech,JordiPont-Tuset,IvanLaptev,JosefSivic,and
Martinet,Marie-AnneLachaux,Timothe´eLacroix,Baptiste CordeliaSchmid. Vid2seq: Large-scalepretrainingofavi-
Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. suallanguagemodelfordensevideocaptioning. InCVPR,
Llama: Open and efficient foundation language models. 2023. 3,8
arXivpreprint,2023. 3 [82] JiahuiYu,ZiruiWang,VijayVasudevan,LeggYeung,Mo-
[68] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive
RaphaelMarinier,MarcinMichalski,andSylvainGelly.To- captionersareimage-textfoundationmodels.arXivpreprint,
wardsaccurategenerativemodelsofvideo:Anewmetric& 2022. 3,5
challenges. arXivpreprint,2018. 8 [83] JiahuiYu,YuanzhongXu,JingYuKoh,ThangLuong,Gun-
[69] Ramakrishna Vedantam, C Lawrence Zitnick, and Devi janBaid,ZiruiWang,VijayVasudevan,AlexanderKu,Yin-
Parikh. Cider: Consensus-basedimage descriptionevalua- fei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han,
tion. InCVPR,2015. 6 Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and
YonghuiWu.Scalingautoregressivemodelsforcontent-rich
[70] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang,
text-to-imagegeneration. TMLR,2022. 2,3
XiangWang,andShiweiZhang. Modelscopetext-to-video
technicalreport. arXivpreprint,2023. 8 [84] SihyunYu,JihoonTack,SangwooMo,HyunsuKim,Junho
Kim, Jung-Woo Ha, and Jinwoo Shin. Generating videos
[71] TengWang,RuimaoZhang,ZhichaoLu,FengZheng,Ran
with dynamics-aware implicit generative adversarial net-
Cheng, and Ping Luo. End-to-end dense video captioning
works. arXivpreprintarXiv:2202.10571,2022. 8
withparalleldecoding. InICCV,2021. 8
[85] YoungjaeYu,JongseokKim,andGunheeKim. Ajointse-
[72] WenjingWang,HuanYang,ZixiTuo,HuiguoHe,Junchen
quence fusion model for video question answering and re-
Zhu,JianlongFu,andJiayingLiu. Videofactory: Swapat-
trieval. InECCV,2018. 7
tentioninspatiotemporaldiffusionsfortext-to-videogener-
[86] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
ation. arXivpreprint,2023. 3,8
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
[73] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-Fang BoxinLi,ChunyuanLi,etal. Florence: Anewfoundation
Wang,andWilliamYangWang. Vatex: Alarge-scale,high- modelforcomputervision. arXivpreprint,2021. 3
qualitymultilingualdatasetforvideo-and-languageresearch.
[87] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu,
InICCV,2019. 2,3
JaeSungPark,JizeCao,AliFarhadi,andYejinChoi. Mer-
[74] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun lot: Multimodalneuralscriptknowledgemodels. NeurIPS,
Huang,ZhiyuZhao,HongjieZhang,JilanXu,YiLiu,Zun 2021. 2,3
Wang,etal. Internvideo: Generalvideofoundationmodels [88] Hang Zhang, Xin Li, and Lidong Bing. Video-llama: An
via generative and discriminative learning. arXiv preprint, instruction-tunedaudio-visuallanguagemodelforvideoun-
2022. 7 derstanding. arXivpreprint,2023. 2,3,4,5,6
[75] Yi Wang, Yinan He, Yizhuo Li, Kunchang Li, Jiashuo Yu, [89] RichardZhang,PhillipIsola,AlexeiAEfros,EliShechtman,
Xin Ma, Xinhao Li, Guo Chen, Xinyuan Chen, Yaohui and Oliver Wang. The unreasonable effectiveness of deep
Wang, etal. Internvid: Alarge-scalevideo-textdatasetfor featuresasaperceptualmetric. InCVPR,2018. 4
multimodal understanding and generation. arXiv preprint [90] SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,
arXiv:2307.06942,2023. 3 MoyaChen,ShuohuiChen,ChristopherDewan,MonaDiab,
11XianLi,XiVictoriaLin,etal. Opt: Openpre-trainedtrans-
formerlanguagemodels. arXivpreprint,2022. 5,3
[91] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q Wein-
berger, andYoavArtzi. Bertscore: Evaluatingtextgenera-
tionwithbert. arXivpreprint,2019. 6
[92] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv,
Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video
generation with latent diffusion models. arXiv preprint,
2022. 3,8
[93] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards
automatic learning of procedures from web instructional
videos. InAAAI,2018. 2,3
[94] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mo-
hamed Elhoseiny. Minigpt-4: Enhancing vision-language
understandingwithadvancedlargelanguagemodels. arXiv
preprint,2023. 2,3,4,6
12Panda-70M: Captioning 70M Videos with Multiple Cross-Modality Teachers
Supplementary Material
Table of Contents
A.DetailsofSemantics-AwareVideoSplittingAlgorithm 1
A.1.Stage1:SplittingbasedonShotBoundaryDetection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1
A.2.Stage2:StitchingbasedonSemanticsSimilarity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2
B.DetailsofTeacherCaptioningModels:Pool,Inference,andSelection 2
B.1.Introductionof31CaptioningModelsPool. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
B.2.InferenceofCross-ModalityTeacherModelforVideoCaptioning . . . . . . . . . . . . . . . . . . . . . . . . 3
B.3.Selecting8CaptioningModelsbasedonaHumanEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . 3
C.DetailsofFine-GrainedVideo-to-TextRetrieval:Dataset,Training,andInference 5
C.1.CollectionofDataset . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
C.2.FinetuningofRetrievalModel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
C.3.InferenceofRetrievalModelonPanda-70M . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5
D.DetailsofStudentCaptioningModel:ArchitectureandTraining 6
D.1.ModelArchitecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6
D.2.TrainingDetails . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
E.VisualizationofPanda-70MDataset 7
E.1.Category:Animal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7
E.2.Category:Scenery . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
E.3.Category:Food . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8
E.4.Category:SportsActivity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
E.5.Category:Vehicles . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9
E.6.Category:TutorialandNarrative . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
E.7.Category:NewsandTVShows . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10
E.8.Category:Gamingand3DRendering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11
A.DetailsofSemantics-AwareVideoSplittingAlgorithm
In Section 3.1, we propose a video splitting algorithm to cut a long video into several semantically coherent clips. The
algorithmincludestwostages,splittingandstitching,forwhichthedetailsaredescribedinAppendixA.1andA.2.
A.1.Stage1: SplittingbasedonShotBoundaryDetection
WefirstsplitalongvideobyPySceneDetect[1]. Specifically,weuseContentDetectorwithcutscene thresholdof25and
min scene lenof15frames. Next,wedesignatwo-steppost-processingalgorithmtohandle1)longvideoswithcomplex
transitions,suchasfade-inandfade-outeffects,thatcannotbereliablydetectedbyPySceneDetectand2)uneditedfootage
thatdoesnotcontainanycut-scenesbuthassemanticchangeswithinthesameclip.
To handle both cases, we propose creating artificial scene cuts each 5 seconds for clips without cut-scene. That is, if a
video clip is longer than 5 seconds, we cut out the first 5 seconds as a new clip and recursively apply the same procedure
to the remaining part. Since we are only interested in semantically consistent video clips, we extract the ImageBind [25]
featuresoftheframesnearthebeginningortheend. Ifthefeaturesofthesetwoframesaredramaticallydifferentweremove
thatclip. Specifically,givenan-framevideoclipC,weextractthefeaturesf(C )andf(C )forthenumber0.1×nand
A B
0.9×nframes,denotedasC andC . Weonlykeepthevideoclipsifsatisfying∥f(C )−f(C )∥ ≤ 1.0. Assuch,we
A B A B
canexcludevideoclipswithtransitioneffectsorsignificantsemanticschangeswithinaclip.
1Video Duration (second)
Figure7.DistributionofvideodurationofPanda-70M.
Table7.Overviewof31teachermodels. 31teachermodelsarecomposedof6basemodelswithvariousweightsandinputinformation.
Inputdataincludesvision(V),subtitles(S),andmetadata(M).Visiondataiseitheravideoorastaticvideoframe,dependingonthetype
ofbasemodel.Metadataincludesthetitleandthedescriptionofavideo.Forexample,V-S-MforMiniGPT-4meansMiniGPT-4withthe
inputsofavideoframe,subtitles,andmetadata.
InputInformation
BaseModel Type Weights #ofModels
V V-S V-M V-S-M
Video-LLaMA[88] VideoVQA pretrain/finetune ✓ ✓ ✓ ✓ 8
VideoChat[38] VideoVQA 7B ✓ ✓ ✓ ✓ 4
VideoChatText[38] NLP-basedVideoVQA - ✓ ✓ ✓ ✓ 4
Video-ChatGPT[51] VideoVQA - ✓ ✓ ✓ ✓ 4
BLIP-2[37] ImageCaptioning opt2.7b/opt6.7b/flant5xl ✓ ✗ ✗ ✗ 3
MiniGPT-4[94] ImageVQA 7B/13B ✓ ✓ ✓ ✓ 8
A.2.Stage2: StitchingbasedonSemanticsSimilarity
Thefirststageintroducesmanyshortconsecutiveclipswiththesamesemanticcontent.Tothisend,weproposeanadditional
proceduretomergetheclipswiththesamesemanticcontent. Formally,giventwoadjacentclipsC1andC2insequence,we
concatenatethemintoaclipif∥f(C1)−f(C2)∥≤0.6.
B A
Finally,weperformapost-processingtostabilizethequalityanddiversityofthevideoclipswiththefollowingsteps:
• First,weexcludetheclipsshorterthan2secondsorclipsthatcontainonlyslightmotion(i.e.,∥f(C )−f(C )∥≤0.15).
A B
Forthevideoslongerthan60seconds,weonlyretainthefirst60seconds.
• Next,werepresenteachclipbytheaverageofImageBindfeaturesextractedfromstage1(SectionA.1)andonlykeep
thevideoclipsthataresemanticallydifferent(i.e., Euclideandistance> 0.3)fromtheprecedentclipstoincreasethe
diversityofthevideosamples.
• Finally,wetrimoutthefirstandlast10%ofavideoclipaswenoticethatthebeginningandtheendingofaclipusually
containunstablecameramovementortransitioneffects.
Withtheproposedsplittingalgorithm,wesplit3,790,459longvideosinto70,817,169clipswithanaverageclipduration
of8.477seconds. WeplotthedistributionofvideolengthinFigure7.
B.DetailsofTeacherCaptioningModels: Pool,Inference,andSelection
In Section 3.2, we propose to use multiple cross-modality teacher models for captioning. Specifically, we start with a
large pool including 31 captioning models. We elaborate on the composition of the model pool and how we implement
themforvideocaptioninginAppendixB.1andB.2respectively. Asrunningtheinferenceofthemodelsto70Mvideosis
computationally expensive, we select only 8 models as the representative, based on a human evaluation. We will describe
moredetailsaboutthisprocessinAppendixB.3.
2
)%(
oitaRYou are given some information about a video and will be asked to summarize the video (or the given video frame).
The subtitles of the video: “(video subtitles)”
Some descriptions of the video: [“(video title)”, “(video description)”]
Please faithfully summarize the video (or the video frame) in one sentence.
Figure8.PrompttemplateoftheVQAmodels.
B.1.Introductionof31CaptioningModelsPool
Theprimaryreasontoutilizecross-modalityteachermodelsistoleveragemultimodaldatathatwouldbenefitvideocaption-
ing. Assuch,weconsiderthebasemodels,includingimage/videovisual-question-answering(VQA)andimagecaptioning
models. Specifically, we employ Video-LLaMA [88], VideoChat [38], VideoChat Text [38], Video-ChatGPT [51], BLIP-
2 [37], and MiniGPT-4 [94] as the base models. Based on these models, we collect 31 captioning models in total using
differentweightsandinputinformation. WelistthesummaryofallcaptioningmodelsinTable7.
B.2.InferenceofCross-ModalityTeacherModelforVideoCaptioning
Welisttheinferencedetailsofeachbasemodelasfollows:
• Video-LLaMA[88]isavideoVQAmodel. Weonlyusethevisionbranchanddonotusetheaudioone. Themodel
uses Vicuna-7B [18] as the LLM to implement VQA. We use two official weights, including the pretraining weight,
whichistrainedon2.5Mvideo-textpairsandLLaVA-CC3M[43],andthefinetuningweight,whichisfurtherfinetuned
oninstruction-tuningdatafrom[38,43,94].
• VideoChat [38] and Video-ChatGPT [51] are video VQA models. We use Vicuna-7B as the LLM and follow the
officialcodebasefortherestoftheconfiguration.
• VideoChatText[38]isanatural-languageprocessing(NLP)-basedvideoVQAmodel. Themodelwouldtextualizethe
videocontentintovideotags,densecaptions,andageneralcaptionrespectivelybythreemodels[45,56,77]. Assuch,
userscanhaveaconversationwithachatbotanddiscussthevideobasedontheextractedtextualcontent. Theoriginal
codebaseusesChatGPT-4[53]asthechatbot,whichis,however,notfreelyreleasedtothepublic. Thus,wereplaceit
withLLaMA[67]forlarge-scalecaptioning.
• BLIP-2[37]isalanguage-imagepretrainingmodel. Weonlyuseitforimagecaptioninganddonotinputtexts. Weuse
theweightspretrainingwithdifferentLLMs,includingOPT[90](opt2.7bandopt6.7b)andFlanT5[19](flant5xl).
• MiniGPT-4[94]isanimageVQAmodel. WeusetwovariantsrespectivelywithVicuna-7BandVicuna-13BasLLMs.
Toimplementcross-modalityteachermodelsforvideocaptioning, wedesignthealgorithmsspecificallyforthemodels
of different modalities. For an image model, given an n-frame video clip, we randomly sample a video frame in-between
number 0.3 × N and 0.7 × N frames as the input. For a VQA model, in addition to the visual data, we also input a
text prompt that could include additional textual information, such as video title, description, and subtitles, to assist video
captioning.Specifically,weusetheprompttemplateinFigure8ifwewouldliketoincludetheinformationofeithermetadata
orsubtitlesorbothforcaptioning. Incontrast,weuseadummyprompt: “Pleasefaithfullysummarizethevideo(orimage)
inonesentence.” ifweonlyinputthevisiondataforcaptioning.
B.3.Selecting8CaptioningModelsbasedonaHumanEvaluation
Running 31 captioning models on 70M videos requires significant computation resources. Hence, we propose to find a
well-performingsubsetofthemodelsbyatwo-stepalgorithm,includingahumanevaluationandmodelselectionalgorithm.
Humanevaluation. First,weconductauserstudybyshowingtheoutputcaptionsofeachmodeltohumans. Specifically,
we randomly sample 1K video clips and perform the inference of 31 captioning models on each video. Next, the human
annotators are asked to select “every good caption”, where a good caption is defined as: “the caption cannot contain any
wrong information and needs to cover the main action OR all of the main objects presented in the video.” If none of the
captions is a good caption, the annotators are asked to select the “All Bad” option. We randomly shuffle 31 captions to
minimize the annotator’s bias on the order of the captions. Considering that a human is hard to focus on reading all 31
captionsentencesatthesametime,wesplitthecaptionsintothreegroups. Theannotatorwillseethesamevideothreetimes
withatmost11captionsonce. WeshowtheinterfaceofthisuserstudyinFigure9andplottheresultsinFigure10.
Algorithmofmodelselection. Inthesecondstep,wecollectalistof8captioningmodelsastherepresentativetoreducethe
computationforlarge-scalecaptioning. Intuitively,onemayoptforthemodelsexhibitingthetop8performance. Nonethe-
3Figure9.Screenshotoftheuserstudyinterface.
Video-LLaMA Video-LLaMA VideoChat VideoChatText Video-ChatGPT BLIP-2 MiniGPT-4 MiniGPT-4 All Bad
pretrain finetune opt2.7b/6.7b/flant5xl 7B 13B
Vision Vision-Subtitles Vision-Metadata Vision-Subtitles-Metadata
Figure10. Ratioofanindividualcaptioningmodeltopredictagoodcaption. Eachbarrepresentsanindividualmodelandiscolored
byitsinputinformation.Wehighlightthe8selectedteachermodelswithgray.Notethatwealsoreporttheratioof“AllBad”atrightmost.
less, such behavior does not align with the philosophy of our captioning algorithm. Specifically, our algorithm utilizes
multiplecross-modalitymodelstocovergoodcaptioningonvarioustypesofvideosandonlyretrievesonebestcaptionas
theannotationforeachvideo(asdescribedinSection3.3). Accordingly,weproposetousethesetofmodelsthatcanjointly
coveragoodcaptionformostvideosamples. Thealgorithmstartsbyselectingthebest-performingmodel(i.e.,BLIP-2with
opt6.7b). Next, weonlyconsiderthevideosthatthepreviouslyselectedmodel(s)cannotgenerateagoodcaptionandthen
greedily find the model that performs best on those videos. We recursively collect the models under this mindset until we
makethelistof8captioningmodels. The8selectedmodelsarehighlightedinFigure10.
Additionalfindings. FromFigure10,wecanalsoobservethatasinglecaptioningmodelcanpredictagoodcaptionforat
most30.8%ofthevideos. Incomparison,all31captioningcanjointlypredictatleastonegoodcaptionfor84.7%ofthe
videos (based on the “All Bad” ratio of 15.3%). This fact supports our motivation to use multiple cross-modality teacher
models to jointly predict the captions for a video. Last but not least, according to our statistics, using 8 selected teacher
captioningmodelscanjointlypredictagoodcaptionfor76.8%ofthevideoswhichshowscomparableperformancewithall
31modelswhilesignificantlyreducingthecomputationalrequirements.
4
)%(
etaRevitceleS13.93
8.15
5.27
6.34
10.54
10.23
9.85 UMT matching scores > 0.43
UMT matching scores < 0.43
6.51
Figure11.DistributionofthesourceteachermodelsofthecaptionsinPanda-70M.
C.DetailsofFine-GrainedVideo-to-TextRetrieval: Dataset,Training,andInference
InSection3.3,wementionthattheavailablegenericretrievalmodels[25,39]cannotpickthebestcaptionfrom8candidates
predictedbyourteachermodels. Themainreasonisthatallofthecandidatecaptionsarehighlyrelevanttothevideosample
and require the model to discern subtle distinctions within each caption for optimal performance. To better perform our
“fine-grained”retrievaltask,wefirstannotateasubsetofvideosamplesbymanuallyselectingthebestcaptionasdetailedin
AppendixC.1. Next, wefinetuneUnmaskedTeacher[39](UMT)andruntheinferenceofthemodelonallvideosamples
respectivelyinAppendixC.2andC.3.
C.1.CollectionofDataset
Werandomlysample100Kvideosamplesfromourdatasetandaskhumanannotatorstoselect“thebestcaption”foreach
video. Atthebeginningofthetask,theannotatorwillreadthetaskdescriptionasfollows:
“You are presented with a short video clip and a set of textual summaries that describe this clip. Choose the textual
summarythatisthemostfaithfulanddescriptiveofthecontentofthevideoclip. Imagineyouaretalkingonthephonewith
yourfriendandyouneedtodescribethevideotohim.”
NotethatthistaskisdifferentfromtheuserstudyinAppendixB.3,whereahumanisaskedtoselect“everygoodcaption”.
But,wealsorandomlyshufflethecaptionsandprovidean“AllBad”optionifallofthecaptionscontainwronginformation.
Wefilterout12,064videoswiththe“AllBad”optionselectedandsplitthedatasetinto86,131and1,805videosfortraining
andvalidation. WeplottheselectiverateofeachteachermodelonthevalidationsetinFigure3(bluebar).
C.2.FinetuningofRetrievalModel
WefinetuneUnmaskedTeacher[39]asthetextretrievalmodelonthetrainingset. Weusethelargermodelconfiguration,
consisting of ViT-L/16 [21] and BERTlarge [20], and initialize the model with the weights pretrained on 25M image-text
andvideo-textpairs. Wefollowtheoriginalcodebaseandonlyusethevideo-textcontrastive(VTC)andvideo-textmatch-
ing (VTM) loss functions for finetuning. For VTC, we implement hard negative mining [16, 35] which guides the model
focusing on distinguishing the selected caption (i.e., the positive sample) and the other 7 captions (i.e., the hard negative
samples). Specifically, we set the training weights of the positive and hard negatives as 1 while the weights of other neg-
atives (i.e., captions from other videos) as 0.01. For the training videos, we randomly sample 12 video frames and apply
RandomResizedCroptransformationwithscale[0.5,1.0]togetthevideowiththeresolutionof224×224px. Weusethe
AdamW[46]optimizerwithalearningrateof2e−5,β = [0.9,0.999],andaweightdecayof0.02. Wesetthebatchsizeof
32andlastthetrainingfor10epochs. Themodelisfuntunedon8NvidiaA100GPUs(80GB).
C.3.InferenceofRetrievalModelonPanda-70M
WiththefinetunedUMT,weautomaticallyretrievethebestcaptionastheannotationforall70Mvideos. Weillustratethe
distributionofthefinetunedUMT’sselectioninFigure11andthecaptionlengthinFigure12. Wealsoplotthewordcloud
oftherandomlysampled100KcaptionannotationsinFigure13tohighlighttherichcontentwithintheannotatedcaptions.
5<5 >40
Caption Length (#words)
Figure12.DistributionofcaptionlengthofPanda-70M.
Figure13.Wordcloudof100KcaptionsamplesinPanda-70M.
Inadditiontotheretrievalresult,UMTalsopredictsamatchingscoreforthevideo-textpair. Inpractice,wefindthescore
ishighlycorrelatedtothealignmentofthecontentswithinthevideo-textpair. Ascorehigherthan0.43usuallyrepresents
a strong association between the video and the caption. Numerically, 89.6% of the samples in Panda-70M have matching
scoreshigherthan0.43.
D.DetailsofStudentCaptioningModel: ArchitectureandTraining
D.1.ModelArchitecture
Figure4showsthearchitectureofthestudentcaptioningmodel. Themodelincludesavisionbranchandatextbranchfor
additionalsubtitleandmetadatainputs.
ThevisionbranchsharesthesamedesignasVideo-LLaMA[88]. Specifically,givenan8-framevideowiththeresolution
of 224×224px, a visual encoder first individually encodes each video frame into multiple frame-level features with the
dimensionof32×768. Thevisualencoderiscomposedofafrozenpretrainedvisualencoder, includingaViT-G/14from
EVA-CLIP [22] and a Q-former [37]. Subsequently, the temporal fusion module aggregates multiple frame-level features
intoasingle32×768videorepresentation. Themoduleincludesapositionembeddinglayertoinjecttemporalinformation
intovideoframesandavideoQ-formertofusetheframe-levelfeatures. Finally,themodelprojectsthevideorepresentation
intoa32×4096featurebyalinearlayer.
Forthetextbranch,givenapromptwithanarbitrarylength,themodelfirsttokenizesthepromptandembedseachtoken
intoafeaturevectorwith4096lengthbyapretrainedembeddinglayer[18].Consideringthatthenumberoftokenembedding
mightbelargeforalongerpromptandtheinformationofthepromptmightnotwellalignwiththevideocontent,wethen
designatextQ-Formertoextractafixedandshorterlengthoftextembeddingandatthesametime,betterbridgethefeature
6
)%(
oitaRoftheinputvideoandtextprompt. Specifically,thetextQ-Formertakestheinputsofthe32×4096videorepresentationas
thequeriesandmultipletokenembeddingasthekeyandvalue. Themodulethenoutputsa32×4096textrepresentation.
Finally,wecombinethemultimodalinputsbyconcatenatingthetextandvideorepresentationsinsequencetogeta64×4096
featureandinputittotheLLMtopredictthevideocaption.
D.2.TrainingDetails
Thetrainingdataincludesavideo-captionpairandadditionaltextinformation(i.e.,themetadataandsubtitles).Forthevideo
data,werandomlysample8framesandapplythesamevideoreadingalgorithmasinAppendixC.2. Forthetextbranch,we
embedtheextratextinformationintotheprompt. Tolearnacaptioningmodelthatcantakebothvideo-onlyandvideo-text
inputs,wedroppartofthetextinputsatrandom. Formally,weusetheprompttemplateinFigure8andemploythemetadata
or/andsubtitlesinformationwiththeprobabilityof0.5(thesamplingformetadataandsubtitlesareindependent).
We use the AdamW [46] optimizer. The learning rate is initialized as 1e−6 and linearly warmed up to 1e−4 within the
first 2,500 steps and gradually decreased to 5e−5 based on cosine annealing strategy [47]. We set β = [0.9,0.99] and use
aweightdecayof0.05. WetrainthemodelonthewholePanda-70Mwithabatchsizeof48andlastthetrainingfor300K
steps. Themodelistrainedon48NvidiaA100GPUs(80GB).
E.VisualizationofPanda-70MDataset
Inthefollowingsubsections,wevisualizevideo-textpairsinPanda-70Mbycategory.
E.1.Category: Animal
”A person is holding a long haired dachshund in their arms.”
”A group of dolphins are swimming in the ocean.”
”A rhino and a lion are fighting in the dirt.”
”A cat laying on a rug with a leash around its neck.”
7E.2.Category: Scenery
”There is a beach with waves and rocks in the foreground, and a city skyline in the background.”
”An aerial view of a freeway intersection at dusk.”
”The waves are crashing on the beach and the water is foamy.”
”There is a field of reeds blowing in the wind against a cloudy sky.”
E.3.Category: Food
”Someone is frying dough balls in a pan with oil.”
”A person is using a chef's knife to chop fresh parsley on a wooden cutting board.”
”A person is making a pie crust on a table.”
”There are sausages cooking on a grill, and a person is using tongs to turn them over.”
8E.4.Category: SportsActivity
”A female gymnast is practicing her skills on a climbing wall.”
”A group of young girls are playing soccer on a green grass field with a goalin the background.”
”A man paddles a canoe on a wave in the ocean.”
”A skateboarder performs a trick in a skate park.”
E.5.Category: Vehicles
”An orange dodge challenger parked in front of a house.”
”It is a rally car driving on a dirt road in the countryside, with people watching from the side of the road.”
”A remote controlmonster truck is driving on rough terrain.”
”A blue off-road truck is driving on a sand dune and jumping into the air.”
9E.6.Category: TutorialandNarrative
”A person in blue gloves is connecting an electrical supply to an injector.”
”A person is welding a piece of metal using a welding torch, and the metal is glowing red hot.”
”A person is using an electric drill to make a hole in a piece of cardboard.”
”A person is making a green clay model of a monster using different tools.”
E.7.Category: NewsandTVShows
”The columns of the temple of mars ultorin rome, italy are surrounded by trees and buildings.”
”A large pile of lava blocking a road.”
”Two men hugging each other in front of a trophy.”
”A rocket launches into space on the launch pad.”
10E.8.Category: Gamingand3DRendering
”A man in a spartan armor kneeling down.”
”A screenshot of a minecraftgame showing a snowy landscape.”
”A 3d rendering of a zoo with animals and a train.”
”The luxury yacht is sailing on calm waters with a beautiful sunset in the background.”
11