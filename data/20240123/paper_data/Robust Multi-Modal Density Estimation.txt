Robust Multi-Modal Density Estimation
AnnaMe´sza´ros∗, JulianF.Schumann∗, JavierAlonso-Mora,
ArkadyZgonnikov† and JensKober†
CognitiveRobotics,TUDelft,Netherlands
{A.Meszaros,J.F.Schumann,J.AlonsoMora,A.Zgonnikov,J.Kober}@tudelft.nl
Abstract
Developmentofmulti-modal, probabilisticpredic-
tion models has lead to a need for comprehensive
evaluationmetrics. Whileseveralmetricscanchar-
acterize the accuracy of machine-learned models
(e.g., negative log-likelihood, Jensen-Shannon di-
vergence),thesemetricstypicallyoperateonprob-
abilitydensities. Applyingthemtopurelysample-
based prediction models thus requires that the un-
derlying density function is estimated. However,
common methods such as kernel density estima-
tion(KDE)havebeendemonstratedtolackrobust-
ness, while more complex methods have not been
evaluated in multi-modal estimation problems. In
thispaper,wepresentROME(RObustMulti-modal
density Estimator), a non-parametric approach for
density estimation which addresses the challenge
ofestimatingmulti-modal,non-normal,andhighly
correlated distributions. ROME utilizes clustering
tosegmentamulti-modalsetofsamplesintomulti-
pleuni-modalonesandthencombinessimpleKDE
estimates obtained for individual clusters in a sin-
Figure 1: a) A schematic of a probabilistic model evaluation
gle multi-modal estimate. We compared our ap-
pipeline, using different metrics. b) ROME takes predicted sam-
proachtostate-of-the-artmethodsfordensityesti-
plesofagivenmodelandestimatestheirdensitiestoenablethecal-
mationaswellasablationsofROME,showingthat culationofprobability-basedmetrics(e.g.,negativelog-likelihood,
it not only outperforms established methods but is Jensen-Shannondivergence,etc.).
also more robust to a variety of distributions. Our
results demonstrate that ROME can overcome the
issuesofover-fittingandover-smoothingexhibited ial networks [Goodfellow et al., 2020], variational autoen-
byotherestimators,promisingamorerobusteval-
coders[Weietal.,2020],normalizingflows[Kobyzevetal.,
uationofprobabilisticmachinelearningmodels.
2020],andtheirnumerousvariations.
When probabilistic models are trained on multi-modal
data, they are often evaluated using simplistic metrics (e.g.,
1 Introduction MeanSquaredError(MSE)fromthegroundtruth).However,
suchsimplisticmetricsareunsuitedfordetermininghowwell
ThecriticalityofmanymodernAImodelsnecessitatesathor-
apredicteddistributioncorrespondstotheunderlyingdistri-
oughexaminationoftheiraccuracy.Becauseofthis,arguably
bution,astheydonotcapturethefittothewholedistribution.
anydata-drivenmodelcanonlybetakenseriouslyifdemon-
Forexample,thelowestMSEvaluecouldbeachievedbyac-
strated to sufficiently match the ground truth data. Never-
curatepredictionsofthemeanofthetrueunderlyingdistribu-
theless, a significant challenge arises when evaluating prob-
tionwhereaspotentialdifferencesinvarianceandshapeofthe
abilistic models that incorporate randomness into their out-
distribution would not be penalized. This necessitates more
puts. Examplesofsuchmodelsincludegenerativeadversar-
advancedmetricsthatevaluatethematchbetweenthemodel
∗Jointfirstauthors andthe(potentiallymulti-modal)data.Forinstance,Negative
†Jointlastauthors Log-Likelihood (NLL), Jensen-Shannon Divergence (JSD),
4202
naJ
91
]GL.sc[
1v66501.1042:viXraandExpectedCalibrationError(ECE)canbeusedtoevalu- andBengio,2002]. Itusesauniqueanisotropickernelforev-
ate how well the full distribution of the data is captured by erydatapoint,estimatedbasedonthecorrelationofthedirect
the learned models [Xu et al., 2019; Mozaffari et al., 2020; neighbors of each sample. However, it has not been previ-
Rasouli,2020]. However,mostdata-drivenmodelsrepresent ouslytestedinhigh-dimensionalbenchmarks,whichisespe-
the learned distribution implicitly, only providing individual cially problematic as the required memory scales quadrati-
samplesandnotthefulldistributionasanoutput. Thiscom- callywiththedimensionalityoftheproblem.
plicates the comparison of the model output to the ground- Another common subtype of Parzen windows are
truthdatadistributionssincetheabovemetricsrequiredistri- GMMs[Deisenrothetal.,2020],whichassumethatthedata
butions, not samples, as an input. Practically, this issue can distributioncanbecapturedthroughaweightedsumofGaus-
be addressed by estimating the predicted probability density sian distributions. The parameters of the Gaussian distri-
basedonsamplesgeneratedbythemodel. butions – also referred to as components – are estimated
Simple methods like Gaussian Mixture Models (GMM), throughexpectedlikelihoodmaximization. Nonetheless, es-
Kernel Density Estimation (KDE) [Deisenroth et al., 2020], peciallyfornon-normaldistributions,oneneedstohaveprior
and k-Nearest Neighbors (kNN) [Loftsgaarden and Quesen- knowledgeoftheexpectednumberofcomponentstoachieve
berry, 1965] are commonly used for estimating probabil- a good fit without over-fitting to the training data or over-
ity density functions. These estimators however rely on smoothing[McLachlanandRathnayake,2014].
strongassumptionsabouttheunderlyingdistribution,andcan Besides different types of Parzen windows, a prominent
thereby introduce biases or inefficiencies in the estimation alternative is kNN [Loftsgaarden and Quesenberry, 1965]
process. For example, problems can arise when encounter- which uses the local density of the k nearest neighbors
ingmulti-modal,non-normal,andhighlycorrelateddistribu- of every data point to estimate the overall density. While
tions (see Section 2). While more advanced methods such this method is non-parametric, it cannot be guaranteed that
as Manifold Parzen Windows (MPW) [Vincent and Bengio, the resulting distribution will be normalized [Silverman,
2002]andVineCopulas(VC)[NaglerandCzado,2016]ex- 1998]. This could be rectified by using Monte Carlo sam-
ist, they have not been thoroughly tested on such problems, pling to obtain a good coverage of the function’s domain
whichquestionstheirperformance. and obtain an accurate estimate of the normalization fac-
To overcome these limitations, we propose a novel den- tor,which,however,becomescomputationallyintractablefor
sity estimation approach: RObust Multi-modal Estimator high-dimensionaldistributions.
(ROME). ROME employs a non-parametric clustering ap- When it comes to estimating densities characterized by
proach to segment potentially multi-modal distributions into correlations between dimensions, copula-based methods are
separate uni-modal ones (Figure 1). These uni-modal sub- an often favored approach. Copula-based methods decom-
distributions are then estimated using a downstream proba- poseadistributionintoitsmarginalsandanadditionalfunc-
bilitydensityestimator(suchasKDE).Wetestourproposed tion, called a copula, which describes the dependence be-
approach against a number of existing density estimators in tween these marginals over a marginally uniformly dis-
three simple two-dimensional benchmarks designed to eval- tributed space. The downside of most copula-based ap-
uateamodel’sabilitytosuccessfullyreproducemulti-modal, proaches is that they rely on choosing an adequate copula
highly-correlated, and non-normal distributions. Finally, we function(e.g.,Gaussian,Student,orGumbel)andestimating
test our approach in a real-world setting using a distribution theirrespectiveparameters[Joe,2014]. Onenon-parametric
over future trajectories of human pedestrians created based copula-baseddensityestimator[BakamandPommeret,2023]
ontheForkingPathsdataset[Liangetal.,2020]. aimstoaddressthislimitationbyestimatingcopulaswiththe
helpofsuperimposedLegendrepolynomials. Whilethiscan
achievegoodresultsinestimatingthedensityfunction,itmay
2 RelatedWork
become computationally intractable as the distribution’s di-
The most common class of density estimators are so-called mensionalityincreases. Anotherapproachinvolvestheuseof
Parzen windows [Parzen, 1962], which estimate the den- VC[NaglerandCzado,2016],whichassumethatthewhole
sity through a combination of parametric Probability Den- distributioncanbedescribedasaproductofbivariateGaus-
sity Functions (PDFs). A number of common methods use siandistributions,thusalleviatingthecurseofdimensionality.
this approach, with KDE being a common non-parametric Its convergence, however, can only be guaranteed for nor-
method[Silverman,1998].Providedatypeofkernel–which maldistributions. Elsewhere[OtneimandTjøstheim,2017],
isoftentimesaGaussianbutcanbeanyothertypeofdistribu- asimilarapproachwaspursued, withchangessuchasusing
tion–KDEplacesthekernelsaroundeachsampleofadata logarithmic splines instead of univariate KDEs for estimat-
distributionandthensumsoverthesekernelstogetthefinal ing the marginals. However, both of these approaches are
densityestimationoverthedata. Thismethodisoftenchosen notdesignedformulti-modaldistributionsandhavenotbeen
asitdoesnotassumeanyunderlyingdistributiontype[Silver- thoroughlytestedonsuchproblems.
man,1998]. However,iftheunderlyingdistributionishighly
correlated, then the common use of a single isotropic ker- 3 RObustMulti-modalEstimator(ROME)
nel function can lead to over-smoothing [Vincent and Ben-
Theproblemofdensityestimationcanbeformalizedasfind-
gio, 2002; Wang et al., 2009]. Among multiple approaches
ingqueryablep∈P,where
for overcoming this issue [Wang et al., 2009; Gao et al., (cid:98)
2022],especiallynoteworthyistheMPWapproach[Vincent P =(cid:8) g :RM →R+|(cid:82) g(x)dx=1(cid:9) ,Algorithm1ROME 3.1 ExtractingClusters
functionTRAINROME(X) To cluster samples X, ROME employs the OPTICS algo-
▷Clustering(OPTICS) rithm [Ankerst et al., 1999] that can detect clusters of any
X I,N,R N ←REACHABILTYANALYSIS(X) shape with varying density. This algorithm uses reacha-
C,S ←{{1,...,N}},−1.1 bility analysis to sequentially transfer samples from a set
forallϵ∈εdo of unincluded samples X to the set of included samples
U,i
C ϵ ←DBSCAN(R I,N,ϵ) X I,i, starting with a random sample x 1 (X I,1 = {x 1} and
S ϵ ←SIL(C ϵ,X I,N) X U,1 = X\{x 1}). Thesamplesx i arethenselectedatiter-
ifS ϵ >S then ationiinthefollowingway:
C,S ←C ,S
ϵ ϵ
x = argmin r (x)= argmin min d (x,x) . (1)
forallξ ∈ξdo i i r (cid:101)
C ←ξ-clustering(R ,ξ)
x∈XU,i−1 x∈XU,i−1 x(cid:101)∈XI,i−1
ξ I,N
S ξ ←SIL(C ξ,X I,N) This sample is then transferred between sets, with X I,i =
ifS ξ >S then X I,i−1∪{x i}andX U,i = X U,i−1\{x i},whileexpanding
C,S ←C ξ,S ξ thereachabilitysetR i =R i−1∪{r i(x i)}(withR 1 ={∞}).
forallC ∈C do Thereachabilitydistanced r inEquation(1)isdefinedas
▷Decorrelation (cid:40) (cid:41)
x ←MEAN(X )
C C d (x,x)=max ∥x−x∥, min ∥x−x∥ ,
X ←X −x
r (cid:101) (cid:101) kc (cid:98)
C C C x(cid:98)∈X\{x}
R ←PCA(X )
C C
▷Normalization wherek cis:
Σ(cid:101)C ←STD(X CR CT) (cid:26) (cid:26) NM(cid:27)(cid:27)
▷T C PD← FR EsCT tiΣ m(cid:101) a− C t1 ion k c =min k max,max k min, α k . (2)
(cid:0) (cid:1)
p ←f X T Here, k is needed to ensure that the method is stable, as
(cid:98)C KDE C C min
atoolowk wouldmakethesubsequentclusteringvulnera-
returnC,{p ,x ,T |C ∈C} c
(cid:98)C C C
ble to sampling fluctuations. Meanwhile, k ensures that
max
thereachabilitydistancesareactuallybasedononlylocalin-
functionQUERYROME(x,X)
formation, and are not including points from other modes.
C,{p (cid:98)C,x C,T C|C ∈C}←TRAINROME(X)
Lastly,thetermNM/α isusedtokeepr (seeEquation(1))
l=0 k i
independentfromthenumberofsamples,whileallowingfor
forallC ∈C do
the higher number of samples needed in higher-dimensional
x←(x−x )T
(cid:98) C C spaces.
l←l+ln(p (x))+ln(|C|)−lnN+ln(|det(T )|)
(cid:98)C (cid:98) C The next part of the OPTICS algorithm – after obtain-
returnexp(l) ingthereachabilitydistancesR andtheorderedsetX
N I,N
– is the extraction of a set of clusters C, with clusters
C = {c ,...,c } ∈ C (with X = {x |j ∈
min max C I,N,j
C} ∈ R|C|×M and |C| ≥ 2). As the computational
suchthatpiscloseinsomesensetothenon-queryablePDF
(cid:98) cost of creating such a cluster set is negligible compared
p underlying the N available M-dimensional samples X ∈
to the reachability analysis, we can test multiple cluster-
RN×M: X ∼p.
ingsgeneratedusingtwodifferentapproaches(withr =
bound
A solution to the above problem would be an estimator min{r ,r }):
N,cmin N,cmax+1
f :RN×M →P,resultinginp (cid:98)=f(X). Ourproposedesti- • First,weuseDBSCAN[Esteretal.,1996]forclustering
matorf (Algorithm1)isbuiltontopofnon-parametric
ROME basedonanabsolutelimitϵ,whereaclustermustfulfill
clusterextraction. Namely, byseparatinggroupsofsamples
thecondition:
surrounded by areas of low density – expressing the mode
of the underlying distribution – we reduce the multi-modal r <ϵ≤r ∀c∈C\{c }. (3)
N,c bound min
densityestimationproblemtomultipleuni-modaldensityes-
timation problems for each cluster. The distributions within • Second, we use ξ-clustering [Ankerst et al., 1999] to
each cluster then become less varied in density or correla- cluster based on a proportional limit ξ, where a cluster
tion than the full distribution. Combining this with decorre- fulfills:
lationandnormalization,theuseofestablishedmethodssuch r
ξ ≤1− N,c ∀c∈C\{c }. (4)
asKDEtoestimateprobabilitydensitiesforthoseuni-modal r min
bound
distributionsisnowmorepromising,asproblemswithmulti-
modalityandcorrelatedmodes(seeSection2)areaccounted Samples not included in any cluster C that fulfills the re-
for. The complete multi-modal distribution is then obtained spectiveconditionsabovearekeptinaseparatenoisecluster
as a weighted average of the estimated uni-modal distribu- C ∈ C. UpongeneratingdifferentsetsofclustersC for
noise
tions. ϵ ∈ ε and ξ ∈ ξ, we select the C that achieves the highestAniso Varied TwoMoons
Figure2:Samplesfromthetwo-dimensionalsyntheticdistributions
usedforevaluatingdifferentprobabilitydensityestimators.
silhouettescore1 S =SIL(C,X )∈[−1,1][Rousseeuw,
I,N 3 4 5 6 7
1987]. The clustering then allows us to use PDF estimation x[m]
methodsonuni-modaldistributions.
Figure3:Samplesfromthemulti-modalpedestriantrajectorydistri-
3.2 FeatureDecorrelation
bution[Liangetal.,2020]usedforevaluatingdifferentprobability
In much of real-life data, such as the distributions of a per- densityestimators. Thetrajectoriesspan12timestepsrecordedata
son’s movement trajectories, certain dimensions of the data frequencyof2.5Hz
are likely to be highly correlated. Therefore, the features
in each cluster C ∈ C should be decorrelated using a ro- Thebandwidthb isthensetusingSilverman’srule[Silver-
tation matrix R C ∈ RM×M. In ROME, R C is foundusing man,1998]: C
PrincipalComponentAnalysis(PCA)[Woldetal.,1987]on
the cluster’s centered samples X C = X C −x C (x C is the b
=(cid:18)
M +2 n
(cid:19)− M1
+4 , n
=(cid:40)
1 C =C noise .
cluster’smeanvalue). Anexceptionarethenoisesamplesin C 4 C C |C| else
C ,whicharenotdecorrelated(i.e.,R = I).Onecan
noise Cnoise
Toevaluatethedensityfunctionp=f (X)foragiven
thengetthedecorrelatedsamplesX : (cid:98) ROME
PCA,C
samplex,wetaketheweightedaveragesofeachcluster’sp :
(cid:98)C
XT =R XT .
PCA,C C C (cid:88) |C|
p(x)= p ((x−x )T )|det(T )|.
3.3 Normalization (cid:98) N (cid:98)C C C C
C∈C
Afterdecorrelation,weusethematrixΣ(cid:101)C ∈ RM×M tonor-
Here, the term |C|/N is used to weigh the different distri-
malizeX :
PCA,C butions of each cluster with the size of each cluster, so that
(cid:16) (cid:17)−1 (cid:16) (cid:17)−1 each sample is represented equally. As the different KDEs
X(cid:99)C =X
PCA,C
Σ(cid:101)C =X CR CT Σ(cid:101)C =X CT C.
p arefittedtothetransformedsamples, weapplythemnot
(cid:98)C
to the original sample x, but instead apply the equal trans-
Here,Σ(cid:101)C isadiagonalmatrixwiththeentriesσ (cid:101)C(m).Toavoid
formationusedtogeneratethosetransformedsamples,using
over-fittingtohighlycorrelateddistributions,weintroducea
p ((x−x )).Toaccountforthechangeindensitywithina
regularizationwithavalueσ (similarto[VincentandBen- (cid:98)C C
min clusterCintroducedbythistransformation,weusethefactor
gio,2002])thatisappliedtotheempiricalstandarddeviation
|det(T )|.
σ(m) =(cid:112)V (X )(V isthevariancealongfeature C
PCA,C m PCA,C m
m)ofeachrotatedfeaturem: 4 Experiments
 (cid:26) √ (cid:27)
max (cid:80) V |Cm |( −X 1C) ,σ
min
ifC =C
noise
W erae tuc ro em (p Va Cre [o Nur aga lp ep rro anac dh Ca zg aa din os ,t 2tw 01o 6b ]a as ne dlin Mes Pf Wrom [Vt ih ne celi nt-
t
σ (cid:101)C(m) = (cid:18) C (cid:19) , and Bengio, 2002]) in four scenarios, using three metrics.
 1− maxσm σi (n m) σ P(m CA) ,C +σ min otherwise Additionally,wecarryoutanablationstudyonourproposed
m PCA,C methodROME.
withC ∈C\{C noise}.
Forthehyperparameterspertainingtotheclusteringwithin
ROME (see Section 3.1), we found empirically that stable
3.4 EstimatingtheProbabilityDensityFunction
results can be obtained using 199 possible clusterings, 100
Taking the transformed data (decorrelated and normalized),
forDBSCAN(Equation(3))
ROMEfitstheGaussianKDEf oneachseparatecluster
KDE (cid:26) (cid:16)α (cid:17)2
C aswellasthenoisesamplesC . Foragivenbandwidth
noise ε= minR + (max(R \{∞})−minR )|
b fordatainclusterC,thisresultsinapartialPDFp . N 99 N N
C (cid:98)C
p (cid:98)C(x (cid:98))=f
KDE(cid:16) X(cid:99)C(cid:17)
(x (cid:98))=
|C1
|
(cid:88)
N (x (cid:98)|χ,b CI) .
α∈{0,...,99}(cid:111)
χ∈X(cid:98)C combinedwith99forξ-clustering(Equation(4))
(cid:26) (cid:27)
1Thesilhouettescoremeasuresthesimilarityofeachobjecttoits β
ξ = |β ∈{1,...,99} ,
owncluster’sobjectscomparedtotheotherclusters’objects. 100
]m[y
8
7
6
5
4Table1:BaselineComparison–markedinredarecaseswithnotablypoorperformance;bestvaluesareunderlined.
D
JS
↓1
0
W(cid:99) →0 L(cid:98) ↑
Distribution ROME MPW VC ROME MPW VC ROME MPW VC
Aniso 0.010±0.001 0.026±0.002 0.005±0.001 −0.13±0.31 −0.60±0.13 1.91±0.91 −2.53±0.02 −2.57±0.02 −3.19±0.02
Varied 0.011±0.001 0.025±0.002 0.008±0.001 −0.13±0.20 −0.49±0.11 1.27±0.53 −4.10±0.03 −4.12±0.03 −4.29±0.03
TwoMoons 0.002±0.001 0.023±0.002 0.008±0.002 1.40±0.52 −0.52±0.10 1.36±0.51 −1.02±0.01 −0.36±0.01 −0.95±0.01
Trajectories 0.008±0.002 0.016±0.001 0.743±0.005 1.03±0.22 1.24±0.24 9.30±1.30 29.32±0.02 26.09±0.02 −215.23±17.6
ROME MPW VC • To test for over-fitting, we first sample two differ-
ent datasets X and X (N samples each) from p
1 2
(X ,X ∼p).Wethenusetheestimatorf tocreatetwo
1 2
queryable distributions p = f(X ) and p = f(X ).
(cid:98)1 1 (cid:98)2 2
If those distributions p and p are similar, it would
(cid:98)1 (cid:98)2
mean the tested estimator does not over-fit; we mea-
sure this similarity using the Jensen–Shannon Diver-
gence[Lin,1991]:
Figure4:SamplesobtainedwithROME,MPWandVC(pink)con-
1 (cid:88)
trastedwithsamplesfromp(blue);Varied. D (p ∥p )= h (x)+h (x)
JS (cid:98)1 (cid:98)2 2Nln(2) 1 2
x∈X1∪X2
(cid:18) (cid:19)
as well as using k = 5, k = 20, and α = 400 for p (x) 2p (x)
min max k h (x)= (cid:98)i ln (cid:98)i
calculatingk c(Equation(2)). i p (cid:98)1(x)+p (cid:98)2(x) p (cid:98)1(x)+p (cid:98)2(x)
4.1 Distributions
• To test the goodness-of-fit of the estimated density, we
In order to evaluate different aspects of a density estimation
methodf,weusedanumberofdifferentdistributions.
first generate a third set of samples X(cid:99)1 ∼ p
(cid:98)1
with N
samples. WethenusetheWassersteindistanceW [Vil-
• Threetwo-dimensionalsyntheticdistributions(Figure2)
lani,2009]onthedatatocalculatetheindicatorW(cid:99):
were used to test the estimation of distributions with
multiple clusters, which might be highly correlated
(Aniso)orofvaryingdensities(Varied),orexpressnon- W(cid:99) =
W(X 1,X(cid:99)1)−W(X 1,X 2)
W(X ,X )
normaldistributions(TwoMoons). 1 2
• A multivariate, 24-dimensional, and highly correlated
Here, W(cid:99) > 0 indicates over-smoothing or misplaced
distributiongeneratedfromasubsetoftheForkingPaths
modes,while−1≤W(cid:99) <0indicatesover-fitting.
dataset [Liang et al., 2020] (Figure 3). The 24 dimen-
sions correspond to the x and y positions of a human • Noteverydensityestimatorf hastheabilitytogenerate
jp ee cd toes rt ir ei san (xa ∗cr ∈oss R1 12 2×t 2im ),e wst eep ds e. fiB nea dse td heon un6 do er ri lg yi in na gl dtr ia s-- thesamplesX(cid:99)1. Consequently,weneedtotestforover-
i smoothing (and over-fitting as well) without relying on
tributionpinsuchaway,thatonecouldcalculateasam-
plex∼pwith:
X(cid:99)1. Therefore,weusetheaveragelog-likelihood
x=sx∗ iR θT +Ln, withi∼U{1,6}.
L(cid:98) =
N1 (cid:88)
ln(p (cid:98)1(x)) ,
Here, R θ ∈ R2×2 is a rotation matrix rotating x∗ i by x∈X2
θ ∼N(0, π ),whiles∼N(1,0.03)isascalingfactor.
n=N(0,1 08 .0 03I)∈R12×2isadditionalnoiseaddedon which would be maximized only for p = p (cid:98)1 as long
all dimensions using L ∈ R12×12, a lower triangular as X 2 is truly representative of p (see Gibbs’ inequal-
ity[Kvalseth,1997]). However,usingthismetricmight
matrixthatonlycontainsones.
bemeaninglessifp isnotnormalized, asthepresence
(cid:98)1
4.2 EvaluationandMetrics
oftheunknownscalingfactormakestheL(cid:98)valuesoftwo
estimatorsincomparable.
Whenestimatingdensityp,sincewecannotquerythedistri-
(cid:98)
bution p underlying the samples X, we require metrics that For each candidate method f, we used N = 3000, and
can provide insights purely based on those samples. To this everymetriccalculationisrepeated100timestotakeintoac-
endweusethefollowingthreemetricstoquantifyhowwell counttheinherentrandomnessofsamplingfromthedistribu-
a given density estimator f can avoid both over-fitting and tions,withthestandarddeviationinthetablesbeingreported
over-smoothing. withthe .
±Table 2: Ablations (W(cid:99) → 0, Varied): Clustering is essential to Table3:Ablations(L(cid:98)↑,Aniso):Whenclustering,decorrelationand
preventover-smoothing. ROMEhighlightedingray. Notethatthe normalizationimproveresultsfordistributionswithhighintra-mode
differencesbetweenSilhouetteandDBCVarenotstatisticallysig- correlation.ROMEhighlightedingray.
nificant.
Norm. Nonorm.
Norm. Nonorm. Clustering Decorr. Nodecorr.
f
Clustering Decorr. Nodecorr. GMM
Silhouette −2.53±0.02 −2.70±0.01 −2.79±0.01
Silhouette −0.13±0.20 −0.13±0.20 −0.13±0.19 −0.14±0.20
DBCV −2.56±0.02 −2.69±0.01 −2.83±0.02
DBCV −0.09±0.23 −0.09±0.23 −0.07±0.23 −0.03±0.24
Noclusters 2.28±0.72 2.26±0.72 −0.17±0.26 10.53±2.54 Table4:Ablations(W(cid:99)→0,TwoMoons):Excludingnormalization
orusingf asthedownstreamestimatorisnotrobustagainstnon-
GMM
normaldistributions.ROMEhighlightedingray.
4.3 Ablations
To better understand the performance of our approach, we Norm. Nonorm.
f
investigatedvariationsinfourkeyaspectsofROME: Clustering Decorr. Nodecorr. GMM
• Clustering approach. First, we replaced the silhouette Silhouette 1.40±0.52 1.41±0.52 3.65±0.99 4.37±1.14
score (see Section 3.1) with density based cluster val-
idation (DBCV) [Moulavi et al., 2014] when selecting DBCV 1.59±0.56 1.59±0.56 4.24±1.13 4.77±1.23
the optimal clustering out of the 199 possibilities. Fur- Noclusters 1.82±0.60 1.84±0.60 3.17±0.89 5.29±1.34
thermore,weinvestigatedtheapproachofnoclustering
(C ={{1,...,N}}).
• DecorrelationvsNodecorrelation. Weinvestigatedthe two-dimensional distributions compared to our approach, as
effectofablatingrotationbysettingR C =I. quantified by lower D JS values achieved by ROME. MPW
does, however, achieve a better log-likelihood for the Two
• Normalization vs No normalization. We studied the
Moons distribution compared to ROME. This could be due
sensitivity of our approach to normalization by setting
to the locally adaptive non-Gaussian distributions being less
Σ(cid:101)C =I.
susceptible to over-smoothing than our approach of using a
• Downstream density estimator. We replaced f with single isotropic kernel for each cluster if such clusters are
KDE
two other candidate methods. First, we used a single- highly non-normal. Lastly, in the case of the pedestrian tra-
componentGaussianMixtureModelf jectoriesdistribution,ourapproachoncemoreachievesbetter
GMM
performance than MPW, with MPW performing worse both
(cid:16) (cid:17)
f GMM(X)(x)=N x|µ (cid:98)X,Σ(cid:98)X intermsofD JSandthelog-likelihoodestimates.
Meanwhile, the Vine Copulas approach exhibits the ten-
fittedtotheobservedmeanµ
(cid:98)
andcovariancematrixΣ(cid:98) dency to over-smooth the estimated densities (large positive
of a dataset X. Second, we used a k-nearest neighbor W(cid:99) values), and even struggles with capturing the different
approachf [LoftsgaardenandQuesenberry,1965] modes (see Figure 4). This is likely because VC uses KDE
kNN
with Silverman’s rule of thumb, which is known to lead to
k
f (X)(x)= over-smoothinginthecaseofmulti-modaldistributions[Hei-
kNN N V min ∥x−x∥M denreichetal.,2013]. Furthermore,onthepedestriantrajec-
M k (cid:98)
x(cid:98)∈X
tories distribution, we observed both high D
JS
and W(cid:99) val-
whereV isthevolumeoftheM-dimensionalunithy- ues, indicating that VC is unable to estimate the underlying
M √
persphere. We used the rule-of-thumb k = ⌊ N⌋. density;thisisalsoindicatedbythepoorlog-likelihoodesti-
However, this estimator does not enable sample gener- mates.
ation. Overall,whilethebaselineswereabletoachievebetterper-
formanceinselectedcases(e.g.,MPWbetterthanROMEin
Whilethosefourfactorswouldtheoreticallyleadto24esti-
mators,f aswellasf beinginvariantagainstrotation termsofW(cid:99)andL(cid:98)ontheTwoMoonsdistribution),theyhave
KDE kNN
and f being invariant against any linear transformation theirapparentweaknesses. Specifically,MPWachievespoor
GMM
meansthatonly14ofROME’sablationsareactuallyunique. resultsformostmetricsinthecaseofvaryingdensitieswithin
themodes(Varied),whileVineCopulasobtaintheworstper-
formanceacrossallthreemetricsinthecaseofthemultivari-
5 Results
atetrajectorydistributions.ROME,incontrast,achievedhigh
5.1 BaselineComparison performanceacrossallthetestcases.
We found that ROME avoids major pitfalls of the two base-
5.2 AblationStudies
line methods on the four tested distributions (Table 1). Out
of the two baseline methods, the Manifold Parzen Windows Whenitcomestothechoiceoftheclusteringmethod,ourex-
approachhasastrongertendencytoover-fitinthecaseofthe perimentsshownoclearadvantageforusingeitherthesilhou-Table5: Ablations(D ↓1,Trajectories; valuesaremultipliedby10foreasiercomprehension): Usingf asthedownstreamestimator
JS 0 kNN
tendstoleadtoover-fitting.ROMEhighlightedingray.
Normalization Nonormalization
Decorrelation Nodecorrelation f
GMM
Clustering f f f f f f
KDE kNN KDE kNN KDE kNN
Silhouette 0.084±0.016 1.045±0.064 0.777±0.116 1.808±0.112 0.015±0.011 1.887±0.118 0.032±0.007
DBCV 0.090±0.015 1.119±0.073 0.897±0.154 1.937±0.109 0.017±0.012 1.934±0.116 0.043±0.010
Noclusters 0.009±0.004 0.453±0.051 0.015±0.012 1.044±0.104 0.005±0.003 1.478±0.132 0.017±0.011
ROME KDE ROME ROMEwithf GMM
Figure 5: Samples generated by ROME and KDE – equivalent to Figure6: SamplesgeneratedbyROME,andROMEwithf as
GMM
ROME without clustering, decorrelation and normalization – (in thedownstreamestimator(inpink)contrastedwithsamplesfromp
pink) contrasted with samples from p (in blue); Aniso. Note that (inblue);TwoMoons. Notethatthesampleswhenusingf are
GMM
the samples by KDE are more spread out which indicates over- more spread out, despite having a component for each of the two
smoothing. clusters,whichclearlydisplaysover-smoothing.
ette score or DBCV. But as the silhouette score is computa- over-smooththeestimateddensityincaseswheretheunder-
tionallymoreefficientthanDBCV,itisthepreferredmethod. lying distribution is not Gaussian (high W(cid:99) in Table 4). The
However, using clustering is essential, as otherwise there is over-smoothingcausedbyf isfurthervisualisedinFig-
GMM
ariskofover-smoothing,suchasinthecaseofmulti-modal ure6.
distributionswithvaryingdensitiesineachmode(Table2). In conclusion, our ablation studies confirmed that using
Testing variants of ROME on the Aniso distribution (Ta- f KDE incombinationwithdataclustering,normalizationand
ble 3) demonstrated not only the need for decorrelation, decorrelation provides the most reliable density estimation
throughtheuseofrotation,butalsonormalizationinthecase for different types of distributions without relying on prior
of distributions with highly correlated features. There, us- knowledge.
ingeitherofthetwoclusteringmethodsincombinationwith
normalization and decorrelation (our full proposed method) 6 Conclusion
isbetterthanthetwoalternativesofomittingonlydecorrela-
In our comparison against two established and sophisticated
tion or both decorrelation and normalization. In the case of
densityestimators,weobservedthatROMEachievedconsis-
clusteringwiththesilhouettescore,thefullmethodissignif-
tentlygoodperformanceacrossalltestcases,whileManifold
icantlymorelikelytoreproducetheunderlyingdistributionp
ParzenWindowsandVineCopulasweresusceptibletoover-
byafactorof1.19(p < 10−50)asopposedtoomittingonly
fitting and over-smoothing. Furthermore, as part of several
decorrelation,andby1.30comparedtoomittingbothdecor-
ablation studies, we found that ROME could overcome the
relationandnormalization. Similartrendscanbeseenwhen
shortcomings of other common density estimators, such as
clusteringbasedonDBCV,withthefullmethodbeingmore
the over-fitting exhibited by kNN or the over-smoothing by
likelytoreproducepbyafactorof1.14and1.31respectively.
GMM. In those studies, we additionally demonstrated that
Results on the Aniso distribution further show that KDE on
ourapproachofusingclustering, decorrelation, andnormal-
itsownisnotabletoachievethesameresultsasROME,but
ization is indispensable for overcoming the deficiencies of
ratherithasatendencytoover-smooth(Figure5).
KDE.Futureworkcanfurtherimproveonourresultsbyin-
Additionally, the ablation with and without normalization vestigatingtheintegrationofmoresophisticateddensityesti-
ontheTwoMoonsdistribution(Table4)showedthatnormal- mation methods, such as MPW or VC, instead of the kernel
ization is necessary to avoid over-smoothing in the case of densityestimatorinourproposedapproach.
non-normaldistributions. Overall,byprovidingasimplewaytoaccuratelyestimate
Lastly, investigating the effect of different downstream learned distributions based on model-generated samples,
densityestimators,wefoundthatusingROMEwithf in- ROME can help evaluate AI models in non-deterministic
kNN
steadoff leadstoover-fitting(highestD valuesinTa- scenarios, paving the way for more reliable AI applications
KDE JS
ble5). Meanwhile,ROMEwithf exhibitsatendencyto acrossvariousdomains.
GMMAcknowledgments [Lin,1991] JianhuaLin. Divergencemeasuresbasedonthe
shannonentropy. IEEETransactionsonInformationThe-
ThisresearchwassupportedbyNWO-NWAproject“Acting
ory,37(1):145–151,1991.
underuncertainty”(ACT),NWA.1292.19.298.
[LoftsgaardenandQuesenberry,1965] Don O Loftsgaarden
References andCharlesPQuesenberry.Anonparametricestimateofa
multivariatedensityfunction.TheAnnalsofMathematical
[Ankerstetal.,1999] Mihael Ankerst, Markus M Breunig,
Statistics,36(3):1049–1051,1965.
Hans-Peter Kriegel, and Jo¨rg Sander. Optics: Ordering
points to identify the clustering structure. ACM Sigmod [McLachlanandRathnayake,2014] Geoffrey J McLachlan
Record,28(2):49–60,1999. and Suren Rathnayake. On the number of components
in a gaussian mixture model. Wiley Interdisciplinary Re-
[BakamandPommeret,2023] Yves I Ngounou Bakam and
views: DataMiningandKnowledgeDiscovery,4(5):341–
Denys Pommeret. Nonparametric estimation of copulas
355,2014.
and copula densities by orthogonal projections. Econo-
metricsandStatistics,2023. [Moulavietal.,2014] DavoudMoulavi,PabloAJaskowiak,
Ricardo JGB Campello, Arthur Zimek, and Jo¨rg Sander.
[Deisenrothetal.,2020] Marc Peter Deisenroth, A Aldo
Density-based clustering validation. In Proceedings of
Faisal, and Cheng Soon Ong. Mathematics for Machine
the2014SIAMInternationalConferenceonDataMining,
Learning. CambridgeUniversityPress,2020.
pages839–847.SIAM,2014.
[Esteretal.,1996] Martin Ester, Hans-Peter Kriegel, Jo¨rg
[Mozaffarietal.,2020] SajjadMozaffari,OmarYAl-Jarrah,
Sander, and Xiaowei Xu. A density-based algorithm for
MehrdadDianati,PaulJennings,andAlexandrosMouza-
discovering clusters in large spatial databases with noise.
kitis. Deeplearning-basedvehiclebehaviorpredictionfor
InProceedingsoftheSecondInternationalConferenceon
autonomousdrivingapplications: Areview. IEEETrans-
KnowledgeDiscoveryandDataMining,volume96,pages
actions on Intelligent Transportation Systems, 23(1):33–
226–231,1996.
47,2020.
[Gaoetal.,2022] Jia-Xing Gao, Da-Quan Jiang, and Min-
[NaglerandCzado,2016] Thomas Nagler and Claudia
PingQian. Adaptivemanifolddensityestimation. Journal
Czado. Evading the curse of dimensionality in nonpara-
of Statistical Computation and Simulation, 92(11):2317–
metric density estimation with simplified vine copulas.
2331,2022.
JournalofMultivariateAnalysis,151:69–89,2016.
[Goodfellowetal.,2020] Ian Goodfellow, Jean Pouget-
[OtneimandTjøstheim,2017] Ha˚kon Otneim and Dag
Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Tjøstheim. The locally gaussian density estimator for
SherjilOzair,AaronCourville,andYoshuaBengio. Gen-
multivariate data. Statistics and Computing, 27:1595–
erative adversarial networks. Communications of the
1616,2017.
ACM,63(11):139–144,2020.
[Heidenreichetal.,2013] Nils-Bastian Heidenreich, Anja [Parzen,1962] Emanuel Parzen. On estimation of a proba-
Schindler, and Stefan Sperlich. Bandwidth selection for bility density function and mode. The Annals of Mathe-
kerneldensityestimation: areviewoffullyautomaticse- maticalStatistics,33(3):1065–1076,1962.
lectors. AStA Advances in Statistical Analysis, 97:403– [Rasouli,2020] Amir Rasouli. Deep learning for
433,2013. vision-based prediction: A survey. arXiv preprint
[Joe,2014] HarryJoe. Dependencemodeling withcopulas. arXiv:2007.00095,2020.
CRCpress,2014. [Rousseeuw,1987] PeterJRousseeuw. Silhouettes:agraph-
[Kobyzevetal.,2020] IvanKobyzev,SimonJDPrince,and icalaidtotheinterpretationandvalidationofclusteranal-
Marcus A Brubaker. Normalizing flows: An introduc- ysis. JournalofComputationalandAppliedMathematics,
tionandreviewofcurrentmethods. IEEETransactionson 20:53–65,1987.
Pattern Analysis and Machine Intelligence, 43(11):3964–
[Silverman,1998] BernardWSilverman.Densityestimation
3979,2020.
forstatisticsanddataanalysis. Routledge,1998.
[Kvalseth,1997] TaraldOKvalseth. Generalizeddivergence
[Villani,2009] Ce´dric Villani. Optimal transport: Old and
andgibbs’inequality. In1997IEEEInternationalConfer-
new,volume338. Springer,2009.
ence on Systems, Man, and Cybernetics. Computational
CyberneticsandSimulation,volume2,pages1797–1801. [VincentandBengio,2002] PascalVincentandYoshuaBen-
IEEE,1997. gio. Manifoldparzenwindows. AdvancesinNeuralInfor-
mationProcessingSystems,15,2002.
[Liangetal.,2020] Junwei Liang, Lu Jiang, Kevin Mur-
phy, Ting Yu, and Alexander Hauptmann. The garden [Wangetal.,2009] Xiaoxia Wang, Peter Tino, Mark A
of forking paths: Towards multi-future trajectory predic- Fardal,SomakRaychaudhury,andArifBabul.Fastparzen
tion. In Proceedings of the IEEE/CVF Conference on window density estimator. In 2009 International Joint
Computer Vision and Pattern Recognition, pages 10508– ConferenceonNeuralNetworks,pages3267–3274.IEEE,
10518,2020. 2009.[Weietal.,2020] Ruoqi Wei, Cesar Garcia, Ahmed El-
Sayed, Viyaleta Peterson, and Ausif Mahmood. Vari-
ations in variational autoencoders-a comparative evalua-
tion. IEEEAccess,8:153651–153670,2020.
[Woldetal.,1987] Svante Wold, Kim Esbensen, and Paul
Geladi. Principalcomponentanalysis. Chemometricsand
IntelligentLaboratorySystems,2(1-3):37–52,1987.
[Xuetal.,2019] DonnaXu,YaxinShi,IvorWTsang,Yew-
Soon Ong, Chen Gong, and Xiaobo Shen. Survey on
multi-outputlearning. IEEETransactionsonNeuralNet-
worksandLearningSystems,31(7):2409–2429,2019.