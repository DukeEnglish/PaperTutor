Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data
LiheYang1 BingyiKang2† ZilongHuang2 XiaogangXu3,4 JiashiFeng2 HengshuangZhao1†
1TheUniversityofHongKong 2TikTok 3ZhejiangLab 4ZhejiangUniversity
†correspondingauthors
https://depth-anything.github.io
Figure1.Ourmodelexhibitsimpressivegeneralizationabilityacrossextensiveunseenscenes.Lefttwocolumns:COCO[35].Middletwo:
SA-1B[27](ahold-outunseenset).Righttwo:photoscapturedbyourselves.Ourmodelworksrobustlyinlow-lightenvironments(1stand
3rdcolumn),complexscenes(2ndand5thcolumn),foggyweather(5thcolumn),andultra-remotedistance(5thand6thcolumn),etc.
Abstract 1.Introduction
ThisworkpresentsDepthAnything1,ahighlypractical Thefieldofcomputervisionandnaturallanguageprocessing
solutionforrobustmonoculardepthestimation. Withoutpur- iscurrentlyexperiencingarevolutionwiththeemergenceof
suingnoveltechnicalmodules,weaimtobuildasimpleyet “foundationmodels”[6]thatdemonstratestrongzero-/few-
powerfulfoundationmodeldealingwithanyimagesunder shotperformanceinvariousdownstreamscenarios[44,58].
anycircumstances. Tothisend,wescaleupthedatasetby Thesesuccessesprimarilyrelyonlarge-scaletrainingdata
designingadataenginetocollectandautomaticallyanno- thatcaneffectivelycoverthedatadistribution. Monocular
tatelarge-scaleunlabeleddata(∼62M),whichsignificantly DepthEstimation(MDE),whichisafundamentalproblem
enlarges the data coverage and thus is able to reduce the withbroadapplicationsinrobotics[65],autonomousdriv-
generalizationerror. Weinvestigatetwosimpleyeteffective ing[63,79],virtualreality[47],etc.,alsorequiresafoun-
strategiesthatmakedatascaling-uppromising.First,amore dation model to estimate depth information from a single
challengingoptimizationtargetiscreatedbyleveragingdata image. However, this has been underexplored due to the
augmentationtools. Itcompelsthemodeltoactivelyseek difficultyofbuildingdatasetswithtensofmillionsofdepth
extravisualknowledgeandacquirerobustrepresentations. labels. MiDaS[45]madeapioneeringstudyalongthisdi-
Second, an auxiliary supervision is developed to enforce rectionbytraininganMDEmodelonacollectionofmixed
themodeltoinheritrichsemanticpriorsfrompre-trained labeled datasets. Despite demonstrating a certain level of
encoders. Weevaluateitszero-shotcapabilitiesextensively, zero-shotability,MiDaSislimitedbyitsdatacoverage,thus
includingsixpublicdatasetsandrandomlycapturedphotos. sufferingdisastrousperformanceinsomescenarios.
Itdemonstratesimpressivegeneralizationability(Figure1). Inthiswork,ourgoalistobuildafoundationmodelfor
Further,throughfine-tuningitwithmetricdepthinformation MDEcapableofproducinghigh-qualitydepthinformation
fromNYUv2andKITTI,newSOTAsareset.Ourbetterdepth foranyimagesunderanycircumstances. Weapproachthis
modelalsoresultsinabetterdepth-conditionedControlNet. targetfromtheperspectiveofdatasetscaling-up. Tradition-
Ourmodelsarereleasedhere. ally,depthdatasetsarecreatedmainlybyacquiringdepth
datafromsensors[18,54],stereomatching[15],orSfM[33],
whichiscostly,time-consuming,orevenintractableinpartic-
TheworkwasdoneduringaninternshipatTikTok.
1Whilethegrammaticalsoundnessofthisnamemaybequestionable, ularsituations. Weinstead,forthefirsttime,payattentionto
wetreatitasawholeandpayhomagetoSegmentAnything[27]. large-scaleunlabeleddata. Comparedwithstereoimagesor
1
4202
naJ
91
]VC.sc[
1v19801.1042:viXralabeledimagesfromdepthsensors,ourusedmonocularunla- cheap,anddiverseunlabeledimagesforMDE.
beledimagesexhibitthreeadvantages: (i)(simpleandcheap
• We point out a key practice in jointly training large-
toacquire)Monocularimagesexistalmosteverywhere,thus
scalelabeledandunlabeledimages. Insteadoflearning
they are easy to collect, without requiring specialized de-
rawunlabeledimagesdirectly,wechallengethemodel
vices. (ii)(diverse)Monocularimagescancoverabroader
withaharderoptimizationtargetforextraknowledge.
rangeofscenes,whicharecriticaltothemodelgeneraliza-
tionabilityandscalability. (iii)(easytoannotate)Wecan • We propose to inherit rich semantic priors from pre-
simplyuseapre-trainedMDEmodeltoassigndepthlabels trainedencodersforbettersceneunderstanding,rather
forunlabeledimages,whichonlytakesafeedforwardstep. thanusinganauxiliarysemanticsegmentationtask.
Morethanefficient,thisalsoproducesdenserdepthmaps • Ourmodelexhibitsstrongerzero-shotcapabilitythan
thanLiDAR[18]andomitsthecomputationallyintensive MiDaS-BEiT [5]. Further,fine-tunedwithmetric
L-512
stereomatchingprocess. depth,itoutperformsZoeDepth[4]significantly.
Wedesignadataenginetoautomaticallygeneratedepth
annotationsforunlabeledimages,enablingdatascaling-up 2.RelatedWork
toarbitraryscale.Itcollects62Mdiverseandinformativeim-
agesfromeightpubliclarge-scaledatasets,e.g.,SA-1B[27], Monoculardepthestimation(MDE).Earlyworks[23,36,
OpenImages[30],andBDD100K[81]. Weusetheirraw 50]primarilyreliedonhandcraftedfeaturesandtraditional
unlabeledimageswithoutanyformsoflabels. Then,inor- computervisiontechniques. Theywerelimitedbytheirre-
dertoprovideareliableannotationtoolforourunlabeled lianceonexplicitdepthcuesandstruggledtohandlecomplex
images, we collect 1.5M labeled images from six public sceneswithocclusionsandtexturelessregions.
datasetstotrainaninitialMDEmodel. Theunlabeledim- Deeplearning-basedmethodshaverevolutionizedmonoc-
ages are then automatically annotated and jointly learned ulardepthestimationbyeffectivelylearningdepthrepresen-
withlabeledimagesinaself-trainingmanner[31]. tations from delicately annotated datasets [18, 54]. Eigen
Despitealltheaforementionedadvantagesofmonocular et al. [17] first proposed a multi-scale fusion network to
unlabeledimages,itisindeednottrivialtomakepositiveuse regressthedepth. Followingthis,manyworksconsistently
ofsuchlarge-scaleunlabeledimages[72,89],especiallyin improvethedepthestimationaccuracybycarefullydesign-
thecaseofsufficientlabeledimagesandstrongpre-training ing the regression task as a classification task [3, 34], in-
models. Inourpreliminaryattempts,directlycombiningla- troducingmorepriors[32,53,75,82],andbetterobjective
beledandpseudolabeledimagesfailedtoimprovethebase- functions[67,77],etc. Despitethepromisingperformance,
lineofsolelyusinglabeledimages. Weconjecturethat,the theyarehardtogeneralizetounseendomains.
additionalknowledgeacquiredinsuchanaiveself-teaching Zero-shotdepthestimation. Ourworkbelongstothisre-
mannerisratherlimited. Toaddressthedilemma,wepro- searchline. WeaimtotrainanMDEmodelwithadiverse
pose to challenge the student model with a more difficult trainingsetandthuscanpredictthedepthforanygivenim-
optimization target when learning the pseudo labels. The age. Somepioneeringworks[10,66]exploredthisdirection
student model is enforced to seek extra visual knowledge bycollectingmoretrainingimages,buttheirsupervisionis
andlearnrobustrepresentationsundervariousstrongpertur- verysparseandisonlyenforcedonlimitedpairsofpoints.
bationstobetterhandleunseenimages. To enable effective multi-dataset joint training, a mile-
Furthermore,therehavebeensomeworks[9,21]demon- stoneworkMiDaS[45]utilizesanaffine-invariantlossto
strating the benefit of an auxiliary semantic segmentation ignorethepotentiallydifferentdepthscalesandshiftsacross
taskforMDE.Wealsofollowthisresearchline,aimingto varyingdatasets. Thus,MiDaSprovidesrelativedepthinfor-
equipourmodelwithbetterhigh-levelsceneunderstanding mation. Recently,someworks[4,22,78]takeastepfurther
capability. However,weobservedwhenanMDEmodelis toestimatethemetricdepth. However,inourpractice,we
already powerful enough, it is hard for such an auxiliary observesuchmethodsexhibitpoorergeneralizationability
task to bring further gains. We speculate that it is due to than MiDaS, especially its latest version [5]. Besides, as
severelossinsemanticinformationwhendecodinganim- demonstratedbyZoeDepth[4], astrongrelativedepthes-
ageintoadiscreteclassspace. Therefore,consideringthe timationmodelcanalsoworkwellingeneralizablemetric
excellentperformanceofDINOv2insemantic-relatedtasks, depthestimationbyfine-tuningwithmetricdepthinforma-
weproposetomaintaintherichsemanticpriorsfromitwith tion. Therefore, we still follow MiDaS in relative depth
asimplefeaturealignmentloss. Thisnotonlyenhancesthe estimation,butfurtherstrengthenitbyhighlightingthevalue
MDEperformance,butalsoyieldsamulti-taskencoderfor oflarge-scalemonocularunlabeledimages.
bothmiddle-levelandhigh-levelperceptiontasks.
Leveragingunlabeleddata. Thisbelongstotheresearch
Ourcontributionsaresummarizedasfollows:
areaofsemi-supervisedlearning[31,55,89],whichispop-
• Wehighlightthevalueofdatascaling-upofmassive, ularwithvariousapplications[70,74]. However,existing
2works typically assume only limited images are available. Dataset Indoor Outdoor Label #Images
Theyrarelyconsiderthechallengingbutrealisticscenario
LabeledDatasets
where there are already sufficient labeled images but also
larger-scaleunlabeledimages. Wetakethischallengingdi- BlendedMVS[76] ✓ ✓ Stereo 115K
rectionforzero-shotMDE.Wedemonstratethatunlabeled DIML[13] ✓ ✓ Stereo 927K
HRWSI[67] ✓ ✓ Stereo 20K
imagescansignificantlyenhancethedatacoverageandthus
IRS[61] ✓ Stereo 103K
improvemodelgeneralizationandrobustness.
MegaDepth[33] ✓ SfM 128K
TartanAir[62] ✓ ✓ Stereo 306K
3.DepthAnything
UnlabeledDatasets
Our work utilizes both labeled and unlabeled images to
BDD100K[81] ✓ None 8.2M
facilitate better monocular depth estimation (MDE). For-
GoogleLandmarks[64] ✓ None 4.1M
mally,thelabeledandunlabeledsetsaredenotedasDl = ImageNet-21K[49] ✓ ✓ None 13.1M
{(x i,d i)}M i=1 and Du = {u i}N i=1 respectively. We aim to LSUN[80] ✓ None 9.8M
learn a teacher model T from Dl. Then, we utilize T to Objects365[52] ✓ ✓ None 1.7M
assignpseudodepthlabelsforDu. Finally,wetrainastu- OpenImagesV7[30] ✓ ✓ None 7.8M
dentmodelS onthecombinationoflabeledsetandpseudo Places365[87] ✓ ✓ None 6.5M
labeledset. AbriefillustrationisprovidedinFigure2. SA-1B[27] ✓ ✓ None 11.1M
3.1.LearningLabeledImages
Table1. Intotal,ourDepthAnythingistrainedon1.5Mlabeled
imagesand62Munlabeledimagesjointly.
This process is similar to the training of MiDaS [5, 45].
However, since MiDaS did not release its code, we first
reproducedit.Concretely,thedepthvalueisfirsttransformed
oureasy-to-acquireanddiverseunlabeledimageswillcom-
into the disparity space by d = 1/t and then normalized
prehendthedatacoverageandgreatlyenhancethemodel
to 0∼1 on each depth map. To enable multi-dataset joint
generalizationabilityandrobustness.
training, we adopt the affine-invariant loss to ignore the
Furthermore,tostrengthentheteachermodelT learned
unknownscaleandshiftofeachsample:
fromtheselabeledimages,weadopttheDINOv2[42]pre-
trained weights to initialize our encoder. In practice, we
HW
L = 1 (cid:88) ρ(d∗,d ), (1) applyapre-trainedsemanticsegmentationmodel[69]tode-
l HW i i tecttheskyregion,andsetitsdisparityvalueas0(farthest).
i=1
whered∗andd arethepredictionandgroundtruth,respec- 3.2.UnleashingthePowerofUnlabeledImages
i i
tively. Andρistheaffine-invariantmeanabsoluteerrorloss:
Thisisthemainpointofourwork. Distinguishedfromprior
ρ(d∗,d ) = |dˆ∗ −dˆ|, wheredˆ∗ anddˆ arethescaledand
i i i i i i works that laboriously construct diverse labeled datasets,
shiftedversionsofthepredictiond∗andgroundtruthd :
i i we highlight the value of unlabeled images in enhancing
the data coverage. Nowadays, we can practically build a
d −t(d)
dˆ = i , (2) diverse and large-scale unlabeled set from the Internet or
i s(d)
publicdatasetsofvarioustasks. Also, wecaneffortlessly
obtainthedensedepthmapofmonocularunlabeledimages
where t(d) and s(d) are used to align the prediction and
simplybyforwardingthemtoapre-trainedwell-performed
groundtruthtohavezerotranslationandunitscale:
MDE model. This ismuch moreconvenient and efficient
HW thanperformingstereomatchingorSfMreconstructionfor
1 (cid:88)
t(d)=median(d), s(d)=
HW
|d i−t(d)|. (3) stereoimagesorvideos. Weselecteightlarge-scalepublic
i=1 datasets as our unlabeled sources for their diverse scenes.
Theycontainmorethan62Mimagesintotal. Thedetailsare
Toobtainarobustmonoculardepthestimationmodel,we
providedinthebottomhalfofTable1.
collect1.5Mlabeledimagesfrom6publicdatasets. Details
Technically,giventhepreviouslyobtainedMDEteacher
ofthesedatasetsarelistedinTable1. Weusefewerlabeled
modelT,wemakepredictionsontheunlabeledsetDu to
datasetsthanMiDaSv3.1[5](12trainingdatasets),because
obtainapseudolabeledsetDˆu:
1) we do not use NYUv2 [54] and KITTI [18] datasets to
ensurezero-shotevaluationonthem,2)somedatasetsare Dˆu ={(u ,T(u ))|u ∈Du}N . (4)
i i i i=1
notavailable(anymore),e.g.,Movies[45]andWSVD[60],
and3)somedatasetsexhibitpoorquality,e.g.,RedWeb(also WiththecombinationsetDl∪Dˆuoflabeledimagesand
lowresolution)[66]. Despiteusingfewerlabeledimages, pseudo labeled images, we train a student model S on it.
3HRWSI: 102684_LookInStereoDotComDSCF0486 feature
SA1B: sa_10000139 alignment
labeled image labeled prediction manual label
LiDAR,
sup
encoder decoder matching,
SfM, etc
S
semantic
preservation
teacher
encoder
sup model
unlabeled image unlabeled prediction pseudo label
Figure2.Ourpipeline.Solidline:flowoflabeledimages,dottedline:unlabeledimages.Weespeciallyhighlightthevalueoflarge-scale
unlabeledimages.TheSdenotesaddingstrongperturbations(Section3.2).Toequipourdepthestimationmodelwithrichsemanticpriors,
weenforceanauxiliaryconstraintbetweentheonlinestudentmodelandafrozenencodertopreservethesemanticcapability(Section3.3).
(cid:80)
Followingpriorworks[73],insteadoffine-tuningS fromT, where we omit the and pixel subscript i for simplicity.
were-initializeS forbetterperformance. Thenweaggregatethetwolossesviaweightedaveraging:
Unfortunately,inourpilotstudies,wefailedtogainim-
(cid:80) (cid:80)
M (1−M)
provementswithsuchaself-trainingpipeline,whichindeed L = LM + L1−M. (8)
u HW u HW u
contradictstheobservationswhenthereareonlyafewla-
beledimages[55]. Weconjecturethat, withalreadysuffi- We use CutMix with 50% probability. The unlabeled
cient labeled images in our case, the extra knowledge ac- imagesforCutMixarealreadystronglydistortedincolor,
quired from additional unlabeled images is rather limited. buttheunlabeledimagesfedintotheteachermodelT for
Especially considering the teacher and student share the pseudolabelingareclean,withoutanydistortions.
samepre-trainingandarchitecture,theytendtomakesimilar
3.3.Semantic-AssistedPerception
correct or false predictions on the unlabeled set Du, even
withouttheexplicitself-trainingprocedure. Thereexistsomeworks[9,21,28,71]improvingdepthes-
Toaddressthedilemma,weproposetochallengethestu- timationwithanauxiliarysemanticsegmentationtask. We
dentwithamoredifficultoptimizationtargetforadditional believethatarmingourdepthestimationmodelwithsuch
visualknowledgeonunlabeledimages. Weinjectstrongper- high-level semantic-related information is beneficial. Be-
turbationstounlabeledimagesduringtraining. Itcompels sides,inourspecificcontextofleveragingunlabeledimages,
ourstudentmodeltoactivelyseekextravisualknowledge theseauxiliarysupervisionsignalsfromothertaskscanalso
andacquireinvariantrepresentationsfromtheseunlabeled combatthepotentialnoiseinourpseudodepthlabel.
images. Theseadvantageshelpourmodeldealwiththeopen Therefore,wemadeaninitialattemptbycarefullyassign-
worldmorerobustly. Weintroducetwoformsofperturba- ingsemanticsegmentationlabelstoourunlabeledimages
tions:oneisstrongcolordistortions,includingcolorjittering withacombinationofRAM[85]+GroundingDINO[37]+
and Gaussian blurring, and the other is strong spatial dis- HQ-SAM[26]models. Afterpost-processing,thisyieldsa
tortion, whichisCutMix[83]. Despitethesimplicity, the classspacecontaining4Kclasses. Inthejoint-trainingstage,
two modifications make our large-scale unlabeled images themodelisenforcedtoproducebothdepthandsegmenta-
significantlyimprovethebaselineoflabeledimages. tionpredictionswithasharedencoderandtwoindividual
WeprovidemoredetailsaboutCutMix. Itwasoriginally decoders. Unfortunately,aftertrialanderror,westillcould
proposedforimageclassification,andisrarelyexploredin notboosttheperformanceoftheoriginalMDEmodel. We
monoculardepthestimation. Wefirstinterpolatearandom speculatedthat,decodinganimageintoadiscreteclassspace
pairofunlabeledimagesu aandu bspatially: indeedlosestoomuchsemanticinformation. Thelimited
informationinthesesemanticmasksishardtofurtherboost
u =u ⊙M +u ⊙(1−M), (5)
ab a b ourdepthmodel,especiallywhenourdepthmodelhases-
tablishedverycompetitiveresults.
whereM isabinarymaskwitharectangleregionsetas1.
Therefore,weaimtoseekmoreinformativesemanticsig-
The unlabeled loss L is obtained by first computing
u nalstoserveasauxiliarysupervisionforourdepthestimation
affine-invariant losses in valid regions defined by M and
task. Wearegreatlyastonishedbythestrongperformance
1−M,respectively:
ofDINOv2models[42]insemantic-relatedtasks,e.g.,im-
LM =ρ(cid:0) S(u )⊙M, T(u )⊙M(cid:1) , (6) ageretrievalandsemanticsegmentation,evenwithfrozen
u ab a
weightswithoutanyfine-tuning. Motivatedbytheseclues,
L1−M =ρ(cid:0) S(u )⊙(1−M),T(u )⊙(1−M)(cid:1) , (7)
u ab b weproposetotransferitsstrongsemanticcapabilitytoour
4KITTI[18] NYUv2[54] Sintel[7] DDAD[20] ETH3D[51] DIODE[59]
Method Encoder
AbsRel δ AbsRel δ AbsRel δ AbsRel δ AbsRel δ AbsRel δ
1 1 1 1 1 1
MiDaSv3.1[5] ViT-L 0.127 0.850 0.048 0.980 0.587 0.699 0.251 0.766 0.139 0.867 0.075 0.942
ViT-S 0.080 0.936 0.053 0.972 0.464 0.739 0.247 0.768 0.127 0.885 0.076 0.939
DepthAnything ViT-B 0.080 0.939 0.046 0.979 0.432 0.756 0.232 0.786 0.126 0.884 0.069 0.946
ViT-L 0.076 0.947 0.043 0.981 0.458 0.760 0.230 0.789 0.127 0.882 0.066 0.952
Table2.Zero-shotrelativedepthestimation.Better:AbsRel↓,δ ↑.WecomparewiththebestmodelfromMiDaSv3.1.NotethatMiDaS
1
doesnotstrictlyfollowthezero-shotevaluationonKITTIandNYUv2,becauseitusestheirtrainingimages.Weprovidethreemodelscales
fordifferentpurposes,basedonViT-S(24.8M),ViT-B(97.5M),andViT-L(335.3M),respectively.Best,secondbestresults.
depthmodelwithanauxiliaryfeaturealignmentloss. The depthregression. Alllabeleddatasetsaresimplycombined
featurespaceishigh-dimensionalandcontinuous,thuscon- togetherwithoutre-sampling. Inthefirststage,wetraina
tainingrichersemanticinformationthandiscretemasks. The teachermodelonlabeledimagesfor20epochs.Inthesecond
featurealignmentlossisformulatedas: stage of joint training, we train a student model to sweep
across all unlabeled images for one time. The unlabeled
HW
1 (cid:88) images are annotated by a best-performed teacher model
L =1− cos(f ,f′), (9)
feat HW i i with a ViT-L encoder. The ratio of labeled and unlabeled
i=1
imagesissetas1:2ineachbatch. Inbothstages,thebase
wherecos(·,·)measuresthecosinesimilaritybetweentwo learningrateofthepre-trainedencoderissetas5e-6,while
featurevectors. f isthefeatureextractedbythedepthmodel therandomlyinitializeddecoderusesa10×largerlearning
S, whilef′ isthefeaturefromafrozenDINOv2encoder. rate. WeusetheAdamWoptimizeranddecaythelearning
We do not follow some works [19] to project the online ratewithalinearschedule.Weonlyapplyhorizontalflipping
featuref intoanewspaceforalignment,becausearandomly asourdataaugmentationforlabeledimages. Thetolerance
initializedprojectormakesthelargealignmentlossdominate marginαforfeaturealignmentlossissetas0.15. Formore
theoveralllossintheearlystage. details,pleaserefertoourappendix.
Anotherkeypointinfeaturealignmentisthat,semantic
4.2.Zero-ShotRelativeDepthEstimation
encoderslikeDINOv2tendtoproducesimilarfeaturesfor
differentpartsofanobject,e.g.,carfrontandrear. Indepth As aforementioned, this work aims to provide accurate
estimation,however,differentpartsorevenpixelswithinthe depth estimation for any image. Therefore, we compre-
samepart,canbeofvaryingdepth. Thus,itisnotbeneficial hensivelyvalidatethezero-shotdepthestimationcapability
toexhaustivelyenforceourdepthmodeltoproduceexactly ofourDepthAnythingmodelonsixrepresentativeunseen
thesamefeaturesasthefrozenencoder. datasets: KITTI[18],NYUv2[54],Sintel[7],DDAD[20],
Tosolvethisissue,wesetatolerancemarginαforthe ETH3D[51],andDIODE[59]. Wecomparewiththebest
featurealignment. Ifthecosinesimilarityoff andf′ has DPT-BEiT modelfromthelatestMiDaSv3.1[5],which
i i L-512
surpassedα,thispixelwillnotbeconsideredinourL . uses more labeled images than us. As shown in Table 2,
feat
Thisallowsourmethodtoenjoyboththesemantic-aware bothwithaViT-Lencoder,ourDepthAnythingsurpasses
representationfromDINOv2andthepart-leveldiscrimina- thestrongestMiDaSmodeltremendouslyacrossextensive
tiverepresentationfromdepthsupervision. Asasideeffect, scenesintermsofboththeAbsRel(absoluterelativeerror:
ourproducedencodernotonlyperformswellindownstream |d∗−d|/d)andδ (percentageofmax(d∗/d,d/d∗)<1.25)
1
MDEdatasets,butalsoachievesstrongresultsintheseman- metrics. Forexample, whentestedonthewell-knownau-
ticsegmentationtask. Italsoindicatesthepotentialofour tonomousdrivingdatasetDDAD[20],weimprovetheAb-
encodertoserveasauniversalmulti-taskencoderforboth sRel(↓)from0.251→0.230andimprovetheδ (↑)from
1
middle-levelandhigh-levelperceptiontasks. 0.766→0.789.
Finally,ouroveralllossisanaveragecombinationofthe Besides,ourViT-Bmodelisalreadyclearlysuperiorto
threelossesL ,L ,andL . the MiDaS based on a much larger ViT-L. Moreover, our
l u feat
ViT-S model, whose scale is less than 1/10 of the MiDaS
4.Experiment model,evenoutperformsMiDaSonseveralunseendatasets,
including Sintel, DDAD, and ETH3D. The performance
4.1.ImplementationDetails
advantage of these small-scale models demonstrates their
WeadopttheDINOv2encoder[42]forfeatureextraction. greatpotentialincomputationally-constrainedscenarios.
FollowingMiDaS[5,45],weusetheDPT[46]decoderfor Itisalsoworthnotingthat,onthemostwidelyusedMDE
5Higherisbetter↑ Lowerisbetter↓ Higherisbetter↑ Lowerisbetter↓
Method Method
δ1 δ2 δ3 AbsRel RMSE log10 δ1 δ2 δ3 AbsRel RMSE RMSElog
AdaBins[3] 0.903 0.984 0.997 0.103 0.364 0.044 AdaBins[3] 0.964 0.995 0.999 0.058 2.360 0.088
DPT[46] 0.904 0.988 0.998 0.110 0.357 0.045 DPT[46] 0.959 0.995 0.999 0.062 2.573 0.092
P3Depth[43] 0.898 0.981 0.996 0.104 0.356 0.043 P3Depth[43] 0.953 0.993 0.998 0.071 2.842 0.103
SwinV2-L[39] 0.949 0.994 0.999 0.083 0.287 0.035 NeWCRFs[82] 0.974 0.997 0.999 0.052 2.129 0.079
AiT[41] 0.954 0.994 0.999 0.076 0.275 0.033 SwinV2-L[39] 0.977 0.998 1.000 0.050 1.966 0.075
VPD[86] 0.964 0.995 0.999 0.069 0.254 0.030 NDDepth[53] 0.978 0.998 0.999 0.050 2.025 0.075
ZoeDepth∗[4] 0.951 0.994 0.999 0.077 0.282 0.033 GEDepth[75] 0.976 0.997 0.999 0.048 2.044 0.076
ZoeDepth∗[4] 0.971 0.996 0.999 0.054 2.281 0.082
Ours 0.984 0.998 1.000 0.056 0.206 0.024
Ours 0.982 0.998 1.000 0.046 1.896 0.069
Table3. Fine-tuningandevaluatingonNYUv2[54]withour
pre-trainedMDEencoder.Wehighlightbest,secondbestresults, Table 4. Fine-tuning and evaluating on KITTI [18] with our
aswellasmostdiscriminativemetrics.∗:Reproducedbyus. pre-trainedMDEencoder.∗:Reproducedbyus.
benchmarksKITTIandNYUv2,althoughMiDaSv3.1uses coderwithmetricdepthinformationfromNYUv2[54](for
thecorrespondingtrainingimages(notzero-shotanymore), indoorscenes)orKITTI[18](foroutdoorscenes). There-
ourDepthAnythingisstillevidentlysuperiortoitwithout fore, we simply replace the MiDaS encoder with our bet-
training with any KITTI or NYUv2 images, e.g., 0.127 vs. terDepthAnythingencoder,leavingothercomponentsun-
0.076inAbsReland0.850vs. 0.947inδ onKITTI. changed.AsshowninTable5,acrossawiderangeofunseen
1
datasetsofindoorandoutdoorscenes,ourDepthAnything
4.3.Fine-tunedtoMetricDepthEstimation
results in a better metric depth estimation model than the
originalZoeDepthbasedonMiDaS.
Apartfromtheimpressiveperformanceinzero-shotrelative
depthestimation,wefurtherexamineourDepthAnything
4.4.Fine-tunedtoSemanticSegmentation
modelasapromisingweightinitializationfordownstream
metricdepthestimation. Weinitializetheencoderofdown- In our method, we design our MDE model to inherit the
streamMDEmodelswithourpre-trainedencoderparameters richsemanticpriorsfromapre-trainedencoderviaasim-
and leave the decoder randomly initialized. The model is ple feature alignment constraint. Here, we examine the
fine-tunedwithcorrepondingmetricdepthinformation. In semanticcapabilityofourMDEencoder. Specifically,we
thispart,weuseourViT-Lencoderforfine-tuning. fine-tuneourMDEencodertodownstreamsemanticsegmen-
Weexaminetworepresentativescenarios: 1)in-domain tation datasets. As exhibited in Table 7 of the Cityscapes
metric depth estimation, where the model is trained and dataset [15], our encoder from large-scale MDE training
evaluatedonthesamedomain(Section4.3.1),and2)zero- (86.2mIoU)issuperiortoexistingencodersfromlarge-scale
shotmetricdepthestimation,wherethemodelistrainedon ImageNet-21K pre-training, e.g., Swin-L [38] (84.3) and
one domain, e.g., NYUv2 [54], but evaluated in different ConvNeXt-XL[40](84.6). Similarobservationsholdonthe
domains,e.g.,SUNRGB-D[56](Section4.3.2). ADE20Kdataset[88]inTable8. Weimprovetheprevious
bestresultfrom58.3→59.4.
Wehopetohighlightthat,witnessingthesuperiorityof
4.3.1 In-DomainMetricDepthEstimation
ourpre-trainedencoderonbothmonoculardepthestimation
AsshowninTable3ofNYUv2[54],ourmodeloutperforms and semantic segmentation tasks, we believe it has great
thepreviousbestmethodVPD[86]remarkably,improving potentialtoserveasagenericmulti-taskencoderforboth
theδ 1 (↑)from0.964→0.984andAbsRel(↓)from0.069 middle-levelandhigh-levelvisualperceptionsystems.
to0.056. SimilarimprovementscanbeobservedinTable4
oftheKITTIdataset[18]. Weimprovetheδ (↑)onKITTI 4.5.AblationStudies
1
from0.978→0.982. Itisworthnotingthatweadoptthe
Unless otherwise specified, we use the ViT-L encoder for
ZoeDepthframeworkforthisscenariowitharelativelyba-
ourablationstudieshere.
sicdepthmodel,andwebelieveourresultscanbefurther
enhancedifequippedwithmoreadvancedarchitectures. Zero-shot transferring of each training dataset. In Ta-
ble6,weprovidethezero-shottransferringperformanceof
eachtrainingdataset,whichmeansthatwetrainarelative
4.3.2 Zero-ShotMetricDepthEstimation
MDEmodelononetrainingsetandevaluateitonthesix
WefollowZoeDepth[4]toconductzero-shotmetricdepth unseendatasets. Withtheseresults,wehopetooffermore
estimation. ZoeDepthfine-tunestheMiDaSpre-traineden- insightsforfutureworksthatsimilarlyaimtobuildageneral
6SUNRGB-D[56] iBims-1[29] HyperSim[48] VirtualKITTI2[8] DIODEOutdoor[59]
Method
AbsRel(↓) δ (↑) AbsRel δ AbsRel δ AbsRel δ AbsRel δ
1 1 1 1 1
ZoeDepth[4] 0.520 0.545 0.169 0.656 0.407 0.302 0.106 0.844 0.814 0.237
DepthAnything 0.500 0.660 0.150 0.714 0.363 0.361 0.085 0.913 0.794 0.288
Table5.Zero-shotmetricdepthestimation.Thefirstthreetestsetsintheheaderareindoorscenes,whilethelasttwoareoutdoorscenes.
FollowingZoeDepth,weusethemodeltrainedonNYUv2forindoorgeneralization,whileusethemodeltrainedonKITTIforoutdoor
evaluation.Forfaircomparisons,wereporttheZoeDepthresultsreproducedinourenvironment.
KITTI[18] NYUv2[54] Sintel[7] DDAD[20] ETH3D[51] DIODE[59] Mean
Trainingset
AbsRel δ AbsRel δ AbsRel δ AbsRel δ AbsRel δ AbsRel δ AbsRel δ
1 1 1 1 1 1 1
BlendedMVS[76] 0.089 0.918 0.068 0.958 0.556 0.689 0.305 0.731 0.148 0.845 0.092 0.921 0.210 0.844
DIML[13] 0.099 0.907 0.055 0.969 0.573 0.722 0.381 0.657 0.142 0.859 0.107 0.908 0.226 0.837
HRWSI[67] 0.095 0.917 0.062 0.966 0.502 0.731 0.270 0.750 0.186 0.775 0.087 0.935 0.200 0.846
IRS[61] 0.105 0.892 0.057 0.970 0.568 0.714 0.328 0.691 0.143 0.845 0.088 0.926 0.215 0.840
MegaDepth[33] 0.217 0.741 0.071 0.953 0.632 0.660 0.479 0.566 0.142 0.852 0.104 0.910 0.274 0.780
TartanAir[62] 0.088 0.920 0.061 0.964 0.602 0.723 0.332 0.690 0.160 0.818 0.088 0.928 0.222 0.841
Alllabeleddata 0.085 0.934 0.053 0.971 0.492 0.748 0.245 0.771 0.134 0.874 0.070 0.945 0.180 0.874
Table6.Examinethezero-shottransferringperformanceofeachlabeledtrainingset(left)tosixunseendatasets(top).Betterperformance:
AbsRel↓,δ ↑.Wehighlightthebest,second,andthirdbestresultsforeachtestdatasetinbold,underline,anditalic,respectively.
1
Method Encoder mIoU(s.s.) m.s. Method Encoder mIoU
Segmenter[57] ViT-L[16] - 82.2 Segmenter[57] ViT-L[16] 51.8
SegFormer[69] MiT-B5[69] 82.4 84.0 SegFormer[69] MiT-B5[69] 51.0
Mask2Former[12] Swin-L[38] 83.3 84.3 Mask2Former[12] Swin-L[38] 56.4
OneFormer[24] Swin-L[38] 83.0 84.4 UperNet[68] BEiT-L[2] 56.3
OneFormer[24] ConvNeXt-XL[40] 83.6 84.6 ViT-Adapter[11] BEiT-L[2] 58.3
DDP[25] ConvNeXt-L[40] 83.2 83.9 OneFormer[24] Swin-L[38] 57.4
OneFormer[24] ConNeXt-XL[40] 57.4
Ours ViT-L[16] 84.8 86.2
Ours ViT-L[16] 59.4
Table7.TransferringourMDEpre-trainedencodertoCityscapes
forsemanticsegmentation. WedonotuseMapillary[1]forpre- Table8.TransferringourMDEencodertoADE20Kforsemantic
training.s.s./m.s.:single-/multi-scaleevaluation. segmentation.WeuseMask2Formerasoursegmentationmodel.
monoculardepthestimationsystem. Amongthesixtraining since the labeled images are already sufficient. However,
datasets, HRWSI [67] fuels our model with the strongest withstrongperturbations(S)appliedtounlabeledimages
generalizationability,eventhoughitonlycontains20Kim- duringre-training,thestudentmodelischallengedtoseek
ages. Thisindicatesthedatadiversitycountsalot, which additional visual knowledge and learn more robust repre-
iswellalignedwithourmotivationtoutilizeunlabeledim- sentations. Consequently,thelarge-scaleunlabeledimages
ages. Somelabeleddatasetsmaynotperformverywell,e.g., enhancethemodelgeneralizationabilitysignificantly.
MegaDepth [33], however, it has its own preferences that Moreover,withourusedsemanticconstraintL ,the
feat
arenotreflectedinthesesixtestdatasets. Forexample,we powerofunlabeledimagescanbefurtheramplifiedforthe
findmodelstrainedwithMegaDepthdataarespecializedat depthestimationtask. Moreimportantly,asemphasizedin
estimatingthedistanceofultra-remotebuildings(Figure1), Section4.4,thisauxiliaryconstraintalsoenablesourtrained
whichwillbeverybeneficialforaerialvehicles. encodertoserveasakeycomponentinamulti-taskvisual
systemforbothmiddle-levelandhigh-levelperception.
Effectivenessof1)challengingthestudentmodelwhen
learningunlabeledimages,and2)semanticconstraint. ComparisonwithMiDaStrainedencoderindownstream
AsshowninTable9,simplyaddingunlabeledimageswith tasks. Our Depth Anything model has exhibited stronger
pseudolabelsdoesnotnecessarilybringgainstoourmodel, zero-shotcapabilitythanMiDaS[5,45]. Here,wefurther
7Ours MiDaS Ours MiDaS
Figure4.WecompareourdepthpredictionwithMiDaS.Meantime,
Figure3.Qualitativeresultsonsixunseendatasets.
weuseControlNettosynthesizenewimagesfromthedepthmap
(thelastrow).Firstrow:inputimage,secondrow:depthprediction.
L L S L KI NY SI DD ET DI
l u feat
✓ 0.085 0.053 0.492 0.245 0.134 0.070 NYUv2 KITTI ADE20K
✓ ✓ 0.085 0.054 0.481 0.242 0.138 0.073 Encoder
✓ ✓ ✓ 0.081 0.048 0.469 0.235 0.134 0.068 AbsRel(↓) δ 1(↑) AbsRel δ 1 mIoU(↑)
✓ ✓ ✓ ✓ 0.076 0.043 0.458 0.230 0.127 0.066 DINOv2 0.066 0.973 0.058 0.971 58.8
Ours 0.056 0.984 0.046 0.982 59.4
Table9.Ablationstudiesof:1)challengingthestudentwithstrong
perturbations(S)whenlearningunlabeledimages,and2)semantic Table11.ComparisonbetweentheoriginalDINOv2andourpro-
constraint(L feat). Limitedbyspace,weonlyreporttheAbsRel ducedencoderintermsofdownstreamfine-tuningperformance.
(↓)metric,andshortenthedatasetnamewithitsfirsttwoletters.
impressivelyindownstreamtransferringperformance.
NYUv2 KITTI Cityscapes ADE20K
Method
AbsRel δ AbsRel δ mIoU mIoU 4.6.QualitativeResults
1 1
MiDaS 0.077 0.951 0.054 0.971 82.1 52.4 Wevisualizeourmodelpredictionsonthesixunseendatasets
Ours 0.056 0.984 0.046 0.982 84.8 59.4 inFigure3. Ourmodelisrobusttotestimagesfromvarious
domains. Inaddition,wecompareourmodelwithMiDaS
Table10.ComparisonbetweenourtrainedencoderandMiDaS[5] inFigure4. Wealsoattempttosynthesisnewimagescon-
trainedencoderintermsofdownstreamfine-tuningperformance.
ditionedonthepredicteddepthmapswithControlNet[84].
Betterperformance:AbsRel↓,δ ↑,mIoU↑.
1 Our model produces more accurate depth estimation than
MiDaS,aswellasbettersynthesisresults,althoughtheCon-
trolNet is trained with MiDaS depth. For more accurate
compareourtrainedencoderwithMiDaSv3.1[5]trained
synthesis,wehavealsore-trainedabetterdepth-conditioned
encoderintermsofthedownstreamfine-tuningperformance.
ControlNet based on our Depth Anything, aiming to pro-
AsdemonstratedinTable10,onboththedownstreamdepth
vide better control signals for image synthesis and video
estimation task and semantic segmentation task, our pro-
editing. Please refer to our project page or the following
ducedencoderoutperformstheMiDaSencoderremarkably,
supplementarymaterialformorequalitativeresults,
e.g.,0.951vs. 0.984intheδ metriconNYUv2,and52.4
1
vs. 59.4inthemIoUmetriconADE20K.
5.Conclusion
ComparisonwithDINOv2indownstreamtasks. Wehave
demonstratedthesuperiorityofourtrainedencoderwhen Inthiswork,wepresentDepthAnything,ahighlypractical
fine-tunedtodownstreamtasks. Sinceourfinallyproduced solution to robust monocular depth estimation. Different
encoder(fromlarge-scaleMDEtraining)isfine-tunedfrom frompriorarts,weespeciallyhighlightthevalueofcheap
DINOv2 [42], we compare our encoder with the original and diverse unlabeled images. We design two simple yet
DINOv2encoderinTable11. Itcanbeobservedthatour highly effective strategies to fully exploit their value: 1)
encoderperformsbetterthantheoriginalDINOv2encoder posingamorechallengingoptimizationtargetwhenlearning
in both the downstream metric depth estimation task and unlabeled images, and 2) preserving rich semantic priors
semanticsegmentationtask. AlthoughtheDINOv2weight frompre-trainedmodels. Asaresult,ourDepthAnything
hasprovidedaverystronginitialization(alsomuchbetter modelexhibitsexcellentzero-shotdepthestimationability,
thantheMiDaSencoderasreportedinTable10),ourlarge- andalsoservesasapromisinginitializationfordownstream
scaleandhigh-qualityMDEtrainingcanfurtherenhanceit metricdepthestimationandsemanticsegmentationtasks.
8Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data
Supplementary Material
6.MoreImplementationDetails
α KITTI NYU Sintel DDAD ETH3D DIODE Mean
Weresizetheshortersideofallimagesto518andkeepthe 0.00 0.085 0.055 0.523 0.250 0.134 0.079 0.188
originalaspectratio. Allimagesarecroppedto518×518 0.15 0.080 0.053 0.464 0.247 0.127 0.076 0.175
0.30 0.079 0.054 0.482 0.248 0.127 0.077 0.178
duringtraining. Duringinference,wedonotcropimages
andonlyensurebothsidesaremultipliersof14,sincethe
Table 12. Ablation studies on different values of the tolerance
pre-definedpatchsizeofDINOv2encoders[42]is14. Eval-
marginαforthefeaturealignmentlossL .Limitedbyspace,
uationisperformedattheoriginalresolutionbyinterpolating feat
weonlyreporttheAbsRel(↓)metrichere.
theprediction. FollowingMiDaS[5,45],inzero-shoteval-
uation, the scale and shift of our prediction are manually
L Unseendatasets(AbsRel↓)
alignedwiththegroundtruth. feat Mean
Whenfine-tuningourpre-trainedencodertometricdepth U L KITTI NYU Sintel DDAD ETH3D DIODE
estimation,weadopttheZoeDepthcodebase[4]. Wemerely
0.083 0.055 0.478 0.249 0.133 0.080 0.180
replacetheoriginalMiDaS-basedencoderwithourstronger ✓ 0.080 0.053 0.464 0.247 0.127 0.076 0.175
DepthAnythingencoder,withafewhyper-parametersmod- ✓ 0.084 0.054 0.472 0.252 0.133 0.081 0.179
ified. Concretely, the training resolution is 392×518 on
NYUv2 [54] and 384×768 on KITTI [18] to match the Table13.Ablationstudiesofapplyingourfeaturealignmentloss
patch size of our encoder. The encoder learning rate is L tounlabeleddata(U)orlabeleddata(L).
feat
setas1/50ofthelearningrateoftherandomlyinitialized
decoder,whichismuchsmallerthanthe1/10adoptedfor
becausethelabeleddatahasrelativelyhigher-qualitydepth
MiDaSencoder,duetoourstronginitialization. Thebatch
annotations. Theinvolvementofsemanticlossmayinterfere
sizeis16andthemodelistrainedfor5epochs.
withthelearningoftheseinformativemanuallabels. Incom-
Whenfine-tuningourpre-trainedencodertosemanticseg-
parison,ourpseudolabelsarenoisierandlessinformative.
mentation,weusetheMMSegmentationcodebase[14]. The
Therefore,introducingtheauxiliaryconstrainttounlabeled
trainingresolutionissetas896×896onbothADE20K[88]
datacancombatthenoiseinpseudodepthlabels,aswellas
and Cityscapes [15]. The encoder learning rate is set as
armourmodelwithsemanticcapability.
3e-6 and the decoder learning rate is 10× larger. We use
Mask2Former[12]asoursemanticsegmentationmodel.The
8.LimitationsandFutureWorks
modelistrainedfor160KiterationsonADE20Kand80K
iterations on Cityscapes both with batch size 16, without Currently,thelargestmodelsizeisonlyconstrainedtoViT-
anyCOCO[35]orMapillary[1]pre-training. Othertraining Large[16]. Therefore,inthefuture,weplantofurtherscale
configurationsarethesameastheoriginalcodebase. up the model size from ViT-Large to ViT-Giant, which is
alsowellpre-trainedbyDINOv2[42]. Wecantrainamore
7.MoreAblationStudies powerful teacher model with the larger model, producing
moreaccuratepseudolabelsforsmallermodelstolearn,e.g.,
AllablationstudieshereareconductedontheViT-Smodel. ViT-LandViT-B.Furthermore,tofacilitatereal-worldap-
Thenecessityoftolerancemarginforfeaturealignment. plications,webelievethewidelyadopted512×512training
AsshowninTable12,thegapbetweenthetolerancemargin resolutionisnotenough. Weplantore-trainourmodelona
of0and0.15or0.30clearlydemonstratesthenecessityof largerresolutionof700+oreven1000+.
thisdesign(meanAbsRel: 0.188vs. 0.175).
9.MoreQualitativeResults
Applyingfeaturealignmenttolabeleddata. Previously,
we enforce the feature alignment loss L feat on unlabeled Pleaserefertothefollowingpagesforcomprehensivequali-
data. Indeed, it is technically feasible to also apply this tativeresultsonsixunseentestsets(Figure5forKITTI[18],
constrainttolabeleddata. InTable13,apartfromapplying Figure6forNYUv2[54],Figure7forSintel[7],Figure8
L feat onunlabeleddata,weexploretoapplyittolabeled forDDAD[20], Figure9forETH3D[51], andFigure10
data. Wefindthataddingthisauxiliaryoptimizationtarget forDIODE[59]). Wecompareourmodelwiththestrongest
tolabeleddataisnotbeneficialtoourbaselinethatdoesnot MiDaSmodel[5],i.e.,DPT-BEiT . Ourmodelexhibits
L-512
involveanyfeaturealignment(theirmeanAbsRelvaluesare higherdepthestimationaccuracyandstrongerrobustness.
almostthesame: 0.180vs. 0.179). Weconjecturethatthisis
9Input image Our prediction MiDaS v3.1 prediction
Figure5.QualitativeresultsonKITTI.Duetotheextremelysparsegroundtruthwhichishardtovisualize,weherecompareourprediction
withthemostadvancedMiDaSv3.1[5]prediction.Thebrightercolordenotesthecloserdistance.
10Input image Our prediction MiDaS v3.1 prediction
Figure6.QualitativeresultsonNYUv2.ItisworthnotingthatMiDaS[5]usesNYUv2trainingdata(notzero-shot),whilewedonot.
11Input image Our prediction MiDaS v3.1 prediction
Figure7.QualitativeresultsonSintel.
12Input image Our prediction MiDaS v3.1 prediction Input image Our prediction MiDaS v3.1 prediction
Figure8.QualitativeresultsonDDAD.
13Input image Our prediction MiDaS v3.1 prediction
Figure9.QualitativeresultsonETH3D.
14Input image Our prediction MiDaS v3.1 prediction
Figure10.QualitativeresultsonDIODE.
15References [17] David Eigen, Christian Puhrsch, and Rob Fergus. Depth
mappredictionfromasingleimageusingamulti-scaledeep
[1] ManuelLo´pezAntequera,PauGargallo,MarkusHofinger,
network. InNeurIPS,2014. 2
Samuel Rota Bulo`, Yubin Kuang, and Peter Kontschieder.
[18] AndreasGeiger,PhilipLenz,ChristophStiller,andRaquel
Mapillaryplanet-scaledepthdataset. InECCV,2020. 7,9
Urtasun. Visionmeetsrobotics:Thekittidataset. IJRR,2013.
[2] HangboBao,LiDong,SonghaoPiao,andFuruWei. Beit: 1,2,3,5,6,7,9
Bertpre-trainingofimagetransformers. InICLR,2022. 7
[19] Jean-BastienGrill,FlorianStrub,FlorentAltche´,Corentin
[3] ShariqFarooqBhat,IbraheemAlhashim,andPeterWonka. Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doer-
Adabins: Depthestimationusingadaptivebins. InCVPR, sch,BernardoAvilaPires,ZhaohanGuo,MohammadGhesh-
2021. 2,6 laghiAzar,etal. Bootstrapyourownlatent-anewapproach
[4] ShariqFarooqBhat,ReinerBirkl,DianaWofk,PeterWonka, toself-supervisedlearning. InNeurIPS,2020. 5
andMatthiasMu¨ller. Zoedepth:Zero-shottransferbycom- [20] VitorGuizilini,RaresAmbrus,SudeepPillai,AllanRaventos,
biningrelativeandmetricdepth. arXiv:2302.12288,2023. 2, andAdrienGaidon.3dpackingforself-supervisedmonocular
6,7,9 depthestimation. InCVPR,2020. 5,7,9
[5] ReinerBirkl,DianaWofk,andMatthiasMu¨ller. Midasv3. [21] VitorGuizilini,RuiHou,JieLi,RaresAmbrus,andAdrien
1–amodelzooforrobustmonocularrelativedepthestimation. Gaidon. Semantically-guidedrepresentationlearningforself-
arXiv:2307.14460,2023. 2,3,5,7,8,9,10,11 supervisedmonoculardepth. InICLR,2020. 2,4
[6] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ [22] VitorGuizilini,IgorVasiljevic,DianChen,RaresAmbrus,
, ,
Altman, Simran Arora, Sydney von Arx, Michael S Bern- andAdrienGaidon. Towardszero-shotscale-awaremonocu-
stein, JeannetteBohg, AntoineBosselut, EmmaBrunskill, lardepthestimation. InICCV,2023. 2
etal. Ontheopportunitiesandrisksoffoundationmodels. [23] DerekHoiem,AlexeiAEfros,andMartialHebert. Recover-
arXiv:2108.07258,2021. 1 ingsurfacelayoutfromanimage. IJCV,2007. 2
[7] DanielJButler,JonasWulff,GarrettBStanley,andMichaelJ [24] JiteshJain,JiachenLi,MangTikChiu,AliHassani,Nikita
Black. A naturalistic open source movie for optical flow Orlov,andHumphreyShi. Oneformer: Onetransformerto
evaluation. InECCV,2012. 5,7,9 ruleuniversalimagesegmentation. InCVPR,2023. 7
[8] YohannCabon,NailaMurray,andMartinHumenberger. Vir- [25] YuanfengJi,ZheChen,EnzeXie,LanqingHong,XihuiLiu,
tualkitti2. arXiv:2001.10773,2020. 7 ZhaoqiangLiu,TongLu,ZhenguoLi,andPingLuo. Ddp:
Diffusionmodelfordensevisualprediction. InICCV,2023.
[9] Po-Yi Chen, Alexander H Liu, Yen-Cheng Liu, and Yu-
7
Chiang Frank Wang. Towards scene understanding: Un-
[26] LeiKe,MingqiaoYe,MartinDanelljan,YifanLiu,Yu-Wing
supervisedmonoculardepthestimationwithsemantic-aware
Tai,Chi-KeungTang,andFisherYu. Segmentanythingin
representation. InCVPR,2019. 2,4
highquality. InNeurIPS,2023. 4
[10] WeifengChen,ZhaoFu,DaweiYang,andJiaDeng. Single-
[27] AlexanderKirillov,EricMintun,NikhilaRavi,HanziMao,
imagedepthperceptioninthewild. InNeurIPS,2016. 2
ChloeRolland,LauraGustafson,TeteXiao,SpencerWhite-
[11] ZheChen, YuchenDuan, WenhaiWang, JunjunHe, Tong
head,AlexanderCBerg,Wan-YenLo,etal. Segmentany-
Lu,JifengDai,andYuQiao. Visiontransformeradapterfor
thing. InICCV,2023. 1,2,3
densepredictions. InICLR,2023. 7
[28] MarvinKlingner,Jan-AikeTermo¨hlen,JonasMikolajczyk,
[12] BowenCheng,IshanMisra,AlexanderGSchwing,Alexander
andTimFingscheidt. Self-supervisedmonoculardepthes-
Kirillov,andRohitGirdhar. Masked-attentionmasktrans-
timation: Solvingthedynamicobjectproblembysemantic
formerforuniversalimagesegmentation. InCVPR,2022. 7,
guidance. InECCV,2020. 4
9
[29] TobiasKoch,LukasLiebel,FriedrichFraundorfer,andMarco
[13] JaehoonCho,DongboMin,YoungjungKim,andKwanghoon Korner. Evaluationofcnn-basedsingle-imagedepthestima-
Sohn. Diml/cvlrgb-ddataset: 2mrgb-dimagesofnatural tionmethods. InECCVW,2018. 7
indoorandoutdoorscenes. arXiv:2110.11590,2021. 3,7
[30] AlinaKuznetsova,HassanRom,NeilAlldrin,JasperUijlings,
[14] MMSegmentation Contributors. MMSegmenta- IvanKrasin,JordiPont-Tuset,ShahabKamali,StefanPopov,
tion: Openmmlab semantic segmentation toolbox MatteoMalloci,AlexanderKolesnikov,etal. Theopenim-
and benchmark. https://github.com/open- agesdatasetv4:Unifiedimageclassification,objectdetection,
mmlab/mmsegmentation,2020. 9 andvisualrelationshipdetectionatscale. IJCV,2020. 2,3
[15] MariusCordts,MohamedOmran,SebastianRamos,Timo [31] Dong-HyunLeeetal. Pseudo-label:Thesimpleandefficient
Rehfeld,MarkusEnzweiler,RodrigoBenenson,UweFranke, semi-supervisedlearningmethodfordeepneuralnetworks.
StefanRoth,andBerntSchiele. Thecityscapesdatasetfor InICMLW,2013. 2
semanticurbansceneunderstanding. InCVPR,2016. 1,6,9 [32] Bo Li, Chunhua Shen, Yuchao Dai, Anton Van Den Hen-
[16] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, gel,andMingyiHe. Depthandsurfacenormalestimation
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, frommonocularimagesusingregressionondeepfeaturesand
MostafaDehghani,MatthiasMinderer,GeorgHeigold,Syl- hierarchicalcrfs. InCVPR,2015. 2
vainGelly, etal. Animageisworth16x16words: Trans- [33] ZhengqiLiandNoahSnavely. Megadepth:Learningsingle-
formersforimagerecognitionatscale. InICLR,2021. 7, viewdepthpredictionfrominternetphotos. InCVPR,2018.
9 1,3,7
16[34] ZhenyuLi,XuyangWang,XianmingLiu,andJunjunJiang. [49] OlgaRussakovsky,JiaDeng,HaoSu,JonathanKrause,San-
Binsformer: Revisitingadaptivebinsformonoculardepth jeevSatheesh,SeanMa,ZhihengHuang,AndrejKarpathy,
estimation. arXiv:2204.00987,2022. 2 AdityaKhosla,MichaelBernstein,etal. Imagenetlargescale
[35] Tsung-YiLin,MichaelMaire,SergeBelongie,JamesHays, visualrecognitionchallenge. IJCV,2015. 3
PietroPerona,DevaRamanan,PiotrDolla´r,andCLawrence [50] Ashutosh Saxena, Min Sun, and Andrew Y Ng. Make3d:
Zitnick. Microsoft coco: Common objects in context. In Learning3dscenestructurefromasinglestillimage. TPAMI,
ECCV,2014. 1,9 2008. 2
[36] Ce Liu, Jenny Yuen, Antonio Torralba, Josef Sivic, and [51] ThomasSchops,JohannesLSchonberger,SilvanoGalliani,
WilliamTFreeman. Siftflow:Densecorrespondenceacross TorstenSattler,KonradSchindler,MarcPollefeys,andAn-
differentscenes. InECCV,2008. 2 dreas Geiger. A multi-view stereo benchmark with high-
[37] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao resolutionimagesandmulti-cameravideos. InCVPR,2017.
Zhang,JieYang,ChunyuanLi,JianweiYang,HangSu,Jun 5,7,9
Zhu,etal. Groundingdino: Marryingdinowithgrounded [52] ShuaiShao,ZemingLi,TianyuanZhang,ChaoPeng,Gang
pre-trainingforopen-setobjectdetection. arXiv:2303.05499, Yu,XiangyuZhang,JingLi,andJianSun. Objects365: A
2023. 4 large-scale,high-qualitydatasetforobjectdetection.InICCV,
[38] ZeLiu,YutongLin,YueCao,HanHu,YixuanWei,Zheng 2019. 3
Zhang, StephenLin, andBainingGuo. Swintransformer: [53] ShuweiShao,ZhongcaiPei,WeihaiChen,XingmingWu,and
Hierarchicalvisiontransformerusingshiftedwindows. In ZhengguoLi. Nddepth:Normal-distanceassistedmonocular
ICCV,2021. 6,7 depthestimation. InICCV,2023. 2,6
[39] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, [54] NathanSilberman,DerekHoiem,PushmeetKohli,andRob
YixuanWei,JiaNing,YueCao,ZhengZhang,LiDong,etal. Fergus.Indoorsegmentationandsupportinferencefromrgbd
Swintransformerv2:Scalingupcapacityandresolution. In images. InECCV,2012. 1,2,3,5,6,7,9
CVPR,2022. 6 [55] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao
[40] ZhuangLiu,HanziMao,Chao-YuanWu,ChristophFeicht- Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk,
enhofer,TrevorDarrell,andSainingXie. Aconvnetforthe AlexeyKurakin,andChun-LiangLi. Fixmatch:Simplifying
2020s. InCVPR,2022. 6,7 semi-supervisedlearningwithconsistencyandconfidence.In
[41] Jia Ning, Chen Li, Zheng Zhang, Chunyu Wang, Zigang NeurIPS,2020. 2,4
Geng,QiDai,KunHe,andHanHu. Allintokens:Unifying [56] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.
outputspaceofvisualtasksviasofttoken. InICCV,2023. 6 Sunrgb-d:Argb-dsceneunderstandingbenchmarksuite. In
[42] MaximeOquab,Timothe´eDarcet,The´oMoutakanni,HuyVo, CVPR,2015. 6,7
MarcSzafraniec,VasilKhalidov,PierreFernandez,Daniel [57] Robin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia
Haziza,FranciscoMassa,AlaaeldinEl-Nouby,etal. Dinov2: Schmid. Segmenter:Transformerforsemanticsegmentation.
Learningrobustvisualfeatureswithoutsupervision. TMLR, InICCV,2021. 7
2023. 3,4,5,8,9 [58] HugoTouvron,ThibautLavril,GautierIzacard,XavierMar-
[43] VaishakhPatil,ChristosSakaridis,AlexanderLiniger,and tinet, Marie-Anne Lachaux, Timothe´e Lacroix, Baptiste
LucVanGool. P3depth:Monoculardepthestimationwitha Rozie`re, Naman Goyal, Eric Hambro, Faisal Azhar, et al.
piecewiseplanarityprior. InCVPR,2022. 6 Llama: Open and efficient foundation language models.
[44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya arXiv:2302.13971,2023. 1
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, [59] Igor Vasiljevic, Nick Kolkin, Shanyi Zhang, Ruotian Luo,
AmandaAskell,PamelaMishkin,JackClark,etal. Learning Haochen Wang, Falcon Z Dai, Andrea F Daniele, Mo-
transferablevisualmodelsfromnaturallanguagesupervision. hammadrezaMostajabi,StevenBasart,MatthewRWalter,
InICML,2021. 1 et al. Diode: A dense indoor and outdoor depth dataset.
[45] Rene´ Ranftl, Katrin Lasinger, David Hafner, Konrad arXiv:1908.00463,2019. 5,7,9
Schindler,andVladlenKoltun. Towardsrobustmonocular [60] ChaoyangWang,SimonLucey,FedericoPerazzi,andOliver
depthestimation:Mixingdatasetsforzero-shotcross-dataset Wang. Web stereo video supervision for depth prediction
transfer. TPAMI,2020. 1,2,3,5,7,9 fromdynamicscenes. In3DV,2019. 3
[46] Rene´Ranftl,AlexeyBochkovskiy,andVladlenKoltun. Vi- [61] QiangWang,ShizhenZheng,QingsongYan,FeiDeng,Kaiy-
siontransformersfordenseprediction. InICCV,2021. 5, ongZhao,andXiaowenChu. Irs:Alargenaturalisticindoor
6 roboticsstereodatasettotraindeepmodelsfordisparityand
[47] AlexRaslaandMichaelBeyeler. Therelativeimportance surfacenormalestimation. InICME,2021. 3,7
ofdepthcuesandsemanticedgesforindoormobilityusing [62] WenshanWang, DelongZhu, XiangweiWang, YaoyuHu,
simulatedprostheticvisioninimmersivevirtualreality. In Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and
VRST,2022. 1 SebastianScherer. Tartanair:Adatasettopushthelimitsof
[48] Mike Roberts, Jason Ramapuram, Anurag Ranjan, Atulit visualslam. InIROS,2020. 3,7
Kumar,MiguelAngelBautista,NathanPaczan,RussWebb, [63] YanWang,Wei-LunChao,DivyanshGarg,BharathHariha-
andJoshuaMSusskind.Hypersim:Aphotorealisticsynthetic ran,MarkCampbell,andKilianQWeinberger. Pseudo-lidar
dataset for holistic indoor scene understanding. In ICCV, fromvisualdepthestimation:Bridgingthegapin3dobject
2021. 7 detectionforautonomousdriving. InCVPR,2019. 1
17[64] TobiasWeyand,AndreAraujo,BingyiCao,andJackSim. [81] FisherYu,HaofengChen,XinWang,WenqiXian,Yingying
Google landmarks dataset v2-a large-scale benchmark for Chen,FangchenLiu,VashishtMadhavan,andTrevorDar-
instance-levelrecognitionandretrieval. InCVPR,2020. 3 rell. Bdd100k:Adiversedrivingdatasetforheterogeneous
[65] DianaWofk,FangchangMa,Tien-JuYang,SertacKaraman, multitasklearning. InCVPR,2020. 2,3
andVivienneSze. Fastdepth:Fastmonoculardepthestima- [82] WeihaoYuan,XiaodongGu,ZuozhuoDai,SiyuZhu,and
tiononembeddedsystems. InICRA,2019. 1 PingTan. Newcrfs:Neuralwindowfully-connectedcrfsfor
[66] KeXian,ChunhuaShen,ZhiguoCao,HaoLu,YangXiao, monoculardepthestimation. arXiv:2203.01502,2022. 2,6
RuiboLi,andZhenboLuo. Monocularrelativedepthper- [83] SangdooYun, DongyoonHan, SeongJoonOh, Sanghyuk
ceptionwithwebstereodatasupervision. InCVPR,2018. 2, Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-
3 larizationstrategytotrainstrongclassifierswithlocalizable
[67] KeXian,JianmingZhang,OliverWang,LongMai,ZheLin, features. InICCV,2019. 4
andZhiguoCao. Structure-guidedrankinglossforsingle [84] LvminZhang,AnyiRao,andManeeshAgrawala. Adding
imagedepthprediction. InCVPR,2020. 2,3,7 conditional control to text-to-image diffusion models. In
[68] TeteXiao,YingchengLiu,BoleiZhou,YuningJiang,and ICCV,2023. 8
JianSun. Unifiedperceptualparsingforsceneunderstanding. [85] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li,
InECCV,2018. 7 ZhaochuanLuo,YanchunXie,YuzhuoQin,TongLuo,Yaqian
[69] EnzeXie,WenhaiWang,ZhidingYu,AnimaAnandkumar, Li,ShilongLiu,etal. Recognizeanything:Astrongimage
Jose M Alvarez, and Ping Luo. Segformer: Simple and taggingmodel. arXiv:2306.03514,2023. 4
efficientdesignforsemanticsegmentationwithtransformers.
[86] WenliangZhao,YongmingRao,ZuyanLiu,BenlinLiu,Jie
InNeurIPS,2021. 3,7
Zhou, and Jiwen Lu. Unleashing text-to-image diffusion
[70] MengdeXu,ZhengZhang,HanHu,JianfengWang,Lijuan
modelsforvisualperception. InICCV,2023. 6
Wang,FangyunWei,XiangBai,andZichengLiu.End-to-end
[87] BoleiZhou,AgataLapedriza,AdityaKhosla,AudeOliva,
semi-supervisedobjectdetectionwithsoftteacher. InICCV,
andAntonioTorralba. Places:A10millionimagedatabase
2021. 2
forscenerecognition. TPAMI,2017. 3
[71] XiaogangXu,HengshuangZhao,VibhavVineet,Ser-Nam
[88] BoleiZhou,HangZhao,XavierPuig,SanjaFidler,AdelaBar-
Lim,andAntonioTorralba. Mtformer: Multi-tasklearning
riuso,andAntonioTorralba. Sceneparsingthroughade20k
viatransformerandcross-taskreasoning. InECCV,2022. 4
dataset. InCVPR,2017. 6,9
[72] IZekiYalniz,Herve´Je´gou,KanChen,ManoharPaluri,and
[89] BarretZoph,GolnazGhiasi,Tsung-YiLin,YinCui,Hanx-
DhruvMahajan. Billion-scalesemi-supervisedlearningfor
iaoLiu,EkinDogusCubuk,andQuocLe. Rethinkingpre-
imageclassification. arXiv:1905.00546,2019. 2
trainingandself-training. InNeurIPS,2020. 2
[73] LiheYang,WeiZhuo,LeiQi,YinghuanShi,andYangGao.
St++: Make self-training work better for semi-supervised
semanticsegmentation. InCVPR,2022. 4
[74] Lihe Yang, Lei Qi, Litong Feng, Wayne Zhang, and
YinghuanShi. Revisitingweak-to-strongconsistencyinsemi-
supervisedsemanticsegmentation. InCVPR,2023. 2
[75] XiaodongYang,ZhuangMa,ZhiyuJi,andZheRen.Gedepth:
Groundembeddingformonoculardepthestimation. InICCV,
2023. 2,6
[76] YaoYao,ZixinLuo,ShiweiLi,JingyangZhang,YufanRen,
LeiZhou,TianFang,andLongQuan. Blendedmvs:Alarge-
scaledatasetforgeneralizedmulti-viewstereonetworks. In
CVPR,2020. 3,7
[77] WeiYin,YifanLiu,ChunhuaShen,andYouliangYan. En-
forcinggeometricconstraintsofvirtualnormalfordepthpre-
diction. InICCV,2019. 2
[78] WeiYin,ChiZhang,HaoChen,ZhipengCai,GangYu,Kaix-
uan Wang, Xiaozhi Chen, and Chunhua Shen. Metric3d:
Towardszero-shotmetric3dpredictionfromasingleimage.
InICCV,2023. 2
[79] YurongYou,YanWang,Wei-LunChao,DivyanshGarg,Ge-
offPleiss,BharathHariharan,MarkCampbell,andKilianQ
Weinberger. Pseudo-lidar++: Accuratedepthfor3dobject
detectioninautonomousdriving. InICLR,2020. 1
[80] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas
Funkhouser,andJianxiongXiao. Lsun: Constructionofa
large-scaleimagedatasetusingdeeplearningwithhumansin
theloop. arXiv:1506.03365,2015. 3
18