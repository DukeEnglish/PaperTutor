SCENES: Subpixel Correspondence Estimation With Epipolar Supervision
DominikA.Kloepfer Joa˜oF.Henriques DylanCampbell
VisualGeometryGroup VisualGeometryGroup SchoolofComputing
UniversityofOxford UniversityofOxford AustralianNationalUniversity
dominik@robots.ox.ac.uk joao@robots.ox.ac.uk dylan.campbell@anu.edu.au
Abstract
Extracting point correspondences from two or more
views of a scene is a fundamental computer vision prob-
lem with particular importance for relative camera pose
estimation and structure-from-motion. Existing local fea-
turematchingapproaches,trainedwithcorrespondencesu-
pervision on large-scale datasets, obtain highly-accurate (a)Sourcepixel (b)Putativematch,epipolarline
matches on the test sets. However, they do not generalise
well to new datasets with different characteristics to those
they were trained on, unlike classic feature extractors. In-
stead, they require finetuning, which assumes that ground-
truth correspondences or ground-truth camera poses and
3Dstructureareavailable. Werelaxthisassumptionbyre-
moving the requirement of 3D structure, e.g., depth maps
(c)MatchesbeforeSCENESfinetuning
orpointclouds,andonlyrequirecameraposeinformation,
which can be obtained from odometry. We do so by re-
placing correspondence losses with epipolar losses, which
encourageputativematchestolieontheassociatedepipo-
lar line. While weaker than correspondence supervision,
weobservethatthiscueissufficientforfinetuningexisting
modelsonnewdata. Wethenfurtherrelaxtheassumption
of known camera posesby using pose estimates in a novel (d)MatchesafterSCENESfinetuning
bootstrappingapproach.Weevaluateonhighlychallenging
datasets,includinganindoordronedatasetandanoutdoor Figure 1. SCENES (Subpixel Correspondence EstimatioN with
smartphonecameradataset,andobtainstate-of-the-artre- Epipolar Supervision) learns to find high-quality local image
matches without requiring correspondence supervision. Instead,
sultswithoutstrongsupervision.
pose supervision alone is used, encouraging matches to lie on
epipolarlines.Theredpixelin(a)correspondstotheredepipolar
linein(b). Thenetworkinitiallymatchestheredpixeltotheblue
1.Introduction
pixel,butourepipolarlossespreferencematchesontheepipolar
line(notnecessarilytheclosestpoint).Thecorrespondencesfound
The task of finding corresponding points in two or more
bythestate-of-the-artMatchFormeralgorithm[45]aredisplayed
views of a scene—that is, the subpixel positions that cor-
before(c)andafter(d)finetuningwithepipolarsupervision. The
respond to the same visible 3D surface location—is a fun-
colourdenoteswhetherthesquaredsymmetricalepipolardistance
damentallow-levelimageprocessingproblem. Itunderpins isbelow(green)orabove(red)athresholdof5·10−4.Imagesare
manycomputervisiontasks,includingrelativeposeorfun-
fromthechallengingEuRoC-MAVdronedataset[6].
damentalmatrixestimation[18,24,49],visuallocalisation
[31,39],Structure-from-Motion(SfM)[33,36],andSimul-
taneousLocalisationandMapping(SLAM)[14,27],andis Classically approached in two stages with keypoint de-
related to optical flow estimation [2, 20, 21, 40] when the tection followed by local feature extraction and matching
cameramotionissmall. [12, 26, 30], recent approaches [9, 38, 45] eschew the de-
4202
naJ
91
]VC.sc[
1v68801.1042:viXratection step to ensure that dense matches can be extracted 3Dhumanposeestimation,whileBhalgatetal.[4]guidea
evenintexture-poorregions. However, thesupervisionre- transformer model to prioritise attending to features along
quirements of most approaches are onerous: ground-truth the epipolar line, leading to enhanced 3D object pose esti-
correspondences are assumed to be available. While these mation. Yifanetal.[48]introducegeometricalembeddings
canbeobtainedfromcameraposesand3Dstructure(depth based on epipolar geometry in place of positional embed-
maps, point clouds or meshes), these are often not readily dings for 3D reconstruction tasks using transformer mod-
accessible. This makes it challenging to train or finetune els. Furthermore,Prasadetal.[28],Lietal.[23],andYang
thesemodelsonnewdatasetsorinnewregimes. Forexam- etal.[47]allemployepipolargeometryfordepthprediction
ple, a robot preloaded with the latest local feature match- tasks.
ing network would not be able to adapt the model when Wealsomakeuseofepipolargeometryinourproposed
deployed in a new environment, unless it were able to re- pixelcorrespondenceestimator.However,wedonotexplic-
constructtheenvironment. Obtainingagoodqualityrecon- itly constrain the model to predict matches on the epipo-
structionwoulditselfrequireagoodmatchingnetwork,un- lar lines, but rather encourage this behaviour through our
lessauxiliarysourcesofinformation,likea3Dsensor,were epipolar loss functions. In this respect, our work is most
available. Toresolvethischicken-and-eggproblem,wein- similar to CAPS [44], which also learns keypoint feature
stead propose an approach that only requires approximate descriptors. Thatapproachhoweverisdesignedfortraining
known camera poses, which can be acquired from odome- fromscratch,socannotbenefitfrompre-trainingandalways
tryorGPScombinedwithanaccelerometer.Thisallows,in requirescameraposesupervision, unlikeourproposedun-
particular, for model adaptation and finetuning in domains supervisedstrategy.
whereground-truth3Dstructureisdifficulttoprocure.
Toaddressthisproblem,weproposetofine-tuneexisting
Keypointdetectionandfeaturematching. Theproblem
models using epipolar losses, drop-in replacements for the
ofcreatingcorrespondencesbetweenpointsinimagepairs
correspondencelossesinstate-of-the-arttransformer-based
hasbeenalongstandingchallengeincomputervision.Early
matching networks. These encourage putative matches to
efforts involved hand-crafted criteria for keypoint detec-
lieontheassociatedepipolarlines,whicharecomputedus-
tion, description, and matching in well-established meth-
ing the relative camera poses. While significantly weaker
ods such as the Harris corner detector [17], SIFT [26],
than correspondence supervision, we observe that this cue
SURF[3], andORB[30]. Morerecently, deepneuralnet-
issufficientforfinetuningexistingmodelsonnewdata. Fi-
work methods have outperformed classical techniques for
nally, we propose a bootstrapping approach that removes
both keypoint detection and matching. Approaches like
the need for camera pose supervision as well, allowing us
SuperPoint [11], D2Net [13], R2D2 [29], and DISK [43]
touseanexistingpre-trainedmodeltoimproveitself. This
excel at keypoint detection, while works such as Super-
expandsthetypesofsettingswherethesemodelscanbede-
Glue[32],SeededGNN[8],ClusterGNN[37],andthevery
ployedandreducestheannotationandpre-processingbur-
recent LightGlue[25] achieve impressive feature matching
dens. Tosummarise,ourcontributionsare:
results. An alternative class of approaches eschew the de-
1. Epipolar losses—drop-in replacements for existing
tection step entirely. These detector-free methods, includ-
matching losses that remove the need for strong corre-
ingLoFTR[38],MatchFormer[45]andASpanFormer[9],
spondencesupervision;and
have demonstrated remarkable performance by creating
2. Strategies for adapting pre-trained models to new do-
matches on a dense grid, eliminating the need for sepa-
mainswithonlyposesupervision,ornosupervision.
ratelyselectedkeypoints,andachievestate-of-the-artaccu-
Weevaluateontwochallengingdatasetswithsignificantly
racy. Due to their success, we focus on this detector-free
different characteristics from the pre-training data: an in-
paradigm as well, although our approach is not limited to
door drone dataset and an outdoor phone camera dataset.
thesemodels.
Comparedtotheoriginalmodel,ourweaklysupervisedand
unsupervisedfinetuningstrategiesallowustofindkeypoint
Unsupervised domain adaptation. Finetuning a large
matchesthatachievesignificantlyimprovedposeestimation
modelpre-trainedonlargeamountsofdata(acomputation-
results.
allyexpensiveprocess)inordertoimproveperformanceor
decrease training time is a well-established practice in the
2.Relatedwork
field of deep learning, and a comprehensive review would
Epipolar geometry in deep learning. Several studies exceed the scope of this paper. Of particular relevance to
have leveraged geometric priors based on epipolar geom- the present work is the sub-field of unsupervised domain
etry to enhance the performance of deep learning methods adaptation (see, e.g., Wilson and Cook [46] and Farahani
across various tasks. For instance, He et al. [19] conduct etal.[15]forsurveys). Inunsuperviseddomainadaptation,
direct feature aggregation along epipolar lines to improve a base model trained on labeled data is fine-tuned to im-proveperformanceonadifferentdatasetwithlimitedorno pointcorrespondencesorfromtheirrelativeposeandcam-
labels.However,themajorityofpapersinthisareafocuson era intrinsics, without requiring any information about the
methodstomakethebasemodelmoreadaptabletodomain 3D geometry of the scene. That is, given two camera ma-
shifts,withcomparativelylessemphasisonthefine-tuning tricesP = K [I | 0]andP = K [R | t]witharotationR,
1 1 2 2
processitself.Weinsteadconsiderhowtofine-tuneamodel translationt(uptoscale)andcameraintrinsicmatricesK
1
thatwastrainedwithstrongsupervisionindomainswhere andK ,thefundamentalmatrixF∈R3×3isgivenby
2
suchground-truthinformationisunavailable.
F=K−T[t] RK−1, (2)
2 × 1
3.Epipolarsupervisionformatching
In this section, we present Subpixel Correspondence Esti- with [t] × =
(cid:104) t0
3
− 0t3 −t2 t1(cid:105)
is the matrix representation of
−t2 t1 0
matioNwithEpipolarSupervision(SCENES),ourmethod thecrossproduct.Sinceagoodestimateoftherelativecam-
forestimatingimagepointmatcheswithoutcorrespondence eraposeisoftenavailablefromodometryandtheintrinsic
supervision. We first define the problem, then review the parameters are often recorded in the image metadata, then
relevant background material on epipolar geometry and thefundamentalmatrixandhencetheepipolarlinescanbe
detector-free feature matching methods. Next, we present easilyobtained.
ourepipolarlossfunctions,drop-inreplacementsforexist-
ingmatchinglosses. Finally, wedescribetwotrainingset-
Local feature matching. State-of-the-art detector-free
tingsforadaptingpre-trainedmodelstonewdomains, one
image matching algorithms, starting with LoFTR [38] and
withposesupervisionandtheotherwithoutsupervision.
includingMatchFormer[45]andASpanFormer[9],aimto
The problem under consideration is that of finding the
find as many accurate matches as possible for m pixels in
correspondingpointsintwoormoreviewsofascene: the
thefirstimage,arrangedinagridatlocationsxi ∈I with
(subpixel)imagepositionsthatcorrespondtothesamevis- 1 1
i=1,...,m. Theyinvolvetwostages: coarseandfine.
ible 3D point. Without loss of generality, we consider the
In the first stage, these methods estimate a set of high-
twoimagecase.Formally,giventwoimagesI andI with
1 2
level matches by extracting coarse features, generally for
dimensionsH ×W andH ×W andcameramatricesP
1 1 2 2 1
8 × 8 pixel patches. These coarse features are then used
andP ,weaimtofindpairsof(homogeneous)imagecoor-
2 to construct a confidence matrix C ∈ Rm×m that for each
dinates (x ∈ I ,x ∈ I ) with high recall and precision
1 1 2 2
such that x 1 = P 1X and x 2 = P 2X, where X ∈ R3 is a s ba ilm itp yli tn hg atl to hc eat ri eo sn pex ci 1 tii vn eth pe ixfi elrs pt ai tm chag ae te lost ci am tia ot nes xt jhe ∈pr Iob ia n-
visiblesurfacepointonthescenegeometry. 2 2
thesecondimageisacorrectmatch. Thiscoarse-levelcon-
3.1.Background fidencematrixissupervisedusingaclassification-typeloss
L , such as cross-entropy or negative log-likelihood, with
Epipolar geometry. We begin with a brief review of c
respecttoaground-truthbinarymaskM ∈ Rm×m thatis1
epipolar geometry; for a more detailed treatment, refer to
forground-truthcorrespondences(calculatedusingground-
HartleyandZisserman[18]. Theepipolargeometryoftwo
truth camera poses and depths) and 0 otherwise. Letting
views is the geometry of the intersection of the two image
xi ∈ I be the location of the ground-truth match for lo-
planes with the pencil of planes through the baseline join- g 2
ing the camera centres. For a 3D point X imaged in two cation xi 1, we have M ij = 1iffxj 2 = xi g. The coarse level
viewsat2Dpointsx andx ,theepipolargeometrydefines loss is then a function of this ground-truth matrix and the
1 2
therelationbetweentheseimagepointsviathefundamen- confidencematrix: L c =f(C,M).
tal matrix F. Expressing x in homogeneous coordinates In the second stage, an initial set of m′ ≤ m coarse
1
andinterpretingitasarayextendingfromthecameracen- matchescomputedfromthecoarse-levelconfidencematrix
trethroughx intheimageplane,thentheprojectionofthis arerefinedatafinerlevel. Fromasetoffinerfeatures(gen-
1
lineintoasecondimageistheepipolarlinecorresponding erally2×2or4×4pixelpatches),thesecondstagecrops
to x . The corresponding point x must lie on this line, a windowssurroundingeachofthecoarselevelmatches,and
1 2
relationthatiscapturedbytheequation returnsarefinedsetofmatchedimagecoordinatesxˆk with
2
k = 1,...,m′. Whereasthecoarsestageisonlydesigned
xTFx =0 (1)
2 1 to compute matches on a discrete grid, the fine stage re-
wheretheepipolarlinecorrespondingtothepointx inim- turns floating-point coordinates. The supervision of this
1
age2isgivenbyl = Fx . Thisconstitutesasignificant stage therefore also differs from that of the coarse stage:
12 1
constraint on where the corresponding point might be lo- insteadofaclassification-typeloss,aregressionlossL f is
cated,reducingthesearchspacefrom2Dto1D. used, whichisafunctionofthedistancedbetweenthelo-
Thefundamentalmatrixrelatingtwoimagescanbecom- cation of the ground-truth match and the refined matching
puteddirectlyfromtheimages,eitherfrom(atleastseven) coordinates: L = 1 (cid:80)m′ g(d(xˆk,xk)).
f m′ k 2 gEpipolar regression loss. For the fine-level regression
loss, we replace the distance between the estimated and
xˆ2 theground-truthmatchwiththeperpendiculardistancebe-
depi tween the estimated match xˆi and the epipolar line li , as
2 12
wasalsodoneinWangetal.[44]. Inhomogeneouscoordi-
l12=Fx1 nates,withthethirdcoordinatenormalisedto1,
(a)Classification:C (b)Classification:Mepi (c)Regression:depi |xiTFTxˆi|
depi(xi,xˆi)= 1 2 . (4)
Figure2. Visualisationoftheepipolarclassification(coarse)and 1 2 (cid:112) ([FTxˆi 2] 1)2+([FTxˆi 2] 2)2
regression (fine) losses. The epipolar classification loss assigns
the‘ground-truth’matchlocationtothehighestprobabilitypoint Here[FTxˆi] referstothek-thcomponentofthethree-
2 k
ontheepipolarlineandcomparestheresultingbinarymaskMepito dimensionalvector[FTxˆi].
2
thepredictedconfidencemapCviaacross-entropylossorsimilar.
Theepipolarregressionlosscomputestheperpendiculardistance
betweenthepredictedmatchxˆ andtheepipolarlinel .Notethe Totalepipolarloss. Thetotalepipolarlossisthen
2 12
thinepipolarlineisplottedin(a)and(b)forvisualisationpurposes
only,itisnotpartoftheconfidencemapCorbinarymaskM. λ
(cid:88)m′
Lepi =(1−λ)f(C,Mepi)+ g(depi(xk,xk)) (5)
m′ 1 2
k
3.2.Epipolarlosses
where typically the function f is a binary cross entropy
The aforementioned classification and regression losses function, g is a simple linear scaling, and λ is a hyper-
usedtotrainstate-of-the-artlocalfeaturematchingmodels parameterthatcontrolstherelativeimportanceoftheclas-
requiretheground-truthcorrespondences. However,unless sification and regression losses. We note that it is gener-
thedatasethasground-truthcameraposesand3Dgeometry, ally straightforward to adapt most loss functions that use
these can be difficult to obtain. To circumvent this limita- ground-truth image correspondences to supervise keypoint
tion,wereformulatebothlosstypessuchthattheyonlyre- matches.
quirethefundamentalmatrixFforagivenimagepairI and
1
3.3.Finetuningwithweaksupervision
I , byreplacingtheground-truthcorrespondencematrixM
2
andthedistancetotheground-truthmatchdwithepipolar Reformulatingthelossesintermsoftheepipolarlinesand
variants Mepi and depi, as visualised in Fig. 2. The funda- distanceslessensthesupervisionrequirements. Thisfacili-
mentalmatrixFofanimagepaircanbecomputedfromthe tatestheadaptationofpre-trainedmodelstonewdomains,
relativecamerapose,orestimatedusingakeypointmatches whereground-truthcorrespondencesmaybedifficulttoob-
betweenthetwoimages. tain. Instead, given a fundamental matrix relating two im-
ages,whichcanbeobtainedfromtherelativeposebetween
thecameras(arotationRandtranslationtuptoscale)and
Epipolarclassificationloss. Forthecoarseclassification
thecameraintrinsicmatricesK andK asshowninEq.2,
loss,asimplestrategywouldbetoreplacethebinarymask 1 2
both Mepi and depi can be computed. A pre-trained model
M of the ground-truth point correspondences with a binary
canthenbefinetunedonanewdatasetbyoptimisingLepi.
maskoftheepipolarlinescomputedusingthefundamental
matrix. With ei 1 ⊂ I 2 the set of coordinates in the second 3.4.Finetuningwithoutsupervision
image that form the epipolar line for point xi ∈ I , this
1 1
would require Mepi = 1iffxj ∈ ei. However, fine-tuning Inscenarioswhereaccurateground-truthcameraposesare
ij 2 1
themodelinthiswaywouldtrainittopredictequallyhigh unavailable, the model can still be refined via the epipo-
matchingscoresforeverypixelalongtheepipolarline.This lar loss using a bootstrapping strategy. While the ground-
means that the probability mass for the coarse matches is truth fundamental matrix F cannot be computed without
diffusedalongtheentireepipolarline,reducingthereliabil- ground-truth camera poses, it can be estimated from (at
ity of the coarse matches. Instead, we assign the ground- leastseven)correspondences,suchasthosefoundbyapre-
truthmatchlocationatthepositionontheepipolarlinethat trained matching model. Our strategy, then, is to apply a
hasthehighestprobability,takinginspirationfromthemax- pre-trained model to a dataset from a new domain to ob-
epipolarlossofBhalgatetal.[4]. Thatis,wedefine tainasetof(poorquality)imagecorrespondences,thenuse
a robust estimator such as RANSAC to estimate the fun-
(cid:40) damental matrices, which are used as supervision for our
1 ifj =argmax (C )
Mepi = k:xk 2∈ei 1 ik (3) epipolarloss.Thekeyintuitionisthatevenifthepre-trained
ij 0 otherwise.
modelelicitsmanyincorrectmatchesforanimagepair,therobustestimatorshouldstillbeabletoestimatethe(approx- In this section, we demonstrate the utility of SCENES
imately) correct fundamental matrix in many cases, which fine-tuning by comparing the performance of fine-tuned
willprovideasupervisionsignalfortheincorrectmatches. modelsonthedownstreamtaskofrelativecameraposees-
As we show in Sec. 4, while this bootstrapping method is timation with that of a number of existing keypoint cor-
naturallylessperformantthanfinetuningusingground-truth respondence estimation methods. We evaluate on three
fundamentalmatrices,itcanstillleadtosignificantperfor- datasets: theEuRoC-MAVdronedataset[6],theSanFran-
manceimprovementsonchallengingdatasets. cisco Landmark dataset [7], and the Aachen day–night
dataset[34,35]inthesupplement.Wethenconductanabla-
3.5.Discussion tionstudyonthefine-tuningalgorithmtoassessthedesign
decisions.
Whilewedonotknowtheground-truthlocationsofthekey-
point matches in the absence of ground-truth depth and/or
4.1.Pose-onlysupervision: EuRoC-MAVdataset
ground-truthposeinformation,wedoknowthattheymust
lieontherespectiveepipolarlines. The EuRoC-MAV drone dataset [6] comprises eleven im-
Forthecoarsestage,wewanttorefinethenetwork’spre- age sequences captured by a remotely-operated drone in
dictedconfidencematrixtoalignwithourbestestimateof two distinct environments: a spacious machine room and
thelocationofthematch. Sincewearestartingfromapre- a smaller room. Leveraging a laser tracker in the machine
trainednetworkthatdoesnotmakerandompredictions,this roomandamotioncapturesystemintheotherroom,accu-
bestestimateisthelocationwiththehighestprobabilityfor ratedrone(andcamera)posesweremeticulouslyrecorded.
beingamatchthatisalsoontheepipolarline,i.e.,theloca- The machine room significantly differs in scale and con-
tionontheepipolarlineforwhichthematchingconfidence tent from ScanNet [10]. It is much larger than the offices,
C ismaximised. Thisprovidesausefultrainingsignalso apartments, etc. included in ScanNet, and contains indus-
ij
longasthehighestprobabilitylocationontheepipolarline trial machinery that is not commonly found in such loca-
isnearthecorrectmatchlocationsufficientlyfrequently. tions. Theimagesequencesvaryindifficultybasedontheir
For thefine stage, replacing thedistance tothe ground- lightinglevelsandamountofmotionblur. Eventheimages
truth match location with the distance to the epipolar line inthe‘easy’sequencesdiffersignificantlyfromthoseinthe
intheregressionlossalsoprovidesanimperfect-yet-useful ScanNetdataset, especiallywithrespecttothedistribution
trainingsignal. Eventhoughthegradientvectorwhentak- of camera poses, when comparing drone footage to hand-
ing the derivative of depi w.r.t. xk may not point exactly heldcameraimages.
2
in the same direction as the gradient vector from taking To use the dataset for correspondence matching, we
the derivative of d w.r.t. xk, the angle between them is need to generate image pairs that have enough overlap to
2
less than ninety degrees (they have a positive inner prod- makematchingkeypointspossible,butnotsomuchthatthe
uct⟨ ∂d ,∂depi⟩ > 0). Followingthegradientvectorofthe task becomes trivial (i.e., nearly-identical images). With-
∂xk 2 ∂xk 2 out ground-truth depth information, we cannot calculate
epipolarlossthereforealsotendstodecreasethedistanceof
thetrueoverlapbetweentheimages. Instead, wecompute
thematchtotheground-truthmatchlocation.
a ‘pseudo-overlap’ score using the known camera poses,
The bootstrapping approach, using estimated instead of
whichvisualinspectionshowsyieldsagoodrangeofover-
ground-truthfundamentalmatrices,ultimatelyimprovesthe
lapsbetweenimagepairs. Moredetailsonthedatasetcon-
keypoint matches generated by improving the consistency
structioncanbefoundinthesupplementarymaterial.
of the matches. By design, algorithms such as RANSAC
ignoremanyormostavailablepoint-matchesandonlyuse
asubsettocomputethefinalhypothesisforthefundamen- Implementationdetails. Wefine-tunetheMatchFormer-
tal matrix. While the points that were used for fundamen- litemethod[45]pre-trainedontheindoorScanNetdataset
tal matrix estimation will experience minimal loss, the re- [10], and evaluate using the images from two held-out se-
mainingmatchesreceivearichgradientsignalduringtrain- quences of the highest difficulty, one from the machine
ing. Inanefforttoimprovetheconsistencyofallgenerated roomandonefromtheindoorroom. Weleavealltraining
matches with the subset of matches that was used to esti- settingsfromtheoriginaltrainingrununchanged, withthe
mate the fundamental matrices, the network will learn to exceptionofdecreasingtheweightdecayfrom0.1to0.01
adapttothenewenvironment’sidiosyncrasies.
anddecreasingthelearningratefrom3×10−3to1×10−4.
Computational limitations also mean that we reduced the
4.Experiments batch size from 64 to 12. We also fine-tune the ASpan-
Former method [9], also pretrained on the indoor ScanNet
To demonstrate the utility of the SCENES fine-tuning for dataset [10]. Again, we mostly retain the training settings
downstreamtasks,wecomparetheperformanceofanum- from the original training run, except that we reduce the
berofkeypointcorrespondenceestimationmethods batchsizeto8duetocomputationallimitations.Table 1. Relative pose estimation performance on the indoor
EuRoC-MAV dataset [6]. We report the area under the curve
(AUC)atthresholdsof5◦, 10◦ and20◦ andthematchingpreci-
sion(P)atathresholdof5·10−4.OurSCENESmethodexhibits
significantgainsinperformanceacrossthesemetrics.
PoseEstimationAUC(%)
Method P(%) (a)MatchesbeforeSCENESfine-tuning
@5◦ @10◦ @20◦
SP[11]+SG[32] 0.6 2.0 4.4 17.3
SP[11]+CAPS[44] 1.4 6.4 16.3 20.6
SP[11]+LG[25] 7.6 22.9 40.2 38.3
DISK[43]+LG[25] 6.6 17.9 31.6 35.9
MatchFormer-large[45] 5.1 16.0 31.8 38.4
(b)MatchesafterSCENESfine-tuning
MatchFormer-lite[45] 3.0 11.6 25.1 35.0
+SCENESfine-tuning +6.1 +11.9 +17.7 +28.8
=Ours(MF) 9.1 23.5 42.8 63.8
ASpanFormer[9] 4.8 17.6 34.5 38.7
+SCENESfine-tuning +6.8 +8.8 +10.2 +18.1
=Ours(ASF) 11.6 26.2 44.7 66.8
(c)MatchesbeforeSCENESfine-tuning
Baselines. Wecomparewithanumberofstate-of-the-art
imagematchersonthedataset. Sinceitslossfunctiononly
requirescameraposes,wetraintheCAPS[44]methodfrom
scratch on EuRoC-MAV using the authors’ settings. For
the other baselines, we use the implementations and pre-
trainedweights(nottrainedonEuRoC-MAV)providedby
(d)MatchesafterSCENESfine-tuning
therespectiveauthors,choosingthe‘indoor’versionswhere
there is a choice. For every method we compute relative Figure3. QualitativematchingresultsontheEuRoC-MAVdrone
camera pose estimates from the matched keypoints using dataset [6]. The correspondences found by the state-of-the-art
a five-point essential matrix estimator with RANSAC, as MatchFormer algorithm [45] are displayed before (a, c) and af-
implemented in the OpenCV [5] library, with a matching ter (b, d) SCENES fine-tuning. The colour denotes whether the
thresholdof0.5pixels. squaredsymmetricalepipolardistanceisbelow(green)orabove
(red)athresholdof5·10−4.Ourmethodimprovesthequalityand
numberofcorrectmatches.
Results. Like previous works [9, 25, 32, 38, 45], we re-
porttheareaunderthecumulativecurve(AUC)ofthepose
error (the maximum of the angular error in rotation and truthpointcorrespondencesonadifferentdataset.
translation) at thresholds of 5◦, 10◦ and 20◦. We also re-
4.2.Bootstrappedsupervision: SFdataset
port the matching precision (P): the average percentage of
predictedmatcheswithasquaredsymmetricalepipolardis- The San Francisco Landmark dataset [7] is a collection
tance smaller than 5·10−4. As can be seen in Tab. 1 and of roughly one million perspective images obtained from
in the qualitative results in Fig. 3, the fine-tuned methods panoramaimagescollectedbyacarindowntownSanFran-
showalargejumpinperformancecomparedtotheoriginal cisco. TheGPSmeasurementsforthecamerapositionand
pre-trained methods. The fine-tuned version of the small orientation are also provided, though these measurements
MatchFormer-lite network even outperforms the previous are frequently inaccurate by up to dozens of metres. Test
state-of-the-art,thecombinationoftheSuperPointdetector imageswerecollectedusingcameraphones,andToriietal.
and the LightGlue matcher. The performance of the base- [42] provides accurate reference poses for 598 of them.
line methods on this dataset is considerably worse than on WhiletheGPSmeasurementsaretooinaccuratetocompute
the dataset they were trained on (ScanNet), showing that fundamentalmatrices,wecanstillusethemtogenerateim-
theirabilitytogeneralisetonoveldatadistributionsislim- age pairs using a similar ‘pseudo-overlap’ score as before.
ited. Our SCENES fine-tuning approach goes some way Thereaderisreferredtotheappendixfordetails.
to alleviating this issue. The CAPS algorithm also strug- This dataset represents a realistic example of a setting
gles, not having benefited from pre-training with ground- withoutanyposesupervisionavailablefortrainingorfine-tuning,whilestillcontainingasmalltestdatasetwithrefer- Table 2. Relative pose estimation performance on the outdoor
enceposesthatallowsustoevaluatetheeffectofthefine- San Francisco Landmarks dataset [7]. We report the area un-
der the curve (AUC) at thresholds of 5◦, 10◦ and 20◦ and the
tuning.Wegiveresultsofourbootstrappingmethodapplied
matchingprecision(P)atathresholdof10−4. Evenwithoutany
to the EuRoC-MAV dataset (where accurate poses can be
availableground-truthcorrespondences,posesor3Dstructure,our
collectedmuchmoreeasily)inthesupplementarymaterial.
SCENESfine-tuningmethodisabletoimprovetheperformance
ofmatchingalgorithmsonthischallengingdataset.
Implementation details. We again fine-tune the
Matchformer-lite [45] and ASpanFormer [9] methods, PoseEstimationAUC(%)
Method P(%)
this time using the respective ‘outdoor’ weights trained on
@5◦ @10◦ @20◦
theMegaDepthdataset[22]asinitialisation. Toemploythe
bootstrappingstrategydescribedinSec.3.4intheabsence SP[11]+SG[32] 1.2 4.1 9.9 15.1
of ground-truth poses for training images, we use each of SP[11]+LG[25] 19.4 37.4 54.1 34.4
DISK[43]+LG[25] 19.7 37.8 55.2 36.7
the base models to compute fundamental matrices for one
MatchFormer-large[45] 21.2 39.6 56.8 32.4
million randomly selected image pairs using OpenCV’s
implementation of RANSAC [16]. To maximise the MatchFormer-lite[45] 19.8 36.9 54.0 32.1
reliability of these estimates, we only use those image +SCENESfine-tuning +0.1 +2.0 +2.2 -1.0
pairs for training for which the base model extracted at =Ours(MF) 19.9 38.9 56.2 31.1
least 100 keypoint matches and for which the estimated ASpanFormer[9] 20.1 38.4 55.4 33.4
fundamental matrices result in at least 20 inliers. This +SCENESfine-tuning +1.1 +0.4 0.0 -0.4
leaves749,581and688,466imagepairsforfine-tuningthe =Ours(ASF) 21.2 38.8 55.4 33.0
Matchformer-liteandASpanFormermethodsrespectively.
Thedifferentdatasetcollectionmethodsforthetraining
andtestingdatasetsleadtomeaningfullydifferentdatadis- FollowingSarlinetal.[32],thethresholdforthematching
tributions. Toensurethatthenetworksarefine-tunedtoad- precisioninthisoutdoorsettingis1·10−4.
just to the San Francisco environment without overfitting This setting, where the images of the training dataset
on features such as the camera location, we add an equal were collected in a very different way to those in the test-
numberofimagepairsfromtheMegaDepthdataset[22]to ing dataset, does not play to the strengths of a fine-tuning
eachbatch. ThelossesfortheseMegaDepthimagesarethe approach, as an important type of regularity that the net-
originalcorrespondencelosses. workmightadapttoisnowthesourceofmisleadingtrain-
FortheMatchformer-litemodel, weusethesametrain- ing signals. Despite this, and despite the additional noise
ing settings as during the initial training, with the excep- resulting from the bootstrapping approach, the results in
tion of again decreasing the weight decay to 0.01 and the Tab. 2 show noticeable performance improvements after
learningrateto1·10−4. Wealsodecreasedthebatchsize fine-tuning. When fine-tuning with bootstrapping on the
to8imagepairstakenfromeachoftheSanFranciscoand EuRoC-MAVdataset(seethesupplementary),wheretrain-
theMegaDepthdatasets. FortheASpanFormermodel, we ingandtestingsetswerecollectedinmoresimilarways,the
decreased the batch size further to two image pairs from performancegainsareevenmoresubstantial.
each of the two datasets, and also decreased the fraction These results show the feasibility of our bootstrapping
of coarse matches that are used for the fine-level supervi- strategyforfine-tuningakeypointmatcherondatasetswith-
sion during training from 30% in the original training run outanyground-truthinformation.
to 10%. Due to computational limitations, we also had to
changethefunctionf usedtocomputethecoarselevelloss 4.3.Ablationstudy
fromacrossentropyfunctiontothesparseformulationused
Inthissection,weanalysetheimpactofdesignchoiceson
inLoFTR[38].
our method. We evaluate modifications to the fine-tuning
oftheMatchFormer-litemodelontheEuRoC-MAVdataset
Baselines. We provide results for the same set of state- [6]. The results of the ablations are in Tab. 3, using the
of-the-art baselines as for the EuRoC-MAV dataset, com- samemetricsasbeforeinTab.1. Weevaluatetheimpactof
putingtherelativecameraposesusingRANSACasbefore. threealterationstothefullmethod: removingthefine-level
Thistime,wechoosethe‘outdoor’versionswhenmultiple loss (λ = 0), removing the coarse-level loss (λ = 1), and
setsofweightsareavailable. Toaidcomparabilitybetween removingtheargmaxinEq.3,settingMepi = 1iffxj ∈ ei
ij 2 1
differentmethods,wepadallimagesto640×640pixels. andzerootherwise. Thissetsthemaskto1forallpixelson
theepipolarline,notjustthehighestprobabilitypixel.
Results. We report the area under the cumulative curve If we remove the loss term for either the coarse or fine
(AUC) of the pose error at thresholds of 5◦, 10◦ and 20◦. matchingstage, thenetworkfocusesexclusivelyonthere-Table3. Ablationstudycomparingvariantsofourdesign,evalu- Forfurtherquantitativeandqualitativeresultsandabla-
ated with respect to relative pose estimation performance on the tions,werefertheinterestedreadertotheappendix.
indoor EuRoC-MAV dataset [6]. We report the area under the
curve(AUC)atthresholdsof5◦, 10◦ and20◦ andthematching 5.DiscussionandConclusion
precision(P)atathresholdof5·10−4. Therow“w/oclassifica-
tionargmax”denotesusingthena¨ıveepipolarclassificationloss, Limitations. One limitation is that the training signal is
whereMe ip ji =1forallpixelsontheepipolarline,notjustthehigh- weaker than the fully-supervised approach, since a match
est probability pixel. The rows ‘perturb poses by x◦’ randomly predictedtobeatthewrongplaceontheepipolarlinedoes
perturbthegroundtruthtranslationandrotationbyxdegrees.
not incur a penalty. Moreover, the epipolar classification
lossinparticularcanbemisleading,sinceitencouragesthe
PoseEstimationAUC(%)
Method P(%) match to move towards the highest probability position on
@5◦ @10◦ @20◦ theepipolarline,whichmaybesuboptimalandentrenches
anybiaseslearnedbythepre-trainedmodel.
MatchFormer-lite[45] 3.0 11.6 25.1 35.0
Ours(MF) 9.1 23.5 42.8 63.8
w/ofine-levelloss 5.5 15.2 29.4 61.2 Conclusion. We have proposed a method for estimating
w/ocoarse-levelloss 10.0 20.7 34.4 69.9 subpixelcorrespondencesbetweentwoimages,withoutre-
w/oclassificationargmax 0.0 0.0 0.0 0.0 quiring ground-truth correspondences for training. To do
perturbposesby1◦ 8.4 20.2 36.5 63.1 so, we reformulate the standard classification and regres-
perturbposesby2◦ 4.2 14.7 30.0 41.1
sionmatchinglossesasepipolarlosses,whichonlyrequire
ground-truth fundamental matrices during training. These
can be computed when the relative camera poses and in-
mainingtaskanditsperformanceontheotherdeteriorates.
trinsicsareknown,orcanbeestimatedbyanadequatepre-
Removing the fine-level loss leads the network to create
trained matching network. We develop two strategies for
a large number of approximate matches, but these are too
adaptingpre-trainedmatchingmodelstonewdomains: one
noisytoestimateaccurateposes.Removingthecoarse-level
where the fundamental matrices are provided, and another
lossleadsthenetworktocreatefewermatches. Thesemay
where they must be estimated, a bootstrapping approach.
be very accurate, but too few matches make the RANSAC
We demonstrate compelling performance on challenging
poseestimatorlessrobustandpronetofailentirely. While
datasets, improving the pose estimation results of state-of-
this model does better for metrics where high accuracy is
the-art methods using our weakly supervised and unsuper-
important, it does notably worse at the metrics with less
visedfinetuningstrategies. Thisopensthedoortotransfer
stringentthresholds(10◦and20◦).Onbalance,amixtureof
learningoractivelearningapplications.
bothlossesleadstotheoverallmostreliableperformance.
Finally, as mentioned in Sec. 3.2, removing the argmax
Future work. Four directions warrant further investiga-
in the classification loss actively discourages the network
tion. First,thebootstrappingapproachcouldbemadeitera-
to select good coarse matches. The matching probability
tive,wherethefundamentalmatricesarere-estimatedevery
in the confidence matrix C becomes spread-out along the
few epochs as the model improves. This may reduce the
entire epipolar line, which reduces the probability that an
impact of incorrect matches from the pre-trained network.
approximatelycorrectcoarsematchwillbeselected. Fine-
Second,theapproachcouldbecoupledwithrecentmethods
tuningwiththislosscausesthemodeltodiverge,failingto
thatinfercameraposesgivenasparsecollectionofimages
providesufficientaccuratematchesforposeestimation.
[24,49]. Third,givenuncertaintyestimatesfortherelative
poses, the epipolar losses could be modified to reflect this
ImpactofNoisyGround-truthPoses. Wealsoevaluate
uncertainty,transforminganepipolarlineintoaprobability
the impact of noise in the ground truth poses on the ef-
distribution on the image plane. Finally, note that the true
fectiveness of the SCENES fine-tuning. We fine-tune the
match must lie between the epipole (projection of the first
Matchformer-lite model on the EuRoC-MAV dataset as in
cameracentreintothesecondimage)andtheprojectionof
Sec.4.1,butrandomlyperturbthegroundtruthposes(both
thepointatinfinity.Thisadditionalconstraintcouldbeused
rotationandtranslation)bymagnitudesofoneandtwode-
toimprovethesignalobtainedfromourepipolarlosses.
grees. These results can also be found in Tab. 3. While
seeminglysmalllevelsofposenoiseaffecttheperformance
Acknowledgements. We are grateful for funding
considerably,thisisnottooproblematic. Astheresultsus-
from EPSRC AIMS CDT EP/S024050/1 (D.K.), Con-
ingourbootstrappingapproach(cf.Sec.4.2andthesupple- tinental AG (D.C.), and the Royal Academy of En-
mentary material) show, existing keypoint matching meth- gineering (RF/201819/18/163, J.H.). We would also
ods already perform well enough to estimate fundamental like to thank Yash Bhalgat for valuable discussions.
matricestosufficientaccuracyforSCENESfine-tuning.References localfeatures. InProceedingsoftheieee/cvfconferenceon
computervisionandpatternrecognition,pages8092–8101,
[1] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pa-
2019. 2
jdla,andJosefSivic. Netvlad: Cnnarchitectureforweakly
[14] Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct
supervised place recognition. In Proceedings of the IEEE
sparseodometry. IEEETPAMI,40(3):611–625,2017. 1
conference on computer vision and pattern recognition,
[15] Abolfazl Farahani, Sahar Voghoei, Khaled Rasheed, and
pages5297–5307,2016. 3
Hamid R Arabnia. A brief review of domain adaptation.
[2] Simon Baker and Iain Matthews. Lucas–Kanade 20 years
Advancesindatascienceandinformationengineering:pro-
on:Aunifyingframework. IJCV,56:221–255,2004. 1
ceedingsfromICDATA2020andIKE2020,pages877–894,
[3] Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. Surf:
2021. 2
Speeded up robust features. In Computer Vision–ECCV
2006: 9thEuropeanConferenceonComputerVision,Graz, [16] Martin A Fischler and Robert C Bolles. Random sample
Austria,May7-13,2006.Proceedings,PartI9,pages404– consensus:aparadigmformodelfittingwithapplicationsto
imageanalysisandautomatedcartography.Communications
417.Springer,2006. 2
oftheACM,24(6):381–395,1981. 7
[4] YashBhalgat,JoaoFHenriques,andAndrewZisserman. A
lighttouchapproachtoteachingtransformersmulti-viewge- [17] ChrisHarris,MikeStephens,etal. Acombinedcornerand
ometry. InCVPR,pages4958–4969,2023. 2,4 edge detector. In Alvey vision conference, pages 10–5244.
[5] G. Bradski. The OpenCV Library. Dr. Dobb’s Journal of Citeseer,1988. 2
SoftwareTools,2000. 6 [18] RichardHartleyandAndrewZisserman. MultipleViewGe-
[6] Michael Burri, Janosch Nikolic, Pascal Gohl, Thomas ometryinComputerVision. CambridgeUniversityPress,2
Schneider,JoernRehder,SammyOmari,MarkusWAchte- edition,2004. 1,3
lik, and Roland Siegwart. The euroc micro aerial vehicle [19] YihuiHe, RuiYan, KaterinaFragkiadaki, andShoou-IYu.
datasets. The International Journal of Robotics Research, Epipolar transformers. In Proceedings of the ieee/cvf con-
2016. 1,5,6,7,8,2 ference on computer vision and pattern recognition, pages
[7] David Chen, Sam Tsai, Vijay Chandrasekhar, Gabriel 7779–7788,2020. 2
Takacs, Huizhong Chen, Ramakrishna Vedantham, Radek [20] BertholdKPHornandBrianGSchunck. Determiningopti-
Grzeszczuk, and Bernd Girod. Residual enhanced visual calflow. Artificialintelligence,17(1-3):185–203,1981. 1
vectors for on-device image matching. In 2011 Confer- [21] ShihaoJiang, DylanCampbell, YaoLu, HongdongLi, and
enceRecordoftheFortyFifthAsilomarConferenceonSig-
RichardHartley. Learningtoestimatehiddenmotionswith
nals,SystemsandComputers(ASILOMAR),pages850–854. global motion aggregation. In ICCV, pages 9772–9781,
IEEE,2011. 5,6,7,2 2021. 1
[8] HongkaiChen,ZixinLuo,JiahuiZhang,LeiZhou,Xuyang
[22] ZhengqiLiandNoahSnavely. Megadepth:Learningsingle-
Bai, Zeyu Hu, Chiew-Lan Tai, and Long Quan. Learning
view depth prediction from internet photos. In Computer
tomatchfeatureswithseededgraphmatchingnetwork. In
VisionandPatternRecognition(CVPR),2018. 7,1,3
ProceedingsoftheIEEE/CVFInternationalConferenceon
[23] ZhaoshuoLi, XingtongLiu, NathanDrenkow, AndyDing,
ComputerVision,pages6301–6310,2021. 2
Francis X Creighton, Russell H Taylor, and Mathias Un-
[9] HongkaiChen,ZixinLuo,LeiZhou,YurunTian,Mingmin
berath. Revisitingstereodepthestimationfromasequence-
Zhen,TianFang,DavidMckinnon,YanghaiTsin,andLong
to-sequenceperspective withtransformers. In Proceedings
Quan. Aspanformer: Detector-free image matching with
of the IEEE/CVF international conference on computer vi-
adaptivespantransformer. InECCV,pages20–36.Springer,
sion,pages6197–6206,2021. 2
2022. 1,2,3,5,6,7
[24] AmyLin,JasonYZhang,DevaRamanan,andShubhamTul-
[10] Angela Dai, Angel X Chang, Manolis Savva, Maciej Hal-
siani. Relpose++: Recovering 6d poses from sparse-view
ber, Thomas Funkhouser, and Matthias Nießner. Scannet:
observations. arXivpreprintarXiv:2305.04926,2023. 1,8
Richly-annotated 3d reconstructions of indoor scenes. In
ProceedingsoftheIEEEconferenceoncomputervisionand [25] PhilippLindenberger,Paul-EdouardSarlin,andMarcPolle-
patternrecognition,pages5828–5839,2017. 5,1 feys. LightGlue:LocalFeatureMatchingatLightSpeed. In
ICCV,2023. 2,6,7,5
[11] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabi-
novich. Superpoint: Self-supervisedinterestpointdetection [26] DavidGLowe.Objectrecognitionfromlocalscale-invariant
anddescription. InProceedingsoftheIEEEconferenceon features. InICCV,pages1150–1157.IEEE,1999. 1,2
computer vision and pattern recognition workshops, pages [27] Raul Mur-Artal, Jose Maria Martinez Montiel, and Juan D
224–236,2018. 2,6,7,5 Tardos. ORB-SLAM: a versatile and accurate monocular
[12] MihaiDusmanu,IgnacioRocco,TomasPajdla,MarcPolle- SLAMsystem.IEEETransactionsonRobotics,31(5):1147–
feys,JosefSivic,AkihikoTorii,andTorstenSattler. D2-net: 1163,2015. 1
A trainable cnn for joint description and detection of local [28] VigneshPrasad,DipanjanDas,andBrojeshwarBhowmick.
features. InCVPR,pages8092–8101,2019. 1 Epipolar geometry based learning of multi-view depth and
[13] MihaiDusmanu,IgnacioRocco,TomasPajdla,MarcPolle- ego-motion from monocular sequences. In Proceedings of
feys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2- the 11th Indian Conference on Computer Vision, Graphics
net: A trainable cnn for joint description and detection of andImageProcessing,pages1–9,2018. 2[29] JeromeRevaud,PhilippeWeinzaepfel,Ce´sarDeSouza,Noe [43] Michał Tyszkiewicz, Pascal Fua, and Eduard Trulls. Disk:
Pion,GabrielaCsurka,YohannCabon,andMartinHumen- Learning local features with policy gradient. Advances in
berger.R2d2:repeatableandreliabledetectoranddescriptor. Neural Information Processing Systems, 33:14254–14265,
arXivpreprintarXiv:1906.06195,2019. 2 2020. 2,6,7,5
[30] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary [44] Qianqian Wang, Xiaowei Zhou, Bharath Hariharan, and
Bradski. Orb:Anefficientalternativetosiftorsurf. In2011 Noah Snavely. Learning feature descriptors using camera
International conference on computer vision, pages 2564– pose supervision. In Proc. European Conference on Com-
2571.Ieee,2011. 1,2 puterVision(ECCV),2020. 2,4,6
[31] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and [45] QingWang,JiamingZhang,KailunYang,KunyuPeng,and
MarcinDymczyk. Fromcoarsetofine: Robusthierarchical RainerStiefelhagen. Matchformer: Interleavingattentionin
localization at large scale. In CVPR, pages 12716–12725, transformers for feature matching. In ACCV, pages 2746–
2019. 1,2,5 2762,2022. 1,2,3,5,6,7,8,4
[32] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, [46] GarrettWilsonandDianeJCook. Asurveyofunsupervised
and Andrew Rabinovich. Superglue: Learning feature deep domain adaptation. ACM Transactions on Intelligent
matchingwithgraphneuralnetworks.InCVPR,pages4938– SystemsandTechnology(TIST),11(5):1–46,2020. 2
4947,2020. 2,6,7,5 [47] Zhenpei Yang, Zhile Ren, Qi Shan, and Qixing Huang.
[33] Paul-EdouardSarlin,PhilippLindenberger,ViktorLarsson, Mvs2d: Efficient multi-view stereo via attention-driven 2d
and Marc Pollefeys. Pixel-perfect structure-from-motion convolutions. InProceedingsoftheIEEE/CVFConference
withfeaturemetricrefinement. IEEETPAMI,2023. 1 on Computer Vision and Pattern Recognition, pages 8574–
[34] Torsten Sattler, Tobias Weyand, Bastian Leibe, and Leif 8584,2022. 2
Kobbelt. Imageretrievalforimage-basedlocalizationrevis-
[48] Wang Yifan, Carl Doersch, Relja Arandjelovic´, Joao Car-
ited. InBMVC,page4,2012. 5,2
reira, and Andrew Zisserman. Input-level inductive biases
[35] Torsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, for3dreconstruction.InProceedingsoftheIEEE/CVFCon-
LarsHammarstrand,ErikStenborg,DanielSafari,Masatoshi ferenceonComputerVisionandPatternRecognition,pages
Okutomi,MarcPollefeys,JosefSivic,etal. Benchmarking 6176–6186,2022. 2
6dofoutdoorvisuallocalizationinchangingconditions. In
[49] JasonY.Zhang,DevaRamanan,andShubhamTulsiani.Rel-
ProceedingsoftheIEEEconferenceoncomputervisionand
Pose:Predictingprobabilisticrelativerotationforsingleob-
patternrecognition,pages8601–8610,2018. 5,2
jectsinthewild. InECCV,2022. 1,8
[36] JohannesLSchonbergerandJan-MichaelFrahm. Structure-
from-motion revisited. In CVPR, pages 4104–4113, 2016.
1
[37] YanShi,Jun-XiongCai,YoliShavit,Tai-JiangMu,Wensen
Feng,andKaiZhang. Clustergnn: Cluster-basedcoarse-to-
finegraphneuralnetworkforefficientfeaturematching. In
ProceedingsoftheIEEE/CVFConferenceonComputerVi-
sionandPatternRecognition,pages12517–12526,2022. 2
[38] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and
XiaoweiZhou. LoFTR:detector-freelocalfeaturematching
withtransformers. InCVPR,pages8922–8931,2021. 1,2,
3,6,7
[39] Hajime Taira, Masatoshi Okutomi, Torsten Sattler, Mircea
Cimpoi,MarcPollefeys,JosefSivic,TomasPajdla,andAk-
ihiko Torii. InLoc: indoor visual localization with dense
matchingandviewsynthesis. InCVPR,2018. 1
[40] Zachary Teed and Jia Deng. RAFT: recurrent all-pairs
fieldtransformsforopticalflow. InECCV,pages402–419.
Springer,2020. 1
[41] CarlToft,WillMaddern,AkihikoTorii,LarsHammarstrand,
Erik Stenborg, Daniel Safari, Masatoshi Okutomi, Marc
Pollefeys, Josef Sivic, Tomas Pajdla, Fredrik Kahl, and
Torsten Sattler. Long-term visual localization revisited.
IEEE Transactions on Pattern Analysis and Machine Intel-
ligence,44(4):2074–2088,2022. 3
[42] Akihiko Torii, Hajime Taira, Josef Sivic, Marc Pollefeys,
MasatoshiOkutomi,TomasPajdla,andTorstenSattler. Are
large-scale3dmodelsreallynecessaryforaccuratevisuallo-
calization? IEEEtransactionsonpatternanalysisandma-
chineintelligence,43(3):814–829,2019. 6SCENES: Subpixel Correspondence Estimation With Epipolar Supervision
Supplementary Material
A.HeuristicforCreatingImagePairs Table1. Ablationstudycomparingvariantsofourdesign,evalu-
ated on the indoor EuRoC-MAV dataset [6]. We report the area
In this section, we describe in more detail the heuristics underthecurve(AUC)atthresholdsof5◦, 10◦ and20◦ andthe
usedtocreateimagepairsforbothtrainingandtesting,us- matchingprecision(P)atathresholdof5·10−4. Foradetailed
ingonlycameraposeestimates. explanationoftheparameterθ,pleaserefertothemainbodyand
Ifground-truthdepthinformationisavailable, theover- inparticulartoEq.6. Therowlabelled‘Updatedversion’refers
lapbetweentwoimagescanbecomputedandvalidimage toafinetunedmodelthatwasfinetunedforlongerthantheorigi-
nalfinetunedversion(‘Ours’)butnotaslongastheablations(26
pairs are those with an overlap within some range. This
epochscomparedto20and36epochsrespectively).
range is chosen to ensure there is enough overlap between
thetwoimagesofapairtocomputeaccuratekeypoints,but
PoseEstimationAUC(%)
notsomuchoverlaptomakethistasktrivial. Method P(%)
Without ground-truth-depth, we assume a ‘pseudo- @5◦ @10◦ @20◦
ground-truth-depth’, and use this to compute approximate
MatchFormer-lite[45] 3.0 11.6 25.1 35.0
image overlaps. While this approximation is extremely Ours(MF) 9.1 23.5 42.8 63.8
rough, visual inspection of the generated image pairs Updatedversion(MF) 12.9 26.7 42.1 69.9
showedthatitnonethelessselectsimagepairswiththede- w/θ=1.0 11.8 27.0 43.8 71.0
√
siredcharacteristicofenough,butnottoomuch,overlap. w/θ=3 2 14.8 30.3 46.9 56.8
w/ScanNetsamples 13.9 28.5 44.2 68.2
EuRoC-MAV. For both the EuRoC-MAV training and
testingdatasets,weassumethateachimagewastakenfrom
B.FurtherAblationStudy
insideahemisphere. Thehemisphereconsistsofahorizon-
talplaneatacertainheightz plane < z camera andaspherical Inthissection,wegivetheresultsofsomeadditionalabla-
domewithradiusr spherecenteredat(x camera,y camera,z plane). tions (Tabs. 1 and 2). Firstly, we investigate the impact of
For the machine room environments, we chose z plane = slightvariationsinthedefinitionofei 1 ∈I 2,thesetofcoor-
−2.0m and r sphere = 10.0m, and for the smaller room we dinates in the second image that form the epipolar line for
chosez plane =0.0mandr sphere =3.0m. thepointxi 1 ∈I 1. Sinceweareworkingwithpixelpatches
WealsoundistorttheimagesintheEuRoC-MAVdataset of finite size, ei is the set of pixels that the epipolar line
1
andscaleandcropthemtoaresolutionof640×480pixels. “passes through”. Formally, if w is the width of a single
pixelpatch,thismeansthat
San Francisco. The San Francisco training set images
w
are provided together with GPS camera pose information. ei 1 ={x 2 :x 2 ∈I 2,depi(xi 1,x 2)≤θ 2} (6)
These are too inaccurate to use as supervision signal, but
weareabletousethemtogenerateimagepairs. where θ is a parameter determining how close the epipo-
Sincemostimagesinthetrainingsetaretakenfromthe larlinehastopasstothecenterofthepixelpatchtocount
middle of a road with multi-storey building on either side, as “passing through” the patch. In the experiments in the
√
we assume that each image was taken from inside a rect- main paper, we set θ = 2, but other values are possi-
angularboxalignedwiththecurrentdrivingdirection. The ble. Secondly, we investigate the impact of adding or re-
left and right sides of the box are at a distance of 10 me- movingimagesfromtheoriginaltrainingset(ScanNet[10]
ters from the camera, the bottom plane is 2 meters below forindoormodels,andMegadepth[22]foroutdoormodels)
the camera, the front and back planes are at a distance of toeachbatch, usingtheoriginalcorrespondencelossesfor
25 meters from the camera, and the top plane is at infin- thesesamples.
ity. Weestimatethedrivingdirectionasthevectorbetween TheadditionalablationsfortheEuRoC-MAVdatasetin
locationsoftheprecedingandthesucceedingimages. Pix- Tab. 1 are consistently stronger than the original finetuned
els whose rays intersect with the front or back planes are model that we evaluated in the main paper. This suggests
alwayscountedasnotoverlappingwithanotherimage. thatthemodelwhoseresultswerereportedinthemainpa-
Since the test set contained comparatively few images, per had not fully converged. This hypothesis that is sup-
weselectedappropriateimagepairsmanually. ported by the similarly improved results for the ‘Updated
Wescaletheimagessothattheirlongestsidehaslength version’,whichwastrainedforlongerthantheoriginalfine-
640pixels,thenpadthemtosize640×640pixels. tunedversionreportedinthemainpaper,butnotaslongasTable2. Ablationstudycomparingvariantsofourdesign,evalu- Table 3. Relative pose estimation performance on the indoor
atedontheoutdoorSanFranciscodataset[7]. Wereportthearea EuRoC-MAV dataset [6]. We report the fraction of angular ro-
underthecurve(AUC)atthresholdsof5◦, 10◦ and20◦ andthe tationandtranslationerrorsbelowthethresholds5◦,10◦and20◦
matchingprecision(P)atathresholdof10−4. Foradetailedex- inpercentandthemedianangularrotationandtranslationerrors
planationoftheparameterθ,pleaserefertothemainbodyandin indegree. OurSCENESmethodexhibitssignificantgainsinper-
particulartoEq.6. formanceacrossthesemetrics.
PoseEstimationAUC(%) Rotation
Method P(%) Method
@5◦ @10◦ @20◦ <5◦ <10◦ <20◦ Median
(%) (%) (%) (◦)
MatchFormer-lite[45] 19.8 36.9 54.0 32.1
SP[11]+SG[32] 5.20 12.0 29.3 39.3
Ours(MF) 19.9 38.9 56.2 31.1
√ SP[11]+LG[25] 54.0 67.6 77.1 4.16
w/θ=3 2 19.7 38.1 55.2 31.4 DISK[43]+LG[25] 44.1 56.5 64.7 6.86
w/oMegadepthsamples 18.3 35.6 52.7 30.1 MatchFormer-large[45] 43.5 61.6 74.3 6.18
MatchFormer-lite[45] 37.3 54.4 64.7 7.66
+SCENESfinetuning +17.6 +26.0 +29.6 -3.40
the ablations in Tab. 1 (26 epochs compared to 20 and 35 =Ours(MF) 54.9 80.4 94.3 4.26
epochsrespectively). ASpanFormer[9] 48.4 65.9 74.6 5.28
In any case, the consistently strong improvements with +SCENESfinetuning +10.3 +12.3 +15.9 -1.43
=Ours(ASF) 58.7 78.2 90.5 3.85
respect to the baseline model show the robustness of our
finetuning method to moderate changes in certain parame- Translation
ters and the training algorithm. We can note that a lower <5◦ <10◦ <20◦ Median
valueforθ leadstolowerposeestimationperformance, as (%) (%) (%) (◦)
the pixel patch with maximum confidence on the epipolar SP[11]+SG[32] 2.80 7.53 17.2 48.6
lineismorelikelytobefurtherfromthecorrectmatch. The SP[11]+LG[25] 26.1 50.9 65.1 9.74
lowervalueforθhoweverleadstofewercoarsematches,so DISK[43]+LG[25] 19.9 38.8 51.4 18.1
MatchFormer-large[45] 18.8 38.8 56.2 14.0
thatthematchesthatarefoundaremorelikelytobeaccu-
rate,increasingtheprecision. MatchFormer-lite[45] 11.8 30.3 48.5 21.5
+SCENESfinetuning +14.9 +19.3 +24.6 -11.2
TheablationsfortheSanFranciscodatasetinTab.2il-
=Ours(MF) 26.7 49.6 73.1 10.3
lustratetheimportanceoftheaddedMegadepthsamplesto
ASpanFormer[9] 21.1 45.3 61.7 11.7
avoid excessively overfitting onto the San Francisco train-
+SCENESfinetuning +7.9 +7.3 +10.7 -2.49
ingsetdistribution,whichisslightlydifferenttothetestset =Ours(ASF) 29.0 52.6 72.4 9.21
distribution.Whileincreasingtheparameterθbyafactorof
3slightlydecreasesthelocalisationperformance,finetuning
stillimprovesperformance.
Franciscodataset.
The performance improvements on the EuRoc-MAV
C.AdditionalQuantitativeResults
datasetafterfinetuningusingground-truthcameraposesare
verynoticeable,withbothmoreandmoreaccuratekeypoint
In this section we provide some additional quantitative re-
matches being found. The performance improvements on
sults on the relative pose estimation task for the EuRoC-
the San Francisco dataset after finetuning using our boot-
MAVandtheSanFranciscodatasets.Wereportthefraction
strappingmethodwithoutground-truthposesarelessstark
ofrelativeposeestimatesforwhichtheangularrotationer-
butstillevidentinanincreasednumberofaccuratematches
ror and the angular translation error are below thresholds
beingfound.Theimprovementsareweakestforimagepairs
of 5◦,10◦,20◦, as well as the median angular rotation and
thatthebasemodelalsofailson. Thiscanbeexplainedby
translationerrorsindegrees(Tab.3,Tab.4).
thefinetuningmostlyincreasingperformancebyimproving
theconsistencyofmatches.
D.AdditionalQualitativeResults
In this section, we provide some additional qualitative E.LocalisationonAachenDay-Night
results for finetuning the Matchformer-lite model on the
EuRoC-MAV[6]datasetinFig.4andtheSanFrancisco[7] We also provide results of models fine-tuned with our
datasetinFig.5.Thelinesindicatingmatchedkeypointsare method as part of a larger image localisation pipeline. In
colouredbasedonwhetherthe matches’symmetricepipo- particular, following previous papers [9, 25, 32, 38], we
lar error is above or below a threshold of 5·10−4 for the usetheHLocpipeline[31]andtheAachenDay-Night-v1.1
indoorEuRoC-MAVdatasetand10−4 fortheoutdoorSan [34,35]dataset.(a)
(b)
(c)
(d)
Figure4.MatchesfortheMatchformer-lite[45]modelbefore(left)andafter(right)SCENES-finetuningontheEuRoC-MAVdataset.
Implementation Details. To create the image pairs for Baselines. Wealsoprovideresultsforthesamesetofkey-
finetuningintheabsenceofevenapproximatecamerapose point localisation and matching algorithms as in our other
information, we compute embeddings for each image us- experiments. To ensure the results are comparable, we re-
ingNetVLAD[1]andretainallimagepairsforwhichthese size all images so that their longest side has a length of
embeddingshaveacosinesimilarityscoreofatleast0.24. 640 pixels. To reduce the computational cost, we follow
the pre-implemented algorithms in the HLoc pipeline and
We finetune the Matchformer-lite outdoor model using
restrict the matches to return a maximum of one match
theAachenDay-Nighttrainingimagesusingthesameset-
per 8 × 8 pixel patch for the dense matching algorithms
tingsasfortheSanFranciscodataet,usingourbootstrapped
(Matchformer[45]andASpanFormer[9]).
approachwithfundamentalmatricesestimatedbythebase
model, and adding an equal number of image pairs from
the MegaDepth dataset [22] to each batch. We also again Results. The results as computed by the Long-Term Vi-
set the weight decay to 0.01, the learning rate to 1·10−4, sualLocalizationbenchmark[41]server(https://www.
andthebatchsizeto8imagepairstakenfromtheAachen visuallocalization.net) can be found in Tab. 5.
Day-NightandMegaDepthdatasetseach. Our bootstrapped SCENES method improves the locali-(a)
(b)
(c)
(d)
Figure5.MatchesfortheMatchformer-lite[45]modelbefore(left)andafter(right)SCENES-finetuningontheSanFranciscodataset.
sation performance on the daytime images, but decreases creasedgeneralisationability.
the localisation performance on nighttime images. This is
notentirelysurprising–theAachenDay-Nighttrainingset F.BootstrappingonEuRoC-MAV
consists only of daytime images, and finetuning processes
In this section we provide the results of fine-tuning the
generally sacrifice improved performance for slightly de-
MatchFormer-liteandASpanFormermodelsontheEuRoC-Table4.RelativeposeestimationperformanceontheoutdoorSan Table 6. Relative pose estimation performance on the indoor
Francisco dataset [7]. We report the fraction of angular rotation EuRoC-MAV dataset [6]. We report the area under the curve
andtranslationerrorsbelowthethresholds5◦,10◦and20◦inper- (AUC)atthresholdsof5◦, 10◦ and20◦ andthematchingpreci-
centandthemedianangularrotationandtranslationerrorsinde- sion(P)atathresholdof5·10−4.OurSCENESmethodexhibits
gree. significant gains in performance across these metrics even when
notusinganygroundtruthposesforfine-tuning(‘bootstrapping’).
Rotation
Method
<5◦ <10◦ <20◦ Median Method PoseEstimationAUC(%) P(%)
(%) (%) (%) (◦) @5◦ @10◦ @20◦
SP[11]+SG[32] 7.95 17.4 32.7 33.0
SP[11]+SG[32] 0.6 2.0 4.4 17.3
SP[11]+LG[25] 64.5 79.0 87.4 3.32
SP[11]+LG[25] 7.6 22.9 40.2 38.3
DISK[43]+LG[25] 64.0 80.8 89.0 3.38
MatchFormer-large[45] 65.8 82.6 88.7 3.30 DISK[43]+LG[25] 6.6 17.9 31.6 35.9
MatchFormer-large[45] 5.1 16.0 31.8 38.4
MatchFormer-lite[45] 63.4 79.0 87.2 3.41
+SCENESfinetuning +0.4 +1.6 +2.0 +0.1 MatchFormer-lite[45] 3.0 11.6 25.1 35.0
=Ours(MF) 63.8 80.6 89.2 3.42 +SCENESfinetuning +6.1 +11.9 +17.7 +28.8
ASpanFormer[9] 63.4 79.7 87.2 3.47 =Ours(MF) 9.1 23.5 42.8 63.8
+SCENESfinetuning +1.7 +0.2 +0.4 -0.18
MatchFormer-lite[45] 3.0 11.6 25.1 35.0
=Ours(ASF) 65.1 79.9 87.6 3.29
+SCENESbootstrapping +5.2 +12.3 +17.7 +6.5
Translation =Ours(MF-BS) 8.2 23.9 42.8 41.5
<5◦ <10◦ <20◦ Median ASpanFormer[9] 4.8 17.6 34.5 38.7
(%) (%) (%) (◦)
+SCENESfinetuning +6.8 +8.8 +10.2 +18.1
SP[11]+SG[32] 10.6 26.3 49.9 20.0 =Ours(ASF) 11.6 26.2 44.7 66.8
SP[11]+LG[25] 51.2 68.7 78.6 4.77
DISK[43]+LG[25] 51.7 70.9 78.8 4.76 ASpanFormer[9] 4.8 17.6 34.5 38.7
MatchFormer-large[45] 54.1 71.5 81.0 4.30 +SCENESbootstrapping +3.8 +7.1 +8.4 +3.4
=Ours(ASF-BS) 8.6 24.7 42.9 42.1
MatchFormer-lite[45] 51.7 68.9 79.0 4.69
+SCENESfinetuning +2.8 +2.4 +1.4 -0.35
=Ours(MF) 54.5 71.3 80.4 4.34
ASpanFormer[9] 53.6 71.5 80.6 4.42 duetousingmoreapproximateestimatedfundamentalma-
+SCENESfinetuning -0.4 -0.6 -1.1 +0.12 trices,theperformancegainduetotheSCENESfine-tuning
=Ours(ASF) 53.2 70.9 79.5 4.54 isalmostassubstantialaswhenusingground-truthrelative
posesandfundamentalmatrices.
Table 5. Image Localisation performance on the Aachen Day- We might explain this by noting that the epipolar loss
Night-v1.1[34,35]datasetusingtheHLoc[31]toolbox. already only provides an approximate supervisory signal
thatisleastlikelytobemisleadingwhenthedetectedpixel
Method Day Night match is relatively accurate. The estimated fundamental
(0.25m,2◦)/(0.5m,5◦)/(1.0m,10◦) matricesusedinthebootstrappingmethodareleastaccurate
SP[11]+SG[32] 87.1/94.3/98.3 69.1/88.0/97.9 for image pairs that the pre-trained model performs worst
SP[11]+LG[25] 88.1/94.7/98.9 72.3/89.5/99.0 on, but these are the image pairs that the epipolar loss is
DISK[43]+LG[25] 89.7/95.9/99.0 77.0/91.6/99.5 leastusefulevenwhenusingground-truthfundamentalma-
ASpanFormer[9] 88.3/95.5/98.9 76.4/90.6/99.0
trices. Theadditionalnoiseduetoestimatingfundamental
MatchFormer-large[45] 87.7/95.8/98.9 74.9/91.1/99.0
matrices therefore does not do a lot of additional harm in
MatchFormer-lite[45] 87.7/94.3/98.4 70.7/90.6/99.0
thosecases. Thishypothesisissupportedbythedecreasein
+SCENESfinetuning +0.2/+0.4/+0.1 -1.6/-0.5/-0.6
=Ours(MF) 87.9/94.7/98.5 69.1/90.1/98.4 performanceofthebootstrappingmethodbeinglargestfor
thesmallestthreshold;whenthekeypointaccuracyneedsto
behighest,theadditionalnoiseismostdeleterious.
MAV dataset without ground-truth pose supervision. In- Theseresultsunderscorethatourbootstrappingmethod
stead, we follow our bootstrapping strategy and compute isaveryviableapproachtoeasilyadaptapre-trainedpixel
theepipolarlossusingthefundamentalmatricesestimated matchingmodeltonewdomains.
bythepre-trainedmodels.
ThedatasetandevaluationarethesameasinSec.4.1in
themainpaper,andtheresultsareavailable(togetherwith
theresultsfromthemainpaper)inTab.6.
Evenwiththeadditionalnoiseinthesupervisorysignal