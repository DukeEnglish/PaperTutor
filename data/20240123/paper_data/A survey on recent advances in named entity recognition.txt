A SURVEY ON RECENT ADVANCES IN NAMED ENTITY
RECOGNITION
ImedKeraghel StanislasMorbieu MohamedNadif
CentreBorelliUMR9010 KernixSoftware CentreBorelliUMR9010
UniversitéParisCité Paris,France UniversitéParisCité
Paris,France smorbieu@kernix.com Paris,France
imed.keraghel@u-paris.fr mohamed.nadif@u-paris.fr
ABSTRACT
NamedEntityRecognitionseekstoextractsubstringswithinatextthatnamereal-worldobjectsand
todeterminetheirtype(forexample,whethertheyrefertopersonsororganizations). Inthissurvey,
wefirstpresentanoverviewofrecentpopularapproaches,butwealsolookatgraph-andtransformer-
basedmethodsincludingLargeLanguageModels(LLMs)thathavenothadmuchcoverageinother
surveys. Second, wefocusonmethodsdesignedfordatasetswithscarceannotations. Third, we
evaluatetheperformanceofthemainNERimplementationsonavarietyofdatasetswithdiffering
characteristics(asregardstheirdomain,theirsize,andtheirnumberofclasses). Wethusprovidea
deepcomparisonofalgorithmsthatareneverconsideredtogether. Ourexperimentsshedsomelight
onhowthecharacteristicsofdatasetsaffectthebehaviorofthemethodsthatwecompare.
Keywords NamedEntityRecognition·InformationExtraction·NaturalLanguageProcessing·LargeLanguage
Models·MachineLearning
1 Introduction
NamedEntityRecognition(NER)isafieldofcomputerscienceandnaturallanguageprocessing(NLP)thatdealswith
theidentificationandclassificationofnameditemsinunstructuredtext. Theitemsinquestionbelongtopredefined
semantictypessuchaspersons,locations,andorganizations[GrishmanandSundheim,1996a]. NERistodayakey
componentinareasincludingmachinetranslation[BabychandHartley,2003],question-answering[Molláetal.,2006],
andinformationretrieval[Guoetal.,2009].
A number of NER systems have been developed, particularly for English, but also for other languages, including
Chinese[Liuetal.,2022]andFrench[Mikheevetal.,1999]. EarlyNERsystemsusedalgorithmsbasedonhandcrafted
rules,lexicons,andspellingfeatures[Rau,1991]. Systemsweresubsequentlydevelopedthatusedalgorithmsbased
onmachinelearning[NadeauandSekine,2007],neuralnetworks[Collobert,2011],andtransformers[Labuschetal.,
2019a].
SeveralNERsurveyshavebeenpublished. Amongthese,wemightmention[NadeauandSekine,2007,Shaalan,2014],
inwhichtheauthorspresentanoverviewofmethods,fromtheriseofrule-basedNERsystemsthroughtothosebased
onmachinelearning. Theauthorsin[Goulartetal.,2011]analyzerelevantworkinrelationtobiomedicaltextsoverthe
period2007-2009. Marreroetal.[2013]summarizetheworkonNERfromatheoreticalandpracticalpointofview. An
earlystudyonneuralnetwork-basedNERisavailablein[Sharnagat,2014]. Deeplearningmethodsarestudiedin-depth
in[Lietal.,2020a]. However,theauthorsdonotaddresstheuseoftransformersbecausetheiruseremainedlimitedat
thetimeoftheirstudy.
Surveysthatincludetransformer-basedmethodsarelargelyabsentfromtheliterature. Somerecentstudiesinclude
methodsbasedonneuralnetworksandtransformers[Lietal.,2022],butthesearefarfromcompletesincetheydo
not include recent methods such as large language models (LLMs) and graph-based NER. The study Wang et al.
[2022]focusesongraph-basedmethodsspecificallyfornestednamedentities,butdoesnotaddressflatnamedentities.
Moreover,apartfromonestudy,namelyLietal.[2020a](whichdoesnotfeatureanymentionoftransformers),the
4202
naJ
91
]LC.sc[
1v52801.1042:viXraAsurveyonrecentadvancesinNER
studiesinquestiondonotcovertoolsusedforNER.Sincetoolsarekeycomponentsofpracticalinterest,andsince
transformer-basedmethodshavehadasignificantimpact,weareseekingheretopresentacomprehensivesurveyof
NERtoolsandframeworks,includingpopularonesthatmaystillberelevantforlow-resourcelanguagesorinrestricted
computationalenvironments.
Wealsofocusonmethodsintendedtobetrainedonsmalldatasets,anareathat,toourknowledge,hasnotyetbeen
addressedcomprehensivelyinotherworks. Methodsofthiskindwilloftenbeappropriatewherespecifictypesof
entitiesaretoberetrievedandannotateddataarecostlytoproduce.
Theoutlineofthispaperisasfollows. First,wedefinethetaskofNERandprovidesomeexamplesofitsapplications.
Wethendescribethemainresearchapproachesthathavebeendevelopedinthisfield(seeSection4);inparticular,we
focusheavilyonLargeLanguageModels(LLMs)andgraph-basedapproaches. Methodsdesignedforcontextswhere
onlyasmallsampleofannotateddataisavailablearepresentedinSection5. Section6isdevotedtoanoverviewofthe
best-knowntoolsforpre-trainedmodels. AfterdescribingtheevaluationschemesusedinNER(Section7),wepresent
corporafromavarietyoffieldsthatmaybeusefulresourcesfortheresearchcommunity(Section8). InSection9,for
purposesofcomparison,weapplythelatestversionsoffivepopularframeworksonselecteddatasets. Finally,wegive
ourconclusionandperspectivesinSection10.
2 Taskdefinition
NERstandsforNamedEntityRecognition,whichisasubtaskofNLPfocusingonidentifyingandclassifyingnamed
entities within text. Named entities are specific words or phrases that refer to real-world objects such as people,
organizations,locations,dates,quantities,geneandproteinnamesinthebiomedicaldomain. NERaimstolocateand
categorizetheseentitiesintopredefinedcategories.
Figure1: Givenasequenceoftokens,NERoutputstheboundariesofthenamedentitiesalongwiththeirassociated
types.
Inaformalcontext,givenasequenceoftokensdenotedT =(t ,t ,...,t ),NERinvolvesproducingacollectionof
1 2 N
tuples(I ,I ,ℓ),wheresandearebothintegersthatliewithintheinterval[1,N];I andI correspondtothebeginning
s e s e
andtheendingindicesofanamedentitymentionrespectively,andℓdenotesthetypeofentityfromapredefinedsetof
categories. Forexample,inthesentence"BarackObamawasborninHonolulu."NERwouldidentify"BarackObama"
astheperson’snameand"Honolulu"asthelocation,asillustratedinFigure1.
3 ApplicationsofNER
Inthissection,weprovideafewexamplesofNERapplications.
• Informationextraction: NERcanbeusedtoextractstructureddatafromunstructuredtext[Westonetal.,
2019],suchasretrievingthenamesofpersons,organizations,andlocations.
• Informationretrieval:UsingNERinthecontextofinformationretrievalenhancessearchoutcomes[Banerjee
etal.,2019]. Thisenhancementisachievedthroughtheidentificationofrelevantnamedentitieswithinboth
searchqueriesandtheobtainedresults.
2AsurveyonrecentadvancesinNER
Figure2: MainapproachestoNER.
• Documentsummarization: TheintegrationofNERintodocumentsummarizationprocessescanenhancethe
qualityandrelevanceofgeneratedsummaries. Byidentifyingandcategorizingkeyentities,NERcanensure
thatthesummarycapturestheessentialinformationrelatedtotheseentities[Rohaetal.,2023].
• Socialmediamonitoring: Integratedwithsocialmediamonitoring,NERempowersbusinessestoautomati-
callyidentifyandcategorizementionsofentitieslikebrandsandpersons. Thisaidsintrackingbrandvisibility,
sentimentanalysis,competitorinsights,andcrisismanagement. NERalsohelpsspottrends,assesscampaign
effectiveness,andleverageinfluencermarketing[Sufietal.,2022].
• Virtualassistants: Byidentifyingentitieswithinuserinput,NERenablesvirtualassistantstocomprehend
contextandoffertailoredresponses[Parketal.,2023]. Forexample,whenauserinquiresabouttheweather
ataspecificlocation,NERcanextractthelocationentity,allowingthevirtualassistanttofurnisharelevant
response.
• Namedentitydisambiguation: NERcanhelpdisambiguateentitieswiththesamename[Al-Qawasmeh
etal.,2016]. Forexample,"Apple"canrefertothecompanyorthefruit,andNERcandeterminethecorrect
interpretationbasedonthesurroundingcontext.
• Questionanswering: NERcanplayaroleinansweringquestionsthatrequirepinpointingparticularentities
[Molláetal.,2006]. Forexample,inaquestionlike"WhenSteveJobsdied?",NERcanidentify"stevejobs"
asapersonandextractthedeathdate.
• Languagetranslation: NERcanhelpimprovetheaccuracyofmachinetranslationbypreservingthenamed
entities[Lietal.,2020b].
4 Methods
Inthissection,weexplorevariousapproachesthathavebeenemployedforNER.Anoverviewoftheseapproachesis
presentedinFigure2.
4.1 Knowledge-basedmethods
Knowledge-based approaches emerged in the field of linguistics. For instance, Borkowski [1966] introduced an
algorithm that employs rule-based lists, utilizing indicators like capital letters to detect company names. Lexical
markersareelementsthatadjointhenamedentityand,insodoing,revealitspresence(suchasMr. orMs. infrontof
3AsurveyonrecentadvancesinNER
names). Herewemaycite,forinstance,CasEN transducercascade[Maureletal.,2011],createdtorecognizeFrench
namedentities.
Knowledge-basedmethodsdonotrequireannotateddataandprimarilydependonrulesanddictionariesrelyingon
lexicalresourcesanddomain-specificknowledge[SekineandNobata,2004,Etzionietal.,2005a],asdepictedinFigure
3. Theyconsistofthreecomponents: (1)asetofrulestoextractnamedentities,(2)anoptionalsetofentitydictionaries,
and(3)anextractionenginethatappliescomponents(1)and(2)totheinputtext. Theseapproachesrequiredomain
experts to provide a large number of rules using lexical markers [Zhang and Elhadad, 2013] or entity dictionaries
[Etzionietal.,2005a,SekineandNobata,2004].
Entitydictionariescompilethemostcommonlyoccurringentitieswithinaspecificapplicationdomain. Forexample,
theProMinersystem[Hanischetal.,2005]addressesthechallengeofidentifyinggeneandproteinnamesinbiomedical
text, which is complicated due the presence of synonyms and ambiguous terms. ProMiner employs a rule-based
approachandasynonymdictionariestodetectpotentialnameoccurrencesinbiomedicaltext. Itassociatesprotein
andgenedatabaseidentifierswiththesematches,focusingonmulti-wordnames. Thesystemhasbeenextendedto
handlehighlyambiguousandcase-sensitivesynonymsusingspecificdetectionprocedures. Themostlikelydatabase
identifiersarelinkedtothedetectedsynonymsinanabstract. Thesystem’sperformancewastestedintheBioCreAtIvE
competition,achievingpromisingresults.
Figure3: Thearchitectureofknowledge-basedmethodsforNER.
Quimbaya et al. [2016] introduced a method centered around entity dictionaries in electronic health records. The
experimentalfindingsdemonstratethatthisapproachenhancestherecallscorewhileminimallyaffectingtheprecision
score. Notethatforthisfamilyofmethods,precision(seeSection7.2)isgenerallyhighwhilerecallislowdueto
domain,language-specificrulesandincompletedictionaries. Moreover,incorporatingnewrulesanddictionariesis
costly.
4.2 Featureengineering-basedmethods
Insteadofmanuallycraftingsetsofrules,featureengineering-basedmethodsaregroundedintheprincipleofautomating
theextractionofnamedentitiesfromdata. Suchtechniquescanbedividedintothreeprimarygroups: unsupervised
learningmethods,supervisedlearningmethods,andsemi-supervisedlearningmethods.
4.2.1 Unsupervisedlearningmethods
Unsupervisedlearningmethodsdonotrequireanydataannotationbeforehand. Differentapproachesexistandare
mostlybasedonsimilaritiesinthedata. Thekeyideabehindthesemethodsistogroupsyntagmastogetherbasedon
theirsharedproperties.
Distributionsofwordsareharnessedin[ShinyamaandSekine,2004a]inordertoextractnamedentities. Theauthors
of this article capitalized on the observation that named entities tend to appear together in multiple news articles,
unlikecommonnames. In[Bonnefoyetal.,2011],theauthorscomputeasemanticproximityscorebycomparing
worddistributionsindocumentslinkedtoanentityandtheentitytype. ShinyamaandSekine[2004b]observe(like
[ShinyamaandSekine,2004a])thatnamedentitiesoftenappearsynchronouslyindifferentnewsreports, whereas
commonnounsdonot.
4AsurveyonrecentadvancesinNER
Nadeauetal.[2006]introducedanunsupervisedsystemforbuildingdictionariesandresolvingtheambiguityofnamed
entities. Inspired by previous works such as [Etzioni et al., 2005b], their approach involves the amalgamation of
extracteddictionarieswithpubliclyaccessibleones,resultingincommendableperformance.
4.2.2 Semi-supervisedlearningmethods
Semi-supervisedlearningmethodsleverageacombinationoflabeledandunlabeleddatatoimprovemodelperformance.
Unliketraditionalsupervisedapproachesthatrelyonlabeleddata,semi-supervisedmethodsusetheadditionalinforma-
tionavailableinunlabeleddatatoimprovetheNERsystem’sperformance. Theylearnfromthesmalldatasetusinga
setofrulesdesignedtolearnextractionpatternsbasedonasetofrelevantmarkers. Theythenattempttofindother
samplesofnamedentitiesadjoinedbythesemarkers. Thislearningprocessisthenappliedtothenewsamplesinorder
todiscovernewcontextualmarkers. Byrepeatingthisprocess,alargenumberofentitiesshouldbefound. Collinsand
Singer[1999]showedthatameresetofsevensimplerules,coupledwithunlabeleddata,canbeadequateforachieving
effectivesupervision.
Anapproachthatiscommonlyadoptedinsemi-supervisedNERconsistsinusingco-training[Kozarevaetal.,2005],
wheremultipleclassifiersaretrainedondifferentdataviews,andtheyiterativelylabelunlabeledinstancestobuilda
morerobustmodel. Anothermethodisself-training[Gaoetal.,2021],wheretheinitialmodelistrainedonthelabeled
data,andthenusedtopredictthelabelsfortheunlabeleddata. Themostconfidentpredictionsareaddedtothelabeled
data,andtheprocessisiterated.
Tosumup,semi-supervisedlearningmethodsofferseveralbenefits,suchasreducingthedependencyonmanually
labeleddatasetsandimprovingperformanceinlow-resourcescenarios. Nonetheless,theyalsocomewithchallenges,
includingthepotentialerrorpropagationfromtheinitiallabeleddataandtheneedforcarefulhandlingofnoisyor
incorrectpredictionsfromtheunlabeleddata. However,whenappliedwisely,semi-supervisedlearningtechniques
cansignificantlyenhanceNERperformanceandaddressreal-worldchallengeswherelabeleddatamightbelimitedor
expensivetoobtain.
4.2.3 Supervisedlearningmethods
Supervisedlearningmodelsinvolvelearningrulesfromannotateddata,wherehumaninterventionisrequiredtolabela
setofsamples. Theselabeledsamplesserveasguidanceforthemodelduringthelearningprocess. Alearningmethodis
thenusedtotrainthemodeltorecognizethedistinctivecharacteristicsofthenamedentities. Subsequently,thelearning
systemgeneralizesthisknowledgetoproduceamodelcapableofextractingnamedentitiesfromnewdocuments. The
effectivenessofsuchmethodimprovesinlinewiththequantityandqualityoftheannotateddatausedfortraining.
Usingthem,theNERtaskcanbeviewedastwosub-tasks,namelyclassificationandsequencelabeling. Theaimisto
reproduceanannotationschemelearnedfromlabeleddata. Themostcommonalgorithmsormodelsinthiscategory
aretheHiddenMarkovModel(HMM)[Morwaletal.,2012,Bikeletal.,1999],theMaximumEntropymodel(ME)
[Borthwick,1999,Linetal.,2004],SupportVectorMachines(SVM)[IsozakiandKazawa,2002,Makinoetal.,2002],
andConditionalRandomField(CRF)[McCallumandLi,2003,Settles,2004a].
Hidden Markon Model An HMM is a statistical model in which the system being modeled is assumed to be a
Markovprocess. Inourcontext,itisusedtoidentifyandclassifynamedentitieswithinasequenceoftokens. Withthis
model,theobservedtokensrepresentthevisiblestates,whilethedifferententitylabelsaretreatedashiddenstates. The
modelassumesthattheobservedtokensdependonlyonthecurrenthiddenstate,andcanbeusedtoinferthemostlikely
sequenceofnamedentitylabelsgiventheobservedtokens. AnHMMismathematicallydescribedbyfiveparametersas
follows:
HMM={S,O,π,Tr,Em} (1)
where S refers to the number of hidden states (entity labels), O to the number of observations (tokens), π is the
distributionofinitialstateprobabilities,Tristhetransitionprobabilitymatrix,andEmistheemissionprobability
matrix.
TheNERproblemcanbetranslatedintoanHMMproblemanddefinedas
P(S|O)=P(EN|T),
whichmeansthatgivenasequenceoftokensT,theprobabilityofobtainingthesequenceofnamedentities(EN)given
T isequivalenttocalculatingtheprobabilityofobtainingthesequenceofhiddenstatesS giventheobservationsO. ?
proposedIdentiFinder,afirstHMMthatlearnstorecognizeandclassifynames,dates,times,andnumericalquantities.
Otherworkshavefollowedthispath[Morwaletal.,2012,Zhao,2004].
5AsurveyonrecentadvancesinNER
MaximimEntropy MEmodelsrepresentastatisticalmodelingapproachemployedinvariousNLPtasks,including
NER.Thefundamentalconceptunderlyingthesemodelsistoidentifyaprobabilitydistributionthatincludespotential
outcomes,optimizingentropywhileadheringtoapredefinedsetofobservedconstraints. Theresultingprobability
distributionhasthehighestentropy,isdistinctive,alignswiththemaximum-likelihooddistribution,andtakesonthe
form:
k
p(o|h)= 1 (cid:89) αfj(h,o) (2)
Z(h) j
j=1
whereoreferstotheoutcome,hthecontext,andZ(h)isanormalizationfunction. Theconstraintsareusuallyderived
fromthetrainingdata,andthemodelaimstoassignhigherprobabilitiestooutcomesthataremorelikelygiventhe
observeddata.
InthecontextofNER,aMEmodelcouldbetrainedtopredictthenamedentitylabelofatokenbasedonitssurrounding
contextandotherrelevantfeatures. Thefeaturesmayincludeinformationaboutthecurrenttoken,itsneighboring
tokens,POStags,andmore. Themodellearnstheweightsassociatedwiththesefeaturesduringtraining. In[Borthwick
etal.,1998],theauthorsproposedasystemcalledMaximumEntropyNamedEntity(MENE)byapplyingtheME
modelandusingaflexibleobject-basedarchitecture,whichallowsittomakeuseofabroadrangeofknowledgesources
inmakingitstaggingdecisions.
Conditional Random Fields CRFs are models used for labeling sequences, like sentences. They consider that
neighboringitemscanaffecteachother’slabels,makingthemgoodforNER.Theycapitalizeontheirabilitytomodel
sequentialdependenciesbetweentokenswithinasequence,makingthemparticularlyadeptatcapturingtheintricate
contextualrelationshipsthatdefinenamedentities. Byintegratingbothlocalandglobalinformation,theycanpredict
entitylabelsbyconsideringthelabelsofneighboringtokens,effectivelyreducinglabelingambiguity. ACRFmodelis
definedasfollows:
T K
1 (cid:110)(cid:88)(cid:88) (cid:111)
P(Y|X)= exp λ f (y ,y ,x,t) (3)
Z k k t−1 t
0
t=1k=1
where Z is the normalization factor for all possible sequences of states (labels), f are feature functions, each
0 k
representingtheoccurrenceofaspecificcombinationofobservationsandassociatedlabels,y isthelabelofthe
t−1
previousword,y isthelabelofthecurrentword,x isthewordatpositiontintheobservedsequence. λ arethe
t t k
modelparametersandcanbeinterpretedastheimportanceorreliabilityoftheinformationprovidedbythebinary
function.
TheNERproblemcanbeconvertedintoaCRF,wheretheobservationsaretheprocessedstringsandthelabelsare
thepossiblenamedentities. Thus,findingthebestsequenceofnamedentitiescorrespondstoasequenceoftokensin
NER,whichisequivalenttofindingthebestsequenceoflabels,i.e.,argmaxP(Y =y|X =x). Shishtlaetal.[2008]
implementedasystemthatextractsinformationfromresearcharticlesusingCRF.Theyinvestigatedregularization
problemsusingGaussianmodelandfocusedontheefficientuseoffeaturespacewithCRF.Settles[Settles,2004b]
presentedaframeworkforrecognizingbiomedicalentitiesusingCRFwithavarietyoffeatures. HeshowedthataCRF
withonlysimpleorthographicfeaturescanachievegoodperformance.
SupportVectorMachine SVMsareaclassofmachinelearningalgorithmscommonlyusedforclassificationtasks.
WhileSVMsarenotaswidelyusedforNERassomeotherapproacheslikeCRFmodels,theycanstillbeapplied
effectivelywithappropriatefeatureengineeringandconsiderations. Yamadaetal.[2002]introducedaSVM-based
NERsystemforJapanese. TheirapproachisanextensionofKudo’schunkingsystem[KudoandMatsumoto,2001]. In
thissystem,eachwordinasentenceisclassifiedsequentiallyeitherfromthebeginningortheendofthesentence.
4.3 Deeplearning-basedmethods
ThelandscapeofNERanddeeplearninghasfurtherevolved,withthemostprevalentmethodologiesencompassing
CNNs,RNNs,andtheirhybridcombinations. Additionally,thesedeeplearningarchitecturesarefrequentlycombined
withothersupervisedlearningalgorithmssuchasSVMsorCRFs.Collobert[2011]playedapioneeringrolebyadopting
deeplearningtechniquesforNER.TheirworkshowcasedtheapplicationofaCNNnotonlyforNERbutalsoforother
NLPtaskssuchasSemanticRoleLabeling,POStagging,andchunking. Ofsignificantimportance,thesetaskswere
accomplishedautomatically,breakingawayfromtheneedformanuallyengineeredfeatures. Thismarkedapivotal
momentasdeepneuralnetworkswereharnessedforthesetasksforthefirsttime.
Theoverarchingstructureofdeeplearning-basedapproachesusuallyencompassesthreefundamentalstages(referto
Figure4): first,therepresentationofdata;followedbycontextencoding;andconcludingwithentitydecoding. These
stageswillbeelaborateduponinthesubsequentsections.
6AsurveyonrecentadvancesinNER
Figure4: Generalstepsforadeeplearning-basedNERmodel.
4.3.1 Datarepresentation
NERrequiresdatatoberepresentedinamachine-understandableform. Textualembeddingscanbeusedtosatisfythis
requirement. Severaltextualembeddingtechniquesexist,whichweseparateintothetwocategoriesdescribedbelow:
wordembeddingsandcharacterembeddings.
Wordembeddingsaretypeofwordrepresentationsthatconvertwordsintovectorrepresentationsinahigh-dimensional
space. Theserepresentationscapturethesemanticmeaningandcontextofwordsbasedontheirsurroundingwordsina
givencorpus. WorthyofmentionareOne-Hotencoding,TF-IDF[Ramosetal.,2003],Word2vec[Mikolovetal.,2013],
GloVe[Penningtonetal.,2014],fastText[Bojanowskietal.,2016]andmorerecentlyGPT[Radfordetal.,2018].
Traditionalwordrepresentations,likeone-hotencodings,representeachwordasasparsebinaryvector,whereonlyone
elementissetto1toindicatethepresenceofthatwordinthevocabulary. InNER,theuseofonehotencodinginvolves
automaticallyconstructingtrainingdata,suchasfromsocialmediamessagingapplications[LeeandKo,2020].
TheTF-IDFweightisusedtomeasuretheimportanceofawordinadocumentrelativetoacorpus.TheTermFrequency
(TF)componentassesseshowoftenawordappearsinaspecificdocument,whiletheInverseDocumentFrequency
(IDF)componentmeasurestherarityofawordacrosstheentirecorpus. BymultiplyingTFandIDF,TF-IDFhighlights
significantwordsinadocument. TF-IDFcanbeemployedinNERasafeaturerepresentationmethod[Karaa,2011].
Asnamedentitiesareoftenrarewords,usingTF-IDF,namedentitiesthatappearfrequentlyinaspecificdocumentbut
rarelyacrossthecorpusareassignedhigherTF-IDFscores,makingthemstandoutinthefeaturevectors.
Furthermore,wordembeddingsaretypicallylearnedusingunsupervisedlearningtechniqueslikeWord2Vec,GloVe,
andfastText. Thesemethodsprocesslargeamountsoftextdataandlearnwordrepresentationsthatcapturesemantic
similaritiesandsyntacticrelationshipsbetweenwords. Severalworks,including[Collobertetal.,2011,Huangetal.,
2015],usewordembeddingsforNER.MaandHovy[2016],forinstance,comparedtheperformanceoftheirmodel
usingavarietyofwordembeddingsincludingWord2vecandGlove. Theroleplayedbytheseembeddingsinobtaining
goodresultsishighlightedin[Lampleetal.,2016].
Finally,notethatwordembeddingsmayalsobecombined,asin[Dadas,2019],whereaknowledgebasefromWikipedia
isusedtoannotatenamedentities. Intheworkcited,thelabelsaretransformedintoone-hotvectorsandconcatenated
withWord2VecorELMo[Petersetal.,2018a]wordembeddings.
Characterembeddingsaretypeofwordrepresentationsthatcapturetheinternalstructureofwordsatthecharacter
level. Insteadofrepresentingwordsasvectorsliketraditionalwordembeddings,characterembeddingsrepresenteach
wordasasequenceofvectorsrepresentingitsconstituentcharacters. Theprocessofcreatingcharacterembeddings
involvesbreakingdowneachwordintoitscharactersandthenrepresentingeachcharacterasavector. Thesevectorsare
combinedtoformtherepresentationoftheword,asillustratedinFigure5throughtheutilizationofaCNNarchitecture.
7AsurveyonrecentadvancesinNER
Figure5: AnexampleofcharacterembeddingsusingaCNN).
Spellingvariationsareofgreatimportanceandrevealthepresenceofnamedentities. Representingindividualcharacters
can capture spelling variations by highlighting the syntax of words and their morphology. It can also generate
representationsofwordsthatwerenotencounteredduringthelearningphase,i.e. outofvocabulary,bycombining
differentcharactervectorsinordertorepresentaword.
Characterembeddingsaregeneratedusingvariousmethods. Asimpleapproachisone-hotencoding,wherecharacters
arerepresentedasbinaryvectorswitha1atthecorrespondingindex. Character2VecextendsWord2Vectothecharacter
level,predictingembeddingsbyconsideringneighboringcharacters. CNN-basedcharacterembeddingstreatcharacter
sequencesasimages,extractingmeaningfulfeaturesthroughCNNfilters. BiLSTM-basedcharacterembeddingsuse
bidirectionalLSTMnetworkstocapturecontextualinformationfromcharactersequences.
NotethattheCNNsandRNNswedescribeindetailintheupcomingsection,areusedin[MaandHovy,2016,Peters
etal.,2017,2018b]tocomputeafeaturevectorforeachword. Oneofthefindingsin[Lampleetal.,2016]isthat
recurrentmodelstendtofavorlaterentriesandthattheresultingfeaturevectorsencodesuffixesmorethantheyencode
prefixes. Forthisreason,theauthorsofthecitedworkrecommendBi-LSTMtobettercapturetheprefix.
4.3.2 Contextencoding
Toimproveentityrecognition,contextencodingisacrucialaspectofNERsystemsaimingatcapturingthecontextual
informationofwordsinasentence. Contextencodingtechniquesfocusonrepresentingwordsintheirsurrounding
contexttotakeintoaccountthedependenciesandrelationshipsbetweenneighboringwords. Itemploysfeaturevectors
obtainedusingtextualembeddings,whichcanbegroundedinwords,characters,orevenacombinationofboth. Inthe
sequel,wepresentpopularcontextencoderarchitecturessuchasCNNandRNN.Othermodelsliketransformerswill
bepresentedseparatelyindedicatedsections.
TheCNNmodelshavebeendevotedinitiallyforimages[O’SheaandNash,2015]. Theyusefilterstodetectpatterns
andfeatures. TheiruseisextendedtosuccessfullytoNLPtaskslikeNER,wheretheinputtextistypicallyrepresented
asasequenceofwordembeddingsorcharacters. CNNisemployedtocapturecontextualinformationandlocalpatterns.
Theconvolutionallayersslideovertheinputsequence,andmultiplefiltersareusedtodetectdifferentlinguisticpatterns
atmanypositions. Thesefilterslearntorecognizecommonpatterns,suchassuffixes,prefixes,andcombinationsof
wordsthatareindicativeofnamedentities.
SeveralauthorshaveusedCNNforNERtask. Collobertetal.[2011]proposedasentence-basednetworkfortagging
words, considering the whole sentence. Each word is represented as a vector, and a convolutional layer generates
local features around each word. The global feature vector is formed by combining these local features, with a
fixeddimensionregardlessofsentencelength. Globalfeaturesarethenpassedtoatagdecoderforpredictingtags
(c.f. Section 4.3.3). In [Gui et al., 2019], the authors proposed an approach for Chinese NER using a CNN with
lexiconsandarethinkingmechanism[Lietal.,2018]. Ratherthanrelyingonasingledatapassforfinaldecisions,the
rethinkingmechanismintroducesfeedbackconnections. Theseconnectionsenablethenetworktoreevaluatedecisions
byintegratinghigh-levelfeedbackintofeatureextraction. Theauthorsshowedthatthismethodcanmodelcharacters
andpotentialwordsinparallel,andtherethinkingmechanismcanresolvewordconflictsbyrefininghigh-levelfeatures
iteratively. ForbiomedicalNERtasks,Zhuetal.[2018]proposedadeeplearningapproachcalledGRAM-CNN.This
8AsurveyonrecentadvancesinNER
methodutilizeslocalcontextsbasedonn-gramcharacterandwordembeddingsthroughCNN.Byleveraginglocal
informationaroundaword,GRAM-CNNcanautomaticallylabelwordswithoutrequiringspecificknowledgeorfeature
engineering.
Ontheotherhand,RNNhavealsobeenwidelyusedforNERtasks. Theyaredesignedtoprocesssequentialdata,
makingthemwell-suitedfortaskswherethecontextofeachwordiscrucialforaccuratelabeling[Sherstinsky,2020].
InthecontextofNER,theinputtextisrepresentedasasequenceofembeddings,andeachwordisfedintotheRNN
oneatatime. TheRNNmaintainsahiddenstatethatcapturesinformationfromthepreviouswords. Thishiddenstateis
updatedateachtimestepastheRNNprocesseseachword,allowingittorememberimportantinformation. Oneofthe
challengeswithRNNisthevanishinggradientproblem,whichlimitstheirabilitytocapturelong-termdependencies.
Toaddressthisissue,variantsofRNNlikeLongShort-TermMemory(LSTM)[Sherstinsky,2020]andGatedRecurrent
Unit(GRU)[Chungetal.,2014]havebeenintroduced. LSTMandGRUnetworksusegatingmechanismsthathelp
themretainandupdateinformationmoreeffectivelyoverlongsequences,makingthembettersuitedforNERtasks.
Thereby,Huangetal.[2015]proposedaLSTMmodelforNERanddemonstratedthatincorporatingaCRFlayerasa
tagdecoderenhancedperformance. Inotherdomains,similarsystemswereappliedbyChalapathyetal.[2016]for
DrugNER,andZhangandYang[2018]forChineseNER.RNNbasedonaBi-LSTMarchitecturewereusedin[Huang
etal.,2015];thistechniquehassubsequentlybeenusedbyotherresearchers[MaandHovy,2016,Lampleetal.,2016].
4.3.3 Entitydecoding
TagdecoderarchitecturesplayavitalroleinNERsystemsforpredictingentitylabelsforeachworkinagivensequence.
Severaltagdecoderarchitectureshavebeenproposed,eachwithitsstrengthsandsuitabilityfordifferentNERscenarios.
SomecommontagdecoderarchitecturesforNERinclude: CRF,Multi-LayerPerceptron(MLP),andPointerNetworks
[Vinyals et al., 2015]. The research conducted by [Lample et al., 2016, Ma and Hovy, 2016], demonstrated that
employingCRFforentitydecodingyieldsimprovedoutcomesinNER.ManydeeplearningbasedNERmodelsusea
CRFlayerasthetagdecoderontopofanBiLSTM[Luoetal.,2018,Linetal.,2019]oraCNNlayer[Knobelreiter
etal.,2017,Fengetal.,2020].
NERistypicallyapproachedasasequencelabelingtask,wherethegoalistoassignentitytagstoindividualwords. One
simplemethodtoachievethisisbyusingaMLPwithaSoftmaxlayerasthetagdecoder. Thisformulationtransforms
thesequencelabelingtaskintoamulti-classclassificationproblem,whereeachtoken’stagisindependentlypredicted,
seeforinstance[Galloetal.,2008,Linetal.,2019].
Ontheotherhand,PointerNetworksareRNNdevotedfortaskswithlargeoutputspaces,likesequence-to-sequence
tasks. Thekeyideaisallowingthemodeltopointdirectlytoelementsintheinputsequenceratherthanrelyingona
fixedvocabulary. Thisflexibilityisbeneficialwhendealingwithoutputsequencescontainingitemsnotpresentinthe
trainingdataorwithunknownoutputlengths. Themodelemploysattentionmechanismstocomputesoftalignment
scoresbetweeninputandoutputelements,representingtheprobabilityofpointingtoeachinputitemforgenerating
specificoutputs. ThisapproachhasprovensuccessfulinmanyNLPtaskssuchasNER[Zhaietal.,2017].
4.4 Transformer-basedlanguagemethods
LanguagemodelsareakeycomponentofNLP.Theyledtodevelopmentofmethodsthatcanprocess,analyze,and
generatenaturaltexttrainedonahugedataset[OCDE,2023]. Themostrecentofthesemethodsarebuiltuponthe
transformerarchitecture.
Letusrememberthatatransformer[Vaswanietal.,2017]isaneuralnetworkpre-trainedonalargecorpusoftextsthat
aimstosolvetaskssequence-to-sequencewhileeasilyhandlinglong-distancedependencies. Ingeneral,thetransformer
modelisbasedontheencoder-decoderarchitecture. Thetaskoftheencoderistoencodetheoutputsequenceintoa
fixedlengthvectorcalledacontextvector,whichisthenfedintoadecoder. Thedecoderreadstheencoderoutputalong
withthedecoderoutputattheprevioustimestepstogenerateanoutputsequence. Figure6depictsthearchitectural
representationofatransformersimilartooneintheoriginalpaper[Vaswanietal.,2017]. Thereby,Transformer-based
modelsbelongtotwobroadfamilies: encoder-basedmodelsanddecoder-basedmodels. Inthefollowing,wefocus
onencoder-basedTransformermodels,andwillcovermodelsbasedondecodersoracombinationofboth,including
LLMs,inthenextsection.
Transformer-basedencodermodelshavehadasignificantimpactonNLP,includingonNER.Theycanbeusedeither
directly for NER as context encoder, or to provide inputs to other models such as CRF or LSTM. BERT [Devlin
etal.,2018],forinstance,isusedin[SchweterandBaiter,2019]ascharacterrepresentationsforGermanlanguage.
Otherauthors[Labuschetal.,2019b]demonstratedthatusingBERTasaclassifieroutperformsotherBi-LSTM-CRF
based methods, such as that proposed in [Riedl and Padó, 2018]. In [Boutalbi et al., 2022, Ait-Saada and Nadif,
2023a,b,NadifandRole,2021],theauthorshavestudiedtheperformanceofBERTinunsupervisedlearningcontext.
9AsurveyonrecentadvancesinNER
Figure6: TheTransformerarchitecture[Vaswanietal.,2017].
SeveralhybridizedworkscombiningBERTandRNN,suchasby[Liuetal.,2021a]. TheyrelyonBERTtoextract
theunderlyingfeaturesoftext,aBiLSTMtolearnacontextrepresentationofthetextandaMulti-HeadedAttention
Mechanism(MHATT)toextractchapter-levelfeatures.
Transformer-basedencodermodelsmaybeadaptedandimprovedinvariouswaystomakethemmoreefficientfor
thepurposesofNER.In[Yanetal.,2019]thetransformerencoderisused,notonlytoextractword-levelcontextual
information, butalsotoencodecharacter-levelinformationwithinaword. In[Choudhryetal.,2022], theauthors
proposedanapproachforFrenchlanguageusingadversarialadaptationtocounterthelackoflabelledNERdatasets.
Theytrainedthemodelsonlabeledsourcedatasetsandusedlargercorporafromotherdomainstoimprovefeature
learning.
SeveralvariantsofBERTsuchasdistilbert[Sanhetal.,2019]andRoberta[Liuetal.,2019a]haveproventobevery
effectiveandwidelyadoptedforNERtasks. DistilBERTisadistilledversionofBERT,createdthroughaknowledge
distillationprocess,whichretainsmuchofBERT’slanguageunderstandingcapabilitieswhilebeingsignificantlysmaller
insizeandfasterincomputation. Robertautilizesalargerpre-trainingcorpusandemploysanoveltrainingapproach
calledmaskedlanguagemodel. Thisenablesittocapturedeepercontextualembeddingsandrepresentations,makingit
exceptionallyskilledatunderstandingintricatelinguisticstructuresandcontext-dependentinformation.
ThepaperofAbadeer[2020]exploresthechallengesposedbythelargesizeandresource-intensivenatureofBERT
modelsindeployingthemtoproductionenvironments. Toaddressthisissue,theresearchersfine-tuneDistilBERTon
medicaltextfortheNERtaskinvolvingProtectedHealthInformation(PHI)andmedicalconcepts(MC).Notably,for
PHINER,DistilBERTachievesnearlyidenticalF1scorestomedicalversionofBERTwhileconsumingalmosthalfthe
runtimeanddiskspace. However,inthedetectionofMC,DistilBERT’sF1scoreislowerthanmedicalBERTvariant.
Theauthorsinthepaper[MehtaandVarma,2023]triedtosolveamultilingualcomplexNERchallengebyrefiningthe
basicXLM-Robertamodelondatasetsfromseveraldifferentlanguages.
4.5 Largelanguagemodel-basedmethods
LargeLanguageModels(LLMs)representanovelcategoryofdeeplearningmodelswiththecapabilityofperforminga
widearrayoftasks,includingtranslation,summarization,classification,andcontentgeneration. Thesemodels,encom-
passingtransformer-basedlanguagemodels,aredistinguishedbytheirextensiveparametercount,oftennumbering
inthetensorhundredsofbillions. Theyundergotrainingonsubstantialdatasets,suchasGPT[Brownetal.,2020],
BloomZ[Muennighoffetal.,2022],andLlaMA[Touvronetal.,2023].
10AsurveyonrecentadvancesinNER
Table1: SummaryofstudiesonLLMsinNER
Study Approach Context Outcome
GPT-NER [Wang Transforms se- GeneralNERtasks Comparable to fully supervised base-
etal.,2023] quence labeling to lines, better in low-resource and few-
textgeneration shotsetups
PromptNER[Ashok Usesentitytypedef- Few-shot learning State-of-the-art performance on few-
andLipton,2023] initionsforfew-shot inNER shotNER,significantimprovementson
learning variousdatasets
ChatGPT Evalua- EvaluatesChatGPT VariousNERtasks Impressiveinseveraltasks,butfarfrom
tion [Laskar et al., on various NER solvingmanychallengingtasks
2023] tasks
Injecting compari- Compares prop- Task-Oriented Dia- Effectively addresses ambiguity han-
son skills in TOD erties of multiple logueSystems dlingindatabasesearchresults
Systems[Kimetal., entities
2023]
Zero-Shot on his- Explores zero-shot Historical texts in Showspotentialforhistoricallanguages
torical texts with abilitiesforNER multiplelanguages lackinglabeleddatasets,error-pronein
T0 [De Toni et al., naiveapproach
2022]
ResolvingECCNPs Proposes a gen- German medical Outperformsrule-basedbaseline
[Kammer et al., erative encoder- texts
2023] decoder Trans-
former
Large code gener- Utilizes generative Information extrac- Consistently outperforms fine-tuning
ation models [Li LLMs of code for tiontasks moderate-size models and prompting
etal.,2023] InformationExtrac- NL-LLMsinfew-shotsettings
tiontasks
ThebasisofLLMsistobefoundinthetransformerdecodermodel,wheremultipleattentionlayersarestackedto
createahighlycomplexneuralnetwork. Thearchitecturesandpre-trainingobjectivesemployedinexistingLLMs
closelyresemblethoseofsmallerlanguagemodels. Thedifferenceliesinthesignificantlyincreasedsizeofboththe
modelandthetrainingdata. SomeLLMs,likeT5[Raffeletal.,2020],arehybrids,utilizingbothencoderanddecoder
componentsoftheTransformerarchitecturetoenhanceunderstandingandgenerationcapabilities,allowingformore
versatileandnuancedlanguageprocessing.
ThepopularityofLLMsisduetotheiroutstandingperformanceacrossvariousNLPtasks. Theyexcelintasksliketext
classification[Hegselmannetal.,2023],questionanswering[Robinsonetal.,2022](QA),textgeneration[Muennighoff
etal.,2022],andmachinetranslation[Hendyetal.,2023]. However,despitetheirimpressiveperformanceinnumerous
NLPtasks,LLMshaveshownsomelimitationswhenappliedtoNER.ThisisprimarilybecauseNERisasequence
labelingtask,whileLLMsareoriginallydesignedfortextgeneration.
ToaddressthegapbetweenLLMsandNER,Wangetal.[2023]introducedamethodknownasGPT-NER,which
effectivelytransformsthesequencelabelingtaskintoatextgenerationtask.WithGPT-NER,thetransformationinvolves
representingtheNERtaskasatextgenerationproblem. Forinstance,thetaskofidentifyinglocationentitiesinthe
sentence"Parisisacity"istransformedbyGPT-NERintothetaskofgeneratingthetextsequence"@@Paris##is
acity,"wherethespecialtokens"@@"and"##"areusedtomarktheboundariesofnamedentities. Inaseriesof
experimentsGPT-NERwasabletoproducepromisingresultscomparabletosupervisedapproacheswhenappliedto
mediumorlargedatasets,evenoutperformingsupervisedapproacheswhendealingwithsmalldatasets. Thisinnovative
approachsuccessfullybridgesthegapbetweenLLMsandsequencelabelingtasks,makingthemmoresuitableforNER
andshowingtheirpotentialforimpressiveperformanceinthisarea.
AshokandLipton[2023]proposedPromptNER,afew-shotlearningalgorithmforNER.PromptNERrequiresasetof
entitytypedefinitionsinadditiontoannotatedexamples. Givenasentence,PromptNERpromptsanLLMtoproducea
listofentitieswithcorrespondingexplanationsjustifyingtheircompatibilitywiththeentitytypedefinitionsprovided.
TheresultsshowthatPromptNERoutperformsotherfew-shotandcross-domainmethodsoncertaindatasets.
Laskaretal.[2023]conductedanevaluationofChatGPTonvariousNERtasks,providinginsightsintothestrengths
andlimitationsofLLMsinthisarea. ThemixedresultshighlighttheimportanceofcontextualizingtheuseofLLMs
accordingtothespecificitiesofthetask. Similarly,Huetal.[2023]investigatedChatGPTforclinicalentityrecognition
inazero-shotcontext. ThestudycomparestheperformanceofChatGPTagainstthatofGPT-3[Brownetal.,2020]and
aBioClinicalBERT[Alsentzeretal.,2019]modelspecificallytrainedonsyntheticclinicalnotesfromMTSamples.
11AsurveyonrecentadvancesinNER
ChatGPTwasshowntooutperformGPT-3inthisparticularsetting. However,ChatGPTperformslesswellthanthe
supervisedBioClinicalBERTmodel.
Kimetal.[2023]exploredthecombineduseofGPT-2[Radfordetal.,2019]andBERTfordisambiguatingnamed
entitiesindialoguesystems,illustratingtheeffectivenessofahybridapproach. GPT-2servesasageneratorduring
thetrainingphase,andinputsentencesandtargetentitiesareusedtogether. Duringinference,inputsentencesguide
theevaluationtoidentifywhetherthetargetentitycanbematchedexactly. WhiletheaccuracyoftheGPT-2-based
approachisslightlylowerthanthatofBERT,itillustratesthepotentialofcombiningLLMsforspecificNERtasks.
SeveralstudieshavelookedathowLLMsmaybeappliedinspecificcontexts. Forinstance, DeTonietal.[2022]
testedthemultitaskT0modelonhistoricaltexts,highlightingthechallengesassociatedwithNERinhistoricaland
multilingualcontexts. Theyfoundthatthemodelstruggleswithaccuratezero-shotNERonhistoricaldocumentsdueto
variousfactors,includingopticalcharacterrecognitionerrorsandlinguisticvariation. TheaccuracyofT0’spredictions
varieddependingonthelanguageofdocumentsandonthehistoricalperiodinwhichtheyoriginated. Whilethemodel
underperformedcomparedtothestateoftheartinNERforhistoricaldocuments,itdemonstratedaparticularabilityto
identifythelanguageandpublicationdateofthedocuments.
Incontrast,Kammeretal.[2023]focusedontheuniquechallengespresentedbyellipticalcompoundnominalphrases
inGermanmedicaltexts. Ellipticalcompoundnominalphrasesrepresentadistinctivelinguisticconstructionwhere
certainelementsofsentencesorcoordinatedsegmentsareomittedtoavoidrepetition. Forinstance,thephrase"Vitamin
C,VitaminE,andVitaminA"iscondensedto"VitaminC,E,andA". Thesephrasespresentchallengesforentity
extractionanddisambiguation. Toaddressthisissue,adatasetofover4,000manuallyannotatedECCNPs(Elliptical
CompoundCoordinateNominalPhrases)wascreated,andagenerativemodeldevelopedbasedontheTransformer
architecture,whichincludesbothencoderanddecodermodules. Thismodeldemonstratedahighaccuracy,significantly
outperformingarule-basedmethod[AepliandVolk,2013]. LargeLanguageModels(LLMs)likeGPT-3.5,however,
performedlesswell.
Lietal.[2023]introducedanovelapproachforimprovingLLMperformanceininformationextraction(IE),especially
forNERandrelationshipextraction(RE),byadvocatingtheuseofcode-basedLLMs(Code-LLMs)suchasCodex
[Chenetal.,2021a]ratherthannaturallanguageLLMs(NL-LLMs)suchasGPT-3. Thisalternativeapproachinvolves
reformulatingIEtasksascodegenerationtasks,aligningstructuredoutputsmoreefficientlywithIErequirements. A
keyfindingisthatCode-LLMswithcode-stylepromptscanbebetterthanspeciallypre-trainedmodelsforIEand
NL-LLMsinfew-shotlearningsetups. Additionally,code-stylepromptsyieldoutputswithlowerstructuralerrorrates,
demonstratingthepotentialofCode-LLMsforspecificsophisticatedapplicationsinIE.
DespitetherecententhusiasmforLLMs,itmustberememberedthatthesemodelsremaincostly. Indeed,themain
drawbackofLLMsistheconsiderablecostoftrainingthem,whichfornowisamajorimpedimenttotheirwidespread
adoption. The extensive computational resources and time required for training, given the enormous number of
parametersinthesemodels,maketheprocessexpensiveandresource-intensive. Asaresult,LLMsarepredominantly
restrictedtolargeorganizationsandresearchinstitutionswithaccesstosubstantialcomputingpower.However,although
thehightrainingcostlimitsthewidespreadavailabilityandaccessibilityofLLMs,effortsareunderwaytoaddressthese
challengestomakethemmoreefficientandmorecost-effective.
4.6 Graph-basedmethods
TheworkofMarcheggianiandTitov[2017]pavedthewayfortheuseofGraphConvolutionalNetworks(GCNs)[Kipf
andWelling,2016]inNLP.GCN-basedmethodsconsidereachtokenasanodeofagraph,withtheedgesrepresenting
contextuallinkstoneighboringnodes(refertoFigure7). Amoredetailedexplanationofhowthesemethodswork
istobefoundin[Daigavaneetal.,2021]. Theauthorsof[Cetolietal.,2017]lookedathowGCNmaybeusedin
addressingtheNERtask,andtheresultsthattheyobtainedontheOntoNotes5.0dataset[Weischedeletal.,2013]show
asignificantimprovementinperformance. Liuetal. [Liuetal.,2019b]proposedanarchitecturebasedonGCNto
creategraphembeddings. Thesegraphembeddingsarethencombinedwithtokenembeddings,andpassedthrougha
Bi-LSTM-CRF network to extract named entities. Harrando and Troncy [2021] suggested that NER, traditionally
consideredasasequence-labelingproblem,maybeconsideredasagraphclassificationproblem,whereeachtokenis
representedasanode. Eachnodecanthenbeassociatedwithcontextualinformationandotherelementsofexternal
knowledge,suchasmorphologicalshapeoraPOStag. Thisapproachhasshownpromisingresults.
12AsurveyonrecentadvancesinNER
Figure7: Leftside: Conventionalmodelforsequencetagging. Rightside: Everywordwithinasentencetransforms
intoanodeofagraphconnectedtosurroundingwordsandadditionalfeatureslikegrammaticalcharacteristics. This
graphissubsequentlyencodedandfedintoaclassifiertopredictentitytags.
5 Low-resourceNER
RecentmethodsbasedonneuralnetworksandtransformershaveshowngoodresultsforNER.Thesemethodsneed
tobetrainedonasufficientquantityofdata;wherethereisascarcityofdata,thishasaseverenegativeimpacton
performance. However,labeleddataissometimesdifficulttoobtain. Inthissection,wepresentsomewaysinwhich
thisproblemmaybeaddressed.
5.1 Transferlearning
Transferlearninginvolvestakingknowledgethathasbeenobtainedwhilesolvingonetask,andapplyingthisknowledge
toarelatedtask[Zhuangetal.,2020]. Thistechniquehasbeenshowntoimprovemodelperformanceinareasincluding
imageclassification[ShahaandPawar,2018],speechrecognition[WangandZheng,2015],andtimeseriesclassification
[Fawazetal.,2018].
TransferlearninginNERinvolvespretrainingamodelonalargeamountofgeneraltextdataandthenfine-tuningiton
asmallerdatasetspecificallyforthetargetNERtask. Thisapproachleveragestheknowledgelearnedfromthegeneral
datatoimproveperformanceonthespecifictask. Anumberofstudieshavebeencarriedoutontransferlearning. Lee
etal.[2017]lookedattransferlearningusingRNNintheanonymizationofhealthdata. Authorslike[Francisetal.,
2019, Liu et al., 2021b] explored the use of transformers for NER, and showed that transfer learning significantly
improvesperformance. Recently,Fabregatetal.[2023]proposedseveralarchitecturesbasedonaBi-LSTMandaCRF
inordertodetectbiomedicalnamedentities.
5.2 Dataaugmentation
Dataaugmentationartificiallyincreasestheamountoftrainingdatabycreatingmodifiedcopiesofadatasetusing
existing data. This includes making small changes to data [Dai and Adel, 2020, Sawai et al., 2021, Duong and
Nguyen-Thi,2021](synonymreplacement,randomdeletion,randominsertion,randomswap,back-translation,lexical
substitution,etc.) orusinggenerativemethodstocreatenewdata[Sharmaetal.,Keragheletal.,2020].
TheapplicationofdataaugmentationtechniquestoNLPhasbeendoneinareasincluding,forexample,textclassification
[DaiandAdel,2020,Karimietal.,2021],machinetranslation[Sawaietal.,2021],andsentimentanalysis[Duongand
Nguyen-Thi,2021]. However,unlikeotherNLPtasks,NERmakespredictionsaboutwords,andnotaboutsentences.
Therefore,applyingtransformationstowordsmaychangetheirlabels. Becauseofthisdifficulty,therehasbeenvery
littleinterestinusingdataaugmentationforNER.However,someauthorshaveattemptedtoadaptdataaugmentation
techniquesforthispurpose. In[DaiandAdel,2020],differentsimpledataaugmentationstrategies(wordreplacement,
namedentityreplacement,etc.) areappliedwiththeaimofimprovingmodelperformance,especiallywhenthedataset
containsveryfewexamples. Otherauthors,includingSharmaetal.,usemorecomplexstrategiessuchasparaphrasing
togeneratenewdata.
DataaugmentationisavitaltechniqueintrainingNERmodels,especiallywhendealingwithlimiteddatasets.Traditional
methodsincludeparaphrasing,synonymreplacement,andback-translation. Inthefuture,however,dataaugmentation
inNERcouldbesignificantlyfacilitatedandenhancedbyleveragingLLMs. LLMscangeneraterealisticanddiverse
13AsurveyonrecentadvancesinNER
textdata,whichcanbeusedtoaugmentexistingdatasetsforNERtraining. Thisapproach,asexemplifiedbytechniques
likeGPT3Mix[Yooetal.,2021],makesitpossibletocreatemorerobustandmoreaccurateNERmodelsbyenriching
trainingdatawithawiderangeoflinguisticvariationsandcontextualscenarios.
5.3 Activelearning
Activelearningisaformofsemi-supervisedlearning. Themainideainactivelearningisthatifalearner(thelearning
algorithm)isabletochoosethedatathatitwishestolearnfrom,itcanperformbetterthanitotherwisewouldusing
traditionallearningschemes. Oneofthemainchallengesofactivelearningistodeterminewhatconstitutesthemost
informativedata,andhowthelearnercanrecognizethisdata.Themostcommonapproachtodayisuncertaintysampling
[Settles,2009],inwhichthemodelselectstheexamplesforwhichitscurrentpredictionistheleastreliable.
WhereactivelearningissuccessfullyappliedtoNLP,eitherperformancemaybeimprovedusingthesameamount
ofdata,orasimilarperformancemaybemaintainedwhilereducingtheamountofdataandannotationrequiredto
developaneffectivemodel. Inthefieldofdeeplearningresearch,pioneeringadoptionsofactivelearninghaveproduced
promisingresults[SiddhantandLipton,2018]. AsregardsNER,approachesbasedondeeplearninghavebeenproposed
inseveralworks[Shenetal.,2017,Yanetal.,2022].
5.4 Few-shotlearning
Few-shot learning aims to build accurate machine learning models with less training data. This technique can be
implementedbyapplyingtransformationstothedata,byapplyingchangesinthealgorithms,orbyusingdedicated
algorithms[Wangetal.,2020]. Applyingtransformationstothedatainvolvesgeneratingnewdatafromthetrainingdata
usingdataaugmentationoragenerativenetwork. Applyingchangesinthealgorithmsinvolvesusingpre-trainedmodels
asfeatureextractor,orrefiningthealreadytrainedmodelinrelationtothenewdatabycontinuingtheback-propagation
ofthegradient. Usingdedicatedalgorithmsinvolvesusingnetworksthatdonotlearnfromasingleinstance,butfrom
pairsortripletsofinstances. Alargerdatabasecanthereforebeobtained. Inthecontextofnamedentities,studies
including [Fritzler et al., 2019, Hou et al., 2020] proposed adapting prototypical networks [Snell et al., 2017] for
NER.However,theiruseofthistechniquefailedtoachievegoodperformances. Theauthorsin[YangandKatiyar,
2020] propose a few-shot learning method based on nearest neighbors and structured inference, with an approach
that goes beyond classical meta-learning approaches. Cui et al. [2021] treat the NER task as a language template
classificationproblem. Theirapproachoutperformstraditionalsequencelabelingmethods. Agrowingnumberofworks
arerecognizingthepotentialoffew-shotlearninginNER[Hoferetal.,2018,Huangetal.,2020].
TheemergenceofLLMssuchasthoseusedinPromptNER[AshokandLipton,2023]hasfurtheradvancedfew-shot
learninginNER.TheseLLMsuseprompt-basedmethodsandChain-of-Thoughtprompting,significantlyimproving
adaptabilityandperformanceinfew-shotsettingswithoutextensivedatasetrequirements.
5.5 Zero-shotlearning
Zero-shotlearningusesapre-trainedmodeltoassignclassestoelementsthatthemodelhasneverencounteredbefore
[Larochelleetal.,2008,Lampertetal.,2013,Dingetal.,2017]. Thiskindofapproachhasbeenexploredforentity
linking[Wuetal.,2019]andnamedentitytyping[Obeidatetal.,2019](i.e.,attributingasemanticlabeltoagiven
entity).
Zero-shotlearningcanbeappliedinNERasawayofdetectingnewtypesofnamedentities. Alyetal.[2021]propose
anarchitectureusingtextualdescriptions. ZERO[VanHoangetal.,2021]performszero-shotlearningandfew-shot
learningbyincorporatingexternalknowledgeintheformofsemanticrepresentationsofwords. In[Yangetal.,2022],
multilingualsequencetranslationisproposedasapossiblesolutioninthecaseoflow-resourcelanguageswherelabeled
dataisscarceorabsent. Multilingualsequencetranslationactsasabridgebytransferringknowledgefromthesource
languagetoalanguagepossessinglargeamountsofannotateddata. Additionally,theriseofprompt-basedlearning
methods,asdetailedin[DeTonietal.,2022],hasintroducedanewparadigmintrainingandfine-tuningLLMsfor
applicationslikeNER,enhancingthecapabilitiesofzero-shotlearninginthisarea.
6 Softwareframeworks
Inthissectionwepresentthebest-knownandmostcommonlyusedNERframeworkstoday:
• OpenAI[OpenAI]offersarangeofAItools,includingGPTmodels,fortextgeneration,questionanswering,
andmore. AlthoughitsfocuswasnotoriginallyonNER,OpenAIisnowbeginningtoexplorethisdomain.
TheAPIisknownforitsflexibilityanduser-friendliness,withanadditionalemphasisonsafe,ethicalAIuse.
14AsurveyonrecentadvancesinNER
Table2: DatasetsforEnglishNER.Datasetshighlightedingrayarethoseselectedforourstudy.
Dataset Year Domain Tags URL
MUC-6 1995 News 7 https://cs.nyu.edu/~grishman/muc6.html
MUC-7 1997 News 7 https://catalog.ldc.upenn.edu/LDC2001T02
NIST-IEER 1999 News 3 https://www.nist.gov/el/intelligent-systems-division-73500/ieee-1588
CoNLL-2002 2002 News 4 https://www.clips.uantwerpen.be/conll2002/ner/‘
CoNLL-2003 2003 News 4 https://www.clips.uantwerpen.be/conll2003/ner/
GENIA 2003 Medical 5 http://www.geniaproject.org/genia-corpus
NCBIDisease 2014 Medical 1 https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3951655/
i2b2-2014 2015 Medical 32 https://www.i2b2.org/NLP/DataSets/Main.php
BC5CDR 2016 Medical 2 https://biocreative.bioinformatics.udel.edu/tasks/biocreative-v/track-3-cdr/
MedMentions 2019 Medical 128 https://github.com/chanzuckerberg/MedMentions
BioNLP2004 2004 Bioinformatics 5 https://www.ncbi.nlm.nih.gov/research/bionlp/Data/
ACE2004 2005 Various 7 https://catalog.ldc.upenn.edu/LDC2005T09
ACE2005 2006 Various 7 https://catalog.ldc.upenn.edu/LDC2006T06
OntoNotes5.0 2013 Various 18 https://catalog.ldc.upenn.edu/LDC2013T19
MultiCoNER 2022 Various 33 https://multiconer.github.io/
WikiGold 2009 Wikipedia 4 https://aclanthology.org/W09-3302
WiNER 2012 Wikipedia 4 https://github.com/ghaddarAbs/WiNER
WikiFiger 2012 Wikipedia 112 https://orkg.org/paper/R163134
Few-NERD 2021 Wikipedia 66 https://github.com/thunlp/Few-NERD
HYENA 2012 Wikipedia 505 https://aclanthology.org/C12-2133.pdf
WikiAnn 2017 Wikipedia 3 https://aclanthology.org/P17-1178/
WNUT2017 2017 Socialmedia 6 https://noisy-text.github.io/2017/emerging-rare-entities.html
MalwareTextDB 2017 Malware 4 https://statnlp-research.github.io/resources/
SciERC 2018 Scientific 6 http://nlp.cs.washington.edu/sciIE/
HIPE-2022-data 2022 Historical 3 https://github.com/hipe-eval/HIPE-2022-data
MITMovie 2013 Queries 12 http://groups.csail.mit.edu/sls/
MITRestaurant 2013 Queries 8 http://groups.csail.mit.edu/sls/
FIN 2015 Financial 4 https://aclanthology.org/U15-1010/
• spaCy[HonnibalandMontani,2017]isafreeopen-sourcelibraryforadvancednaturallanguageprocessing
(NLP)inPython. Itisdesignedtomakeiteasytoconstructsystemsforinformationextractionorgeneral-
purpose natural language processing. spaCy offers a number of analysis operations such as tokenization,
classification,POStaggingandNER.Inadditiontotheentitiesincludedbydefault,spaCyallowstheaddition
ofnewclassesbytrainingthemodelsonnewdata. Avarietyofpre-trainedmodelsareavailable,whichcan
eitherbeuseddirectlyfortaskssuchasNER,orre-trainedonaspecificdataset. Thesemodelsarebasedon
convolutionalnetworks(CNNs)andtransformers.
• NLTK[Bird,2006]isasuiteofPythonmodulesdedicatedtoNLP.NLTKintegratesmorethan50corpora
andlexicalresourcessuchasWordNet,aswellasasuiteofwordprocessinglibrariesfortextualanalysis
operationssuchastokenization,POStagging,sentimentanalysis,topicsegmentation,andspeechrecognition.
UnlikespaCy,whichincludesalgorithmsadaptedtodifferentproblemsandwhicharemanagedbythelibrary,
NLTKgivesusersthefreedomtochoosefromamongawiderangeofalgorithms.
• StanfordCoreNLP[Manningetal.,2014]isalibrarydevelopedbytheassociatedresearchgroupatStanford
University. ItisasetofnaturallanguageanalysistoolswritteninJavaallowingtheseparationoftextinto
tokens,POStaggings,andtrainingmodelsforNER(basedonCRF).Unfortunately,thesefeaturesarenot
available for all languages, and each language has its own specific characteristics. For example, NER is
availableonlyforEnglish,Spanish,German,andChinese.
• ApacheOpenNLP[Kwartler,2017]isalibrarythatsupportsthemostcommonNLPtasks,suchasNER,
language detection, POS tagging, chunking, etc. Unlike other frameworks, which detect all NER-named
entitiesusingthesamemodel,OpenNLPoffersaspecializedmodel(basedonmaximumentropy)foreach
typeofnamedentity.
• Polyglot[Al-Rfouetal.,2015]isaNLPpipelineforJava. Itcanhandleamuchwiderrangeoflanguagesthan
otherframeworks. ForNERitincludesmodelsfor40differentlanguages.
• Flair[Akbiketal.,2019]isafree,open-sourcelibraryallowingthecreationofanNLPpipelinesupporting
multilingualapplications. Differentlanguagemodels,suchasFlair[Akbiketal.,2019]orBERT[Vaswani
etal.,2017]canbeusedaloneorincombination. Flairhasitsownembeddings,butalsooffersELMo[Peters
etal.,2018a]andBERTembeddings.
• Hugging Face [Wolf et al., 2020] provides open-source NLP technologies. It offers two categories of
services: eitherfreeservicesorpaidservicesaimedatbusinesses. Thisframeworkowesitspopularitytoits
transformerslibrary,whichhasanAPIprovidingaccesstoseveralpre-trainedmodels. HuggingFacealso
offersacollaborativeplatformallowinguserstocreate,train,andsharetheirdeeplearningmodels.
15AsurveyonrecentadvancesinNER
• Gate[Cunningham,2002]isatoolwritteninJava. ItisusedbyanumberofNLPcommunitiesfordifferent
languages. Gateprovidesaninformationextractionsystem,knownasANNIE,whichisabletorecognize
severaltypesofentities(people,places,andorganizations).
• TNER[UshioandCamacho-Collados,2022]isaPythonlibraryfortrainingandtuningNERmodelsimple-
mentedinPytorch. Ithasawebapplicationwithanintuitiveinterfaceallowingpredictionstobevisualized.
PackageslikeApacheOpenNLP,StanfordCoreNLP,andspaCyarealsoaccessibleinlanguagesotherthanPython.
Forinstance,openNLP1 isanRpackagethatleveragesthecapabilitiesoftheApacheOpenNLPlibrary,originally
Java-based,byactingasaninterfacewithintheRenvironment. Similarly,thespacyr2packageconnectsRwithspaCy.
Notably, the spacyr package facilitates NER using spaCy’s pre-trained language models. Other solutions like the
reticulate3packagemakeiteasiertoachieveinteroperabilitybetweenRandPythonenablingPythonlibrariessuchas
HuggingFacetobeaccessiblewithinR.
7 EvaluationofNERsystems
TheevaluationofNERsystemsrequiresanannotationscheme, anevaluationstrategy, andmetrics. Eachofthese
requirementsisdiscussedbelow.
7.1 Annotationschemes
Tosegmentandlabelentities,differenttypesofannotationschemeshavebeenproposed. Theseannotationschemesor
encodingsareusedtodeterminethetypeandpositionofentitiesinthesentence. Consideredasasequence-labeling
task,thepurposeofNERistofindthecorrespondingsequencetagforagiventextualsequence. Inexistingannotation
schemes,thefirsttokenofanamedentityistaggedasB(Begin). Whereanamedentitycomprisesmorethanonetoken,
thelasttokenistaggedaseitherE(End)orI(Inside),dependingontheschema,withanyintermediatetokensbeing
taggedasI.TokensthatarenotpartofanamedentityaretaggedasO(Outside). Atagschemacanincludeanyorallof
thesefourtags. Commonschemesare: BIO,IO,IOE,IOBES,IEandBIES.
• TheIOschemeisthesimplestmethodapplicabletothistask,whereeachtokeninthedatasetreceiveseither
an(I)oran(O)tag. The(I)tagrepresentsnamedentities, whilethe(O)tagrepresentsotherwords. One
limitationofthisschemaisitsinabilitytodifferentiatebetweenconsecutiveentitynamesofthesametype.
• TheBIOschema,widelyusedandadoptedbytheCoNLLConference,assignsoneofthreetagstoeachtoken:
(B)forthestartofanamedentity, (I)forinsidetagswithintheentity, and(O)foroutsidetagsindicating
non-entitywords.
• TheIOEschemaissimilartoIOB,butinsteadofmarkingthestartofthenamedentity(B),itdenotestheend
oftheentity(E).
• IOBESservesasanalternativetotheIOBscheme,offeringmoreinformationregardingtheboundariesof
namedentities. Itusesfourtags: (B)forthebeginningofanentity,(I)forinsidetagswithintheentity,(E)for
theendofanentity,(S)forsingle-tokenentities,and(O)fornon-entitywordsoutsidenamedentities.
• TheIEschemefunctionsmuchlikeIOE,withthedifferencelyinginthelabelingoftheendofnon-entity
wordsas(E-O)andtherestas(I-O).
• TheBIESschemeisanextensionofIOBES.Itutilizestagssuchas(B-O)forthebeginningofnon-entity
words,(I-O)forinsidetagswithinnon-entitywords,(E-O)fortheendofnon-entitywords,and(S-O)for
singlenon-entitytokenslocatedbetweentwoentities.
Foracomparisonoftheseannotationschemes,refertoTable3whichillustratestheapplicationofeachschemetoa
singlesentenceexample.
ItisworthnotingthatthetagschemechosenmayalsoaffectNERperformance;forexample,AlshammariandAlanazi
[2021]foundthatIOoutperformsotherschemesinthecaseofarticleswritteninArabic. Thestudy[Chenetal.,2021b]
showsthatIOschemeismoresuitableforthesteelE-commercedatathanBIOandBIEOschemes.
1https://cran.r-project.org/web/packages/openNLP/index.html
2https://cran.r-project.org/web/packages/spacyr/index.html
3https://cran.r-project.org/web/packages/reticulate/index.html
16AsurveyonrecentadvancesinNER
Table3: Comparisonofdifferentannotationschemesonasinglesentence,where"PER"denotesapersonand"ORG"
representsanorganization.
Words IO BIO IOE IE IOBES BIES
Emma I-PER B-PER I-PER I-PER B-PER B-PER
Charlotte I-PER I-PER I-PER I-PER I-PER I-PER
Duerre I-PER I-PER I-PER I-PER I-PER I-PER
Watson I-PER I-PER E-PER E-PER E-PER E-PER
was O O O I-O B-O B-O
born O O O I-O I-O I-O
in O O O E-O E-O E-O
Paris I-ORG B-ORG I-ORG I-ORG S-ORG S-ORG
. O O O I-O B-O S-O
7.2 Evaluationstrategies: exactorrelaxedevaluation
TheevaluationofNERsystemsisbasedoncomparingpredictionswithagoldstandard. Inmakingthiscomparison,
either of two strategies may be employed, namely exact evaluation or relaxed evaluation. In exact evaluation, the
contoursofthenamedentityanditstypemustexactlymatchthegoldstandard. Incontrast, relaxedassessmentis
basedonascoringsystemwhereanyentitywiththecorrecttypeearnsacertainnumberofpoints,eventhoughthe
contoursmaynotbecorrect. Similarly,entitiesalsoearnpointswherethecontoursarecorrect,butnotthetype. The
MUC[GrishmanandSundheim,1996b]andACE[Doddingtonetal.,2004]evaluationsarebasedonrelaxedevaluation
methods. Ontheotherhand,CoNLL-2003[SangandDeMeulder,2003]usesexactevaluation,whichisthestrategy
mostcommonlyusedtodayforevaluatingNERsystems.
7.3 Metrics
Classicalmetricssuchasprecision,recall,andF1-scoreareoftenusedforevaluatingnamedentities:
• Precisioncorrespondstotheproportionofnamedentitiessuccessfullyrecognizedbyamodelinrelationtothe
totalnumberofnamedentities.
TP
precision= (4)
TP+FP
whereTPisthenumberofTruePositivesandFPisthenumberofFalsePositives.
• Recallmeasuresthenumberofrelevantnamedentitiesretrievedbyamodelagainstthetotalnumberofrelevant
namedentities.
TP
recall= (5)
TP+FN
whereFNisthenumberofFalseNegatives.
• F1-scorereflectsamodel’sabilitytoeffectivelydetectnamedentities,withatrade-offbetweenprecisionand
recall.
precision×recall
F1=2× (6)
precision+recall
Theabovemetricscanbecomputedforeachclassofentities. Theycanbeaggregatedwhenmorethanonetypeof
entityisbeingconsidered.
• Macro-average: themetric(forinstance, F1-score)iscomputedforeachclassseparately, withthemacro-
averagecorrespondingtothemeanofthesevalues.
• Micro-average: itgivesequalweighttoeachsample.
Togobeyondtheseaggregatedmetrics, theauthorsin[Fuetal.,2020]proposedanewevaluationmethod, which
involvesdefiningasetofattributespossessedbyentities(suchaslength,ordensity). Theyfoundthatmodelsoften
haveabettercorrelationwithsomeattributesthanwithothers.
8 NERdatasets
Namedentitiesoftenbelongtobroadcategoriessuchaspersons,locations,andorganizations. However,categoriescan
bemuchnarrowerthanthis: forexample,theymightcorrespondtobooks,periodicals,magazines,etc. Table8givesan
17AsurveyonrecentadvancesinNER
overviewof26datasetsforEnglishNER,withbetweenoneand505typesofentitiesinvariousdomains: medicaldata,
news,socialmedia,etc.
Intheremainderofoursurveyandinourexperiments,wemakeuseofthefollowingdatasetsfromvarioussources:
• CoNLL-2003: adatasetconsistingmainlyofnewsarticlesfromReuters.
• OntoNotes5.0: adatasetcomprisingvariousgenresoftexts(phoneconversations,newswires,newsgroups,
broadcastnews,broadcastconversations,weblogs,andreligioustexts).
• WNUT2017: adatasetcomprisingvariousgenresoftexts(tweets,redditcomments,YouTubecomments,and
StackExchange).
• BioNLP2004: amedicaldatasetthatprovides2000MEDLINEabstracts.
• FIN:adatasetcontainingasetoffinancialdatamadepublicthroughtheUSSecurityandExchangeCommis-
sion. Thisdatasethasbeenannotatedby[Alvaradoetal.,2015].
• NCBIDisease: adatasetcontainingdiseasenamesandconceptannotationsfromtheNCBIDiseaseCorpus.
• BC5CDR:adatasetconsistingofthreeseparatesetsofarticleswithchemicals,diseases,andtheirrelations.
• MITRestaurant: adatasetcontainingasetofonlinerestaurantreviews.
• Few-NERD:adatasetcontainingasetofWikipediaarticlesandnewsreports. Sincethisdatasetisverylarge,
wetrainedthemodelsonalimitedpartofit(20%ofthedatawhichrepresents32941samples).
• MultiCoNER:alargemultilingualdatasetthatcoversthreedomains: Wikisentences,questions,andsearch
queries.
ThecharacteristicsofthesedatasetsaregiveninTable4.
Table4: Descriptionofthedatasetsusedinourcomparativestudy. “#”referstothenumberofsamples.
Corpus #train #test #validation tags
CoNLL-2003 14,041 3,453 3,250 4
OntoNotes5.0 59,924 8,262 8,528 18
WNUT2017 2,395 1,287 1,009 6
BioNLP2004 16,619 3,856 1,927 5
FIN 1,018 305 150 4
NCBIDisease 5,433 941 924 1
BC5CDR 5,228 5,865 5,330 2
MITRestaurant 6,900 1,521 760 8
Few-NERD 131,767 37,648 18,824 66
MultiCoNER 16,778 249,980 871 33
9 Experiments
Inthissection,wedetailtheprotocoladoptedtoevaluatetheeffectivenessoftheselectedalgorithms.
9.1 Settings
Thissectiondescribestheexperimentalconditionsusedforourstudy.
9.1.1 Datasets
Ourexperimentswereconductedontendatasetswithtextsfromvariousdomains(Table4). Theyhavedifferentsizes
anddifferentnumbersofclasses,enablingustotestthemodelsinavarietyofcircumstances.
9.1.2 Models
Weselecttheframeworksbasedontheiravailability(freeandopensource)andtheirabilitytotrainmodelsoncustom
datasets. Consequently,thoseretainedwereFlair,StanfordCoreNLP,ApacheOpenNLP,spaCy,andHuggingFace.
NLTKwasexcludedfromtheanalysisbecauseitcomeswiththeStanfordNamedEntitiestagger. Forframeworksthat
containmorethanonealgorithm,wechooseseveralmodels. ForspaCythethreemodelschosenwerethreenetworks:
asmallCNN(en_core_web_sm),alargeCNN(en_core_web_lg),andatransformerbasedonbasicRoBERTa[Liu
18AsurveyonrecentadvancesinNER
etal.,2019a]. ForHuggingFace,wechoosefourmodels: basicBERTarchitecture,withandwithoutlowercasing,
thedistilledversionofBERT,andawideRoBERTaarchitecture. ForOpenAI,weselectthemostrecentandupdated
versionofGPT,knownasGPT-4-1106-preview.
9.1.3 Materialconditions
AllexperimentswereconductedonanAmazonp3.2xlargeVMwithAmazonLinux2andPython3.10. Theinstance
has8vCPUs,16GBofGPU(NvidiaTeslav100)and64GBofmemory.
9.2 Method
Experimentsinvolvedthefollowingthreesteps:
9.2.1 Dataformatandannotationscheme
EachcorpuswasencodedintheCoNLL-UformatinaccordancewiththeBIOschemewhere: a)wordlinescontainthe
annotationofeachword,2)blanklinesmarksentenceboundaries. Sinceeachframeworkusesitsownformatfordata
representation,theinitialCoNLL-Uformatthenhadtobeconvertedtotherequiredframework-specificformats. For
GPT-4,dedicatedpromptswereused,eachtailoredtospecificdatasets. Everypromptoutlinestheprimarycategoriesof
thenamedentitiestargeted,accompaniedbyafewexamples. Figure8presentsanexampleofapromptwitharesponse
formattedinJSON.
Inourstudy,wedeliberatelyadoptedanexperimentalapproachthatslightlydivergesfromtheonepresentedinthe
GPT-NERpaperWangetal.[2023]. Althoughthemethodusedinthatpriorresearchdemonstratedeffectiveness,itwas
basedontheuseofspecificpromptsforeachnamedentitytype,queryingthemodelseparately. Whileeffective,this
techniquetendstosimplifythetaskforthemodelbyreducingtheneedfordisambiguationbetweendifferententity
types. Ourchoicewastoemployalessdirectedstrategy,aimingtoassessthemodel’soverallabilitytoaccurately
identifyandcategorizenamedentitiesinmorecomplexandvariedsituations. Thisapproachmorecloselyresembles
real-worldusageconditions,whereentitydisambiguationandcontextunderstandingplayacrucialroleinthemodel’s
overallperformance.
9.2.2 Modelsandhyper-parameters
Flair ForFlairmodels,wetookthestandardarchitecturebasedonanLSTM-CRFnetworkwithFlairembeddings.
Thehyper-parameterswereobtainedviaagridsearchwherehidden_layer_sizeisin(64,128,256)andlearning_ratein
(0.05,0.1,0.15,0.2). ThevaluesgivingthebestF1-scoreonthevalidationsetwereselected. Thisgavealearning_rate
of0.1andahidden_layer_sizeof256,thesevaluesalsobeingusedinthepre-trainedversionofthechosenarchitecture.
HuggingFace FortheHuggingFacemodels,CRFwasusedtodecodetheentities(c.fSection4.3.3). Thesamegrid
searchmethodwasusedwithlearning_ratein(0.001,0.0001,0.00001),withandwithoutmax_grad_norm. Thenumber
ofepochswassetto10andweight_decayto0.01.
ApacheOpenNLP Thedefaulthyper-parametervaluewasselected: onlythenumberofiterationscanbechanged,so
wekeptthedefaultvalue(i.e. 300).
StanfordCoreNLPandspaCy Thevaluesofthepre-trainedmodelswereselected.
9.2.3 Evaluation
Toassessthequalityoftheresultsweuseastrategyofexactevaluation.OurchosenmetricisF1-score,sincethisreflects
thetwoothermetricsdiscussedinsection7.3(namely,precisionandrecall). InthecaseofApacheOpenNLP,resultsfor
somedatasetsareabsent,sinceonlythreetypesofnamedentitiesaredetectedbythisframework(persons,organizations,
andlocations),anddatasetsnotcontainingthesethreetypeswerethereforenotincludedintheevaluation.
Toassessthedifferencesbetweenthemodels,weperformFriedmantest[Friedman,1940]. Themodelsareordered
accordingtotheirF1-scoreandthenullhypothesiscanbestatedas: "themedianisequal". Whenthenullhypothesisis
rejected(wechoose0.1asthresholdofsignificance),weapplyNemenyi’smethod[Nemenyi,1963]todeterminethe
groupsthatdiffer. WereplacemissingF1-scorevalueswithzeros,specificallyforApacheOpenNLP.
19AsurveyonrecentadvancesinNER
Figure8: AnexampleofapromptfordetectingnamedentitiesintheNCBIDiseasedataset.
9.3 Resultsanddiscussion
TheresultsofourexperimentsarereportedinTable5. Themainfindingsarethefollowing.
Table 5: Comparison of NER frameworks in terms of Macro-averaged F1-score. Best and second-best scores are
respectivelyinboldandunderlined. ThedashesrepresentmissingvaluesbecauseApacheOpenNLPdoesnotallow
theadditionofothernamedentitycategoriesbeyondthefourstandardtypes: people,organizations,locations,and
miscellaneous.
CoNLL-2003 OntoNotes WNUT2017
FIN
BioNLP2004 NCBIDisease BC5CDR MITRestaurant Few-NERD MultiCoNER
Frameworks Algorithms Macro-averagedF1-score
ApacheOpenNLP MaximumEntropy 80.00 67.83 - 63.24 - - - - - -
StanfordCoreNLP CRF 85.18 63.87 8.34 55.25 73.26 86.10 85.22 70.57 45.13 19.39
Flair LSTM-CRF 90.35 80.10 38.07 74.23 71.64 86.21 90.27 78.33 60.03 56.27
spaCy CNN-small 81.26 69.30 9.01 55.12 65.92 77.92 80.83 75.62 40.55 35.63
CNN-large 85.64 69.60 9.78 54.71 66.17 79.15 79.66 76.39 40.01 35.82
roberta-base 89.92 81.04 41.84 63.18 66.56 87.05 87.08 79.09 59.15 55.21
HuggingFace xlm-roberta-large 91.46 81.57 43.92 48.68 71.43 85.25 87.41 80.12 61.59 58.15
distilbert-base-cased 88.12 77.63 25.45 43.74 69.63 84.42 84.03 77.67 58.62 55.17
bert-base-uncased 88.89 76.99 32.12 46.84 70.50 85.64 85.78 79.18 58.16 59.96
bert-base-cased 90.09 79.55 33.32 39.53 69.46 85.27 85.14 78.48 59.48 56.64
OpenAI GPT-4 62.74 33.61 18.82 36.70 41.32 57.46 55.67 41.38 44.96 33.61
20AsurveyonrecentadvancesinNER
Ondatasetswithalargetrainingset(OntoNotes,CoNLL-2003,Few-NERDandMultiCoNER),thearchitecturesgiving
thebestresultsarethosebasedontransformers. Ourhypothesisisthattransformersareover-parameterizedandare
thusfavoredbylargeamountsoftrainingdata. However,theGPTmodelseemstonotworkcorrectlywhenitcomes
torecognizingnamedentities;itdoesnotevenappearinthetop3,andthisisthecaseacrossalldatasets. Thiscan
beexplainedbythefactthatthemodelstrugglestodisambiguatenamedentitiesthatcanbeassociatedwithmultiple
classes. Inaddition,theGPTmodelalsofacesdifficultiesindetectingcompositenamedentities,suchasdiseasenames.
Indeed, thesecompositeentitiesoftenconsistofmultiplewordsandcanbeparticularlychallengingforthemodel
torecognizeaccurately. Trainingasupervisedmodelbecomesevenmorecrucialwhendealingwithcomplexand
compositenamedentities,asitallowsforbetterdisambiguationandclassificationinsuchcases.
ForBioNLP2004,whichalsohasalargetrainingset,LSTM-CRFandCRFhaveslightlybetterresultsthanHugging
Face’s transformers. Our intuition is that biomedical data has little in common with the data used to pre-train the
transformers, so the pre-training does not greatly contribute to their performance. However, on medical datasets
containing very few kinds of named entities (NCBI Disease and BC5CDR), transformers appear among the most
efficientarchitecturesalongwiththeonebasedonLSTM-CRF,withaslightdifferenceinfavorofLSTM-CRF.This
performanceimprovementovertheBioNLP2004datasetmayberelatedtothereducednumberofnamedentitytypesin
thesedatasets.
OnadatasetsuchasFINwithasmalltrainingset,transformerscanbeseentoperformlesswell: LSTM-CRFobtains
farbetterresults,withanF1-scoreof74.23,comparedto63.18forthebesttransformer,namelyRoBERTa-base. The
differenceinperformanceisevenmoremarkedinthecaseofxlm-roberta-large,withanF1-scoreof48.68,although
thistransformerexcelsontheotherdatasets. ForFINonlythreeofthefourtypesofentitiesarepresentinthevalidation
set,whichexplainsthisdropinperformanceoftransformers: theyaresensitivetohyper-parametertuningandrequire
well-formedvalidationsets.
On MITRestaurant and WNUT2017 (which is also a small dataset), xlm-roberta-large performs the best. Since
RoBERTaistrainedonCommonCrawl,andsincethetypeofdatainthesetwodatasetsisabundantontheweb,the
pre-trainingislikelytobeofconsiderablebenefit.
ItisalsointerestingtonotethatLSTM-CRFperformswellonalldatasets,thusmakingitagoodchoiceindependently
ofconsiderationsregardingthetypeofdata.
Itwillalsoberemarkedthatthebert-base-casedandbert-base-uncasedmodelsdonotyieldthesameresults,despitethe
factthattheyweretrainedunderthesameconditions. Withbert-base-uncased,textislowercasedbeforethetokenization
step,whilewithbert-base-caseditdoesnotundergoanytransformation.OntheCoNLL-2003,OntoNotes,MultiCoNER
andFew-NERDdatasets,theuncasedmodelgivesbetterresults,whilethecasedmodelperformsbetterontheothers.
BasedonFriedmantest,thenullhypothesisisrejected(p-value=4.32e-10),andonecanassumeastatisticallysignificant
differencebetweenthemedianvalues: acommonbehavioramongthemodelshaslittleprobability.
Figure9: Thecriticaldifference(CD)diagrambasedonF1-score. Theplotshowsthemeanranksofeachmodelon10
datasets. Thelowertherank,thebettertheperformanceofamodel. Ahorizontallineindicatesnosignificantdifference
betweenthemodelscrossedbytheline.
Nemenyi’spost-hoctestresultsaregiveninFigure9.Onewouldassumethattherearenosignificantdifferencesbetween
LSTM-CRFandallthetransformersusedinthisstudy. OnecanalsoassumethatLSTM-CRFandxlm-roberta-large
havethesamemedians. However,nosignificantdifferenceisfoundbetweenthebert-base-casedandbert-base-uncased
models.
21AsurveyonrecentadvancesinNER
10 Conclusionandperspectives
Thisarticleproposesacomprehensivesurveyonrecentadvancesonnamedentityrecognitioninclassificationcontext.
Inparticular,wereviewedrecentmethodssuchasLLMs,graph-basedapproaches,andmethodsfortrainingmodelson
smalldatasets. Weevaluatedthemostpopularframeworksandtoolsondatasetswithdifferentcharacteristics.
Transformer-basedarchitectures, particularlyonlargerdatasets, havedemonstratedgoodperformanceduetotheir
substantial parameterization and adaptability. However, our analysis suggests that despite the overall success of
transformers,specificmodelslikeGPT,andbyextension,GPT-4,donotconsistentlyrankatthetopforNERtasks.
Thisisattributedtochallengesindisambiguatingandaccuratelydetectingcompositenamedentities, crucialtasks
inNER.Incontrast,LSTM-CRFmodelshaveshownremarkableconsistencyandrobustnessacrossvariousdatasets,
makingthemareliablechoiceforawiderangeofapplications. Thespecificneedsofbiomedicaldatasetshighlight
theimportanceofdomain-specificmodelingandthelimitsofgeneralpre-trainingtypicallyemployedintransformer
models.
Lookingtothefuture,theroleofLLMsmodelsinenhancingNERcannotbeunderstated.Despitethecurrentlimitations
notedinmodelslikeGPTforspecificNERtasks,therapidevolutionoflanguagemodelssuggestsapromisingavenue
for incorporating such technologies into more refined and accurate entity recognition systems. GPT-4’s ability to
understandandgeneratenuancedtext, combinedwithcontinuallearningandadaptation, presentsopportunitiesto
overcomethecurrentchallengesnotedwithcompositeanddomain-specificentities.
FurtherresearchshouldaimtoharnessandadaptthecapabilitiesofGPT-4andsimilarmodelsforimprovingperformance
inNER.Thisinvolvesnotonlyfine-tuningthesemodelsforspecificNERtasksbutalsoexploringinnovativetraining
techniques, data augmentation strategies, and model architectures that leverage the contextual understanding and
flexibilityofLLMs. Additionally,adeeperdiveintopreprocessingtechniquesandtheirimpactonvariousmodelswill
provideamorecomprehensiveunderstandingofhowtooptimizeperformanceacrossdifferentscenarios.
References
RalphGrishmanandBethMSundheim. Messageunderstandingconference-6: Abriefhistory. InCOLING1996
Volume1: The16thInternationalConferenceonComputationalLinguistics,1996a.
BogdanBabychandAnthonyHartley. Improvingmachinetranslationqualitywithautomaticnamedentityrecognition.
InInternationalEAMTworkshoponMTandotherlanguagetechnologytools,ImprovingMTthroughotherlanguage
technologytools,ResourceandtoolsforbuildingMTatEACL,2003.
DiegoMollá,MennoVanZaanen,andDanielSmith. Namedentityrecognitionforquestionanswering. InProceedings
oftheAustralasianlanguagetechnologyworkshop2006,pages51–58,2006.
JiafengGuo,GuXu,XueqiCheng,andHangLi. Namedentityrecognitioninquery. InSIGIR,pages267–274,2009.
Pan Liu, Yanming Guo, Fenglei Wang, and Guohui Li. Chinese named entity recognition: The state of the art.
Neurocomputing,473:37–53,2022.
AndreiMikheev,MarcMoens,andClaireGrover. Namedentityrecognitionwithoutgazetteers. InNinthConferenceof
theEuropeanChapteroftheAssociationforComputationalLinguistics,pages1–8,1999.
LisaFRau. Extractingcompanynamesfromtext. InConferenceonArtificialIntelligenceApplication,pages29–30,
1991.
DavidNadeauandSatoshiSekine. Asurveyofnamedentityrecognitionandclassification. LingvisticaeInvestigationes,
30(1):3–26,2007.
RonanCollobert. Deeplearningforefficientdiscriminativeparsing. InProceedingsofthefourteenthinternational
conferenceonartificialintelligenceandstatistics,pages224–232.JMLRWorkshopandConferenceProceedings,
2011.
KaiLabusch,PreußischerKulturbesitz,ClemensNeudecker,andDavidZellhöfer. Bertfornamedentityrecognition
in contemporary and historical german. In Proceedings of the 15th conference on natural language processing,
Erlangen,Germany,pages8–11,2019a.
KhaledShaalan. ASurveyofArabicNamedEntityRecognitionandClassification. ComputationalLinguistics,40(2):
469–510,062014. ISSN0891-2017. doi:10.1162/COLI_a_00178. URLhttps://doi.org/10.1162/COLI_a_
00178.
RodrigoRafaelVillarrealGoulart,VeraLúciaStrubedeLima,andClarissaCastellãXavier. Asystematicreviewof
namedentityrecognitioninbiomedicaltexts. JournaloftheBrazilianComputerSociety,17:103–116,2011.
22AsurveyonrecentadvancesinNER
MónicaMarrero,JuliánUrbano,SoniaSánchez-Cuadrado,JorgeMorato,andJuanMiguelGómez-Berbís. Named
entityrecognition: fallacies,challengesandopportunities. ComputerStandards&Interfaces,35(5):482–489,2013.
RahulSharnagat. Namedentityrecognition: Aliteraturesurvey. CenterForIndianLanguageTechnology,pages1–27,
2014.
JingLi,AixinSun,JiangleiHan,andChenliangLi. Asurveyondeeplearningfornamedentityrecognition. IEEE
TransactionsonKnowledgeandDataEngineering,34(1):50–70,2020a.
Xiaole Li, Tianyu Wang, Yadan Pang, Jin Han, and Jin Shi. Review of research on named entity recognition. In
ArtificialIntelligenceandSecurity,pages256–267,2022.
YuWang,HanghangTong,ZiyeZhu,andYunLi. Nestednamedentityrecognition: Asurvey. ACMTrans.Knowl.
Discov. Data, 16(6), jul 2022. ISSN 1556-4681. doi:10.1145/3522593. URL https://doi.org/10.1145/
3522593.
LeighWeston,VaheTshitoyan,JohnDagdelen,OlgaKononova,AmalieTrewartha,KristinAPersson,GerbrandCeder,
andAnubhavJain. Namedentityrecognitionandnormalizationappliedtolarge-scaleinformationextractionfrom
thematerialsscienceliterature. Journalofchemicalinformationandmodeling,59(9):3692–3702,2019.
ParthaSarathyBanerjee,BaisakhiChakraborty,DeepakTripathi,HardikGupta,andSourabhSKumar. Ainformation
retrievalbasedonquestionandansweringandnerforunstructuredinformationwithoutusingsql. WirelessPersonal
Communications,108:1909–1931,2019.
VishalSinghRoha,NaveenSaini,SriparnaSaha,andJoseGMoreno. Moo-cmds+ner:Namedentityrecognition-based
extractivecomment-orientedmulti-documentsummarization. InEuropeanConferenceonInformationRetrieval,
pages580–588.Springer,2023.
FahimKSufi,ImranRazzak,andIbrahimKhalil. Trackinganti-vaxsocialmovementusingai-basedsocialmedia
monitoring. IEEETransactionsonTechnologyandSociety,3(4):290–299,2022.
CheoneumPark,SeohyeongJeong,andJuaeKim.Admit:Improvingnerinautomotivedomainwithdomainadversarial
trainingandmulti-tasklearning. ExpertSystemswithApplications,225:120007,2023.
OmarAl-Qawasmeh,MohammadAl-Smadi,andNisreenFraihat. Arabicnamedentitydisambiguationusinglinked
open data. In 2016 7th International Conference on Information and Communication Systems (ICICS), pages
333–338.IEEE,2016.
Zhen Li, Dan Qu, Chaojie Xie, Wenlin Zhang, and Yanxia Li. Language model pre-training method in machine
translationbasedonnamedentityrecognition. InternationalJournalonArtificialIntelligenceTools,29(07n08):
2040021,2020b.
CasimirGeorgeBorkowski. Asystemforautomaticrecognitionofnamesofpersonsinnewspapertexts. 1966.
Denis Maurel, Nathalie Friburger, Jean-Yves Antoine, Iris Eshkol, and Damien Nouvel. Cascades de trans-
ducteurs autour de la reconnaissance des entités nommées. Revue TAL, 52(1):69–96, 2011. URL https:
//hal.archives-ouvertes.fr/hal-00682805.
Satoshi Sekine and Chikashi Nobata. Definition, dictionaries and tagger for extended named entity hierarchy. In
LREC’04,2004.
OrenEtzioni,MichaelCafarella,DougDowney,Ana-MariaPopescu,TalShaked,StephenSoderland,DanielS.Weld,
and Alexander Yates. Unsupervised named-entity extraction from the web: An experimental study. Artificial
Intelligence, 165(1):91–134, 2005a. ISSN 0004-3702. doi:https://doi.org/10.1016/j.artint.2005.03.001. URL
https://www.sciencedirect.com/science/article/pii/S0004370205000366.
Shaodian Zhang and Noémie Elhadad. Unsupervised biomedical named entity recognition: Experiments with
clinical and biological texts. Journal of Biomedical Informatics, 46(6):1088–1098, 2013. ISSN 1532-0464.
doi:https://doi.org/10.1016/j.jbi.2013.08.004. URL https://www.sciencedirect.com/science/article/
pii/S1532046413001196. SpecialSection: SocialMediaEnvironments.
Daniel Hanisch, Katrin Fundel, Heinz-Theodor Mevissen, Ralf Zimmer, and Juliane Fluck. Prominer: rule-based
proteinandgeneentityrecognition. BMCbioinformatics,6(1):1–9,2005.
Alexandra Pomares Quimbaya, Alejandro Sierra Múnera, Rafael Andrés González Rivera, Julián Camilo Daza
Rodríguez,OscarMauricioMuñozVelandia,AngelAlbertoGarciaPeña,andCyrilLabbé. Namedentityrecognition
overelectronichealthrecordsthroughacombineddictionary-basedapproach. ProcediaComputerScience,100:
55–61,2016.
YusukeShinyamaandSatoshiSekine. Namedentitydiscoveryusingcomparablenewsarticles. InCOLING2004:
Proceedingsofthe20thInternationalConferenceonComputationalLinguistics,pages848–853,Geneva,Switzerland,
aug23–aug272004a.COLING. URLhttps://aclanthology.org/C04-1122.
23AsurveyonrecentadvancesinNER
LudovicBonnefoy,PatriceBellot,andMichelBenoit. Mesurenon-superviséedudegréd’appartenanced’uneentité
àuntype. TALN2011,juin2011. URLhttps://www.lirmm.fr/TALN2011/PDF_court/Bonnefoy_taln11_
submission_120.pdf.
YusukeShinyamaandSatoshiSekine. Namedentitydiscoveryusingcomparablenewsarticles. InCOLING2004:
Proceedingsofthe20thInternationalConferenceonComputationalLinguistics,pages848–853,2004b.
David Nadeau, Peter D Turney, and Stan Matwin. Unsupervised named-entity recognition: Generatinggazetteers
and resolving ambiguity. In Advances in Artificial Intelligence: 19th Conference of the Canadian Society for
ComputationalStudiesofIntelligence,CanadianAI2006,QuébecCity,Québec,Canada,June7-9,2006.Proceedings
19,pages266–277.Springer,2006.
OrenEtzioni,MichaelCafarella,DougDowney,Ana-MariaPopescu,TalShaked,StephenSoderland,DanielSWeld,
and Alexander Yates. Unsupervised named-entity extraction from the web: An experimental study. Artificial
intelligence,165(1):91–134,2005b.
Michael Collins and Yoram Singer. Unsupervised models for named entity classification. In 1999 Joint SIGDAT
ConferenceonEmpiricalMethodsinNaturalLanguageProcessingandVeryLargeCorpora,1999. URLhttps:
//aclanthology.org/W99-0613.
ZornitsaKozareva,BoyanBonev,andAndresMontoyo. Self-trainingandco-trainingappliedtospanishnamedentity
recognition. InMexicanInternationalconferenceonArtificialIntelligence,pages770–779.Springer,2005.
ShangGao,OliveraKotevska,AlexandreSorokine,andJBlairChristian. Apre-trainingandself-trainingapproachfor
biomedicalnamedentityrecognition. PloSone,16(2):e0246310,2021.
Sudha Morwal, Nusrat Jahan, and Deepti Chopra. Named entity recognition using hidden markov model (hmm).
InternationalJournalonNaturalLanguageComputing(IJNLC)Vol,1,2012.
DanielM.Bikel,RichardSchwartz,andRalphM.Weischedel. Analgorithmthatlearnswhat‘sinaname. Mach.
Learn.,34(1–3):211–231,feb1999. ISSN0885-6125. doi:10.1023/A:1007558221122. URLhttps://doi.org/
10.1023/A:1007558221122.
AndrewEliotBorthwick. Amaximumentropyapproachtonamedentityrecognition. NewYorkUniversity,1999.
Yi-FengLin,Tzong-HanTsai,Wen-ChiChou,Kuen-PinWu,etal. Amaximumentropyapproachtobiomedicalnamed
entityrecognition. InInternationalConferenceonDataMininginBioinformatics,pages56–61,2004.
HidekiIsozakiandHidetoKazawa. Efficientsupportvectorclassifiersfornamedentityrecognition. InCOLING2002:
The19thInternationalConferenceonComputationalLinguistics,2002.
TakakiMakino,YoshihiroOhta,Jun’ichiTsujii,etal. Tuningsupportvectormachinesforbiomedicalnamedentity
recognition. InACL-02workshoponNLPinthebiomedicaldomain,pages1–8,2002.
AndrewMcCallumandWeiLi. Earlyresultsfornamedentityrecognitionwithconditionalrandomfields,feature
inductionandweb-enhancedlexicons. InHLT-NAACL2003-Volume4,page188–191,2003.
Burr Settles. Biomedical named entity recognition using conditional random fields and rich feature sets. In Pro-
ceedings of the International Joint Workshop on Natural Language Processing in Biomedicine and its Applica-
tions (NLPBA/BioNLP), pages 107–110, Geneva, Switzerland, August 28th and 29th 2004a. COLING. URL
https://aclanthology.org/W04-1221.
ShaojunZhao. Namedentityrecognitioninbiomedicaltextsusinganhmmmodel. InProceedingsoftheinternational
jointworkshoponnaturallanguageprocessinginbiomedicineanditsapplications(NLPBA/BioNLP),pages87–90,
2004.
AndrewBorthwick,JohnSterling,EugeneAgichtein,andRalphGrishman. Nyu: Descriptionofthemenenamedentity
systemasusedinmuc-7. InSeventhMessageUnderstandingConference(MUC-7): ProceedingsofaConference
HeldinFairfax,Virginia,April29-May1,1998,1998.
PraneethMShishtla,KarthikGali,PrasadPingali,andVasudevaVarma. Experimentsinteluguner: Aconditional
randomfieldapproach. InProceedingsoftheIJCNLP-08WorkshoponNamedEntityRecognitionforSouthand
SouthEastAsianLanguages,2008.
Burr Settles. Biomedical named entity recognition using conditional random fields and rich feature sets. In Pro-
ceedingsoftheinternationaljointworkshoponnaturallanguageprocessinginbiomedicineanditsapplications
(NLPBA/BioNLP),pages107–110,2004b.
HiroyasuYamada,TakuKudo,andYujiMatsumoto. Japanesenamedentityextractionusingsupportvectormachine.
43(1):44–53,2002.
TakuKudoandYujiMatsumoto. Chunkingwithsupportvectormachines. InSecondmeetingoftheNorthAmerican
chapteroftheAssociationforComputationalLinguistics,2001.
24AsurveyonrecentadvancesinNER
JuanRamosetal. Usingtf-idftodeterminewordrelevanceindocumentqueries. InProceedingsofthefirstinstructional
conferenceonmachinelearning,volume242,pages29–48.Citeseer,2003.
TomasMikolov,IlyaSutskever,KaiChen,GregSCorrado,andJeffDean. Distributedrepresentationsofwordsand
phrasesandtheircompositionality. InC.J.Burges,L.Bottou,M.Welling,Z.Ghahramani,andK.Q.Weinberger,
editors,AdvancesinNeuralInformationProcessingSystems,volume26.CurranAssociates,Inc.,2013. URLhttps:
//proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf.
JeffreyPennington,RichardSocher,andChristopherDManning. Glove: Globalvectorsforwordrepresentation. In
EMNLP,volume14,pages1532–1543,2014.
Piotr Bojanowski, Edouard Grave, Armand Joulin, and Tomas Mikolov. Enriching word vectors with subword
information. arXivpreprint,2016.
AlecRadford,KarthikNarasimhan,TimSalimans,andIlyaSutskever. Improvinglanguageunderstandingbygenerative
pre-training. 2018.
SeungwookLeeandYoungjoongKo. Named-entityrecognitionusingautomaticconstructionoftrainingdatafrom
socialmediamessagingapps. IEEEAccess,8:222724–222732,2020.
WahibaBenAbdessalemKaraa.Namedentityrecognitionusingwebdocumentcorpus.arXivpreprintarXiv:1102.5728,
2011.
RonanCollobert,JasonWeston,LéonBottou,MichaelKarlen,KorayKavukcuoglu,andPavelKuksa. Naturallanguage
processing(almost)fromscratch. J.Mach.Learn.Res.,12(null):2493–2537,nov2011. ISSN1532-4435.
Zhiheng Huang, Wei Xu, and Kai Yu. Bidirectional lstm-crf models for sequence tagging. arXiv preprint
arXiv:1508.01991,2015.
XuezheMaandEduardHovy. End-to-endsequencelabelingviabi-directionalLSTM-CNNs-CRF. InProceedingsof
ACL,pages1064–1074,2016.
GuillaumeLample,MiguelBallesteros,SandeepSubramanian,KazuyaKawakami,andChrisDyer.Neuralarchitectures
fornamedentityrecognition. IntheNorthAmericanChapteroftheAssociationforComputationalLinguistics:
HumanLanguageTechnologies,pages260–270,2016.
SławomirDadas.Combiningneuralandknowledge-basedapproachestonamedentityrecognitioninpolish.InArtificial
IntelligenceandSoftComputing,pages39–50,2019.
MatthewE.Peters,MarkNeumann,MohitIyyer,MattGardner,ChristopherClark,KentonLee,andLukeZettlemoyer.
Deepcontextualizedwordrepresentations.InMarilynWalker,HengJi,andAmandaStent,editors,Proceedingsofthe
2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguage
Technologies,Volume1(LongPapers),pages2227–2237,NewOrleans,Louisiana,June2018a.Associationfor
ComputationalLinguistics. doi:10.18653/v1/N18-1202. URLhttps://aclanthology.org/N18-1202.
MatthewE.Peters,WaleedAmmar,ChandraBhagavatula,andRussellPower. Semi-supervisedsequencetaggingwith
bidirectionallanguagemodels. InProceedingsofthe55thAnnualMeetingoftheAssociationforComputationalLin-
guistics(Volume1: LongPapers),pages1756–1765,Vancouver,Canada,July2017.AssociationforComputational
Linguistics. doi:10.18653/v1/P17-1161. URLhttps://aclanthology.org/P17-1161.
MatthewE.Peters,MarkNeumann,MohitIyyer,MattGardner,ChristopherClark,KentonLee,andLukeZettlemoyer.
Deepcontextualizedwordrepresentations.InMarilynWalker,HengJi,andAmandaStent,editors,Proceedingsofthe
2018ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics:HumanLanguage
Technologies,Volume1(LongPapers),pages2227–2237,NewOrleans,Louisiana,June2018b.Associationfor
ComputationalLinguistics. doi:10.18653/v1/N18-1202. URLhttps://aclanthology.org/N18-1202.
KeironO’SheaandRyanNash. Anintroductiontoconvolutionalneuralnetworks. arXivpreprintarXiv:1511.08458,
2015.
TaoGui,RuotianMa,QiZhang,LujunZhao,Yu-GangJiang,andXuanjingHuang. Cnn-basedchinesenerwithlexicon
rethinking. Inijcai,volume2019,2019.
XinLi,ZequnJie,JiashiFeng,ChangsongLiu,andShuichengYan. Learningwithrethinking: Recurrentlyimproving
convolutionalneuralnetworksthroughfeedback. PatternRecognition,79:183–194,2018.
QileZhu,XiaolinLi,AnaConesa,andCécilePereira. Gram-cnn: adeeplearningapproachwithlocalcontextfor
namedentityrecognitioninbiomedicaltext. Bioinformatics,34(9):1547–1554,2018.
AlexSherstinsky. Fundamentalsofrecurrentneuralnetwork(RNN)andlongshort-termmemory(LSTM)network.
Physica D: Nonlinear Phenomena, 404:132306, mar 2020. doi:10.1016/j.physd.2019.132306. URL https:
//doi.org/10.1016%2Fj.physd.2019.132306.
25AsurveyonrecentadvancesinNER
JunyoungChung,CaglarGulcehre,KyungHyunCho,andYoshuaBengio. Empiricalevaluationofgatedrecurrent
neuralnetworksonsequencemodeling. arXivpreprintarXiv:1412.3555,2014.
RaghavendraChalapathy,EhsanZareBorzeshi,andMassimoPiccardi.Aninvestigationofrecurrentneuralarchitectures
fordrugnamerecognition. arXivpreprintarXiv:1609.07585,2016.
YueZhangandJieYang. Chinesenerusinglatticelstm. arXivpreprintarXiv:1805.02023,2018.
OriolVinyals,MeireFortunato,andNavdeepJaitly. Pointernetworks. Advancesinneuralinformationprocessing
systems,28,2015.
LingLuo,ZhihaoYang,PeiYang,YinZhang,LeiWang,HongfeiLin,andJianWang. Anattention-basedbilstm-crf
approachtodocument-levelchemicalnamedentityrecognition. Bioinformatics,34(8):1381–1388,2018.
YingLin,LiyuanLiu,HengJi,DongYu,andJiaweiHan. Reliability-awaredynamicfeaturecompositionforname
tagging. InProceedingsofthe57thannualmeetingoftheassociationforcomputationallinguistics,pages165–174,
2019.
PatrickKnobelreiter,ChristianReinbacher,AlexanderShekhovtsov,andThomasPock. End-to-endtrainingofhybrid
cnn-crfmodelsforstereo. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages
2339–2348,2017.
NaiqinFeng,XiuqinGeng,andLijuanQin. Studyonmrimedicalimagesegmentationtechnologybasedoncnn-crf
model. IEEEAccess,8:60505–60514,2020.
IgnazioGallo,ElisabettaBinaghi,MorenoCarullo,andNicolaLamberti. Namedentityrecognitionbyneuralsliding
window. In2008TheEighthIAPRInternationalWorkshoponDocumentAnalysisSystems,pages567–573.IEEE,
2008.
FeifeiZhai,SaloniPotdar,BingXiang,andBowenZhou. Neuralmodelsforsequencechunking. InProceedingsofthe
AAAIconferenceonartificialintelligence,volume31,2017.
OCDE. Ai language models. (352), 2023. doi:https://doi.org/https://doi.org/10.1787/13d38f92-en. URL https:
//www.oecd-ilibrary.org/content/paper/13d38f92-en.
AshishVaswani,NoamShazeer,NikiParmar,JakobUszkoreit,LlionJones,AidanNGomez,ŁukaszKaiser,andIllia
Polosukhin. Attentionisallyouneed. Advancesinneuralinformationprocessingsystems,30,2017.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional
transformersforlanguageunderstanding. arXivpreprintarXiv:1810.04805,2018.
StefanSchweterandJohannesBaiter. TowardsrobustnamedentityrecognitionforhistoricGerman. InProceedings
ofthe4thWorkshoponRepresentationLearningforNLP,pages96–103.ACL,2019. doi:10.18653/v1/W19-4312.
URLhttps://aclanthology.org/W19-4312.
KaiLabusch,ClemensNeudecker,andDavidZellhöfer. Bertfornamedentityrecognitionincontemporaryandhistoric
german. InKONVENS,2019b.
Martin Riedl and Sebastian Padó. A named entity recognition shootout for German. In Proceedings of the 56th
Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 120–125,
Melbourne, Australia, July 2018.Association forComputational Linguistics. doi:10.18653/v1/P18-2020. URL
https://aclanthology.org/P18-2020.
Rafika Boutalbi, Mira Ait-Saada, Anastasiia Iurshina, Steffen Staab, and Mohamed Nadif. Tensor-based graph
modularityfortextdataclustering. InProceedingsofthe45thInternationalACMSIGIRConferenceonResearch
andDevelopmentinInformationRetrieval,pages2227–2231,2022.
MiraAit-SaadaandMohamedNadif.Contextualwordembeddingsclusteringthroughmultiwayanalysis:Acomparative
study. InInternationalSymposiumonIntelligentDataAnalysis,pages1–14.Springer,2023a.
MiraAit-SaadaandMohamedNadif. Isanisotropytrulyharmful? acasestudyontextclustering. InProceedingsofthe
61stAnnualMeetingoftheAssociationforComputationalLinguistics(Volume2: ShortPapers),pages1194–1203,
2023b.
MohamedNadifandFrançoisRole. Unsupervisedandself-superviseddeeplearningapproachesforbiomedicaltext
mining. BriefingsinBioinformatics,22(2):1592–1603,2021.
JianLiu,LeiGao,SujieGuo,RuiDing,XinHuang,LongYe,QinghuaMeng,AsefNazari,andDhananjayThiruvady.
Ahybriddeep-learningapproachforcomplexbiochemicalnamedentityrecognition. Knowledge-BasedSystems,
221:106958,2021a.
HangYan,BocaoDeng,XiaonanLi,andXipengQiu.Tener:adaptingtransformerencoderfornamedentityrecognition.
arXivpreprint,2019.
26AsurveyonrecentadvancesinNER
ArjunChoudhry,PankajGupta,InderKhatri,AaryanGupta,MaximeNicol,Marie-JeanMeurs,andDineshKumar
Vishwakarma. Transformer-basednamedentityrecognitionforfrenchusingadversarialadaptationtosimilardomain
corpora. arXivpreprintarXiv:2212.03692,2022.
VictorSanh,LysandreDebut,JulienChaumond,andThomasWolf. Distilbert,adistilledversionofbert: smaller,faster,
cheaperandlighter. arXivpreprintarXiv:1910.01108,2019.
YinhanLiu,MyleOtt,NamanGoyal,JingfeiDu,etal. Roberta: Arobustlyoptimizedbertpretrainingapproach. arXiv
preprint,2019a.
Macarious Abadeer. Assessment of DistilBERT performance on named entity recognition task for the detec-
tion of protected health information and medical concepts. In Proceedings of the 3rd Clinical Natural Lan-
guageProcessingWorkshop,pages158–167,Online,November2020.AssociationforComputationalLinguistics.
doi:10.18653/v1/2020.clinicalnlp-1.18. URLhttps://aclanthology.org/2020.clinicalnlp-1.18.
RahulMehtaandVasudevaVarma. Llm-rmatsemeval-2023task2: Multilingualcomplexnerusingxlm-roberta. arXiv
preprintarXiv:2305.03300,2023.
TomBrown,BenjaminMann,NickRyder,MelanieSubbiah,JaredDKaplan,PrafullaDhariwal,ArvindNeelakantan,
PranavShyam,GirishSastry,AmandaAskell,etal. Languagemodelsarefew-shotlearners. Advancesinneural
informationprocessingsystems,33:1877–1901,2020.
NiklasMuennighoff,ThomasWang,LintangSutawika,AdamRoberts,StellaBiderman,TevenLeScao,MSaifulBari,
ShengShen,Zheng-XinYong,HaileySchoelkopf,etal. Crosslingualgeneralizationthroughmultitaskfinetuning.
arXivpreprintarXiv:2211.01786,2022.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,TimothéeLacroix,Baptiste
Rozière,NamanGoyal,EricHambro,FaisalAzhar,etal. Llama: Openandefficientfoundationlanguagemodels.
arXivpreprintarXiv:2302.13971,2023.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,YanqiZhou,WeiLi,and
PeterJ.Liu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer. JournalofMachine
LearningResearch,21(140):1–67,2020. URLhttp://jmlr.org/papers/v21/20-074.html.
ShuheWang,XiaofeiSun,XiaoyaLi,RongbinOuyang,FeiWu,TianweiZhang,JiweiLi,andGuoyinWang. Gpt-ner:
Namedentityrecognitionvialargelanguagemodels. arXivpreprintarXiv:2304.10428,2023.
Dhananjay Ashok and Zachary C Lipton. Promptner: Prompting for named entity recognition. arXiv preprint
arXiv:2305.15444,2023.
MdTahmidRahmanLaskar,MSaifulBari,MizanurRahman,MdAmranHossenBhuiyan,ShafiqJoty,andJimmyXi-
angjiHuang. Asystematicstudyandcomprehensiveevaluationofchatgptonbenchmarkdatasets. arXivpreprint
arXiv:2305.18486,2023.
YongilKim,YerinHwang,JoongboShin,HyunkyungBae,andKyominJung. Injectingcomparisonskillsintask-
orienteddialoguesystemsfordatabasesearchresultsdisambiguation. InFindingsoftheAssociationforComputa-
tionalLinguistics: ACL2023,pages4047–4065,2023.
FrancescoDeToni,ChristopherAkiki,JavierDeLaRosa,ClémentineFourrier,EnriqueManjavacas,StefanSchweter,
and Daniel Van Strien. Entities, dates, and languages: Zero-shot on historical texts with t0. arXiv preprint
arXiv:2204.05211,2022.
NiklasKammer,FlorianBorchert,SilviaWinkler,GerardDeMelo,andMatthieu-PSchapranow. Resolvingelliptical
compoundsingermanmedicaltext. InThe22ndWorkshoponBiomedicalNaturalLanguageProcessingandBioNLP
SharedTasks,pages292–305,2023.
PengLi,TianxiangSun,QiongTang,HangYan,YuanbinWu,XuanjingHuang,andXipengQiu. Codeie: Largecode
generationmodelsarebetterfew-shotinformationextractors. arXivpreprintarXiv:2305.05711,2023.
StefanHegselmann,AlejandroBuendia,HunterLang,MonicaAgrawal,XiaoyiJiang,andDavidSontag. Tabllm: Few-
shotclassificationoftabulardatawithlargelanguagemodels. InInternationalConferenceonArtificialIntelligence
andStatistics,pages5549–5581.PMLR,2023.
JoshuaRobinson,ChristopherMichaelRytting,andDavidWingate. Leveraginglargelanguagemodelsformultiple
choicequestionanswering. arXivpreprintarXiv:2210.12353,2022.
AmrHendy,MohamedAbdelrehim,AmrSharaf,VikasRaunak,MohamedGabr,HitokazuMatsushita,YoungJinKim,
MohamedAfify,andHanyHassanAwadalla. Howgoodaregptmodelsatmachinetranslation? acomprehensive
evaluation. arXivpreprintarXiv:2302.09210,2023.
YanHu,IqraAmeer,XuZuo,XueqingPeng,YujiaZhou,ZehanLi,YimingLi,JianfuLi,XiaoqianJiang,andHuaXu.
Zero-shotclinicalentityrecognitionusingchatgpt. arXivpreprintarXiv:2303.16416,2023.
27AsurveyonrecentadvancesinNER
EmilyAlsentzer,JohnRMurphy,WillieBoag,Wei-HungWeng,DiJin,TristanNaumann,andMatthewMcDermott.
Publiclyavailableclinicalbertembeddings. arXivpreprintarXiv:1904.03323,2019.
Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are
unsupervisedmultitasklearners. OpenAIblog,1(8):9,2019.
NoëmiAepliandMartinVolk. Reconstructingcompletelemmasforincompletegermancompounds. InLanguage
ProcessingandKnowledgeintheWeb: 25thInternationalConference,GSCL2013,Darmstadt,Germany,September
25-27,2013.Proceedings,pages1–13.Springer,2013.
MarkChen,JerryTworek,HeewooJun,QimingYuan,HenriquePondedeOliveiraPinto,JaredKaplan,HarriEdwards,
Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv
preprintarXiv:2107.03374,2021a.
DiegoMarcheggianiandIvanTitov. Encodingsentenceswithgraphconvolutionalnetworksforsemanticrolelabeling.
arXivpreprint,2017.
ThomasNKipfandMaxWelling. Semi-supervisedclassificationwithgraphconvolutionalnetworks. arXivpreprint,
2016.
AmeyaDaigavane,BalaramanRavindran,andGauravAggarwal. Understandingconvolutionsongraphs. Distill,6(9):
e32,2021.
AlbertoCetoli,StefanoBragaglia,AndrewDO’Harney,andMarcSloan. Graphconvolutionalnetworksfornamed
entityrecognition. arXivpreprint,2017.
RalphWeischedel,MarthaPalmer,MitchellMarcus,EduardSanh,etal. Ontonotesrelease5.0ldc2013t19. Linguistic
DataConsortium,Philadelphia,PA,23,2013.
XiaojingLiu,FeiyuGao,QiongZhang,andHuashaZhao. Graphconvolutionformultimodalinformationextraction
fromvisuallyrichdocuments. arXivpreprint,2019b.
IsmailHarrandoandRaphaëlTroncy. Namedentityrecognitionasgraphclassification. InTheSemanticWeb: ESWC
2021SatelliteEvents: VirtualEvent,June6–10,2021,RevisedSelectedPapers18,pages103–108.Springer,2021.
FuzhenZhuang,ZhiyuanQi,KeyuDuan,DongboXi,etal. Acomprehensivesurveyontransferlearning. Proceedings
oftheIEEE,109(1):43–76,2020.
ManaliShahaandMeenakshiPawar.Transferlearningforimageclassification.In2018secondinternationalconference
onelectronics,communicationandaerospacetechnology(ICECA),pages656–660.IEEE,2018.
DongWangandThomasFangZheng. Transferlearningforspeechandlanguageprocessing. In2015Asia-Pacific
SignalandInformationProcessingAssociationAnnualSummitandConference(APSIPA),pages1225–1237.IEEE,
2015.
HassanIsmailFawaz,GermainForestier,JonathanWeber,LhassaneIdoumghar,andPierre-AlainMuller. Transfer
learningfortimeseriesclassification. Ininternationalconferenceonbigdata(BigData),pages1367–1376,2018.
JiYoungLee,FranckDernoncourt,andPeterSzolovits. Transferlearningfornamed-entityrecognitionwithneural
networks. arXivpreprint,2017.
SumamFrancis,JordyVanLandeghem,andMarie-FrancineMoens. Transferlearningfornamedentityrecognitionin
financialandbiomedicaldocuments. Information,10(8):248,2019.
ZihanLiu,FeijunJiang,YuxiangHu,ChenShi,andPascaleFung. Ner-bert: Apre-trainedmodelforlow-resource
entitytagging. arXivpreprint,2021b.
HermenegildoFabregat,AndresDuque,JuanMartinez-Romo,andLourdesAraujo. Negation-basedtransferlearning
forimprovingbiomedicalnamedentityrecognitionandrelationextraction. JournalofBiomedicalInformatics,page
104279,2023.
XiangDaiandHeikeAdel. Ananalysisofsimpledataaugmentationfornamedentityrecognition. arXivpreprint,
2020.
RantoSawai,IncheonPaik,andAyatoKuwana.Sentenceaugmentationforlanguagetranslationusinggpt-2.Electronics,
10(24):3082,2021.
Huu-Thanh Duong and Tram-Anh Nguyen-Thi. A review: preprocessing techniques and data augmentation for
sentimentanalysis. ComputationalSocialNetworks,8(1):1–16,2021.
SaketSharma,AviralJoshi,NamrataMukhija,YiyunZhao,etal. Systematicreviewofeffectofdataaugmentation
usingparaphrasingonnamedentityrecognition. InNeurIPS2022WorkshoponSyntheticDataforEmpoweringML
Research.
28AsurveyonrecentadvancesinNER
AbdenacerKeraghel,KhalidBenabdeslem,andBrunoCanitia. Dataaugmentationprocesstoimprovedeeplearning-
basednertaskintheautomotiveindustryfield. InIJCNN,pages1–8,2020.
AkbarKarimi,LeonardoRossi,andAndreaPrati. Aeda: Aneasierdataaugmentationtechniquefortextclassification.
arXivpreprint,2021.
KangMinYoo,DongjuPark,JaewookKang,Sang-WooLee,andWoomyeongPark. Gpt3mix: Leveraginglarge-scale
languagemodelsfortextaugmentation. arXivpreprintarXiv:2104.08826,2021.
BurrSettles. Activelearningliteraturesurvey. 2009.
AdityaSiddhantandZacharyCLipton. Deepbayesianactivelearningfornaturallanguageprocessing: Resultsofa
large-scaleempiricalstudy. arXivpreprint,2018.
YanyaoShen,HyokunYun,ZacharyCLipton,YakovKronrod,andAnimashreeAnandkumar. Deepactivelearningfor
namedentityrecognition. arXivpreprint,2017.
Chengxi Yan, Xuemei Tang, Hao Yang, and Jun Wang. A deep active learning-based and crowdsourcing-assisted
solutionfornamedentityrecognitioninchinesehistoricalcorpora. AslibJournalofInformationManagement,
(ahead-of-print),2022.
YaqingWang,QuanmingYao,JamesTKwok,andLionelMNi. Generalizingfromafewexamples: Asurveyon
few-shotlearning. ACMcomputingsurveys(csur),53(3):1–34,2020.
AlexanderFritzler,VarvaraLogacheva,andMaksimKretov. Few-shotclassificationinnamedentityrecognitiontask.
InProceedingsofthe34thACM/SIGAPPSymposiumonAppliedComputing,pages993–1000,2019.
YutaiHou,WanxiangChe,YongkuiLai,ZhihanZhou,YijiaLiu,HanLiu,andTingLiu. Few-shotslottaggingwith
collapseddependencytransferandlabel-enhancedtask-adaptiveprojectionnetwork. arXivpreprint,2020.
Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural
informationprocessingsystems,30,2017.
YiYangandArzooKatiyar. Simpleandeffectivefew-shotnamedentityrecognitionwithstructurednearestneighbor
learning. arXivpreprint,2020.
LeyangCui,YuWu,JianLiu,SenYang,andYueZhang. Template-basednamedentityrecognitionusingbart. arXiv
preprint,2021.
MaximilianHofer,AndreyKormilitzin,PaulGoldberg,andAlejoNevado-Holgado. Few-shotlearningfornamedentity
recognitioninmedicaltext. arXivpreprint,2018.
JiaxinHuang,ChunyuanLi,KrishanSubudhi,DamienJose,etal. Few-shotnamedentityrecognition:Acomprehensive
study. arXivpreprint,2020.
HugoLarochelle,DumitruErhan,andYoshuaBengio. Zero-datalearningofnewtasks. InAAAI,volume1,page3,
2008.
ChristophHLampert,HannesNickisch,andStefanHarmeling. Attribute-basedclassificationforzero-shotvisualobject
categorization. IEEEtransactionsonpatternanalysisandmachineintelligence,36(3):453–465,2013.
ZhengmingDing,MingShao,andYunFu. Low-rankembeddedensemblesemanticdictionaryforzero-shotlearning.
InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages2050–2058,2017.
LedellWu,FabioPetroni,MartinJosifoski,SebastianRiedel,andLukeZettlemoyer. Scalablezero-shotentitylinking
withdenseentityretrieval. arXivpreprint,2019.
RashaObeidat,XiaoliFern,HamedShahbazi,andPrasadTadepalli. Description-basedzero-shotfine-grainedentity
typing. InProceedingsofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputational
Linguistics: HumanLanguageTechnologies,Volume1(LongandShortPapers),pages807–814,2019.
RamiAly,AndreasVlachos,andRyanMcDonald. Leveragingtypedescriptionsforzero-shotnamedentityrecognition
andclassification. InACL,pages1516–1528,2021.
NguyenVanHoang,SoerenHougaardMulvad,DexterNeoYuanRong,andYangYue. Zero-shotlearninginnamed-
entityrecognitionwithexternalknowledge. arXivpreprint,2021.
JianYang,ShaohanHuang,ShumingMa,YuweiYin,LiDong,DongdongZhang,HongchengGuo,ZhoujunLi,and
FuruWei. Crop: Zero-shotcross-lingualnamedentityrecognitionwithmultilinguallabeledsequencetranslation.
arXivpreprint,2022.
OpenAI. Openai: Aitoolsandmodels. https://www.openai.com,2024. Accessed: 2024-01-04.
MatthewHonnibalandInesMontani. spaCy2: NaturallanguageunderstandingwithBloomembeddings,convolutional
neuralnetworksandincrementalparsing. 2017.
29AsurveyonrecentadvancesinNER
StevenBird. Nltk: thenaturallanguagetoolkit. InProceedingsoftheCOLING/ACL2006InteractivePresentation
Sessions,pages69–72,2006.
ChristopherDManning,MihaiSurdeanu,JohnBauer,JennyRoseFinkel,StevenBethard,andDavidMcClosky. The
stanfordcorenlpnaturallanguageprocessingtoolkit. InACL,pages55–60,2014.
Ted Kwartler. The OpenNLP Project, pages 237–269. 05 2017. ISBN 9781119282013.
doi:10.1002/9781119282105.ch8.
RamiAl-Rfou,VivekKulkarni,BryanPerozzi,andStevenSkiena. Polyglot-ner: Massivemultilingualnamedentity
recognition. InSIAM,SDM,pages586–594.SIAM,2015.
AlanAkbik,TanjaBergmann,DuncanBlythe,KashifRasul,StefanSchweter,andRolandVollgraf. Flair: Aneasy-
to-use framework for state-of-the-art nlp. In Conference of the North American chapter of ACL, pages 54–59,
2019.
ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,etal. Transformers: State-of-the-artnaturallanguage
processing. InEMNLP,pages38–45,2020.
HamishCunningham. Gate: Aframeworkandgraphicaldevelopmentenvironmentforrobustnlptoolsandapplications.
InACL,pages168–175,2002.
Asahi Ushio and Jose Camacho-Collados. T-ner: an all-round python library for transformer-based named entity
recognition. arXivpreprint,2022.
NasserAlshammariandSaadAlanazi. Theimpactofusingdifferentannotationschemesonnamedentityrecognition.
EgyptianInformaticsJournal,22(3):295–302,2021.
MaojianChen,XiongLuo,HailunShen,ZiyangHuang,andQiaojuanPeng. Anovelnamedentityrecognitionscheme
forsteele-commerceplatformsusingalitebert. CMES-ComputerModelinginEngineering&Sciences,129(1),
2021b.
RalphGrishmanandBethSundheim. MessageUnderstandingConference-6: Abriefhistory. InCOLING1996Volume
1: The16thInternationalConferenceonComputationalLinguistics,1996b. URLhttps://aclanthology.org/
C96-1079.
GeorgeDoddington,AlexisMitchell,MarkPrzybocki,LanceRamshaw,StephanieStrassel,andRalphWeischedel. The
automaticcontentextraction(ACE)program–tasks,data,andevaluation. InProceedingsoftheFourthInternational
ConferenceonLanguageResourcesandEvaluation(LREC’04),Lisbon,Portugal,May2004.EuropeanLanguage
ResourcesAssociation(ELRA). URLhttp://www.lrec-conf.org/proceedings/lrec2004/pdf/5.pdf.
ErikFSangandFienDeMeulder. Introductiontotheconll-2003sharedtask: Language-independentnamedentity
recognition. arXivpreprint,2003.
JinlanFu,PengfeiLiu,andGrahamNeubig. Interpretablemulti-datasetevaluationfornamedentityrecognition. In
EMNLP,pages6058–6069,November2020.
JulioCesarSalinasAlvarado,KarinVerspoor,andTimothyBaldwin. Domainadaptionofnamedentityrecognitionto
supportcreditriskassessment. InProceedingsoftheAustralasianLanguageTechnologyAssociationWorkshop2015,
pages84–90,2015.
MiltonFriedman. Acomparisonofalternativetestsofsignificancefortheproblemofmrankings. TheAnnalsof
MathematicalStatistics,11(1):86–92,1940.
PeterBjornNemenyi. Distribution-freemultiplecomparisons. PrincetonUniversity,1963.
30