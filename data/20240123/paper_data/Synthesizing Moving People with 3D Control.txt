Synthesizing Moving People with 3D Control
BoyiLi JathushanRajasegaran YossiGandelsman AlexeiA.Efros JitendraMalik
UCBerkeley
AAccttoorr
Imitator
Actor
Imitator
Figure1.TheImitationGame:Givenavideoofaperson"TheActor",wewanttotransfertheirmotiontoanewperson"TheImitator".
Inthisfigure,thefirstrowshowsasequenceofframesoftheactor(MichelleKwan),doingherOlympics’98performance.Theinsetrow
showsthe3Dposesextractedfromthisvideo.Now,givenanysingleimageofanewpersonTheImitator,ourmodelcansynthesizenew
renderingsoftheimitator,tocopytheactionsoftheactorin3D.
Abstract posesoftheperson,includingclothing,hair,andplausiblein-
fillingofunseenregions. Thisdisentangledapproachallows
ourmethodtogenerateasequenceofimagesthatarefaithful
Inthispaper,wepresentadiffusionmodel-basedframe-
tothetargetmotioninthe3Dposeand,totheinputimage
workforanimatingpeoplefromasingleimageforagiven
intermsofvisualsimilarity. Inadditiontothat,the3Dcon-
target 3D motion sequence. Our approach has two core
trolallowsvarioussyntheticcameratrajectoriestorendera
components: a)learningpriorsaboutinvisiblepartsofthe
person. Ourexperimentsshowthatourmethodisresilientin
humanbodyandclothing,andb)renderingnovelbodyposes
generatingprolongedmotionsandvariedchallengingand
withproperclothingandtexture. Forthefirstpart,welearn
complexposescomparedtopriormethods. Pleasecheckour
anin-fillingdiffusionmodeltohallucinateunseenpartsofa
websiteformoredetails: 3DHM.github.io.
persongivenasingleimage. Wetrainthismodelontexture
map space, which makes it more sample-efficient since it
1.Introduction
is invariant to pose and viewpoint. Second, we develop a
diffusion-basedrenderingpipeline,whichiscontrolledby Given a random photo of a person, can we accurately an-
3Dhumanposes. Thisproducesrealisticrenderingsofnovel imate that person to imitate someone else’s action? This
4202
naJ
91
]VC.sc[
1v98801.1042:viXraproblemrequiresadeepunderstandingofhowhumanposes recoverhighlyaccuratedensecorrespondencesbetweenim-
changeovertime,learningpriorsabouthumanappearance ages and the body surface to provide dense human pose
andclothing. Forexample,inFigure1theActorcandoa estimation. However,itcannotreflectthetextureinforma-
diversesetofactions,fromsimpleactionssuchaswalking tionfromtheoriginalinputs. Comparedtothislineofwork,
andrunningtomorecomplexactionssuchasfightingand oursfullyutilizesthe3Dmodelstocontrolthemotion,by
dancing. For the Imitator, learning a visual prior about providinganaccuratedense3Dflowofthemotion,andthe
theirappearanceandclothingisessentialtoanimatethem texturemaprepresentationmakesiteasytolearnappearance
atdifferentposesandviewpoints. Totacklethisproblem, priorfromafewthousandsamples.
wepropose3DHM,atwo-stageframework(seeFigure2)
thatsynthesizes3DHumanMotionsbycompletingatexture
mapfromasingleimageandthenrenderingthe3Dhumans
2.RelatedWorks
toimitatetheactionsoftheactor.
Weusestate-of-the-art3Dhumanposerecoverymodel
4DHumans[9,19]forextractingmotionsignalsoftheactor, Controllable Human Generation. Human generation is
by reconstructing and tracking them over time. Once we notaneasytask. Unlikeimagetranslation[16],generating
haveamotionsignalin3D,asasequenceofmeshes,one differenthumansrequiresthemodeltounderstandthe3D
wouldthinkwecansimplyre-texturethemwiththetexture structureofthehumanbody. Givenarbitrarytextpromptsor
mapoftheimitatortogetanintermediaterenderingofthe poseconditions[5,15],weoftenfindoutthatexistinggener-
imitation task. However, this requires a complete texture ativemodelsoftengenerateunreasonablehumanimagesor
mapoftheimitator. Whengivenonlyasingleviewimage videos. Diffusion-HPC[31]proposesadiffusionmodelwith
of the imitator, we see only a part of their body, perhaps HumanPoseCorrectionandfindsthatinjectinghumanbody
thefrontside,orthebacksidebutneverbothsides. Toget structurepriorswithinthegenerationprocesscouldimprove
thecompletetexturemapoftheimitatorfromasingleview thequalityofgeneratedimages. ControlNet[34]isdesigned
image, welearnadiffusionmodeltoin-filltheunseenre- onneuralnetworkarchitecturetocontrolpre-trainedlarge
gionsofthetexturemap.Thisessentiallylearnsapriorabout diffusionmodelstosupportadditionalinputconditions,such
humanclothingandappearance. Forexample,afront-view as Openpose [6]. GestureDiffuCLIP [3] designs a neural
imageofapersonwearingablueshirtwouldusuallyhave network to generate co-speech gestures. However, these
thesamecolorattheback. Withthiscompletetexturemap, techniquesarenottailoredforanimatinghumans,whichcan-
now we can get an intermediate rendering of the imitator notguaranteetherequiredhumanappearanceandclothing.
doingtheactionsoftheactor. Intermediaterenderingmeans,
Synthesizing Moving People. Synthesizing moving peo-
wrappingthetexturemapontopoftheSMPL[17]meshto
ple is very challenging. For example, Make-a-Video [24]
getabody-tightrenderingoftheimitator.
or Imagen Video [23] could synthesize videos based on a
However,theSMPL[17]meshrenderingsarebody-tight giveninstruction. However,thegeneratedvideocannotac-
anddonotcapturedeformationsonclothing,likeskirtsor curatelycapturehumanpropertiescorrectlyandmaycause
varioushairstyles. Tosolvethis,welearnasecondmodel, the weird composition of generated humans. Prior meth-
that maps from mesh renderings to more realistic images, ods[8,29]learnpose-to-pixelsmappingdirectly. However,
bycontrollingthemotionwith3Dposes. Wefindoutsuch these designs could only be trained and used for one per-
a simple framework could successfully synthesize realis- son. RecentworkssuchasSMPLitex[7]considerhuman
tic and faithful human videos, particularly for long video textureestimationfromasingleimagetoanimateaperson.
generations. Weshowthatthe3Dcontrolprovidesamore However,thereisavisualgapbetweenrenderedpeoplevia
fine-grainedandaccurateflowofmotionandcapturesthe predictedtexturemapandrealhumans. Manyworksstart
visualsimilaritiesoftheimitatorfaithfully. todirectlypredictpixelsbasedondiffusionmodels,suchas
Whiletherehasbeenalotofworkonrewritingthemotion Dreampose[14]andDisCO[28]. DreamPoseiscontrolled
ofanactor[4,14,28], eachrequireseitherlargeamounts by DensePose [10], it aims to synthesize a video contain-
ofdata,supervisedcontrolsignals,orrequirescarefulcura- ingbothhumanandfabricmotionbasedonasequenceof
tionsofthetrainingdata. Forexample,Make-a-video[24] humanbodyposes. DisCOisdirectlycontrolledbyOpen-
cangeneratedecentresultswhileforhumanvideos,itoften pose[6],anditaimstoanimatethehumanbasedonthe2D
generatesincompleteornonconsequentialvideosandfails poseinformation. However,theapproachofaligningoutput
atfaithfulreconstructionofhumans. Someworks[8]use pixelsfortrainingregularizationoftenleadsthesemodelsto
Openpose[6]asintermediatesupervision. However,Open- becomeoverlyspecializedtocertaintrainingdata.Moreover,
poseprimarilycontainstheanatomicalkeypointsofhumans, thismethodologylimitsthemodels’generalizationcapabili-
itcannotbeusedtoindicatethebodyshape,depth,orother ties,astheyoftenperformwellonafewpeoplewhosedata
relatedhumanbodyinformation. DensePose[10]aimsto distributioncloselymatchesthatofthetrainingdataset.28-7， 15， 22
Stage 1 Stage 2
(Inpainting) (Rendering)
A photo of Predicted
a person Complete
(Imitator) Texture map
3D Poses Texture mapped Final Rendering
(from Actor) (Imitator) (Imitator)
Figure2.Overviewof3DHM:weshowanoverviewofourmodelpipeline.Givenanimageoftheimitatorandasequenceof3Dposes
fromtheactor,wefirstgenerateacompletefulltexturemapoftheimitator,whichcanbeappliedtothe3Dposesequencesextractedfrom
theactortogeneratetexture-mappedintermediaterenderingsoftheimitator.ThenwepasstheseintermediaterenderingstotheStage-2
modeltoprojecttheSMPLmeshrenderingtomorerealisticrenderingsofrealimages. Note: redboxesrepresentinputs,yellowboxes
representintermediatepredictionsfromstage1,andblueboxesrepresentthefinaloutputsfromstage2.Tocreateamovingpersonanimation
withvariabledurationandanynumberof3Dposes,itisonlynecessarytoexecutestage1onceinordertoacquireacompletetexturemap.
3.SynthesizingMovingPeople 3.1.TexturemapInpainting
Inthissection,wediscussourtwo-stageapproachforimitat- ThegoalofStage-1modelistoproduceaplausiblecomplete
ingamotionsequence.Our 3DHMframeworkembracesthe texturemapbyinpaintingtheunseenregionsoftheimitator.
advantageofaccurate3Dposepredictionfromthestate-of- Weextractapartiallyvisibletexturemapbyfirstrenderinga
the-artpredictingmodels4DHumans[9,19],whichcould 3Dmeshontotheinputimageandsamplecolorsforeach
accurately track human motions and extracts 3D human visibletrianglefollowing4DHumans[9].
posesoftheactorvideos. Foranygivenvideooftheactor Input. Wefirstutilizeacommonapproachtoinferpixel-
wewanttoimitate,weuse3Dreconstruction-basedtracking to-surfacecorrespondencestobuildanincompleteUVtex-
algorithmstoextract3Dmeshsequencesoftheactor. For turemap[7,32]fortexturing3DmeshesfromasingleRGB
theinpaintingandrenderingpart,werelyonthepre-trained image. Wealsocomputeavisibilitymasktoindicatewhich
StableDiffusion[22]model,whichisoneofthemostrecent pixelsarevisiblein3Dandwhichonesarenot.
classes of diffusion models that achieve high competitive Target. Sincetheobjectiveofthismodelingistogenerate
resultsovervariousgenerativevisiontasks. completetexturemaps,wegenerateapseudo-completetex-
Ourapproach3DHMiscomposedoftwocoreparts: In- turemapusingvideodata. Sincethe4DHumanscantrack
painting Diffusion for texture map in-painting as Stage-1 peopleovertime,itcontinuallyupdatesitsinternaltexture
and Rendering Diffusion for human rendering as Stage-2. maprepresentationsasamovingaverageofvisibleregions.
Figure2showsahigh-leveloverviewofourframework. In Howevertoproducemoresharpimages,forthegenerative
Stage-1, first, for a given single view image, we extract a taskwefoundthatamedianfilteringismoresuitablethan
roughestimateofthetexturemapbyrenderingthemeshes amovingaverage. Whilethistechniquecanbeappliedto
ontotheimageandassigningpixelstoeachvisiblemeshtri- anyvideo,inthisstagewerelyon2,205humanvideos. For
anglesuchthatwhenrenderedagainitwillproduceasimilar eachhumanvideo,wefirstextractapartialtexturemapfrom
imageastheinputimage. Thispredictedtexturemaphas eachframe.Sinceeachvideocontains360degreesofhuman
onlyvisiblepartsoftheinputimage. TheStage-1Diffusion views,wecalculateapseudo-completetexturemapfroma
in-paintingmodeltakesthispartialtexturemapandgener- wholevideoandsetitasthetargetoutputforStage1. In
atesacompletetexturemapincludingtheunseenregions. detail, we take the median overall visible parts of texture
Giventhiscompletedtexturemap,wegenerateintermediate mapsofavideo.
renderingsofSMPL[17]meshesanduseStage-2modelto Model. We finetune directly on the Stable Diffusion In-
projectthebody-tightrenderingstomorerealisticimages paintingmodel[21]thatshowsgreatperformanceonimage
withclothing. FortheStage-2Diffusionmodel,weapply completiontasks. Weinputapartialtexturemapandcorre-
3Dcontroltoanimatetheimitatortocopytheactionsofthe spondingvisibilitymaskandobtaintherecoveredpredicted
actor. mapforthehuman. Welockthetextencoderbranchandcvpr1 - 106
28,60， -03794
Stable Diffusion Input Latents
Visibility Mask
Inpainting (64 x 64) Texture mapped Input Image
(at time t) (t=0)
A photo of Predicted
a person Complete
(Imitator) Texture map StableDiffusion 3D Controllable
Extracted
Encoder Branch
Texture map
Figure3.Stage-1of3DHM:Inthefirststage,givenasingleview
imageofanimitator,wefirstapply4Dhumans[9]stylesampling
approachtoextractpartialtexturemapanditscorrespondingvisi- StableDiffusion
bilitymap.Thesetwoinputsarepassedtothein-paintingdiffusion Decoder
modeltogenerateaplausiblecompletetexturemap.Inthisexam-
ple,whileweonlyseethebackviewoftheimitator,themodel
wasabletohallucinateaplausiblefrontregionthatisconsistent decode latents
withtheirclothing. Output Latents Final Rendering
(64 x 64)
(at time t)
alwaysfeed‘realhuman’asinputtextoffixedStableDif- Figure4.Stage-2of3DHM:Thisfigureshowstheinferenceofour
Stage-1approach.Givenanintermediaterenderingoftheimitator
fusionmodels. WerefertoourtrainedmodelasInpainting
withtheposeoftheactorandtheactualRGBimageoftheimitator,
Diffusion. SeeFigure3forthemodelarchitecture.
ourmodelcansynthesizerealisticrenderingsoftheimitatoronthe
3.2.HumanRendering poseoftheactor.
InStage2,weaimtoobtainarealisticrenderingofahuman
imageofthepersonintoRenderingDiffusiontorenderthe
imitatordoingtheactionsoftheactor. Whiletheinterme-
humaninanovelposewitharealisticappearance.
diate renderings (rendered with the poses from the actor
and texture map from Stage-1) can reflect diverse human
Target: Sincewecollectedthedatabyassumingtheactor
motion, these SMPL mesh renderings are body-tight and
istheimitator,wehavethepaireddataoftheintermediate
cannotrepresentrealisticrenderingwithclothing,hairstyles,
renderingsandtherealRGBimages. Thisallowsustotrain
andbodyshapes. Forexample,ifweinputascenewhere
thismodelonlotsofdata,withoutrequiringanydirect3D
agirliswearingadressandsheisdancing,theintermedi-
supervision.
aterenderingsmightbeableto“dance"butitisimpossible
Model. SimilartoControlNet,wedirectlyclonetheweights
to animate the skirt with SMPL mesh rendering. To train
oftheencoderoftheStableDiffusion[20]modelasourCon-
this model, in a fully self-supervised fashion, we assume
trollablebranch("trainablecopy")toprocess3Dconditions.
theactoristheimitator,afterallagoodactorshouldbea
Wefreezethepre-trainedStableDiffusionandinputnoisyla-
goodimitator. Thisway,wecantakeanyvideo,andgeta
tents(64×64).Inthemeanwhile,weinputatexturemapped
sequenceofposesfrom4DHumans[9]andtakeanysingle
3Dhumanattimetandoriginalhumanphotoinputintoa
frame,andgetacompletetexturemapfromStage-1,thenget
fixedVAEencoderandobtaintexturemapped3Dhuman
theintermediaterenderingsbyrenderingthetexturemaps
latents(64×64)andappearancelatents(64×64)ascondi-
onthe3Dposes. Now,wehavepaireddataofintermediate
tioninglatents. Wefeedthesetwoconditioninglatentsinto
renderingsandrealRGBimages. Usingthis,wecollecta
RenderingDiffusionControllablebranch. Thekeydesign
largeamountofpaireddataandtrainourStage-2diffusion
principleofthisbranchistolearntexturesfromhumaninput
modelwithconditioning.
and apply them to the texture mapped 3D human during
Input: Wefirstapplythegeneratedtexturemap(fullycom- trainingthroughthedenoisingprocess. Thegoalistorender
plete)fromStage1toactor3Dbodymeshsequencestoan arealhumanwithvividtexturesfromthegenerated(texture
intermediaterenderingoftheimitatorperformingtheactions mapped) 3D human from Stage 1. We obtain the output
oftheactor. Noteatthistime,intermediaterenderingcan latentandprocessittothepixelspaceviadiffusionsteppro-
only reflect the clothing that fits the 3D mesh (body-tight cedureandfixedVAEdecoder. SametoStage1,welockthe
clothing) but fails to reflect the texture outside the SMPL textencoderbranchandalwaysfeed‘arealhumanisacting’
body,suchasthepuffed-upregionofaskirt,winterjacket,or asinputtextoffixedStableDiffusionmodels. Wereferto
hat. Toobtainthehumanwithcompleteclothingtexture,we our trained model as Rendering Diffusion. In Rendering
inputtheobtainedintermediaterenderingsandtheoriginal Diffusion,wepredictoutputsframebyframe. WeshowtheStage2workflowinFigure4. Method PSNR↑ SSIM↑ FID↓ LPIPS↓ L1↓
DreamPose 35.06 0.80 245.19 0.18 2.12e-04
4.Experiments DisCO 35.38 0.81 164.34 0.15 1.44e-04
Ours 36.18 0.86 154.75 0.12 9.88e-05
4.1.ExperimentalSetup
Dataset. We collect 2,524 3D human videos from Table 1. Quantitative comparison on frame-wise generation
2K2K [11], THuman2.0 [33] and People-Snapshot [2] quality:Wecompareourmethodwithpriorworksonposecon-
datasets. 2K2K is a large-scale human dataset with 3D ditiongenerationtasksandmeasurethegenerationqualityofthe
samples.
human models reconstructed from 2K resolution images.
THuman2.0contains500high-qualityhumanscanscaptured
byadenseDLSRrig. People-Snapshotisasmallerhuman
about2weekstorunourmodelontrainingdatasetsbased
datasetthatcaptures24sequences. Weconvertthe3Dhu-
on 8 NVIDIA A100 GPUs with a batch size of 4. As for
mandatasetintovideosandextract3Dposesfromhuman
inference,weonlyneedtorunStage-1oncetoreconstruct
videosusing4DHumans. Weuse2,205videosfortraining
thefulltexturemapoftheimitator,anditisusedforallother
andothervideosforvalidationandtesting.SeetheAppendix
novelposesandviewpoints. WerunStage-2inferencefor
formoredetailsonthedatasetdistributiononclothing.
each frame independently, however since the initial RGB
EvaluationMetrics. Weevaluatethequalityofgenerated
frameoftheimitatorisconditionedforallframes,theStage-
frames of our method with image-based and video-based
2 model is able to produce samples that are temporarily
metrics. Forimage-basedevaluation,wefollowtheevalua-
consistent.
tionprotocolofDisCO[28]toevaluatethegenerationqual-
ity. WereporttheaveragePSNR[13],SSIM[30],FID[12], 4.2.QuantitativeResults
LPIPS [35], and L1. For video-based evaluation, we use
Baselines. Wecompareourapproacheswithpastandstate-
FVD [26]. For pose evaluating 3D pose accuracy we use
of-the-artmethods: DreamPose[14],DisCo[28]andCon-
MPVPEandPA-MVPVE.MPVPE[18],orMeanPer-Vertex
trolNet[34](forposeaccuracycomparisons)1. Wesetinfer-
PositionError,isacriticalmetricin3Dhumanposeestima-
encestepsas50foralltheapproachesforfaircomparisons.
tion,whichquantifiestheaveragedistancebetweenpredicted
Comparisons on Frame-wise Generation Quality. We
andactual3Dverticesacrossamodel. Thismeasurement
compare 3DHMwithothermethodson2K2Ktestdataset,
is essential for evaluating the accuracy of 3D reconstruc-
whichiscomposedof50unseenhumanvideos,at256×256
tions and pose estimations, with a lower MPVPE indicat-
resolution. Foreachhumanvideo,wetake30framesthat
inghigherprecision. Complementingthis,PA-MPVPE,or
represent the different viewpoints of each unseen person.
Procrustes-Aligned Mean Per-Vertex Position Error, adds
Theanglesrangefrom0◦to360◦,wetakeoneframeevery
another dimension to this evaluation. It involves aligning
12◦tobetterevaluatethepredictionandgeneralizationabil-
thepredictedandgroundtruthdatausingProcrustesAnaly-
ity of each model. As for DisCO, we strictly follow their
sis,whichneutralizesdifferencesinorientation,scale,and
settingandextractOpenPoseforinference. AsforDream-
positionbeforecalculatingthemeanerror. Thisalignment
Pose,weextractDensePoseforinference. Weevaluatethe
allows PA-MPVPE to focus on the structural accuracy of
resultsandcalculatetheaveragescoreoverallframesofeach
predictions, making it a valuable metric for assessing the
video. Wesetthebackgroundasblackforallapproachesfor
relativepositioningofverticesinamodel,independentof
faircomparisons. Wereporttheaveragescoreoverallofthe
theirabsolutespatialcoordinates.
same50videosandshowthecomparisonsinTable1. We
ImplementationDetails. Asfortrainingallthedatasets,we
observethat3DHMoutperformsallthebaselinesindifferent
settheconstantlearningrateas5e-05andusethepre-trained
metrics.
diffusionmodelsfromdiffusers[27]forbothStage-1and
ComparisonsonVideo-levelGenerationQuality. Tover-
Stage-2. AsforStage1InpaintingDiffusion,wefinetuneon
ifythetemporalconsistencyof3DHM,wealsoreportthere-
StableDiffusionInpaintingmodels[21],whichhasan859M
sultsfollowingthesametestsetandbaselineimplementation
totalnumberoftrainableparametersand206Mtotalnumber
as in image-level evaluation. Unlike image-level compar-
ofnon-trainableparameters,sincetheVAEisfrozenduring
isons,weconcatenateeveryconsecutive16framestoform
thisstage. WetrainRenderingDiffusionfor50epochsandit
asampleofeachunseenpersononchallengingviewpoints.
takesabout2weekstorunourmodelonoursoupoftraining The angles range from 150◦ to 195◦, we take one frame
datasets. AsforStage2RenderingDiffusion,wetrainthe
ControllablebranchandfreezeStableDiffusionbackbones. 1Weutilizetheopen-sourceofficialcodeandmodelsprovidedbythe
Thetotalnumberoftrainableparametersinthiscaseis876M authorstoimplementthesebaselines.Weusediffusers[27]forControlNet
andOpenposeextraction, andDetectron2forDensePoseextractionfor
and the total number of non-trainable parameters is 1.1B.
DisCO.Since Chanetal.[8]canonlyworkforanimatingaspecificperson,
We train Rendering Diffusion for 30 epochs and it takes wedon’tcomparewithitinthispaper.Method FID-VID↓ FVD↓ Settings PSNR↑ SSIM↑ FID↓ LPIPS↓ L1↓
Default 36.18 0.86 154.75 0.12 9.88e-05
DreamPose 113.96 950.40
w/oTexturemap 35.00 0.78 237.42 0.20 2.35e-04
DisCO 83.91 629.18 w/oAppearanceLatents 36.07 0.86 167.58 0.12 1.03e-04
addingSMPLparameters 36.42 0.87 157.60 0.12 8.87e-05
Ours 55.40 422.38
Table4.AblationstudyofRenderingDiffusion.Wecomparethe
Table2.Quantitativecomparisononvideo-levelgenerationquality. frame-wisegenerationqualityunderdifferentsettings.Wenotice
bothtexturemapreconstructionandappearancelatentsarecritical
tothemodelperformance.
Method MPVPE↓ PA-MPVPE↓
DreamPose 123.07 82.75
DisCO 112.12 63.33 motionvideosinvariousscenarios. Weconsiderchallenging
ControlNet 108.32 59.80 3Dposesandmotionsfrom3sources: 3Dhumanvideos,
randomYouTubevideos,andtextinput.
Ours 41.08 31.86
PosesfromUnseen3DHumanVideos. Wetestourmodel
ondifferent3Dhumanvideoswithdifferenthumanappear-
Table3.Quantitativecomparisononposeaccuracy.
ancesand3Dposesfromthe2K2Kdataset. Weverifythat
the tested video has never appeared in training data. We
displaytheresultsinFigure5a.
every3◦tobetterevaluatethepredictionandgeneralization
Motions from Random YouTube Videos. We test our
abilityofeachmodel. Wereporttheaveragescoreoverall
modelonverydifferentmotionsfromrandomlydownloaded
of50videosandshowthecomparisonsinTable2. Weob-
Youtubevideosforanunseenhuman. Wedisplaytheresults
servethat3DHM,thoughtrainedandtestedbyperframe,
inFigure5b.
still embrace significant advantage over prior approaches,
MotionsfromTextInputs. Wetestourmodelonmotions
indicatingsuperiorperformanceonpreservingthetemporal
fromarbitrarytextprompts. Werandomlyinputanunseen
consistencywith3Dcontrol.
human photo and motions from random text inputs via a
ComparisonsonPoseAccuracy. Tofurtherevaluatethe
widelyusedhumanmotiongenerativemodel(MDM[25]).
validityofourmodel,weestimate3Dposesfromgenerated
WedisplaytheresultsinFigure5c.
humanvideosfromdifferentapproachesviaastate-of-the-
art3Dposeestimationmodel4DHumans. Weusethesame
5.AnalysisandDiscussion
datasetsettingmentionedaboveandcomparetheextracted
poseswith3Dposesfromthetargetvideos. Followingthe
5.1.AblationStudy
samecomparisonsettingswithgenerationquality,weevalu-
atetheresultsandcalculatetheaveragescoreoverallframes Tofurtherverifythecomponentsofourmethods,wetrain
ofeachvideo. BeyondDreamPoseandDisCO,wealsocom- ontrainingdatasetandtestontestdatasets. Weextractthe
pare with ControlNet, which achieves the state-of-the-art 3D rendered pose from these 50 test video tracks. Same
ingeneratingimageswithconditions,includingopenpose with the settings in quantitative comparison, we calculate
control. SinceControlNetdoesnotinputimages,weinput theaveragescoresofPSNR,SSIM,VGG,L1,LPIPSamong
thesamepromptsasours‘arealhumanisacting’andthe all the generated frames and targeted original frames and
correspondingopenposeasconditions. Wereporttheaver- reporttheresultsonbothframe-wisemetric(Table4),video-
agescoreoverallof50testvideosandshowthecomparisons levelmetric(Table5)andposeaccuracy(Table6). Wefind
in Table 3. We could notice that 3DHM could synthesize thatbothtexturemapreconstructionandappearancelatents
movingpeoplefollowingtheprovided3Dposeswithvery arecriticaltothemodelperformance. Also,wenoticethat
highaccuracy. Atthesametime,previousapproachesmight directly adding SMPL parameters into the model during
notachievethesameperformancebydirectlypredictingthe trainingmaynotbringimprovedperformanceconsidering
pose-to-pixel mapping. We also notice that 3DHM could allevaluationmetrics.Thisispresumablyduetotheimpreci-
achievesuperiorresultsonboth2Dmetricsand3Dmetrics, sionofSMPLparameters,whichcouldprovidecontradictory
evenifDisCOandControlNetarecontrolledbyOpenpose informationthroughoutthediffusiontrainingprocessifthey
andDreamPoseiscontrolledbyDensePose. arenotincorporatedcorrectly.
4.3.QualitativeResults 5.2.2DControland3DControl
Ourworkfocusesonsynthesizingmovingpeople,primar- We also compare the results of the official model from
ily for clothing and the human body. With the aid of 3D DreamPose and DisCO on a random person on a random
assistance,ourapproachhasthepotentialtoproducehuman realhumanphotowhichensuresdistinctdatadistribution.Various
Viewpoints
(a)3DHMwitharandomhumanphotoandarandom3Dposeofvariousviewpoints.Weshowthateveniftheperson’sphotoisfromasideangle,ourstage1
canhelpreconstructthefulltexturemap,whichcouldbeusedtoobtainfullbodyinformation.Stage2canaddtextureinformationbasedonagiveninput.
Motions from
random videos
(b)3DHMwitharandomhumanphotoandmotionsfromrandomYouTubeVideos.ThisexampleisfromGeneKelly’sdancingvideo.
A person turns to his right and paces back and forth.
Motions
from text
(c)3DHMwitharandomhumanphotoandmotionsgeneratedfromtextinputsbyMDM,aHumanMotionDiffusionModel[25].
Figure5.Qualitativeresultsondifferentviewpointsofthesamepose;motionsfromrandomvideosandmotionsfromtextinput.
Wedisplaythequalitativeresultsofvariousviewpointsin onmultiplepublicdatasetsforbettergeneralizabilitytoun-
Figure6. DreamPose,DisCO,and3DHMareallinitialize seen humans, still fails to synthesize people without the
theU-Netmodelwiththepre-trainedweightsofStableDif- target pose. We assumethis is because3DHM adds rigid
fusion. Wenoticethat3DHMcangeneralizewelltounseen 3Dcontroltobettercorrelatetheappearancetotheposes,
realhumansthoughitisonlytrainedbylimited3Dhumans. and preserve the body shape. Training with OpenPose or
SinceDreamPoserequiressubject-specificfinetuningofthe DensePosecannotguaranteethemappingbetweentextures
UNettoachievebetterresults,itcannotdirectlygeneralize andposes,whichmakesithardforthemodelstogeneralize.
wellonarandomhumanphoto. AsforDisCO,thoughithas
beentrainedwithaneffectivehumanattributepre-trainingInputs Various 3D Poses Various Viewpoints
DreamPose
DisCO
Ours
Figure6.Qualitativecomparisonwithother2Dcontrolapproachesonarandomrealhumanphoto(aKoreanactress).Weapplyvarious3D
posesorthesame3Dposefromdifferentviewpoints.Itcouldbenoticedthat2Dposesmaynotbeabletocapturethefoldingmotion,and
detailsofthehumanbody.Wecouldnoticethatourapproach3DHMcouldbridgethisgapwith3Dcontrol.
Method FID-VID↓ FVD↓ Method MPVPE↓ PA-MPVPE↓
Default 55.40 422.38 Default 41.08 31.86
w/oTexturemap 113.97 632.67 w/oTexturemap 92.94 59.18
w/oAppearanceLatents 93.21 715.51 w/oAppearanceLatents 41.99 32.82
addingSMPLparameters 72.35 579.90 addingSMPLparameters 39.16 29.67
Table5.AblationstudyofRenderingDiffusion.Wecomparethe Table6.AblationstudyofRenderingDiffusion.Wecomparethe
video-levelgenerationqualityunderdifferentsettings.Wenotice poseaccuracyunderdifferentsettings.
thatalthoughaddingSMPLparametersachievebetterperformance
onframe-wisesettingbutmayyieldworsetemporalconsistency
thandefaultsettings. ally,since3DHMistrainedonadatasetof2Kpeople,notall
thedetailedtexturescanbereconstructedcompletelyduring
inference(e.g. uniquelogosontheclothes). Wehypothesize
5.3.Limitations thiscouldbealleviatedbytrainingwithmorehumandata.
As3DHMgeneratestheframesofthehumanmotionvideos 6.Conclusion
independently,thereisnoguaranteeofconsistencyintime.
Forexample,theclothinglightmaychangebetweenconsec- In this paper, we propose 3DHM, a two-stage diffusion
utiveframes. Onepossiblesolutionistotrainthemodelto model-basedframeworkthatenablessynthesizingmoving
predictmultipleframessimultaneously. Anotherpossible peoplebasedononerandomphotoandtargethumanposes.
solutionistoconditionthegenerationprocessonpreviously Anotableaspectofourapproachisthatweemployacutting-
generatedframesviastochasticconditioning[1]. Addition- edge3Dposeestimationmodeltogeneratehumanmotiondata,allowingourmodeltobetrainedonarbitraryvideos
without necessitating ground truth labels. Our method is
suitableforlong-rangemotiongeneration,andcandealwith
arbitraryposeswithsuperiorperformanceoverpreviousap-
proaches.
Acknowledgement
We thank the Machine Common Sense project and ONR
MURI award number N00014-21-1-2801. We also thank
Google’sTPUResearchCloud(TRC)forprovidingcloud
TPUs. We thank Georgios Pavlakos, Shubham Goel, and
Loose-fitting Tight-fitting
JaneWufortheconstructivefeedbackandhelpfuldiscus-
sions. 1250
1000
Appendices
750
571
500
A.DatasetAnalysis
250
128
Figures7aand7bpresenttheclothingtypestatisticsofthe
7 2
trainingdata(2,205humans)andtestdata(50humans). We 0
Skirted Attire Suit Casual Wear Others
countpeoplebasedonfourclothingcategories:skirtedattire,
suit, casual wear, and others. In some cases, the clothing (a)Trainingdatadistribution.
belongs to skirted attire and suits or casual wear, we will Loose-fitting Tight-fitting
countthisasskirtedattire. Foreachclothingcategory,we
25
tallytwostyles: tight-fittingandloose-fitting.
Inthispaper,weonlytrainonlimitedhumanvideos,we 20
assumetrainingwithmorehumanvideoscouldlargelyboost
themodelgeneralizationonthefly.Giventhat3DHMmakes 15
use of a cutting-edge 3D pose estimation model and only
requireshumanvideoswithoutadditionallabelsfortraining, 10 8
it could be trained with numerous and any human videos 5
5
suchasmovies,etc.
0 0
0
B.3DHMTrainingFeatures Skirted Attire Suit Casual Wear Others
(b)Testingdatadistribution.
As has been mentioned in the paper, 3DHM is in a fully
self-supervisedfashion. Herewesummarizethekeytraining
Figure 7. Data distribution. We split the clothing type into 4
featuresofourapproach:
categories: skirtedattire,suit,casualwear,andothers. Wesplit
• 3DHM training pipeline (for both stages) is self- eachcategoryintotwotypes:looseandtight.Wereportthenumber
supervised. ofeachcategoryandtypeanddisplaytheoveralldistribution.We
• 3DHM does not use any additional annotations. It is couldnoticethatmostclothingiscasualwearandalargeportion
trainedwithpseudo-ground-truthasweusecutting-edge belongstotight-fitting.
softwarewhichcandetect,segment,trackand3Dfyhu-
mans(H4D).
• 3DHMisscalableanditsscalingcanbedonereadilyin
the future given additional videos of humans in motion
andcomputingresources.References by a two time-scale update rule converge to a local
nashequilibrium. Advancesinneuralinformationpro-
[1] Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu
cessingsystems,30,2017. 5
Salzmann, Lars Petersson, and Stephen Gould. A
[13] AlainHoreandDjemelZiou. Imagequalitymetrics:
stochasticconditioningschemefordiversehumanmo-
Psnrvs.ssim. In201020thinternationalconference
tionprediction. InProceedingsoftheIEEE/CVFCon-
onpatternrecognition,pages2366–2369.IEEE,2010.
ferenceonComputerVisionandPatternRecognition,
5
pages5223–5232,2020. 8
[14] Johanna Karras, Aleksander Holynski, Ting-Chun
[2] ThiemoAlldieck,MarcusMagnor,WeipengXu,Chris-
Wang,andIraKemelmacher-Shlizerman. Dreampose:
tian Theobalt, and Gerard Pons-Moll. Video based
Fashionimage-to-videosynthesisviastablediffusion.
reconstructionof3dpeoplemodels. InProceedingsof
arXivpreprintarXiv:2304.06025,2023. 2,5
theIEEEConferenceonComputerVisionandPattern
Recognition,pages8387–8397,2018. 5 [15] Sumith Kulal, Tim Brooks, Alex Aiken, Jiajun Wu,
Jimei Yang, Jingwan Lu, Alexei A. Efros, and Kr-
[3] TenglongAo,ZeyiZhang,andLibinLiu. Gesturedif-
ishna Kumar Singh. Putting people in their place:
fuclip:Gesturediffusionmodelwithcliplatents. arXiv
Affordance-awarehumaninsertionintoscenes. InPro-
preprintarXiv:2303.14613,2023. 2
ceedingsoftheIEEEConferenceonComputerVision
[4] Christoph Bregler, Michele Covell, and Malcolm
andPatternRecognition(CVPR),2023. 2
Slaney. Videorewrite: Drivingvisualspeechwithau-
dio. InSeminalGraphicsPapers: PushingtheBound- [16] BoyiLi,YinCui,Tsung-YiLin,andSergeBelongie.
aries,Volume2,pages715–722.2023. 2 Sitta:Singleimagetexturetranslationfordataaugmen-
tation. InEuropeanConferenceonComputerVision,
[5] TimBrooksandAlexeiAEfros. Hallucinatingpose-
pages3–20.Springer,2022. 2
compatiblescenes. InEuropeanConferenceonCom-
puterVision,2022. 2 [17] Matthew Loper, Naureen Mahmood, Javier Romero,
[6] Zhe Cao, Tomas Simon, Shih-En Wei, and Yaser Gerard Pons-Moll, and Michael J Black. Smpl: A
Sheikh. Realtime multi-person 2d pose estimation skinnedmulti-personlinearmodel. InSeminalGraph-
usingpartaffinityfields. InProceedingsoftheIEEE icsPapers: PushingtheBoundaries,Volume2,pages
conferenceoncomputervisionandpatternrecognition, 851–866.2023. 2,3
pages7291–7299,2017. 2 [18] Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu
[7] Dan Casas and Marc Comino Trinidad. Smplitex: Lee. Accurate 3d hand pose estimation for whole-
A generative model and dataset for 3d human tex- body 3d human mesh estimation. In Proceedings of
ture estimation from single image. arXiv preprint the IEEE/CVF Conference on Computer Vision and
arXiv:2309.01855,2023. 2,3 PatternRecognition,pages2308–2317,2022. 5
[8] Caroline Chan, Shiry Ginosar, Tinghui Zhou, and [19] Jathushan Rajasegaran, Georgios Pavlakos, Angjoo
AlexeiAEfros. Everybodydancenow. InProceedings Kanazawa,andJitendraMalik.Trackingpeoplebypre-
oftheIEEE/CVFinternationalconferenceoncomputer dicting3dappearance,locationandpose. InProceed-
vision,pages5933–5942,2019. 2,5 ingsoftheIEEE/CVFConferenceonComputerVision
[9] Shubham Goel, Georgios Pavlakos, Jathushan Ra- andPatternRecognition,pages2740–2749,2022. 2,3
jasegaran,AngjooKanazawa,andJitendraMalik. Hu- [20] RobinRombach,AndreasBlattmann,DominikLorenz,
mansin4D:Reconstructingandtrackinghumanswith PatrickEsser,andBjörnOmmer. High-resolutionim-
transformers. InICCV,2023. 2,3,4 agesynthesiswithlatentdiffusionmodels.2022ieee.
[10] RızaAlpGüler,NataliaNeverova,andIasonasKokki- InCVFConferenceonComputerVisionandPattern
nos. Densepose: Densehumanposeestimationinthe Recognition(CVPR),pages10674–10685,2021. 4
wild. InProceedingsoftheIEEEconferenceoncom- [21] RobinRombach,AndreasBlattmann,DominikLorenz,
putervisionandpatternrecognition,pages7297–7306, PatrickEsser,andBjörnOmmer. High-resolutionim-
2018. 2 agesynthesiswithlatentdiffusionmodels. InProceed-
[11] Sang-Hun Han, Min-Gyu Park, Ju Hong Yoon, Ju- ingsoftheIEEE/CVFConferenceonComputerVision
MiKang,Young-JaePark,andHae-GonJeon. High- andPatternRecognition(CVPR),pages10684–10695,
fidelity3dhumandigitizationfromsingle2kresolution 2022. 3,5
images. InProceedingsoftheIEEE/CVFConference [22] RobinRombach,AndreasBlattmann,DominikLorenz,
onComputerVisionandPatternRecognition(CVPR), PatrickEsser,andBjörnOmmer. High-resolutionim-
2023. 5 agesynthesiswithlatentdiffusionmodels. InProceed-
[12] MartinHeusel,HubertRamsauer,ThomasUnterthiner, ingsoftheIEEE/CVFConferenceonComputerVision
BernhardNessler,andSeppHochreiter. Ganstrained andPatternRecognition,pages10684–10695,2022. 3[23] ChitwanSaharia,WilliamChan,SaurabhSaxena,Lala [34] LvminZhangandManeeshAgrawala. Addingcondi-
Li,JayWhang,EmilyLDenton,KamyarGhasemipour, tionalcontroltotext-to-imagediffusionmodels. arXiv
RaphaelGontijoLopes,BurcuKaragolAyan,TimSal- preprintarXiv:2302.05543,2023. 2,5
imans, et al. Photorealistic text-to-image diffusion [35] Richard Zhang, Phillip Isola, Alexei A Efros, Eli
modelswithdeeplanguageunderstanding. Advances Shechtman, and Oliver Wang. The unreasonable ef-
inNeuralInformationProcessingSystems,35:36479– fectivenessofdeepfeaturesasaperceptualmetric. In
36494,2022. 2 ProceedingsoftheIEEEconferenceoncomputervi-
[24] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, sion and pattern recognition, pages 586–595, 2018.
Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, 5
OronAshual,OranGafni,etal. Make-a-video: Text-
to-video generation without text-video data. arXiv
preprintarXiv:2209.14792,2022. 2
[25] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir,
DanielCohen-or,andAmitHaimBermano. Human
motiondiffusionmodel. InTheEleventhInternational
ConferenceonLearningRepresentations,2023. 6,7
[26] ThomasUnterthiner,SjoerdVanSteenkiste,KarolKu-
rach, Raphael Marinier, Marcin Michalski, and Syl-
vain Gelly. Towards accurate generative models of
video: A new metric & challenges. arXiv preprint
arXiv:1812.01717,2018. 5
[27] Patrick von Platen, Suraj Patil, Anton Lozhkov, Pe-
dro Cuenca, Nathan Lambert, Kashif Rasul, Mishig
Davaadorj, and Thomas Wolf. Diffusers: State-of-
the-artdiffusionmodels. https://github.com/
huggingface/diffusers,2022. 5
[28] Tan Wang, Linjie Li, Kevin Lin, Chung-Ching Lin,
ZhengyuanYang,HanwangZhang,ZichengLiu,and
LijuanWang.Disco:Disentangledcontrolforreferring
humandancegenerationinrealworld. arXivpreprint
arXiv:2307.00040,2023. 2,5
[29] Ting-ChunWang,Ming-YuLiu,Jun-YanZhu,Guilin
Liu, Andrew Tao, Jan Kautz, and Bryan Catan-
zaro. Video-to-video synthesis. arXiv preprint
arXiv:1808.06601,2018. 2
[30] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and
EeroPSimoncelli. Imagequalityassessment: fromer-
rorvisibilitytostructuralsimilarity. IEEEtransactions
onimageprocessing,13(4):600–612,2004. 5
[31] ZhenzhenWeng,LauraBravo-Sánchez,andSerenaYe-
ung. Diffusion-hpc: Generatingsyntheticimageswith
realistic humans. arXiv preprint arXiv:2303.09541,
2023. 2
[32] XiangyuXuandChenChangeLoy. 3dhumantexture
estimationfromasingleimagewithtransformers. In
ProceedingsoftheIEEE/CVFinternationalconference
oncomputervision,pages13849–13858,2021. 3
[33] TaoYu, ZerongZheng, KaiwenGuo, PengpengLiu,
QionghaiDai,andYebinLiu. Function4d: Real-time
humanvolumetriccapturefromverysparseconsumer
rgbdsensors. InIEEEConferenceonComputerVision
andPatternRecognition(CVPR2021),2021. 5