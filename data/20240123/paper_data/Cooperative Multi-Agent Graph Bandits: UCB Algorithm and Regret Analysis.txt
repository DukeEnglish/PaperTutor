Cooperative Multi-Agent Graph Bandits: UCB Algorithm and Regret
Analysis
Phevos Paschalidis Runyu Zhang Na Li
Abstract—Inthispaper,weformulatethemulti-agentgraph observe an identical environment where their actions are
bandit problem as a multi-agent extension of the graph bandit independent of one another and there exist restrictions on
problem introduced in [1]. In our formulation, N cooperative
agent communication [6], [7], [8], [9], [10], [11], [12], [13].
agents travel on a connected graph G with K nodes. Upon
Other studies model the multi-agent extension such that
arrival at each node, agents observe a random reward drawn
from a node-dependent probability distribution. The reward two agents selecting the same action “collide” and observe
of the system is modeled as a weighted sum of the rewards no reward [14], [15], [16], [17]. Another line of related
the agents observe, where the weights capture the decreasing works focuses on the combinatorial bandit setting, where
marginal reward associated with multiple agents sampling the
a single centralized decision maker chooses a “super-arm”
samenodeatthesametime.WeproposeanUpperConfidence
from some feasibility set S⊆2[K] and receives a function of
Bound (UCB)-based learning algorithm, Multi-G-UCB, and
prove that√its expected regret over T steps is bounded by the random samples drawn from the arms that comprise the
O(Nlog(T)[ KT+DK]), where D is the diameter of graph G. super-armasareward[18],[19],[20],[21],[22].Intuitively,
Lastly, we numerically test our algorithm by comparing it to we could also consider each individual arm in the super-
alternative methods.
set as being chosen by a different agent, thus defining the
combinatorial bandit framework as a type of multi-agent
I. INTRODUCTION
MAB problem. Importantly, however, none of the multi-
TheMulti-ArmedBandit(MAB)problemisafundamental agent or combinatorial bandit works consider a graph-like
problem in the study of decision-making under uncertainty restriction on agent transitions.
[2], [3], [4], [5]. In its simplest formulation, the MAB
Our Contributions. Motivated by the discussion above,
consists of K independent arms each with an associated
in this paper, we formulate the multi-agent graph bandit
reward probability distribution and a single agent that plays
problem as a viable multi-agent extension of the graph
theMABinstanceforT turns.Oneachturn,theagentselects
bandit problem. Specifically, we consider N cooperative
one of the arms and receives a reward drawn from the arm’s
agents co-existing on the same undirected, connected graph
probability distribution. The agent’s goal is to maximize the
G with the objective of optimizing a total reward that is
expected reward received over the course of its T turns. In
a weighted sum of the rewards of arms that agents se-
order to do so, the agent must balance exploring arms it
lect, where the weight associated with each arm depends
knows little about and exploiting arms already demonstrated
on the number of agents selecting it. We then propose a
to give high rewards.
learning algorithm, Multi-G-UCB, which uses an Upper
One important limitation of the MAB framework is the
Confidence Bound (UCB)-based approach to managing the
assumption that at each time step the agent has access
exploration/exploitationtrade-off.Inparticular,thealgorithm
to all arms regardless of the its previous action. Earlier
maintains a confidence radius for each arm that shrinks
work introduces a graph bandit problem where the agent
proportionally to the number of times the arm has been
traverses an undirected, connected graph, receiving random
sampledandoptimisticallyassumesthatthemeanrewardfor
rewards upon arrival at each node [1]. The graph’s nodes
eacharmisthemaximumvaluewithinthisradius.Weshow
thusrepresenttheK armsandthelocalconnectivitybetween
that Multi-G-UCB achieves a regret upper bounded by
nodes imposes restrictions on which arms can be played (cid:0) (cid:2)√ (cid:3)(cid:1)
O Nlog(T) TK+DK ,whereDrepresentsthediameter
sequentially. This framework has applications in robotics:
ofthegraphG.Wealsodemonstrateviasimulationsthatour
considerastreet-cleaningrobotexploringanunknownphys-
proposed algorithm works well empirically, achieving lower
ical environment or a mobile sensor covering some space to
regret than several important benchmarks including a multi-
findthelocationthatreceivesthestrongestsignals.However,
agent version of G-UCB where each agent runs the G-UCB
[1]onlyconsidersthecaseofasingleagent,whereasinmost
algorithm in isolation.
robotic applications it is useful to consider multiple agents
exploring and exploiting the unknown graph together. The remainder of the paper is organized as follows.
There are many works extending the standard MAB In Section II, we formulate the multi-agent graph bandit
problem to a multi-agent setting. In most studies, agents problem and provide examples of potential applications.
SectionIIIintroducestheMulti-G-UCBalgorithm,Section
P. Paschalidis, R. Zhang, and N. Li are affiliated with IV provides the theoretical analysis of Multi-G-UCB, and
Harvard J. Paulson School of Engineering and Applied Sciences.
V details the experimental simulation results. The complete
ppaschalidis@college.harvard.edu, runyuzhang@fas.harvard.edu,
nali@seas.harvard.edu. regret proof can be found in the Appendix.
4202
naJ
81
]GL.sc[
1v38301.1042:viXraII. PROBLEMFORMULATION 1) Drone-Enabled Internet Access: Consider N coopera-
tive drones deployed over a network of K rural communities
A. Multi-Agent Graph Bandit
to provide internet access. Each drone can serve only one
We define the multi-agent graph bandit problem as fol- community per time period and, between time periods, only
lows. Consider an undirected, connected graph with self- has time to move from its current location to an adjacent
loops G=([K],E) and diameter D, where the diameter of community. At the end of each time period, the fleet of
a graph refers to the maximum distance between any two of drones receives a reward proportional to the communication
its nodes. For each node k∈{1,...,K}=[K], there exists traffic it serviced. Importantly, for any time period, the
an associated probability distribution for its reward, P k(·), demand of each community is sampled independently from
with support [0,1] and mean µ k; the nodes of the graph a distribution associated with that location; the demand of
thus represent the K arms of the bandit problem. Now, let a community at any time step is stochastic, but different
thereexistN individualagents,eachhavingknowledgeofthe communitieshavedifferentaverageinternetneeds.Themore
graph G and able to communicate with the other agents. Let agents positioned over a community, the more of its com-
T beanintegerrepresentingthenumberofstepsinthemulti- munication traffic they can service, but the marginal benefit
armed bandit problem. We initialize each agent i to start on decreases with each additional agent. The goal of the drone
some initial node k i,0. Then, at each time step t ≤T, each fleet is to maximize the total reward before the robots’
agent i chooses an arm k i,t such that (k i,t−1,k i,t)∈E. That batteriesrunoutandtheyarerecalled.Notethatthisproblem
is, the agent traverses an edge in the graph to a neighboring isalsorelatedtothe“sensorcoverage”formulationsof[23],
node/arm. For each time step t, we define c k,t as a count [24], [25], [26], [27].
of the number of agents sampling node k at time t, and 2) Factory Production: Consider a factory with N pro-
further defineC t =(c 1,t,...,c K,t). Then, for each k such that duction lines each of which can manufacture a total of K
c k,t >0, a reward X k,t is drawn independently from P k(·). different products, and assume that there are restrictions
The algorithm observes a system-wide reward of regarding which products can be manufactured sequentially
(depending on the raw materials required, for example).
R := ∑ f (c )X ,
t k k,t k,t Then, we can also model this problem as an instance of
k∈[K]
themulti-agentgraphbanditproblemwhereeachproduction
where for each arm k, f k is defined as a non-decreasing, line is an agent and the actions they take are the products
concave function with f k(0)=0 and f k(1)=1 representing they choose to manufacture at each time step. Each product
diminishing marginal reward for multiple selections of the k isassociatedwithsomerewarddependentonitsstochastic
same arm at the same time. Note that f k is not necessarily demand market as represented by the reward probability
equivalent to f k′ for k̸=k′. At the end of each time step t, distribution P k(·). The more production lines that choose
X k,t is revealed for all arms k with c k,t >0. The goal of the to manufacture product k, the greater the supply which
N agents is thus to travel the graph so as to maximize the negatively affects the price of the commodity; thus the
net expected reward received by the system of agents over diminishing marginal return for additional lines producing
the course of the T time steps. k as represented by f (·).
k
Equivalently, we can define the goal as a minimization of Arelatedprobleminwhichtransitioningbetweenproducts
expected regret. In order to do so, we first need to define is subjected not only to binary yes/no constraint but also
the optimal allocation of agents over the graph. Importantly, associated with some cost can be represented by a frame-
since the rewards the system observes at each node are work extremely similar to ours with the addition of non-
dependent only on the number of agents sampling the node uniform edge weights to G. The algorithm we introduce,
and not the location of specific agents, it suffices to specify Multi-G-UCB, can be modified to address this formula-
the optimal allocation only in terms of the count of agents tion with only a slight modification to the offline planning
at each node. We can thus define the set of possible agent component and the regret analysis (see Remarks 1 and 3).
allocationsasC={C=(c 1,...,c K): ∑ k∈[K]c k=N}andthe
optimal allocation
III. MULTI-AGENTGRAPHBANDITLEARNING
In this section, we present a learning algorithm for the
C∗:=argmax ∑ f (c )µ
k k k multi-agent graph bandit problem. In Section III-A we out-
C∈C k∈[K]
line the algorithm and provide intuition for its structure;
as the count vector that maximizes the expected reward of in Section III-B we explore in depth the offline planning
the system. We can then define the regret of the system at algorithm we use as a subroutine; and in Section III-C we
time t as discuss the algorithm’s initialization phase.
R := ∑ f (c∗)µ −R.
t k k k t A. Algorithm
k∈[K]
The formulation given in Section II introduces a few key
B. Examples
complications with respect to the current literature. If we
In this section, we provide two motivation examples that merely apply MAB algorithms to the graph setting, then at
fit our formulation. any time step the intended arm for an agent can be far awayon the action graph G, requiring multiple time steps of sub- samples, i.e. k , doubles; that is until n =2n . The
min kmin,t min
optimal actions as the agent traverses the path between the doubling scheme is a well-known technique in reinforce-
initial and desired arm. Furthermore, in comparison to the ment learning, e.g. [28]. We would like to remark that
single-agent graph bandit setting, the existence of multiple our specific choice of doubling scheme, doubling the least-
agentsandthenon-linearweightfunctions,the f (·)’s,neces- sampledarm,iscrucialintheregretanalysis.Otherschemes
k
sitates communication and sophisticated planning since the such as doubling the most-sampled arm, would lengthen
reward of each agent is dependent on the actions of other the exploitation phase upsetting the exploration/exploitation
agents. balanceofthealgorithm.Thisconclusionisfurthersupported
With these in mind, we introduce the Multi-G-UCB by the numerical results in Section V when we compare our
algorithm which attempts to minimize this transition cost by proposed doubling scheme withtwo others. The pseudocode
dividing the time horizon T into E episodes and emphasizes for Multi-G-UCB is given by Algorithm 1.
agent communication through communal UCB values. In
each episode, the algorithm performs four major steps: (i) Algorithm 1 Multi-G-UCB:
integrating agent information to compute the UCB values Input: The initial nodes for each agent, {k } . The
i,0 i∈[N]
foreachnode,(ii)calculatingthedesiredallocationusingthe offline planning algorithm SPMatching (see Section
UCB values, (iii) transitioning to the destination allocation III-B) that computes the optimal policy given the graph
using an offline planning algorithm, SPMatching, and (iv) G and a set of computed UCB values, {U } , for
k,te k∈[K]
collectingnewsamplesatthedestinationnodesuntilsamples each node.
double. 1: e←0
Specifically, Multi-G-UCB calculates for each arm the 2: Run the initialization algorithm to visit each vertex in
mean observed reward {µˆ k,te} a∈[K] as well as a confidence G. See Section III-C.
radius and upper confidence bound 3: while coordinator has not received a stopping signal do
(cid:115) 4: e←e+1
2log(t )
b
k,te
:=
n
e and U
k,te
:=µˆ k,te+n k,te, (1) 5: Calculate the UCB values {U k,te} k∈[K].
k,te 6: Solvetheoptimizationproblemin(3)forthedesired
allocationCˆ .
where we denote by t the time at which episode e starts e
e
and by n the number of times arm k has been sampled in 7: Denotebyk minthearmwithfewestobservedsamples
k,te
of all arms k such that cˆ >0 and denote by n its
the first t time steps. Importantly, in calculating the mean k,e min
e
current number of samples (n =n ).
r re ew wa ar rd d, dt ih ste ribal ug to ior nith rm athu erse ts hat nhe thX ek o,t be sd err vaw edn refr wo am rdt whe eigar hm te’ ds 8: Follow the offline plannim ni gn polk im cyin,te SPMatching
until agents are distributed according toCˆ .
by f (·) and utilizes the collective experience of all agents. e
k
Similarly, we update the sample counts as 9: Have each agent i continue to collect rewards at its
current node until n =2n .
(cid:40) kmin,t min
n k,t, if c k,t =0, 10: end while
n := (2)
k,t+1
n +1, otherwise,
k,t
so that they reflect the number of samples drawn from the B. Offline Planning
reward distribution.
The goal of the offline planning subroutine is to tran-
In order to calculate the optimal allocation of agents,
sition from the current state to one in which the agents
Multi-G-UCB optimistically assumes that the true mean are distributed according to Cˆ while incurring the least
e
reward of each arm is equal to its upper confidence bound.
regret possible. In the single-agent setting, the “allocation”
Thus, we define the estimated optimal count vector of
of agents will always consist of just a single destination
episode e as
node,andthusthistransition-of-least-regretisequivalenttoa
Cˆ :=argmax ∑ f (c )U . (3)
shortestpathproblemwheretheweightofeachedge(k′,k)∈
e
C∈C k∈[K]
k k k,te
E is the estimated regret of arm k. More formally, these
paths—which we call regret shortest paths—are shortest
We also define k min as the arm with the fewest observed paths on a graph G′ =([K],E,W) where the weights in W
samplesamongallarmsksuchthatcˆ >0andn =n
k,e min kmin,te are individually defined as
as its current number of samples. These quantities will be
important in defining the length of our episode. w(k′,k)=max(U −U ). (4)
v,te k,te
Following the calculation of Cˆ , the algorithm relies on v∈[K]
e
an offline planning sub-routine, SPMatching, defined in Importantly, we only allow paths of maximum length D,
Section III-B to transition the system of agents from the noting that each pair of vertices is assured to have such a
current state to the desired one. Following this transition path by definition of our graph diameter.
phase, the algorithm exploits the desired state by repeat- In our multi-agent setting, the solution is more compli-
edly sampling the (potentially suboptimal) allocation until cated.Weproposeapolynomial-timepseudo-solution,which
the number of samples of the arm with the fewest prior wecallSPMatching.Inparticular,wedefineS =([K],Cˆ )
e eas a multiset of arms such that the multiplicity of each arm will see in Section IV that the initialization phase does not
k∈[K] is equal to cˆ . We then create a complete bipartite contribute to the asymptotic regret of the system.
k,e
graph G =([N],S ,E ) where the weight of each (i,k)∈E Remark 2 (Algorithmic Extension to Directed Graphs):
B e b b
is the length of the regret shortest path between the current We remark that the initialization phase is the only time that
location of agent i and arm k. We can then calculate the our algorithm uses the assumption that the original graph
minimum weighted perfect matching of G to assign agents G is undirected. This assumption is necessary for the DFS
B
a corresponding arm in S and instruct each agent to follow algorithm, which relies on backtracking. Therefore, given a
e
the shortest path towards that arm [29]. The pseudocode for different initialization technique, Multi-G-UCB could also
SPMatching is given in Algorithm 2. be applied to a multi-agent graph problem with a directed
We call SPMatching a pseudo-solution because there G.
are no theoretical guarantees on its minimization of regret
IV. MAINRESULTS
during the transition phase. Our restriction on the physical
path length of any regret shortest path means that our paths Havingdefinedouralgorithm,wenowturntoatheoretical
may be suboptimal, but even without this constraint the analysis of its regret. We summarize our findings in the
regret shortest paths do not account for the actions of the Theorem 1.
otheragents.Inparticular,iftheoptimalpathsoftwoagents Theorem 1 (Regret of Multi-G-UCB): Let T ≥ 1 be
intersect at some node, then the reward observed at that anypositiveinteger.Givenaninstanceofamulti-agentgraph
node is potentially less than the reward estimated by the banditproblemwithN agents,K arms,andagraphdiameter
shortest path calculations. This increases regret in a manner of D, the expected system-wide regret of Multi-G-UCB
unanticipatedbytheofflineplanningalgorithm.Asweseein after taking a total of T steps (including initialization) is
Section IV, the sub-optimality of our transition phase does bounded by
not adversely affect our regret since its worst-case scenario (cid:34) (cid:35)
T (cid:16) (cid:104)√ (cid:105)(cid:17)
is equivalent to the worst-case of an optimal solution. E ∑R ≤O Nlog(T) KT+DK . (5)
t
t=1
Algorithm 2 SPMatching
Note that our result matches the regret bound for single-
Input: Thecurrenttimet,thecurrentpositionoftheagents,
{k } , and a desired allocation of agents,Cˆ . agent graph bandit [1] when N=1 (with a subtle difference
i,t i∈[N] e onthedependencyonthelogT factor).Itisalsoworthnoting
1: CalculatethemultisetofdestinationarmsS e=([K],Cˆ e).
that the regret in our multi-agent graph bandit formulation
2: For each agent m calculate the regret shortest path
exhibits a linear growth with respect to the number of
between its current location and each of the destination
agents N, which stands in contrast to the results of other
arms.
algorithms(e.g.[22])thatsolvemulti-agentMABwithregret
3: Initialize a complete bipartite graph G B =([N],S e,E B) O(√ KNTlogT). This discrepancy arises primarily from our
where the weight of each edge (i,k)∈E is defined as
B considerationoftheconcaveweightfunctions f forthetotal
thelengthoftheregretshortestpathbetweenthecurrent k
objective which complicates the analysis, as opposed to the
location of i and the destination arm k.
unweightedsetting(i.e. f (·)=1)consideredin[22].Indeed,
4: Calculate the minimum weight perfect matching of G B.
in the case of the
unweik
ghted setting, it can be shown that
5: Have each agent follow the regret shortest path to their √
our algorithm also achieves regret proportional to N. It is
assigned destination node.
an interesting open question whether we can further tighten
the bound on the dependency on N for general f (·).
k
Remark 1 (Algorithmic Extension to Weighted Graphs):
In order to extend Multi-G-UCB to the case of weighted Proof Ideas. The detailed proof is deferred to the Ap-
arm graphs G as discussed in Section II-B.2 we would need pendix. Here we outline the main ideas. In order to prove
only to change the definition of the regret shortest paths. Theorem 1, we analyze the regret of the system by episode,
In particular, if the cost of transition from arm k′ to arm k noting that since our algorithm doubles the samples of at
was α(k′,k), then by redefining the weight of edge (k′,k) least one arm each episode we can bound the number of
in G′ as w(k′,k)=α(k′,k)+max v∈[K](U v,te−U k,te) we fully episodes as Klog(T) (see Lemma 1 in the Appendix). For
account for these transition costs. each episode, we define the “good” event as
C. Initialization G e={∀k∈[K],µ k∈[µˆ k,te−b k,te,µˆ k,te+b k,te]}. (6)
In order for the algorithm to begin in earnest, it needs This is the event where the true mean of each arm is
to have observed at least one reward from each arm so that within the arm’s confidence radius of its estimated mean.
theupperconfidenceboundsarewell-defined.Therefore,we The “bad” event is thus the probability that at least one of
begin the algorithm with an initialization episode in which our confidence bounds fails. Following this decomposition,
eachagentrunsanindependentgraphexplorationalgorithm, ourregretproofconsistsoffoursteps:(1)boundingtheregret
Depth-FirstSearch(DFS),untilallverticeshavebeenvisited contribution of the bad episodes, (2) bounding the transition
at least once [30]. Despite the potential redundancy, we phase of the good episodes, (3) bounding the exploitationphase of the good episodes, and (4) bounding the regret of Combining these four steps gives the desired regret upper
initialization. bound.NotethatSteps1and4areasymptoticallydominated
Step1:RegretContributionofBadEvents.WeuseHo- by Steps 2 and 3 so that our final regret bound is the sum
effding’s inequality to bound the regret of the bad episodes of the transition phase and exploitation phase regret.
by showing that for each episode e the probability that any Remark 3 (Regret Consideration of Weighted Graphs):
single confidence radius fails is 2t−3. We then note that for The consequences of a weighted G to the theoretical
e
anysingletimestepwecanincurregretnomorethanNsince analysis would manifest in Steps 1, 3, and 4. In particular,
our rewards are bounded in [0,1] and the weight functions theassumptionthateachtimestepincursaworst-caseregret
concave,sousingaunionboundoverthearmsandsumming of N would need to be updated to reflect the transition
over all episodes gives a final regret contribution of costs potentially incurred by the agents. So long as these
costs are bounded by some constant B, we conclude that at
O(N(log(K)+loglog(T)). (7)
each time step the regret incurred is no more than (1+B)N
Step 2: Regret of Transition Phase. As discussed, we which does not effect our asymptotic regret bound.
dividethegoodepisodesintotwophases.Thefirstphase,the
V. NUMERICALSIMULATIONS
transition phase, is when the agents move along the graph
To test the performance of our model experimentally, we
to their assigned destination nodes. Since this takes at most
construct a synthetic multi-agent graph bandit problem with
D steps—recall our restriction on the regret shortest paths—
K = 300 arms and N = 20 agents. The graph, which we
and at each time step we incur a regret of no more than N,
display in Figure 1, is initialized as an Erdos-Renyi graph
we can bound the total transition cost after summing over
with probability parameter 0.05. Each agent is assigned a
all episodes by
random start vertex. The reward distribution for each arm is
O(Nlog(T)DK). (8)
distributed as N(µ,σ2) with µ chosen uniformly from the
Note that since we assume the worst possible matching, the interval (0.25,0.75) and σ2 =0.06. For each arm k∈[K],
fact that our offline planning sub-routine SPMatching is we also define the concave function
only a pseudo-solution does not affect our theoretical regret log ( x + 1 )+1
analysis. f k(x)= log2+k (2 10 +2+ 1k )+1.
Step 3: Regret of Exploitation Phase. The second phase 2+k 20 2+k
of the good episodes is the exploitation phase, when the In addition to Multi-G-UCB, we define three benchmark
system repeatedly samples from the estimated optimal arm algorithms. Multi-G-UCB-median is a variation where
allocation represented by Cˆ . In analyzing this phase, we for each episode we sample the estimated optimal arm allo-
e
make use of our definitions for b andU to simplify the cationuntilthearmwiththemediannumberofpriorsamples
k,te k,te
regret at each time step as has its sample count doubled (rather than the minimum).
Multi-G-UCB-max is defined similarly but uses the arm
∑ (cid:2) f k(c∗ k)−f k(cˆ k,e)(cid:3) µ k with the maximum number of prior samples as the baseline
k∈[K] foreachepisode.Lastly,Indv-G-UCBisderivedbyhaving
≤2 ∑ f k(cˆ k,e)b k,te+ ∑ (cid:2) f k(c∗ k)−f k(cˆ k,e)(cid:3) U k,te eachagentindividuallyperformtheG-UCBalgorithmof[1]
k∈[K] k∈[K] without communication and coordination. Figure 2 displays
≤2 ∑ f (cˆ )b , (9) thecumulativeregretofeachalgorithmasafunctionoftime
k k,e k,te
k∈[K] averaged over 10 independent trials each with time horizon
T =1.5·105.
where the last inequality follows from definition of Cˆ as
e
the count vector that maximizes ∑
k (cid:112)∈K
f k(c k)U k,te. (cid:112)Then, we
simplify this sum by extracting a 2log(t )≤ 2log(T)
e
from each confidence radius and showing
f (cˆ ) cˆ N
∑ √k k,e ≤ ∑ √k,e ≤ √ (10)
n n n
k∈[K] k,te k∈[K] k,te min
by noting that for each destination arm k, n ≥n and
k,te min
that ∑ k∈[K]cˆ k,e≤N. A final bound of
√
O(Nlog(T) NK) (11)
Fig.1. ThegraphG. Fig. 2. The average cumulative regret of
follows when we sum over all episodes since for each eachalgorithmacross10trials.
episode e the exploitation phase cannot exceed n steps.
min
Step 4: Initialization Cost. Finally, the initialization cost As expected, all three of the cooperative algorithms
contributes just O(NK) to the regret since each of N agents considerably outperform Indv-G-UCB. Multi-G-UCB
runs DFS until the K vertices have been visited. also outperforms its two variations both of whom over-
prioritize exploitation and under-prioritize exploration. ForMulti-G-UCB-max in particular, once one arm has been [15] K.LiuandQ.Zhao,“Distributedlearninginmulti-armedbanditwith
shown to be part of the optimal set it becomes increasingly multiple players,” IEEE Transactions on Signal Processing, vol. 58,
no.11,p.5667–5681,Nov2010.
harder to find the other arms since the episode lengths
[16] P.-A.Wang,A.Proutiere,K.Ariu,Y.Jedra,andA.Russo,“Optimal
continuetoincreaseevenwhenincludingrelativelyunknown algorithms for multiplayer multi-armed bandits,” in Proceedings of
arms in the destination allocation. the Twenty Third International Conference on Artificial Intelligence
andStatistics. PMLR,Jun2020,p.4120–4129.[Online].Available:
VI. CONCLUSIONSANDFUTUREWORK https://proceedings.mlr.press/v108/wang20m.html
[17] E. Boursier and V. Perchet, “Sic-mmab: Synchronisation involves
In this paper, we define the multi-agent graph bandit communication in multiplayer multi-armed bandits,” Advances in
problem, propose a learning algorithm, Multi-G-UCB, NeuralInformationProcessingSystems,vol.32,2019.
provide a theoretical analysis of its regret and present ex- [18] W.Chen,Y.Wang,andY.Yuan,“Combinatorialmulti-armedbandit:
General framework and applications,” in Proceedings of the 30th
perimental results that validate its performance. We believe International Conference on Machine Learning. PMLR, Feb 2013,
thattheformulationmulti-agentgraphbanditproblemopens p. 151–159. [Online]. Available: https://proceedings.mlr.press/v28/
up many interesting future directions, such as extending our chen13a.html
[19] W. Chen, Y. Wang, Y. Yuan, and Q. Wang, “Combinatorial multi-
algorithm to accommodate decentralized learning scenarios,
armed bandit and its extension to probabilistically triggered arms,”
generalizing to more complicated case where rewards of TheJournalofMachineLearningResearch,vol.17,no.1,pp.1746–
arms are correlated across time and/or across arms, as well 1778,2016.
[20] S. Wang and W. Chen, “Thompson sampling for combinatorial
as performing more comprehensive theoretical studies such
semi-bandits,” in Proceedings of the 35th International Conference
as analyzing instance-dependent regret. on Machine Learning. PMLR, Jul 2018, p. 5114–5122. [Online].
Available:https://proceedings.mlr.press/v80/wang18a.html
REFERENCES [21] Y. Gai, B. Krishnamachari, and R. Jain, “Combinatorial network
optimizationwithunknownvariables:Multi-armedbanditswithlinear
[1] T.Zhang,K.Johansson,andN.Li,“Multi-armedbanditlearningona
rewards and individual observations,” IEEE/ACM Transactions on
graph,”in202357thAnnualConferenceonInformationSciencesand
Networking,vol.20,no.5,p.1466–1478,Oct2012.
Systems(CISS). IEEE,2023,pp.1–6.
[22] B. Kveton, Z. Wen, A. Ashkan, and C. Szepesvari, “Tight regret
[2] T. Lattimore and C. Szepesva´ri, Bandit algorithms. Cambridge
bounds for stochastic combinatorial semi-bandits,” in Proceedings
UniversityPress,2020.
of the Eighteenth International Conference on Artificial Intelligence
[3] A.Slivkinsetal.,“Introductiontomulti-armedbandits,”Foundations
and Statistics. PMLR, Feb 2015, p. 535–543. [Online]. Available:
andTrends®inMachineLearning,vol.12,no.1-2,pp.1–286,2019.
https://proceedings.mlr.press/v38/kveton15.html
[4] R.S.SuttonandA.G.Barto,Reinforcementlearning:Anintroduction.
[23] J. Cortes, S. Martinez, T. Karatas, and F. Bullo, “Coverage control
MITpress,2018.
for mobile sensing networks,” IEEE Transactions on robotics and
[5] S. Bubeck, N. Cesa-Bianchi, et al., “Regret analysis of stochastic
Automation,vol.20,no.2,pp.243–255,2004.
and nonstochastic multi-armed bandit problems,” Foundations and
[24] V.RamaswamyandJ.R.Marden,“Asensorcoveragegamewithim-
Trends®inMachineLearning,vol.5,no.1,pp.1–122,2012.
provedefficiencyguarantees,”in2016AmericanControlConference
[6] J.Zhu,R.Sandhu,andJ.Liu,“Adistributedalgorithmforsequential
(ACC). IEEE,2016,pp.6399–6404.
decision making in multi-armed bandit with homogeneous rewards,”
[25] X. Sun, C. G. Cassandras, and X. Meng, “A submodularity-based
in 2020 59th IEEE Conference on Decision and Control (CDC).
approach for multi-agent optimal coverage problems,” in 2017 IEEE
Jeju, Korea (South): IEEE, Dec 2020, p. 3078–3083. [Online].
56th Annual Conference on Decision and Control (CDC). IEEE,
Available:https://ieeexplore.ieee.org/document/9303836/
2017,pp.4082–4087.
[7] M. Chakraborty, K. Y. P. Chua, S. Das, and B. Juba, “Coordinated
[26] M.Prajapat,M.Turchetta,M.Zeilinger,andA.Krause,“Near-optimal
versusdecentralizedexplorationinmulti-agentmulti-armedbandits,”
multi-agent learning for safe coverage control,” Advances in Neural
in Proceedings of the Twenty-Sixth International Joint Conference
InformationProcessingSystems,vol.35,pp.14998–15012,2022.
on Artificial Intelligence. Melbourne, Australia: International Joint
[27] V.Ramaswamy,D.Paccagnan,andJ.R.Marden,“Multiagentmaxi-
ConferencesonArtificialIntelligenceOrganization,2017,p.164–170.
mumcoverageproblems:Thetradeoffbetweenanarchyandstability,”
[Online].Available:https://www.ijcai.org/proceedings/2017/24
IEEE Transactions on Automatic Control, vol. 67, no. 4, pp. 1698–
[8] D. Mart´ınez-Rubio, V. Kanade, and P. Rebeschini, “Decentralized
1712,2021.
cooperativestochasticbandits,”AdvancesinNeuralInformationPro-
[28] T. Jaksch, R. Ortner, and P. Auer, “Near-optimal regret bounds
cessingSystems,vol.32,2019.
for reinforcement learning,” Journal of Machine Learning Research,
[9] P.-A. Wang, A. Proutiere, K. Ariu, Y. Jedra, and A. Russo, “Opti-
vol. 11, no. 51, pp. 1563–1600, 2010. [Online]. Available:
malalgorithmsformultiplayermulti-armedbandits,”inInternational
http://jmlr.org/papers/v11/jaksch10a.html
ConferenceonArtificialIntelligenceandStatistics. PMLR,2020,pp.
[29] Z. Galil, “Efficient algorithms for finding maximum matching in
4120–4129.
graphs,” ACM Comput. Surv., vol. 18, no. 1, p. 23–38, mar 1986.
[10] P.Landgren,V.Srivastava,andN.E.Leonard,“Distributedcooperative
[Online].Available:https://doi.org/10.1145/6462.6502
decision-making in multiarmed bandits: Frequentist and bayesian
[30] T.H.Cormen,C.E.Leiserson,R.L.Rivest,andC.Stein,Introduction
algorithms,”in2016IEEE55thConferenceonDecisionandControl
toalgorithms. MITpress,2022.
(CDC). IEEE,2016,pp.167–172.
[11] M. Agarwal, V. Aggarwal, and K. Azizzadenesheli, “Multi-agent
APPENDIX
multi-armed bandits with limited communication,” The Journal of
MachineLearningResearch,vol.23,no.1,pp.9529–9552,2022.
PROOFOFTHEOREM1
[12] A. Sankararaman, A. Ganesh, and S. Shakkottai, “Social learning
in multi agent multi armed bandits,” Proceedings of the ACM on BeforebeginningtheproofofTheorem1,wedefineafew
Measurement and Analysis of Computing Systems, vol. 3, no. 3, pp.
relevantquantities.Lets bethenumberofsamplesofarm
1–35,2019. k,e
[13] R. Chawla, A. Sankararaman, A. Ganesh, and S. Shakkottai, “The k in episode e; that is, s k,e =n k,te+1−n k,te. Let H e be the
gossipinginsert-eliminatealgorithmformulti-agentbandits,”inInter- length of episode e; that is H =t −t . Furthermore, let
e e+1 e
national conference on artificial intelligence and statistics. PMLR,
R bethetotalregretofepisodeeandletF bethesmallest
2020,pp.3471–3481. e e
[14] D. Kalathil, N. Nayyar, and R. Jain, “Decentralized learning σ-algebra such that H 0,...,H e are measureable.
for multi-player multi-armed bandits,” in 2012 IEEE 51st IEEE Wealsoderiveausefulresultaboutthenumberofepisodes
Conference on Decision and Control (CDC), Dec 2012, p.
E of the algorithm.
3960–3965, arXiv:1206.3582 [cs, math]. [Online]. Available: http:
//arxiv.org/abs/1206.3582Lemma 1 (Number of episodes): Thenumberofepisodes Webeginwiththefirstterm,thepriceoffailingconfidence
E is logarithmic in T. In particular, bounds. First, we note that
E≤Klog(T). (cid:34) (cid:12) (cid:35)
te+1−1 (cid:12)
Proof: Consider any arm k ∈ [K]. Denote by E k = E[R |¬G ,F ]=E ∑ ∑ [f (c∗)−f (c )]µ (cid:12)F
e e e k k k k,t k(cid:12) e
{e 1,e 2,...,e l(k)} the ordered set of episodes where k is the t=te k∈[K] (cid:12)
destination node with the fewest number of samples and the (cid:34) (cid:12) (cid:35)
(cid:12)
algorithm is not terminated prematurely. That is, E k are the ≤E H e ∑ f k(c∗ k)(cid:12) (cid:12)F e ≤NH e, (13)
episodes whose length is defined such that s k,e >n k,te and k∈[K] (cid:12)
thus we have for each i∈[2,...,l(k)]
n ≥2n ≥2n
sinceE[H e|F e]=H
e
and∑
k∈[K]
f k(c∗ k)≤∑ k∈[K]c∗ k=N.Now,
k,tei+1 k,tei k,tei−1+1 we turn to bounding
wherethelastinequalityfollowssinceinallepisodese∈/E ,
w ofe ss at mill ph lea sv )e os
fk, ae
r≥ m0 k. iN mo mte et dh ia at ten
lyk,te ai− ft1 e+ r1
ei ps it sh oe deco eunt( an nu dm tb he ak r
t
P e[¬G e]≤ k∈∑ [K]P e(cid:2) |µˆ k,te−µ k|>b k,te(cid:3)
i−1
(cid:34) (cid:115) (cid:12) (cid:35)
for all episodes e nb ke ,tt ew i+1ee ≥n 2e ii −− 11 na k,n ted 1+e 1i, ≥e 2∈/
i
E k. Therefore, ≤ k∈∑ [K]te j∑ =− 11 P
e
(cid:12) (cid:12)µˆ k,te−µ k(cid:12) (cid:12)> 2l no kg ,t( et e)(cid:12) (cid:12)
(cid:12)
(cid:12)n
k,te
= j ,
∴ n ≥2l(k)
a,tel(k)+1 where in the first inequality we used a union bound over
whereweusedthefactthatrightafterepisodee wehaveat all possible arms, and in the second we used a union bound
1
least two samples of arm k due to initialization. Every time over all possible values of n k,te for every arm. Then, for any
step, we sample at most max(N,K) arms so by definition, specific k and j, we have by Hoeffding’s inequality
we have
(cid:34) (cid:115) (cid:35)
TK≥ ∑ n
k,tel(k)+1
≥ ∑ 2l(k) P
e
(cid:12) (cid:12)µˆ k,te−µ k(cid:12) (cid:12)> 2log j(t e) ≤2 exp(−4logt e)=2t e−4.
k∈[K] k∈[K]
≥K2∑k∈[K]l( Kk) ≥K2E K−1
As a result,
where the second to last inequality is from Jensen’s inequal- P [¬G ]≤2Kt−3. (14)
e e e
ity.NotethatweuseE−1ratherthanE sincethelastepisode
is potentially cut short so as to not exceed the time limit T. To combine (13) and (14) we note that K < t due to
e
Taking logs of both sides gives initialization, and that H ≤ K+t since it takes at most
e e
K steps to transition to the optimal period after which the
E−1≤Klog(T)/log(2)
algorithm stays stationary for n <t steps. Therefore,
∴ E≤Klog(T) min e
(cid:20) (cid:21)
which completes the proof. E E [R |¬G ]P [¬G ] =E[2NH Kt−3]≤E[4Nt−1]. (15)
e e e e e e e e
Now,wearereadytoproveTheorem1.Wewanttoobtain
a bound on
(cid:34) (cid:35) (cid:34) (cid:35) We now offer an analysis of the second term of our
T E
E ∑R
t
=E ∑R
e
. good/bad decomposition in (12). We bound P e[G e]≤1. Let
i=1 e=1 D e be the number of transition steps in episode e so that
Letusnowconsidersomearbitraryepisodee.Wedecompose for t ≥t e+D e we have C t =Cˆ e. Note that D e ≤D by our
into good and bad events, where we define the “good” event assumption that each regret shortest path have maximum
as physical length of D. Then,
G e={∀k∈[K],µ k∈[µˆ k,te−b a,te,µˆ k,te+b k,te]}, (cid:34)
te+1−1
(cid:12)
(cid:12)
(cid:35)
E [R |G ]=E ∑ ∑ [f (c∗)−f (c )]µ (cid:12)G
the event where the true mean of each arm is within the e e e e k k k k,t k(cid:12) e
arm’s confidence radius of its estimated mean. The “bad”
t=te k∈[K] (cid:12)
(cid:34) (cid:12) (cid:35)
eventoccurswhenatleastoneconfidenceradiusfails.Then, te+1−1 (cid:12)
≤DM+E ∑ ∑ [f (c∗)−f (cˆ )]µ (cid:12)G ,
lettingE =E[·|F ]andP =P[·|F ]wecanwritetheregret e k k k k,e k(cid:12) e
e e e e t=te+Dek∈[K] (cid:12)
of episode e as
(cid:20) (cid:21) since after D steps we will have reached the desired arm
E[R e]=E E e[R e] allocation represented byCˆ e. We now note that by definition
of G , for every k∈[K] we have
(cid:20) (cid:21) (cid:20) (cid:21) e
=E E [R |¬G ]P [¬G ] +E E [R |G ]P [G ] . (12)
e e e e e e e e e e
U −2b =µˆ −b ≤µ ≤µˆ +b =U .
k,te k,te k,te k,te k k,te k,te k,teTherefore, we can simplify the expectation above as observed samples such that cˆ >0. Then, since we know
k,e
(cid:34) (cid:12) (cid:35)
∑ k∈[K]cˆ k,e=N we have
te+1−1 (cid:12)
E e ∑ ∑ [f k(c∗ k)−f k(cˆ k,e)]µ k(cid:12) (cid:12)G e ∑ f √k(cˆ k,e) ≤ ∑ √cˆ k,e ≤ √N . (18)
t=te+Dek∈[K]
(cid:34)
(cid:12)
(cid:12) (cid:35) k∈[K]
n
k,te k∈[K]
n
k,te
n
min
te+1−1 (cid:12)
≤2E ∑ ∑ f (cˆ )b (cid:12)G Note now that by definition of our episode length we have
e k k,e k,te(cid:12) e
te+De k∈[K] (cid:12) at most n min time steps at the destination node since we end
(cid:34) te+1−1 (cid:12) (cid:12) (cid:35) the episode exactly when k min has its number of samples
+E e ∑ ∑ [f k(c∗ k)−f k(cˆ k,e)]U k,te(cid:12) (cid:12)G e doubled. Therefore,
te+De k∈[K] (cid:12)
(cid:34) (cid:35)
≤2E
e(cid:34) t te e+
∑
+1 D− e1
k∈∑ [K]f k(cˆ k,e)b
k,te(cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)G
e(cid:35)
, (16)
4(cid:112) 2log(T)E (cid:112)e∑ =E 1t=te+ t∑ e1 +− D1
e
(cid:34)k∈∑
[ EK]
f √k( ncˆ kk (cid:18),, te e)
N
(cid:19)(cid:35)
w thh ee cre out nh te vl ea cs tt orin te hq au ta mlit ay xif mo il zlo ew ss
∑
kfr ∈o Km
f
k(d ce kfi )n Ui kti ,to en
.
of Cˆ te as ≤4 2log(T)E e∑ =1n min √ n
min
(cid:34) (cid:35)
(cid:112) E √
≤4N 2log(T)E ∑ n .
Combining (15) and (16) and summing over all episodes min
e=1
thus gives an expected regret of
Consideringnowtheexpressionstillintheexpectationand
T (cid:34) E (cid:32) (cid:34) te+1−1 (cid:12) (cid:12) (cid:35)(cid:33)(cid:35) applying Jensen’s law we get
E[∑R]=2E ∑ E ∑ ∑ f (cˆ )b (cid:12)G
t k k,e k,te(cid:12) e √ (cid:115)
t=1 e=1 t (cid:34)e+De k∈[K (cid:35)]
(cid:34)
(cid:12)
(cid:35) E
∑E n min
≤E
∑E e=1n min ≤√ TE≤(cid:112)
TKlog(T).
E E E E
+E ∑DN +E ∑4t−1 . (17) e=1
e
e=1 e=1 Therefore, we achieve a final “price of optimism” of
(cid:16) √ (cid:17)
We bound each of the three terms in (17). We begin with O Nlog(T) TK . (19)
the first term. We invoke Lemma 4 in [1] which states that
for any random variable x satisfying x≥0 almost surely, the The second term in (17) can be bounded using Lemma 1
equation achieving a final transition cost of
E[x|G ]≤2E[x]
e O(NDKlog(T)). (20)
holds for all episodes e. We can therefore get rid of the Finally, the third term can be bounded by noting thatt ≥
e
conditioning on G e. e+1 for each e. Then,
(cid:34) (cid:32) (cid:34) (cid:12) (cid:35)(cid:33)(cid:35) (cid:34) (cid:35) (cid:34) (cid:35)
E te+1−1 (cid:12) E E 1
2E ∑ E ∑ ∑ f (cˆ )b (cid:12)G E ∑4t−1 ≤4E ∑
a k,e k,te(cid:12) e e e
e=1 te+De k∈[K] (cid:12) e=1 e=1
(cid:34) E te+1−1 (cid:35) ≤4E[log(E)]≤O(log(K)+loglog(T)) (21)
=4E ∑ ∑ ∑ f (cˆ )b
k k,e k,te
where in the second inequality we used the bound on
e=1t=te+Dek∈[K]
(cid:34) (cid:35) harmonicsums.Therefore,bycombining(19),(20),and(21)
≤4(cid:112) 2log(T)E ∑E te+ ∑1−1 ∑ f √k(cˆ k,e) . we bound our total regret contribution
n
e=1t=te+Dek∈[K] k,te (cid:34)
T
(cid:35)
(cid:16) (cid:104)√ (cid:105)(cid:17)
E ∑R ≤O Nlog(T) TK+DK . (22)
Now, recall that n is the number of samples of k at t
min min
t=1
the start of episode e where k is the arm with the fewest
min