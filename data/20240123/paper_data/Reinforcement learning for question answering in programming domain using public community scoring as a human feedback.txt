Reinforcement learning for question answering in programming domain
using public community scoring as a human feedback
AlexeyGorbatovski SergeyKovalchuk
ITMOUniversity Huawei
SaintPetersburg,Russia SaintPetersburg,Russia
gorbatoski@itmo.ru sergey.kovalchuk@huawei.com
Abstract Furthermore, a critical challenge is the evalua-
tionofthequalityofresponsesgeneratedbyLLMs.
Inthisstudy,weinvestigatetheenhancementof
ConventionalmetricssuchasBertScoreandRouge
theGPTNeo125M’sperformanceinCommu-
donotcapturetheessenceofresponseseffectively,
nityQuestionAnswering(CQA)withafocus
onprogramming,throughtheintegrationofRe- especially in specialized domains like program-
inforcementLearningfromHumanFeedback ming(Wangetal.,2019). Moreover,theydon’tac-
(RLHF)andtheutilizationofscoresfromStack countforthediversityinvalidanswersandlackin
Overflow. Twodistinctrewardmodeltraining
capturingdeepersemanticcorrectness. Thedevel-
strategies are employed for fine-tuning with
opmentofmorereliableandcontext-sensitiveeval-
ProximalPolicyOptimization(PPO).Notably,
uationmetricsisessential(Kovalchuketal.,2022).
the improvements in performance achieved
throughthismethodarecomparabletothoseof To address these challenges, in this paper, we
GPTNeo’s2.7Bparametervariant. Addition- investigate the application of RLHF to a smaller
ally, anauxiliaryscoringmechanismisintro- model,GPTNeo125M(Blacketal.,2021a),inthe
duced, whichdemonstratesthelimitationsof contextofprogrammingCQA.Weaimnotonlyto
conventionallinguisticmetricsinevaluatingre-
enhancethemodel’sresponsegenerationcapabil-
sponsesintheprogrammingdomain. Through
ities but also to address the evaluation challenge.
accurate analysis, this paper looks at the di-
Ourcontributionsaretwo-fold. First,weexplore
vergence between traditional linguistic met-
the potential and efficacy of RLHF in retraining
ricsandourhuman-preferences-basedreward
model,underscoringtheimperativefordomain- a smaller LLM for programming CQA. Second,
specific evaluation methods. By elucidating through empirical analysis, we highlight the dis-
thecomplexitiesinvolvedinapplyingRLHFto crepanciesbetweentheRLHFrewardmodeland
programmingCQAandaccentuatingthesignif- existinglinguisticmetrics,emphasizingthelimita-
icanceofcontext-awareevaluation,thisstudy
tionsofcurrentevaluationmethodologiesandad-
contributes to the ongoing efforts in refining
vocatingforthedevelopmentofmoresemantically-
Large Language Models through focused hu-
sensitivemeasures.
manfeedback.
Thestructureofthepaperisasfollows: Section
1 Introduction
2providesbackgroundinformationanddescribes
the datasets used in this study. In Section 3, we
Advances in Reinforcement Learning from Hu-
delve into the application of RLHF for program-
man Feedback (RLHF) have revolutionized the
mingCQA,explainingthedatapreparingmethod-
fine-tuning of Large Language Models (LLMs),
ologiesemployed. Section4focusesontheexperi-
facilitatingadaptationforhuman-likeresponsegen-
mentalevaluationandresults. Section5presentsa
erationandprecisebehaviorcontrol(Ouyangetal.,
discussionofthestudyresultsandevaluationmeth-
2022). While RLHF has proven effective in gen-
ods. Finally, Section 6 concludes the paper with
eral domains, its application in specialized fields
finalremarksandreflectionsonourfindings.
such as Community Question Answering (CQA)
for programming remains unexplored (Beeching
2 BackgroundandDataset
etal.,2023). LLMsfaceuniquechallengesinhan-
dlingthecomplexnatureofprogrammingqueries,
2.1 BackgroundonRLHFandLLMs
includingconceptualunderstanding,codegenera-
tion, API usage, and debugging, due to struggles Reinforcement Learning from Human Feedback
withsubtlesemanticrelations. (RLHF)isatechniquewheremodelsaretrainedus-
4202
naJ
91
]LC.sc[
1v28801.1042:viXrainghumanfeedbackasrewards. Thismethodhas usingregularexpressionstoensurealignment
become notably beneficial in refining the perfor- withthestudy’sfocus.
mance and behavior control of Large Language
Models (LLMs). RLHF initiates with models • Tomaintaintextpurityandtheabilitytogen-
trainedusingsupervisedfine-tuning(SFT),which eratearesponsebasedonthecontextonly,we
are then iteratively improved. Crucially, the hu- filteredoutquestionsandanswerscontaining
manscoringprocessinRLHFisoftenautomated images,links,orcodeblocks,designatedby
bytrainingaseparaterewardmodel,servingasan the <pre><code> HTML tags. Code blocks
optimizationproxy. Thisprocessvariesinimple- were filtered out to prevent the model from
mentationandwarrantsfurtherexploration. generating code snippets during training, as
TheapplicationofRLHFinLLMshasbeenex- this would significantly complicate the eval-
plored in various contexts. For instance, Ziegler uationprocessduetothelackofestablished
et al. (2019) studied the impact of reward learn- metricsforassessingthequalityofgenerated
ingonspecifictasks,demonstratingthepotential code.
ofRLHFinenhancingtheperformanceofLLMs.
• HTML content was sanitized and converted
The work of Stiennon et al. (2020) and the Ope-
toplaintextusingtheBeautifulSoup3 library
nAIAlignmentTeamin2021furtherexpandedthe
toprepareitfornaturallanguageprocessing.
scopeofRLHF,applyingittothetaskofsumma-
rizingtextandbooks,respectively.
Weobtainedadatasetof6,946trainingentries
In the context of Question Answering (QA),
and1,000validationentries. Topreventdataleak-
RLHF has been used to train models to navigate
ageandensuretemporalrelevance,thevalidation
theweb(Nakanoetal.,2021)andtofollowinstruc-
setincludedquestionspostedafterDecember14,
tions(Ouyangetal.,2022). However,thesestudies
2021.
havemainlyfocusedongeneraldomainsorspecific
Whilethisdatasetishighlyrelevantforstudying
tasks,andtheapplicationofRLHFinspecialized
RLHF in programming CQA, it is worth noting
fieldssuchasprogrammingCommunityQuestion
thattheconstraintsappliedmayintroducecertain
Answering(CQA)remainslargelyunexplored.
limitations in terms of the diversity of questions
andreal-worldapplicability.
2.2 DatasetSelectionandPreprocessing
In the study, we used Stack Overflow1 (SO) as 3 RLHFforprogrammingQ&A
theprimarydatasourceforprogramming-related
question-answeringtasks. Weregardedtheanswers ThegeneralschemaofRLHFutilizedinthisstudy
on SO as reference solutions. We compiled the consists of several stages, as depicted in Fig. 1.
datasetfromtheoriginaldataavailableonStackEx- Theprocesscommenceswiththetrainingofanini-
change2,focusingonquestionsspecificallytagged tialpolicyviasupervisedlearning. Subsequently,
with’python’. Thisdataset,whichincludestitles, a reward model is trained to acquire human pref-
questions, answers per question, and user scores erencesfromthelabeleddata. Finally,thepolicy
foreach,wasusedforbothsupervisedfine-tuning isfine-tunedusingProximalPolicyOptimization
and partial reward model training. To ensure the (PPO) (Schulman et al., 2017), guided by the re-
dataset’srelevanceandhomogeneity,wesubjected wardmodel.
it to a series of constraints and transformations. In this study, we have adapted RLHF for pro-
Furthermore,weadjusteduserratingsfordifferent gramming Q&A by converting user ratings from
rewardmodeltrainingsetups. Stack Overflow into feedback for training our
We applied several constraints to refine the model. We used two distinct approaches: creat-
datasetandmaintainconsistency: ingregressionscoresandcontrastivescoresforthe
straightforwardcomparisonofanswers. Addition-
• Weonlyselectedquestionsclassifiedas“API ally,toenhancethelogicalalignmentofsentences
Usage”accordingtothetaxonomybyBeyer andmitigatethemodel’serrorsingeneration,we
et al. (2020). This selection was performed completed the dataset for reward model training
withgenerationsfromtheSFTmodel.
1https://stackoverflow.com/
2https://stackexchange.com/ 3https://github.com/wention/BeautifulSoup4Figure1: ThegeneralschemaofReinforcementLearningfromHumanFeedbackforprogrammingQ&A
3.1 TransformationofUserRatings controlthestandarddeviation,thusstabilizingthe
regressiontraining. TheprocessisoutlinedinAl-
Toaccountforbiasesarisingfromfactorslikethe
gorithm1. Therev representsthevotesforeach
question’sageandpopularity,wepreprocessand ij
answera inquestionq ands fortheregression
normalize user ratings. Our approach comprises j i ij
scores.
two distinct transformations: Regression Scores
andContrastiveScores.
3.1.2 ContrastiveScores
Contrastivescoresallowaconvenientcomparison
Algorithm1Regressionscorestransforming
ofanswerratingsbymappingthemtologarithmi-
Input: uservotesforeachanswerN_votes
ij callyscaledscores(Askelletal.,2021). Accepted
Output: regressionscoress
ij answersreceiveanadditionalincrement,whileneg-
1: foreachquestionq i do ative ratings are assigned a standard value. The
2: foreachanswera j inq i do followingAlgorithm2detailsthisprocess:
3: s ij = NN _a_v no st we es rij si Inthisalgorithm,v
j
denotesthevotesforeach
4: endfor
answer, and the contrastive scores, s , are com-
j
5: endfor
putedusingalogarithmicscale.
6: l_bound,u_bound = 1.5×IQR(s ij)
Additionally, wefirstidentifiedquestionswith
7: foreachscores ij do morethanoneanswerafterpreprocessingandfil-
8: ifs ij outsidetherange[l_bound,u_bound] tering, which amounted to 3,076. We compared
then
each of these answers with the highest-voted an-
9: s ij = clip(s ij,−1,1) swer for each question during the training of the
10: else
rewardmodel. Afterthiscomparison,thecontrast
11: s ij = max_abs_scale(s ij,sign(s ij)) datasetcontained1,804rows.
12: endif
13: endfor 3.2 DataGenerationforRewardModel
14: return s ij foralli,j Training
We generated 6,872 additional answers for ques-
tionswithonlyoneanswertocreateacomparison
3.1.1 RegressionScores
set essential for training the reward model. This
Forregressionscores,userratingswerenormalized stepwasundertakentoensureadiversedatasetthat
bythetotalnumberofanswersforeachquestion. simulatesvariousanswerqualities.
Afterclippingoutliers,theratingswerescaledto For the regression approach, we assignedAlgorithm2Contrastivescoresscaling 4.1 Evaluationapproach
Input: votesforeachanswerv j Fig. 2illustratestheevaluationschema. Foreach
Output: contrastivescoress ij questioninthevalidationdataset,themodelgener-
1: foreachanswera j do atestenresponsesusingsampling-baseddecoding.
2: ifv j < 0then Thisapproachallowsustostudytheaveragequal-
3: s j = −1 ityofthegeneratedresponseswithoutbiastoward
4: else the worst or best cases. The parameters used for
5: s j = ⌈log 2(1+v j)⌉ sampling-baseddecodingareasfollows:
6: ifa j isacceptedthen
7: s j = s j +1 • do_sample: true
8: endif
• no_repeat_ngram_size: 2
9: endif
10: endfor • top_k: 50
11: foreachquestionq i do
12: ifN_answers i > 1then • top_p: 0.9
13: s max = max(s ij)forallj
14: foreachanswera k inq i do To evaluate the responses’ content and seman-
15: comparescores k withs max ticsimilarity,weemploytheSacreBLEU,Rouge,
16: endfor andBertScoremetricsascommonmetricsfornat-
17: endif ural language generation tasks. Additionally, the
18: endfor rewardmodelsrateeachgeneratedresponseasan
19: return comparedpairs{(a j,a k)} alternativequalityassessmenttool.
For a more insightful evaluation, we also con-
ductahuman-basedassessment. Asubsetof100
these generated answers a normal distribution randomlyselectedquestionsismanuallyevaluated
N(−0.5,0.12). Thiswasbasedontheobservation by ourselves. Each generated answer for these
thatmostgeneratedanswerswereeithercompletely questionsisinspectedandmarkedasuseful(1)or
uninformative or erroneous. We believe that dis- notuseful(0)forsolvingtheproblemstatedinthe
couragingthegenerationofnonsensicalanswersis question. Thisbinarylabelingenablesthecompu-
ahelpfulpractice. tationoftheMeanReciprocalRank(MRR),which
Inthecontrastiveapproach,thesegeneratedan- assessestherelevanceofthegeneratedresponses.
swerswereincorporatedintothecontrastdataset, Finally, to investigate the consistency between
which previously only included questions with the different metrics and reward model assess-
morethanoneexistinganswer. ments, we employ Spearman’s rank correlation
The generation of this additional data was cru- coefficient. This statistical measure will provide
cialforrobustlytrainingtherewardmodel. Inthe insightintowhethertheautomaticmetricsandthe
experimentalsection,wewilldelveintohowthis rewardmodelassessmentsarealignedinevaluating
datasetwasleveragedtotraintherewardmodelef- responsequality.
fectively,andtheevaluationmetricsusedtoassess
4.2 Experimentalsetup
itsperformance.
All experiments were conducted using the GPT
4 Experimentalevaluation Neomodel(Blacketal.,2021b)with125million
parameters,selectedbasedontheconstraintsdis-
Thissectionaimstoevaluatetheeffectivenessof cussedinthecorrespondingsection.
the RLHF training approach for improving the
qualityofgeneratedresponsesintheprogramming 4.2.1 Supervisedfine-tuning
QA domain. Specifically, we compare three ver- Fine-tuningwasperformedonthetrainingdataset
sionsofthemodel,accordingtoFig. 1-thebase described in Section 2.2. We utilized the Trans-
model, the SFT version, and the RLHF version. formersandPyTorchLightninglibrarieswiththe
Theevaluationfocusesontheperformanceofthe following hyperparameters: optimizer = Adam,
rewardmodeltrainingmethodsandthegenerated Adam betas = (0.9, 0.999), Adam epsilon = 1e-
responses’quality. 08, weight decay = 0.001, learning rate = 2e-05,Figure2: Thegeneralschemaofevaluationapproach
learningratedecayscheme=linear,batchsize=12, wherer andy meanithrewardandtargetscorere-
i i
andmixedprecisiontraining(fp16). Themaximum spectively. Consideringbothpositiveandnegative
lengthoftheconcatenatedquestionandanswerwas rewardsisessentialinreinforcementlearning.
setto512tokens. Iftheinputsequenceexceeded
4.2.3 Fine-tuningwithRL
this length, the question was truncated, ensuring
We employed reinforcement learning using the
thatthefullanswerwasavailablefortraining.
TRLandTransformerslibraries,withtypicalRLHF
4.2.2 RewardModelTraining parameters: optimizer=Adam,Adambetas=(0.9,
0.95),Adamepsilon=1e-08,learningrate=1.41e-
Wetrainedtherewardmodelusingtwoapproaches:
05,epsiloncliprange=0.2,buffersize=64,and
regression and answer comparison. The regres-
batchsize=16. Additionally,weusedadaptiveKL
sionapproachemployedtheMeanSquaredError
controlwithaninitialKLcoefficientof0.2anda
(MSE)asthelossfunction,whiletheanswercom-
targetof6.
parison approach used the Contrastive Loss (see
Duringreinforcementlearning,thetrainingwas
Formula1).
stable,andtheaveragerewardincreasedwhenus-
ing the reward model based on the regression ap-
L(θ)=−E [log(σ(r (x,y )−r (x,y )))] (1) proach. However,trainingwasunstableanddidnot
(x,yj,yk)∼D θ j θ k
convergeusingtherewardmodelbasedonanswer
comparisons. Tuningmodelstartsgeneraterepet-
wherer andy aretherewardmodel’sscoreand
itive words and incoherent sentences. Therefore,
y isthepreferredcandidaterespectively. Bothap-
the results section presents the outcomes for the
proachesusedtheSFTmodelasthebasis.
model trained using the regression-based reward
For the regression approach, the hyperparame-
model.
ters were as follows: optimizer = Adam, Adam
betas=(0.9,0.999),Adamepsilon=1e-08,weight
4.3 Results
decay=0.001,learningrate=2e-05,learningrate
Thissectionpresentstheresultsoftheexperiments,
decayscheme=linear,batchsize=16,andmixed
whichwereconductedtoassesstheefficacyofthe
precisiontraining(fp16).
RLHFtrainingapproachinthecontextofprogram-
Forthecontrastiveapproach,weusedthesame
ming QA response generation. We examine the
hyperparameterswithadifferentlearningrate(3e-
performance of the different models and discuss
05) and batch size (8). Additionally, for both ap-
thecorrelationandconsistencybetweenthemetrics
proaches,theweightsofthefirstandlastlayers,as
employed for evaluating the quality of the gener-
wellasalllinearlayersofthemodel,wereupdated
atedresponses.
duringtraining.
Bothapproachesexhibitedstabilityduringtrain- 4.3.1 ComparisonofAverageMetrics
ing, achieving validation accuracies of 93% and Ourevaluationprocessinvolvedcomputingtheav-
95%respectively. Fortheregressionapproach,ac- eragemetricsfortengenerationattemptsbyfour
curacywascomputedusingtheformula: models: Base GPT Neo 125M (Base 125M), Su-
pervisedFine-tuningGPTNeo125M(SFT125M),
1 (cid:88)
accuracy= [sign(r )=y ] (2) RLHFGPTNeo125M(RLHF125M),andBase
n i iSacreBLEU Rouge1 Rouge2 BertScore Reg. Reward Contr. Reward
Base125M 0.0433 0.1816 0.0233 0.9420 -0.1479 -1.0124
(σ:0.0071) (σ:0.0684) (σ:0.0160) (σ:0.0057) (σ:0.0994) (σ:1.0214)
SFT125M 0.0484 0.1903 0.0237 0.9483 0.1257 -0.0173
(σ:0.0088) (σ:0.0581) (σ:0.0151) (σ:0.0097) (σ:0.0864) (σ:1.0123)
RLHF125M 0.0489 0.1884 0.0230 0.9493 0.1869 0.3955
(σ:0.0092) (σ:0.0545) (σ:0.0149) (σ:0.0105) (σ:0.0767) (σ:0.9720)
Base2.7B 0.0455 0.1906 0.0275 0.9417 -0.1123 -0.0365
(σ:0.0073) (σ:0.0735) (σ:0.0190) (σ:0.0054) (σ:0.1045) (σ:1.1245)
Table1: Comparisonofaveragemetricsfordifferentmodels. Eachentrycontainsthemeanofthecorresponding
metricacrosstengenerationattempts.
GPTNeo2.7B(Base2.7B).Thesemodelsevalu- Table 2 presents the results for Metrics@10,
atedusingseveralmetrics,includingSacreBLEU, which indicates the best metric scores among 10
Rouge 1, Rouge 2 and BertScore, as well as the generationattempts. Mostofthehighlightedvalues
scoresobtainedfromtheregressionandcontrastive havebeendeterminedtobestatisticallysignificant
rewardmodels. aspertheU-testandKS-test. Interestingly, Base
Table 1 presents the average values of these GPTNeo2.7Bexhibitsthehighestaveragemodel
metrics for each model. Notably, the RLHF ver- reward based on the contrastive approach. This
siondemonstratedsuperiorperformancecompared mightsuggestthatthemodel’sresponsesaremore
to the SFT model in terms of SacreBLEU and diverseand,insomecases,closertothereference
BertScore. However, the larger Base GPT Neo answers.
2.7B model surpassed the other models in terms TheRouge2metric,whichfocusesontheover-
oftheRougescores. Allhighlightedmetricswere lap of bigrams between the generated and refer-
deemedstatisticallysignificantviatheKS-test. The encetexts,presentsaclosecompetitionamongthe
inclusionofbootstrappedconfidenceintervalsfur- models. Thisimpliesthattheinclusionofbothcon-
therclarifiesthemodel’simprovementrelativeto tentwordsandtheirorderingarewell-represented
thebaseline. acrossmodels.
4.5 MRRComparison
4.4 MetricsatRankkAnalysis
An additional analysis conducted to assess the
Beyond the mean values, we performed an in- consistency between reward models and metrics
depthanalysisusingthemetric@kapproach. The usedforevaluatingthequalityofthegeneratedre-
term“metricatrankk”referstothehighestscore sponses. Thisanalysisinvolvedtheutilizationof
achievedbyametricamongkrandomlysampled manualannotationsforanswerscorrespondingto
generation attempts. This analysis helps to re- 100randomquestions.
vealthecapabilityofthemodelstogeneratehigh- Inthisanalysis,MRRcalculatedforvaryingval-
quality responses within a certain number of at- uesofk,wherek denotesthetop-rankedanswers
tempts. accordinglytosomemetric. TheMeanReciprocal
Fig. 3illustratesgraphsdepictingtherelation- Rank at k (MRR@k) is a statistical measure for
shipbetweenthemetricvaluesandthenumberof evaluatinganyprocessthatproducesalistofpos-
generationattempts(k). Thesegraphsprovidein- sibleresponsestoasampleofqueries,orderedby
sights into the performance of the models as the probabilityofcorrectness. IfwehaveAanswers,
number of generation attempts increases. Partic- and R is the rank of the first relevant document
i
ularly, afterseveralgenerationattempts, boththe forqueryi(consideringonlythetopk documents),
SFTandRLHFversionsappeartooutperformthe thentheMRR@kis:
larger GPT Neo 2.7B model in terms of the eval-
uatedmetrics. Additionally,theRLHFmodelex-
A (cid:18) (cid:19)
1 (cid:88) 1
hibitssignificantimprovementinBertScore,which MRR@k= ifR ≤k;else0 (3)
A R i
i
suggestsenhancedsemanticsimilaritybetweenthe i=1
generatedandreferenceresponses. Thismethodallowsforunderstandinghoweffec-SacreBLEU Rouge1 Rouge2 BertScore Reg. Reward Contr. Reward
Base125M 0.0724 0.2623 0.0549 0.9517 0.2746 2.6521
SFT125M 0.0875 0.2897 0.0614 0.9578 0.3754 2.8612
RLHF125M 0.0901 0.2903 0.0625 0.9586 0.3711 2.8187
Base2.7B 0.0744 0.2704 0.0607 0.9513 0.3201 3.6581
Table2: Comparisonofaveragemetrics@10fordifferentmodels.
Figure3: Graphsofdependenciesofmetricvaluesonthenumberofkattemptstogenerate
tivelythedifferentmetricsrankthecorrectanswers Rouge 1 and Rouge 2 is evident. Furthermore,
amongitstoppredictions. the reward regression model exhibits a moderate
Table 3 presents the MRR@10 scores, indicat- correlation when responses are generated by the
ingtheMRRvalueswhenconsideringthetop10 fine-tunedmodels. Interestingly,BertScoredemon-
ranked samples. Notably, Rouge 2 and Rouge 1 strates little to no correlation, or even a negative
metrics exhibit higher values, which implies that correlation,withtheothermetrics. Thisraisesques-
theyarekeymetricsinassessingtheaccuracyofthe tionsaboutitsreliabilityasacomparativemeasure
generatedresponses. However,thetrainedreward inthiscontext. Additionally,itisnotablethatthere-
modelsdisplaysuperiorperformancecomparedto wardmodelsdisplayminimalcorrelationamongst
boththeSacreBLEUandBertScoremetrics. themselves,whentrainedthroughdifferentmethod-
ologies.
4.6 CorrelationAnalysis
5 Discussion
In addition to the previous evaluations, a corre-
lation analysis carried out among the assessment ThestudyfocusedonthegeneratingQAhighlights
methodsutilized. Specifically,theSpearmancor- theeffectiveimplementationoftheRLHFinthein-
relation coefficient was computed to understand tricatedomain. ThismethodoutperformsSFTtech-
therelationshipsbetweenthevariousmetrics. The nique,markingitssuperiorityintermsofmetrics
Spearmancorrelationcoefficientisanonparametric performance. Moreover,theapplicationofRLHF
measure that evaluates the strength and direction hasdemonstratedthatit’spossibletocompetitively
oftheassociationbetweentworankedvariables. trainsmallermodels,showcasingitsefficacyeven
Appendix A contains tables that compare the inscenarioswithlimitedresources.
cross-correlation coefficients of the metrics for Regarding the scoring parameters, the study
each model generations. Upon examination, a draws attention to the utility of Rouge scores in
prominent correlation between the rankings of gaugingresponseprecision. Thisimpliesapoten-Base125M SFT125M RLHF125M
SacreBLEU 0.4107 0.3709 0.3262
Rouge1 0.4792 0.4532 0.4091
Rouge2 0.4011 0.4453 0.4220
BertScore 0.2913 0.3403 0.3300
Reg. Reward 0.4015 0.3867 0.4296
Contr. Reward 0.4302 0.3787 0.3527
Table3: ComparisonofMRR@10scoresfordifferentmodelsandmetrics. ThevaluesrepresenttheMRRscores
consideringthetop10rankedsamples.
tialedgeofRougeoveralternativescoringsystems asprogramming. Thesedomainsaremarkedbyin-
likeSacreBLEUandBertScoreincertaincontexts. tricatesemanticrelationshipsandabroadspectrum
However,thereexistsambiguityintheMRRre- ofvalidanswers.
sultsforBertScoreandSacreBLEUmetricswhen As we look to the future, we envision testing
juxtaposedwiththeoutcomesfromthetrainedre- ourmethodologiesandexperimentsetupsonlarger
ward models. This raises questions about the ad- modelstoassessthescalabilityofourapproachand
equacy of these metrics for the programming do- verifytheconsistencyofourresults. Weanticipate
main,whichishallmarkedbycomplexsemanticre- thatthesefurtherinvestigationswouldprovidevalu-
lationshipsandaplethoraofcorrectanswers. This ableinsightsintothebehaviorandperformanceof
ambiguityisfurthercementedbynear-zeroSpear- theselargermodelsunderRLHFbasedfine-tuning,
mancorrelationsassociatedwithvariouslinguistic therebyexpandingthescopeofourcurrentstudy.
metrics. The insights derived from our research enrich
Thesefindingsnotonlyprovideadeeperunder- our understanding of both the potential and the
standingofRLHF’spotentialandboundariesbut limitations of RLHF. They also underline the ne-
alsoemphasizethenecessityfordiverse,domain- cessityfortailoredevaluationmethodsincomplex
specificmethodswhenevaluatinggenerationqual- domains. Aswepersistinhoningandformulating
ity. Inthiscontext,theprogrammingdomainserves innovativetechniquesforefficientgeneration,the
asanexemplar. Thisresearch’sinsightscouldstim- lessons gleaned from our work will undoubtedly
ulatefurtheradvancementsinthedevelopmentof proveinvaluable.
novel and more suitable metrics for similar com-
plexdomains. Limitations
Inthisstudy,weattemptedtoinvestigatetheappli-
6 Conclusion
cationoftheGPTNeomodelwith125Mparame-
In conclusion, our study has demonstrated the tersinassessingthequalityoflinguisticmetricsin
effectiveness of RLHF in enhancing the perfor- theUsageAPIsubcategoryofquestion-and-answer
manceofsmallLLMslikeGPTNeo125Minthe data. Weacknowledgeseverallimitationsthatneed
programming domain. Our experiments focused tobetakenintoaccountwheninterpretingthere-
onfine-tuningthemodelusinguser-generatedre- sults.
sponses from Stack Overflow, employing two re- First,thedatausedintheexperimentsisdomain-
ward model training strategies, regression scores specific,sourcedexclusivelyfromtheUsageAPI
andcontrastivescores,withPPO. subcategory, which lacks code blocks. Although
The study also highlights the critical role of our findings demonstrate discrepancies between
employing the right evaluation measures. While linguisticmetricswithinthischosendomain,their
Rouge scores effectively captured response accu- generalizabilitytootherquestion-and-answercat-
racy,othermetricslikeBertScoreandSacreBLEU egories remains unclear. It’s possible that our re-
presentedambiguities,especiallywhenjuxtaposed wardmodelmayperformdifferentlywhenapplied
with the results from the trained reward models. tomorediversedatasetswithvariedquestiontypes
Thisdisparity,broughtintosharperfocusbynear- andcontent,includingthosethatincorporatecode
zeroSpearmancorrelations,impliesthattraditional blocks.
metricsmightnotsufficeforcomplexfieldssuch Second, the application of the small GPT Neomodelwith125M parametersrepresentsasignif- Sergey V. Kovalchuk, Vadim Lomshakov, and Artem
icant limitation in terms of both computational Aliev.2022. Humanperceivingbehaviormodeling
in evaluation of code generation models. In Pro-
capacityandthemodel’ssemanticunderstanding.
ceedingsofthe2ndWorkshoponNaturalLanguage
Theconstraintsofourcomputingresources,specif-
Generation,Evaluation,andMetrics(GEM),pages
icallytheusageof2NvidiaA6000GPUsandthe 287–294,AbuDhabi,UnitedArabEmirates(Hybrid).
necessitytoaccommodatethreemodelsduringthe AssociationforComputationalLinguistics.
RLHFtraining,haveimposedcertainrestrictions.
ReiichiroNakano,JacobHilton,SuchirBalaji,JeffWu,
OwingtoVRAMlimitations,portionsoftheques- Long Ouyang, Christina Kim, Christopher Hesse,
tion context were omitted during training, poten- ShantanuJain,VineetKosaraju,WilliamSaunders,
tiallyunderminingthemodel’sabilitytofullygrasp et al. 2021. Webgpt: Browser-assisted question-
answering with human feedback. arXiv preprint
thesemanticrelationsinthelanguage.
arXiv:2112.09332.
Anothercaveatconcernsthescaleofthemodel.
Whileourexperimentsillustratedthesmallmodel’s LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,
CarrollWainwright,PamelaMishkin,ChongZhang,
abilitytoenhanceresultstolevelscomparableto
SandhiniAgarwal,KatarinaSlama,AlexRay,etal.
its larger counterparts, the behavior of the larger
2022. Training languagemodelsto followinstruc-
modelsundersimilarexperimentalconditionsisyet tions with human feedback. Advances in Neural
tobeunderstood. Thisquestionremainsopenand InformationProcessingSystems,35:27730–27744.
warrantsfurtherinvestigationinfutureresearch.
John Schulman, Filip Wolski, Prafulla Dhariwal,
Insummary,ourstudyprovidesvaluableinsights Alec Radford, and Oleg Klimov. 2017. Proxi-
intotheuseofsmallerGPTNeomodelsforassess- malpolicyoptimizationalgorithms. arXivpreprint
ing linguistic metrics, but the highlighted limita- arXiv:1707.06347.
tionsunderscoretheneedforadditionalresearchin
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel
broaderdatacontexts,withlargermodels,andcon- Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford,
sideringtheintricatefacetsoflanguagetranslation DarioAmodei,andPaulFChristiano.2020. Learn-
ingtosummarizewithhumanfeedback. Advances
andreformulation.
inNeuralInformationProcessingSystems,33:3008–
3021.
References Qicai Wang, Peiyu Liu, Zhenfang Zhu, Hongxia Yin,
QiuyueZhang,andLindongZhang.2019. Atextab-
AmandaAskell,YuntaoBai,AnnaChen,DawnDrain,
stractionsummarymodelbasedonbertwordembed-
DeepGanguli,TomHenighan,AndyJones,Nicholas
dingandreinforcementlearning. AppliedSciences,
Joseph,BenMann,NovaDasSarma,etal.2021. A
9(21):4701.
generallanguageassistantasalaboratoryforalign-
ment. arXivpreprintarXiv:2112.00861.
DanielMZiegler,NisanStiennon,JeffreyWu,TomB
Brown, Alec Radford, Dario Amodei, Paul Chris-
Edward Beeching, Younes Belkada, Kashif Rasul,
tiano, and Geoffrey Irving. 2019. Fine-tuning lan-
Lewis Tunstall, Leandro von Werra, Nazneen Ra-
guage models from human preferences. arXiv
jani,andNathanLambert.2023. Stackllama: Anrl
preprintarXiv:1909.08593.
fine-tunedllamamodelforstackexchangequestion
andanswering.
A Spearmancorrelationtables
Stefanie Beyer, Christian Macho, Massimiliano
Inthisappendix,wepresentfigs.A1toA3thatfea-
Di Penta, and Martin Pinzger. 2020. What kind
of questions do developers ask on stack overflow? turecomparativetablesofSpearman’scorrelation
a comparison of automated approaches to classify coefficientsforseveralevaluationmetrics: Rouge
postsintoquestioncategories. EmpiricalSoftware
1,Rouge2,SacreBLEU,andBertScoreandused
Engineering,25:2258–2301.
twovariationsofrewardmodels,theregressiveand
Sid Black, Leo Gao, Phil Wang, Connor Leahy, and contrastive. They based on the generations pro-
StellaBiderman.2021a. Gpt-neo:Largescaleautore-
ducedbythreedistinctmodels. Thesemodelsare
gressivelanguagemodelingwithmesh-tensorflow. If
theBaseGPTNeo125M,theSFTGPTNeo125M,
youusethissoftware,pleaseciteitusingthesemeta-
data,58. andtheRLHFGPTNeo125M,respectively.
Sid Black, Leo Gao, Phil Wang, Connor Leahy,
and Stella Biderman. 2021b. GPT-Neo: Large
ScaleAutoregressiveLanguageModelingwithMesh-
Tensorflow. If youuse this software, pleasecite it
usingthesemetadata.FigureA1: SpearmancorrelationcoefficientsforBase
model
FigureA2: SpearmancorrelationcoefficientsforSFT
model
FigureA3: SpearmancorrelationcoefficientsforRLHF
model