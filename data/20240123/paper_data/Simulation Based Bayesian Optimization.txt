Simulation Based Bayesian Optimization
∗
Roi Naveiro
Quantitative Methods Department, CUNEF Universidad
Madrid, Spain 28040
and
Becky Tang
Department of Mathematics and Statistics, Middlebury College
Middlebury, VT 05753
January 22, 2024
Abstract
Bayesian Optimization (BO) is a powerful method for optimizing black-box func-
tionsbycombiningpriorknowledgewithongoingfunctionevaluations. BOconstructs
a probabilistic surrogate model of the objective function given the covariates, which
is in turn used to inform the selection of future evaluation points through an ac-
quisition function. For smooth continuous search spaces, Gaussian Processes (GPs)
are commonly used as the surrogate model as they offer analytical access to poste-
rior predictive distributions, thus facilitating the computation and optimization of
acquisition functions. However, in complex scenarios involving optimizations over
categorical or mixed covariate spaces, GPs may not be ideal.
ThispaperintroducesSimulationBasedBayesianOptimization(SBBO)asanovel
approachtooptimizingacquisitionfunctionsthatonlyrequiressampling-based access
to posterior predictive distributions. SBBO allows the use of surrogate probabilistic
models tailored for combinatorial spaces with discrete variables. Any Bayesian model
in which posterior inference is carried out through Markov chain Monte Carlo can
be selected as the surrogate model in SBBO. In applications involving combinatorial
optimization, we demonstrate empirically the effectiveness of SBBO method using
various choices of surrogate models.
Keywords: Acquisition functions, Bayesian decision theory, Black-box functions, Combina-
torial optimization, Markov chain Monte Carlo
∗Corresponding author. Email: roi.naveiro@cunef.edu
1
4202
naJ
91
]LM.tats[
1v11801.1042:viXra1 Introduction
The problem of optimizing a potentially nonlinear function f(x) over a certain set S has
been extensively studied. In many applications, the objective function is not explicitly
known. This can make optimization challenging, as it is difficult to determine the optimal
solution without accurate knowledge of the function. Moreover, f(x) may be expensive
to evaluate and a budget constraint often restricts evaluations of the objective function to
a sparse selection of points, further complicating the optimization task. As an example,
consider the process of drug discovery. During its early stages, the goal is to identify
candidate molecules with a desired property, such as maximum binding affinity to a given
biologicaltarget. However, measuringthisbindingaffinityrequiressynthesizingandtesting
molecules in vitro, limiting the number of leads that can be evaluated. Furthermore,
expressing affinity as a mathematical function of the molecule’s structure is not possible.
Bayesian Optimization (BO, Mockus (1994)) stands as an efficient approach for iden-
tifying the extrema of a black-box objective function. Its effectiveness originates from
its capacity to integrate prior knowledge about the function with the evaluations previ-
ously conducted, enabling informed decisions on where to carry out additional evaluations.
Bayesian Optimization involves the construction of a Bayesian surrogate model for f(x).
Subsequently, the next evaluation point is selected through the maximization of an acqui-
sition function (Frazier, 2018). Commonly used acquisition functions, including expected
improvement, are defined as the expected values of utility functions computed with respect
to the posterior predictive distribution of f(x) derived from the Bayesian surrogate model.
In the remainder of the paper, we will restrict our attention to this class of acquisition
functions.
Much of the BO literature focuses on continuous search spaces, where Gaussian Pro-
cesses (GPs, Rasmussen et al. (2006)) are predominantly chosen as the Bayesian surrogate
model. GPs are well-suited to optimization tasks in continuous search spaces as they rely
2on the smoothness defined by a kernel to model uncertainty. Moreover, GPs offer the ad-
vantage of yielding a analytical posterior predictive distribution (the posterior is Gaussian),
enabling the analytical computation of commonly used acquisition functions like expected
improvement or probability of improvement. However, in more intricate scenarios, such as
those involving combinatorial (i.e. categorical) or mixed categorical-continuous covariate
spaces (i.e. where the search space is a combination of discrete and continuous variables),
GPs may not be the most suitable choice.
Related work. In the last few years there has been some research in how to perform
Bayesian optimization over combinatorial domains. As highlighted by Wang et al. (2023),
this task comes with two major challenges: the construction of effective surrogate models
over the combinatorial space, and the effective search in the combinatorial domain for the
next structure for evaluation according to the acquisition function. Most previous research
hasbeendevotedtothefirsttask. Withinthis, animportantlineofinvestigationconsistsof
modifying GP kernel functions to accommodate categorical covariate spaces. For instance
Garrido-Mercha´n and Herna´ndez-Lobato (2020) treat categorical variables as continuous
via one-hot encoding. Then, within the kernel function, continuous covariates are suitably
transformed to one-hot encodings. Oh et al. (2019) constructs a combinatorial graph over
the combinatorial search space. Subsequently, graph Fourier transforms are used to derive
a diffusion kernel on the graph.
AnotherinterestinglineofresearchperformsBOoveracontinuouslatentrepresentation
of the combinatorial spaces learned through a Variational Autoencoder (Go´mez-Bombarelli
et al., 2018). Finally, a third line of research gets rid of the GPs, and proposes using
inherently discrete surrogate models. For instance Hutter et al. (2011) use random forests
to model the objective function given the covariates. Baptista and Poloczek (2018) model
theresponseasalinearfunctionofthecovariatesincludingallpossiblepairwiseinteractions.
A heavy-tailed horseshoe prior is used to induce sparsity on the coefficients.
3In this paper, we focus on the second challenge: the task of effective search within the
combinatorial domain. We introduce Simulation Based Bayesian Optimization (SBBO), a
novelapproachtooptimizeacquisitionfunctionsinBayesianOptimization. Unlikeprevious
approaches, SBBO does not require knowing the posterior predictive distribution analyt-
ically. All that is needed is sampling-based access to posterior predictive distributions.
This allows us to use any Bayesian model whose posterior inference is done via Markov
chain Monte Carlo (MCMC). Thus, determining where to make the next function evalu-
ation becomes a simpler task that does not rely on closed-form results that are typically
only available under the Gaussian surrogate. SBBO opens the door to using surrogate
probabilistic models that may be better tailored for Bayesian optimization tasks in com-
binatorial spaces, where the search for optimal solutions becomes more challenging due to
the presence of discrete variables.
The paper is structured as follows: Section 2 provides an overview of Bayesian Opti-
mization. Section 3 presents SBBO and summarises the surrogate probabilistic models that
we use for the experiments presented in Section 4. Finally, Section 5 gives some concluding
remarks and lines of future research.
2 Bayesian Optimization
Bayesian optimization (BO) is a class of methods centered on solving the optimization
problem
x∗ = maxf(x), (2.1)
x∈S
InthesettingofBayesianoptimization, theobjective function f(x)isassumedtobeablack
box, so no assumptions of convexity or linearity may be made. Additionally, it is assumed
that f is expensive to evaluate in that the number of evaluations of f that can be feasibly
performed is small, perhaps due to computing time or financial cost. It is further assumed
that querying the function at x provides a noisy observation y of f(x), with normal zero
4mean, σ2 variance noise.
BO uses a surrogate probabilistic model of the objective function, usually obtained
through Bayesian methods, to inform the next evaluation decision. It works in an iterative
algorithm by 1) fitting the surrogate model and 2) using an acquisition function to obtain a
newpointxatwhichweevaluateorobservef(x). Specifically, thesurrogatemodelisfitted
using all available data D = {(x ,y ),i = 1,...,n}. Then using an acquisition function,
1:n i i
weselectanewx andobtainacorrespondingy . Thesurrogatemodelisthenupdated
n+1 n+1
to include the additional data point (x ,y ). We briefly review each of these two key
n+1 n+1
components of Bayesian Optimization: the surrogate model and the acquisition function.
The following observational model is assumed
y ≡ y(x ) = f(x )+ϵ , ϵ i ∼id N(0,σ2) (2.2)
t t t t t
where f(x ) could be parametric or non-parametric. Given a new location x and appropri-
t
ate choice of prior, the posterior predictive distribution on f(x ) is taken as the surrogate
t
model. If a parametric model is chosen for f, then f(x ) will depend on parameters β and
t
is denoted as f (x ). A prior π(β) for the parameters β needs to be specified. Coupled
β t
withalikelihoodπ(y|x,β,σ2), afterobservingdataD , weobtaintheposteriorπ(β|D ).
1:n 1:n
Then, the surrogate model is
π(f(x)|x,D ) ≡ π(f (x)|x,D ) = E [f (x)]
1:n β 1:n β|D1:n β
If we choose instead a non-parametric model for f, then a prior π(f) over f needs to
be specified. Coupled with the likelihood π(y|f,σ2), we obtain the posterior distribution
π(f|D ) over f. Then for a given new location x, the evaluation of the posterior at x,
1:n
π(f(x)|x,D ), is taken as the surrogate model.
1:n
An acquisition function is used to direct the search for the optimal x∗ ∈ S. There are
several possible choices. In this paper we restrict our attention to the class of acquisition
functions that can be written as expectations of a utility function with respect to the
5posterior π(f(x)|x,D ). Some of the most commonly used acquisition functions belong
1:n
to this class, such as expected improvement and probability of improvement. Let u(f(x),x)
denote the utility achieved by evaluating the function at a new point x and obtaining f(x).
Thus, the next point x is chosen as the one that maximizes the acquisition function,
n+1
i.e. the expectation of the utility function:
x = argmaxE [u(f(x),x)]. (2.3)
n+1
x∈S
f|x,D1:n
Under the choice of GP prior for f, the posterior is Gaussian and under common choices
of the utility functions, there is a closed-form expression for the expectation (2.3). The BO
framework is indeed flexible, but computationally efficient and analytic results rely on this
choice of Gaussian process prior. However, in specific scenarios-such as when the search
space comprises categorical or mixed categorical-continuous variables—this GP prior may
not be the best fit. In these situations, various alternative Bayesian models better tailored
to the problem may be available. However, the alternatives often lack an analytically-
defined posterior distribution, obstructing the selection of the next point x according to
n+1
(2.3). We note, however, that we can sample from the posterior. To address this challenge
presented by a non-GP choice of prior, we introduce a Bayesian optimization methodology
that exclusively requires sampling access to the posterior, enabling the use of Bayesian
models better suited to structured covariates spaces.
It is important to emphasize that choosing the next sampling location in (2.3) implicitly
entailsadoptinga“one-step”assumption: theselectionismadeunderthepresumptionthat
no future evaluations will take place. When there is budget for more than one function
evaluation, Expected Improvement optimization unequivocally adopts a myopic approach.
While several alternative strategies have been proposed for multi-step lookahead Bayesian
Optimization(WuandFrazier,2019;Lametal.,2016), wefocusontheone-steplook-ahead
case.
63 Simulation Based Bayesian Optimization
We incorporate ideas from the simulation-based optimal Bayesian design literature (Mu¨ller,
2005) into the field of Bayesian Optimization, yielding our proposed method of Simulation-
Based Bayesian Optimization (SBBO).
3.1 Main Idea
As in all Bayesian Optimization tasks, we aim to solve the optimization problem 2.1. The
goal is to estimate the objective function f with few evaluations. We do not assume that
the objective function f is continuous, but otherwise all the difficulties of estimating f
hold.
Imagine we have access to (perhaps through previous experiments or function evalua-
tions) data D = {(x ,y ),i = 1,...,n}, where x is a vector of covariates for observation
1:n i i i
i and y is obtained from the observation model in (2.2). Suppose that we have a budget of
i
at most M more function evaluations. Using all of the available information, the task is to
determine the next x ,x ,...,x at which we will evaluate the objective function.
n+1 n+2 n+M
As is common practice in BO, rather than deciding all M new locations/vectors at once,
we proceed in a greedy manner by deciding on new locations in an iterative method.
To select the next point, x , we typically maximize an acquisition function com-
n+1
puted using π(f(x)|x,D ) over possible values of x. Let u(f(x),x) be a non-negative
1:n
and bounded utility function that quantifies the benefits/utility/gains from evaluating the
objective function at x and observing the corresponding f(x), assuming we have already
observed data D . Following Bayesian decision-theoretic principles, the next evaluation
1:n
should be performed at the location x that maximizes the expected utility
(cid:90)
Ψ(x) = u(f(x),x)π(f(x)|x,D )df (3.1)
1:n
In order to perform Bayesian optimization, all we need is to (1) decide on a suitable
7Bayesian surrogate model and (2) solve (3.1). As previously mentioned, except in special
cases, the posterior π(f(x)|x,D ) is typically only accessible through sampling. For this
1:n
purpose, we borrow ideas from the optimal Bayesian design community. The key idea is to
reframe optimization of (3.1) as a simulation problem. Following Bielza et al. (1999), we
define an augmented distribution g that incorporates x as a random variable:
g(x,f(x)|D ) ∝ u(f(x),x)π(f(x)|x,D ). (3.2)
1:n 1:n
It is straightforward to see that the marginal of g in x is proportional to the expected
utility defined in (3.1). Thus, the mode of this marginal distribution coincides with the
optimal next evaluation point. This suggests a simulation-based approach to find x : we
n+1
can simply simulate from (x,f(x)) ∼ g(x,f(x)|D ) using standard MCMC techniques
1:n
and compute the mode of g(x|D ) using the generated samples x. However, this strategy
1:n
is limited to low-dimensional x. For high-dimensional covariate spaces, estimating the
mode of g(x|D ) from the simulation output becomes impracticable. Mu¨ller et al. (2004)
1:n
propose an alternative augmented distribution g defined as
H
H
(cid:89)
g (x,f(x) ,...,f(x) |D ) ∝ u(f(x) ,x)π(f(x) |x,D ), (3.3)
H 1 H 1:n h h 1:n
h=1
where f(x) ,...,f(x) ) are H copies of the random variable f(x). Note that the marginal
1 H
of g in x is
H
g (x|D ) ∝ Ψ(x)H. (3.4)
H 1:n
Thus, the mode of this marginal distribution also coincides with the optimal point at which
we should make our next function evaluation. In particular, if H is sufficiently large, the
marginal distribution g (x) is tightly concentrated on the next x .
H n+1
Mu¨ller et al. (2004) propose an inhomogeneous MCMC sampling scheme to simulate
from g . They begin by generating samples from g using the standard Metropolis-Hastings
H
algorithm. In subsequent iterations, the target distribution is changed to g , with H
H
increasing across iterations. The inhomogeneous MCMC is constructed in such a way that
8the stationary distribution for fixed H is g . We use this idea to find the next evaluation
H
point in the context of Bayesian optimization. The algorithm is illustrated in Algorithm 1,
where a Metropolis-Hastings sampling scheme is used.
If the utility is non-negative and bounded and the state space of the inhomogeneous
Markov chain produced by this sampler is discrete, then Mu¨ller et al. (2004) show that
the chain is strongly ergodic if the updating schedule for H is logarithmic. Moreover, its
stationary distribution is uniform over the set of x maximizing (3.1).
A key point of this approach is that once a candidate x˜ has been proposed, the
n+1
distribution π(f(x)|x˜ ,D ) is used as proposal for generating f˜ (x) for h = 1,...,H.
n+1 1:n h
As a consequence, within each iteration of Algorithm 1, the acceptance ratio is:
(cid:40) (cid:41)
(cid:81)H u(f˜ (x) ,x˜ )π(f˜ (x) |x˜ ,D )π(f(x)(0)|x(0) ,D )q(x(0) |x˜ )
α = min 1, h=1 h n+1 h n+1 1:n h n+1 1:n n+1 n+1
(cid:81)H u(f(x)(0),x(0) )π(f(x)(0)|x(0) ,D )π(f˜ (x) |x˜ ,D )q(x˜ |x(0) )
h=1 h n+1 h n+1 1:n h n+1 1:n n+1 n+1
(cid:40) (cid:41)
(cid:81)H u(f˜ (x) ,x˜ )q(x(0) |x˜ )
= min 1, h=1 h n+1 n+1 n+1
(cid:81)H u(f(x)(0),x(0) )q(x˜ |x(0) )
h=1 h n+1 n+1 n+1
(cid:40) (cid:41)
q(x(0) |x˜ )
= min 1,exp(Hv˜−Hv(0)) n+1 n+1 .
q(x˜ |x(0) )
n+1 n+1
˜
Thus, the posterior predictive density for f(x) cancels out with the proposal density and
we do not need to evaluate it. This allows us to optimize (3.1) when just having sampling
access to the posterior, thus achieving our goal.
3.2 Gibbs Sampler
A possible variation of Algorithm (1) entails using a Gibbs sampler to simulate from g
H
rather than a Metropolis-Hastings scheme for every H. Recall that x is a vector of p
covariates. The conditional distribution of the s-th element of x can be written as
g (x |x ,f(x) ,...,f(x) ),D ) ∝
H s −s 1 H 1:n
(cid:40) (cid:41) (3.5)
H
(cid:88)
exp log[u(f(x) ,x ∪x ]+log[π(f(x) |x ∪x ,D )]
h s −s h s −s 1:n
h=1
9Algorithm 1 Simulation Based Bayesian Optimization
Obtain initial dataset D = {(x ,y ),i = 1,...,n}. Determine form of utility function
1:n i i
u (x,y).
D
Perform SBBO to find the next evaluation location.
1. Choose initial x(0) . For h = 1,...,H, simulate f(x)(0) ∼ π(f(x)|x(0) ,D )
n+1 h n+1 1:n
and evaluate corresponding utility u(0) = u(f(x(0) ),x(0) ). Define v(0) =
h n+1 n+1
1 (cid:80)H logu(0).
H h=1 h
2. Generate candidate x˜ from proposal distribution q(x˜ |x(0) ).
n+1 n+1 n+1
3. For h = 1,...,H, simulate f˜ (x) ∼ π(f(x)|x˜ ,D ) and evaluate corresponding
h n+1 1:n
utility u˜ = u(f˜ (x) ,x˜ ). Define v˜ = 1 (cid:80)H logu˜ .
h h n+1 H h=1 h
4. Compute
(cid:40) (cid:41)
q(x(0) |x˜ )
α = min 1,exp(Hv˜−Hv(0)) n+1 n+1
q(x˜ |x(0) )
n+1 n+1
5. Set


(x˜ ,v˜) w.p. α
 n+1
(x(1) ,v(1)) =
n+1
  (x(0) ,v(0)) w.p. 1−α
n+1
6. Increase H according to cooling schedule.
7. Repeat (2) - (6) until convergence.
Select x as the modal value of the simulated {x(B) ,x(B+1),x(B+2),...}, where the
n+1 n+1 n+1 n+1
first B simulated values are removed for burn-in.
Evaluate the true objective at x to obtain y .
n+1 n+1
Update dataset D = D ∪(x ,y )
1:(n+1) 1:n n+1 n+1
Set n = n+1.
For the particular case in which x is a vector of p categorical variables, this marginal is sim-
plythesoftmax
distributionover(cid:80)H
log[u(f(x) ,x ∪x ]+log[π(f(x) |x ∪x ,D )].
h=1 h s −s h s −s 1:n
10However, sampling from this distribution requires evaluating posterior predictive probabil-
ities, which defeats the purpose of SBBO. As an alternative, we could create a Gibbs sam-
pling scheme to sample from g (x ,x ,...,x |D ) where (f(x) ,...,f(x) ) has been
H 1 2 H 1:n 1 H
integrated out. Sampling from g (x |x ) can be done using Metropolis steps. Assuming
H s −s
thatthecurrentstateofthechainisx,f(x) ,...,f(x) andv = 1 (cid:80) h = 1H logu(f(x) ,x),
1 H H h
we can proceed as follows:
1. Propose a candidate x˜ using a probing distribution q (x˜|x) that only changes the
s s
s-th element of x.
2. For h = 1,...,H, simulate f˜ (x) ∼ π(f(x)|x˜ ∪x ,D ).
h s −s 1:n
3. Compute v˜ = 1 (cid:80)H logu(f(x˜ ∪x ) ,x˜ ∪x )
H h=1 s −s h s −s
4. Evaluate acceptance probability
(cid:26) (cid:27)
q(x|x˜ ∪x )
s −s
α = min 1,exp(Hv˜−Hv)
q(x˜ ∪x |x)
s −s
ThisGibbssamplingschemewouldbeembeddedintheinhomogenousMarkovchaindefined
in Algorithm 1.
SBBO allows us to naturally accomodate the scenario of mixed categorical-continuous
covariate spaces. We simply adapt the proposal distribution of each particular element.
For instance, if x is continuous we could use a Gaussian proposal distribution, while if
q
it is categorical we could use a discrete uniform proposal over all possible choices for the
covariate.
3.3 Probabilistic Surrogate Models
We can now optimize (3.1) for any probabilistic surrogate model, as long as we have access
to posterior samples of f. This flexibility allows us to consider a wide range of surrogate
models. In particular, we have tested the following models, which may be well-suited for
cases where S is the space of p-dimensional categorical covariates.
11Tanimoto Gaussian Process. This surrogate model is particularly useful when x ∈
{0,1}p. The uncertainty on f(x) is modeled through a Gaussian Process with the following
kernel function:
ϕ(x·x′)
k(x,x′) = .
∥x∥2 +∥x′∥2 −x·x′
The numerator is proportional to the number of dimensions where both x and x′ are 1,
and the denominator signifies the total number of dimensions where either x or x′ or both
is 1. The parameter ϕ is usually set to the value maximizing the marginal likelihood
thus following a empirical Bayes approach. For categorical covariates with more than
two possible values, we would need to first convert the covariates into appropriate binary
variables (e.g. via one-hot encoding) before computing the previous kernel. Under this
model, the posterior predictive distribution is Gaussian with a certain mean and variance
analytically available as in any GP.
Sparse Bayesian linear regression. The response variable is modeled as a linear func-
tion of the covariates, including all possible pairwise interactions. A heavy-tailed horseshoe
prior is used to induce sparsity.
(cid:88) (cid:88)
y = f (x)+ϵ = α + α x + α x x +ϵ (3.6)
α 0 j j ij i j
j i,j>i
ϵ ∼ N(0,σ2) (3.7)
α |β ,τ,σ2 ∼ N(0,β2τ2σ2)
k k k
β ,τ ∼ Cauchy (1)
k [0,∞)
p(σ2) ∝ σ−2
This model was first proposed by Baptista and Poloczek (2018) in the context of
Bayesian Optimization. Posterior inference is achieved through Gibbs sampling, as de-
tailed in Baptista and Poloczek (2018). This method allows us to sample from the pos-
terior of α ,β ,τ, and σ2, facilitating easy draws from the posterior of f (x) that could
k k α
12be directly utilized in Algorithm 1. In this scenario, access to analytical posterior predic-
tive distributions is unavailable, making the computation of common acquisition functions
challenging. To address this, Baptista and Poloczek (2018) propose a tailored acquisition
function: a single posterior sample of α is produced, and the acquisition function is de-
fined as f (x)−λ∥x∥ , where a penalty term controlled by λ is included. This acquisition
α 1
function now has a closed-form, appearing as a quadratic form. Its optimization can be
reformulated as a semidefinite program that can be efficiently approximated in polynomial
time to a desired precision. With SBBO, there is no need for tailored acquisition functions
as we just require having sampling access to the posterior predictive distribution. This
enables using model (3.6) with the common acquistion functions in the BO literature.
NGBoost. Natural Gradient Boosting (NGBoost, Duan et al. (2020)) is a non-Bayesian
algorithm for generic probabilistic prediction via gradient boosting. In NGBoost, the con-
ditional distribution of the output given covariates is modelled through a parametric distri-
bution y|x ∼ P (x). The parameters θ(x) are obtained through an additive combination
θ
of M base learners and an initial θ(0).
M
(cid:88)
θ = θ(0) −η ρ(m) ·f(m)(x)
m=1
whereρ(m) arescalingfactorsandη isacommonlearningrate. Thebaselearnersaretrained
to minimize a proper scoring rule using a refinement of the gradient boosting algorithm.
For this training process, NGBoost employs the natural gradient, a variant of gradient
descent that takes into account the underlying geometry of probability distributions. This
leads to faster convergence and improved stability during training.
A notable strength of NGBoost lies in its adaptability, as it can be paired with various
base learners. Our experiments focus on two such learners: shallow decision trees (referred
to as NGBoost-dec) and linear regressions with lasso regularization (named NGBoost-
linCV). For both these configurations, we utilize the log-score as the scoring rule.
13Bayesian Neural Network. Bayesian Neural Networks (BNNs) are probabilistic deep
learning models that replace deterministic weight values of a neural network with weight
distributions. This allows the model to capture the existing epistemic uncertainty in the
predictions due to limited training data. In our experiments, we employ a simple three-
hidden layers fully-connected BNN whose inputs are the one-hot encoded covariates. We
trainthismodelusingvariationalinference, combinedwiththereparametrizationestimator
technique (Kingma and Welling, 2013).
4 Experiments
We conduct experiments in three different combinatorial optimization problems1: a binary
quadratic program with 10 variables, a contamination control in a food supply chain with
25 stages, and RNA sequence optimization with a sequence length of 30. Details of the
problem settings are provided in the subsequent subsections. We evaluate the performance
ofSBBOwiththevariousprobabilisticsurrogatemodelsdescribedattheendoftheprevious
section: Sparse Bayesian linear regression with interactions (sbbo-BLr), Bayesian Neural
Network(sbbo-BNN),TanimotoGaussianProcessRegression(sbbo-GPr), naturalgradient
boosting with a shallow decision tree model as base learner (sbbo-NGBdec), and natural
gradient boosting with an sparse linear regression as base learner (sbbo-NGBlinCV).
We compare performance of these SBBO approaches with two benchmarks: simulated
annealing (SA), a widely used algorithm in combinatorial optimization problems (Spears,
1993) and random local search (RS, Bergstra and Bengio (2012)).
The experimental setup was design as follows. We start with a random initial dataset
containing five pairs of covariates and responses. For each probabilistic surrogate model
and fixed number of iterations t, we follow these steps:
1All code to reproduce the experiments in this paper has been open sourced and is available at https:
//github.com/roinaveiro/sbbo.
141. Usethesurrogatemodeltodeterminethenextevaluationpoint,utilizingtheexpected
improvement as the acquisition function. Thus, the utility function is defined as
u(x) = max[f(x)−f∗(x),0], where f∗(x) represents the current estimate of the
optimal true objective. For SBBO algorithms, we employ a Metropolis-Hastings
sampling scheme, as described in Algorithm 1. The proposal distribution randomly
selects one of the covariates and proposes another value, chosen uniformly at random,
which is accepted according to the acceptance probability in step 4 of Algorithm 1.
The chosen cooling schedule incrementally increases H from 1 to 10000 in steps of
250.
2. Once the next evaluation point is determined, query the black box function to obtain
a noisy observation of the true objective at that location.
3. Append the new pair of data to the dataset and repeat the process.
For each algorithm, we report the best function value found after t evaluations of the
true objective, averaged over 10 runs, with one standard deviation divided by the square
root of the number of runs as the margin of error. Note that t varies depending on the
problem setting.
4.1 Binary Quadratic Problem
The aim here is to maximize a quadratic function while incorporating l regularization.
1
This objective function can be expressed as x⊤Qx−λ∥x∥ , where x is a binary vector of
1
dimension d = 10. The matrix Q is a random d×d matrix with entries generated as follows:
each element is simulated as independent standard Gaussian, and the resulting matrix is
then element-wise multiplied by another d × d matrix K. The elements of matrix K are
definedasK = exp[−(i−j)2/L2],wherethecorrelationlengthL2 governshowrapidlythe
ij c c
valuesofK decayawayfromthediagonal. AlargervalueofL2 resultsinadensermatrixQ,
c
15Table 1: Average objective function values and margin of error for each algorithm, obtained
after 120 iterations for the binary quadratic problem, 500 iterations for the contamination
control problem and 300 iterations for the RNA problem. Bold values denote the best value
for each problem.
Algorithm Binary Quadratic Problem Contamination Problem RNA Problem
RS 9.86±0.27 21.74±0.04 −13.74±0.63
SA 10.75±0.33 21.44±0.04 −9.21±0.61
sbbo-BLr 11.24±0.00 21.26±0.01 −22.65±0.59
sbbo-BNN 10.90±0.22 21.29±0.03 −16.08±0.56
sbbo-GPr 10.76±0.34 21.20±0.00 −15.05±0.92
sbbo-NGBdec 10.65±0.28 21.38±0.02 −13.82±0.59
sbbo-NGBlinCV 11.10±0.09 21.37±0.03 −14.04±0.62
which makes the optimization task more challenging. In our experiments, we set λ to 0 and
fixedL2 at10. Theperformancecomparisonofthedifferentalgorithmsover120iterationsis
c
illustrated in Figure 1 and detailed in the second column of Table 1. As can be seen, SBBO
outperforms RS and SA for most surrogate models. Notably, SBBO with sparse Bayesian
linear regression achieves the highest objective value, closely followed by SBBO with NGB
employing a sparse linear model as the base learner. An essential aspect is the assessment
of performance with a limited number of objective function evaluations, as evaluating the
true objective is the common bottleneck in most BO applications. Remarkably, SBBO-
BLr, SBBO-BNN, and SA demonstrate good performance for low number of evaluations
(<25) (Fig. 1). However, it is worth noting that SA appears to get stuck in local optima
around the 65th evaluation and SBBO-BNN performance does not grow as much as that
of SBBO-BLr.
In this experiment, with a search space cardinality of 1024, it is feasible to compute
the global maximizer of the objective function via complete ennumeration (11.24). Ta-
16Figure 1: BQP problem: average objective function values by number of evaluations,
colored by algorithm. Large objective values indicate better performance.
ble 2 displays the deviation between the optimal solution discovered within 120 function
evaluations for each algorithm and the global optimum. Notably, sbbo-BLr consistently
identifies the optimal solution within 120 ≪ 1024 iterations across all 10 repetitions of the
experiment.
4.2 Contamination Control Problem
The contamination control problem was proposed by Hu et al. (2010). Consider a food
supply with d = 25 stages that may be contaminated with pathogenic microorganisms. Z
i
denotes the fraction of food contaminated at the i-th stage of the supply, for 1 ≤ i ≤ d. Z
evolves according to the random process described as follows. At stage i, a prevention effort
(with fixed cost c ) can be made that decreases contamination a random rate Γ ∈ (0,1).
i i
If no prevention is taken, contamination spreads at a random rate Λ ∈ (0,1). This results
i
17Table 2: Binary quadratic problem: average distance to global optimum achieved after 120
iterations with margin of error. Bold indicates best average performance.
Algorithm Distance to global optimum
RS 1.38±0.23
SA 0.49±0.33
sbbo-BLr 0.00±0.00
sbbo-BNN 0.34±0.22
sbbo-GPr 0.48±0.34
sbbo-NGBdec 0.59±0.28
sbbo-NGBlinCV 0.14±0.09
in the following recursive equation
Z = Λ (1−x )(1−Z )+(1−Γ x )Z
i i i i−1 i i i−1
where x ∈ {0,1} is the decision variable associated with the prevention effort at stage i,
i
and x = 1 denotes the the prevention effort was taken. The goal is to decide, at each stage
i
i, whether to implement a prevention effort. The problem is constrained in that the cost
should be minimized while ensuring that at each stage, the fraction of contaminated food
does not exceed an upper limit U with probability at least 1 − ϵ. The random variables
i
Γ and Λ as well as the initial fraction of contaminated food Z follow beta distributions.
i i 1
We set U = 0.1 for all i and ϵ = 0.05, thus adopting the same framework as in Baptista
i
and Poloczek (2018).
We consider the Lagrangian relaxation of the problem that is given by
(cid:34) (cid:35)
d T
(cid:88) ρ (cid:88)(cid:0) (cid:1)
argmin c x + 1 −(1−ϵ) +λ∥x∥
x
i i
T
{Z ik>Ui} 1
i=1 k=1
where each violation is penalized by ρ = 1. We set T = 100. An l regularization term
1
with parameter λ = 0.0001 was added to encourage the prevention efforts to occur at a
18Figure 2: Contamination control problem: average objective function values by number of
evaluations, colored by algorithm. Small objective values indicate better performance.
small number of stages. The performance comparison of the different algorithms over 500
iterations is illustrated in Figure 2 and detailed in the third column of Table 1. For a small
number of function evaluations, simulated annealing performs very well, but quickly gets
stuckinalocaloptimum. SBBO-BLr, ontheotherhand, showcasesconsistentperformance
acrossbothlowandhighnumbersoffunctionevaluations. Notably, SBBOwithaTanimoto
Gaussian Process as the surrogate probabilistic model yields the most accurate estimation
of the global minimum within 500 function evaluations. However, during initial iterations,
this approach falls short of the performance achieved by SBBO-BLr and SA. Importantly,
above ∼ 350 function evaluations, all SBBO-based methods outperform SA and RS.
194.3 RNA design
An RNA sequence can be represented as a string A = a ···a of length p, where each
1 p
a ∈ {A,U,G,C} corresponds to a nucleotide letter. The secondary structure of this RNA
i
is defined as the arrangement of base pairs that minimizes its free energy. Several RNA
folding algorithms, such as the one proposed by Zuker and Stiegler (1981), employ dynamic
programming and a thermodynamic model to estimate the Minimum Free Energy (MFE)
of a given sequence. However, the computational complexity of these algorithms is O(p3),
which becomes a bottleneck when dealing with a large number of RNA sequences. To
address this challenge, we frame the RNA sequence optimization problem as in Dadkhahi
et al. (2022): given a sequence of length p, the objective is to identify the RNA sequence
that forms a secondary structure with the lowest MFE. In our optimization process, we
utilize the well-known RNAfold package developed by Lorenz et al. (2011) to assess the
MFE for a given RNA sequence. Our ultimate goal is to discover the RNA sequence with
the lowest MFE while minimizing the number of calls made to the MFE evaluator. The
results that follow correspond to sequences of length p = 30.
Figure3andthelastcolumnofTable1showcasetheperformancecomparisonofvarious
algorithms over 300 iterations. Notably, Simulated Annealing significantly under-performs,
even when compared to Random Local Search, possibly due to the extensive search space
(cardinality of 260). Remarkably, SBBO-BLr outperforms its counterparts by a significant
margin. Although SBBO with a Bayesian Neural Network as a surrogate model excels in
scenarios with a low number of function evaluations and finds the second-best maximizer
after 300 objective function evaluations, its performance remains notably distant from that
of SBBO-BLr.
20Figure 3: RNA problem: average objective function values by number of evaluations,
colored by algorithm. Small objective values of minimum free energy (MFE) indicate
better performance.
4.4 General Comments
While most Bayesian Optimization literature have utilized Gaussian Processes as surrogate
probabilistic models, our empirical study suggests that, for combinatorial optimization
problems, alternative Bayesian models may lead to superior performance. Notably, in
two out of three experiments, Sparse Bayesian linear regression with pairwise interactions
demonstrated clear superiority over GPs and maintained competitiveness in the remaining
experiment. This underscores the significance of exploring a broader range of Bayesian
models within the realm of Bayesian Optimization.
Given that many Bayesian models lack closed-form expressions for posterior predictive
distributions, our study emphasizes the relevance of developing algorithms to optimize
acquisition functions in Bayesian Optimization when only sampling access to posteriors is
21available. This is particularly important as inference in most Bayesian models relies on
simulation. SBBO aligns precisely with this objective.
5 Conclusion and Future Work
We have proposed SBBO, a simulation-based framework for optimizing acquisition func-
tions in Bayesian Optimization that just requires sampling access to posterior predictive
distributions of surrogate models. This flexibility enables the utilization of any Bayesian
model whose posterior inference relies on MCMC as surrogate probabilistic model in BO.
This can have advantages in non-standard black-box function optimization settings, as in
thecaseofcombinatorialsearchspaces. Wehavedemonstratedempiricallytheeffectiveness
of SBBO under several surrogate probabilistic models in three combinatorial optimization
problems.
Severalfuturelinesofresearcharepossible. Fromamethodologicalperspective, wehave
only considered the greedy one-step look-ahead BO. SBBO could be extended to multi-step
look-ahead BO. However, this extension is not direct. From a computational perspective
we would suffer from the standard inefficiencies of dynamic programming. Some ideas in
Bayesian sequential design might be relevant Wathen and Christen (2006); Brockwell and
Kadane (2003); Drovandi et al. (2014). In terms of sampling strategies, we have proposed
Metropolis-HastingsandGibbssamplerstosamplefromtheaugmenteddistribution. Other
options could be adopted to better explore high-dimensional multimodal surfaces, such as
particle methods Amzal et al. (2006). In addition, when dealing with continuous search
spaces, MCMC samplers that leverage gradient information, such as Hamiltonian Monte
Carlo or the Metropolis-adjusted Langevin algorithm could be used.
Ontheapplicationsside, giventhatBayesianlinearregressionwithpairwiseinteractions
seems to work well, a model that captures interactions between three or more covariates
is of interest. However, explicitly accounting for beyond-pairwise interactions becomes
22unfeasible when the dimensionality of the search space increases. As an alternative, some
non-parametricapproachessuchasBayesianadditiveregressiontressChipmanetal.(2010)
that capture relevant interactions between more than two covariates could be utilized.
Finally, SBBO could be used with likelihood-free surrogate models, from which posterior
predictive samples could be obtained via Approximate Bayesian Computation.
Acknowledgements: The authors gratefully acknowledge Spain’s consul in Chicago for
his valuable help facilitating this collaboration during the pandemic. This project is based
upon work supported by the Spanish state investigation agency under the Proyectos de
Generaci´on de Conocimiento 2022 grant No. PID2022-137331OB-C33.
Disclosure statement: The authors report there are no competing interests to declare.
References
Amzal, B., Bois, F. Y., Parent, E., and Robert, C. P. (2006). “Bayesian-optimal design via
interacting particle systems.” Journal of the American Statistical association, 101(474):
773–785. 22
Baptista, R.andPoloczek, M.(2018). “Bayesianoptimizationofcombinatorialstructures.”
In International Conference on Machine Learning, 462–471. PMLR. 3, 12, 13, 18
Bergstra, J. and Bengio, Y. (2012). “Random search for hyper-parameter optimization.”
Journal of machine learning research, 13(2). 14
Bielza, C., Mu¨ller, P., andInsua, D.R.(1999). “Decisionanalysisbyaugmentedprobability
simulation.” Management Science, 45(7): 995–1007. 8
Brockwell, A. E. and Kadane, J. B. (2003). “A gridding method for Bayesian sequential
decision problems.” Journal of Computational and Graphical Statistics, 12(3): 566–584.
22
23Chipman, H. A., George, E. I., and McCulloch, R. E. (2010). “BART: Bayesian additive
regression trees.” 23
Dadkhahi, H., Rios, J., Shanmugam, K., and Das, P. (2022). “Fourier representations for
black-box optimization over categorical variables.” In Proceedings of the AAAI Confer-
ence on Artificial Intelligence, volume 36, 10156–10165. 20
Drovandi, C. C., McGree, J. M., and Pettitt, A. N. (2014). “A sequential Monte Carlo
algorithm to incorporate model uncertainty in Bayesian sequential design.” Journal of
Computational and Graphical Statistics, 23(1): 3–24. 22
Duan, T., Anand, A., Ding, D. Y., Thai, K. K., Basu, S., Ng, A., and Schuler, A. (2020).
“Ngboost: Natural gradient boosting for probabilistic prediction.” In International con-
ference on machine learning, 2690–2700. PMLR. 13
Frazier, P. I. (2018). “A tutorial on Bayesian optimization.” arXiv preprint
arXiv:1807.02811. 2
Garrido-Mercha´n, E. C. and Herna´ndez-Lobato, D. (2020). “Dealing with categorical and
integer-valued variables in bayesian optimization with gaussian processes.” Neurocom-
puting, 380: 20–35. 3
Go´mez-Bombarelli, R., Wei, J. N., Duvenaud, D., Herna´ndez-Lobato, J. M., Sa´nchez-
Lengeling, B., Sheberla, D., Aguilera-Iparraguirre, J., Hirzel, T. D., Adams, R. P., and
Aspuru-Guzik, A. (2018). “Automatic chemical design using a data-driven continuous
representation of molecules.” ACS central science, 4(2): 268–276. 3
Hu, Y., Hu, J., Xu, Y., Wang, F., and Cao, R. Z. (2010). “Contamination control in food
supply chain.” In Proceedings of the 2010 Winter Simulation Conference, 2678–2681.
IEEE. 17
24Hutter, F., Hoos, H. H., and Leyton-Brown, K. (2011). “Sequential model-based optimiza-
tion for general algorithm configuration.” In Learning and Intelligent Optimization: 5th
International Conference, LION 5, Rome, Italy, January 17-21, 2011. Selected Papers
5, 507–523. Springer. 3
Kingma, D. P. and Welling, M. (2013). “Auto-encoding variational bayes.” arXiv preprint
arXiv:1312.6114. 14
Lam, R., Willcox, K., and Wolpert, D. H. (2016). “Bayesian optimization with a finite bud-
get: An approximate dynamic programming approach.” Advances in Neural Information
Processing Systems, 29. 6
Lorenz, R., Bernhart, S. H., Ho¨ner zu Siederdissen, C., Tafer, H., Flamm, C., Stadler,
P. F., and Hofacker, I. L. (2011). “ViennaRNA Package 2.0.” Algorithms for molecular
biology, 6: 1–14. 20
Mockus, J. (1994). “Application of Bayesian approach to numerical methods of global and
stochastic optimization.” Journal of Global Optimization, 4: 347–365. 2
Mu¨ller, P. (2005). “Simulation based optimal design.” Handbook of Statistics, 25: 509–518.
7
Mu¨ller, P., Sans´o, B., andDeIorio, M.(2004). “OptimalBayesiandesignbyinhomogeneous
Markov chain simulation.” Journal of the American Statistical Association, 99(467):
788–798. 8, 9
Oh, C., Tomczak, J., Gavves, E., and Welling, M. (2019). “Combinatorial bayesian opti-
mization using the graph cartesian product.” Advances in Neural Information Processing
Systems, 32. 3
Rasmussen, C. E., Williams, C. K., et al. (2006). Gaussian processes for machine learning,
volume 1. Springer. 2
25Spears, W. M. (1993). “Simulated annealing for hard satisfiability problems.” Cliques,
Coloring, and Satisfiability, 26: 533–558. 14
Wang, X., Jin, Y., Schmitt, S., and Olhofer, M. (2023). “Recent advances in Bayesian
optimization.” ACM Computing Surveys, 55(13s): 1–36. 3
Wathen, J. K. and Christen, J. A. (2006). “Implementation of backward induction for
sequentially adaptive clinical trials.” Journal of Computational and Graphical Statistics,
15(2): 398–413. 22
Wu, J. and Frazier, P. (2019). “Practical two-step lookahead Bayesian optimization.”
Advances in neural information processing systems, 32. 6
Zuker, M. and Stiegler, P. (1981). “Optimal computer folding of large RNA sequences using
thermodynamics and auxiliary information.” Nucleic acids research, 9(1): 133–148. 20
26SUPPLEMENTARY MATERIAL
A Empirical assessment of convergence
We include Figure 4 to provide empirical guarantees of SBBO convergence. To make this
plot, weinitializedthecontaminationproblemdescribedinSection4.2withadatasetD
1:100
of 100 of pairs of covariates and outcomes. Metropolis Hastings SBBO (Algorithm 1) with
Tanimoto Gaussian Process regression as surrogate probabilistic model was used to find
the subsequent evaluation location x . This process was repeated 40 times to account
101
for the stochasticity of the SBBO algorithm. In each repetition, the cooling schedule
incrementally increases H from 50 to 2500 in steps of 10. For each H, we approximate the
posterior predictive utility of the solution found by SBBO for that value of H via Monte
Carlo. Expected improvement was used as utility function. H is represented in the x axis
and the corresponding posterior predictive utility in the y axis. Each gray line corresponds
to a repetition and the red line indicates the average.
Figure 4: SBBO with Metropolis-Hastings convergence
27