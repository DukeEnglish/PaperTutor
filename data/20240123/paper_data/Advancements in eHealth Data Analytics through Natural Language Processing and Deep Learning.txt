ADVANCEMENTS IN EHEALTH DATA ANALYTICS THROUGH
NATURAL LANGUAGE PROCESSING AND DEEP LEARNING
Elena-SimonaApostolandCiprian-OctavianTruica˘
NationalUniversityofScienceandTechnologyPolitehnicaBucharest,Bucharest,Romania
elena.apostol@upb.ro, ciprian.truica@upb.ro,
ABSTRACT
The healthcare environment is commonly referred to as "information-rich" but also "knowledge
poor". Healthcaresystemscollecthugeamountsofdatafromvarioussources: labreports,medical
letters, logsofmedicaltoolsorprograms, medicalprescriptions, etc. Thesemassivesetsofdata
canprovidegreatknowledgeandinformationthatcanimprovethemedicalservices, andoverall
thehealthcaredomain,suchasdiseasepredictionbyanalyzingthepatient’ssymptomsordisease
prevention, byfacilitatingthediscoveryofbehavioralfactorsfordiseases. Unfortunately, onlya
relativelysmallvolumeofthetextualeHealthdataisprocessedandinterpreted,animportantfactor
beingthedifficultyinefficientlyperformingBigDataoperations. Inthemedicalfield, detecting
domain-specificmulti-wordtermsisacrucialtaskastheycandefineanentireconceptwithafew
words. Atermcanbedefinedasalinguisticstructureoraconcept,anditiscomposedofoneormore
wordswithaspecificmeaningtoadomain. Allthetermsofadomaincreateitsterminology. This
chapteroffersacriticalstudyofthecurrent,mostperformantsolutionsforanalyzingunstructured
(image and textual) eHealth data. This study also provides a comparison of the current Natural
LanguageProcessingandDeepLearningtechniquesintheeHealthcontext. Finally,weexamineand
discusssomeofthecurrentissues,andwedefineasetofresearchdirectionsinthisarea.
Keywords eHealth·TextualMedicalData·NaturalLanguageProcessing·DeepLearning
1 Introduction
Thehealthcareenvironmentiscommonlyreferredtoas"information-rich"butalso"knowledgepoor". Healthcare
systems collect huge amounts of data from various sources: lab reports, medical letters, logs of medical tools or
programs,medicalprescriptions,etc. Thesemassivesetsofdatacanprovidegreatknowledgeandinformationwhich
can improve the medical services, and overall the healthcare domain, such as disease prediction by analyzing the
patient’ssymptomsordiseaseprevention,byfacilitatingthediscoveryofbehavioralfactorsfordiseases. Unfortunately,
onlyarelativelysmallvolumeofthetextualeHealthdataisprocessedandinterpreted,animportantfactorbeingthe
difficultyinefficientlyperformingBigDataoperations.
Thischapteroffersacriticalstudyofthecurrent,mostperformantsolutionsforanalyzingunstructuredeHealthdata.
Frompatientrecords,healthplans,orinsuranceinformationtomedicalimages,themajorityofmedicaldatagenerated
isunstructured. Unstructureddataisgenerallydefinedasdataforwhichitisdifficultorimpossibletocreateatypical
databaseschema.
Inthemedicalfield,detectingdomain-specificmulti-wordtermsisacrucialtaskastheycandefineanentireconcept
withafewwords. Atermcanbedefinedasalinguisticstructure,aconcept,anditiscomposedofoneormorewords
withaspecificmeaningtoadomain. Allthetermsofadomaincreateitsterminology. Usingasabaselinethetermsin
theterminology,themedicaldatasetsaredividedintolinguistictermsandthenarerepresentedintovectorsofidentifiers.
ThistransformationisdoneusinganalgebraicmodeldenotedasVectorSpaceModels. Thecurrentmostperformant
vectorspacemodelsarethewordembeddings,thatconsidercontextualfeatures,determinesdomain-specificterms,
andifnecessary,alsomaycorrecttextmisspellingbeforeoutputtingthefinalvectorofidentifiers. Inthischapter,we
provideacriticalanalysisofdifferentvectorspacemodels,togetherwiththeirapplicabilityformedicaldata.
4202
naJ
91
]LC.sc[
1v05801.1042:viXraE.S.ApostolandC.OTruica˘
Theoutputofavectorspacemodelisthenfedtolearningmodels. Astremendousamountofdataisgeneratedinthe
medicalfield,theneedtoapplyBigDataDeepLearningtechniquestomakeuseofthisinformationbecomesmoreand
moreacute. Therefore,inthischapter,wealsoprovideadescriptionandcomparisonofthecurrentDeepLearning
techniques,togetherwiththesetofmedical-specifictasksthateachpresentedDeepLearningmodelsolves.
Finally,thedeeplearningmodelscanalsobeappliedtomedicalimageswiththeaim: i)ofdeterminingspecificobjects,
suchastumours,orii)ofestablishingrelationshipsbetweenthefeaturesextractedfromanimagedatasetandsome
correspondingclasses,e.g. usedinprediagnosisofdifferentdiseases. Subsequently,attheendofthischapter,we
presentabriefdiscussionofdeeplearningmodelsformedicalimages.
2 NaturalLanguageProcessingforeHealthdata
Text analytics consists of a set of techniques that apply software algorithms in order to understand the content of
unstructuredtextualdata,morepreciselyofwrittenlanguage. Sometechniques,suchasrule-basedclassification,are
usingasetofrulesorlogicalstatementsdefinedbyspecialists. Naturallanguageprocessing(NLP)employsmore
complextechniquesthatconsiderthecontextandthemeaningofwords,andnotonlysearchingforindividualterms.
Asaresult,suchapproacheshave,inmostcases,bettermetrics,i.e.,accuracy. IneHealth,NLPsolutionsareveryoften
usedinthemedicalstaffintaskssuchasscreening,diagnosingtreatment,orpatientmonitoring. Tothisend,theNLP
techniquesareaddedtocomplexeHealthadaptivesystems,i.e.,computer-aideddiagnosticsystemsanddata-driven
decisionsupportsystems. IntheBigDatacontext,thesesystemsarefedwithlargeamountsofdatacollectedfrom
differentsources,whichtheythenusetolearnhowtoprovidecertainservices. Anexampleisimprovingtheefficiency
indeterminingthediagnosisortherighttreatment,bycombiningthecurrentpatientmedicalresultswithinformation
fromhis/herhistoricalhealthcaredataandtheinformationlearntfromothersimilarcases.
InordertoobtainusefulinformationandtomodelitaccordingtotheneedsoftheeHealthapplicationorservice,NLP
utilisesaprocessingpipeline. Dependingonthedesiredoutput,theNLPpipelinemayconsistofdifferentspecificsteps,
buttheyalwaysgroupintothreemajorsteps: Preprocessing,VectorSpaceModelandDeployment(Figure1).
Figure1: ThePipelinefortextualdataprocessing
Duringthepreprocessingstep,thetextcollectionorthecorpusisseparatedintolinguisticunits,e.g.,words,expressions,
sentences etc. From them, the most important information is extracted, information that will be then given to the
languagemodeltodeducenewmeaningsfromit. Differentsub-stepscanbeaddedtothepreprocessingstage. For
example,whenconsideringdatagatheredfromphysicians’observationsandannotations,somewordsmaybewrongly
spelt,andthusaspell-checkingsub-stepcanbeintroduced.
Inthesecondstep,thebuildingofthespace-vectororterm-vectormodeliscarriedout. Aspacevectormodelrepresents
thelinguistictermsasvectorsofweights. TheresultofthisstepwillbefedtothelanguagemodelintheDeployment
step.
ThefinalmajorstepoftheNLPpipelineisbuildingthelanguagemodel. Themainobjectiveofalanguagemodel
istoestimatetheprobabilitydistributionofthedifferentlinguisticunits. Therearetwoclassesoflanguagemodels:
count-basedandcontinuous-space.
Thecount-basedmodels,suchlikestatisticallanguagemodels,usuallyimplyestimatingtheprobabilityoftheoccurrence
ofparticularwordsequencesbasedonsamplesoftrainingdatasets. Thereareseveralknowndisadvantagesofthistype
2AdvancementsineHealthDataAnalyticsthroughNLPandDL
ofmodels. Oneimportantdrawbackisthatmostofthecount-basedmodelsarenotadaptive,astheirresultsarebased
onexactmatchingofknownpatterns. Asaresult,findingnewunknownwordsduringtestingischallenging.
Thecontinuous-spaceclassreferstotheneurallanguagemodels. Theideabehindthesemodelsisthatthesystem
learnsaneffectiverepresentationofthemeaningsofthewords. Althoughthisincreasestheircomplexityasopposedto
classicalcount-basedmethods,italsogivesthembetterperformance.
2.1 StateoftheartinpreprocessingeHealthtext
As mentioned earlier in this chapter, there is a large quantity of unstructured text in eHealth that is gathered from
differentsources,e.g.,physicians’observations,annotationsmadetocertainmedicalimages,medicalprescriptions,and
soforth. Duetothisheterogeneity,thecollectedmedicaldatacannotbedirectlyfedtothelearningmodel,anditmust
firstgothroughapreprocessingstep. DatapreprocessingisthefirststepoftheNLPpipeline,andonabroadview,it
transformstexttovectorsusingaseriesoflinguisticprocessingmethods. Intheend,itprovidestheinputtotheutilised
vectorspacemodel. Althoughoftenthepreprocessingstepisoverlooked,itcanhaveamajorinfluenceontheaccuracy
ofthevectorspacemodelandthelearningmodel.
Dependingontheavailableinputcorpus,therearedifferentconsistentandefficientvariantsofchainingtheavailable
textprocessingmethods. However,alogicalchainingforthepreprocessingofthemedicaltextswouldbethefollowing:
i)Textcleaning; ii)Expansionoftheabbreviationsthroughabbreviationdisambiguationmethods; iii)Misspelling
correction;iv)Textrepresentation.
2.1.1 AbbreviationDisambiguationMethods
Abbreviation Disambiguation is the task of identifying the intended long-form for an ambiguous short-form text,
anditispartofthelargercategoryofWordSenseDisambiguation(WSD)methods. Inrecentyearsmoreandmore
abbreviations and acronyms have been introduced in the medical field, in order to facilitate the use of frequently
referencedmulti-wordterms. Withtheirincreaseinusage,severalissuescometothefore. Firstly,oneabbreviationor
acronymcanhavemultiplemeanings,eveninthesamedomain. Secondly,textunderstandingmaybeharder,asthe
abbreviations’definitionisnotalwaysfoundalongsidethem.
Asthereisalargesetofacronymsusedinthemedicalfield, themajorityofthedatasetscontainingacronymsare
focusedonthisfield,otherwise,thesetisquitesmallordoesnotcontainambiguousacronyms. Asaresultofthis,many
proposedsolutionsforexpandingabbreviationswereconsideredforthemedicalfieldoratleastusedmedicaltextual
dataintheirexperiments.
Consideringthemostpopularstudiesforthissubject,therearethreemajorapproaches: i)algorithmsbasedonavoting
mechanismusedintermrecognition;ii)unsupervisedmethodsforrecognitionofmulti-wordterms;iii)algorithms
basedonwordembedding.
Forthefirstapproach,thevotingsystemisusedinordertoselectthemostappropriatewordsensefromthemultitude
of outputs. This approach uses the weighted voting strategy based on rankings of a term produced by each term
recognitionalgorithm. ThisalgorithmcanbeimplementedaspartoftheAutomaticTermRecognition(ATR)orthe
AutomaticKeywordExtraction(AKE)preprocessingsteps. ATRisresponsiblefortheextractionoftechnicalterms
fromdomain-specificlanguagecorpora,whileAKEisthetaskofextractingthemostrelevantlinguisticunits. e.g.
words,inadocumentwiththepurposeofautomaticindexing[24].
Paper[45]presentsacomprehensivecomparisonofwellknownATRandAKEmethodsthatusesavotingsystem
forabbreviationdisambiguation. Theexperimentsconcludedthatthevotingsystemdidnotnecessarilyimprovethe
performanceofthealgorithmduetothecorpus’qualityandthespecificitytomulti-wordterms. Anotherobservation
isthatalgorithmsdesignedformulti-wordtermswillsufferfromlowprecisionwhencountingsingle-wordtermsor
vice-versa.
FlexiTerm[37]isanexampleofanunsupervisedmethodforrecognitionofmulti-wordtermsthatcanbeusedfor
acronymrecognition. FlexiTermperformstermrecognitioninthreesteps: i)Lexico-syntacticfiltering-thatisused
inordertoselectmulti-wordcandidates;ii)Normalisingthetermcandidates-inordertoneutralizetermvariation;
iii)Computingthetermhood. Termhoodmeasurestheassociationstrengthofatermtodomainconcepts. Likewise,
sinceanacronymcanbematchedtomultiplenormalizedtermrepresentatives,disambiguationneedstobeperformed.
Thisprocessisaccomplishedbycomparingpotentialnormalisedtermrepresentativeswithrespecttotheirfrequencyof
occurrenceandselectingthemostfrequentoneasthemostplausiblefullform. Inthecaseofatie,thecomparison
ismadeusingtheterms’lengthmeasuredbythenumberoftokens. Intheunlikelyeventthatanacronymremains
ambiguous,brute-forceisusedtoselectthefirstterminalphabeticalorder.
3E.S.ApostolandC.OTruica˘
AlimitationofFlexiTermisthatitfailstoclassifysomewordformationsthatcontainanacronym,e.g.,”exacerbation
ofchronicobstructivepulmonarydisease”and”exacerbationofCOPD”shouldbeconsideredequivalent,asCOPDis
notastandaloneterm.
ThemostefficientsolutionsforabbreviationdisambiguationuseWordEmbeddingtechniquescombinedwithback-off
methodsbasedonstringsimilarity,e.g. [22],[4],[40]. Awordembeddingisalearnedrepresentationoftextualdata
thatallowswordswithverysimilarconnotationstohaveasimilarrepresentation.
Paper[40]proposesasupervisedmachinelearning-basedwordsensedisambiguationmethodforclinicaldata. Their
solutionusedadatasetconsistingofunlabeledclinicalnotes,inordertotraintheneuralwordembeddings,andtwo
manuallyannotatedclinicaldatasetsfortesting. ThetrainingwasperformedusingaSupportVectorMachine(SVM)[5]
neuralnetwork. SVMisasuperviseddiscriminativebinaryclassifierthatdeterminesthebestseparationhyperplaneto
groupthelabeledpointsfromadataset. Thisclassifierachievesstate-of-the-artperformanceinmanysolutionsforword
sensedisambiguation. Thiscanalsobeobservedfortheresultspresentedinpaper[40],wheretheaverageaccuracy
exceeds0.92forallthedatasetsandregardlessofthechosensetoffeaturesforthemodel.
2.1.2 Misspellingcorrection
Inthissection,wefocusonaparticulartypeofmedicaldatasets,namelythedatathatcomefromtheobservations
andannotationsofphysiciansordifferentcategoriesofmedicalreports. Sincesuchtextualdocumentsarewrittenin
manycasesundertimepressure,thecollectedrawdatasetmaycontainmisspeltwordswhich,ifnotcorrected,willbe
consideredasnewtermsbytheNLPalgorithms.
Automated detection and correction of misspellings is a popular research subject in many domains. Likewise, in
recentyearsthistopichasgainedinterestinthemedicalfield,e.g. [20],[15],[7]. Thesimplesttechniquesinvolve
dictionarylookup,n-gramanalysisandisolated-worderrorcorrection,whereasthemostcomplexsolutionsarebased
oncontext-dependentmisspellingscorrection.
n-gramanalysisusesaprobabilisticlanguagemodelforpredictingthenextitemofann-gram. n-gramsareconsecutive
sequences of items from a text corpus. These items can be letters, syllables or words. For automatic recognition
ofmisspellingsbasedonn-gramanalysis,thebasicideaistheusageofletter-frequencystatisticstodetectunusual
sequencesofcharacters,whichcanbeindicatorsofpossibleerrors.
Systemsbasedonisolated-wordspellingcorrectionconsiderthatmostofthespellingerrorsconsistoftinymistakes,
suchastheinsertionordeletionofaletter,transposedorswitchedlettersortheuseofanotherletterinplaceofthecorrect
one. Therefore,theycalculatetheminimumeditdistancebetweenwordsasthenumberofneededtransformations
necessaryforamisspeltwordtoreachitscorrectvariant.
Thestate-of-the-artsolutionsforcontext-dependentmisspellingscorrectionareusingwordembeddingmodelstolearn
theassociationbetweenmisspellingsandtheircorrespondingcorrectwords. Suchsolutions,basedonwordembedding,
werealsodesignedforthemedicalfield,e.g.,[42]. Itisworthnotingthatthemajorityofcurrentwordembedding
models,e.g.,[31],donotexplicitlysupportmisspellingscorrection,andtheyrequireasinputdataasetofpre-trained
embeddings. OnelinguisticmodelthatprovidesmisspellingcorrectionfunctionalitiesisMOE(MisspellingOblivious
WordEmbeddings).
MOE(MisspellingObliviousWordEmbeddings)[30]isanNLPmodelforefficientlearningofwordrepresentations
andsentenceclassification. Thismodelisextendedwithasupervisedtaskthatembedsmisspellingsclosetotheir
correctvariants,calledthespellcorrectionloss. Practically,thespellcorrectionlossisatypicallogisticfunctionwhich
appliesascalarproductbetweenthesumofvectorswithsub-wordsobtainedfromthemisspeltanditscorresponding
correctword.
AttheoppositepoleregardingthesupervisedtaskofferedbyMOE,isthesolutionproposedinpaper[8]. Thesolution
usesanunsupervisedcontext-sensitivespellingcorrectionmethod. Thismethodgeneratescandidatesforthefound
misspellingsandusesneuralembeddingstorankthemaccordingtotheirsemanticfit. Themethodwascomparedwith
twoothertools,oneofthembeingHun-Spell,anopen-sourcespellcheckerusedbymanybrowsers. Theexperimental
resultsshowthattheproposedmethodgreatlyoutperformsthebaselinetools,althoughamoreinterestingcomparison
wouldbewithanotherembeddingsbasedsolution,suchasMOE.
2.2 EfficientVectorSpaceModelsforMedicalData
TheVectorSpacemodelisanalgebraicmodelthatrepresentstextualdataasvectorsofidentifiers. Thevectorspace
dimensionisitscardinality(i.e. numberofvectors)overthebasefield. Fortextualdata,thedimensionmayrepresent
thetermsfromadictionary. Let’sconsiderasimpleexample, i.e. forbuildingavectorspacemodel, "Avirusisa
4AdvancementsineHealthDataAnalyticsthroughNLPandDL
microorganism. Avirusinvadeslivingcells.". Afterremovingthestopwordslikea, is, etc., thetextkeywordsare
extracted,e.g. virus,microorganism,etc. Usingthesekeywords,onecanbuildasimplevectorspacemodelbased
on the calculated weight of each keyword, e.g., the vector has a magnitude of 2 in the "virus" direction. Such an
examplecanbeconsideredpartoftraditionalcount-basedtextualmodelsofwhichBagofWords[43]modelsbelongs.
Althoughsomesolutionsfromthisfamilyofmodelsareeffectiveinextractingfeaturesfromtextualdata(i.e.,especially
forsmalltextsofthesamedomain),informationsuchasstructure,semanticsandcontextarenotconsidered. More
sophisticatedmodelsprovidethesefeaturesasavectorrepresentationofwords,alsoknownasembeddings. Inthe
followingsubsections,wewillpresentnovelsolutionstobuildtheVectorSpaceModelusingWordEmbeddings.
2.2.1 TrainingtheWordEmbeddingModel
Thetrainingofawordembeddingsmodelisachievedbyfittingthemodeltothespecifictextualcorpusandbytuning
itshyper-parameters. Formodelfitting,itisnecessarytofindouthowmanyiterationsshouldbeappliedtoobtaina
sufficientperformantwordembeddingmodel,withoutoverfitting. Themodel’shyper-parametersareconfiguration
dependentvaluesthataresetupempiricallybeforetraining,andcanbeconsideredthefollowing: i)theinputoroutput
size;ii)thenumberofepochsforthemodel;iii)whetherornotitisdesiredtohaveanearlystopping(e.g.,bymonitoring
thelossonthedataset);iv)thenumberofartificialneuronsfromthenetwork. Trainingagoodwordembeddingmodel
ishighlydependentonthechosensetofhyper-parameters.
Thetrainingofthewordembeddingsnetworkcanbedonewithasubsetfromtheoriginalcorpus. However,there
aremanymedicaldatasets, publiclyavailable, thatcanbeusedfortrainingtheembeddingsnetwork, e.g., CHDS -
ChildHealthandDevelopmentStudiesdatasetsareintendedtoresearchhowdiseaseandhealthpassdownthrough
generation1;KentRidgeBiomedicalDatasets-High-dimensionaldatasetsinthebiomedicalfield2.
Although it is advisable to train your own word embeddings with datasets from the utilised domain, i.e., medical
field,thisstepishighlycompute-intensiveasitrequiresmanyprocessingresources. Andlikewise,itishighlytime-
consuming,e.g.,theaveragetimeforoneepochcanbearound4-5hours,ofcourse,itdependsalotonthechosen
networkembedding. Ofcourse,thereisthepossibilityofusingpre-trainedwordembeddings. Thepre-trainedword
embeddingsarethenloadedintotheembeddingnetworklayer. Suchpre-trainedembeddingsareofferedfordownload
ondifferentmodellingtoolsandmanylanguages. Forexample,Googlemadeavailablepre-trainedmodelsonover100
billionwordsfromGoogleNewscorpus3.
Adisadvantageofusingthesepre-trainedembeddingsisthattheydonotprovideveryaccurateresultsfordomain-
specifictasks,astheyaretrainedongenericdatasets. Toovercomethislimitation,severalsolutions,thataddterm
specificinformationtopre-trainedwordembeddings,wereproposed. Inpaper[28]isproposedsuchamethodthat
addsinformationfrommedicalcodingdatasetstodifferentpre-trainedwordembeddings. Throughrigoroustesting,the
authorsshowedthatthemodifiedwordembeddingsgiveanimprovementinf-scoreby1%ontheirprivatemedical
dataset.
2.2.2 Modelling. FeatureExtraction
As mentioned in the previous subsection, word embeddings networks provide representations of words in a dense
vectorspace,whilealsoconsideringcontextualandsemanticinformation. Consequently,wordembeddingmodelsare
consideredstateoftheartintextualfeatureextraction. Following,wewillpresentsomeofthemostpopularmodelsand
toolsforfeatureextractionusingwordembeddings.
Word2VecEmbedding
Word2Vec[26]isatwo-layerneuralnetworkdevelopedbyagroupofresearchersfromGooglethatbuildsavectorized
representationforthewordsfromaverylargecorpus,basedonthecontextsinwhichtheyappearinthatcorpus. The
algorithmreturnsanumericvectorialrepresentationforeachword,thisrepresentationhavingthepropertythatwords
withsimilarcontextareclosertoeachotherinthemulti-dimensionalvectorspacethatiscreated. Startingfromthis
property,onecouldcomputethesemanticsimilarityoftwoconceptsbyevaluatingthecosinedistancebetweenthe
vectorialrepresentationoftheseconcepts.
Word2Vecutilizestwotypesofmodelarchitectures: CBOW(ContinuousBag-Of-Words)modelandSkip-grammodel.
TheCBOWmodelaccountsforthetextualdatasetvocabulary,representingdocumentsasasetofpairsofcontinuous
word-multiplicity. IntheSkip-grammodel,documentsarerepresentedassequencesofwordswithgapsbetweenthem.
1http://www.chdstudies.org/research/information_for_researchers.php
2http://leo.ugr.es/elvira/DBCRepository/
3https://code.google.com/archive/p/word2vec
5E.S.ApostolandC.OTruica˘
Theinputtotheneuralnetworkisthetargetwordandtheoutputlayerisreplicatedmultipletimestoaccommodatethe
chosennumberofcontextwords. Thus,thismodelmanagestoembedinthewordrepresentationitslinguisticcontext.
Thetwomodelsmirroreachotherwhilebothpreservecontext. TheCBOWmodelusesmultipleneighbouringwords
topreservethecontextforatargetword,whiletheSkip-grammodelusesawordtopreservethecontextformultiple
targetedneighbouringwords.
Word2VecisstilloneofthemostusednetworkembeddingsineHealthdataanalytics,duetoitssimplicityinuseandthe
factthatitisveryintuitive. Also,animportantfactoristhatitrequireslittlememory,asdataneedlightpre-processing
andcanbedirectlystreamedintothemodel. Forexample,inpaper[12]ispresentedanefficientsolutionforidentifying
relationsinclinicalrecords. Thistaskisnottrivial,asclinicalrecordsmaycontainmorethantwomedicalconcepts,
mostofthetimecomprisedofseveralwords. TheirsolutionusesWord2Vecforbuildingtheembeddingsthatarethen
fedtoaproposedDeepNeuralnetwork.
FastTextEmbedding
FastText[18]isbuiltupontheWord2Vecmodelandimprovesitbyprovidinghigh-qualityembeddingsbyemploying
methodsthatarealsotime-efficient. Sinceitsrelease,FastTexthasconstantlyincreasedinpopularity,recentlyalso
beingusedformedicalcorpora.
Inpaper[35]theauthorsutilisetheFastTextmodeltodevelophigh-qualitySpanishembeddingsforthebiomedical
domain. Theseembeddingsweremadegenerallyavailablebytheauthors. Thetrainingoftheirin-domainembeddings
wasdoneusingtwocorpora: i)alargecorpus(i.e.,SciELOdatabase,whichcontainsfull-textarticlesprimarilyin
English,SpanishandPortuguese);ii)asmallerandfocusedcorpus(i.e.,WikipediaHealth,consistingoffourmain
categories: Pharmacology,Pharmacy,MedicineandBiology). Duringhyper-parameterstuningtheysetthesizeofthe
wordvectorto300andthenumberofepochsto20. Themodelwasthenusedtoidentifypharmacologicalsubstances,
compoundsandproteinsinclinicaltexts.
TheFastTextskipgrammodelisalsousedinpaper[8],presentedinSection2.1.2thatdealswithmisspellings. Inthis
case,FastTextisusedforcreatingvectorrepresentationsofthecandidatesforthecorrectvariantsofthemisspellings
thatareabsentfromthetrainedembeddingspace.
BERTEmbedding
BidirectionalEncoderRepresentationsfromTransformers(BERT)[6]isawordembeddingmodelbasedonamultilayer
bi-directionaldeepnetwork. Itusesatransformerdeepneuralnetworkwithupto24layers,asopposedtotheusual
two-layernetworkusedinothersolutions,i.e.,Word2Vec,FastText. Transformersnetworksuseattentionmechanisms
thatcollectinformationabouttherelevantcontextofagivenlinguisticterm,andthenencodethatcontextinthevector
representationforthatterm.
AlimitationoftheBERTmodelistherelativelysmallsizeofthetextualinputblock. Becauseofthis,arelativelylong
clinicalrecordwillbesplitintomultipleparts,andextrinsictasks,asprediction,aredoneseparatelyforeachpart.
Oncetheprediction,oranotherextrinsictask,isappliedforallthetextualparts,thefinalprobabilityresultsfromtheir
aggregation.
ThereareseveralvariantsofBERT,trainedandfine-tunedfordomain-specifictext,thisalsobeingthecaseforthe
medicalfield,i.e.,BioBERTandClinicalBERT.
BioBERT[21]utilisesalmostthesamearchitectureasBERT,themainalterationsbeingmadeinthepre-trainingand
fine-tuningsteps. BioBERTisadditionallypre-trainedwithtwolargebiomedicalcorpora,namelythePubMedabstracts
andthePMCfull-textarticles. Likewise,thismodelisfine-tunedforthreefundamentalbiomedicalspecifictasks: i)
Namedentityrecognition(NER)whichdealswithrecognizingthedomain-specifictermsinthemedicalcorpora;ii)
Relationextraction(RE)whichprovidesclassifyingrelationsofmedicalnamedentities;andiii)Questionanswering
(QA) which deals with answering NLP questions, given related text. For each of these specific tasks, BioBERT
outperformsthecurrentstateoftheartembeddingmodels,includingtheoriginalBERTmodel.
ClinicalBERT [14] was developed concurrently with BioBERT. This solution is built on top of the BERT neural
network. AsBioBERT,itispre-trainedwithtextualdatafromthemedicaldomain,althoughtheClinicalBERTcorpusis
composedoflargesetsofclinicalnotes,andnotofbiomedicalarticles. ClinicalBERTisfine-tunedforminingclinical
notesforreadmissionprediction. AcomprehensivecomparisonofthecurrentclinicalBERTembedding,including
BioBERTandClinicalBERT,iscarriedoutinarticle[1].
6AdvancementsineHealthDataAnalyticsthroughNLPandDL
Sense-DisambiguationEmbedding
sense2vec[38]isanewembeddingsmodelspecificallydesignedtodisambiguatebetweenwordsenses. Bassically,
sense2vecprovidesmultipleembeddingsforawordbasedonthesenseofthatword. Asopposedtotraditionalword
embeddings,itanalysisthecontextofawordandtriestoassignmoreadequatevectorforthatword.
Inpaper[33]isusedacombinationofbothwordembeddingsandsense-disambiguationembeddings. Thefeaturesof
bothembeddingmodelsareconcatenatedandfedtoalearningmodel,resultinginaccuratedetectionofdiscontinuous
entities,aswellasofoverlappingornestedentities. ThelearningmodelisusedforknowledgerecognitionineHealth
documents.Theproposedsolutionisperformant,evenwithouthandcraftedfeaturesorspecificdomainknowledge.
2.2.3 ModelEvaluation
Wordembeddingsareevaluatedusingextrinsicmethodsthatrequireaprioriknowledgeaboutthetaskthatissolved,e.g.,
inNaturalLanguageProcessingtheembeddingsareevaluatedonspecifictaskswherethecorrectoutcomeisalready
knownsuchaspart-of-speechtaggingandnameentityrecognition. Whenworkingwithmedicaldata,wordembedding
evaluationmustalsobedoneusingintrinsicmethods,as,insomecases,thereisnopredefinedtargetoutcome.
Inthemedicalfield,thereareseveralstandarddatasetsusedinintrinsicevaluation,suchassemanticsimilarityand
relatedness,i.e.,theUMLSsimilarityandrelatednessdatasets. UMLS(UnifiedMedicalLanguageSystem)4consistsof
manuallyannotatedtermsforsimilarityandrelatedness.
However,thestateoftheartintrinsicevaluationforwordembeddingsdoesnotrequireaprioriknowledge,suchasthe
UMLSdatasets. Itfocusesonstudyingthevariationbetweentwomodelsofthetargetword’sneighbors[29]. The
nearestneighborsofatargetwordisdeterminedusingcosinesimilarity. Thevariationbetweentwomodelsiscomputed
usingthedegreeofnearestneighborvariationforeachmodel. Thestepsforcomputingthevarianceforthenearest
neighborsfortwowordsembeddingmodelsM andM are:
1 2
• extractthecommonvocabularyV =V ∪V ,whereV andV arethevocabularyforM ,respectivelyM
1 2 1 2 1 2
• determinetheknearestneighbors(knn)ofeachtermwforbothmodels,i.e.,knn (w)andknn (w)
M1 M2
• computethevariationvar (Equaton(1))forthecommonneighborsamongtheknearestneighbors.
M1,M2
||knn (w)∩knn (w)||
vark (w)=1− M1 M2 (1)
M1,M2 k
Thevariationisascoreintheinterval[0,1]. Valuesclosertotheupperbounddenotethatthetwomodelsaredrastically
different.
The parameter k must be carefully chosen as it impacts the selection of the nearest neighbor of a term. Setting a
smallvalueforkcanresultinomittingrelevantneighborswhilesettingalargevaluecanresultinextractingloosely
relevantneighbors. Experimentalvalidationprovedthatthebestvaluefork is25, butwerecommendperforming
hyper-parametertuningfordeterminingthebestkwhentrainingoncustomdatasets,i.e.,medicalrecords.
Althoughtheintrinsicevaluationismoreimportantinthecaseofmanymedicalcorpora,themajorityofsolutions
formedicaldataprovidesbothintrinsicandextrinsicevaluations. Forexample,consideringtheexamplediscussedin
subsection2.2.2thatproposedhigh-qualitySpanishembeddingsforthebiomedicaldomain [35],theauthorsdetailed
boththeintrinsicandextrinsicevaluation. Fortheintrinsicevaluation,theauthorsusedatraditionalapproachthat
requiresaprioriknowledge,butunfortunately,therearenoannotatedSpanishdatasetsforthemedicaldomain. Hence,
theyadaptedasimilaritydatasetfromEnglishtoSpanish. Webelievethatamoreefficientsolutionwouldhavebeenthe
useofthevariation-basedmodel[29]. Asfortheextrinsicevaluationoftheproposedembeddingsnetwork,theauthors
deployedanartificialneuralnetworkwiththepurposeofidentifyingdifferentsubstances,compoundsandproteinsfrom
anopen-accessmedicaldataset. Finally,theauthorsofferacomparisonoftheextrinsicevaluationbetweenageneral
domainembeddingsandtheoneproposedbythem. Theresultsshowthattheirsolutionisabitbetterthanthegeneric
one,consideringalltheperformancemetrics,i.e.,Accuracy,Precision,RecallandF score(fortheirformulassee
1
subsection2.3.3).
4https://www.nlm.nih.gov/research/umls/index.html
7E.S.ApostolandC.OTruica˘
2.3 AnEvaluationofDeepLearningArchitecturesintheMedicalField
ThelastelementoftheNLPPipelineistheDeploymentphase. Inthefollowingsubsections,wedetailthestateofthe
artmethodsandmodelscorrespondingtothisphase,highlightinghowtheyareappliedforprocessingmedicaltextual
data.
2.3.1 Deep-LearningClassifiers
Thelargecollecteddatasetsofunstructuredmedicaldataplayanimportantroleinthediagnosisprocessandprovidea
comprehensiveviewofthetreatmentandpatienthealthstatus,butatthesametime,itmakestheprocessofanalyzing
datamoredifficult.
Inthiscontextofthemassiveever-growingofeHealthdata,thetraditionalMachineLearningmethodsprovetobein
somedegreeinefficientandinaccurate. DeepLearningalgorithmsevolvedfromtraditionalMachineLearningneural
networks,however,theyareusedtobuildmorecomplexandlargeneuralnetworks. Likewise,thestateoftheartNatural
LanguageProcessingsolutionsappliedonlargescaleeHealthdataarepoweredbysuchDeepLearningalgorithms.
Some of the most popular deep neuro-network architectures Learning algorithms used in eHealth solutions are: i)
ConvolutionalNeuralNetwork(CNN)[12][2][3],ii)RecurrentNeuralNetworks(RNN)[13][41],ii)LongShort-Term
MemoryNetworks(LSTMs)[32]Inthefollowingsubsections,weprovideshortdescriptionsoftheseneuro-network
architecturesalongwithdiscussionsabouthowtheyareappliedinthemedicalfield.
ConvolutionalNeuralNetworks
Convolutionalneuralnetworks(CNNs)areaclassofneuralnetworksthataremainlyusedinvisualimagery. Itcantake
aninputimageandassignimportancetoaspectsorobjectsintheimagesoastodifferentiateonefromanother. Oneof
thedifferencesbetweenregularneuralnetworksandCNNsisthatCNNexplicitlyassumesthattheinputisanimage,
whichmakesthepreprocessingrequiredmuchlowerthanforotherclassificationalgorithms[19].
CNNs are inspired by the connectivity pattern of neurons in the visual cortex. A neuron responds to stimuli in a
restrictedregionofthevisualfield. Thefieldsoverlaptocovertheentirevisualarea.
ACNNconsistsofaninputlayerandanoutputlayer,aswellasmultiplehiddenlayers: convolutionallayer,pooling
layer,andfullyconnectedlayer. Thefullyconnectedlayerisexactlyasthehiddenlayerinregularneuralnetworks.
TheconvolutionlayeroftheneuralnetworkisthecorebuildingblockinCNNsthatdoesmostofthecomputation. A
setoflearnablefiltersformstheparametersforthislayer. Eachfiltercanbeseenasawindowslidingacrosstheinput,
andtheonlypartoftheinputthatisanalyzedatacertaintimeistheoneinsidethewindow. Theresultoftheanalysisis,
actually,thedotproductbetweentheentriesinthefilterandtheinputatthecurrentpositionofthewindow.
ThefunctionofapoolinglayerinaCNNistoprogressivelyreducethespatialsizeoftherepresentationtolowerthe
numberofparametersandcomputationinthenetwork,contributingalsotocontrollingoverfitting. Thatiswhyitis
commontoinsertapoolinglayerbetweenconsecutiveconvolutionlayers. Thepoolinglayeroperatesindependentlyon
everydepthsliceoftheinputandresizesitspatially,usingtheMAXoperation.
ThepoolinglayeracceptsavolumeofsizeW1xH1xD1,whereW1,H1,andD1arethewidth,height,anddepth
oftheinputinthepoolinglayer. ItalsorequirestwohyperparametersF,thespatialextent,andS,thestride,toproduce
avolumeofsizeW2xH2xD2,where: W
2
= W1 S−F +1,H
2
= H1 S−F +1,D
2
=D 1.
Animportantnoteisthat,inpractice,itiscommontohaveF =2andS =2,orF =3andS =2,whichresultsin
overlappingpooling.
Ingeneral,CNNisapplicabletodatasetscontainingimages,althoughitcanalsobeusedfortextclassification,inwhich
casethedepthoftheconvolutionallayeris1or2.
Inrecentbiomedicalpapers,CNNwasquiteoftenusedincombinationwithotherdeepnetworksforresolvingdifferent
medicalproblems,e.g.,detectingabnormalhumanbehaviour,questionanswering,andknowledgebasecompletion.
Inpaper[2],aCNN-baseddeeparchitectureisappliedonmedicaldatasetswiththemainpurposeofdetectinganomalies
relatedtodementia,andindirectlytorecognisedailylifeactivitiesforthemonitoredpatients. Threedeepnetworksare
deployedandtested. Themostcomplexand,atthesametime,themostperformantDLnetworkisacombinationof
two-layerCNNwithanLSTMlayer(describedlaterinthissection),followedbytwodenselayers. Inadenselayer
network,eachneuronisconnectedwithalltheneuronsfromthepreviouslayer. Theauthorscomparedtheperformance
oftheirsolutionwithotherDeep-Learningmethods,i.e.,LSTMonly,andwithmoretraditionalMachine-Learning
techniques,i.e.,HiddenMarkovModels(HMMs),ConditionalRandomFields(CRF).Theirsolutionoutperformsthe
othermethodsintermsofPrecisionandAccuracywithmodethan0.04,andmorethan0.05respectively.
8AdvancementsineHealthDataAnalyticsthroughNLPandDL
Article[12]proposesaCNNarchitecturewithamulti-poolingoperationformedicalrelationclassificationonclinical
records. Therelationclassificationproblemcanbeusedformedicalapplicationssuchasaquestionansweringsystem.
Theauthorsclassifythemedicalconceptsrelationsintopositiveones,e.g.,"Testrevealsmedicalproblem","Treatment
worsens medical problem", and negative ones, e.g., "No relation between treatment and problem". The proposed
architectureusesnoexternalfeatures,suchassemanticrolelabeller,sentimentcategory,manuallylabelledpatterns.The
intervalfeaturesareextractedviamulti-poolingoperation. Thepapercontainsattheendacategory-wiseperformance
comparisonwithotherneuralnetworkmodels, i.e., CNN-basedmodelsusingmax-pooling, SVM(SupportVector
Machine).
RecurrentNeuralNetworks
RecurrentNeuralNetworks(RNNs)areaclassofneuralnetworksthatsolvetheproblemofpersistingtheinformation
aboutpreviouseventsbyhavingloopsinsidethem. Thismakesthemcapableofprocessingtimesequencesandlearning
sequences,whichprovidedgoodresultsinspeechtotextcomprehension,machinetranslation,andlanguagemodeling.
DuetotheloopusedinRNNs,theinformationispassedinthenetworkfromsteptostep,whichcanbethoughtof,
actually,asduplicatesofthesamenetwork,eachonepassingamessagetoasuccessor.
SomeexamplesofRNNsareasfollows:
1. onetoone–thistypeisactuallyequivalenttoaregularneuralnetworkwithfixed-sizedinputandfixed-sized
output
2. onetomany–fixedsizedinputandsequenceoutput,used,forexample,inimagecaptioning
3. manytoone–sequenceinputandfixedsizedoutput,usedinsentimentanalysiswhereasentenceisclassified
asexpressingacertainsentiment
4. manytomany–sequenceinputandsequenceoutput,usedinmachinetranslation
5. syncedmanytomany–syncedsequenceinputandsequenceoutput,usedinvideoclassificationwhereeach
frameneedstobelabeled
Inthemedicalfield,RNN-basedarchitectureswereusedtoprovideperformantsolutionsformedicalapplicationtasks
thatconsidersequenceandtimefeatures. Anexampleofsuchspecifictasksishumanactivityrecognition. Thistask
dealswithpredictingwhatamonitoredpatientisdoingbasedonamovementtrace,anditcanbeusedinriskprevention
andinterventionmedicalapplicationsforimprovingthehealthcarequality.
OneofthemajorlimitationsoftheRNNnetworksisthatthegradientofitslossfunctiondecayswithtime,almost
exponentially. ThisimpliesthatRNNnetworksarenotperformantinresolvingproblemsthatnecessitatelearning
long-termtemporaldependencies.
ManyofthecurrentsolutionsforanalysingunstructuredmedicaldataarenolongerusingthetraditionalRNNs,rather
theirarchitecturecombinesRNNswithlayersofLSTM.LSTMwasspeciallydesignedtoovercometheRNNsgradient
decaylimitation. Inthefollowingsubsection,wewillprovideabriefpresentationoftheLSTMdeepnetworks.
LongShort-TermMemoryNetworks
BeinganextensionoftheRNN,theLSTMmodelimplementationissimilartoit,however,thelattermodelmakesit
easiertorememberpastdatainthenetworkmemory. LSTMusesback-propagationinthetrainingprocess,anditis
intendedtobeusedfordataconsistingoflongtimesequences.
AsinthecaseofRNNs,thegradientoflong-termcomponentsgrowsexponentiallyfasterthantheoneforshort-term
temporalcomponents. ThesolutionofferedbyLSTMarchitectures,alsoknownasclippingthegradient,isbasedon
thresholdingthegradients’valuesbeforeexecutingagradientdescentstep.
Forconsistencywiththerecurrentnetwork,theLSTMhiddenlayersizeisalsosetto256. Arecurrentlayerappliesa
multi-layerlongshort-termmemoryRNNtoaninputsequence. Thismeansthatforeachelementintheinputsequence,
eachlayercomputesasshowninEquation2.
9E.S.ApostolandC.OTruica˘
i =σ(W x +b +W h +b )
t ii t ii hi t−1 hi
f =σ(W x +b +W h +b )
t if t if hf t−1 hf
g =tanh(W x +b +W h +b )
t ig t ig hg t−1 hg
(2)
o =σ(W x +b +W h +b )
t io t io ho t−1 ho
c =f ∗c +i ∗g
t t t−1 t t
h =o ∗tanh(c )
t t t
whereh isthehiddenstateattimet,c isthecellstateattimet,x istheinputattimet,h isthehiddenstateofthe
t t t t−1
layerattimet−1ortheinitialhiddenstateattime0,andi ,f ,g ,o aretheinput,forget,cellandoutputgates. σis
t t t t
thesigmoidfunctionand∗istheHadamardproduct.
In a multilayer LSTM, the input x(l) of the l-th layer, with l ≥ 2, is the hidden state h(l−1) of the previous layer
t t
multipliedbydropoutδ(l−1),whereeachδ(l−1)isaBernoullirandomvariablewhichis0withprobabilitydropout,a
t t
givenhyperparameter.
TherearemanyLSTMarchitecturescommonlyusedintextualprocessing,besidesthetraditionalUnidirectionalLSTM,
e.g.,i)BidirectionalLSTM(BiLSTM);ii)StackLSTM;iii)Attention-basedLSTM.TheBidirectionalLSTMconsists
ofLSTMsthathavetheirhiddenstateoutputmergedatthesametime. Suchanarchitectureenablestheneuralnetwork
tocontrol,ateachtimestep,bothforwardandbackwardinformation. AStackLSTMarchitectureextendsontwo
dimensions,bothontheinputhorizontaldirection(suchasBiLSTM),butalsoonthedepthdirection,formingagrid
likestructure. TherearealsootherLSTMarchitectures,recentlyproposed,suchasFully-connectedLSTM[17].
Themajorityofthecurrentstateoftheartsolutionsforanalysingtextualmedicaldata,useLSTMclassifierseitherto
traintheirembeddings,orforspecificextrinsictasks,orboth. Although,thegreatestusageoftheseneuralnetworksis
intasksrelatedtoconceptextraction. Inthefollowingparagraphs,wepresentseveralcurrentmedicalsolutionsthat
haveembeddedintheirDeep-LearningarchitectureLSTMlayers.
The architecture proposed in article [33] has two Bidirectional Long Short-Term Memory (BiLSTM) layers and a
layerbasedonConditionalRandomField(CRF).ItwasdesignedforthetaskofNamedEntityRecognition(NER)in
biomedicaltexts. Nowadays,thistaskisofgreatimportanceinthemedicalfield,asitisveryimportanttoidentify
quality information that refers to specific entities of interest in large amounts of documents. In this solution, the
BiLSTMnetworkisusedforobtainingthewordandsense-disambiguationembeddings. Thenetworkreturnsforeach
linguistictermssixprobabilitiescorrespondingforeachlabelfromtheappliedencodingformat. TheCRFlayertakes
asinputtheoutputofthelastBiLSTMlayerandobtainsthemostprobablesequenceofpredictedlabels,resultinginan
improvementofthepredictions’accuracy.
Inarticle[17],anewdeeplearningarchitectureisproposedforefficientlyextractingmedicalconcepts,i.e. symptoms,
laboratorytests,andtreatments,fromlargeunstructuredcorpora. ThearchitectureconsistsofFully-ConnectedLSTM,
proposed in this article, and Conditional Random Fields(CRF) layers. The Fully-Connected LSTM (FC-LSTM)
neuralnetworkissimilarinstructurewithastackLSTM,withthedifferencethattheconnectionbetweentheFC-
LSTMlayersistighter. Furthermore,eachFC-LSTMlayeractsasaBiLSTM,asthetrainingusesbothforwardand
backwardinformation. TheauthorscomparedFC-LSTM,onmedicalconceptextractiontasks,bothwithothertypesof
classificationmethods(e.g.,differentCRFmodels)andwithotherclassesofLSTM(e.g.,StackLSTM,Attention-based
LSTM). In all the experiments, the FC-LSTM achieved the highest performance metrics, i.e., F -score, Precision.
1
ThismaybeduetothefactthatFC-LSTMhasshortconnectionpathsandcanobtaininformationonmultiplewords
concurrently.
BoltzmannMachines
ABoltzmannMachineisastochasticrecurrentneuralnetworkthathastwotypesofsymmetricallyfully-connected
nodes: inputorvisiblenodesandhiddennodes. Theconnectionsbetweennodesareweightedandcanberepresented
asanundirectedgraph. Unlikeotherartificialneuralnetworks,ABoltzmannMachinedoesnothaveanoutputlayer
whichgivesthemthenon-deterministicfeature. Instead,eachnodegeneratesstatesaspartoftheentiresystemusingan
energy-basedmodelandhelpsmakestochasticdecisionsbybeingturnedonoroff. Foralearningproblem,aBoltzmann
Machineusesasimplelearningalgorithmthatdiscoversinterestingfeaturesindatasetscomposedofbinaryvectors.
Unfortunately,thelearningprocessofaBoltzmannMachineisveryslowfornetworkarchitectureswithmanylayersof
featuredetectorsastherearemanysmallupdatesontheweights. Tosolvethecomplexityproblemandmakeefficient
weightupdates,restrictionscanbeplacedontheintralayerconnectionsforbothtypesofnodes,thusresultinginthe
RestrictedBoltzmannMachinenetworks.
10AdvancementsineHealthDataAnalyticsthroughNLPandDL
ARestrictedBoltzmannMachineisprobabilistic,unsupervised,generativedeeplearningnetworks. AsBoltzmann
Machine,theinputandhiddenlayerarestillpresentandthetwolayersarefully-connected. Thedifferencebetweenthe
twonetworksisintheintralayerconnections,i.e.,theconnectionsbetweentheinputnodesandtheonesbetweenthe
hiddennodes,thatareremovedfortheRestrictedBoltzmannMachinearchitectures. InthefieldofNaturalLanguage
Processingofmedicaldata,RestrictedBoltzmannMachineshavebeensuccessfullyusedforpredictingpatientdiagnoses
fromElectronicHealthRecords[44]
ADeepBoltzmannMachinearchitectureisanunsupervised,probabilistic,generativemodelwithentirelyundirected
connectionsbetweendifferentlayerscontainingsymmetricallyconnectedstochasticbinarynodes. Itcanbeobtainedby
stackingmultiplehiddenlayers. AsinthecaseofRestrictedBoltzmannMachines,thehiddenlayersandthevisible
layerdonotcontainanyintralayerconnections. OnlyneighboringlayersareconnectedinaDeepBoltzmannMachine
architecture. Thearchitecturecanbefine-tunedusingbackpropagation. DeepBoltzmannMachinenetworkshavebeen
usedtogetherwithontologiestolearnsemanticrepresentationsofwords[39]. Thisapproachcanbeusedinthemedical
domainastherepresentationscorrespondtoconceptsatvarioussemanticlevelsinadomainontology.
ADeepBeliefNetworkcontainsmultiplestacksofRestrictedBoltzmannMachines. LikeinDeepBoltzmannMachine
andRestrictedBoltzmannMachines,therearenointralayerconnectionsbetweenthenodes. Allthelayersarefully-
connectedtotheirneighboringlayers. ThetoptwolayersofaDeepBeliefNetworkcontainsymmetricundirected
connectionsbetweenthenodesofneighboringlayers,similartoaDeepBoltzmannMachine. Theselayersformthe
associativememoryofthearchitecture. UnlikeDeepBoltzmannMachine,thelowerlayerscontaindirectedacyclic
connectionsbetweenthenodesofneighboringlayers. Theselayersareusedtoconvertassociativememorytoobserved
variables. Theinputlayercanacceptdatathatiseitherbinaryorreal.
Deepbeliefnetworkarchitecturesareusedfornamedentityrecognitionfromelectronicmedicalrecords[23]. Another
applicationofDeepbeliefnetworkarchitecturesisextractingandfusingfeaturesofstructureddataandunstructured
textmedicaldatatodealwiththehighlynonlinearrelationshipbetweenmultimodaldata[11].
2.3.2 Cross-validation
Cross-validation is a statistical method used for estimating how well a machine learning model generalizes on an
independentdataset. ThebasicmethodistheHoldoutmethodthatrequiressplittingthedatasetintothetrainingand
testingset. Thetrainingsetcontainstwo-thirdsofthedataset,whilethetestingcontainstheremainingthird. Usingthis
configuration,themodelisbuildusingthetrainingsetandthenvalidatedusingthetestingset.
AgeneralizedversionoftheHoldoutmethodisthek-foldcross-validation. Usingthismethod,thedatasetelementsare
shuffledrandomlyandsplitintokdisjointsubsetsofthesamesize. Foreachsubset,amodelisbuildusingk−1subsets
asthetrainingsetandtheremainingsubsetasthetestingset. Foreachkiterationthemodelisevaluatedand,afterthe
lastiteration,anaverageevaluationscoreiscomputed. Onceassignedtoasubset,adatapointwillnotchangesubsets
forthedurationoftheentireprocedure. Inthecasethedatasetisimbalanced,thedistributionofclassesamongthedata
pointmustbepreservedtobuildageneralizedmachinelearningmodel. Tothismeans,thestratifiedcross-validation
methodisusedtosplitthedatasetintodisjointsubsetsthatmaintainclassdistribution.
2.3.3 ClassifierEvaluation
Machinelearningclassificationisgroupedintofourcategories[36]: i)binary,ii)multi-class,iii)multi-labeled,andiv)
hierarchical. Dependingonthetypeofclassification,weneedtodefinedifferentmeasurestoaccuratelyevaluatea
model.
Binaryclassificationassignsdatapointtooneoftwonon-overlappingclasses,i.e.,C andC ,usuallypositiveand
1 2
negative. Toevaluatethemodelwewillneedtobuildtheconfusionmatrixwherewewilldefinethefollowingtypes:
• TP (TruePositive)isthenumberofobservationsthatbelongtotheC classandareclassifiedcorrectly;
1
• FN (FalseNegative)isthenumberofobservationsthatbelongtotheC classandareclassifiedasC ;
1 2
• FP (FalsePositive)isthenumberofobservationsthatbelongtotheC classandareclassifiedasC ;
2 1
• TN (TrueNegative)isthenumberofobservationsthatbelongtotheC classandareclassifiedcorrectly.
2
Usingtheconfusionmatrix,forbinaryclassificationwecandefinethefollowingevaluationmetrics:
• Accuracy(A= TP+TN )tomeasuretheoveralleffectivenessofaclassifier;
TP+FP+TN+FN
• ErrorRate(E =1−A)tomeasuretheproportionofincorrectlyclassifiedobservations;
11E.S.ApostolandC.OTruica˘
• Precision(P = TP )tomeasuretheclassagreementofthedatalabelswithinthepositivelabelsgivenby
TP+FP
aclassifier;
• Recallorsensitivity(R= TP )tomeasuretheeffectivenessofaclassifiertoidentifypositivelabels;
TP+FN
• Specificity(S = TN )tomeasuretheeffectivenessofaclassifiertoidentifynegativelabels.
TN+FP
• F-Score(F =(β2+1)· P·R )tomeasuretherelationsbetweenpositiveclassesinthedatasetandthose
β β2·P+R
givenbyaclassifier.
Multi-class classification assigns a data point to one and only one class C , with i = 1,l. The l classes are not
i
overlapping. Asinbinaryclassification,anextendedconfusionmatrixthatholdsvaluesforeachclassC isdefined,i.e.,
i
willcontainaseparateTP ,FN ,FP ,andTN foreachclass. Thequalityofthemodelwillbeassessedbythesame
i i i i
evaluationmethods,i.e.,Precision(Equation(3)),Recall(Equation(4)),andF-Score(Equation(5)),butintwoways: i)
macro-averaging(M)tomeasuretheaverageper-classagreementofaclasswiththosedetectedbytheclassifier,andii)
micro-averaging(µ)tomeasuretheper-classagreementofaclasswiththosedetectedbytheclassifier.
(cid:80)
P = 1 (cid:88) l TP i and P = i=1lTP i (3)
M n TP +FP µ (cid:80) l(TP +FP )
i=1 i i i=1 i i
(cid:80)
R = 1 (cid:88) l TP i and R = i=1lTP i (4)
M n TP +FN µ (cid:80) l(TP +FN )
i=1 i i i=1 i i
P ·R P ·R
F =(β2+1)· M M and F =(β2+1)· µ µ (5)
M β2·P +R µ β2·P +R
M M µ µ
Multi-labeled classification assigns a data point to one and or more classes C , with i = 1,l. As in the case of
i
multi-classclassification,theclassesarenotoverlapping. Anobservationx belongstoclassC ifL [i]=1,where
j i j
L ={L [i]|i=1,l}isasetoflabelsfortheobservationwithj =1,nandnthesizeofthedataset. Thequalityof
j j
themodelisassessedthrougheitherpartialorcompleteclasslabelmatchingusinganindicatorfunctionI(Ld =Lc),
j j
whereLdrepresentsthelabelsinthedatasetforanobservationx ,andLc arethelabelspredictedbytheclassifierfor
j j j
thesameobservation. TheExactMatchRatio(Equation(6))measurestheaverageper-labelexactclassification. The
averageper-labelclassificationwithpartialmatchescanbemeasuredusingtheLabellingF-Score(Equation(7)),while
theaverageper-classclassificationwithpartialmatchescanbecomputedusingtheRetrievalF-Score(Equation(8)).
(cid:80)n I(Ld =Lc)
EMR= j=1 j j (6)
n
(cid:80)n 2(cid:80)l i=1(Ld j·Lc j)
j=1 (cid:80)l (Ld+Lc)
F = i=1 j j (7)
n n
(cid:80)l 2(cid:80)n j=1(Ld j·Lc j)
i=1 (cid:80)n (Ld+Lc)
F = j=1 j j (8)
l l
HierarchicalclassificationassignsadatapointtooneandonlyoneclassC ,withi=1,l. Eachclasscanbedivided
i
intosub-classes(C )orgroupedintosuper-classesC . Thehierarchyispre-definedandstable,i.e.,doesnotsuffer
↓ ↑
changesduringclassification. Thequalitymeasuresevaluatedescendant(↓)orancestor(↑)performancebycomputing
Precision(Equation(9)),Recall(Equation(10)),andF-Score(Equation(11))onthecorrectdatalabels,i.e.,Cdand
↓
Cd,andthelabelsassignedbytheclassifier,i.e.,CcandCc.
↑ ↓ ↑
|Cd∩Cc| |Cd∩Cc|
P = ↓ ↓ and P = ↑ ↑ (9)
↓ |Cc| ↑ |Cc|
↓ ↑
|Cd∩Cc| |Cd∩Cc|
R = ↓ ↓ and R = ↑ ↑ (10)
↓ |Cd| ↑ |Cd|
↓ ↑
12AdvancementsineHealthDataAnalyticsthroughNLPandDL
P ·R P ·R
F =(β2+1)· ↓ ↓ and F =(β2+1)· ↑ ↑ (11)
↓ β2·P +R ↑ β2·P +R
↓ ↓ ↑ ↑
2.3.4 Feedback
Amodelcanincorrectlyclassifyobservationinadataset. Theevaluationresultsshouldbeinterpretedbyanexpertin
thefield. Inthecontextofthemedicaldomain,itisrequiredthattheexperthasinsideknowledgeanddetailsabout
theobservations. Forexample,inthecasewheretheobservationsarecollectedfrompatients,thereshouldbeapriori
knowledgeaboutthehealthhistory,physicalandpsychological,andothertestresultsforeachindividualbeingused
toconstructthetrainingdatasetofthemodel. Thus,sensitivityandspecificitymustpresentvaluescloseto100%. A
modelwithhighsensitivitydoesnotpresentmanyfalse-negativeresultsandmanagestoidentifycorrectlyindividuals
usingawell-definedscreeningprocess. Amodelwithahighspecificitydoesnotpresentmanyfalse-positiveresultsand
managestoidentifycorrectlyonlytheindividualsthathavetheconditionforwhichthemodelisbuilt.
Afteramodelistrainedandevaluated,itneedstopassthegoldenstandard. Inthecaseofmedicine,thisstandardtestis
themodelonwhichwecanextrapolateaprocedure,whetherornotitincreasesthenumberofcorrectscreeningof
patients. Usually,anexpertinthedomainmustvalidatetheresultsand,ifrequired,mustproposetheworkflowand
implementationoftheprocedure. Iftheproceduredirectlyinvolvesthetreatmentofpatients,eachpatientrecovery
statusmustbeundercloseobservationtomedicalexpertstointerveneandadjusttherecoveryprocess.
Iftheexpertsobservethatthemodeldoesnotperformaccurately,theywillgiveadjustthemodelthroughfeedbackand
retrainthemodel. Thefeedbackcancontainbutisnotlimitedtonewobservations,newattributesthatwerenottaken
intoaccountwhenthemodelwastrained,abetterresamplingofthedataset,etc.
3 DeepLearningtechniquesformedicalimages
Medicalimages, e.g., photographs, x-rays, MagneticResonanceImaging(MRIs)orComputerAxialTomography
(CAT)scans,canprovideadditionalinformationforadecisionsystemandtheyhavebecomeacoredatatypeinthe
eHealthfieldduetotheirincreasingavailabilityindigitalform. Nevertheless,theymaketheproblemofunderstanding
dataevenmorecomplexastheirrawcontentisnotexplicitformachineprocessingandprocessingthemisahigh
computationallytask. Also,takingintoaccountthemedicalcontext,temporaryconstraintsinobtainingtheresultscan
beimposedontheprocessingtasks. Likewise,inmanycases,thereareprivacyissuesregardingthemanipulationofthe
medicalimagedatasets.
Forextractingusefulinformationfrommedicalimages,differentdeep-learningmodelscanbeapplied,asinthecaseof
thetextualprocessing. AmoredetaileddescriptionofthesemodelsispresentedinSection2.3. Someothertechniques
usedintextualprocessingcanalsobeappliedtomedicalimages,evenifthisfieldisintheearlystages. Forexample,
taggingtechniqueswouldbeachievedbytaggingallimagesofacertainpatientwiththesameidentifier. Anobstacle
wouldberepresentedbythefactthatviewingandanalyzingamedicalimageisconstrainedtousingonlythevendor
equipment. Butlastly,severalstandardshavebeencreatedtoallowtheimplementationofvendor-independentimage
viewers [25]. Imagesearchingtechniquesallowsearchingimagesbyvisualcontentusingimagesearchpatternsora
languagedescriptionofthatpattern,aprocessknownascomputer-basedimageretrieval(CBIR).
Alltheimageprocessingtaskscanbeclassifiedinoneoftwomajorcategories: segmentationandclassification. For
bothofthetwoclasses,specificdeeplearningmodelscanbeapplied. Segmentationtasksareusedtoidentifydifferent
elementsfromanimage,e.g. identifyorgansorlesions,whichcanthenbeappliedincomplexanalysis. Inclassification
tasks,themodelistrainedtoestablishafunctionalrelationshipbetweenthefeaturesextractedfromtheimagedataset
andthecorrespondingclasses,definedinthemodel. Examplesofclassificationtasksincludei)thedetectionofdifferent
types of malignant and benign tumours from medical images, ii) the detection of malaria from blood smear slide
images[10].
However,deepneuralarchitecturescanbeusedforothertasksthanclassificationandsegmentation,suchasimage
pre-processing. AnexampleofsuchataskisImageDenoising. Asolutionforefficientimagedenoisingusingadeep
architectureisproposedinarticle[32]. ThedeeparchitectureemploysLSTMbasedBatchNormalizationandRNN
techniques. TheinputdatasetconsistsofCTimagesofthelung. Theproposedsolutionremovesdifferenttypesof
noises,i.e.,white,saltandpeppernoises,withoutrequiringaprioriknowledge. Incomparisonwithotherdeeplearning
architectures,i.e.,basedonCNNandGaussiandenoisingCNN(DnCNN),theproposedsolutionobtainedthehighest
accuracy.
ThemajorityofdeeplearningsolutionsproposedforprocessingmedicalimagesusedifferentcombinationsofCNNs
andLSTMslayers. AlthoughtheCNNbasedarchitecturesproduceverygoodresultsinprocessing2Dmedicalimages,
13E.S.ApostolandC.OTruica˘
e.g., [9], theyalonedonothavestateoftheartperformanceinprocessing3Dmedicalimages, astheyarehardto
optimizefor3Dvolumetricfeature-basedclassification. BetterperformancecanbeobtainedbyaddingLSTMlayersto
thearchitecture,e.g.,inarticle[34]aCNN-LSTMdeeparchitectureisusedforclassifying,withhighaccuracy,3D
braintumourinMR(MagneticResonance)images.
Inrecentyears,BoltzmannMachinebasednetworksbegantobewidelyusedinmedicalimaging,withstateoftheart
performance. RestrictedBoltzmannMachinesareusedintensivelyforsegmentationtasks,asautomaticidentification
ofBarrett’sesophagusfromendoscopicimagesoftheloweresophagus[27]. DeepBoltzmannMachinenetworksare
usedmostlyinclassificationtasks,wheretheyobtainextraordinaryresults. Inarticle[16],aDeepBoltzmannMachine
network is used for classifying medical images to detect pre-cancerous and post-cancerous regions. The resulting
performance metrics are very high in comparison with other solutions, i.e., as high as 95.5% accuracy and 93.5%
sensitivity.
4 Conclusions
Largeorganizationslikehospitalscollecthugeamountsofdataabouttheirpatientseveryyear,invariousways,e.g.,
lab reports, medical prescriptions, general files of the patient when entering the hospital, x-rays, Computer Axial
Tomographies. Suchmassivesetsofdatacanprovidegreatknowledgeandinformationwhichcanimprovethemedical
services,andoverallthehealthcaredomain,suchasdiseasepredictionbyanalyzingthepatient’ssymptomsordisease
prevention,byfacilitatingthediscoveryofbehavioralfactorswhichcanturnintoriskfactorsfordisease(e.g.,diet,
alcoholconsumption,environmentalpollutionorphysicalactivities).
Unfortunately,onlyarelativelysmallvolumeofmedicaldataisprocessedandinterpreted,animportantfactorbeing
thedifficultyinefficientlyperformingBigDataoperations. Likewise,anotherimportantfactoristhat,often,realandup
todatemedicaldatasets,evenifanonymous,arehardtoobtain,duetoprivacyissues.
IneHealthprocessing,thecomplexityofdataanalysisalsoarisesfromitsheterogeneity,startingwiththecollectionof
individualdataelementsandmovingtothefusionofmultipledatasets. Therefore,inmanycases,itisnecessaryto
combinedifferenttypesandformatsofdatagatheredfromdisparateheterogeneoussources.
Hence the full process is a difficult and intensely computational one, often needing Cloud like processing power.
Nevertheless,thepossibleoutcomeshavetheabilitytorevealnewwaysofpreventing,detectingandtreatingdiseases.
Fortunately, newandimprovedmethodsforknowledgeextraction, frommedicaltextandimage, arecontinuously
proposed.
References
[1] Emily Alsentzer, John Murphy, William Boag, Wei-Hung Weng, Di Jindi, Tristan Naumann, and Matthew
McDermott. PubliclyavailableclinicalBERTembeddings. InProceedingsofthe2ndClinicalNaturalLanguage
ProcessingWorkshop,pages72–78,Minneapolis,Minnesota,USA,June2019.AssociationforComputational
Linguistics. doi: 10.18653/v1/W19-1909. URLhttps://aclanthology.org/W19-1909.
[2] DamlaArifogluandAbdelhamidBouchachia. Detectionofabnormalbehaviourfordementiasufferersusing
convolutionalneuralnetworks. Artificialintelligenceinmedicine,94:88–95,2019.
[3] Toan Duc Bui, Jae-Joon Lee, and Jitae Shin. Incorporated region detection and classification using deep
convolutionalnetworksforboneageassessment. Artificialintelligenceinmedicine,97:1–8,2019.
[4] JeanCharbonnierandChristianWartena. Usingwordembeddingsforunsupervisedacronymdisambiguation.
InInternationalConferenceonComputationalLinguistics,pages2610–2619.AssociationforComputational
Linguistics,2018.
[5] CorinnaCortesandVladimirVapnik. Support-vectornetworks. Machinelearning,20(3):273–297,1995.
[6] JacobDevlin,Ming-WeiChang,KentonLee,andKristinaToutanova. BERT:pre-trainingofdeepbidirectional
transformersforlanguageunderstanding.InJillBurstein,ChristyDoran,andThamarSolorio,editors,Proceedings
ofthe2019ConferenceoftheNorthAmericanChapteroftheAssociationforComputationalLinguistics: Human
LanguageTechnologies,NAACL-HLT2019,Minneapolis,MN,USA,June2-7,2019,Volume1(LongandShort
Papers),pages4171–4186.AssociationforComputationalLinguistics,2019. doi: 10.18653/v1/n19-1423.
[7] JuliuszDziadek,AronHenriksson,andMartinDuneld. Improvingterminologymappinginclinicaltextwith
context-sensitivespellingcorrection. InformaticsforHealth: ConnectedCitizen-LedWellnessandPopulation
Health,235:241–245,2017.
14AdvancementsineHealthDataAnalyticsthroughNLPandDL
[8] PieterFivez,SimonŠuster,andWalterDaelemans. Unsupervisedcontext-sensitivespellingcorrectionofclinical
free-textwithwordandcharactern-gramembeddings.InBioNLP2017.AssociationforComputationalLinguistics,
2017. doi: 10.18653/v1/W17-2317.
[9] MaayanFrid-Adar,IditDiamant,EyalKlang,MichalAmitai,JacobGoldberger,andHayitGreenspan. Gan-based
syntheticmedicalimageaugmentationforincreasedcnnperformanceinliverlesionclassification.Neurocomputing,
321:321–331,2018.
[10] GGopakumarandGorthiRKSaiSubrahmanyam. Deeplearningapplicationstocytopathology: Astudyonthe
detectionofmalariaandontheclassificationofleukaemiacell-lines. InHandbookofDeepLearningApplications,
pages219–257.Springer,2019.
[11] YixueHao,MohdUsama,JunYang,M.ShamimHossain,andAhmedGhoneim. Recurrentconvolutionalneural
networkbasedmultimodaldiseaseriskprediction. FutureGenerationComputerSystems,92:76–83,mar2019.
doi: 10.1016/j.future.2018.09.031.
[12] BinHe,YiGuan,andRuiDai. Classifyingmedicalrelationsinclinicaltextviaconvolutionalneuralnetworks.
Artificialintelligenceinmedicine,93:43–49,2019.
[13] SeyedAmirHosseinHosseini,ChiZhang,KâmilUgˇurbil,SteenMoeller,andMehmetAkçakaya. sraki-rnn:
acceleratedmriwithscan-specificrecurrentneuralnetworksusingdenselyconnectedblocks. InWaveletsand
SparsityXVIII,volume11138,page111381B.InternationalSocietyforOpticsandPhotonics,2019.
[14] KexinHuang,JaanAltosaar,andRajeshRanganath. Clinicalbert: Modelingclinicalnotesandpredictinghospital
readmission. CoRR,abs/1904.05342,2019. URLhttp://arxiv.org/abs/1904.05342.
[15] FaizaHussainandUsmanQamar. Identificationandcorrectionofmisspelleddrugsnamesinelectronicmedical
records(emr). InInternationalConferenceonEnterpriseInformationSystems,pages333–338,2016.
[16] PandiaRajanJeyarajandEdwardRajanSamuelNadar. Deepboltzmannmachinealgorithmforaccuratemedical
imageanalysisforclassificationofcancerousregion. CognitiveComputationandSystems,1(3):85–90,sep2019.
doi: 10.1049/ccs.2019.0004.
[17] JieJi,BairuiChen,andHongchengJiang. Fully-connectedlstm–crfonmedicalconceptextraction. International
JournalofMachineLearningandCybernetics,pages1–9,2020.
[18] ArmandJoulin,EdouardGrave,PiotrBojanowski,MatthijsDouze,HérveJégou,andTomasMikolov. Fasttext.
zip: Compressingtextclassificationmodels. arXivpreprintarXiv:1612.03651,2016.
[19] AlexKrizhevsky,IlyaSutskever,andGeoffreyEHinton. Imagenetclassificationwithdeepconvolutionalneural
networks. InAdvancesinneuralinformationprocessingsystems,pages1097–1105,2012.
[20] KennethH.Lai,MaximTopaz,FosterR.Goss,andLiZhou. Automatedmisspellingdetectionandcorrectionin
clinicalfree-textrecords. JournalofBiomedicalInformatics,55:188–195,jun2015. doi: 10.1016/j.jbi.2015.04.
008.
[21] Jinhyuk Lee, Wonjin Yoon, Sungdong Kim, Donghyeon Kim, Sunkyu Kim, Chan Ho So, and Jaewoo Kang.
Biobert: apre-trainedbiomedicallanguagerepresentationmodelforbiomedicaltextmining. Bioinformatics,36
(4):1234–1240,2020.
[22] ChaoLi,LeiJi,andJunYan. Acronymdisambiguationusingwordembedding. InAAAIConferenceonArtificial
Intelligence,pages4178–4179,2015.
[23] WusuoLi,ShenghuiShi,ZiqiaoGao,WeiWei,QunxiongZhu,XiaoyongLin,DaguangJiang,andShangGao.
Improveddeepbeliefnetworkmodelanditsapplicationinnamedentityrecognitionofchineseelectronicmedical
records. InInternationalConferenceonBigDataAnalysis.IEEE,mar2018. doi: 10.1109/ICBDA.2018.8367707.
[24] JuanAntonioLossio-Ventura,ClementJonquet,MathieuRoche,andMaguelonneTeisseire. Combiningc-value
andkeywordextractionmethodsforbiomedicaltermsextraction. InInternationalSymposiumonLanguagesin
BiologyandMedicine,pages45–49,2013.
[25] KatherineMarconiandHaroldLehmann. Bigdataandhealthanalytics. CrcPress,2014.
[26] TomásMikolov,KaiChen,GregCorrado,andJeffreyDean. Efficientestimationofwordrepresentationsinvector
space. InYoshuaBengioandYannLeCun,editors,1stInternationalConferenceonLearningRepresentations,
ICLR2013,Scottsdale,Arizona,USA,May2-4,2013,WorkshopTrackProceedings,2013.
[27] LeandroA.Passos,LuisA.deSouzaJr.,RobertMendel,AlannaEbigbo,AndreasProbst,HelmutMessmann,
ChristophPalm,andJoãoPauloPapa. Barrett’sesophagusanalysisusinginfinityrestrictedboltzmannmachines.
JournalofVisualCommunicationandImageRepresentation,59:475–485,feb2019. doi: 10.1016/j.jvcir.2019.01.
043.
15E.S.ApostolandC.OTruica˘
[28] Kevin Patel, Divya Patel, Mansi Golakiya, Pushpak Bhattacharyya, and Nilesh Birari. Adapting pre-trained
wordembeddingsforuseinmedicalcoding. InBioNLP2017,pages302–306.AssociationforComputational
Linguistics,2017. doi: 10.18653/v1/W17-2338.
[29] BenedictePierrejeanandLudovicTanguy. Towardsqualitativewordembeddingsevaluation: Measuringneigh-
borsvariation. InProceedingsofthe2018ConferenceoftheNorthAmericanChapteroftheAssociationfor
ComputationalLinguistics: StudentResearchWorkshop.AssociationforComputationalLinguistics,2018. doi:
10.18653/v1/N18-4005.
[30] AleksandraPiktus,NecatiBoraEdizel,PiotrBojanowski,ÉdouardGrave,RuiFerreira,andFabrizioSilvestri.
Misspellingobliviouswordembeddings. InProceedingsofthe2019ConferenceoftheNorthAmericanChapter
oftheAssociationforComputationalLinguistics: HumanLanguageTechnologies,Volume1(LongandShort
Papers),pages3226–3234,2019. doi: 10.18653/v1/N19-1326.
[31] Yuval Pinter, Robert Guthrie, and Jacob Eisenstein. Mimicking word embeddings using subword RNNs. In
ConferenceonEmpiricalMethodsinNaturalLanguageProcessing,pages102–112.AssociationforComputational
Linguistics,2017. doi: 10.18653/v1/D17-1010.
[32] RRajeev,JAbdulSamath,andNKKarthikeyan. Anintelligentrecurrentneuralnetworkwithlongshort-term
memory(lstm)basedbatchnormalizationformedicalimagedenoising. Journalofmedicalsystems,43(8):234,
2019.
[33] Renzo M. Rivera Zavala, Paloma Martínez, and Isabel Segura-Bedmar. A Hybrid Bi-LSTM-CRF model for
knowledge recognition from ehealth documents. CEUR Workshop Proceedings, 2172:65–70, 2018. ISSN
16130073.
[34] IramShahzadi,TongBoonTang,FabriceMeriadeau,andAbdulQuyyum. Cnn-lstm: Cascadedframeworkfor
braintumourclassification. In2018IEEE-EMBSConferenceonBiomedicalEngineeringandSciences(IECBES),
pages633–637.IEEE,2018.
[35] FelipeSoares,MartaVillegas,AitorGonzalez-Agirre,MartinKrallinger,andJordiArmengol-Estapé. Medical
wordembeddingsforspanish: Developmentandevaluation. InClinicalNaturalLanguageProcessingWorkshop,
pages124–133,2019.
[36] Marina Sokolova and Guy Lapalme. A systematic analysis of performance measures for classification tasks.
InformationProcessing&Management,45(4):427–437,jul2009. doi: 10.1016/j.ipm.2009.03.002.
[37] IrenaSpasic. Acronymsasanintegralpartofmulti-wordtermrecognition–atokenofappreciation. IEEEAccess,
6:8351–8363,2018. doi: 10.1109/ACCESS.2018.2807122.
[38] AndrewTrask,PhilMichalak,andJohnLiu.sense2vec-Afastandaccuratemethodforwordsensedisambiguation
inneuralwordembeddings. CoRR,abs/1511.06388,2015. URLhttp://arxiv.org/abs/1511.06388.
[39] HaoWang,DejingDou,andDanielLowd. Ontology-baseddeeprestrictedboltzmannmachine. InInternational
ConferenceonDatabaseandExpertSystemsApplications,pages431–445.SpringerInternationalPublishing,
2016. doi: 10.1007/978-3-319-44403-1_27.
[40] Yonghui Wu, Jun Xu, Yaoyun Zhang, and Hua Xu. Clinical abbreviation disambiguation using neural word
embeddings. InProceedingsofBioNLP15.AssociationforComputationalLinguistics,2015. doi: 10.18653/v1/
W15-3822.
[41] XiYang,JiangBian,RuoguFang,RagnhildurIBjarnadottir,WilliamRHogan,andYonghuiWu. Identifying
relationsofmedicationswithadversedrugeventsusingrecurrentconvolutionalneuralnetworksandgradient
boosting. JournaloftheAmericanMedicalInformaticsAssociation,27(1):65–72,2020.
[42] AzitaYazdani,MarjanGhazisaeedi,NasrinAhmadinejad,MasoumehGiti,HabibeAmjadi,andAzinNahvijou.
Automatedmisspellingdetectionandcorrectioninpersianclinicaltext. JournalofDigitalImaging,pages1–8,
2019. doi: 10.1007/s10278-019-00296-y.
[43] Vithya Yogarajan, Henry Gouk, Tony Smith, Michael Mayo, and Bernhard Pfahringer. Comparing high di-
mensional word embeddings trained on medical text to bag-of-words for predicting medical codes. In In-
telligent Information and Database Systems, pages 97–108. Springer International Publishing, 2020. doi:
10.1007/978-3-030-41964-6_9.
[44] YinyuanZhang,RicardoHenao,ZheGan,YitongLi,andLawrenceCarin. Multi-labellearningfrommedical
plaintextwithconvolutionalresidualmodels. InMachineLearningforHealthcareConference,volume85of
MachineLearningResearch,pages280–294,2018.
[45] ZiqiZhang,JoséIria,ChristopherBrewster,andFabioCiravegna. Acomparativeevaluationoftermrecognition
algorithms. InLREC,volume5,2008.
16