PublishedasaconferencepaperatICLR2024
LDREG: LOCAL DIMENSIONALITY REGULARIZED
SELF-SUPERVISED LEARNING
HanxunHuang1 RicardoJ.G.B.Campello2 SarahErfani1 XingjunMa3
MichaelE.Houle4 JamesBailey1
1SchoolofComputingandInformationSystems,TheUniversityofMelbourne,Australia
2DepartmentofMathematicsandComputerScience,UniversityofSouthernDenmark,Denmark
3SchoolofComputerScience,FudanUniversity,China
4DepartmentofComputerScience,NewJerseyInstituteofTechnology,USA
ABSTRACT
Representations learned via self-supervised learning (SSL) can be susceptible to
dimensional collapse, where the learned representation subspace is of extremely
lowdimensionalityandthusfailstorepresentthefulldatadistributionandmodali-
ties. Dimensionalcollapse–––alsoknownasthe“underfilling”phenomenon–––
is one of the major causes of degraded performance on downstream tasks. Pre-
viousworkhasinvestigatedthedimensionalcollapseproblemofSSLataglobal
level.Inthispaper,wedemonstratethatrepresentationscanspanoverhighdimen-
sionalspaceglobally,butcollapselocally. Toaddressthis,weproposeamethod
calledlocaldimensionalityregularization(LDReg). Ourformulationisbasedon
the derivation of the Fisher-Rao metric to compare and optimize local distance
distributionsatanasymptoticallysmallradiusforeachdatapoint. Byincreasing
thelocalintrinsicdimensionality,wedemonstratethrougharangeofexperiments
thatLDRegimprovestherepresentationqualityofSSL.Theresultsalsoshowthat
LDRegcanregularizedimensionalityatbothlocalandgloballevels.
1 INTRODUCTION
Self-supervised learning (SSL) is now approaching the same level of performance as supervised
learning on numerous tasks (Chen et al., 2020a;b; He et al., 2020; Grill et al., 2020; Chen & He,
2021;Caronetal.,2021;Zbontaretal.,2021;Chenetal.,2021;Bardesetal.,2022;Zhangetal.,
2022).SSLfocusesontheconstructionofeffectiverepresentationswithoutrelianceonlabels.Qual-
itymeasuresforsuchrepresentationsarecrucialtoassessandregularizethelearningprocess. Akey
aspect of representation quality is to avoid dimensional collapse and its more severe form, mode
collapse,wheretherepresentationconvergestoatrivialvector(Jingetal.,2022). Dimensionalcol-
lapse refers to the phenomenon whereby many of the features are highly correlated and thus span
onlyalower-dimensionalsubspace. Existingworkshaveconnecteddimensionalcollapsewithlow
qualityoflearnedrepresentations(He&Ozay,2022;Lietal.,2022;Garridoetal.,2023a;Dubois
etal.,2022). Bothcontrastiveandnon-contrastivelearningcanbesusceptibletodimensionalcol-
lapse(Tianetal.,2021;Jingetal.,2022;Zhangetal.,2022),whichcanbemitigatedbyregularizing
dimensionalityasaglobalproperty,suchaslearningdecorrelatedfeatures(Huaetal.,2021)ormin-
imizingtheoff-diagonaltermsofthecovariancematrix(Zbontaretal.,2021;Bardesetal.,2022).
In this paper, we examine an alternative approach to the problem of dimensional collapse, by in-
vestigating the local properties of the representation. Rather than directly optimizing the global
dimensionalityoftheentiretrainingdataset(intermsofcorrelationmeasures),weproposetoregu-
larizethelocalintrinsicdimensionality(LID)(Houle,2017a;b)ateachtrainingsample. Weprovide
anintuitiveillustrationoftheideaofLIDinFigure1. Givenarepresentationvector(anchorpoint)
anditssurroundingneighbors, ifrepresentationscollapsetoalow-dimensionalspace, itwouldre-
sult in a lower sample LID for the anchor point (Figure 1a). In SSL, each anchor point should be
dissimilar from all other points and should have a higher sample LID (Figure 1b). Based on LID,
werevealaninterestingobservationthat: representationscanspanahighdimensionalspaceglob-
ally,butcollapselocally. Asshowninthetop4subfiguresinFigure1c,thedatapointscouldspan
1
4202
naJ
91
]GL.sc[
1v47401.1042:viXraPublishedasaconferencepaperatICLR2024
over different local dimensional subspaces (LIDs) while having roughly the same global intrinsic
dimension(GID).Thissuggeststhatdimensionalcollapseshouldnotonlybeexaminedasaglobal
property but also locally. Note that Figure 1c illustrates a synthetic case of local dimensional col-
lapse. Laterwewillempiricallyshowthatrepresentationsconvergingtoalocallylow-dimensional
subspacecanhavereducedqualityandthathigherLIDisdesirableforSSL.
Local Dimensional Collapse
Anchor Point LID=0.84 Anchor Point LID=5.14
2.0
1.5
0.0 1.0
FR 0.65 FR 0.50 FR 0.32 FR 0.16
0.5
Random Random
Anchor Point Anchor Point
0.0
0.0 0.0 0.5 1.0 1.5 2.0
(a)Localcollapse (b)Nolocalcollapse (c)Localdimensionalcollapse
Figure 1: Illustrations with 2D synthetic data. (a-b) The LID value of the anchor point (red star)
whenthereis(orisno)localcollapse. (c)Fisher-Rao(FR)metricandmeanLID(mLID)estimates.
FRmeasuresthedistancebetweentwoLIDdistributions,andiscomputedbasedonourtheoretical
results. mLIDisthegeometricmeanofsample-wiseLIDscores. HighFRdistancesandlowmLID
scores indicate greater dimensional collapse. Global intrinsic dimension (GID) is estimated using
theDanCOalgorithm(Cerutietal.,2014).
Toaddresslocaldimensionalcollapse,weproposetheLocalDimensionalityRegularizer(LDReg),
which regularizes the representations toward a desired local intrinsic dimensionality to avoid col-
lapse,asshownatthebottomsubfigureofFigure1c.OurapproachleveragestheLIDRepresentation
Theorem (Houle, 2017a), which has established that the distance distribution of nearest neighbors
in an asymptotically small radius around a given sample is guaranteed to have a parametric form.
ForLDRegtobeabletoinfluencethelearnedrepresentationstowardadistributionallyhigherLID,
werequireawaytocomparedistributionsthatissensitivetodifferencesinLID.Thismotivatesus
to develop a new theory to enable measurement of the ‘distance’ between local distance distribu-
tions,aswellasidentifythemeanofasetoflocaldistancedistributions. Wederiveatheoretically
well-founded Fisher-Rao metric (FR), which considers a statistical manifold for assessing the dis-
tancebetweentwolocaldistancedistributionsintheasymptoticlimit. AsshowninFigure1c, FR
correspondswellwithdifferentdegreesofdimensionalcollapse. MoredetailsregardingFigure1c
canbefoundinAppendixA.
Thetheorywedevelopherealsoleadstotwonewinsights: i)LIDvaluesarebettercomparedusing
thelogarithmicscaleratherthanthelinearscale;ii)ForaggregatingLIDvalues,thegeometricmean
isamorenaturalchoicethanthearithmeticorharmonicmeans. Theseinsightshaveconsequences
forformulatingourlocaldimensionalityregularizationobjective,aswellasbroaderimplicationsfor
comparingandreportingLIDvaluesinothercontexts.
Tosummarize,themaincontributionsofthispaperare:
• Anewapproach,LDReg,formitigatingdimensionalcollapseinSSLviatheregularization
oflocalintrinsicdimensionalitycharacteristics.
• TheorytosupporttheformulationofLIDregularization,insightsintohowdimensionalities
should be compared and aggregated, and generic dimensionality regularization technique
thatcanpotentiallybeusedinothertypesoflearningtasks.
• Consistent empirical results demonstrating the benefit of LDReg in improving multiple
state-of-the-artSSLmethods(includingSimCLR,SimCLR-Tuned,BYOL,andMAE),and
itseffectivenessinaddressingbothlocalandglobaldimensionalcollapse.
2 RELATED WORK
Self-Supervised Learning (SSL). SSL aims to automatically learn high-quality representations
without label supervision. Existing SSL methods can be categorized into two types: generative
2PublishedasaconferencepaperatICLR2024
methodsandcontrastivemethods. Ingenerativemethods,themodellearnsrepresentationsthrough
areconstructionoftheinput(Hinton&Zemel,1993).Inspiredbymaskedlanguagemodeling(Ken-
ton&Toutanova,2019),recentworkshavesuccessfullyextendedthisparadigmtothereconstruc-
tion of masked images (Bao et al., 2022; Xie et al., 2022), such as Masked AutoEncoder (MAE)
(He et al., 2022). It has been theoretically proven that these methods are a special form of con-
trastivelearningthatimplicitlyalignspositivepairs(Zhangetal.,2022). Contrastivemethodscan
furtherbedividedinto1)sample-contrastive,2)dimension-contrastive(Garridoetal.,2023b),and
3)asymmetricalmodels. SimCLR(Chenetal.,2020a)andothersample-contrastivemethods(He
etal.,2020;Chenetal.,2020a;b;Yehetal.,2022)arebasedonInfoNCEloss(Oordetal.,2018).
The sample-contrastive approach has been extended by using nearest-neighbor methods (Dwibedi
etal.,2021;Geetal.,2023),clustering-basedmethods(Caronetal.,2018;2020;Pangetal.,2022),
andimprovedaugmentationstrategies(Wangetal.,2023).Dimension-contrastivemethods(Zbontar
et al., 2021; Bardes et al., 2022) regularize the off-diagonal terms of the covariance matrix of the
embedding. Asymmetricalmodelsuseanasymmetricarchitecture, suchasanadditionalpredictor
(Chen & He, 2021), self-distillation (Caron et al., 2021), or a slow-moving average branch as in
BYOL(Grilletal.,2020).
Dimensional collapse in SSL. Dimensional collapse occurs during the SSL process where the
learnedembeddingvectorsandrepresentationsspanonlyalower-dimensionalsubspace(Huaetal.,
2021;Jingetal.,2022;He&Ozay,2022;Lietal.,2022). GenerativemethodssuchasMAE(He
etal.,2022)havebeenshowntobesusceptibletodimensionalcollapse(Zhangetal.,2022).Sample-
contrastivemethodssuchasSimCLRhavealsobeenobservedtosufferfromdimensionalcollapse
(Jingetal.,2022). Otherstudiessuggestthatwhilestrongeraugmentationandlargerprojectorsare
beneficialtotheperformance(Garridoetal.,2023b),theymaycauseadimensionalcollapseinthe
projectorspace(Cosentinoetal.,2022). Ithasbeentheoreticallyproventhatasymmetricalmodel
methodscanalleviatedimensionalcollapse,andtheeffectiverank(Roy&Vetterli,2007)isause-
ful measure of the degree of global collapse (Zhuo et al., 2023). Effective rank is also helpful in
assessing the representation quality (Garrido et al., 2023a). By decorrelating features, dimension-
contrastive methods (Zbontar et al., 2021; Zhang et al., 2021; Ermolov et al., 2021; Bardes et al.,
2022)canalsoavoiddimensionalcollapse. Inthiswork,wefocusonthelocaldimensionalityofthe
representation(encoder)space,whichlargelydeterminestheperformanceofdownstreamtasks.
LocalIntrinsicDimensionality. Unlikeglobalintrinsicdimensionalitymetrics(Pettisetal.,1979;
Bruske&Sommer,1998),localintrinsicdimensionality(LID)measurestheintrinsicdimensionin
thevicinityofaparticularquerypoint(Levina&Bickel,2004;Houle,2017a). Ithasbeenusedasa
measureforsimilaritysearch(Houleetal.,2012),forcharacterizingadversarialsubspaces(Maetal.,
2018a), for detecting backdoor attacks (Dolatabadi et al., 2022), and in the understanding of deep
learning(Maetal.,2018b;Gongetal.,2019;Ansuinietal.,2019;Popeetal.,2021).InAppendixB,
weprovideacomparisonbetweentheeffectiverankandtheLIDtohelpunderstandlocalvs.global
dimensionality. OurworkinthispapershowsthatLIDisnotonlyusefulasadescriptivemeasure,
butcanalsobeusedaspartofapowerfulregularizerforSSL.
3 BACKGROUND AND TERMINOLOGY
We first introduce the necessary background for the distributional theory underpinning LID. The
dimensionalityofthelocaldatasubmanifoldinthevicinityofareferencesampleisrevealedbythe
growthcharacteristicsofthecumulativedistributionfunctionofthelocaldistancedistribution.
LetF beareal-valuedfunctionthatisnon-zerooversomeopenintervalcontainingr ∈R,r ̸=0.
Definition1((Houle,2017a)). TheintrinsicdimensionalityofF atrisdefinedasfollows,whenever
thelimitexists:
ln(F((1+ϵ)r)/F(r))
IntrDim (r) ≜ lim .
F ϵ→0 ln((1+ϵ)r/r)
Theorem1((Houle,2017a)). IfF iscontinuouslydifferentiableatr,then
r·F′(r)
LID (r) ≜ = IntrDim (r).
F F(r) F
Althoughtheprecedingdefinitionsapplymoregenerally,wewillbeparticularlyinterestedinfunc-
tionsF thatsatisfytheconditionsofacumulativedistributionfunction(CDF).Letxbealocationof
3PublishedasaconferencepaperatICLR2024
interestwithinadatadomainS forwhichthedistancemeasured:S×S →R hasbeendefined.
≥0
Toanygeneratedsamples∈S,weassociatethedistanced(x,s);inthisway,aglobaldistribution
thatproducesthesamplescanbesaidtoinducetherandomvalued(x,s)fromalocaldistribution
ofdistancestakenwithrespecttox. TheCDFF(r)ofthelocaldistancedistributionissimplythe
probabilityofthesampledistancelyingwithinathresholdr—thatis,F(r)≜Pr[d(x,s)≤r].
Tocharacterizethelocalintrinsicdimensionalityinthevicinityoflocationx,weconsiderthelimit
ofLID (r)asthedistancertendsto0. RegardlessofwhetherF satisfiestheconditionsofaCDF,
F
wedenotethislimitby
LID∗ ≜ lim LID (r).
F F
r→0+
Henceforth,whenwerefertothelocalintrinsicdimensionality(LID)ofafunctionF,orofapoint
x whose induced distance distribution has F as its CDF, we will take ‘LID’ to mean the quantity
LID∗.
F
Ingeneral,LID∗ isnotnecessarilyaninteger. Unlikethemanifoldmodeloflocaldatadistributions
F
—wherethedimensionalityofthemanifoldisalwaysaninteger,anddeviationfromthemanifold
isconsideredas‘error’—theLIDmodelreflectstheentirelocaldistributionalcharacteristicswith-
out distinguishing error. However, the estimation of the LID at x often gives an indication of the
dimensionofthelocalmanifoldcontainingxthatwouldbestfitthedistribution.
4 ASYMPTOTIC FORM OF FISHER-RAO METRIC FOR LID DISTRIBUTIONS
We now provide the necessary theoretical justifications for our LDReg regularizer which will be
laterdevelopedinSection5. Intuitively,LDRegshouldregularizetheLIDofthelocaldistribution
of the training samples towards a higher value, determined as the LID of some target distribution.
Thiscanhelptoavoiddimensionalcollapsebyincreasingthedimensionalityoftherepresentation
spaceandproducingrepresentationsthataremoreuniformintheirlocaldimensionalcharacteristics.
To achieve this, we will need an asymptotic notion of distributional distance that applies to lower
taildistributions. Inthissection,weintroduceanasymptoticvariantoftheFisher-Raodistancethat
canbeusedtoidentifythecenter(mean)ofacollectionoftaildistributions.
4.1 FISHER-RAODISTANCEMETRIC
TheFisher-RaodistanceisbasedontheembeddingofthedistributionsonaRiemannianmanifold,
where it corresponds to the length of the geodesic along the manifold between the two distribu-
tions. The metric is usually impossible to compute analytically, except for special cases (such as
certainvarietiesofGaussians). However,intheasymptoticlimitasw → 0,wewillshowthatitis
analyticallytractableforsmoothgrowthfunctions.
Definition 2. Given a non-empty set X and a family of probability density functions ϕ(x|θ) pa-
rameterized by θ on X , the space M = {ϕ(x|θ)|θ ∈ Rd} forms a Riemannian manifold. The
Fisher-Rao Riemannian metric on M is a function of θ and induces geodesics, i.e., curves with
minimumlengthonM. TheFisher-Raodistancebetweentwomodelsθ andθ isthearc-length
1 2
ofthegeodesicthatconnectsthesetwopoints.
Inourcontext,wewillfocusonunivariatelowertaildistributionswithasingleparameterθ corre-
sponding to the LID of the CDF. In this context, the Fisher-Rao distance will turn out to have an
elegantanalyticalform. WewillmakeuseoftheFisherinformationI,whichisthevarianceofthe
gradientofthelog-likelihoodfunction(alsoknownastheFisherscore).Fordistributionsover[0,w]
withasingleparameterθ,thisisdefinedas
(cid:90) w(cid:18) ∂ (cid:19)2
I (θ)= lnF′(r|θ) F′(r|θ)dr.
w ∂θ w w
0
Lemma1. Considerthefamilyoftaildistributionson[0,w]parameterizedbyθ,whoseCDFsare
smoothgrowthfunctionsoftheform
(cid:16)r(cid:17)θ
H (r)= .
w|θ w
4PublishedasaconferencepaperatICLR2024
TheFisher-Raodistanced betweenH andH is
FR w|θ1 w|θ2
(cid:12) (cid:12)
d FR(H w|θ1,H
w|θ2)=(cid:12)
(cid:12) (cid:12)ln
θθ 2(cid:12)
(cid:12)
(cid:12)
.
1
TheFisherinformationI forsmoothgrowthfunctionsoftheformH is:
w w|θ
(cid:90) w(cid:18) ∂ (cid:19)2 1
I (θ) = lnH′ (r) H′ (r)dr = .
w ∂θ w|θ w|θ θ2
0
TheproofofLemma1canbefoundinAppendixC.2.
4.2 ASYMPTOTICFISHER-RAOMETRIC
We now extend the notion of the Fisher-Rao metric to distance distributions whose CDFs (condi-
tionedtothelowertail[0,w])havethemoregeneralformofagrowthfunction. TheLIDRepresen-
tationTheorem(Theorem 3inAppendix C.1)tellsusthatanysuchCDFF (r)canbedecomposed
w
intotheproductofacanonicalformH (r)withanauxiliaryfactorA (r,w):
w|LID∗ F
F
(cid:16)r(cid:17)LID∗ (cid:18)(cid:90) w LID∗ −LID (t) (cid:19)
F (r) = H (r)·A (r,w) = F ·exp F F dt .
w w|LID∗ F F w t
r
FromCorollary3.1(Appendix C.1),theauxiliaryfactorA (r,w)tendsto1asrandwtendto0,
F
providedthatrstayswithinaconstantfactorofw. Asymptotically,then,F canbeseentotendto
w
H asthetaillengthtendstozero,forθ =LID∗. Moreprecisely,foranyconstantc≥1,
w|θ F
F (r)
lim w = lim A (r,w) = 1.
w→0+ H w|LID∗(r) w→0+ F
w/c≤r≤cw F w/c≤r≤cw
Thus,althoughtheCDFF doesnotingeneraladmitafiniteparameterizationsuitableforthedirect
w
definitionofaFisher-Raodistance,asymptoticallyittendstoadistributionthatdoes: H .
w|LID∗
F
UsingLemma1wedefineanasymptoticformofFisher-Raodistancebetweendistancedistributions.
Definition3. Giventwosmooth-growthdistancedistributionswithCDFsF andG,theirasymptotic
Fisher-Raodistanceisgivenby
d AFR(F,G) ≜ wl →im 0+d FR(H w|LID∗ F,H w|LID∗ G) = (cid:12) (cid:12) (cid:12) (cid:12)lnL LI ID D∗ G ∗ F(cid:12) (cid:12) (cid:12) (cid:12) .
4.3 IMPLICATIONS
Remark1.1. AssumethatLID∗ ≥ 1andthatG = U istheone-dimensionaluniformdistri-
F w 1,w
butionovertheinterval[0,w](withLID∗ thereforeequalto1). Wethenhave
G
d (F ,U ) = lnLID∗
AFR w 1,w F
LID∗ = exp(d (F ,U )) .
F AFR w 1,w
WecanthereforeinterpretthelocalintrinsicdimensionalityofadistributionF conditionedtothe
interval[0,w](withLID∗ ≥ 1)astheexponentialofthedistancebetweendistributionF andthe
F
uniformdistributioninthelimitasw →0.
ThereisalsoacloserelationshipbetweenourasymptoticFisher-Raodistancemetricandamathe-
maticallyspecialmeasureofrelativedifference.
Remark1.2. Onecaninterpretthequantity|ln(LID∗ G/LID∗ F)|asarelativedifferencebetweenLID∗
G
andLID∗.Furthermore,itistheonlymeasureofrelativedifferencethatisbothsymmetric,additive,
F
andnormed(To¨rnqvistetal.,1985).
The asymptotic Fisher-Rao distance indicates that the absolute difference between the LID values
of two distance distributions is not a good measure of asymptotic dissimilarity. For example, a
5PublishedasaconferencepaperatICLR2024
pairofdistributionswithLID∗ = 2andLID∗ = 4aremuchlesssimilarundertheasymptotic
F1 G1
Fisher-RaometricthanapairofdistributionswithLID∗ =20andLID∗ =22.
F2 G2
WecanalsousetheasymptoticFisher-Raometrictocomputethe‘centroid’orFre´chetmean1 ofa
setofdistancedistributions,aswellastheassociatedFre´chetvariance.
Definition 4. Given a set of distance distribution CDFs F = {F1,F2,...,FN}, the empirical
Fre´chetmeanofF isdefinedas
N
µ ≜ argmin 1 (cid:88)(cid:0) d (H ,Fi)(cid:1)2 .
F N AFR w|θ
Hw|θ
i=1
TheFre´chetvarianceofF isthendefinedas
N N
σ2 ≜ 1 (cid:88)(cid:0) d (µ ,Fi)(cid:1)2 = 1 (cid:88)(cid:0) lnLID∗ −lnLID∗ (cid:1)2 .
F N AFR F N Fi µF
i=1 i=1
TheFre´chetvariancecanbeinterpretedasthevarianceofthelocalintrinsicdimensionalitiesofthe
distributionsinF,takeninlogarithmicscale.
TheFre´chetmeanhasawell-knowncloseconnectiontothegeometricmean,whenthedistanceis
expressed as a difference of logarithmic values. For our setting, we state this relationship in the
followingtheorem,theproofofwhichcanbefoundinAppendixC.3.
Theorem 2. Let µ be the empirical Fre´chet mean of a set of distance distribution CDFs
F
F = {F1,F2,...,FN} using the asymptotic Fisher-Rao metric d . Then LID∗ =
(cid:16) (cid:17)
AFR µF
exp 1 (cid:80)N lnLID∗ ,thegeometricmeanof{LID∗ ,...,LID∗ }.
N i=1 Fi F1 FN
Corollary2.1. GiventheCDFsF ={F1,F2,...,FN},thequantity 1 (cid:80)N lnLID∗ is:
N i=1 Fi
1. TheaverageasymptoticFisher-RaodistanceofmembersofF totheone-dimensionaluni-
formdistribution(ifforalliwehaveLID∗ ≥1).
Fi
2. ThelogarithmofthelocalintrinsicdimensionoftheFre´chetmeanofF.
3. The logarithm of the geometric mean of the local intrinsic dimensions of the members of
F.
TheproofofAssertion1isinAppendixC.4. Assertions2and3followfromTheorem2.
Itisnaturaltoconsiderwhetherothermeasuresofdistributionaldivergencecouldbeusedinplace
oftheasymptoticFisher-RaometricinthederivationoftheFre´chetmean. Baileyetal.(2022)have
shownseveralotherdivergencesinvolvingtheLIDofdistancedistributions—mostnotablythatof
theKullback-Leibler(KL)divergence. WecaninfactshowthattheasymptoticFisher-Raometric
ispreferable(intheory)totheasymptoticKLdistance,andthegeometricmeanispreferabletothe
arithmeticmeanandharmonicmeanwhenaggregatingtheLIDvaluesofdistancedistributions. For
thedetails,wereferreaderstoTheorem4inAppendixC.5.
Insummary,Theorems2and4(AppendixC.5)showthattheasymptoticFisher-Raometricisprefer-
ableinmeasuringthedistributiondivergenceofLIDs. Thesetheoremsprovidetheoreticaljustifica-
tionforLDReg,whichwillbedescribedinthefollowingsection.
5 LID REGULARIZATION FOR SELF-SUPERVISED LEARNING
In this section, we formally introduce our proposed LDReg method. For an input image x and an
encoderf(·),therepresentationofxcanbeobtainedasz = f(x). DependingontheSSLmethod,
aprojectorg(·),apredictorh(·),andadecodert(·)canbeusedtoobtaintheembeddingvectoror
thereconstructedimagefromz. LDRegisagenericregularizationonrepresentationszobtainedby
theencoder,andassuchitcanbeappliedtoavarietyofSSLmethods(moredetailsareinAppendix
E).WedenotetheobjectivefunctionofanSSLmethodbyLSSL.
1AlsoknownastheKarchermean,theRiemannianbarycenterandtheRiemanniancenterofmass.
6PublishedasaconferencepaperatICLR2024
Local Dimensionality Regularization (LDReg). Following theorems derived in Section 4, we
assumethattherepresentationaldimensionisd,andthatwearegiventherepresentationofasample
x . Suppose that the distance distribution induced by F at x is Fi(r). To avoid dimensional
i i w
collapse,weconsidermaximizingthedistributionaldistancebetweenFi(r)andauniformdistance
w
distribution U (r) (with LID = 1): for each sample, we could regularize a local representation
1,w
thathasalocalintrinsicdimensionalitymuchgreaterthan1(andthusclosertotherepresentational
dimensiond≫1). WecouldregularizebymaximizingthesumofsquaredasymptoticFRdistances
(L -styleregularization),orofabsoluteFRdistances(L -styleregularization).
2 1
InaccordancetoCorollary2.1inSection4.2,weapplyL -regularizationtominimizethenegative
1
logofthegeometricmeanoftheIDvalues. AssumingthatLID∗ isdesiredtobe≥1,
Fi
w
N N
1 (cid:88) 1 (cid:88)
max lim d (Fi(r),U (r)) = min− lnLID∗ , (1)
N w→0 AFR w 1,w N F wi
i i
where N is the batch size. Following Theorem 2 in Section 4.2, we apply L -regularization to
2
maximizetheFre´chetvarianceunderapriorofµ =1:
F
N N
1 (cid:88) 1 (cid:88)(cid:16) (cid:17)2
max lim(d (Fi(r),U (r)))2 = min− lnLID∗ . (2)
N w→0 AFR w 1,w N F wi
i i
Our preference for the geometric mean over the arithmetic mean for L - and L -regularization is
1 2
justifiedbyTheorem4inAppendixC.5. WereferreaderstoAppendixDforadiscussionofother
regularizationformulations.
WeusetheMethodofMoments(Amsalegetal.,2018)asourestimatorofLID,duetoitssimplicity.
Since only the encoder is kept for downstream tasks, we estimate the LID values based on the
encoder representations (z = f(x)). Specifically, we calculate the pairwise Euclidean distance
betweentheencoderrepresentationsofabatchofsamplestoestimatetheLID∗ foreachsample
Fi
x inthebatch: LID∗ = − µk ,wherek denotesthenumberofnearestneigw hborsofz ,w is
i F wi µk−wk i k
thedistancetothek-thnearestneighbor,andµ istheaveragedistancetoallknearestneighbors.
k
Theoveralloptimizationobjectiveisdefinedasaminimizationofeitherofthefollowinglosses:
1 (cid:88)N (cid:32) 1 (cid:88)N (cid:16) (cid:17)2(cid:33)1 2
L =LSSL−β lnLID∗ or L =LSSL−β lnLID∗ , (3)
L1 N F wi L2 N F wi
i i
whereβisahyperparameterbalancingthelossandregularizationterms. Moredetailsofhowtoap-
plyLDRegondifferentSSLmethodscanbefoundinAppendixE;thepseudocodeisinAppendixJ.
6 EXPERIMENTS
WeevaluatetheperformanceofLDRegintermsofrepresentationquality,suchastrainingalinear
classifier on top of frozen representations. We use SimCLR (Chen et al., 2020a), SimCLR-Tuned
(Garrido et al., 2023b), BYOL (Grill et al., 2020), and MAE (He et al., 2022) as baselines. We
performourevaluationwithResNet-50(Heetal.,2016)(forSimCLR,SimCLR-Tuned,andBYOL)
andViT-B(Dosovitskiyetal.,2021)(forSimCLRandMAE)onImageNet(Dengetal.,2009). As
a default, we use batch size 2048, 100 epochs of pretraining for SimCLR, SimCLR-Tuned and
BYOL,and200epochsforMAE,andhyperparameterschoseninaccordancewitheachbaseline’s
recommendedvalues. Weevaluatetransferlearningperformancebyperforminglinearevaluations
onotherdatasets,includingFood-101(Bossardetal.,2014),CIFAR(Krizhevsky&Hinton,2009),
Birdsnap(Bergetal.,2014),StanfordCars(Krauseetal.,2013),andDTD(Cimpoietal.,2014).For
finetuning,weuseRCNN(Girshicketal.,2014)toevaluateondownstreamtasksusingtheCOCO
dataset (Lin et al., 2014). Detailed experimental setups are provided in Appendix F. For LDReg
regularization, weuse k = 64 asthe defaultneighborhoodsize. For ResNet-50, weset β = 0.01
forSimCLRandSimCLRTuned,β =0.005forBYOL.ForViT-B,wesetβ =0.001forSimCLR,
and β = 5×10−6 for MAE. Since L and L perform similarly (see Appendix G.1), here we
L1 L2
mainlyreporttheresultsofL . Anexperimentshowinghowlocalcollapsetriggersmodecollapse
L1
isprovidedinAppendixG.2,whileanablationstudyofhyperparametersisinAppendixG.3.
7PublishedasaconferencepaperatICLR2024
6.1 LDREGREGULARIZATIONINCREASESLOCALANDGLOBALINTRINSICDIMENSIONS
Intrinsicdimensionalityhaspreviouslybeenusedtounderstanddeepneuralnetworksinasupervised
learningcontext(Maetal.,2018b;Gongetal.,2019;Ansuinietal.,2019).ForSSL,Figure2ashows
thatthegeometricmeanofLIDtendstoincreaseoverthecourseoftraining.Forcontrastivemethods
(SimCLRandBYOL),themeanofLIDslightlydecreasesatthelatertrainingstages(dashlines).
WithLDReg,themeanofLIDincreasesforallbaselinemethodsandalleviatesthedecreasingtrend
atthelaterstages(solidlines),mostnotablyonBYOL.
40 20 35
700 Original Original
35 + LDReg + LDReg
19 600 30
30 500 25 18 25
400
20 17 20
15 S St tr re en ng gt th h= =1 0. .0 8 ( (6 64 4. .3 32 0% %) ) 300
10 S S B MYi i Am m O EC C LL LR R (Tuned) S S B MYi i Am m O EC C L ++L L LR R L D+ D( RTL R euD e gn gR ee dg )+LDReg 16 S S St t tr r re e en n ng g gt t th h h= = =0 0 0. . .6 4 2 ( ( (6 6 64 3 3. . .1 6 09 7 1% % %) ) ) 12 00 00 15
5 15 SimCLR SimCLR BYOL MAE 10SimCLR SimCLR BYOL MAE
Epochs Epochs (Tuned) (Tuned)
(a)LIDsinSSL (b)Colorjitterstrength (c)Effectiverank (d)GeometricmeanofLID
Figure 2: (a) Geometric mean of LID values over training epochs. (b) Geometric mean of LID
values with varying color jitter strength in the augmentations for SimCLR. The linear evaluation
resultisreportedinthelegend. (a-b)LIDiscomputedonthetrainingset. (c-d)Theeffectiverank
and LID are computed for samples in the validation set. The solid and transparent bars represent
thebaselinemethodwithandwithoutLDRegregularization,respectively. MAEusesViT-Basthe
encoder,andothersuseResNet-50.
InFigure2b,weadjustedthecolorjitterstrengthoftheSimCLRaugmentationpolicyandobserved
thatthemeanLIDoftherepresentationspacepositivelycorrelateswiththestrength. Thisindicates
thatstrongeraugmentationstendtotriggermorevariationsoftheimageandthusleadtorepresen-
tationsofhigherLID.ThisprovidesinsightsonwhydataaugmentationisimportantforSSLGrill
etal.(2020);VonKu¨gelgenetal.(2021)andwhyitcanhelpavoiddimensionalcollapse(Wagner
etal.,2022;Huangetal.,2023).
Theeffectiverank(Roy&Vetterli,2007)isametrictoevaluatedimensionalityasaglobalproperty
andcanalsobeusedasametricforrepresentationquality(Garridoetal.,2023a). Figure2cshows
thatBYOLislesssusceptibletodimensionalcollapse. SimCLR-Tunedusesthesameaugmentation
asBYOL(whichisstrongerthanthatofSimCLR),yetstillconvergestoalowerdimensionalspace
as compared with BYOL. This indicates that for SimCLR and its variants, stronger augmentation
is not sufficient to prevent global dimensional collapse. Generative method MAE is known to be
pronetodimensionalcollapse(Zhangetal.,2022). Unsurprisingly,ithasthelowesteffectiverank
in Figure 2c. Note that the extremely low effective rank of MAE is also related to its low repre-
sentationdimensionwhichis768inViT-B(othermethodsshownhereusedResNet-50whichhasa
representationdimensionof2048).
We also analyze the geometric mean of the LID values in Figure 2d. It shows that, compared
to other methods, BYOL has a much lower mean LID value. This implies that although BYOL
does not collapse globally, it converges to a much lower dimension locally. We refer readers to
AppendixG.2forananalysisofhowlocalcollapse(extremelylowLID)couldtriggeracomplete
modecollapse,therebydegradingtherepresentationquality.Finally,theuseofourproposedLDReg
regularizationcaneffectivelyvoiddimensionalcollapseandproducebothincreasedglobalandlocal
dimensionalities(asshowninFigures2cand2dwith‘+LDReg’).
6.2 EVALUATIONS
We evaluate the representation quality learned by different methods via linear evaluation, transfer
learning,andfine-tuningondownstreamtasks. AsshowninTable1,LDRegconsistentlyimproves
thelinearevaluationperformanceformethodsthatareknowntobesusceptibletodimensionalcol-
lapse,includingsample-contrastivemethodSimCLRandgenerativemethodMAE.Italsoimproves
BYOL, which is susceptible to local dimensional collapse as shown in Figure 2d. Tables 2 and 3
further demonstrate that LDReg can also improve the performance of transfer learning, finetuning
8
sDIL
fo naeM
cirtemoeG
0 02 04 06 08 001
sDIL
fo naeM
cirtemoeG
0 02 04 06 08 001
knaR
evitceffE
sDIL
fo naeM
cirtemoeGPublishedasaconferencepaperatICLR2024
Table1: Thelinearevaluationresults(accuracy(%))ofdifferentmethodswithandwithoutLDReg.
TheeffectiverankiscalculatedontheImageNetvalidationset. Thebestresultsareboldfaced.
Geometricmean
Model Epochs Method Regularization LinearEvaluation EffectiveRank
ofLID
- 64.3 470.2 18.8
SimCLR
LDReg 64.8 529.6 20.0
SimCLR - 67.2 525.8 24.9
ResNet-50 100
(Tuned) LDReg 67.5 561.7 26.1
- 67.6 583.8 15.9
BYOL
LDReg 68.5 594.0 22.3
- 72.9 283.7 13.3
SimCLR
LDReg 73.0 326.1 13.7
ViT-B 200
- 57.0 86.4 25.8
MAE
LDReg 57.6 154.1 29.8
Table2: Thetransferlearningresultsintermsoflinearprobingaccuracy(%), usingResNet-50as
theencoder. Thebestresultsareboldfaced.
Method Regularization BatchSize Epochs ImageNet Food-101 CIFAR-10 CIFAR-100 Birdsnap Cars DTD
- 64.3 69.0 89.1 71.2 32.0 36.7 67.8
2048 100
LDReg 64.8 69.1 89.2 70.6 33.4 37.3 67.7
SimCLR
- 69.0 71.1 90.1 71.6 37.5 35.3 70.7
4096 1000
LDReg 69.8 73.3 91.8 75.1 38.7 41.6 70.8
Table 3: The performance of the pre-trained models (ResNet-50) on object detection and instance
segmentationtasks,whenfine-tunedonCOCO.Thebounding-box(APbb)andmask(APmk)average
precisionarereportedwiththebestresultsareboldfaced.
ObjectDetection Segmentation
Method Regularization Epochs BatchSize
APbb APbb APbb APmk APmk APmk
50 75 50 75
- 35.24 55.05 37.88 31.30 51.70 32.82
SimCLR
LDReg 35.26 55.10 37.78 31.38 51.88 32.90
100 2048
- 36.30 55.64 38.82 32.17 52.53 34.30
BYOL
LDReg 36.82 56.47 39.62 32.47 53.15 34.60
- 36.48 56.22 39.28 32.12 52.70 34.02
SimCLR 1000 4096
LDReg 37.15 57.20 39.82 32.82 53.81 34.74
onobjectdetectionandsegmentationdatasets. Moreover,longerpretrainingwithLDRegcanbring
moresignificantperformanceimprovement. TheseresultsindicatethatusingLDRegtoregularize
localdimensionalitycanconsistentlyimprovetherepresentationquality.
Table 1 also indicates that the effective rank is a good indicator of representation quality for the
sametypeofSSLmethodsandthesamemodelarchitecture. However,thecorrelationbecomesless
consistentwhencomparedacrossdifferentmethods.Forexample,SimCLR+LDRegandSimCLR-
Tuned have similar effective ranks (∼ 525), yet perform quite differently on ImageNet (with an
accuracy difference of 2.4%). Nevertheless, applying our LDReg regularization can improve both
typesofSSLmethods.
7 CONCLUSION
Inthispaper,wehavehighlightedthatdimensionalcollapseinself-supervisedlearning(SSL)could
occur locally, in the vicinity of any training point. Based on a novel derivation of an asymptotic
variantoftheFisher-Raometric,wepresentedalocaldimensionalityregularizationmethodLDReg
toalleviatedimensionalcollapsefrombothglobalandlocalperspectives. Ourtheoreticalanalysis
implies that reporting and averaging intrinsic dimensionality (ID) should be done at a logarithmic
(ratherthanlinear)scale,usingthegeometricmean(butnotthearithmeticorharmonicmean). Fol-
lowingthesetheoreticalinsights,LDRegregularizestherepresentationspaceofSSLtohavenonuni-
formlocalnearest-neighbordistancedistributions,maximizingthelogarithmofthegeometricmean
ofthesample-wiseLIDs.WeempiricallydemonstratedtheeffectivenessofLDReginimprovingthe
representationqualityandfinalperformanceofSSL.WebelieveLDRegcanpotentiallybeapplied
asagenericregularizationtechniquetohelpotherSSLmethods.
9PublishedasaconferencepaperatICLR2024
REPRODUCIBILITY STATEMENT
DetailsofallhyperparametersandexperimentalsettingsaregiveninAppendixF.Pseudocodefor
LDRegandLIDestimationcanbefoundinAppendixJ.Asummaryoftheimplementationisavail-
ableinAppendixI.Weprovidesourcecodeforreproducingtheexperimentsinthispaper,whichcan
beaccessedhere: https://github.com/HanxunH/LDReg. Wealsodiscusscomputationallimitations
ofLIDestimationinAppendixH.
ACKNOWLEDGMENTS
Xingjun Ma is in part supported by the National Key R&D Program of China (Grant No.
2021ZD0112804) and the Science and Technology Commission of Shanghai Municipality (Grant
No. 22511106102). SarahErfaniisinpartsupportedbyAustralianResearchCouncil(ARC)Dis-
coveryEarlyCareerResearcherAward(DECRA)DE220100680.
REFERENCES
Laurent Amsaleg, Oussama Chelly, Teddy Furon, Ste´phane Girard, Michael E Houle, Ken-ichi
Kawarabayashi, andMichaelNett. Extreme-value-theoreticestimationoflocalintrinsicdimen-
sionality. DataMiningandKnowledgeDiscovery,2018.
Alessio Ansuini, Alessandro Laio, Jakob H Macke, and Davide Zoccolan. Intrinsic dimension of
datarepresentationsindeepneuralnetworks. InNeurIPS,2019.
JamesBailey, MichaelEHoule, andXingjunMa. Localintrinsicdimensionality, entropyandsta-
tisticaldivergences. Entropy,24(9):1220,2022.
HangboBao,LiDong,SonghaoPiao,andFuruWei. BEit: BERTpre-trainingofimagetransform-
ers. InICLR,2022.
AdrienBardes,JeanPonce,andYannLeCun. VICReg: Variance-invariance-covarianceregulariza-
tionforself-supervisedlearning. InICLR,2022.
ThomasBerg,JiongxinLiu,SeungWooLee,MichelleLAlexander,DavidWJacobs,andPeterN
Belhumeur. Birdsnap: Large-scalefine-grainedvisualcategorizationofbirds. InCVPR,2014.
LukasBossard,MatthieuGuillaumin,andLucVanGool. Food-101–miningdiscriminativecompo-
nentswithrandomforests. InECCV,2014.
Jo¨rgBruskeandGeraldSommer. Intrinsicdimensionalityestimationwithoptimallytopologypre-
servingmaps. TPAMI,1998.
MathildeCaron,PiotrBojanowski,ArmandJoulin,andMatthijsDouze. Deepclusteringforunsu-
pervisedlearningofvisualfeatures. InECCV,2018.
Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin.
Unsupervisedlearningofvisualfeaturesbycontrastingclusterassignments. NeurIPS,2020.
Mathilde Caron, Hugo Touvron, Ishan Misra, Herve´ Je´gou, Julien Mairal, Piotr Bojanowski, and
ArmandJoulin. Emergingpropertiesinself-supervisedvisiontransformers. InICCV,2021.
Kevin M Carter, Raviv Raich, and AO Hero. Learning on statistical manifolds for clustering and
visualization. InAllertonConferenceonCommunication,Control,andComputing,2007.
ClaudioCeruti,SimoneBassis,AlessandroRozza,GabrieleLombardi,ElenaCasiraghi,andPaola
Campadelli. Danco: Anintrinsicdimensionalityestimatorexploitingangleandnormconcentra-
tion. Patternrecognition,47(8),2014.
TingChen,SimonKornblith,MohammadNorouzi,andGeoffreyHinton. Asimpleframeworkfor
contrastivelearningofvisualrepresentations. InICML,2020a.
10PublishedasaconferencepaperatICLR2024
Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E Hinton. Big
self-supervisedmodelsarestrongsemi-supervisedlearners. NeurIPS,2020b.
XinleiChenandKaimingHe. Exploringsimplesiameserepresentationlearning. InCVPR,2021.
XinleiChen, SainingXie, andKaiming He. Anempirical studyof trainingself-supervised vision
transformers. InICCV,2021.
Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. De-
scribingtexturesinthewild. InCVPR,2014.
RomainCosentino,AnirvanSengupta,SalmanAvestimehr,MahdiSoltanolkotabi,AntonioOrtega,
Ted Willke, and Mariano Tepper. Toward a geometrical understanding of self-supervised con-
trastivelearning. arXivpreprintarXiv:2205.06926,2022.
MarcoDelGiudice. Effectivedimensionality: Atutorial. Multivariatebehavioralresearch,56(3):
527–542,2021.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale
hierarchicalimagedatabase. InCVPR,2009.
Hadi Mohaghegh Dolatabadi, Sarah Erfani, and Christopher Leckie. Collider: A robust training
frameworkforbackdoordata. InACCV,2022.
AlexeyDosovitskiy,LucasBeyer,AlexanderKolesnikov,DirkWeissenborn,XiaohuaZhai,Thomas
Unterthiner,MostafaDehghani,MatthiasMinderer,GeorgHeigold,SylvainGelly,JakobUszko-
reit,andNeilHoulsby. Animageisworth16x16words: Transformersforimagerecognitionat
scale. InICLR,2021.
YannDubois,StefanoErmon,TatsunoriBHashimoto,andPercySLiang.Improvingself-supervised
learningbycharacterizingidealizedrepresentations. NeurIPS,2022.
DebidattaDwibedi,YusufAytar,JonathanTompson,PierreSermanet,andAndrewZisserman.With
alittlehelpfrommyfriends: Nearest-neighborcontrastivelearningofvisualrepresentations. In
ICCV,2021.
Aleksandr Ermolov, Aliaksandr Siarohin, Enver Sangineto, and Nicu Sebe. Whitening for self-
supervisedrepresentationlearning. InICML,2021.
Quentin Garrido, Randall Balestriero, Laurent Najman, and Yann Lecun. Rankme: Assessing the
downstream performance of pretrained self-supervised representations by their rank. In ICML,
2023a.
Quentin Garrido, Yubei Chen, Adrien Bardes, Laurent Najman, and Yann LeCun. On the duality
betweencontrastiveandnon-contrastiveself-supervisedlearning. InICLR,2023b.
ChongjianGe,JiangliuWang,ZhanTong,ShoufaChen,YibingSong,andPingLuo.Softneighbors
arepositivesupportersincontrastivevisualrepresentationlearning. InICLR,2023.
RossGirshick,JeffDonahue,TrevorDarrell,andJitendraMalik. Richfeaturehierarchiesforaccu-
rateobjectdetectionandsemanticsegmentation. InCVPR,2014.
Sixue Gong, Vishnu Naresh Boddeti, and Anil K Jain. On the intrinsic dimensionality of image
representations. InCVPR,2019.
Jean-Bastien Grill, Florian Strub, Florent Altche´, Corentin Tallec, Pierre Richemond, Elena
Buchatskaya, CarlDoersch,BernardoAvilaPires,ZhaohanGuo,MohammadGheshlaghiAzar,
etal. Bootstrapyourownlatent-anewapproachtoself-supervisedlearning. NeurIPS,2020.
Bobby He and Mete Ozay. Exploring the gap between collapsed & whitened features in self-
supervisedlearning. InICML,2022.
KaimingHe,XiangyuZhang,ShaoqingRen,andJianSun. Deepresiduallearningforimagerecog-
nition. InCVPR,2016.
11PublishedasaconferencepaperatICLR2024
Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for
unsupervisedvisualrepresentationlearning. InCVPR,2020.
Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dolla´r, and Ross Girshick. Masked
autoencodersarescalablevisionlearners. InCVPR,2022.
GeoffreyEHintonandRichardZemel. Autoencoders,minimumdescriptionlengthandhelmholtz
freeenergy. NeurIPS,1993.
MichaelEHoule. LocalintrinsicdimensionalityI:anextreme-value-theoreticfoundationforsimi-
larityapplications. InSISAP,2017a.
MichaelEHoule. LocalintrinsicdimensionalityII:multivariateanalysisanddistributionalsupport.
InSISAP,2017b.
Michael E Houle, Xiguo Ma, Michael Nett, and Vincent Oria. Dimensional testing for multi-step
similaritysearch. InICDM,2012.
Tianyu Hua, Wenxiao Wang, Zihui Xue, Sucheng Ren, Yue Wang, and Hang Zhao. On feature
decorrelationinself-supervisedlearning. InICCV,2021.
WeiranHuang, MingyangYi, XuyangZhao, andZihaoJiang. Towardsthegeneralizationofcon-
trastiveself-supervisedlearning. InICLR,2023.
LiJing,PascalVincent,YannLeCun,andYuandongTian. Understandingdimensionalcollapsein
contrastiveself-supervisedlearning. InICLR,2022.
Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. Bert: Pre-training of deep
bidirectionaltransformersforlanguageunderstanding. InNAACL-HLT,2019.
JonathanKrause,JiaDeng,MichaelStark,andLiFei-Fei. Collectingalarge-scaledatasetoffine-
grainedcars. 2013.
AlexKrizhevskyandGeoffreyHinton. Learningmultiplelayersoffeaturesfromtinyimages. 2009.
ElizavetaLevinaandPeterBickel.Maximumlikelihoodestimationofintrinsicdimension.NeurIPS,
2004.
Alexander C Li, Alexei A Efros, and Deepak Pathak. Understanding collapse in non-contrastive
siameserepresentationlearning. InECCV,2022.
Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr
Dolla´r,andCLawrenceZitnick. Microsoftcoco: Commonobjectsincontext. InECCV,2014.
Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv
preprintarXiv:1608.03983,2016.
IlyaLoshchilovandFrankHutter. Decoupledweightdecayregularization. InICLR,2019.
Xingjun Ma, Bo Li, Yisen Wang, Sarah M. Erfani, Sudanthi Wijewickrema, Grant Schoenebeck,
Michael E. Houle, Dawn Song, and James Bailey. Characterizing adversarial subspaces using
localintrinsicdimensionality. InICLR,2018a.
XingjunMa,YisenWang,MichaelEHoule,ShuoZhou,SarahErfani,ShutaoXia,SudanthiWijew-
ickrema,andJamesBailey. Dimensionality-drivenlearningwithnoisylabels. InICML,2018b.
AaronvandenOord,YazheLi,andOriolVinyals. Representationlearningwithcontrastivepredic-
tivecoding. arXivpreprintarXiv:1807.03748,2018.
BoPang,YifanZhang,YaoyiLi,JiaCai,andCewuLu.Unsupervisedvisualrepresentationlearning
bysynchronousmomentumgrouping. InECCV,2022.
Karl W Pettis, Thomas A Bailey, Anil K Jain, and Richard C Dubes. An intrinsic dimensionality
estimatorfromnear-neighborinformation. TPAMI,1979.
12PublishedasaconferencepaperatICLR2024
Phil Pope, Chen Zhu, Ahmed Abdelkader, Micah Goldblum, and Tom Goldstein. The intrinsic
dimensionofimagesanditsimpactonlearning. InICLR,2021.
SimoneRomano,OussamaChelly,VinhNguyen,JamesBailey,andMichaelEHoule. Measuring
dependencyviaintrinsicdimensionality. InICPR,2016.
Olivier Roy and Martin Vetterli. The effective rank: A measure of effective dimensionality. In
Europeansignalprocessingconference,2007.
StephenTaylor.Clusteringfinancialreturndistributionsusingthefisherinformationmetric.Entropy,
21(2):110,2019.
YuandongTian,XinleiChen,andSuryaGanguli. Understandingself-supervisedlearningdynamics
withoutcontrastivepairs. InICML,2021.
Leo To¨rnqvist, Pentti Vartia, and Yrjo¨ O Vartia. How should relative changes be measured? The
AmericanStatistician,39(1):43–46,1985.
JuliusVonKu¨gelgen, YashSharma, LuigiGresele, WielandBrendel, BernhardScho¨lkopf, Michel
Besserve, and Francesco Locatello. Self-supervised learning with data augmentations provably
isolatescontentfromstyle. NeurIPS,2021.
DianeWagner, FabioFerreira, DannyStoll, RobinTiborSchirrmeister, SamuelMu¨ller, andFrank
Hutter.Ontheimportanceofhyperparametersanddataaugmentationforself-supervisedlearning.
InFirstWorkshoponPre-training: Perspectives,Pitfalls,andPathsForwardatICML,2022.
ZhaoqingWang,ZiyuChen,YaqianLi,YandongGuo,JunYu,MingmingGong,andTongliangLiu.
Mosaicrepresentationlearningforself-supervisedvisualpre-training. InICLR,2023.
ZhendaXie,ZhengZhang,YueCao,YutongLin,JianminBao,ZhuliangYao,QiDai,andHanHu.
Simmim: Asimpleframeworkformaskedimagemodeling. InCVPR,2022.
Chun-Hsiao Yeh, Cheng-Yao Hong, Yen-Chi Hsu, Tyng-Luh Liu, Yubei Chen, and Yann LeCun.
Decoupledcontrastivelearning. InECCV,2022.
YangYou,IgorGitman,andBorisGinsburg. Largebatchtrainingofconvolutionalnetworks. arXiv
preprintarXiv:1708.03888,2017.
JureZbontar,LiJing,IshanMisra,YannLeCun,andSte´phaneDeny. Barlowtwins:Self-supervised
learningviaredundancyreduction. InICML,2021.
QiZhang,YifeiWang,andYisenWang. Howmaskmatters: Towardstheoreticalunderstandingsof
maskedautoencoders. InNeurIPS,2022.
Shaofeng Zhang, Feng Zhu, Junchi Yan, Rui Zhao, and Xiaokang Yang. Zero-cl: Instance and
featuredecorrelationfornegative-freesymmetriccontrastivelearning. InICLR,2021.
ZhijianZhuo,YifeiWang,JinwenMa,andYisenWang.Towardsaunifiedtheoreticalunderstanding
ofnon-contrastivelearningviarankdifferentialmechanism. InICLR,2023.
13PublishedasaconferencepaperatICLR2024
A ACHIEVING DESIRED LOCAL DIMENSIONALITY WITH LDREG
In this section, we provide details regarding how Figure 1c is obtained. Following our theory,
LDReg can obtain representations that have a desired local dimensionality. We use a linear layer
andrandomlygeneratedsyntheticdatapointsin2Dfollowingtheuniformdistribution. Thelinear
layertransformsthesepointsintoarepresentationspace. FollowingDefinition3,onemightspecify
thedesiredlocalintrinsicdimensionsasLID∗. ThedimensionoftherepresentationsisLID∗. To
G F
achievethedesiredlocaldimensionality,weminimizethefollowingobjective:
(cid:32)
1
(cid:88)N LID∗
Fi
(cid:33)
min ln w .
N LID∗
i G
ThisobjectivecorrespondstominimizingtheasymptoticFisher-Raodistancebetweenthedistribu-
tion of the representations and a target distribution of fixed local dimensionality. In Figure 3, we
plotted the results with target dimensions LID∗ equal to [1.0, 1.2, 1.4, 1.6, 1.8, 2.0]. The results
G
showthattheestimatedLIDisveryclosetothedesiredvalues. ThisindicatesthatLDRegcanalso
be used to regularize the representations to a specific value. From this point of view, LDReg can
potentiallybeappliedtootherlearningtasksasagenericrepresentationregularizationtechnique.
mLID=1.1 GID=1.9 mLID=1.3 GID=1.9 mLID=1.5 GID=2.0
2.5 4.0 4.5
2.5 2.5 4.0
3.5 4.0
2.0
2.0 2.0
3.0 3.5 3.5
1.5
1.5 1.5
3.0 3.0
1.0 2.5 1.0 1.0
2.5 2.5
0.5 2.0 0.5 0.5
2.0 2.0
0.0 1.5 0.0 0.0
1.5 1.5
0.5 1.0 0.5 0.5
1.0
1.0
0.5 0.0 0.5 1.0 1.5 2.0 2.5 0.5 0.0 0.5 1.0 1.5 2.0 2.5 0.5 0.0 0.5 1.0 1.5 2.0 2.5
(a)LID∗ =1.0 (b)LID∗ =1.2 (c)LID∗ =1.4
G G G
mLID=1.7 GID=1.9 mLID=1.9 GID=2.0 LID=2.1 GID=2.0
3.0
3.0
2.5 3.5 2.5 3.5 3 3.5
2.0 3.0 2.0 3.0 2 3.0
1.5 1.5
1.0 2.5 1.0 2.5 1 2.5
0.5 0.5
2.0 2.0 0
0.0 0.0 2.0
0.5 1.5 0.5 1.5 1
1.0 1.5
0.5 0.0 0.5 1.0 1.5 2.0 2.5 1.0 0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 1 0 1 2 3
(d)LID∗ =1.6 (e)LID∗ =1.8 (f)LID∗ =2.0
G G G
Figure 3: Each caption of the subfigures shows the desired local dimensionality and each title of
thesubfiguresshowstheestimatedLIDandglobalintrinsicdimensionality(GID).GIDisestimated
usingtheDanCOapproach(Cerutietal.,2014). mLIDisthegeometricmeanofestimatedsample
LIDs.
InthecontextofSSL,toavoiddimensionalcollapse,theresultingrepresentationshouldspan(fill)
the entire space, as shown in Figure 3f. The L - and L -regularization terms used for SSL aim
1 2
to maximize the asymptotic Fisher-Rao distance between local distance distribution F(r) and a
uniformdistancedistributionU (r)(whichhasLIDequalto1,asshowninFigure3a). Inother
1,w
words, the final representations should be ‘further’ from that of Figure 3a, and ‘closer’ to that of
Figure3f.
14
DIL
DIL
DIL
DIL
DIL
DILPublishedasaconferencepaperatICLR2024
B EFFECTIVE RANK VERSUS LOCAL INTRINSIC DIMENSIONALITY
Zhuo et al. (2023) proposed the effective rank (Roy & Vetterli, 2007) as a metric to evaluate the
degreeof(global)dimensionalcollapse. Giventhefeaturecorrelationmatrix,theeffectiverankcor-
respondstotheexponentialoftheentropyofthenormalizedeigenvalues. Itisinvarianttoscaling
andtakesrealvalues,notjustintegers. Ithasamaximumvalueequaltotherepresentationdimen-
sion. Intuitively, the effective rank assesses the degree to which the data ‘fills’ the representation
space,intermsofcovarianceproperties. Onemightalsodefinealocaleffectiverank,whichwould
assessthedegreetowhichtherepresentationspacesurroundingaparticularanchorsampleisfilled.
In contrast to the effective rank, the local intrinsic dimension (LID) is a local measure. Roughly
speaking,ataparticularanchorsample,theLIDassessesthegrowthrateofthedistancedistribution
ofnearestneighbors. Itisthus focusedondistanceproperties(distances betweensamples), rather
than covariance properties between features. Comparisons between intrinsic dimensionality and
effectiverank,emphasizingtheirdifferences,havebeendiscussedinDelGiudice(2021).
C PROOFS
C.1 BACKGROUND
Theorem 3 (LID Representation Theorem (Houle, 2017a)). Let F : R → R be a real-valued
function, and assume that LID∗ exists. Let r and w be values for which r/w and F(r)/F(w)
F
are both positive. If F is non-zero and continuously differentiable everywhere in the interval
[min{r,w},max{r,w}],then
F(r) (cid:16)r(cid:17)LID∗ (cid:18)(cid:90) w LID∗ −LID (t) (cid:19)
= F ·A (r,w), where A (r,w) ≜ exp F F dt ,
F(w) w F F t
r
whenevertheintegralexists.
Corollary 3.1 ((Houle, 2017a)). Let c and w be real constants such that c ≥ 1 and w > 0. Let
F : R → R be a real-valued function satisfying the conditions of Theorem 3 over the interval
(0,cw]. Then
lim A (r,w)=1.
F
w→0+
w/c≤r≤cw
IfF(0) = 0,andF isnon-decreasingandcontinuouslydifferentiableoversomeinterval[0,w]for
w >0,thenF isreferredtoasasmoothgrowthfunction. IfF isaCDF,weusethenotationF to
w
refertoF conditionedover[0,w];thatis,F (r)=F(r)/F(w).
w
C.2 PROOFOFLEMMA1
Proof. We leverage a result from (Taylor, 2019), which shows that the Fisher-Rao metric be-
tween two one-dimensional distributions with a single parameter can be expressed as d(θ ,θ ) =
1 2
|(cid:82)θ2u(θ)dθ|, where u(θ)2 = I(θ), and I(θ) is the Fisher information with respect to the single
θ1
parameter θ. In our context, θ corresponds to the local intrinsic dimensionality. For the Fisher
informationforfunctionsrestrictedtotheformH ,wewillthereforeusethequantity
w|θ
(cid:90) w(cid:18) ∂ (cid:19)2
I (θ)= lnH′ (r) H′ (r)dr.
w ∂θ w|θ w|θ
0
15PublishedasaconferencepaperatICLR2024
We now derive an expression for the Fisher-Rao distance. From (Taylor, 2019), and noting that
Fisherinformationisgreaterthanorequaltozero,
(cid:12) (cid:12)
(cid:12)(cid:90) θ2(cid:112) (cid:12)
d (H ,H ) = (cid:12) I (θ)dθ(cid:12)
FR w|θ1 w|θ2 (cid:12) w (cid:12)
(cid:12) θ1 (cid:12)
(cid:12) (cid:12) (cid:12)(cid:90) θ2(cid:32) (cid:90) w(cid:18) ∂ (cid:18) θ (cid:16)r(cid:17)θ−1(cid:19)(cid:19)2 θ (cid:16)r(cid:17)θ−1 (cid:33) 21 (cid:12) (cid:12) (cid:12)
= (cid:12) ln dr dθ(cid:12)
(cid:12) ∂θ w w w w (cid:12)
(cid:12) θ1 0 (cid:12)
(cid:12) (cid:12) (cid:12)(cid:90) θ2(cid:32) (cid:90) w(cid:18) 1 r(cid:19)2 θ (cid:16)r(cid:17)θ−1 (cid:33) 21 (cid:12) (cid:12) (cid:12)
= (cid:12) +ln dr dθ(cid:12) .
(cid:12) θ w w w (cid:12)
(cid:12) θ1 0 (cid:12)
Withthesubstitutionv
=(cid:0)r(cid:1)θ
,weobtain
w
(cid:12) (cid:12) (cid:12)(cid:90) θ2(cid:32) (cid:90) 1(cid:18) 1 1 (cid:19)2 (cid:33) 21 (cid:12) (cid:12) (cid:12)
d FR(H w|θ1,H w|θ2) = (cid:12)
(cid:12) θ
+
θ
lnv dv dθ(cid:12)
(cid:12)
(cid:12) θ1 0 (cid:12)
(cid:12) (cid:12)
= (cid:12) (cid:12) (cid:12)(cid:90) θ2 1 θ(cid:113) (cid:2) v+2(vlnv−v)+(vln2v−2vlnv+2v)(cid:3)(cid:12) (cid:12)1 0dθ(cid:12) (cid:12)
(cid:12)
(cid:12) θ1 (cid:12)
= (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)(cid:90) θ1θ2 1 θ dθ(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) = (cid:12) (cid:12) (cid:12) (cid:12)lnθ θ2 1(cid:12) (cid:12) (cid:12) (cid:12) .
C.3 PROOFOFTHEOREM2
Proof. WefindthedistributionH thatminimizestheexpression
w|θ
N
θ = argmin
1 (cid:88)(cid:0)
d (H
,Fi)(cid:1)2
,
G N AFR w|θ
θ
i=1
by taking the partial derivative with respect to θ, and solving for the value θ = θ for which the
G
partialderivativeiszero.
∂∂ θ (cid:32) N1 (cid:88) i=N 1(cid:12) (cid:12) (cid:12) (cid:12)ln LIDθ ∗ Fi(cid:12) (cid:12) (cid:12) (cid:12)2(cid:33)(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
θ=θG
= 0
(cid:12)
∂ (cid:88)N (cid:0) ln2θ+ln2LID∗ −2lnθlnLID∗ (cid:1)(cid:12) (cid:12) = 0
∂θ Fi Fi (cid:12)
(cid:12)
i=1 θ=θG
(cid:88)N (cid:18) 2lnθ
G
−2lnLID∗ Fi(cid:19)
= 0
θ θ
G G
i=1
N
(cid:88)
Nlnθ = lnLID∗
G Fi
i=1
(cid:32) N (cid:33)
1 (cid:88)
θ = exp lnLID∗ .
G N Fi
i=1
In a similar fashion, the second partial derivative can be shown to be strictly positive at θ = θ .
1
Since the original expression is continuous and non-negative over all θ ∈ [0,∞), θ is a global
1
minimum.
16PublishedasaconferencepaperatICLR2024
C.4 PROOFOFCOLLARY2.1
Proof. Assertion1followsfromthefactthat
N1 (cid:88)N d AFR(Fi,U 1) = N1 (cid:88)N (cid:12) (cid:12) (cid:12) (cid:12)lnLID 1∗ Fi(cid:12) (cid:12) (cid:12)
(cid:12)
i=1 i=1
N
1 (cid:88)
= lnLID∗ (ifLID∗ ≥1).
N Fi Fi
i=1
C.5 KULLBACK-LEIBLERDIVERGENCEASDISTRIBUTIONALDIVERGENCE
Itisnaturaltoconsiderwhetherothermeasuresofdistributionaldivergencecouldbeusedinplace
oftheasymptoticFisher-Raometric,inthederivationoftheFre´chetmean. Baileyetal.(2022)have
shown several other divergences and distances which, when conditioned to a vanishing lower tail,
tend to expressions involving the local intrinsic dimensionalities of distance distributions — most
notablythatoftheKullback-Leibler(KL)divergence. Here,wedefineanasymptoticdistributional
distancefromthesquarerootoftheasymptoticKLdivergenceconsideredin(Baileyetal.,2022).
Lemma2((Baileyetal.,2022)). Giventwosmooth-growthdistancedistributionswithCDFsF and
G,theirasymptoticKLdistanceisgivenby
(cid:115)
(cid:112)
LID∗ LID∗
d (F,G) ≜ lim D (F ,G ) = G −ln G −1,
AKL
w→0+
KL w w LID∗
F
LID∗
F
where
(cid:90) w F′(t)
D (F ,G ) ≜ F′(t)ln w dt
KL w w w G′ (t)
0 w
istheKLdivergencefromF toGwhenconditionedtothelowertail[0,w].
When aggregating the LID values of distance distributions, it is worth considering how well the
arithmeticmean(equivalenttotheinformationdimension(Romanoetal.,2016))andtheharmonic
meanmightserveasalternativestothegeometricmean. Theorem4showsthatthearithmeticand
harmonic means of distributional LIDs are obtained when the asymptotic Fisher-Rao metric is re-
placedbytheasymptoticKLdistance,inthederivationoftheFre´chetmean.
Theorem 4. Given a set of distributions F = {F1,F2,...,FN}, consider the metric used in
computingtheFre´chetmeanµ =H .
F w|θ
1. UsingtheasymptoticFisher-Raometricd (H ,Fi)asinDefinition4givesθ equal
AFR w|θ
tothegeometricmeanof{LID∗ ,...,LID∗ }.
F1 FN
w w
2. Replacingd (H ,Fi)bytheasymptoticKLdistanced (H ,Fi)givesθ equal
AFR w|θ AKL w|θ
tothearithmeticmeanof{LID∗ ,...,LID∗ }.
F1 FN
3. Replacingd (H ,Fi)bythe(reverse)asymptoticKLdistanced (Fi,H )gives
AFR w|θ AKL w|θ
θequaltotheharmonicmeanof{LID∗ ,...,LID∗ }.
F1 FN
Proof. Assertion1hasbeenshowninTheorem2.
ForAssertion2,wefindthevalueofθforwhichthefollowingexpressionisminimized:
N
θ =argmin
1 (cid:88)(cid:0)
d (H
,Fi)(cid:1)2
.
A N AKL w|θ
θ
i
UsingLemma2,andobservingthatLID∗ =θ,
Hw|θ
(cid:0) d (H ,Fi)(cid:1)2 = lim D (H ,Fi) =
LID∗
Fi
−lnLID∗
Fi −1.
AKL w|θ w→0 KL w|θ θ θ
17PublishedasaconferencepaperatICLR2024
AsintheproofofTheorem2,theminimizationisaccomplishedbysettingthepartialderivativeto
zeroandsolvingforθ =θ .
A
∂ (cid:32) 1 (cid:88)N (cid:18) LID∗ LID∗ (cid:19)(cid:33)(cid:12) (cid:12)
Fi −ln Fi −1 (cid:12) = 0
∂θ N θ θ (cid:12)
(cid:12)
i=1 θ=θA
1 (cid:88)N (cid:18) LID∗ 1 (cid:19)
− Fi + = 0
N θ2 θ
i=1 A A
N
1 (cid:88)
θ = LID∗ .
A N Fi
i=1
ForAssertion3,wesimilarlyfindthevalueofθforwhichthefollowingexpressionisminimized:
N
θ = argmin 1 (cid:88)(cid:0) d (Fi,H )(cid:1)2 .
H N AKL w|θ
θ
i
Onceagain,wetakethepartialderivativewithrespecttoθ,setittozero,andsolveforθ =θ :
H
∂ (cid:32) 1 (cid:88)N (cid:18) θ θ (cid:19)(cid:33)(cid:12) (cid:12)
−ln −1 (cid:12) = 0
∂θ N LID∗ LID∗ (cid:12)
i=1 Fi Fi (cid:12) θ=θH
N (cid:18) (cid:19)
1 (cid:88) 1 1
− = 0
N LID∗ θ
i=1 Fi H
N
θ = .
H (cid:80)N 1
i=1 LID∗
Fi
Note that θ and θ can be verified as minima by computing the second partial derivatives with
G H
respecttoθ.
ThesquarerootoftheKLdivergenceisonlya‘weakapproximation’oftheFisher-Raometricon
statistical manifolds, and this approximation is known to degrade as distributions diverge (Carter
et al., 2007). Moreover, the KL divergence is also nonsymmetric. Theorem 4 therefore indicates
that the asymptotic Fisher-Rao metric is preferable (in theory) to the asymptotic KL distance, and
thegeometricmeanispreferabletothearithmeticmeanandharmonicmeanwhenaggregatingthe
LIDvaluesofdistancedistributions.
D OTHER REGULARIZATION FORMULATIONS
Weelaborateonourchoicesofregularizationterm.
Remark 4.1. The proposed regularization is equivalent to maximizing the (log of the) geometric
meanoftheIDsofthesamples.
Theorem4providedargumentsforwhyuseofgeometricmeanispreferabletoothermeansforthe
purpose of computing the Frechet mean of a set of distributions. We can similarly consider why
a regularization corresponding to the arithmetic mean of the LIDs of the samples would be less
preferable. i.e. max 1 (cid:80)NLID∗ . Notethat
N i F wi
N N
1 (cid:88) 1 (cid:88) (cid:16) (cid:17)
max LID∗ = max exp lim d (Fi(r),U (r)) .
N F wi N w→0 FR w 1,w
i i
Observethatthisarithmeticmeanregularization,duetotheexponentialtransformation,wouldapply
ahighweightingtosampleswithverylargedistancesfromtheuniformdistribution(thatis,samples
withlargeID).Inotherwords,sucharegularizationobjectivecouldbeoptimizedbymakingtheID
ofasmallnumberofsamplesextremelylarge.
18PublishedasaconferencepaperatICLR2024
E LDREG AND SSL METHODS
Inthissection, weprovidemoredetailsonhowtoapplyLDRegondifferentSSLmethods. Since
LDRegisappliedtotherepresentationobtainedbytheencoder,thevaryingcombinationsofprojec-
tor,predictor,decoder,andoptimizingobjectiveusedbySSLmethodsdoesnotdirectlyaffecthow
LDRegisapplied. Asaresult,LDRegcanberegardedasageneralregularizationforSSL.
SimCLR.ForinputimagesxofbatchsizeN,anencoderf(·),andaprojectorg(·),therepresenta-
tionsareobtainedbyz =f(x),andtheembeddingsaree=g(z). Givenabatchof2N augmented
inputs,theNT-XentlossusedbySimCLR(Chenetal.,2020a)forapositivepairofinputs(x ,x )
i j
is:
exp(sim(e ,e )/τ)
LNTXent =−ln i j ,
i (cid:80)2N
exp(sim(e ,e )/τ)
m̸=j i m
whereτ isthetemperature,andthefinallossiscomputedacrossallpositivepairs.
ForapplyingLDRegwithL termonSimCLR,weoptimizethefollowingobjective:
L1
2N
1 (cid:88)
L =LNTXent−β lnLID∗ ,
L1 2N F wi
i
where the LID for each sample is estimated using the method of moments: LID∗ = − µk ,
F wi µk−wk
whereµ istheaverageddistancetotheknearestneighborsofz ,andw isthedistancetothek-th
k i k
nearestneighborofz .
i
BYOL.BYOL(Grilletal.,2020)usesanadditionalpredictorh(·)toobtainpredictionsp = h(e),
amomentumencoder(exponentialmovingaverageoftheweightsoftheonlinemodel)wheree′ is
obtained,andthelossfunctionisthescaledcosinesimilaritybetweenpositivepairs,definedas:
p ·e′
LBYOL =2−2 i j ,
i ∥p ∥ ∥e′∥
i 2 j 2
withthefinallosscomputedsymmetricallyacrossallpositivepairs.
ForapplyingLDRegwithL termonBYOL,weoptimizethefollowingobjective:
L1
2N
1 (cid:88)
L =LBYOL−β lnLID∗ .
L1 2N F wi
i
TheLIDforeachsampleisestimatedinthesamewayasapplyingLDRegonSimCLR.ForBYOL,
weuserepresentationsobtainedbyboththeonlineandmomentumbranchesasthereferenceset.
MAE. MAE (He et al., 2022) uses a decoder that aims to reconstruct the input image. Unlike a
contrastiveapproach, itdoesnotrelyontwodifferentaugmentedviewsofthesameimage. MAE
usesanencoderf(·)toobtaintherepresentationz =f(x′)foramaskedimagex′,andthedecoder
t(·)aimstoreconstructtheoriginalimagexbytakingrepresentationrasinput. Specifically,MAE
optimizesthefollowingobjective:
LMAE =∥t(f(x′)),x ∥ .
i i i 2
ForapplyingLDRegwithL termonMAE,weoptimizethefollowingobjective:
L1
N
1 (cid:88)
L =LMAE−β lnLID∗ .
L1 N F wi
i
TheLIDforeachsampleisestimatedinthesamewayasapplyingLDRegonSimCLR.
F EXPERIMENTAL SETTINGS
For each baseline method, we follow their original settings, except in the case of BYOL, where
wechangedtheparameterfortheexponentialmovingaveragefrom0.996to0.99,whichperforms
19PublishedasaconferencepaperatICLR2024
betterwhenthenumberofepochsissetto100. Detailedhyperparametersettingscanbefoundin
Tables 5-11. We use 100 epochs of pretraining and a batch size of 2048 as defaults. For LDReg
regularization,weusek = 128asthedefaultneighborhoodsize. ForResNet-50,weuseβ = 0.01
forSimCLRandSimCLRTuned,β =0.005forBYOL.ForViT-B,weuseβ =0.001forSimCLR,
andβ = 5×10−6 forMAE.Weperformlinearevaluationsfollowingexistingworks(Chenetal.,
2020a;Grilletal.,2020;Heetal.,2022;Garridoetal.,2023b). Forlinearevaluations,weusebatch
size4096onImageNet—othersettingsareshowninTable9.
FollowingSimCLR(Chenetal.,2020a)andBYOL(Grilletal.,2020),weevaluatetransferlearning
performancebyperforminglinearevaluationsonotherdatasets,includingFood-101(Bossardetal.,
2014), CIFAR (Krizhevsky & Hinton, 2009), Birdsnap (Berg et al., 2014), Stanford Cars (Krause
et al., 2013), and DTD (Cimpoi et al., 2014). Due to computational constraints, for the transfer
learning experiments, we did not perform full hyperparameter tuning for each model and dataset.
ThereproducedresultsofbaselinemethodsareslightlylowerthanthereportedresultsbyChenetal.
(2020a);Grilletal.(2020). Foralldatasets,weuse30epochs,weightdecayto0.0005,learningrate
0.01,batchsize256,andSGDwithNesterovmomentumasoptimizer. Thesesettingsarebasedon
theVISSLlibrary2.
We evaluate the finetuning performance with downstream tasks using the COCO dataset
(train2017 and val2017) (Lin et al., 2014). We use ResNet-50 with RCNN-C4 (Girshick
et al., 2014) with batch size 16 and base learning rate 0.02. We use the popular framework
detectron23,andourconfigurationsfollowtheMOCO-v1officialimplementation4exactly.
We conducted our experiments on Nvidia A100 GPUs with PyTorch implementation, with each
experimentdistributedacross4GPUs. Weusedautomaticmixedprecisionduetoitsmemoryeffi-
ciency.Theestimatedruntimeis40hoursforpretrainingandlinearevaluations.Ascanbeseenfrom
the pseudocode in Appendix J, the additional computation mainly depends on the calculation and
sortingofpairwisedistances. AsshowninTable4,weobservednosignificantadditionalcomputa-
tionalcostsforLDReg. Opensourcecodeisavailablehere: https://github.com/HanxunH/LDReg.
Table4:Wall-clockcomparisonsforpretrainingwithDistributedData-Paralleltraining.Eachexper-
imentuses4GPUsdistributedoverdifferentnodes. Resultsarebasedon100epochsofpretraining.
Communicationoverheadscouldhaveaslighteffectontheresults.
Method Wall-clocktime
SimCLR 27.8hours
SimCLR+LDReg 27.1hours
Table5: PretrainingsettingforSimCLR (Chenetal.,2020a).
Baselearningrate 0.075
√
Learningratescaling 0.075× BatchSize
Learningratedecay Cosine(Loshchilov&Hutter,2016)withoutrestart
WeightDecay 1.0×10−6
Optimizer LARS(Youetal.,2017)
TemperatureforLNTXent 0.1
Projector 2048-128
DataAugmentations. ForeachbaselineandLDRegversion,weusethesameaugmentationasin
existingworks. AugmentationpolicyforSimCLR(Chenetal.,2020a)isinTable10,forSimCLR-
Tuned(Garridoetal.,2023b)andBYOL(Grilletal.,2020)isinTable11.
2https://github.com/facebookresearch/vissl
3https://github.com/facebookresearch/detectron2
4https://github.com/facebookresearch/moco/tree/main/detection/configs
20PublishedasaconferencepaperatICLR2024
Table6: PretrainingsettingforSimCLR-Tuned(Garridoetal.,2023b).
Baselearningrate 0.5
Learningratescaling 0.5× BatchSize
256
Learningratedecay Cosine(Loshchilov&Hutter,2016)withoutrestart
WeightDecay 1.0×10−6
Optimizer LARS(Youetal.,2017)
TemperatureforLNTXent 0.15
Projector 8192-8192-512
Table7: PretrainingsettingforBYOL(Grilletal.,2020).
Baselearningrate 0.4
Learningratescaling 0.4× BatchSize
256
Learningratedecay Cosine(Loshchilov&Hutter,2016)withoutrestart
WeightDecay 1.5×10−6
Optimizer LARS(Youetal.,2017)
τ formovingaverage 0.99
Projector 4096-256
Predictor 4096-256
Table8: PretrainingsettingforMAE(Heetal.,2022).
Baselearningrate 1.5×10−4
Learningratescaling 1.5×10−4× BatchSize
256
Learningratedecay Cosine(Loshchilov&Hutter,2016)withoutrestart
WeightDecay 0.05
Optimizer AdamW(Loshchilov&Hutter,2019)
β fortheoptimizer 0.9
1
β fortheoptimizer 0.95
2
Table9: LinearevaluationsettingforImageNet.
Epochs 90
Baselearningrate 0.1
Learningratescaling 0.1× BatchSize
256
Minimallearningrate 1.0×10−6
Learningratedecay Cosine(Loshchilov&Hutter,2016)withoutrestart
WeightDecay 0
Optimizer LARS(Youetal.,2017)
Table10: ImageaugmentationpolicyforSimCLR (Chenetal.,2020a).
Parameter View1 View2
Randomcropprobability 1.0 1.0
Horizontalflipprobability 0.5 0.5
Colorjitteringprobability 0.8 0.8
Colorjitteringstrength(s) 1.0 1.0
Brightnessadjustmentmaxintensity 0.8×s 0.8×s
Contrastadjustmentmaxintensity 0.8×s 0.8×s
Saturationadjustmentmaxintensity 0.8×s 0.8×s
Hueadjustmentmaxintensity 0.2×s 0.2×s
Grayscaleprobability 0.2 0.2
Gaussianblurringprobability 0.5 0.5
21PublishedasaconferencepaperatICLR2024
Table11: ImageaugmentationpolicyforBYOL (Grilletal.,2020)andSimCLR-Tuned(Garrido
etal.,2023b).
Parameter View1 View2
Randomcropprobability 1.0 1.0
Horizontalflipprobability 0.5 0.5
Colorjitteringprobability 0.8 0.8
Brightnessadjustmentmaxintensity 0.4 0.4
Contrastadjustmentmaxintensity 0.4 0.4
Saturationadjustmentmaxintensity 0.2 0.2
Hueadjustmentmaxintensity 0.1 0.1
Grayscaleprobability 0.2 0.2
Gaussianblurringprobability 1.0 0.1
Solarizationprobability. 0.0 0.2
G ADDITIONAL EXPERIMENTAL RESULTS
G.1 COMPARINGLOSSTERMS
ItcanbeobservedfromTable12thattherearenosignificantdifferencesbetweentheregularization
termsL andL forimprovingtheperformanceofSSL.
L1 L2
Table12: ComparingtheresultsoflinearevaluationsofregularizationtermsofLDReg. Allmodels
aretrainedonImageNetfor100epochs. Theresultsarereportedaslinearprobingaccuracy(%).
Method Regularization k=64 k=128
L 64.8 64.6
SimCLR L1
L 64.4 64.5
L2
G.2 LOCALCOLLAPSETRIGGERINGCOMPLETECOLLAPSE
Inthissection,wedemonstratethatlocalcollapsecouldtriggertheworst-casemodecollapse,where
theoutputrepresentationisatrivialvector. LDRegisageneralregularizationtoolthatregularizes
representation to achieve a target LID. One can also use LDReg to achieve lower LID and, in the
extremecase,localcollapse. Specifically,weoptimizethefollowingobjectivefunction:
(cid:32) N (cid:33)
1 (cid:88)
min lnLID∗ .
N F wi
i
This objective regularizes the geometric mean of LID (of the representations) to 1. We use β to
controlthestrengthoftheregularizationtermanddenoteitasMinLID.
Table 13: Comparing the results of linear evaluations of regularization terms of LDReg, MinLID
andbaseline. AllmodelsaretrainedonImageNetfor100epochsusingResNet-50asencoder. The
resultsarereportedaslinearprobingaccuracy(%)onImageNet.
Method Regularization β LinearAcc EffectiveRank GeometricmeanofLID
LDReg 0.01 64.8 529.6 20.0
- - 64.3 470.2 18.8
MinLID 0.01 64.2 150.7 16.0
SimCLR
MinLID 0.1 63.1 15.0 3.8
MinLID 1.0 46.4 1.0 1.6
MinLID 10.0 Completecollapse - -
AsshowninTable13,itcanbeobservedthatusingMinLIDwithstronger(largerβ)willregularize
therepresentationtohaveextremelyloweffectiverankandeventuallyresultincompletecollapse,
even if SimCLR explicitly uses negative pairs to prevent this. Figure 4 shows the visualizations
22PublishedasaconferencepaperatICLR2024
of learned representations with different regularizations. Additionally, the performance of linear
evaluationsdegradesasdimensionalitydecreases. ThisresultindicatesthatlowLIDisundesirable
forSSL.
30 0 40 0 0 0
20 1 2 30 1 2 30 1 2 40 1 2
3 3 3 3
10 4 5 20 4 5 20 4 5 20 4 5
6 10 6 6 6
10
0
7 8 9 10
0
7 8 9 1 00 7 8 9 0 7 8 9
20 20 10 20
30 30 20
40 40
40 30
40 30 20 10 0 10 20 30 40 30 20 10 0 10 20 30 40 30 20 10 0 10 20 30 40 20 0 20 40
(a)LDRegβ=0.01 (b)Noregularization (c)MinLIDβ=0.1 (d)MinLIDβ=1.0
Figure 4: t-SNE visualizations of the representations learned by different pretraining. Results are
based on ResNet-50 with SimCLR with ImageNet validation set. Only the first 10 classes are se-
lectedforvisualizations.
G.3 ABLATIONSTUDY
We examine the effects of varying β and k for LDReg using SimCLR as the baseline. It can be
observedinFigure5aand5bthatlinearevaluationperformanceisrelativelystableacrossdifferent
valuesofkandβ. Foreffectiverank,Figure5cshowsthatgreaterstrengthofLDRegregularization
(largerβ)actuallydecreasestheeffectiverank. Thisisnotsurprising,sinceLIDisalocalmeasure,
andeffectiverankisaglobalmeasure. ThedifferencesbetweeneffectiverankandLIDareoutlined
inAppendixB.Figure5dshowsthatasmallervalueforkismorebeneficialforLDReg.Smallerkis
indeedmorepreferable,asithelpstopreservethelocalityassumptionsuponwhichLIDestimation
depends.
66 56 .. 70 50 S Si im mC CL LR R + LDReg 66 56 .. 70 50 S Si im mC CL LR R + LDReg 55 05 00 S Si im mC CL LR R + LDReg 56 70 50 S Si im mC CL LR R + LDReg
65.50 65.50 550 65.25 65.25 450 525 65.00 65.00 500
64.75 64.75 400 475
64.50 64.50 350 450
64.25 64.25 425
64.000.1 0.05 0.01 64.0032 64 128 256 3000.1 0.05 0.01 40032 64 128 256
k k
(a)LinearAcc (b)LinearAcc (c)Effectiverank (d)Effectiverank
Figure5: (a-b)Linearevaluationresultsand(c-d)effectiverankswithvaryingβ andk. Allmodels
aretrainedonImageNetfor100epochs. Theresultsarereportedaslinearprobingaccuracy(%)on
ImageNet.
Table 14 shows that LDReg can consistently improve the baseline with different batch sizes. We
reduce the k at the same rate as N is reduced, e.g. 16 and 32 for batch sizes 512 and 1024, re-
spectively. ItcanbeobservedthatLDRegcanconsistentlyimprovethebaselineindifferentbatch
sizes.
Table14: ComparingtheresultsoflinearevaluationswithdifferentbatchsizesN. Allmodelsare
trainedonImageNetfor100epochs. Theresultsarereportedaslinearprobingaccuracy(%).
Method Regularization N=512 N=1024 N=2048
- 63.6 64.2 64.3
SimCLR
LDReg 64.1 64.7 64.8
23
ccA borP
raeniL
ccA borP
raeniL
knaR evitceffE knaR evitceffEPublishedasaconferencepaperatICLR2024
G.4 ADDITIONALLINEAREVALUATIONRESULTS
Inthissubsection,wefurtherverifytheeffectivenessofLDRegwithdecorrelatingfeaturemethods
such as VICReg (Bardes et al., 2022) and Barlow Twins (Zbontar et al., 2021). We also evaluate
with another SOTA sample-contrastive method MoCo (He et al., 2020). For applying LDReg on
VICReg, we use β = 0.025 and k = 64. For Barlow Twins, we use β = 1.0 and k = 64. For
MoCo, we use β = 0.05 and k = 128. All other settings are kept the same as each baseline’s
originallyreportedhyperparameters. ResultscanbefoundinTable15.
Table15:Thelinearevaluationresults(accuracy(%))ofdifferentmethodswithandwithoutLDReg.
TheeffectiverankiscalculatedontheImageNetvalidationset. Thebestresultsareboldfaced.
Geometricmean
Model Epochs Method Regularization LinearEvaluation EffectiveRank
ofLID
- 68.7 595.0 17.1
MoCo
LDReg 69.6 651.8 22.3
- 66.7 546.7 21.5
ResNet-50 100 VICReg
LDReg 66.9 602.4 22.5
- 65.5 602.1 20.8
BarlowTwins
LDReg 65.6 754.0 24.1
VICReg and Barlow Twins are SSL methods rather than regularizers. For example, compared to
MoCo,VICRegandBarlowTwinsusedifferentprojectorarchitecturesandlossfunctions. It’snot
fairtocompareLDRegacrossdifferenttypesofSSLmethods.Forexample,MoCowithLDReghas
alinearevaluationof69.6,yetMoCoalonecanachieve68.7,whileVICRegunderthesamesetting
is only 66.7. However, if we apply our regularizer to these methods, their performance can all be
improved,asshowninthetable15.
To fairly compare the regularizers, we use the covariance (denoted as Cov) and variance (denoted
asVar)asalternativeregularizerstoreplaceLDRegandapplytoBYOL.Notethattheyarepseudo-
global dimension regularizers, as we cannot use the entire training set to calculate the covariance,
it is calculated on a mini-batch. We also performed a hyperparameter search for Cov and Var (β
forthestrengthoftheregularization). WeusedthesameregularizationformulaasVICReg(Bardes
etal.,2022)forCovandVarasthefollowing:
n n
1 (cid:88) 1 (cid:88)
C(Z)= (z −z¯)(z −z¯)T, z¯= z , (4)
n−1 i i n i
i=1 i=1
1(cid:88)
Cov(Z)=c(Z)= [C(Z)]2 , (5)
d i,j
i̸=j
wheredistherepresentationdimensions.
d
1(cid:88)
Var(Z)= max(0,γ−S(zj,ϵ)) whereS(·)isthestandarddeviation. (6)
d
j=1
Weapplytheregularizationonrepresentationslearnedbytheencoder,thesameasinLDReg. The
resultscanbefoundintheTable16. Allresultsarebasedon100epochpretrainingwithBYOLand
ResNet-50. AllsettingsareexactlythesameasLDRegexceptfortheregularizationterm.
Table 16: The linear evaluation results on comparing different regularization terms. The effective
rankiscalculatedontheImageNetvalidationset. Thebestresultsareboldfaced.
Method Regularizer β LinearEvaluation EffectiveRank GeometricmeanofLID
None - 67.6 583.8 15.9
Cov 0.01 67.6 583.5 15.9
Cov 0.1 67.5 593.5 15.8
BYOL
Cov+Var 0.01 67.8 539.2 15.5
Cov+Var 0.1 67.7 798.4 16.8
LDReg 0.005 68.5 594.0 22.3
Although the covariance and variance regularizer can increase the global dimension, it does not
improve the local dimension. It also has a rather minor effect on the linear evaluation accuracy,
whereasLDRegimprovesbyalmost1%. ThisfurtherconfirmstheeffectivenessofLDReg.
24PublishedasaconferencepaperatICLR2024
H LIMITATIONS AND FUTURE WORK
Themaintheoryofourworkisbasedonthelocalintrinsicdimensionalitymodelandwell-founded
Fisher-Raometric. ComputationofLDRegandtheFisher-Raometricallrequiretheabilitytoaccu-
ratelyestimatethelocalintrinsicdimension. Wehaveusedthemethodofmomentsinthispaperfor
LIDestimation, duetoitssimplicityandattractivenessforincorporationwithinagradientdescent
framework. Other estimation methods could be used instead; however, all estimation methods for
LIDareknowntodegradeinperformanceasthedimensionalityincreases.Moreover,ourestimation
of LID is based on nearest neighbor sets computed from within a minibatch. This choice is made
duetofeasibilityofcomputation, butentailsareductioninaccuracyascomparedtousingnearest
neighbors computed from the whole dataset. In future work, one might explore other estimation
methodsandtradeoffsbetweenestimationaccuracyandcomputationtime.
Basedonexistingworks, LDRegassumesthathigherdimensionalityisdesirableforSSL.LDReg
relies on a hyperparameter β to adjust the strength of the regularization term. The theory devel-
oped in this work allows LDReg to achieve any desired dimensionality. However, the optimal di-
mensionalities for SSL are dependent on the dataset and loss function. Knowledge of the optimal
dimensionality(ifitcouldbedetermined)canbeintegratedintoLDRegforbestperformance.
I IMPLEMENTATION DETAILS
ForimplementationwithPyTorch,Garridoetal.(2023b)havediscussedpopularopen-sourceimple-
mentationsofSimCLR(compatiblewithDDPusinggather)thatuseslightlyinaccurategradients.
TheimplementationinVICReg(Bardesetal.,2022)codebase5 iscorrectandshouldbeused. We
findthatthisslightlyaffectstheperformancewhenreproducingSimCLR’sresults. Thisalsoaffects
LDRegforestimatingLIDswithDDP.Forallofourexperiments,weusethesameimplementation
asGarridoetal.(2023b);Bardesetal.(2022).
EstimatingLIDneedstocomputethepairwisedistance,inPyTorch,thecdistfunctionbydefault
uses a matrix multiplication approach. For Nvidia Ampere or newer GPUs, TensorFloat-32
tensor coresshouldbedisabledduetoprecisionlossinthematrixmultiplication. Thispreci-
sionlosscansignificantlyaffecttheLIDestimations.
J PSEUDOCODE
Algorithm1:MethodofmomentsforLIDestimationusingpytorchpseudocode.
# data: representations
# reference: reference points
# k: the number of nearest neighbours
def lid_mom_est(data, reference, k):
r = torch.cdist(data, reference, p=2) # Pairwise distance
a, idx = torch.sort(r, dim=1)
m = torch.mean(a[:, 1:k], dim=1) # mu_k
lids = m / (a[:, k] - m) # a[:, k] is the w_k
return lids
5https://github.com/facebookresearch/vicreg
25PublishedasaconferencepaperatICLR2024
Algorithm2:LDRegusingpytorchpseudocode.
# f: representations
# k: the number of nearest neighbours
# beta: the hyperparameter $\beta$
# loss: SSL loss (such as NTXent)
# reg_type: "l1" or "l2" (L1 or L2 loss)
lids = lid_mom_est(data=f, reference=f.detach(), k=k)
if reg_type == "l1":
lid_reg = - torch.abs(torch.log(lids))
elif reg_type == "l2":
lid_reg = - torch.sqrt(torch.square(torch.log(lids)))
total_loss = loss + beta * lid_reg
total_loss = total_loss.mean(dim=0)
26