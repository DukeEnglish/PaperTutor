ArtiFade: Learning to Generate High-quality Subject from Blemished Images
ShuyaYang*,ShaozheHao*,YukangCao†,Kwan-YeeK.Wong†
TheUniversityofHongKong
Abstract generation.DreamBoothsharesasimilarideabutaddition-
ally fine-tunes the diffusion model to preserve more iden-
Subject-driven text-to-image generation has witnessed re-
titysemantics.Plentyofsuccessiveeffortshavebeenmade
markable advancements in its ability to learn and capture
to advance this task from various perspectives, including
characteristics of a subject using only a limited number of
images.However,existingmethodscommonlyrelyonhigh- generationquality,compositionality,andefficiency(Kumari
qualityimagesfortrainingandmaystruggletogeneraterea- etal.2023;Chenetal.2023;Kawaretal.2023).
sonableimageswhentheinputimagesareblemishedbyar- Both of the above mentioned methods, along with their
tifacts. This is primarily attributed to the inadequate capa- follow-up works, however, rely heavily on the presence of
bilityofcurrenttechniquesindistinguishingsubject-related
unblemished input images that contain only relevant iden-
featuresfromdisruptiveartifacts.Inthispaper,weintroduce
tity information. This is often expensive or even unavail-
ArtiFadetotacklethisissueandsuccessfullygeneratehigh-
ableinreal-worldapplications.Instead,inpracticalscenar-
qualityartifact-freeimagesfromblemisheddatasets.Specif-
ios such as scraping web images of a desired subject, it is
ically,ArtiFadeexploitsfine-tuningofapre-trainedtext-to-
imagemodel,aimingtoremoveartifacts.Theeliminationof commontoencounterimagesthatareblemishedbyvarious
artifactsisachievedbyutilizingaspecializeddatasetthaten- visibleartifactssuchaswatermarks,drawings,andstickers.
compassesbothunblemishedimagesandtheircorresponding Additionally, there also exist invisible artifacts like adver-
blemishedcounterpartsduringfine-tuning.ArtiFadealsoen- sarial noises (Van Le et al. 2023) that are not easily de-
suresthepreservationoftheoriginalgenerativecapabilities tectable or removable using off-the-shelf tools. These arti-
inherent within the diffusion model, thereby enhancing the facts can significantly impede the comprehensive learning
overallperformanceofsubject-drivenmethodsingenerating
of the subject and lead to a catastrophic decline in perfor-
high-qualityandartifact-freeimages.Wefurtherdeviseeval-
manceacrossmultipledimensions(seeFig.1).Thislimita-
uation benchmarks tailored for this task. Through extensive
tionarisesfromthefeatureconfusioninherentintheexisting
qualitativeandquantitativeexperiments,wedemonstratethe
subject-drivenlearningprocess.Theprocesssimultaneously
generalizabilityofArtiFadeineffectiveartifactremovalun-
derbothin-distributionandout-of-distributionscenarios. captures subject-related features and disruptive artifact in-
terference. It lacks the discriminative power to distinguish
thesetwofromeachother,andfailstopreservetheintegrity
1 Introduction
of subject characteristics while mitigating negative effects
With the rapid advancement of generative diffusion mod-
caused by artifacts. As blemished inputs are inevitable in
els (Rombach et al. 2022; Song, Meng, and Ermon 2021;
applications, a pressing challenge emerges: Can we effec-
Saharia et al. 2022; Zhang, Rao, and Agrawala 2023; Ho,
tively perform subject-driven text-to-image generation
Jain,andAbbeel2020),subject-driventext-to-imagegener- usingblemishedimages?Wetermthisnovelproblem(i.e.,
ation(Galetal.2023;Ruizetal.2023;Kumarietal.2023;
generatingsubject-drivenimagesfromblemishedinputs)as
Kawar et al. 2023; Chen et al. 2023), which aims to cap-
blemishedsubject-drivengenerationinthispaper.
ture distinct characteristics of a subject by learning from a
To answer the above question, we present ArtiFade, the
few images of the subject, has gained significant attention.
firstmodeltotackleblemishedsubject-drivengenerationby
Thisapproachempowersindividualstoseamlesslyincorpo-
adaptingvanillasubject-drivenmethods(e.g.,TextualInver-
rate their preferred subjects into diverse and visually cap-
sion(Galetal.2023)andDreamBooth(Ruizetal.2023))to
tivatingscenesbysimplyprovidingtextconditions.Repre-
effectively extract subject-specific information from blem-
sentative works such as Textual Inversion (Gal et al. 2023)
ished training data. The key objective of ArtiFade is to
and DreamBooth (Ruiz et al. 2023) have shown promising
learn the implicit relationship between natural images and
resultsonthistask.Specifically,TextualInversionproposes
their blemished counterparts through alignment optimiza-
to optimize a textual embedding to encode identity charac-
tion. Specifically, we introduce a specialized dataset con-
teristicsthatproviderichsubjectinformationforsubsequent
structionmethodtocreatepairsofunblemishedimagesand
*Equalcontribution theircorrespondingcounterparts.Thesepairscanbeapplied
†Correspondingauthors tofine-tunevarioussubject-drivenapproachesinthecontext
4202
peS
5
]VC.sc[
1v54730.9042:viXraInput images ArtiFade (Ours) Textual Inversion Input images ArtiFade (Ours) DreamBooth
Visible artifacts Invisible artifacts
Figure1:Blemishedsubject-drivengenerationwithourArtiFadeandvanillasubject-drivenmethods.Wedisplayimagesgen-
eratedusingArtiFadeandTextualInversiononwatermarkartifactsontheleft,andArtiFadeandDreamBoothonadversarial
noise artifacts (Van Le et al. 2023) on the right. In contrast to the poor performance of Textual Inversion and DreamBooth,
which are negatively affected by the visiable or invisible artifacts, ArtiFade produces much better fidelity of the subject with
high-qualitygeneration.
ofblemishedsubject-drivengeneration.Besides,wealsoob- 2018; Zhu et al. 2019; Li et al. 2019a; Ruan et al. 2021;
servefine-tuninganextralearnableembeddinginthetextual Zhang et al. 2021; Cheng et al. 2020; Qiao et al. 2019;
space, named artifact-free embedding, can enhance prompt Yin et al. 2019), demonstrating impressive results with im-
fidelityintheblemishedsubject-drivengeneration. proved resolution (Zhang et al. 2017, 2018) and fidelity of
We further introduce an evaluation benchmark that en- fine details (Xu et al. 2018). Diffusion models in text-to-
compasses (1) multiple test sets of blemished images with imagesynthesishavealsoyieldedremarkableresultsowing
diverse artifacts, and (2) tailored metrics for accurately as- totheirabilityingeneratingpreciseandcustomizedimages
sessingtheperformanceofblemishedsubject-drivengener- that better align with individual text specifications (Nichol
ation methods. A thorough experimental evaluation shows etal.2022;Sahariaetal.2022;Rameshetal.2022;Guetal.
that our method consistently outperforms other existing 2022;Rombachetal.2022).
methods, both qualitatively and quantitatively. Notably,
ArtiFade exhibits superb capabilities in handling out-of- Subject-drivengeneration Subject-drivengenerationhas
distribution (OOD) scenarios involving diverse types of ar- gained popularity due to its ability to generate personal-
tifacts that are distinct from the training data. This inher- izedimagesbasedonagivensetofsubjectimagesandtext
entgeneralizabilityindicatesourmodelcaneffectivelylearn prompts. One prominent method in subject-driven genera-
todiscernanddistinguishthepatternsexhibitedbyartifacts tion is Textual Inversion (Gal et al. 2023), which involves
andunblemishedimages,insteadofoverfittingtoaspecific learninganembeddingvectorbyminimizingtheLatentDif-
typeofartifacts. fusion Model loss (Rombach et al. 2022) on input images.
Insummary,ourkeycontributionsareasfollows: Thelearnedembeddingvectorcanbeeffectivelycombined
withtextprompts,allowingseamlessintegrationinthetext-
• Wearethefirsttotacklethenovelchallengeofblemished
to-imagegenerationprocess.Recentapproaches(Ruizetal.
subject-driven generation. To address this task, we pro-
2023;Kumarietal.2023;Luetal.2023)havesignificantly
pose ArtiFade that fine-tunes diffusion models to align
enhanced subject reconstruction fidelity by incorporating
unblemishedandblemisheddata.
fine-tuningtechniques.
• Weintroduceanevaluationbenchmarktailoredforeffec-
tively assessing the performance of blemished subject- Artifacts removal Shadow and watermark removal are
drivengenerationtechniques. classic tasks in image processing and computer vision. At
theearlystage,mostapproachesforshadowremovalorim-
• Weconductextensiveexperimentsanddemonstratethat
age recovery relied on the properties of intensity and illu-
ArtiFade outperforms current methods significantly. We
mination (Finlayson, Drew, and Lu 2009; Finlayson et al.
shownoteworthygeneralizabilityofArtiFade,effectively
2006; Zhang, Zhang, and Xiao 2015; Xiao et al. 2013b,a;
addressing both in-distribution and out-of-distribution
Finlayson,Hordley,andDrew2002;Khanetal.2015;Shor
scenarioswithvarioustypesofartifacts.
andLischinski2008;ArbelandHel-Or2010;Guo,Dai,and
Hoiem 2011). Some methods also incorporated color fea-
2 RelatedWork
tures to improve their results (Guo, Dai, and Hoiem 2011).
Text-to-imagesynthesis Text-to-imagegenerationhasat- Deep learning techniques and Convolutional Neural Net-
tracted considerable attention in recent years by leverag- works (CNNs) have played a significant role in advanc-
ingGenerativeAdversarialNetworks(GANs)(Goodfellow ingshadowremovalmethodsandproducingimpressivere-
et al. 2014) and diffusion models (Ho, Jain, and Abbeel sults in recent years (Ding et al. 2019; Hu et al. 2019; Le
2020; Rombach et al. 2022). Reed et al. (2016) was the and Samaras 2019; Liu et al. 2021; Wang, Li, and Yang
firsttointegrateGANsintotext-to-imagegeneration.Since 2018;Zhuetal.2022;Chenetal.2021;Jinetal.2023;Fu
then, several influential works had been proposed (Zhang etal.2021).Severalstudies(Wang,Li,andYang2018;Liu
et al. 2017, 2018; Xu et al. 2018; Zhang, Xie, and Yang et al. 2021; Hu et al. 2019; Ding et al. 2019) have incor-Fine-tuning Inference
Training datasetC Reconstruction Loss
Output
T steps
Sample I
KV KV KKVV KV
I Unblemished
I
, 
Blemished β artifacts    , 
Sample
Trainable

Textual Inversion
S S
′
Figure2:OverviewofArtiFade.Onth  eleft,wepresentArtifactRectificationTraining,w    h  ichinvolvesaniterativeprocessof
calculatingreconstructionlossbetweenanunblemishedimageandthereconstructionofitsblemishedembedding.Theright-
handsideistheinferencestagethattestsArtiFadeonunseenblemishedimages.Toavoidambiguity,we(1)simplifythetraining
ofTextualInversionintoaninput-outputform,and(2)use“fine-tuning”and“inference”torespectivelyrefertothefine-tuning
stageofArtiFadeandtheuseofArtiFadeforsubject-drivengeneration.
porated GANs to further enhance the results of shadow re- In the following subsections, we elaborate our automatic
movaltechniques.Moreover,withtheincreasingpopularity construction of the training dataset, which consists of both
ofdiffusionmodelsinimagegeneration,anoveldiffusion- blemishedandunblemisheddata,illustratedinSec.3.1.We
based method for shadow removal has recently been intro- thenintroduceArtifactRectificationTraining,amethodfor
duced(Guoetal.2023). fine-tuningthemodeltoaccommodateblemishedimages,as
The most widely adopted methods for recovering con- discussedinSec.3.2.WefinallypresenttheuseofArtiFade
cealed information from watermarked images include forhandlingblemishedimagesinSec.3.3.
the application of generalized multi-image matting algo-
Preliminary Latent Diffusion Model (LDM) (Rombach
rithms(Dekeletal.2017),complementedbyimageinpaint-
et al. 2022) is a latent text-to-image diffusion model
ing techniques (Xu, Lu, and Zhou 2017; Qin et al. 2018;
derived from Diffusion Denoising Probabilistic Model
HuangandWu2004),andtheutilizationofdeepneuralnet-
(DDPM)(Ho,Jain,andAbbeel2020).LDMleveragesapre-
worksandCNNs(Chengetal.2018).Similartoshadowre-
trainedautoencodertomapimagefeaturesbetweentheim-
moval, GANs and Conditional GANs (Mirza and Osindero
ageandlatentspace.Thisautoencodercomprisesanencoder
2014) are also widelyused in watermark removal tasks (Li
E,whichtransformsimagesintolatentrepresentations,and
etal.2019b;Caoetal.2019;Liu,Zhu,andBai2021).Our
adecoderD,whichconvertslatentrepresentationsbackinto
work is closely related to these previously mentioned stud-
images.Theautoencoderisoptimizedusingasetofimages
ies.Wearethefirsttoaddresstheartifactissuesintherealm
sothatthereconstructedimagexˆ≈D(E(x)).Additionally,
ofsubject-driventext-to-imagegeneration.
LDMintroducescross-attentionlayers(Vaswanietal.2017)
within the U-Net (Ronneberger, Fischer, and Brox 2015),
3 Method
enablingtheintegrationoftextpromptsasconditionalinfor-
Given a set of blemished input images, our objective is to mationduringtheimagegenerationprocess.TheLDMloss
eliminate their negative impacts on the quality of subject- isdefinedas
driven image generation. To achieve this goal, we present
(cid:104) (cid:105)
ArtiFade, an efficient framework that learns to discern and L :=E ∥ϵ−ϵ (z ,t,y)∥2 , (1)
LDM z∼E(I),y,ϵ∼N(0,1) θ t 2
distinguish the patterns exhibited by various types of arti-
factsandunblemishedimages.Inthissection,wefocusex- whereEencodestheimageIintothelatentrepresentationz.
clusivelyonArtiFadebasedonTextualInversion.However, Here,z denotesthenoisylatentrepresentationattimestept,
t
it is important to note that the ArtiFade framework can be ϵ referstothedenoisingnetwork,andy representsthetext
θ
generalized to other subject-driven generation methods. As conditionthatispassedtothecross-attentionlayer.
showninFig.2,ArtiFadebasedonTextualInversionincor- BasedonLDM,TextualInversion(Galetal.2023)aims
poratestwomaincomponents,namelythefine-tuningofthe to capture the characteristics of a specific subject from a
partialparameters(i.e.,keyandvalueweights)inthediffu- smallsetofimages.Specifically,TextualInversionlearnsa
sionmodelandthesimultaneousoptimizationofanartifact- unique textual embedding by minimizing Eq. (1) on a few
free embedding 〈Φ〉. We begin by discussing the prelimi- images that contain the particular subject. It can produce
nariesoftheLatentDiffusionModelandTextualInversion. promising generation results with high-quality inputs, but
a >Φ< otohp fo

]
[
a >Φ< otohp fo
′ 
]tset
[Unblemished Subset ByapplyingEq.(4)onN subsetswithLtypesofartifacts,
1,...,
1
1  2  3   w die nge sn Vd u =p {w [Vit βh k]a }Nse ,t Lof N ,w× hiL chb wle im lli bs ehe ud set dex it nua thl ee sm ub be sd e--
i i=1,k=1
1 quentmodelfine-tuning.AswehaveillustratedinFig.1,di-
rectlypromptingthediffusionmodelwith[Vβk]willleadtoa
i
1,...,

s oi bg jn ei cfi tic va ent isde tocre roa bse usin tlyge hn ae nr dat li eon blq eu ma il si hty e. dC eo mns be eq du de in nt gly s, ao nu dr
effectivelyeliminatethedetrimentalimpactofartifacts.We

achieve this by devising a partial fine-tuning paradigm for
the pre-trained diffusion model on the constructed training
Figure3:ExamplesoftrainingdatasetD thatcontainsboth setD,aselaboratedinthefollowingsubsection.
unblemishedimagesandblemishedcounterparts.
3.2 ArtifactRectificationTraining
AfterestablishingthecurateddatasetD,weembarkontrain-
fails on input images that are blemished by artifacts (see
ingageneralizableframeworkonD,capableofgenerating
Fig. 1). This problem arises from the inherent limitation of
unblemished images using blemished textual embeddings.
TextualInversioninlearningsharedcharacteristicsexhibited
Tothisend,weproposeartifactrectificationtraining,which
intheinputimageswithoutthecapabilityindifferentiating
consistsoftwokeycomponents,namelypartialfine-tuning
artifactsfromunblemishedsubjects.Inthispaper,weaimto
ofapre-traineddiffusionmodelandtheoptimizationofan
addressthisissueondeterioratedgenerationqualityofTex-
artifact-freeembedding,toeliminatetheartifactsanddistor-
tualInversioninthepresenceofblemishedimages.
tionsinthegeneratedimages.
We fine-tune only partial parameters that are involved in
3.1 DatasetConstruction
processingthetextualconditions.Thisstrategyallowsusto
Existing subject-driven generation methods operate under optimizetherelevantcomponentsassociatedwiththeblem-
the assumption of unblemished training data, consisting of ishedtextualembedding[Vβk].Consideringthatonlythekey
i
solely high-quality images devoid of any artifacts. How- and value weights in the diffusion model’s cross-attention
ever,thisassumptiondoesnotalignwithreal-worldapplica- layer are involved in the processing of textual embedding,
tions, where obtaining blemished images from the internet we choose to fine-tune these two types of parameters Wk
isacommonplace.Toaddressthisblemishedsubject-driven and Wv. Moreover, we find that optimizing an additional
generationinthispaper,wefirstconstructatrainingsetthat embedding, 〈Φ〉, in the textual space with partial parame-
incorporates both unblemished images and their blemished ters could improve prompt fidelity by retaining the textual
counterpartsthatareaugmentedwithartifacts. informationofthemodel,aspresentedlaterinSec.4.7.
Augmentation of multiple artifacts We construct our Training objective During each iteration, we will first
datasetbycollectingamulti-subjectsetC ofN imagesub- randomlysampleanunblemishedimageI fromthetrain-
i,j
setsfromexistingworks(Galetal.2023;Ruizetal.2023; ingsetD andatypeofartifactβ ∈ B toobtaintheblem-
k
Kumarietal.2023)andasetBofLdifferentartifacts: ishedtextualembedding[Vβk] ∈ V thatisoptimizedonthe
i
C ={S }N , S ={I }Mi , B ={β }L , (2)
blemishedsubsetS iβk.
i i=1 i i,j j=1 k k=1 Specifically,giventhesampledblemishedtextualembed-
whereS
i
denotestheimagesubsetcorrespondingtotheith ding [Vβ ik], we form the prompt “a 〈Φ〉 photo of [Vβ ik]”,
subject,M isthetotalnumberofimagesinS ,andβ repre- which will be input to the text encoder to acquire the text
i i k
sentsatypeofartifactforimageaugmentation.Ourdataset condition yβk. Our optimization objective will then be de-
i
D can then be constructed by applying each artifact β k to finedasreconstructingtheunblemishedimageI i,j bycon-
eachimageI inS iseparately,i.e., ditioning the denoising process on the text condition yβk.
i
Thus, we can formulate the final loss for training ArtiFade
Sβk ={Iβk}Mi , D ={S ,{Sβk}L }N , (3)
i i,j j=1 i i k=1 i=1 as
w
c ai
uh
fi
ge
c
mr ae erI
nti
tiβ
f
e, ajk dci ts
vβ
et kh rs.e
iS
oc
o
no
m
sun
e
wte
e
ir
tx
hp aa
m
drt
ip
so
l te
if nsI coi t, fj aoa
rr
tu
i ig
fg aim
n
ctae sln it cme ad naw
g
bei esth
a
fot nh ude nts dhp
e
ie
i
n-
r
L ArtiFade := (cid:104)E ∥z ϵ∼ −E(I ϵi {, Wj), ky ,iβ Wk, vϵ ,∼ ⟨ΦN ⟩( }0 (, z1)
t,t,y iβk)∥2
2(cid:105)
,
(5)
Fig.3.SeetheAppendixformorevisualizations.
where{Wk,Wv,⟨Φ⟩}isthesetofthetrainableparameters
Blemished textual embedding For each blemished sub- ofArtiFade.
set, we perform Textual Inversion to optimize a blemished
textualembedding[Vβk],i.e., 3.3 Subject-drivenGenerationwithBlemished
i
Images
Sβk TextualInversion [Vβk],
i −−−−−−−−−−−→ i (4) After artifact rectification training, we obtain the ArtiFade
i=1,2,...,N; k =1,2,...,L model, prepared for the task of blemished subject-drivenWM-model on WM-ID-test Input images Ours Textual Inversion Ours Textual Inversion
Method
IDINO↑ RDINO↑ ICLIP↑ RCLIP↑ TCLIP↑
TI(unblemished) 0.488 1.349 0.730 1.070 0.283
TI(blemished) 0.217 0.852 0.576 0.909 0.263
Ours(TI-based) 0.337 1.300 0.649 1.020 0.282 in the street in the snow
'  '
[      ] [      ]
Table1:Quantitativeresults-ID.
WM-model on WM-OOD-test
Method with a mountain in the background with a city in the background
IDINO↑ RDINO↑ ICLIP↑ RCLIP↑ TCLIP↑ [     '   ] [     '   ]
Figure 4: Qualitative Comparison - ID. Unlike Textual In-
TI(unblemished) 0.488 1.278 0.730 1.136 0.283 version which struggles to produce reasonable generation
TI(blemished) 0.229 0.858 0.575 0.929 0.262
from blemished inputs, our method (WM-model) consis-
Ours(TI-based) 0.356 1.237 0.654 1.079 0.282
tently learns the distinguished features of the given subject
andachieveshigh-qualitygenerationwithoutdistortion.
Table2:Quantitativeresults-OOD.
Input images Ours Textual Inversion Ours Textual Inversion
generation.GivenatestimagesetSβ′
inwhichallimages
test
areblemishedbyanarbitraryartifactβ′,theArtiFademodel
cangeneratehigh-qualitysubject-drivenimagesusingblem-
ishedsampleswithease.
on top of a wooden floor with a city in the background
Specifically,wefirstobtaintheblemishedtextualembed-  '  '
[      ] [      ]
ding[Vβ′ ]byapplyingTextualInversiononthetestsetSβ′
.
test test
We then simply infer the ArtiFade model with a given text
promptthatincludestheblemishedtextualembedding,i.e.,
“a〈Φ〉photoof[Vβ te′ st]”.Attheoperationallevel,thesoledis-
[  


'
]
in the movie theater
[  


'
]
in a luxurious interior living room
tinctionbetweenourapproachandvanillaTextualInversion Figure 5: Qualitative Comparison - OOD. Our method
liesininputtingtextpromptscontaining[Vβ′ ]intothefine- (WM-model)isgeneralizabletoprocessout-of-distribution
test
tuned ArtiFade instead of the pre-trained diffusion model. artifactsthatareunseenduringthefine-tuning,demonstrat-
This simple yet effective method resolves the issue of Tex- ing much better performance than Textual Inversion. Best
tual Inversion’s incapacity to handle blemished input im- viewedinPDFwithzoom.
ages,bearingpracticalutility.
DetailsofArtiFademodels WechooseN =20subjects, and DreamBooth to demonstrate the efficiency of our pro-
including pets, plants, containers, toys, and wearable items posed contributions. See the Appendix for additional com-
toensureadiverserangeofcategories.Weexperimentwith parisonsandapplications.
theArtiFademodelbasedonTextualInversiontrainedwith
visiblewatermarkartifacts,namely WM-model.Thetrain- 4.2 EvaluationBenchmark
ingsetof WM-model involvesL =10typesofwater-
WM Test dataset We construct the test dataset using 16 novel
marks, characterized by various fonts, orientations, colors,
subjects that differ from the subjects in the training set.
sizes,andtextcontents.Therefore,weobtain200blemished
These subjects encompass a wide range of categories, in-
subsets in total within the training set of WM-model. We
cluding pets, plants, toys, transportation, furniture, and
fine-tune WM-model foratotalof16ksteps.
wearable items. We form the visible test artifacts into two
categories: (1) in-distribution watermarks (WM-ID-test)
4 Experiment
containing the same type as the training data, and (2) out-
4.1 ImplementationDetails of-distribution watermarks (WM-OOD-test) of different
typesfromthetrainingdata.WithintheWM-ID-testand
Weemploythepre-trainedLDM(Rombachetal.2022)fol-
WM-OOD-test,wesynthesize5distinctartifactsforeach
lowingtheofficialimplementationofTextualInversion(Gal
category,resultingin80testsets.
etal.2023)asourbasediffusionmodel.Wetraintheblem-
ished textual embeddings for 5k steps using Textual Inver- Evaluation metrics We evaluate the performance of
sion.Weusealearningrateof5e-3tooptimizeourArtifact- blemished subject-driven generation from three perspec-
free embedding and 3e-5 for the partial fine-tuning of key tives:(1)thefidelityofsubjectreconstruction,(2)thefidelity
andvalueweights.Notethatallotherparameterswithinthe oftextconditioning,and(3)theeffectivenessofmitigating
pre-traineddiffusionmodelremainfrozen.Allexperiments the negative impacts of artifacts. Following common prac-
areconductedon2NVIDIARTX3090GPUs.Inthemain tice (Gal et al. 2023; Ruiz et al. 2023), we use CLIP (Rad-
paper, we focus on the comparison with Textual Inversion ford et al. 2021) and DINO (Caron et al. 2021) similaritiesfor measuring these metrics. For the first metric, we calcu- WM-ID-test
Method
late the CLIP and DINO similarity between the generated
IDINO↑ RDINO↑ ICLIP↑ RCLIP↑ TCLIP↑
imagesandtheunblemishedversionoftheinputimages,re-
spectivelydenotedasICLIPandIDINO.Forthesecondmetric, TI(unblemished) 0.488 1.349 0.730 1.070 0.283
TI(blemished) 0.217 0.852 0.576 0.909 0.263
wecalculatetheCLIPsimilaritybetweenthegeneratedim-
DB(blemished) 0.503 0.874 0.738 0.939 0.272
ages and the text prompt, denoted as TCLIP. For the third Ours(TI-based) 0.337 1.300 0.649 1.020 0.282
metric,wecalculatetherelativeratioofsimilaritiesbetween Ours(DB-based) 0.589 1.308 0.795 1.083 0.284
generated images and unblemished input images compared
totheirblemishedversions,definedas Table3:QuantitativecomparisonwithDreamBooth.
RCLIP =ICLIP/ICLIP RDINO =IDINO/IDINO (6)
β β IInnppuutt iimmaaggeess Ours DreamBooth Ours DreamBooth
where ICLIP and IDINO respectively denote CLIP and DINO
β β
similaritiesbetweenthegeneratedimagesandtheblemished
input images. A relative ratio greater than 1 indicates that
generated images resemble unblemished images more than
sks cat with a beautiful sunset sks cat with the Eiffel tower in the background
blemished counterparts, suggesting fewer artifacts. Con-
versely, a ratio less than 1 indicates that generated images
areheavilydistortedwithmoreartifacts.WeuseDINOViT-
S/16(Caronetal.2021)andCLIPViT-B/32(Radfordetal.
2021)tocomputeallmetrics. sks motorbike in the snow sks motorbike in the jungle
Figure6:QualitativecomparisonwithDreamBooth.
4.3 QuantitativeComparisons
We conduct both in-distribution and out-of-distribution
quantitative evaluations of our method and compare it to In-distributionanalysis TheimagesgeneratedbyTextual
TextualInversionwithblemishedembeddings.Weaddition- Inversion exhibit noticeable limitations when using blem-
ally report the results using Textual Inversion on unblem- ished textual embeddings. Specifically, as depicted in Fig.
ishedimagesasareference,althoughitisnotadirectcom- 4, all rows predominantly exhibit cases of incorrect back-
parisontoourmodel. grounds that are highly polluted by watermarks. By using
ArtiFade, we are able to eliminate the background water-
In-distribution (ID) analysis We consider the in-
marks.
distribution scenarios by testing WM-model on
WM-ID-test. In Tab. 1, we can observe that the use Out-of-distributionanalysis Inaddition,weconductex-
of blemished embeddings in Textual Inversion leads to perimentswithour WM-model toshowcaseitscapabilityto
comprehensive performance decline including: (1) lower remove out-of-distribution watermarks, as shown in Fig. 5.
subject reconstruction fidelity (i.e., IDINO and ICLIP) due Itisimportanttonotethatinthefirstrow,thewatermarkin
to the subject distortion in image generation; (2) lower the input images may not be easily noticed by human eyes
efficiency for artifact removal (i.e., RDINO and RCLIP) due upon initial inspection due to the small font size and high
to inability to remove artifacts; (3) lower prompt fidelity imageresolution.However,theseartifactshaveasignificant
(i.e., TCLIP) since the prompt-guided background is un- effect when used to train blemished embeddings for gener-
recognizable due to blemishing artifacts. In contrast, our atingimages.ArtiFadeeffectivelyeliminatestheartifactson
method consistently achieves higher scores than Textual thegeneratedimages,improvingreconstructionfidelityand
Inversion with blemished embeddings across the board, backgroundaccuracy,henceleadingtosubstantialenhance-
demonstratingtheefficiencyofArtiFadeinvariousaspects. mentsinoverallvisualquality.
Out-of-distribution (OOD) analysis We pleasantly dis- 4.5 ArtiFadewithDreamBooth
cover that WM-model possesses the capability to han-
The ArtiFade fine-tuning framework is not limited to Tex-
dle out-of-distribution scenarios, owing to its training with
tual Inversion with textual embedding; it can also be gen-
watermarks of diverse types. We consider the out-of-
eralized to DreamBooth. We use the same training dataset
distribution (OOD) scenarios for WM-model by testing it
and blemished subsets as in the case of the WM-model
on WM-OOD-test, as presented in Tab. 2. Similar to ID
(i.e., N = 20, L = 10). The vanilla DreamBooth fine-
evaluation, all of our metrics yield higher results than Tex- WM
tunesthewholeUNetmodel,whichconflictswiththefine-
tual Inversion with blemished embeddings. These results
tuning parameters of ArtiFade. We therefore use Dream-
furtherdemonstratethegeneralizabilityofourmethod.
Boothwithlow-rankapproximation(LoRA)1totrainLoRA
4.4 QualitativeComparisons adapters (Hu et al. 2022) for the text encoder, value, and
query weights of the diffusion model for each blemished
Wepresentqualitativecomparisonsbetweentheoutputgen-
subset using Stable Diffusion v1-5. For simplicity, we will
erated via ArtiFade and Textual Inversion with blemished
textual embeddings, including in-distribution scenarios in 1https://huggingface.co/docs/peft/main/en/task guides/
Fig.4andout-of-distributionscenariosinFig.5. dreambooth loraIInnppuutt iimmaaggeess Ours DreamBooth Ours DreamBooth Method Wkv Wq 〈Φ〉 IDINO RDINO ICLIP RCLIP TCLIP
Var ✓ 0.154 1.412 0.566 0.984 0.265
A
Var ✓ ✓ 0.283 1.230 0.617 0.978 0.277
B
Var ✓ 0.342 1.292 0.652 1.019 0.280
C
Ours ✓ ✓ 0.337 1.300 0.649 1.020 0.282
sks person sks person in the jungle
Table4:Quantitativecomparisonofablationstudies.
Input images Var A Var B Var C WM-model (Ours)
sks person in the snow sks person with a city in the background
Figure7:QualitativeComparisonbetweenoursandDream-
Booth when inputs are blemished by invisible adversarial
noises.
in a movie theater
'
[    '   ]
useDreamBoothtorefertoDreamBoothwithLoRAbelow.
During the fine-tuning of DreamBooth-based ArtiFade, we
loadthepre-trainedadaptersandonlyunfreezekeyweights
in the snow
sincevalueweightsarereservedforDreamBoothsubjectin-
'
formation.InTab.3,itisevidentthatourmethod,basedon Figure8:Qualitativecomp[a  r   i'  s  ]onofablationstudies.
DreamBooth,yieldsthehighestscoresamongallcases.Our
method also maintains DreamBooth’s advantages in gener-
atingimageswithhighersubjectfidelityandmoreaccurate is reasonable, as the artifact-free embedding can be easily
textprompting,outperformingArtiFadewithTextualInver- overfittedtothetrainingdata,resultingingeneratedimages
sion.WeshowsomequalitativeresultsinFig.6. thatresembleafusionoftrainingimages(Fig.8,Var ).As
A
aresult,thedenominatorofRDINO,namelythesimilaritybe-
4.6 InvisibleArtifactsBlemishedSubject tweenthegeneratedimageandtheblemishedimage,issig-
Generation nificantlydecreased,leadingtoahighRDINO.Duetosimilar
ArtiFadedemonstratesexceptionalperformanceinhandling reason, Var A shows lowest IDINO, ICLIP, and TCLIP among
subjectscharacterizedbyintricatefeaturesandblemishedby allvariants,indicatingthatitfailstoreconstructthecorrect
imperceptibleartifacts.Wecollect20humanfiguredatasets subject.Overall,bothquantitativeandqualitativeevaluation
fromtheVGGFace2dataset(Caoetal.2018).Wethenuse showcasesthatsolelyoptimizingtheartifact-freeembedding
theAnti-DreamBooth(VanLeetal.2023)ASPLmethodto isinsufficienttocapturethedistinctcharacteristicspresented
add adversarial noises to each group of images, producing intheblemishedinputimage,demonstratingthenecessityof
20 blemished datasets for fine-tuning a DreamBooth-based partialfine-tuning.
ArtiFade model. The model is fine-tuned for 12k steps. As
Effect of fine-tuning key and value weights As shown
illustratedinFig.7,ourapproachsurpassestheDreamBooth
in Tab. 4 and Fig. 8, Var yields unsatisfactory outcomes
B
in differentiating the learning of adversarial noises from inallaspectscomparedtoours.ThelowerRDINO andRCLIP
human face features. In contrast to DreamBooth, which is
suggestthatthegeneratedimagesretainartifact-likefeatures
fooled into overfitting adversarial noises, thereby generat-
andbearcloserresemblancestotheblemishedsubsets.Fur-
ing images with a heavily polluted background, our model thermore, the reduced TCLIP indicates diminished prompt
reconstructshumanfiguresinimagegenerationwhilemain-
fidelity, as the approach fails to accurately reconstruct the
taininghighfidelitythroughtextprompting.
subject from the blemished embeddings, which is also evi-
dencedbyFig.8.Thesefindingssuggestthatfine-tuningthe
4.7 AblationStudies
parametersassociatedwithtextfeaturesyieldssuperioren-
We conduct ablation studies to demonstrate the efficiency hancementsintermsofartifactremovalandpromptfidelity.
ofourmethodbycomparingwiththreealternativevariants,
which encompass (1) Var , where we solely fine-tune the
A Effectoftheartifact-freeembedding WithVar ,weex-
artifact-free embedding; (2) Var , where we fine-tune pa- C
B cludetheoptimizationofartifact-freeembedding.InTab.4,
rameters related to image features, i.e., query weights Wq,
we can observe that Var yields higher IDINO and ICLIP
alongwiththeartifact-freeembedding,and(3)Var ,where C
C but lower RDINO and RCLIP compared to our WM-model,
we fine-tune key and value weights, i.e., Wk and Wv, ex-
whichindicatesthattheapproachachieveshighersubjectfi-
clusively.Weuseour WM-model tocompareitwithother
delitybutlowerefficiencyineliminatingartifactswhengen-
variantsbytestingontheWM-ID-test.
erating images. Since our primary objective is to generate
Effect of partial fine-tuning As shown in Tab. 4, com- artifact-freeimagesfromblemishedtextualembedding,our
pared to Var , our full method yields higher scores on WM-model chooses to trade off subject reconstruction fi-
A
all metrics by a significant margin, except for RDINO. This delity for the ability to remove artifacts. Additionally, thisInput images Ours Textual Inversion Input images Ours Textual Inversion Glasseffectremoval. Wefurthertest WM-model onin-
put images that are blemished by glass effect in Fig. 9b.
Weapplyaflutedglasseffecttoimagestoreplicatereal-life
scenarioswhereindividualscapturephotographsofsubjects
positioned behind fluted glass. This glass can have specific
reflectionsandblurring,whichmaycompromisetheoverall
quality of image generation when using Textual Inversion.
Theuseofourmodelcanfixthedistortionsofthesubjects
and the unexpected background problem, significantly im-
provingimagequality.
6 Conclusion
In conclusion, we introduce ArtiFade to address the novel
problemofgeneratinghigh-qualityandartifact-freeimages
in the blemished subject-driven generation. Our approach
involves fine-tuning a diffusion model along with artifact-
freeembeddingtolearnthealignmentbetweenunblemished
(a)Stickerremoval. images and blemished information. We present an evalua-
tionbenchmarktothoroughlyassessamodel’scapabilityin
Preserved for more example Input images Ours Textual Inversion thetaskInopfubt limemagiesshedsubject-drOivuresngeneration.WTeexdtuealm Invoenrs-ion
stratetheeffectivenessofArtiFadeinremovingartifactsand
addressing distortions in subject reconstruction under both
elpmaxe erom of devreserP in-distributionandout-of-distributionscenarios.
(b)Glasseffectremoval.
Figure 9: Applications. Our WM-model can be applied to
Preserved for more example
removevariousunwantedartifactsintheinputimages,e.g.
stickers,glasseffect,etc.
elpmaxe erom of devreserP
approach produces lower TCLIP than ours, suggesting that
theartifact-freeembeddingeffectivelyimprovesthemodel’s
capabilitytobetterpreservetextinformation(seeFig.8).
5 MoreApplications
We apply our WM-model to more artifact cases, such as
stickersandglasseffects,showcasingitsbroadapplicability.
Sticker removal. In Fig. 9a, we test WM-model on in-
putimagesthatareblemishedbycartoonstickers.Thecar-
toon sticker exhibits randomized dimensions and is posi-
tionedarbitrarilywithineachimage. WM-model caneffec-
tively eliminate any stickers while concurrently addressing
improper stylistic issues encountered during image genera-
tion.References Fu,L.;Zhou,C.;Guo,Q.;Juefei-Xu,F.;Yu,H.;Feng,W.;
Liu,Y.;andWang,S.2021.Auto-exposurefusionforsingle-
Arbel,E.;andHel-Or,H.2010. Shadowremovalusingin-
imageshadowremoval. InCVPR,10571–10580.
tensity surfaces and texture anchor points. IEEE TPAMI,
33(6):1202–1216. Gal, R.; Alaluf, Y.; Atzmon, Y.; Patashnik, O.; Bermano,
A. H.; Chechik, G.; and Cohen-Or, D. 2023. An image is
Avrahami, O.; Aberman, K.; Fried, O.; Cohen-Or, D.; and
worthoneword:Personalizingtext-to-imagegenerationus-
Lischinski, D. 2023. Break-A-Scene: Extracting Multiple
ingtextualinversion. InICLR.
Concepts from a Single Image. In SIGGRAPH Asia 2023
ConferencePapers,1–12. Goodfellow, I.; Pouget-Abadie, J.; Mirza, M.; Xu, B.;
Warde-Farley, D.; Ozair, S.; Courville, A.; and Bengio, Y.
Bahdanau, D.; Cho, K.; and Bengio, Y. 2015. Neural ma-
2014. GenerativeAdversarialNets. InNeurIPS.
chine translation by jointly learning to align and translate.
InICLR. Gu, S.; Chen, D.; Bao, J.; Wen, F.; Zhang, B.; Chen, D.;
Yuan, L.; and Guo, B. 2022. Vector quantized diffusion
Cao, Q.; Shen, L.; Xie, W.; Parkhi, O. M.; and Zisserman,
modelfortext-to-imagesynthesis. InCVPR,10696–10706.
A. 2018. Vggface2: A dataset for recognising faces across
pose and age. In 2018 13th IEEE international conference Guo,L.;Wang,C.;Yang,W.;Huang,S.;Wang,Y.;Pfister,
onautomaticface&gesturerecognition(FG2018),67–74. H.;andWen,B.2023. Shadowdiffusion:Whendegradation
IEEE. priormeetsdiffusionmodelforshadowremoval. InCVPR,
14049–14058.
Cao,Z.;Niu,S.;Zhang,J.;andWang,X.2019. Generative
adversarial networks model for visible watermark removal. Guo,R.;Dai,Q.;andHoiem,D.2011.Single-imageshadow
IETImageProcessing,1783–1789. detectionandremovalusingpairedregions.InCVPR,2033–
2040.
Caron,M.;Touvron,H.;Misra,I.;Je´gou,H.;Mairal,J.;Bo-
janowski, P.; and Joulin, A. 2021. Emerging properties in Ho, J.; Jain, A.; and Abbeel, P. 2020. Denoising diffusion
self-supervisedvisiontransformers. InICCV,9650–9660. probabilisticmodels. InNeurIPS,6840–6851.
Chen, W.; Hu, H.; Li, Y.; Ruiz, N.; Jia, X.; Chang, M.- Hu,E.J.;Shen,Y.;Wallis,P.;Allen-Zhu,Z.;Li,Y.;Wang,
W.;andCohen,W.W.2023. Subject-drivenText-to-Image S.;Wang,L.;andChen,W.2022. LoRA:Low-RankAdap-
Generation via Apprenticeship Learning. In Oh, A.; Neu- tationofLargeLanguageModels. InICLR.
mann,T.;Globerson,A.;Saenko,K.;Hardt,M.;andLevine, Hu,X.;Jiang,Y.;Fu,C.-W.;andHeng,P.-A.2019. Mask-
S., eds., NeurIPS, volume 36, 30286–30305. Curran Asso- shadowgan: Learning to remove shadows from unpaired
ciates,Inc. data. InICCV,2472–2481.
Chen,Z.;Long,C.;Zhang,L.;andXiao,C.2021. Canet:A Huang,C.-H.;andWu,J.-L.2004. Attackingvisiblewater-
context-awarenetworkforshadowremoval.InICCV,4743– markingschemes. IEEETMM,6(1):16–30.
4752.
Jin,Y.;Li,R.;Yang,W.;andTan,R.T.2023. Estimatingre-
Chen,Z.;Zhang,Y.;Gu,J.;Zhang,Y.;Kong,L.;andYuan, flectance layer from a single image: Integrating reflectance
X.2022.CrossAggregationTransformerforImageRestora- guidance and shadow/specular aware learning. In AAAI,
tion. InNeurIPS. 1069–1077.
Cheng, D.; Li, X.; Li, W.-H.; Lu, C.; Li, F.; Zhao, H.; and Kawar,B.;Zada,S.;Lang,O.;Tov,O.;Chang,H.;Dekel,T.;
Zheng,W.-S.2018.Large-scalevisiblewatermarkdetection Mosseri,I.;andIrani,M.2023.Imagic:Text-BasedRealIm-
and removal with deep convolutional networks. In PRCV, ageEditingWithDiffusionModels. InCVPR,6007–6017.
27–40.
Khan, S. H.; Bennamoun, M.; Sohel, F.; and Togneri, R.
Cheng, J.; Wu, F.; Tian, Y.; Wang, L.; and Tao, D. 2020. 2015. Automaticshadowdetectionandremovalfromasin-
Rifegan:Richfeaturegenerationfortext-to-imagesynthesis gleimage. IEEETPAMI,38(3):431–446.
frompriorknowledge. InCVPR,10911–10920.
Kumari,N.;Zhang,B.;Zhang,R.;Shechtman,E.;andZhu,
Dekel,T.;Rubinstein,M.;Liu,C.;andFreeman,W.T.2017. J.-Y. 2023. Multi-concept customization of text-to-image
Ontheeffectivenessofvisiblewatermarks.InCVPR,2146– diffusion. InCVPR,1931–1941.
2154.
Le,H.;andSamaras,D.2019. Shadowremovalviashadow
Ding, B.; Long, C.; Zhang, L.; and Xiao, C. 2019. Ar- imagedecomposition. InICCV,8578–8587.
gan: Attentive recurrent generative adversarial network for
Li,B.;Qi,X.;Lukasiewicz,T.;andTorr,P.2019a. Control-
shadowdetectionandremoval. InICCV,10213–10222.
labletext-to-imagegeneration. InNeurIPS,volume32.
Finlayson,G.;Hordley,S.;Lu,C.;andDrew,M.2006. On
Li,X.;Lu,C.;Cheng,D.;Li,W.-H.;Cao,M.;Liu,B.;Ma,
theremovalofshadowsfromimages. IEEETPAMI,28(1):
J.;andZheng,W.-S.2019b. Towardsphoto-realisticvisible
59–68.
watermark removal with conditional generative adversarial
Finlayson, G. D.; Drew, M. S.; and Lu, C. 2009. Entropy networks. InICIG,345–356.
minimizationforshadowremoval. IJCV,85(1):35–57.
Liang, J.; Niu, L.; Guo, F.; Long, T.; and Zhang, L. 2021.
Finlayson, G. D.; Hordley, S. D.; and Drew, M. S. 2002. Visible watermark removal via self-calibrated localization
Removingshadowsfromimages. InECCV,823–836. andbackgroundrefinement. InACMMM,4426–4434.Liu, Y.; Zhu, Z.; and Bai, X. 2021. WDNet: Watermark- B.; Salimans, T.; et al. 2022. Photorealistic text-to-
DecompositionNetworkforVisibleWatermarkRemoval.In image diffusion models with deep language understanding.
WACV,3685–3693. NeurIPS,35:36479–36494.
Liu,Z.;Yin,H.;Wu,X.;Wu,Z.;Mi,Y.;andWang,S.2021. Shor, Y.; and Lischinski, D. 2008. The shadow meets the
From shadow generation to shadow removal. In CVPR, mask:Pyramid-basedshadowremoval. Comput.Graph.Fo-
4927–4936. rum,27(2):577–586.
Lu, H.; Tunanyan, H.; Wang, K.; Navasardyan, S.; Wang, Song,J.;Meng,C.;andErmon,S.2021.Denoisingdiffusion
Z.; and Shi, H. 2023. Specialist Diffusion: Plug-and-Play implicitmodels. InICLR.
Sample-Efficient Fine-Tuning of Text-to-Image Diffusion VanLe,T.;Phung,H.;Nguyen,T.H.;Dao,Q.;Tran,N.N.;
Models To Learn Any Unseen Style. In CVPR, 14267– andTran,A.2023. Anti-dreambooth:Protectingusersfrom
14276. personalizedtext-to-imagesynthesis. InICCV,2116–2127.
Mirza, M.; and Osindero, S. 2014. Conditional generative Vaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,
adversarialnets. arXiv:1411.1784. L.;Gomez,A.N.;Kaiser,Ł.;andPolosukhin,I.2017. At-
tentionisallyouneed. InNeurIPS,5998––6008.
Mou, C.; Wang, Q.;and Zhang, J. 2022. Deep generalized
unfoldingnetworksforimagerestoration. InCVPR,17399– Wang, J.; Li, X.; and Yang, J. 2018. Stacked conditional
17410. generativeadversarialnetworksforjointlylearningshadow
detectionandshadowremoval. InCVPR,1788–1797.
Nichol, A. Q.; Dhariwal, P.; Ramesh, A.; Shyam, P.;
Mishkin,P.;McGrew,B.;Sutskever,I.;andChen,M.2022. Wang, Z.; Cun, X.; Bao, J.; Zhou, W.; Liu, J.; and Li, H.
Glide:Towardsphotorealisticimagegenerationandediting 2022. Uformer: A general u-shaped transformer for image
withtext-guideddiffusionmodels. InICML,16784–16804. restoration. InCVPR,17683–17693.
Pei, S.-C.; and Zeng, Y.-C. 2006. A novel image recovery Xiao, C.; She, R.; Xiao, D.; and Ma, K.-L. 2013a. Fast
algorithmforvisiblewatermarkedimages. IEEETrans.Inf. shadow removal using adaptive multi-scale illumination
ForensicsSecur.,1(4):543–550. transfer. Comput.Graph.Forum,32(8):207–218.
Xiao, C.; Xiao, D.; Zhang, L.; and Chen, L. 2013b. Effi-
Qiao, T.; Zhang, J.; Xu, D.; and Tao, D. 2019. Mirror-
cient shadow removal using subregion matching illumina-
gan:Learningtext-to-imagegenerationbyredescription. In
tiontransfer. Comput.Graph.Forum,32(7):421–430.
CVPR,1505–1514.
Xu, C.; Lu, Y.; and Zhou, Y. 2017. An automatic visible
Qin,C.;He,Z.;Yao,H.;Cao,F.;andGao,L.2018. Visible
watermark removal technique using image inpainting algo-
watermarkremovalschemebasedonreversibledatahiding
rithms. InICSAI,1152–1157.
andimageinpainting. SignalProcess.ImageCommun.,60:
160–172. Xu, T.; Zhang, P.; Huang, Q.; Zhang, H.; Gan, Z.; Huang,
X.; and He, X. 2018. Attngan: Fine-grained text to image
Radford,A.;Kim,J.W.;Hallacy,C.;Ramesh,A.;Goh,G.;
generationwithattentionalgenerativeadversarialnetworks.
Agarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;
InCVPR,1316–1324.
etal.2021.Learningtransferablevisualmodelsfromnatural
languagesupervision. InICML,8748–8763. Yin,G.;Liu,B.;Sheng,L.;Yu,N.;Wang,X.;andShao,J.
2019.Semanticsdisentanglingfortext-to-imagegeneration.
Ramesh,A.;Dhariwal,P.;Nichol,A.;Chu,C.;andChen,M.
InCVPR,2327–2336.
2022. Hierarchical text-conditional image generation with
Zamir, S. W.; Arora, A.; Khan, S.; Hayat, M.; Khan, F. S.;
cliplatents. arXiv:2204.06125.
andYang,M.-H.2022. Restormer:Efficienttransformerfor
Reed, S.; Akata, Z.; Yan, X.; Logeswaran, L.; Schiele, B.;
high-resolutionimagerestoration. InCVPR,5728–5739.
andLee,H.2016. Generativeadversarialtexttoimagesyn-
Zamir, S. W.; Arora, A.; Khan, S.; Hayat, M.; Khan, F. S.;
thesis. InICML,1060–1069.PMLR.
Yang, M.-H.; and Shao, L. 2021. Multi-stage progressive
Rombach,R.;Blattmann,A.;Lorenz,D.;Esser,P.;andOm-
imagerestoration. InCVPR,14821–14831.
mer, B. 2022. High-resolution image synthesis with latent
Zhang, H.; Koh, J. Y.; Baldridge, J.; Lee, H.; and Yang, Y.
diffusionmodels. InCVPR,10684–10695.
2021. Cross-modal contrastive learning for text-to-image
Ronneberger,O.;Fischer,P.;andBrox,T.2015.U-net:Con- generation. InCVPR,833–842.
volutionalnetworksforbiomedicalimagesegmentation. In
Zhang, H.; Xu, T.; Li, H.; Zhang, S.; Wang, X.; Huang,
MICCAI,234–241.
X.; and Metaxas, D. N. 2017. Stackgan: Text to photo-
Ruan, S.; Zhang, Y.; Zhang, K.; Fan, Y.; Tang, F.; Liu, Q.; realisticimagesynthesiswithstackedgenerativeadversarial
andChen,E.2021. Dae-gan:Dynamicaspect-awareganfor networks. InICCV,5907–5915.
text-to-imagesynthesis. InICCV,13960–13969.
Zhang,H.;Xu,T.;Li,H.;Zhang,S.;Wang,X.;Huang,X.;
Ruiz,N.;Li,Y.;Jampani,V.;Pritch,Y.;Rubinstein,M.;and andMetaxas,D.N.2018. Stackgan++:Realisticimagesyn-
Aberman,K.2023. Dreambooth:Finetuningtext-to-image thesis with stacked generative adversarial networks. IEEE
diffusion models for subject-driven generation. In CVPR, TPAMI,41(8):1947–1962.
22500–22510. Zhang,L.;Rao,A.;andAgrawala,M.2023. AddingCondi-
Saharia,C.;Chan,W.;Saxena,S.;Li,L.;Whang,J.;Denton, tionalControltoText-to-ImageDiffusionModels. InICCV,
E. L.; Ghasemipour, K.; Gontijo Lopes, R.; Karagol Ayan, 3836–3847.Zhang,L.;Zhang,Q.;andXiao,C.2015.ShadowRemover:
ImageShadowRemovalBasedonIlluminationRecovering
Optimization. IEEETIP,24(11):4623–4636.
Zhang, Z.; Xie, Y.; and Yang, L. 2018. Photographic text-
to-image synthesis with a hierarchically-nested adversarial
network. InCVPR,6199–6208.
Zhu, M.; Pan, P.; Chen, W.; and Yang, Y. 2019. Dm-gan:
Dynamicmemorygenerativeadversarialnetworksfortext-
to-imagesynthesis. InCVPR,5802–5810.
Zhu,Y.;Huang,J.;Fu,X.;Zhao,F.;Sun,Q.;andZha,Z.-J.
2022. BijectiveMappingNetworkforShadowRemoval. In
CVPR,5627–5636.A TrainingDatasetDetails E AnalysisofTrainingDatasetSize
Our training dataset consists of 20 training subjects, used Weconductananalysistoinvestigatetheimpactofthenum-
for the fine-tuning stage of our ArtiFade models. We show beroftrainingsubjects(i.e.,thesizeofthetrainingdataset)
anexampleimageofeachsubjectinFig.10.InFig.11,we on the performance of our model. We utilize the same set
showcaseseveralunblemishedimagesalongsidetheircorre- ofartifactsL WM = 10,asdescribedinMethodinthemain
sponding blemished versions, each featuring one of the 10 paper.Weconstructblemishedtrainingdatasetsinfourdif-
watermarktypes. ferent sizes: (1) with 5 subjects, (2) with 10 subjects, (3)
with15subjects,and(4)with20subjects.Wegenerate50,
100,150,and200blemisheddatasetsforeachofthesecases.
Subsequently, we fine-tune four distinct ArtiFade models,
eachwith16ktrainingsteps.
Wecomparethemodelstrainedusingdifferentdatasizes
under the in-distribution scenario (see Fig. 15a) and under
theout-of-distributionscenario(seeFig.15b).Wenotethat
when the number of training subjects is less than 15, IDINO
and TCLIP are relatively lower than the other two cases in
both ID and OOD scenarios. This observation can be at-
tributedtoasignificantlikelihoodofsubjectorbackground
overfitting during the reconstruction and image synthesis
processes,asvisuallyillustratedinFig.16andFig.17.How-
ever, as the number of training subjects reaches or exceeds
15, we observe a convergence in the values of IDINO and
TCLIP,indicatingareductioninsubjectoverfitting.Regard-
ingRDINO,wenotethatallcasesexhibitvaluesgreaterthan
one,withaslightlyincreasingtrendasthenumberoftrain-
ingsubjectsrises.
Figure 10: Examples of unblemished training images. We
F FailureCases
showatotalof20images,eachcontainingonedistinctsub-
ject. We present several failure cases when applying ArtiFade
basedonTextualInversion.Wedemonstratethelimitations
B TestDatasetDetails of our WM-model in Fig. 18. Despite the model’s ability
InFig.12,weillustrateourWM-ID-TESTwatermarktypes to eliminate watermarks, we still encounter issues with in-
(seethefirstrow)andWM-OOD-TESTwatermarktypes(see correctsubjectcolor,asshowninFig.18a,whicharisesdue
thesecondrow).TheWM-ID-TESTwatermarksarechosen to the influence of the watermark color. We also encounter
incorrectsubjectidentityinsomecases,asdemonstratedin
from the training watermarks displayed in Fig. 11. On the
other hand, the WM-OOD-TEST watermarks differ in font Fig.18b.Onepossiblereasonisthatthewatermarkssignifi-
cantlycontaminatetheimages,causingthelearningprocess
size,orientation,content,orcolorfromallthetrainingwa-
of embedding to focus on the contaminated visual appear-
termarkspresentedinFig.11.
ance instead of the intact subject. Another failure case is
subject overfitting, as shown in Fig. 18c. In this case, the
C AnalysisofWatermarkDensity
constructed subject overfits with a similar subject type that
InFig.13,wepresentresultstoillustratetheimpactofvary- appearsinthetrainingdataset.Thisproblemoccursbecause
ingwatermarkdensities(i.e.,varyingqualities),highlighting the blemished embedding of the testing subject closely re-
therobustabilityofour WM-model toremovewatermarks sembles some blemished embeddings of the training sub-
underallconditions. jects. Surprisingly, we find those problems can be solved
by using ArtiFade based on DreamBooth, which is men-
D AnalysisofUnblemishedImageRatio tioned in Sec. 4.5. Therefore, we recommend using Arti-
Fade based on DreamBooth when encountering the limita-
We employ our WM-model to evaluate the performance
tionsmentionedabove.
when the input images contain different proportions of un-
blemishedimages.Wetestour WM-model andTextualIn-
G AdditionalComparisonwithTextual
version on five ratios of unblemished images: 100%, 75%,
Inversion
50%,25%,and0%.TheresultsareshowninFig.14.
Notably,evenwhenthereisonlyoneblemishedimagein WeusethesametrainingsubjectswithN=20fromSec.3.3
thesecondcolumnexample,theimpactonTextualInversion totrainanArtiFademodelnamedRC-modelusingredcir-
isalreadyevident,whichdeterioratesastheratiodecreases. cle artifacts. For the training set of RC-model, due to the
Instead,ourmethodeffectivelyeliminatesartifactsinallset- simplicityofredcircles,weonlysynthesizeasingleblem-
tingsofunblemishedimageratio,demonstratingitsversatil- ished subset (i.e., L = 1) for each subject, deriving 20
RC
ityinreal-lifescenarios. blemished subsets in total. We augment each image with aUnblemished image Blemished images
Figure11:Examplesofthetrainingdataset:unblemishedimagesandtheircorrespondingblemishedimages.ID
1
RC-test
Method
ID
IDINO RDINO ICLIP RCLIP TCLIP
2
TI(unblemished) 0.488 1.021 0.730 1.077 0.283
TI(blemished) 0.406 0.990 0.672 1.042 0.284
Ours(RC-model) 0.476 1.013 0.722 1.065 0.285
Ours(WM-model) 0.474 1.006 0.727 1.063 0.282
OOD
Table5:QuantitativeresultsofRC-test.
Figure 12: Example of test watermark types. The first row
displays the WM-ID-TEST, while the second row presents datasets, we only fine-tune RC-model for 8k steps. We
theWM-OOD-TEST. further introduce RC-test, which applies only one type
of artifact (i.e., red circle) to our 16 test subjects, resulting
in 16 test sets. We test both RC-model and WM-model
Input images Ours Textual Inversion
on RC-test. The quantitative and qualitative results are
showninTab.5andFig.19,respectively.
Quantitative results analysis. From Tab. 5, we can ob-
serve that both RC-model and WM-model yield higher
results in nearly all cases than Textual Inversion (Gal et al.
2023)withblemishedinputs,showingthecapabilityofour
models to eliminate artifacts and generate subjects with
higher fidelity. It is important to note that the RC-test is
considered out-of-distribution with respect to WM-model.
Nevertheless, the metrics produced by WM-model remain
comparabletothoseofRC-model,withaminordifference
observed.Theseresultsprovideadditionalevidencesupport-
ingthegeneralizabilityofour WM-model.
Qualitative results analysis. As illustrated in Fig. 19,
Textual Inversion struggles with accurate color reconstruc-
tion. It also showcases subject distortions and introduces
red-circle-like artifacts during image generation when us-
ingblemishedembeddings.Incontrast,ourRC-model(see
Fig. 19a) and WM-model (see Fig. 19b) are capable of
Figure 13: Varying qualities of input images. Our method
generating high-quality images that accurately reconstruct
(WM-model)canbeusedtoremovewatermarkswhenin-
thecolorandidentitiesofsubjectswithoutanyinterference
putimagesareofanyquality.
fromartifactsduringtheimagesynthesis.
Ratio 100% 75% 50% 25% 0% H AdditionalQualitativeComparisons
WepresentadditionalqualitativeresultscomparingourAr-
Input
images tiFade models with Textual Inversion (Gal et al. 2023) and
DreamBooth (Ruiz et al. 2023) in Fig. 20. We employ
WM-model andArtiFadebasedonDreamBoothmentioned
in Sec. 4.5. Textual Inversion generates images with dis-
Textual
Inversion torted subjects and backgrounds contaminated by water-
marks, whereas DreamBooth can effectively capture intri-
catesubjectdetailsandaccuratelyreproducewatermarkpat-
terns. In contrast, our models (i.e., TI-based and DB-based
Ours ArtiFade) generate images devoid of watermark pollution
with correct subject identities for both in-distribution (see
the first three rows in Fig. 20) and out-of-distribution (see
Figure14:Comparisonbetweendifferentratiosofunblem- the last two rows in Fig. 20) cases. Notably, our method
ishedimages.ArtiFadecanperformwellunderanyscenar- based on DreamBooth preserves the high fidelity and finer
ioswithdifferentratiosofunblemishedimages. detail reconstruction benefits of vanilla DreamBooth, even
inthecontextofblemishedsubject-drivengeneration.
In Fig. 21, we show qualitative results for subjects with
redcirclemarkthatisrandomlyscaledandpositionedonthe complexfeatures(e.g.,humanfaces)usingourmodels,Tex-
sourceimage.ConsideringthesmallscaleofRC-model’s tual Inversion, DreamBooth and Break-a-Scene (Avrahami2 ECCV2024Submission#*****
Author Guidelines for ECCV Submission
001 001
002 AnonymousECCV2024Submission 002
003 PaperID#***** 003
0.34 0.36
0.33 0.35
0.32 0.34
0.31 0.33
0.3 0.32
0.29 0.31
0.28 0.3
5 10 15 20 5 10 15 20
1.3 1.24
1.29
1.23
1.28
1.22
1.27
1.26 1.21
1.25 1.2
5 10 15 20 5 10 15 20
0.29 0.29
0.28 0.28
0.27 0.27
0.26 0.26
5 10 15 20 5 10 15 20
Numberoftrainingsubjects Numberoftrainingsubjects
(a)ID (b)OOD
Fig.1: Numberoftrainingsubjects-ID. Fig.2: Number of training subjects - OOD.
Figure15:Analysisofthenumberoftrainingsubjects.
Input images (1) (2) (3) (4) only images. However, we find that Break-a-scene fails to
separatehumansfromartifacts,resultinginpollutedimages.
Asaresult,ourmethods(i.e.,TI-basedandDB-basedArti-
Fade) consistently surpass Textual Inversion, DreamBooth,
and Break-a-Scene, achieving high-quality image genera-
tion of complex data in in-distribution cases, as shown in
'
[    '   ] the first two rows of Fig. 21, and out-of-distribution cases,
asillustratedinthelastrowofFig.21.
I MoreApplications
with a beautiful sunset We explore more applications of our WM-model, demon-
' stratingitsversatilitybeyondwatermarkremoval.Asshown
Figure16:Qualitativeresul[t  s   '  o  ]fdifferentnumberoftraining
in Fig. 22, our model exhibits the capability to effectively
subjects-ID.
eliminate unwanted artifacts from images, enhancing their
visual quality. Furthermore, our model showcases the abil-
Input images (1) (2) (3) (4) ity to recover incorrect image styles induced by artifacts,
therebyrestoringtheintendedstyleoftheimages.
J SocialImpact
Our research addresses the emerging challenge of generat-
in the snow ingcontentfromimageswithembeddedwatermarks,asce-
'
[    '   ] nario we term blemished subject-driven generation. Users
often source images from the internet, some of which may
containwatermarksintendedtoprotecttheoriginalauthor’s
copyright and identity. However, our method is capable of
removingvarioustypesofwatermarks,potentiallycompro-
at the beach
' mising the authorship and copyright protection. This could
Figure17:Qualitativeresultso[  f   d'   i]fferentnumberoftraining leadtoincreasedinstancesofimagepiracyandthegenera-
subjects-OOD. tionofillicitcontent.Hence,weadvocateforlegalcompli-
anceandtheimplementationofusagerestrictionstogovern
et al. 2023). Break-a-Scene can separate multiple subjects thedeploymentofourtechniqueandsubsequentmodelsin
insideoneimage.WeuseBreak-a-scenetogeneratehuman- thefuture.
ONIDI
ONIDR
PILCT
ONIDI
ONIDR
PILCTInput images Ours (TI-based)Ours (DB-based)Textual Inversion DreamBooth
Input images Ours (TI-based)Ours (DB-based)Textual Inversion DreamBooth
Input images Ours (TI-based)Ours (DB-based)Textual Inversion DreamBooth
(a)Incorrectsubjectcolor
(b)Incorrectsubjectidentity
(c)Subjectoverfitting
Figure 18: Failure cases of ArtiFade based on Textual Inversion. We observe three main types of failure cases of our
WM-model: (a) incorrect subject color, (b) incorrect subject identity, and (c) subject overfitting. However, those limitations
canberesolvedbyusingArtiFadewithDreamBooth-basedfine-tuning.Input images Ours Textual Inversion Ours Textual Inversion
at the beach
'  '
[      ] [      ]
with a beautiful sunset
'  '
(a [)  R   C  -  ]modelonRC-test. [      ]
Input images Ours Textual Inversion Ours Textual Inversion
at the beach
'  '
[      ] [      ]
in the jungle
'  '
(b) [  W   M  -  ]model onRC-test. [      ]
Figure19:QualitativeresultsofRC-test.Ourmodelsconsistentlyoutputhigh-qualityandartifact-freeimagescomparedto
TextualInversion.Input images Ours (TI-based)Ours (DB-based)Textual Inversion DreamBooth
′
[      ]
′
[      ]
′
[      ]
′
[      ]
′
Figure20:Additionalqualitative[  co   m   p]arisons.
Ours Textual Inversion
at the beach
′
[      ]
in the street
′
[      ]
in the movie theater
′
[      ]Input images Ours (TI-based)Ours (DB-based)Textual Inversion DreamBooth Break-a-scene
′
[      ]
′
[      ]
′
Figure21:Additionalqualitativecomp[a  r  i  s  o  ]ns-HumanFaces.
DI-MW
DOO-MWInput images Ours Textual Inversion
Figure22:Moreapplications.Our WM-model canbeusedtoeliminatevariousstickersandfixtheincorrectimagestyle.