Lexicon3D: Probing Visual Foundation Models for
Complex 3D Scene Understanding
YunzeMan1 ShuhongZheng1 ZhipengBao2
MartialHebert2 Liang-YanGui1 Yu-XiongWang1
1UniversityofIllinoisUrbana-Champaign 2CarnegieMellonUniversity
https://yunzeman.github.io/Lexicon3D
Abstract
Complex3Dsceneunderstandinghasgainedincreasingattention,withsceneen-
codingstrategiesplayingacrucialroleinthissuccess. However,theoptimalscene
encodingstrategiesforvariousscenariosremainunclear,particularlycomparedto
theirimage-basedcounterparts. Toaddressthisissue,wepresentacomprehensive
study that probes various visual encoding models for 3D scene understanding,
identifyingthestrengthsandlimitationsofeachmodelacrossdifferentscenarios.
Ourevaluationspanssevenvisionfoundationencoders,includingimage-based,
video-based,and3Dfoundationmodels. Weevaluatethesemodelsinfourtasks:
Vision-LanguageSceneReasoning,VisualGrounding,Segmentation,andRegis-
tration,eachfocusingondifferentaspectsofsceneunderstanding. Ourevaluations
yield key findings: DINOv2 demonstrates superior performance, video models
excelinobject-leveltasks,diffusionmodelsbenefitgeometrictasks,andlanguage-
pretrainedmodelsshowunexpectedlimitationsinlanguage-relatedtasks. These
insightschallengesomeconventionalunderstandings,providenovelperspectives
onleveragingvisualfoundationmodels,andhighlighttheneedformoreflexible
encoderselectioninfuturevision-languageandscene-understandingtasks.
1 Introduction
Recently, complex 3D scene understanding has emerged as a pivotal area in computer vision,
encompassing tasks such as scene generation [24, 25, 26, 33, 74], reasoning [5, 35, 52, 55], and
interaction[36,108]. Leveraginglarge-scalevisionfoundationmodels,approacheslike[42,64,68,
84,91]haveachievedpromisingresults,therebyenablingawiderangeofreal-worldapplications,
fromautonomousdriving[54,75,79,112],robotics[57,108],tomulti-modalagents[1,78].
Whilenumerousstudies[6,67,99]haveprovidedguidanceontheuseofvisionfoundationmodels
for2Dimage-basedtasks,thestrategiesfor3Dscenariosremainunclear. Asystematicunderstanding
of complex real-world scenarios involves not only semantic and depth awareness [6], which is
possibletoevaluatewithinthe2Ddomain,butalsogeometricawarenessandtheabilitytoalignwith
multi-modalinformationforreasoningandgroundingtasks. Toaddressthisgap,ourworkevaluates
theuseofdifferenttypesofvisualfoundationmodelsforcomplexsceneunderstandingandseeksto
identifythestrengthsandlimitationsofeachmodelindifferentscenarios. Ultimately,thisstudyaims
tocontributetothedevelopmentofmoreeffectiveandefficientsceneunderstandingsystems.
Concretely,weaimtoaddressseveralkeyquestions. First,giventhatmostvisionfoundationmodels
are trained on image or video data, we want to determine whether 2D foundation models can
effectivelyinterpret3Dscenes. Second,sincevideomodelsinherentlycontaintemporalinformation
thatcapturesaspectsofthe3Dstructureaswell,weinvestigatewhethertheyleadtobetter3Dfeature
Preprint.Underreview.
4202
peS
5
]VC.sc[
1v75730.9042:viXraVisual Foundation Models for Scene Understanding
By Input Modality By Pretraining Task
DINOv2 .lla ceR)
Image Self-supervised Learning LSeg n o(
…
E C.g o. n, M traA sE t,
i
vD ei s leti all ra nt ii no gn, StC aL bI lP
e
itartsig
eR
Diffusion Semantic Segmentation (mIoU)
Video Language-guided Learning
V-JEPA
g).ccA
(
E.g., CLIP-family n
id
Stable Video n u
o
3D Points Generation Diffusion rG
la
u
E.g., Diffusion-based Family Swin3D
siV
Vision-Language Reasoning (BLEU-4)
Figure1: Evaluationsettingsandmajorresultsofdifferentvisionfoundationmodelsforcomplex3Dscene
understanding. Weprobevisualfoundationmodelsofdifferentinputmodalitiesandpretrainingobjectives,
assessingtheirperformanceonmulti-modalscenereasoning,grounding,segmentation,andregistrationtasks.
representationscomparedtoimagemodels. Finally,weseektoidentifythemostsuitablescenarios
fordifferentfoundationmodelstrainedundervarioussettings.
Toanswerthesequestions,wedesignaunifiedparadigmtosystematicallyprobevisualencoding
models for complex 3D scene understanding from different perspectives. Our evaluation spans
sevenvisionfoundationmodelsinimages,videos,and3D-basedones,asshowninTable1. Our
evaluationsareconductedamongfourdiversetasks: Vision-LanguageSceneReasoningassesses
the model’s ability to reason about scenes based on textual descriptions, evaluating scene-level
representation; Visual Grounding tests the model’s capacity to associate language with specific
objectswithinascene,reflectingobject-levelrepresentation;Segmentationevaluatesthemodel’s
ability to assign semantic labels to each pixel, assessing semantic understanding; Registration
measurestheperformanceofaligningdifferentviewsofascene,testinggeometriccapacity. Through
thesetasks,ouraimistoexplorethestrengthsandweaknessesofdifferentvisionfoundationmodels
in3Dsceneunderstanding,providinginsightsintotheirapplicabilityinvariousscenarios. Withthe
majorresultsdemonstratedinFigure1,ourkeyfindingsinclude:
• Imageorvideofoundationmodelsachievepromisingresultsfor3Dsceneunderstanding.
Among them, DINOv2 [58] demonstrates the best overall performance, showing strong
generalizability and flexibility, which is consistent with the observation in 2D [6]. Our
evaluationfurtherverifiesitscapabilityinglobalandobject-level3Dvision-languagetasks.
Itcanserveasageneralbackbonefor3Dsceneunderstanding.
• Videomodels,benefitingfromtemporallycontinuousinputframes,excelinobject-leveland
geometricunderstandingtasksbydistinguishinginstancesofthesamesemanticsinascene.
• Visualencoderspretrainedwithlanguageguidancedonotnecessarilyperformwellinother
language-relatedevaluationtasks,challengingthecommonpracticeofusingsuchmodelsas
defaultencodersforvision-languagereasoningtasks.
• Generativepretrainedmodels,beyondtheirwell-knownsemanticcapacity,alsoexcelin
geometricalunderstanding,offeringnewpossibilitiesforsceneunderstanding.
WenameourworkasLexicon3D,aunifiedprobingarchitectureandthefirstcomprehensiveevalua-
tionof3Dsceneunderstandingwithvisualfoundationmodels. Thekeyfindingswehaveachieved
above,inconjunctionwithotherinterestingobservations,suggestexploringmoreflexibleencoder
selectionsinfuturevision-languagetaskstooptimizeperformanceandgeneralization.
2 RelatedWork
Ourworkiscloselyrelatedtomethodsthatfocusonextractionoffeaturesfromimages,videos,and
3Dassets,aswellaslearningjointspacesforvision-languagefusion.Alargebodyofrecentliterature
has explored the representation learning for multi-modal visual inputs and their complementary
performance in image understanding. In contrast, our paper presents a comprehensive analysis
oftheuseofpretrainedvisualencodersforzero-shot3Dsceneunderstanding. Tothebestofour
2Model InputModality Architecture Supervision Dataset
DINOv2[58] ViT-L/14 SSL LVD-142M
LSeg[44] ViT-L/16 VLM LSeg-7Mix
Image
CLIP[65] ViT-L/14 VLM WIT-400M
StableDiffusion[70] UNet Generation LAION
V-JEPA[11] ViT-L/16 SSL VideoMix2M
Video
StableVideoDiffusion[12] UNet Generation LVD-F
Swin3D[93] 3DPoints Swin3D-L Annotation Structure3D
Table1:Detailsofthesevenevaluatedvisualencodingmodels.
knowledge,wearethefirsttoexaminepretrainedvideoencoderson3Dsceneunderstandingtasks
andtocompareimage,video,and3Dpointencodingstrategiesinthiscontext.
Image self-supervised learning. In recent years, learning robust and generalizable pretrained
imagerepresentationshasbecomeaprevalentresearchdirectionincomputervisionandmulti-modal
research. Onelineofworkfocusesonlearningtask-agnosticimagefeaturesusingself-supervised
learning (SSL) signals, which include pretext tasks such as colorization [100], inpainting [62],
transformationprediction[27], andself-distillation[14,18,19,29,30]. Therecentdevelopment
ofthepatch-basedimagetokenizer,ViT[22],hasalsoledtotheemergenceofmaskautoencoder
architectures(MAE)forfeatureextraction[8,31,111]. Ofparticularinterest,DINOv2[58],com-
bining a masked-image modeling loss and an invariance-based self-distillation loss, has become
oneofthemostscalableandcompetitiveself-supervisedlearningarchitecturesusingonlyimage
signals. Anotherlineofworkproposeslearningimagefeatureswithtextguidance,i.e.,usingtextual
descriptionstoguidethepretrainingoftheimageencoders[38,53]. Buildinguponthepowerful
image-textencoderCLIP[65],LSeg[44]andBLIP[45,46]extendtheimagepretrainingobjective
tomorecomplexvisualperceptiontasksbyincorporatingpixel-levelsemanticunderstandingand
encouragingbetteralignmentwithlargelanguagemodels(LLMs)[13,66,102,103],respectively.
Video and 3D representation learning. Self-supervised representation learning has also been
exploredinthecontextofvideosand3Dpointclouds. ExtendingthesuccessoftheCLIParchitec-
ture[65]fromimagestovideos,abodyofworkproposestopretrainavideoencoderbyaligning
thefeaturespacewithtextualguidanceextractedfromvideocaptions[3,85,89,97]. Otherpretext
tasksusedinvideorepresentationlearningincludenextframeprediction[10]andMAE[28,80,83].
Amongthem,Bardesetal.[11]adapttheMAE-inspiredjointembeddingpredictionarchitecture
(JEPA) [4, 43] to the spatio-temporal domain, achieving state-of-the-art performance on a wide
spectrumofvideoandimagetasks. Despitetheextensiveresearchon2Dvisualfoundationencoders,
pretrained models for 3D point clouds are significantly fewer due to the lack of large-scale 3D
datasets. Existingworkhasexploredcontrastivepretraining[37,88,105]andmaskedsignalmodel-
ing[48,59,87,92,96,101]forpointrepresentationlearning. Recently,benefitingfromtherapid
advancementof3Ddatarenderingandalargesyntheticdataset[109],Swin3D[93]hasoutperformed
otherpretrainingmethodsbyasignificantmarginusingsupervisedpretraining.
Generationandmixtureofexperts(MoE)forfeatureextraction. Withthesuccessofdiffusion-
based generative models [32, 70, 76], a line of research has begun to explore their role in image
perceptiontasks. Thesemethodsextractfeaturemapsorattentionmapsofagivenimagefromthe
U-Net architectures of diffusion models and perform various downstream tasks, including depth
estimation [23, 71, 107], semantic segmentation [9, 51, 56, 86, 107], object detection [17], and
panoptic segmentation [90]. Another line of work [60, 98, 99] investigates the complementary
natureofdifferentembeddingsextractedbymultiplefoundationbackbonesandtheirjointeffecton
downstreamtasks[6,67]. However,theseinvestigationshavebeenlimitedtothe2Ddomain,leaving
thepotentialofleveragingpretrainedencodersforperceptionandreasoningtasksincomplex3D
scenes[5,21,34,35,40,52,55,63,113]largelyunexplored.
3 ProbingVisualEncodersforSceneUnderstanding
TheobjectiveofourLexicon3Distoevaluatedifferentvisualfoundationmodelsincomplexscene
understandingtasks. Wefirstconstructaunifiedarchitecturecapableofprobingdifferentvisualfoun-
dationmodelsonaspectrumofdownstreamtasks. Then,webreakdownthe3Dsceneunderstanding
3Image Video 3D Points
Evaluation Task Heads
Vision-Language Reasoning
…
(Scene-level) 3D VQA
Probed Vision Foundation Models (Object-level) Grounding
Semantic Understanding
Semantic Segmentation
Geometric Understanding
Registration
Multi-view 3D Projection
Posed Video time 3D Feature Field
Figure2:Ourunifiedprobingframeworktoevaluatevisualencodingmodelsonvarioustasks.
Input Scene DINOv2 LSeg CLIP SD SVD V-JEPA Swin3D
Figure3: VisualizationsofextractedscenefeaturesfromdifferentvisualencodersusingPCA.Theclear
distinctionbetweencolorsandpatternsdemonstratesthebehaviorsofdifferentmodels.
taskintofoursub-tasks,including(1)vision-languagereasoning,(2)visualgrounding,(3)semantic
understanding,and(4)geometricunderstanding,foramoredetailedevaluation.
3.1 AUnifiedProbingFramework
Wedesignaunifiedframework,asshowninFigure2,toextractfeaturesfromdifferentfoundation
models, construct a 3D feature embedding as scene embeddings, and evaluate them on multiple
downstreamtasks.Foracomplexindoorscene,existingworkusuallyrepresentsitwithacombination
of 2D and 3D modalities. For realistic scenarios [15, 20, 94], videos are usually first captured
with handheld cameras and then 3D points are obtained from reconstruction algorithms such as
COLMAP[72]. Fordigitalandsyntheticscenarios[69,109],3Dassetsaredesignedandgenerated
first, beforeimagesand/orvideosarerenderedwithinthecreatedspace. Givenacomplexscene
representedinposedimages,videos,and3Dpointclouds,weextracttheirfeatureembeddingswitha
collectionofvisionfoundationmodels. Forimageandvideo-basedmodels,weprojecttheirfeatures
intothe3Dspaceforsubsequent3Dsceneevaluationtaskswithamulti-view3Dprojectionmodule.
Following [21, 34, 35, 63], for a point cloud P, this module produces features f for each point
p
p∈Pgivenimagefeaturesf andtheposeandcamerainformationK,R. Wefirstprojectallpoints
ontotheimageplanetoobtaintheircorrespondingpixelfeatures. Concretely,forapointp,weobtain
itsprojectedpixeluontheimageiwith
u˜ =K R p˜, u˜,p˜ representhomogeneouscoordinatesofu,p. (1)
i i
Inaddition,weuseanindicatorfunctionI(p,i)torepresentwhetherapointpisvisibleintheimage
ofthei-thframe. Afterfindingcorrespondingpixelsofthegivenpointinallimageframes,weuse
meanpoolingasanaggregationfunctionϕtofuseallpixelfeaturestoformthepointfeaturef .
p
AssumingthereareMimagesintotal,theprojectionandaggregationprocessisrepresentedas:
f =ϕM (I(p,i)·f (K R p˜)). (2)
p i=1 i i i
Afterprojection,weobtain3Dfeaturefieldsrepresentedaspointcloudfeatureembeddingsforeach
visualfoundationmodel. Weusetheseasinputtotheshallowprobingheadstoevaluatevarious
4ScanQA(highermeansbetterforallmetrics) SQA3D(highermeansbetterforallmetrics)
Model BLEU-1 BLEU-4 METEOR ROUGE CIDEr EM-1 BLEU-1 METEOR ROUGE CIDEr
3D-LLM[35](forref.) 39.3 12.0 14.5 35.7 69.4 48.1 47.3 35.2 48.6 124.5
DINOv2 39.2 13.4 15.3 36.8 73.2 50.1 49.5 35.6 50.7 129.1
LSeg 36.8 11.5 14.6 36.0 71.0 47.4 46.5 33.2 47.8 122.5
CLIP 36.4 10.7 14.4 36.0 70.3 48.1 47.3 34.6 48.6 124.5
StableDiffusion 35.5 11.7 14.1 34.9 68.2 47.7 47.2 33.6 48.3 124.0
V-JEPA 37.4 12.1 14.7 36.7 71.4 48.4 48.1 34.8 50.0 125.7
StableVideoDiffusion 38.5 12.5 14.5 35.4 70.6 48.5 47.9 34.4 49.0 127.7
Swin3D 36.1 10.5 13.9 35.4 70.0 48.3 48.0 34.1 47.3 123.9
Table2:Evaluationofvision-languagereasoningonScanQA[5]andSQA3D[52]datasets.Top-2resultsfor
eachmetricareshownin red and green,respectively.Priormethod3D-LLMresultsareshownforreference,
indicatingrelativepositionofourevaluationresultswithrespecttoleadingmodelstrainedonthistask.
Figure4:EvaluationcurvesontheScanQAbenchmark.DINOv2exhibitsclearsuperiorperformance.
downstreamtasks. Tominimizetheeffectofthemodelfinetuningprocess,wefreezetheparameters
fortheencodingmodelstobeevaluated,andonlytunethelinearorshallowprobingheadforall
tasks.
Models. Inthiswork,weprimarilyfocusonevaluatingvisualfoundationmodelsthatarefrequently
leveragedbyrecentcomplexsceneunderstandingandmulti-modalreasoningmodels. Acomplex
scenecanoftenberepresentedinposed2Dimagesandvideosorin3Dpointclouds. Theimageand
videomodalitiessacrificeexplicitgeometryinformation,buttheypreserverichanddensesemantic
andtexturalinformationofascene. Conversely,thepointcloudmodalityofferstheoppositetrade-
offs. Additionally,the2Dmodalitiesbenefitfromstrongfoundationmodelstrainedonvastamounts
ofdata,while3Dpointbackbonesonlyleveragemuchsmallerdatasets.
We categorize visual foundation models into three categories, with an overview of the evaluated
modelsprovidedinTable1. Forimageencoders,weevaluatedDINOv2[58],LSeg[44],CLIP[65],
andStableDiffusion(SD)[70]. Forthevideomodality,weevaluateV-JEPA[11],thestate-of-the-art
videounderstandingmodelsucceedingVideoMAE[80,83]forawidespectrumofperceptionand
reasoningtasks,aswellasStableVideoDiffusion(SVD)[12],avideogenerativemodel. Thelackof
large-scale3Dscene-leveldatasetshindersthedevelopmentofstrongzero-shotgeneralizable3D
foundationmodelsasopposedtotheir2Dcounterparts. However, forcomparison, weevaluated
Swin3D [93], a 3D backbone that achieves leading performance in zero-shot perception tasks in
multipleevaluationdatasetscomparedtoearliermethods[37,88,105]. Swin3Dispretrainedon
Structured3D[109],adataset10timeslargerthanScanNet[20].
Featurevisualization. Figure3visualizesthescenefeaturesextractedbythevisionfoundation
models.Tovisualizeahigh-dimensionalfeaturespacewithCchannels,weapplyprincipalcomponent
analysis(PCA)toreducethefeaturedimensionstothree,normalizethemtorange[0,1],andinterpret
them as RGB color channels. The visualizations reveal several intuitive findings. The image
models,DINOv2andLSeg,demonstratestrongsemanticunderstanding,withLSegexhibitingclearer
discriminationduetoitspixel-levellanguagesemanticguidance. Thediffusion-basedmodels,SD
andSVD,inadditiontotheirsemanticmodeling,excelatpreservingthelocalgeometryandtextures
ofthescenes,becauseofthegeneration-guidedpretraining. Thevideomodels,SVDandV-JEPA,
showcaseauniqueabilitytoidentifydifferentinstancesofthesamesemanticconcepts,suchasthe
twotreesinthefirstsceneandthechairsinbothscenes. The3Dmodel,Swin3D,alsoexhibitsstrong
semanticunderstanding. However,duetolimitedtrainingdataanddomainshift,itsqualityisnoton
parwiththeimagefoundationmodels,despitebeingpretrainedonperfectsemanticannotations.
3.2 Vision-LanguageReasoning
Thevision-languagereasoningtaskrequiresamodeltoengageindialoguesoranswerquestions
aboutglobalunderstandingandlocalconceptsandobjectsrelatedtoagivencomplex3Dindoor
5scene. Following[35,108],weformulatethisasavisual-questionanswering(VQA)taskusingLarge
LanguageModels(LLMs)asthebackbone–givena3Dscenefrommulti-viewimagesandpoint
clouds,andauser-promptquestion,theLLMsareaskedtogeneratetheanswertothequestionin
anauto-regressiveway. Thistaskencompassesuniversallanguage-guidedreasoningofthecomplex
indoorscene,rangingfromgloballayouttolocaldetails.
Datasetsandoptimization. Weevaluatetheperformanceontwochallengingindoor3DVQA
datasets: ScanQA[5]andSQA3D[52]. Followingtheevaluationmethodologyof[5,35,52,55],we
reportthemetricsBLEU[61],ROUGE[47],METEOR[7],andCIDEr[82]. WefinetuneaQ-Former
module[46]toalignfeaturesfromdifferentencoderstotheLLMinputspace. Moredatasetsand
optimizationdetailsareprovidedinthesupplementarymaterial.
Evaluationresults. Table2andFigure4presenttheresultsofourevaluation. Weobservethat
imageandvideoencodersgenerallyoutperformthe3Dpointencoder,withDINOv2achievingthe
bestperformance,followedcloselybyV-JEPAandSVD.Interestingly,wefindthatforLSegand
CLIP,whicharepretrainedbylanguageguidance,theiradvantageinlanguagealignmentdoesnot
translate into superior performance on the LLM-guided VQA task. This finding challenges the
commonpracticeofusinglanguage-pretrainedvisualfoundationmodels[44,45,46,65]asdefault
encodersforLLM-basedvision-languagereasoningtasks. Instead, itsuggeststheimportanceof
consideringawiderrangeofencoders,suchasDINOv2andV-JEPA,tosupportsuchmodels.
3.3 VisualGrounding
Visualgroundingisthetaskoflocatinganobjectina3Dscenebasedonatextdescription. Compared
to the 3D VQA task, visual grounding places a greater emphasis on object-level reasoning and
matchingcapabilities. Thetaskcanbebrokendownintotwosub-tasks: objectdetectionandtarget
discrimination(matchingthetextdescriptionwiththetargetobject). Althoughsomemethodsfocus
on learning models to tackle both tasks [16, 104], others primarily focus on the discrimination
problem[2]byassumingaccesstoground-truthboundingboxes. Forsimplicityandtopreventtask
entanglement,weadoptthelattersettinginourevaluation. Morespecifically,givena3Dscenein
theformofmulti-viewimagesandpointclouds,afree-formlanguagedescriptionofobjects,and
theground-truth3Dboundingboxesofallobjectsinthescene,ourmodel’sobjectiveistofindthe
correctobjectsinthescenethatmatchthelanguagedescription. Webelievethattheobjectdetection
taskrequiressemanticinformationfromthevisualencoder,whichissimilarinnaturetothesemantic
segmentationtaskandwillbeanalyzedinSection3.4.
Forthetargetdiscriminationtask,wefirstobtainthefeatureforeachobjectinthescenebytakingthe
averagepoolingofallpointsinsideitsgroundtruthboundingbox. FollowingMulti3DRefer[104],
weuseaCLIPtextencodertotokenizethetextdescription,andadopttheattentionheadin[104]to
fusethetextandvisualembeddingsfromthepreviousstepsandoutputanobjectscore.
Datasets. WeevaluateontheScanRefer[16]dataset,whichprovides51Ktextdescriptionsof11K
objectsin800ScanNetscenes[20]. Wereportaccuracyforunique,multiple,andoverallcategories,
withuniquereferringtoinstancesthathaveauniquesemanticclassinagivenscene(easier).
Optimization. Themodelistrainedwithacross-entropylossusingtheAdamW[50]optimizer
following[104]. Wetrainourmodelsfor30epochsuntilconvergence.
Evaluation results. Table 3 presents our re-
Model Unique↑ Multiple↑ Overall↑
sults,whichshowthatvideoencodingmodels
demonstratesignificantadvantagesoverimage M3DRef[104](forref.) 88.0 46.1 54.3
and3Dencoders. Theperformancegapprimar- DINOv2 87.0 43.4 52.0
LSeg 88.1 41.2 50.4
ilyliesinthemultiplecategory,indicatingthat
CLIP 86.5 41.6 50.4
thesemodelsexcelatdiscriminatingthecorrect StableDiffusion 86.4 41.9 50.6
objectamongmultipleobjectsofthesamese- V-JEPA 85.6 44.9 52.9
manticcategory. Thiscapabilitylargelystems StableVideoDiffusion 88.0 46.5 54.7
from the temporally continuous input frames, Swin3D 85.7 43.2 51.6
whichprovideinstance-awaremulti-viewcon-
Table3:Evaluationof3DobjectgroundingonScanRe-
sistentguidance. Incomparison,theimageen-
fer[16].Videomodelsexhibitsignificantadvantages.
coderLSeg,withitslanguage-guidedpretrain-
ingfeaturesalignedwithlanguagesemantics,canalsoachievehighaccuracyintheuniquecategory.
However,itsperformancedropssignificantlyinthemultiplecategory.
6RGB GT DINOv2 LSeg CLIP StableDiffusion V-JEPA StableVideoDiffusion Swin3D
Figure5:Visualizationof3DsemanticsegmentationonScanNet[20].Imageencodersobtainbetterperformance.
Insights from vision-language tasks. Our evaluation of vision-language reasoning and visual
groundingrevealsseveralkeyfindings: (1)TheDINOv2unsupervisedimagelearningmodeldemon-
stratesstronggeneralizabilityandflexibilityinglobalandobject-levelvision-languagetasks. (2)
Videoencodersbenefitfromtemporallycontinuousinputframesandlearntodistinguishinstances
ofthesamesemanticsinascene,whichishighlyvaluableforobject-levelunderstandingtasks. (3)
Visualencoderspretrainedwithlanguageguidancedonotnecessarilyleadtostrongperformancein
otherlanguage-relatedevaluationtasks. Thesefindingssuggestexploringamoreflexibleencoder
selectioninfuturevision-languagetaskstooptimizeperformanceandgeneralization.
3.4 SemanticSegmentation
Semanticsegmentationisthetaskofpredictingsemanticlabelsateach3Dposition,whichrequires
fine-grainedsemanticawarenessofthescenes. AsmentionedinSection3.1,alltypesoffeaturesare
unifiedintheformofpointclouds;therefore,semanticlabelsarepredictedforeachpointwithinthe
pointcloudinoursetting. Morespecifically,givena3Dsceneintheformofmulti-viewimagesand
pointclouds,theobjectiveinthistaskistopredictthesemanticlabelforeverypointinthecloud.
Dataset. WeconducttheexperimentsontheScanNet[20]segmentationdatasetwhichhas1,201and
312scenesfortrainingandvalidation,respectively,withatotalof20semanticclassesforevaluation.
Optimization. Tomakethesemanticpredictionperformancebetterreflectthefine-grainedsemantic
understandingcapabilityofdifferentfeatures,weuseasinglelinearlayerfollowedbyaSigmoid
functiontoperformalinearprobetopredicttheprobabilitydistributiony∈RN×C forallthelabels
fromthefoundationmodelfeaturex ∈ RN×d: y = Sigmoid(FC(x)), whereN isthenumberof
pointsineachpointcloud,disthefeaturedimension,andCisthenumberofclassesforsegmentation.
WeadoptthestandardAdamoptimizer[41]withalearningrateof1e-4anduseacross-entropyloss
totrainthelinearlayerfor20epochs.
Evaluationresults. Table4andFigure5demonstrates
Model Acc↑ mAcc↑ mIoU↑
thatimageencodershavebetterperformancethanvideo
GrowSP[106](forref.) 73.5 42.6 31.6
and 3D encoders on 3D semantic segmentation tasks.
DINOv2 82.5 75.4 62.8
The reason is that image encoders like DINOv2 and
LSeg 78.2 58.5 47.5
LSeggaintheirsemanticawarenessduringtrainingwith CLIP 39.7 7.2 3.4
contrastiveobjectivesviaeitherSSLorlanguage-driven StableDiffusion 77.2 55.5 42.6
guidance.Incomparison,videoencodershavetheriskof V-JEPA 58.7 13.2 8.1
StableVideoDiffusion 71.5 40.5 30.4
over-smoothingthemulti-viewinformationduringmulti-
Swin3D 78.0 44.8 35.2
frameintegration,whichmayharmthefine-grainedse-
Table4:Resultsofsemanticsegmentation.
mantic understanding capability. As for 3D encoders
likeSwin3D,thedatascarcityin3Dcomparedto2Dfortrainingthefoundationmodelsleadsto
inferiorperformanceonsemanticunderstanding.
3.5 Registration: GeometricCorrespondence
Toevaluatethegeometricinformationcontainedinthefoundationmodelfeatures,wedesignthe
following new task, partial scene registration, based on the point cloud registration [49, 95]
task. Fromacompletepointcloudrepresentingtheentirescene,wesampleapairofpointclouds
P
1
∈RN1×3andP
2
∈RN2×3withinthescene,whereP 1andP 2containallthepointsthatcanbe
observedintwosetsofconsecutiveviews,respectively. OurgoalistofindthehomographymatrixH
7Model RR@0.05m(%) RR@0.1m(%) RR@0.2m(%) RRE(◦) RTE(m)
DINOv2 82.1 93.9 96.8 1.72 0.14
LSeg 4.8 23.7 63.8 9.80 0.59
CLIP 18.6 51.3 78.2 7.96 0.44
StableDiffusion 91.7 96.8 98.4 1.15 0.09
V-JEPA 90.4 96.5 99.4 1.37 0.10
StableVideoDiffusion 96.8 99.0 99.7 0.83 0.06
Swin3D 60.3 81.1 91.3 3.60 0.23
Table5:EvaluationofpartialsceneregistrationonScanNet[20].WeemployRegistrationRecall(RR)atvarious
RMSEthresholds,RelativeRotationError(RRE),andRelativeTranslationError(RTE)asevaluationmetrics.A
higherRRindicatesbetterperformance,whilelowerRREandRTEvaluessignifysuperiorresults.
Model Time(sample) Time(scene) Mem. ecnam141.40 DINOv2
D
L CS
LIN
e Ig
POv2 22
9
35
1
4.
.
.0
2
5m
m
ms
s
s
8
17
7
0.
.
.5
4
4s
s
se
e
ec
c
c
1
2
1.
.
.1
5
19
1
9G
G
G
UrofreP 4-1 13 211. .230
0
V-JEPA S SV DD
Lseg
V
SS tt
-
aa
J
bb
E
ll
P
ee
A
VD ii dff eu os Dio in
ffusion
1 67
64
5
72
.
..
1
17
m
mm
s
ss 11
3
22
.
..
3
58
s
ss
e
ee
c
cc 1115 ... 730 018 GGG
ELB
AQ nacS1 11 011 .. 010
0
10Swin3 MD
odel Inference
Tim1 1e00
0
C
/
sL eI cP
onds (log-scale)
1 10 000
0
Swin3D 937.4ms 0.9sec 1.34G
Figure 6: Memory usage of different en-
coders. An ideal model should be a small
Table6:Complexityanalysisofvisualfoundationmodels.
circleandbepositionedintheupperleft.
thatcorrectlytransformsthepointsinP toregisterwithP . Comparedtothesemanticsegmentation
1 2
taskevaluatedinSection3.4,thepartialsceneregistrationtaskrequiresthefoundationmodelfeatures
tohavethecapabilityoffindinggeometriccorrespondenceforregistration,whichcannotbeachieved
simplybyfindingthecorrespondenceaccordingtosemanticunderstanding. Forexample,insemantic
correspondence,wemayfindtwosemanticallysimilarpoints,oneontheleftsideofthesofainP ,
1
whiletheotherontherightsideofthesofainP . Asaresult,ifweregisterthetwopartialpoint
2
cloudssolelybasedonsemanticcorrespondence,wewillfailtofindthecorrecthomographytoalign
onepointcloudwiththeother. Thefoundationmodelfeaturesneedtobeequippedwithgeometric
understandingcapabilitytoachievedecentperformanceonourpartialsceneregistrationtask.
Datasetand probe head. Webuild ourpartial sceneregistrationbenchmark basedon theScan-
Net[20]dataset. ForeachsceneinScanNet,wechooseviews#0∼#31andviews#32∼#63for
renderingP andP ,respectively,sothattheycanhaveacertainlevelofoverlapthatallowsthe
1 2
registration of two partial point clouds. Afterwards, P is transformed by a homography H that
2
consistsofarotationR ∈ SO(3)andatranslationt ∈ R3. Riscreatedbyarandomlygenerated
quaternionq∈R4foreachscene,whileeachcomponentoftisrandomlysampledfromtheuniform
distribution[−1.0m,1.0m]. WefollowREGTR[95]toadoptatransformercross-encodermodule,
followedbyalightweightdecoder,toobtainthecorrespondingpositionofeachpointintheother
pointcloud. Moredetailsondatasetandoptimizationareprovidedinthesupplementarymaterial.
Evaluation results. Table 5 demonstrates the results for the partial scene registration. We can
observethatStableDiffusionandStableVideoDiffusionshowcasesuperiorgeometriccapabilityinour
partialsceneregistrationtask. Itdemonstratesthatthepretrainingobjectiveofgenerationempowers
thefoundationmodelstohaveadecentcapabilityoffindinggeometriccorrespondencesin3Dscenes.
Anotherobservationisthatvideoencodersgenerallyperformbetterthanimageencoders. Thereason
isthatvideofoundationmodelshaveabetterunderstandingofobjectshapesandgeometrywithinthe
scenesfromthemulti-viewinputframes.
4 Analysis
Thepurposeofthissectionistoprovideadditionalexplorationtowardstheoptimalusageofvisual
foundationmodels.Theselectionofencodingmethodsrequiresconsiderationofthetrade-offbetween
memoryusage,runningtime,andperformance. Wewilldiveintocomplexityanalysisandthestudy
ofdesignchoicesforvariousandacombinationoffoundationmodels.
8Keyframe Sampling Performance (%)
100 100 98.18
96.89
…
SKIP SKIP 95.23
94.60
95
90.77
…
SKIP 90 Keyframe Sampling 87.79
Clip Sampling Sampling Rate
85
Clip Sampling
Full 1/4 1/7 1/10
Figure7:EvaluationondifferentvideodownsamplingstrategiesforV-JEPAonthesegmentationtask.Keyframe
Sampling samples every N frames to form a new video sequence, while Clip Sampling directly samples
consecutivevideoclips. Theperformancebeforedownsamplingisregardedas100%. Keyframesampling
demonstrateslessperformancedropwiththesamelevelofdownsampling.
4.1 ComplexityAnalysis
Wecomparememoryusage,computationtime,andmodelperformance(vision-languagereasoningon
ScanQA)inTable6andFigure6. Ourfindingsshowthatimageencodersgenerallyrequirelesstime
toprocessasamplecomparedtovideoand3Dencoders. Anddiffusion-basedmodels,whenusedfor
featureextraction,requiresignificantlymorememorythanotherdiscriminativemodels. Noticeably,
thedrawbacksinrunningtimebecomeevidentfor2Dbackbones,especiallyimageencoders,when
attemptingtoobtainasceneembeddingbyaggregatingmulti-viewimageembeddings. Toillustrate
this,weconsidera300-framevideoasanexemplarofposed2Dinformationforacomplexscene
(a10-secondvideoat30FPS).Asthelengthofthevideoincreases,2Dmethods,whichnecessitate
featureextractionforeachimageframe,rapidlyconsumeasubstantialamountoftimetoprocess
asinglescene. Incontrast,a3Dpointencoderrequiressignificantlylesstimetoprocessascene.
Nevertheless,3Dencodersexhibitrelativelypoormodelperformance,whichcanbeattributedtothe
scarcityoftrainingdata. Tofullydemonstratetheirpotentialinsceneunderstandingtasks,efforts
shouldbedirectedtowardenhancingthegeneralizabilityof3Dfoundationmodels. Allanalysesand
computationswereconductedonanNvidiaA100GPU.
4.2 AblationStudy–InsightsintoOptimalUsageofVisualFoundationModels
Videodownsamplingstrategy. Longandhighframe-per-secondvideostakealotofspacetostore
andtimetoprocess. Weexploretwostraightforwardwaysofconductingtemporaldownsamplingto
achievemoreefficientprocessingwithoutsacrificingtoomuchperformance. AsshowninFigure7,
weexplorethekeyframesampling(blue)andclipsampling(orange)strategies. Wecanobservethat
keyframesamplingisabetterstrategythanclipsamplinginthissetting,morewiselybalancingthe
trade-offbetweenvideoprocessingoverheadandtaskperformance.
Combination of multiple encoders. We explore whether mIoU (↑)
amixtureoffoundationmodels(experts)hasthepotentialto 55
strengthenthecapabilityof3Dsceneunderstanding. Weexper-
45
imentonthe3Dsemanticsegmentationtaskwiththreefeature
sources: LSeg,StableDiffusion,andSwin3D.Whencombin- 35
ingdifferentfeaturesources,weconcatenateallfeaturesalong
25
thechanneldimensionforeverypointinthepointcloud. The 1 2 3 1+2 1+3 2+3 1+2+3
resultsareshowninFigure8. Aftercombiningfeaturesfrom Figure 8: Evaluation on the segmen-
tationtaskwith(1)LSeg,(2)SD,(3)
different sources, there exists potential that the semantic un-
Swin3D,andtheircombinations.
derstandingcapabilitycanbeboostedinamixtureofexperts
manner. However,itisnotnecessarilytruethatcombiningthe
bestfeatureswillleadtothebestperformance. Forexample,LSeg(1)hasstrongercapabilityon
semanticsegmentationthanStableDiffusion(2)andSwin3D(3)individually,butitisStableDiffusion
+Swin3D(2+3)thatreachesthebestperformancewhencombiningtwofeaturestogether.
Appendix(supplementarymaterial). Theappendixoffersacomprehensiveintroductionofallof
ourevaluatedmodelsandadditionalexperimentdetails,andincludesmorevisualizationandablation
experiments. Wealsoelaborateonthelimitations,broaderimpact,andfuturedirectionofourwork.
95 Conclusion
This paper presents the first comprehensive analysis of leveraging visual foundation models for
complex3Dsceneunderstanding. Weexplorethestrengthsandweaknessesofmodelsdesignedfor
variousmodalitiesandtrainedwithdifferentobjectives. Ourstudyrevealsthesuperiorperformance
ofDINOv2,theadvantagesofvideomodelsinobject-leveltasks,andthebenefitsofdiffusionmodels
ingeometricregistrationtasks. Surprisingly,wefindlimitationsoflanguage-pretrainedmodelsin
language-relatedtasks. Theextensiveanalysissuggeststhatamoreflexibleencoderselectioncan
playacrucialroleinfuturesceneunderstandingandmulti-modalreasoningtasks.
References
[1] J. Achiam, S. Adler, S. Agarwal, L. Ahmad, I. Akkaya, F. L. Aleman, D. Almeida,
J. Altenschmidt, S. Altman, S. Anadkat, et al. GPT-4 technical report. arXiv preprint
arXiv:2303.08774,2023. 1
[2] P. Achlioptas, A. Abdelreheem, F. Xia, M. Elhoseiny, and L. Guibas. ReferIt3D: Neural
listenersforfine-grained3Dobjectidentificationinreal-worldscenes. InECCV,2020. 6
[3] H. Akbari, L. Yuan, R. Qian, W.-H. Chuang, S.-F. Chang, Y. Cui, and B. Gong. VATT:
Transformers for multimodal self-supervised learning from raw video, audio and text. In
NeurIPS,2021. 3
[4] M.Assran,Q.Duval,I.Misra,P.Bojanowski,P.Vincent,M.Rabbat,Y.LeCun,andN.Ballas.
Self-supervisedlearningfromimageswithajoint-embeddingpredictivearchitecture.InCVPR,
2023. 3
[5] D.Azuma,T.Miyanishi,S.Kurita,andM.Kawanabe. ScanQA:3Dquestionansweringfor
spatialsceneunderstanding. InCVPR,2022. 1,3,5,6,17
[6] M. E. Banani, A. Raj, K.-K. Maninis, A. Kar, Y. Li, M. Rubinstein, D. Sun, L. Guibas,
J.Johnson,andV.Jampani. Probingthe3Dawarenessofvisualfoundationmodels. InCVPR,
2024. 1,2,3,18
[7] S.BanerjeeandA.Lavie. METEOR:AnautomaticmetricforMTevaluationwithimproved
correlationwithhumanjudgments. InProceedingsoftheACLWorkshoponIntrinsicand
ExtrinsicEvaluationMeasuresforMachineTranslationand/orSummarization,2005. 6
[8] H.Bao,L.Dong,S.Piao,andF.Wei. BeiT:Bertpre-trainingofimagetransformers. InICLR,
2022. 3
[9] D.Baranchuk,I.Rubachev,A.Voynov,V.Khrulkov,andA.Babenko. Label-efficientsemantic
segmentationwithdiffusionmodels. InICLR,2022. 3
[10] A.Bardes,J.Ponce,andY.LeCun. MC-JEPA:Ajoint-embeddingpredictivearchitecturefor
self-supervisedlearningofmotionandcontentfeatures. arXivpreprintarXiv:2307.12698,
2023. 3
[11] A.Bardes,Q.Garrido,J.Ponce,X.Chen,M.Rabbat,Y.LeCun,M.Assran,andN.Ballas.
V-JEPA: Latent video prediction for visual representation learning, 2024. URL https:
//openreview.net/forum?id=WFYbBOEOtv. 3,5,17
[12] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi,
Z.English,V.Voleti,A.Letts,V.Jampani,andR.Rombach. StableVideoDiffusion: Scaling
latentvideodiffusionmodelstolargedatasets. arXivpreprintarXiv:2311.15127,2023. 3,5,
16
[13] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan,
R.Child,A.Ramesh,D.M.Ziegler,J.Wu,C.Winter,C.Hesse,M.Chen,E.Sigler,M.Litwin,
S.Gray,B.Chess,J.Clark,C.Berner,S.McCandlish,A.Radford,I.Sutskever,andD.Amodei.
Languagemodelsarefew-shotlearners. InNeurIPS,2020. 3
[14] M.Caron,H.Touvron,I.Misra,H.Jégou,J.Mairal,P.Bojanowski,andA.Joulin. Emerging
propertiesinself-supervisedvisiontransformers. InICCV,2021. 3
[15] A.Chang,A.Dai,T.Funkhouser,M.Halber,M.Niessner,M.Savva,S.Song,A.Zeng,and
Y.Zhang. Matterport3D:Learningfromrgb-ddatainindoorenvironments. In3DV,2017. 4
10[16] D.Z.Chen,A.X.Chang,andM.Nießner. ScanRefer: 3Dobjectlocalizationinrgb-dscans
usingnaturallanguage. InECCV,2020. 6,17
[17] S.Chen,P.Sun,Y.Song,andP.Luo. Diffusiondet: Diffusionmodelforobjectdetection. In
ICCV,2023. 3
[18] T.Chen,S.Kornblith,M.Norouzi,andG.Hinton.Asimpleframeworkforcontrastivelearning
ofvisualrepresentations. InICML,2020. 3
[19] X.ChenandK.He. Exploringsimplesiameserepresentationlearning. InCVPR,2021. 3
[20] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, and M. Nießner. ScanNet:
Richly-annotated3Dreconstructionsofindoorscenes. InCVPR,2017. 4,5,6,7,8,17
[21] R.Ding,J.Yang,C.Xue,W.Zhang,S.Bai,andX.Qi.PLA:Language-drivenopen-Vocabulary
3Dsceneunderstanding. InCVPR,2023. 3,4
[22] A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai,T.Unterthiner,M.De-
hghani,M.Minderer,G.Heigold,S.Gelly,J.Uszkoreit,andN.Houlsby. Animageisworth
16x16words: Transformersforimagerecognitionatscale. InICLR,2021. 3
[23] Y.Duan,X.Guo,andZ.Zhu. Diffusiondepth: Diffusiondenoisingapproachformonocular
depthestimation. arXivpreprintarXiv:2303.05021,2023. 3
[24] C. Fang, X. Hu, K. Luo, and P. Tan. Ctrl-Room: Controllable text-to-3D room meshes
generationwithlayoutconstraints. arXivpreprintarXiv:2310.03602,2023. 1
[25] R.Fridman,A.Abecasis,Y.Kasten,andT.Dekel. SceneScape: Text-drivenconsistentscene
generation. InNeurIPS,2023. 1
[26] G.Gao,W.Liu,A.Chen,A.Geiger,andB.Schölkopf. GraphDreamer: Compositional3D
scenesynthesisfromscenegraphs. InCVPR,2024. 1
[27] S.Gidaris,P.Singh,andN.Komodakis. Unsupervisedrepresentationlearningbypredicting
imagerotations. InICLR,2018. 3
[28] R.Girdhar,A.El-Nouby,M.Singh,K.V.Alwala,A.Joulin,andI.Misra. OmniMAE:Single
modelmaskedpretrainingonimagesandvideos. InCVPR,2023. 3
[29] J.-B.Grill,F.Strub,F.Altché,C.Tallec,P.H.Richemond,E.Buchatskaya,C.Doersch,B.A.
Pires,Z.D.Guo,M.G.Azar,B.Piot,K.Kavukcuoglu,R.Munos,andM.Valko. Bootstrap
yourownlatent-anewapproachtoself-supervisedlearning. InNeurIPS,2020. 3
[30] K.He,H.Fan,Y.Wu,S.Xie,andR.Girshick. Momentumcontrastforunsupervisedvisual
representationlearning. InCVPR,2020. 3
[31] K.He,X.Chen,S.Xie,Y.Li,P.Dollár,andR.Girshick. Maskedautoencodersarescalable
visionlearners. InCVPR,2022. 3
[32] J.Ho,A.Jain,andP.Abbeel. Denoisingdiffusionprobabilisticmodels. InNeurIPS,2020. 3
[33] L.Höllein,A.Cao,A.Owens,J.Johnson,andM.Nießner. Text2Room: Extractingtextured
3Dmeshesfrom2Dtext-to-imagemodels. InICCV,2023. 1
[34] Y.Hong, C.Lin, Y.Du, Z.Chen, J.B.Tenenbaum, andC.Gan. 3Dconceptlearningand
reasoningfrommulti-viewimages. InCVPR,2023. 3,4
[35] Y.Hong,H.Zhen,P.Chen,S.Zheng,Y.Du,Z.Chen,andC.Gan. 3D-LLM:Injectingthe3D
worldintolargelanguagemodels. InNeurIPS,2023. 1,3,4,5,6,17
[36] Y.Hong,Z.Zheng,P.Chen,Y.Wang,J.Li,andC.Gan. MultiPLY:Amultisensoryobject-
centricembodiedlargelanguagemodelin3Dworld. InCVPR,2024. 1
[37] J.Hou,B.Graham,M.Nießner,andS.Xie. Exploringdata-efficient3Dsceneunderstanding
withcontrastivescenecontexts. InCVPR,2021. 3,5
[38] A.Joulin,L.VanDerMaaten,A.Jabri,andN.Vasilache. Learningvisualfeaturesfromlarge
weaklysuperviseddata. InECCV,2016. 3
[39] W.Kabsch.Asolutionforthebestrotationtorelatetwosetsofvectors.ActaCrystallographica
SectionA:CrystalPhysics,Diffraction,TheoreticalandGeneralCrystallography,32(5):922–
923,1976. 17
[40] J.Kerr,C.M.Kim,K.Goldberg,A.Kanazawa,andM.Tancik. LERF:Languageembedded
radiancefields. InICCV,2023. 3
11[41] D.P.KingmaandJ.Ba. Adam: Amethodforstochasticoptimization. InICLR,2015. 7,17
[42] X.Lai,Z.Tian,Y.Chen,Y.Li,Y.Yuan,S.Liu,andJ.Jia. LISA:Reasoningsegmentationvia
largelanguagemodel. arXivpreprintarXiv:2308.00692,2023. 1
[43] Y.LeCun. Apathtowardsautonomousmachineintelligenceversion0.9.2,2022-06-27. Open
Review,62(1),2022. 3
[44] B.Li,K.Q.Weinberger,S.Belongie,V.Koltun,andR.Ranftl. Language-drivensemantic
segmentation. InICLR,2022. 3,5,6,16
[45] J. Li, D. Li, C. Xiong, and S. Hoi. BLIP: Bootstrapping language-image pre-training for
unifiedvision-languageunderstandingandgeneration. InICML,2022. 3,6
[46] J.Li,D.Li,S.Savarese,andS.Hoi. BLIP-2: Bootstrappinglanguage-imagepre-trainingwith
frozenimageencodersandlargelanguagemodels. InICML,2023. 3,6,17
[47] C.-Y.Lin. ROUGE:Apackageforautomaticevaluationofsummaries. InACL,2004. 6
[48] H.Liu,M.Cai,andY.J.Lee. Maskeddiscriminationforself-supervisedlearningonpoint
clouds. InECCV,2022. 3
[49] J. Liu, G. Wang, Z. Liu, C. Jiang, M. Pollefeys, and H. Wang. RegFormer: An efficient
projection-awaretransformernetworkforlarge-scalepointcloudregistration. InICCV,2023.
7
[50] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. arXiv preprint
arXiv:1711.05101,2017. 6,17
[51] C. Ma, Y. Yang, C. Ju, F. Zhang, J. Liu, Y. Wang, Y. Zhang, and Y. Wang. DiffusionSeg:
Adaptingdiffusiontowardsunsupervisedobjectdiscovery. arXivpreprintarXiv:2303.09813,
2023. 3
[52] X. Ma, S. Yong, Z. Zheng, Q. Li, Y. Liang, S.-C. Zhu, and S. Huang. SQA3D: Situated
questionansweringin3Dscenes. InICLR,2023. 1,3,5,6,17
[53] D.Mahajan,R.Girshick,V.Ramanathan,K.He,M.Paluri,Y.Li,A.Bharambe,andL.Van
DerMaaten. Exploringthelimitsofweaklysupervisedpretraining. InECCV,2018. 3
[54] Y.Man,L.-Y.Gui,andY.-X.Wang. BEV-GuidedMulti-ModalityFusionforDrivingPercep-
tion. InCVPR,2023. 1
[55] Y. Man, L.-Y. Gui, and Y.-X. Wang. Situational awareness matters in 3d vision language
reasoning. InCVPR,2024. 1,3,6
[56] K.Namekata,A.Sabour,S.Fidler,andS.W.Kim. EmerDiff: Emergingpixel-levelsemantic
knowledgeindiffusionmodels. InICLR,2024. 3
[57] S. Nasiriany, F. Xia, W. Yu, T. Xiao, J. Liang, I. Dasgupta, A. Xie, D. Driess, A. Wahid,
Z.Xu, Q.Vuong, T.Zhang, T.-W.E.Lee, K.-H.Lee, P.Xu, S.Kirmani, Y.Zhu, A.Zeng,
K.Hausman,N.Heess,C.Finn,S.Levine,andB.Ichter. PIVOT:Iterativevisualprompting
elicitsactionableknowledgeforVLMs. arXivpreprintarXiv:2402.07872,2024. 1
[58] M. Oquab, T. Darcet, T. Moutakanni, H. Vo, M. Szafraniec, V. Khalidov, P. Fernandez,
D.Haziza,F.Massa,A.El-Nouby,M.Assran,N.Ballas,W.Galuba,R.Howes,P.-Y.Huang,
S.-W.Li,I.Misra,M.Rabbat,V.Sharma,G.Synnaeve,H.Xu,H.Jegou,J.Mairal,P.Labatut,
A.Joulin,andP.Bojanowski. DINOv2: Learningrobustvisualfeatureswithoutsupervision.
Transactions on Machine Learning Research, 2024. ISSN 2835-8856. URL https://
openreview.net/forum?id=a68SUt6zFt. 2,3,5,16
[59] Y.Pang,W.Wang,F.E.Tay,W.Liu,Y.Tian,andL.Yuan. Maskedautoencodersforpoint
cloudself-supervisedlearning. InECCV,2022. 3
[60] Z.Pang,Z.Xie,Y.Man,andY.-X.Wang.Frozentransformersinlanguagemodelsareeffective
visualencoderlayers. InICLR,2024. 3
[61] K.Papineni,S.Roukos,T.Ward,andW.-J.Zhu. BLEU:amethodforautomaticevaluationof
machinetranslation. InACL,2002. 6
[62] D.Pathak,P.Krahenbuhl,J.Donahue,T.Darrell,andA.A.Efros. Contextencoders: Feature
learningbyinpainting. InCVPR,2016. 3
[63] S.Peng,K.Genova,C.M.Jiang,A.Tagliasacchi,M.Pollefeys,andT.Funkhouser.Openscene:
3Dsceneunderstandingwithopenvocabularies. InCVPR,2023. 3,4
12[64] Z.Peng,W.Wang,L.Dong,Y.Hao,S.Huang,S.Ma,Q.Ye,andF.Wei.Kosmos-2:Grounding
multimodallargelanguagemodelstotheworld. InICLR,2024. 1
[65] A.Radford, J.W.Kim, C.Hallacy, A.Ramesh, G.Goh, S.Agarwal, G.Sastry, A.Askell,
P.Mishkin,J.Clark,G.Krueger,andI.Sutskever. Learningtransferablevisualmodelsfrom
naturallanguagesupervision. InICML,2021. 3,5,6,16
[66] C.Raffel,N.Shazeer,A.Roberts,K.Lee,S.Narang,M.Matena,Y.Zhou,W.Li,andP.J.
Liu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-texttransformer. JMLR,21
(140):1–67,2020. 3
[67] M.Ranzinger,G.Heinrich,J.Kautz,andP.Molchanov. AM-RADIO:Agglomerativemodel–
reducealldomainsintoone. InCVPR,2024. 1,3
[68] H.Rasheed,M.Maaz,S.Shaji,A.Shaker,S.Khan,H.Cholakkal,R.M.Anwer,E.Xing,
M.-H.Yang,andF.S.Khan. GLaMM:Pixelgroundinglargemultimodalmodel. InCVPR,
2024. 1
[69] M. Roberts, J. Ramapuram, A. Ranjan, A. Kumar, M. A. Bautista, N. Paczan, R. Webb,
andJ.M.Susskind. HyperSim: Aphotorealisticsyntheticdatasetforholisticindoorscene
understanding. InICCV,2021. 4
[70] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image
synthesiswithlatentdiffusionmodels. InCVPR,2022. 3,5,16,18
[71] S.Saxena,A.Kar,M.Norouzi,andD.J.Fleet. Monoculardepthestimationusingdiffusion
models. arXivpreprintarXiv:2302.14816,2023. 3
[72] J.L.SchonbergerandJ.-M.Frahm. Structure-from-motionrevisited. InCVPR,2016. 4
[73] C.Schuhmann,R.Beaumont,R.Vencu,C.Gordon,R.Wightman,M.Cherti,T.Coombes,
A.Katta,C.Mullis,M.Wortsman,P.Schramowski,S.Kundurthy,K.Crowson,L.Schmidt,
R. Kaczmarczyk, and J. Jitsev. Laion-5B: An open large-scale dataset for training next
generationimage-textmodels. InNeurIPS,2022. 16
[74] J.Schult,S.Tsai,L.Höllein,B.Wu,J.Wang,C.-Y.Ma,K.Li,X.Wang,F.Wimbauer,Z.He,
P.Zhang,B.Leibe,P.Vajda,andJ.Hou. ControlRoom3D:Roomgenerationusingsemantic
proxyrooms. InCVPR,2024. 1
[75] C.Sima,K.Renz,K.Chitta,L.Chen,H.Zhang,C.Xie,P.Luo,A.Geiger,andH.Li.DriveLM:
Drivingwithgraphvisualquestionanswering. arXivpreprintarXiv:2312.14150,2023. 1
[76] J.Sohl-Dickstein,E.Weiss,N.Maheswaranathan,andS.Ganguli. Deepunsupervisedlearning
usingnonequilibriumthermodynamics. InICML,2015. 3
[77] L.Tang,M.Jia,Q.Wang,C.P.Phoo,andB.Hariharan. Emergentcorrespondencefromimage
diffusion. InNeurIPS,2023. 16
[78] G.Team,R.Anil,S.Borgeaud,Y.Wu,J.-B.Alayrac,J.Yu,R.Soricut,J.Schalkwyk,A.M.
Dai,A.Hauth,etal. Gemini: Afamilyofhighlycapablemultimodalmodels. arXivpreprint
arXiv:2312.11805,2023. 1
[79] X.Tian,J.Gu,B.Li,Y.Liu,C.Hu,Y.Wang,K.Zhan,P.Jia,X.Lang,andH.Zhao.DriveVLM:
Theconvergenceofautonomousdrivingandlargevision-languagemodels. arXivpreprint
arXiv:2402.12289,2024. 1
[80] Z.Tong,Y.Song,J.Wang,andL.Wang. VideoMAE:Maskedautoencodersaredata-efficient
learnersforself-supervisedvideopre-training. InNeurIPS,2022. 3,5
[81] S. Umeyama. Least-squares estimation of transformation parameters between two point
patterns. TPAMI,13(04):376–380,1991. 17
[82] R.Vedantam,C.LawrenceZitnick,andD.Parikh.CIDER:Consensus-basedimagedescription
evaluation. InCVPR,2015. 6
[83] L.Wang,B.Huang,Z.Zhao,Z.Tong,Y.He,Y.Wang,Y.Wang,andY.Qiao. VideoMAEv2:
Scalingvideomaskedautoencoderswithdualmasking. InCVPR,2023. 3,5
[84] W.Wang,Z.Chen,X.Chen,J.Wu,X.Zhu,G.Zeng,P.Luo,T.Lu,J.Zhou,Y.Qiao,and
J.Dai. VisionLLM:Largelanguagemodelisalsoanopen-endeddecoderforvision-centric
tasks. InNeurIPS,2023. 1
13[85] Y.Wang,K.Li,Y.Li,Y.He,B.Huang,Z.Zhao,H.Zhang,J.Xu,Y.Liu,Z.Wang,S.Xing,
G.Chen,J.Pan,J.Yu,Y.Wang,L.Wang,andY.Qiao. InternVideo: Generalvideofoundation
modelsviagenerativeanddiscriminativelearning. arXivpreprintarXiv:2212.03191,2022. 3
[86] W.Wu,Y.Zhao,M.Z.Shou,H.Zhou,andC.Shen. DiffuMNask: Synthesizingimageswith
pixel-levelannotationsforsemanticsegmentationusingdiffusionmodels. InICCV,2023. 3
[87] X. Wu, X. Wen, X. Liu, and H. Zhao. Masked scene contrast: A scalable framework for
unsupervised3Drepresentationlearning. InCVPR,2023. 3
[88] S. Xie, J. Gu, D. Guo, C. R. Qi, L. Guibas, and O. Litany. PointContrast: Unsupervised
pre-trainingfor3Dpointcloudunderstanding. InECCV,2020. 3,5
[89] H.Xu,G.Ghosh,P.-Y.Huang,D.Okhonko,A.Aghajanyan,F.Metze,L.Zettlemoyer,and
C.Feichtenhofer. VideoCLIP:Contrastivepre-trainingforzero-shotvideo-textunderstanding.
InEMNLP,2021. 3
[90] J.Xu,S.Liu,A.Vahdat,W.Byeon,X.Wang,andS.DeMello. Open-vocabularypanoptic
segmentationwithtext-to-imagediffusionmodels. InCVPR,2023. 3
[91] J.Xu,X.Zhou,S.Yan,X.Gu,A.Arnab,C.Sun,X.Wang,andC.Schmid. Pixelaligned
languagemodels. InCVPR,2024. 1
[92] S. Yan, Y. Yang, Y. Guo, H. Pan, P.-s. Wang, X. Tong, Y. Liu, and Q. Huang. 3D feature
predictionformasked-autoencoder-basedpointcloudpretraining. InICLR,2024. 3
[93] Y.-Q. Yang, Y.-X. Guo, J.-Y. Xiong, Y. Liu, H. Pan, P.-S. Wang, X. Tong, and B. Guo.
Swin3D: A pretrained transformer backbone for 3D indoor scene understanding. arXiv
preprintarXiv:2304.06906,2023. 3,5,17
[94] C.Yeshwanth,Y.-C.Liu,M.Nießner,andA.Dai. ScanNet++: Ahigh-fidelitydatasetof3D
indoorscenes. InICCV,2023. 4
[95] Z.J.YewandG.H.Lee. REGTR:End-to-endpointcloudcorrespondenceswithtransformers.
InCVPR,2022. 7,8,17
[96] X.Yu, L.Tang, Y.Rao, T.Huang, J.Zhou, andJ.Lu. Point-BERT:Pre-training3Dpoint
cloudtransformerswithmaskedpointmodeling. InCVPR,2022. 3
[97] R.Zellers,J.Lu,X.Lu,Y.Yu,Y.Zhao,M.Salehi,A.Kusupati,J.Hessel,A.Farhadi,and
Y.Choi. Merlotreserve: Neuralscriptknowledgethroughvisionandlanguageandsound. In
CVPR,2022. 3
[98] G.Zhan,C.Zheng,W.Xie,andA.Zisserman. Whatdoesstablediffusionknowaboutthe3d
scene? arXivpreprintarXiv:2310.06836,2023. 3
[99] J.Zhang,C.Herrmann,J.Hur,L.PolaniaCabrera,V.Jampani,D.Sun,andM.-H.Yang. A
taleoftwofeatures:Stablediffusioncomplementsdinoforzero-shotsemanticcorrespondence.
InNeurIPS,2023. 1,3,18
[100] R.Zhang,P.Isola,andA.A.Efros. Colorfulimagecolorization. InECCV,2016. 3
[101] R. Zhang, Z. Guo, P. Gao, R. Fang, B. Zhao, D. Wang, Y. Qiao, and H. Li. Point-M2AE:
multi-scalemaskedautoencodersforhierarchicalpointcloudpre-training. InNeurIPS,2022.
3
[102] R.Zhang,J.Han,A.Zhou,X.Hu,S.Yan,P.Lu,H.Li,P.Gao,andY.Qiao. LLaMA-Adapter:
Efficientfine-tuningoflanguagemodelswithzero-initattention. InICLR,2024. 3
[103] S.Zhang,S.Roller,N.Goyal,M.Artetxe,M.Chen,S.Chen,C.Dewan,M.Diab,X.Li,X.V.
Lin,T.Mihaylov,M.Ott,S.Shleifer,K.Shuster,D.Simig,P.S.Koura,A.Sridhar,T.Wang,
and L. Zettlemoyer. OPT: Open pre-trained transformer language models. arXiv preprint
arXiv:2205.01068,2022. 3
[104] Y.Zhang,Z.Gong,andA.X.Chang. Multi3DRefer: Groundingtextdescriptiontomultiple
3Dobjects. InICCV,2023. 6
[105] Z.Zhang,R.Girdhar,A.Joulin,andI.Misra. Self-supervisedpretrainingof3Dfeatureson
anypoint-cloud. InICCV,2021. 3,5
[106] Z.Zhang,B.Yang,B.Wang,andB.Li. GrowSP:Unsupervisedsemanticsegmentationof3D
pointclouds. InCVPR,2023. 7
14[107] W.Zhao,Y.Rao,Z.Liu,B.Liu,J.Zhou,andJ.Lu. Unleashingtext-to-imagediffusionmodels
forvisualperception. InICCV,2023. 3
[108] H.Zhen, X.Qiu, P.Chen, J.Yang, X.Yan, Y.Du, Y.Hong, andC.Gan. 3D-VLA:A3D
vision-language-actiongenerativeworldmodel. InICML,2024. 1,6
[109] J.Zheng,J.Zhang,J.Li,R.Tang,S.Gao,andZ.Zhou. Structured3D:Alargephoto-realistic
datasetforstructured3Dmodeling. InECCV,2020. 3,4,5
[110] J.Zheng,J.Zhang,J.Li,R.Tang,S.Gao,andZ.Zhou. Structured3D:Alargephoto-realistic
datasetforstructured3Dmodeling. InECCV,2020. 17
[111] J. Zhou, C. Wei, H. Wang, W. Shen, C. Xie, A. Yuille, and T. Kong. iBOT: Image bert
pre-trainingwithonlinetokenizer. InICLR,2022. 3
[112] Y. Zhou, L. Huang, Q. Bu, J. Zeng, T. Li, H. Qiu, H. Zhu, M. Guo, Y. Qiao, and H. Li.
Embodiedunderstandingofdrivingscenarios. arXivpreprintarXiv:2403.04593,2024. 1
[113] Z.Zhu,X.Ma,Y.Chen,Z.Deng,S.Huang,andQ.Li. 3D-VisTA:Pre-trainedtransformerfor
3Dvisionandtextalignment. InICCV,2023. 3
15Lexicon3D: Probing Visual Foundation Models for Complex 3D
Scene Understanding
SupplementaryMaterial
A AdditionalExperimentDetails
In this section, we provide a detailed introduction of all the visual foundation models we have
evaluated,includingthecheckpointsweuseandhowweextractfeaturerepresentationsfromthe
encoderbackbones. Ourcodewillbepubliclyreleasedforreplicationandfurtherevaluation.
A.1 EvaluatedVisualFoundationModels
OurevaluationandanalysisareconductedmainlyonthesevenmodelslistedinTable1inthemain
body paper. We have chosen models such that they cover most of the backbones used by recent
3Dsceneunderstandingandreasoningwork. Inthispart,wediscussallthemodelswehaveused
inourexperimentsandexplaintheirpretrainingobject,thedatasetusedforpretraining,thepublic
checkpointswechoose,andthemethodweleveragetoextractfeaturesfromtheirbackbones. We
startwithimagefoundationmodels,andthenvideoand3Dmodels.
DINOv2 [58]. DINOv2leveragesanimage-wisecontrastiveobjectivebyminimizingthedistance
offeaturesfromthesamesamples,andmaximizingthosefromdifferentsamples. Theyalsoinclude
apatch-wisedenoisingobjectivebyperformingreconstructionfrommaskedinputs. Theytraintheir
modelonalarge-scaleimagedataset,LVD-142M[58],whichcontains142millionunlabeledimages.
WetakethestandardDINOv2implementation1andusethepretrainedViT-L/14checkpointforour
evaluations.
LSeg [44].LSegaimstoalignvisualfeaturesfromimageswithcorrespondingsemanticinformation
providedbynaturallanguagedescriptionsbymaximizingthecorrelationbetweenthetextembedding
andtheimagepixelembeddingoftheground-truthclassofthepixel. Weusetheofficialcheckpoint2
ofViT-L/16thatistrainedonamixtureofsevendatasets[44].
CLIP [65]. CLIPalignsvisualandtextualrepresentationsinasharedembeddingspacethrough
contrastivelearningbymaximizingthesimilaritybetweentheembeddingsofcorrespondingimage-
captionpairswhileminimizingthesimilarityofnon-matchingpairs. CLIPwastrainedonalargeand
diversedatasetofimage-captionpairssourcedfromtheinternetincludingover400millionimage-text
pairs. Weusetheofficialimplementationandcheckpoint3withaViT-L/14asthebackboneforour
evaluations.
StableDiffusion(SD) [70]. SDisadiffusion-basedmodelusedforgeneratinghigh-qualityimages
from text prompts. The model is trained to gradually remove noise from images, transforming
randomnoiseintocoherentimagesthatmatchtheprovidedtextdescriptions. Thismodelistrainedon
LAION5B[73]whichcontainsoverfivebillionofimagespairedwithdetailedcaptions. Wefollow
DIFT[77]4 toextractfeaturesfromSDandweusethecheckpointSD2.1forourevaluation. We
usethefeaturesfromblockindex1foralltasks. Thenoisetimestepissetto100bydefault. Weuse
null-promptasthetextcondition.
StableVideoDiffusion(SVD) [12]. SVDisanextensionofSDfromimagegenerationtovideo
generationbyincorporatingadditionaltemporalmodules. SVDisfirstinitializedfromanimage-level
pretrained diffusion checkpoint (SD2.1), then is further finetuned on 10 million videos. We use
theirpubliclyreleasedimage-to-videovariant(SVD-xt)5. Webuildourfeatureextractorpipeline
followingDIFT[77]andextractthefeaturesfromindex1foralltasks. Thenoisetimestepissetto
25bydefault. Weusethefirst-frameimageastheconditionforallthecross-attentionmoduleswhile
weusetheunconditionalversionforthelatentinputoftheUNet–weconcatenateanall-zerovector
1https://github.com/facebookresearch/dinov2
2https://github.com/isl-org/lang-seg
3https://github.com/openai/CLIP
4https://github.com/Tsingularity/dift
5https://huggingface.co/stabilityai/stable-video-diffusion-img2vid-xt
16totheframewiseembeddings. Eachtime,wefeedavideoclipof25framestoSVDtoprocessthe
features.
V-JEPA [11]. V-JEPAaimstolearnrobustvisualrepresentationsbypredictingfuturestatesof
visualdata. Thismodelispretrainedonamixedvideodatasetcontainingover2millionofvideos.
Wetaketheirofficialimplementation6andthecheckpointViT-L/16underresolutionof224×224.
Weobtaintheirper-patchrepresentationbyremovingthelastpoolingandlinearlayers. Eachtime,
wefeedavideoclipof16framestoV-JEPAtoprocessthefeatures.
Swin3D [93]. Swin3D adapts the Swin Transformer to handle 3D data, such as point clouds
andvolumetricdata. Weusetheofficialcheckpoint7 thattakesSwin3D-Lasthebackboneandis
pretrainedontheStructure3Ddataset[110]withsemanticsegmentationasthetarget.
A.2 AdditionalEvaluationDetailsforVision-LanguageSceneReasoning
Datasets. Weevaluatetheperformanceontwochallengingindoor3DVQAdatasets: ScanQA[5]
andSQA3D[52]. SQA3Dfeaturesover33KQApairs,whileScanQAconsistsofmorethan41K
pairs. Eachentryinthesedatasetsincludesacomplex3Dindoorscene,aquestion,andcorresponding
answers. Weusethesplitsprovidedbytherespectivedatasets.
Optimization. WekeeptheLLMparametersfrozenandfinetunetheshallowvisualprojection
Q-Formermodule[46]toalignfeaturesfromdifferentencoderstotheLLMinputspace. Different
from[35],wetraintheQ-Formermodulefromscratchforafaircomparisonofallencoders.Following
theapproachof3D-LLM,wepretrainthemodulefor10epochsusing3D-Languagedataset[35]and
thenfinetuneitonthetrainingsplitofthetwoevaluationdatasetsfor35epochs. Bothstagesusethe
AdamW[50]optimizerwithalinearwarmupandcosinedecaylearningratescheduler. Whilelonger
trainingcanfurtherimproveperformance,trendsstabilizeafter35trainingepochs.
A.3 AdditionalEvaluationDetailsforRegistration
Dataset generation. When generating corresponding partial scene point clouds from ScanNet
dataset,duetomemoryconstraint,wedownsamplethepartialscenepointcloudsto4,096pointseach
withthefarthestpointsampling(FPS)algorithm,ifthenumberofpointsinP andP isover4,096.
1 2
Wefollowthesametrain/valsplitonthesemanticsegmentationtaskinourpartialsceneregistration
task.
Optimization. WefollowREGTR[95]toadoptaTransformercross-encodermoduletoenable
cross-reasoningofthefoundationmodelfeaturesfromtwopointclouds,followedbyalightweight
decodertoobtainthecorrespondingpositionofeachpointintheotherpointcloud,formingaltogether
N + N pairs of correspondences, where N and N are the number of points in P and P ,
1 2 1 2 1 2
respectively. Afterward,therotationRandthetranslationtcanbeobtainedinaclosed-formsolution
solvedbyaweightedversionoftheKabsch-Umeyama[39,81]algorithm. WeuseAdam[41]for
optimizationandtrainourmodelfor30epochs.
A.4 LicenseofDatasetUsed
Inthissection,welistthelicensesofallthedatasetswehaveusedduringourevaluation:
• ScanNet[20]: MITLicense.
• ScanQA[5]: CCBY-NC-SA3.0License.
• SQA3D[52]: CC-BY-4.0License.
• ScanRefer[16]: CCBY-NC-SA3.0License.
• 3DLanguage-Data[35]: MITLicense.
Inaddition,weutilizeanumberofpublicfoundationmodelcheckpointspretrainedonvariousdata
sourcesinourpaper. Pleaserefertotheiroriginalpaperforthelicenseofdatasetstheyhaveusedin
pretrainingtheirmodels.
6https://github.com/facebookresearch/jepa
7https://github.com/microsoft/Swin3D
1750 3
DINOv2
LSeg
CLIP
5 0.3
StableDiffusion
V-JEPA
StableVideoDiffusion
Swin3D
0.5 0.03
0 5 10 15 20 25 30 0 5 10 15 20 25 30
Epoch Epoch
FigureA:EvaluationcurvesofRelativeRotationError(RRE)andRelativeTranslationError(RTE)onthe
partialsceneregistrationtaskduringdifferenttrainingstages.
StableDiffusion BLEU-1 BLEU-4 METEOR ROUGE CIDEr
Evaluationofnoiselevelt
t=1step 35.3 11.6 14.0 34.5 68.5
t=25steps 35.6 11.5 14.0 34.2 68.3
t=100steps 35.5 11.7 14.1 34.9 68.2
t=200steps 34.3 10.9 13.9 33.9 66.6
Evaluationoffeaturelayerl
l=0 33.6 10.5 13.3 32.6 65.9
l=1 35.5 11.7 14.1 34.9 68.2
l=2 34.9 11.4 14.0 34.5 68.0
Table7: EvaluationofdiffusionnoiselevelandfeaturelayerswhenusingStableDiffusion[70]forfeature
extraction.Thesettingwechoosearehighlightedinbold.
B AdditionalExperimentResults
B.1 EvaluationCurvesduringDifferentTrainingStages
WeshowtheevaluationcurvesforthepartialsceneregistrationinFigureA.Wecanobservethatthe
performancerankingofdifferentfoundationmodelsstaysmainlyunchangedthroughoutthetraining
process.
B.2 DiffusionNoiseLevelandFeatureLayer
InTable7,weevaluatetheeffectofdifferentnoiselevel(noisesteps)anddifferentfeaturelayersin
thedecodermoduleinleveragingStableDiffusion(SD)[70]forfeatureextraction. Theresultsshow
thatforSD,addingnoiset<100stepsingeneralleadstothebestperformance. Whentincreases
beyond100steps,theperformancestartstodowngrade. Asfordecoderlayers,thedecodingportion
oftheUNetconsistsof4blocks. Weskipthefinallayerclosesttotheoutputandconsiderlayers0,
1,and2. Theresultsdemonstratethattheoutputfeaturesofthelayeronedecoderleadtothebest
performance. Theseobservationsareconsistentwiththestudyin[6,99].
B.3 AdditionalQualitativeResults
WeshowadditionalqualitativeresultsforpartialsceneregistrationinFigureB,demonstratingthatthe
familyofStableDiffusionandStableVideoDiffusionwhichusetheobjectiveofgenerativepretraining
obtainssuperiorperformance. Inaddition,videoencoderslikeV-JEPAandStableVideoDiffusionare
equippedwithastrongercapabilitytofindgeometriccorrespondences.
18
)°(
rorrE
noitatoR
evitaleR
)m(
rorrE
noitalsnarT
evitaleRDINOv2 LSeg CLIP
𝑃𝑃1 𝑃𝑃2
StableDiffusion V-JEPA StableVideoDiffusion Swin3D
DINOv2 LSeg CLIP
𝑃𝑃1 𝑃𝑃2
StableDiffusion V-JEPA StableVideoDiffusion Swin3D
DINOv2 LSeg CLIP
𝑃𝑃1 𝑃𝑃2
StableDiffusion V-JEPA StableVideoDiffusion Swin3D
FigureB:Visualizationofpartialsceneregistrationresults.ThefamilyofgenerativemodelsStableDiffusionand
StableVideoDiffusionobtainssuperiorperformance.Also,videoencoderslikeV-JEPAandStableVideoDiffusion
havebettergeometricunderstandingcapabilitythanimageencoders.
C LimitationsandFutureWork
Althoughwehavemadeasubstantialefforttoexploretheroleofvisualfoundationmodelsinvarious
sceneunderstandingtasks,ourperceptionofthisproblemremainsrelativelylimited. Thissection
providesadetaileddiscussionofthelimitationsandoutlinespotentialfuturedirections.
Modelcapacitiesnotstrictlyidenticalorcomparable. Ourevaluationfocusesonsevenvision
foundationmodelsduetotheiravailabilityandcommonuseinrecentwork. Consequently,allour
experimentsarebasedonpubliclyavailablecheckpointsreleasedbytheprojectowners. Althoughwe
haveattemptedtochoosemodelswithsimilarcapacities,achievingstrictlyidenticalbackbonearchi-
19tectureswasnotpossiblewithoutre-trainingallthebaselinesourselves. However,suchexperiments
requireanenormousamountofcomputationalresourcesthatwecannotafford.
Ourevaluationfocusesonindoorscenarios. Recentliteratureoftenseparatesthestudyofindoor
scene perception and reasoning from outdoor scenarios, which are often relevant to autonomous
drivingorroboticsapplications. Outdoorscenariospresentdifferentchallengescomparedtoindoor
scenes. Lexicon3D focuses its evaluation solely on indoor scenes. While this is a valid choice
consideringthatmostscene-levelmulti-modalbenchmarksarestillbasedonindoorscenes,itisnot
comprehensive. Outdoorscenarioscontainlargeego-movementspeedsandmanymoredynamic
movingobjectsthanindoorscenes. Evaluatingthesesceneswilllikelyleadtouniqueobservations,
andweconsiderthisadirectfuturedirection.
Weadoptthemoststraightforwardapproachtoprobing. Toevaluatethecapabilitiesofthe
visualfoundationmodels,wefreezetheirparametersandonlytunethelinearorshallowprobing
head. Thisapproachallowsustoanalyzethepretrainedmethods’capabilitieswithoutalteringtheir
modelsthroughthefinetuningprocess. Whilewearguethatprobingthefrozenencoderprovides
themostaccurateunderstandingofthesemodels,weacknowledgethattheabilitytoquicklyadapt
tonewtaskswithfinetuningisalsoanimportantaspectofanencoder. However,finetuningthese
large-scalemodels,whichoftenhaveclosetobillion-levelparameters,requiresasignificantamount
oftimeandcomputationalresources. Weleavethisstudyforfuturework.
D SocietalImpact
Weanticipateapotentialpositivesocialimpactfromourwork. Lexicon3Drepresentsoneofthefirst
stepstowardsacomprehensiveunderstandingoflarge-scalevisualfoundationmodelsinreal-world
3Dsceneanalysisandreasoning. Thisunderstandingcouldleadtothedevelopmentofmorerobust
andefficientsceneencodingsystems,benefitingawiderangeofapplications,includingautonomous
driving,virtualreality,householdrobots,andmulti-modalchatbots. Ultimately,thiscouldcontribute
toamoreinclusive,efficient,andsaferworld,wheretechnologyunderstandsandadaptstothediverse
wayshumansperceiveandnavigatetheirenvironments.
Potentialnegativesocietalimpact. Wedonotseeadirectnegativesocietalimpactonourwork.
Indirectpotentialnegativeimpactinvolvesmisusingstrongsceneencodingfoundationmodelsfor
surveillanceorvirtualreality. Webelieveitiscrucialforresearcherstoproactivelyconsiderthese
concernsandestablishguidelinestoensuretheresponsibleusageofthesemodels.
20