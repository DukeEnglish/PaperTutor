Submitted to INFORMS Journal on Data Science
manuscript IJDS
Non-stationary and Sparsely-correlated Multi-output
Gaussian Process with Spike-and-Slab Prior*
Xinming Wang
DepartmentofIndustrialEngineeringandManagement,PekingUniversity,China.wang-xm20@stu.pku.edu.cn
Yongxiang Li
DepartmentofIndustrialEngineeringandManagement,ShanghaiJiaotongUniversity,China
Xiaowei Yue
DepartmentofIndustrialEngineering,TsinghuaUniversity,China.
Jianguo Wu
DepartmentofIndustrialEngineeringandManagement,PekingUniversity,China.j.wu@pku.edu.cn
Multi-outputGaussianprocess(MGP)iscommonlyusedasatransferlearningmethodtoleverageinforma-
tionamongmultipleoutputs.AkeyadvantageofMGPisprovidinguncertaintyquantificationforprediction,
which is highly important for subsequent decision-making tasks. However, traditional MGP may not be
sufficientlyflexibletohandlemultivariatedatawithdynamiccharacteristics,particularlywhendealingwith
complextemporalcorrelations.Additionally,sincesomeoutputsmaylackcorrelation,transferringinforma-
tionamongthemmayleadtonegativetransfer.Toaddresstheseissues,thisstudyproposesanon-stationary
MGP model that can capture both the dynamic and sparse correlation among outputs. Specifically, the
covariance functions of MGP are constructed using convolutions of time-varying kernel functions. Then a
dynamic spike-and-slab prior is placed on correlation parameters to automatically decide which sources
are informative to the target output in the training process. An expectation-maximization (EM) algorithm
is proposed for efficient model fitting. Both numerical studies and a real case demonstrate its efficacy in
capturing dynamic and sparse correlation structure and mitigating negative transfer for high-dimensional
time-series data. Finally, a mountain-car reinforcement learning case highlights its potential application in
decision making problems.
Key words: Transfer learning, Gaussian process, non-stationary correlation, negative transfer.
1. Introduction
Gaussian process (GP) provides an elegant and flexible Bayesian non-parametric framework for
modeling nonlinear mappings (Williams and Rasmussen 2006). Characterized solely by mean and
covariance functions, it is capable of capturing complex input-output relationships, as well as
measuring prediction uncertainty which is critical for decision-making. As a result, GP has been
widely applied in various fields, such as Bayesian optimization (Frazier 2018), experiment design
*The code capsule has been submitted to Code Ocean with provisional DOI: 10.24433/CO.4010696.v1.
1
4202
peS
5
]LM.tats[
1v94130.9042:viXraWang et al.: Non-stationaryandSparsely-correlatedMGP
2 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
(Gramacy 2020), and product quality monitoring (Zhang et al. 2016). However, standard GP is
designed for only one single output, which limits its use in multi-output or multi-task scenarios
arisinginvariousfields,suchasBayesianoptimization(Shenetal.2023),trafficnetwork(Rodrigues
et al. 2019), and computer simulation emulator (Fricker et al. 2013). Consequently, multi-output
Gaussian process (MGP) has been gaining increasing attention from researchers and has emerged
asanimportantmemberinthevastfamilyoftransferlearning(PanandYang2009)andmulti-task
learning (Zhang and Yang 2021) methods.
Stationary GP and MGP models are commonly used with covariance function depending only
onthedistanceamongdatapoints.However,thisinvariancetoinputspacetranslationmakesthem
unsuitable for non-stationary environments, where the data characteristics vary across the input
domain (Williams and Rasmussen 2006). This phenomenon is quite common in time-series data.
For instance, in the energy field, the mean of power consumption in a household is different in
every season (Paun et al. 2023). In clinical studies, sepsis is very likely to cause changes in the
cross-correlation among vital signs in the early onset (Fairchild et al. 2017). In kinesiology, the
cooperation patterns of human joints vary across different gestures, e.g., both hands move jointly
in a ‘shoot’ action but separately in a ‘throw’ action (Xu et al. 2022). In such cases, non-stationary
models, that allow all or a subset of parameters to vary are generally more appropriate. Modeling
and capturing such a structural change are important in subsequent decision making tasks, such
as identifying the risk of disease and taking a medical care (Paun et al. 2023).
Mainly two kinds of methods have been proposed to capture the dynamic characteristics of the
non-stationary data. The first category assumes that the parameters are the same within local
regions but different across regions. For example, a Bayesian tree-based GP (Gramacy and Lee
2008) uses a tree structure to partition the input space of computer simulation emulator. Another
method,calledjumpGP,cutstheinputspaceintoseveralsegmentstomodelpiece-wisecontinuous
functions(Park2022).Thismodelisoptimizedusingtheexpectation-minimization(EM)algorithm
or variational inference. Besides, a clustering-based GP (Heaton et al. 2017) partitions the spatial
data into groups by calculating a cluster dissimilarity and constructs stationary GPs for each
group of data. A space-partitioning based GP is further extended to the active learning area to
accelerate the design of experiments of heterogeneous systems (Lee et al. 2023). However, these
methods are not suitable for data with gradually-changing characteristics. To address this issue,
the methods in the second category abandon the locally-stationary assumption. They allow all or
some parameters to be input/time-dependent, and model those parameters by additional GPs. For
instance,non-stationaryGPintroducedin(Heinonenetal.2016)and(Paunetal.2023)appliesGP
priors on the amplitude and length-scale parameters of a square-exponential covariance function.
In addition, based on these single-output non-stationary GPs, researchers have explored MGPWang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 3
models for multivariate data with dynamic characteristics. For example, a non-stationary MGP is
established to model the varying correlation between vital signals, where a GP prior is imposed on
the time-dependent correlation matrix (Meng et al. 2021). However, in this state-of-the-art MGP
model, the GP prior has no shrinkage effect, which does not encourage a sparse estimation of
cross-correlation among multiple outputs.
Pursuing sparse estimation of cross-correlation is rooted in the negative information transfer,
whichisanothercriticalchallengewhenusingMGP.Transferlearningisverypromisingforleverag-
ing information to data-insufficient domain, called target domain, from other correlated domains,
called source domain. However, not all the data from the source domain are necessarily correlated
with the target domain. If knowledge is transferred from uncorrelated domains, it may reduce the
performanceoftargetlearning,knownasnegativetransfer(PanandYang2009,YoonandLi2018).
For example, the motion signal of a specific human joint may only be correlated with a subset
of the other joints. In order to recover the joint’s motion information by borrowing information
from others, it is necessary to detect which joints share similar moving trajectories with the target
joint. Therefore, it is crucial that researchers or engineers can make the best choice on using which
sources to transfer information.
Negative transfer exists widely in transfer learning, often stemming from the excessive inclusion
of source data. To handle this issue, one straightforward approach is to measure the relatedness of
each source to the target and choose the most related one for information transfer. For example,
the method proposed in (Ye and Dai 2021) takes Jensen-Shannon (JS) divergence as a criterion
and selects the source with the least divergence for knowledge transfer. However, such a method
only takes the pairwise transferability into account and ignores the global structure between the
targetandthesources.Andthischoiceismadeindependentlyonthespecificmodelbeforetraining,
which is far from an optimal decision. An alternative approach is the regularized MGP (Wang
et al. 2022), which jointly models all outputs and selects informative sources during the training
process. However, all these approaches assume that the source-target cross-correlation is fixed in
the time space, and thus cannot model the dynamic and sparse structure among multiple outputs.
To this end, we propose a non-stationary MGP model to capture the varying characteristics
of data and mitigate the negative transfer simultaneously. Specifically, we focus on modeling the
dynamic and sparse correlations between the sources and the target. In the proposed framework,
we first construct a convolution-process-based MGP for transfer learning, whose covariance func-
tion parameters are allowed to vary in time space. We then apply a spike-and-slab prior to the
parameters that are related to the sparse correlation between the sources and the target. The slab
part mainly accounts for smoothly-changing or constant correlation parameters, while the spike
part is responsible for shrinking some parameters to zero, thereby removing the correspondingWang et al.: Non-stationaryandSparsely-correlatedMGP
4 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
uninformativesources.Tothebestofourknowledge,thisisthefirstresearchonMGPthatsimulta-
neously handles dynamic relationship and negative transfer. Our contributions can be summarized
as follows:
1. A novel non-stationary MGP model is established using the convolution of latent processes
and time-dependent kernel functions, which is suitable for modeling multiple outputs with
varying characteristics.
2. A dynamic spike-and-slab prior is applied to capture the temporal and sparse correlations
among outputs, deciding from which sources to transfer information to the target.
3. The mixture weight of the spike and slab priors is automatically adjusted during the training
process using an EM-based optimization algorithm, which can effectively prevent placing
shrinkage effects on non-zero elements.
The rest of this paper is organized as follows. In Section 2, we revisit the related literature and
the static MGP model. Section 3 presents the proposed non-stationary MGP and an efficient EM
algorithm for model training. In Section 4, we evaluate the effectiveness of our model on simulated
data. In Section 5, we perform one time-series analysis case on human gesture data (Fothergill
et al. 2012) and one control policy optimization case on the mountain-car problem (Moore 1990).
In Section 6, we conclude the paper with a discussion.
2. Preliminaries
In this section, we first review researches that are related to our work. We then introduce the static
MGP based on the convolution process, which has been widely applied in various areas due to its
flexibility (Boyle and Frean 2004, Kontar et al. 2018, Hu and Wang 2021).
2.1. Related work
To deal with non-stationary data, a natural extension of GPs is to release the restriction that
the parameters of the covariance functions are invariant throughout the input space. Most of the
existing approaches either encourage the parameters to be constant in a local area and construct
a piece-wise model (the first category, e.g., (Gramacy and Lee 2008, Heaton et al. 2017, Lee et al.
2023, Park 2022)), or allow them to vary at each point and model them using other GPs (the
second category, e.g., (Paciorek and Schervish 2003, Heinonen et al. 2016, Paun et al. 2023)). The
methods in the second category have a similar structure to that of a two-layer Deep Gaussian
Process (Deep GP), where the input is first transformed by the first GP layer into a latent input,
and then fed into the second GP layer to obtain the output (Damianou and Lawrence 2013, Ko
and Kim 2022, AlBahar et al. 2022). However, the parameters of Deep GP are stationary, which
differs from the second category where the covariance parameters are dynamic.Wang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 5
Non-stationary GPs mainly focus on modeling the dynamic mean, smoothness, and amplitude
parameters. With regards to MGP, dynamic correlation is another key characteristic that needs to
be considered. In classical Linear model of Coregionalization (LMC), each output is a linear com-
bination of several latent Gaussian processes, and the covariance matrix is modeled by a Kronecker
product of a correlation matrix and a single GP’s covariance matrix (Goulard and Voltz 1992).The
existing non-stationary MGPs are mainly extensions of the classical LMC model. For example, the
approach in (Gelfand et al. 2004) allows the correlation matrix to vary with inputs to model the
dynamic relationship among outputs. In (Meng et al. 2021), a non-stationary MGP combines the
time-varying correlation (across outputs) and smoothness (within each output) together. However,
as extensions of the traditional LMC, these methods also suffer from the limitation that all out-
puts possess the same covariance structure. More flexible MGPs are proposed by constructing each
output through the convolution process and modeling them with individual parameters (Boyle
and Frean 2004). However, these approaches are for stationary data. Furthermore, all existing
approaches fail to capture a sparse correlation structure in a non-stationary environment.
Spatial-temporal modeling of non-stationary data is closely related to our work. In compar-
ison with normal time-series modeling, spatial-temporal analysis requires to model the spatial
correlation to enhance the prediction accuracy. A large number of spatial-temporal models have
beeninvestigated,suchasspatial-temporalauto-regressiveintegratedmovingaveragemethod(ST-
ARIMA) (Stathopoulos and Karlaftis 2003), spatial-temporal k-nearest neighbors (ST-KNN) (Xia
etal.2016),spatial-temporalrandomfields(Christakos2012,Yunetal.2022),andspatial-temporal
deep neural networks (Wang et al. 2020b, Wen et al. 2023). Based on the aforementioned methods,
a number of recent works try to extend them to handle non-stationary spatial-temporal data. One
popular and efficient solution is utilizing some change detection algorithm to partition the time-
series into several stationary periods, and then applying the stationary model for each period, e.g.,
a ST-KNN with a wrapped K-means partition algorithm (Cheng et al. 2021), an auto-regressive
modelcoupledwithablock-fused-Lassochange-pointdetectionalgorithm(Baietal.2022).Besides
partitioning the time-series into stationary parts, the method proposed by (Shand and Li 2017)
maps the non-stationary space-time process into a high-dimensional stationary process through
augmenting new dimensions. Another type of non-stationary spatial-temporal model is Bayesian
random fields with non-stationary space-time kernels (Garg et al. 2012, Ton et al. 2018, Wang
et al. 2020a, Zou et al. 2023), whose hyper-parameters change over time or location. Deep learning
methods are also explored on non-stationary data recently, such as non-stationary recurrent neural
networks (Rangapuram et al. 2018, Liu et al. 2020), long short-term memory networks (Wang
et al. 2019), and transformer-based networks (Liu et al. 2022, Wen et al. 2023). In contrast to
the spatial-temporal model, MGP does not impose a restriction that the source outputs must beWang et al.: Non-stationaryandSparsely-correlatedMGP
6 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
sampled during the same period as the target outputs. Furthermore, it does not depend on spa-
tial distance to establish correlations among outputs. As a result, the MGP model is capable of
accommodating a wider range of scenarios.
Itisimportanttomentionthatweuseadynamic spike-and-slabpriorinourmodel.Theclassical
spike-and-slab prior is a Bayesian selection approach (George and McCulloch 1993) that has been
used for feature selection in (generalized) linear models, additive models, and Gaussian processes
(Ishwaran and Rao 2005, Scheipl et al. 2012, Dance and Paige 2022). With this prior, smaller
parameters tend to be more influenced by the spike prior to reach zero, while the larger ones are
mainly dominated by the slab part and bear little shrinkage. However, the classical spike-and-slab
prior cannot account for the modeling of dynamic and sparse correlation parameters in our model.
Therefore, we propose to extend this prior to a dynamic version. Although current works have
explored the dynamic variable selection for the varying-coefficient linear models (Kalli and Griffin
2014, Huber et al. 2021, Rockova and McAlinn 2021), no work utilizes a dynamic spike-and-slab
prior to model the dynamic and sparse correlation among outputs in a non-stationary MGP.
2.2. Static MGP based on convolution process
Consider a set of m outputs f :X (cid:55)→R, i=1,...,m, where X is a d-dimensional input domain
i
applied to all outputs. Suppose that the observation y is accompanied with independent and
i
identically distributed (i.i.d.) noise ϵ ∼N(0,σ2), i.e.,
i i
y (x)=f (x)+ϵ .
i i i
where x∈Rd is the input. Denote the n observed data for the ith output as D ={X ,y }, where
i i i i
X = (x ,...,x )T and y = (y ,...,y )T are the collections of input points and associated
i i,1 i,ni i i,1 i,ni
(cid:80)
observations respectively. Let the total number of observations be represented by N = n for
i i
m outputs. Denote the data of all outputs as D = {X,y}, where X = (XT,...,XT)T ∈ RN×d
1 m
and y=(yT,...,yT)T ∈RN. In an MGP model, the observation vector y follows a joint Gaussian
1 m
distribution:
(cid:0) (cid:1)
y|X∼N 0,K , (1)
where K =K(X,X)∈RN×N is a block-partitioned covariance matrix. The (i,i′)-th block of K,
K i,i′ = covf i,i′(X i,X i′)+τ i,i′σ i2I ∈ Rni×n i′ represents the covariance matrix between the output
i and output i′ (τ equals to 1 if i=i′, and 0 otherwise). The function covf (x,x′) measures
i,i′ i,i′
the covariance between f (x) and f (x′). In the covariance matrix K, the cross-covariance block
i i′
K (i̸=i′) is the most important part to realize information transfer, as it models the correlation
i,i′
between different outputs.Wang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 7
As the convolution of a GP and a smoothing kernel is still a GP, we can construct each output f
i
through convolving a group of shared latent processes {z (x)}h and kernel functions {g (x)}h
j j=1 ji j=1
in the following way (Boyle and Frean 2004, Alvarez and Lawrence 2011):
h h (cid:90) ∞
(cid:88) (cid:88)
f (x)= α g (x)∗z (x)= α g (x−u)z (u)du (2)
i ji ji j ji ji j
j=1 j=1 −∞
where ∗ represents convolution operation, α is the amplitude parameter, and h is the number of
ji
shared latent processes. Usually, {z (x)}h are independent white Gaussian noise processes with
j j=1
cov(z(x),z(x′))=δ(x−x′), where δ(·) is the Dirac delta function. Thus, the covariance function
can be derived as:
h
(cid:88)
covf (x,x′)=cov[f (x),f (x′)]= cov{α g (x)∗z (x),α g (x′)∗z (x′)}
i,i′ i i′ ji ji j ji′ ji′ j
j=1
h (cid:90) ∞
(cid:88)
= α α g (u)g (u−v)du, (3)
ji ji′ ji ji′
j=1 −∞
where v=x−x′. It such a way, the covariance between f (x) and f (x′) is dependent on their
i i′
difference x − x′, the amplitude parameters, and the hyperparameters in kernels g and g .
ji ji′
Compared with the classical LMC model f
(x)=(cid:80)h
α q (x), where q (x) is a latent GP with
i j=1 ji j j
covariance k (x,x′). The convolution-process-based MGP is more flexible than LMC, as it does
j
not restrict all outputs to having the same auto-covariance pattern.
At a new point x , the posterior distribution of y (x ) given data {X,y} is:
∗ i ∗
y (x )|X,y∼N (µ(x ),Σ(x )), (4)
i ∗ ∗ ∗
where the predictive mean µ(x ) and variance Σ(x ) can be expressed as:
∗ ∗
µ(x )=KTK−1y, (5)
∗ ∗
Σ(x )=covf(x ,x )+σ2−KTK−1K , (6)
∗ ii ∗ ∗ i ∗ ∗
where KT =[covf (x ,X )T,...,covf (x ,X )T] is the covariance between the new point x and
∗ i1 ∗ 1 im ∗ m ∗
all observed data. From the posterior distribution, we can find that the covariance function plays a
crucialroleinprediction.Forinstance,thepredictedmeanisthelinearcombinationofoutputdata,
where the weight is decided by the covariance matrix. However, in the static MGP, the covariance
between two data points depends solely on their distance and does not change dynamically. Addi-
tionally, some outputs may be uncorrelated with others, therefore the estimated covariance matrix
should possess a sparse structure to avoid negative transfer between the uncorrelated outputs. In
the following section, we will propose a novel non-stationary MGP to simultaneously address both
problems.Wang et al.: Non-stationaryandSparsely-correlatedMGP
8 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
Figure1 The graphical structure of non-stationary MGP. Latent processes and kernel functions are with a gray
background, while the parameters are with a white background. The parameters’ priors are shown in rectangles.
𝑖𝑖∈[1,𝑚𝑚−1]
𝒇𝒇𝒎𝒎
First layer 𝒇𝒇𝒊𝒊
+
𝜶𝜶𝒊𝒊𝒊𝒊,𝒕𝒕 𝜶𝜶𝒊𝒊𝒎𝒎,𝒕𝒕 𝜶𝜶𝒎𝒎𝒎𝒎,𝒕𝒕
convolve
𝒈𝒈𝒊𝒊𝒊𝒊,𝒕𝒕∗ ∗𝒈𝒈𝒊𝒊𝒎𝒎,𝒕𝒕 𝒈𝒈𝒎𝒎𝒎𝒎,𝒕𝒕∗
summarize
∗
+ 𝒊𝒊 𝒛𝒛𝒎𝒎
𝒛𝒛
𝜽𝜽𝒊𝒊𝒊𝒊,𝒕𝒕 𝜽𝜽𝒊𝒊𝒎𝒎,𝒕𝒕 𝜽𝜽𝒎𝒎𝒎𝒎,𝒕𝒕
Second layer 𝜸𝜸⋅𝒕𝒕𝒑𝒑𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔(𝜶𝜶⋅𝒎𝒎,𝒕𝒕|𝜶𝜶⋅𝒎𝒎,𝒕𝒕−𝟏𝟏)
𝒑𝒑𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔(𝜶𝜶𝒊𝒊𝒊𝒊,𝒕𝒕|𝜶𝜶𝒊𝒊𝒊𝒊,𝒕𝒕−𝟏𝟏)
+(𝟏𝟏−𝜸𝜸⋅𝒕𝒕)𝒑𝒑𝒔𝒔𝒑𝒑𝒊𝒊𝒑𝒑𝒑𝒑(𝜶𝜶⋅𝒎𝒎,𝒕𝒕)
𝒑𝒑𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔(𝜽𝜽𝒊𝒊𝒊𝒊,𝒕𝒕|𝜽𝜽𝒊𝒊𝒊𝒊,𝒕𝒕−𝟏𝟏) 𝒑𝒑𝒔𝒔𝒔𝒔𝒔𝒔𝒔𝒔(𝜽𝜽⋅𝒎𝒎,𝒕𝒕|𝜽𝜽⋅𝒎𝒎,𝒕𝒕−𝟏𝟏)
3. Model Development
We propose a non-stationary MGP model for transfer learning that can capture sparse source-
target correlation in a dynamic environment. Specifically, we assume that the correlations between
the target and each source vary over time. Besides, some sources may not be related to the target
during certain time periods. Under such a circumstance, a spike-and-slab prior is utilized to model
the varying and sparse correlation structure.
3.1. The proposed model.
The structure of our hierarchical model is illustrated in Fig. 1. The first layer constructs outputs
through the convolution of time-dependent kernel functions and latent white Gaussian noise pro-
cesses,andthesecondlayerconsistsofpriorsonfunctionparametersdesignedtoencouragedesired
properties, such as smoothness and sparsity.
3.1.1. Dynamic MGP In this subsection, we introduce the major part of the proposed non-
stationaryMGP,whichcorresponds tothefirstlayerinFig. 1.Tosimplifythe notation,weslightly
abuse the notation used in the previous section. Specifically, we take the first m−1 outputs
f :X (cid:55)→R, i=1,...,m−1 as the sources, and the last one f :X (cid:55)→R as the target. We still
i m
assume that the observation y is accompanied with the i.i.d. measurement noise ϵ ∼N(0,σ2). Let
i i i
I={1,2,...,m} be the index set of all outputs, and IS =I/{m} contain the indices of all sources.
For simplicity yet without loss of generality, we assume the source and target data are sampled atWang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 9
thetimet∈{1,2,...,n},i.e.,X =(x ,...,x ,...,x )T andy =(y ,...,y ,..,y )T.InAppendix
i i,1 i,t i,n i i,1 i,t i,n
A, we will show a more general case where each output is observed only at a subset of time stamps.
Based on the above assumption, our dynamic MGP model is formulated as:
y (x )=f (x )+ϵ =α g (x )∗z (x )+ϵ ,i∈IS
i t i t i ii,t ii,t t i t i
(cid:88)
y (x )=f (x )+ϵ = α g (x )∗z (x )+ϵ (7)
m t m t m jm,t jm,t t j t m
j∈I
where α and α are time-varying amplitude parameters, g (x) and g (x) are time-varying
ii,t jm,t ii,t jm,t
kernel functions, and {z (x)}m are latent white Gaussian noise processes independent of each
i i=1
other. This model is highly flexible as various types of kernel functions can be utilized. We choose
to employ a Gaussian kernel which is widely used due to its flexibility (Boyle and Frean 2004,
Alvarez and Lawrence 2011). The kernel is given by
(cid:18) (cid:19)
1
g ij,t(x)=(2π)−d 4|θ ij,t|−1 4 exp − 2xTθ i− j,1 tx , (8)
where θ is a diagonal matrix representing the length-scale for each input feature. More impor-
ij,t
tantly, such a Gaussian kernel can yield closed-from covariance functions through the convolution
operation in Eq. (3) (Paciorek and Schervish 2003, Wang et al. 2020a).
Thisflexiblemodelallowseachsourcetohaveitsownkernelg ,therebyallowingforheterogene-
ii
ity among the sources. In order to transfer knowledge from the sources to the target, the target
is connected to {z }m−1 though the kernel function g . Regarding the parameters, {α ,θ }m
i i=1 im,t ii,t ii,t i=1
are responsible for the non-stationary behavior within each output, while {α ,θ }m−1 capture
im,t im,t i=1
the dynamic correlation between target and sources. More specifically, the amplitude parameter
α controlstheknowledgetransfer.Forexample,ifα =0,thenf willnottransferinformation
im,t im,t i
to f at time t.
m
As the latent processes are independent of each other, the covariance matrix among sources is
zero-valued. Therefore, the covariance matrix can be re-partitioned as:
K ··· 0 K 
1,1 1,m
K=  . . . ... . . . . . .  =(cid:18) K (ss) K (sm)(cid:19) , (9)
 0 ···K m−1,m−1K m−1,m K (T sm) K mm
KT ··· KT K
1,m m−1,m m,m
where the (i,i′)-th block K =covf (X ,X )+τ σ2I, the block-diagonal matrix K rep-
i,i′ i,i′ i i′ i,i′ i (ss)
resents the covariance of source outputs, and K represents the cross-covariance between the
(sm)
sources and the target. Based on Eq. (3), we can obtain covariance functions for the proposed
non-stationary MGP model, as shown below:
covf(x ,x )
ii t t′Wang et al.: Non-stationaryandSparsely-correlatedMGP
10 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
=α α
|θ ii,t|41|θ ii,t′|41 exp(cid:20) −1
(x −x )T(θ +θ )−1(x −x
)(cid:21)
, (10a)
ii,t ii,t′ |θ ii,t+θ ii,t′|21 2 t t′ ii,t ii,t′ t t′
covf (x ,x )
im t t′
=α α
|θ ii,t|41|θ im,t′|41 exp(cid:20) −1
(x −x )T(θ +θ )−1(x −x
)(cid:21)
, (10b)
ii,t im,t′ |θ ii,t+θ im,t′|21 2 t t′ ii,t im,t′ t t′
covf (x ,x )
mm t t′
=(cid:88)
α α
|θ jm,t|1 4|θ jm,t′|41 exp(cid:20) −1
(x −x )T(θ +θ )−1(x −x
)(cid:21)
, (10c)
j∈I
jm,t jm,t′ |θ jm,t+θ jm,t′|21 2 t t′ jm,t jm,t′ t t′
where i ∈ IS. Equations (10a- 10c) represent the covariance within the sources, between the
sources and the target, and within the target, respectively. To ensure the positivity of those hyper-
parameters, we utilize a soft-plus transformation for them (Heinonen et al. 2016): α =log[1+
ij,t
exp(α˜ )],θ =log[1+exp(θ˜ )], where α˜ ,θ˜ are underlying parameters to estimate, whose
ij,t ij,t ij,t ij,t ij,t
range is [−∞,∞]. The proposed covariance functions can be viewed as an extension of the non-
stationary kernels (Paciorek and Schervish 2003) from the single-output to the multi-output case.
Specifically, the auto-covariance of each source in Eq. (10a) is the same as the covariance for a
single-output non-stationary GP. From the cross-covariance between each source and the target,
we can clearly see that the amplitude parameter α controls whether the cross-correlation is zero
im,t
or not. The validity of the proposed covariance functions is outlined in Proposition 1:
Proposition 1. The proposed non-stationary MGP covariance matrix in Eq. (9) is positive-
definite, i.e., ∀y̸=0,
yTKy>0.
The proof is provided in Appendix B.
Based on Eq. (9), the joint distribution of all sources and the target is expressed as:
(cid:18)
y
(cid:19)(cid:12) (cid:18)(cid:20) 0(cid:21) (cid:20)
K K
(cid:21)(cid:19)
(s) (cid:12)X∼N , (ss) (sm) , (11)
y (cid:12) 0 KT K
m (sm) mm
where y is the collection of all source data. For notational convenience, we partition the param-
(s)
eters in a similar way, α = {α }n ,θ = {θ }n ,σ = {σ }m−1,α = {α }n ,θ =
(s) (s),t t=1 (s) (s),t t=1 (s) i i=1 m m,t t=1 m
{θ }n , where α = {α }m−1, θ = {θ }m−1, α = {α }m , and θ = {θ }m .
m,t t=1 (s),t ii,t i=1 (s),t ii,t i=1 m,t im,t i=1 m,t im,t i=1
Furthermore, we denote the collection of all parameters as Φ = {Φ ,Φ }, where Φ =
(s) m (s)
{α ,θ ,σ } and Φ ={α ,θ ,σ }.
(s) (s) (s) m m m m
In the proposed model, the most important and challenging task is to estimate those time-
varyingparameters.Ifnorestrictionisapplied,modeltrainingmaysufferfromaseriousover-fitting
problem. To address this issue, Gaussian processes are typically employed to model the kernel
parameters, e.g., log(α ) ∼ GP(0,k (t,t′)),log(θ ) ∼ GP(0,k (t,t′)), where k ,k are covariance
t α t θ α θWang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 11
functions for the amplitude and length-scale parameters respectively. Although this technique can
force the parameters to vary smoothly and reduce over-fitting, it cannot model a sparse correlation
betweenthesourcesandthetarget.Consequently,thisapproachcannotavoidthenegativetransfer
caused by unrelated sources.
3.1.2. Dynamic spike-and-slab prior. The classical spike-and-slab prior (George and
McCulloch 1993, Dance and Paige 2022) only handles the shrinkage of one single parameter and
cannot model the smooth-varying parameters. To take both the dynamic and sparse property of
correlation into account, we propose a dynamic spike-and-slab prior placing on α :
m
α |γ ,α ∼(1−γ )p (α )+γ p (α |α )
im,t i,t im,t−1 i,t spike im,t i,t slab im,t im,t−1
γ |η∼Bern(η), (12)
i,t
where γ ∈ {0,1} is a binary sparse indicator for α following a Bernoulli distribution,
i,t im,t
p (α ) is a zero-mean spike prior pushing parameter to zero, p (α |α ) is a slab prior
spike im,t slab t,im t−1,im
connecting α and α , and η is a prior weight between the spike and slab. If there is no prior
im,t−1 im,t
information regarding the weight, we can set η to 0.5. The spike-and-slab prior is shown in the
second layer of the graphical structure in Fig. 1. As for all the other parameters, we do not force
them to possess sparsity, so only the slab prior is placed on them to control the smoothness, i.e.,
α |α ∼p (α |α ),
ii,t ii,t−1 slab ii,t ii,t−1
θ |θ ∼p (θ |θ ),
ii,t ii,t−1 slab ii,t ii,t−1
θ |θ ∼p (θ |θ ). (13)
im,t im,t−1 slab im,t im,t−1
Note that θ is a diagonal matrix, so that the slab prior is placed on its d diagonal elements
ij,t
independently, i.e., p (θ |θ
)=(cid:81)d
p ({θ } |{θ } ). By using the conditional distri-
slab ii,t ii,t−1 l=1 slab ii,t l ii,t−1 l
butions as priors, we can control the change of amplitude or smoothness from the previous time
step to the current one, e.g., from α to α . Compared with the dynamic spike-and-slab prior
ii,t−1 ii,t
used in linear models (Rockova and McAlinn 2021), our method does not constrain the slab prior
to be a stable autoregressive process. Besides, we use a simpler but more flexible prior for γ ,
i,t
while the work in (Rockova and McAlinn 2021) uses a prior conditional on the coefficients of the
linear model.
Thespikepriorisresponsibleforshrinkingtheparameterstozeroandcuttingdowntheinforma-
tion transfer channel to the target. Common choices for this prior include point mass distribution,
Laplace distribution, and Gaussian distribution with a small variance. Considering the shrinkageWang et al.: Non-stationaryandSparsely-correlatedMGP
12 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
performance and optimization convenience, we choose the Laplace distribution as the spike prior,
i.e.,
(cid:18) (cid:19)
1 |α |
p (α )= exp − im,t , (14)
spike im,t 2ν ν
0 0
where ν is the length-scale for Laplace distribution. If we maximize the log-likelihood function
0
to optimize the model, this prior will become a L norm penalty and have the ability to shrink
1
parameters. The slab prior encourages the smoothness of parameter change. In this work, we
consider two types of slab priors. The first one is a hard slab prior,
(cid:18) (cid:19)
1 |α −α |
phard(α |α )= exp − im,t im,t−1 , (15)
slab im,t im,t−1 2ν ν
1 1
which encourages α to remain constant in a continuous period, approximating a piecewise
im,t
model. In the second one, the parameters are allowed to change smoothly,
1
(cid:18)
(α −ρα
)2(cid:19)
psoft(α |α )= √ exp − im,t im,t−1 , (16)
slab im,t im,t−1 2πν 2ν
1 1
where ν is variance of Gaussian distribution, and ρ<1 is an autoregressive coefficient. A similar
1
smoothing approach can also be found in (Hu and Wang 2021). These two slab priors make the
current parameter value exactly or roughly concentrated around the previous value. Typically, we
√
set ν to be much smaller than ν in the soft slab prior (or ν in the hard slab prior) to make the
0 1 1
twopriorsmoreseparableandtoputmorepenaltyonsparsity.Besides,thevaluesofα atmultiple
im
time steps before t can be included in the soft slab prior, e.g., psoft(α |α ,α ,...). We
slab im,t im,t−1 im,t−2
choose the simplest form psoft(α |α ) due to its wide application and robust performance
slab im,t im,t−1
in practice.
At time t, η can be interpreted as the prior probability that α belongs to the slab process. It
im,t
influences the strength of shrinkage effect that α bears. Ideally, for non-zero α , the posterior
im,t im,t
mean of γ should be close to one so that α is barely impacted by the spike prior. In the
i,t im,t
optimization algorithm developed in the next subsection, we will show that the estimated mean of
γ is automatically adjusted based on the estimated α to avoid shrinking non-zero elements.
i,t im,t
This makes our method superior to traditional Lasso methods where the sparse penalty weights
are identical for zero and non-zero parameters.
Finally, based on the above discussion, the whole hierarchical model of the proposed non-
stationary MGP can be expressed as follows:
y ,y |Φ ,Φ ∼N(0,K)
(s) m (s) m
α |α ∼p (α |α ),i∈IS,
ii,t ii,t−1 slab ii,t ii,t−1Wang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 13
θ |θ ∼p (θ |θ ),i∈IS,
ii,t ii,t−1 slab ii,t ii,t−1
α |γ ,α ∼(1−γ )p (α )+γ p (α |α ),i∈I,
im,t i,t im,t−1 i,t spike im,t i,t slab im,t im,t−1
γ |η∼Bern(η),i∈I,
i,t
θ |θ ∼p (θ |θ ),i∈I. (17)
im,t im,t−1 slab im,t im,t−1
3.2. Expectation-maximization-based optimization algorithm
The widely-used algorithm for a Bayesian model is Markov Chain Monte Carlo (MCMC) sam-
pling,butitiscomputationally-inefficientfortheproposednon-stationarymodelwithconsiderable
time-varyingparameters.Therefore,wedevelopanefficientEMalgorithm.Insteadofdirectlymax-
imizingtheposteriorp(Φ|y)=p(Φ ,Φ |y ,y ),weproceediterativelyintermsofthecomplete
(s) m (s) m
logposteriorlogp(Φ,γ|y),wherethebinaryparametersγ aretreatedas“missingdata”.Sincethis
function is not observable, in the Expectation-step (E-step), we calculate its conditional expecta-
tiongiventheobserveddataandthecurrentestimatedparameters.ThenintheMaximization-step
(M-step), we maximize the expected complete log-posterior with respect to Φ. More precisely, the
E-step and M-step at the (k+1)th iteration can be expressed as:
(cid:0) (cid:1)
E−step: Q Φ|Φ(k) =E {logp(Φ,γ|y)},
γ|Φ(k),y
(cid:8) (cid:0) (cid:1)(cid:9)
M−step: Φ(k+1)=argmax Q Φ|Φ(k) (18)
Φ
where E (·) is the conditional expectation on posterior of γ, and Φ(k) is the optimized
γ|Φ(k),y
parameters at the kth iteration. For simplicity, we use E (·) to denote E (·).
γ γ|Φ(k),y
Based on Bayes’ Theorem and the property of multivariate normal distribution, the expectation
of logp(Φ,γ|y) can be as (derivation details can be found in Appendix C):
1(cid:110) (cid:111)
E {logp(Φ,γ|y)}=− yT K−1 y +log|K |+(y −µ)TΣ−1(y −µ)+log|Σ|
γ 2 (s) (ss) (s) (ss) m m
m (cid:88)−1 (cid:88)n (cid:104) (cid:105)
+ logp (θ |θ )+logp (α |α )
slab ii,t ii,t−1 slab ii,t ii,t−1
i=1 t=2
(cid:88)m (cid:88)n (cid:104)
+ logp (θ |θ )+(1−E γ )logp (α )
slab im,t im,t−1 γ i,t spike im,t
i=1 t=2
(cid:105)
+E γ logp (α |α ) +const., (19)
γ i,t slab im,t im,t−1
where µ=KT K−1 y is the conditional mean of target given the sources and Σ=K −
(sm) (ss) (s) mm
KT K−1 K is the conditional covariance.
(sm) (ss) (sm)
In the E-step, since γ is only dependent on Φ , the posterior of γ is calculated as:
m i,t
p(α(k) |γ )p(γ )
p(γ |Φ(k))= im,t i,t i,t ∝p(α(k) |γ )p(γ )
i,t m p(α(k) ) im,t i,t i,t
im,t
(cid:104) (cid:105)
= (1−γ )p (α(k) )+γ p (α(k) |α(k) ) ·ηγi,t(1−η)(1−γi,t). (20)
i,t spike im,t i,t slab im,t im,t−1Wang et al.: Non-stationaryandSparsely-correlatedMGP
14 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
Then the conditional expectation of γ can be updated as:
i,t
ηp (α(k) |α(k) )
E γ = slab im,t im,t−1 , (21)
γ i,t (1−η)p (α(k) )+ηp (α(k) |α(k) )
spike im,t slab im,t im,t−1
The posterior mean E γ can be interpreted as the posterior probability of classifying the
γ i,t
current parameter α into a slab process as opposed to a spike process, based on the past value
im,t
α . For example, we set η to 0.5 as a non-informative prior, and take a small ν (e.g., 0.01)
im,t−1 0
for the spike prior and a large ν (e.g., 0.1) for the soft slab prior. Based on Eq. (21), supposing
1
that (α −α )2 is small and |α | is large, the expectation E γ will tend towards one,
im,t im,t−1 im,t γ i,t
indicating that α is more likely from the slab prior. On the other hand, if |α | is small, E γ
im,t im,t γ i,t
is close to zero, enforcing strong shrinkage on α .
im,t
In the M-step, we can optimize the objective function in Eq. (19) with various gradient ascent
methods, such as (stochastic) ADAM, L-BFGS, etc. (Kingma and Ba 2015, Zhu et al. 1997). This
objective function is actually a standard Gaussian process log-likelihood with additional regular-
ization terms. The regularization terms penalize the difference between parameters at successive
time points and shrink the amplitude parameters to facilitate source selection. The weights of the
regularization terms are modulated by the expectation of γ. Ideally, for non-zero α , E γ will
im,t γ i,t
equal to one, so no shrinkage effect will be placed on it. In other words, the strength of sparse
penalty is automatically adjusted through Eq. (21), and this adjustment has explicit statistical
interpretability.
In our case studies, we find the algorithm converges rapidly, e.g., achieving convergence after
only five iterations. The whole algorithm is summarized in Algorithm 1, where an ADAM method
(Kingma and Ba 2015) is utilized in the M-step. Note that the large parameter space of Φ poses a
challenge in identifying the modes of the full posterior. To speed up the convergence, we propose
to initialize the source parameter Φ by maximizing the sum of sources’ marginal log-likelihood
(s)
and source parameter prior:
max logp(y |Φ )+logp(Φ ) (22)
(s) (s) (s)
Φ(s)
For target parameters, we find simple random initialization can achieve satisfactory performance
in experiments.
3.3. Computational Challenge
There are three main computational challenges that we need to address. The first one is the
calculation of integration for a convolution kernel. To avoid an intractable integration for the
covariance, we utilize the Gaussian kernel in Eq. (8) and derive closed-form covariance functions
in Equations (10a- 10c).Wang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 15
Thesecondchallengeiscalculatingtheinverseofcovariancematrix,whichisacriticalissueforall
GPs. The computational complexity of Algorithm 1 is approximately
O((cid:80)m
n3), where n is the
i=1 i i
number of data points for each output. In comparison, the computational cost for traditional MGP
is
O(((cid:80)m
n )3) with the same size of data and non-separable covariance. The main computational
i=1 i
load of our model is in calculating the inverse of K and Σ in the source marginal distribution
(ss)
logp(y |Φ ) and the target conditional distribution logp(y |Φ ,y ,Φ ) respectively in Eq.
(s) (s) m m (s) (s)
(19). Since all the latent processes are independent of each other, K is a block-diagonal matrix
(ss)
and the complexity is reduced from
O((cid:80)m−1n3)
to
O((cid:80)m−1n3).
The calculation of the inverse of
i=1 i i=1 i
Σ is O(n3 ). Therefore, the overall computational complexity is reduced to
O((cid:80)m
n3).
m i=1 i
Finally,itisahardtasktoestimateaconsiderablenumberoftime-varyingparameters.Therefore,
we develop the EM-based algorithm to fit the model rather than using a sampling method. Based
on the results of a non-stationary linear model (Rockova and McAlinn 2021), the MCMC and EM
algorithm lead to very close prediction errors, but the running time of MCMC is about ten times
longer than that of the EM.
Algorithm 1 The optimization algorithm for the non-stationary MGP model
Input: Data {X ,y }m , ν , ν , ρ, η.
i i i=1 0 1
1: Set starting value: Φ(−1),Φ(0).
(s) m
2: Initialization: obtain Φ(0) through Eq. (22).
(s)
3: for k iterations do
out
4: E-step given Φ(k): Update {E γ }m,n through Eq. (21).
γ i,t i=1,t=2
5: M-step given E γ :
γ i,t
6: for k epochs do
in
7: Calculate K , K , and K according to Eq. (10).
(ss) (sm) mm
8: Calculate the value and gradient of objective function in Eq. (19).
9: Obtain Φ(k+1) using the ADAM method.
10: end for
11: end for
3.4. Tuning Parameter Selection
The tuning parameters for our model is ν , ν (for the hard slab prior), and ν (for the soft slab
0 1 2
prior). Here, since the key of our method is selecting the most informative sources, we propose to
maximize the following criterion:
B(ν)=Nlogp(y|X)−log(N)c (α ), (23)
ν mWang et al.: Non-stationaryandSparsely-correlatedMGP
16 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
where N is the number of data, logp(y|X) is the log-likelihood for both the source and the
target outputs, and c (α ) is the number of nonzero elements in α given ν. Similar criterion is
ν m m
proposed in (Zhang et al. 2021). Note that ν ={ν ,ν } for the hard slab prior and ν ={ν ,ν }
0 1 0 2
for the soft slab prior. This criterion is similar to the Bayesian Information Criterion (BIC). The
first term tends to select a more complex model with a larger likelihood for all outputs, while
the second term prefers simpler models where less sources chosen to transfer information to the
source. To reduce the computation, we first determine the ratios r = ν /ν and r = ν /ν to
1 1 0 2 2 0
make the spike and slab priors separable (Rockova and McAlinn 2021). Then we design a two-
dimension search grid for (ν ,ν /r ) with the hard slab or (ν ,ν /r ) with the soft slab. The
1 1 1 2 2 1
optimal value of ν is searched over the two-dimensional grid. For example, we set r ∈ {5,10}
1
and (ν ,ν )∈{(1/5,1/25),(1/5,1/50),(1/10,1/50),(1/10,1/100),(1/15,1/75),(1/15,1/150)} for a
1 0
hard slab prior.
3.5. Model Prediction
Since the EM algorithm only estimates the value of parameters at the observed time points, given
a new input x of interest, we first need to estimate the target parameter α and θ at the
t∗ m,t∗ m,t∗
new time point t∗, then derive the predictive distribution of y (x ).
m t∗
3.5.1. Forecasting. In the forecasting task, t∗>n. The estimated value of θ is,
im,t∗
(cid:26)
θ , for hard slab prior,
θ = im,n
im,t∗ ρt∗−nθ , for soft slab prior,
im,n
which is actually the mode of p (θ |θ ). As for α , if E γ ≥0.5, we consider it from
slab im,t∗ im,n im,t∗ γ i,n
the slab process and estimate it using the same method as θ . Otherwise, it is classified to a
im,t∗
spike process and shrunk to zero (Roˇckov´a and George 2014).
3.5.2. Recovery. In the recovery task, the target data are unobserved at some specific time
points and we aim to recover the missing data, i.e., 1<t∗<n. Define t and t to be the nearest
be af
observed time points before and after t∗ respectively. Denote the nearest observation time to t∗ as
t , i.e.,
near
t = argmin |t−t∗|.
near
t∈{tbe,taf}
As the parameters before and after t are already optimized by the EM algorithm, the estimation
∗
of θ becomes:
im,t∗
(cid:26)
θ , for hard slab prior,
θ = im,tnear
im,t∗ LSE(θ ,θ ), for soft slab prior,
im,tbe im,taf
where LSE(·) represents a least-square estimation for auto-regressive process introduced in
(Choong et al. 2009). We also let α =0, if E γ <0.5. Otherwise, we estimate its value in
im,t∗ γ i,tnear
the same way as θ .
im,t∗Wang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 17
Then, given the parameter α and θ , and the new input point x , the joint distribution
m,t∗ m,t∗ t∗
of y (x ) and observations y can be expressed as:
m t∗ m
(cid:18) (cid:19) (cid:18)(cid:20) (cid:21) (cid:20) (cid:21)(cid:19)
y µ Σ Σ
m ∼N , t∗ , (24)
y (x ) µ ΣT Σ
m t∗ t∗ t∗ t∗,t∗
where µ =KT K−1 y , Σ =K −KT K−1 K , and Σ =K −KT K−1 K .
t∗ (s∗) (ss) (s) t∗ m∗ (sm) (ss) (s∗) t∗,t∗ t∗,t∗ (s∗) (ss) (s∗)
In the above equations, K is the cross-covariance matrix of the sources and the new input x ,
(s∗) t∗
K is the covariance of the target observation and the new point, and K is the variance at
(m∗) t∗,t∗
x . Then, the posterior distribution of y (x ) can be derived as:
t∗ m t∗
(cid:16) (cid:17)
y (x )∼N µ +ΣT Σ−1(y −µ),Σ −ΣT Σ−1Σ . (25)
m t∗ t∗ t∗ m t∗,t∗ t∗ t∗
4. Numerical study
In this section, we verify the effectiveness of the proposed non-stationary MGP with the spike-
and-slab prior (denoted as DMGP-SS) using synthetic data. In Section 4.1, we briefly describe the
general settings for the numerical study and benchmark methods. In Section 4.2, we introduce the
design of simulation cases, where the cross-correlation of the sources and the target are dynamic
and sparse. In Section 4.3, we demonstrate our model’s capability in detecting the underlying
correlation pattern as well as improving target prediction performance on the synthetic data.
4.1. General settings
Similar to the assumption made in the model development section, we generate m sequences con-
sisting of m−1 sources and 1 target, sampled at the same timestamps. The input space is simply
time.ToinvestigatethesourceselectioncapabilityofDMGP-SS,weassignonlym <m−1sources
t
to be correlated with the target at time t. Besides, a source will remain either correlated or uncor-
related continuously for a certain period of time.
For comparison, we consider three benchmarks:
1. GP. The target is modeled using a single-output GP, with a squared-exponential covariance
function.
2. MGP-L1.Itisastate-of-artstaticmethodintroducedin(Wangetal.2022).MGP-L1models
the target and sources in one MGP model, with the same covariance structure as in Eq. (9).
The scaling parameters {α }m−1 are penalized by L term to achieve source selection. The
im i=1 1
regularized log-likelihood of this model is:
m−1
1 1 (cid:88)
logF =− yTK−1y− log|K|−λ |α |−const.
2 2 im
i=1
where K is calculated using the static covariance functions in (Wang et al. 2022) and λ is a
tunning parameter.Wang et al.: Non-stationaryandSparsely-correlatedMGP
18 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
3. DMGP-GP. This is a state-of-art non-stationary MGP model, which constructs an LMC
model for all outputs and assumes the hyper-parameters follow other GPs (Meng et al. 2021).
More details can be found in the Appendix D. In this model, the covariance for the m outputs
is
cov[y(x ),y(x )]=A ATk(x ,x )+diag{σ },
t t′ t t′ t t′ i
where A AT ∈Rm×m is the correlation matrix of m outputs, and diag{σ } is the diagonal
t t′ i
matrix with {σ }m . In this study, we focus on the correlation between the sources and target,
i i=1
which corresponds to the last column of A AT (except the last element (A AT) ), i.e.,
t t t t mm
(A AT) .
t t 0:m−1,m
All methods are implemented in Python 3.8 and executed on a computer with an Inter(R)
Core(TM) i5-7400 CPU with 3.00GHz and 16GB RAM. Both GP and MGP-L1 are implemented
using gpflow (Matthews et al. 2017). DMGP-GP is implemented using TensorFlow and optimized
with ADAM (Kingma and Ba 2015), with a maximum iteration limit of 500. The EM algorithm
for DMGP-SS is also based on Tensorflow, and we use stochastic ADAM with four batches in the
M-step. We set the k and k in Algorithm 1 to 5 and 400 respectively.
out in
For MGP-L1, the weight of L penalty λ is a tuning parameter. For DMGP-GP, we use square
1
exponential functions for k and k . For simplicity, we apply the same tuning parameters for both
α θ
kernels,i.e.,theamplitudeα# andlength-scaleθ#.Thoseparametersaretunedbycross-validation.
In the case of DMGP-SS, the prior sparsity parameter η is set to 0.5. We repeat each case 50 times
and report the prediction performance through averaging the results.
4.2. Simulation cases
Themainobjectiveofthissectionistodemonstratetheeffectivenessofourmethodincapturingthe
non-stationary and sparse cross-correlation between the sources and the target. For simplicity, we
hold the other characteristics constant over time, e.g., the smoothness of each output. Specifically,
we design two simulation cases with different cross-correlation patterns. The first case involves a
piece-wise constant cross-correlation, while the second case has a smoothly-changing correlation.
In each case, the input data {x }130 are evenly spaced in [1,130]. The observed data are generated
t t=1
from sine functions with measurement noise ϵ ∼N(0,0.32).
t
Case 1. In this case, we define four kinds of source functions:
Y (x )=3sin(πx /20+e )+ϵ , Y (x )=2sin(2πx /20+e )exp[0.5(x %40−1)]+ϵ ,
1 t t 1 t 2 t t 2 t t
Y (x )=3sin(4πx /20+e )+ϵ , Y (x )=2sin(5πx /20+e )+ϵ ,
3 t t 3 t 4 t t 4 t
wheree ∼N(0,0.22)isarandomphasetomakethesampledoutputsdifferentfromeachother,and
i
“%” represents the reminder operation. The term exp[0.5(x %40−1)] is used to deviate the shape
tWang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 19
ofY fromthestandardsinefunction.Ineachexperiment,wegenerate4k sourcesthroughsampling
2
each kind of function k times, i.e., m=4k+1. Specifically, the sources {y |0≤j ≤k−1} are
i+4j
sampled from Y . Then, we define the dynamic target output as:
i
f (x )=a sin(πx /20)+a sin(2πx /20)+a sin(4πx /20)+a sin(5πx /20)
m t 1,t t 2,t t 3,t t 4,t t
In this case, we simulate a piece-wise constant cross-correlation by setting:
a =(2+2a )I , a =(2+2a )I +(1+a )I ,
1,t 1 t<40 2,t 2 40≤t<80 2 80≤t≤130
a =(1+a )I , a =0.
3,t 3 80≤t≤130 4,t
Therefore, there are three segments, [0,40), [40,80), and [80,130]. Only the 1st, the 2nd, and the
2ndand3rdsourcesarecorrelatedtothetargetinthethreeperiods,respectively.Theothersources
remain uncorrelated to the target at all times.
Case 2. Compared with Case 1, we only change the coefficients {a }3 into smoothly-changing
i,t i=1
ones in this case. Specifically, we let them vary along sine-cosine trajectories,
a =[(2+a )cos(πt/120)+0.5]I ,
1,t 1 t<40
a =[(2+a )sin(πt/120−π/6)+0.5]I ,
2,t 2 40≤t<130
a =[(2+a )sin(πt/120−π/2)+0.5]I .
3,t 3 80≤t<130
In all cases, we set k=1,4 to generate four and sixteen source outputs for each case. In order
to compare the prediction performance of different methods, we randomly remove three short
sequences of length 10 from [10,30], [50,70] and [90,110] respectively. These 30 data points are
treated as missing data, while the others are used as training data.
4.3. Simulation results
To begin with, we demonstrate that the proposed DMGP-SS is capable of capturing the dynamic
and sparse correlation between the sources and the target. Fig. 6 illustrates the estimated {α }4
im i=1
for MGP-L1, the {α }4 for DMGP-SS, and the estimated (A AT) for DMGP-GP in Case
im,t i=1 t t 1:4,m
1 and 2 with four sources. And Fig. 3 visualizes the sources and target prediction results in Case
1 with four sources. DMGP-SS is implemented with the hard and soft slab priors for Case 1 and 2
respectively. Note that the value of a and α are not identical, since a is a linear combination
i,t im,t i,t
weight rather than the correlation parameter in MGP.
Overall, DMGP-SS successfully recovers the true dynamic structural sparsity of correlation
shown in the first column of Fig. 6. Firstly, DMGP-SS tracks closely the periods of correlation
between each source and the target, achieving a dynamic selection of sources. Since the target’sWang et al.: Non-stationaryandSparsely-correlatedMGP
20 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
Figure2 Dynamic correlation detection results with four sources: (a) Case 1, (b) Case 2. The first column is
the true a , the second and fourth columns are the estimated α for MGP-L1 and DMGP-SS respectively, and
i,t m
the third column shows the estimated (A AT) for DMGP-GP.
t t 0:m−1,m
True MGP-L1 DMGP-GP DMGP-SS
4 a1,t 4 1m 4 (AtAtT)1m 4 1m,t
2 2 2 2
0 0 0 0
4 a2,t 4 2m 4 (AtAtT)2m 4 2m,t
2 2 2 2
0 0 0 0
4 a3,t 4 3m 4 (AtAtT)3m 4 3m,t
2 2 2 2
0 0 0 0
4 a4,t 4 4m 4 (AtAtT)4m 4 4m,t
2 2 2 2
0 0 0 0
0 40 80 130 0 40 80 130 0 40 80 130 0 40 80 130
t t t t
(a)
True MGP-L1 DMGP-GP DMGP-SS
4 a1,t 4 1m 4 (AtAtT)1m 4 1m,t
2 2 2 2
0 0 0 0
4 a2,t 4 2m 4 (AtAtT)2m 4 2m,t
2 2 2 2
0 0 0 0
4 a3,t 4 3m 4 (AtAtT)3m 4 3m,t
2 2 2 2
0 0 0 0
4 a4,t 4 4m 4 (AtAtT)4m 4 4m,t
2 2 2 2
0 0 0 0
0 40 80 130 0 40 80 130 0 40 80 130 0 40 80 130
t t t t
(b)
characteristics do not change abruptly (as shown in Fig. 3), it is reasonable that the estimated
correlation change points are about ten time-steps before or after the designed change times. Sec-
ond, the hard slab prior encourages nearly piece-wise correlation, while the soft slab prior allows
smoothlychangingcorrelation.DuetotheappropriateselectionofsourcesatdifferenttimesinFig.
3, the proposed DMGP-SS achieves precise prediction with the lowest prediction uncertainty. This
highly improves the confidence of decision making when using the recovered series. The difference
on confidence interval of three MGP models is due to that the uncorrelated sources ’poison’ the
correlation structure and decrease the influence of the truly-correlated sources, resulting in a lower
value of variance reduction term ΣT Σ−1Σ in posterior prediction Eq. (25).
t,∗ t,∗
Incontrast,anothernon-stationarymethodDMGP-GPfailstoestimateasparsestructureinthe
source-targetcorrelationsincetheGPprioronparameterslackstheshrinkageeffect.Theproposed
DMGP-SSaddressesthisproblemthroughcombiningthesmoothslabpriorandtheshrinkingspikeWang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 21
Figure3 Visualization of prediction results in Case 1 (k=1). The shaded region represents the 99% confidence
interval.
prediction observed data missing data
5
0
5
source 1 source 2 source 3 source 4
5
0
5
0 40 80 130 0 40 80 130 0 40 80 130 0 40 80 130
t t t t
GP MGP-L1 DMGP-GP DMGP-SS
Figure4 The estimated E γ for DMGP-SS in Case 1.
γ 1:4
1.0 1.0 1.0 1.0
0.5 0.5 0.5 0.5
0.0 0.0 0.0 0.0
0 40 80 130 0 40 80 130 0 40 80 130 0 40 80 130
t t t t
prior. Regarding MGP-L1, although it can estimate a sparse structure of α , the estimated values
m
of non-zero parameters are constant over time and cannot reflect the change of correlation. As a
result, it even performs worse than GP in recovering the target output in Fig. 3.
Another advantage of DMGP-SS is the adaptive adjustment of the spike-and-slab combination
weight, E γ . In our settings, ν−1 in the spike prior is much larger than ν−1 in the hard slab prior
γ i,t 0 1
(cid:112)
(or ν−1 in the soft slab prior), to put more penalty on the correlation sparsity. For example, we
1
set ν =0.02 and ν =0.1 in Case 1. However, this sparse penalty does not cause significant bias on
0 1
non-zero α because of the automatically adjusted E γ in the EM algorithm. Specifically, we
im,t γ i,t
initialize it with 0.99 to barely shrink parameters at the beginning. Then E γ is updated in the
γ i,t
E-stepoftheEMalgorithm.Fig.4showsitsestimatedvalueafterfiveiterations.Forthecorrelated
sources (e.g., the first source during t∈[0,50]), their corresponding E γ is very close to 1, so they
γ i,t
bear negligible shrinkage effect from the spike prior. In contrast, for the uncorrelated sources (e.g.,
the first source after t=50), E γ is approximately 0.2, implying a substantial shrinkage effect.
γ i,t
The value 0.2 can be derived based on Eq. (21). For consecutive zero elements, p =η(2ν )−1,
slab 1
and p =(1−η)(2ν )−1, resulting in E γ ≈ν−1/ν−1.
spike 0 γ i,t 1 0
TABLE 1 summarizes the results of 40 repetitions for each case. DMGP-SS outperforms all the
other methods in both cases. And the increasing of source number does not affect the prediction
accuracy,demonstratingitsremarkablerobustnessindealingwithmoderatedata.MGP-L1exhibits
slightly higher prediction accuracy than GP. Because the static covariance structure limits its
ability to transfer accurate information at all times. Under some circumstances, this limitation
1y
my
1
E E
2
2y
my
3y
my
3
E
4y
my
E
4Wang et al.: Non-stationaryandSparsely-correlatedMGP
22 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
Table1 Prediction error for Case 1 and 2. Values in the brackets are standard deviations.
Case 1 Case 2
outputs
GP MGP-L1 DMGP-GP DMGP-SS GP MGP-L1 DMGP-GP DMGP-SS
0.99 0.89 0.76 0.56 1.12 1.04 0.85 0.66
5
(0.26) (0.21) (0.23) (0.13) (0.27) (0.24) (0.26) (0.17)
0.95 0.91 0.75 0.50 1.15 1.02 0.78 0.64
17
(0.25) (0.21) (0.19) (0.15) (0.26) (0.21) (0.17) (0.16)
will cause a negative transfer effect on the learning of target (for example, the result shown in
Fig. 3). DMGP-GP has a better performance on target prediction than GP and MGP-L1, due
to the ability to model dynamic correlation. However, it does not achieve the same prediction
accuracy as DMGP-SS in these cases. On the one hand, it lacks the ability to exclude the impact
of uncorrelated sources. On the other hand, DMGP-GP is an extension of LMC and uses the same
function k(x ,x ) to model the auto-covariance of every output. This feature makes it unsuitable
t t′
for our cases where the source covariance functions have four kinds of length-scales. Nevertheless,
the proposed DMGP-SS models each source with separate kernels and latent functions, highly
increasing its flexibility.
TABLE 2 lists the computational time of the four methods in Case 1. Between the two non-
stationarymethods,DMGP-SSrequiresmuchlesscomputationtimethanDMGP-GP.Thisexactly
verifies the analysis in 3.3 that our model can save a large mount of time than the traditional non-
stationary MGP method, due to a block-sparse covariance matrix. In fact, our model is scalable
for larger size of data, which is described in Appendix E.
Table2 Computational time for the different methods in Case 1.
outputs n GP MGP-L1 DMGP-GP DMGP-SS
5 130 0.13 8.6 130 73
17 130 0.10 26 2100 300
5. Case study
In this section, we apply DMGP-SS to two cases: human movement signal modeling and control
policy optimization. In the first case, these signals are recorded by sensors attached to different
joints, such as hands and feet. As the movement of joints are dynamically correlated across differ-
ent gestures (Xu et al. 2022), it is reasonable to utilize a non-stationary method to capture the
varying correlation and leverage information among the signals of joints. Regarding the control
policy iteration, we study on a classical reinforcement learning problem, mountain-car. We evalu-
ate the performance of DMGP-SS on leveraging knowledge between difference systems when the
environment is non-stationary.Wang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 23
5.1. Movement Signal Modeling
5.1.1. Data Description We use the MSRC-12 gesture dataset (Fothergill et al. 2012) con-
sisting of sequences of human skeletal body movement. Twenty sensors are distributed across the
body, and each sensor can record three-dimensional coordinates of joint positions. The motion
signal has a sampling frequency of 30 Hz and an accuracy of ±2cm. The dataset comprises 12
different gestures performed by 30 participants. The experiment involves each participant starting
from a standing position, performing one gesture, and returning to the standing state. This process
repeats several times continuously within each sample.
TodemonstratetheeffectivenessofDMGP-SS,weconnecttheinstancesofthreegestures(“Gog-
gles”, “Shoot”, and “Throw”) performed by the same individual. Fig. 6a shows the snapshots of
the standing position and the selected gestures. In the first two gestures, the participant stretches
both arms in front of him to perform searching or shooting motions. In the third gesture, the
participant only uses his left arm to make an overarm throwing movement. In these gestures, the
main acting joints are hands, wrists, and elbows, where the trajectories of hands and wrists are
almost identical. Therefore, we select the movement signals of four joints (left wrist, left elbow,
right wrist and right elbow) as twelve outputs. We choose the z-coordinate movement of the left
elbow as a target output, while using the remaining eleven movement signals as sources.
We select two 120-frame-long instances for each gesture and down-sample each instance to 30
frames. Therefore, there are 180 points for each output. To eliminate the difference in initial
coordinatevalues,weresettheinitialthree-dimensionalcoordinatevalueto(0,0,2)acrossdifferent
recordings. Additionally, we re-scale all outputs to [−2,2]. Fig. 6b displays the 12 outputs and the
change of joints’ positions.
5.1.2. Results Intuitively, the cross-correlation between the source and target signals should
remain constant for a single gesture, so a hard slab prior is used in DMGP-SS. All other settings
forthiscaseareidenticaltothoseusedinthesimulationstudies.Westillsimulateconsecutivedata
missing for the target. From the 60-points-long time-series of each gesture, we randomly remove a
sequence of length ten as missing data.
First of all, Fig. 7a shows the estimated correlation between the sources and the target. MGP-L1
selects six sources (the third ’R-W-z’, the forth ’R-E-x’, the sixth ’R-E-z’, the seventh ’L-W-x’,
the ninth ’L-W-z’, and the eleventh ’L-E-y’) as the correlated signals for the whole time period,
in which the ninth source has the strongest correlation. DMGP-SS selects the sixth source (‘R-
E-z’) and the ninth source (‘L-W-z’) as correlated sources when 0≤t≤120 and 120≤t≤180,
respectively. DMGP-GP does not provide a sparse estimation of cross-correlation. Among them,
theproposedDMGP-SSaccuratelycapturestheunderlyingphysicalmovementsofeachgesture.InWang et al.: Non-stationaryandSparsely-correlatedMGP
24 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
Figure5 (a). The snapshots of “Stand”, “Google”, “Shoot” and “Throw”. The twenty joints are represented
by circles. We mark out the four selected joints for our study, where “L” and “R” represent “left-side” and
“right-side” respectively, and “E” and “W” represent the elbow and wrist joints respectively. (b). The twelve
movement signals of the selected joints in the three gestures’ data, where the red signal is takes as the target
signal and the others are the source signals. In the label of vertical axis, “x”, “y” and “z” represent different
coordinates. Each signal has a vertical range of [−2,2], with the horizontal ticks representing time indexes.
1 R-W-x
2 R-W-y
R-E 3 R-W-z
L-E
R-W
4 R-E-x
L-W
5 R-E-y
6 R-E-z
7 L-W-x
Stand Shoot
8 L-W-y
9 L-W-z
10 L-E-x
11 L-E-y
12 L-E-z
0 60 120 180
t
Google (a) Throw (b)
the “Google” and “Shoot” gestures, both elbows have almost the same trajectory. In the “throw”
gesture, the left elbow’s movement is highly correlated with that of the left wrist. These findings
align well with the signals shown in Fig. 6b. On the contrary, limited by the static correlation
structure, MGP-L1 can only select some sources and force them to be correlated with the target
all the time, but such signals do not exist in the dynamic environment. For DMGP-GP, the results
cannot provide us an intuitive understanding on the joints’ relationship.
Fig. 7b displays the recovered target signal in one experiment. Notably, DMGP-SS accurately
recovers the missing data with high precision. Besides, it has the minimal uncertainty because
it can selects the most correlated sources at each time and such a high correlation improves the
confidence on prediction results. Conversely, the predictions of MGP-L1 and DMGP-GP display
an obvious bias in the first gap and higher uncertainty for all gaps. Besides, similar to the results
of numerical studies, the proposed DMGP-SS also gives predictions with the lowest uncertainty,
which significantly improves the confidence of decision making.
We further repeat the experiments 36 times. Table 3 compares the prediction accuracy of four
methods in terms of both the MAE and the continuous-ranked-probability-score (CRPS). The
CRPS measure is a widely used metric to evaluate the probabilistic forecast for a real-valued
outcome (Hersbach 2000):
CRPS=n−1
n (cid:88)test(cid:90)
(cid:2)
Φ(yˆ)−1
(cid:3)2
dyˆ
test i yˆi≥yi) i
i=1Wang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 25
Figure6 (a). Correlation estimation results for the data of three gestures. The first and third row are the
estimated α for MGP-L1 and DMGP-SS respectively, and the second row shows the estimated {A AT}
m t t 1:11,12
for DMGP-SS. (b). Recovery results of four methods for the movement signal of right wrist. The shaded region
represents the 99% confidence interval.
1 prediction observed data missing data
0.75
6 0.50
0.25 2
11 0.00 0
MGP-L1 -2
GP
1 2
2.5 0
6 0.0
-2
11 2.5 MGP-L1
DMGP-GP 2
0
1 1 -2 DMGP-GP
6 2
11 0 0
0 60 120 180 -2
0 60 120 180
t
t
DMGP-SS DMGP-SS
(a) (b)
Table3 Prediction error for real case. The values in the bracket are standard deviations.
GP MGP-L1 DMGP-GP DMGP-SS
MAE-mean 0.43 0.22 0.50 0.18
MAE-std. (0.12) (0.10) (0.25) (0.08)
CRPS-mean 0.30 0.16 0.41 0.15
where yˆ is the predicted output and Φ is the cumulative density function (CDF) of the predicted
i
distribution. A low CRPS value suggests that the predicted posterior are concentrated around
the true value, indicating a high probabilistic forecast performance. As expected, DMGP-SS out-
performs the other methods due to its ability to capture the dynamic and sparse correlation
accurately. MGP-L1 performs better than GP, benefiting from the information borrowed from
the other sources. However, it cannot model a dynamic correlation, resulting in lower prediction
accuracy than DMGP-SS. Regarding DMGP-GP, although it captures the change of correlations
betweenthetargetandsomesources,itspredictionaccuracyisevenlowerthanGP.Thisresultmay
be attributed to that non-sparse correlations lead to potential negative transfer effects. Besides,
since the sources’ smoothness is heterogeneous, it is inappropriate to use the same auto-covariance
function to model all outputs.
5.2. Control Policy Optimization
In reinforcement learning problems, there are five important elements for the agent: state s, action
a, reward r, value V and policy W. Starting from one state s, the agent takes an action a, transi-
tions into a new state u and gets an immediate reward r(u) from the environment. In general, a
reinforcement learning framework includes two primary procedures:
my
my
my
myWang et al.: Non-stationaryandSparsely-correlatedMGP
26 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
Figure7 The illustration of mountain-car problem.
Goal
Position
Height
1. Model estimation: estimating the state transition function U(s,a) based on the observed
transition samples [(s,a),u].
2. Policy iteration: iteratively estimating the state value V(s) (the long-term expected reward)
for each state and improving the control policy W(s).
MoredetailscouldbefoundintheworksonGP-basedreinforcementlearning(KussandRasmussen
2003, Verstraeten et al. 2020).
In this study, we employ the well-known reinforcement learning problem, mountain-car, to
demonstrate the application of our model in decision making. Fig. 7 illustrates such a problem. A
car begins in a valley and aims to reach a goal position of the right-most hill. Due to the steep
slopeofthehill,thecarcannotdirectlyacceleratetothegoalposition.Instead,itneedstodriveup
the opposite side, turn back, and accelerate to reach the goal position. In system, the agent state
is described by the position spos ∈[−1.2,1.0] and the velocity svel ∈[−0.07,0.07] of the car, i.e.,
s=(spos,svel). The agent action is a horizontal force a∈[−1,1]. The car starts at the state s =
init
(svel,svel) with spos∈[−0.6,−0.5] and svel =0, aiming to reach the goal state s =(0.45,0). The
init init init init goal
reward function is the probability density function of N(s ,diag{0.052,0.00352}). The dynamic
goal
equation of this system is approximated by:
svel=svel +P ·a −G·cos(3·spos)
t t−1 t−1 t−1
spos=spos +svel (26)
t t−1 t
where P is the horizon power unit and G is the vertical force unit.
We consider the control problem involving a car in a non-stationary environment with lim-
ited transition samples. During t∈[0,20), the target car runs in an environment with (P,G)=
(0.009,0.0015)andweget20randomsamples.However,att=20,anunknownchangeoccurs,alter-
ing the environment factors to (P,G)=(0.0011,0.0026). We sampled another 20 random samples
during t∈[20,40). After t=40, we start to control the car to reach the goal position and the envi-
ronment does not change any more. Given that those samples are too limited to build an accurateWang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 27
a transition model, we transfer information from two historical source datasets, each consisting of
200samplesfromstationaryenvironmentswith(P,G)=(0.01,0.0015)and(P,G)=(0.001,0.0025),
respectively. We can see that the target environment is close to the first source environment before
t=20, and is close to the second one after that.
Algorithm2summarizestheworkflowofreinforcementlearning,whereweemploytheDMGP-SS
as a transition model. For simplicity, we model the source transition data with stationary GPs in
DMGP-SS. The sources and the target are expressed as:
u (s,a)=α g (s,a)∗z (s,a)+ϵ ,i∈IS,
i ii ii i i
(cid:88)
u (s,a)= α g (s,a)∗z (s,a)+ϵ . (27)
m jm,t jm,t j m
j∈I
Here, u is the position or velocity state to where the agent transitioned from a state s after
taking an action a. After fitting this model both the target and source samples, we train a GP for
the value model V(s) with even-spaced support points S and rewards r(S ). Based on the
supp supp
transition model U(s,a), we iteratively improve the policy W(s) and value model V(s) until the
prediction of V(s) converges. Details on the policy improvement procedure can be found in (Kuss
and Rasmussen 2003, Verstraeten et al. 2020). While we focus on the offline setting in this case,
it is worth noting that this framework can be readily extended to accommodate an online setting,
wherein the training sample consists of the visited states and the transition models are updated
every few steps.
Algorithm 2 Control Policy Optimization for Mountain Car Case
Input: Source transition samples {(s ,a ),u }2,200, target transition samples
i,n i,n i,n i,n=1
{(s ,a ),u }40 , reward function {r(s)}, 256 support points S .
3,t 3,t 3,t t=1 supp
1: Initialize: policy W(s)← random policy, value V =r(S ).
supp supp
2: Fit two DMGP-SS models for the state transition using both the source and the target tran-
sition samples, one for the position state and the other for the velocity state.
3: Fit a GP for the state value model V(s) using {S ,V }.
supp supp
4: Improve policy W(s) and value model V(s) iteratively based on the state-transition model
until V(S ) converges.
supp
5: Execute the optimized policy starting at the state s :
init
We choose two reinforcement learning benchmark methods (Kuss and Rasmussen 2003, Ver-
straeten et al. 2020) with the stationary GP and MGP as the state transition model, respectively.
The maximum execution steps are 600 for each method. In TABLE 4, we report the the mean ofWang et al.: Non-stationaryandSparsely-correlatedMGP
28 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
Table4 Predictive accuracy on state transition and the mean of absolute distances to the goal state in the
mountain-car case.
Metric RL-GP RL-MGP RL-DMGP-SS
The mean of absolute distances to
0.98 0.83 0.27
the goal position
Predictive MAE on velocity
3.8 5.8 0.51
transition (10−3)
Predictive MAE on position
3.6 4.9 8.2
transition (10−3)
Figure8 The control path of the three methods: (a) RL-GP (b) RL-MGP, and (c) RL-DMGP-SS. The blue
points mark the positions every 50 steps.
600 600 600
500 500 500
400 400 400
300 300 300
200 200 200
100 100 100
0 0 0
1.20 0.50 0.00 0.45 1.00 1.20 0.50 0.00 0.45 1.00 1.20 0.50 0.00 0.45 1.00
Position Position Position
(a) RL-GP (b) RL-MGP (c) RL-DMGP-SS
absolute distances to the goal state for three methods. The DMGP-SS-based control policy has the
shortest average distance to the goal state in 600 steps. Specifically, Fig. 8 compares the position
of the car controlled by the three policies. With DMGP-SS as the transition model, the car reaches
the goal position and stays there after about 250 time steps. However, the other methods cannot
find a good policy to reach the goal position within 600 moves, since their stationary transition
models cannot account for the environment change and make the policy iteration hard to converge.
In TABLE 4, we further compare the predictive MAE on state transition for three methods. The
proposed method has a significantly lower prediction error on velocity state transition than the
the other methods, since it can capture the change of environment and transfer information from
the related sources. Fig. 9 illustrates the estimated α from DMGP-SS trained on the velocity
m
transition data. We can find that the proposed method successfully finds that the correlation
between the sources and the target changes at t=20. Therefore, during the policy improvement
stage, it can leverage information from the similar source (the second one) and avoid the negative
transfer from the uncorrelated source (the first one). Regarding the prediction error on position
transition, although RL-DMGP-SS has a higher MAE than the other benchmarks, the difference
is minor considering the position range [−1.2,0.6]. Therefore, our method can provide the best
control policy.
spetS spetS spetSWang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 29
Figure9 The estimated correlation parameter α from DMGP-SS on velocity transition.
m
3
1m,t 2
1
2m,t
0 10 20 30
t
6. Conclusion
This paper proposes a flexible non-stationary multi-output Gaussian process for modeling multi-
variate data in transfer learning. The novelty of our approach lies in its ability to capture dynamic
and sparse cross-correlations between sources and targets. We achieve this by allowing correlation
parameters to follow a spike-and-slab prior, where the slab prior ensures correlation variation over
time, and the spike prior encourages parameters to shrink to zero, eliminating negative transfer
effectsfromuncorrelatedsources.Theratioofthesetwopriorsisautomaticallyadjustedinthepro-
posed EM algorithm, preventing the shrinkage effect on non-zero correlation parameters. Through
the experiments on simulation and human gesture dataset, we demonstrate that our method is
well-suited for both capturing non-stationary characteristics and mitigating negative transfer.
The proposed data-driven method provides a powerful tool for researchers and engineers to
select the most informative sources to transfer knowledge. Except high-dimensional time-series
modeling, our approach could also find applications in both sequential decision making and change
point detection. For instance, transfer learning has arisen to handle the critical challenge of sparse
feedbacks in reinforcement learning (RL), a popular framework for solving sequential decision
makingproblems.However,negativetransferisstillanotablechallengeformulti-taskreinforcement
learning and it is risky to naively share information across all tasks (Zhu et al. 2023, Yang et al.
2020). Therefore, we embedded our model with the offline RL transfer framework to automatically
select correlated sources to share knowledge in a non-stationary environment (Padakandla et al.
2020). In the future, we can further develop our method to account for an online RL task. Besides
that, the estimated dynamic correlation among outputs can help us to understand the structure
of time-series even with some missing data, as well as to detect some structural change points for
subsequent decision making.
Many extensions are possible for our model. Although the proposed methodology is flexible
enough to capture complex and dynamic correlation, scaling it up to large datasets is computa-
tionally challenging. Possible solutions include utilizing a sparse approximation for the covariance
matrixordevelopingamoreefficientoptimizingalgorithm.Inaddition,theproposedEMoptimiza-
tion algorithm solely provides estimated values of the model parameters without incorporating any
uncertainty measurement. To address this issue, we can use variational inference to approximate
thetrueposteriordistributionofparameters,therebycapturingtheinherentuncertaintyassociatedWang et al.: Non-stationaryandSparsely-correlatedMGP
30 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
with these parameters. The approximated uncertainty would be propagated into the prediction at
new points.Wang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 31
Appendix A: DMGP-SS with missing data
In general, observation may be missing at some time points within the range of 1≤t≤n. Under such a
circumstance, the observation number for each output is n ≤n. To account for the missing data, we re-
i
denote the data for the ith output as X =(x ,...,x ,...,x )T and y =(y ,...,y ,..,y )T, where
i i,t1 i,ti i,tni i i,t1 i,ti i,tni
t represents the observation time index.
i
Compared with the model presented in the main text, the general DMGP-SS requires only adjustments
on the parameter priors. The general model can be expressed as follows,
y ,y |Φ ,Φ ∼N(0,K)
(s) m (s) m
α |α ∼p (α |α ),
ii,ti ii,ti−1 slab ii,ti ii,ti−1
θ |θ ∼p (θ |θ ),
ii,ti ii,ti−1 slab ii,ti ii,ti−1
α |γ ,α ∼(1−γ )p (α )
im,ti i,ti im,ti−1 i,ti spike im,ti
+γ p (α |α ),
i,ti slab im,ti im,ti−1
γ |η∼Bern(η),
i,ti
θ |θ ∼p (θ |θ ). (28)
im,ti im,ti−1 slab im,ti im,ti−1
The spike prior is the same as that in the main text. The slab priors are re-defined as,
1 (cid:18) |α −α |(cid:19)
phard(α |α )= exp − im,ti im,ti−1 , (29)
slab im,ti im,ti−1 2ν ν
1 1
1−ρ (cid:18) (α −ρti−ti−1α )2(1−ρ2)(cid:19)
psoft(α |α )= √ exp − im,ti im,ti−1 , (30)
slab im,ti im,ti−1 (1−ρti−ti−1) 2πν
1
2ν 1(1−ρ2(ti−ti−1))
where the soft prior is derived based on the property of an auto-regressive process.
Appendix B: Proof of Proposition 1
The proposed non-stationary MGP covariance matrix Eq. (9) is positive-definite, i.e., ∀y̸=0,
yTKy>0.
Proof of Proposition 1 Recall that the covariance functions are generated by the convolution of kernel
functions:
(cid:90)
covf(x ,x )=α α g (x −u)g (x −u)du
ii t t′ ii,t ii,t′ ii,t t ii,t′ t′
(cid:90)
covf (x ,x )=α α g (x −u)g (x −u)du
im t t′ ii,t im,t′ ii,t t im,t′ t′
m (cid:90)
(cid:88)
covf (x ,x )= α α g (x −u)g (x −u)du
mm t t′ jm,t jm,t′ jm,t t jm,t′ t′
j=1
Decompose this quadratic form as follows,
(cid:88) (cid:88) (cid:88)(cid:88)
yTKy= y y [covf (x ,x )+σ2I ]
i,t j,t′ i,j t t′ i i=j,t=t′
1≤i≤m1≤j≤m t t′
(cid:88) (cid:88)(cid:88) (cid:88) (cid:88)(cid:88)
= y y covf(x ,x )+2 y y covf (x ,x )
i,t i,t′ ii t t′ i,t m,t′ im t t′
1≤i≤m−1 t t′ 1≤i≤m−1 t t′Wang et al.: Non-stationaryandSparsely-correlatedMGP
32 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
(cid:88)(cid:88) (cid:88)(cid:88)
+ y y covf (x ,x )+ y2 σ2
m,t m,t′ mm t t′ i,t i
t t′ i t
(cid:40)
(cid:90)
(cid:88) (cid:88)(cid:88)
= y y α α g (x −u)g (x −u)du
i,t i,t′ ii,t ii,t′ ii,t t ii,t′ t′
1≤i≤m−1 t t′
(cid:90)
(cid:88)(cid:88)
+ y y α α g (x −u)g (x −u)du
m,t m,t′ im,t im,t′ im,t t im,t′ t′
t t′
(cid:41)
(cid:90)
(cid:88)(cid:88)
+2 y y α α g (x −u)g (x −u)du
i,t m,t′ ii,t im,t′ ii,t t im,t′ t′
t t′
(cid:90)
(cid:88)(cid:88) (cid:88)(cid:88)
+ y y α α g (x −u)g (x −u)du+ y2 σ2
m,t m,t′ mm,t mm,t′ mm,t t mm,t′ t′ i,t i
t t′ i t
(cid:40) (cid:34)
(cid:90)
(cid:88) (cid:88) (cid:88)
= y α g (x −u) y α g (x −u)
i,t ii,t ii,t t i,t′ ii,t′ ii,t′ t′
1≤i≤m−1 t t′
(cid:88) (cid:88)
+ y α g (x −u) y α g (x −u)
m,t im,t im,t t m,t′ im,t′ im,t′ t′
t t′
(cid:35) (cid:41)
(cid:88) (cid:88)
+2 y α g (x −u) y α g (x −u) du
i,t ii,t ii,t t m,t′ im,t′ im,t′ t′
t t′
(cid:90)
(cid:88) (cid:88) (cid:88)(cid:88)
+ y α g (x −u) y α g (x −u)du+ y2 σ2
m,t mm,t mm,t t m,t′ mm,t′ mm,t′ t′ i,t i
t t′ i t
 
(cid:88) (cid:90)
(cid:34)
(cid:88) (cid:88)
(cid:35)2

= y α g (x −u)+ y α g (x −u) du
i,t ii,t ii,t t m,t′ im,t′ im,t′ t′
 
1≤i≤m−1 t t′
(cid:90)
(cid:34) (cid:35)2
(cid:88) (cid:88)(cid:88)
+ y α g (x −u) du+ y2 σ2>0 (31)
m,t mm,t mm,t t i,t i
t i t
Proof completes.
Appendix C: Derivation of the objective function in M-step
Based on Bayes theorem, the parameter posterior can be expressed as:
p(Φ,γ|y)∝p(y|Φ)p(Φ|γ)p(γ).
And based on the theory of multivariate Gaussian distribution, we have:
p(y|Φ)=p(y ,y |Φ ,Φ )
(s) m (s) m
(cid:18)(cid:20)
y
(cid:21)(cid:12)(cid:20) 0(cid:21) (cid:20)
K K
(cid:21)(cid:19)
=N (s) (cid:12) , (ss) (sm)
y (cid:12) 0 KT K
m (sm) mm
(cid:0) (cid:1)
=N y |0,K N(y |µ,Σ), (32)
(s) (ss) m
where µ = KT K−1 y is the conditional mean of target given the sources and Σ = K −
(sm) (ss) (s) mm
KT K−1 K is the conditional covariance.
(sm) (ss) (sm)
Therefore, the objection function can be derived as:
E {logp(Φ,γ|y)}
γ
=E {logp(y|Φ,γ)p(Φ,γ)}+const.
γ
=logp(y |Φ )+logp(y |Φ ,y ,Φ )
(s) (s) m m (s) (s)Wang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 33
+logp(Φ )+E {logp(Φ |γ)+logp(γ)}+const.
(s) γ m
=logp(y |Φ )+logp(y |Φ ,y ,Φ )
(s) (s) m m (s) (s)
+logp(θ )+logp(α )+logp(θ )+E {logp(α |γ)}+const.
(s) (s) m γ m
1(cid:110) (cid:111)
=− yT K−1 y +log|K |+(y −µ)TΣ−1(y −µ)+log|Σ|
2 (s) (ss) (s) (ss) m m
m (cid:88)−1 (cid:88)n (cid:104) (cid:105)
+ logp (θ |θ )+logp (α |α )+logp (θ |θ )
slab ii,t ii,t−1 slab ii,t ii,t−1 slab im,t im,t−1
i=1 t=2
(cid:88)m (cid:88)n (cid:104) (cid:105)
+ (1−E γ )logp (α )+E γ logp (α |α ) +const.. (33)
γ i,t spike im,t γ i,t slab im,t im,t−1
i=1 t=2
Appendix D: Details of DMGP-GP
DMGP-GP is a state-of-art non-stationary MGP model, which constructs a LMC model for all outputs
and assumes the hyper-parameters follow other GPs Meng et al. (2021):
y(x )=A q(x )+ϵ
t t t
log(A )∼GP(0,k (t,t′))
ii,t α
A ∼GP(0,k (t,t′)),i̸=j
ij,t α
q (x )∼GP(0,k(x ,x )),
i t t t′
(cid:115)
2θ θ
(cid:20)
(x −x
)2(cid:21)
k(x ,x )= t t′ exp t t′
t t′ θ2+θ2 2(θ2+θ2)
t t′ t t′
log(θ )∼GP(0,k (t,t′)) (34)
t θ
where y(x )=[y (x ),...,y (x )]T are m outputs, A ∈Rm×m is the time-varying coefficient matrix, q(x )=
t 1 t m t t t
[q (x ),...,q (x )]T are m i.i.d. latent Gaussian processes with zero mean and the same covariance function
1 t m t
k(x ,x ), and ϵ=(ϵ ,...ϵ ) is measurement noise with ϵ ∼N(0,σ2). The covariance for the m outputs is
t t′ 1 m i i
cov[y(x ),y(x )]=A ATk(x ,x )+diag{σ },
t t′ t t′ t t′ i
where A AT ∈Rm×m is the correlation matrix of m outputs, and diag{σ } is the diagonal matrix with
t t′ i
elements {σ }m .
i i=1
Appendix E: Scalability of DMGP-SS
Since our model supposes that the latent processes z (x) are independent on each other, the computational
i
complexity is O(mn3) when all the m outputs have an equal length of n. In comparison, the computational
complexity of the classical MGP is O(m3n3), much larger than that of ours. Therefore, the proposed model
can handle a number of outputs much easier. For example, we test our method on one numerical case with
uptomn=8580points.TABLE5showthepredictionerrorandmodelfittingtime.Thepredictionaccuracy
of the proposed method is better than that of GP in all the three experiments. Besides, although the third
experimentshavetwotimesasmanypointsasthefirstone,thefittingtimeofthethirdone(m=33,n=260)
is only two times that of the first one (m=17,n=260), which identifies that the computational complexity
of our method is O(mn3). Besides, the fitting time of the second experiment (m=17,n=520) is only fourWang et al.: Non-stationaryandSparsely-correlatedMGP
34 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
times that of the first one (m=17,n=260), which means the second experiment takes less gradient descent
steps to converge than the first one does.
Table5 Prediction error and fitting time of DMGP-SS with up to 8580 points.
m=17, n=260 m=17, n=520 m=33, n=260
GP MAE 0.803 0.903 0.781
MAE 0.601 0.432 0.606
DMGP-SS
Time (seconds) (508) (1938) (1273)
Acknowledgments
This work was supported by NSFC under Grants NSFC-72171003, NSFC-71932006.
References
AlBahar A, Kim I, Wang X, Yue X (2022) Physics-constrained bayesian optimization for optimal actuators
placementincompositestructuresassembly.IEEE Transactions on Automation Science and Engineer-
ing .
Alvarez MA, Lawrence ND (2011) Computationally efficient convolved multiple output gaussian processes.
The Journal of Machine Learning Research 12:1459–1500.
BaiY,SafikhaniA,MichailidisG(2022)Hybridmodelingofregionalcovid-19transmissiondynamicsinthe
u.s. IEEE Journal of Selected Topics in Signal Processing 16(2):261–275, URL http://dx.doi.org/
10.1109/JSTSP.2022.3140703.
Boyle P, Frean M (2004) Dependent gaussian processes. Advances in neural information processing systems
17.
ChengS,LuF,PengP(2021)Short-termtrafficforecastingbyminingthenon-stationarityofspatiotemporal
patterns. IEEE Transactions on Intelligent Transportation Systems 22(10):6365–6383, URL http://
dx.doi.org/10.1109/TITS.2020.2991781.
ChoongMK,CharbitM,YanH(2009)Autoregressive-model-basedmissingvalueestimationfordnamicroar-
ray time series data. IEEE Transactions on information technology in biomedicine 13(1):131–137.
Christakos G (2012) Random field models in earth sciences (Courier Corporation).
Damianou A, Lawrence ND (2013) Deep gaussian processes. Artificial intelligence and statistics, 207–215
(PMLR).
Dance H, Paige B (2022) Fast and scalable spike and slab variable selection in high-dimensional gaussian
processes. International Conference on Artificial Intelligence and Statistics, 7976–8002 (PMLR).
Fairchild KD, Lake DE, Kattwinkel J, Moorman JR, Bateman DA, Grieve PG, Isler JR, Sahni R (2017)
Vital signs and their cross-correlation in sepsis and nec: a study of 1,065 very-low-birth-weight infants
in two nicus. Pediatric research 81(2):315–321.Wang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 35
FothergillS,MentisH,KohliP,NowozinS(2012)Instructingpeoplefortraininggesturalinteractivesystems.
Proceedings of the SIGCHI conference on human factors in computing systems, 1737–1746.
Frazier PI (2018) Bayesian optimization. Recent advances in optimization and modeling of contemporary
problems, 255–278 (Informs).
FrickerTE,OakleyJE,UrbanNM(2013)Multivariategaussianprocessemulatorswithnonseparablecovari-
ance structures. Technometrics 55(1):47–56.
GargS,SinghA,RamosF(2012)Learningnon-stationaryspace-timemodelsforenvironmentalmonitoring.
Proceedings of the AAAI Conference on Artificial Intelligence, volume 26, 288–294.
Gelfand AE, Schmidt AM, Banerjee S, Sirmans C (2004) Nonstationary multivariate process modeling
through spatially varying coregionalization. Test 13(2):263–312.
George EI, McCulloch RE (1993) Variable selection via gibbs sampling. Journal of the American Statistical
Association 88(423):881–889.
GoulardM,VoltzM(1992)Linearcoregionalizationmodel:toolsforestimationandchoiceofcross-variogram
matrix. Mathematical Geology 24(3):269–286.
GramacyRB(2020)Surrogates:Gaussianprocessmodeling,design,andoptimizationfortheappliedsciences
(CRC press).
Gramacy RB, Lee HKH (2008) Bayesian treed gaussian process models with an application to computer
modeling. Journal of the American Statistical Association 103(483):1119–1130.
Heaton MJ, Christensen WF, Terres MA (2017) Nonstationary gaussian process models using spatial hier-
archical clustering from finite differences. Technometrics 59(1):93–101.
Heinonen M, Mannerstr¨om H, Rousu J, Kaski S, L¨ahdesm¨aki H (2016) Non-stationary gaussian process
regression with hamiltonian monte carlo. Artificial Intelligence and Statistics, 732–740 (PMLR).
HersbachH(2000)Decompositionofthecontinuousrankedprobabilityscoreforensemblepredictionsystems.
Weather and Forecasting 15(5):559–570.
HuZ,WangC(2021)Nonlinearonlinemultioutputgaussianprocessformultistreamdatainformatics.IEEE
transactions on industrial informatics 18(6):3885–3893.
Huber F, Koop G, Onorante L (2021) Inducing sparsity and shrinkage in time-varying parameter models.
Journal of Business & Economic Statistics 39(3):669–683.
IshwaranH,RaoJS(2005)Spikeandslabvariableselection:frequentistandbayesianstrategies.The Annals
of Statistics 33(2):730–773.
Kalli M, Griffin JE (2014) Time-varying sparsity in dynamic regression models. Journal of Econometrics
178(2):779–793.
Kingma DP, Ba JL (2015) Adam: A method for stochastic optimization. Proceedings of 3rd International
Conference on Learning Representations.Wang et al.: Non-stationaryandSparsely-correlatedMGP
36 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
KoJ,KimH(2022)Deepgaussianprocessmodelsforintegratingmultifidelityexperimentswithnonstation-
ary relationships. IISE Transactions 54(7):686–698.
KontarR,ZhouS,SankavaramC,DuX,ZhangY(2018)Nonparametricmodelingandprognosisofcondition
monitoring signals using multivariate gaussian convolution processes. Technometrics 60(4):484–496.
Kuss M, Rasmussen C (2003) Gaussian processes in reinforcement learning. Advances in neural information
processing systems 16.
Lee C, Wang K, Wu J, Cai W, Yue X (2023) Partitioned active learning for heterogeneous systems. Journal
of Computing and Information Science in Engineering 23(4):041009.
Liu Y, Gong C, Yang L, Chen Y (2020) Dstp-rnn: A dual-stage two-phase attention-based recurrent neu-
ral network for long-term and multivariate time series prediction. Expert Systems with Applications
143:113082.
LiuY,WuH,WangJ,LongM(2022)Non-stationarytransformers:Exploringthestationarityintimeseries
forecasting. Advances in Neural Information Processing Systems 35:9881–9893.
Matthews AGdG, van der Wilk M, Nickson T, Fujii K, Boukouvalas A, Le´on-Villagr´a P, Ghahramani Z,
HensmanJ(2017)GPflow:AGaussianprocesslibraryusingTensorFlow.JournalofMachineLearning
Research 18(40):1–6.
MengR,SoperB,LeeHK,LiuVX,GreeneJD,RayP(2021)Nonstationarymultivariategaussianprocesses
for electronic health records. Journal of Biomedical Informatics 117:103698.
Moore AW (1990) Efficient memory-based learning for robot control. Technical report, University of Cam-
bridge, Computer Laboratory.
PaciorekC,SchervishM(2003)Nonstationarycovariancefunctionsforgaussianprocessregression.Advances
in neural information processing systems 16.
PadakandlaS,KJP,BhatnagarS(2020)Reinforcementlearningalgorithmfornon-stationaryenvironments.
Applied Intelligence 50:3590–3606.
PanSJ,YangQ(2009)Asurveyontransferlearning.IEEETransactions onknowledgeanddataengineering
22(10):1345–1359.
ParkC(2022)Jumpgaussianprocessmodelforestimatingpiecewisecontinuousregressionfunctions.Journal
of Machine Learning Research 23(278):1–37.
Paun I, Husmeier D, Torney CJ (2023) Stochastic variational inference for scalable non-stationary gaussian
process regression. Statistics and Computing 33(2):44.
RangapuramSS,SeegerMW,GasthausJ,StellaL,WangY,JanuschowskiT(2018)Deepstatespacemodels
for time series forecasting. Advances in neural information processing systems 31.
Roˇckov´aV,GeorgeEI(2014)Emvs:Theemapproachtobayesianvariableselection.JournaloftheAmerican
Statistical Association 109(506):828–846.Wang et al.: Non-stationaryandSparsely-correlatedMGP
ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS 37
Rockova V, McAlinn K (2021) Dynamic variable selection with spike-and-slab process priors. Bayesian
Analysis 16(1):233–269.
Rodrigues F, Henrickson K, Pereira FC (2019) Multi-output gaussian processes for crowdsourced traffic
dataimputation.IEEE Transactions on Intelligent Transportation Systems 20(2):594–603,URLhttp:
//dx.doi.org/10.1109/TITS.2018.2817879.
Scheipl F, Fahrmeir L, Kneib T (2012) Spike-and-slab priors for function selection in structured additive
regression models. Journal of the American Statistical Association 107(500):1518–1532.
Shand L, Li B (2017) Modeling nonstationarity in space and time. Biometrics 73(3):759–768.
ShenB,GnanasambandamR,WangR,KongZJ(2023)Multi-taskgaussianprocessupperconfidencebound
for hyperparameter tuning and its application for simulation studies of additive manufacturing. IISE
Transactions 55(5):496–508.
Stathopoulos A, Karlaftis MG (2003) A multivariate state space approach for urban traffic flow modeling
and prediction. Transportation Research Part C: Emerging Technologies 11(2):121–135.
TonJF,FlaxmanS,SejdinovicD,BhattS(2018)Spatialmappingwithgaussianprocessesandnonstationary
fourier features. Spatial statistics 28:59–78.
VerstraetenT,LibinPJ,Now´eA(2020)Fleetcontrolusingcoregionalizedgaussianprocesspolicyiteration.
ECAI, 1571–1578.
Wang K, Hamelijnck O, Damoulas T, Steel M (2020a) Non-separable non-stationary random fields. Inter-
national Conference on Machine Learning, 9887–9897 (PMLR).
Wang S, Cao J, Philip SY (2020b) Deep learning for spatio-temporal data mining: A survey. IEEE transac-
tions on knowledge and data engineering 34(8):3681–3700.
WangX,WangC,SongX,KirbyL,WuJ(2022)Regularizedmulti-outputgaussianconvolutionprocesswith
domainadaptation.IEEETransactionsonPatternAnalysisandMachineIntelligence 45(5):6142–6156.
Wang Y, Zhang J, Zhu H, Long M, Wang J, Yu PS (2019) Memory in memory: A predictive neural network
forlearninghigher-ordernon-stationarityfromspatiotemporaldynamics.ProceedingsoftheIEEE/CVF
conference on computer vision and pattern recognition, 9154–9162.
Wen Q, Zhou T, Zhang C, Chen W, Ma Z, Yan J, Sun L (2023) Transformers in time series: a survey.
Proceedings of the Thirty-Second International Joint Conference on Artificial Intelligence, 6778–6786.
Williams CK, Rasmussen CE (2006) Gaussian processes for machine learning, volume 2 (MIT press Cam-
bridge, MA).
Xia D, Wang B, Li H, Li Y, Zhang Z (2016) A distributed spatial–temporal weighted model on mapreduce
for short-term traffic flow forecasting. Neurocomputing 179:246–263.
Xu R, Wu J, Yue X, Li Y (2022) Online structural change-point detection of high-dimensional streaming
data via dynamic sparse subspace learning. Technometrics 1–14.Wang et al.: Non-stationaryandSparsely-correlatedMGP
38 ArticlesubmittedtoINFORMSJournalonDataScience;manuscriptno. IJDS
YangR,XuH,WuY,WangX(2020)Multi-taskreinforcementlearningwithsoftmodularization.Advances
in Neural Information Processing Systems 33:4767–4777.
Ye R, Dai Q (2021) Implementing transfer learning across different datasets for time series forecasting.
Pattern Recognition 109:107617.
Yoon H, Li J (2018) A novel positive transfer learning approach for telemonitoring of parkinson’s disease.
IEEE Transactions on Automation Science and Engineering 16(1):180–191.
Yun S, Zhang X, Li B (2022) Detection of local differences in spatial characteristics between two spatiotem-
poral random fields. Journal of the American Statistical Association 117(537):291–306.
Zhang C, Yan H, Lee S, Shi J (2021) Dynamic multivariate functional data modeling via sparse subspace
learning. Technometrics 63(3):370–383.
Zhang L, Wang K, Chen N (2016) Monitoring wafers’ geometric quality using an additive gaussian process
model. IIE Transactions 48(1):1–15.
Zhang Y, Yang Q (2021) A survey on multi-task learning. IEEE Transactions on Knowledge and Data
Engineering 34(12):5586–5609.
ZhuC,ByrdRH,LuP,NocedalJ(1997)Algorithm778:L-bfgs-b:Fortransubroutinesforlarge-scalebound-
constrained optimization. ACM Transactions on mathematical software (TOMS) 23(4):550–560.
Zhu Z, Lin K, Jain AK, Zhou J (2023) Transfer learning in deep reinforcement learning: A survey. IEEE
Transactions on Pattern Analysis and Machine Intelligence .
ZouZ,CareemM,DuttaA,ThawdarN(2023)Jointspatio-temporalprecodingforpracticalnon-stationary
wireless channels. IEEE Transactions on Communications 71(4):2396–2409.