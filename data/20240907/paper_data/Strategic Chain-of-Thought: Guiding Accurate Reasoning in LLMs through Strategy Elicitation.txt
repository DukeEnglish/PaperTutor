Strategic Chain-of-Thought: Guiding Accurate Reasoning in LLMs through
Strategy Elicitation
YuWang1,2,ShiwanZhao3,4,ZhihuWang1,HeyuanHuang1,MingFan2,YuboZhang1,
ZhixingWang1,HaijunWang2,TingLiu2
1HuaweiTechnologiesLtd.
2Xi’anJiaotongUniversity
3NankaiUniversity
4ShanghaiJiaoTongUniversity
Abstract Vo�ng-based CoT Enhancement
e.g.Self-Consistency
The Chain-of-Thought (CoT) paradigm has emerged as a
CoT1 Vo�ng
critical approach for enhancing the reasoning capabilities Mul�-Query Language
Input Query CoT 2 Answer
of large language models (LLMs). However, despite their Model …
widespread adoption and success, CoT methods often ex- CoT N Lowefficiency
hibit instability due to their inability to consistently ensure
the quality of generated reasoning paths, leading to sub- RAG–based CoT Enhancement
optimalreasoningperformance.Toaddressthischallenge,we e.g.StepBack&BufferofThoughts
propose the Strategic Chain-of-Thought (SCoT), a novel External Knowledge Language
InputQuery Answer
methodology designed to refine LLM performance by inte- Source Model
External sources
gratingstrategicknowledgepriortogeneratingintermediate required
reasoningsteps.SCoTemploysatwo-stageapproachwithin
a single prompt: first eliciting an effective problem-solving Strategic Chain-of-Thought
strategy,whichisthenusedtoguidethegenerationofhigh-
qualityCoTpathsandfinalanswers.Ourexperimentsacross Input Query Language Single-Query Answer
eightchallengingreasoningdatasetsdemonstratesignificant Model Step1: Strategy Elicita�on
Step2: Answer Genera�on
improvements,includinga21.05%increaseontheGSM8K
datasetand24.13%ontheTracking Objectsdataset,respec-
Figure1:ComparisonofsomepopularmethodswithSCoT:
tively, using the Llama3-8b model. Additionally, we extend
As a single-query method, SCoT is efficient and does not
theSCoTframeworktodevelopafew-shotmethodwithau-
rely on external knowledge sources, distinguishing it from
tomatically matched demonstrations, yielding even stronger
results.ThesefindingsunderscoretheefficacyofSCoT,high- otherapproaches.
lighting its potential to substantially enhance LLM perfor-
manceincomplexreasoningtasks.
thereremainsariskthatthefinaloutcomemaybeerroneous.
Introduction This phenomenon is analogous to findings in cogni-
tivescience,wheredifferentproblem-solvingstrategies,al-
The rapid development of large language models (LLMs)
though correct, can vary in their likelihood of produc-
has highlighted their remarkable effectiveness in reason-
ing errors. According to Sweller’s Cognitive Load The-
ing tasks (Huang and Chang 2022; Chang et al. 2024),
ory(Sweller1988),differentproblem-solvingstrategiesim-
particularly when integrated with various prompting tech-
pose varying levels of cognitive load, leading to different
niques (Sivarajkumar et al. 2023). These techniques con-
probabilitiesoferror.
sistently enable impressive performance across diverse do-
mains.Amongthem,theChain-of-Thought(CoT)paradigm Thisvariabilityinerrorprobability,influencedbytheun-
has played a pivotal role in enhancing the reasoning capa- determined strategies used to generate CoT paths, can un-
bilities of LLMs (Kojima et al. 2022; Zhang et al. 2022; dermine the reliability of the CoT approach in critical ap-
Wangetal.2023).Asaresult,CoThasbecomeafundamen- plicationswherepreciseandreliablereasoningisessential.
tal component of contemporary LLMs and is now widely Therefore, further refinement and improvement of the CoT
adoptedinthefieldofnaturallanguageprocessing. methodology are necessary to enhance its performance in
Despite the demonstrated effectiveness of the CoT ap- complexreasoningscenarios,drawingoninsightsfromboth
proachinvariousapplications,itfacessignificantchallenges artificialintelligenceandcognitivescience.
incomplexreasoningtasks.Thesechallengesprimarilyarise Various methods have been developed to address this
fromthevariabilityinthequalityofthereasoningpathsgen- challengebyenhancingthequalityofCoTpathsinLLMs,as
eratedbytheCoTmethod(Wangetal.2022),whicharenot illustratedinFigure1.Amongthesemethods,voting-based
consistently optimal. Consequently, even when LLMs pro- approaches enhance reasoning accuracy by generating di-
duceaCoTpaththatalignswithavalidreasoningprocess, verse reasoning paths and then voting on the most reliable
4202
peS
5
]IA.sc[
1v17230.9042:viXraand correct answer (Wang et al. 2022; Zhang et al. 2023). Llama3-8bmodel.Theseresultsvalidatetheeffectivenessof
Retrieval-Augmented Generation (RAG)-based approaches theSCoTapproach.
introduce external sources to access additional knowledge Thecontributionsofthisworkaresummarizedasfollows:
through multi-step prompting strategies (Lewis et al. 2021;
• We introduce a two-stage methodology that integrates
Yangetal.2024b;Zhengetal.2023).Theseapproachesim-
strategicknowledge,guidingtheLLMtogeneratehigh-
provethereasoningprocessbysystematicallyincorporating
qualityCoTpathsbyfirstdevelopingaproblem-solving
and aligning external information before arriving at the fi-
strategyandthenproducingthefinalanswer.
nal result. Additionally, Suzgun and Kalai (2024) have in-
• Weproposeamethodthatleveragesstrategicknowledge
tegrated various prompt enhancement algorithms, dynami-
toselectandmatchrelevantdemonstrations,enablingthe
callyselectingtheoptimalonetoproducethemostaccurate
precisepairingofhigh-qualityCoTexamples.
resultsduringactualoperation.
These approaches do help mitigate the variability in • Our experimental results validate the effectiveness of
path quality; however, they often come with signifi- SCoT, demonstrating promising outcomes in reasoning
cant resource demands. For instance, methods like Self- tasksacrossmultipledomains.
Consistency(Wangetal.2022)mayrequireupto40queries,
while techniques such as BoT (Yang et al. 2024b) involve RelatedWork
multi-stagequeries.Additionally,someapproachesmayne-
StrategicDiversityinProblemSolving
cessitate the integration of external knowledge sources to
achieve optimal performance, which places high demands In the realm of problem-solving, there is rarely a one-size-
onexpertresources. fits-allapproach.Thecomplexityofeachproblemoftenne-
To tackle this challenge, we propose a novel approach cessitateavarietyofstrategiestoreachaneffectivesolution.
called Strategic Chain-of-Thought (SCoT). SCoT is de- In the fields of education and cognitive science, the phe-
signed to improve the quality of CoT path generation for nomenon of using multiple approaches to solve problems
reasoning tasks by incorporating strategic knowledge. The is quite common (Sweller 1988; Rusczyk 2003). Similarly,
methodinvolvesatwo-stepprocesswithinasingleprompt. researchers have found that LLMs might generate diverse
First, it explores and identifies various problem-solving solutionpathsforonequestion,wheretheproblem-solving
strategies, eliciting the most effective one as the guiding strategiesandanswersofthesemethodsmightvarysignifi-
strategicknowledge.Subsequently,thisstrategicknowledge cantly(WangandZhou2024;Wangetal.2022).
directsthemodelingeneratinghigh-qualityCoTpathsand
EnhancementofCoTPath
producing accurate final answers, ensuring a more effec-
tive reasoning process. We further extend the SCoT frame- Current methods for enhancing the quality of model-
workbyadaptingittoafew-shotmethod.Inthisapproach, generatedcontentarediverseandsophisticated.
strategicknowledgeisusedtoautomaticallyselectthemost Some approaches utilize a voting-based mechanism. For
relevant demonstrations. These examples can be employed example,Wangetal.(2022)introducedtheSelf-Consistency
within both the few-shot and SCoT frameworks to further method, which improves reasoning accuracy by first gen-
enhance reasoning capability. SCoT enhances the model’s erating more than 20 CoT paths and then voting for the
reasoning capabilities without the need for multi-query ap- most consistent answer. Other methods incorporate exter-
proaches or additional knowledge sources. By eliminating nal sources. Zheng et al. (2023) introduced Step Back,
the requirement for multiple queries and external knowl- which prompts models to generate an abstract of the ques-
edgeintegration,SCoTreducescomputationaloverheadand tion to capture deeper logical structures, thereby enhanc-
operational costs, making it a more practical and resource- ingretrieval-augmentedgeneration(RAG)capabilities.Sim-
efficientsolution. ilarly, Yang et al. (2024b) developed another RAG-based
The concept of strategic knowledge in our approach is method, Buffer of Thoughts, which uses knowledge ex-
also inspired by the recent Re-TASK framework (Wang tractedfromexternalsourcesandpredefinedknowledgecat-
et al. 2024), which revisits LLM tasks from the perspec- egories for each task. These elements are integrated into
tives of capability, skill, and knowledge. While Re-TASK a predefined task prompt template, enabling the model to
enhancesLLMcapabilitiesthroughknowledgeinjectionand generate more accurate answers. Additionally, some meth-
skilladaptationviacapabilityitems,SCoTtakesadifferent ods introduce external tools to aid problem-solving. Gao
approach by eliciting knowledge rather than relying on ex- etal.(2023)proposedPAL,whichleverageslargelanguage
plicitknowledgeinjection.Furthermore,thedemonstrations models to parse problems and generate programs as inter-
basedonstrategicknowledgeinSCoTareanalogoustothe mediatereasoningsteps,delegatingthesolutiontoaruntime
capabilityitemsinRe-TASK. environmentlikeaPythoninterpreter.Thisneural-symbolic
We conducted experiments across eight reasoning collaboration has demonstrated improved accuracy across
datasets spanning five distinct domains: mathematical rea- various tasks. Suzgun and Kalai (2024) introduced meta-
soning, commonsense reasoning, physical reasoning, spa- prompting, which integrates existing prompt-based frame-
tialreasoning,andmulti-hopreasoning.Theresultsrevealed works,enablingdynamicselectionofthemosteffectiverea-
substantial improvements across various models, including soning strategy. These strategies, with their complex tem-
a 21.05% increase in accuracy on the GSM8K dataset and plates and multi-stage prompting, provide models with so-
a24.13%increaseontheTracking Objectsdatasetwiththe phisticatedtoolsforadvancingCoTgenerationinLLMs.These methods are inherently complex, with some be- Figure 3(a) illustrates a prompt template utilizing the
ing task-sensitive and others involving multi-turn prompt- SCoT approach. Our prompt consists of five components:
ing;however,theyhavedemonstratedsubstantialefficacyin Role, Workflow, Rule, Initialization, and Task Input. The
enhancing the reasoning capabilities of LLMs, thereby ad- promptincorporatesastructuredworkflowcomprisingthree
vancingthefrontiersofCoTgenerationinmachinelearning. stepsintegratedintoasingleprompt.Thefirsttwostepsare
designedtoidentifyandelicitstrategicknowledgeforsolv-
Method ingtheproblem,whilethethirdstepfocusesonapplyingthe
In this section, we introduce the strategic knowledge, the strategytogeneratetheanswer,asshowninFigure4.
Strategic Chain-of-Thought (SCoT) method, and its exten- We demonstrate that the rules for strategic knowledge
sionthroughthefew-shotapproach. identificationvaryacrossdifferentdomains.Inmathematics,
strategic knowledge favors generating elegant and efficient
StrategicKnowledge solutions,suchasusingthearithmeticseriesformulatosum
LLMstendtoproducevariedCoTpathsforthesameprob- sequences.Inphysics,itinvolvesselectingthemostrelevant
lem.However,thequalityoftheseCoTpathscanvarysig- and straightforward formulas or processes, such as apply-
nificantly (Wang and Zhou 2024; Wang et al. 2022). As ing F = ma to calculate force. For multi-hop reasoning,
shown in the left part of Figure 2(a), when solving the strategicknowledgefocusesondeterminingtheappropriate
math question ”compute the sum of all integers s such that granularity for problem decomposition and recalling perti-
−26<s<24”,onepossibleapproachutilizestermpairing nentinformation.Similarly,inotherdomains,themodelfirst
andsummingthepairstogeneratethefinalanswer.Another developsanoverarchingmethodorworkflowbeforesystem-
possible approach employs the arithmetic series sum for- atically applying it to solve problems, such as optimizing
mulatocomputethefinalresultdirectly.Whilebothmeth- complexsystemsthroughalgorithmsandheuristics.
odsarevalidforproblem-solving,thefirstapproachresults
inlessstableoutputstypicallyduetothecomplexityofthe Few-shotStrategicChain-of-Thought
intermediatesteps.Incontrast,thesecondapproach,which
applies the arithmetic series formula, generally results in WerefinetheSCoTmethodintoafew-shotversionbylever-
betterqualityandmorestablemodeloutputs.Thearithmetic aging the strategy to select demonstrations. Our approach
seriesformulaisconsideredstrategicknowledge. is structured into two stages: constructing a strategy-based
Strategic knowledge (Strategy) refers to a well-defined demonstrationcorpusandperformingmodelinference.
methodorprinciplethatguidesreasoningtowardsacorrect Stage1:StrategicKnowledge-BasedDemonstrationCor-
and stable solution. It involves using structured processes pusConstruction.
that logically lead to the desired outcome, thereby enhanc- This stage involves the following two steps, as shown in
ingthestabilityofCoTgenerationandimprovingtheoverall Figure2(b):
qualityoftheresults. 1. SCoT Answer Generation: We apply the zero-shot
Specifically,strategicknowledgeshouldadheretothefol- SCoT method to the training set to generate a correspond-
lowingprinciples: ingSCoTanswerforeachquestioninthedataset.
1. Correct and Comprehensive Problem-Solving Ap-
2.DemonstrationCorpusConstruction:Thegeneratedan-
proach: It provides a systematic approach that allows the
swersarecomparedwiththegroundtruth.Onlythoseaccu-
modeltogenerateaccurateanswerswhenitfollowstherea-
ratequestion-SCoTanswerpairsareretained.Thisstepas-
soningstepscarefully.
sumes that the strategic knowledge used in these problems
2.RelativelyStraightforwardProblem-SolvingSteps:The
is both correct and relevant. The validated question-SCoT
steps of the method should not be overly complex, while
answerpairsarethencompiledintoademonstrationcorpus
eachstepshouldbesufficientlydetailedtoensureaccuracy
basedonstrategicknowledge.
andpreventoverlybriefoutputsthatcouldleadtoambiguity.
Stage2:ModelInference.
StrategicChain-of-Thought This stage involves the following three steps in a two-
queryprocess,asshownintherightofFigure2(a):
Buildingontheconceptofstrategicknowledge,wepropose
aprompt-basedmethodtoenhancethereasoningqualityof 1. Strategic Knowledge Generation: The LLM generates
LLMs,calledStrategicChain-of-Thought(SCoT). strategicknowledgerelativetotheproblem,focusingonun-
TheSCoTmethodenablesthemodeltofirstelicitstrate- derstandingtheproblemratherthanproducingthefinalan-
gicknowledgebeforegeneratingananswer,ratherthanpro- swer.
ducingananswerdirectly.Specifically,inasingle-queryset- 2. Demonstration Matching: The generated strategic
ting,SCoTinvolvestwokeysteps: knowledge is used to search the demonstration corpus cre-
1.ElicitationofStrategicKnowledge:Themodeliden- atedinStage1.Thesystemidentifiesandmatchesthemost
tifiesanddeterminesoneofthemosteffectiveandefficient relevant demonstrations with the SCoT answers from the
methods for solving the problem, which then serves as the mostsimilarexamples.
strategicknowledgeforthetask. 3. Few-shot Inference: The selected demonstrations are
2.ApplicationofStrategicKnowledge:Themodelsub- integratedasfew-shotexamplesintotheinputprompt(Fig-
sequentlyappliestheidentifiedstrategicknowledgetosolve ure3(b)).Thisintegrationguidesthemodeltogeneratethe
theproblemandderivethefinalanswer. finalpredictionbasedontheprovidedexamples.Ques�on Ques�on Strategy Strategy Demo Corpus
compute the sum of all computethesumofall Elicita�on To solve this problem, we
integers s such that -26 < s < 24? integers s such that -26 < s < 24? c sea qn u u es ne c A er sit uh mm fe o� rc m ula. Training Set Ques�on 3
CoT Zero-shot SCoT Step1: Ques�on2
Strategy Elicita�on Ques�on 1
Step2: There are 18 apples for 28
CoT Answer Answer Genera�on Demonstra�on students.Howmanystudents
T 2h 5e , - i 2n 4te , -g 2e 3rs , …in , t 2h 2e , 2ra 3nge are: - SCoT Answer Corpus c aa nn a h pa pv lee s e ix f c elu acsi hv e st a uc dc ee ns ts to
needs one and no more than
Pairing terms and adding the The Arithme�c sequence sum 2 can share?
pairs: formula can be the problem-
-The sum of -25 + 23 is -2, solvingstrategyforthisques�on. Demonstra�on Selected Zero-shot SCoT
-The sum of -24 + 22 is -2,
-Con�nue this process un�l the The integers in the range are: -25, Ques�on A Ques�onB … SCoTAnswer3
middle of the sequence. -24,-23,…,22,23 SCoT Answer A SCoTAnswerB SCoT Answer 2
The sequence length is 49, The formula for the sum S of an SCoTAnswer1
which is odd. Hence, there are arithme�c series is given by: Wecanse�ngupan
24 pairs that sum to -2, the S =num_of_term/2*(firstterm equa�on…
middle number is 0. + last term) Few-shot SCoT Thus, the answer is 10
Calculatethesumofthepairs: Apply the sum formula:
24 * (-2) = -48 S =49/2*(-25+23)=-49 Demonstra�on Corpus
A -4d 8d + th 0e = m -4id 8dle number: T suh cu hs , t t hh ae t s -2u 6m < o sf <a l 2l 4in it se :g e -4rs 9 s Ques�on 1 Ques�on 2 …
The sum is: -48 SCoT Answer 1 SCoT Answer 2
(a) FrameworkofZero-shotandFew-shotStrategicChain-of-Thought.Thesolidlineinthemiddle (b) Construction of Demonstra-
representsZero-shotSCoT,whilethedashedlineontherightrepresentsFew-shotSCoT. tionCorpus
Figure2:IllustrationofZero-shotandFew-shotStrategicSCoT.Few-shotSCoTbuildsuponZero-shotSCoTbyincorporating
selecteddemonstrations.DetailsoftheFew-shotSCoTapproachareomittedduetospacelimitations.
andmulti-hopreasoning,andspatialreasoning:
# Role Se�ng # RoleSe�ng 1. Mathematics and Physical Reasoning: We assess the
##Workflow ##Workflow modelsusingdatasetssuchasMathQA(Aminietal.2019),
## Demonstra�on
## Rules AQuA(Lingetal.2017),GSM8K(Cobbeetal.2021),and
## Rules
##Ini�aliza�on MMLU-high-school-math(Hendrycksetal.2021)formath-
##Ini�aliza�on
ematical reasoning tasks. These datasets feature a range of
Task Input Task Input
mathematicalproblemswithvaryinglevelsofdifficulty,de-
(a) SCoT (b) Few-shotSCoT mandingstrongmathematicalreasoningabilities.Addition-
ally, we evaluated the models on ARC Challenge (Clark
Figure3:Prompttemplatesforzero-shotandfew-shotSCoT et al. 2018) for physical reasoning, i.e., a popular dataset
thatpresentssignificantchallengesinthisdomain.
2. Commonsense and Multi-hop Reasoning: We evalu-
##Workflow
atethemodelsonCommonsenseQA(CSQA)(Talmoretal.
1. Search for all 1. Analyze the problem and iden�fy any relevant
validProblem- mathema�cal formulas, or approaches that might be helpful, 2019) for commonsense reasoning tasks and StrategyQA
solving methods. and select the approaches that can solve the problem. (SQA) (Geva et al. 2021) for multi-hop reasoning tasks.
2. Selectthe most efficient and prac�cal approach. For Thesedatasetsarewell-regardedintheirrespectivedomains
2 st. r S ae tele gc ict one as example,whenaskedtofindthesumofallintegersfrom-25 andofferasubstantiallevelofdifficulty.
Knowledge. to 23, consider using the summa�on formula of arithme�c
3. Spatial Reasoning: We also evaluate the models us-
insteadofsimplyaddingthenumbersonebyone.The
summa�on formula of arithme�c sequence is an elegant and ingtheTracking Object(Object)(BIG-benchauthors2023)
3.UseStrategic prac�calsolu�on,whilerudelyaddingthenumbersisnot. dataset,whichrepresentsalesscommonbuthighlyintrigu-
Knowledge to 3. Solve the problem step by step following the selected ingtypeofreasoningtask.
complete the task. approach carefully.
Inthefew-shotversionofSCoT,weconductexperiments
exclusively on the MathQA, AQuA, GSM8K, and ARC
Figure4:ExampleofaWorkflowinaMathTaskPrompt
datasets. This selection is due to the requirement that the
datasetmusthaveasufficientlylargetrainingsetwithgold
answers for constructing the demonstration corpus in the
ExperimentalSetup
firststep.Onlythesefourdatasetsmeetthiscriterion.
Inthissection,weintroducethedetailedexperimentalsetup
forvalidationofSCoT,includingthedatasetsusedfortest- Models
ing,themodelscovered,andthebaselinesemployed.
To verify the effectiveness of the SCoT method, we uti-
lize the following LLMs: the Llama3 series (Dubey et al.
DatasetsandTasks
2024) (including Llama3-8B, Llama3-70B, Llama3.1-8B,
TovalidatetheeffectivenessoftheSCoTmethod,wecollect and Llama3.1-70B); the Llama2 series (Touvron et al.
a range of reasoning-related datasets covering domains in- 2023) (including Llama2-7B, Llama2-13B, and Llama2-
cludingmathematicsandphysicalreasoning,commonsense 70B);Mistral-7B(Jiangetal.2023);theQwen2series(Yanget al. 2024a) (including Qwen2-7B and Qwen2-72B); and performanceimprovementsacrossmostdatasets,highlight-
ChatGLM4-9B(TeamGLMetal.2024).ChatGLM4-9Bis ing the effectiveness of the matched demonstrations. The
chat-orientedandothermodelsareinstruction-tuned. SCoT1-shot,whichcombinesbothstrategicknowledgeand
strategy-matched demonstrations, achieves the best results
Baselines overall.
We use zero-shot prompts (Kojima et al. 2022), Self-
Consistency(Wangetal.2022)andStepBack(Zhengetal. ResultsacrossallModels
2023) as baselines. We only conducted experiments on 5
Theexperimentalresultsforallmodelsonthethreedatasets
datasets using Step Back because Step Back is not well-
are shown in Table 2. The experiments demonstrate that
suitedforotherdatasets.BoT(Yangetal.2024b)isnotcho-
SCoTcanenhanceperformanceacrossmostmodels.Inpar-
sen because its template has not been available, making it
ticular,withtheexceptionoftheLlama3.1-8Bmodel,where
impossibletoreproduce.
the addition of SCoT results in a slight decrease in accu-
Weselecttheaccuracyasthemetricfortheperformance,
racyontheMMLUtask,othermodelsexhibitaccuracyim-
whichiscalculatedbytheaverageresultsofthreeindepen-
provementsrangingfrom1.11%to24.13%acrossthethree
dentinferencesoneachmodel.Theexperimentalparameter
datasets.NotethattheCoT0-shothasachieved100%accu-
settingsareprovidedintheappendix.
racywithLlama3.1-70BmodelonTracking Objectdataset,
andSCoT0-shotmaintainsthisperformance.
ExperimentalResults
Inthissection,weempiricallyevaluatetheeffectivenessof
MathQA MMLU CSQA
the Strategic Chain-of-Thought (SCoT) approach. To ver-
70 70 70
ify SCoT’s efficacy across all datasets, we test it using two
open-sourcemodels,Llama3-8BandMistral-7B.Tofurther 60 60 60
validate SCoT’s effectiveness across different models, we 50 50 50
selectonedatasetfromeachofthethreereasoningtaskcat- 40 40 40
egoriesandconducttestsonall7models.Wealsoexamine
30 30 30
theimpactofmodelsize,performablationstudiesonSCoT
20 20 20
components,conductcasestudies,andanalyzeexperimental
7 13 70 7 13 70 7 13 70
efficiencytounderstandthefactorsinfluencingtheeffective- Model Scale (#parameters in billions)
nessofSCoT. CoT 0-shot SCoT 0-shot
Figure5:Accuracy(%)acrossthreedatasetsusingdifferent
ResultsacrossallDatasets
scalesofmodelsinLlama2series
Theexperimentalresultsacrossalldatasetsusingtwomod-
els are presented in Table 1. Notably, in zero-shot settings,
SCoToutperformstheCoTapproachinmosttasks,withpar-
ModelScale
ticularlysignificantimprovementsobservedontheGSM8K
dataset, where accuracy increases from 52.11% to 73.16% Here we investigate the impact of model size on the effec-
afterincorporatingstrategicknowledge.Additionally,SCoT tivenessofSCoT.ExperimentsontheLlama2modelseries
achieves a 24.13% improvement on the Tracking Object with three different sizes are conducted, and the results are
dataset.However,theLlama3-8Bmodelexhibitsa2.6%de- shown in Figure 5. It demonstrates that SCoT can lead to
crease in performance on the ARC dataset. In general, the accuracyimprovementsacrossallsizesoftheLlama2mod-
Llama3-8Bmodelshowsanaverageimprovementof6.92% els.However,ageneraltrendemergesthatperformanceim-
on all datasets, while the Mistral-7B model demonstrates provement decreases marginally with model size. Further-
an average improvement of 3.81% on comparable datasets. more, manual inspection of the model outputs reveals that
Compared to Step Back and Self-Consistency, SCoT also largermodelsaremorelikelytogenerateCoTpathcontain-
performs better than these two methods except for the re- ingstrategicknowledgein0-shotsettings.
sultofSelf-ConsistencywithLlama3-8BmodelontheARC
dataset. Nevertheless, our SCoT still achieves comparable AblationStudy
resultstoit.Notably,SCoTshowssubstantialgainsincom-
We explore the effects of various components within the
monsensereasoningtaskscomparedwithothermethods.
prompt (such as role, workflow, structure, and the quantity
Furthermore, we extend the SCoT framework to sup-
of demonstrations) on accuracy. The experimental results
port few-shot settings by automatically matching demon-
are illustrated in Table 3. Building on the CoT 0-shot ap-
strations, resulting in even stronger performance. The
SCoT1-shot−, as shown in Table 1, refers to CoT prompt- proach,weobservedthataddingroles,incorporatingwork-
flows, and formatting prompts in markdown progressively
ing with demonstrations matched through strategic knowl-
edge. Compared to CoT 0-shot1, SCoT1-shot−, which increasedaccuracy.Wealsoexploredtheimpactofthenum-
berofdemonstrationsonaccuracywithinthefew-shotSCoT
uses strategy-matched demonstrations, shows significant
framework.Experimentalresultsindicatethatasthenumber
1WedonotpresenttheaccuracyofCoT1-shotseparatelyasit ofdemonstrationsincreases,theperformanceofSCoTeither
wascomparabletoCoT0-shotinourexperiments. slightlyimprovesorremainsunchanged.
)%(ycaruccAModel Method MathQA AQuA GSM8K MMLU ARC SQA CSQA Object
CoT0-shot 56.33 49.61 52.11 46.67 80.60 64.60 71.13 44.27
Self-Con 57.00 51.90 48.48 49.52 81.00 66.00 72.06 54.00
StepBack 56.33 50.39 – 47.78 75.80 64.64 – –
Llama3-8B SCoT0-shot 56.67 51.85 73.16 50.00 78.02 68.56 74.00 68.40
SCoT1-shot− 56.33 50.87 74.91 – 73.40 – – –
SCoT1-shot 57.67 55.12 76.57 – 80.60 – – –
CoT0-shot 30.00 29.13 36.26 29.75 67.20 56.22 61.80 21.40
Self-Con 31.42 32.87 34.50 31.88 68.78 53.50 62.69 24.50
StepBack 31.43 32.87 – 31.85 68.00 56.72 – –
Mistral-7B SCoT0-shot 30.44 33.60 38.97 32.35 72.20 61.89 68.00 24.75
SCoT1-shot− 34.33 31.50 45.57 – 67.40 – – –
SCoT1-shot 37.00 35.04 47.38 – 73.20 – – –
Table1:Accuracy(%)usingLlama2-8BandMistral-7Bacrossalldatasets.SCoT1-shot− referstotheresultsobtainedusing
thestandardfew-shotCoTtemplatebutwithdemonstrationsmatchedbystrategy.
Dataset Method Llama3-8B Mistral-7b Chatglm4-9B Qwen2-7B Qwen2-70B Llama3.1-8B Llama3.1-70B
MMLU CoT0-shot 46.67 29.75 66.67 71.97 84.20 59.63 85.19
Math SCoT0-shot 50.00 32.35 68.15 71.85 85.93 56.42 85.19
+3.33 +2.59 +1.48 +1.73
CoT0-shot 64.60 56.22 61.80 61.00 75.22 73.11 64.67
SQA
SCoT0-shot 68.56 61.89 64.67 61.00 77.67 74.22 82.33
+3.96 +5.67 +2.87 +2.45 +1.11 +1.33
CoT0-shot 44.27 21.40 61.80 46.20 93.93 62.60 100.00
Object
SCoT0-shot 68.40 24.67 69.00 47.53 97.47 77.60 100.00
+24.13 +3.27 +7.20 +1.33 +3.54 +15.00
Table2:Accuracy(%)acrosssevenmodelsonMMLU,SQAandTracking Objectdatasets
Method AQuA ARC solvingtheproblem.
Mistral-7B* 29.13% 67.20% Inthefieldofphysics,wefindthatthemodel’sCoTout-
putcouldbemisledbyspecificphrasesinthetaskinput(e.g.,
Mistral-7B+Role* 27.95% 69.80%
”capacitor”),leadingtotheselectionofanincorrectformula.
Mistral-7B+Role 32.28% 71.20% Incontrast,theSCoTapproachsuccessfullyelicitedthecor-
Mistral-7B+WorkFlow* 33.07% 70.40% rect formula. Similarly, in multi-hop reasoning tasks, CoT
Mistral-7B+WorkFlow 31.89% 70.40% outputoftenfocusesondetails,resultinginincompletesub-
sequentlogicalreasoning,whereasSCoTgeneratesanswers
SCoT0-shot(Ours) 33.60% 72.20%
byconsideringtheoverallcontext.
SCoT1-shot(Ours) 35.04% 73.20%
SCoT3-shot(Ours) 35.43% 73.20% EfficiencyAnalysis
Table3:AblationstudyonSCoTpromptcomponents:*de-
Dataset Method Llama3-8B Mistral-7B
notesanon-markdownformat,whileno*indicatesamark-
CoT0-shot 361.384 270.260
downformat.
AQuA
SCoT0-shot 370.378 458.413
CoT0-shot 130.532 858.507
CaseStudy GSM8K
SCoT0-shot 206.278 611.848
We conduct a detailed case study focusing on the validity
CoT0-shot 121.460 89.654
ofthestrategicknowledgeelicitedfromthemodel.Figure6 Object
showsseveraltypicalcases. SCoT0-shot 174.888 162.822
Inthedomainofmathematics,weobservethattheSCoT
output tends to favor solving problems using inequalities Table4:TokenlengthcomparisonforSCoTandCoT0-shot
rather than directly analyzing the problem to reach an an- methods
swer.FortheinstanceoffrogjumpingcalculationintheFig-
ure6,anincorrectsolutionmaymiscalculatethefinaljump’s Due to SCoT’s mechanism of generating strategy before
impact. While generating a strategy ensures accurate cal- solving problems in one query, it is more efficient than
culations by considering all constraints and systematically multi-query methods. However, compared to single-queryMath Ques�on A A well is 30 meters deep. A frog at the bo�om jumps 3 meters up each day but slips back 2 meters each night. How
many days will it take for the frog to climb out of the well?
CoT Answer To solve this problem, we need to consider the frog's daily progress.
On the first day, the frog jumps up 3 meters but slips back 2 meters, resul�ng in a net gain of 1 meter.
Fromtheseconddayonward,thefroggains1metereachday.
The frog needs to climb a total of 30 meters to get out of the well. A�er the first day, it s�ll needs to climb 29 meters.
Therefore,thetotalnumberofdaysrequiredis1+29=30days.
SCoT Answer To address this problem elegantly, we can u�lize a mathema�cal model. The frog gains a net height of 1 meter each
day. However, on the day it reaches the top of the well, it will no longer slip back down.
Let nrepresentthenumberofdaysittakesforthefrogtoclimboutofthewell.Wecanformulatethefollowing
inequality: n -1 + 3 >= 30
Thissimplifiesto:n>=28.Thus,thetotalnumberofdaysforthefrogtoclimboutofthewellis28days.
Physical Ques�on B How much work is required to charge a 10μF capacitor to a poten�al difference of 100V?
CoT Answer Using the formula for work done by a capacitor, which is W=k×V×C, and subs�tu�ngthe values, we find that 0.09Jof
work needs to be done.
SCoTAnswer Usingthecapacitorchargingformula.TheformulaforcapacitorchargingisE =(1/2)CV^2
To subs�tute the values, it needs to do 0.05J of work
Mul�-hop Ques�on C Is shrimp scampi definitely free of plas�c?
CoT Answer Shrimp scampi is a dish made with shrimp, bu�er, garlic, and some�mes lemon, usually served with pasta. The
ingredients typically include fresh or frozen shrimp, bu�er,garlic, and lemon juice. There is no connec�on between
shrimp scampi and plas�c, as plas�c is not an ingredient in this dish. Therefore, shrimp scampi is Truedefinitely
freeofplas�c.
SCoT Answer Shrimp scampi usually does not contain plas�c, but contamina�on is possible. It is typically made with ingredients
like shrimp, garlic, bu�er, and parsley, which do not contain plas�c, and there are no known instances of shrimp
scampibeingmadewithplas�c-containingingredients. Therefore,whileshrimpscampiisusuallymadewithout
plas�c. So, the answer is: No.
Figure6:ComparisonofthepathsgeneratedbyCoTandSCoTindifferentdomains.
methods, the output token length might be longer, poten- that the automatic generation of SCoT-based prompt tem-
tiallydecreasingefficiency.Toinvestigatethis,wemeasure platesisfeasible.
theoutputtokenlengthsfortheAQuA,GSM8K,andTrack-
ing ObjectdatasetsusingbothCoT0-shotandSCoT0-shot
Method Accuracy
methods.TheresultsareshowninTable4.
The results indicate that the token length output by the CoT0-shot 29.13
Mistral-7BmodelontheGSM8Kdatasetdecreaseswiththe SCoT0-shot 33.60
SCoT method. This reduction may be due to the model’s AutoSCoT 31.89
tendency to repetitively generate a specific answer span up
totheinferencelengthlimitontheGSM8KdatasetinCoT
Table 5: Accuracy(%) using automatically generated
0-shot,leadingtoadeclineinaccuracy.SCoTmitigatesthis
promptsbyLLMsbasedontheSCoTconcept
issue. Besides, the length of SCoT varies from 1.03 to 1.8
times that of CoT, averaging around 1.5 times. This shows
thatwhileourmethodissomewhatslowerthanCoT,theef-
ficiencyremainsmanageable.
Conclusion
Discussions Inthispaper,weintroducetheStrategicChain-of-Thought,
a method that enables LLMs to autonomously generate an
AutomaticSCoT
optimalChain-of-Thoughtpath.Byintegratingastructured
To demonstrate that our experimental results are not influ- workflow for eliciting and applying strategic knowledge,
encedbyhuman-craftedpromptsbutratherduetothecon- SCoTenhancesthemodel’sabilitytoproduceahighquality
cept of SCoT, we conduct a preliminary test to evaluate outputs. We further extend SCoT to a few-shot version by
whether the SCoT prompt templates can be automatically matchingdemonstrationsthroughstrategicknowledgefrom
generated. We provide the SCoT concept to Qwen2-72B apredefinedstrategicknowledge-basedcorpus.Experimen-
to generate the corresponding prompt templates and tested talresultsdemonstratetheeffectivenessofboth0-shotSCoT
these on the AQuA dataset. The results are presented in andfew-shotSCoT.
Table 5. The findings indicate that while the accuracy of Overall,SCoToffersapromisingframeworkforimprov-
prompts automatically generated based on the SCoT con- ing the quality of reasoning path in LLMs. Future research
cept is lower than that of manually crafted SCoT prompts, willfocusonevaluatingitseffectivenesswithmorecomplex
itisstillsuperiorto0-shotCoTperformance.Thissuggests problemsandexploringfurtherapplications.References Ling, W.; Yogatama, D.; Dyer, C.; and Blunsom, P.
2017. Program Induction by Rationale Generation :
Amini,A.;Gabriel,S.;Lin,P.;Koncel-Kedziorski,R.;Choi,
Learning to Solve and Explain Algebraic Word Problems.
Y.;andHajishirzi,H.2019.MathQA:TowardsInterpretable
arXiv:1705.04146.
Math Word Problem Solving with Operation-Based For-
malisms. arXiv:1905.13319. Rusczyk,R.2003.TheArtofProblemSolving.Washington,
D.C.:MathematicalAssociationofAmerica.
BIG-bench authors. 2023. Beyond the Imitation Game:
Quantifying and extrapolating the capabilities of language Sivarajkumar, S.; Kelley, M.; Samolyk-Mazzanti, A.;
models. TransactionsonMachineLearningResearch. Visweswaran, S.; and Wang, Y. 2023. An Empirical Eval-
uation of Prompting Strategies for Large Language Mod-
Chang, Y.; Wang, X.; Wang, J.; Wu, Y.; Yang, L.; Zhu, K.;
els in Zero-Shot Clinical Natural Language Processing.
Chen,H.;Yi,X.;Wang,C.;Wang,Y.;etal.2024. Asurvey
arXiv:2309.08008.
onevaluationoflargelanguagemodels. ACMTransactions
onIntelligentSystemsandTechnology,15(3):1–45. Suzgun, M.; and Kalai, A. T. 2024. Meta-prompting: En-
hancing language models with task-agnostic scaffolding.
Clark, P.; Cowhey, I.; Etzioni, O.; Khot, T.; Sabharwal, A.;
arXivpreprintarXiv:2401.12954.
Schoenick,C.;andTafjord,O.2018.ThinkyouhaveSolved
Sweller,J.1988. CognitiveLoadDuringProblemSolving:
Question Answering? Try ARC, the AI2 Reasoning Chal-
EffectsonLearning. CognitiveScience,12(2):257–285.
lenge. arXiv:1803.05457.
Talmor,A.;Herzig,J.;Lourie,N.;andBerant,J.2019.Com-
Cobbe, K.; Kosaraju, V.; Bavarian, M.; Chen, M.; Jun, H.;
monsenseQA: A Question Answering Challenge Targeting
Kaiser,L.;Plappert,M.;Tworek,J.;Hilton,J.;Nakano,R.;
Commonsense Knowledge. In Burstein, J.; Doran, C.; and
Hesse, C.; and Schulman, J. 2021. Training Verifiers to
Solorio,T.,eds.,Proceedingsofthe2019Conferenceofthe
SolveMathWordProblems. arXiv:2110.14168.
North American Chapter of the Association for Computa-
Dubey,A.;Jauhri,A.;Pandey,A.;Kadian,A.;Al-Dahle,A.; tional Linguistics: Human Language Technologies, Volume
Letman,A.;andetal.2024. TheLlama3HerdofModels. 1(LongandShortPapers),4149–4158.Minneapolis,Min-
arXiv:2407.21783. nesota:AssociationforComputationalLinguistics.
Gao, L.; Madaan, A.; Zhou, S.; Alon, U.; Liu, P.; Yang, Team GLM; Zeng, A.; Xu, B.; Wang, B.; Zhang, C.;
Y.; Callan, J.; and Neubig, G. 2023. Pal: Program-aided Yin, D.; and et al. 2024. ChatGLM: A Family of Large
languagemodels. InInternationalConferenceonMachine Language Models from GLM-130B to GLM-4 All Tools.
Learning,10764–10799.PMLR. arXiv:2406.12793.
Geva, M.; Khashabi, D.; Segal, E.; Khot, T.; Roth, D.; and Touvron,H.;Martin,L.;Stone,K.;Albert,P.;Almahairi,A.;
Berant, J. 2021. Did Aristotle Use a Laptop? A Question Babaei,Y.;andetal.2023. Llama2:OpenFoundationand
Answering Benchmark with Implicit Reasoning Strategies. Fine-TunedChatModels. arXiv:2307.09288.
arXiv:2101.02235.
Wang,L.;Xu,W.;Lan,Y.;Hu,Z.;Lan,Y.;Lee,R.K.-W.;
Hendrycks,D.;Burns,C.;Basart,S.;Zou,A.;Mazeika,M.; and Lim, E.-P. 2023. Plan-and-solve prompting: Improv-
Song,D.;andSteinhardt,J.2021. MeasuringMassiveMul- ingzero-shotchain-of-thoughtreasoningbylargelanguage
titaskLanguageUnderstanding. arXiv:2009.03300. models. arXivpreprintarXiv:2305.04091.
Huang, J.; and Chang, K. C.-C. 2022. Towards reason- Wang,X.;Wei,J.;Schuurmans,D.;Le,Q.;Chi,E.;Narang,
ing in large language models: A survey. arXiv preprint S.; Chowdhery, A.; and Zhou, D. 2022. Self-consistency
arXiv:2212.10403. improves chain of thought reasoning in language models.
arXivpreprintarXiv:2203.11171.
Jiang, A. Q.; Sablayrolles, A.; Mensch, A.; Bamford, C.;
Chaplot, D. S.; de las Casas, D.; and et al. 2023. Mistral Wang,X.;andZhou,D.2024. Chain-of-thoughtreasoning
7B. arXiv:2310.06825. withoutprompting. arXivpreprintarXiv:2402.10200.
Kojima, T.; Gu, S. S.; Reid, M.; Matsuo, Y.; and Iwasawa, Wang, Z.; Zhao, S.; Wang, Y.; Huang, H.; Shi, J.; Xie, S.;
Y. 2022. Large language models are zero-shot reason- Wang, Z.; Zhang, Y.; Li, H.; and Yan, J. 2024. Re-TASK:
ers. Advancesinneuralinformationprocessingsystems,35: Revisiting LLM Tasks from Capability, Skill, and Knowl-
22199–22213. edgePerspectives. arXiv:2408.06904.
Kwon, W.; Li, Z.; Zhuang, S.; Sheng, Y.; Zheng, L.; Yu, Yang, A.; Yang, B.; Hui, B.; Zheng, B.; Yu, B.; Zhou, C.;
C. H.; Gonzalez, J. E.; Zhang, H.; and Stoica, I. 2023. Ef- and et al. 2024a. Qwen2 Technical Report. arXiv preprint
ficient Memory Management for Large Language Model arXiv:2407.10671.
Serving with PagedAttention. In Proceedings of the ACM Yang, L.; Yu, Z.; Zhang, T.; Cao, S.; Xu, M.; Zhang, W.;
SIGOPS29thSymposiumonOperatingSystemsPrinciples. Gonzalez, J. E.; and Cui, B. 2024b. Buffer of Thoughts:
Thought-AugmentedReasoningwithLargeLanguageMod-
Lewis, P.; Perez, E.; Piktus, A.; Petroni, F.; Karpukhin,
els. arXivpreprintarXiv:2406.04271.
V.; Goyal, N.; Ku¨ttler, H.; Lewis, M.; tau Yih, W.;
Rockta¨schel, T.; Riedel, S.; and Kiela, D. 2021. Retrieval- Zhang, Z.; Zhang, A.; Li, M.; and Smola, A. 2022. Auto-
Augmented Generation for Knowledge-Intensive NLP maticchainofthoughtpromptinginlargelanguagemodels.
Tasks. arXiv:2005.11401. arXivpreprintarXiv:2210.03493.Zhang, Z.; Zhang, A.; Li, M.; Zhao, H.; Karypis, G.; and
Smola,A.2023. Multimodalchain-of-thoughtreasoningin
languagemodels. arXivpreprintarXiv:2302.00923.
Zheng,H.S.;Mishra,S.;Chen,X.;Cheng,H.-T.;Chi,E.H.;
Le, Q. V.; and Zhou, D. 2023. Take a step back: Evoking
reasoning via abstraction in large language models. arXiv
preprintarXiv:2310.06117.DetailsofExperiments shown in Figure 16. The automated SCOT prompts were
generatedusingLLMsbygiventheideaofSCoT.
ModelsDetails
This experiment involves ten models, nine of which are
public (Llama3-8B, Llama2-7B, Mistral-7B, Llama3.1-8B,
Qwen2-7B, ChatGLM4-9B, Llama3-70B, Llama3.1-70B,
Llama2-70B,andQwen2-72B),whileonemodelisprivate.
The sources and licenses for all public models are detailed
inTable6.
DatasetsDetails
This experiment involves eight datasets: MathQA, AQuA,
GSM8K, MMLU, ARC, StrategyQA, CommonsenseQA,
andTracking Object.Alldatasetsusedinthisstudyarepub-
liclyavailable,withtheirsourcesandlicensesdetailedinTa-
ble7.
MathQA, AQuA, MMLU, ARC, StrategyQA, Common-
senseQA, and Tracking Object consist of multiple-choice
questions. To determine correctness, we compare the pre-
dicted choice with the gold (correct) choice. For GSM8K,
theanswersarenumericaltextspans;weassesscorrectness
bycheckingifthepredictedanswerexactlymatchesthegold
answer.
OtherDetails
For all experiments, except those involving Self-
Consistency, the temperature is set to 0, and the top p
parameter is set to 1. For Self-Consistency, following the
settings from the original paper (Wang et al. 2022), the
temperatureisadjustedto0.5,andtop pissetto0.5.
Weutilizevllm(Kwonetal.2023)astheinferenceframe-
workforalldeployments.
Results
AllResults
Accuracy is used as the evaluation metric. We conducted
threeindependentinferencerunsforallexperimentsandcal-
culatedtheaverageresults.However,duetothehighcompu-
tationalcost,weperformedonlyasingleinferenceforSelf-
Consistency.Theaccuracyandstandarddeviationresultsare
presentedinTable8andTable9.
CaseStudy
We conducted a detailed case study to assess the validity
of the strategic knowledge elicited from the model. Fig-
ures 7 and 8 present several representative cases spanning
math reasoning, physical reasoning, commonsense reason-
ing,multi-hopreasoning,andspatialreasoning.
ExperimentalPrompts
The prompt for standard zero-shot Chain-of-Thought is
shown in Figure 9. Prompts for zero-shot Strategic Chain-
of-Thought are displayed in Figure 10 (for math reason-
ing), Figure 11 (for multi-hop reasoning), Figure 13 (for
physicalreasoning),Figure12(forcommonsensereasoning)
andFigure14(forspatialreasoning).Promptsforone-shot
StrategicChain-of-ThoughtareshowninFigure15.Finally,
the prompts for automated Strategic Chain-of-Thought areModels Modelsources License
Llama2-7B-chat https://huggingface.co/meta-llama/Llama-2-7b-chat llama2license
Llama2-13B https://huggingface.co/meta-llama/Llama-2-13b-chat llama2license
Llama2-70B https://huggingface.co/meta-llama/Llama-2-70b-chat llama2license
https://www.modelscope.cn/models/FlagAlpha/
Llama3-8B ApacheLicense2.0
Llama3-Chinese-8B-Instruct/summary
Llama3.1-8B https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct llama3.1license
Llama3.1-70B https://huggingface.co/meta-llama/Meta-Llama-3.1-70B-Instruct llama3.1license
Mistral-7B https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2 ApacheLicense2.0
Qwen2-7B https://huggingface.co/Qwen/Qwen2-7B-Instruct ApacheLicense2.0
Qwen2-72B https://huggingface.co/Qwen/Qwen2-72B-Instruct ApacheLicense2.0
ChatGLM4-9b https://huggingface.co/THUDM/glm-4-9b-chat glm-4-9bLicense
Table6:Models,sourcesandlicensesusedinthiswork
Datasets Sources Licenses
MathQA https://huggingface.co/datasets/datafreak/MathQA ApacheLicense2.0
AQuA https://github.com/google-deepmind/AQuA ApacheLicense2.0
GSM8K https://huggingface.co/datasets/openai/gsm8k MITLicense
MMLU https://huggingface.co/datasets/cais/mmlu MITLicense
ARC https://huggingface.co/datasets/allenai/ai2 arc CC-BY-SA-4.0License
StrategyQA https://huggingface.co/datasets/ChilleD/StrategyQA/viewer/default/test MITLicense
CommonsenseQA https://huggingface.co/datasets/tau/commonsense qa MITLicense
https://github.com/google/BIG-bench/tree/092b196c1f8f14a54bbc62f24759d43bde46dd3b
ObjectTracking ApacheLicense2.0
/bigbench/benchmark tasks/tracking shuffled objects/three objects
Table7:Datasets,sourcesandlicensesusedinthiswork
Model Method MathQA AQuA GSM8K MMLU ARC SQA CSQA Object
CoT0-shot 56.33 49.61 52.11 46.67 80.60 64.60 71.13 44.27
±0.000 ±1.790 ±0.129 ±0.000 ±0.000 ±0.646 ±0.094 ±0.736
Self-Con 57.00 51.90 48.48 49.52 81.00 66.00 72.06 54.00
StepBack 56.33 50.39 – 47.78 75.80 64.64 – –
±0.272 ±0.000 ±0.000 ±0.248 ±0.2722
Llama3-8B SCoT0-shot 56.67 51.85 73.16 50.00 78.02 68.56 74.00 68.40
±0.000 ±1.299 ±0.163 ±0.000 ±0.000 ±0.566 ±0.000 ±0.000
SCoT1-shot− 56.33 50.87 74.91 – 73.40 – – –
±0.000 ±2.140 ±0.000 ±0.000
SCoT1-shot 57.67 55.12 76.57 – 80.60 – – –
±0.000 ±0.000 ±0.000 ±0.000
CoT0-shot 30.00 29.13 36.26 29.75 67.20 56.22 61.80 21.40
±0.000 ±1.245 ±1.854 ±0.924 ±0.356 ±0.314 ±0.000 ±0.000
Self-Con 31.42 32.87 34.50 31.88 68.78 53.50 62.69 24.50
StepBack 31.43 32.87 – 31.85 68.00 56.72 – –
±0.000 ±0.322 ±0.495 ±0.000 ±0.000
Mistral-7B SCoT0-shot 30.44 33.60 38.97 32.35 72.20 61.89 68.00 24.75
±0.874 ±1.523 ±0.655 ±1.665 ±0.370 ±0.415 ±0.000 ±0.165
SCoT1-shot− 34.33 31.50 45.57 – 67.40 – – –
±0.000 ±0.964 ±1.087 ±0.000
SCoT1-shot 37.00 35.04 47.38 – 73.20 – – –
±0.000 ±0.000 ±0.107 ±0.000
Table8:Accuracy(%)usingLlama2-8BandMistral-7Bacrossalldatasets.SCoT1-shot− referstotheresultsobtainedusing
thestandardfew-shotCoTtemplatebutwithdemonstrationsmatchedbystrategy.Dataset Method Llama3-8B Mistral-7b Chatglm4-9B Qwen2-7B Qwen2-70B Llama3.1-8B Llama3.1-70B
MMLU CoT0-shot 46.67 29.75 66.67 71.97 84.20 59.63 85.19
±0.000 ±0.924 ±0.302 ±0.349 ±0.349 ±0.000 ±0.605
Math SCoT0-shot 50.00 32.35 68.15 71.85 85.93 56.42 85.19
±0.000 ±1.665 ±0.907 ±0.302 ±0.302 ±0.175 ±0.000
CoT0-shot 64.60 56.22 61.80 61.00 75.22 73.11 64.67
SQA ±0.595 ±0.314 ±0.363 ±0.000 ±0.314 ±0.314 ±0.000
SCoT0-shot 68.56 61.89 64.67 61.00 77.67 74.22 82.33
±0.566 ±0.415 ±0.408 ±0.157 ±0.272 ±0.157 ±0.157
CoT0-shot 44.27 21.40 61.80 46.20 93.93 62.60 100.00
Object ±0.736 ±0.000 ±0.000 ±0.000 ±0.525 ±0.411 ±0.000
SCoT0-shot 68.40 24.67 69.00 47.53 97.47 77.60 100.00
±0.000 ±0.000 ±0.000 ±0.094 ±0.339 ±0.993 ±0.000
Table9:Accuracy(%)acrosssevenmodelsonMMLU,SQAandTracking Objectdatasets
MathQues�onA A wellis30metersdeep.Afrogatthebo�omjumps3metersupeachdaybutslipsback2meterseachnight.Howmanydayswill
it take for the frog to climb out of the well?
CoT Answer To solvethis problem, we need to consider the frog's daily progress.
On the first day, the frog jumps up 3 meters but slips back 2 meters, resul�ng in a net gain of 1 meter.
From the second day onward, the frog gains 1 meter each day.
The frog needs to climb a total of 30 meters to get out of the well. A�er the first day, it s�ll needs to climb 29 meters. Therefore,
the total number of days required is 1+29=30 days.
SCoT Answer To address this problem elegantly, we can u�lize a mathema�cal model. The frog gains a net height of 1 meter each day. However,
on the day it reaches the top of the well, it will no longer slip back down.
Let n represent the number of days it takes for the frog to climb out of the well. We can formulate the following inequality: n -1
+ 3 >= 30
This simplifies to: n >= 28. Thus, the total number of days for the frog to climb out of the well is 28 days.
Physical Ques�on B How much work is required to charge a 10μF capacitor to a poten�al difference of 100V?
CoT Answer Using the formula for work done by a capacitor, which is W=k×V×C, and subs�tu�ng the values, we find that 0.09Jof work needs
tobedone.
SCoT Answer Using the capacitor charging formula. The formula for capacitor charging is E = (1/2)CV^2
Tosubs�tutethevalues,itneedstodo0.05Jofwork
Math Ques�on C Jordan plays video games for 2 hours every day. He also has a part-�me job where he earns $10 an hour. How much money would
Jordanearninoneweekifhespenthisvideogame�meworkinginstead?
CoT Answer To calculate how much money Jordan would earn in one week if he spent his video game �me working instead, we need to follow
these steps:
Step1:Jordanplaysvideogamesfor2hourseveryday.Thereare7daysinaweek,sohewouldworkfor7-2 =5hourseachday.
Step 2: Jordan earns 10 an hour. If he works 5 hours a day, his daily earnings would be 5 hours * 10/hour = 50.
To find his weekly earnings, we mul�ply his daily earnings by the number of days in a week: 50/day * 7 days/week = 350/week.
SCoT Answer Calculate the total hours he would have worked in a week by mul�plying the number of hours he plays per day by the number of
days in a week. Given: 1. Hours Jordan plays video games per day = 2 hours; 2. Days in a week = 7 days
Totalhours=2hours/day×7 days/week=14hours/week
Calculate the total amount of money Jordan would have earned in one week if he worked those 14 hours.
Total earnings = 14hours/week ×$10/hour = $140/week
Figure7:ComparisonofthepathsgeneratedbyCoTandSCoTindifferentdomains.Object Ques�on A Alice, Bob, and Claire are playing a game. At the start of the game, they are each holding a ball: Alice has a brown ball, Bob has a red
ball, and Claire has a purple ball. As the game progresses, pairs of players trade balls. First, Bob and Claire swap balls. Then, Alice and
Claire swap balls. Finally, Alice and Bob swap balls. At the end of the game, Claire has the
A) brown ball; B) red ball; C) purple ball
CoT Answer Let's break down the sequence of swaps:
1.BobandClaireswapballs:Bobgetsthepurpleball,andClairegetstheredball.
2. Alice and Claire swap balls: Alice gets the red ball (which Claire had just go�en), and Claire gets the brownball.
3. Alice and Bob swap balls: Alice gets the red ball(which she had just given to Claire), and Bob gets the brown ball.
So,attheendofthegame,thefinalposi�onsare:Clairehasthepurpleball.
SCoT Answer Ini�al State: Alice has a brown ball; Bob has a redball; Claire has a purple ball
Process Exchanges:
1.BobandClaireswapballs:Alices�llhasabrownball;Bobhasapurpleball;Clairehasaredball
2. Alice and Claire swap balls: Alice has a red ball; Bob has a purple ball; Claire has a brownball
3. Alice and Bob swap balls: Alice has a purple ball; Bob has a red ball; Claire has a brown ball
DeterminetheAnswer:ThecorrectansweristhatClairehasthebrownball.
MathQues�on B Find out which of the following values is the mul�ple of X, if it is divisible by 9 and 12?
A)36;B)15;C)17;D)5;E)7
CoT Answer To find the mul�ple of a number that is divisibleby both 9 and 12, we need to find the common mul�ple of 9 and 12.
Theleastcommonmul�ple(LCM)of9and12canbefoundusingtheprimefactoriza�onmethodortheLCMtable.
The prime factoriza�on of 9 is3^2, and the prime factoriza�on of 12 is 2^2 * 2 * 3.
Since 3^2 is a factor of both 3^2 and 2^2 * 2 * 3, the LCM of 9 and 12 is 3^2 * 2^2 * 2 * 3 = 3^2 * 2^2 * 3 = 3^2 * 9 = 9^2 =81.
Now,let'scheckwhichop�onisaperfectsquareof9.Op�onB,15, isaperfectsquareof9,itisthecorrectanswer..
SCoT Answer The problem states that X is divisible by 9 and 12. To find the mul�ple of X that fits the given condi�ons, we need to find the least
common mul�ple (LCM) of 9 and 12.
Choosethemostefficientandprac�calapproach:theLCMof9and12is36.
Therefore, the correct answer is A) 36.
Mul�-hop Ques�on C Is shrimp scampi definitely free of plas�c?
CoT Answer Shrimp scampiis a dish made with shrimp, bu�er, garlic, and some�mes lemon, usually served with pasta. The ingredients typically
include fresh or frozen shrimp, bu�er, garlic, and lemonjuice. There is no connec�on between shrimp scampi and plas�c, as plas�c
is not an ingredient in this dish. Therefore, shrimp scampi is Truedefinitely free of plas�c.
SCoTAnswer Shrimpscampiusuallydoesnotcontainplas�c,butcontamina�onispossible.Itistypicallymadewithingredientslikeshrimp, garlic,
bu�er, and parsley, which do not contain plas�c, and there are no known instances of shrimp scampi being made with plas�c-
containing ingredients. Therefore,while shrimp scampi is usuallymade without plas�c. So, the answer is: No.
Figure8:ComparisonofthepathsgeneratedbyCoTandSCoTindifferentdomains.
I will provide you with a math problem and 5 op�ons. Please choose
the correct op�on from the five provided and indicate your answer
with [Answer]op�on[Answer], such as [Answer]C[Answer].
Please output the answer at the end in strict accordance with the
Zero-shot CoT template
output format.
Problem: [Please Put Your Ques�ons Here]
Op�ons: [Please Put Your Op�ons Here]
Answer: Let's think step by step.
Figure9:Anexampleofpromptingforstandardzero-shotCoT# Role
A highlyskilledmathema�cianandalgorithmexpert.
# Workflow
1.Analyzetheproblemandiden�fyanyrelevantmathema�calformulas,or
approachesthatmightbehelpful,andselecttheapproachesthatcansolvethe
problem.
2.Choosethemostefficientandprac�calapproach.Forexample,whenaskedtofind
thesumofallintegersfrom-25to23,considerusingthesumma�onformulaof
arithme�csequenceinsteadofsimplyaddingthenumbersonebyone.The
summa�onformulaofarithme�csequenceisanelegantandprac�calsolu�on,while
rudelyaddingthenumbersisnot.
3.Solvetheproblemstepbystepfollowingtheselectedapproachcarefully.
Zero-shotSCoTtemplate
##Rules
1.Avoidusingbruteforcemethods,astheydonotreflecttheprofessionalism.
2.Indicateyouranswerwith[Answer]op�on[Answer],suchas[Answer]C[Answer].
Pleaseoutputtheanswerattheendinstrictaccordancewiththeoutputformat.
##Ini�aliza�on
As<Role>,pleasefollow<Rules>strictly.Yourtaskistosolvethemathproblem
following<Workflow>.Iwillprovideyouwithaproblemand5op�ons.Pleasechoose
thecorrectop�onfromthefiveprovided.
Problem:[PleasePutYourQues�onsHere]
Op�ons:[PleasePutYourOp�onsHere]
Answer:Let'sthinkstepbystep.
Figure10:AnexampleofpromptingforstandardStrategicChain-of-Thoughtinmathreasoningtasks
# Role
Anexpertofworldknowledgewithstronglogicalskills.
# Workflow
1.Analyzetheproblemandbreakdownthecomplexqueryintosimplersub-
ques�ons.
2.Sequen�allyfindingreliableanswersforeachsub-ques�on.
3.Integra�ngtheseanswerstoformacomprehensive.Directlyansweringthemain
ques�onisrude,butbreakingitdown,answeringthesub-ques�ons,andthen
integra�ngtheanswersiselegantandprac�cal.
##Rules
Zero-shotSCoTtemplate
1.Avoidusingbruteforcemethods,astheydonotreflecttheprofessionalism.
2.Indicateyouranswerwith[Answer]op�on[Answer],suchas[Answer]C[Answer].
Pleaseoutputtheanswerattheendinstrictaccordancewiththeoutputformat.
##Ini�aliza�on
As<Role>,pleasefollow<Rules>strictly.Yourtaskistosolvetheproblemfollowing
<Workflow>.Iwillprovideyouwithaproblemand5op�ons.Pleasechoosethe
correctop�onfromthefiveprovided.
Problem:[PleasePutYourQues�onsHere]
Op�ons:[PleasePutYourOp�onsHere]
Answer:Let'sthinkstepbystep.
Figure11:AnexampleofpromptingforstandardStrategicChain-of-Thoughtinmulti-hopreasoningtasks# Role
Anexpertwithworldknowledgeandreasoningabili�es.
# Workflow
1.UnderstandingtheQues�on:Iden�fykeyconceptsandcomprehendtheques�on's
context.Ensureyougraspthemainideaandanyanalogiesbeingused.Searchforany
concept,knowledge,orcommonsenserelatedtothetopic.
2.AnalyzingtheOp�ons:Readeachchoicecarefully,understanditsmeaning,and
relateittotheques�on'scontexttodeterminerelevance.
3.LogicalReasoning:Uselogicalreasoningtoeliminateop�onsthatareclearly
irrelevantorincorrectbasedontheques�on'scontext.Comparetheremaining
op�onstoiden�fytheonethatbestalignswiththeques�on'srequirementsandthe
contextprovided.
Zero-shotSCoTtemplate
##Rules
1.Avoidusingbruteforcemethods,astheydonotreflecttheprofessionalism.
2.Indicateyouranswerwith[Answer]op�on[Answer],suchas[Answer]C[Answer].
Pleaseoutputtheanswerattheendinstrictaccordancewiththeoutputformat.
##Ini�aliza�on
As<Role>,pleasefollow<Rules>strictly.Yourtaskistosolvetheproblemfollowing
<Workflow>.Iwillprovideyouwithaproblemand5op�ons.Pleasechoosethe
correctop�onfromthefiveprovided.
Problem:[PleasePutYourQues�onsHere]
Op�ons:[PleasePutYourOp�onsHere]
Answer:Let'sthinkstepbystep.
Figure12:AnexampleofpromptingforstandardStrategicChain-of-Thoughtincommonsensereasoningtasks
# Role
A careful expert proficient in various world knowledge.
# Workflow
1. Careful Ques�on Analysis:
-ReadtheProblemandtheOp�onsCarefully:Ensureyouunderstandthebackgroundandspecificques�onbeingasked.
-Iden�fy Keywords: Extract key terms or phrases from the Problem and the Op�ons, try recalling their meanings.
-Understand the Problem: Ensure you clearly understand what the Problem is asking, including any specific condi�ons or
requirements. Eliminate op�ons that are not relevant to the problem.
2.Iden�fyRelevantKnowledgeandapproaches:
-Recall Related Knowledge or approach: Iden�fy all the relevant concepts, principles, or formulas that might apply to the
Problem.
-Select Appropriate Knowledge: Choose the knowledge, formulas and approaches that can solve the problem.
3.ChoosetheMostEfficientandPrac�calKnowledgeandFormulas:Whensolvingtheproblem,selectthemostefficientand
prac�cal knowledge, formulas or approaches. For example, when the descrip�on of a problem is related to poten�al energy
and kine�c energy of an object, a�er using the formula PE = mgh, carefully analyze each op�on to judge right or wrong,
rather than relying on experience or ready-made theorems to select op�ons.
4.CarefulApplica�onofKnowledgeandFormulas:
Zero-shot SCoT template -Detailed Analysis: When applying formulas and knowledge, pay a�en�on to the specific condi�ons and variables in the
problem.
-Logical Reasoning: Carefully analyze each variable in the formula or methodically derive conclusions based on the
knowledgepoint,ensuringthereasoningprocessisconsistentandcorrect.Forexample,whenusingPE=mgh, youneedto
analyze the overall effect of all variables, including m, g, and h, rather than just one variable.
## Rules
1.Avoidusingbruteforcemethods,astheydonotreflecttheprofessionalism.
2. Indicate your answer with [Answer]op�on[Answer], such as [Answer]C[Answer]. Please output the answer at the end in
strict accordance with the output format.
##Ini�aliza�on
As <Role>, please follow <Rules> strictly. Yourtask is to solve the problem following <Workflow>.I will provide you with a
problem and 5 op�ons. Please choose the correct op�on from the five provided.
Problem:[PleasePutYourQues�onsHere]
Op�ons: [Please Put Your Op�ons Here]
Answer: Let's think step by step.
Figure13:AnexampleofpromptingforstandardStrategicChain-of-Thoughtinphysicalreasoningtasks# Role
A veryme�culouslogicalAnalyst.
# Workflow
1.Ini�alState:First,listtheini�alstateoftheballseachpersonhasaccordingtothe
problemstatement.
2.ProcessExchanges:Next,carefullyreadtheproblemstatement.Foreachexchange,
updatethecurrentstateoftheballsanddocumenttheresultofeachexchange.
3.DeterminetheAnswer:Onceallexchangesarecompleted,iden�fywhichfriend's
ballcolorisbeinginquiredaboutintheproblemstatementandselectthecorrect
answer.
Zero-shotSCoTtemplate ##Rules
1.Avoidusingbruteforcemethods,astheydonotreflecttheprofessionalism.
2.Indicateyouranswerwith[Answer]op�on[Answer],suchas[Answer]C[Answer].
Pleaseoutputtheanswerattheendinstrictaccordancewiththeoutputformat.
##Ini�aliza�on
As<Role>,pleasefollow<Rules>strictly.Yourtaskistosolvetheproblemfollowing
<Workflow>.Iwillprovideyouwithaproblemand5op�ons.Pleasechoosethe
correctop�onfromthefiveprovided.
Problem:[PleasePutYourQues�onsHere]
Op�ons:[PleasePutYourOp�onsHere]
Answer:Let'sthinkstepbystep.
Figure14:AnexampleofpromptingforstandardStrategicChain-of-Thoughtinspatialreasoningtasks
# Role
A highlyskilledmathema�cianandalgorithmexpert.
# Workflow
1.Analyzetheproblemandiden�fyanyrelevantmathema�calformulas,or
approachesthatmightbehelpful,andselecttheapproachesthatcansolvethe
problem.
2.Choosethemostefficientandprac�calapproach.Forexample,whenaskedtofind
thesumofallintegersfrom-25to23,considerusingthesumma�onformulaof
arithme�csequenceinsteadofsimplyaddingthenumbersonebyone.The
summa�onformulaofarithme�csequenceisanelegantandprac�calsolu�on,while
rudelyaddingthenumbersisnot.
3.Solvetheproblemstepbystepfollowingtheselectedapproachcarefully.
##Demonstra�ons
Problem:[PleasePutYourDemonstra�onProblemHere]
One-shotSCoTtemplate
Op�ons:[PleasePutYourDemonstra�onOp�onsHere]
Answer:[PleasePutYourDemonstra�onAnswerHere]
##Rules
1.Avoidusingbruteforcemethods,astheydonotreflecttheprofessionalism.
2.Indicateyouranswerwith[Answer]op�on[Answer],suchas[Answer]C[Answer].
Pleaseoutputtheanswerattheendinstrictaccordancewiththeoutputformat.
##Ini�aliza�on
As<Role>,pleasefollow<Rules>strictly.Yourtaskistosolvethemathproblem
following<Workflow>,<Demonstra�on>issomeexamples.Iwillprovideyouwitha
problemand5op�ons.Pleasechoosethecorrectop�onfromthefiveprovided.
Problem:[PleasePutYourQues�onsHere]
Op�ons:[PleasePutYourOp�onsHere]
Answer:Let'sthinkstepbystep.
Figure15:Anexampleofpromptingforone-shotStrategicChain-of-ThoughtYou are tasked with solving a reasoning problem by first iden�fying
the most effec�ve strategy before arriving at the final answer.
Carefully consider the problem and generate the strategic knowledge
that would best guide the problem-solving process.
Problem: [Please Put Your Problem Here]
Auto Zero-shot SCoT
template
Next, use the generated strategic knowledge to work through the
problem step by step, showing all necessary reasoning, and arrive at
the final solu�on.
Op�ons: [Please Put Your Op�ons Here]
Answer: Let's think step by step.
Figure16:AnexampleofpromptingforautomaticStrategicChain-of-Thought