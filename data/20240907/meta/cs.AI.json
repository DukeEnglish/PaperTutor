[
    {
        "title": "Lexicon3D: Probing Visual Foundation Models for Complex 3D Scene Understanding",
        "authors": "Yunze ManShuhong ZhengZhipeng BaoMartial HebertLiang-Yan GuiYu-Xiong Wang",
        "links": "http://arxiv.org/abs/2409.03757v1",
        "entry_id": "http://arxiv.org/abs/2409.03757v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03757v1",
        "summary": "Complex 3D scene understanding has gained increasing attention, with scene\nencoding strategies playing a crucial role in this success. However, the\noptimal scene encoding strategies for various scenarios remain unclear,\nparticularly compared to their image-based counterparts. To address this issue,\nwe present a comprehensive study that probes various visual encoding models for\n3D scene understanding, identifying the strengths and limitations of each model\nacross different scenarios. Our evaluation spans seven vision foundation\nencoders, including image-based, video-based, and 3D foundation models. We\nevaluate these models in four tasks: Vision-Language Scene Reasoning, Visual\nGrounding, Segmentation, and Registration, each focusing on different aspects\nof scene understanding. Our evaluations yield key findings: DINOv2 demonstrates\nsuperior performance, video models excel in object-level tasks, diffusion\nmodels benefit geometric tasks, and language-pretrained models show unexpected\nlimitations in language-related tasks. These insights challenge some\nconventional understandings, provide novel perspectives on leveraging visual\nfoundation models, and highlight the need for more flexible encoder selection\nin future vision-language and scene-understanding tasks.",
        "updated": "2024-09-05 17:59:56 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03757v1"
    },
    {
        "title": "WildVis: Open Source Visualizer for Million-Scale Chat Logs in the Wild",
        "authors": "Yuntian DengWenting ZhaoJack HesselXiang RenClaire CardieYejin Choi",
        "links": "http://arxiv.org/abs/2409.03753v1",
        "entry_id": "http://arxiv.org/abs/2409.03753v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03753v1",
        "summary": "The increasing availability of real-world conversation data offers exciting\nopportunities for researchers to study user-chatbot interactions. However, the\nsheer volume of this data makes manually examining individual conversations\nimpractical. To overcome this challenge, we introduce WildVis, an interactive\ntool that enables fast, versatile, and large-scale conversation analysis.\nWildVis provides search and visualization capabilities in the text and\nembedding spaces based on a list of criteria. To manage million-scale datasets,\nwe implemented optimizations including search index construction, embedding\nprecomputation and compression, and caching to ensure responsive user\ninteractions within seconds. We demonstrate WildVis's utility through three\ncase studies: facilitating chatbot misuse research, visualizing and comparing\ntopic distributions across datasets, and characterizing user-specific\nconversation patterns. WildVis is open-source and designed to be extendable,\nsupporting additional datasets and customized search and visualization\nfunctionalities.",
        "updated": "2024-09-05 17:59:15 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03753v1"
    },
    {
        "title": "LLM-CI: Assessing Contextual Integrity Norms in Language Models",
        "authors": "Yan ShvartzshnaiderVasisht DudduJohn Lacalamita",
        "links": "http://arxiv.org/abs/2409.03735v1",
        "entry_id": "http://arxiv.org/abs/2409.03735v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03735v1",
        "summary": "Large language models (LLMs), while memorizing parts of their training data\nscraped from the Internet, may also inadvertently encode societal preferences\nand norms. As these models are integrated into sociotechnical systems, it is\ncrucial that the norms they encode align with societal expectations. These\nnorms could vary across models, hyperparameters, optimization techniques, and\ndatasets. This is especially challenging due to prompt sensitivity$-$small\nvariations in prompts yield different responses, rendering existing assessment\nmethodologies unreliable. There is a need for a comprehensive framework\ncovering various models, optimization, and datasets, along with a reliable\nmethodology to assess encoded norms.\n  We present LLM-CI, the first open-sourced framework to assess privacy norms\nencoded in LLMs. LLM-CI uses a Contextual Integrity-based factorial vignette\nmethodology to assess the encoded norms across different contexts and LLMs. We\npropose the multi-prompt assessment methodology to address prompt sensitivity\nby assessing the norms from only the prompts that yield consistent responses\nacross multiple variants. Using LLM-CI and our proposed methodology, we\ncomprehensively evaluate LLMs using IoT and COPPA vignettes datasets from prior\nwork, examining the impact of model properties (e.g., hyperparameters,\ncapacity) and optimization strategies (e.g., alignment, quantization).",
        "updated": "2024-09-05 17:50:31 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03735v1"
    },
    {
        "title": "Planning In Natural Language Improves LLM Search For Code Generation",
        "authors": "Evan WangFederico CassanoCatherine WuYunfeng BaiWill SongVaskar NathZiwen HanSean HendryxSummer YueHugh Zhang",
        "links": "http://arxiv.org/abs/2409.03733v1",
        "entry_id": "http://arxiv.org/abs/2409.03733v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03733v1",
        "summary": "While scaling training compute has led to remarkable improvements in large\nlanguage models (LLMs), scaling inference compute has not yet yielded analogous\ngains. We hypothesize that a core missing component is a lack of diverse LLM\noutputs, leading to inefficient search due to models repeatedly sampling highly\nsimilar, yet incorrect generations. We empirically demonstrate that this lack\nof diversity can be mitigated by searching over candidate plans for solving a\nproblem in natural language. Based on this insight, we propose PLANSEARCH, a\nnovel search algorithm which shows strong results across HumanEval+, MBPP+, and\nLiveCodeBench (a contamination-free benchmark for competitive coding).\nPLANSEARCH generates a diverse set of observations about the problem and then\nuses these observations to construct plans for solving the problem. By\nsearching over plans in natural language rather than directly over code\nsolutions, PLANSEARCH explores a significantly more diverse range of potential\nsolutions compared to baseline search methods. Using PLANSEARCH on top of\nClaude 3.5 Sonnet achieves a state-of-the-art pass@200 of 77.0% on\nLiveCodeBench, outperforming both the best score achieved without search\n(pass@1 = 41.4%) and using standard repeated sampling (pass@200 = 60.6%).\nFinally, we show that, across all models, search algorithms, and benchmarks\nanalyzed, we can accurately predict performance gains due to search as a direct\nfunction of the diversity over generated ideas.",
        "updated": "2024-09-05 17:44:49 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03733v1"
    },
    {
        "title": "A Different Level Text Protection Mechanism With Differential Privacy",
        "authors": "Qingwen Fu",
        "links": "http://arxiv.org/abs/2409.03707v1",
        "entry_id": "http://arxiv.org/abs/2409.03707v1",
        "pdf_url": "http://arxiv.org/pdf/2409.03707v1",
        "summary": "The article introduces a method for extracting words of different degrees of\nimportance based on the BERT pre-training model and proves the effectiveness of\nthis method. The article also discusses the impact of maintaining the same\nperturbation results for words of different importance on the overall text\nutility. This method can be applied to long text protection.",
        "updated": "2024-09-05 17:13:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.03707v1"
    }
]