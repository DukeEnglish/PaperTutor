[
    {
        "title": "INN-PAR: Invertible Neural Network for PPG to ABP Reconstruction",
        "authors": "Soumitra KunduGargi PandaSaumik BhattacharyaAurobinda RoutrayRajlakshmi Guha",
        "links": "http://arxiv.org/abs/2409.09021v1",
        "entry_id": "http://arxiv.org/abs/2409.09021v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09021v1",
        "summary": "Non-invasive and continuous blood pressure (BP) monitoring is essential for\nthe early prevention of many cardiovascular diseases. Estimating arterial blood\npressure (ABP) from photoplethysmography (PPG) has emerged as a promising\nsolution. However, existing deep learning approaches for PPG-to-ABP\nreconstruction (PAR) encounter certain information loss, impacting the\nprecision of the reconstructed signal. To overcome this limitation, we\nintroduce an invertible neural network for PPG to ABP reconstruction (INN-PAR),\nwhich employs a series of invertible blocks to jointly learn the mapping\nbetween PPG and its gradient with the ABP signal and its gradient. INN-PAR\nefficiently captures both forward and inverse mappings simultaneously, thereby\npreventing information loss. By integrating signal gradients into the learning\nprocess, INN-PAR enhances the network's ability to capture essential\nhigh-frequency details, leading to more accurate signal reconstruction.\nMoreover, we propose a multi-scale convolution module (MSCM) within the\ninvertible block, enabling the model to learn features across multiple scales\neffectively. We have experimented on two benchmark datasets, which show that\nINN-PAR significantly outperforms the state-of-the-art methods in both waveform\nreconstruction and BP measurement accuracy.",
        "updated": "2024-09-13 17:48:48 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09021v1"
    },
    {
        "title": "An Efficient and Streaming Audio Visual Active Speaker Detection System",
        "authors": "Arnav KunduYanzi JinMohammad SekhavatMax HortonDanny TormoenDevang Naik",
        "links": "http://arxiv.org/abs/2409.09018v1",
        "entry_id": "http://arxiv.org/abs/2409.09018v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09018v1",
        "summary": "This paper delves into the challenging task of Active Speaker Detection\n(ASD), where the system needs to determine in real-time whether a person is\nspeaking or not in a series of video frames. While previous works have made\nsignificant strides in improving network architectures and learning effective\nrepresentations for ASD, a critical gap exists in the exploration of real-time\nsystem deployment. Existing models often suffer from high latency and memory\nusage, rendering them impractical for immediate applications. To bridge this\ngap, we present two scenarios that address the key challenges posed by\nreal-time constraints. First, we introduce a method to limit the number of\nfuture context frames utilized by the ASD model. By doing so, we alleviate the\nneed for processing the entire sequence of future frames before a decision is\nmade, significantly reducing latency. Second, we propose a more stringent\nconstraint that limits the total number of past frames the model can access\nduring inference. This tackles the persistent memory issues associated with\nrunning streaming ASD systems. Beyond these theoretical frameworks, we conduct\nextensive experiments to validate our approach. Our results demonstrate that\nconstrained transformer models can achieve performance comparable to or even\nbetter than state-of-the-art recurrent models, such as uni-directional GRUs,\nwith a significantly reduced number of context frames. Moreover, we shed light\non the temporal memory requirements of ASD systems, revealing that larger past\ncontext has a more profound impact on accuracy than future context. When\nprofiling on a CPU we find that our efficient architecture is memory bound by\nthe amount of past context it can use and that the compute cost is negligible\nas compared to the memory cost.",
        "updated": "2024-09-13 17:45:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09018v1"
    },
    {
        "title": "VAE Explainer: Supplement Learning Variational Autoencoders with Interactive Visualization",
        "authors": "Donald BertucciAlex Endert",
        "links": "http://arxiv.org/abs/2409.09011v1",
        "entry_id": "http://arxiv.org/abs/2409.09011v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09011v1",
        "summary": "Variational Autoencoders are widespread in Machine Learning, but are\ntypically explained with dense math notation or static code examples. This\npaper presents VAE Explainer, an interactive Variational Autoencoder running in\nthe browser to supplement existing static documentation (e.g., Keras Code\nExamples). VAE Explainer adds interactions to the VAE summary with interactive\nmodel inputs, latent space, and output. VAE Explainer connects the high-level\nunderstanding with the implementation: annotated code and a live computational\ngraph. The VAE Explainer interactive visualization is live at\nhttps://xnought.github.io/vae-explainer and the code is open source at\nhttps://github.com/xnought/vae-explainer.",
        "updated": "2024-09-13 17:40:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09011v1"
    },
    {
        "title": "Predicting Trust In Autonomous Vehicles: Modeling Young Adult Psychosocial Traits, Risk-Benefit Attitudes, And Driving Factors With Machine Learning",
        "authors": "Robert KaufmanEmi LeeManas Satish BedmuthaDavid KirshNadir Weibel",
        "links": "http://arxiv.org/abs/2409.08980v1",
        "entry_id": "http://arxiv.org/abs/2409.08980v1",
        "pdf_url": "http://arxiv.org/pdf/2409.08980v1",
        "summary": "Low trust remains a significant barrier to Autonomous Vehicle (AV) adoption.\nTo design trustworthy AVs, we need to better understand the individual traits,\nattitudes, and experiences that impact people's trust judgements. We use\nmachine learning to understand the most important factors that contribute to\nyoung adult trust based on a comprehensive set of personal factors gathered via\nsurvey (n = 1457). Factors ranged from psychosocial and cognitive attributes to\ndriving style, experiences, and perceived AV risks and benefits. Using the\nexplainable AI technique SHAP, we found that perceptions of AV risks and\nbenefits, attitudes toward feasibility and usability, institutional trust,\nprior experience, and a person's mental model are the most important\npredictors. Surprisingly, psychosocial and many technology- and\ndriving-specific factors were not strong predictors. Results highlight the\nimportance of individual differences for designing trustworthy AVs for diverse\ngroups and lead to key implications for future design and research.",
        "updated": "2024-09-13 16:52:24 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.08980v1"
    },
    {
        "title": "Modeling Rational Adaptation of Visual Search to Hierarchical Structures",
        "authors": "Saku SourulahtiChristian P JanssenJussi PP Jokinen",
        "links": "http://arxiv.org/abs/2409.08967v1",
        "entry_id": "http://arxiv.org/abs/2409.08967v1",
        "pdf_url": "http://arxiv.org/pdf/2409.08967v1",
        "summary": "Efficient attention deployment in visual search is limited by human visual\nmemory, yet this limitation can be offset by exploiting the environment's\nstructure. This paper introduces a computational cognitive model that simulates\nhow the human visual system uses visual hierarchies to prevent refixations in\nsequential attention deployment. The model adopts computational rationality,\npositing behaviors as adaptations to cognitive constraints and environmental\nstructures. In contrast to earlier models that predict search performance for\nhierarchical information, our model does not include predefined assumptions\nabout particular search strategies. Instead, our model's search strategy\nemerges as a result of adapting to the environment through reinforcement\nlearning algorithms. In an experiment with human participants we test the\nmodel's prediction that structured environments reduce visual search times\ncompared to random tasks. Our model's predictions correspond well with human\nsearch performance across various set sizes for both structured and\nunstructured visual layouts. Our work improves understanding of the adaptive\nnature of visual search in hierarchically structured environments and informs\nthe design of optimized search spaces.",
        "updated": "2024-09-13 16:33:18 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.08967v1"
    }
]