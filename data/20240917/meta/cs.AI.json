[
    {
        "title": "The unknotting number, hard unknot diagrams, and reinforcement learning",
        "authors": "Taylor ApplebaumSam BlackwellAlex DaviesThomas EdlichAndrás JuhászMarc LackenbyNenad TomaševDaniel Zheng",
        "links": "http://arxiv.org/abs/2409.09032v1",
        "entry_id": "http://arxiv.org/abs/2409.09032v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09032v1",
        "summary": "We have developed a reinforcement learning agent that often finds a minimal\nsequence of unknotting crossing changes for a knot diagram with up to 200\ncrossings, hence giving an upper bound on the unknotting number. We have used\nthis to determine the unknotting number of 57k knots. We took diagrams of\nconnected sums of such knots with oppositely signed signatures, where the\nsummands were overlaid. The agent has found examples where several of the\ncrossing changes in an unknotting collection of crossings result in hyperbolic\nknots. Based on this, we have shown that, given knots $K$ and $K'$ that satisfy\nsome mild assumptions, there is a diagram of their connected sum and $u(K) +\nu(K')$ unknotting crossings such that changing any one of them results in a\nprime knot. As a by-product, we have obtained a dataset of 2.6 million distinct\nhard unknot diagrams; most of them under 35 crossings. Assuming the additivity\nof the unknotting number, we have determined the unknotting number of 43 at\nmost 12-crossing knots for which the unknotting number is unknown.",
        "updated": "2024-09-13 17:59:52 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09032v1"
    },
    {
        "title": "Agents in Software Engineering: Survey, Landscape, and Vision",
        "authors": "Yanxian HuangWanjun ZhongEnsheng ShiMin YangJiachi ChenHui LiYuchi MaQianxiang WangZibin ZhengYanlin Wang",
        "links": "http://arxiv.org/abs/2409.09030v1",
        "entry_id": "http://arxiv.org/abs/2409.09030v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09030v1",
        "summary": "In recent years, Large Language Models (LLMs) have achieved remarkable\nsuccess and have been widely used in various downstream tasks, especially in\nthe tasks of the software engineering (SE) field. We find that many studies\ncombining LLMs with SE have employed the concept of agents either explicitly or\nimplicitly. However, there is a lack of an in-depth survey to sort out the\ndevelopment context of existing works, analyze how existing works combine the\nLLM-based agent technologies to optimize various tasks, and clarify the\nframework of LLM-based agents in SE. In this paper, we conduct the first survey\nof the studies on combining LLM-based agents with SE and present a framework of\nLLM-based agents in SE which includes three key modules: perception, memory,\nand action. We also summarize the current challenges in combining the two\nfields and propose future opportunities in response to existing challenges. We\nmaintain a GitHub repository of the related papers at:\nhttps://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE.",
        "updated": "2024-09-13 17:55:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09030v1"
    },
    {
        "title": "Towards Leveraging Contrastively Pretrained Neural Audio Embeddings for Recommender Tasks",
        "authors": "Florian GrötschlaLuca SträssleLuca A. LanzendörferRoger Wattenhofer",
        "links": "http://arxiv.org/abs/2409.09026v1",
        "entry_id": "http://arxiv.org/abs/2409.09026v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09026v1",
        "summary": "Music recommender systems frequently utilize network-based models to capture\nrelationships between music pieces, artists, and users. Although these\nrelationships provide valuable insights for predictions, new music pieces or\nartists often face the cold-start problem due to insufficient initial\ninformation. To address this, one can extract content-based information\ndirectly from the music to enhance collaborative-filtering-based methods. While\nprevious approaches have relied on hand-crafted audio features for this\npurpose, we explore the use of contrastively pretrained neural audio embedding\nmodels, which offer a richer and more nuanced representation of music. Our\nexperiments demonstrate that neural embeddings, particularly those generated\nwith the Contrastive Language-Audio Pretraining (CLAP) model, present a\npromising approach to enhancing music recommendation tasks within graph-based\nframeworks.",
        "updated": "2024-09-13 17:53:06 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09026v1"
    },
    {
        "title": "AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents",
        "authors": "Zhe SuXuhui ZhouSanketh RangrejiAnubha KabraJulia MendelsohnFaeze BrahmanMaarten Sap",
        "links": "http://arxiv.org/abs/2409.09013v1",
        "entry_id": "http://arxiv.org/abs/2409.09013v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09013v1",
        "summary": "To be safely and successfully deployed, LLMs must simultaneously satisfy\ntruthfulness and utility goals. Yet, often these two goals compete (e.g., an AI\nagent assisting a used car salesman selling a car with flaws), partly due to\nambiguous or misleading user instructions. We propose AI-LieDar, a framework to\nstudy how LLM-based agents navigate scenarios with utility-truthfulness\nconflicts in a multi-turn interactive setting. We design a set of realistic\nscenarios where language agents are instructed to achieve goals that are in\nconflict with being truthful during a multi-turn conversation with simulated\nhuman agents. To evaluate the truthfulness at large scale, we develop a\ntruthfulness detector inspired by psychological literature to assess the\nagents' responses. Our experiment demonstrates that all models are truthful\nless than 50% of the time, although truthfulness and goal achievement (utility)\nrates vary across models. We further test the steerability of LLMs towards\ntruthfulness, finding that models follow malicious instructions to deceive, and\neven truth-steered models can still lie. These findings reveal the complex\nnature of truthfulness in LLMs and underscore the importance of further\nresearch to ensure the safe and reliable deployment of LLMs and AI agents.",
        "updated": "2024-09-13 17:41:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09013v1"
    },
    {
        "title": "VAE Explainer: Supplement Learning Variational Autoencoders with Interactive Visualization",
        "authors": "Donald BertucciAlex Endert",
        "links": "http://arxiv.org/abs/2409.09011v1",
        "entry_id": "http://arxiv.org/abs/2409.09011v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09011v1",
        "summary": "Variational Autoencoders are widespread in Machine Learning, but are\ntypically explained with dense math notation or static code examples. This\npaper presents VAE Explainer, an interactive Variational Autoencoder running in\nthe browser to supplement existing static documentation (e.g., Keras Code\nExamples). VAE Explainer adds interactions to the VAE summary with interactive\nmodel inputs, latent space, and output. VAE Explainer connects the high-level\nunderstanding with the implementation: annotated code and a live computational\ngraph. The VAE Explainer interactive visualization is live at\nhttps://xnought.github.io/vae-explainer and the code is open source at\nhttps://github.com/xnought/vae-explainer.",
        "updated": "2024-09-13 17:40:01 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09011v1"
    }
]