[
    {
        "title": "An Efficient and Streaming Audio Visual Active Speaker Detection System",
        "authors": "Arnav KunduYanzi JinMohammad SekhavatMax HortonDanny TormoenDevang Naik",
        "links": "http://arxiv.org/abs/2409.09018v1",
        "entry_id": "http://arxiv.org/abs/2409.09018v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09018v1",
        "summary": "This paper delves into the challenging task of Active Speaker Detection\n(ASD), where the system needs to determine in real-time whether a person is\nspeaking or not in a series of video frames. While previous works have made\nsignificant strides in improving network architectures and learning effective\nrepresentations for ASD, a critical gap exists in the exploration of real-time\nsystem deployment. Existing models often suffer from high latency and memory\nusage, rendering them impractical for immediate applications. To bridge this\ngap, we present two scenarios that address the key challenges posed by\nreal-time constraints. First, we introduce a method to limit the number of\nfuture context frames utilized by the ASD model. By doing so, we alleviate the\nneed for processing the entire sequence of future frames before a decision is\nmade, significantly reducing latency. Second, we propose a more stringent\nconstraint that limits the total number of past frames the model can access\nduring inference. This tackles the persistent memory issues associated with\nrunning streaming ASD systems. Beyond these theoretical frameworks, we conduct\nextensive experiments to validate our approach. Our results demonstrate that\nconstrained transformer models can achieve performance comparable to or even\nbetter than state-of-the-art recurrent models, such as uni-directional GRUs,\nwith a significantly reduced number of context frames. Moreover, we shed light\non the temporal memory requirements of ASD systems, revealing that larger past\ncontext has a more profound impact on accuracy than future context. When\nprofiling on a CPU we find that our efficient architecture is memory bound by\nthe amount of past context it can use and that the compute cost is negligible\nas compared to the memory cost.",
        "updated": "2024-09-13 17:45:53 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09018v1"
    },
    {
        "title": "Pushing the boundaries of event subsampling in event-based video classification using CNNs",
        "authors": "Hesam AraghiJan van GemertNergis Tomen",
        "links": "http://arxiv.org/abs/2409.08953v1",
        "entry_id": "http://arxiv.org/abs/2409.08953v1",
        "pdf_url": "http://arxiv.org/pdf/2409.08953v1",
        "summary": "Event cameras offer low-power visual sensing capabilities ideal for\nedge-device applications. However, their high event rate, driven by high\ntemporal details, can be restrictive in terms of bandwidth and computational\nresources. In edge AI applications, determining the minimum amount of events\nfor specific tasks can allow reducing the event rate to improve bandwidth,\nmemory, and processing efficiency. In this paper, we study the effect of event\nsubsampling on the accuracy of event data classification using convolutional\nneural network (CNN) models. Surprisingly, across various datasets, the number\nof events per video can be reduced by an order of magnitude with little drop in\naccuracy, revealing the extent to which we can push the boundaries in accuracy\nvs. event rate trade-off. Additionally, we also find that lower classification\naccuracy in high subsampling rates is not solely attributable to information\nloss due to the subsampling of the events, but that the training of CNNs can be\nchallenging in highly subsampled scenarios, where the sensitivity to\nhyperparameters increases. We quantify training instability across multiple\nevent-based classification datasets using a novel metric for evaluating the\nhyperparameter sensitivity of CNNs in different subsampling settings. Finally,\nwe analyze the weight gradients of the network to gain insight into this\ninstability.",
        "updated": "2024-09-13 16:14:45 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.08953v1"
    },
    {
        "title": "A Diffusion Approach to Radiance Field Relighting using Multi-Illumination Synthesis",
        "authors": "Yohan Poirier-GinterAlban GauthierJulien PhillipJean-Francois LalondeGeorge Drettakis",
        "links": "http://dx.doi.org/10.1111/cgf.15147",
        "entry_id": "http://arxiv.org/abs/2409.08947v1",
        "pdf_url": "http://arxiv.org/pdf/2409.08947v1",
        "summary": "Relighting radiance fields is severely underconstrained for multi-view data,\nwhich is most often captured under a single illumination condition; It is\nespecially hard for full scenes containing multiple objects. We introduce a\nmethod to create relightable radiance fields using such single-illumination\ndata by exploiting priors extracted from 2D image diffusion models. We first\nfine-tune a 2D diffusion model on a multi-illumination dataset conditioned by\nlight direction, allowing us to augment a single-illumination capture into a\nrealistic -- but possibly inconsistent -- multi-illumination dataset from\ndirectly defined light directions. We use this augmented data to create a\nrelightable radiance field represented by 3D Gaussian splats. To allow direct\ncontrol of light direction for low-frequency lighting, we represent appearance\nwith a multi-layer perceptron parameterized on light direction. To enforce\nmulti-view consistency and overcome inaccuracies we optimize a per-image\nauxiliary feature vector. We show results on synthetic and real multi-view data\nunder single illumination, demonstrating that our method successfully exploits\n2D diffusion model priors to allow realistic 3D relighting for complete scenes.\nProject site\nhttps://repo-sam.inria.fr/fungraph/generative-radiance-field-relighting/",
        "updated": "2024-09-13 16:07:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.08947v1"
    },
    {
        "title": "Pushing Joint Image Denoising and Classification to the Edge",
        "authors": "Thomas C MarkhorstJan C van GemertOsman S Kayhan",
        "links": "http://arxiv.org/abs/2409.08943v1",
        "entry_id": "http://arxiv.org/abs/2409.08943v1",
        "pdf_url": "http://arxiv.org/pdf/2409.08943v1",
        "summary": "In this paper, we jointly combine image classification and image denoising,\naiming to enhance human perception of noisy images captured by edge devices,\nlike low-light security cameras. In such settings, it is important to retain\nthe ability of humans to verify the automatic classification decision and thus\njointly denoise the image to enhance human perception. Since edge devices have\nlittle computational power, we explicitly optimize for efficiency by proposing\na novel architecture that integrates the two tasks. Additionally, we alter a\nNeural Architecture Search (NAS) method, which searches for classifiers to\nsearch for the integrated model while optimizing for a target latency,\nclassification accuracy, and denoising performance. The NAS architectures\noutperform our manually designed alternatives in both denoising and\nclassification, offering a significant improvement to human perception. Our\napproach empowers users to construct architectures tailored to domains like\nmedical imaging, surveillance systems, and industrial inspections.",
        "updated": "2024-09-13 16:01:27 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.08943v1"
    },
    {
        "title": "ClearDepth: Enhanced Stereo Perception of Transparent Objects for Robotic Manipulation",
        "authors": "Kaixin BaiHuajian ZengLei ZhangYiwen LiuHongli XuZhaopeng ChenJianwei Zhang",
        "links": "http://arxiv.org/abs/2409.08926v1",
        "entry_id": "http://arxiv.org/abs/2409.08926v1",
        "pdf_url": "http://arxiv.org/pdf/2409.08926v1",
        "summary": "Transparent object depth perception poses a challenge in everyday life and\nlogistics, primarily due to the inability of standard 3D sensors to accurately\ncapture depth on transparent or reflective surfaces. This limitation\nsignificantly affects depth map and point cloud-reliant applications,\nespecially in robotic manipulation. We developed a vision transformer-based\nalgorithm for stereo depth recovery of transparent objects. This approach is\ncomplemented by an innovative feature post-fusion module, which enhances the\naccuracy of depth recovery by structural features in images. To address the\nhigh costs associated with dataset collection for stereo camera-based\nperception of transparent objects, our method incorporates a parameter-aligned,\ndomain-adaptive, and physically realistic Sim2Real simulation for efficient\ndata generation, accelerated by AI algorithm. Our experimental results\ndemonstrate the model's exceptional Sim2Real generalizability in real-world\nscenarios, enabling precise depth mapping of transparent objects to assist in\nrobotic manipulation. Project details are available at\nhttps://sites.google.com/view/cleardepth/ .",
        "updated": "2024-09-13 15:44:38 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.08926v1"
    }
]