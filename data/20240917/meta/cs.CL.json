[
    {
        "title": "Agents in Software Engineering: Survey, Landscape, and Vision",
        "authors": "Yanxian HuangWanjun ZhongEnsheng ShiMin YangJiachi ChenHui LiYuchi MaQianxiang WangZibin ZhengYanlin Wang",
        "links": "http://arxiv.org/abs/2409.09030v1",
        "entry_id": "http://arxiv.org/abs/2409.09030v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09030v1",
        "summary": "In recent years, Large Language Models (LLMs) have achieved remarkable\nsuccess and have been widely used in various downstream tasks, especially in\nthe tasks of the software engineering (SE) field. We find that many studies\ncombining LLMs with SE have employed the concept of agents either explicitly or\nimplicitly. However, there is a lack of an in-depth survey to sort out the\ndevelopment context of existing works, analyze how existing works combine the\nLLM-based agent technologies to optimize various tasks, and clarify the\nframework of LLM-based agents in SE. In this paper, we conduct the first survey\nof the studies on combining LLM-based agents with SE and present a framework of\nLLM-based agents in SE which includes three key modules: perception, memory,\nand action. We also summarize the current challenges in combining the two\nfields and propose future opportunities in response to existing challenges. We\nmaintain a GitHub repository of the related papers at:\nhttps://github.com/DeepSoftwareAnalytics/Awesome-Agent4SE.",
        "updated": "2024-09-13 17:55:58 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09030v1"
    },
    {
        "title": "AI-LieDar: Examine the Trade-off Between Utility and Truthfulness in LLM Agents",
        "authors": "Zhe SuXuhui ZhouSanketh RangrejiAnubha KabraJulia MendelsohnFaeze BrahmanMaarten Sap",
        "links": "http://arxiv.org/abs/2409.09013v1",
        "entry_id": "http://arxiv.org/abs/2409.09013v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09013v1",
        "summary": "To be safely and successfully deployed, LLMs must simultaneously satisfy\ntruthfulness and utility goals. Yet, often these two goals compete (e.g., an AI\nagent assisting a used car salesman selling a car with flaws), partly due to\nambiguous or misleading user instructions. We propose AI-LieDar, a framework to\nstudy how LLM-based agents navigate scenarios with utility-truthfulness\nconflicts in a multi-turn interactive setting. We design a set of realistic\nscenarios where language agents are instructed to achieve goals that are in\nconflict with being truthful during a multi-turn conversation with simulated\nhuman agents. To evaluate the truthfulness at large scale, we develop a\ntruthfulness detector inspired by psychological literature to assess the\nagents' responses. Our experiment demonstrates that all models are truthful\nless than 50% of the time, although truthfulness and goal achievement (utility)\nrates vary across models. We further test the steerability of LLMs towards\ntruthfulness, finding that models follow malicious instructions to deceive, and\neven truth-steered models can still lie. These findings reveal the complex\nnature of truthfulness in LLMs and underscore the importance of further\nresearch to ensure the safe and reliable deployment of LLMs and AI agents.",
        "updated": "2024-09-13 17:41:12 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09013v1"
    },
    {
        "title": "Optimizing Rare Word Accuracy in Direct Speech Translation with a Retrieval-and-Demonstration Approach",
        "authors": "Siqi LiDanni LiuJan Niehues",
        "links": "http://arxiv.org/abs/2409.09009v1",
        "entry_id": "http://arxiv.org/abs/2409.09009v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09009v1",
        "summary": "Direct speech translation (ST) models often struggle with rare words.\nIncorrect translation of these words can have severe consequences, impacting\ntranslation quality and user trust. While rare word translation is inherently\nchallenging for neural models due to sparse learning signals, real-world\nscenarios often allow access to translations of past recordings on similar\ntopics. To leverage these valuable resources, we propose a\nretrieval-and-demonstration approach to enhance rare word translation accuracy\nin direct ST models. First, we adapt existing ST models to incorporate\nretrieved examples for rare word translation, which allows the model to benefit\nfrom prepended examples, similar to in-context learning. We then develop a\ncross-modal (speech-to-speech, speech-to-text, text-to-text) retriever to\nlocate suitable examples. We demonstrate that standard ST models can be\neffectively adapted to leverage examples for rare word translation, improving\nrare word translation accuracy over the baseline by 17.6% with gold examples\nand 8.5% with retrieved examples. Moreover, our speech-to-speech retrieval\napproach outperforms other modalities and exhibits higher robustness to unseen\nspeakers. Our code is publicly available\n(https://github.com/SiqiLii/Retrieve-and-Demonstration-ST).",
        "updated": "2024-09-13 17:38:03 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09009v1"
    },
    {
        "title": "E2MoCase: A Dataset for Emotional, Event and Moral Observations in News Articles on High-impact Legal Cases",
        "authors": "Candida M. GrecoLorenzo ZangariDavide PiccaAndrea Tagarelli",
        "links": "http://arxiv.org/abs/2409.09001v1",
        "entry_id": "http://arxiv.org/abs/2409.09001v1",
        "pdf_url": "http://arxiv.org/pdf/2409.09001v1",
        "summary": "The way media reports on legal cases can significantly shape public opinion,\noften embedding subtle biases that influence societal views on justice and\nmorality. Analyzing these biases requires a holistic approach that captures the\nemotional tone, moral framing, and specific events within the narratives. In\nthis work we introduce E2MoCase, a novel dataset designed to facilitate the\nintegrated analysis of emotions, moral values, and events within legal\nnarratives and media coverage. By leveraging advanced models for emotion\ndetection, moral value identification, and event extraction, E2MoCase offers a\nmulti-dimensional perspective on how legal cases are portrayed in news\narticles.",
        "updated": "2024-09-13 17:31:09 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.09001v1"
    },
    {
        "title": "Safeguarding Decentralized Social Media: LLM Agents for Automating Community Rule Compliance",
        "authors": "Lucio La CavaAndrea Tagarelli",
        "links": "http://arxiv.org/abs/2409.08963v1",
        "entry_id": "http://arxiv.org/abs/2409.08963v1",
        "pdf_url": "http://arxiv.org/pdf/2409.08963v1",
        "summary": "Ensuring content compliance with community guidelines is crucial for\nmaintaining healthy online social environments. However, traditional\nhuman-based compliance checking struggles with scaling due to the increasing\nvolume of user-generated content and a limited number of moderators. Recent\nadvancements in Natural Language Understanding demonstrated by Large Language\nModels unlock new opportunities for automated content compliance verification.\nThis work evaluates six AI-agents built on Open-LLMs for automated rule\ncompliance checking in Decentralized Social Networks, a challenging environment\ndue to heterogeneous community scopes and rules. Analyzing over 50,000 posts\nfrom hundreds of Mastodon servers, we find that AI-agents effectively detect\nnon-compliant content, grasp linguistic subtleties, and adapt to diverse\ncommunity contexts. Most agents also show high inter-rater reliability and\nconsistency in score justification and suggestions for compliance. Human-based\nevaluation with domain experts confirmed the agents' reliability and\nusefulness, rendering them promising tools for semi-automated or\nhuman-in-the-loop content moderation systems.",
        "updated": "2024-09-13 16:29:25 UTC",
        "interpretation": "解释内容未找到",
        "id": "2409.08963v1"
    }
]