An Efficient and Streaming Audio Visual Active
Speaker Detection System
Arnav Kundu, Yanzi Jin, Mohammad Sekhavat, Maxwell Horton, Danny Tormoen, Devang Naik
{a kundu,yanzi jin,m sekhavat,mchorton,dtormoen,naik.d}@apple.com
Apple
Abstract—ThispaperdelvesintothechallengingtaskofActive CNNs and have some future context depending on the kernel
Speaker Detection (ASD), where the system needs to determine size of the temporal CNN layers. Such an architectural choice
in real-time whether a person is speaking or not in a series
introduces delay in the system because the encoders predic-
of video frames. While previous works have made significant
tions are not aligned on the time axis (ie. latest output doesn
stridesinimprovingnetworkarchitecturesandlearningeffective
representationsforASD,acriticalgapexistsintheexplorationof correspondtothelatestinputframe).Inourproposedsolution,
real-time system deployment. Existing models often suffer from we make the encoders independent of future context making
high latency and memory usage, rendering them impractical the output embeddings aligned in time.
for immediate applications. To bridge this gap, we present two
The modality encoder embeddings are then fed into a
scenarios that address the key challenges posed by real-time
combination step which can be as simple as concatenation
constraints. First, we introduce a method to limit the number
offuturecontextframesutilizedbytheASDmodel.Bydoingso, or additon in the feature space [4] or more complicated like
wealleviatetheneedforprocessingtheentiresequenceoffuture usingcross-attentionmechanism[1],[6],whichletsthevisual
frames before a decision is made, significantly reducing latency. streams attend to the audio streams at the same time.
Second, we propose a more stringent constraint that limits
After the two features are combined, self-attention [1], [2]
the total number of past frames the model can access during
or RNN-based [4] architectures are used to keep track of
inference. This tackles the persistent memory issues associated
with running streaming ASD systems. Beyond these theoretical the person who is speaking the most clearly throughout the
frameworks, we conduct extensive experiments to validate our whole conversation. These methods take advantage of the fact
approach.Ourresultsdemonstratethatconstrainedtransformer thataudioandvisualinformationoftenprovidecomplimentary
models can achieve performance comparable to or even better
signals. Additionally, there has been extensive recent work on
than state-of-the-art recurrent models, such as uni-directional
makingmulti-modalrepresentationsstrongerbylearninggood
GRUs, with a significantly reduced number of context frames.
Moreover, we shed light on the temporal memory requirements phonetic representations for the ASD task [7].
of ASD systems, revealing that larger past context has a more However, to get more accurate models all these techniques
profoundimpactonaccuracythanfuturecontext.Whenprofiling use long temporal context (often the entire video) to make
onaCPUwefindthatourefficientarchitectureismemorybound
predictions for every frame [1]–[5]. This introduces a latency
by the amount of past context it can use and that the compute
bottleneck since predictions for a given frame depend on
cost is negligible as compared to the memory cost.
Index Terms—Active Speaker Detection, Streaming System, all future frames. To this end, we propose a new model
AVA-Dataset, Efficient ML. architecture that allows the model to have limited future
context and the complete past context for a given frame to
I. INTRODUCTION
generatepredictions.Thisway,wecanmaintainafixedlatency
With the shift from in-person to audio-visual online in- without significantly compromising accuracy. We perform 2
teractions, identifying active speakers in conversations has architecturalchangestodothis:wemaketheaudioandvisual
become crucial for effective communication and understand- encodersindependentoffuturecontextandwemakethefusion
ing. In multi-modal conversations, Active Speaker Detection encoder constrained to future context during training.
(ASD) serves as a fundamental pre-processing module for Furthermore, to mitigate the high memory requirement
speech-related tasks, including audio-visual speech recogni- causedbyunlimitedpastcontext,wemakethefusionencoder
tion, speech separation, and speaker diarization. constrained to both past and future context. We then analyze
Working with ASD in real-world situations is challenging. theimpactofvariouspastandfuturecontextsontheaccuracy
Itrequiresleveragingaudioandvisualinformationandunder- of the model to arrive to a configuration (memory and latency
standinghowtheyrelateovertime.Tomeettheseneeds,ASD bounds) that is suitable for real-time applications while not
modelsaredesignedtobuildaudio-visualfeaturesandprocess compromising the accuracy significantly.
them over long times to catch important timing information.
Most ASD frameworks [1]–[5] start with separate encoders
II. RELATEDWORKS
that create embeddings for each modality and then use audio- ThissectionoutlinesthegeneralASDframeworkcommonly
visualfusiontechniquestobringthesedifferentrepresentations found in literature [1]–[5].
together so that they can be modeled jointly. The audio and Feature Extraction: The framework begins by processing
visual encoders in these methods mostly use spatio-temporal the input video frames. Each face in the video is detected,
4202
peS
31
]VC.sc[
1v81090.9042:viXraFig.1. a:VisualEncoderBlock(notethepaddingchangeinblue),b:Visual/AudioEncoder,c:Fullmodelarchitectureattraintime
cropped, stacked, and converted to normalized grayscale im- Inalltheabovemethods,thefusingconsistsofself-attention
ages, denoted as x . Correspondingly, the audio waveform is or bi-directional recurrent layers to model the temporal rela-
v
transformed into MFCC features, x . The extracted features tionship of features. This makes these models contextually
a
arethenfedintoseparateencoderstogenerateembeddingsfor constrained to the entire video clip, i.e. to predict a label
the audio and visual modalities as follows: at time step 0 the model needs the inputs from the end of
the video. This makes such models unsuitable for real-time
e =E (x )
a a a (1) applications.
e =E (x )
v v v
III. STREAMINGASD
Fusion and temporal modeling: The audio-visual fusion
module comes into play to integrate information between For real-time applications, the models used for active
audio and visual cues. Recent methods such as TalkNet [3] speaker detection need to use only a restricted future context
have introduced innovative approaches to this step. They for a good runtime latency. We propose to solve this problem
utilize cross-attention layers, enabling effective alignment of by enhancing the existing Light-ASD [4] architecture shown
the audio embedding with the visual information of the cor- in Figure 1. Each video and audio encoder has two branches
responding speaker. This leads to the generation of attention- with different kernel sizes; a fusion model is applied to
weighted features, f and f , which are then concatenated the concatenation of the audio/visual embedding. The future
a v
to form the fused audio-visual features f . This method context can be introduced in two parts: the encoder and
av
is inspired from cross modal supervision used for voice the fusion model. The audio and video encoders consist of
activity detection using video. Other techniques like [1], [2], 3-D CNN layers: spatial convolution S Conv and temporal
[5] leverage the information from multiple speakers in the convolution T Conv) [8]. Figure 1 represents the architecture
scene. Loconet [5] takes this inter-dependence further and of the visual encoder (the audio encoder is similar). Light-
leveragesLong-termIntra-speakerModeling(LIM)andShort- ASD [4] encoders use equal padding (1 and 2 for kernel size
term Inter-speaker Modeling (SIM) in an interleaved manner. of 3 and 5 in each branch respectively) for T Conv on both
This makes the model aware of multiple people in the scene sides making them dependent on future context. To ensure
and handles edge cases like occluded faces. However, such streaming capabilities we ensure that the encoders do not rely
architectures have very high parameter counts, therefore they on any future context and just the past context of a fixed
might not be suitable for real-time applications. To adhere to receptivefield.Thishasbeendonebymodifyingthetemporal
real-time applications, we study Light-ASD [4] which uses a padding during training as shown in blue in Figure 1. For the
simpleaudioandvisualbackbonefollowedbyabi-directional temporal convolutions (T Conv) of sizes 3 and 5, we left-pad
GRU for active speaker detection. This model is trained end the input by 2 and 4 respectively. In the audio encoder, the
to end on the final task along with an auxiliary loss on the spatial convolution is replaced by a 3×1×1 Conv3D layer.
visualencodertoidentifyspeakinglabelsusingjustthevisual Nextinthefusionmodel,weaddresstheproblemofrelying
information. on long future context in our first iteration by replacing theDuring training this uni-directional context is enforced by
introducing a self-attention mask as shown in Figure 2 to
ensure that the model cannot attend to any future inputs.
This helps to use the same training graph and dataset without
the need for writing custom code to split videos into clips
to enforce uni-directional context. This mask can also be
modifiedtoallowsomefuturecontextbyshiftingthediagonal
ofthislowertriangularmatrixasshowninFigure2.Theonly
problem with this approach would be using a huge KV-cache
[10]tostoretheembeddingsofthepastinputstocomputethe
attention score with the current embeddings.
Therefore,tomakethemodelmemoryefficientwecanalso
Fig.2. Constrainedmaskfortransformerencodertolimitthefuturecontext limit the number of past frames the model can attend to by
usedbythemodelforpredictingonelabel. modifyingthemaskabovetoanewmaskasshowninFigure3
and described in Equation (4). This mask ensures that for
predicting an output for time step t inputs from only t−T to
1
t+T are used, where T is the number of past frames and
2 1
T is the number of future frames.
2
Duringinference,wedonotneedthesemasksandthemodel
can be fed video and audio frames in a streaming fashion.
The audio and video encoders being fully convolutional can
processinputsframebyframe.Theoutputsfromtheencoders
can be then fed into the transformer layers which maintain
their own KV-cache [10] and use full self attention to predict
the output corresponding to the Tth frame for given input
frames from [X ,X ].
T−T1 T+T2
Fig.3. Constrainedmaskfortransformerencodertolimitthepastandfuture IV. EXPERIMENTS
contextusedbythemodelforpredictingonelabel.
A. Datasets
TheAVA-ActiveSpeakerdataset[11]isthego-tolarge-scale
bi-directional GRU in the Light-ASD architecture with a uni-
standard benchmark for active speaker detection. It features
directional GRU layer. As illustrated later in the experiments
262 Hollywood movies, 120 for training, 33 for validation,
section, we found the accuracy of these models to be signif-
and 109 for testing (though the test set is withheld for the
icantly worse than the baseline model. Given the success of
ActivityNet challenge). With over 5.3 million labeled face
transformers in replacing RNNs we replace the GRU layers
detection, speaking or nonspeaking, it contains many chal-
fromLight-ASD[4]withTransformerLayers.Intransformers
lenging scenarios like occlusions, low-resolution faces, low-
[9], the output for a given time T is a function of inputs from
quality audio, and tricky lighting. As is standard practice, we
all timestamps. This relationship between input and output is
evaluatemodelsonthevalidationsetofAVA-ActiveSpeakerin
established in the self-attention layer as illustrated in Equa-
ourexperiments.Duringtrainingweaugmentthevideoframes
tion (2), where Q represents the query vector corresponding
T by one rotation(-15 to 15 degrees), flipping or cropping (0 to
totimeT andK ,V representthekeyandvaluevectorsused
t t 30%) augmentations. Once an augmentation is selected it is
in self-attention from times 0≤t≤T. Tˆ represents the total
applied to the entire video. For augmenting the audio frames
time duration of the video.
weoverlapthesourceaudiowithrandomaudiosampledfrom
y =ΣTˆ softmax(Q ∗K )∗V (2) the training data with SNR ratio of -5dB to 5dB.
T t=0 T t t
Tomakesurethatfuturecontextisnotusedinthecomputation B. Implementation details
of attention during training, we can modify Equation (2) to The final architecture is built in PyTorch and trained on
Equation (3). eight NVIDIA A100 GPUs (40GB). They are trained using
y =ΣT softmax(Q ∗K )∗V the Adam optimizer [12] for 60 epochs with a weight decay
T t=0 T t t
of 5e − 4. The learning rate for the transformer model is
=ΣTˆ softmax(Q ∗K ∗M)∗V , (3)
t=0 T t t set to 0.003, which is then adjusted dynamically using a
where M =1 {t≤T}. cosine learning rate scheduler [13] throughout training which
is generally a standard for most transformer models. For the
y =Q ∗ΣT−T2 K ∗V
T T t=T−T1 t t GRU-based models, we use the same setting as the baseline
=Q ∗ΣTˆ softmax(Q ∗K ∗M)∗V , (4) Light-ASD setting ie. 0.001 learning rate with 5% decay per
T t=0 T t t
where M =1 . epoch trained on a single GPU. We train our models from
{T−T1≤t≤T−T2}Model Encoder Encoder Fusion Fusion mAP(%) Latency Memory
Past(#frames) Future(#frames) Past(#frames) Future(#frames) (ms)
Light-ASD[4] 6 6 ∞ ∞ 94.06 ∞ ∞
Uni-directionalGRU[4] 6 6 ∞ 0 92.6 240 512KB
Bi-directionalGRU 12 0 ∞ ∞ 93.5 ∞ ∞
Uni-directionalGRU 12 0 ∞ 0 91.9 0 512KB
∞ ∞ 93.93 ∞ ∞
∞ 0 92.95 0 ∞
1 1 91.13 40 512KB
Transformer-Encoder 12 0 3 3 92.65 120 1.5MB
6 12 92.93 480 3MB
12 6 93.2 240 6MB
32 8 93.8 320 16MB
TABLEI
ACCURACY,LATENCYANDMEMORYFORCOMBINATIONOFENCODERSANDFUSIONMODELSWITHDIFFERENTPASTANDFUTURECONTEXTS.
scratch on the AVA-ActiveSpeaker training dataset. Following
the use of multi-task loss for AV-ASD [4], [14] we use an
auxiliary classification head from the video encoder to apply
an auxiliary classification loss in addition to the classification
loss from the fusion head.
C. Evaluation criteria and metrics
Following the common protocol suggested by previous
works [1], [3]–[5], mean Average Precision (mAP) is used
asanevaluationmetricfortheAVA-ActiveSpeakervalidation
set [11].
Fig.4. Latency(futurecontextframes)vsmemory(pastcontextframes)trade-
We divide our experiments into 2 parts: Encoders with and
off on accuracy (mAP%). In this contour plot, the color of each data point
without future context, corresponding to the padding change (X,Y)indicatesthemAPvaluecorrespondingto(memory=X,latency=Y).The
in Figure 1. We have used Light-ASD [4] as our baseline mAPremainsconstantalongeachlevelcurve,withtheslopeofthesecurves
indifferentregionsrevealingwhichvariableexertsgreaterinfluence.Inareas
modelwhichusesabi-directionalGRUasitstemporalmodel.
where level curves are horizontal, future context primarily affects accuracy.
As reported in the Light-ASD experiments the mAP for uni- Conversely,verticallevelcurvessignifythedominantimpactofpastcontext.
directionalGRUissignificantlyworsethanthatofthebaseline
total runtime of our model is 32ms per frame with a given
model [4] with 240ms latency and minimal memory cost,
context of 32 past frames and 8 future frames. This makes
shown as the top group in Table I. Note that these models
the realtime latency of our model 352ms including the wait
use encoders with future context.
time for future frames. We found that compute time for self
Next, we train these models with encoders with no future
attention is just 5ms which allows us to avoid KV-caching
context and find that the accuracy drops significantly. We
andrecomputeallK,Vforallframes.Therefore,wejuststore
hypothesize that transformers would be better suited as fusion
intermediateembeddingsfromtheindividualencodersmaking
models with such encoders and we validate it by training
the total memory requirement 16MB (512KB per frame).
them as shown in Table I. This acts as a good baseline for
our further experiments with constrained transformers. Our V. CONCLUSION
resultsinTableIindicatethataconstrainedtransformermodel
We built a streaming and resource-constrained model us-
outperforms the uni-directional GRU with 3 past and 3 future
ing transformers for audio-visual active speaker detection to
frames as context. Furthermore, we have shown that with 32
bring the gap to real-time application. We demonstrated that
pastand8futureframeswhichtranslatesto16MBofstorage
transformers are better suited as fusion models to capture
and 320 ms of latency in terms of wait time.
temporal features. It has nearly state-of-the-art accuracy with
InFigure4,weillustratetheimpactoffuturecontextonthe
a significantly lower latency and memory usage. We’ve done
accuracy of our latency-constrained models. It is visible that
a comprehensive ablation study on the hyper-parameters that
beyond 6 future context frames the improvement in accuracy
affect latency and memory usage in real-time. It showed
with more future context is marginal (color of the dots donot
including large future context and thereby increasing the
changeforafixedpastcontext).Wealsoinvestigatetheimpact
latency of the model has very marginal gains in accuracy.
of past context on the accuracy of the model as illustrated
Meanwhile having a large past context has more impact on
in Figure 4. It can be observed from the contour plot that
the accuracy of the model than future context. Therefore, for
past context has more impact on accuracy and only beyond a
streaming applications, it is important to have high memory
certainlimitfuturecontextactuallystartsplayingaroleinthe
availability for running ASD.
accuracy of the model.
We also profiled the execution time of our model on a
Intel(R) Xeon(R) Platinum 8275CL CPU and found that theREFERENCES
[1] Juan Leo´n Alca´zar, Fabian Caba, Long Mai, Federico Perazzi, Joon-
Young Lee, Pablo Arbela´ez, and Bernard Ghanem, “Active speakers
in context,” in Proceedings of the IEEE/CVF conference on computer
visionandpatternrecognition,2020,pp.12465–12474.
[2] Juan Leo´n Alca´zar, Moritz Cordes, Chen Zhao, and Bernard Ghanem,
“End-to-end active speaker detection,” in European Conference on
ComputerVision.Springer,2022,pp.126–143.
[3] Ruijie Tao, Zexu Pan, Rohan Kumar Das, Xinyuan Qian, Mike Zheng
Shou, and Haizhou Li, “Is someone speaking? exploring long-term
temporalfeaturesforaudio-visualactivespeakerdetection,”inProceed-
ingsofthe29thACMinternationalconferenceonmultimedia,2021,pp.
3927–3935.
[4] Junhua Liao, Haihan Duan, Kanghui Feng, Wanbing Zhao, Yanbing
Yang, and Liangyin Chen, “A light weight model for active speaker
detection,” in Proceedings of the IEEE/CVF Conference on Computer
VisionandPatternRecognition,2023,pp.22932–22941.
[5] Xizi Wang, Feng Cheng, and Gedas Bertasius, “Loconet: Long-short
context network for active speaker detection,” in Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2024,pp.18462–18472.
[6] PunarjayChakravartyandTinneTuytelaars, “Cross-modalsupervision
for learning active speaker detection in video,” in Computer Vision–
ECCV2016:14thEuropeanConference,Amsterdam,TheNetherlands,
October11-14,2016,Proceedings,PartV14.Springer,2016,pp.285–
301.
[7] Chaeyoung Jung, Suyeon Lee, Kihyun Nam, Kyeongha Rho, You Jin
Kim,YoungjoonJang,andJoonSonChung,“Talknce:Improvingactive
speakerdetectionwithtalk-awarecontrastivelearning,”inICASSP2024-
2024 IEEE International Conference on Acoustics, Speech and Signal
Processing(ICASSP).IEEE,2024,pp.8391–8395.
[8] DuTran,HengWang,LorenzoTorresani,JamieRay,YannLeCun,and
ManoharPaluri,“Acloserlookatspatiotemporalconvolutionsforaction
recognition,”inProceedingsoftheIEEEconferenceonComputerVision
andPatternRecognition,2018,pp.6450–6459.
[9] AVaswani,“Attentionisallyouneed,”AdvancesinNeuralInformation
ProcessingSystems,2017.
[10] Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin,
James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and
Jeff Dean, “Efficiently scaling transformer inference,” Proceedings of
MachineLearningandSystems,vol.5,pp.606–624,2023.
[11] JosephRoth,SourishChaudhuri,OndrejKlejch,RadhikaMarvin,An-
drewGallagher,LiatKaver,SharadhRamaswamy,ArkadiuszStopczyn-
ski, Cordelia Schmid, Zhonghua Xi, et al., “Ava active speaker: An
audio-visual dataset for active speaker detection,” in ICASSP 2020-
2020 IEEE International Conference on Acoustics, Speech and Signal
Processing(ICASSP).IEEE,2020,pp.4492–4496.
[12] Diederik P Kingma, “Adam: A method for stochastic optimization,”
arXivpreprintarXiv:1412.6980,2014.
[13] Ilya Loshchilov and Frank Hutter, “Sgdr: Stochastic gradient descent
withwarmrestarts,” arXivpreprintarXiv:1608.03983,2016.
[14] Otavio Braga and Olivier Siohan, “Best of both worlds: Multi-task
audio-visualautomaticspeechrecognitionandactivespeakerdetection,”
in ICASSP 2022-2022 IEEE International Conference on Acoustics,
SpeechandSignalProcessing(ICASSP).IEEE,2022,pp.6047–6051.