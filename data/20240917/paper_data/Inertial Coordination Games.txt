Inertial Coordination Games
Andrew Koh∗ Ricky Li† Kei Uzui‡
MIT MIT MIT
This version: September 13, 2024
Abstract
Weanalyzeinertialcoordinationgames: dynamiccoordinationgameswith
anendogenouslychangingstatethatdependson(i)apersistentfundamental
thatplayersprivatelylearnabout; and(ii)pastplay. Wegiveatightcharac-
terization of how the speed of learning shapes equilibrium dynamics: the
risk-dominant action is selected in the limit if and only if learning is slow
such that posterior precisions grow sub-quadratically. This generalizes re-
sults from static global games and endows them with an alternate learning
foundation. Conversely,whenlearningisfast,equilibriumdynamicsexhibit
persistenceandlimitplayisshapedbyinitialplay. Whenevertheriskdomi-
nantequilibriumisselected,thepathofplayundergoesasuddentransition
whensignalsareprecise,andagradualtransitionwhensignalsarenoisy.
∗ MITDepartmentofEconomics;email: ajkoh@mit.edu
†MITDepartmentofEconomics;email: rickyli@mit.edu
‡MITDepartmentofEconomics;email: kuzui@mit.edu
This paper was previously circulated under the title “Contagion vs Learning Characterizes
Risk Dominance”. We are especially grateful to Drew Fudenberg and Stephen Morris for
insightful discussions and suggestions. We also thank Isaiah Andrews, Roberto Corrao, Tuval
Danenberg, Daniel Luo, Parag Pathak, Alex Wolitzky, Muhamet Yildiz, and participants at MIT
TheoryLunchforexcellentcomments.
4202
peS
21
]HT.noce[
1v54180.9042:viXra1 Introduction
Coordination games are at the heart of many economic phenomena. In such
games, shocks to the fundamentals can sometimes propagate: as traders attack
a currency peg, the central bank has to expand reserves to maintain the peg;
as reserves are drained, more traders are incentivized to take advantage of this
weaknesswhich,inturn,furtherdrainsthecentralbank’sreserves,enticingeven
more attacks. But shocks can also fizzle out: traders might learn quite quickly
thatthecentralbank’sbalancesheetishealthysothattheproportionofthepop-
ulation who are (incorrectly) pessimistic thins out rapidly. Thus, this downward
spiralofaninitialattackbegettingfurtherattackseventuallylosessteam.1 When
do shocks propagate and become self-fulfilling, and when do they fizzle out?
Wewillshowthatthespeedofprivatelearningisacrucialdeterminantofwhether
shocks exhibit persistence. Our model is a dynamic coordination game with an
endogenously changing state. A continuum of agents repeatedly decide whether
ff
or not to take a risky action whose payo comprises (i) a persistent fundamen-
tal component; and (ii) an endogeneous component which depends on past play.
In each period, each agent observes a private signal about the fundamental and
makesinferencesaboutthecurrentstate,whichdetermineshercurrentbehavior.
Agents’currentaggregatedecisions,inturn,propagateintothefuturebyshaping
the evolution offuture states. Thus, our environment featuresboth private learn-
ing—in which dispersion in beliefs about fundamentals across the population
shrinks with time—as well as intertemporal contagion, in which higher current
play increases future states which incentivizes higher future play. This endoge-
nous inertia in how the state evolves plays a critical role in the dynamics of play
and we call such environments inertial coordination games.2
Ourmainresult(Theorem1)developsatightcharacterizationoftherelationship
between the speed of learning and limit play. We identify a critical threshold
for the speed of learning that determines limit play: the risk-dominant action
is played in the limit if and only if posterior precisions grow sub-quadratically.
Sub-quadratic learning includes canonical learning environments such as (i) in-
dependent and identically distributed signals; and (ii) the receipt of a one-time
1Similarforcesoperatewithbankruns(viabankliquidity),networkedproducts(viaproduct
improvements),firmpricing(viastickyconsumers).
2Our model is consistent with extensive empirical documentation of bank failures: Correia,
Luck, and Verner (2024) find that “most bank failures are the result of a deterioration in bank
solvency.” (inourmodel,ashock)ratherthanbeingpurelyself-fulfilling. Furthermore,theyfind
that“Depositorstendtobeslowtoreacttoanincreasedriskofbankfailure”(inourmodel,the
intertia).
1signal. Super-quadratic learning includes social learning, where in each period,
agents are randomly matched and sample their partner’s information sets.
Indeed, static global games (Carlsson and Van Damme, 1993) are recovered ex-
actlyasaspecialcaseofourmodelinwhichplayersonlyreceiveasinglesignalat
time t = 1: since this learning process is sub-quadratic (posterior precisions are
unchanged after t =1), the risk-dominant profile is—as in global games—played
in the limit. This connection arises from observing that best-responding to ex-
pected past play along each time step is equivalent in our model to the interim
elimination of strictly dominated strategies in the static game. As we make pre-
cise in Appendix D, this uncovers a formal connection between “contagion” ar-
guments over type spaces (which encode hierarchies of higher-order beliefs) and
“contagion” over time (where the current state depends on past play). Thus, our
results generalize the predictions of static global games to sub-quadratic learn-
ff
ingando erasimpledynamiclearningfoundationforthe‘vanishingnoiselimit’
commonly invoked in that literature.
A notable implication of our main result is that slow learning implies history
independence: the initial shock to aggregate play as well as any finite path of
playareirrelevantforlimitplay. Conversely,whentheposteriorprecisiongrows
ffi
super-quadratically,learningissu cientlyfastwhichinduceshysteresisandhistory-
dependence that leads to non-risk dominant limit play. We further characterize
all possible equilibria: any monotone selection between initial play-dominance
(under common-knowledge of the fundamental state, players best-respond to
the conjecture that the proportion of players choosing the risky action is given
by initial play) and risk-dominance can be implemented via some appropriate
ff
learning rate. Thus, our results o er a simple and unified analysis for when our
model’s dynamics exhibit history-dependence such that initial shocks can be self-
fulfilling,3 and when it exhibits history-independence so that limit play is inde-
pendent of the initial shock.
Finally, our model makes sharp qualitative predictions about the path of play
(Proposition 3) whenever learning is slow enough such that the risk-dominant
profile is played in the limit. In particular, transition dynamics toward limit
play depend starkly on the precision of signals. When signals are precise, aggre-
gate play exhibits a sudden transition from approximately all players playing the
non-risk-dominantactiontoapproximatelyallplayersplayingtherisk-dominant
action. Conversely,whensignalsarenoisy,aggregateplayexhibitsagradualtran-
sitionasthemeasureofplayersplayingrisk-dominantactionincreasesgradually.
3See Section I of Krugman (1991) for an excellent discussion of history-dependence and ex-
pectationsineconomics.
2ff ff
These di ering transition regimes are driven by di erences in time-varying het-
erogeneity in beliefs. The importance of heterogeneity has been recognized as
a crucial determinant of equilibrium uniqueness in coordination games (Morris
and Shin, 2006). Our results complement this insight by highlighting that the
rate of decrease of heterogeneity driven by learning is also a crucial determinant
of the path of play. It also suggests that “spikes” in the time-series of aggre-
gate actions (e.g., withdrawal decisions or adoption of networked goods) can be
consistentwithtransitiontolimitequilibriumplayandneednotnecessarilycor-
respond to “equilibrium shifts” (Morris and Yildiz (2019)).
Related literature. Our paper directly relates to the large literature on global
games. Abroadtakeawayfromtheliteratureonstaticglobalgames(Carlssonand
Van Damme, 1993; Kajii and Morris, 1997; Frankel, Morris, and Pauzner, 2003,
among others) is that perturbing common knowledge by introducing a vanish-
ing amount of private uncertainty often selects the risk dominant equilibrium.4
This approach has been widely deployed in a variety of economic applications
to select among multiple equilibria (see, e.g., Morris and Shin (1998); Goldstein
and Pauzner (2005) among many others). Our model nests the canonical global
gamesmodelofCarlssonandVanDamme(1993)withNormalsignalsasaspecial
case with a private signal at t =1 and no learning thereafter. The “contagion ar-
gument” in global games where players iteratively eliminate interim dominated
strategies is sometimes viewed metaphorically or as a convenient way to select
among equilibria.5 By developing a simple dynamic environment with endoge-
ff
neous time-varying states, we thus o er an explicit dynamic foundation for the
“contagion” logic of global games. Moreover, we generalize the prediction of
risk-dominant selection to the regime of sub-quadratic learning.
Another strand of the literature has investigated equilibrium selection from a
non-Bayesian perspective, in particular under exogenous mutations or limited
memory(FudenbergandHarris(1992),Kandori,Mailath,andRob(1993),Young
(1993),BerginandLipman(1996),SteinerandStewart(2008),Block,Fudenberg,
and Levine (2019)). In contrast, we propose a model of private Bayesian learn-
ing in which the speed of learning and initial conditions jointly mediate limit
play. Our finding that super-quadratic learning rates lead limit play to depend
on initial conditions is related to the “hysteresis equlibrium” in macroeconomic
settings modelled as coordination games discussed in Cooper (1994); Krugman
(1991) and, more recently, in Morris and Yildiz (2019). Our results develop a
tight characterization of when hysteresis plays a role for limit outcomes, and
4Butnotalways(WeinsteinandYildiz,2007).
5See,Morris(2002)andChapter2ofSpiegler(2024).
3when initial conditions wash out.6
Our work also relates to dynamic global games (Angeletos, Hellwig, and Pavan,
2007; Dasgupta, 2007; Dasgupta, Steiner, and Stewart, 2012, among others). In
muchofthisliterature,coordinationiscontemporaneousi.e.,time-t playmatters
for time-t incentives.7 Our setting is one in which incentives are intertempo-
rally linked because the path of past play shapes the current state which in turn
shapes current incentives. In this regard, our paper is perhaps most related to
the elegant paper of Mathevet and Steiner (2013) who study finite-time horizon
dynamic global games with non-contemporaneous coordination motives where
the terminal outcome depends on both the state and path of play. Our models
are non-nested and deliver complementary insights. In our model, the state is
ff
time-varying and endogenous to past play so players’ payo s depend on interim
play which in turn drives the state-evolution. In Mathevet and Steiner (2013)’s
ff
model, ex-post payo s depend on a binary outcome which is determined by a
persistentfundamentalandthepathofaggregateplay. Thismappingfrompaths
to a binary outcome allows Mathevet and Steiner (2013) to tractably handle a
ff ff
wide class of payo s. By contrast, our payo s are more specific, but evolve en-
dogeneously and take on a continuum of values. We exploit the structure of our
environmenttoderivesharpimplicationsoflearningspeedsonlimitoutcomes.8
Finally, our results are closely connected to the literature on learning in coordi-
nation games. One interpretation of our model is one of best-response dynamics
withBayesianlearningaboutafundamentalstateorpastplay. Toourknowledge,
thisisnoveltotheextantliteratureonlearningingameswhichassumesthatpast
playisperfectlyobservedandthereisnofundamentaluncertainty.9 Ourpaperis
related to an important paper of Crawford (1995) who studies adaptive dynam-
ics in complete information coordination games by modelling player strategies
asalinearadjustmentruletobeestimated;thisissimilartotheinterpretationof
players in our model as best-responding to past play. Thus, a contribution of our
6See also Chen and Suen (2016) and Kozlowski, Veldkamp, and Venkateswaran (2020) for a
different mechanism of how model uncertainty and rare events interact and give rise to persis-
tence.
7ImportantexceptionsarethepapersofDasgupta,Steiner,andStewart(2012)andMathevet
andSteiner(2013).
8The main result of Mathevet and Steiner (2013) is that aggregate play at the critical state
at which outcomes hinge only depends on payoffs; by contrast, we show that limit aggregate
playatanystatedependssharplyonthespeedoflearning. NotealsothatMathevetandSteiner
(2013)’sdefinitionof“fastlearning”takestheprecisionofeachsignalarbitrarilylarge,whichis
sufficientforequilibriumexistence. Thisisdifferentfromournotionwhichrequiresthe(weaker)
condition of super-quadratic growth in posterior precisions, and where limit equilibrium play
alwaysconverges.
9SeeFudenbergandLevine(1998)forasurvey.
4paperistoanalyzehowBayesianlearningaboutfundamentaluncertaintycanin-
teract with adaptive learning about strategic uncertainty (Camerer and Hua Ho,
1999).10
2 Model
Payoffs. Thereisaunitmeasureofagentsindexedbyi ∈[0,1].11 Timeisdiscrete
andindexedbyt ∈T ={1,2,...}. Ateachtimet ∈T ,eachagenti choosesbetween
a risky action (a = 1) or a safe action (a = 0). Let λ denote the measure of
it it t
agents who play the risky action at time t.
There is an unknown and persistent fundamental θ ∈R. For each t ∈T , let θ be
t
the state at time t and for each t >1,12
θ t := θ + λ t−1 with initial play λ 0 ∈[0,1].
(cid:124)(cid:123)(cid:122)(cid:125) (cid:124)(cid:123)(cid:122)(cid:125)
Persistent Endogenous
ff
Each agent i’s payo s at time t are:

 θ if a =1
 t it
u(a ,θ ):=
it t  
c if a =0
it
ff
λ 0 is common knowledge but interim payo s as well as (λ t) t≥1 are unobserved
and inferred via beliefs about the state θ.13 Our model is equivalent to one in
which agents observe noisy signals of past play (λ t) t≥1, or both signals about the
state as well as past play—we formalize this an equivalence in Appendix B and
discuss this below.
Interpretinginitialplay. Weinterpretλ ∈[0,1]asashock. Thisisequivalentto
0
aone-off andtransitoryshocktothefundamentalorcostsattime1.14 Theimpact
of this shock is equivalent to that of non-zero initial play: positive shocks which
10Camerer and Hua Ho (1999) develop a model of learning in complete information games
which features both modes of learning; by contrast, in the best-response interpretation of our
model, different modes of learning correspond to different kinds of uncertainty:‘fundamental
uncertainty’isresolvedviaBayesianlearningbut‘strategicuncertainty’isresolvedviaadaptive
orreinforcementlearning.
11Endow [0,1] with the Lebesgue extension constructed in Sun and Zhang (2009) so that the
continuumlawoflargenumberholds.
12Foranyα∈(0,1),analogsofourresultsapplyforθ
t
=αθ+(1−α)λ t−1;seeAppendixE.
13AnidenticalassumptionismadebyAngeletos,Hellwig,andPavan(2007).
14More explicitly, suppose there is zero initial play but there is an initial shock to costs of
playing the risky action. Costs are time-varying c ,c ,... with c := c = c = c = ... and c (cid:44) c.
1 2 2 3 4 1
Then,thedynamicsofthismodelwithzeroinitialplayisequivalenttothemodelwithλ =c−c .
0 1
5incentive the risky action corresponds to high λ and vice versa. In the language
0
of macroeconomics, this corresponds to an unexpected shock (“MIT shock”) and
we analyze the dynamics of aggregate play following the shock. In the language
of fictitious play, this corresponds to a shock to initial play and we analyze how
the speed of learning shapes the limit action profile.
TimingandInformation. Thetimingofthegameandtheinformationalenviron-
ment are as follows. All agents share a common improper uniform prior about
θ. At each time t ∈T , each agent i receives a private signal x ∼ N(θ,σ2) that is
it t
iid across agents and independent across time. Each agent i then updates to her
time-t posterioraboutθ conditionalonthenaturalfiltrationF generatedbypast
t
signals{ x }t . FromstandardBayesianupdating,thisposteriorisalsoGaussian:
is s=1
θ|F ∼ N(µ ,η2), where µ :=η2(cid:80) t σ−2x and η2 :=((cid:80) t σ−2)−1. Finally, each
t it t it t s=1 s is t s=1 s
agenti choosesa ∈{0,1}tomaximizeherexpectedutility. Notethatsinceagents
it
are atomless, they cannot influence future aggregate play, so it is without loss to
assume they are myopic.15
Figure 1: Timing and intertemporal linkages
Limits and Learning. Let the learning process Σ := (σ t2) t∈T be a strictly positive
sequenceofsignalvariancesdeterminingthespeedoflearning. Notethatinstead
of Σ, we may equivalently directly specify a nonnegative, increasing sequence of
posterior precisions
(η−2)
, since such a sequence is induced by
σ−2 :=η−2− η−2
.
t t t t t−1
When convenient, we will therefore wlog consider
(η−2)
as a primitive.
t t
We maintain the following assumption unless otherwise stated.
Assumption1(Limit learning). Σ satisfies lim t→∞η t−2 =+∞.
Akeyobjectofanalysisinthispaperislimitaggregateplayatstateθ,asafunction
ofsignalvariancesΣandinitialaggregateplayλ 0. Wedenotethisbyλ∞(θ|Σ,λ 0):=
so1 m5F eo δr ∈e (x 0a ,m 1)p .le, we could redefine agent i’s payoffs as max (ait)t≥1(cid:80) t≥1δtE t[u(a it,λ t−1,θ)] for
6lim t→∞λ t(θ|Σ,λ 0). When(Σ,λ 0)isunderstoodtobefixed,weomitdependencies.
In particular, we study when limit aggregate play converges to the risk dominant
(RD) action at state θ, denoted by
(cid:110) (cid:111)
RD(θ):=1 θ ≥ c−(1/2) .16
The connection to risk-dominance is as follows: RD(θ) is the best-response for
each agent if she knew θ and believed that half the population played the risky
action.17 Analogously, define the non-risk dominant action as NRD(θ) := 1 −
RD(θ).
It will be helpful to discuss various aspects of our model:
• Discussionofstatetransition. Ourmodelisoneinwhichthecurrentstate
is shaped by past play. For instance, in a currency crisis, if more players at-
tackedthecurrencyinthepreviousperiod,thisweakensthefundamentals—
e.g.,thequantityofcentralbankreserves—whichleadstoahigherstateto-
day. With networked goods, if more players bought the good the previous
period, this generates revenue or data for the firm to improve its product
which leads to a better product today. We further note that although the
currentstateθ
t
onlydependsdirectlyonλ t−1,italsodependsindirectlyon
thefull(endogeneous)pathof(λ s) s<t−1;thisisbroadlyanalogoustohowan
AR(1) process depends indirectly on realizations of the process which are
far into the past (although our setting is non-stationary).18
• DiscussionofGaussianlearningstructure. We have explicitly parameter-
izedthemodelwithGaussiansignals;doingsoallowsustoanalyzethelaw
ofmotionofaggregateplayasadiscretedynamicalsystem. Itwillturnout,
however, that our main result (Theorem 1) does not rely on the parametric
form of Gaussian learning because we will show that whether risk domi-
nance obtains is a tail event (Corollary 1) and, furthermore, under regu-
larityassumptions,theposteriordistributiongeneratedfromasequenceof
bounded-variance noise structures converges to the normal distribution.19
16Ournotionofconvergenceoffunctionsofθisa.e. pointwiseconvergence.
17SeeMorrisandShin(2003)forasurveyoftheconnectionbetweenrisk-dominanceandcon-
tinuumplayerglobalgames.
18Analternateinterpretationisthatourmodelisaformofbest-responsedynamics. Ofcourse,
some care is required with this interpretation since past play is unobserved: players are thus
best-respondingtotheirbestguessofpastplay. Experimentalevidencein(completeinformation)
coordination games find that the vast majority (96%) of play constitute myopic best-responses
(Ma¨sandNax,2016).
19ByastraightforwardapplicationoftheBernstein–vonMisestheorem(VanderVaart,2000).
7Thus, our result characterizing sub/super-quadratic boundary as critical to
limit play holds for more general learning processes.
• Discussion of observability. We have assumed agents do not observe ag-
gregate past play.20 However, our model is equivalent to one in which
agents receive private signals both about the state, and about aggregate
past play.21 Explicitly, suppose that agent i ∈ [0,1], in addition to observ-
ing x ∼ N(θ,σ2), additionally observes an independent signal distributed
it t
y
it
∼ N(Φ−1(λ t−1),τ t2); this signal structure was introduced by Dasgupta
(2007) and used recently by Trevino (2020) to study financial contagion.
Then, we can show by induction that equilibrium behavior is equivalent
to a model in which players receive a more precise signal about θ and no
informationaboutaggregatepastplay;AppendixBformalizesthisconnec-
tion. In light of this observation, we load all learning on the state, and
agents make inferences about aggregate past play conditional on their be-
liefs about the state.
• Discussionofcontinuumplayers. Wefollowtheliteratureonglobalgames
by assuming that there is a continuum of atomless players. By the contin-
uumlawof largenumbers, thisimpliesthatconditioned onthefundamen-
tal state θ, all aggregate randomness in private signal realizations across
the population washes out. This allows us to analyze the dynamics of ag-
ff
gregate play cleanly as the solution to a non-linear deterministic di erence
equation. In Appendix C we develop a finite-player version of our model
with N < +∞ players and show via standard concentration arguments of
the empirical distribution of beliefs that limit play behaves similarly in
the finite-player version of our model: for all (Σ,λ ) and fundamental θ,
0
(cid:12) (cid:12)
sup N≥2(cid:12) (cid:12)λN ∞(θ)− λ∞(θ)(cid:12) (cid:12)=0 almost surely (see Theorem 2 in Appendix C).
3 Equilibriumlawofmotion
Webeginbyanalyzingthejointevolutionofaggregatebeliefsandaggregateplay
for the learning process and initial profile of play (Σ,λ ). Appendix A contains
0
the relevant proofs.
Dynamicsofthresholdbeliefs. Recallthatagenti’stime-tposteriorisN(µ ,η2).
it t
Agent i chooses a = 1 at time t if and only if µ ≥ µ∗ , where the belief threshold
it it t
20Aggregatepastplayisaninjectivefunctionofthestate,soifagentsweretoobservethis,they
wouldimmediatelylearnthestate.
21Forinstance,seeDasgupta(2007);Trevino(2020).
8∗ ff
µ is implicitly defined by the indi erence condition
t
(cid:20) (cid:21)
µ∗ t+E θ∼N(µ∗ ,η2) λ t−1(θ) =c
t t
LetΦ bethestandardGaussianCDF.Expandingthisindiff erenceconditionyields
∗
thelawofmotionforbeliefthresholds(µ ) ,whichcanbewrittenexplicitlyasthe
t t
ff
nonlinear di erence equation
(cid:18)(cid:113) (cid:19)
µ∗ =µ∗ + η2 +η2 ·Φ−1(c− µ∗ ) for all t ≥1 (µ∗ -LOM)
t t−1 t−1 t t
with boundary µ∗ := c− λ . Note that the dynamics of (µ∗ ) are (i) monotone in
1 0 t t
t; (ii) do not depend on the state θ; and (iii) slow down as agents’ beliefs become
moreprecise. Intuitively,whenagentsareconfidentaboutthestate,theyarecon-
ff
fident that their peers have already coordinated on an action, so the indi erence
condition should then not vary much.
Dynamicsofaggregateplay. Wenowgofromthresholdbeliefstoaggregateplay.
By the continuum law of large numbers,22 the time-t distribution of posterior
means in the population of agents is µ ∼ N(θ,η2). Hence, aggregate time-t play
it t
is simply given by
(cid:18) (cid:19)
(cid:32)
−
∗(cid:33)
θ µ
λ (θ)=P µ ≥ µ∗ =Φ t
t µ it∼N(θ,η t2) it t η
t
Since we assumed limit learning, lim t→∞η
t
=0. Taking t →∞ yields
λ∞(θ)=1 [θ ≥ µ∗ ∞]
∗
Hence, the limit belief threshold µ∞ pins down limit aggregate play at all states
θ. In particular, λ∞(θ) = RD(θ) a.e. if and only if µ∗ ∞ = c−(1/2). In words, the
risky action is adopted by the whole population in the limit if and only if, in the
complete information game in which the fundamental θ is known, playing the
riskyactionisthebestresponsetotheuniformconjecturethat1/2-proportionof
players are playing the risky action. Hence, we call c−(1/2) the risk dominance
threshold.
We now turn to two simple but informative examples.
Example 1 (Global games). For σ > 0, let the learning process Σ = (σ2,∞ ,∞ ,...)
be such that agents only receive information at time t = 1. Since η2 = σ2 for all
t
22See,e.g.,SunandZhang(2009).
9t ∈T , (µ∗ -LOM) reduces to the stationary diff erence equation
√
µ∗ =µ∗
+
2σΦ−1(c− µ∗
)
t t−1 t
For any σ >0 and λ ∈(0,1), this equation has a unique, globally attracting fixed
0
point at µ∗ ∞(σ,λ 0) = c−(1/2). Hence, for any initial signal precision and initial
aggregate play, the risk dominance threshold is attained in the limit. However,
without limit learning, limit play is not exactly risk-dominant since some agents
remain mistakenly optimistic/pessimistic about the state. But, as the initial sig-
nal becomes arbitrarily precise,
limλ∞(θ| σ,λ 0)=RD(θ).
σ↓0
Indeed, in Appendix D we develop a two-player interpretation which recovers
an equivalence between best-response dynamics and iterated deletion of interim
strictly dominated strategies dynamics. Thus, the special case with learning
only at t = 1 and noisy signals thereafter is exactly the model of Carlsson and
Van Damme (1993); Appendix D makes this connection precise. ♦
Example 2 (Perfect learning). Consider any learning process Σ with σ =0, such
1
that θ is common knowledge from time t = 1 onwards. Suppose that θ+λ > c.
0
We show by induction that λ =1 for all t ∈T . For the base case, note that every
t
agenti strictlyprefersa =1ifandonlyifθ+λ >c,whichholdsbyassumption.
it 0
Hence, λ 1 = 1 is common knowledge. For the inductive step, assume λ t−1 = 1.
Then,
θ+λ t−1− c=θ+1− c≥ θ+λ 0− c>0
which implies λ = 1. The argument for θ+λ < c is analogous so λ = 0 for all
t 0 t
t ∈T . Hence, limit play depends on λ 0: λ∞(θ|Σ,λ 0)=1 [θ ≥ c− λ 0]. ♦
Examples 1 and 2 resemble the well-understood gap between small perturba-
tions of common knowledge highlighted in the literature on static coordination
games (Example 1). In our environment, complete information does not induce
multiplicitybecauseplayers’incentivesarenon-contemporaneous;butitinduces
history-dependence: limit play is sensitive to initial play.23
ff
What are we to make of these di ering predictions? In what follows we will
provideaunifiedaccountofwhenrisk-dominanceorhistory-dependenceobtains
asafunctionthelearningprocess. ItwillturnoutthatExample1isaspecialcase
23This is consistent with experimental evidence from coordination games which have largely
focusedoncompleteinformationsettings;see,e.g. VanHuyck,Battalio,andBeil(1991).
10of slow learning since posterior precisions grow sub-quadratically (in Example 1,
they do not change beyond t =1). On the other hand, Example 2 is a special case
offastlearningsinceposteriorprecisionsaresuper-quadratic(inExample2,they
“jump immediately to infinity” at time t =1).24
4 Characterizationoflimitplay
Our main result gives a tight characterization of limit outcomes in terms of the
learning rate. All proofs are in Appendix A. We use standard asymptotic nota-
tion: f(t) = O(g(t)) if there exists constants C < +∞ and T < +∞ such that t ≥ T
implies f(t) ≤ Cg(t). f(t) = Ω(g(t)) if there exists constants C < +∞ and T < +∞
such that t ≥ T implies f(t)≥ Cg(t).
Theorem1. The following gives a tight characterization of risk-dominance:
(i) For all p<2 and all λ ∈(0,1),
0
η t−2 =O(tp) =⇒ λ∞(θ)=RD(θ) a.e.
(ii) For all q>2 and all λ ∈(0,1),
0
η t−2 =Ω(tq) =⇒ λ∞(θ)=NRD(θ)
for some positive measure interval (θ,θ).
Theorem 1 identifies the critical threshold for limit play and, in so doing, devel-
opsatightconnectionbetweensub-quadraticgrowthofposteriorprecisionsand
risk-dominance. Economically, it suggests that whether or not coordination suc-
ceeds, and how it depends on initial play, depends sharply on the learning rate.
We discuss the implications of each part in turn.
Theorem 1 (i) generalizes the global games prediction to the case where learn-
ffi
ing is su ciently slow. Thus, our model with an endogenously changing state
ff
o ers an alternate foundation for equilibrium selection. Moreover, it clarifies
ffi
that sub-quadratic growth of posterior precision is both necessary and su cient
for risk-dominant selection. Thus, although the ‘vanishing noise limit’ in static
global games (Carlsson and Van Damme, 1993; Morris and Shin, 2003) is often
24Translated into a static incomplete information game, this correspond to interspersing in-
terim Bayesian updating with rounds of interim deletion of strictly dominated strategies. Ap-
pendixDmakesthisconnectionprecise.
11interpreted as modelling players with vanishing uncertainty, this actually cor-
responds to slow learning in our dynamic model with endogenously changing
states.25
ff
Figure 2: Path of aggregate play under di erent learning regimes
Parameters: θ=0.4,λ =0.75,c=1(nottoscale).
0
(a)Sub-quadraticposteriorprecision (b)Super-quadraticposteriorprecision
Economically, thisalsoimpliesthatshockstoaggregateincentives(whichλ can
0
ffi
be interpreted as) do not persist when learning is su ciently slow. Figure 2 (a)
illustrates a typical path of aggregate play under slow learning. The true state is
θ =0.4andthisgovernsthelocationof(heterogeneous)beliefsinthepopulation.
The cost of playing the risky action is c = 1 so under the conjecture that 1/2-
proportion of the population plays the risky action and common-knowledge of
the state θ = 0.4, the risk-dominant profile is to not play the risky action. Thus,
following the initial shock λ = 0.75, aggregate play converges back to the safe
0
action.
ffi
Theorem 1 (ii) breaks risk-dominance by showing that su ciently fast learning
can reintroduce the influence of initial play. This generalizes Example 2’s pre-
diction from instantaneous learning to fast learning. Economically, this implies
shocks to aggregate incentives are persistent when learning is fast. Figure 2 (b)
illustrates a typical path of aggregate play under super-quadratic learning. Un-
der the conjecture that λ =0.75 proportion of players play the risky action and
0
common-knowledgeofthestateθ =0.4,playingtheriskyactionisindeedabest-
response.
ffi
TheresultinTheorem1(ii)thatsu cientlyfastlearninginourmodelcanbreak
risk-dominance is related to the idea that public signals can yield non-risk dom-
inant play even as uncertainty about the fundamentals vanish; see Morris and
25WearegratefultoMuhametYildizforpointingoutthisinterpretation.
12Shin (2003) Chapter 3.2. In both cases, fast private learning or public signals
enable players to quickly approach common knowledge of the fundamental and
their peers’ play, thus enabling coordination on the non-risk dominant action.
Ourmainresultcomplementsandenhancesthisinsightbyprovidinganexplicit
dynamic foundation, as well as a precise characterization of how fast learning
needs to be for this intuition to hold. This allows us to classify several canonical
learning environments and their resulting limit outcomes.
IntuitionforTheorem1. Recallourpreviousobservationthatthespeedofevolu-
ff ∗
tionofthetime-t indi erentbelief(µ ) isinverselyrelatedtothespeedoflearn-
t t
∗
ing: whenlearningisslowenough,(µ ) graduallyconvergestotheriskdominant
t t
thresholdµ∗ ∞ =c−(1/2)fromanyinitialcondition. However,whenlearningisfast
∗ ∗
enough,thedynamicsof(µ t)
t
freeze,andconsequentlyµ∞ isboundedawayfrom
theriskdominantthreshold. TheproofofTheorem1formalizesthisintuitionby
explicitly constructing fictitious threshold sequences that converge to (non)-risk
dominant play, matching them to the appropriate learning rates.
Figure 3: Slow learning implies risk dominance
Figure 3 illustrates the forces underlying Theorem 1 (i) for the case where λ <
0
ff
1/2. Each panel corresponds to a di erent time. In each panel, the dotted line
represents the state θ, and the red curve is the distribution of posterior means
∗
µ . The blue line is the threshold belief µ , and the shaded area is the mass of
t t
agentsλ whotaketheriskyactionbecausetheirbeliefsexceedthethreshold. As
t
time passes, agents learn and µ contracts around θ. In Figure 3, learning and
t
13∗
hence the speed of this contraction is slow. Consequently, the evolution of µ is
t
relatively fast. As t → ∞, µ∗ converges to c−(1/2) and µ converges to a point
t t
mass on θ due to limit learning. Since θ ≥ c−(1/2), aggregate play converges to
the risk dominant (risky) action.
Figure 4 illustrates the forces underlying Theorem 1 (ii) for the case where λ <
0
1/2. With super-quadratic learning, the speed of contraction of µ is fast. Conse-
t
quently, the evolution of µ∗ is relatively slow. As t → ∞, µ∗ converges to a limit
t t
threshold µ∗ ∞ that is bounded away from c−(1/2), and since θ ∈ (c−(1/2),µ∗ ∞),
aggregate play converges to the non-risk dominant (safe) action.
Figure 4: Fast learning implies non-risk dominance
Several corollaries follow relatively straightforwardly from Theorem 1.
Corollary 1 (Risk-dominance is a tail event). Consider any sub-quadratic learn-
ing process Σ = (σ t2) t∈T . Then, for any finite T < +∞ and any (σ t′ ) t≤T, the risk-
dominant action profile is selected almost surely in the limit under the learning
process Σ′ :=[((σ t′ )2) t≤T,(σ t2) t>T].
Thisgivesanexplicitlydynamicfoundationforthegapbetweenalmost-common
ff
knowledge and common knowledge of payo s in static incomplete information
games. Indeed, Corollary 1 suggests that the Gaussian form of private signals is
not necessary since, for (i) an arbitrary noise structure (with bounded variance);
14ffi
and (ii) “su ciently smooth priors”, as T grows large the distribution over pos-
terior beliefs after T grows close to the normal distribution by the Bernstein-von
Mises Theorem (Van der Vaart, 2000).
Corollary2. The following are limit outcomes of canonical learning processes:
(i) (IID Gaussian signals) Suppose Σ=(σ2,σ2,...). Then for any λ ∈(0,1) and
0
σ >0, limit aggregate play is the risk dominant action.
(ii) (Social learning) At each time t ≥ 2, player i exchanges information with
player j. Thus, defining Σ(σ) to be: σ2 :=σ2 and for all t ≥2, σ2 :=η2 , we
1 t t−1
have that for any λ , there exists some σ > 0 such that under Σ(σ) for any
0
σ < σ, there exists an open interval of states at which limit aggregate play
is the non-risk dominant action.
Corollary 2 (i) shows that limit risk dominant play independent of initial condi-
tions holds for a canonical informational environment. Corollary 2 (ii) studies
a social learning setting where at time 1, each agent observes an iid private sig-
nal, and at each subsequent time, each agent is matched at random with another
player in the population and observes the other player’s information. This pro-
vides a simple informational setting where, if the initial signal is informative
enough, non risk-dominant coordination arises.
Comparative statics. Theorem 1 identified a sub- and supercritical regime for
how learning speeds impact limit play. We now develop comparative statics on
how the set of equilibria vary with primitives. Recall that limit play is jointly
pinned down by the limit threshold µ∗ ∞ and the fundamental θ: λ∞ =1 (θ ≥ µ∗ ∞).
Thus,µ∗
∞
=c−1/2correspondstorisk-dominance;as| µ∗ ∞−(c−1/2)|becomeslarge,
the non-risk dominant limit profile is selected on a wider set of states.
Proposition 1 (Comparative statics). The following are comparative statics on
| µ∗ ∞−(c−1/2)|:
(i) Extreme initial play induces extreme limit play. For any | λ −(1/2)| ≤
0
| λ′ −(1/2)| and any Σ,
0
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)µ∗
∞(Σ,λ
0)−(c−1/2)(cid:12)
(cid:12)
(cid:12)≤(cid:12)
(cid:12)
(cid:12)µ∗ ∞(Σ,λ′ 0)−(c−1/2)(cid:12)
(cid:12) (cid:12).
(ii) Monotonicityinlearningspeeds. For any
Σ≥Σ′
,
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)µ∗
∞(Σ,λ
0)−(c−1/2)(cid:12)
(cid:12)
(cid:12)≤(cid:12)
(cid:12)
(cid:12)µ∗ ∞(Σ′
,λ
0)−(c−1/2)(cid:12)
(cid:12) (cid:12).
15(iii) Earlylearningratesmattermore. FixanyΣ=(σ2) anddefinethepairwise
t t
s↔ s′ permutation Σs↔s′ :=(σ˜2) where
t t

 σ2 if t (cid:44) s,s′
  t

σ˜ t2 =   σ s2 ′ if t =s

 σ2 if t =s′ .
s
Then, for any s≥ s′ and σ2 ≥ σ2,
s s′
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)µ∗
∞(Σ,λ
0)−(c−1/2)(cid:12)
(cid:12)
(cid:12)≤(cid:12)
(cid:12)
(cid:12)µ∗ ∞(Σs↔s′
,λ
0)−(c−1/2)(cid:12)
(cid:12) (cid:12).
Proposition1isintuitive. Part(i)statesthatthenon-riskdominantplayexpands
(i.e., on a wider set of states the non-risk dominant equilibrium is played in the
limit) whenever initial play is more extreme on either side of 1/2; Part (ii) states
thatnon-risk dominantplay expandswhen signalsare moreprecise ateach time
step (although, per Theorem 1, this only has bite in the superquadratic regime);
Part (iii) shows that non-risk dominant play expands when the learning process
is permuted so that more informative signals arrive early, and less informative
signalsarrivelate. Intuitively,learningandaggregatebehavioratearlytimesteps
are more important because they propagate into the future.
Possible limit play. Theorem 1 states that when learning is suffi ciently fast, the
limit action is non-risk dominant at some open interval of states. Proposition
1 established how this interval varies as primitives change. This raises the fol-
lowing question: what kinds of limit play can be rationalized by some learning
rate?
Saythataprofileisλ -dominantif,undercommon-knowledgeofθ,playersplay
0
as-if they best-respond to the conjecture that λ proportion of players play the
0
riskyaction.26 Ournextresultestablishesthatanyprofilebetweenλ -dominance
0
and risk-dominance can be rationalized by some learning speed.
Proposition2(The image of learning rates). For any λ ∈(0,1),
0
(cid:91) (cid:91)
λ∞(θ|Σ,λ 0)= 1 (θ ≥ µ∗ ).
∗
Σ µ between
c−λ0andc−1/2
In particular, for any threshold µ∗ between c− λ and c−(1/2), there exists some
0
26Thisisequivalenttothedefinitionofp-dominanceinMorris,Rob,andShin(1995).
16learning rate Σ≥0 such that:
λ∞(θ|Σ,λ 0)=1 (θ ≥ µ∗ ) a.e.
The intuition behind Proposition 2 is that for any time t, we can arbitrarily re-
ffi
strict the evolution of the system by choosing a su ciently precise signal for
time t. This verifies that any level of coordination on the non-risk dominant
action can be rationalized by some sequence of signal noises.27 Together with
our comparative static on learning speeds (Proposition 1 (ii)) this also implies
lim Σ↓0λ∞(θ|Σ,λ 0)=1 (θ ≥ c− λ 0)—that is, in the vanishing noise limit along the
whole path, limit play converges to that of the complete environment with all
players best-responding to λ .
0
Proposition 2 also highlights the following asymmetry between initial play and
learning rates: when initial play is large (λ > 1/2 large), slow learning hurts
0
ffi ff
e ciencysincethesetoffundamentalstatesatwhichthepayo -dominantaction
is played shrinks to { θ ≥ c−1/2}; conversely, when initial play is small (λ < 1/2
0
small), slow learning improves effi ciency since on the set of fundamentals { θ ≥ c−
1/2} the payoff -dominant action is played in the limit.
5 Transitiondynamics
We have thus far focused on limit play. We now turn to studying the connec-
tion between the speed of learning and the time path of aggregate play. We will
consider the iid signal case such that the learning process Σ(σ) is parametrized
by the standard deviation of each period’s signal σ >0. We focus on the iid case
becauseitisacanonicalmodeloflearning,andmakesresultseasiertostatesince
Σ is now parametrized by a scalar; nonetheless, the qualitative features of our
resultsextendbeyondthiscase. Withinthisiidlearningenvironment,recallthat
Corollary2(i)impliesthatthelimitactionisriskdominantforallfundamentals
andinitialconditions. Thefollowingresultshowsthatthepathofaggregateplay
ff
exhibits qualitatively di erent behavior depending on learning rates.
Proposition 3. Fix any λ ∈ (0,1) and θ between c−1/2 and c− λ . Define β :=
0 0
1+| c−(1/2)− θ| /2. Foranyϵ>0,α ∈(0,1),andβ ∈(1,β),thereexistsσ,σ >0such
that:
(i) Sudden transition. For all σ < σ, there exists T(σ) ≥ 1 such that: for all
1≤ t ≤ αT(σ),| λ (θ)− NRD(θ)| <ϵ andforallt ≥ βT(σ),| λ (θ)− RD(θ)| <ϵ;
t t
27Recallthatµ∗ =c−λ andµ∗ evolvesmonotonicallyaway.
1 0 t
17(ii) Gradualtransition. For all σ >σ and all t ≥1, | λ (θ)− λ (θ)| <ϵ.
t+1 t
ffi
Proposition3saysthat,withintheregimeofiidlearning,su cientlyfastlearning
yields a sudden transition from aggregate non-risk dominant to risk dominant
ffi
play that occurs around some finite time. Conversely, su ciently slow learning
ffi
yields su ciently gradual transitions in aggregate play.
Figure 5: Mechanism underlying transition regimes
The intuition behind Proposition 3 is illustrated in Figure 5. In the case where
learningisslow,thedistributionofposteriormeansisdispersedsoasthethresh-
oldchanges, arelativelysmallmassofnewagentstaketheriskdominantaction.
Conversely, when learning is fast, the distribution of posterior means contracts
aroundθ sothatthereisasuddenspikeinagentstakingtheriskdominantaction
when it is crossed.
Figure 6 numerically illustrates the time path of play (λ ) for iid learning. Panel
t t
(a)showsthedynamicswhenσ =1;panel(b)showsthedynamicswhenσ =0.01.
In particular, when learning is fast, an analyst would observe next-to no change
in the measure of agents playing the risky action before a sudden spike.
Proposition3issubtlyconnectedtotheliteratureoncoordinationgames. Acom-
mon theme in this literature is that heterogeneity—in preferences, beliefs, etc.—
across players can induce uniqueness in static supermodular games (Morris and
Shin, 2006). A dynamic version of this insight is that with lots of hetrogeneity,
as aggregate incentives shift, the set of equilibria vary smoothly—this is analo-
gous to Part (i) of Proposition 3; by contrast, with little heterogeneity, the set of
equilbiria can vary discontinuously—this is analogous to Part (ii) of Proposition
3. The key insight of Proposition 3 is that learning rates induce time-varying
hetrogeneity in beliefs which, in turn, matters for transition dynamics.28
28KohandMorris(2022)relatesheterogeneitytothespeedofcontagioninanetworkedpopu-
18Figure 6: Gradual vs sudden transition to risk dominant play
Parameters: θ=0.6,λ =0.2,c=1.
0
(a)σ =1 (b)σ =0.01
ff
Economically, Proposition 3 o ers a simple alternate interpretation of sudden
changes in the time-series of aggregate play: when learning is precise, players
haveverysimilarbeliefsbythetimethe“tippingpoint”arrivesandthestatehas
increased to the point that is optimal for such players to play the risky action.
Sudden spikes have been commonly interpreted as “equilibrium shifts” driven
by shocks or new public information (Chwe, 2013; Morris and Yildiz, 2019) in-
ff
teracting with uniform rank beliefs. Our result o ers an alternate explanation
driven by private learning and time-varying heterogeneity.
6 ConcludingRemarks
We have developed a simple but novel model of dynamic coordination with (i)
Bayesianlearningaboutfundamentaluncertainty;and(ii)adaptivebest-response
dynamics. This has allowed us to combine and unify insights from the literature
on best-response dynamics in complete-information coordination games (Craw-
ford, 1995) and global games (Carlsson and Van Damme, 1993; Morris and Shin,
2003). Our model serves as a natural description of many economic environ-
ments with both shrinking uncertainty about fundamentals (e.g., solvency of a
bank, strength of the economy, quality of a networked product) as well as an
endogenous time-varying state which is influenced by aggregate past play.
lation. Thiscanbeequivalentlyinterpreted(see, e.g., Morris(1997))asbest-responsedynamics
orinterimdeletionofstrictlydominatedstrategieswhereeachplayer’sbeliefsandhigher-order
beliefs are encoded by the network. However, Koh and Morris (2022) study speed of contagion
for graphs/beliefs which are held fixed i.e., in the incomplete information game interpretation,
thereisnolearning.
19Withinourenvironment,weuncoveredatightconnectionbetweenlearningrates
and limit play (Theorem 1) and established sub/super-quadratic reduction in
posterior variance as the critical threshold at which limit play undergoes a shift
away from risk-dominance. We also show that, subject to initial conditions, any
shift from risk-dominance can be attained by some learning rate. Moreover, in
ff
the sub-quadratic regime, we further identified qualitatively di erent behavior
in the path of aggregate play (Proposition 3): when learning is slow, transition
dynamics are gradual; when learning is fast, sudden.
References
Angeletos, G.-M., C. Hellwig, and A. Pavan (2007): “Dynamic global games of
regime change: Learning, multiplicity, and the timing of attacks,” Economet-
rica, 75, 711–756.
Bergin,J.andB.L.Lipman(1996): “Evolutionwithstate-dependentmutations,”
Econometrica: Journal of the Econometric Society, 943–956.
Block, J. I., D. Fudenberg, and D. K. Levine (2019): “Learning dynamics with
socialcomparisonsandlimitedmemory1,”TheoreticalEconomics,14,135–172.
Camerer, C. and T. Hua Ho (1999): “Experience-weighted attraction learning in
normal form games,” Econometrica, 67, 827–874.
Carlsson, H. and E. Van Damme (1993): “Global games and equilibrium selec-
tion,” Econometrica: Journal of the Econometric Society, 989–1018.
Chen, H. and W. Suen (2016): “Falling dominoes: a theory of rare events and
crisis contagion,” American Economic Journal: Microeconomics, 8, 228–255.
Chwe, M. S.-Y. (2013): Rational ritual: Culture, coordination, and common knowl-
edge, Princeton University Press.
Cooper, R. (1994): “Equilibrium selection in imperfectly competitive economies
with multiple equilibria,” The Economic Journal, 104, 1106–1122.
Correia, S. A., S. Luck, and E. Verner (2024): “Failing Banks,” Tech. rep., Na-
tional Bureau of Economic Research.
Crawford, V. P. (1995): “Adaptive dynamics in coordination games,” Economet-
rica: Journal of the Econometric Society, 103–143.
Dasgupta, A. (2007): “Coordination and delay in global games,” Journal of Eco-
nomic Theory, 134, 195–225.
Dasgupta, A., J. Steiner, and C. Stewart (2012): “Dynamic coordination with
individual learning,” Games and Economic Behavior, 74, 83–101.
Frankel, D. M., S. Morris, and A. Pauzner (2003): “Equilibrium selection in
global games with strategic complementarities,” Journal of Economic Theory,
108, 1–44.
Fudenberg, D. and C. Harris (1992): “Evolutionary dynamics with aggregate
shocks,” Journal of Economic Theory, 57, 420–441.
Fudenberg, D. and D. K. Levine (1998): The theory of learning in games, vol. 2,
MIT press.
20Goldstein, I. and A. Pauzner (2005): “Demand–deposit contracts and the prob-
ability of bank runs,” Journal of Finance, 60, 1293–1327.
Harsanyi, J. C., R. Selten, et al. (1988): “A general theory of equilibrium selec-
tion in games,” MIT Press Books, 1.
Kajii,A.andS.Morris(1997): “Therobustnessofequilibriatoincompleteinfor-
mation,” Econometrica: Journal of the Econometric Society, 1283–1309.
Kandori, M., G. J. Mailath, and R. Rob (1993): “Learning, mutation, and long-
run equilibria in games,” Econometrica: Journal of the Econometric Society, 29–
56.
Koh, A. and S. Morris (2022): “Speed vs Resilience in Contagion,” Available at
SSRN.
Kozlowski, J., L. Veldkamp, and V. Venkateswaran (2020): “The tail that wags
the economy: Beliefs and persistent stagnation,” Journal of Political Economy,
128, 2839–2879.
Krugman, P. (1991): “History versus expectations,” The Quarterly Journal of Eco-
nomics, 106, 651–667.
Ma¨s, M. and H. H. Nax (2016): “A behavioral study of “noise” in coordination
games,” Journal of Economic Theory, 162, 195–208.
Mathevet,L.andJ.Steiner(2013): “Tractabledynamicglobalgamesandappli-
cations,” Journal of Economic Theory, 148, 2583–2619.
Morris,S.(1997): “Interactiongames: Aunifiedanalysisofincompleteinforma-
tion, local interaction and random matching games,” .
———(2002): “Coordination,Communication,andCommonKnowledge: ARet-
rospective on the Electronic-mail Game,” Oxford review of economic policy, 18,
433–445.
Morris, S., R. Rob, and H. S. Shin (1995): “p-Dominance and belief potential,”
Econometrica: Journal of the Econometric Society, 145–157.
Morris, S. and H. S. Shin (1998): “Unique equilibrium in a model of self-
fulfilling currency attacks,” American Economic Review, 587–597.
——— (2003): “Global Games: Theory and Applications,” in Advances in Eco-
nomicsandEconometrics: TheoryandApplications,EighthWorldCongress,ed.by
M. Dewatripont, L. P. Hansen, and S. Turnovsky, Cambridge University Press,
56–114.
——— (2006): “Heterogeneity and uniqueness in interaction games,” The Econ-
omy as an Evolving Complex System, 3, 207–42.
Morris, S. and M. Yildiz (2019): “Crises: Equilibrium shifts and large shocks,”
American Economic Review, 109, 2823–2854.
Spiegler, R. (2024): The Curious Culture of Economic Theory, MIT Press.
Steiner, J. and C. Stewart (2008): “Contagion through learning,” Theoretical
Economics, 3, 431–458.
Sun, Y. and Y. Zhang (2009): “Individual risk and Lebesgue extension without
aggregate uncertainty,” Journal of Economic Theory, 144, 432–443.
Trevino, I. (2020): “Informational channels of financial contagion,” Economet-
rica: Journal of the Econometric Society, 88, 297–335.
Van der Vaart, A. W. (2000): Asymptotic statistics, vol. 3, Cambridge university
21press.
Van Huyck, J. B., R. C. Battalio, and R. O. Beil (1991): “Strategic uncertainty,
equilibriumselection,andcoordinationfailureinaverageopiniongames,”The
Quarterly Journal of Economics, 106, 885–910.
Weinstein, J. and M. Yildiz (2007): “A structure theorem for rationalizability
with application to robust predictions of refinements,” Econometrica, 75, 365–
400.
Young, H. P. (1993): “The evolution of conventions,” Econometrica: Journal of the
Econometric Society, 57–84.
22APPENDIX TO INERTIAL COORDINATION GAMES
AppendixA: Omittedproofs
SectionA.1containsseveralusefullemmasthatcharacterizethedynamicsofour
model. Sections A.2-A.5 collect all remaining proofs. We sometimes assume
λ > 1 without loss of generality because symmetric arguments apply to λ < 1.
0 2 0 2
A.1 SystemDynamics. Lemmas1-3showthatinequilibrium,agentsuseacut-
ff
o strategy with respect to posterior means. Lemmas 4-7 study useful auxiliary
system dynamics.
Lemma 1. At each time t ≥1 and for each player i, a =1 if and only if µ ≥ µ∗ ,
it it t
where µ∗ := c− λ and for all t > 1, µ∗ is the unique solution to the nonlinear
1 0 t
ff
di erence equation
 
µ∗
t+Φ 
 
 
(cid:113)µ∗ t− µ∗
t−1
 
 
  =c
(µ∗
-LoM)
 
 η2 +η2
t−1 t
Hence, for each t ≥1,
(cid:32)
−
∗(cid:33)
θ µ
λ (θ)=Φ t (λ-LoM)
t
η
t
Proof of Lemma 1. We prove Lemma 1 by induction. At t = 1, each agent i’s pos-
terior belief is given by θ|F ∼ N(x ,σ2). Hence, she takes a = 1 if and only if
i1 i1 1 i1
E [θ ]=µ =x ≥ c− λ =µ∗ . Since x | θ ∼ N(θ,σ2),
i1 1 i1 i1 0 1 i1 1
(cid:32) ∗ − (cid:33) (cid:32) − ∗ (cid:33)
µ θ θ µ
λ (θ)=P (x ≥ µ∗ )=1−Φ 1 =Φ 1 .
1 θ i1 1 σ σ
1 1
Hence, (λ-LoM) holds for t =1.
Now, for t > 1, suppose that for all agents j, a j,t−1 = 1 if and only if µ j,t−1 ≥ µ∗ t−1.
By standard Gaussian-Gaussian Bayesian updating, we have θ|F ∼ N(µ ,η2),
it it t
where
 −1
µ
=(cid:88)t
σ
s−2
x and η2 =  
(cid:88)t
σ−2    .
it η−2 is t   s  
s=1 t s=1
Conditional on θ, the time-(t −1) distribution of posterior means is µ j,t−1| θ ∼
N(θ,η2 ). Since agents’ information sets are independent conditional on θ, this
t−1
implies that, under agent i’s time-t posterior belief, the marginal distribution of
time-(t−1)posteriormeansisµ j,t−1|F
it
∼ N(µ it,η t2+η t2 −1). Hence,bytheinductive
23step, i takes a =1 if and only if
it
 
c≤E it[θ+λ t−1(θ)]=µ it+P it(µ j,t−1 ≥ µ∗ t−1)=µ it+Φ      (cid:113)µ it− µ∗ t−1      

 
 η2 +η2
t−1 t
Let F(µ ) be the RHS function. Since F is continuous and strictly increasing in
it
µ ,andF(c)>c andF(c−1)<c hold,F(µ )=c hasauniquesolutionµ∗ ∈(c−1,c).
it it t
ff ∗ ∗
Hence, there exists a unique cuto µ , defined implicitly by (µ -LoM) at time t,
t
such that i takes a =1 if and only if µ ≥ µ∗ .
it it t
Finally, since µ | θ ∼ N(θ,η2), we see that
it t
(cid:32)
−
∗(cid:33)
θ µ
λ (θ)=P (µ ≥ µ∗ )=Φ t
t θ it t
η
t
and hence (λ-LoM) holds at time t, as desired.
Lemma 2. (µ∗ ) has a unique steady state µ∗ =c−1/2. For any t ∈T , µ∗ >µ∗ if
t t ss t+1 t
∗ ∗
and only if µ <µ .
t ss
Proof. Assume µ∗ = c−(1/2). Since µ∗ := c−(1/2) solves (µ∗ -LoM) and it has a
t−1 t
uniquesolution,c−(1/2)isasteadystate. Toseethatit’stheuniquesteadystate,
assume µ∗ =µ∗ . Then, µ∗ =c−(1/2), as desired.
t−1 t t
Next, fix any t ∈T . By (µ∗ -LoM), we have
∗ ∗
µ >µ
t+1 t
 
   µ∗ − µ∗    1
⇔ Φ  (cid:113)t+1 t   >
    2
 η2+η2 
t t+1
⇔ ∗ ∗
µ <µ .
t+1 ss
Hence, if µ∗ >µ∗ , µ∗ <µ∗ <µ∗ . Conversely, if µ∗ ≤ µ∗ , which is equivalent to
t+1 t t t+1 ss t+1 t
µ∗ ≥ µ∗ , we have µ∗ ≥ µ∗ ≥ µ∗ .
t+1 ss t t+1 ss
Lemma3. (i) If λ 0 > 1 2, then µ∗ t ↑ µ∗ ∞ ∈(µ∗ 1,µ∗ ss].
(ii) If λ 0 < 1 2, then µ∗ t ↓ µ∗ ∞ ∈[µ∗ ss,µ∗ 1).
(iii) | µ∗ − µ∗| is decreasing in t.29
t+1 t
(iv) λ∞(θ)=1 [θ ≥ µ∗ ∞] a.e.
29Notethatparts(i),(ii),and(iii)ofthisLemmaholdwithoutAssumption1.
24Proof. Parts(i)and(ii). Wlog,assumeλ > 1. SinceLemma2impliesthat(µ∗ ) is
0 2 t t
astrictlyincreasingsequenceboundedabovebyµ∗ ss,itconvergestoµ∗
∞
∈(µ∗ 1,µ∗
ss].
∗
Part(iii). Note that (µ -LoM) implies
(cid:113)
(cid:12) (cid:12)
| µ∗ − µ∗|= η2+η2 (cid:12) (cid:12)Φ−1(c− µ∗ )(cid:12) (cid:12).
t+1 t t t+1 t+1
(cid:113)
Note that η2+η2 is non-increasing in t. Wlog, assume λ > 1. Then, (c− µ∗ )
t t+1 0 2 t+1
decreasing in t implies
Φ−1(c− µ∗
)>0 is decreasing in t, and therefore
|Φ−1(c−
t+1
µ∗ )| is decreasing in t. Hence, | µ∗ − µ∗| is also decreasing in t.
t+1 t+1 t
Part (iv). Wlog, assume λ > 1. First, we show (θ − µ∗ )/η → −∞ when θ <
0 2 t t
µ∗ ∞. Since ((θ− µ∗ t)/η t)
t
is decreasing and becomes negative for suffi ciently large
t, it should either converge to some negative constant or diverge to −∞. Since
lim t→∞(θ− µ∗ t)<0, lim t→∞(θ− µ∗ t)/η
t
=−∞ must hold by limit learning.
Next, we show (θ− µ∗ t)/η
t
→∞ when θ >µ∗ ∞. Lemma 3 (i) implies
− ∗ − ∗
θ µ∞ θ µ
t
0< < .
η η
t t
Since (θ− µ∗ ∞)/η
t
→∞ as t →∞, (θ− µ∗ t)/η
t
→∞ holds.
Lemma4. Define γ :=Φ−1(λ ), and for t ≥2, define
1 0
∗− ∗ (cid:113)
µ µ
γ
t
:= (cid:113)t t−1 and A
t
:= η t2+η t2
−1
>0
η2+η2
t t−1
Then,
(i) (γ t) t≥2 obeys the law of motion
A tγ
t
=Φ(γ t−1)−Φ(γ t) (γ-LoM)
(ii) If λ
0
>(1/2), then γ
t
↓ γ∞ ≥0.
(iii) If λ
0
<(1/2), then γ
t
↑ γ∞ ≤0.
Proof. Part(i). Subtracting (µ∗ -LoM) at time t−1 from (µ∗ -LoM) at time t yields:
(cid:18) (cid:19) (cid:18) (cid:19)
A tγ
t
=µ∗ t− µ∗
t−1
= c−Φ(γ t) − c−Φ(γ t−1) =Φ(γ t−1)−Φ(γ t)
Parts(ii)and(iii). Given the initial condition µ∗ =c− λ , defining γ :=Φ−1(λ )
1 0 1 0
yields µ∗ =c−Φ(γ ). Note that for all t ≥2,
1 1
γ t−1 ≥ γ t ⇐⇒ Φ(γ t−1)≥Φ(γ t) ⇐⇒ γ t ≥0
25Wlog, assume If λ > (1/2), then γ > 0. Suppose for a contradiction that γ < 0:
0 1 2
then γ < γ < 0, which is a contradiction since γ > 0 by definition. Hence,
1 2 1
γ ≥ γ ≥ 0. By induction, (γ ) is a monotone decreasing sequence bounded
1 2 t t
below by 0, so it converges: γ
t
↓ γ∞ ≥0, as desired.
Lemma5. For any A>0, let F :[0,∞)→[0,∞) be implicitly defined as follows:
A
AF (x)=Φ(x)−Φ(F (x))
A A
Then,
(i) Fix any x>0. F (x) is decreasing in A, and F (x)↓0 as A→∞.
A A
(ii) Fix any A>0. F (x) is increasing in x.
A
′
Proof. Part(i). Fix any x>0. For any A >A>0, note that
A′ F A′(x)− AF A(x)=Φ(F A(x))−Φ(F A′(x))
Suppose for a contradiction that F A′(x) ≥ F A(x). Since Φ is increasing, the above
implies
A′ F A′(x)≤ AF A(x)
′
which is a contradiction since A >A.
By an exactly analogous argument as in Lemma 4(ii), note that x > 0 implies
x≥ F (x)≥0, which implies Φ(x)−Φ(F (x))∈[0,Φ(x)−(1/2)] and hence
A A
(cid:20) (cid:21)
F (x)∈ 0,A−1(Φ(x)−(1/2)) ⇒ F (x)↓0 as A→∞
A A
as desired.
Part (ii). Let y = F (x) and use the implicit function theorem to diff erentiate it
A
with respect to x:
Ay′ =ϕ(x)− ϕ(y)y′
ϕ(x)
=⇒ y′ = >0,
A+ϕ(y)
where ϕ is a standard normal PDF.
Note that (γ-LoM) states: γ
t
=F
A
(γ t−1).
t
Lemma6. For any γ∗ >0, let A γ∗ :[γ∗ ,∞)→[0,∞) be defined as:
(cid:18) (cid:19)
Φ(x)−Φ (γ∗ +x)/2
A γ∗(x):=
∗
(γ +x)/2
Then, there exists
m>γ∗
such that A γ∗(x)>0 and
A′
γ∗(x)>0 for all
x∈(γ∗
,m).
26Proof. Note that
(cid:18) γ∗ +x(cid:19)−2(cid:18) γ∗ +x(cid:20)
1
(cid:18) γ∗ +x(cid:19)(cid:21) 1(cid:20) (cid:18) γ∗ +x(cid:19)(cid:21)(cid:19)
A′ γ∗(x)=
2 2
ϕ(x)− 2ϕ
2
−
2
Φ(x)−Φ
2
,
and
∗ ∗
γ ϕ(γ )
′ ∗
A γ∗(γ )=
2(γ∗ )2
>0.
SinceA′
γ∗ iscontinuous,
thereexistsm>γ∗ suchthatA′ γ∗(x)>0forallx∈[γ∗
,m).
SinceA
γ∗(γ∗
)=0holdsbydefinition,thisimpliesA
γ∗(x)>0forallx∈[γ∗
,m).
∗
Note that (γ-LoM) states: F
A γ∗(γ
t−1)(γ t−1)=(γ +γ t−1)/2.
Lemma7. For any γ >0, let A :[0,γ ]→[0,∞) be defined as
1 γ 1
1
Φ(γ )−Φ(x)
1
A (x):= .
γ
1 x
Then,A (γ )=0andA isdecreasingon[0,γ ]. Thus,foranyfixedγ∗ ∈(0,γ ),
γ 1 γ 1 1
1 1
A
(x)↑
A
(γ∗ )=:A¯∗
γ γ
1 1
holds as x↓ γ∗ .
Proof. Note that
(cid:20) (cid:18) (cid:19)(cid:21)
A′ (x)=x−2 − xϕ(x)− Φ(γ )−Φ(x) <0 ∀ x∈[0,γ ]
γ 1 1
1
Then A (x)↑ A¯∗ follows from continuity of A (·).
γ γ
1 1
Note that (γ-LoM) states: F (γ ) = x. With these lemmas in hand, we can
A (x) 1
γ1
understand how (γ ) changes as a function of (A ) .
t t t t
A.2 Theorem1andCorollary2. Without loss of generality, assume λ > 1.
0 2
Proof of Theorem 1 (i). Suppose that η−2 ≤ Ctp holds for all t ≥ T. Then we have
t
η−2 ≤ Ctp =⇒ η2 ≥ C−1t−p =⇒ η2+η2 ≥2η2 ≥2C−1t−p
t t t t−1 t
=⇒ (η2+η2 )−1/2 ≤(C/2)1/2tp/2
t t−1
for t ≥ T. By Lemma 3 (iii), µ∗ − µ∗ is decreasing in t, which implies
t+1 t
(cid:88)t
µ∗ − µ∗ ≤ t−1 (µ∗ − µ∗ )=t−1(µ∗ − µ∗ )≤ t−1.
t+1 t s+1 s t+1 1
s=1
27∗
Using this inequality, we can apply the squeeze theorem and compute µ∞ as fol-
lows:
∗ − ∗
µ µ
0≤ (cid:113)t+1 t ≤(C/2)1/2tp/2−1
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
η2+η2 (cid:124) (cid:123)(cid:122) (cid:125)
t t+1 →0 ∵p<2
 
=⇒ µ∗ ∞
=c−Φ 
   lim
(cid:113)µ∗ t+1− µ∗
t
 
   =c−(1/2)
  t→∞
η2+η2
 

t t+1
and the conclusion follows from Lemma 3 (iv).
Proof of Theorem 1 (ii). Throughout this proof, fix any q > 2 and (wlog) λ > 1/2.
0
When convenient, we omit dependencies on these variables. Let “p.m.” denote
“on a set of positive (Lebesgue) measure.”
Step1. First, we show the following lemma.
Lemma8. There exists C >0 such that, for any T ≥1,
η t−2 ≥ Ctq ∀ t ≥ T =⇒ λ∞(θ)=NRD(θ) p.m.
Proof of Lemma 8. Let γ
:=Φ−1(λ
) and define the following expressions:
1 0
2−q
q
(cid:88)∞ (cid:90) t=∞
1
L:= , r := >1, S := t−r ≤ t−r dt = <∞ ,
2−q+1 2 r−1
t=1
t=2
(cid:34) (cid:35)2
1ϕ(γ )
K :=
1
, C :=LK, C
:=C−1
2 S
Note that C does not depend on t. We begin with the case T =1. Fix any Σ with
induced posterior precisions (η−2) such that η−2 ≥ Ctq for all t ≥ 1. Then the
t t t
following implications hold:
η−2 ≥ Ctq ∀ t ≥1
t
=⇒ η2 ≤ Ct−q ∀ t ≥1
t
=⇒ η2+η2 ≤ C(t−q+(t−1)−q) ∀ t ≥2
t t−1
=⇒ (A )2 :=η2+η2 ≤ Kt−q ∀ t ≥2
t t t−1
1ϕ(γ )
=⇒
A
≤ 1 t−r ∀
t
≥2
t
2 S
where the third implication follows from
C
2−q t−q
=L= ≤ ∀ t ≥2
K
2−q+1 t−q+(t−1)−q
28since
(cid:18) (cid:19)
sign(L′
(t))=sign
(t−1)−1− t−1
=1.
Next, for each ϵ∈(0,γ ) and each t ≥2, define:
1
∆ (ϵ):=
γ 1− ϵ
t−r,
γ (ϵ):=γ
−(cid:88)t
∆ (ϵ), A (ϵ):=
Φ(γ t−1(ϵ))−Φ(γ t(ϵ))
t t 1 t t
S γ (ϵ)
t
s=2
with the initial condition γ (ϵ)=γ . Observe that γ (ϵ)↓ ϵ as t →∞ and
1 1 t
Φ(γ t−1(ϵ))−Φ(γ t(ϵ))≥ ϕ(γ t−1(ϵ))(γ t−1(ϵ)− γ t(ϵ))
γ (ϵ)
≥ ϕ(γ )∆ (ϵ)≥ t ϕ(γ )∆ (ϵ) ∀ t ≥2
1 t 1 t
γ
1
ϕ(γ )
=⇒ 1 ∆ (ϵ)≤ A (ϵ) ∀ t ≥2
t t
γ
1
because (γ (ϵ)) is decreasing, and γ (ϵ)<γ holds for all t ≥2. Also note that
t t t 1
ϕ(γ )γ − ϵ ϕ(γ )
1 1 1
lim =
ϵ↓0 γ
1
S S
Choose ϵ>0 small enough, independent of t, such that
1ϕ(γ ) ϕ(γ )γ − ϵ
A ≤ 1 t−r ≤ 1 1 t−r ≤ A (ϵ) ∀ t ≥2.
t t
2 S γ S
1
By Lemma 5, we can show γ ≥ γ (ϵ) for t ≥2 inductively. Hence,
t t
γ∞ = tl →im ∞F
A
t(γ t−1)≥ tl →im ∞F
A
t(ϵ)(γ t−1(ϵ))=ϵ>0
where γ∞ is the limit of (γ t)
t
induced by Σ.
Next, fix any T ≥1, and let C be the constant derived above. Fix any Σ satisfying
η−2 ≥ Ctq for all t ≥ T, and define:
t
(cid:34) (cid:35)−2
1ϕ(γ )
C˜ (Σ):=L−1 T ≤
C
2 S
where the inequality follows because γ ≤ γ . Hence, Σ satisfies
T 1
η−2 ≥ C{ t˜+(T −1)}q ≥ C(t˜)q ≥ C˜ (Σ)(t˜)q ∀ t˜≥1
t˜
where t˜:=t−(T −1). By an exactly analogous argument as the T =1 case, γ∞ >0
holds as desired.
29Step2. To finish the argument, we want to show:
η t−2 =Ω(tq) =⇒ λ∞(θ)=NRD(θ) p.m.
Fix any C > 0, T ≥ 1, and Σ. Fix any q′ ∈ (2,q), and choose T′ ≥ T large enough
such that
C(T′ )q−q′ ≥ C(q′ ,λ ),
0
′
where C(q ,λ ) is the constant from Lemma 8. By Lemma 8, the following impli-
0
cations hold:
η−2 ≥ Ctq =Ctq−q′ · tq′ ∀ t ≥ T
t
=⇒ η−2 ≥ C(q′
,λ
)tq′ ∀
t
≥ T′
t 0
=⇒ λ∞(θ)=NRD(θ) p.m.
Proof of Corollary 2. Part(i). Ifσ =σ forallt
≥1,thenη−2 =σ−2t.
TakeC
:=σ−2
t t
and p:=1, and the conclusion directly follows from Theorem 1 (i).
Part(ii). If σ 1 :=σ and σ t :=η t−1 for all t ≥2, then
η−2 =σ−22t−1
t
Fixq=3andanyλ ,andletC betheconstantfromLemma8whenq=2. Choose
0
σ >0 small enough such that
σ−22t−1 ≥ Ct3 ∀ t ≥1,
and note that η−2 ≥ σ−22t−1 for all σ < σ. The conclusion then follows from
t
Lemma 8.
A.3 Proposition1.
Proof. Without loss of generality, we assume λ > 1.
0 2
Parts (i) and (ii): First, we prove Part (ii). Fix Σ ≥ Σ′ . Then it follows that
(cid:113)
(cid:112)
A := η2+η2 ≥ (η′ )2+(η′ )2 =: A′ for all t ≥ 2. Let (γ ) and (γ′ ) be the
t t t−1 t t−1 t t t t t
induced sequences, respectively. We use induction to show that γ ≤ γ′ for all
t t
t ≥ 1. For the base case, note that γ = Φ−1(λ ) = γ′ . For the inductive step,
1 0 1
assume γ ≤ γ′ . By Lemma 5,
t t
γ =F (γ )≤ F ′ (γ )≤ F ′ (γ′ )=γ′
t+1 A t+1 t A t+1 t A t+1 t t+1
Hence, γ∞ ≤ γ∞′ which implies
µ∗
∞
=c−Φ(γ∞)≥ c−Φ(γ∞′ )=(µ∗ ∞)′
30as desired.
′
To prove Part (i), assume λ >(1/2) and Σ=Σ . Then,
0
λ ≥ λ′ =⇒ γ ≥ γ′ =⇒ γ =F (γ )≥ F (γ′ )=γ′
0 0 1 1 2 A 2 1 A 2 1 2
=⇒
γ t
≥
γ
t′ ∀
t
=⇒
γ∞
≥ γ∞′ =⇒ µ∗
∞
≤(µ∗ ∞)′
,
where we use the same inductive argument as above.
Part(iii). Letη2 andη˜2 denotethetime-t
posteriorvariancesunderΣandΣs↔s′
,
t t
respectively. Then, it immediately follows that η2 ≥ η˜2 holds for all t. Hence, by
t t
an analogous argument as in Part (i), we have µ∗ (Σ,λ ) ≥ µ∗ (Σs↔s′ ,λ ) holds for
t 0 t 0
all t, which implies µ∗ ∞(Σ,λ 0)≥ µ∗ ∞(Σs↔s′ ,λ 0).
A.4 Proposition2. We prove Proposition 2 in two steps. First, we show that for
any monotone selection rule with threshold µ∗ ∈(c− λ ,c−(1/2)), there exists an
0
exponential sequence (A t) t with A t ↓0 as t →∞ that implements it, i.e., µ∗ ∞ =µ∗ .
Second, we show that there exists a noise process Σ = (σ ) that implements the
t t
sequence (A ) constructed in the first step.
t t
Proof. Without loss of generality, we assume λ > 1.
0 2
Step 1. Fix any µ∗ ∈ (c− λ ,c−(1/2)). Our goal is to construct an exponential
0
sequence (A ) that satisfies A ↓0 as t →∞, A2 − A2 ≤ A2− A2 for t ≥2, and
t t t t+1 t+2 t t+1
∗ ∗
µ∞ =µ .
By taking t →∞ for (µ∗ -LoM), we see that
∗
µ∞+Φ(γ∞)=c
and hence
µ∗ ∞ =µ∗ ⇐⇒ γ∞ =γ∗ :=Φ−1(c− µ∗ )∈(0,γ 1)
where γ :=
Φ−1(λ
). Hence, it
suffi
ces to find an exponential sequence (A )
1 0 t t
whose induced (γ ) via (γ-LoM) satisfies γ ↓ γ∗ . By Lemma 7, we can choose
t t t
γ
∈(γ∗
,γ ) close enough to
γ∗
such that
2 1
√
A
2
:=A
γ
(γ 2)> 2A γ∗(γ 2)>0
1
because i) A
γ
(γ∗ )=A¯∗ >0=A γ∗(γ∗ ); ii) A γ∗(γ 2)>0 for γ
2
∈(γ∗ ,m); and iii) A
γ
(·)
1 1
and A γ∗(·) are continuous. For all t ≥3, we define
Φ(γ t−1)−Φ(γ t) γ∗ +γ t−1
A
t
:=A γ∗(γ t−1)= and γ
t
:= ,
γ 2
t
which implies
− ∗
γ γ
2 ∗
γ = +γ .
t 2t−2
31By construction, γ ↓ γ∗ and A ↓ 0. It remains to show that (A ) is exponential.
t t t t
Note that for all t ≥3,
γ t−1− γ
t
=γ t− γ∗ =4(γ 2− γ∗ )2−t.
Since ϕ is decreasing on (0,∞), we have
ϕ(γ 2)(γ t−1− γ t)≤ ϕ(γ t−1)(γ t−1− γ t)≤Φ(γ t−1)−Φ(γ t)
≤ ϕ(γ t)(γ t−1− γ t)≤ ϕ(γ∗ )(γ t−1− γ t),
wherethesecondandthethirdinequalitiesfollowfromthemeanvaluetheorem.
Hence,
4ϕ(γ 2)
(γ − γ∗ )2−t ≤
Φ(γ t−1)−Φ(γ t)
≤ A
2 t
γ γ
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)2(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) 2
(cid:124) (cid:123)(cid:122) (cid:125)
=:c
≤
Φ(γ t−1)−Φ(γ t) ≤4ϕ(γ∗ )
(γ − γ∗ )2−t
∗ ∗ 2
γ γ
(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)
(cid:124) (cid:123)(cid:122) (cid:125)
=:C
as desired, where the second and the third inequalities follow from γ∗ < γ ≤ γ
t 2
for all t ≥2.
Finally,wewanttoshowthatthesequence(A ) constructedabovesatisfiesA2 −
t t t+1
A2
t+2
≤ A2
t
− A2
t+1
for t ≥ 2 if √we take γ
2
suffi ciently close to γ 2. We defined the
sequence of A so that A ≥ 2A holds. Hence, we have
t 2 3
√
A ≥ 2A =⇒ A2 ≥2A2 ≥2A2− A2 =⇒ A2− A2 ≤ A2− A2
2 3 2 3 3 4 3 4 2 3
as desired. For t ≥3, note that
c2−t ≤
A
≤ C2−t
t
(cid:32) (cid:33) (cid:32) (cid:33)
=⇒ c2−
C2
2−2t ≤ A2− A2 ≤ C2−
c2
2−2t
4 t t+1 4
Thus, a suffi cient condition for A2 − A2 ≤ A2− A2 is
t+1 t+2 t t+1
(cid:32)
C2−
c2(cid:33)
2−2(t+1)
≤(cid:32)
c2−
C2(cid:33)
2−2t ⇔ C =
ϕ(γ∗ )/γ∗ ≤(cid:114)
17 ,
4 4 c ϕ(γ )/γ 8
2 2
∗
which is guaranteed by choosing γ close enough to γ .
2
Step2. To complete the proof of Proposition 2, we show that there exists a noise
processΣ=(σ ) thatimplementsthesequence(A ) constructedabove. First,we
t t t t
claim that it suffi ces to find a non-negative, weakly decreasing sequence (η2) of
t t
posteriorvariances. Thisistruebecausegivenanysuch(η2) ,thesequence(η−2)
t t t t
32is non-negative and weakly increasing. Thus, (η2) is implemented by the noise
t t
process
σ−2 :=η−2, σ−2 =η−2− η−2 ∀
t
≥2
1 1 t t t−1
Thus, it suffi ces to find a non-negative and weakly decreasing sequence (η t2) t≥1
such that
A2 =η2+η2 ∀ t ≥2
t t t−1
Notethatanychoiceofη 12 pinsdownacandidatesolution(η t2) t≥1 byη t2
+1
=A2 t+1−
η2. We therefore proceed by stating the implied constraints on η2.
t 1
Step2.1. Let us begin by establishing the formula
(cid:88)t
η2(η2):=(−1)t+1η2+ (−1)t+sA2
t 1 1 s
s=2
This follows by induction. For the base case, note that η2 = A2 − η2. For the
2 2 1
inductive step, suppose the formula holds for η2. Then,
t
(cid:88)t
η2 (η2)=A2 − η2(η2)=A2 +(−1)(t+1)+1η2+ (−1)(t+1)+sA2
t+1 1 t+1 t 1 t+1 1 s
s=2
(cid:88)t+1
=(−1)t+2η2+ (−1)(t+1)+sA2
1 s
s=2
since 2(t+1) is even.
Step 2.2. Next, we pin down the implied constraints on η2. The base case con-
1
straints are:
A2
0≤ A2− η2 ≤ η2 ⇐⇒ L := 2 ≤ η2 ≤ A2 =:U
2 1 1 1 2 1 2 1
For every t ≥2, we have the constraint
0≤ η2 ≤ η2
t+1 t
We can split this set of constraints into two cases.
Case 1. If t is even, then the time-t constraint from above is
(cid:88)t+1 (cid:88)t A2
(−1)t+sA2 ≤ η2 ≤ (−1)t+sA2− t+1
s 1 s 2
s=2 s=2
which is equivalent to
L :=(A2− A2)+···+(A2− A2 )≤ η2 ≤(A2− A2)+···+(A2−(A2 /2))=:U
t 2 3 t t+1 1 2 3 t t+1 t
33Case 2. If t is odd, then the time-t constraint from above is:
(cid:88)t A2 (cid:88)t+1
(−1)(t+1)+sA2+ t+1 ≤ η2 ≤ (−1)(t+1)+sA2
s 2 1 s
s=2 s=2
which is equivalent to:
L :=(A2− A2)+···+(A2 − A2)+(A2 /2)≤ η2
t 2 3 t−1 t t+1 1
≤(A2− A2)+···+(A2 − A2)+A2 =:U
2 3 t−1 t t+1 t
Note that for odd t, L t ≥ L t−1 and for even t, U t ≤ U t−1. Hence, only the lower
bounds for odd t and the upper bounds for even t have bite.
Step 2.3. Finally, we show that the previously derived properties of (A ) im-
t t
ply that there exists η2 satisfying the above constraints, and hence pins down a
1
solution Σ. First, note that (L ) are increasing, since for any odd t ≥1,
t todd
A2 A2
L ≤ L ⇐⇒ t+1 ≤ A2 − A2 + t+3
t t+2 2 t+1 t+2 2
⇐⇒ A2 − A2 ≤ A2 − A2
t+2 t+3 t+1 t+2
whichholdsbyStep1. Ananalogousargumentimpliesthat(U ) aredecreas-
t teven
ing. Second, note that A2 decreasing implies L ≤ U for any odd t. Hence, by
t t t+1
the nested intervals theorem, we have:
∞
(cid:88)
supL =η2 = (A2 − A2 )= inf U
t 1 2s 2s+1 t
teven
todd
s=1
and this sum converges because
(cid:32) (cid:33) (cid:32) (cid:33)
c2−
C2
2−4s ≤ A2 − A2 ≤ C2−
c2
2−4s
4 2s 2s+1 4
(cid:32) (cid:33) ∞ (cid:32) (cid:33)
1 C2 (cid:88) 1 c2
⇒ c2− ≤ (A2 − A2 )≤ C2− .
15 4 2s 2s+1 15 4
s=1
√
Note that η2 is positive because C/c≤ 17/8 implies c2− C2/4>0.
1
A.5 Proposition3.
Proof. Without loss of generality, we assume λ > 1. Part (i). Fix any θ ∈ (c−
0 2
λ ,c−(1/2)). For any σ > 0, define T(σ) ∈ N implicitly as µ∗ ≤ θ < µ∗ .
0 T(σ) T(σ)+1
Note that T(σ) is well-defined because for any fixed σ >0, µ∗ ↑ c−(1/2).
t
First, we show that T(σ) → ∞ as σ ↓ 0. Notice that T(σ) is weakly decreasing in
σ because for any σ′ > σ, θ ≥ µ∗ (σ′ ) > µ∗ (σ) implies T(σ) ≥ T(σ′ ) by the
T(σ′) T(σ′)
34definition of T(σ). Hence, T(σ) either converges to some constant or diverges to
∞asσ ↓0. Supposeforacontradictionthatlim σ↓0T(σ)=T holdsforsomeT >0.
Since T(σ)↑ T as σ ↓0, for any σ, we have
∗ ∗
µ (σ)>µ (σ)>θ,
T+1 T(σ)+1
which implies lim σ↓0µ∗ T+1(σ)≥ θ. This contradicts lim σ↓0µ∗ T+1(σ)=c− λ 0.
Next, fix any α ∈(0,1). For any 0≤ t ≤ αT(σ), we have:
T(σ)−1 T(σ)−1 (cid:114)
(cid:88) (cid:88) 1 1
θ− µ∗ >µ∗ − µ∗ = (µ∗ − µ∗ )= σ + Φ−1(c− µ∗ )
t T(σ) t s+1 s s+1 s s+1
s=t s=t
√  
=⇒ λ t =Φ(cid:32) t(θ σ− µ∗ t)(cid:33) >Φ      T (cid:88)(σ)−1(cid:114) s+t 1 + st Φ−1(c− µ∗ s+1)      
s=t
(cid:32) (cid:114) (cid:33)
≥Φ (T(σ)− t) t + t Φ−1(c− θ)
T(σ) T(σ)−1
(cid:32) (cid:114) (cid:33)
1 1
≥Φ (1− α)T(σ) + Φ−1(c− θ) →1 as σ ↓0.
T(σ) T(σ)−1
Since the lower bound on θ− µ∗ holds uniformly for t ∈[0,αT(σ)], for any ϵ >0,
t
there exists σ >0 such that for all σ <σ and all t ∈[0,αT(σ)], λ ≥1− ϵ.
α α t
Finally, fix any β ∈(1,β), where
β :=c+(1/2)− θ− δ and δ=(c−(1/2)− θ)/2
and note that, for any t ∈[βT(σ),∞),
βT(σ)−1 (cid:114)
(cid:88) 1 1
θ− µ∗ ≤ θ− µ∗ <µ∗ − µ∗ =− σ + Φ−1(c− µ∗ )
t βT(σ) T(σ)+1 βT(σ) s+1 s s+1
s=T(σ)+1
 
βT(σ)−1 (cid:114)
=⇒ λ t <Φ      − (cid:88) s+t 1 + st Φ−1(c− µ∗ s+1)      
s=T(σ)+1
 (cid:115) 
<Φ  
 

−(cid:18) (β−1)T(σ)−1(cid:19)
1+
βTβ (T σ( )σ −) 1Φ−1(c− µ∗ βT(σ))  
 


 (cid:115) 
<Φ  
 

−(cid:18) (β−1)T(σ)−1(cid:19)
1+
βTβ (T σ( )σ −) 1Φ−1(c− θ−(β−1))  
 


→0 as σ →0
35where the inequality follows from
βT(σ)−1 βT(σ)−1
(cid:88) (cid:88) 1
µ∗ − µ = (µ∗ − µ∗ )≤
βT(σ) T(σ) s+1 s s
s=T(σ) s=T(σ)
1
≤(β−1)T(σ) =β−1
T(σ)
1
=⇒ c− µ∗ ≥ c− θ−(β−1)>c− θ−(β−1)>
βT(σ) 2
=⇒ Φ−1(c− µ∗ )>Φ−1(c− θ−(β−1))>0.
βT(σ)
Since this upper bound on θ− µ∗ holds uniformly for t ∈ [βT(σ),∞), there exists
t
σ >0 such that for all σ <σ and all t ∈[βT(σ),∞), λ <ϵ.
β β t
Finally, define σ :=min{ σ ,σ }.
α β
Part(ii). For any t ≥1, we have
(cid:12) (cid:12) (cid:32) θ− µ∗ (cid:33) (cid:32) θ− µ∗(cid:33)(cid:12) (cid:12)
| λ − λ |=(cid:12) (cid:12)Φ √ t+1 −Φ √t (cid:12) (cid:12)
t+1 t (cid:12) (cid:12)
(cid:12) σ/ t+1 σ/ t (cid:12)
(cid:12) √ √ √ √ (cid:12)
≤ √ 1 (cid:12) (cid:12)( t+1− t)θ− t+1µ∗ + tµ∗(cid:12) (cid:12)
(cid:12) t+1 t(cid:12)
2πσ
√ √ √ √
t+1−
t
t+1µ∗ − tµ∗
≤ √ | θ|+ √t+1 t
2πσ 2πσ
√ √ √ √ √
= t √+1− t | θ|+ √t+1 (µ∗ − µ∗ )+ ( t+ √1− t)µ∗ t
t+1 t
2πσ 2πσ 2πσ
√ √ √ √ √
t+1− t t+1 c( t+1− t)
< √ | θ|+ √ + √
2πσ 2πσt 2πσ
√ √
1 (cid:110) (cid:111)
≤ √ ( 2−1)(| θ|+c)+ 2 ,
2πσ
√
wherethesecondlinefollowsfromΦ′ (·)≤1/ 2π,andthefifthlinefollowsfrom
µ∗ <candµ∗ − µ∗ <t−1. Hence,foranyϵ,thereexistsσ >0suchthat| λ − λ | <ϵ
t t+1 t t+1 t
holds for all t ≥ 1 and σ > σ. Note that this argument does not depend on the
value of θ as long as | θ| <∞ holds.
36AppendixB: Signalsaboutpastplay
Inthemaintext,weclaimedthatourmodelisequivalenttooneinwhichplayers
receive noisy signals of past play. We now formalize this claim by considering a
single modification to our model: at each time t, each player i ∈ [0,1] observes
the independent signals
(cid:18) (cid:19) (cid:18) (cid:19)
x
it
∼ N θ,σ t2 and y
it
∼ N Φ−1(λ t−1),τ t2 for t >1
This formulation follows that of Dasgupta (2007) and Trevino (2020).
Proposition4. (λ ) is consistent with some Σ under our main model if and only
t t
if (λ t) t is consistent with some (τ t2,σ t2) t≥1 under the Appendix B model.
Proof. Notice that the environment in the main text can be viewed as the case
in which τ2 = +∞ for all t ≥ 0. Also observe that the case in which σ2 < +∞,
t 1
σ2 =+∞ for all t >1 can be viewed as a social learning environment: at the start
t
ofthegame,eachplayerreceivesanindependentsignalofthestateand,overthe
course of the game, the path of play also aggregates information about the state.
We now show that the dynamics of the game induced by the path (τ t2,σ t2) t≥1
can be replicated by some alternate path
(σ′2
t) t≥1 in our main model in which
past play is unobserved. Let
η−2
and
η′−2
be the posterior precisions induced by
t t
(τ t2,σ t2) t≥1 and Σ′ , respectively.
As before, let η2 be the posterior variance at time t for t ≥ 2 with the initial
t
conditions µ =x and η2 =σ2. Then, the law of motion is unchanged from the
i1 i1 1 1
main model and given by Lemma 1.
We can show the law of motion of µ and η2 and (λ-LoM) by induction. Define
it t
I
is
= (x is,y is) s≤t. At t = 1, agent i’s posterior is given by θ| I
i1
∼ N(x i1,σ 12), which
implies µ =x and η2 =σ2. Since λ is common knowledge, she takes a =1 if
i1 i1 1 1 0 i1
and only if µ ≥ c− λ =µ∗ . Since µ | θ ∼ N(θ,η2), λ is given by
i1 0 1 i1 1 1
(cid:32) − ∗ (cid:33)
θ µ
λ =Pr(x ≥ µ∗ )=Φ 1 .
1 i1 1 σ
Nowsupposethatthestatementholdsfort−1. Byassumption,wehaveΦ−1(λ t−1)=
(θ− µ∗ t−1)/η t−1, which implies
µ it−1| θ ∼ N(θ,η t2 −1),
x | θ ∼ N(θ,σ2),
it t
(η t−1y it+µ∗ t−1)| θ ∼ N(θ,τ t2η t2 −1).
Since they are independent of each other conditional on θ, Bayesian updating
37yields η−2 =σ−2, µ =x , and for each t ≥2,
1 1 i1 i1
η−2 =η−2 (1+τ−2)+σ−2
t t−1 t t
µ
=(cid:88)t (cid:32) η s− −2 1τ s−2
y˜ +
σ s−2
x
(cid:33)
it η−2 is η−2 is
s=1 t t
where
τ−2
:=0. This implies:
1
(cid:88)t (cid:89)t
η−2
=
σ−2 (1+τ−2)
t s r
s=1 r>s
∗
Since (µ -LoM) remains unchanged from Lemma 1, we obtain (λ-LoM) by the
′
proofofLemma1. Then,wecanfindasequence(σ ) suchthatthedynamicsare
t t
equivalent:
σ′−2 :=η−2− η−2
t t t−1
38AppendixC: FinitePlayerApproximation
Considerthefollowinggame,whichwedenoteastheN-playermodel,incontrast
to the main text’s continuum model. There are N players, indexed by i ∈ [N].
Actions, timing, and information are the same as in the main text.30 Agent i’s
ff
payo s at time t are:

  θ+λ N,−i,t−1 if a it =1
u(a it,λ N,−i,t−1,θ):=

 c if a =0
it
where
1 (cid:88)
λ N,−i,t−1 =
N
−1 1[a j,t−1 =1]
j∈[N]\{i}
istheempiricalproportionofnon-i agentswhotooktheriskyactionattimet−1
for t >1, and λ N,−i,t−1 :=λ 0 is common knowledge. Let
1
(cid:88)N
λ := 1[a =1]
N,t it
N
i=1
be the empirical proportion of all agents who attack at time t.
The purpose of Appendix C is to show that the dynamics of the N-player model
converge to the dynamics of the continuum model as N becomes large. Let us
brieflydiscussthisexercise. Withafinitenumberofplayers,thereisrandomness
intheempiricaldistributionofbeliefsattime-t. This,inturn,translatesintoran-
domness in the path of aggregate play. Theorem 2 shows that in an environment
with many players, this randomness washes out.
To show Theorem 2, we begin by analyzing the dynamics of belief thresholds in
the N-player model.
Lemma9. Ateachtimet ≥1,eachagenti ∈[N]takestheriskyactionifandonly
ifµ it ≥ µ∗ t,where(µ∗ t) t≥1 isthesamesequenceofthresholdsasinthemainmodel.
Proof. Base case: At time t = 1, each agent i takes the risky action iff µ = x ≥
i1 i1
c− λ =:µ∗ .
0 1
Inductive step: Fixt >1andassumethatattimet−1,eachagenti takestherisky
action iff µ i,t−1 ≥ µ∗ t−1. At time t, each agent i takes the risky action iff
(cid:20) (cid:21) 1 (cid:88) (cid:18) (cid:19)
E it θ+λ N,i,t−1 =µ it+
N
−1 P it µ j,t−1 ≥ µ∗ t−1 ≥ c
j(cid:44)i
30Definetherandomvariables(x it) i≥1,t≥1 onthesameprobabilityspace.
39Note that at time t, agent i believes θ ∼ N(µ it,η t2) and for each j (cid:44) i, µ j,t−1| θ ∼
N(θ,η2 ). Hence, at time t agent i believes
t−1
µ j,t−1 ∼ N(µ it,η t2 −1+η t2)
ff
which implies she takes the risky action at time t i
 
µ it+Φ      (cid:113)µ it− µ∗ t−1       ≥ c ⇐⇒ µ it ≥ µ∗ t
 
 η2 +η2
t−1 t
as desired.
With this lemma in hand, observe that
λ =1− F (µ∗ ) and λ (θ)=1− F (µ∗ )
N,t N,t t t θ,t t
where F is the empirical CDF of N draws of µ and F is the CDF of µ ∼
N,t it θ,t it
N(θ,η2).
t
Theorem2. For any (Σ,λ ) and θ ∈R,
0
sup| λ N,∞− λ∞(θ)|=0 P θ−a.s.
N≥1
Proof. For each N ≥ 1 and each i ∈ [N], note that µ i,∞ = θ P θ-a.s. by Doob’s
theorem. Hence, λ N,∞ =λ∞(θ) P θ-a.s.
40AppendixD: AlternativeInterpretationsofModel
Overlapping generations model. There is an initial unit mass of agents with
proportion λ ∈ (0,1) who take the risky action, all of whom share a common
0
improper uniform prior about θ. At each time t ≥ 1, a unit mass of children is
born (generation t), each of whom inherits her prior about θ from her parent’s
(generation t−1) belief. Each child i receives a signal x ∼ N(θ,σ2), updates her
it t
belief about θ, chooses an action, and gets uniformly matched to play the static
game below with a member of generation t−1. Finally, child i becomes parent
i and continues to take her time-t action whenever matched with an agent from
generationt+1. Weinterpretthisasduetoswitchingcosts—sinceagents’actions
ff
when playing against the future generation are not payo -relevant, switching
costs ensure this.
Two-playerinterpretation. Considerthesetofstaticsymmetrictwo-playergames,
indexed by the state θ ∈R, whose payoff matrix is
A N
(cid:18) (cid:19)
A (θ+1,θ+1) (θ,c)
N (c,θ) (c,c)
Notethatforθ ∈(c−1,c),therearetwopurestrategyequilibria: (A,A)and(N,N).
ApplyingHarsanyi,Seltenetal.(1988)’sriskdominancecriterionselects(A,A)if
and only if θ ≥ c−(1/2).
Carlsson and Van Damme (1993) propose a static global games framework in
which two players play the incomplete information game with state-dependent
matrix from above.31 Consider the case where both players share a common im-
properuniformprior, andeachreceiveiidGaussiansignalsdistributedN(θ,σ2).
CarlssonandVanDamme(1993)thenshowthatasσ ↓0,theriskdominantequi-
librium is selected.
Inthissection,weshowatightconnectionbetweentheabovestaticglobalgames
setting and our model where Σ=(σ2,∞ ,∞ ,...), which we study in Example 1. In
∗
this case, (µ ) evolves according to the law of motion
t t
(cid:32) ∗− ∗ (cid:33)
µ µ
µ∗ +Φ t√ t−1 =c
t
2σ
with µ∗ = c− λ . As we have shown, this equation has a unique fixed point at
1 0
c−(1/2) and hence we have global convergence µ∗ → c−(1/2) for any λ ∈ (0,1).
t 0
∗
Furthermore,theentirepathof(µ ) correspondstotheiteratedbest-responsedy-
t t
namicswithsymmetricswitchingstrategiesinCarlssonandVanDamme(1993)’s
∗
two-player setting with the informational environment from above, in which µ∞
31Ourstatictwo-playergamecorrespondstoFigure1ofCarlssonandVanDamme(1993)with
x=4(c−θ),α =α =N,andβ =β =A.
1 2 1 2
41is the unique rationalizable switching strategy. To see this, let P (θ,x′ ) denote
x
′
the joint posterior over the state θ and my opponent’s signal x , conditional on
my signal x.
The key observation is that, if my opponent follows a switching strategy with
ff ′ ff
cuto µ , my expected payo from A given x is
(cid:18) (cid:19)
P (x′ ≥ µ′ ) E [θ| x′ ≥ µ′ ]+1 +P (x′ <µ′ )E [θ| x′ <µ′ ]
x x x x
(cid:90)
=E [θ]+P (x′ ≥ µ′ )=x+ P(x′ ≥ µ′| θ,x)dP (θ)=x+E [λ(θ)]
x x x x
θ
ff ∗
which proves that if my opponent follows a switching strategy with cuto µ ,
t−1
ff ∗
my best response is to follow a switching strategy with cuto µ , as desired.
t
An analogous argument allows us to extend the interpretation of our model dy-
namics as two-player iterated joint learning and best-response dynamics by con-
sidering our model for arbitrary Σ. In this case, note that if my opponent re-
ceivessignaldistributedN(θ,η2 )(correspondingtothetime-(t−1)distribution
t−1
of posterior means, conditional on the state) and my belief is about the state is
N(µ ,η2), then my (state-unconditional) belief about my opponent’s signal is
it t
 
x′ ∼ N(µ it,η t2 −1+η t2) =⇒ P it(x′ ≥ µ′ )=Φ      (cid:113)µ it− µ′      

 
 η2 +η2
t−1 t
and hence our general setting is also consistent with iterated joint learning and
best-response dynamics. This can be interpreted as best-response dynamics with
bounded rationality, since at each time each player is only able to take into ac-
count one extra signal.
42AppendixE: Payoffextensions
ff
In the main text, we assumed that the payo of taking action 1 is the sum of the
stateandthemeasureofagentswhoplayedaction1inthepreviousperiod. Now
ff
we relax this assumption and suppose agent i’s payo s at time t are:

  αθ+(1− α)λ t−1 if a it =1
u(a it,λ t−1,θ):=

 c if a =0
it
with α ∈(0,1). The parameter of α can be interpreted as the inverse of the extent
of intertemporal strategic complementarity. To simplify the notations, we define
the followings:
α
˜
θ(α):= θ
1−
α
c
c˜(α):=
1−
α
(cid:18)
α
(cid:19)2
σ˜2(α):= σ2
t 1−
α
t
(cid:18)
α
(cid:19)2
η˜2(α):= η2.
t 1−
α
t
In this environment, we define the risk dominant action at state θ as
(cid:20)
c
11− α(cid:21)
RD(θ):=1
θ
≥ − =1 [θ˜ (α)≥ c˜(α)−(1/2)]
α 2 α
i.e.,theactionthatonewouldtakeifsheknewthestateandconjecturedthathalf
ofallagentstaketheriskyaction. Analogously,wedefinethenon-riskdominant
action as NRD(θ):=1− RD(θ).
Proposition 5. All the results in the main text hold by replacing c, σ2, η2, and
(cid:12) t t (cid:12)
β¯ =1+| c−(1/2)− θ| /2 with c˜(α), σ˜2(α), η˜2(α), and β˜:=1+(cid:12) (cid:12)c˜(α)−(1/2)− θ˜ (α)(cid:12) (cid:12)/2,
t t
respectively.
∗
Proof. We show (µ -LoM) and (λ-LoM) by induction:
 
µ∗
t+Φ 
 

 (cid:113)
µ∗ t− µ∗
t−1
 
 


=c˜(α)
 
 η˜2 (α)+η˜2(α)
t−1 t
(cid:110) (cid:111)
(cid:32)
θ˜ (α)−
µ∗(cid:33)
λ (θ)=1 θ˜ (α)≥ µ∗ =Φ t
t t η˜ (α)
t
At t = 1, agent i’s posterior belief is given by θ| I ∼ N(x ,σ2), which implies
i1 i1 1
θ˜ (α)| I ∼ N(µ˜ ,σ˜2(α)) with µ˜ := E [θ˜ (α)] = αx /(1− α) =: x˜ . Since λ is
i1 i1 1 i1 i1 i1 i1 0
43common knowledge, she takes a = 1 if and only if µ˜ ≥ c˜− λ = µ∗ . Since
i1 i1 0 1
µ˜ | θ˜ (α)∼ N(θ˜ (α),σ˜2(α)), λ is given by
i1 1 1
(cid:32) µ∗ − θ˜ (α)(cid:33) (cid:32) θ˜ (α)− µ∗ (cid:33)
λ =P (µ˜ ≥ µ∗ )=1−Φ 1 =Φ 1 .
1 θ i1 1 σ˜ (α) σ˜ (α)
1 1
Hence, (λ-LoM) holds for t =1.
Now, for t > 1, suppose that a j,t−1 = 1 if and only if µ˜ j,t−1 ≥ µ∗ t−1. By standard
Gaussian-Gaussian Bayesian updating, we have θ˜ (α)| I ∼ N(µ˜ ,η˜2(α)), where
it it t
µ˜
it
=
η˜−2
(η α˜ t− )−2
1
+(α σ˜)
−2(α)µ˜ i,t−1+
η˜−2
(σ α˜ t− )2 +(α σ˜)
−2(α)x˜
it
=η˜
t2(α)(cid:88)t
σ˜ s−2(α)x˜
is
t−1 t t−1 t s=1
 −1
η˜2 (α) (cid:88)t 
η˜2(α)= t−1 =   σ˜−2(α)   ,
t 1+σ˜−2(α)η˜2 (α)   s  
t t−1 s=1
where x˜ := αx /(1− α). Then, by an analogous argument as in the proof of
is is
∗ ∗
Lemma1,(µ -LoM)and(λ-LoM)holdfort. Onceweobtain(µ -LoM)and(λ-LoM),
the rest of the arguments carry through by replacing c, σ2, η2, and β¯ = 1+| c−
(cid:12) t t (cid:12)
(1/2)− θ| /2 with c˜(α), σ˜2(α), η˜2(α), and β˜ := 1+(cid:12) (cid:12)c˜(α)−(1/2)− θ˜ (α)(cid:12) (cid:12)/2, respec-
t t
tively.
ff
In particular, since learning rates are una ected by scaling, the tight connection
between learning rates and limit play in Theorem 1 is robust to the extent of
coordination:
Corollary3. Theorem1holdsexactlyasstatedinthemaintext,withoutreplacing
the above expressions with their α-counterparts.
Proof. Note that η−2 =O(tp) if and only if η˜−2(α)=O(tp), and that η−2 =Ω(tq) if
t t t
and only if
η˜−2(α)=Ω(tq).
Hence, Theorem 1 holds without replacing the above
t
expressions with their α-counterparts.
44