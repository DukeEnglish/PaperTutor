Agents in Software Engineering: Survey, Landscape, and Vision
YanxianHuang1,WanjunZhong1,EnshengShi2,MinYang3,JiachiChen1,
HuiLi4,YuchiMa5,QianxiangWang5,ZibinZheng1,YanlinWang1∗
1SunYat-senUniversity,2Xi’anJiaotongUniversity
3ShenzhenInstituteofAdvancedTechnology,ChineseAcademyofSciences
4XiamenUniversity,5HuaweiCloudComputingTechnologiesCo.,Ltd.
Abstract Liu et al., 2024b; Chen et al., 2023a), etc. Many
studiescombiningLLMswithSEhaveemployed
In recent years, Large Language Models
theconceptofagentsfromtheartificialintelligence
(LLMs)haveachievedremarkablesuccessand
field,eitherexplicitlyorimplicitly. Explicitusein-
havebeenwidelyusedinvariousdownstream
dicates that the paper directly mentions the use
tasks,especiallyinthetasksofthesoftwareen-
ofagent-relatedtechnologies,whereasimplicituse
gineering(SE)field. Wefindthatmanystudies
combiningLLMswithSEhaveemployedthe suggeststhatwhiletheconceptofintelligentagents
conceptofagentseitherexplicitlyorimplicitly. isutilized,itmaybedescribedusingdifferentter-
However,thereisalackofanin-depthsurvey minologyorpresentedinalternativeforms.
tosortoutthedevelopmentcontextofexisting
Anagent(Wangetal.,2024c)representsanin-
works, analyze how existing works combine
telligent entity capable of perceiving, reasoning,
theLLM-basedagenttechnologiestooptimize
various tasks, and clarify the framework of andtaking action. Itperceivesthe environment’s
LLM-based agents in SE. In this paper, we state and selects actions based on its goals and
conductthefirstsurveyofthestudiesoncom- designtomaximizespecificperformancemetrics,
biningLLM-basedagentswithSEandpresent serving as a crucial technical foundation for ac-
a framework of LLM-based agents in SE
complishing diverse tasks and objectives. LLM-
whichincludesthreekeymodules: perception,
basedagentsgenerallyuseLLMsasthecognitive
memory, and action. Wealsosummarize the
core of the agent and perform well in scenarios
currentchallengesincombiningthetwofields
suchasautomation,intelligentcontrol,andhuman-
and propose future opportunities in response
toexistingchallenges. WemaintainaGitHub computerinteraction,leveragingthepowerfulca-
repository of the related papers at: https: pabilitiesofLLMsinlanguageunderstandingand
//github.com/DeepSoftwareAnalytics/ generation,learningandreasoning,contextpercep-
Awesome-Agent4SE.
tion and memory, and multimodality, etc. With
the development of various fields, the concept of
1 Introduction
traditionalandLLMs-basedagentshasgradually
Inrecent years, Large LanguageModels(LLMs) becomeclearandwidelyusedinthefieldofNat-
haveachievedremarkablesuccessandhavebeen uralLanguageProcessing(NLP)(Xietal.,2023).
widelyusedinmanydownstreamtasks,especially However,althoughexistingworkseitherexplicitly
in various tasks in the field of software engineer- or implicitly use this concept in SE, there is still
ing (SE) (Zheng et al.), such as code summariza- no clear definition of agents. There is a lack of
tion (Ahmed et al., 2024; Sun et al., 2023b; Hal- anin-depthsurveytoanalyzehowexistingworks
dar and Hockenmaier, 2024; Mao et al., 2024; combinetheagenttechnologiestooptimizevarious
Guo et al., 2023; Wang et al., 2021), code gen- tasks,sortoutthedevelopmentcontextofexisting
eration(Jiangetal.,2023a;Huetal.,2024b;Yang works,andclarifytheframeworkofagentsinSE.
etal.,2023a;TianandChen,2023;Lietal.,2023e; In this paper, we conduct an in-depth analysis
Wang et al., 2024b), code translation (Pan et al., oftheworkoncombiningLLM-basedagentswith
2024; Pan et al.), vulnerability detection and re- SE,summarizethecurrentchallengesincombin-
pair(Zhouetal.,2024;IslamandNajafirad,2024; ing the two fields, and propose possible opportu-
deFitero-Dominguezetal.,2024;Leetal.,2024; nities for future research in response to existing
challenges. Specifically, we first collect papers
∗*Corresponding author: Yanlin Wang, wangylin36@
mail.sysu.edu.cn on the application of LLM-based agent technol-
4202
peS
31
]ES.sc[
1v03090.9042:viXraogytoSEandobtain115papersafterfilteringand toimprovetheefficiencyofmulti-agentcol-
quality assessment. Then, inspired by the defini- laboration is also an opportunity for future
tion of the traditional agent (Wang et al., 2024c; work.
Xi et al., 2023), we present a general conceptual
• Technologies in the SE field can also pro-
frameworkfortheLLM-basedagentsinSE(Sec-
motethedevelopmentoftheAgentfield,need-
tion 2), comprising three key components: per-
ing more work to explore integrating more
ception, memory, action. We first introduce the
advanced technologies in the SE field into
perception module (Section 2.1), which can han-
Agent,promotingthedevelopmentofAgent
dle inputs of different modalities, such as textual
andprogressintheSEfield.
input, visual input, auditory input, etc. Next, we
present the memory module(Section 2.2), which
Inaddition,ThetechnologiesinSE,especially
includessemanticmemory,episodicmemory,and
thoserelatedtocode,canalsopromotethedevel-
proceduralmemory,helpingtheagenttomakerea-
opmentoftheagentfield,indicatingthemutually
soningdecisions. Finally,weintroducetheaction
reinforcing relationship between these two very
module(Section 2.3) which contains internal ac-
different fields. However, there is little work ex-
tionssuchasreasoning,retrieval,andlearning,as
ploringSEtechnologyinagents,andthefocusis
well as external actions like interacting with the
stillonthesimpleandbasictechnologyofSE,such
environment. Afterthat,weprovideadetailedand
asfunctioncalls,HTTPrequests,andothertools.
thoroughintroductiontothechallengesandoppor-
Therefore, in this paper, we mainly focus on the
tunities of LLM-based agents in SE (Section 3).
workrelatedtoagentsinSE,andonlybrieflydis-
Specifically,weproposethefollowingopportuni-
cussthestudiesaboutSEtechnologyinagentsin
ties for future research in response to the current
Section3.6,asanopportunityforfuturework.
challengesofLLM-basedagentsinSE:
2 LLM-basedAgentsinSE
• Most existing work on exploring perception
modulesfocusesontoken-basedinputintex- WepresentaframeworkoftheLLM-basedagents
tualinputbutlacksworkonexploringother in SE after sorting out and analyzing the studies
modalities. obtainedduringdatacollection. AsshowninFig-
ure 2, a single agent contains three key modules:
• Numerousnewtasksremainoutsidethecur-
perception,memory,andaction. Specifically,the
rent learning scope of LLMs, and complex
perceptionmodulereceivesexternalenvironment
tasksintheSEfieldnecessitateagentswitha
information of various modalities and converts it
diverserangeofcapabilities. Therefore,itis
into an input form that the LLM can understand
crucialtoexplorehowLLM-basedagentsplay
and process. The action module includes inter-
newrolesandeffectively balancetheability
nal and external actions, which are responsible
toperformmultipleroles.
for making reasoning decisions based on the in-
put of LLM and refining the decisions based on
• Itlacksanauthoritativeandrecognizedknowl-
the feedback obtained from interacting with the
edgebasecontainingrichcode-relatedknowl-
external environment, respectively. The memory
edge as an external retrieval base in the SE
moduleincludessemantic,episodic,andprocedu-
field.
ral memory, which can provide additional useful
informationtohelpLLMmakereasoningdecisions.
• AlleviatingthehallucinationsofLLM-based
At the same time, the action module can also up-
agents can improve the overall performance
datedifferentmemoriesinthememorymoduleby
of the agent, while agent optimization can
learning actions, providing more effective mem-
reverselyalleviatethehallucinationsofLLM-
oryinformationforreasoningandretrievalactions.
basedagents.
Furthermore,multi-agentcollaborationconsistsof
• The multi-agent collaboration process re- multiplesingleagents,whoareresponsibleforpart
quiresalargeamountofcomputingresources ofthetaskandcompletethetasktogetherthrough
andadditionalcommunicationoverheadgen- collaborativecooperation. Inthissection,wewill
eratedbysynchronizingandsharingvarious introducethedetailsofeachmoduleintheframe-
typesofinformation. Exploringtechnologies workoftheLLM-basedagentsinSE.Ahmedetal.(2024),Al-Kaswanetal.(2023),
Token-basedInput Arakelyanetal.(2023), AlqarniandAzim(2022),
Beurer-Kellneretal.(2023)
TextualInput Tree/graph-basedInput Maetal.(2023b), Maetal.(2023a),
Zhangetal.(2023g),Ochsetal.(2023)
Perception Hybrid-basedInput Niuetal.(2022)
(§2.1)
VisualInput Behrangetal.(2018),Reissetal.(2014),
Xieetal.(2019),Driessetal.(2023)
AuditoryInput Baoetal.(2020)
SemanticMemory Wangetal.(2024b),Zhangetal.(2024),
(Mainlyexistingintheexternal EghbaliandPradel(2024),Pateletal.(2023),
knowledgebases,includingdoc, Zhouetal.(2022),Renetal.(2023),
libraries,APIinformation) Senetal.(2023),Zhangetal.(2023d)
EpisodicMemory
Mainlyincludinghistorymessages, Lietal.(2023c),Renetal.(2023),
retrievingrelevantcodesfrom Weietal.(2023a),Zhangetal.(2023b),
EghbaliandPradel(2024),Ahmedetal.(2023),
Memory codebase,examplesinvolvedin
FengandChen(2023)
(§2.2) ICLtechnology,etc
Christopoulouetal.(2022a)Shenetal.(2023)
Gunasekaretal.(2023)Friedetal.(2022)
ImplicitKnowledge Roziereetal.(2023)Zanetal.(2022b)
(StoredintheLLMweights) Xuetal.(2023a)Thoppilanetal.(2022a)
Chandeletal.(2022)Lietal.(2022d)
Luoetal.(2023)Svyatkovskiyetal.(2020),
ProceduralMemory Ahmadetal.(2021b),Christopoulouetal.(2022b)
ExplicitKnowledge Pateletal.(2023),Shinetal.(2023),
(Writtenintheagent’scode) Zhangetal.(2023a),Zanetal.(2022c),
Xuetal.(2023b),Thoppilanetal.(2022b)
Suetal.(2023),FengandChen(2023),
Jiangetal.(2023a),Bairietal.(2023),
Lietal.(2023a),Huangetal.(2024),
ReasoningActions Lietal.(2023e),TianandChen(2023),
Christianosetal.(2023),Zhangetal.(2023c),
Yangetal.(2023a),Wangetal.(2024a),
Huetal.(2024b),Leetal.(2023),
Maetal.(2023b)
Zanetal.(2022a),Zhangetal.(2023b),
Lietal.(2022a),Lietal.(2023b),
InternalActions RetrievalActions Nashidetal.(2023),Zhangetal.(2024),
Gengetal.(2023),Zhouetal.(2022),
Lietal.(2023c),EghbaliandPradel(2024),
Zhangetal.(2023d),Xiaetal.(2023a)
Weyssowetal.(2023),Xiaetal.(2023b),
UpdatingImplicitKnowledge Wangetal.(2023b),Weietal.(2023a),
Weietal.(2023b)
LearningActions UpdatingSemanticMemory Huetal.(2023b),Zanetal.(2023),
Action withKnowledge Wuetal.(2022),Muennighoffetal.(2023)
(§2.3) UpdatingAgentCode Lietal.(2023d)
Jainetal.(2023),Pauletal.(2023),
Yangetal.(2023b),Muetal.(2023),
Moonetal.(2023),Shojaeeetal.(2023),
DialoguewithHuman/Agent Liuetal.(2023b),Wangetal.(2023e),
Sunetal.(2023a),Shinnetal.(2023),
Madaanetal.(2023),Weietal.(2023c),
Wangetal.(2023c),Hongetal.(2023b),
ExternalActions Huangetal.(2023a)
Weietal.(2023c),Agrawaletal.(2023),
Zhangetal.(2024),Wangetal.(2024a),
DigitalEnvironments Zhangetal.(2023e),Wangetal.(2023c),
Jainetal.(2023),Shojaeeetal.(2023),
Liuetal.(2023b),Shinnetal.(2023),
Wangetal.(2022)
Figure1: TaxonomyofLLM-basedagentsinsoftwareengineering.
2.1 Perception Next, we will introduce the details of different
modalinputsintheperceptionmodule.
The perception module connects the LLM-based
2.1.1 TextualInput
agent to the external environment and is the core
ofprocessingexternalinput. Itcanprocessinputs Different from the textual input format in NLP,
ofdifferentmodalitiessuchastextual,visual,and considering the characteristics of code, the tex-
auditory input, and convert them into an embed- tual input format in the SE includes token-based,
dingformatthatLLM-basedagentscanunderstand tree/graph-based,andhybrid-basedinput.
and process, laying the foundation for reasoning Token-basedInput. Token-basedinput(Ahmed
anddecision-makingactionsofLLM-basedagents. et al., 2024; Al-Kaswan et al., 2023; Arakelyan
gnireenignEerawtfoSnistnegAAction
Internal External
Digital Environment Dialogue with Human/Agents
OJPlatform
Feedback
Reasoning Learning Retrieval &Refine CodeInterpreter Human Other Agents
Single Agent
Memory
Environment
Procedural Episodic
Perception
Agent code Textual Visual
Relevant code LLM
Multi-Agent Model UML
parameter
Collaboration
Semantic Examples … Ex Re ec su ut li ton Auditory
Code base History message Text/Code
Figure2: AnoverviewofagentframeworkinSE.
et al., 2023; Beurer-Kellner et al., 2023; Alqarni putandmakesinferencedecisionsthroughmodel-
andAzim,2022;Lietal.,2022b;Guetal.,2022; ing and analysis of images. Many related works
Duetal.,2021)isthemostmainstreaminputmode, in NLP have explored this modality. For exam-
whichdirectlyregardsthecodeasnaturallanguage ple,Driessetal.(2023)proposePaLM-E,anem-
text and directly uses the token sequence as the bodiedmulti-modallanguagemodelwhoseinputs
inputofLLM,ignoringthecharacteristicsofcode. are multi-modal sentences that interleave visual,
Tree/Graph-basedInput. Comparedtonatural continuous state estimation, and textual input en-
language,codehasstrictstructureandgrammatical codings. Traditional soft engineering fields also
rules,andisusuallywrittenfollowingthegrammar have tasks for visualizing input, such as UI code
ofaspecificprogramminglanguage. Basedonthe search(Behrangetal.,2018;Reissetal.,2014;Xie
characteristicsofcode,tree/graph-basedinput(Ma etal.,2019)whichusesUIsketchesasqueriesfor
et al., 2023b,a; Zhang et al., 2023g; Ochs et al., useful code snippets. However, there is a lack of
2023;Bietal.,2024;Shietal.,2023a,2021;Wang workonvisualmodalityasinputstoLLMs.
andLi,2021)canconvertcodeintotreestructures
such as abstract syntax trees or graph structures
2.1.3 AuditoryInput
like control flow graphs to model the structural
informationofcode. However,thereisachallenge Auditory input takes auditory data such as audio
andopportunitythatcurrentworkrelatedtoLLM- as input and interacts with LLM in the form of
basedSEagentshasnotexploredsuchmodalities. speech. Traditional software engineering fields
Hybrid-basedInput. Hybridinput(Niuetal., havetasksforauditoryinput,suchasprogramming
2022;Huetal.,2024a;Guoetal.,2022)combines videosearch(Baoetal.,2020)whichusesvideosas
multiplemodalitiestoprovideLLMwithdifferent sourcesassourcesforusefulcodesnippets. How-
types of information. For example, hybrid input ever,thereisalsoalackofworkonauditoryinput
containing token-based and tree-based input can forLLMs.
combinethesemanticandstructuralinformationof
thecodetobettermodelandunderstandthecode.
2.2 Memory
However, there is also no work related to LLM-
basedagentsinSEexploringthismodality. Thememorymodulesincludesemantic,episodic,
andproceduralmemory, whichcanprovideaddi-
2.1.2 VisualInput
tionalusefulinformationtohelpLLMmakereason-
Visual input uses visual image data such as UI ingdecisions. Next,wewillintroducethedetails
sketches or UML design drawings as model in- ofthesethreetypesofmemoryrespectively.2.2.1 SemanticMemory videadditionalknowledgeforLLMreasoning,so
Semantic memory stores acknowledged world many works introduce such information into the
knowledge of LLM-based agents, usually in the reasoningprocessofLLM(Zhongetal.,2024;Li
formofexternalknowledgeretrievalbaseswhich etal.,2023c;FengandChen,2023;Ahmedetal.,
includedocuments,libraries,APIs,orotherknowl- 2023; Wei et al., 2023a; Ren et al., 2023; Zhang
edge. There have been many works (Wang et al., etal.,2023b;EghbaliandPradel,2024;Shietal.,
2024b; Zhang et al., 2024; Eghbali and Pradel, 2022). For example, Li et al. (2023c) propose a
2024; Patel et al., 2023; Zhou et al., 2022; Ren newpromptingtechniquenamedAceCoder,which
etal.,2023;Zhangetal.,2023d)exploringseman- selectssimilarprogramsasexamplesinprompts. It
ticmemory. Specifically,documentsandAPIsare provideslotsofrelevantcontent(e.g.,algorithms,
themostcommoninformationinexternalknowl- APIs)aboutthetargetcode.FengandChen(2023)
edge bases. For example, Zhou et al. (2022) in- proposeAdbGPT,anewlightweightapproachwith-
troduce a novel natural-language-to-code gener- outanytrainingandhard-codingeffort,whichcan
ation approach named DocPrompting, which ex- automaticallyreproducethebugsfrombugreports
plicitlyleveragesdocumentationbyretrievingthe usingIn-contextlearningtechniques.Ahmedetal.
relevantdocumentationpiecesbasedonanNLin- (2023) find that adding semantic facts can help
tent. Zhang et al. (2024) constructs a manually LLM to improve performance on code summa-
curated benchmark for repo-level code genera- rization..Weietal.(2023a)proposeanewmodel
tion named CODEAGENTBENCH, which con- named Coeditor, which predicts edits to a code
tains documentation, code dependency, and run- region based on recent changes within the same
time environment information. Ren et al. (2023) codebase using a multi-round code auto-editing
proposeKPC,anovelKnowledge-drivenPrompt setting. Inaddition,introducingexperienceinfor-
Chaining-basedcodegenerationapproach,which mation such as historical interaction information
utilizesfine-grainedexception-handlingknowledge canhelptheLLM-basedagentsbetterunderstand
extractedfromAPIdocumentationtoassistLLMs thecontextandmakecorrectdecisions. Somework
incodegeneration. Inadditiontodocuments,APIs uses experience information from past reasoning
are also common information in external knowl- anddecision-makingprocessestoobtainmoreac-
edge bases. For example, Eghbali and Pradel curate answers by iteratively querying and mod-
(2024) propose De-Hallucinator, an LLM-based ifying answers. For example, Ren et al. (2023)
code completion technique, which automatically proposeKPC,anovelKnowledge-drivenPrompt
identifies project-specific API references related Chaining-basedcodegenerationapproach,which
to code prefixes and the model’s initial predic- decomposescodegenerationintoanAIchainwith
tions, and adds these referenced information to iterativecheck-rewritestepsZhangetal.(2023b)
the prompt. Zhang et al. (2023d) integrate API proposeRepoCoder,asimple,generic,andeffec-
searchtoolsintothegenerationprocess,allowing tiveframeworkwhichmakeseffectiveutilization
themodeltoselectanAPIautomaticallyusingthe of repository-level information for code comple-
search tool to get suggestions. In addition, some tion in an iterative retrieval generation pipeline.
works also involve other information. For exam- EghbaliandPradel(2024)presentDe-Hallucinator,
ple,Pateletal.(2023)examinesthecapabilitiesand an LLM-based code completion technique that
limitations of different LLMs in generating code retrieves suitable API references and iteratively
basedonlibrariesdefinedincontext. Wangetal. queriesthemodelwithincreasinglysuitablecon-
(2024b)usesaugmentedfunctions,alongwiththeir text information in the prompt to ground the pre-
corresponding docstrings, to fine-tune a selected dictionsofamodel.
codeLLM.
2.2.3 ProceduralMemory
2.2.2 EpisodicMemory TheproceduralmemoryofAgentsinsoftwareen-
Episodic memory records content related to the gineering contains the implicit knowledge stored
currentcaseandexperienceinformationfrompre- in the LLM weights and the explicit knowledge
viousdecision-makingprocesses. Contentrelated writtenintheagent’scode.
to the current case (such as relevant information ImplicitknowledgeisstoredintheLLMparam-
foundinthesearchdatabase,samplesprovidedby eters. ExistingworkusuallyproposesnewLLMs
In-contextlearning(ICL)technology,etc.) canpro- withrichimplicitknowledgetocompletevariousdownstream tasks, by training the model with a agentsandChain-of-Though(CoT)isaneffective
large amount of data. Zheng et al. (2023) sorted wayofreasoning. WiththehelpofCoT,theLLMs
outthecodeLLMsintheSEfieldbasedontheir can deeply understand the problem, decompose
affiliationtype,includingcompanies,universities, complextasks,andgeneratehigh-qualityanswers.
researchteams&open-sourcecommunities,andin- AsshowninFigure3,existingworkhasexplored
dividuals&anonymouscontributors. differentformsofCoT,includingnaiveCoT/Plan,
Explicit knowledge is written in the agent’s SCoT, brainstorming, tree CoT, etc. Specifically,
code,enablingtheagenttooperateautomatically. naive CoT/Plan refers to a paragraph of text in
SeveralworksPateletal.(2023);Shinetal.(2023); thepromptdescribingtheprocessofreasoningfor
Zhangetal.(2023a)haveexploreddifferentways theproblem. Intheearlywork,asimplesentence
ofconstructingtheagent’scode. Specifically,Pa- was added to the prompt to guide LLMs in gen-
tel et al. (2023) use three types of in-context erating a chain of thought and better solving the
supervision to specify library functions includ- problem. Forexample, Huetal.(2024b)propose
ingDemonstrations,Description,andImplementa- anin-contextlearningapproachthatusesa"print
tion.Shinetal.(2023)investigatetheeffectiveness debugging"methodtoguideLLMstodebug. As
ofstate-of-the-artLLMwiththreedifferentprompt- LLMtechnologydevelops,thedesignofCoThas
ingengineeringtechniques(i.e.,basicprompting, become more complex. Inspired by the process
in-context learning, and task-specific prompting) ofdevelopersvalidatingthefeasibilityoftestsce-
against fine-tuned LLMs on three typical ASE narios, Su et al. (2023) design chain-of-thought
tasks.Zhangetal.(2023a)exploretheperformance (CoT)reasoningtoextracthuman-likeknowledge
ofsoftwarevulnerabilitydetectionusingChatGPT andlogicalreasoningfromLLMs. Leetal.(2023)
with different prompt designs(i.e., Basic Prompt- proposeCodeChain,anovelframeworkforinfer-
ing, Prompting with Auxiliary Information, and encesthatgeneratesachainofself-revisionsguided
Chain-of-ThoughtPrompting). bysomerepresentativesub-modulesgeneratedin
previous iterations. Huang et al. (2024) present
2.3 Action
CodeChain-of-Thought(CodeCoT)thatgenerates
Theactionmoduleincludestwotypes: internaland test cases to validate whether the code has syn-
externalactions. Theexternalactionsinteractwith taxerrorsduringtheexecutionandthenemploys
theexternalenvironmenttoobtainfeedbackinfor- aself-examinationphase, integrating CoTwitha
mation, including Dialogue with humans/agents self-examinationprocessforcodegeneration. Tian
andinteractionwiththedigitalenvironment,while andChen(2023)proposeanovelpromptingtech-
the internal actions reason and make decisions niquetodevisebothsophisticatedthought-eliciting
based on the input of the LLM and refine the de- promptingandfeedbackbasedonpromptingand
cision based on the obtained feedback, including makethefirstexplorationtoimprovethecodegen-
reasoning,retrieval,andlearningactions. Next,we erationperformanceofLLMs.
willintroduceeachactionindetail.
Considering the characteristics of code, some
2.3.1 InternalAction
works proposed structured CoT to introduce the
Internal actions include reasoning, retrieval, and structural information of the code. As shown in
learningactions. Separately,reasoningactionsare (b) in Figure 3, the structured CoT presents the
responsibleforanalyzingproblems,reasoning,and reasoningprocessinapseudo-code-likeform,in-
makingdecisionsbasedontheinputoftheLLM- volvingstructuressuchasloops,branches,etc. For
basedagent. Retrievalactionscanretrieverelevant example,Lietal.(2023a)proposeStructuredCoTs
informationfromtheknowledgebasetoassistrea- (SCoTs) which can efficiently use the rich struc-
soningactionsinmakingcorrectdecisions. Learn- turalinformationofsourcecodeandpresentSCoT
ingactionsarecontinuouslylearningandupdating prompting,anovelpromptingtechniqueforcode
knowledgebylearningandupdatingsemantic,pro- generation. Christianosetal.(2023)presentsagen-
cedural,andepisodicmemories,therebyimproving eral framework model to utilize the construction
thequalityandefficiencyofreasoninganddecision- ofintrinsicandextrinsicfunctionstoaddprevious
making. understandingsofreasoningstructures,integrating
Reasoning Action. A rigorous reasoning pro- and learning structured reasoning into AI agents’
cessisthekeytocompletingtasksbyLLM-based policies. In addition, some works have proposedother forms of CoT, such as brainstorming and usingtheseretrievedAPIs. De-Hallucinator(Egh-
tree-shapedCoT,asshownin(c)and(d)inFigure baliandPradel,2024)retrievessuitableAPIrefer-
3. Brainstormingistogeneraterelatedkeywords encesaddingtothepromptanditerativelyquery-
basedontheinput. Forexample,Lietal.(2023e) ingthemodelwiththeobtainedprompt. (2)Text-
introducesanovelBrainstormframeworkforcode Text. Sometimes, requirements are also used as
generation which leverages a brainstorming step input to retrieve relevant documentation or simi-
thatgeneratesandselectsdiversethoughtsonthe lar questions to help complete the task. For ex-
problemtofacilitatealgorithmicreasoningbefore ample, Zhou et al. (2022) introduce DocPrompt-
generatingcode. Thetree-shapedCoT(Fengand ing,anaturallanguage-to-codegenerationmethod
Chen,2023)dynamicallyexploresandupdatesthe that explicitly leverages documents by retrieving
CoT, and the nodes in the tree involve multiple relevant document fragments for a given NL in-
states including completed, new, and newly in- tent. Zhang et al. (2024) present CodeAgent, a
ferred,pending. novelLLM-basedagentframeworkthatintegrates
external tools to retrieve relevant information for
Therearealsosomestudiesexploringothertech-
effective repo-level code generation, enabling in-
niques to improve the reasoning ability and rea-
teraction with software artifacts for information
soning efficiency of LLM-based agents. For ex-
retrieval, code symbol navigation, and code test-
ample, Wang et al. (2024a) propose TOOLGEN
ing. (3) Code-Code. Code can also be used as
thatcomprisesTriggerInsertionandModelFine-
inputtoretrievesimilarorrelatedcodetoprovide
tuningphases(Offline),andTool-integratedCode
areferenceforgeneratingthetargetcode. Forex-
Generationphases(Online). TOOLGENreasons
ample, Zhang et al. (2023b) propose RepoCoder,
thepositionstotriggerauto-completiontoolsusing
a simple, generic, and effective framework that
theaugmentsfunctionswithinagivencodecorpus
retrievessimilarity-basedrepository-levelinforma-
with a special mark token. Yang et al. (2023a)
tioninaniterativeretrievalgenerationpipeline. (4)
designanovelapproachCOTTONwhichcanlever-
Hybrid-Code. Inadditiontousingasingletypeof
agelightweightLanguageModelstogenerateCoTs
informationsuchastextorcodeasinputtoretrieve
for code generation automatically. Zhang et al.
relatedcode,multipletypesofinformationcanalso
(2023c)presentself-speculativedecoding,anovel
becombinedintohybridinformationtoimprovere-
inference scheme that generates draft tokens and
trievalaccuracy. Forexample,Lietal.(2022a)uti-
then employs the original LLM to validate those
lizesthepowerfulcodegenerationmodeltobenefit
draft output tokens in one forward pass. (Zhou
thecoderetrievaltaskbyaugmentingthedocumen-
etal.,2023)introduceanAdaptive-Solverframe-
tationquerywithitsgeneratedcodesnippetsfrom
workthatstrategicallyadjuststhesolvingstrategy
thecodegenerationmodel(generationcounterpart)
according to the difficulty of the problem, which
andthenusestheaugmentedquerytoretrievecode.
notonlyimprovesthecomputationalefficiencybut
ToolCoder (Zhang et al., 2023d) uses an online
alsoimprovestheoverallperformance.
searchengineanddocumentationsearchtooltoget
Retrieval Action. The retrieval action can re- theproperAPIsuggestion,assistinginAPIselec-
trieve relevant information from the knowledge tionandcodegeneration. Inaddition,theretrieved
basetoassistthereasoningactioninmakingcor- contentisnotlimitedtoasingletypeofinforma-
rect decisions. The input used for retrieval and tion. (5)Code-Hybrid. Itusescodeasinputand
the output content obtained by retrieval have dif- retrievesvariousrelevant information. Forexam-
ferent types. As shown in Table 1, the input and ple,Nashidetal.(2023)presentsanoveltechnique
output can be text, code, or hybrid information namedCEDARforpromptcreationthatautomati-
containing both text and code. Specifically, it callyretrievescodedemonstrationssimilartothe
canbedividedintothefollowingtypes: (1)Text- code-relateddevelopertask,basedonembedding
Code. Typicallyrequirementsaretreatedasinput orfrequencyanalysis.Gengetal.(2023)generates
to retrieve related code or used APIs which are multi-intent comments for code by adopting the
addedtotheprompttogeneraterespondingcode. in-context learning paradigm which selects vari-
For example, Zan et al. (2022a) propose a novel ous code-comments examples from the example
frameworkwithAPIRetrieverandAPICodermod- pool. (6)Text-Hybrid. Itusesrequirementsasin-
ules. Specifically, theAPIRetrieverretrievesuse- puttoretrieverelevantcodeandsimilarquestions
ful APIs, and then the APICoder generates codeRequirement
Generates a function prime_fibthat returns the n-th (c) Brainstorming
Fibonacci number that is also prime. Dynamic Programming
(a) Native CoT/Plan
1. Create a function to check if a number is prime. (d) Tree CoT
2. Generate a Fibonacci sequence.
3. Check if each number in the Fibonacci sequence is Check if a
Create a function
prime, decrement the counter. number is
to check if a
4. If the counter is 0, return the Fibonacci number. prime if
number is prime.
n>=2.
(b) SCoT
def prime_fib(n: int):
Branch Structure
def is_prime(n: int): Return False
Generate a
if n < 2 then if n<2
def prime_fib Fibonacci
Sequence return False Loop Structure sequence
for iin range(2, n):
Structure Completed
check if a number is prime.
return True Check if each
Generate a Fibonacci sequence fib_seq number in the Next
Generate a counter. Fibonacci
while counter > 0: sequence is Newly Inferred
Check if each number in the Fibonacci prime, decrement
sequence is prime, decrement the counter. the counter. pending
return fib_seq[-1]
Figure3: DifferentCoTsfromdifferentmethods,where(a)isnativecot/plan,whichisobtainedbylettingLLM
thinkstepbystepinthepromptandincludesthedetailedprocessofanalyzingtheproblemandstepstosolvethe
problem. (b)isStructuredCoTs(SCoT),whichcombinescodefeaturestogenerateacodeskeletoncontaining
structures such as brach and loop in the graph. The blue font abstractly summarizes the description of LLM
generating specific code based on SCoT. (c) is the result of brainstorming, which is obtained by analyzing the
problem description and using knowledge of algorithms, data structures, and mathematics to provide ideas for
solvingit. (d)isanexampleofaTreeCoT,whichdynamicallyexploresanditerativelyupdatestheCoTtogradually
decomposeandcompletetheproblem.
to be referenced. For example, Li et al. (2023b) distance between natural language texts (Hayati
proposeLAIL(LLM-AwareIn-contextLearning), etal.,2018)orabstractsyntaxtrees(ASTs)ofcode
a novel learning-based selection approach to se- snippets (Zhang et al., 2020; Poesia et al., 2022).
lectexamplesforcodegeneration.Lietal.(2023c) Andsomeapproachesleverageknowledgegraphs
introduces a novel mechanism named AceCoder forretrieval(Yeetal.,2021;Shuetal.,2022).
which uses requirements to retrieve similar pro-
Dense-based and sparse-based retrieval meth-
gramsasexamplesinprompts,whichprovidelots
ods are the two most mainstream retrieval meth-
ofrelevantcontent(e.g.,algorithms,APIs).
ods. Among them, dense-based retrieval tech-
niquesgenerallyoffersuperiorperformancecom-
Accordingtopreviousresearch(Lietal.,2022c;
pared to sparse-based retrieval methods. How-
Zhao et al., 2024; Hu et al., 2023a), existing re-
ever,sparse-basedretrievalisoftenmoreefficient
trieval methods can be divided into sparse-based
andcanachievecomparableperformancelevelsto
retrieval,dense-basedretrieval(Wangetal.,2024e),
dense-based retrieval. As a result, many studies
and other methods (Hayati et al., 2018; Zhang
choosetoemploysparse-basedretrievalmethods
et al., 2020; Poesia et al., 2022; Ye et al., 2021;
duetotheirefficiency.
Shu et al., 2022). Figure 4 shows the pipelines
ofsparse-basedretrievalanddense-basedretrieval LearningAction. Learningactionsarecontinu-
respectively. Thedense-basedretrievalmethodcon- ouslylearningandupdatingknowledgebylearning
vertstheinputintoahigh-dimensionalvectorand andupdatingsemanticandproceduralmemories,
thencomparesthesemanticsimilaritytoselectthe therebyimprovingthequalityandefficiencyofrea-
k samples with the highest similarity, while the soninganddecision-making. (1)UpdatingSeman-
sparse-based retrieval method calculates metrics tic Memory with Knowledge. Semantic memory
suchasBM25orTF-IDFtoevaluatethetextsimi- mainly exists in the knowledge base that stores
laritybetweensamples. Additionally,variousalter- basicworldknowledgeandcanbeupdatedbyup-
nativeretrievalmethodshavebeenexplored. For datingtheknowledgebaseusingrecognizedcode
instance,somestudiesfocusoncalculatingtheedit knowledge or constructing a new one. For ex-Table1: Differenttypesofretrievalactionsclassifiedbyinputandoutput.
Types Input Output Studies
APIRetriever(Zanetal.,2022a),
Text-Code Requirements API
De-Hallucinator(EghbaliandPradel,2024)
RelevantDocuments, DocPrompting(Zhouetal.,2022),
Text-Text Requirements
RepoDocumentation CodeAgent(Zhangetal.,2024)
IncompleteCode SimilarCode RepoCoder(Zhangetal.,2023b)
Code-Code
TargetHole’sLocation RepoContext RepoFusion(Shrivastavaetal.,2023)
Requirements,
API ToolCoder(Zhangetal.,2023d)
Hybrid-Code UnfinishedCode
Requirements,
TargetCode QueryExpansion(Lietal.,2022a)
GeneratedCode
CEDAR(Nashidetal.,2023),
Code-Hybrid CodeSnippet Code-CommentExamples
Multi-intent(Gengetal.,2023)
LAIL(Lietal.,2023b),
Text-Hybrid Requirements Examples
AceCoder(Lietal.,2023c)
Dense-based Sparse-based
Query: search engine ranking
D1 D2 D3 D4 …
D1 D2 D3 D4 …
0.2 0.1 0.4 0.1 …
0.1 0.2 0.1 0.4 …
SemanticSimilarity
Retrieval database
Text Similarity
search engine ranking
D1: Information retrieval and search engines information
D2: Search engine optimization retrieval
D3: Ranking algorithms in information retrieval and
D4: Pagerankalgorithm and search engines search
… engines
Figure 4: The pipeline of different retrieval methods. The left part is the pipeline of the dense-based retrieval
method,whichcanusedifferentmodelstoconvertthetextintoahigh-dimensionalembeddingvectorandcompare
thesemanticsimilaritytoretrievethesamplewiththehighestsimilarityscore. Therightpartisthepipelineofthe
sparse-basedretrievalmethod,whichjustcomparesthetextsimilarityandignoressemantics.
ample, Liao et al. (2023) propose a novel code pre-trained model which updates the full param-
generationframework,calledA3-CodGen,which eters of the model (Xia et al., 2023b; Wei et al.,
generates higher quality code by leveraging the 2023a,b; Tao et al., 2024; Wang et al., 2024d;
information retrieved from a retrieval base that Liu et al., 2023a; Wang et al., 2023d; Shi et al.,
contains three types of information: local aware- 2023b). However,astheparameterscaleincreases,
nessinformationfromthecurrentcodefile,global the cost of fine-tuning the model also increases.
awarenessinformationfromothercodefiles,and Someworkstrytoexploreparameter-efficientfine-
third-party library information. Du et al. (2024) tuningtechniques(Weyssowetal.,2023;Shietal.,
proposeanovelLLM-basedvulnerabilitydetection 2023c). For example, Weyssow et al. (2023) de-
techniqueVul-RAGwhichconstructsavulnerabil- liveracomprehensivestudyofParameter-Efficient
ityknowledgebasebyextractingmulti-dimension Fine-Tuning(PEFT)techniques(PEFT)techniques
knowledgeviaLLMsfromexistingCVEinstances. forLLMsundertheautomatedcodegenerationsce-
(2) Updating Implicit Knowledge. Since the im- nario.Wangetal.(2023b)insertandfine-tunethe
plicit knowledge is stored in the LLM parame- parameter-efficient structure adapter, rather than
ters, it can be updated by fine-tuning the model fine-tuningthepre-trainedmodel. Mostofthecur-
toupdatetheLLMparameters. Earlyworkusually rent work uses effective fine-tuning technologies
constructs new data to supervise fine-tuning the tofine-tunethemodel(Guoetal.,2024;Shietal.,2023d). and online buffer data generated by interacting
(3) Updating Agent Code. Agent code refers withthecompilertocalculatelossandupdatethe
totheprogramoralgorithmthattheagentrunsto model weights through gradient feedback. Wang
guideitsbehavioranddecision-making. TheLLM- etal.(2023e)proposeChatCoder,amethodtore-
basedagentconstructscorrespondingpromptsas fine the requirements via chatting with large lan-
agent codes to regulate how to perceive the en- guage models. Sun etal. (2023a)propose Clover
vironment, reason and make decisions, and per- thatliesacheckertochecktheconsistencyamong
formactions. Manyworksuseinstruction-tuning code, docstrings, and formal annotations. Clari-
techniquestoaligntheoutputofLLMwiththein- fyGPT (Mu et al., 2023) prompts another LLM
putinstructions. Forexample,.Muennighoffetal. togeneratetargetedclarifyingquestionstorefine
(2023) leverage the natural structure of Git com- theambiguousrequirementinputtedbyusers. Re-
mits that pair code changes with human instruc- flexion (Shinn et al., 2023) can interact with hu-
tionsandapplyinstructiontuningusingthem.Hu mans and other agents to generate external feed-
et al. (2023b) constructed the first instruction- back. Self-Refine (Madaan et al., 2023) uses a
tuning dataset named InstructCoder which is de- single LLM as the generator, refiner, and feed-
signed to adapt LLMs for general-purpose code backprovider,ratherthanrequiringanysupervised
editing. Thesehigh-qualityqualitydatacanbring trainingdata,additionaltraining,orreinforcement
newknowledgetothelargerlanguagemodeland learning. Repilot (Wei et al., 2023c) synthesizes
update semantic memory. Zan et al. (2023) con- acandidatepatchthroughtheinteractionbetween
ductextensiveexperimentsof8popularprogram- an LLM and a Completion Engine. Specifically,
minglanguagesonStarCodertoexplorewhether Repilotprunesawayinfeasibletokenssuggestedby
programminglanguagescanboosteachothervia theLLM.Wangetal.(2023c)introduceMINT,a
instruction-tuning. benchmarkthatcanevaluateLLMs’abilitytosolve
tasks with multi-turn interactions by leveraging
2.3.2 ExternalAction
users’naturallanguagefeedbacksimulatedbyGPT-
DialoguewithHuman/AgentsAgentscaninter- 4.Hongetal.(2023b)proposeMetaGPT,aninno-
act with humans or other agents, and get rich in- vativemeta-programmingframeworkthatincorpo-
formation in the interaction process as feedback, ratesefficienthumanworkflowsintoLLM-based
expandingtheknowledgeoftheagentandrefining multi-agent collaborations. Huang et al. (2023a)
theanswersofLLMmorecorrector. Specifically, introduces AgentCoder, a novel code generation
many works use LLMs as agents to interact (Lu solutioncomprisingamulti-agentframeworkwith
et al., 2024; Jain et al., 2023; Paul et al., 2023; specializedagents: theprogrammeragent,thetest
Shojaeeetal.,2023;Liuetal.,2023b;Wangetal., designeragent,andthetestexecutoragent.
2023e;Muetal.,2023;Madaanetal.,2023). Jain
etal.(2023)proposeRLCFthatusesfeedbackfrom DigitalEnvironmentAgentscaninteractwith
adifferentLLMthatcomparesthegeneratedcode digitalsystems,suchasOJplatforms,webpages,
to a reference code to further train a pre-trained compilers, and other external tools, and the in-
LLMviareinforcementlearning. REFINER(Paul formation obtained during the interaction pro-
etal.,2023)isaframeworkthatcaninteractwith cess can be used as feedback to optimize them-
a critic model that provides automated feedback selves. Specifically, compilers are the most com-
on the reasoning. Yang et al. (2023b) study and monexternaltools(Jainetal.,2023;Shojaeeetal.,
elucidate how LLMs benefit from discriminative 2023;Liuetal.,2023b;Wangetal.,2022;Zhang
models.Moonetal.(2023)constructanewdataset et al., 2023e). For example, RLCF (Jain et al.,
specificallydesignedforcodefixingwithfeedback 2023)trainsthepre-trainedLLMviareinforcement
andthenusethisdatasettogainamodelthatcan learning,usingthecompiler-derivedfeedbackon
automaticallygeneratehelpfulfeedbackforcode whether the code it generates passes a set of cor-
editingviaPreference-OptimizedTuningandSe- rectnesschecks. PPOCoder(Shojaeeetal.,2023)
lection. PPOCoder(Shojaeeetal.,2023)consists can incorporate compiler feedback and structure
oftwoparts,criticandactor,andwillbeoptimized alignmentsasextraknowledgeintothemodelopti-
with PPO through the interaction between these mizationtofine-tunecodegenerationmodelsvia
two models. RLTF (Liu et al., 2023b) interacts deepreinforcementlearning(RL).RLTF(Liuetal.,
with other models that utilize ground truth data 2023b) interacts with the compiler to produce atraining data pair and then stores it in the online Kellneretal.,2023;AlqarniandAzim,2022)often
buffer. Wangetal.(2022)proposeCOMPCODER treatcodeastext,andthereisstillalackofworkon
thatutilizescompilerfeedbackforcompilablecode LLM-based agents in SE that explore tree/graph-
generation. Zhangetal.(2023e)proposeSelf-Edit, basedinputmodalities. Inaddition,thereisstilla
a generate-and-edit approach that utilizes execu- lackofresearchonexploringvisualandauditory
tion results of the generated code from LLMs to inputmodalities.
improve the code quality on the competitive pro-
grammingtask. Inaddition,manyworksconstruct
3.2 Role-playingAbilities
toolssuchassearchengines,completionengines,
andotherstoexpandthecapabilitiesofintelligent LLM-based agents are often required to play dif-
agents (Wang et al., 2024a; Zhang et al., 2024; ferent roles across a range of tasks, necessitating
Agrawaletal.,2023;Weietal.,2023c;Zhangetal., specificskillsforeachrole. Forexample,anagent
2023f). Wang et al. (2024a) introduce TOOL- mayfunctionasacodegeneratorwhentaskedwith
GEN,anapproachthatintegratesautocompletion generatingcode,andasacodetesterwhentasked
toolsintothecodeLLMgenerationprocesstoad- withcodetesting. Furthermore,incertainscenar-
dressdependencyerrorssuchasundefined-variable ios,theseagentsmayneedtohavemultiplecapa-
andno-membererrors.Zhangetal.(2024)present bilitiesatthesametime. Forexample,inthecode
CodeAgent,anovelLLM-basedagentframework generation scenario, the agent needs to play the
that integrates five programming tools, enabling roleofbothacodegeneratorandtesterandaccord-
interactionwithsoftwareartifactsforinformation inglyneedstohavetheabilitytobothgenerateand
retrieval,codesymbolnavigation,andcodetesting test code (Huang et al., 2023b). In the software
foreffectiverepo-levelcodegeneration.Agrawal engineeringfield,therearevariousnichetasksfor
etal.(2023)proposeMGD,amonitor-guidedde- which LLM learning is not enough and complex
codingmethodwhereamonitorusesstaticanalysis tasksthatrequireagentstohavemultiplecapabil-
toguidethedecoding. Repilot(Weietal.,2023c) ities, such as test generation scenario, front-end
synthesizesacandidatepatchthroughtheinterac- development,repository-levelissueresolution,etc.
tion between an LLM and a Completion Engine. Therefore, advancing research on how to enable
Specifically,Repilotcompletesthetokenbasedon agentstoeffectivelyadoptnewrolesandmanage
the suggestions provided by the Completion En- thedemandsofmulti-roleperformancerepresents
gine. apromisingdirectionforfuturework.
3 ChallengesandOpportunities
3.3 LackofKnowledgeRetrievalBase
Upon analyzing the work related to LLM-based
The external knowledge retrieval base is an im-
agents in software engineering, it is evident that
portantpartofthesemanticmemoryintheagent
therearestillmanychallengesinthecurrentinte-
memorymoduleandoneoftheimportantexternal
grationofthesetwofields,whichlimitsthedevel-
toolsthattheagentcaninteractwith. InNLPfields,
opmentofboth. Inthissection,wewilldiscussin
thereareknowledgebasessuchasWikipediaasex-
detailthechallengesfacedbycurrentLLM-based
ternalretrievalbases(Zhaoetal.,2023). However,
agents in SE and discuss some opportunities for
intheSEfield, thereiscurrentlynoauthoritative
futureworkbasedontheanalysisofexistingchal-
andrecognizedknowledgebasethatcontainsrich
lenges.
code-relatedknowledge,suchasthebasicsyntax
of various programming languages, various com-
3.1 LackofExplorationofPerceptionModule
monlyusedalgorithms,knowledgerelatedtodata
AsmentionedinSection2.1,thereisalackofwork structures and operating systems, etc. In future
exploring the perception module of LLM-based research,effortscouldbedirectedtowardsdevel-
agentsinSE.Unlikenaturallanguage,codeisaspe- oping a comprehensive code knowledge base to
cialrepresentationthatcanbetreatedasordinary serve as an external retrieval base for the agent.
text or converted into an intermediate representa- This knowledge base would enrich the available
tionwithcodecharacteristics,suchasAST,CFG, information, thereby enhancing both the quality
andsoon. Existingworks(Ahmedetal.,2024;Al- and efficiency of reasoning and decision-making
Kaswanetal.,2023;Arakelyanetal.,2023;Beurer- processes.3.4 HallucinationsofLLM-basedAgents betweenthesetwodomains. Forexample,software
testingtechniquescanbeadaptedtoidentifyabnor-
ManystudiesrelatedtoLLM-basedagentsconsider
malbehaviorsandpotentialdefectsinLLM-based
LLMsasthecognitivecoreoftheagents,withthe
agents. Additionally, improvements in software
agents’overallperformancebeingcloselytiedto
tools, such as APIs and libraries, can also boost
thecapabilitiesoftheunderlyingLLMs. Existing
theperformanceofLLM-basedagentsespecially
research (Pan et al., 2024; Liu et al., 2024a) has
for those with tool using abilities. Furthermore,
shown that LLM-based agents may produce hal-
softwarepackagemanagementtechniquescanbe
lucinations,suchasgeneratingnon-existentAPIs
adapted for effectively managing agent systems.
when completing tasks in the SE field. Mitigat-
Forexample,versioncontrolcanbeappliedtomon-
ing these hallucinations can improve the overall
itorandcoordinateupdatesacrossdifferentagents
performanceoftheagents. Atthesametime,the
in an agent system, enhancing compatibility and
optimizationoftheagentscanalsoreverselyalle-
systemintegrity.
viatethehallucinationsoftheLLM-basedagents,
However, research in this line remains limited.
highlightingthebidirectionalrelationshipbetween
Therefore,exploringtheincorporationofmoreso-
agent performance and hallucination mitigation.
phisticatedSEtechniquesintoagentsystemsrepre-
Althoughsomeeffortshavebeenmadetoinvesti-
sentsapromisingareaforfutureresearch,withthe
gatethehallucinationsofLLMs,significantchal-
potentialtodriveadvancementsinbothfields.
lengesremaininaddressingthehallucinationissue
withinLLM-basedagents. Exploringwhattypesof
4 Conclusion
hallucinationsexistinLLM-basedagents,deeply
analyzingthecausesofthesehallucinations,and
Toconductanin-depthanalysisoftheworkoncom-
proposingeffectivehallucinationmitigationmeth-
biningLLM-basedagentswithSE,wefirstcollect
odsaregreatopportunitiesforfutureresearch.
manystudiesthatcombineLLM-basedagentswith
tasksinthesoftwareengineeringfield. Then, we
3.5 EfficiencyofMulti-agentCollaboration
presentaframeworkofLLM-basedagentsinsoft-
In the process of multi-agent collaboration, each ware engineering which contains three key mod-
individualagentneedstoplaydifferentrolestoac- ules: perception,memory,andactions,aftersorting
complishspecifictasks,andtheoutcomesofeach outandanalyzingthestudiesobtainedduringdata
agent’sdecisionsarethencombinedtocollectively collection. Finally,weintroducethedetailsofeach
tacklemorecomplexobjectives(Chenetal.,2023b; moduleintheframework,analyzethecurrentchal-
Hong et al., 2023a; Huang et al., 2023b; Wang lengesforLLM-basedagentsintheSEfield,and
etal.,2023a). However,thisprocessoftenrequires pointoutsomeopportunitiesforfuturework.
a large amount of computing resources for each
agent,resultinginresourcewasteandreducedef-
ficiency. In addition, each single agent needs to References
synchronize and share various types of informa-
LakshyaAAgrawal,AdityaKanade,NavinGoyal,Shu-
tion,whichintroducesadditionalcommunication
vendu K. Lahiri, and Sriram K. Rajamani. 2023.
costsandaffectsthereal-timeandresponsespeed Guidinglanguagemodelsofcodewithglobalcontext
ofcollaboration. Effectivelymanagingandallocat- usingmonitors. ArXiv,abs/2306.10763.
ing computing resources, minimizing inter-agent
WasiUddinAhmad,SaikatChakraborty,BaishakhiRay,
communicationcosts,andreducingthereasoning
and Kai-Wei Chang. 2021a. Unified pre-training
overheadofindividualagentsrepresentkeychal- for program understanding and generation. arXiv
lengesforenhancingtheefficiencyofmulti-agent preprintarXiv:2103.06333.
collaboration. Addressingtheseissuespresentsa
WasiUddinAhmad,SaikatChakraborty,BaishakhiRay,
significantopportunityforfutureresearch.
and Kai-Wei Chang. 2021b. Unified pre-training
for program understanding and generation. ArXiv,
3.6 SETechnologiesinLLM-basedAgents
abs/2103.06333.
Techniquesfromthesoftwareengineeringdomain,
ToufiqueAhmed,KunalSureshPai,PremDevanbu,and
particularlythosetocode,havethepotentialtosig-
EarlT.Barr.2023. Automaticsemanticaugmenta-
nificantly advance the development of the agent
tion of language model prompts (for code summa-
field,indicatingamutuallybeneficialrelationship rization).Toufique Ahmed, Kunal Suresh Pai, Premkumar De- ShubhamChandel,ColinBClement,GuillermoSerrato,
vanbu,andEarlT.Barr.2024. Automaticsemantic and Neel Sundaresan. 2022. Training and evaluat-
augmentationoflanguagemodelprompts(forcode ingajupyternotebookdatascienceassistant. arXiv
summarization). preprintarXiv:2201.12901.
AlfredV.AhoandJeffreyD.Ullman.1972. TheTheory AshokK.Chandra,DexterC.Kozen,andLarryJ.Stock-
of Parsing, Translation and Compiling, volume 1. meyer.1981. Alternation. JournaloftheAssociation
Prentice-Hall,EnglewoodCliffs,NJ. forComputingMachinery,28(1):114–133.
Ali Al-Kaswan, Toufique Ahmed, Maliheh Izadi,
ChongChen,JianzhongSu,JiachiChen,YanlinWang,
Anand Ashok Sawant, Premkumar Devanbu, and
TingtingBi,YanliWang,XingweiLin,TingChen,
ArievanDeursen.2023. Extendingsourcecodepre-
andZibinZheng.2023a. Whenchatgptmeetssmart
trainedlanguagemodelstosummarisedecompiled
contract vulnerability detection: How far are we?
binaries.
arXivpreprintarXiv:2309.05520.
MansourAlqarniandAkramulAzim.2022. Lowlevel
Weize Chen, Yusheng Su, Jingwei Zuo, Cheng Yang,
sourcecodevulnerabilitydetectionusingadvanced
Chenfei Yuan, Chen Qian, Chi-Min Chan, Yujia
bertlanguagemodel. ProceedingsoftheCanadian
Qin, Yaxi Lu, Ruobing Xie, et al. 2023b. Agent-
ConferenceonArtificialIntelligence.
verse: Facilitatingmulti-agentcollaborationandex-
ploringemergentbehaviorsinagents. arXivpreprint
American Psychological Association. 1983. Publica-
arXiv:2308.10848.
tionsManual. AmericanPsychologicalAssociation,
Washington,DC.
Filippos Christianos, Georgios Papoudakis, Matthieu
RieKubotaAndoandTongZhang.2005. Aframework Zimmer,ThomasCoste,ZhihaoWu,JingxuanChen,
forlearningpredictivestructuresfrommultipletasks KhyatiKhandelwal,JamesDoran,XidongFeng,Ji-
and unlabeled data. Journal of Machine Learning achengLiu,ZhengXiong,YichengLuo,JianyeHao,
Research,6:1817–1853. Kun Shao, Haitham Bou-Ammar, and Jun Wang.
2023. Pangu-agent: Afine-tunablegeneralistagent
GalenAndrewandJianfengGao.2007. Scalabletrain- withstructuredreasoning. ArXiv,abs/2312.14878.
ingofL1-regularizedlog-linearmodels. InProceed-
ingsofthe24thInternationalConferenceonMachine Fenia Christopoulou, Gerasimos Lampouras, Milan
Learning,pages33–40. Gritta, Guchun Zhang, Yinpeng Guo, Zhongqi Li,
Qi Zhang, Meng Xiao, Bo Shen, Lin Li, et al.
ShushanArakelyan,RocktimJyotiDas,YiMao,and 2022a. Pangu-coder: Program synthesis with
XiangRen.2023. Exploringdistributionalshiftsin function-level language modeling. arXiv preprint
largelanguagemodelsforcodeanalysis. arXiv:2207.11280.
Ramakrishna Bairi, Atharv Sonwane, Aditya
Fenia Christopoulou, Gerasimos Lampouras, Milan
Kanade, C VageeshD, Arun Shankar Iyer, Suresh
Gritta, GuchunZhang, YinpengGuo, ZhongyiLi,
Parthasarathy, SriramK.Rajamani, B.Ashok, and
Qi Zhang, Meng Xiao, Bo Shen, Lin Li, Hao Yu,
Shashank P. Shet. 2023. Codeplan: Repository-
LiyuYan,PingyiZhou,XinWang,YuMa,Ignacio
level coding using llms and planning. ArXiv,
Iacobacci,YashengWang,GuangtaiLiang,JiaWei,
abs/2309.12499.
Xin Jiang, Qianxiang Wang, and Qun Liu. 2022b.
Pangu-coder: Programsynthesiswithfunction-level
Lingfeng Bao, Zhenchang Xing, Xin Xia, David Lo,
languagemodeling. ArXiv,abs/2207.11280.
Ming hui Wu, and Xiaohu Yang. 2020. psc2code.
ACM Transactions on Software Engineering and
Colin B. Clement, Dawn Drain, Jonathan Timcheck,
Methodology(TOSEM),29:1–38.
Alexey Svyatkovskiy, and Neel Sundaresan. 2020.
Pymt5: Multi-modetranslationofnaturallanguage
FarnazBehrang,StevenP.Reiss,andAlessandroOrso.
andpythoncodewithtransformers. InConferenceon
2018. Guifetch: Supporting app design and devel-
EmpiricalMethodsinNaturalLanguageProcessing.
opment through gui search. 2018 IEEE/ACM 5th
InternationalConferenceonMobileSoftwareEngi-
DaviddeFitero-Dominguez,EvaGarcia-Lopez,Anto-
neeringandSystems(MOBILESoft),pages236–246.
nioGarcia-Cabot,andJose-JavierMartinez-Herraiz.
LucaBeurer-Kellner,MarcFischer,andMartinVechev. 2024. Enhanced automated code vulnerability re-
2023. Promptingisprogramming: Aquerylanguage pair using large language models. arXiv preprint
forlargelanguagemodels. ProceedingsoftheACM arXiv:2401.03741.
onProgrammingLanguages,7(PLDI):1946–1969.
Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey
Wendong Bi, Lun Du, Qiang Fu, Yanlin Wang, Shi Lynch,AakankshaChowdhery,BrianIchter,Ayzaan
Han,andDongmeiZhang.2024. Makeheterophilic Wahid,JonathanTompson,QuanHoVuong,Tianhe
graphs better fit gnn: A graph rewiring approach. Yu, Wenlong Huang, Yevgen Chebotar, Pierre Ser-
IEEE Transactions on Knowledge and Data Engi- manet,DanielDuckworth,SergeyLevine,Vincent
neering. Vanhoucke,KarolHausman,MarcToussaint,KlausGreff,AndyZeng,IgorMordatch,andPeterR.Flo- DanGusfield.1997. AlgorithmsonStrings,Treesand
rence.2023. Palm-e: Anembodiedmultimodallan- Sequences. CambridgeUniversityPress,Cambridge,
guage model. In International Conference on Ma- UK.
chineLearning.
Rajarshi Haldar and Julia Hockenmaier. 2024. An-
LunDu,XiaozhouShi,YanlinWang,EnshengShi,Shi alyzing the performance of large language mod-
Han,andDongmeiZhang.2021. Isasinglemodel els on code summarization. arXiv preprint
enough? mucos: Amulti-modelensemblelearning arXiv:2404.08018.
approachforsemanticcodesearch. InProceedingsof
the30thACMInternationalConferenceonInforma- ShirleyAnugrahHayati,RaphaelOlivier,PravalikaAv-
tion&KnowledgeManagement,pages2994–2998. varu,PengchengYin,AnthonyTomasic,andGraham
Neubig.2018. Retrieval-basedneuralcodegenera-
Xueying Du, Geng Zheng, Kaixin Wang, Jiayi Feng,
tion. arXivpreprintarXiv:1808.10025.
WentaiDeng,MingweiLiu,BihuanChen,XinPeng,
Tao Ma, and Yiling Lou. 2024. Vul-rag: Enhanc- Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng
ingllm-basedvulnerabilitydetectionviaknowledge- Cheng,JinlinWang,CeyaoZhang,ZiliWang,Steven
levelrag. arXivpreprintarXiv:2406.11147. KaShingYau,ZijuanLin,LiyangZhou,etal.2023a.
Metagpt:Metaprogrammingformulti-agentcollabo-
Aryaz Eghbali and Michael Pradel. 2024. De-
rativeframework. arXivpreprintarXiv:2308.00352.
hallucinator: Iterativegroundingforllm-basedcode
completion. ArXiv,abs/2401.01701.
SiruiHong,XiawuZheng,JonathanP.Chen,Yuheng
SidongFengandChunyangChen.2023. Promptingis Cheng,CeyaoZhang,ZiliWang,StevenKaShing
allyourneed: Automatedandroidbugreplaywith Yau, Zi Hen Lin, Liyang Zhou, Chenyu Ran,
largelanguagemodels. ArXiv,abs/2306.01987. LingfengXiao,andChenglinWu.2023b. Metagpt:
Meta programming for multi-agent collaborative
DanielFried,ArmenAghajanyan,JessyLin,SidaWang, framework. ArXiv,abs/2308.00352.
EricWallace,FredaShi,RuiqiZhong,Wen-tauYih,
LukeZettlemoyer,andMikeLewis.2022. Incoder: Fan Hu, Yanlin Wang, Lun Du, Xirong Li, Hongyu
Agenerativemodelforcodeinfillingandsynthesis. Zhang, Shi Han, and Dongmei Zhang. 2023a. Re-
arXivpreprintarXiv:2204.05999. visiting code search in a two-stage paradigm. In
ProceedingsoftheSixteenthACMInternationalCon-
MingyangGeng,ShangwenWang,DezunDong,Hao ferenceonWebSearchandDataMining,pages994–
Wang,GeLi,ZhiJin,XiaoguangMao,andXiangke 1002.
Liao. 2023. Large language models are few-shot
summarizers: Multi-intentcommentgenerationvia FanHu,YanlinWang,LunDu,HongyuZhang,Dong-
in-contextlearning. InInternationalConferenceon mei Zhang, and Xirong Li. 2024a. Tackling long
SoftwareEngineering. codesearchwithsplitting,encoding,andaggregat-
ing. InProceedingsofthe2024JointInternational
Wenchao Gu, Yanlin Wang, Lun Du, Hongyu Zhang,
ConferenceonComputationalLinguistics,Language
ShiHan,DongmeiZhang,andMichaelRLyu.2022.
Resources and Evaluation (LREC-COLING 2024),
Acceleratingcodesearchwithdeephashingandcode
pages15500–15510.
classification. arXivpreprintarXiv:2203.15287.
QishengHu, KaixinLi, XuZhao, YuxiXie, Tiedong
Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio
Liu,HuiChen,QizheXie,andJunxianHe.2023b.
CésarTeodoroMendes,AllieDelGiorno,Sivakanth
Instructcoder: Empowering language models for
Gopi,MojanJavaheripi,PieroKauffmann,Gustavo
codeediting. ArXiv,abs/2310.20329.
deRosa,OlliSaarikivi,etal.2023. Textbooksareall
youneed. arXivpreprintarXiv:2306.11644.
Xueyu Hu, Kun Kuang, Jiankai Sun, Hongxia Yang,
DayaGuo,ShuaiLu, NanDuan, YanlinWang, Ming andFeiWu.2024b. Leveragingprintdebuggingto
Zhou,andJianYin.2022. Unixcoder: Unifiedcross- improvecodegenerationinlargelanguagemodels.
modal pre-training for code representation. arXiv ArXiv,abs/2401.05319.
preprintarXiv:2203.03850.
DongHuang,QiBu,JieM.Zhang,MichaelLuck,and
HanyangGuo,XiangpingChen,YuanHuang,Yanlin HemingCui.2023a. Agentcoder: Multi-agent-based
Wang,XiDing,ZibinZheng,XiaocongZhou,and codegenerationwithiterativetestingandoptimisa-
Hong-NingDai.2023. Snippetcommentgeneration tion. ArXiv,abs/2312.13010.
basedoncodecontextexpansion. ACMTransactions
onSoftwareEngineeringandMethodology,33(1):1– DongHuang,QingwenBu,YuhaoQing,andHeming
30. Cui.2024. Codecot: Tacklingcodesyntaxerrorsin
cotreasoningforcodegeneration.
Lianghong Guo, Yanlin Wang, Ensheng Shi, Wanjun
Zhong,HongyuZhang,JiachiChen,RuikaiZhang, DongHuang,QingwenBu,JieMZhang,MichaelLuck,
Yuchi Ma, and Zibin Zheng. 2024. When to stop? andHemingCui.2023b. Agentcoder: Multi-agent-
towardsefficientcodegenerationinllmswithexcess basedcodegenerationwithiterativetestingandopti-
tokenprevention. arXivpreprintarXiv:2407.20042. misation. arXivpreprintarXiv:2312.13010.NafisTanveerIslamandPeymanNajafirad.2024. Code codegenerationmodelsarebetterfew-shotinforma-
security vulnerability repair using reinforcement tionextractors. ArXiv,abs/2305.05711.
learningwithlargelanguagemodels. arXivpreprint
arXiv:2401.07031. Xinyu Li, Jiang-Tian Xue, Zheng Xie, and Ming Li.
2023e. Thinkoutsidethecode:Brainstormingboosts
AbhinavC.P.Jain,ChimaAdiole,SwaratChaudhuri, large language models in code generation. ArXiv,
T.Reps,ChrisJermaineRiceUniversity,UtAustin, abs/2305.10679.
andUniversityofWisconsin.2023. Coarse-tuning
models of code with reinforcement learning feed- YujiaLi,DavidChoi,JunyoungChung,NateKushman,
back. Julian Schrittwieser, Rémi Leblond, Tom Eccles,
James Keeling, Felix Gimeno, Agustin Dal Lago,
XueJiang,YihongDong,LechengWang,QiweiShang,
etal.2022d. Competition-levelcodegenerationwith
and Ge Li. 2023a. Self-planning code generation
alphacode. Science,378(6624):1092–1097.
withlargelanguagemodel. ArXiv,abs/2303.06689.
DianshuLiao,ShidongPan,QingHuang,XiaoxueRen,
XueJiang,YihongDong,LechengWang,FangZheng,
Zhenchang Xing, Huan Jin, and Qinying Li. 2023.
QiweiShang,GeLi,ZhiJin,andWenpinJiao.2023b.
Context-awarecodegenerationframeworkforcode
Self-planning code generation with large language
repositories: Local, global, and third-party library
models. ACMTransactionsonSoftwareEngineering
awareness. arXivpreprintarXiv:2312.05772.
andMethodology.
FangLiu,YangLiu,LinShi,HoukunHuang,Ruifeng
Hung Le, Hailin Chen, Amrita Saha, Akash Gokul,
Wang,ZhenYang,andLiZhang.2024a. Exploring
DoyenSahoo,andShafiqR.Joty.2023. Codechain:
and evaluating hallucinations in llm-powered code
Towards modular code generation through chain
generation. arXivpreprintarXiv:2404.00971.
of self-revisions with representative sub-modules.
ArXiv,abs/2310.08992.
Hao Liu, Yanlin Wang, Zhao Wei, Yong Xu, Juhong
TanKhangLe,SabaAlimadadi,andStevenYKo.2024. Wang,HuiLi,andRongrongJi.2023a. Refbert: A
Astudyofvulnerabilityrepairinjavascriptprograms two-stage pre-trained framework for automatic re-
withlargelanguagemodels. InCompanionProceed- namerefactoring. InProceedingsofthe32ndACM
ings of the ACM on Web Conference 2024, pages SIGSOFTInternationalSymposiumonSoftwareTest-
666–669. ingandAnalysis,pages740–752.
Dong Li, Yelong Shen, Ruoming Jin, Yi Mao, Kuan Jiate Liu, Yiqin Zhu, Kaiwen Xiao, Qiang Fu, Xiao
Wang, and Weizhu Chen. 2022a. Generation- Han,WeiYang,andDehengYe.2023b. Rltf: Rein-
augmentedqueryexpansionforcoderetrieval. ArXiv, forcementlearningfromunittestfeedback. ArXiv,
abs/2212.10692. abs/2307.04349.
Haochen Li, Chunyan Miao, Cyril Leung, Yanxian PengLiu,HeWang,ChenZheng,andYuqingZhang.
Huang, Yuan Huang, Hongyu Zhang, and Yan- 2024b. Prompt fix: Vulnerability automatic repair
lin Wang. 2022b. Exploring representation-level technologybased onprompt engineering. In 2024
augmentation for code search. arXiv preprint InternationalConferenceonComputing,Networking
arXiv:2210.12285. andCommunications(ICNC),pages116–120.IEEE.
Huayang Li, Yixuan Su, Deng Cai, Yan Wang, and
JianqiaoLu,WanjunZhong,YufeiWang,ZhijiangGuo,
LemaoLiu.2022c. Asurveyonretrieval-augmented
QiZhu,WenyongHuang,YanlinWang,FeiMi,Bao-
textgeneration. arXivpreprintarXiv:2202.01110.
junWang,YashengWang,etal.2024. Yoda:Teacher-
student progressive learning for language models.
JiaLi,GeLi,YongmingLi,andZhiJin.2023a. Struc-
arXivpreprintarXiv:2401.15670.
tured chain-of-thought prompting for code genera-
tion.
ShuaiLu,DayaGuo,ShuoRen,JunjieHuang,Alexey
Svyatkovskiy,AmbrosioBlanco,ColinB.Clement,
JiaLi,GeLi,ChongyangTao,HuangzhaoZhang,Fang
DawnDrain,DaxinJiang,DuyuTang,GeLi,Lidong
Liu, and Zhi Jin. 2023b. Large language model-
Zhou, Linjun Shou, Long Zhou, Michele Tufano,
awarein-contextlearningforcodegeneration. ArXiv,
MingGong,MingZhou,NanDuan,NeelSundare-
abs/2310.09748.
san, Shao Kun Deng, Shengyu Fu, and Shujie Liu.
JiaLi,YunfeiZhao,YongmingLi,GeLi,andZhiJin. 2021. Codexglue: Amachinelearningbenchmark
2023c. Acecoder: Utilizingexistingcodetoenhance datasetforcodeunderstandingandgeneration. ArXiv,
codegeneration. abs/2102.04664.
PengLi,TianxiangSun,QiongTang,HangYan,Yuan- Ziyang Luo, Can Xu, Pu Zhao, Qingfeng Sun, Xi-
binWu,XuanjingHuang,XipengQiuAcademyfor uboGeng,WenxiangHu,ChongyangTao,JingMa,
EngineeringTechnology, Fudan University, School QingweiLin,andDaxinJiang.2023. Wizardcoder:
of Materials Science, Technology, and East Empoweringcodelargelanguagemodelswithevol-
China Normal University. 2023d. Codeie: Large instruct. arXivpreprintarXiv:2306.08568.Wei Ma, Shangqing Liu, Zhihao Lin, Wenhan Wang, Rangeet Pan, Ali Reza Ibrahimzada, Rahul Krishna,
Qiang Hu, Ye Liu, Cen Zhang, Liming Nie, Li Li, Divya Sankar, Lambert Pouguem Wassi, Michele
and Yang Liu. 2023a. Lms: Understanding code Merler,BorisSobolev,RajuPavuluri,SaurabhSinha,
syntaxandsemanticsforcodeanalysis. and Reyhaneh Jabbarvand. 2024. Lost in transla-
tion: Astudyofbugsintroducedbylargelanguage
Yingwei Ma, Yue Yu, Shanshan Li, Yu Jiang, Yong modelswhiletranslatingcode. InProceedingsofthe
Guo,YuanliangZhang,YutaoXie,andXiangkeLiao. IEEE/ACM 46th International Conference on Soft-
2023b. Bridging code semantic and llms: Seman- wareEngineering,pages1–13.
ticchain-of-thoughtpromptingforcodegeneration.
ArXiv,abs/2310.10698. Arkil Patel, Siva Reddy, Dzmitry Bahdanau, and
PradeepDasigi.2023. Evaluatingin-contextlearn-
AmanMadaan, NiketTandon,PrakharGupta,Skyler ing of libraries for code generation. ArXiv,
Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, abs/2311.09635.
Nouha Dziri, Shrimai Prabhumoye, Yiming Yang,
Sean Welleck, Bodhisattwa Prasad Majumder, DebjitPaul,MeteIsmayilzada,MaximePeyrard,Beat-
Shashank Gupta, Amir Yazdanbakhsh, and Peter rizBorges,AntoineBosselut,RobertWest,andBoi
Clark. 2023. Self-refine: Iterative refinement with Faltings.2023. Refiner: Reasoningfeedbackonin-
self-feedback. ArXiv,abs/2303.17651. termediaterepresentations. ArXiv,abs/2304.01904.
YingjieMao,XiaoLi,ZongweiLi,andWenkaiLi.2024. GabrielPoesia,OleksandrPolozov,VuLe,AshishTi-
Automated smart contract summarization via llms. wari,GustavoSoares,ChristopherMeek,andSumit
arXivpreprintarXiv:2402.04863. Gulwani. 2022. Synchromesh: Reliable code gen-
eration from pre-trained language models. arXiv
Seungjun Moon, Yongho Song, Hyungjoo Chae, preprintarXiv:2201.11227.
DongjinKang,TaeyoonKwon,KaiTzuiunnOng,
SeungwonHwang,andJinyoungYeo.2023. Coffee: AlecRadford,JeffreyWu,RewonChild,DavidLuan,
Boostyourcodellmsbyfixingbugswithfeedback. DarioAmodei,IlyaSutskever,etal.2019. Language
ArXiv,abs/2311.07215. modelsareunsupervisedmultitasklearners. OpenAI
blog,1(8):9.
FangwenMu,LinShi,SongWang,ZhuohaoYu,Bin-
quanZhang,ChenxueWang,ShichaoLiu,andQing MohammadSadeghRasooliandJoelR.Tetreault.2015.
Wang. 2023. Clarifygpt: Empowering llm-based Yaraparser: Afastandaccuratedependencyparser.
codegenerationwithintentionclarification. ArXiv, ComputingResearchRepository,arXiv:1503.06733.
abs/2310.10996. Version2.
Niklas Muennighoff, Qian Liu, Qi Liu, Armel Ze- StevenP.Reiss,YunMiao,andQiXin.2014. Seeking
baze,QinkaiZheng,BinyuanHui,TerryYueZhuo, theuserinterface. AutomatedSoftwareEngineering,
SwayamSingh,XiangruTang,LeandrovonWerra, 25:157–193.
andS.Longpre.2023. Octopack: Instructiontuning
codelargelanguagemodels. ArXiv,abs/2308.07124. Xiaoxue Ren, Xinyuan Ye, Dehai Zhao, Zhenchang
Xing,andXiaohuYang.2023. Frommisusetomas-
Noor Nashid, Mifta Sintaha, and Ali Mesbah. 2023. tery: Enhancing code generation with knowledge-
Retrieval-based prompt selection for code-related driven ai chaining. 2023 38th IEEE/ACM Interna-
few-shot learning. 2023 IEEE/ACM 45th Interna- tionalConferenceonAutomatedSoftwareEngineer-
tionalConferenceonSoftwareEngineering(ICSE), ing(ASE),pages976–987.
pages2450–2462.
BaptisteRoziere,JonasGehring,FabianGloeckle,Sten
Changan Niu, Chuanyi Li, Vincent Ng, Jidong Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi,
Ge, LiGuo Huang, and Bin Luo. 2022. Spt- Jingyu Liu, Tal Remez, Jérémy Rapin, et al. 2023.
code:Sequence-to-sequencepre-trainingforlearning Codellama:Openfoundationmodelsforcode. arXiv
sourcecoderepresentations. 2022IEEE/ACM44th preprintarXiv:2308.12950.
InternationalConferenceonSoftwareEngineering
(ICSE),pages01–13. SachinK.Sen,GourChandraKarmakar,andShaoning
Pang.2023. Criticaldatadetectionfordynamically
MarcelOchs, KrishnaNarasimhan, andMiraMezini. adjustableproductqualityiniiot-enabledmanufac-
2023. Evaluating and improving transformers pre- turing. IEEEAccess,11:49464–49480.
trainedonastsforcodecompletion. In2023IEEE
InternationalConferenceonSoftwareAnalysis,Evo- BoShen,JiaxinZhang,TaihongChen,DaoguangZan,
lutionandReengineering(SANER),pages834–844. BingGeng,AnFu,MuhanZeng,AilunYu,Jichuan
Ji,JingyangZhao,etal.2023. Pangu-coder2: Boost-
RPan,ARIbrahimzada,RKrishna,DSankar,LPWassi, inglargelanguagemodelsforcodewithrankingfeed-
MMerler,BSobolev,RPavuluri,SSinha,andRJab- back. arXivpreprintarXiv:2307.14936.
barvand. Understanding the effectiveness of large
languagemodelsincodetranslation.preprint(2023). Ensheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang,
arXivpreprintarXiv:2308.03109. ShiHan,DongmeiZhang,andHongbinSun.2021.Cast: Enhancing code summarization with hierar- YanqiSu,DianshuLiao,ZhenchangXing,QingHuang,
chicalsplittingandreconstructionofabstractsyntax Xiwei Xu, and Qinghua Lu. 2023. Enhancing ex-
trees. arXivpreprintarXiv:2108.12987. ploratorytestingbylargelanguagemodelandknowl-
edgegraph.
Ensheng Shi, Yanlin Wang, Lun Du, Hongyu Zhang,
ShiHan,DongmeiZhang,andHongbinSun.2023a. ChuyueSun, YingSheng, OdedPadon, andClarkW.
Cocoast: representingsourcecodeviahierarchical Barrett.2023a. Clover: Closed-loopverifiablecode
splittingandreconstructionofabstractsyntaxtrees. generation. ArXiv,abs/2310.17807.
EmpiricalSoftwareEngineering,28(6):135.
Weisong Sun, Chunrong Fang, Yudu You, Yun Miao,
Ensheng Shi, Yanlin Wang, Wenchao Gu, Lun Du, YiLiu,YuekangLi,GeleiDeng,ShenghanHuang,
HongyuZhang,ShiHan,DongmeiZhang,andHong- YuchenChen, QuanjunZhang, etal.2023b. Auto-
bin Sun. 2023b. Cocosoda: Effective contrastive maticcodesummarizationviachatgpt: Howfarare
learningforcodesearch. In2023IEEE/ACM45th we? arXivpreprintarXiv:2305.12865.
InternationalConferenceonSoftwareEngineering
(ICSE),pages2198–2210.IEEE. Alexey Svyatkovskiy, Shao Kun Deng, Shengyu Fu,
and Neel Sundaresan. 2020. Intellicode compose:
codegenerationusingtransformer. Proceedingsof
EnshengShi,YanlinWang,WeiTao,LunDu,Hongyu
the28thACMJointMeetingonEuropeanSoftware
Zhang,ShiHan,DongmeiZhang,andHongbinSun.
EngineeringConferenceandSymposiumontheFoun-
2022. Race: Retrieval-augmentedcommitmessage
dationsofSoftwareEngineering.
generation. arXivpreprintarXiv:2203.02700.
WeiTao,YuchengZhou,YanlinWang,HongyuZhang,
EnshengShi,YanlinWang,HongyuZhang,LunDu,Shi
HaofenWang,andWenqiangZhang.2024. Kadel:
Han,DongmeiZhang,andHongbinSun.2023c. To-
Knowledge-aware denoising learning for commit
wardsefficientfine-tuningofpre-trainedcodemod-
messagegeneration. ACMTransactionsonSoftware
els: An experimental study and beyond. In Pro-
EngineeringandMethodology.
ceedingsofthe32ndACMSIGSOFTInternational
SymposiumonSoftwareTestingandAnalysis,pages
RomalThoppilan,DanielDeFreitas,JamieHall,Noam
39–51.
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.
Ensheng Shi, Fengji Zhang, Yanlin Wang, Bei Chen,
2022a. Lamda: Languagemodelsfordialogapplica-
LunDu,HongyuZhang,ShiHan,DongmeiZhang,
tions. arXivpreprintarXiv:2201.08239.
andHongbinSun.2023d. Sotana: Theopen-source
software development assistant. arXiv preprint
Romal Thoppilan, Daniel De Freitas, Jamie Hall,
arXiv:2308.13416.
Noam M. Shazeer, Apoorv Kulshreshtha, Heng-
Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker,
Jiho Shin, Clark Tang, Tahmineh Mohati, Maleknaz
Yu Du, Yaguang Li, Hongrae Lee, Huaixiu Steven
Nayebi, Song Wang, and Hadi Hemmati. 2023.
Zheng,AminGhafouri,MarceloMenegali,Yanping
Prompt engineering or fine tuning: An empirical
Huang, Maxim Krikun, Dmitry Lepikhin, James
assessmentoflargelanguagemodelsinautomated
Qin, Dehao Chen, Yuanzhong Xu, Zhifeng Chen,
softwareengineeringtasks. ArXiv,abs/2310.10508.
AdamRoberts,MaartenBosma,YanqiZhou,Chung-
ChingChang,I.A.Krivokon,WillardJamesRusch,
NoahShinn,FedericoCassano,BeckLabash,Ashwin
MarcPickett, KathleenS.Meier-Hellstern, Mered-
Gopinath, Karthik Narasimhan, and Shunyu Yao.
ithRingelMorris,TulseeDoshi,RenelitoDelosSan-
2023. Reflexion: language agents with verbal re-
tos, Toju Duke, Johnny Hartz Søraker, Ben Zeven-
inforcement learning. In Neural Information Pro-
bergen,VinodkumarPrabhakaran,MarkDíaz,Ben
cessingSystems.
Hutchinson,KristenOlson,AlejandraMolina,Erin
Hoffman-John,JoshLee,LoraAroyo,RaviRajaku-
Parshin Shojaee, Aneesh Jain, Sindhu Tipirneni, and mar,AlenaButryna,MatthewLamm,V.O.Kuzmina,
Chandan K. Reddy. 2023. Execution-based code JosephFenton,AaronCohen,RachelBernstein,Ray
generationusingdeepreinforcementlearning. ArXiv, Kurzweil, Blaise Aguera-Arcas, Claire Cui, Mar-
abs/2301.13816. ianRogersCroak,EdHuaihsinChi,andQuocLe.
2022b. Lamda: Languagemodelsfordialogapplica-
Disha Shrivastava, Denis Kocetkov, Harm de Vries, tions. ArXiv,abs/2201.08239.
DzmitryBahdanau,andTorstenScholak.2023. Re-
pofusion: Trainingcodemodelstounderstandyour ZhaoTianandJunjieChen.2023. Test-case-drivenpro-
repository. arXivpreprintarXiv:2306.10998. grammingunderstandinginlargelanguagemodels
forbettercodegeneration. ArXiv,abs/2309.16120.
Yiheng Shu, Zhiwei Yu, Yuhan Li, Börje F Karlsson,
TingtingMa,YuzhongQu,andChin-YewLin.2022. BingWang,ChangyuRen,JianYang,XinnianLiang,
Tiara: Multi-grainedretrievalforrobustquestionan- JiaqiBai,Qian-WenZhang,ZhaoYan,andZhoujun
sweringoverlargeknowledgebases. arXivpreprint Li. 2023a. Mac-sql: Multi-agent collaboration for
arXiv:2210.12925. text-to-sql. arXivpreprintarXiv:2312.11242.Chong Wang, Jian Zhang, Yebo Feng, Tianlin JiayiWei,GregDurrett,andIs¸ılDillig.2023a. Coeditor:
Li, Weisong Sun, Yang Liu, and Xin Peng. Leveragingcontextualchangesformulti-roundcode
2024a. Teaching code llms to use autocompletion auto-editing. ArXiv,abs/2305.18584.
tools in repository-level code generation. ArXiv,
abs/2401.06391. YuxiangWei,ZheWang,JiaweiLiu,YifengDing,and
LingmingZhang.2023b. Magicoder: Sourcecodeis
Chong Wang, Jian Zhang, Yebo Feng, Tianlin Li, allyouneed. ArXiv,abs/2312.02120.
Weisong Sun, Yang Liu, and Xin Peng. 2024b.
Teaching code llms to use autocompletion tools in YuxiangWei,ChunXia,andLingmingZhang.2023c.
repository-levelcodegeneration. Copilotingthecopilots: Fusinglargelanguagemod-
elswithcompletionenginesforautomatedprogram
DezeWang,BoxingChen,ShanshanLi,WeiLuo,Shao-
repair. Proceedingsofthe31stACMJointEuropean
liang Peng, Wei Dong, and Xiang ke Liao. 2023b.
SoftwareEngineeringConferenceandSymposiumon
Oneadapterforallprogramminglanguages? adapter
theFoundationsofSoftwareEngineering.
tuning for code search and summarization. 2023
IEEE/ACM 45th International Conference on Soft-
Martin Weyssow, Xin Zhou, Kisub Kim, David Lo,
wareEngineering(ICSE),pages5–16.
andHouariA.Sahraoui.2023. Exploringparameter-
efficientfine-tuningtechniquesforcodegeneration
LeiWang,ChenMa,XueyangFeng,ZeyuZhang,Hao
withlargelanguagemodels. ArXiv,abs/2308.10462.
Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang,
XuChen,YankaiLin,etal.2024c. Asurveyonlarge
ZhiyongWu,YaoxiangWang,JiachengYe,andLing-
languagemodelbasedautonomousagents. Frontiers
peng Kong. 2022. Self-adaptive in-context learn-
ofComputerScience,18(6):186345.
ing: An information compression perspective for
XinWang,YashengWang,YaoWan,FeiMi,YitongLi, in-context example selection and ordering. In An-
nualMeetingoftheAssociationforComputational
PingyiZhou,JinLiu,HaoWu,XinJiang,andQun
Liu.2022. Compilableneuralcodegenerationwith
Linguistics.
compilerfeedback. InFindings.
ZhihengXi,WenxiangChen,XinGuo,WeiHe,Yiwen
XingyaoWang,ZihanWang,JiatengLiu,YangyiChen, Ding, Boyang Hong, Ming Zhang, Junzhe Wang,
Lifan Yuan, Hao Peng, and Heng Ji. 2023c. Mint: Senjie Jin, Enyu Zhou, et al. 2023. The rise and
Evaluatingllmsinmulti-turninteractionwithtools potential of large language model based agents: A
andlanguagefeedback. ArXiv,abs/2309.10691. survey. arXivpreprintarXiv:2309.07864.
YanlinWang,LianghongGuo,EnshengShi,Wenqing ChunXia,YifengDing,andLingmingZhang.2023a.
Chen,JiachiChen,WanjunZhong,MenghanWang, The plastic surgery hypothesis in the era of large
HuiLi,HongyuZhang,ZiyuLyu,etal.2023d. You language models. 2023 38th IEEE/ACM Interna-
augmentme: Exploringchatgpt-baseddataaugmen- tionalConferenceonAutomatedSoftwareEngineer-
tationforsemanticcodesearch. In2023IEEEInter- ing(ASE),pages522–534.
nationalConferenceonSoftwareMaintenanceand
Evolution(ICSME),pages14–25.IEEE. ChunXia,YifengDing,andLingmingZhang.2023b.
Revisiting the plastic surgery hypothesis via large
Yanlin Wang, Yanxian Huang, Daya Guo, Hongyu languagemodels. ArXiv,abs/2303.10494.
Zhang, and Zibin Zheng. 2024d. Sparsecoder:
Identifier-awaresparsetransformerforfile-levelcode YingtaoXie,TaoLin,andHongyanXu.2019. Userin-
summarization. arXivpreprintarXiv:2401.14727. terfacecoderetrieval: Anovelvisual-representation-
awareapproach. IEEEAccess,7:162756–162767.
Yanlin Wang and Hui Li. 2021. Code completion by
modeling flattened abstract syntax trees as graphs.
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
InProceedingsoftheAAAIconferenceonartificial
PuZhao,JiazhanFeng,ChongyangTao,andDaxin
intelligence,volume35,pages14015–14023.
Jiang. 2023a. Wizardlm: Empowering large lan-
guagemodelstofollowcomplexinstructions. arXiv
YanlinWang,EnshengShi,LunDu,XiaodiYang,Yux-
preprintarXiv:2304.12244.
uan Hu, Shi Han, Hongyu Zhang, and Dongmei
Zhang. 2021. Cocosum: Contextual code summa-
Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,
rizationwithmulti-relationalgraphneuralnetwork.
PuZhao,JiazhanFeng,ChongyangTao,andDaxin
arXivpreprintarXiv:2107.01933.
Jiang. 2023b. Wizardlm: Empowering large lan-
Yanlin Wang, Yanli Wang, Daya Guo, Jiachi Chen, guagemodelstofollowcomplexinstructions. ArXiv,
RuikaiZhang, YuchiMa, andZibinZheng.2024e. abs/2304.12244.
Rlcoder: Reinforcementlearningforrepository-level
codecompletion. arXivpreprintarXiv:2407.19487. Guang Yang, Yu Zhou, Xiang Chen, Xiangyu
Zhang, Terry Yue Zhuo, and Taolue Chen. 2023a.
ZejunWang,JiaLi,GeLi,andZhiJin.2023e. Chat- Chain-of-thought in neural code generation: From
coder: Chat-basedrefinerequirementimprovesllms’ and for lightweight language models. ArXiv,
codegeneration. ArXiv,abs/2311.00272. abs/2312.05562.LinyiYang,ShuibaiZhang,ZhuohaoYu,Guangsheng KechiZhang,GeLi,JiaLi,ZhuoLi,andZhiJin.2023d.
Bao, Yidong Wang, Jindong Wang, Ruochen Xu, Toolcoder: Teachcodegenerationmodelstouseapi
Weirong Ye, Xing Xie, Weizhu Chen, and Yue searchtools. ArXiv,abs/2305.04032.
Zhang.2023b. Supervisedknowledgemakeslarge
Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin.
language models better in-context learners. ArXiv,
2024. Codeagent: Enhancingcodegenerationwith
abs/2312.15918.
tool-integrated agent systems for real-world repo-
levelcodingchallenges. ArXiv,abs/2401.07339.
XiYe,SemihYavuz,KazumaHashimoto,YingboZhou,
and Caiming Xiong. 2021. Rng-kbqa: Generation
KechiZhang,ZhuoLi,JiaLi,GeLi,andZhiJin.2023e.
augmentediterativerankingforknowledgebaseques-
Self-edit: Fault-awarecodeeditorforcodegenera-
tionanswering. arXivpreprintarXiv:2109.08678.
tion. ArXiv,abs/2305.04087.
DaoguangZan,BeiChen,ZeqiLin,BeiGuan,Yongji Sheng Zhang, Hui Li, Yanlin Wang, Zhao Wei,
Wang,andJian-GuangLou.2022a. Whenlanguage Yong Xiu, Juhong Wang, and Rongong Ji. 2023f.
modelmeetsprivatelibrary. InConferenceonEm- Code search debiasing: Improve search results be-
piricalMethodsinNaturalLanguageProcessing. yond overall ranking performance. arXiv preprint
arXiv:2311.14901.
Daoguang Zan, Bei Chen, Dejian Yang, Zeqi Lin,
MinsuKim,BeiGuan,YongjiWang,WeizhuChen, Yuwei Zhang, Ge Li, Zhi Jin, and Ying Xing. 2023g.
and Jian-Guang Lou. 2022b. Cert: continual pre- Neural program repair with program dependence
trainingonsketchesforlibrary-orientedcodegenera- analysis and effective filter mechanism. ArXiv,
tion. arXivpreprintarXiv:2206.06888. abs/2305.09315.
Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren
DaoguangZan,BeiChen,DejianYang,ZeqiLin,Minsu
Wang, Yunteng Geng, Fangcheng Fu, Ling Yang,
Kim, Bei Guan, Yongji Wang, Weizhu Chen, and
Wentao Zhang, and Bin Cui. 2024. Retrieval-
Jian-GuangLou.2022c. Cert: Continualpre-training
augmented generation for ai-generated content: A
onsketchesforlibrary-orientedcodegeneration. In
survey. arXivpreprintarXiv:2402.19473.
International Joint Conference on Artificial Intelli-
gence. Ruochen Zhao, Xingxuan Li, Shafiq Joty, Chengwei
Qin, and Lidong Bing. 2023. Verify-and-edit: A
DaoguangZan,AilunYu,BoShen,JiaxinZhang,Tai- knowledge-enhanced chain-of-thought framework.
hongChen,BingGeng,B.Chen,JichuanJi,Yafen arXivpreprintarXiv:2305.03268.
Yao,YongjiWang,andQianxiangWang.2023. Can
programminglanguagesboosteachotherviainstruc- ZZheng, KNing, JChen, YWang, WChen, LGuo,
tiontuning? ArXiv,abs/2308.16824. andWWang. Towardsanunderstandingoflargelan-
guagemodelsinsoftwareengineeringtasks(2023).
ChenyuanZhang,HaoLiu,JiutianZeng,KejingYang, arXivpreprintarXiv:2308.11396.
Yuhong Li, and Hui Li. 2023a. Prompt-enhanced
Zibin Zheng, Kaiwen Ning, Yanlin Wang, Jingwen
softwarevulnerabilitydetectionusingchatgpt. ArXiv,
Zhang, Dewu Zheng, Mingxi Ye, and Jiachi Chen.
abs/2308.12697.
2023. Asurveyoflargelanguagemodelsforcode:
Evolution,benchmarking,andfuturetrends. arXiv
FengjiZhang,B.Chen,YueZhang,JinLiu,Daoguang
preprintarXiv:2311.10372.
Zan, Yi Mao, Jian-Guang Lou, and Weizhu Chen.
2023b. Repocoder: Repository-levelcodecomple-
WanjunZhong,LianghongGuo,QiqiGao,HeYe,and
tion through iterative retrieval and generation. In
YanlinWang.2024. Memorybank: Enhancinglarge
Conference on Empirical Methods in Natural Lan-
language models with long-term memory. In Pro-
guageProcessing.
ceedingsoftheAAAIConferenceonArtificialIntelli-
gence,volume38,pages19724–19731.
HeZhang,MuhammadAliBabar,andPaoloTell.2011.
Identifyingrelevantstudiesinsoftwareengineering. Jianpeng Zhou, Wanjun Zhong, Yanlin Wang, and Ji-
Information and Software Technology, 53(6):625– ahai Wang. 2023. Adaptive-solver framework for
637. dynamicstrategyselectioninlargelanguagemodel
reasoning. arXivpreprintarXiv:2310.01446.
Jian Zhang, Xu Wang, Hongyu Zhang, Hailong Sun,
and Xudong Liu. 2020. Retrieval-based neural Shuyan Zhou, Uri Alon, Frank F. Xu, Zhiruo
sourcecodesummarization. InProceedingsofthe Wang,ZhengbaoJiang,andGrahamNeubig.2022.
ACM/IEEE42ndInternationalConferenceonSoft- Docprompting: Generating code by retrieving the
wareEngineering,pages1385–1397. docs. InInternationalConferenceonLearningRep-
resentations.
Jinchao Zhang, Jue Wang, Huan Li, Lidan Shou,
XinZhou, SicongCao, XiaobingSun, andDavidLo.
KeChen,GangChen,andSharadMehrotra.2023c.
2024. Large language model for vulnerability de-
Draft & verify: Lossless large language model ac-
tection and repair: Literature review and roadmap.
celeration via self-speculative decoding. ArXiv,
arXivpreprintarXiv:2404.02525.
abs/2309.08168.