1
SGFormer: Single-Layer Graph Transformers
with Approximation-Free Linear Complexity
Qitian Wu, Kai Yang, Hengrui Zhang, David Wipf, Junchi Yan
Abstract—Learningrepresentationsonlargegraphsisalong-standingchallengeduetotheinter-dependencenature.Transformers
recentlyhaveshownpromisingperformanceonsmallgraphsthankstoitsglobalattentionforcapturingall-pairinteractionsbeyond
observedstructures.ExistingapproachestendtoinheritthespiritofTransformersinlanguageandvisiontasks,andembrace
complicatedarchitecturesbystackingdeepattention-basedpropagationlayers.Inthispaper,weattempttoevaluatethenecessityof
adoptingmulti-layerattentionsinTransformersongraphs,whichconsiderablyrestrictstheefficiency.Specifically,weanalyzeageneric
hybridpropagationlayer,comprisedofall-pairattentionandgraph-basedpropagation,andshowthatmulti-layerpropagationcanbe
reducedtoone-layerpropagation,withthesamecapabilityforrepresentationlearning.Itsuggestsanewtechnicalpathforbuilding
powerfulandefficientTransformersongraphs,particularlythroughsimplifyingmodelarchitectureswithoutsacrificingexpressiveness.As
exemplifiedbythiswork,weproposeaSimplifiedSingle-layerGraphTransformers(SGFormer),whosemaincomponentisasingle-layer
globalattentionthatscaleslinearlyw.r.t.graphsizesandrequiresnoneofanyapproximationforaccommodatingall-pairinteractions.
Empirically,SGFormersuccessfullyscalestotheweb-scalegraphOGBN-PAPERS100M,yieldingorders-of-magnitudeinference
accelerationoverpeerTransformersonmedium-sizedgraphs,anddemonstratescompetitivenesswithlimitedlabeleddata.
IndexTerms—GraphRepresentationLearning,GraphNeuralNetworks,Transformers,LinearAttention,Scalability,Efficiency
✦
1 INTRODUCTION
LEARNINGonlargegraphsthatconnectinterdependent unobservedinteractions,andleadstosuperiorperformance
data points is a fundamental challenge in machine overgraphneuralnetworks(GNNs)insmall-graph-based
learning and pattern recognition, with a broad spectrum applications[17],[18],[19],[20],[21].
of applications ranging from social sciences to natural However,aconcerningtrendincurrentarchitecturesis
sciences[1],[2],[3],[4],[5].Onekeyproblemishowtoobtain theirtendencytoautomaticallyadoptthedesignphilosophy
effective node representations, i.e., the low-dimensional ofTransformersusedinvisionandlanguagetasks[22],[23],
vectors (a.k.a. embeddings) that encode the semantic and [24].Thisinvolvesstackingdeepmulti-headattentionlayers,
topologicalfeatures,especiallyunderlimitedcomputation which results in large model sizes and the data-hungry
budget(e.g.,timeandspace),thatcanbeefficientlyutilized natureofthemodel.However,thisdesignapproachposes
fordownstreamtasks. a significant challenge for Transformers in scaling to large
Recently,Transformershaveemergedasapopularclass graphswherethenumberofnodescanreachuptomillions
of foundation encoders for graph-structured data by treat- orevenbillions,particularlyduetotwo-foldobstacles.
ing nodes in the graph as input tokens and have shown 1) The global all-pair attention mechanism is the key
highly competitive performance on graph-level tasks [6], componentofmodernTransformers.Becauseoftheglobal
[7],[8],[9],[10]andnode-leveltasks[11],[12],[13],[14]on attention, the time and space complexity of Transformers
graph data. The global attention in Transformers [15] can often scales quadratically with respect to the number of
captureimplicitinter-dependenciesamongnodesthatarenot nodes, and the computation graph grows exponentially
embodiedbyinputgraphstructures,butcouldpotentially as the number of layers increases. Thereby, training deep
makeadifferenceindatageneration(e.g.,theundetermined Transformers for large graphs with millions of nodes can
structuresofproteinsthatlackknowntertiarystructures[16], be extremely resource-intensive and may require delicate
[17]).ThisadvantageprovidesTransformerswiththedesired techniques for partitioning the inter-connected nodes into
expressivityforcapturinge.g.,long-rangedependenciesand smaller mini-batches in order to mitigate computational
overhead[12],[13],[14],[25].
2) In small-graph-based tasks such as graph-level pre-
• QitianWuiswithBroadInstituteofMITandHarvard.E-mail:wuqi-
tian@broadinstitute.org. diction for molecular property [26], where each instance
• Kai Yang is with Department of Computer Science and Engineering, isagraph,andtherearetypicallyabundantlabeledgraph
ShanghaiJiaoTongUniversity.E-mail:icarus1411@sjtu.edu.cn.
instances,largeTransformersmayhavesufficientsupervision
• Hengrui Zhang is with University of Illinois, Chicago. E-mail:
hzhan55@uic.edu. forgeneralization.However,inlarge-graph-basedtaskssuch
• DavidWipfiswithAmazonWebService.E-mail:davidwipf@gmail.com. as node-level prediction for protein functions [27], where
• JunchiYaniswithSchoolofArtificialIntelligence,ShanghaiJiaoTong thereisusuallyasinglegraphandeachnodeisaninstance,
University.E-mail:yanjunchi@sjtu.edu.cn.
labeled nodes can be relatively limited. This increases the
This paper is an expanded version from the conference paper SGFormer: difficultyofTransformerswithcomplicatedarchitecturesin
SimplifyingandEmpoweringTransformersforLarge-GraphRepresen-
learningeffectiverepresentationsinsuchcases.
tations which is published in Advances in Neural Information Processing
Systems(NeurIPS)heldonDec10-16,2023inNewOrleans,U.S. Thispaperpresentsanattempttoinvestigatethenecessity
4202
peS
31
]GL.sc[
1v70090.9042:viXra2
layer 1 layer 2 layer K Contour of energy heat map
multi-layer
one-layer
...
Energy of layer 1
Energy o.f l.a.yer 2
local smooth global smooth
Energy of layer K first dimension of z
(a) Two-fold regularization effect (b) Multi-layer model as optimization trajectories (c) Multi-layer v.s. one-layer models
Fig.1.IllustrationofthemaintheoreticalresultsinSec.3.(a)Thelayer-wiseupdatingruleofmessagepassingmodels(e.g.,GNNsandTransformers)
isequivalenttoagradientdescentstepminimizingaregularizedenergyingraphsignaldenoising.Theenergyhastwo-foldregularizationeffects,
whichenforcelocalandglobalsmoothness,respectively.(b)CommonTransformersstackingmultiplepropagationlayerscanbeseenasacascade
ofdescentstepsonlayer-dependentenergy(sincetheattentionscoresandfeaturetransformationsarespecifictoeachlayer).(c)Themulti-layer
modelcanbereducedtoaone-layermodelwherethelattercontributestothesamedenoisingeffect,i.e.,yieldingtheequivalentoutputembeddings.
ofusingdeeppropagationlayersinTransformersforgraph havenegligiblysmallapproximationerrorcomparedtothe
representationsandexploreanewtechnicalpathforsimpli- updatedembeddingsyieldedbythemulti-layermodel(see
fyingTransformerarchitecturesthatcanscaletolargegraphs. Fig.1(c)).Particularly,suchasingle-layermodelalsoadopts
Particularly, we start from the interpretation of message- apropagationlayerofthehybridformandcorrespondstoa
passing-basedpropagationlayers(i.e.,aTransformerorGNN singlegradientdescentsteponafixedenergy.Thelatter,in
layer) as optimization dynamics of a classic graph signal principle,reducestheredundancywithintheoptimization
denoising problem. This viewpoint lends us a principled dynamics for graph signal denoising. This result implies
way to reveal the underlying mechanism of graph neural that multi-layer propagation can be reduced to one-layer
networks and Transformers, based on which we naturally propagation that can achieve equivalent (up to negligible
derive a hybrid propagation layer that combines global approximationerror)expressivenessforrepresentations.It
attentionandgraph-basedpropagationinonceupdates.This furtherenlightensapotentialwaytosimplifyTransformer
hybrid layer can be seen as a generalization of common architecturesongraphswithoutsacrificingeffectiveness.
GNNs’ and vanilla Transformer’s propagation layers by ▶Q3: How to Unleash the Power of Single-Layer
interpolationbetweentwomodelclasses,andsuchadesign Attention? In light of the theoretical results, we propose
is also adopted by state-of-the-art Transformer models on anencoderbackboneforlearningrepresentationsonlarge
graphs, e.g., [10], [12], [14]. On top of this, we answer the graphs, referred to as Simplified Graph Transformer (SG-
followingresearchquestionswiththeoreticalanalysis,model Former).Specifically,SGFormeradoptsahybridarchitecture
designsandempiricalresultsasourmaincontributions. thatlinearlycombinesasingle-layerglobalattentionanda
▶Q1:IsMulti-LayerPropagationNecessary?Weshow GNN network. In particular, we propose a simple global
thatonehybridpropagationlayercorrespondstoaone-step attention function that linearly scales w.r.t. the number
gradientdescentonaregularizedenergythatenforcescertain of nodes and accommodates all-pair interactions without
smoothness effects for node representations. In particular, usinganyapproximation.IntermsoftheGNNnetwork,we
the smootheness effects are two-fold that facilitate local simplyinstantiateitasashallowGCNwhosecomputation
and global regularization (see Fig. 1(a)). Since the energy is desirably efficient on large graphs. Equipped with such
functiondependsonthelayer-specificattentionscoresand designs, SGFormer shows expressiveness to capture the
feature transformation weights, a model involving multi- implicitdependenciesand,meanwhile,incorporatesgraph
layerpropagationcanbeseenasacascadeofdescentsteps inductive bias. Moreover, compared to peer graph Trans-
minimizingdifferentobjectives.Inthisregard,usingmulti- formers,SGFormerrequiresnopositionalencodings,feature
layer propagation may lead to potential redundancy from pre-processing,extralossfunctions,oredgeembeddings.
the perspective of graph signal denoising, since the layer- ▶Q4:HowDoesASimpleTransformerModelPerform?
wiseupdatescontributetodisparatesmoothingeffectsand Despite using simple architecture, experiments show that
could interfere with each other (see Fig. 1(b)). Mapping SGFormer achieves highly competitive performance in an
backtomodelarchitecturaldesigns,sucharedundancychal- extensiverangeofnodepropertypredictiondatasets,which
lengesthenecessityofstackingdeeppropagationlayersin areusedascommonbenchmarksformodelevaluationw.r.t.
Transformers,whichconsiderablyrestrictsthecomputational representation learning on large graphs. In terms of the
efficiencyforscalingtolargegraphs. efficiency,onmedium-sizedgraphs,wherethenodenumbers
▶Q2: How Powerful Is Single-Layer Propagation? range from 1K to 10K, SGFormer achieves up to 20x and
Based on the above result, we prove that for any model 30x speedup in terms of training and inference time costs,
stacking multiple propagation layers, there exists an en- respectively,overrecentlyproposedscalableTransformers.In
ergy function such that one-step gradient descent from termsofscalability,thetimeandmemorycostsofSGFormer
the initial point yields the equivalent denoising effects both scale linearly w.r.t. graph sizes, with lower growth
as the multi-layer model. Pushing further, there exists a rate than existing linearly-complex Transformers. Notably,
single-layerpropagationmodelwhoseupdatedembeddings SGFormercanscalesmoothlytotheweb-scalegraphOGBN-
z
fo
noisnemid
dnoces3
PAPERS100Mwith0.1Bnodes,whereexistingmodelstoour 2.2 GraphTransformers
knowledgefailtodemonstrate.Inaddition,SGFormershows
Beyondmessagepassingwithinlocalneighborhoods,Trans-
superiorperformancewithlimitedlabeleddatafortraining.
formers have recently gained attention as powerful graph
We also conduct thorough ablation studies that validate
encoders [6], [7], [8], [9], [10], [12], [39], [40], [41]. These
the effectiveness of the proposed designs, particularly, the
models use global all-pair attention, which aggregates all
advantage of single-layer attentions over multi-layer ones.
nodeembeddingstoupdatetherepresentationofeachnode:
Theimplementationispubliclyavailableathttps://github.
com/qitianwu/SGFormer. z( uk+1) =η(k)(z( uk+1)), z( uk+1) =Agg({z( vk)|v ∈V}). (2)
The global attention can be seen as a generalization of
2 PRELIMINARY AND BACKGROUND GNNs’messagepassingtoadenselyconnectedgraphwhere
Inthissection,weintroducenotationsasbuildingblocksof
R(u)=V,andequipsthemodelwiththeabilitytocapture
theanalysisandtheproposedmodel.Inthemeanwhile,we unobservedinteractionsandlong-rangedependence.
brieflyreviewtheliteraturerelatedtothepresentwork.
However,theall-pairattentionincursO(N2)complexity
Notations. We denote a graph as G = (V,E) where and becomes a computation bottleneck that limits most
the node set V comprises N nodes and the edge set Transformerstohandlingonlysmall-sizedgraphs(withup
E = {(u,v) | a uv = 1} is defined by a symmetric (and tohundredsofnodes).Forlargergraphs,recenteffortshave
usually sparse) adjacency matrix A = [a uv] N×N, where resorted to strategies such as sampling a small (relative
a uv = 1 if node u and v are connected, and 0 otherwise. to N) subset of nodes for attention computation [42] or
DenotebyD = diag({d u}N u=1)thediagonaldegreematrix using ego-graph features as input tokens [11], [13]. These
ofA,whered
u
=(cid:80)N v=1a uv.EachnodehasaD-dimensional strategies sacrifice the expressivity needed to capture all-
input feature vector x u ∈ RD and a label y u which can pair interactions among arbitrary nodes. Another line of
recent works designs new attention mechanisms that can
be a scalar or a vector. The nodes in the graph are only
partially labeled, forming a node set denoted as V tr ⊂ V efficiently achieve all-pair message passing within linear
(wherein |V tr| ≪ N). Learning representations on graphs complexity [10], [12]. Nevertheless, these schemes require
aimstoproducenodeembeddingsz u ∈Rd thatareuseful approximationthatcanleadtotraininginstability.
AnotherobservationisthatnearlyalloftheTransformers
for downstream tasks. The size of the graph, as measured
bythenumberofnodesN,canbearbitrarilylarge,usually mentionedabovetendtostackdeepattentionlayers,inline
withthedesignoflargemodelsusedinvisionandlanguage
rangingfromthousandstobillions.
tasks [22], [23], [24]. However, this architecture presents
challengesforscalingtoindustry-scalegraphs,whereN can
2.1 GraphNeuralNetworks
reachbillions.Moreover,duetothecomplicatedarchitecture,
Graph Neural Networks (GNNs) [28], [29] compute node the model can become vulnerable to overfitting when the
embeddingsthroughmessagepassingrulesoverobserved numberoflabelednodes|V tr|ismuchsmallerthanN.Thisis
structures. The layer-wise message passing of GNNs can acommonissueinextremelylargegraphswherenodelabels
be defined as recursively propagating the embeddings of arescarce[27].Thequestionremainshowtobuildanefficient
neighboringnodestoupdatethenoderepresentation: andscalableTransformermodelthatmaintainsthedesired
z(k+1) =η(k)(z(k+1)), z(k+1) =Agg({z(k)|v ∈R(u)}), expressivenessforlearningeffectivegraphrepresentations.
u u u v
(1)
where z( uk) ∈ Rd denotes the embedding at the k-th layer, 2.3 Node-Levelv.s.Graph-LevelTasks
η denotes (parametric) feature transformation, and Agg is Beforegoingtoourmethodology,wewouldliketopinpoint
an aggregation function over the embeddings of nodes in thedifferencesbetweentwograph-basedpredictivetasksof
R(u).Thelatteristhereceptivefieldofnodeudetermined wideinterest.Node-leveltasks(ourfocus)targetasinglegraph
byG.CommonGNNs,suchasGCN[29]andGAT[30],along
connectingalltheinstancesasnodeswhereeachinstancehas
withtheirnumeroussuccessors,e.g.,[31],[32],[33],[34],[35], alabeltopredict.Differently,ingraph-leveltasks,eachinstance
typicallyassumeR(u)tobethesetofneighboringnodesin
(e.g.,molecule)itselfisagraphwithalabel,andgraphsizes
G. By stacking multiple layers of local message passing as areoftensmall,incontrastwiththearbitrarilylargegraph
definedby(1),themodelcanintegrateinformationfromthe innodeclassificationdependingonthenumberofinstances
localneighborhoodintotherepresentation. inadataset.Thedifferentinputscalesresultinthatthetwo
However, since the number of neighboring nodes in- problemsoftenneeddisparatetechnicalconsiderations[27].
volved in the computation exponentially increases as the While GNNs exhibit comparable competitiveness in both
layernumbergoesup,theaggregatedinformationfromdis- tasks, most of current Transformers are tailored for graph
tantnodeswillbedilutedwithanexponentialratew.r.t.the classification (on small graphs) [6], [7], [8], [9], [10], [39],
modeldepth.Thisproblemreferredtoasover-squashing[36] [40],[41],[43],anditstillremainslargelyunder-exploredto
canlimittheexpressivenessofGNNsforlearningeffective design powerful and efficient Transformers for node-level
representations. Moreover, recent evidence suggests that tasksonlargegraphs[11],[12],[44].
GNNsyieldunsatisfactoryperformanceinthecaseofgraphs
with heterophily [33], long-range dependencies [37] and
3 THEORETICAL ANALYSIS AND MOTIVATION
structuralincompleteness[38].Thisurgesthecommunityto
explorenewarchitecturesthatcanovercomethelimitations Before we introduce the proposed model, we commence
ofGNNs’localmessagepassing. withmotivationfromthetheoreticalperspectivewhichsheds4
someinsightsonthemodeldesign.Basedonthediscussions GiventhesymmetricpropertyofW(k),wehavethegradient
inSec.2,ouranalysisinthissectionaimsatansweringhow ofE(Z;Z(k),P(k),W(k))evaluatedatthepointZ=Z(k):
tosimultaneouslyachievethetwoconcerningcriteriaregard- (cid:12)
∂E(Z;Z(k),P(k),W(k))(cid:12)
ing effectiveness and efficiency for building powerful and (cid:12)
scalable Transformers on large graphs. We will unfold the ∂Z (cid:12) (cid:12)
Z=Z(k)
analysisinaprogressivemannerthatlendsusaprincipled (cid:104) (cid:105) (6)
=2∆(k)Z(k)W(k)+2 Z(k)−(βI+D(k))Z(k)W(k)
waytoderivethemodelarchitecture.
Inspecific,ourstartingpointisrootedontheinterpreta- =2Z(k)−2βZ(k)W(k)−2P(k)Z(k)W(k),
tionofmessage-passing-basedpropagationlayersadopted
bycommonGNNsandTransformersasoptimizationdynam- whereIdenotestheN ×N identitymatrix.Usinggradient
icsofaclassicgraphsignaldenoisingproblem[45],[46],[47]. descentwithstepsize 1 tominimizeE(Z;Z(k),P(k),W(k))
2
Thelattercanbeformulatedassolvingaminimizationprob- atthecurrentlayeryieldsanupdatingrule:
lemassociatedwithanenergyobjectiveandallowsameans (cid:12)
1 ∂E(Z;Z(k),P(k),W(k))(cid:12)
todissecttheunderlyingmechanismofdifferentmodelsfor Z(k+1) =Z(k)− (cid:12)
representationlearning.Ontopofthis,wepresentahybrid 2 ∂Z (cid:12) (cid:12) (7)
Z=Z(k)
architecturethatintegratesthedesiredexpressivenessofall- =P(k)Z(k)W(k)+βZ(k)W(k).
pairglobalattentionandgraphinductivebiasintoaunified
model.Then,asafurtherstep,weshowthatthehybridmulti- Wethusconcludetheproofforthetheorem.
layermodelcanbesimplifiedtoaone-layercounterpartthat
can, in principle, significantly enhance the computational TheassumptionofsymmetricW(k) can,tosomeextent,
efficiencywithoutsacrificingtherepresentationpower. limittheapplicabilityofthistheorem,whereas,asweshow
later,theconclusioncanbegeneralizedtothecaseinvolving
arbitraryW(k) ∈Rd×d.Nowwediscusstheimplicationsof
3.1 AHybridModelBackbone
Theorem1.ThefirsttermofEqn.4canbewrittenas
D emen bo edte db iny gZ sa(k t) th= e[ kz -( u tk h)] lN u a= y1 er.∈ WR eN c× ond st ih de erst ga ec nk ero if cN men so sad ge es -’ (cid:88) p( uk v)∥z u−z v∥2
W(k)
=(cid:88) p( uk v)(z u−z v)⊤W(k)(z u−z v),
passingnetworks,whichcanunifythelayer-wiseupdating u,v u,v
rulesofcommonGNNsandTransformersasapropagation (8)
layerwithself-loopconnection(a.k.a.residuallink): which can be considered as generalization of the Dirichlet
energy [48] defined over a discrete space of N nodes
Z(k+1) =P(k)Z(k)W(k)+βZ(k)W(k), (3) where the pairwise distance between any node pair (u,v)
where β ≥ 0 is a weight on the self-loop path, P(k) = is given by
p( uk v)
and the signal smoothness is measured
[p u(k v)]
N×N
denotesthepropagationmatrixatthek-thlayer through a weighted space ∥ · ∥ W(k). The second term of
and W(k) ∈ Rd×d denotes the layer-specific trainable Eqn.4aggregatesthesquaredistancebetweentheupdated
weight matrix for feature transformation. For GNNs, the nodeembeddingz u andthelast-layerembeddingz( uk) after
propagation matrix is commonly instantiated as a fixed
transformationof(β+d( uk))W(k).Overall,theobjectiveof
sparse matrix, e.g., the normalized graph adjacency. For Eqn.4formulatesagraphsignaldenoisingproblemdefined
Transformers,P(k) becomesalayer-specificdenseattention over N nodes in a system that aims at smoothing the
matrixcomputedbyZ(k). node embeddings via two-fold regularization effects [49]
Propagation Layers as Optimization Dynamics. The (asillustratedinFig.1(a)):thefirsttermpenalizestheglobal
following theorem shows that, under mild conditions, the smoothnessamongnodeembeddingsthroughtheproximity
updatingruledefinedbyEqn.3isessentiallyanoptimiza- defined by P(k); the second term penalizes the change of
tion step on a regularized energy that promotes a certain nodeembeddingsfromtheonespriortothepropagation.
smoothnesseffectforgraphsignaldenoising. Thetheoremrevealsthatwhilethelayer-wiseupdating
ruleadoptedbyeitherGNNsorTransformerscanbeunified
Theorem1. ForanypropagationmatrixP(k) =[p( uk v)] N×N and asadescentsteponaregularizedenergy,thesetwomodel
symmetricweightmatrixW(k),Eqn.3isagradientdescentstep
backbonescontributetoobviouslydifferentsmoothnessef-
withstepsize 1 2 fortheoptimizationproblemw.r.t.thequadratic fects.ForGNNsthatusegraphadjacencyasthepropagation
energy:E(Z;Z(k),P(k),W(k))≜ matrix, in which situation p( uk v) = 0 for (u,v)’s that are
(cid:88) p( uk v)∥z u−z v∥2 W(k)+(cid:88) ∥z u−(β+d( uk))W(k)z( uk)∥2 2, (4) disconnectedinthegraph,theenergyonlyenforcesglobal
smoothnessoverneighboringnodesinthegraph.Incontrast,
u,v u
Transformers using all-pair attention induce the energy
whered( uk) =(cid:80)N v=1p( uk v)
andtheweightedvectornormisdefined regularizingtheglobalsmoothnessoverarbitrarynodepairs.
by∥x∥2 =x⊤Wx. Thelatterbreakstherestrictionofobservedgraphsandcan
W
facilitate leveraging the unobserved interactions for better
Proof. We denote by D(k) = diag({d( uk)}N u=1) and ∆(k) =
representations.Ontheotherhand,all-pairattentiondiscards
D(k)−P(k).ThefirstterminE(Z;Z(k),P(k),W(k))canbe
the input graph, which can play a useful inductive bias
expressedastr(Z⊤∆(k)ZW(k))anditsgradientw.r.t.Zcan
roleinlearninginformativerepresentations(especiallywhen
becomputedby
theobservedstructuresstronglycorrelatewithdownstream
∂tr(Z⊤∆(k)ZW(k)) (cid:16) (cid:17) labels). In light of the analysis, we next consider a hybrid
=∆(k)Z· W(k)+(W(k))⊤ . (5) propagationlayerthatsynthesizestheeffectofbothmodels.
∂Z5
AHybridModelBackbone.Wedefineamodelbackbone continuousw.r.t.W(k).ThensimilartoTheorem27of[50],
withthelayer-wiseupdatingrulecomprisedofthreeterms: we can prove that for any weight matrix W(k) ∈ Rd×d,
thereexistZ˜(k+1) ∈ CN×d,right-invertibleT ∈ Cd′×d and
Z(k+1) =(1−α)P( Ak)Z(k)W(k)+αP GZ(k)W(k)+βZ(k)W(k), herimitiaW˜ (k) ∈CN×d′ suchthatZ˜(k+1) =P(k)Z(k)W˜ (k)+
(9) βZ(k)W˜ (k) and
whereP(k)
isanall-pairattention-basedpropagationmatrix
A
specifictothek-thlayer,P G isasparsegraph-basedpropa- ∥Z˜(k+1)−Z(k+1)∥<ϵ,∀ϵ>0. (12)
gationmatrix(associatedwithinputgraphG),and0≤α<1
is a weight. We assume P( Ak) = [c( uk v)] N×N and P G = B coy ma ps ls eu xm di on mg aT in.∈ ThC end× wd e, cw ae nup sr eov the et sh ae mc eo tn ec cl hu ns ii qo un ei an st th he
e
[w uv] N×N. Particularly, the hybrid model can be treated
asanextensionofEqn.3whereP(k) =(1−α)P(k)+αP(k) proofafterTheorem27inAppendixE.3of[50]togeneralize
A G theconclusionfromthecomplexdomaintotherealdomain.
andspecifically
p(k)
=(cid:40) (1−α)c( uk v)+αw uv, if(u,v)∈E
(10) Thissuggeststhatforanypropagationlayer(asdefined
uv (1−α)c( uk v), if(u,v)∈/ E. byEqn.3)withW(k) ∈ Rd×d thatisevenasymmetric,we
can find a surrogate matrix W˜ (k) that is symmetric, such
WecanextendtheresultofTheorem1andnaturallyderive
that the latter produces the node embeddings which can
theregularizedenergyoptimizedbythehybridmodel. bearbitrarilyclosetotheonesproducedbyW(k).Pushing
Corollary 1. Foranyattention-basedpropagationmatrixP(k) further, one-layer updates of the message passing model
A
and graph-based propagation matrix P G, if W(k) is a symmet- corresponds to a descent step on the regularized energy
ric matrix, then Eqn. 9 is a gradient descent step with step
E(Z;Z(k),P(k),W˜ (k)).
size 1 for the optimization problem w.r.t. the quadratic energy
2
E(Z;Z(k),P(k),P ,W(k))≜
A G 3.2 ReductionfromMulti-LayertoOne-Layer
(cid:88)(cid:104)
(1−α)c(k)∥z −z ∥2 +αw ∥z −z ∥2
(cid:105)
The analysis so far targets the updates on embeddings of
uv u v W(k) uv u v W(k)
one propagation layer, yet the model practically used for
u,v
+(cid:88)(cid:13)
(cid:13)z
−(cid:16)
β+(1−α)d˜(k)+αd
(cid:17) W(k)z(k)(cid:13) (cid:13)2
,
computingrepresentationsoftenstacksmultiplepropagation
(cid:13) u u u u (cid:13) layers. While using deep propagation may endow the
2
u modelwithdesiredexpressivity,italsoincreasesthemodel
(11)
complexity and hinders its scalability to large graphs. We
whered˜( uk) =(cid:80)N v=1c( uk v) andd u =(cid:80)N v=1w uv. next zoom in on whether using multi-layer propagation
isanecessaryconditionforsatisfactoryexpressivenessfor
Proof. TheproofforthiscorollarycanbeadaptedbyTheo- learningrepresentations.Ontopofthis,theanalysissuggests
rem1withthesimilarreasoningline. apotentialwaytosimplifytheTransformerarchitecturefor
learningonlargegraphs.
Thehybridmodeliscapableofaccommodatingobserved
Multi-Layer v.s. One-Layer Models. The analysis in
structural information and in the meanwhile capturing
Sec. 3.2 reveals the equivalence between the embedding
unobserved interactions beyond input graphs. Such an
updates of one propagation layer and one-step gradient
architectural design incorporates the graph inductive bias
descent on the regularized energy. Notice that since the
into the vanilla Transformer and is adopted by state-the-
attention matrix P(k) (dependent on Z(k)) and the feature
of-art Transformers on graphs, e.g., [10], [12], [14], that A
transformation W(k) vary at different layers, the energy
show superior performance in different tasks of graph
objectiveoptimizedbythemodel(Eqn.3orEqn.9)isalso
representationlearning.
specifictoeachlayer.Inthisregard,themulti-layermodel,
GeneralizationtoAsymmetricWeightMatrix.Theabove
analysisassumestheweightmatrixW(k) tobesymmetric, whichiscommonlyadoptedbyexistingTransformers,canbe
seenasacascadeofdescentstepsonlayer-dependentenergy
whichmaylimittheapplicabilityoftheconclusionssincein
objectives. From this viewpoint, there potentially exists
commonneuralnetworkstheweightmatrixcanpotentially
takeanyvalueintheentireRd×d.Inourcontext,itcanbe certain redundancy in the optimization process for graph
difficult to directly analyze the case of asymmetric W(k) signalprocessing,sincethedescentstepsofdifferentlayers
pursuedifferenttargetsandmayinterferewitheachother.
and derive any closed form of the energy. However, the
To resolve this issue, we introduce the next theorem that
followingpropositionallowsustogeneralizetheconclusion
ofTheorem1toarbitraryW(k) ∈Rd×d. furthersuggestsaprincipledwaytosimplifytheTransformer
model, and particularly, we can construct a single-layer
Proposition 1. For any weight matrix W(k) ∈ Rd×d, there model that yields the same denoising effect as the multi-
exists a symmetric matrix W˜ (k) ∈ Rd×d such that the up- layercounterpart.
datedembeddingsZ˜(k+1) =P(k)Z(k)W˜ (k)+βZ(k)W˜ (k) yield
∥Z˜(k+1)−Z(k+1)∥<ϵ,∀ϵ>0. Theorem2. ForanyK-layermodel(whereK isanarbitrarypos-
itiveinteger)whoselayer-wiseupdatingruleisdefinedbyEqn.9
Proof. WeextendtheproofofTheorem9in[50]toourcase. producing the output embeddings Z(K), there exists a (sparse)
The updating rule considered in Lemma 26 of [50] can be graph-based propagation matrix P∗
G
= [w u∗ v] N×N, a dense
replaced by our updating rule Z(k+1) = P(k)Z(k)W(k) + attention-basedpropagationmatrixP∗
A
=[c∗ uv] N×N,andasym-
βZ(k)W(k), so that the updated embeddings Z(k+1) are metricweightmatrixW∗ ∈Rd×d suchthatonegradientdescent6
step for optimization (from the initial point Z(0) = [z( u0)]N u=1) energy objective of Eqn. 13. The latter produces the node
w.r.t.thesurrogateenergyE∗(Z;Z(k),P∗,P∗,W∗)≜ embeddingsthathavenegligiblysmallapproximationerror
A G
(cid:88)(cid:104)
(1−α)c∗ ∥z −z ∥2 +αw∗ ∥z −z ∥2
(cid:105) compared to the ones yielded by the multi-layer model.
uv u v W∗ uv u v W∗ BasedonTheorem2andextendingtheanalysisofSec.3.1,
u,v wecanarriveatthefollowingresultthatsuggestsasimplified
(13)
+(cid:88)(cid:13)
(cid:13)z
−(cid:16)
(1−α)d˜(k)+αd
(cid:17) W∗z(k)(cid:13) (cid:13)2
, one-layermodel.
(cid:13) u u u u (cid:13)
2
u Corollary2. ForanyK-layermodelwhoselayer-wiseupdating
w emh be er de dd i˜ n( uk g) sZ= ∗ s(cid:80) atN v is= fy1 ic n( u gk v) ∥Zan ∗d −d Zu (K= )∥(cid:80) <N v ϵ= ,1 ∀w ϵu >v, 0y .ields node r mu al te ri is xd Pefi ∗ Gn ,e ad nb ay ttE eq nn ti. o9 n, -bth ae sr ee de px ri os pts aga ag tir oa nph m-b aa ts re ixd Ppr ( Ao kp )a ,g aa nti don a
weightmatrixW∗ ∈ Rd×d,suchthattheone-layermodelwith
Proof. By definition, the K-layer model is comprised of K
theupdatingrule
feed-forwardupdatinglayerseachofwhichadoptstheprop-
agationP(k) =(1−α)P( Ak)+αP G(wherenoticethatP( Ak) is Z∗ =(1−α)P∗ AZ(0)W∗+αP∗ GZ(0)W∗, (19)
computedbyZ(k))toupdatethenodeembeddingsfromZ(k)
yields the equivalent result with up to negligible approximation
toZ(k+1).Thefeed-forwardcomputationyieldsasequence
error∥Z∗−Z(K)∥<ϵ,∀ϵ>0.
of results Z(0),P(0),Z(1),P(1),··· ,Z(K). We introduce an
augmentedpropagationmatrixP(k)
andrewriteEqn.9as
Proof. Theorem 2 indicates that there exists P∗ G, P( Ak) , and
W˜ ∗ ∈ Rd×d that is symmetric so that one-step gradient
Z(k+1) =(P(k)+βI)Z(k)W(k) =P(k) Z(k)W(k). (14) descent on E∗(Z;Z(0),P∗,P∗,W˜ ∗) yields the node em-
A G
beddingsZ˜∗ withtheapproximationerror∥Z˜∗−Z(K)∥ <
By stacking K layers of propagation, we can denote the ϵ. Furthermore, similar to the reasoning line of Theo-
outputembeddingsas 2
rem 1, we can show that one-step gradient descent on
Z(K) =P(K−1) ···P(0) Z(0)W(0)···W(K−1) E∗(Z;Z(0),P∗ A,P∗ G,W˜ ∗)asdefinedbyEqn.13inducesthe
=P∗ Z(0)W˜ ∗. (15) updatingrule
Z˜∗ =(1−α)P∗Z(0)W˜ ∗+αP∗Z(0)W˜ ∗. (20)
Wenextconcludetheproofbyconstruction.Assume A G
BycomparingZ˜∗ andZ∗ inEqn.19andapplyingProposi-
P∗
A
= 1−1 α(cid:16) P∗ −αP∗ G(cid:17) , (16) tion1,wehavetheresult∥Z˜∗−Z∗∥< 2ϵ.Thenthecorollary
canbeobtainedusingthetriangleinequality.
where P∗ is a sparse matrix associated to G which can be
G
arbitrarilygiven,e.g.,P∗ =PK.ThelatterbecomestheK- In this sense, the single-layer model produces nearly
G G
order(normalized)adjacencymatrixifP Gisthe(normalized) the same denoising effect (where the approximation error
adjacencymatrix.Thenweconsidertheoptimizationprob- can be arbitrarily small) as the multi-layer model, and the
lem w.r.t. the energy E∗(Z;Z(0),P∗,P∗,W∗) defined by outputnodeembeddingsexhibitinthesamewaytoleverage
A G
Eqn.13.FromtheinitialpointZ(0) =[z( u0)]N u=1,thegradient theglobalinformationfromothernodes.Amoreintuitive
illustrationisprovidedinFig.1(b).
w.r.t.Zcanbeevaluatedby
(cid:12)
∂E∗(Z;Z(0),P∗,P∗,W∗)(cid:12)
3.3 ImplicationsforModelDesigns
A G (cid:12)
∂Z (cid:12)
(cid:12) Asconclusionoftheanalysisinthissection,weframeour
Z=Z(0)
=2(1−α)(D∗ −P∗)Z(0)W∗+α(D∗ −P∗)Z(0)W∗ mainresultsasthefollowingtwostatementsandshedmore
A A G G
(cid:104) (cid:105) insightsonpracticalmodeldesigns,especiallyforbuilding
+2 Z(0)−((1−α)D∗ +αD∗)Z(0)W∗
A G powerfulandscalableTransformersonlargegraphs.
=2Z(0)−(1−α)P∗ AZ(0)W∗−2αP∗ GZ(0)W∗. • Statement1.Thelayer-wisepropagationruleofthe
(17) genericmessagepassingmodel(Eqn.3)isequivalent
to an optimization step for the objective of graph
Insertingthegradientintotheupdatingandusingstepsize
1 willinducethedescentstep: signaldenoising(Theorem1).Thelatterregularizes
2 two-foldsmoothness(asillustratedbyFig.1(a))and
(cid:12)
Z(0)− 1 ∂E∗(Z;Z(0),P∗ A,P∗ G,W∗)(cid:12) (cid:12) =P∗ Z(0)W∗. this principled viewpoint further induces a hybrid
2 ∂Z (cid:12) propagation layer synthesizing the advantage of
(cid:12)
Z=Z(0) GNNsandall-pairglobalattention(Corollary1).
(18)
• Statement 2. From the perspective of graph signal
Wethushaveshownthatsolvingtheoptimizationproblem
denoising,stackingmultiplepropagationlayersisnot
w.r.t.Eqn.13viaone-stepgradientdescentinducestheoutput
embeddings Z∗ = P∗ Z(0)W∗. According to Eqn. 15 and anecessityforachievingthedesiredexpressiveness,
since there exists a one-layer model producing the
usingProposition1,wehave∥Z∗−Z(K)∥<ϵ,∀ϵ>0.
equivalent embeddings as the multi-layer model
This theorem indicates that for any multi-layer model (Theorem2andCorollary2).Moreover,ascompared
whosepropagationlayersaredefinedbyEqn.9,itsinduced inFig.1(b),themulti-layermodeloptimizesdifferent
optimization trajectories composed of multiple gradient objectives at each layer, while the one-layer model
descent steps (updating node embeddings from Z(0) to contributestoasteepestdescentsteponasinglefixed
Z(K)) can be reduced to one-step gradient descent on the objective,reducingthepotentialredundancy.7
Input Data Simple Global Output
Attention
norm
Graph Neural
Network norm \
(a) Forward computation flow of SGFormer (b) Computation flow of simple attention function
Fig.2.(a)DataflowofSGFormer.TheinputdataentailsnodefeaturesXandgraphadjacencyA.SGFormeriscomprisedofasingle-layerglobal
attentionandaGNNnetwork.Themodeloutputsnoderepresentationsforfinalprediction.(b)Computationflowofthesimpleattentionfunction
utilizedbySGFormerwhichaccommondatesall-pairinfluenceamongN nodesforcomputingtheupdatedembeddingswithinO(N)complexity.
Therefore,fromthisstandpoint,themulti-layermodelcan hinders its scalability for large graphs. Alternatively, we
be simplified to the single-layer model without sacrificing introduceasimpleattentionfunctionwhichcanreducethe
theexpressivenessforrepresentation.Thisresultenlightens computationalcomplexitytoO(N)andstillaccommodate
apotentialwaytobuildefficientandpowerfulTransformers all-pairinteractions.Specifically,withtheinitialembeddings
onlargegraphs,aswillbeexemplifiedinthenextsection. Z(0)asinput,wefirstusefeaturetransformationsf Q,f K and
f V toobtainthekey,queryandvaluematrices,respectively,
asisdonebycommonTransformers:
4 PROPOSED MODEL
Inthissection,weintroduceourmodel,referredasSimplified Q=f Q(Z(0)), K=f K(Z(0)), V=f V(Z(0)), (22)
Graph Transformer (SGFormer), under the guidance of
wheref Q,f K andf V areinstantiatedasafully-connected
our theoretical results in Sec. 3 (as distilled in Sec. 3.3).
layerinourimplementation.Thenweconsidertheattention
Overall,thearchitecturaldesignadoptsthehybridmodelin
functionthatcomputestheall-pairsimilarities:
Eqn.19andfollowstheOccam’sRazorprincipleforspecific
instantiations. Particularly, SGFormer only requires O(N) 1 (cid:18) Q (cid:19) (cid:18) K (cid:19)⊤
C=I+ · , (23)
complexityforaccommodatingtheall-pairinteractionsand N ∥Q∥ ∥K∥
F F
computingN nodes’representations.Thisisachievedbya
simpleattentionfunctionwhichhasadvancedcomputational
C=diag−1(cid:0) C1(cid:1)
·C, (24)
efficiency and is free from any approximation scheme. where 1 is an N-dimensional all-one column vector. In
Apart from the scalability advantage, the light-weighted Eqn. 23 the scaling factor 1 can improve the numerical
N
architectureendowsSGFormerwithdesiredcapabilityfor
stabilityandtheadditionofaself-loopcanhelptostrengthen
learningonlargegraphswithlimitedlabels.
theroleofcentralnodes.Eqn.24servesasrow-normalization
which is commonly used in existing attention designs. If
4.1 ModelDesign one uses the attention matrix C to compute the updated
W toe nfi or dst eu es mea ben de du ir na gll say iner thto em laa tp enin tp spu at cfe ea ,t iu .er .e ,s ZX (0)= =[x fu I]N u (X=1
)
e Om (Nbe 2d )d ci on mgs p, lei. xe i. t, yZ
,
sA iN nce= thC eV co, mth pe utc ao tim onpu ot fat ti ho en ar te teq nu ti ir oe ns
wheref I canbeashallow(e.g.,one-layer)MLP.Thenbased m coa st tr oix f( OE (q Nn. 22 )3 .) Na on td abth lye ,u thp ed sa it med ple em ab tte ed nd tii on ngs fub no cth tion nee ad lls ot whe
s
ontheresultofSec.3.2,particularlythesingle-layermodel
analternativewayforcomputingtheupdatedembeddings
presentedinCorollary2,weconsiderthefollowinghybrid
viachangingtheorderofmatrixproducts.Inspecific,assume
architectureforupdatingtheembeddings:
Q˜ = Q and K˜ = K , and we can rewrite the
Z out =(1−α)AN(Z(0))+αGN(Z(0),A), (21) comput∥ aQ ti∥ oF nflowoftheat∥ teK n∥ tF ion-basedpropagation:
where 0 ≤ α < 1 again is a hyper-parameter, and AN N=diag−1(cid:18)
I+
1 Q˜(K˜⊤1)(cid:19)
, (25)
and GN denote a global attention network and a graph- N
based propagation network, respectively. Then the node (cid:20) 1 (cid:21)
pre rp edre is ce tin ot nati Yˆon =sZ fo Out (Zar oe utf )e ,d win ht eo rean
f
Oou it spu at fn ue lu lyr -a cl ol nay ne er ctf eo dr Z AN =N· V+ NQ˜(K˜⊤V) . (26)
layerinourimplementation.Wenextdelveintothedetailed One can verify through basic linear algebra that the result
instantiationsofANandGN. ofEqn.26isequivalenttotheoneobtainedbyZ AN =CV
Simple Global Attention Network. There exist many which explicitly computes the all-pair attention. In other
potentialchoicesforglobalattentionfunctionsastheinstanti- words, while the computation flow of Eqn. 26 does not
ationofAN(Z(0)),e.g.,thewidelyadoptedSoftmaxattention computetheall-pairattentionmatrix,itstillaccommodates
thatisoriginallyusedby[15].WhiletheSoftmaxattention the all-pair interactions as the original attention. More
possessesprovableexpressivity[51],itrequiresO(N2)com- importantly,thecomputationofEqn.(26)canbeachievedin
plexityforcomputingtheall-pairattentionsandupdatingthe O(N)complexity,whichismuchmoreefficientthanusing
representationsofN nodes.Thiscomputationalbottleneck the original computation flow. Therefore, such a simple8
global attention design reduces the quadratic complexity Algorithm1Feed-forwardandTrainingofSGFormer.
to O(N) and in the meanwhile guarantee the expressivity 1: Input: Node feature matrix X, input graph adjacency
forcapturingall-pairinteractions. matrixA,labelsoftrainingnodesY tr,weightongraph-
Graph-basedPropagationNetwork.Foraccommodating basedpropagationα.
thepriorinformationoftheinputgraphG,existingmodels 2: whilenotreachingthebudgetoftrainingepochsdo
tend to use positional encodings [10], edge regulariza- 3: EncodeinputnodefeaturesZ(0) =f I(X);
tion loss [12] or augmenting the Transformer layers with 4: Computequery,keyandvaluematricesQ=f Q(Z(0)),
GNNs[8].Hereweresorttoasimple-yet-effectivescheme K=f K(Z(0))andV=f V(Z(0));
andimplementGN(Z(0),A)(inEqn.21)throughasimple 5: ComputenormalizationQ˜ = Q andK˜ = K ;
∥Q∥F ∥K∥F
graphneuralnetwork(e.g.,GCN[29])thatpossessesgood 6: Compute denominator of global attention N =
(cid:16) (cid:17)
scalabilityforlargegraphs.Wenotethatwhilethetheoretical diag−1 I+ 1Q˜(K˜⊤1) ;
N
results in Sec. 3 suggest that using one-layer propagation
7: Compute updated embeddings by global attention
caninprincipleachieveequivalentexpressivenessasmulti- (cid:104) (cid:105)
layerpropagation,itdoesnotnecessarilymeaninpractice Z AN =N· V+ N1Q˜(K˜⊤V) ;
themodelhastobeconstrainedtosingle-layerarchitectures. 8: Computefinalrepresentationsbygraph-basedpropa-
Themainadvantageofreducingthepropagationlayerslies
gationZ
out
=(1−α)ZAN +αGN(Z(0),A);
in the improvement in computational efficiency, which is 9: CalculatepredictedlabelsYˆ =f O(Z out);
alreadyachievedbySGFormerwiththeadoptionofsingle- 10: ComputethesupervisedlossLfromYˆ tr andY tr;
layerglobalattentionsincetheglobalattentiondetermines 11: UseLtoupdatethetrainableparameters;
thecomputationaloverheadofTransformers.Inthisregard, 12: endwhile
onecanstilluseshallowlayersofGNNsinpracticethatare
desirablyefficientandrequirenegligibleextracosts.
ComplexityAnalysis.Algorithm1presentsthefeedfor- theirarchitectures,expressivity,andscalability.Mostexisting
ward computation and training process of SGFormer. The Transformershavebeendevelopedandoptimizedforgraph
overallcomputationalcomplexityofourmodelisO(N+E), classificationtasksonsmallgraphs,whilesomerecentworks
where E = |E|, as the GN module requires O(E). Due to havefocusedonTransformersfornodeclassification,where
the typical sparsity of graphs (i.e., E ≪ N2), our model thechallengeofscalabilityarisesduetolargegraphsizes.
canscalelinearlyw.r.t.graphsizes.Furthermore,withonly • Architectures.Regardingmodelarchitectures,someex-
single-layerglobalattentionandsimpleGNNarchitectures, istingmodelsincorporateedge/positionalembeddings(e.g.,
our model is fairly lightweight, enabling efficient training Laplaciandecompositionfeatures[6],degreecentrality[9],
andinference. Weisfeiler-Lehmanlabeling[7])orutilizeaugmentedtraining
Scaling to Larger Graphs. For larger graphs that even loss (e.g., edge regularization [12], [41]) to capture graph
GCNcannotbetrainedonusingfull-batchprocessingwitha information.However,thepositionalembeddingsrequirean
singleGPU,wecanusetherandommini-batchpartitioning additional pre-processing procedure with a complexity of
method utilized by [12], which we found works well and
uptoO(N3),whichcanbetime-andmemory-consuming
efficientlyinpractice.Specifically,werandomlyshuffleall forlargegraphs,whiletheaugmentedlossmaycomplicate
the nodes and partition the nodes into mini-batches with the optimization process. Moreover, existing models typi-
the size B. Then in each iteration, we feed one mini-batch cally adopt a default design of stacking deep multi-head
(theinputgraphamongtheseB nodesaredirectlyextracted attention layers for competitive performance. In contrast,
by the subgraph of the original graph) into the model for SGFormer does not require any of positional embeddings,
loss computation on the training nodes within this mini- augmented loss or pre-processing, and only uses a single-
batch.Thisschemeincursnegligibleadditionalcostsduring layer,single-headglobalattention,makingitbothefficient
training and allows the model to scale to arbitrarily large andlightweight.
graphs. Moreover, owing to the linear complexity w.r.t. • Expressivity.Therearesomerecentlyproposedgraph
nodenumbersrequiredbySGFormer,wecanemploylarge Transformers for large graphs [11], [13], [44] that limit
batch sizes (e.g., B = 0.1M), which facilitate the model the attention computation to a subset of nodes, such as
in capturing informative global interactions among nodes neighboringnodesorsamplednodesfromthegraph.This
withineachmini-batch.Apartfromthissimplescheme,our approachallowslinearscalingw.r.t.graphsizes,butsacrifices
model is also compatible with other techniques such as theexpressivityforaccommodatingall-pairinteractions.In
neighborsampling[52],graphclustering[53],andhistorical contrast,SGFormermaintainsattentioncomputationoverall
embeddings[54].Thesetechniquesmayrequireextratime N nodesineachlayerwhilestillachievingO(N)complexity.
costs for training, and we leave exploration along this Moreover,unlikeNodeFormer[12]andGraphGPS[10]which
orthogonal direction for future works. Fig. 2 presents the relyonrandomfeaturemapsasapproximation,SGFormer
dataflowoftheproposedmodel. doesnotrequireanyapproximationorstochasticcomponents
andismorestableduringtraining.
• Scalability.Intermsofalgorithmiccomplexity,most
4.2 ComparisonwithExistingModels existinggraphTransformershaveO(N2)complexitydueto
We next provide a more in-depth discussion comparing global all-pair attention, which is a critical computational
our model with prior art and illuminating its potential bottleneck that hinders their scalability even for medium-
in wide application scenarios. Table 1 presents a head-to- sized graphs with thousands of nodes. While neighbor
headcomparisonofcurrentgraphTransformersintermsof samplingcanserveasaplausibleremedy,itoftensacrifices9
TABLE1
Comparisonof(typical)graphTransformersw.r.t.requiredcomponents(positionalencodings,multi-layerattentions,augmentedlossfunctionsand
edgeembeddings),all-pairexpressivityandalgorithmiccomplexityw.r.t.nodenumberN andedgenumberE(oftenE≪N2).Thelargest
demonstrationmeansthelargestgraphsizeusedbythepapers.(m:numberofsamplednodes,s:numberofaugmentations,h:numberofhops.)
ModelComponents All-pair AlgorithmicComplexity Largest
Model
PosEnc Multi-LayerAttn AugLoss EdgeEmb Expressivity Pre-processing Training Demo.
GraphTransformer[6] R R - R Yes O(N3) O(N2) 0.2K
Graphormer[9] R R - R Yes O(N3) O(N2) 0.3K
GraphTrans[8] - R - - Yes - O(N2) 0.3K
SAT[40] R R - - Yes O(N3) O(N2) 0.2K
EGT[41] R R R R Yes O(N3) O(N2) 0.5K
GraphGPS[10] R R - R Yes O(N3) O(N+E) 1.0K
Graph-Bert[7] R R R R Yes O(N2) O(N2) 20K
Gophormer[11] R R R - No - O(Nsm2) 20K
NodeFormer[12] R R R - Yes - O(N+E) 2.0M
ANS-GT[44] R R - - No - O(Nsm2) 20K
NAGphormer[13] R R - R No O(N3) O(Nh2) 2.0M
DIFFormer[14] - R - - Yes - O(N+E) 1.6M
SGFormer(ours) - - - - Yes - O(N+E) 0.1B
performance due to the significantly reduced receptive where the graphs have high homophily ratios, and four
field [25]. SGFormer scales linearly w.r.t. N and supports heterophilicgraphsACTOR[56],SQUIRREL,CHAMELEON[57]
full-batch training on large graphs with up to 0.1M nodes. and DEEZER-EUROPE [58], where neighboring nodes tend
For further larger graphs, SGFormer is compatible with to have distinct labels. These graphs have 2K-30K nodes
mini-batch training using large batch sizes, which allows andthedetailedstatisticsarereportedinTable2.Thelarge-
themodeltocaptureinformativeglobalinformationwhile sized datasets include the citation networks OGBN-ARXIV
having a negligible impact on performance. Notably, due and PGBN-PAPERS100M, the protein interaction network
tothelinearcomplexityandsimplearchitecture,SGFormer OGBN-PROTEINS[27],theitemco-occurrencenetworkAMA-
canscaletotheweb-scalegraphOGBN-PAPERS100M(with ZON2M[59],andthesocialnetworkPOKEC[60].Inparticular,
0.1B nodes) when trained on a single GPU, two orders-of- AMAZON2Mentailslong-rangedependencyandPOKECis
magnitudelargerthanthelargestdemonstrationamongmost aheterophilicgraph.Thedetailedstatisticsarepresentedin
graphTransformers. Table3andthelargestdatasetOGBN-PAPERS100Mcontains
morethan0.1Bnodes.
5 EMPIRICAL EVALUATION Implementation.Theinputlayerf I isinstantiatedasa
fully-connectedlayerwithReLUactivation.Theoutputlayer
We apply SGFormer to real-world graph datasets whose
f O isinstantiatedasafully-connectedlayer(withSoftmax
predictive tasks can be modeled as node-level prediction.
forclassification).TheGNmoduleisbasicallyimplemented
Thelatteriscommonlyusedforeffectivenessevaluationof
as a GCN [29] with shallow (e.g., 1-3) propagation layers.
learninggraphrepresentationsandscalabilitytolargegraphs.
We use different training schemes for graph datasets of
We present the details of implementation and datasets in
differentscales.Formedium-sizedgraphs,weusefull-graph
Sec. 5.1. Then in Sec. 5.2, we test SGFormer on medium-
training:thewholegraphdatasetisfedintothemodelduring
sized graphs (from 2K to 30K nodes) and compare it with
trainingandinference.Forlarge-sizedgraphs,weadoptthe
anextensivesetofexpressiveGNNsandTransformers.In
mini-batchtrainingasintroducedinSec.4.1.Inspecific,we
Sec. 5.3, we scale SGFormer to large-sized graphs (from
set the batch size as 10K, 0.1M, 0.1M and 0.4M for OGBN-
0.1M to 0.1B nodes) where its superiority is demonstrated
PROTEINS, AMAZON2M, POKEC and OGBN-PAPERS100M,
overscalableGNNsandTransformers.LaterinSec.5.5,we
respectively.Thenforinferenceontheselarge-sizedgraphs,
further compare the performance with different ratios of
following the pipeline used by [27], we feed the whole
labeleddata.Inaddition,wecomparethemodel’stimeand
graphintothemodelusingCPU,whichallowscomputing
space efficiency and scalability in Sec. 5.4. In Sec. 5.6, we
the all-pair attention among all the nodes in the dataset.
analyzetheimpactofseveralkeycomponentsinourmodel.
Particularly,forthegiganticgraphOGBN-PAPERS100Mthat
Sec.5.7providesfurtherdiscussionsonhowthesingle-layer
cannot be fed as whole into common CPU with moderate
modelperformscomparedwiththemulti-layercounterpart.
memory, we adopt the mini-batch partition strategy used
in training to reduce the overhead. For hyper-parameter
5.1 ExperimentDetails settings, we use the model performance on the validation
set as the reference. Unless otherwise stated, the hyper-
Datasets.Weevaluatethemodelon12real-worlddatasets
parameters are selected using grid search with the search-
with diverse properties. Their sizes, as measured by the
ingspace:learningratewithin{0.001,0.005,0.01,0.05,0.1},
number of nodes in the graph, range from thousand-
weightdecaywithin{1e−5,1e−4,5e−4,1e−3,1e−2},
level to billion-level. We use 0.1M as the threshold and
hidden size within {32,64,128,256}, dropout ratio within
group these datasets into medium-sized datasets (with
{0,0.2,0.3,0.5},weightαwithin{0.5,0.8}.
lessthan0.1Mnodes)andlarge-sizeddatasets(withmore
than 0.1M nodes). The medium-sized datasets include EvaluationProtocol.Wefollowthecommonpracticeand
threecitationnetworks CORA, CITESEER and PUBMED [55], setafixednumberoftrainingepochs:300formedium-sized10
TABLE2
Meanandstandarddeviationoftestingscoresonmedium-sizedgraphbenchmarks.Weannotatethenodeandedgenumberofeachdatasetand
OOMindicatesout-of-memorywhentrainingonaGPUwith24GBmemory.Wemarkthemodelrankedinthefirst/second/thirdplace.
Dataset CORA CITESEER PUBMED ACTOR SQUIRREL CHAMELEON DEEZER
#Nodes 2,708 3,327 19,717 7,600 2223 890 28,281
#Edges 5,278 4,552 44,324 29,926 46,998 8,854 92,752
GCN 81.6±0.4 71.6±0.4 78.8±0.6 30.1±0.2 38.6±1.8 41.3±3.0 62.7±0.7
GAT 83.0±0.7 72.1±1.1 79.0±0.4 29.8±0.6 35.6±2.1 39.2±3.1 61.7±0.8
SGC 80.1±0.2 71.9±0.1 78.7±0.1 27.0±0.9 39.3±2.3 39.0±3.3 62.3±0.4
JKNet 81.8±0.5 70.7±0.7 78.8±0.7 30.8±0.7 39.4±1.6 39.4±3.8 61.5±0.4
APPNP 83.3±0.5 71.8±0.5 80.1±0.2 31.3±1.5 35.3±1.9 38.4±3.5 66.1±0.6
H2GCN 82.5±0.8 71.4±0.7 79.4±0.4 34.4±1.7 35.1±1.2 38.1±4.0 66.2±0.8
SIGN 82.1±0.3 72.4±0.8 79.5±0.5 36.5±1.0 40.7±2.5 41.7±2.2 66.3±0.3
CPGNN 80.8±0.4 71.6±0.4 78.5±0.7 34.5±0.7 38.9±1.2 40.8±2.0 65.8±0.3
GloGNN 81.9±0.4 72.1±0.6 78.9±0.4 36.4±1.6 35.7±1.3 40.2±3.9 65.8±0.8
GraphormerSMALL OOM OOM OOM OOM OOM OOM OOM
GraphormerSMALLER 75.8±1.1 65.6±0.6 OOM OOM 40.9±2.5 41.9±2.8 OOM
GraphormerULTRASSMALL 74.2±0.9 63.6±1.0 OOM 33.9±1.4 39.9±2.4 41.3±2.8 OOM
GraphTransSMALL 80.7±0.9 69.5±0.7 OOM 32.6±0.7 41.0±2.8 42.8±3.3 OOM
GraphTransULTRASSMALL 81.7±0.6 70.2±0.8 77.4±0.5 32.1±0.8 40.6±2.4 42.2±2.9 OOM
NodeFormer 82.2±0.9 72.5±1.1 79.9±1.0 36.9±1.0 38.5±1.5 34.7±4.1 66.4±0.7
GraphGPS 80.9±1.1 68.6±1.5 78.5±0.7 37.1±1.5 41.2±2.1 42.5±4.0 66.7±0.3
ANS-GT 82.4±0.9 70.7±0.6 79.6±1.0 35.8±1.4 40.7±1.4 42.6±2.8 66.5±0.7
DIFFormer 85.9±0.4 73.5±0.3 81.8±0.3 36.5±0.7 41.6±2.5 42.5±2.5 66.9±0.7
SGFormer 84.5±0.8 72.6±0.2 80.3±0.6 37.9±1.1 41.8±2.2 44.9±3.9 67.1±1.1
graphs,1000forlarge-sizedgraphs,and50fortheextremely GraphTrans,weuseGraphTrans (3layersand4heads)
SMALL
largegraph OGBN-PAPERS100M.WeuseROC-AUCasthe andGraphTrans
ULTRASMALL
(2layersand1head).
evaluationmetricforOGBN-PROTEINSandAccuracyforother Results. Table 2 reports the results of all the models.
datasets.Thetestingscoreachievedbythemodelthatreports We found that SGFormer significantly outperforms three
thehighestscoreonthevalidationsetisusedforevaluation. standard GNNs (GCN, GAT and SGC) by a large margin,
Weruneachexperimentwithfiveindependenttrialsusing with up to 25.9% impv. over GCN on ACTOR, which sug-
differentinitializations,andreportthemeanandvarianceof geststhatoursingle-layerglobalattentionmodelisindeed
themetricsforcomparison. effective despite its simplicity. Moreover, we observe that
therelativeimprovementsofSGFormeroverthreestandard
5.2 ComparativeResultsonMedium-sizedGraphs GNNs are overall more significant on heterophilic graphs
ACTOR,SQUIRREL,CHAMELEONandDEEZER.Thepossible
Setup.Wefirstevaluatethemodelonmedium-sizeddatasets.
reasonisthatinsuchcasestheglobalattentioncouldhelpto
For citation networks CORA, CITESEER and PUBMED, we
filteroutspuriousedgesfromneighboringnodesofdifferent
follow the commonly used benchmark setting, i.e., semi-
classes and accommodate dis-connected yet informative
supervised data splits adopted by [29]. For ACTOR and
nodesinthegraph.ComparedtootheradvancedGNNsand
DEEZER-EUROPE,weusetherandomsplitsofthebenchmark
graphTransformers(NodeFormer,ANS-GTandGraphGPS),
settingintroducedby[61].ForSQUIRRELandCHAMELEON,
the performance of SGFormer is highly competitive and
weusethesplitsproposedbyarecentevaluationpaper[62]
even superior with significant gains over the runner-ups
thatfilterstheoverlappednodesintheoriginaldatasets.
in most cases. These results serve as concrete evidence for
Competitors.Giventhemoderatesizesofgraphswhere
verifyingtheefficacyofSGFormerasapowerfullearnerfor
most of existing models can scale smoothly, we compare
node-levelprediction.WealsofoundthatbothGraphormer
with multiple sets of competitors from various aspects.
andGraphTranssufferfromseriousover-fitting,duetotheir
Basically, we adopt standard GNNs including GCN [29],
relativelycomplexarchitecturesandlimitedratiosoflabeled
GAT [30] and SGC [32] as baselines. Besides, we com-
nodes.Incontrast,thesimpleandlightweightarchitectureof
pare with advanced GNN models, including JKNet [31],
SGFormerleadstoitsbettergeneralizationabilitygiventhe
APPNP [63], SIGN [64], H2GCN [33], CPGNN [35] and
limitedsupervisioninthesedatasets.
GloGNN [34]. In terms of Transformers, we mainly com-
pare with the state-of-the-art scalable graph Transformers
5.3 ComparativeResultsonLarge-sizedGraphs
NodeFormer [12], GraphGPS [10], ANS-GT [44] and DIF-
Former [14]. Furthermore, we adapt two powerful Trans- Setup.Wefurtherevaluatethemodelonlarge-sizedgraph
formerstailoredforgraph-leveltasks,i.e.,Graphormer[9] datasets where the numbers of nodes range from millions
and GraphTrans [8], for comparison. In particular, since to billions. For three OGB datasets OGBN-ARXIV, OGBN-
the original implementations of these models are of large PROTEINSandOGBN-PAPERS100M,weusethepublicsplits
sizes and are difficult to scale on all node-level prediction provided by [27]. Furthermore, we follow the splits used
datasets considered in this paper, we adopt their smaller bytherecentwork[12]forAMAZON2Mandadoptrandom
versionsforexperiments.WeuseGraphormer (6layers splitswiththeratio1:1:8forPOKEC.
SMALL
and 32 heads), Graphormer (3 layers and 8 heads) Competitors. Due to the large graph sizes, most of the
SMALLER
and Graphormer (2 layers and 1 head). As for expressive GNNs and Transformers compared in Sec. 5.2
ULTRASMALL11
TABLE3
Testingresultsonlarge-sizedgraphbenchmarks.OOTindicatesthatthetrainingcannotbefinishedwithinanacceptabletimebudget.
Dataset OGBN-PROTEINS AMAZON2M POKEC OGBN-ARXIV OGBN-PAPERS100M
#Nodes 132,534 2,449,029 1,632,803 169,343 111,059,956
#Edges 39,561,252 61,859,140 30,622,564 1,166,243 1,615,685,872
MLP 72.04±0.48 63.46±0.10 60.15±0.03 55.50±0.23 47.24±0.31
GCN 72.51±0.35 83.90±0.10 62.31±1.13 71.74±0.29 OOM
SGC 70.31±0.23 81.21±0.12 52.03±0.84 67.79±0.27 63.29±0.19
GCN-NSampler 73.51±1.31 83.84±0.42 63.75±0.77 68.50±0.23 62.04±0.27
GAT-NSampler 74.63±1.24 85.17±0.32 62.32±0.65 67.63±0.23 63.47±0.39
SIGN 71.24±0.46 80.98±0.31 68.01±0.25 70.28±0.25 65.11±0.14
NodeFormer 77.45±1.15 87.85±0.24 70.32±0.45 59.90±0.42 OOT
DIFFormer 79.49±0.44 85.21±0.62 69.24±0.76 68.52±0.49 OOT
SGFormer 79.53±0.38 89.09±0.10 73.76±0.24 72.63±0.13 66.01±0.37
TABLE4
EfficiencycomparisonofSGFormerandgraphTransformercompetitorsw.r.t.trainingtimeperepoch,inferencetimeandGPUmemorycostsona
TeslaT4.WeusethesmallmodelversionsofGraphormerandGraphTrans.Themissingresultsarecausedbytheout-of-memoryissue.
Model CORA PUBMED AMAZON2M
Tr(ms) Inf(ms) Mem(GB) Tr(ms) Inf(ms) Mem(GB) Tr(ms) Inf(ms) Mem(GB)
Graphormer 563.5 537.1 5.0 - - - - - -
GraphTrans 160.4 40.2 3.8 - - - - - -
NodeFormer 68.5 30.2 1.2 321.4 135.5 2.9 5369.5 1410.0 4.6
GraphGPS 60.8 26.1 1.2 423.1 217.7 1.6 - - -
ANS-GT 570.1 539.2 1.0 511.9 461.0 2.1 - - -
DIFFormer 49.7 9.6 1.2 85.3 30.8 2.4 3683.8 523.1 4.5
SGFormer 15.0 3.8 0.9 15.4 4.4 1.0 2481.4 382.5 2.7
SGFormer NodeFormer SGFormer w/ Softmax providesstrongevidencethatshowsthepromisingpower
Training time GPU memory cost ofSGFormeronextremelylargegraphs,producingsuperior
12.5
10.0 performancewithlimitedcomputationbudget.
100
7.5
50 5.0
2.5
2 4 6 8 10 2 4 6 8 10 5.4 EfficiencyandScalability
# nodes (104) # nodes (104)
We next provide more quantitative comparisons w.r.t. the
Fig.3.ScalabilitytestoftrainingtimeperepochandGPUmemorycost
efficiencyandscalabilityofSGFormerwiththegraphTrans-
w.r.t.graphsizes(a.k.a.nodenumbers).NodeFormerreportsOOMwhen
#nodesreachesmorethan30K. formercompetitorsindatasetsofdifferentscales.
Efficiency. Table 4 reports the training time per epoch,
inference time and GPU memory costs on CORA, PUBMED
arehardtoscalewithintheacceptablebudgetoftimeand and AMAZON2M. Since the common practice for model
memorycosts.Therefore,wecomparewithMLP,GCNand training in these datasets is to use a fixed number of
two scalable GNNs, i.e., SGC and SIGN. We also compare training epochs, we report the training time per epoch
with GNNs using the neighbor sampling technique [52]: here for comparing the training efficiency. We found that
GCN-NSamplerandGAT-NSampler.Ourmaincompetitors notably,SGFormerisorders-of-magnitudefasterthanother
are NodeFormer [12] and DIFFormer [14], the recently competitors.ComparedtoGraphormerandGraphTransthat
proposedscalablegraphTransformerswithall-pairattention. require quadratic complexity for global attention and are
Results. Table 3 presents the experimental results. We difficult for scaling to graphs with thousands of nodes,
found that SGFormer yields consistently superior results SGFormersignificantlyreducesthememorycostsduetothe
acrossfivedatasets,withsignificantperformanceimprove- simpleglobalattentionofO(N)complexity.Intermsoftime
ments over GNN competitors. This suggests the effective- costs, it yields 38x/141x training/inference speedup over
ness of the global attention that can learn implicit inter- Graphormer on CORA, and 20x/30x speedup over Node-
dependencies among a large number of nodes beyond Former on PUBMED. This is mainly because Graphormer
inputstructures.Furthermore,SGFormeroutperformsNode- requiresthecompututationofquadraticglobalattentionthat
Former by a clear margin across all the cases, which istime-consuming,andNodeFormeradditionallyemploys
demonstrates the superiority of SGFormer that uses sim- theaugmentedlossandGumbeltricks.ForGPUcosts,the
plerarchitectureandachievesbetterperformanceonlarge memory usage of SGFormer, NodeFormer and ANS-GT
graphs. For the largest dataset OGBN-PAPERS100M where on two medium-sized graphs is comparable, while on the
prior Transformer models fail to demonstrate, SGFormer large graph AMAZON2M, SGFormer consumes much less
scales smoothly with decent efficiency and yields highly memorythanNodeFormer.OnthelargegraphAMAZON2M,
competitiveresults.Specifically,SGFormerreachesthetesting whereNodeFormerandSGFormerbothleveragemini-batch
accuracyof66.0withconsumptionofabout3.5hoursand training, SGFormer is 2x and 4x faster than NodeFormer
23.0 GB memory on a single GPU for training. This result regardingtrainingandinference,respectively.
)sm(
emiT
)BG(
yromeM12
TABLE5
TestingresultswithdifferenttrainingratiosonCORA.
TrainingRatio 50% 40% 30% 20% 10% 5% 4% 3% 2% 1%
GCN 87.71±0.56 87.26±0.20 86.33±0.37 85.37±0.42 83.06±0.74 79.94±1.01 79.36±0.33 76.16±0.49 67.50±0.23 62.50±1.22
GAT 87.86±0.32 87.41±0.26 86.32±0.33 85.53±0.58 83.24±0.99 79.84±0.48 79.24±0.61 75.63±0.51 67.16±0.51 63.30±0.76
SGC 86.50±0.13 84.56±0.52 83.04±0.57 80.43±0.14 77.51±0.21 72.49±0.24 71.00±0.80 67.08±0.27 60.39±0.31 52.87±0.22
JKNet 87.42±0.69 86.18±0.58 85.53±0.45 82.85±0.49 79.92±0.24 75.33±0.76 74.77±0.87 71.55±1.09 62.69±0.44 56.09±1.50
APPNP 88.51±0.19 88.21±0.46 87.40±0.11 86.08±0.25 84.55±0.36 81.55±0.33 80.53±0.34 77.99±0.26 68.65±0.15 64.55±0.93
H2GCN 88.63±0.28 87.64±0.58 87.28±0.23 86.19±0.27 83.35±0.49 79.47±0.79 79.29±0.36 77.16±0.59 67.01±1.15 64.04±0.67
SIGN 85.29±0.08 84.03±0.28 82.85±0.29 80.81±1.24 76.73±1.01 71.38±0.49 67.99±1.10 64.35±0.98 55.95±0.39 50.20±0.68
Graphormer 75.07±0.80 74.24±0.95 71.60±0.97 68.87±0.23 62.23±0.99 54.03±0.73 52.77±0.43 49.15±1.36 43.15±0.64 39.06±1.55
GraphTrans 87.62±0.40 87.22±0.62 86.25±0.48 84.34±0.81 81.64±0.75 79.00±0.34 78.48±1.23 75.12±1.02 67.29±0.80 63.68±0.98
NodeFormer 87.36±0.81 86.41±0.82 84.89±1.15 82.81±1.10 77.71±1.91 67.62±0.63 67.84±0.63 65.85±0.92 58.12±0.20 53.47±1.90
GraphGPS 86.97±0.59 85.55±0.33 85.56±0.55 81.81±0.53 79.25±0.94 75.76±0.98 75.46±1.55 71.58±2.16 63.36±1.92 55.06±1.65
ANS-GT 83.79±0.27 83.53±0.64 83.31±0.34 81.82±0.81 80.64±0.11 79.87±0.57 77.96±0.63 74.74±0.47 69.63±0.24 64.72±0.84
DIFFormer 88.24±0.31 86.73±0.40 86.58±0.68 83.81±0.35 80.79±0.72 78.34±0.82 78.24±1.38 77.01±0.96 66.44±1.47 63.19±0.32
SGFormer 89.22±0.18 88.85±0.04 87.85±0.14 87.81±0.04 86.11±0.23 82.05±0.43 81.40±0.41 77.39±0.94 69.84±0.33 65.14±0.66
TABLE6
TestingresultswithdifferenttrainingratiosonPUBMED.
TrainingRatio 50% 40% 30% 20% 10% 5% 4% 3% 2% 1%
GCN 86.74±0.13 86.67±0.07 84.59±0.16 86.63±0.07 85.77±0.06 85.63±0.04 83.59±0.09 84.63±0.03 83.21±0.07 82.21±0.13
GAT 88.26±0.09 88.10±0.07 86.36±0.19 86.32±0.12 85.63±0.24 84.81±0.20 83.50±0.21 84.22±0.24 82.88±0.09 82.52±0.29
SGC 81.99±0.05 81.13±0.10 80.96±0.10 81.11±0.08 80.56±0.08 79.38±0.06 79.83±0.02 78.05±0.04 79.52±0.05 76.29±0.03
JKNet 89.10±0.31 88.39±0.26 87.94±0.20 87.74±0.29 85.97±0.31 84.22±0.58 83.71±0.50 83.03±0.14 82.51±0.37 81.19±0.38
APPNP 87.55±0.17 87.15±0.08 86.73±0.12 86.99±0.11 86.26±0.07 85.77±0.08 85.40±0.18 84.92±0.09 84.22±0.10 82.80±0.23
H2GCN 89.49±0.07 88.73±0.04 84.28±0.26 84.38±0.10 83.98±0.18 83.73±0.18 83.86±0.05 83.16±0.09 83.52±0.17 82.64±0.07
SIGN 89.51±0.04 88.88±0.09 88.42±0.06 87.73±0.05 85.79±0.14 84.13±0.43 83.78±0.20 82.77±0.11 81.49±0.04 78.75±1.08
Graphormer OOM OOM OOM OOM OOM OOM OOM OOM OOM OOM
GraphTrans 88.77±0.12 88.30±0.14 87.78±0.26 87.40±0.18 85.76±0.16 83.63±0.10 83.35±0.06 83.19±0.27 83.06±0.06 82.08±0.11
NodeFormer 88.77±0.21 88.67±0.29 88.28±0.29 88.14±0.13 86.01±0.58 85.83±0.22 85.33±0.08 84.70±0.28 84.11±0.12 81.02±1.97
GraphGPS 89.97±0.15 89.43±0.21 89.04±0.18 88.24±0.18 86.46±0.16 85.14±0.24 84.70±0.16 83.92±0.16 83.28±0.25 80.53±0.47
ANS-GT 86.73±0.40 86.54±0.37 86.14±0.12 85.53±0.19 83.67±0.53 82.58±0.42 82.33±0.45 82.15±0.64 81.85±0.39 80.81±0.74
DIFFormer 90.29±0.10 89.66±0.21 88.96±0.16 87.73±0.12 86.22±0.25 85.50±0.12 85.18±0.30 84.73±0.25 83.90±0.43 82.55±0.32
SGFormer 90.52±0.19 89.90±0.02 89.33±0.09 88.53±0.11 86.89±0.08 86.24±0.08 85.83±0.11 85.01±0.31 84.29±0.13 82.93±0.12
TABLE7
TestingresultswithdifferenttrainingratiosonCHAMELEON.
TrainingRatio 50% 40% 30% 20% 10% 5% 4% 3% 2% 1%
GCN 42.87±3.36 42.15±2.98 41.75±1.80 37.27±3.09 34.42±4.03 36.52±1.88 36.13±2.88 33.82±5.01 31.52±5.79 31.00±5.97
GAT 38.79±2.74 38.94±2.26 39.05±2.38 36.10±1.81 36.13±1.77 34.82±1.32 30.76±6.27 33.75±4.70 32.27±3.85 30.29±4.59
SGC 35.16±3.24 35.48±3.07 33.72±4.62 32.78±5.38 32.69±5.37 29.82±6.63 27.41±6.40 27.74±6.72 26.54±5.33 26.61±5.71
JKNet 40.36±2.47 39.62±1.81 39.53±1.80 38.84±2.41 38.03±2.36 34.81±2.00 33.06±4.24 31.62±3.95 31.95±5.33 30.76±5.06
APPNP 37.13±1.86 37.50±2.46 37.81±1.85 36.33±2.02 35.68±2.28 34.57±1.23 34.36±2.32 33.46±4.34 32.70±4.36 30.38±5.96
H2GCN 35.70±3.57 36.28±2.51 35.51±1.56 35.00±1.43 33.94±1.57 32.87±1.43 32.97±2.78 31.46±4.30 30.28±4.37 30.23±4.40
SIGN 34.17±3.18 34.01±3.01 33.77±1.97 33.59±2.62 32.42±2.54 30.13±3.48 29.04±3.46 28.02±4.64 28.42±4.23 28.05±4.04
Graphormer 27.09±2.47 27.24±1.77 27.18±1.76 26.53±2.91 25.85±2.53 25.29±3.36 23.87±3.06 24.38±3.08 24.47±3.05 25.02±2.71
Graphtrans 27.09±2.47 27.18±1.85 26.93±1.99 26.69±2.83 25.56±2.52 25.29±3.36 24.55±3.19 24.28±3.14 24.47±3.05 25.02±2.71
NodeFormer 39.64±3.04 38.75±2.84 37.58±2.44 37.84±2.81 35.58±2.10 30.77±4.02 31.55±5.28 27.87±5.37 27.88±5.73 28.14±4.96
GraphGPS 41.57±3.01 41.63±2.82 40.45±1.28 38.47±2.12 37.32±2.03 35.02±5.39 34.80±2.73 33.40±5.26 32.23±4.72 30.18±4.57
ANS-GT 38.49±1.78 37.23±1.92 40.09±1.92 36.94±2.11 35.59±1.57 30.94±2.02 28.70±1.98 28.39±3.42 29.28±1.54 28.11±1.10
DIFFormer 39.10±2.97 39.04±3.13 39.45±1.70 38.41±2.79 36.01±2.46 33.24±1.55 34.50±1.35 32.77±3.19 31.86±3.72 30.36±5.17
SGFormer 47.98±2.88 43.97±3.07 43.19±2.02 40.94±1.88 39.60±1.49 38.25±2.03 37.82±3.96 35.53±2.23 32.72±5.23 31.62±2.94
Scalability Test.Wefurthertestthemodel’sscalability training data. We consider three datasets CORA, PUBMED
w.r.t.thenumbersofnodeswithinonecomputationbatch. and CHAMELEON and randomly split the nodes in each
WeadopttheAMAZON2Mdatasetandrandomlysamplea datasetintotraining/validation/testingwithdifferentratios.
subsetofnodeswiththenodenumberrangingfrom10Kto In specific, we fix the validation and testing ratios both as
100K.Forfaircomparison,weusethesamehiddensize256 25%andchangethetrainingratiofrom50%to1%.Table5,
for all the models. In Fig. 3, we can see that the time and 6 and 7 present the experimental results on three datasets,
memorycostsofSGFormerbothscalelinearlyw.r.t.graph respectively, where we compare SGFormer with the GNN
sizes. When the node number goes up to 40K, the model andTransformercompetitors.Wefoundthatasthetraining
(SGFormerw/Softmax)thatreplacesourattentionwiththe ratiodecreases,theperformanceofallthemodelsexhibits
Softmax attention suffers out-of-memory and in contrast, degradation to a certain degree. In contrast, SGFormer
SGFormercostsonly1.5GBmemory. maintainsthesuperiorityandachievesthebesttestingscores
across most of the cases. This validates that our model
5.5 ResultswithLimitedLabeledData possessesadvancedabilitytolearnwithlimitedtrainingdata,
attributingtothesimpleandlight-weightedarchitecture.
As extension of the benchmark results, we next test the
modelwithdifferentamountoflabeleddatatoinvestigate
intothegeneralizationcapabilityofthemodelwithlimited13
 6 * ) R U P H U   $ F F X U D F \  5 2 &  $ 8 &   6 * ) R U P H U   7 L P H 
                       
                 
          
                                           
  D   F R U D   E   F L W H V H H U   F   S X E 0 H G   G   D F W R U
                                
                                                       
       
                                          
  H   V T X L U U H O   I   F K D P H O H R Q   J   G H H ] H U   K   R J E Q  S U R W H L Q V
  
                                   
                             
                
                           
  L   $ P D ] R Q  0   M   S R N H F   N   R J E Q  D U [ L Y   O   R J E Q  S D S H U V    0
Fig.4.Comparisonofsingle-layerv.s.multi-layermodelson12experimentaldatasets.Ineachdataset,weplotthetrainingtimecostperepochand
testingscores(Accuracy/ROC-AUC)ofSGFormerw.r.t.thenumberofattentionlayers.
 6 * ) R U P H U   $ F F X U D F \  5 2 &  $ 8 &   6 * ) R U P H U  Z   6 R I W P D [  6 * ) R U P H U  Z  R  V H O I  O R R S  1 R G H ) R U P H U
  
     
  
     
  
  
    
                                           
   O D \ H U V    O D \ H U V    O D \ H U V    O D \ H U V
  D   F R U D   E   F L W H V H H U   F   S X E 0 H G   G   D F W R U
  
    
     
          
             
                                          
   O D \ H U V    O D \ H U V    O D \ H U V    O D \ H U V
  H   V T X L U U H O   I   F K D P H O H R Q   J   G H H ] H U   K   R J E Q  S U R W H L Q V
           
  
        
  
  
                           
   O D \ H U V    O D \ H U V    O D \ H U V    O D \ H U V
  L   $ P D ] R Q  0   M   S R N H F   N   R J E Q  D U [ L Y   O   R J E Q  S D S H U V    0
Fig.5.Performancecomparisonofsingle-layerv.s.multi-layermodelson12experimentaldatasets.Ineachdataset,weplotthetestingscores
(Accuracy/ROC-AUC)ofSGFormer,SGFormerw/oself-loop,SGFormerw/SoftmaxandNodeFormerw.r.t.thenumberofattentionlayers.
TABLE8 5.6 AblationStudyandHyper-parameterAnalysis
Ablationstudyonattentionfunctions.
Apartfromthecomparativeexperiments,weprovidemore
Dataset CORA ACTOR OGBN-PROTEINS discussionstojustifytheeffectivenessofourmodel,includ-
SGFormer(default) 89.22±0.18 37.36±1.97 79.18±0.44 ing ablation study w.r.t. different attention functions and
w/GATAttention 83.21±0.96 35.81±1.31 73.20±0.68 analysisontheimpactofthehyper-parameterα.
w/SoftmaxAttention 76.47±0.70 27.59±1.21 68.34±0.53
w/NodeFormerAttention 81.33±2.14 35.93±0.96 72.91±0.78 ImpactofAttentionFunction.SGFormeradoptsanew
attention function for computing the all-pair interactions
to compute the updated node embeddings. To verify its
TABLE9
effectiveness, we replace it by the GAT [30]’s attention,
Performancecomparisonwithdifferentα.
Softmax attention used by [15] and NodeFormer [12]’s
Dataset CORA ACTOR OGBN-PROTEINS attention.Table8presentstheresultsusingdifferentattention
SGFormer(α=0.8) 89.22±0.18 35.64±3.07 79.04±0.18
functionsonCORA,ACTORandPROTEINS.Wefoundthatthe
SGFormer(α=0.5) 88.74±0.27 37.36±1.97 79.12±0.40 defaultattentionfunctionyieldsthebestresultsacrossthree
SGFormer(α=0.2) 75.42±0.57 37.16±1.58 79.18±0.44 datasets,whichsuggeststhatourdesignissuperiorforlearn-
SGFormer(α=0) 75.57±0.61 36.75±1.74 73.06±0.34
ingnoderepresentations.Inparticular,thevanillaSoftmax
attentionleadstounsatisfactoryperformancecomparedto
otherchoices.Thisispossiblyduetotheover-normalizing
  V P   H P L 7
 \ F D U X F F $
 \ F D U X F F $
 \ F D U X F F $
  V P   H P L 7
  V P   H P L 7
 \ F D U X F F $
 \ F D U X F F $
 \ F D U X F F $   V P   H P L 7
  V P   H P L 7
  V P   H P L 7
 \ F D U X F F $
 \ F D U X F F $
 \ F D U X F F $
 \ F D U X F F $
 \ F D U X F F $
 \ F D U X F F $
  V P   H P L 7
  V P   H P L 7
  V P   H P L 7
 \ F D U X F F $
 \ F D U X F F $
 \ F D U X F F $
 \ F D U X F F $
 \ F D U X F F $
 \ F D U X F F $
  V P   H P L 7
  V P   H P L 7
  V P   H P L 7
 & 8 $  & 2 5
 \ F D U X F F $
 \ F D U X F F $
 \ F D U X F F $
 & 8 $  & 2 5
 \ F D U X F F $14
issue as identified by [12], i.e., the normalized attention global attention and local propagation in each layer, and
scoresaftertheSoftmaxoperatortendtoattendonveryfew using the single-layer model could sacrifice the efficacy of
nodesandcausegradientvanishingforothernodesduring the latter. On top of all of these, we can see that it can be
optimization.Incontrast,thelinearattentionfunctionused apromisingfuturedirectionforexploringeffectiveshallow
by SGFormer directly computes the all-pair similarity via attentionmodelsthatcanworkconsistentlyandstablywell.
dot-productandcanovercometheover-normalizingissue. Wealsofoundwhenusingdeepermodeldepth,SGFormer
Impactofα.Thehyper-parameterαcontrolstheweights w/o self-loop exhibits clear performance degradation and
ontheGNmoduleandtheall-pairattentionforcomputing muchworsethantheresultsofSGFormer,whichsuggests
the aggregated embedding. To be specific, larger α means thattheself-looppropagationinEqn.23canhelptomaintain
thatmoreimportanceisattachedtotheGNmoduleandthe thecompetitivenessofSGFormerwithmulti-layerattentions.
input graph for computing the representations. In Table 9
we report the results with different α’s on three datasets,
which shows that the optimal setting of α varies case 6 CONCLUSIONS
by case. In particular, α = 0.8 yields the best result on
This paper aims at unlocking the potential of simple
CORA,presumablybecausethisdatasethashighhomophily
Transformer-stylearchitecturesforlearninglarge-graphrep-
ratio and the input graph is useful for the predictive task.
resentations where the scalability challenge plays a bottle-
Differently, for ACTOR, which is a heterophilic graph, we
neck.ByanalysisonthelearningbehaviorsofTransformers
found using α = 0.5 achieves the best performance. This
on graphs, we reveal a potential way to build powerful
is due to that in such a case, the observed structures can
Transformersviasimplifyingthearchitecturetoasingle-layer
be noisy and the all-pair attention becomes important for
model. Based on this, we present Simplified Graph Trans-
learning useful unobserved interactions. The similar case
formers (SGFormer) that possesses desired expressiveness
appliestoPROTEINS,whichisalsoaheterophilicgraph,and
forcapturingall-pairinteractionswiththeminimalcostof
we found setting a relatively small α leads to the highest
one-layerattention.Thesimpleandlightweightarchitecture
testing score. In fact, the informativeness of input graphs
enablestoscalesmoothlytoalargegraphwith0.1Bnodes
canvarycasebycaseinpractice,dependingtotheproperty
andyieldssignificantaccelerationoverpeerTransformerson
ofthedatasetandthedownstreamtask.SGFormerallows
medium-sizedgraphs.Ontopofourtechnicalcontributions,
enoughflexibilitytocontroltheimportanceattachedtothe
webelievetheresultscouldshedlightsonanewpromising
inputstructuresbyadjustingtheweightαinourmodel.
direction for building powerful and scalable Transformers
onlargegraphs,whichislargelyunder-explored.Forfuture
5.7 FurtherDiscussions work, we plan extend SGFormer to the setting of solving
combinatorialproblems[65],[66],[67]wherebytheGNNhas
Weproceedtoanalyzetheimpactofthenumberofmodel
beenapopularbackbone.
layersontheperformanceandfurthercomparemulti-layer
and single-layer models on different datasets. Fig. 4 and 5
presenttheexperimentalresultsacross12datasets. REFERENCES
Single-layerv.s.multi-layerattentionsofSGFormer.In
Fig. 4, we plot the training time per epoch and the testing [1] P.SchmidtkeandX.Barril,“Understandingandpredictingdrug-
gability.ahigh-throughputmethodfordetectionofdrugbinding
performanceofSGFormerwhenthelayernumberofglobal
sites,”Journalofmedicinalchemistry,vol.53,no.15,pp.5858–5867,
attentionincreasesfromonetomore.Wefoundthatusing 2010.
morelayersdoesnotcontributetoconsiderableperformance [2] D.GhersiandR.Sanchez,“Improvingaccuracyandefficiencyof
blind protein-ligand docking by focusing on predicted binding
boostandinsteadleadstoperformancedropinsomecases
sites,”Proteins:Structure,Function,andBioinformatics,vol.74,no.2,
(e.g., the heterophilic graphs ACTOR and DEEZER). Even pp.417–424,2009.
worse, multi-layer attention requires more training time [3] J.Tang,J.Sun,C.Wang,andZ.Yang,“Socialinfluenceanalysisin
costs. Notably, using one-layer attention of SGFormer can large-scalenetworks,”inACMSIGKDDInternationalConferenceon
KnowledgeDiscoveryandDataMining,2012,pp.807–816.
consistently yield highly competitive performance as the
[4] A.Sanchez-Gonzalez,J.Godwin,T.Pfaff,R.Ying,J.Leskovec,and
multi-layer attention. These results verify the theoretical P.W.Battaglia,“Learningtosimulatecomplexphysicswithgraph
results in Sec. 3 and the effectiveness of our single-layer networks,”inInternationalConferenceonMachineLearning,2020,pp.
attentionthathasdesiredexpressivityandsuperiorefficiency. 8459–8468.
[5] C.Gao,J.Chen,S.Liu,L.Wang,Q.Zhang,andQ.Wu,“Room-and-
Single-layer v.s. multi-layer attentions of other Trans-
objectawareknowledgereasoningforremoteembodiedreferring
formers. In Fig. 5, we present the testing performance of expression,” in IEEE Conference on Computer Vision and Pattern
SGFormer,NodeFormer,SGFormerw/oself-loop(removing Recognition,2021,pp.3064–3073.
[6] V.P.DwivediandX.Bresson,“Ageneralizationoftransformer
the self-loop propagation in Eqn. 23) and SGFormer w/
networkstographs,”CoRR,vol.abs/2012.09699,2020.
Softmax(replacingourattentionbytheSoftmaxattention), [7] J. Zhang, H. Zhang, C. Xia, and L. Sun, “Graph-bert: Only
w.r.t. different numbers of attention layers in respective attention is needed for learning graph representations,” CoRR,
models. We found that using one-layer attention for these vol.abs/2001.05140,2020.
[8] Z. Wu, P. Jain, M. A. Wright, A. Mirhoseini, J. E. Gonzalez,
modelscanyielddecentresultsinquiteafewcases,which
andI.Stoica,“Representinglong-rangecontextforgraphneural
suggeststhatforotherimplementationsofglobalattention, networkswithglobalattention,”inAdvancesinNeuralInformation
usingasingle-layermodelalsohaspotentialforcompetitive ProcessingSystems,2021,pp.13266–13279.
performance. In some cases, for instance, NodeFormer [9] C.Ying,T.Cai,S.Luo,S.Zheng,G.Ke,D.He,Y.Shen,andT.Liu,
“Dotransformersreallyperformbadforgraphrepresentation?”in
produces unsatisfactory results on OGBN-PROTEINS with
AdvancesinNeuralInformationProcessingSystems,2021,pp.28877–
onelayer.ThisispossiblybecauseNodeFormercouplesthe 28888.15
[10] L. Rampa´sek, M. Galkin, V. P. Dwivedi, A. T. Luu, G. Wolf, graph convolutional networks,” in International Conference on
and D. Beaini, “Recipe for a general, powerful, scalable graph LearningRepresentations,2017.
transformer,”inAdvancesinNeuralInformationProcessingSystems, [30] P.Velickovic,G.Cucurull,A.Casanova,A.Romero,P.Lio`,and
2022,pp.5998–6008. Y.Bengio,“Graphattentionnetworks,”inInternationalConference
[11] J.Zhao,C.Li,Q.Wen,Y.Wang,Y.Liu,H.Sun,X.Xie,andY.Ye, onLearningRepresentations,2018.
“Gophormer:Ego-graphtransformerfornodeclassification,”CoRR, [31] K.Xu,C.Li,Y.Tian,T.Sonobe,K.Kawarabayashi,andS.Jegelka,
vol.abs/2110.13094,2021. “Representation learning on graphs with jumping knowledge
[12] Q.Wu,W.Zhao,Z.Li,D.Wipf,andJ.Yan,“Nodeformer:Ascalable networks,”inInternationalConferenceonMachineLearning,2018,pp.
graphstructurelearningtransformerfornodeclassification,”in 5449–5458.
AdvancesinNeuralInformationProcessingSystems,2022,pp.27387– [32] F.Wu,A.H.S.Jr.,T.Zhang,C.Fifty,T.Yu,andK.Q.Weinberger,
27401. “Simplifyinggraphconvolutionalnetworks,”inInternationalConfer-
[13] J.Chen,K.Gao,G.Li,andK.He,“Nagphormer:Atokenizedgraph enceonMachineLearning,2019,pp.6861–6871.
transformerfornodeclassificationinlargegraphs,”inInternational [33] J.Zhu,Y.Yan,L.Zhao,M.Heimann,L.Akoglu,andD.Koutra,
ConferenceonLearningRepresentations,2023. “Beyondhomophilyingraphneuralnetworks:Currentlimitations
[14] Q.Wu,C.Yang,W.Zhao,Y.He,D.Wipf,andJ.Yan,“DIFFormer: andeffectivedesigns,”inAdvancesinNeuralInformationProcessing
Scalable (graph) transformers induced by energy constrained Systems,2020.
diffusion,”inInternationalConferenceonLearningRepresentations, [34] X. Li, R. Zhu, Y. Cheng, C. Shan, S. Luo, D. Li, and W. Qian,
2023. “Findingglobalhomophilyingraphneuralnetworkswhenmeeting
[15] A.Vaswani,N.Shazeer,N.Parmar,J.Uszkoreit,L.Jones,A.N. heterophily,”inInternationalConferenceonMachineLearning,2022,
Gomez,L.Kaiser,andI.Polosukhin,“Attentionisallyouneed,” pp.13242–13256.
inAdvancesinNeuralInformationProcessingSystems,2017,pp.5998– [35] J.Zhu,R.A.Rossi,A.Rao,T.Mai,N.Lipka,N.K.Ahmed,and
6008. D.Koutra,“Graphneuralnetworkswithheterophily,”inAAAI
ConferenceonArtificialIntelligence,2021,pp.11168–11176.
[16] R. Nagarajan, S. Ahmad, and M. Michael Gromiha, “Novel ap-
proachforselectingthebestpredictorforidentifyingthebinding [36] U.AlonandE.Yahav,“Onthebottleneckofgraphneuralnetworks
sitesindnabindingproteins,”Nucleicacidsresearch,vol.41,no.16, anditspracticalimplications,”inInternationalConferenceonLearning
pp.7606–7614,2013.
Representations,2021.
[37] H.Dai,Z.Kozareva,B.Dai,A.J.Smola,andL.Song,“Learning
[17] Q. Yuan, S. Chen, J. Rao, S. Zheng, H. Zhao, and Y. Yang,
steady-statesofiterativealgorithmsovergraphs,”inInternational
“Alphafold2-awareprotein-dnabindingsitepredictionusinggraph
ConferenceonMachineLearning,2018,pp.1114–1122.
transformer,”BriefingsBioinform.,vol.23,no.2,2022.
[38] L.Franceschi,M.Niepert,M.Pontil,andX.He,“Learningdiscrete
[18] L.Chanussot,A.Das,S.Goyal,T.Lavril,M.Shuaibi,M.Riviere,
structuresforgraphneuralnetworks,”inInternationalConference
K.Tran,J.Heras-Domingo,C.Ho,W.Hu,A.Palizhati,A.Sriram,
onMachineLearning,2019,pp.1972–1982.
B. M. Wood, J. Yoon, D. Parikh, C. L. Zitnick, and Z. W. Ulissi,
[39] D.Kreuzer,D.Beaini,W.L.Hamilton,V.Le´tourneau,andP.Tossou,
“Theopencatalyst2020(OC20)datasetandcommunitychallenges,”
“Rethinking graph transformers with spectral attention,” in Ad-
CoRR,vol.abs/2010.09990,2020.
vancesinNeuralInformationProcessingSystems,2021,pp.21618–
[19] Z.Fan,T.Chen,P.Wang,andZ.Wang,“Cadtransformer:Panoptic
21629.
symbol spotting transformer for CAD drawings,” in IEEE/CVF
[40] D. Chen, L. O’Bray, and K. M. Borgwardt, “Structure-aware
Conference on Computer Vision and Pattern Recognition, 2022, pp.
transformer for graph representation learning,” in International
10976–10986.
ConferenceonMachineLearning,2022,pp.3469–3489.
[20] Y.LiaoandT.E.Smidt,“Equiformer:Equivariantgraphattention
[41] M. S. Hussain, M. J. Zaki, and D. Subramanian, “Global self-
transformerfor3datomisticgraphs,”inInternationalConferenceon
attentionasareplacementforgraphconvolution,”inACMSIGKDD
LearningRepresentations,2023.
ConferenceonKnowledgeDiscoveryandDataMining,2022,pp.655–
[21] E.Min,Y.Rong,T.Xu,Y.Bian,P.Zhao,J.Huang,D.Luo,K.Lin,
665.
andS.Ananiadou,“Maskedtransformerforneighhourhood-aware
[42] W.L.Hamilton,Z.Ying,andJ.Leskovec,“Inductiverepresentation
click-throughrateprediction,”CoRR,vol.abs/2201.13311,2022.
learning on large graphs,” in Advances in Neural Information
[22] J.Devlin,M.Chang,K.Lee,andK.Toutanova,“BERT:pre-training ProcessingSystems,2017,pp.1024–1034.
ofdeepbidirectionaltransformersforlanguageunderstanding,”
[43] J. Kim, T. D. Nguyen, S. Min, S. Cho, M. Lee, H. Lee, and
CoRR,vol.abs/1810.04805,2018.
S.Hong,“Puretransformersarepowerfulgraphlearners,”CoRR,
[23] T.B.Brown,B.Mann,N.Ryder,M.Subbiah,J.Kaplan,P.Dhariwal, vol.abs/2207.02505,2022.
A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, [44] Z.Zhang,Q.Liu,Q.Hu,andC.Lee,“Hierarchicalgraphtrans-
A.Herbert-Voss,G.Krueger,T.Henighan,R.Child,A.Ramesh, former with adaptive node sampling,” in Advances in Neural
D. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, InformationProcessingSystems,2022,pp.21171–21183.
M.Litwin,S.Gray,B.Chess,J.Clark,C.Berner,S.McCandlish, [45] D.I.Shuman,S.K.Narang,P.Frossard,A.Ortega,andP.Van-
A.Radford,I.Sutskever,andD.Amodei,“Languagemodelsare dergheynst,“Theemergingfieldofsignalprocessingongraphs:
few-shot learners,” in Advances in Neural Information Processing Extendinghigh-dimensionaldataanalysistonetworksandother
Systems,2020,pp.1877–1901. irregulardomains,”IEEESignalProcessingMagazine,vol.30,no.3,
[24] A.Dosovitskiy,L.Beyer,A.Kolesnikov,D.Weissenborn,X.Zhai, pp.83–98,2013.
T.Unterthiner,M.Dehghani,M.Minderer,G.Heigold,S.Gelly, [46] V. Kalofolias, “How to learn a graph from smooth signals,” in
J.Uszkoreit,andN.Houlsby,“Animageisworth16x16words: ArtificialIntelligenceandStatistics,2016,pp.920–929.
Transformers for image recognition at scale,” in International [47] G. Fu, P. Zhao, and Y. Bian, “p-Laplacian based graph neural
ConferenceonLearningRepresentations,2021. networks,”inInternationalConferenceonMachineLearning,2022,pp.
[25] E. Min, R. Chen, Y. Bian, T. Xu, K. Zhao, W. Huang, P. Zhao, 6878–6917.
J. Huang, S. Ananiadou, and Y. Rong, “Transformer for [48] D.ZhouandB.Scho¨lkopf,“Regularizationondiscretespaces,”in
graphs:Anoverviewfromarchitectureperspective,”CoRR,vol. Pattern Recognition, 27th DAGM Symposium, vol. 3663, 2005, pp.
abs/2202.08455,2022. 361–368.
[26] W.Hu,M.Fey,H.Ren,M.Nakata,Y.Dong,andJ.Leskovec,“OGB- [49] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Scho¨lkopf,
LSC: A large-scale challenge for machine learning on graphs,” “Learningwithlocalandglobalconsistency,”inAdvancesinNeural
in Neural Information Processing Systems Track on Datasets and InformationProcessingSystems,2004,pp.321–328.
Benchmarks,2021. [50] Y.Yang,Y.Wang,Z.Huang,andD.Wipf,“Implicitvsunfolded
[27] W. Hu, M. Fey, M. Zitnik, Y. Dong, H. Ren, B. Liu, M. Catasta, graphneuralnetworks,”CoRR,vol.abs/2111.06592,2021.
andJ.Leskovec,“Opengraphbenchmark:Datasetsformachine [51] A.Bastos,A.Nadgeri,K.Singh,H.Kanezashi,T.Suzumura,and
learningongraphs,”inAdvancesinNeuralInformationProcessing I.O.Mulang’,“Howexpressivearetransformersinspectraldomain
Systems,2020. forgraphs?”TransactionsonMachineLearningResearch,vol.2022,
[28] F.Scarselli,M.Gori,A.C.Tsoi,M.Hagenbuchner,andG.Mon- 2022.
fardini,“Thegraphneuralnetworkmodel,”IEEEtransactionson [52] H.Zeng,H.Zhou,A.Srivastava,R.Kannan,andV.K.Prasanna,
neuralnetworks,vol.20,no.1,pp.61–80,2008. “Graphsaint:Graphsamplingbasedinductivelearningmethod,”in
[29] T.N.KipfandM.Welling,“Semi-supervisedclassificationwith InternationalConferenceonLearningRepresentations,2020.16
[53] W.Chiang,X.Liu,S.Si,Y.Li,S.Bengio,andC.Hsieh,“Cluster-
gcn: An efficient algorithm for training deep and large graph
convolutionalnetworks,”inACMSIGKDDInternationalConference
onKnowledgeDiscovery&DataMining,2019,pp.257–266.
[54] M.Fey,J.E.Lenssen,F.Weichert,andJ.Leskovec,“Gnnautoscale:
Scalable and expressive graph neural networks via historical
embeddings,”inInternationalConferenceonMachineLearning,2021,
pp.3294–3304.
[55] P.Sen,G.Namata,M.Bilgic,L.Getoor,B.Gallagher,andT.Eliassi-
Rad,“Collectiveclassificationinnetworkdata,”AImagazine,vol.29,
no.3,pp.93–106,2008.
[56] J.Tang,J.Sun,C.Wang,andZ.Yang,“Socialinfluenceanalysisin
large-scalenetworks,”inACMSIGKDDInternationalConferenceon
KnowledgeDiscoveryandDataMining,2009,pp.807–816.
[57] B.Rozemberczki,C.Allen,andR.Sarkar,“Multi-scaleattributed
nodeembedding,”JournalofComplexNetworks,vol.9,no.2,2021.
[58] B.RozemberczkiandR.Sarkar,“Characteristicfunctionsongraphs:
Birdsofafeather,fromstatisticaldescriptorstoparametricmodels,”
in ACM International Conference on Information and Knowledge
Management,2020,pp.1325–1334.
[59] J. J. McAuley, R. Pandey, and J. Leskovec, “Inferring networks
ofsubstitutableandcomplementaryproducts,”inACMSIGKDD
International Conference on Knowledge Discovery and Data Mining,
2015,pp.785–794.
[60] J.LeskovecandA.Krevl,“Snapdatasets:Stanfordlargenetwork
datasetcollection.”2016.
[61] D.Lim,X.Li,F.Hohne,andS.Lim,“Newbenchmarksforlearning
onnon-homophilousgraphs,”CoRR,vol.abs/2104.01404,2021.
[62] O. Platonov, D. Kuznedelev, M. Diskin, A. Babenko, and
L. Prokhorenkova, “A critical look at the evaluation of GNNs
underheterophily:Arewereallymakingprogress?”inInternational
ConferenceonLearningRepresentations,2023.
[63] J. Klicpera, A. Bojchevski, and S. Gu¨nnemann, “Predict then
propagate:Graphneuralnetworksmeetpersonalizedpagerank,”
inInternationalConferenceonLearningRepresentations,2019.
[64] E.Rossi,F.Frasca,B.Chamberlain,D.Eynard,M.M.Bronstein,
andF.Monti,“SIGN:scalableinceptiongraphneuralnetworks,”
CoRR,vol.abs/2004.11198,2020.
[65] J.Yan,S.Yang,andE.Hancock,“Learningforgraphmatchingand
relatedcombinatorialoptimizationproblems,”inProceedingsofthe
InternationalJointConferenceonArtificialIntelligence(IJCAI),2020.
[66] R.Wang,J.Yan,andX.Yang,“Combinatoriallearningofrobust
deep graph matching: an embedding based approach,” IEEE
TransactionsonPatternAnalysisandMachineIntelligence(TPAMI),
vol.45,no.6,pp.6984–7000,2023.
[67] Z.Jiang,T.Wang,andJ.Yan,“Unifyingofflineandonlinemulti-
graphmatchingviafindingshortestpathsonsupergraph,”IEEE
transactions on pattern analysis and machine intelligence (TPAMI),
vol.43,no.10,pp.3648–3663,2021.