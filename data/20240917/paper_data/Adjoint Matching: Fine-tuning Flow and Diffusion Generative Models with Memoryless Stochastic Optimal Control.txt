Adjoint Matching: Fine-tuning Flow and
Diffusion Generative Models with
Memoryless Stochastic Optimal Control
Carles Domingo-Enrich1, Michal Drozdzal1, Brian Karrer1, Ricky T. Q. Chen1
1FAIR, Meta
Dynamicalgenerativemodelsthatproducesamplesthroughaniterativeprocess,suchasFlowMatching
and denoising diffusion models, have seen widespread use, but there has not been many theoretically-
sound methods for improving these models with reward fine-tuning. In this work, we cast reward
fine-tuning as stochastic optimal control (SOC). Critically, we prove that a very specific memoryless
noiseschedulemustbeenforcedduringfine-tuning,inordertoaccountforthedependencybetweenthe
noise variable and the generated samples. We also propose a new algorithm named Adjoint Matching
which outperforms existing SOC algorithms, by casting SOC problems as a regression problem. We
find that our approach significantly improves over existing methods for reward fine-tuning, achieving
better consistency, realism, and generalization to unseen human preference reward models, while
retaining sample diversity.
Correspondence: Carles Domingo-Enrich at cd2754@nyu.edu
Base model (Flow Matching) w/ Guidance Adjoint Matching (Ours)
Figure1 WeintroduceAdjointMatching,atheoretically-drivenyetsimplealgorithmforrewardfine-tuningthatworks
for a large family of dynamical generative models, including for the first time, Flow Matching models. Text prompts:
“Beautiful colorful sunset midst of building in Bangkok Thailand”, “Beautiful grandma and granddaughter are mixing
salad and smiling while cooking in kitchen”, “The beautiful young woman in sunglasses is standing at the background of
field and hill. She is smiling and looking over shoulder”, “Chess, intellectual games, figure horse, chess board”.
1
4202
peS
31
]GL.sc[
1v16880.9042:viXra1 Introduction
Flow Matching (Lipman et al., 2023; Albergo and Vanden-Eijnden, 2023; Liu et al., 2023b) and denoising
diffusion (Song and Ermon, 2019; Ho et al., 2020; Song et al., 2021; Kingma et al., 2021) models are being
used for many generative modeling applications, including text-to-image (Rombach et al., 2022; Esser et al.,
2024), text-to-video (Singer et al., 2022), and text-to-audio (Le et al., 2024; Vyas et al., 2023). In most cases,
the base generative model does not achieve the desired sample quality. To improve the generated samples, it
is common to resort to techniques such as classifier-free guidance (Ho and Salimans, 2022; Zheng et al., 2023)
to get better text-to-sample alignment, or to fine-tune using human preference reward models to improve
sample quality and realism (Wallace et al., 2023a; Clark et al., 2024).
In the adjacent field of large language models, the behavior of the model is aligned to human preferences
through fine-tuning with reinforcement learning from human feedback (RLHF). Either explicitly or implicitly,
RLHF methods (Ziegler et al., 2020; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022) assume a
reward model r(x) that captures human preferences, with the goal of modifying the base generative model
such that it generates the following tilted distribution:
p∗(x)∝pbase(x)exp(r(x)), (1)
where p is the base generative model’s sample distribution.
base
Inspired by this, fine-tuning methods have been developed to improve denoising diffusion models based on
human preference data; either using a reward-based approach (Fan and Lee, 2023; Black et al., 2024; Fan
et al., 2023; Xu et al., 2023; Clark et al., 2024; Uehara et al., 2024a,b), or direct preference optimization
(Wallace et al., 2023a). However, unlike the fine-tuning methods designed for large language models, most of
the existing methods to a large degree ignore pbase and focus solely on the reward model. Reward models
can range from standard evaluation metrics such as ClipScore (Hessel et al., 2021; Kirstain et al., 2023b)
to specialized models that have been trained on human preferences (Schuhmann and Beaumont, 2022; Xu
et al., 2023; Wu et al., 2023b). As these are parameterized by neural networks, they fall pray to adversarial
examples which lead to the generation of undesirable artifacts (Goodfellow et al., 2014; Mordvintsev et al.,
2015). This has led some works to consider adding regularization during fine-tuning (Fan et al., 2024; Uehara
et al., 2024b) to incentivize staying close to the base model distribution; however, there does not yet exist a
simple approach which actually provably generates from the tilted distribution (1).
The main contributions of our paper are as follows:
(i) Wepresentastochasticoptimalcontrol(SOC)formulationforrewardfine-tuningofdynamicalgenerative
models. Importantly, we prove that the naïve approach considered by prior works lead to a value
function bias problem that biases the fine-tuned model away from the tilted distribution (1). This
problem has also been observed by Uehara et al. (2024b) but they propose a more complicated solution
which involves training a separate generative model for the optimal noise distribution.
(ii) Instead, we propose a very simple solution: the memoryless noise schedule. This is a unique noise
schedule that completely removes the dependency between noise variables and the generated samples,
resulting in provable convergence to the tilted distribution. This allows us to fine-tune dynamical
generativemodelsinfullgenerality,includingbeingthefirsttofine-tunenoiselessFlowMatchingmodels.
(iii) We also propose a new method for solving SOC problems, called Adjoint Matching, which combines the
scalability of gradient-based methods and the simplicity of a least-squares regression objective. This is
orthogonal to the reward fine-tuning application and can be applied to general SOC problems.
(iv) We perform extensive comparisons to baseline approaches, and analyze them from multiple perspectives
such as realism, consistency, and diversity. We find that our proposed method provides generalization to
unseen human preference reward models, better text-to-sample consistency, and retains good diversity.
In the following, sections are broken down as follows: Section 2 summarizes the algorithms used for sampling
from pre-trained Flow Matching and diffusion models, while Section 3 provides a common notation that we
will use throughout. Sections 4 and 5 form the core of our contributions. Section 4 details the value function
bias problem and our proposed solution via the memoryless noise schedule. Section 5 details the new Adjoint
Matching algorithm for solving SOC problems.
22 Preliminaries on dynamical generative models
We are interested in fine-tuning base generative models pbase(X ) where samples are generated through the
1
simulation of a stochastic process. That is, these models transform noise variables into a sample through
an iterative process. In particular, we discuss the specific constructions and sampling processes of Flow
Matching(Lipmanetal.,2023;Liuetal.,2023b;Liu,2022;AlbergoandVanden-Eijnden,2023)andDenoising
Diffusion Models (Ho et al., 2020; Song et al., 2021, 2022). The goal of this section is to provide background
information on these methods, which we will later unify into a single consistent notation in Section 3.
GivenrandomvariablesfromaninitialdistributionX¯ ∼p =N(0,I),andX¯ whicharedistributedaccording
0 0 1
to some data distribution, we define the reference flow X¯ =(X¯ ) where
t t∈[0,1]
X¯ =β X¯ +α X¯ , (2)
t t 0 t 1
where (α ) ,(β ) are functions such that α =β =0 and α =β =1. Diffusion models and Flow
t t∈[0,1] t t∈[0,1] 0 1 1 0
Matching construct generative Markov processes X with initial distribution X ∼N(0,I) that result in flows
t 0
X = (X ) with the same time marginals as the reference flow X¯, i.e., the random variables X and
t t∈[0,1] t
X¯ have identical distribution for all times t∈[0,1]. This implies X has the same distribution as the data
t 1
distribution, so simulating the Markov process from random noise X is a way to generate artificial samples1.
0
2.1 Flow Matching
In its simplest form, the generative Markov process of a Flow Matching model is an ordinary differential
equation (ODE) of the form:
dX =v(X ,t)dt, X ∼N(0,I). (3)
t t 0
where v(X ,t) is a parametric velocity that is optimized to match the derivative of the reference flow, i.e.,
t
v(X t,t)=argmin vˆE(cid:13) (cid:13)vˆ(X¯ t,t)− dd tX¯ t(cid:13) (cid:13)2 (seee.g.Lipmanetal.(2023)fordetailsonpre-trainingFlowMatching
models). It can then be proven that the solution of the generative process (3) has the same time marginals as
the reference flow (Lipman et al., 2023; Liu, 2022; Albergo and Vanden-Eijnden, 2023), and a commonly used
choice is α = t and β = 1−t. One can also consider a family of stochastic differential equations (SDEs)
t t
with an arbitrary state-independent diffusion coefficient2:
(cid:18) (cid:16) (cid:17)(cid:19)
dX
t
= v(X t,t)+ 2βt(σ
αα˙
tt(t β) t2
−β˙ t)
v(X t,t)− αα˙ ttX
t
dt+σ(t)dB t, X
0
∼N(0,I), (4)
where (B ) is a Brownian motion. The generative processes in (3) and (4) have the same time marginals.
t t≥0
ThiscanbeseenbywritingdowntheFokker-Planckequationsfor(3)and(4), andobservingthattheyarethe
same up to a cancellation of terms (Maoutsa et al., 2020). The diffusion coefficient σ(t) in (4) is compensated
by the second term in the drift which scales proportionally as σ(t)2.
2.2 Denoising Diffusion Models
We next discuss diffusion models, in particular the sampling scheme proposed by Denoising Diffusion Implicit
Model (DDIM; Song et al. (2022)) which we will later relate to Denoising Diffusion Probabilistic Models
(DDPM; Ho et al. (2020)) as a particular case of the former. For sampling from a diffusion model, the DDIM
update rule3 (Song et al. (2022), Eq. 12), typically stated in discrete time with k ∈{0,...,K}, is:
X
k+1
=√ α¯ k+1(cid:0)Xk−√ 1 √− αα ¯¯ kkϵ(Xk,k)(cid:1) +(cid:112) 1−α¯ k+1−σ k2ϵ(X k,k)+σ kε k, ε
k
∼N(0,I), X
0
∼N(0,I), (5)
where α¯ is an increasing sequence such that α¯ =0, α¯ =1, and the sequence σ is arbitrary. That is, one
k 0 K k
samples an initial Gaussian random variable x , and applies the stochastic update (5) iteratively K times in
0
order to obtain an artificial sample X . Updates can be interpreted as progressively denoising the iterate:
K
x is completely noisy and x is fully denoised. The noise predictor model ϵ(x ,k) is trained to predict the
0 K k
noise of x (see e.g. Ho et al. (2020) for details on pre-training denoising diffusion models).
k
1Inourderivations,wewillsimplyassumethebasemodelhasbeentrainedperfectlyduringthepre-trainingphase.
2Weusethecommonshort-hand“over-dot” notationtodenotethetimederivative,i.e.,x˙t= dd txt.
3WeslightlydepartfromthenotationinSongetal.(2022)byflippingthedirectionoftimeandusingα¯ whichcorresponds
k
totheα inSongetal.(2022)whileitcorrespondstotheα¯ inHoetal.(2020).
k k
33 Flow Matching and diffusion models from a common perspective
We formulate Flow Matching and diffusion models in a unified framework, which we will later use throughout
the paper. Firstly, to simplify notation, we will be using continuous-time formulations. This will also directly
enable fine-tuning methods inspired by the continuous-time paradigm, which we find tends to perform better
than discrete-time counterparts in our empirical validations. Secondly, by consolidating notation, we will be
abletodiscussfine-tuningofdynamicalgenerativemodelsthatfollowthesametimemarginalsasthereference
flow (2), pre-trained with either the Denoising Diffusion or Flow Matching framework, in full generality.
To convert DDIM to a continuous-time stochastic process, we can show that the DDIM update rule (5), up to
a first-order approximation, is equivalent to the Euler-Maruyama discretization of the following SDE:
dX t =(cid:0) 2α¯˙ α¯t tX t−(cid:0) 2α¯˙ α¯t
t
+ σ( 2t)2(cid:1)ϵb √as 1e( −X α¯t t,t)(cid:1) dt+σ(t)dB t, X 0 ∼N(0,I). (6)
See Appendix B.1 for the full derivation. To go from (5) to (6), we assumed a uniform discretization of time,
i.e. t= k. This results in identifying the discrete-time process (X ) with a continuous-time process
K k k∈{0,...,K}
(X t) t∈[0,1], where α¯
k
:=α¯ t, σ
k
:= √1 Kσ(t), and ϵ(X k,k) with ϵbase(X
k
√,t). In relation √to the reference flow (2),
the generative process in (6) has the same time marginals when α = α¯ and β = 1−α¯ (Ho et al., 2020).
t t t t
Furthermore, when viewed up to first order approximations, the DDPM sampling scheme (Ho et al. (2020);
Algorithm 2) can be seen as special instance of the DDIM sampling scheme when σ(t)=(cid:112) α¯˙ t/α¯t. This results
in the following generative process:
(cid:113)
dX t =(cid:0) 2α¯˙ α¯t tX t− α α¯ ¯˙ t tϵb √as 1e( −X α¯t t,t)(cid:1) dt+ α α¯ ¯˙ t tdB t, X 0 ∼N(0,I), (7)
We can further consolidate notation by converting all quantities to the score function s(x,t)—defined as the
gradient of the log density of the random variable X —which is possible when X is Normal-distributed and
t 0
under the affine reference flow (2). In particular, the velocity vbase from Flow Matching can be expressed in
terms of the score function:
vbase(x,t)= α α˙t tx+β t( αα˙t tβ t−β˙ t)s(x,t). (8)
And the noise predictor ϵbase also admits an expression in terms of the score function (see Appendix B.3):
ϵbase(x,t)=−√s(x,t) . (9)
1−α¯t
Plugging these two equations into (4) and (6), respectively, and rewriting them in terms of only the α and β
t t
in (2), we can unify both the Flow Matching and continuous-time DDIM generative processes as:
dX =b(X ,t)dt+σ(t)dB , X ∼N(0,I), (10)
t t t 0
where b(x,t)=κ tx+(cid:0)σ( 2t)2 +η t(cid:1) s(x,t), κ
t
= αα˙t t, η
t
=β t(α α˙t tβ t−β˙ t) (11)
where (α ,β ) are coefficients of the reference flow (2). We have hence expressed the generative process of a
t t
base model, whether it is a Flow Matching or a diffusion model, as an SDE of the form (10)-(11), unified by
the choice of reference flow.
4 Fine-tuning as “memoryless” stochastic optimal control
We now discuss the crux of the problem: how to produce a fine-tuned generative model that produces samples
X which follow the tilted distribution involving a reward model (1). An obvious direction is to construct
1
a fine-tuning objective involving both the base generative model and the reward model, where the optimal
solution results in a fine-tuned generative model for the tilted distribution. However, as we will explain, this
turns out to be non-trivial, because a naïve formulation will introduce bias into the solution.
In Section 4.1, we discuss the problem formulation of stochastic optimal control, a general framework for
optimizing SDEs, and its relation to the maximum entropy reinforcement learning framework commonly used
4for RLHF fine-tuning. Next, in Section 4.2, we discuss the initial value function bias problem which plagues
existing approaches and so far has seen no simple solution. Finally, in Section 4.3, we propose a novel simple
solution that circumvents the bias problem, by enforcing a particular diffusion coefficient, the memoryless
noise schedule, to be used during fine-tuning. This results in an extremely simple fine-tuning objective that
provably converges to a model which generates the tilted distribution (1) without any statistical bias.
4.1 Preliminaries on the stochastic optimal control problem formulation
Stochastic optimal control (SOC; Bellman (1957); Fleming and Rishel (2012); Sethi (2018)) considers
general optimization problems over stochastic differential equations, but we only need to consider a common
instantiation, the control-affine problem formulation:
minE(cid:2)(cid:82)1(cid:0)1∥u(Xu,t)∥2+f(Xu,t)(cid:1) dt+g(Xu)(cid:3) , (12)
u∈U 0 2 t t 1
s.t. dXu =(cid:0) b(Xu,t)+σ(t)u(Xu,t)(cid:1) dt+σ(t)dB , Xu ∼p (13)
t t t t 0 0
where in (13), Xu ∈Rd is the state of the stochastic process, u:Rd×[0,1]→Rd is commonly referred to as
t
the control vector field, b:Rd×[0,1]→Rd is a base drift, and σ :[0,1]→Rd×d is the diffusion coefficient.
These jointly define the controlled process Xu ∼pu that we are interested in optimizing; often both b and σ
are fixed and we only optimize over the control u.
As part of the objective functional (12), we have an affine control cost 1∥u(Xu,t)∥2, a running state cost
2 t
f :Rd×[0,1]→R and a terminal state cost g :Rd →R.
The stochastic optimal control (SOC) objective (12) can be decomposed recursively from the final time value.
It is common to define the cost functional which is the expected future cost starting from state x at time t:
J(u;x,t):=E X∼pu(cid:104) (cid:82) t1(cid:0)1 2∥u(X s,s)∥2+f(X s,s)(cid:1) ds+g(X 1)(cid:12) (cid:12)X
t
=x(cid:105) . (14)
From here, the value function is the optimal value of the cost functional4 :
V(x,t):=min J(u;x,t)=J(u∗;x,t), (15)
u∈U
where u∗ is the optimal control, i.e., minimizer of (12). Furthermore, a classical result is that the value
functioncanbeexpressedintermsoftheuncontrolled baseprocesspbase (Kappen(2005), seeDomingo-Enrich
et al. 2023, Eq. 8, App. B for a self-contained proof):
V(x,t)=−logE X∼pbase(cid:104) exp(−(cid:82) t1 f(X s,s)ds−g(X 1))(cid:12) (cid:12)X
t
=x(cid:105) . (16)
A useful expression for the optimal control (which we will make use of in deriving the Adjoint Matching
objective in Section 5) is that it is related to the gradient of the value function:
u∗(x,t)=−σ(t)⊤∇ V(x,t)=−σ(t)⊤∇ J(u∗,x,t). (17)
x x
Relation to MaxEnt RL. Stochasticoptimalcontrolwiththecontrol-affineformulation(12)isthecontinuous-
time equivalence of maximum entropy reinforcement learning (MaxEnt RL; Todorov (2006); Ziebart et al.
(2008)) with a KL regularization instead of only an entropy regularization. In particular, by the Girsanov
theorem (Theorem 2), the affine control cost is equivalent to a Kullback–Leibler (KL) divergence between the
base process pbase, when u=0, and the controlled process pu, when conditioned on the same initial state X
0
(see Appendix C.4):
D KL(cid:0) pu(X|X 0)(cid:13) (cid:13)pbase(X|X 0)(cid:1) =E Xu∼pu(cid:104) (cid:82) 01 21∥u(X tu,t)∥2dt(cid:105) , (18)
resulting in the KL-regularized RL interpretation of (12):
max E (cid:104) E (cid:2)(cid:82)1 −f(Xu,t)dt−g(Xu)(cid:3) −D (pu(X|X )∥pbase(X|X ))(cid:105) , (19)
u∈U X0∼p0 X∼pu(·|X0) 0 t 1 KL 0 0
where the negative state costs correspond to intermediate and terminal rewards in the RL interpretation. The
KL divergence incentivizes the optimal solution to stay close to the distribution of the base process.
4NotethatthereisaslightdifferenceinterminologybetweenSOCandreinforcementlearning,whereourcostfunctionalis
referredtoasthestatevaluefunctionandourvaluefunctionistheoptimalstatevaluefunctioninRL.
54.2 The initial value function bias problem
We next discuss why naïvely adding a KL regularization does not lead to the tilted distribution (1). From
(19), we can also show that the optimal distribution conditioned on X is5
0
p∗(X|X )∝pbase(X|X )exp(cid:0) −(cid:82)1 f(X ,t)dt−g(X )(cid:1) . (20)
0 0 0 t 1
This is analogous to the exponentiated reward distribution in MaxEnt RL (Rawlik et al., 2013), but since we
generalize the entropy regularization to a KL regularization, pbase acts as a prior distribution.
Importantly, in order to relate this to the tilted distribution (1) that we want to achieve for fine-tuning, we
need to marginalize all time values and check the distribution of p∗(X ). In order to do this, first notice that
1
the normalization constant of (20) is exactly the value function at t=0:
E (cid:104) exp(cid:0) −(cid:82)1 f(X ,t)dt−g(X )(cid:1)(cid:105) =exp(cid:0) −V(X ,0)(cid:1) , (21)
X∼pbase(X|X0) 0 t 1 0
where the equality is due to (16). Therefore, we see that this normalization constant depends on X . Dividing
0
(20) by (21) and multiplying by p (X ), we obtain the normalized distribution over the full path X,
0 0
p∗(X)=pbase(X)exp(cid:0) −(cid:82)1 f(X ,t)dt−g(X )+V(X ,0)(cid:1) . (22)
0 t 1 0
Setting f =0 and g =−r, we arrive at an expression for the optimal distribution
p∗(X ,X )∝pbase(X ,X )exp(cid:0) r(X )+V(X ,0)(cid:1) . (23)
0 1 0 1 1 0
Thisunfortunatelydoesnotleadtothetilteddistribution(1)becausewehaveabiasintheoptimaldistribution
that is due to the value function of the initial distribution V(X ,0). That is to say, naïvely adding a KL
0
regularization (18) to the fine-tuning objective in the sense of (19) leads to a biased distribution (22) after
fine-tuning and is not equivalent to the tilted distribution (1). For instance, when the sampling procedure is
noiseless, i.e., σ(t)=0, fine-tuning naïvely will not have any effect because X completely determines X .
0 1
This is unlike the situation for large language models (Ouyang et al., 2022; Rafailov et al., 2023), where there
is no dynamical process that samples X iteratively and hence no dependence on the initial noise variable
1
X . Although this KL regularization is a common objective for RLHF of large language models, it has seen
0
seldom use in fine-tuning diffusion models, likely due to this issue of the initial value function bias.
In the context of diffusion models, KL regularization (19) has been explored in prior works (Fan et al., 2024),
butitsbehaviorwasnotwell-understoodandtheydidnotrelatethefine-tunedmodeltothetilteddistribution
(1). Another direction that has been proposed is to learn the initial distribution p to cancel out the bias
0
(Uehara et al., 2024b; Tang, 2024) but this simply shifts the work into tilting the initial distribution and
requires an auxiliary model for parameterizing the optimal initial distribution. In contrast, we show in the
next section that it is possible to remove the value function bias by simply choosing a very particular noise
schedule during the fine-tuning procedure.
4.3 The memoryless noise schedule for fine-tuning dynamical generative models
In this section, we propose a very simple method of turning (23) into the tilted distribution (1) through the
use of a particular memoryless noise schedule. Throughout, we provide an intuitive explanation of why this
noise schedule is sufficient for fine-tuning while discussing the full theoretical result where we show that the
memoryless noise schedule is actually not only sufficient but also necessary.
Intuitively, the main reason we cannot arrive at the tilted distribution from (23) is due to the pbase(X ,X )
0 1
distribution not factoring into X and X . Hence, we define a memoryless generative process as follows:
0 1
Definition 1 (Memoryless generative process). A generative process of the form (10)-(11) is memoryless if
X and X are independent, i.e., pbase(X ,X )=pbase(X )pbase(X ).
0 1 0 1 0 1
5Note(20)isinformalbecausedensitiesovercontinuous-timeprocessesareill-defined;theformalstatementis dPd bP a∗ se(X|X0)=
exp(−(cid:82) 01f(Xt,t)dt−g(X1)),where dPd bP a∗
se
denotestheRadon-Nikodymderivative. Wetreatthisformallyintheproofs.
6κ η Diffusion coefficient σ(t) Memoryless X
t t t
Flow Matching (3) αα˙ tt β t(cid:0)α α˙t tβ t−β˙ t(cid:1) General ( √commonly 0) No
Memoryless Flow Matching (4) α α˙t
t
β t(cid:0)α α˙t tβ t−β˙ t(cid:1) 2η
t
Yes
DDIM (6) α¯˙ t α¯˙ t General (commonly 0) No
2α¯t 2α¯t
√
DDPM (7) 2α¯˙ α¯t
t
2α¯˙ α¯t
t
2η t Yes
Table 1 Diffusion coefficient σ(t) and the factors κ , η for the Flow Matching, Memoryless Flow Matching, DDIM,
t t √
and DDPM generative processes. When the diffusion coefficient is σ(t)= 2η , the generative process is memoryless,
t
i.e., samples X will be independent of the initial noise X .
1 0
When the base generative process is memoryless, this implies:
p∗(X )=(cid:82) pbase(X )pbase(X )exp(r(X )+V(X ,0))dX ∝pbase(X )exp(r(X )). (24)
1 0 1 1 0 0 1 1
Thatis,solvingtheSOCproblem(12)-(13)withamemorylessbasemodelwillresultinafine-tunedmodelthat
generates samples p∗(X ) according to the tilted distribution (1). This memoryless property is not satisfied
1
generally by the family of generative processes captured by (12)-(13). For instance, the Flow Matching and
DDIM generative processes with zero diffusion coefficient (i.e., σ(t)=0) are definitely not memoryless due to
X and X being theoretically invertible. Below, we provide the sufficient and neccessary condition for the
0 1
noise schedule in order to have a memoryless generative process.
Proposition1(Memorylessnoiseschedules). Withinthefamilyofgenerativeprocesses (10)-(11), agenerative
process is memoryless if and only if the noise schedule is chosen as:
σ(t)2 =2η +χ(t), where χ:[0,1]→R is s.t. ∀t∈(0,1], lim α exp(cid:0) −(cid:82)t χ(s)ds(cid:1) =0. (25)
t t′→0+ t′ t′ 2β2
s
√
In particular, we refer to σ(t)= 2η as the memoryless noise schedule.
t
Due to the endpoint constraints of (α ,β ) for the reference flow (2), the memoryless noise schedule σ(t) is
t t
infiniteatt=0andapproacheszeroatt=1. Thisprovidesawayforthegenerativeprocesstomixwhenclose
to noise X while stay steadying when close to the sample X . Hence, the sample will have no information
0 1
about X due to the enormous amount of mixing with a large diffusion coefficient. Furthermore, while we
0
have intuitively justified the memoryless noise schedule through its independence property, our theoretical
result is actually even stronger: all generative models of the form (10)-(11) must be fine-tuned using the
memoryless noise schedule. We formalize this in the following theorem, which we prove in Appendix D.2:
Theorem 1 (Fine-tuningrecipeforgeneralnoiseschedulesampling). Withinthefamilyofgenerativeprocesses
(10)-(11), in order to allow the use of arbitrary noise schedules and still generate samples according to the tilted
distribution (1), the fine-tuning problem (12)-(13) with f =0 and g =−r must be done with the memoryless
√
noise schedule σ(t)= 2η .
t
Theorem 1 states that we need to use the memoryless noise schedule for fine-tuning with the SOC objective—
or equivalently, the KL regularized reward objective (19). This is the only noise schedule that retains the
relationship between the velocity and score function, allowing the conversion to arbitrary noise schedules (e.g.,
σ(t)=0) after fine-tuning. It is worth noting that when using the memoryless noise schedule for DDIM, this
recovers what we derived as the continuous-time limit of the DDPM generative process (7). However, the
DDPM sampler (Ho et al., 2020) is not commonly used while the DDIM sampler (Song et al., 2022) and
Flow Matching models typically generate samples using σ(t)=0, so an explicit conversion to the memoryless
noise schedule is necessary for fine-tuning. To the best of our knowledge, we are not aware of any existing
works that have proposed a time-varying diffusion coefficient with theoretical guarantees. Table 1 summarizes
the memoryless schedule for diffusion and Flow Matching models, which we refer to as Memoryless Flow
Matching. In Figure 2, we visualize fine-tuning a 1D model, where we see that constant σ(t) leads to biased
distributions whereas the memoryless noise schedule perfectly converges to the tilted distribution (1).
For convenience, we plug the memoryless noise schedule into the controlled process for fine-tuning (13), and
express them in terms of each respective framework. Let ϵbase, vbase denote the pre-trained vector fields
7(a) Pretrained FM vbase (b) Fine-tuned FM vfinetune (c) Fine-tuned FM vfinetune (d) Fine-tuned FM vfinetune
√
with σ(t)=0.2 with σ(t)=1.0 with memoryless σ(t)= 2η
t
Figure 2 Visualization of Theorem 1 showing that fine-tuning must be done with the memoryless noise schedule to
ensure convergence to the tilted distribution (1). (a) Shows the base Flow Matching model. (b, c) Fine-tuning using a
constant σ(t) leads to biased distributions. (d) Fine-tuning using the memoryless noise schedule leads to the correct
tilted distribution. Note that sample generation can use any noise schedule after fine-tuning, including σ(t)=0.
and ϵfinetune, vfinetune the fine-tuned vector fields. Then we have the following expressions for the full drift
√
b(x,t)+σ(t)u(x,t) and control u(x,t) when σ(t)= 2η :
t
DDIM / DDPM:
(cid:113)
b(x,t)+σ(t)u(x,t)= α¯˙ t x− α¯˙ tϵfin √etune(x,t), u(x,t)=− α¯˙ t (ϵfinetune(x,t)−ϵbase(x,t)). (26)
2α¯t α¯t 1−α¯t α¯t(1−α¯t)
Memoryless Flow Matching:
(cid:114)
b(x,t)+σ(t)u(x,t)=2vfinetune(x,t)− α˙tx, u(x,t)= 2 (vfinetune(x,t)−vbase(x,t)). (27)
αt βt(α α˙t tβt−β˙ t)
Thus, to solve the SOC problem (12)-(13) in practice, we parameterize the control u in terms of ϵfinetune or
vfinetune and optimize these vector fields instead. After plugging in (26)-(27), the SOC problem (12)-(13)
can then be solved using any SOC algorithm in order to perform fine-tuning, and we proposed an especially
effective algorithm next in Section 5. After fine-tuning, ϵfinetune and vfinetune can simply be plugged back into
their respective generative processes (3)-(7) to sample from the tilted distribution (1) using any choice of
diffusion coefficient.
5 Adjoint Matching for control-affine stochastic optimal control
We discuss existing methods and also propose a new method for optimizing control-affine SOC problems. The
new Adjoint Matching method is a combination of the time-tested continuous adjoint method (Pontryagin,
1962) with recent developments on constructing least-squares objectives for solving SOC problems (Domingo-
Enrich et al., 2023). In this section, we briefly discuss preliminaries on existing methods, their pros and
cons, then detail the Adjoint Matching algorithm and its surprising connections to the prior methods. For
numerical optimization, we now assume that the control u is a parametric model with parameters θ.
85.1 Existing methods for stochastic optimal control
5.1.1 The adjoint method
The most basic method of optimizing the simulation of an SDE is to directly differentiate through the
simulation using gradients from the SOC objective function. The adjoint method simply uses the objective:
L(u;X):=(cid:82)1(cid:0)1∥u(X ,t)∥2+f(X ,t)(cid:1) dt+g(X ), X ∼pu. (28)
0 2 t t 1
This is a stochastic estimate of the control objective in (12), and the goal is to take compute the gradient
of L(u;X) with respect to the parameters θ of the control u. Due to the continuous-time nature of SDEs,
there are two main approaches to implementing this numerically. Firstly, the Discrete Adjoint method uses a
“discretize-then-differentiate” approach, where the numerical solver for simulating the SDE is simply stored
in memory then differentiated through, and it has been studied extensively (e.g., Bierkens and Kappen
(2014); Gómez et al. (2014); Hartmann and Schütte (2012); Kappen et al. (2012); Rawlik et al. (2013); Haber
and Ruthotto (2017)). This approach, however, uses an extremely large amount of memory as the full
computational graph of the numerical solver must be stored in memory and implementations often must rely
on gradient checkpointing (Chen et al., 2016) to reduce memory usage.
Secondly, the Continuous Adjoint method exploits the continuous-time nature of SDEs and uses an analytical
expression for the gradient of the control objective with respect to the intermediate states X , expressed as an
t
adjoint ODE, and then applies a numerical method to simulate this gradient itself, hence it is referred to as a
“differentiate-then-discretize” approach (Pontryagin, 1962; Chen et al., 2018; Li et al., 2020). We first define
the adjoint state as:
a(t;X,u):=∇ (cid:0)(cid:82)1(cid:0)1∥u(X ,t′)∥2+f(X ,t′)(cid:1) dt′+g(X )(cid:1) ,
Xt t 2 t′ t′ 1 (29)
where X solves dX =(cid:0) b(X ,t)+σ(t)u(X ,t)(cid:1) dt+σ(t)dB .
t t t t
This implies that E (cid:2) a(t;X,u)|X =x(cid:3) =∇ J(u;x,t), where J denotes the cost functional defined in
X∼pu t x
(14). It can then be shown that this adjoint state satisfies 6:
da(t;X,u)=−(cid:104) a(t;X,u)T(cid:0) ∇ (b(X ,t)+σ(t)u(X ,t))(cid:1) +∇ (cid:0) f(X ,t)+ 1∥u(X ,t)∥2(cid:1)(cid:105) , (30)
dt Xt t t Xt t 2 t
a(1;X,u)=∇g(X ). (31)
1
The adjoint state is solved backwards in time, starting from the terminal condition (31). Compututation of
(30) can be done with a vector-Jacobian product which can be efficiently done on automatic differentiation
software (Paszke et al., 2019). Once the adjoint state has been solved for t ∈ [0,1], then the gradient of
L(u;X) with respect to the parameters θ can be obtained by integrating over the entire time interval:
dL = 1(cid:82)1 ∂ ∥u(X ,t)∥2dt+(cid:82)1 ∂u(Xt,t)T σ(t)Ta(t;X,u)dt, (32)
dθ 2 0 ∂θ t 0 ∂θ
where the first term is the partial derivative of L w.r.t. θ and the second term is the partial derivative
through the sample trajectory X. See Proposition 6 in Appendix E.1 for a statement and proof of this
result. The discrete and continuous adjoint methods converge to the same gradient as the step size of the
numerical solvers go to zero. Both are scalable to high dimensions and have seen their fair share of usage in
optimizing neural ODE/SDEs (Chen et al., 2018, 2020; Li et al., 2020). As the adjoint methods are essentially
gradient-based optimization algorithms applied on a highly non-convex problem, many have also reported
they can be unstable empirically (Mohamed et al., 2020; Suh et al., 2022; Domingo-Enrich et al., 2023).
5.1.2 Importance-weighted matching objectives for regressing onto the optimal control
An alternative is to consider regressing onto the optimal control u∗, which is the approach of the cross-entropy
method (Rubinstein and Kroese, 2013; Zhang et al., 2014) and stochastic optimal control matching (SOCM;
Domingo-Enrich et al. (2023)). These methods make use of path integral theory (Kappen, 2005) to express
6NoteweusetheconventionthataJacobianmatrixJ =∇xv(x)isdefinedasJij = ∂v ∂i x( jx).
9the optimal control through importance sampling, resulting in an importance-weighted least-squares objective
function
L (u;X):=(cid:82)1 ∥u(X ,t)−uˆ∗(X ,t)∥2dt×ω(u,X), X ∼pu, (33)
SOCM 0 t t
where ω is an importance weighting that approximates sampling from the optimal distribution p∗, and uˆ∗
is a stochastic estimator of the optimal control relying on having sampled from the optimal process. We
defer to Domingo-Enrich et al. (2023) for the exact details. The functional landscape of this objective is
convex, which is argued to help yield stable training. However, the need for importance sampling renders this
impractical for high dimensional applications: the variance of the importance weighting ω grows exponentially
with dimension of the stochastic process, leading to catastrophic failure. This unfortunately means that
such importance-weighted matching objectives are impractical for fine-tuning dynamical generative models;
however, aleast-squaresobjectiveisgreatlycovetedasitcanleadtostabletrainingandsimpleinterpretations.
5.2 Adjoint Matching
We make two important observations which lead to our proposed method: (i) it is possible to construct a
matching objective without any importance weighting, and (ii) there are unnecessary terms in the adjoint
differential equation (30) that can lead to higher variance at convergence.
Firstly, we notice that we can simply match the gradient of the cost functional under the current control.
That is, while SOCM carefully constructs an importance-weighted estimator of the optimal control u∗ =
−σ(t)T∇J(u∗;x,t)(17),weclaimthatwecanactuallyjustregressontothetargetvectorfield−σ(t)T∇J(u;x,t)
whereuisthecurrentcontrol,andfurthermore,thisresultsinagradientequalinexpectationtothecontinuous
adjoint method. We formalize this in the following proposition, proven in Appendix E.2:
Proposition 2. Let us define, for now, the basic Adjoint Matching objective as:
L Basic−Adj−Match(u;X):= 1 2(cid:82) 01(cid:13) (cid:13)u(X tu¯,t)+σ(t)Ta(t;Xu¯,u¯)(cid:13) (cid:13)2 dt, X ∼pu¯, u¯=stopgrad(u), (34)
where u¯=stopgrad(u) means that the gradients of u¯ with respect to the parameters θ of the control u are
artificially set to zero. The gradient of L (u;X) with respect to θ is equal to the gradient dL in
equation (32). Importantly, the only critB ica asi lc− poA id nj− tM ofat Ech (cid:2) L (cid:3) is the optimal control u∗. dθ
Basic−Adj−Match
CriticalpointsofLarecontrolsusuchthat δ L(u)=0,where δ Ldenotesthefirstvariationofthefunctional
δu δu
L. In other words, Proposition 2 states that the only control that satisfies the first-order optimality condition
for the basic Adjoint Matching objective is the optimal control, which provides theoretical grounding for
gradient-based optimization algorithms.
An intuitive way to understand the basic Adjoint Matching objective is that it is a consistency loss. The
Adjoint Matching objective is based off of the observation that the optimal control u∗(x,t) is the unique
fixed-point of the relation u(x,t)=−σ(t)T∇ J(u;x,t) (see Lemma 6 in Appendix E.2) and so we are directly
x
optimizing for a control that fits this relation, while using the adjoint state as a stochastic estimator of
∇ J(u;x,t) (29).
x
The basic Adjoint Matching objective in Proposition 2 does not yet yield a novel algorithm for stochastic
optimal control, because it produces the same gradient as the continuous adjoint method. This can be seen
by taking the gradient w.r.t. θ after expanding the square in (34) and removing terms that do not depend
on θ to arrive exactly at the continuous adjoint method (32). However, it provides the means of deriving a
simpler leaner objective function.
The“Lean” Adjoint. Theminimizerofaleast-squaresobjectiveistheconditionalexpectationoftheregression
target, so for the Adjoint Matching objective, at the optimum we have that
u∗(x,t)=E (cid:2) −σ(t)Ta(t;X,u∗)|X =x(cid:3) . (35)
X∼p∗ t
Multiplying both sides by the Jacobian ∇ u∗(x,t) and re-arranging, we get the relation
x
E (cid:2) u∗(x,t)T∇ u∗(x,t)+a(t;X,u∗)Tσ(t)∇ u∗(x,t)|X =x(cid:3) =0. (36)
X∼p∗ x x t
10Algorithm 1 Adjoint Matching for fine-tuning Flow Matching models
Input: Pre-trained FM velocity field vbase, step size h, number of fine-tuning iterations N.
Initialize fine-tuned vector fields: vfinetune =vbase with parameters θ.
for n∈{0,...,N −1} do
(cid:113)
Sample m trajectories X =(X ) with memoryless noise schedule σ(t)= 2β (α˙tβ −β˙ ), e.g.:
t t∈{0,...,1} t αt t t
(cid:16) (cid:17) √
X =h 2vfinetune(X ,t)− α˙tX + hσ(t)ε , ε ∼N(0,I), X ∼N(0,I). (40)
t+h θ t αt t t t 0
For each trajectory, solve the lean adjoint ODE (38)-(39) backwards in time from t=1 to 0, e.g.:
(cid:16) (cid:17)
a˜ =a˜ +ha˜T∇ 2vbase(X ,t)− α˙tX , a˜ =−∇ r(X ). (41)
t−h t t Xt t αt t 1 X1 1
Note that X and a˜ should be computed without gradients, i.e., X =stopgrad(X ), a˜ =stopgrad(a˜ ).
t t t t t t
For each trajectory, compute the Adjoint Matching objective (37):
L Adj−Match(θ)=(cid:80) t∈{0,...,1−h}(cid:13) (cid:13) σ(2 t)(cid:0) v θfinetune(X t,t)−vbase(X t,t)(cid:1) +σ(t)a˜ t(cid:13) (cid:13)2. (42)
Compute the gradient ∇ L(θ) and update θ using favorite gradient descent algorithm.
θ
end
Output: Fine-tuned vector field vfinetune
Notice that the terms inside the expectation in (36) show up as part of the adjoint differential equation (30),
which we have now shown to have expectation zero at the optimal solution. Furthermore, the variance of the
terms inside (36) is non-zero even at the optimal solution, so the basic Adjoint Matching (34) and hence the
continuous adjoint method will also have non-vanishing gradients even when u=u∗.
Therefore, we motivate the definition of a lean adjoint state a˜ with the terms in (36) removed. Plugging this
lean adjoint back into the least-squares objective, we obtain our final proposed Adjoint Matching objective:
L Adj−Match(u;X):= 1 2(cid:82) 01(cid:13) (cid:13)u(X t,t)+σ(t)Ta˜(t;X)(cid:13) (cid:13)2 dt, X ∼pu¯, u¯=stopgrad(u), (37)
where da˜(t;X)=−(a˜(t;X)⊤∇ b(X ,t)+∇ f(X ,t)), (38)
dt x t x t
a˜(1;X)=∇ g(X ). (39)
x 1
Equations (38)-(39) define the lean adjoint state, and (37) is the complete Adjoint Matching objective. The
unique critical point of E[L ] is the optimal control, which we prove relying on Proposition 2 and
Adj−Match
equation (36) (see Proposition 7 in Appendix E.3).
Compared to the importance sampling methods (Section 5.1.2), Adjoint Matching is a simple least-squares
regression objective and has no importance weighting. This allows it to avoid the pitfalls of high variance
importance weights and makes it as scalable as the adjoint methods while retaining the interpretation of
matching a target vector field.
Comparedtotheadjointmethod(Section5.1.1),AdjointMatchingproducesadifferent gradient in expectation
than the continuous adjoint. This is because the lean adjoint state is not related to the gradient of the cost
functional anymore, i.e., (29) is not true, except at the optimum when u=u∗. Even at the optimal solution,
sinceAdjointMatchingremovestermsthathaveexpectationzero,itcanpotentiallyexhibitbetterconvergence
and lower variance than the continuous adjoint method. Additionally, computation of the lean adjoint state
(38) also exhibits a smaller computational cost due to the removal of the extra terms (no longer need the
Jacobian of the control ∇ u). We provide a rigorous derivation of Adjoint Matching and the above claims in
x
Appendix E.3.
AdjointMatchingcanbeappliedtorewardfine-tuningofdynamicalgenerativemodelsthroughthememoryless
SOC formulation discussed in Section 4. We provide pseudo-code for this in Algorithm 1 for Flow Matching
models and in Algorithm 2 in Appendix E.4 for denoising diffusion models.
116 Related work
Fine-tuning from human feedback. There are two main overarching approaches to RLHF: the reward-based
approach (Ziegler et al., 2020; Stiennon et al., 2020; Ouyang et al., 2022; Bai et al., 2022) and direct preference
optimization (DPO; Rafailov et al. (2023)). The reward-based approach (Ziegler et al., 2020; Stiennon et al.,
2020; Ouyang et al., 2022; Bai et al., 2022) consists in learning the reward model r(x) from human preference
data, and then solving a maximum entropy RL problem with rewards produced by r(x). DPO merges the two
previous steps into one: there is no need to learn r(x) as human preference data is directly used to fine-tune
the model. However, DPO is typically only applied with a filtered dataset, and does not work explicitly with
a reward model. Furthermore, for flow and diffusion models specifically, it is possible to differentiate the
reward function, so there is a larger emphasis on reward-based approaches.
Fine-tuning for diffusion models. Among existing reward-based diffusion fine-tuning methods, Fan and Lee
(2023) interpret the denoising process as a multi-step decision-making task and use policy gradient algorithms
to fine-tune diffusion samplers. Black et al. (2024) makes use of proximal policy gradients for fine-tuning
but this does not make use of the differentiability of the reward model. Fan et al. (2023) also consider
KL-regularized rewards (19) but do not make the critical connection to the tilted distribution (1) that we
flesh out in Section 4.2. The fine-tuning algorithms of Xu et al. (2023); Clark et al. (2024) directly take
gradients of the reward model and use heuristics to try to stay close to the original base generative model,
but their behavior is not well understood and unrelated to the tilted distribution: Xu et al. (2023) takes
gradients of the reward applied on the denoised sample at different points in time, and Clark et al. (2024)
backpropagates the reward function through all or part of the diffusion trajectory. Finally, Uehara et al.
(2024b) also fine-tune diffusion models with the goal of sampling from the tilted distribution (1), but their
approach is much more involved than ours as it requires learning a value function, and solving two stochastic
optimal control problems.
Inference-time optimization methods. Some have proposed methods that do not update the base model but
insteadmodifythegenerationprocessdirectly. Oneapproachistoaddaguidancetermtothevelocity(Chung
et al., 2022; Song et al., 2023; Pokle et al., 2023); however, this is a heuristic and it is not well-understood
what particular distribution is being generated. Another approach is to directly optimize the initial noise
distribution (Li, 2021; Wallace et al., 2023b; Ben-Hamu et al., 2024); this is taking an opposite approach to
theinitalvaluebiasproblemthanusbymovingalloftheworkintooptimizingtheinitialdistribution. Amore
computationally intensive approach is to perform online estimation of the optimal control, for the purpose
of heuristically solving an optimal control problem within the sampling process (Huang et al., 2024; Rout
et al., 2024); these approaches aim to solve a separate control problem for each generated sample, instead of
performing amortization (Amos et al., 2023) to learn a fine-tuned generative model.
Optimal control in generative modeling. Methods from optimal control have been used to train dynamical
generative models parameterized by ODEs (Chen et al., 2018), SDEs (Li et al., 2020), and jump processes
(Chen et al., 2020), enabled through the adjoint method. They can be used to train arbitrary generative
processes,butforsimplifiedconstructionsthesehavefalleninfavortosimulation-freematchingobjectivessuch
as denoising score matching (Vincent, 2011) and Flow Matching (Lipman et al., 2023). The optimal control
formalism also has significance in sampling from un-normalized distributions (Zhang and Chen, 2022; Berner
et al., 2023). The inclusion of a state cost has been used to solve transport problems where intermediate path
distributions are of importance (Liu et al., 2023a; Pooladian et al., 2024). These collective advances naturally
lead to the consideration of the optimal control formalism for reward fine-tuning.
7 Experiments
We experimentally validate our proposed method on reward fine-tuning a Flow Matching base model (Lipman
et al., 2023). In particular, we use the usual setup of pre-training an autoencoder for 512×512 resolution
images,thentrainingatext-conditionalFlowMatchingmodelonthelatentvariableswithaU-netarchitecture
(Long et al., 2015), similar to the setup in Rombach et al. (2022). We pre-trained our base model using a
12Fine-tuning Fine-tuning Sampling DreamSim
ClipScore↑ PickScore↑ HPS v2↑
Method σ(t) σ(t) Diversity↑
√
None 2η 24.15 17.25 16.19 53.60
N/A t ±0.26 ±0.06 ±0.17 ±1.37
(Base model) 0 28.32 18.15 17.89 56.53
±0.22 ±0.07 ±0.16 ±1.52
√ √
2η 2η 30.18 19.38 24.61 25.54
DRaFT-1 t t ±0.24 ±0.08 ±0.17 ±0.99
0 0 30.95 19.37 24.37 27.39
±0.28 ±0.06 ±0.17 ±1.14
√ √
2η 2η 26.94 18.34 19.98 41.98
DRaFT-40 t t ±0.28 ±0.19 ±1.02 ±2.14
0 0 30.07 19.45 24.06 36.53
±0.39 ±0.08 ±0.24 ±1.69
√ √
2η 2η 24.11 17.24 16.15 53.27
DPO t t ±0.22 ±0.06 ±0.14 ±1.36
0 0 27.77 17.92 17.30 54.11
±0.18 ±0.07 ±0.20 ±1.50
√ √
2η 2η 28.59 18.68 22.24 32.71
ReFL t t ±0.31 ±0.10 ±0.46 ±2.76
0 0 30.06 19.07 23.06 32.69
±0.63 ±0.21 ±0.41 ±1.28
√
Cont. Adjoint √ 2η 26.99 18.33 20.83 46.59
2η t ±0.43 ±0.16 ±0.63 ±1.40
λ=12500 t 0 29.49 18.98 21.34 48.41
±0.32 ±0.16 ±0.53 ±1.44
√
Disc. Adjoint √ 2η 28.04 18.44 20.04 54.90 2η t ±0.57 ±0.21 ±0.39 ±2.03
λ=12500 t 0 29.28 18.82 19.73 53.36
±0.17 ±0.14 ±0.17 ±2.48
√
Adj.-Matching √ 2η 30.36 19.29 24.12 40.89
2η t ±0.22 ±0.08 ±0.17 ±1.50
λ=1000 t 0 31.41 19.57 23.29 43.10
±0.22 ±0.09 ±0.18 ±1.76
√
Adj.-Matching √ 2η 30.59 19.49 24.85 37.07
2η t ±0.40 ±0.10 ±0.23 ±1.47
λ=2500 t 0 31.64 19.71 24.12 39.88
±0.21 ±0.09 ±0.27 ±1.59
√
Adj.-Matching √ 2η 30.62 19.50 24.95 34.50
2η t ±0.30 ±0.09 ±0.28 ±1.33
λ=12500 t 0 31.65 19.76 24.49 37.24
±0.19 ±0.08 ±0.27 ±1.57
Table 2 Evaluation metrics of different fine-tuning methods for text-to-image generation. The second and third
√
columns show the noise schedules σ(t) used for fine-tuning and for sampling: σ(t)= 2η corresponds to Memoryless
t
Flow Matching, and σ(t)=0 to the Flow Matching ODE (3). We report standard errors estimated over 3 runs of
the fine-tuning algorithm on random sets of 40000 training prompts, each evaluated over a random set of 1000 test
prompts.
Adjoint Matching (Ours) DRaFT-1
Figure 3 Our proposed Adjoint Matching using the memoryless SOC formulation introduces a much more principled
way of trading off how close to stay to the base model while optimizing the reward model. In contrast, baseline
methods such as DRaFT-1 only optimize the reward model and must rely on early stopping to perform this trade off,
resulting in a much more sensitive hyperparameter. Samples are produced using σ(t)=0 with the same noise sample.
Text prompts: “Handsome Smiling man in blue jacket portrait” and “Quinoa and Feta Stuffed Baby Bell Peppers”.
13
0001=λ
0052=λ
00521=λ
senilesaB
COS
sselyromeM
1000
itrs.
2000
itrs.
4000
itrs.Text prompt: “Man sitting on sofa at home in front of Text prompt: “3D World Food Day Morocco”
fireplace and using laptop computer, rear view”
Figure 4 Generated samples from varying classifier-free guidance weight w, from an Adjoint Matching fine-tuned
model. Higher guidance increases text-to-image consistency but loses diversity and has use cases for generating highly
structured images such as 3D renderings. Corresponding samples from the base model can be found in Figure 6.
dataset of licensed text and image pairs. Then for fine-tuning, we consider the reward function:
r(x):=λ×RewardModel(x) (43)
corresponding to a scaled version of the reward model, which we take to be ImageReward (Xu et al., 2023).
Different values of λ provide different tradeoffs between the KL regularization and the reward model (19).
For evaluation and benchmarking purposes, we report metrics that separately quantify text-to-image con-
sistency, human preference, and sample diversity, capturing the tradeoff between each aspect of generative
models (Astolfi et al., 2024). For consistency, we make use of the standard ClipScore (Hessel et al., 2021) and
PickScore (Kirstain et al., 2023a); for generalization to unseen human preferences, we use the HPSv2 model
(Wu et al., 2023a); and for diversity, we compute averages of pairwise distances of the DreamSim features (Fu
et al., 2023). More details are provided in Appendix G.4.
As our baselines, we consider the DPO (Wallace et al., 2023a), ReFL (Xu et al., 2023), and DRaFT-K
algorithms (Clark et al., 2024). DPO does not use gradients from the reward function, while ReFL and
DRaFT make use of heuristic gradient stopping approaches to stay close to the base generative model. Out
of these baseline methods, we find that DRaFT-1 performs the best, so we perform additional ablation
experimentscomparingtothismethod. WithinthesameSOCformulationasourmethod,wealsoconsiderthe
discrete and continuous adjoint methods. We provide full experimental details in Appendix G; an important
implementation detail is that we slightly offset σ(t) in order to avoid division by zero.
Evaluation results. In Table 2 we report the evaluation metrics for the baselines as well as our proposed
Adjoint Matching approach. We compare each method at roughly the same wall clock time (see the times and
number of iterations in Table 8). We find that across all metrics, our proposed memoryless SOC formulation
outperforms existing baseline methods. The choice of SOC algorithms also obviously favors Adjoint Matching
over continuous and discrete adjoint methods, which result in poorer consistency and human preference
metrics.
Ablation: base model vs. reward tradeoff. We note that the scaling in front of the reward model λ determines
how strongly the we should prefer the reward model over the base model. As such, we see a natural tradeoff
curve: higher λ results in better consistency and human preference, but lower diversity in the generated
samples. Overall, we find that Adjoint Matching performs stably across all values of λ. Our method of
regularizing the fine-tuning procedure through memoryless SOC works much better than baseline methods
14
0.0=
w
0.1=
w
0.4=
wAdj. Match. w=0 Adj. Match. w=1 Adj. Match. w=4
DRaFT-1 w=0 DRaFT-1 w=1 DRaFT-1 w=4
33.5 25.5 33.5
25.0
33.0 33.0
24.5
32.5 32.5
24.0
32.0 23.5 32.0
31.5 23.0 31.5
22.5
31.0 31.0
22.0
30.5 30.5
21.5
20 25 30 35 40 20 25 30 35 40 21.5 22.0 22.5 23.0 23.5 24.0 24.5 25.0 25.5
DreamSim Diversity DreamSim Diversity HPS v2
Figure 5 Tradeoffs between different aspects of generative models: text-to-image consistency (ClipScore), sample
diversity for each prompt (DreamSim Diversity), and generalization to unseen human preferences (HPS v2). Different
points are obtained from varying values of λ for Adjoint Matching and varying number of fine-tuning iterations for the
DRaFT-1 baseline. Overall, we find our proposed method Adjoint Matching has the best Pareto fronts.
which often must employ early stopping. We show the qualitative effect of varying λ in Figure 3, while for the
DRaFT-1 baseline we show the effect of varying the number of fine-tuning iterations.
Ablation: classifier-free guidance. We note that it is possible to apply classifier-free guidance (CFG; Ho
and Salimans (2022); Zheng et al. (2023)) after fine-tuning. We use the formula (1+w)v(x,t|y)−wv(x,t),
where w is the guidance weight, v(x,t|y) is a fine-tuned text-to-image model while v(x,t) is an unconditional
image model. This is not principled as only the conditional model is fine-tuned, but generally it is unclear
what distribution guided models sample from anyhow. In Figure 5 we show the evaluation metrics with
classifier-free guidance applied. Comparing three different guidance weight values, we see a higher weight does
improve text-to-image consistency, and to some extent, human preference, but this comes at the cost of being
worse in terms of diversity. We show qualitative differences in Figure 4.
8 Conclusion
We investigate the problem of fine-tuning dynamical generative models such as Flow Matching and propose
the use of a stochastic optimal control (SOC) formulation with a memoryless noise schedule. This ensures
we converge to the same tilted distribution that the large language modeling literature uses for learning
from human feedback. In particular, the memoryless noise schedule corresponds to DDPM sampling for
diffusion models and a new Memoryless Flow Matching generative process for flow models. In conjunction,
we propose a novel training algorithm for solving stochastic optimal control problems, by casting SOC as a
regression problem, which we call the Adjoint Matching objective. Empirically, we find that our memoryless
SOC formulation works better than multiple existing works on fine-tuning diffusion models, and our Adjoint
Matching algorithm outperforms related gradient-based methods. In summary, we are the first to provide
a theoretically-driven algorithm for fine-tuning Flow Matching models, and we find that our approach
significantly outperforms baseline methods across multiple axes of evaluation—text-to-image consistency,
generalization to unseen human preference, and sample diversity—on large-scale text-to-image generation.
References
Michael S Albergo, Nicholas M Boffi, and Eric Vanden-Eijnden. Stochastic interpolants: A unifying framework for
flows and diffusions. arXiv preprint arXiv:2303.08797, 2023. Cited on page 35.
15
erocSpilC 2v
SPH
erocSpilCMichael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In The
Eleventh International Conference on Learning Representations, 2023. Cited on pages 2, 3, and 35.
Brandon Amos et al. Tutorial on amortized optimization. Foundations and Trends® in Machine Learning, 16(5):
592–732, 2023. Cited on page 12.
PietroAstolfi,MarleneCareil,MelissaHall,OscarMañas,MatthewMuckley,JakobVerbeek,AdrianaRomeroSoriano,
and Michal Drozdzal. Consistency-diversity-realism pareto fronts of conditional image generative models. arXiv
preprint arXiv:2406.10429, 2024. Cited on page 14.
Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort,
Deep Ganguli, Tom Henighan, Nicholas Joseph, Saurav Kadavath, Jackson Kernion, Tom Conerly, Sheer El-Showk,
NelsonElhage, ZacHatfield-Dodds, DannyHernandez, TristanHume, ScottJohnston, ShaunaKravec, LianeLovitt,
Neel Nanda, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, Ben Mann,
and Jared Kaplan. Training a helpful and harmless assistant with reinforcement learning from human feedback,
2022. Cited on pages 2 and 12.
RichardBellman. Dynamicprogramming. PrincetonLandmarksinMathematics.PrincetonUniversityPress,Princeton,
NJ, 2010., 1957. Cited on page 5.
Heli Ben-Hamu, Omri Puny, Itai Gat, Brian Karrer, Uriel Singer, and Yaron Lipman. D-flow: Differentiating through
flows for controlled generation, 2024. Cited on page 12.
Julius Berner, Lorenz Richter, and Karen Ullrich. An optimal control perspective on diffusion-based generative
modeling, 2023. Cited on page 12.
JorisBierkensandHilbertJKappen. Explicitsolutionofrelativeentropyweightedcontrol. Systems & Control Letters,
72:36–43, 2014. Cited on page 9.
KevinBlack,MichaelJanner,YilunDu,IlyaKostrikov,andSergeyLevine.Trainingdiffusionmodelswithreinforcement
learning. In The Twelfth International Conference on Learning Representations, 2024. Cited on pages 2, 12, and 36.
Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and David K Duvenaud. Neural ordinary differential equations.
In Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018. Cited on pages 9
and 12.
Ricky T. Q. Chen, Brandon Amos, and Maximilian Nickel. Learning neural event functions for ordinary differential
equations. arXiv preprint arXiv:2011.03902, 2020. Cited on pages 9 and 12.
Tianqi Chen, Bing Xu, Chiyuan Zhang, and Carlos Guestrin. Training deep nets with sublinear memory cost. arXiv
preprint arXiv:1604.06174, 2016. Cited on page 9.
Hyungjin Chung, Jeongsol Kim, Michael T Mccann, Marc L Klasky, and Jong Chul Ye. Diffusion posterior sampling
for general noisy inverse problems. arXiv preprint arXiv:2209.14687, 2022. Cited on page 12.
Kevin Clark, Paul Vicol, Kevin Swersky, and David J. Fleet. Directly fine-tuning diffusion models on differentiable
rewards. In The Twelfth International Conference on Learning Representations, 2024. Cited on pages 2, 12, and 14.
ValentinDeBortoli,JamesThornton,JeremyHeng,andArnaudDoucet. Diffusionschrödingerbridgewithapplications
to score-based generative modeling. In Advances in Neural Information Processing Systems, volume 34, pages
17695–17709. Curran Associates, Inc., 2021. Cited on page 33.
Carles Domingo-Enrich, Jiequn Han, Brandon Amos, Joan Bruna, and Ricky T. Q. Chen. Stochastic optimal control
matching, 2023. Cited on pages 5, 8, 9, 10, 45, and 46.
Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik
Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In
Forty-first International Conference on Machine Learning, 2024. Cited on page 2.
Ying Fan and Kangwook Lee. Optimizing ddpm sampling with shortcut fine-tuning. In International Conference on
Machine Learning, 2023. Cited on pages 2 and 12.
Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad
Ghavamzadeh,KangwookLee,andKiminLee. Dpok: Reinforcementlearningforfine-tuningtext-to-imagediffusion
models, 2023. Cited on pages 2 and 12.
16Ying Fan, Olivia Watkins, Yuqing Du, Hao Liu, Moonkyung Ryu, Craig Boutilier, Pieter Abbeel, Mohammad
Ghavamzadeh,KangwookLee,andKiminLee. Reinforcementlearningforfine-tuningtext-to-imagediffusionmodels.
Advances in Neural Information Processing Systems, 36, 2024. Cited on pages 2 and 6.
W.H. Fleming and R.W. Rishel. Deterministic and Stochastic Optimal Control. Stochastic Modelling and Applied
Probability. Springer New York, 2012. Cited on page 5.
StephanieFu,NetanelTamir,ShobhitaSundaram,LucyChai,RichardZhang,TaliDekel,andPhillipIsola. Dreamsim:
Learning new dimensions of human visual similarity using synthetic data. arXiv:2306.09344, 2023. Cited on pages
14 and 53.
Vicenç Gómez, Hilbert J Kappen, Jan Peters, and Gerhard Neumann. Policy search for path integral control. In Joint
European Conference on Machine Learning and Knowledge Discovery in Databases, pages 482–497. Springer, 2014.
Cited on page 9.
Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv
preprint arXiv:1412.6572, 2014. Cited on page 2.
Eldad Haber and Lars Ruthotto. Stable architectures for deep neural networks. Inverse problems, 34(1):014004, 2017.
Cited on page 9.
CarstenHartmannandChristofSchütte. Efficientrareeventsimulationbyoptimalnonequilibriumforcing. Journal of
Statistical Mechanics: Theory and Experiment, 2012(11):P11004, 2012. Cited on page 9.
Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation
metric for image captioning. arXiv preprint arXiv:2104.08718, 2021. Cited on pages 2 and 14.
Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Cited on
pages 2, 15, and 23.
JonathanHo,AjayJain,andPieterAbbeel.Denoisingdiffusionprobabilisticmodels.InAdvancesinNeuralInformation
Processing Systems, volume 33. Curran Associates, Inc., 2020. Cited on pages 2, 3, 4, and 7.
YujiaHuang,AdishreeGhatare,YuanzheLiu,ZiniuHu,QinshengZhang,ChandramouliSSastry,SiddharthGururani,
Sageev Oore, and Yisong Yue. Symbolic music generation with non-differentiable rule guided diffusion. arXiv
preprint arXiv:2402.14285, 2024. Cited on page 12.
Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,
Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt.
Openclip, July 2021. Cited on page 52.
H J Kappen. Path integrals and symmetry breaking for optimal control theory. Journal of Statistical Mechanics:
Theory and Experiment, 2005(11), nov 2005. Cited on pages 5 and 9.
Hilbert J Kappen, Vicenç Gómez, and Manfred Opper. Optimal control as a graphical model inference problem.
Machine learning, 87(2):159–182, 2012. Cited on page 9.
Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. On density estimation with diffusion models. In
A.Beygelzimer,Y.Dauphin,P.Liang,andJ.WortmanVaughan,editors,AdvancesinNeuralInformationProcessing
Systems, 2021. Cited on page 2.
Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open
dataset of user preferences for text-to-image generation. 2023a. Cited on pages 14 and 52.
Yuval Kirstain, Adam Polyak, Uriel Singer, Shahbuland Matiana, Joe Penna, and Omer Levy. Pick-a-pic: An open
dataset of user preferences for text-to-image generation. In Thirty-seventh Conference on Neural Information
Processing Systems, 2023b. Cited on page 2.
Matthew Le, Apoorv Vyas, Bowen Shi, Brian Karrer, Leda Sari, Rashel Moritz, Mary Williamson, Vimal Manohar,
YossiAdi,JayMahadeokar,etal. Voicebox: Text-guidedmultilingualuniversalspeechgenerationatscale. Advances
in neural information processing systems, 36, 2024. Cited on page 2.
Dongzhuo Li. Differentiable gaussianization layers for inverse problems regularized by deep generative models. arXiv
preprint arXiv:2112.03860, 2021. Cited on page 12.
Xuechen Li, Ting-Kam Leonard Wong, Ricky T. Q. Chen, and David Duvenaud. Scalable gradients for stochastic
differential equations. In International Conference on Artificial Intelligence and Statistics, pages 3870–3882. PMLR,
2020. Cited on pages 9 and 12.
17Yaron Lipman, Ricky T. Q. Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative
modeling. In The Eleventh International Conference on Learning Representations, 2023. Cited on pages 2, 3, 12,
and 35.
Guan-Horng Liu, Yaron Lipman, Maximilian Nickel, Brian Karrer, Evangelos A Theodorou, and Ricky T. Q. Chen.
Generalized schrodinger bridge matching. arXiv preprint arXiv:2310.02233, 2023a. Cited on page 12.
Qiang Liu. Rectified flow: A marginal preserving approach to optimal transport, 2022. Cited on page 3.
Xingchao Liu, Chengyue Gong, and qiang liu. Flow straight and fast: Learning to generate and transfer data with
rectified flow. In The Eleventh International Conference on Learning Representations, 2023b. Cited on pages 2
and 3.
Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In
Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431–3440, 2015. Cited on
page 12.
Dimitra Maoutsa, Sebastian Reich, and Manfred Opper. Interacting particle solutions of fokker–planck equations
through gradient–log–density estimation. Entropy, 22(8):802, 2020. Cited on page 3.
Shakir Mohamed, Mihaela Rosca, Michael Figurnov, and Andriy Mnih. Monte carlo gradient estimation in machine
learning. Journal of Machine Learning Research, 21(132):1–62, 2020. Cited on page 9.
Alexander Mordvintsev, Christopher Olah, and Mike Tyka. Inceptionism: Going deeper into neural networks. Google
research blog, 20(14):5, 2015. Cited on page 2.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens,
Amanda Askell, Peter Welinder, Paul F Christiano, Jan Leike, and Ryan Lowe. Training language models to
follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages
27730–27744. Curran Associates, Inc., 2022. Cited on pages 2, 6, and 12.
Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming
Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.
Advances in neural information processing systems, 32, 2019. Cited on page 9.
Ashwini Pokle, Matthew J Muckley, Ricky T. Q. Chen, and Brian Karrer. Training-free linear image inversion via
flows. arXiv preprint arXiv:2310.04432, 2023. Cited on page 12.
L.S. Pontryagin. The Mathematical Theory of Optimal Processes. Interscience Publishers, 1962. Cited on pages 8
and 9.
Aram-Alexandre Pooladian, Carles Domingo-Enrich, Ricky T. Q. Chen, and Brandon Amos. Neural optimal transport
with lagrangian costs. arXiv preprint arXiv:2406.00288, 2024. Cited on page 12.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct
preference optimization: Your language model is secretly a reward model. In Thirty-seventh Conference on Neural
Information Processing Systems, 2023. Cited on pages 6 and 12.
Konrad Rawlik, Marc Toussaint, and Sethu Vijayakumar. On stochastic optimal control and reinforcement learning by
approximate inference. In Twenty-Third International Joint Conference on Artificial Intelligence, 2013. Cited on
pages 6 and 9.
Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image
synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pages 10684–10695, 2022. Cited on pages 2 and 12.
Litu Rout, Yujia Chen, Nataniel Ruiz, Abhishek Kumar, Constantine Caramanis, Sanjay Shakkottai, and Wen-Sheng
Chu. Rb-modulation: Training-free personalization of diffusion models using stochastic optimal control. arXiv
preprint arXiv:2405.17401, 2024. Cited on page 12.
Reuven Y Rubinstein and Dirk P Kroese. The cross-entropy method: a unified approach to combinatorial optimization,
Monte-Carlo simulation and machine learning. Springer Science & Business Media, 2013. Cited on page 9.
Christoph Schuhmann and Romain Beaumont. Laion-aesthetics, 2022. Cited on page 2.
S.P. Sethi. Optimal Control Theory: Applications to Management Science and Economics. Springer International
Publishing, 2018. Cited on page 5.
18Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual,
OranGafni,etal. Make-a-video: Text-to-videogenerationwithouttext-videodata. arXivpreprintarXiv:2209.14792,
2022. Cited on page 2.
Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022. Cited on pages 3, 7,
and 30.
Jiaming Song, Arash Vahdat, Morteza Mardani, and Jan Kautz. Pseudoinverse-guided diffusion models for inverse
problems. In International Conference on Learning Representations, 2023. Cited on page 12.
Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. arXiv preprint
arXiv:1907.05600, 2019. Cited on page 2.
Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-
based generative modeling through stochastic differential equations. In International Conference on Learning
Representations (ICLR 2021), 2021. Cited on pages 2 and 3.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei,
andPaulFChristiano. Learningtosummarizewithhumanfeedback. InAdvances in Neural Information Processing
Systems, volume 33, pages 3008–3021. Curran Associates, Inc., 2020. Cited on pages 2 and 12.
Hyung Ju Suh, Max Simchowitz, Kaiqing Zhang, and Russ Tedrake. Do differentiable simulators give better policy
gradients? In International Conference on Machine Learning, pages 20668–20696. PMLR, 2022. Cited on page 9.
Wenpin Tang. Fine-tuning of diffusion models via stochastic control: entropy regularization and beyond, 2024. Cited
on page 6.
EmanuelTodorov. Linearly-solvablemarkov decisionproblems. Advances in neural information processing systems, 19,
2006. Cited on page 5.
Masatoshi Uehara, Yulai Zhao, Tommaso Biancalani, and Sergey Levine. Understanding reinforcement learning-based
fine-tuning of diffusion models: A tutorial and review, 2024a. Cited on page 2.
Masatoshi Uehara, Yulai Zhao, Kevin Black, Ehsan Hajiramezanali, Gabriele Scalia, Nathaniel Lee Diamant, Alex M
Tseng, Tommaso Biancalani, and Sergey Levine. Fine-tuning of continuous-time diffusion models as entropy-
regularized control, 2024b. Cited on pages 2, 6, 12, and 36.
Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation, 23(7):
1661–1674, 2011. Cited on page 12.
Apoorv Vyas, Bowen Shi, Matthew Le, Andros Tjandra, Yi-Chiao Wu, Baishan Guo, Jiemin Zhang, Xinyue Zhang,
Robert Adkins, William Ngan, et al. Audiobox: Unified audio generation with natural language prompts. arXiv
preprint arXiv:2312.15821, 2023. Cited on page 2.
BramWallace,MeihuaDang,RafaelRafailov,LinqiZhou,AaronLou,SenthilPurushwalkam,StefanoErmon,Caiming
Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization, 2023a. Cited
on pages 2, 14, 21, and 50.
Bram Wallace, Akash Gokul, Stefano Ermon, and Nikhil Naik. End-to-end diffusion latent optimization improves
classifier guidance, 2023b. Cited on page 12.
XiaoshiWu,YimingHao,KeqiangSun,YixiongChen,FengZhu,RuiZhao,andHongshengLi. Humanpreferencescore
v2: Asolidbenchmarkforevaluatinghumanpreferencesoftext-to-imagesynthesis. arXivpreprintarXiv:2306.09341,
2023a. Cited on pages 14 and 53.
Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference
score v2: A solid benchmark for evaluating human preferences of text-to-image synthesis, 2023b. Cited on page 2.
Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward:
Learning and evaluating human preferences for text-to-image generation. In Thirty-seventh Conference on Neural
Information Processing Systems, 2023. Cited on pages 2, 12, 14, 21, and 49.
QinshengZhangandYongxinChen. Pathintegralsampler: Astochasticcontrolapproachforsampling. InInternational
Conference on Learning Representations, 2022. Cited on page 12.
Wei Zhang, Han Wang, Carsten Hartmann, Marcus Weber, and Christof Schütte. Applications of the cross-entropy
method to importance sampling and optimal control of diffusions. SIAM Journal on Scientific Computing, 36(6):
A2654–A2672, 2014. Cited on page 9.
19QinqingZheng,MattLe,NetaShaul,YaronLipman,AdityaGrover,andRickyT.Q.Chen. Guidedflowsforgenerative
modeling and decision making. arXiv preprint arXiv:2311.13443, 2023. Cited on pages 2 and 15.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement
learning. In Aaai, volume 8, pages 1433–1438. Chicago, IL, USA, 2008. Cited on pages 5 and 36.
Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B. Brown, Alec Radford, Dario Amodei, Paul Christiano, and
Geoffrey Irving. Fine-tuning language models from human preferences, 2020. Cited on pages 2 and 12.
20Appendix
Contents
A Additional Figures & Tables 22
B Results on DDIM and Flow Matching 30
B.1 The continuous-time limit of DDIM. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
B.2 Forward and backward stochastic differential equations . . . . . . . . . . . . . . . . . . . . . . 30
B.2.1 Proof of Lemma 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
B.2.2 Proof of Lemma 2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
B.2.3 Proof of Proposition 4 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
B.3 The relationship between the noise predictor ϵ and the score function . . . . . . . . . . . . . . 35
B.4 The relationship between the vector field v and the score function. . . . . . . . . . . . . . . . 35
C Stochastic optimal control as maximum entropy RL in continuous space and time 36
C.1 Maximum entropy RL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
C.2 From maximum entropy RL to stochastic optimal control . . . . . . . . . . . . . . . . . . . . 37
C.3 Proof of Proposition 5: from MaxEnt RL to SOC . . . . . . . . . . . . . . . . . . . . . . . . . 38
C.4 Proof of equation (18): the control cost is a KL regularizer . . . . . . . . . . . . . . . . . . . 40
D Proofs of Section 4.3: memoryless noise schedule and fine-tuning recipe 41
D.1 Proof of Proposition 1: the memoryless noise schedule . . . . . . . . . . . . . . . . . . . . . . 41
D.2 Proof of Theorem 1: fine-tuning recipe for general noise schedules. . . . . . . . . . . . . . . . 42
E Loss function derivations 45
E.1 Derivation of the Continuous Adjoint method . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
E.2 Proof of Proposition 2: Theoretical guarantees of the basic Adjoint Matching loss . . . . . . . 47
E.3 Theoretical guarantees of the Adjoint Matching loss . . . . . . . . . . . . . . . . . . . . . . . 48
E.4 Pseudo-code of Adjoint Matching for DDIM fine-tuning . . . . . . . . . . . . . . . . . . . . . 49
F Adapting diffusion fine-tuning baselines to flow matching 49
F.1 Adapting ReFL (Xu et al., 2023) to flow matching . . . . . . . . . . . . . . . . . . . . . . . . 49
F.2 Adapting Diffusion-DPO (Wallace et al., 2023a) to flow matching . . . . . . . . . . . . . . . . 50
G Experimental details 51
G.1 Noise schedule details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
G.2 Selection of gradient evaluation timesteps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
G.3 Loss function clipping: the LCT hyperparameter . . . . . . . . . . . . . . . . . . . . . . . . . 52
G.4 Computation of evaluation metrics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
21A Additional Figures & Tables
Fine-tuning Fine-tuning Sampling ClipScore PickScore Total time (s) /
ImageReward↑
loss σ(t) σ(t) diversity↑ diversity↑ # iterations
√
None 2η −1.384 28.07 1.63
N/A t ±0.040 ±1.40 ±0.08 N/A
(CFG=1.0) 0 −0.920 30.29 1.82
±0.042 ±1.53 ±0.09
√ √
2η 2η 1.357 16.86 1.21 140k
DRaFT-1 t t ±0.039 ±0.98 ±0.07 ±5.9k
0 0 1.251 16.76 1.27 / 4000
±0.040 ±1.06 ±0.07
√ √
2η 2η −0.560 24.07 1.64 148k
DRaFT-40 t t ±0.138 ±1.37 ±0.12 ±4.2k
0 0 0.424 20.99 1.67 / 1500
±0.042 ±1.54 ±0.08
√ √
2η 2η −1.386 27.80 1.62 118k
DPO t t ±0.033 ±1.40 ±0.08 ±0.6k
0 0 −0.957 29.81 1.68 / 1000
±0.040 ±1.43 ±0.10
√ √
2η 2η 0.687 19.49 1.22 173k
ReFL t t ±0.085 ±1.76 ±0.08 ±10.9k
0 0 0.709 18.39 1.31 / 6000
±0.080 ±1.11 ±0.10
√
Cont. Adjoint √ 2η −0.448 26.97 1.82 153k
2η t ±0.135 ±1.37 ±0.09 ±0.9k
λ=12500 t 0 −0.249 26.25 1.90 / 750
±0.116 ±1.30 ±0.10
√
Disc. Adjoint √ 2η −0.557 30.40 1.91 152k
2η t ±0.113 ±2.39 ±0.09 ±1.5k
λ=12500 t 0 −0.552 28.37 1.97 / 1000
±0.041 ±2.26 ±0.09
√
Adj.-Matching √ 2η 0.550 23.00 1.65
2η t ±0.043 ±1.27 ±0.08
λ=1000 t 0 0.454 22.76 1.73
±0.055 ±1.40 ±0.09
√
Adj.-Matching √ 2η 0.755 21.33 1.55 156k
2η t ±0.040 ±1.71 ±0.08 ±1.9k
λ=2500 t 0 0.671 21.42 1.64 / 1000
±0.047 ±1.54 ±0.08
√
Adj.-Matching √ 2η 0.882 20.49 1.50
2η t ±0.058 ±1.48 ±0.09
λ=12500 t 0 0.778 20.34 1.57
±0.050 ±1.49 ±0.09
Table 3 Metrics for various fine-tuning methods for text-to-image generation. The second and third columns show the
√
noise schedules σ(t) used for fine-tuning and for inference: σ(t)= 2η corresponds to Memoryless Flow Matching,
t
and σ(t)=0 to the Flow Matching ODE (3). Confidence intervals show standard errors of estimates; computed over 3
runs of the fine-tuning algorithm on separate fine-tuning prompt datasets of size 40000 each. Test prompt sets are of
size 1000, and also different for each run.
Text prompt: “Man sitting on sofa at home in front of Text prompt: “3D World Food Day Morocco”
fireplace and using laptop computer, rear view”
Figure 6 Generated samples from varying classifier-free guidance weights, from the pre-trained Flow Matching model.
Corresponding samples from the fine-tuned model can be found in Figure 4.
22
0.0=
w
0.1=
w
0.4=
wFine-tuning #iter. Fine-tun. Sampl. DreamSim
w ImageReward↑ ClipScore↑ PickScore↑ HPSv2↑
loss /λ σ(t) σ(t) diversity↑
√
0.0 None N/A N/A
2ηt −1.384±0.040 24.15±0.26 17.25±0.06 16.19±0.17 53.60±1.37
0 −0.920±0.042 28.32±0.22 18.15±0.07 17.89±0.16 56.53±1.52
√ √
1000
2ηt 2ηt 0.913±0.068 29.80±0.22 19.16±0.06 23.63±0.16 35.21±1.93
0 0 0.626±0.195 30.48±0.32 18.91±0.34 21.92±1.63 38.52±2.01
√ √
0.0 DRaFT-1 2000
2ηt 2ηt 1.204±0.046 29.90±0.43 19.29±0.12 24.40±0.27 28.51±1.68
0 0 1.052±0.088 30.65±0.24 19.27±0.11 23.81±0.44 32.11±2.37
√ √
3000
2ηt 2ηt 1.307±0.041 29.96±0.22 19.31±0.06 24.42±0.13 26.57±1.32
0 0 1.173±0.058 30.86±0.25 19.37±0.06 24.17±0.23 29.69±1.30
√ √
4000
2ηt 2ηt 1.357±0.039 30.18±0.24 19.38±0.08 24.61±0.17 25.54±0.99
0 0 1.251±0.040 30.95±0.28 19.37±0.06 24.37±0.17 27.39±1.14
√ √
1000
2ηt 2ηt 0.550±0.043 30.36±0.22 19.29±0.08 24.12±0.17 40.89±1.50
0 0 0.454±0.055 31.41±0.22 19.57±0.09 23.29±0.18 43.10±1.76
√ √
0.0 Adj.-Match. 2500
2ηt 2ηt 0.755±0.040 30.59±0.40 19.49±0.10 24.85±0.23 37.07±1.47
0 0 0.671±0.047 31.64±0.21 19.71±0.09 24.12±0.27 39.88±1.59
√ √
12500
2ηt 2ηt 0.882±0.058 30.62±0.30 19.50±0.09 24.95±0.28 34.50±1.33
0 0 0.778±0.050 31.65±0.19 19.76±0.08 24.49±0.27 37.24±1.57
√
1.0 None N/A N/A
2ηt −0.269±0.050 30.41±0.22 18.74±0.07 20.47±0.18 43.82±1.24
0 −0.123±0.041 31.83±0.17 19.28±0.07 20.95±0.16 42.59±1.23
√ √
1000
2ηt 2ηt 1.123±0.051 32.06±0.19 19.69±0.06 24.56±0.17 28.25±1.55
0 0 0.856±0.167 32.32±0.25 19.38±0.34 22.88±1.54 29.98±1.86
1.0 DRaFT-1 2000 0 0 1.177±0.053 32.36±0.18 19.67±0.08 24.48±0.28 25.09±1.82
3000 0 0 1.255±0.038 32.36±0.19 19.70±0.06 24.64±0.17 23.24±1.19
4000 0 0 1.296±0.033 32.30±0.19 19.68±0.06 24.71±0.14 21.54±0.96
1000 0 0 0.782±0.044 33.05±0.22 20.20±0.09 24.81±0.18 32.67±1.26
√ √
1.0 Adj.-Match. 2500
2ηt 2ηt 1.027±0.038 32.85±0.21 20.08±0.08 25.88±0.20 29.83±1.00
0 0 0.910±0.040 33.20±0.17 20.29±0.09 25.39±0.24 30.34±1.51
12500 0 0 0.985±0.041 33.10±0.18 20.28±0.08 25.61±0.27 28.86±1.37
√
4.0 None N/A N/A
2ηt 0.277±0.043 32.68±0.18 19.50±0.07 22.29±0.16 35.12±0.92
0 0.209±0.046 32.83±0.17 19.79±0.07 22.30±0.17 32.05±1.05
√ √
1000
2ηt 2ηt 1.062±0.045 32.29±0.16 19.48±0.06 23.67±0.13 25.03±1.32
0 0 0.604±0.395 31.80±0.86 19.09±0.53 21.69±2.10 25.92±2.57
4.0 DRaFT-1 2000 0 0 1.112±0.046 32.29±0.20 19.34±0.11 23.31±0.22 21.02±1.67
3000 0 0 1.151±0.036 32.31±0.21 19.36±0.06 23.29±0.14 19.53±1.24
4000 0 0 1.172±0.040 32.20±0.22 19.30±0.07 23.20±0.15 18.45±1.06
1000 0 0 0.852±0.046 33.50±0.22 20.31±0.08 24.97±0.19 25.83±0.82
√ √
4.0 Adj.-Match. 2500
2ηt 2ηt 1.052±0.039 33.51±0.19 20.15±0.07 25.56±0.18 26.21±0.73
0 0 0.942±0.042 33.61±0.19 20.35±0.08 25.34±0.21 24.30±0.86
12500 0 0 1.007±0.052 33.48±0.20 20.29±0.08 25.50±0.29 23.48±0.81
Table 4 Evaluation metrics when using classifier-free guidance (CFG; Ho and Salimans (2022)).
LR/ Fine-tuning Fine-tun. Generat. DreamSim
ImageReward↑ ClipScore↑ PickScore↑ HPSv2↑
Adamβ1 loss σ(t) σ(t) diversity↑
√ √
3×10−5 DRaFT-1 2ηt 2ηt 1.467±0.029 30.28±0.56 19.37±0.09 24.70±0.15 21.20±0.93
/0.97 Adj.-Match. √ √
λ=12500
2ηt 2ηt 1.130±0.034 31.01±0.27 19.60±0.08 25.01±0.25 26.73±0.88
√ √
2×10−5 Disc. Adj. 2ηt 2ηt −1.186±0.553 21.95±4.29 16.94±0.95 12.34±4.40 28.33±10.26
/0.95 λ=12500 0 0 −0.961±0.653 24.07±4.71 17.86±1.17 15.93±5.80 33.62±7.80
Table 5 Metrics for alternative optimization hyperparameters (learning rate and Adam β ).
1
23Fine-tuning Fine-tuning Generative DreamSim
ImageReward↑ ClipScore↑ PickScore↑ HPSv2↑
loss σ(t) σ(t) diversity↑
Adj.-Matching
1
1 0.009±0.077 29.18±0.51 18.66±0.09 20.75±0.32 41.33±1.24
λ=12500 0 0.454±0.055 31.41±0.22 19.57±0.09 23.29±0.18 43.10±1.76
√
A λd =j.- 1M 25a 0t 0ching √ 2ηt 2 0ηt 00 .. 78 78 82± ±0 0. .0 05 58
0
3 30 1. .6 62 5± ±0 0. .3 10
9
1 19 9. .5 70 6± ±0 0. .0 09
8
2 24 4. .9 45 9± ±0 0. .2 28
7
3 34 7. .5 20 4± ±1 1. .3 53
7
Table 6 Comparison with an alternative fine-tuning noise schedule σ(t)=1. We see that the initial value function
bias (Section 4.2) results in the model not having a high reward function (ImageReward is the reward function used
for fine-tuning). Its performance on other metrics are also lower than when fine-tuning with the memoryless noise
schedule, except for diversity.
#sampl. Fine-tuning Fine-tun. Sampl. DreamSim
ImageReward↑ ClipScore↑ PickScore↑ HPSv2↑
timesteps loss σ(t) σ(t) diversity↑
√
None(Base) N/A
2ηt −2.279±0.001 13.99±0.12 14.98±0.05 7.37±0.10 5.07±0.13
0 −1.386±0.040 26.26±0.24 17.64±0.07 14.92±0.17 51.26±1.38
√ √
10
DRaFT-1
2ηt 2ηt 1.033±0.051 25.98±0.25 18.28±0.07 22.08±0.18 14.47±0.67
0 0 1.236±0.038 31.54±0.27 19.53±0.07 24.47±0.19 24.78±0.88
√
λAd =j.- 1M 25a 0t 0ch. √ 2ηt 2 0ηt − 02 .6.1 00 74 ±± 00 .0.0 57 54 1 37 1. .1 32 6± ±0 0. .5 26
0
1 15 9. .7 56 6± ±0 0. .2 00
8
1 21 3. .4 28 3± ±1 0. .0 23
8
39 3. .8 78 5± ±0 1.8 .41
8
√
None(Base) N/A
2ηt −2.275±0.002 14.58±0.13 15.07±0.05 7.47±0.10 11.27±0.33
0 −1.017±0.055 27.92±0.19 18.01±0.07 17.17±0.15 54.69±1.45
√ √
20
DRaFT-1
2ηt 2ηt 1.301±0.039 27.09±0.24 18.93±0.07 23.78±0.20 21.05±1.12
0 0 1.255±0.038 31.14±0.25 19.43±0.06 24.52±0.16 26.15±1.11
√
A λd =j.- 1M 25a 0t 0ch. √ 2ηt 2 0ηt − 00 .7.0 63 82 ±± 00 .0.0 47 82 2 35 1. .0 77 0± ±0 0. .2 17
7
1 18 9. .0 71 3± ±0 0. .0 07
8
2 20 4. .7 35 0± ±0 0. .2 23
6
2 39 5. .0 96 0± ±2 1. .3 54
2
√
None(Base) N/A
2ηt −1.384±0.040 24.15±0.26 17.25±0.06 16.19±0.17 53.60±1.37
0 −0.920±0.042 28.32±0.22 18.15±0.07 17.89±0.16 56.53±1.52
√ √
40
DRaFT-1
2ηt 2ηt 1.357±0.039 30.18±0.24 19.38±0.08 24.61±0.17 25.54±0.99
0 0 1.251±0.040 30.95±0.28 19.37±0.06 24.37±0.17 27.39±1.14
√
A λd =j.- 1M 25a 0t 0ch. √ 2ηt 2 0ηt 0 0. .8 78 72 8± ±0 0. .0 05 58
0
3 30 1. .6 62 5± ±0 0. .3 10
9
1 19 9. .5 70 6± ±0 0. .0 09
8
22 44 .. 49 95 ±± 00 .. 22 78 3 34 7. .5 20 4± ±1 1. .3 53
7
√
None(Base) N/A
2ηt −0.881±0.041 27.83±0.19 18.10±0.07 18.43±0.17 57.21±1.50
0 −0.881±0.036 28.65±0.18 18.22±0.06 18.20±0.17 57.73±1.68
√ √
100
DRaFT-1
2ηt 2ηt 1.343±0.040 30.64±0.20 19.38±0.08 24.37±0.17 25.51±1.10
0 0 1.239±0.037 30.74±0.28 19.33±0.06 24.24±0.17 28.70±1.11
√
A λd =j.- 1M 25a 0t 0ch. √ 2ηt 2 0ηt 0 0. .8 79 72 9± ±0 0. .0 04 44
8
3 31 1. .2 63 4± ±0 0. .2 13
7
1 19 9. .6 75 6± ±0 0. .0 08
8
22 44 .. 59 72 ±± 00 .. 22 53 3 35 8. .1 23 6± ±1 1. .4 60
5
√
None(Base) N/A
2ηt −0.848±0.048 28.37±0.21 18.27±0.08 18.56±0.19 58.00±1.58
0 −0.871±0.036 28.50±0.18 18.23±0.06 18.25±0.14 57.84±1.60
√ √
200
DRaFT-1
2ηt 2ηt 1.331±0.044 30.69±0.23 19.36±0.07 24.21±0.17 26.41±1.18
0 0 1.222±0.042 30.77±0.27 19.32±0.06 24.18±0.16 29.09±1.07
√
A λd =j.- 1M 25a 0t 0ch. √ 2ηt 2 0ηt 0 0. .8 76 69 6± ±0 0. .0 06 52
0
3 31 1. .3 63 1± ±0 0. .2 11
6
1 19 9. .6 78 5± ±0 0. .0 09
8
22 44 .. 58 21 ±± 00 .. 23 40 3 35 8. .9 60 0± ±1 1. .5 35
8
Table 7 Performance metrics for different number of sampling steps. Only the number of sampling steps is ablated;
the fine-tuned models used in all cases are the ones fine-tuned using 40 steps.
24Fine-tun. Fine-tun. Generat. DreamSim Runtime/
ImageReward↑ ClipScore↑ PickScore↑ HPSv2↑
loss σ(t) σ(t) diversity↑ #iter.
√ √
ReFL
2ηt 2ηt 0.459±0.096 28.46±0.25 18.77±0.09 22.54±0.17 37.51±3.50 43k±2.7k
0 0 0.330±0.114 29.63±0.61 19.08±0.18 22.46±0.77 39.51±1.30 /1500
√ √
DRaFT-1
2ηt 2ηt 0.913±0.068 29.80±0.22 19.16±0.06 23.63±0.16 35.21±1.93 35k±1.5k
0 0 0.626±0.195 30.48±0.32 18.91±0.34 21.92±1.63 38.52±2.01 /1000
√ √
Draft-40
2ηt 2ηt −1.427±0.267 23.39±1.72 17.24±0.45 15.72±1.80 41.98±2.14 49k±1.4k
0 0 −0.097±0.052 29.12±0.41 18.97±0.14 21.93±0.20 46.35±1.34 /500
√
λAd =j.- 1M 00a 0tch. √ 2ηt 2 0ηt 0 0. .1 00 57 1± ±0 0. .0 04 46
4
2 39 0. .3 57 8± ±0 0. .2 15
7
1 19 9. .0 35 1± ±0 0. .0 07
7
2 22 1. .7 99 3± ±00 .. 22 30 4 46 8. .3 18 2± ±1 1. .3 56
6
√
A λd =j.- 2M 50a 0tch. √ 2ηt 2 0ηt 0 0. .1 19 09 6± ±0 0. .0 06 68
7
2 39 0. .2 47 3± ±0 0. .2 21
4
1 19 9. .0 37 2± ±0 0. .1 10
1
2 22 2. .9 18 6± ±00 .. 33 30 4 45 7. .0 63 1± ±1 1. .6 41
9
39 /k 2± 50 0.5k
√
λAd =j.- 1M 25a 0t 0ch. √ 2ηt 2 0ηt 0 0. .2 29 29 4± ±0 0. .0 09 55
1
2 39 0. .6 71 0± ±0 0. .3 27
3
1 19 9. .2 56 2± ±0 0. .1 14
1
2 23 2. .6 97 3± ±00 .. 22 17 4 43 4. .3 66 2± ±1 1. .9 73
9
√
C λo =nt 1. 2A 50d 0j. √ 2ηt 2 0ηt − −0 0. .9 61 80 1± ±0 0. .1 01 56
1
2 26 8. .2 59 0± ±0 0. .4 14
9
1 18 8. .0 66 9± ±0 0. .1 16
1
1 18 9. .8 96 0± ±00 .. 58 08 5 51 0. .6 80 7± ±1 1. .9 57
2
51 /k 2± 50 0.3k
√
D λi =sc. 12A 5d 0j 0. √ 2ηt 2 0ηt − −0 0. .9 77 98 1± ±0 0. .1 02 63
5
2 26 8. .6 68 6± ±0 0. .7 36
3
1 18 8. .5 51 1± ±0 0. .1 11
1
1 18 8. .5 53 3± ±00 .. 22 88 5 55 4. .9 75 8± ±1 2. .7 00
0
38 /k 2± 50 0.4k
Table 8 Additional metrics for various fine-tuning methods for text-to-image generation, which complement the ones
in Table 2 (both tables correspond to the same runs). The second and third columns show the noise schedules σ(t)
√
used for fine-tuning and for inference: σ(t)= 2η corresponds to Memoryless Flow Matching, and σ(t)=0 to the
t
Flow Matching ODE (3).
25Base Flow Matching model Adjoint Matching (Ours) DRaFT-1
Figure 7 Generated samples with CFG=1.0 and σ(t)=0 across ten selected prompts. Each row corresponds to a
different prompt and each image corresponds to a different random seed consistent across models.
26Base Flow Matching model Adjoint Matching (Ours) DRaFT-1
Figure8 GeneratedsampleswithCFG=1.0andσ(t)=0acrosstenselectedpromptswithpeople. Eachrowcorresponds
to a different prompt and each image corresponds to a different random seed consistent across models.
27Figure 9 Generated samples with CFG=0.0 and σ(t)=0 across seven selected prompts. Each row corresponds to a
different finetuning algorithm. Prompts: “Seaside view poster with palm trees vector image”, “Cayucos Beach Inn”,
“Happy Summer Life- Aloha Flowers and Melon - Pattern Metal Print”, “Castle Square, Warsaw Old Town”, “Funny
girl blowing soap bubbles. High quality photo”, “Colombian man with sweatshirt over yellow wall listening to something
by putting hand on the ear”, “man in the hood black mask masquerade”.
28
)esaB(enoN
1-TFaRD
04-TFaRD
LFeR
.jdA
.tnoC
.jdA
.csiD
.hctam
.jdA
.hctam
.jdA
.hctam
.jdA
00521=λ
00521=λ
0001=λ
0052=λ
00521=λ√
Figure 10 GeneratedsampleswithCFG=0.0andσ(t)= 2η acrosssevenselectedprompts. Eachrowcorrespondsto
t
a different finetuning algorithm. The prompts are the same as in Figure 9.
29
)esaB(enoN
1-TFaRD
04-TFaRD
LFeR
.jdA
.tnoC
.jdA
.csiD
.hctam
.jdA
.hctam
.jdA
.hctam
.jdA
00521=λ
00521=λ
0001=λ
0052=λ
00521=λB Results on DDIM and Flow Matching
B.1 The continuous-time limit of DDIM
The DDIM inference update (Song et al., 2022, Eq. 12) is
x
k+1
=√ α¯ k+1(cid:0)xk−√ 1 √− αα ¯¯ kkϵ(xk,k)(cid:1) +(cid:112) 1−α¯ k+1−σ k2ϵ(x k,k)+σ kϵ k, x
K
∼N(0,I). (44)
If we let ∆α¯ =α¯ −α¯ , we have that
k k+1 k
(cid:113) (cid:113) (cid:113) (cid:113)
α¯k+1 = α¯k+α¯k+1−α¯k = 1+ α¯k+1−α¯k = 1+ ∆α¯k ≈1+ ∆α¯k, (45)
α¯k α¯k α¯k α¯k 2α¯k
√
where we used the first-order Taylor approximation of 1+x. And
−(cid:113) α¯ α¯k+ k1(1−α¯ k)+(cid:112) 1−α¯ k+1−σ k2 =−(cid:113) (cid:0) 1+ ∆ α¯α¯ kk(cid:1) (1−α¯ k)+(cid:112) 1−α¯ k+1−σ k2
=−(cid:113) 1+ ∆ α¯α¯ kk −α¯ k−∆α¯ k+(cid:112) 1−α¯ k+1−σ k2 =−(cid:113) 1−α¯ k+1+ ∆ α¯α¯ kk +(cid:112) 1−α¯ k+1−σ k2
=√ 1−α¯ k+1(cid:0) −(cid:113) 1+ α¯k(1∆ −α¯ α¯k
k+1)
+(cid:113) 1− 1−σ α¯k2 k+1(cid:1) ≈√ 1−α¯ k+1(cid:0) −(cid:0) 1+ 2α¯k(∆ 1−α¯ α¯k k+1)(cid:1) +1− 2(1−σ α¯k2 k+1)(cid:1)
=−(cid:0)∆α¯k + σ k2(cid:1)√ 1 ,
2α¯k 2 1−α¯k+1
(46)
where we used the same first-order Taylor approximation. Thus, up to first-order approximations, (44) is
equivalent to
x k−1 =(cid:0) 1+ ∆ 2α¯α¯ kk(cid:1) x k−(cid:0)∆ 2α¯α¯ kk + σ 2k2(cid:1)√ϵ 1(x −k α¯,k k+)
1
+σ kϵ k, x K ∼N(0,I). (47)
If we modify our notation slightly, we can rewrite this as
√
X (k+1)h =(cid:0) 1− h 2αα ¯¯˙ kk hh(cid:1) X kh+(cid:0)h 2αα ¯¯˙ kk hh − hσ( 2kh)2(cid:1)ϵ √(X 1k −h α¯,k kh h) + hσ(kh)ϵ k, X 0 ∼N(0,I). (48)
To go from (47) to (48), we introduced a continuous time variable and a stepsize h=1/K, and we regard the
√
increment hα¯ as approximately equal to h times the derivative of α¯. We also identified σ with hσ(kh),
k k
where σ(kh) plays the role of a diffusion coefficient. Note that equation (48) can be reverse-engineered as the
Euler-Maruyama discretization of the SDE
dX t =(cid:0) − 2α¯˙ α¯t
t
+(cid:0) 2α¯˙ α¯t
t
− σ( 2t)2(cid:1) √ϵ( 1X −t, α¯t) t(cid:1) dt+σ(t)dB t, X 0 ∼N(0,I). (49)
B.2 Forward and backward stochastic differential equations
Let (κ ) and (η ) such that
t t∈[0,1] t t∈[0,1]
∀t∈[0,1], η ≥0, (cid:82)1 κ ds=+∞, 2(cid:82)1 η exp(cid:0) −2(cid:82)t κ ds(cid:1) dt′ =1. (50)
t 0 1−s 0 1−t′ t′ 1−s
A
η
ts =sh βo tw (cid:0)n
αα˙
tti βn tT −a βb
˙
tle (cid:1).1, DDIM corresponds to κ t = 2α˙ αt t, η t = 2α˙ αt t, and Flow Matching corresponds to κ t = α α˙t t,
Lemma 1 (DDIM and Flow Matching fulfill the conditions (50)). The choices of (κ ) and (η ) for
t t∈[0,1] t t∈[0,1]
DDIM and Flow Matching fulfill the conditions (50). For DDIM, we have that
(cid:82)t κ ds=−1logα =⇒ (cid:82)1 κ ds=+∞,
0 1−s 2 1−t 0 1−s (51)
2(cid:82)t η exp(cid:0) −2(cid:82)t κ ds(cid:1) dt′ =1−α =⇒ 2(cid:82)1 η exp(cid:0) −2(cid:82)t κ ds(cid:1) dt′ =1.
0 t′ t′ s 1−t 0 t′ t′ s
For Flow Matching,
(cid:82)t κ ds=−logα =⇒ (cid:82)1 κ ds=+∞, (52)
0 1−s 1−t 0 1−s
2(cid:82)t η exp(cid:0) −2(cid:82)t κ ds(cid:1) dt′ =β2 =⇒ 2(cid:82)1 η exp(cid:0) −2(cid:82)t κ ds(cid:1) dt′ =1. (53)
0 t′ t′ s 1−t 0 t′ t′ s
30Forward and backward SDEs Consider the forward and backward SDEs
√
dX⃗ =−κ X⃗ dt+ 2η dB , X⃗ ∼p , (54)
t 1−t t 1−t t 0 data
√
dX =(cid:0) κ X +2η s(X ,t)(cid:1) dt+ 2η dB , X ∼N(0,I), (55)
t t t t t t t 0
where we let p⃗ be the density of X⃗ , and we define the score function as s(x,t):=∇logp⃗ (x). Similarly,
t t 1−t
we let p be the density of X . p⃗ and p solve the Fokker-Planck equations:
t t t t
∂ p⃗ =∇·(cid:0) κ xp⃗ (cid:1) +η ∆p⃗ , p⃗ =p , (56)
t t 1−t t 1−t t 0 data
∂ p =∇·(cid:0)(cid:0) −κ x−2η ∇logp⃗ (X )(cid:1) p (cid:1) +η ∆p , p =N(0,I). (57)
t t t t 1−t t t t t 0
Lemma 2 (Solution of the forward SDE). Let (κ ) , (η ) with η ≥ 0, and (ξ ) be arbitrary. The
t t≥0 t t≥0 t t t≥0
solution X⃗ of the SDE
t
√
dX⃗ =(cid:0) −κ X⃗ +ξ (cid:1) dt+ 2η dB , X⃗ ∼p (58)
t 1−t t t 1−t t 0 data
is
√
X⃗ =X⃗ exp(cid:0) −(cid:82)t κ ds(cid:1) +(cid:82)t exp(cid:0) −(cid:82)t κ ds(cid:1) ξ dt′+(cid:82)t 2η exp(cid:0) −(cid:82)t κ ds(cid:1) dB , (59)
t 0 0 1−s 0 t′ 1−s 1−t′ 0 1−t′ t′ 1−s t′
which has the same distribution as the random variable
(cid:113)
Xˆ =X⃗ exp(cid:0) −(cid:82)t κ ds(cid:1) +(cid:82)t exp(cid:0) −(cid:82)t κ ds(cid:1) ξ dt′+ 2(cid:82)t η exp(cid:0) −2(cid:82)t κ ds(cid:1) dt′ϵ,
t 0 0 1−s 0 t′ 1−s 1−t′ 0 1−t′ t′ 1−s (60)
ϵ∼N(0,I).
Applying Lemma 2 with ξ ≡0, we obtain that p⃗ is also the distribution of
t 1
(cid:113)
Xˆ =X⃗ exp(cid:0) −(cid:82)t κ ds(cid:1) + 2(cid:82)t η exp(cid:0) −2(cid:82)t κ ds(cid:1) dt′ϵ=ϵ, (61)
1 0 0 1−s 0 1−t′ t′ 1−s
where ϵ∼N(0,I). The third equality in (61) holds by (50). Hence we obtain that p⃗ =N(0,I). Note also
1
that
∂ p⃗ =−∇·(cid:0) κ xp⃗ (cid:1) −η ∆p⃗ =−∇·(cid:0)(cid:0) −κ x−2η ∇logp⃗ (x)(cid:1) p⃗ (cid:1) +η ∆p⃗ (62)
t 1−t t 1−t t 1−t t t 1−t 1−t t 1−t
Thus, p⃗ is a solution of the backward Fokker-Planck equation (57), which proves the following:
1−t
Proposition 3 (Equality of marginal distributions). For any time t∈[0,1], the densities of the solutions X⃗ ,
t
X of the forward and backward SDEs are equal up to a time flip: p =p⃗ .
t t 1−t
Forward and backward SDEs with arbitrary noise schedule Next, we look at the following pair of forward-
backward SDEs:
dX⃗ =(cid:0) −κ X⃗ +(cid:0)σ(1−t)2 −η (cid:1) s(X⃗ ,1−t)(cid:1) dt+σ(1−t)dB , X⃗ ∼p , (63)
t 1−t t 2 1−t t t 0 data
dX =(cid:0) κ X +(cid:0)σ(t)2 +η (cid:1) s(X ,t)(cid:1) dt+σ(t)dB , X ∼N(0,I), (64)
t t t 2 t t t 0
Here, the score function s is the same vector field as in (64). Remark that equations (54)-(55) are a particular
√
case of (63)-(64) for which σ(t)= 2η . The Fokker-Planck equations for (63)-(64) are:
t
∂ p⃗ =∇·(cid:0)(cid:0) κ x+(cid:0) − σ(1−t)2 +η (cid:1) s(X ,t)(cid:1) p⃗ (cid:1) +η ∆p⃗ , p⃗ =p , (65)
t t 1−t 2 1−t t t 1−t t 0 data
∂ p =∇·(cid:0)(cid:0) −κ x−(cid:0)σ(t)2 +η (cid:1) s(X ,t)(cid:1) p (cid:1) + σ(t)2 ∆p , p =N(0,I). (66)
t t t 2 t t t 2 t 0
It is straight-forward to see that for any σ, the solutions p⃗ and p of (65)-(66) are also solutions of (56)-(57).
t t
Hence, the marginals X⃗ and X are equally distributed for all noise schedules σ, and they are equal to each
t t
other up to a time flip.
31Equality of distributions over trajectories The result in Proposition 3 can be made even stronger:
Proposition 4 (Equality of distributions over trajectories). Let X⃗, X be the solutions of the SDEs (63)-(64)
with arbitrary noise schedule. For any sequence of times (t ) , the joint distribution of (X⃗ ) is equal
i 0≤i≤I ti 0≤i≤I
to the joint distribution of (X ) , or equivalently, that the probability measures ⃗P, P of the forward and
1−ti 0≤i≤I
backward processes X⃗, X are equal, up to a flip in the time direction.
This result states that sampling trajectories from the backward process is equivalent to sampling them from
the forward process and then flipping their order.
B.2.1 Proof of Lemma 1
As shown in Table 1, DDIM corresponds to κ t = 2α˙ αt t, η t = 2α˙ αt t. Thus, η t ≥0 because α t is increasing, and
(cid:82)t κ ds=(cid:82)t α˙1−s ds=−1(cid:82)t ∂ logα ds=−1(logα −logα )=−1logα ,
0 1−s 0 2α1−s 2 0 s 1−s 2 1−t 1 2 1−t (67)
=⇒ (cid:82)1 κ ds=−1logα =+∞
0 1−s 2 0
2(cid:82)t η exp(cid:0) −2(cid:82)t κ ds(cid:1) dt′ =(cid:82)t α˙ 1−t′ exp(cid:0) −(cid:82)t α˙1−s ds(cid:1) dt′
0 t′ t′ s 0 α 1−t′ t′ α1−s
=(cid:82)t α˙ 1−t′ α1−t dt′ =α (cid:82)t ∂ (cid:0) 1 (cid:1) dt′ =α (cid:0) 1 − 1 (cid:1) =1−α , (68)
0 α 1−t′ α 1−t′ 1−t 0 t′ α 1−t′ 1−t α1−t α1 1−t
=⇒ 2(cid:82)1 η exp(cid:0) −2(cid:82)t κ ds(cid:1) dt′ =1−α =1.
0 t′ t′ s 0
w hah ve ere thw ae tu ηse ≥d t 0ha bt ecα
a1
us= e1 αan isd inα
c0
r= eas0 i. ngAn and dF βlow isM da ectc rh eain sig ngco ,r ar nes dponds to κ
t
= α α˙ tt, η
t
=β t(cid:0)α α˙t tβ t−β˙ t(cid:1). We
t t t
(cid:82)t κ ds=(cid:82)t α˙1−s ds=−(cid:82)t ∂ logα ds=−(logα −logα )=−logα ,
0 1−s 0 α1−s 0 s 1−s 1−t 1 1−t (69)
(cid:82)1
=⇒ κ ds=−logα =+∞,
0 1−s 0
and
2(cid:82)t η exp(cid:0) −2(cid:82)t κ ds(cid:1) dt′ =2(cid:82)t β (cid:0)α˙ 1−t′β −β˙ (cid:1) exp(cid:0) −2(cid:82)t α˙1−s ds(cid:1) dt′
0 1−t′ t′ 1−s 0 1−t′ α 1−t′ 1−t′ 1−t′ t′ α1−s (70)
=2(cid:82)t β (cid:0)α˙ 1−t′β −β˙ (cid:1)(cid:0)α1−t (cid:1)2 dt′,
0 1−t′ α 1−t′ 1−t′ 1−t′ α 1−t′
To develop the right-hand side, note that by integration by parts,
(cid:82)t β˙ β (cid:0)α1−t (cid:1)2 dt′ =−(cid:82)t ∂ (cid:0)β 12 −t′(cid:1)(cid:0)α1−t (cid:1)2 dt′
0 1−t′ 1−t′ α 1−t′ 0 t′ 2 α 1−t′ (71)
=−(cid:2)β 12 −t′(cid:0)α1−t (cid:1)2(cid:3)1 +(cid:82)t β 12 −t′∂ (cid:0)α1−t (cid:1)2 dt′ =−(cid:2)β 12 −t′(cid:0)α1−t (cid:1)2(cid:3)t +(cid:82)t β2 α2 1−tα˙ 1−t′ dt′.
2 α 1−t′ 0 0 2 t′ α 1−t′ 2 α 1−t′ 0 0 1−t′ α3 1−t′
And if we plug this into the right-hand side of (70), we obtain
2(cid:82)t η exp(cid:0) −2(cid:82)t κ ds(cid:1) dt′ =(cid:2) β2 (cid:0)α1−t (cid:1)2(cid:3)t =β2 −β2(cid:0)α1−t(cid:1)2 =β2 , (72)
0 1−t′ t′ 1−s 1−t′ α 1−t′ 0 1−t 1 α1 1−t
=⇒ 2(cid:82)1 η exp(cid:0) −2(cid:82)t κ ds(cid:1) dt′ =β2 =1. (73)
0 1−t′ t′ 1−s 1
where we used that β =0, α =1.
1 1
B.2.2 Proof of Lemma 2
We can solve this equation by variation of parameters. To simplify the notation, we replace κ , η and
1−s 1−s
ξ by κ , η and ξ . Defining f(X⃗ ,t)=X⃗ exp(cid:0)(cid:82)t κ ds(cid:1), we get that
1−s s s s t t 0 1−s
df(X⃗ ,t)=κ X⃗ exp(cid:0)(cid:82)t κ ds(cid:1) dt+exp(cid:0)(cid:82)t κ ds(cid:1) dX⃗
t 1−t t 0 1−s 0 1−s t
√
=κ X⃗ exp(cid:0)(cid:82)t κ ds(cid:1) dt+exp(cid:0)(cid:82)t κ ds(cid:1)(cid:0) (−κ X⃗ +ξ )dt+ 2η dB (cid:1) (74)
1−t t 0 1−s 0 1−s 1−t t 1−t 1−t t
√
(cid:0)(cid:82)t (cid:1) (cid:0)(cid:82)t (cid:1)
=exp κ ds ξ dt+ 2η exp κ ds dB .
0 1−s 1−t t 0 1−s t
32Integrating from 0 to t, we get that
X⃗ exp(cid:0)(cid:82)t κ ds(cid:1) =X⃗ +(cid:82)t exp(cid:0)(cid:82)t′ κ ds(cid:1) ξ dt′+(cid:82)t√ 2η exp(cid:0)(cid:82)t′ κ ds(cid:1) dB , (75)
t 0 1−s 0 0 0 1−s 1−t′ 0 1−t′ 0 1−s t′
√
⇐⇒ X⃗ =X⃗ exp(cid:0) −(cid:82)t κ ds(cid:1) +(cid:82)t exp(cid:0) −(cid:82)t κ ds(cid:1) ξ dt′+(cid:82)t 2η exp(cid:0) −(cid:82)t κ ds(cid:1) dB .
t 0 0 1−s 0 t′ 1−s 1−t′ 0 1−t′ t′ 1−s t′
(76)
Since
√
E(cid:2)(cid:0)(cid:82)t 2η exp(cid:0) −(cid:82)t κ ds(cid:1) dB (cid:1)2(cid:3) =2(cid:82)t η exp(cid:0) −2(cid:82)t κ ds(cid:1) dt′, (77)
0 1−t′ t′ 1−s t′ 0 1−t′ t′ 1−s
√ (cid:113)
weobtainthat(cid:82)t 2η exp(cid:0) −(cid:82)t κ ds(cid:1) dB hasthesamedistributionas 2(cid:82)t η exp(cid:0) −2(cid:82)t κ ds(cid:1) dt′ϵ,
0 1−t′ t′ 1−s t′ 0 1−t′ t′ 1−s
where ϵ∼N(0,1).
B.2.3 Proof of Proposition 4
This is a result that has been used by previous works, e.g. (De Bortoli et al., 2021, Sec. 2.1), but their
derivation lacks rigor as it uses some unexplained approximations. While natural, the result is not common
knowledge in the area. We provide a derivation which is still in discrete time, and hence not completely
formal, but that corrects the gaps in the proof of De Bortoli et al. (2021).
We introduce the short-hand
⃗b(x,t)=−κ x+(cid:0)σ(1−t)2 −η (cid:1) s(x,1−t), (78)
1−t 2 1−t
b(x,t)=κ X +(cid:0)σ(t)2 +η (cid:1) s(X ,t), (79)
t t 2 t t
⃗σ(t)=σ(1−t). (80)
Remark that b(x,t)=−⃗b(x,1−t)+σ(t)2s(X ,t).
t
Suppose that we discretize the forward process X⃗ using K+1 equispaced timesteps:
√
x =x +h⃗b(x ,kh)+ h⃗σ(kh)ϵ , with ϵ ∼N(0,1). (81)
k+1 k k k k
It is important to remark that x −x =O(h1/2). Throughout the proof we will keep track of all terms up
k+1 k
to linear order in h, while neglecting terms of order O(h3/2) and higher. The distribution of the discretized
forward process is:
p⃗(x )=p⃗ (x )(cid:81)K−1p⃗ (x |x ), where p⃗ (x |x )=
exp(cid:0) −∥xk+1− 2x hk ⃗σ− (kh h⃗b( )x 2k,kh)∥2(cid:1)
(82)
0:K 0 0 k=0 k+1|k k+1 k k+1|k k+1 k (2πh⃗σ(kh)2)d/2
Using telescoping products, we have that
p⃗(x )=p⃗ (x )(cid:81)K−1p⃗ (x |x ) p⃗k(xk)
0:K K K k=0 k+1|k k+1 k p⃗k+1(xk+1) (83)
=p⃗ (x
)(cid:81)K−1p⃗
(x |x
)exp(cid:0)
log(p⃗ (x ))−log(p⃗ (x
))(cid:1)
K K k=0 k+1|k k+1 k k k k+1 k+1
We can use a discrete time version of Ito’s lemma:
logp⃗(x ,(k+1)h)≈logp⃗(x ,kh)+h(cid:0) ∂ logp⃗(x ,kh)+ ⃗σ(kh)2 ∆logp⃗(x ,kh)(cid:1) (84)
k+1 k t k 2 k
+⟨∇logp⃗(x ,kh),x −x ⟩+O(h3/2). (85)
k k+1 k
Using equation (81) and a Taylor approximation, observe that
⟨∇logp(x ,kh),x −x ⟩
k k+1 k
=⟨∇logp(x ,(k+1)h)−∇2logp(x ,(k+1)h)(x −x ),x −x ⟩+O(h3/2)
k+1 k+1 k+1 k k+1 k
=⟨∇logp(x ,(k+1)h),x −x ⟩ (86)
k+1 k+1 k
√ √
−⟨h⃗b(x ,kh)+ h⃗σ(kh)ϵ ,∇2logp(x ,(k+1)h)(cid:0) h⃗b(x ,kh)+ h⃗σ(kh)ϵ (cid:1) ⟩+O(h3/2)
k k k+1 k k
=⟨∇logp(x ,(k+1)h),x −x ⟩−h⃗σ(kh)2∆logp(x ,(k+1)h)+O(h3/2).
k+1 k+1 k k+1
33And since p⃗ satisfies the Fokker-Planck equation
∂ p⃗ =∇·(cid:0) (−⃗b(x,t)+ ⃗σ(t)2 ∇logp⃗ (x))p⃗ (cid:1) , (87)
t t 2 t t
we have that
∂ logp⃗ = ∂tp⃗t =
∇·(cid:0) (−⃗b(x,t)+⃗σ( 2t)2 ∇logp⃗t(x))p⃗t(cid:1)
t t p⃗t p⃗t (88)
=−∇·⃗b(x,t)+ ⃗σ(t)2 ∆logp⃗ (x)+⟨−⃗b(x,t)+ ⃗σ(t)2 ∇logp⃗ (x),∇logp⃗ (x)⟩.
2 t 2 t t
Hence,
∂ logp(x ,kh)=∂ logp(x ,(k+1)h)+O(h1/2)
t k t k+1
=−∇·⃗b(x ,(k+1)h)+ ⃗σ((k+1)h)2 ∆logp⃗(x ,(k+1)h) (89)
k+1 2 k+1
+⟨−⃗b(x ,(k+1)h)+ ⃗σ((k+1)h)2 ∇logp⃗(x ,(k+1)h),∇logp⃗(x ,(k+1)h)⟩+O(h1/2).
k+1 2 k+1 k+1
If we plug (86) and (89) into (84), we obtain
logp(x ,(k+1)h)−logp(x ,kh)
k+1 k
=h(cid:0) −∇·⃗b(x ,(k+1)h)+⟨−⃗b(x ,(k+1)h)+⃗σ((k+1)h)2 ∇logp⃗(x ,(k+1)h),∇logp⃗(x ,(k+1)h)⟩(cid:1)
k+1 k+1 2 k+1 k+1
+⟨∇logp(x ,(k+1)h),x −x ⟩+O(h3/2)
k+1 k+1 k
=
⟨2h⃗σ(kh)2∇logp(xk+1,(k+1)h),xk+1−xk−h⃗b(xk+1,(k+1)h)⟩
2h⃗σ(kh)2
+h(cid:0) −∇·⃗b(x ,(k+1)h)+ ⃗σ((k+1)h)2 ∥∇logp⃗(x ,(k+1)h)∥2(cid:1) +O(h3/2).
k+1 2 k+1
(90)
Applying a discrete time version of Ito’s lemma again, we have that
⃗b(x ,kh)=⃗b(x ,(k+1)h)−h(cid:0) ∂⃗b(x ,(k+1)h)+ ⃗σ((k+1)h)2 ∆⃗b(x ,(k+1)h)(cid:1)
k k+1 t k+1 2 k+1
+∇⃗b(x ,(k+1)h)⊤(x −x )+O(h3/2) (91)
k+1 k k+1
=⃗b(x ,(k+1)h)+∇⃗b(x ,(k+1)h)⊤(x −x )+O(h).
k+1 k+1 k k+1
where ∆⃗b denotes the component-wise Laplacian of⃗b. Thus,
logp⃗ (x |x )
k+1|k k+1 k
=−dlog(cid:0) 2πh⃗σ(kh)2(cid:1)
−
∥xk+1−xk−h⃗b(xk,kh)∥2
2 2h⃗σ(kh)2
=−dlog(cid:0) 2πh⃗σ(kh)2(cid:1) − ∥xk+1−xk−h(⃗b(xk+1,(k+1)h)+∇⃗b(xk+1,(k+1)h)⊤(xk−xk+1))∥2 +O(h3/2)
2 2h⃗σ(kh)2
(92)
=−dlog(cid:0) 2πh⃗σ(kh)2(cid:1) − ∥xk+1−xk−h⃗b(xk+1,(k+1)h)∥2 + ⟨xk+1−xk,∇⃗b(xk+1,(k+1)h)⊤(xk−xk+1)⟩ +O(h3/2)
2 2h⃗σ(kh)2 ⃗σ(kh)2
=−dlog(cid:0) 2πh⃗σ(kh)2(cid:1) − ∥xk+1−xk−h⃗b(xk+1,(k+1)h)∥2 − h⃗σ(kh)2⟨ϵk,∇⃗b(xk+1,(k+1)h)⊤ϵk⟩ +O(h3/2)
2 h⃗σ(kh)2 ⃗σ(kh)2
=−dlog(cid:0) 2πh⃗σ(kh)2(cid:1) − ∥xk+1−xk−h⃗b(xk+1,(k+1)h)∥2 −h∆⃗b(x ,(k+1)h)+O(h3/2)
2 h⃗σ(kh)2 k+1
Combining (90) and (92), we obtain that
(cid:0) (cid:1)
logp⃗ (x |x )− logp(x ,(k+1)h)−logp(x ,kh)
k+1|k k+1 k k+1 k
=−dlog(cid:0) 2πh⃗σ(kh)2(cid:1) − ∥xk+1−xk−h⃗b(xk+1,(k+1)h)+h⃗σ(kh)2∇logp(xk+1,(k+1)h)∥2 +O(h3/2) (93)
2 h⃗σ(kh)2
=−dlog(cid:0) 2πh⃗σ((k+1)h)2(cid:1) − ∥xk+1−xk−h⃗b(xk+1,(k+1)h)+h⃗σ((k+1)h)2∇logp(xk+1,(k+1)h)∥2 +O(h3/2).
2 h⃗σ((k+1)h)2
By Bayes rule, and taking the exponential of this equation, we obtain
p⃗ (x |x ):=p⃗ (x |x ) p⃗k(xk)
k+1|k k+1 k k+1|k k+1 k p⃗k+1(xk+1)
(94)
exp(cid:0) −∥xk−xk+1+h⃗b(xk+1,(k+1)h)−h⃗σ((k+1)h)2∇logp(xk+1,(k+1)h)∥2(cid:1)
= 2h⃗σ((k+1)h)2 +O(h3/2).
(2πh⃗σ((k+1)h)2)d/2
34Up to the O(h3/2) term, the right-hand side is the conditional Gaussian corresponding to the update
√
x =x +h(cid:0) −⃗b(x ,(k+1)h)+⃗σ((k+1)h)2∇logp(x ,(k+1)h)(cid:1) + h⃗σ((k+1)h)ϵ , ϵ ∼N(0,I).
k k+1 k+1 k+1 k+1 k+1
(95)
If we define y =x , and we use that b(x,t)=−⃗b(x,1−t)+⃗σ(t)2∇logp(x,1−t), we can rewrite (95) as
k K−k
y =y +h(cid:0) −⃗b(y ,(K−k−1)h)+⃗σ((K−k−1)h)2∇logp(y ,(K−k−1)h)(cid:1) (96)
K−k K−k−1 K−k−1 K−k−1
√ √
+ h⃗σ((K−k−1)h)ϵ =y +hb(y ,kh)+ hσ(kh)ϵ , (97)
k K−k−1 K−k−1 K−k−1
√
=⇒ y =y +hb(y ,kh)+ hσ(kh)ϵ . (98)
k+1 k k k
And this is the Euler-Maruyama discretization of the backward process X. If we plug (94) into (83), we
obtain that
p⃗(x )≈p⃗ (x )(cid:81)K−1p⃗ (x |x ). (99)
0:K K K k=0 k+1|k k+1 k
which concludes the proof, as p⃗ (x ) is the initial distribution of the backward process, and p⃗ (x |x )
K K k+1|k k+1 k
are its transition kernels.
B.3 The relationship between the noise predictor ϵ and the score function
Applying Lemma 2 with the choices of (κ ) and (η ) for DDIM, we obtain that X⃗ has the same
t t≥0 t t≥0 t
distribution as
√ √
Xˆ = α X⃗ + 1−α ϵ, ϵ∼N(0,1). (100)
t 1−t 0 1−t
Since X⃗ and Xˆ have the same distribution, predicting the noise of X⃗ is equivalent to predicting the noise of
t t t
Xˆ . The noise predictor ϵ can be written as:
t
ϵ(x,t):=E[ϵ|Xˆ
1−t
=x]=E(cid:2) ϵ|√ α tX⃗ 0+√ 1−α tϵ=x(cid:3) =E(cid:2)x− √√ 1−α αtX⃗ t0|√ α tX⃗ 0+√ 1−α tϵ=x(cid:3) (101)
And the score function s(x,t):=∇logp⃗ (x) admits the expression
1−t
s(x,t):=∇logp⃗ (x)= ∇p⃗1−t(x) = ∇E[p⃗1−t|0(x|X⃗ 0)] = E[∇logp⃗1−t|0(x|X⃗ 0)p⃗1−t|0(x|X⃗ 0)] , (102)
1−t p⃗1−t(x) p⃗1−t(x) p⃗1−t(x)
where
√ √
p⃗ (x|X⃗ )= exp(−∥x− αtY1∥2/(2(1−αt))) =⇒ ∇logp⃗ (x|Y )=−x− αtY1. (103)
1−t|0 0 (2π(1−αt))d/2 t|1 1 1−αt
Plugging this into the right-hand side of (102) and using Bayes’ rule, we get
s(x,t)=E(cid:2) − x−√ αtX⃗ 0|√ α X⃗ +√ 1−α ϵ=x(cid:3) . (104)
1−αt t 0 t
Comparing the right-hand sides of (101) and (104), we obtain that ϵ(x,t)=−√s(x,t) .
1−αt
B.4 The relationship between the vector field v and the score function
By construction (Lipman et al., 2023; Albergo and Vanden-Eijnden, 2023; Albergo et al., 2023), we have that
v(x,t)=E[α˙ Y +β˙ Y |x=α Y +β Y ]
t 1 t 0 t 1 t 0
=E[α˙t(x−βtY0) +β˙ Y |x=α Y +β Y ] (105)
αt t 0 t 1 t 0
= αα˙ ttx+(β˙ t− α α˙t tβ t)E[Y 0|x=α tY 1+β tY 0],
35
⃗where we used that Y =(x−β Y )/α . Also, we can write the score as follows
1 t 0 t
s(x,t):=∇logp (x)=∇pt(x)=∇E[pt|1(x|Y1)] =E[∇pt|1(x|Y1)] =E[pt|1(x|Y1)∇logpt|1(x|Y1)] , (106)
t pt(x) pt(x) pt(x) pt(x)
where
p t|1(x|Y 1)= exp(−∥x (2− πα βt 2Y )1 d∥ /22/(2β t2)) =⇒ ∇logp⃗ t|1(x|Y 1)=−x− βα 2tY1 (107)
t t
Plugging this back into the right-hand side of (106), we obtain
s(x,t)=−E[pt|1(x|Y1)x− βα t2tY1] =−(cid:82)p⃗t|1(x|Y1)p1(Y1)x− βα t2tY1 dY1
pt(x) p⃗t(x) (108)
=−(cid:82) p 1|t(Y 1|x)x− βα t2tY1 dY
1
=−E[x− βα t2tY1|x=α tY 1+β tY 0]=−E[Y0|x=α βt tY1+βtY0]
The last equality holds because (x−α Y )/β =Y . Putting together (105) and (108), we obtain that
t 1 t 0
v(x,t)= α α˙t tx+β t( αα˙ ttβ t−β˙ t)s(x,t) ⇐⇒ s(x,t)=
βt(α α˙t
t1
βt−β˙
t)(cid:0) v(x,t)− αα˙ ttx(cid:1) (109)
Thus, the ODE (3) can be rewritten like this:
d dX tt = αα˙ ttX t+β t(α α˙t tβ t−β˙ t)s(X t,t), X 0 ∼p 0. (110)
To allow for an arbitrary diffusion coefficient, we need to add a correction term to the drift:
dX
t
=(cid:0) αα˙ ttX t+(cid:0)σ( 2t)2 +β t(α α˙t tβ t−β˙ t)(cid:1) s(X t,t)(cid:1) dt+σ(t)dB t, X
0
∼p 0. (111)
This can be easily shown by writing down the Fokker-Planck equations for (110) and (111), and observing
that they are the same up to a cancellation of terms. Finally, if we plug the right-hand side of (109) into
(111), we obtain the SDE for Flow Matching with arbitrary noise schedule (equation (4)).
C Stochastic optimal control as maximum entropy RL in continuous
space and time
In this section, we bridge KL-regularized (or MaxEnt) reinforcement learning and stochastic optimal control.
We show that when the action space is Euclidean and the transition probabilities are conditional Gaussians,
taking the limit in which the stepsize goes to zero on the KL-regularized RL problem gives rise to the SOC
problem. A consequence of this connection is that all algorithms for KL-regularized RL admit an analog
for diffusion fine-tuning. This is not novel, but it may be useful for researchers that are familiar with RL
fine-tuning formulations.
Appendix C.4 is providing a more direct, rigorous, continuous-time connection between SOC and MaxEnt
RL, as it shows that the expected control cost is equal to the KL divergence between the distributions over
trajectories, conditioned on the starting points (see equation (18)).
C.1 Maximum entropy RL
Several diffusion fine-tuning methods (Black et al., 2024; Uehara et al., 2024b) are based on KL-regularized
RL, also known as maximum entropy RL, which we review in the following. In the classical reinforcement
learning (RL) setting, we have an agent that, starting from state s ∼ p , iteratively observes a state s ,
0 0 k
takes an action a according to a policy π(a ;s ,k) which leads to a new state s according to a fixed
k k k k+1
transitionprobabilityp(s |a ,s ),andobtainsrewardsr (s ,a ). Thiscanbesummarizedintoatrajectory
k+1 k k k k k
τ =((s ,a ))K . The goal is to optimize the policy π in order to maximize the expected total reward, i.e.
k k k=0
max E [(cid:80)K r (s ,a )].
π τ∼π,p k=0 k k k
Maximum entropy RL (MaxEnt RL; Ziebart et al. (2008)) amounts to adding the entropy H(π) of the policy
π(·;s ,k) to the reward for each step k, in order to encourage exploration and improve robustness to changes
k
36in the environment: max E [(cid:80)K r (s ,a )+(cid:80)K−1H(π(·;s ,k))] 7. As a generalization, one can
π τ∼π,p k=0 k k k k=0 k
regularize using the negative KL divergence between π(·;s ,k) and a base policy π (·;s ,k):
k base k
max E [(cid:80)K r (s ,a )−(cid:80)K−1KL(π(·;s ,k)||π (·;s ,k))], (112)
π τ∼π,p k=0 k k k k=0 k base k
which prevents the learned policy to deviate too much from the base policy. Each policy π induces a
distribution q(τ) over trajectories τ, and the MaxEnt RL problem (112) can be expressed solely in terms of
such distributions (Lemma 3 in Appendix C.3):
max E [(cid:80)K r (s ,a )]−KL(q||qbase), (113)
q τ∼q k=0 k k k
where qbase is the distribution induced by the base policy π , and the maximization is over all distributions
base
q such that their marginal for s is p . We can further recast this problem as (Lemma 4 in Appendix C.3):
0 0
min KL(q||q∗), where q∗(τ):=qbase(τ)exp(cid:0)(cid:80)K r (s ,a )−V(s ,0)(cid:1) , (114)
q k=0 k k k 0
where
V(s
,k):=log(cid:0)E [exp(cid:0)(cid:80)K
r (s ,a
)(cid:1)
|s
](cid:1)
k τ∼πbase,p k′=k k′ k′ k′ k (115)
=max E (cid:2)(cid:80)K r (s ,a )−(cid:80)K−1KL(π(·;s ,k′)||π (·;s ,k′))|s (cid:3)
π τ∼π,p k′=k k′ k′ k′ k′=k k′ base k′ k
is the value function. Problem (114) directly implies that the distribution induced by the optimal policy π∗ is
the tilted distribution q∗ (which has initial marginal p ).
0
C.2 From maximum entropy RL to stochastic optimal control
The following well-known result, which we prove in Appendix C.3, shows that in a natural sense, the
continuous-time continuous-space version of MaxEnt RL is the SOC framework introduced in Section 4.1. In
particular, when states and actions are vectors in Rd, policies are specified by a vector field u (the control),
and transition probabilities are conditional Gaussians, the MaxEnt RL problem becomes an SOC problem
when the number of timesteps grows to infinity.
Proposition 5. Suppose that
(i) The state space and the action space are Rd,
(ii) Policies π are specified as π(a ;s ,k)=δ(a −u(s ,kh)), where u:Rd×[0,T]→Rd is a vector field,
k k k k
and δ denotes the Dirac delta,
(iii) Transition probabilities are conditional Gaussian densities: p(s |a ,s ) = N(s + h(b(s ,kh) +
k+1 k k k k
σ(kh)a ),hσ(kh)σ(kh)⊤), where h=T/K is the stepsize, and b and σ are defined as in Section 4.1.
k
Then, in the limit in which the number of steps K grows to infinity, the problem (112) is equivalent to the
SOC problem (12)-(13), identifying
• the sequence of states (s )k with the trajectory Xu =(Xu) ,
k k=0 t t∈[0,1]
• the running reward (cid:80)K−1r (s ,a ) with the negative running cost −(cid:82)T f(Xu,t)dt,
k=0 k k k 0 t
• the terminal reward r (s ,a ) with the negative terminal cost −g(Xu),
K K K T
• the KL regularization E [(cid:80)K−1KL(π(·;s ,k)||π (·;s ,k))] with 1 times the expected L2 norm of
τ∼π,p k=0 k base k 2
the control
1E(cid:2)(cid:82)T ∥u(Xu,t)∥2dt(cid:3)
,
2 0 t
• and the value function V(s ,k) defined in (115) with the negative value function −V(x,t) defined in
k
Section 4.1.
A first consequence of this result is that every loss function designed for generic MaxEnt RL problems has a
corresponding loss function for SOC problems. The geometric structure of the latter allows for additional
7Theentropytermsareusuallymultipliedbyafactortotunetheirmagnitude,butonecanequivalentlyrescaletherewards,
whichiswhywedonotaddanyfactor.
37losses that do not have an analog in the classical MaxEnt RL setting; in particular, we can differentiate the
state and terminal costs.
A second consequence of Proposition 5 is that the characterization (114) can be translated to the SOC setting.
The analogs of the distributions q∗, qbase induced by the optimal policy π∗ and the base policy πbase are the
distributions p∗,pbase induced by the optimal control u∗ and the null control. For an arbitrary trajectory
X =(X ) , the relation between P∗ and Pbase is given by
t t∈[0,T]
dP∗ (X)=exp(−(cid:82)T f(X ,t)dt−g(X )+V(X ,0)) (116)
dPbase 0 t T 0
where V is the value function as defined in Section 4.1. Note that this matches the statement in (22).
C.3 Proof of Proposition 5: from MaxEnt RL to SOC
Since the transition p(s |a ,s ) is fixed, for each π we can define
k+1 k k
π˜(a ,s ;s ,k)=π(a ;s ,k)p(s |a ,s ) and π˜ (a ,s ;s ,k)=π (a ;s ,k)p(s |a ,s ),
k k+1 k k k k+1 k k base k k+1 k base k k k+1 k k
(117)
and reexpress (112) as (see Lemma 3)
min E [(cid:80)K r (s ,a )−(cid:80)K−1KL(π˜(·,·;s ,k)||π˜ (·,·;s ,k))]. (118)
π˜ τ∼π˜ k=0 k k k k=0 k base k
Using the hypothesis of the proposition, we can write
π˜(a ,s ;s ,k)=δ(a −u(s ,kη))N(s +η(b(s ,kη)+σ(kη)a ),ησ(kη)σ(kη)⊤)
k k+1 k k k k k k (119)
=δ(a −u(s ,kη))π˜(s ;s ,k),
k k k+1 k
where π˜(s ;s ,k) = N(s +η(b(s ,kη)+σ(kη)u(s ,kη)),ησ(kη)σ(kη)⊤) is the state transition kernel.
k+1 k k k k
We set the base policy as π (a ;s ,k) = δ(a ), and we obtain analogously that π˜(a ,s ;s ,k) =
base k k k k k+1 k
δ(a )π˜ (s ;s ,k) with π˜ (s ;s ,k)=N(s +ηb(s ,kη),ησ(kη)σ(kη)⊤). Now, if we take K large,
k base k+1 k base k+1 k k k
the trajectory (s )K generated by π˜ can be regarded as the Euler-Maruyama discretization of a solution Xu
k k=0
of the controlled SDE (13), while the trajectory generated by π˜ is the discretization of the uncontrolled
base
process X0 obtained by setting u=0. As a consequence
lim E [(cid:80)K−1KL(π˜(·,·;s ,k)||π˜ (·,·;s ,k))]
K→∞ τ∼π˜ k=0 k base k (120)
=lim K→∞E τ∼π˜[(cid:80)K k=− 01KL(π˜(·;s k,k)||π˜ base(·;s k,k))]=E Xu∼Pu[logd dP Pu 0(Xu)],
where Pu and P0 are the measures of the processes Xu and X0, respectively. The Girsanov theorem
(Theorem 2) implies that logdPu(Xu) = −(cid:82)T ⟨u(Xu,t),dB ⟩ − 1(cid:82)T ∥u(Xu,t)∥2dt, which implies that
dP0 0 t t 2 0 t
E Xu∼Pu[logd dP Pu 0(Xu)] = −1 2E Xu∼Pu[(cid:82) 0T ∥u(X tu,t)∥2dt]. Setting the rewards r k(a k,s k) = ηf(s k,kη) for k ∈
{0,...,K−1} and r (a ,s )=ηg(s ), where f and g are as in Section 4.1, yields the following limiting
K K K k
object:
lim K→∞E τ∼π˜[(cid:80)K k=0r k(s k,a k)]=E Xu∼Pu[(cid:82) 0T f(X tu,t)dt+g(X Tu)]. (121)
Hence, the limit of the MaxEnt RL loss (118) is the SOC loss (12).
Lemma3. Letπ˜(a ,s ;s ,k)andπ˜ (a ,s ;s ,k)beasdefinedin (117). KL(π˜(·,·;s ,k)||π˜ (·,·;s ,k))]
k k+1 k base k k+1 k k base k
and KL(π(·;s ,k)||π (·;s ,k))] are equal. Moreover, if q, qbase denote the distributions over trajectories
k base k
induced by π, π , we have that
base
KL(q||qbase)=E[(cid:80)K−1KL(π(·;s ,k)||π (·;s ,k))]. (122)
k=0 k base k
38Proof. We have that
KL(π˜(·,·;s ,k)||π˜ (·,·;s ,k))]=(cid:80) π˜(a ,s ;s ,k)log π˜(ak,sk+1;sk,k)
k base k ak,sk+1 k k+1 k π˜base(ak,sk+1;sk,k)
=(cid:80) π(a ;s ,k)p(s |a ,s )log π(ak;sk,k)p(sk+1|ak,sk)
ak,sk+1 k k k+1 k k πbase(ak;sk,k)p(sk+1|ak,sk)
=(cid:80) π(a ;s ,k)p(s |a ,s )log π(ak;sk,k) (123)
ak,sk+1 k k k+1 k k πbase(ak;sk,k)
=(cid:80) π(a ;s ,k)(cid:0)(cid:80) p(s |a ,s )(cid:1) log π(ak;sk,k)
ak k k sk+1 k+1 k k πbase(ak;sk,k)
=(cid:80) π(a ;s ,k)log π(ak;sk,k) =KL(π(·;s ,k)||π (·;s ,k))].
ak k k πbase(ak;sk,k) k base k
To prove (122), by construction we can write
q(τ)=p (s )(cid:81)K−1π˜(a ,s ;s ,k), qbase(τ)=p (s )(cid:81)K−1π˜ (a ,s ;s ,k), (124)
0 0 k=0 k k+1 k 0 0 k=0 base k k+1 k
which means that
KL(q||qbase)=E [log q(τ) ]=E [(cid:80)K−1log π˜(ak,sk+1;sk,k) ]
τ∼q qbase(τ) τ∼q k=0 π˜base(ak,sk+1;sk,k)
=(cid:80)K−1E [log π˜(ak,sk+1;sk,k) ]
k=0 τ∼q0:(k+1) π˜base(ak,sk+1;sk,k)
=(cid:80)K−1E [(cid:80) π˜(a ,s ;s ,k)log π˜(ak,sk+1;sk,k) ]
k=0 τ∼q0:k ak,sk+1 k k+1 k π˜base(ak,sk+1;sk,k) (125)
=(cid:80)K−1E
[KL(π˜(·,·;s ,k)||π˜ (·,·;s ,k))]
k=0 τ∼q0:k k base k
=(cid:80)K−1E
[KL(π(·;s ,k)||π (·;s ,k))]
k=0 τ∼q0:k k base k
=E [(cid:80)K−1KL(π(·;s ,k)||π (·;s ,k))]
τ∼q0:k k=0 k base k
Here, the notation q0:k denotes the trajectory q up to the state s .
k
Lemma 4. The distribution-based MaxEnt RL formulation in (113) is equivalent to the the following problem:
(cid:0) (cid:1)
min qKL(q||q∗), where q∗(τ):=
p0(1 s0)(cid:80)
{τq ′b |a ss
′
0e =(τ s0) }ex qp base(cid:80) (τK k ′= )e0 xr pk (cid:0)(s (cid:80)k,a
K
kk =)
0rk(s′ k,a′
k)(cid:1), (126)
where the minimization is over q with marginal p at step zero. The optimum of the problem is q∗, which
0
satisfies the marginal constraint. The following alternative characterization of q∗ holds:
q∗(τ)=qbase(τ)exp(cid:0)(cid:80)K r (s ,a )−V(s ,0)(cid:1) , (127)
k=0 k k k 0
where V(x,k)=max E (cid:2)(cid:80)K r (s ,a )−(cid:80)K−1KL(π(·;s ,k′)||π (·;s ,k′))|s =x(cid:3) . (128)
π τ∼π,p k′=k k′ k′ k′ k′=k k′ base k′ k
Proof. Let us expand KL(q||q∗):
KL(q||q∗)=E (cid:2) log q(τ) (cid:3)
τ∼q q∗(τ)
=E (cid:2) logq(τ)−logqbase(τ)−(cid:80)K r (s ,a )
τ∼q k=0 k k k
+log(cid:0) 1 (cid:80) qbase(τ′)exp(cid:0)(cid:80)K r (s′,a′)(cid:1)(cid:1)(cid:3) (129)
p0(s0) {τ′|s′ 0=s0} k=0 k k k
=KL(q||qbase)−E (cid:2)(cid:80)K r (s ,a )(cid:3)
τ∼q k=0 k k k
+E (cid:2) log(cid:0) 1 (cid:80) qbase(τ′)exp(cid:0)(cid:80)K r (s′,a′)(cid:1)(cid:1)(cid:3) ,
s0∼p0 p0(s0) {τ′|s′ 0=s0} k=0 k k k
where the third equality holds because the marginal of q at step zero is p by hypothesis. Since the third
0
term in the right-hand side is independent of q, this proves the equivalence between (113) and (126).
Next, we prove that the marginal of q∗ at step zero is p :
0
(cid:0) (cid:1)
(cid:80) {τ|s0=x}q∗(τ):=(cid:80) {τ|s0=x} p01 (x)(cid:80) {τq ′b |a ss ′ 0e =(τ x})e qx bp ase((cid:80) τ′K k )= ex0 prk (cid:0)( (cid:80)sk K k,a =k 0) rk(s′ k,a′ k)(cid:1) =p 0(x). (130)
39Now, for an arbitrary s , let q , q∗ be the distributions q, q∗ conditioned on the initial state being s . We
can write an analog to e0 quatios n0 (1s 209) for q , q∗ : 0
s0 s0
KL(q s0||q s∗ 0)=E τ∼qs0(cid:2) logq qs s∗0 0( (τ τ) )(cid:3)
=E (cid:2) logq (τ)−logqbase(τ)−(cid:80)K r (s ,a )
τ∼qs0 s0 s0 k=0 k k k
+log(cid:0) 1 (cid:80) qbase(τ′)exp(cid:0)(cid:80)K r (s′,a′)(cid:1)(cid:1)(cid:3) (131)
p0(s0) {τ′|s′ 0=s0} s0 k=0 k k k
=KL(q ||qbase)−E (cid:2)(cid:80)K r (s ,a )(cid:3)
s0 s0 τ∼qs0 k=0 k k k
+log(cid:0) 1 (cid:80) qbase(τ′)exp(cid:0)(cid:80)K r (s′,a′)(cid:1)(cid:1) ,
p0(s0) {τ′|s′ 0=s0} k=0 k k k
Hence,
0=min KL(q ||q∗ )=−max {E (cid:2)(cid:80)K r (s ,a )(cid:3) −KL(q ||qbase)}
qs0 s0 s0 qs0 τ∼qs0 k=0 k k k s0 s0 (132)
+log(cid:0) 1 (cid:80) qbase(τ′)exp(cid:0)(cid:80)K r (s′,a′)(cid:1)(cid:1) .
p0(s0) {τ′|s′ 0=s0} k=0 k k k
And applying (122) from (122), we obtain that
log(cid:0) 1 (cid:80) qbase(τ′)exp(cid:0)(cid:80)K r (s′,a′)(cid:1)(cid:1)
p0(s0) {τ′|s′ 0=s0} k=0 k k k (133)
=max E (cid:2)(cid:80)K r (s ,a )−(cid:80)K−1KL(π(·;s ,k)||π (·;s ,k))|s (cid:3) =V(s ,0),
π τ∼π,p k=0 k k k k=0 k base k 0 0
which concludes the proof.
C.4 Proof of equation (18): the control cost is a KL regularizer
Theorem 2 (Girsanov theorem for SDEs). If the two SDEs
dX =b (X ,t)dt+σ(X ,t)dB , X =x (134)
t 1 t t t 0 init
dY =(b (Y ,t)+b (Y ,t))dt+σ(Y ,t)dB , Y =x (135)
t 1 t 2 t t t 0 init
admit unique strong solutions on [0,T], then for any bounded continuous functional Φ on C([0,T]), we have
that
E[Φ(X)]=E(cid:2) Φ(Y)exp(cid:0) −(cid:82)T σ(Y ,t)−1b (Y ,t)dB − 1(cid:82)T ∥σ(Y ,t)−1b (Y ,t)∥2dt(cid:1)(cid:3)
0 t 2 t t 2 0 t 2 t (136)
=E(cid:2) Φ(Y)exp(cid:0) −(cid:82)T σ(Y ,t)−1b (Y ,t)dB˜ + 1(cid:82)T ∥σ(Y ,t)−1b (Y ,t)∥2dt(cid:1)(cid:3) ,
0 t 2 t t 2 0 t 2 t
where B˜ =B +(cid:82)t σ(Y ,s)−1b (Y ,s)ds. More generally, b and b can be random processes that are adapted
t t 0 s 2 s 1 2
to filtration of B.
Consider the SDEs
dX =b(X ,t)dt+σ(t)dB , X =x , (137)
t t t 0 0
dXu =(cid:0) b(Xu,t)+σ(t)u(Xu,t)(cid:1) dt+σ(t)dB , Xu =x . (138)
t t t t 0 0
If we let P| , Pu| be the probability measures of the solutions of (137) and (138), Theorem 2 implies that
x0 x0
log dd PP u| |x x0 0(Xu)=−(cid:82) 01 u(X tu,t)dB t− 1 2(cid:82) 01 ∥u(X tu,t)∥2dt. (139)
Hence,
D KL(cid:0)Pu| x0 (cid:13) (cid:13)P| x0(cid:1) =E(cid:2) logd dP Pu || xx 00(Xu)|X 0u =x 0(cid:3) =−E(cid:2) log dd PP u| |x x0 0(Xu)|X 0u =x 0(cid:3)
=E(cid:2)(cid:82)1 u(Xu,t)dB + 1(cid:82)1 ∥u(Xu,t)∥2dt|Xu =x (cid:3) =E(cid:2)1(cid:82)1 ∥u(Xu,t)∥2dt|Xu =x (cid:3) ,
0 t t 2 0 t 0 0 2 0 t 0 0
(140)
where we used that stochastic integrals are martingales.
40D Proofs of Section 4.3: memoryless noise schedule and fine-tuning
recipe
D.1 Proof of Proposition 1: the memoryless noise schedule
We consider the forward-backward SDEs (63)-(64) with arbitrary noise schedule. By Proposition 4, the
trajectories X⃗, X of these two processes are equally distributed up to a time flip, which also means that
their marginals satisfy p⃗ = p , for all t ∈ [0,1]. First, we develop an explicit expression for the score
t 1−t
function s(x,t) = ∇logp (x). By the properties of flow matching, we know that p is the distribution of
t t
the interpolation variable X¯ = β X¯ +α X¯ , where X¯ ∼ N(0,I),X¯ ∼ pdata are independent. Thus,
t t 0 t 1 0 1
X¯ t− βα ttX¯ 1 ∼N(0,I), which means that we can express the density p t as
p t(x)=(cid:82)
Rd
exp(cid:0)
(2− π∥ βx 2− )2 dα β /t t2
2y∥2(cid:1)
pdata(y)dy. (141)
t
Thus,
s(x,t)=∇logp t(x)=− βx
t2
+ βα t2t (cid:82) (cid:82)R Rd dy ee xx pp (cid:0)(cid:0) −− ∥∥ xx
−
2− α2 βα β
t
t2t t2 yy ∥∥ 22 (cid:1)(cid:1) pp dd aa tata (( yy )) dd yy :=−x−α βt t2ξt(x), (142)
where we defined
ξ t(x)= (cid:82) (cid:82)R Rd dy ee xx pp (cid:0)(cid:0) −− ∥∥ xx
−
2− α2 βα β
t
t2t t2 yy ∥∥ 22 (cid:1)(cid:1) pp dd aa tata (( yy )) dd yy . (143)
Hence, we can rewrite the forward SDE (63) as
dX⃗ =(cid:0) −κ X⃗ −(cid:0)σ(1−t)2 −η (cid:1)X⃗ t−α1−tξ1−t(X⃗ t)(cid:1) dt+σ(1−t)dB , X⃗ ∼p (144)
t 1−t t 2 1−t β2 t 0 data
1−t
Hence, if we substitute κ ← κ + σ(1−t)2−2η1−t, ξ ← α1−t(σ(1−t)2−2η1−t)ξ (X⃗ ) (where we ignore
1−t 1−t 2β2 1−t 2β2 1−t t
√ 1−t 1−t
the dependency on X⃗ ), 2η ←σ(1−t), we can apply Lemma 2, which yields
t 1−t
X⃗ =X⃗ exp(cid:0) −(cid:82)t(cid:0) κ + σ(1−s)2−2η1−s(cid:1) ds(cid:1)
t 0 0 1−s 2β2
1−s
+(cid:82)t exp(cid:0) −(cid:82)t(cid:0) κ + σ(1−s)2−2η1−s(cid:1) ds(cid:1)α 1−t′(σ(1−t′)2−2η 1−t′) ξ (X⃗ )dt′ (145)
0 t′ 1−s 2β2 2β2 1−t′ t′
1−s 1−t′
+(cid:82)t σ(1−t′)exp(cid:0) −(cid:82)t(cid:0)
κ +
σ(1−s)2−2η1−s(cid:1) ds(cid:1)
dB .
0 t′ 1−s 2β2 t′
1−s
We simplify the recurring expression:
(cid:0) (cid:1)
κ + σ(1−s)2−2η1−s = α˙1−s + σ(1−s)2−2β1−s α α˙1 1− −s sβ1−s−β˙ 1−s = σ(1−s)2 + β˙ 1−s (146)
1−s 2β 12
−s
α1−s 2β 12
−s
2β 12
−s
β1−s
Thus,
(cid:82)t(cid:0) κ + σ(1−s)2−2η1−s(cid:1) ds=(cid:82)t(cid:0)σ(1−s)2 −∂ logβ (cid:1) ds=(cid:82)t σ(1−s)2 ds−(cid:0) logβ −logβ (cid:1) , (147)
t′ 1−s 2β2 t′ 2β2 s 1−s t′ 2β2 1−t 1−t′
1−s 1−s 1−s
which means that
exp(cid:0) −(cid:82)t(cid:0) κ + σ(1−s)2−2η1−s(cid:1) ds(cid:1) =exp(cid:0) −(cid:82)t σ(1−s)2 ds(cid:1)β1−t , (148)
t′ 1−s 2β 12 −s t′ 2β 12 −s β 1−t′
α 1−t′(σ(1−t′)2−2η 1−t′) ξ (X⃗ )=(cid:0)σ(1−t′)2 + β˙ 1−t′ − α˙ 1−t′(cid:1) ξ (X⃗ ). (149)
2β 12 −t′ 1−t′ t′ 2β 12 −t′ β 1−t′ α 1−t′ 1−t′ t′
41If we define σ¯2(1−s) such that σ2(1−s)=2β (cid:0)α˙1−sβ −β˙ (cid:1) +χ(1−s), we obtain that
1−s α1−s 1−s 1−s
exp(cid:0) −(cid:82)t σ(1−s)2 ds(cid:1)β1−t =exp(cid:0) −(cid:82)t(cid:0)α˙1−s
−
β˙
1−s +
χ(1−s)(cid:1) ds(cid:1)β1−t
t′ 2β 12 −s β 1−t′ t′ α1−s β1−s 2β 12 −s β 1−t′ (150)
=exp(cid:0)(cid:82)t(cid:0)
∂ logα −∂ logβ −
χ(1−s)(cid:1) ds(cid:1)β1−t =exp(cid:0) −(cid:82)t χ(1−s)ds(cid:1)α1−t
,
t′ s 1−s s 1−s 2β 12 −s β 1−t′ t′ 2β 12 −s α 1−t′
(cid:0)σ(1−t′)2 + β˙ 1−t′ − α˙ 1−t′(cid:1) ξ (X⃗ )= χ(1−t′)ξ (X⃗ ) (151)
2β 12 −t′ β 1−t′ α 1−t′ 1−t′ t′ 2β 12 −t′ 1−t′ t′
If we plug equations (150)-(151) into (148)-(149), and then those into (145), we obtain that
X⃗ =X⃗ exp(cid:0) −(cid:82)t χ(1−s)ds(cid:1)α1−t +(cid:82)t exp(cid:0) −(cid:82)t χ(1−s)ds(cid:1)α1−t χ(1−t′)ξ (X⃗ )dt′
t 0 0 2β 12 −s α1 0 t′ 2β 12 −s α 1−t′ 2β 12 −t′ 1−t′ t′ (152)
+(cid:82)t(cid:0) 2β (cid:0)α˙ 1−t′β −β˙ (cid:1) +χ(1−t′)(cid:1) exp(cid:0) −(cid:82)t χ(1−s)ds(cid:1)α1−t dB .
0 1−t′ α 1−t′ 1−t′ 1−t′ t′ 2β 12 −s α 1−t′ t′
and if we take the limit t→1− and use that α =1,
1
X⃗ =X⃗ (cid:0) lim exp(cid:0) −(cid:82)t χ(1−s)ds(cid:1) α (cid:1) +lim (cid:82)t exp(cid:0) −(cid:82)t χ(1−s)ds(cid:1)α1−t χ(1−t′)ξ (X⃗ )dt′
1 0 t→1− 0 2β 12 −s 1−t t→1− 0 t′ 2β 12 −s α 1−t′ 2β 12 −t′ 1−t′ t′
+lim (cid:82)t(cid:0) 2β (cid:0)α˙ 1−t′β −β˙ (cid:1) +χ(1−t′)(cid:1) exp(cid:0) −(cid:82)t χ(1−s)ds(cid:1)α1−t dB .
t→1− 0 1−t′ α 1−t′ 1−t′ 1−t′ t′ 2β 12 −s α 1−t′ t′
(153)
The assumption on χ in (25) is equivalent, up to a rearrangement of the notation and a flip in the time
variable, to the statement that for all t′ ∈[0,1),
lim exp(cid:0) −(cid:82)t χ(1−s)ds(cid:1) α =0. (154)
t→1− t′ 2β2 1−t
1−s
Hence, under assumption (25), the factor accompanying X⃗ in equation (153) is zero. Moreover, this
0
assumption also implies that
lim (cid:82)t exp(cid:0) −(cid:82)t χ(1−s)ds(cid:1)α1−t χ(1−t′)ξ (X⃗ )dt′
t→1− 0 t′ 2β 12 −s α 1−t′ 2β 12 −t′ 1−t′ t′ (155)
=(cid:82)1(cid:0) lim exp(cid:0) −(cid:82)t χ(1−s)ds(cid:1) α (cid:1) 1 χ(1−t′)ξ (X⃗ )dt′ =0.
0 t→1− t′ 2β 12 −s 1−t α 1−t′ 2β 12 −t′ 1−t′ t′
If we plug (154) and (155) into (153), we obtain that
X⃗ =lim (cid:82)t(cid:0) 2β (cid:0)α˙ 1−t′β −β˙ (cid:1) +χ(1−t′)(cid:1) exp(cid:0) −(cid:82)t χ(1−s)ds(cid:1)α1−t dB , (156)
1 t→1− 0 1−t′ α 1−t′ 1−t′ 1−t′ t′ 2β 12 −s α 1−t′ t′
which shows that X⃗ is independent of X⃗ . Next, we leverage that X⃗ and X have equal distributions over
1 0
trajectories (Proposition 4). In particular, the joint distribution of (X⃗ ,X⃗ ) is equal to the joint distribution
0 1
of (X ,X ). We conclude that X and X are independent, which is the definition of the memorylessness
1 0 1 0
property. Hence, the assumption (25) is sufficient for memorylessness to hold.
It remains to prove that the assumption (25) is necessary. Looking at equation (152) we deduce that generally,
for any t ∈ [0,1), X⃗ and X⃗ are not independent, because the first two terms in (152) are different from
0 t
zero. Thus, if there existed a t′ ∈[0,1) such that the limit (154) is different from zero, then X⃗ would not be
1
independent from X⃗ , which means that in general it would not be independent of X⃗ either.
t′ 0
D.2 Proof of Theorem 1: fine-tuning recipe for general noise schedules
The proof of this result relies heavily on the properties of the Hamilton-Jacobi-Bellman equation:
Theorem 3 (Hamilton-Jacobi-Bellman equation). If we define the infinitesimal generator
L:= 1(cid:80)d (σσ⊤) (t)∂ ∂ +(cid:80)d b (x,t)∂ , (157)
2 i,j=1 ij xi xj i=1 i xi
the value function V for the SOC problem (12)-(13) solves the following Hamilton-Jacobi-Bellman (HJB)
partial differential equation:
∂ V(x,t)=−LV(x,t)+ 1∥(σ⊤∇V)(x,t)∥2−f(x,t),
t 2 (158)
V(x,T)=g(x).
42ConsiderforwardSDEslike(63),startingfromthedistributionspbase andp∗,wherep∗(x)∝pbase(x)exp(r(x)).
dX⃗ =⃗b(X⃗ ,t)dt+σ(t)dB , X⃗ ∼pbase, (159)
t t t 0
dX⃗∗ =⃗b∗(X⃗∗,t)dt+σ(t)dB , X⃗ ∼p∗. (160)
t t t 0
where the drifts are defined as
⃗b(x,t)=−κ x+(cid:0)σ(1−t)2 −η (cid:1) s(x,1−t)=−κ x+(cid:0)σ(1−t)2 −η (cid:1) ∇logp⃗ (x),
1−t 2 1−t 1−t 2 1−t t (161)
⃗b∗(x,t)=−κ x+(cid:0)σ(1−t)2 −η (cid:1) s∗(x,1−t)=−κ x+(cid:0)σ(1−t)2 −η (cid:1) ∇logp⃗∗(x),
1−t 2 1−t 1−t 2 1−t t
and p⃗ , p⃗∗ are the densities of X , X⃗ , respectively. p⃗ , p⃗∗ satisfy Fokker-Planck equations:
t t t t t t
∂ p⃗ =∇·(⃗b(x,t)p⃗ )+∇·(σ(1−t)2 ∇p⃗ ), p⃗ =pbase,
t t t 2 t 0 (162)
∂ p⃗∗ =∇·(⃗b∗(x,t)p⃗∗)+∇·(σ(1−t)2 ∇p⃗∗), p⃗ =p∗.
t t t 2 t 0
Plugging (161) into (162), we obtain
∂ p⃗ =∇·(κ xp⃗ )+∇·(cid:0) η ∇p⃗ (cid:1) , p⃗ =pbase,
t t 1−t t 1−t t 0 (163)
∂ p⃗∗ =∇·(κ xp⃗∗)+∇·(cid:0) η ∇p⃗∗(cid:1) , p⃗ =p∗.
t t 1−t t 1−t t 0
We apply the Hopf-Cole transformation to obtain PDEs for −logp⃗ (and −logp⃗∗ analogously):
t t
(cid:0) (cid:1)
−∂ (−logp⃗ )= ∂tpt = ∇·(κ1−txp⃗t)+∇· η1−t∇p⃗t
t t pt pt
(164)
=κ ∇·x+κ ⟨x,∇logp⃗ ⟩+η ∇·(∇logp⃗texp(logpt))
1−t 1−t t 1−t pt
=κ d+κ ⟨x,∇logp⃗ ⟩+η
(cid:0)
∆logp⃗ +∥∇logp⃗
∥2(cid:1)
.
1−t 1−t t 1−t t t
Hence, if we define V(x,t) = −logp⃗ (x), V∗(x,t) = −logp⃗∗(x), then V and V∗ satisfy the following
t t
Hamilton-Jacobi-Bellman equations:
−∂ V =κ d−κ ⟨x,∇V⟩+η (cid:0) −∆V +∥∇V∥2(cid:1) , V(x,0)=−logpbase(x), (165)
t 1−t 1−t 1−t
−∂ V∗ =κ d−κ ⟨x,∇V∗⟩+η (cid:0) −∆V∗+∥∇V∗∥2(cid:1) , V∗(x,0)=−logp∗(x). (166)
t 1−t 1−t 1−t
Now, define Vˆ(x,t)=V∗(x,t)−V(x,t). Subtracting (166) from (165), we obtain
−∂ Vˆ=−κ ⟨x,∇Vˆ⟩+η (cid:0) −∆Vˆ+∥∇V∗∥2−∥∇V∥2(cid:1)
t 1−t 1−t
=−κ ⟨x,∇Vˆ⟩+η (cid:0) −∆Vˆ+∥∇(Vˆ+V)∥2−∥∇V∥2(cid:1)
1−t 1−t
=−κ ⟨x,∇Vˆ⟩+η (cid:0) −∆Vˆ+∥∇Vˆ∥2+2⟨∇V,∇Vˆ⟩(cid:1)
1−t 1−t (167)
=⟨−κ x+2η ∇V,∇Vˆ⟩+η (cid:0) −∆Vˆ+∥∇Vˆ∥2(cid:1)
1−t 1−t 1−t
=⟨−κ x−2η s(x,1−t),∇Vˆ⟩+η (cid:0) −∆Vˆ+∥∇Vˆ∥2(cid:1) ,
1−t 1−t 1−t
Vˆ(x,0)=−logp∗(x)+logpbase(x)=−r(x)+log(cid:0)(cid:82) pbase(y)exp(r(y))dy(cid:1)
.
Hence, Vˆ also satisfies a Hamilton-Jacobi-Bellman equation. If we define V such that Vˆ(x,t)=V(x,1−t),
we have that
∂ V =⟨−κ x−2η s(x,t),∇V⟩+η
(cid:0)
−∆V
+∥∇V∥2(cid:1)
,
V(x,1)=r(x)−log(cid:0)(cid:82) pbase(y)exp(r(y))dy(cid:1)
.
t t t t
(168)
Using Theorem 3, we can reverse-engineer V as the value function of the following SOC problem:
minE(cid:2)1(cid:82)1 ∥u(Xu,t)∥2dt−r(x)+log(cid:0)(cid:82) pbase(y)exp(r(y))dy(cid:1)(cid:3) , (169)
u∈U 2 0 t
√ √
s.t. dXu=(cid:0) κ x+2η s(x,t)+ 2η u(Xu,t)(cid:1) dt+ 2η dB , Xu ∼p . (170)
t t t t t t t 0 0
43√
NotethatthisSOCproblemisequaltotheproblem(12)-(13)withthechoicesf =0,g =−r,andσ(t)= 2η .
t
By equation (17), the optimal control of the problem (169)-(170) is of the form:
√ √ √
u∗(x,t)=− 2η ∇V(x,t)=− 2η ∇Vˆ(x,1−t)=− 2η (cid:0) ∇V∗(x,1−t)−∇V(x,1−t)(cid:1)
t t t (171)
√ √
=− 2η (cid:0) −∇logp⃗∗ (x)+∇logp⃗ (x)(cid:1) = 2η (cid:0) s∗(x,t)−s(x,t)(cid:1) ,
t 1−t 1−t t
√
⇐⇒ s∗(x,t)=s(x,t)+u∗(x,t)/ 2η . (172)
t
As in (64), the backward SDEs corresponding to the forward SDEs (160) take the following form:
dX∗ =(cid:0) κ X∗+(cid:0)σ(t)2 +η (cid:1) s∗(X∗,t)(cid:1) dt+σ(t)dB , X∗ ∼N(0,I). (173)
t t t 2 t t t 0
If we plug (172) into this equation, we obtain
dX t∗ =(cid:0) κ tX t∗+(cid:0)σ( 2t)2 +η t(cid:1)(cid:0) s(X t∗,t)+ u∗ √(X 2ηt∗ t,t)(cid:1)(cid:1) dt+σ(t)dB t, X 0∗ ∼N(0,I), (174)
⇐⇒ dX t∗ =(cid:0) b(X t∗,t)+ σ( √2t)2 2η+ tηtu∗(X t∗,t)(cid:1) dt+σ(t)dB t, X 0∗ ∼N(0,I). (175)
where we used that b(x,t)=κ x+(cid:0)σ(t)2 +η (cid:1) s(x,t) by definition in equation (11).
t 2 t
(cid:113)
The fine-tuned inference SDE for DDIM Now, for DDIM, we have that u∗(x,t)=− α˙t (ϵ∗(x,t)−
αt(1−αt)
ϵbase(x,t)) by (26). Hence,
σ( √2t)2 2η+ tηtu∗(x,t)=−σ( 2t (cid:113))2 + α˙t2α˙ αt t(cid:113) αt(1α˙ −t αt)(ϵ∗(x,t)−ϵbase(x,t))=−σ( √2t) 12 −+ α2α t˙ αt t(ϵ∗(x,t)−ϵbase(x,t)), (176)
αt
=⇒ b(x,t)+ σ( √2t)2 2η+ tηtu∗(x,t)= 2α˙ αt tX t−(cid:0) 2α˙ αt
t
+ σ( 2t)2(cid:1)ϵb √as 1e( −X αt t,t) − σ( √2t) 12 −+ α2α t˙ αt t(ϵ∗(x,t)−ϵbase(x,t))
(177)
= 2α˙ αt tX t−(cid:0) 2α˙ αt
t
+ σ( 2t)2(cid:1)ϵ √∗( 1X −t α,t t).
We obtain that the fine-tuned inference SDE for DDIM is
dX t∗ =(cid:0) 2α˙ αt tX t∗−(cid:0) 2α˙ αt
t
+ σ( 2t)2(cid:1)ϵ √∗( 1X −t∗ α,t t)(cid:1) dt+σ(t)dB t, X 0∗ ∼N(0,I), (178)
which is matches the SDE (6) with the choice ϵ=ϵ∗.
(cid:114)
Thefine-tunedinferenceSDEforFlowMatching ForFlowMatching,wehavethatu∗(x,t)= 2 (v∗(x,t)−
βt(α α˙t tβt−β˙ t)
vbase(x,t)) by (27). Hence,
σ( √2t)2 2η+ tηtu∗(x,t)= σ( (cid:113)2t)2 2β+ tβ (t
α
α˙(
t
tαα˙ βtt tβ −t β−
˙
tβ )˙ t)(cid:114)
βt(α α˙t
t2
βt−β˙
t)(v∗(x,t)−vbase(x,t))
(179)
=(cid:0) 1+ σ(t)2 (cid:1) (v∗(x,t)−vbase(x,t)).
2βt( αα˙ ttβt−β˙ t)
=⇒ b(x,t)+ σ( √2t)2 +ηtu∗(x,t)=vbase(x,t)+ σ(t)2 (cid:0) vbase(x,t)− α˙tx(cid:1)
2ηt 2βt(α α˙t tβt−β˙ t) αt
+(cid:0) 1+ σ(t)2 (cid:1) (v∗(x,t)−vbase(x,t)) (180)
2βt(α α˙t tβt−β˙ t)
=v∗(x,t)+ σ(t)2 (cid:0) v∗(x,t)− α˙tx(cid:1) .
2βt(α α˙t tβt−β˙ t) αt
We obtain that the fine-tuned inference SDE for Flow Matching is
dX t∗ =(cid:0) v(X t∗,t)+ 2βt(σ
α α˙
tt(t β) t2
−β˙
t)(cid:0) v∗(X t∗,t)− αα˙ ttX t∗(cid:1)(cid:1) dt+σ(t)dB t, X 0∗ ∼N(0,I), (181)
which matches equation (4) with the choice v =v∗.
44E Loss function derivations
E.1 Derivation of the Continuous Adjoint method
Proposition 6. The gradient dL of the adjoint loss L(u;X) defined in (28) with respect to the parameters θ
dθ
of the control can be expressed as in (32).
Proof. First, note that we can write
∇
E(cid:2)(cid:82)T (cid:0)1∥u (Xuθ,t)∥2+f(Xuθ,t)(cid:1) dt+g(Xuθ)(cid:3)
θ 0 2 θ t t T
=E(cid:2)(cid:82)T ∇ u (Xuθ,t)u (Xuθ,t)dt(cid:3) +∇ E(cid:2)(cid:82)T (cid:0)1∥v(Xuθ,t)∥2+f(Xuθ,t)(cid:1) dt+g(Xuθ)(cid:3) | .
0 θ θ t θ t θ 0 2 t t T v=stopgrad(uθ)
(182)
Todevelopthesecondterm, weapplyLemma5. Namely, bytheLeibnizruleandequation(187), wehavethat
∇
E(cid:2)(cid:82)T (cid:0)1∥v(Xuθ,t)∥2+f(Xuθ,t)(cid:1) dt+g(Xuθ)(cid:3)
|
θ 0 2 t t T v=stopgrad(uθ)
=E(cid:2) ∇ (cid:0)(cid:82)T (cid:0)1∥v(Xuθ,t)∥2+f(Xuθ,t)(cid:1) dt+g(Xuθ)(cid:1) | (cid:3) (183)
θ 0 2 t t T v=stopgrad(uθ)
=E(cid:2)(cid:82)T (∇ u )(Xuθ(ω),t)⊤σ(t)⊤a (ω)dt(cid:3) .
0 θ θ t t
Plugging the right-hand side of this equation into (182) concludes the proof.
Lemma 5. Let v be an arbitrary fixed vector field. The unique solution of the ODE
da(t;Xu,u)=−(cid:20)(cid:16) ∇ (b(Xu,t)+σ(t)u(Xu,t))(cid:17)T a(t;Xu,u)+∇ (cid:0) f(Xu,t)+ 1∥v(Xu,t)∥2(cid:1)(cid:21) , (184)
dt X tu t t X tu t 2 t
a(1;Xu,u)=∇g(Xu), (185)
1
satisfies:
a(t;Xu,u):=∇ (cid:0)(cid:82)1(cid:0)1∥u(Xu,t′)∥2+f(Xu,t′)(cid:1) dt′+g(Xu)(cid:1) ,
X tu t 2 t′ t′ 1 (186)
where Xu solves dXu =(cid:0) b(Xu,t)+σ(t)u(Xu,t)(cid:1) dt+σ(t)dB .
t t t t
Moreover, when u=u is parameterized by θ we have that
θ
∇ (cid:0)(cid:82)T (cid:0)1∥v(Xuθ,t)∥2+f(Xuθ,t)(cid:1) dt+g(Xuθ)(cid:1) =(cid:82)T (∇ u )(Xuθ(ω),t)σ(t)⊤a (ω)dt. (187)
θ 0 2 t t T 0 θ θ t t
Proof. We use an approach based on Lagrange multipliers which mirrors and extends the derivation of
the adjoint ODE (Domingo-Enrich et al., 2023, Lemma 8). For shortness, we use the notation ˜b (x,t) :=
θ
b(x,t)+σ(t)u (x,t). Define a process a:Ω×[0,T]→Rd such that for any ω ∈Ω, a(ω,·) is differentiable.
θ
For a given ω ∈Ω, we can write
(cid:82)T (cid:0)1∥v(Xuθ,t)∥2+f(Xuθ,t)(cid:1) dt+g(Xuθ)
0 2 t t T
=(cid:82)T (cid:0)1∥v(Xuθ,t)∥2+f(Xuθ,t)(cid:1) dt+g(Xuθ) (188)
0 2 t t T
−(cid:82)T ⟨a (ω),(dXuθ(ω)−˜b (Xuθ(ω),t)dt−σ(t)dB )⟩.
0 t t θ t t
By stochastic integration by parts (Domingo-Enrich et al., 2023, Lemma 9), we have that
(cid:82) 0T ⟨a t(ω),dX tuθ(ω)⟩=⟨a T(ω),X Tuθ(ω)⟩−⟨a 0(ω),X 0uθ(ω)⟩−(cid:82) 0T ⟨X tuθ(ω),d da tt(ω)⟩dt. (189)
45Hence, if Xuθ =x is the initial condition, we have that8
0 0
∇
(cid:0)(cid:82)T (cid:0)1∥v(Xuθ,t)∥2+f(Xuθ,t)(cid:1) dt+g(Xuθ)(cid:1)
x0 0 2 t t T
=∇ (cid:0)(cid:82)T (cid:0)1∥v(Xuθ,t)∥2+f(Xuθ,t)(cid:1) dt+g(Xuθ)
x0 0 2 t t T
−⟨a T(ω),X Tuθ(ω)⟩+⟨a 0(ω),X 0uθ(ω)⟩+(cid:82) 0T (cid:0) ⟨a t(ω),˜b θ(X tuθ(ω),t)⟩+⟨d da tt(ω),X tuθ(ω)⟩(cid:1) dt
(cid:82)T (cid:1)
+ ⟨a (ω),σ(t)dB ⟩
0 t t
=(cid:82)T ∇ Xuθ(ω)⊤∇ (cid:0)1∥v(Xuθ,t)∥2+f(Xuθ(ω),t)(cid:1) dt+∇ Xuθ(ω)⊤∇ g(Xuθ(ω)) (190)
0 x0 t x 2 t t x0 T x T
−∇ Xuθ(ω)⊤a (ω)+∇ Xuθ(ω)⊤a (ω)
x0 T T x0 0 0
+(cid:82) 0T (cid:0) ∇ x0X tuθ(ω)⊤∇ x˜b θ(X tuθ(ω),t)⊤a t(ω)+∇ x0X tuθ(ω)⊤d da tt(ω)(cid:1) dt
=(cid:82) 0T ∇ x0X tuθ(ω)⊤(cid:0) ∇ x(cid:0)1 2∥v(X tuθ,t)∥2+f(X tuθ(ω),t)(cid:1) +∇ x˜b θ(X tuθ(ω),t)⊤a t(ω)+ d da tt(ω)(cid:1) dt
+∇ Xuθ(ω)⊤(cid:0) ∇ g(Xuθ(ω))−a (ω)(cid:1) +a (ω).
x0 T x T T 0
In the last line we used that ∇ Xuθ(ω)=∇ x =I. If choose a such that
x0 0 x0 0
da (ω)=(cid:0) −∇ ˜b (Xuθ(ω),t)⊤a (ω)−∇ (cid:0)1∥v(Xuθ,t)∥2+f(Xuθ(ω),t)(cid:1)(cid:1) dt,
t x θ t t x 2 t t (191)
a (ω)=∇ g(Xuθ(ω)),
T x T
which is the ODE (184)-(185), then we obtain that
∇ (cid:0)(cid:82)T (cid:0)1∥v(Xuθ,t)∥2+f(Xuθ,t)(cid:1) dt+g(Xuθ)(cid:1) =a (ω) (192)
x0 0 2 t t T 0
Without loss of generality, this argument can be extended from t=0 to an arbitrary t∈[0,1], which proves
the first statement of the lemma.
To prove (187), we similarly write
∇
(cid:0)(cid:82)T (cid:0)1∥v(Xuθ,t)∥2+f(Xuθ,t)(cid:1) dt+g(Xuθ)(cid:1)
θ 0 2 t t T
=∇ (cid:0)(cid:82)T (cid:0)1∥v(Xuθ,t)∥2+f(Xuθ,t)(cid:1) dt+g(Xuθ)
θ 0 2 t t T
−⟨a T(ω),X Tuθ(ω)⟩+⟨a 0(ω),X 0uθ(ω)⟩+(cid:82) 0T (cid:0) ⟨a t(ω),˜b θ(X tuθ(ω),t)⟩+⟨d da tt(ω),X tuθ(ω)⟩(cid:1) dt
(cid:82)T (cid:1)
+ ⟨a (ω),σ(t)dB ⟩
0 t t
=(cid:82)T ∇ Xuθ(ω)⊤∇ (cid:0)1∥v(Xuθ,t)∥2+f(Xuθ(ω),t)(cid:1) dt+∇ Xuθ(ω)⊤∇ g(Xuθ(ω)) (193)
0 θ t x 2 t t θ T x T
−∇ Xuθ(ω)⊤a (ω)+∇ Xuθ(ω)⊤a (ω)
θ T T θ 0 0
+(cid:82) 0T (cid:0) ∇ θX tuθ(ω)⊤∇ x˜b θ(X tuθ(ω),t)⊤a t(ω)+∇ θ˜b θ(X tuθ(ω),t)⊤a t(ω)+∇ θX tuθ(ω)⊤d da tt(ω)(cid:1) dt
=(cid:82) 0T ∇ θX tuθ(ω)⊤(cid:0) ∇ x(cid:0) 21∥v(X tuθ,t)∥2+f(X tuθ(ω),t)(cid:1) +∇ x˜b θ(X tuθ(ω),t)⊤a t(ω)+ d da tt(ω)(cid:1) dt
+∇ Xuθ(ω)⊤(cid:0) ∇ g(Xuθ(ω))−a (ω)(cid:1) +(cid:82)T (∇ ˜b )(Xuθ(ω),t)⊤a (ω)dt.
θ T x T T 0 θ θ t t
In the last line we used that ∇ Xuθ(ω)=∇ x=0. When a satisfies (191), we obtain that
θ 0 θ
∇
(cid:0)(cid:82)T (cid:0)1∥v(Xuθ,t)∥2+f(Xuθ,t)(cid:1) dt+g(Xuθ)(cid:1)
θ 0 2 t t T (194)
=(cid:82)T (∇ ˜b )(Xuθ(ω),t)a (ω)dt=(cid:82)T (∇ u )(Xuθ(ω),t)⊤σ(t)⊤a (ω)dt.
0 θ θ t t 0 θ θ t t
The last equality holds because˜b (x,t):=b(x,t)+σ(t)u (x,t).
θ θ
8Unlike(Domingo-Enrichetal.,2023,Lemma8),weusetheconventionthataJacobianmatrixJ =∇xv(x)isdefinedas
Jij = ∂v ∂i x( jx). Theirdefinitionof∇xv isthetransposeofours.
46E.2 Proof of Proposition 2: Theoretical guarantees of the basic Adjoint Matching
loss
Let u¯=stopgrad(u ). We can rewrite equation (32) as:
θ
∇ L(u ;Xu¯)= 1(cid:82)1 ∇ ∥u (Xu¯,t)∥2dt+(cid:82)1 ∇ u(Xu¯,t)Tσ(t)Ta(t;Xu¯,u¯)dt (195)
θ θ 2 0 θ θ t 0 θ t
= 1(cid:82)1 ∇ ∥u (Xu¯,t)+σ(t)Ta(t;Xu¯,u¯)∥2dt=∇ L (u ;Xu¯) (196)
2 0 θ θ t θ Basic−Adj−Match θ
This proves the first statement of the proposition. To prove that the only critical point of the expected
basic Adjoint Matching loss is the optimal control, we first compute the first variation of E[L ].
Basic−Adj−Match
Letting v :Rd×[0,T]→Rd be arbitrary, we have that
dE[L (u+ϵv;Xu¯)]= dE(cid:2)1(cid:82)T ∥(u+ϵv)(Xu¯,t)+σ(t)⊤a(t,Xu¯,u¯)∥2dt(cid:3)
dϵ Basic−Adj−Match dϵ 2 0 t
=E(cid:2)(cid:82)T ⟨v(Xu¯,t),u(Xu¯,t)+σ(t)⊤a(t,Xu¯,u¯)⟩dt(cid:3)
0 t t (197)
=E(cid:2)(cid:82)T ⟨v(Xu¯,t),u(Xu¯,t)+σ(t)⊤E(cid:2) a(t,Xu¯,u¯)|Xu¯(cid:3) ⟩dt(cid:3)
0 t t t
=⇒ δ E[L (u)(x,t)=u(x,t)+E(cid:2) a(t,Xu¯,u¯)|Xu¯ =x(cid:3)
δu Basic−Adj−Match t
Hence, critical points satisfy that
u(x,t)=−σ(t)⊤E[a(t,Xu,u)|Xu =x]=−σ(t)⊤E(cid:2) ∇ (cid:82)T (cid:0)1∥v(Xv,t)∥2+f(Xv,t)(cid:1) dt+g(Xv)|Xv =x(cid:3)
t X tv t 2 t t T 0
=−σ(t)⊤∇ E(cid:2)(cid:82)T (cid:0)1∥v(Xv,t)∥2+f(Xv,t)(cid:1) dt+g(Xv)|Xv =x(cid:3) =−σ(t)⊤∇J(u;x,t),
x t 2 t t T 0
(198)
In this equation, the second equality holds by equation (186) from Lemma 5, and the third equality holds by
the Leibniz rule.
Lemma 6 shows that any control u that satisfies (198) is equal to the optimal control, which concludes the
proof.
Lemma 6. Suppose that for any x∈Rd, t∈[0,T], u(x,t)=−σ(t)⊤∇ J(u;x,t). Then, J(u;·,·) satisfies the
x
Hamilton-Jacobi-Bellman equation (158). By the uniqueness of the solution to the HJB equation, we have
that J(u;x,t)=V(x,t) for any x∈Rd, t∈[0,T]. Hence, u(x,t)=−σ(t)⊤∇ V(x,t) is the optimal control.
x
Proof. Since J(u;x,t)=E(cid:2)(cid:82)T (cid:0)1∥u(Xu,t)∥2+f(Xu,t)(cid:1) ds+g(Xu)|Xu =x(cid:3), we have that
t 2 t t T t
J(u;x,t)=E(cid:2) J(u;Xu ,t+∆t)|X =x(cid:3) +E(cid:2)(cid:82)t+∆t(cid:0)1∥u(Xu,t)∥2+f(Xu,t)(cid:1) ds|X =x(cid:3) , (199)
t+∆t t t 2 t t t
which means that
E[J(u;Xu ,t+∆t)|X =x]−J(u;x,t) E(cid:2)(cid:82)t+∆t(cid:0)1∥u(Xu,t)∥2+f(Xu,t)(cid:1) ds|X =x(cid:3)
0= t+∆t t + t 2 t t t (200)
∆t ∆t
Recall that the generator Tu of the controlled SDE (13) takes the form:
(cid:2) (cid:3)
Tuf(x,t):=lim E f(X tu +∆t,t)|Xt=x −f(x,t)
∆t→0 ∆t (201)
=∂
f(x,t)+⟨∇f(x,t),b(x,t)+σ(t)u(x,t)⟩+Tr(cid:0)σ(t)σ(t)⊤ ∇2f(x,t)(cid:1)
t 2
Hence, if we take the limit ∆t→0 on equation (200), we obtain that:
0=TuJ(u;x,t)+ 1∥u(x,t)∥2+f(x,t)
2 (202)
=∂ J(u;x,t)+⟨∇J(u;x,t),b(x,t)+σ(t)u(x,t)⟩+Tr(cid:0)σ(t)σ(t)⊤ ∇2J(u;x,t)(cid:1) + 1∥u(x,t)∥2+f(x,t).
t 2 2
Now using that u(x,t)=−σ(t)⊤∇ J(u;x,t), we have that
x
⟨∇J(u;x,t),σ(t)u(x,t)⟩+ 1∥u(x,t)∥2 =−∥σ(t)⊤∇ J(u;x,t)∥2+ 1∥σ(t)⊤∇ J(u;x,t)∥2
2 x 2 x (203)
=−1∥σ(t)⊤∇ J(u;x,t)∥2.
2 x
47Plugging this back into (202), we obtain that
0=∂ J(u;x,t)+⟨∇J(u;x,t),b(x,t)⟩+Tr(cid:0)σ(t)σ(t)⊤ ∇2J(u;x,t)(cid:1) − 1∥σ(t)⊤∇ J(u;x,t)∥2+f(x,t). (204)
t 2 2 x
And since J(u;x,T)=g(x) by construction, we conclude that J(u;x,t) satisfies the HJB equation (158).
E.3 Theoretical guarantees of the Adjoint Matching loss
Proposition 7 (Theoretical guarantee of the Adjoint Matching loss). The only critical point of the loss
E[L ] is the optimal control u∗.
Adj−Match
Proof. If a˜(t,Xv):=a˜(ω,t) is the solution of the Lean Adjoint ODE (38)-(39), we obtain that E[a˜(t,Xv)|Xv]
t
is a solution of
dE[a˜(t, dX tv)|X tv] =−∇ xb(X tv,t)E[a˜(t,Xv)|X tv]−∇ xf(X tv,t), (205)
E[a˜(T,Xv)|Xv]=∇g(Xv), (206)
T T
Let us rewrite E[L ] as follows:
Adj−Match
E[L Adj−Match(u)]:=E(cid:2)(cid:82) 0T (cid:13) (cid:13)u(X tv,t)+σ(t)⊤E(cid:2) a˜(t,Xv)|X tv(cid:3)(cid:13) (cid:13)2 dt(cid:3) |
v=stopgrad(u) (207)
+E(cid:2)(cid:82) 0T (cid:13) (cid:13)σ(t)⊤(cid:0)E(cid:2) a˜(t,Xv)|X tv(cid:3) −a˜(t,Xv)(cid:1)(cid:13) (cid:13)2 dt(cid:3) | v=stopgrad(u),
Now, suppose that uˆ is a critical point of E[L ]. By definition, this implies that the first variation of
Adj−Match
E[L ] is zero. Using (207), we can write this as follows:
Adj−Match
0= δ E[L (uˆ)](x)=2(cid:0) uˆ(x,t)+σ(t)⊤E[a˜(t,Xuˆ)|Xuˆ =x](cid:1) , (208)
δu Adj−Match t
=⇒ uˆ(x,t)=−σ(t)⊤E[a˜(t,Xuˆ)|Xuˆ =x]. (209)
t
Hence, we have
−∇ uˆ(Xuˆ,t)σ(t)⊤E[a˜(t,Xuˆ)|Xuˆ]−∇ uˆ(Xuˆ,t)uˆ(Xuˆ,t)=0. (210)
x t t x t t
Adding this to the right-hand side of (205), we obtain that E[a˜(t,Xuˆ)|Xuˆ] also solves the ODE
t
dE[a˜(t,X dtuˆ)|X tuˆ] =−∇ x(b(X tuˆ,t)+σ(t)uˆ(X tuˆ,t))E[a˜(t,Xuˆ)|X tuˆ]
(211)
−∇ (f(Xuˆ,t)+ 1∥uˆ(Xuˆ,t)∥2),
x t 2 t
E[a˜(T,Xuˆ)|Xuˆ]=∇g(Xuˆ), (212)
T T
If a(t,Xuˆ):=a(ω,t) is the solution of the Adjoint ODE (30)-(31), we obtain that E[a(t,Xuˆ)|Xuˆ] is a solution
t
of
dE[a(t,X dtuˆ)|X tuˆ] =−∇ x(b(X tuˆ,t)+σ(t)uˆ(X tuˆ,t))E[a(t,Xuˆ)|X tuˆ]
(213)
−∇ (f(Xuˆ,t)+ 1∥uˆ(Xuˆ,t)∥2),
x t 2 t
E[a(T,Xuˆ)|Xuˆ]=∇g(Xuˆ), (214)
T T
Remark that (213)-(214) is the same ODE as (211)-(212). By uniqueness of ODE solutions, we obtain that
E[a˜(t,Xuˆ)|Xuˆ]=E[a(t,Xuˆ)|Xuˆ] for all t∈[0,T] when uˆ is a critical point of E[L ]. Since we can
t t Adj−Match
reexpress the basic Adjoint Matching loss as
E[L Basic−Adj−Match(u)]:=E(cid:2)(cid:82) 0T (cid:13) (cid:13)u(X tv,t)+σ(t)⊤E(cid:2) a(t,Xv)|X tv(cid:3)(cid:13) (cid:13)2 dt(cid:3) |
v=stopgrad(u) (215)
+E(cid:2)(cid:82) 0T (cid:13) (cid:13)σ(t)⊤(cid:0)E(cid:2) a(t,Xv)|X tv(cid:3) −a(t,Xv)(cid:1)(cid:13) (cid:13)2 dt(cid:3) | v=stopgrad(u),
we obtain that when uˆ is a critical point of L ,
Adj−Match
d E[L (uˆ)](x)=2(cid:0) uˆ(x,t)+σ(t)⊤E[a(t,Xuˆ)|Xuˆ =x](cid:1) (216)
du Basic−Adj−Match t
=2(cid:0) uˆ(x,t)+σ(t)⊤E[a˜(t,Xuˆ)|Xuˆ =x](cid:1) =0. (217)
t
Thus, we deduce that the critical points of E[L ] are critical points of E[L ]. By
Adj−Match Basic−Adj−Match
Proposition 2, E[L ] has a single critical point, which is the optimal control u∗, which concludes
Basic−Adj−Match
the proof of the statement for E[L ].
Adj−Match
48E.4 Pseudo-code of Adjoint Matching for DDIM fine-tuning
Algorithm 2 Adjoint Matching for fine-tuning DDIM
Input: Pre-trained denoiser ϵbase, number of fine-tuning iterations N.
Initialize fine-tuned denoiser: ϵfinetune =ϵbase with parameters θ.
for n∈{0,...,N −1} do
Sample m trajectories X =(X ) according to DDPM, e.g.:
t t∈{0,...,1}
X
k+1
=(cid:113) α¯ αk ¯+ k1(cid:0) X k− 1− √α¯ 1k −/α α¯ ¯k k+1ϵfinetune(X k,k)(cid:1) +(cid:113) 1− 1−α¯ αk ¯+ k1(cid:0) 1− α¯α k¯ +k 1(cid:1) ε k, ε
k
∼N(0,I), X
0
∼N(0,I),
(218)
(cid:113)
or X
k+1
=X k+ α¯k+ 2α1 ¯− kα¯kX k− αα ¯¯ kk√+1 1− −α α¯ ¯k kϵfinetune(X k,k)+ α¯k+ α¯1 k−α¯kε k. (219)
For each trajectory, solve the lean adjoint ODE (38)-(39) backwards in time from k=K to 0, e.g.:
a˜
k
=a˜ k+1+a˜T k+1∇
Xk(cid:18)(cid:113)
α¯ αk ¯+ k1(cid:0) X k− 1− √α¯ 1k −/α α¯ ¯k k+1ϵbase(X k,k)(cid:1) −X
k(cid:19)
, a˜
K
=∇ XKr(X K), (220)
(cid:16) (cid:17)
or a˜
k
=a˜ k+1+a˜T k+1∇
Xt
α¯k+ 2α1 ¯− kα¯kX k− αα ¯¯ kk√+1 1− −α α¯ ¯k kϵbase(X k,k) , a˜
K
=∇ XKr(X K). (221)
Note that X and a˜ should be computed without gradients, i.e., X =stopgrad(X ), a˜ =stopgrad(a˜ ).
k k k k k k
For each trajectory, compute the Adjoint Matching objective (37):
L Adj−Match(θ)=(cid:80) k∈{0,...,K−1}(cid:13) (cid:13)(cid:113) α¯k(1α¯ −k+ α¯1 k+1)(cid:0) 1− α¯α k¯ +k 1(cid:1) (ϵfinetune(X k,k)−ϵbase(X k,k))
(222)
−(cid:113) 1− 1−α¯ αk ¯+ k1(cid:0) 1− α¯α k¯ +k 1(cid:1) a˜ k(cid:13) (cid:13)2,
or L Adj−Match(θ)=(cid:80) k∈{0,...,K−1}(cid:13) (cid:13)(cid:113) αα ¯¯ kk+ (11 −− α¯α¯ kk )(ϵfinetune(X k,k)−ϵbase(X k,k))−(cid:113) α¯k+ α¯1 k−α¯ka˜ k(cid:13) (cid:13)2. (223)
Compute the gradient ∇ L(θ) and update θ using favorite gradient descent algorithm.
θ
end
Output: Fine-tuned vector field vfinetune
Note that for each pair of equations (218)-(219), (220)-(221), (222)-(223), the first equation corresponds to
the updates in the DDPM paper, while the second equation is an Euler-Maruyama / Euler discretization of
the continuous-time object. To check that both discretizations are equal up to first order, remark that
(cid:113) (cid:113)
α¯k+1 = 1+ α¯k+1−α¯k ≈1+ α¯k+1−α¯k +O((α¯ −α¯ )2). (224)
α¯k α¯k 2α¯k k+1 k
F Adapting diffusion fine-tuning baselines to flow matching
F.1 Adapting ReFL (Xu et al., 2023) to flow matching
Reward Feedback Learning (ReFL) is a diffusion fine-tuning algorithm introduced by Xu et al. (2023) which
tries to increase the reward on denoised samples. Namely, if X = (X ) is the solution of the DDPM
t t∈[0,1]
SDE (7), we can denoise X as
t
√
Xˆ 1(X t)= Xt− 1 √− αα ¯¯ ttϵ(Xt,t). (225)
This equation follows from the stochastic interpolant equation (2) if we replace X¯ with the noise predictor
0
ϵ(X ,t). And then, the ReFL optimization update is based on the gradient:
t
√
∇ θr(Xˆ 1(X t))=∇ θr(cid:0)Xt− 1− √α¯ α¯t tϵθ(Xt,t)(cid:1) , (226)
where the trajectories have been detached.
49To adapt ReFL to Flow Matching, we need to express the denoiser map in terms of the vector field v. We
have that
v(x,t)=E(cid:2) β˙ tX¯ 0+α˙ tX¯ 1(cid:12) (cid:12)β tX¯ 0+α tX¯
1
=x(cid:3) =E(cid:2)β β˙ t t(cid:0) β tX¯ 0+α tX¯ 1(cid:1) +(cid:0) α˙ t− β β˙ t tα t(cid:1) X¯ 1(cid:12) (cid:12)β tX¯ 0+α tX¯
1
=x(cid:3)
= β˙ tx+(cid:0) α˙ − β˙ tα (cid:1) Xˆ (x,t).
βt t βt t 1
(227)
where we defined the denoiser map Xˆ (x,t):=E(cid:2) X¯ |β X¯ +α X¯ =x(cid:3). Hence,
1 1 t 0 t 1
Xˆ 1(x,t)= v α( ˙x t, −t)
β
β−
˙t
tββ α˙ tt tx . (228)
F.2 Adapting Diffusion-DPO (Wallace et al., 2023a) to flow matching
The Diffusion-DPO loss assumes access to ranked pairs of generated samples xw ≻xl, where xw and xl are
1 1
the winning and losing samples. For DDPM, the loss implemented in practice reads (Wallace et al., 2023a,
Eq. 46):
L (θ)=−E (cid:2)
DPO (xw,xl)∼D,k∼U[0,K],xw ∼q(xw |xw),xl∼q(xl |xl)
1 1 kh kh 1 t kh 1
logS(cid:0) − β˜(cid:0) ∥εw−ϵ (xw ,kh)∥2−∥εw−ϵ (xw ,kh)∥2 (229)
2 θ kh ref kh
−(cid:0) ∥εl−ϵ (xl ,kh)∥2−∥εl−ϵ (xl ,kh)∥2(cid:1)(cid:1)(cid:1)(cid:3) ,
θ kh ref kh
where S(x)= 1 denotes the sigmoid function, and q(x∗ |x∗) is the conditional distribution of the forward
1+e−x √ √ kh 1
process, i.e. x∗ is sampled as x∗ = γ x∗ + 1−γ ϵ, ϵ ∼ N(0,I). Following the derivation of the
kh kh kh 1 kh
Diffusion-DPO loss in (Wallace et al., 2023a, Sec. S4), we observe that the term −β˜ ∥εw−ϵ (xw ,kh)∥2 arises
2 θ kh
from
− β˜ ∥xˆ (xw )−xw∥2, (230)
21−γkh 1 kh 1
γkh
up to a constant term in θ. If we switch to the more general flow matching scheme, the analog of this term is
− β˜ ∥xˆ (xw )−xw∥2. (231)
2βk2
h
1 kh 1
α2
kh
Using the expression of the denoiser map in terms of the vector field v in equation (228), we can rewrite (231)
as:
− β˜ (cid:13) (cid:13)v(xw kh,kh)− ββ˙ kk hhxw kh −xw(cid:13) (cid:13)2 =−β˜(cid:13) (cid:13)v(xw kh,kh)−β β˙ k kh hxw kh − αkhxw(cid:13) (cid:13)2 . (232)
2 αβk2
2
kh
h
α˙kh−β β˙ k kh hαkh 1 2 α α˙k kh hβkh−β˙ kh βkh 1
Thus, the Diffusion-DPO loss for Flow Matching reads
L (θ)=−E (cid:2)
DPO (xw,xl)∼D,k∼U[0,K],xw ∼q(xw |xw),xl∼q(xl |xl)
1 1 kh kh 1 t kh 1
logS(cid:0) − β 2˜(cid:0)(cid:13) (cid:13)vθ(x αα˙w k kkh hh,k βh k) h− −β β β˙ ˙k k kh h hxw kh − α βkk hhxw 1(cid:13) (cid:13)2 −(cid:13) (cid:13)vref(x α α˙w k k kh h h, βk kh h) −− ββ β ˙˙ kk k hh hxw kh − α βkk hhxw 1(cid:13) (cid:13)2 (233)
−(cid:0)(cid:13) (cid:13)vθ(xl kh,kh)− ββ˙ kk hhxl kh − αkhxl(cid:13) (cid:13)2 −(cid:13) (cid:13)vref(xl kh,kh)− ββ˙ kk hhxl kh − αkhxl(cid:13) (cid:13)2(cid:1)(cid:1)(cid:1)(cid:3) ,
αα˙ kk hhβkh−β˙
kh
βkh 1 α α˙k kh hβkh−β˙
kh
βkh 1
(Wallace et al., 2023a, Sec. 5.1) claim that β ∈[2000,5000] yields good performance on Stable Diffusion 1.5
and Stable Diffusion XL-1.0, which if we translate to our notation corresponds to β˜∈[4000,10000].
Whenwehaveaccesstotherewardfunctionr,insteadofawinningsamplexw andalosingsamplexl,wehave
1 1
a pair of samples (xa 1,xb 1) with winning weights S(r(xa 1)−r(xb 1))= (cid:0) 1 (cid:1), S(−(r(xa 1)−r(xb 1)))=
1+exp r(xb)−r(xa)
1 1
501 . Hence, the loss (233) becomes:
(cid:0) (cid:1)
1+exp −(r(xb)−r(xa))
1 1
(cid:20)
L (θ)=−E (cid:80) S(cid:0) s(r(xa)−r(xb))(cid:1) ×
DPO (xa,xb)∼D,k∼U[0,K],xa ∼q(xa |xa),xb∼q(xb |xb) s∈{±1} 1 1
1 1 kh kh 1 t kh 1
logS(cid:0) − sβ˜(cid:0)(cid:13) (cid:13)vθ(xa kh,kh)− ββ˙ kkh hxa kh − αkhxa(cid:13) (cid:13)2 −(cid:13) (cid:13)vref(xa kh,kh)− ββ˙ kk hhxa kh − αkhxa(cid:13) (cid:13)2 (234)
2 α α˙k kh hβkh−β˙
kh
βkh 1 α α˙k kh hβkh−β˙
kh
βkh 1
−(cid:0)(cid:13) (cid:13)vθ(xb kh,kh)− ββ˙ kk hhxb kh − αkhxb(cid:13) (cid:13)2 −(cid:13) (cid:13)vref(xb kh,kh)− ββ˙ kk hhxb kh − αkhxb(cid:13) (cid:13)2(cid:1)(cid:1)(cid:1)(cid:21) ,
α α˙ kk hhβkh−β˙
kh
βkh 1 α α˙k kh hβkh−β˙
kh
βkh 1
G Experimental details
Unless otherwise specified, we used the same hyperparameters across all fine-tuning methods. Namely, we
used:
• K =40 timesteps.
• Adam optimizer with learning rate 2×10−5 and parameters β =0.95, β =0.999, ϵ=1×10−8, weight
1 2
decay 1×10−2, gradient norm clipping value 1. For Discrete Adjoint, these hyperparameters resulted in
fine-tuning instability (see Table 5); the results that we report in all other tables for Discrete Adjoint
were obtained with learning rate 1×10−5.
• Bfloat16 precision.
• Effective batch size 40; for each run we used two 80GB A100 GPUs with batch size 20 each.
• A set of 40k fine-tuning prompts taken from a licensed dataset consisting of text and image pairs
(note that we disregarded the images). Thus, each epoch lasts 1000 iterations; see the total amount of
fine-tuning iterations for each algorithm in Table 3. For each of the three runs that we perform for each
data point that we report, the set of 40k prompts is sampled independently among a total set of 100k
prompts.
G.1 Noise schedule details
Since we use K = 40 discretization steps, the timesteps are t ∈ {0,0.025,0.05,0.075,0.1,...,0.95,0.975}.
To sample X from X we use equation (40). We use the choices α = t, β = 1−t, which means that
t+h t t t
(cid:113) (cid:113) (cid:113)
σ(t)= 2β t(α α˙t tβ t−β˙ t)= 2(1−t)(1− tt +1)= 2(1 t−t).
Note that if we plug t=0 into this expression, we obtain infinity, and if we plug t⪅1, we obtain σ(t)≈0.
For obvious reasons, the former issue requires a fix: we simply add a small offset to the denominator of σ(t),
replacing (cid:112) 1/t by (cid:112) 1/(t+h) (note that h := 1/K = 0.025). But the latter issue is also not completely
satisfactory from a practical standpoint, because looking at the adjoint matching loss (37), we observe that
u(Xu¯,t)istrainedtoapproximatetheconditionalexpectationofσ(t)Ta˜(t;Xu¯). Thus, ifwesetσ(t)veryclose
t
to zero for t⪅1, we are forcing the control u to be close to zero as well, or equivalently preventing vfinetune
from deviating from vbase. While this is the right thing to do from a theoretical perspective, we concluded
experimentally that setting σ(t) just slightly larger results in substantially faster fine-tuning, thanks to the
additional leeway provided to vfinetune to deviate from vbase. In particular, we added a small offset to the
factor 1−t in the numerator 1−t of σ(t): we replaced 1−t by 1−t+h. Thus, the expression that we used
to compute the diffusion coefficient in our experiments is
(cid:113)
σ(t)= 2(1−t+h). (235)
t+h
When solving the lean adjoint ODE (38)-(39) backwards in time via the Euler scheme (41), the timesteps
we use are t ∈ {1,0.975,0.95,0.925,0.9,...,0.05,0.025}. We do not actually initialize the adjoint state as
∇ g(X ), but rather as ∇ g(Xˆ ), where Xˆ := X +hvbase(X ,1−h). That is, Xˆ is obtained by
x 1 x 1 1 1−h 1−h √ 1
performing a final noiseless update, instead of using noise σ(1−h) = 4h given by equation (235). The
reason for this is that the regular final iterate X contains some noise that was added in the final step, and
1
51that can distort the gradient ∇ g(X ). By setting a˜(1;X)=∇ g(X ), we get rid of this bias. Note that in
x 1 x 1
the continuous time limit h→0, Xˆ =X , which means that this small trick is consistent.
1 1
G.2 Selection of gradient evaluation timesteps
In Algorithm 1, equation (42), we state that the term (cid:13) (cid:13) σ(2 t)(cid:0) v θfinetune(X t,t)−vbase(X t,t)(cid:1) +σ(t)a˜ t(cid:13) (cid:13)2 must be
computed for all K steps in {0,...,1−h}. However, the gradient signal provided by backpropagating through
this expression for consecutive times t and t+h is quite similar. In the interest of computational efficiency, we
sample a subset K of timesteps, and we only compute and backpropagate the terms (cid:13) (cid:13) σ(2 t)(cid:0) v θfinetune(X t,t)−
vbase(X t,t)(cid:1) +σ(t)a˜ t(cid:13) (cid:13)2 for those timesteps. We construct K by sampling ten timesteps uniformly without
repetition among {0,...,0.725}, and always sampling the last ten timesteps {0.75,...,0.975}. This is because
fine-tuning the last ten steps (25% of the total) well is critical for good empirical performance, while the
initial steps are not as important.
G.3 Loss function clipping: the LCT hyperparameter
Note that the magnitude of σ(t)Ta(t;Xu¯,u¯) is much larger for times t⪆0 than for times t⪅1. The reason is
two-fold:
• As discussed in Appendix G.1, σ(t) is much larger for t⪆0 than for t⪅1.
• The magnitude of the lean adjoint state a˜ grows roughly exponentially as t goes backward in time. In
fact, if we assumed that ∇ b(X ,t) is constant in time, this statement would be exact.
x t
Observe that when σ(t)Ta(t;Xu¯,u¯) is large, the gradient ∇ θ(cid:13) (cid:13) σ(2 t)(cid:0) v θfinetune(X t,t)−vbase(X t,t)(cid:1) +σ(t)a˜ t(cid:13) (cid:13)2
also has a high magnitude. Including such terms in our gradient computation decreases the signal to noise
ratio of the gradient. Even more so, as discussed in Appendix G.2 for good practical performance it is critical
to get a good gradient signal from the last 25% steps. Hence, including the high-magnitude terms for t⪅0 in
our gradients can muffle these other important, low-magnitude terms.
To fix this issue, we clip the terms such that (cid:13) (cid:13) σ(2 t)(cid:0) v θfinetune(X t,t)−vbase(X t,t)(cid:1) +σ(t)a˜ t(cid:13) (cid:13)2 >LCT, where
LCT stands for the loss clipping threshold. That is, the adjoint matching loss that we use in our experiments
is of the form:
Lˆ Adj−Match(θ)=(cid:80) t∈Kmin(cid:8) LCT,(cid:13) (cid:13) σ(2 t)(cid:0) v θfinetune(X t,t)−vbase(X t,t)(cid:1) +σ(t)a˜ t(cid:13) (cid:13)2(cid:9) , (236)
where K is the random timestep subset described in Appendix G.2.
For adjoint matching, we set LCT = 1.6×λ2. Remark that LCT needs to grow quadratically with λ,
because the magnitude of the lean adjoint a˜ grows quadratically with λ. We set the constant 1.6 through
experimentation; all or almost all of the terms for the last ten timesteps fall below LCT, but only a fraction of
the terms (≈25%) for the first ten steps fall below LCT. The constant for LCT is a relevant hyperparameter
that needs to be tuned to obtain a similar behavior.
We also used loss function clipping on the continuous adjoint loss. For that loss we set LCT = 1600×λ2.
The reason is that the magnitude of the regular adjoint states is significantly larger than the magnitude of the
lean adjoint states (which is a big reason why adjoint matching outperforms the continuous adjoint).
G.4 Computation of evaluation metrics
We used the open_clip library (Ilharco et al., 2021) to compute ClipScores. We computed ClipScore diversity
as the variance of Clip embeddings of 40 generations for a given prompt, averaged across 25 prompts. Namely,
ClipScore_Diversity= 1 (cid:80)40 2 (cid:80) ∥Clip(gk)−Clip(gk)∥2, (237)
40 k=1 25·24 1≤i<j≤25 i j
where gk denotes the i-th generation for the k-th prompt.
i
We used the transformers library to compute the PickScore processor and model (Kirstain et al., 2023a).
PickScore diversity is computed in analogy with ClipScore diversity.
52We used the hps library to compute values of Human Preference Score v2 (Wu et al., 2023a).
TocomputeDreamsimdiversityweusethedreamsimlibrary(Fuetal.,2023). Dreamsimdiversityiscomputed
in analogy with ClipScore diversity.
53