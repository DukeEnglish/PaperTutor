A Bayesian Approach to Clustering via the
Proper Bayesian Bootstrap: the Bayesian Bagged
Clustering (BBC) algorithm
Federico Maria Quetti1, Silvia Figini2, and Elena Ballante2
1 Department of Mathematics, University of Pavia, Italy
2 Department of Political and Social sciences, University of Pavia, Italy
Abstract. The paper presents a novel approach for unsupervised tech-
niques in the field of clustering. A new method is proposed to enhance
existing literature models using the proper Bayesian bootstrap to im-
prove results in terms of robustness and interpretability.
Our approach is organized in two steps: k-means clustering is used for
prior elicitation, then proper Bayesian bootstrap is applied as resam-
pling method in an ensemble clustering approach. Results are analyzed
introducingmeasuresofuncertaintybasedonShannonentropy.Thepro-
posalprovidesclearindicationontheoptimalnumberofclusters,aswell
as a better representation of the clustered data. Empirical results are
provided on simulated data showing the methodological and empirical
advances obtained.
Keywords: ProperBayesianBootstrap·BayesianClustering·Bagging
· Fuzzy Clustering
1 Introduction
Cluster analysis is the field of Machine Learning (ML) that deals with parti-
tioning data by finding groups of similar units in an unsupervised framework.
Clustering techniques are widely used in various applications, where capturing
the inherent structure of data plays a pivotal role for the analysis.
Alargebodyofliteratureexistsinthefieldregardingmethodsofclustering[1–3];
yet, being the problem unsupervised, the research on improvements to existing
methods is still an open point, leaving room to further developments. In partic-
ular, the integration of ML with techniques from Bayesian statistical learning
hasbeenshowntoprovidesignificantimprovementsinthesupervisedframework
by[4]and[5].Inanunsupervisedsetting,ithasbeenshownintheliteraturethat
the application of bagging techniques to standard clustering methods improves
results and brings new information of fuzzy clustering type [6].
Theaimofthisworkistoextendthecurrentstateoftheartinclustering,adopt-
ing Bayesian Bootstrap techniques in unsupervised learning resorting to a prior
knowledge integration scheme. The rest of the paper is structured as follows:
4202
peS
31
]LM.tats[
1v45980.9042:viXra2 F.M. Quetti et al.
Section 2 reviews the existing literature on clustering, bootstrap, and proper
Bayesian bootstrap. Section 3 introduces our proposal and Section 4 reports
empirical evidence. Conclusions are drawn in Section 5.
2 Background
In this paper a traditional clustering problem is faced integrating a classical
partition based clustering algorithm with a Bayesian bootstrap method. In the
present section, a review of the background topics is described, and particular
emphasisisgiventotheBayesiannon-parametricapproachtoensemblelearning.
2.1 Bootstrap
Bootstrap methods Bootstrap is a statistical resampling technique used to
estimate the distribution of a statistic, by providing an approximation of the
empirical distribution function of data. Formally, given {X ,...,X } i.i.d. real-
1 n
ization of a random variable X, we are interested in estimating the distribution
of a functional Φ(F,X), depending on the cumulative distribution function F of
the variable X. In order to generate the distribution of the estimator Φˆ for the
functional, an approximation F∗ for the cumulative distribution of X is needed.
A first method was proposed by Efron in [7]: the approximation for the cdf
is obtained by generating replications with replacement from the sample. The
procedureconsistsindrawingaweightsvectorfortheobservationsfromaMulti-
nomial distribution, and defining the estimate for the population cdf as:
n
F∗(x)=(cid:88)w
iI[X ≤x] (1)
n i
i=1
with (w ,...,w )∼Mult(n, 11 ) and I[X ≤x] indicator function.
1 n n n i
AnalternativemethodwasproposedbyRubinin[8],calledBayesianbootstrap:
themethodissimilartoEfron’sbutmodifiesthedefinitionoftheweights,which
areobtainedbysamplingfromaDirichletdistribution.Thediscretecdfapprox-
imating the empirical is then:
n
(cid:88)
F∗(x)= w I[X ≤x] (2)
i i
i=1
where (w ,...,w )∼D(1 ).
1 n n
The two bootstrap methods are asymptotically equivalent [9] and first order
equivalent from the predictive point of view, as the conditional probability of a
new observation is estimated by only using observed values from the sample [4].
Proper Bayesian Bootstrap In order to present the bootstrap method cen-
tral to the following work, a brief digression on the theoretical foundations ofBayesian Bagged Clustering algorithm 3
Bayesiannon-parametriclearningisneeded.Theframeworkpresentedhererefers
to the univariate case.
Givenanexchangeablesequenceofrealrandomvariables{X }onaprobability
n
space (Ω,F,P), De Finetti’s Representation Theorem ensures the existence of
a random distribution F conditionally on which the variables are i.i.d with dis-
tribution F. Let Φ(F,X) be a functional depending on the random distribution
and on the sample values X. The Bayesian approach to the evaluation of the
conditional probability requires to elicit a prior distribution for F on the space
of distribution functions, with the aim of using the posterior of F to estimate
the distribution of the functional Φ given the sample values. Ferguson, in a fun-
damental paper on Bayesian approach to non-parametric statistics [10], defined
a prior for the random distribution, referred to as Dirichlet process. Given a
proper distribution function F interpreted as the prior guess at F, and a pos-
0
itive real number k interpreted as a confidence parameter in this guess, kF is
0
the parameter of the process denoted as DP(kF ). The relevance of this process
0
definition is motivated by the fact that it is conjugate: given a random sample
{x ,...,x }fromF ∼DP(kF ),theposteriorisagainaDirichletprocess,with
1 n 0
updated parameter.
F|X ∼DP((k+n)G ) (1)
n
where
k n
G = F + F (2)
n k+n 0 k+n n
The parameter of the Dirichlet process, given the data, becomes a convex com-
bination of the prior guess F and the empirical cdf F . Posterior estimations of
0 n
different functionals Φ(F,X) are then easily computable by first updating the
parameters of the prior.
The first approach to prior knowledge integration to the process of resampling
data was proposed by Muliere and Secchi in [11]: a proper distribution function
F is introduced as the baseline parameter, and the sampling is performed from
0
the posterior which is the process with baseline (k +n)G . If k → 0 then a
n
non-informative prior on F is considered, and we fall back on the proposal of
0
Rubin; if k → ∞ the parameter of the posterior Dirichlet Process is reduced
to F , so that the empirical information is of no relevance in resampling. The
0
proposal is detailed in Algorithm 1.
Algorithm 1 Proper Bayesian bootstrap
1: procedure ProperBayesianBootstrap(B,m,k,F ,L={x ,...,x })
0 1 n
2: for b in 1:B do
3: Generate m observations x∗,...,x∗ from (k+n)−1(kF +nF ).
1 m 0 n
4: Draw wb,...,wb from D(n+k,...,n+k), weights for the observations.
1 m m m
5: end for
6: end procedure4 F.M. Quetti et al.
2.2 Clustering
Clustering is the process of grouping data points together based on their sim-
ilarity with respect to certain features, aiming to uncover inherent patterns or
structureswithinthedataset.Literaturemethods[1]differontheapproachcho-
sen and with respect to the similarity definition used, the most common ones
are:
– hierarchical clustering. It involves organizing data into a tree-like structure,
where clusters are nested within one another according to a similarity mea-
sure [12];
– density-based clustering. It assigns groups based on regions of high density
separated by regions of low density [13];
– partitional clustering. Given the number of clusters, creates a partition of
data points iteratively, aiming to minimize a cost function [2].
Inthispaper,thefocusisonK-meansalgorithm[14]asrepresentativeofpartition-
based clustering methods. Chosen a number of clusters K, the cluster centroids
are picked from the data at random, and the partition is defined so that each
point is assigned to the cluster with nearest centroid µ in L2 norm. The cen-
k
troids are then updated as means of points in each component, and the assign-
ment repeated. At each step, this procedure amounts to minimizing the total
within sum of squares of the K clusters:
K
WSS =(cid:88) (cid:88) ∥x −µ ∥2 (3)
i k
k=1i∈Ck
Typically, the procedure quickly converges to a local optimum.
Another type of clustering paradigm, called fuzzy clustering, differs from the
aboveaslabelsarenotassignedasone-hotvectors.Instead,membershipsu are
k
assigned to data points and indicate the degree to which they belong to each
cluster: u ∈ [0,1],
(cid:80)K
u = 1. For fuzzy c-means [15], the counterpart
k k=1 k
of standard K-means, at each iteration the cost function to optimize given the
centroids c is J , m being an hyperparameter:
k m
(cid:80)N umx
c = i=1 ik i
k (cid:80)N um
i=1 ik
(4)
K N
(cid:88)(cid:88)
J = um∥x −c ∥2
m ik i k
k=1i=1
ThemaindifferencefromtheclassicalK-meansisthattheoutputisnotlimited
to an hard assignment, but the memberships returned give an idea of the con-
fidence in the possible assignment. Note that in the following, differently fromBayesian Bagged Clustering algorithm 5
fuzzyc-means,membershipsarenotgivenbyanoptimizationprocedure.Instead,
they are the result of the aggregation of labels obtained applying clustering on
the bootstrap replicas.
Bootstrap in clustering In this work, a bagging procedure where results
obtained for each bootstrap replica are aggregated is applied. The approach
stems from the work of [6]: the methodology linking bootstrap with clustering
is drawn from the algorithm BagClust1, that we outline. Given a learning set
L, the number of bootstrap samples B, the chosen number of clusters K, and
a clustering method P, the algorithm applies the clustering procedure P to the
originallearningsetLobtaininginitialclusterlabels.Afterwards,foreachboot-
strap sample, it applies the clustering procedure again and permutes the cluster
labels to maximize overlap with the original clustering. After B iterations, it
assigns an aggregated cluster label to each observation based on majority vote.
Moreover, the algorithm retrieves a fuzzy type of result by also recording the
clustermemberships,givinginformationontheconfidenceoftheresultinglabel.
Algorithm 2 BagClust1
1: procedure BagClust1(L={x ,...,x },B,K,P)
1 n
2: Apply the clustering P to the learning set L obtaining cluster labels:
3: P(x ;L)=yˆ for each observation x , i=1,...,n, yˆ,i=1,...,K.
i i i i
4: for b=1 to B do
5: Form the b-th bootstrap sample L =(x ,...,x ).
b b1 bn
6: Apply the procedure P to the bootstrap replica L obtaining cluster labels
b
P(x ;L ) for each observation in L .
bi b b
7: Permute the cluster labels assigned to the bootstrap learning set L for
b
maximum overlap with the original clustering labels: let S denote the set
K
of all permutations of the integers 1,...,K. Find τ ∈ S that maximizes:
b K
(cid:80)n I (P(x ;L ))=P(x ;L).
i=1 τb bi b bi
8: end for
9: For the data points, record cluster memberships as the proportion of votes
in favor of each cluster assignment: u (x ) = (cid:80) {b:xi∈Lb}Iτb(P(xi;Lb))=k. Assign a
bagged cluster label for each
observatiok
n
ii
by
majority|{ vb o:x ti e∈ :L abr} g|
max u (x ).
1≤k≤K k i
10: end procedure
3 Our methodological proposal: the Bayesian Bagged
Clustering (BBC)
The proposal of this work is twofold.
In the first part, the BBC clustering algorithm is proposed, aimed at bettering
the chosen algorithm with improved stability as well as additional information
about the uncertainty in the assignments for the dataset.
Directly from the results of the above approach, the second part of the proposal6 F.M. Quetti et al.
focusesonitsexploitationbydiscussinganoptimalchoiceschemeforthenumber
of clusters K detected in the dataset.
3.1 Clustering procedure
The BBC procedure is divided in two parts: firstly, cluster information about
data is retrieved, from which the prior F is defined; secondly, proper Bayesian
0
bootstrap is performed to find clustering results.
In the initial step, we apply the partitioning P with a chosen number of clus-
ters K on data. This information is used to define a suitable baseline prior for
the generating process underlying bootstrap resampling. F is imposed as the
0
cumulative of a suitable Gaussian mixture probability density, as follows:
K
(cid:88)
f = p f (5)
θ j µj,Σj
j=1
where θ =(p ,µ ,Σ ), j =1,...,K are the mixture parameters, and f ∼
j j j µj,Σj
N(µ ,Σ ) is the multivariate Gaussian distribution with mean µ and covari-
j j j
ance matrix Σ .
j
The shape of the prior F is determined via the parameters of the distribution
0
calculated from the first step: the mixture parameters are associated to each
component j. The weight p , 0 ≤ p ≤ 1,
(cid:80)K
p = 1, is evaluated as the
j j j=1 j
proportion of data assigned to cluster j in the dataset initial cluster labeling.
The mean µ , representing the cluster centroid, is taken as centroid j got from
j
the partitioning P. The variance matrix Σ is linked to the informativeness of
j
the prior. In the analysis described in Section 4 this is defined as the empirical
covariance matrix of the data points in cluster j, Σ∗, multiplied for a constant
j
value s as to consider different concentrations. Upon selection of a value for s
in the variance matrix Σ = sΣ∗, a prior pdf for the whole dataset is entirely
j j
defined.
ThesecondstepoftheprocedureisbasedonproperBayesianbootstrap:m=n
observationsaregeneratedfromtheconvexcombinationofthedefinedpriorand
empirical cdf defined as:
G =(k+n)−1(kF +nF )
n 0 n
where k is the assigned confidence parameter. The proper Bayesian bootstrap
resamplespresentnewlysampledvaluesaswellasoriginaldatasetvalues,which
are the focus of the cluster labels assignment. K-means is applied to the B re-
sampledlearningsets,obtainingaclusterpartitioningforeach.Asaresult,each
of the original data points will be assigned to a given cluster a certain total
numberoftimes:clustermembershipsareevaluatedasthefractionbetweenthis
total and the number of times the point has been selected overall.
The procedure finally gives an aggregated value of the cluster label for the orig-
inal data points from the memberships, as argmax u (x ).
1≤k≤K k iBayesian Bagged Clustering algorithm 7
Algorithm 3 Proposal: BBC clustering procedure
1: procedure Bayesian Bagged Clustering (L={x ,...,x },K,P,m,k,s)
1 n
2: ApplyP tothelearningsetL,retrievingtheclusterlabelsforthedatapoints.
3: for j =1 to K do
4: Evaluatetheparametersofcomponentj:p =n /n,n beingthenumberof
j j j
pointsinclusterj;µ asthejthcentroid;Σ =sΣ∗fromtheempiricalcovariance
j j j
matrix of points in cluster j weighted with the parameter s.
5: end for
6: Define the prior pdf: f =(cid:80)K p N(µ ,Σ ).
θ j=1 j j j
7: for b=1 to B do
8: Generate n observations from (k+n)−1(kF +nF )
0 n
9: Draw wb,...,wb, from D(k+n,...,k+n) weights distribution of the obser-
1 n n n
vations.
10: Perform P on the dataset obtained above, assigning labels.
11: Permute the cluster labels assigned to the data points that come from the
original dataset for maximum overlap with the original clustering labels.
12: Record the labels of the data points which come from the original dataset.
13: end for
14: For the data points, update the cluster memberships u (x ), defined as the
k i
proportions of votes from the last step.
15: Assign final cluster labels for each data point equal to k corresponding to the
highest cluster membership: argmax u (x ).
1≤k≤K k i
16: end procedure
Geometrical interpretation As each vector u (x ) describes a probability
k i
distributionovertheclusterlabels,arepresentationoftheoriginaldataspaceof
points in a K−1 dimensional simplex is induced from the definition. It follows
thattheconcentrationofpointsinthesimplexwilldependontheconcentration
of the membership assignment to few components, hence on the optimality of
assignment. The ideal scenario would be of points close to the vertices of the
simplex; a real scenario is shown in Fig. 1, for the case of a uniform dataset in 2
dimensions while clustering with K =3. It can be seen that points in the worst
case tend to be assigned with uncertainty to more than one cluster.
3.2 Optimal choice of K
Fromthefirstpartoftheproposal,werecovertheK-dimensionalvectorofclus-
ter memberships of data points x , denoted as u(x ), i = 1,...,n. The vectors
i i
evaluated on the dataset will depend on the parameters of the proper Bayesian
bootstrap,onthechosenclusteringalgorithmP andinparticularonthenumber
ofclustersK.Thisfactmotivatestheanalysisoftheirbehaviourunderclustering
algorithm P, with fixed parameters, for varying K, in order to recover the un-
derlying cluster structure of the dataset. In the clustering procedure via proper
Bayesianbootstrap,K isthenumberofcomponentsoftheprior;moreover,each
component parameter (weight, mean, variance) depends on K. Under cluster-
ing, each replica by definition of the generating process is expected to exhibit8 F.M. Quetti et al.
1.25
1.00
0.75 count
80
60
40
20
0.50
0.25
0.00
0.0 0.5 1.0
X
Fig.1: Simplex representation of membership vectors obtained with the proper
Bayesian bootstrap, choosing K =3. The dataset is comprised by 500 points in
2 dimensions, uniformly generated.
an intermediate behaviour between the original dataset and the prior model.
Therefore,thealgorithmiseffectivelyenforcingaK-clusterstructureofdataby
implementing the prior as prescribed.
The fundamental idea of our proposal is that better choices of K lead to easily
assignable labels for the dataset, because the algorithm is more able to disam-
biguate between clusters.
To quantify the uncertainty about the membership assignments, we seek to de-
termine how the weight of the components is distributed over the normalized
membership vector u(x ). To do so, we employ the following two measures:
i
– S(u(x
))=−(cid:80)k
u (x )log u (x ),Shannonentropyofthevector,quan-
i i=1 k i 2 k i
tifies how the decision is dispersed between every vector component
– S l,m(u(x i)) = −( ul+ul
um
log
2
ul+ul
um
+ ulu +m
um
log
2
umu +l um), defined as the
Shannon entropy of the normalized two component vector (u ,u ), quan-
l m
tifiesthepairwiseindecisionbetweenclustersl,minlabelingthedatapoint.
The proposed measures lead to the following observations about the expected
results.Ifthenumberisoptimal,oneexpectstheresultsofthealgorithmtogive
the most crisp assignments of data points to the clusters: from an information
theory viewpoint, the smallest mean value of S as function of K corresponds to
the best choice of number of clusters.
Moreover, for each K, the arguments l,m of the maximum assumed by the
dataset average of S indicate which two clusters are most ill defined as sep-
l,m
arated instead of joint; the corresponding value of the measure quantifies the
worst case of pairwise indecision stemming from the choice of K.
The implementation of this line of reasoning is as follows: the proposed clus-
tering procedure is performed with multiple values of K in a range of plausible
YBayesian Bagged Clustering algorithm 9
cluster numbers; furthermore, for each case we repeat the procedure with differ-
ent choices of prior corresponding to different values of s. While for clustering
the selection of specific parameters is required, for this part of the proposal the
usage of different values of the parameter s ensures coverage of multiple cases
withtheaimofrobustlyassessingthebehaviourofthemeasuresundereveryK:
differentscenariosofinformativenessofthepriorcorrespondtodifferentparam-
eter choices, as the cluster structure modeled by the prior becomes increasingly
overlapped with increasing s. The confidence parameter ω = k is set to 0.5
k+n
as to equally weigh prior and empirical distribution.
One retrieves the optimal number of clusters as indicated by the two mea-
sures: for the first one, the value of K corresponding to the minimum of S¯ =
(cid:80)N
S(u(x ))/N;forthesecondone,thevalueofK correspondingtothemin-
i=1 i
imum of S¯ =max (cid:80)N S (u(x ))/N.
l,m (l,m) i=1 l,m i
Finally, the joint usage of the two parameters leads to a taxonomy of clustering
results for the original dataset based on information theory, aimed at enriching
the understanding about the behaviour for different choices of K.
4 Results
In this section we present results on the clustering proposal as well as examples
of analysis of the optimal number of cluster of various datasets performed using
the proposed method.
4.1 Clustering proposal results
In order to show how our proposed method works, results obtained for the clus-
teringalgorithmappliedontheIrisdatasetareshown.Theresultsarepresented
as contingency tables where rows represent true clustering labels while columns
the predicted ones. In this part the number of clusters K is considered a known
parameter.
The Iris dataset, extensively studied and considered as a benchmark for cluster-
ing applications, is comprised of 150 data points under 4 features; it comes with
the true cluster labels of the points, to be tested against the assignment of the
clustering procedure.
Against the benchmark results for K-means, shown in Table 1, our proposal is
testedwithdifferentchoicesofparameter;resultsfordifferentvaluesofthevari-
ance parameter s, and of the confidence w, are shown in Table 2a and Table 2b,
respectively.
From Table 2b, we see that the method is robust when the prior has variance
s=1, meaning that the model imposed for the dataset probability distribution
function (pdf) is well posed. The results on the sensitivity analysis on the vari-
ance described in Table 2a show that increasing s the cluster structure is not
capturedwell,worseningtheperformanceincluster2.Theseresultsconfirmthe
necessity of an accurate tuning of model parameters.10 F.M. Quetti et al.
1 2 3
1 50 0 0
2 0 48 2
3 0 14 36
Table 1: Clustering results for classical K-means clustering.
1 2 3 s w 1 2 3 s w
1 50 0 0 1 0.1 1 50 0 0 1 0.3
2 0 47 3 1 0.1 2 0 46 4 1 0.3
3 0 13 37 1 0.1 3 0 13 37 1 0.3
1 50 0 0 10 0.1 1 50 0 0 1 0.5
2 1 46 3 10 0.1 2 0 45 5 1 0.5
3 0 15 35 10 0.1 3 0 10 40 1 0.5
1 50 0 0 100 0.1 1 50 0 0 1 0.7
2 1 2 47 100 0.1 2 0 45 5 1 0.7
3 0 0 50 100 0.1 3 0 9 41 1 0.7
(a) Contingency tables for different (b) Contingency tables for different
choices of variance parameter s. choices of confidence w.
Table 2: Results for the proposed method varying the parameters related to the
variance of the prior s and the confidence given to the prior w.Bayesian Bagged Clustering algorithm 11
4.2 Optimal choice of K results
Inordertoshowcasetheinterpretationstemmingfromtheproposal,evaluations
on the synthetic datasets described in Table 3 are shown in the following. For
thecaseoftruenumberofclustersK =3anddimensionofthepointsp=2,the
datasetsaregenerateddifferingbyspecificgroundtruthcharacteristics:overlaps
between cluster components, different numerosity between cluster components,
different covariances between features i.e. different shapes.
Number of Points in
Dataset Dimension p Centroids Covariance Σ
Clusters K each component
 
(1.5,0)  
  10
1 2 3 33, 33, 33  (−1.5,0)
  
 √  01
(0,3 3)
2
 
(1.5,0)  
  10
2 2 3 99, 66, 33  (−1.5,0)
  
 √  01
(0,3 3)
2
 
(1,0)  
  10
3 2 3 33, 33, 33  (−1,0)
  
 √  01
(0, 3)
 
(1.5,0)  
  1 0.25
4 2 3 33, 33, 33  (−1.5,0)
  
 √  0.25 1
(0,3 3)
2
 
(3,0)
 
  (0,3)    
  0.75 0
5 2 5 66, 66, 66, 66, 66  (−3,0)
  
  0 0.75
 (0,−3)

 
(0,0)
 
(1,1,1)  
  100
6 3 4 66, 66, 66, 66     ( (1 −, 1− ,1 1, ,− −1 1) )   

   010  

  001
(−1,−1,1)
Table 3: Fundamental parameters of the datasets generated.12 F.M. Quetti et al.
The results obtained for each dataset are shown in Tables 4 and 5, for the
measures S¯ and S¯ respectively. The method is compared against the results
l,m
obtainedwithtraditionalK-selectionmethods:silhouettemethod[16],shownin
Table 6, and gap statistic [2], shown in Table 7.
BothproposedmeasuresareabletofindasoptimalK theoneusedforgeneration
ofthedataset,withthechoiceofparameters=1;onthecontrary,thesilhouette
method fails to do so for datasets 5 and 6, and the gap statistic for dataset 6.
As visual examples, datasets 1 and 5 are shown in Figure 2 and Figure 5.
Dataset K = 2 3 4 5 6
1 0.120 0.055 0.139 0.168 0.207
2 0.082 0.052 0.115 0.102 0.155
3 0.168 0.113 0.179 0.177 0.186
4 0.216 0.061 0.183 0.112 0.179
5 0.036 0.183 0.049 0.032 0.124
6 0.235 0.228 0.113 0.130 0.199
Table 4: S¯, entropy S averaged over the dataset, choosing different values of K.
Results are shown for the case with parameter s = 1. Each row refers to the
corresponding dataset in Table 3. The measure takes the minima for the value
of K used for dataset generation, in all cases.
Dataset K = 2 3 4 5 6
1 0.173 0.028 0.091 0.055 0.067
2 0.118 0.031 0.086 0.042 0.048
3 0.243 0.065 0.086 0.111 0.081
4 0.311 0.040 0.148 0.061 0.076
5 0.052 0.134 0.035 0.020 0.124
6 0.340 0.200 0.049 0.056 0.079
Table 5: S¯ : maximum value with respect to the cluster pairs of the pairwise
l,m
entropy S averaged over the dataset, choosing different values of K. Results
l,m
areshownforthecasewithparameters=1.Eachrowreferstothecorrespond-
ing dataset in Table 3. The measure takes the minima for the value of K used
for dataset generation, in all cases.Bayesian Bagged Clustering algorithm 13
Dataset K = 2 3 4 5 6
1 0.357 0.472 0.416 0.343 0.359
2 0.403 0.457 0.417 0.351 0.354
3 0.337 0.387 0.347 0.325 0.335
4 0.409 0.474 0.388 0.398 0.351
5 0.362 0.392 0.469 0.458 0.430
6 0.321 0.365 0.352 0.343 0.348
Table 6: Value of the silhouette parameter for different values of K. In bold are
the maximum values, indicating the optimal K for the silhouette method. Each
row refers to the corresponding dataset in Table 3.
Dataset K = 2 3 4 5 6
1 0.135 0.253 0.159 0.132 0.149
2 0.312 0.338 0.239 0.256 0.262
3 0.179 0.200 0.132 0.092 0.107
4 0.173 0.238 0.171 0.159 0.113
5 0.132 0.177 0.261 0.272 0.247
6 0.278 0.267 0.209 0.223 0.219
Table 7: Value of the gap statistic for different values of K. In bold are the
maximum values, indicating the optimal K for the gap statistic method. Each
row refers to the corresponding dataset in Table 3.14 F.M. Quetti et al.
Fig.2:Visualrepresentationofdataset1,eachcolourassociatedwithadifferently
generated component.
Regarding dataset 1, in Figure 3 and Figure 4 are shown the values of the two
measuresforvaryingK,withdifferentchoicesofparameters:themeasurestend
to be robust with respect to the different choices of informativeness parameter
s. In fact, all the curves show a minimum for K=3.Bayesian Bagged Clustering algorithm 15
Fig.3: Dataset 1, behaviour of the measure S¯, averaged over the dataset, as
function of the chosen number of clusters K. Each line represents the results
of the method with different values of the parameter s = 1,1.5,3,4.5,6. The
measure takes the minimum for K = 3, the number of clusters used in the
dataset generation.
Fig.4:Dataset1,behaviourofthemeasureS¯ asfunctionofthechosennumber
l,m
ofclustersK.Eachlinerepresentstheresultsofthemethodwithdifferentvalues
oftheparameters=1,1.5,3,4.5,6.ThemeasuretakestheminimumforK =3,
the number of clusters used in the dataset generation.16 F.M. Quetti et al.
Fig.5:Visualrepresentationofdataset5,eachcolourassociatedwithadifferently
generated component.
Fig.6: Dataset 5, behaviour of the measure S¯, averaged over the dataset, as
function of the chosen number of clusters K. Each line represents the results
of the method with different values of the parameter s = 1,1.5,3,4.5,6. The
measure takes the minimum for K = 5, the number of clusters used in the
dataset generation, in all cases except when s=1.5.Bayesian Bagged Clustering algorithm 17
Fig.7:Dataset5,behaviourofthemeasureS¯ asfunctionofthechosennumber
l,m
ofclustersK.Eachlinerepresentstheresultsofthemethodwithdifferentvalues
of the parameter s = 1,1.5,3,4.5,6. The measure takes the minima for K = 4
or K =5, depending on the parameter s.
The values of the two measures for varying K, with different choices of param-
eter s, are shown for dataset 5 in Figures 6 and 7. The difficulty in choosing
between K =4,5 is highlighted by the fact that different values of s correspond
to different optima, especially in Figure 7. This behaviour means that the gen-
eral uncertainty of assignments is minimum for K = 5, even if the difference
is not so big with K = 4, but going from 4 to 5 clusters does not reduce the
uncertainty between a pair of clusters.
We underline that the method can be used with two different perspectives. As a
wrapped method for a quick choice of K or as a tool to explore in a deeper way
the geometric structure of data, led by the idea of entropy of the memberships.18 F.M. Quetti et al.
5 Conclusions
In this work we proposed a new clustering approach, based on the deployment
of bagging techniques, which enhances a large family of existing methods. With
little computational effort, the method adds the benefit of a Bayesian interpre-
tation of the data generating process, at the same time having the potential to
retrieve additional information of participation while preserving (and in some
cases bettering) the benchmark of usual methods. The proposal of an optimal
schemeforchoosingtheclusternumberfollowsnaturallyfromthecharacteristics
of the clustering proposal: the introduction of prior knowledge in the clustering
procedure enforces regular K-cluster behaviours, while the fuzzy information
retrieved leads to quantifiable measures aimed at evaluating optimality. Future
directions of work include the usage of different methods from K-means, more
refined choices of prior, and the extension of the method to other types of ag-
gregation used in a cluster setting (e.g. BagClust2, [6]). Further analysis will be
also carried out for different datasets, focusing particularly on dimensionality
effects.
Acknowledgements
F.M. Quetti acknowledges RES, in particular C.E.O. Federico Bonelli for the
award of Ph.D. scholarship.
References
1. Anil K. Jain, M. Narasimha Murty, Patrick J. Flynn. (1999). Data clustering: a
review. ACM computing surveys (CSUR), 31(3), 264–323. ACM New York, NY,
USA.
2. Hastie,T.,Tibshirani,R.,Friedman,J.H.,(2009).Theelementsofstatisticallearn-
ing:datamining,inference,andprediction,Vol.2,pp.1-758.NewYork:Springer.
3. Jaeger,A.,Banks,D.(2023).Clusteranalysis:Amodernstatisticalreview.Wiley
Interdisciplinary Reviews: Computational Statistics, 15(3), e1597.
4. Galvani, M., Bardelli, C., Figini, S., Muliere, P. (2021). A Bayesian nonparamet-
ric learning approach to ensemble models using the proper Bayesian bootstrap.
Algorithms, 14(1), 11.
5. Ballante, E. (2023). An extension of Generalized Bayesian Ensemble Tree Models
to survival analysis. Far East Journal of Theoretical Statistics, 67(2), 137-146.
6. Sandrine Dudoit, Jane Fridlyand. (2003). Bagging to improve the accuracy of a
clustering procedure. Bioinformatics, 19(9), 1090–1099. Oxford University Press.
7. BradleyEfron.(1992).Bootstrapmethods:anotherlookatthejackknife.InBreak-
throughs in statistics: Methodology and distribution (pp. 569–593). Springer.
8. Donald B. Rubin. (1981). The bayesian bootstrap. The annals of statistics, 130–
134. JSTOR.
9. Lo, A. Y. (1987). A large sample study of the Bayesian bootstrap. The Annals of
Statistics, 360-375.Bayesian Bagged Clustering algorithm 19
10. ThomasS.Ferguson.(1973).ABayesiananalysisofsomenonparametricproblems.
The annals of statistics, 209–230. JSTOR.
11. PietroMuliere,PiercesareSecchi.(1996).Bayesiannonparametricpredictiveinfer-
ence and bootstrap techniques. Annals of the Institute of Statistical Mathematics,
48, 663–673. Springer.
12. Murtagh, F., Contreras, P. (2012). Algorithms for hierarchical clustering: an
overview.WileyInterdisciplinaryReviews:DataMiningandKnowledgeDiscovery,
2(1), 86-97.
13. Ester, M., Kriegel, H. P., Sander, J., Xu, X. (1996, August). A density-based al-
gorithmfordiscoveringclustersinlargespatialdatabaseswithnoise.Inkdd(Vol.
96, No. 34, pp. 226-231).
14. James MacQueen. (1967). Some methods for classification and analysis of multi-
variateobservations.InProceedings of the fifth Berkeley symposium on mathemat-
ical statistics and probability (Vol. 1, No. 14, pp. 281-297).
15. Dunn, J. C. (1973). A Fuzzy Relative of the ISODATA Process and Its Use in
DetectingCompactWell-SeparatedClusters.JournalofCybernetics.3(3):32–57.
16. Rousseeuw, Peter J. (1987). Silhouettes: a graphical aid to the interpretation and
validation of cluster analysis. Journal of computational and applied mathematics
20, 53-65.