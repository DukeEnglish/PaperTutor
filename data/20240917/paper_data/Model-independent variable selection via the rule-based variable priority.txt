MODEL-INDEPENDENT VARIABLE SELECTION VIA THE RULE-BASED
VARIABLE PRIORITY
BY MIN LU1,a AND HEMANT ISHWARAN1,b
1DivisionofBiostatistics,UniversityofMiami,am.lu6@umiami.edu;bhishwaran@miami.edu
Whileachievinghighpredictionaccuracyisafundamentalgoalinma-
chinelearning,anequallyimportanttaskisfindingasmallnumberoffeatures
with high explanatory power. One popular selection technique is permuta-
tionimportance,whichassessesavariable’simpactbymeasuringthechange
inpredictionerrorafterpermutingthevariable.However,thiscanbeprob-
lematic due to the need to create artificial data, a problem shared by other
methodsaswell.Anotherproblemisthatvariableselectionmethodscanbe
limitedbybeingmodel-specific.Weintroduceanewmodel-independentap-
proach, Variable Priority (VarPro), which works by utilizing rules without
theneedtogenerateartificialdataorevaluatepredictionerror.Themethod
isrelativelyeasytouse,requiringonlythecalculationofsampleaveragesof
simplestatistics,andcanbeappliedtomanydatasettings,includingregres-
sion,classification,andsurvival.Weinvestigatetheasymptoticpropertiesof
VarProandshow,amongotherthings,thatVarProhasaconsistentfiltering
propertyfornoisevariables.Empiricalstudiesusingsyntheticandreal-world
datashowthemethodachievesabalancedperformanceandcomparesfavor-
ablytomanystate-of-the-artprocedurescurrentlyusedforvariableselection.
1. Introduction. Althoughmanymachinelearningproceduresarecapableofmodeling
alargenumberofvariablesandachievinghighpredictionaccuracy,findingasmallnumberof
featureswithnear-equivalentexplanatorypowerisequallydesirable.Thisallowsresearchers
to identify which variables play a prominent role in the problem setting, thus providing in-
sight into the underlying mechanism for what might otherwise be considered a black box.
In machine learning, variable selection is often performed using variable importance, de-
scribed by how much a prediction model’s accuracy depends on the information in each
feature [5, 20, 65, 34, 60, 37, 14, 27, 22, 51, 67]. One of the most popular methods is per-
mutationimportance,introducedbyLeoBreimaninhisfamousrandomforestspaper[5].To
calculateavariable’spermutationimportance,thegivenvariableisrandomlypermutedinthe
out-of-sampledata(out-of-bag,orOOB,data)andthepermutedOOBdataisdroppeddown
atree.OOBpredictionerroristhencalculated.Thedifferencebetweenthisandtheoriginal
OOB error without permutation, averaged over all trees, is the importance of the variable.
Thelargerthepermutationimportanceofavariable,themorepredictivethevariable.
Because permutation importance is not part of the forest construction and does not make
use of the forest ensemble, it can be considered a filtering (screening) feature selection
method, which refers to selection procedures performed without using a final prediction
model.Filteringproceduresarewidelyadoptedinmanysettings,particularlyfordealingwith
ultra-highdimensionaldata[16,70,15,64,69].Ontheotherhand,permutationimportance,
and other types of prediction-based importance measures, can also be used for embedded
selection[see,forexample,54],whichreferstofeatureselectionprocessesembeddedinthe
learningphase.Animportantclassofembeddedproceduresispenalizationmethodslikethe
arXiv:2010.00000
MSC2020subjectclassifications:Primary62GXX;secondary6208.
Keywordsandphrases:Conditionalexpectation,Releasedrule,Signalandnoisevariables,Variableselection.
1
4202
peS
61
]LM.tats[
2v30090.9042:viXra2 LUANDISHWARAN
lasso[63].Recently,therehasbeensignificantattentiongiventodevelopingvariableimpor-
tance measures that can apply more generally across different types of learning procedures
within the framework of model selection [66, 48, 17]. [49] discuss the difference between
modelselectionandvariableselection.Anexampleofthelatteristhewrapperapproach[42]
for feature subset selection. The term wrapper generally refers to a generic induction algo-
rithm used as a black box to score the feature subsets. Another method worth mentioning is
knockoffs [8], which is a screening method that can be applied across different procedures
andhastheusefulpropertyofpreservingthefalsediscoveryrate.
In this paper, we take a broader approach in the spirit of these latter methods. We call
our proposed method VarPro, which refers to a model-independent framework of variable
priority. The term model-independent reflects borrowing from the best parts of both model-
dependent selection and model-free variable selection in the literature. This is because we
construct trees just like in permutation importance and tree filtering methods, but our goal
is different: to obtain rules and regions of the feature space over which to calculate our
importance score, rather than using predicted outcomes and prediction error to determine
importance. In this paper, we are interested in developing a consistent model-independent
rule-basedvariableselectionprocedureapplicableacrossdifferentdatasettings.
Let Y be the response variable and X(1),...,X(p) the set of p potential explanatory fea-
tures.Weconsiderthesettingwheretheresearcherisinterestedintheconditionaldistribution
of the response Y given the features X=(X(1),...,X(p)). Our goal is to identify the vari-
ables of importance for a given function of the conditional distribution of Y given X. We
callthetargetofinterestψ(X)=E(g(Y) X),whereg isaprechosenfunctionspecifictothe
|
problembeingstudied.Someexamplesaregivenbelow:
1. Regression.Hereψ(X)=E(Y X)whereg(Y)=Y andthegoalisdeterminingvariables
|
affectingtheconditionalmean.
2. Classification. For a categorical response with categories c ,...,c , interest could focus
1 L
on the conditional probability ψ(X) = P Y = c X for a specific category c , where
l l
{ | }
g(Y)=I Y =c .Forexample,instudyingthepresence,absence,orrecurrenceofcan-
l
{ }
cer,theresearchermayfocusontherecurrenceofcancertostudythehypothesisthatthe
probabilityofrecurrencedependsoncertainfeatures.
3. Time to event. With survival analysis, the focus of interest can be the survival function
ψ(X)=P To>t X ,where Y =To isthesurvivaltime.Inthiscase, g(To)=I To>
{ | } {
t .
}
Since it is expected that ψ will depend on a smaller subset of the p variables S
⊂
1,...,p , the task is to find the minimal set S for which this holds, which we call the
{ }
“signal variables”, while simultaneously excluding the non-relevant variables, which we
call “noise variables”. To make this idea more precise we provide the following defini-
tion. Write X(S) = X(j) for the feature vector X restricted to coordinates j S and
j∈S
{ } ∈
X\(S)= X(j) forcoordinatesnotinS.
j̸∈S
{ }
DEFINITION 1.1. S 1,...,p is the set of signal variables if S is the minimal set
⊂{ }
of coordinates that ψ depends on. Thus, S is the smallest subset of coordinates satisfying
P ψ(X)=ψ(X(S)) =1suchthatP ψ(X(S))=ψ(X(S′)) =0foreveryS′ whereS⊈S′.
{ } { }
ThecomplementarysetN = 1,...,p S isunrelatedtoψandthereforecontainsthenoise
{ }\
variables.IfS = ,thenψ=E(g(Y))isconstantandN = 1,...,p .However,werulethis
∅ { }
trivialcaseoutandalwaysassumeS = .
̸ ∅
The definition of S specifies that it is the smallest set of coordinates such that ψ(x)=
ψ(x(S))foralmostallx.Furthermore,thereisnoothersetS′=S (whereS ⊈S′)forwhich
̸VARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 3
ψ(X)=ψ(X(S′)) holds with non-zero probability. Another way to view Definition 1.1 is
that it implies a type of conditional independence for noise variables. For example, if p=2
andS = 1 ,thensinceX(2) isanoisevariable
{ }
E g(Y) X(1),X(2) =E g(Y) X(1) . (1)
| |
(cid:16) (cid:17) (cid:16) (cid:17)
Wenote(1)isweakerthantheusualconditionalindependenceassumptionusedforvariable
selection(seeequation(2)below)sinceitdependsuponthechoiceofg andψ.Forexample,
in classification, if the analysts chooses g(Y)=I Y =c for a specific class label c of
l l
{ }
interest,(1)impliesaconditionalindependenceofX(2) forclasslabelc ,butnotnecessarily
l
forotherclasslabels.Thus,variablesaffectingrelapseofcancermaybedifferentfromthose
affectingabsenceordeathduetocancer.Theabilitytoselect(g,ψ)tosuitthedataquestion
beingstudiedisanimportantfeatureofourapproachandprovidesresearcherswithaflexible
tool to study how variables play a role in their data settings. As another example, consider
survival analysis. As discussed in Section 5, a useful way to summarize lifetime behavior
is by restricted mean survival time (RMST) [56]. The RMST equals the integrated survival
functionuptoafixedtimehorizonvalueτ >0andequals
τ
ψ (X)= P To>t X dt=E(g(To) X), whereg(To)=To τ.
τ
{ | } | ∧
0
(cid:90)
Becauseψ (X)canvarywithtime,thelistofsignalvariablescanvarywithτ.Forexample,
τ
identifying variables affecting year one breast cancer survival is crucial for properly man-
aging early treatment, but knowing which variables affect lifetime after year one is equally
importantfortailoringtreatmentforlong-termsurvival.
Ourapproachworkswiththefunctionψobtainedbyintegrationwithrespecttothecondi-
tional distribution of Y given X, but another method often used is to work directly with the
conditional distribution. This assumes a conditional independence between Y and the noise
variablesgiventhesignalvariables:
Y X\(S) X(S). (2)
⊥ |
Let S 1,...,p be the variables of interest, and the goal is to determine if S contains
⊂{ }
signal variables. The strategy is to construct a test statistic θˆ (S) using augmented features
n
(X(S),X\(S),X˜(S),X˜\(S)), where X˜(S) and X˜\(S) are new artificial features. The test is
constructed in such a way that, due to conditional independence, θˆ (S) is statistically non-
n
significantifS arenoisevariables.
Wementiontwopopularmethodsusingthisidea.Thefirstispermutationimportancefor
random forests mentioned earlier (referred to hereafter as VIMP for variable importance).
In VIMP, the feature vector X\(S) is permuted to obtain X˜\(S); then the predicted value for
(X(S),X\(S)) is compared to the predicted value for (X(S),X˜\(S)), where this difference
should be nearly zero if (2) holds. However, a well-known problem with VIMP is that the
permutedsampledoesnothavethesamedistributionasX,whichcanleadtoflawedvariable
selection [61]. The second technique is knockoffs [8]. In knockoffs, a simulation according
tothedistributionofXisusedtoobtain(X˜(S),X˜\(S)),wheretheartificialdataissimulated
soastosatisfy
(X(S),X\(S),X˜(S),X˜\(S))=d (X(S),X˜\(S),X˜(S),X\(S)). (3)
Thisisusedtocomputeaknockoffstatisticforfilteringvariables.Bymakinguseof(2),the
knockoff test statistic can achieve a desired false discovery level. This novel idea avoids the
problems of permutation importance; however, it may rely on strong assumptions about the
parentdistribution,andachieving(3)couldbedifficultincertainsituations.4 LUANDISHWARAN
1.1. Contributionsofthisworkandoutlineofthepaper. TheVarPromethodintroduces
a novel approach to using rules. A similar method is the mean decrease impurity (MDI)
score [7]. For a given variable X(s), the MDI score is obtained by summing the weighted
impurity decrease across all tree nodes that involve a split using X(s) [for forests, the MDI
is averaged over all trees in the forest; see 51]. This score provides a useful summary of a
feature’s importance and is used for ranking variables. When the Gini index is used as the
impurityfunctioninclassification,thismeasureisknownastheGiniimportance.VarProcan
beviewedasageneralizationofMDIsinceMDIisapproximatelyrecoveredintheproposed
framework when setting g to either the Gini index for classification or the variance in re-
gression. However, VarPro extends MDI because the VarPro importance score for a feature
is determined by a local estimator that drops all conditions on the feature at once, whereas
MDI considers the marginal effect of the feature (i.e., the impurity drop at a specific node
thatsplitsonX(s)).
Thestructureofthepaperisasfollows.Section2describestheVarProprocedureindetail.
Givenarule,theVarProimportancescoreforasetofvariablesS isdefinedasthedifference
betweentwolocalestimatorsofψ,whereoneisestimatedfromtheregionofthedatadefined
by the rule and the other by removing any constraints on the rule involving the features in
S. The local estimators are simple averages of statistics and are relatively easy to calculate.
Examples are provided to motivate the method and explain the concepts of a region and its
released region, which are essential to understanding the VarPro importance score. We then
contrast VarPro with permutation importance to highlight its advantages. In this discussion,
we find that VarPro can be written as a permutation test, allowing us to pinpoint the short-
comingsofpermutationimportanceanddemonstratehowVarProovercomestheseissues.
Section 3 provides theoretical justification for VarPro. Our theory considers both the null
case, when the variable is noise, and the alternative case, when the variable has signal. We
show that VarPro consistently filters out noise variables and, for signal variables, we derive
theasymptoticlimitofabiastermrepresentingthelimitingVarProimportancescore.These
results hold under relatively mild conditions, such as a smoothness property for ψ and cer-
tain conditions for the rules used by VarPro. These conditions are expected to hold for any
reasonably constructed tree procedure, making VarPro generally agnostic to the rule-based
procedureusedforrulegeneration.
Section 4 presents empirical results demonstrating VarPro’s effectiveness using synthetic
and real-world data for regression and classification in both low and high-dimensional fea-
ture spaces. Building on the work in Section 2, we extend VarPro to survival analysis in
Section 5. A large cardiovascular study and high-dimensional simulations are used to illus-
trate the effectiveness of this extension. Section 6 concludes with a discussion summarizing
the strengths and weaknesses of the method. All proofs and supplementary information are
locatedintheAppendix.
2. A new rule-based variable selection approach. We begin by providing a broad
overview of the idea, after which we will delve into specific details. Let S again represent
thesetofvariablesofinterest.Givenaruleζ,VarProcalculatesasampleaveragedestimator
θˆ (ζ) for the target function ψ of interest by using the data in ζ’s region. Then a released
n
rule ζS isconstructedbyremovinganyconstraintsontheindicesin S,anditssampleaver-
aged estimator θˆ (ζS) is calculated over the released region. The estimator θˆ (ζS) is then
n n
contrasted with θˆ (ζ) to measure the importance of S. Many existing methods for variable
n
importance rely on either resampling or refitting models, which can introduce finite sample
bias, or they make use of artificial data as described earlier. A unique feature of VarPro is
thatitbypassestheneedforthis,andinsteadconstructsestimatorsθˆ (ζ)andθˆ (ζS)directly
n nVARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 5
from the data. These estimators serve as local estimates of ψ, and because θˆ (ζS) is calcu-
n
latedusingthedatafromreleasingcoordinatesonS,itimpliesundercertainconditionsdue
tothelawsofaveragesthat θˆ (ζ) θˆ (ζS) p 0fornoisevariablesS.However,forsignal
n n
| − |→
variables, the limit is different. Therefore, this makes it possible to consistently filter noise
variablesusingtheVarProimportancescore θˆ (ζ) θˆ (ζS) .
n n
| − |
2.1. Regions and release regions. A key aspect of VarPro is the idea of the region of a
rule ζ. A region is a subset of the feature space obtained by a function R that maps ζ X.
(cid:55)→
In many cases, ζ can be associated with a series of univariate rules. The region mapped by
R in this case is denoted by R(ζ)= x X :x(1) R(ζX(1)),...,x(p) R(ζX(p)) , where
{ ∈ ∈ ∈ }
ζX(j) denotestheunivariateruleforfeaturej.Forexample,ifallthefeaturesarecontinuous,
we can imagine a rule with region R(ζ)= x Rp :a x(1) b ,...,a x(p) b .
1 1 p p
{ ∈ ≤ ≤ ≤ ≤ }
Boundaries like this naturally arise in machine learning methods constructed from decision
rules.
In order to construct the VarPro importance score we need to introduce the idea of a re-
leasedruleandareleasedregion.
DEFINITION2.1. LetR(ζ) X bearegionofthefeaturespace.Tochecktheimportance
⊂
ofthevariables X(S) totherule ζ,weintroducetheconceptofareleasedregion R(ζS) for
thereleasedruleζS obtainedbyremovingthedependenceofζ onthecoordinatesX(S):
R(ζS)= x X :x\(S) R(ζ) .
{ ∈ ∈ }
In other words, the released region R(ζS) is the set of all x whose S-coordinate values
are unconstrained but with non-S-coordinate values that lie in R(ζ). In particular, R(ζ)
⊆
R(ζS).
Example 1. In this example, shown in Fig. 1, the feature space is X R2 and the rule
⊆
ζ corresponds to the branch of a tree using splits x(1) 0.7, x(2) 0.8, x(2) 0.7, and
≤− ≥− ≤
x(1) 1.95.Wecanthinkofζ asaproductofindicatorfunctions:
≥−
ζ =I x(1) 0.7 I x(2) 0.8 I x(2) 0.7 I x(1) 1.95 .
{ ≤− }· { ≥− }· { ≤ }· { ≥− }
TheregionR(ζ)isthebluerectangleinFig.1(A):
R(ζ)= (x(1),x(2)): 1.95 x(1) 0.7, 0.8 x(2) 0.7 .
{ − ≤ ≤− − ≤ ≤ }
Consider testing whether X(2) is a noise variable. Then, by Definition 2.1, we set S = 2
{ }
andobtainthereleasedregionbyremovingthedependenceoncoordinateX(2):
R(ζS)= (x(1),x(2)): 1.95 x(1) 0.7 .
{ − ≤ ≤− }
The released region is therefore the rectangle with the sides for the released coordinates re-
moved(seethepinkregioninFig.1(C)).Interestingly,sinceweareworkingwithaclassical
tree,thereleasedruleisitselfatreebranchequaltotheoriginalrulealteredsuchthatwhen-
ever a binary decision is made on a variable in S, the decision is always 1. In this example,
ζS =I x(1) 0.7 I x(1) 1.95 .
{ ≤− }· { ≥− }
Example 2. Fig. 2 illustrates how Definition 2.1 can apply to rules other than those from
classical trees. In both illustrations, the region of interest is a function of the variables that
is not decomposable into products of conditions on individual variables as in a typical tree.
The top left panel shows an elliptical region, which is a rule condition given by a quadratic
inequalityrelatingtobothvariables.AccordingtoDefinition2.1,toreleasetheregionalong6 LUANDISHWARAN
(A)OriginalRule (B)PermutationVariableImportance(VIMP)
3 3
2 2
1 1
0 0
1 1
− −
2 2
− −
3 3
− 3 2 1 0 1 2 3 − 3 2 1 0 1 2 3
− − − − − −
X(1) X(1)
(C)VarProReleaseRegion (D)VarProImportanceScore
3 3
2 2
I
1
1 1
g(yi)
θˆ n(ζ)= (x( i1),x( iX2)) ∈I1×I2
0 0 I 2 # {(x(1),x(2)) ∈I1 ×I2}
−1 −1 g(yi)
θˆ (ζS) = x( iX1) ∈I1
2 2
n # {x(1) ∈I1}
− −
3 3
− 3 2 1 0 1 2 3 − 3 2 1 0 1 2 3 4
− − − − − −
X(1) X(1)
FIG 1.Two-dimensional illustrationof howVarPro differsfrom artificial datamethods. (A) Thetwo-
dimensional region for ζ is a rectangle. The data of interest are marked in blue. (B) Permutation
variable importance (VIMP) for X(2). The data was permuted along X(2) and data marked in red
withtrianglesidentifyvaluesthatdonotmatchthejointdistributionofX.Themodel-basedpredicted
valuesy˜:=y˜(x(1),x˜(2))fortheseartificialpointsareextrapolatedfromaregionofthefeaturespace
that could be from potentially different responses. (C) VarPro release region for X(2). The original
ruleismodifiedtotheS-releasedruleζS (whereS= 2 )shownusingapinkbackgroundcolor.(D)
{ }
VarProimportancescoreisdefinedusingtheestimatorcalculatedusingobserveddatavaluesinblue
compared to the estimator where the new released values in red are additionally used. No artificial
dataneedstobecreated.
)2(X
)2(X
)2(X
)2(XVARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 7
X(2), we take the set of all x with coordinates X(1) inside the ellipse, while X(2) is left
unconstrained. Therefore, the released region R(ζS) is a box that touches the left and right
sidesoftheellipseasshowninthetopmiddlepanel.Thepaneltoitsrightshowstherelease
region when X(1) is released. This is a box that touches the top and bottom of the ellipse.
The second illustration, given in the bottom left of Fig. 2, is a hyperplane region. This is a
rule condition given by a simple linear inequality condition relating to both variables (for
instance,thiswouldoccurforrulesextractedfromrandomprojectiontrees).Themiddleand
right bottom panels display the release region when releasing coordinates X(2) and X(1).
Observe that in both of our examples, the released rule ζS obtained by releasing S may not
correspondtoarulegeneratedbytheoriginalmachinelearningprocedure.Thisisbecauseif
the region is a function that depends on the features using some type of threshold function,
then the resulting rule may be different from the rules generated by the original procedure.
Nonetheless,ourframeworkisdesignedtoaccommodatesuchascenario.
3 3 3
2 2 2
1 1 1
0 0 0
−1
R(ζ):
−1
R(ζS): +
−1
R(ζS): +
−2 −2 −2
−3 −3 −2 −1 X0
(1)
1 2 3 −3 −3 −2 −1 X0
(1)
1 2 3 −3 −3 −2 −1 X0
(1)
1 2 3
3 3 3
2 2 2
1 1 1 R(ζS): +
0 0 0
−1
R(ζ):
−1
R(ζS): +
−1
−2 −2 −2
−3 −3 −2 −1 X0
(1)
1 2 3 −3 −3 −2 −1 X0
(1)
1 2 3 −3 −3 −2 −1 X0
(1)
1 2 3
FIG2.RegionsR(ζ)(inblue)forrulesζ producedbyamachinelearningprocedure.Topleftisforan
ellipticalrule;bottomleftisforahyperplanerule.Middleandrightcolumnfiguresarereleaseregion
R(ζS)whenreleasingcoordinatesX(2) withS= 2 (redplusblueregion)andX(1) withS= 1
{ } { }
(greenplusblueregion),respectively.
2.2. VarPro importance score. The VarPro importance score is defined formally as fol-
lows. Let (X ,Y ),...,(X ,Y ) X Y be the i.i.d. learning data sampled from P. Ob-
1 1 n n
∈ ×
servationsY forX R(ζ)areusedtoestimatetheconditionalmeanofg(Y)inR(ζ)via
i i
∈
1
θˆ (ζ)= g(Y ). (4)
n i
# X R(ζ)
i
{ ∈ }X ∈R(ζ)
i(cid:88)
ThereleasedregionR(ζS)isusedtoobtainthereleasedestimator
1
θˆ (ζS)= g(Y ). (5)
n # X R(ζS) i
i
{ ∈ }X ∈R(ζS)
i(cid:88)
)2(X
)2(X
)2(X
)2(X
)2(X
)2(X8 LUANDISHWARAN
The VarPro importance of S is defined as the absolute difference between (4) and (5),
θˆ (ζS) θˆ (ζ) .
n n
| − |
We return to our previous example of Fig. 1 to give some intuition for this estimator by
contrastingittopermutationimportance(VIMP).RecallthatR(ζ)isthebluerectanglein(A)
and the released region R(ζS) is the pink area in (C). Panel (B) displays the data after per-
mutation on coordinate X(2). Permuted data (x(1),x˜(2)) R(ζS) are marked in red. Notice
∈
thatinsomecases(indicatedbytriangles)thesedatadeviatestronglyfromthetruedistribu-
tion of (X(1),X(2)). This creates problems for VIMP. This is because VIMP calculates the
importanceofX(2) bycomparingtheteststatisticcalculatedusingtheobserveddatainR(ζ)
(blue points in (A)) to that calculated using the permuted data in R(ζS) (red points in (B)).
If this were a regression setting, this corresponds to averaging observations y for i R(ζ)
i
∈
(1) (2)
and comparing them to averaged estimated values y˜ :=y˜(x ,x˜ ). Unfortunately, since
i i i i
y˜ has to be model estimated (since it must estimate ψ for features not in the training data),
i
(2)
thisproducesatypical y˜ if x˜ isfromadifferentregionofthedataspacethantheoriginal
i i
data(liketheredtriangularpoints).ThiscanresultinlargeVIMPevenwhenX(2) isanoise
variable.
This problem occurs because the learner is being applied to artificially created data that
deviatesfromthefeaturedistribution.Toavoidthis,VarProtakesadirectapproach.Usingthe
originaltrainingdata,estimatesareobtainedfortheconditionalmeanofg(Y)forX R(ζ)
∈
using observed values g(y ) via (4) and (5), where in a regression setting g(y)=y. Shown
i
in Fig. 1 (D), θˆ (ζ) is calculated from observations in the blue box (blue points), which is
n
compared to θˆ (ζS) calculated using the data in the pink rectangle (blue and red points),
n
yieldingtheimportancevalue
y y
i i
(cid:12)θˆ n(ζS) −θˆ n(ζ) (cid:12)=(cid:12) (cid:12) (cid:12)#x x( i(cid:88)1 () 1∈ )I 1
I 1 − #
(( xx (( i 11) ), ,x x( i(cid:88)2 () 2) )∈ )I 1 × II 12
I 2
(cid:12) (cid:12) (cid:12),
(cid:12) (cid:12) (cid:12) { ∈ } { ∈ × }(cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
whereI =[ 1.95, 0.7]andI (cid:12)=[ .8, 0.7].IfX(1) isasignalfeature,but(cid:12)X(2) isanoise
1 − − 2(cid:12) − (cid:12)
variable, then the two averages should converge to nearly the same value because ψ(X)=
E(Y X)=ψ(X(1))dependsonlyonX(1).Ontheotherhand,ifX(2)isasignalfeature,then
|
thedifferencewillnotnecessarilybezero.
VARPRO KEY IDEA. Releasing a rule ζ along the noise coordinates does not change ψ,
thereforethedifferencebetweentheaverageof g intherule’sregionanditsreleasedregion
will be asymptotically the same, and subsequently we can expect a zero importance. For
signalvariables,theoppositehappens,andsinceψ changesalongthereleaseddirection,we
can expect a difference in average g values in the released and non-released regions, and
thereforewecanexpectanon-zeroimportancevalueasymptotically.
2.3. Connection of VarPro to permutation tests. We use the following construction to
gain insight into the dangers of using artificial data as well as to expand on our previous
points about VIMP. Surprisingly, it turns out that we can write the VarPro estimator as a
specializedtypeofpermutationtest.Thiswillbecontrastedagainstmodel-basedimportance
to explain what goes wrong with artificial data and to explain how VarPro avoids this. This
also motivates the idea of using an external estimator to be used in Section 5 for survival
analysis.
Withoutlossofgenerality,assumethat x isreorderedsothat x=(x(S),x\(S)).Consider
(S) \(S)
allpossiblepermutationsofthetrainingdata (y ,x ,x ) .Thenitwillbeshown
{ i j i }1≤i,j≤nVARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 9
(seebelow)that
n i=1g(y i)I {x
i
∈R(ζS)
} =
n
i=1
n j=1g(y i)I {(x( jS) ,x\ i(S) ) ∈R(ζ)
}. (6)
(cid:80)
n i=1I {x
i
∈R(ζS)
} (cid:80)
n
i=(cid:80) 1
n
j=1I
{(x( jS) ,x\ i(S)
) ∈R(ζ) }
The left-han(cid:80)d side is the VarPro released(cid:80)estim(cid:80)ator θˆ (ζS) described in (5), whereas the
n
right-hand side is an estimator using the permuted data. Obviously in practice the permuted
estimatorontherightsideof(6)isimpracticalsinceitinvolvesO(n2)calculationsandwould
be an inefficient way to calculate θˆ (ζS). However, we use it to describe why permutation
n
VIMPgenerallydoesnotsatisfyanidentitylike(6),whichwillhighlighttheproblem.
First, however, let us explain why (6) holds. Let ζ\(S) be the rule that relaxes the con-
straints of ζ on the indices not in S. We have the following identity for rules expressible
as products ζ = p I x(s) I where I R are real-valued intervals (this is like the
s=1 { ∈ s } s ⊆
exampledescribedinFig.1):
(cid:81)
I x R(ζ) =I x\(S) R(ζ) I x(S) R(ζ) =I x R(ζS) I x R(ζ\(S)) . (7)
{ ∈ } { ∈ }· { ∈ } { ∈ }· { ∈ }
The right side follows by Definition 2.1 since x R(ζS) implies x\(S) R(ζ) and x
R(ζ\(S))impliesx(S) R(ζ)andvice-versa. ∈ ∈ ∈
∈
Therefore,forrulessatisfying(7)
I (x(S) ,x\(S) ) R(ζ) =I (x(S) ,x\(S) ) R(ζS) I (x(S) ,x\(S) ) R(ζ\(S))
{ j i ∈ } { j i ∈ }· { j i ∈ }
=I x R(ζS) I x R(ζ\(S)) (8)
i j
{ ∈ }· { ∈ }
wherethelastlineisbecause(x(S) ,x\(S)
)
R(ζS)dependsonlyonx\(S) and(x(S) ,x\(S)
)
j i ∈ i j i ∈
R(ζ\(S))dependsonlyonx(S)
.Hencewehave
j
n n
g(y )I
(x(S) ,x\(S)
) R(ζ)
i=1 j=1 i { j i ∈ }
(cid:80)
n
(cid:80)
n
I
(x(S) ,x\(S)
) R(ζ)
i=1 j=1 { j i ∈ }
(cid:80) (cid:80) n n g(y )I x R(ζS) I x R(ζ\(S))
= i=1 j=1 i { i ∈ } { j ∈
n n I x R(ζS) I x R(ζ\(S))
(cid:80) i=(cid:80)1 j=1 { i ∈ } { j ∈ }
(cid:80)n g((cid:80)y )I x R(ζS) n I x R(ζ\(S))
= i=1 i { i ∈ } j=1 { j ∈ }
n I x R(ζS) n I x R(ζ\(S))
(cid:80) i=1 { i ∈ } (cid:80)j=1 { j ∈ }
(cid:80)n g(y )I x R(ζS(cid:80))
= i=1 i { i ∈ } :=θˆ (ζS).
n I x R(ζS) n
(cid:80) i=1 { i ∈ }
The first line is from (8). The last line is due to the cancellation of the common term in the
(cid:80)
numeratoranddenominatorwhichisdirectlyrelatedtoworkingwithg(y ).
i
Now let us compare this to VIMP. Recall that unlike VarPro, VIMP does not use the
observed response to estimate ψ but uses a model-based estimator instead. Let ψ be this
n
estimator. For example, this could be the ensemble estimator from a random forest analysis
or a treeboosted estimator using gradientboosting. Then using the permuted dataas above,
theVIMPestimatoris
n n
ψ
(x(S) ,x\(S)
)I
(x(S) ,x\(S)
) R(ζ)
θ˜ (ζS)= i=1 j=1 n j i { j i ∈ }
VIMP
(cid:80) (cid:80)
n n
I
(x(S) ,x\(S)
) R(ζ)
i=1 j=1 { j i ∈ }
n (cid:80)n ψ(cid:80)(x(S) ,x\(S) )I x R(ζS) I x R(ζ\(S))
= i=1 j=1 n j i { i ∈ } { j ∈ }
n I x R(ζS) n I x R(ζ\(S))
(cid:80) (cid:80) i=1 { i ∈ } j=1 { j ∈ }
(cid:80) (cid:80)10 LUANDISHWARAN
wherethelastidentityisdueto(8).
Notice, however, that the cancellation that occured previously in the numerator and de-
nominatorisnolongerguaranteedtohold,anditisnottruethatθ˜ (ζS)isthesameasthe
VIMP
non-permutedestimator
n ψ (x )I x R(ζs)
θ˜ (ζS)= i=1 n i { i ∈ }, (9)
n n I x R(ζs)
(cid:80) i=1 { i ∈ }
which is the analog to θˆ (ζS) since it(cid:80)replaces the observed value g(y ) with the model
n i
estimated value ψ (x ). Just like θˆ (ζS), the estimator θ˜ (ζS) leads to consistent variable
n i n n
selection(tobeshowninTheorem5.1)sotheequalitywouldshowthatthemodel-basedper-
mutationimportancehasgoodproperties.Butthiscancellationoccursandthetwoestimators
becomethesameonlyif
ψ (x)=ψ (x\(S)) (10)
n n
becausethen
n ψ (x\(S) )I x R(ζS) n I x R(ζ\(S))
θ˜ (ζS)= i=1 n i { i ∈ } j=1 { j ∈ }
VIMP n I x R(ζS) n I x R(ζ\(S))
(cid:80) i=1 { i ∈ } j=1(cid:80) { j ∈ }
n (cid:80)ψ (x\(S) )I x R(ζ(cid:80)S)
= i=1 n i { i ∈ } :=θ˜ (ζS).
n I x R(ζS) n
(cid:80) i=1 { i ∈ }
However (10) is a very stron (cid:80)g assumption. If S consists entirely of noise variables then
ψ(x)=ψ(x\(S)),sointhiscase(10)isassertingthatthemodel-basedestimatorhascorrectly
eliminatedallnoisevariables. Thisisaverystrongrequirementandis askingalotfromthe
underlyingproceduresinceafterallthiswouldmeanthattheprocedurehasachievedperfect
variableselectiononitsown.Therefore,itisnotreasonabletoexpect(10)toholdingeneral,
whichmeansθ˜ (ζS)willnotequalθ˜ (ζS)ingeneral.
VIMP n
We emphasize that while this highlights the issue of using ψ with θ˜ (ζS), this does
n VIMP
not necessarily detract from the potential usefulness of an external estimator. A point we
explore is how to use the external estimator in a more coherent way. We argue that the non-
permutedestimator θ˜ (ζS) definedin (9)presents onesuchopportunity. Afterall, weknow
n
that machine learning methods like gradient boosting and random forests produce highly
accurateestimatorsinmanysettings.VIMPmisusesψ byapplyingittoartificialdata,which
n
can produce biased estimation. However, the non-permuted estimator θ˜ (ζS), which was
n
introducedasananalogtotheVarProestimator,onlyappliesψ tothetrainingdata.Thisis
n
whyitdoesnotequalpermutationVIMPandwhyitrepresentsalegitimatewaytoproceed.
Infact,inSection5wedevelopanestimatorlikethisforsurvivalanalysistoextendVarPro
to handle censored data. As will be discussed, in censored settings an external estimator
makessense.Howeveratthesametime,iftheresponseisobserved,thenweprefertousethe
VarProimportancescore
n g(y )I x R(ζS) n g(y )I x R(ζ)
θˆ (ζS) θˆ (ζ) = i=1 i { i ∈ } i=1 i { i ∈ }
(cid:12)
n − n
(cid:12)
(cid:12)
(cid:12)(cid:80)
n i=1I {x i ∈R(ζS)
}
−
(cid:80)
n i=1I {x i ∈R(ζ)
}
(cid:12)
(cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
which i(cid:12)s model-indepen(cid:12)den(cid:12)t an(cid:80)d makes use of the actual obse(cid:80)rved responses, rather(cid:12)than an
(cid:12) (cid:12)
externalimportancescore
n ψ (x )I x R(ζS) n ψ (x )I x R(ζ)
θ˜ (ζS) θ˜ (ζ) = i=1 n i { i ∈ } i=1 n i { i ∈ }
(cid:12)
n − n
(cid:12)
(cid:12)
(cid:12)(cid:80)
n i=1I {x i ∈R(ζS)
}
−
(cid:80)
n i=1I {x i ∈R(ζ)
}
(cid:12)
(cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:80) (cid:80) (cid:12)
(cid:12) (cid:12)VARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 11
whichismodel-specific.Theformerleadstoasimplerprocedurebasedonaveragesofinde-
pendentobservationswhichisconsistentunderrelativelymildconditions.Alsoitiseasierto
applytodifferentg functionswithouthavingtofitanewlearningmethodwheng ischanged
bytheresearcher.
3. Theory. How does one construct a rule ζ to identify variables informative for ψ? In
practice,therearemanyprocedurestochoosefromthatproducerulesforVarPro,including
simpledecisionrules[62],rulelearning[21],trees[7],Bayesiantrees[52],Bayesianadditive
regressiontrees[11],Bayesianforests[50]andrandomforests[5].Intheexamplespresented
inthispaper,therulesarechosenbyrandomlyselectingbranchesfromaCARTtree.
We assume hereafter that we have constructed a rule, or more generally, a collection of
rules for a specific problem. Previously we had defined the VarPro importance score on the
basisofasinglerule,butwewillactuallyusemanyrulesandaveragetheirscorestoobtaina
morestableestimator(seethediscussionfollowingTheorem3.2foranexplanationforwhy
more rules improve stability). The rules are assumed to be constructed independently of the
datausedbyVarPro,andthereforewithoutlossofgenerality,itwillbeassumedthatallrules
are deterministic. For each n, let ζ ,...,ζ denote these rules. Notice that the number
n,1 n,K
n
ofrulesK canvarywithnandalsothattherulesthemselvesareallowedtochangewithn.
n
Foragivenruleζ,define
n n
θˆ (ζ)=m (ζ)−1 g(Y )I X R(ζ) , m (ζ)= I X R(ζ) ,
n n i i n i
{ ∈ } { ∈ }
i=1 i=1
(cid:88) (cid:88)
which is a slightly more compact way of writing θˆ than (4) (notice that m (ζ) equals the
n n
samplesizeofaruleζ).Inalikewisefashion,wecandefineθˆ (ζS)=m (ζS)−1 n g(Y )I X
R(ζS) . n n i=1 i { i ∈
For} notational ease, let m =m (ζ ), mS =m (ζS ) and R =R(ζ(cid:80) ), RS =
n,k n n,k n,k n n,k n,k n,k n,k
R(ζS ).TheVarProimportancescoreforS isdefinedastheweightedaverageddifference
n,k
K
n
∆ (S)= W θˆ (ζS ) θˆ (ζ ) (11)
n n,k n n,k n n,k
| − |
k=1
(cid:88)
K n n
n 1 1
= W g(Y )I X RS g(Y )I X R
(cid:88)k=1 n,k (cid:12) (cid:12)mS n,k (cid:88)i=1 i { i ∈ n,k }− m n,k (cid:88)i=1 i { i ∈ n,k }(cid:12) (cid:12)
(cid:12) (cid:12)
where0 W 1are(cid:12)weights(deterministicorrandom)suchthat K n W =1.I(cid:12)nthe
≤ n,k ≤ (cid:12) k=1 n,k (cid:12)
followingsections,westudytheasymptoticpropertiesof(11),breakingthisupintothecase
(cid:80)
ofnoiseandsignalvariables.
3.1. Consistency for noise variables. The following assumptions will be used. Our first
assumptionrequiresthat:
(A1) E[g(Y)2]< andE[ψ(X)2]< .
∞ ∞
Wealsorequireψ satisfiesasmoothnesspropertyandthatthefeaturesspaceisconnected.
(A2) ψ is continuous and differentiable over the connected space X Rp and possesses a
gradientf = ψ:X Rp satisfyingtheLipschitzcondition ⊆
∇ →
f(S)(x ) f(S)(x ) C x(S) x(S) , forallx ,x X, (12)
| 1 − 2 |≤ 0 | 1 − 2 | 1 2 ∈
for some constant C < where f(S) denotes the subvector of f with coordinates in S
0
(note that the Lipschitz c∞ ondition only applies to f(S) as f is zero over the coordinates
fromN).12 LUANDISHWARAN
Werequirethatregionsshrinktozerouniformlyoverthesignalfeatures(therateisunspeci-
fied):
(A3) Fork=1,...,K ,P X R >0anddiam (R ) r forsomesequencer
n n,k S n,k n n
{ ∈ } ≤ →
(S) (S)
0wherediam (R )=sup x x .
S n,k {x 1,x 2∈R n,k} || 1 − 2 ||2
Thelastconditionpertainstotheweights:
(A4) Foreachn,thereexistsx R fork=1,...,K suchthat
n,k n,k n
∈
W ψ(x ) C , W f(x ) C , (13)
n,k n,k 1 n,k n,k 2 2
| |≤ || || ≤
k=1 k=1
(cid:88) (cid:88)
forsomeconstantsC ,C < .
1 2
∞
Assumption(A4)stipulatesatradeoffbetweentheweightsandthebehaviorofψ andf over
the region of the feature space identified by a rule. Condition (13) is satisfied if ψ and f are
bounded. The condition also holds if all rules are constructed so their regions are contained
withinsomeclosedboundedsubspaceinX definedbythesignalcoordinates(becauseψand
f willbeboundedduetocontinuity).
Thefollowingtheoremshowsthatundertheseassumptions,VarProisconsistentfornoise
variables.
THEOREM 3.1. Assume that (A1), (A2), (A3) and (A4) hold. If K
n
O(logn) and
m m =n1/2γ whereγ ataratefasterthanlogn,then∆
(S)≤p
0ifS N.
n,k n n n n
≥ ↑∞ → ⊆
Goingbacktoourassumptions,θˆ (ζS )andθˆ (ζ )from(11)areaveragesofindepen-
n n,k n n,k
dent variables. Seen in this light, (A1) is really just a second moment condition needed to
ensure that they converge. Assumptions (A2) and (A4) are smoothness and boundedness
conditions for ψ and its derivative. Roughly speaking, since m Ï n1/2 , then due
n
to the large sample properties of averages, θˆ (ζS ) θˆ (ζ ) converges → to E∞ (ψ(X) X
n n,k − n n,k | ∈
RS ) E(ψ(X) X R ).Forthistoprovideusefulinformationaboutavariable’simpor-
n,k − | ∈ n,k
tance, it is necessary for ψ to have certain nice properties. Regarding the number of rules
K usedbytheestimator,wewanttouseasmanyrulesaspossibletoimprovestability,but
n
thereisatrade-offbecausewealsohavetomaintainuniformconvergence,whichholdswhen
K O(logn).
n
≤
Assumption(A3)andtherateconditionm aretheonlyconditionsthatspecificallyapply
n
to a region. (A3) requires that the diameter of a region shrinks to zero uniformly over the
signal variables while m places a lower bound on the sample size of a region which must
n
increase to . While these two conditions are diametrically opposed and therefore might
∞
seem unusual, they are fairly standard assumptions used in the asymptotic analysis of trees.
Fortreeconsistency,atypicalrequirementisthatthediameterofaterminalnodeconverges
to0 andthe numberof itspointsconverges to inprobability. SeeTheorem 6.1from [13]
∞
andTheorem4.2from[29].
Infact,wedirectlyshowthatconditions(A3)andtherateconditionm holdforarandom
n
treeconstruction.AssumethatX =[0,1]p.Consideratreeconstructionwhereatthestartof
split k 1, each of the k leaves of the tree is a p-dimensional rectangle (when k =1, the
one lea≥ f equals the root node of the tree, [0,1]p). Among these k leaves (rectangles), one is
selectedatrandomanditslongestsideissplitatarandompoint;yielding,attheendofstep
k, two new rectangles of reduced volume from the original rectangle. The tree construction
isrepeatedforatotalofK splits,yieldingK +1brancheswhicharetherules.
n n
If K =logn, then it can be shown that the following holds in probability [see the proof
n
ofTheorem2of2]:VARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 13
(i) Thenumberofdatapointsinarectangleisgreaterthanm =nδlognforany1/2<δ<
n
1.
(ii) ThemeanlengthofthelargestsideofarectangleislessthanE (3/4)T n whereT n p
→∞
doesnotdependonthespecificrectangle;hencethediameterofeachrectangleconverges
(cid:2) (cid:3)
uniformlytozero.
Therefore, (ii) shows (A3) holds. Also, (i) shows that m Ïn1/2 achieves the lower bound
n
requiredbyTheorem3.1.Thisisaninterestingconsequenceoftherandomconstructionand
allows the bound to be achieved without supervision. This can be seen informally from the
fact that K =O(logn) which naturally forces data to pile up in the leaves and if tree node
n
sizesareapproximatelyevenlydistributed,m n/logn.
n
≍
3.2. Limiting behavior for signal variables. For the analysis of signal variables, we as-
sume that regions are rectangles. This is made for technical reasons to simplify arguments
but does not limit applications of VarPro. Thus, the region for a rule ζ can be written as
R(ζ)= p I whereI Rarereal-valuedintervals.Fornotationalsimplicityweassume
×j=1 j j ⊆
the first S coordinates are signal variables and the remaining N coordinates are noise
| | | |
d p
variables. Therefore, we can write R(ζ)=A B where A= I , B = I and
j=1 j j=d+1 j
× × ×
d= S pisthenumberofsignalvariables.
| |≤
THEOREM 3.2. Assumethattheregionforeachruleisarectanglecontainedwithinthe
connected space X Rp, then under the same conditions as Theorem (3.1), if S = s is a
⊆ { }
signalvariable,
K
n
∆ (s)= 1+o (1) W E ψs (X(s)) ψs (x(s) ) X Rs +o (1),
n p n,k n,k − n,k n,k ∈ n,k p
(cid:16) (cid:17)(cid:88)k=1 (cid:12) (cid:12) (cid:104) (cid:0) (cid:1)(cid:12) (cid:105)(cid:12) (cid:12)
(cid:12)
whereψs (z)=ψ(x(1) ,...,x(s−1)(cid:12) ,z,x(s+1) ,...,xd )andx =(x(1) ,...,(cid:12) x(p) )′ R
n,k n,k n,k n,k n,k n,k n,k n,k ∈ n,k
isafixedpointineachrectangleasdefinedin(A4).
Theorem3.2 showsthat thelimit ofthe VarProestimator isnotnecessarily zeroas inthe
noisecase.Thelimitiscomplicatedaswewouldexpectsinceitwilldependonthesizeofthe
signalaswellasthefeaturedistribution,howevertohelpunderstandwhatthislimitmightbe,
considerthecasewhenψ isanadditivefunction,ψ(x)= d ϕ (x(j)).Thenbydefinition
j=1 j
ψs (x)= ϕ (x(j) )+ϕ (x)and
n,k j∈S\s j n,k s (cid:80)
E ψs (cid:80) (X(s)) ψs (x(s) ) X Rs =E ϕ (X(s)) ϕ (x(s) ) X Rs .
n,k − n,k n,k ∈ n,k s − s n,k ∈ n,k
(cid:104)(cid:16) (cid:17) (cid:12) (cid:105) (cid:104)(cid:16) (cid:17) (cid:12) (cid:105)(14)
This is the average difference be(cid:12)tween ϕ (X(s)) for a random X Rs (cid:12)compared with
s ∈ n,k
ϕ (x(s) ) for a fixed point x R . Because X(s) is unrestricted, and can take any
s n,k n,k ∈ n,k
value in the s coordinate direction of X, ϕ (X(s)) ϕ (x(s) ) should be non-zero on av-
s − s n,k
erage. Also, even if (14) equals zero by chance, keep in mind this applies to each rectangle
R ,...,R ,thuswecanexpectanaveragenonzeroeffectwhensummingoverallrules.
n,1 n,K
n
Thisalsoexplainswhyitisbettertousemanyrulesthanjustonerule.Infact,Theorem3.2
allowsforuptoO(logn)rules.14 LUANDISHWARAN
4. Empiricalresults. HerewestudytheperformanceofVarProinregressionandclassi-
ficationproblems.Algorithm1describesthecomputationalprocedureusedforouranalysis.
Inline2,thedataofsizeN issplitrandomlyintotwoparts.Inline3,thefirstdatasetofsize
N nisusedforrulegeneration.Inline4,theseconddatasetofsizenisusedtoobtainthe
−
importance score using the rules obtained in line 3. Data split proportions used are 0.632N
fortherulegenerationstepandn=(1 0.632)N forcalculatingtheimportancescore.This
−
typeofdatasplitisfairlycommoninmachinelearning.Forexample,inpermutationimpor-
tance of random forests, each tree is constructed from a bootstrap sample of approximately
63.2% N and the remaining 36.8% N (the so-called out-of-bag data) is put aside for
× ×
calculatingpermutationimportance.
Algorithm1 VarProforModel-IndependentVariableSelection
1: forb=1,...,B>0do
2: SplitthedataDofsizeN randomlyintoseparatepartsD=Drg∪Dvp whereDrg isofofsizeN−n
andDvpisofsizen=(1−α)N whereα=.632.
3: UseDrgfortherulegenerationstep,yieldingrulesRn,1,...,R n,Kn.
(s)
4: Using Dvp, calculate the VarPro importance (11) for variable X for s = 1,...,p using rules
Rn,1,...,R n,Kn generated in the previous step. When calculating (11) use weights W n,k =
m n,k/(cid:80)K k=n 1m n,k.Let∆b n(s)denotetheimportancevaluefors=1,...,p.
5: endfor
1 B
6: Calculatethesampleaverage∆n(s)andsamplevarianceVarn(s)of{∆n(s),...,∆n(s)}.Define
(s) ∆n(s)
I n,B(X )=
(cid:112)
, s=1,...,p. (15)
Varn(s)
(s)
Call(15)theVarProstandardizedimportanceofX .
We use trees for the rule generating procedure (line 3). Our trees are constructed using a
guided tree-splitting strategy. As in random forests, the tree is grown using random feature
selection where the split for an internal node is determined from a randomly chosen subset
offeatures.However,insteadofauniformdistribution,featuresareselectedwithprobability
proportionaltoapre-calculatedsplit-weight.Thisisdonetoencouragetheselectionofsignal
features in the tree construction in accordance with assumption (A3). The split-weights are
obtained prior to growing the tree by taking the standardized regression coefficients from a
lasso fit and adding these to the variable’s split frequency obtained from a previously con-
structed forest of shallow trees. The rationale for this approach is that combining lasso with
treesborrowsthestrengthsofparametricandnonparametricmodeling.Afterconstructingthe
tree,K branchesarerandomlychosen,yieldingtherulesR ,...,R ofline3.
n n,1 n,K
n
Finally, for the purpose of reducing variance and producing a standardized importance
value, the entire procedure is repeated B times and the importance scores are standardized
(line 6). Variable X(s) is selected if (15) exceeds a cutoff value Z . This cutoff can be pre-
0
chosen (for example Z =2) or selected by out-of-sample validation. Examples using both
0
strategieswillbepresented.
4.1. Regression. Regression performance of VarPro was tested on 20 synthetic datasets
with different N and p (see Appendix E) covering linear and nonlinear models as well as
models that switched between between linear and nonlinear. Features varied from uncorre-
latedtocorrelated,withthelatterretainingthesamedistributionbutadjustedtoa0.9corre-
lationviaacopula,exceptforspecificcaseslikelmandlmi2wherefeatureswerecorrelated
withinblocksofsize5(detailsareprovidedinAppendixE).VARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 15
Each VarPro run used B =500 with K =75 rules extracted. Comparison methods in-
n
cluded permutation importance [6] (referred to as Breiman-Cutler variable importance, ab-
breviatedasBC-VIMP),generalizedboostedregressionmodeling(GBM)withtrees;i.e.gra-
dientboostedtrees[20],knockoffs[8],lasso[63],meandecreaseimpurity(MDI)[51],wrap-
pers[42]andtheReliefalgorithm[41]viaitsadaptationRReliefF[55].
R-packages used for the analysis included: randomForestSRC [36], gbm [26],
knockoff [53], glmnet [18], ranger [68], mlr [4] (using a sequential forward search
engine) and CORElearn [55]. Two types of knockoff test statistics were calculated from
differences using: (1) lasso coefficient estimates and (2) random forest impurity compared
to their knockoff counterparts. Two types of wrappers were used: (1) k-nearest neighbors
(wrapperKNN)and(2)boostedgradienttrees(wrapperGBM).Thelassoregularizationpa-
rameter, the number of boosting iterations for GBM and wrapper GBM and the number of
nearestneighborsk forwrapperKNNweredeterminedby10-foldcross-validation.
Featureselectionperformancewasassessedusingtheareaundertheprecision-recallcurve
(AUC-PR) and the geometric mean (gmean) of TPR (true positive rate for selecting signal
variables;i.e.thesensitivity)andTNR(truenegativerateforselectingnoisevariables;i.e.the
specificity). These metrics were selected since many of the simulations had far more noise
variables than signal variables, presenting an imbalanced classification problem. AUC-PR
evaluatesthetrade-offbetweenTPR(recall)andpositivepredictivevalue(precision)without
beingaffectedbytheimbalanceratiosincetherecallandprecisionareevaluatedbyselection
thresholdsvariedoverallpossiblevalues,makingitsuitableforimbalanceddatasets.Gmean
provides a balanced measure of performance across both the majority and minority classes,
andisthereforeisalsoappropriateforclassimbalancedsituations[43].
To calculate AUC-PR, which does not require a threshold value, a method’s output was
converted to a continuous score. Scores used were: For BC-VIMP, permutation importance
(a value that can be both positive and negative); for GBM, the relative influence (a non-
negative value); for knockoffs, the absolute value of the knockoff test statistic; for lasso,
the absolute value of the coefficient estimates; for MDI the impurity score; for ReliefF, the
attributeevaluationscore;forwrappers,thiswasazero-onevaluereflectingifavariablewas
selected;forVarPro,thestandardizedimportancevalue(15)(anon-negativevalue).
Forgmean,azerothresholdwasuniformlyapplied.GBM,lasso,MDI,ReliefFandwrap-
pers used the same score value as their AUC-PR calculations. For BC-VIMP, negative im-
portance values were converted to zero. For knockoffs, the knockoff test statistic was set to
zero for variables screened under a target FDR value of 0.1. For VarPro, standardized im-
portance was set to zero for values less than an adaptive cutoff value Z selected using an
0
out-of-sampleapproach.Inthisstrategy,agridofcutoffvaluesisused.Afterrankingfeatures
indescendingorderoftheirVarProimportance,arandomforestisfittothosefeatureswhose
VarPro importance value is within a given cutoff value, and the out-of-bag error rate for the
forest is stored. The optimized Z value is defined as the cutoff with smallest out-of-bag
0
error.
Eachexperimentwasrun100timesindependently.Resultsfortheuncorrelatedandcorre-
lated feature experiments are shown in Fig. E1 and Fig. E2 (Appendix E). We will focus on
themorechallengingcorrelatedsimulations.ThisissummarizedinFig.3,whichshowsthe
rankofeachprocedureforthecorrelatedsetting.ConsideringFig.E2andFig.3,wecanob-
servethatBC-VIMPandGBMgenerallyhavesimilarbehavior.BothachievehighAUC-PR
valuesbutbothalsohavepoorgmeanperformance.Thisisbecausewhiletheyrankvariables
reasonably well, which yields high AUC-PR, both methods tend to overfit and they are not
able to threshold variables effectively, thus leading to a low gmean metric. The lasso and
glmnet knockoffs show more balanced gmean results but face issues with complex models
like inx1, inx2, and inx3, affecting their consistent performance across experiments for both16 LUANDISHWARAN
knockoff knockoff wrapper wrapper
BC−VIMP GBM lasso MDI RReliefF VarPro
forest glmnet GBM KNN
10.0
7.5
Performance
AUC−PR
gmean
5.0
2.5
FIG3.Rankofeachprocedureforcorrelatedregressionsimulationexperiments(lowerindicatesbetter
performance).
metrics.Forestknockoffsgenerallyunderperformcomparedtoglmnet.TheKNNandGBM
wrappersperformsimilarlyandarenotaseffectiveaslassoandglmnetknockoffs.RReliefF
and MDI both perform poorly in terms of gmean, with MDI slightly outperforming RReli-
efFintermsofAUC-PR.VarProstandsoutwiththebestoverallperformance,achievinghigh
AUC-PRforaccuratevariablerankingandhighgmeanforeffectivesignalandnoisedifferen-
tiation.TotestifVarPro’sperformanceisstatisticallysuperior,weappliedapairedone-sided
Wilcoxon signed rank test with a Bonferroni correction for multiple comparisons. The only
method equivalent to VarPro is BC-VIMP in terms of AUC-PR (adjusted p-value <.00001
forallothermethods),andnomethodisequivalentorisabletooutperformVarProinterms
ofgmean(adjustedp-value<.00001).Therefore,thisdemonstratesVarProhasrobustoverall
performance.
4.2. Classification. The following synthetic example was used to compare VarPro to
random forest permutation importance. The simulation model used was a multiclass model
withL=3specifiedaccordingto:
p
ϕ (x)
y=argmax ψ (x) , whereψ (x)= l , ϕ (x)=exp β x(j)
l { l } l L l=1ϕ l(x) l 
j=1
j,l 
(cid:88)
 
β =1 forj=1,2,3, otherwiseβ(cid:80)=0
j,1 j,1
β =1 forj=4,5,6, otherwiseβ =0
j,2 j,2
β =1 forj=7,8,9, otherwiseβ =0.
j,3 j,3
knaRVARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 17
HereX(1),...,X(9) aresignalvariablesforallthreeclassesduetotheconstraint ψ =1.
l l
However,coordinates1,2,3areespeciallyinformativeforclass1,coordinates4,5,6forclass
(cid:80)
2,andcoordinates7,8,9forclass3.Thefeaturesweredrawnfromamultivariatenormalwith
marginalsX(j) N(0,1)suchthatallcoordinateswereindependentofoneanother,except
∼
forpairs (X(3),X(10)), (X(6),X(15)), (X(9),X(20)) whichwerecorrelatedwithcorrelation
ρ=.9.SimulationsusedN =2000andp=20.
Standardized VarPro importance values were subtracted by the constant Z =2, yielding
0
astandardizedimportancevalueforeachvariableX(s),s 1,...,p ,calibratedtozerofor
∈{ }
eachclass.Theresultsfrom250independentrunsaregiveninFig.4.VarPro’sresultsshown
in the top panel are excellent which is in contrast to the bottom panel displaying permuta-
tion importance (BC-VIMP). The latter shows clear issues in separating noise from signal.
The poor performance of BC-VIMP is due to the correlation between the signal variables
X(3),X(6),X(9) and noise variables X(10),X(15),X(20) which causes permutation impor-
tance for signal to be degraded while artificially inflating noise importance values. In each
class label instance, BC-VIMP incorrectly identifies 2 or 3 noise variables as signal. This
differs from VarPro where signal variable importance is not degraded and noise importance
scores are substantially smaller and all are non-significant. The group structure is also clear
(for example features X(1),X(2),X(3) are highly informative for class 1) and it is evident
thateachconditionalprobabilitydependsonX(1),...,X(9).
4.2.1. Lowsamplesize,high-dimensionalmicroarrayclassificationexperiments. Inour
next example we run a benchmark experiment using a collection of 22 small sample size,
high-dimensional microarray datasets. The sample sizes vary from N =62 to N =248 and
thedimensionsvaryfromp=456top=54613.Ineachexampletheoutcomeisaclasslabel
where the number of classes L vary from L=2 to L=10. Most of the datasets are related
tocancer.ThedataisavailablefromtheR-packagedatamicroarrayandisdescribedin
moredetailthere.
For each dataset, the original class labels were replaced by a synthetically generated set
of class labels. A parametric lasso model was fit using the original Y which was regressed
usingmultinomialregressionagainst X equaltothegeneexpressionvalues.Threedifferent
syntheticY’swerecreated.Inthefirst,Y wasthepredictedclassfromthepreliminarylasso
model. In the second, the expression values were squared and then applied to the original
estimated coefficients to obtain Y. The third Y was similar to the second simulation except
thatthetopthreefeatureswerereplacedwiththeirpairwiseinteractions.Thuswehavethree
simulation models: (A) linear, (B) quadratic, (C) quadratic-interaction. Note that the simu-
lated datasets use the original X, only the Y is replaced with the synthetic Y and in each
simulationweknowwhichfeaturesaresignalandwhichfeaturesarenoise.
VarPro feature selection used an optimized Z cutoff using out-of-sampling similar to
0
the regression benchmark experiments. For comparison procedures, several well-known
methods suitable for dealing with high-dimensional data were used: (1) BC-VIMP us-
ing the R-package rfsrc [36]. (2) MCFS (Monte Carlo Feature Selection) using the R-
package rmcfs [15]. MCFS calculations were performed using the function mcfs where
the cutoff value for thresholding was determined by permutation. As recommended by the
rmcfs package, we used a large number of permutations and ran the MCFS algorithm for
each permutation, which was used by the package to determine the permutation thresh-
old. (3) SES (Statistically Equivalent Signature) which uses constraint-based learning of
Bayesiannetworksforfeatureselection.CalculationsusedtheSESfunctionfromtheMXMR-
package[46].(4)Borutawhichisafeaturefilteringalgorithmthatcreatesartificialdatainthe
formof“shadowvariables”whichactasareferenceforfilteringnoisevariables[45].Boruta18 LUANDISHWARAN
(A)VarPro
Y = 1 Y = 2 Y = 3
9
Variable
1
2
3
4 6 5
6
7
8
9
10
3 11
12
13
14
15
16
0 17
18
19
20
(B) BC−VIMP
Y = 1 Y = 2 Y = 3
0.3 Variable
1
2
3
4
5
0.2 6
7
8
9
10
11
12
0.1 13
14
15
16
17
18
19
0.0 20
FIG4.Multiclassexperimentwherevariables1–3aremostinformativeforclass1,variables4–6for
class2andvariables7–9forclass3;variables10–20arenoisevariables.Variables3and10,6and
15,9and20arestronglycorrelated:thusthereiscorrelationbetweensignalandnoisefeatures.(A)
VarPro importance correctly identifies the group structure and is not influenced by correlation. (B)
BC-VIMPfromrandomforestsisinfluencedbycorrelationthatdegradesitsperformance.
computationswereimplementedusingtheBorutaR-packageusingrandomforestsforthe
trainingclassifier.(5)RecursiveFeatureElimination(RFE)whichisawrapperfeatureselec-
tion procedure [28]. Calculations used the rfe function from the caret R-package [44].
RandomforestswereusedastheRFEwrapperasrecommendedby[10].
The simulations were run 100 times independently for each microarray dataset. In order
to introduce variability across runs, rather than using all p variables, a random subset of
two-thirdsofthenoisevariableswereselectedwhereanoisevariablewasdefinedasagene
expressionfeaturenotselectedinthepreliminarylassomodel.Featureselectionperformance
wasassessedbygeometricmean(gmean),precision(1minusfalsediscoveryrate)andaccu-
racy(1minusmisclassificationerror).Valueswereaveragedoverthe100runs.
The results are recorded in Fig. 5. Going from scenarios (A) to (C), performance for all
methods decreases as the complexity of the simulation increases as is to be expected. How-
ever, at the same time, the rank of each method and their various characteristics are main-
tainedacrosssimulations.BC-VIMPisalwaysoneofthebestproceduresintermsofgmean
whereas it is always one of the worst for precision and accuracy. SES is always the worst
ecnatropmi
dezidradnatS
ecnatropmi
noitatumrePVARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 19
(A)Linear
1.00 1.00 1.0
0.75 0.75 0.9 B BC or− uV taIMP 0.50 0.50 M RFC EFS
0.8 SES
0.25 0.25 VarPro
0.7
0.00 0.00
(B)Quadratic
1.0 1.00 1.00
0.8 0.75 0.95 BC−VIMP
Boruta 0.90 MCFS
0.6 0.50 RFE
0.85 SES
0.4 0.25 VarPro
0.80
0.00
(C)Quadratic-Interaction
1.0 1.00 1.00
0.95
0.8 0.75 BC−VIMP
0.90 B Mo Cr Fut Sa
0.6 0.50 RFE
0.85 SES
0.4 0.25 0.80 VarPro
0.00 0.75
FIG5.Featureselectionperformanceoverhigh-dimensionalmicroarrayclassificationdatasets.Three
simulationmodelswereusedforcreatingsyntheticY classificationlabels:(A)Linear;(B)Quadratic;
(C)Quadratic-Interaction.
intermsofgmeanperformance,butitisalwaysamongthetopperformersforprecisionand
accuracy.VarPro,ontheotherhand,doeswellinallthreemetricsandhasbetteroverallper-
formancethananyothermethod(adjustedp-value<0.05forallexperiments).Theseresults
are consistent with our benchmark regression experiments that showed VarPro achieves a
balancedperformance.
4.2.2. DNAmethylationsubtypesforadultgliomamulticlassification. Inournextexam-
ple we reanalyze a subset of data used by [9] for studying adult diffuse gliomas. The study
used DNA methylation data obtained from CpG probes for molecular profiling. A total of
p=1206probescollectedoverN =880humantissueswasusedintheiranalysis.
Animportantfindingin[9]wasthatDNAmethylationwaseffectiveinseparatingglioma
subtypes. To informally validate these results, we use the clusters developed in the study as
outcomes in a multiclass analysis. There were L=7 labels: Classic-like, Codel, G-CIMP-
high, G-CIMP-low, LGm6-GBM, Mesenchymal-like, and PA-like. For an added challenge,
clinical data and molecular data available for the samples were also included as features for
the VarPro analysis. In total, p=1241 variables were used. The data is available from the
R-packageTCGAbiolinksGUI.data[57].
Fig. 6 displays the VarPro standardized importance values for the selected features. IDH
status,telomerelength,andTERTpromoterstatus,whichwerethreeoftheaddedmolecular
variables, are seen to be informative. [9] found that IDH status was a major determinant of
the molecular profile of diffuse glioma, so these results are in agreement with the original
study. The VarPro analysis also identified DNA methylation probes with significant impor-
naemG
naemG
naemG
noisicerP
noisicerP
noisicerP
ycaruccA
ycaruccA
ycaruccA20 LUANDISHWARAN
tance values. This is interesting and useful since it identifies probes that potentially provide
additionalpowertoseparatesubtypesbeyondthemolecularvariablesmentioned.
To explore the ability of probes to separate subtypes, Fig. 7 shows some of the top DNA
methylationprobesversusgliomasubtypes.Datadistributionforeachvariableandfrequency
distributionofclustersubtypeisgivenalongthediagonal.Lowersubdiagonalblocksinblue
arescatterplotsofprobevaluesinpairs;uppersubdiagonalinpurplearedensitycontoursfor
thepairs.BoxplotsinredshowDNAmethylationprobevaluesforeachsubtype.Forexample,
the top right boxplot is cg15439862, which shows particularly low values for LGm6-GBM
andMesenchymalcomparedtoothersubtypes,indicatingthisprobe’seffectivenessindistin-
guishing these two subtypes from the rest. Similar patterns are observed with other probes,
demonstratingtheirabilitytoseparatesubtypes.Theseresultssupporttheconclusionsofthe
originalstudyandhighlightVarPro’seffectivenessinaddressinghigh-dimensionalbioinfor-
maticchallenges.
5. Modified VarPro using external estimators: time to event data. Now we discuss
theuseofexternalestimators,whichisawaytoextendtheVarPromethodfrombeingmodel-
independenttomodel-specific.RecallthatVarProestimatesψ(X)=E(g(Y) X)byusinga
|
sample average of g(Y) calculated locally within a region defined by a rule. For a rule ζ
n,k
anditsregionR =R(ζ )withsamplesizem ,VarPromakesuseoftheestimator(4)
n,k n,k n,k
n
1
θˆ (ζ )= g(Y )I X R(ζ ) .
n n,k i i n,k
m { ∈ }
n,k
i=1
(cid:88)
However,dependingonthecomplexityoftheproblem,thismaynotalwaysbesuitableasit
couldbeinefficientordifficulttoestimateψ locallythisway.
Thestrategytoovercomethisistouseanexternalestimatorbuildingontheideasdiscussed
inSection2.Foreachn,letψ :X Rbeanexternalestimatorforψ.ThemodifiedVarPro
n
estimatorisdefinedasfollows.For→ eachrule ζ ,themodifiedprocedurereplaces θˆ (ζ )
n,k n n,k
with
n
1
θ˜ (ζ )= ψ (X )I X R .
n n,k n i i n,k
m { ∈ }
n,k
i=1
(cid:88)
Likewise,thereleaseruleθˆ (ζS ),whichreleasestheruleζ alongthecoordinatesofaset
n n,k n,k
S 1,...,p ,isreplacedwith
⊂{ }
n
1
θ˜ (ζS )= ψ (X )I X RS .
n n,k mS n i { i ∈ n,k }
n,k i=1
(cid:88)
Therefore, g(Y ) used in the original formulation is replaced with ψ (X ) which serves to
i n i
estimate the conditional mean of g(Y ). Given rules ζ ,...,ζ , the modified VarPro
i n,1 n,K
n
measureofimportanceforS is
K
n
∆˜ (S)= W θ˜ (ζS ) θ˜ (ζ )
n n,k n n,k n n,k
| − |
k=1
(cid:88)
forprechosenweights0 W 1, K n W =1.
≤ n,k ≤ k=1 n,k
An important application of the modified procedure is survival analysis. This is because
(cid:80)
trying to locally estimate the survival function is difficult if right censoring is present. In
suchsettings,duetocensoring,theobserveddatais(T,δ,X),whereT =min(To,Co)isthe
time of event, δ =I To Co is the censoring indicator, and To,Co are the true survival
{ ≤ }VARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 21
0 1 2 3 4
standardized importance
TERT promoter status
telomere length tumor
cg00669856
IDH status
cg24719575
cg15752043
cg04062391
cg19815904
cg01817029
cg11251858
cg01295203
cg23131007
cg12045002
cg01009664
cg10150813
cg18438777
cg15439862
FIG 6. Heatmap of standardized importance from VarPro multiclass analysis of adult glioma DNA
methylationdata(N =880,p=1241).
FIG7.DNAmethylationprobevaluesandgliomasubtypes.
AP ledoC wol−PMIC−G hgih−PMIC−G MBG−6mGL cissalC lamyhcneseM22 LUANDISHWARAN
andcensoringtimes.Becauseonly(min(To,Co),δ,X)isobserved,thetimeofeventisonly
observedifcensoringisnotpresent.ThesurvivalfunctionisS(t X)=P To>t X ,and
| { | }
this corresponds to ψ(X)=E(g(To) X) where g(To)=I To >t . Let ζ be a rule and
| { }
R(ζ) its region. Because To is potentially unobserved, we cannot estimate S(t X) locally
|
usinganestimatorlike
θˆ (ζ):=
n i=1g(Y i)1
{X i∈R(ζ)} =
n i=1I {T io>t,X
i
∈R(ζ)
}.
n n I X R(ζ) n I X R(ζ)
(cid:80) i=1 { i ∈ } (cid:80) i=1 { i ∈ }
Ifwewanttouseasampleaverageoftheobserveddata,asintheoriginalVarProformulation,
(cid:80) (cid:80)
then we have to account for censoring. For example, under the assumption of conditional
independence, one approach could be to use an IPCW (inverse of probability of censoring
weighting)estimatorsuchas
n −1 n
δ
θˆ (ζ)= I X R(ζ) i I T >t,X R(ζ)
n i i i
{ ∈ } G(T ) { ∈ }
(cid:32) (cid:33) i
(cid:88)i=1 (cid:88)i=1 (cid:104) (cid:105)
where G(u)=P C >u is the unknown censoring distribution. However, there are known
{ }
issues with IPCW estimators, such as estimation for G, and problems with inverse weights
becominglarge[25,24].
As another example, consider variable selection for the restricted mean survival time
(RMST) [33, 1, 56, 40]. The RMST is a useful quantity summarizing lifetime behavior and
isdefinedasthesurvivalfunctionintegratedfrom[0,τ]
τ τ To∧τ
S(t X)dt=E 1 dt X =E dt X =E[To τ X]. (16)
{To>t}
| ∧ |
(cid:90)0 (cid:20)(cid:90)0 (cid:12) (cid:21) (cid:20)(cid:90)0 (cid:12) (cid:21)
The time horizon τ is usually selected t(cid:12)o represent a clinica(cid:12)lly meaningful endpoint, such
(cid:12) (cid:12)
as 1 year or 5 year survival. Just like the survival function due to censoring, the RMST is
difficulttoestimatelocally.Therefore,usinganexternalestimatorisawaytoovercomethis
issue.
The following result shows that the modified VarPro procedure is consistent under the
assumption (A5) that ψ converges uniformly to ψ. While this might seem to be a strong
n
assumption,convergenceonlyhastoholdoverasuitablydefinedsubspace(seeAppendixF
forfurtherdiscussionregarding(A5)).
THEOREM5.1. TheconclusionsofTheorem3.1andTheorem3.2holdfor∆˜ n(S)under
theirstatedconditionsifinadditionassumption(A5)ofAppendixFholds.
Inthenextsubsectionswepresentexamplesillustratingthemodifiedprocedureappliedto
rightcensoredsurvivalsettings.
5.1. High-dimensional, low sample size, variable selection for survival. For our first il-
lustration we consider a high-dimensional survival simulation. The survival time To was
simulatedby
p
2 4 1 2 3
To=log 1+V exp β X(j) , V exp(.5)+ exp(1)+ exp(1.5)+ exp(3)
j
   ∼ 10 10 10 10
(cid:88)j=1 (cid:110) (cid:111)
  
where V was sampled independently of X and is a four-component mixture of exponential
randomvariableswithrateparameters(inversemeans).5,1,1.5,3.Thefirstp =10features
0
of X were signal variables: these were assigned coefficient values β = 1log(1+ p/p ).
j 2 0
(cid:112)VARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 23
All features had marginal uniform U(0,1) distributions. The noise features were uncorre-
lated; the signal features had pairwise correlation ρ=3/7. Random censoring at 50% and
75%rateswereused.AsamplesizeN =200wasusedwhilevaryingp.
For ψ, we used the integrated cumulative hazard function (CHF) and for the external es-
timator ψ , we chose random survival forests (RSF) [37] using the randomForestSRC
n
package [36]. Because the ensemble CHF is piecewise constant, no approximation was
neededtointegrateit.
Comparison procedures included Cox regression with lasso penalization (coxnet) using
the glmnet package [58] and BC-VIMP and minimal depth (MD) [39, 38] using the
randomForestSRC[36]R-package.Thesemethodswerespecificallyselectedduetotheir
proven effectiveness in feature selection for high-dimensional survival problems. Given the
complexnatureofthesimulations,carefultuningwasnecessaryforallmethods.Forcoxnet,
thedefaultone-standarderrorrulefromglmnetforchoosingthelassoregularizationparam-
eter λ during 10-fold validation resulted in overly sparse solutions. Therefore, we opted for
theλthatproducedthesmallestout-of-sampleerror(theminimumrule).Forminimaldepth,
we used the mean minimal depth threshold value [39, 38]. For VarPro, the Z cutoff value
0
was obtained using an out-of-sample technique similar to the regression and classification
benchmarkexperimentsofSection4.Specifically,sequentialmodelsusedtoselecttheopti-
mized cutoff value were fit using RSF. Out-of-sample performance was evaluated using the
continuousrankprobabilityscore(CRPS)[25,23].
Simulations were repeated 100 times independently. Feature selection was assessed by
gmean, precision and accuracy. The results were averaged across the runs and are given
inTable1.Similartoourpreviousbenchmarkexperimentsforregressionandclassification,
weobservethatVarProachievesthebestoverallperformanceinallmetrics(adjustedp-value
<0.00001)
TABLE1
High-dimensionalsurvivalsimulation(N =200,p=500,1000,1500andp0=10)wheresignal
variablesarecorrelated.Randomcensoringwasappliedat50%and75%rates.Resultsare
averagedover100independentruns.
50%Censoring 75%Censoring
p Gmean Precision Accuracy Gmean Precision Accuracy
500 0.91 0.49 0.97 0.77 0.42 0.97
coxnet 1000 0.93 0.42 0.98 0.79 0.36 0.98
1500 0.94 0.39 0.99 0.79 0.32 0.99
500 0.95 0.55 0.98 0.87 0.42 0.97
VarPro 1000 0.96 0.53 0.99 0.91 0.43 0.99
1500 0.97 0.57 0.99 0.92 0.43 0.99
500 0.86 0.07 0.74 0.81 0.06 0.70
BC-VIMP 1000 0.89 0.05 0.79 0.84 0.04 0.75
1500 0.91 0.04 0.82 0.86 0.03 0.79
500 0.88 0.37 0.80 0.77 0.41 0.81
MD 1000 0.88 0.43 0.82 0.72 0.38 0.79
1500 0.89 0.52 0.84 0.67 0.52 0.83
5.2. Heartraterecoverylongtermsurvival. Exercisestresstestingiscommonlyusedto
assess patients with known or suspected coronary artery disease. A useful predictor of mor-
talityisfallinheartrateimmediatelyafterexercisestresstesting,orheartraterecovery[32].
Heartraterecoveryisdefinedastheheartrateatpeakexerciseminustheheartratemeasured24 LUANDISHWARAN
1minutelater.Thehypothesisthatheartraterecoverypredictsmortalityhasbeentestedand
validatedinanumberofcohorts[12].Herewestudythisissuebyconsideringhowpredictive
heartraterecoveryisinthepresenceofotherpotentiallyusefulfeaturesbymakinguseofthe
modifiedVarProprocedure.
Forthisanalysisweusedatafromthestudyin[35].ThestudyconsideredN =23,701pa-
tientsreferredforsymptom-limitedexercisetesting.Eachpatientunderwentanuprightcool-
downperiodforthefirst2minutesafterrecovery.Detaileddataregardingreasonfortesting,
symptoms, cardiac risk factors, other medical diagnoses, prior cardiac and noncardiac pro-
cedures, medications, resting electrocardiogram, resting heart rate, and blood pressure were
recorded prospectively prior to testing. During each stage of exercise, and during the first
5 minutes of recovery, data were recorded regarding heart rate, blood pressure, ST-segment
changes, symptoms, and arrhythmias. In total p=85 variables were available for the anal-
ysis. All cause mortality was used for the survival outcome. Data was right-censored with
meanfollow-upof5.7years(range.75to10.1years)duringwhich1,617patientsdied.
For ψ, we use the RMST evaluated at τ =3 years. The RMST was calculated by (16)
using the estimated survival function from a RSF analysis. As before, calculations used the
randomForestSRC package [36]. The standardized importance values from VarPro are
given in Fig. 8 (D). Signal variables (red) and noise variables (blue) are identified using a
Z cutoff value where the latter was calculated using an out-of-sample strategy as in the
0
previous section. From the figure, we can observe that heart rate recovery is identified as a
signalvariable.However,atthesametime,wealsoobserveseveralothersignificantvariables,
somewithevenlargerimportancevalues,suchasage,peakmet,peakheartrateandsex.
For comparison, we re-analyzed the data using tree boosting with Cox regression. Com-
putations were run using the R-package gbm [26]. The top 5 variables from boosting were
age,peakmet,heartraterecovery,COPDandheartrate,whichgenerallyagreewiththetop
variables found by VarPro. However, an interesting difference between the two procedures
wastheirrankingforsex.VarProrankedsexasamongitstop5variableswhereassexdidnot
evenmakeitintothetop12variablesforboosting.Thisisimportantbecausesexisavariable
thatisoftenoverlookedintheheartfailureliterature.
Further analysis is presented with separate Kaplan-Meier survival curves for men and
women in Fig. 8 (A), showing minimal differences. However, when stratifying peak met (a
significant variable) into deciles and plotting survival curves for each category, a notable
distinction emerges. Formen (C), survival decreasesmore significantly than forwomen (B)
at lower peak met values (survival curves descending from top to bottom). This suggests
that men are more adversely affected by reduced peak met, implying greater sensitivity to
exercisecapacity,aspeakmetisanindicatorofthis.Theseobservationsareconsistentwith
prior research by [31], which indicated that women have a lower mortality risk than men at
anygivenlevelofexerciseoroxygencapacity.
6. Discussion. We demonstrated that VarPro performs well in many settings. In a large
cardiovascular survival study (Section 5), VarPro was able to identify a variable without a
maineffectbutwithastronginteraction,whichledtodiscoveringameaningfuldifferencein
long-term survival for patients at risk for heart disease. This is impressive because identify-
ingvariableswithnomaineffectsbutsignificantinteractionsisgenerallyconsidereddifficult,
even for the very best variable selection methods. The ability to deal with interactions was
also evident in the synthetic regression experiments (Section 4), where models frequently
included interaction terms. VarPro not only performed well but also stood out in scenarios
with correlated features, which is valuable since real-world data often involves complex in-
teractions and correlations. Section 4 provided further illustration of the ability to handle
real-worldcorrelationusingalargebenchmarkanalysisofmicroarraydatasets.VARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 25
(A) (D)
100 0 10 20 30
80 wtht
weight
60 vtr vvvvvvvp vvvvvvvvvvtttttttttt tttttrttrrrrrrrrrr rrrrrrrppppppppppe ppppppp c e eeeeeeeeeeeeeeeexxxxxxxxxxxxxxxxx
40
stntttttttttttttttttttttt ottttttttttcttcccccccccccccccc nllllllllllllaaallaalllaaaaa daaaaaaauuuuuuuuuuuuuuuuuddddddddddddddddd
ssssssssssssssssstttttttttttttttattaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbnnnnnnnnnnnnnnnnnbbbbbbbbbbbbbbbbb
20 ssssssssssssssssstttttttttttttttattaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbnnnnnnnnnnnnnnnnn
smknow
0 sbprest
0 2 4 6 8 10 rest rrrrrrrrrrs rrrrrrrbbbbbbbbbbbbbbbbbt bbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbbb
Years priorpppppppppppppp cpppvvvvvvvvvvvv avvvvvdddddddddddddd dddd
peak met
(B) 100 peak hr
nnnnnnnnnnnnnnnnnsssssssssssssssssvvvvvvvvvvvvvvvvvttttttttttttttt tt r rrrrrrrrrrrrrrrreeeeeeeeeeeeeeeeeccccccccccccccccc
nnnnnnnnnnnnnnnnniiiiiiiiiiiitttiittiiittttttttttrttrrrrrrrrrrrrrrrraaaaaaaaaaaaaaaaatttttttttttttttetteeeeeeeeeeeeeeeesssssssssssssssss
80 nnnnnnnnnnnnnnnnniiiiiiiiiiiifffiiffiiifffffffffeeefffeeeeeeeeeeeeeeddddddddddddddddd
llllllllllllooolloollloooooooooooowwwwwwwwwwwwwwwwwrrrrrrrrrrrrrrrrreeeeeeeeeeeeeeeeeccccccccccccccccc
60 llllllllllllooolloollloooooooooooowwwwwwwwwwwwwwwwwcccccccccccccccccrrrrrrrrrrrrrrrrriiiiiiiiiiiiiiiii
iiiiiiiiiiiinnniinniiinnnnnnnnnnnnsssssssssssssssssuuuuuuuuuuuuuuuuulllllllllllliiilliillliiiiiiinnniinniiinnnnnnnnnnnn
image
40 htnrx
hhhhhhhhhhhhhhhhhtttttttttttttttnttnnnnnnnnnnnnnnnn
20 hhhhhhhhhhhhhhhhhiiiiiiiiiiiiccciicciiicccccccccccchhhhhhhhhhhhhhhhhooooooooooooooooolllllllllllllllll
height
heart rate recovery
0 heart rate
0 2 4 6 8 10 gender
fffffffffffffffpffppppppppppppppppvvvvvvvvvvvvvvvvvccccccccccccccccc r rrrrrrrrrrrrrrrrsssssssssssssssssttttttttttttttttt
Years fp fv ffffffffffffffpffpppppppppppc
pppppvvvvvvvvvv
vvvvvvvr ccccccccccccccccce
e
eeeec
eeeeeeeeeeeexxxxxxxxxxxxxxxxx
fitness
(C) 100 exec
esrd
eeeeeeeeeeeeeeeeecccccccccccccccccgggggggggggggggggllllllllllllvvvllvvlllvvvvvvvvvvvvhhhhhhhhhhhhhhhhh
80 dddddddddddddddddttttttttttttttt tt t ttttttttttttttetteeeeeeeeeeeeeeeesssssssssssssssssttttttttttttttttt
diabetes
60 dbprest
cccccccccccccccccvvvvvvvvvvvvvvvvvaaaaaaaaaaaaaaaaa
copd
40 bsa
bbbbbbbbbbbbbbbbbpppppppppppppppppvvvvvvvvvvvvvvvvvccccccccccccccccc r rrrrrrrrrrrrrrrrsssssssssssssssssttttttttttttttttt
20 bbbbbbbbbbbbbbbbbpppppppppppppppppvvvvvvvvvvvvvvvvvccccccccccccccccc r rrrrrrrrrrrrrrrreeeeeeeeeeeeeeeeeccccccccccccccccc
bbbbbbbbbbbbbbbbbpppppppppppppppppvvvvvvvvvvvvvvvvvccccccccccccccccc e eeeeeeeeeeeeeeeexxxxxxxxxxxxxxxxx
0 bmi bbbbbbbbbbbbbbbbbeeeeeeeeeeeeeeeeetttttttttttttttattaaaaaaaaaaaaaaaabbbbbbbbbbbbbbbbbllllllllllllooolloolllooooooooooookkkkkkkkkkkkkkkkk
0 2 4 6 8 10 bbbbbbbbbbbbbbbbbdddddddddddddddddiiiiiiiiiiiillliilliiilllllllaaallaalllaaaaaaaaaaaattttttttttttttttt
aaaaaaaaaaaaaaaaasssssssssssssssssooooooooooooooooo
Years a aaaaag aaaaaaaaaaaaccccce
cccccccccccceeeeeeeeeeeeeeeeeiiiiiiiiiiiiiiiii
FIG 8.ResultsofheartraterecoverysurvivalanalysisusingthemodifiedVarProprocedure.(A)Sur-
vivalformenandwomen;(B)Survivalforwomen,stratifiedbypeakmet;(C)Survivalformen,strati-
fiedbypeakmet;(D)Standardizedimportanceforvariables(redaresignal,bluearenoisevariables).
Another strength of VarPro is that it makes use of rule-based selection. This replaces the
problem of building a high-performance model [49, 47] with a series of lower-dimensional
localized variable selection problems that can be solved computationally fast. All the ex-
amples presented in this paper can be computed efficiently, and in our experience, we have
foundVarProtobeveryfast.Asanillustration,Fig.9showstheCPUtimesforVarPro(red
lines) across different simulation sizes from N =250 to N =15,000 and dimensions from
p=10 to p=5000 using the Friedman 1 regression simulation (Appendix E). For context,
CPU times for BC-VIMP (black lines) using the R-package ranger [68] and for gradient
boosting (blue lines) using the R-package gbm [26] are also displayed, noting that gradient
boostingtimeswerelimitedbysamplesizesanddimensionsduetolongruntimedurations.
VarPro’sruntimesprovetobemorefavorableas N and p increase,highlightingitscompu-
tationalefficiencyandviabilityinhandlinglargedatasetscomparedtoexistingmethods.
Despite the encouraging results, the method can still be enhanced. A potential area for
improvementisrulegeneration.Ourapproachutilizedtreesguidedbysplit-weightsderived
fromcombininglassowithshallowtrees.Althougheffectiveinthiscontext,thedevelopment
of automated strategies for generating high-performing rules needs additional exploration.
Nonetheless,astrengthofourmethodologyliesinprovidingatheoreticalroadmapforwhat
lavivruS
lavivruS
lavivruS26 LUANDISHWARAN
p = 10 p = 50 p = 100 p = 250 p = 1000 p = 5000
5.0
Method
2.5
BC−VIMP
GBM
0.0 VarPro
−2.5
0 5000 10000 150000 5000 10000 150000 5000 10000 150000 5000 10000 150000 5000 10000 150000 5000 10000 15000
Sample size
FIG9.LogCPUtimesinsecondsforVarPro(redlines)usingtheFriedman1simulation(AppendixE)
withvaryingsamplesizeN anddimensionp.ShownforcomparisonarelogCPUtimesforBC-VIMP
(black lines) using the ranger R-package (500 trees used for each forest) and gradient boosting
(bluelines)usingthegbm R-package(theoptimalnumberoftreesusedineachboostedexperiment
wasdeterminedby10-foldvalidationwithamaximumvalueof2000trees).
needstobedone.Therulegenerationstepneedstoguaranteethatregionsforsignalvariables
shrinktozero,butashasbeenpointedout,suchanassumptionseemsreasonable.Giventhe
minimalassumptionsonruleconstruction,thisapproachshouldfacilitatefurtheradaptations
andapplications.
Anotherpotentiallimitationisidentificationofsignalvariablesinscenarioswhereaunique
Markovboundarymaynotexist.Asdiscussedby[59],therecanbemultipleMarkovbound-
aries of the response variable. In other words, ψ may not be identified because several vari-
ablescouldinterchangeablyprovidethesameamountofinformationforψ.Sinceourcurrent
discovery algorithm is to quantify the relative importance of predictors one by one, we tend
toselectthesignalvariablesastheunionsetofmultipleMarkovboundariesthatcontainre-
dundant information. However, in this case, our method retains its property of being able to
filternoisevariables.
We use the following simulation for illustration. Data was drawn from the regression
model
Y =X(1)+X(2)+ε, ε N(0,1).
∼
Thefeaturevectorisofdimension p=150 anddrawnfromamultivariatenormalwithzero
meanandequicorrelationmatrixwithcorrelationparameterρ=.4.VariablesX(3) andX(4)
weremodifiedsothat
X(3)=βX(1)+(1 β)X(2), X(4)=(1 β)X(1)+βX(2).
− −
Therefore,thisreflectsaregressionsettingwheresomecolumnsarelinearlyrelatedtoothers.
Noticethat
ψ(X)=E(Y X)=X(1)+X(2)=X(3)+X(4).
|
emit
UPC
golVARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 27
ThereforeaccordingtoDefinition1.1,thesignalvariablesmustbeS = 1,2,3,4 .Itcannot
be S = 1,2 , since we would have ψ(X) = ψ(X\(S)) = ψ(X{3,4}).{ Likewis} e it cannot
be S = { 3,4} , since then ψ(X)=ψ(X\(S))=ψ(X{1,2}). Therefore S must contain all 4
{ }
variables,creatingaredundantscenario.
Fordemonstration,VarProwasexecuted1000timesindependentlyusingtheabovesimu-
lation,withasamplesizeofN=1500andβ=1/4.ThefirstrowofTable2liststheaveraged
standardized importance for the first 4 variables, while all remaining variables are grouped
underthe“noise”category.Thesecondrowshowsthefrequencyofeachvariable’sselection.
The results highlight VarPro’s capabilities: (1) efficiently weeding out noise variables and
(2) identifying redundant signal variables, confirming the earlier statements about VarPro’s
performanceinscenarioswithnon-uniqueMarkovboundaries.
TABLE2
Regressionsimulationwithnon-uniqueMarkovboundary.ThefirstrowlistsVarProstandardized
importancevalues.Thesecondrowliststheselectionfrequencyforeachvariable.Theconditional
meancanbedescribedbyboth X(1),X(2) and X(3),X(4) ,thereforethereisredundancyinthe
{ } { }
definitionofasignalvariable.VariablesX(j)forj=5,...,150arecombinedintocategory“noise”.
(1) (2) (3) (4)
X X X X noise
Importance 5.63 5.61 10.30 10.29 0.00
Percent 98.70 99.10 100.00 100.00 0.03
APPENDIXA: UNIFORMAPPROXIMATIONFORTHEAVERAGESIZEOFA
REGION
Thefollowinglemmawillbeusedinseveralplacesandprovidesauniformapproximation
fortheaveragesamplesizeofaregion.Thenotationb Ïa isusedtosignifyb /a .
n n n n
→∞
LEMMA A.1. Let R
n,k
X be a collection of sets such that P R
n,k
> 0, k =
1,...,K ,anddefinem =⊆ n I X R whereX ,...,X are{ i.i.d.r} andomvec-
n n,k i=1 { i ∈ n,k } 1 n
tors over X. If K O(logn) and m m Ï√nloglogn, then the following identity
n n,k n
holdsoverasetwith≤ probabili(cid:80)tytendingto≥
oneuniformlyoverk=1,...,K :
n
n 1 loglogn
= 1+ξ∗ , where ξ∗ 0. (17)
m
n,k
P(R n,k) n,k | n,k |≤ (cid:112)n−1/2m
n
→
(cid:16) (cid:17)
PROOF. Observe that m
n,k
= n i=1I {X
i
∈
R
n,k
}
is a sum of i.i.d. Bernoulli random
variables.ThereforebyHoeffding’sinequality[30],
(cid:80)
P max n−1m P(R ) ε 2K exp 2ε2n .
n,k n,k n
1≤k≤K − ≥ ≤ −
(cid:26) n (cid:27)
Let A nc be the event inside(cid:12) (cid:12)the probability. If ε(cid:12) (cid:12)is allowed to depe(cid:0) nd on n(cid:1) so that 2ε2n
2(loglogn), then the bound on the right is order K (logn)−2 0. Therefore over A , a≥ n
n n
→
eventwhichoccurswithprobabilitytendingtoone,
ξ ξ , whereξ =P(R ) n−1m , ξ =n−1/2 loglogn.
n,k n n,k n,k n,k n
| |≤ −
Usingm n,k m n,thenwithprobabilitytendingtooneoverA n: (cid:112)
≥
n 1
ξ∗
ξ ξ
= + n,k , where ξ∗ := n,k n .
m n,k P(R n,k) P(R n,k) (cid:12) n,k n−1m n,k(cid:12)≤ n−1m n
(cid:12) (cid:12)
Uponrearrangementthisgives(17). (cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)28 LUANDISHWARAN
APPENDIXB: UNIFORMAPPROXIMATIONFORSAMPLEAVERAGESWITH
VARYINGSIZE
Inorderfor∆ (S)tohavegoodtheoreticalperformance,θˆ (ζ ),...,θˆ (ζ )andthe
n n n,1 n n,K
n
released values θˆ (ζS ),...,θˆ (ζS ) must be controlled uniformly. This imposes a limit
n n,1 n n,K
n
onthesequencesK ,m (ζ )andm (ζS ).Wedescribealemmaforthispurpose.
n n n,k n n,k
First we rewrite the VarPro estimators in a slightly different way. For a given rule ζ, let
b(ζ)=E[ψ(X)I X R(ζ) ] where recall that ψ(X)=E(g(Y) X). Notice that θˆ can be
n
{ ∈ } |
rewrittenas
n
1 nb(ζ)
θˆ (ζ)= Z (ζ)+ , (18)
n i
m (ζ) m (ζ)
n n
i=1
(cid:88)
whereZ (ζ)=g(Y )I X R(ζ) b(ζ)arei.i.d.randomvariableswithameanofzero:
i i i
{ ∈ }−
E(Z (ζ))=E E (g(Y) ψ(X)I X R(ζ) X
i
− { ∈ }
(cid:26) (cid:104) (cid:12) (cid:105)(cid:27)
(cid:12)
=E I X R(ζ) E (g(Y) ψ(X)(cid:12) ) X =0.
{ ∈ } −
(cid:26) (cid:27)
(cid:104) (cid:12) (cid:105)
Recall that m =m (ζ ), mS =m (ζS ) and R =R((cid:12)ζ ), RS =R(ζS ). Ap-
n,k n n,k n,k n n,k n,k n,k n,k n,k
plyingasimilarcenteringasin(18)tothereleasedrule,wehave
n n
1 1
θˆ (ζS ) θˆ (ζ )= Z (ζS ) Z (ζ )
n n,k − n n,k (cid:34)mS
n,k i=1
i n,k − m
n,k i=1
i n,k
(cid:35)
(cid:88) (cid:88)
nb(ζS ) nb(ζ )
n,k n,k
+ , (19)
(cid:34)
mS
n,k
− m
n,k (cid:35)
where(similardefinitionsapplytoζS ):
n,k
Z (ζ )=g(Y )I X R b(ζ ), b(ζ )=E[ψ(X)I X R ].
i n,k i i n,k n,k n,k n,k
{ ∈ }− { ∈ }
The sums appearing in (19) will be shown to converge to zero uniformly using the lemma
givennext.Thequantityinthesecondsquarebracketwillbedealtwithlater.Itrepresentsa
“bias”termthatisasymptoticallyzerofornoisevariablesbutnon-zeroforsignalvariables.
B.1. Key lemma for uniform approximation of sample averages. For each n, let
Z(n) ,...,Z(n) be independent random variables such that E(Z(n) ) = 0 and E[(Z(n) )2]
1,k n,k i,k i,k ≤
σ2< fori=1,...,nandk=1,...,K .Let
n
∞
n n
1 1
(n) (n)
S = Z , and T = Z = S
n,k i,k n,k M i,k M n,k
n,k n,k
i=1 i=1
(cid:88) (cid:88)
(n) (n)
where M are random values (not necessarily independent of Z ,...,Z ) satisfying
n,k 1,k n,k
n M M >0fork=1,...,K .Wewishtoidentifyconditionsforthedeterministic
n,k n n
≥ ≥
sequencesM ,K suchthatT convergestozerouniformlyoverk=1,...,K asn .
n n n,k n
→∞
LetL =(nK )−1/2.ThenbyChebyshev’sinequality,foranyconstantC >0,
n n
n C L2 n σ2
P L S C =P Z(n) n E (Z(n) )2 . (20)
n n,k ≥ i,k ≥ L ≤ C2 i,k ≤ C2K
(cid:8) (cid:12) (cid:12) (cid:9) (cid:40)(cid:12) (cid:12)(cid:88)i=1 (cid:12) (cid:12) n(cid:41) (cid:88)i=1 (cid:104) (cid:105) n
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12)VARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 29
Let B = ω : L S C and B = K n B . Because M M , setting δ =
n,k { | n n,k |≥ } n k=1 n,k n,k ≥ n n
L−1M−1=M−1(nK )1/2 weobtain
n n n n (cid:83)
L−1
T = n L S I + T I Cδ + max T I . (21)
n,k n n,k Bc n,k B n n,k B
| | M n,k| | n | | n ≤ 1≤k≤K n| | n
For the first term on the right of (21) to converge, M must converge at a rate faster than
n
(nK )1/2.Forthesecondterm,using(20),observethat
n
K
n
P(B )=P B
n n,k
(cid:32) (cid:33)
k=1
(cid:91)
=P max L S C
n n,k
1≤k≤K | |≥
(cid:26) n (cid:27)
K n K σ2 σ2
n
P L S C = .
≤ {| n n,k |≥ }≤ C2K C2
n
k=1
(cid:88)
ThereforeusingMarkov’sinequality,wehaveforeachε>0,
K n E( T I )
P max T I ε k=1 | n,k | B n
n,k B
1≤k≤K | | n ≥ ≤ ε
(cid:26) n (cid:27) (cid:80)
K
n
E T2 P(B )
n,k n
(cid:88)k=1(cid:114)
(cid:16) (cid:17) (Cauchy-Schwarz)
≤ ε
K
σ n
E T2 (boundfromP(B )above).
≤ Cε n,k n
(cid:88)k=1(cid:114)
(cid:16) (cid:17)
BecauseE(T2 ) M−2 n E (Z(n) )2 M−2nσ2,
n,k ≤ n i=1 i,k ≤ n
(cid:104) (cid:105)
(cid:80) σ2n1/2K
n
P max T I ε .
n,k B
(cid:26)1≤k≤K n| | n ≥ (cid:27)≤ CεM n
ThereforeifM−1n1/2K 0,thenby(21)wehaveshown
n n
→
T Cδ +o (C−1σ2M−1n1/2K )
n,k n p n n
| |≤
uniformly over k. But notice that M−1n1/2K =K1/2 δ Cδ eventually, thus we have
n n n n n
≥
proventhefollowinglemma.
LEMMAB.1. IfM
n
suchthatM n−1n1/2K
n
0then T
n,k
o p(1)uniformlyover
↑∞ → | |≤
k=1,...,K .
n
APPENDIXC: CONSISTENCYFORNOISEVARIABLES(PROOFOF??)
The idea for the proof is as follows. The two sums in the first square bracket of (19) will
(n)
be dealt with by using Lemma B.1 (set M =m , M =m and Z =Z (ζ ), then
n,k n,k n n i,k i n,k
T in Lemma B.1 equals m−1 n Z (ζ )). The term inside the second square bracket
n,k n,k i=1 i n,k
of(19)isabiastermandwillbeshowntobeasymptoticallyequalto
(cid:80)
β (S)=E(ψ(X(S)) X RS ) E(ψ(X(S)) X R ). (22)
n,k n,k n,k
| ∈ − | ∈30 LUANDISHWARAN
Thiswillbeshowntoconvergetozerofornoisevariablesbyusingthesmoothnessassump-
tionsforψ andotherconditionsassumedbythetheorem.
PROOF. Apply Lemma B.1 to each of the sums in the first square bracket of (19). The
lemma applies since Z (ζ ),Z (ζS ) are centered i.i.d. variables with bounded second
{ i n,k i n,k }
moment. The latter holds by Assumption (A1). For Z (ζ ) , let M =m ,M =m
i n,k n,k n,k n n
{ }
where notice that m−1n1/2K 0 when K O(logn) and m = n1/2γ where γ Ï
n n n n n n
logn;thusverifyingtheratecon→ ditionofthelem≤ ma.Moreover,becausemS m since
k,n≥ k,n
R RS ,theconditionsofthelemmaalsoholdfor Z (ζS ) withM =mS ,M =
n,k ⊆ n,k { i n,k } n,k n,k n
m .
n
ThereforebyLemmaB.1,whichholdsuniformly,
K
n
∆ (S) W o (1)+b
n n,k p n,k
≤ | |
k=1
(cid:88)
K
n
o (1)+ W b , uniformly, (23)
p n,k n,k
≤ | |
k=1
(cid:88)
where(seethesecondtermof(19)anduseψ(X)=ψ(X(S)))
n n
b = E[ψ(X(S))I X RS ] E[ψ(X(S))I X R ]. (24)
n,k mS { ∈ n,k } − m { ∈ n,k }
n,k n,k
UsingLemmaA.1wewillshow(24)approximates
E[ψ(X(S))I X RS ] E[ψ(X(S))I X R ]
{ ∈ n,k} { ∈ n,k }
P(RS ) − P(R )
n,k n,k
=E(ψ(X(S)) X RS ) E(ψ(X(S)) X R )
n,k n,k
| ∈ − | ∈
:=ES (ψ) E (ψ), (25)
n,k n,k
−
where for notational simplicity we write ES and E for the conditional expectation of
n,k n,k
X given RS and R . Observe that (25) is the asymptotic bias β :=β (S) discussed
n,k n,k n,k n,k
earlierin(22).
ApplyLemmaA.1notingm m =n1/2γ Ï√nloglogn.By(17),thereexistsaset
n,k n n
≥
A withprobabilitytendingtoone,suchthat
n
n 1
ξ∗
= + n,k , where ξ∗ ξ∗ =γ−1 loglogn 0.
m P(R ) P(R ) | n,k |≤ n n →
n,k n,k n,k
(cid:112)
In a similar fashion, using mS m m =n1/2γ , there exists a set AS with proba-
n,k ≥ n,k ≥ n n n
bilitytendingtoone,suchthat
n 1
ξ∗S
= + n,k , where ξ∗S ξ∗.
mS P(RS ) P(RS ) | n,k |≤ n
n,k n,k n,k
Thusfrom(24),overthesetA AS (aneventwithprobabilitytendingto1),wehave
n n
b = β +ξ∗SES (ψ) (cid:84)ξ∗ E (ψ) β +ξ∗ ES (ψ) + E (ψ) . (26)
n,k n,k n,k n,k n,k n,k n,k n n,k n,k
| | − ≤| | | | | |
(cid:12) (cid:12) (cid:16) (cid:17)
Thesmo(cid:12)othnessassumption(A2)forψand(cid:12)theshrinkingcondition(A3)forR
n,k
arenow
(cid:12) (cid:12)
used to expand ES (ψ) and E (ψ) to first order which will show (25) is asymptotically
n,k n,kVARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 31
zero and will enable us to further bound (26). Let x be an arbitrary point in R . By the
n,k n,k
mean-valuetheorem,foreachx R thereexistsaλ [0,1],suchthat
n,k n,k
∈ ∈
ψ(x) ψ(x )=(x x )′f(x∗ )
n,k n,k n,k
− −
where x∗ =x +λ (x x ) (note that the dependence of λ on x is suppressed).
n,k n,k n,k − n,k n,k
Using f(x∗ )=f(x )+[f(x∗ ) f(x )], the Lipshitz condition (12), and keeping in
n,k n,k n,k − n,k
mindf iszerooverthecoordinatesforN,
ψ(x) ψ(x ) (x(S) x(S) )′f(S)(x ) +C (x(S) x(S) ) ′ (x∗(S) x(S) ) .
| − n,k |≤| − n,k n,k | 0 | − n,k || n,k − n,k |
ApplyingtheCauchy-Schwarzinequalitytothefirsttermontheright,andusingassumption
(A3),wehaveforx R
n,k
∈
ψ(x) ψ(x ) (x(S) x(S) ) f(S)(x ) +C (x(S) x(S) ) ′ (x∗(S) x(S) )
| − n,k |≤|| − n,k ||2 || n,k ||2 0 | − n,k || n,k − n,k |
diam (R ) f(S)(x ) +C
S n,k n,k 2 0
≤ || ||
(cid:104) (cid:105)
r f(S)(x ) +C =r f(x ) +C . (27)
n n,k 2 0 n n,k 2 0
≤ || || || ||
(cid:104) (cid:105) (cid:104) (cid:105)
ThereforeE (ψ)=ψ(x )+r ,where
n,k n,k n,k
r :=E (ψ(X) ψ(x )) r f(x ) +C :=r∗ .
n,k n,k n,k n n,k 2 0 n,k
− ≤ || ||
(cid:12) (cid:12) (cid:104) (cid:105)
Byasimilara(cid:12)rgument,ES (ψ)=ψ(x )+r(cid:12)S where rS r∗ satisfiesthesamebound
(cid:12) n,k n,k (cid:12)n,k | n,k|≤ n,k
as r . This is because (27) holds for x RS because RS only differs from R along
n,k ∈ n,k n,k n,k
thenoisecoordinates(sinceS N).
⊆
ThereforeES (ψ)=ψ(x )+rS andE (ψ)=ψ(x )+r ,andhence
n,k n,k n,k n,k n,k n,k
β = ψ(x )+rS [ψ(x )+r ]=rS r ,
n,k n,k n,k n,k n,k n,k n,k
− −
and(26)canbefurtherb(cid:2)oundedasfollow(cid:3)s:
b rS + r +ξ∗ ψ(x )+rS + ψ(x )+r
n,k n,k n,k n n,k n,k n,k n,k
| |≤| | | | | | | |
2(1+ξ∗)r∗ +2ξ(cid:16)∗ ψ(x ) . (cid:17)
n n,k n n,k
≤ | |
Henceby(23),andassumption(A4),withprobabilitytendingto1,
K
n
∆ (S) o (1)+2(1+ξ∗)r W f(x ) +C
n p n n n,k n,k 2 0
≤ || ||
(cid:88)k=1 (cid:104) (cid:105)
K
n
+2ξ∗ W ψ(x )
n n,k n,k
| |
k=1
(cid:88)
o (1)+O(r )+O(ξ∗)=o (1),
p n n p
≤
wheretheconvergenceisuniform.
APPENDIXD: LIMITINGBEHAVIORFORSIGNALFEATURES(PROOFOF??)
The proof for S = s a signal variable is similar to the noise variable case. The key dif-
{ }
ferenceisdealingwiththebiasterm β :=β (s) (22)whichisnolongerasymptotically
n,k n,k
zero.32 LUANDISHWARAN
PROOF. Adopting the same notation as in the proof of Theorem 3.1, let E
n,k
and Es
n,k
be the conditional expectation for X in R and Rs . The same bound (27) used to derive
n,k n,k
E (ψ) applies here. Thus ψ(x)=ψ(x )+r (x) where r (x) r∗ for x R
n,k n,k n,k | n,k |≤ n,k ∈ n,k
andE (ψ)=ψ(x )+r wherer =E (r (X)) r∗ .
n,k n,k n,k n,k n,k n,k ≤ n,k
The previous argument used for ES (ψ) however no longer applies because the released
n,k
regionnowcontainsasignalvariable.Todealwiththis,letR =A B whereA =
n,k n,k n,k n,k
d ×
I isthesubspaceofR definedbythesignalfeatures.Byassumption(A3),R
l=1 n,k,l n,k n,k
×
is shrinking to zero in the signal features, thus I are shrinking intervals for l=1,...,d.
n,k,l
On the other hand, Rs releases the coordinates in the direction of s and therefore it is
k,n
shrinkinginthesignalcoordinatesinalldirectionsexceptthesdirection.Thisisthesubspace
As = I .NoticethatAs canbewrittenastheunionoftwodisjointregions
n,k l∈S\s n,k,l n,k
×
˙ s−1 d
As =A A∗s , whereA∗s = I Ic I
n,k n,k n,k n,k n,k,l n,k,s n,k,l
× × ×
l=1 l=s+1
(cid:91)
andinparticularthisimplies
I x(S) As =I x(S) A +I x(S) A∗s . (28)
n,k n,k n,k
{ ∈ } { ∈ } { ∈ }
Forx Rs ,
∈ n,k
ψ(x)= ψ(x )+r (x) I x(S) A +ψ(x)I x(S) A∗s .
n,k n,k n,k n,k
{ ∈ } { ∈ }
Using(28),withsom(cid:2)erearrangment,th(cid:3)isimpliesforeachx Rs
∈ n,k
ψ(x)=ψ(x )I x(S) As
n,k n,k
{ ∈ }
+r (x)I x(S) A
n,k n,k
{ ∈ }
[ψ(x) ψ(x )]I x(S) A
n,k n,k
− − { ∈ }
+[ψ(x) ψ(x )]I x(S) As .
n,k n,k
− { ∈ }
ThereforeintegratingwithrespecttoEs ,
n,k
Es (ψ(X))=ψ(x )
n,k n,k
+rs
n,k
Es [ψ(X) ψ(x )]I X(S) A
n,k n,k n,k
− − { ∈ }
(cid:16) (cid:17)
+Es ψ(X) ψ(x ) (29)
n,k n,k
−
(cid:16) (cid:17)
wherers =Es (r (X)I X(S) A )andnoticethat
n,k n,k n,k { ∈ n,k }
rs Es (r∗ I X(S) A ) r∗ Es (I X(S) As )=r∗ .
n,k n,k n,k n,k n,k n,k n,k n,k
| |≤ { ∈ } ≤ { ∈ }
In the proof of Theorem 3.1 it was shown ψ(x) ψ(x ) r∗ for x R . Therefore
| − n,k |≤ n,k ∈ n,k
thethirdtermof(29)isaremaindertermoforderr∗ .
n,k
Thisleavesthefourthtermin(29).Tohandlethis,considerthelocalbehaviorofψ(x)for
x As around the point x˜ =(x(1) ,...,x(s−1) ,x(s),x(s+1) ,...,x(p) )′ As . By the
∈ n,k n,k n,k n,k n,k n,k ∈ n,k
mean-valuetheoremthereexistsapointx˜∗ =x˜ +λ (x x˜ )forsome0 λ 1,
n,k n,k n,k − n,k ≤ n,k ≤
suchthat
ψ(x) ψ(x˜ )=(x x˜ )′f(x˜∗ ).
n,k n,k n,k
− −VARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 33
Becausecoordinatesofx x˜ iszero,
n,k
−
ψ(x) ψ(x˜ ) = (x(l) x(l) )f(l)(x˜∗ )
| − n,k | − n,k n,k
(cid:12) (cid:12)l∈ (cid:88)S\s(cid:16) (cid:17)(cid:12)
(cid:12)
(cid:12) (cid:12)
(cid:12) x(l) x(l) f(l)(x˜∗ ) (cid:12)
≤ | − n,k|·| n,k |
l∈ (cid:88)S\s(cid:16) (cid:17)
x(l) x(l) f(l)(x˜∗ ) + x˜ x(s) f(s)(x˜∗ )
≤ | − n,k|·| n,k | | − n,k|·| n,k |
l∈ (cid:88)S\s(cid:16) (cid:17)
= (x˜ x(S) ) ′ f(S)(x˜∗ )
| − n,k || n,k |
wherex˜=(x(1),...,x(s−1),x˜,x(s+1),...,x(p))′andx˜canbechosentobeanarbitraryvalue
inI .Noticethatx˜ A .Thereforetheright-handsidecanbeboundedusingtheargu-
n,k,s n,k
∈
mentofTheorem3.1fromwhichitfollowsthat
ψ(x) ψ(x˜ ) r∗ , forx As .
n,k n,k n,k
− ≤ ∈
(cid:12) (cid:12)
Recallthatψs (z)=ψ((cid:12) x(1) ,...,x(s−1) ,(cid:12) z,x(s+1) ,...,xd ).Therefore,ψ(x˜ )=ψs (x(s))
n,k (cid:12) n,k n,k (cid:12) n,k n,k n,k n,k
andψ(x )=ψs (x(s) ),fromwhichitfollows
n,k n,k n,k
Es ψ(X) ψ(x ) =Es ψs (X(s)) ψs (x(s) ) +O(r∗ ), (30)
n,k − n,k n,k n,k − n,k n,k n,k
(cid:16) (cid:17) (cid:16) (cid:17)
andhenceusing(29),
β =ES (ψ) E (ψ)
n,k n,k n,k
−
= ψ(x )+Es ψs (X(s)) ψs (x(s) ) +O(r∗ ) ψ(x )+r
k,n n,k n,k − n,k n,k n,k − k,n n,k
(cid:104) (cid:16) (cid:17) (cid:105) (cid:104) (cid:105)
=Es ψs (X(s)) ψs (x(s) ) +O(r∗ ).
n,k n,k − n,k n,k n,k
(cid:16) (cid:17)
Thusthebiasdoesnotvanishasymptoticallyasinthenoisevariablecase.
TofinishtheproofwefollowtherestoftheproofofTheorem3.1.Tosimplifynotationlet
h (z)=ψs (z) ψs (x(s) ).Then
n,k n,k − n,k n,k
K
n
∆ (s)=o (1)+ W β +ξ∗s Es (ψ) ξ∗ E (ψ)
n p n,k n,k n,k n,k n,k n,k
−
(cid:88)k=1 (cid:12) (cid:12)
(cid:12) (cid:12)
K (cid:12) (cid:12)
n
=o (1)+ W β +ξ∗s Es (ψ)
p n,k n,k n,k n,k
(cid:88)k=1 (cid:12) (cid:12)
(cid:12) (cid:12)
K (cid:12) (cid:12)
n
=o (1)+ W (1+ξ∗s )Es (h (X(s)))
p n,k n,k n,k n,k
(cid:88)k=1 (cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) K (cid:12)
n
=o (1)+(1+o (1)) W Es (h (X(s))) .
p p n,k n,k n,k
(cid:88)k=1 (cid:12) (cid:12)
(cid:12) (cid:12)
Goingfromlinetwotolinethree,wehaveusedξ∗s Es(cid:12) (ψ)=ξ∗s Es (h(cid:12) (X(s)))+o (1),
n,k n,k n,k n,k n,k p
wherethe o (1) termisuniformandisdueto(29)combinedwith(30).Finally,thelastline
p34 LUANDISHWARAN
holdsbecause
K
n
(1+ξ∗) W Es (h (X(s)))
n n,k n,k n,k
(cid:88)k=1 (cid:12) (cid:12)
(cid:12) (cid:12)
K (cid:12) (cid:12)
n
W (1+ξ∗s )Es (h (X(s)))
n,k n,k n,k n,k
≥
(cid:88)k=1 (cid:12) (cid:12)
(cid:12) (cid:12)
K (cid:12) K (cid:12)
n n
W Es (h (X(s))) ξ∗ W Es (h (X(s)))
n,k n,k n,k n n,k n,k n,k
≥ −
(cid:88)k=1 (cid:12) (cid:12) (cid:88)k=1 (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12)K (cid:12) (cid:12) (cid:12)
n
=(1 ξ∗) W Es (h (X(s))) .
n n,k n,k n,k
−
(cid:88)k=1 (cid:12) (cid:12)
(cid:12) (cid:12)
Therightinequalityisbecause a+(cid:12)b a b for(cid:12)anyreal-valueda,b.
| |≥| |−| |
APPENDIXE: REGRESSIONSYNTHETICEXPERIMENTSUSEDFOR
BENCHMARKING
RegressionsimulationmodelsusedtotestVarProarelistedbelow:
1. cobra2: ψ(x) = x(1)x(2) +(x(3))2 x(4)x(7) +x(8)x(10) (x(6))2, X(j) U( 1,1),
− − ∼ −
ε N(0,0.12).
∼
2. cobra8:ψ(x,ε)=I x(1)+(x(4))3+x(9)+sin(x(2)x(8))+ε>0.38 ,X(j) U( .25,1),
{ } ∼ −
ε N(0,0.12).
∼
3. friedman1:ψ(x)=10sin(πx(1)x(2))+20(x(3) 0.5)2+10x(4)+5x(5),X(j) U(0,1),
− ∼
ε N(0,1).
∼
x(2)x(3) 1/(x(2)x(4))
4. friedman3:ψ(x)=arctan − ,X(1) U(0,100),X(2) U(40π,560π),
x(1) ∼ ∼
(cid:20) (cid:21)
X(3),...,X(p) U(0,1),ε N(0,1).
∼ ∼
5. inx1:ψ(x)=x(1)(x(2))2 x(3) + x(4) x(5)x(6) ,X(j) U( 1,1),ε N(0,0.12).
| | ⌊ − ⌋ ∼ − ∼
(cid:112)
(x(5))2
6. inx2: ψ(x) = x(3)(x(1) + 1)|x(2)| , X(j) U( 1,1), ε
− (cid:115) x(4) + x(5) + x(6) ∼ − ∼
| | | | | |
N(0,0.12).
7. inx3: ψ(x) = cos(x(1) x(2)) + arcsin(x(1)x(3)) arctan(x(2) (x(3))2), X(j)
− − − ∼
U( 1,1),ε N(0,0.12).
8.
lm:−
ψ(x)=
∼15
x(j),X(j) N(0,1),ε N(0,152).
j=1 ∼ ∼
9. lmi1: ψ(x) = .05f (x) + exp(.02f (x)f (x)), where f (x) = 10 x(j), f (x) =
(cid:80) 1 1 2 1 j=1 2
20 x(j),X(j) U(0,1),ε N(0,.12).
j=11 ∼ ∼ (cid:80)
10. lmi2:ψ(x)=3( 15 x(j))2,X(j) N(0,1),ε N(0,152).
(cid:80) j=1 ∼ ∼
1
11. sup:ψ(x)=10x(cid:80)(1)x(2)+.25 ,X(j) U(0.05,1),ε N(0,0.52).
x(3)x(4)+10x(5)x(6) ∼ ∼
x(9) x(7)
12. sup2:ψ(x)=πx(1)x(2)√2x(3) arcsin(x(4))+log(x(3)+x(5)) x(2)x(7),
− −x(10) x(8)−
(cid:114)
X(j) U(0.5,1),ε N(0,0.52).
∼ ∼
13. supX: SameassupbutwithsmallerN andlargerp.
14. supX2: Sameassup2butwithsmallerN andlargerp.VARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 35
Simulations cobra are from [3] and friedman are from [19]. All experiments used N =
2000andp=40exceptforsupXandsup2XwhereN =500andp=200.Inafirstsetofruns,
featureswereindependentlysampledasdescribedabove.Inasecondrun,allfeaturesretained
thesamemarginaldistributionasbefore,butweretransformedusingacopulasoastomake
all features correlated with correlation ρ=0.9. This was done for all simulations except lm
and lmi2 where the 15 signal features X(1),...,X(15) were correlated within blocks of size
5(1–5,6–10and11–15).
E.1. Additional simulations. Synthetic data was also obtained using the function
regDataGenfromtheR-packageCORElearn.Intheseexperiments,theresponseisran-
domly selected from two different functions. The choice depends on a “switch variable”
which determines whether ψ(x) = x(4) 2x(5) + 3x(6) or ψ(x) = cos(4πx(4))(2x(5)
3x(6)). Therefore, half the time the mode− l is linear and half the time it is nonlinear. Als− o
there are 4 discrete variables a ,a ,a ,a , the first two a ,a carry information about the
1 2 3 4 1 2
switch variable. Variables X(1),X(2) also carry information about the hidden variable. The
simulation was modified to allow for additional noise variables drawn from a U(0,1) dis-
tribution. Different N and p and varying configurations for information about the hidden
variablewereused.Thefollowingsimulationswereconsidered:
15. corelearn1:a ,a andX(1),X(2) containnoinformation;N =300.
1 2
16. corelearn2:a ,a andX(1),X(2) containnoinformation;N =200.
1 2
17. corelearn3:a ,a containfullinformationandX(1),X(2) containnoinformation;N =
1 2
100.
18. corelearn4:a ,a andX(1),X(2) containfullinformation;N =100.
1 2
19. corelearn5:a ,a andX(1),X(2) containfullinformation;N =100;50noisevariables
1 2
added.
20. corelearn6:a ,a andX(1),X(2)containfullinformation;N =100;200noisevariables
1 2
added.
In the first set of runs, features were sampled as described in regDataGen. In the
second run, all features retained the same marginal distribution as before, but were trans-
formedtohavecorrelationρ=0.9.Notethatsignalvariablesforthefirsttwosimulationsare
X(4),X(5),X(6) while the third simulation adds a ,a and the last three simulations adds
1 2
a ,a andX(1),X(2) assignals.
1 2
APPENDIXF: ASYMPTOTICSFORTHEMODIFIEDPROCEDURE(PROOFOF??)
Thefollowingassumptionwillbeusedintheproof:
(A5) Thereexistsasequencer˜ 0andsubspaceX X containingallregions K n R
n → n ⊆ k=1 k,n
andreleasedregions K n RS suchthat ψ (x) ψ(x) r˜ forx X .
k=1 n,k | n − |≤ n ∈ n (cid:83)
The assumption requires (cid:83)ψ
n
to converge uniformly to ψ but some flexibility is allowed in
thatconvergenceonlyhastoholdoverasuitablydefinedsubspace.Forexample,ifψ (x)=
n
h( p α x(l)) and ψ(x)=h( p α x(l)) for h a real-valued function with derivative
l=1 n,l l=1 0,l
h′,thenbythemeanvaluetheorem
(cid:80) (cid:80)
p p
ψ (x) ψ(x) h′ α∗ x(l) x(l)(α α )
n n,l n,l 0,l
| − |≤(cid:12) (cid:32) (cid:33)(cid:12) −
(cid:12) (cid:88)l=1 (cid:12)(cid:88)l=1(cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) p (cid:12) (cid:12) p (cid:12)
(cid:12) (cid:12)
h′ α∗ x(l) x α α 2
≤(cid:12) (cid:12) (cid:32) (cid:88)l=1 n,l (cid:33)(cid:12) (cid:12)·|| ||2 ·(cid:118) (cid:117) (cid:117)(cid:88)l=1| n,l − 0,l |
(cid:12) (cid:12) (cid:116)
(cid:12) (cid:12)
(cid:12) (cid:12)36 LUANDISHWARAN
cobra2 cobra8 corelearn1 corelearn2
1.00
0.75
0.50
0.25
0.00
corelearn3 corelearn4 corelearn5 corelearn6
1.00
0.75
0.50
Method
0.25
0.00
BC−VIMP
friedman1 friedman3 inx1 inx2 GBM 1.00 knockoff forest
0.75 knockoff glmnet
0.50 lasso
0.25 MDI
0.00 RReliefF
inx3 lm lmi1 lmi2 VarPro
1.00 wrapper GBM
0.75 wrapper KNN
0.50
0.25
0.00
sup sup2 supX supX2
1.00
0.75
0.50
0.25
0.00
cobra2 cobra8 corelearn1 corelearn2
1.00
0.75
0.50
0.25
0.00
corelearn3 corelearn4 corelearn5 corelearn6
1.00
0.75
0.50
0.25 Method
0.00
BC−VIMP
friedman1 friedman3 inx1 inx2 GBM
1.00 knockoff forest
0.75 knockoff glmnet
0.50 lasso
0.25 MDI
0.00 RReliefF
inx3 lm lmi1 lmi2 VarPro
1.00 wrapper GBM
0.75 wrapper KNN
0.50
0.25
0.00
sup sup2 supX supX2
1.00
0.75
0.50
0.25
0.00
FIG E1. Area under the precision recall curve (AUC-PR) and gmean (geometric mean of TPR and
TNR)featureselectionperformanceinregressionsimulationswherevariablesareuncorrelated.
whereα∗ issomevaluebetweenα andα .Thesimplestwaytosatisfy(A5)istorequire
n,l n,l 0,l
boundednesswhereX X forX aclosedboundedsubspaceofX.Then(A5)holdsunder
n 0 0
therelativelymildassum⊆ ptionsthath′ iscontinuousand p α α 0wherecon-
l=1| n,l − 0,l |→
vergence can be at any rate. The boundedness condition is easily met as the size of a region
(cid:80)
isentirelycontrolledbythedataanalyst.
RP−CUA
naemgVARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 37
cobra2 cobra8 corelearn1 corelearn2
1.00
0.75
0.50
0.25
0.00
corelearn3 corelearn4 corelearn5 corelearn6
1.00
0.75
0.50
Method
0.25
0.00
BC−VIMP
friedman1 friedman3 inx1 inx2 GBM 1.00 knockoff forest
0.75 knockoff glmnet
0.50 lasso
0.25 MDI
0.00 RReliefF
inx3 lm lmi1 lmi2 VarPro
1.00 wrapper GBM
0.75 wrapper KNN
0.50
0.25
0.00
sup sup2 supX supX2
1.00
0.75
0.50
0.25
0.00
cobra2 cobra8 corelearn1 corelearn2
1.00
0.75
0.50
0.25
0.00
corelearn3 corelearn4 corelearn5 corelearn6
1.00
0.75
0.50
0.25 Method
0.00
BC−VIMP
friedman1 friedman3 inx1 inx2 GBM
1.00 knockoff forest
0.75 knockoff glmnet
0.50 lasso
0.25 MDI
0.00 RReliefF
inx3 lm lmi1 lmi2 VarPro
1.00 wrapper GBM
0.75 wrapper KNN
0.50
0.25
0.00
sup sup2 supX supX2
1.00
0.75
0.50
0.25
0.00
FIGE2.SimilartoFig.E1butusingcorrelatedvariables.
PROOF. For the proof we use a centering argument for ∆˜ n(S) similar to that used for
∆ (S). Let Z⋆(ζ)=ψ(X )I X R(ζ) b(ζ) where b(ζ)=E[ψ(X)I X R(ζ) ]. Us-
n i i { i ∈ }− { ∈ }
ingψ =ψ+(ψ ψ),itfollowsthat
n n
−
n n
1 1
θ˜ (ζS ) θ˜ (ζ ) = Z⋆(ζS ) Z⋆(ζ )
n n,k − n n,k | (cid:34)mS
n,k i=1
i n,k − m
n,k i=1
i n,k
(cid:35)
(cid:88) (cid:88)
RP−CUA
naemg38 LUANDISHWARAN
nb(ζS ) nb(ζ )
n,k n,k
+
(cid:34)
mS
n,k
− m
n,k (cid:35)
n n
1 1
+ Z˜ (ζS ) Z˜ (ζ ) , (31)
(cid:34)mS
n,k i=1
n,i n,k − m
n,k i=1
n,i n,k
(cid:35)
(cid:88) (cid:88)
where Z˜ (ζ)=[ψ (X ) ψ(X )]I X R(ζ) . Observe that the second term in (31) is
n,i n i i i
− { ∈ }
thebiastermasymptoticallyequaltoβ (S)(22)workedoutintheprevioustheorems.The
n,k
terms in the first square bracket in (31) are sums of i.i.d. centered variables and therefore
are similar to the sums in (19) and can be dealt with by Lemma B.1 to show they converge
to zero uniformly in probability. Therefore we only need consider the terms inside the third
backetof(31).
Therefore,considerthebound
K n n
n 1 1
W Z˜ (ζS ) Z˜ (ζ )
(cid:88)k=1 n,k (cid:12) (cid:12)mS n,k (cid:88)i=1 n,i n,k − m n,k (cid:88)i=1 n,i n,k (cid:12) (cid:12)
(cid:12) (cid:12)
K(cid:12) n K n (cid:12)
(cid:12)n W n,k Z˜ (ζS ) + n W n,k Z˜(cid:12) (ζ ) . (32)
≤ mS | n,i n,k | m | n,i n,k |
k=1 n,k i=1 k=1 n,k i=1
(cid:88) (cid:88) (cid:88) (cid:88)
Beginwiththefirstsumontherightof(32).By(A5),
Z˜ (ζS ) ψ (X ) ψ(X ) I X RS r˜ I X RS .
n,i n,k n i i i n,k n i n,k
| |≤| − | { ∈ }≤ { ∈ }
Therefore
K n K n
n W n W
n,k Z˜ (ζS ) r n,k I X RS =r 0.
mS | n,i n,k |≤ n mS { i ∈ n,k } n →
k=1 n,k i=1 k=1 n,k i=1
(cid:88) (cid:88) (cid:88) (cid:88)
Thesecondsumontherightof(32)involvingZ˜ (ζ )isdealtwithsimilarly.
n,i n,k
CodeAvailability. OurcodeispubliclyavailableasanR-packagevarPro andisavail-
ableattherepositoryhttps://github.com/kogalur/varPro.
Funding. Research for the authors was supported by the National Institute Of General
Medical Sciences of the National Institutes of Health, Award Number R35 GM139659 and
the National Heart, Lung, and Blood Institute of the National Institutes of Health, Award
NumberR01HL164405.
REFERENCES
[1] ANDERSEN,P.K.,HANSEN,M.G.andKLEIN,J.P.(2004).Regressionanalysisofrestrictedmeansur-
vivaltimebasedonpseudo-observations.LifetimeDataAnalysis10335–350.
[2] BIAU, G., DEVROYE, L. and LUGOSI, G. (2008). Consistency of random forests and other averaging
classifiers.JournalofMachineLearningResearch92015–2033.
[3] BIAU,G.,FISCHER,A.,GUEDJ,B.andMALLEY,J.D.(2016).COBRA:Acombinedregressionstrategy.
JournalofMultivariateAnalysis14618–28.
[4] BISCHL,B.,LANG,M.,KOTTHOFF,L.,SCHIFFNER,J.,RICHTER,J.,STUDERUS,E.,CASALICCHIO,G.
andJONES,Z.M.(2016).mlr:MachineLearninginR.JournalofMachineLearningResearch171-5.
[5] BREIMAN,L.(2001).Randomforests.MachineLearning455–32.
[6] BREIMAN,L.(2002).Manualonsettingup,using,andunderstandingrandomforestsv3.1.StatisticsDe-
partmentUniversityofCaliforniaBerkeley,CA,USA1.VARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 39
[7] BREIMAN, L., FRIEDMAN, J., STONE, C. J. and OLSHEN, R. A. (1984).ClassificationandRegression
Trees.CRCpress.
[8] CANDES, E., FAN, Y., JANSON, L. and LV, J. (2018). Panning for gold:‘model-X’ knockoffs for high
dimensionalcontrolledvariableselection.JournaloftheRoyalStatisticalSociety:SeriesB(Statistical
Methodology)80551–577.
[9] CECCARELLI,M.,BARTHEL,F.P.,MALTA,T.M.,SABEDOT,T.S.,SALAMA,S.R.,MURRAY,B.A.,
MOROZOVA,O.,NEWTON,Y.,RADENBAUGH,A.,PAGNOTTA,S.M.etal.(2016).Molecularpro-
filing reveals biologically discrete subsets and pathways of progression in diffuse glioma. Cell 164
550–563.
[10] CHEN, R.-C.,DEWI, C.,HUANG, S.-W.andCARAKA, R. E.(2020).Selectingcriticalfeaturesfordata
classificationbasedonmachinelearningmethods.JournalofBigData752.
[11] CHIPMAN, H. A.,GEORGE, E. I.andMCCULLOCH, R. E.(2010).BART:Bayesianadditiveregression
trees.TheAnnalsofAppliedStatistics4266–298.
[12] COLE, C. R., FOODY, J. M., BLACKSTONE, E. H. and LAUER, M. S. (2000).Heartraterecoveryafter
submaximalexercisetestingasapredictorofmortalityinacardiovascularlyhealthycohort.Annalsof
InternalMedicine132552–555.
[13] DEVROYE, L., GYÖRFI, L. and LUGOSI, G. (2013). A Probabilistic Theory of Pattern Recognition 31.
Springer.
[14] DOKSUM,K.,TANG,S.andTSUI,K.-W.(2008).Nonparametricvariableselection:theEARTHalgorithm.
JournaloftheAmericanStatisticalAssociation1031609–1620.
[15] DRAMIN´SKI, M.andKORONACKI, J.(2018).rmcfs:anRpackageforMonteCarlofeatureselectionand
interdependencydiscovery.JournalofStatisticalSoftware851–28.
[16] FAN,J.andLV,J.(2008).Sureindependencescreeningforultrahighdimensionalfeaturespace.Journalof
theRoyalStatisticalSocietySeriesB:StatisticalMethodology70849–911.
[17] FISHER,A.,RUDIN,C.andDOMINICI,F.(2019).AllModelsareWrong,butManyareUseful:Learning
aVariable’sImportancebyStudyinganEntireClassofPredictionModelsSimultaneously.Journalof
MachineLearningResearch201–81.
[18] FRIEDMAN,J.,HASTIE,T.andTIBSHIRANI,R.(2010).Regularizationpathsforgeneralizedlinearmodels
viacoordinatedescent.JournalofStatisticalSoftware331-22.
[19] FRIEDMAN,J.H.(1991).Multivariateadaptiveregressionsplines.TheAnnalsofStatistics191–67.
[20] FRIEDMAN,J.H.(2001).Greedyfunctionapproximation:agradientboostingmachine.AnnalsofStatistics
291189–1232.
[21] FÜRNKRANZ,J.(1997).Pruningalgorithmsforrulelearning.MachineLearning27139–172.
[22] GENUER, R., POGGI, J.-M. and TULEAU-MALOT, C. (2010). Variable selection using random forests.
PatternRecognitionLetters312225–2236.
[23] GERDS, T. A.,CAI, T.andSCHUMACHER, M.(2008).Theperformanceofriskpredictionmodels.Bio-
metricalJournal:JournalofMathematicalMethodsinBiosciences50457–479.
[24] GERDS,T.A.andSCHUMACHER,M.(2006).ConsistentestimationoftheexpectedBrierscoreingeneral
survivalmodelswithright-censoredeventtimes.BiometricalJournal481029–1040.
[25] GRAF, E.,SCHMOOR, C.,SAUERBREI, W.andSCHUMACHER, M.(1999).Assessmentandcomparison
ofprognosticclassificationschemesforsurvivaldata.StatisticsinMedicine182529–2545.
[26] GREENWELL, B., BOEHMKE, B., CUNNINGHAM, J. and DEVELOPERS, G. (2020). gbm: Generalized
BoostedRegressionModelsRpackageversion2.1.8.
[27] GRÖMPING, U. (2009). Variable importance assessment in regression: linear regression versus random
forest.TheAmericanStatistician63308–319.
[28] GUYON,I.,WESTON,J.,BARNHILL,S.andVAPNIK,V.(2002).Geneselectionforcancerclassification
usingsupportvectormachines.Machinelearning46389–422.
[29] GYÖRFI, L., KOHLER, M., KRZYZAK, A., WALK, H. etal.(2002).ADistribution-FreeTheoryofNon-
parametricRegression1.Springer.
[30] HOEFFDING, W.(1963).ProbabilityInequalitiesforSumsofBoundedRandomVariables.Journalofthe
AmericanStatisticalAssociation5813–30.
[31] HSICH, E., CHADALAVADA, S., KRISHNASWAMY, G., STARLING, R. C., POTHIER, C. E., BLACK-
STONE, E. H.andLAUER, M. S.(2007).Long-termprognosticvalueofpeakoxygenconsumption
inwomenversusmenwithheartfailureandseverelyimpairedleftventricularsystolicfunction.The
AmericanJournalofCardiology100291–295.
[32] IMAI, K.,SATO, H.,HORI, M.,KUSUOKA, H.,OZAKI, H.,YOKOYAMA, H.,TAKEDA, H.,INOUE, M.
andKAMADA,T.(1994).Vagallymediatedheartraterecoveryafterexerciseisacceleratedinathletes
butbluntedinpatientswithchronicheartfailure.JournaloftheAmericanCollegeofCardiology24
1529–1535.40 LUANDISHWARAN
[33] IRWIN, J. O. (1949). The standard error of an estimate of expectation of life, with special reference to
expectationoftumourlesslifeinexperimentswithmice.Epidemiology&Infection47188–189.
[34] ISHWARAN, H. (2007). Variable importance in binary regression trees and forests. Electronic Journal of
Statistics1519–537.
[35] ISHWARAN,H.,BLACKSTONE,E.H.,POTHIER,C.E.andLAUER,M.S.(2004).Relativeriskforestsfor
exerciseheartraterecoveryasapredictorofmortality.JournaloftheAmericanStatisticalAssociation
99591–600.
[36] ISHWARAN,H.andKOGALUR,U.B.(2023).RandomForestsforSurvival,Regression,andClassification
(RF-SRC)Rpackageversion3.2.2.
[37] ISHWARAN, H., KOGALUR, U. B., BLACKSTONE, E. H. and LAUER, M. S. (2008). Random survival
forests.TheAnnalsofAppliedStatistics2841–860.
[38] ISHWARAN,H.,KOGALUR,U.B.,CHEN,X.andMINN,A.J.(2011).Randomsurvivalforestsforhigh-
dimensionaldata.StatisticalAnalysisandDataMining:TheASADataScienceJournal4115–132.
[39] ISHWARAN, H., KOGALUR, U. B., GORODESKI, E. Z., MINN, A. J. and LAUER, M. S. (2010).High-
dimensionalvariableselectionforsurvivaldata.JournaloftheAmericanStatisticalAssociation105
205–217.
[40] KIM, D. H., UNO, H. and WEI, L.-J. (2017).RestrictedMeanSurvivalTimeasaMeasuretoInterpret
ClinicalTrialResults.JAMACardiology21179–1180.
[41] KIRA, K. and RENDELL, L. A. (1992). The feature selection problem: Traditional methods and a new
algorithm.InProceedingsoftheTenthNationalConferenceonArtificialintelligence129–134.
[42] KOHAVI,R.andJOHN,G.H.(1997).Wrappersforfeaturesubsetselection.ArtificialIntelligence97273–
324.
[43] KUBAT,M.,HOLTE,R.andMATWIN,S.(1997).Learningwhennegativeexamplesabound.InEuropean
ConferenceonMachineLearning146–153.Springer.
[44] KUHN,M.(2022).caret:ClassificationandRegressionTrainingRpackageversion6.0-91.
[45] KURSA,M.B.andRUDNICKI,W.R.(2010).FeatureSelectionwiththeBorutaPackage.JournalofSta-
tisticalSoftware361–13.
[46] LAGANI,V.,ATHINEOU,G.,FARCOMENI,A.,TSAGRIS,M.andTSAMARDINOS,I.(2017).FeatureSe-
lection with the R Package MXM: Discovering Statistically Equivalent Feature Subsets. Journal of
StatisticalSoftware80. https://doi.org/10.18637/jss.v080.i07
[47] LEE,K.-Y.,LI,B.andZHAO,H.(2016).Variableselectionviaadditiveconditionalindependence.Journal
oftheRoyalStatisticalSociety:SeriesB(StatisticalMethodology)781037-1055.
[48] LEI, J., G’SELL, M., RINALDO, A., TIBSHIRANI, R. J. and WASSERMAN, L. (2018).Distribution-free
predictiveinferenceforregression.JournaloftheAmericanStatisticalAssociation1131094–1111.
[49] LI, L.,DENNIS COOK, R.andNACHTSHEIM, C. J.(2005).Model-freevariableselection.Journalofthe
RoyalStatisticalSociety:SeriesB(StatisticalMethodology)67285-299.
[50] LIU,Y.,ROCˇKOVÁ,V.andWANG,Y.(2021).VariableselectionwithABCBayesianforests.Journalofthe
RoyalStatisticalSociety:SeriesB(StatisticalMethodology)83453-481.
[51] LOUPPE, G.,WEHENKEL, L.,SUTERA, A.andGEURTS, P.(2013).Understandingvariableimportances
inforestsofrandomizedtrees.AdvancesinNeuralInformationProcessingSystems26431–439.
[52] NUTI, G., JIMÉNEZ RUGAMA, L. A. and CROSS, A.-I. (2021). An explainable Bayesian decision tree
algorithm.FrontiersinAppliedMathematicsandStatistics71–9.
[53] PATTERSON,E.andSESIA,M.(2022).knockoff:TheKnockoffFilterforControlledVariableSelectionR
packageversion0.3.5.
[54] RAMÓN,D.-U.andALVAREZDEANDRÉS,S.(2006).Geneselectionandclassificationofmicroarraydata
usingrandomforest.BMCbioinformatics71–13.
[55] ROBNIK-ŠIKONJA, M.,KONONENKO, I.etal.(1997).AnadaptationofReliefforattributeestimationin
regression.InMachinelearning:Proceedingsofthefourteenthinternationalconference(ICML’97)5
296–304.Citeseer.
[56] ROYSTON, P. and PARMAR, M. K. B. (2011). The use of restricted mean survival time to estimate the
treatmenteffectinrandomizedclinicaltrialswhentheproportionalhazardsassumptionisindoubt.
StatisticsinMedicine302409–2421.
[57] SILVA, T. (2022). TCGAbiolinksGUI.data: Data for the TCGAbiolinksGUI package. R package version
1.14.1.
[58] SIMON,N.,FRIEDMAN,J.,HASTIE,T.andTIBSHIRANI,R.(2011).RegularizationpathsforCox’spro-
portionalhazardsmodelviacoordinatedescent.JournalofStatisticalSoftware391-13.
[59] STATNIKOV, A., LEMEIR, J. and ALIFERIS, C. F. (2013).AlgorithmsfordiscoveryofmultipleMarkov
boundaries.TheJournalofMachineLearningResearch14499–566.
[60] STROBL,C.,BOULESTEIX,A.-L.,KNEIB,T.,AUGUSTIN,T.andZEILEIS,A.(2008).Conditionalvari-
ableimportanceforrandomforests.BMCBioinformatics91–11.VARIABLESELECTIONVIATHERULE-BASEDVARIABLEPRIORITY 41
[61] STROBL,C.,BOULESTEIX,A.-L.,ZEILEIS,A.andHOTHORN,T.(2007).Biasinrandomforestvariable
importancemeasures:Illustrations,sourcesandasolution.BMCbioinformatics81–21.
[62] TAN,A.C.,NAIMAN,D.Q.,XU,L.,WINSLOW,R.L.andGEMAN,D.(2005).Simpledecisionrulesfor
classifyinghumancancersfromgeneexpressionprofiles.Bioinformatics213896–3904.
[63] TIBSHIRANI, R.(1996).Regressionshrinkageandselectionviathelasso.JournaloftheRoyalStatistical
Society:SeriesB(Methodological)58267–288.
[64] TONG, Z., CAI, Z., YANG, S. and LI, R. (2022). Model-free conditional feature screening with FDR
control.JournaloftheAmericanStatisticalAssociation1–13.
[65] VAN DER LAAN, M. J.(2006).Statisticalinferenceforvariableimportance.TheInternationalJournalof
Biostatistics2.
[66] WEI, P., LU, Z. and SONG, J. (2015).Variableimportanceanalysis:acomprehensivereview.Reliability
Engineering&SystemSafety142399–432.
[67] WILLIAMSON,B.D.,GILBERT,P.B.,SIMON,N.R.andCARONE,M.(2023).Ageneralframeworkfor
inferenceonalgorithm-agnosticvariableimportance.JournaloftheAmericanStatisticalAssociation
1181645–1658.
[68] WRIGHT, M. N. and ZIEGLER, A. (2017). ranger: A Fast Implementation of Random Forests for High
DimensionalDatainC++andR.JournalofStatisticalSoftware771–17.
[69] ZHONG, W., LIU, Y. and ZENG, P. (2023). A model-free variable screening method based on leverage
score.JournaloftheAmericanStatisticalAssociation118135–146.
[70] ZHU,L.-P.,LI,L.,LI,R.andZHU,L.-X.(2011).Model-freefeaturescreeningforultrahigh-dimensional
data.JournaloftheAmericanStatisticalAssociation1061464–1475.