SIEVE: GENERAL PURPOSE DATA FILTERING SYSTEM
MATCHING GPT-4O ACCURACY AT 1% THE COST
JifanZhang,RobertNowak
UniversityofWisconsin-Madison
{jifan@cs.wisc.edu, rdnowak@wisc.edu}
ABSTRACT
Creatingspecializedlargelanguagemodelsrequiresvastamountsofclean,spe-
cial purpose data for training and fine-tuning. With only a handful of existing
large-scale,domain-specificdatasets,creationofnewdatasetsisrequiredinmost
applications. Thisrequiresthedevelopmentofnewapplication-specificfilteringof
web-scaledata. Filteringwithahigh-performance,general-purposeLLMsuchas
GPT-4ocanbehighlyeffective,butthisisextremelyexpensiveatweb-scale. This
paperproposesSIEVE,alightweightalternativethatmatchesGPT-4oaccuracy
atafractionofthecost. SIEVEcanperformupto500filteringoperationsforthe
costofoneGPT-4ofilteringcall. ThekeytoSIEVEisaseamlessintegrationof
GPT-4oandlightweightT5models,usingactivelearningtofine-tuneT5inthe
backgroundwithasmallnumberofcallstoGPT-4o. Oncetrained,itperformsas
wellasGPT-4oatatinyfractionofthecost. WeexperimentallyvalidateSIEVEon
theOpenWebTextdataset,usingfivehighlycustomizedfiltertaskstargetinghigh
qualityanddomain-specificcontent. Ourresultsdemonstratetheeffectivenessand
efficiencyofourmethodincuratinglarge,high-qualitydatasetsforlanguagemodel
trainingatasubstantiallylowercost(1%)thanexistingtechniques. Tofurthervali-
dateSIEVE,experimentsshowthatSIEVEandGPT-4oachievesimilaraccuracy,
withhumanevaluatorspreferringSIEVE’sfilteringresultstothoseofGPT-4o.
1 INTRODUCTION
LargeLanguageModels(LLMs)haverevolutionizednaturallanguageprocessing,demonstrating
remarkablecapabilitiesacrossawiderangeoftasks. Asthefieldprogresses,thereisgrowinginterest
indevelopingspecializedLLMstailoredtospecificdomainsorapplications(Leeetal.,2023;Gupta
et al., 2024; Li et al., 2024). A critical component in this process is the curation of high-quality,
domain-specificdatasetsfortrainingandfine-tuningthesemodels. However,thetaskoffilteringvast
amountsofweb-scaledatatocreatesuchdatasetspresentssignificantchallengesintermsofcost,time,
andeffectiveness. Existingapproachestodatafilteringandcurationprimarilyrelyontwostrategies:
(1)sourcingdatafromspecific,trustedsources,and(2)employingpre-existingqualityandtoxicity
detectionmodelstofilterharmfulcontent. Forinstance,medicalLLMsoftenutilizedatasetsderived
fromPubMedtoensuredomainspecificity. Whilethesedatafilteringmethodshavetheirmerits,they
sufferfromnotablelimitationsintermsofbothflexibilityandcomprehensiveness. Manydomains
lackcomprehensive,exclusivesourcesofhigh-qualitytextdata,andpre-traineddetectionmodels
typicallyfocusongeneralqualitymetricsandtoxicity,limitingtheirapplicabilityfordomain-specific
queriesorcustomizedqualityrubrics. Furthermore,relyingsolelyonestablisheddatasourcesignores
the vast amount of potentially valuable information available on the broader internet, leading to
datasetsthatmaybelimitedinscopeanddiversity. Theseconstraintscansignificantlyhinderthe
developmentofspecializedlanguagemodelsthatrequirerich,domain-specifictrainingdata.
Recentadvancementsingeneral-purposelanguagemodels,suchasGPT-4o,offerapotentialsolution
tothesechallenges. Thesemodelscanactaseffectivefilteringmechanismswhenprovidedwith
appropriatefilteringprompts. Toillustrateasanexample,onecouldemployGPT-4otoiterateover
alltextsnippetsoftheinternettodeterminewhethereachpiecepertainstothe2024presidential
election. Inthiscontext,themachinelearningmodelservesasabinaryclassificationmechanism,
evaluatingonesnippetatatime. However,thecomputationalcostofapplyingmodelslikeGPT-4o
toweb-scaledatasetsisprohibitivelyexpensiveformostorganizations. Inthispaper,wepresent
1
4202
tcO
3
]LC.sc[
1v55720.0142:viXraFigure1: SystemOverview. Fromuser’sperspective,SIEVEactsasifapplyingGPT-4owiththe
filteringprompttoalltextsnippetsinaweb-scaledataset. TheoutputfromtheSIEVEsystemis
thesetofalltextsnippetsthatreceivea‘pass’. Toreducetheprohibitivelyhighcostofapplying
GPT-4ooneverysnippet,SIEVEutilizesactivelearningtodistilllightweightfilteringmodelsbased
onpretrainedT5encoders,effectivelyreducingtheoverallcosttolessthan1%.
a novel system SIEVEthat addresses these limitations, providing versatile and high-quality data
filteringatafractionofthecostofusinghighperformance,general-purposemodelsdirectly. Shown
inFigure2,ourmethodisbasedonalightweightT5modelincombinationwithasmallnumberof
callstoGPT-4o,effectivelyreducingthecostperfilteringtoonecalltoT5insteadofGPT-4o. Thisis
accomplishedbytraininganddistillingalightweightT5modelinthebackgroundthatlearnstomimic
thefilteringdecisionsofGPT-4oandeventuallyhandlesall/mostofthefiltering. Thisbackground
jobisoptimizedviaanovelactivelearningalgorithmthatfurtherreducestheneedtocallGPT-4o.
Ouractivelearningalgorithmoperatesinastream-basedsetting,sequentiallyandadaptivelyselecting
the most informative text snippets for evaluation by GPT-4o. Inspired by previous work in deep
active learning (Zhang et al., 2022a; Nuggehalli et al., 2023), we propose a novel stream-based
algorithm designed to tackle scenarios where the data distribution is imbalanced. As all of the
filteringdecisionsarehighlyimbalanced(seeTable3),oneofourmaincontributionsisproposing
the algorithm to tackle this imbalance issue. Our algorithm is designed to efficiently label a set
ofmorebalancedanduncertainexamples. Asdemonstratedinourexperiments,SIEVEachieves
GPT-4oqualityfilteringperformanceatlessthan1%ofthecost. Inaddition,comparedtorandom
samplingmethods,ouractivelearningalgorithmreducesthenumberofqueriestoGPT-4obymore
than5x(seeSection5). InSection4,weprovidetheoreticalanalysisofouralgorithm,establishing
formalproofsofbalancednessboundsforactivelearninginscenarioswithimbalancedunderlying
datadistributions. Tothebestofourknowledge,thisrepresentsthefirstattempttoproviderigorous
theoreticalguaranteesinthiscontext.
TheimplicationsofSIEVEarefar-reaching,democratizingaccesstoclean,task-specificdataand
facilitatingthedevelopmentofspecializedlanguagemodelsacrossvariousdomains. Bydramatically
reducingthecostandcomplexityofdatafiltering,SIEVEopensnewavenuesforresearchersand
organizationstocreatetailoredLLMsforspecificapplications,industries,orscientificdisciplines.
Insummary,thecorecontributionsofthisworkare:
• Anovel,cost-effectivesystem,SIEVE,forhigh-qualitydatafilteringthatachievesGPT-4olevel
performanceatlessthan1%ofthecost.
• Astream-basedactivelearningalgorithmforimbalanceddatasetsthatefficientlyselectsinformative
snippets,reducingthenumberofqueriestoGPT-4omorethan5xcomparedtorandomsampling.
• Theoreticalanalysisandformalproofsofbalancednessboundsforouractivelearningalgorithmin
scenarioswithimbalanceddatadistributions.
• ExperimentalvalidationofSIEVEontheOpenWebTextdatasetusingfivehighlyspecificfilters,
demonstratingitseffectivenessandversatility.
22 A GENERAL PURPOSE DATA FILTERING SYSTEM
InthissectionweprovideadetaileddescriptionoftheSIEVEsystem. Avisualizationofoursystem
canbefoundinFigure1.
A Bird’s-Eye View. When viewed as a black box, SIEVEprocesses a web-scale dataset of N
text snippets along with a filtering prompt that specifies the criteria for passing or failing each
snippet. Thispromptspecifiesthecriteriaforpassingorfailingeachsnippet,similartohowone
wouldinstructanyhigh-performance,general-purposeLLM,suchasGPT-4o. Fromthisperspective,
SIEVEefficientlycategorizeseachsnippetas‘pass’or‘fail’basedontheprovidedprompt,with
filteringqualitycomparabletothatofGPT-4o.
AstraightforwardapproachwouldbetoapplyGPT-4owiththefilteringprompttoeachtextsnippet.
However, thisbecomesextremelycostlyforweb-scaledatasetscontainingbillionsortrillionsof
tokens. For example, filtering the OpenWebText dataset (Gokaslan & Cohen, 2019) used in this
study,whichcontains9billiontokens,wouldcostapproximately$67,000usingGPT-4odirectly.
ForevenlargerdatasetslikethePILE(Gaoetal.,2020),thiscostwouldincreasebyatleast1000
times,makingitprohibitivelyexpensive. Toovercomethischallenge,weutilizeanactivedistillation
frameworkasdetailedinthefollowingsection.
2.1 ANACTIVEDISTILLATIONFRAMEWORK
In this section, we describe the inner workings of SIEVE, which employs a lightweight binary
classificationmodeltrainedonGPT-4o’sfilteringdecisions. Byleveragingactivelearningtechniques,
we selectively gather GPT-4o decisions on a small, informative subset of text snippets from the
web-scale dataset. This approach significantly reduces costs while maintaining filtering quality
comparabletoGPT-4o.
Activelearningminimizesannotationcostsfromexpensivesourcesbyselectingthemostinformative
subsetofsnippetstoqueryforlabels. Thegoalistotrainahigh-performancemodelf withminimal
annotation cost. In our framework, GPT-4oserves asthe expensive annotation source, while we
fine-tuneapretrainedT5encodermodelforbinaryclassificationonthecollecteddataandannotations
toachievehighperformance. Activelearningstrategiescollectannotationsincrementally,retraining
thelightweightT5modelf afterlabelingeveryB newsnippets. Whilemostdeepactivelearning
literaturefocusesonthepool-basedsetting(Ashetal.,2019;Nuggehallietal.,2023;Fairsteinetal.,
2024;Lesci&Vlachos,2024),thesealgorithmsrequireforwardinferenceoff ontheentiredataset
ateveryiteration,incurringhighcomputationalcostsforlarge-scaledatasets(seeSection5.3for
details). To mitigate this, we apply active learning in the streaming setting, an area well-studied
classicallybutwithlimitedresearchforneuralnetworks. Wespecificallydesignedouralgorithm
totackletheimbalanceinfilteringdecisionsgeneratedbyGPT-4o(seeTable3),aimingtoquery
GPT-4oonamorebalancedandinformativesetoftextsnippets.
Formally, we assume access to a stream of i.i.d. drawn snippets x ,x ,...,x , all following the
1 2 N
sameunderlyingdatadistributionP . WeletS = x ,...,x denotethestream. Inpractice, we
X 1 N
constructthestreambyrandomlyshufflingallsnippetsintheOpenWebTextdataset(Gokaslan&
Cohen,2019). Attimei,theactivelearningalgorithmobservesx ∼ P anddecideswhetherto
i X
queryGPT-4oforitsannotationbasedonthesnippet’sinformativenesstomodelf. Ifqueried,we
obtainthecorrespondingfilteringdecisionfromGPT-4o,whichwedenotebyy (x )∈{0,1}.
GPT i
Here,therandomnesscomesfromthenondeterministicnatureofGPT-4o’sresponse. Afterevery
B newannotations,wefine-tunetheT5-decodermodelfromitspretrainedcheckpointtoobtainan
updatedmodelf. ThedistillationprocessterminatesafteratotalannotationbudgetofT snippets.
Thefinalmodelf fine-tunedonqueriedsnippetsisthenappliedtofiltertheentireweb-scaledataset.
3 ACTIVE LEARNING ALGORITHM FOR DISTILLING GPT-4O
In this section, we present a novel stream-based active learning algorithm for class-imbalanced
scenarios. Weuseamodifiedversionoftheuncertaintysamplingstrategy(Lewis&Gale,1994;Tong
&Koller,2001;Balcanetal.,2006;Settles,2009;Kremeretal.,2014). Inoursetting,uncertainty
sampling labels snippets x that have predictive sigmoid score around 0.5, i.e. f(x ) ≈ 0.5, as
i i
3Figure2: DemonstrationoftheTRMthreshold. Snippets(shownonthebottom)arefirstordered
basedontheirpredictivesigmoidscores. GPT-4oclasslabels0and1arerepresentedbythesolid
ordashedborders. Queriedsnippetsareshaded. Underimbalancedscenarios,sigmoidscoreof0.5
generallywillnotprovideagoodindicationofwheretosample,andwilllikelyresultinlabeling
muchmoresnippetsinthemajorityclass. Theprobabilityµ(s)denotesthelikelihoodofasnippet
withsigmoidscoresbelongingtoclass0. TheTRMthresholdisdefinedtobestseparatethetwo
classesofsnippets.
Algorithm1Stream-BasedClass-BalancingActiveLearning
Input: DataStreamS,labelingfunctiony basedonGPT-4oandspecifiedfilteringprompt,
GPT
batchsizeB,totalbudgetT andconfidencelevelδ.
Initialize: Queryx ,...,x i ∼idP toformtheinitiallabeledsetL={x ,y }B .
1 B X i i i=1
forr =1,..., T do
B
Fine-tunepretrainedT5encodermodelonthelatestlabeledsetLtoformf :S →[0,1].
Initializeconfidencesetµ,µ¯ ←0,1,countert←0.
Letj indextheheadofthestreamS,x .
j
while|L|<(s+1)Bdo //Findoptimalseparationthresholdforf.
ReceivethenextsnippetinthestreamS,x ∼P .
j+t X
iff(x )∈[µ,µ¯]then
j+t
QueryGPT-4oforlabely byy ,andinserttosetL←L∪{x ,y }.
j+t GPT j+t j+t
endif
ift∈2N+ then //Updateconfidenceinterval.
StorepreviouslycomputedsigmoidscoresF ←{0,f(x ),...,f(x )}.
t j j+t
ComputetheempiricalTRMthresholdass
(cid:98)t
←min s∈FtL(cid:98)t(s),where
L(cid:98)t(s):= t+1 1((cid:80) i∈[j,j+t]:f(xi)≤s1{y
i
̸=0}+(cid:80) i∈[j,j+t]:f(xi)>s1{y
i
̸=1}).
Updateµ,µ¯bethesmallestandlargestthresholdss∈F suchthat
t
(cid:112)
L(cid:98)t(s)−L(cid:98)t(s (cid:98)t)≤β
t+1
(|{s′ ∈F
t
:min(s,s (cid:98)t)≤s′ ≤max(s,s (cid:98)t)}|−1)/t+β t2 +1/2
(cid:112)
whereβ = 2log(2log (t+1)2N2/δ)/(t+1))ischosensothattheTRMthreshold
t+1 2
liesintheupdatedconfidencethresholdwithprobabilityofatleast1−δ/log (N).
2
endif
Updatecountert←t+1. //LoopoversnippetsinS.
endwhile
endfor
Return: T5encodermodelf finetunedonL. f isthenusedforfilteringontheentiredataset.
thesearebelievedtobethemostinformativedatatobelabeled. However,ourbinaryclassification
tasksofdatafilteringisnaturallyimbalancedasshowninTable3. Previousactivelearningwork
byZhangetal.(2022a)andNuggehallietal.(2023)observedthatthethresholdof0.5isgenerally
biasedtowardsthemajorityclassunderimbalancedscenarios,whichmeansthatmostofthesnippets
selectedbyuncertaintysamplingwillbeinthemajorityclass. Thistranslatesintopoorperformance
indetectingsnippetsfromthetargetminorityclass. Tocombatthis,ourapproachinsteadaimsto
findathresholdnearwherethemajorityandminorityclassesareequiprobable,whichwewillrefer
toasanTrueRiskMinimizer(TRM)threshold. SelectingandlabelingsnippetsaroundtheTRM
4thresholdyieldsalabeleddatasetthatismorebalancedandincludesuncertainsnippets. Inanutshell,
ouralgorithm(detailedinAlgorithm1)alternatesbetween
1. SpendingBbudgetinfindingandlabelingclosetotheTRMthreshold;
2. Fine-tuninganewT5encodermodelf onalllabeledsnippetsthusfar.
MoreconcretelyasshowninFigure2,whenorderingsnippetsaccordingtotheirsigmoidscores,
labelingsnippetsaroundthethresholdof0.5canoftenresultinselectingandlabelingofmostof
snippetsfromasingle(majority)class. Ourproposedalternative,theTRMthreshold,minimizes
theexpectednumberofclass1snippetstoitsleftandclass0snippetstoitsright. Thus,theTRM
thresholdminimizestheexpectednumberofmisclassifications.Formally,letη denotetheprobability
j
ofsnippetx belongingtoclass0
j
η :=
P(cid:0)
y (x
)=0(cid:1)
j GPT j
where the possible randomness is with respect to GPT-4o’s response as well as the distribution
underlyingthesnippetx ∼ P . ThedistributionP isunknown,so{η }areunknownaswell.
j X X j
Formodelf andastreamofi.i.d. textsnippetsx ,...,x ∼ P ,theTrueRiskMinimizer(TRM)
1 N X
thresholdisthenthesigmoidscores⋆ ∈{0,f(x ),...,f(x )},where
1 N
 
(cid:88) (cid:88)
s⋆ := argmin  (1−η j)+ η j. (1)
s∈{f(x1),...,f(xN)}
j:f(xj)≤s j:f(xj)>s
Theoptimizationobjectiveabovecanbeviewedastheexpectedrisk(probabilityoferror)ofafinite
setofthresholdclassifierswiththresholdlocationsat{0,f(x ),...,f(x )}. Athresholdclassifier
1 N
atscategorizessnippetsintoclass0iftheirsigmoidscoreislessthanorequaltos,andintoclass1
iftheirscoreisgreaterthans. TheTRMthresholdcanalsobeviewedaswherethetwoclassesbest
separatefromeachother. Snippetsaroundthisthresholdarethereforetrulyuncertaintotheneural
networkmodel. Moreover,inSection4,wewillprovideanoveltheoreticalanalysisdemonstrating
labelingaroundtheTRMthresholdalsoimprovesthebalancednessofthequeriedsnippets,alleviating
thedataimbalanceissueswhentrainingthelightweightmodelf. Ofcourse,theTRMthresholdis
unknown(becausetheprobabilities{η }areunknown),soweemployanefficientactivelearning
j
proceduretoidentifyathresholdclosetoit.
Ouralgorithm,showninAlgorithm1,appliesagnosticactivelearningtechniques(Dasguptaetal.,
2007; Jamieson & Jain, 2022; Katz-Samuels et al., 2021) to threshold classifiers. It focuses on
identifyingtheTRMthreshold. ThealgorithminitializesbyqueryingthefirstBsnippetsfromthe
streamtocreatealabeledsetL. Eachiterationbeginswithfine-tuningclassifierf onL. Toestimate
theTRMthresholds⋆,wemaintainahigh-confidenceinterval[µ,µ¯]whileannotatingstreamsnippets.
SnippetswithpredictivesigmoidscoreswithinthisintervalarequeriedusingGPT-4o. Weupdatethe
confidenceintervalonageometricschedule,every2isnippets,usingempiricalestimatess ofthe
(cid:98)t
optimalthreshold. Thisinterval,centeredaroundtheempiricalestimate,ensuress⋆ ∈ [µ,µ¯]with
atleast1−δprobabilityacrossallt=2iforthecurrentiteration. Forconstructingtheconfidence
interval,weemployanempiricalBernsteinbound,asintroducedinJamieson&Jain(2022). This
approachdiffersfromtheoriginalCALalgorithm(Dasguptaetal.,2007), whichusesauniform
convergenceboundrequiringsynchronousquerying. OurmethodallowsforparallelGPT-4oqueries,
significantlyreducingcomputationaltimeinpractice.
4 THEORETICAL ANALYSIS
In this section, we first provide formal guarantee of the performance of our algorithm. We then
proceedtoanalyzethebalancednessofthesnippetsouralgorithmqueries,showingitsimprovement
incollectingamorebalancedsetofsnippets. Notethatallofouranalysisareconductedforany
singleiterationsinAlgorithm1. Definethetrueriskfunctionoverallsnippets1,...,N atthreshold
sisdenotedby
 
1 (cid:88) (cid:88)
R(s) =
N
 (1−η i)+ η i .
i≤N:f(xi)≤s i≤N:f(xi)>s
Recall the goal is to find a threshold near s∗ = argmin R(s). This learning problem over the
s
discretesetofthresholdclassifiersatthresholdsf(x ),...,f(x )canbeexactlyrepresentedinthe
1 N
frameworkofJamieson&Jain(2022),leadingtoAlgorithm1andthefollowingbound.
5Theorem4.1(Jamieson&Jain(2022)). DuringiterationrofAlgorithm1,giventheclassifiermodel
f,withprobabilityatleast1−δ,bothR(µ)−R(s⋆)andR(µ¯)−R(s⋆)areupperboundedby
(cid:112)
c R(s⋆)+c β R(s⋆)+c β2. (2)
0 1 t 2 t
for all the confidences intervals [µ,µ¯] updated at time t ∈ 2N+. Here, c ,c and c are some
0 1 2
universalconstants.
Thetheoremsuggeststhatthegapinthetrueriskoftheconfidenceintervalsshrinksroughlyonthe
scaleof √1
t
overtimesinceβ
t
=O(cid:101)(√1 t). Forlarget,thisgapgoesto0.
Definition 4.2 (Score Re-Ordering). Let π denote the permutation of {1,...,N} such that
f(x )≤···≤f(x ).
π(1) π(N)
Definition4.3(DiscreteSmoothness). LetL=max |η −η |denotethemaximum
j∈[N−1] π(j) π(j+1)
changeinprobabilitiesη . ThismirrorstheLipschitzsmoothnessinadiscretefashion. Notewe
(j)
alwayshaveL≤1.
Withoutlossofgenerality,assumeclass0tobetheminorityclass. Theexpectedimbalanceratiois
then (cid:80)N j=1ηj <1. Whens⋆ =0,itmeanseventhelowestsigmoidscoresnippetsaremorelikely
(cid:80)N j=11−ηj
tobeinthemajorityclass. Inthiscase,areasonablestrategyissimplyquerythelowestsigmoid
scoresnippets,whichisexactlywhatouralgorithmdoes.
Theorem4.4(BalancednessofLabeledSnippets). Assumeclass0istheminorityclassands⋆ ̸=0.
Consideranintervalofscores[µ,µ¯]withs⋆ ∈ [µ,µ¯]. Letthecorrespondinggapsinriskdenoted
byγ :=R(µ)−R(s⋆)>0andγ :=R(µ¯)−R(s⋆)>0. Whenlabelingsnippetsindexedwithin
0 1
thisintervaluniformlyatrandom,theimbalanceratioλ(µ,µ¯)betweentheminorityclassandthe
majorityclassmustsatisfy
(cid:80)
λ(µ,µ¯)=
j:f(xj)∈[µ,µ¯]η j ≥1−min(Nγ¯+LN ,√
L·
Nγ¯+LN +1
)
(cid:80) (cid:112)
1−η 1.5−2L (1−L) Nγ
j:f(xj)∈[µ,µ¯] j
whereγ :=min(γ ,γ )andγ¯ :=max(γ ,γ ),withL,γ,γ¯ <1. Thisimplies,
0 1 0 1
(a)ifγ¯ →0andL→,λ(µ,µ¯)≥1− Nγ¯+LN →1(perfectbalance);
1.5−2L
√
(b)ifγ ≥ c forsomeconstantsc>0,thenλ(µ,µ¯)≥1− L· Nγ¯+LN+1. WhenL→0,weagain
N c(1−L)
recoversλ(µ,µ¯)→1(perfectbalance).
(cid:80) (cid:80)
Proof sketch. Let A = η and B = 1 − 2η , we can rewrite the
j:f(xj)∈[µ,µ¯] j j:f(xj)∈[µ,µ¯] j
imbalance ratio into λ(µ,µ¯) = A . Since N(γ −γ ) = (cid:80) 1−2η ≥ B −LN,
A+B 1 0 j:f(xj)∈(µ,µ¯] j
wecanlowerboundthebalancednessby A . WethereforeneedtoprovethatAis
A+N(γ1−γ0)+LN
sufficientlylargeascomparedtoN(γ −γ )+LN.
1 0
Toprovethis,wenotethattheηvaluesaroundtheoptimalseparationthresholds⋆ arecloseto.5.
Therefore,bythesmoothnesscondition,whenLissmall,η ≈.5forallf(x )∈[µ,µ¯],soAroughly
j j
(cid:113)
scaleslinearlyinthenumberofelementswithin[µ,µ¯]. WecanprovethatthereareatleastO( Nγ¯)
L
elementsinthisconfidenceinterval. Athereforefollowsasimilarscale,whichismuchgreaterthan
N(γ −γ )+LN whenL→0. Undersuchcase,werecoverabalancednessboundof1,i.e. the
1 0
annotatedsnippetsareperfectlybalanced. OurdetailedproofisprovidedinAppendixB.
5 EXPERIMENTS Filter ImbalanceRatioλ
Politics 0.153
Inthissection,wepresentourexperimentsconductedfor Climate 0.043
the five highly customized data filters in Table 1: poli- AI 0.026
tics, climate, AI, mainstream knowledge and text qual- Mainstream 0.208
ityfilters. ThesefiltersareappliedtotheOpenWebText Quality 0.457
dataset(Gokaslan&Cohen,2019),whichisdividedinto
Figure3: Imbalanceratioofminorityvs
majoritydecisions,calculatedbasedon
6 5000randomlysampledsnippets. λ =
#Minority/#Majority.Bal.Accuracy HumanPreference #Queriesto Lightweight
Filter Method (GPT-4oasGT) OverGPT-4o GPT-4o ModelCost TotalCost
GPT-4o 95.6%∗ — 13.5M $0 $67,000
Politics
SIEVE(Ours) 95.6% — 60K $270 $570
GPT-4o 96.6%∗ — 13.5M $0 $67,000
Climate
SIEVE(Ours) 96.7% — 7.5K $180 $220
GPT-4o 95.5%∗ — 13.5M $0 $67,000
AI
SIEVE(Ours) 95.7% — 6K $180 $210
GPT-4o 92.4%∗ 50% 13.5M $0 $67,000
Mainstream
SIEVE(Ours) 91.0% 54% 100K $400 $900
GPT-4o 88.2%∗ 50% 13.5M $0 $67,000
Quality
SIEVE(Ours) 86.3% 53% 60K $270 $570
Table1: PerformanceresultsofapplyingSIEVEonfivehighlyspecializedfilters(seeAppendixA).
SIEVEcanmatchorexceedGPT-4o’squalityintermsofbalancedaccuracyandhumanpreference.
Ontheotherhand,SIEVEsavesmorethan99%ofthecostcomparedtousingGPT-4o. SeeSection5
forexperimentdetails. ∗WeassessGPT-4o’saccuracybymeasuringoutputconsistencybetween
identical API calls using greedy decoding (temperature set to 0). Some inconsistency persists,
possiblyduetohardwarenon-determinism. ThismaybeamplifiedwhenusingourCoTprompts.
around13.5Msnippetsof1024tokens. Thefirstthreefil-
tersidentifytextsnippetsrelatedtoaparticulartopic,with
highlydetailedspecificationsoffilteringpromptstoGPT-
4oaboutthesubdomainoftopicsthatshouldbeincluded.
Themainstreamknowledgepromptaimstoexcludeanyobscureandnichecontentdeterminedby
GPT-4o. Lastly,thequalityfilteraimstoidentifytextsnippetsthatareconsideredhighqualityby
GPT-4o. DetailedpromptscanbefoundinAppendixA.
Throughoutourexperiments,weusetheencoderpartofpretrainedT5-large(Raffeletal.,2020)as
thelightweightmodel. Alinearlayerisattachedtotheencodermodelforbinaryclassification. The
classificationmodelhaslessthan770Mparameters,ordersofmagnitudesmallerthanGPT-4o. To
mitigatetheimbalancednatureofthesnippetsshowninTable3,weutilizethefocalloss(Lin,2017).
WereferthereaderstoAppendixCformoretrainingdetails.
5.1 RESULTS: GPT-4O-BASEDANDHUMAN-BASEDEVALUATION
GPT-4o-Based Evaluation. Table 1 demonstrates that SIEVEachieves comparable balanced
accuracytoGPT-4oonpolitics,climate,andAIfilters,whilecloselymatchingitsperformanceon
mainstreamknowledgeandtextqualityfilters. Weevaluatedthesefiltersusingasetoftestsnippets
randomlysampledfromOpenWebText. GroundtruthlabelswereestablishedthroughindividualGPT-
4oAPIcallsforeachsnippet.ToassessGPT-4o’sperformanceagainstthisgroundtruth,weconducted
additionalAPIcallsusingidenticalpromptsforeachtestsnippet,effectivelymeasuringthemodel’s
self-consistencyrate. Surprisingly,weobservedinherentnoiseinGPT-4o’sdecisions,evenwhen
usinggreedydecoding(temperaturesetto0). Thisvariabilitylikelystemsfromnon-deterministic
factorsinthehardwareinfrastructure. Whileweemployedchain-of-thought(CoT)reasoninginour
filteringprompts(detailedinAppendixA)toenhancedecisionquality,thecompoundingnoisefrom
eachgeneratedtokenappearstohavecontributedtothenoticeableinconsistencies. Wealsonote
thatCoTplaysanincreasingroleininference-timescalingasdemonstratedbyOpenAI’so1model,
whichmayinevitablycauseinconsistenciesinthemodels’decisions.
HumanEvaluation. Intheabove,wecomparedourdistilledlightweightmodel’saccuracytoGPT-
4oforbothmainstreamandqualityfilters.WhenevaluatedbyGPT-4oitself,ourmodel’sperformance
appearedlower,raisingthequestion: Isourlightweightmodelactuallyworse,orisGPT-4obiased
whenjudgingitsowndecisions? Toinvestigate,weconductedahumanevaluation. Foreachfilter,
werandomlyselected100textsnippetswhereGPT-4oandourlightweightmodeldisagreed. We
thenrecruitedtwogroupsof13annotatorsperfiltertomanuallyassessthesechallengingcases. The
groundtruthforeachsnippetwasdeterminedbythemajorityvoteoftheannotators,allowingus
tocomparebothmodels’performanceagainsthumanjudgment. Asshowninthefourthcolumnof
7>3x Saving
>5x Saving
(a)Politicsfilteraccuracy (b)Climatefilteraccuracy (c)Politicsfilterimbalanceratioof
queriedsnippets
Figure4: ActivevsRandomDistillation: Performanceofthedistilledlightweightmodelacross
differentnumberofqueriesmadetoGPT-4o. Withactivelearning,wecansavethenumberofqueries
toGPT-4obymorethan5xforthepoliticsfilterandmorethan3xfortheclimatefilter.
Bal.Accuracy #Queriesto GPT-4o Lightweight Lightweight Total
Filter Method (GPT-4oasGT) GPT-4o Cost ModelTraining ModelInference Cost
GPT-4o 95.6%∗ 13.5M $67,000 $0 $0 $67,000
SIEVE(Ours) 95.6% 60K $300 $120 $150 $570
Politics
SIEVE(Ours) 95% 25K $125 $30 $150 $305
Random 95% 100K $500 $20 $150 $670
GPT-4o 96.6%∗ 13.5M $67,000 $0 $0 $67,000
Climate SIEVE(Ours) 96.7% 7.5K $40 $30 $150 $220
Random 96.6% 25K $125 $20 $150 $315
Table2: Costbreakdownofpoliticsandclimatefilters. TotalcostisconsistedofGPT-4oquerying
cost,lightweightmodeltrainingcostandT5inferencecost.
Table1,weseeevenaslightedgeofourlightweightmodeloverGPT-4owhenjudgedbyhuman.
ThissuggestsourlightweightmodelisatleastcomparabletothedecisionsmadebyGPT-4o,while
incurringmuchlesscomputationalcostwhenfilteringweb-scaledatasets.
5.2 ACTIVEVSRANDOMDISTILLATION
Inthissection,wecomparetheeffectivenessofactivelearningversusrandomqueryingfordistilling
ourlightweightmodel. TherandomdistillationstrategyinvolvesqueryingGPT-4onapredetermined
number of randomly selected snippets from the OpenWebText dataset, followed by fine-tuning
the T5 encoder model on this queried data. As illustrated in Figure 4, our comparison of the
lightweightmodel’sperformancewhentrainedwithactiveversusrandomsamplingforbothpolitics
andclimatefiltersrevealssignificantadvantagesforactivedistillation. Specifically,activedistillation
demonstratesremarkableefficiency,requiringover5timesfewerqueriesforthepoliticsfilterand
more than 3 times fewer for the climate filter compared to random distillation. Figure 4c also
demonstratesouractivelearningalgorithm’seffectivenessinqueryingamuchmorebalancedsetof
snippets. It’snoteworthythatevenwith100,000queries,randomdistillationfailstomatchGPT-4o’s
performance.Duetobudgetaryconstraints,wedidnotextendrandomdistillationexperimentsbeyond
thisquerycount. Importantly,weobservethatasaccuracyincreases,activelearningtypicallyyields
greaterquerybudgetsavingscomparedtorandomsampling(Citovskyetal.,2021;Ashetal.,2021;
Zhangetal.,2024a),suggestingthatforthepoliticsfilter,inparticular,theefficiencygainsfrom
activedistillationcouldpotentiallyexceedtheobserved5-foldreductioninquerycosts. Asinference
costofstate-of-artLLMsincreases,activedistillationcouldplayanincreasinglyimportantrolein
SIEVE.
5.3 COMPUTATIONALCOSTCOMPARISONS
CostBreakdownandComparison. Table2presentsacomprehensivebreakdownofSIEVE’s
computationalcosts,consistingofGPT-4oquerycosts,lightweightmodeltrainingcosts,andinfer-
encecostsforfilteringtheentireOpenWebTextdataset. ThesumofGPT-4oqueryandtrainingcosts
representsthetotalmodeldistillationexpenseforSIEVE.Ourexperimentsindicatethatquerying
8GPT-4ofor1000snippetscostsapproximately$5. Thelightweightmodelinferencecost,aconserva-
tiveestimateforprocessing13.5Msnippetsusinga770Mparametermodel,isexpectedtobelower
inpracticebyparallelizationacrossmultipleCPUsorcheapinferenceGPUs. Thelightweightmodel
trainingcostincludescostsformodelfine-tuningateachiterationofAlgorithm1,inferencecosts
forcomputingsigmoidscoresofdatainstream, andnegligibleuncertaintyupdateexpenses. We
calculatedthesecostsbasedonthehourlyratefor8xA10080GBGPUs,multipliedbytheactual
time spent on each experiment. Together, these components provide a detailed overview of the
computationalresourcesrequiredforSIEVE’simplementationandapplicationtotheentiredataset.
AsshowninTable2,activedistillationoffersamorecost-effectiveapproachcomparedtorandom
distillationinachievingequivalentmodelperformance. Thiscostadvantagebecomesevenmore
pronounced when considering more expensive teacher models, such as the recently released o1,
whichsignificantlyincreasesquerycostsandpotentiallydominatesthetotalexpenses. Giventhat
activedistillationsubstantiallyreducesthenumberofrequiredqueries,thecostdisparitybetween
activeandrandomdistillationmethodsisexpectedtowidenfurther,especiallywhenutilizingmore
advancedandcostlyteachermodels.
Stream-BasedvsPool-BasedActiveDistillation. Thechoiceofastream-basedactivelearning
approachforSIEVEisjustifiedbyitssignificantcostadvantagesoverpool-basedmethods. While
stream-basedalgorithmsprocesssnippetsonlyonce,pool-basedactivelearningrequiresrepeated
forwardinferenceontheentiredatasetof13.5Msnippetsforeachbatchofqueriedsnippets. This
differencetranslatestosubstantialadditionalcosts: approximately$1800forthepoliticsfilterand
$750 for the climate filter. These extra expenses would dramatically increase the overall cost of
activedistillation. Ourdecisiontodevelopastream-basedactivelearningalgorithmforSIEVE,
particularlyeffectiveinimbalancedscenarios,isthusstronglysupportedbythesecostconsiderations,
ensuringamoreeconomicallyviablesolutionforoursystem.
6 RELATED WORK
DataFilteringforLargeLanguageModels. Datacurationisfundamentaltothedevelopment
ofLLMs(Longpreetal.,2023;Zhouetal.,2023). Asinterestindomain-specificLLMsgrows,the
needforextensive,relevantdatacollectionbecomesincreasinglyimportant. Foracomprehensive
overviewofexistingdatasets,wedirectreaderstoRaffeletal.(2020);Gaoetal.(2020);Liuetal.
(2024). Currentmethodsforacquiringdomain-specificdatapredominantlyrelyonafewestablished
large-scaledatabases,includingtextbooks(Gunasekaretal.,2023),coderepositories(Muennighoff
etal.,2023;Gaoetal.,2020),medicalliterature,andotherspecializedsources(Gaoetal.,2020;
Chengetal.,2023). However,forrapidlyevolvingtopicslikethe2024presidentialelection,climate
changeandartificialintelligence,relevantinformationisoftendispersedacrosstheinternet,making
comprehensivedatacollectionchallenging. Ourapproachinvolvestraininglightweight,task-specific
datafilteringmodelsdistilledfromGPT-4. Thesemodelsarethenappliedtoweb-scaledatasetsto
identifypertinentinformationacrossvariousdomains. Existingdatafilteringtechniquesspanawide
range,frombasicrule-basedmethodsutilizingsentence-levelstatisticalfeatures(Raeetal.,2021;
Yang,2019;Laurençonetal.,2022;Zhangetal.,2022b)tosophisticatedfiltersleveragingpretrained
neuralnetworksfortextquality (Brown,2020;Duetal.,2022;Chowdheryetal.,2023;Touvron
etal.,2023;Enomotoetal.,2024;Qianetal.,2024)andtoxicity(Leesetal.,2022;Friedl,2023)
filtering. Toourknowledge,thisworkrepresentsthefirstattempttodevelopdomain-specificdata
filteringmodelsadaptabletoadiversearrayoffilteringrequirementsandspecializeddomains.
ActiveLearning Activelearningisastrategyaimedatreducingdataannotationcostsbyselectively
choosingwhichexamplestolabel. Traditionalapproachesiterativelyupdatemachinelearningmodels
basedonnewlylabeleddata,usingvariousinformativenessmetricstoguidetheselectionprocess.
Thesemetricstypicallyincludeuncertainty(Lewis&Gale,1994;Tong&Koller,2001;Settles,2009;
Balcanetal.,2006;Kremeretal.,2014;Galetal.,2017;Ducoffe&Precioso,2018;Beluchetal.,
2018),diversity(Sener&Savarese,2017;Geifman&El-Yaniv,2017;Citovskyetal.,2021),and
expectedmodelchange(Ashetal.,2019;2021;Wangetal.,2021;Elenteretal.,2022;Mohamadi
etal.,2022). However,mostexistingmethodsaredesignedforpool-basedsettingswithbalanced
datadistributions,whichmaynotbesuitableforallreal-worldscenarios.
9Incontrast,ourworkfocusesonstream-basedalgorithmscapableofhandlingdataimbalance,an
areathathasreceivedlimitedattentionindeeplearningcontexts. Whilepool-basedalgorithmshave
shownsuccessinaddressingclassimbalance(Aggarwaletal.,2020;Kothawadeetal.,2021;Emam
etal.,2021;Zhangetal.,2022a;Colemanetal.,2022;Jinetal.,2022;Cai,2022;Nuggehallietal.,
2023;Zhangetal.,2024b;Lesci&Vlachos,2024;Fairsteinetal.,2024),stream-basedapproaches
fordeepneuralnetworksremainunderstudied. TherecentworkbySaranetal.(2023)introducesan
onlinevolumesamplingtechniqueforannotatingdiversesetsofexamplesintherepresentationspace.
Representationdiversity,however,hasbeenshowntostruggleonclass-imbalanceddatadistribution
underthepoolbasedsetting(Zhangetal.,2022a;Nuggehallietal.,2023;Lesci&Vlachos,2024;
Fairsteinetal.,2024). Specificallythesestudiessuggestthatsamplingdiverselyintherepresentation
spacedoesnotnecessarilyimprovetheclass-balancednessoftheannotatedexamples. Toaddressthis
gap,weproposewhatwebelievetobethefirststream-basedalgorithmspecificallydesignedforclass
imbalancescenariosinactivelearning.
As part of our algorithm, we use an agnostic active learning algorithm for identifying the TRM
threshold. AgnosticactivelearninghasbeenwidelystudiedintheclassicalPAClearningsetups,
wherethelabelsofanyparticularexampleisinherentlynoisy. Ourprocedureisadirectapplication
ofJamieson&Jain(2022),whichwasinspiredbyDasguptaetal.(2007). Thealgorithmisprovento
benearminimaxoptimalintheseliterature. Inaddition,ouralgorithmcanalsobeseenasaninstance
ofthealgorithmproposedbyKatz-Samuelsetal.(2021)forthresholdclassifiers,wheretheyalso
provesuchalgorithmisnearinstance-optimal. Inthispaper,wealsoprovethefirstboundtowards
balancedness of labeled examples in agnostic active learning, focusing on the class of threshold
classifiers.
KnowledgeDistillation Knowledgedistillationisatechniquewhereasmaller“student"model
learnstoemulatealarger,moresophisticated“teacher"model. Withtheincreasingcapabilitiesof
largelanguagemodels(LLMs)acrossdiversetasks,recentresearchhasexploredusingLLMsas
annotatorstotraindomain-specificstudentmodels. Foracomprehensivereview,seeTanetal.(2024).
WhilemostresearchinLLMknowledgedistillationfocusesonknowledgeextractionmethods,few
studiesaddressthehighcomputationalcostofusingLLMsforlarge-scaleannotation. Recentwork
byZhangetal.(2023)andRouzegar&Makrehchi(2024)hasbeguntotacklethisissuebyusing
activelearning. Ourpaperappliesknowledgedistillationtothespecificproblemofdatafiltering. We
employastraightforwardapproach,usingLLMannotationsasbinaryclassificationlabelstoactively
fine-tuneanencoder-basedmodel. FutureresearchcouldexploreusingGPT-4’schain-of-thought
outputstodistilladecoder-basedstudentmodelwithinthemulti-tasklearningframeworkproposed
byHsiehetal.(2023). It’sworthnotingthatclassicknowledgedistillationwork,suchasHinton
(2015),trainsstudentclassifierstomatchtheteachermodels’outputlogits. However,inourcase,
usingchain-of-thoughtfilteringpromptsmakesitimpracticaltoobtainprobabilitiesforthebinary
decision.
7 CONCLUSION, LIMITATIONS AND FUTURE WORK
In this paper, we introduced SIEVE, demonstrating the feasibility of achieving GPT-4o quality
datafilteringacrossadiverserangeofuser-specifiedfilteringprompts. Ourcomprehensivestudy
showcases the effectiveness of SIEVEin curating large-scale, high-quality datasets for language
modeltrainingatafractionofthecostofexistingtechniques. Theexperimentalresults,validated
ontheOpenWebTextusingfivehighlycustomizedfiltertasks,providestrongevidenceofSIEVE’s
capabilitytomatchGPT-4o’saccuracywhilesignificantlyreducingcomputationalexpenses.
Whileourinitialstudypresentspromisingresults,weacknowledgethatthereareseveralavenuesfor
futureenhancementandexploration. Forfuturework,weareparticularexcitedinscalingSIEVEto
evenlargerdatasetslikethePILE,andmoredatamodalitiesbeyondtext. Themodularnatureof
SIEVEalsoallowsfortheintegrationofmoreadvancedactivelearningalgorithms. Additionally,
SIEVEservesasanexcellenttestbedforthesealgorithms,offeringimmediatereal-worldimpact.
Futureworkcouldalsoinvestigatetheincorporationofsemi-supervisedlearningtechniquestofurther
reduce annotation costs following the framework proposed by Zhang et al. (2024a). Moreover,
whileourcurrentimplementationfocusesonT5architectures,futureresearchcouldexaminethe
efficacyofSIEVEwithabroaderrangeofpretrainedmodelarchitecturesfortransferlearning. Lastly,
exploringtheuseofmorepowerfulmodelsbeyondGPT-4o,suchaso1,forhandlingcomplexfiltering
10promptscouldextendthecapabilitiesofSIEVEtoevenmorechallengingscenarios. Insuchcases,
theimportanceofactivelearningbecomesevenmorepronounced,astheincreasedqueryingcosts
associatedwiththeseadvancedmodelsnecessitatehighlyefficientsamplingstrategies.
ACKNOWLEDGMENTS
WewouldliketothankSiddharthSureshforhishelpwithhostingthehumanstudy. Wealsothank
LalitJainandAndrewWagenmakerforprovidinginsightfulfeedbackonourmanuscript.
11REFERENCES
UmangAggarwal,AdrianPopescu,andCélineHudelot. Activelearningforimbalanceddatasets.
In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pp.
1428–1437,2020.
JordanAsh,SurbhiGoel,AkshayKrishnamurthy,andShamKakade. Gonefishing: Neuralactive
learningwithfisherembeddings. AdvancesinNeuralInformationProcessingSystems,34:8927–
8939,2021.
JordanTAsh,ChichengZhang,AkshayKrishnamurthy,JohnLangford,andAlekhAgarwal. Deep
batchactivelearningbydiverse,uncertaingradientlowerbounds.arXivpreprintarXiv:1906.03671,
2019.
Maria-FlorinaBalcan,AlinaBeygelzimer,andJohnLangford. Agnosticactivelearning. InProceed-
ingsofthe23rdinternationalconferenceonMachinelearning,pp.65–72,2006.
WilliamHBeluch,TimGenewein,AndreasNürnberger,andJanMKöhler. Thepowerofensembles
foractivelearninginimageclassification. InProceedingsoftheIEEEconferenceoncomputer
visionandpatternrecognition,pp.9368–9377,2018.
TomBBrown. Languagemodelsarefew-shotlearners. arXivpreprintarXiv:2005.14165,2020.
XinmengCai. Activelearningforimbalanceddata: Thedifficultyandproportionsofclassmatter.
WirelessCommunicationsandMobileComputing,2022,2022.
Daixuan Cheng, Shaohan Huang, and Furu Wei. Adapting large language models via reading
comprehension. InTheTwelfthInternationalConferenceonLearningRepresentations,2023.
Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam
Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm:
Scalinglanguagemodelingwithpathways.JournalofMachineLearningResearch,24(240):1–113,
2023.
Gui Citovsky, Giulia DeSalvo, Claudio Gentile, Lazaros Karydas, Anand Rajagopalan, Afshin
Rostamizadeh,andSanjivKumar. Batchactivelearningatscale. AdvancesinNeuralInformation
ProcessingSystems,34:11933–11944,2021.
CodyColeman,EdwardChou,JulianKatz-Samuels,SeanCulatana,PeterBailis,AlexanderCBerg,
RobertNowak,RoshanSumbaly,MateiZaharia,andIZekiYalniz. Similaritysearchforefficient
activelearningandsearchofrareconcepts. InProceedingsoftheAAAIConferenceonArtificial
Intelligence,volume36,pp.6402–6410,2022.
SanjoyDasgupta,DanielJHsu,andClaireMonteleoni. Ageneralagnosticactivelearningalgorithm.
Advancesinneuralinformationprocessingsystems,20,2007.
NanDu,YanpingHuang,AndrewMDai,SimonTong,DmitryLepikhin,YuanzhongXu,Maxim
Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language
modelswithmixture-of-experts.InInternationalConferenceonMachineLearning,pp.5547–5569.
PMLR,2022.
MelanieDucoffeandFredericPrecioso. Adversarialactivelearningfordeepnetworks: amargin
basedapproach. arXivpreprintarXiv:1802.09841,2018.
JuanElenter,NavidNaderiAlizadeh,andAlejandroRibeiro. Alagrangiandualityapproachtoactive
learning. arXivpreprintarXiv:2202.04108,2022.
Zeyad Ali Sami Emam, Hong-Min Chu, Ping-Yeh Chiang, Wojciech Czaja, Richard Leapman,
Micah Goldblum, and Tom Goldstein. Active learning at the imagenet scale. arXiv preprint
arXiv:2111.12880,2021.
12RintaroEnomoto,ArsenyTolmachev,TakuroNiitsuma,ShuheiKurita,andDaisukeKawahara. Inves-
tigatingwebcorpusfilteringmethodsforlanguagemodeldevelopmentinjapanese. InProceedings
of the 2024 Conference of the North American Chapter of the Association for Computational
Linguistics:HumanLanguageTechnologies(Volume4:StudentResearchWorkshop),pp.154–160,
2024.
YaronFairstein,OrenKalinsky,ZoharKarnin,GuyKushilevitz,AlexanderLibov,andSofiaTolmach.
Classbalancingforefficientactivelearninginimbalanceddatasets. InProceedingsofThe18th
LinguisticAnnotationWorkshop(LAW-XVIII),pp.77–86,2024.
Paul Friedl. Dis/similarities in the design and development of legal and algorithmic normative
systems: thecaseofperspectiveapi. Law,InnovationandTechnology,15(1):25–59,2023.
YarinGal,RiashatIslam,andZoubinGhahramani. Deepbayesianactivelearningwithimagedata.
InInternationalConferenceonMachineLearning,pp.1183–1192.PMLR,2017.
LeoGao,StellaBiderman,SidBlack,LaurenceGolding,TravisHoppe,CharlesFoster,JasonPhang,
HoraceHe,AnishThite,NoaNabeshima,etal. Thepile: An800gbdatasetofdiversetextfor
languagemodeling. arXivpreprintarXiv:2101.00027,2020.
Yonatan Geifman and Ran El-Yaniv. Deep active learning over the long tail. arXiv preprint
arXiv:1711.00941,2017.
AaronGokaslanandVanyaCohen. Openwebtextcorpus. http://Skylion007.github.io/
OpenWebTextCorpus,2019.
SuriyaGunasekar,YiZhang,JyotiAneja,CaioCésarTeodoroMendes,AllieDelGiorno,Sivakanth
Gopi,MojanJavaheripi,PieroKauffmann,GustavodeRosa,OlliSaarikivi,etal. Textbooksareall
youneed. arXivpreprintarXiv:2306.11644,2023.
AmanGupta, AnupShirgaonkar,AngelsdeLuisBalaguer, BrunoSilva,DanielHolstein, Dawei
Li, Jennifer Marsman, Leonardo O Nunes, Mahsa Rouzbahman, Morris Sharp, et al. Rag vs
fine-tuning: Pipelines,tradeoffs,andacasestudyonagriculture. arXivpreprintarXiv:2401.08406,
2024.
GeoffreyHinton. Distillingtheknowledgeinaneuralnetwork. arXivpreprintarXiv:1503.02531,
2015.
Cheng-YuHsieh,Chun-LiangLi,Chih-KuanYeh,HootanNakhost,YasuhisaFujii,AlexanderRatner,
RanjayKrishna,Chen-YuLee,andTomasPfister. Distillingstep-by-step! outperforminglarger
languagemodelswithlesstrainingdataandsmallermodelsizes. arXivpreprintarXiv:2305.02301,
2023.
KevinJamiesonandLalitJain. Interactivemachinelearning. 2022.
QiuyeJin,MingzhiYuan,HaoranWang,ManningWang,andZhijianSong. Deepactivelearning
modelsforimbalancedimageclassification. Knowledge-BasedSystems,257:109817,2022.
JulianKatz-Samuels,JifanZhang,LalitJain,andKevinJamieson. Improvedalgorithmsforagnostic
pool-basedactiveclassification. InInternationalConferenceonMachineLearning,pp.5334–5344.
PMLR,2021.
SurajKothawade,NathanBeck,KrishnatejaKillamsetty,andRishabhIyer. Similar: Submodular
informationmeasuresbasedactivelearninginrealisticscenarios. AdvancesinNeuralInformation
ProcessingSystems,34:18685–18697,2021.
Jan Kremer, Kim Steenstrup Pedersen, and Christian Igel. Active learning with support vector
machines. WileyInterdisciplinaryReviews: DataMiningandKnowledgeDiscovery,4(4):313–326,
2014.
HugoLaurençon,LucileSaulnier,ThomasWang,ChristopherAkiki,AlbertVillanovadelMoral,
TevenLeScao,LeandroVonWerra,ChenghaoMou,EduardoGonzálezPonferrada,HuuNguyen,
etal. Thebigsciencerootscorpus: A1.6tbcompositemultilingualdataset. AdvancesinNeural
InformationProcessingSystems,35:31809–31826,2022.
13PeterLee,SebastienBubeck,andJosephPetro. Benefits,limits,andrisksofgpt-4asanaichatbot
formedicine. NewEnglandJournalofMedicine,388(13):1233–1239,2023.
AlyssaLees,VinhQTran,YiTay,JeffreySorensen,JaiGupta,DonaldMetzler,andLucyVasserman.
A new generation of perspective api: Efficient multilingual character-level transformers. In
Proceedingsofthe28thACMSIGKDDconferenceonknowledgediscoveryanddatamining,pp.
3197–3207,2022.
PietroLesciandAndreasVlachos. Anchoral: Computationallyefficientactivelearningforlargeand
imbalanceddatasets. arXivpreprintarXiv:2404.05623,2024.
DDLewisandWAGale.Asequentialalgorithmfortrainingtextclassifiers.InSIGIR’94:Proceedings
oftheSeventeenthAnnualInternationalACM-SIGIRConferenceonResearchandDevelopmentin
InformationRetrieval,organisedbyDublinCityUniversity,pp.3–12,1994.
BeibinLi,YiZhang,SébastienBubeck,JeevanPathuri,andIshaiMenache. Smalllanguagemodels
forapplicationinteractions: Acasestudy. arXivpreprintarXiv:2405.20347,2024.
TLin. Focallossfordenseobjectdetection. arXivpreprintarXiv:1708.02002,2017.
Yang Liu, Jiahuan Cao, Chongyu Liu, Kai Ding, and Lianwen Jin. Datasets for large language
models: Acomprehensivesurvey. arXivpreprintarXiv:2402.18041,2024.
ShayneLongpre,GregoryYauney,EmilyReif,KatherineLee,AdamRoberts,BarretZoph,Denny
Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. A pretrainer’s guide to training
data: Measuring the effects of data age, domain coverage, quality, & toxicity. arXiv preprint
arXiv:2305.13169,2023.
Mohamad Amin Mohamadi, Wonho Bae, and Danica J Sutherland. Making look-ahead active
learningstrategiesfeasiblewithneuraltangentkernels. arXivpreprintarXiv:2206.12569,2022.
NiklasMuennighoff,QianLiu,ArmelZebaze,QinkaiZheng,BinyuanHui,TerryYueZhuo,Swayam
Singh,XiangruTang,LeandroVonWerra,andShayneLongpre. Octopack: Instructiontuning
codelargelanguagemodels. arXivpreprintarXiv:2308.07124,2023.
ShyamNuggehalli,JifanZhang,LalitJain,andRobertNowak. Direct: Deepactivelearningunder
imbalanceandlabelnoise. arXivpreprintarXiv:2312.09196,2023.
CrystalQian,EmilyReif,andMinsukKahng. Understandingthedatasetpractitionersbehindlarge
languagemodeldevelopment. arXivpreprintarXiv:2402.16611,2024.
JackWRae,SebastianBorgeaud,TrevorCai,KatieMillican,JordanHoffmann,FrancisSong,John
Aslanides, Sarah Henderson, Roman Ring, Susannah Young, et al. Scaling language models:
Methods,analysis&insightsfromtraininggopher. arXivpreprintarXiv:2112.11446,2021.
ColinRaffel,NoamShazeer,AdamRoberts,KatherineLee,SharanNarang,MichaelMatena,Yanqi
Zhou,WeiLi,andPeterJLiu. Exploringthelimitsoftransferlearningwithaunifiedtext-to-text
transformer. Journalofmachinelearningresearch,21(140):1–67,2020.
HamidrezaRouzegarandMasoudMakrehchi. Enhancingtextclassificationthroughllm-drivenactive
learningandhumanannotation. arXivpreprintarXiv:2406.12114,2024.
AkankshaSaran,SafooraYousefi,AkshayKrishnamurthy,JohnLangford,andJordanTAsh. Stream-
ingactivelearningwithdeepneuralnetworks. InInternationalConferenceonMachineLearning,
pp.30005–30021.PMLR,2023.
Ozan Sener and Silvio Savarese. Active learning for convolutional neural networks: A core-set
approach. arXivpreprintarXiv:1708.00489,2017.
BurrSettles. Activelearningliteraturesurvey. 2009.
ZhenTan,AlimohammadBeigi,SongWang,RuochengGuo,AmritaBhattacharjee,BohanJiang,
Mansooreh Karami, Jundong Li, Lu Cheng, and Huan Liu. Large language models for data
annotation: Asurvey. arXivpreprintarXiv:2402.13446,2024.
14SimonTongandDaphneKoller. Supportvectormachineactivelearningwithapplicationstotext
classification. Journalofmachinelearningresearch,2(Nov):45–66,2001.
HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-AnneLachaux,Timothée
Lacroix, BaptisteRozière, NamanGoyal, EricHambro, FaisalAzhar, etal. Llama: Openand
efficientfoundationlanguagemodels. arXivpreprintarXiv:2302.13971,2023.
HaonanWang,WeiHuang,AndrewMargenot,HanghangTong,andJingruiHe. Deepactivelearning
byleveragingtrainingdynamics. arXivpreprintarXiv:2110.08611,2021.
Zhilin Yang. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv
preprintarXiv:1906.08237,2019.
JifanZhang,JulianKatz-Samuels,andRobertNowak. Galaxy: Graph-basedactivelearningatthe
extreme. arXivpreprintarXiv:2202.01402,2022a.
JifanZhang,YifangChen,GregoryCanal,ArnavMohantyDas,GantavyaBhatt,StephenMussmann,
YinglunZhu,JeffBilmes,SimonShaoleiDu,KevinJamieson,etal. Labelbench:Acomprehensive
frameworkforbenchmarkingadaptivelabel-efficientlearning. JournalofData-centricMachine
LearningResearch,2024a.
JifanZhang,ShuaiShao,SaurabhVerma,andRobertNowak. Algorithmselectionfordeepactive
learningwithimbalanceddatasets. AdvancesinNeuralInformationProcessingSystems,36,2024b.
RuoyuZhang,YanzengLi,YongliangMa,MingZhou,andLeiZou. Llmaaa: Makinglargelanguage
modelsasactiveannotators. arXivpreprintarXiv:2310.19596,2023.
SusanZhang,StephenRoller,NamanGoyal,MikelArtetxe,MoyaChen,ShuohuiChen,Christopher
Dewan,MonaDiab,XianLi,XiVictoriaLin,etal. Opt: Openpre-trainedtransformerlanguage
models. arXivpreprintarXiv:2205.01068,2022b.
TongZhou,YuboChen,PengfeiCao,KangLiu,JunZhao,andShengpingLiu. Oasis: Datacuration
andassessmentsystemforpretrainingoflargelanguagemodels. arXivpreprintarXiv:2311.12537,
2023.
15A FILTERING PROMPTS
A.1 POLITICS
Pleaseanalyzethefollowingtextsnippetanddetermineifitisrelevanttoaspectsofapresidential
election. Thesnippetmaybearbitrarilycutofffromalongerarticle,sopleaseignoreanyoddities
causedbythetruncationandfocusontheoverallrelevancetopresidentialelections.
1. Readthetextsnippetcarefully.
2. Identifyanykeyterms,phrases,orconceptsrelatedtopresidentialelection. Theseincludebyare
notlimitedtoCandidateinformation(biographies,backgrounds,policypositions)
• Economicpoliciesandtheirpotentialimpacts
• Socialissuesandproposedsolutions
• Foreignpolicystancesandinternationalrelations
• Campaignevents,debates,andpublicappearances
• Pollingdata,electoralprojections,andvoterdemographics
• Mediacoverage,endorsements,andfact-checking
• Campaignfinanceandfundraisingefforts
• Partydynamicsandinternalpolitics
• Electoralprocesses,includingvotingsystemsandpotentialreforms
• Controversiesorscandalsinvolvingcandidatesortheircampaigns
• Vicepresidentialcandidatesandpotentialcabinetmembers
• Analysisofkeybattlegroundstatesorregions
• Digitalcampaigningstrategiesandsocialmediapresence
• Grassrootsorganizingandvolunteerefforts
• Externaleventsorcrisesthatmayinfluencetheelection
3. Ignoreanyabruptbeginningorendingofthesnippetandfocusonthemaincontent.
4. Assigneither"PASS"forcontentrelevanttopresidentialelectionand"FAIL"forthosethatare
irrelevant.
Basedonyouranalysis,determineifthesnippetisrelevantorirrelevanttothegeneralknowledgeof
presidentialelections. Thinkstepbystepandprovideyourfinalansweraseither"PASS"or"FAIL"
attheendofyourresponseandnothingelse.
Textsnippet: <InsertTextSnippet>
A.2 CLIMATE
Pleaseanalyzethefollowingtextsnippetanddetermineifitisrelevanttothegeneralknowledgeof
climatechange. Thesnippetmaybearbitrarilycutofffromalongerarticle,sopleaseignoreany
odditiescausedbythetruncationandfocusontheoverallrelevancetoclimatechange.
1. Readthetextsnippetcarefully.
2. Identifyanykeyterms,phrases,orconceptsrelatedtoclimatechange,suchasglobalwarming,
carbonemission,energy,technologyinnovation,agriculture,naturalresources,pollution,landfill,
chemistry,risingsealevels,extremeweatherevents,climatepolicies,environmentaljusticeand
sustainability.
3. Assesswhetherthesnippetdiscussescauses,effects,orsolutionstoclimatechange,orprovides
informationthatcontributestotheunderstandingofclimatechange.
4. Ignoreanyabruptbeginningorendingofthesnippetandfocusonthemaincontent.
5. Assigneither"PASS"forcontentrelevanttoclimatechangeand"FAIL"forthosethatareirrelevant.
Basedonyouranalysis,determineifthesnippetisrelevantorirrelevanttothegeneralknowledgeof
climatechange. Thinkstepbystepandprovideyourfinalansweraseither"PASS"or"FAIL"atthe
endofyourresponseandnothingelse.
Textsnippet: <InsertTextSnippet>
16A.3 AI
PleaseanalyzethefollowingtextsnippetanddetermineifitisrelevanttoaspectsofAI.Thesnippet
maybearbitrarilycutofffromalongerarticle,sopleaseignoreanyodditiescausedbythetruncation
andfocusontheoverallrelevancetoartificialintelligence.
1. Readthetextsnippetcarefully.
2. Findcontentrelatedtoartificialintelligencethatdiscussescomputersystemsorsoftwaredesigned
toperformtaskstypicallyrequiringhumanintelligence,suchasvisualperception,speechrecogni-
tion,decision-making,andlanguagetranslation. Lookforexplanationsoftechnologiesenabling
machinestolearnfromexperience,adjusttonewinputs,andperformhuman-liketaskswithout
explicitprogramming. Includedescriptionsofsystemsthatcaninterpretvisualinformationfrom
the world, as well as content about software capable of processing, analyzing, generating, or
understandinghumanlanguage. Seekinformationonmachinesorprogramsthatcanimprove
theirperformancethroughexperienceordata. IncludediscussionsofAIapplicationsinvarious
fieldssuchashealthcare,finance,transportation,education,orentertainment. Considercontent
addressingethicalconsiderationssurroundingAI,includingbias,privacy,jobdisplacement,or
long-termimplicationsofadvancedAIsystems. LookforhistoricalaccountsofAIdevelopment,
majormilestones,breakthroughs,orsetbacksinthefield. IncludeexplanationsofAIalgorithms,
theirworkings,strengths,limitations,andpotentialapplications. Capturedebatesordiscussions
aboutthefutureofAI,includingtopicslikeartificialgeneralintelligence,superintelligence,or
potentialsocietalimpactsofwidespreadAIadoption. Includereportsoncurrentresearch,new
methodologies,experimentalresults,ortheoreticaladvancementsinAI.Considercontentabout
prominentfigures,organizations,orcompaniessignificantlycontributingtoAIresearchanddevel-
opment. LookfordiscussionsofAIpolicy,regulation,orgovernanceatorganizational,national,
orinternationallevels. IncludeexplanationsoftherelationshipbetweenAIandotherfieldssuchas
robotics,InternetofThings,bigdata,orquantumcomputing. Finally,capturecontentaddressing
challengesinAIdevelopment,suchasdataquality,computationalrequirements,ortheneedfor
explainableAIsystems.
3. Ignoreanyabruptbeginningorendingofthesnippetandfocusonthemaincontent.
4. Assigneither"PASS"forcontentrelevanttoAIand"FAIL"forthosethatareirrelevant.
Based on your analysis, determine if the snippet is relevant or irrelevant to aspects of artificial
intelligence,itsdevelopment,applications,orimplications. Thinkstepbystepandprovideyourfinal
answeraseither"PASS"or"FAIL"attheendofyourresponseandnothingelse.
Textsnippet: <InsertTextSnippet>
A.4 MAINSTREAMKNOWLEDGE
Weaskedforobscureknowledgeinstead,soanysnippetthat"Failed"wouldbeconsidered
mainstreamknowledge.
Pleaseanalyzethefollowingtextsnippetanddetermineifitcontainsobscureornicheknowledgethat
lessthan10000peopleknowandunderstand. Thesnippetmaybearbitrarilycutofffromalonger
article,sopleaseignoreanyodditiescausedbythetruncationandfocusontheoverallrelevanceto
artificialintelligence.
1. Readthetextsnippetcarefully.
2. Filterthedatasetforcontentrelatedtoobscure,specialized,orhighlynicheknowledgethatis
not commonly known or easily accessible to the general public. Include information on rare
historicalevents,obscurescientifictheories,uncommonphilosophicalconcepts,extinctlanguages,
highlyspecializedmathematics,nicheliteraryworks,raremedicalconditions,uncommonspecies,
esotericsubcultures,mysticalpractices,experimentaltechnologies,obscurelaws,raregeological
formations,lesser-knownartmovements,specializedcraftingtechniques,raremusicalinstruments,
uncommon culinary practices, obscure sports, theoretical cosmological concepts, and highly
specializedareasofarchaeologyoranthropology. Excludeanycontentthatiscommonlyknown,
partofstandardeducation, regularlydiscussedinpopularmedia, orwidelyunderstoodbythe
generalpublic. Theidealcontentshouldrequirespecializedknowledge,extensiveresearch,or
accesstouncommonsourcesofinformation,ratherthanbeingsomethinganaveragepersonwould
encounterindailylifeorthroughcasualexposuretomediaandeducation.
173. Ignoreanyabruptbeginningorendingofthesnippetandfocusonthemaincontent.
4. Assigneither"PASS"forobscureandnicheknowledgeand"FAIL"forthosethatarecommon
knowledge.
Thinkstepbystep. Then,youmustprovideyourfinalansweraseither"PASS"or"FAIL"attheend
ofyourresponseandnothingelse.
Textsnippet: <InsertTextSnippet>
A.5 QUALITY
Pleaseanalyzethefollowingtextsnippetanddetermineifitishighqualityorlowqualitytraining
dataforalargelanguagemodel. Thesnippetmaybecutoffabruptlyfromalongerpieceoftext,
butfocusyouranalysisonthequalityfactorspresentintheprovidedtextratherthantheawkward
truncation. Qualityfactorstoconsiderinclude:
1. Evaluate the spelling, grammar, and overall writing quality of the snippet. Note any errors or
inconsistenciesthatcouldnegativelyimpactthemodel’slearning.
2. Assessthefactualaccuracyandreliabilityoftheinformationpresentedinthesnippet. Consider
whetherthecontentappearstrustworthyandwell-researched.
3. Analyzetheclarity,coherence,andlogicalflowofideasinthesnippet. Determineifthetextis
easytounderstandandfollow.
4. Gaugethebreadthanddepthofknowledgeconveyedinthesnippet. Considerwhetherthecontent
providesvaluableinformationorinsightsonthetopicathand.
5. Examine the neutrality and objectivity of the tone and perspective presented in the snippet.
Considerifthetextappearsbiasedorpresentsabalancedviewpoint.
6. Basedontheabovefactors,determineifthesnippetis:
PASS:Highqualitytrainingdata
FAIL:Lowqualitytrainingdata
ThinkstepbystepandanswerwitheitherPASSorFAILasyourfinaldecisionintheendandnothing
else.
Textsnippet: <InsertTextSnippet>
18B ANALYSIS PROOF
(cid:80) (cid:80)
Proof. LetA = η andB = 1−2η ,wecanrewritetheimbalance
j:f(xj)∈[µ,µ¯] j j:f(xj)∈[µ,µ¯] j
ratiointoλ(µ,µ¯)= A . SinceN(γ −γ )=(cid:80) 1−2η ≥B−LN,wecanlower
A+B 1 0 j:f(xj)∈(µ,µ¯] j
boundthebalancednessby A .
A+N(γ1−γ0)+LN
Asthelowerbound A increasesasAincreases,wewouldnowliketoprovealower
(cid:80)
A+N(γ1−γ0)+LN
boundofA= η .
j:f(xj)∈[µ,µ¯] j
Recallπ(1),...,π(N)istheorderingofexamplesbasedonsigmoidscore. Weletπ−1(·)denote
the inverse mapping of π, so that π−1(π(i)) = i. We let r = min({j : f(x ) ∈ [µ,s⋆]}),
π(j)
r¯=max{j :f(x )∈(s⋆,µ¯]}andr∗denotetheindexwheref(x )=s⋆.
π(j) π(r⋆)
Firstnotesinceclass0istheminorityclass,wemusthaver⋆ ̸=N. Sincer⋆ ̸=0andr⋆ ̸=N,we
musthaveη ≥0.5andη ≤0.5. Otherwise,r⋆+1orr⋆−1willhavelowerriskthan
π(r⋆) π(r⋆+1)
R(η ). Bythesmoothnessdefinitionabove,wefurtherhave∀j ≤r⋆,0.5−(r⋆−j)L≤η ≤
π(r⋆) π(j)
0.5+(r⋆−j+1)L,and∀j ≥r⋆,η ≥0.5−(j−r⋆+1)L.
π(j)
(cid:80) (cid:80)
A= η canthenberewrittenintherankedformatasA= η .
j:f(xj)∈[µ,µ¯] j j∈[r,r¯] j
First,wedividethesamplingrangeinto[r,r⋆]and[r⋆+1,r¯]. Whensamplingin[r,r⋆],wehave
R(f(x ))−R(s⋆)=γ >0 =⇒
π(r) 0
(cid:88) (cid:88) (cid:88) (cid:88)
Nγ = η − (1−η )≥−1+ η − (1−η ). (3)
0 (j) (j) (j) (j)
j∈(r,r⋆] j∈(r,r⋆] j∈[r,r⋆] j∈[r,r⋆]
Whensamplingin[r⋆+1,r¯],sinceR(r¯)−R(r⋆)=γ ,wemusthave
1
(cid:88) (cid:88) (cid:88)
Nγ =N ·(R(r¯)−R(r⋆)))= 1−2η = (1−η )− η . (4)
1 (j) (j) (j)
j∈(r⋆,r¯] j∈[r⋆+1,r¯] j∈[r⋆+1,r¯]
(cid:80)
Toobtainthelowerboundof η ,westartbyboundingr¯andr. Specifically,byequation4,
j∈[r,r¯] (j)
wehave
(cid:88) (cid:88)
Nγ = (1−2η )≤ (1−2(0.5−(j−r⋆)L))
1 (j)
j∈[r⋆+1,r¯] j∈[r⋆+1,r¯]
r¯−r⋆
(cid:88) (cid:88)
= 2(j−r⋆)L= jL=(r¯−r⋆)(r¯−r⋆+1)L≤L(r¯−r⋆+1)2.
j∈[r⋆+1,r¯] j=1
(cid:113) (cid:113)
Asaresultr¯≥ Nγ1 +r⋆−1. Letα = Nγ1 −1,wethenhaver¯≥r⋆+α .
L 1 L 1
Similarly,wehave
(cid:88) (cid:88)
Nγ ≤ (2η −1)≤ (2·(0.5+(r⋆−j+1)L)−1)
0 (j)
j∈[r,r⋆] j∈[r,r⋆]
r⋆−r+1
(cid:88) (cid:88)
= 2(r⋆−j+1)L= 2jL≤(r⋆−r+2)2L,
j∈[r,r⋆] j=1
(cid:113) (cid:113)
sor ≤r⋆− Nγ0 +2. Letα := Nγ0 −2,wethenhaver ≤r⋆−α
L 0 L 0
19(cid:80)
Now,wecanbound η bythefollowing
j∈[r⋆+1,r¯] (j)
(cid:88)
r⋆ (cid:88)+α1 r⋆ (cid:88)+α1
η = η ≥ 0.5−(j−r⋆)L
(j) (j)
j∈[r⋆+1,r¯] j=r⋆+1 j=r⋆+1
=r⋆ (cid:88)+α1
0.5−jL=
α
1 −
α 1(α 1+1)L
2 2
j=1
(1−L)α −α2L
= 1 1 .
2
(cid:80)
Similarly,wecanbound η bythefollowing
j∈[r,r⋆] (j)
r⋆ r⋆
(cid:88) (cid:88) (cid:88)
η = η ≥ 0.5−(r⋆−j)L
(j) (j)
j∈[r,r⋆] j=r⋆−α0 j=r⋆−α0
=(cid:88)α0
0.5−jL=
α 0+1
−
α 0(α 0+1)L
2 2
j=0
(1−L)α −α2L+1
= 0 0
2
Together,wehave
(cid:88)
η ≥
(1−L)(α 0+α 1)−(α 02+α 12)L+1
.
(j) 2
j∈[r,r¯]
(cid:114) (cid:114)
1 1 Nγ Nγ
≥ ((1−L)(α +α )+1)≥ (2(1−L) −2)=(1−L) −1
2 0 1 2 L L
.
(cid:80)
Fortheedgecase,sincetheintervalmusthavemorethanthreesnippets,wecanbound η ≥
j∈[r,r¯] (j)
(cid:113)
(cid:80) Nγ
1.5−2L. Therefore,wehave η ≥max(1.5−2L,(1−L) −1).
j∈[r,r¯] (j) L
Finally,wecanboundthebalancednessby
(cid:80) (cid:80)
η η
j∈[r,r¯] (j) j∈[r,r¯] (j)
≥
(cid:80) (cid:80)
(1−η ) η +N(γ −γ )+LN
j∈[r,r¯] (j) j∈[r,r¯] (j) 1 0
N(γ −γ )+LN
≥1− 1 0
(cid:80)
η +N(γ −γ )+LN
j∈[r,r¯] (j) 1 0
Nγ¯+LN
≥1−
(cid:80)
η +Nγ¯+LN
j∈[r,r¯] (j)
Nγ¯+LN Nγ¯+LN
≥1−min( , )
(cid:113)
1.5+Nγ¯+LN −2L Nγ
(1−L) +Nγ¯+LN −1
L
Nγ¯+LN √ Nγ¯+LN +1
≥1−min( , L· )
(cid:112)
1.5−2L (1−L) Nγ
C TRAINING DETAILS
Ourmodelisfine-tunedusingtheAdamWoptimizerwithacosinelearningrateschedule. Forevery
training of f in Algorithm 1, we train up to 5 epochs. When measuring performance, we use a
separatevalidationsettofindthehighestperformancecheckpoint. Forfocalloss,weuseγ =5,and
αissettotheimbalanceratioestimatedfromTable3fortheminorityclass.
20