Salient Information Prompting to Steer Content in
Prompt-based Abstractive Summarization
LeiXu1,MohammedAsadKarim2†,SaketDingliwal1,AparnaElangovan1
1AmazonAWSAILabs
2CarnegieMellonUniversity
{leixx, skdin, aeg}@amazon.com mkarim2@cs.cmu.edu
Abstract andlossfunctions,hinderingwidespreadadoption,
especiallyforlargelanguagemodels(LLMs)with
Largelanguagemodels(LLMs)cangenerate
billions of parameters. Recent work (Li et al.,
fluentsummariesacrossdomainsusingprompt-
2023a) trains a separate network using reinforce-
ingtechniques,reducingtheneedtotrainmod-
ment learning (RL) to generate keyphrases for
elsforsummarizationapplications. However,
LLMprompts,buttrainingRLmodelisnon-trivial
craftingeffectivepromptsthatguideLLMsto
generatesummarieswiththeappropriatelevel duetoconvergenceandstabilityissues(Wangetal.,
ofdetailandwritingstyleremainsachallenge. 2024). Emphasizing salient information in the
Inthispaper,weexploretheuseofsalientin- prompt can help zero-shot LLMs generate more
formationextractedfromthesourcedocument complete summaries, and steer LLMs to gener-
toenhancesummarizationprompts. Weshow
atesummariesthatalignwiththedesiredusecase.
thataddingkeyphrasesinpromptscanimprove
However, there is also a lack of analysis on how
ROUGEF1andrecall,makingthegenerated
emphasizingsalientinformationinpromptswould
summaries more similar to the reference and
more complete. The number of keyphrases affecttheLLMbehavior.
cancontroltheprecision-recalltrade-off. Fur- Wefirstaddressthechallengeofapplyingsalient
thermore,ouranalysisrevealsthatincorporat- information to LLMs. We obtain keyphrases us-
ingphrase-levelsalientinformationissuperior ingastand-alonekeyphrasesignalextractorcalled
to word- or sentence-level. However, the im-
SigExt, and prompt the LLMs to consider these
pactonhallucinationisnotuniversallypositive
keyphrases when generating summaries. Unlike
acrossLLMs. Toconductthisanalysis,wein-
priorworkrelyingoncomplexkeyphrasegenera-
troduceKeyphraseSignalExtractor(SigExt),
torsoptimizedforspecificLLMs,SigExtisLLM-
a lightweight model that can be finetuned to
extract salient keyphrases. By using SigExt, agnostic,allowingleveragingsalientinformation
weachieveconsistentROUGEimprovements with large API-based models that cannot be fine-
across datasets and open-weight and propri- tuned. We demonstrate consistent improvement
etaryLLMswithoutanyLLMcustomization. inROUGEscoreson4representativesummariza-
Ourfindingsprovideinsightsintoleveraging
tion datasets and 3 recent LLMs – Claude, Mis-
salient information in building prompt-based
tral (Jiang et al., 2023), and Falcon (Almazrouei
summarizationsystems.
et al., 2023) – highlighting the wide adaptability
of our approach. Secondly, we conduct compre-
1 Introduction hensiveexperimentsusingSigExttogaininsights
intohowkeyphrasesinpromptsaffectdifferentas-
Abstractive summarization aims to generate con-
pects of summary quality. We show that adding
cisesummariesthatcapturethemostsalientinfor-
keyphrasesimprovesROUGEF1andrecall,mak-
mationfromlengthysourcedocuments. Priorwork
ingthegeneratedsummariesmoresimilartotheref-
hasshownthatemphasizingkeywordsfromsource
erenceandmorecomplete. Adjustingthenumber
documents can enhance summarization perfor-
ofkeyphrasesinfluencesthetrade-offbetweenpre-
manceonsupervisedfinetuned(SFT)models(Gu
cisionandrecall. Includingadditionalkeyphrases
etal.,2016). However,existingapproaches(Nalla-
intheprompttendstoproducemoredetailedsum-
patietal.,2016;Seeetal.,2017;Liuetal.,2021)
maries, enhancing recall. Our findings indicate
requireextensivemodificationstothearchitecture
thatusingphrase-levelsalientinformationismore
†WorkdoneduringaninternshipatAWSAILabs. effectivethanword-orsentence-levelapproaches.
1
4202
tcO
3
]LC.sc[
1v14720.0142:viXraLabels for Training 1 1 1 1 1 1 0 0 0 0 0 0
Longformer
Longformer Phrase Extractor
Article: The 2025 NBA All - Star Game will take place at home of the Golden State Warriors ....
phrase 1 phrase 2 phrase 3 phrase 4
75.5% 31.3% 24.2% 32.6%
Best character-level
fuzzy matching score
Summary: San Francisco Bay Area to host NBA All - Star Game 2025
Figure1:SigExt–afinetunedLongformertoextractkeyphrasesfromanarticle.Weconstructlabelsbythresholding
thecharacter-levelfuzzymatchingscorebetweenphrasesinthearticleandthesummary.
However, for certain large language models like boththesourcedocumentandthesummary,then
Mistral,addingkeyphrasesmayleadtomorehallu- optimizingthecrossentropyloss. Comparedtopre-
cinations. viousakeyphrasegeneratorthatusesRL(Lietal.,
Ouranalysisoffersguidanceforapplyingsimi- 2023a),SigExtallowseasiercontrolofkeyphrase
larstrategiesinreal-worldsummarizationapplica- numbers,fastertrainingandinference,andbetter
tions. Whileincorporatingsalientinformationisan consistencyacrossdomains. Wedirectlyincorpo-
effectivemethodforenhancingandcontrollingthe ratekeyphrasesinprompt,makingitgeneralizable
completenessofsummaries,andusingphrase-level acrossLLMs. Tohandlelongerinputlengthswhile
granularityprovesmoreeffective,theriskofintro- maintainingefficiency,webuildSigExtusingLong-
ducinghallucinationsmustbecarefullyconsidered. former,sothattrainingandinferencecanbedone
ThisriskdependsonthespecificLLMbeingused, onasingleGPU.
themethodforgatheringsalientinformation,and
2.1 Phrasetokenization
thecriticalityoftheapplication.
Ourkeycontributionsareasfollows: Let x = x 1,...,x n be a source document of n
1) We present SigExt, a simple yet effective tokens, and y = y 1,...,y m be the target sum-
keyphraseextractionmodelusingafinetunedLong- mary of m tokens. The document is segmented
former(Beltagyetal.,2020). Oncetrained,SigExt into non-overlapping phrases by removing stop-
isLLM-agnostic,enablingperformanceboostfor words and puctuation. After this, we get a se-
differentLLMsbyaddingextractedkeyphrasesin quenceofT non-overlappingphrases,denotedas
promptswithoutrequiringLLMfinetuning. Phrase(x) = [p i = x li...x ri] i=1...T. Similarly,
2) We provide a comprehensive analysis on the we get T′ phrases from the summary denoted as
impact of adding salient information in prompts Phrase(y) = [q i = y l′ ...y r′] j=1...T′.
j j
forsummarization,includinginsightsonsummary
2.2 Labelsandlearningobjective
length,referencealignment,completeness,andhal-
lucination. Welabeleachinputphrasebycomputethefuzzy
3) We demonstrate that SigExt has cross-domain matchingscore
generalizationcapabilitythroughageneral-purpose
fuzz(a,b)= |longest_common_sequence(a,b)|,
version(GP-SigExt)pretrainedon7datasets. max(|a|,|b|)
againstallphrasesinthesummary. Ifthemaximum
2 Method
scoreexceedscertainthresholdϵ,itisconsidereda
Inthissection,weintroduceSigExt–akeyphrase keyphrase,formally
extractor designed for boosting summarization
(cid:40)
quality of prompt-based LLMs. Figure 1 gives label(pi)=
1 max j∈1...T′fuzz(pi,qj)≥ϵ,
0 otherwise.
an overview. SigExt tokenizes the source docu-
mentintophrases(phrasetokenizationisdetailed Wetrainaclassificationmodeltopredictthelabel.
inSection2.1),andsimultaneouslypredictwhether Specifically,weuseaLongformerandaddaclassi-
each phrase is important. To train the model, we ficationheadontopofeachtoken. Wecomputethe
createtargetlabelsbyidentifyingphrasesappearin crossentropylossontokensthatbelongtophrases,
2whileignoringpredictionsonpunctuationandstop- LLMs and Prompts: We evaluate SigExt on
word tokens. We apply class balancing weight λ Claude-Instant, Mistral-7B-Instruct, and Falcon-
whenthelabelofthetokenis0. 40B-Instruct LLMs. We do not use Falcon on
ArXiv and MeetingBank datasets due to its lim-
2.3 ApplicationofSigExtonsummarization
itedcontextwindow. Wemanuallyoptimizedthe
We first finetune SigExt on the summarization promptsforeachmodelandtasktoachievecom-
dataset to get a task-specific keyphrase extrac- petitive zero-shot performance. All prompts are
tor. During inference, we use SigExt to extract listedinAppendixA.
keyphrases, then wrap the source article with a SigExt & GP-SigExt Parameters: We use
summarizationprompt,andincludekeyphrasesin Longformer-large(433M)forthekeyphraseextrac-
theprompt. Hereisanexampleprompt: tor. Wesetthefuzzymatchingthresholdϵ = 70%,
Here is an news article: <text> \nHere and the class balancing weight λ = 0.1. For
are a few keyphrases from the article: < SigExt,wesample1000examplesfromtrainingset,
key_phrases> \nPlease write an summary
wetrainSigExtstartingwithoriginalLongformer-
for the article. \nSummary:
largecheckpoint. ForGP-SigExt,wesample10000
To select keyphrases, we first score each phrase
examples from each of the 7 dataset mentioned
bycalculatingtheaveragelogitsofitstokens. We
in Sec. 2.4. We train SigExt and GP-SigExt for
thenselectthetop-K deduplicatedphrasesaccord-
10 epochs, and use validation set to pick the best
ingtotheirlogitsscores,removinganyduplicates
checkpointbasedonrecall@20(Metricdefinedin
thatexceedafuzzymatchingthresholdϵandkeep-
Sec.3.7).
ing the longer phrase in those cases. We replace
During prompting, we try K = 10,15,20
<key_phrases>withcommaseparatedkeyphrases.
keyphrasesfortheCNN,SAMSum,andMeeting-
This prompt then serves as the input to the LLM
Bankdatasets,andK = 30,35,40keyphrasesfor
whichproducesthefinalsummary.
the ArXiv dataset. We pick the best number of
keyphrasesbasedonROUGEscoresonthevalida-
2.4 Crossdomaingeneralization
tionset. Wealsoconductanablationstudyonthe
In order to generalize the keyphrase extractor
effectofdifferentnumbersofkeyphrases.
model to new domains without fine-tuning for
Baseline: We compare our methods with naive
the target domain, we train a general purpose
zero-shot prompting. We adapt a 2-pass extract-
keyphrase extractor using a combination of 7
then-abstract method (Zhang et al., 2023) to the
datasets. ThedatasetsareXSUM(Narayanetal.,
threeLLMsanduseitasabaseline. Thismethod
2018), Multi-News (Fabbri et al., 2019), Giga-
usestheLLMtoextractsentencesfromthesource
word(Nallapatietal.,2017),Big-Patent(Sharma
documentinthefirstpass,thenusesthesecondpass
etal.,2019),AESLC(ZhangandTetreault,2019),
to revise the extracted sentences into an abstrac-
BillSum(KornilovaandEidelman,2019),andWik-
tivesummary. WealsocomparewithDirectional
iHow (Koupaee and Wang, 2018). We call this
StimulusPrompting(Lietal.,2023b)whichutilize
general-purposekeyphrasesignalextractormodel
reinforcementlearningtoselectgoodkeywords.
GP-SigExt.
Evaluation Metrics: We compute ROUGE-1/-L
F1scores(abbreviatedasR1-f,RL-f)toevaluate
3 Experiments
summaryquality. WealsoreportROUGE-1Recall
Datasets: We select 4 representative datasets (R1-r)toassessthecompleteness. WeuseAlign-
– SAMSum (Gliwa et al., 2019), CNN/Daily- Score(Zhaetal.,2023)toevaluatethefaithfulness
Mail(Nallapatietal.,2016),ArXiv(Cohanetal., ofthesummary.
2018),andMeetingBank(Huetal.,2023)–toeval-
3.1 MainResults
uate our method. These datasets cover short and
longtext,aswellasregulardocumentandconver- Table1showstheROUGEscoresonall4datasets.
sation summarization. Dataset details are shown The F1 scores are improved by using GP-SigExt
inTable11inAppendix. Wetruncateinputtextto withoutanyfine-tuningonnewdatasets. Byfine-
4,000tokenstofitthecontextwindowoftheLong- tuning only the phrase extractor, SigExt further
former model. We follow the convention to eval- improves the score, showing that using a super-
uate on 500 randomly sampled examples (Zhang visely learned keyphrase extractor can make the
etal.,2020). Wereportresultsaveragedon3runs. LLMgeneratesummariesmoresimilartotheref-
3SAMSum CNN/DailyMail ArXiv MeetingBank Avg.
Method R1-f RL-f R1-r R1-f RL-f R1-r R1-f RL-f R1-r R1-f RL-f R1-r ∆R1-f
Claude-Ins. 40.0 30.3 52.8 38.1 23.9 41.9 44.4 23.1 53.2 32.2 21.8 43.4
+2-stage 40.3 31.0 46.9 39.2 24.6 48.3 44.0 22.9 50.4 30.8 20.7 43.8 -0.1
+GP-SigExt 40.0 30.0 57.3 40.2 24.9 47.5 44.7 23.2 53.5 36.3 25.7 53.1 1.6
+SigExt 41.6 30.9 59.5 42.0 26.6 48.6 45.2 23.5 53.7 42.3 31.9 60.5 4.1
Mistral-7B 40.5 31.7 48.2 38.9 24.8 42.6 43.1 24.6 41.6 34.4 25.2 50.3
+2-stage 38.7 30.6 45.4 38.0 24.4 48.6 39.5 22.0 41.9 32.0 23.5 52.0 -2.2
+GP-SigExt 41.9 32.2 50.7 39.5 25.2 45.3 42.8 23.8 44.7 34.1 24.7 54.8 0.4
+SigExt 44.1 33.9 54.5 40.9 26.0 47.9 43.6 24.2 45.2 37.0 27.2 58.7 2.2
Falcon-40B 37.1 28.7 46.3 25.7 16.4 33.8
+2-stage 36.1 28.1 54.1 34.2 22.1 53.2 3.8
+GP-SigExt 38.5 29.4 54.1 31.9 20.4 42.3 3.8
+SigExt 39.9 30.4 56.1 33.5 21.3 43.2 5.3
0-shotSOTA 38.8 30.6 - 36.0 22.3 - 34.6 18.3 36.4 26.8 -
Table1: PerformanceofSigExt&GP-SigExtonsummarizationusingClaudeInstant,Mistral-7B-Instruct,and
Falcon-40B-Instruct. SigExtistrainedwith1000examples,whileGP-SigExtisnotfine-tunedonthedataset. We
compareourmethodswithzero-shotpromptingand2-stageextract-then-abstractbaselines. WeshowROUGE-1
F-Measure(R1-f),ROUGE-LF-Measure(RL-f),andROUGE-1recall(R1-r). TheLLMsarenotfine-tuned. We
directlycopyzero-shotSOTAforSAMSumandCNNfromLaskaretal.(2023),ArXivfromXiaoetal.(2022),and
MeetingBankfromHuetal.(2023).
erence. On average, compared to the already ChatGPT(gpt-3.5-turbo) in Table 2. We show
strongzero-shotClaudeInstantbaseline,R1-Fim- thatSigExtcanalsoboostChatGPTzero-shotper-
proves by 1.6% with GP-SigExt and 4.1% with formance,andoutperformthebaseline.
SigExt. Similar improvements are also observed
onMistralandFalconmodels. BesidesF1scores, Method #examples R1-f RL-f
adding keyphrases extracted by both SigExt and Vanilla 0 38.5 25.5
GP-SigExt into the prompts can significantly in- DirectionalStimulus 4000 40.2 26.8
SigExt 4000 42.2 27.0
creasetheR1-rscore,showingthataddingsalient
informationcanimprovethecompletenessofthe
Table2: ComparingSigExtwithbaselinesusingChat-
summary. Our method achieves a smaller gain GPTandCNNdataset.
on the ArXiv dataset compared to other datasets.
Wehypothesizethatthisisbecausepaperabstracts
3.2 HumanQualitativeCheck
have a standard format, and the keyphrases they
shouldcontainarethusmorepredictable. Asare- To verify the quality of the notes, we follow Liu
sult, the zero-shot LLM can already identify and et al. (2023) and conduct a human evaluation,
includethesekeyphrasesintheoutput. Forother in which they annotated Atomic Content Units
datasets, where the summary is more subjective, (ACUs) for several public datasets. Each ACU
ourmethodcanhelptheLLMincorporateproper represents a fact that should appear in the sum-
informationinthesummarytobetteralignwiththe mary. Weselect50documentsfromtheCNNand
reference. SAMSumdatasets,respectively,andaskhumanan-
notatorstoverifywhetherthegivenACUappears
Althoughthelengthofthesummaryslightlyin-
inthesummary. WereportboththerawACUcov-
creasewiththeintroductionofkeyphrases,wedo
erageandlength-normalizedACUcoverage,as
notachievetheseimprovementsbyexcessivelyin-
proposedbyLiuetal.(2023). Table3showsthat
creasing the length of the summary. On average,
SigExtconsistentlyoutperformsthevanillaLLM
thelengthofClaudeInstantsummariesincreases
intermsofACUcoverage.
by4.7wordsafteraddingkeyphrases, whereasit
increasesby13.6wordsforMistraland12.3words 3.3 NumberofKeyphrases
forFalcon.
We try different numbers of keyphrases in the
WealsocomparetheperformanceofSigExtwith prompt for each dataset, and show the ROUGE-
recentDirectionalStimulusPromptingbaselineon 1 Precision/Recall/F1 curves in Figure 2. The F1
4R1-f (vanilla) R1-f R1-r R1-p
CNN/DailyMail SAMSum ArXiv MeetingBank
50 60 60
50
45 50
40 40
40
40
10 20 30 10 20 30 25 50 75 10 20 30
# keyphrases # keyphrases # keyphrases # keyphrases
CNN/DailyMail SAMSum ArXiv MeetingBank
60
50 47.5
50
45.0
40
40
42.5
40
10 20 30 10 20 30 25 50 75 10 20 30
# keyphrases # keyphrases # keyphrases # keyphrases
Figure2: Effectofusingdifferentnumberofkeyphrasesontheprecision-recalltradeoff.
RawACU NomalizedACU variance. The word-level information performs
poorlyontheArXivdatasetbecauseforacademic
Claude +SigExt Claude +SigExt
papers, there are many multi-word phrases that
CNN 43.8% 52.4% 40.7% 47.3%
SAMSum 53.6% 63.3% 38.4% 40.7% are important in the summary. If these are split,
they are no longer helpful for summarization. In
Table3: ACUcoveragehumanevaluationonCNNand contrast, the sentence-level information is not so
SAMSumusingClaudeInstantgeneratedsummaries.
effective, especially on the MeetingBank dataset.
Whenthedatasetishighlyabstractive,theimpor-
tantwordsaredispersedacrossthedocument,mak-
scoresofourmodelarestablewhenchangingthe
ing it difficult to extract a few sentences to cover
numberofkeyphraseswithinafairlywiderange,
thecontentofthesummary(SeeexamplesinAp-
showing that introducing keyphrases can consis-
pendixTable10).
tentlyimprovethesummaryquality.
Asweincreasethenumberofkeyphrases,there
Claude-Instant R1-f RL-f R1-f RL-f
isacleartrendofincreasingrecallanddecreasing
precisionfortheMistralmodel. Thisislessevident SAMSum CNN
+SigExt(word) 41.4 30.9 42.0 26.2
fortheClaudemodel. Sinceweaddalengthcon-
+SigExt(phrase) 41.6 30.9 42.0 26.6
straintexplicitlyintheprompt(e.g.,"writeasum- +SigExt(sent) 39.1 29.7 40.3 25.7
maryin3sentences"),theClaudemodelappears ArXiv M.Bank
tofollowtheseinstructionsbetterthantheMistral +SigExt(word) 42.2 21.0 41.9 31.7
+SigExt(phrase) 45.2 23.5 42.3 31.9
models. Mistralmodelstendtotrytocoverallthe
+SigExt(sent) 44.8 23.8 36.2 25.8
keywords provided in the prompt. Consequently,
therecallincreasessignificantlywhenincreasing Table4: Differentgranularityofsalientinformation.
thenumberofkeywordsfortheMistralmodels.
3.4 GranularityofSalientInformation 3.5 SummaryFactuality
Wealsoexplorehowdifferentgranularityofsalient As shown in Table 5, the effect of adding
information can affect the summarization perfor- keyphrases on the AlignScore is LLM and task-
mance. Wecompareword-,phrase-,andsentence- specific. FortheClaudeInstantandFalconmodels,
levelSigExt. TheresultsareshowninTable4. The theAlignScoreistypicallyimprovedbyincorporat-
phrase-levelsalientinformationcanalwaysachieve ingkeyphrases. Incontrast,theAlignScorealways
topornear-topperformance,whiletheword-level decreasesfortheMistralmodel. Theseresultssug-
and sentence-level approaches have much larger gestthatkeyphrasesarenotuniversallyhelpfulfor
5
tnatsnI
edualC
tcurtsnI-B7-lartsiMimproving the faithfulness of the generated sum- 2004). Recent work has proposed transformer-
maries. Table8showsafewexampleswherehal- based keyphrase extraction models (Sun et al.,
lucinationisintroducedinthesummaryduetothe 2020;DingandLuo,2021)thatfocusongenerating
keyphrases. The failure pattern is if a keyphrase noun phrases to better align with human annota-
isnegatedinthedocument,Mistralmodelwould tion. However,inoursetting,theoraclekeyphrases
ignorethenegation. areconstructedheuristicallyandarenotlimitedto
noun phrases, making thesemodels apoor fitfor
SamSum CNN ArXiv M.Bank comparison. Therefore, we do not include them.
The evaluation results are shown on Table 7. We
ClaudeIns. 85.8 83.8 53.7 73.1
+SigExt 88.0 82.3 60.0 74.7 showthatGP-SigExtalreadyoutperformsstatisti-
Mistral-7B 88.9 88.8 56.9 79.1 calmethods. Andthefine-tunedSigExtachieves
+SigExt 84.7 87.0 49.5 77.1 additional 5.9% and 3.7% improvements on two
Falcon-40B 81.6 67.7 datasetsrespectively.
+SigExt 81.6 75.0
SAMSum CNN M.Bank Arxiv
Table5: SummaryfactualitymeasuredbyAlignScore. Method R@15 R@15 R@15 R@35
Rake 68.3 11.9 17.1 14.2
TextRank 80.5 20.8 19.3 22.4
3.6 IntroducingExternalOracleKeyphrases
GP-SigExt 75.5 27.7 40.3 31.7
(+32ex.) 81.5 29.7 47.3 32.0
We also analyze how external keyphrases which
(+128ex.) 85.5 32.9 62.2 32.7
do appear in the source document would affect
SigExt(1kex.) 83.3 33.6 65.7 35.4
the performance. We use oracle keyphrases that
appearsinthereferencesummarybutdonotappear
Table7: Keyphraseextractionperformance.
inthesourcedocumentasadditionalinformationin
theprompt. TheROUGE-1scoreandAlignScore
3.8 Casestudy
areshownonTable6. TheROUGEscoreincreases
significantlywhiletheAlignScorefalls. Itindicates WeshowsomeexamplesinAppendixTable9. We
thatintroducingexternalkeyphrasesmighthurtthe foundtheextractedkeyphrasescanhelptheLLM
factualityofthesummary. incorporateprecisedetailsinthesummary,hence
thesummariesbetteralignwiththegoldsummary.
Claude-Ins. R1-f Align. R1-f Align. In the first two examples, the keyphrases contain
SamSum CNN exactnumbersandtimes,andtheLLMwasableto
+SigExt 41.6 88.0 42.0 82.3 includetheminthesummary. Inthethirdexample,
+Oracle 50.0 86.3 50.0 78.8
withSigExt,thesummarycoversmoretopicsthan
ArXiv M.Bank the vanilla model. Since we instruct the LLM to
+SigExt 45.2 60.0 42.3 74.7
“consider”thesekeyphrases,theLLMwasableto
+Oracle 51.6 45.9 48.2 56.7
skiporrephrasesometogetmorefluentresults.
Table6: Summaryqualitywithoraclekeyphrases.
4 RelatedWork
Leveragingkeywordinabstractivesummarization
3.7 Effectivenessofkeyphraseextraction
has been explored in many works. Switching
In this part, we analyze the effectiveness of Generator-Pointer(Nallapatietal.,2016)andCopy-
the Longformer keyphrase extractor. We define Net(Guetal.,2016)modifyarecurrentneuralnet-
recall@K metrictoevaluatethekeyphraseextrac- workmodel(Chopraetal.,2016)todirectlycopy
tionperformance. Wedefinetherecall@K asthe keywordsfromthesourcetext. Morerecentwork
recall of oracle keyphrases in the top-K dedupli- has adopted transformer architectures (Vaswani
catedkeywords,whereoraclekeyphrasesarecon- etal.,2017),whichhavebecomedominantinnat-
structedbyfindingthephraseinthesourcedocu- ural language processing. Liu et al. (2022) intro-
ment with the highest fuzzy match score to each duces a bias in the attention matrix to help trans-
phrase in the target summary. We compare our formermodelsfocusonkeywords. Allthesemod-
methodwithtwostatisticalmethods,Rake(Rose els need to be trained or finetuned on large-scale
et al., 2010) and TextRank (Mihalcea and Tarau, training data. While finetuned models typically
6achieve higher ROUGE scores than prompting a tivelyimprovetheROUGEscoresofthegenerated
pretrainedmodel,prompt-basedsummarizersare summaries, indicating a higher similarity to ref-
preferredinsomeindustrialusecasesduetotheir erencesummaries. Introducingkeyphrasesinthe
flexibilityandreducedneedfordatacollection. In- promptenhancesthefaithfulnessofthesummary
corporating keyphrases in the prompt can effec- byensuringthatimportantinformationiscaptured.
tivelycontrolthelengthandcontentcoverageofthe Additionally,ourapproachofferscontroloverthe
summary,somethingthatfine-tuningmethodscan- length and precision/recall trade-off of the sum-
noteasilyachieve. Therefore,wecannotcompare mary. Notably,ourpretrainedkeyphraseextractor
withthesemethodsusingmetricslikeROUGE. –GP-SigExt–canimprovesummarizationperfor-
InstructionfinetunedLLMs(Chungetal.,2022; manceout-of-the-boxwithoutanyfinetuning,even
Touvron et al., 2023; Zhang et al., 2022) have incaseswheretrainingdataisnotavailable.
shown strong performance on summarization
Limitations
purelyviaprompts,withoutfinetuningdata. Such
models are often offered via APIs, enabling eas-
Model Design: We use Longformer as the back-
ier development and deployment of summariza-
bone model to build SigExt because it is light-
tionapplications. Keyphrasesarestillhelpfulfor
weightandsupportslongcontextlength. However,
theselargemodels,asLietal.(2023a)showthat
wedonotevaluatetheimpactofusingothersimilar-
akeyphrasegeneratertrainedwithreinforcement
sized pre-trained language models. Additionally,
learningcanimprovesummarizationperformance.
weextracttraininglabelsusingafuzzymatching
There has been interest in 2-stage extractive-
approach to make the model more generalizable,
then-abstractive approaches (Su et al., 2020; Liu
butmoredomain-specificapproachesforkeyphrase
etal.,2021;Lietal.,2021;Suetal.,2022;Yang
extractionmayyieldbetterperformance.
et al., 2023). These first extract keyphrases or
Evaluation: Asiscommoninsummarizationre-
sentencesbeforeabstractivelysummarizingthem.
search, we rely primarily on automatic metrics
Thesemethodsaretrainedend-to-endfordomain-
andqualitativeexamplecheckstoevaluateperfor-
specific use cases, while our method can be pre-
mance. Thesetechniqueshaveknownlimitations
trained for general purpose zero-shot use cases.
inassessingsummaryquality. Meanwhile,human
Practically, any keyword extractor, for example
evaluationhasitsownchallenges. Therefore,how
KeyBERTorLLMBERT(Grootendorst,2020),can
tobestevaluatethequalityofabstractivesumma-
beusedforthefirststagetoenhancethesummariza-
rizationmodelsremainasanopenquestion.
tioninthesecondstage. The2-stagemethodscould
also be implemented as Chain-of-Thought (CoT)
by generatingintermediate hintsand final results References
in the same prompt, such as Adams et al. (2023).
Griffin Adams, Alex Fabbri, Faisal Ladhak, Eric
Inourexperiments,wecompareourmethodwitha
Lehman,andNoémieElhadad.2023. Fromsparseto
2-stagepromptingapproach–firstgeneratingkey-
dense: GPT-4summarizationwithchainofdensity
wordsusingoneprompt,thenusingthosekeywords prompting. InProceedingsofthe4thNewFrontiers
for summarization in the second prompt. While inSummarizationWorkshop,pages68–74,Singapore.
AssociationforComputationalLinguistics.
slightlydifferentfrompreviouswork,the2-stage
baselineeffectivelycapturestheuseofintermediate
EbtesamAlmazrouei,HamzaAlobeidli,AbdulazizAl-
reasoningstepsofLLMs.
shamsi, Alessandro Cappelli, Ruxandra Cojocaru,
MérouaneDebbah,ÉtienneGoffinet,DanielHesslow,
5 Conclusion JulienLaunay,QuentinMalartic,DanieleMazzotta,
BadreddineNoune,BaptistePannier,andGuilherme
Penedo.2023. Thefalconseriesofopenlanguage
Inthispaper,weproposealightweightapproachto
models. Preprint,arXiv:2311.16867.
incorporatekeyphrasesintothepromptforLLM-
basedabstractivesummarization. SigExtinvolves Iz Beltagy, Matthew E. Peters, and Arman Cohan.
trainingaphraseextractorusingsupervisedlearn- 2020. Longformer: Thelong-documenttransformer.
ing to identify salient keyphrases from the input
Preprint,arXiv:2004.05150.
text. These keyphrases are then injected into the
SumitChopra,MichaelAuli,andAlexanderM.Rush.
promptprovidedtotheLLMforsummarygenera-
2016. Abstractivesentencesummarizationwithat-
tion. Wedemonstratethatthisapproachcaneffec- tentiverecurrentneuralnetworks. InProceedingsof
7the2016ConferenceoftheNorthAmericanChap- AlbertQ.Jiang,AlexandreSablayrolles,ArthurMen-
teroftheAssociationforComputationalLinguistics: sch,ChrisBamford,DevendraSinghChaplot,Diego
Human Language Technologies, pages 93–98, San delasCasas,FlorianBressand,GiannaLengyel,Guil-
Diego, California. Association for Computational laumeLample,LucileSaulnier,LélioRenardLavaud,
Linguistics. Marie-AnneLachaux,PierreStock,TevenLeScao,
Thibaut Lavril, Thomas Wang, Timothée Lacroix,
HyungWonChung,LeHou,ShayneLongpre,Barret andWilliamElSayed.2023. Mistral7b. Preprint,
Zoph,YiTay,WilliamFedus,YunxuanLi,Xuezhi arXiv:2310.06825.
Wang,MostafaDehghani,SiddharthaBrahma,etal.
2022. Scalinginstruction-finetunedlanguagemodels. Anastassia Kornilova and Vladimir Eidelman. 2019.
arXivpreprintarXiv:2210.11416. BillSum: Acorpusforautomaticsummarizationof
USlegislation. InProceedingsofthe2ndWorkshop
Arman Cohan, Franck Dernoncourt, Doo Soon Kim, on New Frontiers in Summarization, pages 48–56,
TrungBui,SeokhwanKim,WalterChang,andNazli HongKong,China.AssociationforComputational
Goharian.2018. Adiscourse-awareattentionmodel Linguistics.
forabstractivesummarizationoflongdocuments. In
Proceedings of the 2018 Conference of the North MahnazKoupaeeandWilliamYangWang.2018. Wik-
AmericanChapteroftheAssociationforComputa- ihow: A large scale text summarization dataset.
tionalLinguistics: HumanLanguageTechnologies, Preprint,arXiv:1810.09305.
Volume 2 (ShortPapers), pages 615–621, New Or-
leans,Louisiana.AssociationforComputationalLin-
Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur
guistics.
Rahman, MdAmranHossenBhuiyan, ShafiqJoty,
and Jimmy Huang. 2023. A systematic study and
HaoranDingandXiaoLuo.2021. Attentionrank: Un-
comprehensiveevaluationofChatGPTonbenchmark
supervisedkeyphraseextractionusingselfandcross
datasets. In Findings of the Association for Com-
attentions. InProceedingsofthe2021Conferenceon
putational Linguistics: ACL 2023, pages 431–469,
EmpiricalMethodsinNaturalLanguageProcessing,
Toronto,Canada.AssociationforComputationalLin-
pages1919–1928.
guistics.
AlexanderFabbri,IreneLi,TianweiShe,SuyiLi,and
HaoranLi,ArashEinolghozati,SrinivasanIyer,Bhar-
DragomirRadev.2019. Multi-news: Alarge-scale
gavi Paranjape, Yashar Mehdad, Sonal Gupta, and
multi-documentsummarizationdatasetandabstrac-
Marjan Ghazvininejad. 2021. Ease: Extractive-
tivehierarchicalmodel. InProceedingsofthe57th
abstractivesummarizationwithexplanations. arXiv
AnnualMeetingoftheAssociationforComputational
preprintarXiv:2105.06982.
Linguistics,pages1074–1084,Florence,Italy.Asso-
ciationforComputationalLinguistics.
ZekunLi,BaolinPeng,PengchengHe,MichelGalley,
JianfengGao,andXifengYan.2023a. Guidinglarge
BogdanGliwa,IwonaMochol,MaciejBiesek,andAlek-
languagemodelsviadirectionalstimulusprompting.
sanderWawer.2019. SAMSumcorpus: Ahuman-
arXivpreprintarXiv:2302.11520.
annotated dialogue dataset for abstractive summa-
rization. In Proceedings of the 2nd Workshop on
ZekunLi,BaolinPeng,PengchengHe,MichelGalley,
NewFrontiersinSummarization,pages70–79,Hong
JianfengGao,andXifengYan.2023b. Guidinglarge
Kong,China.AssociationforComputationalLinguis-
languagemodelsviadirectionalstimulusprompting.
tics.
arXivpreprintarXiv:2302.11520.
Maarten Grootendorst. 2020. Keybert: Minimal key-
wordextractionwithbert. ShuaiqiLiu,JiannongCao,RuosongYang,andZhiyuan
Wen. 2022. Key phrase aware transformer for ab-
JiataoGu,ZhengdongLu,HangLi,andVictorO.K.Li. stractivesummarization. InformationProcessing&
2016. Incorporatingcopyingmechanisminsequence- Management,59(3):102913.
to-sequencelearning. InProceedingsofthe54thAn-
nualMeetingoftheAssociationforComputational YixinLiu,AlexFabbri,PengfeiLiu,YilunZhao,Liny-
Linguistics (Volume 1: Long Papers), pages 1631– ong Nan, Ruilin Han, Simeng Han, Shafiq Joty,
1640, Berlin, Germany. Association for Computa- Chien-ShengWu,CaimingXiong,etal.2023. Re-
tionalLinguistics. visitingthegoldstandard: Groundingsummarization
evaluationwithrobusthumanevaluation. InProceed-
Yebowen Hu, Timothy Ganter, Hanieh Deilamsalehy, ingsofthe61stAnnualMeetingoftheAssociationfor
FranckDernoncourt,HassanForoosh, andFeiLiu. ComputationalLinguistics(Volume1: LongPapers),
2023. MeetingBank: Abenchmarkdatasetformeet- pages4140–4170.
ingsummarization. InProceedingsofthe61stAn-
nualMeetingoftheAssociationforComputational Yizhu Liu, Qi Jia, and Kenny Zhu. 2021. Keyword-
Linguistics(Volume1: LongPapers),pages16409– aware abstractive summarization by extracting set-
16423,Toronto,Canada.AssociationforComputa- levelintermediatesummaries. InProceedingsofthe
tionalLinguistics. WebConference2021,pages3042–3054.
8RadaMihalceaandPaulTarau.2004. TextRank: Bring- Baptiste Rozière, Naman Goyal, Eric Hambro,
ingorderintotext. InProceedingsofthe2004Con- Faisal Azhar, et al. 2023. Llama: Open and effi-
ferenceonEmpiricalMethodsinNaturalLanguage cient foundation language models. arXiv preprint
Processing,pages404–411,Barcelona,Spain.Asso- arXiv:2302.13971.
ciationforComputationalLinguistics.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
RameshNallapati,FeifeiZhai,andBowenZhou.2017. Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz
Summarunner: A recurrent neural network based Kaiser,andIlliaPolosukhin.2017. Attentionisall
sequencemodelforextractivesummarizationofdoc- youneed. InAdvancesinNeuralInformationPro-
uments. InProceedingsoftheAAAIconferenceon cessingSystems,volume30.CurranAssociates,Inc.
artificialintelligence.
Xu Wang, Sen Wang, Xingxing Liang, Dawei Zhao,
Ramesh Nallapati, Bowen Zhou, Cicero dos Santos, JincaiHuang,XinXu,BinDai,andQiguangMiao.
Caglar Gulcehre, and Bing Xiang. 2016. Abstrac- 2024. Deepreinforcementlearning: Asurvey. IEEE
tivetextsummarizationusingsequence-to-sequence TransactionsonNeuralNetworksandLearningSys-
RNNs and beyond. In Proceedings of the 20th tems,35(4):5064–5078.
SIGNLLConferenceonComputationalNaturalLan-
guage Learning, pages 280–290, Berlin, Germany. WenXiao,IzBeltagy,GiuseppeCarenini,andArman
AssociationforComputationalLinguistics. Cohan. 2022. PRIMERA: Pyramid-based masked
sentencepre-trainingformulti-documentsummariza-
Shashi Narayan, Shay B. Cohen, and Mirella Lapata. tion. InProceedingsofthe60thAnnualMeetingof
2018. Don’tgivemethedetails,justthesummary! theAssociationforComputationalLinguistics(Vol-
topic-aware convolutional neural networks for ex- ume 1: Long Papers), pages 5245–5263, Dublin,
treme summarization. In Proceedings of the 2018 Ireland.AssociationforComputationalLinguistics.
Conference on Empirical Methods in Natural Lan-
guageProcessing,pages1797–1807,Brussels,Bel- Chengran Yang, Jiakun Liu, Bowen Xu, Christoph
gium.AssociationforComputationalLinguistics. Treude, YunboLyu, MingLi, andDavidLo.2023.
Apidocbooster: Anextract-then-abstractframework
Stuart Rose, Dave Engel, Nick Cramer, and Wendy leveraginglargelanguagemodelsforaugmentingapi
Cowley.2010. Automatickeywordextractionfrom documentation. arXivpreprintarXiv:2312.10934.
individualdocuments. Textmining: applicationsand
theory,pages1–20. YuhengZha,YichiYang,RuichenLi,andZhitingHu.
2023. AlignScore: Evaluating factual consistency
AbigailSee,PeterJ.Liu,andChristopherD.Manning. with a unified alignment function. In Proceedings
2017. Gettothepoint: Summarizationwithpointer- of the 61st Annual Meeting of the Association for
generatornetworks. InProceedingsofthe55thAn- ComputationalLinguistics(Volume1: LongPapers),
nualMeetingoftheAssociationforComputational pages11328–11348,Toronto,Canada.Association
Linguistics (Volume 1: Long Papers), pages 1073– forComputationalLinguistics.
1083,Vancouver,Canada.AssociationforComputa-
tionalLinguistics. Haopeng Zhang, Xiao Liu, and Jiawei Zhang. 2023.
ExtractivesummarizationviaChatGPTforfaithful
Eva Sharma, Chen Li, and Lu Wang. 2019. BIG- summary generation. In Findings of the Associa-
PATENT: A large-scale dataset for abstractive and tionforComputationalLinguistics: EMNLP2023,
coherentsummarization. InProceedingsofthe57th pages3270–3278,Singapore.AssociationforCom-
AnnualMeetingoftheAssociationforComputational putationalLinguistics.
Linguistics,pages2204–2213,Florence,Italy.Asso-
ciationforComputationalLinguistics. JingqingZhang,YaoZhao,MohammadSaleh,andPe-
terJLiu.2020. Pegasus: pre-trainingwithextracted
JingSu,LongxiangZhang,HamidRezaHassanzadeh, gap-sentencesforabstractivesummarization. InPro-
andThomasSchaaf.2022. Extractandabstractwith ceedings of the 37th International Conference on
bartforclinicalnotesfromdoctor-patientconversa- MachineLearning,pages11328–11339.
tions. Proc.Interspeech2022,pages2488–2492.
RuiZhangandJoelTetreault.2019. Thisemailcould
Ming-Hsiang Su, Chung-Hsien Wu, and Hao-Tse saveyourlife: Introducingthetaskofemailsubject
Cheng. 2020. A two-stage transformer-based ap- linegeneration. InProceedingsofthe57thAnnual
proachforvariable-lengthabstractivesummarization. Meeting of the Association for Computational Lin-
IEEE/ACMTransactionsonAudio,Speech,andLan- guistics,pages446–456,Florence,Italy.Association
guageProcessing,28:2061–2072. forComputationalLinguistics.
YiSun,HangpingQiu,YuZheng,ZhongweiWang,and Susan Zhang, Stephen Roller, Naman Goyal, Mikel
ChaoranZhang.2020. Sifrank:anewbaselineforun- Artetxe,MoyaChen,ShuohuiChen,ChristopherDe-
supervisedkeyphraseextractionbasedonpre-trained wan,MonaDiab,XianLi,XiVictoriaLin,etal.2022.
languagemodel. IEEEAccess,8:10896–10906. Opt: Openpre-trainedtransformerlanguagemodels.
arXivpreprintarXiv:2205.01068.
HugoTouvron,ThibautLavril,GautierIzacard,Xavier
Martinet,Marie-AnneLachaux,TimothéeLacroix,
9Appendix
A AllPrompts
Hereweshowallthepromptsweusedintheexperiments. Inprompt,<text>willbereplacedwithsource
documents,and<keywords>willbereplacedwithcommaseparatedkeyphrasesextractedbySigExt. We
conductlightpromptengineeringtogetareasonablygoodzero-shotprompt.
A.1 Zero-shotClaudeInstantPrompts
SAMSum
Here is a conversation:
<text>
Please write a very short 1 sentence summary.
SAMSumwithSigExt
Here is a conversation:
<text>
Please write a very short 1 sentence summary. Consider include the following
information: <keywords>
CNN/DailyMail
Here is a news article:
<text>
Please write a summary for the article in 2-3 sentences.
CNN/DailyMailwithSigExt
Here is a news article:
<text>
Please write a summary for the article in 2-3 sentences. Consider include the
following information: <keywords>.
ArXiv
Here is a research paper:
<text>
Please write a comprehensive paper abstract section.
ArXivwithSigExt
Here is a research paper:
<text>
Please write a comprehensive paper abstract section. Consider include the following
information: <keywords>
MeetingBank
Here is a conversation:
<text>
Please write a summary in about 5 sentences.
MeetingBankwithSigExt
Here is a conversation:
<text>
Please write a summary in about 5 sentences. Consider include the following
information: <keywords>
10A.2 Zero-shotMistralPrompts
SAMSum
<s>[INST]Here is a conversation:
<text>
Please write a short 1 sentence summary. [/INST]
SAMSumwithSigExt
<s>[INST]Here is a conversation:
<text>
Please write a short 1 sentence summary. Consider include the following information:
<keywords>[/INST]
CNN/DailyMail
<s>[INST]Here is a news article:
<text>
Please write a short summary for the article in 1-2 sentences.[/INST]
CNN/DailyMailwithSigExt
<s>[INST]Here is a news article:
<text>
Please write a short summary for the article in 1-2 sentences. Consider include the
following information: <keywords>[/INST]
ArXiv
<s>[INST]Here is a research paper:
<text>
Please write a short abstract in about 3 sentences.[/INST]
ArXivwithSigExt
<s>[INST]Here is a research paper:
<text>
Please write a short abstract in about 3 sentences. Consider include the following
information: <keywords>[/INST]
MeetingBank
<s>[INST]Here is a conversation:
<text>
Please write a 2-3 sentence summary.[/INST]
MeetingBankwithSigExt
<s>[INST]Here is a conversation:
<text>
Please write a 2-3 sentence summary. Consider include the following information: <
keywords>[/INST]
A.3 Zero-shotFalconandFlan-T5Prompts
SAMSum
Here is a conversation:
<text>
Please write a short 1 sentence summary. Summary:
SAMSumwithSigExt
11Here is a conversation:
<text>
Please write a short 1 sentence summary. Consider include the following information:
<keywords>
Summary:
CNN/DailyMail
Here is a news article:
<text>
Please write a short summary for the article in 1-2 sentences.
Make sure the summary is no more than 2 sentences. Summary:
CNN/DailyMailwithSigExt
Here is a news article:
<text>
Please write a short summary for the article in 1-2 sentences. Consider include the
following information: <keywords>.
Make sure the summary is no more than 2 sentences. Summary:
ArXiv
Here is a research paper:
<text>
Please write a short abstract in about 3 sentences.
Abstract:
ArXivwithSigExt
Here is a research paper:
<text>
Please write a short abstract in about 3 sentences. Consider include the following
information: <keywords>.
Abstract:
MeetingBank
Here is a conversation:
<text>
Please write a 2-3 sentence summary.
Summary:
MeetingBankwithSigExt
Here is a conversation:
<text>
Please write a 2-3 sentence summary. Consider include the following information: <
keywords>
Summary:
B LossFunction
ThetrainingobjectiveforSigExtis
(cid:88) (cid:88) (cid:3)
L = − [label(p )logf(x )+λ(1−label(p ))log(1−f(x )) ,
i k i k
i∈1...Tk∈li...ri
wheref(x )denotesthebinaryclassificationprobabilityontokenx bytheclassificationhead,andλis
k k
theclassbalancingweight.
12Jake:wedon’thaveenglishtoday
Nadia:whooooah!
Document
Vanessa:cool!yousurethough?
Jake:Smithcalledinsick,theycouldn’tfindareplacement,ohyeah
Keyphrases Vanessa;Smithcalled;Jake;replacement;englishtoday
Mistral Jakeannouncedthattheydon’thaveEnglishclasstodayduetoSmithbeingsickandunableto
findareplacement.
+SigExt VanessaisexcitedthatJakeisteachingEnglishtodayinsteadofSmith,whocalledinsickand
couldn’tbereplaced
Sam:Goodafternoon,isthisavailable?
Lia:Hi,howcanIhelpyou?
Sam:I’minterestedinthisproduct.Aretheearphonesavailable?
Lia:Yes,theyare.
Sam:wherecanIcollectthem?
Lia:Well,youcanhavethemdeliveredtoyouthroughcourrier-thiscosts£4(payondelivery)
Lia:I’mafraidcollectingthempersonallyisalittledifficult,astheproductcanonlybecollected
inGarstang(wherethecompanyislocated)
Sam:Okay.
Document
Sam:Aretheynew?Doyouhaveanyothercolorsthatwhite?
Lia:Completelynew-wealsooffergreyandpink.
Sam:ThenIwouldlikethewhiteonesplease.
Lia:Okay,soIjustneedyouraddressandtelephone
Sam:Thetotalcosts£187-canIlowerthepriceto£180??
Lia:fraidnot-companypolicy.
Sam:Oh,okay.
Sam:Howlongdoesittakefortheproducttoarrive?
Lia:Itgenerallyarrivesthenextday,inrareinstancesit’slonger.
Keyphrases Lia;earphonesavailable;Garstang;£180;collected;Sam;product;white;delivered;company.
Mistral Samisinterestedinpurchasingearphonesandasksabouttheiravailability,collectionoptions,
anddeliverytime.
+SigExt Liaconfirmedthattheearphonesareavailable,canbedeliveredfor£4,andareonlyavailablefor
collectioninGarstang,butofferedSamtheoptiontopurchasethemfor£180
Kim:I’mgoingtoSeoul!
Agatha:wowfinally!
Mark:When?Bringussomekimchi!
Kim:PeopleonaplanewillkillmeifIdo:D
Document Kim:I’mgoinginApril<unk>3
Agatha:AreyougoingwithJane?
Kim:yes,ofcourse-she’llbeourtourguide
Mark:sojealous...I’dlovetocomeaswell
Kim:Really?Iaskedyoulikeahundredtimes
Keyphrases Seoul;Kim;Jane;Agatha;Mark;going;kimchi;tourguide;Kim.
Mistral KimisgoingtoSeoulinAprilwithJaneastheirtourguideandAgathaandMarkexpresstheir
excitementandjealousy.
+SigExt KimisgoingtoSeoulinAprilwithJaneastheirtourguide,andtheywillbringsomekimchi
backforMarkwhoisjealousandwisheshecouldcometoo
Table8: Exampleswhereaddingkeyphrasesinprompthurtsthefaithfulnessofthesummary. Thefailurepatternis
ifakeyphraseisnegatedinthedocument,theLLMwouldignorethenegation.
13GoldSummary NHSgaveout404,500prescriptionsforsuncreamatacostof£13min2014.Alsohandedout
4.7millionprescriptionsforindigestionpillscosting£29m. Otheritemsroutinelyprescribed
includevitamins,Vaselineandtoothpaste.Criticsbrandedprescriptions’ludicrous’attimeof
financialcrisisforNHS.
KeyPhrases toothpaste;suncream;patients;404,500prescriptions;NHS;indigestionpillscosting£29million;
NHShanded;£13million;spendingmillions;prescribed
Mistral-7B TheNHSisspendingmillionsonprescriptionsforsuncreamandotherhouseholdessentials,
accordingtoadamninganalysis. ThefigurescomeastheUKranksbelowmanyEuropean
countriesinhealthandwellness,andastheNHSfacesahugefinancialcrisis.
+SigExt TheNHSisspendingmillionsofpoundsonprescribingtoothpasteandsuncreamtopatients,ac-
cordingtoadamninganalysis.In2014,theNHShandedout404,500prescriptionsforsuncream
atacostof£13millionand4.7millionprescriptionsforindigestionpillscosting£29million.
GoldSummary Clonedcamelwasbornin2009andcalledInzaz,whichmeansachievement.Shewascloned
fromovariancellsandbornbysurrogatemother.Injazisnowsixyearsoldandissaidtohave
conceivednaturally.She’sexpectedtogivebirthlatethisyear,provingclonedanimals’fertility.
KeyPhrases Injaz;pregnant;slaughteredcamel;cloned;ovariancells;surrogatemother;firstclonedcamel;
2009;conceivednaturally;whosename
Mistral-7B Theworld’sfirstclonedcamel,Injaz,ispregnantandexpectedtohaveacalf. Thismarksthe
firsttimethataclonedcamelhasbeenabletoreproducenaturally,andscientistsareexcitedto
seeifclonedcamelsareasfertileasnaturallyproducedones.
+SigExt Injaz,theworld’sfirstclonedcamelbornfromtheovariancellsofaslaughteredcamelin2009,
isnowpregnantandexpectedtogivebirthtoacalflaterthisyear.Thecamel’sconceptionwas
natural,provingthatclonedcamelsarefertileandcapableofreproducing.
GoldSummary LewisHamiltonwonSunday’sBarhainGrandPrixaheadofKimiRaikkonen.Hamiltonisout
ofcontractattheendoftheyearandisyettosignnewdeal.IthasbeensuggestedthatHamilton
couldreplaceRaikkonenatFerrari.ButteamprincipalMaurizioArrivabenesaysheishappy
withdriverline-up.
KeyPhrases KimiRaikkonen;Ferrari;nextseason;replaceRaikkonen;Mercedes;30;LewisHamiltonyet;
Bahrain;FerrariteamprincipalMaurizioArrivabene;Vettel
Mistral-7B FerrariteamprincipalMaurizioArrivabenehasrevealedthatheusesacarrot-and-stickmethod
withKimiRaikkonentokeephisdesiretoremainwiththeMaranellomarquehigh.TheFinnhas
arguablyneverbeenhappierduringhisFormulaOnecareer,thankstotheatmospheregenerated
withintheteambythearrivalofArrivabene.
+SigExt FerrariteamprincipalMaurizioArrivabenehasrevealedthatheusesacarrot-and-stickmethodto
keepKimiRaikkonen’sdesiretoremainwiththeMaranellomarquehigh.WithLewisHamilton
yettosignanewcontract,ithasbeensuggestedthathewillreplaceRaikkonenatFerrarifor
nextseason.IntheBahrainGrandPrix,Raikkonenfinishedonthepodiumforthefirsttimein
26racesasrunner-uptoHamilton.
Table9: ExamplesofusingSigExtwithMistral-7BmodelonCNNdataset.
14Jenkin:heywhatisyourspiritanimal?
Sophie:what?
Jenkin:goon?
Sophie:Idontknowafoxlol
Jenkin:areyouwiley?
Sophie:sometimes
Jenkin:Iama
Sophie:Ithinkyouareabitmadlikethemad
Document Jenkin:Ihavebeenreadingaboutanimalspiritsitsquitegood
Sophie: youwillhavetotellmeaboutthefox.. doyoudecidewhatyouranimalisordoes
someonetellyou?
Jenkin:Thereisapackofcardsandyouchoosetheonethatyouaredrawnto
Sophie:ohrightIwouldchoosetheFox
Jenkin:wellIdidn’tknowbutIwasdrawntothedolphin
Sophie:oh
Jenkin:Iwillbringthemovertomorrow
Sophie:ohyespleasethatwillbegreat
Reference Jenkinhasbeenreadingaboutspiritanimalsandhewasdrawntoadolphin. Sophiewould
chooseafox.JenkinwillbringpackofcardswithspiritanimalstoSophietomorrow.
Jacky:Ithinkyouwererightyesterday.
David:Whatabout?I’mrightaboutmostthings:P
Jacky:Yeah,wholeyou;)
Document
Jacky:Abouttakingtheblameetc.
David:Okey,Iremeber.We’lltalklater?
Jacky:Withpleasure.I’llcallyouwhenIgethome.
Reference AccordingtoJacky,Daviddidtherightthingtakingtheblame.TheywilltalkwhenJackcomes
backhome.
Jill:Sobored!
Nate:Well...can’thelpyouthere
Nate:Stillatwork
Document
Jill:ughIneedtofindajob
Jill:I’vewatchedeverythingonyoutubealready
Nate:Doubtit:PI’llcallyouwhenIgetoffwork
Reference JillisboredandhaswatchedYouTube.NateisatworkandwillcallJillwhenhefinishesit.
Table10:VisualizationofoverlappingwordsbetweenthedocumentandreferencesummaryontheSAMSumdataset.
Thewordsaredispersedacrossthedocument,makingitdifficulttoextractsentence-levelsalientinformation.
Dataset Description Input/Output
CNN Newsarticleheadlinegeneration 773/58
SAMSum Messenger-likeconversationssummarization 127/23
ArXiv Researchpaperabstractgeneration 6446/166
MeetingBank Meetingtranscriptsummarization 3095/66
Table11: Datasetdescriptionandinput/outputlength.
15