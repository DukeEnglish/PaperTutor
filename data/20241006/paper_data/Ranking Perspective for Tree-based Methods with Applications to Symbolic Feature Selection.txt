Ranking Perspective for Tree-based Methods with Applications to
Symbolic Feature Selection
Hengrui Luo hrluo@rice.edu
Lawrence Berkeley National Laboratory
Berkeley, CA, 94720, USA
Department of Statistics, Rice University
Houston, TX, 77005, USA
Meng Li meng@rice.edu
Department of Statistics
Department of Statistics, Rice University
Houston, TX 77005, USA
Abstract
Tree-based methods are powerful nonparametric techniques in statistics and machine learning.
However, their effectiveness, particularly in finite-sample settings, is not fully understood. Recent
applications have revealed their surprising ability to distinguish transformations (which we call
symbolic feature selection)thatremainobscureundercurrenttheoreticalunderstanding. Thiswork
provides a finite-sample analysis of tree-based methods from a ranking perspective. We link oracle
partitionsintreemethodstoresponserankingsatlocalsplits,offeringnewinsightsintotheirfinite-
samplebehaviorinregressionandfeatureselectiontasks. Buildingonthislocalrankingperspective,
we extend our analysis in two ways: (i) We examine the global ranking performance of individual
trees and ensembles, including Classification and Regression Trees (CART) and Bayesian Additive
Regression Trees (BART), providing finite-sample oracle bounds, ranking consistency, and poste-
riorcontractionresults. (ii)Inspiredbytherankingperspective, weproposeconcordantdivergence
statistics T to evaluate symbolic feature mappings and establish their properties. Numerical ex-
0
perimentsdemonstratethecompetitiveperformanceofthesestatisticsinsymbolicfeatureselection
tasks compared to existing methods.
Keywords: Symbolic regressions, ranking models, Bayesian regression trees.
1 Introduction
1.1 Tree regressions
Tree-based methods, including CART (Breiman et al., 1987; Agarwal et al., 2022), Bayesian CART
(Chipmanetal.,1998),andtheirensemblessuchasrandomforests(Hastieetal.,2009)andBayesian
additive regression trees (BART Chipman et al. (2010)), are popular nonparametric techniques in
statisticsandmachinelearning(Breimanetal.,1987;Hastieetal.,2009). Thesetree-basedmethods
are highly effective in practice, showing competitive performance in wide-ranging tasks such as re-
gression(Grinsztajnetal.,2022;LuoandPratola,2023;Luoetal.,2024), classification(YichenZhu
and Dunson, 2023), causal inference (Hahn et al., 2020), and feature selection (Bleich et al., 2014).
Their empirical success has motivated a growing literature that aims to provide theoretical guar-
antees (Linero, 2018; Athey et al.; Ročková and van der Pas, 2020; Ronen et al., 2022). However,
much of this research relies on asymptotic analysis, and finite-sample understanding, which directly
addresses observed empirical effectiveness, remains limited.
In addition, some of their recent applications in interpretable machine learning show success
beyond the reach of current theoretic understanding. In particular, Ye et al. (2024) has proposed
the use of BART to achieve effective and scalable regression tools for symbolic regression, a rapidly
©2024HengruiLuoandMengLi.
License: CC-BY4.0,seehttps://creativecommons.org/licenses/by/4.0/.
4202
tcO
3
]TS.htam[
1v32620.0142:viXraLuo and Li
developing field that seeks to identify the nonlinear dependence between data from a given set of
mathematical expressions (Makke and Chawla, 2024). The authors show empirical evidence that
BART is able to distinguish transformations of the same active variable with a small sample size,
which is a crucial property to ensure accurate performance of a nonparametric variable selection
method in symbolic regression. The existing statistical literature has a limited scope to decipher
this property as the relevance of transformations of the same active variable is invariant from a
traditional nonparametric variable selection perspective.
Inthisarticle,weattempttooffernewinsightsfromrankingperspectiveinthecurrentpaperand
propose a new divergence based on this perspective. We address the challenges above by connecting
tree-based methods with ranking (Clémençon and Robbiano, 2015; Clémençon and Vogel, 2019),
which is an underdeveloped perspective in the literature on feature selection. This connection is
inherently within the finite-sample regime. It leads to conceptual elucidation of a class of tree-based
methods, and additionally motivates new statistics that are particularly useful for screening trans-
formations in symbolic regression. Our finite-sample connection is broadly applicable to Bayesian
and non-Bayesian tree-based methods. We also establish asymptotic theory by connecting BART
with ranking.
Following the background introduced in Section 1, the rest of the paper is organized as follows:
Section 2 introduces the notations, concept of trees, and the principal decision ratios, around which
we organized our discussions. Section 3 analyzes the oracle partition, refines per-node analysis
along a single tree and discusses the feature selection behavior when we only consider local splits on
univariate input variables. Section 4 furthers our discussions from the behavior of local splits to the
behavior of global regression using CART and BART with a focus on ranking. Section 5 introduces
a novel divergence statistics T inspired by the local splits studied. Section 6 provides experimental
0
evidence showing that our theoretical analysis and T results match the BART symbolic feature
0
selection on synthetic datasets. Section 7 concludes the paper and outlines several directions for
future research.
1.2 Tree methods for variable selections
Variable importance is crucial for identifying significant features. Tree-based methods, such as
CARTandBART,havesignificantlyadvancedvariableandfeatureselection(Linero,2018;Breiman
et al., 1987; Bleich et al., 2014). They provide robust mechanisms for assessing variable importance
and operational selection. Random forests are also widely used for this purpose through various
mechanisms. A foundational work applied random forests to microarray data for gene selection,
demonstrating the method’s accuracy in identifying relevant genes, outperforming other techniques
(Díaz-Uriarte and De Andrés, 2006).
On one hand, random forests can produce variable importance indices for variable and model
selections (Genuer et al., 2010). For example, the ranger implementation optimized random forests
for high-dimensional data and large-scale applications (Wright and Ziegler, 2017), which provides
measures of variable importance from the random forest. These measures indicate the significance
of each feature in predicting the response variable. To address biases in random forests especially
with correlated predictors (Strobl et al., 2008), instead of permuting values unconditionally, the
values are permuted conditionally based on the values of other variables.
Operational selection can be achieved during dynamic growth of the tree and involves actively
choosingvariablesbasedonspecificcriteria. Forexample, theBorutamethod(KursaandRudnicki,
2010) uses random forests for the selection of all relevant characteristics by comparing the original
attributes with the randomized counterparts, ensuring that all relevant characteristics are retained.
Ontheotherhand,BARTcapturestheuncertaintyinvariableimportance,leadingtomoreaccurate
2Ranking Perspective for Tree-based Symbolic Regressions
models, particularly in gene regulation studies (Horiguchi et al., 2021) but also induces variable
importance for selection (Bleich et al., 2014).
1.3 Tree-based rankings
Clémençon et al. (2011) explored adaptive partitioning schemes for bipartite ranking, showing how
decision trees can create adaptive partitions to improve ranking performance. These schemes dy-
namically adjust tree partitions to better capture the distribution of positive and negative classes.
Adaptivepartitioningusesdecisiontreestorefinepartitionsbasedonrankingperformancefeedback.
Their TreeRank method adapts the tree structure to split nodes to improve the AUC of ROC. It
is useful for complex data distributions that require flexible partitioning. Furthermore, TreeRank
Tournamentalgorithm(ClémençonandRobbiano,2015)enhancesthisbyintegratingmultipletrees,
stabilizing ranking performance, and extends its capability in feature selection.
Clémençon and Vayatis (2009) examined partitioning rules for bipartite ranking, highlighting
the role of decision tree methods in creating effective partitions to approximate the optimal ROC
curve. They showed that tree-based partitioning rules could be optimized to improve ranking by
focusing on informative splits that maximize class separation. CART variants and other decision
tree methods are essential for statistical ranking problems, including bipartite ranking (Menon and
Williamson, 2016; Uematsu and Lee, 2017).
Beyond bipartite ranking, a scoring function can be estimated from leaf nodes of a CART or
other regression model (Cossock and Zhang, 2006). When an instance is classified into a leaf node,
the score of that leaf node serves as the estimated score for the instance. This approach transforms
the tree model into a scoring function that can be used for ranking purposes, which we will revisit
in Section 4.
2 Oracle Partitions
2.1 Notation
We consider the regression problem and the following data generating model:
y = f(x )+ϵ , i = 1,2,··· ,N, (1)
i i i
ϵ ∼ N(0,σ2)
i ϵ
z = (θ x ,··· ,θ x ) ∈ Rq, (2)
i 1 i q i
with continuous covariates variables x ∈ Rd and their transformed symbolic features z ∈ Rq under
i i
a collection of feature mappings θ : Rd → R. This collection of feature mappings θ ,··· ,θ
• 1 q
is usually constructed and selected by symbolic regressions (e.g., sin,cos,exp), to approximate
continuous responses y ∈ R. When q = d and θ = π (i.e., projection onto the i-th coordinate)
i i i
(1) reduces to the usual regression setting of y = f(x )+ϵ . In other words, the feature mappings
i i i
θ ,··· ,θ allow us to consider a more general regression setting by sending the data matrix into
1 q
the following feature matrix
     
x θ x , ··· ,θ x z
1 1 1 q N 1
. . . .
 . .  ∈ RN×d (cid:55)→  . . . .  =  . .  ∈ RN×q.
     
x θ x , ··· ,θ x z
N 1 N q N N
We shall distinguish x and z by referring to x as inputs and z as features or predictors. The
i i i i
population counterpart of the model above omits the index i in the notation when we do not need
to refer to each sample.
3Luo and Li
:Binary O then unitary O operations.
b u
step 1
*
x+x x×x x+x x×x x+x x×x
1 2 1 2 1 1 1 1 2 2 2 2
step 2
Id(x 1+x 2) (x 1+x 2)3 Id(x 1×x 2) (x 1×x 2)3 Id(2x1) (2x1)3 Id(x2 1) (x 12)3 Id(2x2) (2x2)3 Id(x2 2) (x 22)3
:Unitary O then binary O operations.
u b
step 1
*
Id(x) x3 Id(x) x3
step 2 1 1 2 2
Id(x)+Id(x ) Id(x)×Id(x ) x3+Id(x ) x3×Id(x ) Id(x)+Id(x ) Id(x)×Id(x ) x3+Id(x ) x3×Id(x )
Id(x1)+Id(x1 ) Id(x1)×Id(x1 ) x31+Id(x1 ) x31×Id(x1 ) Id(x2)+Id(x1 ) Id(x2)×Id(x1 ) x32+Id(x1 ) x32×Id(x1 )
Id(x1)+x3 2 Id(x1)×x3 2 x31+x3 2 x31×x3 2 Id(x2)+x3 2 Id(x2)×x3 2 x32+x3 2 x32×x3 2
Id(x1)+x31 Id(x1)×x31 x31+x31 x31×x31 Id(x2)+x31 Id(x2)×x31 x32+x31 x32×x31
1 2 1 2 1 2 1 2 2 2 2 2 2 2 2 2
Figure 1: We illustrate 2-layer symbolic regression with O = {id,x3} and O = {+,×}. We also
u b
follow the notation convention O(2) and O(2) for the architectures specified in Ye et al. (2024). We
displayed all of the possible featuA reu s in a 2-A sbtep symbolic composition using tree structure, showing
the rapidly increasing number q of features, namely transformed symbolic feature z’s.
Example 1 (Symbolic feature mappings) Consider d = 2 and input variable x = (x ,x ) ∈ R2
1 2
and an operator set O = {+,×}, in one step composition there are q = 2 × 2 × 2 = 8 features
b
(i.e., combinations {x ,x }⊗{+,×}⊗{x ,x }) z = 2x ,z = x +x ,··· ,z = x2 (but only 6
1 2 1 2 1 1 2 1 2 8 2
distinct features as shown in Figure 1); suppose the next step we have an operator set O = {id,x3}
u
then in one step composition there are q = 6 × 2 = 12 different features. Even with only two
composition steps, the dimensionality q of features grows quickly (Bryant, 1992). Due to repetitive
use of this composition construction in symbolic regression, q ≫ d and these q features are usually
highly correlated, making feature (pre-)selection necessary in symbolic regressions.
In Ye et al. (2024), BART is applied to perform feature selection among all these symbolic
features (which are highly correlated, and identical if noiseless composition is assumed) and achieve
good performance from the corresponding symbolic regression model using the chosen features. If we
(2)
take different orders of compositions (i.e., taking unitary operation first, denoted as O ; or taking
Au
(2)
binary operation first, denoted as O ), the number of features will change.
A
b
Although we assume additive noises ϵ such that Eϵ = 0 and Var(ϵ ) = σ2 > 0, the ranking
i i i ϵ
perspective studied in this article also covers the noise-free setting with σ2 = 0. We denote the
ϵ
pairs of observations as
X(N) = {x ,x ,··· ,x } = {(x ,··· ,x ),(x ,··· ,x ),··· ,(x ,··· ,x )},
1 2 N 1,1 1,d 2,1 2,d N,1 N,d
Z(N) = {z ,z ,··· ,z }, Y(N) = {y ,y ,··· ,y }.
1 2 N 1 2 N
4Ranking Perspective for Tree-based Symbolic Regressions
We use an enclosing round bracket when the observation pairs are sorted according to the ranks of
response y’s, i.e.,
(cid:0) z ,y (cid:1) ,(cid:0) z ,y (cid:1) ,··· ,(cid:0) z ,y (cid:1) , (3)
(1) (1) (2) (2) (N) (N)
or (cid:0) x ,y (cid:1) ,(cid:0) x ,y (cid:1) ,··· ,(cid:0) x ,y (cid:1) , (4)
(1) (1) (2) (2) (N) (N)
wherey < y < ··· < y ;herex = (cid:0) x ,x ,··· ,x (cid:1)whenwrittenintheformofeach
(1) (2) (N) (i) (i),1 (i),2 (i),d
coordinate, and likewise z = (cid:0) z ,z ,··· ,z (cid:1). We assume that there are no ties among
(i) (i),1 (i),2 (i),q
the responses throughout the paper.
2.2 Recursive partition in tree-based models
A binary tree divides the predictor space and consists of internal nodes and leaf nodes. The leaf
nodesformapartitionofthepredictorspace; foratreeT consistingofK leafnodes, theconditional
mean of the response E(y | x) = g(x) = (cid:80)K µ 1(x ∈ P |T), where µ ∈ R is the mean value
i=1 i i i
associated with the ith leaf node P for i = 1,...,K.
i
To recursively construct binary trees (Quinlan, 1986; Breiman et al., 1987; Hastie et al., 2009),
we can split at each node by splitting coordinates and splitting values (a.k.a., cutoff values); in
particular, for an internal node η with split parameters (C ,k ), we divide η into the left and right
η η
child nodes x ≤ C and x > C , respectively, depending on whether the k -th coordinate is
•,kη η •,kη η η
greater than the splitting value C . With respect to every node η ∈ T in a tree structure, we adopt
η
the notation i ∈ η to indicate that the i-th sample (x ,y ) or (z ,y ) is assigned to the node η.
i i i i
The recursive partition induced by a tree method is dictated by interpretable decision rules
including the choice of splitting coordinates k and splitting values C . For any internal node η,
η η
let n be its size (i.e., the number of samples that fall into this node) and {y : 1 ≤ i ≤ n } be the
η i η
responses contained in η. CART selects (C ,k ) by
η η
(C ,k ) = argmin(cid:88) (y −µC,k)21(x ≤ C)+(cid:88) (y −µC,k)21(x > C), (5)
η η i L i,k i R i,k
C,k
i∈η i∈η
where(µC,k,µC,k)arethesampleaverageofresponseswithintheleftandrightnodesthatminimizes
L R
in-node sum of squares, i.e.,
(cid:80) (cid:80)
y ·1(x ≤ C) y ·1(x > C)
µC,k := i∈η i i,k , µC,k := i∈η i i,k . (6)
L (cid:80) 1(x ≤ C) R (cid:80) 1(x > C)
i∈η i,k i∈η i,k
Cycling through all internal nodes induces a recursive partition defined by the leaf nodes. In the
existing tree literature (Quinlan, 1986; Breiman et al., 1987; Hastie et al., 2009), this partition is
viewed as one of the predictor space, which indeed defines basis functions. These basis functions
are used to approximate the regression function f in (1).
We take a slightly different perspective to view this partition as one that is induced by the
observed samples, noting that the splitting action separates all observations D(N) into two groups
corresponding to the left and right children nodes.
Conceptually, the basis function view is at the population level operated on the support of
f, while our analysis is inherently finite-sample and it operated on the indices of observations
[N] = {1,...,N}. In particular, each internal node η collects a subset of observations and can
be characterized by a subset of [N], i.e., η ⊂ [N], and the left and right child nodes of η lead to
a finer partition of η by (cid:8) i : x ≤ C ,i ∈ η ⊂ [N](cid:9), and (cid:8) i : x > C ,i ∈ η ⊂ [N](cid:9). As such,
i,kη η i,kη η
the recursive partition of the prediction space, once realized by finite samples, leads to a recursive
5Luo and Li
partitionof[N], whichsubsequentlyyieldsarecursivepartitionofX(N) andY(N). Asweshallshow
later, this finite-sample perspective connects tree methods with the ranking of Y(N).
In the symbolic regression setting, a tree method sees the transformed features Z(N) and Y(N),
and would proceed using the transformed symbolic features Z(N) instead of X(N) as predictor. We
reiterate that these transformed features are of rapidly growing dimensionality and present high
correlations, which require feature (pre-)selection as a necessary step in symbolic regression. Then
fitting a regression tree using the transformed symbolic features, the criteria (6) contains a split
along the feature space Z(N):
(cid:80) (cid:80)
y ·1(z ≤ C) y ·1(θ x ≤ C)
µC,k = i∈η i i,k = i∈η i k i , (7)
L (cid:80) 1(z ≤ C) (cid:80) 1(θ x ≤ C)
i∈η i,k i∈η k i
(cid:80) (cid:80)
y ·1(z > C) y ·1(θ x ≤ C)
µC,k = i∈η i i,k = i∈η i k i . (8)
R (cid:80) 1(z > C) (cid:80) 1(θ x ≤ C)
i∈η i,k i∈η k i
Here, we omit the subscript and it is clear that C = C associated with the node η. The first
η
key observation from (7) and (8) is that by definition (2) the k-th coordinate of z is obtained by
i
applying the feature mapping θ to x . Therefore, the split along the feature space Z(N) can be
k i
attained by splitting along the original space X(N). For example, if θ = π , which projects onto
k 1
the first coordinate, then splitting on the k-th coordinate of z can be equivalently obtained from
i
splitting along x . The second key observation is that regardless of the growing dimensionality of
i,1
q, the estimators (7) and (8) only rely on one splitting coordinate, making it particularly suitable
for the symbolic regession scenario where q grows rapidly, as shown in Example 1.
In the rest of this paper, we focus on this feature set Z(N), which includes the original data
X(N) as a special case where the transformation sets map x to each of its coordinates such that
i
z = x .
i i
2.3 Local splits and principal decision ratio
In the sequel, we first analyze each internal node η and suppose η contains n observations indexed
η
by {i(η) : i = 1,...,n } using the original serial indices. For simplicity, we drop the dependence
η
on η in the node-specific notation with the understanding that our analysis is generally applicable
to any internal node η. For example, we will use (n,C,k) for (n ,C ,k ), respectively, and with
η η η
a slight abuse of notation, use [n] = {1,...,n} to denote the indices of the enclosed observations
{i(η) : i = 1,...,n }.
η
At the stage of deciding the split (consisting of the splitting coordinate and splitting value), for
any two pairs of decision rules (k ,C ) and (k ,C ), we introduce the principal decision ratio:
1 1 2 2
exp(cid:0) −(cid:80)n (y −µ1)21(z ≤ C )−(cid:80)n (y −µ1)21(z > C )(cid:1)
τ = i=1 i L i,k1 1 i=1 i R i,k1 1 , (9)
exp(cid:0) −(cid:80)n (y −µ2)21(z ≤ C )−(cid:80)n (y −µ2)21(z > C )(cid:1)
i=1 i L i,k2 2 i=1 i R i,k2 2
where µ1 = µC1,k1 (and µ1 = µC1,k1), µ2 = µC2,k2 (and µ2 = µC2,k2) as specified in (6). This
L L R R L L R R
means that we select splitting values from one of the observed input features z ’s, following the
i
common practice in Breiman et al. (1987).
The numerator and denominator of (9) exponentiate the loss in Equation (5); this formulation
assumesaGaussianlikelihoodwithuniterrorstandarddeviation,whichiscommonlyusedinmodel-
based tree methods such as Bayesian CART (Chipman et al., 1998, 2002). Consequently, τ may
represent a likelihood ratio. Based on τ, the splitting coordinate k and splitting value C that
minimize the loss function (5) would have the highest τ value compared to any other splitting rules.
6Ranking Perspective for Tree-based Symbolic Regressions
The principal decision ratio in (9) also encodes key information for Bayesian trees to make
splitting decisions. In particular, the fitting procedure of Bayesian trees typically relies on an
adapted form of τ. For Bayesian CART (Chipman et al., 1998) and its extension to sum-of-tree
counterpart BART (Chipman et al., 2010), trees are sampled using a stochastic search via Markov
chainMonte-CarlothatutilizesaMetropolis-Hastingsratiobetweentheoriginaltreeandaproposed
tree. Andthisratioreducestoτ whencomparingtwosplittingdecisions. Similarly, Bayesiandyadic
trees (Li and Ma, 2021) would draw posterior samples of partitioning directions with probabilities
determinedbyallpairsofτ whenthesplittingvaluesarerestrictedtohalvesofeachcoordinate. The
exact posterior sampling procedures in these Bayesian tree methods also include inference on other
parameters, such as the mean and stand derivation parameters at leaf nodes, and rules to prune a
complete tree; for stochastic search, the Metropolis-Hastings ratio also involves other operations in
addition to growing a tree via splitting internal nodes, such as the swapping operation.
Central to our analysis of tree methods is the principal decision ratio τ, and its interplay with
ranking and feature selection. As explained, the principal decision ratio (9) is closely related to
the loss function and Metropolis-Hastings ratio in a per-node strategy, which is advocated by An-
dronescu and Brodie (2009). Also analyzing a per-node strategy, Luo and Daniels (2021) observed
that splits using relevant predictors yield higher Metropolis-Hastings ratios, indicating a preference
forthesepredictorsinthesplittingprocess. Thisoccurrenceofτ, whichweaimtosubstantiatewith
our analysis, exemplifies the intricate connection between feature selection and splitting decisions.
Therefore,analyzingτ providesinsightintohowtree-basedmethodsbehaveatlocalsplits,andthese
per-node analyses findings will later be extended to the entire decision tree and tree ensembles.
3 Local Ranking at Local Splits
In this section, we will establish the connection between the response rankings (i.e., rankings of
Y(N)) and the optimal principal decision ratios in a single tree. This leads to two observations
which we summarize here at a high level:
1. The optimal partition of samples into children nodes depends only on the rankings of y, which
we called the oracle partition. This oracle partition is solely determined by the ranks (or
orders) of response y, in the sequential minimizations of the L loss function (5) in splitting
2
parameters C,k (where C can only be selected from samples), when the sizes of the nodes of
the children are fixed. When children node sizes are not fixed, the minimization of L loss
2
involves the relative magnitude of the responses y.
2. However, the partitions in tree-based methods are determined by the actual configuration of
predictors z’s. This means that even if the oracle partition is completely determined by the
responses y, we may only attain sub-optimal partitions in the form of {(z ,y ) | z ≤ C} and
i i i,k
{(z ,y ) | z > C} for some k and C.
i i i,k
We rewrite the loss function in (5) in a more general form as a function of partitions P ,P
1 2
(cid:88) (cid:88)
L(P ,P ) = SS(P )+SS(P ) =: (y −µ∗)2+ (y −µ∗)2, (10)
1 2 1 2 i 1 i 2
i:yi∈P1 i:yi∈P2
where(P ,P )isa2-partitionof{y ,...,y },andtheinternalsumofsquaresSS(P) := (cid:80) (y −
1 2 1 n i:yi∈P i
µ(P))2 with µ(P) = 1 (cid:80) y . Then the loss function in (5) is L(P ,P ) when (P ,P ) are
|P| i:yi∈P i 1 2 1 2
constrained to take the form P = {y : z ≤ C} and P = {y : z > C}, and µ∗,µ∗ are
1 i i,k 2 i i,k 1 2
the corresponding means of y ’s associated with these two partitions. This function L(P ,P ) is
i 1 2
invariant to the ordering of (P ,P ), and its optimal solution is expected to be unique only up to
1 2
this ordering.
7Luo and Li
3.1 Optimal 2-partition
We next study the optimal partition without imposing the aforementioned predictor-dependent
constraints on the partition induced by a tree method. This study is applicable to any selected
coordinate k ∈ [q]. We begin with an illustrative example that contains 5 observations where we
assumethedimensionalityq ofZ(N) tobe1fortheeaseofvisualizationinFigure2buttheargument
works regardless of q.
Example 2 (5-sample example oracle 2-partition with fixed partition sizes) Assume n = 5 and
consider 2-partitions (P ,P ) with sizes (2,3). We proceed with the explicit computation of the loss
1 2
function (10). First consider the 2-partition with P = {y ,y } and P = {y ,y ,y }:
1 (1) (3) 2 (2) (4) (5)
(cid:18) y (1)+y (3)(cid:19)2 (cid:18) y (1)+y (3)(cid:19)2 1 (cid:0) (cid:1)2
SS(P ) = y − + y − = y −y ,
1 (1) 2 (3) 2 2 (1) (3)
(cid:18) y +y +y (cid:19)2 (cid:18) y +y +y (cid:19)2 (cid:18) y +y +y (cid:19)2
(2) (4) (5) (2) (4) (5) (2) (4) (5)
SS(P ) = y − + y − + y −
2 (2) 3 (4) 3 (5) 3
(cid:18)2y −y −y (cid:19)2 (cid:18)−y +2y −y (cid:19)2 (cid:18)−y −y +2y (cid:19)2
(2) (4) (5) (2) (4) (5) (2) (4) (5)
= + +
3 3 3
1 (cid:16) (cid:17)
= 6y2 +6y2 +6y2 −6y y −6y y −6y y .
9 (2) (4) (5) (2) (4) (4) (5) (2) (5)
Similarly, for the 2-partition with P = {y ,y } and P = {y ,y ,y }, we have
1 (1) (2) 2 (3) (4) (5)
SS(P ) = 1 (cid:0) y −y (cid:1)2 , SS(P ) = 1 (cid:16) 6y2 +6y2 +6y2 −6y y −6y y −6y y (cid:17) .
1 2 (1) (2) 2 9 (3) (4) (5) (3) (4) (4) (5) (3) (5)
Taking the difference of these two sets of expressions (note that y < y < y < y < y )
(1) (2) (3) (4) (5)
shows that the change of SS(P ) and SS(P ), respectively denoted by ∆ and ∆ , are both greater
1 2 1 2
than zero:
1 (cid:0) (cid:1)2 1 (cid:0) (cid:1)2 1 (cid:0) (cid:1)(cid:0) (cid:1)
∆ = y −y − y −y = y −y y −y +y −y > 0,
1 2 (1) (3) 2 (1) (2) 2 (2) (3) (1) (3) (1) (2)
1 (cid:16) (cid:17)
∆ = 6y2 +6y2 +6y2 −6y y −6y y −6y y
2 9 (2) (4) (5) (2) (4) (4) (5) (2) (5)
1 (cid:16) (cid:17)
− 6y2 +6y2 +6y2 −6y y −6y y −6y y
9 (3) (4) (5) (3) (4) (4) (5) (3) (5)
= 1 (cid:16) 6y2 −6y2 −6(cid:0) y −y (cid:1) y −6(cid:0) y −y (cid:1) y (cid:17)
9 (2) (3) (2) (3) (4) (2) (3) (5)
2 (cid:0) (cid:1)(cid:0) (cid:1)
= y −y y +y −y −y > 0.
3 (2) (3) (2) (3) (4) (5)
Therefore, the second 2-partition exchanging y and y reduces the total sum L(P ,P ) in (10).
(2) (3) 1 2
Using this argument repeatedly, we will arrive at the conclusion that P = {y ,y } and P =
1 (1) (2) 2
{y ,y ,y } form a (locally) optimal 2-partition with sizes (2, 3). Similarly, another (local)
(3) (4) (5)
optimal 2-partition with sizes (2, 3) is P = {y ,y } and P = {y ,y ,y }. From their
1 (4) (5) 2 (1) (2) (3)
expressions, we can see that only the ranks and the magnitude of the difference between responses
affect the ∆ and ∆ .
1 2
Like expressions (7) and (8), the oracle partition depends only on the response values y’s. The
second column in Figure 2 shows two possible configurations based on the two datasets shown in
8Ranking Perspective for Tree-based Symbolic Regressions
y
(x,y)
1 1
(a) (x 5,y 5)
(x,y)
2 2 (x,y)
4 4
(x,y)
3 3
(a)
x x
y
(x,y)
1 1
(b)
(x 2,y 2) (x 5,y 5)
(x,y)
4 4
(x,y)
3 3
x x
Figure 2: A depth 2 tree with 5 observations showing two possible oracle partitions in Lemma 1.
In the first column, we present the raw (x ,y ) pair of dataset; In the second column, we present
i i
the oracle partition using red and blue colors, and the support of indicator functions on the x-axis.
The horizontal solid lines represent the group mean of y values (as prediction value as well); the
vertical dashed lines represent the point-to-mean distances.
In the third column, we illustrate the loss function (10) The minimum in row (a) is attained by
{y ,y } = {y ,y } and {y ,y ,y } = {y ,y ,y }. The minimum in orw (b) is attained
(4) (5) 1 5 (1) (2) (3) 2 3 4
by {y ,y ,y } = {y ,y ,y } and {y ,y } = {y ,y }. We color the dots by the actual loss
(3) (4) (5) 1 2 5 (1) (2) 3 4
function values, and annotate the ordered statistics near each dot.
the first column of the same figure. In both rows (a) and (b), the x-axis is the first element in
the size 2 partition component P , y-axis is the second element in size 2 partition component P ,
1 1
therefore the figure is symmetric. The minimum is attained by P∗ = {y ,y } = {y ,y } and
1 (1) (2) 3 4
P∗ = {y ,y ,y } = {y ,y ,y }, or P∗∗ = {y ,y } = {y ,y } and P∗∗ = {y ,y ,y } =
2 (3) (4) (5) 1 2 5 1 (4) (5) 2 5 2 (1) (2) (3)
{y ,y ,y }.
3 4 1
Example 2 serves as a representative case for our result applicable to general sample sizes, as
formalized in the lemma below.
Lemma 1 (Oracle 2-partition with fixed sizes) For a 2-partition of n elements y < y < ··· <
(1) (2)
y into components of size i and n−i, we assume that n > 4,min(n−i,i) ≥ 2 to ensure variances
(n)
are defined. Then the following partitions
• P∗ = {y ,y ,··· ,y } and P∗ = {y ,y ,··· ,y } OR,
1 (1) (2) (i) 2 (i+1) (i+2) (n)
• P∗∗ = {y ,y ,··· ,y } and P∗∗ = {y ,y ,··· ,y }
1 (1) (2) (n−i) 2 (n−i+1) (i+2) (n)
are the only 2-partitions of size i and n−i that minimize (10).
Proof See Appendix A.
9Luo and Li
Remark 2 Thelemma1statesthatthelossfunctiononlyhastwolocalminimaattainedby(P∗,P∗)
1 2
(correspondingtosize(i,n−i))or(P∗∗,P∗∗). Comparingthelossfunctionvalue (10)at(P∗,P∗)or
1 2 1 2
(P∗∗,P∗∗)givestheglobalminimumofthelossfunction, hencedeterminingthesplitmaximizing (9).
1 2
This optimal 2-partition, however, may not be attained by those 2-partitions induced by splitting on
z values only once. Lemma 1 is a refined version of optimal splits in trees with continuous outcome
under L2 loss, which was studied as a grouping problem instead of a ranking problem in Fisher
(1958).
We next present a simple example to demonstrate Lemma 1 and the induced partitions on the
response and input coordinates.
Example 3 (5-sample oracle 2-partition with varying partition sizes) Now we consider the same
dataset as in Example 2 but we do not fix the partition sizes to (2,3) any more.
In the third column of Figure 2, we show the oracle 2-partitions on the response y and the
corresponding landscape of the loss function (10). In panel (a), we put (x ,y ) = (0.1,5), (x ,y ) =
1 1 2 2
(0.3,2.1), (x ,y ) = (0.5,1), (x ,y ) = (0.6,2), (x ,x ) = (0.9,4), and the oracle partition that
3 3 4 4 5 5
minimizes the total sum is {y ,y ,y } of size 3 and {y ,y } of size 2. In panel (b), we
(1) (2) (3) (4) (5)
put (x ,y ) = (0.25,3.9), but the rest of the pairs remain the same, and the oracle partition that
2 2
minimizes the total sum is {y ,y } of size 2 and {y ,y ,y } of size 3. It turns out that
(1) (2) (3) (4) (5)
the oracle 2-partition of size (2,3) is indeed the configuration that minimizes the (10), compared to
oracle 2-partitions of size (1,4).
Note that if we observe (z ,y ),··· ,(z ,y ) instead: since the oracle partition for the loss
1 1 5 5
function depends only on the response y’s, this does not affect our oracle partition above. However,
it is clear from Figure 2 that each of the two candidate optimal partitions (in the sense that they
minimize (10)) P∗,P∗ or P∗∗,P∗∗ creates 3 partition components on the X domain, which cannot
1 2 1 2
beattainedbysplittingonxvaluesonlyonce. Ifwechoosethetransformedsymbolicfeaturez = x2,
then the oracle 2-partition for y can be realized by partitioning on Z domain.
Remark 3 Applying Lemma 1 repeatedly leads to solutions to finding optimal 2-partitions with
varying sizes. In particular, for y < y < ··· < y with n > 4, the loss function (10) is
(1) (2) (n)
minimized by solving the following problem:
i n
(cid:88) (cid:88)
min (y −µ∗)2+ (y −µ∗)2 = min L(P ,P ), (11)
(j) 1 (j) 2 1 2
i∈{1,2,···,n−1} i∈{1,2,···,n}
j=1 j=i+1
and form the associated partitions. There are still two possible minimizers for (11) as stated in
Lemma 1. From the angle of grouping (Fisher, 1958) or analysis of variance (ANOVA), the prob-
lem of (11) can be considered as finding a division of Y into two groups such that the in-group
variance is as small as possible. In other words, the resulting minimizer would produce “most in-
group homogeneous” group partitions. The optimal 2-partition for components with varying sizes
depends not only on the ranking information, as it would for the oracle 2-partition with fixed sizes,
but also on the distribution information of the responses y.
However,mosttreemodelscreatepartitionsonx’s(e.g.,whichisaspecialcasewithq = 1,z ∈ R)
but not y’s domain for prediction purposes, so how well we can predict depends on how “similar”
(or “concordant”) the response rankings and input rankings are. For example, if the inputs x’s and
the responses y’s have the same rankings, then the oracle 2-partition on response can be realized
10Ranking Perspective for Tree-based Symbolic Regressions
by corresponding oracle 2-partition on x’s. The following corollary 4, which is straightforward from
Lemma 1, gives a sufficient and necessary condition to attain oracle 2-partitions when splitting only
on x.
Corollary 4 (Oracle 2-partition with fixed sizes with univariate x) For a 2-partition of n elements
y < y < ··· < y intocomponentsoffixedsizeiandn−i, weassumethatn > 4,min(n−i,i) >
(1) (2) (n)
2 and assume that there exists some C such that
• P∗ = {y < y < ··· < y } = {y′ | the pair (x,y) s.t. x ≤ C}, and P∗ = {y <
1 (1) (2) (i) 2 (i+1)
y < ··· < y } = {y′ | the pair (x,y) s.t. x > C}.
(i+2) (n)
• P∗∗ = {y < y < ··· < y } = {y′ | the pair (x,y) s.t. x ≤ C}, and P∗∗ = {y <
1 (1) (2) (n−i) 2 (n−i+1)
y < ··· < y } = {y′ | the pair (x,y) s.t. x > C}.
(n−i+2) (n)
Then these are the only 2-partitions of size i and n−i that minimize (10).
This means that a sufficient condition for us to attain optimal partition by splitting once on x
is that the ranks of x and y are the same or linearly related. In fact, when the input and response
rankings are the same, the CART loss function described by Scornet et al. (2015) formula (2) or
Hastie et al. (2009) Chapter 9, takes the following form. When input and response rankings are the
same and x ≤ C < x the following loss function value remains the same:
(i) (i+1)
 
i n
(cid:88) (cid:88)
min  (y (j)−µ∗ 1)2+ (y (j)−µ∗ 2)2  = (10) = (11).
i∈{1,2,···,n}
j=1 j=i+1
In this case, such x with the same ranking of y will be the most likely splitting coordinate. We next
provide two such examples using monotonic transformation and interpolators, respectively.
Example 4 (Monotonic transformation) Suppose that d = 1 and noiseless y(x) = f(x) is a mono-
tonic function of univariate x ∈ R, then choosing any observation x ∈ X(N) as a splitting value
will give us an oracle 2-partition corresponding to the fixed sizes. This is because under monotonic
transformation θ = f, splitting on any observed x is equivalent to an oracle 2-partition on y. By
Lemma 1, there cannot be any other 2-partition of the same size on y that gives us a strictly larger
principal decision ratio.
Furthermore, if we assume that z = θ x for d = 1 and another θ that is monotonic as well, then
1 1
we can come to the expected conclusion that y(x) = f(θ x) always induces the optimal 2-partition,
1
since θ◦θ = f◦θ is again monotonic. This example shows that when the true underlying function
1 1
is monotonic, it will induce an oracle partition on X and Y domains simultaneously.
Example 5 (Interpolator) Suppose that d = 1, then for any pairs of (x ,y ) for i = 1,...,n, we
i i
can construct an n-degree polynomial interpolator, denoted by g as a mapping, such that y = g(x )
i i
for all i. This transformed symbolic feature z = g(x) maintains the same ranking of the response
and thus attains the optimal oracle 2-partition. Then, by Corollary 4, using the output of such an
interpolator as input will always maximize the (10) and hence the principal decision ratio. Even
without exact interpolation, when n ≪ q, it is easy to observe over-fitting, which means we can use
the transformed features to construct such an interpolator mapping. This example shows a major
difference between finite-sample and asymptotic scenarios.
Therefore, a decision tree tends to split on a feature for which the response is a monotonic
transformation, and likewise, an interpolator would be the most likely splitting feature if seen by
11Luo and Li
the tree. This perfectly explains the fact that decision tree is scale-invariant in the sense that
its prediction remains unchanged if we multiply the input by a scalar as noted in Bleich et al.
(2014). In fact, this shows a stronger result that it is multi-way scale-invariant. Namely, if we
simply multiply possibly different but non-zero scalars to each coordinate of input features, the
principal decision ratio still remains unchanged (since multiplying a non-zero scalar is a monotonic
transformationandpreservestheranks). Whiletheinabilityofdecisiontreestodistinguishbetween
monotonic transformations is well known in the literature (Bleich et al., 2014), interpolators are
less discussed but an interesting example that shows there always exists a mapping in finite sample
sizes to mislead decision trees. Such data-dependent mappings are typically ruled out in the pool of
symbolic expressions considered in symbolic selection, making it more robust to regular regressions.
We next analyze the principal decision ratios when selecting between a more general class of
transforms that generates symbolic features z from x, other than the two examples above.
3.2 Piece-wise monotonic transforms
We now analyze the splitting behavior of the decision trees when selecting features z generated by
transformations. Such selection is crucial in symbolic regressions (See Example 1). In particular,
we consider piece-wise monotonic transforms, which are widely used to generate symbolic features
and are also of interest due to their flexibility, as they can approximate well a sufficiently large class
of transformations of interest (Newman et al., 1972).
Throughout this section, we consider two generic features z = θ x and z = θ x for an
i,k1 1 i i,k2 2 i
arbitrary i (in shorthand, z = θx ), where both θ and θ are piece-wise monotonic in the form of
k k 1 2
θx , where univariate mappings θ transforms the k-th coordinate of x.
i,k
A piece-wise monotonic transform θ defined on R is characterized by a partition, which consists
of finitely many disjoint intervals (i.e., monotonic intervals) on each θ is monotonic. Such partitions
are not unique as one can always divide a subset while maintaining the strict monotonicity of θ
on the finer partition. Unless stated otherwise, we always choose the partition with the smallest
cardinality.
Weaimtocharacterizetheprincipaldecisionratioattwopairsofsplittingvaluesandcoordinates
(C ,k ) and (C ,k ), i.e., the ratio of
1 1 2 2
n n
(cid:88)(cid:16)
y
−µC1,k1(cid:17)2
1(z ≤ C
)+(cid:88)(cid:16)
y
−µC1,k1(cid:17)2
1(z > C ) (12)
i L i,k1 1 i R i,k1 1
i=1 i=1
and
n n
(cid:88)(cid:16)
y
−µC2,k2(cid:17)2
1(z ≤ C
)+(cid:88)(cid:16)
y
−µC2,k2(cid:17)2
1(z > C ). (13)
i L i,k2 2 i R i,k2 2
i=1 i=1
The following definitions and simple properties of θ and θ are useful.
1 2
Definition 5 (Refined monotonic intervals) Consider two piece-wise monotonic transforms θ and
1
θ mapping from R onto R, with monotonic intervals I and I on R respectively. We define
2 1 2
the refined monotonic intervals for the transformation pair (θ ,θ ) to be the collection of intervals
1 2
I := {I = I ∩I | I ∈ I ,I ∈ I }.
1∩2 1 2 1 1 2 2
The refined monotonic intervals I is a new partition of the x-domain induced by (θ ,θ ). On
1∩2 1 2
each refined monotonic interval I ∈ I , the two piece-wise monotonic transforms θ and θ are
1∩2 1 2
both monotonic; that is, the restricted transforms θ | and θ | are both monotonic. The next
1 I 2 I
result follows directly from Definition 5 but introduces pre-images of the restricted transforms that
are useful to study the principal decision ratio.
12Ranking Perspective for Tree-based Symbolic Regressions
Corollary 6 On each refined monotonic interval I ∈ I and for any value C ∈ R, the piece-wise
1∩2
monotonic transform θ and θ can have 0 or 1 pre-image. That means, there exists 0 or 1 value
1 2
t ∈ I such that the restricted transform θ | (t) = C and θ | (t) = C.
1 I 2 I
We next study the principal decision ratio of these two transformations at arbitrary splitting
values C and C . Although θ and θ are not necessarily globally invertible, they are invertible on
1 2 1 2
each refined monotonic interval I ∈ I . By definition (2), we note that z ≤ C can be written
1∩2 i,k1 1
as θ x ≤ C , which can be reduced to x ≤ θ−1C on each I ∈ I where θ−1C is well-defined.
1 i,k 1 i,k 1 1 1∩2 1 1
Using Corollary 6, for a refined interval I ∈ I we can link the relative magnitude of principal
1∩2
decision ratio τ to the behavior of covariate x instead of z. The following result Proposition 7 is
consistent with the discussion that splitting will decrease the Bayes risk in Section 9.3 and Theorem
9.5 of Breiman et al. (1987).
Proposition 7 For any splitting value C ∈ R, the means µC,k,µC,k defined for z as in (8), the
L R
parent node mean µ♯, and the k-th coordinate z , we have
k
n n
(cid:88) (y −µC,k)21(z ≤ C)+(cid:88) (y −µC,k)21(z > C) =
i L i,k i R i,k
i=1 i=1
n n n
(cid:88) (y −µC,k)21(z ≤ C)+(cid:88) (y −µC,k)21(z > C) ≤ (cid:88) (y −µ♯)2,
(i) L (i),k (i) R (i),k (i)
i=1 i=1 i=1
where the equality holds if and only if µC,k = µC,k = µ♯.
L R
Proof See Appendix B.
In the spirit of Theorem 9.5 of Breiman et al. (1987), we can see from the above argument that a
feature mapping that induces more splits will be favored in the sense that it increases the principal
decision ratio, hence the likelihood for the splitted children nodes. This helps compare two trans-
formations in terms of the existence of pre-images, or to “contrast” these two transformations at a
finer resolution level. Below, we start with an illustrative example, followed by its generalization.
Example 6 Let us consider the following three cases with fixed θ (x) = x+1.2, θ (x) = −4x2+4x,
1 2
withI = {[0,1]},I = {[0,1/2],[1/2,1]},andtherefinedmonotonicintervalsI = {[0,1/2],[1/2,1]}.
1 2 1∩2
And we consider expressions in (12), (13) along with z = θ x and z = θ x , as illustrated by
i,k1 1 i i,k2 2 i
Figure 3. We can consider the following cases:
Let us consider I = [0,1/2] first, if we fixed C ∈ (−∞,1.2)∪(2.2,∞) and choose C ∈ (−∞,0)∪
1 2
(1,∞) as shown in the (a) in Figure 3. We cannot differentiate between θ and θ based solely on the
1 2
principal decision ratio when splitting on inputs x ∈ I, since both θ and θ will have 0 pre-image
1 2
in I, so the corresponding splits associated with C ,C have the same chance of being selected on
1 2
this interval. However, if we choose C ∈ (1.2,1.7) or [1.7,2.2),C ∈ (−∞,0)∪(1,∞) in such a
1 2
way shown as (b) or (c), then over the I = [0,1/2] or [1/2,1] we have a higher chance of selecting
θ from analysis of expressions in (12), (13). If we fixed C ∈ (−∞,1.2)∪(2.2,∞),C ∈ [0,1] this
1 1 2
analysis remains the same and we learn that (d) in Figure 3 can differentiate θ ,θ .
1 2
Now if we choose C ∈ [1.2,2.2],C ∈ [0,1], then θ ,θ will both have 1 pre-image in I as shown
1 2 1 2
in (e) and (f) in Figure 3. Then we are back to the computation of (11).The detailed analysis will
be given in Example 7.
The observation in the above example can be summarized as the following result linking the
principal decision ratios and the refined intervals of a given univariate feature mapping.
13Luo and Li
(a) θ(*) (b) (c)
C θ(1
*)
θ1
θ(*)
θ1
θ(*)
θ1
C1
C2 CC 21 C2
C1
0 1/2 1 θ2 0 1/2 1 θ2 0 1/2 1 θ2
C2 C2 C2
[0,1/2] [1/2,1] [0,1/2] [1/2,1] [0,1/2] [1/2,1]
θ1 pre-img. 0 0 θ1 pre-img. 1 0 θ1 pre-img. 1 0
θ2 pre-img. 0 0 θ2 pre-img. 0 0 θ2 pre-img. 0 0
(d) θ(*) (e) (f)
C θ(1
*)
θ1
θ(*)
θ1
θ(*)
θ1
C1
C1
C1
C2 C2 C2
0 1/2 1 θ2 0 1/2 1 θ2 0 1/2 1 θ2
[0,1/2] [1/2,1] [0,1/2] [1/2,1] [0,1/2] [1/2,1]
θ1 pre-img. 0 0 θ1 pre-img. 1 0 θ1 pre-img. 1 0
θ2 pre-img. 1 1 θ2 pre-img. 1 1 θ2 pre-img. 1 1
Figure 3: Refined monotonic intervals I = {[0,1/2],[1/2,1]} for the θ (x) = x, θ (x) = −4x2+4x
2 1 2
shown. We use vertical black dashed lines to illustrate the refined monotonic intervals, and count
the number of pre-images for θ ,θ over each refined intervals.
1 2
Proposition 8 Consider one splitting variable x for a fixed k ∈ {1,2,··· ,d} and two piece-
k
wise strictly monotonic transformations θ and θ with refined monotonic intervals I . For any
1 2 1∩2
C ,C ∈ R and any I ∈ I , we have
1 2 1∩2
(i) When θ and θ have 0 pre-image for C and C on I, the principal decision ratio is 1 for
1 2 1 2
the transformed variates θ x and θ x over this interval I ∈ I .
1 k 2 k 1∩2
(ii) When θ and θ have different numbers of pre-images for C and C on I, the principal
1 2 1 2
decision ratio is larger for the transform with 1 pre-image over this interval I ∈ I .
1∩2
(iii) When θ and θ have 1 pre-image for C and C on I, the principal decision ratio is larger
1 2 1 2
for the transform θ that solves the following problem:
i
n n
min (cid:88) (y −µj)21(x ≤ θ−1C )+(cid:88) (y −µj )21(x > θ−1C )
j=1,2
i L i,kj j j i R i,kj j j
i=1 i=1
When k = k = k and C = C = C, this reduces to (11).
1 2 1 2
Proof See Appendix C.
Using the characterization in Proposition 8, the probability of θ having a larger principal decision
1
14Ranking Perspective for Tree-based Symbolic Regressions
ratio when compared to θ can be calculated as follows. Let x be the k-th coordinate of x, i.e., x .
2 k
In practice we usually use C = C = C, and it is not hard to exclude the case (iii) in the above
1 2
Proposition 8: Example 4 tells us that θ +C and θ for C ∈ R will not change principal decision
1 0 1 0
ratio τ. Assuming that supθ and infθ are finite, we can pick C = supθ −infθ +1 to ensure
2 1 0 2 1
that any C = C = C will not lead to the case (iii).
1 2
Proposition 9 Under the assumption of Proposition 8 and there is not I where both θ ,θ both
1 2
have 1 pre-images, assume additionally that the input location x is distributed as P and denote the
N +N refined monotonic intervals as
1 2
• I1,··· ,IN1 where θ has 1 pre-image and θ has 0 pre-image of C (θ has higher chance of
1 1 1 2 1
being selected);
• I1,··· ,IN2 where θ has 1 pre-image and θ has 0 pre-image of C (θ has higher chance of
2 2 2 1 2
being selected).
Then, the probability that the principal decision ratio prefers θ over θ can be computed as
1 2
p =
(cid:88)N1 (cid:90) 1dP−(cid:88)N2 (cid:90)
1dP =
P(cid:16)
∪N1
Ii(cid:17) −P(cid:16)
∪N2
Ij(cid:17)
. (14)
1>2 i=1 1 j=1 2
Ii Ij
i=1 1 j=1 2
Note that these intervals I1,··· ,IN1 and I1,··· ,IN2 are dependent on the fixed transformation
1 1 2 2
pair (θ ,θ ) and it is possible to optimize over C and C to maximize (or minimize) the quantity
1 2 1 2
p in (14).
1>2
Example 7 (p calculation)SupposethatPistheuniformmeasureon[0,1],andconsiderθ (x) =
1>2 1
x+1.2, θ (x) = −4x2+4x with C = C = C as in Example 6, and the refined monotonic intervals
2 1 2
I = {[0,1/2],[1/2,1]}. Applying (14) leads to
2
C ∈ (−∞,0) C ∈ (0,1) C ∈ [1,1.2) C ∈ [1.2,1.7) C ∈ [1.7,2.2) C ∈ (2.2,∞)
case (i) I1 = [0,1] case (i) I1 = [0,1/2] I2 = [1/2,1] case (i)
2 1 1
For other C’s, if we assume P is uniform, probabilities p can be filled in as:
1>2
C ∈ (−∞,0) C ∈ (0,1) C ∈ [1,1.2) C ∈ [1.2,1.7) C ∈ [1.7,2.2] C ∈ (2.2,∞)
0. 1. 0. 0.5 0.5 0.
With this table, one can observe that the value of p can be maximized by choosing C ∈
1>2
[1.2,2.2]; and it can be minimized by choosing C ∈ (0,1).
4 Global Rankings with Regressions
The ranking perspective established in preceding sections is focused on local splits. We now turn
to extending this perspective to study the global performance of tree-based methods, including
both a single tree consisting of multiple splits and tree ensembles, in terms of ranking. In this
section, we always consider the full dataset of size N instead of node-specific sample size n. On this
full dataset, we will show that tree-based methods such as CART and BART, when trained in a
supervised regression context, yield good ranking performance.
Throughout this section, we consider the model (1) as a “noisy scoring” model. In particular,
the mean function f, now considered as a scoring function, takes the feature x and assign it
i
15Luo and Li
a “score” f(x ), and its noisy “score” is y . Any scoring function f can induce a permutation
i i
r = {j ,j ,··· ,j } on the full set of {1,2,··· ,N} with the same cardinality, such that f(x ) ≥
1 2 N j1
f(x ) ≥ ··· ≥ f(x ). We consider the following criterion to assess the ranking performance of
j2 jN
scoring functions f through r:
N−1 N
2 (cid:88) (cid:88)
T (r) = (E y −E y ), (15)
N(N −1) yji|xji ji yji′|xji′ j i′
i=1 i′=i+1
which has been studied in the ranking literature such as Cossock and Zhang (2006) and can be also
generalized to operate on subsets instead of the full dataset. Note that previously we considered
rankings with actual responses y ’s; in the presence of noise, the metric in (15) consider orderings
i
on the conditional means instead of orderings on the responses y in (3) to avoid the noise effect.
j
The Bayes-scoring function f (x ) = E y is defined as the conditional expectation of y
B j yj|xj j j
conditioning on the j-th input x . Its incuded permutation, denoted by r , maximizes T (r) in (15)
j B
(see, e.g., Cossock and Zhang (2006)). Hence, the Bayes rank can be defined by this permutation
r that sorts the conditional means, and the (15) measures how any other permutation deviates
B
from this “optimal permutation”. From this discussion, we can see that the optimal rank-preserving
function is not unique and can be obtained from pre-composite monotonic transformations to f
B
like τ ◦f .
B
WearenowinapositiontoestablishoracleboundsundertherankingmetricT (r)forafully-split
singleCARTtreewithfinitesamples,basedontheoraclepropertiesfromKlusowskiandTian(2024).
Consider the function class G that collects functions with an additive form f(x) = (cid:80)d f (x ),
i=1 i i
where each coordinate f : R → R has bounded variation (hence f has bounded variation), and the
i
G is a pre-chosen model class that might deviate from G as set up in Klusowski and Tian (2024),
0
which allows for possible model misspecification. Here we need this pre-chosen model class G to
0
contain G. We consider a random design where X(N) is a simple random sample of a distribution
P . For a function f, its ℓ norm is defined as ∥f∥2 = (cid:82) f(u)2dP (u), and its supremum norm is
X 2 X
denoted as ∥f∥ .
∞
Theorem 10 (Oracle inequality for ranking) Suppose that the Bayes scoring function f ∈ G ⊃ G
B 0
and that we have a complete binary regression tree g of depth K ≥ 1 constructed by CART. Then
c,K
the permutation r induced by g satisfies
c,K c,K
P (T (r )−T (r ) > ε )
(X(N),Y(N)) B c,K N
(cid:40) (cid:41)
1024 ∥g∥2 2Klog2N log(Nd) 32R J (2R ,G )
≤ · inf ∥f −g∥2+ TV +C + ∞ 0 √ 2 0 , (16)
ε4 g∈G B K +3 B N ε2 N
N N
for any sequence ε > 0 that tends to 0, where R = 2sup ∥f∥ ,R = 2sup ∥f∥,
N ∞ f∈G0 ∞ 2 f∈G0
J (2R ,G ) is the entropy as (2.2) in van de Geer (2014) for the function class G , and the constant
0 2 0 0
C depends on the supremum norm of f and the class G.
B B
When G = G, the upper bound in (16) can be simplified to
0
(cid:40) (cid:41)
1024 ∥f ∥2 2Klog2N log(Nd) 32R J (2R ,G)
· B TV +C + ∞ 0 √ 2 , (17)
ε4 K +3 B N ε2 N
N N
and the constant C only depends on the total variation of f .
B B
16Ranking Perspective for Tree-based Symbolic Regressions
Proof See Appendix D.
Theright-handsideof (16)decomposesthefirstterminsidetheinfimumintothreecomponents:
the possible approximation error ∥f −g∥2 induced by finite depth K; the total variation caused
B
by the tree approximant g; the decaying term showing that d can grow exponentially without losing
the consistency of a CART fit. The second term bounds the difference between the empirical norm
and the ℓ norm.
2
Theorem 10 holds for any sample size. This theorem yields asymtpotic rates by letting N
diverge; we can see that the obtained rate ϵ is not faster than N−1/4, which is much slower than
N
that obtained in the following Theorem 13. Substituting ε ≍ O(cid:0) N−1/4−δ(cid:1) yields the following
N
consistency result for ranking.
Corollary 11 Under the same conditions as in Theorem 10, if we assume that the depth K = K
N
of the tree grows with the sample size N in such a way that
∥f ∥2 ≍ o(cid:16) ε4 √ K(cid:17) , 2Klog2N log(Nd) ≍ o(cid:0) ε4 (cid:1) ,
B TV N N N
where ε4 N → ∞ (i.e., ε ≍ O(cid:0) N−1/4−δ(cid:1) for δ > 0) as N → ∞, then there holds
N N
lim E (|T(r )−T(r )|) → 0.
(X(N),Y(N)) B c,K
N→∞
(cid:16) √ (cid:17)
Remark 12 The assumptions ∥g∥2 ≍ o ε2 K , 2Klog2Nlog(Nd) ≍ o(cid:0) ε2 (cid:1) are parallel to the
TV N N N
assumptions imposed by Corollary 4.4 in Klusowski and Tian (2024).
The next result shows that, under conditions, BART has a posterior concentration close to f if
B
the y ’s are generated by (1). We consider a fixed and regular design as described in Definition 7.1
i
of Ročková and Saha (2019) or Definition 3.3 in Ročková and van der Pas (2020). In the context
of Ročková and van der Pas (2020), a regular design refers to a fixed dataset where the diameters
of the cells in a k-d tree partition are controlled and relatively uniform. Specifically, the maximal
diameter in the partition components should not be significantly larger than a typical diameter. An
example of a regular dataset would be a fixed design on a regular grid, where the points are evenly
spaced and no cells have an excessive spread of points compared to others. In contrast, a dataset
with highly skewed or isolated points in certain directions might not meet the regularity condition.
We use the probability measure corresponding to responses generated using the Bayes scoring
function f in model (1), which implies that the response should be considered evaluated at these
B
fixed inputs with random noise.
Theorem 13 (Fixed design) Assume that the Bayes scoring function f is ν-Holder continu-
B
ous for ν ∈ (0,1], with the norm ∥f ∥ log1/2N and a regular design over the set X(N) =
B ∞
{x 1,··· ,x N} ⊂ Rd where d ≲ log1/2N. L(cid:62)et the function class F be defined as a set of additive
simple functions as described in (3) of Ročková and Saha (2019). Consider the BART prior with a
fixed number of trees and node splitting probability p (η) = αdepth(η) for a node η and α ∈ (cid:2)1, 1(cid:1) .
split N 2
(cid:81)
Then, the following contraction for BART posterior holds for the resulting posterior distribution
and the BART induced ranking functions r from (1):
f
(cid:89)
(f ∈ F : T (r )−T (c ) > M ε |y ,··· ,y ) → 0
B f N N 1 N
in probability measure of the y ,··· ,y , where ε = N−α/(2α+d)log1/2N, and for any sequence
1 N N
M → ∞, as the sample size N → ∞ and the dimensionality d → ∞.
N
17Luo and Li
Proof See Appendix E.
Remark 14 TheBayesscoringfunctionf (x )alwaysexists(seeTheorem1inCossockandZhang
B j
(2006)). Theorem 13 indicates that as we fit BART with an increasing number of samples X sat-
isfying a regular design, the resulting posterior will concentrate around the Bayes scoring function.
Unlike the finite-sample result for a single binary tree established in Theorem 10, Theorem 13 pro-
vides an asymptotic result concerning the posterior distribution of BART.
Building on our previous discussions from a ranking perspective, we can summarize the findings
as follows: Locally, at each split, the partitions are most likely divided into rank-consistent groups,
but within each partition, no ranking is available since the scoring function remains constant within
each partition. Globally, in the asymptotic behavior of BART, the posterior tends to concentrate
near the Bayes scoring functions that minimize the L (hence T metric) error. Meanwhile, CART
2
also achieves consistent ranking performance, with finite-sample bounds available.
5 Concordant Divergence
We now shift to leveraging the ranking perspective to study symbolic feature selection as illustrated
inExample1. Althoughtree-basedmethodshavedemonstratedstrongfinite-sampleperformancein
distinguishing between symbolic features that are transformations of one another, it remains elusive
since the existing theory, viewed through the lens of nonparametric variable selection, is unaffected
bysuchtransformations. Inthissection,weextendourpreviousanalysistouncoveradditionalchar-
acteristics of tree-based methods. Building on these insights, we introduce a concordant divergence
statistic, T , which can evaluate feature mappings.
0
The local split analysis in previous sections has provided some insight into tree-based methods
when transformations are involved. Section 3.1 shows that the oracle partition, relevant only to
the ranks of responses y’s, may not be attainable with a single split along the input domain, unless
there is monotonicity along one coordinate. Section 3.2 compares two piecewise transformations in
a relative sense. Next, we first present another local-level observation before extending the ranking
perspective to study a general mapping g in an absolute sense, moving beyond local splits.
Lemma 15 (Magnitude of swaps) Under the same assumptions as in Lemma 1, we suppose that
the only reversed pairs are (y ,y ) and (y ,y ) where y > y ,y > y and both y ,y ∈ P but
α γ β γ α γ β γ α β 1
y ∈ P . If y > y > y , then the swap for the pair (y ,y ) reduces the loss (10) more than the
γ 2 α β γ α γ
swap for the pair (y ,y ) .
β γ
Proof See Appendix F.
Lemma 1 highlights the effect of the magnitude of the responses. In particular, we might prioritize
the swapping of a reverse pair with a larger “size”, i.e., the difference between the responses y in the
reversed pair. It is possible that there exist two reversed pairs with the same “sizes”. However, with
our assumption that both inputs and responses are continuous, it is with zero probability that we
have two reverse pairs such that their magnitudes are identical.
Based on the discussion of tree-like models for the univariate case (Lemmas 1 and 15), we can
summarize the principle behind feature selection as follows: it selects the feature mapping g that
takes x into transformed images z that have the most similar rankings as the response y. The
discrepancies between these rankings can be described by the “gaps” between ordered statistics of
18Ranking Perspective for Tree-based Symbolic Regressions
y’s. Motivated by Lemma 15, we develop T to evaluate arbitrary feature mapping θ:
0
T (θ) =
(cid:88) 2|y π(1)−y π(2)|
·{1(θ(x ) ≥ θ(x ))·1(y < y )
0 n(n−1) π(1) π(2) π(1) π(2) (18)
π
+1(θ(x ) < θ(x ))·1(y ≥ y )},
π(1) π(2) π(1) π(2)
where the summation takes over all permutations of length 2 as (π(1),π(2)) with π(1),π(2) ∈
{1,··· ,n}. Guided by the ranking behavior induced by tree methods, T0(θ) evaluates a symbolic
feature θ(x) by measuring to what extent it can recover the order of y’s. In particular, if θ(x(i)) <
θ(x(j)), we have a zero summand; otherwise, we will have a non-negative summand (cid:12) (cid:12)y(i)−y (j)(cid:12) (cid:12).
A larger T (θ) indicates more discrepancy between the rankings of θ(x) and y, and we accumulate
0
all these discrepancies. Note that despite the intimate connection with the ranking performance of
tree methods, to use this divergence, we do not need to consider a tree model, but simply a finite
number of samples.
Remark 16 The T is similar to Kendall’s tau (Hollander et al., 2013) but with the additional
0
(cid:12) (cid:12)
non-negative multiplier, (cid:12)y π(1)−y π(2)(cid:12), representing “the magnitude of swaps”. It involves both the
ranks and the actual values of the responses y. This T is also not the same as linear correlation
0
coefficients. Daniels (1944) (in Section 5) stated that the linear correlation coefficients ρ satisfy
ρ = ρ ρ for any transformation θ, which means that the correlation ρ cannot increase
θx,y θx,x x,y θx,y
beyond ρ since ρ ≤ 1. However, the behavior of T is not constrained in the same way when
x,y θx,x 0
transformations are introduced.
For inactive variables, we want to exclude both the variable itself and all its transformations.
Fromthefollowingdefinition,itisclearthatanytransformationofinactivevariableswillalsoremain
inactive.
Definition 17 (Inactivevariable)AfeatureX ∈ RofacontinuousinputvariableX,k ∈ {1,··· ,d}
k
is called inactive (for function f as in (1)), if the distribution of y is independent of the distribution
of f(X ).
k
Considering a random design where each row of X is drawn independently from a distribution,
we have the following properties for T when evaluating transformations of X :
0 k
Proposition 18 (i) if X is an inactive variable, then E T (f) ̸→ 0 as n → ∞.
k X k,y 0
(ii) if there exists a transformation θ = g such that g(x ) ≥ g(x ) ⇔y ≥ y , then E T (g) = 0.
1 2 1 2 X,y 0
Proof See Appendix G.
These two results establish the fact that T will never prefer a mapping consisting of an inactive
0
variable (Proposition 18, i), unless that mapping is a “fake interpolator” for the given finite sample
(Proposition18, iiandExample5). Inanextremecasewhereallvariablesareinactive, thatis, none
of the coordinates in X determine the value of y, part (i) of the proposition implies that T will not
0
be zero. This means that if we take g = id, then such an identity mapping is not an interpolator
(see Example 5). However, this does not rule out the possible existence of an interpolator g ̸= id
such that g “reorganizes” X and y in a concordant way, even if X are completely inactive.
As we do not actually have to use a tree structure when computing T , this gives us convenience
0
in practice. In symbolic feature selection, we will calculate the correlation T(y(z)) between the
response y and each coordinate of z ’ to find useful features (i.e., correlation between the y vector
i i
and the q columns of z matrix).
19Luo and Li
Example 8 (Power of T ) In Figure 4, we present 5 different feature mappings θ ,··· ,θ and
0 1 5
compute their pairwise correlations and T and compare its performance to other correlations. For
0
ease of comparison, we use log-scale for T and [0,1] scale for the other correlations. We will expect
0
the divergence to be close to 0, indicating dependence between the sample x and θ (x) to various
i
extents (i.e., T = 0 as in Proposition 18 (ii)). Since θ = x coincide with θ ,··· ,θ to different
0 1 2 5
extents, we also want the correlation reflect the degree of dependence.
The Chatterjee (2021)’s correlation coefficient ξ (X,y) and the T are both asymmetric and
n 0
measure both capture non-linear dependencies between pairs of random variables, particularly non-
linear dependencies. The ξ (X,y) rearranges data pairs based on sorted values and computes
n
rank-based statistics, making it sensitive to changes in the data’s distribution and structure. On
the other hand, T is a permutation-based measure that evaluates the sum of contributions from all
0
possible pairs of data points, considering differences in values and their rankings. This exhaustive
approachincomputingdivergenceisrobustagainstoutliersandprovidesadetailedunderstandingof
pairwisedependencies. However,itiscomputationallyintensiveduetotherelianceonpermutations,
especially for moderate to large datasets.
From Figure 4, we can observe that classic correlations like Pearson, Spearman, and Kendall
cannot detect the functional dependence between x and the other θ ’s regardless of the signal-
i
to-noise ratio, which is proportional to 1/σ2. However, the Chatterjee (2021) correlation and T
0
are capable of capturing this dependence when the signal-to-noise ratio is high and the sample
size is sufficiently large (N = 50). In addition, we may also observe that compared to Chatterjee
correlation, T will not falsely detect functional dependence when the signal-to-noise ratio is low,
0
even with only N = 50. Chatterjee correlation seems to stumble when the sample size is limited.
θ and the rest θ ,··· ,θ have different degree of overlapping, which is reflected by the magnitude
1 2 5
of T (when noise variance is small), but not by the other correlations.
0
This example elucidates the behavior of various selection criteria when applied to four distinct
features generated from the same input. In contrast to Pearson, Spearman and Kendall correlation,
which measure linear and ordinal association respectively, the T statistic demonstrates an effective
0
approach. It does not erroneously filter out the correct function when compared to the correlations
between the true function and the different features θ x. As shown in the experimental results in
i
Figure 4, only T can detect the functional dependence and being sensitive to signal-to-noise ratio;
0
and this makes T a suitable correlation of detecting even different sampling plans. This exemplifies
0
T ’s utility in feature selection, where the goal is to maintain the true influential features.
0
6 Experiments
In this section, we conduct simulations to assess the performance of the proposed T divergence for
0
selectingvariablesandsymbolicexpressions, andlendsupporttothetheoreticalresultsinpreceding
sections. We expect that the concordant divergence statistics, which is motivated as derived from
the discussion that “tree-based splits are attempting to match the rankings of z and y”, will behave
similarly to the tree-based methods, on these symbolic regression tasks.
6.1 AUC for feature selection
Continuing the discussion in Example 1 We consider a 3-dimensional input variables (x ,x ,x ) ∈
1 2 3
[0,1]3 and the following model
y = 2x3+5x +10+ϵ, (19)
1 3
where ϵ ∼ N(0,σ2). With sample size n, we generate an n×3 matrix X for input variables using
uniform random variables, and the corresponding y using (19). We consider two architectures
20Ranking Perspective for Tree-based Symbolic Regressions
θ (x) θ (x) θ (x) θ (x) θ (x)
1 2 3 4 5

x+1 x ∈ [−1.0,−0.5)

(cid:40)  
+x x > 0 −x x ∈ [−0.5,0.0)
x −θ (x) −θ (x)
1 4
x x ≤ 0 x x ∈ [0.0,0.5)



−x+1 x ∈ [0.5,1.0]
Sample size N=50
Sample size N=500
Figure 4: Correlation between x and y = θ (x) for i = 1,...,5. The expression and figure for each
i
θ are reported in the top two rows in the table. Left to Right (in the 3rd and 4th rows): Chatterjee
i
correlation (Chatterjee, 2021), absolute Pearson correlation, absolute Spearman correlation and
absolute Kendall correlation, log(T ). The T is shown on a log-scale for better comparison. We
0 0
generate an equally spaced x on [−1,1] with sample size N = 50 (3rd row) and N = 500 (4th row).
Gaussian noises with variance σ2 are added to θ (x).
i
for generating transformations: O(2) = O ◦ O and O(2) = O ◦ O , where O = {id,x3} and
O = {+,×}. The design matrix foA ru each au rchiteb cture haA sbthe folb lowinu g dimensionu ality:
b
21Luo and Li
θ (x) θ (x) θ (x) θ (x)
1 2 3 4
x 2sin(x) 2sin(7x) sin(x)
BART Global Max 18.0 10.0 10.0 0.0
BART Local 29.0 29.0 25.0 0.0
Pearson 0.0 100.0 0.0 0.0
Kendall 100.0 0.0 0.0 0.0
T 100.0 100.0 100.0 0.0
0
x sin(4x+0.2) sin(4x+0.1) sin(4x)
BART Global Max 0.0 0.0 5.0 64.0
BART Local 0.0 0.0 9.0 83.0
Pearson 0.0 0.0 0.0 100.0
Kendall 0.0 0.0 0.0 100.0
T 0.0 0.0 0.0 100.0
0
x cos(x) sin(2x) sin(x)
BART Global Max 4.0 5.0 6.0 5.0
BART Local 7.0 11.0 13.0 13.0
Pearson 0.0 0.0 0.0 100.0
Kendall 100.0 0.0 0.0 0.0
T 100.0 0.0 100.0 100.0
0
x sin(4x) sin(6x) sin(5x)
BART Global Max 0.0 16.0 0.0 61.0
BART Local 1.0 29.0 0.0 68.0
Pearson 0.0 0.0 0.0 100.0
Kendall 0.0 0.0 0.0 100.0
T 0.0 0.0 0.0 100.0
0
Table 1: The inclusion percentage, as an empirical approximation to the inclusion probability,
by methods BART (bartMachine R package (m = 50)), T , Pearson’s correlation and Kendall’s
0
tau between x,θx are provided for comparison. The true signal θ is highlighted in bold, and all
4
experiments are done with x ∼ N(0,1) with sample size 500.
1. For O(2), after the first layer of binary operations, we have 2(C2+C1) = 12 different features
Au 3 3
and a n×12 matrix. Then, we take this n×12 matrix as the input of the next layer of unary
operations and produce C1×12 = 24 different features and a n×24 matrix.
2
2. ForO(2), similarlytothecalculationabove, thefirstlayerofunaryoperationsgivesC1×3 = 6
A 2
b
features, and the second layer of binary operations give 2(C2+C1) = 42 features.
6 6
22Ranking Perspective for Tree-based Symbolic Regressions
Noise variance 0 Noise variance 0.01 Noise variance 0.1
Figure5: WeillustratethePRcurvesfrom50repeats(n = 100)ofa2-layersymbolicregressionwith
O = {id,x3} and O = {+,×}. The true signal is (19) with no noise. The first row corresponds
u b
to the architecture of O(2) and the second row corresponds to the architecture of O(2). We provide
the boxplot to show theA Au UC values amongst 50 repeats. A b
(2) (2)
For O For O
Au A
b
x +x x ×x x +x x +x x3+x x +x3
1 1 1 3 1 1 1 3 1 3 3 3
(x +x )3 (x ×x )3 x ×x x ×x x3×x x ×x3
1 1 1 3 1 1 1 3 1 3 3 3
x ×x x +x x +x3 x +x3 x3+x3 x3+x3
1 1 3 3 1 1 1 3 1 3 3 3
(x ×x )3 (x +x )3 x ×x3 x ×x3 x3×x3 x3×x3
1 1 3 3 1 1 1 3 1 3 3 3
x +x x ×x x3+x3 x3+x3 x +x
1 3 3 3 1 1 1 1 3 3
(x +x )3 (x ×x )3 x3×x3 x3×x3 x ×x
1 3 3 3 1 1 1 1 3 3
Table 2: Correct features for the problem (19) as they only contains x , x , or their transforms.
1 3
The goal of symbolic regression is to select features from the n×24 matrix (if O(2)) or n×42 matrix
Au
(if O(2)) given data.
A
Thbe true signal in (19) uses only x and x . For evaluation, we consider a feature to be “correct”
1 3
as long as it only contains x , x , or their transforms (as listed out in Table 2). To obtain a useful
1 3
ROC curve, we first create an array called ground_truth of size N , initialized with zeros. We
total
assign N (= 1) true feature to the ground truth array by setting its corresponding element to 1.
true
Next, we create an array called predicted_scores of size N , containing random scores for each
total
feature. We then assign the highest N predicted scores from our BART selection procedure
selected
to the selected features by sorting the scores and assigning the top N values to the selected
selected
23Luo and Li
feature indices. We use N = 3 in this experiment to indicate that there may be x ,x and
selected 1 3
the constant intercept (which is included by default) in (6.1) will be correctly selected.
Finally, we plot the performance curve1 along with the reference diagonal line representing the
performance of a random classifier. According to the criterion where we consider a feature to be
“correct” as long as it only contains x , x , we can examine each of the (24 or 42) features and
1 3
label them as 1 if “correct”; as 0 if not. With this manually examined ground truth label, we also
compare the N = 3 labels to this ground truth, we can compute the precision-recall curve
selected
and its AUC. The AUC value is shown in the legend, providing a measure of the performance of
our feature selection method. The higher the AUC, the better our method is at identifying the
true feature among the selected features. In symbolic regression, we focus on keeping the correct
signals involving active variables, metrics like AUC for Precision-Recall curve is more appropriate
for evaluating the performance of each method than the usual TDR/FDR AUC, as we provided in
Figure 5. A high PR AUC indicates that the model achieves both high recall and high precision,
maintaining a good balance, especially when a positive class is of great interest or when negative
examples outnumber positive ones.
From Figure 5, we can observe that as the noise variance increases, the AUC decreases. It is
also of interest to observe that in the O(2) setting, the AUC is higher than that of the architecture
Au
of O(2). This lends support to the architecture design in Ye et al. (2024) that the binary operator
A
shouldb be introduced as the first alternating layer.
6.2 Comparison against other methods
Furthermore, we use the same experiment to compare the performance of different model-based fea-
tureselectionmethods,andourT usingthesignal(19). Weselectthefeaturesusingy withdifferent
0
additivenoisevariancesandthecorrespondingX fromO(2) (24features)orO(2) (42features)archi-
tectures. For comparison, we include LASSO (glmnet
==Au
4.1−8, Hastie et
aA
l.b(2009) with lambda
chosen by default cross-validation (lambda=-1)), SCAD (ncvreg == 3.14.1, Fan et al. (2014)) and
step-wise subset selection using linear models (LMSTEPWISE, leaps :: regsubsets == 3.1) as
competitors of model-based feature selections methods. Our T in (18) inspired by ranking perspec-
0
tive from BART is the only method that is not model-based. In what follows, our concern is the
correct feature selection instead of predictive performance. Thus, we simply use the same dataset
for selecting the features.
Previously, we observed in Example 8 that T behaves differently than classical correlation
0
coefficient. InthissetofexperimentalresultsinFigure6,wedisplaytheaverageinclusionprobability
(AIP)asanapproximationtothefrequencyoffeaturesbeingselected,sinceT isnotaformalfeature
0
selection method that can be evaluated by AUC curve, yet we still want to see how well it performs
when we select the feature with smallest concordant divergence statistics. All those features only
contains x , x or their transforms (of any kind) are considered correct if selected. Then we repeat
1 3
the experiment for 50 different random X and computed the frequency that each of these correct
features are selected. Then we sum up these probabilities and divide by N = 3, as our AIP
selected
metricinFigure6. ThatmeansweaskeachmethodtopickN = 3featuresamongallpossible
selected
features for 50 times, and AIP represents the average probability that these features are all correct.
The higher AIP means more correct features are chosen in this configuration of sample size, noise
variance, and method.
1. Wecalculatethefalsepositiverate(FPR)andtruepositiverate(TPR)atvariousthresholdsusingtheroc_curve
function from sklearn.metrics, which takes the ground truth labels and predicted scores as input. The AUC
value is computed using the auc function, which calculates the area under the ROC curve using the trapezoidal
rule.
24Ranking Perspective for Tree-based Symbolic Regressions
Figure 6: We illustrate the average inclusion probabilities from 50 repeats of a 2-layer symbolic
regression with O = {id,x3} and O = {+,×}. The true signal is (19). The first row corresponds
u b
to the architecture of O(2) and the second row corresponds to the architecture of O(2).
Au A
b
ItisnothardtoseethatbothBARTandT areperformingextremelywellforlownoisevariances,
0
followedbyLASSO.LASSOandSCADarebehavedsurprisinglywellinthis2-layerexampleperhaps
due to the relatively small number of features, in contrast to the Ye et al. (2024)’s setting where
a much large number of features need to be screened. However, the low AIP associated with
linear model stepwise selection (LMSTEPWISE) is clearly not suitable for this scenario for O(2) nor
Au
O(2). One step further, we point out that T is the fastest method, even if it involves summation
A 0
ovebr permutations, followed by LASSO. While BART has the benefit of providing uncertainty
quantificationandhigherselectionpower,itisamongtheslowermethodsduetoitsMCMCsampling
step.
Concurrently, we also test these nonparametric methods on classic ODE-Strogatz repository for
symbolic regression dataset as an example of “ground-truth regression problems” (La Cava et al.,
2021). However, we did not intend to compete with the formal symbolic regression methods but
focus on the feature selection accuracy like above. In this experiment, we use different orders
of symbolic compositions (e.g., ub, ubb) instead of alternating layers to ensure that the correct
composition of symbols can be obtained through the architecture. In addition to different layers, we
alsousedifferentsetsofbinaryandunitaryoperatorstohavebettergenerality. TheLMSTEPWISE
cannot work properly due to the >100 co-linear features in these examples. Since all expressions
containx (xvariateinrawdata)andx (yvariateinrawdata),webringinamorestringentcriteria:
1 2
all those features only contains x , x and their correct transforms are considered correct.
1 3
Tomakethecomparisonfair,weenforcethattheregressiondoesnotattempttoestimaterelevant
coefficientsforsymbolictermsbutlookattheselectedfeaturesamongallpossibleexpressions. From
25Luo and Li
Table 3, we can observe that for simple ODEs (vdp2), all four methods behave reasonably well.
However, when we study additive signals with different magnitudes (glider1, vdp1), the LASSO
and SCAD do not recognize the correct format. BART behaves bad too, while T actually identify
0
features that coincide with the original ODE signal better. For complicated composition (glider2),
it seems that all methods except SCAD and T work pretty well, even if coefficient estimates are
0
not allowed.
Symbolic composition order
dataset truth O O order
u b
glider1 −0.05x2−sin(x ) {sin(x),x2} {+,−} ub
1 2
glider2 x −cos(x )/x {cos(x),id} {−,/} ubb
1 2 1
vdp1 −10/3x3+10/3x +10x {x3} {+,−} ubb
1 1 2
vdp2 −x /10 / / /
1
Results by each method with noise variance 0.100
method glider1 glider2 vdp1 vdp2
BART x2 cos(x )/x −x −x x3+x3−x3 x
2 2 2 1 2 1 2 2 2
LASSO sin(x )/sin(x ) cos(x )/x −x −x x3 x
1 2 2 2 1 2 2 2
SCAD x2 x x3 x
2 1 2 2
sin(x )−sin(x ) x −cos(x )/cos(x )−x x3−x3−x3 x
T 1 2 1 1 2 1 1 2 1 2
0 or sin(x )−x or x
1 1 2
Table 3: The most frequently selected expressions from datasets (n = 400) in the ODE-Strogatz
repository https://github.com/lacava/ode-strogatz, as generated by using the first principles
physical models. The LMSTEPWISE (linear model with step-wise selection) runs into error due to
the high co-linearity in the input of these datasets.
7 Discussion and Future work
Tree-based methods are highly effective for a wide range of real-world tasks. The current under-
standing of this effectiveness often relies on asymptotic analysis or heuristics. While advancements
in these two directions are both useful, they yield a substantial gap that calls for a formal investi-
gation of tree-based methods that can generalize and closely link to their empirical success. In this
paper, we develop a comprehensive ranking perspective for understanding tree-based methods. We
provide a series of finite-sample analyses concerning the interplay between splits and ranking, cov-
ering local splits, single trees, and tree ensembles. Asymptotics results are also established when we
evaluate selected tree-based methods using their ranking performance. One particular application
is symbolic feature selection in the presence of transformations of input variables, a crucial step
in symbolic regression that the empirical success of tree-based methods has only been observed re-
26Ranking Perspective for Tree-based Symbolic Regressions
cently. Ourrankingperspectiveleadstoinsightswhencomparingtransformationsandalsoprovides
new divergence statistics as a method to select symbolic features.
The motivation for this work was to develop a better understanding of a broad class of tree-
based methods through ranking. A future objective is to provide a foundation for more model
structures to which tree-based methods can be applied, including classification, non-Gaussian error
assumptionsandnon-standardinputs(LuoandMa,2024;Luoetal.,2024). Therankingperspective
is presumably more robust to model misspecification, which might help explain the robustness of
tree methods in real-world applications. Similarly, Clémençon et al. (2008) highlighted that ranking
theory, when extended beyond two items, significantly depends on the designated loss, marking an
interesting area for further research when the principal decision ratio τ is defined by other norms
(e.g., L1). Finally, our ranking perspective on tree methods can be expanded to provide uncertainty
quantification for ranking.
Acknowledgment
HL thanks for Mikael Vejdemo Johansson for kindly providing computational resource in pilot
experiments. HLwassupportedbytheDirector,OfficeofScience,oftheU.S.DepartmentofEnergy
under Contract DE-AC02-05CH11231, and DE-FOA-0002958. HL is also supported by NSF grant
DMS 2412403. ML’s research was partially supported by NSF grant DMS/NIGMS-2153704.
27Luo and Li
References
Abhineet Agarwal, Yan Shuo Tan, Omer Ronen, Chandan Singh, and Bin Yu. Hierarchical Shrink-
age: Improving the Accuracy and Interpretability of Tree-based Models. In International Con-
ference on Machine Learning, pages 111–135. PMLR, 2022.
Mirela Andronescu and Mark Brodie. Decision Tree Learning using a Bayesian Approach at Each
Node. InAdvances in Artificial Intelligence: 22nd Canadian Conference on Artificial Intelligence,
CanadianAI2009Kelowna, Canada, May25-27, 2009Proceedings22,pages4–15.Springer,2009.
Susan Athey, Julie Tibshirani, and Stefan Wager. Generalized random forests. The Annals of
Statistics, 47(2):1148 – 1178.
Justin Bleich, Adam Kapelner, Edward I George, and Shane T Jensen. Variable Selection for
BART: an Application to Gene Regulation. The Annals of Applied Statistics, 8(3):1750–1781,
2014. Publisher: Institute of Mathematical Statistics.
Leo Breiman, Jerome Friedman, Richard A. Olshen, and Charles J. Stone. Classification and
Regression Trees. Routledge, 1987.
Randal E Bryant. Symbolic Boolean Manipulation with Ordered Binary-decision Diagrams. ACM
Computing Surveys (CSUR), 24(3):293–318, 1992.
Sourav Chatterjee. A New Coefficient of Correlation. Journal of the American Statistical Associa-
tion, 116(536):2009–2022, 2021.
Hugh A Chipman, Edward I George, and Robert E McCulloch. Bayesian CART Model Search.
Journal of the American Statistical Association, 93(443):935–948, 1998.
Hugh A Chipman, Edward I George, and Robert E McCulloch. Bayesian Treed Models. Machine
Learning, 48:299–320, 2002.
Hugh A. Chipman, Edward I. George, and Robert E. McCulloch. BART: Bayesian Additive Re-
gression Trees. The Annals of Applied Statistics, 4(1):266–298, 2010.
Stéphan Clémençon, Gábor Lugosi, and Nicolas Vayatis. Ranking and Empirical Minimization of
U-statistics. The Annals of Statistics, 36(2), 2008.
Stéphan Clémençon and Sylvain Robbiano. The TreeRank Tournament Algorithm for Multipartite
Ranking. Journal of Nonparametric Statistics, 27(1):107–126, 2015.
Stéphan Clémençon and Nicolas Vayatis. On Partitioning Rules for Bipartite Ranking. In Artificial
Intelligence and Statistics, pages 97–104. PMLR, 2009.
Stephan Clémençon and Robin Vogel. On Tree-based Methods for Similarity Learning. In In-
ternational Conference on Machine Learning, Optimization, and Data Science, pages 676–688.
Springer, 2019.
Stéphan Clémençon, Marine Depecker, and Nicolas Vayatis. Adaptive Partitioning Schemes for
Bipartite Ranking. Machine Learning, 83(1):31–69, 2011.
David Cossock and Tong Zhang. Subset ranking using regression. In Learning Theory: 19th Annual
Conference on Learning Theory, COLT 2006, pages 605–619. Springer, 2006.
28Ranking Perspective for Tree-based Symbolic Regressions
H. E. Daniels. The Relation Between Measures of Correlation in the Universe of Sample Permuta-
tions. Biometrika, 33(2):129, 1944.
Ramon Díaz-Uriarte and Sara Alvarez De Andrés. Gene Selection and Classification of Microarray
Data using Random Forest. BMC Bioinformatics, 7(1):1–13, 2006.
Jianqing Fan, Yunbei Ma, and Wei Dai. Nonparametric Independence Screening in Sparse Ultra-
high-dimensional Varying Coefficient Models. Journal of the American Statistical Association,
109(507):1270–1284, 2014.
Walter D Fisher. On Grouping for Maximum Homogeneity. Journal of the American statistical
Association, 53(284):789–798, 1958.
Robin Genuer, Jean-Michel Poggi, and Christine Tuleau-Malot. Variable Selection using Random
Forests. Pattern recognition letters, 31(14):2225–2236, 2010.
Léo Grinsztajn, Edouard Oyallon, and Gaël Varoquaux. Why do tree-based models still outperform
deep learning on typical tabular data? Advances in neural information processing systems, 35:
507–520, 2022.
P Richard Hahn, Jared S Murray, and Carlos M Carvalho. Bayesian Regression Tree Models for
Causal Inference: Regularization, Confounding, and Heterogeneous Effects (with discussion).
Bayesian Analysis, 15(3):965–1056, 2020.
Trevor Hastie, Robert Tibshirani, Jerome H Friedman, and Jerome H Friedman. The Elements of
Statistical Learning: Data mining, Inference, and Prediction, volume 2. Springer, 2009.
Myles Hollander, Douglas A Wolfe, and Eric Chicken. Nonparametric Statistical Methods. John
Wiley & Sons, 2013.
Akira Horiguchi, Matthew T Pratola, and Thomas J Santner. Assessing Variable Activity for
Bayesian Regression Trees. Reliability Engineering & System Safety, 207:107391, 2021.
Jason M Klusowski and Peter M Tian. Large scale prediction with decision trees. Journal of the
American Statistical Association, 119(545):525–537, 2024.
Miron B Kursa and Witold R Rudnicki. Feature selection with the boruta package. Journal of
Statistical Software, 36(11):1–13, 2010.
William La Cava, Patryk Orzechowski, Bogdan Burlacu, Fabrício Olivetti de França, Marco Vir-
golin, Ying Jin, Michael Kommenda, and Jason H Moore. Contemporary Symbolic Regression
Methods and their Relative Performance. arXiv preprint arXiv:2107.14351, 2021.
Michel Ledoux and Michel Talagrand. Probability in Banach Spaces: isoperimetry and processes,
volume 23. Springer Science & Business Media, 1991.
Meng Li and Li Ma. Learning Asymmetric and Local Features in Multi-dimensional Data through
Wavelets with Recursive Partitioning. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 44(11):7674–7687, 2021.
Antonio R Linero. Bayesian Regression Trees for High-dimensional Prediction and Variable Selec-
tion. Journal of the American Statistical Association, 113(522):626–636, 2018.
29Luo and Li
Chuji Luo and Michael J Daniels. Variable Selection Using Bayesian Additive Regression Trees.
arXiv preprint arXiv:2112.13998, 2021.
Hengrui Luo and Anna Ma. Frontal Slice Approaches for Tensor Linear Systems. arXiv preprint
arXiv:2408.13547, 2024.
Hengrui Luo and Matthew T Pratola. Sharded Bayesian Additive Regression Trees. arXiv preprint
arXiv:2306.00361, 2023.
Hengrui Luo, Akira Horiguchi, and Li Ma. Efficient Decision Trees for Tensor Regressions. arXiv
preprint arXiv:2408.01926, 2024.
Nour Makke and Sanjay Chawla. Interpretable Scientific Discovery with Symbolic Regression: A
Review. Artificial Intelligence Review, 57(1):2, 2024.
Aditya Krishna Menon and Robert C Williamson. Bipartite Ranking: a Risk-theoretic Perspective.
The Journal of Machine Learning Research, 17(1):6766–6867, 2016.
DJ Newman, Eli Passow, and Louis Raymon. Piecewise monotone polynomial approximation.
Transactions of the American Mathematical Society, 172:465–472, 1972.
J. Ross Quinlan. Induction of decision trees. Machine learning, 1:81–106, 1986.
Veronika Ročková and Enakshi Saha. On theory for BART. In The 22nd international conference
on artificial intelligence and statistics, pages 2839–2848. PMLR, 2019.
Veronika Ročková and Stéphanie van der Pas. Posterior Concentration for Bayesian Regression
Trees and Forests. The Annals of Statistics, 48(4):2108–2131, 2020.
Omer Ronen, Theo Saarinen, Yan Shuo Tan, James Duncan, and Bin Yu. A mixing time lower
bound for a simplified version of bart. arXiv preprint arXiv:2210.09352, 2022.
Erwan Scornet, Gérard Biau, and Jean-Philippe Vert. Consistency of random forests. The Annals
of Statistics, 43(4):1716–1741, 2015.
Carolin Strobl, Anne-Laure Boulesteix, Thomas Kneib, Thomas Augustin, and Achim Zeileis. Con-
ditional variable importance for random forests. BMC Bioinformatics, 9(1):1–11, 2008.
Kazuki Uematsu and Yoonkyung Lee. On Theoretically Optimal Ranking Functions in Bipartite
Ranking. Journal of the American Statistical Association, 112(519):1311–1322, 2017.
Sara van de Geer. On the uniform convergence of empirical norms and inner products, with appli-
cation to causal inference. Electronic Journal of Statistics, 8(1):543 – 574, 2014.
Marvin N Wright and Andreas Ziegler. ranger: A fast implementation of random forests for high-
dimensional data in c++ and r. Journal of Statistical Software, 77:1–17, 2017.
Shengbin Ye, Thomas P Senftle, and Meng Li. Operator-induced structural variable selection with
applications to materials genomes. Journal of the American Statistical Association, 119(545):
81–94, 2024.
Cheng Li Yichen Zhu and David B. Dunson. Classification Trees for Imbalanced Data: Surface-
to-Volume Regularization. Journal of the American Statistical Association, 118(543):1707–1717,
2023.
30Ranking Perspective for Tree-based Symbolic Regressions
Appendices
Appendix A. Proof of Lemma 1
Proof SupposethatP′ = {y < y < ··· < y }andP′ = {y < y < ··· < y }andthe
1 (1) (2) (i+1) 2 (i) (i+2) (n)
varianceofP∗ isstrictlysmallerthanthevarianceofP∗∗,thenwecanwriteexplicitlythatthegroup
1 1
means for P′ and P′: µ′ = 1 ·(cid:0) µ∗·i−y +y (cid:1) and µ′ = 1 ·(cid:0) µ∗·(n−i)−y +y (cid:1),
1 2 1 i 1 (i) (i+1) 2 n−i 2 (i+1) (i)
where µ∗,µ∗ are corresponding in-group means of P∗,P∗. Since we assume that the variance of P∗
1 2 1 2 1
is strictly smaller than the variance of P∗∗, our idea is to prove that switching y and y will
1 (i+1) (i)
reduce P′ to P∗ and P′ to P∗ with strictly smaller sum of group variances.
1 1 2 2
Now,weconsiderthedifferencesµ∗−µ′ = µ∗−1·(cid:0) µ∗·i−y +y (cid:1) = −1·(cid:0) −y +y (cid:1) <
1 1 1 i 1 (i) (i+1) i (i) (i+1)
0 and µ∗−µ′ = µ∗− 1 ·(cid:0) µ∗·i−y +y (cid:1) = − 1 ·(cid:0) −y +y (cid:1) > 0.
2 2 2 i 2 (i) (i+1) n−i (i+1) (i)
(cid:88) (y −µ′ )2+ (cid:88) (y −µ′ )2 (20)
(j) 1 (j) 2
y ∈P′ y ∈P′
(j) 1 (j) 2
i−1 n
=(cid:88) (y −µ′ )2+(y −µ′ )2+(y −µ′ )2+ (cid:88) (y −µ′ )2
(j) 1 (i+1) 1 (i) 2 (j) 2
j=1 j=i+2
i−1 n
=(cid:88) (y −µ∗+µ∗−µ′ )2+(y −µ∗+µ∗−µ′ )2+(y −µ∗+µ∗−µ′ )2+ (cid:88) (y −µ∗+µ∗−µ′ )2
(j) 1 1 1 (i+1) 2 2 1 (i) 1 1 2 (j) 2 2 2
j=1 j=i+2
 
i−1 i−1 i−1
=(cid:88) (y (j)−µ∗ 1)2+(cid:88) 2(y (j)−µ∗ 1)(µ∗ 1−µ′ 1)+(cid:88) (µ∗ 1−µ′ 1)2 +(y (i+1)−µ∗ 2+µ∗ 2−µ′ 1)2
j=1 j=1 j=1
 
n n n
+ (cid:88) (y (j)−µ∗ 2)2+ (cid:88) 2(y (j)−µ∗ 2)(µ∗ 2−µ′ 2)+ (cid:88) (µ∗ 2−µ′ 2)2 +(y (i)−µ∗ 1+µ∗ 1−µ′ 2)2
j=i+2 j=i+2 j=i+2
 
i−1 i−1 i−1
=(cid:88) (y (j)−µ∗ 1)2+2(µ∗ 1−µ′ 1)(cid:88) (y (i)−µ∗ 1)+(cid:88) (µ∗ 1−µ′ 1)2 +(y (i+1)−µ∗ 2)2
j=1 j=1 j=1
+2(y −µ∗)(µ∗−µ′ )+(µ∗−µ′ )2
(i+1) 2 2 1 2 1
 
n i−1 n
+ (cid:88) (y (j)−µ∗ 2)2+2(µ∗ 2−µ′ 2)(cid:88) (y (j)−µ∗ 2)+ (cid:88) (µ∗ 2−µ′ 2)2 +(y (i)−µ∗ 1)2
j=i+2 j=1 j=i+2
+2(y −µ∗)(µ∗−µ′ )+(µ∗−µ′ )2 (21)
(i) 1 1 2 1 2
 
i i−1 i−1
=(cid:88) (y (j)−µ∗ 1)2+2(µ∗ 1−µ′ 1)(cid:88) (y (i)−µ∗ 1)+(µ∗ 2−µ′ 1)2+(cid:88) (µ∗ 1−µ′ 1)2 
j=1 j=1 j=1
 
n i−1 n
+ (cid:88) (y (j)−µ∗ 2)2+2(µ∗ 2−µ′ 2)(cid:88) (y (j)−µ∗ 2)+(µ∗ 1−µ′ 2)2+ (cid:88) (µ∗ 2−µ′ 2)2 
j=i+1 j=1 j=i+2
(22)
31Luo and Li
 
i i−1
=(cid:88) (y (j)−µ∗ 1)2− 2
i
(cid:0) −y (i)+y (i+1)(cid:1)2 +(µ∗ 2−µ′ 1)2+(cid:88) (µ∗ 1−µ′ 1)2 
j=1 j=1
 
n n
+ (cid:88) (y (j)−µ∗ 2)2− n−2
i
(cid:0) −y (i+1)+y (i)(cid:1)2 +(µ∗ 1−µ′ 2)2+ (cid:88) (µ∗ 2−µ′ 2)2  (23)
j=i+1 j=i+2
We use red and blue colored fonts to show how we group the terms in formula, and note that
the red and blue parts are essentially the variances of P∗ and P∗, namely (cid:80) (y −µ∗)2 +
1 2 y ∈P∗ (j) 1
(j) 1
(cid:80) (y −µ∗)2,andweshowbelowthattherestpartisgreaterthanzero. Fromtheassumption
y ∈P∗ (j) 2
(j) 2
(for the last inequality) that n > 4,min(n−i,i) > 2, we have
i−1 (cid:18) (cid:19)
2(µ∗−µ′ )(cid:88) (y −µ∗) = 2 −1 ·(cid:0) −y +y (cid:1) (cid:0) −y +y (cid:1)
1 1 (i) 1 i (i) (i+1) (i) (i+1)
j=1
= −2 (cid:0) −y +y (cid:1)2 ≥ −(cid:0) −y +y (cid:1)2 (24)
i (i) (i+1) (i+1) (i)
i−1 (cid:18) (cid:19)
2(µ∗−µ′ )(cid:88) (y −µ∗) = 2 − 1 ·(cid:0) −y +y (cid:1) (cid:0) −y +y (cid:1)
2 2 (j) 2 n−i (i+1) (i) (i+1) (i)
j=1
= − 2 (cid:0) −y +y (cid:1)2 ≥ −(cid:0) −y +y (cid:1)2 (25)
n−i (i+1) (i) (i+1) (i)
Now we want to compare −2 (cid:0) −y +y (cid:1)2 and (µ∗−µ′)2. But from the sorted assumption and
i (i) (i+1) 2 1
(24), µ′ ≤ y < y ≤ µ∗,
1 (i) (i+1) 2
−2 (cid:0) −y +y (cid:1)2 +(µ∗−µ′ )2 ≥ −(cid:0) −y +y (cid:1)2 +(µ∗−µ′ )2 ≥ 0 (26)
i (i) (i+1) 2 1 (i) (i+1) 2 1
Similarly, we can compare − 2 (cid:0) −y +y (cid:1)2 and (µ∗−µ′)2 where we use (25) and µ∗ ≤ y <
n−i (i+1) (i) 1 2 1 (i)
y ≤ µ′:
(i+1) 2
− 2 (cid:0) −y +y (cid:1)2 +(µ∗−µ′ )2 ≥ −(cid:0) −y +y (cid:1)2 +(µ∗−µ′ )2 ≥ 0 (27)
n−i (i+1) (i) 1 2 (i) (i+1) 2 1
Using both (26) and (27) in (23), we have proven that
(cid:88) (y −µ′ )2+ (cid:88) (y −µ′ )2 ≥ (cid:88) (y −µ∗)2+ (cid:88) (y −µ∗)2.
(j) 1 (j) 2 (j) 1 (j) 2
y (j)∈P 1′ y (j)∈P 2′ y (j)∈P 1∗ y (j)∈P 2∗
This means that switching y and y indeed reduces the total in-group variances. For more
(i) (i+1)
general situations, given two partitions P ,P of fixed sizes, and assume µ < µ . We can first sort
1 2 1 2
responses and find any pair of responses (y ,y ) such that y ∈ P , y ∈ P and y > y . We put
α β α 1 β 2 α β
y into P and y into P and repeat the argument above to show that the in-group variances for
α 2 β 1
both partition group decreases.
Similarly, assuming that the variance of P∗ is strictly larger than the variance of P∗∗, we can
1 1
prove that another global minimum of the loss function is given by assuming partitions of P∗∗ and
1
P∗∗. The key observation is that, the loss can be considered as a function of two sets P′,P′ and
2 1 2
there are two local minima attained by P∗,P∗ or P∗∗,P∗∗. The above arguments only prove that
1 2 1 2
P∗,P∗ and P∗∗,P∗∗ both attain local minima, and they are the only possible local minima.
1 2 1 2
32Ranking Perspective for Tree-based Symbolic Regressions
Appendix B. Proof of Proposition 7
Proof Without loss of generality, we assume that the LHS takes the ordered form (cid:80)n left(y −
i=1 (i)
µC,k)2+(cid:80)n (y −µC,k)2 where n is the number of observations in the left node.
L i=n left+1 (i) R left
(cid:88)n n (cid:88)left (cid:88)n
(y −µ♯)2 = (y −µ♯)2+ (y −µ♯)2
(i) (i) (i)
i=1 i=1 i=n +1
left
=
n (cid:88)left
(y −µC,k +µC,k −µ♯)2+
(cid:88)n
(y −µC,k +µC,k −µ♯)2
(i) L L (i) R R
i=1 i=n +1
left
=
n (cid:88)left
(y −µC,k)2+
(cid:88)n
(y
−µC,k)2+n (cid:88)left
(µC,k −µ♯)2+
(cid:88)n
(µC,k −µ♯)2
(i) L (i) R L R
i=1 i=n +1 i=1 i=n +1
left left
+2n (cid:88)left
(y −µC,k)(µC,k −µ♯)+2
(cid:88)n
(y −µC,k)(µC,k −µ♯)
(i) L L (i) R R
i=1 i=n +1
left
(cid:124) (cid:123)(cid:122) (cid:125) (cid:124) (cid:123)(cid:122) (cid:125)
=0 =0
=
n (cid:88)left
(y −µC,k)2+
(cid:88)n
(y
−µC,k)2+n (cid:88)left
(µC,k −µ♯)2+
(cid:88)n
(µC,k −µ♯)2
(i) L (i) R L R
i=1 i=n +1 i=1 i=n +1
left left
We attained the desired inequality by dropping the third and fourth summation in the last equality.
Appendix C. Proof of Proposition 8
Proof As the statement of the proposition, we can consider three cases as follows, with an illus-
trative reference to Figure 3. The corresponding three cases for (1) both of them have 0 pre-image
(2) both of them have 1 pre-image (3) one of them have 0 pre-image and the other has 1 pre-image,
are detailed as follows:
1. θ has 0 pre-image of C on I; θ has 0 pre-image of C on I. Then, on the refined interval
1 1 2 2
I ∈ I , either θ (u) ≤ C or θ (u) > C , for ∀u ∈ I. Note that pre-image is well-defined
1∩2 1 1 1 1
when the θ (and θ ) is restricted on a refined interval I ∈ I . This indicates that over
1 2 1∩2
I the splitting value corresponding to θ−1(C ) will not separate any x ∈ I. Similarly, the
1 1 k
splitting value for x corresponding to θ−1(C ) will not separate any univariate inputs x ∈ I.
2 2 k
The corresponding terms in the principal decision ratios 9 becomes:
exp(cid:0) −(cid:80)n (y −µ1)21(z ≤C )1(x ∈I)−(cid:80)n (y −µ1)21(z >C )1(x ∈I)(cid:1)
τ | = i=1 i L i,k1 1 i,k1 i=1 i R i,k1 1 i,k1 (28)
I exp(cid:0) −(cid:80)n (y −µ2)21(z ≤C )1(x ∈I)−(cid:80)n (y −µ2)21(z >C )1(x ∈I)(cid:1)
i=1 i L i,k2 2 i,k2 i=1 i R i,k2 2 i,k2
(cid:16) (cid:17)
exp −(cid:80)n1 (y −µ1)21(z ≤C )1(x ∈I)−(cid:80)n (y −µ1)21(z >C )1(x ∈I)
=
i=1 (i) L (i),k1 1 (i),k1 i=n1+1 (i) R (i),k1 1 (i),k1
(cid:16) (cid:17)
exp −(cid:80)n2 (y −µ2)21(z ≤C )1(x ∈I)−(cid:80)n (y −µ2)21(z >C )1(x ∈I)
i=1 (i) L (i),k2 2 (i),k2 i=n2+1 (i) R (i),k2 2 (i),k2
(29)
Since there are 0 pre-images for θ over I, either 1(z ≤ C )1(x ∈ I) ≡ 0 or 1(z >
1 i,k1 1 i,k1 i,k1
C )1(x ∈ I) ≡ 0; similarly since there are 0 pre-images for θ over I, either 1(z ≤
1 i,k1 2 i,k2
C )1(x ∈ I) ≡ 0or1(z > C )1(x ∈ I) ≡ 0. Inthecase1(z ≤ C )1(x ∈ I) ≡ 0
2 i,k2 i,k2 2 i,k2 i,k1 1 i,k1
33Luo and Li
and 1(z ≤ C )1(x ∈ I) ≡ 0, 29 becomes
i,k2 2 i,k2
exp(cid:0) −(cid:80)n (y −µ1)21(z > C )1(x ∈ I)(cid:1)
τ | = i=n1+1 (i) R (i),k1 1 (i),k1
I exp(cid:0) −(cid:80)n (y −µ2)21(z > C )1(x ∈ I)(cid:1)
i=n2+1 (i) R (i),k2 2 (i),k2
exp(cid:0) −(cid:80)n
(y
−µ1)2(cid:1)
= i=1 (i) R
exp(cid:0) −(cid:80)n
(y
−µ2)2(cid:1)
i=1 (i) R
exp(cid:0) −(cid:80)n
(y
−µ1)2(cid:1)
= i=1 i R
exp(cid:0) −(cid:80)n
(y
−µ2)2(cid:1)
i=1 i R
= 1 by definition, µ1 = µ2.
R R
The case 1(z > C )1(x ∈ I) ≡ 0 and 1(z > C )1(x ∈ I) ≡ 0 follows the same
i,k1 1 i,k1 i,k2 2 i,k2
argument. In the case 1(z ≤ C )1(x ∈ I) ≡ 0 and 1(z > C )1(x ∈ I) ≡ 0 we
i,k1 1 i,k1 i,k2 2 i,k2
have
exp(cid:0) −(cid:80)n (y −µ1)21(z > C )1(x ∈ I(cid:1)
τ | = i=n1+1 (i) R (i),k1 1 (i),k1
I exp(cid:0) −(cid:80)n2 (y −µ2)21(z ≤ C )1(x ∈ I)(cid:1)
i=1 (i) L (i),k2 2 (i),k2
exp(cid:0) −(cid:80)n
(y
−µ1)2(cid:1)
= i=1 (i) R
exp(cid:0) −(cid:80)n
(y
−µ2)2(cid:1)
i=1 (i) L
exp(cid:0) −(cid:80)n
(y
−µ1)2(cid:1)
= i=1 i R
exp(cid:0) −(cid:80)n
(y
−µ2)2(cid:1)
i=1 i L
= 1 by definition, µ1 = µ2.
R L
Note that this case we also have all observations allocated to right and left nodes under two
transforms. The case 1(z > C )1(x ∈ I) ≡ 0 and 1(z ≤ C )1(x ∈ I) ≡ 0
i,k1 1 i,k1 (i),k2 2 i,k2
follows the same argument.
2. θ has 1 pre-image of C on I; θ has 0 pre-image of C on I. (The discussion of the case: θ
1 1 2 2 1
has0pre-imageofC onI; θ has1pre-imageofC onI,issimilar. ) Firstnotethatµ2 = µ2
1 2 2 L R
as θ has 0 pre-image but µ1 ̸= µ1, yielding that the corresponding principal decision ratios:
2 L R
exp(cid:0) −(cid:80)n (y −µ1)21(z ≤C )1(x ∈I)−(cid:80)n (y −µ1)21(z >C )1(x ∈I)(cid:1)
τ | = i=1 (i) L (i),k1 1 (i),k1 i=1 (i) R (i),k1 1 (i),k1 ,
I exp(cid:0) −(cid:80)n (y −µ2)21(z ≤C )1(x ∈I)(cid:1)
i=1 (i) L (i),k2 2 (i),k2
exp(cid:0) −(cid:80)n (y −µ1)21(z ≤C )−(cid:80)n (y −µ1)21(z >C )1(x ∈I)(cid:1)
or i=1 (i) L (i),k1 1 i=1 (i) R (i),k1 1 (i),k1 .
exp(cid:0) −(cid:80)n (y −µ2)21(z >C )1(x ∈I)(cid:1)
i=1 (i) R (i),k2 2 (i),k2
Therefore, the ratio is larger for θ by Proposition 7 and we should always prefer θ because
1 1
the fit for (x,y) using two constants µ1 · 1(z ≤ C ) and µ1 · 1(z > C ) defined
L (i),k1 1 R (i),k1 1
on I can not be worse than the fit for (x,y) using one constant µ2 · 1(z ≤ C ) (or
L (i),k2 2
µ2 ·1(z > C )), as stated in the next Proposition 7.
R (i),k2 2
3. θ has 1 pre-image of C on I; θ has 1 pre-image of C on I. Then, on the refined interval
1 1 2 2
I ∈ I , 29 becomes
1∩2
(cid:16) (cid:17)
exp −(cid:80)n1 (y −µ1)21(x ≤θ−1C )1(x ∈I)−(cid:80)n (y −µ1)21(x >θ−1C )1(x ∈I)
τ | =
i=1 (i) L (i),k1 1 1 (i),k1 i=n1+1 (i) R (i),k1 1 1 (i),k1
I exp(cid:16) −(cid:80)n2 (y −µ2)21(x ≤θ−1C )1(x ∈I)−(cid:80)n (y −µ2)21(x >θ−1C )1(x ∈I)(cid:17)
i=1 (i) L (i),k2 2 2 (i),k2 i=n2+1 (i) R (i),k2 2 2 (i),k2
From Corollary 4, we know that both the numerator and denominator of τ | are fixed
I
size oracle 2-partitions on y. However, we need to decide which of x ≤ θ−1C and
(i),k1 1 1
x ≤ θ−1C gives us a larger sum of variances. From Lemma 15, we know that it reduces
(i),k2 2 2
to compare n−1 possible values of sum of variances (corresponding to n−1 different split
values).
34Ranking Perspective for Tree-based Symbolic Regressions
Appendix D. Proof of Theorem 10
Proof Using Theorem 3 in Cossock and Zhang (2006), we know that for the full dataset X(N), the
following holds:
 1/2
N
4 (cid:88)
T (r B)−T (r c,K) ≤ √  (f B(x j)−f c,K(x j))2  , (30)
N
j=1
where r is induced by the f ∈ G ⊂ G constructed from a CART. This inequality suggests
c,K c,K 0
that the a scoring function f approximated by CART of depth K ≥ 1 with low approximating
c,K
L error to the Bayes scoring function f can attain low approximating ranking error T as well.
2 B
Assumingthatthetruesignalf ∈ G andtheCART-inducedf ∈ G ⊂ G asinthestatement
B 0 c,K 0
of Theorem 10, now we apply Theorem 4.3 in Klusowski and Tian (2024). We can assert that the
CART prediction f constructed from splitting a complete binary tree using CART loss (10) of
c,K
depth K satisfy the following universal consistency as:
(cid:40) (cid:41)
(cid:16) (cid:17) ∥g∥2 2Klog(Nd)
E ∥f −f ∥2 ≤ 2inf ∥f −g∥2+ TV +C , (31)
(X(N),Y(N)) B c,K
g∈G
B
K +3
B
N
where the constant C depends on the uniform bound on the total variations along coordinates
B
f : R → R as assumed. Applying Markov’s inequality yields
B,i
(cid:16) (cid:17)
E ∥f −f ∥2
P
(cid:16)
∥f −f ∥2 > α
(cid:17)
≤
(X(N),Y(N)) B c,K
(32)
(X(N),Y(N)) B c,K 1
α
1
(cid:40) (cid:41)
2 ∥g∥2 2Klog2N log(Nd)
≤ inf ∥f −g∥2+ TV +C , (33)
B B
α 1 g∈G K +3 N
for any α > 0. This means that the tree-like functions in class G can approximate the Bayes score
1
function well enough, and bounded from above.
Since the result in (33) considers the exact norm, we need one more step to connect the exact
norm to the empirical norm used in the statement of Theorem 3 in Cossock and Zhang (2006). To
attain this, we invoke classical results from empirical process regarding the convergence of empirical
norm(LedouxandTalagrand,1991). AccordingtoTheorem2.2invandeGeer(2014),theempirical
norm ∥f∥2 := 1 (cid:80)N f(x )2 converges to the exact ℓ2-norm ∥f∥2 = (cid:82) f(u)2dP (u) at a rate of
N N i=1 i X
(cid:16) (cid:17)
O √1 for any f ∈ G
0
from a possibly larger class than tree-like functions G. In particular, we
N
have that the empirical norm of f ∈ G can be approximated as well
0
(cid:32) (cid:33)
2R J (2R ,G )
E sup |∥f∥ −∥f∥| ≤ ∞ √0 2 0 , (34)
(X(N),Y(N))
f∈G0
N
N
which implies
(cid:32) (cid:33) E (cid:0) sup |∥f∥ −∥f∥|(cid:1)
P sup |∥f∥ −∥f∥| > α ≤
(X(N),Y(N)) f∈G0 N
(35)
(X(N),Y(N)) N 2
α
f∈G0 2
2R J (2R ,G )
≤ ∞ 0 √ 2 0 , (36)
α N
2
35Luo and Li
for any α > 0 by applying Markov’s inequality. Applying the inequality in (30), we obtain
2
P (T (r )−T (r ) > ε )
(X(N),Y(N)) B c,K N
 
 1/2
N
4 (cid:88)
≤ P (X(N),Y(N)) √
N
 (f B(x j)−f c,K(x j))2  > ε N

j=1
   
1 (cid:88)N ε2
= P (X(N),Y(N))
N
 (f B(x j)−f c,K(x j))2  > 1N 6
j=1
= P (X(N),Y(N))(cid:18) ∥f c,K −f B∥ N > ε 12 N 6 and (cid:12) (cid:12)∥f c,K −f B∥ N −∥f c,K −f B∥(cid:12) (cid:12) ≤ ε 32 N 2(cid:19)
+P (X(N),Y(N))(cid:18) ∥f c,K −f B∥ N > ε 12 N 6 and (cid:12) (cid:12)∥f c,K −f B∥ N −∥f c,K −f B∥(cid:12) (cid:12) > ε 32 N 2(cid:19)
=: A +A . (37)
1 2
ForthefirsttermA , itrepresentshowwellthetree-likefunctionclassG canapproximatetheBayes
1
scoring functions. We substitute α = ε2 N into (33) and bound it by
1 32
(cid:32) (cid:33) (cid:40) (cid:41)
(cid:20) ε2 (cid:21)2 1024 ∥g∥2 2Klog2N log(Nd)
P ∥f −f ∥2 > N ≤ ·inf ∥f −g∥2+ TV +C .
(X(N),Y(N)) B c,K 32 ε4 g∈G B K +3 B N
N
For the second term A , it represents how fast empirical norm with respect to X converges when
2
the Bayesian scoring function is in a potentially larger class G (but the tree approximating scoring
0
functionf ∈ G). Thisenclosedeventhasanintersectioncomponentinformof (36)withα = ε2 N.
c,K 2 16
Therefore, A
2
≤ 32R∞ εJ 20√(2 NR2,G0) .
Combining these twNo bounds yields
P (T (r )−T (r ) > ε )
(X(N),Y(N)) B c,K N
(cid:40) (cid:41)
1024 ∥g∥2 2Klog2N log(Nd) 32R J (2R ,G )
≤ · inf ∥f −g∥2+ TV +C + ∞ 0 √ 2 0 .
ε4 g∈G B K +3 B N ε2 N
N N
This completes the proof of the inequality (16).
When G = G, substituting g = f into the upper bound (16) leads to the simplified upper
0 B
bound (17).
Appendix E. Proof of Theorem 13
Proof Using Theorem 3 in Cossock and Zhang (2006), we know that for the full dataset X(N) we
have the bound
 1/2
N
4 (cid:88)
T (r B)−T (r f) ≤ √  (f B(x j)−f(x j))2  (38)
N
j=1
36Ranking Perspective for Tree-based Symbolic Regressions
Therefore, it justifies that the a scoring function f with low approximating L error to the Bayes
2
scoring function f can attain low approximating ranking error T as well.
B
Ročková and Saha (2019) presents the posterior concentration of BART in their Theorem 7.1
(adapted to our notations above). Namely, when the f is ν-Holder continuous with 0 < ν ≤ 1 and
∥f ∥ log1/2N, assume a regular design X(N) = {x ,x ,··· ,x } ⊂ Rd,d ≲ log1/2N for the
B ∞ 1 2 N
features (cid:62). With a fixed number of trees and node η splitting probability p split(η) = αdepth(η),α ∈
(cid:2)1, 1(cid:1)proportionaltothedepthofnodeη withrespecttoeachtree,wehaveposteriorconcentration
N 2
results from BART, when the regression aims at approximating the optimal f . Precisely we assert
B
that the BART posterior measure Π(· | y ,y ,··· ,y ) concentrates on all scoring functions f that
1 2 N
ismeasurablewithrespecttotheproductσ-fieldF generatedbythejointmeasureofy ,y ,··· ,y .
1 2 N
That is, when the approximating scoring function f is approximated by BART, we have
(cid:89)
(f ∈ F : ∥f(x)−f (x)∥ > M ε |y ,y ,··· ,y ) → 0,as N → ∞ for all x ∈ X(N)
B N N N 1 2 N
for any sequence M → 0 in the joint probability of y ,y ,··· ,y , as the sample size N and the
N 1 2 N
dimensionality d → ∞ and ε = N−α/(2α+d)log1/2N. Here we use the empirical norm definition
N
∥f∥2 := 1 (cid:80)N f(x )2 and (38) in the second line.
N N i=1 i
(cid:89)
(f ∈ F : T (r )−T (r ) > M ε |y ,y ,··· ,y )
B f N N 1 2 N
 (cid:12) 
 1/2 (cid:12)
N
(cid:89) 4 (cid:88) (cid:12)
≤  f ∈ F : √  (f B(x j)−f(x j))2  > M Nε N(cid:12) (cid:12)y 1,y 2,··· ,y N

N (cid:12)
j=1 (cid:12)
   (cid:12) 
= (cid:89) f ∈ F : 1 (cid:88)N (f B(x j)−f(x j))2  > M N2ε2 N(cid:12) (cid:12) (cid:12)y 1,y 2,··· ,y N
N 16 (cid:12)
j=1 (cid:12)
=
(cid:89)(cid:16)
f ∈ F : ∥f −f ∥ > M♯ ε
(cid:12)
(cid:12)y ,y ,··· ,y
(cid:17)
→ 0,as the design size N → ∞.
B N N N(cid:12) 1 2 N
where M♯ = M N2εN can be chosen to be any sequence converging to 0 as N → ∞. The key of this
N 16
argument is to convert the error measured in T metric to the empirical norm.
Appendix F. Proof of Lemma 15
Proof Either swapping (y ,y ) or (y ,y ) leaves us with no reversed pairs. Thus, it suffices to
α γ β γ
note that if we swap (y ,y ) the RHS of (26) and (27) become
α γ
−(−y +y )2+(µ∗ −µ′ )2 ≥ 0,
γ α 2,(α,γ) 1
(cid:16) (cid:17) (cid:16) (cid:17)
where the µ′ = 1 y +y +(cid:80) y and µ∗ = 1 y +(cid:80) y . Similarly,
1 n1 α β y∈P1,y̸=yα,y β 2,(α,γ) n2 α y∈P2,y̸=yγ
if we swap (y ,y ) the RHS of (26) and (27) become
β γ
−(−y +y )2+(µ∗ −µ′ )2 ≥ 0,
γ β 2,(β,γ) 1
(cid:16) (cid:17) (cid:16) (cid:17)
where the µ′ = 1 y +y +(cid:80) y and µ∗ = 1 y +(cid:80) y . It follows
1 n1 α β y∈P1,y̸=yα,y β 2,(β,γ) n2 β y∈P2,y̸=yγ
that µ∗ > µ∗ > µ′, and the assumption y > y > y that
2,(α,γ) 2,(β,γ) 1 α β γ
−(−y +y )2+(µ∗ −µ′ )2+(−y +y )2−(µ∗ −µ′ )2
γ α 2,(α,γ) 1 γ β 2,(β,γ) 1
(cid:104) (cid:105) (cid:104) (cid:105)
= (−y +y )2−(−y +y )2 + (µ∗ −µ′ )2−(µ∗ −µ′ )2 ≥ 0.
γ β γ α 2,(α,γ) 1 2,(β,γ) 1
37Luo and Li
This means that the reduction (24)+(25) is larger if we swap (y ,y ).
α γ
Appendix G. Proof of Proposition 18
Proof The statistics T (g) in (18) can be written as
0
(cid:88)(cid:8) (cid:0) (cid:1) (cid:12) (cid:12) (cid:0) (cid:1) (cid:12) (cid:12) (cid:9)
1 x
π(1)
≥ x
π(2)
·(cid:12)y π(1)−y π(2)(cid:12)·1(y
π(1)
< y π(2))+1 x
π(1)
< x
π(2)
·(cid:12)y π(1)−y π(2)(cid:12)·1(y
π(1)
≥ y π(2)) .
π
Without loss of generality, we next consider one summation involving 1(cid:0) f(x ) ≥ f(x )(cid:1), and
π(1) π(2)
the argument remains the same for the other summation. Now since X ,k ∈ {1,··· ,d} is inactive,
k
the distribution of y is independent of the distribution of f(X ). Based on this observation, we can
k
remove the conditioning inside the expectation with respect to X in the first line:
k
E
X
k,yT 0(g) = n(n2
−1)
·(cid:2)P
X
k(cid:0) f(x π(1)) ≥ f(x π(2))(cid:1)(cid:3) ·(cid:88)(cid:2)E
y|X
k(cid:12) (cid:12)y π(1)−y π(2)(cid:12) (cid:12)·1(y
π(1)
< y π(2))(cid:3)
π
=
n(n2
−1)
·(cid:2)P
X
k(cid:0)
f(x π(1)) ≥ f(x
π(2))(cid:1)(cid:3) ·(cid:88)(cid:2)E y(cid:12)
(cid:12)y π(1)−y
π(2)(cid:12)
(cid:12)·1(y
π(1)
< y
π(2))(cid:3)
π
=
(cid:2)P
X
k(cid:0)
f(x π(1)) ≥ f(x
π(2))(cid:1)(cid:3)
·
n(n2
−1)
·
1
2
(cid:88)(cid:2)E y(cid:12)
(cid:12)y π(1)−y
π(2)(cid:12) (cid:12)(cid:3)
π
≍
O(1)·(cid:2)P (cid:0)
f(x ) ≥ f(x
)(cid:1)(cid:3)
> 0,
X k π(1) π(2)
where the second-to-last line uses an symmetric argument as
(cid:88)(cid:2)E y(cid:12)
(cid:12)y π(1)−y
π(2)(cid:12)
(cid:12)·1(y
π(1)
< y
π(2))(cid:3)
=
(cid:88)(cid:2)E y(cid:12)
(cid:12)y π(1)−y
π(2)(cid:12)
(cid:12)·1(y
π(1)
> y
π(2))(cid:3)
,
π π
and their sum is (cid:80) π(cid:2)E y(cid:12) (cid:12)y π(1)−y π(2)(cid:12) (cid:12)(cid:3) . This proves part (i).
On the other hand, if we take the expectation with respect to all y | X:
E X,yT 0(g) = n(n2
−1)
·E X,y(cid:88) 1(cid:0) f(x π(1)) ≥ f(x π(2))(cid:1) ·(cid:12) (cid:12)y π(1)−y π(2)(cid:12) (cid:12)·1(y
π(1)
< y π(2))
π
= n(n2
−1)
·E XE y|X(cid:88) 1(cid:0) f(x π(1)) ≥ f(x π(2))(cid:1) ·(cid:12) (cid:12)y π(1)−y π(2)(cid:12) (cid:12)·1(y
π(1)
< y π(2))
π
=
n(n2
−1)
·(cid:2)E X1(cid:0)
f(x π(1)) ≥ f(x
π(2))(cid:1)(cid:3) ·(cid:88)(cid:2)E y|X(cid:12)
(cid:12)y π(1)−y
π(2)(cid:12)
(cid:12)·1(y
π(1)
< y
π(2))(cid:3)
π
=
n(n2
−1)
·(cid:2)P X(cid:0)
f(x π(1)) ≥ f(x
π(2))(cid:1)(cid:3) ·(cid:88)(cid:2)E y|X(cid:12)
(cid:12)y π(1)−y
π(2)(cid:12)
(cid:12)·1(y
π(1)
< y
π(2))(cid:3)
.
π
(39)
38Ranking Perspective for Tree-based Symbolic Regressions
if there exists such a g that g(x ) ≥ g(x ) ⇔y ≥ y then
1 2 1 2
(39) = (cid:2)P X(cid:0) g(x π(1)) ≥ g(x π(2))(cid:1)(cid:3) ·(cid:88) n(n2
−1)
·(cid:2)E y|X(cid:12) (cid:12)y π(1)−y π(2)(cid:12) (cid:12)·1(cid:0) g(x π(1)) < g(x π(2))(cid:1)(cid:3)
π
=
(cid:2)P (cid:0)
g(x ) ≥ g(x
)(cid:1)(cid:3) ·(cid:88) 2 ·(cid:2)E (cid:0)
y −y
(cid:1) ·1(cid:0)
g(x ) < g(x
)(cid:1)(cid:3)
X π(1) π(2) n(n−1) y|X π(2) π(1) π(1) π(2)
π
=
(cid:2)P (cid:0)
g(x ) ≥ g(x
)(cid:1)(cid:3)
·
2
X π(1) π(2) n(n−1)
(cid:40) (cid:41)
× (cid:88) E y ·1(cid:0) g(x ) < g(x )(cid:1) −(cid:88) E y ·1(cid:0) g(x ) < g(x )(cid:1)
y|X π(2) π(1) π(2) y|X π(1) π(1) π(2)
π π
Fortheothersummationwehaveequalvalue(cid:80) E y ·1(cid:0) g(x ) ≥ g(x )(cid:1) −(cid:80) E y ·
π y|X π(2) π(1) π(2) π y|X π(1)
1(cid:0) g(x ) ≥ g(x )(cid:1) and cancels out the last row of expressions. This proves part (ii).
π(1) π(2)
39