Retrieval-Enhanced Knowledge Editing for
Multi-Hop Question Answering in Language Models
YuchengShi QiaoyuTan XuanshengWu
UniversityofGeorgia NewYorkUniversity UniversityofGeorgia
Athens,Georgia,USA NewYork,USA Athens,Georgia,USA
yucheng.shi@uga.edu qiaoyu.tan@nyu.edu yd6eb@virginia.edu
ShaochenZhong KaixiongZhou NinghaoLiu
RiceUniversity NorthCarolinaStateUniversity UniversityofGeorgia
Houston,Texas,USA Raleigh,NorthCarolina,USA Athens,Georgia,USA
shaochen.zhong@rice.edu kzhou22@ncsu.edu ninghao.liu@uga.edu
ABSTRACT 1 INTRODUCTION
LargeLanguageModels(LLMs)haveshownproficiencyinquestion- Largelanguagemodels(LLMs)exhibitimpressiveperformancein
answeringtasksbutoftenstruggletointegratereal-timeknowledge question-answering(QA)tasks[25,31,33],yettheyfaceasignifi-
updates,leadingtopotentiallyoutdatedorinaccurateresponses. cantchallenge:theirstaticknowledgebasecannotbeeasilyupdated
Thisproblembecomesevenmorechallengingwhendealingwith inreal-time,leadingtotheriskofgeneratingoutdatedorincorrect
multi-hopquestionssincetheyrequireLLMstoupdateandinte- responses.Toovercomethisissue,modeleditinghasbeenproposed
gratemultipleknowledgepiecesrelevanttothequestions.Totackle forpre-trainedLLMstoaligntheiroutputwithup-to-dateknowl-
theproblem,weproposetheRetrieval-AugmentedmodelEditing edge[6].Previouseditingmethodshaveshowntobeeffectivein
(RAE)frameworktailoredformulti-hopquestionanswering.RAE updatinganswerstosingle-hopquestions[7,17,18].However,it
first retrieves edited facts and then refines the language model remainsachallengingtaskformodeleditingtohandlemulti-hop
throughin-contextlearning.Specifically,ourretrievalapproach, questions,whichiscrucialtoevaluatemachine’scomprehension
basedonmutualinformationmaximization,leveragesthereasoning andreasoningabilities.
abilitiesofLLMstoidentifychainfactsthatnaïvesimilarity-based Answeringmulti-hopquestionsrequirestheintegrationofmul-
searchesmightmiss.Additionally,ourframeworkincorporatesa tiplepiecesofknowledge.Forinstance,toanswerthequestion
pruningstrategytoeliminateredundantinformationfromthere- "Whatisthenationalityoftheauthorof‘HarryPotter’?",wemust
trievedfacts,whichenhancestheeditingaccuracyandmitigates linktwopiecesofknowledge:"(HarryPotter,author,J.K.Rowling)"
thehallucinationproblem.Ourframeworkissupportedbytheoreti- and"(J.K.Rowling,citizenof,UnitedKingdom)",whichcollectively
caljustificationforitsfactretrievalefficacy.Finally,comprehensive constituteafactchain[46].Ifweconductacounterfactualediton
evaluationacrossvariousLLMsvalidatesRAE’sabilityinproviding thefirstfact,i.e.,“J.K.Rowling"isreplacedby“StephenKing",the
accurateanswerswithupdatedknowledge. subsequentknowledgemustbeadjustedaccordingly,resultingin
atotallydifferentfactchain:"(HarryPotter,author,StephenKing),
KEYWORDS
(StephenKing,citizenof,UnitedStates)."Here,weusecounterfac-
tualeditstosimulatereal-worldupdates.Successfulmodelediting
Modelediting,questionanswering,retrieval-augmentedgeneration
formulti-hopquestionsrequiresthattheeditedLLMsidentifyand
adopttheupdatedknowledgetoderivethefinalanswers.
ACMReferenceFormat: Unfortunately,existingeditingmethodsareoftenlimitedinhan-
YuchengShi,QiaoyuTan,XuanshengWu,ShaochenZhong,KaixiongZhou,
dlingmulti-hopquestions.First,methodsthataltermodelparame-
andNinghaoLiu.2018.Retrieval-EnhancedKnowledgeEditingforMulti-
ters,includingfine-tuning[47],locate-then-edit[17,18],andmeta-
HopQuestionAnsweringinLanguageModels.InProceedingsofMake
learning[19],sufferfromthecatastrophicforgettingissue[6,7,43],
suretoenterthecorrectconferencetitlefromyourrightsconfirmationemai
(Conferenceacronym’XX).ACM,NewYork,NY,USA,11pages.https://doi. wherethepreviouslyencodedknowledgecouldbelostafterediting.
org/XXXXXXX.XXXXXXX Second,methodsthatdependontrainingauxiliarymodelsalsofall
shortinthesescenarios.Theauxiliarymodelsareusuallysmaller
languagemodels,whichlackthenecessaryreasoningcapabilityto
infercorrectanswers[20].Incontrast,athirdcategoryofmethods,
Permissiontomakedigitalorhardcopiesofallorpartofthisworkforpersonalor basedonRetrieval-AugmentedGeneration(RAG),modifiesmodel
classroomuseisgrantedwithoutfeeprovidedthatcopiesarenotmadeordistributed
outputsinamoreeffectivemanner[6,45,46].Thesemethodsinte-
forprofitorcommercialadvantageandthatcopiesbearthisnoticeandthefullcitation
onthefirstpage.CopyrightsforcomponentsofthisworkownedbyothersthanACM grateupdatedknowledgedirectlyintothemodelprompt,guiding
mustbehonored.Abstractingwithcreditispermitted.Tocopyotherwise,orrepublish, LLMs through in-context learning [45]. RAG-based approaches
topostonserversortoredistributetolists,requirespriorspecificpermissionand/ora
fee.Requestpermissionsfrompermissions@acm.org. haveshownsignificantadvantagessincetheyareimmunetothe
Conferenceacronym’XX,June03–05,2018,Woodstock,NY catastrophicforgettingproblem,andtheeditingprocesscanbecon-
©2018AssociationforComputingMachinery. ductedonthefly[9,34].Yet,theapplicationofRAGtomodelediting
ACMISBN978-1-4503-XXXX-X/18/06...$15.00
https://doi.org/XXXXXXX.XXXXXXX withmulti-hopquestionsstillremainslargelyunderexplored.
4202
raM
82
]LC.sc[
1v13691.3042:viXraConferenceacronym’XX,June03–05,2018,Woodstock,NY Shi,etal.
Original Knowledge: andcauseserious“hallucination”problems[15,16],whereLLMs
[1](Harry Potter, author, J. K. Rowling) [2](Stephen King, citizen of, United States)
willgeneratefactuallyincorrectcontentbasedonnoiseinsteadof
[3](Misery, author, Stephen King) [4](Harry Potter, citizen of, United Kingdom)
relevantfacts.Thus,itiscrucialtoreducenoisebeforeapplying
Counterfactual Edit
retrievedfactstoprompts.
Edited Facts Memory:
[1](Harry Potter, author, Stephen King) [2](Stephen King, citizen of, Canada) To bridge the gap, we propose a novel Retrieval-Augmented
[3](Misery, author, Harry Harrison) [4](Harry Potter, citizen of, United States) model Editing(RAE) framework,wherewe firstretrieve edited
Single Hop Question 2-Hop Question factsandthenrefinethetargetmodelwiththesefactsthroughin-
Who is Harry Potter’s author? What is the nationality of Harry Potter’s author? contextlearning.Toaddressthefirstchallenge,weproposeMutual-
Top-2 Similarity Search information(MI)Maximizationforeditedfactsretrieval.Here,
MIquantifiesthesharedinformationbetweenthetargetquestion
Edited Edited
Memory [1] (Harry [4](Harry Memory andeditedfacts.EditedfactswithhigherMIaremorerelevant.
[3] (Misery, Potter, Potter,
author, Harry author, citizen of, However,directlycomputingMIischallenging.Thus,wedecom-
Target facts within Harrison) Stephen King) U Stn ai tt ee sd ) Target facts not within poseMIintoaseriesofconditionalprobabilities,andweutilizethe
retrieved Top-2: retrieved Top-2. next-wordpredictioncapabilitiesofpre-trainedLLMstoestimate
Retrieved facts: [1][3] Retrieved facts: [1][4]
Answer fact: [1] Target fact:[1][2] theseprobabilities.Inthisprocess,weusethetargetLLM’scon-
[2](Stephen King, Missing fact: [2]
citizen of, Canada) textualunderstandingandreasoningabilitiestoidentifynecessary
Naïve Retrieval
factsforsuccessfulmodelediting.Totacklethesecondchallenge,
Retrieval-Augmented Generation for Model Editing
weproposeUncertainty-basedRedundantFactsPruningby
utilizingLLMoutputconfidence.Specifically,itselectivelyretains
Edit Who is Harry Potter’s author? J. K. Rowling
factsthatincreasetheLLMs’confidenceinansweringeditedques-
Given fact [1][3], who is Harry Potter’s author? Stephen King tionsanddiscardsirrelevantinformation.Finally,wetheoretically
Edit justifytheformulationofourretrievalobjective.Overall,ourmain
What is the nationality of Harry Potter’s author? United Kingdom
contributionsarelistedbelow:
Given fact [1][4], what is the nationality of Harry
Potter’s author? US Should be Canada • Weintroduceanovelfactretrievalapproachformulti-hopques-
tionsinmodelediting.Thisapproacheffectivelyharnessesthe
Figure 1: An example of the traditional similarity-based reasoningcapabilitiesofLLMstoretrievethemostrelevantmulti-
searchthatfailstoretrievethecorrectfactsforLLMediting. hopfactsforeachquestion.
• Weproposeaknowledgepruningstrategytoreducenoiseafter
theinitialretrieval,mitigatingthehallucinationproblem.Addi-
Furthermore,inpractice,modeleditingofteninvolveshandlinga tionally,weprovidetheoreticalanalysistojustifyourdesignfor
largevolumeofedits,whicharestoredasfacttripletsinamemory theretrievalobjective.
bank.Insuchcases,RAG-basedmulti-hopeditingisexpectedtore- • Weconductextensiveexperimentsonvariouslanguagemodelsin
trievethemostrelevantfactsforeachquestion.However,thistask differentsizestoverifytheeffectivenessofourproposedediting
ischallengingfortworeasons.First,extractingmulti-hopfacts method.Empiricalresultsdemonstratethesuperiorityofour
requirestheretrievertounderstandthecomplexconnections RAEframeworkcomparedtostate-of-artbaselines.
amongmulti-relationswithinthequestion.Anaïvedesignof
theretrieverisapplyingsimilarity-basedsearch[9,14,34]toobtain 2 PRELIMINARY:MODELEDITING
thetop-𝐾 factsthataremostsemanticallysimilartothequestion.
2.1 ModelEditingforSingle-hopQuestions
However,semanticsimilarityalonedoesnotguaranteethatthese
factscontainthenecessaryinformationtocorrectlyanswerthe In LLMs, a single model edit refers to updating a specific piece
offactualknowledge[18,19,45].Eachknowledgeisdefinedasa
question.WeillustratethisissuewithanexampleinFigure1.The
editedfactthatshouldberetrievedis“[2]”:(StephenKing,citizenof,
triplet𝛿 := (ℎ,𝑟,𝑡),whereℎ,𝑟,and𝑡 denotetheheadentity,the
relation, and the tail entity, respectively, such as (Harry Potter,
Canada),not“[4]”:(HarryPotter,citizenof,UnitedStates).Thelatter
author,J.K.Rowling).Aneditisdefinedaschangingthetailentity
fact“[4]”isretrievedbecauseitcontains“HarryPotter”and“citizen
of”thatresembleourquestion“WhatisthenationalityofHarry 𝑡 toanewentity𝑡′,i.e.,𝛿 →𝛿′ := (ℎ,𝑟,𝑡) → (ℎ,𝑟,𝑡′),where𝛿′
istheeditedknowledge.Let𝑞denotethelanguagemodel’sinput.
Potter’sauthor?".Despiteitshighersimilarityscore,thefact“[4]”
Thegoalofmodeleditingistomodifyatargetmodel𝑓 ,sothat
isactuallyirrelevanttothequestion.Therefore,effectiveretrieval 𝜃
requiresadeepunderstandingofthequestion,acapabilitythat
thenewmodel𝑓 𝜃′producesanoutput𝑓 𝜃′(𝑞)thatisalignedwiththe
goesbeyondexistingsimilarity-basedsearchmethods.Second,the
newfact𝛿′,where𝑓 𝜃′(𝑞)≠𝑓 𝜃(𝑞).Specifically,given𝑞= [ℎ;𝑟],the
modelisexpectedtooutput𝑡′ =𝑓′([ℎ;𝑟])ifℎ,𝑟 ∈𝛿′,where[;]is
retrievedknowledgecouldcontainredundantinformation 𝜃
anddegrademodeleditingperformance.Itisusuallyimprac- theconcatenationoperator.However,iftheinputquestionisnot
ticaltodeterminetheexactamountofinformationtoanswera relevanttotheedit,i.e.,ℎ∉𝛿′or𝑟 ∉𝛿′,themodelshouldoutput
specificquestion,soexistingretrievalmethodstendtoreturnan 𝑡 =𝑓′([ℎ;𝑟])thatreflectstheknowledgeofLLMsbeforeediting.
𝜃
extensivenumberoffacts[8,21,27,42]formorecomprehensive
coverage.Whilethesemethodsdoretrieverelevantfacts,theyalso 2.2 ModelEditingforMulti-hopQuestions
introduceredundantinformation.Itisworthnotingthatincorporat- Answering multi-hop questions presents a greater challenge. A
ingirrelevantknowledgeintotheLLMinputcouldmisleadmodels multi-hopquestionseekstoidentifyaspecifictailentity𝑡 based
𝑘Retrieval-EnhancedKnowledgeEditingfor
Multi-HopQuestionAnsweringinLanguageModels Conferenceacronym’XX,June03–05,2018,Woodstock,NY
Table1:Answeringa3-hopquestion𝑞withmodelediting. 3 METHODOLOGY
Thepre-editedandeditedanswerare𝑡5and𝑡 3∗,respectively.
OurRetrieval-AugmentedEditing(RAE)framework,asshownin
𝑡5and𝑡 3∗arethetailentityof𝛿5and𝛿 3′.𝐺 𝑞and𝐺 𝑞∗denotethe
Figure2,containstwokeysteps:(1)retrievingeditedfactsrelevant
pre-editedandeditedfactchain. tothetargetquestion,and(2)editingthelanguagemodelusing
theseretrievedfactsviain-contextlearning.Wewillfirstdiscuss
EditedFactBankΔandUneditedFacts step(2)withthemotivationofourdesigninthefollowingsection.
(𝛿1→𝛿 1′) HarryPotter,author,J.K.Rowling→StephenKing #edit Thedetailsofstep(1)canbefoundinSections3.2and3.3.
(𝛿2)StephenKing,citizenof,UnitedStates
(𝛿3→𝛿 3′) UnitedStates,capital,Washington,D.C.→Boston #edit 3.1 Retrieval-AugmentedEditing
(𝛿4)J.K.Rowling,citizenof,UnitedKingdom Anaïveapproachisusingsimilarity-basedsearchtoretrieveedited
(𝛿5)UnitedKingdom,capital,London factssimilartotargetquestion𝑞[9,34,46].Thesefactsarethen
integratedintoaprompttemplateforeditingviain-contextlearning:
A3-hopQuestion𝑞
(𝑞) Which city is the capital of the country where the author of 𝑓 𝜃′(𝑞) = 𝑓 𝜃(𝑇 𝑒(𝑞,{𝛿 1′,𝛿 2′,...𝛿 𝐾′ })),where𝑇 𝑒 istheeditingtemplate.
HarryPotter heldcitizenship? Forexample,𝑇 𝑒(·) canbemadeas"Givenfact: {𝛿′}, {𝑞} ?".The
Top-𝐾 nearesteditedfactstoquestion𝑞intheembeddingspace
P ( (𝑡 𝑡r 5 ′e ) )-e L Bd o oi n st tde ood nnAnswer𝑡5andEditedAnswer𝑡 3′ a sir med (·e )n do et ne od ta es st{ h𝛿 e1′, s𝛿 im2′, i. l. a.𝛿 ri𝐾′ ty} f= unT co tip o- n𝐾 a𝛿 n∈ dΔ 𝑔s 𝑧im is( a𝑔 n𝑧( e𝛿 m), b𝑔 e𝑧 d( d𝑞 in)) g, mwh oder ee l.
3 However,editedfactsΔ neededtoanswer𝑞arehardtoretrieve
𝑞
Pre-editedFactChain𝐺𝑞 bythisapproachsincetheyusuallycontainentitiesdifferentfrom
(𝛿1)(HarryPotter,author, J.K.Rowling) 𝑞,whichwillresultinalowsimilarityscoreinalargebankΔ(e.g.,
(𝛿4)(J.K.Rowling,citizenof, UnitedKingdom) inTable1, UnitedStates in𝛿 3′,butnotin𝑞).
(𝛿5)(UnitedKingdom,capital, London) Toaddressthisproblem,weproposeeditedfactchainextrac-
EditedFactChain𝐺 𝑞∗
t ei do gn et go rao pb htai (n KG𝐺 )𝑞∗. [4In 6h ].e Sre un ct hly K,e Ga sch ca𝐺 n𝑞∗ bf eor rm ets ria evc eo dnn be yct ie ted rk atn ivo ew lyl-
(𝛿 1′)(HarryPotter,author, StephenKing) #editedfact traversinglinksfromoneentitytoanother.Take𝐺 𝑞∗ ={𝛿 1′,𝛿2,𝛿 3′}
(𝛿2)(StephenKing,citizenof, UnitedStates) #uneditedfact inTable1asanexample.Itiscomposedoftwoeditedfacts𝛿′,𝛿′
1 3
(𝛿 3′)(UnitedStates,capital,Boston) #editedfact andoneuneditedfact𝛿2.Wecanobservethatthequestionentity:
(HarryPotter )istheheadentityℎ1 in𝛿 1′ = {ℎ1,𝑡1,𝑡 1′},andthe
editedtailentity𝑡 1′:(StephenKing )isalsotheheadentityℎ2in
onasequenceoflinkedfacts:{(ℎ1,𝑟1,𝑡1),(ℎ2,𝑟2,𝑡2),...,(ℎ 𝑘,𝑟 𝑘,𝑡 𝑘)}, thenextfact𝛿2={ℎ2,𝑟2,𝑡2}.Moreover,foreachsubsequentfactin
whereeachtailentityistheheadentityofthenextfact:𝑡 𝑖 =ℎ 𝑖+1. thechain,itsheadentityisalwaysthetailentityofthepreviousfact.
Eachinputquestion𝑞isassociatedwithasequencefactchain𝐺 𝑞 ByeffectivelyretrievingtheKGthatrepresentsthefactchain𝐺 𝑞∗,we
forquestionanswering.A𝑘-hopquestionisusuallyformulated areabletocapturealltheeditedfactualtripletsΔ
𝑞
={𝛿 𝑖′,𝛿′ 𝑗,...𝛿 𝑘′}.
usingonlytheinitialheadentityℎ1,andaseriesofrelationships Inlightofthis,wedefineourretrieval-augmentededitingas:
i{ s𝑟1 s, h𝑟 o2 w,.. n.,𝑟 in𝑘} T. aA bn lee 1x .a Dm ip ffl ee reo nf tm fao cd te cl he ad ii nti sn ag refo ur sa ed3- th oo ap nsq wue es rti to hn
e
𝑓 𝜃′(𝑞)=𝑓 𝜃(𝑇 𝑒(𝑞,𝐺 𝑞∗)), (1)
questionbeforeandafterediting.Onekeyobservationisthatfact wherewegiveanexampleofsucheditinginFigure2.Inthenextsec-
chainsrepresentconnectedknowledgegraphs,whereasingleen- tion,wewillintroducethedetailedstrategyofretrieving𝐺∗,where
𝑞
tityisinvolvedintwoconsecutivefacts.Additionally,wenotice wefirstproposeamutualinformation-basedretrievalstrategyto
a"rippleeffect"inthesechains:Aneditinthefirstfact𝛿1 will extractfactsneededtoanswerthetargetquestion(Section3.2).
leadtochangesinthesubsequentfacts,forminganewchain𝐺∗. Then,weproposeapruningmethodtodeleteirrelevantfactsfrom
𝑞
Inpracticalscenarios,editingisusuallyconductedinbatches,in- theinitialretrievalresult(Section3.3).
volvingmultiplefactchangessimultaneously,resultinginanedited
factbank,denotedasΔ = {𝛿′,𝛿′,...,𝛿′ },where𝑁 istypicallya 3.2 EditedFactsRetrievalviaMaximizingMI
1 2 𝑁
largenumber[18].Locatingtherelevanteditedfactsforonespe- Wefirstconstructaknowledgegraphthatconnectsdifferentfacts.
cificquestionisnon-trivialduetothe"rippleeffect".Tocorrectly Then,weintroduceourproposedretrievalobjectiveofextracting
answermulti-hopquestionsinmodelediting,itiscrucialtoaddress relevantsubgraphsgiveninputquestions.
theretrievaltaskformallydefinedasfollows:
3.2.1 ExternalKnowledgeGraphforSubgraphRetrieval. According
Problem1(Retrieval-AugmentedEditing). Givenanedited toourpreviousdiscussion,weaimtoretrievethefactchain𝐺 𝑞∗for
fact bank Δ = {𝛿 1′,𝛿 2′,...,𝛿 𝑁′ } with 𝑁 instances and a multi-hop modelediting.However,itisworthnotingthat𝐺 𝑞∗consistsofboth
question𝑞whoseanswerrequiresmodelediting,wewanttoretrieve editedanduneditedfacts,whereastheuneditedfactsdonotexist
its corresponding edited facts Δ 𝑞 = {𝛿 𝑖′,𝛿′ 𝑗,...,𝛿 𝑘′}. The goal is to inoureditedfactbankΔbydefault.Toeffectivelyincorporateboth
ensuretheretrievedfactscontainalltheeditedfactsthatappearin typesoffactsintoourretrievalprocess,weproposeintegrating
factchain𝐺 𝑞∗,i.e.,Δ 𝑞 ⊆𝐺 𝑞∗ andΔ\Δ 𝑞 ⊄𝐺 𝑞∗.Then,thesefactsare alleditedfactsintoanexternalknowledgegraphG.Byselecting
usedtorefinethetargetmodel𝑓 𝜃 toconductmodelediting. acomprehensiveKGsuchasWikiData[32],thenewgraph G∗Conferenceacronym’XX,June03–05,2018,Woodstock,NY Shi,etal.
Misery, author, Stephen King Step 1.1:Knowledge Step 1.3: Redundant knowledge Pruning
insertion/edition Misery, author, Richard Dawkins
Edit
Ellie Kemper, citizen of, Croatia
Misery, author, Richard Dawkins Moscow, continent, Africa
Fact Reggie Miller, sport, basketball
Chain Richard Dawkins, citizen of, U.K. Edited Graph External Knowledge Graph Editing Memory Retrieved Facts Refined Facts
What is the nationality of the author of “Misery”? Step 1.2: Mutual Information based retrieval Step 2: In-context learning for editing
Original fact TeE mdi pti ln ag te Question Misery, author, Richard Dawkins
Edited fact Richard Dawkins, citizen of, U.K.
Multi-hop question
United Kingdom (U.K.)
Subgraph with “Misery” in center
Figure2:Theoverallframeworkofourretrieval-augmentedin-contextmodeleditingmethod.
will encompass both unedited and edited facts. It complements theobjectiveas:
oureditedfactbankΔandconnectsdifferententities.Besides,the
externalknowledgegraphprovidesextrafactualknowledgethat
max𝑝(𝑞,𝐺 =𝐺 𝑆)
log
𝑝(𝑞,𝐺 =𝐺 𝑆)
. (5)
canenhancelanguagetooutputcorrectanswers. 𝐺𝑆 𝑝(𝐺 =𝐺 𝑆) 2 𝑝(𝐺 =𝐺 𝑆)
Specifically,giventheeditsΔ={𝛿 1′,...,𝛿 𝑛′}andanexternalG,we Inthefollowing,wewilldiscusshowtoestimateprobability𝑝(𝑞,𝐺 =
considertwotypesofoperationstocombinethem.(1)Modifying 𝐺 𝑆)and𝑝(𝐺 =𝐺 𝑆)efficiently.
existingfacts:IftheoriginalfactappearsinG,i.e.,(ℎ,𝑟,𝑡) ∈ G,
wewillmodifytheKGaccordingtotheedits,soG∗ = (ℎ,𝑟,𝑡′)∪ 3.2.3 ProbabilitiesEstimation. Weproposetocomputeprobabil-
itiesbyleveragingthenext-wordpredictioncapabilityofLLMs.
G\(ℎ,𝑟,𝑡).(2)Addingnewfacts:Iftheoriginalfactdoesnot
Giventhatthefactchainformsatail-to-headconnectedknowl-
appearinG,i.e.,(ℎ,𝑟,𝑡)∉G,thenweappendthemodifiedfactto
theKG,soG∗=(ℎ,𝑟,𝑡′)∪G.Next,givenaquestion𝑞,weretrieve edgegraph,ourextractedsubgraph𝐺 𝑆 canberepresentedas𝐺 𝑆 =
asubgraph𝐺 𝑆 fromG∗,sothat𝐺 𝑆 ⊂ G∗.Ourgoalistoensure
(ℎ1,𝑟1,𝑡1,...,ℎ 𝑛,𝑟 𝑛,𝑡 𝑛),whereℎ
𝑖
and𝑡
𝑖
arenodes,𝑟
𝑖
istheedge,
and𝑛isthenumberofretrievedtriplets.Thus,wecanestimate
that𝐺
𝑆
containsfactchainscorrespondingto𝑞,i.e.,𝐺 𝑞∗ ⊆𝐺 𝑆.
𝑝(𝑞,𝐺=𝐺𝑆)
as:
𝑝(𝐺=𝐺𝑆)
3.2.2 MutualInformationbasedRetrievalObjective. Foreffective
editing,theretrievedsubgraph𝐺 𝑆 mustsharerelevantinformation 𝑝(𝑞,𝐺 =𝐺 𝑆) = 𝑝(𝑟1,𝑡1,ℎ2,𝑟2,𝑡2,...,ℎ 𝑛,𝑟 𝑛,𝑡 𝑛|𝑞,ℎ1) ·𝑝(𝑞,ℎ1) . (6)
withthequestion.Therefore,wedefinetheobjectiveofsubgraph 𝑝(𝐺 =𝐺 𝑆) 𝑝(𝑟1,𝑡1,ℎ2,𝑟2,𝑡2,...,ℎ 𝑛,𝑟 𝑛,𝑡 𝑛|ℎ1) 𝑝(ℎ1)
retrievalasmaximizingthemutualinformation(MI)betweenthe Specifically,fortheterm𝑝(𝑟1,𝑡1,ℎ2,𝑟2,𝑡2,...|𝑞,ℎ1),wecanfurther
subgraph and a set of questions𝑄 whose answers require edit- decomposeitintofollowingform:
ing.Theobjectiveisformalizedasbelow,wherethetheoretical
justificationisprovidedinSection3.4:
𝑝(𝑟1,𝑡1,ℎ2,𝑟2,𝑡2,...,ℎ 𝑛,𝑟 𝑛,𝑡 𝑛|𝑞,ℎ1)
(7)
=𝑝(𝑡1,ℎ2,𝑟2,𝑡2,...,ℎ 𝑛,𝑟 𝑛,𝑡 𝑛|𝑞,ℎ1,𝑟1)·𝑝(𝑟1|𝑞,ℎ1).
m 𝐺a 𝑆x𝐼(𝑄,𝐺 𝑆)=𝐻(𝑄)−𝐻(𝑄 |𝐺 =𝐺 𝑆). (2)
Thisdecompositionallowsustoinitiallyfocusonestimatingthe
𝑝(𝑟1|𝑞,ℎ1).Specifically,theheadentityℎ1isdeterminedif𝑞isgiven,
Givenafixedquestionset𝑄,itsShannonentropy𝐻(𝑄)remains sinceweassumeℎ1ismentionedinquestion𝑞.Candidaterelations
constant.Therefore,maximizingthemutualinformation𝐼(𝑄,𝐺 𝑆) for𝑟1canalsobeselectedfromtheeditedKG.Practically,wecan
isequivalenttominimizingtheconditionalentropy𝐻(𝑄 |𝐺 =𝐺 𝑆). estimatetheprobability𝑝(𝑟1|𝑞,ℎ1)foreachcandidaterelationusing
Thus,wefocusonoptimizingthefollowingobjective: anauto-regressivelanguagemodel𝑓 [25,38]:
𝜙
max𝐼(𝑄,𝐺 𝑆)=min𝐻(𝑄 |𝐺 =𝐺 𝑆) (3)
𝑝(𝑟1|𝑞,ℎ1)≈
𝐺𝑆 𝐺𝑆 |𝑟1|
=m 𝐺a 𝑆x 𝑞∑︁ ∈𝑄𝑝(𝑞|𝐺 =𝐺 𝑆)log 2𝑝(𝑞|𝐺 =𝐺 𝑆). (4) (cid:214) 𝑖=1𝑓 𝜙(𝑤 𝑟( 1𝑖) |𝑤 𝑞(1),...,𝑤 𝑞(|𝑞|),𝑤 ℎ(1 1),...,𝑤 ℎ( 1|ℎ1|),𝑤 𝑟( 11),...,𝑤 𝑟( 1𝑖−1) ),
(8)
Inpractice,quantifying𝑝(𝑞|𝐺 =𝐺 𝑆)posesasignificantchallenge
where𝑓 isthepredictedwordprobability,and𝑤 ,𝑤 ,𝑤 denote
duetoitscomputationalcomplexity.Thiscomplexityarisesasthere
𝜙 𝑞 ℎ1 𝑟1
wordsinthequestion𝑞,headentityℎ1,andrelation𝑟1,respectively.
arenumeroussubgraphcandidates𝐺 withintheentireknowledge
𝑆 Wecanemployopen-sourceLLMslikeGPT-2[25]forthisestima-
graph,makingitprohibitivelyexpensivetoexhaustivelysearchfor
tion.Pleasenotethat,themodel𝑓 beingediteddoesnotneedto
𝜃
theoptimalsubgraph.Tocircumventthisissue,wefirstreplacethe
bethesamemodelusedforprobabilityestimation,makingour
intractableterm𝑝(𝑞|𝐺 =𝐺 𝑆)with 𝑝 𝑝(𝑞 (𝐺,𝐺 == 𝐺𝐺 𝑆𝑆 )).Then,supposewe methodapplicableevenforeditingproprietaryLLMs.Withaspe-
consideronequestioneachtime,where𝑄 =𝑞,wecanreformulate cificinputcontext{𝑞,ℎ1},thelanguagemodelwillassigndifferentRetrieval-EnhancedKnowledgeEditingfor
Multi-HopQuestionAnsweringinLanguageModels Conferenceacronym’XX,June03–05,2018,Woodstock,NY
probabilitiestoeachrelationbasedonitscontextualunderstanding 3.3.1 EditingUncertainty. Wedefineeditinguncertaintyasthe
andreasoningability. uncertainty of the output generated by large language models.
Then,𝑝(𝑡1,ℎ2,𝑟2,𝑡2,...|𝑞,ℎ1,𝑟1)canbefurtherdecomposeinto Formally,theoutputuncertaintyisquantifiedbyShannonentropy:
𝑝(ℎ2,𝑟2,𝑡2,...|𝑞,ℎ1,𝑟1,𝑡1) ·𝑝(𝑡1|𝑞,ℎ1,𝑟1). In our case, we assume
𝑝(𝑡1|𝑞,ℎ1,𝑟1) =1,sinceonerelationusuallyonlycorrespondsto 𝐻(𝑌|𝑋 =𝑥)=−∑︁ 𝑝(𝑦|𝑥)log 2(𝑝(𝑦|𝑥)), (11)
onetailentity.Whentherearemultipletailentities,wefindtheas-
𝑦
sumptionstillworkswellempirically.So,𝑝(ℎ2,𝑟2,𝑡2,...|𝑞,ℎ1,𝑟1,𝑡1)
canbedecomposedinto𝑝(𝑟2,𝑡2,...|𝑞,ℎ1,𝑟1,𝑡1,ℎ2)·𝑝(ℎ2|𝑞,ℎ1,𝑟1,𝑡1). where𝑦representseachpossibleanswergeneratedbythelanguage
Additionally,sincethetailentityinonefactbecomestheheadentity model,and𝑥 ={𝑞,𝐺 𝑆}isthemodelinputcomposedoftheques-
inthesubsequentfact,wecanalsohave𝑝(ℎ2|𝑞,ℎ1,𝑟1,𝑡1)=1.Thus, tion𝑞andfacts𝐺 𝑆.Ahigherentropyvalue𝐻(𝑌|𝑋 =𝑥)signifies
wecanhave𝑝(𝑡1,ℎ2,𝑟2,𝑡2,...|𝑞,ℎ1,𝑟1) =𝑝(𝑟2,𝑡2,...|𝑞,ℎ1,𝑟1,𝑡1,ℎ2). lessconfidenceintheanswer,reflectinggreateruncertainty.Con-
Thisisanicepropertythathelpsusiterativelydecomposethis versely,alowerentropyvalueindicateshigherconfidenceandless
intractableprobabilityterm.Byiterativelyapplyingtheaforemen- uncertainty.Ideally,ifinputfacts𝐺 areexactlytheeditedques-
𝑆
tionedstepfor𝑛times,wecancomputetheconditionalprobability tionfactchain𝐺 𝑞∗,i.e.,𝐺 𝑆 = 𝐺 𝑞∗,thenthemodeloutputshould
ofallsubgraphswithinan𝑛-hopdistancefromthequestionentity. exhibitmaximumconfidencewithminimalentropy,since𝐺∗con-
𝑞
Thefinalestimationcanbeexpressedas: tainsthepreciseknowledgetoanswerquestion𝑞.Inthenextpart,
weconductempiricalexperimentstoverifythisassumption.
𝑝(𝑟1,𝑡1,ℎ2,𝑟2,𝑡2,...,ℎ 𝑛,𝑟 𝑛,𝑡 𝑛|𝑞,ℎ1)
Inourexperiments,GPT-J(6B)[33]ischosenasthebaselan-
=𝑝(𝑟 𝑛|𝑞,ℎ1,𝑟1,𝑡1,...,ℎ 𝑛−1,𝑟 𝑛−1,𝑡 𝑛−1,ℎ 𝑛)· guage model. We select 1000 instances for each of 2, 3, and 4-
𝑝(𝑟 𝑛−1|𝑞,ℎ1,𝑟1,𝑡1,...,ℎ 𝑛−2,𝑟 𝑛−2,𝑡 𝑛−2,ℎ 𝑛−1)· (9) hopquestionsfromtheMQUAKE-CFdataset[46]fortesting.The
...· MQUAKE-CFdatasetcomprisesmulti-hopquestionsthatarebased
onreal-worldfacts,wheretheeditedfactsarecounterfactual,mean-
𝑝(𝑟2|𝑞,ℎ1,𝑟1,𝑡1,ℎ2)·𝑝(𝑟1|𝑞,ℎ1).
ingtheydonotexistinactualreal-worldscenarios.Anexampleof
suchaquestionwithaneditisprovidedinTable1.
Tillnow,wehavedecomposed𝑝(𝑟1,𝑡1,ℎ2,...|𝑞,ℎ1)inEquation(6)
Ourexperimentseekstoidentifythefactset𝐺′ that,whenused
intotheproductofconditionalprobabilitiesofpredictingdifferent 𝑆
asmodelinput,yieldsthelowestoutputentropy,indicatingmini-
relationswithinthe𝑛-hopsubgraph.Thisnicepropertyensures
maleditinguncertainty.Ourfirststepistoconstructdifferentfact
theselectionofthesubgraphwillonlybedeterminedbyrelation
probability,whichisfreefromtheinterferenceofanypotential
setcandidates.Webeginwiththefirstfact𝛿1inthefactchain𝐺 𝑞∗
editedtailentity.Similarly,wecandecomposethedenominator asourinitialfactset𝐺 𝑆′.Then,weaddeachsubsequentfactfrom
term𝑝(𝑟1,𝑡1,ℎ2,...|ℎ1)into: thechainuntilthe𝐺 𝑆′ encompassestheentirefactchain𝐺 𝑞∗.After
that,weintroduceunrelatedfacts𝛿ˆintotheset.Thisprocessis
𝑝(𝑟1,𝑡1,ℎ2,𝑟2,𝑡2,...,ℎ 𝑛,𝑟 𝑛,𝑡 𝑛|ℎ1) repeateduntil𝐺′ containssixelements.Finally,webuildaprefix
𝑆
=𝑝(𝑟 𝑛|ℎ1,𝑟1,𝑡1,...,ℎ 𝑛−1,𝑟 𝑛−1,𝑡 𝑛−1,ℎ 𝑛)· (10) set𝐺¯ 𝑞withallthesixsubsets𝐺 𝑆′.Specifically,fora4-hopquestion,
𝑝(𝑟 𝑛−1|ℎ1,𝑟1,𝑡1,...,ℎ 𝑛−2,𝑟 𝑛−2,𝑡 𝑛−2,ℎ 𝑛−1)·...·𝑝(𝑟1|ℎ1). wehave𝐺¯ 𝑞 = {{𝛿1},{𝛿1,𝛿2},{𝛿1,𝛿2,𝛿3},...,{𝛿1,𝛿2,𝛿3,𝛿4,𝛿ˆ 5,𝛿ˆ 6}},
Then,forthelastterm𝑝(𝑞,ℎ1)/𝑝(ℎ1) inEquation(6),basedon
w wehe cr oe n𝛿 d1 u, c𝛿 t2 i, n𝛿 -3 c, o𝛿 n4 te∈ x𝐺 te𝑞∗ da itn ind g𝛿ˆ 5 u, s𝛿 iˆ n6 gar ee act hwo sui brr se el te 𝐺va ′n ft rofa mct ts h. eFi pn ra el fily x,
wBa hy ie cs h’ it sh aeo cr oe nm s, taw ne tc va an lut era gn is vf eo nrm asit pi en ct io fic𝑝 q(𝑞 u, eℎ s1 t) io/ n𝑝( 𝑞ℎ .1 W)= ec𝑝 a( n𝑞| aℎ l1 so), set𝐺¯
𝑞
witheditingtemplate𝑇 𝑒:"Givenfact:{𝐺 𝑆′},𝑆 {𝑞}?".Inourex-
periments,forcomputationalsimplicity,weconsidermodeloutput
applymodel𝑓 toestimatethisconditionalprobability.Now,since
𝜙 𝑦tobeeachofthenextpredictedword.Wereporttheentropyover
weareabletoestimateeveryterminEquation(6)and(5),wecan
allthewordsinthevocabularyastheeditinguncertainty.
effectivelyidentifythesubgraphthatyieldsthemaximumMutual
TheeditinguncertaintywithdifferentsubsetsislistedinFig-
Information.Additionally,weutilizethebeamsearchtechnique[26]
ure3a.Forcomparison,wealsoreporttheeditinguncertaintywith
toexpeditethecomputationalprocess,eliminatingthenecessityfor
randomfactsselectedfromWikidata,asshowninFigure3b.Our
exhaustivelytraversingallconnectednodes.Furtherdetailsonour
observationsrevealaphenomenon:thelanguagemodelproduces
implementationsareprovidedintheAppendixB.Inthiswork,we
answerswithmuchlowerentropywhen𝐺′ isequaltotheground-
treat𝑛asahyperparametersincethenumberofhopsrequiredto 𝑆
truthfactchain𝐺∗.If𝐺′ presentsredundantfactsorinsufficient
answeraquestionisunknowninadvance.Toensurethoroughex- 𝑞 𝑆
facts,theentropywillincrease.Meanwhile,iftheLLMisfedwith
ploration,𝑛isassignedalargevalue,enablinganextensivesearch
randomfacts,theentropylevelalsoremainsconsistentlyhigh.This
range.However,thisapproachwillalsointroduceirrelevantinfor-
observationjustifiestheuseofentropyastheindicatorofwhether
mationintheretrievedsubgraph𝐺 ,whichcanpotentiallymislead
𝑆 𝐺′ containsthecorrectfactsforLLMinference.
thelanguagemodeltogenerateundesiredanswers[15,16].Thus, 𝑆
inthenextsection,wewilldiscusshowtomitigatethisproblem.
3.3.2 KnowledgePruningwithEditingUncertainty. Sinceincorpo-
ratingthemostrelevantfactswillresultinthelowestentropy,we
3.3 Uncertainty-basedRedundantFactPruning proposetoutilizethisfindingforknowledgepruning.Specifically,
Thissectionintroducesapruningmethod,whichutilizesmodel wefirstfollowSection3.2toretrieveaknowledgegraph𝐺 con-
𝑆
outputuncertainty,toeliminateredundantfactsfrom𝐺 𝑆. taining𝑛tripletsforquestion𝑞:𝐺
𝑆
={𝛿1,𝛿2,...,𝛿 𝑛},where𝑛isaConferenceacronym’XX,June03–05,2018,Woodstock,NY Shi,etal.
Subset1 Subset2 Subset3 Subset4 Subset5 Subset6 Motivatedbytheaboveanalysis,asin-contextprompt𝑆isthe
edited knowledge in our design, we seek to include the edited
1 knowledgethatsharesthesamelatentconcept𝜃 asquestion𝑞.
𝑐
Ideally,thiswillactivatein-contextlearningforeffectivemodel
0.5 editing.Formally,wecandefinesuchknowledgegraphas
𝐺
𝑆
=argmax𝐼(𝐺;𝜃 𝑐), (13)
2-hops 3-hops 4-hops 𝐺∈G
(a)Factchain𝐺 𝑞∗ withredundantknowledge. where𝜃 𝑐 isthelatentconceptusedtogeneratequestion𝑞,andwe
usemutualinformation𝐼(𝐺;𝜃 𝑐)toquantifytheshareinformation.
However,thisisanon-trivialtasksinceconcept𝜃 isanintractable 1 𝑐
hiddenvariable.Toaddressthisissue,weproposeobtainingthe
targetknowledgegraphthatmaximizesthelowerboundofsuch
0.5
anobjective.Specifically,wecanhavethefollowingtheorem:
2-hops 3-hops 4-hops Theorem1. Givenretrievedgraph𝐺 𝑆 ∈G,thelatentconcept𝜃 𝑐,
andthequestion𝑞sampledconditionedonconcept𝜃 𝑐,thereexistsa
(b)Randomfactchainwithoutusefulknowledge. mutualinformationinequality:
Figure3:Distributionofnormalizedmodeleditingentropy 𝐼(𝐺 𝑆;𝜃 𝑐) ≥𝐼(𝐺 𝑆;𝑞). (14)
withdifferentfactsubsetsasinput.Alowernormalizeden-
Theorem1showsthatwecanmaximizethemutualinformation
tropy on a given subset indicates that the model is more
betweentheselectedknowledgegraph𝐺 andquestionconcepts𝜃
confident in answering the question with the given facts. 𝑆 𝑐
Subset1includesthefirstfact{𝛿1},Subset2includesthefirst bymaximizingthemutualinformationbetweentheselectedgraph
twofacts{𝛿1,𝛿2},andsoon.Figure3ashowsthattheentropy 𝐺 𝑆 andthequestion𝑞itself.Inthisway,thein-contextlearning
abilityofLLMswouldbeeffectivelytriggered.Whenweapplysuch
issignificantlylowerifthesubsetcontainsexactlytheentire
knowledgeastheprompt,wecaneffectivelyconductthein-context
factchainofthequestion(e.g.,Subset2for2-hopquestions).
editing.TheproofofTheorem1isinAppendixA.
sufficientlylargenumber,and𝐺 𝑆 couldcontainredundantknowl- 4 EXPERIMENTS
edge.Then,toremoveredundantknowledge,wefirstbuildthe
prefixsets𝐺¯ fortargetquestion𝑞basedontheretrievedgraph
Weconductexperimentstoanswerthefollowingquestions.Q1:
𝑞 DoesRAEsuccessfullyeditmodeloutput?Q2:Howdoesourre-
𝐺 .Then,wecanobtaintheprunedfactset𝐺∗usingtheobjective:
𝑆 𝑆 trievalstrategyperformcomparedwithotherretrievalmethods?
𝐺 𝑆∗ =argmin−∑︁ 𝑝(𝑦|𝑇 𝑒(𝑞,𝐺 𝑆′))log 2(𝑝(𝑦|𝑇 𝑒(𝑞,𝐺 𝑆′))). (12) Q3:Doesourproposedpruningtechniqueremoveredundantfacts
𝐺 𝑆′∈𝐺¯ 𝑞 𝑦 fromtheretrievedfacts?Q4:DoesRAEworkforproprietyLLMs?
Finally, we can apply𝐺∗ as our retrieved fact chain for the in-
𝑆 4.1 ExperimentSettings
contextlearningintroducedinSection3.1toconductediting.
4.1.1 LanguageModels. WeevaluateRAEacrossvariouskindsof
languagemodelsindifferentsizesandfamilies,includingGPT-2
3.4 TheoreticalJustification
(1.3B)[25],GPT-J(6B)[33],Falcon(7B)[1],Vicuna(7B)[4],and
Inthissubsection,wetheoreticallyjustifythatthefactscollected
Llama2-chat(7B)[30].Amongthem,GPT-2,GPT-J,andFalconare
byourretrievalobjective(Eq.(2))areeffectiveinperformingmodel
pre-trainedlanguagemodelswithoutinstructiontuning[5,24],
editingwithin-contextlearning.Tobeginwith,wediscusswhat
whileVicunaisaninstruction-tunedvariationofLlama1[31]and
kindsofinputcaneffectivelyactivatein-contextlearning.Then,
Llama2-chatistheinstruction-tunedversionofLlama2.Instruction-
weexplorehowtobuildsucheffectiveinputformodelediting.
tunedmodels(VicunaandLlama2-chat)areexpectedtobetterfol-
Ourproposededitingmethodreliesonthein-contextlearning
lowtheinstructionsinthepromptcomparedtonativepre-trained
abilityofLLMs.Inthefollowing,weprovideananalysisofhow
models(GPT-2,GPT-J,andFalcon).Weincludebothkindsofmodels
in-contextlearningcanbeeffectivelytriggered.Theoretically,the
toverifytheeffectivenessoftheproposedmethods.
textgenerationprocessofalanguagemodelcanbeunderstood
asaHiddenMarkovModel[2,39].Themodelinitiallyselectsa 4.1.2 EditingBaselines. Forcomparison,weconsiderthreekinds
concept𝜃 𝑐 ∈ Θ from a set of underlying concepts denoted as of model editing methods: (1) Model weight updating methods:
Θ, and then samples a sequence of words based on the chosen Fine-tuning[47]editstheentiremodelweightsbylanguagemod-
concept.Basedonthat,thein-contextlearningcanbewrittenas eling the edited knowledge. ROME [17] and MEMIT [18] focus
𝑝(𝑦|𝑆,𝑥) = ∫ 𝜃𝑐∈Θ𝑝(𝑦|𝑆,𝑥,𝜃 𝑐)𝑝(𝜃 𝑐|𝑆,𝑥)𝑑𝜃 𝑐, where𝑆 denotes in- onidentifyingandupdatingparticularneuronsassociatedwith
contextpromptand𝑥 denotesquery.Existingresearchhastheo- theknowledgethatneedsediting.(2)Auxiliarymodelsmethods:
reticallyproventhattheconditiontoactivatein-contextlearning SEARC[20]trainsanextralanguagemodeltostoreupdatedknowl-
iswhenthereisasharedlatentconcept𝜃 betweenprompttext𝑆 edge,anditswitchestotheauxiliarymodelwhenansweringques-
𝑐
andtheinputquery𝑥.Morediscussionscanbefoundin[39]. tionsrelevanttotheeditedfacts.Detailsabouttheauxiliarymodels
ledoMdezilamroN
ledoMdezilamroN
yportnEtidE
yportnEtidERetrieval-EnhancedKnowledgeEditingfor
Multi-HopQuestionAnsweringinLanguageModels Conferenceacronym’XX,June03–05,2018,Woodstock,NY
fordifferentlanguagemodelsareincludedinAppendixB.(3)In- • QuestionReform[21,41]appendstheretrievedentitytothe
context learning-based method: Mello [46] edits model outputs previousqueryforthenexthopretrieval.Thisdesignismotivated
withmulti-roundconversations. bysimulatingareasoningpathwheretheretrievedfactateach
hopisviewedasanintermediatereasoningresult.
4.1.3 Implementation Details. We evaluate our editing method • QuestionDecompose,proposedbyMello[46],decomposes
ontheMQUAKE-CFandMQUAKE-Tdatasets[46],comprising amulti-hopquestionintomultiplesing-hopquestions.Specif-
1000counterfactualeditinginstancesper2-hop,3-hop,and4-hop ically,acomprehensiveconversational-stylepromptisapplied
questionsinMQUAKE-CF,andtotal1868editinginstancesfor2- tointeractwiththeLLMforcollectingeachsingle-hopquestion.
hopand3-hopquestionsinMQUAKE-T.Inoursetting,weedit Foreachsingle-hopquestion,thecorrespondingknowledgeis
allthesefactsatthesametime.Followingpreviouswork[46],we retrievedwiththenaïvemethoddescribedinKGLink.
leveragecasesfromtheMQUAKE-CF-9kdatasettocraftprompt (2)Incontrastwiththeembedding-basedmethodsthatestimatethe
templatesforbothbaselinesandourmethod.Aneditisconsidered similaritiesbetweenthequeryandthefactswiththedotproducts
successfuliftheeditedLLMcorrectlypredictstheupdatedanswer oftheirembeddings,probability-basedmethodsleveragelanguage
withinthefirsttengeneratedtokens.Thiscriterionisreportedas modelstodirectlyestimatethissimilarity.
editedaccuracyinTable2.MoresettingdetailsareinAppendix.B. • MaxProb[29,44]retrievesthesubgraph𝐺 𝑆 ∈ G∗ thatmaxi-
mizestheconditionalprobability𝑝(𝐺 𝑆|𝑞),where𝑞istheinput
multiplequestion.Inourexperiments,weextracta𝐾-hopsub-
4.2 EditingPerformanceEvaluation
graphfromtheeditedknowledgegraph G∗ andestimatethe
ToanswerQ1,weassessourmodeleditingmethodacrossvarious
probabilitywithalanguagemodel𝑓 asshowninSection3.2.3.
languagemodels,comparedagainstdifferentbaselinemethods.We 𝜙
focusontheeditedaccuracyofthesemodelsinansweringmulti- 4.3.2 ImplementationDetails. Weselect300casesforeachofthe2,
hopquestionswitheditedanswers.Forreferences,wealsoreport 3,and4-hopproblemsfromtheMQUAKE-CFdataset.Foreachtype,
theaccuracyofun-uneditedmodelsinpredictingbothoriginal wereporttwomatchingaccuracyscoresinTable3.Specifically,
andeditedanswers.Ourkeyobservationsare:(1)OurRAEout- withintheretrievedfacts,ifanyfactalsoappearsinthefactchainof
performsallothersinbothdatasetsacrossfivelanguagemodels, amulti-hopquestion,wenameitapartialmatch(PM).Andifevery
achievinganaverageimprovementof40.57%overthesecond-best retrievedfactmatchesthefactchainforthemulti-hopquestion,
method(Mello)whenconductingthousandsofeditsatthesame wenameitexactmatch(EM).
time.Thissuperiorperformanceprimarilystemsfromournovel
MI-basedretrievalobjectiveandaneffectivepruningstrategy.Our 4.3.3 Results. Wehavethefollowingobservations:(1)Ourpro-
designcanalsoseamlesslyintegrateanexternalknowledgegraph, posedmutualinformation-basedretrievalmethoddemonstrates
whicheffectivelylinksalleditedfacts,therebyfacilitatingthemulti- excellentperformanceacrossvariousLLMsinmulti-hopfactex-
hopeditingprocess.(2)Mellodemonstratesgoodperformanceon traction.Wealsofinditssuccesswithrelativelysmalllanguage
theMQUAKE-Tdatasetwithmodelslargerthan6B.However,it models,suchasGPT-2,showingstronggeneralizationability.(2)
underperformsontheMQUAKE-CFdatasetandfailswithGPT-2, Incontrast,traditionalembedding-basedmethods(KGLinkand
likelyduetoGPT-2’sinabilitytofollowMello’scomplexprompts, QuestionReform)underperforminthismulti-hopfactretrieval
resultinginnooutput.(3)Othermethodsgenerallyshowlower challenge.Theirlimitationmainlyliesinfailingtocomprehendthe
performanceacrossalllanguagemodels,whichalignswiththe complexinterplaybetweenmultiplerelationsinaquestion.Thus,it
findingsfromMQUAKEdatasetpaper[46]. hinderstheextractionoftargetfactsfromtheextensiveknowledge
base.(3)Mellodemonstratestheefficacyofdecomposingmulti-
hopquestionsintosingle-hopquestionsforthismulti-hopfacts
4.3 RetrievalPerformanceEvaluation
retrievaltask.Notably,itsEMperformancedropssignificantlywith
ToanswerQ2,weassesstheeffectivenessofourmutualinformation-
anincreasingnumberofhops,probablybecauseitbecomesmuch
basedretrievalmethodformulti-hopquestion-answeringtasks.
morechallengingforlanguagemodelstoperformquestiondecom-
4.3.1 RetrievalBaselines. Weconsidertwotypesofretrievalmeth-
posing.(4)Probabilitymaximization-basedmethodsoutperform
traditionalembeddingmethods,whilethereisstillagapcompared
odsasbaselines,namelytheembedding-basedandtheprobability-
toours,probablybecausethepredictedmostprobablefactmaynot
basedmethods.Weintroducetheirdetailsasfollows.
bethemostnecessaryoneforansweringaspecificquestion.
(1)Embedding-basedretrievalmethodsmostlyconcentrateontext
retrievalratherthantripletretrieval.Forafaircomparison,weadapt
themtofitourtaskbyemployingtheContriever[11]toencode 4.4 AblationStudiesonPruningStrategy
alleditedfactsintoembeddings,andthencachetheseembeddings To answer Q3, we verify that our proposed pruning strategies
forsimilaritysearch.Differentretrievalmethodsusedistinctstrate- arebeneficialtomulti-hopeditingtasks.Tosimulatethesituation
giestomodifytheoriginal𝐾-hopsquestionsasthefinalqueryto wheretheretrievedfactscontainredundantinformation,wecon-
performthesearchviadotproductbetweentheembeddings. ductourexperimentbyalwaysretrieving2additionalfactsover
• KGLink[8,40]isastraightforwardstrategy,whichidentifies thefactsneededbytheoriginalquestion.Thatis,givena𝐾-hop
thequeryentityanditslinkedentitiesascandidatestofindthe question,wesetthetotalnumberofretrievedfactsas𝑛=𝐾+2.
onethatisthemostsimilartotheoriginalquestion.Thisprocess Table4reportstheeditedaccuracyofRAEworkingwithorwith-
isrepeated𝐾 timestoretrievetheentirefactchain. outthepruningstrategy,andwedrawthefollowingconclusions:Conferenceacronym’XX,June03–05,2018,Woodstock,NY Shi,etal.
Table2:Editedaccuracy(%)onMQUAKEdatasets(MQUAKE-CFandMQUAKE-T).
EditingMethods
w/oedit w/oedit Fine
LanguageModels Datasets ROME MEMEIT SEARC Mello Ours
(origans) (editedans) Tuned
MQUAKE-CF 10.7 0.0 3.8 1.7 2.3 4.0 0.0 62.8
GPT-2
MQUAKE-T 4.7 0.0 5.8 6.4 1.6 2.7 0.0 61.8
Modelsw/o MQUAKE-CF 18.1 0.0 7.7 7.6 8.1 6.8 15.3 69.3
GPT-J
InstructionTuning MQUAKE-T 15.3 3.1 3.1 4.1 10.6 2.8 36.7 63.9
MQUAKE-CF 33.7 1.0 5.6 1.7 2.3 7.9 10.7 66.8
Falcon
MQUAKE-T 35.9 8.9 17.2 7.3 1.6 4.5 51.5 61.6
MQUAKE-CF 43.2 1.0 4.8 8.4 7.6 7.9 10.2 67.2
Vicuna
Modelsw/ MQUAKE-T 15.3 3.1 23.1 5.0 1.7 4.5 51.7 63.2
InstructionTuning Llama2 MQUAKE-CF 29.0 1.1 5.4 6.3 3.8 7.9 20.7 69.1
(chat) MQUAKE-T 26.7 5.1 17.1 8.7 1.7 4.5 49.4 66.2
Table3:Multi-hopfactsretrievalaccuracy(%)comparison. GPT-3.5-instruct
GPT-4
MQUAKE-CF(300casesforeachkindofquestion) 60 GPT-3.5-turbo
Babbage
QuestionType 2-hops 3-hops 4-hops GPT-4
Category RetrievalMethod PM EM PM EM PM EM 40
GPT-3.5-instruct
KGLink 52.7 28.7 18.2 3.7 14.0 0.0
GPT-3.5-turbo
Embedding QuestionReform 62.3 7.7 14.7 0.0 12.3 0.0 20
Mello(Llama2) 84.3 80.0 80.7 42.3 83.3 25.7 Mello
Probability MaxProb(GPT-2) 77.7 50.3 67.3 25.3 65.0 20.0 0 Babbage RAE(ours)
MaxProb(Llama2) 78.3 55.7 79.7 37.0 69.3 28.7
RAE(GPT-2) 83.0 66.3 77.3 41.0 80.3 43.7 0 5 10 15 20
RAE(GPT-J) 83.0 69.7 81.3 53.7 82.7 54.0 TotalInferenceCost(USDollar)
Mutual
RAE(Falcon) 82.3 70.7 72.3 44.3 81.7 47.3
Information Figure4:Editingperformancecomparedwithinferencecost
RAE(Vicuna) 81.0 66.7 79.3 50.3 85.0 50.0
RAE(Llama2) 82.7 69.3 84.0 49.3 82.0 47.0 overdifferentproprietarymodels.
asChatGPT[22].Insuchcases,RAEutilizesadifferentlightweight
PM:PartialMatch.EM:ExactMatch.
Table4:Editedaccuracy(%)with(w)orwithout(w/o)pruning. languagemodeltoperformrelevantfactsretrievalandpruningas
introducedinSection3.2andSection3.3.Then,theobtainedfacts
Dataset MQUAKE-CF
willbefedintotheproprietarymodelstoperformin-contextediting.
Llama2
Type Strategy GPT-2 GPT-J Falcon Vicuna (chat) WeevaluateourproposededitingmethodonGPT-babbage-002,GPT-
w/oPruning 63.0 63.7 65.2 63.8 70.1 3.5-turbo-0613,GPT-3.5-instruct,andGPT-4-0613models[23].Weuse
2-hops w/Pruning 73.3 75.5 74.5 73.5 75.8 GPT-2(1.3B)asourretrievalmodel.Wereporttheeditedaccuracy
Improved 10.3↑ 11.8↑ 9.3↑ 9.7↑ 5.7↑ and total editing cost of our method for randomly selected 300
w/oPruning 43.1 53.8 55.6 55.0 60.3 cases(MQUAKE-CF)inFigure4.Theeditingcostincludesthetotal
3-hops w/Pruning 53.2 65.4 62.1 62.7 65.8 feesofcallingAPIsandthecostofrunningGPT-2forknowledge
Improved 10.1↑ 11.6↑ 6.5↑ 7.7↑ 5.5↑ retrievalonrentedGPUs.WealsoreporttheresultsofMello[46]
w/oPruning 49.9 58.8 55.2 61.5 61.6
forcomparison,whereonlytheAPIfeesarecounted.
4-hops w/Pruning 61.9 66.9 62.9 65.5 65.8
InFigure4,wefirstobservethatBabbagehas0.0%editedac-
Improved 12.0↑ 8.1↑ 7.7↑ 4.0↑ 4.2↑
curacybyusingMello,showingitsineffectivenessineditingthe
un-instructiontunedproprietarylanguagemodel.Itisexpected
(1)Theproposedpruningtechniquesignificantlyenhancestheper- sinceMelloreliesontheconversationalabilityoflanguagemodels
formanceofmodelediting,demonstratedbyachievinganaverage todecomposemulti-hopquestions.Incontrast,RAEiseffectivein
accuracyimprovementof8.3%acrossvariouslanguagemodels.(2) editingalltheseproprietarymodelswitharemarkablylowercost
Weobserveamoreprofoundimprovementforsmallerlanguage thanMello.Inparticular,RAEimprovesMelloforeditingGPT-4
modelsoncomplexquestions,whilethisbenefittolargerlanguage withalmost20%editedaccuracybyonlycostingaroundits15%
modelsisrelativelylesssignificant.Inparticular,theperformance budget.Thishighlightsthebenefitofutilizingtheinherentlan-
improvementofGPT-2onthe4-hopquestionsreaches12.0%,while guagemodelingabilityinsteadoftheinstructionfollowingability
forLlama-2itisjust4.2%.Thisobservationsuggeststhatlarger toperformknowledgeretrievalformulti-hopquestionanswering.
modelsaremorerobusttoredundantinformation.
4.6 CaseStudy
4.5 EditingPerformanceonProprietaryLLMs InFigure5,wepresenttwocasesfromtheMQUAKE-CFdatasetto
To answer Q4, we apply the proposed RAE to edit proprietary demonstratetheretrievingprocessoveraknowledgegraphandthe
languagemodels,wherewecanonlyaccessthemodelviaAPIs,such pruningprocessoftheretrievedfacts.Forvisualizations,thered,
)%(ycaruccARetrieval-EnhancedKnowledgeEditingfor
Multi-HopQuestionAnsweringinLanguageModels Conferenceacronym’XX,June03–05,2018,Woodstock,NY
black,anddottedlinesrepresentthefinal,candidate,anddiscarded 5 RELATEDWORK
paths in the knowledge graph with beam search, reflecting the
5.1 ModelEditing
decision-makingprocessofourretrievaldesign.InFigure5a,apo-
Existingmodeleditingmethodsforsingle-hopfacteditingcanbe
tentialpathMisery–language–>English–country–>U.K.isdiscarded
categorizedintothefollowingkinds:(1)Meta-learningmethod:
eventhoughitcanleadtothecorrectanswer(U.K.inthiscase).
MEND[19]employsanauxiliarynetworkforgradienttransforma-
Thisisbecauseitdoesnotsharetheinformationthatisneededto
tionduringfine-tuning.(2)Localmodificationmethods:MEMIT[18]
answerthetargetquestion,resultinginalowMIscore.InFigure5b,
andROME[17]editspecificneuronswithinmodelswithminimal
eventhoughthisquestionisassociatedwiththreeeditedfacts,our
impactonunrelatedweights.(3)Extension-basedmethods:SERAC
methodcansuccessfullylocateallofthem,demonstratingrobust-
utilizesafine-tunedsmallauxiliarylanguagemodeltoanswerques-
nessovermulti-hopquestions.Inbothcases,ourpruningstrategy
tionsthatrequireeditedfacts.Meanwhile,GRACE[10]provides
successfullytruncatestheretrievedfactchaintoonlycontainnec-
atheoreticalbasisforconcepterasureacrossmodellayers.(4)In-
essaryfactsrelyingonthenormalizedmodeleditingentropy.
contextlearningmethods:IKE[45]editmodelsthroughin-context
learning,bypassingtheneedforparameterupdates.
Incontrast,modeleditingmethodsformulti-hopfactediting
What is the nationality of the author of Misery ? remain underexplored. One such method, Mello [46], utilizes a
comprehensiveprompttodecomposethemulti-hopquestioninto
Retrieval single-hop questions and then retrieve edited knowledge itera-
Anglic United Kingdom of tively.Arecentcontemporaneouswork,DeepEdit[36],editsLLMs
English Great Britain and
Ireland throughdecodingwithconstraints,whereaneuro-symbolicmethod
isproposedforbetterreasoncoherence.However,theseapproaches
Misery Richard U.K.
Dawkins heavilyrelyonthestronginstructionfollowingandreasoningabil-
London
ityofLLMs,whichreferstotheircapacitytounderstandandfollow
Thriller Royal Society of theinputinstructiontogeneratecoherent,context-awareresponses.
Literature
Stephen Final path Existingmethods[46]fallshortformodelsinrelativelysmallsizes,
King U.S. Candidate path
Discarded path suchasGPT-J(6B)orVicuna(7B),comparedtopowerfulGPT-3.5.
Original path
Pruning
Misery—author– Richard Dawkins – citizen of—U.K– capital—London
| | | | 5.2 Multi-hopQAwithKnowledgeGraphs
Normalized Model 1.0 0.0 0.31
Editing Entropy: Truncate here! Multi-hopKnowledgeGraphQuestionAnswering(KGQA)involves
(a)Atwo-hopquestionexample(CaseID2). locatinganswerswithinaKGthatareseveralhopsawayfromthe
startingquestionentities.Existingtechniquesusuallyfirstextracta
Which country does the spouse of the performer of
Bangerz belong to? relevantsubgraphfromtheentireKG,thenapplymulti-hopreason-
ing[28,44].Initialretrievalstrategiesapplysimilarityscoreslike
Retrieval U.S. Final path PageRank[28]orembeddingsimilarity[13,44]forsubgraphselec-
U.K. Candidate path tion,butthesemethodsoftenfailtograspthelogicofthequestion,
Pharrell
Discarded path
Williams potentiallyoverlookingvitalfacts.Inthereasoningstage,earlier
Original path
researchusedgraphnetworkarchitecturesforreasoning[12,42].
Bangerz Elvis Jamie
Presley Hewlett SomerecentapproachesnowemployLLMsforreasoning[29,35].
Kathmandu
Think-on-graphappliestheLLMasaninteractiveagentonKGs,
Nepal
usingretrievedknowledgeforreasoning[29].However,thesemeth-
Miley Priscilla odsrelyonstrongreasoningcapabilities,whileLLMswithless
Cyrus Presley Ne rp ua pl ee ese strongreasoningabilitytendtounderperformwiththesemethods.
Pruning
Bangerz—performer — Elvis —spouse— J a m i e —citizen—Nepal — capital—Kathmandu
Presley Hewlett
| | | | |
Normalized Model 1.0 0.84 0.0 0.33 6 CONCLUSION
Editing Entropy: Truncate here!
WeproposeanovelLLMeditingframeworkformulti-hopQA.We
(b)Athree-hopquestion(CaseID1143).
employastrategythatutilizesmutualinformationmaximization
Figure 5: Case studies for edited facts retrieval and prun- fortheretrievaloffactsandaself-optimizingtechniqueforprun-
ing.Theretrievalprocessinvolvesthebeamsearch,starting ingredundantinformation.Thisapproacheffectivelyaddressesthe
fromthequeryentityandnavigatingthroughtheknowledge challengesofintegratingreal-timeknowledgeupdatesinLLMs.
graph with two beams. The two primary candidate edges Theoreticaljustificationsandcomprehensiveevaluationsdemon-
(beams)ateachentityhopareboldfaced,andtheothersare strateRAE’seffectivenessinenhancingtheaccuracyofupdated
discardedanddenotedwithdashedlines.Weemphasizethe LLMresponses.Ourcontributionsmarkasubstantialadvancement
finalbeamsearchresultwithredcolor. inmodeleditingresearch,presentingapromisingdirectionforthe
integrationofdynamicknowledgeinlanguagemodels.Conferenceacronym’XX,June03–05,2018,Woodstock,NY Shi,etal.
A THEORETICALJUSTIFICATION REFERENCES
Theorem1. Givenretrievedgraph𝐺 𝑆 ∈G,thelatentconcept𝜃 𝑐, [1] EbtesamAlmazrouei,HamzaAlobeidli,AbdulazizAlshamsi,AlessandroCappelli,
RuxandraCojocaru,MérouaneDebbah,ÉtienneGoffinet,DanielHesslow,Julien
andthequestion𝑞sampledconditionedonconcept𝜃 𝑐,thereexistsa Launay,QuentinMalartic,etal.2023.Thefalconseriesofopenlanguagemodels.
mutualinformationinequality:𝐼(𝐺 𝑆;𝜃 𝑐) ≥𝐼(𝐺 𝑆;𝑞). arXivpreprintarXiv:2311.16867(2023).
Proof. Considerlatentconcepts𝜃 isinferredfromaknowl- [2] LeonardEBaumandTedPetrie.1966. Statisticalinferenceforprobabilistic
𝑐 functionsoffinitestateMarkovchains.Theannalsofmathematicalstatistics37,
edgegraph𝐺 𝑆,andaquestion𝑞isthengeneratedbasedonthis 6(1966),1554–1563.
concept.(Forinstance,thiscanbeasstraightforwardasprompt- [3] SidBlack,LeoGao,PhilWang,ConnorLeahy,andStellaBiderman.2021.GPT-
ingLLMstoformulateaquestionaboutagivensetoffacts).The Neo:LargeScaleAutoregressiveLanguageModelingwithMesh-Tensorflow. https:
//doi.org/10.5281/zenodo.5297715Ifyouusethissoftware,pleaseciteitusing
followingMarkovchain:𝐺 𝑆 →𝜃 𝑐 →𝑞exists,indicatingthat𝑞is thesemetadata..
conditionallyindependentto𝐺 .Accordingtothechainrulefor [4] Wei-LinChiang,ZhuohanLi,ZiLin,YingSheng,ZhanghaoWu,HaoZhang,
𝑠
LianminZheng,SiyuanZhuang,YonghaoZhuang,JosephE.Gonzalez,IonStoica,
mutualinformation,wecanhave:
andEricP.Xing.2023.Vicuna:AnOpen-SourceChatbotImpressingGPT-4with
𝐼(𝐺 𝑆;𝜃 𝑐,𝑞)=𝐼(𝐺 𝑆;𝑞)+𝐼(𝐺 𝑆;𝜃 𝑐|𝑞)=𝐼(𝐺 𝑆;𝜃 𝑐)+𝐼(𝐺 𝑆;𝑞|𝜃 𝑐). (15)
[5]
9 H0 y% u* nC gh Wa otG nP CT huQ nu ga ,l Lit ey. Hoh utt ,p Ss h: a// ylm nesy Ls o. no grg p/ rb el ,o Bg a/ r2 r0 e2 t3 Z- o0 p3 h-3 ,Y0- iv Ti ac yu ,n Wa/
illiamFedus,
Giventhatthequestion𝑞isconditionallyindependentofthegraph EricLi,XuezhiWang,MostafaDehghani,SiddharthaBrahma,etal.2022.Scaling
instruction-finetunedlanguagemodels.arXivpreprintarXiv:2210.11416(2022).
𝐺 𝑆,theterm𝐼(𝐺 𝑆;𝑞|𝜃 𝑐)equalszero.Therefore,weareleftwith:
[6] RoiCohen,EdenBiran,OriYoran,AmirGloberson,andMorGeva.2023.Evalu-
𝐼(𝐺 𝑆;𝜃 𝑐) ≥𝐼(𝐺 𝑆;𝑞).Equalityholdspreciselywhen𝐼(𝐺 𝑆;𝑞|𝜃 𝑐)is atingtherippleeffectsofknowledgeeditinginlanguagemodels.arXivpreprint
zero,meaningthatthegraph𝐺 providesnoadditionalinformation arXiv:2307.12976(2023).
𝑆 [7] DamaiDai,LiDong,YaruHao,ZhifangSui,BaobaoChang,andFuruWei.2021.
about𝑞once𝜃 𝑐 isknown. □ Knowledgeneuronsinpretrainedtransformers.arXivpreprintarXiv:2104.08696
(2021).
[8] RajarshiDas,AmeyaGodbole,DilipKavarthapu,ZhiyuGong,AbhishekSinghal,
B IMPLEMENTATIONDETAILS
MoYu,XiaoxiaoGuo,TianGao,HamedZamani,ManzilZaheer,etal.2019.
Inourfactretrievalprocess,weemployabeamsearchwithawidth Multi-stepentity-centricinformationretrievalformulti-hopquestionanswering.
of2,meaningweconsideronlytwocandidaterelationsateachhop.
InProceedingsofthe2ndWorkshoponMachineReadingforQuestionAnswering.
113–118.
Forthetotalnumberofretrievalhops𝑛,wesetitto4,5,and6for [9] XiaoqiHan,RuLi,HongyeTan,WangYuanlong,QinghuaChai,andJeffPan.
2-hop,3-hop,and4-hopquestions,respectively.Forsimplicity,we 2023.ImprovingSequentialModelEditingwithFactRetrieval.InFindingsofthe
AssociationforComputationalLinguistics:EMNLP2023.11209–11224.
omittheterm𝑝(𝑞|ℎ1)asitremainsconstantforeachquestion,and
[10] ThomasHartvigsen,SwamiSankaranarayanan,HamidPalangi,YoonKim,and
itsexclusiondoesnotimpactempiricalperformance.Regarding MarzyehGhassemi.2022. AgingwithGRACE:LifelongModelEditingwith
theSEARCbaseline,wepairsmallermodelswithlargerauxiliary DiscreteKey-ValueAdaptors.arXivpreprintarXiv:2211.11031(2022).
[11] GautierIzacard,MathildeCaron,LucasHosseini,SebastianRiedel,PiotrBo-
models:GPT-2-small(124M)[25]withGPT-2-xl(1.3B)[25],GPT-2- janowski,ArmandJoulin,andEdouardGrave.2021. Unsuperviseddensein-
large(774M)[25]withGPT-J(6B)[33],andGPT-Neo(2.7B)[3]with formationretrievalwithcontrastivelearning. arXivpreprintarXiv:2112.09118
(2021).
bothFalcon(7B)[1]andVicuna(7B)[4],aswellasLlama2(7B)[30].
[12] JinhaoJiang,KunZhou,WayneXinZhao,andJi-RongWen.2022. Great
Weimplementthesemodelswiththecodeandcheckpointsavailable TruthsareAlwaysSimple:ARatherSimpleKnowledgeEncoderforEnhancing
fromHuggingfacelibrary[37]. theCommonsenseReasoningCapacityofPre-TrainedModels.arXivpreprint
arXiv:2205.01841(2022).
[13] JinhaoJiang,KunZhou,WayneXinZhao,andJi-RongWen.2022. Unikgqa:
ACKNOWLEDGMENTS Unifiedretrievalandreasoningforsolvingmulti-hopquestionansweringover
knowledgegraph.arXivpreprintarXiv:2212.00959(2022).
Theworkis,inpart,supportedbyNSF(#IIS-2223768).Theviews
[14] PatrickLewis,EthanPerez,AleksandraPiktus,FabioPetroni,VladimirKarpukhin,
andconclusionsinthispaperarethoseoftheauthorsandshould NamanGoyal,HeinrichKüttler,MikeLewis,Wen-tauYih,TimRocktäschel,
notbeinterpretedasrepresentinganyfundingagencies. etal.2020.Retrieval-augmentedgenerationforknowledge-intensivenlptasks.
AdvancesinNeuralInformationProcessingSystems33(2020),9459–9474.
[15] JunyiLi,JieChen,RuiyangRen,XiaoxueCheng,WayneXinZhao,Jian-YunNie,
andJi-RongWen.2024.TheDawnAftertheDark:AnEmpiricalStudyonFactu-
alityHallucinationinLargeLanguageModels.arXivpreprintarXiv:2401.03205
(2024).
[16] JunyiLi,XiaoxueCheng,WayneXinZhao,Jian-YunNie,andJi-RongWen.2023.
HELMA:ALarge-ScaleHallucinationEvaluationBenchmarkforLargeLanguage
Models.arXivpreprintarXiv:2305.11747(2023).
[17] KevinMeng,DavidBau,AlexAndonian,andYonatanBelinkov.2022.Locating
andeditingfactualassociationsinGPT.AdvancesinNeuralInformationProcessing
Systems35(2022),17359–17372.
[18] KevinMeng,ArnabSenSharma,AlexAndonian,YonatanBelinkov,andDavid
Bau.2022.Mass-editingmemoryinatransformer.arXivpreprintarXiv:2210.07229
(2022).
[19] EricMitchell,CharlesLin,AntoineBosselut,ChelseaFinn,andChristopherD
Manning.2021. Fastmodeleditingatscale. arXivpreprintarXiv:2110.11309
(2021).
[20] EricMitchell,CharlesLin,AntoineBosselut,ChristopherDManning,andChelsea
Finn.2022.Memory-basedmodeleditingatscale.InInternationalConferenceon
MachineLearning.PMLR,15817–15831.
[21] PingNie,YuyuZhang,ArunRamamurthy,andLeSong.2020.AnsweringAny-
hopOpen-domainQuestionswithIterativeDocumentReranking.arXivpreprint
arXiv:2009.07465(2020).
[22] OpenAI.2023.GPT-3.5.https://openai.com/blog/gpt-3-5/. Accessedon[Date].
[23] OpenAI.2023.ModelsReferredtoasGPT-3.5.https://platform.openai.com/docs/
models/gpt-3-5. Accessedon[Date].Retrieval-EnhancedKnowledgeEditingfor
Multi-HopQuestionAnsweringinLanguageModels Conferenceacronym’XX,June03–05,2018,Woodstock,NY
[24] LongOuyang,JeffreyWu,XuJiang,DiogoAlmeida,CarrollWainwright,Pamela [47] ChenZhu,AnkitSinghRawat,ManzilZaheer,SrinadhBhojanapalli,DaliangLi,
Mishkin,ChongZhang,SandhiniAgarwal,KatarinaSlama,AlexRay,etal.2022. FelixYu,andSanjivKumar.2020.Modifyingmemoriesintransformermodels.
Traininglanguagemodelstofollowinstructionswithhumanfeedback.Advances arXivpreprintarXiv:2012.00363(2020).
inNeuralInformationProcessingSystems35(2022),27730–27744.
[25] AlecRadford,JeffreyWu,RewonChild,DavidLuan,DarioAmodei,IlyaSutskever, Received20February2007;revised12March2009;accepted5June2009
etal.2019.Languagemodelsareunsupervisedmultitasklearners.OpenAIblog
1,8(2019),9.
[26] CARNEGIE-MELLONUNIVPITTSBURGHPADEPTOFCOMPUTERSCIENCE.
1977.SpeechUnderstandingSystems.SummaryofResultsoftheFive-YearResearch
EffortatCarnegie-MellonUniversity.
[27] GeorgiosSidiropoulos,NikosVoskarides,SvitlanaVakulenko,andEvangelos
Kanoulas.2021. CombiningLexicalandDenseRetrievalforComputationally
EfficientMulti-hopQuestionAnswering.InProceedingsoftheSecondWorkshop
onSimpleandEfficientNaturalLanguageProcessing,NafiseSadatMoosavi,Iryna
Gurevych,AngelaFan,ThomasWolf,YufangHou,AnaMarasović,andSujith
Ravi(Eds.).AssociationforComputationalLinguistics,Virtual,58–63. https:
//doi.org/10.18653/v1/2021.sustainlp-1.7
[28] HaitianSun,BhuwanDhingra,ManzilZaheer,KathrynMazaitis,RuslanSalakhut-
dinov,andWilliamWCohen.2018. Opendomainquestionansweringusing
earlyfusionofknowledgebasesandtext.arXivpreprintarXiv:1809.00782(2018).
[29] JiashuoSun,ChengjinXu,LumingyuanTang,SaizhuoWang,ChenLin,Yeyun
Gong,Heung-YeungShum,andJianGuo.2023. Think-on-graph:Deepand
responsiblereasoningoflargelanguagemodelwithknowledgegraph. arXiv
preprintarXiv:2307.07697(2023).
[30] HugoTouvron,ThibautLavril,GautierIzacard,XavierMartinet,Marie-Anne
Lachaux,TimothéeLacroix,BaptisteRozière,NamanGoyal,EricHambro,Faisal
Azhar,etal.2023.Llama:Openandefficientfoundationlanguagemodels.arXiv
preprintarXiv:2302.13971(2023).
[31] HugoTouvron,LouisMartin,KevinStone,PeterAlbert,AmjadAlmahairi,Yas-
mineBabaei,NikolayBashlykov,SoumyaBatra,PrajjwalBhargava,ShrutiBhos-
ale,etal.2023. Llama2:Openfoundationandfine-tunedchatmodels. arXiv
preprintarXiv:2307.09288(2023).
[32] DennyVrandečićandMarkusKrötzsch.2014. Wikidata:afreecollaborative
knowledgebase.Commun.ACM57,10(2014),78–85.
[33] BenWangandAranKomatsuzaki.2021.GPT-J-6B:A6BillionParameterAutore-
gressiveLanguageModel.https://github.com/kingoflolz/mesh-transformer-jax.
[34] WeixuanWang,BarryHaddow,andAlexandraBirch.2023.Retrieval-augmented
MultilingualKnowledgeEditing.arXivpreprintarXiv:2312.13040(2023).
[35] XintaoWang,QianwenYang,YongtingQiu,JiaqingLiang,QianyuHe,Zhouhong
Gu,YanghuaXiao,andWeiWang.2023.Knowledgpt:Enhancinglargelanguage
modelswithretrievalandstorageaccessonknowledgebases. arXivpreprint
arXiv:2308.11761(2023).
[36] YiweiWang,MuhaoChen,NanyunPeng,andKai-WeiChang.2024.DeepEdit:
KnowledgeEditingasDecodingwithConstraints.arXivpreprintarXiv:2401.10471
(2024).
[37] ThomasWolf,LysandreDebut,VictorSanh,JulienChaumond,ClementDelangue,
AnthonyMoi,PierricCistac,TimRault,RémiLouf,MorganFuntowicz,etal.
2019.Huggingface’stransformers:State-of-the-artnaturallanguageprocessing.
arXivpreprintarXiv:1910.03771(2019).
[38] XuanshengWu,HuachiZhou,WenlinYao,XiaoHuang,andNinghaoLiu.2023.
TowardsPersonalizedCold-StartRecommendationwithPrompts.arXivpreprint
arXiv:2306.17256(2023).
[39] SangMichaelXie,AditiRaghunathan,PercyLiang,andTengyuMa.2021.An
explanationofin-contextlearningasimplicitbayesianinference.arXivpreprint
arXiv:2111.02080(2021).
[40] WenhanXiong,MoYu,XiaoxiaoGuo,HongWang,ShiyuChang,MurrayCamp-
bell,andWilliamYangWang.2019. Simpleyeteffectivebridgereasoningfor
open-domainmulti-hopquestionanswering. arXivpreprintarXiv:1909.07597
(2019).
[41] VikasYadav,StevenBethard,andMihaiSurdeanu.2020.Unsupervisedalignment-
basediterativeevidenceretrievalformulti-hopquestionanswering. arXiv
preprintarXiv:2005.01218(2020).
[42] MichihiroYasunaga,HongyuRen,AntoineBosselut,PercyLiang,andJure
Leskovec.2021. QA-GNN:Reasoningwithlanguagemodelsandknowledge
graphsforquestionanswering.arXivpreprintarXiv:2104.06378(2021).
[43] YuexiangZhai,ShengbangTong,XiaoLi,MuCai,QingQu,YongJaeLee,and
YiMa.2023. InvestigatingtheCatastrophicForgettinginMultimodalLarge
LanguageModels.arXivpreprintarXiv:2309.10313(2023).
[44] JingZhang,XiaokangZhang,JifanYu,JianTang,JieTang,CuipingLi,andHong
Chen.2022.Subgraphretrievalenhancedmodelformulti-hopknowledgebase
questionanswering.arXivpreprintarXiv:2202.13296(2022).
[45] CeZheng,LeiLi,QingxiuDong,YuxuanFan,ZhiyongWu,JingjingXu,and
BaobaoChang.2023.CanWeEditFactualKnowledgebyIn-ContextLearning?
arXivpreprintarXiv:2305.12740(2023).
[46] ZexuanZhong,ZhengxuanWu,ChristopherDManning,ChristopherPotts,and
DanqiChen.2023.MQuAKE:AssessingKnowledgeEditinginLanguageModels
viaMulti-HopQuestions.arXivpreprintarXiv:2305.14795(2023).