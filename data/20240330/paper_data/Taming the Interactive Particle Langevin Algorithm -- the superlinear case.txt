Taming the Interacting Particle Langevin Algorithm
- the superlinear case
Tim Johnston1, Nikolaos Makras1, and Sotirios Sabanis1,2,3
1School of Mathematics, University of Edinburgh, UK
2The Alan Turing Institute, UK
3National Technical University of Athens, Greece
March 2024
Abstract
Recent advances in stochastic optimization have yielded the interactive parti-
cle Langevin algorithm (IPLA), which leverages the notion of interacting particle
systems (IPS) to efficiently sample from approximate posterior densities. This
becomes particularly crucial within the framework of Expectation-Maximization
(EM), where the E-step is computationally challenging or even intractable. Al-
though prior research has focused on scenarios involving convex cases with gra-
dients of log densities that grow at most linearly, our work extends this frame-
work to include polynomial growth. Taming techniques are employed to produce
an explicit discretization scheme that yields a new class of stable, under such
non-linearities, algorithms which are called tamed interactive particle Langevin
algorithms (tIPLA). We obtain non-asymptotic convergence error estimates in
Wasserstein-2 distance for the new class under an optimal rate.
1 Introduction
The Expectation-Maximization (EM) algorithm is widely used for locating maximizers
of posterior distributions. Applications span, but are not confined to, hyperparameter
estimation, mixture models, hidden variable models, and variational inference [4]. At
its essence, each iteration of the EM algorithm consists of two fundamental steps: the
Expectation (E) and the Maximization (M) step. The algorithm is defined by alter-
nating iteratively between these two steps. Given a data specification p (x,y) param-
θ
eterized by θ, where x represents the latent variable (interpreted as incomplete data)
1
4202
raM
82
]RP.htam[
1v78591.3042:viXraand y the observed data, our aim is to find θ∗ that maximizes the marginal likelihood
q (x) = p (x,y)dy.
θ Rdx θ
WhentheintegralintheE-stepiscomputationallychallengingorintractable, Markov
R
ChainMonteCarlo(MCMC)methodsareoftenemployed, traditionallyusingMetropolis-
Hastings-type algorithms [3]. However, these approaches introduce scalability concerns
and are susceptible to local mode entrapment.
In this landscape, a new method was introduced by [21], such that samples of the la-
tent space variable were generated via an Unadjusted Langevin Algorithm (ULA) chain.
Nevertheless, thegeneral applicabilityofthese resultsiscurtailedbyparticularchoices of
step sizes. Analternative avenue was pioneered in[11], inwhich thestudy of thelimiting
behaviour of various gradient flows associated with appropriate free energy functionals
led to an interacting particle system (IPS) that provides efficient estimates for maxi-
mum likelihood estimations. Further, [1] thoughtfully expanded on this framework by
injecting noise into the dynamics of the parameter θ itself, thereby transitioning from
deterministic tostochastic dynamics. This key modificationprovides astochastic system
with an invariant measure, allowing the establishment of non-asymptotic convergence to
θ∗, the maximizer of the marginal likelihood.
In our work, while the convexity assumption is maintained, we address the challenge
posed by superlinear growth exhibited by gradients of log densities, which makes other
known algorithms, such as vanilla Langevin based algorithms, unstable. To counter-
act this, we implement taming techniques, initially researched for non-globally Lips-
chitz drifts for SDEs in [9] and subsequently in [17] and [18]. The latter approach has
found applications in optimization and machine learning and led to the design of new
MCMC algorithms as one typically deals with high nonlinear objective function, see
e.g. [2],[15],[14],[19]. The underlying principle of all these algorithms is the rescaling of
ULA’s drift coefficient in such a way that maintains stability without significantly in-
creasing computational complexity as in the case of implicit schemes, or by introducing
additional constrains via adaptive stepsizes.
In this paper, we study two new algorithms from the tIPLA class, namely the coor-
dinate wise version, known as tIPLAc, and the uniformly tamed version tIPLAu. Those
algorithms are tamed versions of IPLA (developed in [1]) as explained in Subsection 2.2.
The optimal rate of convergence is recovered for both algorithms and our estimates are
explicit regarding their dependencies ondimension andthenumber ofparticles employed
N.
1.1 Notation
We conclude this subsection with some basic notation. For u,v Rd, define the scalar
∈
product u,v = d u v and the Euclidian norm u = u,u 1/2. For all continuously
h i i=1 i i | | h i
differentiable functions f and we denote by f it’s gradient. The integer part of a real
P ∇
2number x is denoted by x . For any p N, we denote by (Rd) the set of probability
⌊ ⌋ ∈ P
measures on (Rd) and by (Rd) = π : x pdπ(x) < the set of all
B Pp { ∈ P Rd | |p ∞}
probability measures over (Rd) with finite p-th moment. For any two Borel probability
B R
measures µ and ν, we define the Wasserstein distance of order p 1 as
≥
1/p
W (µ,ν) = inf x y pdζ(x,y) ,
p
(cid:18)ζ∈Q(µ,ν) ZRd×Rd| − |
(cid:19)
where by (µ,ν) we denote the set of transference plans of µ and ν. Moreover, for all
µ, ν (Rd), there exists a transference plan ζ∗ (µ,ν) such that for any coupling
p
∈ P Q ∈
(X,Y) distributed according to ζ∗, W (µ,ν) = E[ X Y p]1/p .
p | Q− |
2 Setting and Definitions
2.1 Initial setup
Let p (x, )betheaforementioned jointprobability density functionofthelatent variable
θ
·
x and for fixed (observed) data y. The goal of maximum marginal likelihood estimation
(MMLE)istofindtheparameterθ∗ thatmaximisesthemarginallikelihood(Dempsteret
al.)[4]. To deal with the aforementioned log-density we define the negative log-likelihood
for fixed y Rdy as follows
∈
U (θ,x) := logp (x,y),
y θ
−
thus gaining the following notation for the quantity we are interested in maximizing
k (θ) = q (y) = p (x,y)dx = e−Uy(θ,x)dx. Lastly, formattersofclaritywealsodenote
y θ θ
h (u) = U (u) so that hx(u) = U (u) and hθ(u) = U (u), where u := (θ,x).
y ∇ y R y R ∇x y y ∇θ y
Henceforth, we drop the reference to the (fixed) data, i.e. to y, for reasons of brevity.
Following the convention of [11], N particles i,N for i 1,...,N areused to estimate
Xt ∈ { }
the gradient of q , which are governed by following continuous-time dynamics:
θ
N
1 2
dϑN = U(ϑN, j,N)+ dB0,N, (1)
t −N ∇θ t Xt N t
r
j=1
X
d i,N = U(ϑN, i,N)+√2dBi,N, (2)
Xt −∇x t Xt t
for i = 1,..,N, where (B )i,N N is a family of independent Brownian motions. The
{ t t≥0}i=0
discrete time Markov chain associated with the above IPS (1)-(2) is obtained by the
3corresponding Euler-Maruyama discretization scheme of the given Langevin SDEs:
N
λ 2λ
θλ =θ , θλ = θλ hθ(θλ,Xi,λ)+ ξ(0) ,
0 0 n+1 n − N n n N n+1
r
i=1
X
Xi,λ =xi, Xi,λ = Xi,λ λhx(θλ,Xi,λ)+√2λξ(i) , i 1,...,N ,
0 0 n+1 n − n n n+1 ∀ ∈ { }
where θ Rdθ, xi Rdx, λ > 0 the step-size parameter and for i = 0 and i
0 ∈ 0 ∈ ∀ ∈
1,...,N ,(ξ n(i) ) n∈N are i.i.d. standard dθ and dx respectively dimensional Gaussian
{ }
variables. For reasons that become apparent after the introduction of the taming func-
tions in Subsection 2.2.2 we also consider the following dynamics which are a different
time-scaled version of the original equations (1)-(2):
N
1 2
dϑN = U(ϑN, j,N)+ dB0,N, (3)
t − Np+1 ∇θ t Xt Np+1 t
r
j=1
X
1 2
d i,N = U(ϑN, i,N)+ dBi,N, (4)
Xt − Np∇x t Xt Np t
r
where p is controlled by the order of polynomial growth ℓ in U(θ,x), see Remark
∇
1. Such an adjustment allows the generalization of the results in [1] to the case of
superlinear drift coefficients while keeping the underlying stationary distribution of the
dynamics identical.
2.2 Taming approach
2.2.1 Introduction of the underlying taming technique
For a fixed T > 0, consider an SDE given by
dY(t) = b(t,Y(t))dt+σ(t,Y(t))dB , t [0,T], (5)
t
∀ ∈
where b(t,y) and σ(t,y) are assumed to be (R ) (Rd) measurable functions. Then,
+
B ×B −
the discrete time Markov chain associated with the ULA algorithm is obtained by the
Euler-Maruyama discretization scheme of SDE(5) and is defined for all n N by:
∈
Y(t ) = Y(t ) λb(t ,Y(t ))+√λσ(t ,Y(t ))ξ ,
n+1 n n n n n n+1
−
where λ > 0is thestepsize and(ξ ) arei.i.d. standardGaussianrandomvariables. In
n n≥1
thecase where the drift coefficient b issuperlinear, it isshown in [8] thatULAis unstable
in the sense that any p absolute moment of the algorithm (p 1) diverges to infinity.
− ≥
In the SDE approximation literature, a new class of explicit numerical schemes has been
4introduced to study the case of non-globally Lipschitz conditions by modifying both the
drift and diffusion coefficients in such a way that they grow at most linearly, for example
see, [18], [20] and [10]. The efficiency of such schemes and their respective properties
of p convergence create a strong incentive to extend those techniques to sampling and
L
optimization. This adjustment iskey tothedevelopment ofalgorithmsthatapproximate
non-linear systems while remaining computationally tractable. Typically tamedschemes
are given by,
Y(t ) = Y(t ) λb (t ,Y(t ))+√λσ (t ,Y(t ))ξ ,
n+1 n λ n n λ n n n+1
−
for an appropriate choice of taming functions b : Rd Rd and σ : Rd Rd×d.
λ λ
7→ 7→
2.2.2 Application in the IPLA framework
We extend this notion of taming to our setting when dealing with the superlinear nature
of U andaconstant diffusioncoefficient. To thisend, weintroduce afamilyofataming
∇
functions (h ) with h : Rdθ Rdx Rdθ Rdx which are close approximations of
λ λ≥0 λ
× → ×
U in a sense made precise below.
∇
We suggest two such taming functions h (v), one of which is uniformly tamed and
λ
another by using a coordinate-wise approach:
h(v) µv
h (v) = − +µv, (6)
λ,u 1+λ1/2N−p/2 h(v) µv
| − |
h(i)(v) µv(i)
h (v) = − +µv(i) , (7)
λ,c 1+λ1/2 h(i)(v) µv(i)
(cid:26) | − | (cid:27)i∈{1,...,dθ+dx}
where µ is the strong convexity constant given in A2. In [2] it’s experimentally estab-
lished that the coordinate-wise version outperforms the uniform taming approach. This
is in agreement with the observation that uniform taming cannot distinguish between
the different levels of contribution each coordinate offers to the gradient. However, there
is a trafeoff for using the coordinate-wise approach, as in order to obtain an appropri-
ate dissipativity condition for the tamed function h , one needs to require additional
λ
smoothness on the potential U.
We propose two different algorithms for the EM implementation within the tIPLA
class, which are determined by the choice of the taming function and the stochastic dy-
namics. We find that when the uniform taming is used, then the time-scaled dynamics
(3)-(4) are more suitable as the addition of the exponent in the number of particles N is
essential to guarantee convergence. Hence (3)-(4) paired with (6) leads to the following
5scheme:
N
λ 2λ
θλ,u = θ , θλ,u = θλ,u hθ (θλ,u,Xi,λ,u)+ ξ(0) , (8)
0 0 n+1 n − Np+1 λ,u n n Np+1 n+1
r
i=1
X
λ 2λ
Xi,λ,u = xi, Xi,λ,u = Xi,λ,u hx (θλ,u,Xi,λ,u)+ ξ(i) , i 1,...,N . (9)
0 0 n+1 n − Np λ,u n n Np n+1 ∀ ∈ { }
r
In the case of the coordinate-wise taming, the extra smoothness required obviates any
modification to the original dynamics. Therefore one uses (1)-(2) with (7) to obtain the
second algorithm:
N
λ 2λ
θλ,c = θ , θλ,c = θλ,c hθ (θλ,c,Xi,λ,c)+ ξ(0) , (10)
0 0 n+1 n − N λ,c n n N n+1
r
i=1
X
Xi,λ,c = xi,Xi,λ,c = Xi,λ,c λhx (θλ,c,Xi,λ,c)+√2λξ(i) , i 1,...,N . (11)
0 0 n+1 n − λ,c n n n+1 ∀ ∈ { }
It is important to note that despite the differences in their setup, both algorithms yield
the same non-asymptotic convergence behavior and dependence on the dimension of the
Eoptimization challenge. Bothschemes aresummarised inthealgorithms(tIPLAu) and
(tIPLAc) given below. It is apparent that the newly proposed algorithms, designated
Algorithm 1 Tamed Interactive Particle Langevin Algorithm (tIPLAu)
Require: N,λ,π (Rdθ ) ((Rdx )N)
init
∈ P ×P
Draw (θ , Xi,N N ) from π
0 { 0 }i=1 init
for n = 0 to n = T/λ do
T
⌊ ⌋
λ 2λ
θλ,u = θλ,u N hθ (θλ,u,Xi,λ,u)+ ξ(0)
n+1 n − Np+1 i=1 λ,u n n Np+1 n+1
r
λP 2λ
Xi,λ,u = Xi,λ,u hx (θλ,u,Xi,λ,u)+ ξ(i) , i 1,...,N
n+1 n − Np λ,u n n Np n+1 ∀ ∈
r
end for
return θ
nT+1
as tIPLAu and tIPLAc, incorporate foundational elements that exist in [11], which
employed deterministic dynamics in the θ component. Their design profits also from the
approach in [1] which introduced stochastic noise in the θ direction, allowing for the
−
explicit calculation of the invariant measure. A pivotal enhancement that comes with
this approach involves the application of a taming technique via the recalibration of the
original drift terms, h and h , into tamed counterparts, as described in (6) and (7).
x θ
This adjustment is instrumental in deriving nonasymptotic results, particularly in cases
characterized by superlinear gradients in the log-likelihood function.
6Algorithm 2 Tamed Interactive Particle Langevin Algorithm - Coordinatewise
(tIPLAc)
Require: N,λ,π (Rdθ) ((Rdx)N)
init
∈ P ×P
Draw (θ , Xi,N N ) from π
0 { 0 }i=1 init
for n = 0 to n = T/λ do
T
⌊ ⌋
λ 2λ
θλ,c = θλ,c N hθ (θλ,c,Xi,λ,c)+ ξ(0)
n+1 n − N i=1 λ,c n n N n+1
r
Xi,λ,c = Xi,λ,c Pλhx (θλ,c,Xi,λ,c)+√2λξ(i) , i 1,...,N
n+1 n − λ,c n n n+1 ∀ ∈
end for
return θ
nT+1
2.3 Essential quantities and proof strategy of main result
In this subsection, we lay out the analytical framework and convergence properties of
the sequences (θλ,·) of either tIPLAu (θλ,u) or tIPLAc (θλ,c) , resulting in their
n n≥0 n n≥0 n n≥0
convergence to the minimiser θ∗. A central concept of the technical analysis is the
rescaling of the original dynamics of the following form
N = ϑN,N−1/q 1,N,...,N−1/q N,N , (12)
Zt t Xt Xt
(cid:16) (cid:17)
along with the corresponding rescaling that is required for the algorithms and their
continuous time interpolations, see more details in Appendix A. This rescaled version of
the particle system is pivotal in bridging the dynamics represented by either (1)-(2) or
(3)-(4)withthepairs i,N := (ϑN, i,N)generatedbythealgorithms. Theessenceofthis
Vt t Xt
rescaling lies in its ability to equate the moments of N with the averaged moments of
Zt
the pairs i,N across any Lq norm. However, given our focus on Wasserstein-2 distance,
Vt
we opt for a rescaling factor of q = 2. This choice yields the critical property:
N
1
2 = i 2.
|Zt | N |Vt|
i=1
X
It is imperative to note that this convenient rescaling does not detract from the analysis
objectives, as our primary interest is in the convergence of the θ component. The in-
−
troduction of particles serves primarily to facilitate sampling, hence their specific scaling
does not impact the core convergence analysis.
In the present analysis, the interactive particle systems defined in equations (1)-(2)
and (3)-(4) are deliberately constructed to target the same invariant measure, πN
∗ ∝
exp( N U(θ,xi)). This aspect is critical as it assigns the number of particles N a
− i=1
role akin to the inverse temperature parameter encountered in simulated annealing algo-
P
rithms, thereby regulating the concentration of the θ marginal of the invariant measure
−
7π ΘN, towards the minimizer θ∗. This behavior, wherein π ΘN
→
δ θ∗ as N
→
∞, is estab-
lished in Proposition 2.
Moreover, the convergence rate of the Langevin diffusion to its invariant measure is
a standard result, one could consult [6] and the references within. This is further gener-
alized to particle systems in the research conducted by [1]. By integrating their findings
into our framework, we arrive at Proposition 3. Equipped with these guarantees, we can
now theoretically approximate the minimizer θ∗, and subsequently, our analysis focuses
to the discretization errors inherent in the proposed schemes.
One expects a dimensional scaling of at least (d1/2) for the Wasserstein-2 numerical
O
error of Langevin based algorithms, for example [5]. In the context of the E optimization
where N particles are generated, the true dimentionality of the problem is dθ +N dx,
·
which can be naively interpreted as the rate of convergence worsening as N increases.
However, this dependency on N is effectively mitigated by making use of the implied
symmetry within the dynamics of the interactive particle system by considering only
the θ-marginal in the analysis. Such an approach enables us to attain the classical
Euler-Maruyama convergence rate for the numerical solutions of stochastic differential
equations, as demonstrated in Proposition 4.
In closing, one decomposes the global 2-error, denoted as E θ∗ θλ,· 2 1/2 , into
L | − n |
three distinct components:
(cid:2) (cid:3)
E |θ∗ −θ nλ,· |2 1/2
≤
W 2(δ θ∗,π ΘN)+W 2(π ΘN, L(ϑN nλ))+W 2( L(ϑN nλ), Lθ0(θ nλ,·)),
where θλ(cid:2),· stands for(cid:3)the iterates of either of the algorithms, tIPLAu and tIPLAc alike.
n
Moreover, W 2(δ θ∗,π ΘN) quantifies the deviation of the invariant measure from the min-
imizer, W (πN, (ϑN )) captures the discrepancy between the law of the dynamics and
2 Θ L nλ
their invariant measure, and W ( (ϑN ), (θλ,·)) encompasses the error induced by the
2 L nλ Lθ0 n
discretization scheme, namely, the divergence between the law of the iterations of our
algorithms and their continuous counterparts.
3 Main Assumptions and Results
In this subsection we provide the assumptions on the potential U and it’s gradient U
∇
that define the framework, under which the main results for the non-asymptotic be-
haviour of the newly proposed algorithms, tIPLAu and tIPLAc, are derived.
Let d := dθ +dx, U be a C1 Rd function and recall that h(v) := h(θ,x) := U(θ,x) is
∇
a locally Lipschitz function in θ and x.
(cid:0) (cid:1)
A1 There exist L > 0 and ℓ > 0 such that
h(v) h(v′) L 1+ v ℓ + v′ ℓ v v′ , v,v′ Rd.
| − | ≤ | | | | | − | ∀ ∈
(cid:0) (cid:1)
8Additionally we require U to be µ strongly convex.
−
A2. There exists µ > 0 such that
v v′,h(v) h(v′) µ v v′ 2, v,v′ Rd.
h − − i ≥ | − | ∀ ∈
Assumption A1 is a significant relaxation of the global Lipschitz condition which is
widely used in the literature. It allows the gradient h to grow polynomially fast at
infinity. Assumption A2 guarantees that U has a unique minimiser and also can be
seen as a monotonicity type condition which is satisfied by the drift coefficients of the
corresponding SDEs.
We also present a growth condition for the gradient U, which is essential in the
∇
case where the coordinate-wise taming function (7) is used.
A3. For each i in 1,...,d there exists µ > 0 such that
{ }
µ 1
h(i)(v)v(i) v(i) 2 h(i) 2.
≥ 2| | − 2µ| 0 |
Assumption A3 is a coordinate-wise dissipativity type condition which plays a crucial
role in establishing moment bounds for tIPLAc. Its need stems from the fact that
Remark 2 does not guarantee that the taming function (7) preservers the dissipativity
of the drift h(v) = U(v).
∇
Here, we emphasize that Assumption 3 is primarily influenced by the structural
properties of the taming function. This assumption could potentially be relaxed to
necessitate only argument-wise dissipativity, as described by the following mathematical
formulation:
µ 1 µ 1
hθ(v)θ θ 2 hθ 2 and hx(v)x x 2 hx 2.
≥ 2| | − 2µ| 0| ≥ 2| | − 2µ| 0|
This relaxation is applicable if one considers the taming function
hw(v) µw
hw (v) = − +µw for w = θ, x,
λ,a 1+λ1/2 hw(v) µw
| − |
which represents an intermediate scenario bridging the uniform and coordinate-wise ap-
proaches. Suchconsideration leadstoanalternativeformulationofthetIPLAalgorithm.
However, this variant of the tIPLA algorithm does not capture our interest, primarily
due to its inherent limitations. Specifically, it fails to adequately discriminate between
the components of the gradient that exhibit explosive behavior and those that do not.
While simultaneously, this alternative formulation imposes more stringent requirements
than those necessitated by the uniform case.
9Remark 1. One notices that assumption A1 allows for control of the growth of
h(v) = U(v), i.e., for every v Rd,
∇ ∈
h(v) K(1+ v ℓ+1),
≤ | |
where K = 2L+ h .
0
| |
Remark 2. In view of assumption A2, strong convexity implies dissipativity, i.e, for
every v Rd,
∈ µ
v,h(v) v 2 b,
h i ≥ 2| | −
1
where b = h 2.
0
2µ| |
3.1 Taming functions and inherited properties
Important properties of the suggested taming functions, which act as drift coefficients
in the corresponding numerical schemes, are established in the section.
Property 1. For all λ > 0 and u Rd one has
∈
h (v) µ v +λ−1/2Np/2.
λ,u
| | ≤ | |
The original and tamed functions are sufficiently close for λ > 0 small enough.
Property 2. For all λ > 0 and u Rd one has
∈
h (v) h(v) C λ1/2N−p/2(1+ v 2(ℓ+1)),
λ,u 1
| − | ≤ | |
where C = 22(ℓ+3/2)max K2,µ2 .
1
{ }
The tamed function h (v) inherits the dissipativity condition established in Remark 2.
λ,u
Property 3. For all λ > 0 and u Rd one has
∈
µ
v,h (v) v 2 b,
λ,u
h i ≥ 2| | −
where b is given in Remark 2. To see this consider the cases in which h(v) µv,v is
h − i
greater or less than 0 separately.
10In the context of unified framework established in [12], the properties 1-3 therein are
recovered for δ = 2 and γ = 1/2 which is on par with THǫO POULA [13] and TUSLA
[15] aglorithms.
Comment. Regarding tIPLAc, where the coordinate-wise taming function (7) is used,
Properties 1-2 are also guaranteed without the N-factor and also a slightly worse coef-
ficient in Property 1, that is: h (v) µ v +(dθ +dx)λ−1/2. However for Property 3
λ,c
| | ≤ | |
to be recovered, we require the additional smoothness provided by assumption A3.
3.2 Preliminary Theoretical Statements
Both interacting SDEs defined by (1)-(2) and (3)-(4) respectively, admit a strong solu-
tion under A1-A2. This follows since each of the drift coefficients of the SDEs are locally
Lipschitz functions and dissipative in view of Remark 2 whereas the diffusion coefficients
are constant, one could consult [16](Theorem 2.3.5) for more details. Moreover, they
exhibit the same invariant measure, as one would expect from a standard Langevin dif-
fusion.
Proposition 1. Let A1 and A2 hold, then for any N N,
the measure πN(θ,x1,...,xN) exp−PN i=1U(θ,xi) is th∈ e invariant measure for the IPS
∗ ∝
(1)-(2) and (3)-(4) likewise.
Proof. The existence of an invariant measure is established by invoking the Krylov-
Bogoliubov Theorem, as presented in Theorem 7.1 [7]. This theorem guarantees the
existence of an invariant measure for a Markov process generated by it’s corresponding
semigroup, given a tightness condition on the sequence of probability measured associ-
ated with that process. According to Da Prato’s Proposition 7.10 [7], the establishment
ofuniformmomentboundsservesasasufficient conditionfortightness, whichisprovided
in Lemma 1. The uniqueness of the invariant measure follows from Theorem 7.16(ii)
[7], as Hypothesis 7.13 is satisfied due to A1 and A2. Furthermore, one can verify that
πN(θ,x1,...,xN) exp−PN i=1U(θ,xi) is indeed the invariant measure in discussion by a
∗ ∝
direct substitution at the Fokker-Planck equation.
Henceforward we consider exclusively the θ marginal of the invariant measure and we
−
show that indeed the number of particles N plays the role of the inverse temperature
parameter in temperature annealing algorithms in the sense that it grants us control on
the marginal’s concentration around the minimiser.
11Proposition 2. Let πN denote the θ marginal of the invariant measure and θ∗ as the
Θ −
maximiser of k(θ). Then, under A1 and A2, for any N N and any initial condition
∈
(θ ,x1:N) one has the bound
0 0
2dθ
W 2(π ΘN,δ θ∗)
≤
sµN.
Proof. Follows from Proposition 3 in [1].
Lastly we need the following ergodicity result to obtain the convergence of the law of
the IPS to the invariant measure.
Proposition 3. Let A1 and A2 hold and consider the first component, namely ϑN,
t
of the continuous dynamics (3)-(4) rescaled as described in (12). Then, for any N N
∈
and any initial condition (θ ,x1:N)
0 0
dxN +dθ 1/2
W (ϑN),πN e−µt z z∗ + .
2 Lθ0 t Θ ≤ | 0 − | µN
!
(cid:18) (cid:19)
(cid:0) (cid:1)
Proof. Follows from Proposition 4 in [1].
3.3 Discretisation Error Estimates
We consider the continuous interpolation of the algorithm tIPLAu which approximates
λ,u
the time-scaled version of the continuous dynamics (3)-(4) denoted as Z . We note
t
that the law of the discretized process and it’s interpolation coincide at grid-points, i.e.
(Zλ,u) = (Zλ,u ). The continuous-time interpolation of the tIPLAu can be defined as:
L n L n
N
λ 2λ
θλ,u =θ , dθλ,u = hθ (θλ,u,Xi,λ,u)dt+ dB0,λ, (13)
0 0 t −Np+1 λ,u ⌊t⌋ ⌊t⌋ Np+1 t
r
i=1
X
λ 2λ
Xi,λ,u =xi, dXi,λ,u = hx (θλ,u,Xi,λ,u)dt+ dBi,λ, (14)
0 0 t −Np λ,u ⌊t⌋ ⌊t⌋ Np t
r
for all i 1,...,N , while the time-changed SDEs (3)-(4)are given by:
∈ { }
N
λ 2λ
dϑN = hθ(ϑN, i,N)dt+ dB0,λ, (15)
λt −Np+1 λt Xλt Np+1 t
r
i=1
X
λ 2λ
d i,N = hx(ϑN, i,N)dt+ dBi,λ, (16)
Xλt −Np λt Xλt Np t
r
12where in both cases Bλ := B /√λ,t 0, is a Brownian motion under its completed
t λt ≥
natural filtration λ := .
Ft Fλt
Proposition 4. Let A1 and A2 hold. Then, for every λ < N2ℓ+1/4µ, there exists
0
a constant C > 0 independent of N,n,λ such that for any λ (0,λ ) one has
0
∈
E θλ,u ϑN 2 1/2 λ1/2C (1+dθ/N +dx)ℓ+1 ,
| n − nλ| ≤ |z0|,µ,b,ℓ,L
h i
for all n N.
∈
Proof. The proof is postponed to Appendix A.
Following the same lines we define the corresponding auxiliary processes for the iterates
of tIPLAc. The continuous-time interpolation is given by:
N
λ 2λ
θλ,c = θ , dθλ,c = hθ (θλ,c,Xi,λ,c)dt+ dB0,λ, (17)
0 0 t −N λ,c ⌊t⌋ ⌊t⌋ N t
r
i=1
X
Xi,λ,c = xi, dXi,λ,c = λhx (θλ,c,Xi,λ,c)dt+√2λdBi,λ, (18)
0 0 t − λ,c ⌊t⌋ ⌊t⌋ t
and the time changed SDEs of (1)-(2) by
N
λ 2λ
dϑN = hθ(ϑN, i,N)dt+ dB0,λ, (19)
λt −N λt Xλt N t
r
i=1
X
d i,N = λhx(ϑN, i,N)dt+√2λdBi,λ, (20)
Xλt − λt Xλt t
where the Brownian motions are defined as in (15)-(16).
Proposition 5. Let A1, A2 and A3 hold. Then, for every λ < 1/4µ, there exists
0
a constant C > 0 independent of N,n,λ such that for any λ (0,λ ) one has
0
∈
E θλ,c ϑN 2 1/2 λ1/2C (1+dθ +dx)ℓ+1 ,
| n − nλ| ≤ |z0|,µ,b,ℓ,L
h i
for all n N.
∈
Proof. The proof is postponed to Appendix A.
133.4 Main result and global error
Theorem 1. Consider the iterates (θλ,u) as given in tIPLAu, and let A1 and A2
n n≥0
hold. Then, for every λ < N2ℓ+1/4µ, there exists a constant C > 0 independent of
0
N,n,λ such that, for any λ (0,λ ), one has
0
∈
2dθ dxN +dθ 1/2
E θ∗ θλ,u 2 1/2 +e−µnλ/N2ℓ+1 z z∗ +
| − n | ≤ sµN | 0 − | µN
!
(cid:18) (cid:19)
(cid:2) (cid:3)
+λ1/2C (1+dθ/N +dx)ℓ+1,
|z0|,µ,b,ℓ,L
for all n N.
∈
Proof. Combining the results from Propositions 2, 3 and 4, we are able to decompose
the expectation into a term describing the concentration of the πN around θ∗, a term
Θ
describing the convergence of the IPS to its invariant measure, and a term describing
the error induced by the time discretisation:
E |θ∗ −θ nλ,· |2 1/2 = W 2(δ θ∗, Lθ0(θ nλ,·))
(cid:2) (cid:3)
≤
W 2(δ θ∗,π ΘN)+W 2(π ΘN, L(ϑN nλ))+W 2( L(ϑN nλ), Lθ0(θ nλ,·)). (21)
Substituting theboundsfromtheaforementionedPropositionsyieldsthefinalinequality.
Theorem 2. Consider the iterates (θλ,c) as given in tIPLAc, and let A1, A2 and A3
n n≥0
hold. Then, for every λ < 1/4µ, there exists a constant C > 0 independent of N,n,λ
0
such that, for any λ (0,λ ), one has
0
∈
2dθ dxN +dθ 1/2
E θ∗ θλ,c 2 1/2 +e−µnλ z z∗ +
| − n | ≤ sµN | 0 − | µN
!
(cid:18) (cid:19)
(cid:2) (cid:3)
+λ1/2C (1+dθ +dx)ℓ+1,
|z0|,µ,b,ℓ,L
for all n N.
∈
Proof. To prove the above inequality, one replaces the bound for W ( (ϑN ), (θλ,·))
2 L nλ Lθ0 n
in (21) given by Proposition 4 by the bound given by Proposition 5.
It is observed that the dependence on the dimension dθ of the parameter space in The-
orem 2 exhibits a slight deterioration compared to that presented in Theorem 1 and in
Theorem 1 of [1]. Notably, the factor 1/N is absent in the third term of Theorem 2.
This is interpreted as a compromise entailed by the use of the coordinate-wise taming
function (7), which requires the direct derivation of moment bounds for the pairs i,N,
Vt
as discussed in Subsection 2.3. Consequently, this approach results in the loss of the
symmetrical structure that is attained by considering the N-particle system i,N in its
Xt
entirety.
14References
[1] O¨. Deniz Akyildiz et al. Interacting Particle Langevin Algorithm for Maximum
Marginal Likelihood Estimation. 2023.
[2] Nicolas Brosse et al. “The tamed unadjusted Langevin algorithm”. In: Stochastic
Processes and their Applications 129.10 (2019), pp. 3638–3663. issn: 0304-4149.
[3] Ngoc Huy Chau et al. “On Stochastic Gradient Langevin Dynamics with Depen-
dent Data Streams: The Fully Nonconvex Case”. In: SIAM Journal on Mathemat-
ics of Data Science 3.3 (2021), pp. 959–986.
[4] A. P. Dempster, N. M. Laird, and D. B. Rubin. “Maximum Likelihood from In-
complete Data via the EM Algorithm”. In: Journal of the Royal Statistical Society.
Series B (Methodological) 39.1 (1977), pp. 1–38. issn: 00359246.
[5] Alain Durmus and E´ric Moulines. “High-dimensional Bayesian inference via the
unadjusted Langevin algorithm”. In: Bernoulli 25.4A (2019), pp. 2854–2882.
[6] AlainDurmusandE´ricMoulines. “Nonasymptoticconvergence analysisfortheun-
adjusted Langevin algorithm”. In: The Annals of Applied Probability 27.3 (2017),
pp. 1551–1587.
[7] Da Prato G. An Introduction to Infinite-Dimensional Analysis. Springer, 2006.
[8] Martin Hutzenthaler, Arnulf Jentzen, and Peter E. Kloeden. “Strong and weak
divergence in finite time of Euler’s method for stochastic differential equations
with non-globally Lipschitz continuous coefficients”. In: Proceedings of the Royal
Society A: Mathematical, Physical and Engineering Sciences 467.2130(Dec. 2010),
pp. 1563–1576. issn: 1471-2946.
[9] Martin Hutzenthaler, Arnulf Jentzen, and Peter E. Kloeden. “Strong convergence
of an explicit numerical method for SDEs with nonglobally Lipschitz continuous
coefficients”. In: The Annals of Applied Probability 22.4 (Aug. 2012). issn: 1050-
5164.
[10] Chaman Kumar andSotiriosSabanis. “Onexplicit approximations forL´evy driven
SDEs with super-linear diffusion coefficients”. In: Electronic Journal of Probability
22.none (2017), pp. 1–19.
[11] Juan Kuntz, Jen Ning Lim, and Adam M. Johansen. “Particle algorithms for max-
imum likelihood training of latent variable models”. In: Proceedings of The 26th
International Conference on Artificial Intelligence and Statistics. Ed. by Francisco
Ruiz,JenniferDy,andJan-WillemvandeMeent. Vol.206.ProceedingsofMachine
Learning Research. PMLR, 2023, pp. 5134–5180.
15[12] Dong-YoungLimandSotiriosSabanis.Polygonal Unadjusted LangevinAlgorithms:
Creating stable and efficient adaptive algorithms for neural networks. 2021.
[13] Dongjae Lim et al. “Langevin dynamics based algorithm e-THǫO POULA for
stochasticoptimizationproblemswithdiscontinuousstochasticgradient”.In:ArXiv
abs/2210.13193 (2022).
[14] Dongjae Lim et al. “Non-asymptotic estimates for TUSLA algorithm for non-
convex learning with applications to neural networks with ReLU activation func-
tion”. In: ArXiv abs/2107.08649 (2021).
[15] Attila Lovas et al. “Taming Neural Networks with TUSLA: Nonconvex Learning
via Adaptive Stochastic Gradient Langevin Algorithms”. In: SIAM Journal on
Mathematics of Data Science 5.2 (2023), pp. 323–345.
[16] Xuerong Mao. Stochastic Differential Equations and Applications. Elsevier, 2007.
[17] Sotirios Sabanis. “A note on tamed Euler approximations”. In: Electronic Com-
munications in Probability 18.none (2013), pp. 1–10.
[18] Sotirios Sabanis. “Euler approximations with varying coefficients: The case of su-
perlinearly growing diffusion coefficients”. In: The Annals of Applied Probability
26.4 (Aug. 2016). issn: 1050-5164.
[19] SotiriosSabanisandYingZhang.“HigherorderLangevinMonteCarloalgorithm”.
In: Electronic Journal of Statistics 13.2 (2019), pp. 3805–3850.
[20] SotiriosSabanis andYingZhang. “Onexplicit order 1.5approximations withvary-
ing coefficients: The caseof super-linear diffusion coefficients”. In: Journal of Com-
plexity 50 (Feb. 2019), pp. 84–115. issn: 0885-064X.
[21] De Bortoli V., Durmus A., and Pereyra M. et al. Efficient stochastic optimisation
by unadjusted Langevin Monte Carlo. 2021.
A Proofs of subsection 3
A.1 Uniform moments bounds
Lemma 1. Consider either thedynamics givenby (1)-(2)or(3)-(4)andlet Assumptions
A1-A2 hold, then there exists a constant C > 0 such that
sup E N 2 C,
|Zt | ≤
0≤t≤T
(cid:2) (cid:3)
for every T > 0.
16Proof. Consider the rescaling dynamics as given in (12) for the original system of equa-
tions (1)-(2):
N
1
N 2 = ϑN 2 + i,N 2.
|Zt | | t | N |Xt |
i=1
X
Oneviastandardargumentsusingstoppingtimes, Gr¨onwall’slemmaandFatou’sobtains
that there is a constant c, which depends on time, such that sup E N 2 c for
0≤t≤T |Zt | ≤
all T > 0. Moreover, through the use of Itˆo’s formula we derive
(cid:2) (cid:3)
t 1 N 2 2 t
N 2 ϑN 2 2 ϑN, hθ( i,N) ds+ dθt+2 ϑNdB0,N
|Zt | ≤ | 0 | − h s N Vs i N N s s
Z0 i=1 r r Z0
X
1 N 2 N t 2√2 N t
+ i,N 2 i,N,hx( i,N) ds+√2dxt+ i,NdBi,Nds
N |X0 | − N hXs Vs i N Xs s
i=1 i=1 Z0 i=1 Z0
X X X
t 1 N
N 2 2 i,N,h( i,N) ds+√2(dθ/N +dx)t
≤ |Z0 | − N hVs Vs i
Z0
i=1
X
2 t 2√2 N t
+2 ϑNdB0,N + i,NdBi,Nds
N s s N Xs s
r Z0 i=1 Z0
X
t
N 2 µ N 2ds+2bt+√2(dθ/N +dx)t
≤ |Z0 | − |Zs |
Z0
2 t 2√2 N t
+2 ϑNdB0,N + i,NdBi,Nds.
N s s N Xs s
r Z0 i=1 Z0
X
Taking the expectation on both sides results in
t
E N 2 E N 2 µ E N 2 ds+(2b+√2dθ/N +√2dx)t.
|Zt | ≤ |Z0 | − |Zs |
Z0
(cid:2) (cid:3) (cid:2) (cid:3) (cid:2) (cid:3)
One notices that
d
eµtE N 2 eµtC,
dt |Zt | ≤
which via integrating implies (cid:2) (cid:3)
E N 2 C/µ sup E N 2 C,
|Zt | ≤ ⇒ |Zt | ≤
0≤t≤T
(cid:2) (cid:3) (cid:2) (cid:3)
as C is a constant independent of time. The corresponding result regarding equations
(3)-(4) follows by going through the same steps, where the calculations differ only up to
a constant in the SDEs coefficients.
17A.2 Uniformly tamed scheme - tIPLAu
A.2.1 Key quantities for the proof of the main Lemmas.
The following definitions that refer to the rescaled dynamics (13)-(14) of the algorithm
tIPLAu and its continuous time interpolations (15)-(16) are given as:
Zλ,u = θλ,u ,N−1/2X1,λ,u,...,N−1/2XN,λ,u , (22)
n n+1 n+1 n+1
(cid:16) (cid:17)
Zλ,u
=
θλ,u ,N−1/2X1,λ,u ,...,N−1/2XN,λ,u
, (23)
t t t t
(cid:16) (cid:17)
N = ϑN,N−1/2 1,N,...,N−1/2 N,N . (24)
Zλt λt Xλt Xλt
(cid:16) (cid:17)
A.2.2 Moment and increment bounds
Lemma 2. Let A1 and A2 hold. Then, for any 0 λ < Np/4µ, it holds that
≤
E Zλ,u 2 C (1+dθ/N +dx),
| n | ≤ |z0|,µ,b
for a constant C > 0 indepe(cid:2) ndent o(cid:3) f N,n,λ,dx and dθ, which is given in the proof.
Proof. Consider the rescaled iterates as described in (22):
N
2 2 1 2
Zλ,u = θλ,u + Xi,λ,u
n+1 n+1 N n+1
(cid:12) (cid:12) (cid:12) (cid:12) Xi=1 (cid:12) (cid:12)
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12)2 (cid:12)
(cid:12) λ (cid:12) N (cid:12) (cid:12) (cid:12) 2(cid:12)λ
= θλ,u hθ (θλ,u,Xi,λ,u) + ξ(0) 2
(cid:12) n − Np+1 λ,u n n (cid:12) Np+1| n+1|
(cid:12) Xi=1 (cid:12)
(cid:12) N (cid:12)
(cid:12) 2λ λ (cid:12)
+2(cid:12) θλ,u hθ (θλ(cid:12),u,Xi,λ,u),ξ(0)
Np+1 n − Np+1 λ,u n n n+1
r * +
i=1
X
N 2
1 λ 2λ
+ Xi,λ,u hx (θλ,u,Xi,λ,u) + ξ(i) 2
N n − Np λ,u n n Np| n+1|
i=1 (cid:12) (cid:12)
X (cid:12) (cid:12)
2λ(cid:12) λ (cid:12)
+ 2 (cid:12) Xi,λ,u hx (θλ,u,Xi,λ(cid:12),u),ξ(i) .
Np n − Np λ,u n n n+1
r (cid:28) (cid:29)!
18Taking the conditional expectation on both sides with respect to the filtration generated
by Zλ,u the cross terms are vanishing to 0 due to the independence between the ξ(i) ’s
n n+1
and Zλ,u, yielding
n
N
2 2λ
E Zλ,u Zλ,u = E θλ,u 2 Zλ,u E θλ,u,hθ (θλ,u,Xi,λ,u) Zλ,u
n+1 | n n | n − Np+1 h n λ,u n n i| n
(cid:20) (cid:12) (cid:12) (cid:21) h(cid:12) (cid:12) i Xi=1 (cid:2) (cid:3)
(cid:12) (cid:12) (cid:12) (cid:12) λ(cid:12) 2 (cid:12) N 2 2λdθ
+ E hθ (θλ,u,Xi,λ,u) Zλ,u +
N2(p+1)  λ,u n n | n  Np+1
(cid:12) (cid:12)
(cid:12)Xi=1 (cid:12)
(cid:12) (cid:12)
N  (cid:12) (cid:12) 
1
+ E X(cid:12) i,λ,u 2 Zλ,u (cid:12)
N n | n
Xi=1 h
(cid:12) (cid:12)
i
N (cid:12) (cid:12)
2λ
E Xi,λ,hx (θλ,u,Xi,λ,u) Zλ,u
− Np+1 h n λ,u n n i| n
i=1
X (cid:2) (cid:3)
λ2 N 2λdx
+ E hx (θλ,u,Xi,λ,u) 2 Zλ,u + .
N2p+1 | λ,u n n | | n Np
i=1
X (cid:2) (cid:3)
Furthermore by using the elementary inequality (t +...+t )p mp−1(tp+...,tp ) and
1 m ≤ 1 m
the fact that all of the expressions within the conditional expectations are measurable
we obtain
2 1 N 2λdθ 2λdx
E Zλ,u Zλ,u θλ,u + Xi,λ,u 2 + ++
n+1 | n ≤ | n | N | n | Np+1 Np
(cid:20) (cid:12) (cid:12) (cid:21) Xi=1
(cid:12) (cid:12) N
(cid:12) (cid:12) 2λ
θλ,hθ (θλ,u,Xi,λ,u)
− Np+1 h n λ,u n n i
i=1
X
N
2λ
Xi,λ,u,hx (θλ,u,Xi,λ,u)
− Np+1 h n λ,u n n i
i=1
X
λ2 N λ2 N
+ hθ(θλ,u,Xi,λ,u) 2 + hx (θλ,u,Xi,λ,u) 2.
N2p+1 | λ n n | N2p+1 | λ,u n n |
i=1 i=1
X X
Recalling that (hθ ,hx ) = h , further leads to
λ,u λ,u λ,u
2 2λ N λ2 N
E Zλ,u Zλ,u Zλ,u 2 Vi,λ,u,h (Vi,λ,u) + h (Vi,λ,u) 2
n+1 | n ≤ n − Np+1 h n λ,u n i N2p+1 | λ,u n |
(cid:20) (cid:12) (cid:12) (cid:21)
(cid:12) (cid:12)
Xi=1 Xi=1
(cid:12) (cid:12) (cid:12)2λ (cid:12)
(cid:12) (cid:12) + (dθ/N +dx).
Np
19Now using Properties 1 and 3 of the taming function, we get
N
2 µλ 2λb
E Zλ,u Zλ,u Zλ,u 2 Vi,λ,u 2 +
n+1 | n ≤ n − Np+1 n Np
(cid:20) (cid:12) (cid:12) (cid:21)
(cid:12) (cid:12)
Xi=1
(cid:12) (cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
(cid:12) 2λ2µ2(cid:12) N (cid:12) 2λ2λ(cid:12)−1Np
2λ
+ Vi,λ,u 2 + + (dθ/N +dx)
N2p+1 n N2p Np
i=1
X(cid:12) (cid:12)
1 λµ +(cid:12) 2λ2µ(cid:12)2 Zλ,u 2 + 2λ b+1+dθ/N +dx .
≤ − Np N2p n Np
(cid:18) (cid:19)
(cid:12) (cid:12) (cid:0) (cid:1)
Np (cid:12) (cid:12)
By considering the restriction λ < and iterating the above bound, we conclude with
4µ
2 µλ n 2 1 (1 µλ/2)n 2λ
E Zλ,u 1 E Zλ,u + − − (b+1+dθ/N +dx)
n+1 ≤ − 2Np 0 µλ/2Np Np
(cid:20) (cid:21) (cid:18) (cid:19) (cid:20) (cid:21)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) 2 4 (cid:12) (cid:12)
(cid:12) (cid:12) E Zλ,u + (b(cid:12) +1(cid:12)+dθ/N +dx)
≤ 0 µ
(cid:20) (cid:21)
(cid:12) (cid:12)
(cid:12) (cid:12) 2 4
E (cid:12) Zλ (cid:12),u + (b+1) (1+dθ/N +dx)
≤ 0 µ
(cid:18) (cid:20) (cid:21) (cid:19)
(cid:12) (cid:12)
C (cid:12) (1+(cid:12) dθ/N +dx).
≤ |z0|,µ(cid:12),b (cid:12)
Lemma 3. Let A1 and A2 hold true so Lemma 2 holds as well. Then, for any 0 λ <
≤
Np/4µ and q [2, ) N, it holds that,
∈ ∞ ∩
E Zλ,u 2q C (1+dθ/N +dx)q,
| n | ≤ |z0|,b,q,µ
for a constant C > 0 indep(cid:2) endent(cid:3) of N,n,λ,dx and dθ.
20Proof. To make the forthcoming calculations more readable we define the following aux-
iliary processes:
N
λ 2λ
∆λ,θ = θλ,u hθ θλ,Xi,λ , Gλ,θ = ξ(0) ,
n n − Np+1 λ,u n n n Np+1 n+1
r
i=1
X (cid:0) (cid:1)
λ 2λ
∆λ,x,i = Xi,λ,u hx θλ,Xi,λ , Gλ,x,i = ξ(i) ,
n n − Np λ,u n n n Np n+1
r
(cid:0) (cid:1)
but from this point onwards till the completion of the proof we denote λ/Np by just λ
to make the computations less hectic. Recall that have already established via Lemma
2 the bound
N
1 λµ
Aλ := ∆λ,θ 2 + ∆λ,i,x 2 1 Zλ,u 2 +2λC,
n | n | N | n | ≤ − 2 | n |
i=1 (cid:18) (cid:19)
X
and lastly let us define the quantity
N N
2 1
Bλ = 2 ∆θ,Gλ,θ + ∆λ,x,i,Gλ,x,i + Gλ,θ 2 + Gλ,x,i 2.
n h n n i N h n n i | n | N | n |
i=1 i=1
X X
Regarding the 2q-th moment one writes
Zλ,u 2q = Aλ +Bλ q
| n+1| n n
q
(cid:0) (cid:1) q
(Aλ)q +2q(Aλ)q−1Bλ + Aλ q−k Bλ k. (25)
≤ n n n k | n| | n|
k=2(cid:18) (cid:19)
X
We shall deal with each term separately
q
λµ
E (Aλ)q Zλ,u = (Aλ)q 1 Zλ,u 2 +2λC
n | n n ≤ − 2 | n |
(cid:18)(cid:18) (cid:19) (cid:19)
(cid:2) (cid:3) q−1 q q−1
λµ λµ 4
1+ 1 Zλ,u 2q + 1+ 2qλqCq
≤ 4 − 2 | n | λµ
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
q−1 q−1
λµ λµ 4
1 1 Zλ,u 2q + λ+ λ(2C)q
≤ − 4 − 2 | n | µ
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
rλ Zλ,u 2q +wλ. (26)
≤ q| n | q
where rλ = (1 λµ/4)q−1(1 λµ/2) and wλ = (λ+4/µ)q−1λ(2C)q. Notice that we made
q − − q
use of the elementary equation (r + s)q (1 + ǫ)q−1rq + (1 + 1/ǫ)q−1sq for the choice
≤
ǫ = λµ/4. Additionally for the shake of simplicity, let us denote d = dθ/N +dx.
21Now on a similar note with the previous term
E 2q(Aλ)q−1Bλ Zλ,u = 2q(Aλ)q−1E Bλ Zλ,u = 4qλ dθ/N +dx (Aλ)q−1
n n| n n n| n n
4qλ dθ/N +dx rλ Zλ,u 2(q−1) +wλ
(cid:2) (cid:3) ≤ (cid:2) q−(cid:3)1| n | (cid:0) q−1(cid:1)
4qλd rλ Zλ,u 2(q−1) +wλ . (27)
≤ (cid:0) q−1| n |(cid:1)(cid:0) q−1 (cid:1)
(cid:0) (cid:1)
The 3rd term on (25) can be further expanded to
q q−2
q q
Aλ q−k Bλ k = Aλ q−2−m Bλ m+2
k | n| | n| m+2 | n| | n|
k=2(cid:18) (cid:19) m=0(cid:18) (cid:19)
X X
q−2
q q 1 q 2
= − − Aλ q−2−m Bλ m Bλ 2
m+2m+1 m | n| | n| | n|
m=0(cid:18) (cid:19)
X
q(q 1) Aλ + Bλ q−2 Bλ 2
≤ − | n| | n| | n|
q(q 1)2q−3 Aλ q−2 Bλ 2 +q(q 1)2q−3 Bλ q
≤ − (cid:0) | n| | (cid:1)n| − | n|
:= D+F. (28)
Taking the expectation of the above terms yields
E[D Zλ,u] = q(q 1)2q−3 Aλ q−2E Bλ 2 Zλ,u ,
| n − | n| | n| | n
(cid:2) (cid:3)
where
2
N N
2 1
E Bλ 2 Zλ,u = E 2 ∆θ,Gλ,θ + ∆λ,x,i,Gλ,x,i Gλ,θ 2 + Gλ,x,i 2 Zλ,u
| n| | n (cid:12) h n n i N h n n i| n | N | n | (cid:12) | n 
(cid:2) (cid:3)
(cid:12) Xi=1 Xi=1 (cid:12)
(cid:12) (cid:12)
 (cid:12) N (cid:12) 
4
4 E(cid:12) 4 ∆λ,θ 2 Gλ,θ 2 Zλ,u +E ∆λ,x,i 2 Gλ,x,i 2 Zλ,u (cid:12)
≤ | n | | n | | n N | n | | n | | n
" #
i=1
(cid:2) (cid:3) X
N
1
+ E Gλ,x,i 4 Zλ,u +E Gλ,x,i 4 Zλ,u
| n | | n N | n | | n
" #!
i=1
(cid:2) (cid:3) X
N
4
4 4 ∆λ,θ 2E Gλ,θ 2 Zλ,u + ∆λ,x,i 2E Gλ,x,i 2 Zλ,u
≤ | n | | n | | n N | n | | n | | n
i=1
(cid:2) (cid:3) X (cid:2) (cid:3)
N
1
+ E Gλ,x,i 4 Zλ,u +E Gλ,x,i 4 Zλ,u .
| n | | n N | n | | n
" #!
i=1
(cid:2) (cid:3) X
22Recalling that each Gλ,·,i is distributed according to a Gaussian distribution, we subse-
n
quently derive
N
4
E Bλ 2 Zλ,u 4 4(2λdθ/N) ∆λ,θ 2 + (2λdx) ∆λ,x,i 2 +3(2λdθ/N)2 +3(2λdx)2
| n| | n ≤ | n | N | n |
!
i=1
(cid:2) (cid:3) X
N
1
16(2λd) ∆λ,θ 2 + ∆λ,x,i 2 +24(2λd)2
≤ | n | N | n |
!
i=1
X
16(2λd)Aλ +24(2λd)2.
≤ n
By substituting this result, the term D can be expressed as follows
E[D Zλ,u] = q(q 1)2q−3 Aλ q−2 16(2λd)Aλ +24(2λd)2
| n − | n| n
= 16(2λd)q(q 1)2q−3 rλ Zλ,u 2(q−1) +wλ
− (cid:0) q−1| n | q−1(cid:1)
+24(2λd)2q(q 1)2q−3 rλ Zλ,u 2(q−2) +wλ . (29)
− (cid:0) q−2| n | q−2(cid:1)
(cid:0) (cid:1)
Additionally,
E[F Zλ,u] = q(q 1)2q−3 Bλ q
| n − | n|
N
2
= q(q 1)2q−3E 2 ∆θ,Gλ,θ + ∆λ,x,i,Gλ,x,i
− h n n i N h n n i
"(cid:12)
(cid:12) Xi=1
(cid:12) q
N
1 (cid:12)
+ Gλ,θ 2 + (cid:12)Gλ,x,i 2 Zλ,u
| n | N | n | | n
(cid:12) #
Xi=1 (cid:12)
q(q 1)2q−34q−1 E 2q(cid:12) ∆λ,θ q Gλ,θ q Zλ,u
≤ − |(cid:12) (cid:12) n | | n | | n
q
2q N (cid:0) (cid:2) (cid:3)
+E ∆λ,x,i Gλ,x,i Zλ,u +E Gλ,θ 2q Zλ,u (30)
Nq | n || n | | n | n | | n
" (cid:12) (cid:12) #
(cid:12)Xi=1 (cid:12)
(cid:2) (cid:3)
(cid:12) N (cid:12)
1 (cid:12) (cid:12)
+E N(cid:12)q−1 Gλ,x,i 2q Z(cid:12)λ,u (31)
Nq | n | | n
" #!
i=1
X
. We handle the second term in (31) by applying the multinomial expansion:
q
N N
q
∆λ,x,i Gλ,x,i = ∆λ,x,i Gλ,x,i ki,
(cid:12)
(cid:12)Xi=1
| n || n |(cid:12)
(cid:12) k1+.
X..+kN=q(cid:18)k 1,...,k N(cid:19)
Yi=1
(cid:0)| n || n |
(cid:1)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
23and using the fact that ∆λ,x,i are Zλ,u measurable, hence
n n −
q
2q N
E ∆λ,x,i Gλ,x,i Zλ,u
Nq "(cid:12) | n || n |(cid:12) | n #
(cid:12)Xi=1 (cid:12)
2q (cid:12) (cid:12) q N (cid:12) (cid:12)
= E (cid:12) (cid:12)∆λ,x,i Gλ,x,i ki Zλ,u
Nq k ,...,k | n || n | | n
" k1+. X..+kN=q(cid:18) 1 N(cid:19) Yi=1
(cid:0) (cid:1)
#
2q q N N
= ∆λ,x,i ki E Gλ,x,i ki Zλ,u
Nq k ,...,k | n | | n | | n
k1+. X..+kN=q(cid:18) 1 N(cid:19) Yi=1 Yi=1
(cid:2) (cid:3)
2q q N N
= ∆λ,x,i ki (2λdx)ki/2k !!
Nq k ,...,k | n | i
k1+. X..+kN=q(cid:18) 1 N(cid:19) Yi=1 Yi=1
(cid:0) (cid:1)
2q q N
= ∆λ,x,i ki(2λdx)PN i=1ki/2k !!
Nq k ,...,k | n | i
k1+. X..+kN=q(cid:18) 1 N(cid:19) Yi=1
2qq!!(2λdx)q/2 q N
∆λ,x,i ki
≤ Nq k ,...,k | n |
k1+. X..+kN=q(cid:18) 1 N(cid:19) Yi=1
q q/2
2qq!!(2λdx)q/2 N 2qq!!(2λdx)q/2 N
= ∆λ,x,i ∆λ,x,i 2
Nq | n | ≤ Nq−1 | n |
! !
i=1 i=1
X X
q/2
N
1
2qq!!(2λdx)q/2 ∆λ,x,i 2 .
≤ N | n |
!
i=1
X
Plugging these results into (31) we get
E[F Zλ,u] q(q 1)23q−5 2qq!!(2λdθ/N)q/2 ∆λ,θ 2 q/2
| n ≤ − | n |
(cid:16) N q(cid:0)/2 (cid:1)
1
+2qq!!(2λdx)q/2 ∆λ,x,i 2 +(2q)!!(2λdθ/N)q +(2q)!!(2λdx)q
N | n | 
!
i=1
X
q/2 
N
1
q(q 1)23q−5 2qq!!(2λd)q/2 ∆λ,θ 2 + ∆λ,x,i 2 +2(2q)!!(2λd)q
≤ −  | n | N | n | 
!
i=1
X
q(q 1)24q−5( 2q)!! (2λd)q/2 Aλ q/2 +(2λd)q 
≤ − n
q(q 1)24q−5(2q)!!((cid:16) 2λd)q/2 r(cid:12)λ Z(cid:12) λ,u q +wλ (cid:17)
≤ − (cid:12)q/2|(cid:12) n | q/2
+q(q 1)24q−5(2q)!!(2λd)q. (32)
(cid:0) (cid:1)
−
24Combining (29),(32) yields the following bound for (28),
q
q
E Aλ q−k Bλ k
k | n| | n|
" #
k=2(cid:18) (cid:19)
X
16(2λd)q(q 1)2q−3 rλ Zλ,u 2(q−1) +wλ
≤ − q−1| n | q−1
+24(2λd)2q(q 1)2q−3 rλ Zλ,u 2(q−2) +wλ
− (cid:0) q−2| n | q−2(cid:1)
+(2q)!!(2λd)q/2q(q 1)24q−5 rλ Zλ,u q +wλ
− (cid:0) q/2| n | q/2(cid:1)
+(2q)!!(2λd)qq(q 1)24q−5. (33)
(cid:0) (cid:1)
−
Hence one obtains for (25) via (26),(27) and (33),
E Z 2q Z rλ Zλ,u 2q +wλ
| n+1 | | n ≤ q| n | q
+4q(2λd)(1+4(q 1)2q−3) rλ Zλ,u 2(q−1) +wλ
(cid:2) (cid:3) − q−1| n | q−1
+24(2λd)2q(q 1)2q−3 rλ Zλ,u 2(q−2) +wλ
− q−2|(cid:0) n | q−2 (cid:1)
+(2q)!!(2λd)q/2q(q 1)24q−5 rλ Zλ,u q +wλ
− (cid:0) q/2| n | q/2(cid:1)
+(2q)!!(2λd)qq(q 1)24q−5. (34)
(cid:0) (cid:1)
−
2d 2d
Consider Zλ,u (2q)!!q(q 1)24q−5 1/2 (2q)!!q(q 1)24q−5 1/q . Then
| n | ≥ µ/4 { − } ≥ µ/4 { − }
r r
one observes
E Zλ,u 2q Z rλ Zλ,u 2q +wλ
| n+1| | n ≤ q| n | q
h i λµ (λµ)2
+ Zλ,u 2 rλ Zλ,u 2(q−1) + Zλ,u 4 rλ Zλ,u 2(q−2)
2 4| n | q−1| n | 2 42 | n | q−2| n |
· ·
(λµ)q/2 (cid:0) (cid:1) (cid:0) (cid:1)
+ Zλ,u q rλ Zλ,u q +(2q)!!q(q 1)24q−5(2λd)q
2 4q/2| n | q/2| n | −
·
+(2q)!!q(q 1)2(cid:0)4q−5 (2λd)w(cid:1)λ +(2λd)2wλ +(2λd)q/2wλ
− q−1 q−2 q/2
q/2 q/2−1
λµ (cid:0) 1 λµ λµ (cid:1)
rλ 1 + 1
≤ q/2 − 4 2 4 − 4
(cid:18) (cid:19) (cid:18) (cid:19)(cid:18) (cid:19)
2 q/2−2 q/2
1 λµ λµ 1 λµ
+ 1 + Zλ,u 2q
2 4 − 4 2 4 | n |
!
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
+wλ +(2q)!!q(q 1)24q−5(2λd)q
q −
+(2q)!!q(q 1)24q−5 (2λd)wλ +(2λd)2wλ +(2λd)q/2wλ .
− q−1 q−2 q/2
(cid:0) (cid:1)
25Using the fact that λµ 1 we get
≤
λµ
E Zλ,u 2q Z 1 Zλ,u 2q
| n+1| | n ≤ − 2 | n |
(cid:18) (cid:19)
h i
+wλ +(2q)!!q(q 1)24q−5(2λd)q
q −
+(2q)!!q(q 1)24q−5 (2λd)wλ +(2λd)2wλ +(2λd)q/2wλ
− q−1 q−2 q/2
λµ
1 Zλ,u 2q(cid:0) +(2q)!!q(q 1)24q−2(8/µ)q−1λ(2C)q. (cid:1)
≤ − 2 | n | −
(cid:18) (cid:19)
2d
Consequently, on Zλ,u (2q)!!q(q 1)24q−5 1/2 , we have
{| n | ≤ µ/4 { − } }
r
λµ
E Zλ,u 2q Z 1 Zλ,u 2q
| n+1| | n ≤ − 2 | n |
(cid:18) (cid:19)
h i
+(2q)!!q(q 1)24q−2(8/µ)q−1λ(2C)q
−
+ (2q)!!q(q 1)24q−2 q/2 (8/µ)q−1λ(2C)q.
−
So all in all (34) results to (cid:0) (cid:1)
λµ
E Zλ,u 2q 1 E Zλ,u 2q +M (8/µ)qCq
| n+1| ≤ − 2 | n | q
(cid:18) (cid:19)
h i
C (1+(cid:2) dθ/N +d(cid:3)x)q,
≤
|z0|,b,q,µ
where C = E Zλ 2q M (8/µ)q(b+1)q and M = (2q)!!q(q 1)26q−5.
|z0|,b,q,µ | 0| q q −
Lemma 4. Let A1(cid:2)and A(cid:3)2 hold. Then, for every λ < Np/4µ, there exists a constant
0
C > 0 independent of N,n,λ such that for any λ (0,λ ) one has
0
∈
E Zλ,u Zλ,u 4 λ2N−2pC (1+dθ/N +dx)2.
| n+1− n | ≤ |z0|,µ,b
h i
Proof.
2
N
1
Zλ,u Zλ,u 4 = θλ,u θλ,u 2 + Xi,λ,u Xi,λ,u 2
| n+1 − n | | n+1 − n | N | n+1 − n |
!
i=1
X
2
N
λ 2λ
= − hθ (θλ,u,Xi,λ,u)+ ξ(0)
 Np+1 λ,u n n Np+1 n+1
(cid:12) r (cid:12)
(cid:12) Xi=1 (cid:12)
(cid:12) (cid:12)
 (cid:12) N 2 (cid:12)2
1(cid:12) λ 2λ (cid:12)
+ hx(θλ,u,Xi,λ,u)+ ξ(i)
N (cid:12)−Np λ n n rNp n+1
(cid:12)

Xi=1 (cid:12) (cid:12)
(cid:12) (cid:12)

(cid:12) (cid:12)
(cid:12) (cid:12)
26λ2 N λ2 N
hθ (θλ,u,Xi,λ,u) 2 + hx(θλ,u,Xi,λ,u) 2
≤ N2(p+1)| λ,u n n | N2p+1 | λ n n |
i=1 i=1
X X
N
λ 2λ
2 hθ (θλ,u,Xi,λ,u), ξ(0)
− hNp+1 λ,u n n Np+1 n+1i
r
i=1
X
2
N N
λ 2λ 2λ 2λ
2 hx (θλ,u,Xi,λ,u), ξ(i) + ξ(0) 2 + ξ(i) 2 .
− Np+1 h λ,u n n Np n+1i Np+1| n+1| Np+1 | n+1|
r !
i=1 i=1
X X
Expanding the square yields
2
λ4 N λ2 N
Zλ,u Zλ,u 4 h (θλ,u,Xi,λ,u) 2 +2 h (θλ,u,Xi,λ,u) 2
| n+1 − n | ≤ N4p+2 | λ,u n n | N2p+1 | λ,u n n |
! !
i=1 i=1
X X
N
λ 2λ
2 hθ (θλ,u,Xi,λ,u), ξ(0)
× − hNp+1 λ,u n n Np+1 n+1i
r
i=1
X
N N
λ 2λ 2λ 2λ
2 hx (θλ,u,Xi,λ,u), ξ(i) + ξ(0) 2 + ξ(i) 2
− Np+1 h λ,u n n Np n+1i Np+1| n+1| Np+1 | n+1|
r !
i=1 i=1
X X
N
λ 2λ
+ 2 hθ (θλ,u,Xi,λ,u), ξ(0)
− hNp+1 λ,u n n Np+1 n+1i
(cid:12) r
(cid:12) Xi=1
(cid:12) 2
λ N (cid:12) 2λ 2λ 2λ N
2 hx (θλ(cid:12),u,Xi,λ,u), ξ(i) + ξ(0) 2 + ξ(i) 2 . (35)
− Np+1 h λ,u n n Np n+1i Np+1| n+1| Np+1 | n+1|
r (cid:12)
Xi=1 Xi=1 (cid:12)
(cid:12)
(cid:12)
The first term in (35) is immediately bounded by the moments of Zλ,u, (cid:12)
n
2 2
λ4 N λ4 N
E h (θλ,u,Xi,λ,u) 2 Zλ,u 2µ2 Vi,λ,u 2 +2λ−1Np
N4p+2 | λ,u n n | | n  ≤ N4p+2 | n |
! !
i=1 i=1
X X
  2
λ4 1 N λ2
4µ4 Vi,λ,u 2 +4
≤ N4p N2 | n | N2p
!
i=1
X
λ2 λ2
µ2 Zλ,u 4 +4 . (36)
≤ N2p| n | N2p
where the last inequality follows due to the restriction λ < Np/4µ.
27Notice that in the second term the inner products , vanish under expectation
h· ·i
λ2 N 2λ 2λ N
2 h (θλ,u,Xi,λ,u) 2 E ξ(0) 2 Zλ,u + E ξ(i) 2 Zλ,u
N2p+1 | λ,u n n | Np+1 | n+1| | n Np+1 | n+1| | n
! !
Xi=1 h i Xi=1 h i
λ2 λ λ λ3
2 2µ2 Zλ,u 2 +2λ−1Np 2 dθ/N +2 dx 8 d(µ2 Zλ,u 2 +λ−1Np)
≤ N2p | n | Np Np ≤ N3p | n |
(cid:18) (cid:19)
(cid:0) (cid:1) λ2 λ2
2dµ Zλ,u 2 +8d . (37)
≤ N2p| n | N2p
Using the usual elementary inequality and the Cauchy-Schwartz on the last term of (35)
we get
2
16λ2 N 2λ
hθ (θλ,u,Xi,λ,u) E ξ(0) 2
N2p+2 λ,u n n | Np+1 n+1|
(cid:12) (cid:12) " r #
(cid:12)Xi=1 (cid:12)
16λ(cid:12) (cid:12)2 N (cid:12) (cid:12) 2λ
+ (cid:12) hx(θλ,u,Xi,λ,u(cid:12)) 2E ξ(i) 2
N2p+1 | λ n n | | Np n+1|
" r #
i=1
X
16λ2 16λ2 N
+ E ξ(0) 4 + E ξ(i) 4
N2p+2 | n+1| N2p+1 | n+1|
h i Xi=1 h i
λ2 λ 1 N
16 2 dθ/N hθ (θλ,u,Xi,λ,u) 2
≤ N2p Np N | λ,u n n |
(cid:18) (cid:19) i=1
X
λ2 λ 1 N
+16 2 dx hx(θλ,u,Xi,λ,u) 2
N2p Np N | λ n n |
(cid:18) (cid:19) i=1
X
λ2 λ2
+16 (dθ/N)2 +16 (dx)2
N2p N2p
λ3 1 N λ2
16 (2d) 2µ2 Vi,λ,u 2 +2λ−1Np +16 d2
≤ N3p N | n | N2p
i=1
X(cid:0) (cid:1)
λ3 λ2 λ2
16 4 µ2d Zλ,u 2 +16 4 d+16 d2
≤ · N3p | n | · N2p N2p
λ2 λ2 λ2
16dµ Zλ,u 2 +64 d+16 d2. (38)
≤ N2p| n | N2p N2p
Hence (35) in view of (36),(37) and (38) can be further bounded as
λ2 λ2 λ2
E Zλ,u Zλ,u 4 Zλ,u µ2 Zλ,u 4 +18dµ Zλ,u 2 +4 (4+18d+d2)
| n+1 − n | | n ≤ N2p| n | N2p| n | N2p
h i λ2
µ2 Zλ,u 4 +18dµ Zλ,u 2 +4(4+18d+d2) .
≤ N2p | n | | n |
(cid:0) (cid:1)
28The final result follows by taking the expectation and using the Lemmas 2-3
λ2
E Zλ,u Zλ,u 4 Zλ,u 81 µ2C2 +18µC +4 (1+dθ/N +dx)2.
| n+1 − n | | n ≤ N2p |z0|,µ,b |z0|,b,2,µ
h i
(cid:0) (cid:1)
A.2.3 Proof of Proposition 4
Proof. Consider theusualsplitonthedifferencebetween theinterpolationandthescaled
dynamics and apply the Itˆo’s formula for x x 2
→ | |
N
1
Zλ,u 2 = θλ,u ϑ 2 + Xi,λ,u i 2
| t −Zλt | | t − λt | N | t −Xλt|
i=1
X
t 1 N 1 N
= 2λ hθ (θλ,u,Xi,λ,u) hθ(ϑλ , i ),θλ,u ϑ ds
− Np+1 λ,u ⌊t⌋ ⌊t⌋ − Np+1 λt Xλt t − λt
Z0 *
i=1 i=1
+
X X
2λ N t
hx (θλ,u,Xi,λ,u) hx(ϑλ , i ),Xi,λ,u i ds
−Np+1 λ,u ⌊t⌋ ⌊t⌋ − λt Xλt t −Xλt
Xi=1
Z0
D E
2λ N t
= h (Vi,λ,u) h( i ),Vi,λ,u i ds.
−Np+1 h λ,u ⌊t⌋ − Vλt t −Vλti
i=1 Z0
X
In order to make the forthcoming calculations more readable we define the following
quantities:
ei = Vi,λ,u i and e = Zλ,u ,
t t −Vλt t t −Zλt
so that it follows: 1/N N ei 2 = e 2. Then by taking the derivative of the above
i=1| t| | t |
expansion we obtain
P
N
d 2λ
e 2 = h (Vi,λ,u) h( i ),ei
dt| t | −Np+1 h λ,u ⌊t⌋ − Vλt ti
i=1
X
N N
2λ 2λ
= h(Vi,λ,u ) h( i ),ei h (Vi,λ,u) h(Vi,λ,u),ei
−Np+1 h t − Vλt ti− Np+1 h λ,u ⌊t⌋ − ⌊t⌋ ti
i=1 i=1
X X
N
2λ
h(Vi,λ,u) h(Vi,λ,u ),ei
− Np+1 h ⌊t⌋ − t ti
i=1
X
:= k (t)+k (t)+k (t). (39)
1 2 3
The first term k (t) is controlled through the convexity of the initial potential, the
1
taming error k (t) can be controlled via the properties of the taming function and the
2
29additional moment bounds that we have established, which are also used to control the
discretisation error k (t). In particular ,
3
N
2λ
k (t) = h(Vi,λ,u ) h( i ),ei
1 −Np+1 h t − Vλt ti
i=1
X
N
2λµ 2λµ
ei 2 = e 2. (40)
≤ −Np+1 | t| − Np | t |
i=1
X
Using the Cauchy-Schwarz inequality and the ǫ-Young inequality for ǫ = µ/2 yields
N
2λ
k (t) = h (Vi,λ,u) h(Vi,λ,u),ei
2 −Np+1 h λ,u ⌊t⌋ − ⌊t⌋ ti
i=1
X
N
2λ µ 1
ei 2 + h (Vi,λ,u) h(Vi,λ,u) 2
≤ Np+1 4| t| µ| λ,u ⌊t⌋ − ⌊t⌋ |
i=1 (cid:18) (cid:19)
X
N
2λµ 2λ λC
e 2 + 1 (1+ Vi,λ,u 4(ℓ+1))
≤ 4Np| t | µNp+1 Np | ⌊t⌋ |
i=1
X
2(ℓ+1)
λµ 2λ2C 2λ2C 1 N
e 2 + 1 + 1 Vi,λ,u 2
≤ 2Np| t | µN2p µNp N | ⌊t⌋ |
!
i=1
X
λµ 2λ2C 2λ2C
e 2 + 1 + 1 Zλ,u 4(ℓ+1). (41)
≤ 2Np| t | µNp µNp | ⌊t⌋|
Note that the constant C was introduced in Property 2 and that we choosed p = 2ℓ+1
1
in order to get an expression with a power of Zλ,u . Similarly for the last term we get
| ⌊t⌋|
N
2λ
k (t) = h(Vi,λ,u) h(Vi,λ,u ),ei
3 −Np+1 h ⌊t⌋ − t ti
i=1
X
N
2λ µ 1
ei 2 + h(Vi,λ,u) h(Vi,λ,u ) 2
≤ Np+1 4| t| µ| ⌊t⌋ − t |
i=1 (cid:18) (cid:19)
X
N
λµ 2λC
e 2 + 2 (1+ Vi,λ,u 2ℓ + Vi,λ,u 2ℓ) Vi,λ,u Vi,λ,u 2
≤ 2Np| t | µNp+1 | ⌊t⌋ | | t | | ⌊t⌋ − t |
Xi=1 (cid:16) (cid:17)
N N
λµ 2λC
e 2 + 2 1+ Vi,λ,u 2ℓ + Vi,λ,u 2ℓ Vi,λ,u Vi,λ,u 2.
≤ 2Np| t | µNp+1 | ⌊t⌋ | | t | | ⌊t⌋ − t |
Xi=1 (cid:16) (cid:17)Xi=1
30By reordering, taking the expectation and using the Holder’s inequality we further get:
λµ
E[k (t)] E e 2
3 ≤ 2Np | t |
(cid:2) (cid:3) N N
2λC 1 1
+ 2 E 1+ Vi,λ,u 2ℓ + Vi,λ,u 2ℓ Vi,λ,u Vi,λ,u 2
µN(p+1)/2 Nℓ | ⌊t⌋ | | t | N | ⌊t⌋ − t |
" #
Xi=1 (cid:16) (cid:17) Xi=1
λµ 2λC
E e 2 + 2E 1+ Zλ,u 2ℓ + Zλ,u 2ℓ Zλ,u Zλ,u 2
≤ 2Np | t | µ | ⌊t⌋ | | t | | ⌊t⌋ − t |
h(cid:16) (cid:17) i
λµ (cid:2) (cid:3) 6λC 1/2 1/2
E e 2 + 2E 1+ Zλ,u 4ℓ + Zλ,u 4ℓ E Zλ,u Zλ,u 4
≤ 2Np | t | µ | ⌊t⌋ | | t | | ⌊t⌋ − t |
h(cid:16) (cid:17)i h i
λµ (cid:2) (cid:3) 6λ2C C 1/2
E e 2 + 2 3E 1+ Zλ,u 4ℓ + Zλ,u 4ℓ (1+dθ/N +dx)
≤ 2Np | t | µNp | ⌊t⌋| | t |
h(cid:16) (cid:17)i
λµ (cid:2) (cid:3) 6λ2C C C
E e 2 + 2 3 4 (1+dθ/N +dx)ℓ+1. (42)
≤ 2Np | t | µNp
(cid:2) (cid:3)
Taking the expectation for the rest of the terms in (40) and (41) respectively and com-
bining them with (42), one obtains in view of (39),
d µλ 2λ2C 2λ2C C
E e 2 E e 2 + 1 + 1 5 (1+dθ/N +dx)2(ℓ+1)
dt | t | ≤ −Np | t | µNp µNp
(cid:2) (cid:3) 6λ2C C(cid:2)C (cid:3)
+ 2 3 4 (1+dθ/N +dx)ℓ+1
µNp
µλ λ2C
E e 2 + 6 (1+dθ/N +dx)2(ℓ+1),
≤ −Np | t | µNp
by multiplying both sides by
th(cid:2)
e
int(cid:3)
egrating factor
eλµt/Np
and rearranging the term one
gets
d λ2C
eλµt/NpE e 2 6 (1+dθ/N +dx)2(ℓ+1)eλµt/Np
dt | t | ≤ µNp
(cid:0) (cid:2) (cid:3)(cid:1) C
E e 2 λ 6 (1+dθ/N +dx)2(ℓ+1).
| t | ≤ µ2
(cid:2) (cid:3)
A.3 Coordinate-wise tamed scheme - tIPLAc
A.3.1 Key quantities for the proof of the main Lemmas.
The following definitions that refer to the rescaled dynamics (17)-(18) of the algorithm
tIPLAc and its continuous time interpolations (19)-(20) are given as:
Zλ,c = θλ,c ,N−1/2X1,λ,c,...,N−1/2XN,λ,c , (43)
n n+1 n+1 n+1
(cid:16) (cid:17)
31Zλ,c
=
θλ,c ,N−1/2X1,λ,c ,...,N−1/2XN,λ,c
, (44)
t t t t
(cid:16) (cid:17)
N = ϑN,N−1/2 1,N,...,N−1/2 N,N . (45)
Zλt λt Xλt Xλt
(cid:16) (cid:17)
A.3.2 Moment and increment bounds
Lemma 5. Let A1 and A3 hold. Then, for any 0 λ < 1/4µ, it holds that,
≤
E Vi,λ,c 2 C (1+dθ +dx),
| n | ≤ |z0|,µ,b
for a constant C > 0 indepen(cid:2) dent of(cid:3) N,n,λ,dx and dθ, i 1,...,N .
∀ ∈ { }
Proof.
2 2 2
Vi,λ,c = θλ,c + Xi,λ,c
n+1 n+1 n+1
(cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 2
(cid:12) (cid:12) (cid:12) (cid:12) λ (cid:12)N (cid:12) 2λ
(cid:12) (cid:12) = (cid:12)θλ,c (cid:12) (cid:12) hθ(cid:12) (θλ,c,Xi,λ,c) + ξ(0) 2
(cid:12) n − N λ,c n n (cid:12) N | n+1|
(cid:12) Xi=1 (cid:12)
(cid:12) N (cid:12)
(cid:12) 2λ λ (cid:12)
+2(cid:12) θλ,c hθ (θλ,c(cid:12),Xi,λ,c),ξ(0)
N n − N λ,c n n n+1
r * +
i=1
X
+ Xi,λ,c λhx (θλ,c,Xi,λ,c) 2 +2λ ξ(i) 2
n − λ,c n n | n+1|
+(cid:12)2√2λ Xi,λ,c λhx (θλ,c,(cid:12)Xi,λ,c),ξ(i) .
(cid:12) n − λ,c n (cid:12) n n+1
D E
Taking the conditional expectation on both sides with respect to the filtration generated
by Vi,λ,c the cross terms are vanishing to 0, yielding
n
N
2 2λ
E Vi,λ,c Vi,λ,c = E θλ,c 2 Vi,λ,c E θλ,c,hθ (θλ,c,Xi,λ,c) Vi,λ,c
n+1 | n n | n − N h n λ,c n n i| n
(cid:20) (cid:12) (cid:12) (cid:21) h
(cid:12) (cid:12)
i Xi=1
(cid:2) (cid:3)
(cid:12)
(cid:12)
(cid:12)
(cid:12)
λ2(cid:12) (cid:12)N 2
2λdθ
+ E hθ (θλ,c,Xi,λ,c) Vi,λ,c + +E Xi,λ,c 2 Vi,λ,c
N2  λ,c n n | n  N n | n
(cid:12) (cid:12)
(cid:12)Xi=1 (cid:12) h(cid:12) (cid:12) i
(cid:12) (cid:12)
2λE  X(cid:12) i,λ,c,hx (θλ,c,Xi,λ,c(cid:12)) Vi,λ,c (cid:12) (cid:12)
− h (cid:12)n λ,c n n (cid:12)i| n
+λ2E hx (θλ,c,Xi,λ,c) 2 Vi,λ,c +2λdx.
(cid:2) | λ,c n n | | n (cid:3)
Furthermore by using the (cid:2) elementary inequality (t(cid:3) +...+t )p mp−1(tp+...,tp ) and
1 m ≤ 1 m
the fact that all of the expressions within the conditional expectations are measurable
32we obtain
2 2λ N λ2 N
E Vi,λ,c Vi,λ,c θλ,c θλ,c,hθ (θλ,c,Xi,λ,c) + hθ(θλ,c,Xi,λ,c) 2
n+1 | n ≤ | n |− N h n λ,c n n i N | λ n n |
(cid:20) (cid:12) (cid:12) (cid:21) Xi=1 Xi=1
(cid:12) (cid:12) 2λdθ
(cid:12) (cid:12) + + Xi,λ,c 2 2λ Xi,λ,c,hx (θλ,c,Xi,λ,c)
N | n | − h n λ,c n n i
+λ2 hx (θλ,c,Xi,λ,c) 2 +2λdx
| λ,c n n |
N dθ
2λ
Vi,λ,c 2 θλ hθj (θλ,c,Xi,λ,c)
≤ n − N n,j · λ,c n n
i=1 j=1
(cid:12) (cid:12) XX
λ(cid:12) 2 N (cid:12) dθ dx
+ hθj (θλ,c,Xi,λ,c) 2 2λ Xi,λ hxj (θλ,c,Xi,λ,c)
N | λ,c n n | − n,j · λ,c n n
i=1 j=1 j=1
XX X
dx
+λ2 hxj (θλ,c,Xi,λ,c) 2 +2λ(dθ/N +dx)
| λ,c n n |
j=1
X
N dθ
2λ µ
Vi,λ,c 2 θλ,c 2 b
≤ n − N 2| n,j| −
(cid:12) (cid:12)
Xi=1 Xj=1 (cid:16) (cid:17)
λ(cid:12) 2 N (cid:12) dθ dx µ
+ 2µ2 θλ,c 2 +2λ−1 2λ Xi,λ,c 2 b
N | n,j| − 2| n,j | −
Xi=1 Xj=1 (cid:16) (cid:17) Xj=1 (cid:16) (cid:17)
dx
+λ2 2µ2 Xi,λ,c 2 +2λ−1 +2λ(dθ/N +dx).
| n,j |
Xj=1 (cid:16) (cid:17)
Executing the summations yields
2
E Vi,λ,c Vi,λ,c Vi,λ,c 2 λµ θλ,c 2 λµ Xi,λ,c 2 +2λ2µ2 θλ,c 2 +2λ2µ2 Xi,λ,c 2
n+1 | n ≤ n − | n | − | n | | n | | n |
(cid:20) (cid:21)
(cid:12) (cid:12)
(cid:12) (cid:12) +2(cid:12) λb(dθ(cid:12) +dx)+2λ(dθ +dx)+2λ(dθ/N +dx)
(cid:12) (cid:12)
(cid:12) (cid:12)
(1 λµ+2λ2µ2) Vi,λ,c 2 +2λ(b+1)(dθ +dx)+2λ(dθ/N +dx).
≤ − n
(cid:12) (cid:12)
Consider the restriction λµ < 1/4, then(cid:12)this im(cid:12) plies that
2
E Vi,λ,c (1 λµ/2)E Vi,λ,c 2 +2λ(b+1)(dθ +dx)+2λ(dθ/N +dx).
n+1 ≤ − n
(cid:20)(cid:12) (cid:12) (cid:21) h i
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
(cid:12) (cid:12)
33Iterating the above bound finally yields
2 2 1 (1 λµ/2)n
E Vi,λ,c (1 λµ/2)n Vi,λ,c + − − 4λ(b+1)(dθ +dx)
n+1 ≤ − 0 λµ/2
(cid:20) (cid:21)
(cid:12) (cid:12) (cid:12) (cid:12)
(cid:12) (cid:12) V 2 +(8/µ(cid:12))(b+(cid:12)1) (1+dθ +dx).
(cid:12) (cid:12) ≤ | 0 | (cid:12) (cid:12)
(cid:0) (cid:1)
Lemma 6. Let A1 and A3 hold so that Lemma 5 holds true as well. Then, for any
0 λ < 1/4µ and p [2, ) N, it holds that,
≤ ∈ ∞ ∩
E Vi,λ,c 2p C (1+dθ +dx)p,
| n | ≤ |z0|,µ,b,p
for a constant C > 0 indep(cid:2)endent of(cid:3)N,n,λ,dx and dθ, i 1,...,dθ +Ndx .
∀ ∈ { }
Proof. Similarly as in the proof of Lemma 3 we define the following auxiliary quantities:
Aλ := ∆λ,θ 2 + ∆λ,i,x 2,
n | n | | n |
Bλ = 2 ∆θ,Gλ,θ +2 ∆λ,x,i,Gλ,x,i + Gλ,θ 2 + Gλ,x,i 2.
n h n n i h n n i | n | | n |
Now for the 2p-th Moment one writes:
Vi,λ,c 2p = Aλ +Bλ p
| n+1 | n n
p
(cid:0) (cid:1) p
(Aλ)p +2p(Aλ)p−1Bλ + Aλ p−k Bλ k (46)
≤ n n n k | n| | n|
k=2(cid:18) (cid:19)
X
We shall deal with each term separately,
p
E Aλ p Zλ = (Aλ)p (1 λµ/2) Vi,λ,c 2 +2λ(b+1)(dθ +dx)
| n| | n n ≤ − n
(cid:2) (cid:3) λµ(cid:16) p−1 λ(cid:12)µ p (cid:12) 4 p−1(cid:17)
1+ 1 (cid:12) V(cid:12) i,λ,c 2p + 1+ 2pλpCp
≤ 4 − 2 | n | λµ
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
p−1 p−1
λµ λµ 4
1 1 Vi,λ,c 2p + λ+ λ(2C)p
≤ − 4 − 2 | n | µ
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
rλ Vi,λ,c 2p +wλ. (47)
≤ p| n | p
where rλ = (1 λµ/4)p−1(1 λµ/2) and wλ = (λ+4/µ)p−1λ(2C)p.
p − − p
On a similar note,
E 2p(Aλ)p−1Bλ Vi,λ,c = 2p(Aλ)p−1E Bλ Zλ = 4pλ dθ/N +dx (Aλ)p−1
n n| n n n| n n
4pλ dθ/N +dx rλ Vi,λ,c 2(p−1) +wλ
(cid:2) (cid:3) ≤ (cid:2) p(cid:3)−1| n (cid:0) | p(cid:1)−1
4p(2λd) rλ Vi,λ,c 2(p−1) +wλ . (48)
≤ (cid:0) p−1| (cid:1)n(cid:0) | p−1 (cid:1)
(cid:0) (cid:1)
34The 3rd term on (46) can be further expanded to
p p−2
p p
Aλ p−k Bλ k = Aλ p−2−k Bλ k+2
k | n| | n| ℓ+2 | n| | n|
k=2(cid:18) (cid:19) ℓ=0 (cid:18) (cid:19)
X X
p−2
p p 1 p
= − Aλ p−2−k Bλ k Bλ 2
ℓ+2 ℓ+1 ℓ | n| | n| | n|
ℓ=0 (cid:18) (cid:19)
X
p(p 1) Aλ + Bλ p−2 Bλ 2
≤ − | n| | n| | n|
p(p 1)2p−3 Aλ p−2 Bλ 2 +p(p 1)2p−3 Bλ p.
≤ − (cid:0) | n| | (cid:1)n| − | n|
= D +F (49)
Taking the expectation of the above terms yields
E[D Vi,λ,c] = p(p 1)2p−3 Aλ p−2E Bλ 2 Vi,λ,c ,
| n − | n| | n| | n
(cid:2) (cid:3)
where
E Bλ 2 Vi,λ,c = E 2 ∆θ,Gλ,θ +2 ∆λ,x,i,Gλ,x,i + Gλ,θ 2 + Gλ,x,i 2 2 Vi,λ,c
| n| | n h n n i h n n i | n | | n | | n
(cid:2) (cid:3) 4 hE(cid:12) 4 ∆λ,θ 2 Gλ,θ 2 Vi,λ,c +E 4 ∆λ,x,i 2 Gλ,x,i 2 Vi,λ(cid:12),c i
≤ (cid:12) | n | | n | | n | n | | n | | n (cid:12)
+ E Gλ,x,i 4 Vi,λ,c +E Gλ,x,i 4 Vi,λ,c
(cid:0) | (cid:2)n | | n | (cid:3)n | |(cid:2)n (cid:3)
4 4(2λdθ/N) ∆λ,θ 2 +4(2λdx) ∆λ,x,i 2 +12(2λdθ/N)2 +12(2λdx)2
≤ (cid:2) | n(cid:3) | (cid:2) | n (cid:3) |(cid:1)
16(2λd) ∆λ,θ 2 + ∆λ,x,i 2 +96(2λd)2
≤ (cid:0) | n | | n | (cid:1)
16(2λd)Aλ +96(2λd)2.
≤
(cid:0)n (cid:1)
with d = max dθ/N,dx . Plugging in this result for the term D we get
{ }
E[D Vi,λ,c] = p(p 1)2p−3 Aλ p−2 16(2λd)Aλ +96(2λd)2
| n − | n| n
= 16(2λd)p(p 1)2p−3 rλ Vλ 2(p−1) +wλ
− (cid:0) p−1| n| p−1(cid:1)
+96(2λd)2p(p 1)2p−3 rλ Vλ 2(p−2) +wλ .
− (cid:0) p−2| n| p−2(cid:1)
(cid:0) (cid:1)
35Finally,
E[F Vi,λ,c] = p(p 1)2p−3 Bλ p
| n − | n|
= p(p 1)2p−3E 2 ∆θ,Gλ,θ +2 ∆λ,x,i,Gλ,x,i + Gλ,θ 2 + Gλ,x,i 2 p Vi,λ,c
− h n n i h n n i | n | | n | | n
p(p 1)2p−34p−1 E 2p ∆λ,θ p Gλ,θ p Vi,λ,c +E 2p ∆λ,x,i p Gλ,x,i p Vi,λ,c
≤ − (cid:2)(cid:12) (cid:12) | n | | n | | n | n | | n |(cid:12) (cid:12) | n (cid:3)
+E Gλ,θ 2p Vi,λ,c +E Gλ,x,i 2p Vi,λ,c
| n | | n (cid:0) (cid:2) | n | | n (cid:3) (cid:2) (cid:3)
p((cid:2)p 1)23p−5 2(cid:3)p(2λd(cid:2)θ/N)p/2p!! ∆λ(cid:3),θ(cid:1)2 p/2 +2pp!!(2λdx)p/2 ∆λ,x,i 2 p/2
≤ − | n | | n |
+(2p)!!(2λdθ/N)(cid:16)p +(2p)!!(2λdx)p (cid:0) (cid:1) (cid:0) (cid:1)
(p(p 1))p+124p−5 (2λd)p/2 ∆(cid:1)λ,θ 2 + ∆λ,x,i 2 p/2 +2(2λd)p
≤ − | n | | n |
(p(p 1))p+124p−5(cid:16) (2λd)p/2(cid:0)Aλ p/2 +2(2λd)p (cid:1) (cid:17)
≤ − n
(p(p 1))p+124p−5((cid:16) 2λd)p/2 r(cid:12)λ V(cid:12) λ p +wλ (cid:17)
≤ − (cid:12)p/2|(cid:12)n| p/2
+(p(p 1))p+124p−52(2λd)p.
(cid:0) (cid:1)
−
Combining the above results in view of (49) yields
p
p
E Aλ p−k Bλ k
k | n| | n|
" #
k=2(cid:18) (cid:19)
X
16(2λd)p(p 1)2p−3 rλ Vλ 2(p−1) +wλ
≤ − p−1| n| p−1
+96(2λd)2p(p 1)2p−3 rλ Vλ 2(p−2) +wλ
− (cid:0) p−2| n| p−2(cid:1)
+(p(p 1))p+124p−5(2λd)p/2 rλ Vλ p +wλ
− (cid:0) p/2| n| p/2(cid:1)
+(p(p 1))p+124p−4(2λd)p. (50)
(cid:0) (cid:1)
−
Substituting (47),(48) and (50) into (46) also yields
E Vi,λ,c 2p Vi,λ,c rλ Vi,λ,c 2p +wλ
| n+1 | | n ≤ p| n | p
h i +4p(2λd)(1+4(p 1)2p−3) rλ Vi,λ,c 2(p−1) +wλ
− p−1| n | p−1
+96(2λd)2p(p 1)2p−3 rλ Vi,λ,c 2(p−2) +wλ
− p−2|(cid:0)n | p−2 (cid:1)
+(p(p 1))p+124p−5(2λd)p/2 rλ Vi,λ,c p +wλ
− (cid:0) p/2| n | p/2(cid:1)
+(p(p 1))p+124p−4(2λd)p. (51)
(cid:0) (cid:1)
−
2d 2d
Consider Vi,λ,c 2(p(p 1))p+124(p−1) 1/2 2(p(p 1))p+124(p−1) 1/p .
| n | ≥ µ/4 − ≥ µ/4 −
r r
(cid:8) (cid:9) (cid:8) (cid:9)
36Then one observes
λµ
E Vi,λ,c 2p Vi,λ,c rλ Vi,λ,c 2p +wλ + Vi,λ,c 2 rλ Vi,λ,c 2(p−1)
| n+1 | | n ≤ p| n | p 2 4| n | p−1| n |
h i (λµ)2 · (cid:0) (cid:1)
+ Vi,λ,c 4 rλ Vi,λ,c 2(p−2)
2 42| n | p−2| n |
·
(λµ)p/2 (cid:0) (cid:1)
+ Vi,λ,c p rλ Vi,λ,c p +(p(p 1))p+124(p−1)(2λd)p
2 4p/2| n | p/2| n | −
·
+(p(p 1))p+124((cid:0)p−1) (2λd)w(cid:1)λ +(2λd)2wλ +(2λd)p/2wλ
− p−1 p−2 p/2
p/2 p/2−1
λµ (cid:0) 1 λµ λµ (cid:1)
rλ 1 + 1
≤ p/2 − 4 2 4 − 4
(cid:18) (cid:19) (cid:18) (cid:19)(cid:18) (cid:19)
2 p/2−2 p/2
1 λµ λµ 1 λµ
+ 1 + Vi,λ,c 2p
2 4 − 4 2 4 | n |
!
(cid:18) (cid:19) (cid:18) (cid:19) (cid:18) (cid:19)
+wλ +(p(p 1))p+124(p−1)(2λd)p
p −
+(p(p 1))p+124(p−1) (2λd)wλ +(2λd)2wλ +(2λd)p/2wλ .
− p−1 p−2 p/2
Using the fact that λµ 1 we get (cid:0) (cid:1)
≤
λµ
E Vi,λ,c 2p Vi,λ,c 1 Vi,λ,c 2p
| n+1 | | n ≤ − 2 | n |
(cid:18) (cid:19)
h i
+wλ +(p(p 1))p+124(p−1)(2λd)p
p −
+(p(p 1))p+124(p−1) (2λd)wλ +(2λd)2wλ +(2λd)p/2wλ
− p−1 p−2 p/2
λµ
1 Vi,λ,c 2p(cid:0) +(p(p 1))p+124(p−1)10pCp(dλ)p. (cid:1)
≤ − 2 | n | −
(cid:18) (cid:19)
2d
Consequently, on Vi,λ,c 2(p(p 1))p+124(p−1) 1/2 , we get
{| n | ≤ µ/4 − }
r
(cid:8) (cid:9)
λµ
E Vi,λ,c 2p Vi,λ,c 1 Vi,λ,c 2p
| n+1 | | n ≤ − 2 | n |
(cid:18) (cid:19)
h i
+(p(p 1))p+124(p−1)10pCp(dλ)p
−
p
4
+3 p(p 1)p+124(p−1) p (2d)p 2p−1.
− µ
(cid:18) (cid:19)
(cid:0) (cid:1)
Hence the bound in (51) is further improved to
λµ
E Vi,λ,c 2p 1 E Vi,λ,c 2p +C(d,p,λ,µ),
| n+1 | ≤ − 2 | n |
(cid:18) (cid:19)
h i
(cid:2) (cid:3)
and the usual bound for recursive sequences follows.
37A.3.3 Proof of Proposition 5
Proof. Consider theusualsplitonthedifferencebetween theinterpolationandthescaled
dynamics and apply the Itˆo’s formula for x x 2
→ | |
N
1
Zλ,c 2 = θλ,c ϑ 2 + Xi,λ,c i 2
| t −Zλt | | t − λt | N | t −Xλt|
i=1
X
t 1 N 1 N
= 2λ hθ (θλ,c,Xi,λ,c) hθ(ϑλ , i ),θi,λ,c ϑi ds
− N λ,c ⌊t⌋ ⌊t⌋ − N λt Xλt t − λt
Z0 * i=1 i=1 +
X X
2λ N t
hx (θλ,c,Xi,λ,c) hx(ϑλ , i ),Xi,λ,c i ds
−N λ,c ⌊t⌋ ⌊t⌋ − λt Xλt t −Xλt
Xi=1 Z0 D E
2λ N t
= h (Vi,λ,c) h( i ),Vi,λ,c i ds.
−N h λ,c ⌊t⌋ − Vλt t −Vλti
i=1
Z0
X
In order to make the forthcoming calculations more readable we define the following
quantities:
ei = Vi,λ,c i and e = Zλ,c ,
t t −Vλt t t −Zλt
so that it follows: 1/N N ei 2 = e 2. Then by taking the derivative of the above
i=1| t| | t |
expansion we obtain
P
N
d 2λ
e 2 = h (Vi,λ,c) h( i ),ei
dt| t | −N h λ,c ⌊t⌋ − Vλt ti
i=1
X
N
2λ
= h(Vi,λ,c ) h( i ),ei
−N h t − Vλt ti
i=1
X
N
2λ
h (Vi,λ,c) h(Vi,λ,c),ei
− N h λ,c ⌊t⌋ − ⌊t⌋ ti
i=1
X
N
2λ
h(Vi,λ,c) h(Vi,λ,c ),ei
− N h ⌊t⌋ − t ti
i=1
X
= k (t)+k (t)+k (t). (52)
1 2 3
The first term k (t) is controlled through the convexity of the initial potential, the
1
taming error k (t) can be controlled via the properties of the taming function and the
2
additional moment bounds that we have established, which are also used to control the
38discretisation error k (t). In particular,
3
N
2λ
k (t) = h(Vi,λ,c ) h( i,λ),ei
1 −N h t − Vλt ti
i=1
X
N
2λµ
ei 2 = 2λµ e 2. (53)
≤ − N | t| − | t |
i=1
X
Using the Cauchy-Schwarz inequality and the ǫ-Young inequality for ǫ = µ/2 yields
N
2λ
k (t) = h (Vi,λ,c) h(Vi,λ,c),ei
2 −N h λ,c ⌊t⌋ − ⌊t⌋ ti
i=1
X
N
2λ 1 µ
h (Vi,λ,c) h(Vi,λ,c) 2 + ei 2
≤ N µ| λ,c ⌊t⌋ − ⌊t⌋ | 4| t|
i=1 (cid:18) (cid:19)
X
N
λµ 2λ
2 e 2 + 2λC2(1+ Vi,λ,c 4(ℓ+1))
≤ 4 | t | µN 2 | ⌊t⌋ |
i=1
X
λµ 4λ2 4λ2 N
2 e 2 + C2 + Vi,λ,c 4(ℓ+1). (54)
≤ 4 | t | µ 2 µN | ⌊t⌋ |
i=1
X
Similarly for the last term we get
N
2λ
k (t) = h(Vi,λ,c) h(Vi,λ,c ),ei
3 −N h ⌊t⌋ − t ti
i=1
X
N
2λ 1 µ
h(Vi,λ,c) h(Vi,λ,c ) 2 + ei 2
≤ N µ| ⌊t⌋ − t | 4| t|
i=1 (cid:18) (cid:19)
X
λµ 2λL2 N
2 e 2 + (1+ Vi,λ,c ℓ + Vi,λ,c ℓ)2 Vi,λ,c Vi,λ,c 2 . (55)
≤ 4 | t | µN | ⌊t⌋ | | t | | ⌊t⌋ − t |
Xi=1 (cid:16) (cid:17)
39By combining the above bounds (53),(54) and (55) in view of (52) and taking the
expectation, one obtains
d 4λ2C2 4λ2 1 N
E e 2 µλE e 2 + 2 + E Vi,λ,c 4(ℓ+1)
dt | t | ≤ − | t | µ µ N | ⌊t⌋ |
(cid:2) (cid:3) (cid:2) (cid:3)
Xi=1 h i
2λL2 1 N 1/2 1/2
+ E (1+ Vi,λ,c ℓ + Vi,λ,c ℓ)4 E Vi,λ,c Vi,λ,c 4
µ N | ⌊t⌋ | | t | | ⌊t⌋ − t |
Xi=1 (cid:16) h i(cid:17) (cid:16) h i(cid:17)
4λ2(C2 +C )
µλE e 2 + 2 3
t
≤ − | | µ
2λL2 (cid:2) (cid:3)
+ (6max 1,2C )λ µC +6 dµC +10d+2
4 5 6
µ { }
(cid:16) p µ (cid:17)
µλE e 2 +λ2M(L,µ,d,ℓ) λ E e 2 +λM .
t t
≤ − | | ≤ −2 | |
(cid:16) (cid:17)
(cid:2) (cid:3) (cid:2) (cid:3)
by multiplying both sides by the integrating factor eλµt/2 and rearranging the term one
gets
d
eλµt/2E e 2 λ2Meλµt/2
t
dt | | ≤
(cid:0) E e(cid:2) 2 (cid:3)(cid:1) λ2M .
t
| | ≤ µ
(cid:2) (cid:3)
40