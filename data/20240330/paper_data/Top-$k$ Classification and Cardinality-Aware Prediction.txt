1–40
Top-k Classification and Cardinality-Aware Prediction
AnqiMao AQMAO@CIMS.NYU.EDU
CourantInstituteofMathematicalSciences,NewYork
MehryarMohri MOHRI@GOOGLE.COM
GoogleResearchandCourantInstituteofMathematicalSciences,NewYork
YutaoZhong YUTAO@CIMS.NYU.EDU
CourantInstituteofMathematicalSciences,NewYork
Abstract
Wepresentadetailedstudyoftop-kclassification,thetaskofpredictingthekmostprobable
classesforaninput,extendingbeyondsingle-classprediction. Wedemonstratethatseveralprevalent
surrogatelossfunctionsinmulti-classclassification,suchascomp-sumandconstrainedlosses,are
supportedbyH-consistencyboundswithrespecttothetop-kloss. Theseboundsguaranteeconsis-
tencyinrelationtothehypothesissetH,providingstrongerguaranteesthanBayes-consistencydue
totheirnon-asymptoticandhypothesis-setspecificnature.Toaddressthetrade-offbetweenaccuracy
andcardinalityk,wefurtherintroducecardinality-awarelossfunctionsthroughinstance-dependent
cost-sensitivelearning. Forthesefunctions,wederivecost-sensitivecomp-sumandconstrained
surrogatelosses,establishingtheirH-consistencyboundsandBayes-consistency. Minimizingthese
lossesleadstonewcardinality-awarealgorithmsfortop-kclassification. Wereporttheresultsof
extensiveexperimentsonCIFAR-100,ImageNet,CIFAR-10,andSVHNdatasetsdemonstratingthe
effectivenessandbenefitofthesealgorithms.
1. Introduction
Top-k classification consists of predicting the k most likely classes for a given input, as opposed
tosolelypredictingthesinglemostlikelyclass. Severalcompellingreasonssupporttheadoption
of top-k classification. First, it enhances accuracy by allowing the model to consider the top k
predictions, accommodating uncertainty and providing a more comprehensive prediction. This
provesparticularlyvaluableinscenarioswheremultiplecorrectanswersexist,suchasimagetagging,
whereatop-kclassifiercanidentifyallrelevantobjectsinanimage. Furthermore,top-kclassification
finds application in ranking and recommendation tasks, like suggesting the top k most relevant
products in e-commerce based on user queries. The confidence scores associated with the top k
predictionsalsoserveasameanstoestimatethemodel’suncertainty,acrucialaspectinapplications
requiringinsightintothemodel’sconfidencelevel.
Ensembling can also benefit from top-k predictions as they can be combined from multiple
models,contributingtoimprovedoverallperformancebyintroducingamorerobustanddiverseset
ofpredictions. Inaddition,top-k predictionscanserveasinputfordownstreamtaskslikenatural
languagegenerationordialoguesystems,enhancingtheperformanceofthesetasksbyprovidinga
broaderrangeofpotentialcandidates. Finally,theinterpretabilityofthemodel’sdecision-making
processisenhancedbyexaminingthetopk predictedclasses,allowinguserstogaininsightsintothe
rationalebehindthemodel’spredictions.
However,thetop-klossfunctionisnon-continuousandnon-differentiable,anditsdirectoptimiza-
tionisintractable. Therefore,top-kclassificationalgorithmstypicallyresorttoasurrogateloss(Lapin
© A.Mao,M.Mohri&Y.Zhong.
4202
raM
82
]GL.sc[
1v52691.3042:viXraMAOMOHRIZHONG
etal.,2015,2016;Berradaetal.,2018;Reddietal.,2019;YangandKoyejo,2020;Thilagaretal.,
2022). Thisraisescriticalquestions: Whichsurrogatelossfunctionsadmittheoreticalguarantees
andefficientminimizationproperties? Canwedesignaccuratetop-k classificationalgorithms?
Unlikestandardclassification,thisproblemhasbeenrelativelyunexplored. Acrucialproperty
inthiscontextisBayes-consistency,whichhasbeenextensivelystudiedinbinaryandmulti-class
classification (Zhang, 2004a; Bartlett et al., 2006; Zhang, 2004b; Bartlett and Wegkamp, 2008).
WhileBayes-consistencyhasbeenexploredforvarioustop-k surrogatelosses(Lapinetal.,2015,
2016, 2018; Yang and Koyejo, 2020; Thilagar et al., 2022), some face limitations. Non-convex
"hinge-like" surrogates (Yang and Koyejo, 2020), inspired by ranking (Usunier et al., 2009), and
polyhedral surrogates (Thilagar et al., 2022) cannot lead to effective algorithms as they cannot
beefficientlycomputedandoptimized. Negativeresultsindicatethatseveralconvex"hinge-like"
surrogates (Lapin et al., 2015, 2016, 2018) fail to achieve Bayes-consistency (Yang and Koyejo,
2020). Canweshedmorelightontheseresults?
On the positive side, it has been shown that the logistic loss (or cross-entropy loss used with
thesoftmaxactivation)isaBayes-consistentlossfortop-k classification(Lapinetal.,2015;Yang
andKoyejo,2020). Thispromptsfurtherinquiries: Whichothersmoothlossfunctionsadmitthis
property? Moreimportantly,canweestablishnon-asymptoticandhypothesisset-specificguarantees
for these surrogates, quantifying their effectiveness? Beyond top-k classification, it is important
to consider the trade-off between accuracy and the cardinality k. This leads us to introduce and
studycardinality-awaretop-k classificationalgorithms,whichaimtoachieveahighaccuracywhile
maintainingasmallaveragecardinality.
This paper presents a detailed study of top-k classification. We first show that, remarkably,
several widely used families of surrogate losses used in standard multi-class classification admit
H-consistencybounds(Awasthietal.,2022a,b;Maoetal.,2023f,b)withrespecttothetop-k loss.
Thesearestrongconsistencyguaranteesthatarenon-asymptoticandspecifictothehypothesisset
H adopted, which further imply Bayes-consistency. In Section 3, we demonstrate this property
forthebroadfamilyofcomp-sumlosses(Maoetal.,2023f),whichincludesthelogisticloss,the
sum-exponentialloss,themeanabsoluteerrorloss,andthegeneralizedcross-entropyloss. Further,
inSection4,weproveitforconstrainedlosses,originallyintroducedformulti-classSVM(Leeetal.,
2004), including the constrained exponential loss, constrained hinge loss and squared hinge loss,
andtheρ-marginloss. Theseguaranteesprovideastrongfoundationforprincipledalgorithmsin
top-k classification,leveragingtheminimizationofthesesurrogatelossfunctions. Manyofthese
lossfunctionsareknownfortheirsmoothpropertiesandfavorableoptimizationsolutions.
InSection5,wefurtherinvestigatecardinality-awaretop-k classification,aimingtoreturnan
accurate top-k list with the lowest average cardinality k for each input instance. We introduce
a target loss function tailored to this problem through instance-dependent cost-sensitive learning
(Section5.1). Subsequently,wepresenttwonovelsurrogatelossfamiliesforoptimizingthistarget
loss: cost-sensitivecomp-sumlosses(Section5.2)andcost-sensitiveconstrainedlosses(Section5.3).
Theselossfunctionsareobtainedbyaugmentingtheirstandardcounterpartswithinstance-dependent
costterms. WeestablishH-consistencyboundsandthusBayes-consistencyforthesecost-sensitive
surrogatelosseswithrespecttothecardinality-awaretargetloss. Minimizingtheselossesleadsto
newcardinality-awarealgorithmsfortop-k classification. Section6presentsexperimentalresultson
CIFAR-100,ImageNet,CIFAR-10,andSVHNdatasets,demonstratingtheeffectivenessofthese
algorithms.
2TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
2. Preliminaries
Weconsiderthelearningtaskoftop-k classificationwithn≥2classes,thatisseekingtoensurethat
thecorrectclasslabelforagiveninputsampleisamongthetopk predictedclasses. WedenotebyX
theinputspaceandY =[n]∶={1,...,n}thelabelspace. WedenotebyDadistributionoverX ×Y
andwritep(x,y)=D (Y =y ∣X =x)todenotetheconditionalprobabilityofY =y givenX =x.
We also write p(x) = (p(x,1),...,p(x,n)) to denote the corresponding conditional probability
vector.
We denote by ℓ∶H all ×X ×Y → R a loss function defined for the family of all measurable
functions H all. Given a hypothesis set H ⊆ H all, the conditional error of a hypothesis h and the
best-in-classconditionalerroraredefinedasfollows:
C ℓ(h,x)= E [ℓ(h,x,y)]= ∑p(x,y)ℓ(h,x,y)
y∣x y∈Y
C∗ ℓ(H,x)= inf C ℓ(h,x)= inf ∑p(x,y)ℓ(h,x,y).
h∈H h∈H
y∈Y
Accordingly,thegeneralizationerrorofahypothesishandthebest-in-classgeneralizationerrorare
definedby:
E ℓ(h)= E [ℓ(h,x,y)]=E x[C ℓ(h,x)]
(x,y)∼D
E∗ ℓ(H )= inf E ℓ(h)= inf E x[C ℓ(h,x)].
h∈H h∈H
Given a score vector (h(x,1),...,h(x,n)) generated by hypothesis h, we sort its components
in decreasing order and write h k(x) to denote the kth label, that is h(x,h 1(x)) ≥ h(x,h 2(x)) ≥
... ≥ h(x,h n−1(x)) ≥ h(x,h n(x)). Similarly, for a given conditional probability vector p(x) =
(p(x,1),...,p(x,n)), we write p k(x) to denote the kth element in decreasing order, that is
p(x,p 1(x)) ≥ p(x,p 2(x)) ≥ ... ≥ p(x,p n(x)). In the event of a tie for the k-th highest score
or conditional probability, the label h k(x) or p k(x) is selected based on the highest index when
consideringthenaturalorderoflabels.
Thetargetgeneralizationerrorfortop-kclassificationisgivenbythetop-kloss,whichisdenoted
byℓ
k
anddefined,foranyhypothesishand(x,y)∈X ×Yby
ℓ k(h,x,y)=1
y∉{h1(x),...,h
k(x)}.
Thus,thelosstakesvalueonewhenthecorrectlabely isnotincludedinthetop-k predictionsmade
bythehypothesish,zerootherwise. Inthespecialcasewherek =1,thisispreciselythefamiliar
zero-one classification loss. As with the zero-one loss, optimizing the top-k loss is NP-hard for
commonhypothesissets. Therefore,analternativesurrogatelossistypicallyusedtodesignlearning
algorithms.
AcrucialpropertyofthesesurrogatelossesisBayes-consistency. Thisrequiresthat,asymptoti-
cally,nearlyminimizingasurrogatelossoverthefamilyofallmeasurablefunctionsleadstothenear
minimizationofthetop-k lossoverthesamefamily(Steinwart,2007).
Definition1 A surrogate loss ℓ is said to be Bayes-consistent with respect to the top-k loss ℓ
k
if,forallgivensequencesofhypotheses{h n}n∈N ⊂H all andanydistribution,lim n→+∞E ℓ(h n)−
E∗ ℓ(H all)=0implieslim n→+∞E
ℓ
k(h n)−E∗
ℓ
k(H all)=0.
3MAOMOHRIZHONG
Bayes-consistency is an asymptotic guarantee and applies only to the family of all measurable
functions. Recently,Awasthi,Mao,Mohri,andZhong(2022a,b)proposedastrongerconsistency
guarantee,referredtoasH-consistencybounds. Theseareupperboundsonthetargetestimation
errorintermsofthesurrogateestimationerrorthatarenon-asymptoticandhypothesisset-specific
guarantees.
Definition2 GivenahypothesissetH,asurrogatelossℓissaidtoadmitanH-consistencybound
with respect to the top-k loss ℓ if, for some non-decreasing function f, the following inequality
k
holdsforallh∈Handforanydistribution:
f(E
ℓ
k(h)−E∗
ℓ
k(H ))≤E ℓ(h)−E∗ ℓ(H ).
W este imre af te ior nto erE roℓ k r.( Th h) e− seE b∗ ℓ k o( uH nd) sa ims pth lye Bta ar yg ee st -e cost nim sisa tt ei no cn ye wrr ho er na Hnd =E Hℓ( ah ll) ,− byE t∗ ℓ a( kH ing) ta hs et lh ie ms itu orr no bg oa tt he
sides.
We will study H-consistency bounds for common surrogate losses in the multi-class classifi-
cation,withrespecttothetop-k lossℓ . AkeyquantityappearinginH-consistencyboundsisthe
k
minimizabilitygap,whichmeasuresthedifferencebetweenthebest-in-classgeneralizationerrorand
theexpectationofthebest-in-classconditionalerror,definedforagivenhypothesissetHandaloss
functionℓby:
M ℓ(H )=E∗ ℓ(H )−E x[C∗ ℓ(H,x)].
AsshownbyMaoetal.(2023f),theminimizabilitygapisnon-negativeandisupperboundedby
the approximation error A ℓ(H ) = E∗ ℓ(H )−E∗ ℓ(H all): 0 ≤ M ℓ(H ) ≤ A ℓ(H ). When H = H all or
moregenerallyA
ℓ
(H )=0,theminimizabilitygapvanishes. However,ingeneral,itisnon-zero
log
andprovidesafinermeasurethantheapproximationerror. Thus,H-consistencyboundsprovidea
strongerguaranteethantheexcesserrorbounds.
Wewillspecificallystudythesurrogatelossfamiliesofcomp-sumlossesandconstrainedlosses
inmulti-classclassification,whichhavebeenshowninthepasttobenefitfromH-consistencybounds
with respect to the zero-one classification loss, that is ℓ k with k = 1 (Awasthi et al., 2022b; Mao
etal.,2023f)(seealso(Maoetal.,2023c,d,e,a;Zhengetal.,2023;Maoetal.,2024a,c,b;Mohrietal.,
2024)). Wewillsignificantlyextendtheseresultstotop-k classificationandproveH-consistency
boundsfortheselossfunctionswithrespecttoℓ
k
forany1≤k ≤n.
Notethatanothercommonlyusedfamilyofsurrogatelossesinmulti-classclassificationisthe
maxlosses,whicharedefinedthroughaconvexfunction,suchasthehingelossfunctionappliedto
themargin(CrammerandSinger,2001;Awasthietal.,2022b). However,asshownin(Awasthietal.,
2022b),nonon-trivialH-consistencyguaranteeholdsformaxlosseswithrespecttoℓ ,evenwhen
k
k =1.
Wefirstcharacterizethebest-inclassconditionalerrorandtheconditionalregretoftop-k loss,
whichwillbeusedintheanalysisofH-consistencybounds. WedenotebyS[k] ={X ⊂S ∣∣X∣=k}
thesetofallk-subsetsofasetS. Wewillstudyanyhypothesissetthatisregular.
Definition3 LetA(n,k)bethesetoforderedk-tupleswithdistinctelementsin[n]. Wesaythat
a hypothesis set H is regular for top-k classification, if the top-k predictions generated by the
hypothesissetcoverallpossibleoutcomes:
∀x∈X, {(h 1(x),...,h k(x))∶h∈H }=A(n,k).
4TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
Common hypothesis sets such as that of linear models or neural networks, or the family of all
measurablefunctions,areallregularfortop-k classification.
Lemma4 AssumethatHisregular. Then,foranyh∈Handx∈X,thebest-inclassconditional
errorandtheconditionalregretofthetop-k losscanbeexpressedasfollows:
k
C∗
ℓ
(H,x)=1−∑p(x,p i(x))
k
i=1
k
∆C
ℓ
,H(h,x)=∑(p(x,p i(x))−p(x,h i(x))).
k
i=1
TheproofisincludedinAppendixA.Notethat,fork =1,theresultcoincideswiththeknown
identitiesforstandardmulti-classclassificationwithregularhypothesissets(Awasthietal.,2022b,
Lemma3).
Aswith(Awasthietal.,2022b;Maoetal.,2023f),inthefollowingsections,wewillconsider
hypothesissetsthataresymmetricandcomplete. Thisincludestheclassoflinearmodelsandneural
networkstypicallyusedinpractice,aswellasthefamilyofallmeasurablefunctions. Wesaythat
ahypothesissetHissymmetricifitisindependentoftheorderingoflabels. Thatis,forally ∈Y,
thescoringfunctionx↦h(x,y)belongstosomereal-valuedfamilyoffunctionsF. Wesaythata
hypothesissetiscompleteif,forall(x,y)∈X ×Y,thesetofscoresh(x,y)canspanoverthereal
numbers, that is, {h(x,y)∶h∈H } = R. Note that any symmetric and complete hypothesis set is
regularfortop-k classification.
Next, we analyze the broad family of comp-sum losses, which includes the commonly used
logisticloss(orcross-entropylossusedwiththesoftmaxactivation)asaspecialcase.
3. H-ConsistencyBoundsforComp-SumLosses
Comp-sumlossesaredefinedasthecompositionofafunctionΦwiththesumexponentiallosses,as
shownin(Maoetal.,2023f). Foranyh∈Hand(x,y)∈X ×Y,theyareexpressedas
ℓcomp (h,x,y)=Φ⎛
∑
eh(x,y′ )−h(x,y)⎞ ,
⎝y′ ≠y ⎠
where Φ∶R → R is non-decreasing. When Φ is chosen as the function t ↦ log(1+t), t ↦ t,
+ +
t ↦ 1− 1 and t ↦ 1 (1−( 1 )α ), α ∈ (0,1), ℓcomp (h,x,y) coincides with the (multinomial)
logistic l1 o+ st s ℓ (Verhα ulst, 181+ 3t 8, 1845; Berkson, 1944, 1951), the sum-exponential loss ℓcomp
log exp
(WestonandWatkins,1998;Awasthietal.,2022b),themeanabsoluteerrorlossℓ (Ghoshetal.,
mae
2017),andthegeneralizedcrossentropylossℓ (ZhangandSabuncu,2018),respectively. Wewe
gce
willspecificallystudytheselossfunctionsandshowthattheybenefitfromH-consistencybounds
withrespecttothetop-k loss.
3.1. Logisticloss
Wefirstshowthatthemostcommonlyusedlogisticloss,definedasℓ log(h,x,y)=log(∑y′ ∈Yeh(x,y′ )−h(x,y)),
admitsH-consistencyboundswithrespecttoℓ .
k
5MAOMOHRIZHONG
Theorem5 Assume that H is symmetric and complete. Then, for any 1 ≤ k ≤ n, the following
H-consistencyboundholdsforthelogisticloss:
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤kψ−1 (E
ℓ
log(h)−E∗
ℓ
log(H )+M
ℓ
log(H )),
whereψ(t)= 1 2−tlog(1−t)+ 1 2+tlog(1+t),t∈[0,1]. InthespecialcasewhereA
ℓ
log(H )=0,for
any1≤k ≤n,thefollowingupperboundholds:
E
ℓ
k(h)−E∗
ℓ
k(H )≤kψ−1 (E
ℓ
log(h)−E∗
ℓ
log(H )).
The proof is included in Appendix B.1. The second part follows from the fact that when
A ℓ (H ) = 0, the minimizability gap M ℓ (H ) vanishes. By taking the limit on both sides,
log log
Theorem5impliestheH-consistencyandBayes-consistencyoflogisticlosswithrespecttothetop-k
loss. Itfurthershowsthat,whentheestimationerrorofℓ
log
isreducedtoϵ>0,thentheestimation
√
errorofℓ
k
isupperboundedbykψ−1 (ϵ),whichisapproximatelyk 2ϵforϵsmall.
3.2. Sumexponentialloss
Inthissection,weproveH-consistencyboundguaranteesforthesum-exponentialloss, whichis
definedasℓc eo xm pp (h,x,y)=∑y′ ≠yeh(x,y′ )−h(x,y) andiswidelyusedinmulti-classboosting(Saberian
andVasconcelos,2011;MukherjeeandSchapire,2013;Kuznetsovetal.,2014).
Theorem6 Assume that H is symmetric and complete. Then, for any 1 ≤ k ≤ n, the following
H-consistencyboundholdsforthesumexponentialloss:
E ℓ k(h)−E∗ ℓ k(H )+M ℓ k(H )≤kψ−1 (E ℓc eo xm pp(h)−E∗ ℓc eo xm pp(H )+M ℓc eo xm pp(H )),
√
whereψ(t)=1− 1−t2,t∈[0,1]. InthespecialcasewhereA ℓcomp(H )=0,forany1≤k ≤n,
exp
thefollowingboundholds:
E ℓ k(h)−E∗ ℓ k(H )≤kψ−1 (E ℓc eo xm pp(h)−E∗ ℓc eo xm pp(H )),
The proof is included in Appendix B.2. The second part follows from the fact that when
A ℓcomp(H ) = 0, the minimizability gap M ℓcomp(H ) vanishes. As with the logistic loss, the sum
exp exp
exponential loss is Bayes-consistent and H-consistent with respect to the top-k loss. Here too,
whentheestimationerrorofℓcomp isreducedtoϵ,theestimationerrorofℓ isupperboundedby
exp k
√
kψ−1 (ϵ)≈k 2ϵforsufficientlysmallϵ>0.
3.3. Meanabsoluteerrorloss
Themeanabsoluteerrorloss,definedasℓ mae(h,x,y)=1−[∑y′
∈Yeh(x,y′ )−h(x,y)]−1
,isknowntobe
robusttolabelnoisefortrainingneuralnetworks(Ghoshetal.,2017). Thefollowingshowsthatit
benefitsfromH-consistencyboundswithrespecttothetop-k lossaswell.
Theorem7 Assume that H is symmetric and complete. Then, for any 1 ≤ k ≤ n, the following
H-consistencyboundholdsforthemeanabsoluteerrorloss:
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤kn(E ℓmae(h)−E∗ ℓmae(H )+M ℓmae(H )).
InthespecialcasewhereA mae(H )=0,forany1≤k ≤n,thefollowingboundholds:
E
ℓ
k(h)−E∗
ℓ
k(H )≤kn(E ℓmae(h)−E∗ ℓmae(H )).
6TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
The proof is included in Appendix B.3. The second part follows from the fact that when
A ℓmae(H ) = 0, the minimizability gap M ℓmae(H ) vanishes. As for the logistic loss and the sum
exponential loss, the result implies Bayes-consistency. However, different from these losses, the
boundforthemeanabsoluteerrorlossisonlylinear: whentheestimationerrorofℓ isreducedtoϵ,
ϵ
theestimationerrorofℓ isupperboundedbyknϵ. Thedownsideofthismorefavorablelinearrate
k
isthedependencyinthenumberofclassesandthefactthatthemeanabsolutevaluelossisharderto
optimizeZhangandSabuncu(2018).
3.4. Generalizedcross-entropyloss
Here, we provide H-consistency bounds for the generalized cross-entropy loss, which is defined
asℓ gce(h,x,y)= α1 [1−[∑y′ ∈Yeh(x,y′ )−h(x,y)]−α ],α∈(0,1),andisageneralizationofthelogistic
lossandmeanabsoluteerrorlossforlearningdeepneuralnetworkswithnoisylabels(Zhangand
Sabuncu,2018).
Theorem8 Assume that H is symmetric and complete. Then, for any 1 ≤ k ≤ n, the following
H-consistencyboundholdsforthegeneralizedcross-entropy:
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤kψ−1 (E ℓgce(h)−E∗ ℓgce(H )+M ℓgce(H )),
1 1 1−α
where ψ(t) = αn1 α[[(1+t)1−α+ 2(1−t)1−α ] −1], for all α ∈ (0,1), t ∈ [0,1]. In the special case
whereA ℓgce(H )=0,forany1≤k ≤n,thefollowingupperboundholds:
E
ℓ
k(h)−E∗
ℓ
k(H )≤kψ−1 (E ℓgce(h)−E∗ ℓgce(H )),
The proof is presented in Appendix B.4. The second part follows from the fact that when
A ℓgce(H ) = 0, the minimizability gap M ℓgce(H ) vanishes. The bound for the generalized cross-
entropylossdependsonboththenumberofclassesnandtheparameterα. Whentheestimation
√
error of ℓ log is reduced to ϵ, the estimation error of ℓ k is upper bounded by kψ−1 (ϵ) ≈ k 2nαϵ
for sufficiently small ϵ > 0. A by-product of this result is the Bayes-consistency of generalized
cross-entropy.
Intheproofofprevioussections,weusedthefactthattheconditionalregretofthetop-k lossis
thesumofk differencesbetweentwoprobabilities. Wethenupperboundedeachdifferencewiththe
conditionalregretofthecomp-sumloss,usingahypothesisbasedonthetwoprobabilities. Thefinal
boundisderivedbysummingthesedifferences.
3.5. Minimizabilitygapsandrealizability
ThekeyquantitiesinourH-consistencyboundsaretheminimizabilitygaps,whichcanbeupper
bounded by the approximation error, or more refined terms, depending on the magnitude of the
parameterspace,asdiscussedbyMaoetal.(2023f). Aspointedoutbytheseauthors,thesequantities,
alongwiththefunctionalform,canhelpcomparedifferentcomp-sumlossfunctions.
Here,wefurtherdiscusstheimportantroleofminimizabilitygapsundertherealizabilityassump-
tion,andtheconnectionwithsomenegativeresultsofYangandKoyejo(2020).
Definition9(top-k-H-realizability) A distribution D over X ×Y is top-k-H-realizable, if there
existsahypothesish∈HsuchthatP (x,y)∼D(h(x,y)>h(x,h k+1(x)))=1.
7MAOMOHRIZHONG
ThisextendstheH-realizabilitydefinitionfromstandard(top-1)classification(LongandServedio,
2013)totop-k classificationforanyk ≥1.
Definition10 WesaythatahypothesissetHisclosedunderscaling,ifitisacone,thatisforall
h∈Handα∈R ,αh∈H.
+
Definition11 WesaythatasurrogatelossℓisrealizableH-consistentwithrespecttoℓ ,ifforall
k
k ∈[1,n],andforanysequenceofhypotheses{h n}n∈N ⊂H andtop-k-H-realizabledistribution,
lim n→+∞E ℓ(h n)−E∗ ℓ(H )=0implieslim n→+∞E
ℓ
k(h n)−E∗
ℓ
k(H )=0.
WhenHisclosedunderscaling,fork =1andallcomp-sumlossfunctionsℓ=ℓ log,ℓc eo xm pp ,ℓ
gce
and
ℓ mae,itcanbeshownthatE∗ ℓ(H )=M ℓ(H )=0foranyH-realizabledistribution. Forexample,for
ℓ=ℓ log,byusingtheLebesguedominatedconvergencetheorem,
M ℓ log(H )≤E∗ ℓ log(H )≤ βl →im +∞E ℓ log(βh∗ )
= lim log[1+ ∑ eβ(h∗ (x,y′ )−h∗ (x,y)) ]=0
β→+∞ y′ ≠y
whereh∗ satisfiesP (x,y)∼D(h∗(x,y)>h∗(x,h 2(x)))=1Therefore,Theorems5,6,7and8imply
thatalltheselossfunctionsarerealizableH-consistentwithrespecttoℓ
0−1
(ℓ
k
fork =1)whenHis
closedunderscaling.
Theorem12 AssumethatHisclosedunderscaling. Then,ℓ ,ℓcomp,ℓ andℓ arerealizable
log exp gce mae
H-consistentwithrespecttoℓ .
0−1
The formal proof is presented in Appendix C. However, for k > 1, since in the realizability
assumption, h(x,y)isonlylargerthanh(x,h k+1(x))andcanbesmallerthanh(x,h 1(x)), there
mayexistanH-realizabledistributionDsuchthatM
ℓ
(H )>0. Thisexplainstheinconsistency
log
ofthelogisticlossontop-k separabledatawithlinearpredictors,whenk =2andn>2,asshown
in(YangandKoyejo,2020). Moregenerally,theexactsameexamplein(YangandKoyejo,2020,
Proposition5.1)canbeusedtoshowthatallthecomp-sumlosses,ℓ ,ℓcomp ,ℓ andℓ arenot
log exp gce mae
realizableH-consistentwithrespecttoℓ . Nevertheless,aspreviouslyshown,whenthehypothesis
k
setHadoptedissufficientlyrichsuchthatM ℓ(H )=0orevenA ℓ(H )=0,theyareguaranteedto
beH-consistent. Thisistypicallythecaseinpracticewhenusingdeepneuralnetworks.
4. H-ConsistencyBoundsforConstrainedLosses
Constrained losses are defined as a summation of a function Φ applied to the scores, subject to a
constraint,asshownin(Leeetal.,2004;Awasthietal.,2022b). Foranyh∈Hand(x,y)∈X ×Y,
theyareexpressedas
ℓcstnd (h,x,y)=
∑
Φ(−h(x,y′ )),
y′ ≠y
with the constraint ∑y∈Yh(x,y) = 0, where Φ∶R → R
+
is non-increasing. When Φ is chosen as
the function t ↦ e−t, t ↦ max{0,1−t}2 , t ↦ max{0,1−t} and t ↦ min{max{0,1−t/ρ},1},
ρ > 0, ℓcstnd (h,x,y) are referred to as the constrained exponential loss ℓc es xt pnd, the constrained
squaredhingelossℓ ,theconstrainedhingelossℓ ,andtheconstrainedρ-marginlossℓ ,
sq−hinge hinge ρ
respectively(Awasthietal.,2022b). Wenowstudytheselossfunctionsandshowthattheybenefit
fromH-consistencyboundswithrespecttothetop-k loss.
8TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
4.1. Constrainedexponentialloss
Wefirstconsidertheconstrainedexponentialloss,definedasℓc es xt pnd (h,x,y) = ∑y′ ≠yeh(x,y′ ). The
followingresultprovideH-consistencyboundsforℓcstnd.
exp
Theorem13 Assume that H is symmetric and complete. Then, for any 1 ≤ k ≤ n, the following
H-consistencyboundholdsfortheconstrainedexponentialloss:
1
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤2k(E
ℓc es xt
pnd(h)−E∗
ℓc es xt
pnd(H )+M
ℓc es xt
pnd(H ))2.
InthespecialcasewhereA ℓcstnd(H )=0,forany1≤k ≤n,thefollowingboundholds:
exp
1
E
ℓ
k(h)−E∗
ℓ
k(H )≤2k(E
ℓc es xt
pnd(h)−E∗
ℓc es xt
pnd(H ))2.
The proof is included in Appendix D.1. The second part follows from the fact that when
A ℓcstnd(H )=0,wehaveM ℓcstnd(H )=0. Therefore,theconstrainedexponentiallossisH-consistent
exp exp
a ϵ,n td heB na ,y te hs e-c tao rn gs ei tst ee sn tt imw ai tt ih or nes ep rre oc rt sto atiℓ
sk
fi. eI sf Eth ℓe k(s hu )rr −og Ea
∗
ℓt ke (e Hst )im ≤a 2ti ko √ne ϵr .rorE
ℓc es xt
pnd(h)−E∗
ℓc es xt
pnd(H )is
4.2. Constrainedsquaredhingeloss
2
Here,weconsidertheconstrainedsquaredhingeloss,definedasℓ hinge(h,x,y)=∑y′ ≠ymax{0,1+h(x,y′)} .
Thefollowingresultshowsthatℓ admitsanH-consistencyboundwithrespecttoℓ .
sq−hinge k
Theorem14 Assume that H is symmetric and complete. Then, for any 1 ≤ k ≤ n, the following
H-consistencyboundholdsfortheconstrainedsquaredhingeloss:
1
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤2k(E
ℓ
sq−hinge(h)−E∗
ℓ
sq−hinge(H )+M
ℓ
sq−hinge(H ))2.
InthespecialcasewhereA
ℓ
sq−hinge(H )=0,forany1≤k ≤n,thefollowingboundholds:
1
E
ℓ
k(h)−E∗
ℓ
k(H )≤2k(E
ℓ
sq−hinge(h)−E∗
ℓ
sq−hinge(H ))2.
The proof is included in Appendix D.2. The second part follows from the fact that when the
hypothesis set H is sufficiently rich such that A ℓ sq−hinge(H ) = 0, we have M ℓ sq−hinge(H ) = 0. As
w
E
ℓi kth (hth )e −c Eon
∗
ℓ
ks (tr Hai )ne ≤d 2e kx √po ϵn .e Tn hti ia sl alo lss os, imth pe lb ieo su tn hd atis
ℓ
ss qq −u ha inr ge er io so Bt: aE yℓ es sq -− ch oin nge s( ish te) n− tE w∗ ℓ is tq h−h ri en sge p( eH ct) to≤ ℓϵ k.⇒
4.3. Constrainedhingelossandρ-marginloss
Similarly,inAppendixD.3andD.4,westudytheconstrainedhingelossandtheconstrainedρ-margin
loss,respectively. BothareshowntoadmitalinearH-consistencyboundandareBayes-consistent
withrespecttoℓ (SeeTheorems18and19)
k
9MAOMOHRIZHONG
5. Cardinality-AwareLossFunctions
Thestrongtheoreticalresultsoftheprevioussectionsdemonstratethatforcommonhypothesissets
usedinpractice,comp-sumlossesandconstrainedlossescanbeeffectivelyusedassurrogatelosses
for the target top-k loss. Nonetheless, the algorithms seeking to minimize these surrogate losses
offernoguidanceonthecrucialtaskofdeterminingtheoptimalcardinalityk fortop-k classification
applications. Thisselectionisessentialforpracticalperformance,asitdirectlyinfluencesthenumber
ofpredictedpositives.
Inthissection,ourgoalistoselectasuitabletop-k classifierforeachinputinstancex. Foreasier
inputinstances,thetop-k setwithasmallerk containstheaccuratelabel,whileitmaybenecessary
toresorttolargerk valuesforharderinputinstances. Choosingk optimallyforeachinstanceallows
ustomaintainaccuracywhilereducingtheaveragecardinalityused.
Totacklethisproblem,weintroducetargetcardinality-awarelossfunctionsfortop-kclassification
throughinstance-dependentcost-sensitivelearning. Then,weproposetwonovelfamiliesofinstance-
dependant cost-sensitive surrogate losses. These loss functions are derived by augmenting the
standardcomp-sumlossesandconstrainedlosswiththecorrespondingcost. Weshowthebenefits
of these surrogate losses by proving that they admit H-consistency bounds with respect to the
target cardinality-aware loss functions. Minimizing these loss functions leads to a family of new
cardinality-awarealgorithmsfortop-k classification.
5.1. Instance-DependentCost-SensitiveLearning
Givenapre-fixedsubsetK ={k 1,...,k m}⊂[n]ofallpossiblechoicesforcardinalityk,ourgoalis
toselectthebestkinthesamplesuchthatthetop-klossisminimizedwhileusingasmallcardinality.
Moreprecisely,letc∶X ×K ×Ybeainstance-dependentcostfunction,definedas
c(x,k,y)=ℓ k(h,x,y)+λC (k)
(1)
=1
y∉{h1(x),...,h
k(x)}+λC (k)
for some function C ∶[n] → R and parameter λ > 0. Let R be a hypothesis set of functions
+
mappingfromX ×KtoR. Thepredictionofacardinalityselectorr∈Risdefinedasthecardinality
correspondingtothehighestscore, thatisr(x) = argmax k∈Kr(x,k). Intheeventofatieforthe
highestscore,thecardinalityr(x)isselectedbasedonthehighestindexwhenconsideringthenatural
orderoflabels.
Then,ourtargetcardinalityawarelossfunctionℓ̃canbedefinedasfollows: forallr∈R,x∈X
andy ∈Y,
ℓ̃(r,x,y)=c(x,r(x),y). (2)
For example, when the function C is chosen as t∶↦ log(t), the learner will select a cardinality
selectorr∈Rthatselectsthebestk amongKforeachinstancex,intermsofbalancingthetop-k
losswiththemagnitudeoflog(k).
Note that our work focuses on determining the optimal cardinality k for top-k classification,
andthusthecostfunctiondefinedin(1)isbasedonthetop-k sets. However,itcanpotentiallybe
generalizedtoothersettings,suchasthosedescribedin(DenisandHebiri,2017),byusingconfidence
setsandlearningamodelr toselecttheoptimalconfidencesetbasedontheinstance.
(2)isaninstance-dependentcost-sensitivelearningproblem. However,directlyminimizingthis
targetlossisintractable. Inthenextsections,wewillproposenovelsurrogatelossestoaddressthis
10TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
problem. Asausefultool,wecharacterizedtheconditionalregretofthetargetcardinality-awareloss
functioninLemma20,whichcanbefoundinAppendixE.
Withoutlossofgenerality,assumethat0≤c(x,k,y)≤1,whichcanbeachievedbynormalizing
thecostfunction.
5.2. Cost-SensitiveComp-SumLosses
Wefirstintroduceanewfamilyofsurrogatelosses,thatwecalledcost-sensitivecomp-sumlosses.
Theyaredefinedasfollows: forall(r,x,y)∈R ×X ×Y:
ℓ̃comp (r,x,y)= ∑(1−c(x,k,y))ℓcomp (r,x,k).
k∈K
Forexample,whenℓcomp =ℓ log,weobtainthecost-sensitivelogisticlossasfollows:
ℓ̃ log(r,x,y)
= ∑(1−c(x,k,y))ℓ log(r,x,k)
k∈K
= ∑(1−c(x,k,y))log(∑ er(x,k′ )−r(x,k) ). (3)
k∈K k′ ∈K
Similarly,wewilluseℓ̃comp ,ℓ̃ andℓ̃ todenotethecorrespondingcost-sensitivecounterpartsfor
exp gce mae
thesum-exponentialloss,generalizedcross-entropylossandmeanabsoluteerrorloss,respectively.
Next,weshowthatthesecost-sensitivesurrogatelossfunctionsbenefitfromR-consistencybounds
withrespecttothetargetlossℓ̃.
Theorem15 AssumethatRissymmetricandcomplete. Then,thefollowingR-consistencybound
holdsforthecost-sensitivecomp-sumloss:
E ℓ̃(r)−E∗ ℓ̃(R )+M ℓ̃(R )≤γ(E ℓ̃comp(r)−E∗ ℓ̃comp(R )+M ℓ̃comp(R ));
InthespecialcasewhereR =R all,thefollowingholds:
E ℓ̃(r)−E∗ ℓ̃(R all)≤γ(E ℓ̃comp(r)−E∗ ℓ̃comp(R all)),
√ √
where γ(t) = 2 t when ℓ̃comp is either ℓ̃ log or ℓ̃c eo xm pp; γ(t) = 2 nαt when ℓ̃comp is ℓ̃ gce; and
γ(t)=ntwhenℓ̃comp isℓ̃ mae.
TheproofisincludedinAppendixE.1. ThesecondpartfollowsfromthefactthatwhenR =R all,
all the minimizability gaps vanish. In particular, Theorem 15 implies the Bayes-consistency of
cost-sensitivecomp-sumlosses. Theboundsforcost-sensitivegeneralizedcross-entropyandmean
absoluteerrorlossdependonthenumberofclasses,makingthemlessfavorablewhennislarge. As
pointedoutearlier,whilethecost-sensitivemeanabsoluteerrorlossadmitsalinearrate,itisdifficult
tooptimizeeveninthestandardclassification,asreportedbyZhangandSabuncu(2018)andMao
etal.(2023f).
Intheproof,werepresentedthecomp-sumlossasafunctionofthesoftmaxandintroduceda
softmax-dependentfunctionS toupperboundtheconditionalregretofthetargetcardinality-aware
µ
lossfunctionbythatofthecost-sensitivecomp-sumloss. Thistechniqueisnovelanddiffersfrom
theapproachusedinthestandardscenario(Section3).
11MAOMOHRIZHONG
5.3. Cost-SensitiveConstrainedLosses
Motivatedbytheformulationofconstrainedlossfunctionsinthestandardmulti-classclassification,
weintroduceanewfamilyofsurrogatelosses,termedcost-sensitiveconstrainedlosses,whichare
defined,forall(r,x,y)∈R ×X ×Y,by
ℓ̃cstnd (r,x,y)=
∑
c(x,k,y)Φ(−r(x,k)),
k∈K
withtheconstraintthat ∑y∈Yr(x,y)=0,whereΦ∶R →R
+
isnon-increasing. Forexample,when
Φ(t)=e−t,weobtainthecost-sensitiveconstrainedexponentiallossasfollows:
ℓ̃c es xt pnd (r,x,y)=
∑
c(x,k,y)er(x,k),
k∈K
withtheconstraintthat ∑y∈Yr(x,y)=0. Similarly,wewilluseℓ̃ sq−hinge,ℓ̃
hinge
andℓ̃
ρ
todenotethe
correspondingcost-sensitivecounterpartsfortheconstrainedsquaredhingeloss,constrainedhinge
lossandconstrainedρ-marginloss,respectively. Next,weshowthatthesecost-sensitivesurrogate
lossfunctionsbenefitfromR-consistencyboundswithrespecttothetargetlossℓ̃.
Theorem16 AssumethatRissymmetricandcomplete. Then,thefollowingR-consistencybound
holdsforthecost-sensitiveconstrainedloss:
E ℓ̃(r)−E∗ ℓ̃(R )+M ℓ̃(R )≤γ(E ℓ̃cstnd(r)−E∗ ℓ̃cstnd(R )+M ℓ̃cstnd(R ));
InthespecialcasewhereR =R all,thefollowingholds:
E ℓ̃(r)−E∗ ℓ̃(R all)≤γ(E ℓ̃cstnd(r)−E∗ ℓ̃cstnd(R all)),
√
whereγ(t)=2 twhenℓ̃cstnd iseitherℓ̃c es xt pnd orℓ̃ sq−hinge;γ(t)=twhenℓ̃cstnd iseitherℓ̃
hinge
orℓ̃ ρ.
TheproofisincludedinAppendixE.2. ThesecondpartfollowsfromthefactthatwhenR =R all,
all the minimizability gaps vanish. In particular, Theorem 16 implies the Bayes-consistency of
cost-sensitiveconstrainedlosses. Notethatwhiletheconstrainedhingelossandρ-marginlosshavea
morefavorablelinearrateinthebound,theiroptimizationmaybemorechallengingcomparedto
othersmoothlossfunctions.
6. Experiments
Here,wereportempiricalresultsforourcardinality-awarealgorithmandshowthatitconsistently
outperforms top-k classifiers on benchmark datasets CIFAR-10, CIFAR-100 (Krizhevsky, 2009),
SVHN(Netzeretal.,2011)andImageNet(Dengetal.,2009).
We adopted a linear model for the base model h to classify the extracted features from the
datasets. We used the outputs of the second-to-last layer of ResNet (He et al., 2016) as features
fortheCIFAR-10,CIFAR-100andSVHNdatasets. FortheImageNetdataset,weusedtheCLIP
(Radford et al., 2021) model to extract features. We used a two-hidden-layer feedforward neural
networkwithReLUactivationfunctions(NairandHinton,2010)forthecardinalityselectorr. Both
thebasemodelhandthecardinalityselectorr weretrainedusingtheAdamoptimizer(Kingmaand
Ba,2014),withalearningrateof1×10−3,abatchsizeof128,andaweightdecayof1×10−5.
12TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
0.98 0.95
0.94
0.90
0.90
0.85
0.86
0.80
0.82 top-k classifier top-k classifier
cardinality-aware algorithm 0.75 cardinality-aware algorithm
0.78
2 4 6 8 10 2 4 6 8 10
Cardinality Cardinality
CIFAR-100 ImageNet
0.98 0.98
0.96
0.96
0.94
0.94
0.92
0.90 0.92
top-k classifier top-k classifier
0.88
cardinality-aware algorithm 0.90 cardinality-aware algorithm
1.0 1.5 2.0 2.5 3.0 3.5 4.0 1.0 1.5 2.0 2.5 3.0 3.5 4.0
Cardinality Cardinality
CIFAR-10 SVHN
Figure1: Accuracyversuscardinalityonvariousdatasets.
Figure1comparestheaccuracyversuscardinalitycurveofthecardinality-awarealgorithmwith
thatoftop-k classifiers. Theaccuracyofatop-k classifierismeasuredbyE (x,y)∼S[1−ℓ k(h,x,y)],
thatisthefractionofthesampleinwhichthetop-k predictionsincludethetruelabel. Itnaturally
grows as the cardinality k increases, as shown in Figure 1. The accuracy of the carnality-aware
algorithmsismeasuredbyE (x,y)∼S[1−ℓ r(x)(h,x,y)],thatisthefractionofthesampleinwhich
thepredictionsselectedbythemodelr includethetruelabel,andthecorrespondingcardinalityis
measured by E (x,y)∼S[r(x)], that is the average size of the selected predictions. The cardinality
selector r was trained by minimizing the cost-sensitive logistic loss ℓ̃ (Eq. (3)) with the cost
log
c(x,k,y) defined as ℓ k(h,x,y)+λC (k), where λ = 0.05 and C (k) = log(k). We began with a
set K = {1} for the loss function and then progressively expanded it by adding choices of larger
cardinality, eachof whichdoubles thelargestvaluecurrently inK. In Figure1, thelargestset K
fortheCIFAR-100andImageNetdatasetsis{1,2,4,8,16,32,64},whereasfortheCIFAR-10and
SVHN datasets, it is {1,2,4,8}. As the set K expands, there is an increase in both the average
cardinalityandtheaccuracy.
Figure1showsthatthecardinality-awarealgorithmissuperioracrosstheCIFAR-100,ImageNet,
CIFAR-10andSVHNdatasets. Foragivencardinalityk,thecardinality-awarealgorithmalways
achieveshigheraccuracythanatop-kclassifier. Inotherwords,toachievethesamelevelofaccuracy,
thepredictionsmadebythecardinality-awarealgorithmcanbesignificantlysmallerinsizecompared
to those made by the corresponding top-k classifier. In particular, on the CIFAR-100, CIFAR-10
andSVHNdatasets,thecardinality-awarealgorithmachievesthesameaccuracy(98%)asthetop-k
classifierwhileusingroughlyonlyhalfofthecardinality. AswiththeImageNetdataset,itachieves
thesameaccuracy(95%)asthetop-kclassifierwithonlytwo-thirdsofthecardinality. Thisillustrates
theeffectivenessofourcardinality-awarealgorithm.
13
ycaruccA
ycaruccA
ycaruccA
ycaruccAMAOMOHRIZHONG
7. Conclusion
Wegaveaseriesofresultsdemonstratingthatseveralcommonsurrogatelossfunctions,including
comp-sumlossesandconstrainedlossesinstandardclassification,benefitfromH-consistencybounds
withrespecttothetop-k loss. Thesefindingsestablishatheoreticalandalgorithmicfoundationfor
top-k classificationwithafixedcardinalityk. Wefurtherintroducedacardinality-awareframework
fortop-k classificationthroughcost-sensitivelearning,forwhichweproposedcost-sensitivecomp-
sumlossesandconstrainedlossesthatbenefitfromH-consistencyguaranteeswithinthisframework.
Thisleadstoprincipledandpracticalcardinality-awarealgorithmsfortop-k classification,whichwe
showedempiricallytobeveryeffective. Ouranalysisandalgorithmsarelikelytobeapplicableto
othersimilarscenarios.
References
PranjalAwasthi,AnqiMao,MehryarMohri,andYutaoZhong. H-consistencyboundsforsurrogate
lossminimizers. InInternationalConferenceonMachineLearning,pages1117–1174,2022a.
PranjalAwasthi,AnqiMao,MehryarMohri,andYutaoZhong. Multi-classH-consistencybounds.
InAdvancesinneuralinformationprocessingsystems,pages782–795,2022b.
Peter L Bartlett and Marten H Wegkamp. Classification with a reject option using a hinge loss.
JournalofMachineLearningResearch,9(8),2008.
PeterL.Bartlett,MichaelI.Jordan,andJonD.McAuliffe. Convexity,classification,andriskbounds.
JournaloftheAmericanStatisticalAssociation,101(473):138–156,2006.
JosephBerkson. Applicationofthelogisticfunctiontobio-assay. JournaloftheAmericanStatistical
Association,39:357—-365,1944.
JosephBerkson. WhyIpreferlogitstoprobits. Biometrics,7(4):327—-339,1951.
LeonardBerrada,AndrewZisserman,andMPawanKumar. Smoothlossfunctionsfordeeptop-k
classification. InInternationalConferenceonLearningRepresentations,2018.
KobyCrammerandYoramSinger. Onthealgorithmicimplementationofmulticlasskernel-based
vectormachines. Journalofmachinelearningresearch,2(Dec):265–292,2001.
Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Fei-Fei Li. Imagenet: A large-scale
hierarchicalimagedatabase. In2009IEEEconferenceoncomputervisionandpatternrecognition,
pages248–255.Ieee,2009.
ChristopheDenisandMohamedHebiri. Confidencesetswithexpectedsizesformulticlassclassifi-
cation. JournalofMachineLearningResearch,18(102):1–28,2017.
AritraGhosh,HimanshuKumar,andPShantiSastry. Robustlossfunctionsunderlabelnoisefor
deepneuralnetworks. InProceedingsoftheAAAIconferenceonartificialintelligence,2017.
Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image
recognition. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,
pages770–778,2016.
14TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
DiederikPKingmaandJimmyBa. Adam: Amethodforstochasticoptimization. arXivpreprint
arXiv:1412.6980,2014.
AlexKrizhevsky. Learningmultiplelayersoffeaturesfromtinyimages. Technicalreport,Toronto
University,2009.
Vitaly Kuznetsov, Mehryar Mohri, and Umar Syed. Multi-class deep boosting. In Advances in
NeuralInformationProcessingSystems,pages2501–2509,2014.
Maksim Lapin, Matthias Hein, and Bernt Schiele. Top-k multiclass svm. In Advances in neural
informationprocessingsystems,2015.
Maksim Lapin, Matthias Hein, and Bernt Schiele. Loss functions for top-k error: Analysis and
insights. InProceedingsoftheIEEEconferenceoncomputervisionandpatternrecognition,pages
1468–1477,2016.
MaksimLapin,MatthiasHein,andBerntSchiele. Analysisandoptimizationoflossfunctionsfor
multiclass,top-k,andmultilabelclassification. IEEETransactionsonPatternAnalysis&Machine
Intelligence,40(07):1533–1554,2018.
Yoonkyung Lee, Yi Lin, and Grace Wahba. Multicategory support vector machines: Theory and
application to the classification of microarray data and satellite radiance data. Journal of the
AmericanStatisticalAssociation,99(465):67–81,2004.
PhilLongandRoccoServedio. ConsistencyversusrealizableH-consistencyformulticlassclassifica-
tion. InInternationalConferenceonMachineLearning,pages801–809,2013.
AnqiMao,ChristopherMohri,MehryarMohri,andYutaoZhong. Two-stagelearningtodeferwith
multipleexperts. InAdvancesinneuralinformationprocessingsystems,2023a.
AnqiMao,MehryarMohri,andYutaoZhong.H-consistencybounds: Characterizationandextensions.
InAdvancesinNeuralInformationProcessingSystems,2023b.
AnqiMao,MehryarMohri,andYutaoZhong. H-consistencyboundsforpairwisemisrankingloss
surrogates. InInternationalconferenceonMachinelearning,2023c.
AnqiMao,MehryarMohri,andYutaoZhong. Rankingwithabstention. InICML2023Workshop
TheManyFacetsofPreference-BasedLearning,2023d.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. Structured prediction with stronger consistency
guarantees. InAdvancesinNeuralInformationProcessingSystems,2023e.
AnqiMao,MehryarMohri,andYutaoZhong. Cross-entropylossfunctions: Theoreticalanalysis
andapplications. InInternationalConferenceonMachineLearning,2023f.
Anqi Mao, Mehryar Mohri, and Yutao Zhong. Principled approaches for learning to defer with
multipleexperts. InInternationalSymposiumonArtificialIntelligenceandMathematics,2024a.
AnqiMao,MehryarMohri,andYutaoZhong. Predictor-rejectormulti-classabstention: Theoretical
analysisandalgorithms. InAlgorithmicLearningTheory,2024b.
15MAOMOHRIZHONG
AnqiMao,MehryarMohri,andYutaoZhong. Theoreticallygroundedlossfunctionsandalgorithms
forscore-basedmulti-classabstention. InInternationalConferenceonArtificialIntelligenceand
Statistics,2024c.
Christopher Mohri, Daniel Andor, Eunsol Choi, Michael Collins, Anqi Mao, and Yutao Zhong.
Learning to reject with a fixed predictor: Application to decontextualization. In International
ConferenceonLearningRepresentations,2024.
MehryarMohri,AfshinRostamizadeh,andAmeetTalwalkar. Foundationsofmachinelearning. MIT
press,2018.
IndraneelMukherjeeandRobertESchapire. Atheoryofmulticlassboosting. JournalofMachine
LearningResearch,2013.
VinodNairandGeoffreyEHinton. Rectifiedlinearunitsimproverestrictedboltzmannmachines. In
Proceedingsofthe27thinternationalconferenceonmachinelearning(ICML-10),pages807–814,
2010.
YuvalNetzer,TaoWang,AdamCoates,AlessandroBissacco,BoWu,andAndrewYNg. Reading
digitsinnaturalimageswithunsupervisedfeaturelearning. InAdvancesinNeuralInformation
ProcessingSystems,2011.
Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal,
GirishSastry,AmandaAskell,PamelaMishkin,JackClark,etal. Learningtransferablevisual
models from natural language supervision. In International conference on machine learning,
pages8748–8763.PMLR,2021.
SashankJReddi, SatyenKale, FelixYu, DanielHoltmann-Rice, JiecaoChen, andSanjivKumar.
Stochastic negative mining for learning with large output spaces. In The 22nd International
ConferenceonArtificialIntelligenceandStatistics,pages1940–1949,2019.
MohammadSaberianandNunoVasconcelos. Multiclassboosting: Theoryandalgorithms. Advances
inneuralinformationprocessingsystems,24,2011.
IngoSteinwart. Howtocomparedifferentlossfunctionsandtheirrisks. ConstructiveApproximation,
26(2):225–287,2007.
AnishThilagar,RafaelFrongillo,JessicaJFinocchiaro,andEmmaGoodwill. Consistentpolyhedral
surrogatesfortop-kclassificationandvariants. InInternationalConferenceonMachineLearning,
pages21329–21359,2022.
Nicolas Usunier, David Buffoni, and Patrick Gallinari. Ranking with ordered weighted pairwise
classification. InInternationalconferenceonmachinelearning,pages1057–1064,2009.
PierreFrançoisVerhulst. Noticesurlaloiquelapopulationsuitdanssonaccroissement. Correspon-
dancemathématiqueetphysique,10:113—-121,1838.
Pierre François Verhulst. Recherches mathématiques sur la loi d’accroissement de la population.
NouveauxMémoiresdel’AcadémieRoyaledesSciencesetBelles-LettresdeBruxelles,18:1—-42,
1845.
16TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
JasonWestonandChrisWatkins. Multi-classsupportvectormachines. Technicalreport,Citeseer,
1998.
Forest Yang and Sanmi Koyejo. On the consistency of top-k surrogate losses. In International
ConferenceonMachineLearning,pages10727–10735,2020.
Tong Zhang. Statistical behavior and consistency of classification methods based on convex risk
minimization. TheAnnalsofStatistics,32(1):56–85,2004a.
TongZhang. Statisticalanalysisofsomemulti-categorylargemarginclassificationmethods. Journal
ofMachineLearningResearch,5(Oct):1225–1251,2004b.
ZhiluZhangandMertSabuncu. Generalizedcrossentropylossfortrainingdeepneuralnetworks
withnoisylabels. InAdvancesinneuralinformationprocessingsystems,2018.
ChenyuZheng,GuoqiangWu,FanBao,YueCao,ChongxuanLi,andJunZhu. Revisitingdiscrimi-
nativevs.generativeclassifiers: Theoryandimplications. InInternationalConferenceonMachine
Learning,2023.
17MAOMOHRIZHONG
ContentsofAppendix
A ProofofLemma4 19
B ProofsofH-consistencyboundsforcomp-sumlosses 19
B.1 ProofofTheorem5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19
B.2 ProofofTheorem6 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
B.3 ProofofTheorem7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22
B.4 ProofofTheorem8 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
C ProofsofrealizableH-consistencyforcomp-sumlosses 26
D ProofsofH-consistencyboundsforconstrainedlosses 26
D.1 ProofofTheorem13 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26
D.2 ProofofTheorem14 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
D.3 ProofofTheorem18 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
D.4 ProofofTheorem19 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
E ProofsofR-consistencyboundsforcost-sensitivelosses 31
E.1 ProofofTheorem15 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32
E.2 ProofofTheorem16 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36
18TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
AppendixA. ProofofLemma4
Lemma17 AssumethatHisregular. Then,foranyh∈Handx∈X,thebest-inclassconditional
errorandtheconditionalregretofthetop-k losscanbeexpressedasfollows:
k
C∗
ℓ
(H,x)=1−∑p(x,p i(x))
k
i=1
k
∆C
ℓ
,H(h,x)=∑(p(x,p i(x))−p(x,h i(x))).
k
i=1
ProofBydefinition,foranyh∈Handx∈X,theconditionalerroroftop-k losscanbewrittenas
k
C
ℓ
k(h,x)= ∑p(x,y)1
y∉{h1(x),...,h k(x)}
=1−∑p(x,h i(x)).
y∈Y i=1
Bydefinitionofthelabelsp i(x),whicharethemostlikelytop-k labels,C
ℓ
(h,x)isminimizedfor
k
h i(x)=k min(x),i∈[k]. SinceHisregular,thischoiceisrealizableforsomeh∈H. Thus,wehave
k
C∗
ℓ
k(H,x)= hin ∈Hf C
ℓ
k(h,x)=1−
i∑
=1p(x,p i(x)).
Furthermore,thecalibrationgapcanbeexpressedas
k
∆C
ℓ
k,H(h,x)=C
ℓ
k(h,x)−C∗
ℓ
k(H,x)=∑(p(x,p i(x))−p(x,h i(x))),
i=1
whichcompletestheproof.
AppendixB. ProofsofH-consistencyboundsforcomp-sumlosses
B.1. ProofofTheorem5
Theorem5 Assume that H is symmetric and complete. Then, for any 1 ≤ k ≤ n, the following
H-consistencyboundholdsforthelogisticloss:
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤kψ−1 (E
ℓ
log(h)−E∗
ℓ
log(H )+M
ℓ
log(H )),
whereψ(t)= 1 2−tlog(1−t)+ 1 2+tlog(1+t),t∈[0,1]. InthespecialcasewhereA
ℓ
log(H )=0,for
any1≤k ≤n,thefollowingupperboundholds:
E
ℓ
k(h)−E∗
ℓ
k(H )≤kψ−1 (E
ℓ
log(h)−E∗
ℓ
log(H )).
ProofForlogisticlossℓ ,theconditionalregretcanbewrittenas
log
n n
∆C
ℓ
,H(h,x)= ∑p(x,y)ℓ log(h,x,y)− inf ∑p(x,y)ℓ log(h,x,y)
log
y=1
h∈H
y=1
n n
≥ ∑p(x,y)ℓ log(h,x,y)−inf ∑p(x,y)ℓ log(h µ,i,x,y),
y=1
µ∈R
y=1
19MAOMOHRIZHONG
where for any i ∈ [k], h µ,i(x,y) =
⎧ ⎪⎪⎪⎪ ⎨h lo( gx (, ey h) (,
x,pi(x))+µ)
y
y
∉ ={ hp i(i( xx )),h i(x)}
Note that such a
⎪⎪⎪⎪
⎩log(eh(x,hi(x))−µ) y =p i(x).
choiceofh leadstothefollowingequalityholds:
µ,i
∑
p(x,y)ℓ log(h,x,y)=
∑
p(x,y)ℓ log(h µ,i,x,y).
y∉{hi(x),pi(x)} y∉{hi(x),pi(x)}
Therefore,foranyi∈[k],theconditionalregretoflogisticlosscanbelowerboundedas
eh(x,hi(x)) eh(x,pi(x))
∆C ℓ ,H(h,x)≥−p(x,h i(x))log( )−p(x,p i(x))log( )
log ∑y∈Yeh(x,y) ∑y∈Yeh(x,y)
eh(x,pi(x))+µ eh(x,hi(x))−µ
+sup(p(x,h i(x))log( )+p(x,p i(x))log( ))
µ∈R ∑y∈Yeh(x,y) ∑y∈Yeh(x,y)
eh(x,pi(x))+µ eh(x,hi(x))−µ
=sup(p(x,h i(x))log( )+p(x,p i(x))log( )).
µ∈R eh(x,hi(x)) eh(x,pi(x))
By the concavity of the function, differentiate with respect to µ, we obtain that the supremum is
achievedbyµ∗ =
p(x,hi(x))eh(x,hi(x)) −p(x,pi(x))eh(x,pi(x))
. Pluginµ∗,weobtain
p(x,hi(x))+p(x,pi(x))
∆C
ℓ
,H(h,x)
log
p(x,h i(x)) eh(x,hi(x))+eh(x,pi(x))
≥p(x,h i(x))log( )
p(x,h i(x))+p(x,p i(x)) eh(x,hi(x))
p(x,p i(x)) eh(x,hi(x))+eh(x,pi(x))
+p(x,p i(x))log( )
p(x,h i(x))+p(x,p i(x)) eh(x,pi(x))
2p(x,h i(x)) 2p(x,p i(x))
≥p(x,h i(x))log( )+p(x,p i(x))log( ).
p(x,h i(x))+p(x,p i(x)) p(x,h i(x))+p(x,p i(x))
(minimumisachievedwhenh(x,h i(x))=h(x,p i(x)))
letS
i
=p(x,p i(x))+p(x,h i(x))and∆
i
=p(x,p i(x))−p(x,h i(x)),wehave
∆C ℓ ,H(h,x)≥
S i−∆
i
log(S i−∆
i )+
S i+∆
i
log(S i+∆
i )
log 2 S 2 S
i i
1−∆
i
1+∆
i
≥ log(1−∆ i)+ log(1+∆ i) (minimumisachievedwhenS i =1)
2 2
=ψ(p(x,p i(x))−p(x,h i(x))),
whereψ(t)= 1−tlog(1−t)+ 1+tlog(1+t),t∈[0,1]. Therefore,theconditionalregretofthetop-k
2 2
losscanbeupperboundedasfollows:
k
∆C
ℓ
,H(h,x)=∑(p(x,p i(x))−p(x,h i(x)))≤kψ−1 (∆C
ℓ
,H(h,x)).
k log
i=1
Bytheconcavityofψ−1,takeexpectationsonbothsidesoftheprecedingequation,weobtain
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤kψ−1 (E
ℓ
log(h)−E∗
ℓ
log(H )+M
ℓ
log(H )).
20TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
The second part follows from the fact that when A ℓ (H ) = 0, the minimizability gap M ℓ (H )
log log
vanishes.
B.2. ProofofTheorem6
Theorem6 Assume that H is symmetric and complete. Then, for any 1 ≤ k ≤ n, the following
H-consistencyboundholdsforthesumexponentialloss:
E ℓ k(h)−E∗ ℓ k(H )+M ℓ k(H )≤kψ−1 (E ℓc eo xm pp(h)−E∗ ℓc eo xm pp(H )+M ℓc eo xm pp(H )),
√
whereψ(t)=1− 1−t2,t∈[0,1]. InthespecialcasewhereA ℓcomp(H )=0,forany1≤k ≤n,
exp
thefollowingboundholds:
E ℓ k(h)−E∗ ℓ k(H )≤kψ−1 (E ℓc eo xm pp(h)−E∗ ℓc eo xm pp(H )),
ProofForsumexponentiallossℓcomp
,theconditionalregretcanbewrittenas
exp
n n
∆C ℓcomp,H(h,x)= ∑p(x,y)ℓc eo xm pp (h,x,y)− inf ∑p(x,y)ℓc eo xm pp (h,x,y)
exp
y=1
h∈H
y=1
n n
≥ ∑p(x,y)ℓc eo xm pp (h,x,y)−inf ∑p(x,y)ℓc eo xm pp (h µ,i,x,y),
y=1
µ∈R
y=1
where for any i ∈ [k], h µ,i(x,y) =
⎧ ⎪⎪⎪⎪ ⎨h lo( gx (, ey h) (,
x,pi(x))+µ)
y
y
∉ ={ hp i(i( xx )),h i(x)}
Note that such a
⎪⎪⎪⎪
⎩log(eh(x,hi(x))−µ) y =p i(x).
choiceofh leadstothefollowingequalityholds:
µ,i
∑
p(x,y)ℓc eo xm pp (h,x,y)=
∑
p(x,y)ℓc eo xm pp (h µ,i,x,y).
y∉{hi(x),pi(x)} y∉{hi(x),pi(x)}
Therefore,foranyi∈[k],theconditionalregretofsumexponentiallosscanbelowerboundedas
∆C ℓc eo xm pp,H(h,x)≥ y∑
′
∈Yexp(h(x,y′ ))[ expp (( hx (, xh ,i h(x i() x)
)))
+ expp (( hx (, xp ,i p(x i() x) )))]
⎛ p(x,h i(x)) p(x,p i(x)) ⎞
+sup − ∑ exp(h(x,y′ ))[ + ] .
µ∈R⎝ y′ ∈Y exp(h(x,p i(x)))+µ exp(h(x,h i(x)))−µ ⎠
21MAOMOHRIZHONG
By the concavity of the function, differentiate with respect to µ, we obtain that the supremum is
√ √
achievedbyµ∗ =
exp[h(x,hi(x))] √p(x,hi(x))−e √xp[h(x,pi(x))] p(x,pi(x)).
Pluginµ∗,weobtain
p(x,hi(x))+ p(x,pi(x))
∆C ℓcomp,H(h,x)
exp
⎡ √ √ 2 ⎤
≥ ∑ exp(h(x,y′
))⎢
⎢ ⎢
p(x,h i(x))
+
p(x,p i(x))
−
( p(x,h i(x))+ p(x,p i(x))) ⎥
⎥ ⎥
y′ ∈Y
⎢ ⎢exp(h(x,h i(x))) exp(h(x,p i(x))) exp(h(x,p i(x)))+exp(h(x,h i(x)))⎥
⎥
⎣ ⎦
exp(h(x,p i(x))) exp(h(x,h i(x))) √ √ 2
≥[1+ ]p(x,h i(x))+[1+ ]p(x,p i(x))−( p(x,h i(x))+ p(x,p i(x)))
exp(h(x,h i(x))) exp(h(x,p i(x)))
(
∑y′
∈Yexp(h(x,y′))≥exp(h(x,p i(x)))+exp(h(x,h i(x))))
√ √ 2
≥2p(x,h i(x))+2p(x,p i(x))−( p(x,h i(x))+ p(x,p i(x))) .
(minimumisattainedwhen
exp(h(x,pi(x)))
=1)
exp(h(x,hi(x)))
letS
i
=p(x,p i(x))+p(x,h i(x))and∆
i
=p(x,p i(x))−p(x,h i(x)),wehave
√ √ 2
∆C ℓc eo xm pp,H(h,x)≥2S i−⎛
⎝
S i+ 2∆ i + S i− 2∆ i⎞
⎠
≥2⎡ ⎢ ⎢ ⎢ ⎢ ⎢ ⎣1−⎡ ⎢ ⎢ ⎢ ⎢ ⎣(1+∆ i)1 2 + 2(1−∆ i)1 2⎤ ⎥ ⎥ ⎥ ⎥ ⎦2⎤ ⎥ ⎥ ⎥ ⎥ ⎥
⎦
(minimumisachievedwhenS i =1)
√
=1− 1−(∆ i)2
=ψ(p(x,p i(x))−p(x,h i(x))),
√
whereψ(t)=1− 1−t2,t∈[0,1]. Therefore,theconditionalregretofthetop-k losscanbeupper
boundedasfollows:
k
∆C ℓ k,H(h,x)=∑(p(x,p i(x))−p(x,h i(x)))≤kψ−1 (∆C ℓc eo xm pp,H(h,x)).
i=1
Bytheconcavityofψ−1,takeexpectationsonbothsidesoftheprecedingequation,weobtain
E ℓ k(h)−E∗ ℓ k(H )+M ℓ k(H )≤kψ−1 (E ℓc eo xm pp(h)−E∗ ℓc eo xm pp(H )+M ℓc eo xm pp(H )).
ThesecondpartfollowsfromthefactthatwhenA ℓcomp(H )=0,theminimizabilitygapM ℓcomp(H
)
exp exp
vanishes.
B.3. ProofofTheorem7
Theorem7 Assume that H is symmetric and complete. Then, for any 1 ≤ k ≤ n, the following
H-consistencyboundholdsforthemeanabsoluteerrorloss:
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤kn(E ℓmae(h)−E∗ ℓmae(H )+M ℓmae(H )).
InthespecialcasewhereA mae(H )=0,forany1≤k ≤n,thefollowingboundholds:
E
ℓ
k(h)−E∗
ℓ
k(H )≤kn(E ℓmae(h)−E∗ ℓmae(H )).
22TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
ProofFormeanabsoluteerrorlossℓ ,theconditionalregretcanbewrittenas
mae
n n
∆C ℓmae,H(h,x)=
y∑
=1p(x,y)ℓ mae(h,x,y)− hin ∈Hf
y∑
=1p(x,y)ℓ mae(h,x,y)
n n
≥ ∑p(x,y)ℓ mae(h,x,y)−inf ∑p(x,y)ℓ mae(h µ,i,x,y),
y=1
µ∈R
y=1
where for any i ∈ [k], h µ,i(x,y) =
⎧ ⎪⎪⎪⎪ ⎨h lo( gx (, ey h) (,
x,pi(x))+µ)
y
y
∉ ={ hp i(i( xx )),h i(x)}
Note that such a
⎪⎪⎪⎪
⎩log(eh(x,hi(x))−µ) y =p i(x).
choiceofh leadstothefollowingequalityholds:
µ,i
∑
p(x,y)ℓ mae(h,x,y)=
∑
p(x,y)ℓ mae(h µ,i,x,y).
y∉{hi(x),pi(x)} y∉{hi(x),pi(x)}
Therefore,foranyi∈[k],theconditionalregretofmeanabsoluteerrorlosscanbelowerboundedas
∆C ℓmae,H(h,x)
exp(h(x,h i(x))) exp(h(x,p i(x)))
≥p(x,h i(x))(1− )+p(x,p i(x))(1− )
∑y′
∈Yexp(h(x,y′))
∑y′
∈Yexp(h(x,y′))
exp(h(x,h i(x)))−µ exp(h(x,p i(x)))+µ
+sup(−p(x,p i(x))(1− )−p(x,h i(x))(1− )).
µ∈R ∑y′ ∈Yexp(h(x,y′)) ∑y′ ∈Yexp(h(x,y′))
By the concavity of the function, differentiate with respect to µ, we obtain that the supremum is
achievedbyµ∗ =−exp[h(x,p i(x)]. Pluginµ∗,weobtain
∆C ℓmae,H(h,x)
exp(h(x,h i(x))) exp(h(x,h i(x)))
≥p(x,p i(x)) −p(x,h i(x))
∑y′
∈Yexp(h(x,y′))
∑y′
∈Yexp(h(x,y′))
1
≥ n(p(x,p i(x))−p(x,h i(x))) ( ∑e yx ′p ∈Y(h ex(x p, (h hi (( xx ,) y) ′)
))
≥ n1)
Therefore,theconditionalregretofthetop-k losscanbeupperboundedasfollows:
k
∆C
ℓ
k,H(h,x)=∑(p(x,p i(x))−p(x,h i(x)))≤kn(∆C ℓmae,H(h,x)).
i=1
Takeexpectationsonbothsidesoftheprecedingequation,weobtain
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤kn(E ℓmae(h)−E∗ ℓmae(H )+M ℓmae(H )).
ThesecondpartfollowsfromthefactthatwhenA ℓmae(H )=0,theminimizabilitygapM ℓmae(H
)
vanishes.
23MAOMOHRIZHONG
B.4. ProofofTheorem8
Theorem8 Assume that H is symmetric and complete. Then, for any 1 ≤ k ≤ n, the following
H-consistencyboundholdsforthegeneralizedcross-entropy:
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤kψ−1 (E ℓgce(h)−E∗ ℓgce(H )+M ℓgce(H )),
1 1 1−α
where ψ(t) = αn1 α[[(1+t)1−α+ 2(1−t)1−α ] −1], for all α ∈ (0,1), t ∈ [0,1]. In the special case
whereA ℓgce(H )=0,forany1≤k ≤n,thefollowingupperboundholds:
E
ℓ
k(h)−E∗
ℓ
k(H )≤kψ−1 (E ℓgce(h)−E∗ ℓgce(H )),
ProofForgeneralizedcross-entropylossℓ ,theconditionalregretcanbewrittenas
gce
∆C ℓgce,H(h,x)
n n
= ∑p(x,y)ℓ gce(h,x,y)− inf ∑p(x,y)ℓ gce(h,x,y)
y=1
h∈H
y=1
n n
≥ ∑p(x,y)ℓ gce(h,x,y)−inf ∑p(x,y)ℓ gce(h µ,i,x,y),
y=1
µ∈R
y=1
where for any i ∈ [k], h µ,i(x,y) =
⎧ ⎪⎪⎪⎪ ⎨h lo( gx (, ey h) (,
x,pi(x))+µ)
y
y
∉ ={ hp i(i( xx )),h i(x)}
Note that such a
⎪⎪⎪⎪
⎩log(eh(x,hi(x))−µ) y =p i(x).
choiceofh leadstothefollowingequalityholds:
µ,i
∑
p(x,y)ℓ gce(h,x,y)=
∑
p(x,y)ℓ gce(h µ,i,x,y).
y∉{hi(x),pi(x)} y∉{hi(x),pi(x)}
Therefore, for any i ∈ [k], the conditional regret of generalized cross-entropy loss can be lower
boundedas
α∆C ℓgce,H(h,x)
α α
exp(h(x,h i(x))) exp(h(x,p i(x)))
≥p(x,h i(x))(1−[ ] )+p(x,p i(x))(1−[ ] )
∑y′
∈Yexp(h(x,y′))
∑y′
∈Yexp(h(x,y′))
α α
exp(h(x,p i(x)))+µ exp(h(x,h i(x)))−µ
+sup(−p(x,h i(x))(1−[ ] )−p(x,p i(x))(1−[ ] )).
µ∈R ∑y′ ∈Yexp(h(x,y′)) ∑y′ ∈Yexp(h(x,y′))
24TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
By the concavity of the function, differentiate with respect to µ, we obtain that the supremum is
1 1
achievedbyµ∗ =
exp[h(x,hi(x))]p(x,pi(x))α 1−1−exp[h(x,pi(x 1))]p(x,hi(x))α−1.
Pluginµ∗,weobtain
p(x,hi(x))α−1+p(x,pi(x))α−1
α∆C ℓgce,H(h,x)
α
⎡ ⎤
≥p(x,h i(x))⎢ ⎢ ⎢
⎢
⎢
⎣[ ∑ex y′p ∈Y(h ex(x p, (h hi (( xx ,) y) ′) ))+ [e px (xp( ,hh i( (x x, )p )i α(
1
−x 1)) +) p]p (x(x ,p,p i(i x(x ))))
α1
−α 11 −1 ]⎥ ⎥ ⎥
⎥
⎥
⎦
−p(x,h i(x))[ ∑e yx ′p ∈Y( eh x( px (, hh (i( xx ,) y) ′) ))]α
α
⎡ ⎤
+p(x,p i(x))⎢ ⎢ ⎢
⎢
⎢
⎣[ ∑ex y′p ∈Y(h ex(x p, (h hi (( xx ,) y) ′) ))+ [e px (xp( ,hh i( (x x, )p )i α(
1
−x 1)) +) p]p (x(x ,p,h i(i x(x ))))
α1
−α 11 −1 ]⎥ ⎥ ⎥
⎥
⎥
⎦
−p(x,p i(x))[ ∑e yx ′p ∈Y( eh x( px (, hp (i( xx ,) y) ′) ))]α
α
⎡ 1 ⎤
1 ⎛ ⎢ 2p(x,p i(x))α−1 ⎥ ⎞
≥ nα ⎝p(x,h i(x))⎢ ⎢
⎢ ⎣p(x,h
i(x))α1
−1 +p(x,p
i(x))α1 −1⎥ ⎥
⎥
⎦
−p(x,h i(x))
⎠
α
⎡ 1 ⎤
1 ⎛ ⎢ 2p(x,h i(x))α−1 ⎥ ⎞
+ nα ⎝p(x,p i(x))⎢ ⎢
⎢ ⎣p(x,h
i(x))α1
−1 +p(x,p
i(x))α1 −1⎥ ⎥
⎥
⎦
−p(x,p i(x))
⎠
α
(( ∑e yx ′p ∈Y(h ex(x p, (p hi (( xx ,) y) ′) ))) ≥ n1 α andminimumisattainedwhen e ex xp p( (h h( (x x, ,p hi i( (x x) )) )) ) =1)
letS
i
=p(x,p i(x))+p(x,h i(x))and∆
i
=p(x,p i(x))−p(x,h i(x)),wehave
⎡ 1 1 ⎤1−α
∆C ℓgce,H(h,x)≥ αn1 α⎛ ⎜⎢ ⎢ ⎢(S i+∆ i)1−α + 2(S i−∆ i)1−α⎥ ⎥ ⎥ −S i⎞ ⎟
⎝⎢ ⎥ ⎠
⎣ ⎦
⎡ 1 1 ⎤1−α
1 ⎛⎢(1+∆ i)1−α +(1−∆ i)1−α⎥ ⎞
≥ αnα⎜⎢ ⎢ 2 ⎥ ⎥ −1⎟
⎝⎢ ⎥ ⎠
⎣ ⎦
(minimumisachievedwhenS
i
=1)
=ψ(p(x,p i(x))−p(x,h i(x))),
1 1 1−α
whereψ(t)= αn1 α[[(1+t)1−α+ 2(1−t)1−α ] −1],t∈[0,1]. Therefore,theconditionalregretofthe
top-k losscanbeupperboundedasfollows:
k
∆C
ℓ
k,H(h,x)=∑(p(x,p i(x))−p(x,h i(x)))≤kψ−1 (∆C ℓgce,H(h,x)).
i=1
Bytheconcavityofψ−1,takeexpectationsonbothsidesoftheprecedingequation,weobtain
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤kψ−1 (E ℓgce(h)−E∗ ℓgce(H )+M ℓgce(H )).
ThesecondpartfollowsfromthefactthatwhenA ℓgce(H
) =
0,theminimizabilitygapM ℓgce(H
)
vanishes.
25MAOMOHRIZHONG
AppendixC. ProofsofrealizableH-consistencyforcomp-sumlosses
Theorem12 AssumethatHisclosedunderscaling. Then,ℓ ,ℓcomp,ℓ andℓ arerealizable
log exp gce mae
H-consistentwithrespecttoℓ .
0−1
ProofSincethedistributionisrealizable,thereexistsahypothesish∈Hsuchthat
P (x,y)∼D(h∗ (x,y)>h∗ (x,h 2(x)))=1.
Therefore,forthelogisticloss,byusingtheLebesguedominatedconvergencetheorem,
M ℓ log(H )≤E∗ ℓ log(H )≤ βl →im +∞E ℓ log(βh)= βl →im +∞log[1+ y∑
′
≠yeβ(h∗ (x,y′ )−h∗ (x,y)) ]=0.
Forthesumexponentialloss,byusingtheLebesguedominatedconvergencetheorem,
M ℓc eo xm pp(H )≤E∗ ℓc eo xm pp(H )≤ βl →im +∞E ℓc eo xm pp(βh)= βl →im +∞y∑
′
≠yeβ(h∗ (x,y′ )−h∗ (x,y)) =0.
Forthegeneralizedcrossentropyloss,byusingtheLebesguedominatedconvergencetheorem,
⎡ ⎡
⎤−α
⎤
M ℓgce(H )≤E∗ ℓgce(H )≤ βl →im +∞E ℓgce(βh)= βl →im +∞α1⎢ ⎢ ⎢
⎢
⎣1−⎢ ⎢ ⎢
⎢
⎣y∑
′
∈Yeβ(h∗ (x,y′ )−h∗ (x,y))⎥ ⎥ ⎥
⎥
⎦
⎥ ⎥ ⎥
⎥
⎦=0.
Forthemeanabsoluteerrorloss,byusingtheLebesguedominatedconvergencetheorem,
⎡
⎤−1
M ℓmae(H )≤E∗ ℓmae(H )≤ βl →im +∞E ℓmae(βh)= βl →im +∞1−⎢ ⎢ ⎢
⎢
⎣y∑
′
∈Yeβ(h∗ (x,y′ )−h∗ (x,y))⎥ ⎥ ⎥
⎥
⎦
=0.
Therefore,byTheorems5,6,7and8,theproofiscompleted.
AppendixD. ProofsofH-consistencyboundsforconstrainedlosses
D.1. ProofofTheorem13
Theconditionalerrorfortheconstrainedlosscanbeexpressedasfollows:
n n
C ℓcstnd(h,x)= ∑p(x,y)ℓcstnd (h,x,y)= ∑p(x,y) ∑ Φ(−h(x,y′ ))= ∑(1−p(x,y))Φ(−h(x,y)).
y=1 y=1 y′ ≠y y∈Y
Theorem13 Assume that H is symmetric and complete. Then, for any 1 ≤ k ≤ n, the following
H-consistencyboundholdsfortheconstrainedexponentialloss:
1
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤2k(E
ℓc es xt
pnd(h)−E∗
ℓc es xt
pnd(H )+M
ℓc es xt
pnd(H ))2.
InthespecialcasewhereA ℓcstnd(H )=0,forany1≤k ≤n,thefollowingboundholds:
exp
1
E
ℓ
k(h)−E∗
ℓ
k(H )≤2k(E
ℓc es xt
pnd(h)−E∗
ℓc es xt
pnd(H ))2.
26TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
ProofFortheconstrainedexponentiallossℓcstnd,theconditionalregretcanbewrittenas
exp
n n
∆C ℓcstnd,H(h,x)= ∑p(x,y)ℓc es xt pnd (h,x,y)− inf ∑p(x,y)ℓc es xt pnd (h,x,y)
exp
y=1
h∈H
y=1
n n
≥ ∑p(x,y)ℓc es xt pnd (h,x,y)−inf ∑p(x,y)ℓc es xt pnd (h µ,i,x,y),
y=1
µ∈R
y=1
⎧ ⎪⎪⎪⎪h(x,y), y ∉{p i(x),h i(x)}
whereforanyi∈[k],h µ,i(x,y)=⎨h(x,p i(x))+µ y =h i(x) Notethatsuchachoice
⎪⎪⎪⎪
⎩h(x,h i(x))−µ y =p i(x).
ofh leadstothefollowingequalityholds:
µ,i
∑
p(x,y)ℓc es xt pnd (h,x,y)=
∑
p(x,y)ℓc es xt pnd (h µ,i,x,y).
y∉{hi(x),pi(x)} y∉{hi(x),pi(x)}
Letq(x,p i(x))=1−p(x,p i(x))andq(x,h i(x))=1−p(x,h i(x)). Therefore,foranyi∈[k],the
conditionalregretofconstrainedexponentiallosscanbelowerboundedas
∆C ℓcstnd,H(h,x)
exp
≥ inf sup{q(x,p i(x))(eh(x,pi(x)) −eh(x,hi(x))−µ )+q(x,h i(x))(eh(x,hi(x)) −eh(x,pi(x))+µ )}
h∈H µ∈R
√ √ 2
=( q(x,p i(x))− q(x,h i(x))) (differentiatingwithrespecttoµ,htooptimize)
2
⎛ q(x,h i(x))−q(x,p i(x)) ⎞
= √ √
⎝ q(x,p i(x))+ q(x,h i(x))⎠
1
≥ (q(x,h i(x))−q(x,p i(x)))2 (0≤q(x,y)≤1)
4
1
= (p(x,p i(x))−p(x,h i(x)))2.
4
Therefore,byLemma4,theconditionalregretofthetop-k losscanbeupperboundedasfollows:
k 1
∆C
ℓ
k,H(h,x)=∑(p(x,p i(x))−p(x,h i(x)))≤2k(∆C
ℓc es xt
pnd,H(h,x))2.
i=1
Bytheconcavity,takeexpectationsonbothsidesoftheprecedingequation,weobtain
1
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤2k(E
ℓc es xt
pnd(h)−E∗
ℓc es xt
pnd(H )+M
ℓc es xt
pnd(H ))2.
ThesecondpartfollowsfromthefactthatwhenA ℓcstnd(H )=0,wehaveM ℓcstnd(H )=0.
exp exp
27MAOMOHRIZHONG
D.2. ProofofTheorem14
Theorem14 Assume that H is symmetric and complete. Then, for any 1 ≤ k ≤ n, the following
H-consistencyboundholdsfortheconstrainedsquaredhingeloss:
1
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤2k(E
ℓ
sq−hinge(h)−E∗
ℓ
sq−hinge(H )+M
ℓ
sq−hinge(H ))2.
InthespecialcasewhereA
ℓ
sq−hinge(H )=0,forany1≤k ≤n,thefollowingboundholds:
1
E
ℓ
k(h)−E∗
ℓ
k(H )≤2k(E
ℓ
sq−hinge(h)−E∗
ℓ
sq−hinge(H ))2.
ProofFortheconstrainedsquaredhingelossℓ ,theconditionalregretcanbewrittenas
sq−hinge
n n
∆C
ℓ
sq−hinge,H(h,x)=
y∑
=1p(x,y)ℓ sq−hinge(h,x,y)− hin ∈Hf
y∑
=1p(x,y)ℓ sq−hinge(h,x,y)
n n
≥ y∑ =1p(x,y)ℓ sq−hinge(h,x,y)− µin ∈Rf y∑ =1p(x,y)ℓ sq−hinge(h µ,i,x,y),
⎧ ⎪⎪⎪⎪h(x,y), y ∉{p i(x),h i(x)}
whereforanyi∈[k],h µ,i(x,y)=⎨h(x,p i(x))+µ y =h i(x) Notethatsuchachoice
⎪⎪⎪⎪
⎩h(x,h i(x))−µ y =p i(x).
ofh leadstothefollowingequalityholds:
µ,i
∑
p(x,y)ℓ sq−hinge(h,x,y)=
∑
p(x,y)ℓ sq−hinge(h µ,i,x,y).
y∉{hi(x),pi(x)} y∉{hi(x),pi(x)}
Letq(x,p i(x))=1−p(x,p i(x))andq(x,h i(x))=1−p(x,h i(x)). Therefore,foranyi∈[k],the
conditionalregretoftheconstrainedsquaredhingelosscanbelowerboundedas
∆C ℓ sq−hinge,H(h,x)≥ hin ∈Hf s µu ∈Rp{q(x,p i(x))(max{0,1+h(x,p i(x))}2 −max{0,1+h(x,h i(x))−µ}2 )
+q(x,h i(x))(max{0,1+h(x,h i(x))}2 −max{0,1+h(x,p i(x))+µ}2 )}
1
≥ (q(x,p i(x))−q(x,h i(x)))2
4
(differentiatingwithrespecttoµ,htooptimize)
1
= (p(x,p i(x))−p(x,h i(x)))2
4
Therefore,byLemma4,theconditionalregretofthetop-k losscanbeupperboundedasfollows:
k 1
∆C
ℓ
k,H(h,x)=∑(p(x,p i(x))−p(x,h i(x)))≤2k(∆C
ℓ
sq−hinge,H(h,x))2.
i=1
Bytheconcavity,takeexpectationsonbothsidesoftheprecedingequation,weobtain
1
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤2k(E
ℓ
sq−hinge(h)−E∗
ℓ
sq−hinge(H )+M
ℓ
sq−hinge(H ))2.
ThesecondpartfollowsfromthefactthatwhenthehypothesissetHissufficientlyrichsuchthat
A
ℓ
sq−hinge(H )=0,wehaveM
ℓ
sq−hinge(H )=0.
28TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
D.3. ProofofTheorem18
Similarly,westudytheconstrainedhingeloss,definedasℓ hinge(h,x,y)=∑y′ ≠ymax{0,1+h(x,y′)}.
Thefollowingresultshowsthatℓ admitsanH-consistencyboundwithrespecttoℓ . Thesecond
hinge k
partfollowsfromthefactthatwhenthehypothesissetHissufficientlyrichsuchthatA
ℓ
(H )=0,
hinge
we have M ℓ (H ) = 0. Different from the constrained squared hinge loss, the bound for ℓ hinge
hinge
is linear: E ℓ hinge(h)−E∗ ℓ hinge(H ) ≤ ϵ ⇒ E ℓ k(h)−E∗ ℓ k(H ) ≤ kϵ. This also implies that ℓ hinge is
Bayes-consistentwithrespecttoℓ .
k
Theorem18 Assume that H is symmetric and complete. Then, for any 1 ≤ k ≤ n, the following
H-consistencyboundholdsfortheconstrainedhingeloss:
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤k(E
ℓ
hinge(h)−E∗
ℓ
hinge(H )+M
ℓ
hinge(H )).
InthespecialcasewhereA
ℓ
(H )=0,forany1≤k ≤n,thefollowingboundholds:
hinge
E
ℓ
k(h)−E∗
ℓ
k(H )≤k(E
ℓ
hinge(h)−E∗
ℓ
hinge(H )).
ProofFortheconstrainedhingelossℓ ,theconditionalregretcanbewrittenas
hinge
n n
∆C
ℓ
,H(h,x)= ∑p(x,y)ℓ hinge(h,x,y)− inf ∑p(x,y)ℓ hinge(h,x,y)
hinge
y=1
h∈H
y=1
n n
≥ ∑p(x,y)ℓ hinge(h,x,y)−inf ∑p(x,y)ℓ hinge(h µ,i,x,y),
y=1
µ∈R
y=1
⎧ ⎪⎪⎪⎪h(x,y), y ∉{p i(x),h i(x)}
whereforanyi∈[k],h µ,i(x,y)=⎨h(x,p i(x))+µ y =h i(x) Notethatsuchachoice
⎪⎪⎪⎪
⎩h(x,h i(x))−µ y =p i(x).
ofh leadstothefollowingequalityholds:
µ,i
∑
p(x,y)ℓ hinge(h,x,y)=
∑
p(x,y)ℓ hinge(h µ,i,x,y).
y∉{hi(x),pi(x)} y∉{hi(x),pi(x)}
Letq(x,p i(x))=1−p(x,p i(x))andq(x,h i(x))=1−p(x,h i(x)). Therefore,foranyi∈[k],the
conditionalregretoftheconstrainedhingelosscanbelowerboundedas
∆C
ℓ
,H(h,x)≥ inf sup{q(x,p i(x))(max{0,1+h(x,p i(x))}−max{0,1+h(x,h i(x))−µ})
hinge h∈H µ∈R
+q(x,h i(x))(max{0,1+h(x,h i(x))}−max{0,1+h(x,p i(x))+µ})}
≥q(x,h i(x))−q(x,p i(x)) (differentiatingwithrespecttoµ,htooptimize)
=p(x,p i(x))−p(x,h i(x))
Therefore,byLemma4,theconditionalregretofthetop-k losscanbeupperboundedasfollows:
k
∆C
ℓ
,H(h,x)=∑(p(x,p i(x))−p(x,h i(x)))≤k∆C
ℓ
,H(h,x).
k hinge
i=1
29MAOMOHRIZHONG
Bytheconcavity,takeexpectationsonbothsidesoftheprecedingequation,weobtain
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤k(E
ℓ
hinge(h)−E∗
ℓ
hinge(H )+M
ℓ
hinge(H )).
ThesecondpartfollowsfromthefactthatwhenthehypothesissetHissufficientlyrichsuchthat
A
ℓ
(H )=0,wehaveM
ℓ
(H )=0.
hinge hinge
D.4. ProofofTheorem19
The constrained ρ-margin loss is defined as ℓ ρ(h,x,y) = ∑y′ ≠ymin{max{0,1+h(x,y′)/ρ},1}.
Next, weshowthatthatℓ benefitsformH-consistencyboundsaswell. Thesecondpartfollows
ρ
from the fact that when the hypothesis set H is sufficiently rich such that A ℓρ(H ) = 0, we have
M ℓρ(H )=0. Aswiththeconstrainedhingeloss,theboundforℓ
ρ
islinear: E ℓρ(h)−E∗ ℓρ(H )≤ϵ⇒
E
ℓ
k(h)−E∗
ℓ
k(H )≤kϵ. Asaby-product,ℓ
ρ
isBayes-consistentwithrespecttoℓ k.
Theorem19 Assume that H is symmetric and complete. Then, for any 1 ≤ k ≤ n, the following
H-consistencyboundholdsfortheconstrainedρ-marginloss:
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤k(E ℓρ(h)−E∗ ℓρ(H )+M ℓρ(H )).
InthespecialcasewhereA ℓρ(H )=0,forany1≤k ≤n,thefollowingboundholds:
E
ℓ
k(h)−E∗
ℓ
k(H )≤k(E ℓρ(h)−E∗ ℓρ(H )).
ProofFortheconstrainedρ-marginlossℓ ,theconditionalregretcanbewrittenas
ρ
n n
∆C ℓρ,H(h,x)=
y∑
=1p(x,y)ℓ ρ(h,x,y)− hin ∈Hf
y∑
=1p(x,y)ℓ ρ(h,x,y)
n n
≥ ∑p(x,y)ℓ ρ(h,x,y)−inf ∑p(x,y)ℓ ρ(h µ,i,x,y),
y=1
µ∈R
y=1
⎧ ⎪⎪⎪⎪h(x,y), y ∉{p i(x),h i(x)}
whereforanyi∈[k],h µ,i(x,y)=⎨h(x,p i(x))+µ y =h i(x) Notethatsuchachoice
⎪⎪⎪⎪
⎩h(x,h i(x))−µ y =p i(x).
ofh leadstothefollowingequalityholds:
µ,i
∑
p(x,y)ℓ ρ(h,x,y)=
∑
p(x,y)ℓ ρ(h µ,i,x,y).
y∉{hi(x),pi(x)} y∉{hi(x),pi(x)}
Letq(x,p i(x))=1−p(x,p i(x))andq(x,h i(x))=1−p(x,h i(x)). Therefore,foranyi∈[k],the
conditionalregretoftheconstrainedρ-marginlosscanbelowerboundedas
∆C ℓρ,H(h,x)
h(x,p i(x)) h(x,h i(x))−µ
≥ inf sup{q(x,p i(x))(min{max{0,1+ },1}−min{max{0,1+ },1})
h∈H µ∈R ρ ρ
h(x,h i(x)) h(x,p i(x))+µ
+q(x,h i(x))(min{max{0,1+ },1}−min{max{0,1+ },1})}
ρ ρ
≥q(x,h i(x))−q(x,p i(x)) (differentiatingwithrespecttoµ,htooptimize)
=p(x,p i(x))−p(x,h i(x))
30TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
Therefore,byLemma4,theconditionalregretofthetop-k losscanbeupperboundedasfollows:
k
∆C
ℓ
k,H(h,x)=∑(p(x,p i(x))−p(x,h i(x)))≤k∆C ℓρ,H(h,x).
i=1
Bytheconcavity,takeexpectationsonbothsidesoftheprecedingequation,weobtain
E
ℓ
k(h)−E∗
ℓ
k(H )+M
ℓ
k(H )≤k(E ℓρ(h)−E∗ ℓρ(H )+M ℓρ(H )).
ThesecondpartfollowsfromthefactthatwhenthehypothesissetHissufficientlyrichsuchthat
A ℓρ(H )=0,wehaveM ℓρ(H )=0.
AppendixE. ProofsofR-consistencyboundsforcost-sensitivelosses
We first characterize the best-in class conditional error and the conditional regret of the target
cardinalityawarelossfunction(2),whichwillbeusedintheanalysisofR-consistencybounds.
Lemma20 AssumethatRissymmetricandcomplete. Then,foranyr ∈Kandx∈X,thebest-in
classconditionalerrorandtheconditionalregretofthetargetcardinalityawarelossfunctioncanbe
expressedasfollows:
C∗ (R,x)=min ∑p(x,y)c(x,k,y)
ℓ̃ k∈K
y∈Y
∆C
ℓ
,H(r,x)= ∑p(x,y)c(x,r(x),y)−min ∑p(x,y)c(x,k,y).
k
y∈Y
k∈K
y∈Y
ProofBydefinition,foranyr ∈Randx∈X,theconditionalerrorofthetargetcardinalityaware
lossfunctioncanbewrittenas
C ℓ̃(r,x)= ∑p(x,y)c(x,r(x),y).
y∈Y
SinceRissymmetricandcomplete,wehave
k
C∗ (r,x)=inf ∑p(x,y)c(x,r(x),y)=min ∑p(x,y)c(x,k,y).
ℓ̃ r∈R
y∈Y
k∈K
i=1
Furthermore,thecalibrationgapcanbeexpressedas
∆C ℓ̃,H(r,x)=C ℓ̃(r,x)−C∗ ℓ̃(R,x)=
y∑
∈Yp(x,y)c(x,r(x),y)−m k∈i Kn
y∑
∈Yp(x,y)c(x,k,y),
whichcompletestheproof.
31MAOMOHRIZHONG
E.1. ProofofTheorem15
For convenience, we let c(x,k,y) = 1−c(x,k,y), q(x,k) = ∑y∈Yp(x,y)c(x,k,y) ∈ [0,1] and
S (x,k)= ∑k′e ∈Kr(x er,k () x,k′). Wealsoletk min(x)=argmin k∈K(1−q(x,k))=argmin k∈K∑y∈Yp(x,y)c(x,k,y).
Theorem15 AssumethatRissymmetricandcomplete. Then,thefollowingR-consistencybound
holdsforthecost-sensitivecomp-sumloss:
E ℓ̃(r)−E∗ ℓ̃(R )+M ℓ̃(R )≤γ(E ℓ̃comp(r)−E∗ ℓ̃comp(R )+M ℓ̃comp(R ));
InthespecialcasewhereR =R all,thefollowingholds:
E ℓ̃(r)−E∗ ℓ̃(R all)≤γ(E ℓ̃comp(r)−E∗ ℓ̃comp(R all)),
√ √
where γ(t) = 2 t when ℓ̃comp is either ℓ̃ log or ℓ̃c eo xm pp; γ(t) = 2 nαt when ℓ̃comp is ℓ̃ gce; and
γ(t)=ntwhenℓ̃comp isℓ̃ mae.
ProofCaseI:ℓ=ℓ̃ log. Forthecost-sensitivelogisticlossℓ̃ log,theconditionalerrorcanbewrittenas
er(x,k)
C ℓ̃ log(r,x)=− y∑ ∈Yp(x,y) k∑ ∈Kc(x,k,y)log(
∑k′
∈Ker(x,k′ ))=− k∑ ∈Klog(S (x,k))q(x,k).
Theconditionalregretcanbewrittenas
∆C
ℓ̃
log,R(r,x)=−
k∑
∈Klog(S (x,k))q(x,k)− rin ∈Rf(−
k∑
∈Klog(S (x,k))q(x,k))
≥− ∑ log(S (x,k))q(x,k)− inf (− ∑ log(S µ(x,k))q(x,k)),
k∈K µ∈[−S (x,kmin(x)),S (x,r(x))] k∈K
where for any x ∈ X and k ∈ K, S µ(x,k) =
⎧ ⎪⎪⎪⎪ ⎨SS (( xx ,, ky m),
in(x))+µ
yy =∉{ r(k xm )in(x),r(x)}
Note that
⎪⎪⎪⎪
⎩S (x,r(x))−µ y =k min(x).
suchachoiceofS leadstothefollowingequalityholds:
µ
∑
log(S (x,k))q(x,k)=
∑
log(S µ(x,k))q(x,k).
k∉{r(x),kmin(x)} k∉{r(x),kmin(x)}
Therefore,theconditionalregretofcost-sensitivelogisticlosscanbelowerboundedas
∆C
ℓ̃
,H(h,x)≥ sup {q(x,k min(x))[−log(S (x,k min(x)))+log(S (x,r(x))−µ)]
log µ∈[−S (x,kmin(x)),S (x,r(x))]
+q(x,r(x))[−log(S (x,r(x)))+log(S (x,k min(x))+µ)]}.
32TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
By the concavity of the function, differentiate with respect to µ, we obtain that the supremum is
achievedbyµ∗ =
q(x,r(x))S (x,r(x))−q(x,kmin(x))S (x,kmin(x)).
Pluginµ∗,weobtain
q(x,kmin(x))+q(x,r(x))
∆C
ℓ̃
,H(h,x)
log
(S (x,r(x))+S (x,k min(x)))q(x,k min(x))
≥q(x,k min(x))log
S (x,k min(x))(q(x,k min(x))+q(x,r(x)))
(S (x,r(x))+S (x,k min(x)))q(x,r(x))
+q(x,r(x))log
S (x,r(x))(q(x,k min(x))+q(x,r(x)))
2q(x,k min(x)) 2q(x,r(x))
≥q(x,k min(x))log +q(x,r(x))log
q(x,k min(x))+q(x,r(x)) q(x,k min(x))+q(x,r(x))
(minimumisachievedwhenS (x,r(x))=S (x,k min(x)))
(q(x,r(x))−q(x,k min(x)))2
≥
2(q(x,r(x))+q(x,k min(x)))
(alog 2a +blog 2b ≥ (a−b)2 ,∀a,b∈[0,1](Mohrietal.,2018,PropositionE.7))
a+b a+b 2(a+b)
(q(x,r(x))−q(x,k min(x)))2
≥ . (0≤q(x,r(x))+q(x,k min(x))≤2)
4
Therefore,byLemma20,theconditionalregretofthetargetcardinalityawarelossfunctioncanbe
upperboundedasfollows:
1
∆C ℓ̃,H(r,x)=q(x,k min(x))−q(x,r(x))≤2(∆C
ℓ̃
,R(r,x))2.
log
Bytheconcavity,takeexpectationsonbothsidesoftheprecedingequation,weobtain
1
E (r)−E∗ (R )+M (R )≤2(E (r)−E∗ (R )+M (R ))2 .
ℓ̃ ℓ̃ ℓ̃ ℓ̃ log ℓ̃ log ℓ̃ log
ThesecondpartfollowsfromthefactthatM
ℓ̃
(R all)=0.
log
CaseII:ℓ=ℓ̃c eo xm pp
.
Forthecost-sensitivesumexponentiallossℓ̃c eo xm pp
,theconditionalerrorcan
bewrittenas
1
C ℓ̃c eo xm pp(r,x)= y∑ ∈Yp(x,y) k∑ ∈Kc(x,k,y) k∑
′
≠k′er(x,k′ )−r(x,k) = k∑ ∈K(S
(x,k)
−1)q(x,k).
Theconditionalregretcanbewrittenas
1 1
∆C ℓ̃c eo xm pp,R(r,x)= k∑ ∈K(S
(x,k)
−1)q(x,k)− rin ∈Rf( k∑ ∈K(S
(x,k)
−1)q(x,k))
1 1
≥ k∑ ∈K(S
(x,k)
−1)q(x,k)−
µ∈[−S
(x,kminin (xf
)),S
(x,r(x))]( k∑ ∈K(S
µ(x,k)
−1)q(x,k)),
where for any x ∈ X and k ∈ K, S µ(x,k) =
⎧ ⎪⎪⎪⎪ ⎨SS (( xx ,, ky m),
in(x))+µ
yy =∉{ r(k xm )in(x),r(x)}
Note that
⎪⎪⎪⎪
⎩S (x,r(x))−µ y =k min(x).
suchachoiceofS leadstothefollowingequalityholds:
µ
1 1
k∉{r(x)∑ ,kmin(x)}(S
(x,k)
−1)q(x,k)= k∉{r(x)∑ ,kmin(x)}(S
µ(x,k)
−1)q(x,k).
33MAOMOHRIZHONG
Therefore,theconditionalregretofcost-sensitivesumexponentiallosscanbelowerboundedas
1 1
∆C ℓ̃c eo xm pp,H(h,x)≥ µ∈[−S (x,kmis nu (xp )),S (x,r(x))]{q(x,k min(x))[S (x,k min(x)) − S (x,r(x))−µ]
1 1
+q(x,r(x))[S (x,r(x)) − S (x,k min(x))+µ]}.
By the concavity of the function, differentiate with respect to µ, we obtain that the supremum is
√ √
achievedbyµ∗ =
q(x,r(x)) √S (x,r(x))− q(x, √kmin(x))S (x,kmin(x)).
Pluginµ∗,weobtain
q(x,kmin(x))+ q(x,r(x))
∆C ℓ̃comp,H(h,x)
exp
√ √ 2
q(x,k min(x)) q(x,r(x))) ( q(x,k min(x))+ q(x,r(x))))
≥ S (x,k min(x)) + S (x,r(x))) − S (x,k min(x))+S (x,r(x)))
√ √ 2
≥( q(x,k min(x))− q(x,r(x))))
(minimumisachievedwhenS (x,r(x))=S (x,k min(x))= 1 2)
(q(x,r(x)))−q(x,k min(x)))2
≥
√ √ 2
( q(x,r(x)))+ q(x,k min(x)))
(q(x,r(x)))−q(x,k min(x)))2 √ √
≥ . ( a+ b≤2,∀a,b∈[0,1],a+b≤2)
4
Therefore,byLemma20,theconditionalregretofthetargetcardinalityawarelossfunctioncanbe
upperboundedasfollows:
1
∆C ℓ̃,H(r,x)=q(x,k min(x))−q(x,r(x))≤2(∆C ℓ̃comp,R(r,x))2.
exp
Bytheconcavity,takeexpectationsonbothsidesoftheprecedingequation,weobtain
1
E ℓ̃(r)−E∗ ℓ̃(R )+M ℓ̃(R )≤2(E
ℓ̃c eo xm
pp(r)−E∗
ℓ̃c eo xm
pp(R )+M
ℓ̃c eo xm
pp(R ))2 .
ThesecondpartfollowsfromthefactthatM ℓ̃comp(R all)=0.
exp
Case III: ℓ = ℓ̃ gce. For the cost-sensitive generalized cross-entropy loss ℓ̃ gce, the conditional
errorcanbewrittenas
α
1 er(x,k) 1
C ℓ̃gce(r,x)= y∑ ∈Yp(x,y) k∑ ∈Kc(x,k,y) α(1−(
∑k′
∈Ker(x,k′ )) )= α k∑ ∈K(1−S (x,k)α )q(x,k).
Theconditionalregretcanbewrittenas
1 1
∆C ℓ̃gce,R(r,x)=
α
k∑
∈K(1−S (x,k)α )q(x,k)− rin ∈Rf(
α
k∑
∈K(1−S (x,k)α )q(x,k))
1 1
≥ ∑(1−S (x,k)α )q(x,k)− inf ( ∑(1−S µ(x,k)α )q(x,k)),
α k∈K µ∈[−S (x,kmin(x)),S (x,r(x))] α k∈K
34TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
where for any x ∈ X and k ∈ K, S µ(x,k) =
⎧ ⎪⎪⎪⎪ ⎨SS (( xx ,, ky m),
in(x))+µ
yy =∉{ r(k xm )in(x),r(x)}
Note that
⎪⎪⎪⎪
⎩S (x,r(x))−µ y =k min(x).
suchachoiceofS leadstothefollowingequalityholds:
µ
1 1
∑
∑(1−S (x,k)α )q(x,k)=
∑
∑(1−S µ(x,k)α )q(x,k).
α α
k∉{r(x),kmin(x)} k∈K k∉{r(x),kmin(x)} k∈K
Therefore, the conditional regret of cost-sensitive generalized cross-entropy loss can be lower
boundedas
1
∆C ℓ̃gce,H(h,x)=
α µ∈[−S
(x,kmis nu (xp
)),S
(x,r(x))]{q(x,k min(x))[−S (x,k min(x))α +(S (x,r(x))−µ)α ]
+q(x,r(x))[−S (x,r(x))α +(S (x,k min(x))+µ)α ]}.
By the concavity of the function, differentiate with respect to µ, we obtain that the supremum is
achievedbyµ∗ = q(x,r(x))1−1 αS (x,r(x))−q( 1x,kmin(x))1−1 α 1S (x,kmin(x)). Pluginµ∗,weobtain
q(x,kmin(x))1−α+q(x,r(x))1−α
∆C ℓ̃gce,H(h,x)
≥ 1 (S (x,r(x))+S (x,k min(x)))α (q(x,k min(x))1−1 α +q(x,r(x))1−1 α)1−α
α
1 1
− q(x,k min(x))S (x,k min(x))α − q(x,r(x))S (x,r(x))α
α α
≥ αn1 α[2α (q(x,k min(x))1−1 α +q(x,r(x))1−1 α)1−α −q(x,k min(x))−q(x,r(x))]
(minimumisachievedwhenS (x,r(x))=S (x,k min(x))= n1)
(q(x,r(x))−q(x,k min(x)))2
≥ .
4nα
1 1 1−α
((a1−α+b1−α
) −
a+b
≥
α (a−b)2,∀a,b∈[0,1],0≤a+b≤1)
2 2 4
Therefore,byLemma20,theconditionalregretofthetargetcardinalityawarelossfunctioncanbe
upperboundedasfollows:
1
∆C ℓ̃,H(r,x)=q(x,k min(x))−q(x,r(x))≤2nα 2(∆C ℓ̃gce,R(r,x))2.
Bytheconcavity,takeexpectationsonbothsidesoftheprecedingequation,weobtain
1
E (r)−E∗ (R )+M (R )≤2nα 2(E (r)−E∗ (R )+M (R ))2.
ℓ̃ ℓ̃ ℓ̃ ℓ̃gce ℓ̃gce ℓ̃gce
ThesecondpartfollowsfromthefactthatM ℓ̃gce(R all)=0.
CaseIV:ℓ=ℓ̃ mae. Forthecost-sensitivemeanabsoluteerrorlossℓ̃ mae,theconditionalerrorcan
bewrittenas
er(x,k)
C ℓ̃mae(r,x)= y∑ ∈Yp(x,y) k∑ ∈Kc(x,k,y)(1−(
∑k′
∈Ker(x,k′ )))= k∑ ∈K(1−S (x,k))q(x,k).
35MAOMOHRIZHONG
Theconditionalregretcanbewrittenas
∆C ℓ̃mae,R(r,x)=
k∑
∈K(1−S (x,k))q(x,k)− rin ∈Rf(
k∑
∈K(1−S (x,k))q(x,k))
≥ ∑(1−S (x,k))q(x,k)− inf (∑(1−S µ(x,k))q(x,k)),
k∈K µ∈[−S (x,kmin(x)),S (x,r(x))] k∈K
where for any x ∈ X and k ∈ K, S µ(x,k) =
⎧ ⎪⎪⎪⎪ ⎨SS (( xx ,, ky m),
in(x))+µ
yy =∉{ r(k xm )in(x),r(x)}
Note that
⎪⎪⎪⎪
⎩S (x,r(x))−µ y =k min(x).
suchachoiceofS leadstothefollowingequalityholds:
µ
∑(1−S (x,k))q(x,k)= ∑(1−S µ(x,k))q(x,k).
k∈K k∈K
Therefore,theconditionalregretofcost-sensitivemeanabsoluteerrorcanbelowerboundedas
∆C ℓ̃mae,H(h,x)≥
µ∈[−S
(x,kmis nu (xp
)),S
(x,r(x))]{q(x,k min(x))[−S (x,k min(x))+S (x,r(x))−µ]
+q(x,r(x))[−S (x,r(x))+S (x,k min(x))+µ]}.
By the concavity of the function, differentiate with respect to µ, we obtain that the supremum is
achievedbyµ∗ =−S (x,k min(x)). Pluginµ∗,weobtain
∆C ℓ̃mae,H(h,x)
≥q(x,k min(x))S (x,r(x))−q(x,r(x))S (x,r(x))
1
≥ n(q(x,k min(x))−q(x,r(x))). (minimumisachievedwhenS (x,r(x))= n1)
Therefore,byLemma20,theconditionalregretofthetargetcardinalityawarelossfunctioncanbe
upperboundedasfollows:
∆C ℓ̃,H(r,x)=q(x,k min(x))−q(x,r(x))≤n(∆C ℓ̃mae,R(r,x)).
Bytheconcavity,takeexpectationsonbothsidesoftheprecedingequation,weobtain
E (r)−E∗ (R )+M (R )≤n(E (r)−E∗ (R )+M (R )).
ℓ̃ ℓ̃ ℓ̃ ℓ̃mae ℓ̃mae ℓ̃mae
ThesecondpartfollowsfromthefactthatM ℓ̃mae(R all)=0.
E.2. ProofofTheorem16
Theconditionalerrorforthecost-sensitiveconstrainedlosscanbeexpressedasfollows:
C ℓ̃cstnd(r,x)= ∑p(x,y)ℓ̃cstnd (r,x,y)
y∈Y
= ∑p(x,y) ∑ c(x,k,y)Φ(−r(x,k))
y∈Y k∈K
= ∑ q̃(x,k)Φ(−r(x,k)),
k∈K
36TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
where q̃(x,k) = ∑y∈Yp(x,y)c(x,k,y) ∈ [0,1]. Let k min(x) = argmin k∈Kq̃(x,k). We denote by
Φ exp∶t ↦ e−t the exponential loss function, Φ sq−hinge∶t ↦ max{0,1−t}2 the squared hinge loss
function,Φ hinge∶t↦max{0,1−t}thehingelossfunction,andΦ ρ∶t↦min{max{0,1−t/ρ},1},
ρ>0theρ-marginlossfunction.
Theorem16 AssumethatRissymmetricandcomplete. Then,thefollowingR-consistencybound
holdsforthecost-sensitiveconstrainedloss:
E ℓ̃(r)−E∗ ℓ̃(R )+M ℓ̃(R )≤γ(E ℓ̃cstnd(r)−E∗ ℓ̃cstnd(R )+M ℓ̃cstnd(R ));
InthespecialcasewhereR =R all,thefollowingholds:
E ℓ̃(r)−E∗ ℓ̃(R all)≤γ(E ℓ̃cstnd(r)−E∗ ℓ̃cstnd(R all)),
√
whereγ(t)=2 twhenℓ̃cstnd iseitherℓ̃c es xt pnd orℓ̃ sq−hinge;γ(t)=twhenℓ̃cstnd iseitherℓ̃
hinge
orℓ̃ ρ.
ProofCaseI:ℓ=ℓ̃c es xt pnd. Forthecost-sensitiveconstrainedexponentiallossℓ̃c es xt pnd,theconditional
regretcanbewrittenas
∆C
ℓ̃c es xt
pnd,R(r,x)=
k∑
∈Kq̃(x,kΦ exp(−r(x,k))− rin ∈Rf
k∑
∈Kq̃(x,k)Φ exp(−r(x,k))
≥ ∑ q̃(x,k)Φ exp(−r(x,k))−inf ∑ q̃(x,k)Φ exp(−r µ(x,k)),
k∈K
µ∈R
k∈K
whereforanyk ∈K,r
µ(x,k)=⎧ ⎪⎪⎪⎪ ⎨r r( (x x, ,y
k
m),
in(x))+µ
yy ∉ ={ r(k xm )in(x),r(x)}
Notethatsuchachoice
⎪⎪⎪⎪
⎩r(x,r(x))−µ y =k min(x).
ofr leadstothefollowingequalityholds:
µ
∑
q̃(x,k)Φ exp(−r(x,k))=
∑ ∑
q̃(x,k)Φ exp(−r µ(x,k)).
k∉{r(x),kmin(x)} k∉{r(x),kmin(x)}k∈K
Therefore,theconditionalregretofcost-sensitiveconstrainedexponentiallosscanbelowerbounded
as
∆C ℓ̃cstnd,R(r,x)
exp
≥infsup{q̃(x,k min(x))(er(x,kmin(x)) −er(x,r(x))−µ )+q̃(x,r(x))(er(x,r(x)) −er(x,kmin(x))+µ )}
r∈R µ∈R
√ √ 2
=( q̃(x,k min(x))− q̃(x,r(x))) (differentiatingwithrespecttoµ,r tooptimize)
2
⎛ q̃(x,r(x))−q̃(x,k min(x)) ⎞
= √ √
⎝ q̃(x,k min(x))+ q̃(x,r(x))⎠
1
≥ (q̃(x,r(x))−q̃(x,k min(x)))2. (0≤q̃(x,k)≤1)
4
Therefore,byLemma20,theconditionalregretofthetargetcardinalityawarelossfunctioncanbe
upperboundedasfollows:
1
∆C ℓ̃,H(r,x)=q̃(x,r(x))−q̃(x,k min(x))≤2(∆C ℓ̃cstnd,R(r,x))2.
exp
37MAOMOHRIZHONG
Bytheconcavity,takeexpectationsonbothsidesoftheprecedingequation,weobtain
1
E ℓ̃(r)−E∗ ℓ̃(R )+M ℓ̃(R )≤2(E
ℓ̃c es xt
pnd(r)−E∗
ℓ̃c es xt
pnd(R )+M
ℓ̃c es xt
pnd(R ))2 .
ThesecondpartfollowsfromthefactthatM ℓ̃cstnd(R all)=0.
exp
Case II: ℓ = ℓ̃ sq−hinge. For the cost-sensitive constrained squared hinge loss ℓ̃ sq−hinge, the
conditionalregretcanbewrittenas
∆C
ℓ̃
sq−hinge,R(r,x)=
k∑
∈Kq̃(x,k)Φ sq−hinge(−r(x,k))− rin ∈Rf
k∑
∈Kq̃(x,k)Φ sq−hinge(−r(x,k))
≥ k∑ ∈Kq̃(x,k)Φ sq−hinge(−r(x,k))− µin ∈Rf k∑ ∈Kq̃(x,k)Φ sq−hinge(−r µ(x,k)),
whereforanyk ∈K,r
µ(x,k)=⎧ ⎪⎪⎪⎪ ⎨r r( (x x, ,y
k
m),
in(x))+µ
yy ∉ ={ r(k xm )in(x),r(x)}
Notethatsuchachoice
⎪⎪⎪⎪
⎩r(x,r(x))−µ y =k min(x).
ofr leadstothefollowingequalityholds:
µ
∑
q̃(x,k)Φ sq−hinge(−r(x,k))=
∑ ∑
q̃(x,k)Φ sq−hinge(−r µ(x,k)).
k∉{r(x),kmin(x)} k∉{r(x),kmin(x)}k∈K
Therefore, the conditional regret of cost-sensitive constrained squared hinge loss can be lower
boundedas
∆C
ℓ̃
sq−hinge,R(r,x)
≥infsup{q̃(x,k min(x))(max{0,1+r(x,k min(x))}2 −max{0,1+r(x,r(x))−µ}2 )
r∈R µ∈R
+q̃(x,r(x))(max{0,1+r(x,r(x))}2 −max{0,1+r(x,k min(x))+µ}2 )}
1
≥ (q̃(x,k min(x))−q̃(x,r(x)))2. (differentiatingwithrespecttoµ,r tooptimize)
4
Therefore,byLemma20,theconditionalregretofthetargetcardinalityawarelossfunctioncanbe
upperboundedasfollows:
1
∆C ℓ̃,H(r,x)=q̃(x,r(x))−q̃(x,k min(x))≤2(∆C
ℓ̃
sq−hinge,R(r,x))2.
Bytheconcavity,takeexpectationsonbothsidesoftheprecedingequation,weobtain
1
E (r)−E∗ (R )+M (R )≤2(E (r)−E∗ (R )+M (R ))2 .
ℓ̃ ℓ̃ ℓ̃ ℓ̃ sq−hinge ℓ̃ sq−hinge ℓ̃ sq−hinge
ThesecondpartfollowsfromthefactthatM
ℓ̃
sq−hinge(R all)=0.
CaseIII:ℓ=ℓ̃ hinge. Forthecost-sensitiveconstrainedhingelossℓ̃ hinge,theconditionalregret
canbewrittenas
∆C
ℓ̃
hinge,R(r,x)=
k∑
∈Kq̃(x,k)Φ hinge(−r(x,k))− rin ∈Rf
k∑
∈Kq̃(x,k)Φ hinge(−r(x,k))
≥ ∑ q̃(x,k)Φ hinge(−r(x,k))−inf ∑ q̃(x,k)Φ hinge(−r µ(x,k)),
k∈K
µ∈R
k∈K
38TOP-k CLASSIFICATIONANDCARDINALITY-AWAREPREDICTION
whereforanyk ∈K,r
µ(x,k)=⎧ ⎪⎪⎪⎪ ⎨r r( (x x, ,y
k
m),
in(x))+µ
yy ∉ ={ r(k xm )in(x),r(x)}
Notethatsuchachoice
⎪⎪⎪⎪
⎩r(x,r(x))−µ y =k min(x).
ofr leadstothefollowingequalityholds:
µ
∑
q̃(x,k)Φ hinge(−r(x,k))=
∑ ∑
q̃(x,k)Φ hinge(−r µ(x,k)).
k∉{r(x),kmin(x)} k∉{r(x),kmin(x)}k∈K
Therefore,theconditionalregretofcost-sensitiveconstrainedhingelosscanbelowerboundedas
∆C
ℓ̃
hinge,R(r,x)≥ rin ∈Rfs µu ∈Rp{q(x,k min(x))(max{0,1+r(x,k min(x))}−max{0,1+r(x,r(x))−µ})
+q(x,r(x))(max{0,1+r(x,r(x))}−max{0,1+r(x,k min(x))+µ})}
≥q(x,r(x))−q(x,k min(x)). (differentiatingwithrespecttoµ,r tooptimize)
Therefore,byLemma20,theconditionalregretofthetargetcardinalityawarelossfunctioncanbe
upperboundedasfollows:
∆C ℓ̃,H(r,x)=q̃(x,r(x))−q̃(x,k min(x))≤∆C
ℓ̃
,R(r,x).
hinge
Bytheconcavity,takeexpectationsonbothsidesoftheprecedingequation,weobtain
E (r)−E∗ (R )+M (R )≤E (r)−E∗ (R )+M (R ).
ℓ̃ ℓ̃ ℓ̃ ℓ̃ hinge ℓ̃ hinge ℓ̃ hinge
ThesecondpartfollowsfromthefactthatM
ℓ̃
(R all)=0.
hinge
CaseIV:ℓ=ℓ̃ ρ. Forthecost-sensitiveconstrainedρ-marginlossℓ̃ ρ,theconditionalregretcan
bewrittenas
∆C ℓ̃ρ,R(r,x)=
k∑
∈Kq̃(x,k)Φ ρ(−r(x,k))− rin ∈Rf
k∑
∈Kq̃(x,k)Φ ρ(−r(x,k))
≥ ∑ q̃(x,k)Φ ρ(−r(x,k))−inf ∑ q̃(x,k)Φ ρ(−r µ(x,k)),
k∈K
µ∈R
k∈K
whereforanyk ∈K,r
µ(x,k)=⎧ ⎪⎪⎪⎪ ⎨r r( (x x, ,y
k
m),
in(x))+µ
yy ∉ ={ r(k xm )in(x),r(x)}
Notethatsuchachoice
⎪⎪⎪⎪
⎩r(x,r(x))−µ y =k min(x).
ofr leadstothefollowingequalityholds:
µ
∑
q̃(x,k)Φ ρ(−r(x,k))=
∑ ∑
q̃(x,k)Φ ρ(−r µ(x,k)).
k∉{r(x),kmin(x)} k∉{r(x),kmin(x)}k∈K
Therefore,theconditionalregretofcost-sensitiveconstrainedρ-marginlosscanbelowerboundedas
∆C ℓ̃ρ,R(r,x)
r(x,k min(x)) r(x,r(x))−µ
≥infsup{q̃(x,k min(x))(min{max{0,1+ },1}−min{max{0,1+ },1})
r∈R µ∈R ρ ρ
r(x,r(x)) r(x,k min(x))+µ
+q̃(x,r(x))(min{max{0,1+ },1}−min{max{0,1+ },1})}
ρ ρ
≥q̃(x,r(x))−q̃(x,k min(x)). (differentiatingwithrespecttoµ,r tooptimize)
39MAOMOHRIZHONG
Therefore,byLemma20,theconditionalregretofthetargetcardinalityawarelossfunctioncanbe
upperboundedasfollows:
∆C ℓ̃,H(r,x)=q̃(x,r(x))−q̃(x,k min(x))≤∆C ℓ̃ρ,R(r,x).
Bytheconcavity,takeexpectationsonbothsidesoftheprecedingequation,weobtain
E (r)−E∗ (R )+M (R )≤E (r)−E∗ (R )+M (R ).
ℓ̃ ℓ̃ ℓ̃ ℓ̃ρ ℓ̃ρ ℓ̃ρ
ThesecondpartfollowsfromthefactthatM ℓ̃ρ(R all)=0.
40